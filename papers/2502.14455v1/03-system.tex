\section{System implementation}

In this section, we first introduce the nano-UAV platform used in our work, along with the Webots simulation employed.
Afterward, we discuss the deep learning models we port to our platform for the detection of pest insects and the routing algorithms we adopt for path planning.


\subsection{Robotic platform} \label{subsec:robotic_platforms}

Our work relies on a Crazyflie 2.1\footnote{https://www.bitcraze.io/products/crazyflie-2-1-plus} nano-UAV for the exploration of the vineyard that consumes, on average, evaluated with wind speeds between 0.5 and \SI{3.4}{\meter/\second}, $\sim$\SI{8.8}{\watt}~\cite{9811834}, including the electronics.
The UAV features an STM32 MCU, providing auto-pilot functionalities and acting as an interface for peripherals and sensors, that requires up to \SI{138}{\milli\watt}@\SI{168}{\mega\hertz}.
Basic functionalities involve state estimation and a low-level proportional-integral-derivative (PID) controller.
The UAV has two expansion boards, as detailed in Figure~\ref{fig:gap9drone}-A, in addition to the flow deck: a custom-made board that connects the STM VL53LC5CX\footnote{\href{https://www.st.com/resource/en/datasheet/vl53l5cx.pdf}{https://www.st.com/resource/en/datasheet/vl53l5cx.pdf}} ToF depth sensor to the Crazyflie 2.1 via I2C bus; and a GAP9Shield~\cite{müller2024gap9shield150gopsaicapableultralow}.
We report in Figure~\ref{fig:pie_chart_power} the power breakdown of our robotic platform.
The ToF depth sensor provides 8$\times$8 depth maps at~\SI{15}{\hertz} while consuming~\SI{313}{\milli\watt}. 
Each pixel of the 8$\times$8 depth map represents an independent reading of depth in a square field-of-view (FoV) with a \SI{63}{\degree} diagonal; depth ranges between 0 and~\SI{4}{\meter}.
The sensor specification reports an accuracy of $\pm15$\si{\milli\meter} for readings in the~\SI{20}{\milli\meter} to~\SI{20}{\centi\meter} range, while from~\SI{20}{\centi\meter} to~\SI{4}{\meter} the error grows to $\pm11\%$. 
The framerate of the depth sensor provides an upper limit to the objects' speed to guarantee its detectability.  
We report in Figure~\ref{fig:heatmap_distances_speed} the maximum detectable speed, which ranges between \SI{4.5}{\meter/\second} and \SI{19.5}{\meter/\second}, that depends on both the distance from the sensor and the object side projected on the depth sensor frame.
We consider distances up to~\SI{0.65}{\meter} since it is the typical working distance of the depth sensor in the case of gray targets, with 17\% reflectance, which represents a worst-case scenario for our depth sensor.



\begin{figure*}[tb]
\centering
\includegraphics[width=1.0\linewidth]{images/gap9shield_crazyflie.jpg}
%\caption{GAP9Shield block diagram (A) and picture of our nano-UAV prototype (B).}
\caption{Picture of our nano-UAV prototype (A) and the GAP9Shield block diagram (B).}
\label{fig:gap9drone}
\end{figure*}

\begin{wrapfigure}{R}{0.5\textwidth}
  \begin{center}
    \includegraphics[width=0.48\textwidth]{images/pie_chart_rebuttal_gap9.png}
  \end{center}
  \caption{\label{fig:pie_chart_power}Power breakdown of our robotic platform.}
\end{wrapfigure}




The GAP9Shield provides onboard intelligence enabled by a RISC-V-based GAP9\footnote{https://greenwaves-technologies.com/gap9\_processor} SoC together with an RGB OmniVision OV5647 camera with up to \SI{45}{frame/\second} @ VGA image resolution.
The SoC, depicted in Figure~\ref{fig:gap9drone}-B, features ten cores equipped with a single-precision floating point unit (FPU).
The cores are logically split into two domains, i.e., the \textit{fabric controller} (FC) that features one core and the \textit{cluster} (CL) with nine cores. 
The FC orchestrates the work for the multi-core cluster and provides an interface with external peripherals, while the CL is particularly suitable for CNNs since it can handle general-purpose intense parallel workloads.
The GAP9Shield has a four levels memory hierarchy which is composed of two on-chip memories, an L1 memory of~\SI{128}{\kilo\byte}, and an L2 memory of~\SI{1.6}{\mega\byte} while the~\SI{32}{\mega\byte} RAM and the~\SI{64}{\mega\byte} FLASH are hosted off-chip.
The GAP9 SoC includes an \textit{int8} hardware accelerator, the NE16, specifically tailored for linear algebra computations that can, among others, accelerate convolutions execution.

\subsection{Insect detector}

To address insect detection and classification, we utilize a CNN that we designed~\cite{crupi2024deep} to fit the computational constraints of the GAP9. 
Specifically, we focus on fine-tuning, testing, and deploying a MobileNet-based architecture~\cite{howard2019searching} that was originally pre-trained on the COCO dataset~\cite{cocodataset}. 
Fine-tuning, a well-known method in transfer learning~\cite{TransferLearning}, involves adapting a model trained on one task -- such as image classification on the COCO dataset~\cite{cocodataset} -- to a similar task in a different domain while taking advantage of the general features learned in the original training.

\begin{wrapfigure}{tb!}{0.5\textwidth}
  \begin{center}
    \includegraphics[width=0.48\textwidth]{images/heatmap_distances_speed.png}
  \end{center}
  \caption{\label{fig:heatmap_distances_speed}The maximum detectable object speed depends on both the object dimension ($O_\text{side}$) and the distance from the sensor. $O_\text{side}$ is defined as the projection of the object on the sensor frame.}
\end{wrapfigure}


As Figure~\ref{fig:ssdlite_mnet} reports, we selected the MobileNetV3 architecture combined with an SSDLite detector, which is a variant of the MobileNetV3 with SSD, as proposed in~\cite{9601235} for edge device deployment. 
The backbone of the network is a MobileNetV3~\cite{howard2019searching} model, consisting of~\SI{3.44}{\mega\nothing} parameters and requiring~\SI{584}{\mega\nothing MAC} per inference, similar to the architecture described in~\cite{9601235}. 
The SSDLite detector, serving as the network’s head, modifies the standard SSD by replacing regular convolutions with depth-wise separable convolutions. 
This change significantly reduces computational cost -- up to 3.3$\times$ -- without affecting the mAP, as shown in Table~\ref{tab:other_models_params}. 
The SSDLite detector also includes a non-maximal suppression layer to filter out redundant or low-confidence predictions. 
This model takes a 320$\times$240 pixel image as input and outputs multiple bounding boxes along with class labels and confidence scores. 
We fine-tune the network over 300 epochs using an SGD optimization algorithm with a learning rate of 0.00025. 
The training process follows a learning rate schedule with a momentum of 0.9, a weight decay of 0.0005, a warmup phase of 10 epochs, and a minimum learning rate of 0.00005.

\begin{wrapfigure}{R}{0.5\textwidth}
  \begin{center}
    \includegraphics[width=0.48\textwidth]{images/ssdlite_mnet.png}
  \end{center}
  \caption{\label{fig:ssdlite_mnet}SSDLite with MobileNetV3 backbone architecture.}
\end{wrapfigure}

\subsubsection{Dataset}


Since our task is insect detection, we adopt the dataset from~\citet{9601235}, which includes more than 3,300 images of insects, spanning three classes: \textit{Popillia japonica}, \textit{Cetonia aurata}, and \textit{Phyllopertha horticola}. 
The first is a dangerous pest insect, while the other two, often mistaken for the first, are not.
In total, the dataset contains 1,422, 1,318, and 877 samples for each class, respectively.
Note that the dataset is the result of a strict filtering process starting from a collection of more than 36,000 images gathered through the Internet. 
This ensures the near-absence of duplicates that could harm model evaluation.
Even though the dataset is not publicly available, a similar dataset can be built by means of popular image search engines, e.g., Google, Flickr, using the 3 insect names as search keys.
Figure~\ref{fig:datasetSamples} shows examples of the three insect classes.
The dataset was split into training and testing data, with an 80-20\% ratio.

\begin{wrapfigure}{t}{0.5\textwidth}
  \begin{center}
    \includegraphics[width=0.48\textwidth]{images/samples_dataset.png}
  \end{center}
  \caption{\label{fig:datasetSamples}Samples of the three classes in our dataset~\citep{9601235}.}
\end{wrapfigure}


\subsubsection{Deployment}

To deploy the CNN, we use a platform-specific tool, NN-Tool, developed by GWT, that converts Python code into C code tailored for the GAP9 SoC.
NN-Tool supports deployment from \textit{onnx} or \textit{tflite} files. 
In our case, we fine-tune the MobileNetV3 with the SSDLite detector (pre-trained on COCO in PyTorch) on our dataset to produce a \textit{float32} CNN, which is then converted into an \textit{onnx} file. 
To ensure compatibility with NN-Tool, non-maximal suppression is implemented outside of the neural model, as it is not supported by the tool. 
NN-Tool also handles tensor tiling and memory hierarchy management, optimizing usage across all on-chip memory levels. 
As the GAP9 has a single-precision FPU, we deploy the network in float16 and int8 formats.
Additionally, we utilize the NE16 hardware accelerator for CNN applications, which requires CNNs quantized in int8. 
Thus, we deploy on the GAP9 SoC three versions of our CNN: float16, int8, and int8 with NE16 acceleration.
Although the platform includes an FPU for hardware-based floating point operations, we still apply int8 quantization to reduce memory usage with minimal accuracy loss, as can be seen in section~\ref{subsec:object_detection_performance}.
For quantization, we estimate the range of \textit{float32} values to map into \textit{int8}, using a subset of the training data to compute expected activation values for the neural network's layers.


\begin{figure}[tb]
\centering
\includegraphics[width=1.0\columnwidth]{images/environment.jpg}
\caption{A) Split of the vineyard. Each UAV explores one portion of the environment. B) Three different testing areas.}
\label{fig:webots_worlds}
\end{figure}

\begin{figure}[tb]
\centering
\includegraphics[width=1.0\columnwidth]{images/depth_map_to_2d_map_ver_06.jpg}
\caption{A sample from the Webots simulator where the nano-UAV is facing an obstacle, with an external view in (A), onboard camera in (B), and ToF readings (in \SI{}{\meter}) in (C). D) 4$\times$\SI{4}{\meter} quantized (\SI{10}{\centi\meter} cells) local occupancy map obtained from the depth sensor. The map reports the local and the global path, with the collision point of the global path on the obstacle detected by the depth sensor.}
\label{fig:depth_to_occupancy}
\end{figure}

\subsection{Routing}
\label{subsec:routing}
\begin{wraptable}{R}{0.5\textwidth}
    \small\centering
    \caption{Cost of the path depending on the destination node occupancy and corresponding color in the occupancy map.}
    \addtolength{\tabcolsep}{10pt}
    \label{tab:occupancy_scheme}
    \resizebox{0.48\columnwidth}{!}{%
    \begin{tabular}{lll}
    \toprule
    \textbf{Destination node}&\textbf{Color scheme}&\textbf{Cost (c)}\\
    \midrule
    \textbf{Global path}&Violet&0\\
    \textbf{Free cell}&Green&25\\
    \textbf{Unknown cell}&Blue&50\\
    \textbf{Safety area}&Orange&75\\
    \textbf{Obstacle}&Red&$\infty$\\
    \bottomrule
    \end{tabular}
    }
\end{wraptable}


The routing algorithm that we use for obstacle avoidance relies on A*. 
This algorithm takes as input a 2D occupancy map, the source ($u_s$, $v_s$), and the destination ($u_d$, $v_d$), which are defined on the map with the horizontal and vertical coordinate ($u$, $v$).
The algorithm aims to find the optimal path (based on a cost metric) that connects the source and the destination, considering only feasible cells, i.e., cells that do not contain obstacles.
%The cost metrics that we employ in this work are two: the first depends only on the Euclidean distance between the nodes, while the second sums the Euclidean distance between the nodes and a cost $c$ of the connection that changes depending on the next node state, i.e., if it is part of the global path, a free cell, an unknown cell, if it is in the safety area close to an obstacle or an obstacle, according to Table~\ref{tab:occupancy_scheme}.
%\firstreviewer{Cells where we detect an obstacle are marked as obstacle cells, and cells closer than two cells at obstacles are marked as safety area cells.
%Cells that are occluded from the line of sight of the ToF distance sensor are marked as unknown cells, while cells that are in the line of sight but are not marked as obstacles or safety areas are marked as free cells.
%Finally, cells that belong to the preplanned global path, which are not obstacle or safety area cells, are marked as global path cells.}
In this work, we employ two cost metrics to evaluate connections between nodes. 
The first metric considers only the Euclidean distance between nodes. 
The second combines the Euclidean distance with an additional cost $c$, which varies depending on the state of the next node. 
Nodes can belong to one of several categories: obstacle cells, which are identified as containing obstacles; safety area, which is within two cells of an obstacle; unknown cells, which are outside the line of sight of the ToF distance sensor; free cells, which are within the line of sight but not classified as obstacles or safety areas; and global path cells, which are part of the preplanned global path, provided they are not obstacles or within safety areas. 
These classifications, along with their associated costs, are detailed in Table~\ref{tab:occupancy_scheme}.

%\firstreviewer{The cost for each cell is selected to avoid cells with an obstacle, i.e., in our $40\times40$ cells map, the cost for the obstacle should be at least $40\times40\times \text{cost}_\text{safety area}$ thus exceeding the cost of the second most expensive cell through the entire map. 
%For this purpose, we assign an inf cost to obstacle cells.
%To ensure safe obstacle avoidance, we prefer to use the safety area cells only if no other path is available thus, we assign the second most expensive cost to safety area cells.
%Free cells should take precedence over all other cell types except for global path cells. 
%In each iteration of the algorithm, this category consists solely of cells that are neither obstacles nor within the safety area. 
%Global path cells form the shortest route on the global map and include predefined waypoints essential for inspection. 
%As such, we prioritize them and assign a lower cost.
%The specific values of 0, 25, 50, 75, and inf can be adjusted as long as they follow the previously established rules. 
%These adjustments will not impact the computed path as soon as obstacle cells are never considered destinations and the distance-based cost exceeds the minimum cost difference between the cell types.}

The cost is designed to guarantee safe obstacle avoidance, with global path cells having the lowest cost and obstacle cells having the highest. 
In a local map of size  $N\times M$  (e.g.,  N = 40  and  M = 40  in our setup), the cost of obstacle cells must exceed the aggregate cost of all non-obstacle cells to ensure they are never chosen by the path planning algorithm as such they are assigned with a cost value of infinity ($\infty$).
Safety area cells, located around obstacles, are assigned the next highest cost and are intended for traversal only when no alternative routes exist.  
Unknown cells, which represent areas outside the depth sensor’s field of view, are assigned a higher cost than free cells because they could potentially contain hidden obstacles or be near them (safety area cells). 
Free cells, apart from global path cells, are prioritized as the most suitable for traversal. 
However, global path cells take precedence over all others since they form part of the predefined route, which includes essential waypoints for inspection.
While the absolute values of the costs (e.g., 0, 25, 50, 75, and  $\infty$ ) can be adjusted, it is crucial to maintain the same ranking between cells' cost to ensure effective pathfinding.

The time that A* requires to find a solution that connects the source to the destination depends on the cost metric.
In fact, the cost metric is used to prune the number of paths that need to be updated.
A convex cost metric results in a lower number of updates since, at each update, the cost represents a global minimum.
A measure of the efficiency of a cost metric for the A* algorithm is named b*, i.e., the effective branching factor.
Given N, the number of nodes generated during the expansion, and d, the depth of the solution, the effective branching factor can be determined by solving the equation: $N=b^*+(b^*)^2+...+b^d$.


The multi-UAV path-planning problem is reduced to multiple single-UAV routing problems by splitting the entire field into several areas of maximum size 40$\times$\SI{40}{\meter} as depicted in Figure~\ref{fig:webots_worlds}-A; this is the maximum size that a single UAV can explore, at an average flight speed of \SI{1}{\meter/\second}, within its battery lifetime, i.e., $\sim$\SI{6}{\minute}.
To improve the variety of our testing areas, we create different sizes, as depicted in Figure~\ref{fig:webots_worlds}-B, i.e., Env 1 of size 10$\times$\SI{10}{\meter}, Env2 of size 20$\times$\SI{20}{\meter}, and Env 3 of size 40$\times$\SI{40}{\meter}.
% Each UAV has to cover the entire assigned portion of the vineyard.
The routing problem is split into two steps: a global and a local part.

The global routing problem is solved once for each UAV before the start of the exploration mission, and a predefined path through waypoints, which define the pose of the drone in 4 degrees of freedom (x, y, z, $\phi$), is created to explore all the rows of the vineyard. 
The global routing is performed on a quantized global occupancy map representing the entire environment of size $L_{global - map} \times L_{global - map}$ with $L_{global - map}=$\SI{10}{\meter},~\SI{20}{\meter} or~\SI{40}{\meter} depending on the environment as reported in Figure~\ref{fig:webots_worlds}-B.
The cells of the map are of side $L_{cell}=$10$\times$\SI{10}{\centi\meter}, which is equal to the side of the drone.
However, it considers only static objects that are present a priori in the map.
If the environment is static, i.e., no new obstacles appear on the map, the UAV can follow the predefined path and thus perform the exploration, but since the vineyard is a dynamic environment, new obstacles may appear between waypoints. 

To address this problem, we employ A* running entirely on the STM32 MCU available onboard the UAV. 
Our system is designed for 2D obstacle avoidance, limiting UAV movements to a horizontal plane, as vineyards primarily consist of open, horizontal areas. 
Although UAVs can avoid obstacles by moving vertically we want to apply the same obstacle avoidance algorithm to ground robots, that must eventually reach pest hotspots, which can only navigate in 2D. 
The need for a ground robot creates a critical requirement for largely unobstructed spaces to enable effective operation.
From the 8$\times$8 depth maps obtained from the ToF depth sensor, we extract the 4th row starting from the top, thus obtaining a 1$\times$8 array as shown in Figure~\ref{fig:depth_to_occupancy}-C. 
The eight distances of the 4th row, measured with the ToF sensor, are then back-projected on a quantized 2D local occupancy map, representing the top view of the environment in the FoV of the sensor per each instant.
The local occupancy map is a square of dimension $L_{map} \times L_{map}$ quantized with cells of side $L_{cell}$.
We define $L_{map}=$\SI{4}{\meter} since the maximum distance of the depth sensor is \SI{4}{\meter}.
Figure~\ref{fig:depth_to_occupancy}-D reports the occupancy map with the detected obstacles depicted in red as reported by the color scheme in Table~\ref{tab:occupancy_scheme}. 
Our system maps obstacles conservatively by marking an entire cell as occupied if any part contains an obstacle. 
This approach ensures the UAV avoids potential collisions with inaccurately mapped obstacles, as the method reliably designates all objects requiring avoidance as obstacle cells.

Our approach operates independently of time and is effective for static and dynamic obstacles by generating a new map for each depth map measurement. 
For dynamic obstacles as for static ones, we initiate local planning whenever an obstacle intersects the UAV’s current path. 
If an obstacle that previously caused replanning for an earlier depth map measurement overlaps the path again, the system triggers local planning, as it does not store a history of obstacle positions.
The pose of the UAV in the world frame for our work is assumed to be provided through tag-based localization or ultra-wideband systems~\cite{10257228}. 
Examples of maps are reported in Figure~\ref{fig:webots_worlds}-B.


\subsubsection{Simulation and onboard implementation}


\label{subsec:sim_env}
Webots\footnote{https://cyberbotics.com/\#webots} is an open-source, cross-platform simulator that integrates a model of the Crazyflie 2.1 nano-UAV, accurately simulating its dynamics and all of its sensors, including the camera. 
We use the simulator to develop and test our routing algorithms in the three different areas described in Section~\ref{subsec:routing}.
Figure~\ref{fig:depth_to_occupancy} shows a screenshot from the simulator, with the 3rd person view, the onboard camera view, the depth map obtained from the ToF, and the local occupancy map.
%Figure~\ref{fig:depth_to_occupancy}-A depicts a simulated vineyard, one of our testing environments; Figure~\ref{fig:depth_to_occupancy}-B shows the camera view from the UAV; Figure~\ref{fig:depth_to_occupancy}-C reports a depth map obtained from the ToF depth sensor onboard the UAV.



We implement the system in two versions: a Python-based version for performing simulations in Webots and a C-based version for deployment on the Crazyflie nano-UAV.
Since the local occupancy map is defined in the body frame of the UAV to perform a local routing, we consider the source always as the central cell on the $x$ axis, i.e., the cell ($\floor*{\frac{L_{map}}{L_{cell}}}$, 0) while as the destination we consider the next waypoint in the world frame projected on the local 2D occupancy map.
Figure~\ref{fig:depth_to_occupancy}-D depicts the source, the destination, the predefined path (red line), and the path that the UAV is currently following (black line), i.e., the locally planned path computed to avoid the obstacle.
The path obtained in the body frame of the UAV is then back-projected in the 3D space in the world frame to perform the tracking with the onboard controller. 
The back projection is performed considering a fixed flying altitude of \SI{1}{\meter}.

Since our UAV requires 4 degree-of-freedom waypoints (x, y, z, and $\phi$) to define a path, we consider the path in the world frame, adding the yaw for each step to orient the UAV with the ToF sensor continuously facing the direction of movement.
To obtain this behavior, we compute the yaw for each step as the $\phi=\arctantwo{(y_{d} - y_{s}, x_{d} - x_{s})}$ where s is the UAV current point and d is a point that is five steps forward on the path or the last point if the remaining path has less than five steps.
