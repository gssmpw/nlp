\section{Introduction}
Tabular data is pervasive in our daily lives, appearing in formats such as databases, scientific articles, web pages, and spreadsheets~\cite{chen2000mining,hurst2000interpretation,hu2023chatdb}. The structured nature of tabular data enables the systematic organization of information into rows and columns, facilitating efficient sorting, querying, and manipulation~\citep{pujara2021tables, chen2020open}. Consequently, table understanding and reasoning have emerged as a significant area of interest in NLP, garnering much attention from researchers~\citep{bao2018table, zhang2024survey}.
\input{figure/example}

Thrived on the logical reasoning capabilities of Large Language Models (LLMs), using LLMs for dealing with table-related tasks has become a mainstream research direction~\cite{chen2023large,zhang2024survey,dong2024large}. Existing table understanding methods convert tables into linear text sequences and focus on designing prompts or instructions to stimulate LLMs to conduct effective reasoning over tables~\cite{chen2023large,wang2024chainoftable}. However, they typically provide a fixed text representation of the tabular format for reasoning. Recent studies have also shown that LLMs are sensitive to the text representation of tables~\cite{liu-etal-2024-rethinking}, motivating researchers to explore the most suitable text-based tabular formats for different table understanding scenarios~\cite{zhang2024flextaf,sui2024table,singha2023tabular}.


Besides text-based table representations, many works use the screenshot of table as its image-based representation during reasoning to explore the effectiveness of Multi-modal Large Language Models (MLLMs) in understanding table images~\cite{deng2024tables,ZhengFSS0J024}. As shown in Figure~\ref{fig:example}, both text-based and image-based table representations potentially play distinct roles in enhancing the table reasoning abilities of MLLMs. Specifically, in the first case, the question asks, ``What is the total number of wins listed for the United States?'', which requires the model to identify the wins of the United States, namely ``18'', ``2'' and ``2'', and then sum them to obtain the correct answer, ``22''. The text-based table representation enables LLMs to produce the correct answer because the question relies more on the arithmetic ability of language models. In contrast, the image-based table representation allows MLLMs to correctly answer the question in the second case. This is enabled by the visual annotation of teams with different colors to represent the win-loss situation. Both the color and cell position in the image provide crucial semantics to help MLLMs accurately answer the question. Despite these advantages, existing works~\cite{deng2024tables,ZhengFSS0J024} mainly focus on investigating the table understanding capabilities of MLLMs using unimodal representations, leaving room for further exploration of multi-modal representations to enable more effective table reasoning.

This paper introduces the \textbf{H}ybr\textbf{I}d-modal \textbf{P}reference o\textbf{P}timizati\textbf{O}n (\method{}) model, which integrates both text-based and image-based table representations for enhancing the table understanding capability of MLLMs. Specifically, \method{} proposes a Hybrid-Modal Preference Optimization method to guide MLLMs in answering questions by leveraging more comprehensive information from different modalities of table representations. \method{} prompts the MLLM to generate responses based on both unimodal and multi-modal representations of the table. Then, it selects the most representative negative responses using the self-consistency~\cite{liu-etal-2024-rethinking} of MLLMs when answering questions based on different modalities, thereby mitigating unnecessary modality bias during training. These negative responses are subsequently collected to optimize the MLLMs using the DPO method~\cite{rafailov2023direct}, helping the model to assign higher probabilities to ground truth answers over negative responses.

Our experiments demonstrate the effectiveness of our \method{} model by achieving more than a 4\% improvement over different table understanding models, which underscores the importance of incorporating both text-based and image-based representations in table understanding tasks. Additionally, \method{} significantly enhances the performance of MLLMs even with unimodal table representations, illustrating the generalization ability of our training method. Our further analyses show that \method{} optimizes MLLMs to better extract semantic information, generate more consistent answers, and engage in diverse reasoning processes based on table representations of different modalities, thereby enabling more accurate predictions based on multi-modal table representations.