\input{table/ablation_hippo_single_modality}
\section{Appendix}

\subsection{License}
This study uses MMTab (Apache-2.0 License\footnote{\href{https://www.apache.org/licenses/LICENSE-2.0}{https://www.apache.org/licenses/LICENSE-2.0}}), 
TABMWP (MIT License\footnote{\href{https://opensource.org/licenses/MIT}{https://opensource.org/licenses/MIT}}), 
WTQ (MIT License), 
HiTab (C-UDA License\footnote{\href{https://cdla.dev/computational-use-of-data-agreement-v1-0/}{https://cdla.dev/computational-use-of-data-agreement-v1-0/}}), 
TAT-QA (MIT License), 
FeTaQA (CC-BY-SA 4.0 License\footnote{\href{https://creativecommons.org/licenses/by-sa/4.0/}{https://creativecommons.org/licenses/by-sa/4.0/}}), 
TabFact (CC-BY-SA 4.0 License), 
and InfoTabs (Apache-2.0 License) in experiments. 
All of these licenses and agreements allow the use of their data for academic purposes.

\subsection{Performance of \method{} Using Table Representations of Different Modalities}
This section shows the performance of \method{} using various table representations: text-based, image-based, and multi-modal table representations.

As shown in Table~\ref{tab:ablation_hippo}, \method{} with a multi-modal table representation outperforms both text-based and image-based representations across most TQA and TFV datasets. Specifically, on average, the multi-modal representation yields a significant improvement of over 3.5\% across all datasets, compared to both image-based and text-based representations. The superior performance underscores \method{}'s ability to effectively learn and integrate semantic information from both text-based and image-based representations, leading to more comprehensive table understanding.

\subsection{Effectiveness of Different Training Strategies}
This experiment evaluates the effectiveness of table understanding by examining the prediction consistency across table understanding models trained using different strategies.

As shown in Figure~\ref{fig:consistency}, we evaluate the consistency of Zero-Shot, SFT, DPO, and \method{} methods in predicting the golden labels. Specifically, we randomly sample 500 cases and ask each model to generate 10 outputs for consistency evaluation. A higher consistency score indicates that the model produces more confident predictions aligned with the ground truth. Overall, DPO-based optimization methods improve the consistency of MLLM predictions with respect to the ground truth, suggesting that DPO assigns a higher probability than SFT for generating the correct answer by learning from preference pairs. Notably, \method{} further enhances its prediction consistency on both datasets, demonstrating that the training strategy of \method{} helps MLLMs make more confident and accurate predictions. \method{} enhances the DPO training process by sampling more diverse responses from the table representations of different modalities.
\input{figure/consistency_ratio}


\input{table/different_table_size}
\input{figure/prompt}
\subsection{Performance of \method{} on Tables of Different Scales}
In this section, we analyze the performance of \method{} and Zero-Shot on tables of varying scales.

In our experiments, we categorize tables into three groups: Small, Medium, and Large. Specifically, Small refers to tables with fewer than 1,000 tokens, Medium includes tables with 1,000 to 2,000 tokens, and Large encompasses tables with more than 2,000 tokens. The distribution is as follows: Small tables (70.28\%), Medium tables (19.45\%), and Large tables (10.27\%).

As shown in Table~\ref{tab:performance_different_table_size}, both models exhibit a decrease in accuracy as table size increases, highlighting the challenge of capturing and reasoning with complex information from larger tables. Notably, \method{} consistently outperforms Zero-Shot across all table scales, particularly for large tables, demonstrating its superior robustness in handling larger tables. This performance advantage suggests that \method{} remains effective even as the complexity and scale of the tabular data increase.

\subsection{Additional Experimental Details}\label{app:experiments}
In this section, we provide a detailed description of the steps to construct the DPO training data.

The training data is sourced from~\citet{ZhengFSS0J024}. The inputs are categorized into three types: text-based table representations ($L(T)$), image-based table representations ($V(T)$), and multi-modal table representations, which are formed by concatenating the text-based and image-based representations ($L(T)$, $V(T)$). The construction of the DPO training dataset utilized \texttt{TABMWP}, \texttt{WTQ}, \texttt{TAT-QA}, \texttt{TabFact}, and \texttt{InfoTabs}. We exclude \texttt{FeTaQA} due to its evaluation metric being BLEU, which does not focus on the accuracy of question answering. Additionally, the \texttt{HiTab} dataset is excluded because it involves multi-level tables, which present formatting challenges when converted to Markdown. This conversion can lead to formatting inconsistencies, making it less suitable for training. From each of the chosen datasets, we extract 2,000 instances, resulting in a combined dataset of 10,000 training instances.

For data sampling, we use the MiniCPM-V-2.6 model with a temperature setting of 1 to generate 10 candidate responses for each modality. These responses are rigorously evaluated against ground truth answers to assess their accuracy. For DPO training, the ground truth is labeled as the positive response $y^+$, while the most frequent incorrect response is designated as the negative one $y^-$ for DPO training (Eq.~\ref{eq:dpo}).


\subsection{Prompt Templates Used in \method{}} \label{app:hippo_prompt}
We follow the approach of previous work~\cite{ZhengFSS0J024} modifying the prompt templates to better align with our objectives for multi-modal table representations. The prompt templates used in our experiments are shown in Figure~\ref{fig:prompt}.

\subsection{Prompt Templates Used to Generate Chain-of-Thought}\label{app:cot_prompt}
In this section, we represent the CoT (Chain of Thought) prompt we used in Figure~\ref{fig:cot_prompt}. For each table representation: image, text, and multi-modal, we provide the model with both the table representation and its corresponding answer. The model is then instructed to generate a modality-specific thinking step.

\input{figure/cot_prompt}