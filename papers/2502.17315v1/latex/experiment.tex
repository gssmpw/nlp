\input{table/data_stastic}
\section{Experimental Methodology}
In this section, we describe the datasets, evaluation metrics, baselines, and implementation details used in our experiments.

\textbf{Datasets}. Following \citet{ZhengFSS0J024}, we use Table Question Answering (TQA) and Table Fact Verification (TFV) tasks for training and evaluation.
All data statistics are shown in Table~\ref{tab:dataset}.


The TQA task consists of five evaluation benchmarks, including: \texttt{TABMWP}~\cite{lu2023dynamic}, \texttt{WikiTQ}~\cite{pasupat-liang-2015-compositional}, \texttt{HiTab}~\cite{cheng-etal-2022-hitab}, \texttt{TAT-QA}~\cite{zhu-etal-2021-tat} and \texttt{FeTaQA}~\cite{Nan2021FeTaQAFT}. For TFV tasks, \texttt{TabFact}~\cite{2019TabFactA} and \texttt{InfoTabs}~\cite{gupta-etal-2020-infotabs} are used. %These table fact verification tasks need to verify the integrity of statements according to the given table.%\texttt{WikiTQ} is a table based question-answering dataset, which consists of complex questions annotated by crowd workers. \texttt{WikiTQ} asks models to answer the question according to the table provided by Wikipedia.
%\texttt{TABMWP} is a dataset designed to explore the generation and solving of mathematical word problems (MWPs) within the context of tabular data.





\textbf{Evaluation Metrics}. For TQA, we evaluate model performance using Accuracy (Acc.) on \texttt{WTQ}, \texttt{TABMWP}, \texttt{TAT-QA}, and \texttt{HiTab}, while the BLEU score~\cite{papineni2002bleu} is used for \texttt{FeTaQA}. In TFV, we use the binary classification accuracy for \texttt{TabFact} (true/false outputs) and multi-class accuracy for \texttt{InfoTabs} (entail/contradict/neutral outputs). All experiment settings are the same as \citet{ZhengFSS0J024}.

\textbf{Baselines}. In our experiments, we keep the same experimental setting with \citet{ZhengFSS0J024} that compares open-sourced LLMs and MLLMs regarding tables in the experiments. We compare \method{} against three categories of models: (1) LLMs with text-based table representations, (2) MLLMs with image-based table representations, and (3) MLLMs with the combination of text-based and image-based table representations.

\textit{LLMs (Text)}: We represent tables using text-based representations by converting tables into Markdown formats and evaluate three LLMs, Llama2~\cite{touvron2023llama}, TableLlama~\cite{zhang-etal-2024-tablellama}, and Llama3~\cite{dubey2024llama}.

\textit{MLLMs (Image)}: In this category, tables are provided as image-based representations to MLLMs for question answering. We compare \method{} with MiniGPT-4~\cite{zhu2024minigpt}, Qwen-VL~\cite{bai2023qwen}, InternLM-XComposer~\cite{zhang2023internlm}, mPLUG-Owl~\cite{ye2023mplug}, mPLUG-Owl2~\cite{ye2024mplug}, LLaVA v1.5~\cite{liu2024improved}, Vary-toy~\cite{wei2024small}, Monkey~\cite{li2024monkey}, Table-LLaVA~\cite{ZhengFSS0J024}, and MiniCPM-V-2.6~\cite{yao2024minicpm}.

\textit{MLLMs (Image \& Text)}: For MLLMs (Image \& Text), MLLMs are fed with the multi-modal representation which combines both the text-based and image-based representations of the table for answering questions. We compare \method{} Table-LLaVA-13B and MiniCPM-V-2.6.


\input{table/overall}

\textbf{Implementation Details}. 
For DPO training, we use the Swift~\cite{zhao2024swift} framework and set the learning rate to $1e-4$, and batch size to 1. During training, we use the AdamW~\cite{kingma2014adam} optimizer. We apply LoRA~\cite{hu2022lora} to train the model for 1 epoch. During the DPO data generation phase, tables are provided as inputs in three categories: text-based representation, image-based representation, and multi-modal representation. The temperature is set to 1.0, with ten samples taken for each input format. During evaluation, we leverage the VLLM~\cite{kwon2023efficient} framework for efficient inference and configure the MLLMs to employ beam search decoding and set the maximum token limit of 8,192. More experimental details and prompt templates are shown in Appendix~\ref{app:experiments} and Appendix~\ref{app:hippo_prompt}, respectively.



%The construction of the DPO training dataset utilized \texttt{TABMWP}, \texttt{WTQ}, \texttt{TAT-QA}, \texttt{TabFact}, and \texttt{InfoTabs}. We did not include \texttt{FeTaQA} due to its evaluation metric being BLEU, which does not focus on the accuracy of question answering. Additionally, the \texttt{HiTab} dataset is excluded because it involves multi-level tables, which present formatting challenges when converted to Markdown. This conversion can lead to formatting inconsistencies, making it less suitable for training. From each of the chosen datasets, we extracted 2,000 instances, resulting in a total of 10,000 training dataset.