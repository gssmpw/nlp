\section{Evaluation Results}
In this section, we first present the overall performance of \method{}. We then investigate the effectiveness of various training strategies and examine the role of table representations across different modalities in \method{}. Finally, case studies are shown.

\subsection{Overall Performance}
In this experiment, we evaluate the table understanding effectiveness of \method{} and baseline models on both the TQA and TFV tasks. Specifically, we assess LLMs by feeding text representation of tables and evaluate the capabilities of MLLMs by providing either images or a combination of both texts and images.


As shown in Table~\ref{tab:overall}, these LLM based baselines typically exhibit comparable performance to the MLLMs that only use images to represent the table, highlighting the importance of both text and image modalities in table understanding. Among all the unimodal-based table understanding models, MiniCPM-V-2.6 achieves the best performance, demonstrating its strong ability to perform effective reasoning over the images of tables.
In addition to images, we further incorporate a text representation of the table as the input for the MLLMs to assess their effectiveness in table understanding. The experimental results on both Table-LLaVA and MiniCPM-V-2.6 show performance improvements, indicating that the text representation aids MLLMs in conducting necessary reasoning.

Next, we implement multi-modal table understanding models based on the MiniCPM-V-2.6 model and compare both vanilla SFT and \method{} models to evaluate the effectiveness of different training strategies. Overall, \method{} shows its effectiveness by achieving an improvement of more than 4\% over LLM (Text) and MLLM (Image) baselines. The evaluation results show that the vanilla SFT method yields inconsistent performance across different datasets and reduces the performance of the zero-shot model on the TQA task, demonstrating that fine-tuning on ground truth labels leads to overfitting. In contrast, \method{} consistently improves performance on both TQA and TFV tasks, achieving 3.6\% and 2.8\% improvements over the zero-shot model, respectively. These results demonstrate the effectiveness of \method{} in training MLLMs to perform more effective reasoning on tables by leveraging both text and image modalities.



\subsection{Ablation Study}
The ablation studies are conducted to demonstrate the effectiveness of different training strategies used in our \method{} model.

As shown in Table~\ref{tab:ablation}, we compare DPO, \method{} (Random), and \method{} models to evaluate the effectiveness of different training strategies. Specifically, the DPO method directly samples responses based on the multi-modal table representations for optimization. Both \method{} and \method{} (Random) employ a hybrid-modality sampling approach for DPO training, sampling responses from both unimodal and multi-modal table representations. The key distinction between \method{} (Random) and \method{} is that \method{} uses the modality-consistent sampling method, whereas \method{} (Random) employs the random selection strategy.
\input{table/ablation}

By using our hybrid-modality sampling approach to generate preferences for DPO training, the performance of MLLM on the TQA task is improved, highlighting the effectiveness of the hybrid-modality sampling strategy. The primary reason for this improvement lies in the fact that the hybrid-modality sampling method increases the diversity of sampled responses, enabling MLLM to learn more signals from different modalities during DPO training. Furthermore, \method{} introduces a modality-consistent sampling method for selecting negatives to construct preference pairs, which helps prevent the model from learning modality bias. As a result, \method{} achieves an improvement of over 1\%, demonstrating its ability to generate higher-quality negatives for DPO training.



\subsection{Exploring the Role of Multi-Modal Table Representations in \method{}}

In this experiment, we sample 500 examples from TAT-QA and TabFact datasets respectively to investigate the roles of different modalities in table understanding. Specifically, we analyze the output similarity of MLLMs based on unimodal and multi-modal table representations and then evaluate the effectiveness of \method{} based on unimodal table representations.

\textbf{Output Similarity.} As shown in Figure~\ref{fig:similarity}, we assess the similarities of sampled answers and Chain-of-Thought~\cite{wei2022chain} produced by different models using unimodal and multi-modal table representations. The prompt templates are shown in Appendix~\ref{app:cot_prompt}. Three training strategies are compared in this experiment, including Zero-Shot, DPO, and \method{}.

\input{figure/similarity_analyse}
\input{figure/modality_ablation}
\input{figure/case_study}
First, we ask each model to generate several answers based on both unimodal and multi-modal representations, then calculate the Jaccard similarity to estimate the output similarity, as shown in Figure~\ref{fig:sim:jaccard}. \method{} demonstrates higher Jaccard similarity than DPO, highlighting its effectiveness in helping the model generate more consistent responses based on both unimodal and multi-modal table representations. This suggests that \method{} enables MLLMs to better leverage semantic information from different modalities, rather than overfitting to a particular modality.

Then, we prompt models to generate Chain-of-Thought~\cite{wei2022chain} for solving the question and use the BGE model~\cite{bge} to evaluate the reasoning similarity of the models based on unimodal and multi-modal table representations. The evaluation results show that \method{} typically exhibits a lower similarity score compared to Zero-Shot and DPO, indicating that \method{} encourages MLLMs to adopt more distinct reasoning mechanisms across different modalities.





\textbf{Table Understanding with Unimodal Representations.} Next, we further analyze the effectiveness of our modality-consistent sampling method within \method{} by providing MLLMs with either text-based or image-based table representations and then evaluating their table understanding performance on both TQA and TFV tasks. 

In this experiment, we compare \method{} and \method{} (Random). Different from \method{} (Random), \method{} employs a modality-consistent sampling approach during DPO training. As shown in Figure~\ref{fig:modality}, \method{} significantly outperforms \method{} (Random) when provided with either text-based or image-based table representations. This demonstrates that \method{} is more effective in capturing key information from each modality to answer the question, thereby extending its applicability to various scenarios where only text-based or image-based table representations are available. Furthermore, by enhancing accuracy within each modality, \method{} generates more precise and consistent predictions when combining both modalities.



\subsection{Case Studies}
As shown in Figure~\ref{fig:case_study}, we randomly select two cases to analyze the effectiveness of \method{} by prompting MLLMs to generate Chain-of-Thought~\cite{wei2022chain} to answer the question.

In the first case, we compare \method{} with the SFT method. As shown in this case, the SFT method performs an incorrect reasoning process by stating, ``The United States has 1 gold medal'', indicating that the SFT model fails to perform the comparison needed to identify the country with 1 gold medal. In contrast, \method{} correctly identifies that ``Hungary earned 1 gold medal'', demonstrating its effectiveness in training MLLMs to generate more reliable reasoning results. By utilizing preference pairs, \method{} contrastively optimizes the MLLM, guiding it to produce the correct answer.

In the second case, we feed text-based, image-based, and multi-modal table representations to \method{} in order to analyze the role of different modalities. While both the text-based and image-based representations produce the correct intermediate reasoning step—``find the entries labeled 1990 World Cup qualifying games''—they incorrectly identify ``Canada'' and ``Costa Rica'' as the entries. This illustrates that table representations from different modalities may lead MLLMs to generate different incorrect answers. However, when both modalities are combined, the correct answer ``Jamaica'' is produced, demonstrating that both modalities contribute crucial semantic information to support the correct answer. This further underscores the important roles that different modalities play in the reasoning process of table understanding.










