\section{Related Work}
Large Language Models (LLMs), \textit{e.g.} GPT-4~\cite{openai2023gpt} and LLaMA~\cite{touvron2023llama}, have shown strong emergent ability and demonstrate their effectiveness in table understanding through prompts and instructions~\cite{chen2023large,wang2024chainoftable}. Inspired by the Chain-of-Thought (CoT) reasoning~\cite{wei2022chain}, the work decomposes the questions into sub-problems to help LLMs solve complex problems more effectively, thereby benefiting the table understanding and reasoning abilities of LLMs~\cite{zhou2022least,wang2024chainoftable,cheng2023binding}. Furthermore, some models also ask LLMs to generate SQL or Python programs and then leverage the program executors to produce the code execution outcomes, making LLMs produce more accurate answers~\cite{cheng2023binding,ye2023large,gao2023pal,ni2023lever}.

Even though these table reasoning methods exhibit strong capabilities in tabular understanding, they often underestimate the impact of text-based tabular formats on table reasoning and operations~\cite{sui2024table,singha2023tabular}. Specifically, table understanding tasks demonstrate varying performance across different tabular formats~\cite{sui2024table}, and these formats display differing levels of robustness to various noise operations~\cite{singha2023tabular}. Moreover, \citet{zhang2024flextaf} propose a more effective method to choose a tailored text-based table representation to help LLMs answer the question. Specifically, they explore table representations using Markdown, Dict, List, Pandas, and Database formats, designing distinct mechanisms to aggregate responses across these diverse text modalities.
\input{figure/model}


With the rapid advancements in Multi-modal Large Language Models (MLLMs)~\cite{yao2024minicpm,bai2023qwen,liu2024improved}, many studies have focused on image-grounded table question answering tasks~\cite{kim2024tablevqa,ZhengFSS0J024}, enabling table understanding and reasoning over images from practical scenarios, such as scanned documents and web pages. \citet{deng2024tables} individually investigate the effectiveness of text-based and image-based table representations in facilitating the table reasoning capability of LLMs and MLLMs. Their findings reveal that language models exhibit robust performance with image-based table representations, and in some cases, these representations even outperform text-based ones. However, these studies have not yet explored how to effectively combine the strengths of both image and text modalities to further improve the table reasoning capabilities of MLLMs.
