\section{Methodology}
As illustrated in Figure~\ref{fig:model}, this section introduces our \textbf{H}ybr\textbf{I}d-modal \textbf{P}reference o\textbf{P}timizati\textbf{O}n (\method{}) model. We begin with a detailed explanation of the multi-modal table representation method (Sec.~\ref{sec:representation}). Then, \method{} leverages a hybrid-modal preference optimization approach, enabling MLLMs to effectively utilize the semantics derived from different modalities (Sec.~\ref{sec:preference}).

\subsection{Table Understanding Using Image-Based and Text-Based Representations}\label{sec:representation}
Given a table $T$ and a question $Q$, we prompt the MLLM to generate response $y$ to answer the question based on the information provided in the table.

To effectively capture both the textual semantics and the visual structural semantics of the table $T$, we utilize a combination of text-based and image-based table representations as inputs to the MLLM ($\mathcal{M}$), such as MiniCPM-V~\cite{yao2024minicpm}. The response $y$, which answers the question, is then generated as follows:
\begin{equation}
y = \mathcal{M} (\text{Instruct}_\mathcal{Z}, Q, L(T), V(T)), 
\end{equation}
where $\text{Instruct}_\mathcal{Z}$ represents the instruction specifically designed for table understanding tasks $\mathcal{Z}$, including table question answering and table fact verification tasks. Next, we detail the process of constructing both text-based representation $L(T)$ and image-based table representation $V(T)$ .



\textbf{Text-Based Table Representation.} To conduct text-based table representations for MLLMs, existing methods typically verbalize a table into its textual form, denoted as $L(T)$.

These methods employ various data formats to convert tables into text sequences, such as Markdown, Dict, List, Pandas, and Database formats, before feeding the text-based representations into LLMs~\cite{zhang2024flextaf}. Existing works show that different table input formats will lead to different results~\cite{wang2024chainoftable,zhang2024flextaf}. For constructing the text-based representation $L(T)$ of a given table $T$, we adopt Markdown, one of the most widely used data formats for tables.
However, text-based table representations often encounter difficulties in fully capturing the layout semantics of tables. Thus some approaches incorporate additional cell location information, such as the number of rows and columns~\cite{liu2021tapex}.


\textbf{Image-Based Table Representation.} In contrast to the table conversion process required for text-based representations, image-based methods directly represent a table using its screenshot, denoted as $V(T)$~\cite{ZhengFSS0J024}.


A table image inherently preserves its layout, formatting, and stylistic features, providing an alternative to an intermediate textual table~\cite{sui2024table}. By leveraging the multi-modal capabilities of MLLMs, these models can efficiently perform OCR and parse table layouts, thereby enhancing document-level comprehension~\cite{luo2024layoutllm,yu2025visrag}. The image modality captures richer structural semantics, including cell positions, borders, and background colors, which significantly aid in table understanding and reasoning. Nonetheless, image-based approaches face inherent challenges when conducting complex table operations, such as lookup and sum, during reasoning.



\subsection{Optimizing MLLMs via Hybrid-Modal Preference Optimization}\label{sec:preference}
\method{} utilizes both text-based ($L(T)$) and image-based ($V(T)$) representations to enhance the semantics of the table $T$. While each modality has its unique strengths and limitations, it is critical to teach MLLMs to capture more appropriate semantics from different modalities to generate accurate responses. To achieve this, \method{} proposes the Hybrid-Modal Preference Optimization method, which optimizes MLLMs using the hybrid-modal sampling based DPO method.

\textbf{Hybrid-Modal Sampling Based DPO.} The hybrid-modal sampling based DPO method initiates by inputting both unimodal and multi-modal table representations into the MLLM ($\mathcal{M}$). For each kind of table representation, the responses are sampled from the MLLM to construct the preference pairs for DPO training:
\begin{equation}\label{eq:multimodal}
 \begin{aligned}
 \Tilde{y}_l &\sim \mathcal{M} (\text{Instruct}_\mathcal{Z}, Q, L(T)),\\
 \Tilde{y}_v &\sim \mathcal{M} (\text{Instruct}_\mathcal{Z}, Q, V(T)),\\
 \Tilde{y} &\sim \mathcal{M} (\text{Instruct}_\mathcal{Z}, Q, L(T), V(T)).
 \end{aligned}
\end{equation}
After sampling, the generated responses from hybrid-modalities are collected into a set $\Tilde{Y} = \{\Tilde{y}_l^1,\dots,\Tilde{y}_l^K,\Tilde{y}_v^1,\dots,\Tilde{y}_v^K, \Tilde{y}^1,\dots,\Tilde{y}^K\}$, where $K$ is the hyperparameter that denotes the number of responses that sampled from different modalities. 


Then the positive response $\Tilde{y}^+$ and the negative response $\Tilde{y}^-$ are selected from this set of sampled responses $\Tilde{Y}$. The quadruples $(Q, T, \Tilde{y}^+, \Tilde{y}^-)$ are then collected from each table understanding task, thereby constructing the training set $\mathcal{D}$. Finally, the MLLM is optimized on the collected dataset $\mathcal{D}$ using the Direct Preference Optimization (DPO)~\cite{rafailov2023direct} method:
\begin{equation}\label{eq:dpo}
\begin{aligned}
 & \mathcal{L} = -\mathbb{E}_{\mathcal{D}} [\log \sigma (\beta \log \frac{\mathcal{M}(\Tilde{y}^+ \mid Q, T)}{\mathcal{M}^\text{ref}(\Tilde{y}^+ \mid Q, T)}\\
 &- \beta \log \frac{\mathcal{M}(\Tilde{y}^- \mid Q, T)}{\mathcal{M}^\text{ref} (\Tilde{y}^- \mid Q, T)})],
\end{aligned}
\end{equation}
where $\beta$ is a hyperparameter and $\sigma$ denotes the Sigmoid function. $\mathcal{M}^{\text{ref}}$ represents the reference model, which remains fixed throughout the training process. $\mathcal{M}$ is the table understanding model that can be optimized.
Since these responses $\Tilde{Y}$ are sampled from both unimodal and multi-modal representations of tables, we propose the modality-consistency based response sampling method to find more typical negatives for DPO training.




\textbf{DPO Sampling via Modality-Consistency.} To effectively construct the DPO training dataset $\mathcal{D}$, it is crucial to carefully select both the positive response $\Tilde{y}^+$ and the negative response $\Tilde{y}^-$ from the sampled response set $\Tilde{Y}$. Instead of randomly sampling negative examples across varying modalities, \method{} employs a modality-consistency strategy. This approach prioritizes the selection of representative negative responses for training, thereby minimizing the introduction of spurious signals that could inadvertently bias the modality preference during optimization.

More concretely, the DPO loss, as defined in Eq.~\ref{eq:dpo}, is derived from the Bradley-Terry model:
\begin{equation}\label{eq:bt}
 \mathcal{L} = -\mathbb{E}_\mathcal{D} [\log \sigma (r(Q, T, \Tilde{y}^+)-r(Q, T, \Tilde{y}^-))],
\end{equation}
where $r(\cdot)$ calculates the reward for the generated responses. If the generated response $\Tilde{y}$ matches the ground truth answer $y^*$, then $r(\Tilde{y})=1$; otherwise, $r(\Tilde{y})=0$. By minimizing the DPO loss $\mathcal{L}$, the model $\mathcal{M}$ learns to assign higher probabilities to the positive response ($\Tilde{y}^+$) while reducing the probabilities of the negative response ($\Tilde{y}^-$).
To ensure the effectiveness of DPO training, \method{} enhances the diversity of sampled responses by incorporating responses from different modalities (Eq.~\ref{eq:multimodal}). However, suppose the negative response $\Tilde{y}^-$ is consistently sampled from a specific modality. In that case, the model may develop a preference bias for the other modality to reduce the loss (Eq.~\ref{eq:bt}), which has been observed in multi-modal contrastive training scenarios~\cite{liu2023universal}. To address this issue, we sample modality-consistent responses as negatives to better optimize the model.

Specifically, we retain the query $Q$ that contains a ground truth answer within the set of LLM-generated responses and designate the ground truth answer $y^*$ as the positive response $\Tilde{y}^+$. From the hybrid-modality sampled responses $\Tilde{Y}$, we collect all incorrect responses to construct the negative response set $\Tilde{Y}_\text{Neg}$. Among these, we select the most frequent response as the negative response $\Tilde{y}^-$ for DPO training:
\begin{equation}
 \Tilde{y}^- = \arg\max_{\Tilde{y} \in Y_\text{Neg}} \text{Freq} (\Tilde{y}),
\end{equation}
where $\text{Freq} (\Tilde{y})$ calculates the occurrence frequency of the response $\Tilde{y}$ across the whole set $Y_\text{Neg}$.
