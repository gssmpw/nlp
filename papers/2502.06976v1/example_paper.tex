%%%%%%%% ICML 2025 EXAMPLE LATEX SUBMISSION FILE %%%%%%%%%%%%%%%%%

\documentclass[table]{article}

% Recommended, but optional, packages for figures and better typesetting:
\usepackage{microtype}
\usepackage{graphicx}
% \usepackage{subfigure}
\usepackage{booktabs} % for professional tables

% hyperref makes hyperlinks in the resulting PDF.
% If your build breaks (sometimes temporarily if a hyperlink spans a page)
% please comment out the following usepackage line and replace
% \usepackage{icml2025} with \usepackage[nohyperref]{icml2025} above.
% \usepackage{hyperref}
\usepackage{float}
% Attempt to make hyperref and algorithmic work together better:
% \newcommand{\theHalgorithm}{\arabic{algorithm}}

% Use the following line for the initial blind version submitted for review:
% \usepackage{icml2025}

% If accepted, instead use the following line for the camera-ready submission:
\usepackage[accepted]{icml2025}

% For theorems and such
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{amsthm}
\usepackage{enumitem}
% if you use cleveref..
\usepackage[capitalize,noabbrev]{cleveref}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% THEOREMS
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\theoremstyle{plain}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{assumption}[theorem]{Assumption}
\theoremstyle{remark}
\newtheorem{remark}[theorem]{Remark}
\usepackage{balance} % for balancing columns on the final page
% \usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage[detect-all]{siunitx}
\usepackage{mathtools}
\usepackage[british]{babel}
\usepackage{hhline}
\usepackage{multirow}
\usepackage{amsthm}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\newcommand{\sid}[1]{\textcolor{red}{[SID: #1]}}
% Todonotes is useful during development; simply uncomment the next line
%    and comment out the line below the next line to turn off comments
%\usepackage[disable,textsize=tiny]{todonotes}
\usepackage[textsize=tiny]{todonotes}


% The \icmltitle you define below is probably too long as a header.
% Therefore, a short form for the running title is supplied here:
\icmltitlerunning{Who is Helping Whom? Analyzing Inter-dependencies to Evaluate Cooperation in Human-AI Teaming}

\begin{document}
\twocolumn[
\icmltitle{Who is Helping Whom? Analyzing Inter-dependencies to Evaluate Cooperation in Human-AI Teaming}

% It is OKAY to include author information, even for blind
% submissions: the style file will automatically remove it for you
% unless you've provided the [accepted] option to the icml2025
% package.

% List of affiliations: The first argument should be a (short)
% identifier you will use later to specify author affiliations
% Academic affiliations should list Department, University, City, Region, Country
% Industry affiliations should list Company, City, Region, Country

% You can specify symbols, otherwise they are numbered in order.
% Ideally, you should not use this facility. Affiliations will be numbered
% in order of appearance and this is the preferred way.
\icmlsetsymbol{equal}{*}

\begin{icmlauthorlist}
\icmlauthor{Upasana  Biswas}{sch}
\icmlauthor{Siddhant Bhambri}{sch}
\icmlauthor{Subbarao Kambhampati}{sch}
% \icmlauthor{}{sch}
%\icmlauthor{}{sch}
\end{icmlauthorlist}

% \icmlaffiliation{yyy}{Department of XXX, University of YYY, Location, Country}
% \icmlaffiliation{comp}{Company Name, Location, Country}
\icmlaffiliation{sch}{School of Computing and Augmented Intelligence, Arizona State University, Tempe, USA}

\icmlcorrespondingauthor{Upasana Biswas}{ubiswas2@asu.edu}
% \icmlcorrespondingauthor{Firstname2 Lastname2}{first2.last2@www.uk}

% You may provide any keywords that you
% find helpful for describing your paper; these are used to populate
% the "keywords" metadata in the PDF but will not be shown in the document
% \icmlkeywords{Machine Learning, ICML}

\vskip 0.3in
]

% this must go after the closing bracket ] following \twocolumn[ ...

% This command actually creates the footnote in the first column
% listing the affiliations and the copyright notice.
% The command takes one argument, which is text to display at the start of the footnote.
% The \icmlEqualContribution command is standard text for equal contribution.
% Remove it (just {}) if you do not need this facility.

\printAffiliationsAndNotice{}  % leave blank if no need to mention equal contribution
% \printAffiliationsAndNotice{\icmlEqualContribution} % otherwise use the standard text.

\begin{abstract}
The long-standing research challenges of Human-AI Teaming(HAT) and Zero-shot Cooperation(ZSC) have been tackled by applying multi-agent reinforcement learning(MARL) to train an agent by optimizing the environment reward function and evaluating their performance through task performance metrics such as task reward.
However, such evaluation focuses only on task completion, while being agnostic to `how' the two agents work with each other.  Specifically, we are interested in understanding the cooperation arising within the team when trained agents are paired with humans. 
% Specifically, we are interested in understanding if its the AI agent relying on the human's help or vice-versa - a problem that has been overlooked by the existing literature. 
To formally address this problem, we propose the concept of \textit{interdependence} - measuring how much agents rely on each otherâ€™s actions to achieve the shared goal - as a key metric for evaluating cooperation in human-agent teams. Towards this, we ground this concept through a symbolic formalism and define evaluation metrics that allow us to assess the degree of reliance between the agents' actions. 
We pair state-of-the-art agents trained through MARL for HAT, with learned human models for the the popular Overcooked domain, and evaluate the team performance for these human-agent teams. 
Our results demonstrate that trained agents are not able to induce cooperative behavior, reporting very low levels of interdependence across all the teams.
We also report that teaming performance of a team is not necessarily correlated with the task reward.
\end{abstract}
\section{Introduction}
% Advances in robotics in recent years have significantly expanded their scope of applications with multiple agents, to settings where other robots and even humans may be involved~\cite{}. 
Developing agents that can learn to cooperate and interact with unseen partners, especially humans, remains a well-established challenge in the field of multi-agent reinforcement learning (MARL)~\cite{hrc1, zsc1, zsc2, zsc3}.
Several approaches have been proposed for learning agents to team with humans in an ad-hoc setting, such as Fictitious co-play~\cite{fcp}, Maximum Entropy Population-based Training(MEP)~\cite{mep}, Hidden-Utility Self-Play(HSP)~\cite{hsp}, Cooperative Open-ended Learning(COLE)~\cite{cole}. These methods commonly use task performance metrics
for evaluating cooperation such as mean episode rewards over multiple runs~\cite{hsp,fcp,metric1} or the time-steps taken to complete the task in the environment~\cite{metric2, metric3}. 
% Subjective metrics include user studies, where questions are posed to the human teammates to measure the situational awareness, trust, and engagement of the agent. 
Evaluating a team using metrics which objectively measure only the task performance obscure critical details about the performance of the individual teammates and the interactions that arise between them, especially in cases where the they can successfully complete the task without necessarily cooperating with each other. 
Here, we borrow from the distinction introduced in ~\cite{rc}. Required cooperation(RC) needs the participation of all the agents to achieve the goal. Non-required cooperation(Non-RC), on the other hand, can be achieved independently by a single agent, without requiring participation of the other agents.
Therefore, it is unclear whether agents trained through MARL actually learn how to induce cooperation when paired with an unseen teammate for problems which do not satisfy conditions of RC. This becomes a particularly worrisome problem when these agents are paired with human teammates, where the human may pick the slack of the non-performing AI teammate and still be able to complete the task efficiently. \newline
\begin{figure}[]
  \centering
\includegraphics[width=0.75\linewidth]{pictures/annotated_env.pdf}
  \caption{Environment}
  \label{fig:logo}
\end{figure}
For example, consider the environment layout shown in Figure \ref{fig:logo} from the Overcooked Game. The agents act together in the environment to cook and deliver soups by collecting onions, cooking them in a pot, transferring the soup to a dish, and delivering it at the serving station. This setting does not satisfy RC i.e. the participation of both agents is not required to complete the task. 
Consider the case of a human-agent team trying to achieve the goal. The team could successfully complete the task using a strategy with minimal interactions between the team members, such as one where the human is doing the task by themselves and the agent is merely staying out of the human's way.
Here, the task reward provided by the environment will be shared equally by the agent and the human, irrespective of the lack of cooperation in the team, where the human teammate takes on most of the workload to complete the task.
% Coordinating with agents is achieved through a set of complementary and interdependent relationships between agents which are required (with group goals) or opportunistic (in non-group goals) to efficiently achieve task success. 
Furthermore, subjective user studies only offer limited insight into the quality of cooperation existing within the team~\cite{metric3,metric4,metric5}. Therefore, to objectively evaluate the cooperative interactions arising when agents trained through MARL are paired with humans, our work aims to shift the focus from measuring only the task reward to a quantifiable measurement of cooperative behavior for teams. In teaming research, interdependence is a recurring theme to measure how well teammates can complement each other, manage dependencies, and work toward common goals~\cite{int1,int2,int3,int4}. We aim to develop a generalizable framework to symbolically understand the joint policy and evaluate the interdependencies occurring in a team. Therefore, we propose to formally ground the concept of interdependence. \newline
 \begin{figure}[]
  \centering
        \includegraphics[width=0.6\columnwidth]{pictures/independent.pdf}
        
        \includegraphics[width=0.6\columnwidth]{pictures/coordinated.pdf}
         \caption{Depicted are two strategies to fill a pot with onions in a cooking game. The coordinated strategy (down) is more efficient than the individual strategies (up), but runs the risk of failure if cooperation is not achieved. }
         \label{fig:coor-example}
\end{figure}
To illustrate this, we refer to Figure \ref{fig:coor-example}, where there are two different strategies to satisfy this objective.  The first one involves either of the two agents working independently to cook and serve the soup. Suppose the blue-hat agent($A_1$) picks up the onion, drops it at the pot, the dish and serves the soup, without any interaction with the green-hat agent($A_2$). There are no interdependencies in the agents' actions, except for making sure they don't run into each other's paths. In this case, only the blue-hat agent's actions contribute to the overall task reward. In the second strategy, the counter in the middle is used as a passing station. $A_2$ passes onions to $A_1$ on the counter, who puts them in the pot and then serves the soup, leading to a more interactive and collaborative workflow. Compared to the first case, there is increased coupling between the agents' actions as both agents work towards the shared goal through synchronized, complementary actions. In this case, both the agents' actions contribute towards the task reward. Thus, the second strategy introduces \textit{functional interdependence} where $A_1$ and $A_2$ depend on each other while working towards task completion and the successful execution of one agent's actions acts as a precondition for the execution of the other agent's actions. $A_2$ picks the the onions and places them on the counter which serves as a precondition for $A_1$ to pick the onions and place them in the pot to cook soup. At the same time, as $A_1$ picks the onion from the counter to place it in the pot, the counter gets empty, thus emptying the counter space and allowing $A_2$ to place an onion on it. Thus, this mutual reliance facilitates the existence of the cooperative strategy. We propose a novel metric for measuring such interdependencies between the human and the agent working as a team, which can be used as a quantifiable measure of cooperation.
We map a two-player Markov Game to a symbolic STRIPS formalism, introducing symbolic structure to the world states and the actions, allowing tracking of the interdependencies within the agents in a human-agent team.
We pair state-of-the-art(SOTA) methods trained using MARL for the Overcooked domain with a learned human model~\cite{zsceval} and use the proposed metrics to comprehensively evaluate the teaming performance of these human-agent teams. 
Doing so, we are trying to answer the following questions:
\begin{enumerate}[topsep=0pt,noitemsep]
    \item 
    Are agents trained for ZSC and HAT able to exhibit cooperative behavior when partnered with a human teammate in a non-RC setting?
    \item   What is the relationship between task reward and the cooperation measured for the team? How does each team member contribute to the dynamics of this measured cooperative behavior within the team? 
    \item  To what extent do team members initiate opportunistic interdependencies, and how frequently are these interdependencies successfully accepted by their teammate? How do team members perceive and respond to interdependencies initiated by their partner?
\end{enumerate}
We show a lack of cooperation occurring when SOTA methods are paired with the human model to achieve the task of delivering soup in the counter circuit layout. We also show that the task performance of a team is not representative of it's teaming performance, as our results indicate that even if a team achieves a higher task reward than the other, they can have a comparatively lower level of cooperation occurring within the team. Our results also indicate that there is significant misalignment between the agent and the human, as attempts to cooperate are mostly not successful for both teammates. We argue that the formal grounding of interdependencies as a measure of cooperation will contribute heavily to the evaluation of MARL approaches for HAT, as well as to the design of future techniques.
\section{Related Works}
Previous works in human-agent teaming use task performance or episodic reward~\cite{fcp,hsp,mep,cole,zsceval,metric1} to evaluate the team's performance. \cite{metric3,metric6,metric7} emphasize the significance of designing different metrics for evaluation
such as collaborative fluency, robot and human idle time etc.~\cite{metric3} and subjective user studies to measure trust, engagement and fluency of the agents when paired with a human~\cite{metric3,metric4,metric5}.~\cite{int5} places interdependence at the center of their model for designing human-machine systems, making it the organizing principle around which the rest of the team's structure and behavior revolves.
~\cite{int1} emphasizes that an effective integration of AI into human teams depends the ability of AI agents to collaborate with humans by managing interdependencies.
\section{Preliminaries}
\subsection{Two-Player Markov Game}
A two-player Markov game for a human-AI cooperation scenario can be defined as $\langle S, A, T, R \rangle $ where S is the set of world states, $A : A_{1} \times A_{2}$ where $A_{i}$ is set of possible actions for agent i, $T : S \times A_{1} \times A_{2} \rightarrow S $ is the transition function mapping the present state and the joint action of the agents to the next state of the world,  $R_{i} : S \times A_{1} \times A_{2} \rightarrow R_{i}$ is the reward function mapping the state of the world and the joint action to the global reward. For a 2-player cooperative markov game, $R = R_{1} = R_{2} $ where R is the global environment reward function. The joint policy is defined as $\pi = \left( \pi_{1}, \pi_{2} \right)$ where the policy $\pi_{i} : S \rightarrow A_{i} $ is defined for an agent $i$ over set of possible actions $A_{i}$. The objective of each agent i is to maximize the expected discounted return $\mathbb{E}_{\pi} \left[ \sum_{t=0}^{\infty} \gamma^{t}R(s^{t}, a^{t}_{1}, a^{t}_{2}) \right]$ by following the policy $\pi$ from a given state. Therefore, the policy $\pi$ is learned by optimizing the task reward received by the agents from the environment. 
\subsection{Multi-Agent Planning Problem}

A STRIPS problem is represented as $\langle P, A, I, G \rangle$ where P is the set of propositions which can be used to denote facts about the world, A is the set of planning actions, I is the initial state and G is the goal state. Each fluent $p \in P$ is a symbolic, binary variable that describes the current state of the environment, with each proposition representing a specific property. The possible fluents for the Overcooked environment can be \textit{counter-empty} - describes whether the counter is empty or not, \textit{pot-ready} - indicates whether the soup is ready in the pot, \textit{soup-served} - indicates whether the soup has been served at the serving station etc. I denotes the propositions representing the initial state of the world and G denotes the propositions corresponding to the goal state of the world. A planning action can be defined as $a= <\text{pre(a)}, \text{add(a)}, \text{del(a)} >$ where pre(a) is the set of propositions that must be true before the action can be executed, add(a) are the propositions that become true after the action is performed and del(a) are the propositions that become false after the action is performed. Extending this to multiple agents, a Multi-agent Planning task can be denoted as $\langle P, N, \{A_i\}_{i=1}^N, I, G \rangle$ where N is the number of agents and $A_i$ is the set of actions for the agent i. We assume that the agents take turns to act and not in parallel. A plan is defined as a sequence of actions $\left( \{a_{i}^{1}\}_{i=1}^N , \{a_{i}^{2}\}_{i=1}^N, \dots , \{a_{i}^{n}\}_{i=1}^N \right)$ where n is the number of steps in the plan. 
A plan is a solution $\Pi$ if it is a sequence of actions that can be applied to the initial state I and results in a world state which satisfies G i.e. $\Pi = \left( \{a_{i}^{1}\}_{i=1}^N , \{a_{i}^{2}\}_{i=1}^N, \dots , \{a_{i}^{n}\}_{i=1}^N \right)$ is a valid solution plan if \begin{math}
   \{ a_{i}^{n}\}_{i=1}^N \left( \dots  \dots \left( \{ a_{i}^{2}\}_{i=1}^N \left( \{ a_{i}^{1}\}_{i=1}^N \left(I\right) \right) \right)  \right) \subseteq G 
\end{math}
\section{Interdependencies}
\subsection{Problem Statement}
We pose the human-agent teaming problem as a two-player Markov game, where the actions of the teammates take place sequentially. We focus on the case where the team is trying to reach a set of goal states $S_{G}$ such that $S_{G} \subseteq S$. The states in $S_{G}$ are absorbing i.e. $\forall s \in S_{G}$ and $a^{G}_{i} \in A_{i}$, we have $T(s, \{a^{G}_{i}\}_{i=1}^{2}) = 0$. 
We represent the solution trajectory for a single agent $\tau_i$ as $\tau_i = \left( a^t_i, a^{t+1}_i,\dots a^k_i \dots a^n_i\right)$ and the joint-action solution trajectory $\tau$ of two agents starting from timestep \textit{t} and reaching a goal state at timestep \textit{n} as $\tau = \left(\left(a^{t}_{1},a^{t}_{2}\right), \left(a^{t+1}_{1}, a^{t+1}_{2}\right) \dots \left(a^{n}_{1},a^{n}_{2}\right)  \right)$. 
An execution trace \textit{Tr} of a policy $\pi$ from an initial state $s^{t}$ 
as is denoted as $ \left( s^{t}, a^{t}, s^{t+1}, a^{t+1}, \dots s^{n}  \right) $, where \textit{Tr} corresponds to the state-action sequence that starts at timestep t and terminates at a goal state $s^{n} \in S_{G}$ at a timestep \textit{n}, with $a^{k}= \left(a^{k}_{1},a^{k}_{2}\right)$ and $a^{k}_{i} = \pi_{i}\left( s^{k}\right)$. The agents receive a task reward $R_{\text{task}}$ at the end of \textit{Tr} and $\tau$ on reaching the goal state. We define our problem
 Given the execution trace \textit{Tr} and the joint solution trajectory $\tau$ of a team, we only receive $R_{\text{task}}$ which does not represent how good or "cooperative" the solution trajectory $\tau$ is. To capture the cooperative interactions arising between the teammates in $\tau$, we define the concept of interdependence in the next section.
\subsection{Mapping the Markov Game to STRIPS}
In a Markov Game, the state at a current timestep $s_{t} \in S$ is typically a high-dimensional vector. $s_{t}$ can be denoted as a symbolic state with a set of true propositions $p_{t}$ which denotes the current state of the world. Doing this, we effectively describe each state as a finite set of relevant symbolic facts. 
Therefore, there exists a function $\mathcal{F} : S \rightarrow P $ mapping the states to symbolic propositions.
Here, we refer to Fig. \ref{fig:coor-example}. We consider the predicate \textit{counter-empty} to denote if the middle counter is empty. We consider the transition when the green-hat agent ($A_2$) takes an action to place the onion on the counter. The state at which the agent performs this action has the proposition \textit{counter-empty} set as True, while the action sets \textit{counter-empty} as False in the next state. Therefore, mapping the state to a symbolic state helps us capture the effect of the agents' actions in terms of relevant symbols. We can recall from the execution trace \textit{Tr} of a Markov Game that the state of the world at time \textit{t} is $s^{t}$. From $s^{t}$, taking action $a^{t}$ causes the state of the world to change to $s^{t+1}$. We can map each transition $\left(s^{t}, s^{t+1}, a^{t} \right)$ to the symbolic formulation with the help of $\mathcal{F}$. $s^{t+1}$ can be represented as a set of true propositions $p_{t+1}$ and $s^{t}$ can be represented as $p_{t}$. Similarly, we now map the action $a^{t} = \left( a_{1}^{t}, a_{2}^{t} \right)$ to a symbolic representation. Recall that since the teammates take turns to play, $a^{t} = \left( a_{1}^{t}, \text{no-op} \right)$ or $a^{t} = \left( \text{no-op}, a_{2}^{t} \right)$. For action $a^{t}_{i}$, there exists a mapping from $\left(s^{t}, a^{t}_{i}, s_{t+1} \right)$ to a STRIPS style planning action such that $\text{pre} \left(  a^{t}_{i} \right) \subseteq p_{t}, \text{add} \left( a^{t}_{i} \right) \subseteq p_{t+1}$ and $\text{del} \left( a^{t}_{i} \right) \subseteq P \setminus p_{t+1}$. 
% Extending it to two player scenario, we can map the action $a^t$ to a multi-agent planning action such that $\text{pre} \left(a^{t} \right) = \text{pre} \left( a^{t}_{1} \right) \cup \text{pre} \left( a^{t}_{2} \right) \subseteq p_{t}, \text{add} \left( a^{t} \right) = \text{add} \left( a^{t}_{1} \right) \cup \text{add} \left( a^{t}_{2} \right) \subseteq p_{t+1}$ and $\text{del}\left( a^{t} \right) =  \text{del} \left( a^{t}_{1} \right) \cup \text{del} \left( a^{t}_{2} \right) \subseteq P \setminus p_{t+1}$. 
Therefore, the solution trajectory $\tau$ can be represented as a joint solution plan $\Pi$, where each single-agent action $a_{i}^{t}$ in the trajectory can be represented as $a_{i}^{t}= \left< \text{pre($a_{i}^{t}$)}, \text{add($a_{i}^{t}$)}, \text{del($a_{i}^{t}$)} \right>$. This way we can track the preconditions and effects of the actions of individual agents in the trajectory as symbolic propositions and track the interdependencies between them. 

\subsection{Agent Interdependencies}
Given a joint-action solution trajectory $\tau$ and the solution trajectory $ \tau_{i}$ for an agent i, we define the following properties about $\tau$ and $\tau_{i}$ to formalize the concept of interdependence for the solution trajectory:
% Referring to Fig. \ref{fig:coor-example}, we focus on the cooperative strategy(bottom). When $A_{2}$ takes an action to put an onion on the counter at timestep \textit{k - t}, the proposition \textit{onion-on-counter} becomes True as the effect of the action.
\begin{definition}
    For $\tau_i$, \textit{Independent} actions $\text{I}_i$ are the set of actions which have no possible interactions with the actions of the other agent. This is defined as: $ \text{Ind}_{i}=\left( a_{i}\mid 
        \forall a_{j} \in A_{j} \cap j \not = i, 
        \text{eff}\left( a_{i} \right)\not \subseteq \text{pre} \left( a_{j} \right) \cup \text{pre}\left( a_{i}  \right) 
        \not \subseteq \text{eff} \left( a_{j} \right) \right)$ where $a_{i} \in \tau_{i}$. 
            The set of \textit{Coordination} actions for all agents in a team as: $C_{i} = \tau_{i} - I_{i}$. 
            \label{def:ind}
\end{definition}
In Figure \ref{fig:logo}, an example of an \textit{Independent} action is $A_{1}$ picking an onion from the onion dispenser. The effect of this action is the proposition \textit{holding-onion} getting set to True for $A_{1}$. This action cannot interact with any actions of $A_{2}$, since there is no direct passing of onions between the agents. An example of a \textit{Coordination} action is $A_{1}$ putting the third onion into the pot, leading to the soup being ready to be picked up by $A_{2}$. This action leads to the proposition \textit{soup-ready} getting set to True, which interacts with the action of $A_{2}$ if they perform the action of collecting soup from the pot. Referring to Figure \ref{fig:ac_distribution}, the outermost ring shows the distribution of all the actions into \textit{Independent} and \textit{Coordination} actions.   
\begin{definition}
    For $\tau_i$, the set of \textit{Trigger} actions is $\texttt{Tr}_{i} = \left( a_{i}\mid 
        \forall a_{j} \in A_{j} \cap j \not = i,  \text{eff} \left( a_{i} \right) \subseteq \text{pre} \right) $ where $a_{i} \in C_{i}$. 
        The set of \textit{Accept} actions is $ \texttt{Ac}_{i} = \left( a_{i} \mid 
        \forall a_{j} \in A_{j} \cap j \not = i, \text{pre} \left( a_{i} \right) \subseteq \text{eff} \left( a_{j} \right) \right)$ where $a_{i} \in C_{i}$.
        \label{def:tr}
\end{definition}
A \textit{Trigger} action for $A_{1}$ is placing the onion on the counter, since it could potentially be the precondition for $A_{2}$ picking that onion from the counter. Similarly, a \textit{Accept} action for $A_{2}$ is picking an onion from the counter, since it's precondition \textit{onion-on-counter} could potentially be satisfied by the $A_{1}$'s action of placing the onion on the counter.
Note that the $C_{i} = \text{Tr}_{i} + \text{Ac}_{i}$ as shown in Figure \ref{fig:ac_distribution}. Therefore, $C_{i}$ consists of all the actions of agent \textit{i} which have the possibility of interacting with the other agent's actions, either as \textit{Trigger} actions or \textit{Accept} actions.  
\begin{definition}
    For $\tau$, \textit{Interdependent} actions are pair of actions $(a_i^k, a_j^{k-t})_{i \neq j}$ such that $add(a_j^{k-t}) \subseteq pre(a_i^k)$. 
\end{definition}
\begin{definition}
An \textit{Interdependent} pair of actions $(a_i^k, a_j^{k-t})_{i \neq j}$ has two agents, a \textit{Giver} agent performing the action $a_j^{k-t}$ and a \textit{Receiver} agent performing the action $a_i^k$. 
\label{def:int}
\end{definition}
\begin{definition}
    For $\tau$, an agent $i$ has a set of \textit{Giver} actions which is the set of actions where agent $i$ acts as the giver in an interdependent pair and \textit{Receiver} actions which is the set of actions where agent $i$ acts as the receiver in an interdependent pair. This can be defined as : \\
     $G_{i} = (a_{i}^{k} \in \tau_{i} \| \exists a_{j}^{k+t} \wedge add(a_i^{k}) \subseteq pre(a_j^{k+t}))$ \newline
     $R_{i} = (a_{i}^{k} \in \tau_{i} \| \exists a_{j}^{k-t} \wedge add(a_{j}^{k-t}) \subseteq pre(a_{i}^{k}))$.
     \label{def:gi}
\end{definition}
The set of \textit{Interdependent} actions $\text{Int}_i = G_i + R_i$ as we can see in Figure \ref{fig:ac_distribution}. We define \textit{Interdependence} as the property of a solution trajectory $\tau$ if there exists an \textit{interdependent} pair of actions in $\tau$. This is intended to capture the opportunistic interdependencies occurring in the solution trajectory of the team, representing the interactions between the agents actions.
Referring to Fig. \ref{fig:coor-example}, we focus on the cooperative strategy(bottom). When $A_{2}$ takes an action to put an onion on the counter at timestep \textit{k - t}, the proposition \textit{onion-on-counter} becomes True as the effect of the action. If successful passing happens, $A_{1}$ takes an action to pick that onion from the counter at a timestep \textit{k} $>$ \textit{ k - t }. The precondition of this action is that the proposition \textit{onion-on-counter} should be True. Since \textit{onion-on-counter} was set True by $A_{2}$, it is an interdependent pair of actions. $A_{2}$ is the giver agent who sets \textit{onion-on-counter} as True as the effect of their action and $A_{1}$ is the receiver agent who needs \textit{onion-on-counter} to be True as the precondition of their action. 
For a 2-player Markov game, the size of total set of interdependent actions $|\text{Int}^\tau| = |\text{Int}_1| + |\text{Int}_2|$ represents the total interdependencies arising within the team's joint-action solution trajectory $\tau$.
While $|G_{i}|$ and $|R_{i}|$ represent how many interdependencies in the solution trajectory $\tau$ were  provided and receieved by agent \textit{i} respectively, denoting the individual contributions of the teammates to the cooperation arising in the team. Also, $|\texttt{Tr}_{i} - G_i|$ denotes the number of interdependencies triggered by the agent which were not accepted by it's teammate, representing instances of miscordination in the team where interdependencies triggered were not accepted by the teammate.
\begin{figure}[htp]
    \centering
    \includegraphics[width=0.65\linewidth]{pictures/cole-1.png}
    \caption{Action Distribution for COLE paired with $H_{\text{proxy}}$} where the outermost ring divides the total actions of the agent into independent and coordination actions, middle ring divides them into trigger and accept actions, innermost ring divides these into unsuccessful and successful interdependent actions.
    \label{fig:ac_distribution}
\end{figure}
\subsection{Human-AI Cooperation Problem}
Now, we extend the above definitions to the case where one of the agents is a human user i.e. the human and the agent are working together to finish a task. In such human-agent teams, we propose a quantitative measure of teamwork to judge the team through interdependencies,
Rewriting the definitions above for this case, we define \textit{Interdependent} pair of actions $(a_i^k, a_j^{k-t})_{i \neq j}$ for cases where :
\begin{itemize}[topsep=0pt,nosep]
    \item Human is the \textit{giver} agent i.e. $(a_h^k, a_r^{k-t})_{i \neq j}$ such that $add(a_h^{k-t}) \subseteq pre(a_r^k)$
    \item Human is the \textit{receiver} agent i.e. $(a_r^k, a_h^{k-t})_{i \neq j}$ such that $add(a_r^{k-t}) \subseteq pre(a_h^k)$
\end{itemize}
where $a_h$ refers to the human user and $a_r$ refers to the trained agent. We can also define the \textit{Giver} actions for the human as $G_{h} = (a_{h}^{k} \in \tau_{h} \| \exists a_{r}^{k+t} \wedge add(a_h^{k}) \subseteq pre(a_i^{k+t}))$ and the \textit{receiver} list $R_{H} = (a_{H}^{k} \in A_{i} \| \exists a_{i}^{k-t} \wedge add(a_{i}^{k-t}) \subseteq pre(a_{H}^{k})) $, and \textit{giver} list for the agent as $G_{i} = (a_{i}^{k} \in A_{i} \| \exists a_{H}^{k+t} \wedge add(a_i^{k}) \subseteq pre(a_H^{k+t}))$ and the \textit{receiver} list as $ R_{i} = (a_{i}^{k} \in A_{i} \| \exists a_{H}^{k-t} \wedge add(a_{H}^{k-t}) \subseteq pre(a_{i}^{k}))$.
The contribution of each agent is assessed by examining the size and distribution of the human \textit{giver} list $G_H$ and \textit{receiver} list $R_H$, along with the agent counterparts $G_i$ and $R_i$. A balanced team performance is indicated when both the human and the agents exhibit a similar number of giver and receiver actions. If $G_H$ is significantly larger than $R_H$, it suggests that the human is contributing disproportionately to the task. Conversely, if the agent giver list $G_i$ exceeds the agent receiver list $R_i$, the agent is actively supporting human task execution, signifying effective human-AI collaboration. 
\begin{algorithm}[htp]
  \caption{Analyzing interdependencies in multi-agent solution trajectory}\label{euclid}
  \begin{algorithmic}[1]
    \Procedure{Evaluate}{$\textit{T}, N$}
      \For{i in 1 to N}\Comment{Initialize all giver and receiver lists\\ to be empty for all agents}
        \State \texttt{$\text{add-list}_{i} = \emptyset$} 
        \State \texttt{$G_{i}, R_{i} = \emptyset$} 
      \EndFor
      \While{\texttt{t $<$ n}} \Comment{While game is not over}
        \State $\textbf{a}^t = (\textbf{a}^1, \dots \textbf{a}^{i}, \dots \textbf{a}^{N}) $
        \For{\texttt{i in 1 to N}}
            \State $\textbf{a}^{t}_{i}$ $\xrightarrow[\text{$\textbf{a}^{t}_{i}$}]{\text{$s_{t}, s_{t+1}$}}$ pre($\textbf{a}^{t}_{i}$), add($\textbf{a}^{t}_{i}$), del($\textbf{a}^{t}_{i}$)
            \State $ \text{add-list}_{i} = \text{add-list}_{i} \cup \text{pre}(\textbf{a}^{t}_{i}) $ 
            \For{\texttt{j in 1 to N}}
                \If{j $\neq$ i and pre($\textbf{a}^{t}_{i}$) $\in \text{add-list}_{j}$}
                        \State $R_{i} = R_{i} \cup \text{pre}(\textbf{a}^{t}_{i})$
                        \State $G_{j} = G_{j} \cup \text{pre}(\textbf{a}^{t}_{i})$
                        \State $\text{add-list}_{i} = \text{add-list}_{i} \setminus \text{pre}(\textbf{a}^{t}_{i})$
                    
                \EndIf
            \EndFor
        \EndFor 
      \EndWhile\label{while game is going on} 
    \EndProcedure
  \end{algorithmic}
\end{algorithm}

\section{Experiments}
In this section, we evaluate the performance of state-of-the-art methods in the counter circuit layout (Fig.\ref{fig:logo} from the Overcooked domain when teamed with a learned human model as presented in~\cite{zsceval}. The performance of these teams is assessed using the concept of interdependence, which captures the cooperative interactions between agents.
We utilize metrics derived from the trigger list, the giver list, the receiver list of the teammates. We also conduct a comprehensive analysis of the  coordination dynamics within the team reflected by these metrics.
\subsection{Environment}
The team of 2 players is in a gridworld environment with onion dispensers, dish dispensers, pots, serving stations, and empty counters. The players can either move in the environment or interact with these objects. The objective of the game is to cook and deliver three soups as quickly as possible. To do this, the team must do the following tasks: pick and drop three onions from the onion dispenser, place them in the cooking pot, and wait for the soup to be done. The next steps are to pick a dish from the dish dispenser, transfer the cooked soup to the empty dish, and deliver the soup to the serving station. Each player and each counter can hold only one object at a time. On successful delivery of a soup, both the players receive the task reward. Therefore, both players are incentivised to collaborate to prepare the soup and deliver it as many times as possible. The environment is fully observable and communication is not allowed between agents in the environment.
\textbf{Subtasks} : At each step, players can perform either of these eight actions: stay in the same cell, move one cell up, move one cell down, move one cell to the right, move one cell to the left and interact with the object in front. The result of this action depends on the item the player is holding (empty, onion, empty dish, filled dish) and the type of object they are facing(dispenser, pot, empty counter, serving station). 
Since the environment we are working with has a distinct \textsc{\textit{interact}} action, we can enumerate all possible outcomes of the the interact action, and use these as our sub-tasks - Pick up onion from onion dispenser,  Pick up onion from counter, Pick up dish from dish dispenser, Pick up dish from counter, Place onion in pot, Place onion on counter, Get soup from pot, Place dish on counter, Get soup from pot, Place soup on counter, Serve soup in serving station.
\subsection{SOTA Methods}
FCP~\cite{fcp}, MEP~\cite{mep},HSP~\cite{hsp} and COLE~\cite{cole} are trained using a two-stage training framework, where a diverse partner population is created through self-play in the first stage, followed by the second stage where the ego agent is iteratively trained by having it play against sampled partners from the population and optimizing mainly the task reward using reinforcement learning.
All these methods focus on improving the diversity of the partner population in the first stage.
While MEP adds maximum entropy to the reward for increasing the diversity of the population, 
HSP tries to model the human teammate's reward as event-based rewards to construct a  set of behavior-preferring agents. COLE presents cooperative games as graphic-form games and calculates the reward from the cooperative incompability distribution.
The ego agent in all these approaches are trained to optimize the episodic task reward, which is also the objective metric being used to measure cooperation when these agents are paired with an unseen teammate (also human). We use the evaluation partners generated in ~\cite{zsceval} as learned models of human behavior ($H_{\text{proxy}}$). 
\section{Results}
\subsection{Task vs Teaming Score}
We compare the task reward for a team, as measured by the time it takes for the team to cook and deliver 3 soups, while the teaming performance is measured by the interdependencies occurring within the team. Specifically, we monitor the ratio of \textit{Interdependent Actions} (Def.\ref{def:int}) compared to the total number of actions performed by the team,  to monitor the proportion of actions in the solution trajectory $\tau$ which involves interactions between the teammates.  
% Specifically, we monitor the number of non-interdependent/independent actions \textcolor{blue}{give definition in previous section} performed by the human and the agent across five runs and compute average performance metrics. 
Additionally, we compare the length of the \textit{Giver List} and \textit{Receiver List} (Def.\ref{def:gi}) of the team members to understand the individual contribution of each team member towards task success achieved through successful cooperation. also write about symmetry of the roles.
We evaluate the SOTA methods in the domain by pairing it with a itself as well as H-proxy with task and teaming performance averaged over 10 runs, and present it in Table.\ref{table:tvt}. We observe that for all the teams evaluated, the $\%\textit{Interdependent Actions}$ are much lower than expected for a layout where the optimal joint policy involves cooperation and multiple interdependencies arising between the teammates. Also, the contribution of the human teammate is higher as against when the agents are paired with themselves.
Our results in  also indicate that a quicker soup delivery aka higher task reward does not necessarily correlate with better teamwork.
HSP-$H_{\text{proxy}}$ takes a shorter time to deliver soup as compared to COLE-$H_{\text{proxy}}$. In contrast, COLE-$H_{\text{proxy}}$ outperforms HSP-$H_{\text{proxy}}$ when it comes to teaming performance suggesting strong cooperation occuring in the team. We observe a similar trend where MEP-$H_{\text{proxy}}$ outperforms FCP-$H_{\text{proxy}}$ in terms of efficiency of task completion, but performs similarly when it comes to teaming performance. Comparing the contribution of the SOTA agents and $H_{\text{proxy}}$ to cooperation occurring in the team, we can observe that $H_{\text{proxy}}$ gives more interdependencies than it receives as compared to the agents.
Also, while HSP-$H_{\text{proxy}}$ has the best task performance among all the teams, the contribution of HSP to the interdependencies successfully occurring in the team(0.16) is the least among all the other agents, while the contribution of $H_{\text{proxy}}$ is much higher (2.58). 
We observe a similar trend in MEP-$H_{\text{proxy}}$ as well. We also note that the contribution ratio for the COLE agent and $H_\text{proxy}$ is the most similar, suggesting that both team members contribute proportionately to the teamwork occurring in the team.
% \begin{table}[]
% \tiny
% \begin{tabular}{|l|l|l|l|}
% \hline
% Agent & Time & $\%Ag_1$  & $\%Ag_2$  \\ \hline
% \begin{tabular}{l} $Ag_1:$ COLE, $Ag_2:$ COLE \\ $Ag_1:$ COLE, $Ag_2:$ H-proxy \end{tabular} 
% & \begin{tabular}{l} 
%  798.8     \\ 
%  813    \\
% \end{tabular}
% & \begin{tabular}{l} 
%  25.99(15.49+11.34)    \\ 
%  24.84(13.5+11.34))    \\
% \end{tabular}
% & \begin{tabular}{l} 
%  20.84(10.5+11.34)     \\ 
%  20.84(10.5+10.34)
% \end{tabular}
% \\ \hline
% \begin{tabular}{l} 
% $Ag_1:$ HSP, $Ag_2:$ HSP \\ 
% $Ag_1:$ HSP, $Ag_2:$ H-proxy \end{tabular} 
% & \begin{tabular}{l} 
%  866.02 \\ 
%  813.8   \\
% \end{tabular}
% & \begin{tabular}{l} 
%  30.667(19+11.67) \\ 
%  25(3.6+21.4) \\
% \end{tabular}
% & \begin{tabular}{l} 
%  35.51(13.17+22.34)     \\ 
%  18.6(11.4+7.2)
% \end{tabular}
% \\ \hline
% \begin{tabular}{l} 
% $Ag_1:$ MEP, $Ag_2:$ MEP \\ 
% $Ag_1:$ MEP, $Ag_2:$ H-proxy \end{tabular} 
% & \begin{tabular}{l} 
%  829.12     \\ 
%  847   \\
% \end{tabular}
% & \begin{tabular}{l} 
%  18.334(8.167+10.167)     \\ 
%  13(6.4+6.6) \\
% \end{tabular}
% & \begin{tabular}{l} 
%  20.167(11.5+8.67)     \\ 
%  11.6(10.4+1.2)
% \end{tabular}
% \\ \hline
% \begin{tabular}{l} 
% $Ag_1:$ FCP, $Ag_2:$ FCP \\ 
% $Ag_1:$ FCP, $Ag_2:$ H-proxy \end{tabular} 
% & \begin{tabular}{l} 
%  808.67     \\ 
%  850    \\
% \end{tabular}
% & \begin{tabular}{l} 
%  13.17(4.17+9.0) \\ 
%  17.5 (10.0+7.5)    \\
% \end{tabular}
% & \begin{tabular}{l} 
%  12.34(8.0+4.34)      \\ 
%  6.5(4.75+1.75)
% \end{tabular}
% \\ \hline
% \end{tabular}
% \caption{Taskwork vs Teamwork for Counter Circuit. $Ind_H$ refers to the proportion of independent actions performed by human out of the total actions done by the human, $Ind_R$ refers to the proportion of independent actions performed by agent out of the total actions done by the agent, Giver list is the average length of the giver list of agent, Receiver List is the average length of the receiver list of agent}
% \end{table}
\begin{table}[htp]
\tiny
\begin{tabular}{|l|l|l|l|l|}
\hline
$\text{Ag}_1, \text{Ag}_2$ & Time & $\%\textit{Interdependent}$  & $\frac{G_{\text{Ag}_1}}{R_{\text{Ag}_1}}$ & $\frac{G_{\text{Ag}_2}}{R_{\text{Ag}_2}}$  \\ \hline
% \rowcolor{red!80}
\begin{tabular}{l} COLE, COLE \\   COLE, H-proxy \end{tabular} 
& \begin{tabular}{l} 
 798.8     \\ 
 \rowcolor{lightgray} 850
 \\
\end{tabular}
& \begin{tabular}{l} 
 23.42    \\ 
 \rowcolor{lightgray} 27.85    \\
\end{tabular}
& \begin{tabular}{l} 
   1.36      \\ 
    1.19
\end{tabular}
& \begin{tabular}{l} 
   0.92     \\ 
    1.01
\end{tabular}
\\ \hline
\begin{tabular}{l} 
HSP, HSP \\ 
HSP, H-proxy \end{tabular} 
& \begin{tabular}{l} 
 866.02 \\ 
 \cellcolor{red!55} 813.8   \\
\end{tabular}
& \begin{tabular}{l} 
 33.06 \\ 
 \cellcolor{red!55} 17.80 \\
\end{tabular}
& \begin{tabular}{l} 
 1.63     \\ 
 \cellcolor{red!55} 0.16
\end{tabular}
& \begin{tabular}{l} 
 0.59     \\ 
 2.58
\end{tabular}
\\ \hline
\begin{tabular}{l} 
MEP, MEP \\ 
MEP, H-proxy \end{tabular} 
& \begin{tabular}{l} 
 829.12     \\ 
 897   \\
\end{tabular}
& \begin{tabular}{l} 
 19.26     \\ 
 12.32 \\
\end{tabular}
& \begin{tabular}{l} 
 0.8    \\ 
 0.96
\end{tabular}
& \begin{tabular}{l} 
 1.32    \\ 
 8.67
\end{tabular}
\\ \hline
\begin{tabular}{l} 
FCP, FCP \\ 
 FCP, H-proxy \end{tabular} 
& \begin{tabular}{l} 
 808.67     \\ 
 910    \\
\end{tabular}
& \begin{tabular}{l} 
 12.75 \\ 
 12.01    \\
\end{tabular}
& \begin{tabular}{l} 
 0.46      \\ 
 1.33
\end{tabular}
& \begin{tabular}{l} 
 1.84      \\ 
 2.71
\end{tabular}
\\ \hline
\end{tabular}
\caption{Taskwork vs Teamwork. $\%\textit{Interdependent}$ refers to the proportion of interdependent actions performed out of the total actions done for both agents in the run, contribution of $\text{Ag}_i = \frac{\textit{Giver list}}{\textit{Receiver list}}$ represents how many interdependencies is $\text{Ag}_i$ giving versus how many it is receiving to give an idea of how much the agent is contributing to the cooperation achieved in the team.}
\label{table:tvt}
\end{table}
\subsection{Trigger vs Giver List for Agent}
% \begin{table}[]
% \tiny
% \begin{tabular}{|l|l|l|l|l|l|l|}
% \hline
% Agent & $\%  \text{Ag-Sub}_{\text{coor}}^{\text{Receiver}} $ & $\%  \text{Ag-Sub}_{\text{coor}}^{\text{Giver}} $ & $\%  \text{H-Sub}_{\text{coor}}^{\text{Receiver}} $ & $\%  \text{H-Sub}_{\text{coor}}^{\text{Giver}} $\\ \hline
% COLE                 &11.34  &13.5    &10.34 &10.5                  \\ \hline
% HSP               & 21.4   &3.6    &7.2 &11.4      
% \\ \hline
% MEP            & 6.6  & 6.4   &1.2 &10.4                 \\ \hline
% FCP                & 7.5   &10.0    &1.75 &4.75             \\ \hline
% \end{tabular}
% \caption{ For Counter Circuit layout, $\text{Sub}_\text{Tot}$ refers to total number of subtasks performed by the agent, $\%  \text{Sub}_\text{coor} $ indicates the percentage of coordination subtasks, $\%  \text{Sub}_{\text{coor}}^{\text{Trig}} $ denotes the percentage of coordination subtasks that were executed by the agent to trigger interdependencies, $\%  \text{Sub}_{\text{coor}}^{\text{Trig-acc}} $ represents the percentage of trigger coordination subtasks that were accepted by the human.
% }
% \label{table:abs}
% \end{table}
We analyze the proportion of the triggered interdependencies (Def.\ref{def:tr}) that are subsequently accepted by the human (Def.\ref{def:gi}. The acceptance of an event by H-proxy indicates the human's willingness to engage with the agent. However, the agent's trigger list may include multiple attempts to initiate interactions or request help which, if perceived as unnecessary or irrelevant, could lead to negative consequences, such as frustration or disengagement on the part of the human~\cite{frust1,frust2}. In such scenarios, the human may perceive the agent's actions as interruptive~\cite{interrupt1,interrupt2}. From Table.\ref{table:coor}, we observe that the interdependencies triggered by the agent are not accepted by the $H_{\text{proxy}}$ less than 50$\%$ of the times. Most of the interdependencies triggered by the agent fail. 
% All the SOTA agents paired with $H_{\text{proxy}}$ receive more interdependencies that it gives, as seen in. 
FCP paired with $H_{\text{proxy}}$ tries to initiate an interdependence most frequently, most of which the $H_{\text{proxy}}$ rejects, lowering the efficiency of task completion as well as lowering the teaming score. In contrast, COLE paired with $H_{\text{proxy}}$ triggers interdependencies equally frequently, but get accepted by $H_{\text{proxy}}$ more often, leading to improved task and teaming performance. Whereas, HSP triggers interdependencies least frequently, out of which only 15$\%$ get accepted, leading to a lower contribution to the teaming score.
% \textbf{Asymmetric Advantages}: We can observe that agents are doing more independent tasks in this layout. When doing interdependent subtasks, we can observe that agents are very frequently trying to initiate cooperation, but failing even more miserably here, as humans don't accept the help. This shows that this environment has lesser scope for cooperation.
% HSP initiates more interdependent actions than the others, and is also more successful than other agents in having those interdependencies accepted by humans. COLE and FCP also trigger interdependencies, but are less successful in having interdependencies accepted by humans compared to HSP.
\begin{table}[htp]
\tiny
\begin{tabular}{|l|l|l|l|l|l|l|}
\hline
Agent  & $\%  \text{Ag-Sub}_{\text{coor}}^{\text{Trig}} $ & $\%  \text{Ag-Sub}_{\text{Trig}}^{\text{Trig-Acc}} $ & $\%  \text{H-Sub}_{\text{coor}}^{\text{Trig}} $ & $\%  \text{H-Sub}_{\text{Trig}}^{\text{Trig-Acc}} $ \\ \hline
COLE                 &\cellcolor{lightgray} 32.67             &\cellcolor{lightgray} 41.32  &34.64      &\cellcolor{lightgray} 30.31          \\ \hline
HSP                 & 21.8           & 16.51     & \cellcolor{lightgray}39.8       &28.64  
\\ \hline
MEP               & 29.8            & 21.47     &37.6       &27.66            \\ \hline
FCP                & 39.75           & 33.61      & 31.75     &14.96        \\ \hline
\end{tabular}
\caption{ For Counter Circuit layout,
$\%  \text{Ag/H-Sub}_{\text{coor}}^{\text{Trig}} $ denotes the percentage of coordination subtasks that were executed by the agent/$H_{\text{proxy}}$ to trigger interdependencies, $\%  \text{Ag/H-Sub}_{\text{coor}}^{\text{Trig-acc}} $ represents the percentage of trigger coordination subtasks that were accepted by the teammates.}
\label{table:coor}
\end{table}
\subsection{Trigger vs Giver List for Human}
We analyze the the proportion of the interdependencies triggered by the $H_{\text{proxy}}$ that are subsequently accepted by the agent. The acceptance of an event by the agent is characterized as a cooperative response, signifying the agent's capacity to understand and engage with the human's proposed action. 
However, a lack of acceptance or recognition by the agent may indicate its failure to detect or respond to the opportunistic interdependencies initiated by the human. This could impede effective collaboration, as the agentâ€™s refusal or inability to act may result in missed opportunities for mutual benefit while working towards the shared goal. Additionally, repeated non-responses from the agent might lead to the human perceiving the agent as either uncooperative or incapable of understanding complex interdependencies. Thus, while the agent's behavior may not be perceived as intrusive in this case, its failure to respond adequately to human-initiated triggers could undermine the efficiency of teaming in this case. We observe in Table.\ref{table:coor} that $H_{\text{proxy}}$ triggers interdependencies more often than the SOTA agents, suggesting human's willingness to cooperate with the agent. In particular, $H_{\text{proxy}}$ tries to initiate interdependencies most often when paired with HSP. However, the triggered interdependencies are accepted less than 30$\%$times, similar to the trend in the last section. This suggests significant misalignment and cooperative incompatibility~\ref{cole} between SOTA agents and $H_{\text{proxy}}$ when they are trying to work together as a team. 
% As observed in Table. \ref{table: total}, the distribution of contribution to the team, between HSP and $H_{\text{proxy}}$ exhibits a significant asymmetry. Specifically, HSP receives substantially more interdependencies than it gives. A similar pattern is observed in the interactions between MEP and $H_{\text{proxy}}$.
\subsection{Event Distribution}
\begin{figure}[htp]
    \centering
    \includegraphics[width=1\linewidth]{pictures/cole_1.png}
    \caption{Event Distribution for COLE}
    \label{fig:enter-label}
\end{figure}
In this section, we investigate the distribution of the coordination subtasks for Counter Circuit and Asymmetric Advantages.  For each layout, we pick the teams with the highest levels of teamwork (COLE- $H_{\text{proxy}}$). We aim to focus on the key patterns of interactions that emerge during the teaming process. COLE and $H_{\text{proxy}}$ are highly involved in onion-passing tasks, and there is some coordination being achieved when it comes to passing onions. While both the agents in this layout are symmetrical in terms of the tasks they can reach, the agents converge to the strategy where $H_{\text{proxy}}$ is putting the onion on the counter and COLE is picking up the onion from it.  
However, an important observation is that $H_{\text{proxy}}$ places onions more frequently than they are being picked up by the COLE agent. This suggests an inefficiency in onion-passing for the team. Since the cooperative strategy in this layout does not involve dish-passing or soup-passing, there is little to no involvement from both agents for dish-passing and soup-passing tasks.  
Another notable pattern is the existence of cases where the team members accept interdependencies that they themselves triggered. For example, considering the event distribution for $H_{\text{proxy}}$, we observe the number of times $H_{\text{proxy}}$ pick the soup on the counter to be more than the number of times COLE put the soup on the counter. Coming to the number of times $H_{\text{proxy}}$ puts the soup on the counter, we can infer that $H_{\text{proxy}}$ is picking the soup it placed on the counter itself. This trend can be observed when we look at the action distribution for $H_{\text{proxy}}$ in Fig.\ref{fig:had}. It is clear that most of the \textit{Accept Actions} (Def.\ref{def:tr}) are not part of an interdependence with it's teammate, from which we can infer that most of these are performed $H_{\text{proxy}}$ to accept an interdependence it itself triggered.
\begin{figure}[htp]
    \centering
    \includegraphics[width=1\linewidth]{pictures/cole_2.png}
    \caption{Event Distribution for $H_{\text{proxy}}$}
    \label{fig:enter-label}
\end{figure}
\begin{figure}[htp]
    \centering
    \includegraphics[width=0.65\linewidth]{pictures/cole-2.png}
    \caption{Action Distribution for $H_{\text{proxy}}$ paired with COLE}
    \label{fig:had}
\end{figure}
\section{Conclusion}
Our analysis reveals that, in general, the level of cooperation observed within the evaluated teams is significantly lower than expected, even in scenarios where optimal joint policy should necessitate extensive collaboration. The measured proportion of interdependent actions remains minimal across all SOTA agents when paired with a human proxy, indicating that agents are not effectively leveraging cooperative strategies to achieve task success.
A key observation from our experiments is that the human proxy contributes more to cooperation when paired with agents, as compared to when agents are paired with themselves. This suggests that human teammates take on the burden of initiating cooperative behaviors, compensating for the lack of proactive engagement from the agents. 
Another crucial takeaway is that higher task reward does not necessarily equate to improved teamwork. While some teams demonstrate efficiency in task completion, their teaming performance remains suboptimal.
Our results also highlight that a significant portion of interdependencies triggered are not accepted by their teammates, leading to both diminished teaming scores and reduced task performance. This suggests a fundamental issue of misalignment between SOTA agent and humans, where the teammates fail to generate meaningful interdependencies or produce them in a manner that is ineffective or disruptive to their partner. Currently, most layouts in the Overcooked domain lack the sufficient coordination events necessary to adequately test cooperation, highlighting the need for improved layouts with increased potential interdependencies to facilitate the development of agents.
This decoupling of task success from teaming performance allows us to actually evaluate the cooperation within a team. We conclude that agents trained using MARL for HAT are not inducing cooperative behavior when paired with a human teammate. 
\textit{This paper proposes an objective framework for assessing team performance by presenting a novel formalization for cooperation within a team, which will be a significant contribution to guide the design and development of improved MARL approaches that can achieve robust teaming.
We also establish a novel method to judge how well agents collaborate, enabling the development of more effective learning strategies that can induce cooperation better.}

\section{Acknowledgment}
This research is supported in part by ONR grant N0001423-1-2409, DARPA grant HR00112520016, and gifts from Qualcomm, J.P. Morgan and Amazon.
% \begin{figure*}
% \begin{subfigure}{.3\textwidth}
%  \centering
%   \includegraphics[width=1\linewidth]{pictures/fcp-1.png}
%   \caption{Total actions by agent = 43}
% \end{subfigure}
% \begin{subfigure}{.3\textwidth}
%   \centering
%   \includegraphics[width=1\linewidth]{pictures/fcp-2.png}
%   \caption{Total actions by human = 44}
% \end{subfigure}
% \begin{subfigure}{.3\textwidth}
%   \centering
%   \includegraphics[width=1\linewidth]{pictures/mep-1.png}
%   \caption{Total actions by agent = 49.0}
% \end{subfigure} \\

% \begin{subfigure}{.3\textwidth}
%   \centering
%   \includegraphics[width=1\linewidth]{pictures/mep-2.png}
%   \caption{Total actions by human = 34.4}
% \end{subfigure} 
% \begin{subfigure}{.3\textwidth}
%   \centering
%   \includegraphics[width=1\linewidth]{pictures/hsp-1.png}
%   \caption{Total actions by agent = 27.2}
% \end{subfigure}
% \begin{subfigure}{.3\textwidth}
%   \centering
%   \includegraphics[width=1\linewidth]{pictures/hsp-2.png}
%   \caption{Total actions by human = 63.8}
% \end{subfigure}\\

% \begin{subfigure}{.3\textwidth}
%   \centering
%   \includegraphics[width=1\linewidth]{pictures/cole-1.png}
%   \caption{Total actions by agent = 50.0}
% \end{subfigure}
% \begin{subfigure}{.3\textwidth}
%   \centering
%   \includegraphics[width=1\linewidth]{pictures/cole-2.png}
%   \caption{Total actions by human = 62.34}
% \end{subfigure} 
% \label{fig:exp2}
% \caption{}
% \end{figure*}

\bibliography{example_paper}
\bibliographystyle{icml2025}
\end{document}


% This document was modified from the file originally made available by
% Pat Langley and Andrea Danyluk for ICML-2K. This version was created
% by Iain Murray in 2018, and modified by Alexandre Bouchard in
% 2019 and 2021 and by Csaba Szepesvari, Gang Niu and Sivan Sabato in 2022.
% Modified again in 2023 and 2024 by Sivan Sabato and Jonathan Scarlett.
% Previous contributors include Dan Roy, Lise Getoor and Tobias
% Scheffer, which was slightly modified from the 2010 version by
% Thorsten Joachims & Johannes Fuernkranz, slightly modified from the
% 2009 version by Kiri Wagstaff and Sam Roweis's 2008 version, which is
% slightly modified from Prasad Tadepalli's 2007 version which is a
% lightly changed version of the previous year's version by Andrew
% Moore, which was in turn edited from those of Kristian Kersting and
% Codrina Lauth. Alex Smola contributed to the algorithmic style files.
