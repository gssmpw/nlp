\section{Related Work}
\label{s:related-work}
	
	The problem of VM placement in cloud data centers is well studied \cite{dias2021,imran2022,saidi2023task,lin2024energy,alahmad2024multiple,xu2024meta,regaieg2021multi}. GPU-enabled VMs introduce additional challenges due to GPU virtualization constraints \cite{hong2017gpu}. NVIDIA introduced GRID \cite{nvidia_grid_doc}, which supported only homogeneous tenants. Later, MIG \cite{nvidia_mig_doc} enabled both homogeneous and heterogeneous instances. In \cite{siavashi2019gpucloudsim}, NVIDIA GRID is examined for GPU-enabled VM placement, highlighting inefficiencies in the first-fit strategy as VM numbers grow. The proposed first-fit increasing algorithm improves acceptance rate (59\%), makespan (25\%), and energy consumption (21\%). In \cite{kulkarni2021gpu}, NVIDIA GRID is further analyzed for GPU-aware VM placement, enhancing energy efficiency, reducing host search space, and shortening makespan while tackling GPU resource management challenges in cloud applications.
	
	An ILP model for GPU-enabled VM placement using NVIDIA GRID GPU virtualization is proposed in \cite{garg2019virtual}, aiming to minimize physical GPU usage in the cloud. It is compared with two heuristics, showing similar performance to ILP while achieving faster execution. A metaheuristic approach using dense neural networks (DNN) was proposed in \cite{sivaraman2019tecn}. The authors built a simulator for GPU-enabled VM placement with NVIDIA GRID, implemented six heuristics, and trained DNNs on various configurations. They also explored combining multiple DNNs. Using a custom metric based on GPU utilization and job waiting time, their method outperformed heuristics in 76\% of test cases. In \cite{sivaraman2018task}, authors explore a GPU-enabled VM placement problem for NVIDIA GRID, optimizing performance using a cost function based on GPU utilization, fully executed VMs, and GPU execution time. Various VM placement policies are analyzed, with one achieving the best results. 
	
	In \cite{li2022miso}, authors highlight the rapid advancements in GPU technology, enhancing HPC and AI/ML research but also leading to inefficient resource utilization. To address this, they propose MISO, a technique leveraging MIG on NVIDIA datacenter GPUs (e.g., A100, H100) for dynamic GPU resource partitioning. MISO utilizes the lightweight Multi-Process Service (MPS) \cite{nvidia_mps_doc} to predict optimal MIG partitioning without incurring exploration overhead. This approach improves GPU efficiency, achieving 49\% and 16\% lower average job completion times compared to unpartitioned and optimal static GPU partitioning, respectively.
	
	To optimize MIG partitions for serving DNNs, \cite{tan2021serving} introduces the Reconfigurable Machine Scheduling (RMS) problem, a new NP-hard abstraction. They propose MIG-serving, an algorithmic pipeline using greedy heuristics, Genetic Algorithm (GA), and Monte Carlo Tree Search (MCTS) for cost-efficient deployments. MIG-serving includes an optimizer for deployment generation and a controller for seamless application. Experiments on a 24-GPU A100 cluster show a 40\% GPU reduction over baseline static allocations with minimal service disruptions. In \cite{lee2024parvagpu}, the authors propose ParvaGPU, a GPU space-sharing framework that integrates MIG and MPS to optimize DNN inference in cloud environments. It mitigates GPU underutilization with the Optimal Triplet Decision algorithm, which selects MPS-activated MIG instances for maximum throughput. The Demand Matching algorithm optimally assigns instances to meet high request rates. To reduce GPU non-contiguous space inefficiency, Segment Relocation redistributes instances, while Allocation Optimization minimizes fragmentation by resizing partitions. Evaluations on A100 GPUs show that ParvaGPU ensures Service-Level Objective (SLO) compliance while reducing GPU usage.
	
	In \cite{arima2022optimizing}, the authors integrate GPU partitioning with power-aware scheduling to optimize resource allocation in CPU-GPU heterogeneous systems under power constraints. Leveraging MIG for fine-grained workload co-location, their approach employs scalability and interference models to improve efficiency and achieve near-optimal configurations across diverse workloads. In \cite{saroliya2023hierarchical}, authors integrate MPS and MIG hierarchically, using reinforcement learning to jointly optimize partitioning and job co-scheduling. Experimental results demonstrate up to 1.87Ã— throughput improvement over time-sharing scheduling.
	
	In \cite{weng_beware_2023}, authors identify GPU fragmentation as a key inefficiency in large ML clusters, where traditional bin packing fails to address partial GPU allocations. They propose Fragmentation Gradient Descent (FGD), which quantifies fragmentation and schedules tasks along its steepest descent to optimize GPU use. Evaluated on a 6,200-GPU cluster, FGD reduces unallocated GPUs by 49\%, reclaiming 290 more GPUs than existing schedulers.
	
	Our work differs from previous studies by considering MIG constraints, which impose unique placement rules. We formulate the problem such that MIG-enabled VMs require entire, indivisible GPU instances. Additionally, we optimize across three conflicting objectives: maximizing acceptance, minimizing active hardware, and reducing migrations. Furthermore, we enforce a stricter definition of active hardware, counting idle GPUs only when the entire machine is idle.