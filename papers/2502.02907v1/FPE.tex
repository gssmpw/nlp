



% \DeclareMathOperator*{\argmax}{arg\,max}

\chapter{Robust Pole Estimation of Irregular Objects using Spectral Decomposition}
\label{chapter:FPE}

% \DeclareMathOperator*{\argmin}{arg\,min}
% \DeclareMathOperator*{\argmax}{arg\,max}

% \newtheorem{theorem}{Theorem}[section]
% \newtheorem{definition}{Definition}[section]
% \newtheorem{lemma}{Lemma}[section]
% \newtheorem{corollary}{Corollary}[section]
% \newtheorem{assumption}{Assumption}[section]


% \begin{abstract}
%     We present an image-based algorithm to estimate the pole of an irregular object rotating about its principal axis as it is observed from multiple camera views. Given a batch of images collected from a fixed camera latitude, the pole projection onto the camera plane is observed by extracting symmetry in the co-added image using the Discrete Fourier Transform. By repeating the procedure from at least two camera latitudes, the pole orientation can be resolved. The proposed method uses silhouette observations and hence can be used at lower-resolutions of the object image such as during the approach phase. We demonstrate robustness to challenging lighting conditions (e.g., high sun phase angles) as well as inaccuracies in pointing and object centroiding.
% \end{abstract}

\section{Introduction}

Estimating the rotational motion of a space object is a common task in spaceflight applications. When navigating in the vicinity of a small celestial body, the rotating, body-fixed reference frame should be accurately determined to characterize the object, e.g., for shape reconstruction, and effectively perform navigation. The planetary-body rotation can affect the spacecraft dynamics, e.g., due to the non-uniform gravity field. Furthermore, reference points on the surface are often measured through images to determine the surface-relative spacecraft position. Target-rotation estimation is also required for missions to artificial objects, such as in rendezvous, proximity operations, and docking (RPOD) scenarios; this is especially important when approaching uncooperative targets. In this work, we assume that the target axis of rotation is inertially fixed, i.e., the object rotates about its principal axis. In this case, the rotation axis is often denoted as the pole. Principal-axis rotation is common among small celestial bodies, but also for some artificial targets. The problem we address is referred to as pole estimation.

Several image-based methods exist to estimate the pole of an irregular object. Lightcurve analysis is often used to determine approximate pole estimates of unresolved objects, typically using ground-based observations\cite{kaasalainen2001optimization}. However, it can lead to multiple pole hypotheses which may not be disambiguated. Landmark-based techniques can also be used for pole estimation: surface points, also known as landmarks, are tracked across consecutive images; the observed motion relative to the camera can be used to infer the pole orientation\cite{panicucci2023vision}. This technique, however, requires the imaged surface to be sufficiently resolved to detect landmarks, and hence can only be executed when the observing camera is close to the target. Furthermore, the camera-surface relative motion can complicate pole estimation due to the spacecraft translational motion relative to the target.

In many missions, it is common for the spacecraft to fly through a lengthy approach phase to arrive at the target. During approach, the rotating object is typically observed in low-resolution images for a long period of time. Once the object is sufficiently resolved (e.g., spanning tens to hundreds of pixels across the image), its rotational motion may be detected. As such, it is desirable to extract a pole estimate during this time interval in order to characterize the target rotation before arrival.

In this work, we present a pole-estimation algorithm which is suited for approach-phase imagery (but could be used in close proximity). The algorithm extracts the camera pole projection --- the pole direction projected onto the camera plane --- by observing the evolution of the object's silhouette, i.e., the contour between the object foreground and the image background. Silhouette observations from consecutive images are co-added and the pole projection is estimated as the direction of maximum symmetry in the co-added image. This process is affected by two primary error sources: (1) surface shadowing, which often changes the appearance of the rotating object, and (2) centerfinding errors, which can corrupt the co-added image due to inaccurate alignment between frames. To improve robustness to such effects, the proposed algorithm transforms images to the frequency domain, using the Discrete Fourier Transform (DFT). Symmetry-detection is then performed using the DFT amplitude spectrum of the co-added silhouette image. To determine the pole orientation in 3D space, pole projections from multiple camera views are combined, a process denoted \textit{pole triangulation}. The presented algorithm is named Fourier Pole Estimation (FPE).

\section{Problem Formulation}

Consider an irregular body represented by the volume $\mathcal{V}\subset \mathbb{R}^3$ which rotates about its pole. Let $\boldsymbol{\omega}_\mathcal{N} \in \mathbb{R}^3,\, ||\boldsymbol{\omega}_\mathcal{N}||=1$ be the pole direction with respect to the inertial frame $\mathcal{N}$. Define $\mathcal{B}$ as the body-fixed reference frame, rotating with $\mathcal{V}$. The frame $\mathcal{B}$ is such that its origin coincides with the body's center of mass and its z-axis is aligned with  $\boldsymbol{\omega}_\mathcal{N}$. Consider also an observing camera, defined by its position $\mathbf{r}_\mathcal{B} \in \mathbb{R}^3$ and orientation $R^{\mathcal{C}}_\mathcal{B} \in \mathbb{R}^{3\times 3}$ . Here, $\mathbf{r}_\mathcal{B}$ is a vector expressed relative to the center of mass of the rotating object; $R^{\mathcal{C}}_\mathcal{B}$ is the rotation matrix which transforms a vector from the camera-fixed frame $\mathcal{C}$ to the body-fixed frame $\mathcal{B}$, defined by the camera axes $\mathbf{i}_{c,\mathcal{B}}$, $\mathbf{j}_{c,\mathcal{B}}$, and $\mathbf{k}_{c,\mathcal{B}}$ as:

$R^\mathcal{C}_\mathcal{B} = [\mathbf{i}_{c,\mathcal{B}} \; \mathbf{j}_{c,\mathcal{B}} \; \mathbf{k}_{c,\mathcal{B}}  ]$

By convention, we assume that $\mathbf{i}_{c,\mathcal{B}}$ and $\mathbf{j}_{c,\mathcal{B}}$ are aligned with the image horizontal and vertical, from left to right and from top to bottom, respectively; $\mathbf{k}_{c,\mathcal{B}}$ points toward the imaged scene.

Let $\mathbf{p}_\mathcal{B}=[p_{1,\mathcal{B}},p_{2,\mathcal{B}},p_{3,\mathcal{B}}]^\top\in\mathcal{V}$ be a point belonging to the observed object. We model camera observations using a pinhole camera model, such that $\mathbf{p}_\mathcal{B}$ is observed as a point $\mathbf{u}=[u,v]^\top\in\mathbb{P}^2$ in the camera plane, where $\mathbb{P}^2$ denotes the 2D projective space, suitable to describe point projections into the camera plane\cite{henry2023absolute}. Given a camera view $\nu$, the measurement model of $\mathbf{u}$ can be written as

\begin{equation}
    \bar{\mathbf{u}} = C_\nu \bar{\mathbf{p}}_\mathcal{B}
\end{equation}

where $\bar{\mathbf{p}}_\mathcal{B}=[p_{B,1},p_{B,2},p_{B,3},1]^\top$ and $\bar{\mathbf{u}}=[u,v,1]^\top$ are the points $\mathbf{p}_\mathcal{B}$ and $\mathbf{u}$ expressed in homogeneous coordinates, respectively, and $C_\nu\in \mathbb{R}^{3\times 4}$ is the camera matrix corresponding to $\nu$, given by:

\begin{equation}
\label{eq:C_nu}
    C_\nu = K \left[ R^B_C \mid -\mathbf{r}_B \right]
\end{equation}

where $K\in \mathbb{R}^{3\times 3}$ is the camera calibration matrix and $R^B_C = \left( R^C_B \right)^\top$ is the rotation matrix transforming a vector from frame $\mathcal{B}$ to frame $\mathcal{C}$\cite{henry2023absolute}.

It is convenient to express the camera position in spherical coordinates, such that

\begin{equation}
    \mathbf{r}_\mathcal{B} = r [\mathrm{cos}(\lambda)\mathrm{cos}(\phi),\mathrm{cos}(\lambda)\mathrm{sin}(\phi),\mathrm{sin}(\lambda)]^\top
\end{equation}

where $r = ||\mathbf{r}||$, $\lambda$, and $\phi$ are the camera distance, latitude, and longitude, respectively.

In this work, we assume that each batch of images is acquired from constant camera distance and latitude, which we refer to as a hovering camera. Let $\nu_j=(r_j,\lambda_j)$ denote a hovering-camera view with fixed camera distance $r_j$ and latitude $\lambda_j$; the longitude $\phi$ varies due to the object's rotation about its pole. Further, let 

\begin{equation}
\label{eq:I_nu_j}
    \mathcal{I}_{\nu_j}=\{I_{\nu_j,\phi_{j,1}} , \dots, I_{\nu_j,\phi_{j,M_j}} \}
\end{equation}

be the set of $M_j$ images acquired from view $\nu_j$, where  $I_{\nu_j,\phi_{j,k}} \in \mathbb{R}^{N\times N}$ is acquired from the view $(\lambda_j, \phi_{j,k})$ \footnote{For simplicity, we carry out the discussion for square $N\times N$ matrices.}. In this work, the problem statement is to estimate $\boldsymbol{\omega}_N$ using two or more sets of images $\mathcal{I}_{\nu_j}$.

\subsection{Assumptions}
\label{sec:assumptions}

This study is based on the following assumptions:

\begin{enumerate}
    \item The camera orientation with respect to the inertial frame, $R^\mathcal{C}_\mathcal{N}$, is known for all camera views. 
    \item The volume $\mathcal{V}$ is fully contained within the camera field of view.
    \item The camera boresight $\mathbf{k}_c$ points approximately toward the center of mass of $\mathcal{V}$, in the sense that $\mathrm{cos}(-\mathbf{k}^\top_c \mathbf{r}) \approx 1$. This assumption allows to simplify the theoretical development of the proposed approach. However, this assumption does not require the image coordinates of the center of mass of $\mathcal{V}$ to be known. In fact, we will discuss robustness to common centerfinding inaccuracies, such as those due to the offset between the center of mass and the image centroid (e.g., the center of brightness).
    \item The image-foreground region is correctly labeled, i.e., background-subtraction, has already been performed.
    \item The camera intrinsic parameters (e.g., the camera calibration matrix \cite{henry2023absolute}) are known.
    \item The camera distance and latitude are constant over a given hovering-camera image batch $\mathcal{I}_{\nu_j}$. This assumption ensures that changes in the object appearances, from frame to frame, are solely due to (1) the rotation of the object about its pole and (2) small inaccuracies in camera pointing. This assumption can be relaxed if scale changes in the object appearance can be corrected for (e.g., using a-priori knowledge of the camera distance and its evolution) and perspective changes in the object appearance are negligible. Both conditions are sufficiently met for an image batch acquired during the approach phase, when the target object is relatively far.
\end{enumerate}

\section{Theoretical Development}
\label{sec:theor_dev}

% % summary of conceptual understanding of the theory

% % lemma: direction of line projection doesn't change with the range

% \subsection{Mathematical Preliminaries}

% Some of the mathematical constructs used to develop the proposed approach are defined in the following.



% % def: reflective symmetry (for a point vs region)

\subsection{Executive Summary}
Consider an irregular object rotating about its pole and a set of object silhouettes --- i.e., the outline of the object projected onto the camera plane --- acquired from a hovering camera view, with constant distance and latitude and varying longitude (Definition \ref{def:s_v}). Also consider the intensity function (i.e., the continuous representation of an image) obtained by co-adding silhouette observations acquired through a camera longitudinal range (Definition \ref{def:s_bar_v}). Then, the co-added intensity function can be expressed as the sum of a symmetric and an asymmetric function; the symmetric function exhibits symmetry about the pole projection into the camera plane  (Theorem \ref{th:s_bar}); hence, such a pole direction can be estimated using the symmetric structure. When the camera longitudinal range completes a full revolution about the object, the asymmetric component is null and the intensity function is perfectly symmetric. 

In practice, silhouette observations are often corrupted by imaging error sources. In this work, we analyze shadowing effects and centerfinding errors. We show that, even in the presence of such measurement errors, the co-added silhouette image retains its additive signal-and-noise form, whereby the underlying symmetry about the pole can be extracted (Theorem \ref{th:x_+_e}). This results relies on assuming sufficiently high imaging cadence and resolution (Assumption \ref{assumpt:O}), such that the continuous-function approximation in Definition \ref{def:s_v} is representative of the discrete image set.

Symmetry-detection performance can be significantly degraded by the above error sources. For instance, surface shadows can occlude a large portion of the object, severely altering the silhouette shape and its symmetry. To improve robustness to imaging error sources, we propose transforming silhouette images to the frequency domain, using the Discrete Fourier Transform (DFT, Definition \ref{def:dft}). By focusing on the frequency components,  the DFT captures global patterns and symmetries within the image, while improving robustness to noise. In particular, the DFT amplitude spectrum preserves symmetries present in the original image (Corollary \ref{cor:A_symm}) and is invariant to translation (Theorem \ref{th:shift}), making it suitable to handle centerfinding errors. Similarly to what shown for space-domain images, we show that the squared amplitude spectrum of the co-added silhouette image can also be written as the sum of a symmetric image --- the signal --- and an asymmetric image --- the noise (Lemma \ref{lemma:A_symm_plus_blob}). We assume that the amplitude spectrum of the asymmetric image (due to shadowing and centerfinding errors) does not exhibit reflective symmetry (Assumption \ref{assumption:const_xcorr}). Based on this assumption, we show that the pole projection onto the camera plane coincides with the direction of maximum symmetry in the amplitude spectrum of the co-added silhouette. This result holds despite the presence of error sources affecting imagery. The principle described is used to estimate the pole projection as seen from a single hovering camera. (The symmetry metric used here is from Definition \ref{def:psi_I} .) Due to the combined effect of reflective and conjugate symmetries affecting the DFT amplitude spectrum, the latter is symmetric not only about the pole projection, but also about the axis orthogonal to it. This effect causes a four-fold ambiguity in the pole orientation, which can be solving incorporating a-priori information or processing all pole hypotheses in the pole triangulation step described below.

By combining pole projections extracted from two or more camera views, the 3D orientation can be extracted using a process denoted as pole triangulation. Each pole-projection angle is treated as a measurement of the 3D orientation, which is then estimated by solving a least-squares problem (Theorem \ref{th:pole_triang}).

\subsection{Symmetry of Silhouette Observations}
\label{sec:perfect_silh}

In this section, we discuss the relationship between the pole direction and symmetry in the silhouette evolution. Consider a set of hovering-camera views --- i.e., varying longitudinal views with constant latitude and radial distance. The camera acquires silhouette observations of the rotating object. We will show that the \textit{stacked} image obtained by co-adding such silhouettes exhibits reflection symmetry with respect to the pole direction, which can therefore be estimated. The formalism proposed in this section treats images and the camera-view set as continuous quantities, neglecting the effect of image discretization and a finite number of views. The rationale is that image resolution and the distribution of camera longitudinal views should be sufficient for the continuous approximation to hold. We will assess the validity of this assumption empirically in Section \ref{sec:results}.

We begin by studying \textit{perfect silhouettes}, i.e., the object observed in the camera plane by neglecting error sources which corrupt the silhouette structure. In order to define regions in the image, it is convenient to firstly introduce indicator functions.

\begin{definition}
\label{def:indicator}
Let $\mathcal{A} \subseteq \mathcal{W}$, where $\mathcal{W}$ is a generic set. Also let $\mathbf{w}\in\mathcal{W}$. The indicator function of $\mathcal{A}$, $a:\mathcal{W}\rightarrow \mathbb{B}$ is defined as:
    
    \begin{equation}
        a(\mathbf{w}) = \begin{cases}
    1 & \text{if } \mathbf{w} \in \mathcal{A} \\
    0   & \text{otherwise }
  \end{cases}
    \end{equation}

    where $\mathbb{B}=\{0,1\}$ is the Boolean set.
\end{definition}

Then, we begin by defining the \textit{silhouette} of an object, as seen from a camera view.

\begin{definition}
    \label{def:s_v}
    Let $\mathcal{V}\subset \mathbb{R}^3$ be a volume. We define the silhouette $\mathcal{S}_{\nu,\phi} \subset \mathbb{P}^2$ of the volume $\mathcal{V}$ associated with the camera view $\nu$ and longitude $\phi$, as

    \begin{equation}
        \mathcal{S}_{\nu,\phi} = \{ \mathbf{u} \in \mathbb{P}^2 \; | \; \bar{\mathbf{u}} = C_{\nu,\phi} \bar{\mathbf{p}}_\mathcal{B}, \, \mathbf{p}_\mathcal{B} \in \mathcal{V} \}
    \end{equation}

where $C_{\nu,\phi}$ is the camera projection matrix (Equation \ref{eq:C_nu}) associated with $\nu$ and $\phi$. $\mathcal{S}_{\nu,\phi}$ is tied to the indicator function $s_{\nu,\phi}(\mathbf{u}): \mathbb{P}^2 \rightarrow \mathbb{B}$, according to Definition \ref{def:indicator}.
\end{definition}

To simplify the analysis, we introduce a reference frame aligned with the pole and the observer's direction.

\begin{definition}
    \label{eq:L_frame}
    Given a camera position $\mathbf{r}_\mathcal{B}$ and the pole direction $\boldsymbol{\omega}$, the pole-aligned camera reference frame $\mathcal{L}$ is defined by the coordinate set $(\ell_\perp,\ell_\omega,\ell_r)$ such that the corresponding directions, $\boldsymbol{\ell}_r$ , $\boldsymbol{\ell}_\perp$, $\boldsymbol{\ell}_\omega$, satisfy:

    \begin{equation}
        \boldsymbol{\ell}_r \parallel \mathbf{r}_\mathcal{B},\;\;\; \boldsymbol{\ell}_\perp \parallel \boldsymbol{\omega} \times \boldsymbol{\ell}_r,\;\;\; \boldsymbol{\ell}_\parallel \parallel \boldsymbol{\ell}_\perp \times \boldsymbol{\omega}
    \end{equation}

    The point $\mathbf{p}_\mathcal{L}$ expressed in the reference frame $\mathcal{L}$ can be written as:

    \begin{equation}
    \label{eq:p_L}
        \mathbf{p}_\mathcal{L} = p
        \begin{bmatrix}
            \mathrm{cos}(\lambda')\mathrm{cos}(\phi')\\
            \mathrm{cos}(\lambda')\mathrm{sin}(\phi')\\
            \mathrm{sin}(\lambda')
        \end{bmatrix}
    \end{equation}

    where $p=||\mathbf{p}||$ whereas $\lambda'$ and $\phi'$ are the camera-relative latitude and longitude in frame $\mathcal{L}$.
\end{definition}

\begin{definition}
    Given the camera and pole-aligned reference frames, $\mathcal{C}$ and $\mathcal{L}$, the in-plane pole angle $\alpha$ is given by:

    \begin{equation}
        \alpha = \mathrm{cos}^{-1}(\boldsymbol{\ell}^\top_\perp \mathbf{i}_c)
    \end{equation}
\end{definition}

Based on the assumptions stated in Section \ref{sec:assumptions}, $\boldsymbol{\ell}_\perp$ and $\mathbf{i}_c$ are considered coplanar; hence, $\alpha$ is also assumed to lie onto the camera plane.

\begin{definition}
    \label{def:u'}
    Given a point $\mathbf{u}=[u,v]^\top\in\mathbb{P}^2$, we introduce the pole-aligned 2D coordinates $\mathbf{u}'=[u',v']^\top\in\mathbb{P}^2$ such that:

    \begin{equation}
        \mathbf{u}' = R_\alpha \mathbf{u}
    \end{equation}

    where

    \begin{equation}
        R_\alpha = 
        \begin{bmatrix}
            \mathrm{cos}(\alpha) & -\mathrm{sin}(\alpha)\\
            \mathrm{sin}(\alpha) & \mathrm{cos}(\alpha)\\
        \end{bmatrix}
    \end{equation}

    Observe that $u'$ and $v'$ are measured along $\ell_\perp$ and $\ell_\omega$, respectively.
\end{definition}

The silhouette parameters used in this work are illustrated in Figure \ref{fig:silh_scheme}.

\begin{figure}
    \centering
    \includegraphics[width=0.6\linewidth]{figs/silh_scheme.png}
    \caption{Schematic of the main silhouette parameters used in this work: the perfect silhouette $\mathcal{S}$, the observed silhouette $\mathcal{O}$, the hidden silhouette $\mathcal{H}$. The coordinate systems used are also shown.}
    \label{fig:silh_scheme}
\end{figure}


We are interested in showing that $\mathcal{S}_{\nu,\phi}$ evolves symmetrically with respect to $\ell_\parallel$, i.e., the axis defined by the angle $\alpha$. We begin by identifying symmetries for individual silhouette points.

\begin{lemma}
    \label{lemma:s_v}
    The silhouette indicator function $s_{\nu,\phi'}(\mathbf{u}')$ satisfies the following symmetry property:

    \begin{equation}
        s_{\nu,-\phi'}([-u',v']^\top) = s_{\nu,\phi'}([u',v']^\top), \, \phi' = [0,2\pi),\, \forall \mathbf{u}' \in \mathbb{P}^2
    \end{equation}
\end{lemma}

\begin{proof}
    Consider a point $\mathbf{p}_\mathcal{B}\in\mathcal{V}$ observed from a camera position $\mathbf{r}_\mathcal{B}$. By expressing the observed point in the pole-aligned camera frame $\mathcal{L}$, using Definition \ref{def:u'}, the projection $\mathbf{u}'_\mathbf{p}$ of $\mathbf{p}_\mathcal{L}$ can be written as

\begin{equation}
\label{eq:bar_u'}
    \bar{\mathbf{u}}'_\mathbf{p} = K \left[ R^\mathcal{L}_\mathcal{L} \mid \mathbf{r}_\mathcal{L} \right]\bar{\mathbf{p}}_\mathcal{L}
\end{equation}

where $R^\mathcal{L}_\mathcal{L}=I_3$ and $I_3$ is the $3\times 3$ identity matrix. Then, substituting Equation \ref{eq:C_nu}, Equation \ref{eq:bar_u'} can be expanded as

\begin{equation}
\label{eq:u'_v'}
    \begin{bmatrix}
        u'_\mathbf{p} \\ v'_\mathbf{p}
    \end{bmatrix} =
    \dfrac{p}{p\,\mathrm{cos}(\lambda')\mathrm{cos}(\phi')-r}K
    \begin{bmatrix}
        \mathrm{cos}(\lambda')\mathrm{sin}(\phi')\\
        \mathrm{sin}(\lambda')
    \end{bmatrix}
\end{equation}

Now suppose that there are two camera views, $\mathbf{r}_a=(r,\lambda,\phi_a)$ and $\mathbf{r}_b=(r,\lambda,\phi_b)$, where $r$ and $\lambda$ are constant distance and latitude coordinates whereas $\phi_a$ and $\phi_b$ are such that $\phi'_b=-\phi'_a$. That is, the camera longitudes are symmetric with respect to the point longitude. Let $\mathbf{u}'_{\mathbf{p},a}=[u'_{\mathbf{p},a},v'_{\mathbf{p},a}]^\top$ and $\mathbf{u}'_{\mathbf{p},b}=[u'_{\mathbf{p},b},v'_{\mathbf{p},b}]^\top$ be the camera coordinates of the point $\mathbf{p}$ observed from $\mathbf{r}_a$ and $\mathbf{r}_b$, respectively. 

From Definition \ref{def:s_v}, we know that $s_{\nu,\phi'_a}(\mathbf{u}'_{\mathbf{p},a})=s_{\nu,\phi'_b}(\mathbf{u}'_{\mathbf{p},b})=1$. The objective is then to derive the relationship between $\mathbf{u}'_{\mathbf{p},a}$ and $\mathbf{u}'_{\mathbf{p},b}$. Based on Equation \ref{eq:u'_v'}, and since $\mathrm{sin}(-\phi')=-\mathrm{sin}(\phi'),\,\mathrm{cos}(-\phi')=\mathrm{cos}(\phi')$, we have:

\begin{equation}
    \phi'_b = -\phi'_a \Rightarrow u'_{\mathbf{p},b} = -u'_{\mathbf{p},a},\, v'_{\mathbf{p},b}=v'_{\mathbf{p},a},\, \forall \phi'\in[0,2\pi)
\end{equation}

from which Lemma \ref{lemma:s_v} follows.

\end{proof}

Lemma \ref{lemma:s_v} can be interpreted by examining the trajectories of individual points in the camera plane. The rotation of a point $\mathbf{p}_\mathcal{N}\in\mathcal{V}$ about the pole $\boldsymbol{\omega}_\mathcal{N}$ follows a circular trajectory with respect to the inertial frame. A circle projected onto the camera plane is an ellipse (most commonly in our applications), a parabola, or a hyperbola symmetric about the pole. That is, the trajectory of any given $\mathbf{p}_\mathcal{B}$ is observed as a camera point $\mathbf{u}(\phi)=C_\nu(\phi) \mathbf{p}_\mathcal{B}$ which follows a symmetric locus of point (e.g., an ellipse) about the direction $\ell_\parallel$.

The proposed pole-estimation approach, however, is based not on observing points $\mathbf{p}_\mathcal{B}\in\mathcal{V}$ individually, but rather silhouette regions $\mathcal{S}_\nu$ as a whole in the image. We posit that the image symmetry caused by the pole orientation can be extracted by co-adding silhouettes in consecutive frames. The models supporting silhouette co-addition are introduced next.

\begin{definition}
    Given an indicator function $a_\zeta(\mathbf{d})$ where $\mathbf{d}\in \mathcal{D}$ is a variable and $\zeta \in [\zeta_0,\zeta_f],\zeta \in \mathbb{R}$ is a parameter, the cumulative intensity function $\bar{a}_{[\zeta_1,\zeta_2]}(\mathbf{d}):\mathcal{D} \rightarrow \mathcal{D}$ is

    \begin{equation}
        \bar{a}_{[\zeta_1,\zeta_2]}(\mathbf{d}) = \int_{\zeta_1}^{\zeta_2} a_\zeta(\mathbf{d}) d\zeta
    \end{equation}
    
\end{definition}

\begin{definition}
    \label{def:s_bar_v}
    Consider a set of silhouettes $\{ \mathcal{S}_{\nu,\phi} \}_{\phi=\phi_0}^{\phi_f}$ and the corresponding indicator functions $\{ s_{\nu,\phi} \}_{\phi=\phi_0}^{\phi_f}$. The cumulative intensity function associated with the observation range $\phi \in [\phi_0, \phi_f]$ is

    \begin{equation}
    \label{eq:s_bar}
        \bar{s}_{\nu,[\phi_0,\phi_f]}(\mathbf{u}) = \int_{\phi_0}^{\phi_f} s_{\nu,\phi}(\mathbf{u})\, d\phi.
    \end{equation}
\end{definition}

In this work, we use $\bar{s}_{\nu,[\phi_0,\phi_f]}(\mathbf{u})$ to model the image intensity originating from co-added silhouettes. By modeling silhouettes with their corresponding indicator function (see Equation \ref{eq:s_bar}), we treat each of them as a constant-intensity region in the image. Effectively, this approach retains the silhouette structure while filtering out photometric information within it. The idea for this approach is that the pole-related symmetries previously described (e.g., Lemma \ref{lemma:s_v}) manifest as \textit{occupancy}, rather than \textit{photometric}, information. Conversely, photometric changes due to unrelated effects (e.g., surface reflectance and albedo) can dilute the amount of symmetry within the stacked image. In this sense, the proposed approach can be seen as conservative; future work may explore the effect of preserving photometric information to compute the co-added image.

For a given set of viewing-camera longitudes $[\phi_0, \phi_f]$, not all silhouette points in $s_{\nu,\phi}(\mathbf{u})$ (and hence in $\bar{s}_{\nu,[\phi_0,\phi_f]}(\mathbf{u})$) contribute to pole-direction estimation, as discussed below.

\begin{definition}
    \label{def:s_signal}
    Consider a set of camera views $(\nu,\phi),\,\phi=[\phi_0,\phi_f]$ and the corresponding silhouettes $\{\mathcal{S}_{\nu,\phi}\}_{\phi=\phi_0}^{\phi_f}$. We define the symmetric silhouette, $\mathcal{S}^\mathrm{symm}_{\nu,\phi} \subseteq \mathcal{S}_{\nu,\phi}$ as:

    \begin{equation}
        \mathcal{S}^\mathrm{symm}_{\nu,\phi} = \{ \mathbf{u}=C_\nu \mathbf{p}_\mathcal{B}\in\mathcal{S}_{\nu,\phi} \; | \; \mathbf{p}_\mathcal{B} = (p,\lambda',\phi') \; \text{and} \; \exists -\phi',\; \phi\in [\phi_0,\phi_f], \,\forall \mathbf{p}_\mathcal{B}\in\mathcal{V} \}
    \end{equation}

    The asymmetric silhouette $\mathcal{S}^\mathrm{blob}_{\nu,\phi'} \subseteq \mathcal{S}_{\nu,\phi'}$ is then defined as:

    \begin{equation}
        \mathcal{S}^\mathrm{blob}_{\nu,\phi'} = \{ \mathbf{u}=C_{\nu,\phi'} \mathbf{p}_\mathcal{B}\in\mathcal{S}_{\nu,\phi'} \; | \; \mathbf{u} \notin \mathcal{S}^\mathrm{symm}_{\nu,\phi'} \}
    \end{equation}
\end{definition}

The distinction made in Definition \ref{def:s_signal} highlights the effect of specific silhouette sub-regions on determining the pole orientation. If a point $\mathbf{p}_\mathcal{B}\in\mathcal{V}$ is observed from two longitudinally-symmetric camera views $\phi'_a$ and $\phi'_b$, where $\phi'_b = -\phi'_a$ (note that $\phi'$ denotes the longitude coordinate relative to the camera) then the corresponding observations $\mathbf{u}(\phi')$ and $\mathbf{u}(-\phi')$ are symmetric about the pole direction $\boldsymbol{\ell}_\parallel$. Since they reveal the underlying symmetry about $\boldsymbol{\ell}_\parallel$, $\mathbf{u}(\phi')$ and $\mathbf{u}(-\phi')$ can then be considered the \textit{signal} in the pole estimation process. Conversely, when the point is observed from two different views where $\phi'_b \neq \phi'_a$, the observations $\mathbf{u}(\phi')$ do not provide information on $\ell_\parallel$ and hence are considered noise.

The signal and noise sub-regions can be also be defined based on the evolution of individual point projections in the following way.

\begin{lemma}
\label{lemma:u_p}
Let $\mathbf{u}_\mathbf{p}(\phi')=C_{\nu,\phi'}\mathbf{p}_\mathcal{B}$ be the projection of the point $\mathbf{p}_\mathcal{B}=(p,\lambda_\mathbf{p},\phi_\mathbf{p})\in\mathcal{V}$. Suppose that $\mathbf{p}_\mathcal{B}$ is observed from the camera longitudes $\phi\in [\phi_0,\phi_f]$. Then,

\begin{equation}
    \mathbf{u}_\mathbf{p}(\phi') \in
    \begin{cases}
        \mathcal{S}^\mathrm{symm}_{\nu,\phi'} & \text{if} \; \phi_\mathbf{p} \in [\phi_0,\phi_f] \; \text{and} \; |\phi'| \leq \mathrm{min}(\phi_\mathbf{p}-\phi_0,\phi_f - \phi_\mathbf{p})\\
        \mathcal{S}^\mathrm{blob}_{\nu,\phi'} & \text{otherwise}
    \end{cases}
\end{equation}
\end{lemma}

\begin{proof}
    Lemma \ref{lemma:u_p} follows directly from combining Lemma \ref{lemma:s_v} and Definition \ref{def:s_signal}. 
\end{proof}

\begin{corollary}
    \label{cor:s_noise}
    If the camera longitude interval is $[\phi_0,\phi_f]=[0,2\pi)$, then $\mathcal{S}^\mathrm{blob}_{\nu,\phi'} = \emptyset$.
\end{corollary}

From Lemma \ref{lemma:u_p}, we see that each point trajectory $\mathbf{u}_\mathbf{p}(\phi')$ can be composed of both a symmetric and a non-symmetric sub-arc. This result relates the mechanics of individual points rotating about the pole to their manifestation as a finite silhouette.  Using this framework, we can now write the main result of this section.

\begin{theorem}
\label{th:s_bar}
    Consider a set of silhouettes $\{\mathcal{S}_{\nu,\phi}\}_{\phi=\phi_0}^{\phi_f}$ observed from the camera views $(\nu,\phi),\,\phi=[\phi_0,\phi_f]$. Then, the cumulative intensity function $\bar{s}_{\nu,[\phi_0,\phi_f]}$ can be written as the sum of a signal and a noise term as

    \begin{equation}
        \bar{s}_{\nu,[\phi_0,\phi_f]}(\mathbf{u}') = \bar{s}^{\mathrm{symm}}_{\nu,[\phi_0,\phi_f]}(\mathbf{u}') + \bar{s}^{\mathrm{blob}}_{\nu,[\phi_0,\phi_f]}(\mathbf{u}')
    \end{equation}

    where $\bar{s}^{\mathrm{symm}}_{\nu,[\phi_0,\phi_f]}(\mathbf{u}')$ exhibits reflective symmetry with respect to $\ell_\parallel$, i.e.:

    \begin{equation}
        \bar{s}^{\mathrm{symm}}_{\nu,[\phi_0,\phi_f]}([-u',v]^\top) = \bar{s}^{\mathrm{symm}}_{\nu,[\phi_0,\phi_f]}([u',v]^\top)
    \end{equation}

\end{theorem}

\begin{proof}
    Let $s^{\mathrm{symm}}_{\nu,\phi}$ and $s^{\mathrm{noise}}_{\nu,\phi}$ be the indicator functions of $\mathcal{S}^\mathrm{symm}_{\nu,\phi}$ and $\mathcal{S}^\mathrm{blob}_{\nu,\phi}$, respectively. From, Definition \ref{def:s_signal}, we know that $\mathcal{S}^\mathrm{signal}_{\nu,\phi} \cap \mathcal{S}^\mathrm{blob}_{\nu,\phi} = \emptyset$. Hence, the definition of $\bar{s}_{\nu,[\phi_0,\phi_f]}$ in Equation \ref{eq:s_bar} can be expanded as

    \begin{align}
        \bar{s}_{\nu,[\phi_0,\phi_f]}(\mathbf{u}) &= \int_{\phi_0}^{\phi_f} \left( s^{\mathrm{symm}}_{\nu,\phi}(\mathbf{u}) + s^{\mathrm{blob}}_{\nu,\phi}(\mathbf{u}) \right)\, d\phi \\
        &= \int_{\phi_0}^{\phi_f} s^{\mathrm{symm}}_{\nu,\phi}(\mathbf{u})\, d\phi + \int_{\phi_0}^{\phi_f} s^{\mathrm{blob}}_{\nu,\phi}(\mathbf{u}) \, d\phi \\
        &= \bar{s}^{\mathrm{symm}}_{\nu,[\phi_0,\phi_f]}(\mathbf{u}') + \bar{s}^{\mathrm{blob}}_{\nu,[\phi_0,\phi_f]}(\mathbf{u}')
    \end{align}
    
    To study the symmetry of $\bar{s}^{\mathrm{symm}}_{\nu,[\phi_0,\phi_f]}(\mathbf{u}')$, evaluate $\bar{s}^{\mathrm{symm}}_{\nu,[\phi_0,\phi_f]}([-u',v]^\top)$ using the camera-relative longitude $\phi'$ (Definition \ref{eq:L_frame}); the integration bounds $[-\phi'_\mathrm{max},\phi'_\mathrm{max}]$ can be derived from Lemma \ref{lemma:u_p}.

    \begin{equation}
        \bar{s}^{\mathrm{symm}}_{\nu,[\phi_0,\phi_f]}([-u',v']^\top) = \int_{-\phi'_\mathrm{max}}^{\phi'_\mathrm{max}} s^{\mathrm{symm}}_{\nu,\phi'}([-u',v'])\, d\phi
    \end{equation}

Applying a variable substitution $\varphi=-\phi'$, we have:

\begin{align}
    \bar{s}^{\mathrm{symm}}_{\nu,[\phi_0,\phi_f]}([-u',v']^\top) &= \int_{\phi'_\mathrm{max}}^{-\phi'_\mathrm{max}} s^{\mathrm{symm}}_{\nu,-\phi'}([-u',v'])\, (-d\varphi) \\
    &= \int_{-\phi'_\mathrm{max}}^{\phi'_\mathrm{max}} s^{\mathrm{symm}}_{\nu,-\phi'}([-u',v'])\, d\varphi \label{eq:s_bar_signal_temp1}\\
    &= \int_{-\phi'_\mathrm{max}}^{\phi'_\mathrm{max}} s^{\mathrm{symm}}_{\nu,\phi'}([u',v'])\, d\varphi \label{eq:s_bar_signal_temp2}\\
    &= \bar{s}^{\mathrm{symm}}_{\nu,[\phi_0,\phi_f]}([u',v']^\top)
\end{align}

where the equality between Equations \ref{eq:s_bar_signal_temp1} and \ref{eq:s_bar_signal_temp2} is a consequence of Lemma \ref{lemma:s_v}.
\end{proof}

Theorem \ref{th:s_bar} provides a model to determine the pole direction from imaging data. It implies that, for any finite interval of longitudinal camera views $[\phi_0,\phi_f]$, there exist some deterministic symmetric structure encoded in the co-added silhouette observations $\bar{s}_{\nu,[\phi_0,\phi_f]}$; furthermore, it states that the axis of symmetry is oriented along the pole projection onto the camera plane. Importantly, while observing a full rotation of the object maximizes the symmetric image component, and hence the pole-direction signal (as stated in Corollary \ref{cor:s_noise}), symmetries may be extracted from smaller longitudinal ranges of observations, such that $0 < \phi_0 < \phi_f < 2\pi$.

In this section, we developed a theoretical framework leveraging the "perfect silhouette" assumption (Definition \ref{def:s_v}). In the next section, we relax said assumption and assess common error sources in space imagery.

\subsection{Effect of Imaging Error Sources}

In practice, the co-added image obtained by stacking silhouettes (Equation \ref{eq:s_bar}) is usually corrupted by error sources. In this work, we discuss two primary error sources affecting the data of interest, namely shadowing effects and centerfinding errors. We will show that, despite these effects, the symmetric structure caused by the pole orientation  (see Section \ref{sec:perfect_silh}) can still exist in corrupted images and that the proposed pole-estimation approach exhibits some robustness to error sources.

\subsubsection{Shadowing Errors}

Typically, shadowing is the major effect influencing the silhouette appearance. The shape of irregular objects cast shadows onto the surface, and hence a portion of the observed silhouette can be occluded. This effect depends on the specific surface shape, as well as the geometry between the surface, the sunlight direction, and the camera view. Thus, the specific interaction of shadows with silhouettes is challenging to model in general. In the following, we introduce an abstracted representation of the shadowing effects, which we will study in Section \ref{sec:imaging_msr_model}.

\begin{definition}
\label{def:K}
    Given the set of points $\mathbf{p}_\mathcal{B}\in\mathcal{V}$, the set of visible points $\mathcal{K}_{\nu,\phi}\subset \mathcal{V}$ is defined as

    \begin{equation}
    \label{eq:K_nu}
        \mathcal{K}_{\nu,\phi} = \{ \mathbf{p}_\mathcal{B}\in\mathcal{V} \; | \: (\mathbf{p}_\mathcal{B}-\mathbf{r}_\mathcal{B}) \cap \mathcal{V} = \{ \mathbf{p}_\mathcal{B} \} \}
    \end{equation}

    where $\cap$ denotes the intersection between the locus of points located on  $\mathbf{p}_\mathcal{B}-\mathbf{r}_\mathcal{B}$ and the points in $\mathcal{V}$, whereas $\{\mathbf{p}_\mathcal{B}\}$ denotes the set containing $\mathbf{p}_\mathcal{B}$ only.
\end{definition}

The subset $\mathcal{K}_{\nu,\phi}$ represents the surface portion of $\mathcal{V}$ which is not affected by any self-occlusions with respect to the camera viewpoint $\mathbf{r}_\mathcal{B}$. The condition in Equation \ref{eq:K_nu} implies that there are no points occluding the line of sight between $\mathbf{r}_\mathcal{B}$ and the point $\mathbf{p}_\mathcal{B}$ of interest.

\begin{definition}
\label{def:L}
    Given the set of points $\mathbf{p}_\mathcal{B}\in\mathcal{V}$, the set of illuminated points $\mathcal{Z}_\mathbf{l}\subset \mathcal{V}$ is defined as

    \begin{equation}
        \mathcal{Z}_\mathbf{l} = \{ \mathbf{p}_\mathcal{B}\in\mathcal{V} \; | \: (\mathbf{p}_\mathcal{B} - \mathbf{l}_\mathcal{B}) \cap \mathcal{V} = \{ \mathbf{p_\mathcal{B}}\} \}
    \end{equation}

    where $\mathbf{l}_\mathcal{B}$ is the sun (i.e., the light source) position.
\end{definition}

\begin{definition}
    \label{def:O_nu}
    The observable silhouette $\mathcal{O}_{\nu,\phi}\subseteq \mathcal{S}_{\nu,\phi}$ is the silhouette region which is both visible and illuminated, defined as

    \begin{equation}
        \mathcal{O}_{\nu,\phi} = \{ \mathbf{u}=C_\nu \mathbf{p}_\mathcal{B}\in\mathcal{V} \; | \; \mathbf{p}\in \mathcal{K}_{\nu,\phi}, \mathbf{p}_\mathcal{B}\in \mathcal{Z}_\mathbf{l} \}
    \end{equation}

        with indicator function $o_{\nu,\phi}: \mathbb{P}^2 \rightarrow \mathbb{B}$.

        Conversely, the hidden silhouette $\mathcal{H}_{\nu,\phi} \subseteq \mathcal{S}_{\nu,\phi}$, i.e., the silhouette region which is not observable, is the subset defined as

    \begin{equation}
        \mathcal{H}_{\nu,\phi} = \{ \mathbf{u}\in \mathcal{S}_{\nu,\phi} \; | \; \mathbf{u} \notin \mathcal{O}_{\nu,\phi} \}
    \end{equation}

    with indicator function $h_{\nu,\phi}:\mathbb{P}^2 \rightarrow \mathbb{B}$ and

    \begin{equation}
        \mathcal{O}_{\nu,\phi} \cap \mathcal{H}_{\nu,\phi} = \emptyset
    \end{equation}
\end{definition}

Due to shadowing, $\mathcal{O}_{\nu,\phi}$ is the only silhouette region actually observed in each image. We will use this fact to derive the silhouette-image model.

\subsubsection{Centerfinding Errors}

When imaging an unknown space object, the image coordinates of its center of mass are typically unknown a priori and need to be estimated --- a procedure known as \textit{centerfinding}. In many cases, and especially for irregular objects, images only provide an approximate estimates of the center of mass, often referred to as the centroid. For instance, one common centerfinding techniques is determining the so-called center of brightness in the image, which serves as an approximation of the actual center of mass and by using pixel intensities of the imaged object\cite{owen2011methods}. The offset between the centroid and the true center of mass is then an important error source to consider for space-imaging applications.

In the context of the proposed technique, the centroid is necessary to align and combine silhouette observations from multiple frames in order to produce the co-added silhouette image. Indeed, the imaged object can move in the camera plane, from frame to frame. A centerfinding step is then required to align multiple silhouette observations. Note that the symmetric image structure discussed in Section \ref{sec:perfect_silh} exhibits symmetry with respect to the object's true pole, which intersects the location of the center of mass. In this section, we then model the effect of centroid errors on image symmetry.

\begin{definition}
    \label{def:s_tilde}
    Given a silhouette $\mathcal{S}_{\nu,\phi}$ and the associated centroid $\mathbf{c}\in\mathbb{P}^2$, the centered silhouette $\widetilde{\mathcal{S}}_{\nu,\phi}$ is defined as

    \begin{equation}
    \label{eq:S_centered}
        \widetilde{\mathcal{S}}_{\nu,\phi} = \{ \tilde{\mathbf{u}}\in\mathbb{P}^2\; | \; \tilde{\mathbf{u}} = \mathbf{u} - \mathbf{c},\, \mathbf{u} \in \mathcal{S}_{\nu,\phi}\}
    \end{equation}
\end{definition}

\begin{definition}
\label{def:s_tilde_int_and_err}
    Given the centered silhouette $\widetilde{\mathcal{S}}_{\nu,\phi}$, let $\mathcal{S}^{\cap}_{\nu,\phi}$ and $\tilde{\mathcal{S}}^\emptyset_{\nu,\phi}$ be the intersection and no-intersection regions, respectively, with the associated indicator functions $s^\cap_{\nu,\phi}$ and $s^\emptyset_{\nu,\phi}$, such that

        \begin{equation}
        \mathcal{S}^{\cap}_{\nu,\phi} = \widetilde{\mathcal{S}}_{\nu,\phi} \cap \mathcal{S}_{\nu,\phi}
    \end{equation}

    where $\mathcal{S}_{\nu,\phi}$ is the perfect silhouette (Definition \ref{def:s_v}) and

    \begin{equation}
    \label{eq:S_tilde}
        \widetilde{\mathcal{S}}_{\nu,\phi} = \mathcal{S}^{\cap}_{\nu,\phi} \cup \tilde{\mathcal{S}}^\emptyset_{\nu,\phi},\;\;\; \mathcal{S}^{\cap}_{\nu,\phi} \cap \tilde{\mathcal{S}}^\emptyset_{\nu,\phi} = \emptyset
    \end{equation}
\end{definition}

As shown by Definition \ref{def:s_tilde}, centerfinding errors produce a translation bias in the observed silhouette $\widetilde{\mathcal{S}}_{\nu,\phi}$, with respect to the true silhouette $\mathcal{S}_{\nu,\phi}$. $\widetilde{\mathcal{S}}_{\nu,\phi}$ and $\mathcal{S}_{\nu,\phi}$ partially overlap with each other; the extent of the overlapping region $\mathcal{S}^\cap_{\nu,\phi}$ depends on the centroiding error $\mathbf{c}$ (Equation \ref{eq:S_centered}). (When centerfinding errors are small, $\widetilde{\mathcal{S}}_{\nu,\phi}$ and $\mathcal{S}_{\nu,\phi}$ largely overlap and the region $\tilde{\mathcal{S}}^\emptyset_{\nu,\phi}$ is small.) For our purposes, $\mathcal{S}^\cap_{\nu,\phi}$ is considered the signal as it encodes the symmetry about the true pole. For example, in the ideal case where the centroid coincides with the center of mass, we have $\mathbf{c}=0 \Rightarrow \mathcal{S}^\cap_{\nu,\phi} = \mathcal{S}_{\nu,\phi}$. Conversely, $\tilde{\mathcal{S}}^\emptyset_{\nu,\phi}$ does not contribute to the symmetry about the true pole in general, as its evolution depends on specific realizations of $\mathbf{c}$.

The silhouette regions resulting from centerfinding errors are illustrated in Figure \ref{fig:silh_overlap}.

\begin{figure}
    \centering
    \includegraphics[width=0.6\linewidth]{figs/silh_overlap.png}
    \caption{Silhouette regions resulting from centerfinding errors.}
    \label{fig:silh_overlap}
\end{figure}

\subsection{Silhouette-Image Model}
\label{sec:imaging_msr_model}

Accounting for the combined effect of shadowing and centerfinding errors, we can conveniently define the \textit{effective silhouette}, i.e., the silhouette portion which contributes to pole estimation directly.

\begin{definition}
    Let $\widetilde{\mathcal{O}}_{\nu,\phi}\subseteq \mathcal{S}_{\nu,\phi}$ and $\mathcal{S}^\cap_{\nu,\phi}\subseteq \mathcal{S}_{\nu,\phi}$ be the observed and intersection silhouette regions, according to Definitions \ref{def:O_nu} and \ref{def:s_tilde_int_and_err}, respectively. Then, the effective silhouette $\mathcal{X}_{\nu,\phi} \subseteq \mathcal{S}_{\nu,\phi}$ and error silhouette $\mathcal{E}_{\nu,\phi} \subseteq \mathcal{S}_{\nu,\phi}$ are defined as

    \begin{equation}
        \widetilde{\mathcal{X}}_{\nu,\phi} = \{ \mathbf{u}\in\mathcal{S}_{\nu,\phi} \; | \; \mathbf{u}\in \widetilde{\mathcal{O}}_{\nu,\phi},\, \mathbf{u}\in\mathcal{S}^\cap_{\nu,\phi} \}
    \end{equation}
    \begin{equation}
\widetilde{\mathcal{E}}_{\nu,\phi} = \{ \mathbf{u}\in\mathcal{S}_{\nu,\phi} \; | \; \mathbf{u}\in \widetilde{\mathcal{O}}_{\nu,\phi},\, \mathbf{u}\notin\mathcal{S}^\cap_{\nu,\phi} \}
    \end{equation}

    with associated indicator function $\tilde{x}_{\nu,\phi}$ and $\tilde{e}_{\nu,\phi}$ and

    \begin{equation}
    \label{eq:X_cap_E}
        \widetilde{\mathcal{X}}_{\nu,\phi} \cap \widetilde{\mathcal{E}}_{\nu,\phi} = \emptyset
    \end{equation}
\end{definition}

Finally, we can derive a model for the silhouette image which accounts for the above error sources.

\begin{theorem}
    \label{th:x_+_e}
    Given a set of co-added silhouette observations $\{ \widetilde{\mathcal{O}}_{\nu,\phi} \}_{\phi=-\phi_0}^{\phi_f}$, observed through a camera longitudinal range $[\phi_0,\phi_f]$, the associated cumulative intensity function $\bar{\tilde{o}}_{\nu,[\phi_0,\phi_f]}(\mathbf{u})$ can be written as:

\begin{equation}
    \bar{\tilde{o}}_{\nu,[\phi_0,\phi_f]}(\mathbf{u}) = \bar{\tilde{x}}^{\mathrm{symm}}_{\nu,[\phi_0,\phi_f]} + \bar{\tilde{e}}_{\nu,[\phi_0,\phi_f]}
\end{equation}

    where $\bar{\tilde{x}}^{\mathrm{blob}}_{\nu,[\phi_0,\phi_f]}$ is the cumulative intensity function of the signal, which exhibits reflective symmetry about the pole such that

    \begin{equation}
        \bar{\tilde{x}}^{\mathrm{symm}}_{\nu,[\phi_0,\phi_f]}([-u',v]^\top) = \bar{\tilde{x}}^{\mathrm{symm}}_{\nu,[\phi_0,\phi_f]}([u',v]^\top)
    \end{equation}

    whereas $\bar{\tilde{e}}_{\nu,[\phi_0,\phi_f]}$ is the noise cumulative intensity function.
    
\end{theorem}

\begin{proof}
Equation \ref{eq:X_cap_E} implies that

    \begin{equation}
       \tilde{o}_{\nu,\phi} = \tilde{x}_{\nu,\phi} + \tilde{e}_{\nu,\phi}
    \end{equation}

The intensity function describing the observed silhouettes can then be expanded as

\begin{align}
    \bar{\tilde{o}}_{\nu,[\phi_0,\phi_f]} &= \int_{\phi_0}^{\phi_f} \tilde{o}_{\nu,\phi} \, d\phi \\
    &= \int_{\phi_0}^{\phi_f} \left( \tilde{x}_{\nu,\phi} + \tilde{e}_{\nu,\phi} \right) \, d\phi \\
    &= \int_{\phi_0}^{\phi_f} \tilde{x}_{\nu,\phi} \, d\phi + \bar{\tilde{e}}_{\nu,[\phi_0,\phi_f]} \label{eq:o_bar_tilde_3}\\
\end{align}

Since $\widetilde{X}_{\nu,\phi}\subseteq \mathcal{S}_{\nu,\phi}$, we can use Lemma \ref{lemma:s_v} to expand Equation \ref{eq:o_bar_tilde_3} as:

\begin{align}
    \bar{\tilde{o}}_{\nu,[\phi_0,\phi_f]} &= \bar{\tilde{x}}^{\mathrm{symm}}_{\nu,[\phi_0,\phi_f]} + \bar{\tilde{x}}^{\mathrm{blob}}_{\nu,[\phi_0,\phi_f]} + \bar{\tilde{e}}_{\nu,[\phi_0,\phi_f]} \\
    &= \bar{\tilde{x}}^{\mathrm{symm}}_{\nu,[\phi_0,\phi_f]} + \bar{\tilde{n}}_{\nu,[\phi_0,\phi_f]}
\end{align}


where, from Lemma \ref{lemma:s_v} , $\bar{\tilde{x}}^{\mathrm{symm}}_{\nu,[\phi_0,\phi_f]}$ is symmetric about the pole projection $\boldsymbol{\ell}_\parallel$ and the asymmetric component is given by:

\begin{equation}
    \bar{\tilde{n}}_{\nu,[\phi_0,\phi_f]}  = \bar{\tilde{x}}^{\mathrm{symm}}_{\nu,[\phi_0,\phi_f]} + \bar{\tilde{e}}_{\nu,[\phi_0,\phi_f]}
\end{equation}
\end{proof}

Theorem \ref{th:x_+_e} is the main result in support of using silhouette images for pole estimation. It suggests that, despite the presence of error sources corrupting the signal, the image information preserves some level of symmetry with respect to the pole projection. It also highlights the additive relationship between signal and noise terms; the relative magnitudes between the two depend on the severity of error sources and the total longitudinal range covered by camera views. Generally, it is reasonable to expect that lower sun phase angles (i.e., less severe shadowing effects), higher centerfinding accuracy, and longer camera longitudinal arcs would yield a higher level of symmetry in the image. (A more quantitative metric for the amount of symmetry is introduced in Section \ref{sec:fourier}.)

Before proceeding any further, it is important to recognize that the above relationships have been derived for continuous quantities. In practice, silhouettes would be observed in imagery affected by quantization. The validity of the proposed approach is then contingent upon the following statements.

\begin{definition}
    \label{def:S_img}
    Given a silhouette $\mathcal{S}_{\nu,\phi}\in\mathbb{P}^2$ with indicator function $s_{\nu,\phi}$, the corresponding silhouette images $S_{\nu,\phi} \in \mathbb{B}^{N\times N}$ and $S'_{\nu,\phi} \in \mathbb{B}^{N\times N}$ are such that
    
    \begin{equation}
        S_{\nu,\phi}(m,n) = \mathrm{sup}\left( \{ s_{\nu,\phi}(\mathbf{u}=[u,v]^\top) \; | \; u^\ulcorner_n \leq u < u^\ulcorner_{n+1},\, v^\ulcorner_m \leq v < v^\ulcorner_{m+1}\} \right)
    \end{equation}

    \begin{equation}
    \label{eq:S'}
        S'_{\nu,\phi}(m,n) = \mathrm{sup}\left( \{ s_{\nu,\phi}(\mathbf{u}'=[u',v']^\top) \; | \; u'^\ulcorner_n \leq u' < u'^\ulcorner_{n+1},\, v'^\ulcorner_m \leq v' < v'^\ulcorner_{m+1} \} \right)
    \end{equation}

    where the notation $S(m,n)$ indicates the $mn$-esimal component ($m$-th row, $n$-th column) of the image $S$ and $\mathbf{u}^\ulcorner_{nm}=[u^\ulcorner_n,v^\ulcorner_m]^\top \in \mathbb{Z}^2$ are the coordinates of the upper-left corner of the $mn$-esimal pixel.
    
\end{definition}

We will later see that it is convenient to define images with respect to both a generic coordinate system $[u,v]$ and the coordinate system aligned with the pole direction, $[u',v']$.

From Definition \ref{def:S_img}, the silhouette image $S_{v,\phi}$ can be interpreted as a Boolean occupancy grid where each pixel containing a portion of the surface is equal to 1, whereas all other pixels are equal to zero. It is effectively a discrete representation of the original, continuous silhouette $\mathcal{S}_{\nu,\phi}$.

\begin{definition}
     \label{def:O_bar}
     Let $\{ O_{\nu,\phi_k} \}_{k = 0}^M$  be a set of $M$ silhouette images observed from the camera views defined by $\nu$ and $\{ \phi_0, \dots, \phi_M \}$. The co-added silhouette image $\bar{O}_{\nu,[\phi_0,\phi_f]}\in \mathbb{B}^{N\times N}$ is such that the $mn$-esimal component  ($m$-th row, $n$-th column), $\bar{O}_{\nu,[\phi_0,\phi_f]}(m,n)$, is given by:

     \begin{equation}
         \bar{O}_{\nu,[\phi_0,\phi_f]}(m,n) = \sum_{k=0}^M O_{\nu,\phi_k}(m,n)
     \end{equation}
     
\end{definition}

\begin{assumption}
    \label{assumpt:O}
    The co-added silhouette image $\bar{O}_{\nu,[\phi_0,\phi_f]}$ is such that

    \begin{equation}
        \sum_{k=0}^M O_{\nu,\phi_k}(m,n) \approx \bar{\tilde{o}}_{\nu,[\phi_0,\phi_{n_\phi}]}(\mathbf{u}_{nm}),\, m,n = 1,\dots,N
    \end{equation}
\end{assumption}

Assumption \ref{assumpt:O} requires that the cumulative intensity obtained by adding discrete silhouette images, acquired from a discrete set of camera views, is approximated well by its continuous-function counterpart. In turn, this statement defines conditions on image resolution and imaging cadence, both of which need to be sufficiently high for the continuous-function approximation to hold. This assumption should be assessed for the specific scenario of interest. In this work, we empirically show the validity of this assumption for notional observation scenarios (see Section \ref{sec:results}).

\subsection{Symmetry in Images}

In this section, we introduce parameters to describe reflective symmetry in images, which will be used later to describe the FPE algorithm.

% \begin{definition}
%     Let $\mathbf{x}\in\mathbb{R}^2$ be a point. Then, the reflective transformation of $\mathbf{x}$ with respect to the origin and the line with an angle $\theta$ with respect to the horizontal axis is a matrix $T_\mathrm{Ref,\theta}\in\mathbb{R}^{2\times 2}$ defined as:

%     \begin{equation}
%         T_\mathrm{Ref,\theta} =
%         \begin{bmatrix}
%             \mathrm{cos}(2\theta) & \mathrm{sin}(2\theta)\\
%             \mathrm{sin}(2\theta) & -\mathrm{cos}(2\theta)\\
%         \end{bmatrix}
%     \end{equation}

%     The corresponding reflected point $\mathbf{x}_{\mathrm{Ref},\theta}$ is given by

%     \begin{equation}
%         \mathbf{x}_{\mathrm{Ref},\theta} = T_\mathrm{Ref,\theta} \mathbf{x}
%     \end{equation}
% \end{definition}

% \begin{definition}
% \label{def:T_Ref}
% Let $I\in\mathbb{R}^{N\times N}$ be an image. Then, $I$ exhibits reflective symmetry with respect to $\theta$ if and only if

% \begin{equation}
%     I(m_{\mathrm{Ref},\theta},n_{\mathrm{Ref},\theta}) = I(m,n),\, m,n = 1,\dots, N
% \end{equation}

% where $m_{\mathrm{Ref},\theta},n_{\mathrm{Ref},\theta}$ are the pixel indexes corresponding to the reflection of the $mn$-esimal pixel, such that

% \begin{equation}
% \begin{bmatrix}
% u_{m_{\mathrm{Ref},\theta}}\\
% v_{n_{\mathrm{Ref},\theta}}\\
% \end{bmatrix} = T_{\mathrm{Ref},\theta}
% \begin{bmatrix}
%     u_m \\ v_n
% \end{bmatrix},\, u_{m_{\mathrm{Ref},\theta}}, v_{n_{\mathrm{Ref},\theta}} \in \mathbb{Z}
% \end{equation}
    
% \end{definition}

\begin{definition}
    Let $\mathbf{x}\in\mathbb{R}^2$ be a vector and $\mathbf{d}_\theta = [d_{\theta,1},d_{\theta,2}]^\top \in \mathbb{R}^2,\,||\mathbf{d}_\theta||=1$ a direction defined by the angle $\theta$, i.e.:

    \begin{equation}
        \theta = \mathrm{atan2}(d_{\theta,2},d_{\theta,1})
    \end{equation}

    A reflective transformation of $\mathbf{x}$ with respect to $\mathbf{d}_\theta$ is defined by the matrix $T_{\mathrm{Ref},\theta}$ such that:

\begin{equation}
    T_{\mathrm{Ref},\theta} = \begin{bmatrix}
        \mathrm{cos}(2\theta) & \mathrm{sin}(2\theta) \\
        \mathrm{sin}(2\theta) & -\mathrm{cos}(2\theta)
    \end{bmatrix}
\end{equation}

% The point reflected according to $T_\mathrm{Ref}(\theta)$ is denoted as $\mathbf{x}_{\mathrm{Ref},\theta} = T_\mathrm{Ref}(\theta) \mathbf{x}$.
\end{definition}



\begin{definition}
    \label{def:refl_symm_alpha}
    
    Let $I\in\mathbb{R}^{N\times N}$ be an image and let $[u_n,v_m]^\top \in \mathbb{P}^2$ be the coordinates of the pixel $I(m,n)$. Suppose that $[u_{n_{\mathrm{Ref},\theta}},v_{m_{\mathrm{Ref},\theta}}]^\top$ are the coordinates of the reflection of $[u_n,v_m]^\top$ about the axis $\mathbf{d}_\theta$, such that:

    \begin{equation}
        \begin{bmatrix}
            u_{n_{\mathrm{Ref},\theta}} \\ v_{m_{\mathrm{Ref},\theta}}
        \end{bmatrix} = T_{\mathrm{Ref},\theta}
        \begin{bmatrix}
            u_n \\ v_m
        \end{bmatrix}
    \end{equation}

    Then, $I$ exhibits reflective symmetry with respect to the axis $\mathbf{d}_\theta$ if and only if

    \begin{equation}
        \exists \; m_{\mathrm{Ref},\theta},n_{\mathrm{Ref},\theta} \in \mathbb{Z} \; | \; I(m_{\mathrm{Ref},\theta},n_{\mathrm{Ref},\theta}) = I(m,n),\; m,n=1,\dots,N
    \end{equation}
\end{definition}



\begin{definition}
    \label{def:vert_refl_symm}
    An image $I\in\mathbb{R}^{N\times N}$ is said to exhibit vertical reflective symmetry if and only if:

    \begin{equation}
        I(m,n) = I(m,N-n+1),\,m,n=1,\dots,N
    \end{equation}
\end{definition}

In practice, co-added images rarely exhibit perfect symmetry, due to the presence of error sources such as the shadowing and centerfinding effects previously discussed. It is therefore of interest to introduce a metric to quantify the level of symmetry for corrupted images. To do so, we firstly need to introduce the image-rotation operator.

\begin{definition}
    \label{def:I_theta}
    Let $I\in\mathbb{R}^{N\times N}$ be an image and let $R_\theta \in \mathbb{R}^{2\times 2}$ be a 2D rotation by the angle $\theta$, such that

    \begin{equation}
        R_\theta =
        \begin{bmatrix}
            \mathrm{cos}(\theta) & -\mathrm{sin}(\theta)\\
            \mathrm{sin}(\theta) & \mathrm{cos}(\theta)
        \end{bmatrix}
    \end{equation}
    
 Then, the image $I_\theta \in \mathbb{R}^{N\times N}$ obtained by applying the rotation $R_\theta$ to $I$ is such that:

    \begin{equation}
        I_\theta(m,n) = \mathcal{R}_I(m_{-\theta},n_{-\theta})
  \end{equation}

  where $\mathcal{R}_I:\mathbb{R}^2 \rightarrow \mathbb{R}$ is an image-interpolation function and $m_{-\theta},n_{-\theta}$ are the pixel indexes such that:

  \begin{equation}
  \label{eq:u_m,v_n}
\begin{bmatrix}
u_{n_{-\theta}} \\
v_{m_{-\theta}}
\end{bmatrix} = R_{-\theta}
\begin{bmatrix}
u_{n} \\
v_{m}
\end{bmatrix}
\end{equation}

where $[u_n, v_m]^\top$ are the coordinates of the center of the $mn$-esimal pixel.

\end{definition}

Due to their discrete nature, image rotation requires an interpolation procedure to assign pixel intensities resulting from the rotation $R_\theta$. In general, rotating integer pixel coordinates (Equation \ref{eq:u_m,v_n}) results in non-integer coordinates, which then need to be interpolated to reconstruct the rotated image. Multiple interpolation techniques exist, such as bilinear and bicubic methods, which make use of $2\times 2$ and $4\times 4$ pixel neighborhoods, respectively\cite{castleman1996digital}. In this work, we use the simple nearest-neighbor interpolation, defined as follows.

\begin{definition}
    Given the image $I\in\mathbb{R}^{N\times N}$, the nearest-neighbor image-interpolation function, $\mathcal{R}_{I,\mathrm{nn}}:\mathbb{R}^2 \rightarrow \mathbb{R}$, is defined as

    \begin{equation}
        \mathcal{R}_{I,\mathrm{nn}}(v,u) = I(\lfloor v \rfloor, \lfloor u \rfloor )
    \end{equation}

    where $\lfloor \;\;\; \rfloor$ denotes the ``floor" operator.
\end{definition}

Nearest-neighbor interpolation is analogous to a ``binning" operation, where each rotated point $[v,u]^\top$ is assigned to the pixel in $I$ containing it.

% add dependence of rotation function to angle theta

Using the above premise, we define the symmetry metric used in this work in Definition \ref{def:psi_I}, building upon Definitions \ref{def:chi} and \ref{def:I_ref}.

\begin{definition}
    \label{def:chi}
    Let $I_a \in \mathbb{R}^{M\times N}$ and $I_b \in \mathbb{R}^{M\times N}$ be two images. The 2D correlation coefficient $\chi$ between $I_a$ and $I_b$ is defined as:

\begin{equation}
\label{eq:chi}
    \chi(I_a,I_b) = \dfrac{\sum_{m=1}^{N}\sum_{n=1}^{N}(I_{a,mn}-\bar{I}_a)(I_{b,mn}-\bar{I}_b) }{\sqrt{\left(\sum_{m=1}^{N}\sum_{n=1}^{N} (I_{a,mn}-\bar{I}_a)^2 \right)\left(\sum_{m=1}^{N}\sum_{n=1}^{N} (I_{b,mn}-\bar{I}_b)^2 \right)}}
\end{equation}

where $\bar{I}_a$ and $\bar{I}_b$ are the average pixel intensities for $I_a$ and $I_b$, respectively, such that:

\begin{equation}
    \bar{I} = \dfrac{1}{N^2} \sum_{m=1}^{N}\sum_{n=1}^{N} I_{mn}
\end{equation}
\end{definition}

\begin{definition}
    \label{def:I_ref}
    Given an image $I \in \mathbb{R}^{N\times N}$, its reflection about the vertical axis is an image $\underline{I}\in\mathbb{R}^{N\times N}$ such that

    \begin{equation}
        \underline{I}(m,n) = I(m,N-n+1),\,m,n=1,\dots,N
    \end{equation}
\end{definition}

\begin{definition}
    \label{def:psi_I}
    Given an image $I\in \mathbb{R}^{N\times N}$, its vertical-symmetry score $\psi(I)$ is given by:

    \begin{equation}
        \psi(I) = \chi(I,\underline{I})
    \end{equation}
\end{definition}
% cite methods to speed up xcorr computation

The rationale behind using $\psi(I)$ for symmetry quantification is that it captures the correlation between the given image $I$ and its vertically-reflected counterpart $\underline{I}$. When $I$ is perfectly symmetric about the vertical axis, it follows from Definition \ref{def:vert_refl_symm} that $I = \underline{I}$, and hence $\psi(I)=1$. But even when $I \neq \underline{I}$, $\psi(I)$ still indicates the amount of vertical reflective symmetry: the value of $\psi$ increases as pixel pairs which are mirrored with respect to the vertical axis are more similar to each other.

Finally, we can write the main result which will be used for symmetry detection in the FPE algorithm.

\begin{theorem}
        \label{th:argmax_alpha}
        Let $I\in\mathbb{R}^{N\times N}$ be an image which exhibits reflective symmetry with respect to the direction $\mathbf{d}_\alpha$ (see Definition \ref{def:refl_symm_alpha}). Further, let $I_\theta$ be the image obtained by applying the rotation $R_\theta$ to $I$ (see Definition \ref{def:I_theta}). Then,

            \begin{equation}
        \alpha = \argmax_{\theta} \psi(I_\theta)
    \end{equation}
    
\end{theorem}

\begin{proof}
    When $\theta = \alpha$, Definition \ref{def:I_theta} implies that the axis of symmetry is oriented along the image vertical. Then, from Equation \ref{eq:chi} and Definition \ref{def:vert_refl_symm}, it is easy to show that $\theta = \alpha \Rightarrow \psi(I_\alpha) = \mathrm{max}(\psi(I_\alpha)) = 1$.
\end{proof}

Theorem \ref{th:argmax_alpha} is a straightforward consequence of vertical reflective symmetry and the correlation-coefficient definition. For an image perfectly symmetrical about $\mathbf{d}_\alpha$, the rotation $R_\alpha$ is the only one to guarantee equality between the image and its vertically reflected version, where $\psi(I_\theta)=1$. This last condition can be achieved for other angles $\theta \neq \alpha$ when the shape exhibits multiple axes of symmetry; some examples are images of a perfect circle, a rectangle, and a hexagon. For irregular objects, however, we consider Theorem \ref{th:argmax_alpha} generally indicative of the true axis of symmetry.

\subsection{Limitations of the Spatial Domain for Symmetry Detection}
\label{sec:limitations_spatial_domain}
While the approach outlined above can be effective when handling perfectly symmetric images or, at most, images containing low levels of noise, symmetry detection in the presence of severe error sources can prove challenging. Shadowing effects and large centerfinding errors often fall into this category. For these cases, the spatial-domain representation of the original images can be inadequate. 

Consider, for example, a co-added silhouette image obtained from a high-sun-phase viewing geometry. If no shadowing were present, the co-added silhouette image would be symmetric about the rotation axis, up to discretization errors. However, the presence of shadows on the actual silhouette can significantly crop the overall shape and reduce the amount of symmetry in the image. The symmetry score $\psi$ evaluated about the true rotation axis would then be significantly lower, due to the increased number of shadowed pixels.

Furthermore, the axis of symmetry (i.e., the pole projection) passes through the center of mass, symmetry detection in the spatial domain requires the center-of-mass location in the image to be accurately known. Knowledge errors on this location would introduce asymmetries and, in turn, inaccuracies in symmetry detection.

To tackle these practical symmetry-detection challenges, we propose transforming the original images to the frequency domain, using spectral decomposition. This approach is discussed next, in Section \ref{sec:fourier}.

\subsection{Image Spectral Decomposition}
\label{sec:fourier}

As discussed, extracting the symmetric component $\bar{\tilde{x}}^{\mathrm{symm}}_{\nu,[\phi_0,\phi_f]}$ from the overall image can be challenging in general, especially when the non-symmetric error term $\bar{\tilde{e}}_{\nu,[\phi_0,\phi_f]}$ is large (see Theorem \ref{th:x_+_e}). To tackle this problem, we propose transforming the original co-added image $\bar{O}_{\nu,[\phi_0,\phi_f]}$ from the space domain to the frequency domain. Specifically, we use the Discrete Fourier Transform (DFT) to perform spectral decomposition\cite{brigham1988fast}.

\begin{definition}
    \label{def:dft}
    Let $I\in \mathbb{R}^{N\times N}$ be a real-valued image. The Discrete Fourier Transform (DFT), $\mathcal{F}$, of $I$ is the $N\times N$ complex-valued image $F=\mathcal{F}(I),\, F \in \mathbb{C}^{N\times N}$ given by:

\begin{equation}
\label{eq:F_xy}
    F(x,y) = \sum_{m=1}^{N} \sum_{n=1}^{N}I(m,n) \, \mathrm{exp}\left[ -2\pi i \left( \frac{xm}{N} + \frac{yn}{N}\right) \right]
\end{equation}

where $I(m,n)$ is the $mn$-esimal (i.e., $m$-esimal row, $n$-esimal column) element of $I$ and $x,y$ represent the indexes of the frequency components. The DFT amplitude spectrum is an image $A \in \mathbb{R}^{N\times N}$ defined as:

\begin{equation}
    A(x,y) = |F(x,y)|
\end{equation}

The DFT phase spectrum $P \in \mathbb{R}^{N\times N}$ is an image defined as:

\begin{equation}
P(x,y) = \mathrm{arctan}\left( \dfrac{\mathrm{Im}(F(x,y)}{\mathrm{Re}(F(x,y))} \right) 
\end{equation}

where $\mathrm{Re(F)}$ and $\mathrm{Im(F)}$ denote the real and imaginary parts of $F$, respectively.
\end{definition}

The 2D DFT decomposes an image into the sum of complex exponential functions. It is closely related to the 1D DFT, where a 1D signal is broken down into a finite set of sinusoidal functions, each with a different amplitude and phase. In fact, the 2D DFT is equivalent to computing the 1D DFT of each image row to produce an intermediate image, and then computing the 1D DFT of each column of such an intermediate image. (The order of the row-wise and column-wise 1D DFT operations does not matter.) The information content in the frequency-domain image is the same as in the space-domain image and the latter can be obtained from the former by applying the so-called inverse DFT.

The DFT has the following important properties, which we will use for our development and are reported here without proof.

\begin{lemma}
\label{lemma:dft_linearity}
The DFT is a linear operator. That is, given two images $I_1,I_2\in\mathbb{R}^{N\times N}$ and two parameters $\zeta_1,\zeta_2\in\mathbb{R}^2$, we have:

\begin{equation}
    \mathcal{F}(\zeta_1 I_1 + \zeta_2 I_2) = \zeta_1 F_1 + \zeta_2 F_2
\end{equation}

where $F_1 = \mathcal{F}(I_1)$ and $F_2 = \mathcal{F}(I_2)$.
\end{lemma}

\begin{lemma}
    \label{lemma:conj_symm}
    
        Let $I\in \mathbb{R}^{N\times N}$ be a real-valued image. Then, $F$ exhibits conjugate symmetry, i.e.:

    \begin{equation}
        F(x,y) = F^*(-x,-y),\forall \; x,y
    \end{equation}

    where $F^*$ denotes the complex conjugate of $F$.
\end{lemma}

\begin{corollary}
     \label{cor:A_symm}
     Let $I\in \mathbb{R}^{N\times N}$ be a real-valued image and let $A = |\mathcal{F}(I)|$ be its DFT amplitude spectrum. Then, $A$ exhibits central symmetry, i.e.:

     \begin{equation}
        A(x,y) = A(-x,-y),\forall \; x,y
    \end{equation}
\end{corollary}

\subsubsection{Amplitude Spectrum for Symmetry Detection}

In this work, we use the DFT amplitude spectrum (Definition \ref{def:dft}) of the co-added silhouette image to identify the direction of maximum symmetry, i.e., the pole projection onto the camera plane. One known property of amplitude spectra is that they preserve the symmetry of the original image, as stated in the following lemma.

\begin{lemma}
    \label{lemma:A_symm}
    Let $I\in\mathbb{R}^{N\times N}$ be an image with vertical reflective symmetry (Definition \ref{def:vert_refl_symm}). Then, its DFT amplitude spectrum $A$ also exhibits vertical reflective symmetry, i.e.:

\begin{equation}
    A(m,n) = A(m,N-n+1)
\end{equation}
\end{lemma}

Notice that the symmetry described in Lemma \ref{lemma:A_symm} is a different effect than from conjugate symmetry (Lemma \ref{lemma:conj_symm}).

\subsubsection{Advantages of Using Amplitude Spectra for Symmetry Detection}

There are two key advantages in using amplitude spectra instead of space-domain images for symmetry detection:

\begin{enumerate}
    \item The Fourier transform is a global analysis of the image, considering the image information as a whole. This makes frequency-domain images more suitable to detect global symmetries that may be less apparent in the space domain. This is especially true for imagery corrupted by error sources, such as shadowing, which severely affect the level of symmetry in the space domain, as discussed in Section \ref{sec:limitations_spatial_domain}.
    \item The DFT amplitude spectrum is invariant to translation, as stated by Lemma \ref{th:shift}. This property greatly simplifies the symmetry-detection process as the exact image coordinates of the rotating object's center of mass need not be known. Conversely, evaluating symmetry in the space domain requires knowing such a reference point accurately. This property is sometimes referred to as the Shift Theorem (Theorem \ref{th:shift}), reported here without proof\footnote{The phase spectrum is affected by image translation and hence does not exhibit the same robustness to translation.}.
\end{enumerate}

\begin{theorem}
    \label{th:shift}
    Let $I_{\mathbf{t}}\in \mathbb{B}^{N\times N}$ be the binary image obtained by applying a circular-shift translation $\mathbf{t}=[t_u,t_v]^\top \in \mathbb{Z}^2$ to the image $I\in \mathbb{B}^{N\times N}$, such that

    \begin{equation}
    \label{eq:I_t}
        I_\mathbf{t}(m, n) = I\left((m - t_v) \, \mathrm{mod} \, N, (n - t_u) \, \mathrm{mod} \, N \right)
    \end{equation}
    
    Let $A$ and $A_\mathbf{t}$ be the DFT amplitude spectra of $I$ and $I_\mathbf{t}$, respectively. Then, $A = A_\mathbf{t}$.
\end{theorem}

Observe that the circular-shift definition in Equation \ref{eq:I_t} is such that the pixels shifted beyond the image boundaries reappear on the opposite side, preserving the cyclical nature of the signal.

The Shift Theorem is particularly relevant for space imagery, for two reasons. First, for this application, the exact center-of-mass image coordinates are typically unknown. Second, pixels close to the image boundary are typically part of the background and do not contain the silhouette. Hence, assuming that their intensity is zero, the overall image structure is not affected by circular shifts.

Furthermore, the DFT of an image can be computed by efficient algorithms, such as the Fast Fourier Transform (FFT)\cite{brigham1988fast}, and hence does not represent a computational bottleneck.

\subsubsection{Symmetry Detection with Noisy Images}

We can now derive results for image symmetry detection in the presence of error sources. We will rely on the symmetry score in Definition \ref{def:psi_I} and the image error model obtained in Theorem \ref{th:x_+_e}.

\begin{lemma}
\label{lemma:A_symm_plus_blob}
    Let $I\in\mathbb{R}^{N\times N}$ be an image which can be written as 

    \begin{equation}
        I = X + E
    \end{equation}
    
    where $X \in \mathbb{R}^{N\times N}$ is the signal image and exhibits reflective symmetry with respect to the angle $\alpha$, whereas $E\in \mathbb{R}^{N\times N}$ is a noise image, according to Theorem \ref{th:x_+_e}. Further, let $F = \mathcal{F}(I)$ be the DFT of $I$ and $A=|F|$. Then, the squared amplitude spectrum $A^2$ can be written as

    \begin{equation}
        A^2 = A^2_{X} + A^2_N
    \end{equation}

    where $A^2_{X}$ is symmetric with respect to $\alpha$ and $A^2_N$ is the noise component in the frequency domain.
    
\end{lemma}

\begin{proof}

    From linearity of the Fourier transform (Lemma \ref{lemma:dft_linearity}), we have:

    \begin{align}
        F &= \mathcal{F}(X + E)\\
        &= \mathcal{F}(X) + \mathcal{F}(E)\\
        &= F_{X} + F_E
    \end{align}

    Using the properties of complex numbers, the squared amplitude spectrum $A^2$ can be rewritten as:

    \begin{align}
        A^2 &= |F_{X} + F_E|^2\\
        &= \left[\mathrm{Re}(F_{X} + F_E)\right]^2 + \left[\mathrm{Im}(F_{X} + F_E)\right]^2\\
        &= \left[\mathrm{Re}(F_{X})\right]^2 + \left[\mathrm{Re}(F_E)\right]^2 + 2\mathrm{Re}(F_{X})\mathrm{Re}(F_E) \nonumber \\
        & \quad + \left[\mathrm{Im}(F_{X})\right]^2 + \left[\mathrm{Im}(F_E)\right]^2 + 2\mathrm{Im}(F_{X})\mathrm{Im}(F_E)\\
        &= |F_{X}|^2 + |F_E|^2 + 2\left( \mathrm{Re}(F_{X})\mathrm{Re}(F_E) + \mathrm{Im}(F_{X})\mathrm{Im}(F_E) \right)\\
        &= A^2_{X} + A^2_N
    \end{align}

    where

    \begin{equation}
        A^2_{X} = |F_X|^2
    \end{equation}

    and

    \begin{equation}
         A^2_N = |F_E|^2 + 2\left( \mathrm{Re}(F_{X})\mathrm{Re}(F_E) + \mathrm{Im}(F_{X})\mathrm{Im}(F_N) \right)
    \end{equation}

    Since $X$ is symmetric, from Lemma \ref{lemma:A_symm} we know that its amplitude spectrum --- and hence the squared spectrum $A^2_{X}$ --- is also symmetric.
\end{proof}

Lemma \ref{lemma:A_symm_plus_blob} reveals the the additive relationship between signal ($A^2_{X}$) and noise ($A^2_N$) also manifests in the frequency domain. The noise term $A^2_N$ represents the totality of image data corrupting the symmetry of $A^2_{X}$. It is caused by both the original noise image $E$ and the coupling between signal and noise images, $X$ and $E$.

We use the following assumption to define the impact of noise on symmetry detection, in the frequency domain.

\begin{assumption}
\label{assumption:const_xcorr}
    Given the squared amplitude spectrum model $A^2 = A^2_{X} + A^2_N$ presented in Lemma \ref{lemma:A_symm_plus_blob}, the noise pixels $A^2_N(m,n)$ are random variables following a distribution with mean $\widebar{A^2_N}=\frac{1}{N}\sum_{m=1}^N\sum_{n=1}^N A^2_N(m,n)$. Furthermore, the variables $A^2_N(m,n)$ exhibit no reflective symmetry, in the sense that

\begin{equation}
        \mathbb{E}\left[ \left(A^2_{\theta,N}(m,n) - \widebar{A^2_N} \right) \left(\underline{A^2_{\theta,N}(m,n)} - \widebar{A^2_N} \right) \right] \approx 0,\; m,n=1\dots,N,\; \theta=[0,2\pi)
    \end{equation}
    
where $A^2_{\theta,N}$ is the image obtained by rotating $A^2_N$ by an angle $\theta$  (Definition \ref{def:I_theta}) and $\underline{A^2_{\theta,N}}$ is the image obtained by reflecting $A^2_{\theta,N}$ about the vertical axis (Definition \ref{def:I_ref}).
\end{assumption}

Assumption \ref{assumption:const_xcorr} is justified by the fact that the noise amplitude spectrum $A^2_N$ encodes the frequency decomposition of the error image $E$. The latter is given by the combined effect of irregular shadowing and centerfinding errors over multiple frames, which are typically characterized by a high level of randomness and the lack of strong reflective symmetries. These traits are reflected into the frequency domain, where the random components tends to spread across the frequency spectrum. This assumption becomes invalid if the object's co-added silhouette image $\bar{O}_{\nu,[\phi_0,\phi_f]}$ (Definition \ref{def:O_bar}) contains symmetric structures which are not themselves symmetric about the pole. Examples are segments resulting from co-adding frames and appearing only on one side of the co-added silhouette (e.g., due to shadowing making them invisible on the opposite side) or a terminator line --- the line separating the lit and shadowed pixels in the image --- which remains rectilinear through all the observed frames. In practice, the shape of irregular objects renders these effects minor or negligible, especially when the rotation is tracked over multiple frames and instantaneous symmetries in any given frame are more likely to be dispersed.

Using the mathematical framework previously described, we can finally write the main result for symmetry detection using the amplitude spectra in the presence of noise.

\begin{theorem}
    \label{th:alpha_argmax_noise}
    Let $I = X + E,\,I\in\mathbb{R}^{N\times N}$ be a signal-and-noise image and let $A^2 = A^2_{X} + A^2_N$ be the corresponding squared amplitude spectra, according to Lemma \ref{lemma:A_symm_plus_blob}. Then,

    \begin{equation}
    \label{eq:alpha_argmax}
        \alpha = \argmax_\theta \mathbb{E}\left[\psi(A^2_\theta)\right]
    \end{equation}

    where $A^2_\theta$ is the image obtained by rotating $A^2$ by an angle $\theta$ (Definition \ref{def:I_theta}).
\end{theorem}

\begin{proof}
    From Lemma \ref{lemma:A_symm_plus_blob}, the symmetry score $\psi$ in Equation \ref{eq:alpha_argmax} can be written as:

    \begin{align}
        \psi(A^2_\theta) &= \psi(A^2_{\theta,X} + A^2_{\theta,N})\\
        &= \chi(A^2_{\theta,X} + A^2_{\theta,N},\underline{A^2_{\theta,X}} + \underline{A^2_{\theta,N}}) \label{eq:psi_2}
    \end{align}
    
    We are interested in studying $\mathbb{E}\left[\psi(A^2_\theta)\right]$ as a function of $\theta$. Applying Definition \ref{def:psi_I}, we have:

    \begin{align}
    \label{eq:psi_A^2_theta}
        \psi(A^2_\theta) &= \dfrac{1}{\xi_{X,N}} \sum_{m=1}^N \sum_{n=1}^N \left(A^2_{\theta,X}(m,n) + A^2_{\theta,N}(m,n) - \widebar{A_\theta^2} \right) \nonumber \\ 
        & \quad\quad\quad\quad\quad\quad \left( \underline{A^2_{\theta,X}}(m,n) + \underline{A^2_{\theta,N}}(m,n) - \widebar{A_\theta^2} \right)
    \end{align}

    where $\xi_{X,N} \in \mathbb{R}$ is the cross-correlation denominator in Equation \ref{eq:chi} --- which does not vary with $\theta$ and hence is considered a constant here --- and $\widebar{A_\theta^2}$ is the mean pixel intensity of $A_\theta$ and $\widebar{A_\theta}$. From linearity, $\widebar{A_\theta^2}$ can be written as

    \begin{equation}
        \widebar{A_\theta^2} = \widebar{A_{\theta,X}^2} + \widebar{A_{\theta,N}^2}
    \end{equation}
    
    where $\widebar{A_{\theta,X}^2}$ and $\widebar{A_{\theta,N}^2}$ are the mean pixel intensities of $A_{\theta,X}^2$ and $A_{\theta,N}^2$, respectively. If we define the mean-subtracted images $\grave{A}^2_{\theta,X}$ and $\grave{A}^2_{\theta,N}$ such that
    
        \begin{equation}
        \grave{A}^2_{\theta,X}(m,n) =  A^2_{\theta,X}(m,n) - \widebar{A_{\theta,X}^2}
    \end{equation}

    \begin{equation}
        \grave{A}^2_{\theta,N}(m,n) =  A^2_{\theta,N}(m,n) - \widebar{A_{\theta,N}^2}
    \end{equation}
    
then, Equation \ref{eq:psi_A^2_theta} can be rewritten as

    \begin{align}
        \psi(A^2_\theta) &= \dfrac{1}{\xi_{X,N}} \sum_{m=1}^N \sum_{n=1}^N \left[\grave{A}^2_{\theta,X}(m,n) + \grave{A}^2_{\theta,N}(m,n) \right] \left[ \underline{ \grave{A}^2_{\theta,X}}(m,n) + \underline{\grave{A}^2_{\theta,N}}(m,n) \right]\\
        &= \dfrac{1}{\xi_{X,N}} \sum_{m=1}^N \sum_{n=1}^N \left[ \grave{A}^2_{\theta,X}(m,n) \underline{\grave{A}^2_{\theta,X}}(m,n) + \grave{A}^2_{\theta,X}(m,n) \underline{\grave{A}^2_{\theta,N}}(m,n) + \nonumber \right. \\
        & \quad\quad\quad\quad\quad\quad \left. \grave{A}^2_{\theta,N}(m,n) \underline{\grave{A}^2_{\theta,X}}(m,n) + \grave{A}^2_{\theta,N}(m,n) \underline{\grave{A}^2_{\theta,N}}(m,n) \right]
    \end{align}

Using the linearity of expectation, we can write:

\begin{align}
\label{eq:E[psi]}
    \mathbb{E}\left[ \psi(A^2_\theta) \right] &= \dfrac{1}{\xi_{X,N}} \sum_{m=1}^N \sum_{n=1}^N \mathbb{E}\left[ \grave{A}^2_{\theta,X}(m,n) \underline{\grave{A}^2_{\theta,X}}(m,n) \right] + \mathbb{E}\left[ \grave{A}^2_{\theta,X}(m,n) \underline{\grave{A}^2_{\theta,N}}(m,n) \right] + \nonumber \\
        & \quad\quad\quad\quad\quad\quad \mathbb{E}\left[ \grave{A}^2_{\theta,N}(m,n) \underline{\grave{A}^2_{\theta,X}}(m,n) \right] + \mathbb{E}\left[ \grave{A}^2_{\theta,N}(m,n) \underline{\grave{A}^2_{\theta,N}}(m,n) \right]
\end{align}

The first term is the product between the deterministic signal and its reflected counterpart; hence:

\begin{equation}
    \mathbb{E}\left[ \grave{A}^2_{\theta,X}(m,n) \underline{\grave{A}^2_{\theta,X}}(m,n) \right] = \grave{A}^2_{\theta,X}(m,n) \underline{\grave{A}^2_{\theta,X}}(m,n)
\end{equation}

The second and third terms are the product between the signal and the random-noise component. Since $\underline{\grave{A}^2_{\theta,N}}$ is the mean-subtracted random variable, by definition we have:

\begin{align}
    \mathbb{E}\left[ \grave{A}^2_{\theta,X}(m,n) \underline{\grave{A}^2_{\theta,N}}(m,n) \right] &= \grave{A}^2_{\theta,X}(m,n)\, \mathbb{E}\left[ 
 \underline{\grave{A}^2_{\theta,N}}(m,n) \right] \\
 &= \grave{A}^2_{\theta,X}(m,n) \cdot 0 = 0
\end{align}

\begin{align}
    \mathbb{E}\left[ \grave{A}^2_{\theta,N}(m,n) \underline{\grave{A}^2_{\theta,X}}(m,n) \right] &= \mathbb{E}\left[ \grave{A}^2_{\theta,N}(m,n) \right] \underline{\grave{A}^2_{\theta,X}}(m,n) \\
 &= 0 \cdot \grave{A}^2_{\theta,X}(m,n) = 0
\end{align}

Lastly, the fourth term is the product between the random-noise component and its reflected counterpart. Using Assumption \ref{assumption:const_xcorr}, we can write:

\begin{equation}
    \mathbb{E}\left[ \grave{A}^2_{\theta,N}(m,n) \underline{\grave{A}^2_{\theta,N}}(m,n) \right] = 0
\end{equation}

Combining the expectation results from each term, Equation \ref{eq:E[psi]} becomes:

\begin{align}
\label{eq:exp_psi}
    \mathbb{E}\left[ \psi(A^2_\theta) \right] &= \dfrac{1}{\xi_{X,N}} \sum_{m=1}^N \sum_{n=1}^N \grave{A}^2_{\theta,X}(m,n) \underline{\grave{A}^2_{\theta,X}}(m,n)\\
    &= \dfrac{\xi_X}{\xi_{X,N}} \psi(A_{\theta,X}^2)
\end{align}

where $\xi_X \in \mathbb{R}$ is the cross-correlation denominator in the expression of $\psi(A_{\theta,X}^2)$. The term $\psi(A_{\theta,X}^2)$ is the symmetry score of the zero-noise signal $A_{\theta,X}^2$. From Theorem \ref{th:argmax_alpha}, we know that $\psi(A_{\theta,X}^2)$ is maximum when the image is rotated and reflected about the axis of symmetry, i.e.:

\begin{equation}
    \alpha = \argmax_\theta \psi(A_{\theta,X}^2)
\end{equation}

Since $\xi_X/\xi_{X,N}$ is a constant, Equation \ref{eq:exp_psi} is also maximized for $\theta=\alpha$.
\end{proof}

Theorem \ref{th:alpha_argmax_noise} provides the key principle to detect the axis-of-symmetry direction --- and hence the pole --- using silhouette images. It demonstrates the robustness of using frequency-domain images to error sources.

However, there is one more phenomenon affecting symmetries in the amplitude spectrum which introduces some level of ambiguity. This effect is discussed in the next section.

\subsubsection{Pole Ambiguities}
\label{sec:pole_ambig}

Reflective symmetry about the direction $\mathbf{d}_\alpha$ does not provide information on the direction of rotation. In other words, the image of interest (e.g., the DFT amplitude spectrum) is equally symmetric about the direction $\mathbf{d}_\alpha$ and $\mathbf{d}_{\alpha \pm \pi}$, which creates ambiguity in the pole direction.

When dealing with the DFT amplitude spectrum, there is an additional ambiguity to consider. As stated in Corollary \ref{cor:A_symm}, the amplitude-spectrum image is always characterized by central symmetry with respect to the image center. If the amplitude spectrum is also symmetric about an axis $\mathbf{d}_\alpha$, then the combined effect of the two symmetries produces another symmetry axis in the image, at an angle $\alpha \pm \frac{\pi}{2}$. Thus, the amplitude spectrum also exhibits symmetry about both $\mathbf{d}_\alpha$ and $\mathbf{d}_{\alpha \pm \frac{\pi}{2}}$. The combined effect of central and reflective symmetries for the amplitude spectrum can be formalized as follows.







\begin{theorem}
    \label{th:double_symm}
    Let $I\in \mathbb{R}^{N\times N}$ be an image which exhibits reflective symmetry with respect to the direction $\mathbf{d}_\alpha$, such that $I = \underline{I_\alpha}$, according to Definition \ref{def:refl_symm_alpha}. Then, $A=|\mathcal{F}(I)|$ exhibits reflective symmetry with respect to the angles $\alpha \pm k \frac{\pi}{2},\, k \in \mathbb{Z}$.
\end{theorem}

\begin{proof}
    By definition, if $I$ exhibits reflective symmetry with respect to $\mathbf{d}_\alpha$, then reflective symmetry holds for the opposite direction $\mathbf{d}_{\alpha \pm \pi}$, which proves the angular ambiguity $\pm \pi$. The additional ambiguity $\pm \frac{\pi}{2}$ can be proven as follows. Let $[u_n,v_m]^\top$ and $[\underline{u}_n,\underline{v}_m]^\top$ be the coordinates of the pixels $I(m,n)$ and $I(\underline{m},\underline{n})$, whose location is reflected about $\mathbf{d}_\alpha$ (Definition \ref{def:refl_symm_alpha}). Suppose that $I$ exhibits reflective symmetry about $\mathbf{d}_\alpha$. Then, from reflective symmetry of the amplitude spectrum (Lemma \ref{lemma:A_symm}), we have $A(m,n)=A(\underline{m},\underline{n})$. However, conjugate symmetry of the DFT (Corollary \ref{cor:A_symm}) also implies that $A(\underline{m},\underline{n})=A(-\underline{m},-\underline{n})$. But using reflective symmetry once again, it follows that
    
    \begin{align}
        A(-\underline{m},-\underline{n})&=A(-\underline{\underline{m}},-\underline{\underline{n}})\\
        &=A(-m,-n)\\
        &=A(m,n)
    \end{align}

    where $(-\underline{\underline{m}},-\underline{\underline{n}})$ denotes the pixel indexes obtained by reflecting the indexes $(-\underline{m},-\underline{n})$ about $\mathbf{d}_\alpha$. By construction, one can see that $(-\underline{m},-\underline{n})$ are the pixel indexes obtained by reflecting $(m,n)$ about the axis $\mathbf{d}_{\alpha \pm \frac{\pi}{2}}$. 
\end{proof}

The above effects combined lead to a four-fold ambiguity in the pole direction. This ambiguity may be resolved using existing image-processing methods, such as dense optical flow or feature tracking\cite{farneback2003two}. These methods estimate the motion of pixels, or visual features, between consecutive frames. Then, one can determine which pole hypothesis better explains the extracted directions of motion. Dense optical flow can be more effective for low-resolution images, whereas feature-based methods can be preferable for higher resolutions.

If available, a-priori information of the pole direction can also be used to resolve the ambiguity. Lastly, one could keep track of multiple pole hypotheses through the pole-estimation process described next, at the cost of a slightly increased computation. In the following, we will assume that the pole has been disambiguated.

% In this work, we propose a pole-disambiguation heuristic based on the energy distribution in the amplitude spectrum of the co-added silhouette. Define the image energy as the squared pixel value of the amplitude spectrum, i.e., $A^2(m,n)$. Then, we argue that the image energy distributed around the true-pole axis is often greater than that around its orthogonal counterpart. The key principle is that, if we neglect imaging error sources, the motion of each surface point $\mathbf{p}\in\mathcal{V}$ is observed in the camera plane as an arc of an ellipse, where the ellipse is oriented about the pole direction. Thus, the co-added silhouette images can be interpreted as the superposition of individual ellipse arcs, where the underlying ellipses share the same eccentricity, as they are projected onto the same image plane. It can be shown that, given the image of an ellipse with nonzero eccentricity, the corresponding energy distribution about the ellipse's minor-axis direction is grater than the energy distribution about the ellipse's major-axis. In our scenario of interest, the ellipses' minor axis is parallel with the pole direction, which can then be determine based on its energy content relative to the direction orthogonal to the pole. This energy difference does not hold when the observer is located at a polar latitude ($\lambda=\dfrac{\pi}{2}$), in which case the points motion appears as a circle.

% In practice, shadowing and centerfinding errors can play a significant role in the overall energy distribution throughout the image, hence the above property cannot be proven in general. However, we empirically show that it is effective for many image sets. The implementation of this pole-disambiguation step is discussed in Section [].

\subsection{Pole Triangulation}

In previous sections, we described the symmetry-detection theory to estimate the pole projection onto a given camera plane. The pole-projection estimate is represented by an angle $\alpha$, associated with its corresponding hovering-camera view $\nu=(\rho,\lambda)$. However, the pole projection does not fully resolve the 2-degree-of-freedom pole orientation $\boldsymbol{\omega}$. In this work, we propose leveraging orientation changes in the camera boresight axis --- or equivalently, in the camera plane --- to resolve the pole direction, a process we call \textit{pole triangulation}. By combining multiple in-plane pole angles $\alpha$, which are treated as pole-direction measurements, $\boldsymbol{\omega}$ can be inferred. To support this approach, we use the argument that, when approaching irregular objects in space, the camera latitude is very often subject to change. The latitude change facilitates changes the camera plane. Thus, the observer's motion often creates the conditions required for pole triangulation.

We formulate pole triangulation as a least-squared problem, described in the following.

\begin{theorem}
    \label{th:pole_triang}
    Let $\alpha_1, \dots, \alpha_\aleph$ be a set pole-projection angles corresponding to the camera views $\nu_1, \dots, \nu_\aleph$, such that

    \begin{equation}
    \label{eq:cos_alpha}
        \mathrm{cos}(\alpha_j) = \boldsymbol{\omega}^\top \mathbf{i}_{c,j}, \; \mathrm{sin}(\alpha_j) = \boldsymbol{\omega}^\top \mathbf{j}_{c,j}, \; j = 1,\dots,\aleph
    \end{equation}

where $\mathbf{i}_{c,j}$ and $\mathbf{j}_{c,j}$ are the camera horizontal and vertical axes, respectively. Then, the pole direction estimate $\hat{\boldsymbol{\omega}}$ can be determined by solving the least-squares problem:

    \begin{equation}
    \label{eq:pole_triang}
        \begin{bmatrix}
            \mathbf{i}_{c,1}^\top \\
            \mathbf{j}_{c,1}^\top \\
            \vdots \\
            \mathbf{i}_{c,\aleph}^\top\\
            \mathbf{j}_{c,\aleph}^\top \\
        \end{bmatrix} \boldsymbol{\omega} = 
        \begin{bmatrix}
            \mathrm{cos}(\alpha_1) \\
            \mathrm{sin}(\alpha_1)\\
            \vdots \\
            \mathrm{cos}(\alpha_\aleph) \\
            \mathrm{sin}(\alpha_\aleph)\\
        \end{bmatrix}
    \end{equation}
    
\end{theorem}

\begin{corollary}
    Given a camera view $\nu_j=(\rho_j,\lambda_j)$, the corresponding in-plane angle $\alpha_j$ is invariant to $\rho_j$.
\end{corollary}

The least-squares problem in Equation \ref{eq:pole_triang} does not incorporate the constraint that $||\boldsymbol{\omega}||=1$, and hence may not provide the optimal solution. Empirically, we verify that this formulation provides sufficiently accurate pole-direction estimates. In future work, we will incorporate the unit-norm constraint into the pole-triangulation formulation.

% emphasize that triangulation performance only depends on camera attitude changes

\section{Algorithm Overview}

Building upon the results from Section \ref{sec:theor_dev}, the Fourier Pole Estimation (FPE) algorithm is described in the following. A set of image batches $\{ \mathcal{I}_{\nu_1}, \dots, \mathcal{I}_{\nu_\aleph} \}$ is given, where the $j$-th batch contains images acquired from a hovering-camera view $\nu_j=(\rho_j,\lambda_j)$ and a set of longitudes $\{ \phi_{j,1}, \dots, \phi_{j,M_j} \}$, according to Equation \ref{eq:I_nu_j}.

Suppose that, for each image observation $I_{\nu_j,\phi_{j,k}}\in\mathbb{R}^{N\times N}$, the corresponding silhouette foreground $O_{\nu_j,\phi_{j,k}}\in\mathbb{B}^{N\times N}$ (Definition \ref{def:O_bar}) has been extracted. Then the PFE algorithm is divided into two phases:

\begin{enumerate}
    \item For each image batch $\mathcal{I}_{\nu_j}$, estimate the corresponding in-plane pole projection angle $\hat{\alpha}_j$, using Algorithm \ref{alg:in-plane}.
    \item Given the extracted in-plane angles $\{ \hat{\alpha}_1, \dots, \hat{\alpha}_\aleph \}$, perform pole triangulation to estimate the pole direction $\hat{\boldsymbol{\omega}}$ (by solving Equation \ref{eq:pole_triang} in a least-squares sense).
\end{enumerate}

\begin{algorithm}
\caption{In-plane Angle Estimation}
\label{alg:in-plane}
\begin{algorithmic}[1]
\Require $O_{\nu,\phi_1}, \dots, O_{\nu,\phi_M},\, O_{\nu,\phi_k}\in\mathbb{B}^{N\times N}$ \Comment{Observed silhouette images from camera view $\nu_j$ (Definition \ref{def:O_bar})}
\Require $\tau\in\mathbb{R}$ \Comment{Cutoff frequency for the DFT amplitude spectrum (Equation \ref{eq:low_pass_filt})}
\Require $\Theta=\{ \theta_1, \dots, \theta_{N_\theta} \},\, \theta_\iota\in\mathbb{R}$ \Comment{Query angles for symmetry evaluation}
\Ensure $\hat{\alpha}$ \Comment{Estimated in-plane pole angle}
\State $\bar{O}_{\nu,[\phi_1,\phi_M]}(m,n) \gets \sum_{k=1}^M O_{\nu,\phi_k}(m,n),\, m,n = 1,\dots,N$ \Comment{Co-add silhouette image}
\State $A \gets |\mathcal{F}(\bar{O}_{\nu,[\phi_1,\phi_M]})|$ \Comment{Compute DFT amplitude spectrum (Definition \ref{def:dft})}
\State $A_\mathrm{filt} \gets \mathcal{T}_\tau(A)$ \Comment{Apply low-pass filter (Equation \ref{eq:low_pass_filt})}
\State $E_\mathrm{filt} \gets \mathrm{log}(1 + A_\mathrm{filt}^2)$ \Comment{Image compression} \label{state:log}
\For{$\iota = 1,\dots,N_{\theta}$}
    \State $E_{\mathrm{filt},\theta}(m,n) \gets \mathcal{R}_{E_\mathrm{filt},\theta}(m,n),\ m,n=1,\dots,N$ \Comment{Apply image rotation (Definition \ref{def:I_theta})}
    \State $\beta_k \gets \psi(E_{\mathrm{filt},\theta})$ \Comment{Evaluate and log symmetry score (Definition \ref{def:psi_I})}
\EndFor
\State $\hat{\alpha} \gets \argmax_\Theta \{ \beta_1, \dots, \beta_{N_\theta} \}$

\end{algorithmic}
\end{algorithm}

% theta between 0 and pi suffice due to conjugate symmetry

Algorithm \ref{alg:in-plane} leverages the symmetry properties of the co-added silhouette image $\bar{O}_{\nu,[\phi_1,\phi_M]}(m,n)$ and its DFT amplitude spectrum, described in Section \ref{sec:theor_dev}. The inputs for the procedure are a set of silhouette images $O_{\nu,\phi_k}$, a set of query angles $\Theta = \{ \theta_1, \dots, \theta_{N_\theta} \}$ to evaluate image symmetry at, and a low-pass filter cutoff frequency $\tau$, described below. First, individual silhouette images are co-added to obtain the co-added image $\bar{O}_{\nu,[\phi_1,\phi_M]}(m,n)$. The latter is then transformed to the frequency domain using DFT, and its amplitude spectrum $A$ is evaluated. At this point, we include an image-compression step, based on a low-pass filter, to increase the symmetry signal and remove high-frequency noise components in the amplitude spectrum. The low-pass filter is a function $\mathcal{T}_\tau:\mathbb{R}^{N\times N} \rightarrow \mathbb{R}^{N\times N}$ which, given an amplitude spectrum, returns its filtered version such that:

\begin{equation}
\label{eq:low_pass_filt}
    \mathcal{T}_\tau(x,y) =
    \begin{cases}
        A(x,y) & \text{if} \; \lVert [x,y]^\top \rVert < \tau \\
        0 & \text{otherwise}
    \end{cases}
\end{equation}

where $\tau \in \mathbb{R}$ is the cut-off frequency parameter.

To evaluate symmetry, we compress the dynamic range of the original squared amplitude spectrum by applying a logarithmic transformation (see State \ref{state:log} in Algorithm \ref{alg:in-plane}). This step enhances symmetric patterns within the amplitude spectrum, which we observe is critical to improve symmetry-detection performance. Note that this is a monotonic transformation with the original squared amplitude spectrum, hence the symmetry properties previously discussed still hold. Finally, the symmetry score is then evaluated along the input query angles; the angle yielding maximum symmetry score is returned as the in-plane angle estimate.

% comment on choice of cutoff frequency
% comment on choice of image-rotation interpolation method
% define x,y vs m,n

\section{Experimental Results}
\label{sec:results}

\subsection{In-plane Angle Estimation Results}
\label{sec:in_plane_est}

Estimation performance for the in-plane pole-estimation algorithm is assessed using numerical simulations. Synthetic images are generated by simulating a constant-latitude hovering camera observing a small celestial body, rotating about its pole for a full rotation. Images are simulated using the small body-rendering tool based on Blender Cycles, presented in Chapter \ref{chapter:rendering}. The simulation setup is outlined in Table \ref{tab:params_images}. We generate two image sets corresponding to two celestial bodies with different morphology: asteroid Bennu \cite{lauretta2019unexpected} and comet 67P Churyumov-Gerasimenko\cite{thomas2015morphological}. While the global shape of Bennu is rather symmetrical and diamond-like, comet 67P exhibits a bilobed, asymmetrical structure, which causes self-shadowing artifacts onto its surface.

We choose a high sun-phase angle to test the algorithm under challenging lighting conditions, where shadowing significantly reduces the amount of symmetry in the co-added silhouette image. To simulate a generic approach scenario and reduce the number of case-specific simulation parameters (e.g., camera focal length), we assume an orthographic projection, i.e., that the camera is infinitely far from the imaged body. Two example images from the simulated sets are shown in Figure \ref{fig:img_0}.

The cutoff-frequency for symmetry detection (see Algorithm \ref{alg:in-plane}) is chosen as $\tau=100$, i.e., the circular region around the image center with radius 100 pixels is used, while the other pixel values are set to zero (Equation \ref{eq:low_pass_filt}). The rationale is to retain the low-frequency, higher-energy components, which typically exhibit a higher level of symmetry, while rejecting the often more noisy higher frequencies.

\begin{table}[]
    \centering
    \caption{Simulation setup for in-plane pole estimation}
    \begin{tabular}{|c|c|c|}
        \hline
        Parameter & Value & Description\\
        \hline
        Imaged bodies & Bennu, 67P & Different Shapes\\
        $[\phi_0,\phi_f]$ & $[0,360]\,\mathrm{deg}$ & Camera longitude range\\
        $\phi_k-\phi_{k-1}$ & $1\,\mathrm{deg}$ & Camera-longitude Increments\\
        $\lambda$ & $14\,\mathrm{deg}$ & Camera latitude \\
        Sun Phase & $90\,\mathrm{deg}$ & Challenging case \\
        $\alpha$ & $20\,\mathrm{deg}$ & True in-plane pole angle\\
        Image resolution & $1024\times 1024\,\mathrm{pixels}$ & \\
        Camera projection & Orthographic & Simulating the approach phase\\
        $\theta_\iota - \theta_{\iota-1}$ & $1\,\mathrm{deg}$ & Query-angle increments (see Algorithm \ref{alg:in-plane})\\
        $\tau$ & 100 & DFT cutoff frequency\\
        \hline
    \end{tabular}
    \label{tab:params_images}
\end{table}

\begin{figure}[t!]
    \centering
    \includegraphics[width=0.49\textwidth, keepaspectratio]{figs/img_0_Bennu.png}
    \includegraphics[width=0.49\textwidth, keepaspectratio]{figs/img_0_67P.png}
    \caption{First frames of the two simulated image sets. The bodies' silhouettes are severely corrupted by shadowing, due to the high sun phase. Left: asteroid Bennu; right: comet 67P.}
    \label{fig:img_0}
\end{figure}

For each image set, we present results based on two cases: (1) assuming perfect center-of-mass knowledge, referred to as the perfect-centroid case; (2) co-adding silhouettes by centering each frame based on its center of brightness, i.e., the brightness-centroid case. We use a standard moment algorithm to compute the center of brightness\cite{owen2011methods}. Figure \ref{fig:ampl_spectr} shows the co-added silhouette images and the corresponding DFT amplitude spectra obtained for each case. The effect of centerfinding errors is mostly noticeable in the frequency domain. Interestingly, the errors introduced by centerfinding produce a smoothing effect on artifacts in the frequency domain, while preserving the overall symmetry about the pole direction. This phenomenon can be explained by the dilution of sharp edges between foreground and background caused by applying small translation errors to each silhouette. Future work will explore this effect further, e.g., reducing artifacts in the amplitude spectrum by artificially introducing small random translations to each silhouette. Another characteristic of the amplitude spectra is the concentration of energy within lower frequencies, which justifies the use of a cutoff-frequency parameter. In future work, an automatic procedure to select the cutoff frequency $\tau$ could be investigated.

% SNR trade-off with number of images
% results as a function of tau
% camera view nu should define both position and attitude
% comment on artifacts of DFT at k pi/2
% amplitude histogram to disambiguate the pole hypothesis
% plot effect of increasing number of images
% rename "in-plane pole angle" to "pole-projection angle"

\begin{figure}
    \centering
    \includegraphics[width=0.3\textwidth, keepaspectratio]{figs/stacked_silh_Bennu.png}
    \includegraphics[width=0.3\textwidth, keepaspectratio]{figs/ampl_Bennu.png}
    \includegraphics[width=0.3\textwidth, keepaspectratio]{figs/ampl_Bennu_zoom.png}
    \includegraphics[width=0.3\textwidth, keepaspectratio]{figs/stacked_silh_Bennu_centroid.png}
    \includegraphics[width=0.3\textwidth, keepaspectratio]{figs/ampl_Bennu_centroid.png}
    \includegraphics[width=0.3\textwidth, keepaspectratio]{figs/ampl_Bennu_centroid_zoom.png}
    \includegraphics[width=0.3\textwidth, keepaspectratio]{figs/stacked_silh_67P.png}
    \includegraphics[width=0.3\textwidth, keepaspectratio]{figs/ampl_67P.png}
    \includegraphics[width=0.3\textwidth, keepaspectratio]{figs/ampl_67P_zoom.png}
    \includegraphics[width=0.3\textwidth, keepaspectratio]{figs/stacked_silh_67P_centroid.png}
    \includegraphics[width=0.3\textwidth, keepaspectratio]{figs/ampl_67P_centroid.png}
    \includegraphics[width=0.3\textwidth, keepaspectratio]{figs/ampl_67P_zoom_centroid.png}
    \caption{Co-added silhouette images in the space and frequency domain for the different case studies: Bennu, perfect centroid (row 1); Bennu, brightness centroid (row 2); 67P, perfect centroid (row 3); 67P, brightness centroid (row 4). Columns show the co-added silhouette images in terms of the space domain (left), the amplitude spectrum (center), and the magnified amplitude spectrum (right), with a $200\times 200$-pixel window; in this last case, image clipping was perform to facilitat visualization.}
    \label{fig:ampl_spectr}
\end{figure}

\begin{figure}
    \centering
    \includegraphics[width=0.7\linewidth]{figs/FPE_in_plane_est.png}
    \caption{Pole-projection estimates. Error magnitudes span between $1\,\mathrm{deg}$ and $3\,\mathrm{deg}$.}
    \label{fig:in_plane_est}
\end{figure}

Estimation results for the pole-projection angle $\alpha$ are shown in Figure \ref{fig:in_plane_est}. For all cases, the symmetry score $\psi$ peaks in the vicinity of the true pole $\alpha$. The minimum and maximum error magnitudes for the case presented are $1\,\mathrm{deg}$ and $3\,\mathrm{deg}$, respectively.

\subsubsection{Effect of Data Volume}

To study the combined effect of image resolution $N$ and longitudinal range $[\phi_0,\phi_f]$ (or number of images $\aleph$) on performance --- which we refer to as data volume --- we perform two additional brightness-centroid case studies of asteroid Bennu and 67P. In this case, the image resolution is 25\% of the original (reported in Table \ref{tab:params_images}), resulting in $256\times 256$-pixel frames. Further, we reduce the camera longitudinal range such that $[\phi_0,\phi_f]=[0,180]\,\mathrm{deg}$. This time, the cutoff frequency threshold is set to $\tau=126\,\mathrm{pixels}$ to preserve the entire spectrum.

The resulting DFT amplitude spectra are reported in Figure \ref{fig:ampl_resized}. Interestingly, the reduction in data volume enhances global symmetries about the pole projection in the frequency domain by reducing the presence of asymmetric structure, compared to those in Figure \ref{fig:ampl_spectr}. Figure \ref{fig:in_plane_est_resized} shows estimation performance, which is similar to the original case (Figure \ref{fig:in_plane_est}) despite the significant dicrease in data volume.

\begin{figure}
    \includegraphics[width=0.49\textwidth, keepaspectratio]{figs/ampl_Bennu_resized.png}
    \includegraphics[width=0.49\textwidth, keepaspectratio]{figs/ampl_67P_resized.png}
    \caption{Squared amplitude spectra ($E$) for the low-data-volume case. The pole projection and its orthogonal axis are displayed (black segments). Left: Asteroid Bennu; right: comet 67P.}
    \label{fig:ampl_resized}
\end{figure}

\begin{figure}
    \centering
    \includegraphics[width=0.7\linewidth]{figs/FPE_in_plane_est_resized.png}
    \caption{Pole-projection estimates for the low-data-volume case. Error magnitudes are $1\,\mathrm{deg}$ (Bennu) and $0\,\mathrm{deg}$ (67P).}
    \label{fig:in_plane_est_resized}
\end{figure}

\subsection{Pole Triangulation Results}
\label{sec:out_of_plane_est}

We conduct a Monte Carlo simulation whose purpose is twofold: validate the proposed pole-triangulation method and quantify estimation performance as a function of the observation geometry. In this analysis, a set of in-plane angle measurements $\{ \alpha_1, \dots, \alpha_\aleph \}$ is given; the goal is to estimate the pole direction $\boldsymbol{\omega}$, according to Equations \ref{eq:cos_alpha} and \ref{eq:pole_triang}. Performance is analyzed based on the following parameters: (1) the angle between the two camera boresight axes (i.e., z-axis) used for triangulation, referred to as boresight angular displacement, $\beta$; (2) the measurement error for pole-projection measurements $\alpha$, defined by the associated error variance $\sigma_\alpha^2$; (3) the total number of hovering-camera views $\aleph$ used to estimate the pole direction. The boresight angular displacement $\beta_{ij}\in\mathbb{R}$ associated with a pair of camera views is defined as:

\begin{equation}
    \beta_{ij} = \mathrm{cos}^{-1}(\mathbf{k}_{c,i}^\top \mathbf{k}_{c,j})
\end{equation}

where $i,j\in\{1,\dots,\aleph\}$ are the indexes of the $i$-th and $j$-th camera views, used to perform triangulation.

We define the pole-estimation error $\epsilon\in\mathbb{R}$ as the angle between the true and estimated pole direction, i.e.:

\begin{equation}
    \epsilon = \mathrm{cos}^{-1}(\hat{\boldsymbol{\omega}}^\top\boldsymbol{\omega})
\end{equation}

The Monte Carlo simulation setup is the following. A total of $10^5$ Monte Carlo trials are performed. For each $i$-th run, both the true pole direction and the camera axes are randomly sampled from the unit sphere; the only constraint is that each triplet of camera axes form an orthonormal, right-handed basis. Angle measurements are perturbed by randomly sampling errors from a zero-mean, normal distribution with variance $\sigma_\alpha^2$; random errors exceeding the $3\sigma_\alpha$ value are rejected.

Monte Carlo results are shown in Figures \ref{fig:FPE_triang_mc_sigma_1deg}-\ref{fig:FPE_triang_views}. Figure \ref{fig:FPE_triang_mc_sigma_1deg} shows the effect of the boresight angular displacement on pole-triangulation performance, for $\sigma_\alpha=1$. This notional value is representative of the in-plane angle estimation performance discussed in Section \ref{sec:in_plane_est}. Results show that $\epsilon$ exhibits a minimum around $\beta=\frac{\pi}{2}$, where the mean $\epsilon$ approaches $\sigma_\alpha$. This can be explained by the fact that projecting the pole $\boldsymbol{\omega}$ onto two orthogonal camera planes maximizes the observability of the pole direction and makes triangulation errors more isotropic. Conversely, when $\beta$ is close to $0$ or $\pi$, the pole projections onto the two camera planes are similar to each other, hence the observability of the out-of-plane pole component is reduced. Nevertheless, these results also show a distinct knee in the mean-error plot, suggesting that there is diminishing return in estimation accuracy when increasing $\beta$. In the case presented, the mean estimation error approaches the minimum value as early as $\beta \approx 20\,\mathrm{deg}$. 

Figure \ref{fig:FPE_triang_mc_sigmas} reports mean estimation errors parametrized over multiple values of $\sigma_\alpha$. While estimation errors increase with $\sigma_\alpha$ overall, the same performance behavior previously discussed is observed for all $\sigma_\alpha$ values.

\begin{figure}
    \centering
    \includegraphics[width=0.49\textwidth, keepaspectratio]{figs/FPE_triang_mc_sigma_1deg_no_zoom.png}
    \includegraphics[width=0.49\textwidth, keepaspectratio]{figs/FPE_triang_mc_sigma_1deg.png}
    \caption{Pole estimation errors ($\epsilon$) as a function of the boresight angular displacement ($\beta$) from the Monte Carlo simulation, for $\sigma_\alpha=1$. Both individual Monte-Carlo samples (dots) and their binned mean (line) are displayed; the bin size used to compute each mean element is $2\,\mathrm{deg}$. Left: original plot; right: plot magnified around mean values.}
    \label{fig:FPE_triang_mc_sigma_1deg}
\end{figure}

\begin{figure}
    \centering
    \includegraphics[width=0.7\linewidth]{figs/FPE_triang_mc_sigmas.png}
    \caption{Mean pole estimation errors ($\epsilon$) versus the boresight angular displacement ($\beta$) for different values of in-plane-angle measurement uncertainty ($\sigma_\alpha$), from the Monte Carlo simulation.}
    \label{fig:FPE_triang_mc_sigmas}
\end{figure}

Lastly, the effect of number of camera views, $\aleph$, is captured in Figure \ref{fig:FPE_triang_views}. Here, each camera view is independent of the others and is randomly sampled as described above. Results show that increasing $\aleph$ yields a lower error variance and a substantial reduction in the number of outliers, i.e., samples associated with very large $\epsilon$ values.

\begin{figure}
    \centering
    \includegraphics[width=0.7\linewidth]{figs/FPE_triang_views.png}
    \caption{Estimation error histograms for various camera views ($\aleph$), for $\sigma_\alpha=1$, from the Monte Carlo simulation. The histogram horizontal span is cropped at $5\,\mathrm{deg}$ to facilitate visualization. The number of points in the range $(5,180]\,\mathrm{deg}$ is 1061, 30, and 5, for $\aleph=2,3,4$, respectively.}
    \label{fig:FPE_triang_views}
\end{figure}

\section{Discussion}

While notional and based on several assumptions, the experiments presented in this work are representative of realistic approach phases. Performance is evaluated for low-resolution images and a 90-deg sun phase; arguably, these position the presented scenarios on the challenging end of the spectrum. In the cases studied, pole-projection estimation via symmetry detection proved effective. Estimation performance suggests that pole-projection errors could be in the order of a few degrees. Pole-projection results exhibit some level of robustness to different object morphologies, cutoff-frequency values, image resolutions, and camera longitudinal range.

In turn, these results inform the pole-triangulation analysis, according to which a camera-boresight angular displacement of $10-20\,\mathrm{deg}$ would then suffice to determine an initial pole estimate. This amount of boresight deviation is typically made available by the spacecraft latitude changes as it moves through the approach trajectory. Then, the pole estimate can be progressively improved as more hovering-camera image batches, and hence more pole-projection measurements, become available throughout the approach.

The proposed technique could potentially be applied to estimate the axis of rotation of a tumbling object, by modeling the rotational dynamics and treating pole-projection measurements --- from each hovering-camera batch --- as instantaneous observations of the time-varying rotation axis. Furthermore, while this study focuses on pole estimation for small celestial bodies, this technique could be applicable to artificial objects, such as uncooperative spacecraft.

% can obtain beta from a combination of latitude and attitude changes

\section{Conclusions}

In this work, we present the Fourier Pole Estimation (FPE) algorithm to determine the pole of an irregular object rotating about its principal axis. The pole projection onto hovering-camera views is extracted based on the symmetry encoded within the frequency domain of the co-added silhouette image. Pole-projection measurements from multiple hovering-camera views are then combined to perform pole triangulation and determine the 3D pole direction. We demonstrate the robustness of FPE to various object morphologies, severe shadowing effects, centerfinding errors, and low image resolutions. The results presented suggest that FPE is suitable to determine the pole during the approach phase of an irregular object.

%\bibliography{sample}
