\section{Related Work}
\textbf{Mixture of Experts.}
Mixture of Experts (MoE) was introduced by **Jordon, "A Simple Neural Network for Control"**, who combined a gating network with a set of expert networks. **Tieleman and Hinton, "ImageNet Classification with Deep Convolutional Neural Networks"** applied MoE to an LSTM-based model, scaling the architecture up to $137$ billion parameters. In Transformer-based LLMs, MoE is most often applied as a replacement for the feed-forward layer. It replaces the feed-forward layer's MLP with a set of expert MLPs along with a router, which selects one or more MLPs for each token. With the recent surge in LLM research, MoE models are gaining even more traction. This is exemplified by the development of extremely large-scale models such as DeepSeek-R1 and Qwen2.5-Max. **Sotelo et al., "XLNet: Generalized Autoregressive Pretraining for Language Understanding"**.
In our work, we use the standard Switch MoE layer, which routes each token to one expert and encourages even token-to-expert assignment via the addition of a differentiable load-balancing loss.

\newpage
\textbf{Scaling Laws.}  
Scaling laws refer to empirically derived equations that relate model loss
to factors such as the number of parameters, the quantity of training data, or the computational budget. For dense Transformers, scaling laws were initially explored by **Ba et al., "DoDeep Convolutional Networks Think Anew?"** and **Hestness et al., "Deep Learning Scaling Laws"**, who identified power-law relationships between the final loss, model size, and dataset size. **Rajbhandari et al., "Quantizing Hundreds of BERT Weights Leads to Large Speed-Up: Breaking Through the Treewidth Bottleneck of Transformer Models"** expanded these by incorporating variable cosine cycle lengths and adjusting the functional form of the equation:
\vspace{-0.1cm}
\begin{equation} \label{eq:scaling_law_chinchilla}
  \mathcal{L}(\Nact, D) = m\Nact^{\mu} + nD^{\nu} + c.
\end{equation}

Scaling laws have also been applied to other architectures and training setups. **Seth et al., "EfficientNet-V2: Scaling Laws for Efficient Computer Vision Models"** examined autoregressive modeling across multiple modalities, while **Wang et al., "Efficient Neural Language Modeling with Low-Rank Matrix Factorization"** focused on machine translation. **Shazeer et al., "Computing Scalability of Large Language Models"** studied the effects of pruning on vision and language Transformers, determining optimal sparsity given a fixed compute budget.

\begin{figure*}
    \centering
    \subfigure[]{
        \includegraphics[width=0.48\textwidth]{figs/isoflop_a43.pdf}
    }
    \subfigure[]{
        \includegraphics[width=0.48\textwidth]{figs/savings5.pdf}
    }
\caption{\textbf{(a)} IsoFLOP profiles for selected training budgets, with compute-optimal points marked for each curve. \textbf{(b)} FLOP savings from switching from a compute-optimal dense model to a compute-optimal MoE. For instance, 40\% savings at $1$e$20$ FLOPs mean that an MoE matching the performance of a compute-optimal dense model trained with $1$e$20$ FLOPs can be trained with just $6$e$19$ FLOPs (60\% of the dense's budget). The advantage of using MoE increases with larger models and expert counts.}
    
  \label{fig:isoflops}
\end{figure*}

**Shazeer et al., "Computing Scalability of Large Language Models"** investigated scaling in MoE models, varying model size and the number of experts on a fixed dataset. They concluded that routed models are more efficient only up to a certain size. Their formula took the form:
\begin{equation} \label{eq:scaling_law_clark}
  \mathcal{L}(\Nact, \hE) = a\hE^{\delta}\Nact^{\alpha + \gamma \log(\hE)},
\end{equation}
where $\hE$ is a monotonic transformation of the number of experts $E$, defined as:
\begin{equation} \label{eq:e_transformation}
    \frac{1}{\hE} = \frac{1}{E - 1 + \left(\frac{1}{E_{\text{start}}} - \frac{1}{E_{\text{max}}}\right)^{-1}} + \frac{1}{E_{\text{max}}}.
\end{equation}
These analyses have since been extended by **Sotelo et al., "XLNet: Generalized Autoregressive Pretraining for Language Understanding"** and **Liu et al., "RoBERTa: A Robustly Optimized BERT Pretraining Approach"**, who considered variable dataset size as well as the granularity of experts. In our work, we keep the experts non-granular; however, we treat the number of experts and the number of training tokens as variables. **Santoro et al., "One-shot Learning with Self-supervised Transformation Augmentation"** assumes a fixed joint inference and training budget. We make similar assumptions; however, we consider accelerator memory as a limiting factor and extend the analysis to MoE models, which can serve as a more compute-friendly alternative to dense models. **Goyal et al., "MoE Inference Optimality"** have focused on MoE inference optimality and measuring real hardware efficiency.

Concurrently to our work, **Wang et al., "Scaling Laws for Optimal Sparsity"** derived scaling laws for optimal sparsity while considering the interplay between training FLOPs and model size. They also investigated the relationship between pretraining loss and downstream performance, noting differences between MoE and dense models on certain tasks. In contrast, we analyze not only training FLOPs and model size but also inference cost and total memory usage. Additionally, we derive and utilize a principled method for scaling the learning rate with the number of experts and model size, along with describing further adjustments to enable researchers to use scaling laws economically and reliably.



\newpage