\section{Related Work}
\textbf{Mixture of Experts.}
Mixture of Experts (MoE) was introduced by ____, who combined a gating network with a set of expert networks. ____ applied MoE to an LSTM-based model____, scaling the architecture up to $137$ billion parameters. In Transformer-based LLMs, MoE is most often applied as a replacement for the feed-forward layer____. It replaces the feed-forward layer's MLP with a set of expert MLPs along with a router, which selects one or more MLPs for each token. With the recent surge in LLM research, MoE models are gaining even more traction. This is exemplified by the development of extremely large-scale models such as DeepSeek-R1 and Qwen2.5-Max____.
In our work, we use the standard Switch MoE layer____, which routes each token to one expert and encourages even token-to-expert assignment via the addition of a differentiable load-balancing loss.

\newpage
\textbf{Scaling Laws.}  
Scaling laws refer to empirically derived equations that relate model loss
to factors such as the number of parameters, the quantity of training data, or the computational budget. For dense Transformers, scaling laws were initially explored by ____ and ____, who identified power-law relationships between the final loss, model size, and dataset size. ____ expanded these by incorporating variable cosine cycle lengths and adjusting the functional form of the equation:
\vspace{-0.1cm}
\begin{equation} \label{eq:scaling_law_chinchilla}
  \mathcal{L}(\Nact, D) = m\Nact^{\mu} + nD^{\nu} + c.
\end{equation}



Scaling laws have also been applied to other architectures and training setups. ____ examined autoregressive modeling across multiple modalities, while ____ focused on machine translation. ____ studied the effects of pruning on vision and language Transformers, determining optimal sparsity given a fixed compute budget.

\begin{figure*}
    \centering
    \subfigure[]{
        \includegraphics[width=0.48\textwidth]{figs/isoflop_a43.pdf}
    }
    \subfigure[]{
        \includegraphics[width=0.48\textwidth]{figs/savings5.pdf}
    }
\caption{\textbf{(a)} IsoFLOP profiles for selected training budgets, with compute-optimal points marked for each curve. \textbf{(b)} FLOP savings from switching from a compute-optimal dense model to a compute-optimal MoE. For instance, 40\% savings at $1$e$20$ FLOPs mean that an MoE matching the performance of a compute-optimal dense model trained with $1$e$20$ FLOPs can be trained with just $6$e$19$ FLOPs (60\% of the dense's budget). The advantage of using MoE increases with larger models and expert counts.}
    
  \label{fig:isoflops}
\end{figure*}

____ investigated scaling in MoE models, varying model size and the number of experts on a fixed dataset. They concluded that routed models are more efficient only up to a certain size. Their formula took the form:
\begin{equation} \label{eq:scaling_law_clark}
  \mathcal{L}(\Nact, \hE) = a\hE^{\delta}\Nact^{\alpha + \gamma \log(\hE)},
\end{equation}
where $\hE$ is a monotonic transformation of the number of experts $E$, defined as:
\begin{equation} \label{eq:e_transformation}
    \frac{1}{\hE} = \frac{1}{E - 1 + \left(\frac{1}{E_{\text{start}}} - \frac{1}{E_{\text{max}}}\right)^{-1}} + \frac{1}{E_{\text{max}}}.
\end{equation}
These analyses have since been extended by ____ and ____, who considered variable dataset size as well as the granularity of experts. In our work, we keep the experts non-granular; however, we treat the number of experts and the number of training tokens as variables. ____ assumes a fixed joint inference and training budget. We make similar assumptions; however, we consider accelerator memory as a limiting factor and extend the analysis to MoE models, which can serve as a more compute-friendly alternative to dense models. ____ have focused on MoE inference optimality and measuring real hardware efficiency.

Concurrently to our work, ____ derived scaling laws for optimal sparsity while considering the interplay between training FLOPs and model size. They also investigated the relationship between pretraining loss and downstream performance, noting differences between MoE and dense models on certain tasks. In contrast, we analyze not only training FLOPs and model size but also inference cost and total memory usage. Additionally, we derive and utilize a principled method for scaling the learning rate with the number of experts and model size, along with describing further adjustments to enable researchers to use scaling laws economically and reliably.



















\newpage