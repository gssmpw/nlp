% Sequence Data 생성할때 보통 어떻게 생성하는지, 어떤 방식을 사용하는지
The processing of sequential data based on deep learning initially started with Recurrent Neural Networks \cite{schmidt2019recurrent}. \cite{schmidt2019recurrent} showed good performance at the time by utilizing information from previous sequences. However, \cite{schmidt2019recurrent} had the gradient vanishing problem. To overcome this limitation, models such as \cite{sak2014long} and \cite{dey2017gate} were introduced, which use gates to enable learning the amount of information from previous sequences. Although these models are complex, they are considered good research contributions as they solve the gradient vanishing problem.

Additionally, with the advancement of Transformers, the performance in processing sequential data has improved further. Particularly in vision-related tasks involving sequential data, such as \cite{ge2022long, yan2021videogpt, yu2023magvit, hong2022cogvideo, ma2024latte} and \cite{arnab2021vivit, liu2022video, fan2021multiscale,li2022mvitv2}, incorporating Transformers has yielded good performance. However, Transformers require high computational costs and have significant computational demands. Although methods like \cite{ahmad2021fpn, lee2023afi} are used to alleviate these issues, they still demand expensive computational resources.

We discovered that in the Lodge model, which generates coarse dances using \cite{ho2020denoising, song2020denoising}, the generated dances do not sequentially connect. To address this issue, we conducted research to utilize the dance information from previous frames in order to create coarse dance information that includes sequential frame data.

% Introduce various methods
% Diffusion RNN
%2018 ICLR, DIFFUSION CONVOLUTIONAL RECURRENT NEURAL NETWORK: DATA-DRIVEN TRAFFIC FORECASTING
% RNN, LSTM, WALT (Transformer), EMA-VFI
%