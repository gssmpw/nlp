% Introduction of this paper
% Introducing stduies for Dance (Video) Generation
% Analyze the problem of SOTA model
% Show our contributions
% lines (====) will be removed
%=================================== Previous works (1) ===================================

Recently, due to the increase in data and improvement in computing power, generative AI technology has experienced rapid growth, leading to significant advancements in dance generation AI as well. Generating novel dances from effective generative model not only can enhance the diversity in visual arts but also reduce dance creating costs and make the process more efficient. Game animators, movie directors, and etc. can save time and help create more immersive and realistic animations.

Previous works \cite{kim2022brand2, ofli2011learn2dance, siyao2022bailando, zhou2019dance} can generate short dances which is not proper in actual applications that requires dances longer than a few seconds. However, actual applications generally require dances lasting 3 to 5 minutes, and some performances even require dances lasting several hours. Therefore, there is now a demand for the generation of longer dance sequences.To generate long dance, dealing with the high computational costs is needed. Various methods such as auto-regressive model \cite{huang2020dance, li2021ai, zhuang2022music2dance, valle2021transflower}  try to deal with it but they are failed due to their lack of mode coverage. 

Building on the exceptional generation performance of diffusion models, \cite{tseng2023edge} and \cite{li2024lodge} propose music-conditioned dance generation models based on diffusion. \cite{tseng2023edge} introduced a transformer-based diffusion network for long sequences dance generation. They are able to generate arbitrarily long sequences by chaining the shorter clips with local consistency but lack an extreme-long-term dependencies. Additionally, \cite{tseng2023edge} proposed a novel Contact Consistency Loss and Physical Foot Contact Score to eliminate foot sliding physical implausibilities but still such problems are observed in generated frames.

\cite{li2024lodge} attempts to address Edge's challenge regarding the consistency of combined long dance sequences. \cite{li2024lodge} propose a coarse-to-fine two stage diffusion framework and characteristic dance primitives to produce long dances in a parallel manner. The first stage gets long music input to generate dance primitives which is coarse-grained diffusion. And the parallel local diffusion modules follow to generate short dance segments finally concatenated into long dance sequences. This two-stage coarse-to-fine diffusion framework strikes a between overall choreographic patterns and the quality of short-term local movements. \cite{li2024lodge} also proposes a method to overcome the foot-sliding problem, termed the foot refine block to eliminate artifacts.

% 이 사이에 previous work summary를 넣을 수 있긴한데 - 이준 -

Although Lodge could quickly generate long dance sequences, the process of generating coarse movements led to instability and a lack of consistency with previous frames, resulting in awkward transitions in the dance movements. To address this, we added a recurrent recalibration process during the coarse dance generation stage to enhance consistency. This addition ensures that subsequent dance movements are smooth and natural. Our contributions can be summarized as follows:
\newline
\begin{itemize}
    \item A Recurrent Block was added to the Coarse Dance Decoder.
    \item The Recurrent Block enhances the consistency of the dance and ensures that the movements are not awkward.
    \item Achieved state-of-the-art performance in dance coherency on the FineDance \cite{li2023finedance} dataset.
\end{itemize}


