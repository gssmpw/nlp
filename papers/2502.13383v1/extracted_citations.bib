@article{bai2023qwenvl,
  title={Qwen-vl: A versatile vision-language model for understanding, localization, text reading, and beyond},
  author={Bai, Jinze and Bai, Shuai and Yang, Shusheng and Wang, Shijie and Tan, Sinan and Wang, Peng and Lin, Junyang and Zhou, Chang and Zhou, Jingren},
  year={2023}
}

@inproceedings{blip,
  title={Blip-2: Bootstrapping language-image pre-training with frozen image encoders and large language models},
  author={Li, Junnan and Li, Dongxu and Savarese, Silvio and Hoi, Steven},
  booktitle={International conference on machine learning},
  pages={19730--19742},
  year={2023},
  organization={PMLR}
}

@inproceedings{blip1,
  author       = {Junnan Li and
                  Dongxu Li and
                  Caiming Xiong and
                  Steven C. H. Hoi},
  title        = {{BLIP:} Bootstrapping Language-Image Pre-training for Unified Vision-Language
                  Understanding and Generation},
  booktitle    = {International Conference on Machine Learning, {ICML} 2022, 17-23 July
                  2022, Baltimore, Maryland, {USA}},
  volume       = {162},
  pages        = {12888--12900},
  year         = {2022},
}

@misc{chatgpt,
  author       = {OpenAI},
  title        = {ChatGPT},
  year         = {2023},
  url          = {https://openai.com/blog/chatgpt},
}

@inproceedings{clip,
  title={Learning transferable visual models from natural language supervision},
  author={Radford, Alec and Kim, Jong Wook and Hallacy, Chris and Ramesh, Aditya and Goh, Gabriel and Agarwal, Sandhini and Sastry, Girish and Askell, Amanda and Mishkin, Pamela and Clark, Jack and others},
  booktitle={International conference on machine learning},
  pages={8748--8763},
  year={2021},
  organization={PMLR}
}

@article{cui2025process,
  title={Process Reinforcement through Implicit Rewards},
  author={Cui, Ganqu and Yuan, Lifan and Wang, Zefan and Wang, Hanbin and Li, Wendi and He, Bingxiang and Fan, Yuchen and Yu, Tianyu and Xu, Qixin and Chen, Weize and others},
  journal={arXiv preprint arXiv:2502.01456},
  year={2025}
}

@article{dino,
  author       = {Shilong Liu and
                  Zhaoyang Zeng and
                  Tianhe Ren and
                  Feng Li and
                  Hao Zhang and
                  Jie Yang and
                  Chunyuan Li and
                  Jianwei Yang and
                  Hang Su and
                  Jun Zhu and
                  Lei Zhang},
  title        = {Grounding {DINO:} Marrying {DINO} with Grounded Pre-Training for Open-Set
                  Object Detection},
  journal      = {CoRR},
  volume       = {abs/2303.05499},
  year         = {2023},
}

@article{gao2023g,
  title={G-llava: Solving geometric problem with multi-modal large language model},
  author={Gao, Jiahui and Pi, Renjie and Zhang, Jipeng and Ye, Jiacheng and Zhong, Wanjun and Wang, Yufei and Hong, Lanqing and Han, Jianhua and Xu, Hang and Li, Zhenguo and others},
  journal={arXiv preprint arXiv:2312.11370},
  year={2023}
}

@article{gao2024llm,
  title={Llm critics help catch bugs in mathematics: Towards a better mathematical verifier with natural language feedback},
  author={Gao, Bofei and Cai, Zefan and Xu, Runxin and Wang, Peiyi and Zheng, Ce and Lin, Runji and Lu, Keming and Lin, Junyang and Zhou, Chang and Xiao, Wen and others},
  journal={CoRR},
  year={2024}
}

@inproceedings{glipv2,
  author       = {Haotian Zhang and
                  Pengchuan Zhang and
                  Xiaowei Hu and
                  Yen{-}Chun Chen and
                  Liunian Harold Li and
                  Xiyang Dai and
                  Lijuan Wang and
                  Lu Yuan and
                  Jenq{-}Neng Hwang and
                  Jianfeng Gao},
  title        = {GLIPv2: Unifying Localization and Vision-Language Understanding},
  booktitle    = {Advances in Neural Information Processing Systems 35: Annual Conference
                  on Neural Information Processing Systems 2022, NeurIPS 2022, New Orleans,
                  LA, USA, November 28 - December 9, 2022},
  year         = {2022},
}

@inproceedings{grounded-pt,
  author       = {Liunian Harold Li and
                  Pengchuan Zhang and
                  Haotian Zhang and
                  Jianwei Yang and
                  Chunyuan Li and
                  Yiwu Zhong and
                  Lijuan Wang and
                  Lu Yuan and
                  Lei Zhang and
                  Jenq{-}Neng Hwang and
                  Kai{-}Wei Chang and
                  Jianfeng Gao},
  title        = {Grounded Language-Image Pre-training},
  booktitle    = {{IEEE/CVF} Conference on Computer Vision and Pattern Recognition,
                  {CVPR} 2022, New Orleans, LA, USA, June 18-24, 2022},
  pages        = {10955--10965},
  publisher    = {{IEEE}},
  year         = {2022},
}

@article{gu2024survey,
  title={A Survey on LLM-as-a-Judge},
  author={Gu, Jiawei and Jiang, Xuhui and Shi, Zhichao and Tan, Hexiang and Zhai, Xuehao and Xu, Chengjin and Li, Wei and Shen, Yinghan and Ma, Shengjie and Liu, Honghao and others},
  journal={arXiv preprint arXiv:2411.15594},
  year={2024}
}

@article{he2024cmmu,
  title={CMMU: A Benchmark for Chinese Multi-modal Multi-type Question Understanding and Reasoning},
  author={He, Zheqi and Wu, Xinya and Zhou, Pengfei and Xuan, Richeng and Liu, Guang and Yang, Xi and Zhu, Qiannan and Huang, Hua},
  journal={arXiv preprint arXiv:2401.14011},
  year={2024}
}

@article{huang2024hologram,
  title={Hologram Reasoning for Solving Algebra Problems with Geometry Diagrams},
  author={Huang, Litian and Yu, Xinguo and Xiong, Feng and He, Bin and Tang, Shengbing and Fu, Jiawen},
  journal={arXiv preprint arXiv:2408.10592},
  year={2024}
}

@article{huang2024key,
  title={Key-point-driven data synthesis with its enhancement on mathematical reasoning},
  author={Huang, Yiming and Liu, Xiao and Gong, Yeyun and Gou, Zhibin and Shen, Yelong and Duan, Nan and Chen, Weizhu},
  journal={arXiv preprint arXiv:2403.02333},
  year={2024}
}

@article{image-text-data,
  author       = {Weizhi Wang and
                  Khalil Mrini and
                  Linjie Yang and
                  Sateesh Kumar and
                  Yu Tian and
                  Xifeng Yan and
                  Heng Wang},
  title        = {Finetuned Multimodal Language Models Are High-Quality Image-Text Data
                  Filters},
  journal      = {CoRR},
  volume       = {abs/2403.02677},
  year         = {2024},
}

@article{li2024eagle,
  title={EAGLE: Elevating Geometric Reasoning through LLM-empowered Visual Instruction Tuning},
  author={Li, Zhihao and Du, Yao and Liu, Yang and Zhang, Yan and Liu, Yufang and Zhang, Mengdi and Cai, Xunliang},
  journal={arXiv preprint arXiv:2408.11397},
  year={2024}
}

@inproceedings{liang2023unimath,
  title={Unimath: A foundational and multimodal mathematical reasoner},
  author={Liang, Zhenwen and Yang, Tianyu and Zhang, Jipeng and Zhang, Xiangliang},
  booktitle={Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing},
  pages={7126--7133},
  year={2023}
}

@article{liu2024diving,
  title={Diving into Self-Evolving Training for Multimodal Reasoning},
  author={Liu, Wei and Li, Junlong and Zhang, Xiwen and Zhou, Fan and Cheng, Yu and He, Junxian},
  journal={arXiv preprint arXiv:2412.17451},
  year={2024}
}

@inproceedings{llava,
  author       = {Haotian Liu and
                  Chunyuan Li and
                  Qingyang Wu and
                  Yong Jae Lee},
  title        = {Visual Instruction Tuning},
  booktitle    = {Advances in Neural Information Processing Systems 36: Annual Conference
                  on Neural Information Processing Systems 2023, NeurIPS 2023, New Orleans,
                  LA, USA, December 10 - 16, 2023},
  year         = {2023},
}

@article{llava1.5,
  title={Improved baselines with visual instruction tuning},
  author={Liu, Haotian and Li, Chunyuan and Li, Yuheng and Lee, Yong Jae},
  journal={arXiv preprint arXiv:2310.03744},
  year={2023}
}

@article{lu2023mathvista,
  title={Mathvista: Evaluating mathematical reasoning of foundation models in visual contexts},
  author={Lu, Pan and Bansal, Hritik and Xia, Tony and Liu, Jiacheng and Li, Chunyuan and Hajishirzi, Hannaneh and Cheng, Hao and Chang, Kai-Wei and Galley, Michel and Gao, Jianfeng},
  journal={arXiv preprint arXiv:2310.02255},
  year={2023}
}

@article{luo2025ursa,
  title={URSA: Understanding and Verifying Chain-of-thought Reasoning in Multimodal Mathematics},
  author={Luo, Ruilin and Zheng, Zhuofan and Wang, Yifan and Yu, Yiyao and Ni, Xinzhe and Lin, Zicheng and Zeng, Jin and Yang, Yujiu},
  journal={arXiv preprint arXiv:2501.04686},
  year={2025}
}

@article{lyrics,
  author       = {Junyu Lu and
                  Ruyi Gan and
                  Dixiang Zhang and
                  Xiaojun Wu and
                  Ziwei Wu and
                  Renliang Sun and
                  Jiaxing Zhang and
                  Pingjian Zhang and
                  Yan Song},
  title        = {Lyrics: Boosting Fine-grained Language-Vision Alignment and Comprehension
                  via Semantic-aware Visual Objects},
  journal      = {CoRR},
  volume       = {abs/2312.05278},
  year         = {2023},
}

@article{meidani2023snip,
  title={Snip: Bridging mathematical symbolic and numeric realms with unified pre-training},
  author={Meidani, Kazem and Shojaee, Parshin and Reddy, Chandan K and Farimani, Amir Barati},
  journal={arXiv preprint arXiv:2310.02227},
  year={2023}
}

@article{otter,
  author       = {Bo Li and
                  Yuanhan Zhang and
                  Liangyu Chen and
                  Jinghao Wang and
                  Jingkang Yang and
                  Ziwei Liu},
  title        = {Otter: {A} Multi-Modal Model with In-Context Instruction Tuning},
  journal      = {CoRR},
  volume       = {abs/2305.03726},
  year         = {2023},
}

@inproceedings{pformer,
  author       = {Yiren Jian and
                  Chongyang Gao and
                  Soroush Vosoughi},
  title        = {Bootstrapping Vision-Language Learning with Decoupled Language Pre-training},
  booktitle    = {Advances in Neural Information Processing Systems 36: Annual Conference
                  on Neural Information Processing Systems 2023, NeurIPS 2023, New Orleans,
                  LA, USA, December 10 - 16, 2023},
  year         = {2023},
}

@article{qi2024mutual,
  title={Mutual reasoning makes smaller llms stronger problem-solvers},
  author={Qi, Zhenting and Ma, Mingyuan and Xu, Jiahang and Zhang, Li Lyna and Yang, Fan and Yang, Mao},
  journal={arXiv preprint arXiv:2408.06195},
  year={2024}
}

@article{sharegpt4v,
  author       = {Lin Chen and
                  Jinsong Li and
                  Xiaoyi Dong and
                  Pan Zhang and
                  Conghui He and
                  Jiaqi Wang and
                  Feng Zhao and
                  Dahua Lin},
  title        = {ShareGPT4V: Improving Large Multi-Modal Models with Better Captions},
  journal      = {CoRR},
  volume       = {abs/2311.12793},
  year         = {2023},
}

@misc{skyworkopeno12024,
  title={Skywork-o1 Open Series},
  author={Skywork-o1 Team},
  year={2024},
  month={November},
  howpublished={\url{https://huggingface.co/Skywork}},
  url={https://huggingface.co/Skywork},
}

@inproceedings{wang2024math,
  title={Math-shepherd: Verify and reinforce llms step-by-step without human annotations},
  author={Wang, Peiyi and Li, Lei and Shao, Zhihong and Xu, Runxin and Dai, Damai and Li, Yifei and Chen, Deli and Wu, Yu and Sui, Zhifang},
  booktitle={Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
  pages={9426--9439},
  year={2024}
}

@article{wang2024measuring,
  title={Measuring multimodal mathematical reasoning with math-vision dataset},
  author={Wang, Ke and Pan, Junting and Shi, Weikang and Lu, Zimu and Zhan, Mingjie and Li, Hongsheng},
  journal={arXiv preprint arXiv:2402.14804},
  year={2024}
}

@misc{wei2024implementation,
  author= {Wei Xiong and Hanning Zhang and Nan Jiang and Tong Zhang},
  title= {An Implementation of Generative PRM},
  year= {2024},
  howpublished = {\url{https://github.com/RLHFlow/RLHF-Reward-Modeling}},
  url={https://github.com/RLHFlow/RLHF-Reward-Modeling},
}

@article{yao2024mulberry,
  title={Mulberry: Empowering mllm with o1-like reasoning and reflection via collective monte carlo tree search},
  author={Yao, Huanjin and Huang, Jiaxing and Wu, Wenhao and Zhang, Jingyi and Wang, Yibo and Liu, Shunyu and Wang, Yingjie and Song, Yuxin and Feng, Haocheng and Shen, Li and others},
  journal={arXiv preprint arXiv:2412.18319},
  year={2024}
}

@article{yu2023metamath,
  title={Metamath: Bootstrap your own mathematical questions for large language models},
  author={Yu, Longhui and Jiang, Weisen and Shi, Han and Yu, Jincheng and Liu, Zhengying and Zhang, Yu and Kwok, James T and Li, Zhenguo and Weller, Adrian and Liu, Weiyang},
  journal={arXiv preprint arXiv:2309.12284},
  year={2023}
}

@article{zhang2024critic,
  title={Critic-v: Vlm critics help catch vlm errors in multimodal reasoning},
  author={Zhang, Di and Lei, Jingdi and Li, Junxian and Wang, Xunzhi and Liu, Yujie and Yang, Zonglin and Li, Jiatong and Wang, Weida and Yang, Suorong and Wu, Jianbo and others},
  journal={arXiv preprint arXiv:2411.18203},
  year={2024}
}

@article{zhang2024llama,
  title={Llama-berry: Pairwise optimization for o1-like olympiad-level mathematical reasoning},
  author={Zhang, Di and Wu, Jianbo and Lei, Jingdi and Che, Tong and Li, Jiatong and Xie, Tong and Huang, Xiaoshui and Zhang, Shufei and Pavone, Marco and Li, Yuqiang and others},
  journal={arXiv preprint arXiv:2410.02884},
  year={2024}
}

@article{zhang2024mathverse,
  title={Mathverse: Does your multi-modal llm truly see the diagrams in visual math problems?},
  author={Zhang, Renrui and Jiang, Dongzhi and Zhang, Yichi and Lin, Haokun and Guo, Ziyu and Qiu, Pengshuo and Zhou, Aojun and Lu, Pan and Chang, Kai-Wei and Gao, Peng and others},
  journal={arXiv preprint arXiv:2403.14624},
  year={2024}
}

@article{zhang2024mavis,
  title={MAVIS: Mathematical Visual Instruction Tuning},
  author={Zhang, Renrui and Wei, Xinyu and Jiang, Dongzhi and Zhang, Yichi and Guo, Ziyu and Tong, Chengzhuo and Liu, Jiaming and Zhou, Aojun and Wei, Bin and Zhang, Shanghang and others},
  journal={arXiv preprint arXiv:2407.08739},
  year={2024}
}

@article{zhang2024rest,
  title={Rest-mcts*: Llm self-training via process reward guided tree search},
  author={Zhang, Dan and Zhoubian, Sining and Hu, Ziniu and Yue, Yisong and Dong, Yuxiao and Tang, Jie},
  journal={arXiv preprint arXiv:2406.03816},
  year={2024}
}

@article{zhang2025lessons,
  title={The lessons of developing process reward models in mathematical reasoning},
  author={Zhang, Zhenru and Zheng, Chujie and Wu, Yangzhen and Zhang, Beichen and Lin, Runji and Yu, Bowen and Liu, Dayiheng and Zhou, Jingren and Lin, Junyang},
  journal={arXiv preprint arXiv:2501.07301},
  year={2025}
}

@article{zhuang2024math,
  title={Math-puma: Progressive upward multimodal alignment to enhance mathematical reasoning},
  author={Zhuang, Wenwen and Huang, Xin and Zhang, Xiantao and Zeng, Jin},
  journal={arXiv preprint arXiv:2408.08640},
  year={2024}
}

