\section{Related Work}
% In the field of MLLMs, benchmarking multimodal mathematical reasoning is an important research direction, as mathematical reasoning is a key metric for evaluating an LLMâ€™s ability to perform complex, multi-step reasoning and quantitative analysis in visual contexts. Below, we review relevant studies on multimodal mathematics benchmarks, general MLLMs, and math-specific MLLMs.

% \subsection{Benchmarks for Mathematical Evaluation}
% Recent research has made substantial progress in developing benchmarks to evaluate mathematical reasoning abilities. In this section, we review multimodal math benchmarks.

% \paragraph{Multimodal Benchmarks}
% With the rapid advancement of MLLMs, several high-quality benchmarks have been introduced to assess mathematical problem-solving in visual contexts. MathVista~\cite{lu2023mathvista} focuses on visual math QA tasks, evaluating model performance across various mathematical domains, including arithmetic and algebra, within visually rich scenarios. MATH-V~\cite{wang2024measuring} is another benchmark designed to assess multimodal mathematical understanding, with questions primarily sourced from math competitions. MathVerse~\cite{zhang2024mathverse} evaluates MLLMs' comprehension of visual diagrams by employing Chain-of-Thought (CoT) reasoning on 2,612 multimodal math problems. CMMU~\cite{he2024cmmu} is a large-scale Chinese benchmark for multi-disciplinary multimodal understanding, incorporating questions from college exams and textbooks.

% Compared to these existing multimodal mathematical benchmarks, which often have limitations in question length, complexity, and openness to model answers, our MathScape benchmark is designed to be longer and more open-ended.

\subsection{MLLMs for Mathematics}
\paragraph{Commonly Used MLLMs}
The integration of visual knowledge into LLMs has become a pivotal area of research due to the rapid advancements in LLMs. MLLMs combine vision information from vision encoders with LLMs, thus enabling these models to process and interpret visual inputs for various visual tasks ~\cite{dino,glipv2, grounded-pt} with enhanced accuracy and efficiency. Pioneering frameworks like CLIP ~\cite{clip} leverage contrastive learning on expansive image-caption datasets to align modalities, forming the groundwork for cross-modal comprehension. Various adapters ~\cite{llava, llava1.5,  blip, blip1, pformer, lyrics} are introduced to further integrate different modalities. For example, LLaVA ~\cite{llava, llava1.5} employs a straightforward MLP to inject the vision information into LLMs. Whereas more complex implementations like the Q-Former in BLIP ~\cite{blip1, blip} utilize cross-attention to enhance modality integration. 

Recent studies~\cite{image-text-data, sharegpt4v, llava, llava1.5, otter, zhang2024critic, zhuang2024math, luo2025ursa} aim to enhance MLLM performance by improving the quality of both pre-training and fine-tuning datasets. Models such as LLaVA~\cite{llava, llava1.5}, ShareGPT4V~\cite{sharegpt4v}, LLaVA-Next, LLaVA-OneVision~\cite{llava, llava1.5}, Qwen2-VL, and Qwen2.5-VL~\cite{bai2023qwenvl} have demonstrated significant advancements in understanding and executing complex instructions through instruction tuning. Leveraging large-scale training data, these models have also achieved strong performance in solving mathematical problems~\cite{lu2023mathvista}.

\paragraph{MLLMs Designed for Math Problems}
In real-world applications, vision inputs are commonly used to present mathematical problems for models to solve. As a result, it is crucial for Vision-Language Large Models (MLLMs) to demonstrate strong mathematical capabilities. Meidani et al.~\cite{meidani2023snip} pioneered the use of symbolic data to train a Vision-Language Model (VLM) with mathematical proficiency. Building on this work, UniMath~\cite{liang2023unimath} combined vision, table, and text encoders with LLMs, achieving state-of-the-art performance at the time. Additionally, Huang et al.~\cite{huang2024hologram} succeeded in solving algebraic problems that involved geometric diagrams.

Another noteworthy line of research involves using LLMs to tackle geometric problems. G-LLaVA~\cite{gao2023g} fine-tuned LLaVA~\cite{llava} with geometric data, reaching SOTA performance in geometry. Subsequently, MAViS~\cite{zhang2024mavis} and EAGLE~\cite{li2024eagle} achieved SOTA results by introducing math-specific encoders and amassing large amounts of mathematical data.

% \subsection{Mathematical Data Synthesis}
% The demand for high-quality data in the field of LLMs has spurred the flourishing development of the data synthesis domain. Existing data synthesis approaches can be broadly categorized into two types: those based on large language model distillation and those based on Monte Carlo Tree Search (MCTS).

% \textbf{LLM Based Distillation.} MetaMath~\cite{yu2023metamath} leverages GPT-3.5 Turbo models to rewrite existing mathematical problems from multiple perspectives, thereby generating the  MetaMathQA dataset. KPDDS~\cite{huang2024key} utilizes GPT-4 to extract the topics and Key Points from seed questions, processing and sampling them to synthesize new question-answer pairs. JiuZhang3.0~\cite{huang2024key} trains a specialized model for mathematical data synthesis, with the training and retraining datasets generated by GPT-4.

% \textbf{MCTS Based.} This approach was first proposed in OpenAI's o1~\cite{chatgpt}. It significantly expands the search space of model outputs. Compared to direct distillation, it demonstrates superior performance in synthesizing datasets for step-by-step solutions to complex mathematical problems. In ReST-MCTS*~\cite{zhang2024rest}, process reward value is utilized to guide MCTS, ensuring the accuracy of the data reasoning process. Meanwhile, rStar~\cite{qi2024mutual} introduces a more extensive action space at each step of reasoning. LLaMA-Berry~\cite{zhang2024llama} implements SR-MCTS (Self-refine), where each leaf node represents a complete problem-solving state, and child nodes correspond to the criticizing and rewriting of parent nodes. Mulberry~\cite{yao2024mulberry} proposes CoMCTS, which leverages collective knowledge from multiple models during inference and constructs a multimodal dataset, Mulberry-260k, for training MLLMs.

% PRM ORM
\subsection{LLM-as-a-Judge}
In the Reinforcement Learning from Human Feedback (RLHF) or MCTS-based inference, Reward Models (RMs) are employed to assess and score the quality of model outputs, thereby guiding the optimization or reasoning path of LLMs. Reward models can be categorized into Process Reward Models (PRMs) and Outcome Reward Models (ORMs).

\textbf{Outcome Reward Models.} ORMs evaluate only the final mathematical results without considering the solution process. For instance, Qwen2.5-Math-RM-72B~\cite{zhang2025lessons}, released by the Qwen team, assigns a single score to each mathematical response.

\textbf{Process Reward Models.} PRMs are more fine-grained, focusing on whether each step of the reasoning path is logical and correct, providing step-level feedback and guidance signals. For example, Math-Shepherd~\cite{wang2024math} is trained on an automatically constructed (rather than manually annotated) process supervision dataset, scoring each step of mathematical reasoning. MATHMinos-PRM~\cite{gao2024llm} introduces a novel two-stage training paradigm and incorporates step-wise natural language feedback labels. EurusPRM~\cite{cui2025process} utilize implicit PRM, where ORM is trained to evaluate response-level labels. Qwen2.5-Math-PRM~\cite{zhang2025lessons}, currently the SOTA PRM, proposes a consensus filtering mechanism combining Monte Carlo estimation and LLM-as-a-judge. Additionally, there are the Skywork-PRM series~\cite{skyworkopeno12024} and RLHFlow-PRM series~\cite{wei2024implementation} models. Moreover,~\citet{liu2024diving} proposed Multimodal PRM based on Monte Carlo rollouts. For more comprehensive LLM-as-a-Judge please refer to the LLM-as-a-Judge survey~\cite{gu2024survey}.