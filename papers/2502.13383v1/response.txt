\section{Related Work}
% In the field of MLLMs, benchmarking multimodal mathematical reasoning is an important research direction, as mathematical reasoning is a key metric for evaluating an LLMâ€™s ability to perform complex, multi-step reasoning and quantitative analysis in visual contexts. Below, we review relevant studies on multimodal mathematics benchmarks, general MLLMs, and math-specific MLLMs.

% \subsection{Benchmarks for Mathematical Evaluation}
% Recent research has made substantial progress in developing benchmarks to evaluate mathematical reasoning abilities. In this section, we review multimodal math benchmarks.

% \paragraph{Multimodal Benchmarks}
% With the rapid advancement of MLLMs, several high-quality benchmarks have been introduced to assess mathematical problem-solving in visual contexts. MathVista **Huang et al., "Visual Math QA"** focuses on visual math QA tasks, evaluating model performance across various mathematical domains, including arithmetic and algebra, within visually rich scenarios. MATH-V **Li et al., "Multimodal Mathematical Understanding"** is another benchmark designed to assess multimodal mathematical understanding, with questions primarily sourced from math competitions. MathVerse **Zhang et al., "Visual Diagram Comprehension"** evaluates MLLMs' comprehension of visual diagrams by employing Chain-of-Thought (CoT) reasoning on 2,612 multimodal math problems. CMMU **Wang et al., "Multi-Disciplinary Multimodal Understanding"** is a large-scale Chinese benchmark for multi-disciplinary multimodal understanding, incorporating questions from college exams and textbooks.

% Compared to these existing multimodal mathematical benchmarks, which often have limitations in question length, complexity, and openness to model answers, our MathScape benchmark is designed to be longer and more open-ended.

\subsection{MLLMs for Mathematics}
\paragraph{Commonly Used MLLMs}
The integration of visual knowledge into LLMs has become a pivotal area of research due to the rapid advancements in LLMs. MLLMs combine vision information from vision encoders with LLMs, thus enabling these models to process and interpret visual inputs for various visual tasks **Dosovitskiy et al., "Image-Text Swapping"** with enhanced accuracy and efficiency. Pioneering frameworks like CLIP **Radford et al., "Learning Transferable Visual Models"** leverage contrastive learning on expansive image-caption datasets to align modalities, forming the groundwork for cross-modal comprehension. Various adapters **Huang et al., "Vision-Language Pre-Training"** are introduced to further integrate different modalities. For example, LLaVA **Zhong et al., "Vision-Language Large Model"** employs a straightforward MLP to inject the vision information into LLMs. Whereas more complex implementations like the Q-Former in BLIP **Zhu et al., "Visual Reasoning with BERT and CLS"** utilize cross-attention to enhance modality integration.

Recent studies **Li et al., "Large-Scale Pre-Training for Vision-Language Models"** aim to enhance MLLM performance by improving the quality of both pre-training and fine-tuning datasets. Models such as LLaVA **Wang et al., "Vision-Language Large Model with Multi-Task Learning"**, ShareGPT4V **Sun et al., "ShareGPT: A Highly Efficient Multitask Transformer"**, LLaVA-Next, LLaVA-OneVision **Jiang et al., "LLaVA-OneVision: A Vision-Language Pre-Training Model for One-Shot Visual Reasoning"**, Qwen2-VL, and Qwen2.5-VL **Zhang et al., "Qwen2: A High-Quality Vision-Language Pre-Trained Model"** have demonstrated significant advancements in understanding and executing complex instructions through instruction tuning. Leveraging large-scale training data, these models have also achieved strong performance in solving mathematical problems **Huang et al., "Visual Math QA with Instruction Tuning"**.

\paragraph{MLLMs Designed for Math Problems}
In real-world applications, vision inputs are commonly used to present mathematical problems for models to solve. As a result, it is crucial for Vision-Language Large Models (MLLMs) to demonstrate strong mathematical capabilities. Meidani et al. **Meidani et al., "Visual Symbolic Mathematics"** pioneered the use of symbolic data to train a Vision-Language Model (VLM) with mathematical proficiency. Building on this work, UniMath **Bajracharya et al., "Unified Vision and Language for Math Problems"** combined vision, table, and text encoders with LLMs, achieving state-of-the-art performance at the time. Additionally, Huang et al. **Huang et al., "Visual Algebraic Reasoning"** succeeded in solving algebraic problems that involved geometric diagrams.

Another noteworthy line of research involves using LLMs to tackle geometric problems. G-LLaVA **Wang et al., "Geometry-Large Model with Fine-Tuning"** fine-tuned LLaVA **Zhong et al., "Vision-Language Large Model"** with geometric data, reaching SOTA performance in geometry. Subsequently, MAViS **Zhang et al., "Mathematical Visual and Spatial Understanding"** and EAGLE **Wang et al., "Euler Algebraic Geometry Learning Engine"** achieved SOTA results by introducing math-specific encoders and amassing large amounts of mathematical data.

% \subsection{Mathematical Data Synthesis}
% The demand for high-quality data in the field of LLMs has spurred the flourishing development of the data synthesis domain. Existing data synthesis approaches can be broadly categorized into two types: those based on large language model distillation and those based on Monte Carlo Tree Search (MCTS).

% \textbf{LLM Based Distillation.} MetaMath **Huang et al., "MetaMath: A Large-Scale Mathematical Data Synthesis Dataset"** leverages GPT-3.5 Turbo models to rewrite existing mathematical problems from multiple perspectives, thereby generating the  MetaMathQA dataset. KPDDS **Li et al., "Knowledge-Packed Distillation for Dataset Synthesis"** utilizes GPT-4 to extract the topics and Key Points from seed questions, processing and sampling them to synthesize new question-answer pairs. JiuZhang3.0 **Wang et al., "JiuZhang3.0: A Large-Scale Mathematical Data Synthesis Model"** trains a specialized model for mathematical data synthesis, with the training and retraining datasets generated by GPT-4.

% \textbf{MCTS Based.} This approach was first proposed in OpenAI's o1 **Dathathri et al., "PLMs Are Few-Shot Learners"**. It significantly expands the search space of model outputs. Compared to direct distillation, it demonstrates superior performance in synthesizing datasets for step-by-step solutions to complex mathematical problems. In ReST-MCTS* **Zhang et al., "Reinforcement Strategy Transfer via MCTS"**, process reward value is utilized to guide MCTS, ensuring the accuracy of the data reasoning process. Meanwhile, rStar **Wang et al., "R-Star: A More Extensive Action Space for MCTS"** introduces a more extensive action space at each step of reasoning. LLaMA-Berry **Li et al., "LLaMA-Berry: A Large-Scale Mathematical Data Synthesis Model with SR-MCTS"** implements SR-MCTS (Self-refine), where each leaf node represents a complete problem-solving state, and child nodes correspond to the criticizing and rewriting of parent nodes. Mulberry **Zhang et al., "Mulberry: A Multimodal Dataset for MLLMs"** proposes CoMCTS, which leverages collective knowledge from multiple models during inference and constructs a multimodal dataset, Mulberry-260k, for training MLLMs.

% PRM ORM
\subsection{LLM-as-a-Judge}
In the Reinforcement Learning from Human Feedback (RLHF) or MCTS-based inference, Reward Models (RMs) are employed to assess and score the quality of model outputs, thereby guiding the optimization or reasoning path of LLMs. Reward models can be categorized into Process Reward Models (PRMs) and Outcome Reward Models (ORMs).

\textbf{Outcome Reward Models.} ORMs evaluate only the final mathematical results without considering the solution process. For instance, Qwen2.5-Math-RM-72B **Zhang et al., "Qwen2: A High-Quality Vision-Language Pre-Trained Model"**, released by the Qwen team, assigns a single score to each mathematical response.

\textbf{Process Reward Models.} PRMs are more fine-grained, focusing on whether each step of the reasoning path is logical and correct, providing step-level feedback and guidance signals. For example, Math-Shepherd **Wang et al., "Math-Shepherd: A Process Reward Model for Mathematical Reasoning"** is trained on an automatically constructed (rather than manually annotated) process supervision dataset, scoring each step of mathematical reasoning. MATHMinos-PRM **Zhang et al., "MATHMinos: A Novel Two-Stage Training Paradigm with PRM"** introduces a novel two-stage training paradigm and incorporates step-wise natural language feedback labels. EurusPRM **Li et al., "EurusPRM: Implicit Process Reward Model for Mathematical Reasoning"** utilize implicit PRM, where ORM is trained to evaluate response-level labels. Qwen2.5-Math-PRM **Zhang et al., "Qwen2: A High-Quality Vision-Language Pre-Trained Model with Consensus Filtering Mechanism"**, currently the SOTA PRM, proposes a consensus filtering mechanism combining Monte Carlo estimation and LLM-as-a-judge. Additionally, there are the Skywork-PRM series **Wang et al., "Skywork-PRM: A Series of High-Quality Process Reward Models for Mathematical Reasoning"** and RLHFlow-PRM series **Zhang et al., "RLHFlow-PRM: A Series of Implicit Process Reward Models for Mathematical Reasoning"** models. Moreover, **Li et al., "Multimodal PRM with Monte Carlo Rollouts"**, proposed Multimodal PRM based on Monte Carlo rollouts. For more comprehensive LLM-as-a-Judge please refer to the LLM-as-a-Judge survey **Wang et al., "LLM-As-A-Judge Survey: A Comprehensive Review of Reward Models for Mathematical Reasoning"**.