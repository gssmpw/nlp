% This must be in the first 5 lines to tell arXiv to use pdfLaTeX, which is strongly recommended.
\pdfoutput=1
% In particular, the hyperref package requires pdfLaTeX in order to break URLs across lines.

\documentclass[11pt]{article}

% Change "review" to "final" to generate the final (sometimes called camera-ready) version.
% Change to "preprint" to generate a non-anonymous version with page numbers.
\usepackage[preprint]{acl}

% Standard package includes
\usepackage{times}
\usepackage{latexsym}
\usepackage{graphicx}
\usepackage{textcomp}
\usepackage{xcolor}
\usepackage{amsmath} 
\usepackage{booktabs} 
\usepackage{array}
\usepackage{graphicx} 
\usepackage{multirow}
\usepackage{color}
%\usepackage{ulem}
\usepackage{makecell} 
\usepackage{float}
\usepackage{subcaption}
%\usepackage{natbib}
\usepackage{colortbl}
\usepackage{placeins}
%\usepackage{algorithm}
%\usepackage{algorithmic}
\usepackage{comment}
\usepackage{pifont}
\usepackage{url} 


% For proper rendering and hyphenation of words containing Latin characters (including in bib files)
\usepackage[T1]{fontenc}
% For Vietnamese characters
% \usepackage[T5]{fontenc}
% See https://www.latex-project.org/help/documentation/encguide.pdf for other character sets

% This assumes your files are encoded as UTF8
\usepackage[utf8]{inputenc}

% This is not strictly necessary, and may be commented out,
% but it will improve the layout of the manuscript,
% and will typically save some space.
\usepackage{microtype}

% This is also not strictly necessary, and may be commented out.
% However, it will improve the aesthetics of text in
% the typewriter font.
\usepackage{inconsolata}

%Including images in your LaTeX document requires adding
%additional package(s)
\usepackage{graphicx}

% If the title and author information does not fit in the area allocated, uncomment the following
%
%\setlength\titlebox{<dim>}
%
% and set <dim> to something 5cm or larger.

\title{MM-Verify: Enhancing Multimodal Reasoning with Chain-of-Thought Verification}

% Author information can be set in various styles:
% For several authors from the same institution:
% \author{Author 1 \and ... \and Author n \\
%         Address line \\ ... \\ Address line}
% if the names do not fit well on one line use
%         Author 1 \\ {\bf Author 2} \\ ... \\ {\bf Author n} \\
% For authors from different institutions:
% \author{Author 1 \\ Address line \\  ... \\ Address line
%         \And  ... \And
%         Author n \\ Address line \\ ... \\ Address line}
% To start a separate ``row'' of authors use \AND, as in
% \author{Author 1 \\ Address line \\  ... \\ Address line
%         \AND
%         Author 2 \\ Address line \\ ... \\ Address line \And
%         Author 3 \\ Address line \\ ... \\ Address line}

\author{Linzhuang Sun$^{1}$\thanks{Equal Contribution.}, \ Hao Liang$^{2*}$, \ Jingxuan Wei$^{1}$, \ Bihui Yu$^{1}$, \ Tianpeng Li$^{3}$, \\
 {\bf Fan Yang$^{3}$\thanks{Corresponding Author}}, {\bf\ Zenan Zhou$^{3\dagger}$}, {\bf\ Wentao Zhang$^{2\dagger}$}   \\
  $^{1}$University of Chinese Academy of Sciences \\
  $^{2}$Peking University \\
  $^{3}$Baichuan Inc. \\
  \textit{sunlinzhuang21@mails.ucas.ac.cn, hao.liang@stu.pku.edu.cn} \\
  \textit{\{yangfan, zhouzenan\}@baichuan-inc.com, wentao.zhang@pku.edu.cn}
}





%\author{
%  \textbf{First Author\textsuperscript{1}},
%  \textbf{Second Author\textsuperscript{1,2}},
%  \textbf{Third T. Author\textsuperscript{1}},
%  \textbf{Fourth Author\textsuperscript{1}},
%\\
%  \textbf{Fifth Author\textsuperscript{1,2}},
%  \textbf{Sixth Author\textsuperscript{1}},
%  \textbf{Seventh Author\textsuperscript{1}},
%  \textbf{Eighth Author \textsuperscript{1,2,3,4}},
%\\
%  \textbf{Ninth Author\textsuperscript{1}},
%  \textbf{Tenth Author\textsuperscript{1}},
%  \textbf{Eleventh E. Author\textsuperscript{1,2,3,4,5}},
%  \textbf{Twelfth Author\textsuperscript{1}},
%\\
%  \textbf{Thirteenth Author\textsuperscript{3}},
%  \textbf{Fourteenth F. Author\textsuperscript{2,4}},
%  \textbf{Fifteenth Author\textsuperscript{1}},
%  \textbf{Sixteenth Author\textsuperscript{1}},
%\\
%  \textbf{Seventeenth S. Author\textsuperscript{4,5}},
%  \textbf{Eighteenth Author\textsuperscript{3,4}},
%  \textbf{Nineteenth N. Author\textsuperscript{2,5}},
%  \textbf{Twentieth Author\textsuperscript{1}}
%\\
%\\
%  \textsuperscript{1}Affiliation 1,
%  \textsuperscript{2}Affiliation 2,
%  \textsuperscript{3}Affiliation 3,
%  \textsuperscript{4}Affiliation 4,
%  \textsuperscript{5}Affiliation 5
%\\
%  \small{
%    \textbf{Correspondence:} \href{mailto:email@domain}{email@domain}
%  }
%}

\begin{document}
\maketitle
\begin{abstract}
According to the Test-Time Scaling, the integration of External Slow-Thinking with the Verify mechanism has been demonstrated to enhance multi-round reasoning in large language models (LLMs). However, in the multimodal (MM) domain, there is still a lack of a strong MM-Verifier. In this paper, we introduce MM-Verifier and MM-Reasoner to enhance multimodal reasoning through longer inference and more robust verification. First, we propose a two-step MM verification data synthesis method, which combines a simulation-based tree search with verification and uses rejection sampling to generate high-quality Chain-of-Thought (COT) data. This data is then used to fine-tune the verification model, MM-Verifier. Additionally, we present a more efficient method for synthesizing MMCOT data, bridging the gap between text-based and multimodal reasoning. The synthesized data is used to fine-tune MM-Reasoner. Our MM-Verifier outperforms all larger models on the MathCheck, MathVista, and MathVerse benchmarks. Moreover, MM-Reasoner demonstrates strong effectiveness and scalability, with performance improving as data size increases. Finally, our approach achieves strong performance when combining MM-Reasoner and MM-Verifier, reaching an accuracy of 65.3 on MathVista, surpassing GPT-4o (63.8) with 12 rollouts. Our code is made available \url{https://github.com/Aurora-slz/MM-Verify}.
\end{abstract}

\section{Introduction}

\begin{figure}[t]
\centering
\includegraphics[width=0.48\textwidth]{figures/Face.pdf}
\caption{Our 7B MM-Verifier outperform all other models, even large models like GPT-4o, Gemini and Claude on the MathCheck Outcome-Judging benchmark.}
\label{fig:fengmian}
\vspace{-4mm}
\end{figure}

Large language models (LLMs) have demonstrated exceptional performance across diverse tasks spanning myriad domains~\cite{chatgpt, llama}. Based on LLMs, MLLMs~\cite{zhao2023survey, wu2023multimodal, bai2024survey} also show strong understanding ability among different modalities~\cite{llava, bai2023qwenvl}. They have demonstrated strong performance in image classification~\cite{chen2024internvl}, image understanding~\cite{blip, li2023blip}, image captioning~\cite{bai2023qwenvl}, visual question answering~\cite{llava,llava1.5} and image-text retrieval~\cite{chen2024internvl}. Recently, MLLMs have also made significant strides in solving mathematical problems~\cite{liang2023unimath, huang2024hologram}. Researcher in our community have made efforts in designing strong reasoning models and algorithms~\cite{thawakar2025llamav, du2025virgo}. Despite the effort made in processing MLLMs, we still face two challenges:
\paragraph{Lack of Strong MM Verifiers} In pure-text LLMs, models can improve through self-critic methods~\cite{weng2022large, sun2024beats}. However, as shown in Table \ref{tab:mcts}, such methods face challenges in enhancing performance in multimodal models. Therefore, it is crucial to develop robust multimodal (MM) verifiers.

\paragraph{Lack of Long COT Reasoning Data} In the pure-text domain, models like DeepSeek-R1~\cite{guo2025deepseek}, s1~\cite{muennighoff2025s1}, and LIMO~\cite{ye2025limo} have demonstrated the effectiveness of Long COT data. However, in the multimodal domain, most collected mathematics problems are not in the Long COT format~\cite{li2024llava}. Therefore, the development of Long COT synthetic methods is necessary to enhance the reasoning ability of MLLMs.

To address these issues, in this paper, we introduce two novel data synthesis methods and subsequently train MM-Verifier and MM-Reasoner.  First, we perform a tree search, using simulations as rewards, to generate high-quality long MMCOT data. Next, we fine-tune multimodal large language models (MLLMs) on our data, resulting in long chain-of-thought (COT) responses. These data are then Given that MLLMs generate long COT responses, we then apply rejection sampling to further enhance their verification capabilities, leading to the proposal of MM-Verifier. After the introduction of MM-Verifier, we observed that long COT data significantly improves model reasoning performance. As a result, we aim to synthesize large amounts of long COT data to enhance the performance of base MLLMs. Since tree search can be computationally expensive, we use the MAVIS dataset~\cite{zhang2024mavis}, leveraging the descriptions of patterns in MAVIS and inputting them into the pure-text reasoning model, Qwen QWQ. We then pair these patterns with the corresponding responses from the Qwen QWQ model. This method has proven to be effective, scalable, and capable of efficiently constructing large amounts of long MMCOT data.


The core contributions are summarized as follows:
\begin{itemize}
    \item \textbf{MM Reasoning Data Synthesis Method} We propose two novel data synthesis methods for both our MM-Verifier and MM-Reasoner. First, we introduce a two-step MM-verification data synthesis approach that combines simulation-based tree search with GPT-4 verification and rejection sampling to generate high-quality COT data. Additionally, we use graphical software to link multimodal geometric shapes with textual descriptions, enabling the generation of multimodal reasoning data through a purely text-based reasoning model.
    
    \item \textbf{MM-Verifier} We introduce a new multimodal Outcome Reward Model (ORM) called MM-Verifier. MM-Verifier achieves state-of-the-art (SOTA) performance on the MathCheck benchmark, surpassing closed-source models such as GPT-4, Gemini, and Claude. Furthermore, our MM-Verifier-7B outperforms Qwen2-VL-72B across all metrics on the MathVista and MathVerse benchmarks.
    
    \item \textbf{Scalability of MM-Reasoner} We propose a novel MM-Reasoning model based exclusively on our synthetic COT data. Although our MM-Reasoner does not achieve SOTA performance, it outperforms the baseline models and demonstrates scalability as the size of the training dataset increases. This provides new insights for the development of more powerful MM-Reasoners.
    
    \item \textbf{Strong Performance} By combining MM-Verifier and MM-Reasoning, with a model size of only 7B parameters, we outperform both GPT-4 and human performance on the MathVista benchmark, highlighting the strong performance of our method.
\end{itemize}


\section{Related Work}
% In the field of MLLMs, benchmarking multimodal mathematical reasoning is an important research direction, as mathematical reasoning is a key metric for evaluating an LLMâ€™s ability to perform complex, multi-step reasoning and quantitative analysis in visual contexts. Below, we review relevant studies on multimodal mathematics benchmarks, general MLLMs, and math-specific MLLMs.

% \subsection{Benchmarks for Mathematical Evaluation}
% Recent research has made substantial progress in developing benchmarks to evaluate mathematical reasoning abilities. In this section, we review multimodal math benchmarks.

% \paragraph{Multimodal Benchmarks}
% With the rapid advancement of MLLMs, several high-quality benchmarks have been introduced to assess mathematical problem-solving in visual contexts. MathVista~\cite{lu2023mathvista} focuses on visual math QA tasks, evaluating model performance across various mathematical domains, including arithmetic and algebra, within visually rich scenarios. MATH-V~\cite{wang2024measuring} is another benchmark designed to assess multimodal mathematical understanding, with questions primarily sourced from math competitions. MathVerse~\cite{zhang2024mathverse} evaluates MLLMs' comprehension of visual diagrams by employing Chain-of-Thought (CoT) reasoning on 2,612 multimodal math problems. CMMU~\cite{he2024cmmu} is a large-scale Chinese benchmark for multi-disciplinary multimodal understanding, incorporating questions from college exams and textbooks.

% Compared to these existing multimodal mathematical benchmarks, which often have limitations in question length, complexity, and openness to model answers, our MathScape benchmark is designed to be longer and more open-ended.

\subsection{MLLMs for Mathematics}
\paragraph{Commonly Used MLLMs}
The integration of visual knowledge into LLMs has become a pivotal area of research due to the rapid advancements in LLMs. MLLMs combine vision information from vision encoders with LLMs, thus enabling these models to process and interpret visual inputs for various visual tasks ~\cite{dino,glipv2, grounded-pt} with enhanced accuracy and efficiency. Pioneering frameworks like CLIP ~\cite{clip} leverage contrastive learning on expansive image-caption datasets to align modalities, forming the groundwork for cross-modal comprehension. Various adapters ~\cite{llava, llava1.5,  blip, blip1, pformer, lyrics} are introduced to further integrate different modalities. For example, LLaVA ~\cite{llava, llava1.5} employs a straightforward MLP to inject the vision information into LLMs. Whereas more complex implementations like the Q-Former in BLIP ~\cite{blip1, blip} utilize cross-attention to enhance modality integration. 

Recent studies~\cite{image-text-data, sharegpt4v, llava, llava1.5, otter, zhang2024critic, zhuang2024math, luo2025ursa} aim to enhance MLLM performance by improving the quality of both pre-training and fine-tuning datasets. Models such as LLaVA~\cite{llava, llava1.5}, ShareGPT4V~\cite{sharegpt4v}, LLaVA-Next, LLaVA-OneVision~\cite{llava, llava1.5}, Qwen2-VL, and Qwen2.5-VL~\cite{bai2023qwenvl} have demonstrated significant advancements in understanding and executing complex instructions through instruction tuning. Leveraging large-scale training data, these models have also achieved strong performance in solving mathematical problems~\cite{lu2023mathvista}.

\paragraph{MLLMs Designed for Math Problems}
In real-world applications, vision inputs are commonly used to present mathematical problems for models to solve. As a result, it is crucial for Vision-Language Large Models (MLLMs) to demonstrate strong mathematical capabilities. Meidani et al.~\cite{meidani2023snip} pioneered the use of symbolic data to train a Vision-Language Model (VLM) with mathematical proficiency. Building on this work, UniMath~\cite{liang2023unimath} combined vision, table, and text encoders with LLMs, achieving state-of-the-art performance at the time. Additionally, Huang et al.~\cite{huang2024hologram} succeeded in solving algebraic problems that involved geometric diagrams.

Another noteworthy line of research involves using LLMs to tackle geometric problems. G-LLaVA~\cite{gao2023g} fine-tuned LLaVA~\cite{llava} with geometric data, reaching SOTA performance in geometry. Subsequently, MAViS~\cite{zhang2024mavis} and EAGLE~\cite{li2024eagle} achieved SOTA results by introducing math-specific encoders and amassing large amounts of mathematical data.

% \subsection{Mathematical Data Synthesis}
% The demand for high-quality data in the field of LLMs has spurred the flourishing development of the data synthesis domain. Existing data synthesis approaches can be broadly categorized into two types: those based on large language model distillation and those based on Monte Carlo Tree Search (MCTS).

% \textbf{LLM Based Distillation.} MetaMath~\cite{yu2023metamath} leverages GPT-3.5 Turbo models to rewrite existing mathematical problems from multiple perspectives, thereby generating the  MetaMathQA dataset. KPDDS~\cite{huang2024key} utilizes GPT-4 to extract the topics and Key Points from seed questions, processing and sampling them to synthesize new question-answer pairs. JiuZhang3.0~\cite{huang2024key} trains a specialized model for mathematical data synthesis, with the training and retraining datasets generated by GPT-4.

% \textbf{MCTS Based.} This approach was first proposed in OpenAI's o1~\cite{chatgpt}. It significantly expands the search space of model outputs. Compared to direct distillation, it demonstrates superior performance in synthesizing datasets for step-by-step solutions to complex mathematical problems. In ReST-MCTS*~\cite{zhang2024rest}, process reward value is utilized to guide MCTS, ensuring the accuracy of the data reasoning process. Meanwhile, rStar~\cite{qi2024mutual} introduces a more extensive action space at each step of reasoning. LLaMA-Berry~\cite{zhang2024llama} implements SR-MCTS (Self-refine), where each leaf node represents a complete problem-solving state, and child nodes correspond to the criticizing and rewriting of parent nodes. Mulberry~\cite{yao2024mulberry} proposes CoMCTS, which leverages collective knowledge from multiple models during inference and constructs a multimodal dataset, Mulberry-260k, for training MLLMs.

% PRM ORM
\subsection{LLM-as-a-Judge}
In the Reinforcement Learning from Human Feedback (RLHF) or MCTS-based inference, Reward Models (RMs) are employed to assess and score the quality of model outputs, thereby guiding the optimization or reasoning path of LLMs. Reward models can be categorized into Process Reward Models (PRMs) and Outcome Reward Models (ORMs).

\textbf{Outcome Reward Models.} ORMs evaluate only the final mathematical results without considering the solution process. For instance, Qwen2.5-Math-RM-72B~\cite{zhang2025lessons}, released by the Qwen team, assigns a single score to each mathematical response.

\textbf{Process Reward Models.} PRMs are more fine-grained, focusing on whether each step of the reasoning path is logical and correct, providing step-level feedback and guidance signals. For example, Math-Shepherd~\cite{wang2024math} is trained on an automatically constructed (rather than manually annotated) process supervision dataset, scoring each step of mathematical reasoning. MATHMinos-PRM~\cite{gao2024llm} introduces a novel two-stage training paradigm and incorporates step-wise natural language feedback labels. EurusPRM~\cite{cui2025process} utilize implicit PRM, where ORM is trained to evaluate response-level labels. Qwen2.5-Math-PRM~\cite{zhang2025lessons}, currently the SOTA PRM, proposes a consensus filtering mechanism combining Monte Carlo estimation and LLM-as-a-judge. Additionally, there are the Skywork-PRM series~\cite{skyworkopeno12024} and RLHFlow-PRM series~\cite{wei2024implementation} models. Moreover,~\citet{liu2024diving} proposed Multimodal PRM based on Monte Carlo rollouts. For more comprehensive LLM-as-a-Judge please refer to the LLM-as-a-Judge survey~\cite{gu2024survey}.

\section{Methodology}

\begin{figure*}[t]
\centering\textbf{}
\includegraphics[width=\textwidth]{figures/pdf/method2.pdf}
\caption{We present the pipeline for synthesizing MM-Verifier data. In Stage 1, we use a simulation-based algorithm for long-chain CoT reasoning and long verification. In Stage 2, we use the trained Verifier model from Stage 1 to further enhance it using rejection sampling, generating more long CoT verification data.}
\label{fig:method}
\vspace{-4mm}
\end{figure*}

\input{table/dataset}  

In this section, we introduce the construction process of MM-Verifier, as illustrated in Figure~\ref{fig:method}. Section~\ref{sec:3-1} details the data synthesis methodology for MM-Verifier (Stage 1). Section~\ref{sec:3-2} describes the data synthesis scheme employed in MM-Verifier (Stage 2). In Section~\ref{sec:3-3}, we explore the enhancement of multimodal model reasoning capabilities through the integration of long-COT data in pure text form.



\subsection{Stage1: Long CoT MM-Verifier}
\label{sec:3-1}
\subsubsection{Source Data Collection}  
MM-Verifier is designed to verify the correctness of a \( <q,s> \) pair by determining whether the solution \( s \) is correct. To achieve this goal, it is essential to synthesize diverse verification data from a variety of sources. Specifically, we construct the question source pool \( D_s \) from seven categories in MATH360V: Geometry3K, TabMWP, Super-CLEVR, UniGeo, FigureQA, and GEOS, with their statistical details summarized in Table~\ref{tab:dataset}. By collecting data from these categories, we obtained a diverse set of multimodal mathematics questions. 

However, solutions in these datasets are typically very short, and many contain only answers. When verifying reasoning, the answer is often provided in a long COT form. Therefore, we need to construct COT data ranging from short to long, along with their corresponding verifications. To facilitate the generation of long COT data and enhance the diversity of the training set, we design a simulation-based search algorithm to create extended reasoning trajectory data for training MM-Verifier.
\begin{figure}[t]
\centering
\includegraphics[width=0.47\textwidth]{figures/pdf/question_mcts_sample_lh_2.pdf}
\caption{Answer length of direct sampling and simulated-based search. We can see the simulated-based search can synthesize longer COT answers.}
\label{fig:lengthDist}
\vspace{-4mm}
\end{figure}
\subsubsection{Simulation-based Search Algorithm}
Inspired by Monte Carlo Tree Search (MCTS), we propose a simulation-based search algorithm tailored for multimodal models. However, we do not directly apply the traditional MCTS tree search, as multimodal models often fail to generate reliable rewards, which results in suboptimal performance, as illustrated in Appendix \ref{app:MCTS_performance}. To address this challenge, we introduce a simulation-based reward mechanism. 

Starting from the root node \( q_i \), we first simulate \( k \) child nodes for each node. For each child node, we perform simulations where the model directly generates answers based on the current node. For a tree node \( u_d \), which represents a node at depth \( d \), the ancestral path leading up to the root is denoted by the sequence \( \{ u_{d-1}, \dots, u_1 \} \). The simulation answer for this path is given by:
\[
\textit{Simulation Answer} = LLM\left(\bigoplus_{i=1}^{d-1} u_i\right)
\]
These simulations are repeated \( l \) times, and the correctness ratio of the \textit{Simulation Answer} is used as the reward. Once the reward is obtained, we apply the MCTS algorithm for further simulation and data synthesis.

Using the simulation-based MCTS approach, for each question, we perform \( n \) rollouts and collect solution pairs \( \langle q_i, p_j^i \rangle \), where \( j \in \{ 1, 2, \dots, n \} \) and \( n \) represents the number of leaf nodes. These \( n \) solution pairs are then verified as positive and negative cases for training the MM-verifier.


% The root node represents the input question \( q_i \), while the leaf nodes correspond to the deduced final answer. Intermediate nodes is the step-wise reasoning process, and the edges between nodes represent the associated reasoning actions $A_1$: \textit{Inference one step forward} . A node in the tree \(\tau\) is represented as \( u_d \), where \( d \) signifies its depth. For any given node \( u_d \), its ancestral path leading up to the root is denoted by the sequence $\{u_{d-1}, ..., u_1\}$, which is called partial solution $p_d$.

% During node expansion, the algorithm first executes action $A_2$: \textit{Simulation Samples}, with the current path $p_d$ as input to the LMM. The model performs multi rounds of sampling to generate \( m \) candidate reasoning steps, from which one is randomly selected to serve as the expanded node. Once the whole search tree expansion is complete, all solutions terminating at leaf nodes are collected to form the solution pairs, representing as
\input{table/abla_mathvista}
\subsubsection{Long COT Verification Data Synthesize}
After obtaining the \( \langle q_i, p_j^i \rangle \) pair, the next step is to verify \( p_j^i \) to determine whether \( q_i \) has been answered correctly. To do this, we use GPT-4o (gpt-4o-2024-08-06) to verify \( \langle q_i, p_j^i \rangle \) using the instruction "Verify step by step..." (for the detailed prompt, see Figure~\ref{fig:prompt-verify}). The model's output, denoted as \( v_i \), serves as the target for the verifier. The resulting dataset collected at this stage is represented as \( D_v \), which can be expressed as:
\[
D_v = \{(q_i, p_j^i, v_i) \mid i = 1, \dots, m; \, j = 1, \dots, n \}
\]
Additionally, we implement a data-cleaning strategy to filter high-quality synthetic data for training the MM-Verifier. For each data instance \( (q_i, p_j^i, v_i) \) in \( D_v \), we first design an answer extraction prompt, as shown in Figure \ref{fig:prompt-extract}) and use LLaMA-3.2-3B-Instruct for answer extraction, denoted by \texttt{extract()} for clarity. We then apply the following criteria for data cleaning:
\begin{itemize}
    \item \textbf{Condition 1:} If \( \text{extract}(p_j^i) \) matches the golden label \( \text{extract}(y_i) \), and the final result of \( v_i \) is the answer is correct.
    \item \textbf{Condition 2:} If \( \text{extract}(p_j^i) \) does not match \( \text{extract}(y_i) \), and the final result of \( v_i \) is the answer is not correct.
\end{itemize}
We collect the instances that meet the above conditions, \( (q_i, p_j^i, v_i) \), into \( D_{\text{clean}} \). Any instance that does not satisfy these conditions is discarded. The dataset \( D_{\text{clean}} \) is then used for supervised fine-tuning (SFT) on Qwen2-VL-7B-Instruct, yielding the first-stage verifier, MM-Verifier (Stage 1).


% \begin{align}
% D_{\text{clean}} = \{ (q_i, p_i^j, v_i) \mid  
% & (p_j^i = y_i \land v_i = \text{True}) \notag \\
% & \lor (p_j^i \neq y_i \land v_i = \text{False}) \}
% \end{align}

% \subsection{Sampling Further Enhances Verifying Performance}
\subsection{Stage 2: Rejection Sampling Further Improves Verification}
\label{sec:3-2}

In Stage 1, we obtained a Verifier with strong long-chain Chain-of-Thought (CoT) reasoning capabilities. In Stage 2, our goal is to improve the efficiency of the data synthesis process, reduce API costs, and further enhance the Verifier's capabilities.

To achieve this, we first generate corresponding solutions based on a given set of questions. By leveraging the long CoT reasoning ability of the MM-Verifier from Stage 1, we can generate additional long CoT verification data.

The synthetic data is then cleaned using string matching against the correct answer. The filtered data is subsequently fed into the MM-Verifier (Stage 1) for further training, resulting in the enhanced MM-Verifier (Stage 2).
\input{table/abla_mathverse}

\subsection{Bridging the Gap Between Text and MM}
\label{sec:3-3}
When applying MM-Verifier, the base model generates multiple reasoning paths for a given question. The MM-Verifier then evaluates these paths, distinguishing between correct and incorrect inferences. This process inherently requires the base model to produce at least one plausible correct reasoning path for the MM-Verifier to recognize and validate. Therefore, in addition to a promising MM-Verifier,In this section, our goal is to synthesize long COT data to train a robust MM-Reasoner capable of learning long COT reasoning. However, synthesizing long COT data using tree search can be computationally expensive. Moreover, long COT pure text models have demonstrated strong performance. To efficiently generate long COT data, we propose distilling knowledge from pure text models to our MM-Reasoner.

Specifically, we select data from MAVIS-GEOMETRY~\cite{zhang2024mavis}, which includes geometric pattern drawings along with textual instructions. By combining the geometric textual instructions with the original questions, we can feed them into a pure text reasoning model. The outputs generated by the reasoning model, Qwen-QwQ~\cite{qwq-32b-preview} with prompts in Figure \ref{fig:prompt-qwq}, are then collected as target labels for the MM-Reasoner training dataset, denoted as \( D_r \). To ensure the quality of the data, we filter out incorrect QwQ-generated reasoning results from \( D_r \). Then we use the filtered data to train Qwen2-VL-7B-Instruct with supervised fine-tuning (SFT), ultimately obtaining the MM-Reasoner model.


\section{Experiments}
\subsection{Experiment Setting}
\paragraph{Baseline Models.}  
The base models for MM-Verifier and MM-Reasoner are Qwen2-VL-Instruct-7B~\cite{bai2023qwenvl}. For comparison, we include random selection and human performance as baselines, along with two types of closed-source models: GPT-4o (gpt-4o-2024-08-06)~\cite{openai2023gpt} and Qwen-VL-Plus~\cite{bai2023qwen}. For open-source models, we evaluate ten MLLMs: mPLUG-Owl2-7B~\cite{ye2024mplug}, MiniGPT4-7B~\cite{zhu2023minigpt}, LLaVA-1.5-13B~\cite{llava1.5}, SPHINX-V2-13B~\cite{lin2023sphinx}, Deepseek-VL~\cite{lu2024deepseek}, LLaVA-OneVision-7B (llava-onevision-qwen2-7b-ov-hf)~\cite{li2024llava}, Qwen2-VL-Instruct-7B~\cite{bai2023qwenvl}, Llama-3.2-11B-Vision~\cite{llama}, Math-LLaVA~\cite{shi2024math}, and G-LLaVA-7B~\cite{gao2023g}.
\begin{figure*}[t]
\centering
\includegraphics[width=\textwidth]{figures/pdf/mm_scale.pdf}
\caption{The performance of our MM-Reasoner can scale up using different MM-Verifiers. We can see with different scale MM-Reasoner the MM-Verifier consistently outperform majority voting and MM-Verifier Stage1.}
\label{fig:mm_scale}
\vspace{-4mm}
\end{figure*}
\paragraph{MM-Verifier Baselines.}  
\textbf{Majority Voting:} Select the answer that appears most frequently among multiple candidate answers.  
\textbf{Qwen2-VL-7B and Qwen-VL-72B as Judgment:} We use Qwen2-VL-7B-Instruct and Qwen-VL-72B~\cite{bai2023qwenvl} as the judgment model to assess the correctness of each candidate solution. If multiple candidates are deemed correct, we apply a majority voting mechanism to determine the final answer.  

\paragraph{Benchmarks.}  
We evaluate the performance of MM-Verifier, MM-Reasoner, and the baselines on two commonly used benchmarks: MathVista~\cite{lu2023mathvista} and MathVerse~\cite{zhang2024mathverse}. Additionally, we assess the MM-Verifier on the Verify bench MathCheck (Multimodal Outcome Judging)~\cite{zhou2024your}.

\paragraph{Settings.}  
The settings include a maximum token limit of 4096, a top-k value of 5, a temperature of 0.3, and a repetition penalty of 1.05. All experiments are conducted on 8 NVIDIA H20 GPUs.

\subsection{Effectiveness of MM-Verifier and MM-Reasoner}
In this section, we demonstrate the effectiveness of both the MM-Verifier and MM-Reasoner. As shown in Figure \ref{fig:fengmian}, the MM-Verifier outperforms all other models in the MathCheck benchmark. Moreover, Tables \ref{tab:abla_mathvista} and \ref{tab:abla_mathverse} reveal that the MM-Verifier achieves strong performance on the MathVista and MathVerse benchmarks. Our results indicate that the MM-Verifier surpasses both the Majority Voting and Qwen2-VL-72B-Instruct methods. Specifically, MM-Verifier (Stage 2) delivers superior results, demonstrating its robust ability to verify answers and enhance model performance.

Furthermore, the MM-Reasoner outperforms powerful models, such as Qwen2-VL-Instruct-7B and LLaMA-3.2-11B-Vision, across all evaluation metrics. These findings clearly demonstrate that both the MM-Verifier and MM-Reasoner contribute significantly to performance improvements, underscoring their potential for addressing complex multimodal reasoning and verification tasks.

Interestingly, we observe that the performance of Qwen2-VL-72B, when used as a Judgment model, improves when verifying answers for Qwen2-VL and LLaMA-3.2-11B-Vision (from sample 4 to sample 12). However, its performance drops when verifying MM-Reasoner outputs. This discrepancy arises because conventional models struggle to verify the correctness of longer outputs, while our MM-Verifier consistently maintains robust performance. This further emphasizes the versatility of the MM-Verifier across a wide range of scenarios.

\subsection{Scalability of Our MM-Reasoner}

The results in Figure~\ref{fig:mm_scale} illustrate the scalability of MM-Reasoner with respect to the quantity of training data. As the amount of training data increases, the performance of the model consistently improves across all evaluation settings. Specifically, MM-Reasoner achieves steady performance gains when moving from 6,952 to 32,146 training samples, demonstrating its ability to effectively utilize larger datasets for better reasoning.

Additionally, both Verifier-S.1 and Verifier-S.2 show clear improvements as the training data grows, with Verifier-S.2 outperforming Verifier-S.1 in all cases. This trend highlights the effectiveness of the staged verification approach in enhancing reasoning accuracy.

These results emphasize the superiority of our method, as MM-Reasoner scales effectively with increasing training data, achieving higher performance and showcasing the robustness and adaptability of our approach.
\input{table/main_exp}
\begin{figure}[t]
\centering
\includegraphics[width=0.47\textwidth]{figures/pdf/case2.pdf}
\caption{We present a case of MM-Verifier. We can see MM-Verifier correctly verify the answer with Long COT while Qwen2-VL-72B-Instruct failed to.}
\label{fig:case}
\vspace{-7mm}
\end{figure}
\subsection{MM-Verifier and MM-Reasoner achieved SOTA Performance}
We leveraged our proposed MM-Reasoner and MM-Verifier together to enhance multimodal mathematical reasoning. Specifically, MM-Reasoner generated 12 diverse reasoning rollouts per query, while MM-Verifier systematically evaluated and filtered these rollouts, ensuring high-quality and accurate responses. This iterative verification-refinement process significantly improved reasoning precision and robustness.

As shown in Table~\ref{tab:main_exp}, our approach outperforms both open-source and closed-source MLLMs across multiple benchmarks. On the MATHVISTA dataset, our method achieves an overall accuracy of \textbf{65.3}, surpassing human performance (\textbf{60.3}) and even GPT-4o (\textbf{63.8}). Similarly, in the MATHVERSE benchmark, our method consistently achieves strong performance with an overall score of \textbf{25.7}, outperforming strong baselines like Math-LLaVA-13B (\textbf{22.9}) and LLaVA-OneVision (\textbf{20.7}). 

These results demonstrate the robustness of MM-Reasoner and MM-Verifier in improving mathematical reasoning across diverse tasks.
% \input{table/orm_bench}
% \input{table/scale_mm}

\subsection{Case Study}

Figure~\ref{fig:case} presents a case study of MM-Verifier, demonstrating its ability to successfully identify logical errors in the reasoning process through step-by-step verification. In contrast, Qwen2-VL-72B-Instruct fails to provide a step-by-step reasoning trajectory, leading to an error in detecting these mistakes. This case underscores the superior analytical capabilities of MM-Verifier.


\section{Conclusion}
In this paper, we propose two data synthesis methods: the first generates long COT verification data, while the second synthesizes long COT inference data more efficiently. We use these synthetic data to train the MM-Verifier and MM-Reasoner. Our MM-Verifier not only outperforms larger models on the MathCheck benchmark but also demonstrates superior performance against larger models on benchmarks such as MathVista and MathVerse. Additionally, the MM-Reasoner exhibits strong scalability, with performance improving as the data size increases. Furthermore, the combination of MM-Verifier and MM-Reasoner achieves impressive results on the MathVista benchmark, surpassing even GPT-4o. These findings confirm the effectiveness of MM-Verifier and MM-Reasoner in enhancing multimodal reasoning tasks and lay the foundation for future advancements in this domain.

\section{Limitations}
Due to limited funding and computational resources, we were unable to scale our MM-Verifier and MM-Reasoner to Qwen2-VL-72B. Additionally, our scalability tests were restricted to datasets of fewer than 100K samples. We plan to conduct further experiments as soon as additional computational resources become available.

\bibliography{custom}

\clearpage
\appendix



\input{table/appendix_vista}
\input{table/mcts}
\section{Performance of Naive MCTS in Multimodal Reasoning}~\label{app:MCTS_performance}

In NLP-based mathematical reasoning tasks, given a question, the Monte Carlo Tree Search (MCTS) algorithm iteratively refines its responses through continuous self-reflection, ultimately converging to an optimized answer~\cite{zhang2024accessing}. Experimental results demonstrate that this search-based approach yields strong performance. Naturally, we hypothesized that applying the same methodology to multimodal tasks would lead to similar improvements. However, as shown in Table~\ref{tab:mcts},
we compare the performance of MCTS with Majority Voting and our MM-Verifier on the MathVista benchmark. We can see simple MCTS failed to improve reasoning performance. We attribute this to the significantly higher prevalence of hallucinations in multimodal models compared to their text-only counterparts. Therefore inspired by~\cite{wang2024math, sun2024beats} we design a simulation-based tree search.

\section{Detailed Performance of MM-Verifier on Sub-tasks of MathVista}


Table~\ref{tab:appendix_vista} presents a comparative evaluation of Qwen2-VL-Instruct-7B, LLaMA-3.2-11B-Vision, and our proposed MM-Reasoner on the MathVista benchmark. The models were assessed across multiple reasoning-intensive sub-tasks, including GPS, MWP, FQA, TQA, and VQA. 

In the Sample 4 evaluation, MM-Reasoner with the MM-Verifier (Stage 2) achieved an overall accuracy of 61.5, surpassing Qwen2-VL's 59.8 and LLaMA-3.2-11B-Vision's 50.4. This trend persists across Sample 8 and Sample 12, where MM-Reasoner obtained 65.3 and 65.7 respectively, further establishing its robustness in complex multi-modal reasoning tasks. 

Furthermore, the MM-Verifier mechanism contributes significantly to the accuracy gains. Across all models, the transition from Majority Voting to MM-Verifier (Stage 2) consistently improves performance, underscoring the importance of verification-enhanced reasoning. In particular, MM-Reasoner benefits the most from this verification process, indicating that it effectively integrates verification feedback into its reasoning pipeline. Overall, these results validate the effectiveness of MM-Verify as a strong algorithm. 


\begin{figure}[t]
\centering
\includegraphics[width=0.47\textwidth]{figures/pdf/case_appendix.pdf}
\caption{We present case of MM-Verifier. We can see MM-Verifier correctly verify the answer with COT while Qwen2-VL-72B-Instruct failed to.}
\label{fig:case_appendix}
\vspace{-4mm}
\end{figure}

\section{Prompts}~\label{sec:Prompts}

This paper primarily focuses on three key prompts: (1) the prompt for generating Verify data (Figure~\ref{fig:prompt-verify}), (2) the prompt (Figure~\ref{fig:prompt-qwq}) for distilling the QwQ-32B-Preview model after converting multimodal data into a textual format, and (3) the prompt for extracting answer data (Figure~\ref{fig:prompt-extract}).

\begin{figure*}[t]
\centering
\includegraphics[width=1\textwidth]{figures/pdf/verify-prompt2.pdf}
\caption{Verify Label Prompt. This is the prompt we built with reference to~\citet{zhang2024generative}.}
\label{fig:prompt-verify}
\vspace{-2mm}
\end{figure*}

\begin{figure*}[t]
\centering
\includegraphics[width=1\textwidth]{figures/pdf/qwq-prompt2.pdf}
\caption{QwQ-32B-Preview Distill Prompt. This is the prompt we built with reference to~\citet{min2024imitate}.}
\label{fig:prompt-qwq}
\vspace{-2mm}
\end{figure*}

\begin{figure*}[t]
\centering
\includegraphics[width=1\textwidth]{figures/pdf/extract-prompt2.pdf}
\caption{Answer Extract Prompt}
\label{fig:prompt-extract}
\vspace{-2mm}
\end{figure*}

\section{More Case Study}~\label{sec:appendix_case}
As shown in Figure~\ref{fig:case_appendix}, our MM-Verifier is capable of verifying the correctness of a solution that provides only a simple answer by leveraging CoT approach. In contrast, Qwen2-VL-72B-Instruct incorrectly classifies the solution as correct, highlighting its limitations in reasoning-based verification.


\end{document}
