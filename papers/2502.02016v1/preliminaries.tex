\section{Preliminaries}\label{sec:preliminaries}
% \yuxuan{this section looks so similar to the DiffCsp. I would like to suggest do modification. I will reformat, and plz check.}

% \textbf{Crystal Representation with Fractional Coordinate System}
% The 3D crystal structure is essentially an infinite periodic arrangement of atoms and can be represented by the smallest repeating unit, \emph{i.e.}, unit cell. To better reflect the periodicity of the crystal structure, the fractional coordinate system contrast to the Cartesian coordinate system is widely applied to represent the structural information~\cite{jiao2023crystal} \yuxuan{add citations of related parts}. With a fractional coordinate system, a unit cell could be determined with the triplet $\mathcal{M}=(\boldsymbol{A},\boldsymbol{F},\boldsymbol{L})$. Denoting the number of atoms in the unit cell as $N$, $\boldsymbol{A}=(a_1,a_2,\dots,a_N)$ denotes the one-hot representations for atom type of the $N$ atoms; $\boldsymbol{L}=[\boldsymbol{l}_1,\boldsymbol{l}_2,\boldsymbol{l}_3]\in \mathbb{R}^{3\times 3}$ denotes the lattice matrix where every column vector is the periodic basic vector for the crystal and also served as the base vector in the fractional coordinate systems; $\boldsymbol{F}=[\boldsymbol{f}_1,\boldsymbol{f}_2,\dots,\boldsymbol{f}_N]\in [0,1)^{3\times N}$ is the fractional coordinates of atoms. The transformation between the fractional coordinates $\boldsymbol{F}$ and the Cartesian coordinates $\boldsymbol{X}$ could be illustrated as $\boldsymbol{X}=\boldsymbol{L}\boldsymbol{F}\in \mathbb{R}^{3\times N}$. Based on the above representations, the symmetry of crystal geometry includes periodic translational invariance of $\vF$ and rotational invariance of $\vL$ \cite{jiao2023crystal} the detailed definitions of which are provided in \cref{appd:geometric_invar}. 

% \hanlin{How about giving very brief introduction here and refer to appendix }
% \textbf{Symmetry of the Crystal Distribution} 
% We highlight the invariance property that the desired density function $P(\boldsymbol{A},\boldsymbol{F},\boldsymbol{L})$ over the crystal space, should hold following~\cite{jiao2023crystal}: \emph{Permutation Invariance}, \emph{i.e.} $P(\boldsymbol{A},\boldsymbol{F},\boldsymbol{L}) = P(\Pi \boldsymbol{A},\Pi 
%  \boldsymbol{F},\boldsymbol{L})$ for any permutation $\Pi$; \emph{Rotational Invariance over $\boldsymbol{L}$}, \emph{i.e.},  $P(\boldsymbol{A}, \boldsymbol{R}\boldsymbol{L}, \boldsymbol{F})=P(\boldsymbol{A},\boldsymbol{L}, \boldsymbol{F})$ for any rotation matrix $\boldsymbol{R} \in \mathbb{R}^{3 \times 3} $ and $\boldsymbol{R}^{\top} \boldsymbol{R}=\boldsymbol{I}$; \emph{Periodic Translational Invariance over $\boldsymbol{F}$}, $P(\boldsymbol{A},\boldsymbol{L},w(\boldsymbol{F}+\boldsymbol{t} \mathbf{1}^{\top}))=P(\boldsymbol{A},\boldsymbol{L}, \boldsymbol{F})$ for  any translation  $\boldsymbol{t} \in \mathbb{R}^{3 \times 1}$ and $w(\boldsymbol{F})=\boldsymbol{F}-\lfloor\boldsymbol{F}\rfloor \in[0,1)^{3 \times N}$ which ensures the fractional coordinates lies in the range of $[0,1)$. We leave more discussions in Appendix~\yuxuan{help check,}




\textbf{Crystal Representation and Related Manifold} Crystals can be represented as a structure composed of infinite, periodic and repeating unit cells defined by a triplet $\mathcal{M}=(\boldsymbol{A},\boldsymbol{F},\boldsymbol{L})$. Denote the number of atoms in the unit cell as $N$, $\boldsymbol{A}=(\vec{a}_1,\vec{a}_2,\dots,\vec{a}_N)\in \mathcal{S}^{K\times N}$ is the representation of atom types with the length of vocabulary $K$, and every such one-hot discrete variable locates in the simplex $\mathcal{S}^K$ represented by:
\begin{equation}
\mathcal{S}^K\defeq\{\boldsymbol{s}\in \mathbb{R}^K|\sum_{i=1}^{K}s_i=1,s_i\geq0, i=1,\dots,K\}
\end{equation} 
which requires the designed generative path for $\boldsymbol{A}$ should be well defined on the simplex.
Following \citet{jiao2023crystal}, $\boldsymbol{F}=[\boldsymbol{f}_1,\boldsymbol{f}_2,\dots,\boldsymbol{f}_N]\in [0,1)^{3\times N}$ is the fractional coordinates of atoms located in quotient space $\R^{3\times N} / \mathbb{Z}^{3\times N}$ equivalent to the hypertorus $\mathbb{T}^{3\times N}$~\citep{jing2022torsional}. The hypertorus $\mathbb{T}^{3\times N}$ can be represented as the Cartesian product of  $3\times N$ toruses $\mathbb{T}^1$:
\begin{equation}
    \mathbb{T}^1\defeq \{\vz\in\mathbb{R}^2:||\vz||=1\}
\end{equation}
% \oyyw{$\mathbb{T}^1$ denotes 1 
%  or 2 dimension here?}\hanlin{1 degree of freedom}
And $\boldsymbol{L}=[\boldsymbol{l}_1,\boldsymbol{l}_2,\boldsymbol{l}_3]\in \mathbb{R}^{3\times 3}$ denotes the lattice matrix, every column vector of which is the periodic basic vector of the crystal. We can get the Cartesian coordinates representation of unit cell's atom coordinates $\bm{X}$ by $\boldsymbol{X}=\boldsymbol{L}\boldsymbol{F}\in \mathbb{R}^{3\times N}$. The ideal infinite periodic crystal structure of $\mathcal{M}$ can be represented by $\{(a_i',\boldsymbol{x}_i')|a_i'=a_i,\boldsymbol{x}_i'=\boldsymbol{x}_i+\boldsymbol{L}\boldsymbol{k},\forall\boldsymbol{k}\in \mathbb{Z}^{3\times 1}\}$. Based on the above notations, the symmetry of crystal geometry is defined as \emph{periodic E(3) invariance}\footnote{Actually some crystal structures are chiral \citep{flack2003chiral} and their structures are SE(3) invariant. In this paper, we follow \citet{jiao2023crystal} to build an E(3) invariant framework for fair comparison, while the SE(3) equivariance can be simply achieved by transforming an E(3) invariant network into an SE(3) equivariant network by excluding invariance to reflections in the Markov chain. } \citep{jiao2023crystal}, including periodic translational invariance of $\vF$ and rotational invariance of $\vL$ (details in \cref{appd:geometric_invar}) . 
% We define the space of $\mathcal{M}$ as the Cartesian product of the space of each modality over all possible $N$ 
% \begin{equation}
%     \mathcal{M} \in \mathcal{C} \defeq \{\mathcal{S}^{K\times N}\times \mathbb{T}^{3\times N}\times \mathbb{R}^{3\times 3}, N = 1,2,\dots,\infty\}
% \end{equation} which we aim to model in this paper. 

\textbf{Bayesian Flow Networks}
% \yuxuan{Generally this part is long and lacks of content. I will rewrite this part.}
% As a new class of generative model, Bayesian Flow Networks (BFN) \citep{bfn} models the generation process of data $\bold{x}$ as a message transmission process between sender and receiver.
Different from the well-established SDE-based approaches, \emph{e.g.} Diffusion Models~\citep{ddpm,sde,song2020score} and ODE-based approaches \emph{e.g.} Flow Matching~\citep{flowmatching}, Bayesian Flow Networks define a generative process driven by consecutive Bayesian updates on noised samples from the uninformative prior distribution $\vtheta_{0}$ to posteriors $\vtheta_i$ with higher confidence and more information.


To define the consecutive Bayesian update process, Bayesian Flow Networks contain a process to add noise to the clean data samples which is an analogy to the forward process in Diffusion Models. In BFN, such a process is explicitly defined by the so-called \emph{sender distribution} $p_S(\mathbf{y}|\mathbf{x};\alpha)$ where $\alpha$ is the parameter of sender distribution which corresponds to noise level, \emph{e.g.} variance for Gaussian-formed $p_S$. 

 % BFN tends to create a procedure that gradually gains the information of ground truth data $\rvx$ for providing the training signal. 
 BFN aims to create a procedure that gradually acquires information from the ground truth data \(\rvx\) to provide a training signal. To this end, the framework will sample a series of noisy samples $\bold{y}_1, \bold{y}_2, ..., \bold{y}_n$  independently from the \emph{sender distribution} $p_S$ with various accuracy levels $\alpha_1, \alpha_2, ..., \alpha_n$. Based on samples, the framework would simulate an auto-regressive update of $\vtheta$ based on the \emph{Bayesian update function} to reflect an information gathering process from prior to data as:
\begin{align}
    \label{eq:bayesian_update}
    \vtheta_i = h(\boldsymbol{\theta}_{i-1},\bold{y}_{i},\alpha_{i})
\end{align}


The neural network $\Psi$ is trained in a teacher-forcing fashion: approximate the sender distribution $p_S(\cdot| \bold{x};\alpha_i)$ generating $\bold{y}_i$ according to $\btheta_{i-1}$, which has integrated the information of $(\bold{y}_1, \cdots, \bold{y}_{i-1})$ through \cref{eq:bayesian_update}. The network $\Psi$ hence takes  $\vtheta$ obtained by Bayesian update as input, and the distribution implied by $p_I(\cdot|\vtheta)$ is termed \emph{input distribution}. The network output $\Psi(\vtheta)$ is interpreted as the parameter of an updated distribution $p_O(\cdot|\Psi(\vtheta))$ over sample space, referred to as \emph{output distribution}. Combining the network output $\Psi(\vtheta)$ and the sender distribution, we obtain the approximation to $p_S\left(\bold{y}_i \mid \bold{x} ; \alpha_i\right)$ as:
\begin{equation}
\label{eq:abs_rece}
    p_R(\bold{y}_i\mid \vtheta_{i-1},\Psi,\alpha_i)= \mathbb{E}_{p_O(\bold{x}' |\Psi(\boldsymbol{\vtheta}_{i-1}))}p_S\left(\bold{y}_i \mid \bold{x}' ; \alpha_i\right)
\end{equation}
Such distribution is named as \emph{receiver distribution} $p_R$. Note that $\vtheta_{i-1}$ can be seen as a deterministic function mapping, given $(\bold{y}_1,\cdots,\bold{y}_{i-1})$  as $\vtheta_{i-1} = f(\bold{y}_1,\cdots,\bold{y}_{i-1})$ based on the deterministic update function in \cref{eq:bayesian_update}. By combining the objectives of different timesteps and taking expectation over trajectories, we obtain the training objective of BFN as:
\begin{align}\label{eq:loss_n}
    \mathcal{L}(\Psi) = \mathbb{E}_{\x \sim p_{\text{data}}}\mathbb{E}_{\Pi_{i=1}^n p_S(\bold{y}_i|\bold{x},\alpha_i)}  D_{K L} (p_S(\bold{y}_i|\bold{x},\alpha_i)||p_R(\bold{y}_i\mid \vtheta_{i-1},\Psi,\alpha_i))
\end{align}
Here $p_{\text{data}}$ is the empirical distribution. Detailed derivation and discussion are provided in \cref{appd:bfn_cir}. Additionally, we provide toy examples with minimal components illustrating how BFNs work in our code repository.
% \yuxuan{fix.}




% For the sender with a sample $\bold{x}=(x^{(1)},x^{(2)},\dots,x^{(D)})\in \mathcal{X} \sim p_{data}(\bold{x})$ and time steps $i=1,\dots, n$ , he independently corrupts $\bold{x}$ with a predefined accuracy schedule $\alpha_i$ and got noised samples $\bold{y}_1, \bold{y}_2, ..., \bold{y}_n$ which can be represented as $
%     p_S(\bold{y}|\bold{x};\alpha)=\Pi_{d=1}^D p_S(y^{(d)}|x^{(d)};\alpha)
% $
% where $\alpha\in \mathbb{R}^+$ is the parameter to control the signal-to-noise ratio such that $\alpha=0$ samples are pure uninformative noise and the samples are becoming more informative as $\alpha$ increases. 

% For the receiver, he has a belief parameter $\btheta_i$ about ground truth $\bold{x}$ termed \textit{input distribution} 
% \begin{equation}\label{eq:input_dist}
% p_I(\bold{x}|\btheta)\defeq \Pi_{d=1}^{D}p_I(x^{(d)})|\theta^{(d)})
% \end{equation}
%  With a prior $\btheta_0$, at each time step $i$, he observes noised sample $\bold{y}_i$ and does a Bayesian update $h$ independently for each dimension.
% % $\btheta_i$ parameterizes the receiver's belief about ground truth $\bold{x}$ termed \textit{input distribution} expressed as
% % \begin{equation}
% %     p_I(\bold{x}|\btheta)\defeq \Pi_{d=1}^{D}p_I(x^{(d)})|\theta^{(d)})
% % \end{equation}
% % where $p_I(x^{(d)})|\theta^{(d)})$ is parameterized as normal distribution for continuous data, categorical distribution for discrete data in \cite{bfn}, and, proposed in this paper, von Mises distribution for circular data.
% However, $\btheta_i$ does not consider the interdependence between dimensions and the receiver uses the network $\Psi$ to model this interdependency. According to the network output, the receiver rebuilds his belief about $\bold{x}$ over the sample space expressed by \textit{output distribution},
% \begin{equation}
%     p_O(\bold{x} | \btheta, t)=\Pi_{d=1}^D p_O(x^{(d)}|\Psi^{(d)}(\btheta,t))
% \end{equation}
% Now with output distribution, the receiver can give his estimation of the probability of observed $\bold{y}_i$ termed \textit{receiver distribution} $p_R$ expressed as 
% \begin{equation}\label{eq:receiver_dist}
% p_R(\bold{y}|\btheta;t,\alpha)=\underset{p_O(\bx'|\btheta;t)}{\mathbb{E}} p_S(\bold{y}|\bx';\alpha) 
% \end{equation}
% The network is trained to minimize the KL divergence between sender and receiver distribution over all time steps and data distribution. 
% \begin{equation}\label{eq:loss_n}
%     \mathcal{L}^n(\x)=\underset{p(\btheta_0,\dots,\btheta_{n-1})}{\mathbb{E}}\sum_{i=1}^n D_{KL}(p_S(\cdot |\bold{x};\alpha_i)||p_R(\cdot |\btheta_{i-1};t_{i-1},\alpha_i))
% \end{equation} 
% % where $p(\btheta_1,\dots,\btheta_{n-1})$ is auto-regressively factorized by series of Bayesian update marginalizing $\bold{y}_i$
% % \begin{equation}
% %     p(\btheta_1,\dots,\btheta_{n})=\Pi_{i=1}^n p_U(\btheta_i|\btheta_{i-1},\bold{x};\alpha_i)
% % \end{equation}
% % where 
% % \begin{equation}
% % p(\btheta_1,\dots,\btheta_{N-1})=\Pi_{i=1}^n p_U(\btheta_i|\btheta_{i-1},\bold{x};\alpha_i)
% % \end{equation}
% % Optimizing $\mathcal{L}^n$ is equivalent to minimizing the \textit{negative variational lower bound} $\mathcal{L}_{VLB}$ defined by the complete information transmission process,
% % \begin{align}
% %     \mathcal{L}^{VLB}(\x)=-VLB(\bold{x})&=D_{KL}(q||p)-\underset{\bold{y}_1,\dots,\bold{y}_{n}}{\mathbb{E}}\ln p(\bold{x}|\bold{y}_1,\dots,\bold{y}_{n})=\mathcal{L}^n+\mathcal{L}^r
% % \end{align}
% % where $\mathcal{L}^r$ is indirectly optimized by minimizing $\mathcal{L}^n$. 
% The instantiation of $p_I,p_O$ and $h(\cdot)$ need to consider the resembled manifold of the data and we left it in the next section. 

% Such a framework provides a unified modeling approach in the parameter space for manifold-hybrid data and it has been successfully applied to molecular generation \cite{song2023unified}. However, the periodicity of crystals makes the parameter space non-Euclidean which brings inherent theoretical issues not considered in the original BFN \citep{bfn}. In the next section, we overcome such issues and reconsider the Bayesian Flow Networks for such a non-Euclidean space. 
% \oyyw{这里直接However有些突兀，建议逻辑：先写BFN比起其他generative models的好处，也就是我们用BFN的动机，注意和introduction呼应； 接着应该说BFN应用在crystal上是有挑战的，我们将在next section介绍并解决这些挑战。}
% break $\btheta$ into three parts $\btheta^M=(\btheta^A,\btheta^L,\btheta^F)$ and build Bayesian flow separately but with the same entropy decrease rate from prior to data distribution.