\appendix
\clearpage
\section{Bayesian Flow Networks for Circular Data}\label{appd:bfn_cir}
% \yuxuan{Could add some synthetic cases, to provide more intuitive justification}
In this section, we provide a detailed derivation of Bayesian flow networks considering periodicity. 
\subsection{Circular Data and von Mises Distribution}\label{appd:vm_intro}
\begin{figure}
    \centering
    \includegraphics[width=0.8\textwidth]{imgs/vm_params.pdf}
    \caption{Depiction of von Mises distributions with different directions parameters $m$ and concentration parameters $c$. The parameter $m$ denotes the central location about which the distribution is centered, while $c$ functions as a measure of the distribution's concentration. When $c = 0$, the distribution is uniform on the circle. As $c$ increases, the distribution becomes more concentrated around the value $m$, with $c$ serving as a measure of this concentration. In the limit as $c \rightarrow +\infty$, the distribution converges to $\delta(m)$, a Dirac delta distribution centered at $m$.  }
    \label{fig:vm_alpha}
\end{figure}
One-dimensional circular data $x$ refers to observations of random variables supporting on the circumference of the unit circle defined in directional statistics \citep{mardia2009directional,ley2017modern}. This space can be represented by the torus:
\begin{equation}
     \mathbb{T}^1\defeq \{\vz\in\mathbb{R}^2:||\vz||=1\}
\end{equation}

For $n$-dimensional data $\vx\in \mathbb{R}^n$, the set of $\vx$ with every dimension located in $\mathbb{T}^1$ form a compact Riemannian manifold named hyper-torus $\mathbb{T}^n$ formally.
% = \underbrace{\mathbb{T}^1 \times \mathbb{T}^1 \dots \times \mathbb{T}^1}_{n\ \text{times}} 
% \mathbb{T}^n = \underbrace{\mathbb{T}^1 \times \mathbb{T}^1 \dots \times \mathbb{T}^1}_{n\ \text{times}}
% Circular data is commonly seen in life and science areas such as the wind directions, dihedral angles between certain atoms of amino acids, and the fractional coordinate of atoms located in crystal unit cells in this paper. % can be deleted if no space

Wrapped normal distribution used in \citet{jiao2023crystal} and von Mises distribution used in this paper are both circular distributions defined in this space, the probability density function of von Mises distribution with mean direction parameter $m$ and concentration parameter $c$ is 
\begin{equation}
f(x|m,c)=vM(x|m,c)=\frac{\exp(c\cos(x-m))}{2\pi I_0(c)}
\end{equation}
where $I_0(c)$ is the modified Bessel function of the first kind of order 0 as the normalizing constant. The parameters $m$ and $1/c$ are analogous to mean $\mu$ and variance $\sigma^2$ in the normal distribution: 1) $m$ represents the central location around which the distribution is clustered, while $c$ serves as a measure of concentration. 2) We give a depiction of von Mises distributions with different directions parameters in \cref{fig:vm_alpha}. When $c$ equals zero, the distribution is uniform. As $c$ becomes large, the distribution becomes tightly concentrated around the value $m$, with $c$ quantifying this concentration. In the limit as $c\rightarrow+\infty$, the distribution becomes a Dirac delta distribution centered at $m$. Its support can be chosen as any interval of length $2\pi$ and we in this paper choose $[-\pi,\pi)$. Note that the fractional coordinate can be transformed to this interval easily by a linear transformation $g(x)=2\pi x-\pi$. For this modeled interval, the map function from $\R$ to $[-\pi,\pi)$ is 
\begin{equation}
w_{[-\pi,\pi)}(x)=(x-\pi)\%2\pi-\pi=x+2\pi k, \exists k\in \mathbb{Z}
\end{equation}
Such map function is equivalent to the map function $w(x)=x-\lfloor x\rfloor=x+k, \exists k\in \mathbb{Z}$ used in \citet{jiao2023crystal} if we choose the modelled interval as $[0,1)$. Then, we can prove that the probability density distribution of von Mises distribution is equivariant to the periodic translation transformation:
\begin{align}
    \forall t\in \R,\quad f(w_{[-\pi,\pi)}(x+t)|w_{[-\pi,\pi)}(m+t),c)&=f(x+2\pi k'|m+2\pi k+t,c)\nonumber\\
    &=\frac{\exp(c\cos(x+2\pi k'-(m+2\pi k+t)))}{2\pi I_0(c)}\nonumber\\
    &=\frac{\exp(c\cos((x-m))}{2\pi I_0(c)}\nonumber\\
    &=f(x|m,c) \label{eq:vm_period_equi}
\end{align}
The differential entropy of von Mises distribution with mean direction parameter $m$ and concentration parameter $c$ is
\begin{equation}\label{eq:vm_entropy}
    H(vM(x|m,c))=-c\frac{I_1(c)}{I_0(c)}+\ln[2\pi I_0(c)]\quad\text{\citep{mardia2009directional}}
\end{equation}

We opt for von Mises distribution rather than the wrapped normal distribution used in \citet{jiao2023crystal,jing2022torsional} mainly because of the Bayesian conjugacy of von Mises distribution the posterior of which is conjugate to the prior distribution if the likelihood is parameterized as von Mises distribution which is the fundamental basis of constructing a Bayesian flow. Interestingly, there is an intriguing connection between von Mises distribution and crystal force field that the von Mises distribution is the stationary distribution of a drift and diffusion process on the circle in a harmonic potential corresponding to the harmonic force field of crystals \citep{risken1996fokker}.
\subsection{Input Distribution \texorpdfstring{$p_I(\ \cdot\ |\bm{\theta})$}{} and Sender Distribution \texorpdfstring{$p_S(\vy | \vx;\alpha$)}{}}
For circular data $\bold{x}$ which locates in a quotient space $\mathbb{T}^n=\mathbb{R}^{3\times N}/[-\pi,\pi)^{3\times N}$, we define the input distribution of the Bayesian Flow Networks as independently factorized von Mises distribution over the interval $[-\pi,\pi)$.
\begin{equation}
    \btheta \defeq \{\bold{m},\bold{c}\}
\end{equation}
\begin{equation}
    p_I( \bold{x} | \theta)\defeq\Pi_{d=1}^D vM(x^{(d)}|m^{(d)},c^{(d)})
\end{equation}
% where $\theta=(\theta^{(1)},...,\theta^{(D)})$ with $\theta^{(d)}=\{m^{(d)},c^{(d)}\}$ 
where $m^{(d)}\in [-\pi,\pi)$ and $c^{(d)}\in [0,\infty)$. 


In this paper, to ensuring the periodic translational invariance, the prior parameter of \modelname's Bayesian flow is chosed as 
\begin{equation}
p(\bthetaF_0) \defeq \{vM(\vm_0|\vec{0}_{3\times N},\vec{0}_{3\times N}),\delta(\vc_0-\vec{0}_{3\times N})\} = \{U(\vec{0},\vec{1}),\delta(\vc_0-\vec{0}_{3\times N})\}
\end{equation}
where $\bold{0}$ is the length of $D$ vector whose entries are all $0$. Note that this input prior $\btheta_0$ defines a multivariate uniform distribution
\begin{equation}
    p_I( \bold{x} | \btheta_0)=\Pi_{d=1}^D vM(x^{(d)}|0,0)=\Pi_{d=1}^D U(-\pi,\pi)
\end{equation}

which ensures \textit{periodic E(3) invariance} of the prior distribution. 

The sender space $\mathcal{Y}$ is identical to the data space $\mathcal{X}$ for circular data. And the sender distribution is von Mises distribution centered on $\vx$ with concentration parameter $\alpha$ represented by
\begin{equation}
    p_S (\bold{y} |\bold{x};\alpha) =\Pi_{d=1}^{D}vM(y^{(d)}|x^{(d)},\alpha):=vM(\bold{y}|\bold{x},\alpha)
\end{equation}
\subsection{Bayesian Update Function \texorpdfstring{$h(\parsnt{i-1}, \y, \alpha)$}{} and Bayesian Update Distribution \texorpdfstring{$\update(\cdot \mid \parsn, \x; \alpha)$}{}}\label{apdx:bayesian_update_function}
For the receiver, given his last univariate belief parameterized by von Mises distribution with parameter $\theta_{i-1}=\{m_{i-1},\ c_{i-1}\}$, he now observes a sample $y$ from sender distribution with unknown $x$ and known $\alpha$. Now, by Bayesian theorem,
% \quad (p(y) \text{ is constant for }p(x))
\begin{align}
    p(x|y;\alpha,m_{i-1},c_{i-1})&=\frac{p(y|x;\alpha)p(x;m_{i-1},c_{i-1})}{p(y)}\\
    &\propto p(y|x;\alpha)p(x;m_{i-1},c_{i-1})\\
    &= vM(y|x,\alpha)vM(x|m_{i-1},c_{i-1})\\
    &\propto \exp\{{\alpha \cos(x-y)+c_{i-1}\cos(x-m_{i-1})}\} 
\end{align}
The last expression has the form of a von Mises distribution in $x$ and hence:
\begin{equation}
    p(x|y;\alpha,m_{i-1},c_{i-1})=vM(x;m_i,c_i)
\end{equation}
where 
\begin{equation}
m_i=\text{atan2}(\alpha\sin y+c_{i-1}\sin m_{i-1}, {\alpha\cos y+c_{i-1}\cos m_{i-1}})
\end{equation}
\begin{equation}
c_i =\sqrt{\alpha^2+c_{i-1}^2+2\alpha c_{i-1}\cos(y-m_{i-1})}
\end{equation}
We refer readers interested in more detailed deduction to \citet{mardia1976bayesian,guttorp1988finding}. Defining the notation for scaler $x$ in circular space as $\dotx\defeq[\cos x,\sin x]^T$, these two expressions will be much simpler, intuitive and more similar to the Gaussian form:
\begin{equation}
    h(\{\dotm_{i-1},c\},\doty,\alpha)=\{\dotm_i,c_i \}
\end{equation}
where
\begin{equation}\label{eq:update_mi}
\dotm_{i}=\frac{\alpha \doty+c_{i-1}\dotm_{i-1}}{c_i}
\end{equation}
\begin{equation}\label{eq:update_ci}
c_i=||\alpha\doty+c_{i-1}\dotm_{i-1}||_2
\end{equation}

 The Bayesian update distribution $\update(\cdot \mid \parsn, \x; \alpha)$ could be obtained by marginalizing $\y$:
\begin{equation}
p_U(\theta'|\theta,\bold{x};\alpha)=\mathbb{E}_{p_S(\bold{y}|\bold{x};\alpha)}\delta(\theta'-h(\theta,\bold{y},\alpha))=\mathbb{E}_{vM(\bold{y}|\bold{x},\alpha)}\delta(\theta'-h(\theta,\bold{y},\alpha))
\end{equation}
% Optionally include supplemental material (complete proofs, additional experiments and plots) in appendix.
% All such materials \textbf{SHOULD be included in the main submission.}

\subsection{\textit{Non-Additive} Accuracy Issue }
Although all cases including continuous and discrete data are proven to enjoy the so-called additive accuracy property considered in \citet{bfn} defined as:
\begin{align}
\update(\parsn'' \mid \parsn, \x; \alpha_a+\alpha_b) = \E_{\update(\parsn' \mid \parsn, \x; \alpha_a)} \update(\parsn'' \mid \parsn', \x; \alpha_b)\label{eq:additive}.
\end{align}
this property does not hold for von Mises distribution. The untenability of this property for von Mises distribution can be checked out by considering two steps Bayesian updates with prior $\btheta = \{\bold{0},\bold{0}\}$, $\alpha_a,\alpha_b, \y_a, \y_b$,
\begin{align}
    & \update(c'' \mid \parsn, \x; \alpha_a+\alpha_b)  = \delta(c-\alpha_a-\alpha_b) \\
     \ne & \mathbb{E}_{p_U(\parsn' \mid \parsn, \x; \alpha_a)} \update(c'' \mid \parsn', \x; \alpha_b) = \mathbb{E}_{vM(\bold{y}|\bold{x},\alpha_a)}\mathbb{E}_{vM(\bold{y}|\bold{x},\alpha_b)}\delta(c-||\alpha_a \y_a+\alpha_b \y_b||)
\end{align}
Consequently, the Bayesian flow distribution does not equal to one-step Bayesian update distribution with $\beta(t)$:
\begin{align}
\flow(\parsn \mid \x ; t) \ne \update(\parsn \mid \parsnt{0}, \x; \beta(t))\label{param_flow_dist}.
\end{align}
With an accuracy schedule $\alpha_1,\alpha_2,\dots,\alpha_n$ and $\beta(t)=\sum_{j=1}^{t_i} \alpha_j$, this untenability will cause the incongruity between sender's accumulated accuracy $\beta(t)$ and the confidence of receiver's belief $c_i$ over his location parameter $m_i$. Hence we should differentiate the sender's accuracy schedule $\alpha_i$ and the receiver's belief confidence$c_i$. 
% \begin{align}
% \E_{\update(\parsnt{1}\mid\parsnt{0},\x;\alphat{1})}\E_{\update(\parsnt{2}\mid\parsnt{1},\x;\alphat{2})}\dots\E_{\update(\parsnt{n-1}\mid\parsnt{n-2},\x;\alphat{n-1})}\update(\parsnt{n} \mid \parsnt{n-1},\x;\alphat{n} ) = \update\left(\parsnt{n} \mid \parsnt{0}, \x; \sum_{i=1}^n \alphat{i}\right).
% \label{updateseq}
% \end{align}
And $c_i$ is no longer a function but a \textit{distribution} over $t_i$. In consequence, we should define the Bayesian flow distribution parameterized by received sender's accuracies $\alpha_1,\alpha_2,\dots,\alpha_i$ rather than $t$. Furthermore, the information of receiver confidence $c_i$ should be part of the network input as well. 

\subsection{Bayesian Flow Distribution \texorpdfstring{$p_F(\btheta|\x;\alpha_1,\alpha_2,\dots,\alpha_i)$}{}  and Sender Accuracy Schedule \texorpdfstring{$\alpha_i$}{}}
\begin{equation}\label[equation]{eq:cirflow1}
    p_F(\btheta|\x;\alpha_1,\alpha_2,\dots,\alpha_i)=\E_{\update(\parsnt{1} \mid \parsnt{0}, \x ; \alphat{1})}\dots\E_{\update(\parsn \mid \parsnt{i-1}, \x ; \alphat{i})} \update(\parsnt | \parsnt{i-1},\x;\alphat{i} )
\end{equation}
The original definition of Bayesian flow distribution in \cref{eq:cirflow1} provides an iterative algorithm to sample from $p_F$ but which practically is slow resulting the training unaffordable. In fact, noticing the "additive" property of $c_i\dotm_i$ by \cref{eq:update_mi,eq:update_ci}, we can sample from $p_F$ without iteration:
\begin{equation}\label{eq:appd_cirflow_equiv}
p_F(\vec{m}|\x;\alpha_1,\alpha_2,\dots,\alpha_i)=\E_{vM(\y_1|\x,\alpha_1)}\dots\E_{vM(\y_i|\x,\alpha_i)} \delta(\vec{m}-\text{atan2}(\sum_{j=1}^i \alpha_i \cos \y_i,\sum_{j=1}^i \alpha_i \sin \y_i))
\end{equation}
\begin{equation}\label{eq:appd_cirflow_equiv2}
p_F(\vec{c}|\x;\alpha_1,\alpha_2,\dots,\alpha_i)=\E_{vM(\y_1|\x,\alpha_1)}\dots\E_{vM(\y_i|\x,\alpha_i)} \delta(\vec{c}-||(\sum_{j=1}^i \alpha_i \cos \y_i,\sum_{j=1}^i \alpha_i \sin \y_i)||_2)
\end{equation}
\cref{eq:appd_cirflow_equiv,eq:appd_cirflow_equiv2} provides an algorithm allowing sampling from $p_F$ by pure tensor operations without simulating the flow iteratively. Next, we can define the entropy of the receiver's belief as $H(t)$:
\begin{align}
    H(t)&\defeq \E_{p_F(\btheta|\x;\alpha_1,\alpha_2,\dots,\alpha_i)}H(p_I(~\cdot|\btheta))\\
&=\E_{p_F(c_i|\x;\alpha_1,\alpha_2,\dots,\alpha_i)}-c_i\frac{I_1(c_i)}{I_0(c_i)}+\ln[2\pi I_0(c_i)], \text{where}~i=nt
\end{align}
To ensure the information coherence between modalities, we choose to find a sender accuracy schedule to make the receiver's belief entropy $H(t)$ linearly decrease with predefined $c_n$. Formally, we would like to find an accuracy schedule $\alpha_i$ such that
\begin{equation}\label{eq:linear_entropy}
    H(t)=(1-t)H(0)+tH(1)=\ln 2\pi -c_n\frac{I_1(c_n)}{I_0(c_n)}+\ln[2\pi I_0(c_n)]
\end{equation}
Note that \cref{eq:linear_entropy} can not be solved analytically but we can solve it numerically by firstly getting the target sender's accumulated accuracy $\beta(t)$ via binary search due to the monotonicity of \cref{eq:vm_entropy}. Next, we could iteratively search $\alpha_i$ from $i=1$ to $i=n$ by matching the average accuracy toward $c_i$. This process could be done only once and the resultant $\alpha_i$ can be cached for each pre-confirmed hyper-parameter $(c_n, n)$ pair.

\subsection{Output Distribution \texorpdfstring{$\out(\cdot \mid \parsn;t)$}{} and Receiver Distribution \texorpdfstring{$\rec(\cdot \mid \parsn; \alpha,t)$}{}}
Given samples $\btheta=\{\vm,\vc\}$ from Bayesian flow distribution as input, the receiver uses network output $\net(\btheta,t)$ to rebuild his belief over ground truth $\vx$ termed output distribution. Following \citet{bfn}, we parameterize $\out$ using $\bold{\hat{x}}(\boldsymbol\theta,t)=\net(\btheta,t)$ to be $\delta$ prediction of $\x$:
\begin{equation}
    p_O(\bold{x}|\boldsymbol\theta;t)=\delta(\bold{x}-\bold{\hat{x}}(\boldsymbol\theta,t))
\end{equation}
Therefore, the receiver distribution is:
\begin{equation}
p_R(\bold{y}|\boldsymbol\theta;\alpha,t)=\mathbb{E}_{p_O(\bold{x}'|\theta;t)}p_S(\bold{y}|\bold{x}';\alpha)=vM(\bold{y}|\hat{\bold{x}}(\boldsymbol\theta,t),\alpha)
\end{equation}

\subsection{Discrete-Time Loss \texorpdfstring{$L^{n}(\x)$}{}}\label{appd:cir_loss}
From \citet{kitagawa2022kldvm}, the KL divergence between $vM(m_1,c_1)$ and $vM(m_2,c_2)$ is
\begin{equation}\label{eq:kld_vm}
    D_{KL}(vM(m_1,c_1)||vM(m_2,c_2))=-\ln\frac{I_0(c_1)}{I_0(c_2)}+\frac{I_1(c_1)}{I_0(c_1)}(c_1\dotm_1-c_2\dotm_2)'\dotm_1
\end{equation}
From \cref{eq:loss_n}, the discrete-time loss for circular data is
\begin{align}\label{disc_t_loss_exp}
L^{n}(\x) &= n \E_{i \sim \ui{n}, \flow(\parsn \mid \x ; \senderacc)} \kl{\sender{\cdot}{\x ; \alphat{i}}}{\rec(\cdot \mid \parsn; t_{i-1}, \alphat{i})}\\
&=n \E_{i \sim \ui{n}, \flow(\parsn \mid \x ; \senderacc)} \frac{I_1(\alpha_i)}{I_0(\alpha_i)}\alpha_i(1-\cos(\bold{x}-\hat{\bold{x}}(\theta_{i-1},t_{i-1}))
\end{align}
Continuous-time loss is not tractable because the Bayesian flow distribution is not analytical due to the non-additive accuracy property.

\section{Proof of Propositions}\label{appd:geometric_invar}
In this section, we first prove the crystal geometric invariance of \modelname. Crystals remain the same under transformations including permutation, orthogonal transformation, and periodic translation defined as follows:
% permutation variance
\begin{definition} [Permutation Invariance \citep{jiao2023crystal}]
\label{De:pi}
For any permutation matrix $\mP$, $p(\mL,\mF,\mA)=p(\mL,\mF\mP, \mA\mP)$, \emph{i.e.}, changing the order of atoms will not change the distribution.  
% \oyyw{这里$S_N$没有解释。我这里建议这里就不要用公式了,文字描述就能很清晰,而且后续也不会用到这些公式了。}
\end{definition}
% O(3)
\begin{definition} [O(3) Invariance \citep{jiao2023crystal}]
\label{De:oi}
For any orthogonal transformation $\mQ\in\R^{3\times 3}$ satisfying $\mQ^\top\mQ=\mI$, $p(\mQ\mL,\mF,\mA)=p(\mL,\mF,\mA)$, namely, any rotation/reflection of $\mL$ keeps the distribution unchanged.   
\end{definition}
% periodic translation
\begin{definition}[Periodic Translation Invariance \citep{jiao2023crystal}] 
\label{De:PTI}
For any translation $\vt\in\R^{3\times1}$, $p(\mL, w(\mF + \vt\vone^\top),\mA)=p(\mL, \mF ,\mA)$, where the function $w(\mF)=\mF - \lfloor\mF\rfloor \in [0,1)^{3\times N}$ returns the fractional part of each element in $\mF$, and $\vone\in\R^{3\times1}$ is a vector with all elements set to one. It explains that any periodic translation of $\mF$ will not change the distribution.  
\end{definition}
The combination of the above invariances is abbreviated in a compact manner termed \textit{periodic E(3) invariance} proposed by \citet{jiao2023crystal}. The permutational invariance can be easily achieved by using GNN frameworks. Periodic translation and rotation are both space group transformations. We first introduce the basic concept of \emph{G-invariant}.
\begin{definition}
\label{def:ei}
We call a distribution $p(x)$ is $G$-invariant if for any transformation $g$ in the group $G$, $p(g\cdot x) = p(x)$, and a conditional distribution $p(x|c)$ is G-equivariant if $p(g\cdot x|g\cdot c) = p(x|c), \forall g\in G$.
\end{definition}
With a lemma from \citet{xu2021geodiff}, we can prove a Markov-process-generated distribution \emph{G-invariant} by proving the G-invariance of the prior distribution and the G-equivariance of every transition kernel.
\begin{lemma}[\citet{xu2021geodiff}]
\label{lm:mrkv}
Consider the generation Markov process $p(\theta_n) =\int  p(\theta_0)p(\theta_{n:1}|\theta_0)d\theta_{1:n}$. If the prior distribution $p(\theta_0)$ is G-invariant and the Markov transitions $p(\theta_{t+1}|\theta_t), 0 \leq t\leq n-1$ are G-equivariant, the marginal distribution $p(\theta_n)$ is also G-invariant.
\end{lemma}
\begin{proof}
\begin{align*}
\forall g\in G,\quad p(g\cdot \theta_n) & = p(g\cdot \theta_0)\int p(\theta_{n:1}|\theta_0)d\theta_{1:n}\\
    & = p(g\cdot \theta_0)\int\prod_{t=0}^{n-1} p(g\cdot \theta_{t+1}|g\cdot \theta_{t})d\theta_{1:n}\\
    & = p(\theta_0)\int\prod_{t=0}^{n-1}p(g\cdot \theta_{t+1}|g\cdot \theta_t)d\theta_{1:n}\\
    & = p(\theta_0)\int\prod_{t=0}^{n-1}p(\theta_{t+1}|\theta_t)d\theta_{1:n}\\
    & = p(\theta_0)\int p(\theta_0)p(\theta_{n:1}|\theta_0)d\theta_{1:n}\\
    & = p(\theta_n).
\end{align*}
Therefore, the marginal distribution $p(\theta_n)$ is G-invariant.
\end{proof}
With \cref{lm:mrkv}, we can prove the following propositions mentioned in the main text:
\latticeinv*
% \begin{proof}
%     The prior is O(3) invariant since $p(\vmu^L_0)=\delta(\vmu-\Vec{0})=\delta(\vQ\vmu-\Vec{0}),\forall \vQ^T\vQ=\vI$.\\
%     The transition probability of
%     \begin{align*}
%     &p(\vQ\bmuL_{i}|\vQ\bmuL_{i-1},\parsnt{i-1}^A,\parsnt{i-1}^F)\\
%     =&\update^L(\vQ\bmuL_{i}|\hat{\net}_L(\vQ\bmuL_{i-1},\parsnt{i-1}^A,\parsnt{i-1}^F, t_{i-1}))\\
%     =&\mathcal{N}(\vQ\bmuL_{i}|\gamma(t)\hat{\net}_L(\vQ\bmuL_{i-1},\parsnt{i-1}^A,\parsnt{i-1}^F, t_{i-1},\gamma(t)(1-\gamma(t))\bm{I}) \\
%     =&\mathcal{N}(\vQ\bmuL_{i}|\gamma(t)\vQ\hat{\net}_L(\bmuL_{i-1},\parsnt{i-1}^A,\parsnt{i-1}^F, t_{i-1},\gamma(t)(1-\gamma(t))\bm{I})\quad (\text{by equivariance of }\net^L)\\
%     =&\mathcal{N}(\bmuL_{i}|\gamma(t)\hat{\net}_L(\bmuL_{i-1},\parsnt{i-1}^A,\parsnt{i-1}^F, t_{i-1},\gamma(t)(1-\gamma(t))\bm{I}) \quad (\text{by property of isotropic Normal})\\
%     =&\flow^L(\bmuL_{i}|\hat{\net}_L(\bmuL_{i-1},\parsnt{i-1}^A,\parsnt{i-1}^F, t_{i-1}))\\    
%     =&p(\bmuL_{i}|\bmuL_{i-1},\parsnt{i-1}^A,\parsnt{i-1}^F)\\
%     \end{align*}
%     is equivariant. By \cref{lm:mrkv}, the marginal distribution $p(\bmuL_n)$ is O(3)-invariant.
% \end{proof}
\begin{proof}
    The prior is O(3) invariant since $p(\vmu^L_0)=\delta(\vmu-\Vec{0})=\delta(\vQ\vmu-\Vec{0}),\forall \vQ^T\vQ=\vI$.\\
    The transition probability of
    \begin{align*}
    &p(\vQ\bmuL_{i}|\vQ\bmuL_{i-1},\parsnt{i-1}^A,\parsnt{i-1}^F)\\
    =&\update^L(\vQ\bmuL_{i}|\vQ\bmuL_{i-1},\hat{\net}_L(\vQ\bmuL_{i-1},\cdot),t_{i-1})\\
    =&\mathcal{N}(\vQ\bmuL_{i}|\frac{\alpha\hat{\net}_L(\vQ\bmuL_{i-1},\cdot)+\vQ\bmuL_{i-1}\rho_{i-1}}{\rho_{i}},\frac{\alpha}{\rho_i^2}\vI) \\
    =&\mathcal{N}(\vQ\bmuL_{i}|\frac{\alpha\vQ\hat{\net}_L(\bmuL_{i-1},\cdot)+\vQ\bmuL_{i-1}\rho_{i-1}}{\rho_{i}},\frac{\alpha}{\rho_i^2}\vI) \quad (\text{by equivariance of }\net^L)\\
    =&\mathcal{N}(\vQ\bmuL_{i}|\vQ\frac{\alpha\hat{\net}_L(\bmuL_{i-1},\cdot)+\bmuL_{i-1}\rho_{i-1}}{\rho_{i}},\frac{\alpha}{\rho_i^2}\vI)\\
    =&\mathcal{N}(\bmuL_{i}|\frac{\alpha\hat{\net}_L(\bmuL_{i-1},\cdot)+\bmuL_{i-1}\rho_{i-1}}{\rho_{i}},\frac{\alpha}{\rho_i^2}\vI)\quad (\text{by property of isotropic Normal p.d.f})\\
    =&\update^L(\bmuL_{i}|\bmuL_{i-1},\hat{\net}_L(\bmuL_{i-1},\cdot),t_{i-1})\\
    =&p(\bmuL_{i}|\bmuL_{i-1},\parsnt{i-1}^A,\parsnt{i-1}^F)\\
    \end{align*}
    is equivariant. By \cref{lm:mrkv}, the marginal distribution $p(\bmuL_n)$ is O(3)-invariant.
\end{proof}
\fracinv*
\begin{proof}
We first prove that the Bayesian update is periodic translational equivariant. Based on \cref{eq:update_mi,eq:update_ci}, we can interpret the Bayesian update of $\{\dotm_{i-1},c_{i-1}\}$ observing $\doty$ with $\alpha$, as the \textit{vector addition} between $\dotm_{i-1}$ and $\doty$ with weight $c_{i-1}$ and $\alpha$. And the periodic translation $t$ for $x$ corresponds to the rotation of $\dotx$ with angle $t$:
 \begin{align*}
    % & w(x+t)=x+t+k,\exists k\in\mathbb{Z}\\
     % \Leftrightarrow &
     &[\cos(x+t+2\pi k),\sin(x+t+2\pi k)]^T=[\cos(x+t),\sin(x+t)]^T\\
     =&[\cos x\cos t-\sin x\sin t,\sin x\cos t +\cos x\sin t]^T\\
     =&\begin{bmatrix}
\cos t & -\sin t \\
\sin t & \cos t
\end{bmatrix}\begin{bmatrix}
\cos x \\
\sin x 
\end{bmatrix}=\vec{R}_t\dot{\vx}
 \end{align*}
 where $\vec{R}_t$ is the 2-dimensional rotation matrix with angle $t$. Due to the rotational equivariance of 2D vector addition, we can infer that the Bayesian update function $h$ is periodic translational equivariant:
\begin{align}\label{eq:equiv_bu}
&h(\{w(m_{i-1}+t),c_{i-1}\},w(y+t),\alpha)=h(\{\vR_t\dotm_{i-1},c_{i-1}\},\vR_t\doty,\alpha)\nonumber\\
=&\{\frac{\alpha \vR_t\doty+c_{i-1}\vR_t\dotm_{i-1}}{||\alpha\vR_t\doty+c_{i-1}\vR_t\dotm_{i-1}||_2},||\alpha\vR_t\doty+c_{i-1}\vR_t\dotm_{i-1}||_2\}\nonumber\\
=&\{\frac{\alpha \vR_t\doty+c_{i-1}\vR_t\dotm_{i-1}}{||\alpha\doty+c_{i-1}\dotm_{i-1}||_2},||\alpha\doty+c_{i-1}\dotm_{i-1}||_2\}\nonumber\\
 =&\{\vR_t\dotm_i,c_i \}=\{w(m_{i}+t),c_{i}\}
\end{align}

    The prior is periodic translation invariant because $\bmF\sim U(\vec{0},\vec{1})$.\\
    We prove that the Bayesian update distribution $p_U(\bmF_{i}|\bmF_{i-1},\bcF_{i-1},g(\bmF_{i-1});\alpha)$ is periodic translation equivariant if $\net^F$ is periodic translation equivariant:
    \begin{align*}
    &p_U(w(\bmF_{i}+\vt)|w(\bmF_{i-1}+\vt),\bcF_{i-1},\net^F(w(\bmF_{i-1}+\vt));\alpha)\\
    =&\mathbb{E}_{vM(\bold{y}|\net^F(w(\bmF_{i-1}+\vt)),\alpha)}\delta(w(\bmF_{i}+\vt)-h(w(\bmF_{i-1}+\vt),\bcF_{i-1},\bold{y},\alpha))\\
    =&\mathbb{E}_{vM(\bold{y}|w(\net^F(\bmF_{i-1})+\vt),\alpha)}\delta(w(\bmF_{i}+\vt)-h(w(\bmF_{i-1}+\vt),\bcF_{i-1},\bold{y},\alpha))\text{(by equiv.} \net^F\text{)}\\
    =&\mathbb{E}_{vM(w(\bold{y}+\vt)|w(\net^F(\bmF_{i-1})+\vt),\alpha)}\delta(w(\bmF_{i}+\vt)-h(w(\bmF_{i-1}+\vt),\bcF_{i-1},w(\bold{y}+\vt),\alpha))\\
    =&\mathbb{E}_{vM(\y|\net^F(\bmF_{i-1})),\alpha)}\delta(w(\bmF_{i}+\vt)-h(w(\bmF_{i-1}+\vt),\bcF_{i-1},w(\bold{y}+\vt),\alpha))\text{(by \cref{eq:vm_period_equi})}\\
    =&\mathbb{E}_{vM(\y|\net^F(\bmF_{i-1})),\alpha)}\delta(w(\bmF_{i}+\vt)-w(h(\bmF_{i-1},\bcF_{i-1},\bold{y},\alpha)+\vt))\text{(by \cref{eq:equiv_bu})}\\
    =&\mathbb{E}_{vM(\y|\net^F(\bmF_{i-1})),\alpha)}\delta(\bmF_{i}-h(\bmF_{i-1},\bcF_{i-1},\bold{y},\alpha))  \text{(by equivariance of }\delta \text{ function)}\\
    =&p_U(\bmF_{i}|\bmF_{i-1},\bcF_{i-1},\net^F(\bmF_{i-1});\alpha)\\
    \end{align*}    
    % from here
    % The transition probability of 
    % \begin{align*}
    % &p(w(\bmF_{i}+\vt)|w(\bmF_{i-1}+\vt),\bcF_{i-1},\parsnt{i-1}^A,\parsnt{i-1}^L)\\
    % =&p_F(w(\bmF_{i}+\vt)|\net^F(w(\bmF_{i-1}+\vt),\bcF_{i-1},\parsnt{i-1}^A,\parsnt{i-1}^L);\alpha_1,\alpha_2,\dots,\alpha_i)\\
    % % =&p_F(w(\bmF_{i}+\vt)|w(\net^F(\bmF_{i-1},\bcF_{i-1},\parsnt{i-1}^A,\parsnt{i-1}^L)+\vt);\alpha_1,\alpha_2,\dots,\alpha_i)\quad (\text{by equivariance of }\net^F)\\
    % =&\E_{\update(\bmF_1 \mid \bmF_0, \net^F(w(\bmF_{i-1}+\vt) ; \alphat{1})}\dots\E_{\update(\bmF_{i-1} \mid \bmF_{i-2}, \net^F(w(\bmF_0+\vt) ; \alphat{i})} \update(\bmF| \bmF_{i-1},\net^F(w(\bmF_{i-1}+\vt);\alphat{i} )\\
    % =&\E_{\update(w(\bmF_1+\vt) \mid w(\bmF_0+\vt), \net^F(w(\bmF_{0}+\vt) ; \alphat{1})}\dots\E_{w(\update(\bmF_{i-1}+\vt) \mid w(\bmF_{i-2}+\vt), \net^F(w(\bmF_0+\vt) ; \alphat{i})} \\
    % &\hspace{2in}\update(w(\bmF+\vt)| w(\bmF_{i-2}+\vt),\net^F(w(\bmF_{i-1}+\vt);\alphat{i} )\\
    % =&\E_{\update(\bmF_1 \mid \bmF_0, \net^F(\bmF_{0} ; \alphat{1})}\dots\E_{\update(\bmF_{i-1} \mid \bmF_{i-2}, \net^F(w(\bmF_0+\vt) ; \alphat{i})} \update(\bmF|\bmF_{i-2},\net^F(\bmF_{i-1};\alphat{i} )\\
    % =&p_F(\bmF_{i}|\net^F(\bmF_{i-1},\bcF_{i-1},\parsnt{i-1}^A,\parsnt{i-1}^L);\alpha_1,\alpha_2,\dots,\alpha_i)\\
    % =&p(\bmF_{i}|\bmF_{i-1},\bcF_{i-1},\parsnt{i-1}^A,\parsnt{i-1}^L)\\
    % \end{align*}
    % is equivariant. By \cref{lm:mrkv}, the marginal distribution $p(\bmF_n)$ is periodic translation invariant.
\end{proof}

Next, we prove the following proposition:
\cirflowequiv*
\begin{proof}
Combining \cref{eq:update_mi,eq:update_ci},
\begin{align*}
\dotm_{i}\vec{c}_i&=\alpha_i \doty_i+\vec{c}_{i-1}\dotm_{i-1}\\
&=\alpha_i \doty_i + \alpha_{i-1} \doty_{i-1}+\vec{c}_{i-2}\dotm_{i-2}\\
&=\alpha_i \doty_i + \dots + \alpha_{1} \doty_{1}+\vec{c}_0 \dotm_{0}\\
&=\sum_{j=1}^i \alpha_i \doty_i=[\sum_{j=1}^i \alpha_i\cos \y_i,\sum_{j=1}^i \alpha_i\sin \y_i]^T
\end{align*}
Taking the 2-norm to each side,
\begin{align*}
    ||\dotm_{i}\vec{c}_i||_2&=||[\sum_{j=1}^i \alpha_i\cos \y_i,\sum_{j=1}^i \alpha_i\sin \y_i]^T||_2\\
    \vec{c}_i&=||[\sum_{j=1}^i \alpha_i \cos \y_i,\sum_{j=1}^i \alpha_i \sin \y_i]^T||_2
\end{align*}
The vector direction of $\dotm_{i}$ is irrelevant to the scaler $\vec{c}_i$. Therefore,
\begin{equation}
    \dotm_{i}=\text{atan2}(\sum_{j=1}^i \alpha_i \cos \y_i,\sum_{j=1}^i \alpha_i \sin \y_i)
\end{equation}

Hence,
\begin{align*}
% p_F(\btheta_i|\x;\alpha_1,\alpha_2,\dots,\alpha_i)=\\
% \E_{vM(\y_1|\x,\alpha_1)}\dots\E_{vM(\y_i|\x,\alpha_i)} \delta(\vec{m}_i-\text{atan2}(\sum_{j=1}^i \alpha_i \cos \y_i,\sum_{j=1}^i \alpha_i \sin \y_i))
&p_F(\vec{m}_i|\x;\alpha_1,\alpha_2,\dots,\alpha_i)\\=&\E_{\update(\parsnt{1} \mid \parsnt{0}, \x ; \alphat{1})}\dots\E_{\update(\parsn_{i-1} \mid \parsnt{i-2}, \x; \alphat{i-1})} \update(\vm_{i} | \parsnt{i-1},\x;\alphat{i} ) \\
=& \E_{vM(\y_1|\x,\alpha_1)}\dots\E_{vM(\y_i|\x,\alpha_i)} \delta(\vec{m}_{i}-\text{atan2}(\sum_{j=1}^i \alpha_i \cos \y_i,\sum_{j=1}^i \alpha_i \sin \y_i))\\
&p_F(\vec{c}_i|\x;\alpha_1,\alpha_2,\dots,\alpha_i)\\=&\E_{\update(\parsnt{1} \mid \parsnt{0}, \x ; \alphat{1})}\dots\E_{\update(\parsn_{i-1} \mid \parsnt{i-2}, \x; \alphat{i-1})} \update(\vm_{i} | \parsnt{i-1},\x;\alphat{i} ) \\
=& \E_{vM(\y_1|\x,\alpha_1)}\dots\E_{vM(\y_i|\x,\alpha_i)} \delta(\vec{c}_{i}-||[\sum_{j=1}^i \alpha_i\cos \y_i,\sum_{j=1}^i \alpha_i\sin \y_i]^T||_2)
\end{align*}
\end{proof}

% \section{Connection Between Denoising Objective and Learning a Harmonic Force Field}
% Representing the infinite Cartesian crystal atom coordinates as $\vec{X}\in \R^{3\times \infty}$, the crystal structure is determined and the previously mentioned $\vL$ can be inferred from $\vec{X}$. We can see a crystal structure $\vS=(\vX,\vL)$ as a random scaler sampled from the Boltzmann distribution $\pphysics(\vS) \propto \exp(-E(\vS))$, where $E(\vS)$ is the potential energy of $\vS$. The training dataset can be seen as a set of equilibrium structures $\mathcal{D}_{data}=\{\vS_1,\dots, \vS_{l}\}$. Since $\pphysics$ is intractable, we can only approximate it by the empirical distribution $q_0(\x) = \frac{1}{n} \sum_{i = 1}^n \delta(\x = \x_i)$


\section{Implementation Details}\label{appd:imple_details}

\textbf{Training and Sampling Procedure} We provide the training and sampling procedure in \cref{alg:train} and in \cref{alg:sampling}. 

\textbf{Network Architecture} We use CSPNet proposed by \citet{jiao2023crystal} with minor modifications: $(1)$ We add a residual connection from the input to the output of fractional coordinates ensuring the equivariance of the network:
\begin{equation}
\net^F(\bthetaA_i,\bthetaF_i,\bthetaL_i,t_i) = w(\varphi_F(\vh_i^{(S)})+\bthetaF_i),
\end{equation}
By the periodic translational invariance of $\varphi_F(\vh_i^{(S)})$ proved by \citet{jiao2023crystal}, the equivariance of $\predF{i}$ can be easily checked:
\begin{align*}
   \net^F(\bthetaA_i,w(\bthetaF_i+\vt),\bthetaL_i,t_i)= &w(\varphi_F(\vh_i^{(S)})+w(\bthetaF_i+\vt)) \\
   =& w(w(\varphi_F(\vh_i^{(S)})+\bthetaF_i)+\vt) \\
   =& w(\net^F(\bthetaA_i,\bthetaF_i,\bthetaL_i,t_i)+\vt)
\end{align*}

$(2)$We alter the frequency of the Fourier transformation features to model in the interval $[-\pi,\pi)$ with length $2\pi$. $(3)$The concentration parameter of each fractional coordinate $\bcF$ is taken logarithm, normalized and concatenated to time embedding. The network hyper-parameters follows the setting of \citet{jiao2023crystal} including the number of hidden states and layers. 

\textbf{Hyper-parameters} For the network, the CSPNet has 6 layers, 512 hidden states, 128 frequencies for the Fourier feature for each task and dataset following \citep{jiao2023crystal}. For BFN hyper-parameters, we set $\sigma_1^2=0.001$ for continuous variable generation, $\beta_1=1000$ for circular variables generation across all datasets and tasks. For discrete variables, we set $\beta_1=0.4$ for the MP-20 dataset and $\beta_1=3.0$ for the Perov-5 dataset. The number of steps is searched in $\{50,100,500,1000,2000\}$. For optimizations, we apply an AdamW optimizer with an initial learning rate $1\times10^-3$ and a plateau scheduler with a decaying factor of 0.6, a patience of 100 epochs, and a minimal learning rate $1\times10^{-4}$. The weight of every loss is $5\times10^{-2}$. The network is trained for 4000, 5000, 1500, and 1000 epochs for Perov-5, Carbon-24, MP-20, and MPTS-52 respectively.

\textbf{Computational Resources} All training experiments are conducted on a server with 8 $\times$ NVidia RTX 3090 GPU, 64 $\times$ Intel Xeon Platinum 8362 CPU and 256GB memory. Each training task requires one GPU. We also report the required GPU hour across methods to converge in our experimental environment in \cref{tab:train_gpu_hour}.

\begin{table*}[h!]
% \vskip -0.2in
% \renewcommand\arraystretch{0.85}
  \centering
  \caption{Comparison of GPU hours required for training across different methods.}
  \resizebox{0.6\linewidth}{!}{
  \small
    \setlength{\tabcolsep}{2.5pt}
  % \setlength{\tabcolsep}{3.2mm}
    \begin{tabular}{lccccccc}
    \toprule
    \textbf{GPU Hour} & Perov-5  &  MP-20 & MPTS-52 \\
    \midrule
    DiffCSP~\cite{jiao2023crystal}    & 8.59 & 92.22 & 10.42 \\
    FlowMM~\cite{flowmm}    & 16.36 & 106.37 & 16.49 \\
    \modelname    & 10.19  & 85.71 & 12.31 \\
    \bottomrule
    \end{tabular}
    }
\vskip -0.15in
 \label{tab:train_gpu_hour}%
\end{table*}%
% \textbf{Used Assets}. The code of this paper is implemented based on \cite{xie2021crystal} under MIT License. The dataset used in this paper has been cited in \cref{sec:exp}.

% \textbf{Sampling Strategy} In fact, considering the prediction error of the network in the early stage of sampling, it is reasonable to clear the current belief parameterized $\btheta^M_{i-1}$ by transmitting the more precised network output to the Bayesian flow distribution to get $\btheta^M_{i}$ instead of doing one-step Bayesian update. We do such belief rebuild for each step for crystal ab initio generation task and the crystal structure prediction task.

% \begin{algorithm}[h]
% \small
% \caption{Functions of \modelname}\label{alg:funcs}
% \begin{algorithmic}[1]
% \STATE \textbf{Require:} number of steps $n\in\R$, $\sigma_1\in\R^{+},\beta_1\in\R^{+},\alpha_1^F\in\R^{+},\dots,\alpha_n^F\in\R^{+}$. 

% \STATE \textbf{Input:}  atom types $\vec{A}$, fractional coordinates $\vF$, lattice parameter $\vL$
% \STATE Sample $i\sim U\{1,n\}$, $t\leftarrow \frac{(i-1)}{n}$
% \STATE $\gamma^L \leftarrow 1-\sigma_1^{2t}$,~$\beta^A \leftarrow \beta_1 t^2$
% \STATE $\vmu_L\sim\mathcal{N}(\gamma^L \vL,\gamma^L(1-\gamma^L)\vI)$
% \STATE $\y_A' \sim \N{\beta\left(K\oh{\x}{KD}-
% \1{KD}\right)}{\beta K\vI}$
% \STATE $\parsn^A \gets \text{softmax}(\y_A')$
% \STATE $\y_1,\y_2,\dots,\y_i \gets vM(\vF,\alpha_1^F),\dots vM(\vF,\alpha_i^F)$
% \STATE $\vec{c}_{i}\gets ||[\sum_{j=1}^i \alpha_i\cos \y_i,\sum_{j=1}^i \alpha_i\sin \y_i]^T||_2$, $\vec{m}_{i}\gets\text{atan2}(\sum_{j=1}^i \alpha_i \cos \y_i,\sum_{j=1}^i \alpha_i \sin \y_i)$
% \STATE $\predL{i-1},\predF{i-1},\predF{i-1} \gets$ OUTPUT\_PREDICTION($\vmu_L,\parsn^A,\vec{m}_{i},\vec{c}_{i},t)$
% \STATE $\alpha^A_i \leftarrow \beta(1)\left(\frac{2i -1}{n^2}\right)$
% \STATE $\y_A \sim \N{\alpha\left(K\oh{\x}{KD}-
% \1{KD}\right)}{\alpha K\vI}$
% \STATE $\calL_A \gets n\ln \N{\y_A \mid \alphat{i}^A\left(K \oh{\vA}{K\times N} - \vec{1}\right)}{\alphat{i}^A K \vec{I}}$
% \STATE\qquad\qquad$-\sum_{d=1}^N \ln \left(\sum_{k=1}^K \out^{(d)}(k \mid \parsn^A; t_{i-1}) \N{\ydd{d}_A \mid \alphat{i}^A\left(K\oh{k}{K}- \vec{1}\right)}{\alphat{i}^A K \vec{I}}\right)$
% \STATE $\calL_F \gets n~\alpha_i^F\frac{I_1(\alpha_i^F)}{I_0(\alpha_i^F)}(1-\cos(\vF-\predF{i-1}))$
% \STATE $\mathcal{L}_{L} = \frac{n}{2}\left(1-\sigma_1^{2/n}\right)\frac{\left\|\vL -\predL{i-1}\right\|^2}{\sigma_1^{2i/n}}$
% \STATE \textbf{Return} $\calL_A+\calL_F+\calL_L$
% \end{algorithmic}
% \end{algorithm}

% added content from here



\section{More Results}
% \textbf{Efficiency experiments}. Using the CSP task on MP-20 dataset, We alter the number of steps and study the effect of different Number of Function Evaluations (NFE) \emph{i.e.} number of network forward passes.
% \rebuttal{
\textbf{Visualizations} Here we give visualizations of the ab-initio generated structures from CrysBFN and DiffCSP in \cref{fig:vis_apd}. We also provide a gif animation of the generation process in our code repository \url{https://github.com/wu-han-lin/CrysBFN}.
% \clearpage
\begin{figure*}[h!]
    \centering    \includegraphics[width=\textwidth]{imgs/abinitio_vis_comparison.pdf}
    \caption{Visualizations comparison of the ab-initio generated structures from CrysBFN and DiffCSP.}
    \label{fig:vis_apd}
\end{figure*}



\textbf{Error Bars} We report the error bar for the crystal structure prediction task following \citet{jiao2023crystal} in \cref{tab:std} running three experiments with different random seeds. The results are similar to \cref{tab:oto}.
\begin{table*}[h]
% \renewcommand\arraystretch{0.85}
  \centering
  \caption{Results on Perov-5 and MP-20 with error bars. }
  \small
    \setlength{\tabcolsep}{2.5pt}
  % \setlength{\tabcolsep}{3.2mm}
    \begin{tabular}{p{4cm}ccccc}
    \toprule
     &  \multicolumn{2}{c}{Perov-5 } & \multicolumn{2}{c}{MP-20} \\
             & Match rate (\%)$\uparrow$ & RMSE$\downarrow$ & Match rate (\%)$\uparrow$ & RMSE$\downarrow$  \\
    \midrule
    {CDVAE~\citep{xie2021crystal}}& 45.31$\pm$0.49  & 0.1123$\pm$0.0026 &  33.93$\pm$0.15  & 0.1069$\pm$0.0018  \\
    \midrule
    DiffCSP~\citep{jiao2023crystal}  & 52.35$\pm$0.26  & 0.0778$\pm$0.0030 & 51.89$\pm$0.30 &	0.0611$\pm$0.0015 \\
    \midrule
    \modelname  & \textbf{54.58$\pm$0.13}  & \textbf{0.0691$\pm$0.0011} & \textbf{64.33$\pm$0.24} &	\textbf{0.0445$\pm$0.0010} \\
    \bottomrule
    \end{tabular}%
    % }
  \label{tab:std}%
\end{table*}%


\textbf{Uniqueness, Novelty, and Stability} Here we compare the uniqueness, novelty, and stability of ab initio generated samples across methods on MP-20. Using StructureMatcher in pymatgen with default parameters, a generated crystal is considered: 1) unique if it does not match any other generated samples; and 2) novel if it does not match any crystals in the training set, following prior practices \citep{zeni2023mattergen,flowmm}. The stability evaluation procedure follows the approach in \citet{crysllm}. Finally, a sample is considered stable, unique, and novel (S.U.N.) if it satisfies all three conditions. The results are reported in \cref{tab:sun}.
% Specifically, we use M3GNet~\citep{m3gnet} for pre-relaxations and perform DFT~\citep{hafner2008ab} calculations only on meta-stable samples predicted by M3GNet ($\hat{E}_{\text{hull}} < 0.1$) and regard it as stable if $E_{\text{hull}}^{\text{DFT}}<0$. This is because DFT calculations are significantly more computationally expensive. Finally, a sample is considered stable, unique, and novel (S.U.N.) if it satisfies all three conditions. The results are reported in \cref{tab:sun}.}
\begin{table*}[h]
  \centering
  \caption{Uniqueness, novelty, and stability experimental results comparison on ab initio generation task on MP-20 dataset.}
  \small
    \setlength{\tabcolsep}{2.5pt}
  % \setlength{\tabcolsep}{3.2mm}
    \begin{tabular}{p{4cm}cccccc}
    \toprule
    \textbf{Method}& \textbf{Unique} / \% & \textbf{Novel} / \% & \textbf{Metastable} / \% & \textbf{Stable} / \% & \textbf{S.U.N. Rate} / \% \\
    \midrule
    % {CDVAE~\citep{xie2021crystal}}      &  28.8  & 5.4 \\ 
    % \midrule
    DiffCSP~\citep{jiao2023crystal}    & \textbf{96.11} & 90.95 & 37.91 & 12.16 & 9.44 \\
    \midrule
    FlowMM~\citep{flowmm}    & 94.79 &	91.63 & 32.77 & 9.23 & 8.31 \\
    \midrule
    \modelname  & 95.29 & \textbf{92.37} & \textbf{45.91} & \textbf{15.82} & \textbf{12.16}\\
    \bottomrule
    \end{tabular}%
    % }
  \label{tab:sun}%
\end{table*}%


% \begin{table*}[h!]
% \vskip -0.1in
% % \renewcommand\arraystretch{0.85}
%  %  
%   \centering
%   \caption{\rebuttal{Stability results on crystal structure prediction task. Metastability and stability calculations follow the method in \cite{crysllm}.}}
%   \resizebox{0.90\linewidth}{!}{
%   % \small
%     \setlength{\tabcolsep}{2.5pt}
%   % \setlength{\tabcolsep}{3.2mm}
%     \begin{tabular}{lccccccc}
%     \toprule
%     \multirow{2}[2]{*}{} & \multicolumn{2}{c}{Perov-5 } &  \multicolumn{2}{c}{MP-20} & \multicolumn{2}{c}{MPTS-52} \\
%           &  Metastable$\uparrow$ & Stable$\uparrow$   & Metastable$\uparrow$ & Stable$\uparrow$ & Metastable$\uparrow$ & Stable$\uparrow$ \\
%     \midrule
%     DiffCSP~\citep{jiao2023crystal}      & 4.96  & 1.48  & 27.94 &	7.42 & 11.21 & 2.91 \\
%           % & 20    & 98.60 & 0.0128 &  77.93 & 0.0492 & 34.02 & 0.1749 \\
%     \midrule
%     FlowMM~\citep{flowmm}     & 4.62  & 1.32  & 23.94 &	 6.98 & 9.19 &  2.83 \\
%           % & 20    & 98.60 & 0.0128 &  77.93 & 0.0492 & 34.02 & 0.1749 \\
%     % \midrule
%     % EquiCSP~\citep{equicsp} & 1     & 52.02  & 0.0707  & 57.59 &	 0.0510 & 14.85 &  0.1169 \\
%           % & 20    & 98.60 & 0.0128 &  77.93 & 0.0492 & 34.02 & 0.1749 \\
%     \midrule
%     \modelname    & \textbf{5.31}  & \textbf{1.66}  &	\textbf{32.41} & \textbf{9.73} & \textbf{14.38} & \textbf{4.08}\\
%      % & 20 & \textbf{98.64} & \textbf{0.0116} &\textbf{80.72} & \textbf{0.0473} & \textbf{36.86} & \textbf{0.1596} \\
%     \bottomrule
%     \end{tabular}
%     }
% \vskip -0.25in
%  \label{tab:csp_stable}%
% \end{table*}%

% \begin{table*}[t]
%   \centering
%   \caption{Stable\&Metastable rate experimental results comparison on MP-20. The GPU hour is evaluated in our experiemental environment. }
%   \small
%     \setlength{\tabcolsep}{2.5pt}
%   % \setlength{\tabcolsep}{3.2mm}
%     \begin{tabular}{p{4cm}cccccc}
%     \toprule
%     \multirow{2}[2]{*}{\textbf{Method}} & \textbf{Metastable} & \textbf{Stable } &\textbf{GPU Hour}& \textbf{Sampling Efficiency}\\
%           & M3GNet$\uparrow$  & DFT$\uparrow$ & /10k samples$\downarrow$ & \# metastable sample/h$\uparrow$\\
%     \midrule
%     {CDVAE~\citep{xie2021crystal}}      &  28.8  & 5.4 & 8.192 & 351.6\\ 
%     \midrule
%     DiffCSP~\citep{jiao2023crystal}    & 37.9 &	12.1 &1.551 & 2443.5 \\
%     \midrule
%     \modelname  & \textbf{45.9} & \textbf{15.8} & \textbf{1.088} & \textbf{4218.7}\\
%     \bottomrule
%     \end{tabular}%
%     % }
%   \label{tab:performance_vs_nfe}%
% \end{table*}%


\begin{table*}[t!]
% \vskip -0.2in
% \renewcommand\arraystretch{0.85}
 %  
  \centering
  \caption{Ablation study results of entropy-conditioning mechanism across datasets.}
  \resizebox{0.90\linewidth}{!}{
  \small
    \setlength{\tabcolsep}{2.5pt}
  % \setlength{\tabcolsep}{3.2mm}
    \begin{tabular}{lccccccc}
    \toprule
    \multirow{2}[2]{*}{} & \multicolumn{2}{c}{Perov-5 } &  \multicolumn{2}{c}{MP-20} & \multicolumn{2}{c}{MPTS-52} \\
          &  Match rate$\uparrow$ & RMSE$\downarrow$   & Match rate$\uparrow$ & RMSE$\downarrow$ & Match rate$\uparrow$ & RMSE$\downarrow$ \\
    \midrule
    w/o entropy conditioning    & 51.33 & 0.0753 & 52.16 & 0.0631 & 13.41 & 0.1547 \\
    \modelname    & \textbf{54.69}  & \textbf{0.0636} & \textbf{64.35} &	\textbf{0.0433} & \textbf{20.52} & \textbf{0.1038} \\
    % Relative Improvement / \% & 6.55 & 15.54 & 23.37 & 31.38 & 53.02 & 32.90\\
     % & 20 & \textbf{98.64} & \textbf{0.0116} &\textbf{80.72} & \textbf{0.0473} & \textbf{36.86} & \textbf{0.1596} \\
    \bottomrule
    \end{tabular}
    }
% \vskip -0.15in
 \label{tab:entropy_condition_datasets}%
\end{table*}%



% \clearpage
\section{Sampling Efficiency Comparison to ODE Samplers}
\begin{figure}[h!] % 图片靠右，宽度为文本宽度的40%
% \vskip -0.4in
    \centering
    \includegraphics[width=0.5\textwidth]{imgs/match_rate_ode.pdf} 
    \caption{Experimental results on MP-20 with different Number of Function Evaluations (NFE) \emph{i.e.} number of network forward passes including FlowMM. Note that FlowMM has a larger parameter size resulting in a less fair comparison.} % 图像标题
    \label{fig:ode_compare}
\vskip -0.2in
\end{figure}
% We do not include FlowMM in \cref{fig:eff_compare}. due to its larger parameter size which makes the comparison based on NFE unfair. Nonetheless, w
We present the comparison among them in \cref{fig:ode_compare} and find that FlowMM fails in extremely small NFE (20 steps) settings with only 16.18\% match rate, while CrysBFN enjoys 60.02\% match rate with 10 steps and consistently achieves the best sampling quality. 

\section{Detailed Discussion of Related Works}
\label{appd:detailed_related_works}
Discovering new functional materials has been a long-standing scientific problem. Recently, data-driven approaches have been seen as a promising solution to address this challenge ~\cite{peng2022human}.

\textbf{Two-stage crystal generation methods based on implicit crystal representations} One line of approaches indirectly generates crystals in the implicit representation space. Prior practices includes transforming crystals into human-designed fingerprint FTCP~\citep{REN2021}, 3D voxels images~\citep{hoffmann2019data}, 2D images~\citep{noh2019inverse}, 3D electron-density maps~\citep{court20203}, video-like representation~\citep{yang2023scalable}, embedded atom density \citep{zhang2019embedded} in StructRepDiff~\citep{sinha2024representation}. However, their generation quality is hampered by the encoding and reconstruction processes, which may not be fully reversible or fail to respect physical symmetries such as rotational and translational invariance. For example, 3D voxel grids ~\citep{hoffmann2019data} and 3D density maps ~\citep{court20203} are invariant to periodic transformations but not to \(E(3)\) transformations~\citep{zhang2023artificial}, and the video-like representation~\citep{yang2023scalable} is not invariant to permutation, rotation, translation, and periodic transformations. 

\textbf{Direct crystal generation methods} Direct material generation in sample space could bypass the above reversibility problem. Prior works \citep{nouira2018crystalgan,kim2020generative} employ Generative Adversarial Networks \citep{goodfellow2020generative} to generate crystal structures, while their methods fail to respect crystal geometric invariance. Inspired by the success of Diffusion models on images~\citep{ho2020denoising,song2020score,song2019generative}, the multi-step generation paradigm has been introduced into generative modeling of atom systems including molecular conformations~\citep{xu2023geometric}. The geometric invariance of the generation path can be guaranteed by designing a Markov chain with an invariant prior and equivariant transitions \citep{xu2021geodiff}. CDVAE~\citep{xie2021crystal}, its CSP adaptation Cond-CDVAE~\citep{luo2024deep} and SyMat~\citep{luo2024towards}, generate crystalline materials levearging $E(3)$-equivariant graph neural network models~\citep{klipfel2023unified,gasteiger2021gemnet} on 3D multi-edge graphs. Utilizing VAE models, they generate lattice parameters, randomly initialize atom coordinates, and iteratively refine these coordinates using score-matching models~\cite{song2019generative}. With the fractional coordinate system, DiffCSP \citep{jiao2023crystal} firstly introduces the periodic E(3) equivariance of crystals and designs an equivariant diffusion crystal generation model based on periodic diffusion \citep{jing2022torsional}. Subsequently, FlowMM \citep{flowmm} recently introduced Riemannian Flow Matching \citep{riemannianfm} for the task of crystal generation, offering improved sampling efficiency, albeit at the expense of quality.

We argue that such struggles of balancing between sampling quality and efficiency stems from the lack of proper guidance on each transition from noise prior to data distribution especially for crystals, where thermodynamically stable materials represent
only a small fraction in the search space~\citep{flowmm}. For example, early generation states $x_{t-1}$ with low confidence should be preserved less than the later ones to get the next state $x_{t}$. From the perspective of Bayesian updates, Bayesian Flow Networks~\citep{bfn} provides a framework to accurately update each $m_{t-1}$ according to its confidence parameter $\alpha_i$, the effectiveness of which has been proved in \citet{song2023unified}. However, periodicity is not considered in \citet{bfn} and incorporating it into BFN is non-trivial without desirable distributions with mathematical properties like Gaussian. To address above issues, in this paper, we build a Bayesian flow almost from scratch, identifying and tackling the non-additive accuracy via introducing a novel entropy conditioning mechanism, theoretical reformulations of BFN, a fast sampling algorithm, \emph{etc}. We demonstrate the effectiveness of the guidance of entropy in \cref{tab:exp_abl} and \cref{tab:entropy_condition_datasets} and its higher sampling effiency and quality in \cref{fig:eff_compare,fig:ode_compare}.

 Additionally, recently various techniques have also been introduced to boost the performance tailored for crystal generation considering crystal's inductive bias, including \citet{jiao2024space,cao2024space,ai4science2023crystal} which incorporate the space group constraint into the generation process. Recently, EquiCSP~\citep{equicsp} proposed to utilize a periodic CoM-free noising method and introduce lattice permutation invariance loss. Those techniques are orthogonal to the proposed method in this paper.



% \clearpage
% \newpage

 \begin{algorithm}[h]
\small
\caption{Training Procedure}\label{alg:train}
\begin{algorithmic}[1]
\STATE \textbf{Require:} number of steps $n\in\R$, $\sigma_1\in\R^{+},\beta_1\in\R^{+},\alpha_1^F,\dots,\alpha_n^F\in\R^{+}$. 

\STATE \textbf{Input:}  atom types $\vec{A}$, fractional coordinates $\vF$, lattice parameter $\vL$, length of vocabulary $K$
\STATE Sample $i\sim U\{1,n\}$, $t\leftarrow \frac{(i-1)}{n}$
\STATE \textcolor{gray}{\# sampling from Bayesian flow distribution of lattice}
\STATE $\gamma^L \leftarrow 1-\sigma_1^{2t}$
\STATE $\vmu_L\sim\mathcal{N}(\gamma^L \vL,\gamma^L(1-\gamma^L)\vI)$
\STATE \textcolor{gray}{\# sampling from Bayesian flow distribution of atom type}
\STATE $\beta^A \leftarrow \beta_1 t^2$
\STATE $\y_A' \sim \N{\beta\left(K\oh{\x}{KD}-
\1{KD}\right)}{\beta K\vI}$
\STATE $\parsn^A \gets \text{softmax}(\y_A')$
\STATE \textcolor{gray}{\# sampling from Bayesian flow distribution of fractional coordinates}
\STATE $\y_1,\y_2,\dots,\y_i \gets vM(\vF,\alpha_1^F),\dots vM(\vF,\alpha_i^F)$
\STATE $\vec{m}_{i}\gets\text{atan2}(\sum_{j=1}^i \alpha_j \cos \y_j,\sum_{j=1}^i \alpha_j \sin \y_j)$
\STATE \textcolor{gray}{\# calculate the accumulated accuracy \textit{i.e.} entropy}
\STATE $\vec{c}_{i}\gets ||[\sum_{j=1}^i \alpha_j\cos \y_j,\sum_{j=1}^i \alpha_j\sin \y_j]^T||_2$
\STATE \textcolor{gray}{\# use network to do inter-dependency modeling across dimensions conditioning on entropy}
\STATE $\bthetaM\gets (\vmu_L,\parsn^A,\vec{m}_{i},\vec{c}_{i})$
\STATE $\predL{i-1},\predF{i-1},\predF{i-1} \gets \net(\bthetaM,t)$ 
\STATE \textcolor{gray}{\# calculate the losses of all modalities }
\STATE $\alpha^A_i \leftarrow \beta_1^A\left(\frac{2i -1}{n^2}\right)$
\STATE $\y_A \sim \N{\alpha\left(K\oh{\x}{KD}-
\1{KD}\right)}{\alpha K\vI}$
\STATE $\calL_A \gets n\ln \N{\y_A \mid \alphat{i}^A\left(K \oh{\vA}{K\times N} - \vec{1}\right)}{\alphat{i}^A K \vec{I}}$
\STATE\qquad\qquad$-\sum_{d=1}^N \ln \left(\sum_{k=1}^K \out^{(d)}(k \mid \parsn^A; t_{i-1}) \N{\ydd{d}_A \mid \alphat{i}^A\left(K\oh{k}{K}- \vec{1}\right)}{\alphat{i}^A K \vec{I}}\right)$
\STATE $\calL_F \gets n~\alpha_i^F\frac{I_1(\alpha_i^F)}{I_0(\alpha_i^F)}(1-\cos(\vF-\predF{i-1}))$
\STATE $\mathcal{L}_{L} = \frac{n}{2}\left(1-\sigma_1^{2/n}\right)\frac{\left\|\vL -\predL{i-1}\right\|^2}{\sigma_1^{2i/n}}$
\STATE Minimize $\calL_A+\calL_F+\calL_L$
\end{algorithmic}
\end{algorithm}

\begin{algorithm}[h]
\small
\caption{Sampling Procedure}\label{alg:sampling}
\begin{algorithmic}
\STATE \textbf{Require:} number of steps $n\in\R$, length of vocabulary $K$, $\sigma_1\in\R^{+},\beta_1\in\R^{+},\alpha_1^F,\dots,\alpha_n^F\in\R^{+}$
\STATE \textcolor{gray}{\# initialize the prior parameters }
\STATE $\vmu_0$,~$\rho_0\gets \vec{1}$
\STATE $\vec{\theta}_0\gets\frac{1}{K}\vec{1}_{1\times N}$
\STATE $\vm_0\gets U(\vec{0},\vec{1}),\vc_0\gets \vec{0}$
% \COMMENT{\text{feed previous updated belief into network and get output}}
\FOR{$i \gets 1,\cdots, n$}
    \STATE{$t\gets\frac{i-1}{n}$}
    \STATE \textcolor{gray}{\# use network to do inter-dependency modeling across dimensions of all modalities }
    \STATE{$\bthetaM\gets (\vmu_{i-1},\parsn_{i-1},\vec{m}_{i-1},\vec{c}_{i-1})$}
    \STATE{$\predL{i-1},\predF{i-1},\predF{i-1} \gets \net(\bthetaM,t)$}
    \IF{$i<n$}
    \STATE \textcolor{gray}{\# do Bayesian update for lattice parameter }    
    \STATE{$\alpha_i^L\gets \sigma_1^{-2i/n}(1-\sigma_1^{2/n})$}
    \STATE{$\y^L\sim \mathcal{N}(\predL{i-1},\frac{1}{\alpha_i^L}\vI)$}
    \STATE{$\vmu_i\gets \frac{\rho_{i-1}\vmu_{i-1}+\alpha_i^L\y^L}{\rho_{i-1}+\alpha_i^L}$}
    \STATE{$\rho_i\gets \rho_{i-1}+\alpha_i^L$}
    \STATE \textcolor{gray}{\# do Bayesian update for fractional coordinates }   
    \STATE{$\y^F\sim vM(\predF{i-1},\alpha_i^F)$}
    \STATE{$\vm_i\gets \text{atan2}(\alpha_i^F\sin \y+\vc_{i-1}\sin \vm_{i-1}, {\alpha_i^F\cos \y+\vc_{i-1}\cos \vm_{i-1}})$}
    \STATE{$\vc_i\gets ||[\alpha_i^F\sin \y+\vc_{i-1}\sin \vm_{i-1}, {\alpha_i^F\cos \y+\vc_{i-1}\cos \vm_{i-1}}]^T||_2$}
    \STATE \textcolor{gray}{\# do Bayesian update for atom types }   
    \STATE $\alpha_i^A \leftarrow \beta_1\left(\frac{2i -1}{n^2}\right)$
    \STATE $\y^A \sim \N{\alpha_i^A\left(K\oh{\k}{KD}-
    \vec{1}\right)}{\alpha_i^A K\vI}$
    \STATE $\parsn' \gets e^{\y^A} \parsn_{i-1}$
    \STATE $\parsn_i \gets \frac{\parsn'}{\sum_k \parsn'_k}$
    \ENDIF
\ENDFOR
\STATE $\hat{\vec{A}}\sim \predA{n-1}$ \textcolor{gray}{\# sample from the final probability prediction}   
\STATE \textbf{Return} $\hat{\vec{A}},\predF{n-1},\predL{n-1}$
\end{algorithmic}
\end{algorithm}
