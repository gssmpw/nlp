\documentclass{article}

\usepackage{microtype}
\usepackage{graphicx}
\usepackage{subfigure}
\usepackage{booktabs} % for professional tables
\usepackage{hyperref}
\usepackage{xcolor}

\usepackage{multirow}
\usepackage{array}
\usepackage{float}
% Optional math commands from https://github.com/goodfeli/dlbook_notation.
% \input{math_commands.tex}

\usepackage{tablefootnote}
\usepackage{url}
\usepackage{enumitem}
\usepackage{natbib}
\usepackage{times}
\usepackage{fancyhdr}
\usepackage{authblk} % For better author/affiliation formatting

% Attempt to make hyperref and algorithmic work together better:
\newcommand{\theHalgorithm}{\arabic{algorithm}}

% For theorems and such
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{amsthm}


\usepackage[utf8]{inputenc}
\usepackage{amsthm,amsmath,amsfonts,amssymb,bm,dsfont,enumitem,natbib,graphicx}
\usepackage{algorithm,algorithmic}
% \usepackage[usenames]{color}
% \usepackage{tikz}
% \usetikzlibrary{decorations.pathreplacing}
\usepackage{geometry}
% \usepackage[colorlinks,
%             linkcolor=red,
%             anchorcolor=blue,
%             citecolor=blue
%             ]{hyperref}
\usepackage{quoting}
% \usepackage{booktabs}
% \input{notations}
% \usepackage{mystyle}
% \usepackage{times}
% \newcommand{\jiawei}[1]{{\color{magenta}Jiawei: {#1}}}
% \newcommand{\wenzhe}[1]{{\color{blue}Wenzhe: {#1}}}


% if you use cleveref..
\usepackage[capitalize,noabbrev]{cleveref}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% THEOREMS
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\theoremstyle{plain}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{assumption}[theorem]{Assumption}
\theoremstyle{remark}
\newtheorem{remark}[theorem]{Remark}

% Todonotes is useful during development; simply uncomment the next line
%    and comment out the line below the next line to turn off comments
%\usepackage[disable,textsize=tiny]{todonotes}
\usepackage[textsize=tiny]{todonotes}

\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}


\title{Rethinking Mixture-of-Agents: Is Mixing Different \\ Large Language Models Beneficial?}

\author[1]{Wenzhe Li\footnote{Equal contribution.}}
\newcommand\CoAuthorMark{\footnotemark[\arabic{footnote}]}
\author[1]{Yong Lin\protect\CoAuthorMark}
\author[1]{Mengzhou Xia}
\author[1]{Chi Jin}
\affil[1]{Princeton University\thanks{Email: \texttt{\{wenzhe.li,yl7690,mengzhou,chij\}@princeton.edu}.}}
\date{}

\begin{document}

\maketitle

\begin{abstract}
Ensembling outputs from diverse sources is a straightforward yet effective approach to boost performance. Mixture-of-Agents (MoA) is one such popular ensemble method that aggregates outputs from multiple \emph{different} Large Language Models (LLMs). This paper raises the question in the context of language models: is mixing different LLMs truly beneficial?  
We propose Self-MoA --- an ensemble method that aggregates outputs from only the \emph{single} top-performing LLM. Our extensive experiments reveal that, surprisingly, Self-MoA outperforms standard MoA that mixes different LLMs in a large number of scenarios: Self-MoA achieves $6.6\%$ improvement over MoA on the AlpacaEval 2.0 benchmark, and an average of $3.8\%$ improvement across various benchmarks, including MMLU, CRUX, and MATH. Applying Self-MoA to one of the top-ranking models in AlpacaEval 2.0 directly achieves the new state-of-the-art performance on the leaderboard. To understand the effectiveness of Self-MoA, we systematically investigate the trade-off between diversity and quality of outputs under various MoA settings. We confirm that the MoA performance is rather sensitive to the quality, and mixing different LLMs often lowers the average quality of the models. To complement the study, we identify the scenarios where mixing different LLMs could be helpful. This paper further introduces a sequential version of Self-MoA, that is capable of aggregating a large number of LLM outputs on-the-fly over multiple rounds, and is as effective as aggregating all outputs at once.
\end{abstract}

\section{Introduction}
\label{sect:intro}
Large language models have made remarkable strides in improving performance across different domains, with notable examples such as GPT~\citep{achiam2023gpt}, Gemini~\citep{team2023gemini}, and Claude~\citep{anthropic2023introducing}. Significant efforts have been directed toward increasing model size and training data to boost capabilities. However, scaling at training time comes with steep costs, while scaling computation during inference remains largely underexplored.


A straightforward way to utilize test-time compute is ensembling, which aims to combine outputs of multiple LLMs~\citep{wang2024mixture, lin2024mitigatingalignmenttaxrlhf, jiang2023llm, wang2024mixture}.
Among various ensembling approaches,
Mixture-of-Agents (MoA)~\citep{wang2024mixture} has garnered significant interest, achieving superior performance in challenging tasks such as instruction following~\citep{wang2024mixture}, summarization, data extraction~\citep{moa_blog}, and real-world code issue resolution~\citep{zhang2024diversity}. Specifically, MoA first queries multiple LLMs (proposers) to generate responses, and then uses an LLM (aggregator) to synthesize and summarize these responses into a high-quality response. 
% \citet{wang2024mixture} further extends MoA to the multi-layer structure by iteratively using the aggregator's outputs as the proposer's input for the next layer.

Previous research highlights the significance of model diversity within the proposers for optimizing the performance of MoA, primarily focusing on strategies for ensembling a diverse set of individual models. We consider \textbf{cross-model diversity} as the variation among different models. However, pursuing cross-model diversity may inadvertently include low-quality models, resulting in a quality-diversity trade-off. While previous studies mainly concentrate on achieving a high cross-model diversity~\citep{wang2024mixture, zhang2024diversity}, we adopt a holistic perspective on model diversity by considering \textbf{in-model diversity}, which arises from the variability of multiple responses generated by the same model. In-model diversity enables us to aggregate multiple outputs from an individual model. Intuitively, leveraging outputs from the best-performing individual model can more effectively navigate the quality-diversity trade-off by creating a higher-quality proposer mixture. Thus, we propose Self-MoA as depicted in Figure~\ref{fig:illustration}b, which utilizes the same prompting template as MoA but aggregates outputs that are repeatedly sampled from the same model, rather than from a set of different models. To distinguish, we use Mixed-MoA to refer to MoA configurations that combine different individual models when necessary.

Surprisingly, we find that Mixed-MoA is usually sub-optimal compared with Self-MoA, especially when there exist significant quality differences among proposers. Specifically, we revisit the same experiment setting of MoA with six open-source instruction fine-tuned models as~\citet{wang2024mixture}. Compared with Mixed-MoA which aggregates all six models, Self-MoA on the strongest model achieves 6.6 point improvement over its mixed counterpart on the AlpacaEval 2.0 benchmark, showing a case of when intra-model diversity is more effective. Moreover, Self-MoA on two best-performed models on AlpacaEval 2.0 consistently achieves a 2-3 point gain and secures the top position among non-adversarial methods on the leaderboard, which further confirms the effectiveness of Self-MoA in this task.

To explore the limits of model diversity for MoA, we extend our experiments to a setting with three specialized models, each excelling in a specific task. Specifically, we utilize Qwen2-7B-Instruct~\citep{bai2023qwen} for common sense QA (MMLU-redux~\citep{gema2024we}), Qwen2-Math-7B-Instruct~\citep{bai2023qwen} for mathematics (MATH~\citep{hendrycks2020measuring}), and DeepSeek-Coder-V2-Lite-Instruct~\citep{zhu2024deepseek} for coding (CRUX~\citep{gu2024cruxeval}). We compare Self-MoA against a range of Mixed-MoA strategies, evaluating 13 combinations of individual models based on their average performance across the three tasks. Our findings indicate that employing task-specific models as proposers for Self-MoA can significantly outperform the best Mixed-MoA. Furthermore, even in a constructed mixture task tailored for Mixed-MoA where each individual model excels in a specific subtask, only two Mixed-MoA strategies slightly outperform Self-MoA by 0.17\% and 0.35\%.

To better understand the effectiveness of Self-MoA, we conduct a comprehensive investigation of the trade-off between quality and diversity in MoA, involving over 200 experiments. We use the Vendi Score~\citep{dan2023vendi} to evaluate the diversity among the outputs of the proposers, while the average performance of the proposers serves as the measure of quality. In Section~\ref{sect:trade-off}, we confirm that MoA performance has a positive correlation with both quality and diversity. Moreover, we clearly show a trade-off along the achievable Pareto front of quality and diversity. Interestingly, we find that MoA is quite sensitive to variations in quality, with optimal performance typically occurring in regions characterized by high quality and relatively low diversity. This finding naturally explains the effectiveness of Self-MoA, as it utilizes the strongest model as the proposer, ensuring high quality in its outputs.

Finally, we evaluate the performance of Self-MoA under increasing computational budgets. As the number of outputs grows, the scalability of Self-MoA becomes constrained by the context length of the aggregator. To address this issue, we introduce Self-MoA-Seq (Figure~\ref{fig:illustration}c), a sequential version that processes samples using a sliding window, allowing it to handle an arbitrary number of model outputs. Our findings show that Self-MoA-Seq performs at least as effectively as Self-MoA, enabling scalable ensembling for LLMs with shorter context lengths without compromising final performance.

Overall, our contributions are three-fold:
\begin{itemize}
    \item We introduce Self-MoA, which leverages in-model diversity by synthesizing multiple outputs from the same model. Surprisingly, it demonstrates superior performance compared to existing Mixed-MoA approaches, which emphasize cross-model diversity, across a wide range of benchmarks.
    \item Through systematic experiments and statistical analysis, we uncover a core trade-off between diversity and quality among the proposers, emphasizing that MoA is highly sensitive to proposer quality. This finding also explains the success of Self-MoA, which leverages outputs from the highest-performing model, ensuring superior overall quality.
    \item We extend Self-MoA to its sequential version Self-MoA-Seq, which iteratively aggregates a small amount of outputs step by step. Self-MoA-Seq unlocks LLMs that are constrained by the context length and enables computation scaling during inference.
\end{itemize}


\section{Related Work}

\label{sect:related_work}

\paragraph{Ensembles of LLMs.} 
Model ensembling aims to combine strengths from multiple models. Previous studies have explored various methods to leverage a diverse set of models, including but not limited to prompting~\citep{wang2024mixture}, 
weight averaging~\citep{lin2024mitigatingalignmenttaxrlhf,ramé2024warpbenefitsweightaveraged}, routing~\citep{jiang2024mixtralexperts,lu2023routingexpertefficientrewardguided}, training a generative fusion model~\citep{jiang2023llmblenderensemblinglargelanguage}, and so on. \citet{zhang2024towards} argues that the fusion of specialized models with certain general abilities could be a promising direction toward Artificial General Intelligence. 
Mixture-of-Agents (MoA, \citet{wang2024mixture}) first queries multiple LLMs to generate responses, then iteratively aggregates these samples through several rounds of synthesis. MoA shows promising results on several benchmarks, and its variants achieve superior performance on the AlpacaEval 2.0 leaderboard. 
Our method is inspired by the prompt pipeline proposed in MoA. However, while existing MoA focuses on unleashing the strength from multiple different models~\citep{wang2024mixture,jiang2023llmblenderensemblinglargelanguage, zhang2024diversity}, we demonstrate the trade-off between diversity and quality within the proposers, highlighting that focusing solely on diversity may compromise overall quality and final performance.

\paragraph{LLM Inference with Repeated Sampling.}
Previous studies have shown that combining model outputs from repeated sampling can yield a better response in various domains. In tasks with automatic verifiers available, such as math~\citep{hendrycks2021measuring} and code~\citep{chen2021evaluating}, simply sampling LLMs multiple times can significantly improve the pass@k metric and hence boost the success rate of solving the tasks~\citep{roziere2023code,li2022competition,brown2024large}. In more general tasks without verification tools, we can conduct techniques like majority vote, self-consistency, and best-of-n to choose the most promising one from candidate responses~\citep{wang2022self,chen2023universal,gui2024bonbon,li2024agentsneed}.
Therefore, repeated sampling is recently regarded as one approach of scaling compute during inference time~\citep{brown2024large}.
In this work, we identify the surprising effectiveness of repeated sampling in the context of MoA. Unlike majority vote or best-of-N, Self-MoA asks LLMs to synthesize outputs generated from repeated sampling, hence can further improve over each individual output.

\paragraph{Collaborative Agents} There is a surge of interest in building agent systems based on verification, critique, discussion, and refinement. For example, \citet{stechly2023gpt}, \citet{valmeekam2023can}, and \citet{madaan2024self} use self-critique to iteratively refine outputs through a chain structure. \citet{madaan2024self}, \citet{chen2024moa}, and \citet{wang2024mixture} explore the incorporation of multiple models to create a stronger agent that outperforms each individual model. \citet{du2023improving} incorporates multiple LLMs that propose and debate their individual responses over several rounds to reach a common final answer. \citet{liang2023encouraging} proposes Multi-Agent Debate, which encourages divergent thinking during LLM debates to arrive at more informative conclusions and avoid rushing to incorrect answers. \citet{chen2023reconcile} introduces RECONCILE, which adopts a confidence-weighted voting mechanism for better consensus among LLM discussions. Interestingly, \citet{wang2024rethinking} shows that a single model with carefully designed prompts can sometimes match the performance of agent discussions. Moreover, agent discussions mainly outperform a single LLM when the prompts are insufficient.

\section{Is Ensembling Different LLMs Beneficial?}
\label{sect:main_exp}
As introduced in Section~\ref{sect:intro}, previous research primarily emphasizes \textbf{cross-model diversity}, which can inadvertently include low-quality proposers. 
In this work, we introduce Self-MoA (Figure~\ref{fig:illustration}), which uses a single top-performing model to generate multiple outputs and aggregate them to produce the final result. Self-MoA leverages \textbf{in-model diversity} as repeated sampling often produces varied outputs. We propose our research question as follows:

\begin{center}
    \textit{Does the benefit of MoA stem from cross-model diversity? \\ Can we build a stronger MoA using in-model diversity?}
\end{center}


\begin{figure*}[t]
% \vspace{-0.4in}
    \centering  
    \includegraphics[width=0.9\linewidth]{figure/illustration.pdf}
    \vspace{-0.1in}
    \caption{Comparison of MoA, Self-MoA, and Self-MoA-Seq. (a) In MoA, multiple models respond to a query, followed by an aggregator synthesizing their outputs. (b) Self-MoA simplifies this by repeatedly sampling from a single model. (c) Self-MoA-Seq extends Self-MoA by applying a sliding window to combine the best output so far with candidate outputs. At each timestep, the synthesized output is repeated to bias the aggregator towards it, reducing the context length requirements and expanding the method's applicability. Note that MoA can extend to multiple rounds of aggregation (Appendix~\ref{sect:multi-layer}), while Self-MoA and Self-MoA-Seq can extend to more outputs, but we omit them here for clarity.}
    \label{fig:illustration}
    % \vspace{-0.2in}
\end{figure*}

\subsection{Experiments on AlpacaEval 2.0 with General Purpose Models}
\paragraph{Evaluation benchmarks.} We adopt the same experiment setting as \citet{wang2024mixture} in AlpacaEval 2.0 benchmark~\citep{dubois2024length} and compare the performance of Mixed-MoA and Self-MoA\footnote{We note that this experiment is similar to the ``single-proposer" setting in \citet{wang2024mixture}, however our reproduced result is different. We conjecture that such a major difference is due to different choices of the proposer model, which is not mentioned in \citet{wang2024mixture}. As we shall see later in Section~\ref{sect:trade-off}, ensembling performance is more sensitive to quality rather than diversity. Therefore, a worse proposer model will lead to suboptimal performance of Self-MoA.}.
AlpacaEval 2.0 is a widely used benchmark for assessing the instruction-following abilities of LLMs. It offers a set of real-world instructions and employs a GPT-4-based annotator to compare the model's responses against reference answers generated by GPT-4. To address length bias inherent in model-based evaluation, \citet{dubois2024length} introduced the length-controlled (LC) win rate as a more robust evaluation metric.

\paragraph{Models.} Following \citet{wang2024mixture}, we construct MoA based on six individual models: Qwen1.5-110B-Chat~\citep{bai2023qwen}, Qwen1.5-72B-Chat~\citep{bai2023qwen}, WizardLM-8x22B~\citep{xu2023wizardlm}, LLaMA-3-70B-Instruct~\citep{touvron2023llama}, Mixtral-8x22B-Instruct-v0.1~\citep{jiang2024mixtral}, and dbrx-instruct~\citep{mosaic2024introducing}.
Each model is sampled with a temperature of 0.7, following the default in ~\citep{wang2024mixture}. 
For Self-MoA, we aggregate six outputs sampled from WizardLM-2-8x22B, as it consistently outperforms the other models.
In line with \citet{wang2024mixture}, we use Qwen1.5-110B-Chat as the aggregator for both Mixed-MoA and Self-MoA.

\paragraph{Results.} We present the LC win rate for each model configuration in Table~\ref{tab:ae2}. 
For individual models, we report the higher value between the leaderboard results and our reproduction.
Notably, Self-MoA demonstrates remarkable effectiveness in this task, outperforming the Mixed-MoA baseline by 6.6 point. 
This suggests that, while using multiple models intuitively offers greater diversity, ensembling multiple outputs from a single model is more effective.

\begin{table}[t]
% \addtolength{\tabcolsep}{-3pt}
% \footnotesize
\vspace{-0.1in}
    \centering
    \caption{Comparison of Self-MoA and Mixed-MoA on AlpacaEval 2.0 leaderboard. We use Qwen1.5-110B-Chat as the aggregator.}
    \label{tab:ae2}
    \vskip 0.1in
    \begin{tabular}{l|l|c}  % Specifies three columns: left, left, center
        \toprule
         & \textbf{Model Configuration} & \textbf{LC Win Rate} \\
        \midrule
        \multirow{6}{*}{Individual} & WizardLM-2-8x22B & 53.1 \\
                                           & Qwen1.5-110B-Chat & 43.9 \\
                                           & LLaMA-3-70B-Instruct & 34.4 \\
                                           & Qwen1.5-72B-Chat & 36.6 \\
                                           & Mixtral-8x22B-Instruct-v0.1 & 30.2 \\
                                           & dbrx-instruct & 25.4 \\
        \midrule
        \multirow{1}{*}{Mixed-MoA}  & 2-Layer MoA \citep{wang2024mixture} & 59.1 \\
        \midrule
        Self-MoA & 2-Layer Self-MoA + WizardLM & \textbf{65.7} \\
        \bottomrule
    \end{tabular}
% \vspace{-0.2in}
\end{table}

\paragraph{Applying Self-MoA on top performing models.} To further validate the effectiveness of Self-MoA, we apply it to the two top-performing models on AlpacaEval 2.0: gemma-2-9b-it-WPO-HB~\citep{zhou2024wpo} and gemma-2-9b-it-SimPO~\citep{meng2024simpo}. We use each model as both the proposer and the aggregator\footnote{Qwen1.5-110B-Chat is not used as the aggregator since the two top models significantly outperform it.}, with a temperature of 0.7 for all the generations. Due to the context length constraint of Gemma 2~\citep{gemmateam2024gemma2improvingopen}, the aggregator can only take four samples as the input. As shown in Table~\ref{tab:ae2-gemma}, Self-MoA consistently achieves a 2-3 point gain and secures the top position on the leaderboard during submission.

\paragraph{Results on MT-Bench.} Beyond AlpacaEval 2.0, we further evaluate Self-MoA and Mixed-MoA on MT-Bench~\citep{zheng2023judging}, another benchmark used in~\citet{wang2024mixture}. The results align with our findings from AlpacaEval 2.0, reinforcing the effectiveness of Self-MoA. Please refer to Appendix~\ref{sect:mt_bench} for more details.

\begin{table}[t!]
% \addtolength{\tabcolsep}{-3pt}
% \footnotesize
    \centering
    \caption{Self-MoA achieves state-of-the-art performance on the AlpacaEval 2.0 leaderboard when using top-performing models as both proposers and aggregators. We only ensemble 4 outputs due to context window constraints.}
    \label{tab:ae2-gemma}
    \vskip 0.1in
    \begin{tabular}{l|l|c}  % Specifies three columns: left, left, center
        \toprule
        & \textbf{Model Configuration} & \textbf{LC Win Rate} \\
        \midrule
        \multirow{2}{*}{Individual} & gemma-2-9b-it-WPO-HB & 76.7 \\
                                           & gemma-2-9b-it-SimPO & 72.4 \\
        \midrule
        \multirow{2}{*}{Self-MoA} & Self-MoA + gemma-2-9b-it-WPO-HB & \textbf{78.5} \\
                                           & Self-MoA + gemma-2-9b-it-SimPO & 75.0 \\
                                           
        \bottomrule
    \end{tabular}
% \vspace{-0.2in}
\end{table}


\subsection{Experiments on Multiple Datasets with Specialized Models}
\label{sect:mixed_experiment}
In this section, we compare different ensembling methods on a diverse set of benchmarks using specialized models. 

\paragraph{Evaluation datasets.} We conduct evaluations across a diverse set of benchmarks:
\begin{itemize}
    \item MMLU~\citep{hendrycks2020measuring} is a multiple-choice dataset designed to assess a model's multitask accuracy. MMLU is widely used to evaluate both the breadth and depth of language understanding capabilities of current LLMs across a diverse array of subjects, including mathematics, history, computer science, logic, and law. We adopt MMLU-redux~\citep{gema2024we} for evaluation, which is a subset of MMLU with 3,000 samples fixing the errors in the dataset through human re-annotating. 
    \item CRUX~\citep{gu2024cruxeval} consists of 800 Python code functions, each containing 3 to 13 lines along with an input-output pair. Based on this dataset, \cite{gu2024cruxeval} constructs two tasks: input prediction and output prediction. To successfully complete these tasks, the LLM must demonstrate code reasoning abilities. 
    \item MATH~\citep{hendrycks2021measuring} comprises 12,500 challenging competition-level mathematics problems. For our analysis, we utilize the testing subset of MATH, which consists of 5,000 samples.
\end{itemize}

\begin{table}[t!]
% \addtolength{\tabcolsep}{-3pt}
% \footnotesize
\vspace{-0.1in}
    \centering
    \caption{Comparison of Self-MoA and Mixed-MoA in MMLU, CRUX, and MATH. 
    The labels \texttt{i}, \texttt{m}, and \texttt{d} refer to Qwen2-7B-Instruct, DeepSeek-Coder-V2-Lite-Instruct, and Qwen2-Math-7B-Instruct, respectively. The average performance represents the mean accuracy across MMLU, CRUX, and MATH. \texttt{TaskBest} indicates that we use the strongest model for each task as both proposer and aggregator.}
    \label{tab:mix_data}
    \vskip 0.1in
    \begin{tabular}{l|c|c|c|c|c}
    \toprule
        & Aggregator & Proposer & MMLU & CRUX & MATH \\
    \midrule
    \multirow{3}{*}{Individual}&  -    & \texttt{i}& 66.16 & 36.25 & 53.81 \\
    & - &\texttt{d}& 60.91  & 49.51 & 53.82 \\
    & - &\texttt{m}& 54.36 & 27.88 & 69.57\tablefootnote{\label{math}As Qwen2-Math-7B-Instruct only supports context length of 4096, for these two data points, we sample the proposer with a reduced token length of 1024, and only aggregates three outputs from the proposer.} \\
    \midrule 

\multirow{13}{*}{Mixed-MoA}
&\multirow{13}{*}{\texttt{i}} &   \texttt{iimmdd} &     67.89 &     42.88 &     64.38  \\
&&   \texttt{imdddd} &     67.42 &     44.50 &     63.90  \\
&&   \texttt{iiiimd} &     68.90 &     41.25 &     63.00  \\
&&   \texttt{immmmd} &     66.63 &     42.75 &     66.02  \\
&&   \texttt{iimmmm} &     66.23 &     39.25 &     66.10  \\
&&   \texttt{iiimmm} &     67.49 &     38.25 &     64.16  \\
&&   \texttt{iiiimm} &     68.00 &     37.00 &     62.92  \\
&&   \texttt{iidddd} &     68.21 &     45.50 &     62.56  \\
&&   \texttt{iiiddd} &     68.21 &     42.88 &     62.38  \\
&&   \texttt{iiiidd} &     68.47 &     40.75 &     61.24  \\
&&   \texttt{mmdddd} &     66.34 &     46.75 &     66.48  \\
&&   \texttt{mmmddd} &     65.80 &     47.00 &     67.32  \\
&&   \texttt{mmmmdd} &     65.44 &     42.50 &     67.62  \\
         
    \midrule
    \multirow{2}{*}{Self-MoA} &\texttt{i}& $6\times$TaskBest & \textbf{69.01}	& 50.75	&  68.42  \\
        & TaskBest & $6\times$TaskBest &\textbf{69.01} & \textbf{52.62} & \textbf{69.80}\textsuperscript{\ref{math}}  \\
     \bottomrule
    \end{tabular}
% \vspace{-0.2in}
\end{table}
% \footnotemark[*]


\paragraph{Models.} To ensure sufficient diversity, we select three LLMs with specialized strengths: Qwen2-7B-Instruct~\citep{yang2024qwen2technicalreport}, DeepSeek-Coder-V2-Lite-Instruct~\citep{zhu2024deepseek}, and Qwen2-Math-7B-Instruct. 
We fix the number of proposers to six and sweep various combinations of these three models. For convenience, we denote Qwen2-7B-Instruct as \texttt{i}, DeepSeek-Coder-V2-Lite-Instruct as \texttt{d}, and Qwen2-Math-7B-Instruct as \texttt{m}. As shown in Table~\ref{tab:mix_data}, Qwen2-7B-Instruct, DeepSeek-Coder-V2-Lite-Instruct, and Qwen2-Math-7B-Instruct excel on MMLU, CRUX, and MATH, respectively. We use the short name for the mixture of proposers. For example, \texttt{iiddmm} indicates the inclusion of two samples from each model respectively. When a model is represented multiple times in the proposer mixture, we ensure that two samples are generated with different random seeds. We set the temperature of each model to be 0.7 for the individual model, and use temperature 0 for the aggregator. We mainly use Qwen2-7B-Instruct as the aggregator but also try different models as the aggregator.  
We explore various MoA configurations, including individual models, combinations of two or three models as proposers, and using a single top-performing model (TaskBest, for example DeepSeek-Coder-V2-Lite-Instruct for CRUX) as the proposer (Self-MoA).


\paragraph{Results.} The results are presented in Table~\ref{tab:mix_data}. 
When using \texttt{i} as the aggregator, Self-MoA with the TaskBest model consistently outperforms all 13 tested Mixed-MoA configurations across all tasks. Furthermore, adopting a task-specific aggregator yields an additional performance boost of 1-2 points.
Interestingly, increasing model diversity does not always lead to better performance. For instance, while MoA with \texttt{iimmdd} surpasses \texttt{mmmddd} on MMLU, it underperforms on CRUX and MATH. This discrepancy aligns with the relative strengths of the individual models—\texttt{i} excels on MMLU but lags behind on CRUX and MATH. We postpone more discussion to Section~\ref{sect:when-mixed}.



\section{The Quality-Diversity Trade-off}
\label{sect:trade-off}

\begin{figure*}[t!]
\vspace{-0.1in}
    \centering  
    
    \includegraphics[width=0.33\linewidth, trim=0.3in 0 1.5in 1, clip]{figure/QualityDiversity/mmlu_plot.pdf}
    \includegraphics[width=0.31\linewidth, trim=0.8in 0 1.5in 1, clip]{figure/QualityDiversity/crux_plot.pdf}
    \includegraphics[width=0.32\linewidth, trim=0.8in 0 1.2in 1, clip]{figure/QualityDiversity/math_plot.pdf}
    % \vspace{-0.3in}
    \caption{The diversity-quality trade-off: Mixed-MoA incorporates different individual models as proposers, while Self-MoA uses the same individual model for this role. Quality is assessed based on the average performance of each proposer, and diversity is computed with the Vendi Score~\citep{dan2023vendi} of outputs generated by proposers on the same prompts.}
    \label{fig:diversity-quality-trade-off}
    % \vspace{-0.2in}
\end{figure*}

We investigate factors that contribute to the strong performance of Self-MoA through careful experiments. Previous studies have mainly focused on increasing model diversity within the group \citep{wang2024mixture, jiang2023llm, zhang2024diversity}. However, searching for diverse models can sometimes lead to including poorly performed models, resulting in a trade-off between diversity and quality, where quality refers to how well each individual model performs in the group. 

Therefore, we aim to identify the existence of a general relationship between MoA's performance and quality as well as diversity. Following Section~\ref{sect:main_exp}, we evaluate MoA's performance on MMLU, CRUX, and MATH, which cover tasks requiring a wide range of capabilities. We vary the quality and diversity with two orders of freedom: 1) combinations of individual models in proposers from Section~\ref{sect:mixed_experiment}; and 2) sampling temperature. i.e., 0.5, 0.7, 1.0, 1.1, and 1.2. This results in a total of over 70 unique MoA proposer mixtures. We measure the quality and diversity as follows:

\begin{itemize}
    \item \textbf{Diversity}: We utilize the Vendi Score \citep{dan2023vendi} to assess the diversity among individual models in the proposer mixture. The Vendi Score represents the effective number of unique elements within a collection of samples \citep{dan2023vendi}, with further details provided in the Appendix~\ref{sect:vendi}. Specifically, for a given prompt \( x \), we obtain responses from each model, denoted as \( y_1, y_2, \ldots, y_6 \). The diversity of the proposers for prompt \( x \), denoted as \( d(x) \), is calculated using the Vendi Score on the set \([y_1, \ldots, y_6]\). We then compute the overall diversity across the dataset \( S \) as:
  \[
  d = \frac{1}{|S|} \sum_{x \in S} d(x).
  \]
  \item \textbf{Quality}: We first determine the accuracy of each model on the dataset \( S \), yielding values \( q_1, q_2, \ldots, q_6 \). The average accuracy, \( q = \frac{1}{6} (q_1 + q_2 + \ldots + q_6) \), serves as our measure of the quality of the proposers. We will explore additional quality measurement strategies in later sections.
\end{itemize}

\paragraph{Results.} We plot MoA's performance with corresponding diversity and quality for each mixture of proposers in Figure~\ref{fig:diversity-quality-trade-off}. 
We summarize key observations as follows:

\begin{itemize}
    \item The trends among MMLU, CRUX, and MATH are consistently aligned.
    \item When the quality is fixed, increasing diversity can enhance MoA's performance.
    \item When the diversity is fixed, improving quality can also boost MoA's performance.
    \item There exists a trade-off in the achievable Pareto front between diversity and quality.
    \item Notably, the best performance of MoA is typically observed in the bottom right of each subplot, indicating a strong sensitivity to quality.
\end{itemize}
Previous work on ensembles~\citep{wang2024mixture, jiang2023llm,zhang2024diversity} primarily focuses on increasing the diversity of models within the proposer mixture. However, as shown in Figure~\ref{fig:diversity-quality-trade-off}, compared to Self-MoA on the best-performing model, simply aiming for greater diversity in the proposer mixture can result in lower overall quality, which may negatively impact MoA's performance. This trade-off between diversity and quality helps to explain why Self-MoA achieves superior performance across various benchmarks.

\subsection{Statistical Analysis}
To further understand the numerical correlation between MoA's performance and diversity as well as quality, we conduct linear regression for MoA's performance \( t \) on diversity \( d \) and quality \( q \). Specifically, we fit the following equation for each dataset:
\begin{align}
    t = \alpha \times q + \beta \times d + \gamma,
    \label{eqn:linear_fit}
\end{align}
where \( \alpha, \beta, \gamma \in \mathbb{R} \) are real-valued coefficients to be determined. For each dataset, we collect around 70 data points from Figure~\ref{fig:diversity-quality-trade-off} 
to construct the set \( \{q^i, d^i, t^i\}_{i=1}^{N} \). 
The coefficients \( \alpha \), \( \beta \), and \( \gamma \) are then derived by solving a linear regression on \( \{q^i, d^i, t^i\}_{i=1}^{N} \). To make coefficients \( \alpha \) and \( \beta \) comparable, we normalize \( q \) and \( d \) by subtracting their means and dividing by their standard deviations (detailed in Appendix~\ref{sect:normalization}), respectively. The results are presented in Table~\ref{tab:linear_fit}. We observe that the p-values for both \( \alpha \) and \( \beta \) are less than 0.001, indicating a significant correlation between MoA's performance and both quality and diversity \citep{Arnold1990IntroductionTT}. The $R^2$ values from the linear regression across three datasets are approximately around 0.7, indicating that the linear model based on quality and diversity explains 70\% MoA's performance and hence a strong correlation between inputs and outputs, according to Appendix~\ref{sect:implication_r_square}. In later parts, we show that using a more fine-grained quality calculation can further increase the $R^2$ value. 

\begin{table}[t!]
% \footnotesize
% \addtolength{\tabcolsep}{-4pt}
\vspace{-0.1in}
    \centering
     \caption{Linear regression (Equation~\ref{eqn:linear_fit}) of MoA's performance $t$ on diversity $d$ and quality $q$.}
    \label{tab:linear_fit}
    \vskip 0.1in
    \begin{tabular}{l|c|c|c|c|c}
    \toprule
       \multirow{2}{*}{\textbf{Dataset}}   & \multicolumn{2}{c|}{$\alpha$} & \multicolumn{2}{c|}{$\beta$} & \multirow{2}{*}{{$R^2$}}  \\
       % \midrule 
       & Coefficient  & P-value & Coefficient  & P-value \\
       \midrule
        MMLU & 2.558 $\pm$ 0.176 & $<0.001$ & 1.841 $\pm$ 0.176 &  $<0.001$ &  0.771\\
        \midrule
         CRUX & 4.548 $\pm$ 0.459 & $<0.001$ & 1.421 $\pm$ 0.459 &  $<0.001$ &  0.685\\
         \midrule
         MATH &4.719 $\pm$ 0.416 & $<0.001$ & 2.839 $\pm$ 0.416 &  $<0.001$ &  0.760\\  
         \bottomrule
    \end{tabular}
% \vspace{-0.2in}
\end{table}


\paragraph{Comparing the effect strength of quality and diversity.} 
From Table~\ref{tab:linear_fit}, we observe that $\alpha$ is greater than $\beta$ across all three datasets. In particular, for CRUX and MATH, the gap between these two measures is even more pronounced. These results suggest that MoA's performance is particularly sensitive to variations in quality, highlighting the importance of prioritizing quality within the proposer mixture. This finding is also consistent with our observation that MoA achieves its best performance in the bottom right of the plot in Figure~\ref{fig:diversity-quality-trade-off}, further supporting the effectiveness of our proposed Self-MoA approach. 

\paragraph{Alternative quality measurements.} We use the averaged accuracy of each individual model to measure quality in the previous analysis. In this section, we explore alternative methods for assessing the quality of proposers. Recall that \( q_1, \ldots, q_6 \) denote the accuracy of each individual model among proposers, and without loss of generality, we assume \( q_1 \geq q_2 \geq \ldots \geq q_6 \). It is reasonable to assume that the aggregator can select the correct answer from the proposers, particularly when the responses of individual models are inconsistent. In such cases, the aggregator would rely more heavily on models with better individual performance, meaning the weight of \( q_1 \) would be greater than that of \( q_6 \).

Therefore, we compare the following methods to calculate quality: 
\begin{itemize}
    \item \textbf{Average}: \( \frac{1}{6} \sum_{i=1}^6 q_i \).
    \item \textbf{K-Norm}: \( \left( \frac{1}{6} \sum_{i=1}^6 q_i^K \right)^{1/K} \), where a larger \( K \) places more emphasis on stronger individual models.
    \item \textbf{Centered-1/K-Norm}: \( q_1 - \left( \frac{1}{6} \sum_{i=1}^6 (q_1 - q_i)^{1/K} \right)^K \). In this formulation, we first compute the difference between \( q_i \) and the best model's \( q_1 \). The \( 1/K \) norm emphasizes the weights of models whose performance is closer to \( q_1 \). 
\end{itemize}
All three methods are the same when \( K=1 \). For each quality measurement, we fit a linear regression to assess the relationship between MoA's performance and the quality and diversity metrics, reporting the $R^2$ values in Table~\ref{tab:quality_meansurement_method}. Our analysis shows that in MMLU and CRUX, applying a larger weight to better-performing individual models tends to increase the $R^2$ values. However, this trend is inconsistent for MATH. We conjecture that this inconsistency arises because the aggregator Qwen2-7B-Instruct is relatively weak on MATH compared to the strongest individual model, Qwen2-Math-7B-Instruct. This limitation constrains the performance of MoA, leading to an inconsistent trend in the linear regression results. In contrast, on MMLU, where Qwen2-7B-Instruct is the strongest individual model, we find that the $R^2$ value can exceed 0.9 with \( K=2 \) using the Centered-1/K-Norm. This indicates a very strong linear relationship between MoA performance and the quality and diversity metrics. Overall, we conclude that employing Centered-1/K-Norm with \( K=2 \) (marked in {\color{blue}blue}) achieves strong performance across all three datasets.  

\begin{table}[t!]
% \footnotesize
% \addtolength{\tabcolsep}{-3pt}
\vspace{-0.1in}
    \centering
    \caption{The $R^2$ of the linear regression when we use different quality measurement methods. We find using Centered-1/K-Norm with K=2 can achieve good performance among all these three datasets. }
    \label{tab:quality_meansurement_method}
    \vskip 0.1in
    \begin{tabular}{c|c|c c c c c }
        \toprule
       Dataset & Method & Avg. (K=1) & K=2 & K=3 & K=4&  \\
       \midrule
      \multirow{2}{*}{MMLU}   & K-Norm& 0.771 & 0.809 & 0.832 & 0.845\\
       & Centered-1/K-Norm& 0.771 & {\color{blue}0.881} & 0.902 & 0.903\\
      \midrule
      \multirow{2}{*}{CRUX}  & K-Norm&0.685& 0.736& 0.765& 0.779\\
      & Centered-1/K-Norm& 0.685& {\color{blue}0.753}& 0.758& 0.753\\
      \midrule
      \multirow{2}{*}{MATH} & K-Norm& 0.760& 0.720& 0.692& 0.672 \\
       & Centered-1/K-Norm& 0.760& {\color{blue}0.720}& 0.692& 0.672 \\
      \bottomrule
    \end{tabular}
% \vspace{-0.2in}
\end{table}

\subsection{When Mixed-MoA Outperforms Self-MoA?}
\label{sect:when-mixed}
According to the quality-diversity trade-off illustrated in Figure~\ref{fig:diversity-quality-trade-off}, we conjecture that increasing diversity can enhance MoA's performance when the quality is controlled.

\begin{table}[t!]
% \footnotesize
% \addtolength{\tabcolsep}{-4pt}
\vspace{-0.1in}
    \centering
    \caption{Comparison of Self-MoA and Mixed-MoA on the mixture task of MMLU, CRUX, and MATH, measured by the average performance of three tasks from Table~\ref{tab:mix_data}. Mixed-MoA models with top two average performances are highlighted by \underline{underline}.}
    \label{tab:mixture}
    \vskip 0.1in
    \begin{tabular}{l|c|c|c}
    \toprule
        & Aggregator & Proposer & Average \\
    \midrule
    \multirow{3}{*}{Individual}&  -    & \texttt{i} & 52.07\\
        & - &\texttt{d}& 54.74\\
        & - &\texttt{m}& 50.60\\
    \midrule 
    \multirow{13}{*}{Mixed-MoA}
    &\multirow{13}{*}{\texttt{i}} &   \texttt{iimmdd} & 58.38 \\
        &&   \texttt{imdddd} & 58.61 \\
        &&   \texttt{iiiimd} & 57.72 \\
        &&   \texttt{immmmd} & 58.47 \\
        &&   \texttt{iimmmm} & 57.19 \\
        &&   \texttt{iiimmm} & 56.63 \\
        &&   \texttt{iiiimm} & 55.97 \\
        &&   \texttt{iidddd} & 58.76 \\
        &&   \texttt{iiiddd} & 57.82 \\
        &&   \texttt{iiiidd} & 56.82 \\
        &&   \texttt{mmdddd} & \underline{59.86} \\
        &&   \texttt{mmmddd} & \underline{60.04} \\
        &&   \texttt{mmmmdd} & 58.52 \\
    \midrule
    \multirow{3}{*}{Self-MoA } & \texttt{i} &   \texttt{dddddd} & 59.69 \\
        &\texttt{i}& $6\times$TaskBest & 62.73 \\
        & TaskBest & $6\times$TaskBest & \textbf{63.81} \\
    \bottomrule
    \end{tabular}
% \vspace{-0.2in}
  
\end{table}

Mixed-MoA generally exhibits greater diversity than Self-MoA, which can lead to improved performance when the model quality is similar. This advantage arises when individual models achieve similar overall performance while maintaining significant cross-model diversity. To simulate such a scenario, we construct a mixture task combining MMLU, CRUX, and MATH as described in Section~\ref{sect:mixed_experiment}. In this setting, test samples are drawn uniformly from the three tasks, and models do not have prior knowledge of a sample’s origin. For a given MoA strategy, we evaluate its performance on this mixture task by averaging its performance across the three datasets. The results are reported in Table~\ref{tab:mixture}. In this mixture task, each model specializes in different subtasks, with \texttt{i} performing best on MMLU, \texttt{d} on CRUX, and \texttt{m} on MATH. As TaskBest requires additional prior knowledge of the sample origin, we also report Self-MoA with \texttt{d} as the proposer, given that it achieves the highest average performance among individual models.

From Table~\ref{tab:mixture}, we observe that Mixed-MoA indeed outperforms Self-MoA of \texttt{dddddd}. Specifically, Mixed-MoA of \texttt{mmdddd} and \texttt{mmmddd} achieves the average performance of 59.86\% and 60.04\%, improves upon Self-MoA of \texttt{dddddd} by 0.17\% and 0.35\%. Given the reported small margin, we argue that Self-MoA is still a very competitive baseline under this setting, not to mention the dominant performance of Self-MoA over Mixed-MoA when focusing on one single task (Self-MoA with TaskBest models achieve an average of $3.8\%$ improvement from Table~\ref{tab:mixture}). In Appendix~\ref{sect:normalize_mix_data} we also report normalized results that account for different variances among tasks, which leads to a similar conclusion.

We further consider another single-task case on MMLU, involving two individual models: Llama-3.1-8B-Instruct and Qwen2-7B-Instruct, with Qwen2-7B-Instruct serving as the aggregator. We  choose Llama-3.1-8B-Instruct because it performs similarly to Qwen2-7B-Instruct as an individual model. Table~\ref{tab:llama_with_qwen-instruct} demonstrates that even when the performance of two individual models is close, Self-MoA—utilizing six Llama-3.1-8B-Instruct proposers (denoted as \texttt{llllll})—still outperforms the Mixed-MoA configuration (denoted as \texttt{iiilll}).

\begin{table}[t!]
\vspace{-0.1in}
    \caption{MoA of Llama-3.1-8B-Instruct and Qwen2-7B-Instruct. \texttt{l} is short for Llama-3.1-8B-Instruct and \texttt{i} is short for Qwen2-7B-Instruct.}
    \label{tab:llama_with_qwen-instruct}
    \vskip 0.1in
    \centering
    \begin{tabular}{l|c|c|c}
    \toprule
        & Aggregator & Proposer & MMLU \\
         \midrule
         \multirow{2}{*}{Individual}&  -    & \texttt{i} & 66.16 \\
                                        & - & \texttt{l} & 66.40  \\
                                        \midrule 
Mixed-MoA &  \texttt{i} &   \texttt{iiilll} &     70.73 \\
         \midrule
    \multirow{2}{*}{Self-MoA } & \texttt{i} &   \texttt{iiiiii} &     69.01 \\
    &  \texttt{i} &   \texttt{llllll} &     71.27 \\
     \bottomrule
    \end{tabular}
% \vspace{-0.1in}
\end{table}


\section{Scaling Inference Compute with Self-MoA}

In previous sections, we have provided evidence that Self-MoA over one strong model is straightforward but effective. As the community is becoming more aware of scaling inference time computing~\citep{brown2024large,snell2024scalingllmtesttimecompute,wu2024empiricalanalysiscomputeoptimalinference}, one natural question to ask is:
\begin{center}
    \textit{Given a strong model, does Self-MoA's performance scale with the number of repeated samples?}
\end{center}
Intuitively, Self-MoA cannot scale indefinitely by simply increasing the computation budget for at least three reasons: 
\begin{itemize}
    \item As more responses are sampled from a single model, the diversity among those samples tends to plateau.
    \item Aggregating information from many samples is more challenging for LLMs compared to handling a smaller number of samples.
    \item Every LLM has a context length limit (e.g., 8192 tokens for Gemma 2), which restricts the number of responses an aggregator can process at once.
\end{itemize}
While the first limitation is inherent to repeated sampling, we address the latter two by introducing Self-MoA-Seq, a sequential variant designed to manage large numbers of responses without overwhelming the aggregator. Self-MoA-Seq uses a sliding window to aggregate a fixed number of responses at a time, allowing it to handle an unlimited number of responses, regardless of context length constraints. A visual illustration is provided in Figure~\ref{fig:illustration}.

We evaluate the performance of Self-MoA and Self-MoA-Seq with increasing sample sizes on the MMLU and CRUX benchmarks to study their scaling behavior. For each benchmark, we use the best-performing model as both the proposer and aggregator (Qwen2-7B-Instruct for MMLU and DeepSeek-Coder-V2-Lite-Instruct for CRUX), with a sampling temperature of 0.7. In Self-MoA-Seq, the window size is set to six, with the first three slots reserved for the current synthesized output. We vary the number of samples from 6 to 30 and plot the accuracy curves from three runs with different seeds in Figure~\ref{fig:sequential}. Our key observations are as follows:
\begin{itemize}
    \item Both Self-MoA and Self-MoA-Seq significantly improve performance over the individual base model.
    \item Adding more samples can have both positive and negative effects, meaning there is no universal compute-optimal solution.
    \item Self-MoA-Seq delivers performance that is comparable to, or slightly better than, Self-MoA.
\end{itemize}
These findings suggest that Self-MoA-Seq can extend the effectiveness of Self-MoA to LLMs with shorter context lengths, without sacrificing performance.
Following Section~\ref{sect:when-mixed}, we explore whether introducing a second model can enhance performance in the sequential setting. Given that Llama-3.1-8B-Instruct performs similarly to Qwen2-7B-Instruct on the MMLU task, we compare the impact of adding Llama-3.1-8B-Instruct and DeepSeek-Coder-V2-Lite-Instruct (which underperforms Qwen2-7B-Instruct by 5\%) after aggregating 30 samples from Qwen2-7B-Instruct in Self-MoA-Seq. We find that incorporating Llama-3.1-8B-Instruct boosts accuracy by around 2\%, whereas adding DeepSeek-Coder-V2-Lite-Instruct reduces accuracy by more than 1.5\%. This result provides another example of cross-model diversity benefiting MoA, and shows the potential of Self-MoA-Seq with increasing computation budget.

\begin{figure*}[t!]
\vspace{-0.1in}
    \centering  
    \includegraphics[width=0.7\linewidth]{figure/Sequential/merged_plots.pdf}
    \vspace{-0.2in}
    \caption{The performance of Self-MoA and Self-MoA-Seq with a growing number of samples. Dashed lines indicate the performance of a single forward pass with the base model.}
    \label{fig:sequential}
    \vspace{-0.1in}
\end{figure*}

\section{Conclusion}
In this paper, we introduce Self-MoA, an innovative approach that utilizes in-model diversity to enhance the performance of large language models during inference. Our experiments demonstrate that Self-MoA outperforms traditional Mixed-MoA strategies in many popular benchmarks, particularly when the proposer model quality varies. By aggregating outputs from a single high-performing model, Self-MoA effectively addresses the quality-diversity trade-off. We further identify the scenarios where mixing LLM can be potentially beneficial and extend Self-MoA to the constrained context length setting. These findings highlight the potential of in-model diversity in optimizing LLM performance and pave the way for further advancements in ensemble methods.


\newpage


\bibliography{main}
\bibliographystyle{abbrvnat}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% APPENDIX
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage
\appendix
\onecolumn
% \section{You \emph{can} have an appendix here.}

% You can have as much text here as you want. The main body must be at most $8$ pages long.
% For the final version, one more page can be added.
% If you want, you can use an appendix like this one.  

% The $\mathtt{\backslash onecolumn}$ command above can be kept in place if you prefer a one-column appendix, or can be removed if you prefer a two-column appendix.  Apart from this possible change, the style (font size, spacing, margins, page numbering, etc.) should be kept the same as the main body.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Supplements}
\subsection{Multi-Layer MoA}
\label{sect:multi-layer}
MoA can be extended to multiple layers. For MoA with $l$ layers and $n$ LLMs $\left\{A_{i,j}\right\}_{j=1}^n$ in each layer $i$, we can formulate it as follows:
\begin{equation*}
    y_i = \bigoplus_{j=1}^{n} \left[ A_{i,j}(x_i) \right] + x_1, \quad x_{i+1} = y_i,
\end{equation*}
where each LLM $A_i^j$ generates a response for the query $x_i$, which is further concatenated with the original query by the aggregator's prompt $\bigoplus$.

Table~\ref{tab:ae2_3l} compares the performance of 3-Layer Mixed-MoA and 2-Layer Self-MoA as well as the total number of forward passes required for each method. Specifically, one forward pass is counted each time a proposer model generates an output or an aggregator synthesizes a result. 
Notably, Self-MoA outperforms the 3-Layer Mixed-MoA baseline with only half the forward passes. 

\begin{table}[h!]
% \vspace{-0.2in}
    \centering
    \caption{Results of 3-Layer Mixed-MoA.}
    \label{tab:ae2_3l}
    \vskip 0.1in
    \begin{tabular}{l|l|c|c}  % Specifies three columns: left, left, center
        \toprule
         & \textbf{Model Configuration} & \textbf{LC Win Rate} & \textbf{\# Forward Passes} \\
        \midrule
        Mixed-MoA & 3-Layer MoA \citep{wang2024mixture}  & 65.4 & 13 \\
        \midrule
        Self-MoA & 2-Layer Self-MoA + WizardLM-2-8x22B & \textbf{65.7} & 7 \\
        \bottomrule
    \end{tabular}
% \vspace{-0.2in}
\end{table}


\subsection{Vendi Score}
\label{sect:vendi}
The Vendi Score (VS) is a metric designed to evaluate diversity in machine learning. It takes as input a collection of samples along with a pairwise similarity function, and it outputs a single value that represents the effective number of unique elements within the sample set. 

The score is computed using a positive semi-definite similarity matrix \( K \in \mathbb{R}^{n \times n} \) as follows:

\[
VS(K) = \exp\left(-\text{tr}\left(\frac{K}{n} \log\left(\frac{K}{n}\right)\right)\right) = \exp\left(-\sum_{i=1}^{n} \lambda_i \log(\lambda_i)\right)
\]

Here, \( \lambda_i \) are the eigenvalues of the normalized matrix \( \frac{K}{n} \), and \( 0 \log 0 = 0 \). Essentially, the Vendi Score is the exponential of the von Neumann entropy of \( \frac{K}{n} \), which reflects the Shannon entropy of its eigenvalues, also referred to as the effective rank. This metric provides a quantitative measure of diversity based on the distribution of similarity scores among the samples.
\subsection{Normalization of Inputs}
\label{sect:normalization}
Given a sequence of inputs $x_1, ..., x_n$. Let $x'$ denote the normalized $x$. We have 
\begin{align*}
    x'=\frac{x_i - \bar x}{\mbox{std}(x)}, \mbox{ where } \bar x = \frac{1}{n} \sum_{i=1}^n x_i, \mbox{ and } \mbox{std}(x) = \sqrt{\frac{1}{n} \sum_{i=1}^n (x_i -\bar x)^2}
\end{align*}
\subsection{Implication of R-squre}
\label{sect:implication_r_square}
The implications of $R^2$ are presented in Table \ref{tab:R-square_interpration}, illustrating the degree of influence between the independent and dependent variables.
\citep{sarjana2020mathematical}. 
\begin{table}[h]
    \centering
    \caption{The interpretation of R-square}
    \label{tab:R-square_interpration}    \begin{tabular}{c|c}
    \toprule
        R-square & Level \\
         \midrule 
       $[0, 0.2)$  & Very weak\\
       \midrule
       $[0.2, 0.4)$  & Weak\\
       \midrule
       $[0.4, 0.6)$  & Median\\
       \midrule
       $[0.6, 0.8)$  & Strong\\
       \midrule
       $[0.8, 1.0]$  & Very Strong\\
         \bottomrule
    \end{tabular}
\end{table}

\section{Additional Results}
\subsection{MT-Bench Results}
\label{sect:mt_bench}
We also compare MoA and Self-MoA on the MT-Bench~\citep{zheng2023judging} benchmark under the same experiment setting as~\citet{wang2024mixture}. We copy the numbers from~\citet{wang2024mixture} for 3-Layer MoA settings, and report our implemented results for the other experiments to ensure that 2-Layer experiments are fair comparisons. Table~\ref{tab:mt-bench} shows that Self-MoA outperforms its Mixed-MoA counterpart, and using GPT-4o as the aggregator can achieve the best performance even with fewer forward passes compared to 3-Layer MoA with GPT-4o.

\begin{table}[t!]
% \vspace{-0.2in}
    \centering
    \caption{Comparison of Self-MoA and Mixed-MoA on MT-Bench. We use Qwen1.5-110B-Chat and GPT-4o as the aggregator.}
    \label{tab:mt-bench}
    \vskip 0.1in
    \begin{tabular}{l|l|c|c|c|c}  % Specifies three columns: left, left, center
        \toprule
         & \textbf{Model Configuration} & \textbf{Avg.} & \textbf{1st turn} & \textbf{2nd turn} & \textbf{\# Forward Passes} \\
        \midrule
        \multirow{6}{*}{Individual} & WizardLM-2-8x22B & 8.99 & 9.05 & 8.93 & 1 \\
                                           & Qwen1.5-110B-Chat & 8.61 & 8.77 & 8.45 & 1 \\
                                           & LLaMA-3-70B-Instruct & 8.84 & 9.14 & 8.54 & 1 \\
                                           & Qwen1.5-72B-Chat & 8.62 & 8.66 & 8.58 & 1 \\
                                           & Mixtral-8x22B-Instruct-v0.1 & 8.49 & 8.89 & 8.09 & 1 \\
                                           & dbrx-instruct & 7.82 & 8.21 & 7.43 & 1 \\
        \midrule
        \multirow{4}{*}{Mixed-MoA}  & 2-Layer MoA & 9.06 & 9.23 & 8.89 & 7 \\
        & 2-Layer MoA w/ GPT-4o & 9.39 & 9.40 & 9.37 & 7 \\
        & 3-Layer MoA & 9.25 & 9.44 & 9.07 & 13 \\
        & 3-Layer MoA w/ GPT-4o  & 9.40 & 9.49 & 9.31 & 13 \\
        \midrule
        \multirow{2}{*}{\shortstack{Self-MoA + \quad\quad\quad\quad \\ WizardLM-2-8x22B}} & 2-Layer Self-MoA & 9.13 & 9.36 & 8.89 & 7 \\
        & 2-Layer Self-MoA w/ GPT-4o & \textbf{9.52} & 9.56 & 9.47 & 7 \\
        \bottomrule
    \end{tabular}
\end{table}

\subsection{Comparison to Universal Self-Consistency}
We conduct further experiments to compare Self-Consistency~\citep{wang2022self} with MoA and Self-MoA on the AlpacaEval 2.0 benchmark. As this benchmark is an instruction-following task without exact answers, we evaluate on Universal Self-Consistency (USC)~\citep{chen2023universal} which prompts LLMs to generate the most consistent response. We report the result in Table~\ref{tab:usc}, which shows that USC performs worse than its MoA counterpart when proposers and aggregators are controlled. This further suggests that rather than finding the most consistent response, MoA and Self-MoA can encourage LLM to synthesize the references and produce a better response.

\subsection{Normalizing Sub-tasks in Table~\ref{tab:mixture}}
\label{sect:normalize_mix_data}

The results in Table~\ref{tab:mix_data} indicate that the variance of models on CRUX is generally higher than that of the other two tasks, which could bias the average performance towards CRUX. To ensure that each task contributes equally to the overall performance metric, we assign weights to the three tasks based on the inverse of their variance.

For example, considering MMLU, we report 19 performance metrics (including individual models, Mixed-MoA, and Self-MoA) in Table~\ref{tab:mix_data}. The standard deviation of performance for MMLU across these 19 settings is calculated to be 3.50. In comparison, the standard deviation for CRUX and MATH are 5.70 and 4.27, respectively. Consequently, the weight assigned to MMLU when calculating the “WeightedAvg” is given by:

\[
\text{Weight}_{\text{MMLU}} = \frac{1/3.50}{(1/3.50) + (1/5.70) + (1/4.27)}.
\]

The normalized results are shown in Table~\ref{tab:weighted_avg}. 

\begin{table}[t]
    \centering
\caption{This table compares Self-MoA and Mixed-MoA using a weighted composition of three sub-tasks. The weights are assigned to each sub-task to prevent a high-variance task, such as CRUX, from disproportionately influencing the overall performance metrics. This approach ensures a more balanced evaluation, allowing for a fairer comparison between the two models.}
\label{tab:weighted_avg}
\vskip 0.1in
\begin{tabular}{lccccccc}
\toprule
&Aggregator & Proposer & MMLU & CRUX & MATH & Average & WeightedAvg \\
\midrule
Individual & - & \texttt{i} & 66.16 & 36.25 & 53.81 & 52.07 & 54.46 \\
Individual & - &  \texttt{d} & 60.91 & 49.51 & 53.82 & 54.74 & 55.65 \\
Individual & - &  \texttt{m} & 54.36 & 27.88 & 69.57 & 50.60 & 52.80 \\
Mixed-MoA &\texttt{i} & \texttt{iimmdd} & 67.89 & 42.88 & 64.38 & 58.38 & 60.40 \\
Mixed-MoA &\texttt{i} & \texttt{imdddd} & 67.42 & 44.50 & 63.90 & 58.61 & 60.46 \\
Mixed-MoA &\texttt{i} & \texttt{iiiimd} & 68.90 & 41.25 & 63.00 & 57.72 & 59.94 \\
Mixed-MoA &\texttt{i} & \texttt{immmmd} & 66.63 & 42.75 & 66.02 & 58.47 & 60.40 \\
Mixed-MoA &\texttt{i} & \texttt{iimmmm} & 66.23 & 39.25 & 66.10 & 57.19 & 59.38 \\
Mixed-MoA &\texttt{i} & \texttt{iiimmm} & 67.49 & 38.25 & 64.16 & 56.63 & 59.00 \\
Mixed-MoA &\texttt{i} & \texttt{iiiimm} & 68.00 & 37.00 & 62.92 & 55.97 & 58.47 \\
Mixed-MoA &\texttt{i} & \texttt{iidddd} & 68.21 & 45.50 & 62.56 & 58.76 & 60.58 \\
Mixed-MoA &\texttt{i} & \texttt{iiiddd} & 68.21 & 42.88 & 62.38 & 57.82 & 59.86 \\
Mixed-MoA &\texttt{i} & \texttt{iiiidd} & 68.47 & 40.75 & 61.24 & 56.82 & 59.05 \\
Mixed-MoA &\texttt{i} & \texttt{mmdddd} & 66.34 & 46.75 & 66.48 & 59.86 & 61.45 \\
Mixed-MoA &\texttt{i} & \texttt{mmmddd} & 65.80 & 47.00 & 67.32 & 60.04 & 61.57 \\
Mixed-MoA &\texttt{i} & \texttt{mmmmdd} & 65.44 & 42.50 & 67.62 & 58.52 & 60.39 \\
\midrule
Self-MoA &\texttt{i} & \texttt{dddddd} & 65.23 & 50.75 & 63.08 & 59.69 & 60.86 \\
Self-MoA &\texttt{i} & 6×TaskBest & 69.01 & 50.75 & 68.42 & 62.73 & 64.21 \\
Self-MoA &TaskBest & TaskBest & 69.01 & 52.62 & 69.80 & 63.81 & 65.14 \\
\bottomrule
\end{tabular}

\end{table}

\begin{table}[t!]
\vspace{-0.2in}
    \centering
    \caption{Comparison of Self-MoA, Mixed-MoA, and Universal Self-Consistency (USC) on AlpacaEval 2.0 leaderboard. We use Qwen1.5-110B-Chat as the aggregator.}
    \label{tab:usc}
\vskip 0.1in
    \begin{tabular}{l|l|c|c}  % Specifies three columns: left, left, center
        \toprule
         & \textbf{Model Configuration} & \textbf{LC Win Rate} & \textbf{\# Forward Passes} \\
        \midrule

        \multirow{1}{*}{Mixed-MoA}  & MoA & 59.1 & 7 \\

        \midrule
        Self-MoA & Self-MoA + WizardLM-2-8x22B & \textbf{65.7} & 7 \\
        \midrule
        \multirow{2}{*}{Universal Self-Consistency} & Mixed-USC & 53.8 & 7 \\
         & Self-USC + WizardLM-2-8x22B & 60.2 & 7 \\
        \bottomrule
    \end{tabular}
\end{table}

\end{document}

