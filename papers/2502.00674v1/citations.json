[
  {
    "index": 0,
    "papers": [
      {
        "key": "wang2024mixture",
        "author": "Wang, Junlin and Wang, Jue and Athiwaratkun, Ben and Zhang, Ce and Zou, James",
        "title": "Mixture-of-Agents Enhances Large Language Model Capabilities"
      }
    ]
  },
  {
    "index": 1,
    "papers": [
      {
        "key": "lin2024mitigatingalignmenttaxrlhf",
        "author": "Yong Lin and Hangyu Lin and Wei Xiong and Shizhe Diao and Jianmeng Liu and Jipeng Zhang and Rui Pan and Haoxiang Wang and Wenbin Hu and Hanning Zhang and Hanze Dong and Renjie Pi and Han Zhao and Nan Jiang and Heng Ji and Yuan Yao and Tong Zhang",
        "title": "Mitigating the Alignment Tax of RLHF"
      },
      {
        "key": "ram\u00e92024warpbenefitsweightaveraged",
        "author": "Alexandre Ram\u00e9 and Johan Ferret and Nino Vieillard and Robert Dadashi and L\u00e9onard Hussenot and Pierre-Louis Cedoz and Pier Giuseppe Sessa and Sertan Girgin and Arthur Douillard and Olivier Bachem",
        "title": "WARP: On the Benefits of Weight Averaged Rewarded Policies"
      }
    ]
  },
  {
    "index": 2,
    "papers": [
      {
        "key": "jiang2024mixtralexperts",
        "author": "Albert Q. Jiang and Alexandre Sablayrolles and Antoine Roux and Arthur Mensch and Blanche Savary and Chris Bamford and Devendra Singh Chaplot and Diego de las Casas and Emma Bou Hanna and Florian Bressand and Gianna Lengyel and Guillaume Bour and Guillaume Lample and L\u00e9lio Renard Lavaud and Lucile Saulnier and Marie-Anne Lachaux and Pierre Stock and Sandeep Subramanian and Sophia Yang and Szymon Antoniak and Teven Le Scao and Th\u00e9ophile Gervet and Thibaut Lavril and Thomas Wang and Timoth\u00e9e Lacroix and William El Sayed",
        "title": "Mixtral of Experts"
      },
      {
        "key": "lu2023routingexpertefficientrewardguided",
        "author": "Keming Lu and Hongyi Yuan and Runji Lin and Junyang Lin and Zheng Yuan and Chang Zhou and Jingren Zhou",
        "title": "Routing to the Expert: Efficient Reward-guided Ensemble of Large Language Models"
      }
    ]
  },
  {
    "index": 3,
    "papers": [
      {
        "key": "jiang2023llmblenderensemblinglargelanguage",
        "author": "Dongfu Jiang and Xiang Ren and Bill Yuchen Lin",
        "title": "LLM-Blender: Ensembling Large Language Models with Pairwise Ranking and Generative Fusion"
      }
    ]
  },
  {
    "index": 4,
    "papers": [
      {
        "key": "zhang2024towards",
        "author": "Zhang, Kaiyan and Qi, Biqing and Zhou, Bowen",
        "title": "Towards Building Specialized Generalist AI with System 1 and System 2 Fusion"
      }
    ]
  },
  {
    "index": 5,
    "papers": [
      {
        "key": "wang2024mixture",
        "author": "Wang, Junlin and Wang, Jue and Athiwaratkun, Ben and Zhang, Ce and Zou, James",
        "title": "Mixture-of-Agents Enhances Large Language Model Capabilities"
      }
    ]
  },
  {
    "index": 6,
    "papers": [
      {
        "key": "wang2024mixture",
        "author": "Wang, Junlin and Wang, Jue and Athiwaratkun, Ben and Zhang, Ce and Zou, James",
        "title": "Mixture-of-Agents Enhances Large Language Model Capabilities"
      },
      {
        "key": "jiang2023llmblenderensemblinglargelanguage",
        "author": "Dongfu Jiang and Xiang Ren and Bill Yuchen Lin",
        "title": "LLM-Blender: Ensembling Large Language Models with Pairwise Ranking and Generative Fusion"
      },
      {
        "key": "zhang2024diversity",
        "author": "Zhang, Kexun and Yao, Weiran and Liu, Zuxin and Feng, Yihao and Liu, Zhiwei and Murthy, Rithesh and Lan, Tian and Li, Lei and Lou, Renze and Xu, Jiacheng and others",
        "title": "Diversity empowers intelligence: Integrating expertise of software engineering agents"
      }
    ]
  },
  {
    "index": 7,
    "papers": [
      {
        "key": "hendrycks2021measuring",
        "author": "Hendrycks, Dan and Burns, Collin and Kadavath, Saurav and Arora, Akul and Basart, Steven and Tang, Eric and Song, Dawn and Steinhardt, Jacob",
        "title": "Measuring mathematical problem solving with the math dataset"
      }
    ]
  },
  {
    "index": 8,
    "papers": [
      {
        "key": "chen2021evaluating",
        "author": "Chen, Mark and Tworek, Jerry and Jun, Heewoo and Yuan, Qiming and Pinto, Henrique Ponde De Oliveira and Kaplan, Jared and Edwards, Harri and Burda, Yuri and Joseph, Nicholas and Brockman, Greg and others",
        "title": "Evaluating large language models trained on code"
      }
    ]
  },
  {
    "index": 9,
    "papers": [
      {
        "key": "roziere2023code",
        "author": "Roziere, Baptiste and Gehring, Jonas and Gloeckle, Fabian and Sootla, Sten and Gat, Itai and Tan, Xiaoqing Ellen and Adi, Yossi and Liu, Jingyu and Sauvestre, Romain and Remez, Tal and others",
        "title": "Code llama: Open foundation models for code"
      },
      {
        "key": "li2022competition",
        "author": "Li, Yujia and Choi, David and Chung, Junyoung and Kushman, Nate and Schrittwieser, Julian and Leblond, R{\\'e}mi and Eccles, Tom and Keeling, James and Gimeno, Felix and Dal Lago, Agustin and others",
        "title": "Competition-level code generation with alphacode"
      },
      {
        "key": "brown2024large",
        "author": "Brown, Bradley and Juravsky, Jordan and Ehrlich, Ryan and Clark, Ronald and Le, Quoc V and R{\\'e}, Christopher and Mirhoseini, Azalia",
        "title": "Large language monkeys: Scaling inference compute with repeated sampling"
      }
    ]
  },
  {
    "index": 10,
    "papers": [
      {
        "key": "wang2022self",
        "author": "Wang, Xuezhi and Wei, Jason and Schuurmans, Dale and Le, Quoc and Chi, Ed and Narang, Sharan and Chowdhery, Aakanksha and Zhou, Denny",
        "title": "Self-consistency improves chain of thought reasoning in language models"
      },
      {
        "key": "chen2023universal",
        "author": "Chen, Xinyun and Aksitov, Renat and Alon, Uri and Ren, Jie and Xiao, Kefan and Yin, Pengcheng and Prakash, Sushant and Sutton, Charles and Wang, Xuezhi and Zhou, Denny",
        "title": "Universal self-consistency for large language model generation"
      },
      {
        "key": "gui2024bonbon",
        "author": "Gui, Lin and G{\\^a}rbacea, Cristina and Veitch, Victor",
        "title": "BoNBoN Alignment for Large Language Models and the Sweetness of Best-of-n Sampling"
      },
      {
        "key": "li2024agentsneed",
        "author": "Junyou Li and Qin Zhang and Yangbin Yu and Qiang Fu and Deheng Ye",
        "title": "More Agents Is All You Need"
      }
    ]
  },
  {
    "index": 11,
    "papers": [
      {
        "key": "brown2024large",
        "author": "Brown, Bradley and Juravsky, Jordan and Ehrlich, Ryan and Clark, Ronald and Le, Quoc V and R{\\'e}, Christopher and Mirhoseini, Azalia",
        "title": "Large language monkeys: Scaling inference compute with repeated sampling"
      }
    ]
  },
  {
    "index": 12,
    "papers": [
      {
        "key": "stechly2023gpt",
        "author": "Stechly, Kaya and Marquez, Matthew and Kambhampati, Subbarao",
        "title": "Gpt-4 doesn't know it's wrong: An analysis of iterative prompting for reasoning problems"
      }
    ]
  },
  {
    "index": 13,
    "papers": [
      {
        "key": "valmeekam2023can",
        "author": "Valmeekam, Karthik and Marquez, Matthew and Kambhampati, Subbarao",
        "title": "Can large language models really improve by self-critiquing their own plans?"
      }
    ]
  },
  {
    "index": 14,
    "papers": [
      {
        "key": "madaan2024self",
        "author": "Madaan, Aman and Tandon, Niket and Gupta, Prakhar and Hallinan, Skyler and Gao, Luyu and Wiegreffe, Sarah and Alon, Uri and Dziri, Nouha and Prabhumoye, Shrimai and Yang, Yiming and others",
        "title": "Self-refine: Iterative refinement with self-feedback"
      }
    ]
  },
  {
    "index": 15,
    "papers": [
      {
        "key": "madaan2024self",
        "author": "Madaan, Aman and Tandon, Niket and Gupta, Prakhar and Hallinan, Skyler and Gao, Luyu and Wiegreffe, Sarah and Alon, Uri and Dziri, Nouha and Prabhumoye, Shrimai and Yang, Yiming and others",
        "title": "Self-refine: Iterative refinement with self-feedback"
      }
    ]
  },
  {
    "index": 16,
    "papers": [
      {
        "key": "chen2024moa",
        "author": "Chen, Sandy and Zeng, Leqi and Raghunathan, Abhinav and Huang, Flora and Kim, Terrence C",
        "title": "MoA is All You Need: Building LLM Research Team using Mixture of Agents"
      }
    ]
  },
  {
    "index": 17,
    "papers": [
      {
        "key": "wang2024mixture",
        "author": "Wang, Junlin and Wang, Jue and Athiwaratkun, Ben and Zhang, Ce and Zou, James",
        "title": "Mixture-of-Agents Enhances Large Language Model Capabilities"
      }
    ]
  },
  {
    "index": 18,
    "papers": [
      {
        "key": "du2023improving",
        "author": "Du, Yilun and Li, Shuang and Torralba, Antonio and Tenenbaum, Joshua B and Mordatch, Igor",
        "title": "Improving factuality and reasoning in language models through multiagent debate"
      }
    ]
  },
  {
    "index": 19,
    "papers": [
      {
        "key": "liang2023encouraging",
        "author": "Liang, Tian and He, Zhiwei and Jiao, Wenxiang and Wang, Xing and Wang, Yan and Wang, Rui and Yang, Yujiu and Tu, Zhaopeng and Shi, Shuming",
        "title": "Encouraging divergent thinking in large language models through multi-agent debate"
      }
    ]
  },
  {
    "index": 20,
    "papers": [
      {
        "key": "chen2023reconcile",
        "author": "Chen, Justin Chih-Yao and Saha, Swarnadeep and Bansal, Mohit",
        "title": "Reconcile: Round-table conference improves reasoning via consensus among diverse llms"
      }
    ]
  },
  {
    "index": 21,
    "papers": [
      {
        "key": "wang2024rethinking",
        "author": "Wang, Qineng and Wang, Zihao and Su, Ying and Tong, Hanghang and Song, Yangqiu",
        "title": "Rethinking the Bounds of LLM Reasoning: Are Multi-Agent Discussions the Key?"
      }
    ]
  }
]