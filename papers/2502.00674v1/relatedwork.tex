\section{Related Work}
\label{sect:related_work}

\paragraph{Ensembles of LLMs.} 
Model ensembling aims to combine strengths from multiple models. Previous studies have explored various methods to leverage a diverse set of models, including but not limited to prompting~\citep{wang2024mixture}, 
weight averaging~\citep{lin2024mitigatingalignmenttaxrlhf,ram√©2024warpbenefitsweightaveraged}, routing~\citep{jiang2024mixtralexperts,lu2023routingexpertefficientrewardguided}, training a generative fusion model~\citep{jiang2023llmblenderensemblinglargelanguage}, and so on. \citet{zhang2024towards} argues that the fusion of specialized models with certain general abilities could be a promising direction toward Artificial General Intelligence. 
Mixture-of-Agents (MoA, \citet{wang2024mixture}) first queries multiple LLMs to generate responses, then iteratively aggregates these samples through several rounds of synthesis. MoA shows promising results on several benchmarks, and its variants achieve superior performance on the AlpacaEval 2.0 leaderboard. 
Our method is inspired by the prompt pipeline proposed in MoA. However, while existing MoA focuses on unleashing the strength from multiple different models~\citep{wang2024mixture,jiang2023llmblenderensemblinglargelanguage, zhang2024diversity}, we demonstrate the trade-off between diversity and quality within the proposers, highlighting that focusing solely on diversity may compromise overall quality and final performance.

\paragraph{LLM Inference with Repeated Sampling.}
Previous studies have shown that combining model outputs from repeated sampling can yield a better response in various domains. In tasks with automatic verifiers available, such as math~\citep{hendrycks2021measuring} and code~\citep{chen2021evaluating}, simply sampling LLMs multiple times can significantly improve the pass@k metric and hence boost the success rate of solving the tasks~\citep{roziere2023code,li2022competition,brown2024large}. In more general tasks without verification tools, we can conduct techniques like majority vote, self-consistency, and best-of-n to choose the most promising one from candidate responses~\citep{wang2022self,chen2023universal,gui2024bonbon,li2024agentsneed}.
Therefore, repeated sampling is recently regarded as one approach of scaling compute during inference time~\citep{brown2024large}.
In this work, we identify the surprising effectiveness of repeated sampling in the context of MoA. Unlike majority vote or best-of-N, Self-MoA asks LLMs to synthesize outputs generated from repeated sampling, hence can further improve over each individual output.

\paragraph{Collaborative Agents} There is a surge of interest in building agent systems based on verification, critique, discussion, and refinement. For example, \citet{stechly2023gpt}, \citet{valmeekam2023can}, and \citet{madaan2024self} use self-critique to iteratively refine outputs through a chain structure. \citet{madaan2024self}, \citet{chen2024moa}, and \citet{wang2024mixture} explore the incorporation of multiple models to create a stronger agent that outperforms each individual model. \citet{du2023improving} incorporates multiple LLMs that propose and debate their individual responses over several rounds to reach a common final answer. \citet{liang2023encouraging} proposes Multi-Agent Debate, which encourages divergent thinking during LLM debates to arrive at more informative conclusions and avoid rushing to incorrect answers. \citet{chen2023reconcile} introduces RECONCILE, which adopts a confidence-weighted voting mechanism for better consensus among LLM discussions. Interestingly, \citet{wang2024rethinking} shows that a single model with carefully designed prompts can sometimes match the performance of agent discussions. Moreover, agent discussions mainly outperform a single LLM when the prompts are insufficient.