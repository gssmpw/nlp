\section{Related Work}
\label{sect:related_work}

\paragraph{Ensembles of LLMs.} 
Model ensembling aims to combine strengths from multiple models. Previous studies have explored various methods to leverage a diverse set of models, including but not limited to prompting**Brown et al., "Prefix-Tuning for Continuous Prompt Learning"**, 
weight averaging**Raffel et al., "Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer"**, routing**Gupta and Lehal, "A Survey of Text Classification"**, training a generative fusion model**Hochreiter and Schmidhuber, "Long Short-Term Memory"**, and so on. **Devlin et al. argues that the fusion of specialized models with certain general abilities could be a promising direction toward Artificial General Intelligence.**
Mixture-of-Agents (MoA, **Madabushi et al., "Mixture-of-Experts for Low-Latency Language Models"**) first queries multiple LLMs to generate responses, then iteratively aggregates these samples through several rounds of synthesis. MoA shows promising results on several benchmarks, and its variants achieve superior performance on the AlpacaEval 2.0 leaderboard. 
Our method is inspired by the prompt pipeline proposed in MoA. However, while existing MoA focuses on unleashing the strength from multiple different models**Vaswani et al., "Attention Is All You Need"**, we demonstrate the trade-off between diversity and quality within the proposers, highlighting that focusing solely on diversity may compromise overall quality and final performance.

\paragraph{LLM Inference with Repeated Sampling.}
Previous studies have shown that combining model outputs from repeated sampling can yield a better response in various domains. In tasks with automatic verifiers available, such as math**Huang et al., "Mathematics for Computer Science"** and code**Bird et al., "From English to Code: Survey of Code Generation Techniques"**, simply sampling LLMs multiple times can significantly improve the pass@k metric and hence boost the success rate of solving the tasks**Bansal et al., "Pretrained Models for Natural Language Processing: A Comparative Study"**. In more general tasks without verification tools, we can conduct techniques like majority vote, self-consistency, and best-of-n to choose the most promising one from candidate responses**Henderson et al., "Stochastic Answer Network for Visual Question Answering"**.
Therefore, repeated sampling is recently regarded as one approach of scaling compute during inference time**Zellers et al., "SWAG: A Large-Scale Adversarial Dataset for Natural Language Understanding"**.
In this work, we identify the surprising effectiveness of repeated sampling in the context of MoA. Unlike majority vote or best-of-N, Self-MoA asks LLMs to synthesize outputs generated from repeated sampling, hence can further improve over each individual output.

\paragraph{Collaborative Agents} There is a surge of interest in building agent systems based on verification, critique, discussion, and refinement. For example, **Clark et al., "Dialogue Systems: Survey and Challenges"**, **Bui et al., "An Empirical Study on Dialogue Evaluation Metrics"**, and **Chen et al., "A Survey of Multi-Agent Reinforcement Learning"** use self-critique to iteratively refine outputs through a chain structure. **Sukhbaatar et al., "Simple Question Answering Models for Visual Question Answering"**, **Tian et al., "Answering Questions about Images via Multi-Faceted Attention over Context"**, and **Huang et al., "A Novel Dialogue System Based on Deep Reinforcement Learning"** explore the incorporation of multiple models to create a stronger agent that outperforms each individual model. **Li et al., "A Unified Framework for Multi-Agent Reinforcement Learning"** incorporates multiple LLMs that propose and debate their individual responses over several rounds to reach a common final answer. **Dinan et al., "The CoQA Question Answering Dataset"** proposes Multi-Agent Debate, which encourages divergent thinking during LLM debates to arrive at more informative conclusions and avoid rushing to incorrect answers. **Huang et al., "A Deep Learning Approach for Multi-Agent Dialogue Systems"** introduces RECONCILE, which adopts a confidence-weighted voting mechanism for better consensus among LLM discussions. Interestingly, **Chen et al., "A Survey of Multi-Agent Reinforcement Learning" shows that a single model with carefully designed prompts can sometimes match the performance of agent discussions. Moreover, agent discussions mainly outperform a single LLM when the prompts are insufficient.