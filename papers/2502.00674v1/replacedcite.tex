\section{Related Work}
\label{sect:related_work}

\paragraph{Ensembles of LLMs.} 
Model ensembling aims to combine strengths from multiple models. Previous studies have explored various methods to leverage a diverse set of models, including but not limited to prompting____, 
weight averaging____, routing____, training a generative fusion model____, and so on. ____ argues that the fusion of specialized models with certain general abilities could be a promising direction toward Artificial General Intelligence. 
Mixture-of-Agents (MoA, ____) first queries multiple LLMs to generate responses, then iteratively aggregates these samples through several rounds of synthesis. MoA shows promising results on several benchmarks, and its variants achieve superior performance on the AlpacaEval 2.0 leaderboard. 
Our method is inspired by the prompt pipeline proposed in MoA. However, while existing MoA focuses on unleashing the strength from multiple different models____, we demonstrate the trade-off between diversity and quality within the proposers, highlighting that focusing solely on diversity may compromise overall quality and final performance.

\paragraph{LLM Inference with Repeated Sampling.}
Previous studies have shown that combining model outputs from repeated sampling can yield a better response in various domains. In tasks with automatic verifiers available, such as math____ and code____, simply sampling LLMs multiple times can significantly improve the pass@k metric and hence boost the success rate of solving the tasks____. In more general tasks without verification tools, we can conduct techniques like majority vote, self-consistency, and best-of-n to choose the most promising one from candidate responses____.
Therefore, repeated sampling is recently regarded as one approach of scaling compute during inference time____.
In this work, we identify the surprising effectiveness of repeated sampling in the context of MoA. Unlike majority vote or best-of-N, Self-MoA asks LLMs to synthesize outputs generated from repeated sampling, hence can further improve over each individual output.

\paragraph{Collaborative Agents} There is a surge of interest in building agent systems based on verification, critique, discussion, and refinement. For example, ____, ____, and ____ use self-critique to iteratively refine outputs through a chain structure. ____, ____, and ____ explore the incorporation of multiple models to create a stronger agent that outperforms each individual model. ____ incorporates multiple LLMs that propose and debate their individual responses over several rounds to reach a common final answer. ____ proposes Multi-Agent Debate, which encourages divergent thinking during LLM debates to arrive at more informative conclusions and avoid rushing to incorrect answers. ____ introduces RECONCILE, which adopts a confidence-weighted voting mechanism for better consensus among LLM discussions. Interestingly, ____ shows that a single model with carefully designed prompts can sometimes match the performance of agent discussions. Moreover, agent discussions mainly outperform a single LLM when the prompts are insufficient.