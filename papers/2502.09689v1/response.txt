We briefly review existing literature tackling the relevance of visual media in misinformation. Due to the short format of this paper, this review is limited, and we refer the reader to **Zollo et al., "The Spread of Fake News on Twitter During the 2016 US General Election"**__**Allcott & Gentile, "Social Influence: Social Networks and Peer Groups"**__**Shao, "Understanding Deepfakes: Visual Computer Vision as a Tool for Social Media Forensics"** for a more extensive review.

It has been shown that the presence of visual media in misinformation affects the attention, comprehension, memory, and judgment of its audience **Johnson & Raye, "Cognitive and Brain Mechanisms Supporting Deception"**. In addition to the effects on readers, the presence of visual media also increases the chances of such narratives being amplified by the recommendation algorithms on social media and in search engines **Bakshy et al., "The Role of Social Networks in Information Diffusion"**. Put together, it comes as no surprise that the use of visual media in misinformation is rapidly increasing.

One of the first methods aiming to tackle this problem was a supervised model for the classification of image relevance **Rajpurkar et al., "A Simple Framework for Contrastive Learning of Visual Representations"**. However, this method only considers the image and caption pair, leaving out the rest of the article.

Later work proposed to use reverse image search to address this problem **Bakhshi et al., "Visualizing and Understanding Convolutional Neural Networks"**, seeking to empower users to proactively verify the origin of images in articles. When tested in a participant study, however, **Castillo & Brouwer, "Media Effects on Public Opinion: A Social Media Experiment"** found mixed results of efficacy.

Recent work focused on employing automated methods analyzing the semantic coherence of various modalities in a single article **Cheng et al., "A Multimodal Fusion Framework for Visual and Textual Information Retrieval"**. While generally effective, this approach misses critical nuance in cases where the use of older images or images from different places than the main story may be an appropriate practice.

Finally, a recent approach introduced LLMs to the task by fine-tuning a multimodal LLM **Lewis et al., "BART: Denoising Sequence-to-Sequence Pre-training for Text-to-Text Tasks"**. While leading to effective explanations, this approach does not account for the possibility of media being visually relevant to the article but actually taken at a place or time that is not. Similarly, it does not account for tampering with the media.

\begin{figure*}[t] % 'p' for a full page
    \centering
    \begin{tabular}{cc}
        \includegraphics[width=0.48\textwidth]{fig/result-example-relevant.png} &
        \includegraphics[width=0.48\textwidth]{fig/result-example-not-relevant.png} \\
        (a) & (b)
    \end{tabular}
    \caption{
    Screenshots of the prototype web interface displaying a result in which the media were found (a) relevant and (b) not relevant to the news story. The chat interface allows for submitting follow-up questions to the LLM.
    }
    \label{fig:result}
\end{figure*}