@article{humeau2019next,
  title={The next generation of approaches to investigate the link between synaptic plasticity and learning},
  author={Humeau, Yann and Choquet, Daniel},
  journal={Nature neuroscience},
  volume={22},
  number={10},
  pages={1536--1543},
  year={2019},
  publisher={Nature Publishing Group US New York}
}

@article{qian2024partial,
  title={Partial observation can induce mechanistic mismatches in data-constrained models of neural dynamics},
  author={Qian, William and Zavatone-Veth, Jacob A and Ruben, Benjamin S and Pehlevan, Cengiz},
  journal={bioRxiv},
  pages={2024--05},
  year={2024},
  publisher={Cold Spring Harbor Laboratory}
}

@article{chen2017map,
  title={A map of anticipatory activity in mouse motor cortex},
  author={Chen, Tsai-Wen and Li, Nuo and Daie, Kayvon and Svoboda, Karel},
  journal={Neuron},
  volume={94},
  number={4},
  pages={866--879},
  year={2017},
  publisher={Elsevier}
}

@article{tsutsumi2021optical,
  title={Optical interrogation of multi-scale neuronal plasticity underlying behavioral learning},
  author={Tsutsumi, Shinichiro and Hayashi-Takagi, Akiko},
  journal={Current Opinion in Neurobiology},
  volume={67},
  pages={8--15},
  year={2021},
  publisher={Elsevier}
}

@article{allen2019thirst,
  title={Thirst regulates motivated behavior through modulation of brainwide neural population dynamics},
  author={Allen, William E and Chen, Michael Z and Pichamoorthy, Nandini and Tien, Rebecca H and Pachitariu, Marius and Luo, Liqun and Deisseroth, Karl},
  journal={Science},
  volume={364},
  number={6437},
  pages={eaav3932},
  year={2019},
  publisher={American Association for the Advancement of Science}
}
@article{steinmetz2019distributed,
  title={Distributed coding of choice, action and engagement across the mouse brain},
  author={Steinmetz, Nicholas A and Zatka-Haas, Peter and Carandini, Matteo and Harris, Kenneth D},
  journal={Nature},
  volume={576},
  number={7786},
  pages={266--273},
  year={2019},
  publisher={Nature Publishing Group UK London}
}

@article{saglietti2022analytical,
  title={An analytical theory of curriculum learning in teacher-student networks},
  author={Saglietti, Luca and Mannelli, Stefano and Saxe, Andrew},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={21113--21127},
  year={2022}
}
@article{hopfield1982neural,
  title={Neural networks and physical systems with emergent collective computational abilities.},
  author={Hopfield, John J},
  journal={Proceedings of the national academy of sciences},
  volume={79},
  number={8},
  pages={2554--2558},
  year={1982},
  publisher={National Acad Sciences}
}

@article{humphreys2022bci,
  title={BCI learning phenomena can be explained by gradient-based optimization},
  author={Humphreys, Peter C and Daie, Kayvon and Svoboda, Karel and Botvinick, Matthew and Lillicrap, Timothy P},
  journal={bioRxiv},
  pages={2022--12},
  year={2022},
  publisher={Cold Spring Harbor Laboratory}
}


@article{finkelstein2023connectivity,
  title={Connectivity underlying motor cortex activity during naturalistic goal-directed behavior},
  author={Finkelstein, Arseny and Daie, Kayvon and Rozsa, Marton and Darshan, Ran and Svoboda, Karel},
  journal={bioRxiv},
  pages={2023--11},
  year={2023},
  publisher={Cold Spring Harbor Laboratory}
}


@article{barak2017recurrent,
  title={Recurrent neural networks as versatile tools of neuroscience research},
  author={Barak, Omri},
  journal={Current opinion in neurobiology},
  volume={46},
  pages={1--6},
  year={2017},
  publisher={Elsevier}
}
@article{sompolinsky1988chaos,
  title={Chaos in random neural networks},
  author={Sompolinsky, Haim and Crisanti, Andrea and Sommers, Hans-Jurgen},
  journal={Physical review letters},
  volume={61},
  number={3},
  pages={259},
  year={1988},
  publisher={APS}
}

@article{daie2021targeted,
  title={Targeted photostimulation uncovers circuit motifs supporting short-term memory},
  author={Daie, Kayvon and Svoboda, Karel and Druckmann, Shaul},
  journal={Nature neuroscience},
  volume={24},
  number={2},
  pages={259--265},
  year={2021},
  publisher={Nature Publishing Group US New York}
}

@article{hayashi2015labelling,
  title={Labelling and optical erasure of synaptic memory traces in the motor cortex},
  author={Hayashi-Takagi, Akiko and Yagishita, Sho and Nakamura, Mayumi and Shirai, Fukutoshi and Wu, Yi I and Loshbaugh, Amanda L and Kuhlman, Brian and Hahn, Klaus M and Kasai, Haruo},
  journal={Nature},
  volume={525},
  number={7569},
  pages={333--338},
  year={2015},
  publisher={Nature Publishing Group UK London}
}

@article{marshel2019cortical,
  title={Cortical layer--specific critical dynamics triggering perception},
  author={Marshel, James H and Kim, Yoon Seok and Machado, Timothy A and Quirin, Sean and Benson, Brandon and Kadmon, Jonathan and Raja, Cephra and Chibukhchyan, Adelaida and Ramakrishnan, Charu and Inoue, Masatoshi and others},
  journal={Science},
  volume={365},
  number={6453},
  pages={eaaw5202},
  year={2019},
  publisher={American Association for the Advancement of Science}
}

@article{pakkenberg2003aging,
  title={Aging and the human neocortex},
  author={Pakkenberg, Bente and Pelvig, Dorte and Marner, Lisbeth and Bundgaard, Mads J and Gundersen, Hans J{\o}rgen G and Nyengaard, Jens R and Regeur, Lisbeth},
  journal={Experimental gerontology},
  volume={38},
  number={1-2},
  pages={95--99},
  year={2003},
  publisher={Elsevier}
}

@article{martin2000synaptic,
  title={Synaptic plasticity and memory: an evaluation of the hypothesis},
  author={Martin, Stephen J and Grimwood, Paul D and Morris, Richard GM},
  journal={Annual review of neuroscience},
  volume={23},
  number={1},
  pages={649--711},
  year={2000},
  publisher={Annual Reviews 4139 El Camino Way, PO Box 10139, Palo Alto, CA 94303-0139, USA}
}
@article{robinson2020targeted,
  title={Targeted activation of hippocampal place cells drives memory-guided spatial behavior},
  author={Robinson, Nick TM and Descamps, Lucie AL and Russell, Lloyd E and Buchholz, Moritz O and Bicknell, Brendan A and Antonov, Georgy K and Lau, Joanna YN and Nutbrown, Rebecca and Schmidt-Hieber, Christoph and H{\"a}usser, Michael},
  journal={Cell},
  volume={183},
  number={6},
  pages={1586--1599},
  year={2020},
  publisher={Elsevier}
}

@article{pakkenberg2003aging,
  title={Aging and the human neocortex},
  author={Pakkenberg, Bente and Pelvig, Dorte and Marner, Lisbeth and Bundgaard, Mads J and Gundersen, Hans J{\o}rgen G and Nyengaard, Jens R and Regeur, Lisbeth},
  journal={Experimental gerontology},
  volume={38},
  number={1-2},
  pages={95--99},
  year={2003},
  publisher={Elsevier}
}

@book{Horn_Johnson_1991, place={Cambridge}, title={Topics in Matrix Analysis}, publisher={Cambridge University Press}, author={Horn, Roger A. and Johnson, Charles R.}, year={1991}} 
@article{cover1965geometrical,
  title={Geometrical and statistical properties of systems of linear inequalities with applications in pattern recognition},
  author={Cover, Thomas M},
  journal={IEEE transactions on electronic computers},
  number={3},
  pages={326--334},
  year={1965},
  publisher={IEEE}
}
@misc{zaken2022bitfitsimpleparameterefficientfinetuning,
      title={BitFit: Simple Parameter-efficient Fine-tuning for Transformer-based Masked Language-models}, 
      author={Elad Ben Zaken and Shauli Ravfogel and Yoav Goldberg},
      year={2022},
      eprint={2106.10199},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2106.10199}, 
}
@misc{guo2021parameterefficienttransferlearningdiff,
      title={Parameter-Efficient Transfer Learning with Diff Pruning}, 
      author={Demi Guo and Alexander M. Rush and Yoon Kim},
      year={2021},
      eprint={2012.07463},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2012.07463}, 
}
@misc{sung2021trainingneuralnetworksfixed,
      title={Training Neural Networks with Fixed Sparse Masks}, 
      author={Yi-Lin Sung and Varun Nair and Colin Raffel},
      year={2021},
      eprint={2111.09839},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2111.09839}, 
}
@misc{frankle2019lotterytickethypothesisfinding,
      title={The Lottery Ticket Hypothesis: Finding Sparse, Trainable Neural Networks}, 
      author={Jonathan Frankle and Michael Carbin},
      year={2019},
      eprint={1803.03635},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/1803.03635}, 
}
@misc{malach2020provinglotterytickethypothesis,
      title={Proving the Lottery Ticket Hypothesis: Pruning is All You Need}, 
      author={Eran Malach and Gilad Yehudai and Shai Shalev-Shwartz and Ohad Shamir},
      year={2020},
      eprint={2002.00585},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2002.00585}, 
}
@article{Cybenko1989ApproximationBS,
  title={Approximation by superpositions of a sigmoidal function},
  author={George V. Cybenko},
  journal={Mathematics of Control, Signals and Systems},
  year={1989},
  volume={2},
  pages={303-314},
  url={https://api.semanticscholar.org/CorpusID:3958369}
}
@article{HORNIK1989359,
title = {Multilayer feedforward networks are universal approximators},
journal = {Neural Networks},
volume = {2},
number = {5},
pages = {359-366},
year = {1989},
issn = {0893-6080},
doi = {https://doi.org/10.1016/0893-6080(89)90020-8},
url = {https://www.sciencedirect.com/science/article/pii/0893608089900208},
author = {Kurt Hornik and Maxwell Stinchcombe and Halbert White},
keywords = {Feedforward networks, Universal approximation, Mapping networks, Network representation capability, Stone-Weierstrass Theorem, Squashing functions, Sigma-Pi networks, Back-propagation networks},
abstract = {This paper rigorously establishes that standard multilayer feedforward networks with as few as one hidden layer using arbitrary squashing functions are capable of approximating any Borel measurable function from one finite dimensional space to another to any desired degree of accuracy, provided sufficiently many hidden units are available. In this sense, multilayer feedforward networks are a class of universal approximators.}
}
@misc{raghu2017expressivepowerdeepneural,
      title={On the Expressive Power of Deep Neural Networks}, 
      author={Maithra Raghu and Ben Poole and Jon Kleinberg and Surya Ganguli and Jascha Sohl-Dickstein},
      year={2017},
      eprint={1606.05336},
      archivePrefix={arXiv},
      primaryClass={stat.ML},
      url={https://arxiv.org/abs/1606.05336}, 
}
@misc{cohen2016expressivepowerdeeplearning,
      title={On the Expressive Power of Deep Learning: A Tensor Analysis}, 
      author={Nadav Cohen and Or Sharir and Amnon Shashua},
      year={2016},
      eprint={1509.05009},
      archivePrefix={arXiv},
      primaryClass={cs.NE},
      url={https://arxiv.org/abs/1509.05009}, 
}
@misc{montúfar2014numberlinearregionsdeep,
      title={On the Number of Linear Regions of Deep Neural Networks}, 
      author={Guido Montúfar and Razvan Pascanu and Kyunghyun Cho and Yoshua Bengio},
      year={2014},
      eprint={1402.1869},
      archivePrefix={arXiv},
      primaryClass={stat.ML},
      url={https://arxiv.org/abs/1402.1869}, 
}
@article{SIEGELMANN1995132,
title = {On the Computational Power of Neural Nets},
journal = {Journal of Computer and System Sciences},
volume = {50},
number = {1},
pages = {132-150},
year = {1995},
issn = {0022-0000},
doi = {https://doi.org/10.1006/jcss.1995.1013},
url = {https://www.sciencedirect.com/science/article/pii/S0022000085710136},
author = {H.T. Siegelmann and E.D. Sontag},
abstract = {This paper deals with finite size networks which consist of interconnections of synchronously evolving processors. Each processor updates its state by applying a "sigmoidal" function to a linear combination of the previous states of all units. We prove that one may simulate all Turing machines by such nets. In particular, one can simulate any multi-stack Turing machine in real time, and there is a net made up of 886 processors which computes a universal partial-recursive function. Products (high order nets) are not required, contrary to what had been stated in the literature. Non-deterministic Turing machines can be simulated by non-deterministic rational nets, also in real time. The simulation result has many consequences regarding the decidability, or more generally the complexity, of questions about recursive nets.}
}
@misc{joshi2024expressivepowergeometricgraph,
      title={On the Expressive Power of Geometric Graph Neural Networks}, 
      author={Chaitanya K. Joshi and Cristian Bodnar and Simon V. Mathis and Taco Cohen and Pietro Liò},
      year={2024},
      eprint={2301.09308},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2301.09308}, 
}
@misc{khrulkov2018expressivepowerrecurrentneural,
      title={Expressive power of recurrent neural networks}, 
      author={Valentin Khrulkov and Alexander Novikov and Ivan Oseledets},
      year={2018},
      eprint={1711.00811},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/1711.00811}, 
}
@misc{collins2017capacitytrainabilityrecurrentneural,
      title={Capacity and Trainability in Recurrent Neural Networks}, 
      author={Jasmine Collins and Jascha Sohl-Dickstein and David Sussillo},
      year={2017},
      eprint={1611.09913},
      archivePrefix={arXiv},
      primaryClass={stat.ML},
      url={https://arxiv.org/abs/1611.09913}, 
}
@Article{Smale1998,
author={Smale, Steve},
title={Mathematical problems for the next century},
journal={The Mathematical Intelligencer},
year={1998},
month={Mar},
day={01},
volume={20},
number={2a},
pages={7-15},
issn={0343-6993},
doi={10.1007/BF03025291},
url={https://doi.org/10.1007/BF03025291}
}

@misc{montanari2024smales17thproblemreals,
      title={On Smale's 17th problem over the reals}, 
      author={Andrea Montanari and Eliran Subag},
      year={2024},
      eprint={2405.01735},
      archivePrefix={arXiv},
      primaryClass={cs.DS},
      url={https://arxiv.org/abs/2405.01735}, 
}
@misc{subag2024concentrationzerosetlarge,
      title={Concentration for the zero set of large random polynomial systems}, 
      author={Eliran Subag},
      year={2024},
      eprint={2303.11924},
      archivePrefix={arXiv},
      primaryClass={math.PR},
      url={https://arxiv.org/abs/2303.11924}, 
}

﻿@Article{Kim2023,
author={Kim, Christopher M.
and Finkelstein, Arseny
and Chow, Carson C.
and Svoboda, Karel
and Darshan, Ran},
title={Distributing task-related neural activity across a cortical network through task-independent connections},
journal={Nature Communications},
year={2023},
month={May},
day={18},
volume={14},
number={1},
pages={2851},
abstract={Task-related neural activity is widespread across populations of neurons during goal-directed behaviors. However, little is known about the synaptic reorganization and circuit mechanisms that lead to broad activity changes. Here we trained a subset of neurons in a spiking network with strong synaptic interactions to reproduce the activity of neurons in the motor cortex during a decision-making task. Task-related activity, resembling the neural data, emerged across the network, even in the untrained neurons. Analysis of trained networks showed that strong untrained synapses, which were independent of the task and determined the dynamical state of the network, mediated the spread of task-related activity. Optogenetic perturbations suggest that the motor cortex is strongly-coupled, supporting the applicability of the mechanism to cortical networks. Our results reveal a cortical mechanism that facilitates distributed representations of task-variables by spreading the activity from a subset of plastic neurons to the entire network through task-independent strong synapses.},
issn={2041-1723},
doi={10.1038/s41467-023-38529-y},
url={https://doi.org/10.1038/s41467-023-38529-y}
}

@article{E_Gardner_1989,
doi = {10.1088/0305-4470/22/12/004},
url = {https://dx.doi.org/10.1088/0305-4470/22/12/004},
year = {1989},
month = {jun},
publisher = {},
volume = {22},
number = {12},
pages = {1983},
author = {E Gardner and  B Derrida},
title = {Three unfinished works on the optimal storage capacity of networks},
journal = {Journal of Physics A: Mathematical and General},
abstract = {The optimal storage properties of three different neural network models are studied. For two of these models the architecture of the network is a perceptron with +or-J interactions, whereas for the third model the output can be an arbitrary function of the inputs. Analytic bounds and numerical estimates of the optimal capacities and of the minimal fraction of errors are obtained for the first two models. The third model can be solved exactly and the exact solution is compared to the bounds and to the results of numerical simulations used for the two other models.}
}
@misc{wang2024comprehensivesurveycontinuallearning,
      title={A Comprehensive Survey of Continual Learning: Theory, Method and Application}, 
      author={Liyuan Wang and Xingxing Zhang and Hang Su and Jun Zhu},
      year={2024},
      eprint={2302.00487},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2302.00487}, 
}
@misc{mallya2018piggybackadaptingsinglenetwork,
      title={Piggyback: Adapting a Single Network to Multiple Tasks by Learning to Mask Weights}, 
      author={Arun Mallya and Dillon Davis and Svetlana Lazebnik},
      year={2018},
      eprint={1801.06519},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/1801.06519}, 
}
@misc{serrà2018overcomingcatastrophicforgettinghard,
      title={Overcoming catastrophic forgetting with hard attention to the task}, 
      author={Joan Serrà and Dídac Surís and Marius Miron and Alexandros Karatzoglou},
      year={2018},
      eprint={1801.01423},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/1801.01423}, 
}
@misc{mallya2018packnetaddingmultipletasks,
      title={PackNet: Adding Multiple Tasks to a Single Network by Iterative Pruning}, 
      author={Arun Mallya and Svetlana Lazebnik},
      year={2018},
      eprint={1711.05769},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/1711.05769}, 
}
@misc{wortsman2020supermaskssuperposition,
      title={Supermasks in Superposition}, 
      author={Mitchell Wortsman and Vivek Ramanujan and Rosanne Liu and Aniruddha Kembhavi and Mohammad Rastegari and Jason Yosinski and Ali Farhadi},
      year={2020},
      eprint={2006.14769},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2006.14769}, 
}
@misc{louizos2018learningsparseneuralnetworks,
      title={Learning Sparse Neural Networks through $L_0$ Regularization}, 
      author={Christos Louizos and Max Welling and Diederik P. Kingma},
      year={2018},
      eprint={1712.01312},
      archivePrefix={arXiv},
      primaryClass={stat.ML},
      url={https://arxiv.org/abs/1712.01312}, 
}
@misc{ahn2019uncertaintybasedcontinuallearningadaptive,
      title={Uncertainty-based Continual Learning with Adaptive Regularization}, 
      author={Hongjoon Ahn and Sungmin Cha and Donggyu Lee and Taesup Moon},
      year={2019},
      eprint={1905.11614},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/1905.11614}, 
}
@book{heij2006introduction,
  title={Introduction to Mathematical Systems Theory: Linear Systems, Identification and Control},
  author={Heij, C. and Ran, A.C.M. and van Schagen, F.},
  isbn={9783764375492},
  lccn={2006934217},
  url={https://books.google.co.il/books?id=NJ_5Enwpk58C},
  year={2006},
  publisher={Birkh{\"a}user Basel}
}
@article{girko1985circular,
  title={Circular law},
  author={Girko, Vyacheslav L},
  journal={Theory of Probability \& Its Applications},
  volume={29},
  number={4},
  pages={694--706},
  year={1985},
  publisher={SIAM}
}
@article{article,
author = {Azas, Jean-Marc and Wschebor, Mario},
year = {2009},
month = {01},
pages = {},
title = {Level Sets and Extrema of Random Processes and Fields},
isbn = {9780470409336},
journal = {Level Sets and Extrema of Random Processes and Fields},
doi = {10.1002/9780470434642}
}
@article{elman1990finding,
  title={Finding structure in time},
  author={Elman, Jeffrey L},
  journal={Cognitive science},
  volume={14},
  number={2},
  pages={179--211},
  year={1990},
  publisher={Wiley Online Library}
}
@misc{giannou2023expressivepowertuningnormalization,
      title={The Expressive Power of Tuning Only the Normalization Layers}, 
      author={Angeliki Giannou and Shashank Rajput and Dimitris Papailiopoulos},
      year={2023},
      eprint={2302.07937},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2302.07937}, 
}
@misc{zeng2024expressivepowerlowrankadaptation,
      title={The Expressive Power of Low-Rank Adaptation}, 
      author={Yuchen Zeng and Kangwook Lee},
      year={2024},
      eprint={2310.17513},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2310.17513}, 
}
@misc{englert2022adversarialreprogrammingrevisited,
      title={Adversarial Reprogramming Revisited}, 
      author={Matthias Englert and Ranko Lazic},
      year={2022},
      eprint={2206.03466},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2206.03466}, 
}
@misc{elsayed2018adversarialreprogrammingneuralnetworks,
      title={Adversarial Reprogramming of Neural Networks}, 
      author={Gamaleldin F. Elsayed and Ian Goodfellow and Jascha Sohl-Dickstein},
      year={2018},
      eprint={1806.11146},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/1806.11146}, 
}
@article{yao2020adahessian,
  title={ADAHESSIAN: An Adaptive Second Order Optimizer for Machine Learning},
  author={Yao, Zhewei and Gholami, Amir and Shen, Sheng and Keutzer, Kurt and Mahoney, Michael W},
  journal={AAAI (Accepted)},
  year={2021}
}
@misc{saxe2014exactsolutionsnonlineardynamics,
      title={Exact solutions to the nonlinear dynamics of learning in deep linear neural networks}, 
      author={Andrew M. Saxe and James L. McClelland and Surya Ganguli},
      year={2014},
      eprint={1312.6120},
      archivePrefix={arXiv},
      primaryClass={cs.NE},
      url={https://arxiv.org/abs/1312.6120}, 
}

@inproceedings{10.5555/3625834.3626034,
author = {Thangarasa, Vithursan and Gupta, Abhay and Marshall, William and Li, Tianda and Leong, Kevin and DeCoste, Dennis and Lie, Sean and Saxena, Shreyas},
title = {SPDF: sparse pre-training and dense fine-tuning for large language models},
year = {2023},
publisher = {JMLR.org},
abstract = {The pre-training and fine-tuning paradigm has contributed to a number of breakthroughs in Natural Language Processing (NLP). Instead of directly training on a downstream task, language models are first pre-trained on large datasets with cross-domain knowledge (e.g., Pile, MassiveText, etc.) and then fine-tuned on task-specific data (e.g., natural language generation, text summarization, etc.). Scaling the model and dataset size has helped improve the performance of LLMs, but unfortunately, this also lead to highly prohibitive computational costs. Pre-training LLMs often require orders of magnitude more FLOPs than fine-tuning and the model capacity often remains the same between the two phases. To achieve training efficiency w.r.t training FLOPs, we propose to decouple the model capacity between the two phases and introduce Sparse Pre-training and Dense Fine-tuning (SPDF). In this work, we show the benefits of using unstructured weight sparsity to train only a subset of weights during pre-training (Sparse Pre-training) and then recover the representational capacity by allowing the zeroed weights to learn (Dense Fine-tuning). We demonstrate that we can induce up to 75\% sparsity into a 1.3B parameter GPT-3 XL model resulting in a 2.5x reduction in pre-training FLOPs, without a significant loss in accuracy on the downstream tasks relative to the dense baseline. By rigorously evaluating multiple downstream tasks, we also establish a relationship between sparsity, task complexity and dataset size. Our work presents a promising direction to train large GPT models at a fraction of the training FLOPs using weight sparsity, while retaining the benefits of pre-trained textual representations for downstream tasks.},
booktitle = {Proceedings of the Thirty-Ninth Conference on Uncertainty in Artificial Intelligence},
articleno = {200},
numpages = {13},
location = {Pittsburgh, PA, USA},
series = {UAI '23}
}

@misc{kingma2017adammethodstochasticoptimization,
      title={Adam: A Method for Stochastic Optimization}, 
      author={Diederik P. Kingma and Jimmy Ba},
      year={2017},
      eprint={1412.6980},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/1412.6980}, 
}