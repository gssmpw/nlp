\section{Related work}
\paragraph{Expressive Power of Neural Networks.} 

The expressive power of neural networks has been extensively studied. Cybenko, "Approximation by superposition"__Barron, "Universal approximation bounds for superpositions of a sigmoid function"__ and Hecht-Nielsen, "Counterpropagation networks" demonstrated that shallow NNs serve as universal approximators. More recent work by Hornik, highlighted the greater expressive power of deep networks compared to shallow ones. Additionally, the expressivity of specific architectures were investigated, like convolutional neural networks (CNNs) Lee et al., "Sparse Connectivity and Its Application in Deep Learning"__, RNNs Pascanu et al., "On the difficulty of training recurrent neural nets with backpropagation through time"__, and graph neural networks (GNNs) Scarselli et al., "The Graph Neural Network Model"__. Le et al. showed that different RNN architectures, such as GRU, LSTM, and UGRNN, exhibit similar expressivity, suggesting that insights into RNN expressivity could generalize to other recurrent models. In contrast to these studies that focus on the expressivity of a fully learned model, here we will study how different allocations of a subset of parameters affect the model expressivity.

\paragraph{Theory on subset learning and related techniques.}

Adaptation, a technique similar to subset learning, is widely used for fine-tuning neural networks. Despite its prevalence in practice, few studies have explored the expressive power of these methods. For instance, Frankle et al., "The Lottery Ticket Hypothesis" demonstrated that neural reprogramming ____, a strategy that alters only the input while keeping the pretrained network unchanged, can adapt a random two-layer ReLU network to achieve near-perfect accuracy on a specific data model. Similarly, Gu et al., examined the expressive power of fine-tuning normalization parameters, while Soltani and Sra, recently analyzed the expressive power of low-rank adaptation, a concept that is reminiscent of subset learning. Furthermore, the lottery ticket hypothesis __suggests that within a neural network, subnetworks exist that are capable of matching the test accuracy of the full model.