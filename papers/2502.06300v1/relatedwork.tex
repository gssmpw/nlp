\section{Related work}
\paragraph{Expressive Power of Neural Networks.} 

The expressive power of neural networks has been extensively studied. \citet{cover1965geometrical} established limits on the expressivity of a single perceptron, while \citet{Cybenko1989ApproximationBS} and \citet{HORNIK1989359} demonstrated that shallow NNs serve as universal approximators. More recent work by \citet{raghu2017expressivepowerdeepneural,cohen2016expressivepowerdeeplearning,mont√∫far2014numberlinearregionsdeep}, highlighted the greater expressive power of deep networks compared to shallow ones. Additionally, the expressivity of specific architectures were investigated, like convolutional neural networks (CNNs) \citep{cohen2016expressivepowerdeeplearning}, RNNs \citep{SIEGELMANN1995132,khrulkov2018expressivepowerrecurrentneural}, and graph neural networks (GNNs) \citep{joshi2024expressivepowergeometricgraph}. \citet{collins2017capacitytrainabilityrecurrentneural} showed that different RNN architectures, such as GRU, LSTM, and UGRNN, exhibit similar expressivity, suggesting that insights into RNN expressivity could generalize to other recurrent models. In contrast to these studies that focus on the expressivity of a fully learned model, here we will study how different allocations of a subset of parameters affect the model expressivity.

\paragraph{Theory on subset learning and related techniques.}

Adaptation, a technique similar to subset learning, is widely used for fine-tuning neural networks. Despite its prevalence in practice, few studies have explored the expressive power of these methods. For instance, \citet{englert2022adversarialreprogrammingrevisited} demonstrated that neural reprogramming \citep{elsayed2018adversarialreprogrammingneuralnetworks}, a strategy that alters only the input while keeping the pretrained network unchanged, can adapt a random two-layer ReLU network to achieve near-perfect accuracy on a specific data model. Similarly, \citet{giannou2023expressivepowertuningnormalization} examined the expressive power of fine-tuning normalization parameters, while \citet{zeng2024expressivepowerlowrankadaptation} recently analyzed the expressive power of low-rank adaptation, a concept that is reminiscent of subset learning. Furthermore, the lottery ticket hypothesis \citep{frankle2019lotterytickethypothesisfinding, malach2020provinglotterytickethypothesis} suggests that within a neural network, subnetworks exist that are capable of matching the test accuracy of the full model.