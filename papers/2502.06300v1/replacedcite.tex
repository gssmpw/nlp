\section{Related work}
\paragraph{Expressive Power of Neural Networks.} 

The expressive power of neural networks has been extensively studied. ____ established limits on the expressivity of a single perceptron, while ____ and ____ demonstrated that shallow NNs serve as universal approximators. More recent work by ____, highlighted the greater expressive power of deep networks compared to shallow ones. Additionally, the expressivity of specific architectures were investigated, like convolutional neural networks (CNNs) ____, RNNs ____, and graph neural networks (GNNs) ____. ____ showed that different RNN architectures, such as GRU, LSTM, and UGRNN, exhibit similar expressivity, suggesting that insights into RNN expressivity could generalize to other recurrent models. In contrast to these studies that focus on the expressivity of a fully learned model, here we will study how different allocations of a subset of parameters affect the model expressivity.

\paragraph{Theory on subset learning and related techniques.}

Adaptation, a technique similar to subset learning, is widely used for fine-tuning neural networks. Despite its prevalence in practice, few studies have explored the expressive power of these methods. For instance, ____ demonstrated that neural reprogramming ____, a strategy that alters only the input while keeping the pretrained network unchanged, can adapt a random two-layer ReLU network to achieve near-perfect accuracy on a specific data model. Similarly, ____ examined the expressive power of fine-tuning normalization parameters, while ____ recently analyzed the expressive power of low-rank adaptation, a concept that is reminiscent of subset learning. Furthermore, the lottery ticket hypothesis ____ suggests that within a neural network, subnetworks exist that are capable of matching the test accuracy of the full model.