@article{Cybenko1989ApproximationBS,
  title={Approximation by superpositions of a sigmoidal function},
  author={George V. Cybenko},
  journal={Mathematics of Control, Signals and Systems},
  year={1989},
  volume={2},
  pages={303-314},
  url={https://api.semanticscholar.org/CorpusID:3958369}
}

@article{HORNIK1989359,
title = {Multilayer feedforward networks are universal approximators},
journal = {Neural Networks},
volume = {2},
number = {5},
pages = {359-366},
year = {1989},
issn = {0893-6080},
doi = {https://doi.org/10.1016/0893-6080(89)90020-8},
url = {https://www.sciencedirect.com/science/article/pii/0893608089900208},
author = {Kurt Hornik and Maxwell Stinchcombe and Halbert White},
keywords = {Feedforward networks, Universal approximation, Mapping networks, Network representation capability, Stone-Weierstrass Theorem, Squashing functions, Sigma-Pi networks, Back-propagation networks},
abstract = {This paper rigorously establishes that standard multilayer feedforward networks with as few as one hidden layer using arbitrary squashing functions are capable of approximating any Borel measurable function from one finite dimensional space to another to any desired degree of accuracy, provided sufficiently many hidden units are available. In this sense, multilayer feedforward networks are a class of universal approximators.}
}

@article{SIEGELMANN1995132,
title = {On the Computational Power of Neural Nets},
journal = {Journal of Computer and System Sciences},
volume = {50},
number = {1},
pages = {132-150},
year = {1995},
issn = {0022-0000},
doi = {https://doi.org/10.1006/jcss.1995.1013},
url = {https://www.sciencedirect.com/science/article/pii/S0022000085710136},
author = {H.T. Siegelmann and E.D. Sontag},
abstract = {This paper deals with finite size networks which consist of interconnections of synchronously evolving processors. Each processor updates its state by applying a "sigmoidal" function to a linear combination of the previous states of all units. We prove that one may simulate all Turing machines by such nets. In particular, one can simulate any multi-stack Turing machine in real time, and there is a net made up of 886 processors which computes a universal partial-recursive function. Products (high order nets) are not required, contrary to what had been stated in the literature. Non-deterministic Turing machines can be simulated by non-deterministic rational nets, also in real time. The simulation result has many consequences regarding the decidability, or more generally the complexity, of questions about recursive nets.}
}

@misc{cohen2016expressivepowerdeeplearning,
      title={On the Expressive Power of Deep Learning: A Tensor Analysis}, 
      author={Nadav Cohen and Or Sharir and Amnon Shashua},
      year={2016},
      eprint={1509.05009},
      archivePrefix={arXiv},
      primaryClass={cs.NE},
      url={https://arxiv.org/abs/1509.05009}, 
}

@misc{collins2017capacitytrainabilityrecurrentneural,
      title={Capacity and Trainability in Recurrent Neural Networks}, 
      author={Jasmine Collins and Jascha Sohl-Dickstein and David Sussillo},
      year={2017},
      eprint={1611.09913},
      archivePrefix={arXiv},
      primaryClass={stat.ML},
      url={https://arxiv.org/abs/1611.09913}, 
}

@article{cover1965geometrical,
  title={Geometrical and statistical properties of systems of linear inequalities with applications in pattern recognition},
  author={Cover, Thomas M},
  journal={IEEE transactions on electronic computers},
  number={3},
  pages={326--334},
  year={1965},
  publisher={IEEE}
}

@misc{elsayed2018adversarialreprogrammingneuralnetworks,
      title={Adversarial Reprogramming of Neural Networks}, 
      author={Gamaleldin F. Elsayed and Ian Goodfellow and Jascha Sohl-Dickstein},
      year={2018},
      eprint={1806.11146},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/1806.11146}, 
}

@misc{englert2022adversarialreprogrammingrevisited,
      title={Adversarial Reprogramming Revisited}, 
      author={Matthias Englert and Ranko Lazic},
      year={2022},
      eprint={2206.03466},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2206.03466}, 
}

@misc{frankle2019lotterytickethypothesisfinding,
      title={The Lottery Ticket Hypothesis: Finding Sparse, Trainable Neural Networks}, 
      author={Jonathan Frankle and Michael Carbin},
      year={2019},
      eprint={1803.03635},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/1803.03635}, 
}

@misc{giannou2023expressivepowertuningnormalization,
      title={The Expressive Power of Tuning Only the Normalization Layers}, 
      author={Angeliki Giannou and Shashank Rajput and Dimitris Papailiopoulos},
      year={2023},
      eprint={2302.07937},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2302.07937}, 
}

@misc{joshi2024expressivepowergeometricgraph,
      title={On the Expressive Power of Geometric Graph Neural Networks}, 
      author={Chaitanya K. Joshi and Cristian Bodnar and Simon V. Mathis and Taco Cohen and Pietro Liò},
      year={2024},
      eprint={2301.09308},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2301.09308}, 
}

@misc{khrulkov2018expressivepowerrecurrentneural,
      title={Expressive power of recurrent neural networks}, 
      author={Valentin Khrulkov and Alexander Novikov and Ivan Oseledets},
      year={2018},
      eprint={1711.00811},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/1711.00811}, 
}

@misc{malach2020provinglotterytickethypothesis,
      title={Proving the Lottery Ticket Hypothesis: Pruning is All You Need}, 
      author={Eran Malach and Gilad Yehudai and Shai Shalev-Shwartz and Ohad Shamir},
      year={2020},
      eprint={2002.00585},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2002.00585}, 
}

@misc{montúfar2014numberlinearregionsdeep,
      title={On the Number of Linear Regions of Deep Neural Networks}, 
      author={Guido Montúfar and Razvan Pascanu and Kyunghyun Cho and Yoshua Bengio},
      year={2014},
      eprint={1402.1869},
      archivePrefix={arXiv},
      primaryClass={stat.ML},
      url={https://arxiv.org/abs/1402.1869}, 
}

@misc{raghu2017expressivepowerdeepneural,
      title={On the Expressive Power of Deep Neural Networks}, 
      author={Maithra Raghu and Ben Poole and Jon Kleinberg and Surya Ganguli and Jascha Sohl-Dickstein},
      year={2017},
      eprint={1606.05336},
      archivePrefix={arXiv},
      primaryClass={stat.ML},
      url={https://arxiv.org/abs/1606.05336}, 
}

@misc{zeng2024expressivepowerlowrankadaptation,
      title={The Expressive Power of Low-Rank Adaptation}, 
      author={Yuchen Zeng and Kangwook Lee},
      year={2024},
      eprint={2310.17513},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2310.17513}, 
}

