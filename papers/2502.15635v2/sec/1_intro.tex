\section{Introduction}
\label{sec:intro}

As a widely investigated technique in 3D vision, novel view synthesis (NVS)~\cite{mildenhall2021nerf,Kerbl20233dgs} is utilized in two primary ways in the development of autonomous driving systems: (1) It facilitates the transfer of data for trained perception or end-to-end models across different vehicle products and sensor configurations~\cite{chen2023e2esurvey}; (2) It generates sensor frames with realistic geometry and appearance from various viewpoints for closed-loop simulations, particularly in sensor-to-control scenarios~\cite{yang2024drivearena}.

However, most existing NVS methods in autonomous driving, such as ~\cite{Kerbl20233dgs,yan2024street}, primarily focus on evaluating novel views based on interpolation quality rather than lateral viewpoint shifts, i.e., cross-lane NVS. This is due to the lack of datasets and benchmarks specifically designed for this purpose. As a result, the full potential impact of these new algorithms has not been fully demonstrated. Consequently, current simulation platforms have primarily validated the effectiveness of testing strategic changes in longitudinal (speed) behavior and motion planning, while evaluations of lateral (path) planning remain less convincing.

Unfortunately, collecting multi-lane ground truth data within a real-world scene is intrinsically difficult. As a result, XLD~\cite{li2024xld} chooses to use a simulation platform, Carla~\cite{Dosovitskiy17}, to render synthetic data with perfect parameters such as intrinsic, extrinsics, rolling-shutter appearances, and sensor frame poses. They have two primary limitations for further use: First, creating synthetic scenes with high-definition mesh models and materials requires meticulous adjustments for artistic and coherent shading. This process is expensive and must be completed before efficiently generating photo-realistic frames for evaluating driving failure cases on-board. Second, there are still challenging photo-realistic issues, such as inherent noise of real-sensors, and fog-wind-fluid caused dynamics, causing artifacts and domain transfer costs~\cite{zhang2024resimadzeroshot3ddomain}. Therefore, even though sensor data obtained from real recordings may have issues with imprecise parameters, they are an indispensable part of evaluating cross-lane NVS quality.
If we aim to collect the data in a single pass, then a super large structure to rigidly mount multiple cameras is required, however, a typical width of a round lane is about 3-4 meters, which is intricate to design and manufacture such a structure with stable dynamics. To tackle this difficulty, our method opts for multi-pass data collection.

This paper focuses on addressing the challenges of creating a real-world dataset using a multi-pass collection scheme for cross-lane NVS benchmarks by tackling the following issues. First, commonly used inertial navigation systems (INS) for obtaining pseudo ground-truth vehicle trajectories in most datasets~\cite{geiger2012kitti} are insufficient for aligning temporally adjacent sensor frames. This is because the data association between consecutive frames of exterceptive sensors does not directly participate in the maximum-a-posterior pose estimation. Instead, {temporal} alignment is achieved through dual filtering of the RTK-IMU trajectory, and {then composite with extrinsic parameters between IMU and exteroceptive sensors calibrated during end-of-line}.
Second, {pixel-to-point} mapping from camera frames to LiDAR frames is imprecise if we rely solely on the trajectory and LiDAR-IMU-cameras extrinsics after pose estimation{, which becomes worse when we need to remap across multiple scans.} Without establishing and resolving cross-modal feature correspondences between exteroceptive sensors~\cite{pascoe2015robust,song2016robust}, the mapping lacks accuracy. {We have addressed these two issues through our two-phase pose optimization (Sec.~\ref{sec:dataset:slam}).}

We develop a unified framework to construct the Para-Lane dataset, featuring a two-phase pose optimization mechanism for aligning data from exteroceptive sensors both temporally and spatially. Additionally, we implemented an autonomous system equipped with LiDAR and camera sensors to capture data, which is then processed using our proposed framework for cross-modal alignment.
As the first real-world dataset for cross-lane scenarios, we benchmark mainstream methods for driving NVS based on either NeRF or 3DGS. Our findings could inspire further research and enhance end-to-end driving simulations, and the dataset will be released publicly, ultimately and hopefully accelerating the research and development of autonomous driving products.
In summary, our work offers three key contributions:

\begin{itemize}
\item We curate the first real-world cross-lane dataset, dubbed Para-Lane, for evaluating NVS capabilities. It includes ample LiDAR, front-view, and surround-view camera data. Our dataset ensures that all sequences are annotated and grouped for easier benchmarking.
\item We propose a two-stage framework to precisely align exteroceptive sensors using cross-modal correspondences, demonstrating effectiveness in alignment metrics.
\item We evaluate recent NeRF and 3DGS methods, including those designed for autonomous driving scenes, on our curated dataset, offering insights into NVS performance with lateral viewpoint shifts.

%We evaluate recent NeRF and 3DGS methods on our dataset, providing insights into the ability of NVS methods specifically on lateral viewpoint shifts. By offering this dataset, we aim to provide a valuable resource for the research community to further develop the potential of closed-loop simulation for end-to-end autonomous driving evaluation.
\end{itemize}
