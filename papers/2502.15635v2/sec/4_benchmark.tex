\section{Benchmark}

Based on our scanners, scanning-pattern, and pose optimization procedures discussed in Sec.~\ref{sec:dataset}, we have prepared grouped sequences for benchmarking NVS methods on multi-lane scenarios. First of all, we discuss on implementation details of our selected NVS methods in Sec.~\ref{sec:bench:methods}, and then share our protocols and metrics chosen for cross-lane NVS application in Sec.~\ref{sec:bench:design}.

\subsection{Benchmarking Methods}
\label{sec:bench:methods}

% \noindent \textbf{Unified Settings:} 
We select a range of methods, specifically those designed for autonomous driving datasets and based on either NeRF or 3DGS, as our benchmarking methods.
For all methods discussed below, we use the combination of LiDAR $\mathcal{L}^\mathbb{G}$ and SfM $\mathcal{P}^\mathbb{G}$ points to initialize the Gaussians. 
In order to eliminate the influence of dynamic objects, we filter out all the cars and pedestrians in point clouds and images based on semantic labels.
Since the original 3DGS and most of its extensions only support focal points at the absolute center, we rectify the focal point to the center of corresponding images during the post-processing of our dataset.

\textbf{3DGS~\cite{Kerbl20233dgs}:} 
We utilize the implementation from the official release to evaluate our dataset and employ the AdamW optimizer with a learning rate of \(10^{-3}\). The model is trained for 30,000 steps.

\textbf{GaussianPro~\cite{cheng2024gaussianpro}:} 
We train the models for 30,000 iterations across all scenes, adhering to the original training schedule and hyperparameters. The interval step for the progressive propagation strategy is set to 20, with propagation performed three times. 

\textbf{Scaffold-GS~\cite{Lu2024scaffoldgs}:} 
Both the appearance and feature dimensions in the MLP are set to 32, and voxel size is set to 0.005. We adjust the initial and hierarchy factors for anchor growing to 16 and 4, respectively. The model is trained for 30,000 steps.

\textbf{2DGS~\cite{Huang20242DGS}:} 
We keep most parameters consistent with the original implementation. For densification, we adjust the gradient threshold to \(3 \times 10^{-4}\) and set the final densification iterations to 13,000. The model is trained for 30,000 steps.

\textbf{Street Gaussians~\cite{yan2024street}:} To ensure a fair comparison with other methods, we omit the handling of dynamic objects from the original scene graph method. Additionally, we do not utilize the sky mask for an equitable comparison. The parameters remain consistent with those in the official implementation.

\textbf{PVG~\cite{chen2023pvg}:} 
We utilize the Adam optimizer and keep a comparable learning rate for most parameters, consistent with the original implementation. We adjust the gradient threshold to \(3 \times 10^{-4}\) and set the final densification iterations to 13,000. The maximum number of Gaussian spheres is configured to \(10^{6}\). {We omit multi-resolution downsampling, using images at their original resolution for training.}

\textbf{EmerNeRF~\cite{yang2023emernerf}:} 
We train EmerNeRF for 30,000 iterations using its original parameters. The flow branch and temporal interpolation are activated, with both the feature levels of the hash encoder for the static and dynamic branches set to 4.

\subsection{Experimental Protocols and Metrics}
\label{sec:bench:design}


To perform a comprehensive benchmark of all the aforementioned methods on our proposed dataset, we meticulously group all sequences and organize the benchmarks across five different tracks for each method. Specifically, the tracks are categorized as follows: (1) Single lane regression, (2) Adjacent lane prediction, (3) Second-adjacent lane prediction, (4) Adjacent lane prediction (trained from two lanes), and (5) Sandwich lane prediction (trained from two side lanes). A figure illustrating the experimental protocols is provided in Fig.~\ref{fig:expset}. For each track, we uniformly sample 200 frames from training sequences for model learning and 25 frames from test sequences as the ground truth.
% As we have grouped these sequences recording parallel lanes in the same direction, and most road groups have at least 3 parallel lanes, we perform five NVS ability tests listed in Fig.~\ref{fig:expset} for each method: (1) Single lane regression. (2) Adjacent lane prediction. (3) Secondly-adjacent lane prediction. (4) Adjacent lane prediction (train from two lanes). (5) Sandwich lane prediction (train from two side lanes). 

We adhere to the widely used metrics for evaluating the performance of NVS as outlined in \cite{li2024xld}, which includes Peak Signal-to-Noise Ratio (PSNR), Structural Similarity Index (SSIM), and Learned Perceptual Image Patch Similarity (LPIPS).

\subsection{Experimental Results and Notes}
\label{sec:bench:main}

\begin{figure}[t]
\includegraphics[width=\linewidth]{sec/images/expset.pdf}
\caption{{Five evaluation tracks using different combinations of lanes for training (colored in blue) and testing (colored in red).}}
\label{fig:expset}
\end{figure}

\begin{table*}[t]
\caption{Quantitative results of different Gaussian reconstruction methods on our proposed Para-Lane dataset.}
\centering
{
\fontsize{8pt}{9.6pt}\selectfont
\setlength{\tabcolsep}{2.2pt}
\begin{tabular}{l|ccc|ccc|ccc|ccc|ccc}
\toprule
Method                  &   \hmerge{Single}     &  \hmerge{Adjacent}    &  \hmerge{Sec-Adj.}    &  \hmerge{Two-for-One} & \hmergl{Sandwich}       \\
Metrics                 &       \metrics        &      \metrics         &      \metrics         &      \metrics         &      \metrics           \\ 
\midrule
% EmerNeRF                &       &       &       &       &       &       &       &       &       &       &       &       &       &       &         \\
% \midrule
3DGS                    & \cccf{22.99} & \cccf{0.689} & \cccs{0.344} & \ccct{17.05} & \cccs{0.524} & \cccf{0.446} & \ccct{16.26} & \ccct{0.505} & \cccs{0.472} & \ccct{17.85} & \ccct{0.551} & \ccct{0.440} & 18.74 & \ccct{0.563} & \cccs{0.424}    \\
% PVG                     & 22.46 & 0.669 & 0.376 & 15.64 & 0.507 & 0.493 & 14.80 & 0.498 & 0.515 & 15.00 & 0.498 & 0.510 & 15.80 & 0.511 & 0.489    \\
GaussianPro             & \ccct{22.93} & \cccs{0.687} & \cccf{0.343} & 17.01 & 0.521 & \cccf{0.446} & \cccs{16.29} & \ccct{0.505} & \cccs{0.472} & 17.83 & \ccct{0.551} & \cccs{0.439} & 18.66 & 0.562 & \cccs{0.424}   \\
Scaffold-GS             & \cccs{22.96} & \ccct{0.675} & 0.364 & \cccf{17.59} & \cccf{0.538} & \ccct{0.450} & \cccf{17.09} & \cccf{0.525} & \cccf{0.470} & \cccf{18.62} & \cccf{0.565} & \cccf{0.437} & \cccf{19.20} & \cccf{0.574} & \cccf{0.423}   \\
2DGS                    & 22.29 & 0.651 & 0.395 & 16.79 & \ccct{0.523} & 0.469 & 16.01 & \cccs{0.510} & 0.494 & 17.46 & 0.548 & 0.466 & \cccs{19.04} & \cccs{0.572} & 0.451   \\
Street Gaussians        & 22.56 & 0.643 & \ccct{0.353} & \cccs{17.50} & 0.510 & 0.456 & 16.16 & 0.496 & 0.480 & \cccs{17.87} & \cccs{0.555} & 0.453 & \ccct{18.91} & 0.561 & 0.443   \\
\bottomrule
\end{tabular}
}

\label{tab:eval}\vspace{-10pt}
\end{table*}

\begin{figure*}[t]
\includegraphics[width=\linewidth]{sec/images/Exp-results.pdf}
\caption{Comparisons for NVS quality between different methods and designs, see our supplementary video for results on more sequences.}
\label{fig:eval}
\end{figure*}

We benchmark the methods described in Sec.~\ref{sec:bench:methods} according to the framework outlined in Sec.~\ref{sec:bench:design} to evaluate performance for cross-lane NVS. A series of experiments were conducted using an NVIDIA GeForce RTX 3090 24GB GPU. For quantitative metrics across all tested methods and different designs, refer to Tab.~\ref{tab:eval}. Throughout the experiments, we discovered several interesting insights:
\vspace{4pt}

\noindent \textbf{The quality of NVS is significantly affected by the view distribution of the training set.} 
From Tab.~\ref{tab:eval}, we found {\emph{exactly}} the same conclusion for all methods: the performance gradually decreases in the following sequence: Single $>$ Sandwich $>$ Two-for-One $>$ Adjacent $>$ Second-Adjacent. When the training and testing views are on the same trajectory, all methods achieve the best NVS results. However, when the testing viewpoint undergoes lateral shifts, the results are compromised to varying degrees. 
{Besides, although the number of images used for training is the same, our Sandwich {track}, which evenly distributes training views on both sides of the test views, consistently achieves superior rendering quality compared to the Two-for-One {track}, where training views are located only on one side of the testing views.}
This can be attributed to the fact that when training data is more evenly distributed and closer to the target render pose, the learned radiance functions are less likely to overfit to a specific viewpoint. 
Therefore, from an application perspective, to generate images of a target scene from arbitrary viewpoints, it is advisable to utilize multiple passes of data to reconstruct the target scene, thereby minimizing potential artifacts during novel view synthesis.
{Fig.~\ref{fig:eval} presents a visual comparison of novel view synthesis results under different designs, and our supplementary video provides additional examples. }

% \noindent \textbf{The quality of NVS is significantly affected by the view distribution of the training set.} 
% In our experiments, 
% This underscores the importance of gathering training images with a sufficiently even distribution of perspectives and training a scalable Gaussian model that accommodates such diversity when synthesizing novel views in real-world scenarios.

\noindent \textbf{Domain gap between synthetic~\cite{li2024xld} and real datasets.} Most of the methods tested here were also evaluated on the XLD dataset~\cite{li2024xld}, a synthetic cross-lane dataset; however, they generally did not perform as well on our dataset as they did on XLD. We attribute this difference to the domain gaps between synthetic and real-world data for several reasons. Firstly, synthetic data perfectly ensures the accuracy of all parameters, including extrinsic, intrinsic, and timestamps. However, in the real world, despite the optimizations discussed in Sec.~\ref{sec:dataset:slam}, there inevitably remains a gap between our final estimations and the ground truth values. Secondly, the sequences in a group were collected over different time intervals, leading to minor variations in brightness, water stain shapes, and other trivial factors among the cross-lane sequences. This also brings errors that require NVS approaches to handle. Finally, in real-world data, special attention must be given to dynamic objects. Unlike synthetic data, we cannot capture observations of the same dynamic object simultaneously across different locations. Although we used SAM to mask dynamic objects, the model's output is not always precise, and some noise remains.

\noindent \textbf{Scaffold-GS achieves the best NVS performance on our benchmark.} 
Unlike other methods, Scaffold-GS~\cite{Lu2024scaffoldgs} utilizes anchor points to distribute 3D Gaussians. Each anchor point is associated with multiple neural Gaussians. The attributes of these neural Gaussians—such as position, opacity, quaternion, scaling, and color—are determined by a multi-layer perceptron (MLP). The input features for the MLP include the relative poses from the anchor points to the camera views. Our cross-lane experimental results demonstrate that this method, which establishes correlations between view poses and rendering outcomes, is effective.

\subsection{Handling Dynamic Objects}

Besides the main experiment in Sec.~\ref{sec:bench:main} that reflects performance of methods in masked static scenes, we perform another experiment on those experiments capable of handling dynamic objects.

EmerNeRF~\cite{yang2023emernerf} and PVG~\cite{chen2023pvg} are two representative methods that contain procedures on handling dynamic objects in a self-supervised manner, we compare the two methods using {the Single track. We perform reconstruction through both methods with and without automatically labeled mask~\cite{Kirillov2023sam}}
The dataset used in this part includes 6 groups, which are part of the entire 25 groups dataset.

\textbf{Results and analysis.} From Tab.~\ref{tab:eval_dynamic}, we can find that EmerNeRF achieves better PSNR and LPIPS scores. This is probably because of its novel and effective approach of static-dynamic decomposition. On the other hand, PVG excels in SSIM for both tests. This exceptional performance is likely to due to the adaptable design for Gaussian points. 
Qualitative results are shown in Fig.~\ref{fig:pvg}. We can find that the two methods are comparable for nearby scenes. However, PVG results are inferior for distant locations, such as buildings and trees. This is likely due to that PVG advocates to utilize the larger points for faraway scenes as described in its draft~\cite{chen2023pvg}, which results in inadequate expression of details and cause blur.

\begin{table}[htbp]
\centering
{
\fontsize{8pt}{9.6pt}\selectfont
\setlength{\tabcolsep}{5pt}
% \fontsize{9pt}{9pt}\selectfont
% \setlength{\tabcolsep}{0.9pt}
\begin{tabular}{l|ccc}
\toprule
Method                       &   \hmergl{Single}     \\
Metrics                      &       \metrics        \\ 
\midrule
EmerNeRF                     & \cccf{23.67} & 0.668 & \cccf{0.350} \\
PVG                          & 22.08 & \cccf{0.672} & 0.408 \\
\midrule
EmerNeRF (static only)       & \cccf{23.76} & 0.678 & \cccf{0.346} \\
PVG (static only)            & 22.77 & \cccf{0.684} & 0.368 \\
\bottomrule
\end{tabular}
}
\caption{Quantitative results on our proposed Para-Lane dataset with dynamic objects. We perform reconstruction with and without mask for ablation study.}
\label{tab:eval_dynamic}
\end{table}

\begin{figure}[htbp]
\includegraphics[width=\linewidth]{sec/images/pvg-cross.pdf}
\caption{Comparisons for NVS quality in Single lane test between EmerNeRF and PVG.}
\label{fig:pvg}
\end{figure}


\subsection{Limitations}

This section describes the limitations of our current dataset and benchmark. The dynamic object masks provided in the dataset are not manually labeled, so there are a small number of omissions and mislabeling. We also do not yet have 3D bounding box and tracking labels for all dynamic objects. The diversity of the dataset can also be enhanced by collecting more data in the future.
Given the fact that current works rarely support dynamic/static decomposition, we only tested EmerNeRF~\cite{yang2023emernerf} and PVG~\cite{chen2023pvg} in handling dynamic objects. A more comprehensive benchmark for dynamic scenes would be beneficial as more dynamic methods are developed in the future.
% We have not yet conducted more studies about dynamic/static decomposition capabilities, which may involve methods such as PVG, EmerNerf, 4D gaussian~\cite{wu20244dgaussiansplattingrealtime}, etc.