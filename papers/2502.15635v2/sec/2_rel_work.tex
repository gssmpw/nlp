\section{Related Work}
\label{sec:rel_work}

\subsection{Autonomous Driving Datasets for NVS} 
In autonomous driving, there exists a number of available datasets, such as KITTI~\cite{geiger2012kitti}, KITTI-360~\cite{liao2022kitti360}, CityScapes~\cite{cordts2016cityscapes}, Waymo Open Dataset~\cite{sun2020waymo}, nuScenes~\cite{caesar2020nuscenes}, LiDAR-CS~\cite{fang2024lidar} and WayveScenes101~\cite{zürn2024wayvescenes101datasetbenchmarknovel}. In decades, they are regarded as the foundation of numerous autonomous driving solutions and algorithms~\cite{hu2023uniad,liao2023MapTR,chen2023e2esurvey}. However, the community lacks datasets that are specifically or compatible to evaluate NVS tasks, due to the booming requirements of end-to-end autonomous driving research. 
A very recent work, Open MARS Dataset~\cite{li2024openmars}, also features multiple laser scanners and cameras for driving scenes. However, their focus is performing collaboratively multi-agent and multi-traversal data collection, with an emphasis on obtaining spatially nearby sequences, instead of multi-lane sequences. Moreover, the multi-pass frame registration algorithm is not disclosed in their work.
% \md{The most similar one, compared to our dataset design, is  the Open MARS Dataset~\cite{li2024openmars}, which also features multiple laser scanners and cameras, and performs multi-pass scanning to collect data on public urban roads. However, their multi-pass setting mainly focuses on obtaining spatially nearby sequences, not explicitly scanned between parallel lanes on the same road. Moreover, they have not explained the method of multi-pass frame registration.}
The XLD~\cite{li2024xld} dataset, introduced earlier this year, serves as a reliable resource based on synthetic scenes and rendering. However, as discussed in Sec.~\ref{sec:intro}, we argue that real-world datasets are more essential and reliable for comprehensive evaluation, though there are many challenges to curate such a dataset.

\subsection{Multi-Sensor Data Alignment} 
We categorize the multi-sensor dataset alignment into single-modal (camera or LiDAR) and multi-modal (camera and LiDAR) alignments, both of which are crucial for creating a high-quality real-world dataset for NVS evaluation.

% Because some NVS methods~\cite{yan2024street} borrow LiDAR points as an initialization asset for 3D Gaussians\md{, ensuring the quality of camera-LiDAR multi-modal data alignment} is crucial for subsequent forward and backward operations.

% \md{Aiming at precise data alignment, and successfully solving the pose of each sensor frame is a crucial procedure. It contains two aspects: (1) Ensuring accurate alignment between consecutive frames for each single sensor. (2) Ensuring a precise alignment of the acquired data between multiple sensors. Although this can be achieved through a unified multi-sensor SLAM framework, but practically for mass-production of large-scale scenes, the industrial community choose to handles these two steps separately:} In High-Definition Map (HDMap) construction for L4 vehicles, LiDAR mapping is a well solved problem~\cite{yang2018pgo} in earlier years, and thus becomes a standard operating procedure for mass-production \md{and vehicle deployment}. For cameras, Structure-from-Motion (SfM)~\cite{snavely2006photo,scho2016sfm} and multi-view stereo~\cite{moulon2016openmvg,Xu2020ACMP} is another prevalently used pipeline for solving frame poses and perform dense geometric reconstruction, respectively.
Ensuring precise alignment of data from multiple sensors is essential. While a unified multi-sensor SLAM framework can achieve this, the industrial community typically handles these steps separately for large-scale scene production. For Level-4 unmanned vehicles, during High-definition Map (HDMap) production, LiDAR mapping has been a well-established process~\cite{yang2018pgo} and is now standard for mass production and vehicle deployment. For cameras, Structure-from-Motion (SfM)~\cite{snavely2006photo,scho2016sfm} and multi-view stereo~\cite{moulon2016openmvg,Xu2020ACMP} are commonly used for solving frame poses and performing dense geometric reconstruction.

To address the cross-modal data association issue—linking cameras and LiDAR—we need to establish explicit correspondences between LiDAR points and camera pixels for a densely coupled pose estimation. There are two categories of methods based on their inputs: the first category operates between LiDAR frames and camera frames~\cite{jing2022dxqnet}, which is inefficient for generating large sequences. Therefore, assuming LiDAR mapping provides a sufficiently accurate trajectory, the second category—our chosen approach—works between LiDAR sequences and camera frames~\cite{zhou2014colormapopt,li2024vxp}. While some methods primarily use ray-casted depth information, we found that incorporating the intensity channel from LiDAR measurements is beneficial. This allows for the use of both sparse and dense photometric loss alongside geometric loss, as demonstrated in various RGB-D reconstruction methods~\cite{Whelan15rss,dai2017bundlefusion}. To establish photometric loss, we identified Normalized Information Distance (NID)~\cite{ming2004similarity,pascoe2015robust} as the most effective metric for linking these two types of channels, which can assess the quality of multi-modal registration.

\subsection{NeRF and 3DGS Approaches for NVS} 
Beyond the classical dense reconstruction methods that utilize Truncated Signed Distance Function (TSDF)~\cite{dai2017bundlefusion} and Surfels~\cite{Whelan15rss} to represent large-scale scenes. NeRF~\cite{mildenhall2021nerf} and 3DGS~\cite{Kerbl20233dgs} methods are profound innovations in the enhancement of representing geometric and appearance details, respectively. 
For instance, Block-NeRF~\cite{tancik2022block} is a pioneering work that addresses the reconstruction of large-scale urban scenes through division.
MARS~\cite{wu2023mars} is a modular, instance-aware simulator built on NeRF, which separately models dynamic foreground instances and static background environments. 
UniSim~\cite{yang2023unisim} converts recorded logs into realistic closed-loop multi-sensor simulations, incorporating dynamic object priors and using a convolutional network to address unseen regions. Rather than implicitly representing scenes (NeRF) that lack of flexibility of editing and labeling those reconstructed assets, explicitly representing scenes (3DGS) has aroused the interest of many industrial autonomous driving teams~\cite{yan2024street} to operate on their domain-specific database. 
Our dataset and benchmark scanned from the real world are specifically designed to evaluate the performance of cross-lane NVS methods. Some of these methods~\cite{Kerbl20233dgs,chen2023pvg,cheng2024gaussianpro,Lu2024scaffoldgs,Huang20242DGS,yan2024street}, considering the availability of their code, are used as baselines in our evaluation. 
% \chenming{MARS~\cite{wu2023mars}, UniSim~\cite{yang2023unisim} -- please add enough references}
