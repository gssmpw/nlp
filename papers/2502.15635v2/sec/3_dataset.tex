\section{Curation of Para-Lane Dataset}
\label{sec:dataset}

\subsection{Sensor Setup}

To collect and process raw data for our Para-Lane dataset, we implemented an autonomous system equipped with various sensors for street operation. Specifically, to meet the input data specifications for NVS approaches, we installed one front-view camera with a $90^\circ$ Field-of-View (FOV) capturing $1920 \times 1080$ images at 10Hz, along with four surround-view cameras featuring $190^\circ$ fisheye lenses capturing $1920 \times 1080$ images at 10Hz. Additionally, we used three 3D laser scanners with 32 LiDAR channels from $+15^\circ$ to $-55^\circ$ on the vertical FOV capturing at 10Hz. All frame timestamps are synchronized at the hardware level, and we combine points from the three laser scanners into a single LiDAR frame after motion compensation.

Fig.~\ref{fig:assemble} illustrates our autonomous hardware and its assembly, and we refer readers to our {accompanying} calibration parameters {in the dataset} for more details. Besides the sensors depicted in the figure, we have additional sensors inside, such as an Inertial Navigation System (INS) for obtaining a high-quality initial trajectory prior to data alignment. In our dataset, we provide all necessary relative transformations between {grouped} sequences and a reference coordinate system.


\begin{figure}[t]
\includegraphics[width=\linewidth]{sec/images/HW.pdf}
\caption{Sensor assembly and sample frames of our data collection unmanned vehicle, the right fisheye camera is mounted symmetrically opposite to the left fisheye, and the back fisheye is located at the center of the back-side.}
\label{fig:assemble}
\end{figure}

\subsection{Multi-pass Data Acquisition and Processing}

We selected clear sunny days with uncongested road conditions to drive through each parallel lane in the same direction. The data was collected in an anonymous city (details will be disclosed after the review period), consisting of 75 sequences grouped into 25 scenes. Each scene includes three sequences from different lanes, sharing the same start and end positions orthogonal to the road direction, covering approximately 150 meters. As our collection vehicle traveled on public roads, its speed fluctuated, resulting in sequence durations ranging from 10 to 45 seconds, with a median of 20 seconds.

After data collection, we anonymized the images by obscuring vehicle license plates and pedestrian faces. We employed SAM~\cite{Kirillov2023sam}, followed by a manual quality inspection, to segment dynamic elements from static foregrounds and backgrounds, ensuring the dataset is free of ethical issues.

\subsection{Two-phase Pose Optimization}
\label{sec:dataset:slam}

\noindent \textbf{Phase 1: LiDAR mapping.} 
Given the {initial trajectory} from the RTK/INS sensor, we apply~\cite{yang2018pgo} to construct a LiDAR map $\mathcal{L}^\mathbb{G}$ in a reference coordinate $\mathbb{G}$, which involves two components: odometry and loop refinement. We focus on the first component—offline LiDAR odometry—by implementing an enhanced offline LIO system~\cite{shan2020liosam} that utilizes both LOAM features~\cite{zhang2014loam} and dense scan-to-submap point cloud registration~\cite{segal2009gicp} for robustness across various mapping scenarios. For the second component, loop closure and pose graph optimization, we begin with the coarse matching of submaps using Predator~\cite{huang2021predator} and SuperLine3D~\cite{zhao2022superline3d}. We then evaluate the matching scores to select candidate submap pairs for fine registration~\cite{segal2009gicp}, establishing precise relative constraints for the pose graph optimization problem. Relative poses between sequences are determined through multi-pass loop closure associations and joint optimization.


To verify the quality of LiDAR mapping, we use the thickness of structural objects as a reference metric. We first perform mesh reconstruction using VDBFusion~\cite{vizzo2022sensors} and Marching Cubes~\cite{lorensen87marchingcube} with the solved LiDAR frame poses (both voxel size and truncation distance set to 5 cm) to create a triangle mesh representing the maximum-a-posteriori locations of watertight object surfaces. We then calculate the Mean Absolute Error (MAE) and Root Mean Square Error (RMSE)~\cite{millane2018cblox} by comparing the mesh with the stitched point cloud to assess thickness. Tab.~\ref{tab:lidarmapping} and Fig.~\ref{fig:lidarmap} demonstrate the effectiveness of {achieving a thinner LiDAR map after the combination of multiple scaned sequences.}

\noindent \textbf{Phase 2: Registering images to the LiDAR map.} 
After obtaining the LiDAR map $\mathcal{L}^\mathbb{G}$ and the pose of each LiDAR frame, we register the images $\bigcup_i \{\mathbf{C}_i\}$ captured by our camera sensors to $\mathcal{L}^\mathbb{G}$. This involves determining the pose of each camera frame $\mathbf{C}_i$ relative to the reference coordinate, denoted as $\mathbf{T}_{\mathbb{C}_i}^\mathbb{G}$. We begin by coarsely initializing these poses using linear-slerp interpolation~\cite{buss2001linearslerp} between the two adjacent LiDAR frames and an extrinsic parameter.
%$\hat{\mathbf{T}}_{\mathbb{C}_t}^\mathbb{G} \triangleq \phi(\mathbf{T}_{\mathbb{L}_a}^{\mathbb{G}},\mathbf{T}_{\mathbb{L}_b}^{\mathbb{G}}) \cdot \mathbf{T}_{\mathbb{C}}^{\mathbb{L}}, t \in [a, b]$.

We then refine the coarse camera poses by formulating the following factor graph optimization problem~\cite{grisetti2010tutorial} to optimize all camera poses $\mathcal{C}^\mathbb{G} \triangleq \bigcup_i \{\mathbf{T}_{\mathbb{C}_i}^\mathbb{G}\}$ and the 3D positions of visual landmarks $\mathcal{P}^\mathbb{G} \triangleq \bigcup_j \{\mathbf{p}_j^{\mathbb{G}}\}$ using an expectation-maximization (EM) strategy~\cite{bowman2017em}:
%
\begin{equation}
\min_{\mathcal{C}^\mathbb{G}, \mathcal{P}^\mathbb{G}} 
  {\mathbf{E}^1(\mathbf{C}_i, \mathbf{p}_j}) +
  {\mathbf{E}^2(\mathbf{p}_j, \mathcal{L}^\mathbb{G}}) +
  {\mathbf{E}^3(\mathbf{C}_i, \mathbf{p}_j}, \mathbf{L}_i),
\label{equ:sfm}
\end{equation}
%
where $\mathbf{E}(a, b[, c]) = \sum -\log(\mathbf{f}_{a, b[, c]})$ is the sum of negative log-likelihood of a type of valid factor constraints $\mathbf{f}$, making their scale factors become irrelevant constants. $\mathbf{L}_i \triangleq \pi((\mathbf{T}_{\mathbb{C}_i}^\mathbb{G})^{-1} \cdot \mathcal{L}^\mathbb{G})$ is the intensity image ray-casted from $\mathcal{L}^{\mathbb{G}}$ at $\mathbf{T}_{\mathbb{C}_i}^\mathbb{G}$. 
Specifically, these factors $\{\mathbf{f}\}$ are designed as:

%
\begin{table}[t]

\caption{Quantitative metrics for LiDAR mapping. We choose to sample and evaluate the MAE and RMSE of stitched LiDAR frames (in centimeters).}\vspace{-4pt}
\centering
% \fontsize{6.5pt}
\fontsize{8pt}{9.6pt}\selectfont
\setlength{\tabcolsep}{4pt}
\begin{tabular}{l|cccc|cccc}
\toprule
       &  \lmerge{MAE $\downarrow$}            &  \lmergl{RMSE $\downarrow$} \\
Metrics&  \lmetrics               &  \lmetrics  \\
\midrule 
Before & 4.5 & 5.8 & 5.9 & 7.3 & 7.4 & 9.8 &10.3 &12.6  \\
After  & \cccf{1.3} & \cccf{1.4} & \cccf{1.4} & \cccf{1.7} & \cccf{2.9} & \cccf{3.6} & \cccf{4.1} & \cccf{7.1}  \\
\bottomrule
\end{tabular}
\label{tab:lidarmapping}
\end{table}
%
\begin{figure}[t]
\includegraphics[width=\linewidth]{sec/images/LiDARMap.pdf}
\caption{LiDAR map stitching quality visualized in both 20cm periodical height ramp {in rainbow} (left columns) and 10cm cividis colormap reflecting distance with their reconstructed mesh (the right column). Both the error map and zoomed-in views reflect that these refined LiDAR frame poses (the second row), compared to the initial RTK trajectory (the first row), have achieved a thinner stitched cloud with fewer hovering noisy points due to better frame poses.}
\label{fig:lidarmap}
\end{figure}
%

\textbf{$\mathbf{f}^1$: Structure-from-motion (SfM) for cameras.} We use SuperGlue~\cite{wang2019superglue} to associate multiple camera frames with joint observations on sparse features.
We formulate this factor using the reprojection error~\cite{scho2016sfm}, which quantifies the error by projecting the 3D positions of landmarks $\mathbf{p}_j^\mathbb{G}$ onto the image plane of the corresponding frame pose $\mathbf{T}_{\mathbb{C}_i}^\mathbb{G}$:
%
\begin{equation}
  \mathbf{f}_{\mathbf{C}_i, \mathbf{p}_j}^{1} \propto \exp(-\frac{1}{2} \|
  \hat{v}_j - \mathbf{K} ( (\mathbf{T}_{\mathbb{C}_i}^\mathbb{G})^{-1} \cdot \mathbf{p}_j^\mathbb{G} ) 
\|_{\Omega_{1}}^2),
\label{equ:sfm1}
\end{equation}
%
where $\| \mathbf{e} \|_\Omega^2 \triangleq \mathbf{e}^\top \Omega^{-1} \mathbf{e}$ represents the squared Mahalanobis distance with the covariance matrix $\Omega$. Here, $\hat{v}_j$ is the corresponding pixel observed in $\mathbf{C}_i$, and $\mathbf{K}(\cdot)$ denotes the perspective projection and rectification function with respect to the intrinsic and distortion parameters.

\textbf{$\mathbf{f}^2$: SfM points and LiDAR map registration factors.} 
Longitudinal driving sequences often lack sufficient parallax to accurately estimate depth for the sparse landmarks $\mathcal{P}^\mathbb{G}$. Therefore, we constrain the absolute positions of these landmarks once they are effectively associated with nearby LiDAR points. The unary prior factor for each landmark is defined as:
%
\begin{equation}
  \mathbf{f}_{\mathbf{p}_j, \mathcal{L}^\mathbb{G}}^{2} \propto \exp(-\frac{1}{2} \|
  \mathbf{p}_j^\mathbb{G} - \Psi(\mathcal{L}^\mathbb{G}, \mathbf{p}_j^\mathbb{G})
\|_{\Omega_{2}}^2).
\label{equ:sfm2}
\end{equation}
%

We found that the choice between point-to-point and point-to-plane formulations is not critically important; however, the point-picking strategy $\Psi(\cdot)$ is significant. Specifically, we select a group of candidate 3D LiDAR points based on the current nearest search for $\mathbf{p}_j^\mathbb{G}$, and calculate the tangential distance between these points and the rays emitted from multiple camera observations $\mathcal{R}(\mathbb{C}_i, \hat{v}_j^{\mathbb{C}_i})$ as a joint weight for the final prior position.

\textbf{$\mathbf{f}^3$: Cross-modal sparse and dense factors.} Constructing linkage among sparse 3D points through the previous factor $\mathbf{f}^2$, in our scenario, is not sufficient enough for a camera-pixel to LiDAR-point level data integration. Inspired by the tightly-coupled multi-modal registration used in RGB-D reconstruction~\cite{dai2017bundlefusion}, we use a combined sparse-and-dense factor, with SuperGlue~\cite{wang2019superglue} again for providing sparse constraints, and we define cross-modal photometric error~\cite{stein2011densepholoss} for dense constraints as:
%
\begin{align}
  \mathbf{f}_{\mathbf{C}_i, \mathbf{p}_j, \mathbf{L}_i}^{3} \propto \exp(
  &-\frac{1}{2} \| \hat{w}_j - \mathbf{K} ((\mathbf{T}_{\mathbb{C}_i}^\mathbb{G})^{-1} \cdot \mathbf{p}_j^\mathbb{G} ) \|_{\Omega_3^s}^2 \\
  &-\frac{1}{2} \|
  \mathbf{C}_i \ominus \mathbf{L}_i \|_{\Omega_3^d}^2 ),
\label{equ:sfm3}
\end{align}
%
where the first half focuses on maintaining sparse relationships between input and ray-casted frames, while the second half addresses dense matching. To establish sparse correspondences between ray-casted feature points $\hat{w}_j$ and SfM points $\mathbf{p}_j^\mathbb{G}$, we utilize feature points extracted and matched in $\mathbf{C}_i$ to connect these 2D-3D relationships. The operator $\ominus$ represents $E(\xi)$ as defined in Equation 11 of~\cite{stein2011densepholoss}, which employs image gradients to slightly adjust the camera pose.

{\textbf{Parameters.}
We set the number of Superglue correspondences to 500 for both $\mathbf{f}^1$ and $\mathbf{f}^3$. We use parameters $\Omega_1 = 1$, $\Omega_2 = 0.3 \cdot \mathbf{I}_3$, $\Omega_3^s = 1$, and $\Omega_3^d = 0.01$ to perform the maximization step using Levenberg-Marquardt (LM) with 50 iterations. During optimization, we recalculate the point picking strategy $\Psi(\cdot)$ in $\mathbf{f}^2$ and the ray-casted image $\mathbf{L}_i$ from $\pi(\cdot)$ in $\mathbf{f}^2$ once every 3 iterations.}

\textbf{Evaluation and Ablation Study.} To evaluate the quality of multi-camera alignment, we utilize the Normalized Information Distance (NID) proposed in Equation 3 of~\cite{pascoe2015robust} as a quantitative metric. For each pair of original grayscale images and LiDAR intensity images rendered from a given pose, we compute an initial NID by discretizing the continuous pixel values into 64 bins. We then slightly shift the original image along the UV coordinates to identify a position that yields a local minimum of the NID. The offset used to find this local minimum is defined as NID-Loss.

We present the results of the ablation study in Tab.~\ref{tab:sfm} and Fig.~\ref{fig:sfm}. These results demonstrate that the classical SfM framework ($\mathbf{f}^1$) can jointly solve poses between camera frames to ensure single-modal coherency. However, the lack of correspondence to the LiDAR map $\mathcal{L}^\mathbb{G}$ undermines multi-modal coherency. Therefore, an effective approach within the SfM framework is to construct the paired relationship between Visual-SfM points and their corresponding LiDAR points, providing a strong depth prior for triangulation—especially for landmarks established with a narrow baseline (e.g., landmarks on the road or curb while vehicles drive straight). Moreover, photometric factors based on 2D input and ray-casted images can further enhance multi-modal coherency, as our experiments indicate. This is because pixel-level correspondences and gradients operating on grayscale and intensity images, similar to depth map contours used for texture mapping~\cite{zhou2014colormapopt}, directly utilize raw input channels from different sensors.

% We show the results of the ablation study 
% in Tab.~\ref{tab:sfm} and Fig.~\ref{fig:sfm}. Those results demonstrates that the classical SfM design ($\mathbf{f}^1$) can jointly solve poses between camera frames to ensure single-modal coherency, but without adding any linkage to the LiDAR map $\mathcal{L}^\mathbb{G}$ undermines the multi-modal coherency. Hence, based on the SfM framework, an effective way is to link Visual-SfM points to their corresponding LiDAR points, and thus provide a strong depth prior to the triangulation, especially for those landmarks established with narrow baseline (e.g. landmarks on the road or curb while vehicles are driving straight-forwardly). Moreover, photometric factors based on 2D input and ray-casted images are able to further improve the multi-modal coherency according to our experiments, because pixel-level correspondences and gradients operating on the grayscale and the intensity, analogously from depth map contours used for texture-mapping~\cite{zhou2014colormapopt}, are directly operating on raw input channels between different sensors.
%
\begin{table}[t]
\centering
\fontsize{8pt}{9.6pt}\selectfont
\setlength{\tabcolsep}{5pt}
\caption{Quantitative metrics for pose estimation. We choose the reprojection error (in pixel) as a common SfM metric~\cite{scho2016sfm} to evaluate coherency between camera frames, and the NID-loss~\cite{pascoe2015robust} to evaluate coherency between camera frames and the LiDAR map.}
\begin{tabular}{l|cccc}
\toprule
Metric & Init. $\mathbf{T}_{\mathbb{C}_i}^\mathbb{G}$ & Opt. $\mathbf{f}^{1}$ & Opt. $\mathbf{f}^{1-2}$ & Opt. $\mathbf{f}^{1-3}$ \\
\midrule
Reproj. Err. $\downarrow$ &  7.959 & \cccf{1.342} & \ccct{1.344} & \cccs{1.343} \\
\midrule
NID-Loss  $\downarrow$   &  \ccct{5.32} & 10.52 & \cccs{4.47} & \cccf{4.10} \\
\bottomrule
\end{tabular}

\label{tab:sfm}
\end{table}
%
\begin{figure}[t]
\includegraphics[width=\linewidth]{sec/images/SfM.pdf}
\caption{Factors used in our cross-modal pose optimization framework, and we visualize LiDAR-camera alignment quality through an alpha-blending of the colorized intensity map onto its corresponding camera frame. {We refer readers to our supplementary video for the data alignment quality of our LiDAR map and multiple camera frames.}}
\label{fig:sfm}
\end{figure}
%