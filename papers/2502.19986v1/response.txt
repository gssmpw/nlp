\section{Related Work}
GNNAutoScale (GAS) **Zhang, "GNNAutoScale: Graph Neural Network Training on a Single Node"** partitions the graph and performs training on each of the partitions. To handle cross-partition links, historical embeddings are stored. Instead of loading and computing the full computation graph, GAS stays within the current partition and loads the historical embeddings of out-of-partition nodes to avoid information loss. This reduces training time and memory consumption. However, historical information can be stale since historical embeddings of one-hop nodes are not updated during the GNN training on the target partition, and approximation errors can occur. We distinguish ourselves by focusing on improving the historical embeddings in the GAS model to mitigate information loss. 

Another approach that partitions the graph is Cluster-GCN **Chen, "Cluster-GCN: A Graph Convolutional Network for Clustering"** ____ Within the partitions, sampling is performed and the GNN is trained. Due to the limitation of message passing to the current minibatch, potentially useful information outside the current partition is lost. Similar to ClusterGCN, GraphSAINT **Zeng, "GraphSAINT: A Sampling-Based Framework for Scalable Training of Graph Neural Networks"** ____ also first clusters the graph to avoid neighborhood explosion and then forms minibatches inside the partitions. Information across partitions is ignored, leading to a potential decrease of the models performance. 

DistDGLv2 **Wang, "DistDGLv2: An Efficient Synchronous Training Approach for Graph Neural Networks"** ____ employs an efficient GNN training method based on a synchronous training approach with an asynchronous minibatch generation pipeline. Resource usage is optimized, but the loss function does not consider node embeddings across different partitions. 

Besides a mini-batch training method which supports a high degree of parallelism, ByteGNN **Li, "ByteGNN: A Graph Partitioning Method for Efficient GNN Training"** ____ proposes a graph partitioning method adapted to GNN workloads. Depending on the target nodes, the partitioning is adapted to include the k-hop neighborhood of a node. This minimizes data movement and communication. Links across partitions are not considered in the loss function. Therefore, training performance could be affected. 

Betty **Zhang, "Betty: A Graph Partitioning Method with Redundancy-Embedded Graphs"** ____ introduces redundancy-embedded graph (REG) partitioning and memory-aware partitioning. Redundancy is mitigated, and load balance across partitions is improved. Further, the authors move from mini-batch training to micro-batch training. For each micro-batch, partial gradients are calculated which are accumulated to update the weights of the full model. In this way, memory is reduced while bypassing the loss of information when using partitioning and mini-batch training, but information is still lost due to links across micro-batches. 

**Wang, "Grouped Reversible Graph Neural Networks"** ____ focus on training deep GNNs. The authors adopt grouped reversible GNNs which partitions tensors of initial features in groups and employs reversible residual connections. This network only stores final outputs and thus saves memory. While their approach provides fixed memory requirements irrespective of the number of layers, our approach provides fixed memory requirements irrespective of the layer width. Therefore, our approach is complementary and can be used in combination.

**Sun, "REST: Refreshing Stored Embeddings to Mitigate Feature Staleness"** ____ introduce the REST method to address feature staleness by highlighting the importance of refreshing stored embeddings more frequently to align with model updates. Their approach involves performing a forward pass with batches formed according to a certain split, followed by a forward and backward pass with another split. 
In contrast, we propose a novel refinement process that performs multiple forward passes on all batches with an unchanged given split, ensuring that all historical embeddings are updated relative to a consistent state of the network parameters. This is followed by a backward pass designed specifically to enhance the robustness of historical embedding approximations. Additionally, we are the first to establish a connection between this approach and the well-known Waveform Relaxation method for solving ODEs. Furthermore, we provide an in-depth analysis of the gradient discrepancy issue in the GAS method, offering new theoretical insights that extend the understanding of feature staleness mitigation.