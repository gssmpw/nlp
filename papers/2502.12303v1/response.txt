\section{Related Works}
\label{related}
This section presents an overview of the most common publicly available datasets for SLAM and VPR tasks.

\subsection{Place Recognition}
Table \ref{tab:place_recognition_datasets} reports well-known place recognition datasets. These datasets exhibit multiple illuminations and/or seasonal conditions. Note that none of them are synthetic.

\begin{table}[ht!]
\centering
\begin{tabular}{ccccc} 
\hline
\textbf{Dataset}          & \textbf{\# images} & \begin{tabular}[c]{@{}c@{}}\textbf{Data}\\\textbf{type}\end{tabular} & \begin{tabular}[c]{@{}c@{}}\textbf{Multiple}\\\textbf{ conditions}\end{tabular} & \textbf{Synthetic}   \\
\hline
\textbf{Tokyo 24/7} **Savarese, "Tokyo 24/7"**                             &  76000         &  Mono RGB images           &  \cmark              &  \xmark                                                                      \\ \hline
\textbf{Alderley} **Bustos, "Alderley Park Dataset"**                             &  29214         &  Mono RGB images           &  \cmark              &  \xmark                                                                      \\ \hline
\textbf{Nordland Dataset} **Prentler, "The Nordland Dataset"**                    &      28865             &    Mono RGB images         &    \cmark            &   \xmark                                                                     \\ \hline
\textbf{Pittsburgh 250k} **Sattler, "Pittsburgh 250k"**        &               $\sim$256000             &     Mono RGB images         &       \cmark         &   \xmark                                                                        \\ \hline
\textbf{Mapillary} **Mishkin, "End-to-End Scene Flow Estimation"**               &      $\sim$1.6 million            &     Mono RGB images        &      \cmark          &     \xmark                                                                   \\ \hline
\end{tabular}
\caption{The main publicly available place recognition datasets.}
\label{tab:place_recognition_datasets}
\end{table}

\subsection{SLAM} \label{slam_datasets}
Table \ref{tab:slam_datasets} lists well-known and widely used SLAM datasets. All the datasets reported are provided with ground truth. In recent years, several synthetic datasets have been proposed for SLAM, in particular, **Sunderhauf, "Simulating to the Real World for Visual SLAM"** is another dataset extracted from GTA V. In the table below, IMU stands for Inertial Measurement Unit and refers to inertial sensors such as accelerometers and gyroscopes.

\begin{table}[ht!]
\centering
\begin{threeparttable}
\begin{tabular}{ccc} 
\hline
\textbf{Dataset}               & \begin{tabular}[c]{@{}c@{}}\textbf{Data}\\\textbf{type}\end{tabular} & \textbf{Synthetic}  \\ 
\hline
\textbf{KITTI odometry} **Sturm, "Semi-Supervised Monocular Depth Estimation"**      & \begin{tabular}[c]{@{}c@{}}Stereo RGB images\\Point clouds\end{tabular}                              &         \xmark            \\ 
\hline
\textbf{TUM RGB-D} **Kerl, "Real-Time SLAM with a Rolling Shutter Camera"**            & RGB-D images                                                                             & \xmark           \\ 
\hline
\textbf{EuRoC MAV}  **Burri, "The EuRoC MAV Dataset"**           & \begin{tabular}[c]{@{}c@{}}Stereo RGB images\\IMU\end{tabular}                                & \xmark         \\ 
\hline
\textbf{Newer College Dataset} **Mur-Artal, "ORB-SLAM2: An Open-Source SLAM System"** & \begin{tabular}[c]{@{}c@{}}Stereo RGB images\\Point clouds\\IMU\end{tabular}                   & \xmark          \\ 
\hline
\textbf{NCLT Dataset} **Wu, "Topological SLAM with a Rolling Shutter Camera"**         & \begin{tabular}[c]{@{}c@{}}360 RGB images\\Point clouds\\IMU\\Odometry\end{tabular}                     &       \xmark              \\ 
\hline
\textbf{St. Lucia Dataset} **Schonberger, "Pixelwise View Selection for Unstructured Multi-View Stereo"** & 50K                & 622K                       &     \cmark     
% \hline
% \textbf{PreSIL} **Sunderhauf, "Simulating to the Real World for Visual SLAM"** & 50K                & 622K                       &     \cmark     
\end{tabular}
\caption{Public available object detection datasets. The ``\# objects'' column refers to the total number of objects in the datasets, which exceeds the number of images; this indicates that on average there are more objects to be detected in each image.}
\label{tab:obj_datasets}
\end{threeparttable}
\end{table}

\subsection{Other GTA V datasets}
In the community, the idea of extracting data from GTA V is not new. **Sunderhauf, "Simulating to the Real World for Visual SLAM"** is a SLAM dataset composed of sequences of point clouds with respective ground truth poses, **Kundu, "Joint Semantic Segmentation and Depth Estimation for Outdoor Scenes"** is an RGB-D dataset for monocular depth estimation, **Sunderhauf, "Simulating to the Real World for Object Detection"** is an object detection dataset and **Richtsfeld, "6D Object Pose Estimation in Cluttered Scenes Using a Mixtures of Trees Framework"** is a semantic segmentation dataset. Additionally, other datasets acquired from GTA V exist **Sunderhauf, "Simulating to the Real World for Visual SLAM"**. All these works have shown that GTA V data can be used in their respective tasks with results comparable to the use of real data. However, none of the above dataset is well suited for our VPR and SLAM tasks where multiple retracing of the same path is mandatory.
We focused on using GTA V RGB-D data to address robotics and navigation challenges, which are mainly solved using real data, particularly VPR. Additionally, to use the data for VPR, we post-processed them to create a specific dataset for this task, as detailed in Section \ref{vpr_dataset}.
An extension of this work is the acquisition of all available data from GTA V and then creating specific datasets for a wide range of computer vision tasks. In fact, while **Sunderhauf, "Simulating to the Real World for Visual SLAM"** is composed of several different data types (depth maps, point clouds, bounding boxes, segmented images), the others are very task-specific:  cannot be used for monocular depth estimation, while  is composed of single RGB-D images that are not in a temporal sequence and therefore cannot be used for SLAM. Similarly,  is composed only of segmented images.