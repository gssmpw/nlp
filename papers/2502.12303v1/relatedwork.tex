\section{Related Works}
\label{related}
This section presents an overview of the most common publicly available datasets for SLAM and VPR tasks.

\subsection{Place Recognition}
Table \ref{tab:place_recognition_datasets} reports well-known place recognition datasets. These datasets exhibit multiple illuminations and/or seasonal conditions. Note that none of them are synthetic.

\begin{table}[ht!]
\centering
\begin{tabular}{ccccc} 
\hline
\textbf{Dataset}          & \textbf{\# images} & \begin{tabular}[c]{@{}c@{}}\textbf{Data}\\\textbf{type}\end{tabular} & \begin{tabular}[c]{@{}c@{}}\textbf{Multiple}\\\textbf{ conditions}\end{tabular} & \textbf{Synthetic}   \\
\hline
\textbf{Tokyo 24/7} \cite{tokyo}                             &  76000         &  Mono RGB images           &  \cmark              &  \xmark                                                                      \\ \hline
\textbf{Alderley} \cite{alderley}                             &  29214         &  Mono RGB images           &  \cmark              &  \xmark                                                                      \\ \hline
\textbf{Nordland Dataset} \cite{nordland}                    &      28865             &    Mono RGB images         &    \cmark            &   \xmark                                                                     \\ \hline
\textbf{Pittsburgh 250k} \cite{pittsburgh}        &               $\sim$256000             &     Mono RGB images         &       \cmark         &   \xmark                                                                        \\ \hline
\textbf{Mapillary} \cite{mapillary}               &      $\sim$1.6 million            &     Mono RGB images        &      \cmark          &     \xmark                                                                   \\ \hline
\end{tabular}
\caption{The main publicly available place recognition datasets.}
\label{tab:place_recognition_datasets}
\end{table}

\subsection{SLAM} \label{slam_datasets}
Table \ref{tab:slam_datasets} lists well-known and widely used SLAM datasets. All the datasets reported are provided with ground truth. In recent years, several synthetic datasets have been proposed for SLAM, in particular, GTASynth \cite{gtasynth} is another dataset extracted from GTA V. In the table below, IMU stands for Inertial Measurement Unit and refers to inertial sensors such as accelerometers and gyroscopes.

\begin{table}[ht!]
\centering
\begin{threeparttable}
\begin{tabular}{ccc} 
\hline
\textbf{Dataset}               & \begin{tabular}[c]{@{}c@{}}\textbf{Data}\\\textbf{type}\end{tabular} & \textbf{Synthetic}  \\ 
\hline
\textbf{KITTI odometry} \cite{kitti}      & \begin{tabular}[c]{@{}c@{}}Stereo RGB images\\Point clouds\end{tabular}                              &         \xmark            \\ 
\hline
\textbf{TUM RGB-D} \cite{tum}            & RGB-D images                                                                             & \xmark           \\ 
\hline
\textbf{EuRoC MAV}  \cite{euroc}           & \begin{tabular}[c]{@{}c@{}}Stereo RGB images\\IMU\end{tabular}                                & \xmark         \\ 
\hline
\textbf{Newer College Dataset} \cite{newer} & \begin{tabular}[c]{@{}c@{}}Stereo RGB images\\Point clouds\\IMU\end{tabular}                   & \xmark          \\ 
\hline
\textbf{NCLT Dataset} \cite{nctl}         & \begin{tabular}[c]{@{}c@{}}360 RGB images\\Point clouds\\IMU\\Odometry\end{tabular}                     &       \xmark              \\ 
\hline
\textbf{St. Lucia Dataset} \cite{stlucia}         & \begin{tabular}[c]{@{}c@{}}Mono RGB images\\GPS\end{tabular}                     &       \xmark              \\ 
\hline
\textbf{USyd Campus Dataset}\tnote{1} \cite{usyd}         & \begin{tabular}[c]{@{}c@{}}Mono RGB images\\Point clouds\\GPS\\Odometry\end{tabular}                     &       \xmark              \\ 
\hline
\textbf{Virtual KITTI 2} \cite{virtualkitti2}      & RGB-D images                                                                                       &               \cmark      \\ 
\hline
\textbf{ICL-NUIM} \cite{iclnuim}             & RGB-D images                                                                                       & \cmark                   \\ 
\hline
\textbf{Redwood SLAM} \cite{redwood}         & \begin{tabular}[c]{@{}c@{}}RGB-D images\\Point clouds\end{tabular}                                & \cmark                   \\ 
\hline
\textbf{GTASynth}  \cite{gtasynth}            & Point clouds                                                                                   & \cmark                   \\
\hline

\end{tabular}
\begin{tablenotes}
\item[1] USyd does not have a ground truth.
\end{tablenotes}
\end{threeparttable}
\caption{The main publicly available SLAM datasets.}
\label{tab:slam_datasets}
\end{table}


% \subsection{Depth estimation} \label{depth_datasets}
% Table \ref{tab:depth_estimation_datasets} reports datasets commonly used for depth estimation. While most of these datasets are specifically designed for this task, in general, any dataset composed of RGB-D images, as well as SLAM datasets, can be used for this task \cite{monodepth}. Note that HRSD \cite{hrsd} is another synthetic dataset collected using GTA V.
% \begin{table}[ht!]
% \centering
% \begin{tabular}{ccc} 
% \hline
% \textbf{Dataset}      & \begin{tabular}[c]{@{}c@{}}\textbf{Data}\\\textbf{type}\end{tabular}                                                              & \textbf{Synthetic}  \\ 
% \hline
% \textbf{KITTI depth} \cite{kitti}  & \begin{tabular}[c]{@{}c@{}}RGB images\\Sparse depth maps\end{tabular}                    & \xmark                   \\ 
% \hline
% \textbf{NYU Depth V2} \cite{nyudepth} & RGB-D images                                                                       & \xmark           \\ 
% \hline
% \textbf{ETH3D} \cite{eth3d}       & \begin{tabular}[c]{@{}c@{}}RGB-D images\\Point clouds\end{tabular}                  & \xmark           \\ 
% \hline
% \textbf{DIODE}  \cite{diode}      & RGB-D images                                                                      & \xmark           \\ 
% \hline
% \textbf{MegaDepth}  \cite{megadepth}  & RGB-D images                                                                                 &  \xmark                    \\ 
% \hline
% \textbf{SYNTHIA}  \cite{synthia}    & RGB-D images                                                                                  & \cmark                   \\ 
% \hline
% \textbf{HRSD}   \cite{hrsd}      & RGB-D images                                                                                & \cmark                   \\
% \hline
% \end{tabular}
% \caption{Public available datasets used for depth estimation.}
% \label{tab:depth_estimation_datasets}
% \end{table}

% \subsection{Depth completion}
% As for depth estimation, any dataset consisting of RGB-D images and point clouds can be used for depth completion. Therefore, we do not provide any tables here, as the datasets typically used are already shown in Sections \ref{slam_datasets} and \ref{depth_datasets}.

% \subsection{Object Detection}
% Table \ref{tab:obj_datasets} reports the most common datasets used for object detection. In this context, real datasets are still the most widely used, but we also include PreSIL \cite{presil}, a synthetic dataset acquired from GTA V. 

% % \usepackage{tabularray}
% \begin{table}[ht!]
% \centering
% \begin{tblr}{
%   cells = {c},
%   hlines,
% }
% \textbf{Dataset}     & \textbf{\# images} & \textbf{\# categories} & \textbf{\# objects} & \textbf{Synthetic} \\
% \textbf{COCO}\cite{coco}        & 330K              & 80                    & 1.5M                      & \xmark           \\
% \textbf{PASCAL VOC}\cite{pascal}  & 11K               & 20                    & 27K                       & \xmark          \\
% \textbf{ImageNet}\cite{imagenet}    & 476K               & 200                   &   534K                        & \xmark           \\
% \textbf{Objects365}\cite{objects365}  & 600K\textbf{}     & 365                   & 30M                       & \xmark           \\
% \textbf{Open Images}\cite{openimages} & 9M                & 600                   & 16M                       &     \xmark                \\
% \textbf{PreSIL}\cite{presil} & 50K                & 12                   & 622K                       &     \cmark     
% \end{tblr}
% \caption{Public available object detection datasets. The ``\# objects'' column refers to the total number of objects in the datasets, which exceeds the number of images; this indicates that on average there are more objects to be detected in each image.}
% \label{tab:obj_datasets}
% \end{table}

% \subsection{Semantic Segmentation}
% Table \ref{tab:seg_datasets} lists well-known datasets used for semantic segmentation. As for object detection, real datasets are still the most widely used, but we also include GTA5 \cite{gta_seg}, a synthetic dataset for semantic segmentation acquired using GTA V.

% \begin{table}[ht!]
% \centering
% \begin{tabular}{cccc} 
% \hline
% \textbf{Dataset}          & \textbf{\# images} & \textbf{\# categories} & \textbf{Synthetic}  \\ 
% \hline
% \textbf{Cityscapes}\cite{cityscapes}       & 25K               & 30                    & \xmark           \\ 
% \hline
% \textbf{ADE20K}\cite{ade20k}           & 20K               & 150                   & \xmark            \\ 
% \hline
% \textbf{Mapillary Vistas}\cite{mapillaryvistas} & 25K               & 124                   & \xmark            \\ 
% \hline
% \textbf{SUN RGB-D}\cite{sunrgdb}        & 10K\textbf{}      & 37                    & \xmark           \\ 
% \hline
% \textbf{LVIS}\cite{lvis}             & 164K              & 1200                  &    \xmark                  \\ \hline
% \textbf{GTA5}\cite{gta_seg}             & 25K              & 19                  &    \cmark  \\
% \hline
% \end{tabular}
% \caption{Public available semantic segmentation datasets.}
% \label{tab:seg_datasets}
% \end{table}

\subsection{Other GTA V datasets}
In the community, the idea of extracting data from GTA V is not new. GTASynth \cite{gtasynth} is a SLAM dataset composed of sequences of point clouds with respective ground truth poses, HRSD \cite{hrsd} is an RGB-D dataset for monocular depth estimation, PreSIL \cite{presil} is an object detection dataset and GTA5 \cite{gta_seg} is a semantic segmentation dataset. Additionally, other datasets acquired from GTA V exist \cite{gta_drone, gta_people}. All these works have shown that GTA V data can be used in their respective tasks with results comparable to the use of real data. However, none of the above dataset is well suited for our VPR and SLAM tasks where multiple retracing of the same path is mandatory.
We focused on using GTA V RGB-D data to address robotics and navigation challenges, which are mainly solved using real data, particularly VPR. Additionally, to use the data for VPR, we post-processed them to create a specific dataset for this task, as detailed in Section \ref{vpr_dataset}.
An extension of this work is the acquisition of all available data from GTA V and then creating specific datasets for a wide range of computer vision tasks. In fact, while PreSIL is composed of several different data types (depth maps, point clouds, bounding boxes, segmented images), the others are very task-specific: GTASynth is composed solely of point clouds, and cannot be used for monocular depth estimation, while HRSD is composed of single RGB-D images that are not in a temporal sequence and therefore cannot be used for SLAM. Similarly, GTA5 is composed only of segmented images.