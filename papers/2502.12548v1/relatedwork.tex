\section{Related Work}
\subsection{GNNFF models}
\textbf{Graph Neural Network based Force Field}: Given an atomic system with $n$ atoms, each atom has an atomic number and position $ \bm r_i\in\mathbb{R}^{n \times 3}$, and Force Field (FF) models learn from the interactions of atoms to predict the system potential energy $E$ and force $ \bm {F_i}$ for each atom. Typically, the forces on each particle are obtained as $  \bm {F_i} = -\partial E / \partial  \bm {r}_i $ \citep{fu_forces_2022}. In GNNFFs, atoms are considered to be nodes and the interaction or bonds between two atoms are considered to be edges. An edge is built when the distance of two atoms is less than a predefined cutoff threshold. GNNFF learns knowledge from atoms' spatial information like distances, angles between atom pairs, and dihedral of atom groups. The accuracy of FF model is usually evaluated by Energy MAE (EMAE) and Force MAE (FMAE) per-atom, with the unit of meV/atom and meV/Å.

\textbf{NequIP} \citep{batzner_e3-equivariant_2022} is an E(3)-equivariant Message Passing Network employing E(3)-equivariant convolutions for interactions of geometric tensors. It achieves state-of-the-art accuracy on a challenging and diverse set of molecules and materials with remarkable data efficiency.
\textbf{Allegro} \citep{musaelian_learning_2023} is a local interaction based-FF model. It predicts the energy as a function of the final edge embedding rather than the node embeddings. All the pairwise energies are summed to obtain the total energy of the system. Allegro shows high accuracy and great scalability with its local interaction architecture.
\textbf{GemNet} \citep{gasteiger_gemnet_2021} is a Message Passing Network based on directed edge embeddings and two-hop message passing. GemNet and its variants shows high accuracy in OC20 \citep{ocp_dataset_2020} leaderboard but lower scalability than Allegro.  


\begin{figure}[t]
  \centering
  \includegraphics[width=0.48\textwidth]{fig/a2.png}
  \caption{Feature correlation and MD simulation stability of GNNFF models of different layers}
  \label{fig_corr}
  \vspace*{-1.0\baselineskip}
\end{figure}


\subsection{MD simulation stability}

Recently, the stability of MD simulation when using MLFF/GNNFF models to describe atomic interaction is actively discussed in the field. MLFFs may produce unstable prediction result when the learned force field is not robust to the under-sampled data distribution \citep{orlov_nanoscale_2015, rogacheva_non-stoichiometry_2006, dubey_stoichiometric_2019, kostenko_vacancy_2021}. The simulation can enter nonphysical states that would never occur in a realistic simulation and eventually MD simulation will end up as system crash as shown in the left side of Figure \ref{fig_nonphysical}. 

Accordingly, some methods have been proposed to relieve the MD simulation instability issue in MLFF area in recent years. For example, active learning \citep{vandermause_--fly_2020, xie_bayesian_2021, vandermause_active_2022, xie_uncertainty-aware_2023} can be used to improve the accuracy and stability of the MLFF model by increasing the quality and diversity of the training dataset. When the uncertainty in model predictions exceeds a specified threshold, the model is retrained using newly generated training data. However, generating new training data needs additional DFT calculation, which is time and resource consuming. Therefore, even though many methods have emerged to accelerate the active learning process, retraining MLFF model with active learning is still costly and less scalable.

There are already some existing methods dealing with the generalization issue in neural networks, including dropout \citep{Dropout}, weight decay \citep{weightdecay}, early stopping \citep{earlystop}, flatter loss landscapes \citep{keskar_large-batch_2017, dziugaite_computing_2017, jiang_fantastic_2020, vita_data_2023}, etc. But only flatten loss landscapes are disccussed in improving stability of GNNFFs.
\citeauthor{vita_data_2023} used loss entropy to quantify the flatness of the loss landscape, and they used different training parameters to increase the loss entropy and thus improve the MD stability.
\citeauthor{foret_sharpness-aware_2021} approximated the minimization of sharpness by Sharpness-Aware Minimization (SAM), and successfully improved the out-of-sample error of the model on the MLFF model.
\citeauthor{ibayashi_allegro-legato_2023} improves MD stability of Allegro model by SAM in training process. The results show that it can expand the simulation time of Allegro model. However, these methods come at the cost of some training overhead and accuracy loss. For example, Allegro-Legato increases the training time of Allegro model by 75\%, and decreases FMAE from 10.7 mev/Å to 11.6 mev/Å.

Besides, since the traditional metric (FMAE/EMAE) cannot measure MLFFs' stability in real MD simulation scenarios, some other metrics are proposed to quantify MD stability: Time to Failure \citep{ibayashi_allegro-legato_2023}, Wright's Factor (WF), and Jensen-Shannon Divergence (JSD) \citep{rajak_ex-nnqmd_2021} of Radial Distribution Function (RDF) analysis. Time to Failure roughly measures stability with simulation time but misses other important physical metrics in MD simulation. WF and JSD need additional reference data generated from DFT which requires lots of computation resources in simulation.

\begin{figure}[t]
  \centering
  \includegraphics[width=0.45\textwidth]{fig/corr_sindex.png}
  \caption{Feature correlation and MD simulation stability of GNNFF models with our method}
  \label{fig_corr_sindex}
  \vspace*{-1.0\baselineskip}
\end{figure}