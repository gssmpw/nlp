
@inproceedings{minjia_zhang_accelerating_2020,
	title = {Accelerating {Training} of {Transformer}-{Based} {Language} {Models} with {Progressive} {Layer} {Dropping}},
	isbn = {arXiv:2010.13369},
	doi = {arXiv:2010.13369},
	author = {{Minjia Zhang} and {Yuxiong He}},
	month = oct,
	year = {2020},
}

@inproceedings{jing_zhao_fine-_2022,
	title = {Fine- and {Coarse}-{Granularity} {Hybrid} {Self}-{Attention} for {Efficient} {BERT}},
	doi = {arXiv:2203.09055.},
	author = {Jing Zhao},
	year = {2022},
}

@inproceedings{hou_l_token_2022,
	title = {Token {Dropping} for {Efficient} {BERT} {Pretraining}},
	doi = {arXiv:2203.13240},
	author = {{Hou L.} and {Pang R. Y.} and {Zhou T.} and {Wu Y.} and {Song X.} and {Zhou D.}},
	year = {2022},
}


@inproceedings{mostafa_dehghani_universal_2019,
	title = {Universal {Transformers}},
	author = {{Mostafa Dehghani}},
	year = {2019},
}

@inproceedings{zhen_dong_hawq_2019,
	title = {{HAWQ}: {Hessian} {AWare} {Quantization} of {Neu}-ral {Networks} with {Mixed}-{Precision}},
	doi = {arXiv:1905.03696},
	author = {{Zhen Dong}},
	year = {2019},
}

@inproceedings{zhen_dong_hawq-v2_2019,
	title = {{HAWQ}-{V2}: {Hessian} {Aware} trace-{Weighted} {Quantization} of {Neural} {Networks}},
	doi = {arXiv:1911.03852.},
	author = {{Zhen Dong}},
	year = {2019},
}

@inproceedings{yao_z_hawqv3_2021,
	title = {{HAWQV3}: {Dyadic} {Neural} {Network} {Quantization}},
	doi = {arXiv:2011.10680},
	author = {{Yao Z.} and {Dong Z.} and {Zheng Z.} and {Gholami A.} and {Yu J.} and {Tan E.} and {Wang L.} and {Huang Q.}},
	year = {2021},
}

@inproceedings{arici_t_mlim_2021,
	title = {{MLIM}: {Vision}-and-{Language} {Model} {Pre}-training with {Masked} {Language} and {Image} {Modeling}},
	doi = {arXiv:2109.12178},
	author = {{Arici T.} and {Seyfioglu M. S.} and {Neiman T.} and {Xu Y.} and {Train S.} and {Chilimbi T.} and {Zeng B.}},
	year = {2021},
}

@inproceedings{changlin_li_automated_2022,
	title = {Automated {Progressive} {Learning} for {Efficient} {Training} of {Vision} {Transformers}},
	author = {{Changlin Li}},
	year = {2022},
}

@article{jialei_wang_utilizing_2019,
	title = {Utilizing {Second} {Order} {Information} in {Mini}-batch {Stochastic} {Variance} {Reduced} {Proximal} {Iterations}},
	volume = {20},
	number = {42},
	journal = {Journal of Machine Learning Research},
	author = {{Jialei Wang}},
	year = {2019},
}


@article{geonu_kim_benchmark_2023,
	title = {Benchmark of {Machine} {Learning} {Force} {Fields} for {Semiconductor} {Simulations}: {Datasets}, {Metrics}, and {Comparative} {Analysis}},
	journal = {Advances in Neural Information Processing Systems},
	author = {Kim, Geonu and Na, Byunggook and Kim, Gunhee and Cho, Hyuntae and Kang, Seungjin and Lee, Hee Sun and Choi, Saerom and Kim, Heejae and Lee, Seungwon and Kim, Yongdeok},
	year = {2023},
}

@article{kim_review_2022,
	title = {Review of {Semiconductor} {Flash} {Memory} {Devices} for {Material} and {Process} {Issues}},
	volume = {35},
	doi = {10.1002/adma.202200659},
	journal = {Advanced Materials},
	author = {Kim, Seung and Yong, Soo and Kim, Whayoung and Kang, Sukin and Park, Hyeonwoo and Yoon, Kyung and Sheen, Dong and Lee, Seho and Hwang, Cheol},
	month = may,
	year = {2022},
}

@incollection{redaelli_historical_2022,
	series = {Woodhead {Publishing} {Series} in {Electronic} and {Optical} {Materials}},
	title = {Historical review of semiconductor memories},
	isbn = {978-0-12-820758-1},
	url = {https://www.sciencedirect.com/science/article/pii/B9780128207581000042},
	abstract = {In this chapter, the main memory technology from a historical point of view will be presented. Following a chronological line, after the pioneering work exploiting the CMOS technology to build useful memories, first DRAM and then NAND technology will be overviewed, covering at least three decades of the semiconductor memory evolution. Then the attention will be moved to the development history of new technologies, called emerging memory, and on the new era of the 3D integration able to extend further the life of the industry-standard memory. The last paragraph will be dedicated to an outlook on the future developments and applications.},
	booktitle = {Semiconductor {Memories} and {Systems}},
	publisher = {Woodhead Publishing},
	author = {Bez, Roberto and Fantini, Paolo and Pirovano, Agostino},
	editor = {Redaelli, Andrea and Pellizzer, Fabio},
	year = {2022},
	doi = {https://doi.org/10.1016/B978-0-12-820758-1.00004-2},
	keywords = {3D integration, 3DxP, DRAM, Flash, NAND, Nonvolatile memory, NOR, Phase change memory, Spin-torque transfer MRAM, Universal memory},
	pages = {1--26},
}

@inproceedings{orji_metrology_2019,
	title = {Metrology requirements for next generation of semiconductor devices},
	url = {https://tsapps.nist.gov/publication/get_pdf.cfm?pub_id=927435},
	language = {en},
	publisher = {Frontiers of Characterization and Metrology for Nanoelectronics (FCMN), Monterrey, CA},
	author = {Orji, Ndubuisi},
	month = apr,
	year = {2019},
}

@article{nakamae_electron_2021,
	title = {Electron microscopy in semiconductor inspection},
	volume = {32},
	doi = {10.1088/1361-6501/abd96d},
	journal = {Measurement Science and Technology},
	author = {Nakamae, Koji},
	month = mar,
	year = {2021},
}

@article{zhou_impact_2019,
	title = {Impact of {Molecular} {Dynamics} {Simulations} on {Research} and {Development} of {Semiconductor} {Materials}},
	volume = {4},
	doi = {10.1557/adv.2019.360},
	journal = {MRS Advances},
	author = {Zhou, X.},
	month = sep,
	year = {2019},
	pages = {1--18},
}

@article{thompson_lammps_2022,
	title = {{LAMMPS} - a flexible simulation tool for particle-based materials modeling at the atomic, meso, and continuum scales},
	volume = {271},
	issn = {0010-4655},
	url = {https://www.sciencedirect.com/science/article/pii/S0010465521002836},
	doi = {https://doi.org/10.1016/j.cpc.2021.108171},
	abstract = {Since the classical molecular dynamics simulator LAMMPS was released as an open source code in 2004, it has become a widely-used tool for particle-based modeling of materials at length scales ranging from atomic to mesoscale to continuum. Reasons for its popularity are that it provides a wide variety of particle interaction models for different materials, that it runs on any platform from a single CPU core to the largest supercomputers with accelerators, and that it gives users control over simulation details, either via the input script or by adding code for new interatomic potentials, constraints, diagnostics, or other features needed for their models. As a result, hundreds of people have contributed new capabilities to LAMMPS and it has grown from fifty thousand lines of code in 2004 to a million lines today. In this paper several of the fundamental algorithms used in LAMMPS are described along with the design strategies which have made it flexible for both users and developers. We also highlight some capabilities recently added to the code which were enabled by this flexibility, including dynamic load balancing, on-the-fly visualization, magnetic spin dynamics models, and quantum-accuracy machine learning interatomic potentials. Program Summary Program Title: Large-scale Atomic/Molecular Massively Parallel Simulator (LAMMPS) CPC Library link to program files: https://doi.org/10.17632/cxbxs9btsv.1 Developer's repository link: https://github.com/lammps/lammps Licensing provisions: GPLv2 Programming language: C++, Python, C, Fortran Supplementary material: https://www.lammps.org Nature of problem: Many science applications in physics, chemistry, materials science, and related fields require parallel, scalable, and efficient generation of long, stable classical particle dynamics trajectories. Within this common problem definition, there lies a great diversity of use cases, distinguished by different particle interaction models, external constraints, as well as timescales and lengthscales ranging from atomic to mesoscale to macroscopic. Solution method: The LAMMPS code uses parallel spatial decomposition, distributed neighbor lists, and parallel FFTs for long-range Coulombic interactions [1]. The time integration algorithm is based on the Størmer-Verlet symplectic integrator [2], which provides better stability than higher-order non-symplectic methods. In addition, LAMMPS supports a wide range of interatomic potentials, constraints, diagnostics, software interfaces, and pre- and post-processing features. Additional comments including restrictions and unusual features: This paper serves as the definitive reference for the LAMMPS code. References [1]S. Plimpton, Fast parallel algorithms for short-range molecular dynamics. J. Comp. Phys. 117 (1995) 1–19.[2]L. Verlet, Computer experiments on classical fluids: I. Thermodynamical properties of Lennard–Jones molecules, Phys. Rev. 159 (1967) 98–103.},
	journal = {Computer Physics Communications},
	author = {Thompson, Aidan P. and Aktulga, H. Metin and Berger, Richard and Bolintineanu, Dan S. and Brown, W. Michael and Crozier, Paul S. and Veld, Pieter J. in 't and Kohlmeyer, Axel and Moore, Stan G. and Nguyen, Trung Dac and Shan, Ray and Stevens, Mark J. and Tranchida, Julien and Trott, Christian and Plimpton, Steven J.},
	year = {2022},
	keywords = {LAMMPS, Materials modeling, Molecular dynamics, Parallel algorithms},
	pages = {108171},
}

@article{alder_studies_1959,
	title = {Studies in {Molecular} {Dynamics}. {I}. {General} {Method}},
	volume = {31},
	doi = {10.1063/1.1730376},
	number = {2},
	journal = {{\textbackslash}jcp},
	author = {Alder, B. J. and Wainwright, T. E.},
	month = aug,
	year = {1959},
	pages = {459--466},
}

@article{rahman_correlations_1964,
	title = {Correlations in the {Motion} of {Atoms} in {Liquid} {Argon}},
	volume = {136},
	url = {https://api.semanticscholar.org/CorpusID:120778447},
	journal = {Physical Review},
	author = {Rahman, Aneesur},
	year = {1964},
	pages = {405--411},
}

@incollection{frenkel_chapter_2002,
	address = {San Diego},
	edition = {Second Edition},
	title = {Chapter 4 - {Molecular} {Dynamics} {Simulations}},
	isbn = {978-0-12-267351-1},
	url = {https://www.sciencedirect.com/science/article/pii/B9780122673511500067},
	booktitle = {Understanding {Molecular} {Simulation} ({Second} {Edition})},
	publisher = {Academic Press},
	author = {Frenkel, Daan and Smit, Berend},
	editor = {Frenkel, Daan and Smit, Berend},
	year = {2002},
	doi = {https://doi.org/10.1016/B978-012267351-1/50006-7},
	pages = {63--107},
}

@article{van_mourik_density_2014,
	title = {Density functional theory across chemistry, physics and biology {Introduction}},
	volume = {372},
	doi = {10.1098/rsta.2012.0488},
	journal = {Philosophical transactions. Series A, Mathematical, physical, and engineering sciences},
	author = {van Mourik, Tanja and Bhl, Michael and Gaigeot, Marie-Pierre},
	month = mar,
	year = {2014},
	pages = {20120488},
}

@article{iftimie_ab_2005,
	title = {Ab initio molecular dynamics: {Concepts}, recent developments, and future trends},
	volume = {102},
	issn = {0027-8424},
	doi = {10.1073/pnas.0500193102},
	abstract = {The methodology of ab initio molecular dynamics, wherein finite-temperature dynamical trajectories are generated by using forces computed "on the fly" from electronic structure calculations, has had a profound influence in modern theoretical research. Ab initio molecular dynamics allows chemical processes in condensed phases to be studied in an accurate and unbiased manner, leading to new paradigms in the elucidation of microscopic mechanisms, rationalization of experimental data, and testable predictions of new phenomena. The purpose of this work is to give a brief introduction to the technique and to review several important recent developments in the field. Several illustrative examples showing the power of the technique have been chosen. Perspectives on future directions in the field also will be given.},
	language = {English (US)},
	number = {19},
	journal = {Proceedings of the National Academy of Sciences of the United States of America},
	author = {Iftimie, Radu and Minary, Peter and Tuckerman, Mark E.},
	month = may,
	year = {2005},
	note = {Publisher: National Academy of Sciences},
	pages = {6654--6659},
}

@article{lee_simple-nn_2019,
	title = {{SIMPLE}-{NN}: {An} efficient package for training and executing neural-network interatomic potentials},
	volume = {242},
	issn = {0010-4655},
	url = {https://www.sciencedirect.com/science/article/pii/S0010465519301298},
	doi = {https://doi.org/10.1016/j.cpc.2019.04.014},
	abstract = {The molecular dynamics (MD) simulation is a favored method in materials science for understanding and predicting material properties from atomistic motions. In classical MD simulations, the interaction between atoms is described by an empirical interatomic potential, so the reliability of the simulation hinges on the accuracy of the underlying potential. Recently, machine learning (ML) based interatomic potentials are gaining attention as they can reproduce potential energy surfaces (PES) of ab initio calculations, with a much lower computational cost. Therefore, an efficient code for training ML potentials and inferencing PES in new configurations would widen the application range of MD simulations. Here, we announce an open-source package, SNU Interatomic Machine-learning PotentiaL packagE-version Neural Network (SIMPLE-NN) that generates and utilizes the ML potential based on the artificial neural network with the Behler–Parrinello type symmetry function as descriptors for the chemical environments. SIMPLE-NN uses the Atomic Simulation Environment (ASE) package and Google Tensorflow for high expandability and efficient training, and also supports the in-house code for quasi-Newton method. Notably, the package features a weighting scheme based on the Gaussian density function (GDF), which significantly improves accuracy and reliability of ML potentials by resolving sampling bias that exists in typical training sets. For MD simulations, SIMPLE-NN interfaces with the LAMMPS package. We demonstrate the performance and usage of SIMPLE-NN with examples of SiO2. Program summary Program Title: SIMPLE-NN Program Files doi: http://dx.doi.org/10.17632/pjv2yr7pvr.1 Licensing provisions: GPLv3 Programming language: Python/C++ Nature of problem: Inferencing the potential energy surface for the given system with accuracy comparable to ab initio methods but with much lower computational costs. Solution method: Calculate descriptor vectors that encode local chemical environment. High-dimensional neural network is used to predict the total energy from the descriptor vectors. The trained neural network can be used for molecular dynamics simulations.},
	journal = {Computer Physics Communications},
	author = {Lee, Kyuhyun and Yoo, Dongsun and Jeong, Wonseok and Han, Seungwu},
	year = {2019},
	keywords = {Molecular dynamics, Machine learning potential, Neural network, Potential energy surface},
	pages = {95--103},
}

@article{blank_neural_1995,
	title = {Neural network models of potential energy surfaces},
	volume = {103},
	issn = {0021-9606},
	url = {https://doi.org/10.1063/1.469597},
	doi = {10.1063/1.469597},
	number = {10},
	journal = {The Journal of Chemical Physics},
	author = {Blank, Thomas B. and Brown, Steven D. and Calhoun, August W. and Doren, Douglas J.},
	month = sep,
	year = {1995},
	note = {\_eprint: https://pubs.aip.org/aip/jcp/article-pdf/103/10/4129/19201895/4129\_1\_online.pdf},
	pages = {4129--4137},
}

@article{behler_generalized_2007,
	title = {Generalized neural-network representation of high-dimensional potential-energy surfaces.},
	volume = {98 14},
	url = {https://api.semanticscholar.org/CorpusID:37065565},
	journal = {Physical review letters},
	author = {Behler, Jörg and Parrinello, Michele},
	year = {2007},
	pages = {146401},
}

@article{zhang_deep_2018,
	title = {Deep {Potential} {Molecular} {Dynamics}: {A} {Scalable} {Model} with the {Accuracy} of {Quantum} {Mechanics}},
	volume = {120},
	doi = {10.1103/PhysRevLett.120.143001},
	number = {14},
	journal = {{\textbackslash}prl},
	author = {Zhang, Linfeng and Han, Jiequn and Wang, Han and Car, Roberto and E, Weinan},
	month = apr,
	year = {2018},
	note = {\_eprint: 1707.09571},
	keywords = {Computer Science - Machine Learning, Physics - Chemical Physics, Physics - Computational Physics},
	pages = {143001},
}

@article{batzner_e3-equivariant_2022,
	title = {E(3)-equivariant graph neural networks for data-efficient and accurate interatomic potentials},
	volume = {13},
	doi = {10.1038/s41467-022-29939-5},
	journal = {Nature Communications},
	author = {Batzner, Simon and Musaelian, Albert and Sun, Lixin and Geiger, Mario and Mailoa, Jonathan and Kornbluth, Mordechai and Molinari, Nicola and Smidt, Tess and Kozinsky, Boris},
	month = may,
	year = {2022},
}

@article{stocker_how_2022,
	title = {How {Robust} are {Modern} {Graph} {Neural} {Network} {Potentials} in {Long} and {Hot} {Molecular} {Dynamics} {Simulations}?},
	volume = {3},
	doi = {10.1088/2632-2153/ac9955},
	journal = {Machine Learning: Science and Technology},
	author = {Stocker, Sina and Gasteiger, Johannes and Becker, Florian and Günnemann, Stephan and Margraf, Johannes},
	month = nov,
	year = {2022},
}

@inproceedings{rajak_ex-nnqmd_2021,
	title = {Ex-{NNQMD}: {Extreme}-{Scale} {Neural} {Network} {Quantum} {Molecular} {Dynamics}},
	doi = {10.1109/IPDPSW52791.2021.00145},
	author = {Rajak, Pankaj and Aditya, Anikeya and Fukushima, Shogo and Kalia, Rajiv and Linker, Thomas and Liu, Kuang and Luo, Ye and Nakano, Aiichiro and Nomura, Ken-ichi and Shimamura, Kohei and Shimojo, Fuyuki and Vashishta, Priya},
	month = jun,
	year = {2021},
	pages = {943--946},
}

@article{orlov_nanoscale_2015,
	title = {Nanoscale {Potential} {Fluctuation} in {Non}-{Stoichiometric} {Hafnium} {Suboxides}},
	volume = {69},
	doi = {10.1149/06905.0237ecst},
	journal = {ECS Transactions},
	author = {Orlov, Oleg and Krasnikov, Gennady and Gritsenko, V.A. and Kruchinin, V. and Perevalov, T.V. and Vladimir, Aliev and Islamov, Damir and Prosvirin, I.},
	month = oct,
	year = {2015},
	pages = {237--241},
}

@incollection{dubey_stoichiometric_2019,
	title = {Stoichiometric and {Nonstoichiometric} {Compounds}},
	isbn = {978-1-78985-451-0},
	author = {Dubey, Paras and Kaurav, Netram},
	month = nov,
	year = {2019},
	doi = {10.5772/intechopen.89402},
}

@article{kostenko_vacancy_2021,
	title = {Vacancy ordered phases of nonstoichiometric hafnium carbide from evolutionary crystal structure predictions},
	volume = {891},
	doi = {10.1016/j.jallcom.2021.162063},
	journal = {Journal of Alloys and Compounds},
	author = {Kostenko, Maksim and Jingyu, Li and Zeng, Zhi and Zhang, Yongsheng and Sharf, Sergey and Gusev, Aleksandr and Lukoyanov, Alexey},
	month = sep,
	year = {2021},
	pages = {162063},
}

@article{xie_bayesian_2021,
	title = {Bayesian force fields from active learning for simulation of inter-dimensional transformation of stanene},
	volume = {7},
	doi = {10.1038/s41524-021-00510-y},
	journal = {npj Computational Materials},
	author = {Xie, Yu and Vandermause, Jonathan and Sun, Lixin and Cepellotti, Andrea and Kozinsky, Boris},
	month = mar,
	year = {2021},
	pages = {40},
}

@article{vandermause_active_2022,
	title = {Active learning of reactive {Bayesian} force fields applied to heterogeneous catalysis dynamics of {H}/{Pt}},
	volume = {13},
	doi = {10.1038/s41467-022-32294-0},
	journal = {Nature Communications},
	author = {Vandermause, Jonathan and Xie, Yu and Lim, Jin Soo and Owen, Cameron and Kozinsky, Boris},
	month = sep,
	year = {2022},
}

@inproceedings{jin_feature_2022,
	address = {New York, NY, USA},
	series = {{KDD} '22},
	title = {Feature {Overcorrelation} in {Deep} {Graph} {Neural} {Networks}: {A} {New} {Perspective}},
	isbn = {978-1-4503-9385-0},
	url = {https://doi.org/10.1145/3534678.3539445},
	doi = {10.1145/3534678.3539445},
	booktitle = {Proceedings of the 28th {ACM} {SIGKDD} {Conference} on {Knowledge} {Discovery} and {Data} {Mining}},
	publisher = {Association for Computing Machinery},
	author = {Jin, Wei and Liu, Xiaorui and Ma, Yao and Aggarwal, Charu and Tang, Jiliang},
	year = {2022},
	note = {event-place: Washington DC, USA},
	keywords = {deep models, graph neural networks, semi-supervised learning},
	pages = {709--719},
}

@incollection{benesty_pearson_2009,
	title = {Pearson {Correlation} {Coefficient}},
	volume = {2},
	isbn = {978-3-642-00295-3},
	booktitle = {Noise {Reduction} in {Speech} {Processing}},
	author = {Benesty, Jacob and Chen, Jingdong and Huang, Yiteng and Cohen, Israel},
	month = apr,
	year = {2009},
	doi = {10.1007/978-3-642-00296-0_5},
	pages = {1--4},
}

@article{tan_single-model_2023,
	title = {Single-model uncertainty quantification in neural network potentials does not consistently outperform model ensembles},
	volume = {9},
	doi = {10.1038/s41524-023-01180-8},
	journal = {npj Computational Mathematics},
	author = {Tan, Aik Rui and Urata, Shingo and Goldman, Samuel and Dietschreit, Johannes C. B. and Gómez-Bombarelli, Rafael},
	month = jan,
	year = {2023},
	note = {\_eprint: 2305.01754},
	keywords = {Computer Science - Machine Learning, Physics - Chemical Physics},
	pages = {225},
}

@article{mailoa_fast_2019,
	title = {A fast neural network approach for direct covariant forces prediction in complex multi-element extended systems},
	volume = {1},
	doi = {10.1038/s42256-019-0098-0},
	journal = {Nature Machine Intelligence},
	author = {Mailoa, Jonathan and Kornbluth, Mordechai and Batzner, Simon and Samsonidze, Georgy and Lam, Stephen and Vandermause, Jonathan and Ablitt, Chris and Molinari, Nicola and Kozinsky, Boris},
	month = oct,
	year = {2019},
	pages = {471--479},
}

@inproceedings{gu_molecular_2022,
	series = {Journal of {Physics} {Conference} {Series}},
	title = {Molecular {Dynamic} {Simulation} in {Organic} {Semiconductor} {Investigation}},
	volume = {2194},
	doi = {10.1088/1742-6596/2194/1/012024},
	booktitle = {Journal of {Physics} {Conference} {Series}},
	publisher = {IOP},
	author = {Gu, Junwen},
	month = feb,
	year = {2022},
	pages = {012024},
}

@article{grimley_neutron_1990,
	title = {Neutron scattering from vitreous silica {IV}. {Time}-of-flight diffraction},
	volume = {119},
	issn = {0022-3093},
	url = {https://www.sciencedirect.com/science/article/pii/002230939090240M},
	doi = {https://doi.org/10.1016/0022-3093(90)90240-M},
	abstract = {The optimum configuration for a neutron time-of-flight diffractometer to study amorphous solids is discussed and it is concluded that measurements should be performed at relatively low scattering angles using a cold moderator. Good reciprocal space resolution is also important. Time-of-flight data for vitreous silica are combined with twin-axis data from part II (P.A.V. Johnson et al., J. Non-Cryst. Solids 58 (1983) 109) to give an accurate composite interference function and excellent real space resolution on Fourier transformation. The maximum scattering vector, Qmax, is 45.2 Å−1. A peak fit to the resulting correlation function yields a mean SiO bond length of 1.608±0.004 Å.},
	number = {1},
	journal = {Journal of Non-Crystalline Solids},
	author = {Grimley, David I. and Wright, Adrian C. and Sinclair, Roger N.},
	year = {1990},
	pages = {49--64},
}

@article{chmiela_towards_2018,
	title = {Towards {Exact} {Molecular} {Dynamics} {Simulations} with {Machine}-{Learned} {Force} {Fields}},
	volume = {9},
	doi = {10.1038/s41467-018-06169-2},
	journal = {Nature Communications},
	author = {Chmiela, Stefan and Sauceda, Huziel E. and Müller, Klaus-Robert and Tkatchenko, Alexandre},
	month = sep,
	year = {2018},
}

@article{rogacheva_non-stoichiometry_2006,
	title = {Non-stoichiometry and properties of {SnTeCd} semiconducting phase of variable composition},
	volume = {203},
	doi = {10.1002/pssa.200669654},
	journal = {Physica Status Solidi Applied Research},
	author = {Rogacheva, Elena and Nashchekina, Olga},
	month = sep,
	year = {2006},
	pages = {2856--2860},
}

@article{anstine_machine_2023,
	title = {Machine {Learning} {Interatomic} {Potentials} and {Long}-{Range} {Physics}},
	volume = {127},
	doi = {10.1021/acs.jpca.2c06778},
	journal = {The journal of physical chemistry. A},
	author = {Anstine, Dylan and Isayev, Olexandr},
	month = feb,
	year = {2023},
}

@article{chen_measuring_2020,
	title = {Measuring and {Relieving} the {Over}-{Smoothing} {Problem} for {Graph} {Neural} {Networks} from the {Topological} {View}},
	volume = {34},
	doi = {10.1609/aaai.v34i04.5747},
	journal = {Proceedings of the AAAI Conference on Artificial Intelligence},
	author = {Chen, Deli and Lin, Yankai and Li, Wei and Li, Peng and Zhou, Jie and Sun, Xu},
	month = apr,
	year = {2020},
	pages = {3438--3445},
}

@article{park_accurate_2021,
	title = {Accurate and scalable graph neural network force field and molecular dynamics with direct force architecture},
	volume = {7},
	doi = {10.1038/s41524-021-00543-3},
	journal = {npj Computational Materials},
	author = {Park, Cheol Woo and Kornbluth, Mordechai and Vandermause, Jonathan and Wolverton, Chris and Kozinsky, Boris and Mailoa, Jonathan},
	month = dec,
	year = {2021},
}

@incollection{ibayashi_allegro-legato_2023,
	title = {Allegro-{Legato}: {Scalable}, {Fast}, and {Robust} {Neural}-{Network} {Quantum} {Molecular} {Dynamics} via {Sharpness}-{Aware} {Minimization}},
	isbn = {978-3-031-32040-8},
	author = {Ibayashi, Hikaru and Razakh, Taufeq and Yang, Liqiu and Linker, Thomas and Olguin, Marco and Hattori, Shinnosuke and Luo, Ye and Kalia, Rajiv and Nakano, Aiichiro and Nomura, Ken-ichi and Vashishta, Priya},
	month = may,
	year = {2023},
	doi = {10.1007/978-3-031-32041-5_12},
	pages = {223--239},
}

@inproceedings{dziugaite_computing_2017,
	title = {Computing {Nonvacuous} {Generalization} {Bounds} for {Deep} ({Stochastic}) {Neural} {Networks} with {Many} {More} {Parameters} than {Training} {Data}},
	booktitle = {Proceedings of the 33rd {Annual} {Conference} on {Uncertainty} in {Artificial} {Intelligence} ({UAI})},
	author = {Dziugaite, Gintare Karolina and Roy, Daniel M.},
	year = {2017},
	note = {\_eprint: 1703.11008},
}

@inproceedings{keskar_large-batch_2017,
	title = {On {Large}-{Batch} {Training} for {Deep} {Learning}: {Generalization} {Gap} and {Sharp} {Minima}},
	url = {https://openreview.net/forum?id=H1oyRlYgg},
	booktitle = {International {Conference} on {Learning} {Representations}},
	author = {Keskar, Nitish Shirish and Mudigere, Dheevatsa and Nocedal, Jorge and Smelyanskiy, Mikhail and Tang, Ping Tak Peter},
	year = {2017},
}

@article{xie_uncertainty-aware_2023,
	title = {Uncertainty-aware molecular dynamics from {Bayesian} active learning for phase transformations and thermal transport in {SiC}},
	volume = {9},
	doi = {10.1038/s41524-023-00988-8},
	journal = {npj Computational Materials},
	author = {Xie, Yu and Vandermause, Jonathan and Ramakers, Senja and Nakib, Hana and Johansson, Anders and Kozinsky, Boris},
	month = mar,
	year = {2023},
	pages = {36},
}

@article{fu_simulate_2023,
	title = {Simulate {Time}-integrated {Coarse}-grained {Molecular} {Dynamics} with {Multi}-scale {Graph} {Networks}},
	issn = {2835-8856},
	url = {https://openreview.net/forum?id=y8RZoPjEUl},
	journal = {Transactions on Machine Learning Research},
	author = {Fu, Xiang and Xie, Tian and Rebello, Nathan J. and Olsen, Bradley and Jaakkola, Tommi S.},
	year = {2023},
}

@inproceedings{foret_sharpness-aware_2021,
	title = {Sharpness-aware {Minimization} for {Efficiently} {Improving} {Generalization}},
	url = {https://openreview.net/forum?id=6Tm1mposlrM},
	booktitle = {International {Conference} on {Learning} {Representations}},
	author = {Foret, Pierre and Kleiner, Ariel and Mobahi, Hossein and Neyshabur, Behnam},
	year = {2021},
}

@inproceedings{loshchilov_sgdr_2017,
	title = {{SGDR}: {Stochastic} {Gradient} {Descent} with {Warm} {Restarts}},
	url = {https://openreview.net/forum?id=Skq89Scxx},
	booktitle = {International {Conference} on {Learning} {Representations}},
	author = {Loshchilov, Ilya and Hutter, Frank},
	year = {2017},
}

@inproceedings{schutt_schnet_2017,
	address = {Red Hook, NY, USA},
	series = {{NIPS}'17},
	title = {{SchNet}: a continuous-filter convolutional neural network for modeling quantum interactions},
	isbn = {978-1-5108-6096-4},
	abstract = {Deep learning has the potential to revolutionize quantum chemistry as it is ideally suited to learn representations for structured data and speed up the exploration of chemical space. While convolutional neural networks have proven to be the first choice for images, audio and video data, the atoms in molecules are not restricted to a grid. Instead, their precise locations contain essential physical information, that would get lost if discretized. Thus, we propose to use continuous-filter convolutional layers to be able to model local correlations without requiring the data to lie on a grid. We apply those layers in SchNet: a novel deep learning architecture modeling quantum interactions in molecules. We obtain a joint model for the total energy and interatomic forces that follows fundamental quantum-chemical principles. Our architecture achieves state-of-the-art performance for benchmarks of equilibrium molecules and molecular dynamics trajectories. Finally, we introduce a more challenging benchmark with chemical and structural variations that suggests the path for further work.},
	booktitle = {Proceedings of the 31st {International} {Conference} on {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates Inc.},
	author = {Schütt, K. T. and Kindermans, P.-J. and Sauceda, H. E. and Chmiela, S. and Tkatchenko, A. and Müller, K.-R.},
	year = {2017},
	note = {event-place: Long Beach, California, USA},
	pages = {992--1002},
}

@inproceedings{jia_pushing_2020,
	series = {{SC} '20},
	title = {Pushing the limit of molecular dynamics with ab initio accuracy to 100 million atoms with machine learning},
	isbn = {978-1-72819-998-6},
	booktitle = {Proceedings of the {International} {Conference} for {High} {Performance} {Computing}, {Networking}, {Storage} and {Analysis}},
	publisher = {IEEE Press},
	author = {Jia, Weile and Wang, Han and Chen, Mohan and Lu, Denghui and Lin, Lin and Car, Roberto and E, Weinan and Zhang, Linfeng},
	year = {2020},
	note = {Place: Atlanta, Georgia},
	keywords = {ab initio molecular dynamics, deep potential molecular dynamics, GPU, heterogeneous architecture, machine learning, summit},
}

@inproceedings{zhao_pairnorm_2020,
	title = {{PairNorm}: {Tackling} {Oversmoothing} in \{{GNN}\}s},
	url = {https://openreview.net/forum?id=rkecl1rtwB},
	booktitle = {International {Conference} on {Learning} {Representations}},
	author = {Zhao, Lingxiao and Akoglu, Leman},
	year = {2020},
}

@article{li_deeper_2018,
	title = {Deeper {Insights} {Into} {Graph} {Convolutional} {Networks} for {Semi}-{Supervised} {Learning}},
	volume = {32},
	doi = {10.1609/aaai.v32i1.11604},
	journal = {Proceedings of the AAAI Conference on Artificial Intelligence},
	author = {Li, Qimai and Han, Zhichao and Wu, Xiao-Ming},
	month = jan,
	year = {2018},
}

@inproceedings{gasteiger_gemnet_2021,
	title = {{GemNet}: {Universal} {Directional} {Graph} {Neural} {Networks} for {Molecules}},
	booktitle = {Conference on {Neural} {Information} {Processing} {Systems} ({NeurIPS})},
	author = {Gasteiger, Johannes and Becker, Florian and Günnemann, Stephan},
	year = {2021},
}

@inproceedings{fu_forces_2022,
	title = {Forces are not {Enough}: {Benchmark} and {Critical} {Evaluation} for {Machine} {Learning} {Force} {Fields} with {Molecular} {Simulations}},
	url = {https://openreview.net/forum?id=8d2gTDcRMyx},
	booktitle = {{NeurIPS} 2022 {AI} for {Science}: {Progress} and {Promises}},
	author = {Fu, Xiang and Wu, Zhenghao and Wang, Wujie and Xie, Tian and Keten, Sinan and Gomez-Bombarelli, Rafael and Jaakkola, Tommi S.},
	year = {2022},
}

@inproceedings{jiang_fantastic_2020,
	title = {Fantastic {Generalization} {Measures} and {Where} to {Find} {Them}},
	url = {https://openreview.net/forum?id=SJgIPJBFvH},
	booktitle = {International {Conference} on {Learning} {Representations}},
	author = {Jiang, Yiding and Neyshabur, Behnam and Mobahi, Hossein and Krishnan, Dilip and Bengio, Samy},
	year = {2020},
}

@inproceedings{bihani_egraffbench_2023,
	title = {{EGraFFBench}: {Evaluation} of {Equivariant} {Graph} {Neural} {Network} {Force} {Fields} for {Atomistic} {Simulations}},
	url = {https://openreview.net/forum?id=SeXGn7MeUr},
	booktitle = {{AI} for {Accelerated} {Materials} {Design} - {NeurIPS} 2023 {Workshop}},
	author = {Bihani, Vaibhav and Pratiush, Utkarsh and Mannan, Sajid and Du, Tao and Chen, Zhimin and Miret, Santiago and Micoulaut, Matthieu and Smedskjaer, Morten M. and Ranu, Sayan and Krishnan, N. M. Anoop},
	year = {2023},
}

@article{vita_data_2023,
	title = {Data efficiency and extrapolation trends in neural network interatomic potentials},
	volume = {4},
	doi = {10.1088/2632-2153/acf115},
	journal = {Machine Learning: Science and Technology},
	author = {Vita, Joshua and Schwalbe-Koda, Daniel},
	month = aug,
	year = {2023},
}

@article{kocer_neural_2022,
	title = {Neural {Network} {Potentials}: {A} {Concise} {Overview} of {Methods}},
	volume = {73},
	doi = {10.1146/annurev-physchem-082720-034254},
	journal = {Annual Review of Physical Chemistry},
	author = {Kocer, Emir and Tsz Wai, Ko and Behler, Jörg},
	month = apr,
	year = {2022},
}

@article{musaelian_learning_2023,
	title = {Learning local equivariant representations for large-scale atomistic dynamics},
	volume = {14},
	doi = {10.1038/s41467-023-36329-y},
	journal = {Nature Communications},
	author = {Musaelian, Albert and Batzner, Simon and Johansson, Anders and Sun, Lixin and Owen, Cameron and Kornbluth, Mordechai and Kozinsky, Boris},
	month = feb,
	year = {2023},
	pages = {579},
}

@article{vandermause_--fly_2020,
	title = {On-the-fly active learning of interpretable {Bayesian} force fields for atomistic rare events},
	volume = {6},
	doi = {10.1038/s41524-020-0283-z},
	journal = {npj Computational Materials},
	author = {Vandermause, Jonathan and Torrisi, Steven and Batzner, Simon and Xie, Yu and Sun, Lixin and Kolpak, Alexie and Kozinsky, Boris},
	month = dec,
	year = {2020},
}

@article{ocp_dataset_2020,
    author = {Chanussot, Lowik and Das, Abhishek and Goyal, Siddharth and Lavril, Thibaut and Shuaibi, Muhammed and Riviere, Morgane and Tran, Kevin and Heras-Domingo, Javier and Ho, Caleb and Hu, Weihua and Palizhati, Aini and Sriram, Anuroop and Wood, Brandon and Yoon, Junwoong and Parikh, Devi and Zitnick, C. Lawrence and Ulissi, Zachary},
    title = {Open Catalyst 2020 (OC20) Dataset and Community Challenges},
    journal = {ACS Catalysis},
    year = {2021},
    doi = {10.1021/acscatal.0c04525},
}

@incollection{Rogacheva12,
author = {Elena Rogacheva},
title = {Nonstoichiometry and Properties of SnTe Semiconductor Phase of Variable Composition},
booktitle = {Stoichiometry and Materials Science},
publisher = {IntechOpen},
address = {Rijeka},
year = {2012},
editor = {Alessio Innocenti and Norlida Kamarulzaman},
chapter = {5},
doi = {10.5772/34579},
url = {https://doi.org/10.5772/34579}
}

@article{Dropout,
author = {Srivastava, Nitish and Hinton, Geoffrey and Krizhevsky, Alex and Sutskever, Ilya and Salakhutdinov, Ruslan},
title = {Dropout: a simple way to prevent neural networks from overfitting},
year = {2014},
issue_date = {January 2014},
publisher = {JMLR.org},
volume = {15},
number = {1},
issn = {1532-4435},
abstract = {Deep neural nets with a large number of parameters are very powerful machine learning systems. However, overfitting is a serious problem in such networks. Large networks are also slow to use, making it difficult to deal with overfitting by combining the predictions of many different large neural nets at test time. Dropout is a technique for addressing this problem. The key idea is to randomly drop units (along with their connections) from the neural network during training. This prevents units from co-adapting too much. During training, dropout samples from an exponential number of different "thinned" networks. At test time, it is easy to approximate the effect of averaging the predictions of all these thinned networks by simply using a single unthinned network that has smaller weights. This significantly reduces overfitting and gives major improvements over other regularization methods. We show that dropout improves the performance of neural networks on supervised learning tasks in vision, speech recognition, document classification and computational biology, obtaining state-of-the-art results on many benchmark data sets.},
journal = {J. Mach. Learn. Res.},
month = {jan},
pages = {1929–1958},
numpages = {30},
keywords = {deep learning, model combination, neural networks, regularization}
}

@inproceedings{weightdecay,
author = {Krogh, Anders and Hertz, John A.},
title = {A simple weight decay can improve generalization},
year = {1991},
isbn = {1558602224},
publisher = {Morgan Kaufmann Publishers Inc.},
address = {San Francisco, CA, USA},
abstract = {It has been observed in numerical simulations that a weight decay can improve generalization in a feed-forward neural network. This paper explains why. It is proven that a weight decay has two effects in a linear network. First, it suppresses any irrelevant components of the weight vector by choosing the smallest vector that solves the learning problem. Second, if the size is chosen right, a weight decay can suppress some of the effects of static noise on the targets, which improves generalization quite a lot. It is then shown how to extend these results to networks with hidden layers and non-linear units. Finally the theory is confirmed by some numerical simulations using the data from NetTalk.},
booktitle = {Proceedings of the 4th International Conference on Neural Information Processing Systems},
pages = {950–957},
numpages = {8},
location = {Denver, Colorado},
series = {NIPS'91}
}

@article{earlystop,
author = {Yao, Yuan and Rosasco, Lorenzo and Caponnetto, Andrea},
year = {2007},
month = {08},
pages = {289-315},
title = {On Early Stopping in Gradient Descent Learning},
volume = {26},
journal = {Constructive Approximation},
doi = {10.1007/s00365-006-0663-2}
}
