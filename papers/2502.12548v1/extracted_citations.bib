@article{Dropout,
author = {Srivastava, Nitish and Hinton, Geoffrey and Krizhevsky, Alex and Sutskever, Ilya and Salakhutdinov, Ruslan},
title = {Dropout: a simple way to prevent neural networks from overfitting},
year = {2014},
issue_date = {January 2014},
publisher = {JMLR.org},
volume = {15},
number = {1},
issn = {1532-4435},
abstract = {Deep neural nets with a large number of parameters are very powerful machine learning systems. However, overfitting is a serious problem in such networks. Large networks are also slow to use, making it difficult to deal with overfitting by combining the predictions of many different large neural nets at test time. Dropout is a technique for addressing this problem. The key idea is to randomly drop units (along with their connections) from the neural network during training. This prevents units from co-adapting too much. During training, dropout samples from an exponential number of different "thinned" networks. At test time, it is easy to approximate the effect of averaging the predictions of all these thinned networks by simply using a single unthinned network that has smaller weights. This significantly reduces overfitting and gives major improvements over other regularization methods. We show that dropout improves the performance of neural networks on supervised learning tasks in vision, speech recognition, document classification and computational biology, obtaining state-of-the-art results on many benchmark data sets.},
journal = {J. Mach. Learn. Res.},
month = {jan},
pages = {1929–1958},
numpages = {30},
keywords = {deep learning, model combination, neural networks, regularization}
}

@article{batzner_e3-equivariant_2022,
	title = {E(3)-equivariant graph neural networks for data-efficient and accurate interatomic potentials},
	volume = {13},
	doi = {10.1038/s41467-022-29939-5},
	journal = {Nature Communications},
	author = {Batzner, Simon and Musaelian, Albert and Sun, Lixin and Geiger, Mario and Mailoa, Jonathan and Kornbluth, Mordechai and Molinari, Nicola and Smidt, Tess and Kozinsky, Boris},
	month = may,
	year = {2022},
}

@incollection{dubey_stoichiometric_2019,
	title = {Stoichiometric and {Nonstoichiometric} {Compounds}},
	isbn = {978-1-78985-451-0},
	author = {Dubey, Paras and Kaurav, Netram},
	month = nov,
	year = {2019},
	doi = {10.5772/intechopen.89402},
}

@inproceedings{dziugaite_computing_2017,
	title = {Computing {Nonvacuous} {Generalization} {Bounds} for {Deep} ({Stochastic}) {Neural} {Networks} with {Many} {More} {Parameters} than {Training} {Data}},
	booktitle = {Proceedings of the 33rd {Annual} {Conference} on {Uncertainty} in {Artificial} {Intelligence} ({UAI})},
	author = {Dziugaite, Gintare Karolina and Roy, Daniel M.},
	year = {2017},
	note = {\_eprint: 1703.11008},
}

@article{earlystop,
author = {Yao, Yuan and Rosasco, Lorenzo and Caponnetto, Andrea},
year = {2007},
month = {08},
pages = {289-315},
title = {On Early Stopping in Gradient Descent Learning},
volume = {26},
journal = {Constructive Approximation},
doi = {10.1007/s00365-006-0663-2}
}

@inproceedings{fu_forces_2022,
	title = {Forces are not {Enough}: {Benchmark} and {Critical} {Evaluation} for {Machine} {Learning} {Force} {Fields} with {Molecular} {Simulations}},
	url = {https://openreview.net/forum?id=8d2gTDcRMyx},
	booktitle = {{NeurIPS} 2022 {AI} for {Science}: {Progress} and {Promises}},
	author = {Fu, Xiang and Wu, Zhenghao and Wang, Wujie and Xie, Tian and Keten, Sinan and Gomez-Bombarelli, Rafael and Jaakkola, Tommi S.},
	year = {2022},
}

@inproceedings{gasteiger_gemnet_2021,
	title = {{GemNet}: {Universal} {Directional} {Graph} {Neural} {Networks} for {Molecules}},
	booktitle = {Conference on {Neural} {Information} {Processing} {Systems} ({NeurIPS})},
	author = {Gasteiger, Johannes and Becker, Florian and Günnemann, Stephan},
	year = {2021},
}

@incollection{ibayashi_allegro-legato_2023,
	title = {Allegro-{Legato}: {Scalable}, {Fast}, and {Robust} {Neural}-{Network} {Quantum} {Molecular} {Dynamics} via {Sharpness}-{Aware} {Minimization}},
	isbn = {978-3-031-32040-8},
	author = {Ibayashi, Hikaru and Razakh, Taufeq and Yang, Liqiu and Linker, Thomas and Olguin, Marco and Hattori, Shinnosuke and Luo, Ye and Kalia, Rajiv and Nakano, Aiichiro and Nomura, Ken-ichi and Vashishta, Priya},
	month = may,
	year = {2023},
	doi = {10.1007/978-3-031-32041-5_12},
	pages = {223--239},
}

@inproceedings{jiang_fantastic_2020,
	title = {Fantastic {Generalization} {Measures} and {Where} to {Find} {Them}},
	url = {https://openreview.net/forum?id=SJgIPJBFvH},
	booktitle = {International {Conference} on {Learning} {Representations}},
	author = {Jiang, Yiding and Neyshabur, Behnam and Mobahi, Hossein and Krishnan, Dilip and Bengio, Samy},
	year = {2020},
}

@inproceedings{keskar_large-batch_2017,
	title = {On {Large}-{Batch} {Training} for {Deep} {Learning}: {Generalization} {Gap} and {Sharp} {Minima}},
	url = {https://openreview.net/forum?id=H1oyRlYgg},
	booktitle = {International {Conference} on {Learning} {Representations}},
	author = {Keskar, Nitish Shirish and Mudigere, Dheevatsa and Nocedal, Jorge and Smelyanskiy, Mikhail and Tang, Ping Tak Peter},
	year = {2017},
}

@article{kostenko_vacancy_2021,
	title = {Vacancy ordered phases of nonstoichiometric hafnium carbide from evolutionary crystal structure predictions},
	volume = {891},
	doi = {10.1016/j.jallcom.2021.162063},
	journal = {Journal of Alloys and Compounds},
	author = {Kostenko, Maksim and Jingyu, Li and Zeng, Zhi and Zhang, Yongsheng and Sharf, Sergey and Gusev, Aleksandr and Lukoyanov, Alexey},
	month = sep,
	year = {2021},
	pages = {162063},
}

@article{musaelian_learning_2023,
	title = {Learning local equivariant representations for large-scale atomistic dynamics},
	volume = {14},
	doi = {10.1038/s41467-023-36329-y},
	journal = {Nature Communications},
	author = {Musaelian, Albert and Batzner, Simon and Johansson, Anders and Sun, Lixin and Owen, Cameron and Kornbluth, Mordechai and Kozinsky, Boris},
	month = feb,
	year = {2023},
	pages = {579},
}

@article{ocp_dataset_2020,
    author = {Chanussot, Lowik and Das, Abhishek and Goyal, Siddharth and Lavril, Thibaut and Shuaibi, Muhammed and Riviere, Morgane and Tran, Kevin and Heras-Domingo, Javier and Ho, Caleb and Hu, Weihua and Palizhati, Aini and Sriram, Anuroop and Wood, Brandon and Yoon, Junwoong and Parikh, Devi and Zitnick, C. Lawrence and Ulissi, Zachary},
    title = {Open Catalyst 2020 (OC20) Dataset and Community Challenges},
    journal = {ACS Catalysis},
    year = {2021},
    doi = {10.1021/acscatal.0c04525},
}

@article{orlov_nanoscale_2015,
	title = {Nanoscale {Potential} {Fluctuation} in {Non}-{Stoichiometric} {Hafnium} {Suboxides}},
	volume = {69},
	doi = {10.1149/06905.0237ecst},
	journal = {ECS Transactions},
	author = {Orlov, Oleg and Krasnikov, Gennady and Gritsenko, V.A. and Kruchinin, V. and Perevalov, T.V. and Vladimir, Aliev and Islamov, Damir and Prosvirin, I.},
	month = oct,
	year = {2015},
	pages = {237--241},
}

@inproceedings{rajak_ex-nnqmd_2021,
	title = {Ex-{NNQMD}: {Extreme}-{Scale} {Neural} {Network} {Quantum} {Molecular} {Dynamics}},
	doi = {10.1109/IPDPSW52791.2021.00145},
	author = {Rajak, Pankaj and Aditya, Anikeya and Fukushima, Shogo and Kalia, Rajiv and Linker, Thomas and Liu, Kuang and Luo, Ye and Nakano, Aiichiro and Nomura, Ken-ichi and Shimamura, Kohei and Shimojo, Fuyuki and Vashishta, Priya},
	month = jun,
	year = {2021},
	pages = {943--946},
}

@article{rogacheva_non-stoichiometry_2006,
	title = {Non-stoichiometry and properties of {SnTeCd} semiconducting phase of variable composition},
	volume = {203},
	doi = {10.1002/pssa.200669654},
	journal = {Physica Status Solidi Applied Research},
	author = {Rogacheva, Elena and Nashchekina, Olga},
	month = sep,
	year = {2006},
	pages = {2856--2860},
}

@article{vandermause_--fly_2020,
	title = {On-the-fly active learning of interpretable {Bayesian} force fields for atomistic rare events},
	volume = {6},
	doi = {10.1038/s41524-020-0283-z},
	journal = {npj Computational Materials},
	author = {Vandermause, Jonathan and Torrisi, Steven and Batzner, Simon and Xie, Yu and Sun, Lixin and Kolpak, Alexie and Kozinsky, Boris},
	month = dec,
	year = {2020},
}

@article{vandermause_active_2022,
	title = {Active learning of reactive {Bayesian} force fields applied to heterogeneous catalysis dynamics of {H}/{Pt}},
	volume = {13},
	doi = {10.1038/s41467-022-32294-0},
	journal = {Nature Communications},
	author = {Vandermause, Jonathan and Xie, Yu and Lim, Jin Soo and Owen, Cameron and Kozinsky, Boris},
	month = sep,
	year = {2022},
}

@article{vita_data_2023,
	title = {Data efficiency and extrapolation trends in neural network interatomic potentials},
	volume = {4},
	doi = {10.1088/2632-2153/acf115},
	journal = {Machine Learning: Science and Technology},
	author = {Vita, Joshua and Schwalbe-Koda, Daniel},
	month = aug,
	year = {2023},
}

@inproceedings{weightdecay,
author = {Krogh, Anders and Hertz, John A.},
title = {A simple weight decay can improve generalization},
year = {1991},
isbn = {1558602224},
publisher = {Morgan Kaufmann Publishers Inc.},
address = {San Francisco, CA, USA},
abstract = {It has been observed in numerical simulations that a weight decay can improve generalization in a feed-forward neural network. This paper explains why. It is proven that a weight decay has two effects in a linear network. First, it suppresses any irrelevant components of the weight vector by choosing the smallest vector that solves the learning problem. Second, if the size is chosen right, a weight decay can suppress some of the effects of static noise on the targets, which improves generalization quite a lot. It is then shown how to extend these results to networks with hidden layers and non-linear units. Finally the theory is confirmed by some numerical simulations using the data from NetTalk.},
booktitle = {Proceedings of the 4th International Conference on Neural Information Processing Systems},
pages = {950–957},
numpages = {8},
location = {Denver, Colorado},
series = {NIPS'91}
}

@article{xie_bayesian_2021,
	title = {Bayesian force fields from active learning for simulation of inter-dimensional transformation of stanene},
	volume = {7},
	doi = {10.1038/s41524-021-00510-y},
	journal = {npj Computational Materials},
	author = {Xie, Yu and Vandermause, Jonathan and Sun, Lixin and Cepellotti, Andrea and Kozinsky, Boris},
	month = mar,
	year = {2021},
	pages = {40},
}

@article{xie_uncertainty-aware_2023,
	title = {Uncertainty-aware molecular dynamics from {Bayesian} active learning for phase transformations and thermal transport in {SiC}},
	volume = {9},
	doi = {10.1038/s41524-023-00988-8},
	journal = {npj Computational Materials},
	author = {Xie, Yu and Vandermause, Jonathan and Ramakers, Senja and Nakib, Hana and Johansson, Anders and Kozinsky, Boris},
	month = mar,
	year = {2023},
	pages = {36},
}

