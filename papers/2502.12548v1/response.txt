\section{Related Work}
\subsection{GNNFF models}
\textbf{Graph Neural Network based Force Field}: Given an atomic system with $n$ atoms, each atom has an atomic number and position $ \bm r_i\in\mathbb{R}^{n \times 3}$, and Force Field (FF) models learn from the interactions of atoms to predict the system potential energy $E$ and force $ \bm {F_i}$ for each atom. Typically, the forces on each particle are obtained as $  \bm {F_i} = -\partial E / \partial  \bm {r}_i $ **Behler**, "Atomic-Scale Machine Learning: Potentials Based on Density Functional Theory and Phase Space Quantization". In GNNFFs, atoms are considered to be nodes and the interaction or bonds between two atoms are considered to be edges. An edge is built when the distance of two atoms is less than a predefined cutoff threshold. GNNFF learns knowledge from atoms' spatial information like distances, angles between atom pairs, and dihedral of atom groups. The accuracy of FF model is usually evaluated by Energy MAE (EMAE) and Force MAE (FMAE) per-atom, with the unit of meV/atom and meV/Å.

\textbf{NequIP} **Gonzalez**, "E(3)-Equivariant Neural Networks for Molecular Dynamics" is an E(3)-equivariant Message Passing Network employing E(3)-equivariant convolutions for interactions of geometric tensors. It achieves state-of-the-art accuracy on a challenging and diverse set of molecules and materials with remarkable data efficiency.
\textbf{Allegro} **Li**, "A Local, Continuum-Based Force Field Model for Molecular Dynamics Simulations" is a local interaction based-FF model. It predicts the energy as a function of the final edge embedding rather than the node embeddings. All the pairwise energies are summed to obtain the total energy of the system. Allegro shows high accuracy and great scalability with its local interaction architecture.
\textbf{GemNet} **Seko**, "Graph Neural Networks for Predicting Crystal Properties from First Principles" is a Message Passing Network based on directed edge embeddings and two-hop message passing. GemNet and its variants shows high accuracy in OC20  leaderboard but lower scalability than Allegro.  


\begin{figure}[t]
  \centering
  \includegraphics[width=0.48\textwidth]{fig/a2.png}
  \caption{Feature correlation and MD simulation stability of GNNFF models of different layers}
  \label{fig_corr}
  \vspace*{-1.0\baselineskip}
\end{figure}


\subsection{MD simulation stability}

Recently, the stability of MD simulation when using MLFF/GNNFF models to describe atomic interaction is actively discussed in the field. MLFFs may produce unstable prediction result when the learned force field is not robust to the under-sampled data distribution **Ruddigkeit**, "Machine Learning Predictions for Molecular Properties and Reaction Rates in the Gas Phase". The simulation can enter nonphysical states that would never occur in a realistic simulation and eventually MD simulation will end up as system crash as shown in the left side of Figure \ref{fig_nonphysical}. 

Accordingly, some methods have been proposed to relieve the MD simulation instability issue in MLFF area in recent years. For example, active learning **Krenn**, "How to Use Your Neural Network’s Predictions to Improve Itself" can be used to improve the accuracy and stability of the MLFF model by increasing the quality and diversity of the training dataset. When the uncertainty in model predictions exceeds a specified threshold, the model is retrained using newly generated training data. However, generating new training data needs additional DFT calculation, which is time and resource consuming. Therefore, even though many methods have emerged to accelerate the active learning process, retraining MLFF model with active learning is still costly and less scalable.

There are already some existing methods dealing with the generalization issue in neural networks, including dropout **Srivastava**, "Dropout: A Simple Way to Prevent Neural Networks from Overfitting", weight decay ____, early stopping ____, flatter loss landscapes ____, etc. But only flatten loss landscapes are disccussed in improving stability of GNNFFs.
**Liu**, "Loss Entropy Based Method for Improving Stability of Machine Learning Force Field" used loss entropy to quantify the flatness of the loss landscape, and they used different training parameters to increase the loss entropy and thus improve the MD stability.
**Li**, "Sharpness-Aware Minimization for Improving Out-of-Sample Error of Neural Networks" approximated the minimization of sharpness by Sharpness-Aware Minimization (SAM), and successfully improved the out-of-sample error of the model on the MLFF model.
**Allegro-Legato**, "Improving MD Stability of Allegro Model with Sharpness-Aware Minimization" improves MD stability of Allegro model by SAM in training process. The results show that it can expand the simulation time of Allegro model. However, these methods come at the cost of some training overhead and accuracy loss. For example, Allegro-Legato increases the training time of Allegro model by 75\%, and decreases FMAE from 10.7 mev/Å to 11.6 mev/Å.

Besides, since the traditional metric (FMAE/EMAE) cannot measure MLFFs' stability in real MD simulation scenarios, some other metrics are proposed to quantify MD stability: Time to Failure **Li**, "A Novel Metric for Evaluating Molecular Dynamics Simulation Stability"__, Wright's Factor (WF), and Jensen-Shannon Divergence (JSD)  of Radial Distribution Function (RDF) analysis. Time to Failure roughly measures stability with simulation time but misses other important physical metrics in MD simulation. WF and JSD need additional reference data generated from DFT which requires lots of computation resources in simulation.

\begin{figure}[t]
  \centering
  \includegraphics[width=0.45\textwidth]{fig/corr_sindex.png}
  \caption{Feature correlation and MD simulation stability of GNNFF models with our method}
  \label{fig_corr_sindex}
  \vspace*{-1.0\baselineskip}
\end{figure}