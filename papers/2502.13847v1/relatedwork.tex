\section{Related Work}
\subsection{Retrieval-Augmented Generation (RAG)}
Retrieval-Augmented Generation (RAG) systems significantly advance the capabilities of dialogue systems and question-answering tasks by amalgamating external knowledge bases with generative models. \cite{lewis2020retrieval} introduces the RAG models, adeptly merging pre-trained parametric and non-parametric memories for enhanced language generation. Subsequent studies \cite{liu2020retrieval} introduce several enhancements to RAG models, focusing on refining retrieval \cite{wang2023knowledgpt, cheng2024lift} and enhancing generation capabilities \cite{anderson2022lingua, jiang2023longllmlingua}. Recent innovations include FLARE \cite{zhang2023flare}, which introduces a feedback loop augmented retrieval method to iteratively refine retrieval outcomes and bolster generation quality. Additionally, SelfRAG \cite{asai2023selfrag} presents a self-supervised retrieval-augmented framework that boosts both retrieval and generation processes through the strategic use of pseudo-labels generated by the model itself. Despite these significant advancements, the challenge of seamlessly integrating dynamic historical context in RAG models for multi-turn dialogues remains an elusive goal.

Though achieve remarkable progress, most existing approaches continue to depend predominantly on static knowledge bases and do not adequately address the need to capture the evolving contextual nuances within conversations. This gap propels the development of DH-RAG in this paper, aimed at more effectively incorporating both static external knowledge and the transient context prevalent in ongoing dialogues, thereby enhancing the quality and coherence of multi-turn dialogue interactions.

\subsection{Retrieval-based Dialogue Systems} 

Retrieval-based dialogue systems \cite{ni2023recent,tao2021survey} become central in natural language processing, aiming to generate responses from large conversational datasets \cite{achiam2023gpt}. Early models like the reformulation-based retrieval system by \cite{yan2016learning} improve response matching in information-seeking dialogues. Subsequent developments include multi-hop retrieval methods \cite{xu2021beyond} that navigate complex knowledge graphs for richer responses and few-shot learning approaches like \cite{yu2021few}, which enhance performance in resource-scarce scenarios. However, these systems often struggle with generating context-appropriate responses in multi-turn dialogues due to their reliance on selecting pre-existing responses. This has led to the exploration of hybrid models \cite{sawarkar2024blended,xu2024retrieval}, such as the selective knowledge fusion framework by \cite{su2023selective}, which increases response relevance and coherence.

Despite progress, smoothly integrating changing conversational contexts remains challenging. This paper introduces DH-RAG, a novel framework that addresses this gap in multi-turn dialogue systems by creating a dynamic historical information database and a History-Learning Based Query Reconstruction Module. This approach refines queries using past context, enhancing dialogue interaction quality.

\subsection{Long-term and Short-term Memory Theory} 

The study of long-term and short-term memory is essential in cognitive psychology, providing key insights into how humans process information and make decisions, and have inspired many researcher in RAG \cite{cheng2024lift,hu2023reveal} and dialogue system fields \cite{zhang2019memory, bang2015example}. The foundational work by \cite{atkinson1968human} introduced the multi-store model of memory, differentiating between short-term and long-term memory systems. This model has been refined over the years, notably by \cite{baddeley1974working}, who developed the working memory model that focuses on the active management of information in short-term storage. Recent studies have examined the interaction between these memory systems. For example, \cite{dudai2015consolidation} investigates the processes of memory consolidation and reconsolidation. In conversational contexts, \cite{horton2016conversational} showed how both long-term and short-term memory help in forming a shared understanding in dialogues. The work of \cite{kumar2022role} further highlights the importance of working memory in keeping conversations coherent over multiple turns.

In this paper, we are inspired by a common sense notion that both long-term memory from a static database and short-term memory from current dialogues should serve as important contexts in the RAG process. Therefore, we propose the DH-RAG method, which skillfully utilizes the dynamic history context (short-term memory) through two novel modules and a newly constructed database to enhance the RAG process.