% !TEX root =  ../main.tex
\section{CLinSEPAL}\label{app:MADMMSCA_partial}
The problem we want to solve is:

\smoothpartial*

\Cref{prob:nonconvex_prob_approx} makes it explicit that the abstraction morphism is given by three key ingredients: \emph{(i)} the given partial, structural prior information represented by \B; \emph{(ii)} the structural component \Supp to be learned, such that the resulting causal abstraction is constructive; and \emph{(iii)} the abstraction coefficients in \V determining the linear functional forms of the causal abstraction, which have to be learned as well.
Specifically, \enquote{partial} means that some rows of \B have more than one entry equal to one.

Unfortunately, \Cref{prob:nonconvex_prob_approx} is nonconvex because of the objective function and the Stiefel manifold.
Additionally, in this case, the CA results in a bilinear form $\B\odot \Supp \odot \V$, which is not jointly convex in \Supp and \V.
Consequently, the constraint $\B\odot \Supp \odot \V \in \stiefel{\ell}{h}$ has to be carefully handled.

Regarding the nonconvexity of the objective in \Cref{eq:objective_partial_knowledge}, we proceed by leveraging its smoothness.
Specifically, we have the following result.

\begin{corollary}\label{corollary:objective_partial_knowledge}
    The function $f(\V, \Supp)$ in \Cref{eq:objective_partial_knowledge} is smooth.
    Additionally, define $\mathbf{A}\coloneqq\left(\B \odot \Supp \odot \V\right)$ and $\widetilde{\mathbf{A}}\coloneqq\left(\mathbf{A}^\top \covlow \mathbf{A}\right)^{-1}$.
    The partial derivatives w.r.t. \V and \Supp are
    \begin{equation}\label{eq:gradV_partial}
        \Egrad{\V}{f} = 2\left(\B \odot \Supp\right) \odot \left(\left(\covlow\mathbf{A}\widetilde{\mathbf{A}}\right)\left(\identity_h - \covhigh\widetilde{\mathbf{A}}\right)\right)\,,
    \end{equation}
    and
    \begin{equation}\label{eq:gradS_partial}
        \Egrad{\Supp}{f} = 2\left(\B \odot \V\right) \odot \left(\left(\covlow\mathbf{A}\widetilde{\mathbf{A}}\right)\left(\identity_h - \covhigh\widetilde{\mathbf{A}}\right)\right)\,.
    \end{equation}
\end{corollary}
\begin{proof}
    Smoothness directly follows from \Cref{prop:smoothness_and_differentiability} by defining $\mathbf{A}=\left(\B \odot \Supp \odot \V\right)$, which is constrained to \stiefel{\ell}{h} as given in \Cref{eq:prob_madmmsca_VS}.
    The partial derivatives in \Cref{eq:gradV_partial,eq:gradS_partial} follow from the application of \Cref{eq:rules_matrix_calculus}, together with the chain rule for derivatives. 
\end{proof}

At this point, we leverage \Cref{corollary:objective_partial_knowledge} to provide a solution which combines ADMM \citeSupp{boyd2011distributedSupp} and SCA \citeSupp{nedic2018parallelSupp}.
Specifically, ADMM is suitable to isolate and consequently tackle the nonconvexity in different subproblems.
To manage the bilinear form within the first constraint in \Cref{eq:prob_madmmsca_VS}, we introduce two splitting variables, namely \YO and \YT in \stiefel{\ell}{h}, and the corresponding equality constraints
\begin{equation}\label{eq:splitting_constraints_partial}
    \YO - \B\odot\Supp^k\odot \V = 0_{\ell \times h} \quad \text{and} \quad \YT - \B\odot\V^{k+1}\odot\Supp = 0_{\ell \times h}\,, \; \text{respectively.}
\end{equation}
In this way, given the solution at iteration $k$ within the ADMM framework, we optimize separately over \V and \Supp while always tracking \stiefel{\ell}{h}.
Please notice that we use $\V^{k+1}$ since when optimizing over \Supp, \V has already been updated.
The rationale behind the usage of the splitting variable for handling the Stiefel manifold is the same as the \emph{splitting of orthogonality constraints} method (SOC, \citeSupp{lai2014splittingSupp}).
Additionally, to handle $\left(\B \odot \Supp\right)^\top \in \sphere{h}{\ell}$, we introduce another splitting variable $\X \in \sphere{h}{\ell}$, and the corresponding equality constraint $\X - \left(\B \odot \Supp\right)^\top=\zeros_{h\times \ell}$.
Thus, starting from \Cref{eq:prob_madmmsca_VS}, we get the following equivalent minimization problem
\begin{equation}\label{eq:prob_madmmsca_VS_with_splitting}
    \begin{aligned}
        \V^\star, \Supp^\star, \YO^\star, \YT^\star, \X^\star = \argmin_{\substack{\V \in \rmatdim\\ \Supp \in \umatdim \\ \YO \in \stiefel{\ell}{h} \\ \YT \in \stiefel{\ell}{h} \\ \X \in \sphere{h}{\ell}}} &\quad f(\V,\Supp)\,;\\
         \textrm{subject to} & \quad \YO - \B \odot \Supp^k \odot \V = \zeros_{\ell \times h}\,, \\
         & \quad \YT - \B \odot \V^{k+1} \odot \Supp = \zeros_{\ell \times h}\,, \\
         & \quad \X - \left(\B \odot \Supp\right)^\top = \zeros_{h \times \ell}\,, \\
         % & \quad \Supp \ones_h - \ones_\ell = \zeros_\ell\,, \\
         & \quad \ones_h - \left(\B \odot \Supp\right)^\top \ones_\ell \leq \zeros_h\,.
    \end{aligned}
\end{equation}

Starting from \Cref{eq:prob_madmmsca_VS_with_splitting}, considering the penalty $\rho \in \reall_+$, we introduce the scaled dual variables \scaledUO and \scaledUT in \rmatdim; and $\scaledW \in \rmatdimT$, and write the scaled augmented Lagrangian
\begin{equation}\label{eq:scaledAUL_partial}
    \begin{aligned}
    L_\rho\left(\V,\Supp,\YO,\YT,\X,\scaledUO,\scaledUT,\scaledW\right) =& f(\V,\Supp) + \frac{\rho}{2}\frob{\B\odot\Supp^k\odot\V - \YO +\scaledUO}^2 + \\
    +& \frac{\rho}{2}\frob{\B\odot\V^{k+1}\odot\Supp - \YT +\scaledUT}^2 + \frac{\rho}{2}\frob{\left(\B \odot \Supp\right)^\top - \X + \scaledW}^2\,.        
    \end{aligned}
\end{equation}

Now, we can apply ADMM iterative procedure, getting the recursion for updating the primal and scaled dual variables. 
In detail, denote by $k \in \nat$ the current iteration.
We have
\begin{equation}\label{eq:ADMM_partial}
    \begin{aligned}        
        \V^{k+1}=&\argmin_{\V \in \rmatdim} L_\rho\left(\V,\Supp^k,\YO^k,\YT^k,\X^k,\scaledUO^k,\scaledUT^k,\scaledW^k\right)\,;\\
        \Supp^{k+1}=&\argmin_{\Supp \in \umatdim} L_\rho\left(\V^{k+1},\Supp,\YO^k,\YT^k,\X^k,\scaledUO^k,\scaledUT^k,\scaledW^k\right)\,,\\
        & \textrm{subject to} \quad \ones_h - \left(\B \odot \Supp\right)^\top \ones_\ell \leq \zeros_h\,; \\
        \YO^{k+1}=&\argmin_{\YO \in \stiefel{\ell}{h}} L_\rho\left(\V^{k+1},\Supp^{k+1},\YO,\YT^k,\X^k,\scaledUO^k,\scaledUT^k,\scaledW^k\right)\,;\\
        \YT^{k+1}=&\argmin_{\YT \in \stiefel{\ell}{h}} L_\rho\left(\V^{k+1},\Supp^{k+1},\YO^{k+1},\YT,\X^k,\scaledUO^k,\scaledUT^k,\scaledW^k\right)\,;\\
        \X^{k+1}=&\argmin_{\X \in \sphere{h}{\ell}} L_\rho\left(\V^{k+1},\Supp^{k+1},\YO^{k+1},\YT^{k+1},\X,\scaledUO^k,\scaledUT^k,\scaledW^k\right)\,;\\
        \scaledUO^{k+1}=&\scaledUO^k + \left(\B\odot\Supp^{k}\odot\V^{k+1} - \YO^{k+1}\right)\,;\\
        \scaledUT^{k+1}=&\scaledUT^k + \left(\B\odot\V^{k+1}\odot\Supp^{k+1} - \YT^{k+1}\right)\,;\\
        \scaledW^{k+1}=&\scaledW^k + \left(\B \odot \Supp^{k+1}\right)^\top - \X^{k+1}\,.
    \end{aligned}
\end{equation}

Similarly to SOC, we isolate the objective nonconvexity into the first and second (nonconvex) subproblems; and the nonconvexity of the manifold into the third and fourth (nonconvex) ones.
Notably, the first and second subproblems can be managed through SCA. 
Additionally, the third and fourth nonconvex subproblems admit closed-form solutions since they boil down to the \emph{closest orthogonal
approximation problems} \citeSupp{fan1955someSupp,higham1986computingSupp}.
Thus, the latter nonconvexity is somehow resolved.
Finally, we solve the subproblem for $\X^{k+1}$ in closed form as well.

\subsection{\texorpdfstring{Update for $\V^{k+1}$}{Update for V}}\label{subsec:updateV_partial}
Starting from \Cref{eq:scaledAUL_partial,eq:ADMM_partial}, the subproblem we have to solve is
\begin{equation}\label{eq:updateV_nonconvex_partial}
    \V^{k+1}=\argmin_{\V \in \rmatdim}\quad f(\V,\Supp^k) + \frac{\rho}{2}\frob{\B\odot\Supp^k\odot\V - \YO^k +\scaledUO^k}^2\,.
\end{equation}
\Cref{eq:updateV_nonconvex_partial} is nonconvex due to the inherent nonconvexity of $f(\V,\Supp^k)$.
However, the latter function is smooth and differentiable w.r.t. \V, as given in \Cref{corollary:objective_partial_knowledge}.
Hence, we apply the SCA framework.
In detail, denote by $q$ the SCA iteration and set $\V^0=\V^k$ for $q=0$.
We derive a strongly convex surrogate $\widetilde{f}(\V;\V^q, \Supp^k)$ around the point $\V^q$ -- i.e., the solution at the iterate $q$ -- exploiting \Cref{eq:gradV_partial}:
\begin{equation}\label{eq:strongly_convex_surrogate_Vpartial}
    \widetilde{f}(\V;\V^q, \Supp^k) \coloneqq \Tr{\Egrad{\V}{f}\at{(\V^q, \Supp^k)}^\top \left(\V - \V^q\right)} + \frac{\tau}{2}\frob{\V -\V^q}^2\,.
\end{equation}
It is immediate to check that \Cref{eq:strongly_convex_surrogate_Vpartial} is a proper surrogate satisfying the stationarity condition $\Egrad{\V}{f}\at{\V^q}=\Egrad{\V}{\widetilde{f}}\at{\V^q}$.

Therefore, at each SCA iteration $q$, we solve a strongly convex problem in closed-form and then apply the usual smoothing operation by using a diminishing stepsize $\gamma^q \in \reall_+$.
Specifically,
\begin{equation}\label{eq:SCA_recursion_V}
    \begin{aligned}        
        \V^{q+1} &= \argmin_{\V \in \rmatdim}\quad \widetilde{f}(\V;\V^q, \Supp^k) +\frac{\rho}{2}\frob{\B\odot\Supp^k\odot\V - \YO^k +\scaledUO^k}^2\,, \quad\textrm{(Strongly convex problem)}\\
        \V^{q+1} &= \V^q + \gamma^k\left(\V^{q+1}-\V^q\right)\,. \quad\textrm{(Smoothing)}
    \end{aligned}
\end{equation}

The solution of the strongly-convex problem is given element-wise in \Cref{lemma:updateV_elementwise_partial}. 
\begin{lemma}\label{lemma:updateV_elementwise_partial}
    The update for $\V^{q+1}$ can be computed element-wise as
    \begin{equation}\label{updateV_elementwise_partial}
        v_{ij}^{q+1}= \frac{1}{\tau + b_{ij} s_{ij}^{k^2}}\Bigg(\rho\, b_{ij} s_{ij}^k y_{1_{ij}}^k - \rho\, b_{ij} s_{ij}^k u_{1_{ij}}^k + \tau\, v_{ij}^q - \Big[\Egrad{\V}{f\at{(\V^q, \Supp^k}}\Big]_{ij} \Bigg)\,.
    \end{equation}
\end{lemma}
\begin{proof}
    The proof follows by imposing the stationarity condition
    \begin{equation}
        \mathbf{0}_{\ell\times h} = \Egrad{\V}{f\at{(\V^q, \Supp^k}} + \tau \left(\V-\V^q\right) + \rho \, \B \odot \Supp^k \odot \left( \B \odot \Supp^k \odot \V - \YO^k +\scaledUO^k\right)\,,
    \end{equation}
    and solving for \V.
\end{proof}

Additionally, the diminishing stepsize $\gamma^k$ has to satisfy the classical stochastic approximation conditions \citeSupp{nedic2018parallelSupp},
\begin{equation}\label{eq:SCA_stepsize_conditions}
    \text{\emph{(i)}} \sum_{q=1}^\infty \gamma^q = \infty \quad  \text{and} \quad\text{\emph{(ii)}} \sum_{q=1}^\infty \left( \gamma^q \right)^2 < \infty\,.
\end{equation}
In our experiments, we use the decaying rule 
\begin{equation}\label{eq:SCA_stepsize_rule}
    \gamma^{q+1}=\gamma^q\left(1-\varepsilon \,\gamma^q\right)\,, \quad \varepsilon \in (0,1)\,.
\end{equation}
The SCA framework is guaranteed to converge to stationary points of the original nonconvex problem in \Cref{eq:updateV_nonconvex_partial} \citeSupp{nedic2018parallelSupp}.
Accordingly, we establish convergence for the update when
\begin{equation}\label{eq:convergenceV_approx}
    \frob{\V^{q+1}-\V^q}\leq \tau^c\,, \quad \tau^c \approx 0\,;
\end{equation}
and set $\V^{k+1}=\V^{q+1}$.

\subsection{\texorpdfstring{Update for $\Supp^{k+1}$}{Update for S}}\label{subsec:updateS_partial}
Starting from \Cref{eq:scaledAUL_partial,eq:ADMM_partial}, the subproblem we have to solve is
\begin{equation}\label{eq:updateS_nonconvex_partial}
    \begin{aligned}
        \Supp^{k+1}=&\argmin_{\Supp \in \umatdim} f(\V^{k},\Supp) + \frac{\rho}{2}\frob{\B \odot \V^{k+1} \odot \Supp - \YT^k +\scaledUT^k}^2 + \frac{\rho}{2}\frob{\left(\B \odot \Supp\right)^\top - \X^k + \scaledW^k}^2\,,\\
        & \textrm{subject to} \quad \ones_h - \left(\B \odot \Supp\right)^\top \ones_\ell \leq \zeros_h\,. \\
    \end{aligned}
\end{equation}
The subproblem above is nonconvex and constrained.
Similarly to \Cref{subsec:updateV_partial}, we apply the SCA framework.
Denote by $q$ the SCA iteration and set $\Supp^0=\Supp^k$ for $q=0$.
Here, the strongly convex surrogate of $f(\V^{k+1},\Supp)$ reads as
\begin{equation}\label{eq:strongly_convex_surrogate_Spartial}
    \widetilde{f}(\Supp;\V^{k+1}, \Supp^q) \coloneqq \Tr{\Egrad{\Supp}{f}\at{\left(\V^{k+1},\Supp^q\right)}^\top \left(\Supp - \Supp^q\right)} + \frac{\tau}{2}\frob{\Supp -\Supp^q}^2\,,
\end{equation}
which satisfies $\Egrad{\Supp}{f}\at{\left(\V^{k+1},\Supp^q\right)}=\Egrad{\Supp}{\widetilde{f}}\at{\left(\V^{k+1},\Supp^q\right)}$.
At each SCA iteration $q$, we solve a constrained quadratic programming (QP) problem and apply the smoothing step by using the stepsize $\gamma^q \in \reall_+$ complying with the conditions in \Cref{eq:SCA_stepsize_conditions}.
In detail, let $\myvec{\mathbf{A}}$ be the column-wise vectorization of a given matrix $\mathbf{A}$ and define
\begin{equation}\label{eq:Q_and_c_Supdate}
    \begin{aligned}
        \mathbf{Q} &= \tau\identity_{\ell h} + \rho\,\mathrm{diag}\left( \left(\myvec{\B}\odot\myvec{\V^{k+1}}\right)\odot\left(\myvec{\B}\odot\myvec{\V^{k+1}}\right)\right) + \rho\,\mathrm{diag}\left(\myvec{\B}\odot \myvec{\B}\right)\,,\\
        \mathbf{c} &= \myvec{\Egrad{\Supp}{f}\at{\left(\V^{k+1},\Supp^q\right)}} - \tau\, \myvec{\Supp^q} - \rho\, \myvec{\YT^k - \scaledUT^k} \odot \myvec{\B} \odot \myvec{\V^{k+1}} - \rho \, \myvec{\B} \odot \myvec{\left(\X^k - \scaledW^k\right)^\top}\,.
    \end{aligned}
\end{equation}
Additionally, recall that $\myvec{\mathbf{A} \mathbf{C}} = \left(\mathbf{C}^\top \otimes \identity_h \right)\myvec{\mathbf{A}}$, with $\mathbf{A} \in \reall^{h\times \ell}$ and $\mathbf{C} \in \reall^{\ell \times m}$.
Hence, denoting with $\mathbf{K}^{\ell,h}$ the commutation matrix, 
the inequality constraint can be rewritten as
\begin{equation}\label{eq:inequality_constr_S}
    \begin{aligned}
        \ones_h - \myvec{\left(\B \odot \Supp\right)^\top \ones_\ell} &= \ones_h - \left(\ones_\ell^\top \otimes \identity_h \right) \myvec{\left(\B \odot \Supp\right)^\top} \\
        &= \ones_h - \left(\ones_\ell^\top \otimes \identity_h \right) \mathbf{K}^{\ell,h} \myvec{\B \odot \Supp} \\
        &= \ones_h - \left(\ones_\ell^\top \otimes \identity_h \right) \mathbf{K}^{\ell,h} \myvec{\mathrm{diag}\left(\myvec{\B}\right) \myvec{\Supp}}\\
        &= \ones_h - \underbrace{\left(\ones_\ell^\top \otimes \identity_h \right) \mathbf{K}^{\ell,h} \mathrm{diag}\left(\myvec{\B}\right)}_{\G} \myvec{\Supp} \leq \zeros_h\,.
    \end{aligned}    
\end{equation}
At this point, starting from \Cref{eq:updateS_nonconvex_partial} and exploiting \Cref{eq:Q_and_c_Supdate,eq:inequality_constr_S}, we can pose the SCA recursion: 
\begin{equation}\label{eq:SCA_recursion_S}
    \begin{aligned}        
        \myvec{\Supp}^{q+1} =& \argmin_{\Supp \in \umatdim}\quad \frac{1}{2}\myvec{\Supp}^\top \mathbf{Q} \myvec{\Supp} + \mathbf{c}^\top \myvec{\Supp}\,, \quad\textrm{(QP problem)}\\
        &\textrm{subject to} \quad \ones_h -  \G \myvec{\Supp} \leq \zeros_h\,. \\
        \myvec{\Supp}^{q+1} &= \myvec{\Supp}^q + \gamma^k\left(\myvec{\Supp}^{q+1}-\myvec{\Supp}^q\right)\,. \quad\textrm{(Smoothing)}
    \end{aligned}
\end{equation}
The QP problem in \Cref{eq:SCA_recursion_S} can be solved through off-the-shelf quadratic programming solvers.
In our experiments, we use the OSQP \citeSupp{osqpSupp} implementation available in \texttt{cvxpy} \citeSupp{diamond2016cvxpySupp}.
Since the quadratic form involves a diagonal, positive definite matrix $\mathbf{Q}$, in case a solution exists in the feasible set determined by the inequality constraint, it is also unique.
Regarding the smoothing step, $\gamma^q$ follows \Cref{eq:SCA_stepsize_rule}.
Similarly to \Cref{subsec:updateV_partial}, we determine convergence when
\begin{equation}\label{eq:convergenceS_approx}
    \frob{\myvec{\Supp}^{q+1}-\myvec{\Supp}^q}\leq \tau^c\,, \quad \tau^c \approx 0\,;
\end{equation}
and set $\Supp^{k+1}=\Supp^{q+1}$, where $\Supp^{q+1}$ is the reshaping of $\myvec{\Supp}^{q+1}$ in matrix form.

\subsection{\texorpdfstring{Update for $\YO^{k+1}$ and $\YT^{k+1}$}{Update for Y1 and Y2}}
Starting from \Cref{eq:scaledAUL_partial,eq:ADMM_partial}, the subproblem to solve is
\begin{equation}\label{eq:updateY1_partial}
    \begin{aligned}
        \YO^{k+1}&=\argmin_{\YO \in \stiefel{\ell}{h}}\quad \frac{\rho}{2}\frob{\B \odot \Supp^{k+1} \odot \V^{k+1} - \YO +\scaledUO^k}^2\\
                &=\prox_{\stiefel{\ell}{h}}\left(\widetilde{\mathbf{Y}}_1\right)\,, \quad \text{with } \widetilde{\mathbf{Y}}_1\coloneqq\B \odot \Supp^{k+1} \odot \V^{k+1} +\scaledUO^k\,.
    \end{aligned}            
\end{equation}
The evaluation of $\prox_{\stiefel{\ell}{h}}(\widetilde{\mathbf{Y}_1})$ in \Cref{eq:updateY1_partial} is equivalent to the (unique) solution of the closest orthogonal approximation problem \citeSupp{fan1955someSupp,higham1986computingSupp}.
Specifically, it is equal to the $\mathbf{U}_{p_1}$ factor of the polar decomposition of the matrix $\widetilde{\mathbf{Y}}_1=\mathbf{U}_{p_1} \mathbf{P}_{p_1}$, namely
\begin{equation}\label{eq:updateY1_solution}
    \YO^{k+1}=\mathbf{U}_{p_1}\,.
\end{equation}

Similarly, defining $\widetilde{\mathbf{Y}}_2\coloneqq\B \odot \Supp^{k+1} \odot \V^{k+1} +\scaledUT^k$ and considering the polar decomposition $\widetilde{\mathbf{Y}}_2=\mathbf{U}_{p_2} \mathbf{P}_{p_2}$, we have
\begin{equation}\label{eq:updateY2_solution}
    \YT^{k+1}=\mathbf{U}_{p_2} \,.
\end{equation}

\subsection{\texorpdfstring{Update for $\X^{k+1}$}{Update for X}}
Starting from \Cref{eq:scaledAUL_partial,eq:ADMM_partial}, the subproblem reads as
\begin{equation}\label{eq:updateX_partial}
    \begin{aligned}
        \X^{k+1}&= \argmin_{\X \in \sphere{h}{\ell}} \frac{\rho}{2}\frob{\left(\B \odot \Supp^{k+1}\right)^\top - \X + \scaledW^k}^2\\
                &= \prox_{\sphere{h}{\ell}}\left(\left(\B \odot \Supp^{k+1}\right)^\top + \scaledW^k\right)\,.
    \end{aligned}
\end{equation}
The following result gives the solution.

\begin{lemma}\label{lemma:proximal_spDelta}
    Consider 
    \begin{equation}\label{eq:spDelta}
        \sphere{h}{\ell} \coloneqq \Big\{\mathbf{A} \in \{0,1\}^{h\times\ell} \mid  \norm{\mathbf{a}_j}_2=1 \text{ and }\\
        \sum_{i=1}^h a_{ij}=1,\,\,\forall j \in [\ell]  \Big\}\,;
\end{equation}
    and $\A \in \reall^{h \times \ell}$.
    The proximal operator 
    \begin{equation}\label{eq:proximal_spDelta}
        \prox_{\sphere{h}{\ell}}\left(\A\right)\coloneqq \argmin_{\X \in \rmatdimT} \frob{\A - \X}\,,    
    \end{equation}
     is the matrix $\X^\star$ such that
    \begin{equation}
        \forall \, j \in [\ell], \; x_{ij}^\star = \begin{cases}
            1\,, \quad &\text{if } a_{ij} = \argmin_{i} \abs{a_{ij}-1}\,,\\
            0\,, & \text{otherwise.}
        \end{cases}
    \end{equation}
\end{lemma}
\begin{proof}
    To belong to $\sphere{h}{\ell}$, $\X^\star$ must have only a single nonzero entry equal to one for each column $j \in [\ell]$.
    Consequently, the objective in \Cref{eq:proximal_spDelta} is minimized by setting, for each column $j\in [\ell]$, $x_{ij}^\star=1$ in correspondence of the element $a_{ij}$ whose absolute distance from one is minimum.
\end{proof}

\subsection{Stopping criteria}\label{subsec:stopping_criteria_partial}
The empirical convergence of the proposed method is established according to primal and dual feasibility optimality conditions.
In this case, the primal residuals associated with the equality constraints in \Cref{eq:prob_madmmsca_VS_with_splitting} are
\begin{equation}\label{eq:primal_res_partial}
    \begin{aligned}
        \mathbf{R}_{p,1}^{k+1}&\coloneqq\YO^{k+1}-\B\odot\Supp^{k}\odot\V^{k+1}\,;\\
        \mathbf{R}_{p,2}^{k+1}&\coloneqq\YT^{k+1}-\B\odot\V^{k+1}\odot\Supp^{k+1}\,;\\
        \mathbf{R}_{p,3}^{k+1}&\coloneqq \X^{k+1} - \left(\B \odot \Supp^{k+1}\right)^\top\,.
    \end{aligned}
\end{equation}

Additionally, the dual residuals obtained from the stationarity condition are
\begin{equation}\label{eq:dual_res_partial}
    \begin{aligned}        
        \mathbf{R}_{d,1}^{k+1}&\coloneqq \rho \,\B\odot \Supp^{k} \odot \left(\YO^{k+1}-\YO^k\right)\,;\\
        \mathbf{R}_{d,2}^{k+1}&\coloneqq \rho \,\B\odot \V^{k+1} \odot \left(\YT^{k+1} - \YT^k\right)\,;\\ \mathbf{R}_{d,3}^{k+1}&\coloneqq \rho \, \B \odot \left( \X^{k+1}-\X^k\right)^\top\,.
    \end{aligned}
\end{equation}

Following \citeSupp{boyd2011distributedSupp}, denoting with $\tau^a$ and $\tau^r$ in $\reall_+$ the absolute and relative tolerances, respectively, the stopping criteria to be satisfied for empirical convergence are
\begin{equation}\label{eq:stopping_criteria_partial}
    \begin{aligned}
        \frob{\mathbf{R}_{p,1}^{k+1}}&=d_{p,1}^{k+1}\leq \tau^a \sqrt{\ell h} + \tau^r \max{\left(\frob{\YO^{k+1}}, \frob{\B\odot \Supp^{k} \odot \V^{k+1}}\right)}\,,\\
        \frob{\mathbf{R}_{p,2}^{k+1}}&=d_{p,2}^{k+1}\leq \tau^a \sqrt{\ell h} + \tau^r \max{\left(\frob{\YT^{k+1}}, \frob{\B\odot \V^{k+1} \odot \Supp^{k+1}}\right)}\,,\\
        \frob{\mathbf{R}_{p,3}^{k+1}}&=d_{p,3}^{k+1}\leq \tau^a \sqrt{\ell h} + \tau^r \max{\left(\frob{\X^{k+1}}, \frob{\B \odot \Supp^{k+1}}\right)}\,,\\
        \frob{\mathbf{R}_{d,1}^{k+1}}&=d_{d,1}^{k+1}\leq \tau^a \sqrt{\ell h} + \tau^r \rho\, \frob{\B \odot \Supp^{k} \odot \scaledUO^{k+1}}\,,\\
        \frob{\mathbf{R}_{d,2}^{k+1}}&=d_{d,2}^{k+1}\leq \tau^a \sqrt{\ell h} + \tau^r \rho\, \frob{\B \odot \V^{k+1} \odot \scaledUT^{k+1}}\,,\\
        \frob{\mathbf{R}_{d,3}^{k+1}}&=d_{d,3}^{k+1}\leq \tau^a \sqrt{\ell h} + \tau^r \rho \, \frob{\B^\top \odot \scaledW^{k+1}}\,.\\
    \end{aligned}
\end{equation}

The CLinSEPAL method is summarized in \Cref{alg:clinsepal}

\begin{algorithm}[H]
\caption{CLinSEPAL}
\label{alg:clinsepal}
\begin{algorithmic}[1]
\STATE \textbf{Input:} $\covlow$, $\covhigh$, $\B$, $\rho$, $\tau$, $\varepsilon$, $\tau^c$, $\tau^a$, $\tau^r$
\STATE Initialize: $\V^0 \in \rmatdim$, $\Supp^0 = \B$, $\YO^0 \in \stiefel{\ell}{h}$, $\YT^0 \in \stiefel{\ell}{h}$, $\X^0=\B^\top$, $\scaledUO^0 \gets \B\odot\Supp^0\odot\V^0 - \YO^0$, $\scaledUT^0 \gets \B\odot\Supp^0\odot\V^0 - \YT^0$, $\scaledW^0 \gets (\B \odot \Supp^0)^\top - \X^0$
\REPEAT
    \STATE $\V^{k+1} \gets \text{Apply \cref{eq:SCA_recursion_V}}$ 
    \STATE $\Supp^{k+1} \gets \text{Apply \cref{eq:SCA_recursion_S}}$ 
    \STATE $\YO^{k+1} \gets \text{\cref{eq:updateY1_solution}}$
    \STATE $\YT^{k+1} \gets \text{\cref{eq:updateY2_solution}}$
    \STATE $\scaledUO^{k+1} \gets \scaledUO^k + \B\odot\Supp^{k}\odot\V^{k+1} - \YO^{k+1}$
    \STATE $\scaledUT^{k+1} \gets \scaledUT^k + \B\odot\V^{k+1}\odot\Supp^{k+1} - \YT^{k+1}$
    \STATE $\scaledW^{k+1} \gets \scaledW^k + \left(\B \odot \Supp^{k+1}\right)^\top - \X^{k+1}$
\UNTIL{\Cref{eq:stopping_criteria_partial} is satisfied}
\STATE \textbf{Output:} $\V$, $\Supp$, $\YO$, $\YT$, $\X$, $\scaledUO$, $\scaledUT$, $\scaledW$
\end{algorithmic}
\end{algorithm} 

\subsection{Full prior case}\label{subsec:CLinSEPAL_full_prior}
\Cref{prob:nonconvex_prob_approx} simplifies in case of full prior knowledge of \B. 
Indeed, it is not needed to learn \Supp since $\Supp \equiv \B$.
Accordingly, we get the following.

\begin{problem}\label{prob:nonconvex_prob_full}
Given $\covlow \in \pd^{\ell}$, $\covhigh \in \pd^{h}$, and $\B \in \lmatdim$, the linear constructive CA is given by the transpose of the product $\B \odot \V$, where 
    \begin{equation}\label{eq:prob_madmmsca_V}
        \begin{aligned}
            \V^\star = \argmin_{\V \in \rmatdim} &\quad f(\V)\,;\\
             \textrm{subject to} & \quad \B \odot \V \in \stiefel{\ell}{h}\,; \\
        \end{aligned}
    \end{equation}
    and
    \begin{equation}\label{eq:objective_full_knowledge}
        f(\V) \coloneqq \Tr{\left(\left(\B \odot \V\right)^\top \covlow \left(\B \odot \V\right) \right)^{-1} \covhigh} + \log\det {\left(\B \odot\V\right)^\top \covlow \left(\B \odot\V\right) }\,.
    \end{equation}
\end{problem}

The solution can be obtained in a similar manner as for the partial prior knowledge case.
Below, we report the mathematical derivation for completeness without further comments.

\begin{corollary}\label{corollary:objective_full_knowledge}
    The function $f(\V)$ in \Cref{eq:objective_full_knowledge} is smooth.
    Additionally, define $\mathbf{A}\coloneqq\left(\B \odot \V\right)$ and $\widetilde{\mathbf{A}}\coloneqq\left(\mathbf{A}^\top \covlow \mathbf{A}\right)^{-1}$.
    The gradient is
    \begin{equation}\label{eq:gradV_full}
        \Egrad{\V}{f} = 2\B \odot  \left(\left(\covlow\mathbf{A}\widetilde{\mathbf{A}}\right)\left(\identity_h - \covhigh\widetilde{\mathbf{A}}\right)\right)\,,
    \end{equation}
\end{corollary}
\begin{proof}
    Smoothness directly follows from \Cref{prop:smoothness_and_differentiability} by defining $\mathbf{A}\coloneqq\left(\B \odot \V\right)$, which is constrained to \stiefel{\ell}{h} as given in \Cref{eq:prob_madmmsca_V}.
    The gradient in \Cref{eq:gradV_full} follows from the application of \Cref{eq:rules_matrix_calculus}, together with the chain rule for derivatives. 
\end{proof}

Starting from \Cref{eq:prob_madmmsca_V}, we get the following equivalent minimization problem
\begin{equation}\label{eq:prob_madmmsca_V_with_splitting}
    \begin{aligned}
        \V^\star, \Y^\star = \argmin_{\substack{\V \in \rmatdim\\ \Y \in \stiefel{\ell}{h} \\}} &\quad f(\V)\,;\\
         \textrm{subject to} & \quad \Y - \B \odot \V = \zeros_{\ell \times h}\,.
    \end{aligned}
\end{equation}

Considering the scaled dual variable $\scaledU \in\rmatdim$, the scaled augmented Lagrangian is
\begin{equation}\label{eq:scaledAUL_full}
    L_\rho\left(\V,\Y,\scaledU\right)=f(\V) + \frac{\rho}{2}\frob{\B\odot\V - \Y +\scaledU}^2 \,.
\end{equation}

The ADMM recursion is
\begin{equation}\label{eq:ADMM_full}
    \begin{aligned}        
        \V^{k+1}=&\argmin_{\V \in \rmatdim} L_\rho\left(\V,\Y^k,\scaledU^k\right)\,;\\
        \Y^{k+1}=&\argmin_{\Y \in \stiefel{\ell}{h}} L_\rho\left(\V^{k+1},\Y,\scaledU^k\right)\,;\\
        \scaledU^{k+1}=&\scaledU^k + \left(\B\odot\V^{k+1} - \Y^{k+1}\right)\,.\\
    \end{aligned}
\end{equation}

\subsubsection{\texorpdfstring{Update for $\V^{k+1}$}{Update for V}}\label{subsec:updateV_full}
Starting from \Cref{eq:scaledAUL_full,eq:ADMM_full}, the subproblem we have to solve is
\begin{equation}\label{eq:updateV_nonconvex_full}
    \V^{k+1}=\argmin_{\V \in \rmatdim}\quad f(\V) + \frac{\rho}{2}\frob{\B\odot\V - \Y^k +\scaledU^k}^2\,.
\end{equation}
\Cref{eq:updateV_nonconvex_full} is nonconvex due to the inherent nonconvexity of $f(\V)$.
However, the latter function is smooth and differentiable w.r.t. \V, as given in \Cref{corollary:objective_full_knowledge}.
Hence, we apply the SCA framework.
In detail, denote by $q$ the SCA iteration and set $\V^0=\V^k$ for $q=0$.
We derive a strongly convex surrogate $\widetilde{f}(\V;\V^q)$ around the point $\V^q$ -- i.e., the solution at the iterate $q$ -- exploiting \Cref{eq:gradV_full}, 
\begin{equation}\label{eq:strongly_convex_surrogate_Vfull}
    \widetilde{f}(\V;\V^q) \coloneqq \Tr{\Egrad{\V}{f}\at{\V^q}^\top \left(\V - \V^q\right)} + \frac{\tau}{2}\frob{\V -\V^q}^2\,.
\end{equation}

Therefore, at each SCA iteration $q$, we solve a strongly convex problem in closed-form and then apply the usual smoothing operation by using a diminishing stepsize $\gamma^q \in \reall_+$ following \cref{eq:SCA_stepsize_rule} and satisfying \cref{eq:SCA_stepsize_conditions}.
Specifically,
\begin{equation}\label{eq:SCA_recursion_V_full}
    \begin{aligned}        
        \V^{q+1} &= \argmin_{\V \in \rmatdim}\quad \widetilde{f}(\V;\V^q) +\frac{\rho}{2}\frob{\B\odot\V - \Y^k +\scaledU^k}^2\,, \quad\textrm{(Strongly convex problem)}\\
        \V^{q+1} &= \V^q + \gamma^k\left(\V^{q+1}-\V^q\right)\,. \quad\textrm{(Smoothing)}
    \end{aligned}
\end{equation}

The solution of the strongly-convex problem is given element-wise in \Cref{lemma:updateV_elementwise_full}. 
\begin{lemma}\label{lemma:updateV_elementwise_full}
    The update for $\V^{q+1}$ can be computed element-wise as
    \begin{equation}\label{updateV_elementwise_full}
        v_{ij}^{q+1}= \frac{1}{\tau + b_{ij}}\Bigg(\rho\, b_{ij} y_{ij}^k - \rho\, b_{ij} u_{ij}^k + \tau\, v_{ij}^q - \Big[\Egrad{\V}{f\at{\V^q}}\Big]_{ij} \Bigg)\,.
    \end{equation}
\end{lemma}
\begin{proof}
    The proof follows by imposing the stationarity condition
    \begin{equation}
        \mathbf{0}_{\ell\times h} = \Egrad{\V}{f\at{\V^q}} + \tau \left(\V-\V^q\right) + \rho \, \B \odot \left( \B \odot \V - \Y^k +\scaledU^k\right)\,,
    \end{equation}
    and solving for \V.
\end{proof}

We establish convergence for the update when
\begin{equation}\label{eq:convergenceV_full}
    \frob{\V^{q+1}-\V^q}\leq \tau^c\,, \quad \tau^c \approx 0\,;
\end{equation}
and set $\V^{k+1}=\V^{q+1}$.

\subsubsection{\texorpdfstring{Update for $\Y^{k+1}$}{Update for Y}}
Starting from \Cref{eq:scaledAUL_full,eq:ADMM_full}, the subproblem to solve is
\begin{equation}\label{eq:updateY_full}
    \begin{aligned}
        \Y^{k+1}&=\argmin_{\Y \in \stiefel{\ell}{h}}\quad \frac{\rho}{2}\frob{\B \odot \V^{k+1} - \Y +\scaledU^k}^2\\
                &=\prox_{\stiefel{\ell}{h}}(\widetilde{\mathbf{Y}})\,, \quad \text{with } \widetilde{\mathbf{Y}}\coloneqq\B \odot \V^{k+1} +\scaledU^k\,.
    \end{aligned}            
\end{equation}
Denoting by $\mathbf{U}_p \mathbf{P}_p$ the polar decomposition of the matrix $\widetilde{\mathbf{Y}}$, the update is
\begin{equation}\label{eq:updateY_solution}
    \Y^{k+1}=\mathbf{U}_p\,.
\end{equation}

\subsubsection{Stopping criteria}\label{subsec:stopping_criteria_full}
The empirical convergence of the proposed method is established according to primal and dual feasibility optimality conditions \citeSupp{boyd2011distributedSupp}.
The primal residual, associated with the equality constraint in \Cref{eq:prob_madmmsca_V_with_splitting}, is
\begin{equation}\label{eq:primal_res_full}
    \mathbf{R}_p^{k+1}\coloneqq\Y^{k+1}-\B\odot\V^{k+1}\,.
\end{equation}
The dual residual, which can be obtained from the stationarity condition, is
\begin{equation}\label{eq:dual_res_full}
    \mathbf{R}_d^{k+1}\coloneqq \rho \,\B \odot\left(\Y^{k+1}-\Y^k\right)\,.
\end{equation}
As $k \rightarrow \infty$, the norm of the primal and dual residuals should vanish.
Hence, the stopping criterion can be set in terms of the norms
\begin{equation}\label{eq:norms_full}
    \text{\emph{(i)}}\;d_p^{k+1}=\frob{\mathbf{R}_p^{k+1}} \quad \text{and} \quad \text{\emph{(ii)}}\;d_d^{k+1}=\frob{\mathbf{R}_d^{k+1}}\,. 
\end{equation}
Specifically, given absolute and relative tolerance, namely $\tau^a$ and $\tau^r$ in $\reall_+$, respectively, convergence in practice is established following \citetSupp{boyd2011distributedSupp}, when 
\begin{equation}\label{eq:stopping_criteria_full}
    \text{\emph{(i)}}\; d_p^{k+1} \leq \tau^a\sqrt{\ell h} + \tau^r \max{\left(\frob{\Y^{k+1}}, \frob{\B \odot \V^{k+1}}\right)}\,, \quad
    \text{and} \quad
    \text{\emph{(ii)}}\; d_d^{k+1} \leq \tau^a\sqrt{\ell h} + \tau^r \rho  \frob{\B \odot \scaledU^{k+1}}\,.
\end{equation}

The full prior version of CLinSEPAL is summarized in \Cref{alg:clinsepal_fullprior}.

\begin{algorithm}[H]
\caption{CLinSEPAL (full prior case)}
\label{alg:clinsepal_fullprior}
\begin{algorithmic}[1]
\STATE \textbf{Input:} $\covlow$, $\covhigh$, $\B$, $\rho$, $\tau$, $\varepsilon$, $\tau^c$, $\tau^a$, $\tau^r$
\STATE Initialize: $\V^0 \in \rmatdim$, $\Y^0 \in \stiefel{\ell}{h}$, $\scaledU^0 \gets \B\odot\V^0 - \Y^0$
\REPEAT
    \STATE $\V^{k+1} \gets \text{Apply \cref{eq:SCA_recursion_V_full}}$
    \STATE $\Y^{k+1} \gets \text{\cref{eq:updateY_solution}}$
    \STATE $\scaledU^{k+1} \gets \scaledU^k + \B\odot\V^{k+1} - \Y^{k+1}$
\UNTIL{\Cref{eq:stopping_criteria_full} is satisfied}
\STATE \textbf{Output:} $\V$, $\Y$, $\scaledU$
\end{algorithmic}
\end{algorithm} 
