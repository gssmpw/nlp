% !TEX root =  ../main.tex
\section{LinSEPAL-ADMM}\label{app:MADMM}
Let us recall below the nonsmooth Riemannian problem we have to solve.
\nonsmoothprob*

The structure of the objective in \eqref{eq:minKL}, separating into smooth (cf. \cref{prop:smoothness_and_differentiability}) and nonsmooth terms, makes the \emph{alternating direction method of multipliers} (ADMM, \citeSupp{boyd2011distributedSupp}) an appealing optimization framework for deriving a solution.
This is the rationale behind the general framework \emph{manifold ADMM} \citeSupp{kovnatsky2016madmmSupp}, that we decline to our setting in the following, thus obtaining the LinSEPAL-ADMM algorithm.

Starting from \eqref{eq:minKL}, we add a splitting variable $\Y \in \rmatdim$ to be optimized over the Euclidean space to handle the non-smooth term $h(\V)$:

\begin{equation}\label{eq:MADMM}
    \begin{aligned}
        \min_{\V \in \stiefel{\ell}{h}, \Y \in \rmatdim} & \quad \Tr{\left( \V^\top \covlow \V\right)^{-1} \covhigh} + \log\det{\V^\top \covlow \V} + \lambda \norm{\Y}_1 \, , \\
        \textrm{subject to} & \quad \Y - \D \odot \V=\zeros_{\ell \times h}.
    \end{aligned}
    \tag{P2}
\end{equation}

At this point, following \citeSupp{boyd2011distributedSupp}, by denoting by $\scaledU \in \rmatdim$ the scaled dual variable, and by $\rho \in \reall^+$ the ADMM stepsize, the scaled augmented Lagrangian reads as

\begin{equation}\label{eq:MADMMsAUL}
    L_{\rho}\left(\V, \Y, \scaledU \right) =  \Tr{\left( \V^\top \covlow \V\right)^{-1} \covhigh} + \log\det{\V^\top \covlow \V} + \lambda \norm{\Y}_1 + \frac{\rho}{2}\frob{\D\odot \V  - \Y + \scaledU}^2.
\end{equation}

Starting from \Cref{eq:MADMMsAUL}, the ADMM updates at the $k$-th iteration are
\begin{equation}\label{eq:MADMMrecursion_app}
    \begin{aligned}
        \V^{k+1} &= \argmin_{\V \in \stiefel{\ell}{h}} L_{\rho}\left(\V, \Y^k, \scaledU^k \right),\\
        \Y^{k+1} &= \argmin_{\Y \in \rmatdim} L_{\rho}\left(\V^{k+1}, \Y, \scaledU^k \right),\\
        \scaledU^{k+1} &= \scaledU^k + \D \odot \V^{k+1} - \Y^{k+1}. 
    \end{aligned}
    \tag{R1}
\end{equation}

\spara{Solution for $\V^{k+1}$.}

The update for $\V^{k+1}$ in \eqref{eq:MADMMrecursion_app} reads as
\begin{equation}\label{eq:updateV}
    \V^{k+1} = \argmin_{\V \in \stiefel{\ell}{h}} \Tr{\left( \V^\top \covlow \V\right)^{-1} \covhigh} + \log\det{\V^\top \covlow \V} + \frac{\rho}{2}\frob{\D\odot \V  - \Y^k + \scaledU^k}^2     
\end{equation}

\Cref{eq:updateV} is a standard smooth optimization problem over the Stiefel manifold, and it can be solved by standard techniques such as those in \citeSupp{boumal2023introductionSupp}.
Newton and conjugate gradient methods for the Stiefel manifold are discussed in \citeSupp{edelman1998geometrySupp}.
In our experiments, we use the conjugate gradient implementation in \citeSupp{boumal2014manoptSupp}.

\spara{Solution for $\Y^{k+1}$.}
The update for $\Y^{k+1}$ in \eqref{eq:MADMMrecursion_app} reads as
\begin{equation}\label{eq:updateY_madmm}
    \begin{aligned}
        \Y^{k+1} &= \argmin_{\Y \in \rmatdim} \lambda\norm{\Y}_1 + \frac{\rho}{2}\frob{\D \odot \V^{k+1} - \Y + \scaledU^k}^2 = \\
        &= \mathcal{S}_{\lambda/\rho} \left( \D \odot \V^{k+1} + \scaledU^k \right);
    \end{aligned}
\end{equation}
where $\mathcal{S}_{\delta}(x)=\sign(x) \cdot \max(\abs{x}-\delta, 0)$ is the element-wise soft-thresholding operator \citeSupp{parikh2014proximalSupp}.

\spara{Stopping criteria.}
The empirical convergence of LinSEPAL-ADMM is established according to primal and dual feasibility optimality conditions \citeSupp{boyd2011distributedSupp}.
The primal residual, associated with the equality constraint in \Cref{eq:MADMM}, is
\begin{equation}\label{eq:primal_res_madmm}
    \mathbf{R}_p^{k+1}\coloneqq\Y^{k+1}-\D\odot\V^{k+1}\,.
\end{equation}
The dual residual, which can be obtained from the stationarity condition, is
\begin{equation}\label{eq:dual_res_madmm}
    \mathbf{R}_d^{k+1}\coloneqq \rho \,\D \odot\left(\Y^{k+1}-\Y^k\right)\,.
\end{equation}
As $k \rightarrow \infty$, the norm of the primal and dual residuals should vanish.
Hence, the stopping criterion can be set in terms of the norms
\begin{equation}\label{eq:norms_madmm}
    \text{\emph{(i)}}\;d_p^{k+1}=\frob{\mathbf{R}_p^{k+1}} \quad \text{and} \quad \text{\emph{(ii)}}\;d_d^{k+1}=\frob{\mathbf{R}_d^{k+1}}\,. 
\end{equation}
Specifically, given absolute and relative tolerance, namely $\tau^a$ and $\tau^r$ in $\reall_+$, respectively, convergence in practice is established following \citetSupp{boyd2011distributedSupp} when 
\begin{equation}\label{eq:convergence_linsepal}
    \text{\emph{(i)}}\; d_p \leq \tau^a\sqrt{\ell h} + \tau^r \max{\left(\frob{\Y^{k+1}}, \frob{\D \circ \V^{k+1}}\right)}\,, \quad
    \text{and} \quad
    \text{\emph{(ii)}}\; d_d \leq \tau^a\sqrt{\ell h} + \tau^r \rho  \frob{\D \circ \scaledU^{k+1}}\,.
\end{equation}

The LinSEPAL-ADMM algorithm is summarized in \cref{alg:linsepal_admm}.

\begin{algorithm}[H]
\caption{LinSEPAL-ADMM}
\label{alg:linsepal_admm}
\begin{algorithmic}[1]
\STATE \textbf{Input:} $\covlow$, $\covhigh$, $\D$, $\lambda$, $\rho$, $\tau^a$, $\tau^r$
\STATE Initialize: $\V^0 \in \stiefel{\ell}{h}$, $\Y^0 \in \rmatdim$, $\scaledU^0 \gets \D \odot \V^0 - \Y^0$
\REPEAT
    \STATE $\V^{k+1} \gets \text{Solve \cref{eq:updateV} via an off-the-shelf method for smooth Riemannian problems}$ 
    \STATE $\Y^{k+1} \gets \mathcal{S}_{\lambda/\rho}\left( \D \odot \V^{k+1} + \scaledU^k \right)$
    \STATE $\scaledU^{k+1} \gets \scaledU^k + \D \odot \V^{k+1} - \Y^{k+1}$
\UNTIL{\Cref{eq:convergence_linsepal} is satisfied}
\STATE \textbf{Output:} $\V$, $\Y$, $\scaledU$
\end{algorithmic}
\end{algorithm} 
