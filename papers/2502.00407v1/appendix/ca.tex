% !TEX root =  ../main.tex

\section{Causality and causal abstraction.}\label{app:CA}
This section provides additional definitions and examples related to SCMs and the CA framework.

\subsection{Mixing functions}
A set of structural function in a Markovian SCM can be reduced to a set of mixing functions dependent only on the exogenous variables. 

Given an SCM $\scm^n$, recall that $\myfunctional$ is a set of $n$ functional assignments which define the values $X_i=f_i(\parents_i, Z_i)$, $\forall \; i \in [n]$, with $ \parents_i \subseteq \myendogenous \setminus \{ X_i\}$. 
Denote by $\myexogenous^{\mathcal{A}_i} \subseteq \myexogenous \setminus \{ Z_i\}$ the set of exogenous variables corresponding to the ancestors of $X_i$, where $\ancestors_i \subseteq [n] \setminus \{i\}$.
According to \myfunctional, we can identify a set of mixing functions $\mymixing=\{m_1, \ldots, m_n\}$ such that the values of the endogenous random variables are equivalently expressed as $x_i=m_i\left(\{z_j\}_{j\in \ancestors_i}, z_i\right)$, $\forall \; i \in [n]$. 

Further, we can also characterize the product probability measure implied by the SCM purely in terms of the exogenous variables, viz. $\chi^\myendogenous=\prod_{i \in [n]} P\left(X_i | \myexogenous^{\ancestors_i}, Z_i\right)$. 

As an example, consider a causal relation $x_1 \rightarrow x_2$.
%\todo[from=FMZ]{If we need space we can cut on the examples}
In the linear SCM with additive noise \citeSupp{bollen1989structuralSupp,shimizu2006linearSupp} setting we have
\begin{equation}
    \begin{cases}
        x_1=z_1\,,\\
        x_2=c_{2,1} x_1 + z_2 = c_{2,1} z_1 + z_2\,.          
    \end{cases}
\end{equation}
Again, for the post-nonlinear model \citeSupp{zhang2012identifiabilitySupp}, we get
\begin{equation}
    \begin{cases}
        x_1=f_{1,1}(z_1)=m_{1,1}(z_1)\,,\\
        \begin{aligned}
            x_2&=f_{2,2}(f_{2,1}(x_1) + z_2)\\ 
            &= (f_{2,2}\circ f_{2,1} \circ f_{1,1})(z_1) + f_{2,2}(z_2)\\
            &= m_{2,1}(z_1) + m_{2,2}(z_2)\,. 
        \end{aligned}    
    \end{cases}
\end{equation}

\subsection{Interventional consistency}
A typical requirement imposed on CA maps is that they act in a consistent way with respect to interventions \citeSupp{rischel2020categorySupp}. 

\begin{definition}[Interventional consistency]\label{def:interv_consistency}
    Given an $\abst$-abstraction between $\mathsf{M}^\ell$ and $\mathsf{M}^h$ and a set $\mathcal{I}$ of hard interventions on $\mathcal{X}^h_{\mathcal{I}}\subseteq \datahigh$, the abstraction is \emph{interventionally consistent} if, for any intervention in $\mathcal{I}$ and for every set of target variable $\mathcal{Y}^h_{\mathcal{I}}\subseteq \datahigh \setminus \mathcal{X}^h_{\mathcal{I}}$, the following diagram commutes:

    \begin{center}
    \begin{tikzpicture}[shorten >=1pt, auto, node distance=1cm, thick, scale=1.0, every node/.style={scale=1.0}]
    
    \node[] (M0_0) at (0,0) {$\dom{\mathcal{X}^\ell_\mathcal{I}}$};
    \node[] (M0_1) at (4,0) {$\dom{\mathcal{Y}^\ell_\mathcal{I}}$};
    \node[] (M1_0) at (0,-1.75) {$\dom{\mathcal{X}^h_\mathcal{I}}$};
    \node[] (M1_1) at (4,-1.75) {$\dom{\mathcal{Y}^h_\mathcal{I}}$};
    
    \draw[->, draw=mypurple]  (M0_0) to node[above,font=\small]{$P(\mathcal{Y}^\ell_\mathcal{I} \vert \doint(\mathcal{X}^\ell_\mathcal{I}))$} (M0_1);
    \draw[->, draw=cyan]  (M1_0) to node[below,font=\small]{$P(\mathcal{Y}^h_\mathcal{I}\vert \doint(\mathcal{X}^h_\mathcal{I}))$} (M1_1);
    \draw[->, draw=cyan]  (M0_0) to node[left,font=\small]{$\alphamap{\mathcal{X}^h_\mathcal{I}}$} (M1_0);
    \draw[->, draw=mypurple]  (M0_1) to node[right,font=\small]{$\alphamap{\mathcal{Y}^h_\mathcal{I}}$} (M1_1);
    
    \end{tikzpicture}
    \end{center}
    or equivalently,
    \begin{equation}%\label{eq:abserr}
        \alphamap{\mathcal{Y}^h_\mathcal{I}}(P(\mathcal{Y}^\ell_\mathcal{I} \vert \doint(\mathcal{X}^\ell_\mathcal{I}))) =  P(\mathcal{Y}^h_\mathcal{I} \vert \alphamap{\mathcal{X}^h_\mathcal{I}}(\doint(\mathcal{X}^h_\mathcal{I}))),
    \end{equation}

    where $\mathcal{X}^\ell_\mathcal{I}=\amap^{-1}(\mathcal{X}^h_\mathcal{I})$ and $\mathcal{Y}^\ell_\mathcal{I}=\amap^{-1}(\mathcal{Y}^h_\mathcal{I})$.  
\end{definition} 
Essentially, commutativity suggests that we obtain equivalent intervention outcomes in two different ways: \mypurple{\emph{(i)}} either by intervening on the low-level model and then abstracting or, \cyan{\emph{(ii)}} by abstracting to the high-level model and then intervening in an equivalent fashion.

\subsection{Linear abstraction}
The class of abstractions may be restricted by an assumption of the form of the abstraction map \citeSupp{massidda2024learningcausalabstractionslinearSupp}:

\begin{definition}[Linear abstraction]\label{def:lca}
Given an $\abst$-abstraction $\abst = \langle \Rset, \amap, \alphamap{} \rangle$ from $\mathsf{M}^\ell$ to $\mathsf{M}^h$, the abstraction is linear if $\alphamap{} = \V^\top \in \reall^{h \times \ell}$.
\end{definition}

\subsection{Constructive abstraction}
A particularly well-behaved form of abstraction is a constructive abstraction. In the context of the $\tau$-abstraction framework \citeSupp{beckers2019abstractingSupp}, a constructive abstraction is an abstraction such that: \emph{(i)} the variable mapping defines a clustering of the low-level variables (\emph{constructivity}); \emph{(ii)} consistency holds for all high-level interventions (\emph{strongness}); \emph{(iii)} the value map is surjective and it implies a map between exogenous values and between interventions ($\tau$-\emph{abstraction}). In the $\alpha$-framework a few of these properties hold by construction; thus, we define a constructive abstraction as \citeSupp{schooltink2024aligningSupp}: 

\begin{definition}[Constructive abstraction]\label{def:cca}
Given an $\abst$-abstraction $\abst = \langle \Rset, \amap, \alphamap{} \rangle$ from $\mathsf{M}^\ell$ to $\mathsf{M}^h$, the abstraction is constructive if the abstraction is interventionally consistent and implies the existence of a map ${\alphamap{}}_U: \myexogenous^\ell \rightarrow \myexogenous^h$ between exogenous variables.
\end{definition}

\subsection{Measure-theoretic definition of an SCM}\label{subsec:SCM_prob}

Any SCM can be defined in terms of the probability measure spaces underlying it:

\begin{definition}[Measure-theoretic SCM]\label{def:SCM_prob}
    A (Markovian) SCM $\scm^n$ is a triple $\langle (\myexogenousvals,\, \Sigma_{\myexogenousvals}, \zeta), \, (\myendogenousvals,\, \Sigma_{\myendogenousvals}, \chi)\, , \mymixing \rangle$  where:
    \begin{squishlist}
        \item $(\myexogenousvals,\, \Sigma_{\myexogenousvals}, \zeta)$ is a probability space associated with exogenous variables. Specifically, it consists of the product probability measure $\zeta=\zeta_1 \times \ldots \times \zeta_n$ on the product measurable space $(\myexogenousvals,\, \Sigma_{\myexogenousvals})$ where $\myexogenousvals = \myexogenousvals_1 \times \ldots \times \myexogenousvals_n$ is a product set and $ \Sigma_{\myexogenousvals} =  \Sigma_{\myexogenousvals_1} \otimes \ldots \otimes \Sigma_{\myexogenousvals_n}$ is a product $\sigma$-algebra.
        The probability measure is such that, for each $\mathcal{W}_1 \in \Sigma_{\myexogenousvals_1}, \ldots, \, \mathcal{W}_n \in \Sigma_{\myexogenousvals_n}$, we have 
        \begin{equation}
            \zeta_1 \times \ldots \times \zeta_n (\mathcal{W}_1 \times \ldots \times \mathcal{W}_n)=\zeta_1(\mathcal{W}_1) \times \ldots \times \zeta_n(\mathcal{W}_n)\,;
        \end{equation}
        \item $(\myendogenousvals,\, \Sigma_{\myendogenousvals}, \chi)$ is a probability space associated with endogenous variables consisting of a joint probability measure $\chi$ on the product measurable space $(\myendogenousvals,\, \Sigma_{\myendogenousvals})=(\myendogenousvals_1 \times \ldots \times \myendogenousvals_n,\, \Sigma_{\myendogenousvals_1} \otimes \ldots \otimes \Sigma_{\myendogenousvals_n})$;
        \item \mymixing is a set of $n$ mixing measurable maps $\varphi^{m_i}$ (cf. \Cref{def:SCM}) such that the joint probability measure $\chi$ factorizes as 
        \begin{equation}
            \chi = \bigtimes_{i=1}^n \varphi^{m_i}_{\#}\left(\mu_i \left( \myexogenousvals_i \times \myexogenousvals^{\ancestors_i} \right) \right)\,;       
        \end{equation}
        where $\myexogenousvals^{\ancestors_i}=\bigtimes_{j \in \ancestors_i} \myexogenousvals_j$, and, denoting by $\Sigma_{\myexogenousvals^{\ancestors_i}}=\bigotimes_{j \in \ancestors_i} \Sigma_{\myexogenousvals_j}$, $\mu_i$ is a probability measure on the product measurable space $\left( \myexogenousvals_i \times \myexogenousvals^{\ancestors_i},\, \Sigma_{\myexogenousvals_i} \otimes \Sigma_{\myexogenousvals^{\ancestors_i}} \right)$.
    \end{squishlist}
\end{definition}