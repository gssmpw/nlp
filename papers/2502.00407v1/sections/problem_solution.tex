% !TEX root =  ../main.tex
\section{Problem solution}\label{sec:problem_solution}
To solve the nonsmooth and smooth Riemannian problems in \cref{sec:problem_formulation}, we leverage the following:

\begin{restatable}{proposition}{smoothnessth}\label{prop:smoothness_and_differentiability}
    Consider the function
    \begin{equation}\label{eq:general_objective}
        f(\A)\!=\!\Tr{\!\left(\A^\top \covlow \A \right)^{-1} \! \covhigh} + \log\det{\A^\top \covlow \A }\,.
    \end{equation}
    \Cref{eq:general_objective} is smooth for $\A \in \stiefel{\ell}{h}$.
    Additionally, define $\widetilde{\mathbf{A}}\coloneqq\left(\mathbf{A}^\top \covlow \mathbf{A}\right)^{-1}$.
    The gradient of $f\left(\A\right)$ is
    \begin{equation}\label{eq:gradA}
        \Egrad{\A}{f} = 2\left(\covlow\mathbf{A}\widetilde{\mathbf{A}}\right)\left(\identity_h - \covhigh\widetilde{\mathbf{A}}\right)\,,
    \end{equation}
\end{restatable}
\begin{proof}
See \cref{app:proof}.
\end{proof}

\subsection{Solution of the nonsmooth learning problem}\label{subsec:sol_nonsmoot}
Leveraging \cref{prop:smoothness_and_differentiability}, we have that \cref{eq:minKL} is constituted by a smooth yet nonconvex term, $f(\V)$, and a nonsmooth one, $h(\V)$. Hence we solve \cref{prob:nonsmooth} through two different optimization paradigms for nonsmooth Riemannian optimization: MADMM and ManPG.
We term the proposed methods \emph{LinSEPAL-ADMM} and \emph{LinSEPAL-PG}, where \textit{LinSEPAL} stands for \textbf{Lin}ear \textbf{S}emantic \textbf{E}mbedding \textbf{P}rinciple \textbf{A}bstraction \textbf{L}earner.
Next we provide a sketch of the solution and provide the full mathematical derivation in \cref{app:MADMM} and \Cref{app:ManPG}.

\spara{LinSEPAL-ADMM.} The MADMM framework appeals to our setting given the objective function separating into smooth and nonsmooth terms.
To derive the LinSEPAL-ADMM iterative algorithm, we proceed as follows.
First, the nonsmooth term $h(\V)$ is associated with a splitting variable \Y to be optimized over \rmatdim, obtaining an equivalent problem formulation (cf. \cref{eq:MADMM}). LinSEPAL-ADMM proceeds by iteratively minimizing the augmented Lagrangian with respect to the primal variables \V and \Y, while maximizing w.r.t. the scaled dual variable. 
Specifically, LinSEPAL-ADMM solves the subproblem for \V (cf. \Cref{eq:updateV}) through standard techniques for smooth optimization on the Stiefel manifold (e.g., conjugate gradient, \citealp{edelman1998geometry}).
This is the most complex update in the LinSEPAL-ADMM iterative procedure due to the nonconvex objective and the Stiefel manifold.
Next, LinSEPAL-ADMM updates \Y in closed form through the element-wise soft-thresholding operator (cf. \Cref{eq:updateY_madmm}). Finally, the scaled dual variable is updated by adding the primal residual evaluated at the current solution (cf. \cref{eq:MADMMrecursion_app}).
The stopping criteria for LinSEPAL-ADMM are established according to primal and dual feasibility optimality conditions (\citealp{boyd2011distributed}, cf. \cref{app:MADMM}).
To the best of our knowledge, the convergence guarantee for MADMM in the Riemannian space has not been proven.
Consequently, the same holds for LinSEPAL-ADMM.
\Cref{alg:linsepal_admm} summarizes the method.\\

\spara{LinSEPAL-PG.} Our LinSEPAL-PG is an iterative algorithm alternating two updates (cf. \cref{eq:ManPG_app}).
The first update is the proximal mapping providing a proximal gradient direction $\G^k$ onto the tangent space to the Stiefel manifold, using the first-order approximation of the objective around the $k$-th estimate.
The second is the update for $\V^{k+1}$, which exploits the canonical retraction (cf. \cref{eq:stiefel_retractions}) technique for projecting back $\V^k + \G^k$ from the tangent space to the manifold. The hardest step in the LinSEPAL-PG algorithm is the proximal update (cf. \cref{eq:ManPGU1}).
We solve it by declining the regularized semi-smooth Newton method \cite{xiao2018regularized} to our application (cf. \cref{app:ManPG}).
Following the rationale in \cite{si2024riemannian}, differently from the original ManPG method which uses the parameterization of the tangent space, we constrain $\G^k$ to the tangent space by exploiting the basis of the normal space to the manifold (cf. \Cref{eq:basis_nvk}).
This way, we ease the mathematical solution, with benefits from the computational perspective (cf. \citealp{si2024riemannian}).
Next, LinSEPAL-PG updates $\V^{k+1}$ in closed form (cf. \cref{eq:updateV_linsepalpg}) by applying the QR-retraction, employing an Armijo line-search procedure to determine the stepsize.
The optimization stops either when a maximum number of iterations is reached, or when $\KL{\V^{k+1}}$ is below a threshold $\tau^{\mathrm{KL}}\approx 0$.
LinSEPAL-PG inherits the global convergence of the ManPG framework, established in \cite{chen2020}.
\Cref{alg:linsepal_pg} summarizes the method.

\subsection{Solution of the smooth learning problem}\label{subsec:sol_smooth}
We provide a sketch of the solution below and the full mathematical derivation in \Cref{app:MADMMSCA_partial}.
In this case, we want to jointly optimize \Supp and \V, both being components of the linear CA, viz. $(\B \odot \Supp \odot \V)^\top$.
Hence, unlike the nonsmooth case, we constrain to the Stiefel manifold the product $(\B \odot \Supp \odot \V)$. \black{To solve \Cref{prob:nonconvex_prob_approx}, we combine the SOC, ADMM, and SCA methods.}
According to the rationale behind SOC, we add two splitting variables, namely \YO and \YT in \stiefel{\ell}{h} (cf. \cref{eq:splitting_constraints_partial}), to separate the nonconvexity of the objective function from that induced by the manifold.
The reason why we have two splitting variables is that we need to take into account the bilinear form of the first constraint in \Cref{eq:prob_madmmsca_VS}.
Additionally, to manage the second constraint in \Cref{eq:prob_madmmsca_VS}, we introduce another splitting variable $\X \in \sphere{h}{\ell}$.
Starting from the equivalent problem formulation (cf. \Cref{eq:prob_madmmsca_VS_with_splitting}), we write the (nonconvex) scaled augmented Lagrangian (cf. \Cref{eq:scaledAUL_partial}), thus arriving at the update recursion for our proposed method (cf. \Cref{eq:ADMM_partial}).
We term the latter \emph{CLinSEPAL} (Constructive LinSEPAL) to highlight that it returns constructive support for CA.
CLinSEPAL proceeds by iteratively minimizing the augmented Lagrangian w.r.t. the primal variables \V, \Supp, \YO, \YT, and \X; and maximizing it w.r.t. the scaled dual ones.
In the subproblems for \V (cf. \Cref{eq:updateV_nonconvex_partial}) and \Supp (cf. \Cref{eq:updateS_nonconvex_partial}), we adopt the SCA paradigm to manage the nonconvexity of $f(\V,\Supp^k)$ and $f(\V^{k+1},\Supp)$, respectively.
By exploiting the smoothness of $f(\V,\Supp)$ (cf. \cref{corollary:objective_partial_knowledge}), the strongly convex surrogates are derived around the current solution (cf. \cref{eq:strongly_convex_surrogate_Vpartial,eq:strongly_convex_surrogate_Spartial}).
CLinSEPAL solves the strongly convex surrogate subproblems (cf. \cref{eq:SCA_recursion_V,eq:SCA_recursion_S}) exactly.
Due to the presence of the inequality constraints, the subproblem for \Supp is a constrained quadratic programming problem.
CLinSEPAL solves it via standard techniques (e.g., \citealp{osqp}).
These two steps in CLinSEPAL can be seen as an instance of the linearized ADMM framework (Alg.1 in \citealp{lu2021linearized}) where each internal update is solved exactly. Next, CLinSEPAL solves in closed-form the updates for the three splitting variables.
Indeed, the subproblems for \YO and \YT amount to the \emph{closest orthogonal approximation problem} \cite{fan1955some,higham1986computing}, whose solution is obtained in closed form via polar decomposition.
Subsequently, the subproblem for \X is solved in closed-form according to \Cref{lemma:proximal_spDelta}.
Finally, the scaled dual variables are updated with the corresponding primal residuals.
Empirical convergence for CLinSEPAL is established when the norms of primal (cf. \cref{eq:primal_res_partial}) and dual (cf. \cref{eq:dual_res_partial}) residuals vanish, in accordance with absolute and relative tolerances (cf. \cref{eq:stopping_criteria_partial}).
\Cref{alg:clinsepal} summarizes the method.
Additionally, \Cref{subsec:CLinSEPAL_full_prior} details the solution in the special case of full prior knowledge.