% !TEX root =  ../main.tex
\begin{figure}[t]
    \centering
    \includegraphics[width=.9\columnwidth]{figs/20_full_prior_synthetic_exp.pdf}
    \caption{
    Synthetic fp results for all settings $(\ell, h)$ and methods: \emph{(i)} fraction of learned CAs that are constructive, \emph{(ii)} $\KL{\Vhat}$, \emph{(iii)} normalized  absolute Frobenius distance from $\V^\star$, and \emph{(iv)} $\mathrm{F1}$ score.
    }
    \label{fig:full_synth_data}
\end{figure}
\section{Empirical assessment on synthetic data}\label{sec:empirical_assessment}
This section provides the empirical assessment of LinSEPAL-ADMM, LinSEPAL-PG and CLinSEPAL with different degrees of prior knowledge,
from full (\emph{fp}) to partial (\emph{pp}). 
We monitor four metrics to evaluate the learned CA $\Vhat^\top$:  
\emph{(i)} \emph{constructiveness}, as required by \cref{def:semantic_embedding_principle_linear};  
\emph{(ii)} $\KL{\Vhat}$ evaluating the alignment between $\varphi^{\Vhat}_{\#}(\measurelow)$ and \measurehigh; 
\emph{(iii)} the Frobenius distance between the absolute value of \Vhat and that of the ground truth $\V^\star$, normalized by \frob{\V^\star} to make the settings comparable; 
\emph{(iv)} the $\mathrm{F1}$ score computed using the support of the learned CAs and that of $\V^\star$ to evaluate structural interventional consistency.
\cref{app:metrics} provides the definition for the above metrics and the hyper-parameters values used in the experiments.

\spara{Full prior knowledge.}
In the fp case, we investigate three different settings $(\ell,h)\in \{(12,2), (12,4), (12,6)\}$, corresponding to the cases of \emph{high}, \emph{medium-high}, and \emph{medium} coarse-graining.
We do not consider the case where $h>\ell/2$ since the abstraction for $h-\ell/2$ nodes of the low-level model would be fully specified due to the availability of full prior knowledge.
For each setting, we instantiate $S=30$ ground truth abstractions $\V^\star$, and for each simulation $s \in [S]$ we run all the methods $R=50$ times, with different initializations.
Then, for each $s$ and method, we retain the \Vhat minimizing the objective $\KL{}$.\\
\Cref{fig:full_synth_data} shows the performance of the tested methods. 
All the methods provide constructive CAs $\forall \, s \in [S]$, and reach a good level of alignment in terms of $\KL{\Vhat}$.
Recall that, while CLinSEPAL and LinSEPAL-ADMM stop the learning procedure according to primal and dual residuals convergence, LinSEPAL-PG exits when $\KL{\Vhat}$ is below a certain threshold $\tau^{\mathrm{KL}}$ (in the experiments $\tau^{\mathrm{KL}}=10^{-4}$). 
The Frobenius absolute distance shows comparable performances for the three methods, although CLinSEPAL and LinSEPAL-ADMM outperform in case $(\ell, h)=(12,4)$.
This metric tells us that, as $h$ increases, the learned \Vhat tends (in absolute terms) to the ground truth.
Interestingly, when $(\ell, h)=(12,2)$ we observe a high distance from $\V^\star$, although the learned \Vhat has the correct structure (cf. $\mathrm{F1}$ score).
This suggests that under a high coarse-graining, the size of \myker \KL{} grows, and it is more difficult for our methods to estimate $\V^\star$ under \hyperlink{(NA1)}{(NA1)}-\hyperlink{(NA5)}{(NA5)}.
Finally, the $\mathrm{F1}$ score confirms that the methods guarantee the true CA structure of \Vhat, for all the settings.
To sum up, CLinSEPAL and LinSEPAL-ADMM are slightly better choices than LinSEPAL-PG in case of full prior knowledge in our experimental setting.

\begin{figure}[t]
    \centering
    \includegraphics[width=.9\columnwidth]{figs/20_partial_prior_synthetic_exp.pdf}
    \caption{Synthetic pp results for setting $(\ell, h)=(4,2)$, all methods, and prior knowledge amounting to the correct structural mapping for $25\%$, $50\%$, or $75\%$ of the nodes. All plots as in \cref{fig:full_synth_data}.
    }
    \label{fig:partial_synth_data}
\end{figure}

\spara{Partial prior knowledge.}
In the pp case, we consider the setting $(\ell,h)\in \{(4,2)\}$ and simulate partial prior knowledge by forgetting the mapping for $25\%$, $50\%$, and $75\%$ of the variables. For each setting, we instantiate $S=30$ ground truth abstractions $\V^\star$, and for each simulation $s \in [S]$ we run all the methods $R=30$ times, with different initializations.\\
In \cref{fig:partial_synth_data}, the first plot immediately shows that only CLinSEPAL consistently returns a constructive linear CA, as guaranteed by its formulation in \cref{prob:nonconvex_prob_approx}. 
We decided to consider methods performing under a threshold of $90\%$ to be unreliable in returning constructive CAs and not to report their remaining metrics.
In the case of a limited drop of prior knowledge ($25\%$) all methods perform well, similarly to the fp case, with CLinSEPAL and LinSEPAL-ADMM slightly outperforming LinSEPAL-PG.
With a higher drop ($50\%$), LinSEPAL-PG fails to achieve our constructiveness threshold, while CLinSEPAL and LinSEPAL-ADMM still perform well, although LinSEPAL-ADMM provides a lower fraction of constructive CAs. 
Finally, with the highest drop ($75\%$) CLinSEPAL succeeds in learning a constructive CA and lowering $\KL{}$, even if the Frobenius absolute distance slightly increases. 
To sum up, for the pp setting only CLinSEPAL guarantees a constructive abstraction.