% !TEX root =  ../main.tex
\section{Problem formulation}\label{sec:problem_formulation}
Within our category-theoretic framework, CA learning amounts to finding the endogenous components $\amap_\myendogenous$ and $\alphamap{\myendogenous}$ from data.
We start by formulating a \emph{general} learning problem working under the non-assumption \hyperlink{(NA1)}{(NA1)}-\hyperlink{(NA5)}{(NA5)}, and then decline it to the case of linear CA.\\
Our problem formulation relies upon three key ingredients.
\textit{First}, we assume that the data generated by a constructive abstraction adheres to the \emph{semantic embedding principle}. This principle requires that the CA component $\alphamap{\myendogenous}$ admits a right-inverse measurable map. 
\begin{definition}[Semantic embedding principle, SEP]\label{def:semantic_embedding_principle}
    Given an $\abst$-abstraction as in \cref{def:alpha_abstraction_prob}, the semantic embedding principle states that $\alphamap{\myendogenous}$ has a right-inverse measurable map $\beta_{\myendogenous}$, such that $\alphamap{\myendogenous} \circ \beta_{\myendogenous} = \mathrm{Id}_{\left(\myendogenousvals^{h},\, \Sigma_{\myendogenousvals^h}, \measurehigh\right)}$.
    Hence, it holds 
    \begin{equation}\label{eq:semantic_embedding}
        \measurehigh=\varphi^{\alphamap{\myendogenous} \circ \beta_{\myendogenous}}_{\#}(\measurehigh)\,.
    \end{equation}
\end{definition}
The SEP implies that going from the high-level model $\scm^{h}$ to the low-level model $\scm^{\ell}$ and then abstracting back to $\scm^{h}$ allows for perfect reconstruction. 
Notice that SEP only holds in one direction, as suggested by the word embedding; thus, identity on the left inverse is not guaranteed, meaning that the abstraction from the low level to the high level can still shed information, as we would expect in CA.\\
\textit{Second}, because of the non-assumption \hyperlink{(NA3)}{(NA3)} only observational data is available. 
Thus, we can not explicitly use interventional consistency information to drive our learning. 
Only if we identify the true constructive abstraction, we are guaranteed interventional consistency. 
In trying to learn the abstraction, we leverage \hyperlink{(A1)}{(A1)}, which is met in application domains as discussed in \Cref{sec:intro}.\\
\textit{Third}, to learn a CA, we look for a distance function quantifying the misalignment between the probability measures \measurelow and \measurehigh, given $\alphamap{\myendogenous}$.
Since the probability measures belong to spaces of different dimensionality, specifically $\reall^\ell$ and $\reall^h$, we leverage the approach proposed in \cite{cai2022distancesprobabilitydistributionsdifferent} to compute the misalignment through an embedding as $D\left(\measurehigh,\varphi_{\#}^{\alphamap{\myendogenous}}(\measurelow)\right)$, where $D$ is an information-theoretic metric (e.g., $\mathrm{p}$-Wasserstein) or $\phi$-divergence (e.g., Kullback-Leibler).
Please refer to \cref{app:infotheory} for more details.
We can now pose the following general learning problem:
\begin{center}
\scalebox{.97}{
\begin{tcolorbox}[
    colback=white,
    colframe=black,
    boxrule=0.8pt,
    before=\par\smallskip\centering,
    after=\par\smallskip,
]

\begin{problem}\label{prob:calsep}
(SEP-based CA Learning)\\
\textbf{Input}: (i) probability measures $\measurelow$ and $\measurehigh$; (ii) prior information about $\amap_\myendogenous$, and (iii) a distance function $D\left(\measurehigh,\varphi_{\#}^{\alphamap{\myendogenous}}(\measurelow)\right)$.

\textbf{Goal}: learn a measurable map $\alphamap{\myendogenous}^\star$ such that (i) it belongs to $\myker\,D\left(\measurehigh,\varphi_{\#}^{\alphamap{\myendogenous}}(\measurelow)\right)$, (ii) it complies with SEP in \Cref{def:semantic_embedding_principle}, and (iii) it agrees with the prior information about $\amap_\myendogenous$.
\end{problem}
\end{tcolorbox}
}
\end{center}

\black{The zeroing of the distance function implies $\chi^\ell=\varphi^{\alphamap{\myendogenous}^\star}_{\#}(\measurelow)$, which, together with \Cref{eq:semantic_embedding}, yields $\varphi^{\alphamap{\myendogenous}^\star \circ \beta_{\myendogenous}}_{\#}(\measurehigh)=\varphi^{\alphamap{\myendogenous}^\star}_{\#}(\measurelow)\,.$} However, despite solving \cref{prob:calsep}, there is no guarantee that $\alphamap{\myendogenous}^\star$ coincides with the ground truth CA. In other words, the optimal solution is not unique.
For a linear constructive CA, we express $\amap_\myendogenous$ and $\alphamap{\myendogenous}$ as $\B^\top\in\{0,1\}^{h \times \ell}$ and $\V^\top \in \reall^{h\times\ell}$, respectively.
In accordance with constructivity, each row of \B has a single nonzero entry, and each column has at least one nonzero entry. Importantly, for linear CA, a simple yet principled way to satisfy SEP is %to link CA to 
via the geometry of the Stiefel manifold:
\begin{equation}\label{eq:stiefel}
    \stiefel{\ell}{h} \coloneqq \{ \V \in \reall^{\ell \times h} \, \mid \, \V^\top\V = \identity_h \}\,.
\end{equation}
The Stiefel manifold (see \Cref{app:stiefel} for details), is a convenient choice for the following reasons: \emph{(i)} differently from a generic pseudo-inverse matrix, the orthogonality of \V guarantees that the geometry of the high-level space is preserved; \emph{(ii)} the transpose eases the formulation and ensures numerical stability in optimization. Consequently, we restate SEP for the linear case as follows.
\begin{definition}[Semantic embedding principle, linear case]\label{def:semantic_embedding_principle_linear}
    Given the linear constructive CA, viz. $\V^\top$, SEP implies that $\V \in \stiefel{\ell}{h}$. From \cref{eq:semantic_embedding} we get $\chi^h=\varphi^{\V \circ \V^\top}_{\#}(\chi^h)$.
\end{definition}

\black{A pictorial representation of \Cref{def:semantic_embedding_principle_linear} is provided in \Cref{fig:fig1}.}
\Cref{def:semantic_embedding_principle_linear} shapes our methodology for CA learning, posing it as a Riemannian optimization problem \cite{boumal2023introduction}.\\
As an application, in the sequel, we will tackle an implementation of \Cref{prob:calsep} for the linear constructive case $\alphamap{\myendogenous}=\V^\top$, where \emph{(i)} $\measurehigh \sim N(\zeros_h, \covhigh)$ and $\measurelow \sim N(\zeros_\ell, \covlow)$; and \emph{(ii)} $D\left(\measurehigh,\varphi_{\#}^{\V}(\measurelow)\right)=D^{\mathrm{KL}}\left(\measurehigh || \varphi_{\#}^{\V}(\measurelow)\right)$ where $D^{\mathrm{KL}}$ stands for KL divergence.
Specifically, %in our case we have
\begin{equation}\label{eq:KL}
        \KL{\V}\!\!=\!\!\Tr{\!\!\left( \V^\top \! \covlow \V\right)^{-1} \!\covhigh \!} + \log\det{\! \V^\top \! \covlow \V \! } + C\,,
\end{equation}
where $C$ is a constant term.
Additionally, from \cref{eq:KL} it is immediate to see that both $\V^\star$ and $-\V^\star$ belong to \myker \KL{\V}.
Such an application is highly relevant as it is common to deal in practice with Gaussian measures (or quasi) \cite{gabriele2024extracting}; also, in causality, such a measure easily arises from the prominent family of linear models \cite{bollen1989structural,shimizu2006linear} and is investigated in the CA literature \cite{kekic2023targeted,massidda2024learningcausalabstractionslinear}.
KL divergence is a common choice in ML and statistics, but notice that any distance vanishes when evaluated at the ground truth.

\begin{remark}\label{reamrk:poss_existence}
    From \Cref{eq:KL}, it is immediate to derive a criterion to decide on the existence of a linear constructive CA adhering to SEP. For zero-mean Gaussian measures, the variance provides all the relevant information about the data, and via the eigendecomposition we can compute the eigenvalues quantifying the variance associated with each eigenvector. Thus, a linear constructive CA adhering to SEP \emph{might exist} if the eigenvalues of \covhigh are within the range $[\lambda_{\mathrm{min}}^\ell, \lambda_{\mathrm{max}}^\ell]$, where $\lambda_{\mathrm{min}}^\ell$ and $\lambda_{\mathrm{max}}^\ell$ are the minimum and maximum eigenvalues of \covlow.
\end{remark}

We investigate two approaches for injecting the prior information about $\amap_\myendogenous$, encoded in the matrix of \emph{prior knowledge} \B, into our problem.
Please notice that in case \B is not fully specified, it might not comply with the row and column constraints discussed above.
These formulations translate into non-smooth and smooth Riemannian learning problems.

\spara{Nonsmooth problem.} 
In the nonsmooth problem we introduce \B as a penalty term in the objective function. The rationale is to penalize entries in \V corresponding to zeros in \B. Let $\D=(\ones_{\ell \times h} - \B)$.
The problem reads as follows:
\begin{restatable}{problem}{nonsmoothprob}\label{prob:nonsmooth}
    Given $\covlow \in \pd^{\ell}$, $\covhigh \in \pd^{h}$, $\D \in \{0,1\}^{\ell \times h}$, and $\lambda \in \reall_+$, the CA is the transpose of
    \begin{equation}\label{eq:minKL}
        \V^\star = \argmin_{\V \in \stiefel{\ell}{h}} \; f(\V)  + \lambda \underbrace{\norm{\D \odot \V}_1}_{h(\V)}\,.
        % \tag{P1}
    \end{equation}
    % such that $\V^\star$ is constructive.
    Here, $f(\V)$ follows \cref{eq:KL}, omitting the constant $C$.
\end{restatable}
Please notice that, although appealing in its form, \Cref{eq:minKL} does not guarantee the constructiveness of the learned CA.
Moreover, the penalty term introduces a bias in the learned \V in the case of partial prior knowledge.

\spara{Smooth problem.}
In the smooth problem, we introduce \B directly in the objective function $f(\cdot)$. The CA is now defined as the Hadamard product of $\V$ and the support $(\B \odot \Supp)$ integrating prior $\B$ and learned $\Supp$ knowledge. 
This formulation is particularly convenient as it enables us to jointly optimize for $\V \in \rmatdim$ and matrix $\Supp \in \umatdim$. However, we also need to introduce three constraints: 
\blue{\emph{(i)}} by SEP, $\B \odot \Supp \odot \V$ must belong to the Stiefel manifold; 
\blue{\emph{(ii)}} by functionality, the rows of the support $(\B \odot \Supp)^\top$ must sum up to one, meaning that they lie on a sphere, defined as $\sphere{h}{\ell} \coloneqq \Big\{\mathbf{A} \in \{0,1\}^{h\times\ell} \mid  \norm{\mathbf{a}_j}_2=1 \text{ and } \sum_{i=1}^h a_{ij}=1,\,\,\forall j \in [\ell]  \Big\}\,;$ \blue{\emph{(iii)}} by surjectivity, the columns of the support $(\B \odot \Supp)$ must contain at least a one. 
The problem reads as:
\begin{restatable}{problem}{smoothpartial}\label{prob:nonconvex_prob_approx}
Given $\covlow \in \pd^{\ell}$, $\covhigh \in \pd^{h}$, and $\B \in \lmatdim$, the linear constructive CA is given by the transpose of the product $\B \odot \Supp \odot \V$, where 
    \begin{equation}\label{eq:prob_madmmsca_VS}
        \begin{aligned}
            \V^\star, \Supp^\star = \argmin_{\substack{\V \in \rmatdim \\ \Supp \in \umatdim}} &\quad f(\V,\Supp)\,;\\
             \textrm{subject to} & \; \blue{(i)}\; \B \odot \Supp \odot \V \in \stiefel{\ell}{h}\,, \\
             & \; \blue{(ii)}\; \left(\B \odot \Supp\right)^\top \in \sphere{h}{\ell}\,, \\
             & \; \blue{(iii)}\; \ones_h - \left(\B \odot \Supp\right)^\top \ones_\ell \leq \zeros_h\,;
        \end{aligned}
    \end{equation}
    and
    \begin{equation}\label{eq:objective_partial_knowledge}
        \begin{aligned}
            f(\V,\Supp) \!\coloneqq\! & \Tr{\left(\left(\B \odot \Supp \odot \V\right)^\top\! \covlow \!\left(\B \odot \Supp \odot \V\right) \right)^{-1} \!\!\covhigh} \\+ 
            & \!\log\det {\left(\B \odot \Supp \odot\V\right)^\top \! \covlow \! \left(\B \odot \Supp \odot\V\right) }\,.
        \end{aligned}
    \end{equation}
\end{restatable}

Notice that the matrix $\Supp$ does not need to be a logical matrix; it is the product $\B \odot \Supp$ which must be logical. Also, if \B provides full prior knowledge about the structure, we have $\Supp \equiv \B$ and we do not need to learn \Supp.
This approach guarantees the ground-truth structure for the learned CA.
The full prior problem formulation is provided in \cref{subsec:CLinSEPAL_full_prior}.

\black{Unfortunately, both the Stiefel manifold in \Cref{eq:stiefel} and \KL{\V} in \Cref{eq:KL} are nonconvex in \V.
In the next section we devise methods suitable for this setting.}