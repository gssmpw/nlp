
\documentclass[11pt]{article}

% Change "review" to "final" to generate the final (sometimes called camera-ready) version.
% Change to "preprint" to generate a non-anonymous version with page numbers.
\usepackage[final]{acl}

% Standard package includes
\usepackage{times}
\usepackage{latexsym}


% For proper rendering and hyphenation of words containing Latin characters (including in bib files)
\usepackage[T1]{fontenc}
% For Vietnamese characters
% \usepackage[T5]{fontenc}
% See https://www.latex-project.org/help/documentation/encguide.pdf for other character sets

% This assumes your files are encoded as UTF8
\usepackage[utf8]{inputenc}

% This is not strictly necessary, and may be commented out,
% but it will improve the layout of the manuscript,
% and will typically save some space.
\usepackage{microtype}

% This is also not strictly necessary, and may be commented out.
% However, it will improve the aesthetics of text in
% the typewriter font.
\usepackage{inconsolata}

%Including images in your LaTeX document requires adding
%additional package(s)
\usepackage{graphicx}
\usepackage{amssymb}  % Provides \mathbb among other symbols


\usepackage{algorithm}
\usepackage{algorithmicx}
\usepackage{algpseudocode}
% \usepackage[linesnumbered, ruled, vlined]{algorithm2e}
\usepackage{tabularray}
\usepackage{caption}
\usepackage{subcaption}
% \usepackage{lmodern}
\usepackage{amsmath}
\usepackage{multirow}

% ,amsmath,graphicx, spconf}
% \usepackage{minted}

% If the title and author information does not fit in the area allocated, uncomment the following
%
%\setlength\titlebox{<dim>}
%
% and set <dim> to something 5cm or larger.

\title{BnTTS: Few-Shot Speaker Adaptation in Low-Resource Setting}

% Author information can be set in various styles:
% For several authors from the same institution:
% \author{Author 1 \and ... \and Author n \\
%         Address line \\ ... \\ Address line}
% if the names do not fit well on one line use
%         Author 1 \\ {\bf Author 2} \\ ... \\ {\bf Author n} \\
% For authors from different institutions:
% \author{Author 1 \\ Address line \\  ... \\ Address line
%         \And  ... \And
%         Author n \\ Address line \\ ... \\ Address line}
% To start a separate ``row'' of authors use \AND, as in
% \author{Author 1 \\ Address line \\  ... \\ Address line
%         \AND
%         Author 2 \\ Address line \\ ... \\ Address line \And
%         Author 3 \\ Address line \\ ... \\ Address line}

% jahid version 
\author{
\textbf{Mohammad Jahid Ibna Basher}\textsuperscript{1},
\textbf{Md Kowsher}\textsuperscript{2},
\textbf{Md Saiful Islam}\textsuperscript{1},
\textbf{Rabindra Nath Nandi}\textsuperscript{1}, \\
\textbf{Nusrat Jahan Prottasha}\textsuperscript{2},
\textbf{Mehadi Hasan Menon}\textsuperscript{1},
\textbf{Tareq Al Muntasir}\textsuperscript{1}, \\
\textbf{Shammur Absar Chowdhury}\textsuperscript{3},
\textbf{Firoj Alam}\textsuperscript{3},
\textbf{Niloofar Yousefi}\textsuperscript{2}, 
\textbf{Ozlem Ozmen Garibay}\textsuperscript{2} \\
\textsuperscript{1}Hishab Singapore Pte. Ltd, Singapore, 
\textsuperscript{2}University of Central Florida, USA \\
\textsuperscript{3}Qatar Computing Research Institute, Qatar
}




%\author{
%  \textbf{First Author\textsuperscript{1}},
%  \textbf{Second Author\textsuperscript{1,2}},
%  \textbf{Third T. Author\textsuperscript{1}},
%  \textbf{Fourth Author\textsuperscript{1}},
%\\
%  \textbf{Fifth Author\textsuperscript{1,2}},
%  \textbf{Sixth Author\textsuperscript{1}},
%  \textbf{Seventh Author\textsuperscript{1}},
%  \textbf{Eighth Author \textsuperscript{1,2,3,4}},
%\\
%  \textbf{Ninth Author\textsuperscript{1}},
%  \textbf{Tenth Author\textsuperscript{1}},
%  \textbf{Eleventh E. Author\textsuperscript{1,2,3,4,5}},
%  \textbf{Twelfth Author\textsuperscript{1}},
%\\
%  \textbf{Thirteenth Author\textsuperscript{3}},
%  \textbf{Fourteenth F. Author\textsuperscript{2,4}},
%  \textbf{Fifteenth Author\textsuperscript{1}},
%  \textbf{Sixteenth Author\textsuperscript{1}},
%\\
%  \textbf{Seventeenth S. Author\textsuperscript{4,5}},
%  \textbf{Eighteenth Author\textsuperscript{3,4}},
%  \textbf{Nineteenth N. Author\textsuperscript{2,5}},
%  \textbf{Twentieth Author\textsuperscript{1}}
%\\
%\\
%  \textsuperscript{1}Affiliation 1,
%  \textsuperscript{2}Affiliation 2,
%  \textsuperscript{3}Affiliation 3,
%  \textsuperscript{4}Affiliation 4,
%  \textsuperscript{5}Affiliation 5
%\\
%  \small{
%    \textbf{Correspondence:} \href{mailto:email@domain}{email@domain}
%  }
%}

\begin{document}
\maketitle
\begin{abstract}

% \textcolor{green}{Text-to-speech (TTS) systems have made significant strides in generating high-quality speech, yet much of this progress has been concentrated on high-resource languages, leaving low-resource languages like Bangla underrepresented}. 
This paper introduces BnTTS (\textbf{B}a\textbf{n}gla \textbf{T}ext-\textbf{T}o-\textbf{S}peech), the first framework for Bangla speaker adaptation-based TTS, designed to bridge the gap in Bangla speech synthesis using minimal training data. Building upon the XTTS architecture, our approach integrates Bangla into a multilingual TTS pipeline, with modifications to account for the phonetic and linguistic characteristics of the language. We pretrain BnTTS on 3.85k hours of Bangla speech dataset with corresponding text labels and evaluate performance in both zero-shot and few-shot settings on our proposed test dataset. Empirical evaluations in few-shot settings show that BnTTS significantly improves the naturalness, intelligibility, and speaker fidelity of synthesized Bangla speech. Compared to state-of-the-art Bangla TTS systems, BnTTS exhibits superior performance in Subjective Mean Opinion Score (SMOS), Naturalness, and Clarity metrics.


 
% The open-source code is available at Anonymized.
\end{abstract}

\input{sections/Introduction}
\input{sections/BnTTS_short}
\input{sections/Experiments}
\input{sections/Results}

\input{sections/Related_Works}
\section{Conclusion}
In this work, we introduced BnTTS, the first speaker-adaptive TTS system for Bangla, capable of generating natural and clear speech with minimal training data. Built on the XTTS pipeline, BnTTS effectively supports zero-shot and few-shot speaker adaptation, outperforming existing Bangla TTS systems in sound quality, naturalness, and clarity. Despite its strengths, BnTTS faces challenges in handling diverse dialects and short-sequence generation. Future work will focus on training BnTTS from scratch, developing medium and small model variants, and exploring knowledge distillation to optimize inference speed for real-time applications.

% In this work, we introduce BnTTS, the first speaker-adaptive TTS system for Bangla, achieving natural and clear speech synthesis with minimal training data. By adopting the XTTS pipeline, BnTTS supports zero-shot and few-shot speaker adaptation, outperforming existing Bangla TTS systems in sound quality, naturalness, and clarity. However, it faces limitations with diverse dialects, and struggles with short sequences. Future improvements will focus on training BnTTS from scratch, with medium and small variants and also distrilling base model into smaller model for faster inference in realtime infeerence.
% expanding datasets, using more advanced models, and incorporating multilingual support to enhance its versatility.
% \section{Conclusion}

% In this study, we introduce BnTTS, the first open-source speaker adaptation based TTS system, which improves speech generation for Bangla, a low-resource language. By adapting the XTTS pipeline to accommodate Bangla's unique phonetic characteristics, the model is able to produce natural, clear, and accurate speech with minimal training data, supporting both zero-shot and few-shot speaker adaptation. BnTTS outperforms existing Bangla TTS systems in terms of speed, sound quality, and clarity, as confirmed by listener ratings. 

\section{Limitations}
Despite the significant performance of BnTTS, the system has several limitations. It struggles to adapt to speakers with unique vocal traits, especially without prior training on their voices, limiting its effectiveness in speaker adaptation tasks. We found poor performance on short text due to pre-existing issues in the XTTS foundation model. Although we improved performance by modifying generation settings and incorporating additional training with Complete Audio Prompting, the model still fails to generate sequences under two words or 20 characters in some cases. We did not investigate the performance of the XTTS model by training from scratch; instead, we used continual pretraining due to resource constraints, which may have yielded better results.

\section{Acknowledgments}
We are grateful to HISHAB\footnote{\url{https://www.verbex.ai/}}  for providing us with
all the necessary working facilities, computational
resources, and an appropriate environment through-
out our entire work.
% \textcolor{green}{
% \begin{itemize}
%     \item training from scratch is not possible
%     \item Subjective evaluation may change across the experiment
%     \item generated speech may change across inference
%     \item BengaliNamedEntity1000 only 200 are selected
%     5*1000 + 5*1000 + 1000+1000+1000
%     200 * 
% \end{itemize}
% }

% Despite the significant performances of BnTTS, the system has several limitations. It struggles to adapt to speakers with unique vocal traits, especially without prior training on their voices, limiting its effectiveness in speaker adaptation tasks. We found poor performance on short text due to pre-existing issues in the XTTS foundation model. Although we improved performance by modifying generation settings and incorporating additional training with full audio-text prompting, the model still fails to generate sequences under two words or 20 characters in some cases. We don't investigate the performance from XXTS model by training from strach instead we use continul pretraining due to resource contraints , this may provide better result. 
% As we did continual pretraining on 

% Future works include scratch from training using ba

% We found poor performance on short text becasure the prior issues existing in XXTS foundation model. Although, we improve the performance using some modification is generation setting and using an additional training with full audio text in prompting, still it fails to generated sequence under 2 words or 20 characters in some cases. 

% XTTS, the foundation for BnTTS, performs poorly on very short sequences. 



% While adjustments to hyperparameters and generation settings (temperature and TopK) have mostly resolved this issue, instances remain where the model struggles to generate sequences under 2 words or 20 characters. 

% - the system struggles with adapting to speakers with unique vocal traits, especially without prior training on their voices.

% - Its focus on Bangla also restricts its usefulness for other low-resource languages.

% - XTTS, the foundation for BnTTS, performs poorly on very short sequences. While adjustments to hyperparameters and generation settings (temperature and TopK) have mostly resolved this issue, instances remain where the model struggles to generate sequences under 2 words or 20 characters.

% - training from scratch is not possible
% - Subjective evaluations are subject to change because the speech generated by models may exhibit variations across different inference sessions.
% -  Evaluation is time-consuming, so we have to select 200 sentences from the BengaliNamedEntity1000 eval dataset with 1000 sentences. 



% It relies on a small and uniform dataset, which limits its ability to work well with the diverse dialects and accents in Bangla, potentially affecting the naturalness of the speech output for certain regional variations. 


% Some key observations include that training from scratch the original XTTS does not converge for our dataset, which may be due to insufficient amounts of training data. Subjective evaluations are subject to change because the speech generated by models may exhibit variations across different inference sessions. This variability could lead to differences in results and affect the reliability of the evaluations. Evaluation is time-consuming, so we have to select 200 sentences from the BengaliNamedEntity1000 eval dataset with 1000 sentences. 

% BnTTS has some limitations. It relies on a small and uniform dataset, which limits its ability to work well with the diverse dialects and accents in Bangla, potentially affecting the naturalness of the speech output for certain regional variations. The use of GPT-2, chosen due to limited resources, may also limit the system’s scalability and performance compared to newer models. Additionally, the system struggles with adapting to speakers with unique vocal traits, especially without prior training on their voices. Its focus on Bangla also restricts its usefulness for other low-resource languages. Furthermore, XTTS, the foundation for BnTTS, performs poorly on very short sequences. While adjustments to hyperparameters and generation settings (temperature and TopK) have mostly resolved this issue, instances remain where the model struggles to generate sequences under 2 words or 20 characters. Future improvements could involve a larger dataset, more advanced models, and multilingual support to boost its versatility and robustness.

\section{Ethical Considerations}

The development of BnTTS raises ethical concerns, particularly regarding the potential misuse for unauthorized voice impersonation, which could impact privacy and consent. Protections, such as requiring speaker approval and embedding markers in synthetic speech, are essential. Diverse training data is also crucial to reduce bias and reflect Bangla’s dialectal variety. Additionally, synthesized voices risk diminishing dialectal diversity. As an open-source tool, BnTTS requires clear guidelines for responsible use, ensuring adherence to ethical standards and positive community impact.


\bibliography{custom}



\newpage
\appendix
% \clearpage
% \section{Appendix}

% App A: Dataset
% App B: Data Aquization Framework
% C: Eval Data
% D: Eval Metrics
% E: Model Architecture
% F:Training Objective

% \input{sections/Related Works}
\input{sections/Data_Acquisition}
\input{sections/Dataset}
% \input{sections/Evaluation_Dataset}
\input{sections/Model_Architecture}
\input{sections/Evaluation_Metrics}

% Speech_Generation had been added to main paper 
% \input{sections/Speech_Generation}
\input{sections/variable_description}

\end{document}
