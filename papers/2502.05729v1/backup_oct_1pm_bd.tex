
\documentclass[11pt]{article}

% Change "review" to "final" to generate the final (sometimes called camera-ready) version.
% Change to "preprint" to generate a non-anonymous version with page numbers.
\usepackage[review]{acl}

% Standard package includes
\usepackage{times}
\usepackage{latexsym}


% For proper rendering and hyphenation of words containing Latin characters (including in bib files)
\usepackage[T1]{fontenc}
% For Vietnamese characters
% \usepackage[T5]{fontenc}
% See https://www.latex-project.org/help/documentation/encguide.pdf for other character sets

% This assumes your files are encoded as UTF8
\usepackage[utf8]{inputenc}

% This is not strictly necessary, and may be commented out,
% but it will improve the layout of the manuscript,
% and will typically save some space.
\usepackage{microtype}

% This is also not strictly necessary, and may be commented out.
% However, it will improve the aesthetics of text in
% the typewriter font.
\usepackage{inconsolata}

%Including images in your LaTeX document requires adding
%additional package(s)
\usepackage{graphicx}


\usepackage{algorithm}
\usepackage{algorithmicx}
\usepackage{algpseudocode}
% \usepackage[linesnumbered, ruled, vlined]{algorithm2e}
\usepackage{tabularray}
\usepackage{caption}
\usepackage{subcaption}
% \usepackage{lmodern}
\usepackage{amsmath}
\usepackage{multirow}

% ,amsmath,graphicx, spconf}
% \usepackage{minted}

% If the title and author information does not fit in the area allocated, uncomment the following
%
%\setlength\titlebox{<dim>}
%
% and set <dim> to something 5cm or larger.

\title{BnTTS: Few-Shot Speaker Adaptation in Low-Resource Setting}

% Author information can be set in various styles:
% For several authors from the same institution:
% \author{Author 1 \and ... \and Author n \\
%         Address line \\ ... \\ Address line}
% if the names do not fit well on one line use
%         Author 1 \\ {\bf Author 2} \\ ... \\ {\bf Author n} \\
% For authors from different institutions:
% \author{Author 1 \\ Address line \\  ... \\ Address line
%         \And  ... \And
%         Author n \\ Address line \\ ... \\ Address line}
% To start a separate ``row'' of authors use \AND, as in
% \author{Author 1 \\ Address line \\  ... \\ Address line
%         \AND
%         Author 2 \\ Address line \\ ... \\ Address line \And
%         Author 3 \\ Address line \\ ... \\ Address line}

\author{First Author \\
  Affiliation / Address line 1 \\
  Affiliation / Address line 2 \\
  Affiliation / Address line 3 \\
  \texttt{email@domain} \\\And
  Second Author \\
  Affiliation / Address line 1 \\
  Affiliation / Address line 2 \\
  Affiliation / Address line 3 \\
  \texttt{email@domain} \\}

%\author{
%  \textbf{First Author\textsuperscript{1}},
%  \textbf{Second Author\textsuperscript{1,2}},
%  \textbf{Third T. Author\textsuperscript{1}},
%  \textbf{Fourth Author\textsuperscript{1}},
%\\
%  \textbf{Fifth Author\textsuperscript{1,2}},
%  \textbf{Sixth Author\textsuperscript{1}},
%  \textbf{Seventh Author\textsuperscript{1}},
%  \textbf{Eighth Author \textsuperscript{1,2,3,4}},
%\\
%  \textbf{Ninth Author\textsuperscript{1}},
%  \textbf{Tenth Author\textsuperscript{1}},
%  \textbf{Eleventh E. Author\textsuperscript{1,2,3,4,5}},
%  \textbf{Twelfth Author\textsuperscript{1}},
%\\
%  \textbf{Thirteenth Author\textsuperscript{3}},
%  \textbf{Fourteenth F. Author\textsuperscript{2,4}},
%  \textbf{Fifteenth Author\textsuperscript{1}},
%  \textbf{Sixteenth Author\textsuperscript{1}},
%\\
%  \textbf{Seventeenth S. Author\textsuperscript{4,5}},
%  \textbf{Eighteenth Author\textsuperscript{3,4}},
%  \textbf{Nineteenth N. Author\textsuperscript{2,5}},
%  \textbf{Twentieth Author\textsuperscript{1}}
%\\
%\\
%  \textsuperscript{1}Affiliation 1,
%  \textsuperscript{2}Affiliation 2,
%  \textsuperscript{3}Affiliation 3,
%  \textsuperscript{4}Affiliation 4,
%  \textsuperscript{5}Affiliation 5
%\\
%  \small{
%    \textbf{Correspondence:} \href{mailto:email@domain}{email@domain}
%  }
%}

\begin{document}
\maketitle
\begin{abstract}
Text-to-Speech (TTS) systems have achieved remarkable success in generating high-quality speech, yet most advancements focus on high-resource languages, leaving low-resource languages like Bangla underserved. This paper presents BnTTS, the first open-source framework and the first to enable speaker adaptation for Bangla TTS, aiming to bridge the gap in Bangla speech synthesis with minimal training data. Our approach builds upon the XTTS architecture, integrating Bangla into a multilingual TTS pipeline with adjustments for the phonetic and linguistic nuances of the language. BnTTS supports zero-shot and few-shot speaker adaptation, significantly enhancing the naturalness, clarity, and speaker fidelity of synthesized Bangla speech. We evaluate BnTTS against state-of-the-art Bangla TTS systems, demonstrating superior performance in terms of speed, Subjective Mean Opinion Score (SMOS), and audio quality. This work marks a critical advancement towards inclusive TTS technology for low-resource languages, paving the way for broader linguistic accessibility in speech synthesis.
\end{abstract}



%In this paper, we present the BnTTS, an extension of the XTTS framework specifically focusing on synthesizing Bangla, a low-resource language. While XTTS demonstrates state-of-the-art performance across 16 high- and medium-resource languages, it lacks support for Bangla and other low-resource languages. To bridge this gap, we adapt the XTTS architecture to effectively model Bangla speech, enabling zero-shot multi-speaker text-to-speech (ZS-TTS) synthesis for the language. Our approach addresses the challenges associated with low-resource data by leveraging transfer-learning techniques and optimizing voice cloning for Bangla speakers. The results demonstrate significant improvements in naturalness and speaker similarity in Bangla, highlighting the potential for extending TTS capabilities to underrepresented languages.



% Recent advancements in text-to-speech (TTS) \cite{popov2021gradtts,kim2021conditional} have led to the development of speaker-adaptive models \cite{le2023voicebox,kim2023pflow} that can closely replicate target voices. These models are typically divided into zero-shot and one-shot approaches. Zero-shot TTS \cite{le2023voicebox,ju2024naturalspeech3} eliminates the need for additional training, but often requires large datasets and can struggle with out-of-distribution (OoD) voices. In contrast, one-shot TTS \cite{yan2021adaspeech2,wang2023neural} fine-tunes pre-trained models, offering improved adaptability to new speakers while reducing data and model size demands.
\section{Introduction \& Related Works}
Speaker adaptation in Text-to-Speech (TTS) technology has seen substantial advancements in recent years, particularly with speaker-adaptive models enhancing the naturalness and intelligibility of synthesized speech \cite{eren2023deep}. Notably, recent innovations have emphasized zero-shot and one-shot adaptation approaches \cite{kodirov2015unsupervised}. Zero-shot TTS models eliminate the need for speaker-specific training by generating speech from unseen speakers using reference audio samples \cite{min2021meta}. Despite this progress, zero-shot models often require large datasets and face challenges with out-of-distribution (OOD) voices, as they struggle to adapt effectively to novel speaker traits \cite{le2023voicebox, ju2024naturalspeech3}. Alternatively, one-shot adaptation fine-tunes pre-trained models using a single data instance, offering improved adaptability with reduced data and computational demands \cite{yan2021adaspeech2, wang2023neural}; however, the pretraining stage still necessitates substantial datasets \cite{zhang2021transfer}.

Recent works such as YourTTS \cite{bai2022yourtts} and VALL-E X \cite{xu2022vall} have made strides in cross-lingual zero-shot TTS, with YourTTS exploring English, French, and Portuguese, and VALL-E X incorporating language identification to extend support for a broader range of languages \cite{xu2022vall}. These advancements highlight the potential for multilingual TTS systems to achieve cross-lingual speech synthesis. Furthermore, the XTTS model \cite{casanova2024xtts} represents a significant leap by expanding zero-shot TTS capabilities across 16 languages. Based on the Tortoise model \cite{casanova2024xtts}, XTTS enhances voice cloning accuracy and naturalness but remains focused on high- and medium-resource languages, leaving low-resource languages such as Bangla underserved \cite{zhang2022universal, xu2023cross}. 

The scarcity of extensive datasets has hindered the adaptation of state-of-the-art (SOTA) TTS models for low-resource languages. Models like YourTTS \citet{bai2022yourtts}, VALL-E X \cite{baevski2022vall}, and Voicebox \cite{baevski2022voicebox} have demonstrated success in multilingual settings, yet their primary focus remains on languages with rich resources like English, Spanish, French, and Chinese. While a few Bangla TTS systems exist \cite{gutkin2016tts}, they often produce robotic tones \cite{hossain2018development} or are limited to a small set of static speakers \cite{gong2024initial}, lacking instant speaker adaptation capabilities and typically not being open-source.

To address these challenges, we propose the first open-source framework for few-shot speaker adaptation in Bangla TTS. Our approach integrates Bangla into the XTTS training pipeline, with minor architectural modifications to accommodate Bangla’s unique phonetic and linguistic features. This contribution marks a pivotal step for zero-shot TTS in Bangla, enabling natural-sounding speech generation with minimal training data. Building on XTTS’s foundational achievements, our model is optimized for effective few-shot voice cloning, addressing the needs of low-resource language settings.

By extending XTTS to support Bangla, we provide a valuable resource for developing TTS systems in underrepresented languages. Our model enhances voice cloning capabilities and sets the stage for expanding zero-shot TTS to other low-resource languages, thereby promoting greater linguistic inclusivity in speech synthesis technology. Our contributions are summarized as follows:
\begin{itemize}
    \item Introducing the first open-source framework and first Bangla few-shot speaker adaptation in TTS.
    \item Integrating Bangla into a multilingual TTS pipeline with architecture adjustments designed for low-resource language characteristics.
    \item Advancing few-shot TTS for low-resource languages, achieving superior speed, clarity, naturalness, and higher SMOS compared to existing Bangla TTS models.
    
\end{itemize}

% \section{Engines}

% To produce a PDF file, pdf\LaTeX{} is strongly recommended (over original \LaTeX{} plus dvips+ps2pdf or dvipdf). Xe\LaTeX{} also produces PDF files, and is especially suitable for text in non-Latin scripts.



\begin{figure}[h]
    \centering
    \includegraphics[width=1.001\linewidth]{resources/method2.png} 
    \caption{Overview of BnTTS Model} 
    \label{fig:xtts_train_diagram}
\end{figure}

\section{BnTTS}
\textbf{Preliminaries:} Given a text sequence with $N$ tokens, $\mathbf{T} = \{t_1, t_2, \dots, t_N\}$, and a speaker's mel-spectrogram $\mathbf{S} = \{s_1, s_2, \dots, s_L\}$, our objective is to generate speech $\hat{\mathbf{Y}}$ that matches the speaker's characteristics. Let $\mathbf{Y} = \{y_1, y_2, \dots, y_M\}$ denote the ground truth mel-spectrogram frames for the target speech. 

The framework aims to synthesize $\hat{\mathbf{Y}}$ directly from $\mathbf{T}$ and $\mathbf{S}$, such that:
\[
\hat{\mathbf{Y}} = \mathcal{F}(\mathbf{T}, \mathbf{S})
\]
where $\mathcal{F}$ is the model responsible for producing speech conditioned on both the text and the speaker's spectrogram.

\textbf{VQ-VAE: } Our method uses a Vector Quantized-Variational AutoEncoder (VQ-VAE) \cite{tortoise} to encode mel-spectrogram frames $\mathbf{Y}$ into discrete codes. The VQ-VAE encoder maps $\mathbf{Y}$ to a list of discrete codes with finite vocabulary.
% latent vectors $\mathbf{Y_z} \in \mathbb{R}^{M \times d}$, where $d$ is the dimensionality of the model.

\textbf{Conditioning Encoder \& Perceiver Resampler:}
The Conditioning Encoder consists of $l$ layers of $k$-head Scaled Dot-Product Attention, followed by a Perceiver Resampler. Each attention layer processes the input features to capture both local and global contexts.

Given the speaker spectrogram $\mathbf{S}$, it is first transformed into an intermediate representation $\mathbf{S_z} \in \mathbb{R}^{L \times d}$. Each attention layer applies a scaled dot-product attention mechanism:
\[
\text{Attention}(\mathbf{Q}, \mathbf{K}, \mathbf{V}) = \text{softmax}\left(\frac{\mathbf{Q}\mathbf{K}^\top}{\sqrt{d}}\right) \mathbf{V}
\]
where $\mathbf{Q}$, $\mathbf{K}$, and $\mathbf{V}$ are projections of $\mathbf{S_z}$, computed over $k$ attention heads. The output from the final attention layer is passed through a Perceiver Resampler to produce a fixed output dimensionality.

The Perceiver Resampler generates a fixed number of embeddings $\mathbf{S_p} \in \mathbb{R}^{d}$. This resampling step ensures that the final output, $\mathbf{R} \in \mathbb{R}^{P \times d}$, has a fixed size $P$, independent of the variable input length $L$.


\textbf{Text Encoder:} This step consists of a simple embedding layer that projects the input text tokens $\mathbf{T} = \{t_1, t_2, \dots, t_N\}$ into a continuous embedding space. The tokens are mapped to embeddings $\mathbf{T_e} \in \mathbb{R}^{N \times d}$, where $d$ is the embedding dimension.

\textbf{LLM:}
In this framework, we utilize the decoder portion of the transformer-based LLM. The  speaker embeddings $\mathbf{S_p}$, text embeddings $\mathbf{T_e}$, and ground truth spectrogram embeddings $\mathbf{Y_z}$ are concatenated to form the input during training. Let $\mathbf{X}$ denote this combined input:
\[
\mathbf{X} = \mathbf{S_p}  \oplus \mathbf{T_e} \oplus \mathbf{Y_z}
\]
where $\oplus$ represents concatenation, which results in $\mathbf{X} \in \mathbb{R}^{(N + P + M) \times d}$.

The LLM processes $\mathbf{X}$ through its layers, producing an output from the final layer denoted by $\mathbf{H} =  \{ h_1^S, h_2^S, \dots, h_M^S, h_1^T, h_2^T, \dots, h_N^T, h_1^Y, h_2^Y, \dots, h_P^Y \}$, where $\mathbf{H} \in \mathbb{R}^{(N + P + M) \times d}$. Each component of $\mathbf{H}$ corresponds to the hidden states for the text, speaker, and spectrogram embeddings.

During inference, only the text and speaker embeddings are concatenated, forming the input as:
\[
\mathbf{X} = \mathbf{S_p} \oplus \mathbf{T_e}
\]
The LLM then generates spectrogram embeddings $\{h_1^Y, h_2^Y, \dots, h_P^Y\}$, which serve as the predicted output speech representation.

\textbf{HiFi-GAN Decoder: }
The HiFi-GAN Decoder converts the LLM's output into realistic speech, preserving the speaker's unique characteristics. During training, it uses the ground truth spectrogram representation, whereas in inference, it uses the predicted spectrogram. Let $\mathbf{H}_\text{Y} = \{h_1^Y, h_2^Y, \dots, h_P^Y\} \in \mathbb{R}^{P \times d}$ represent spectrogram embedding.

The speaker embedding $\mathbf{S}$ is resized to match $\mathbf{H}_\text{Y}$, resulting in $\mathbf{S}' \in \mathbb{R}^{P \times d}$. We then combine these by adding them element-wise and feeding them into the HiFi-GAN decoder, which produces the final audio waveform $\mathbf{W}$:
\[
\mathbf{W} = g_\text{HiFi}(\mathbf{H}_\text{Y} + \mathbf{S}')
\]

In this way, HiFi-GAN decoder generates speech that reflects the input text while maintaining the speaker’s unique sound qualities.

\textbf{Training Objectives: } Our training process incorporates three different objectives. The first is the text generation objective, which helps the model predict future tokens based on the given text, improving its understanding of the sequence and aiding in advanced speech prediction. The second objective is audio generation, which aligns the synthesized audio with the input text. The third is the HiFi-GAN objective, which learns to transform the LLM's latents to speech signals via adversarial training. Details on each objective function can be found in Appendix E.


\section{Experiments}
\textbf{Model Architecture}
The BnTTS framework utilizes GPT-2 as the base model, selected due to resource constraints, although any LLM could be applied. The Conditioning Encoder consists of six attention blocks, each with 32 heads, to effectively capture contextual information. The Perceiver Resampler processes the output from the Conditioning Encoder to a fixed sequence length of 32. The model's dimensionality aligns with GPT-2, using a hidden size of 1024 and an intermediate layer size of 3072. The maximum sequence length is set to 384 tokens. Further details on the model architecture can be found in Appendix F.

\textbf{Training Dataset: }The BnTTS model was trained on approximately 3,860 hours of Bengali data, sourced from open-source datasets, pseudo-labeled data, synthetic datasets, and in-house data. Since Bengali is a low-resource language with limited large-scale, high-quality TTS data, we developed an automated in-house TTS Data Acquisition Framework. This framework collects high-quality speech data with aligned transcripts by utilizing neural speech processing models and custom algorithms that refine raw audio into word-aligned outputs. For further details about the training dataset, refer to Appendix A, and for more on the Data Acquisition Framework, see Appendix B.

\textbf{Evaluation Dataset: }To evaluate the performance of our TTS system, we used two curated datasets: BnStudioEval  and BnTTSTextEval. The BnStudioEval dataset, derived from in-house studio dataset, assesses the model’s ability to produce high-fidelity speech with accurate speaker impersonation. Conversly, the BnTTSTextEval dataset contains three subsets: BengaliStimuli, which tests the model's handling of diverse phonemes; BengaliNamedEntity1000, focused on conversational accuracy with real-world names and entities; and ShortText200, which evaluates performance in everyday dialogue scenarios. Detailed statistics and descriptions of these datasets are provided in Appendix C.

\textbf{Training setup: }For Bengali XTTS pretraining, we used the AdamW optimizer with betas 0.9 and 0.96, weight decay 0.01, and an initial learning rate of 2e-05 with a batch size equal to 12 with grad accumulation equal to 24 steps for each GPU. We also decayed the learning rate using using MultiStepLR by a gamma of 0.66 using the milestones equaling to completion of an epoch. We have pretrained the model for 15 days on a single A100 80GB GPU. The subsequent fine-tuning took around 30 minutes to 1 hour depending on duration of fine-tuning data available for speaker impersonation.



\textbf{Evaluation Metric: }We evaluated our TTS system using five criteria. The Subjective Mean Opinion Score (SMOS) rates the absolute quality of synthetic speech in isolation, providing insights into perceived audio quality.  To assess transcription accuracy, we used ASR-based Character Error Rate (CER), which compares the speech-to-text output of the synthesized speech with its ground truth transcription \cite{nandi-etal-2023-pseudo}. SpeechBERTScore evaluates the similarity between generated and reference speech by computing BERTScore using self-supervised dense speech features, accounting for variations in sequence length \cite{saeki2024spbertscore}. Finally, Speaker Encoder Cosine Similarity (SECS)\cite{casanova2021sc} objectively measures the resemblance between the speaker characteristics of synthesized and reference speech, ensuring fidelity in speaker identity (\citet{thienpondt2024ecapa2}). See Appendix D for the details of evaluation metrics. 

\textbf{Results and Discussion: }
\\
\textbf{Reference-aware Evaluation on BnStudioEval: }
Table \ref{tab:eval_on_studio_eval} shows the performance of various TTS systems on BnStudioEval dataset. AzureTTS performs best among synthetic methods, with the CER score, even surpassing the Ground Truth (GT) in transcription accuracy. However, GT scores highest on subjective metrics such as SMOS (4.671), Naturalness (4.625), and Clarity (4.9). The proposed BnTTS system closely follows GT, with competitive scores in SMOS (4.584), Naturalness (4.531), and Clarity (4.898).

For speaker similarity, GT achieves perfect SECS scores against reference audio and high scores against the speaker prompt. BnTTS also performs well, with SECS scores of 0.513 (reference) and 0.335 (prompt), just slightly behind GT. BnTTS achieves a SpeechBERTScore of 0.796, while GT maintains a perfect reference score of 1.0. Note that IndicTTS, GTTS, and AzureTTS do not support speaker impersonation, so SECS and SpeechBERTScore were not evaluated for these systems.

\textbf{Reference-independent Evaluation on BnTTSTextEval:} Table \ref{tab:eval_on_BnTTSTextEval} presents the comparative performance of various TTS systems evaluated on the BnTTSTextEval dataset. The AzureTTS and GTTS consistently achieve lower CER scores, with BnTTS following closely in third place, and IndicTTS trailing behind.

BnTTS performs strongly in subjective evaluations, excelling in SMOS, Naturalness, and Intelligibility across the BengaliStimuli53 and BengaliNamedEntity1000 subsets. However, it falls slightly behind AzureTTS in the ShortText200 subset. Despite this, BnTTS overall, remains the top-performing system in all subjective metrics, delivering the highest scores in SMOS(4.383), Naturalness(4.313), and Clarity(4.737).


\textbf{Effect of Sampling and Prompt Length on BnTTS Accuracy:} In this section, we investigate how different configurations of temperature ($T$), top-$k$ sampling, and prompt length (short or long) impact the CER and duration equality of the BnTTS model on the short texts(less than 30 characters) from BnStudioEval dataset. Table~4 provides a comparative performance analysis under four experimental settings, with the goal of understanding how these parameters contribute to transcription accuracy and duration similarity. The results indicate that higher temperatures (i.e., $T=1.0$) combined with lower Top-$k$ values (i.e., $k=2$) improve CER, especially when using short prompts. Short prompts consistently result in better transcription accuracy and duration equality, likely due to their reduced complexity and the model's ability to generate focused outputs. The combination of these parameters offers a promising approach for achieving both accurate and temporally aligned speech synthesis in low-resource languages such as Bengali.


\begin{table}[hbt!]
\centering
\footnotesize
\setlength{\tabcolsep}{2.9pt}
\resizebox{0.47\textwidth}{!}{%

\begin{tabular}{c|c c c c c}
\hline
\textbf{Method} & \textbf{GT} & \textbf{IndicTTS} & \textbf{GTTS} & \textbf{AzureTTS} & \textbf{BnTTS} \\ 
\hline
CER & \textit{0.037} & 0.067 & 0.022 & \textbf{0.019} & 0.036 \\ 
SMOS & \textit{4.671} & 3.038 & 3.788 & 4.083 & \textbf{4.584} \\ 
Naturalness & \textit{4.625} & 2.857 & 3.573 & 3.941 & \textbf{4.531} \\ 
Clarity & \textit{4.9} & 3.945 & 4.869 & 4.796 & \textbf{4.898} \\ 
SECS (Ref.) & \textit{1.0} & - & - & - & \textbf{0.513} \\ 
SECS (Prompt) & \textit{0.361} & - & - & - & \textbf{0.335} \\ 
SpeechBERT Score & \textit{1.0} & - & - & - & \textbf{0.796} \\ 
\hline
\end{tabular}}
\caption{Comparative performance analysis on Reference-aware BnStudioEval dataset}
\label{tab:eval_on_studio_eval}
\end{table}
\begin{table}[h!]
\centering
\footnotesize
\setlength{\tabcolsep}{2.9pt}
\resizebox{0.47\textwidth}{!}{%
\begin{tabular}{c|c|c c c c}
\hline
\textbf{Dataset} & \textbf{Method} & \textbf{CER} & \textbf{SMOS} & \textbf{Naturalness} & \textbf{Clarity} \\ 
\hline
Bengali-& IndicTTS & 0.115 & 2.618 & 2.469 & 3.360 \\ 
                                                      Stimulai-& GTTS     & \textbf{0.072} & 3.654 & 3.448 & 4.698 \\ 
                                                      53& Azure TTS & 0.077 & 3.845 & 3.712 & 4.507 \\ 
                                                      & BnTTS  & 0.090 & \textbf{4.485} & \textbf{4.417} & \textbf{4.825} \\ 
\hline
Bengali-& IndicTTS & 0.051 & 3.058 & 2.867 & 4.014 \\ 
                                                             Named-& GTTS     & 0.042 & 3.881 & 3.680 & 4.713 \\ 
                                                             Entity-& Azure TTS & \textbf{0.039} & 4.105 & 3.972 & 4.733 \\ 
                                                             1000& BnTTS  & 0.054 & \textbf{4.376} & \textbf{4.304} & \textbf{4.740} \\ 
\hline
Short-& IndicTTS & 0.600 & 2.486 & 2.387 & 2.983 \\ 
                                                 Text-& GTTS     & 0.328 & 4.039 & 3.882 & 4.881 \\ 
                                                 200& Azure TTS & \textbf{0.259} & \textbf{4.421} & \textbf{4.350} & \textbf{4.774} \\ 
                                                 & BnTTS  & 0.491 & 4.276 & 4.206 & 4.627 \\ 
\hline
Overall& IndicTTS & 0.145 & 2.806 & 2.649 & 3.593 \\ 
                         & GTTS     & 0.071 & 3.865 & 3.674 & 4.821 \\ 
                         & Azure TTS & \textbf{0.062} & 4.122 & 4.005 & 4.701 \\ 
                         & BnTTS  & 0.100 & \textbf{4.383} & \textbf{4.313} & \textbf{4.737} \\ 
\hline
\end{tabular}}
\caption{Comparative performance analysis on Reference-independent BnTTSTextEval dataset}
\label{tab:eval_on_BnTTSTextEval}
\end{table}

\begin{table}[hbt!]
\centering
\resizebox{0.4\textwidth}{!}{%
\setlength{\tabcolsep}{2.0pt}

\begin{tabular}{c|cccc} 
\hline
\textbf{Exp. No} & \textbf{T and TopK} & \begin{tabular}[c]{@{}c@{}}\textbf{Short}\\\textbf{Prompt}\end{tabular} & \begin{tabular}[c]{@{}c@{}}\textbf{Duration}\\\textbf{Equality}\end{tabular} & \textbf{CER}  \\ 
\hline
1                & T=0.85, TopK=50     & N                                                                       & 0.699                                                                        & 0.081         \\ 

2                & T=0.85, TopK=50     & Y                                                                       & 0.820                                                                        & 0.029         \\ 

3                & T=1.0, TopK=2       & N                                                                       & 0.701                                                                        & 0.023         \\ 

4                & T=1.0, TopK=2       & Y                                                                       & 0.827                                                                        & 0.015         \\
\hline
\end{tabular}}
\caption{Comparative performance analysis on Short-BnStudioEval Dataset}
\label{tab:eval_on_short_Studio_eval}
\end{table}


\section{Conclusion}

In this study, we introduce BnTTS, the first open-source speaker adaptation based TTS system, which improves speech generation for Bangla, a low-resource language. By adapting the XTTS pipeline to accommodate Bangla's unique phonetic characteristics, the model is able to produce natural, clear, and accurate speech with minimal training data, supporting both zero-shot and few-shot speaker adaptation. BnTTS outperforms existing Bangla TTS systems in terms of speed, sound quality, and clarity, as confirmed by listener ratings. 

\section{Limitations}

BnTTS has some limitations. It relies on a small and uniform dataset, which limits its ability to work well with the diverse dialects and accents in Bangla, potentially affecting the naturalness of the speech output for certain regional variations. The use of GPT-2, chosen due to limited resources, may also limit the system’s scalability and performance compared to newer models. Additionally, the system struggles with adapting to speakers with unique vocal traits, especially without prior training on their voices. Its focus on Bangla also restricts its usefulness for other low-resource languages. Furthermore, XTTS, the foundation for BnTTS, performs poorly on very short sequences. While adjustments to hyperparameters and generation settings (temperature and TopK) have mostly resolved this issue, instances remain where the model struggles to generate sequences under 2 words or 20 characters. Future improvements could involve a larger dataset, more advanced models, and multilingual support to boost its versatility and robustness.

\clearpage
\section{Ethical Considerations}

The development of BnTTS raises ethical concerns, particularly regarding the potential misuse for unauthorized voice impersonation, which could impact privacy and consent. Protections, such as requiring speaker approval and embedding markers in synthetic speech, are essential. Diverse training data is also crucial to reduce bias and reflect Bangla’s dialectal variety. Additionally, synthesized voices risk diminishing dialectal diversity. As an open-source tool, BnTTS requires clear guidelines for responsible use, ensuring adherence to ethical standards and positive community impact.


\bibliography{custom}



\newpage
\appendix
% \clearpage
% \section{Appendix}

% App A: Dataset
% App B: Data Aquization Framework
% C: Eval Data
% D: Eval Metrics
% E: Model Architecture
% F:Training Objective

\input{sections/Dataset}
\input{sections/Data_Acquisition}
\input{sections/Evaluation_Dataset}
% \input{sections/Evaluation_Metrics}
% \input{sections/Model_Architecture}
\input{sections/Speech_Generation}

\end{document}
