\section{Experiments}

\noindent \textbf{BnTTS model:} BnTTS employs the pretrained XTTS checkpoint \cite{casanova2024xtts} as its base model, chosen for resource efficiency. The Conditioning Encoder has six attention blocks with 32 heads, capturing contextual information. The Perceiver Resampler reduces the sequence to a fixed length of 32. The model maintains GPT-2’s dimensionality, with a hidden size of 1024 and an intermediate layer size of 3072, handling sequences of up to 400 tokens. (Details in Appendix \ref{app:training_objective}).
% (Appendix \ref{app:training_objective} provides further details)..

\noindent \textbf{Dataset:} We continuously pre-trained the BnTTS model (initialized from the XTTS checkpoint) on 3.85k hours of Bengali speech data, sourced from open-source datasets, pseudo-labeled data, and synthetic datasets. The pseudo-labeled data were collected using an in-house automated TTS Data Acquisition Framework, which segments speech into 0.5 to 11-second chunks with time-aligned transcripts. These segments were further refined using neural speech models and custom algorithms to enhance quality and accuracy. For speaker adaptation, we incorporated 4.22 hours of high-quality studio recordings from four speakers, referred to as In-House HQ Data.

For evaluation, we propose two datasets: (1) BnStudioEval, derived from our In-House HQ Data, to assess high-fidelity speech generation and speaker adaptation, and (2) BnTTSTextEval, a text-only dataset consisting of three subsets: BengaliStimuli53 (assessing phonetic diversity), BengaliNamedEntity1000 (evaluating named entity pronunciation), and ShortText200 (measuring conversational fluency in short sentences, filler words, and common phrases used in everyday dialogue). Further details are provided in Appendices \ref{sec:data_collection}, \ref{app:human_reviewed_data}, and \ref{app:dataset}.

% (2) BnTTSTextEval, a text-only dataset consisting of three subsets designed to evaluate phoneme diversity, named entity pronunciation, and performance in everyday dialogue.

% For evaluation, two datasets were used: BnStudioEval, which assesses high-fidelity speech generation and speaker impersonation, and BnTTSTextEval, comprising subsets that test phoneme diversity, conversational accuracy, and performance in everyday dialogue. Further details are provided in Appendices \ref{sec:data_collection} and \ref{app:dataset}.

% \textbf{Model Architecture}
% The BnTTS framework utilizes GPT-2 as the base model, selected due to resource constraints, although any LLM could be applied. The Conditioning Encoder consists of six attention blocks, each with 32 heads, to effectively capture contextual information. The Perceiver Resampler processes the output from the Conditioning Encoder to a fixed sequence length of 32. The model's dimensionality aligns with GPT-2, using a hidden size of 1024 and an intermediate layer size of 3072. The maximum sequence length is set to 384 tokens. Further details on the model architecture can be found in Appendix F.

% \textbf{Training Dataset: }The BnTTS model was trained on approximately 3,860 hours of Bengali data, sourced from open-source datasets, pseudo-labeled data, synthetic datasets, and in-house data. Since Bengali is a low-resource language with limited large-scale, high-quality TTS data, we developed an automated in-house TTS Data Acquisition Framework. This framework collects high-quality speech data with aligned transcripts by utilizing neural speech processing models and custom algorithms that refine raw audio into word-aligned outputs. For further details about the training dataset, refer to Appendix A, and for more on the Data Acquisition Framework, see Appendix B.

% \textbf{Evaluation Dataset: }To evaluate the performance of our TTS system, we used two curated datasets: BnStudioEval  and BnTTSTextEval. The BnStudioEval dataset, derived from in-house studio dataset, assesses the model’s ability to produce high-fidelity speech with accurate speaker impersonation. Conversly, the BnTTSTextEval dataset contains three subsets: BengaliStimuli, which tests the model's handling of diverse phonemes; BengaliNamedEntity1000, focused on conversational accuracy with real-world names and entities; and ShortText200, which evaluates performance in everyday dialogue scenarios. Detailed statistics and descriptions of these datasets are provided in Appendix C.


% \textbf{Training setup: }For Bengali XTTS pretraining, we used the AdamW optimizer with betas 0.9 and 0.96, weight decay 0.01, and an initial learning rate of 2e-05 with a batch size equal to 12 with grad accumulation equal to 24 steps for each GPU. We also decayed the learning rate using using MultiStepLR by a gamma of 0.66 using the milestones equaling to completion of an epoch. We have pretrained the model for 15 days on a single A100 80GB GPU. The subsequent fine-tuning took around 30 minutes to 1 hour depending on duration of fine-tuning data available for speaker impersonation.

% \noindent \textbf{Training Setup:} We pretrain the BnTTS model using the AdamW optimizer with betas of 0.9 and 0.96, weight decay of 0.01, and an initial learning rate of 2e-05. The batch size was 12, with gradient accumulation over 24 steps per GPU, and the learning rate decay was applied using MultiStepLR. Continual pretraining lasts 15 days on a single A100 80GB GPU, while fine-tuning takes 30 minutes to 1 hour depending on the speaker-specific data size. The HiFi-GAN model was fine-tuned for 3 days on a single A100 80GB GPU.

\noindent \textbf{Training Setup:} We initialized the BnTTS model from the XTTS checkpoint and do continual pre-training using the AdamW optimizer with betas of 0.9 and 0.96, weight decay of 0.01, and an initial learning rate of 2e-05. The batch size was 12, with gradient accumulation over 24 steps per GPU, and the learning rate decay(0.66) was applied using MultiStepLR. All experiments are run on a single NVIDIA A100 GPU with 80GB of VRAM. The pretraining process consists of two stages:

\textbf{a) Partial Audio Prompting:} In this stage, a random segment of the ground truth audio is used as the speaker prompt. Training in this phase lasted for 5 epochs.

\textbf{b) Complete Audio Prompting:} Here, the full duration of audio is used as the speaker prompt. This stage continues from the checkpoint and optimizer state of the first phase and lasts for 1 epoch.

Additionally, the HiFi-GAN vocoder was fine-tuned separately using GPT-2 embeddings derived from the model in stage b. The vocoder was fine-tuned for three days to ensure optimal performance. The audio encoder and speaker encoder remain frozen across all experiments.

\noindent \textbf{Few-shot Speaker Adaptation:} For few-shot speaker adaptation, we fine-tuned the BnTTS model using our In-House HQ dataset, which comprises studio recordings from four speakers. We randomly selected 20 minutes of audio for each speaker and fine-tuned the model in a multi-speaker setting for 10 epochs. This fine-tuning approach is more meaningful with the XTTS-like architecture pretrained on large-scale datasets. The evaluation results are presented in Section \ref{sec:results}.

% We randomly selected 20 minutes of audio for each speaker and trained separate speaker-specific models for 10 epochs. This approach ensures a fair comparison with other mono-speaker TTS systems considered in this study.



% \textbf{Evaluation Metric: }We evaluated our TTS system using five criteria. The Subjective Mean Opinion Score (SMOS) rates the absolute quality of synthetic speech in isolation, providing insights into perceived audio quality.  To assess transcription accuracy, we used ASR-based Character Error Rate (CER), which compares the speech-to-text output of the synthesized speech with its ground truth transcription \cite{nandi-etal-2023-pseudo}. SpeechBERTScore evaluates the similarity between generated and reference speech by computing BERTScore using self-supervised dense speech features, accounting for variations in sequence length \cite{saeki2024spbertscore}. Finally, Speaker Encoder Cosine Similarity (SECS)\cite{casanova2021sc} objectively measures the resemblance between the speaker characteristics of synthesized and reference speech, ensuring fidelity in speaker identity (\citet{thienpondt2024ecapa2}). See Appendix D for the details of evaluation metrics. 


\noindent \textbf{Evaluation Metric: } We evaluate the BnTTS system using six criteria. The Subjective Mean Opinion Score (SMOS) including Naturalness and Clarity evaluates perceived audio quality from \citet{streijl2016mean}, while the ASR-based Character Error Rate (CER) \cite{nandi-etal-2023-pseudo} measures transcription accuracy, SpeechBERTScore assesses similarity to reference speech, and Speaker Encoder Cosine Similarity (SECS) evaluates speaker identity fidelity \cite{saeki2024spbertscore, casanova2021sc, thienpondt2024ecapa2}. See Appendix \ref{app:eval_metrics} for details.
