% \section{BnTTS}

% \textbf{Preliminaries:} Given a text sequence with $N$ tokens, $\mathbf{T} = \{t_1, t_2, \dots, t_N\}$, and a speaker's mel-spectrogram $\mathbf{S} = \{s_1, s_2, \dots, s_L\}$, our objective is to generate speech $\hat{\mathbf{Y}}$ that matches the speaker's characteristics. Let $\mathbf{Y} = \{y_1, y_2, \dots, y_M\}$ denote the ground truth mel-spectrogram frames for the target speech. 

% The framework aims to synthesize $\hat{\mathbf{Y}}$ directly from $\mathbf{T}$ and $\mathbf{S}$, such that:
% \[
% \hat{\mathbf{Y}} = \mathcal{F}(\mathbf{T}, \mathbf{S})
% \]
% where $\mathcal{F}$ is the model responsible for producing speech conditioned on both the text and the speaker's spectrogram.

% \begin{figure}[h]
%     \centering
%     \includegraphics[width=1.001\linewidth]{resources/hqtts.png} 
%     \caption{Overview of BnTTS Model} 
%     \label{fig:xtts_train_diagram}
% \end{figure}


% % \noindent \textbf{VQ-VAE:} Our method employs a Vector-Quantized Variational AutoEncoder (VQ-VAE) \cite{tortoise} as a speech tokenizer. The VQ-VAE transforms mel-spectrogram frames \(\mathbf{Y}\) into discrete codes \(\mathbf{z} \in \mathcal{C}\), where \(\mathcal{C}\) is the learned codebook of embeddings.

% \noindent \textbf{VQ-VAE:} Our method uses a Vector Quantized-Variational AutoEncoder (VQ-VAE) \cite{tortoise} to encode mel-spectrogram frames $\mathbf{Y}$ into discrete codes. The VQ-VAE effectively functions as speech tokenzier with a finite vocabulary.
% % latent vectors $\mathbf{Y_z} \in \mathbb{R}^{M \times d}$, where $d$ is the dimensionality of the model.

% \noindent \textbf{Conditioning Encoder \& Perceiver Resampler:}
% The Conditioning Encoder \cite{casanova2024xtts} consists of $l$ layers of $k$-head Scaled Dot-Product Attention, followed by a Perceiver Resampler. Each attention layer processes the input features to capture both local and global contexts.

% Given the speaker spectrogram $\mathbf{S}$, it is first transformed into an intermediate representation $\mathbf{S_z} \in \mathbb{R}^{L \times d}$. Each attention layer applies a scaled dot-product attention mechanism:
% \[
% \text{Attention}(\mathbf{Q}, \mathbf{K}, \mathbf{V}) = \text{softmax}\left(\frac{\mathbf{Q}\mathbf{K}^\top}{\sqrt{d}}\right) \mathbf{V}
% \]
% where $\mathbf{Q}$, $\mathbf{K}$, and $\mathbf{V}$ are projections of $\mathbf{S_z}$, computed over $k$ attention heads. The output from the final attention layer is passed through a Perceiver Resampler to produce a fixed output dimensionality.

% The Perceiver Resampler generates a fixed number of sequences $P$ from $L$. This resampling step ensures that the final output, $\mathbf{R} \in \mathbb{R}^{P \times d}$, has a fixed size $P$, independent of the variable input length $L$.


% \noindent \textbf{Text Encoder:} This step consists of a simple embedding layer that projects the input text tokens $\mathbf{T} = \{t_1, t_2, \dots, t_N\}$ into a continuous embedding space. The tokens are mapped to embeddings $\mathbf{T_e} \in \mathbb{R}^{N \times d}$, where $d$ is the embedding dimension.

% \noindent \textbf{LLM:}
% In this framework, we utilize the decoder portion of the transformer-based LLM \cite{radford2019language}. The  speaker embeddings $\mathbf{S_p}$, text embeddings $\mathbf{T_e}$, and ground truth spectrogram embeddings $\mathbf{Y_z}$ are concatenated to form the input during training. Let $\mathbf{X}$ denote this combined input:
% \[
% \mathbf{X} = \mathbf{S_p}  \oplus \mathbf{T_e} \oplus \mathbf{Y_z}
% \]
% where $\oplus$ represents concatenation, which results in $\mathbf{X} \in \mathbb{R}^{(N + P + M) \times d}$.

% The LLM processes $\mathbf{X}$ through its layers, producing an output from the final layer denoted by $\mathbf{H} =  \{ h_1^S, h_2^S, \dots, h_M^S, h_1^T, h_2^T, \dots, h_N^T, h_1^Y, h_2^Y, \dots, h_P^Y \}$, where $\mathbf{H} \in \mathbb{R}^{(N + P + M) \times d}$. Each component of $\mathbf{H}$ corresponds to the hidden states for the text, speaker, and spectrogram embeddings.

% During inference, only the text and speaker embeddings are concatenated, forming the input as:
% \[
% \mathbf{X} = \mathbf{S_p} \oplus \mathbf{T_e}
% \]
% The LLM then generates spectrogram embeddings $\{h_1^Y, h_2^Y, \dots, h_P^Y\}$, which serve as the predicted output speech representation.

% \noindent \textbf{HiFi-GAN Decoder: }
% The HiFi-GAN Decoder \cite{kong2020hifi} converts the LLM's output into realistic speech, preserving the speaker's unique characteristics. During training, it uses the ground truth spectrogram representation, whereas in inference, it uses the predicted spectrogram. Let $\mathbf{H}_\text{Y} = \{h_1^Y, h_2^Y, \dots, h_P^Y\} \in \mathbb{R}^{P \times d}$ represent spectrogram embedding.

% The speaker embedding $\mathbf{S}$ is resized to match $\mathbf{H}_\text{Y}$, resulting in $\mathbf{S}' \in \mathbb{R}^{P \times d}$. We then combine these by adding them element-wise and feeding them into the HiFi-GAN decoder, which produces the final audio waveform $\mathbf{W}$:
% \[
% \mathbf{W} = g_\text{HiFi}(\mathbf{H}_\text{Y} + \mathbf{S}')
% \]

% In this way, HiFi-GAN decoder generates speech that reflects the input text while maintaining the speakerâ€™s unique sound qualities.

% \noindent \textbf{Training Objectives: } Our training process incorporates three different objectives. The first is the text generation objective, which helps the model predict future tokens based on the given text, improving its understanding of the sequence and aiding in advanced speech prediction. The second objective is audio generation, which aligns the synthesized audio with the input text. The third is the HiFi-GAN objective, which learns to transform the LLM's latents to speech signals via adversarial training. Details on each objective function can be found in Appendix C.
