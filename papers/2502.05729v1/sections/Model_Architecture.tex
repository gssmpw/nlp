
% \section{Model Architecture}
% The model architecture consists of the following trainable components:

% % \textbf{VQ-VAE:}
% % The DiscreteVAE architecture consists of an encoder, decoder, and a quantization codebook. The encoder uses 2 Conv1d layers with strides of 2, reducing input dimensionality, followed by 3 residual blocks with 1024 channels, each with ReLU activations. The decoder mirrors the encoder, starting with a Conv1d layer, followed by 3 residual blocks and 2 upsampled convolution layers, reconstructing the original input. A Quantize layer is used for vector quantization. The architecture utilizes the DiscretizationLoss function for learning discrete latent representations. This module efficiently encodes and decodes spectrograms with a total parameter count of around 51 million.
% \textbf{Conditioning Encoder and Perceiver Resampler:}
% The Conditioning Encoder \cite{casanova2024xtts} consists of an initial Conv1d layer with 80 input channels and 1024 output channels, followed by 6 Attention blocks. Each Attention block includes a Group normalization layer (32 groups, 1024 dimensions), a Conv1d layer for query-key-value computation (1024 input channels, 3072 output channels), and a final projection Conv1d layer (1024 output channels). The attention mechanism utilizes QKV attention. Dropout with a probability of 0.1 is applied to facilitate regularization. The encoder outputs a sequence, which length is dependent on the input audio duration.

% The Conditioning Encoder is followed by the Perceiver Resampler, which produces a fixed number of embeddings by utilizing cross attention mechanism. The Perceiver Resampler is composed two attention blocks, each with 512-dimensional queries and 1024-dimensional keys and values. The module includes sequential layers with linear projections and GELU activations. For normalization, it uses RMS norm.
% The total number of parameters in Conditioning Encoder and Perceiver Resampler are approximately 25.29 million and 21 millions respectively.


% \textbf{LLM:}
% For LLM, we use a GPT-2 \cite{radford2019language} model with approximately 377.89 million parameters. The GPT-2 is consists of 30 transformer blocks, each with 16 attention heads and a hidden dimension of 1024. It uses layer normalization and attention mechanisms, with the MLP blocks containing two linear layers and a GELU activation.


% \textbf{HiFi-GAN Decoder:}
% The HiFi-GAN Decoder \cite{kong2020hifi} consists of a waveform generator with multiple convolutional layers and residual blocks. It includes 4 parametrized ConvTranspose1d layers for upsampling, followed by a series of residual blocks with various dilation rates. The total number of parameters is 25.86 million. This submodule is responsible for converting intermediate GPT-2 latent representations into high-quality waveform outputs


% \section{Objective Functions}
% \subsection{Language Modeling Loss}

% \textbf{Text Token Prediction} loss, denoted as $\mathcal{L}_{\text{text}}$, measures the discrepancy between the predicted text logits and the target text labels. Let $\hat{y}_{\text{text}}$ represent the predicted logits and $y_{\text{text}}$ the ground truth target labels. The text prediction loss is calculated as:

% \begin{equation}
% \mathcal{L}_{\text{text}} = \frac{1}{N} \sum_{i=1}^{N} \text{CE}(\hat{y}_{\text{text}}^{(i)}, y_{\text{text}}^{(i)}),
% \end{equation}

% where $\text{CE}$ denotes the cross-entropy loss, and $N$ is the number of training samples.

% \paragraph{Audio Token Prediction Loss}

% The second loss is Audio Token Prediction loss, $\mathcal{L}_{\text{mel}}$, evaluates the model's performance in generating acoustic Token that match the target VQ-VAE codes. It is defined as:

% \begin{equation}
% \mathcal{L}_{\text{audio}} = \frac{1}{N} \sum_{i=1}^{N} \text{CE}(\hat{y}_{\text{audio}}^{(i)}, y_{\text{audio}}^{(i)}),
% \end{equation}

% where $\hat{y}_{\text{audio}}$ represents the predicted logits for the audio token, and $y_{\text{audio}}$ are the corresponding target VQ-VAE tokens.


% The total loss used to train the model is a weighted sum of the text and audio losses:

% \begin{equation}
% \mathcal{L}_{\text{total}} = \alpha \mathcal{L}_{\text{text}} + \beta \mathcal{L}_{\text{audio}}
% \end{equation}

% where $\alpha$ and $\beta$ are scaling factors that control the relative importance of each loss term. This combined objective ensures that the model learns both the correct phonetic representations and acoustic features.

% $\alpha$ and $\beta$ are set 0.01  and 1.0 respectively.

% \subsection{Vocoder Loss}
% We used a HiFi-GAN-based vocoder \cite{kong2020hifi} that comprises multiple discriminators: the Multi-Period Discriminator, and Multi-Scale Discriminator. For the sake of clarity, we will refer to these discriminators as a single entity. The HiFi-GAN module is trained using a least squares loss rather than the conventional binary cross-entropy loss. The discriminator is tasked with classifying real audio samples as 1 and generated samples as 0, while the generator is optimized to produce audio that can deceive the discriminator into classifying it as close to 1. The adversarial losses for the generator \(G\) and the discriminator \(D\) are defined as follows:

% \begin{align}
%     \mathcal{L}_{\text{Adv}}(D; G) &= \mathbb{E}_{(x, s)} \left[(D(x) - 1)^2 + D(G(s))^2 \right], \\
%     \mathcal{L}_{\text{Adv}}(G; D) &= \mathbb{E}_{s} \left[(D(G(s)) - 1)^2 \right],
% \end{align}

% where \(x\) represents the real audio samples, and \(s\) denotes the input mel-spectrogram conditions.

% \paragraph{Mel-Spectrogram Loss}
% The model also employs L1 loss between the mel-spectrograms of the real and generated audio. This loss is formulated as:

% \begin{align}
%     \mathcal{L}_{\text{Mel}}(G) = \mathbb{E}_{(x, s)} \left[\left\| \phi(x) - \phi(G(s)) \right\|_{1}\right],
% \end{align}

% where \(\phi\) represents the transformation function that maps a waveform to its corresponding mel-spectrogram.

% \paragraph{Feature Matching Loss}
% The feature matching loss calculates the L1 distance between the intermediate features of the real and generated audio, as extracted from multiple layers of the discriminator. It is defined as:

% \begin{align}
%     \mathcal{L}_{\text{FM}}(G; D) = \mathbb{E}_{(x, s)} \left[\sum_{i=1}^{T} \frac{1}{N_i} \left\| D^i(x) - D^i(G(s)) \right\|_{1}\right],
% \end{align}

% where \(T\) denotes the number of discriminator layers, and \(D^i\) and \(N_i\) represent the features and number of features at the \(i\)-th layer, respectively.

% \paragraph{Final Loss}
% Given that the discriminator is composed of multiple sub-discriminators, the final objectives for training the generator and the discriminator are defined as follows::

% \begin{align}
%     \mathcal{L}_{G} &= \sum_{k=1}^{K} \left[\mathcal{L}_{\text{Adv}}(G; D_k) + \lambda_{\text{FM}} \mathcal{L}_{\text{FM}}(G; D_k)\right] + \lambda_{\text{Mel}} \mathcal{L}_{\text{Mel}}(G), \\
%     \mathcal{L}_{D} &= \sum_{k=1}^{K} \mathcal{L}_{\text{Adv}}(D_k; G),
% \end{align}

% where \(D_k\) denotes the \(k\)-th sub-discriminator and \(\lambda_{\text{FM}} = 2\), \(\lambda_{\text{Mel}} = 45\). 


\section{Training Objectives}
\label{app:training_objective}

Our BnTTS model is composed of two primary modules (GPT-2 and HiFi-GAN), which are trained separately. The GPT-2 module is trained using a Language Modeling objective, while the HiFi-GAN module is optimized using HiFi-GAN loss objective. This section provides an overview of the loss functions applied during training.

\subsection{Language Modeling Loss}
1. \textbf{Text Generation Loss}: Denoted as $\mathcal{L}_{\text{text}}$, it quantifies the difference between predicted logits and ground truth labels using cross-entropy. Let $\hat{y}_{\text{text}}$ represent the predicted logits and $y_{\text{text}}$ the ground truth target labels. For a sequence with $N$ text tokens, the Text Generation Loss is calculated as: 
   \begin{equation}
   \mathcal{L}_{\text{text}} = \frac{1}{N} \sum_{i=1}^{N} \text{CE}(\hat{y}_{\text{text}}^{(i)}, y_{\text{text}}^{(i)})
   \end{equation}
   
2. \textbf{Audio Generation Loss}: Denoted as $\mathcal{L}_{\text{audio}}$, it evaluates the accuracy of generated acoustic tokens against target VQ-VAE codes using cross-entropy loss:
   \begin{equation}
   \mathcal{L}_{\text{audio}} = \frac{1}{N} \sum_{i=1}^{N} \text{CE}(\hat{y}_{\text{audio}}^{(i)}, y_{\text{audio}}^{(i)})
   \end{equation}

where $\hat{y}_{\text{audio}}$ represents the predicted logits for the audio token, $y_{\text{audio}}$ are the corresponding target VQ-VAE tokens, and $N$ is the number of audio token in the sequence.
   
Total loss combines the text generation and audio generation losses with weighted factors:
   \begin{equation}
   \mathcal{L}_{\text{total}} = \alpha \mathcal{L}_{\text{text}} + \beta \mathcal{L}_{\text{audio}} \quad (\alpha = 0.01, \beta = 1.0)
   \end{equation}

where $\alpha$ and $\beta$ are scaling factors that control the relative importance of each loss term.




\subsection{HiFi-GAN Loss}
We used a HiFi-GAN-based vocoder \cite{kong2020hifi} that comprises multiple discriminators: the Multi-Period Discriminator, and Multi-Scale Discriminator. For the sake of clarity, we will refer to these discriminators as a single entity. The HiFi-GAN module is trained using multiple losses mentioned below:

1. \textbf{Adversarial Loss}: The adversarial losses for the generator \(G\) and the discriminator \(D\) are defined as follows:
\begin{align}
    \mathcal{L}_{\text{Adv}}(D; G) &= \mathbb{E}_{(x, s)} \left[(D(x) - 1)^2 + D(G(s))^2 \right] \\
    \mathcal{L}_{\text{Adv}}(G; D) &= \mathbb{E}_{s} \left[(D(G(s)) - 1)^2 \right]
\end{align}

where \(x\) represents the real audio samples, and \(s\) denotes the input conditions.

2. \textbf{Mel-Spectrogram Loss}: This loss calculates L1 distance between the mel-spectrograms of the real and generated audio. This loss is formulated as:
\begin{align}
    \mathcal{L}_{\text{Mel}}(G) = \mathbb{E}_{(x, s)} \left[\left\| \phi(x) - \phi(G(s)) \right\|_{1}\right]
\end{align}
where \(\phi\) represents the transformation function that maps a waveform to its corresponding mel-spectrogram.

3. \textbf{Feature Matching Loss}: The feature matching loss calculates the L1 distance between the intermediate features of the real and generated audio, as extracted from multiple layers of the discriminator. It is defined as:
% \begin{align}
%     \mathcal{L}_{\text{FM}}(G; D) = \mathbb{E}_{(x, s)} \left[\sum_{i=1}^{T} \frac{1}{N_i} \left\| D^i(x) - D^i(G(s)) \right\|_{1}\right]
% \end{align}

\begin{align}
    \mathcal{L}_{\text{FM}}(G; D) = \mathbb{E}_{(x, s)} \sum_{i=1}^{T} \frac{1}{N_i} \left\| D^i(x) - D^i(G(s)) \right\|_{1}
\end{align}

where \(T\) denotes the number of discriminator layers, and \(D^i\) and \(N_i\) represent the features and number of features at the \(i\)-th layer, respectively.


\paragraph{Final Loss:}
Given that the discriminator is composed of multiple sub-discriminators, the final objectives for training the generator and the discriminator are defined as follows:
% \begin{align}
%     \mathcal{L}_{G} &= \sum_{k=1}^{K} \left[\mathcal{L}_{\text{Adv}}(G; D_k) + \lambda_{\text{FM}} \mathcal{L}_{\text{FM}}(G; D_k)\right] + \lambda_{\text{Mel}} \mathcal{L}_{\text{Mel}}(G), \\
%     \mathcal{L}_{D} &= \sum_{k=1}^{K} \mathcal{L}_{\text{Adv}}(D_k; G),
% \end{align}
\begin{align}
    \mathcal{L}_{G} &= \sum_{k=1}^{K} \left[\mathcal{L}_{\text{Adv}}(G; D_k) + \lambda_{\text{FM}} \mathcal{L}_{\text{FM}}(G; D_k)\right] \notag \\
    &\quad + \lambda_{\text{Mel}} \mathcal{L}_{\text{Mel}}(G) \\
    \mathcal{L}_{D} &= \sum_{k=1}^{K} \mathcal{L}_{\text{Adv}}(D_k; G)
\end{align}

where \(D_k\) denotes the \(k\)-th sub-discriminator and \(\lambda_{\text{FM}} = 2\), \(\lambda_{\text{Mel}} = 45\). 




% 1. \textbf{Adversarial Losses}: For generator $G$ and discriminator $D$, using least squares instead of binary cross-entropy:
%    \begin{align}
%    \mathcal{L}_{\text{Adv}}(D; G) &= \mathbb{E}_{(x, s)} \left[(D(x) - 1)^2 + D(G(s))^2 \right], \\
%    \mathcal{L}_{\text{Adv}}(G; D) &= \mathbb{E}_{s} \left[(D(G(s)) - 1)^2 \right]
%    \end{align}
% 2. \textbf{Mel-Spectrogram Loss}: Measures the L1 distance between real and generated audio mel-spectrograms:
%    \begin{equation}
%    \mathcal{L}_{\text{Mel}}(G) = \mathbb{E}_{(x, s)} \left[\| \phi(x) - \phi(G(s)) \|_{1}\right]
%    \end{equation}

% 3. \textbf{Feature Matching Loss}: Compares intermediate features from real and generated audio across discriminator layers:
%    \begin{equation}
%    \mathcal{L}_{\text{FM}}(G; D) = \mathbb{E}_{(x, s)} \left[\sum_{i=1}^{T} \frac{1}{N_i} \| D^i(x) - D^i(G(s)) \|_{1}\right]
%    \end{equation}

% 4. \textbf{Final Loss Objectives}:
%    Generator Loss:
%    \begin{equation}
%    \mathcal{L}_{G} = \mathcal{L}_{\text{Adv}}(G; D) + \lambda_{\text{FM}} \mathcal{L}_{\text{FM}}(G; D) + \lambda_{\text{Mel}} \mathcal{L}_{\text{Mel}}(G)
%    \end{equation}
%    Discriminator Loss:
%    \begin{equation}
%    \mathcal{L}_{D} = \mathcal{L}_{\text{Adv}}(D; G)
%    \end{equation}

% This framework ensures effective training of the model, balancing text and audio prediction tasks, and optimizing for high-quality audio generation.

