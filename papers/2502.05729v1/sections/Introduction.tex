\section{Introduction}
\label{sec:introduction}

Speaker adaptation in Text-to-Speech (TTS) technology has seen substantial advancements in recent years, particularly with speaker-adaptive models enhancing the naturalness and intelligibility of synthesized speech \cite{eren2023deep}. Notably, recent innovations have emphasized zero-shot and one-shot adaptation approaches \cite{kodirov2015unsupervised}. Zero-shot TTS models eliminate the need for speaker-specific training by generating speech from unseen speakers using reference audio samples \cite{min2021meta}. Despite this progress, zero-shot models often require large datasets and face challenges with out-of-distribution (OOD) voices, as they struggle to adapt effectively to novel speaker traits \cite{le2023voicebox, ju2024naturalspeech3}. Alternatively, one-shot adaptation fine-tunes pre-trained models using a single data instance, offering improved adaptability with reduced data and computational demands \cite{yan2021adaspeech2, wang2023neural}; however, the pretraining stage still necessitates substantial datasets \cite{zhang2021transfer}.

Recent works such as YourTTS \cite{bai2022yourtts} and VALL-E X \cite{xu2022vall} have made strides in cross-lingual zero-shot TTS, with YourTTS exploring English, French, and Portuguese, and VALL-E X incorporating language identification to extend support for a broader range of languages \cite{xu2022vall}. These advancements highlight the potential for multilingual TTS systems to achieve cross-lingual speech synthesis. Furthermore, the XTTS model \cite{casanova2024xtts} represents a significant leap by expanding zero-shot TTS capabilities across 16 languages. Based on the Tortoise model \cite{casanova2024xtts}, XTTS enhances voice cloning accuracy and naturalness but remains focused on high- and medium-resource languages, leaving low-resource languages such as Bangla underserved \cite{zhang2022universal, xu2023cross}. 

The scarcity of extensive datasets has hindered the adaptation of state-of-the-art (SOTA) TTS models for low-resource languages. Models like YourTTS \cite{bai2022yourtts}, VALL-E X \cite{baevski2022vall}, and Voicebox \cite{baevski2022voicebox} have demonstrated success in multilingual settings, yet their primary focus remains on languages with rich resources like English, Spanish, French, and Chinese. While a few Bangla TTS systems exist \cite{gutkin2016tts}, they often produce robotic tones \cite{hossain2018development} or are limited to a small set of static speakers \cite{gong2024initial}, lacking instant speaker adaptation capabilities and typically not being open-source.

To address these challenges, we propose the first framework for few-shot speaker adaptation in Bangla TTS. Our approach integrates Bangla into the XTTS training pipeline, with architectural modifications to accommodate Bangla’s unique phonetic and linguistic features. Our model is optimized for effective few-shot voice cloning, addressing the needs of low-resource language settings. \textbf{Our contributions} are summarized as follows: \textbf{(i)} we present the \textit{first} speaker-adapted Bangla TTS system; \textbf{(ii)} we integrate Bangla into a multilingual XTTS pipeline, optimizing the framework to accommodate the unique challenges of low-resource languages; \textbf{(iii)} we make the developed BnTTSTextEval evaluation dataset public.

 % This contribution marks a pivotal step for zero-shot TTS in Bangla, enabling natural-sounding speech generation with minimal training data. Building on XTTS’s foundational achievements,

%\textbf{(4)} Empirical results and comparisons with existing methods show that our framework offers better clarity, naturalness, and higher SMOS compared to existing Bangla TTS models.


% By extending XTTS to support Bangla, we provide a valuable resource for developing TTS systems in underrepresented languages. Our model enhances voice cloning capabilities and sets the stage for expanding zero-shot TTS to other low-resource languages, thereby promoting greater linguistic inclusivity in speech synthesis technology.
