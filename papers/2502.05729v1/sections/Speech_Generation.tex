\section{Speech Generation}
\subsection{Synthesizing Short Sequences}

The generation of short audio sequences presents challenges in the BnTTS model, particularly for texts containing fewer than 30 characters when using the default generation settings (Temperature \(T = 0.85\) and TopK = 50). The primary issues observed are twofold: (1) the generated speech often lacks intelligibility, and (2) the output speech tends to be longer than expected.

To investigate these challenges, we curated a subset of 23 short text-speech pairs from the BnStudioEval dataset. For evaluation, we utilize the Character Error Rate (CER) metric to assess intelligibility, and we introduce the Audio Duration Equality metric to evaluate the alignment between the generated and reference audio durations. The Audio Duration Equality Score quantifies the equality between two audio sample durations, \(a\) and \(b\), producing values between 0 and 1, where a score of 1 indicates identical durations. The metric is defined as:

\begin{equation}
    \text{DurationEquality}(a, b) = \frac{1}{\max\left(\frac{a}{b}, \frac{b}{a}\right)}
\end{equation}



This score approaches 1 as the durations of \(a\) and \(b\) become more equal, providing an effective measure of  discrepancy between duration of reference audio and synthesized audio .


\paragraph{Effect of Short Prompt}
Under the default settings (Exp. 1 in Table X), the model achieves a Character Error Rate (CER) of 0.081 and a Duration Equality Score of 0.699. We hypothesize that the model's inability to accurately synthesize short speech stems from its training process. During training, the model reserves between 1 to 6 seconds of audio for speaker prompting. For audio shorter than 1 second, the model uses half of the audio as the prompt. This implies that the model is accustomed to short audio prompts for short sequences. By aligning the inference process with this training strategy and using short prompts, the generation performance improves markedly, as evidenced by a higher Duration Equality Score of 0.820 and a lower CER of 0.029 in Exp. 2.

\paragraph{Effect of Temperature and Top-K Sampling}
The default temperature (\(T = 0.85\)) and top-K value (50) were found to be sub-optimal for generating short sequences. By adjusting the temperature to \(T = 1.0\) and reducing the top-K value to 2, we observed an improvement in the Duration Equality Score from 0.699 to 0.701, accompanied by a substantial reduction in CER, from 0.081 to 0.023 (as shown in Exp. 3).

\paragraph{Effect of Both Short Prompts and Temperature, Top-K}
Combining short prompts with the adjusted temperature and top-K values yielded the best results. In this configuration, the Duration Equality Score improved to 0.827, with a CER of 0.015, demonstrating that both factors are crucial for accurate short sequence generation.

The ablation study demonstrates that employing short prompts in combination with fine-tuning temperature and top-K values is essential for optimizing short sequence generation in the BnTTS model.

\begin{table}[H]
\centering

    \begin{tabular}{c|l}
        \hline
        \textbf{Variable} & \textbf{Description} \\ \hline
        \( \mathbf{T} \) & Text sequence with \( N \) tokens \\ \hline
        \( N \) & Number of tokens in the text sequence \\ \hline
        \( \mathbf{S} \) & Speaker's mel-spectrogram with \( L \) frames \\ \hline
        \( \hat{\mathbf{Y}} \) & Generated speech that matches the speaker's characteristics \\ \hline
        \( \mathbf{Y} \) & Ground truth mel-spectrogram frames for the target speech \\ \hline
        \( \mathcal{F} \) & Model responsible for producing speech conditioned on both the text and the speaker's spectrogram \\ \hline
        \( \mathbf{z} \) & Discrete codes transformed from mel-spectrogram frames using VQ-VAE \\ \hline
        \( \mathcal{C} \) & Codebook of discrete codes from VQ-VAE \\ \hline
        \( l \) & Number of layers in the Conditioning Encoder \\ \hline
        \( k \) & Number of attention heads in Scaled Dot-Product Attention \\ \hline
        \( \mathbf{S_z} \) & Intermediate representation of speaker spectrogram in \( \mathbb{R}^{L \times d} \) \\ \hline
        \( d \) & Dimensionality of each token or embedding \\ \hline
        \( \mathbf{Q}, \mathbf{K}, \mathbf{V} \) & Projections of \( \mathbf{S_z} \) used in scaled dot-product attention \\ \hline
        \( P \) & Fixed number of sequences produced by the Perceiver Resampler \\ \hline
        \( \mathbf{R} \) & Fixed-size output from the Perceiver Resampler in \( \mathbb{R}^{P \times d} \) \\ \hline
        \( \mathbf{T_e} \) & Continuous embedding space of text tokens in \( \mathbb{R}^{N \times d} \) \\ \hline
        \( \mathbf{S_p} \) & Speaker embeddings \\ \hline
        \( \mathbf{Y_z} \) & Ground truth spectrogram embeddings \\ \hline
        \( \mathbf{X} \) & Combined input during training: concatenation of speaker, text, and spectrogram embeddings \\ \hline
        \( \oplus \) & Concatenation operation \\ \hline
        \( \mathbf{H} \) & Output from the LLM consisting of hidden states for text, speaker, and spectrogram embeddings \\ \hline
        \( \mathbf{H}_\text{Y} \) & Spectrogram embedding from LLM output used for HiFi-GAN \\ \hline
        \( \mathbf{S}' \) & Resized speaker embedding to match \( \mathbf{H}_\text{Y} \) \\ \hline
        \( \mathbf{W} \) & Final audio waveform produced by HiFi-GAN \\ \hline
        \( g_\text{HiFi} \) & HiFi-GAN function converting spectrogram embeddings to audio waveform \\ \hline
    \end{tabular}
    \label{tab:variables_descriptions}
    \caption{Table of Variables and Descriptions}
\end{table}

% \section{Results and Discussion}

% To evaluate the performance of our Bengali TTS system, we employed a combination of subjective and objective metrics across two datasets: BnStudioEval and BnTTSTextEval. The BnStudioEval dataset, consisting of high-quality recordings, was used for reference-aware evaluation, while BnTTSTextEval encompassed subsets focusing on phonetic diversity, named entity pronunciation, and conversational fluency for reference-independent evaluation. The metrics used include subjective measures such as SMOS and objective measures like CER, UTMOS, SECS, and SpeechBERT Precision.



% \subsection{Reference-aware Evaluation (BnStudioEval)}
% Table 1 presents the comparative performance of various TTS systems evaluated on the BnStudioEval dataset. Among the synthetic methods, AzureTTS exhibited the best performance with the lowest Character Error Rate (CER) and the highest UTMOS score, outperforming all other synthetic methods, including Ground Truths (GT) in terms of transcription accuracy. However, interestingly, the CER of the GT remains lower than that of BnTTS synthesized outputs. As expected, the GT, serving as a reference standard, outperforms all synthetic systems across key subjective metrics such as SMOS (4.671), Naturalness (4.625), and Clarity (4.9). In this context, the proposed BnTTS system closely follows, achieving competitive scores in SMOS (4.584), Naturalness (4.531), and Clarity (4.898).

% Regarding speaker similarity, the GT achieved SECS scores of 1.0 when compared to reference audios and 0.361 when compared to the speaker prompt. BnTTS also performed well, with an SECS(Ref.) score of 0.513 and an SECS(Prompt) score of 0.335, falling short of the ground truth by only 0.031 in the latter metric. Additionally, BnTTS received a SpeechBERT Precision score of 0.796, compared to the perfect score of 1.0 set by the ground truth.

% It should be noted that IndicTTS, GTTS, and AzureTTS lack speaker impersonation capabilities, rendering them inapplicable for reference-aware metrics such as SECS and SpeechBERT Precision. Consequently, these metrics were not calculated for these systems.


% \subsection{Reference-independent Evaluation (BnTTSTextEval)}
% Table 2 presents the comparative performance of various TTS systems evaluated on the BnTTSTextEval dataset, encompassing three distinct subsets: BengaliStimuli53, BengaliNamedEntity1000, and ShortText200. The trend observed in the BnStudioEval dataset persists here as well. AzureTTS and GTTS consistently trade leading positions in transcription accuracy (CER) and automated quality prediction (UTMOS), with BnTTS following closely in third place, and IndicTTS trailing behind.

% BnTTS performs strongly in subjective evaluations, excelling in SMOS, Naturalness, and Intelligibility across the BengaliStimuli53 and BengaliNamedEntity1000 subsets. However, it falls slightly behind AzureTTS in the ShortText200 subset, which focuses on model performance in short texts. Despite this, BnTTS overall, remains the top-performing system in all subjective metrics, delivering the highest average scores in SMOS(4.383), Naturalness(4.313), and Clarity(4.737).

