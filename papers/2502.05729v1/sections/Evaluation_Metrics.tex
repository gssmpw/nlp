\section{Evaluation Metrics}
\label{app:eval_metrics}
We employed a combination of subjective and objective metrics to rigorously evaluate the performance of our TTS system, focusing on intelligibility, naturalness, speaker similarity, and transcription accuracy.

\noindent \textbf{Subjective Mean Opinion Score (SMOS):} SMOS is a perceptual evaluation where listeners rate synthesized speech on a Likert scale from 1 (poor) to 5 (excellent). It considers naturalness, clarity, and fluency, providing an absolute score for each sample. A higher SMOS indicates better overall speech quality.

\noindent \textbf{SpeechBERTScore:} SpeechBERTScore adapts BERTScore for speech, using self-supervised learning (SSL) models to compare dense representations of generated and reference speech. For generated speech waveform $\hat{X}$ and reference waveform $X$, the feature representations $\hat{Z}$ and $Z$ are extracted using a pretrained model. SpeechBERTScore is defined as the average maximum cosine similarity between feature vectors:
\[
\text{SpeechBERTScore} = \frac{1}{N_{\text{gen}}} \sum_{i=1}^{N_{\text{gen}}} \max_{j} \text{cos}(\hat{\mathbf{z}}_i, \mathbf{z}_j)
\]
where $\hat{\mathbf{z}}_i$ and $\mathbf{z}_j$ represent the SSL embeddings for generated and reference speech, respectively.

\noindent \textbf{Character Error Rate (CER):} CER measures transcription accuracy by calculating the ratio of errors (substitutions $S$, deletions $D$, and insertions $I$) in automatic speech recognition (ASR) transcriptions:
\[
CER = \frac{S + D + I}{N}
\]
where $N$ is the total number of characters in the reference transcription. A lower CER indicates better transcription accuracy.

\noindent \textbf{Speaker Encoder Cosine Similarity (SECS):} SECS evaluates speaker similarity by calculating the cosine similarity between speaker embeddings of the reference and synthesized speech:

\[
\text{SECS} = \frac{e_{\text{ref}} \cdot e_{\text{syn}}}{\|e_{\text{ref}}\| \|e_{\text{syn}}\|},
\]

where $e_{\text{ref}}$ and $e_{\text{syn}}$ are the speaker embeddings for reference and synthesized speech, respectively. SECS ranges from -1 (low similarity) to 1 (high similarity).

\label{sec:DurationEquality}
\noindent \textbf{Duration Equality Score:} This metric quantifies how closely the durations of the reference ($a$) and synthesized ($b$) speech match, with a score of 1 indicating identical durations:

\[
\text{DurationEquality}(a, b) = \frac{1}{\max\left(\frac{a}{b}, \frac{b}{a}\right)}.
\]

This score helps in assessing duration similarity between reference and generated audio, ensuring consistency in pacing.

Each metric provides a different perspective, allowing a holistic evaluation of the synthesized speech quality.


% \section{Evaluation Metrics}
% To rigorously evaluate the performance of the TTS system, a combination of subjective and objective metrics is employed. These metrics assess various dimensions of speech quality, including intelligibility, naturalness, speaker similarity, and transcription accuracy. The following describes the key evaluation metrics in detail:


% \textbf{Subjective Mean Opinion Score (SMOS)}: SMOS is a perceptual evaluation metric used to assess the overall quality of synthesized speech by human listeners. It is rated on a Likert scale from 1 (bad) to 5 (excellent), considering aspects such as naturalness, clarity, fluency, consistency, and emotional expressiveness. SMOS serves as an absolute rating, where each synthetic sample is evaluated independently, without reference to other samples. In addition to SMOS, separate scores for Naturalness and Clarity are reported for a comprehensive analysis.


% \textbf{SpeechBERTScore}:
% To evaluate the semantic consistency between the generated and reference speech in our proposed system, we employ the SpeechBERTScore metric, which extends the BERTScore framework, commonly used in text generation, to the speech domain by computing the similarity between dense speech representations derived from self-supervised learning (SSL) models. The metric aims to capture semantic congruence between the synthesized speech and a reference, accounting for differences in waveform length.

% Let the generated and reference speech waveforms be denoted as $\hat{X} = (\hat{x}_t \in \mathbb{R} \mid t = 1, \ldots, T_{\text{gen}})$ and $X = (x_t \in \mathbb{R} \mid t = 1, \ldots, T_{\text{ref}})$, respectively, where $T_{\text{gen}}$ and $T_{\text{ref}}$ represent the lengths of the generated and reference waveforms. To extract meaningful features from these waveforms, a pretrained SSL model is employed, which generates sequence representations $\hat{Z} = (\hat{\mathbf{z}}_n \in \mathbb{R}^D \mid n = 1, \ldots, N_{\text{gen}})$ and $Z = (\mathbf{z}_n \in \mathbb{R}^D \mid n = 1, \ldots, N_{\text{ref}})$ for the generated and reference speech, respectively:

% \[
% \hat{Z} = \text{Encoder}(\hat{X}; \theta), \quad Z = \text{Encoder}(X; \theta),
% \]

% where $\theta$ denotes the parameters of the pretrained encoder model, and $N_{\text{gen}}$ and $N_{\text{ref}}$ are determined by $T_{\text{gen}}$ and $T_{\text{ref}}$ based on the encoder's subsampling rate.

% The SpeechBERTScore is defined as the precision metric in the BERTScore framework, measuring the maximum cosine similarity between each feature vector in the generated speech and all feature vectors in the reference speech:

% \[
% \text{SpeechBERTScore} = \frac{1}{N_{\text{gen}}} \sum_{i=1}^{N_{\text{gen}}} \max_{j} \text{cos}(\hat{\mathbf{z}}_i, \mathbf{z}_j),
% \]

% where $\text{cos}(\hat{\mathbf{z}}_i, \mathbf{z}_j)$ is the cosine similarity between the SSL feature vectors $\hat{\mathbf{z}}_i$ from the generated speech and $\mathbf{z}_j$ from the reference speech.

% By leveraging pretrained SSL models, SpeechBERTScore captures high-level semantic information, making it suitable for evaluating synthesized speech's content and meaning. This metric is particularly advantageous for TTS evaluation, where semantic consistency and intelligibility are crucial, even in scenarios where the generated and reference audio lengths may differ.

% \textbf{Character Error Rate (CER)}: CER quantifies transcription accuracy by comparing the output of an automatic speech recognition (ASR) system on synthesized speech against a reference transcription. It is defined as:
% \[
% \text{CER} = \frac{S + D + I}{N}
% \]
% where $S$ is the number of substitutions, $D$ is the number of deletions, $I$ is the number of insertions, and $N$ is the total number of characters in the reference transcription. Lower CER values indicate higher transcription accuracy.

% \textbf{Speaker Encoder Cosine Similarity (SECS)}: SECS measures the speaker similarity between synthesized and reference speech by calculating the cosine similarity between their speaker embeddings:
% \[
% \text{SECS} = \frac{e_{\text{ref}} \cdot e_{\text{syn}}}{\|e_{\text{ref}}\| \|e_{\text{syn}}\|}
% \]
% where $e_{\text{ref}}$ and $e_{\text{syn}}$ are the speaker embeddings of the reference and synthesized speech, respectively. The similarity score ranges from -1 (low similarity) to 1 (high similarity), with higher values indicating closer resemblance in speaker characteristics.

% \textbf{DurationEquality Score}: DurationEquality quantifies the equality between two audio sample durations, \(a\) and \(b\), producing values between 0 and 1, where a score of 1 indicates identical durations. The metric is defined as:
% \begin{equation}
%     \text{DurationEquality}(a, b) = \frac{1}{\max\left(\frac{a}{b}, \frac{b}{a}\right)}
% \end{equation}
% This score approaches 1 as the durations of \(a\) and \(b\) become more equal, providing an effective measure of  discrepancy between duration of reference audio and synthesized audio .


\section{Subjective Evaluation}
For subjective evaluation of our system, we employ the Mean Opinion Score (MOS), a widely recognized metric primarily focusing on assessing the perceptual quality of audio outputs. To ensure the reliability and accuracy of our evaluations, we carefully select a panel of ten experts who are thoroughly trained in the intricacies of MOS scoring. These experts are equipped with the necessary skills and knowledge to critically assess and score the system, providing invaluable insights that help guide the refinement and enhancement of our technology. This structured approach guarantees that our evaluations are both comprehensive and precise, reflecting the true quality of the audio outputs under review.

\subsection{Evaluation Guideline}
For calculating MOS, we consider five essential evaluation criteria:
\begin{itemize} \item \textbf{Naturalness:} Evaluates how closely the TTS output resembles natural human speech. \item \textbf{Clarity:} Assesses the intelligibility and clear articulation of the spoken words. \item \textbf{Fluency:} Examines the smoothness of speech, including appropriate pacing, pausing, and intonation. \item \textbf{Consistency:} Checks the uniformity of voice quality across different texts. \item \textbf{Emotional Expressiveness:} Measures the ability of the TTS system to convey the intended emotion or tone. \end{itemize}

In the evaluation, we employ a five-point rating scale to meticulously assess performance based on specific criteria. This scale ranges from 1, denoting 'Bad' where the output has significant distortions, to 5, representing 'Excellent' where the output nearly replicates natural human speech and excels in all evaluation aspects. To capture more subtle nuances in the TTS output that might not perfectly fit into these whole-number categories, we also recommend using fractional scores. For example, a 1.5 indicates quality between 'Bad' and 'Poor,' a 2.5 signifies improvement over 'Poor' but not quite reaching 'Fair,' a 3.5 suggests better than 'Fair' but not up to 'Good,' and a 4.5 reflects performance that surpasses 'Good' but falls short of 'Excellent.' This fractional scoring allows for a more precise and detailed reflection of the system's quality, enhancing the accuracy and depth of the MOS evaluation.

\subsection{Evaluation Process}
We have developed an evaluation platform specifically designed for the subjective assessment of Text-to-Speech (TTS) systems. This platform features several key attributes that enhance the effectiveness and reliability of the evaluation process. Key features include anonymity of audio sources, ensuring that evaluators are unaware of whether the audio is synthetically generated or recorded from studio environment, or which TTS model, if any, was used. This promotes unbiased assessments based purely on audio quality. Comprehensive evaluation criteria allow evaluators to rate each audio sample on naturalness, clarity, fluency, consistency, and emotional expressiveness, ensuring a holistic review of speech synthesis quality. The user-centric interface is streamlined for ease of use, enabling efficient playback of audio samples and score entry, which reduces evaluator fatigue and maintains focus on the task. Finally, the structured data collection method systematically captures all ratings, facilitating precise analysis and enabling targeted improvements to TTS technologies. This platform is a vital tool for developers and researchers aiming to refine the effectiveness and naturalness of speech outputs in TTS systems.

\subsection{Evaluator Statistics}
For our evaluation process, we carefully selected 10 expert native speakers, achieving a balanced representation with 5 males and 5 females. The age range for these evaluators is between 20 to 28 years, ensuring a youthful perspective that aligns well with our target demographic. All evaluators are either currently enrolled as graduate students or have already completed their graduate studies. They hail from a variety of academic backgrounds, including economics, engineering, computer science, and social sciences, which provides a diverse range of insights and expertise. This careful selection of qualified individuals ensures a comprehensive and informed assessment process, suitable for our needs in evaluating advanced systems or processes where diverse, educated opinions are crucial.

\subsection{Subjective Evaluation Data Preparation} 
For reference-aware evaluation, we selected 20 audio samples from each of the four speakers, resulting in 80 Ground Truth (GT) audios. To facilitate comparison, we generated 400 synthetic samples (80 × 5) using the TTS systems examined in this study. Including the GT samples, the total dataset for this evaluation amounts to 480 audio files (400 + 80).

For the reference-independent evaluation, we utilized 453 text samples from BnTTSTextEval, comprising BengaliStimuli53 (53), BengaliNamedEntity1000 (200), and ShortText200 (200). Given the four speakers in both BnTTS-0 and BnTTS-n, this resulted in 3,624 audio samples (4 × 453 × 2). Additionally, IndicTTS, GTTS, and AzureTTS contributed 1,359 samples (3 × 453). IndicTTS samples were evenly distributed between two male and female speakers, while GTTS and AzureTTS used the "bn-IN-Wavenet-C" and "bn-IN-TanishaaNeural" voices, respectively.

In total, the reference-independent evaluation dataset comprised 5,436 audio samples. When combined with the 480 samples from the reference-aware evaluation, the overall dataset for subjective evaluation amounted to 5,916 audio files. These samples were randomly mixed and distributed to the reviewer team to ensure unbiased evaluations.

\section{Use of AI assistant}
\label{sec:use_of_ai_assistant}
We used AI assistants such as GPT-4o for spelling and grammar checking for the text of the paper.

\newpage

% \section{Potential Risks}
% \label{sec:use_of_potential risks}
% There are no potential risks associated with the outcomes of this research, as we do not utilize any sensitive information. Instead, this work will benefit the community by aiding the development of TTS systems for low-resource languages.


