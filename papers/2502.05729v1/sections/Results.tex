\section{Results}
\label{sec:results}
We evaluated the pretrained BnTTS (BnTTS-0) and speaker-adapted BnTTS (BnTTS-n) alongside IndicTTS \cite{indictts2022} and two commercial systems: Google Cloud TTS (GTTS) and Azure TTS (AzureTTS). The evaluation was conducted on both the BnStudioEval and BnTTSTextEval datasets. For a time-efficient subjective evaluation, we randomly selected 200 sentences from the BengaliNamedEntity1000 subset, which originally contains 1000 samples, maintaining a comprehensive assessment while reducing evaluation overhead.


\noindent \textbf{ Reference-aware Evaluation:}
Table \ref{tab:eval_on_studio_eval} shows the performance of various TTS systems on the BnStudioEval dataset. GTTS outperforms other methods in the CER metric, even surpassing the Ground Truth (GT) in transcription accuracy. As for the subjective measures, the proposed BnTTS-n closely follows the GT, with competitive scores in SMOS (4.624 {\em vs} 4.809), Naturalness (4.600 {\em vs} 4.798), and Clarity (4.869 {\em vs} 4.913). Meanwhile, BnTTS-0 achieves SMOS, Naturalness, and Clarity scores of 4.456, 4.447, and 4.577, respectively. IndicTTS, AzureTTS, and GTTS perform poorly in the subjective metrics.

In speaker similarity evaluation, GT attains a perfect SECS (reference) score and high SECS (prompt) scores. BnTTS-n outperforms BnTTS-0 in both SECS (reference) (0.548 {\em vs} 0.529) and SECS (prompt) (0.586 {\em vs} 0.576). Additionally, BnTTS-n achieves a SpeechBERTScore of 0.791, slightly higher than BnTTS-0 at 0.789, while GT retains a perfect score of 1.0. IndicTTS, GTTS, and AzureTTS do not support speaker adaptation, so SECS and SpeechBERTScore were not evaluated for these systems.

\noindent \textbf{ Reference-independent Evaluation:} Table \ref{tab:eval_on_BnTTSTextEval} presents the comparative performance of various TTS systems evaluated on the BnTTSTextEval dataset. The AzureTTS and GTTS consistently achieve lower CER scores, with BnTTS-n and BnTTS-0 following closely in third and fourth place, respectively, and IndicTTS trailing behind.
BnTTS-n performs strongly in subjective evaluations, excelling in SMOS, Naturalness, and Clarity scores across the BengaliStimuli53, BengaliNamedEntity1000, and ShortText200 subsets. Overall, BnTTS-n achieves the highest scores in SMOS (4.601), Naturalness (4.578), and Clarity (4.832). Meanwhile, AzureTTS performs competitively, surpassing other commercial and open-source models and achieving scores comparable to BnTTS-0. 


\renewcommand{\arraystretch}{1.1} % Adjust the row height (1.0 is default)
\begin{table}[ht!]
\centering
\footnotesize
\setlength{\tabcolsep}{2.9pt}
\resizebox{0.47\textwidth}{!}{%
\begin{tabular}{l|cccccc}
\hline
\textbf{Method} & \textbf{GT} & \textbf{IndicTTS} & \textbf{GTTS} & \textbf{AzureTTS}  &\textbf{BnTTS-0}& \textbf{BnTTS-n}\\ 
\hline
CER & \textit{0.030}& 0.058& \textbf{0.020}& 0.021&0.052& 0.034\\ 
SMOS & \textit{4.809}& 3.475& 4.017& 4.154&4.456& \textbf{4.624}\\ 
Naturalness & \textit{4.798}& 3.406& 3.949& 4.100&4.447& \textbf{4.600}\\ 
Clarity & \textit{4.913}& 4.160& 4.700& 4.686&4.577& \textbf{4.869}\\ 
SECS (Ref.) & \textit{1.0} & - & - & -  &0.529& \textbf{0.548}\\ 
SECS (Prompt) & \textit{0.641}& - & - & -  &0.576& \textbf{0.586}\\ 
SpeechBERT-\\ Score& \textit{1.0} & - & - & -  &0.789& \textbf{0.791} \\ 
\hline
\end{tabular}}
 % \vspace{-0.3cm}
\caption{Comparative average performance for reference-aware BnStudioEval dataset. SECS and SpeechBERTScore are not reported for  IndicTTS, GTTS, and AzureTTS as these systems do not support speaker adaption.}
\label{tab:eval_on_studio_eval}
\vspace{-0.2cm}
\end{table}
% ###############################################################################
\renewcommand{\arraystretch}{1.1} % Adjust the row height (1.0 is default)
\begin{table}[ht!]
\centering
\footnotesize
\setlength{\tabcolsep}{3.5pt}
\resizebox{0.47\textwidth}{!}{%
\begin{tabular}{c|c|c c c c}
\hline
\textbf{Dataset} & \textbf{Method} & \textbf{CER} & \textbf{SMOS} & \textbf{Naturalness} & \textbf{Clarity} \\ 
\hline
Bengali- & IndicTTS & 0.110& 3.445& 3.403& 3.857\\ 
Stimuli-53& GTTS     & 0.063& 4.006& 3.937& 4.688\\ 
          & AzureTTS& \textbf{0.060}& 4.108& 4.064& 4.542\\
 & BnTTS-0& 0.092& 4.622& 4.613&4.719\\ 
          & BnTTS-n& 0.086& \textbf{4.654}& \textbf{4.634}& \textbf{4.854}\\ 
\hline
Bengali- & IndicTTS & 0.049& 3.527& 3.462& 4.179\\ 
Named-& GTTS & 0.037& 4.037& 3.969& 4.712\\ 
                 Entity-& AzureTTS& \textbf{0.032}& 4.182& 4.135& 4.654\\
 1000& BnTTS-0& 0.043& 4.585& 4.613&4.698\\ 
                 (200)& BnTTS-n& 0.040& \textbf{4.635}& \textbf{4.614}& \textbf{4.841}\\ 
\hline
Short- & IndicTTS & 0.204& 3.233& 3.325& 3.893\\ 
Text-200& GTTS   & \textbf{0.043}& 4.058& 3.993& 4.705\\ 
         & AzureTTS& 0.050& 4.294& 4.256& 4.675\\
 & BnTTS-0& 0.116& 4.297& 4.271&4.556\\ 
         & BnTTS-n& 0.092& \textbf{4.554}& \textbf{4.528}& \textbf{4.816}\\ 
\hline
Overall & IndicTTS & 0.125& 3.388& 3.325& 4.017\\ 
        & GTTS     & 0.049 & 4.042& 3.976& 4.706\\ 
        & AzureTTS& \textbf{0.045}& 4.223& 4.180& 4.650\\
 & BnTTS-0& 0.081& 4.463& 4.445&4.639\\ 
        & BnTTS-n& 0.069& \textbf{4.601}& \textbf{4.578}& \textbf{4.832}\\ 
\hline
\end{tabular}}
% \vspace{-0.3cm}
\caption{Comparative average performance analysis on the reference-independent BnTTSTextEval dataset.}
\label{tab:eval_on_BnTTSTextEval}
\vspace{-0.2cm}
\end{table}
\begin{table}[ht!]
\centering
\resizebox{0.4\textwidth}{!}{%
% \setlength{\tabcolsep}{2.0pt}
% ############################################################################
\begin{tabular}{c|cccc} 
\hline
\textbf{Exp.} & \textbf{T and TopK} & \begin{tabular}[c]{@{}c@{}}\textbf{Short}\\\textbf{Prompt}\end{tabular} & \begin{tabular}[c]{@{}c@{}}\textbf{Duration}\\\textbf{Equality}\end{tabular} & \textbf{CER}  \\ 
\hline
1                & T=0.85, TopK=50     & N                                                                       & 0.699                                                                        & 0.081         \\ 

2                & T=0.85, TopK=50     & Y                                                                       & 0.820                                                                        & 0.029         \\ 

3                & T=1.0, TopK=2       & N                                                                       & 0.701                                                                        & 0.023         \\ 

4                & T=1.0, TopK=2       & Y                                                                       & \textbf{0.827 }                                                                       & \textbf{0.015}         \\
\hline
\end{tabular}}
% \vspace{-0.3cm}
\caption{Impact of prompt duration, temperature (T), and Top-K on BnTTS-n performance in the Short-BnStudioEval Dataset.}
\label{tab:eval_on_short_Studio_eval}
\vspace{-0.3cm}
\end{table}


\noindent \textbf{Zero-shot vs. Few-shot BnTTS}: BnTTS-0 consistently falls short of BnTTS-n across all metrics in both reference-aware and reference-independent evaluations. The BnTTS-n model produces more natural and intelligible speech with high speaker fidelity, leading to improved SMOS, CER, and SECS scores. This performance gap is particularly evident in the ShortText-200 dataset, which assesses conversational fluency in short, everyday phrases. The results affirm that finetuning can significantly improve the XTTS-based model for generating natural, fluent, and speaker-adapted speech.

\noindent \textbf{High CER in Text Generation}: Both BnTTS models exhibited higher CER compared to AzureTTS and GTTS in both BnStudioEval and BnTTSTextEval datasets. The AzureTTS and GTTS also achieved a lower CER score than the GT. The BnTTS generates speech with more conversational prosody and expressiveness, which, while improving perceived quality, may negatively impact CER. ASR systems, used for CER evaluation, are often better suited to transcribing standardized speech patterns, as seen in AzureTTS and GTTS. The consistent loudness and simplified prosody in these systems create clearer phonetic boundaries, making them more easily transcribed by the ASR model \cite{Yeunju2022, Wagner2019}.


\noindent \textbf{Effect of Sampling and Prompt Length on Short Speech Generation:} 
The generation of short audio sequences presents challenges in the BnTTS models, particularly for texts containing fewer than 30 characters when using the default generation settings (Temperature \(T = 0.85\) and TopK = 50). The issues observed are twofold: (1) the generated speech often lacks intelligibility, and (2) the output speech tends to be longer than expected. To investigate this, we extracted a subset of 23 short text-speech pairs from the BnStudioEval dataset, which we call ShortBnStudioEval dataset. For evaluation, we utilize the CER metric to assess intelligibility and DurationEquality (Appendix: \ref{sec:DurationEquality}) to quantify duration discrepancies in the BnTTS-n model.

Under the default settings (Exp. 1 in Table \ref{tab:eval_on_short_Studio_eval}), the model achieves a CER of 0.081 and a DurationEquality score of 0.699. We hypothesize that this issue stems from its training process. During training, the model is accustomed to short audio prompts for short sequences. By aligning the inference with this training strategy and using short prompts, the generation performance improves vastly, as evidenced by a higher DurationEquality score of 0.820 and a lower CER of 0.029 (Exp. 2). Further, by adjusting the temperature to \(T = 1.0\) and reducing the top-K value to 2, we observed an improvement in the DurationEquality score from 0.699 to 0.701, accompanied by a substantial reduction in CER from 0.081 to 0.023 (Exp. 3). Combining the short prompt with the adjusted temperature and top-K values yielded the best results. In this configuration, the DurationEquality score improved to 0.827, with a CER of 0.015, demonstrating that both factors are crucial for accurate short speech generation.


% backup
% \noindent \textbf{Effect of Sampling and Prompt Length on Short Sequence Generation:} We investigate how different configurations of temperature ($T$), top-$k$ sampling, and prompt length (short or long) impact the CER and duration equality of the BnTTS model on the short texts(less than 30 characters) from BnStudioEval dataset. Table~4 provides a comparative performance analysis under four experimental settings, with the goal of understanding how these parameters contribute to transcription accuracy and duration similarity. The results indicate that higher temperatures (i.e., $T=1.0$) combined with lower Top-$k$ values (i.e., $k=2$) improve CER, especially when using short prompts. Short prompts consistently result in better transcription accuracy and duration equality, likely due to their reduced complexity and the model's ability to generate focused outputs. The combination of these parameters offers a promising approach for achieving both accurate and temporally aligned speech synthesis in low-resource languages such as Bengali.


