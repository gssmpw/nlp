\section{BnTTS}
\begin{figure}[ht!]
    \raggedleft
    \includegraphics[width=0.90\linewidth]{resources/hqtts.png} 
    \caption{Overview of BnTTS Model.} 
    \label{fig:xtts_train_diagram}
    % \vspace{-.2cm}
\end{figure}


\textbf{Preliminaries:} Given a text sequence with \( N \) tokens, \( \mathbf{T} = \{t_1, t_2, \ldots, t_N\} \), and a speaker's mel-spectrogram \( \mathbf{S} = \{s_1, s_2, \ldots, s_L\} \), the objective is to generate speech \( \hat{\mathbf{Y}} \) that matches the speaker's characteristics. The ground truth mel-spectrogram frames for the target speech are denoted as \( \mathbf{Y} = \{y_1, y_2, \ldots, y_M\} \). The synthesis process can be described as:
% \vspace{-0.12cm}
\[
\hat{\mathbf{Y}} = \mathcal{F}(\mathbf{S}, \mathbf{T})
\]
% \vspace{-0.12cm}
where \( \mathcal{F} \) produces speech conditioned on both the text and the speaker's spectrogram.

\noindent \textbf{Audio Encoder:} A Vector Quantized-Variational AutoEncoder (VQ-VAE) \cite{tortoise} encodes mel-spectrogram frames \( \mathbf{Y} \) into discrete tokens \( M \in \mathcal{C} \), where $\mathcal{C}$ is vocab or codebook. An embedding layer then transforms these tokens into a \( d \)-dimensional vector: \( \mathbf{Y_e} \in \mathbb{R}^{M \times d} \).

\noindent \textbf{Conditioning Encoder \& Perceiver Resampler:} The Conditioning Encoder \cite{casanova2024xtts} consists of \( l \) layers of \( k \)-head Scaled Dot-Product Attention, followed by a Perceiver Resampler. The speaker spectrogram \( \mathbf{S} \) is transformed into an intermediate representation \( \mathbf{S_z} \in \mathbb{R}^{L \times d} \), where each attention layer applies a scaled dot-product attention mechanism. The Perceiver Resampler generates a fixed output dimensionality \( \mathbf{R} \in \mathbb{R}^{P \times d} \) from a variable input length \( L \).

\noindent \textbf{Text Encoder:} The text tokens \( \mathbf{T} = \{t_1, t_2, \ldots, t_N\} \) are projected into a continuous embedding space, yielding \( \mathbf{T_e} \in \mathbb{R}^{N \times d} \).

\noindent \textbf{Large Language Model (LLM):} The transformer-based LLM \cite{radford2019language} utilizes the decoder portion. Speaker embeddings \( \mathbf{S_p} \), text embeddings \( \mathbf{T_e} \), and ground truth spectrogram embeddings \( \mathbf{Y_e} \) are concatenated to form the input:
% \vspace{-0.2cm}
\[
\mathbf{X} = \mathbf{S_p} \oplus \mathbf{T_e} \oplus \mathbf{Y_e} \in \mathbb{R}^{(N + P + M) \times d}
\]
% \vspace{-0.12cm}
The LLM processes \( \mathbf{X} \), producing output \( \mathbf{H} \) with hidden states for the text, speaker, and spectrogram embeddings. During inference, only text and speaker embeddings are concatenated, generating spectrogram embeddings \( \{h_1^Y, h_2^Y, \ldots, h_P^Y\} \) as the output.

\noindent \textbf{HiFi-GAN Decoder:} The HiFi-GAN Decoder \cite{kong2020hifi} converts the LLM's output into realistic speech, preserving the speaker's characteristics. Specifically, it takes the LLM's speech head output \( \mathbf{H}_\text{Y} = \{h_1^Y, h_2^Y, \ldots, h_P^Y\} \). The speaker embedding \( \mathbf{S} \) is resized to match \( \mathbf{H}_\text{Y} \), resulting in \( \mathbf{S}' \in \mathbb{R}^{P \times d} \). The final audio waveform \( \mathbf{W} \) is then generated by:
% \vspace{-0.3cm}
\[
\mathbf{W} = g_\text{HiFi}(\mathbf{H}_\text{Y} + \mathbf{S}')
\]
% \vspace{-0.12cm}
Thus, the HiFi-GAN decoder produces speech that reflects the input text while maintaining the speaker's unique qualities.
