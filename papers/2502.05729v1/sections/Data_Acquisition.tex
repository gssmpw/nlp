
\section{TTS Data Acquisition Framework}
\label{sec:data_collection}

\begin{figure}[hbt!]
    \centering
    \includegraphics[width=0.8\linewidth]{resources/TTS_Data_Collection_Pipeline.png} 
    \caption{Overview of our TTS Data Acquisition Framework. The acquisition process involves using a Speech-to-Text model to obtain transcription, an LLM to restore transcription's punctuation, a noise suppression model to remove unwanted noise, and finally an audio superresolution model to enhance audio quality and loudness.}
    \label{fig:pseudo_labeled_dataset}
\end{figure}

Bangla is a low-resource language, and large-scale, high-quality TTS speech data are particularly scarce. To address this gap, we developed a TTS Data Acquisition Framework (Figure \ref{fig:pseudo_labeled_dataset}) designed to collect high-quality speech data with aligned transcripts. This framework leverages advanced speech processing models and carefully designed algorithms to process raw audio inputs and generate refined audio outputs with word-aligned transcripts. Below, we provide a detailed breakdown of the key components of the framework.


\textbf{1. Speech-to-Text (STT):} The audio files are first processed through an in-house our STT system, which transcribes the spoken content into text. The STT system used here is an enhanced version of the model proposed in \cite{nandi-etal-2023-pseudo}.

\textbf{2. Punctuation Restoration Using LLM:} Following transcription, a LLM is employed to restore appropriate punctuation \cite{openai2023gpt}. This step is crucial for improving grammatical accuracy and ensuring that the text is clear and coherent, aiding in further processing.

\textbf{3. Audio and Transcription Segmentation:} The audio and transcription are segmented based on terminal punctuation (full-stop, question mark, exclamatory mark, comma). This ensures that each audio segment aligns with a complete sentence, maintaining the speaker's prosody throughout.

\textbf{4. Noise and Music Suppression:} To improve audio quality, noise and music suppression techniques \cite{defossez2019music} are applied. This step ensures that the resulting audio is free of background disturbances, which could degrade TTS performance.

\textbf{5. Audio SuperResolution:} After noise suppression, the audio files undergo super-resolution processing to enhance audio fidelity \cite{liu2021voicefixer}. This ensures high-quality audio, crucial for producing natural-sounding TTS outputs.


This pipeline effectively enhances raw audio and corresponding transcription, resulting in a high-quality pseudo-labeled dataset. By combining ASR, LLM-based punctuation restoration, noise suppression, and super-resolution, the framework can generate very high-quality speech data suitable for training speech synthesis models.

\subsection{Dataset Filtering Criteria}
The pseudo-labeled data are further refined using the following criteria:

\begin{itemize}
    \item\textbf{Diarization:} Pyannote's Speaker Diarization v3.1  is employed to filter audio files by separating multi-speaker audios, ensuring that each instance contains only one speaker \cite{Plaquet23}, which is essential for effective TTS model training.

    \item \textbf{Audio Duration}: Audio segments shorter than 0.5 seconds are discarded, as they provide insufficient information for our model. Similarly, segments longer than 11 seconds are excluded to match the modelâ€™s sequence length.
    
    \item \textbf{Text Length}: Segments with transcriptions exceeding 200 characters are removed to ensure manageable input size during training.
    \item \textbf{Silence-based Filtering}: Audio files where over 35\% of the duration consists of silence are discarded, as they negatively impact model performance.
    \item \textbf{Text-to-Audio Ratio}: Based on our analysis, audio segments where the text-to-audio duration ratio falls outside (Figure \ref{fig:unprocessed_data}) the range of 6 to 25 are excluded (Figure \ref{fig:processed_data}), ensuring alignment with natural speech patterns observed in Pseudo-labeled data from Phase A (Figure \ref{fig:reviewed_data}).
\end{itemize}



\begin{figure}[hbt!]
    \centering
    % First Image: Unprocessed Data
    \begin{subfigure}[b]{0.45\textwidth}
        \centering
        \includegraphics[width=\textwidth]{resources/reviewd_data_100.png}
        \caption{The diagram illustrates the linear relationship between audio duration and character length in manually-reviewed Pseudo-labeled Data - Phase A.}
        \label{fig:reviewed_data}
    \end{subfigure}
    \hfill
    % Second Image: Processed Data
    \begin{subfigure}[b]{0.45\textwidth}
        \centering
        \includegraphics[width=\textwidth]{resources/700h_unprocessed.png}
        \caption{The diagram depicts the relationship between audio duration and character length in Pseudo-Labeled Data - Phase B.}
        \label{fig:unprocessed_data}
    \end{subfigure}
    \vskip\baselineskip
    % Third Image: Reviewd Data
    \begin{subfigure}[b]{0.45\textwidth}
        \centering
        \includegraphics[width=\textwidth]{resources/700h_processed.png}
        \caption{The diagram illustrates the audio duration vs. character length graph in Pseudo-Labeled Data - Phase B after filtering.}
        \label{fig:processed_data}
    \end{subfigure}
    \caption{These figures demonstrate how the ratio of text length to audio duration changes before and after processing the data.}
    \label{fig:audio_vs_length_grid}
\end{figure}


% \textcolor{red}{\subsection{Quality Control for Pseudo-Labeled data} These are already described in A1. Plese include phase A data review process by reviewer team} 
% Given the importance of data quality in the pseudo-labeling process, multiple manual and 
% automated verification steps are implemented to ensure accurate alignment between audio and 
% transcriptions. To address potential errors in automated methods, manual verification is conducted, focusing on key areas such as: 

% \noindent \textbf{Semantic Accuracy \& Punctuation Verification} This involves ensuring that the punctuation restoration process has preserved the intended meaning of the transcriptions and correcting any misinterpretations in punctuation placement.


% \noindent \textbf{Segmentation Accuracy}
% The process includes reviewing whether the sentence segmentation has been executed correctly, ensuring that transcriptions are chunked without breaking context. Additionally, manual adjustments are made to the segmentation whenever mis-segmentation errors are detected.


% \noindent \textbf{Speaker Diarization Validation} 
% This stage involves ensuring that each audio segment contains speech from only one speaker and does not contain overlapping speech. Additionally, any incorrectly diarized segments are identified and filtered out to maintain the clarity and accuracy of the speaker attribution in the dataset.

% \noindent \textbf{ASR Inference Correction} 
% The process includes checking if the ASR (Automatic Speech Recognition) model has misinterpreted words or phrases. Corrections are manually applied to the transcriptions for any inaccuracies identified, ensuring the accuracy of the transcribed text.


% \noindent \textbf{Audio Duration vs. Text Length Filtering}
% This involves applying a duration-to-text ratio filter to remove audio segments that are either too short or too long compared to their corresponding transcriptions. This step is critical to maintaining accurate audio-to-text alignment throughout the dataset.

\section{Human Guided Data Preparation}
\label{app:human_reviewed_data}
We curated approximately 82.39 hours of speech data through human-level observation, which we refer to as Pseudo-Labeled Data - Phase A (Table \ref{tab:dataset_info}). The audio samples, averaging 10 minutes in duration, are sourced from copyright-free audiobooks and podcasts, preferably featuring a single speaker in most cases.

Annotators were tasked with identifying prosodic sentences by segmenting the audio into meaningful chunks while simultaneously correcting ASR-generated transcriptions and restoring proper punctuation in the provided text. If a selected audio chunk contained multiple speakers, it was discarded to maintain dataset consistency. Additionally, background noise, mispronunciations, and unnatural speech patterns were carefully reviewed and eliminated to ensure the highest quality TTS training data.
