\section{The Problem}
\label{sec:problem}

% \begin{itemize}
%     \item Examples of how AI might reduce human agency:
%     \begin{itemize}
%         \item Election manipulation: LLM bot armies to influence opinions on social media~\cite{Bernardi2024}.
%         \item Cyberterrorism: LLMs can make it easier to find software vulnerabilities in critical infrastructure~\cite{Bernardi2024}.
%         \item Gradual loss of control to AI decision-makers (algorithmic bias, automation bias)~\cite{Bernardi2024}.
%         \item Institutionalization of misinformation~\cite{Garry2024llm}.
%         \item Liar's Dividend~\cite{Schiff24liar}.
%         \item Misinformation during epidemics; other examples~\cite{Blauth2022AIcrime}.
%         \item Disappearing jobs~\cite{kessler24coding}.
%     \end{itemize}
%     \item Examples of how AI might increase human agency:
%     \begin{itemize}
%         \item Enabling computing substrates with intuitive, seamless interactions.
%         \item Providing information in an unbiased and easy-to-understand way.
%         \item Improving decision-making in complex systems (better weather prediction).
%     \end{itemize}
% \end{itemize}

We are living in a time of rapid and far-reaching change, the ``Age of AI'', driven by emerging AI technologies and tools. This is naturally 
giving rise to a range of concerns about the impacts of these tools, particularly generative AI tools, on society. Our goal, and blue sky idea, 
is to lay out a research perspective for addressing these concerns, especially to mitigate the potential harms. We begin by summarizing 
the problem, before laying out the theoretical and methodological advancements we imagine for addressing it. 

There are multiple kinds of harms from generative AI that have been hypothesized and discussed in the recent literature. These can be grouped 
into the following categories:

\smallskip
\noindent
\textbf{Misuse by malicious actors:} A commonly discussed example is, emerging threats to democracy. \cite{Kreps2023democracy} summarize the three main 
challenges as threats to representation, accountability, and trust. Lawmakers (elected representatives) and 
rule-making agencies respond to public concerns, which are expressed through messages sent to them by the public, by making new laws and 
regulations. This process can be subverted by malicious actors through the use of AI tools by generating large numbers of biased or fake 
messages. Similarly, accountability, i.e., the ability of the public to choose their representation through fair elections, can be subverted by, 
e.g., the spread of misinformation to influence opinions on social media. The repeated misuse of these tools, or even the perception of misuse
can lead to the erosion of trust in the democratic process. Other examples of misuse by malicious actors include the spread of misinformation 
during crises such as epidemics~\citep{Blauth2022AIcrime}, leading to inefficient decision-making and implementation of interventions, and the 
facilitation of cyberterrorism by the increased ability to find exploits~\citep{Bernardi2024}, making people and organizations more vulnerable.

\smallskip
\noindent
\textbf{Exploitation of the changed information landscape:} Just the presence of generative AI tools and their possible use creates some
pernicious effects. The main example here is the ``liar's dividend'', where, e.g., a politician or a company can claim that any evidence of 
wrongdoing by them is fake (created by their opponents using generative AI tools)~\citep{Schiff24liar}. This effectively makes it hard for the 
public to judge evidence and to properly evaluate their choices when voting, buying products, etc.

\smallskip
\noindent
\textbf{Corruption of the tools themselves:} As these tools are being trained and iteratively improved, they can also be prone to corruption. 
Attackers can try to manipulate ML models by, e.g., providing bad training data intended to bias the model or reduce its trustworthiness. These 
are known as \emph{integrity attacks}~\citep{Blauth2022AIcrime}. An unintentional version of this is the ``institutionalization of 
misinformation'', where spurious information generated by Large Language Models (LLMs), say, is used to train other LLMs iteratively, until it 
becomes firmly entrenched or institutionalized~\citep{Garry2024llm}. This is an instance of a broader class of algorithmic bias issues, which 
have many types and sources~\citep{Fazelpour2021bias}. While the commonly known and used AI tools are being built with attempts at safeguards, 
malicious actors could also make versions of these tools which are intentionally designed to function in biased or otherwise pernicious ways. 
Deterioration in the quality of these tools can exacerbate both algorithmic bias and automation bias (as described below).

\smallskip
\noindent
\textbf{Unintended consequences:} Even without malice, generative AI tools are having such a transformative impact on society that unintended 
consequences are inevitable. An example is the rapid changes occurring in the job market, such as a sharp reduction in software engineering jobs 
due to increasing use of AI tools for coding~\citep{kessler24coding}. Other more indirect examples include automation bias and selective 
adherence to algorithmic advice~\citep{AlonBarkat2022}. Automation bias refers to the uncritical use of advice from AI decision support
systems, which is problematic because these systems are not perfect and have inbuilt and poorly understood biases of their own. On the flip
side, when human decision-makers selectively adhere to advice from AI systems, there is the risk that they might do so only when the advice 
supports preconceived notions and stereotypes. The larger and more subtle societal risk here is the gradual loss of control to AI 
decision-makers, whereby humanity might gradually cede control over our society and future~\citep{Bernardi2024}. 

The range of harms, thus, is widely varied, and the listing and categories above are by no means comprehensive. An important question at this 
stage is, do we have to deal with these problems piecemeal, as we encounter them, or can we develop a theoretical and methodological framework 
that will give us a common perspective on these problems?
