\section{The Approach}
\label{sec:approach}

Different strands of research have developed different and complementary pieces of the blue sky theory we envision. These have to be
brought together, extended, and integrated into a cohesive whole and then operationalized into a model. We summarize these strands below.

In keeping with observation O1, the sociological Relational Theory of Agency~\citep{emirbayer98agency, burkitt16relational} gives primacy to the
interactions among agents. Their definition of agency is surprisingly similar to the Planning Theory, with some key differences. In their view, human agency
consists of three parts~\citep{emirbayer98agency}. The first is an ``iterational element'', which consists of ``the selective reactivation of 
past patterns of thought and action'', which is very similar to a plan library. The second is a ``projective element'', which consists of ``the
generation of possible future trajectories of actions'', i.e., the generation of (partial) plans. The third is a ``practical-evaluative 
element'', which is the capacity ``to make practical and normative judgments among alternative possible trajectories of action''. The key
differences are that this perspective emphasizes social interactions (in the practical-evaluative element) and the cognitive aspects of planning
(in the iterational and projective elements). A limitation is that the social interactions considered are not adversarial in particular, thus 
extra work is needed to apply it to our problems of interest. While this theory is not mathematically or computationally formalized, perhaps it 
would not be a big step to do so.

More cognitively-oriented theories of agency, which are aligned with O2 and O3, are based on the ``enactive'' perspective. Enactive agency 
emerges from the interactions among agents~\citep{dejaegher09socialInteraction} and the interactions between agents and their 
environments~\citep{dipaolo17sensorimotor}. However, this theory is more focused on sensorimotor interactions, with a view to defining the
minimal conditions for agency. This contingent nature of agency has been recognized in the AAMAS community from its early days as 
well, when Franklin and Graesser pointed out that agency has be defined with respect to an environment~\citep{franklin97agentTaxonomy}.
In the enactive view, a system is an agent if it meets three requirements: self-individuation, interactional asymmetry, and 
normativity~\cite[Ch. 5]{dipaolo17sensorimotor}. Self-individuation means that the agent must be able to distinguish itself from its 
surroundings. Interactional asymmetry means that the agent is capable of initiating actions. Normativity means that these actions are
performed according to the agent's goals and norms. Thus, once again, there is a notion of goals and (implicitly) plans here, but embedded
in a social and environmental context. The added requirement is self-individuation, which has largely been taken for granted in the Planning
Theory and the BDI model.

However, defining a boundary between the agent and its environment can be tricky as agents can extend their cognition into the
environment~\cite[e.g.,][]{williams10infoDynamics}, and agency can be socially distributed in other ways~\citep{gasser91social}, including
over social networks~\citep{Tasselli2021, Mehrab2024}. But, we believe that this is a very important piece of the puzzle for the problems
we have set out to address. Simply put, if some of the cognition of a person takes place using environmental resources (e.g., computers),
this makes it vulnerable to attacks via those resources (e.g., computers can be hacked and generative AI tools are making it easier).

The use of information theory is promising in this regard as it might also offer a means of measuring the agency of an agent in an 
environment~\citep{jung11empowerment}, thus addressing O4. More recent work along these lines proposes the use of Markov 
blankets~\citep{ramstead2021multiscale} or causal blankets~\citep{rosas20causal}. In fact, Ramstead et al.'s multiscale perspective is that
the boundaries of cognition are dynamically maintained across multiple spatiotemporal scales~\citep{ramstead2021multiscale}. In their 
formulation, organisms act to minimize surprise, which turns out to be equivalent to minimizing a variational free energy. This is similar to
the empowerment idea~\citep{jung11empowerment}, and perhaps a generalization of it. The main limitation is that it is hard to apply this
formalism practically to any but the simplest of agents and environments.

Bringing it all together, our vision of a new theory of agency combines the relational and cognitive perspectives, while still including
the idea of beliefs, goals, and partial plans that are the necessary components of the Planning Theory. This would be a significant
extension of that theory, and its operationalization would be a significant extension of the BDI model. Further, we envision, 
information-theoretic ideas would be used to make this theory quantitative in the sense of providing a framework to judge how much agency changes
dynamically due to the interactions among agents, adversaries, and the environment. In practical terms, to make these judgments, we would
turn to agent-based models and simulations, so that these interactions can be evaluated in particular scenarios.

\subsection{Agent-based Modeling and Simulation}

While agent-based models (ABMs) are used in many domains, we are not aware of their use yet in modeling the harms due to generative AI
tools. LLMs are increasingly being used in ABMs to model human behavior, but they are not being used as LLMs qua LLMs~\citep{Gao2024llm-abm}.
Our idea is to develop ABMs that include representations of ``baseline'' humans using the extended BDI models described above, representations
of humans augmented with AI tools, and autonomous AI agents, all interacting in social situations of interest, such as elections or epidemics.

This blue sky idea also introduces a set of challenges for ABMs and simulation, which we discuss below. To start with, we note that ABMs are 
also generative models that produce complete and richly structured large-scale data sets. However, the theoretical and methodological challenges 
they pose are different from generative models in machine learning.

\smallskip
\noindent
\textbf{Realistic simulation design:} For answering specific questions, it is good to have the specifics represented in the model. For 
example, to assess the impact of school closures on mitigating an epidemic, it is good to have a model that represents schools well, and
also represents the activity patterns of children when schools are closed. Designing realistic simulations of the problems listed in
Section~\ref{sec:problem}~is going to be quite challenging in terms of representing the population accurately with demographics, behaviors,
information flow, etc. The use of synthetic populations or digital twins might be appropriate in this regard~\citep{Raghunathan2021synthetic}.

\smallskip
\noindent
\textbf{Scaling:} The ABMs and simulations we are proposing would be large and complex. Scaling these to large numbers of agents is an 
active area of research, even in the current BDI framework~\cite[e.g.,][]{deMooij2023framework}. Constructing simulations with large 
numbers of LLM-based agents~\cite[e.g.,][]{Fourney2024} and complex interactions is going to be challenging, though there have been some early attempts~\citep{Chopra2024llm}.

\smallskip
\noindent
\textbf{Epistemic uncertainty}: Uncertainty quantification (UQ) methods for ABMs generally attempt to estimate aleatoric uncertainty, which is 
the uncertainty in outcomes due to stochasticity in the design of the ABM. However, we need to have an understanding of epistemic uncertainty, 
especially due to ABMs necessarily being ``stylized'' at some level and available datasets having inherent uncertainties. For example, both 
mobility modeling and temperature data have some resolution, so there is an implied ``uncertainty principle'' in an ABM for estimating heat 
exposure. How will that affect an AI agent that does scheduling for a human to help avoid heat exposure? Consequently, what will be the
impact on the agency of that human? There will also be epistemic uncertainty in the behavior of adversaries.

Once we address these challenges and manage to run these simulations, they will also create new challenges in sense-making from the
results. Some of these challenges are discussed below.

\smallskip
\noindent
\textbf{Explainability}: In ML models, explainability means seeing the trees for the forest, i.e., making sense of the individual components 
(inputs, extracted features) that are together creating the output. In traditional ABMs, explainability means seeing the forest for the trees, 
i.e., they are eminently understandable at the individual level because they are designed at the individual level. The challenge is to explain 
the population-level outcomes observed due to the complex interactions that take place within the ABM. When we include complex
generative AI models in ABMs, we will make them doubly hard to explain, at the level of both the trees and the forest. This will require new 
methods in simulation analytics~\citep{swarup19analytics}.

\smallskip
\noindent
\textbf{Causality}: Large and complex ABMs are an intermediate regime between toy models and (solely)
observation real-world data, in that they produce large and richly structured output datasets, can be run
only a few times, but are complete in the sense that there is no missingness in the data and no unobserved
factors influencing the outcomes. This creates an interesting and challenging domain for causality inference.
New information-theoretic techniques might be useful, but computational challenges remain~\citep{MartinezSanchez2024synergy}.

\smallskip
\noindent
\textbf{Forecasting in agentic systems:} In some cases, particularly competitive or adversarial ones, agents can act in a way to reduce
the predictability of the system. For example, in the stock market, if there is any regularity that can be exploited to make a profit, agents
will do so. However, this will cause prices to adjust precisely in such a way that the regularity disappears. This makes the system appear
largely random, making questions of explainability and causality that much harder. How many of the problems that we are interested in (the ones
described in Section~\ref{sec:problem} and others) have this characteristic is an open question.

\smallskip
\noindent
\textbf{Intervention discovery:} While a simulation can show us what goes wrong in a particular scenario, we generally approach the 
problem of fixing it with a priori ideas about possible interventions. This ignores all the rich informational structure and intelligence
built into the ABM itself. Ideally, we should be able to use that to guide us in discovering possible interventions, which can then be
evaluated using the simulation.





