\section{The Idea}
\label{sec:idea}

% \begin{itemize}
%     \item What is the best theoretical perspective to assess the impact of AI on society? We argue that it is agency.
%     \begin{itemize}
%         \item Agency is the ability to choose your own goals and make effective plans to achieve them~\cite{Bratman1987intentions}.
%         \item We claim that the planning theory is insufficient to study all the problems of agency that arise today.
%         \item We claim that agency is fundamentally a multi-agent phenomenon. It is not that we cannot speak of the agency of a single agent, but that we need a multi-agent setting to see the range of phenomena involved that need to be part of a more complete theory of agency.
%     \end{itemize}
%     \item The BDI model is an operationalization of the planning theory of agency.
%     \begin{itemize}
%         \item However, deep learning-based agents do not strictly follow the BDI model~\cite{Fourney2024}.
%         \item The BDI model is inadequate in various ways to understanding agency, especially in the age of AI.
%         \item To see this, we can consider a thought experiment with an agent and an adversary. The agent has
%         its beliefs, goals, plans, etc. The adversary's goal is to limit or reduce the agent's agency. What are the ways
%         in which it could do this?
%     \end{itemize}
    % \begin{enumerate}
    %     \item It could counteract the agent's plan by preventing its actions from succeeding somehow.
    %     \item It could try to affect the agent's planning process so that it fails to come up with effective plans.
    %     \item It could try to influence the agent's desires and goal selection, so that the agent only develops desires and goals that align with the adversary's choices.
    %     \item It could try to influence the agent's beliefs so that the agent thinks its desires are unachievable.
    %     \item It could try to affect the agent's belief formation system, so that the agent is unable to develop
    %     true beliefs by, e.g., providing misinformation or affecting the agent's ability to distinguish good and bad information~\cite{Garry2024llm}.
    %     \item It could change the environment such that the agent only has bad options available.
    % \end{enumerate}
    % \item From the above discussion, we draw the following conclusions:
    % \begin{enumerate}
    %     \item Agency is fundamentally a multi-agent phenomenon. It is not that we cannot speak of the agency of a single agent, but that we need a multi-agent setting to see the range of phenomena involved that need to be part of a more complete theory of agency.
    %     \item Agent cognition needs to be a part of a theory of agency.
    %     \item Self-monitoring needs to be part of agent cognition, so that the agent can assess its own agency.
    %     \item A theory of agency needs to be quantitative in its assessment of an agent's agency, to make it
    %     possible to understand if an agent's agency is being diminished (or augmented). This doesn't mean we need a single
    %     number to quantify agency.
    % \end{enumerate}
% \end{itemize}

We believe that \emph{agency} is the most appropriate theoretical lens to view the problems outlined above. There are multiple
theories and definitions of agency~\cite[e.g.,][]{kiser99agency, shapiro05agency, Hodges2022agency, emirbayer98agency, Tasselli2021}.
The most commonly accepted theory in the multi-agent systems community is the Planning Theory of Agency, due to~\citet{Bratman1987intentions, 
bratman2007structures, bratman13shared_agency}. Broadly, autonomous agency is the ability to choose your own goals and make effective plans to 
achieve them, i.e., ``the agent herself directs and governs her practical thoughts and action''~\cite[p.4]{bratman2007structures}. This theory 
has been computationally operationalized as the Belief-Desire-Intention (BDI) model~\citep{rao1995bdi}, and its many variants.

Briefly, a BDI agent has \emph{beliefs} about (the current state of) the world, \emph{desires} for preferred states of the world, from which
it selects a \emph{goal}, and \emph{intentions} or (partial) plans about how to achieve its goals. This model has been tremendously 
successful in driving research in MAS. To see the usefulness (and limitations) of this model in unifying the problems from the previous
section, let us briefly take an adversarial perspective.

Consider a thought experiment with an agent and an adversary. The agent has its beliefs, goals, plans, etc. The adversary's goal is to limit or 
reduce the agent's agency. What are the ways in which it could do this? We refer to the following as the adversary's attacks on agency.
\begin{enumerate}[label=(A\arabic*)]
    \item It could counteract the agent's plan by preventing its actions from succeeding somehow.
    \item It could try to affect the agent's planning process so that it fails to come up with effective plans.
    \item It could try to influence the agent's desires and goal selection, so that the agent only develops desires and goals that align with the adversary's choices.
    \item It could try to influence the agent's beliefs so that the agent thinks its desires are unachievable.
    \item It could try to affect the agent's belief formation system, so that the agent is unable to develop
    true beliefs by, e.g., providing misinformation or affecting the agent's ability to distinguish good and bad information.
    \item It could change the environment such that the agent only has bad options available.
\end{enumerate}

Let us consider the problems from the previous section in light of this approach. The threats to representation and accountability in
democracy fall under attacks A3, A4, and A5, as does the spread of misinformation during crises. The threat of trust erosion and that of 
increased cyberterrorism falls under A6. The liar's dividend example is an instance of attack A5, as it affects the agent's ability to 
distinguish true and false information.

Tools extend agency by extending our cognitive abilities and by extending the range of actions we can take, thereby allowing new desires
and correspondingly new plans. Thus, corruption of the tools results in a reduction of agency in a variety of ways. The specifics of the ways 
the tools are used and how they are corrupted determine the class of  attack on agency. For example, AI tools can be used in planning (one's 
schedule, one's investment  strategy, etc.), and their corruption in this context would be an instance of A2. 

Clearly, most of these require that the adversary have much more control and (computational) power than the agent, which has made it impractical
in the past for large-scale attacks on the agency of individuals and populations. However, this is precisely what has changed with the
advent of generative AI tools. These tools make it relatively trivial to generate large volumes of customized messages, which can be used
to power bot armies online, for example for attacks A3--A5. The more widespread these attacks become, the more the other attacks become possible 
as well.

Finally, for the set of problems classified as unintended consequences, we note that the above attacks can happen unintentionally as well.
For example, if a person enrolls for a programming certificate course, thinking that they will easily get a software job at the end of it,
only to find that there has been a sharp drop in the demand for such jobs due to the emergence of AI coding tools~\citep{kessler24coding}, this
is effectively an instance of A6, albeit non-adversarial. Automation bias and selective adherence result in suboptimal planning and thus
are instances of A2, and the loss of control to AI decision-makers is an extreme instance of A3.

We see, therefore, that the general conceptual framework of the Planning Theory and the BDI model bring together most potential harms from 
modern AI tools into a common set of underlying principles. However, in doing so, they also expose a number of limitations of the BDI model in 
particular, and of the Planning Theory more broadly. From the above discussion, we summarize these as the following observations:
\begin{enumerate}[nolistsep,label=(O\arabic*)]
    \item \textbf{Agency is fundamentally a multi-agent phenomenon.} It is not that we cannot speak of the agency of a single agent but, as we 
    see above, we need a multi-agent setting to see the range of phenomena involved that need to be part of a more complete theory of agency. In 
    reality, this picture is even more complicated as agents can be overlapping groups of individuals, organizations, etc., any of which can be 
    allies or adversaries of each other. While the Planning Theory has been extended to small groups~\citep{bratman13shared_agency}, neither it 
    nor the BDI model are yet capable of handling the general case.
    \item \textbf{Agent cognition needs to be a part of a theory of agency.} Attacks A2--A5 are aimed at the agent's mental state and/or the 
    process by which the agent perceives, reasons, and plans, i.e., on the cognitive mechanisms of the agent. These are not typically
    considered intrinsic to the Planning Theory or the BDI model, but should be in this perspective.
    \item \textbf{Self-monitoring needs to be part of agent cognition}, so that the agent can assess its own agency. If it cannot detect
    attacks on its agency, it effectively doesn't have agency since, e.g., it would continue to re-plan despite every plan failing due to
    the adversary's actions.
    \item \textbf{A theory of agency needs to be quantitative} in its assessment of an agent's agency, to make it
    possible to understand if an agent's agency is being diminished (or augmented). This is not to say we need a single
    number to quantify agency, since it doesn't make sense to compare agency across domains (does a doctor have more agency than a lawyer? It 
    depends on whether they are dealing with a medical issue or a legal issue), but we do need the ability to quantify the impact of attacks
    on agency.
\end{enumerate}

There are multiple strands of research which need to be brought together to create this extended theory and model. We discuss these next,
along with the challenges in incorporating these advancements into agent-based models and simulations.