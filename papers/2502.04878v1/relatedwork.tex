\section{Related Work}
\textbf{SAEs for Mechanistic Interpretability.} SAEs have been demonstrated to recover sparse, monosemantic, and interpretable features from language model activations \citep{towardsmonosemanticity, leesaes, scalingmono, topksaes, gatedsaes, jumprelusaes}, however their application to mechanistic interpretability is nascent. After training, researchers often interpret the meaning of SAE latents by examining the dataset examples on which they are active, either through manual inspection using features dashboards \citep{towardsmonosemanticity} or automated interpretability techniques \citep{topksaes}. SAEs have been used for circuit analysis \citep{sparsefeaturecircuits} in the vein of \citep{olah2020zoom, olsson2022context}; to study the role of attention heads in GPT-2 \citep{kissane2024interpreting}; and to replicate the identification of a circuit for indirect object identification in GPT-2 \citep{makelov2024towards}. Transcoders, a variant of SAEs, have been used to simplify circuit analysis and applied to the greater-than circuit in GPT-2 \citep{dunefsky2024transcoders}. While these applications highlight SAEs as valuable tools for understanding language models, it remains unclear whether they identify canonical units of analysis.


\textbf{Representational Structure.} Language models trained on the next-token prediction task learn representations that model the generative process of the data. For example, \citet{li2022emergent} found that transformers trained by next-move prediction to play the board game Othello explicitly represent the board state; and \citet{gurnee2023language} used linear probes to predict geographical and temporal information from language model activations. SAEs learn these structures as well, for example, the activations of a cluster of GPT-2 SAE latents form a cycle when reconstruction activations for weekday name tokens \citep{engels2024not}. \citet{towardsmonosemanticity} find evidence of convergent global structure by applying a 2-dimensional UMAP transformation to decoder directions of SAEs of different sizes. This results in a rich structure of latent projections with regions of latents relating to similar concepts close to each other.

\textbf{Tuning Dictionary Size.} Previous work on tuning dictionary size has mixed findings regarding how SAEs scale with dictionary size. For instance, \citet{scalingmono} observed that larger SAEs learn latents absent in smaller ones, such as specific chemical elements. Conversely, \citet{towardsmonosemanticity} found similar latents across various SAE sizes, noting that latents in smaller SAEs sometimes split into multiple latents as the dictionary size increases (Appendix \ref{app:ss:exampleFeatures} includes such examples taken from our SAEs). Currently, the effect of dictionary size on the learned latents has not been systematically studied.

\textbf{Model Stitching.} Model stitching is a method by which layers from one neural network are ``stitched'' or swapped into another neural network. \citet{lenc2015understanding, bansal2021revisiting} propose that model stitching can be used to quantify representation similarity across different neural network architectures and training regimes by demonstrating that if two networks trained on the same task can be stitched together with minimal loss in performance, it suggests a high degree of alignment in their intermediate representations.