@article{olah2020zoom,
  title={Zoom in: An introduction to circuits},
  author={Olah, Chris and Cammarata, Nick and Schubert, Ludwig and Goh, Gabriel and Petrov, Michael and Carter, Shan},
  journal={Distill},
  volume={5},
  number={3},
  pages={e00024--001},
  year={2020}
}

@article{towardsmonosemanticity,
  title={Towards monosemanticity: Decomposing language models with dictionary learning},
  author={Bricken, Trenton and Templeton, Adly and Batson, Joshua and Chen, Brian and Jermyn, Adam and Conerly, Tom and Turner, Nick and Anil, Cem and Denison, Carson and Askell, Amanda and others},
  journal={Transformer Circuits Thread},
  volume={2},
  year={2023}
}

@article{leesaes,
  title={Sparse autoencoders find highly interpretable features in language models},
  author={Cunningham, Hoagy and Ewart, Aidan and Riggs, Logan and Huben, Robert and Sharkey, Lee},
  journal={arXiv preprint arXiv:2309.08600},
  year={2023}
}

@article{sparsefeaturecircuits,
  title={Sparse feature circuits: Discovering and editing interpretable causal graphs in language models},
  author={Marks, Samuel and Rager, Can and Michaud, Eric J and Belinkov, Yonatan and Bau, David and Mueller, Aaron},
  journal={arXiv preprint arXiv:2403.19647},
  year={2024}
}

@article{wang2022interpretability,
  title={Interpretability in the wild: a circuit for indirect object identification in gpt-2 small},
  author={Wang, Kevin and Variengien, Alexandre and Conmy, Arthur and Shlegeris, Buck and Steinhardt, Jacob},
  journal={arXiv preprint arXiv:2211.00593},
  year={2022}
}

@article{olsson2022context,
  title={In-context learning and induction heads},
  author={Olsson, Catherine and Elhage, Nelson and Nanda, Neel and Joseph, Nicholas and DasSarma, Nova and Henighan, Tom and Mann, Ben and Askell, Amanda and Bai, Yuntao and Chen, Anna and others},
  journal={arXiv preprint arXiv:2209.11895},
  year={2022}
}

@article{meng2022locating,
  title={Locating and editing factual associations in GPT},
  author={Meng, Kevin and Bau, David and Andonian, Alex and Belinkov, Yonatan},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={17359--17372},
  year={2022}
}

@article{toymodels,
  title={Toy models of superposition},
  author={Elhage, Nelson and Hume, Tristan and Olsson, Catherine and Schiefer, Nicholas and Henighan, Tom and Kravec, Shauna and Hatfield-Dodds, Zac and Lasenby, Robert and Drain, Dawn and Chen, Carol and others},
  journal={arXiv preprint arXiv:2209.10652},
  year={2022}
}

@article{topksaes,
  title={Scaling and evaluating sparse autoencoders},
  author={Gao, Leo and la Tour, Tom Dupr{\'e} and Tillman, Henk and Goh, Gabriel and Troll, Rajan and Radford, Alec and Sutskever, Ilya and Leike, Jan and Wu, Jeffrey},
  journal={arXiv preprint arXiv:2406.04093},
  year={2024}
}

@book{scalingmono,
  title={Scaling monosemanticity: Extracting interpretable features from claude 3 sonnet},
  author={Templeton, Adly},
  year={2024},
  publisher={Anthropic}
}

@article{gatedsaes,
  title={Improving dictionary learning with gated sparse autoencoders},
  author={Rajamanoharan, Senthooran and Conmy, Arthur and Smith, Lewis and Lieberum, Tom and Varma, Vikrant and Kram{\'a}r, J{\'a}nos and Shah, Rohin and Nanda, Neel},
  journal={arXiv preprint arXiv:2404.16014},
  year={2024}
}

@article{jumprelusaes,
  title={Jumping Ahead: Improving Reconstruction Fidelity with JumpReLU Sparse Autoencoders},
  author={Rajamanoharan, Senthooran and Lieberum, Tom and Sonnerat, Nicolas and Conmy, Arthur and Varma, Vikrant and Kram{\'a}r, J{\'a}nos and Nanda, Neel},
  journal={arXiv preprint arXiv:2407.14435},
  year={2024}
}

@article{kissane2024interpreting,
  title={Interpreting attention layer outputs with sparse autoencoders},
  author={Kissane, Connor and Krzyzanowski, Robert and Bloom, Joseph Isaac and Conmy, Arthur and Nanda, Neel},
  journal={arXiv preprint arXiv:2406.17759},
  year={2024}
}

@article{makelov2024towards,
  title={Towards principled evaluations of sparse autoencoders for interpretability and control},
  author={Makelov, Aleksandar and Lange, George and Nanda, Neel},
  journal={arXiv preprint arXiv:2405.08366},
  year={2024}
}

@article{dunefsky2024transcoders,
  title={Transcoders Find Interpretable LLM Feature Circuits},
  author={Dunefsky, Jacob and Chlenski, Philippe and Nanda, Neel},
  journal={arXiv preprint arXiv:2406.11944},
  year={2024}
}

@article{makhzani2015winner,
  title={Winner-take-all autoencoders},
  author={Makhzani, Alireza and Frey, Brendan J},
  journal={Advances in neural information processing systems},
  volume={28},
  year={2015}
}

@misc{makhzani2014ksparseautoencoders,
      title={k-Sparse Autoencoders}, 
      author={Alireza Makhzani and Brendan Frey},
      year={2014},
      eprint={1312.5663},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/1312.5663}, 
}

@article{bansal2021revisiting,
  title={Revisiting model stitching to compare neural representations},
  author={Bansal, Yamini and Nakkiran, Preetum and Barak, Boaz},
  journal={Advances in neural information processing systems},
  volume={34},
  pages={225--236},
  year={2021}
}

@inproceedings{lenc2015understanding,
  title={Understanding image representations by measuring their equivariance and equivalence},
  author={Lenc, Karel and Vedaldi, Andrea},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={991--999},
  year={2015}
}

@article{laurence1999concepts,
  title={Concepts and cognitive science},
  author={Laurence, Stephen and Margolis, Eric},
  year={1999}
}

@misc{alignmentforumstrongFeature,
	author = {Lewis Smith},
	title = {The ‘strong’ feature hypothesis could be wrong},
	howpublished ={\url{https://www.alignmentforum.org/posts/tojtPCCRpKLSHBdpn/the-strong-feature-hypothesis-could-be-wrong}, [Accessed 23-09-2024]} ,
	year = {2024},
}

@article{bills2023language,
  title={Language models can explain neurons in language models},
  author={Bills, Steven and Cammarata, Nick and Mossing, Dan and Tillman, Henk and Gao, Leo and Goh, Gabriel and Sutskever, Ilya and Leike, Jan and Wu, Jeff and Saunders, William},
  journal={URL https://openaipublic. blob. core. windows. net/neuron-explainer/paper/index. html.(Date accessed: 14.05. 2023)},
  volume={2},
  year={2023}
}

@article{achiam2023gpt,
  title={Gpt-4 technical report},
  author={Achiam, Josh and Adler, Steven and Agarwal, Sandhini and Ahmad, Lama and Akkaya, Ilge and Aleman, Florencia Leoni and Almeida, Diogo and Altenschmidt, Janko and Altman, Sam and Anadkat, Shyamal and others},
  journal={arXiv preprint arXiv:2303.08774},
  year={2023}
}

@article{wattenberg2024relational,
  title={Relational Composition in Neural Networks: A Survey and Call to Action},
  author={Wattenberg, Martin and Vi{\'e}gas, Fernanda B},
  journal={arXiv preprint arXiv:2407.14662},
  year={2024}
}

@misc{olah2023superposition,
  title={Distributed Representations: Composition \& Superposition},
  author={Chris Olah},
  howpublished={\url{https://transformer-circuits.pub/2023/superposition-composition/index.html}},
  note={An informal note on the relationship between superposition and distributed representations},
  year={2023},
  month={May 4}
}

@misc{anders_etal_2024_composedtoymodels_2d,
   title = { Sparse autoencoders find composed features in small toy models  },
   author = {Anders, Evan AND Neo, Clement AND Hoelscher-Obermaier, Jason AND Howard, Jessica N.},
   year = {2024},
   howpublished = {\url{https://www.lesswrong.com/posts/a5wwqza2cY3W7L9cj/sparse-autoencoders-find-composed-features-in-small-toy}},
}

@article{li2022emergent,
  title={Emergent world representations: Exploring a sequence model trained on a synthetic task},
  author={Li, Kenneth and Hopkins, Aspen K and Bau, David and Vi{\'e}gas, Fernanda and Pfister, Hanspeter and Wattenberg, Martin},
  journal={arXiv preprint arXiv:2210.13382},
  year={2022}
}

@article{gurnee2023language,
  title={Language models represent space and time},
  author={Gurnee, Wes and Tegmark, Max},
  journal={arXiv preprint arXiv:2310.02207},
  year={2023}
}

@article{engels2024not,
  title={Not All Language Model Features Are Linear},
  author={Engels, Joshua and Liao, Isaac and Michaud, Eric J and Gurnee, Wes and Tegmark, Max},
  journal={arXiv preprint arXiv:2405.14860},
  year={2024}
}

@article{olshausen1997sparse,
  title={Sparse coding with an overcomplete basis set: A strategy employed by V1?},
  author={Olshausen, Bruno A and Field, David J},
  journal={Vision research},
  volume={37},
  number={23},
  pages={3311--3325},
  year={1997},
  publisher={Elsevier}
}

@article{lee2007sparse,
  title={Sparse deep belief net model for visual area V2},
  author={Lee, Honglak and Ekanadham, Chaitanya and Ng, Andrew},
  journal={Advances in neural information processing systems},
  volume={20},
  year={2007}
}

@article{olah2018building,
  title={The building blocks of interpretability},
  author={Olah, Chris and Satyanarayan, Arvind and Johnson, Ian and Carter, Shan and Schubert, Ludwig and Ye, Katherine and Mordvintsev, Alexander},
  journal={Distill},
  volume={3},
  number={3},
  pages={e10},
  year={2018}
}

@article{geva2023dissecting,
  title={Dissecting recall of factual associations in auto-regressive language models},
  author={Geva, Mor and Bastings, Jasmijn and Filippova, Katja and Globerson, Amir},
  journal={arXiv preprint arXiv:2304.14767},
  year={2023}
}

@article{nanda2023progress,
  title={Progress measures for grokking via mechanistic interpretability},
  author={Nanda, Neel and Chan, Lawrence and Lieberum, Tom and Smith, Jess and Steinhardt, Jacob},
  journal={arXiv preprint arXiv:2301.05217},
  year={2023}
}

@article{elhage2021mathematical,
   title={A Mathematical Framework for Transformer Circuits},
   author={Elhage, Nelson and Nanda, Neel and Olsson, Catherine and Henighan, Tom and Joseph, Nicholas and Mann, Ben and Askell, Amanda and Bai, Yuntao and Chen, Anna and Conerly, Tom and DasSarma, Nova and Drain, Dawn and Ganguli, Deep and Hatfield-Dodds, Zac and Hernandez, Danny and Jones, Andy and Kernion, Jackson and Lovitt, Liane and Ndousse, Kamal and Amodei, Dario and Brown, Tom and Clark, Jack and Kaplan, Jared and McCandlish, Sam and Olah, Chris},
   year={2021},
   journal={Transformer Circuits Thread},
   note={https://transformer-circuits.pub/2021/framework/index.html}
}

@article{radford2019language,
  title={Language Models are Unsupervised Multitask Learners},
  author={Radford, Alec and Wu, Jeff and Child, Rewon and Luan, David and Amodei, Dario and Sutskever, Ilya},
  year={2019}
}

@misc{lieberum2024gemmascopeopensparse,
      title={Gemma Scope: Open Sparse Autoencoders Everywhere All At Once on Gemma 2}, 
      author={Tom Lieberum and Senthooran Rajamanoharan and Arthur Conmy and Lewis Smith and Nicolas Sonnerat and Vikrant Varma and János Kramár and Anca Dragan and Rohin Shah and Neel Nanda},
      year={2024},
      eprint={2408.05147},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2408.05147}, 
}


@article{team2024gemma,
  title={Gemma 2: Improving open language models at a practical size},
  author={Team, Gemma and Riviere, Morgane and Pathak, Shreya and Sessa, Pier Giuseppe and Hardin, Cassidy and Bhupatiraju, Surya and Hussenot, L{\'e}onard and Mesnard, Thomas and Shahriari, Bobak and Ram{\'e}, Alexandre and others},
  journal={arXiv preprint arXiv:2408.00118},
  year={2024}
}

@misc{neuronpedia,
    title = {Neuronpedia: Interactive Reference and Tooling for Analyzing Neural Networks},
    year = {2023},
    note = {Software available from neuronpedia.org},
    url = {https://www.neuronpedia.org},
    author = {Lin, Johnny}
}

@article{bussmann2024batchtopk,
  title={Batchtopk sparse autoencoders},
  author={Bussmann, Bart and Leask, Patrick and Nanda, Neel},
  journal={arXiv preprint arXiv:2412.06410},
  year={2024}
}

@misc{tpp,
    title = {Evaluating Sparse Autoencoders on Concept Removal Tasks},
    year = {2024},
    note = {Upcoming work, unofficially published},
    url = {https://github.com/anonymous664422},
    author = {Anonymous}
}
@inproceedings{bias_in_bios,
  title={Bias in bios: A case study of semantic representation bias in a high-stakes setting},
  author={De-Arteaga, Maria and Romanov, Alexey and Wallach, Hanna and Chayes, Jennifer and Borgs, Christian and Chouldechova, Alexandra and Geyik, Sahin and Kenthapadi, Krishnaram and Kalai, Adam Tauman},
  booktitle={proceedings of the Conference on Fairness, Accountability, and Transparency},
  pages={120--128},
  year={2019}
}

@article{amazon,
  title={Bridging Language and Items for Retrieval and Recommendation},
  author={Hou, Yupeng and Li, Jiacheng and He, Zhankui and Yan, An and Chen, Xiusi and McAuley, Julian},
  journal={arXiv preprint arXiv:2403.03952},
  year={2024}
}


@misc{codeparrot_github_code,
  title = {GitHub Code Dataset},
  author = {Tunstall, Lewis and Von Werra, Leandro and Wolf, Thomas},
  year = {2023},
  publisher = {Hugging Face},
  howpublished = {\url{https://huggingface.co/datasets/codeparrot/github-code}},
  note = {Accessed: 2024-11-28}
}
