
\documentclass{article} % For LaTeX2e
\usepackage{iclr2025_conference,times}

% Optional math commands from https://github.com/goodfeli/dlbook_notation.
\input{math_commands.tex}

\usepackage{hyperref}
\usepackage{url}
\usepackage{graphicx}
\usepackage{float}
\usepackage{changes}
\usepackage{subcaption}

\makeatletter
\AddToHook{cmd/added/before}{\def\Changes@AuthorColor{red}}
\AddToHook{cmd/deleted/before}{\def\Changes@AuthorColor{red}}
\AddToHook{cmd/replaced/before}{\def\Changes@AuthorColor{red}}
\makeatother

\title{Sparse Autoencoders Do Not Find Canonical Units of Analysis}

% Authors must not appear in the submitted version. They should be hidden
% as long as the \iclrfinalcopy macro remains commented out below.
% Non-anonymous submissions will be rejected without review.

\author{%
    Patrick Leask\thanks{These authors contributed equally to this work.} \\
    Department of Computer Science \\
    Durham University \\
    \texttt{patrickaaleask@gmail.com} \\
    \And
    Bart Bussmann\footnotemark[1] \\
    Independent \\
    \texttt{bartbussmann@gmail.com} \\
            \And
    Michael Pearce \\
    Independent \\
    \And
    Joseph Bloom \\
    Decode Research \\
    \And 
    Curt Tigges \\ 
    Decode Research \\
    \And 
    Noura Al Moubayed \\
    Department of Computer Science \\
    Durham University \\
    \And
    Lee Sharkey \\
    Apollo Research\\
    \And 
    Neel Nanda
}

% The \author macro works with any number of authors. There are two commands
% used to separate the names and addresses of multiple authors: \And and \AND.
%
% Using \And between authors leaves it to \LaTeX{} to determine where to break
% the lines. Using \AND forces a linebreak at that point. So, if \LaTeX{}
% puts 3 of 4 authors names on the first line, and the last on the second
% line, try using \AND instead of \And before the third author name.

\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}


\iclrfinalcopy % Uncomment for camera-ready version, but NOT for submission.
\begin{document}


\maketitle

\begin{abstract}
    A common goal of mechanistic interpretability is to decompose the activations of neural networks into features: interpretable properties of the input computed by the model. Sparse autoencoders (SAEs) are a popular method for finding these features in LLMs, and it has been postulated that they can be used to find a \textit{canonical} set of units: a unique and complete list of atomic features. We cast doubt on this belief using two novel techniques: SAE stitching to show they are incomplete, and meta-SAEs to show they are not atomic. SAE stitching involves inserting or swapping latents from a larger SAE into a smaller one. Latents from the larger SAE can be divided into two categories: \emph{novel latents}, which improve performance when added to the smaller SAE, indicating they capture novel information, and \emph{reconstruction latents}, which can replace corresponding latents in the smaller SAE that have similar behavior. The existence of novel features indicates incompleteness of smaller SAEs. Using meta-SAEs - SAEs trained on the decoder matrix of another SAE - we find that latents in SAEs often decompose into combinations of latents from a smaller SAE, showing that larger SAE latents are not atomic.  The resulting decompositions are often interpretable; e.g. a latent representing ``Einstein'' decomposes into ``scientist'', ``Germany'', and ``famous person''. Even if SAEs do not find canonical units of analysis, they may still be useful tools. We suggest that future research should either pursue different approaches for identifying such units, or pragmatically choose the SAE size suited to their task. We provide an interactive dashboard to explore meta-SAEs: \url{https://metasaes.streamlit.app/}


\end{abstract}



\section{Introduction}

\begin{figure}
    \centering
    \includegraphics[width=1\linewidth]{images/network_legend.png}
    \caption{Decomposition of an SAE latent representing ``Einstein'' into a set of interpretable meta-latents. The edges connecting the nodes indicate shared activation by a meta-latent, with thicker lines representing stronger connections. It demonstrates the ability of meta-SAEs to uncover the underlying compositional structure of SAE latents, revealing how a complex concept can be represented as a sparse combination of meta-latents. We built a dashboard where you can explore all meta-latents: \url{https://metasaes.streamlit.app}}
    \label{fig:metagraph}
\end{figure}

Mechanistic interpretability aims to reverse-engineer neural networks into human-interpretable algorithms \citep{olah2020zoom, meng2022locating, geva2023dissecting, nanda2023progress, elhage2021mathematical}. A key challenge of mechanistic interpretability is identifying the correct units of analysis — fundamental components that can be individually understood and collectively explain the network's function. Ideally, these units would be \textit{unique}, with no variations \citep{towardsmonosemanticity}; \textit{complete}, encompassing all necessary features \citep{toymodels}; and \textit{atomic} or \textit{irreducible}, indivisible into smaller components \citep{engels2024not}. We refer to a set of units with all of these properties as \textbf{canonical}.

Initially, researchers hoped that individual MLP neurons \citep{meng2022locating, olah2020zoom} and attention heads \citep{wang2022interpretability, olsson2022context} could serve as these units. However, these proved insufficient for interpretability due to polysemanticity, where a single neuron responds to multiple unrelated concepts \citep{olah2020zoom, toymodels}. %For example, \cite{towardsmonosemanticity} found a single language model neuron that responds to a mixture of citations, English dialogue, and Korean text. 

Recently, sparse autoencoders (SAEs) have emerged as a promising alternative by decomposing the activations of LLMs into a dictionary of interpretable and monosemantic features \citep{towardsmonosemanticity, leesaes}. A key hyperparameter when training SAEs is the dictionary size, i.e. number of latent units. Previous work conjectured that SAEs might identify a set of ``true features'' with sufficient dictionary size \citep{towardsmonosemanticity}, i.e. the canonical features that are the goal of much mechanistic interpretability research.


One challenge to the theory that SAEs identify a canonical set of units is the phenomenon of \textit{feature splitting}, where latents from smaller SAEs ``split'' into multiple, more fine-grained latents in larger SAEs \citep{towardsmonosemanticity}. For example, \citet{towardsmonosemanticity} find a base64 feature that splits into three features in a larger SAE: activating on letters, digits, and encoded ASCII in base64 text. Furthermore, \citet{scalingmono} finds that a larger SAE has latents that activate on certain specific individual chemical elements that a smaller SAE did not represent. Currently, the effect of dictionary size on the features has not been systematically studied, in part because we lack good methods to compare latents found in SAEs of different sizes.

To better understand how SAEs of different sizes capture features, we develop a method called SAE stitching. When stitching SAEs, we systematically swap clusters of latents between SAEs of different dictionary sizes based on their cosine similarity. Through this method, we observe that larger SAEs learn both more fine-grained versions of latents found in smaller SAEs, but also entirely novel latents. The existence of novel latents suggests that the reconstruction error of smaller SAEs is partially due to missing out information altogether, not just imperfect approximations to features or overly coarse latents, indicating \textit{incompleteness}.

Contrary to the story of feature splitting, we observed that some reconstruction latents had split from \textit{multiple} latents in the smaller SAE, indicating those latents were composing into more complex latents. Previous work has predicted that the sparsity penalty incentivizes latents to  represent composed features, even if they are independent \citep{wattenberg2024relational, towardsmonosemanticity, anders_etal_2024_composedtoymodels_2d}. For instance, consider a neural network that represents color and shape features, each with three values (red/green/blue and circle/square/triangle). A small SAE might learn a latent for each value. A large SAE, however, might learn latents for all 9 color-shape combinations (i.e. blue square) instead of the 6 fundamental features \citep{alignmentforumstrongFeature}. The large SAE can represent this is sparser as only one latent activates per input rather than two, see Figure \ref{fig:bluesquare}.

To investigate this, we introduce meta-SAEs, which are SAEs trained to find sparse decompositions of the decoder directions of another SAE. These decompositions are often interpretable, e.g. a latent representing ``Einstein'' decomposes into meta-latents representing ``scientist'', ``Germany'', and ``prominent figures'', among others (see Figure \ref{fig:metagraph}). This shows that latents are often not \textit{atomic}, especially in larger SAEs. We find that meta-latents are similar to latents in smaller SAEs, demonstrating that latents from larger SAEs can be interpreted as the composition of latents from smaller SAEs. 

In summary, our contributions are:

\begin{enumerate}
    \item \textbf{SAE stitching}, as a method for comparing latents across different sizes of SAE. Latents in a larger SAE are either novel latents, missing in smaller SAEs, or reconstruction latents, similar to some latents in smaller SAEs.
    \item \textbf{Meta-SAEs}, as an approach for decomposing the decoder directions of SAEs into interpretable, monosemantic meta-latents. 
\end{enumerate}

Our empirical results suggest that simply training larger SAEs is unlikely to result in a canonical set of units for all mechanistic interpretability tasks, and that the choice of dictionary size is subjective. We suggest taking a pragmatic approach to applying SAEs to mechanistic interpretability tasks, trying SAEs of several widths to see which is best suited. We are uncertain whether canonical units of analysis exist, but our results suggest that alternative approaches should be explored.

\begin{figure}
    \centering
    \includegraphics[width=\linewidth]{images/blue_square.png}
    \caption{Example of composition of latents in SAEs of different sizes. The smaller SAE has six latents, three of which reconstruct shape features, and three of which reconstruct color features. Reconstructing a shape of a specific color requires two active latents (e.g. blue and square). On the other hand, the larger SAE has nine latents, each of which reconstructs a different color and shape combination. In the larger SAE, only a single active latent is required to reconstruct the colored shape (e.g. blue square). The sparsity penalty incentivizes larger SAEs to learn compositions of latents rather than atomic latents.}
    % Due to the sparsity penalty, larger SAEs are incentivized to learn compositions of latents (e.g. blue square, one latent active) rather than atomic latents (e.g. (square, blue), two latents active).}
    \label{fig:bluesquare}
\end{figure}


\section{Sparse Autoencoders}\label{s:saes}

Sparse dictionary learning is the problem of finding a decomposition of a signal that is both sparse and overcomplete \citep{olshausen1997sparse}. \citet{lee2007sparse} initially applied the sparsity constraint to deep belief networks, with SAEs later being applied to the reconstruction of neural network activations \citep{towardsmonosemanticity, leesaes}. In the context of large language models, SAEs decompose model activations $\mathbf{x} \in \mathbb{R}^n$ into sparse linear combinations of learned directions, which are often interpretable and monosemantic. 

An SAE consists of an encoder and a decoder:

\begin{align}
    \mathbf{f}(\mathbf{x}) &:= \sigma(\mathbf{W}^\text{enc}\mathbf{x} + \mathbf{b}^\text{enc}), \label{eq:1} \\[0.5em]
    \hat{\mathbf{x}}(\mathbf{f}) &:= \mathbf{W}^\text{dec}\mathbf{f} + \mathbf{b}^\text{dec}. \label{eq:2}
\end{align}

where $\mathbf{f}(\mathbf{x}) \in \mathbb{R}^m$ is the sparse latent representation and $\hat{\mathbf{x}}(\mathbf{f}) \in \mathbb{R}^n$ is the reconstructed input. $\mathbf{W}^\text{enc}$ is the encoder matrix with dimension $n \times m$ and  $\mathbf{b}^\text{enc}$ is a vector of dimension $m$; conversely $\mathbf{W}^\text{dec}$ is the decoder matrix with dimension $m \times n$ and  $\mathbf{b}^\text{dec}$ is of dimension $n$.  The activation function $\sigma$ enforces non-negativity and sparsity in $\mathbf{f}(\mathbf{x})$, and a latent $i$ is active on a sample $\mathbf{x}$ if $f_i({\mathbf{x}}) > 0$.

SAEs are trained on the activations of a language model at a particular site, such as the residual stream, on a large text corpus, using a loss function of the form 

\begin{equation}
    \mathcal{L}(\mathbf{x}) := \underbrace{\|\mathbf{x} - \hat{\mathbf{x}}(\mathbf{f}(\mathbf{x}))\|_2^2}_{\mathcal{L}_\text{reconstruct}} + \underbrace{\lambda \mathcal{S}(\mathbf{f}(\mathbf{x}))}_{\mathcal{L}_\text{sparsity}} + \alpha \mathcal{L}_\text{aux}
\end{equation}

where $\mathcal{S}$ is a function of the latent coefficients that penalizes non-sparse decompositions, and $\lambda$ is a sparsity coefficient, where higher values of $\lambda$ encourage sparsity at the cost of higher reconstruction error. Some architectures also require the use of an auxiliary loss $\mathcal{L}_\text{aux}$, for example to recycle inactive latents in TopK SAEs \citep{topksaes}. We expand on the different SAE variants in Appendix \ref{sec:saevars} and provide a glossary of terms in Appendix \ref{sec:glossary}.

\section{Related Work}

\textbf{SAEs for Mechanistic Interpretability.} SAEs have been demonstrated to recover sparse, monosemantic, and interpretable features from language model activations \citep{towardsmonosemanticity, leesaes, scalingmono, topksaes, gatedsaes, jumprelusaes}, however their application to mechanistic interpretability is nascent. After training, researchers often interpret the meaning of SAE latents by examining the dataset examples on which they are active, either through manual inspection using features dashboards \citep{towardsmonosemanticity} or automated interpretability techniques \citep{topksaes}. SAEs have been used for circuit analysis \citep{sparsefeaturecircuits} in the vein of \citep{olah2020zoom, olsson2022context}; to study the role of attention heads in GPT-2 \citep{kissane2024interpreting}; and to replicate the identification of a circuit for indirect object identification in GPT-2 \citep{makelov2024towards}. Transcoders, a variant of SAEs, have been used to simplify circuit analysis and applied to the greater-than circuit in GPT-2 \citep{dunefsky2024transcoders}. While these applications highlight SAEs as valuable tools for understanding language models, it remains unclear whether they identify canonical units of analysis.


\textbf{Representational Structure.} Language models trained on the next-token prediction task learn representations that model the generative process of the data. For example, \citet{li2022emergent} found that transformers trained by next-move prediction to play the board game Othello explicitly represent the board state; and \citet{gurnee2023language} used linear probes to predict geographical and temporal information from language model activations. SAEs learn these structures as well, for example, the activations of a cluster of GPT-2 SAE latents form a cycle when reconstruction activations for weekday name tokens \citep{engels2024not}. \citet{towardsmonosemanticity} find evidence of convergent global structure by applying a 2-dimensional UMAP transformation to decoder directions of SAEs of different sizes. This results in a rich structure of latent projections with regions of latents relating to similar concepts close to each other.

\textbf{Tuning Dictionary Size.} Previous work on tuning dictionary size has mixed findings regarding how SAEs scale with dictionary size. For instance, \citet{scalingmono} observed that larger SAEs learn latents absent in smaller ones, such as specific chemical elements. Conversely, \citet{towardsmonosemanticity} found similar latents across various SAE sizes, noting that latents in smaller SAEs sometimes split into multiple latents as the dictionary size increases (Appendix \ref{app:ss:exampleFeatures} includes such examples taken from our SAEs). Currently, the effect of dictionary size on the learned latents has not been systematically studied.

\textbf{Model Stitching.} Model stitching is a method by which layers from one neural network are ``stitched'' or swapped into another neural network. \citet{lenc2015understanding, bansal2021revisiting} propose that model stitching can be used to quantify representation similarity across different neural network architectures and training regimes by demonstrating that if two networks trained on the same task can be stitched together with minimal loss in performance, it suggests a high degree of alignment in their intermediate representations.

\section{SAE Stitching}\label{s:stitching}

\citet{scalingmono} finds that a larger SAE has latents that activate on certain specific individual chemical elements that a smaller SAE did not represent. Conversely, \citet{towardsmonosemanticity} observes that smaller SAEs learn latents activating on broad mathematical text, while larger SAEs learn latents activating on more specific mathematical categories.  On the one hand, this suggests that training large SAEs is necessary to learn latents relating to all relevant concepts. On the other hand, this means that some of the capacity of larger SAEs is used to represent similar information as smaller SAEs, but with more latents.

To compare latents of SAEs with different dictionary sizes, we introduce SAE stitching. In SAE stitching, we add or replace latents in one SAE with latents from another and observe the effect on the reconstruction performance. We find that larger SAEs learn both finer-grained versions of latents from smaller SAEs (\textit{reconstruction latents}) and entirely new latents that capture additional information (\textit{novel latents}). SAE stitching allows us to identify these two categories of latents in larger SAEs.

\subsection{Stitching Operation}

The output of an SAE can be expressed as a sum of the contributions from the individual latents:

\begin{equation}
    \mathbf{\hat{x}} := \sum_{i=0}^{d} \mathbf{W}^{\text{dec}}_{i} f_i(\mathbf{x}) + \mathbf{b}^{\text{dec}}
\end{equation}

where $d$ is the size of the SAE dictionary, $\mathbf{W}^{\text{dec}}_{i}$ is an individual decoder direction, $f_i(\mathbf{x})$ is the activation value for the latent $i$, and $\mathbf{b}^{\text{dec}}$ is the decoder bias term. To stitch latents from one SAE into another, we modify the reconstruction as follows:

% \begin{equation}
%     \mathbf{\hat{x}} := \sum_{l_0 \in L_0} \mathbf{W}^{\text{dec}}_{0, l_0} f_{0, l_0}(\mathbf{x}) + \textbf{b}^{\text{dec}}_{0} + \sum_{l_1 \in L_1} \mathbf{W}^{\text{dec}}_{1, l_1} f_{1, l_1}(\mathbf{x})
% \end{equation}

\begin{equation}
    \mathbf{\hat{x}} := \alpha \textbf{b}^{\text{dec}}_{0} + (1-\alpha)\textbf{b}^{\text{dec}}_{1} + \sum_{l_0 \in L_0} \mathbf{W}^{\text{dec}}_{0, l_0} f_{0, l_0}(\mathbf{x}) + \sum_{l_1 \in L_1} \mathbf{W}^{\text{dec}}_{1, l_1} f_{1, l_1}(\mathbf{x}) 
\end{equation}


where $\alpha = \frac{|L_0|}{|L_0| + |L_1|}$, $\mathbf{W}^{\text{dec}}_{0, l_0}$  and  $\mathbf{W}^{\text{dec}}_{1, l_1}$ represent individual decoder directions and $L_0$ and $L_1$ are the set of latents we include from the respective SAEs. Unlike with model stitching \citep{bansal2021revisiting}, SAE decoder directions are privileged and require no transformations to stitch.


\begin{figure}
    \centering
    \includegraphics[width=\linewidth]{images/stitching_diagram.png}
    \caption{SAE stitching operation: connected subgraphs of latents can be swapped between SAEs based on cosine similarity}
    \label{fig:stitchingdiagram}
\end{figure}

We experiment with stitching on eight SAEs trained on the residual stream of layer 8 of GPT 2 Small \citep{radford2019language} with dictionary sizes ranging from 768 to 98,304 and two of the Gemma Scope SAEs \citep{lieberum2024gemmascopeopensparse} trained on Gemma 2 2B \citep{team2024gemma} with dictionary size 16,384 and 32,768. For the full list of SAE properties and training details see Appendix \ref{app:ss:modeltable}.

\subsection{Novel and reconstruction latents}

Using SAE stitching, we want to find out whether latents in larger SAEs are just more fine-grained versions of latents of smaller SAEs, or whether they represent novel information that is missed by the smaller SAEs. If we stitch a latent from a larger SAE into a smaller SAE and the reconstruction performance improves, this indicates that the latent is adding new information to the smaller SAE. If the reconstruction performance worsens instead, this indicates that the latent is overlapping and the stitched SAE is now representing the same information twice. In this case, we can instead swap the latent from the smaller SAE with its corresponding latents from the larger SAE.


However, evaluating all combinations of latents when stitching two SAEs is computationally infeasible.  \citet{towardsmonosemanticity} finds that the decoder directions of latents with similar behavior across SAEs of different sizes form local clusters with high cosine similarity.  As such, we propose thresholding on the maximum decoder cosine similarity metric as a heuristic to classify latents into the two categories, i.e. for an SAE decoder direction \( \mathbf{W}^{\text{dec}}_{1, i} \), the feature is assigned to the novel group if 

\begin{equation}
\max_{j} \left( \frac{\mathbf{W}^{\text{dec}}_{1, i} \cdot \mathbf{W}^{\text{dec}}_{0, j}}{\|\mathbf{W}^ \text{dec}_{1, i}\| \|\mathbf{W}^{\text{dec}}_{0, j}\|} \right) < \theta
\end{equation}

where \( \mathbf{W}^{\text{dec}}_{0, j} \) represents any decoder direction in \( \mathbf{W}^{\text{dec}}_{0} \). If the maximum cosine similarity is larger than the threshold, it is assigned to the reconstruction group. Figure \ref{fig:recon-vs-cs} shows the relationship between cosine similarity and the effect on the reconstruction loss of independently stitching that feature for a pair of GPT-2 SAEs. In our experiments we set the cosine threshold to 0.7 for the GPT-2 SAEs, and 0.4 for the Gemmascope SAEs, based on our exploratory analysis. Appendix Figure \ref{fig:auroc} shows the ROC curve for using different thresholds to predict the effect of adding a latent based on the maximum cosine similarity.


\begin{figure}
    \centering
    \includegraphics[width=0.75\linewidth]{images/reconeffect_vs_cosinesim.png}
    \caption{Change in MSE when adding each feature from GPT2-1536 to GPT2-768, plotted against the maximum cosine similarity of that feature to any feature in GPT2-768. Features with cosine similarity less than 0.7 tend to improve MSE, while more redundant features hurt performance. A few extreme outliers with very high cosine similarity and effect on MSE are not visible in this plot.}
    \label{fig:recon-vs-cs}
\end{figure}

Inserting reconstruction latents into the target SAE decreases the performance due to overlapping functionality. We resolve this by removing latents in the target SAE that have a high cosine similarity to the ones we are inserting, effectively swapping them with similar latents. To select which latents we swap, we construct a bipartite graph where latents from the smaller SAE form one set of vertices and latents from the larger SAE form the other. We connect latents if they have a decoder cosine similarity above the threshold. For each connected subgraph, the latents from the smaller SAE can be swapped by their corresponding connected latents in the larger SAE (see Appendix \ref{app:ss:featurefamilies} for examples of connected subgraphs).

Figure \ref{fig:interpolating} shows four transitions between pairs of SAEs of increasing size. For each transition, we first identify novel latents from the larger SAE and add them one by one, increasing the average L0 since we are inserting extra latents. We then identify groups of related reconstruction latents as defined by our bipartite graph and swap each group - removing similar latents from the smaller SAE and replacing them with their counterparts in the larger SAE. This decreases L0 on average since the larger SAE represents similar information more sparsely. Each step leads on average to improved reconstruction performance, allowing us to interpolate between different SAEs in terms of dictionary size, sparsity, and reconstruction performance.

The existence of the novel group of latents demonstrates that smaller SAEs are \textit{incomplete}, and that larger SAE learn features that are missed by the smaller SAE. The reconstruction group seems to largely reconstruct the same features as the latents in the smaller SAE, but uses more features to achieve lower sparsity (Appendix Figure \ref{fig:mse_l0_swap_effect}). In some cases, a single latent splits into two more specific latents - this is the feature splitting observed in \citet{towardsmonosemanticity}. However, we find some reconstruction group latents have high decoder cosine similarity to \textit{multiple} latents in the smaller SAE, suggesting the large SAE latent is an interpolation or composition of the smaller SAE latents (Appendix \ref{app:ss:featurefamilies}). This supports the predictions of  \citet{wattenberg2024relational, towardsmonosemanticity, anders_etal_2024_composedtoymodels_2d} that the sparsity penalty results in the undesirable composition of features that may be sparser, but do not add any new information. We explore this phenomenon further in Section \ref{sec:metasaes}.


\begin{figure}
    \centering
    \includegraphics[width=0.75\linewidth]{images/interpolation.png}
    \caption{Interpolating between SAE pairs of increasing dictionary size (768→1536→3072→6144→12288) through two steps per phase: adding novel latents (increasing L0) then swapping groups of reconstruction latents (decreasing L0 on average). Both steps on average improve reconstruction (MSE). The L0 and MSE are averages over input samples.}
    \label{fig:interpolating}
\end{figure}

\section{Meta-SAEs}\label{sec:metasaes}
In Section \ref{s:stitching}, we demonstrated through SAE stitching that increasing dictionary size leads to larger SAEs learning not only novel features, but also reconstruction latents that encode similar information to the latents in smaller SAEs. We found that some of these reconstruction latents have high cosine similarity with \textit{multiple} latents in the smaller SAE, see Appendix \ref{app:ss:featurefamilies}. This suggests that these smaller SAE latents are composing into more complex latents, such as in the example of a latent representing ``blue'' and another latent representing ``square'' combining in a ``blue square''-latent (see Figure \ref{fig:bluesquare}). If large SAE features are indeed compositions rather than \textit{atomic}, it may be possible to decompose them into more fundamental units. 



% In Section \ref{s:stitching}, we demonstrated through SAE stitching that increasing dictionary size leads to larger SAEs learning not only novel features but also a set of latents that reconstructs similar information to the latents in smaller SAEs. This phenomenon can be illustrated with a simple example: while a small SAE might use a single latent to represent a blue square, a larger SAE might employ multiple latents to encode the same feature. We found that these latents in larger SAEs respond to narrower features and have decoder directions similar to multiple smaller SAE latents.
% The fact that more latents are used in the reconstruction group to represent the same features suggests a kind of sparsity improving composition, as proposed by \cite{wattenberg2024relational, towardsmonosemanticity, anders_etal_2024_composedtoymodels_2d}. Furthermore, our ability to relate clusters of latents in larger SAEs to clusters in smaller SAEs indicates that a sparse decomposition of these composed latents may be possible.


To decompose the latents of larger SAEs, we introduce meta-SAEs. Meta-SAEs are SAEs trained to reconstruct the decoder directions $\mathbf{W}^{\text{dec}}_i$ of a standard SAE using a dictionary of meta-latents, rather than reconstructing network activations. That is, we treat the latents $\mathbf{W}^{\text{dec}}_i$ as the training data for our meta-SAE. We find that meta-latents and meta-SAE decompositions are interpretable, with the meta-latents being monosemantic. Table \ref{tab:meta-examples} provides some examples of meta-SAE decompositions.

\begin{table}[tb]
\centering
\caption{Example GPT-2 SAE latents and their meta-latent decompositions, with model-generated explanations (human-summarized) of what the latents activate on. More latent decompositions can be found on our interactive dashboard: \url{https://metasaes.streamlit.app}}
\vspace{1.em}
\begin{tabular}{|c|p{10cm}|}
    \hline
    \textbf{SAE Latent Description} & \textbf{Meta-Latent Descriptions} \\
    \hline
    \href{https://metasaes.streamlit.app/?page=Feature+Explorer&feature=11329}{Albert Einstein} & Science \& Scientists, Famous People, Space \& Astronomy, Germany, Electricity, Words starting with a capital E \\
    \hline
    
    \href{https://metasaes.streamlit.app/?page=Feature+Explorer&feature=38079}{Rugby} & Sports activities, Words starting with `R', References to Ireland, References to sports leagues, activities \& actions \\
    \hline
    
    \href{https://metasaes.streamlit.app/?page=Feature+Explorer&feature=18157}{Android Operating System} & Mobile phones, operating systems, Californian cities \\
    \hline
\end{tabular}
\label{tab:meta-examples}
\end{table}


The latents of meta-SAEs have similar decoder directions to those found in SAEs of comparable size trained directly on the same network activations. This observation further supports the hypothesis that larger SAE latents are not entirely new features but may be compositions of features already learned, albeit less precisely, by smaller models.

We use the BatchTopK SAE \citep{bussmann2024batchtopk} to train our meta-SAEs. We train our meta-SAE on the decoder directions of the GPT-2 SAE with dictionary size 49152. The meta-SAE has a dictionary size of 2304 meta-latents, with on average 4 of meta-latents active per SAE latent. Due to small number of training samples for the meta-SAE (49152), the meta-SAE is trained for 2000 epochs. We use the Adam optimizer with learning rate 1e-4 and a batch size of 4096. After training, the meta-SAE explains 55.47\% of the variance of the decoder directions of the SAE.

\subsection{Evaluating Meta-SAE Decompositions}



We follow the lead of \citep{bills2023language, towardsmonosemanticity, leesaes, jumprelusaes} in evaluating neural network and SAE latents using automated interpretability with LLMs.

First, we generate explanations of SAE latents by presenting GPT-4o-mini with a list of input sequences that activate an SAE latent to varying degrees, and prompting it to generate a natural language explanation of the feature consistent with the activations. Second, we collect all of the SAE latents on which a meta-SAE latent is active, and prompt again with the explanation and a number of top activating examples of each of the SAE latents, asking the model to provide an explanation of the common behavior of the SAE latents, which becomes the meta-SAE latent explanation. 

We evaluate the meta-SAE latent explanations in a zero-shot multiple-choice-question setting. For a given latent we prompt GPT-4o-mini with the explanations of the meta-SAE latents that are active on that latent, and ask it to choose which of 5 SAE latent explanations most relate to the explanations of the meta-SAE latents. One of these SAE latent explanations is of the correct latent, with the remaining 4 explanations corresponding to random latents from the SAE. On a random sample of 1,000 SAE latents, GPT-4o-mini chose the correct answer of the five options 73\% of the time. 

\begin{figure}[tb]
    \centering
    \includegraphics[width=0.75\linewidth]{images/sael_vs_metasael.png}
    \caption{Cosine similarity between SAE latents and meta-SAE latents. Note the high maximum cosine similarity between latents from a meta-SAE with 2304 latents, and a standard SAE with 3072 latents.}
    \label{fig:sae_meta_sim}
\end{figure}
\subsection{Comparison to Smaller SAE Latents}

In Section \ref{s:stitching}, we hypothesised that latents in larger SAEs can be described as the composition of latents in smaller SAEs. We find that meta-SAEs learn similar latents to similar sized SAEs trained on the original reconstruction problem. Plots of the maximum cosine similarity between meta-SAE latents and latents from SAEs of different sizes are shown in Figure \ref{fig:sae_meta_sim}. We validate this by replacing meta-SAE decoder directions with the most similar SAE decoder direction, and retrained the encoder. This results in only a small decrease in meta-SAE reconstruction performance (Appendix Figure \ref{fig:metasaewithsaedirections}). This suggests that larger SAE latents are indeed composed of latents from smaller SAEs.

\section{Conclusion}

Our findings challenge the idea that SAEs can discover a canonical set of features. Through SAE stitching, we demonstrated that smaller SAEs are incomplete, missing information that novel features in larger SAEs capture. Moreover, our meta-SAE experiments showed that, due to the sparsity penalty, latents in larger SAEs are often not atomic but compositions of interpretable meta-latents. These findings suggest that there is no single SAE width at which it learns a unique and complete dictionary of atomic features that can be used to explain the behavior of the model. 

These results imply that rather than converging on a unique, complete, and irreducible set of features, SAEs of different sizes offer varying granularities and compositions of features. This indicates that the choice of SAE size should be guided by the specific interpretability task at hand, accepting that no single SAE configuration provides a universal solution. However, our methods neither identify canonical units of analysis, nor the size of dictionary to use for a given task. Furthermore, our work only studies two LLMs and does not include very large SAEs, such as in \citet{scalingmono}. We also acknowledge that the use of SAEs in mechanistic interpretability is nascent, and whilst early results are encouraging, associating the learned latents of SAEs with interpretable concepts is still an open problem. In conclusion, our research suggests that alternative methods are required for identifying canonical units, and that SAE practitioners should embrace a pragmatic approach towards choosing dictionary size when using SAEs on interpretability tasks such as probing, unlearning and steering.



% Our investigation into sparse autoencoders with SAE stitching reveals that their latents do not necessarily represent canonical units of analysis within language models. Larger SAEs learn to reconstruct novel features in comparison to smaller SAEs, but they also learn to reconstruct existing features using different latents in order to increase sparsity, resulting in different sets of latents that each seem equally valid. With meta-SAEs we empirically observed that these sparse latents can be decomposed into interpretable meta-latents, corroborating the theorised compositional nature of these latents on language models.

% The observed compositional and relational nature of SAE latents aligns with models of conceptual structure in cognitive philosophy, as described by \cite{laurence1999concepts}. In particular, the \textit{Containment model} of conceptual structure posits that concepts are structured complexes composed of more basic concepts as proper parts, paralleling the idea that some latents can be more atomic than others. Conversely, the \textit{Inferential Model} suggests that concepts are defined by their inferential relations to other concepts, implying that features in language models might be better understood as nodes in a network of relationships rather than isolated units.

% These findings challenge the common assumption in mechanistic interpretability research that SAEs naturally discover canonical, independent units of analysis. Instead, our results suggest that the features learned by SAEs are influenced by hyperparameter choices and may reflect composed or interdependent representations. This raises important questions about the existence of such canonical and independent within neural networks, or whether their features are inherently relational and context-dependent.


% In conclusion, our study suggests that while SAEs are valuable tools for interpreting neural network activations, they may not inherently discover canonical or atomic units of analysis. The learned latents are influenced by hyperparameters and may reflect compositional features and relational structures rather than fundamental computational elements.By introducing SAE stitching and meta-SAEs, we provide methods to further analyze and understand the latents learned by SAEs, highlighting their compositional and relational properties.

% While we do not assert which model of conceptual structure is most applicable to the context of language models, we agree with \cite{alignmentforumstrongFeature} that the field of mechanistic interpretability will make faster progress by “thinking more carefully about the assumptions behind the framings of interpretability work”. Examining our assumptions through the lens of previous work in cognitive- and neuro-philosophy could provide valuable perspectives for advancing mechanistic interpretability research. 

% Our findings cast doubt on the belief that SAEs may find a canonical set of features that can be used to in mechanistic interpretability to understand the model's learned algorithms, and that other approaches should be pursued for this purpose. On the other hand, SAEs have still proven useful for some mechanistic interpretability tasks, and a pragmatic approach to identifying the correct unit of analysis for a task remains valuable. Furthermore, SAE stitching and meta-SAEs are useful new tools for mechanistic interpretability practitioners, by providing methods to relate SAEs of different sizes.

% Future research should focus on developing techniques that identify the basic axes of representational spaces in neural networks, potentially through methods that account for the compositional and relational nature of representations. Exploring theoretical frameworks from cognitive science and philosophy may inform and enhance mechanistic interpretability efforts, leading to a deeper understanding of how neural networks represent and process information.










\bibliography{iclr2025_conference}
\bibliographystyle{iclr2025_conference}

\raggedbottom

\appendix

\section{Appendix / supplemental material}


\subsection{Glossary of Terms}\label{sec:glossary}

\paragraph{Active Latents (L0):} For an input $x$ and SAE activation function $f(x)$, the number of non-zero elements in $f(x)$. Typically measured as average L0 across a batch: $L0 = \frac{1}{n} \sum_i \|f(x_i)\|_0$.

\paragraph{Canonical Unit:} Hypothetical, fundamental building blocks of a LLMs computation that are unique, complete, and atomic.

\paragraph{Cross-Entropy Degradation:} The increase in cross-entropy loss when replacing the model activations with the reconstruction of the SAE.

\paragraph{Decoder Directions:} The columns of the decoder matrix $W^{\text{dec}}$ that map from latent to input space. Two decoder directions with high cosine similarity suggest related features.

\paragraph{Dictionary Size:} The dimensionality of the latent space in an SAE, determining the maximum number of unique features that can be learned.

\paragraph{Feature Splitting:} A phenomenon where a broad latent learned by a smaller SAE splits into more fine-grained latents in a larger SAE.

\paragraph{Latent:} The encoder-decoder pair corresponding to single element in the SAE's dictionary, i.e. a learned feature of the SAE rather than a feature of the data. 

\paragraph{Mechanistic Interpretability:} The study of reverse-engineering neural networks into interpretable algorithms, focusing on identifying and understanding computational features and circuits.

\paragraph{Meta-latents:} Features learned by a meta-SAE when trained on the decoder directions of another SAE.

\paragraph{Monosemantic:} Property of a feature that responds selectively to a single coherent concept. Contrasts with polysemantic features.

\paragraph{Novel Latents:} Features in a larger SAE with maximum cosine similarity below threshold $\theta$ to any feature in a smaller SAE, indicating capture of previously unrepresented information.

\paragraph{Polysemanticity:} The phenomenon where individual neurons or features respond to multiple unrelated concepts.

\paragraph{Reconstruction Latents:} Features in a larger SAE with maximum cosine similarity above threshold $\theta$ to features in a smaller SAE, representing refined or specialized versions of existing features.

\paragraph{Residual Stream:} In the context of transformer architectures, the residual stream refers to the main information flow that bypasses the self-attention and feed-forward layers through residual connections.

\paragraph{SAE Stitching:} A technique for analyzing feature relationships across SAEs of different sizes by systematically transferring latents based on decoder similarity.

\paragraph{Sparsity Coefficient ($\lambda$):} Hyperparameter in the loss function of some SAE architectures $L = \|x - \hat{x}\|^2 + \lambda S(f(x))$ controlling the trade-off between reconstruction accuracy and activation sparsity.

\paragraph{TopK:} A sparsification approach that maintains exactly $k$ non-zero activations per input by zeroing all but the $k$ largest values: $\text{TopK}(x)_i = x_i$ if $x_i$ is among $k$ largest elements, 0 otherwise.



\subsection{SAE Variants}\label{sec:saevars}
\textbf{ReLU SAEs} \citep{towardsmonosemanticity} use the L1-norm $S(\boldsymbol{f}) := || \boldsymbol{f} ||_1$ as an approximation to the L0-norm for the sparsity penalty. This provides a gradient for training unlike the L0-norm, but suppresses latent activations harming reconstruction performance \citep{gatedsaes}. Furthermore, the L1 penalty can be arbitrarily reduced through reparameterization by scaling the decoder parameters, which is resolved in \citet{towardsmonosemanticity} by constraining the decoder directions to the unit norm. Resolving this tension between activation sparsity and value is the motivation behind more recent architecture variants.

\textbf{TopK SAEs} \citep{topksaes, makhzani2014ksparseautoencoders} enforce sparsity by retaining only the top $k$ activations per sample. The encoder is defined as:

\begin{equation}
    \boldsymbol{f}(\boldsymbol{x}) := \text{TopK}(\mathbf{W}^\text{enc}\mathbf{x} + \mathbf{b}^\text{enc})
\end{equation}

where $\text{TopK}$ zeroes out all but the $k$ largest activations in each sample. This approach eliminates the need for an explicit sparsity penalty but imposes a rigid constraint on the number of active latents per sample. An auxiliary loss $\mathcal{L}_{aux} = || e - \hat{e} ||^2$ is used to avoid dead latents, where $\hat{e} = W^{dec} z$ is the reconstruction using only the top-$k_{aux}$ dead latents (usually 512), this loss is scaled by a small coefficient $\alpha$ (usually 1/32). 

\textbf{BatchTopK SAEs} \cite{bussmann2024batchtopk} relaxes the top-k constraint of TopK SAEs to the batch level during training, i.e. retaining the top $b*k$ activations per batch where $b$ is the size of the batch. This introduces a dependency between samples in the batch, so at test time a global threshold parameter $\theta$, estimated on the training data, is used instead, with only activations above this threshold being retained.

\begin{equation}
\theta = \mathbb{E}_\mathbf{X} [\min \{z_{i,j}(\mathbf{X}) | z_{i,j}(\mathbf{X}) > 0 \}]
\end{equation}

\textbf{JumpReLU SAEs} \citep{jumprelusaes} replace the standard ReLU activation function with the JumpReLU activation, defined as

\begin{equation}
    \text{JumpReLU}_\theta(z) := zH(z-\theta)
\end{equation}

where $H$ is the Heaviside step function, and $\theta$ is a learned parameter for each SAE latent, below which the activation is set to zero. JumpReLU SAEs are trained using a loss function that combines L2 reconstruction error with an L0 sparsity penalty, using straight-through estimators to train despite the discontinuous activation function. A major drawback of the sparsity penalty used in JumpReLU SAEs compared to (Batch)TopK SAEs is that it is not possible to set an explicit sparsity and targeting a specific sparsity involves costly hyperparameter tuning. While evaluating JumpReLU SAEs, \citet{jumprelusaes} chose the SAEs from their sweep that were closest to the desired sparsity level, but this resulted in SAEs with significantly different sparsity levels being directly compared. JumpReLU SAEs use no auxiliary loss function.

\subsection{Example latents}\label{app:ss:exampleFeatures}

Figure \ref{fig:decodercosinesim} shows a histogram of the maximum decoder cosine similarity for each latent in GPT2-1536 over all latents in GPT2-768. On the right-hand-side, there is a cluster of latents with high cosine similarity. 

\citet{towardsmonosemanticity} use the cosine similarity between latent activations as a measure for latent similarity. However, we use decoder cosine similarity due its lower computational cost and because it captures the latent's effect on the reconstruction. Empirically, we find a high correlation between these two metrics at values of decoder similarity relevant to our stitching experiments (see Figure \ref{fig:activationsimilarity}).

\begin{figure}[H]
    \centering
    \includegraphics[width=0.75\linewidth]{images/decodercosinesimilarity.png}
    \caption{Distribution of maximum cosine similarities between decoder weights of latents in GPT2-1536 and GPT2-768. Many latents in the larger SAE have high similarity to latents in the smaller SAE, but there is also a long tail of novel latents.}
    \label{fig:decodercosinesim}
\end{figure}

\begin{figure}
    \centering
    \includegraphics[width=0.5\linewidth]{images/activationsimilarity.png}
    \caption{The activation similarity by the decoder cosine similarity on the GPT2-768 and GPT2-1536 SAEs. At high values of cosine similarity ($> 0.5$) the coefficient of determination between these is 0.87.}
    \label{fig:activationsimilarity}
\end{figure}

Figure \ref{fig:highcosinesim} and Figure \ref{fig:lowcosineexamples} show feature dashboards from Neuronpedia \citep{neuronpedia}. Feature dashboards are a popular method to visualize and interpret SAE latents, see also \citet{towardsmonosemanticity}. These dashboards show tokens in input sentences that maximally activate a certain latent in green, and the tokens they boost (in blue) and suppress most (in red).

Figure \ref{fig:highcosinesim} shows an example of a latent from GPT-1536 and a latent from GPT-768 that have a cosine similarity of 0.99. We see that both of these latents activate strongly on the same inputs, and boost similar logits. 

\begin{figure}[H]
    \centering
    \includegraphics[width=0.75\linewidth]{images/high_sim_comparison.png}
    \caption{Neuronpedia dashboard of example latents with high cosine similarity. \href{https://www.neuronpedia.org/}{(Redacted URL)}}
    \label{fig:highcosinesim}
\end{figure}

However, GPT2-1536 has a latent for ``make sure'' that has no counterpart in GPT-768. The nearest latents have a decoder cosine similarity of around 0.3, and are shown in 
\begin{figure}[H]
    \centering
    \includegraphics[width=1\linewidth]{images/lowcosinesim.png}
    \caption{Example GPT2-1536 latent with no similar latent in GPT-768, with the three most similar latents shown. \href{https://www.neuronpedia.org/}{(Redacted URL)}}
    \label{fig:lowcosineexamples}
\end{figure}

We evaluate the reconstruction performance of the two SAEs on inputs where this latent is active and inactive. The reconstruction performance of the smaller SAE is considerably worse on inputs where this larger SAE latent is active, compared to inputs where the latent is not active.

\begin{table}[H]
\begin{tabular}{llll}
\hline
                   & \textbf{Latent inactive} & \textbf{Latent active} & \textbf{Difference} \\ \hline
GPT2-1536 & 2.225                     & 2.518                   & 0.293               \\
GPT2-768 & 2.703                     & 3.292                   & 0.589              \\ \hline
\end{tabular}
\end{table}

Averaging this metric across all 657 latents in GPT-1536 that have low maximum cosine similarity with all latents in GPT-768, we see a similar pattern (Figure \ref{fig:missing-feature-comp})

\begin{figure}[H]
    \centering
    \includegraphics[width=0.75\linewidth]{images/missingfeaturecomparison.png}
    \caption{Reconstruction MSE of SAEs on inputs where novel latents in the larger SAE are active and inactive}
    \label{fig:missing-feature-comp}
\end{figure}

\subsection{Latent families}\label{app:ss:featurefamilies}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.75\linewidth]{images/feature_families.png}
    \caption{Connected subgraphs of the bipartite graph of latent in GPT2-768 and GPT2-1536.}
    \label{fig:feature-families}
\end{figure}


\subsection{Open source SAE weights}\label{modeltabel}
All GPT-2 Small SAEs were trained on the layer 8 residual stream, which was chosen in line with \cite{topksaes}. They were trained for 300M tokens on the OpenWebText dataset, which was processed into sequences of a maximum of 128 tokens for input into the language models. All models were trained using the Adam optimizer with a learning rate of $4 \times 10^{-4}$, $\beta_1 = 0.9$, and $\beta_2 = 0.99$. The batch size used was 4096 and all were trained with a sparsity penalty of $8 \times 10^{-5}$. The GPT-2 SAEs are available on Neuronpedia at Redacted URL. We also use two of the Gemma Scope SAEs \citep{lieberum2024gemmascopeopensparse} trained on Gemma 2 2B \citep{team2024gemma} with dictionary size 16384 and 32768. We used the TransformerLens (\url{https://transformerlensorg.github.io/TransformerLens/}) implementations of GPT-2 and Gemma 2 2B. CELR is the cross entropy loss recovered from either zero or mean ablation.

\label{app:ss:modeltable}
\begin{table}[H]
\caption{Properties of the SAEs used in this study. }
\begin{tabular}{l llllll}
\hline
{\color[HTML]{333333} \textbf{Name}} & \textbf{Model} & \textbf{Dict. size} & \textbf{L0} & \textbf{MSE} & \textbf{CELR Zero} & \textbf{CELR Mean} \\ \hline
GPT2-768      & gpt2-small          & 768                      & 35.2        & 2.72         & 0.915                     & 0.876                     \\
GPT2-1536     & gpt2-small          & 1536                     & 39.5        & 2.22         & 0.942                     & 0.915                     \\
GPT2-3072     & gpt2-small          & 3072                     & 42.4        & 1.89         & 0.955                     & 0.937                     \\
GPT2-6144     & gpt2-small          & 6144                     & 43.8        & 1.63        & 0.965                     & 0.949                     \\
GPT2-12288    & gpt2-small          & 12288                    & 43.9        & 1.46        & 0.971                     & 0.958                     \\
GPT2-24576    & gpt2-small          & 24576                    & 42.9        & 1.33        & 0.975                     & 0.963                     \\
GPT2-49152    & gpt2-small          & 49152                    & 42.4        & 1.21        & 0.978                     & 0.967                     \\
GPT2-98304    & gpt2-small          & 98304                    & 43.9        & 1.14        & 0.980                     & 0.970                     \\
GemmaScope-16384   & Gemma 2 2B & 16384                     &  43.4        & 1.72        & 0.983                   & 0.980                    \\
GemmaScope-32768  & Gemma 2 2B & 32768                    & 41.5       & 1.59        &  0.985                    & 0.982                    \\ \hline

\end{tabular}
\label{tab:saes}
\end{table}


\subsection{Stitching experiments}
\begin{figure}[H]
    \centering
    \includegraphics[width=0.75\linewidth]{images/auroc.png}
    \caption{Based on the maximum cosine similarity it is possible to predict the direction of the effect of adding a latent to another SAE. The Receiver operating curve (ROC) is created by varying the threshold of maximum cosine similarity to classify a latent as novel latent or reconstruction latent.} 
    \label{fig:auroc}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=1\linewidth]{images/mse_l0_swap_effect.png}
    \caption{Effects on MSE and L0 when swapping reconstruction latents from larger SAEs to smaller ones. Swapping latent structures generally increases the MSE but almost always decreases L0. Outliers are not shown. The percentual effects per swap get smaller for larger models as the effects are distributed over more swaps.}
    \label{fig:mse_l0_swap_effect}
\end{figure}


\begin{figure}[H]
    \centering
    \includegraphics[width=1\linewidth]{images/cumulative_adding_effect2.png}
    \caption{Percentage change of MSE of adding in latents from a larger SAE to a smaller SAE in a random order. Adding in all the latents with cosine similarity $\leq$ 0.7 from GPT-1536 in GPT-768 reduces the MSE by almost 10\%.}
    \label{fig:cumulative-adding}
\end{figure}


\begin{figure}[ht]
    \centering
    % First subfigure
    \begin{subfigure}[b]{0.45\textwidth}
    \centering
    \includegraphics[width=\linewidth]{images/proportion_recon_same_size.png}
    \caption{Proportion of reconstruction latents in GPT-2 SAEs with 3072 latents. X and Y labels are random training seeds.}
    \end{subfigure}
    \hfill % Adds spacing between subfigures
    % Second subfigure
    \begin{subfigure}[b]{0.45\textwidth}
        \centering
    \includegraphics[width=\linewidth]{images/proportion_recon_diff_size.png}
    \caption{Proportion of reconstruction latents in SAEs of different size. X and Y labels are exponential expansion factors, or equivalently row indices in Table \ref{tab:saes}.}
    \end{subfigure}
    \caption{Proportion of reconstruction latents between GPT-2 SAEs of the same size and different sizes.}
    \label{fig:proportionReconRatios}
\end{figure}

We compared the proportion of reconstruction latents between SAEs of the same size and SAEs of different sizes. We found that when comparing SAEs with the same size, 94\% of the latents are reconstruction latents (cosine similarity $>$ 0.7). These results are displayed in Figure \ref{fig:proportionReconRatios}.


A concern when stitching two different SAEs is the choice of $\mathbf{b}^{dec}$. However, in practice we find that the  $\mathbf{b}^{dec}$s of SAEs trained on the same latents are very similar (minimum cosine similarity of 0.9970,
differing by less than 0.1\% in magnitude). Figure \ref{fig:b_dec_switching} shows that the decoder biases can be exchanged in the stitching process with negligible impact on the reconstruction performance of an SAE.
\begin{figure}
    \centering
    \includegraphics[width=0.5\linewidth]{images/b_dec_switching.png}
    \caption{Reconstruction performance (MSE) of SAEs of different sizes when their $\mathbf{b}^{dec}$s are replaced with the $\mathbf{b}^{dec}$s of SAEs with a different dictionary size.}
    \label{fig:b_dec_switching}
\end{figure}

\subsubsection{Stitching in Gemma Scope SAEs}

In order to validate that the SAE stitching results are not an artifact of GPT-2 small or the SAEs that we trained, we applied the same methods to the open-source Gemma Scope SAEs. In particular, we compared two SAEs trained on the residual stream of layer 12 of Gemma 2 2B. The first SAE has dictionary size 16384 (average L0 41) and the second SAE has dictionary size 32768. 

We find a lower threshold for distinguishing novel features from reconstruction features (0.4). Using this threshold, we can also smoothly interpolate between SAEs of different sizes trained on Gemma-2-2B.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.75\linewidth]{images/gemma_cossim.png}
    \caption{Change in MSE when adding each feature from GemmaScope-32k to GemmaScope-16k, plotted against the maximum cosine similarity of that feature to any feature in GemmaScope-16k. Features with cosine similarity less than 0.4 tend to improve MSE, while more redundant features hurt performance. }
    \label{fig:gemma_cossim}
\end{figure}

% \begin{figure}[H]
%     \centering
%     \includegraphics[width=0.75\linewidth]{images/gemma_cumulative.png}
%     \caption{The cumulative effect of adding novel latents and reconstruction latents from Gemma Scope 16k to Gemma Scope 32k.}
%     \label{fig:gemma_cumulative}
% \end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.75\linewidth]{images/gemma_transition.png}
    \caption{By first adding in novel latents from Gemma Scope 32k to Gemma Scope 16k and replacing the remaining latents with their similar latents in the larger SAE, we interpolate between the two SAE siezs.}
    \label{fig:gemma_transition}
\end{figure}


\subsection{MetaSAE Additional Figures}

\begin{figure}[H]
    \centering
    \includegraphics[width=.75\linewidth]{images/metasaewithsaedirections.png}
    \caption{Change in reconstruction performance of a meta-SAE when its decoder directions are replaced with the most similar decoder direction from an SAE with a similar dictionary size.}
    \label{fig:metasaewithsaedirections}
\end{figure}

\subsection{Interpretability Experiments}

In this paper, we demonstrate that larger SAEs may learn more narrow, composed concepts in order to improve sparsity rather than just learning concepts that are missing in smaller SAEs. Here, we provide some experimental results on sparse probing \cite{topksaes} and concept removal benchmarks \citep{tpp}\footnote{\citep{tpp} is a benchmarking suite that is currently being developed, to which the researchers were kind enough to grant us early access to benchmark the models in this paper. This work is scheduled to be officially published in early December, well before the decision, and this section will be updated in the camera-ready with a clear citation to that work. We refer to the upcoming work in order to avoid confusion or claiming any credit for that research. The paper and code can be temporarily accessed for the duration of the review period at \url{https://github.com/anonymous664422}}. 

\subsubsection{Sparse Probing}

Similarly to \citet{topksaes}, we use sparse probes to evaluate the presence of known ground-truth features in our SAEs. If we expect a specific feature to be discovered by an SAE, then a metric for autoencoder quality is simply checking whether these features are present as latents. We do this by training a 1-dimensional logistic probe on the activations of the SAE to predict the presence of the feature. We use the benchmark datasets included in \citep{tpp} in our evaluation. These cover a range of domains, for example predicting the sentiment of Amazon reviews or predicting the language of the text from the SAE activations.

In our experiments we use a probe that uses only a single SAE latent in its prediction. The results of these experiments are visualized in Figure \ref{fig:sparse-probing}. They show that the relationship between the size of the SAE and the evaluation accuracy is complex and dataset dependent.

\begin{figure}[H]
    \centering
    \includegraphics[width=1\linewidth]{images/sparse_probing.png}
    \caption{Sparse probing evaluation accuracy by GPT-2 SAE dictionary size across 8 benchmark datasets, with a sparse probe using the top latent.}
    \label{fig:sparse-probing}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=1\linewidth]{images/sparse_probing_top50.png}
    \caption{Sparse probing evaluation accuracy by GPT-2 SAE dictionary size across 8 benchmark datasets, with a sparse probe using the top 50 latents.}
    \label{fig:sparse-probing-50}
\end{figure}

\subsubsection{Concept Removal}

In the SHIFT method \citep{sparsefeaturecircuits}, a human evaluator debiases a classifier by ablating SAE latents. {\citep{tpp}} operationalizes SHIFT into an evaluation for SAE quality, Targeted Probe Perturbation, by training probes on model activations and measuring the effect of ablating sets of SAE latents on the probe accuracy. Ablating a disentangled set of latents should have an isolated causal effect on one class probe, while leaving other class probes unaffected.

They consider a dataset with $M$ classes. For each class with index $i = 1, \ldots, M$ they select the set $L_i$ of the most relevant SAE latents. To select those latents, binary probes are trained detecting the concept $c$ from model activations. The attribution score of latent with index $l$ on concept $c$ is given by 
\begin{equation}
    I(L, c) = (L_{pos} - L_{neg})(\mathbf{d}_l \cdot \mathbf{P})
\end{equation}

where $L$ denotes the batch of activations of latent $l$, $\mathbf{d}_l$ denotes the SAE decoder vector corresponding to $l$, $\mathbf{P}$ is the weight matrix of the binary probe, $L_{pos}$ is the mean activation of the latent for inputs of the targeted concept, and $L_{neg}$ is the mean activation for inputs unrelated to the concept. The latents with the highest attribution score are those select as most relevant.

For each concept $c_i$, they partition the dataset into samples of the targeted concept and a random mix of all other labels. They define the model with probe corresponding to class $c_j$ with $j = 1,\ldots,M$ as a linear classifier $C_j$. Further, $C_{i,j}$ denotes a classifier for $c_j$ where latents $L_i$ are ablated. Then, they iteratively evaluate the accuracy $A_{i,j}$ of all linear classifiers $C_{i,j}$ on the dataset partitioned for the corresponding class $c_j$. The targeted probe perturbation score

\begin{equation}
S_{\text{TPP}} = \text{mean}_{(i=j)}(A_{i,j}) - \text{mean}_{(i\neq j)}(A_{i,j})
\end{equation}

represents the effectiveness of causally isolating a single probe. Ablating a disentangled set of features should only show a significant accuracy decrease if $i = j$, namely if the latents selected for class $i$ are ablated in the classifier of the same class $i$, and remain constant if $i \neq j$

The results of this evaluation by SAE dictionary size is displayed in Figure \ref{fig:tpp}. Whilst here there is a general downward trend, the accuracy is not monotonically decreasing with SAE size.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.65\linewidth]{images/tpp.png}
    \caption{TPP evaluation accuracy by GPT-2 SAE dictionary size across 2 benchmark datasets, ablating up to 50 latents.}
    \label{fig:tpp}
\end{figure}

\end{document}
