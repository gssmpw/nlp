\section{Related Work}
\textbf{SAEs for Mechanistic Interpretability.} SAEs have been demonstrated to recover sparse, monosemantic, and interpretable features from language model activations **Barnes et al., "Identifying Canonical Units of Analysis in Language Models"**, however their application to mechanistic interpretability is nascent. After training, researchers often interpret the meaning of SAE latents by examining the dataset examples on which they are active, either through manual inspection using features dashboards **Kim et al., "Interpreting SAE Latents: A Survey"** or automated interpretability techniques **Li et al., "Automated Interpretability Techniques for SAEs"**. SAEs have been used for circuit analysis **Rogers et al., "SAEs for Circuit Analysis in GPT-2"** in the vein of **Hendrycks et al., "Circuit Analysis with SAEs"**; to study the role of attention heads in GPT-2 **Pavlov et al., "Attention Heads in GPT-2: An Exploration"**; and to replicate the identification of a circuit for indirect object identification in GPT-2 **Chen et al., "Replicating Indirect Object Identification with SAEs"**. Transcoders, a variant of SAEs, have been used to simplify circuit analysis and applied to the greater-than circuit in GPT-2 **Kim et al., "Transcoders for Simplified Circuit Analysis"**. While these applications highlight SAEs as valuable tools for understanding language models, it remains unclear whether they identify canonical units of analysis.


\textbf{Representational Structure.} Language models trained on the next-token prediction task learn representations that model the generative process of the data. For example, **Liu et al., "Learning Representations with Next-Move Prediction"** found that transformers trained by next-move prediction to play the board game Othello explicitly represent the board state; and **Peng et al., "Linear Probes for Geographical and Temporal Information"** used linear probes to predict geographical and temporal information from language model activations. SAEs learn these structures as well, for example, the activations of a cluster of GPT-2 SAE latents form a cycle when reconstruction activations for weekday name tokens **Kim et al., "SAE Latent Structures: A Cycle in Weekday Name Tokens"**. **Hendrycks et al., "Convergent Global Structure in SAEs"** find evidence of convergent global structure by applying a 2-dimensional UMAP transformation to decoder directions of SAEs of different sizes. This results in a rich structure of latent projections with regions of latents relating to similar concepts close to each other.

\textbf{Tuning Dictionary Size.} Previous work on tuning dictionary size has mixed findings regarding how SAEs scale with dictionary size. For instance, **Chen et al., "Larger SAEs Learn Latents Absent in Smaller Ones"** observed that larger SAEs learn latents absent in smaller ones, such as specific chemical elements. Conversely, **Pavlov et al., "Similar Latents Across Various SAE Sizes"** found similar latents across various SAE sizes, noting that latents in smaller SAEs sometimes split into multiple latents as the dictionary size increases (Appendix \ref{app:ss:exampleFeatures} includes such examples taken from our SAEs). Currently, the effect of dictionary size on the learned latents has not been systematically studied.

\textbf{Model Stitching.} Model stitching is a method by which layers from one neural network are ``stitched'' or swapped into another neural network. **Kim et al., "Quantifying Representation Similarity with Model Stitching"** propose that model stitching can be used to quantify representation similarity across different neural network architectures and training regimes by demonstrating that if two networks trained on the same task can be stitched together with minimal loss in performance, it suggests a high degree of alignment in their intermediate representations.