\section{Introduction}

\begin{figure}[t]
	\centering
	\includegraphics[width=1\linewidth]{figure/illustration.pdf}
	% \vspace{-0.5cm}
	\caption{Illustration of (a) context-dependent and (b) reference-dependent evidence from BearFact dataset.}
	\label{fig:illustration}
	% \vspace{-0.5cm}
\end{figure}

The proliferation of misinformation has posed growing challenge in the realm of information reliability. There is a need to develop automated fact-checking methods \cite{survey} to verify the truthfulness of real-world claims using evidence.

Existing fact-checking models \cite{gear,kgat} have shown promise in aggregating and reasoning over multiple evidence sentences to verify a claim. However, the evidence sentences retrieved from a large corpus may contain incomplete information when they are taken out-of-corpus. We need to refer to additional contexts and references from elsewhere to understand coreferential expressions, acronyms, and the scope of a reported finding. For example, Fig. \ref{fig:illustration}(a) illustrates context-dependent evidence, where undefined acronym ``MNC'' in evidence sentence from a paper abstract requires additional context from the abstract to jointly interpret the meaning of acronym ``MNC''. Fig. \ref{fig:illustration}(b) presents reference-dependent evidence, where we need to check the cited paper to understand that ``SARS-CoV-2 infection'' and ``COVID-19 infection'' are coreferential expressions, so that we could accurately fact-check the claim. Such scenario also exists in general domain where evidence sentences from a Wikipedia page may need contextual sentences in the same page and text in the hyperlinked pages to complement the insufficient information in the evidence.

\textbf{Challenges and Approach.} To overcome the limitations of existing methods, we propose \ul{\textbf{Co}}ntext- and \ul{\textbf{R}}eference-augmented \ul{\textbf{Re}}asoning and prompting for fa\ul{\textbf{ct}}-checking (CORRECT), to address two open questions.

First, \emph{how to aggregate both contextual and referential documents into evidence reasoning?} Some models are proposed to capture contextual documents, e.g., MultiVerS \cite{multivers}. Some others are designed for referential documents, e.g., Transformer-XH \cite{transformer_xh} and HESM \cite{hesm}. However, they incorporate either contextual or referential documents, failing to aggregate both of them into unified evidence embedding. Moreover, most of them simply concatenate evidence with contextual or referential documents, and inefficiently input the long text to language models for evidence encoding. Though they have shown that modeling either contexts or references helps fact-checking, integrating both of them for evidence reasoning is still unexplored. In our model, we construct a three-layer graph with evidence, context, and reference layers. We design intra- and cross-layer reasoning to aggregate three graph layers into unified evidence embedding.

Second, \emph{how to integrate evidence reasoning and claim for accurate verdict prediction?} Previous fact-checking methods, e.g., ProToCo \cite{protoco}, rely on natural language as input prompt to language model for claim verification. However, discrete natural language prompts are difficult to design and may result in suboptimal results \cite{coop}. Recently, prompt tuning \cite{soft_prompting} uses continuous and learnable prompt embeddings to replace discrete prompt and has achieved decent result, but no one has explored its design for claim verification. We propose evidence-conditioned prompt encoder, which takes evidence embedding as input, and produces unique prompt embeddings for each claim. We combine prompt embeddings with claim token embeddings to unify evidence and claim for verdict prediction.

\textbf{Contributions.} First, we propose a novel model, Context- and Reference-augmented Reasoning and Prompting (CORRECT), to integrate both contextual and referential documents into evidence reasoning. Second, we design a three-layer evidence graph, and propose intra- and cross-layer reasoning to learn unified evidence embedding. Third, we propose evidence-conditioned prompt embeddings, which are combined with claims to integrate evidence reasoning with claim for fact-checking.