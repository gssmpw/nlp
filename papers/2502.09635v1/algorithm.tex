\section{Pseudo-code of Training Process}
\label{sec:algorithm}

We summarize the training process at Algo. \ref{algo:training_algorithm}.

\begin{algorithm}[h]
	\caption{Training Process of CORRECT}
	\label{algo:training_algorithm}
	\begin{flushleft}
		\hspace*{\algorithmicindent}\textbf{Input}: A fact-checking dataset $ \mathcal{D} $ with claims $ \mathcal{X} $, evidence sentences $ \mathcal{E} $, contextual documents $ \mathcal{C} $, and referential documents $ \mathcal{R} $. Number of prompt embeddings $ M $ and temperature $ \tau $. \\
		\hspace*{\algorithmicindent}\textbf{Output}: Predicted labels $ \widehat{\mathcal{Y}}_{\text{test}} $ for test claims.
	\end{flushleft} 
	\begin{algorithmic}[1] % The number tells where the line numbering should start
		\State Initialize model with pre-trained parameters in biomedical domain or general domain.
		%\nextnr
		\While{not converged}
        \State Construct three-layer evidence graph for each claim $ x\in\mathcal{X} $.
            \For{evidence sentence $ e\in\mathcal{N}_{\text{evid}}(x) $}
            \State Initialization $ \textbf{H}_e^{(l=1)}=\text{TRM}(\textbf{H}_e^{(l=0)}) $.
                \For{$ l=1,2,...,L-1 $}
                \Statex \qquad\qquad\textit{ // Evidence graph reasoning}
                \State Intra-layer reasoning by Eqs. \ref{eq:linear_projection}--\ref{eq:graph_conv_layer}.
                \State Cross-layer reasoning by Eq. \ref{eq:graph_conv_layer_ref}.
                \Statex \qquad\qquad\textit{ // Asymmetric MHA step}
                \State Virtual token concatenation Eq. \ref{eq:virtual_token}.
                \State $ \textbf{H}_e^{(l+1)}=\text{TRM}^{\text{asy}}(\widehat{\textbf{H}}_e^{(l)}) $ by Eq. \ref{eq:asymmetric_attention}.
                \EndFor
            \EndFor
        \State Obtain an evidence embedding by Eq. \ref{eq:evidence_embedding}.
        \Statex \quad\textit{ // Evidence-conditioned prompting}
        \State Initialize $ |\mathcal{Y}| $ sets of base prompt embeddings $ \{\textbf{h}_{m,y}\}_{m=1}^M $ where $ y\in\mathcal{Y} $.
        \State Input evidence embedding $ \textbf{h}_E $ to evidence-conditioned prompt encoder and obtain $ |\mathcal{Y}| $ sets of prompt embeddings $ \{\bm{\pi}_{m,y}\}_{m=1}^M $ where $ y\in\mathcal{Y} $ by Eqs. \ref{eq:scaling_and_shifting}--\ref{eq:scaling_and_shifting2}.
        \State Input $ \{\textbf{P}_{x,y}\}_{y\in\mathcal{Y}} $ to claim encoder and obtain $ |\mathcal{Y}| $ claim embeddings $ \{\textbf{h}_{x,y}\}_{y\in\mathcal{Y}} $.
        \State Minimize loss function $ \mathcal{L} $ in Eq. \ref{eq:objective_function}.
    \EndWhile
    % %\EndProcedure
	\end{algorithmic}
\end{algorithm}
