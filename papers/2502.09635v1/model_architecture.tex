\section{Model Architecture}

\begin{table}[t]
	\centering
	\caption{Summary of mathematical notations.}
	%\vspace{-0.3cm}
	\resizebox{\columnwidth}{!}{
		\begin{tabular}{c|l}
			\toprule
			Notation  & Description  \\
			\hline
			$ \mathcal{D} $ & a fact-checking dataset \\
			$ \mathcal{X} $ & a set of $ N=|\mathcal{X}| $ claims \\
			$ \mathcal{E} $ & a corpus of evidence sentences \\
			$ \mathcal{C} $ & a set of contextual documents \\
                $ \mathcal{R} $ & a set of referential documents \\
			$ \mathcal{N}_{\text{ref}}(e) $ & evidence sentence $ e $'s referential documents \\
                $ \mathcal{N}_{\text{evid}}(x) $ & claim $ x $'s retrieved evidence sentences \\
                $ \mathcal{Y} $ & a set of labels \\
                $ \textbf{h}_{e,\text{CLS}}^{(l)} $ & evidence sentence $ e $'s [CLS] token embedding \\
                $ \hat{\textbf{h}}_{c}^{(l)} $ & aggregated contextual document embedding \\
                $ \hat{\textbf{h}}_{r}^{(l)} $ & aggregated referential document embedding \\
                $ \widehat{\textbf{H}}_{e}^{(l)} $ & evidence sentence $ e $'s augmented embedding matrix \\
                $ \bm{\pi}_{m,y} $ & the $ m $-th prompt embedding for class $ y $ \\
                $ \textbf{h}_{m,y} $ & the $ m $-th base prompt embedding for class $ y $ \\
			\bottomrule
		\end{tabular}
	}
	%\vspace{-0.5cm}
	\label{table:notation_table}
\end{table}

\begin{figure*}[t]
	\centering
	\includegraphics[width=1\linewidth]{figure/model.pdf}
	% \vspace{-0.6cm}
	\caption{Model architecture. (a) A three-layer graph for a claim. (b) Intra- and cross-layer reasoning. (c) A nested architecture with language model and graph reasoning for evidence encoding. (d) Evidence-conditioned prompting.}
	\label{fig:model}
	% \vspace{-0.3cm}
\end{figure*}

We introduce \ul{\textbf{Co}}ntext- and \ul{\textbf{R}}eference-augmented \ul{\textbf{Re}}asoning and prompting for fa\ul{\textbf{ct}}-checking (CORRECT). Table \ref{table:notation_table} summarizes math notations.

\subsection{Problem Formulation}

We are given a fact-checking dataset $ \mathcal{D}=\{\mathcal{X},\mathcal{E},\mathcal{C},\mathcal{R}\} $. Claim set $ \mathcal{X}=\{x_i\}_{i=1}^N $ contains a set of $ N $ claims. Evidence set $ \mathcal{E}=\{e_j\}_{j=1}^E $ is a corpus of $ E $ evidence sentences. For each evidence sentence $ e\in\mathcal{E} $, we have its contextual document $ c\in\mathcal{C} $. Usually, an evidence sentence has only one contextual document, from which this sentence is retrieved. We also have $ e $'s referential documents $ \mathcal{N}_{\text{ref}}(e)=\{r_{e,n}\}_{n=1}^{R_e}\subset\mathcal{R} $. Here $ R_e $ is the number of $ e $'s referential documents. Evidence sentence $ e $ may have multiple referential documents, such as papers cited by $ e $'s paper or Webpages hyperlinked by $ e $. We use $ \mathcal{N}_{\text{ref}}(e) $ to represent the set of $ e $'s referential documents. We use $ \mathcal{N}_{\text{evid}}(x)\subset\mathcal{E} $ to denote the set of evidence sentences for a claim $ x $.

Given $ \mathcal{D} $ as input, we design a model that uses evidence sentences from $ \mathcal{E} $ together with their contextual documents in $ \mathcal{C} $ and referential documents in $ \mathcal{R} $ to verify claims. Eventually, for each claim $ x\in\mathcal{X} $, we output its predicted label $ \hat{y}\in\mathcal{Y}=\{\text{SUPPORT, REFUTE, NEI}\} $, indicating whether the evidence supports, refutes, or does not have enough information to verify the claim.

As shown in Fig. \ref{fig:model}, CORRECT has two modules: (a-c) context- and reference-augmented evidence reasoning on three-layer graph, (d) evidence-conditioned prompting for claim verification.

\subsection{Three-layer Evidence Graph Reasoning}

\textbf{Graph construction.} For each claim $ x\in\mathcal{X}$ and its evidence sentences $ \mathcal{N}_{\text{evid}}(x)\subset\mathcal{E} $, we construct a three-layer graph with evidence, context, and reference layers in Fig. \ref{fig:model}(a). We consider evidence sentences, contextual documents, and referential documents as three types of vertices. Each type of vertices reside on their own layer. Cross-layer links between evidence layer and context layer connect each evidence sentence with its contextual document. Each evidence sentence and its referential documents are connected by cross-layer referential links. Green links in Fig. \ref{fig:model}(a) are cross-layer links. For multi-evidence reasoning, we add intra-layer links on evidence layer where evidence sentences of a claim are fully connected, shown by black links in Fig. \ref{fig:model}(a). The purpose of constructing three layers instead of mixing all vertices into one layer is to better differentiate three types of vertices.

\textbf{Intra-layer reasoning.} Evidence reasoning includes intra- and cross-layer reasoning. We first show intra reasoning (orange arrows in Fig. \ref{fig:model}(b)).

For each evidence sentence $ e\in\mathcal{N}_{\text{evid}}(x) $, we let $ \textbf{H}_e^{(l)}=[\textbf{h}_{e,\text{CLS}}^{(l)},\textbf{h}_{e,1}^{(l)},\textbf{h}_{e,2}^{(l)},...] $ denote the output from the $ l $-th Transformer step. Note that previous works call it the $ l $-th layer, but to distinguish it from our three-layer graph, we instead call it the $ l $-th step. $ \textbf{h}_{e,i}^{(l)}\in\Bbb R^d $ is $ d $-dimensional token embedding. We use graph neural network to aggregate different evidence sentences of a claim. For each evidence sentence $ e $, we first project it by
\begin{equation}
\label{eq:linear_projection}
    \tilde{\textbf{h}}_{e,\text{CLS}}^{(l)}=\textbf{W}_1\textbf{h}_{e,\text{CLS}}^{(l)}.
\end{equation}
The [CLS] token is taken as the evidence sentence embedding, and $ \textbf{W}_1\in\Bbb R^{d\times d} $ is type-specific parameter. We design type-specific attention.
\begin{equation}
\label{eq:neighbor_attention}
\resizebox{\columnwidth}{!}{
	$ a_{e,e^\prime}=\text{softmax}\Bigl(\text{LeakyReLU}(\textbf{b}_1^{\top}[\tilde{\textbf{h}}_{e,\text{CLS}}^{(l)}||\tilde{\textbf{h}}_{e^\prime,\text{CLS}}^{(l)}])\Bigl). $
 }
\end{equation}
$ e^\prime\in\mathcal{N}_{\text{evid}}(x)\textbackslash e $ is another evidence sentence for the same claim $ x $, $ [\cdot||\cdot] $ is concatenation, and $ \textbf{b}_1\in\Bbb R^{2d} $ is learnable parameter. Finally, we aggregate evidence sentences to $ e $ by mean pooling.
\begin{equation}
\label{eq:neighbor_aggregation}
    \hat{\textbf{h}}_e^{(l)}=\text{mean}\Bigl(\tilde{\textbf{h}}_{e,\text{CLS}}^{(l)},\sum_{e^\prime \in \mathcal{N}_{\text{evid}}(x)\textbackslash e}a_{e,e^\prime}\tilde{\textbf{h}}_{e^\prime,\text{CLS}}^{(l)}\Bigl).
\end{equation}
The aggregated sentence embedding $ \hat{\textbf{h}}_e^{(l)} $ captures information of both itself and other evidence sentences. To summarize Eqs. \ref{eq:linear_projection}--\ref{eq:neighbor_aggregation}, we have
\begin{equation}
\label{eq:graph_conv_layer}
\resizebox{\columnwidth}{!}{
    $ \hat{\textbf{h}}_e^{(l)}=f_{\text{GNN}}\Big(\textbf{h}_{e,\text{CLS}}^{(l)},\{\textbf{h}_{e^\prime,\text{CLS}}^{(l)}|e^\prime\in\mathcal{N}_{\text{evid}}(x)\textbackslash e\};\textbf{W}_1,\textbf{b}_1\Big). $
}
\end{equation}

To integrate intra-layer aggregation into the encoding of each evidence sentence, we introduce a \emph{virtual token} to represent the aggregated sentence embedding $ \hat{\textbf{h}}_e^{(l)} $. %inside each Transformer step. 
For evidence sentence $ e $, we concatenate $ \hat{\textbf{h}}_e^{(l)} $ with $ e $'s text token embeddings by $ \widehat{\textbf{H}}_e^{(l)}=\hat{\textbf{h}}_e^{(l)}||\textbf{H}_e^{(l)} $.
After concatenation, $ \widehat{\textbf{H}}_e^{(l)} $ contains information of both evidence sentence $ e $'s text and the aggregated embedding from $ e $'s intra-layer neighbors. We aim to propagate the aggregated sentence embedding to other text tokens of sentence $ e $, so that the text tokens can fully unify other sentences for multi-evidence reasoning. We will introduce \emph{asymmetric} multi-head self-attention to achieve this goal. But before that, we first discuss cross-layer reasoning.

\textbf{Cross-layer reasoning.} We present cross-layer reasoning, which aggregates contextual and referential documents into evidence sentences (green arrows in Fig. \ref{fig:model}(b)). The aggregation from referential documents to evidence sentence is similarly defined by Eq. \ref{eq:graph_conv_layer_ref}. We use reference-specific parameters, $ \textbf{W}_2 $ and $ \textbf{b}_2 $, to preserve graph heterogeneity.
\begin{equation}
\label{eq:graph_conv_layer_ref}
\resizebox{\columnwidth}{!}{
    $ \hat{\textbf{h}}_r^{(l)}=f_{\text{GNN}}\Big(\textbf{h}_{e,\text{CLS}}^{(l)},\{\textbf{h}_{r,\text{CLS}}^{(l)}|r\in\mathcal{N}_{\text{ref}}(e)\};\textbf{W}_2,\textbf{b}_2\Big). $
}
\end{equation}
Each referential document $ r\in\mathcal{N}_{\text{ref}}(e) $ is also encoded, and its [CLS] token is passed to Eq. \ref{eq:graph_conv_layer_ref} for aggregation. Similarly, we have $ \hat{\textbf{h}}_c^{(l)} $ as contextual document embedding. To integrate both embeddings into evidence sentence for cross-layer reasoning, we introduce two more \emph{virtual tokens}.
\begin{equation}
\label{eq:virtual_token}
    \widehat{\textbf{H}}_e^{(l)}=\hat{\textbf{h}}_c^{(l)}||\hat{\textbf{h}}_r^{(l)}||\hat{\textbf{h}}_e^{(l)}||\textbf{H}_e^{(l)}.
\end{equation}
The augmented embedding matrix, i.e., $ \widehat{\textbf{H}}_e^{(l)} $, contains both intra-evidence reasoning as well as cross-layer context and reference augmentation.

To fully unify all three graph layers into evidence sentence $ e $, we input $ \widehat{\textbf{H}}_e^{(l)} $ at Eq. \ref{eq:virtual_token} to the $ (l+1) $-th Transformer step with our proposed \emph{asymmetric} multi-head self-attention ($ \text{MSA}^{\text{asy}} $).
\begin{equation}
\label{eq:asymmetric_attention}
\resizebox{\columnwidth}{!}{
$ \begin{split}
    \text{MSA}^{\text{asy}}(\textbf{H}_e^{(l)},\widehat{\textbf{H}}_e^{(l)},&\widehat{\textbf{H}}_e^{(l)})=\text{softmax}\Big(\dfrac{\textbf{Q}\textbf{K}^\top}{\sqrt{d}}\Big)\textbf{V},\\
    \textbf{Q}=\textbf{H}_e^{(l)}\textbf{W}_{Q}^{(l)},\;\  \textbf{K}&=\widehat{\textbf{H}}_e^{(l)}\textbf{W}_{K}^{(l)},\;\  \textbf{V}=\widehat{\textbf{H}}_e^{(l)}\textbf{W}_{V}^{(l)}.
\end{split} $
}
\end{equation}
Keys $ \textbf{K} $ and values $ \textbf{V} $ are augmented with virtual tokens, but queries $ \textbf{Q} $ are not, to avoid context and reference embeddings being overwritten by evidence sentence embedding. The result of asymmetric MSA is passed to a multi-layer perceptron and layer normalization \cite{transformer}. Finally, we obtain the output from the $ (l+1) $-th step, $ \textbf{H}_e^{(l+1)} $, integrating evidence sentence $ e $, other evidence sentences of the same claim, $ e $'s contextual and referential documents, see Fig. \ref{fig:model}(c).

We conduct such intra-layer and cross-layer reasoning inside each Transformer step to allow different graph layers to fully communicate with each other. We repeat such nested and graph-augmented encoding for $ L $ times, and obtain $ \textbf{h}_e=\textbf{h}_{e,\text{CLS}}^{(L)} $ as the graph-augmented embedding for evidence sentence $ e $. This nested architecture is shown by Fig. \ref{fig:model}(c). For claim $ x $, we have $ \{\textbf{h}_e\}_{e\in\mathcal{N}_{\text{evid}}(x)} $, a set of graph-augmented embeddings for its evidence sentences. Finally, we aggregate them by mean pooling and obtain a single evidence embedding.
\begin{equation}
\label{eq:evidence_embedding}
    \textbf{h}_E=\text{mean}(\textbf{h}_e|e\in\mathcal{N}_{\text{evid}}(x)).
\end{equation}

\subsection{Evidence-conditioned Prompting}

Now we integrate evidence reasoning into claim embedding to fully integrate their information for fact-checking. Prompting \cite{p_tuning} is a powerful method in fact-checking \cite{protoco}. However, existing models are mainly based on natural language as input prompt to language models for verdict prediction. Handcrafted discrete prompt has two disadvantages: First, it is difficult to manually design a prompt that provides a decent performance. Previous works \cite{coop} have shown that the change of a single word in the prompt may lead to significant deterioration of the results, and it is time-consuming to enumerate every prompt. Second, discrete natural language prompt is difficult to optimize, since language models are intrinsically continuous.

To mitigate these problems, we explore learnable and continuous prompt embedding. Below we design a prompt encoder, which takes evidence embedding $ \textbf{h}_E $ as input, and produces evidence-conditioned prompt embeddings. See Fig. \ref{fig:model}(d).

\textbf{Evidence-conditioned prompt encoder.} We consider below continuous embeddings as prompt.
\begin{equation}
\label{eq:input_embeddings_with_prompt}
    \textbf{P}_x=[\textbf{h}_{x,\text{CLS}},\bm{\pi}_1,\bm{\pi}_2,...,\bm{\pi}_M,\textbf{h}_{x,1},\textbf{h}_{x,2},...].
\end{equation}
Here $ \{\bm{\pi}_m\}_{m=1}^M $ where $ \bm{\pi}_m\in\Bbb R^d $ is a set of $ M $ learnable evidence-conditioned prompt embeddings to be explained shortly, and $ M $ is a hyperparameter, indicating the number of prompt embeddings. Each $ \textbf{h}_{x,i}\in\Bbb R^d $ is a $ d $-dimensional embedding of the $ i $-th text token in claim $ x $. In language models, there is an embedding look-up table before language model encoder. In this look-up table, input text tokens are first mapped to the vocabulary to obtain their token embeddings, which are then summed up with positional encodings. $ \textbf{h}_{x,i} $ in Eq. \ref{eq:input_embeddings_with_prompt} is obtained by this look-up table.

Now we explain prompt embeddings $ \{\bm{\pi}_m\}_{m=1}^M $, output from an evidence-conditioned prompt encoder. We first initialize $ M $ \emph{base} prompt embeddings, $ \{\textbf{h}_m\}_{m=1}^M $. We then project evidence embedding $ \textbf{h}_E $ in Eq. \ref{eq:evidence_embedding} to the prompt embedding space, followed by element-wise product and summation.
\begin{equation}
\label{eq:scaling_and_shifting}
\resizebox{\columnwidth}{!}{
    $ \bm{\alpha}_x=\text{tanh}\Big(\dfrac{\textbf{W}_\alpha\textbf{h}_E+\textbf{b}_\alpha}{\tau}\Big),\quad\bm{\beta}_x=\text{tanh}\Big(\dfrac{\textbf{W}_\beta\textbf{h}_E+\textbf{b}_\beta}{\tau}\Big), $
}
\end{equation}
\begin{equation}
\label{eq:scaling_and_shifting2}
    \bm{\pi}_m=\textbf{h}_m\odot(\bm{\alpha}_x+\textbf{1})+\bm{\beta}_x.
\end{equation}
$ \odot $ is element-wise product, and $ \textbf{1}\in\Bbb R^d $ is a vector of ones to ensure that the scaling of $ \textbf{h}_m $ is centered around one. $ \tau $ is a temperature to scale the shape of tanh function. $ \bm{\pi}_m $ is thus conditioned on evidence embedding, and different claims with their own evidence sentences should have their unique claim-specific prompt embeddings, shown by Fig. \ref{fig:model}(d).

Given the label set  $ \mathcal{Y}= $\{SUPPORT, REFUTE, NEI\}, which usually has three types of labels, we apply above evidence-conditioned prompt encoder and correspondingly obtain three sets of prompt embeddings, $ \{\bm{\pi}_{m,y}\}_{m=1}^M $ where $ y\in\mathcal{Y} $. As in Eq. \ref{eq:input_embeddings_with_prompt}, we concatenate each set of prompt embeddings with token embeddings of claim $ x $, and obtain three sets of inputs $ \{\textbf{P}_{x,y}\}_{y\in\mathcal{Y}} $ to claim encoder.
\begin{equation}
\resizebox{\columnwidth}{!}{
$ \begin{split}
    \textbf{H}_{x,y}^{(L)}&=f(\textbf{P}_{x,y})\\
    &=f([\textbf{h}_{x,\text{CLS}},\bm{\pi}_{1,y},\bm{\pi}_{2,y},...,\bm{\pi}_{M,y},\textbf{h}_{x,1},\textbf{h}_{x,2},...])
\end{split} $
}
\end{equation}
$ \textbf{H}_{x,y}^{(L)} $ is the output from the claim encoder, and its [CLS] token is taken as claim embedding $ \textbf{h}_{x,y}=\textbf{h}_{x,y,\text{CLS}}^{(L)} $. Claim encoder shares parameters with evidence encoder. Due to contextualized modeling, claim token embeddings and evidence-conditioned prompt embeddings fully exchange information, and the output claim embedding captures both claim $ x $ and evidence reasoning for fact-checking.

Finally, we use contrastive loss function to predict the veracity of claim $ x $ by
\begin{equation}
\label{eq:objective_function}
\resizebox{\columnwidth}{!}{
    $ \mathcal{L}=-\sum_{x\in\mathcal{X}_{\text{train}}}\log\dfrac{\exp(\textbf{h}_{x,y}^\top\textbf{h}_E)}{\exp(\textbf{h}_{x,y}^\top\textbf{h}_E)+\sum_{y^\prime\in\mathcal{Y}\textbackslash y}\exp(\textbf{h}_{x,y^\prime}^\top\textbf{h}_E)}. $
}
\end{equation}
$ \textbf{h}_E $ is evidence embedding of claim $ x $ obtained by Eq. \ref{eq:evidence_embedding}. $ \mathcal{X}_{\text{train}} $ is a set of training claims. Though we use three types of labels in $ \mathcal{Y} $, more types of labels in $ \mathcal{Y} $ can also be modeled. Algorithm \ref{algo:training_algorithm} summarizes the learning process.

\textbf{Initialization of base prompt embeddings.} Previous works \cite{coop} have shown the importance of the initialization of \emph{base} prompt embeddings $ \{\textbf{h}_{m,y}\}_{m=1}^M $ where $ y\in\mathcal{Y} $. Some of them randomly initialize the embeddings, while others use word embeddings of discrete prompts. Random initialization presents unstable optimization \cite{g2p2}, while it is difficult to choose the right discrete prompts for initialization. We solve these problems by using the three-layer graph.

For a claim $ x $, the vertices on its three-layer graph consistently carry the signal of claim $ x $'s veracity due to semantic relatedness. Thus, for each label in the label set $ y\in\mathcal{Y} $, we have training claims belonging to this label $ \mathcal{X}_{\text{train},y}=\{x\in\mathcal{X}_{\text{train}}|y_x=y\} $. For each of these claims, we truncate its evidence sentences, contextual and referential documents to $ M $ words, and obtain their $ M $ word embeddings in the look-up table of language model. We then take mean pooling for evidence sentences, contextual and referential documents, and obtain $ M $ pooled word embeddings for each claim. Finally, we average all training claims belonging to the same label $ \mathcal{X}_{\text{train},y} $, and obtain $ M $ word embeddings, which are used to initialize $ M $ base prompt embeddings $ \{\textbf{h}_{m,y}\}_{m=1}^M $. They are derived from training claims of the same label, thus provide a more informative starting point than random initialization for verdict prediction. We repeat this process for every label $ y\in\mathcal{Y} $, and obtain initialization for each set of base prompt embeddings.