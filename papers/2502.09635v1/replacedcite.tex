\section{Related Work}
\textbf{Multi-hop fact-checking.} Complex claims usually require reasoning over multiple evidence sentences. Many methods are based on Language Models ____ and Graph Neural Networks ____, such as GEAR ____, KGAT ____, DREAM ____, SaGP ____, DECKER ____, CausalWalk ____, etc. However, they mainly focus on the reasoning within evidence sentences. They ignore the auxiliary contextual and referential documents. Methods incorporating contextual documents are proposed, e.g., ParagraphJoint ____, ARSJoint ____, MultiVerS ____, etc. Some others integrating referential documents include Transformer-XH ____ and HESM ____. However, they incorporate either contextual or referential documents, but not both. In contrast, we construct a three-layer evidence graph to model evidence sentences, contexts, and references. There are fake news detection models where auxiliary graph with Wikidata is used ____. Fake news detection aims to detect the whole article with meta-data, while fact-checking focuses on claim sentences with retrieved evidence.

Some fact-checking works are based on retrieval-augmented generation ____. They unify evidence retrieval and claim verification as a joint approach, while our model mainly focuses on verification, and relies on external tool for evidence retrieval. Our setting is consistent with existing works ____.

\textbf{Prompt-based fact-checking.} Some models verify claims by prompting LLMs ____. ProToCo ____ inputs both evidence sentences and claim to T5 ____. ProgramFC ____ decomposes complex claims into simpler sub-tasks and uses natural language to prompt LLMs. Varifocal ____ formulates fact-checking as question generation and answering. They rely on handcrafted natural language as prompt. The performance heavily relies on the choice of prompt, and it is difficult to design a prompt that produces a decent result, as shown in ____. Our model is designed with learnable prompt embeddings where the prompting instruction is naturally learned by embeddings through optimization.

\textbf{Prompt learning.} Prompting ____ uses natural language as the input to language models to fulfill certain tasks. Many prompting models have been proposed, including natural language prompt ____ and prompt embeddings ____. Prompting also benefits many tasks ____. However, no one has explored prompt embeddings for fact-checking.

\textbf{Text-attributed graph.} Texts are usually connected in a graph structure, termed text-attributed graph ____. Various methods have been developed to learn text embeddings in an unsupervised manner ____. Though both our model and these works construct a text-attributed graph, our work is different from them, since our model is a supervised model for fact-checking.