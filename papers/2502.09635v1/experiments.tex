\section{Experiments}

We conduct extensive experiments and ablation analysis to evaluate the effectiveness of the proposed model CORRECT.

\textbf{Datasets.} We use 4 datasets in Table \ref{table:dataset_statistics}. FEVEROUS \cite{feverous} is a general-domain dataset. Each claim is annotated in the form of sentences and/or cells from tables in Wikipedia pages. Since we focus on textual fact-checking, we follow \cite{programfc} and select claims that only require sentences as evidence. We call this subset \textbf{FEVEROUS-S}. \textbf{BearFact} \cite{bear_fact} is a biomedical dataset with sentences from papers as evidence. Its original dataset does not have evidence for claims in NEI class. We follow \cite{protoco} and select sentences that have the highest \textit{tf-idf} similarity with those claims as evidence. \textbf{Check-COVID} \cite{check_covid} contains claims about COVID-19. \textbf{SciFact} \cite{scifact} is a dataset with sentences in papers as evidence. As in its original paper, for claims in NEI class, we choose sentences from the cited abstract with top-3 highest \textit{tf-idf} similarity with the claim as evidence. Appendix \ref{sec:dataset_preprocessing} contains data preprocessing details.

\begin{table}
	\centering
	\caption{Dataset statistics.}
	% \vspace{-0.2cm}
	\resizebox{\columnwidth}{!}{
		\begin{tabular}{c|cc|c|c}
			\toprule
			\multirow{2}{*}{Name} & \multicolumn{2}{c|}{\#Claims} & \#Contextual & \#Referential \\
			\cline{2-3}
			{} & Train & Test & Documents & Documents \\
			\hline
                FEVEROUS-S & 23,912 & 5,978 & 19,546 & 21,579 \\
			BearFact & 1,158 & 290 & 1,166 & 12,938 \\ 
			Check-COVID & 1,275 & 229 & 347 & 3,132 \\
			SciFact & 809 & 300 & 1,189 & 9,617 \\
			\bottomrule
		\end{tabular}
	}
	% \vspace{-0.3cm}
	\label{table:dataset_statistics}
\end{table}

\begin{table*}[t]
	\centering
	\caption{Verdict prediction results on \emph{fully supervised} setting with \emph{Macro F1} score. Results are in percentage.}
	\vspace{-0.2cm}
	\resizebox{\textwidth}{!}{
		\begin{tabular}{c|cc|cc|cc|cc}
			\toprule
                \multirow{2}{*}{Model} & \multicolumn{2}{c|}{BearFact} & \multicolumn{2}{c|}{Check-COVID} & \multicolumn{2}{c|}{SciFact} & \multicolumn{2}{c}{FEVEROUS-S} \\
			\cline{2-9}
			{} & Gold & Retrieved & Gold & Retrieved & Gold & Retrieved & Gold & Retrieved \\
			\hline
                KGAT & 53.11$ \pm $2.25 & 36.55$ \pm $1.95 & 71.97$ \pm $1.31 & 75.83$ \pm $0.74 & 70.23$ \pm $1.08 & 59.83$ \pm $0.68 & 86.10$ \pm $0.32 & 67.76$ \pm $0.93 \\
                HESM & 44.90$ \pm $2.20 & 42.93$ \pm $0.27 & 62.85$ \pm $0.59 & 71.58$ \pm $1.98 & 68.66$ \pm $0.69 & 50.91$ \pm $2.57 & 83.12$ \pm $0.80 & 67.43$ \pm $0.81 \\
                Transformer-XH & 45.28$ \pm $1.08 & 38.39$ \pm $0.80 & 67.81$ \pm $0.93 & 76.51$ \pm $2.09 & 72.01$ \pm $0.86 & 56.26$ \pm $0.64 & 85.44$ \pm $0.75 & 68.13$ \pm $0.52 \\
                Transformer-XH++ & 46.81$ \pm $1.52 & 41.06$ \pm $1.70 & 70.52$ \pm $0.55 & 78.49$ \pm $0.52 & 73.92$ \pm $0.58 & 57.82$ \pm $2.29 & 85.35$ \pm $0.45 & 69.76$ \pm $0.73 \\
                MultiVerS & 51.56$ \pm $1.30 & 38.71$ \pm $1.96 & 66.32$ \pm $1.27 & 70.01$ \pm $2.23 & 81.33$ \pm $1.63 & \textbf{62.30}$ \pm $\textbf{0.98} & 78.14$ \pm $1.31 & 65.29$ \pm $0.36 \\
                CausalWalk & 45.52$ \pm $1.99 & 34.15$ \pm $0.97 & 71.49$ \pm $1.65 & 71.55$ \pm $2.46 & 71.27$ \pm $2.48 & 57.05$ \pm $0.62 & 80.65$ \pm $0.10 & 71.22$ \pm $1.74 \\
                \hline
                GPT2-PPL & 25.94$ \pm $1.00 & 25.58$ \pm $0.31 & 28.84$ \pm $0.14 & 29.00$ \pm $0.42 & 27.69$ \pm $1.56 & 30.35$ \pm $1.24 & 54.17$ \pm $0.05 & 54.14$ \pm $0.01 \\
                ProToCo & 42.63$ \pm $1.62 & 21.51$ \pm $1.22 & 36.68$ \pm $0.80 & 27.76$ \pm $1.35 & 52.94$ \pm $2.54 & 26.75$ \pm $0.91 & 40.12$ \pm $0.51 & 30.78$ \pm $0.85 \\
                ProgramFC & 46.04$ \pm $1.42 & 32.12$ \pm $0.76 & 62.49$ \pm $1.74 & 71.63$ \pm $0.91 & 60.17$ \pm $3.34 & 53.67$ \pm $1.92 & 86.84$ \pm $0.84 & 69.41$ \pm $2.07 \\
                \hline
                P-Tuning v2 & 52.54$ \pm $0.55 & 36.94$ \pm $0.13 & 73.03$ \pm $1.76 & 75.60$ \pm $3.01 & 76.56$ \pm $1.77 & 55.48$ \pm $2.04 & 87.01$ \pm $0.36 & 68.87$ \pm $0.76 \\
                \hline
                JustiLM & 47.33$ \pm $3.81 & 33.27$ \pm $1.98 & 58.75$ \pm $3.08 & 60.03$ \pm $1.60 & 69.63$ \pm $1.53 & 51.78$ \pm $0.80 & 81.33$ \pm $1.97 & 65.49$ \pm $0.65 \\
                \hline
                CORRECT & \textbf{59.88}$ \pm $\textbf{2.03} & \textbf{44.25}$ \pm $\textbf{1.73} & \textbf{75.34}$ \pm $\textbf{1.02} & \textbf{80.59}$ \pm $\textbf{1.00} & \textbf{83.20}$ \pm $\textbf{0.80} & 60.26$ \pm $1.31 & \textbf{88.41}$ \pm $\textbf{0.19} & \textbf{74.95}$ \pm $\textbf{0.38} \\
			\bottomrule
		\end{tabular}
	}
	% \vspace{-0.3cm}
	\label{table:fully_supervised_macro_f1}
\end{table*}

\begin{table*}[t]
	\centering
	\caption{Verdict prediction results on \emph{fully supervised} setting with \emph{Micro F1} score. Results are in percentage.}
	\vspace{-0.2cm}
	\resizebox{\textwidth}{!}{
		\begin{tabular}{c|cc|cc|cc|cc}
			\toprule
                \multirow{2}{*}{Model} & \multicolumn{2}{c|}{BearFact} & \multicolumn{2}{c|}{Check-COVID} & \multicolumn{2}{c|}{SciFact} & \multicolumn{2}{c}{FEVEROUS-S} \\
			\cline{2-9}
			{} & Gold & Retrieved & Gold & Retrieved & Gold & Retrieved & Gold & Retrieved \\
			\hline
                KGAT & 69.42$ \pm $0.87 & 57.36$ \pm $0.52 & 72.05$ \pm $1.58 & 76.47$ \pm $0.65 & 74.44$ \pm $0.96 & 62.33$ \pm $0.88 & 86.21$ \pm $0.28 & 67.99$ \pm $0.78 \\
                HESM & 63.68$ \pm $1.39 & 58.62$ \pm $0.32 & 63.47$ \pm $0.25 & 71.90$ \pm $1.85 & 72.44$ \pm $0.77 & 53.36$ \pm $2.33 & 83.30$ \pm $0.75 & 68.36$ \pm $0.86 \\
                Transformer-XH & 61.26$ \pm $0.72 & 56.55$ \pm $1.50 & 68.56$ \pm $0.87 & 76.91$ \pm $1.51 & 75.89$ \pm $0.51 & 58.67$ \pm $1.53 & 85.61$ \pm $0.80 & 69.78$ \pm $0.41 \\
                Transformer-XH++ & 64.02$ \pm $1.44 & 58.39$ \pm $1.11 & 70.60$ \pm $0.67 & 78.65$ \pm $0.38 & 77.78$ \pm $0.69 & 60.56$ \pm $1.83 & 85.52$ \pm $0.39 & 70.37$ \pm $0.77 \\
                MultiVerS & 62.93$ \pm $1.17 & 50.69$ \pm $1.46 & 66.65$ \pm $1.71 & 70.70$ \pm $1.73 & 83.68$ \pm $1.40 & \textbf{66.77}$ \pm $\textbf{0.14} & 83.57$ \pm $1.54 & 67.66$ \pm $1.65 \\
                CausalWalk & 69.31$ \pm $1.69 & 60.00$ \pm $0.69 & 71.86$ \pm $1.54 & 71.68$ \pm $2.48 & 77.34$ \pm $2.30 & 59.00$ \pm $1.20 & 86.42$ \pm $0.92 & 71.51$ \pm $1.66 \\
                \hline
                GPT2-PPL & 40.00$ \pm $2.43 & 39.49$ \pm $0.73 & 32.75$ \pm $0.62 & 32.54$ \pm $0.93 & 31.50$ \pm $0.71 & 31.24$ \pm $1.56 & 54.33$ \pm $0.06 & 54.23$ \pm $0.01 \\
                ProToCo & 56.03$ \pm $0.24 & 35.57$ \pm $2.35 & 37.12$ \pm $1.10 & 32.75$ \pm $2.31 & 60.00$ \pm $1.53 & 31.17$ \pm $0.71 & 54.21$ \pm $0.67 & 44.71$ \pm $1.95 \\
                ProgramFC & 62.00$ \pm $2.83 & 54.63$ \pm $3.66 & 65.50$ \pm $2.12 & 72.36$ \pm $1.85 & 65.40$ \pm $3.78 & 59.76$ \pm $3.54 & 86.48$ \pm $0.33 & 69.50$ \pm $2.12 \\
                \hline
                P-Tuning v2 & 70.69$ \pm $0.49 & 60.34$ \pm $0.15 & 72.93$ \pm $1.85 & 77.32$ \pm $1.96 & 80.34$ \pm $0.94 & 57.44$ \pm $1.83 & 87.16$ \pm $0.33 & 70.56$ \pm $0.54 \\
                \hline
                JustiLM & 62.41$ \pm $1.25 & 48.49$ \pm $2.21 & 59.71$ \pm $2.56 & 61.71$ \pm $2.05 & 72.20$ \pm $1.85 & 54.74$ \pm $0.82 & 81.60$ \pm $0.33 & 68.38$ \pm $1.45 \\
                \hline
                CORRECT & \textbf{74.60}$ \pm $\textbf{1.11} & \textbf{61.84}$ \pm $\textbf{0.11} & \textbf{75.33}$ \pm $\textbf{0.93} & \textbf{80.83}$ \pm $\textbf{0.76} & \textbf{85.17}$ \pm $\textbf{0.71} & 63.50$ \pm $1.17 & \textbf{88.51}$ \pm $\textbf{0.19} & \textbf{75.35}$ \pm $\textbf{0.28} \\
			\bottomrule
		\end{tabular}
	}
	%\vspace{-0.3cm}
	\label{table:fully_supervised_micro_f1}
\end{table*}

\textbf{Baselines.} We have 4 categories of baselines. 

\emph{\textbf{i}}) \textbf{Multi-hop fact-checking}, KGAT \cite{kgat}, HESM \cite{hesm}, Transformer-XH \cite{transformer_xh}, MultiVerS \cite{multivers}, and the recent CausalWalk \cite{causalwalk}. MultiVerS models contextual documents, and HESM and Transformer-XH incorporate referential documents. By comparing to them, we highlight the advantage of three-layer graph for modeling both contextual and referential documents. Since our model is built on Transformer-XH, we further extend it by modeling both contextual and referential documents, and name it Transformer-XH++. The comparison showcases the effect of evidence-conditioned prompting.

\emph{\textbf{ii}}) \textbf{Few-shot fact-checking}, GPT2-PPL \cite{gpt2_ppl}, ProToCo \cite{protoco}, and ProgramFC \cite{programfc}. They are mainly designed for few-shot setting. By increasing their training set, we could also compare to them on fully supervised setting. ProToCo and ProgramFC are proposed with handcrafted natural language prompt. By comparison, we verify the usefulness of our evidence-conditioned prompt embedding.

\emph{\textbf{iii}}) \textbf{Prompt tuning} is not for fact-checking. But for completeness, we convert P-Tuning v2 \cite{p_tuning_v2}, a continuous prompting, to our task.

\emph{\textbf{iv}}) \textbf{Retrieval-augmented generation for fact-checking}. Though our model is not designed with retrieval-augmented generation, we still compare to JustiLM \cite{justilm} for completeness.

\textbf{Implementation details.} Following \cite{transformer}, we set $ L $ to 12 and $ d $ to 768. Number of prompt embeddings $ M $ is 8. Temperature $ \tau $ in Eq. \ref{eq:scaling_and_shifting} is 100. For both our model and language model-based baselines, we initialize the model with pre-trained parameters in biomedical domain \cite{pubmedbert} for scientific datasets, and in general domain \cite{bert} for FEVEROUS-S. Each result is obtained by 5 independent runs. Experiments are done on 4 NVIDIA A100 80GB GPUs. More details are in Appendix \ref{sec:experiment_environment}.

We present two experimental settings below.

\begin{table*}[t]
	\centering
	\caption{Verdict prediction results on \emph{5-shot} setting with \emph{Macro F1} score. Results are in percentage.}
	%\vspace{-0.2cm}
	\resizebox{\textwidth}{!}{
		\begin{tabular}{c|cc|cc|cc|cc}
			\toprule
                \multirow{2}{*}{Model} & \multicolumn{2}{c|}{BearFact} & \multicolumn{2}{c|}{Check-COVID} & \multicolumn{2}{c|}{SciFact} & \multicolumn{2}{c}{FEVEROUS-S} \\
			\cline{2-9}
			{} & Gold & Retrieved & Gold & Retrieved & Gold & Retrieved & Gold & Retrieved \\
			\hline
                KGAT & 36.62$ \pm $2.28 & 29.92$ \pm $3.99 & 35.65$ \pm $4.56 & 47.81$ \pm $2.40 & 39.07$ \pm $2.06 & 35.12$ \pm $2.79 & 50.21$ \pm $0.95 & 50.68$ \pm $1.21 \\
                HESM & 35.40$ \pm $3.77 & 26.00$ \pm $2.34 & 35.41$ \pm $4.78 & 42.82$ \pm $6.50 & 38.87$ \pm $1.69 & 34.03$ \pm $5.61 & 51.36$ \pm $0.35 & 51.92$ \pm $0.39 \\
                Transformer-XH & 29.45$ \pm $2.49 & 31.69$ \pm $2.06 & 40.48$ \pm $2.73 & 49.24$ \pm $1.60 & 47.65$ \pm $3.99 & 33.47$ \pm $1.11 & 52.45$ \pm $2.71 & 49.41$ \pm $1.93 \\
                Transformer-XH++ & 31.34$ \pm $4.07 & 29.74$ \pm $1.30 & 38.73$ \pm $1.35 & 50.56$ \pm $0.64 & 47.53$ \pm $0.65 & 33.79$ \pm $1.87 & 58.19$ \pm $0.75 & 52.78$ \pm $1.60 \\
                MultiVerS & 24.34$ \pm $3.12 & 20.92$ \pm $0.48 & 32.16$ \pm $2.50 & 50.80$ \pm $1.78 & \textbf{52.29}$ \pm $\textbf{1.92} & 29.64$ \pm $1.53 & 38.26$ \pm $0.18 & 38.82$ \pm $0.37 \\
                CausalWalk & 32.01$ \pm $3.35 & 31.10$ \pm $1.56 & 31.73$ \pm $5.13 & 43.79$ \pm $3.25 & 39.48$ \pm $5.51 & 34.95$ \pm $5.28 & 59.46$ \pm $1.78 & 55.37$ \pm $5.55 \\
                \hline
                GPT2-PPL & 24.99$ \pm $0.90 & 26.28$ \pm $0.18 & 25.05$ \pm $4.47 & 23.89$ \pm $2.65 & 27.69$ \pm $0.41 & 27.45$ \pm $0.66 & 51.33$ \pm $2.55 & 51.54$ \pm $2.50 \\
                ProToCo & 35.11$ \pm $0.40 & 21.51$ \pm $0.78 & 35.62$ \pm $5.32 & 29.72$ \pm $3.85 & 48.68$ \pm $3.38 & 25.93$ \pm $5.60 & 40.48$ \pm $0.88 & 31.00$ \pm $0.57 \\
                ProgramFC & 31.42$ \pm $1.20 & 30.88$ \pm $1.98 & 36.17$ \pm $0.73 & 49.06$ \pm $1.14 & 48.69$ \pm $0.46 & 33.18$ \pm $0.89 & 49.13$ \pm $2.57 & 51.62$ \pm $0.62 \\
                \hline
                P-Tuning v2 & 35.68$ \pm $2.36 & 31.86$ \pm $0.33 & 38.90$ \pm $4.81 & 50.63$ \pm $4.22 & 43.94$ \pm $0.54 & 33.33$ \pm $2.48 & 56.70$ \pm $1.82 & 48.53$ \pm $2.23 \\
                \hline
                JustiLM & 31.38$ \pm $2.07 & 26.01$ \pm $2.08 & 36.48$ \pm $2.78 & 44.39$ \pm $2.41 & 44.42$ \pm $2.08 & 31.04$ \pm $1.47 & 45.35$ \pm $1.18 & 42.48$ \pm $1.02 \\
                \hline
                CORRECT & \textbf{40.91}$ \pm $\textbf{1.42} & \textbf{33.47}$ \pm $\textbf{0.46} & \textbf{40.77}$ \pm $\textbf{1.19} & \textbf{52.40}$ \pm $\textbf{1.21} & 49.12$ \pm $0.30 & \textbf{35.30}$ \pm $\textbf{1.05} & \textbf{61.00}$ \pm $\textbf{1.95} & \textbf{57.04}$ \pm $\textbf{0.68} \\
			\bottomrule
		\end{tabular}
	}
	% \vspace{-0.35cm}
	\label{table:5_shot_macro_f1}
\end{table*}
\begin{table*}[t]
	\centering
	\caption{Verdict prediction results on \emph{5-shot} setting with \emph{Micro F1} score. Results are in percentage.}
	% \vspace{-0.2cm}
	\resizebox{\textwidth}{!}{
		\begin{tabular}{c|cc|cc|cc|cc}
			\toprule
                \multirow{2}{*}{Model} & \multicolumn{2}{c|}{BearFact} & \multicolumn{2}{c|}{Check-COVID} & \multicolumn{2}{c|}{SciFact} & \multicolumn{2}{c}{FEVEROUS-S} \\
			\cline{2-9}
			{} & Gold & Retrieved & Gold & Retrieved & Gold & Retrieved & Gold & Retrieved \\
			\hline
                KGAT & 44.66$ \pm $0.25 & 36.78$ \pm $3.86 & 37.55$ \pm $2.86 & 50.98$ \pm $1.13 & 42.22$ \pm $3.06 & 36.78$ \pm $3.52 & 51.13$ \pm $1.55 & 51.66$ \pm $1.38 \\
                HESM & 48.85$ \pm $2.42 & 28.97$ \pm $3.26 & 36.68$ \pm $5.15 & 50.11$ \pm $3.22 & 39.56$ \pm $2.45 & 35.89$ \pm $4.67 & 56.33$ \pm $0.91 & 53.68$ \pm $0.37 \\
                Transformer-XH & 32.53$ \pm $3.13 & 40.80$ \pm $3.32 & 41.67$ \pm $2.19 & 51.63$ \pm $1.16 & 48.89$ \pm $2.84 & 35.11$ \pm $1.95 & 52.94$ \pm $2.65 & 51.56$ \pm $0.73 \\
                Transformer-XH++ & 37.93$ \pm $4.10 & 35.06$ \pm $3.66 & 41.40$ \pm $1.78 & 52.41$ \pm $1.22 & 50.00$ \pm $0.85 & 36.67$ \pm $1.65 & 59.97$ \pm $1.50 & 53.23$ \pm $1.19 \\
                MultiVerS & 40.86$ \pm $0.74 & 39.49$ \pm $1.70 & 41.01$ \pm $1.29 & 49.82$ \pm $1.56 & \textbf{54.99}$ \pm $\textbf{1.90} & 43.33$ \pm $1.74 & 51.39$ \pm $1.33 & 51.84$ \pm $1.54 \\
                CausalWalk & 45.52$ \pm $3.47 & 41.38$ \pm $3.24 & 37.70$ \pm $4.59 & 43.79$ \pm $3.25 & 44.02$ \pm $2.70 & 41.78$ \pm $2.71 & 60.60$ \pm $1.98 & 55.20$ \pm $4.18 \\
                \hline
                GPT2-PPL & 36.38$ \pm $4.14 & 40.69$ \pm $0.49 & 33.19$ \pm $0.67 & 34.62$ \pm $0.72 & 29.44$ \pm $2.50 & 29.00$ \pm $1.46 & 53.02$ \pm $1.11 & 53.11$ \pm $1.12 \\
                ProToCo & 51.03$ \pm $2.44 & 33.80$ \pm $2.44 & 41.05$ \pm $2.47 & 34.50$ \pm $2.31 & 51.55$ \pm $2.27 & 36.78$ \pm $1.35 & 54.72$ \pm $1.32 & 42.45$ \pm $2.73 \\
                ProgramFC & 38.45$ \pm $2.19 & 38.28$ \pm $0.49 & 37.55$ \pm $0.38 & 50.00$ \pm $1.08 & 49.93$ \pm $0.36 & 36.17$ \pm $1.25 & 52.94$ \pm $2.67 & 51.94$ \pm $0.25 \\
                \hline
                P-Tuning v2 & 48.70$ \pm $1.95 & 41.90$ \pm $3.10 & 41.05$ \pm $3.88 & 52.07$ \pm $3.09 & 45.78$ \pm $1.07 & 37.67$ \pm $1.85 & 58.02$ \pm $1.68 & 52.06$ \pm $0.76 \\
                \hline
                JustiLM & 42.70$ \pm $1.64 & 37.72$ \pm $5.08 & 40.82$ \pm $2.20 & 46.73$ \pm $1.53 & 47.41$ \pm $1.07 & 33.00$ \pm $1.58 & 52.54$ \pm $1.71 & 49.38$ \pm $1.60 \\
                \hline
                CORRECT & \textbf{51.72}$ \pm $\textbf{1.04} & \textbf{42.76}$ \pm $\textbf{0.73} & \textbf{43.37}$ \pm $\textbf{1.82} & \textbf{54.46}$ \pm $\textbf{0.76} & 53.00$ \pm $1.30 & \textbf{44.36}$ \pm $\textbf{0.84} & \textbf{63.33}$ \pm $\textbf{0.91} & \textbf{57.14}$ \pm $\textbf{0.82} \\
			\bottomrule
		\end{tabular}
	}
	%\vspace{-0.3cm}
	\label{table:5_shot_micro_f1}
\end{table*}
\begin{figure*}[h]
	\centering
	\includegraphics[width=1\textwidth]{figure/different_shots_results.pdf}
	% \vspace{-0.7cm}
	\caption{Few-shot veracity prediction with different number of shots.}
	\label{fig:different_shots_results}
	% \vspace{-0.2cm}
\end{figure*}

\textbf{Fully supervised v.s. Few-shot.} For fully supervised setting, we train the model on the training set. If the dataset provides data split, we follow the split and obtain training and test sets. Otherwise, we split the dataset into 80:20 for training and test, respectively. Among training set, we further reserve 10\% for validation. For few-shot setting, we report 5-shot experiments as the main results, i.e., for each class in the label set $ y\in\mathcal{Y} $, we randomly sample 5 instances from training set, obtaining $ 5\times|\mathcal{Y}| $ training instances. This setting is consistent with existing work \cite{protoco}. For a fair comparison, we sample instances using 5 random seeds. We keep the same sampling for our model and baselines. We report the results on test set.

\textbf{Gold v.s. Retrieved evidence.} For gold evidence setting, we observe the ground-truth evidence sentences, and we verify the claim based on the gold sentences. For retrieved evidence setting, we do not observe any evidence sentences, and retrieve sentences from an evidence corpus, based on which we make prediction. We follow \cite{programfc} and use BM25 \cite{bm25} to retrieve top-3 evidence sentences for each claim. In the original Check-COVID dataset, if a claim is labeled as REFUTE based on the evidence, this claim is \emph{reused} in NEI class with another random evidence. Thus, there are two claims with the same content, but different evidence and labels. However, in our retrieved evidence setting, both claims will receive the same retrieved evidence, but they are labeled differently, making model training inconsistent. Thus, for retrieved evidence setting, we remove claims in NEI class for Check-COVID.

\subsection{Empirical Evaluation}

\textbf{Fully supervised setting.} We follow \cite{multivers} and report Macro F1 score for both gold and retrieved evidence settings in Table \ref{table:fully_supervised_macro_f1}. We also show Micro F1 score in Table \ref{table:fully_supervised_micro_f1}. Transformer-XH++ consistently outperforms Transformer-XH, verifying that contextual and referential documents bring useful information. By comparing CORRECT to Transformer-XH++, we design evidence-conditioned prompting to integrate evidence and claim embeddings, and further improve the performance. Models with handcrafted prompt do not predict verdict as accurately as our model, which showcases the advantage of continuous prompt embeddings. Overall, the results on gold evidence setting are higher than on retrieved evidence setting, because the retrieved evidence sentences may not be always correct and may contain noisy information. The only exception is Check-COVID, because the retrieved evidence setting has only two labels, making the prediction task easier. MultiVerS is slightly better than CORRECT on SciFact, because the evidence sentences in SciFact contain sufficient information for fact-checking as shown in \cite{scifact,multivers}, and referential documents do not bring much additional benefit.

\begin{figure*}[t]
	\centering
	\includegraphics[width=1\textwidth]{figure/model_analysis.pdf}
	% \vspace{-0.7cm}
	\caption{Model analysis on Check-COVID and FEVEROUS-S.}
	\label{fig:model_analysis}
	% \vspace{-0.4cm}
\end{figure*}

\begin{figure}[t]
	\centering
	\includegraphics[width=0.8\linewidth]{figure/case_study.pdf}
	% \vspace{-0.3cm}
	\caption{Case study on BearFact dataset.}
	\label{fig:case_study}
	% \vspace{-0.6cm}
\end{figure}

\textbf{Few-shot setting.} We report 5-shot results in Table \ref{table:5_shot_macro_f1} for Macro F1 score and Table \ref{table:5_shot_micro_f1} for Micro F1 score. Overall, HESM and Transformer-XH perform better than others, since referential documents contain useful information to complement evidence sentences for accurate prediction. Our model further improves them, verifying the strength of both contextual and referential documents. P-Tuning v2 outperforms models with handcrafted prompt, since continuous prompt embeddings can better adapt to the training data. By comparing to it, we design an evidence-conditioned prompt encoder to integrate contextual and referential documents into prompt embeddings, and produce more accurate results. We vary the number of shots in $ \{1, 3, 5, 10, 15\} $ in Fig. \ref{fig:different_shots_results}. Though our model is competitive with MultiVerS on SciFact, we are still better than it on other datasets, due to the advantage of both contextual and referential documents.

\subsection{Model Analysis}

\textbf{Effect of intra- and cross-layer reasoning.} We respectively remove each graph layer from the complete model. Macro F1 score is shown in Fig. \ref{fig:model_analysis}(a). The model with all three layers performs the best, indicating that all three layers bring useful information. Intra-layer reasoning on evidence sentence layer plays the most important role, since evidence sentences provide the most immediate information for verification. Contexts and references are important, and disregarding them deteriorates the results.

\textbf{Different numbers of prompt embeddings $ M $.} We vary the number of prompt embeddings $ M $ in Fig. \ref{fig:model_analysis}(b). When $ M=2 $, we cannot fully capture the interaction between evidence and claims, causing a low accuracy. After we increase $ M $, we observe an improvement. An overly high $ M $ hurts the result, because overfitting problem appears.

\textbf{Prompt initialization and encoder.} Our prompt encoder has both initialization of base prompt embeddings and evidence-conditioned prompt encoder. \emph{\textbf{i}}) To test the effect of initialization, we replace it with random initialization and report the results in Fig. \ref{fig:model_analysis}(c). Our initialization produces better results, because evidence graph separates different sets of prompt embeddings and provides a more informative starting point. \emph{\textbf{ii}}) We remove prompt encoder from our model, and only retain base prompt embeddings. Fig. \ref{fig:model_analysis}(c) shows that removing prompt encoder hurts the results, indicating that prompt encoder is necessary to combine evidence and claim for accurate prediction.

\textbf{Effect of prompting.} We design two ablated models. \emph{\textbf{i}}) We replace prompting with an MLP classifier, which concatenates evidence and claim embeddings as input, and produces predicted label. Here claim embedding is obtained without prompt embeddings. \emph{\textbf{ii}}) We directly consider evidence embedding as prompt embedding, and do not assume base prompt embeddings. Fig. \ref{fig:model_analysis}(d) shows that prompting performs better than MLP classifier, because prompt embeddings and claim token embeddings are input to claim encoder together, and the contextualized encoding helps exchange information between evidence and claim for accurate prediction. Base prompt embeddings are also helpful, since they store general fact-checking knowledge and help generalize across different claims.

\textbf{Case study.} To intuitively understand that our model captures useful information in referential documents, we conduct a case study and visually show the attention values between an evidence sentence and its cited papers in graph neural networks. Fig. \ref{fig:case_study} shows that the highest attention scores appear between the evidence sentence and referential documents that indeed contain useful information. This visualization verifies that referential documents are crucial to improve claim verification.