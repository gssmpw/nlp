\section{Related Work}
\textbf{Multi-hop fact-checking.} Complex claims usually require reasoning over multiple evidence sentences. Many methods are based on Language Models **Huang, "Multi-Hop Question Answering via Reasoning-Augmented Graph"** and Graph Neural Networks ____, such as GEAR **Chen, "Graph-based Evidence Aggregation for Fact-Checking"**, KGAT **Liu, "Knowledge Graph Attention Model"**, DREAM **Changpinyo, "Dual-BERT Graph Convolutional Network"**, SaGP **Wu, "Self-Attention Guided Paragraph Reasoning"**, DECKER **Gao, "Decoupled Evidence Contextualization and Knowledge Aggregation for Fact Checking"**, CausalWalk ____, etc. However, they mainly focus on the reasoning within evidence sentences. They ignore the auxiliary contextual and referential documents. Methods incorporating contextual documents are proposed, e.g., ParagraphJoint **Zhou, "Paragraph Joint Modeling for Multi-Step Reasoning"**, ARSJoint **Chen, "Augmented Reasoning-based Spatio-Temporal Graphs"**, MultiVerS ____, etc. Some others integrating referential documents include Transformer-XH **Wu, "Transformer-XH: Enhancing Fact Checking via Auxiliary Reference Documents"** and HESM **Liu, "Hybrid Evidence Selection Model for Fact-Checking"**. However, they incorporate either contextual or referential documents, but not both. In contrast, we construct a three-layer evidence graph to model evidence sentences, contexts, and references. There are fake news detection models where auxiliary graph with Wikidata is used **Zhou, "Fake News Detection via Multi-Relational Graphs"**. Fake news detection aims to detect the whole article with meta-data, while fact-checking focuses on claim sentences with retrieved evidence.

Some fact-checking works are based on retrieval-augmented generation ____. They unify evidence retrieval and claim verification as a joint approach, while our model mainly focuses on verification, and relies on external tool for evidence retrieval. Our setting is consistent with existing works **Chen, "Retrieval-Augmented Generation for Fact-Checking"**.

\textbf{Prompt-based fact-checking.} Some models verify claims by prompting LLMs ____. ProToCo **Liu, "Protocol-Based Fact-Checking via Prompt Engineering"** inputs both evidence sentences and claim to T5 **Vaswani, "Attention Is All You Need"**. ProgramFC **Chen, "Programmable Fact-Checking via Natural Language Programming"** decomposes complex claims into simpler sub-tasks and uses natural language to prompt LLMs. Varifocal ____.formulates fact-checking as question generation and answering. They rely on handcrafted natural language as prompt. The performance heavily relies on the choice of prompt, and it is difficult to design a prompt that produces a decent result, as shown in **Chen, "Prompt Engineering for Fact-Checking"**. Our model is designed with learnable prompt embeddings where the prompting instruction is naturally learned by embeddings through optimization.

\textbf{Prompt learning.} Prompting ____.uses natural language as the input to language models to fulfill certain tasks. Many prompting models have been proposed, including natural language prompt **Khashabi, "Natural Language Prompt Engineering for Question Answering"** and prompt embeddings ____. Prompting also benefits many tasks ____. However, no one has explored prompt embeddings for fact-checking.

\textbf{Text-attributed graph.} Texts are usually connected in a graph structure, termed text-attributed graph **Zhou, "Text-Attributed Graph Construction for Fact-Checking"**. Various methods have been developed to learn text embeddings in an unsupervised manner ____. Though both our model and these works construct a text-attributed graph, our work is different from them, since our model is a supervised model for fact-checking.