\section{Experiments}



\begin{table}[t]
\scriptsize
\centering
\renewcommand{\arraystretch}{1.15}
\setlength{\tabcolsep}{4pt}

\caption{Quantitative comparison results on DPG-Bench. Note that we support multiple languages.}
\label{tab:dpg}

\resizebox{\linewidth}{!}{%
\begin{tabular}{lcccccc}
\toprule[0.8pt]


\multirow{1}{*}{\textbf{Method}} &\multicolumn{1}{c}{\textbf{Param}}& \multicolumn{1}{c}{\textbf{Multi-Ling}} & \multicolumn{1}{c}{\textbf{DPG-Bench}} \\ \hline


    
% LDM\cite{radford2021learning} &$611M$& $\times$&$63.18$  \\ 
SD1.5 \cite{rombach2022high} &${0.86B}$& $\times$&${61.18}$  \\ 
SDv2.1 \cite{rombach2022high} &${0.89B}$& $\times$&${68.09}$  \\ 
LlamaGen \cite{sun2024autoregressive} &${0.78B}$& $\times$&${65.16}$  \\ 

HART \cite{tang2024hart} &${0.73B}$& $\times$&${80.89}$  \\ 

Sana \cite{xie2024sana} &${0.60B}$& $\checkmark$&$83.6$  \\ 
ELLA \cite{hu2024ella} &${0.93B}$& $\times$&${80.79}$  \\ 
LLM4GEN\cite{liu2024llm4genleveragingsemanticrepresentation} &${0.86B}$& $\times$&${67.34}$  \\ 
Pixart-$\alpha$ \cite{chen2023pixart} &${0.61B}$& $\times$&${71.11}$  \\ 
% Pixart-S \cite{radford2021learning} &${71.11}$& $\times$&${71.11}$  \\ 
Ours  &${0.63B}$&$\checkmark$& ${80.57}$  \\  
\midrule
SD3-Medium \cite{esser2024scaling} &${2.0B}$&$\times$& ${84.08}$  \\ 
SDXL \cite{podell2023sdxl} &${2.6B}$&$\times$& ${74.65}$  \\ 
Janus \cite{wu2024janus} &${1.3B}$& $\times$&${79.68}$  \\ 
Janus-Pro \cite{chen2025janus} &${7B}$& $\times$&${84.19}$  \\ 
Emu-3 \cite{wang2024emu3} &${8.0B}$& $\times$&${80.60}$  \\ 
 % \cite{radford2021learning} &${50.5}$& $\textbf{73.8}$  \\ 
 DALL-E 3 \cite{betker2023improving} &${-}$&$\times$& ${83.50}$  \\ 

FLUX-Dev \cite{flux2024} &${12.0B}$& $\times$&${84.0}$  \\ 
% \hline





\toprule[0.8pt]

\end{tabular}%
}

\end{table}


\begin{figure*}[t]
	\centering
	\includegraphics[width=0.95\linewidth]{./fig/multi.png}

	\caption{Multilingual qualitative visualization results. For each panel's eight images, we generate them using eight different languages but only display the prompt in one of the languages used. Note that LDGen uses only English prompts during training but achieves zero-shot multilingual generation due to the capabilities of the LLM.}
	\label{fig:multi-ling}
	% \vspace{-0.5em}
\end{figure*}




\begin{table*}[t]
\centering
    \caption{We compare our method with baseline methods and fine-tuned baseline methods on DPG-Bench and Geneval, demonstrating the effectiveness of our approach.}
\renewcommand{\arraystretch}{1}
\setlength{\tabcolsep}{5pt}
    % \renewcommand{\arraystretch}{1.0}
    \resizebox{\linewidth}{!}{
    
    \begin{tabular}{lccccccccccccc}
    \toprule[1pt]
    \multirow{2}{*}{Method}&\multicolumn{1}{c}{Param} & \multicolumn{5}{c}{DPG-Bench \cite{hu2024ella}}  & \multicolumn{5}{c}{Geneval\cite{ghosh2023geneval}}  \\
    \cmidrule(lr){3-7} \cmidrule(lr){8-12} 
    && Global & Entity & Attri. & Other &Overall  & Single Obj. & Two Obj. & Counting & Color Attri. & Overall\\
    \midrule


    Pixart-$\alpha$~\cite{chen2023pixart}& 0.61B & 74.97 & 79.32 & 78.60 & 76.69 & 71.11 & \textbf{0.98} & 0.50 & \textbf{0.44} & 0.07 & 0.48  \\
    Pixart-$\alpha$(fine-tuned) & 0.61B& 83.18 & 84.06 & 84.07 & 83.61 & 75.05& 0.95 & 0.37 & 0.37 & 0.43 & 0.46  \\
    Ours & 0.63B& \textbf{85.88} & \textbf{87.83} & \textbf{85.21} & \textbf{87.85} & \textbf{80.57} & 0.88 & \textbf{0.55} &  0.35& 0.42 & \textbf{0.51} \\
    \bottomrule
    \end{tabular}
    }
  
\label{Tab: com_pixart}
\end{table*}




\paragraph{Model Details.}

Our method is based on the work of PixArt-$\alpha$~\cite{chen2023pixart}, which is a classic diffusion transformer text-to-image model. It uses the T5-XXL text encoder~\cite{raffel2020exploring} and has demonstrated excellent performance. We use Qwen2.5-7B-Instruct~\cite{yang2024qwen2} as the LLM and adopt the output features from the last layer, which has a dimension of 3584. The VAE remains consistent with PixArt-$\alpha$~\cite{chen2023pixart}. For the LLM feature alignment module, we employ a 3-layer encoder-decoder transformer structure, which includes linear layers to align the LLM dimension of 3584 with the T5 dimension of 4096. The cross-modal refiner uses only one block.
\paragraph{Training Details.}

To reduce computational resources, we've structured our training process into several key stages. First, we train the LLM feature alignment module using approximately 80 million text entries from internal image descriptions, with about 20\% of this data being multilingual. Given that T5-XXL~\cite{raffel2020exploring} doesn't support multiple languages, we align the multilingual features from the LLM output with the English output features of T5-XXL~\cite{raffel2020exploring}. This initial phase consumes around 80 A100 GPU days. Next, drawing inspiration from PixArt-$\alpha$'s training methodology, we adapt our model to 512 resolution and fine-tune it using 24 million text-image pairs. To minimize dataset-specific biases in training, we maintain a data scale similar to PixArt-$\alpha$'s~\cite{chen2023pixart} original approach and incorporate various datasets with overlapping ranges, such as JourneyDB~\cite{sun2023journeydb}. In the final stage, we continue training at a 1024 resolution, utilizing 14 million aesthetic data entries. The entire training process requires approximately 120 A100 GPU days. The count of GPU days excludes the time for T5, Qwen, and VAE feature extraction. LDGen takes only approximately 26\% of the GPU days compared to PixArt-$\alpha$.





\paragraph{Evaluation Metrics.}

We evaluate our approach using two publicly available benchmarks: Geneval~\cite{ghosh2023geneval} and DPG-Bench~\cite{hu2024ella}. Geneval is a challenging text-to-image generation benchmark designed to showcase a model's comprehensive generative capabilities through detailed instance-level analysis. DPG-Bench, comprises 1,065 semantically dense long prompts, aimed at evaluating model performance in complex semantic alignment. 

These two datasets provide a comprehensive assessment of generative models from different perspectives.


\subsection{Performance Comparison and Analysis}





We focus on evaluating the performance of our method compared to the baseline model, PixArt-$\alpha$~\cite{chen2023pixart}. As shown in Tab.~\ref{tab:dpg} and Tab.~\ref{Tab: com_pixart}, we utilize two evaluation benchmarks, DPG-Bench~\cite{hu2024ella} and Geneval~\cite{ghosh2023geneval}, to thoroughly assess image-text consistency.

Furthermore, we compare our results with advanced models such as the Stable Diffusion series and enhancement methods like ELLA~\cite{hu2024ella} and LLM4GEN~\cite{liu2024llm4genleveragingsemanticrepresentation}. Our model not only surpasses these baseline models but also achieves approximately a 13\% performance improvement on DPG-Bench compared to PixArt-$\alpha$, approaching the metrics of some larger-scale models. For the Geneval results, we notice that while single-object scores might decrease due to the LLM's data alignment scale being significantly smaller than the hundreds of millions of samples used for text encoder training, we see significant improvements in multiple aspects, such as color attributes, with the LLM's use.

Although we have made progress, there remains a gap when compared to state-of-the-art models such as HART~\cite{tang2024hart} and Sana~\cite{xie2024sana}, which are trained from scratch with extensive resources and incorporate cutting-edge techniques. Nevertheless, our method achieves significant performance gains on the base model with relatively minimal overhead. Tab.~\ref{tab:dpg} presents our evaluation scores across different languages. Even without using multilingual image-text pairs during training, our model achieves a score of 61.3 in some common languages, nearly matching the 61.2 of certain English-trained image generation models (like SD1.5~\cite{rombach2022high}). As shown in Tab.~\ref{tab:multi-lang}, we conduct a multilingual generation comparison with Sana and additionally support languages that are not supported by Sana.

As shown in Fig.~\ref{fig:comparasions}, we present visual comparisons with other enhancement methods like ELLA~\cite{hu2024ella} and LLM4GEN~\cite{liu2024llm4genleveragingsemanticrepresentation}, as well as the baseline PixArt-$\alpha$. Our method exhibits significant improvements in both aesthetics and text alignment, attributed to the integration of an LLM model with robust comprehension capabilities. Even without employing multilingual image-text data during fine-tuning, our model can generate aesthetically pleasing, instruction-following images in multiple languages.

As shown in Fig.~\ref{fig:multi-ling}, we present generation results in eight languages, displayed from top left to bottom right: German, Spanish, Portuguese, Russian, Italian, Korean, English, and Arabic. Although the model may not generate high-fidelity details across different languages, it is still capable of creating many common scenes and objects.


\begin{table}[t]
\scriptsize
\centering
\renewcommand{\arraystretch}{1.0}
\setlength{\tabcolsep}{2.8pt}
%
\caption{Quantitative comparisons of multilingual generation results.
% \textbf{Bold} figures indicate the best performance. 
We additionally support some languages that are not supported by Sana. }
\label{tab:multi-lang}

\resizebox{\linewidth}{!}{%
\begin{tabular}{lcccccc}
\toprule[0.8pt]


\multirow{1}{*}{\textbf{Language}} &\multicolumn{1}{c}{\textbf{Overall$\uparrow$}} & \multicolumn{1}{c}{Glob.} & \multicolumn{1}{c}{Enti. } & \multicolumn{1}{c}{Attr.} & \multicolumn{1}{c}{Rela. } & \multicolumn{1}{c}{Other} \\ \hline


    
Korean (Sana) &$10.6$& $20.3$ & $21.3$ & $20.1$ &  $20.5$ &$23.7$  \\ 
Korean  (Ours)&$\textbf{50.5}$& $\textbf{73.8}$ & $\textbf{63.6}$ & $\textbf{68.1}$ &  $\textbf{70.4}$ &$\textbf{68.6}$  \\ 
% \hline
\midrule


Arabic (Sana) &$12.5$& $22.1$ & $26.1$ & $23.8$ &  $25.4$ &$31.2$  \\ 
Arabic (Ours) &$\textbf{50.0}$& $\textbf{64.4}$ & $\textbf{66.3}$ & $\textbf{66.4}$ &  $\textbf{72.9}$ &$\textbf{66.5}$  \\ 

\midrule


Russian (Sana) &$42.2$& $57.5$ & $57.2$ & $56.6$ &  $59.7$ &$62.2$  \\ 
Russian (Ours)&$\textbf{55.9}$& $\textbf{76.1}$ & $\textbf{70.8}$ & $\textbf{71.4}$ &  $\textbf{73.5}$ &$\textbf{70.2}$  \\ 
\midrule


Spanish (Sana) &$\textbf{67.4}$& $\textbf{78.9}$ & $\textbf{78.1}$ & $\textbf{79.6}$ &  $79.8$ &$75.3$  \\ 
Spanish (Ours) &$61.3$& $74.1$ & $72.0$ & $76.7$ &  $\textbf{80.3}$ &$\textbf{77.9}$  \\ 





\toprule[0.8pt]

\end{tabular}%
}

\end{table}


\subsection{Ablation Study}



In this section, we validate our language representation strategy, LLM alignment module, and cross-modal refiner. First, we conduct a detailed ablation analysis of our Human Instruction (HI) design, with specific details provided in the appendix. Some captions' length exceed CLIP's evaluation capacity, but with LongCLIP~\cite{zhang2024long} supporting up to 248 tokens, we use the LongCLIP score as an additional metric. We randomly select 5,000 samples from the training dataset for calculating their CLIPScore~\cite{hessel2021clipscore} and LongCLIP-Score. As shown in Tab.~\ref{tab:hi scores}, our HI strategy significantly enhances the CLIP scores of the original captions, demonstrating that our language representation strategy accurately extracts text embeddings and effectively improves text-image alignment during model training.

Although our training data size is similar to PixArt-$\alpha$, to eliminate the potential benefits of extra data, we fine-tune the original PixArt-$\alpha$ weights using the T5-XXL~\cite{raffel2020exploring} with the same training data. As shown in Tab.~\ref{Tab: com_pixart}, our method remains superior to this fine-tuned model, validating the effectiveness of our LLM alignment module and cross-modal refiner.










\section{Conclusion}


This paper presents LDGen, which integrates LLMs with diffusion models to enhance text-to-image generation. By using the language representation strategy, LLM alignment module, and cross-modal refiner, we improve semantic alignment between text and images, reduce training demands, and enable zero-shot multilingual generation. Experiments indicate the superiority of LDGen and provide new insights into LLM-T2I tasks.



\section{Limitations}


Our work integrates LLM into diffusion models with text encoders, enhancing text-image alignment and enabling excellent zero-shot multilingual image generation using limited resources. However, our LLM alignment training data is smaller compared to classic text encoders, potentially affecting the understanding of complex prompts and alignment for certain concepts. Additionally, uneven multilingual corpora distribution leads to varied performance across languages. We plan to expand training data in the future to address these issues.