\section{Introduction}

% In text-to-image (T2I) generation tasks
Text-to-image (T2I) models aim to generate images from text descriptions.~\cite{rombach2022high,podell2023sdxl,saharia2022photorealistic,bai2024meissonic,nichol2022glide}. Thus, natural language descriptions serve as a critical bridge for conveying user intent and generating visually appealing images that accurately capture the intended semantic information. Despite the impressive performance demonstrated by advanced text-to-image models, their reliance on text encoders such as CLIP~\cite{radford2021learning} and T5~\cite{raffel2020exploring}, which are primarily tailored for English, 
% limits their understanding of other languages and 
constrains their multilingual capabilities due to the linguistic limitations of training datasets.

Recently, large language models~\cite{bai2023qwen,liu2024deepseek,achiam2023gpt,glm2024chatglm,dubey2024llama} have achieved notable success in the field of natural language processing. These models possess advanced language comprehension abilities, enabling them to deeply analyze prompts and provide rich, precise semantic guidance for image generation. Furthermore, many LLMs~\cite{team2024gemma,bai2023qwen,achiam2023gpt} are trained on multilingual corpora, granting them the ability to support multiple languages. These advantages have motivated researchers to explore the use of LLMs in text-to-image generation tasks. However, some prior approaches~\cite{xie2024sana,ma2024exploring,xing2024mulan,ye2024altdiffusion} have attempted to directly replace text encoders with LLMs, leading to unstable training processes and significant challenges for researchers with limited computational resources. For instance, ELLA~\cite{hu2024ella} and LLM4GEN~\cite{liu2024llm4genleveragingsemanticrepresentation} seek to align LLMs with the CLIP model but require extensive training data to adapt LLMs representations within diffusion models. These methods often treat LLMs features as mere text conditions, thereby failing to fully exploit the comprehensive language understanding capabilities of LLMs. As shown in Appendix A, directly employing LLMs for image descriptions can introduce unintended content, resulting in semantic biases and adversely affecting the output quality of diffusion models.


To effectively address these challenges and integrate large language models into existing text-to-image tasks under resource constraints, we propose LDGen. Our approach enables the efficient incorporation of LLM into current diffusion models based on T5/CLIP text encoders with minimal computational demands. As shown in Fig.~\ref{fig:pipeline}, we introduce a robust language representation strategy (LRS). By utilizing hierarchical caption optimization and human instruction strategies, LRS fully harnesses the instruction-following, in-context learning, and reasoning capabilities of LLM to accurately derive textual information, thereby enhancing semantic alignment between text and image.

Furthermore, inspired by recent advancements in alignment methods~\cite{hu2024ella,zhao2024bridging,tan2024empirical}, we employ a lightweight adapter to align LLM features with T5-XXL, substantially reducing the training time required for text-image alignment. Additionally, we introduce a cross-modal refiner to improve text comprehension and facilitate interaction between LLM and image features. After alignment, the LLM features processed through this refiner exhibit enhanced representational capability. Specifically, the cross-modal refiner integrates self-attention layers, cross-attention layers, and feed-forward neural networks.
% with cross-attention layers utilizing LLM features as queries to refine the current features through latent keys and values.


By employing this method, LLM can be effectively integrated into existing diffusion models with minimal training. Moreover, the multilingual capabilities of LLM are preserved, enabling zero-shot multilingual image generation without the necessity for training on multilingual text-image datasets. Our experimental results demonstrate that by leveraging the intrinsic features of LLM alongside our innovative modules, LDGen surpasses the prompt comprehension performance of advanced baseline models while seamlessly supporting multiple languages. As shown in Fig.~\ref{fig:teaser}, we present several generated images. Our contributions can be summarized as follows:
\begin{itemize}[noitemsep]
    \item We present LDGen, which efficiently integrates LLM into existing text encoder-based diffusion models and supports zero-shot multilingual text-to-image generation.
    
    \item We propose a language representation strategy that leverages the capabilities of LLM through hierarchical caption optimization and human instruction strategies.
    
    \item We introduce LLM alignment and a cross-modal refiner to achieve LLM feature alignment and enhance interaction between LLM and image features, enhancing the semantic consistency of conditions.
    % while significantly cutting down on training demands.
\end{itemize}
