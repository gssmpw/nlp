\section{Related Work}
\begin{figure*}[t]
  \centering
  \includegraphics[width=0.92\textwidth]{./fig/pipeline1.pdf}
\vspace{-1em}
  \caption{ Overview of LDGen. The dashed box shows our language representation strategy, with the bottom is our LLM alignment and cross-modal refiner training process. The detailed design of the cross-modal refiner is shown in the green box on the right.
  }
  \vspace{-1em}
  \label{fig:pipeline}
\end{figure*}

\paragraph{Text-to-Image.}

% 
Recently, denoising diffusion probabilistic models (DDPM) \cite{ho2020denoising,nichol2021improved} have achieved breakthroughs in image synthesis and downstream applications~\cite{zhang2023adding,zhou2024magictailor,li2024photomaker,wei2023elite,li2023layerdiffusion,li2024tuning,li2024pruning,feng2024dit4edit}. By mapping the image pixels to a more compact latent space where a denoising network is trained to learn the reverse diffusion process, prominent text-guided generation models have achieved impressive results in terms of image quality and semantic fidelity. Earlier methods \cite{rombach2022high, podell2023sdxl} based on the UNet have been tremendously successful in various generative tasks. With the success of the transformer architecture in various fields, diffusion transformer-based methods~\cite{peebles2023scalable,gao2023masked} are notably developing. Techniques like FLUX \cite{flux2024} and SD3 \cite{esser2024scaling} introduced the MMBlock to further align text and images during training. PixArt-$\alpha$ \cite{chen2023pixart} explored efficient text-to-image training schemes and achieved the first Transformer-based T2I model capable of generating high-quality images at 1024 resolution. Models like Lumina-T2X \cite{gao2024lumina} and GenTron \cite{chen2024gentron} extended diffusion transformers from image generation to video generation. Playgroundv3 (PG3) \cite{liu2024playground} proposed a comprehensive VAE training, caption annotation, and evaluation strategy. 



\paragraph{Large Models in T2I.} 




The text encoder plays a crucial role in the text-to-image task. In the initial LDM \cite{rombach2022high}, CLIP \cite{radford2021learning} was used as the text encoder, providing the diffusion model with text comprehension capabilities. Later, Imagen \cite{saharia2022photorealistic} discovered that using a large language model with an encoder-only structure like T5 \cite{raffel2020exploring} significantly enhanced the model's text understanding. Following this, several works \cite{chen2023pixart, chen2024pixart, sun2024autoregressive, betker2023improving, esser2024scaling} utilized the T5 series of models as text encoders during pre-training. Additionally, some other works \cite{liu2024llm4genleveragingsemanticrepresentation, hu2024ella, zhao2024bridging, tan2024empirical}, attempted to adapt the T5 and LLMs~\cite{dubey2024llama} to the base models pre-trained based on CLIP. Considering the recent success of decoder-only large language models, some works have sought to apply them in image generation frameworks. PG3 \cite{liu2024playground} focused on model structure, believing that knowledge in LLMs spans all layers, thus replicating all Transformer blocks from the LLM. LiDiT \cite{ma2024exploring}, from an application perspective, designed an LLM-infused Diffuser framework to fully exploit the capabilities of LLMs. Sana \cite{xie2024sana}, focusing on efficiency, directly used the final layer of LLM features as text encoding features. Kolors \cite{kolors} adapts LLMs for use with SDXL by simply replacing the original CLIP text encoder with ChatGLM. These efforts collectively demonstrate that LLMs still hold significant research potential in the field of image generation.



