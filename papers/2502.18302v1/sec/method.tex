\section{Method}

\subsection{Motivation}

Text encoding is a pivotal component in text-to-image models, significantly influencing the quality of the generated images. As shown in Fig.~\ref{fig:text_encoder}, the CLIP~\cite{radford2021learning} and T5~\cite{raffel2020exploring} series models currently dominate the field of text encoders. However, the rapid advancement of large language models~\cite{achiam2023gpt,team2024gemma} is noteworthy. These models employ autoregressive language modeling techniques~\cite{yang2019xlnet,black2022gpt} in unsupervised learning. Through processing vast amounts of text data, they are beginning to exhibit remarkable reasoning and contextual understanding capabilities. 
They excel across a range of textual tasks. In particular, LLMs trained on multilingual corpora have demonstrated substantial promise in text-to-image generation tasks. Nonetheless, a critical challenge persists: many existing models rely on CLIP/T5 series text encoders, which are predominantly trained on English corpora and perform effectively. 
Transitioning to LLMs by replacing the existing text encoders and retraining these models from scratch would involve considerable resource expenditures. To address this issue, we employ LDGen, which seamlessly integrates LLMs into existing diffusion models based on T5/CLIP text encoders, utilizing only a small portion of the initial training resources. These new models not only outperform the originals but also enable zero-shot text-to-image generation across multiple languages.






\subsection{Language Representation Strategy}

Based on the above analysis, while large language models offer substantial advantages, they still encounter several significant challenges. As dialogue models, LLMs employing a decoder-only architecture rely on autoregressive language modeling methods. These models learn linguistic patterns through unsupervised training on large-scale text datasets by predicting the subsequent word in a sequence. However, this characteristic often makes it difficult to control the model outputs, leading to producing a lot of redundant information.

We observe that both LiDiT~\cite{ma2024exploring} and Sana~\cite{xie2024sana} utilize human instructions to help LLMs produce more stable content. However, as shown in Fig.~\ref{fig:human instruction}, these methods can conflict with the original captions. Incorrect human instructions may cause outputs to deviate from factual accuracy and generate fabricated information, thereby disrupting text-image alignment and potentially decreasing the effectiveness of training.

To address these challenges, we employ a hierarchical captioning strategy. This approach is complemented by extensive human instruction optimization to achieve optimal language representation and enhance semantic alignment between text and images. First, similar to PG3's~\cite{liu2024playground} multi-level image description technique, we utilize the Internvl2-40B model~\cite{chen2024far,chen2024expanding} to re-caption all image data. We generate six captions of varying lengths, ranging from simple to detailed, to comprehensively capture the image content. For detailed captioning prompts, please refer to Appendix Fig.~\ref{fig:appendix_hi}, HI-05. During training, these hierarchical captions are randomly sampled and input into the LLM. As shown in Tab.~\ref{tab:hi scores}, compared to original single-caption methods, LRS enables the model to more effectively capture the hierarchical structure of language concepts while maintaining a high CLIP score.

For these complex and varied-length hierarchical captions, we further refined human instructions to ensure that the LLM's outputs maintain a high CLIP score and avoid generating non-existent information. As shown in Tab.~\ref{tab:hi scores}, the LLM surprisingly enhances the CLIP scores of the original captions, revealing that our language representation strategy effectively extracts semantic information and enhances text-to-image alignment during model training. To support multilingual text-to-image generation, we evaluated several mainstream LLMs. We selected Qwen~\cite{yang2024qwen2} as our preferred model because it is one of the few trained on multilingual corpora and exhibits exceptional performance in text-related tasks.







\begin{figure}[t]
  \centering
  \includegraphics[width=0.47\textwidth]{./fig/encoder_language_distribution_pie_charts.png}

  \caption{Distribution of text encoder and supported languages. English-based CLIP/T5 series models remain the primary text encoders.
  }
    \vspace{-0.5em}

  \label{fig:text_encoder}
  
\end{figure}

\begin{figure}[t]
  \centering
  \includegraphics[width=0.48\textwidth]{./fig/human_i.png}

  \caption{The red words in Sana's generated result highlight elements that do not align with the image. Providing incorrect instructions can change the original caption, potentially creating inaccurate descriptions.
  }
  \vspace{-1em}
  \label{fig:human instruction}
\end{figure}




\definecolor{pz}{RGB}{230,230,230}
\begin{table*}[h]
\centering

\caption{Human Instruction Comparison. Each entry has the CLIP-Score~\cite{hessel2021clipscore} on the left and the LongCLIP-Score~\cite{zhang2024long} on the right, with the average word number is in \textcolor{gray}{gray brackets (.)}. \textbf{Original} refers to the initial caption. "HI" indicates outputs from various Human Instruction strategies. The highest scores are highlighted in \textbf{bold}, while the second-highest scores are \underline{underlined}. Scores that surpass the original captions are marked with a \colorbox{pz}{gray background}.}
\renewcommand{\arraystretch}{1.1}
\resizebox{\textwidth}{!}{
\begin{tabular}{lcccccccc}
\toprule[1.2pt]
 & \textbf{Original} & \textbf{Ours} & \textbf{No-HI} & \textbf{HI-01} & \textbf{HI-02} & \textbf{HI-03} & \textbf{HI-04} & \textbf{HI-05} \\
\hline
Caption-1 & 27.65/29.53 &  \colorbox{pz}{\textbf{27.66}}/\colorbox{pz}{\underline{29.74}} & 22.21/27.44 & 22.79/27.95 & \underline{23.33}/\textbf{30.04} & 22.61/26.59 & 23.06/29.61 & 22.40/27.97 \\
& \textcolor{gray}{(4.48)} & \textcolor{gray}{(7.63)} & \textcolor{gray}{(204.29)} & \textcolor{gray}{(335.19)} & \textcolor{gray}{(87.52)} & \textcolor{gray}{(171.14)} & \textcolor{gray}{(254.19)} & \textcolor{gray}{(224.43)} \\
 \hline
 
Caption-2 & 29.65/31.49 & \colorbox{pz}{\textbf{29.66}}/\colorbox{pz}{\underline{31.63}} & 22.89/28.35 & 23.76/30.31 & \underline{24.47}/\textbf{31.78} & 23.32/28.89 & 24.07/31.25 & 23.41/29.93 \\
& \textcolor{gray}{(8.99)} & \textcolor{gray}{(11.67)} & \textcolor{gray}{(173.66)} & \textcolor{gray}{(307.68)} & \textcolor{gray}{(70.00)} & \textcolor{gray}{(172.76)} & \textcolor{gray}{(245.64)} & \textcolor{gray}{(214.01)} \\
\hline

Caption-3 & 30.20/33.24 & \textbf{29.50}/\textbf{32.92} & 23.75/29.52 & 24.40/31.05 & \underline{25.29}/\underline{32.77} & 24.31/30.31 & 24.72/32.37 & 24.33/31.03 \\
& \textcolor{gray}{(21.71)} & \textcolor{gray}{(25.25)} & \textcolor{gray}{(183.91)} & \textcolor{gray}{(306.33)} & \textcolor{gray}{(80.78)} & \textcolor{gray}{(194.68)} & \textcolor{gray}{(226.49)} & \textcolor{gray}{(192.64)} \\
 \hline
 
Caption-4 & 27.53/34.64 & \textbf{27.13}/\textbf{33.76} & 24.56/30.03 & 24.67/31.49 & \underline{25.33}/\underline{33.35} & 24.63/30.42 & 25.01/33.25 & 24.89/32.19 \\
& \textcolor{gray}{(45.16)} & \textcolor{gray}{(46.96)} & \textcolor{gray}{(249.35)} & \textcolor{gray}{(329.49)} & \textcolor{gray}{(116.45)} & \textcolor{gray}{(253.19)} & \textcolor{gray}{(205.26)} & \textcolor{gray}{(167.07)} \\
 \hline
 
Caption-5 & 25.39/34.43 & \colorbox{pz}{\textbf{25.40}}/\colorbox{pz}{\textbf{33.74}} & 23.87/30.33 & 24.86/31.78 & \underline{25.37}/33.37 & 24.38/29.65 & 25.22/\underline{33.52} & 25.30/33.20 \\
& \textcolor{gray}{(118.06)} & \textcolor{gray}{(106.18)} & \textcolor{gray}{(304.45)} & \textcolor{gray}{(350.39)} & \textcolor{gray}{(183.10)} & \textcolor{gray}{(334.34)} & \textcolor{gray}{(205.27)} & \textcolor{gray}{(177.34)} \\
 \hline
 
Caption-6 & 25.42/34.65 & \colorbox{pz}{\underline{25.48}}/\textbf{33.96} & 23.72/31.09 & 25.05/32.18 & 25.36/33.60 & 24.26/30.45 & 25.38/33.89 & \textbf{25.59}/\underline{33.91} \\
& \textcolor{gray}{(118.06)} & \textcolor{gray}{(106.18)} & \textcolor{gray}{(304.45)} & \textcolor{gray}{(350.39)} & \textcolor{gray}{(183.10)} & \textcolor{gray}{(334.34)} & \textcolor{gray}{(205.27)} & \textcolor{gray}{(177.34)} \\
% \hline
\toprule[1.2pt]
\end{tabular}
}
\label{tab:hi scores}
\end{table*}

\begin{figure*}[t]
	\centering
	\includegraphics[width=0.98\linewidth]{./fig/main_compare.pdf}

	\caption{Comparison of our method with recent enhancement generative models ELLA~\cite{hu2024ella}, baseline Models SDXL~\cite{podell2023sdxl} and PixArt-$\alpha$~\cite{chen2023pixart}. Our method achieves the best results in terms of instruction adherence and visual appeal.}
	\label{fig:comparasions}
	% \vspace{-0.5em}
\end{figure*}





\subsection{LLM Alignment}




For pre-trained diffusion models~\cite{chen2023pixart,podell2023sdxl}, aligning the original text encoder with LLMs features using linear layers is challenging. This is primarily due to the significant differences in the output feature spaces of T5/CLIP encoders and LLMs. As a result, directly modifying and training the existing model structure can lead to instability. To address this, we employ a two-step approach: first, we align the feature spaces, then fine-tune the model weights to adapt to the new feature space. This method significantly reduces training time.

Specifically, we first multiply the LLM output by a small coefficient to match the numerical range of T5. This effectively speeds up the feature alignment training. Next, similar to previous methods~\cite{tan2024empirical}, we design a three-layer encoder-decoder Transformer adapter to align the feature spaces of the T5 encoder and LLM output. During the adapter training, we utilize the following alignment loss functions: $\lambda_{1}* \mathcal{L}_{\text{cos}} + \lambda_{2}*\mathcal{L}_{\text{MSE}}$. The cosine similarity loss aligns the feature space directions, and mean squared error (MSE) loss can further enhance alignment accuracy in terms of numerical range.

By optimizing the alignment loss, we achieve a rough alignment between LLM and T5 output feature spaces. This allows us to quickly integrate the LLM into the pre-trained diffusion model, enhancing its overall performance and adaptability.




\subsection{Cross-Modal Refiner}


To improve text comprehension and facilitate interaction between LLM features and image features, we introduce a lightweight module called the cross-modal refiner. This module employs a sequence of components to optimize and refine LLM feature representations, enabling efficient integration of text and image features. As shown in Fig.~\ref{fig:pipeline}, it includes elements such as self-attention mechanisms, cross-attention mechanisms, feedforward neural networks, residual connections, normalization layers, and learnable scaling factors.

To enhance the interaction between image and text features, the cross-attention layer serves as a pivotal component of modal interaction. This layer utilizes LLM features as queries, with latent image features acting as keys and values, to facilitate deep interaction between text and image elements. This design enables the refinement and adjustment of text features based on relevant image information, thereby enhancing the model's understanding of cross-modal content. Learnable scaling factors allow the model to gradually balance between original and optimized features during training, ensuring a seamless transition from pre-trained weights to new LLM input features. This mechanism effectively integrates the original LLM's robust semantic understanding into the pre-trained models, boosting overall performance.



The cross-modal refiner module preserves the original LLM features and effectively integrates image-related information to produce richer, semantically aligned conditional representations. This approach allows us to efficiently integrate the LLM into existing diffusion models within relatively short training times, providing highly semantically aligned conditional information for text-to-image generation tasks, significantly enhancing the quality and relevance of generated results.


