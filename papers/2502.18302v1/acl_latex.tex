% This must be in the first 5 lines to tell arXiv to use pdfLaTeX, which is strongly recommended.
\pdfoutput=1
% In particular, the hyperref package requires pdfLaTeX in order to break URLs across lines.

\documentclass[11pt]{article}

% Change "review" to "final" to generate the final (sometimes called camera-ready) version.
% Change to "preprint" to generate a non-anonymous version with page numbers.
\usepackage[]{acl}

% Standard package includes
\usepackage{times}
\usepackage{latexsym}
\usepackage{booktabs}
\usepackage[ruled,linesnumbered]{algorithm2e}
% \usepackage{algorithm} 
\usepackage{algorithmic}
% The "axessiblity" packag
\usepackage{tabularx}  % 在导言区添加这一行
\usepackage{multirow}

% For proper rendering and hyphenation of words containing Latin characters (including in bib files)
\usepackage[T1]{fontenc}
% For Vietnamese characters
% \usepackage[T5]{fontenc}
% See https://www.latex-project.org/help/documentation/encguide.pdf for other character sets

% This assumes your files are encoded as UTF8
\usepackage[utf8]{inputenc}

% This is not strictly necessary, and may be commented out,
% but it will improve the layout of the manuscript,
% and will typically save some space.
\usepackage{microtype}

% This is also not strictly necessary, and may be commented out.
% However, it will improve the aesthetics of text in
% the typewriter font.
\usepackage{inconsolata}

%Including images in your LaTeX document requires adding
%additional package(s)
\usepackage{graphicx}
\usepackage{amssymb}
\usepackage{enumitem}
\usepackage{amsmath}
\usepackage{stfloats}
% If the title and author information does not fit in the area allocated, uncomment the following
%
%\setlength\titlebox{<dim>}
%
% and set <dim> to something 5cm or larger.

\title{LDGen: Enhancing Text-to-Image Synthesis via Large Language Model-Driven Language Representation}

% Author information can be set in various styles:
% For several authors from the same institution:
% \author{Author 1 \and ... \and Author n \\
%         Address line \\ ... \\ Address line}
% if the names do not fit well on one line use
%         Author 1 \\ {\bf Author 2} \\ ... \\ {\bf Author n} \\
% For authors from different institutions:
% \author{Author 1 \\ Address line \\  ... \\ Address line
%         \And  ... \And
%         Author n \\ Address line \\ ... \\ Address line}
% To start a separate ``row'' of authors use \AND, as in
% \author{Author 1 \\ Address line \\  ... \\ Address line
%         \AND
%         Author 2 \\ Address line \\ ... \\ Address line \And
%         Author 3 \\ Address line \\ ... \\ Address line}

\author{Pengzhi Li, Pengfei Yu$^{{\dag}}$, Zide Liu, Wei He,Xuhao Pan,
\\ 
\textbf{Xudong Rao, Tao Wei, Wei Chen$^{{\S}}$}\\
Li Auto Inc.
}
% \institute{Li Auto Inc.\ \}
% \author{P \\
%   Affiliation / Address line 1 \\
%   Affiliation / Address line 2 \\
%   Affiliation / Address line 3 \\
%   \texttt{email@domain} \\\And
%   Second Author \\
%   Affiliation / Address line 1 \\
%   Affiliation / Address line 2 \\
%   Affiliation / Address line 3 \\
%   \texttt{email@domain} \\}

%\author{
%  \textbf{First Author\textsuperscript{1}},
%  \textbf{Second Author\textsuperscript{1,2}},
%  \textbf{Third T. Author\textsuperscript{1}},
%  \textbf{Fourth Author\textsuperscript{1}},
%\\
%  \textbf{Fifth Author\textsuperscript{1,2}},
%  \textbf{Sixth Author\textsuperscript{1}},
%  \textbf{Seventh Author\textsuperscript{1}},
%  \textbf{Eighth Author \textsuperscript{1,2,3,4}},
%\\
%  \textbf{Ninth Author\textsuperscript{1}},
%  \textbf{Tenth Author\textsuperscript{1}},
%  \textbf{Eleventh E. Author\textsuperscript{1,2,3,4,5}},
%  \textbf{Twelfth Author\textsuperscript{1}},
%\\
%  \textbf{Thirteenth Author\textsuperscript{3}},
%  \textbf{Fourteenth F. Author\textsuperscript{2,4}},
%  \textbf{Fifteenth Author\textsuperscript{1}},
%  \textbf{Sixteenth Author\textsuperscript{1}},
%\\
%  \textbf{Seventeenth S. Author\textsuperscript{4,5}},
%  \textbf{Eighteenth Author\textsuperscript{3,4}},
%  \textbf{Nineteenth N. Author\textsuperscript{2,5}},
%  \textbf{Twentieth Author\textsuperscript{1}}
%\\
%\\
%  \textsuperscript{1}Affiliation 1,
%  \textsuperscript{2}Affiliation 2,
%  \textsuperscript{3}Affiliation 3,
%  \textsuperscript{4}Affiliation 4,
%  \textsuperscript{5}Affiliation 5
%\\
%  \small{
%    \textbf{Correspondence:} \href{mailto:email@domain}{email@domain}
%  }
%}

\begin{document}
% \maketitle
\twocolumn[{
\renewcommand\twocolumn[1][]{#1}
\maketitle
\begin{center}
    \captionsetup{type=figure}
    	\includegraphics[width=0.95\linewidth]{./fig/teaser.pdf}
    % \includegraphics[width=.4\textwidth]{example-image}
  
	\caption{Generated image samples from LDGen. We present a composed prompt with each language in a different color, along with the corresponding image that exhibits high aesthetic quality and text-image alignment.}
	\label{fig:teaser}
\end{center}
}]

% Equal contribution
\let\thefootnote\relax\footnotetext{
$^\S$Corresponding author, $^\dag$ Project leader.}

\begin{abstract}
In this paper, we introduce LDGen, a novel method for integrating large language models (LLMs) into existing text-to-image diffusion models while minimizing computational demands. Traditional text encoders, such as CLIP and T5, exhibit limitations in multilingual processing, hindering image generation across diverse languages. We address these challenges by leveraging the advanced capabilities of LLMs. Our approach employs a language representation strategy that applies hierarchical caption optimization and human instruction techniques to derive precise semantic information,. Subsequently, we incorporate a lightweight adapter and a cross-modal refiner to facilitate efficient feature alignment and interaction between LLMs and image features. LDGen reduces training time and enables zero-shot multilingual image generation. Experimental results indicate that our method surpasses baseline models in both prompt adherence and image aesthetic quality, while seamlessly supporting multiple languages. Project page: \url{https://zrealli.github.io/LDGen}.

% In this paper, we introduce LDGen, a novel approach to integrating large language models (LLMs) into existing text-to-image (T2I) diffusion models with minimal computational resources. Traditional text encoders like CLIP and T5 struggle with multilingual capabilities, making it difficult to generate images in diverse languages. We address this by leveraging the advanced language language model. We first employ a Language Representation Strategy (LRS) that utilizes hierarchical caption optimization and human instruction strategies to extract precise text embeddings. Next, we introduce a lightweight adapter and cross-modal refiner for efficient feature alignment and interaction between LLMs and image features. LDGen reduces training time and additionally supports zero-shot multilingual image generation. Experimental results demonstrate that our method surpasses baseline models in prompt-following and aesthetic quality while supporting multiple languages. 
% Our contributions significantly enhance the semantic matching capabilities of LLMs in T2I tasks under resource constraints.

% In this paper, we present LDGen, an efficient approach for integrating large language models (LLMs) into text-to-image diffusion models using minimal computational resources. While traditional text encoders like CLIP and T5 struggle with multilingual generation, LDGen leverages LLMs' advanced language comprehension and multilingual training to enhance semantic alignment between text and image. We introduce a Language Representation Strategy (LRS) alongside a novel cross-modal refiner to optimize text embeddings and improve LLM-image feature interaction. Our method enables zero-shot multilingual image generation and demonstrates superior performance in prompt comprehension compared to existing models, all while maintaining efficiency under resource constraints.


\end{abstract}


\input{sec/introduction}    
\input{sec/related}
\input{sec/method}
\input{sec/exp}
\input{sec/conclusion}






% \section{Appendix}
% \label{sec:appendix}

% This is an appendix.

% \bibliographystyle{splncs04}
\bibliography{custom}
\input{./sec/appendix}

\end{document}
