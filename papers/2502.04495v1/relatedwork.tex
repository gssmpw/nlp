\section{Related Work}
\label{sec:related_work}
\vspace{-0.05cm}
This work is inspired by the ideas and limitations of previous research in dynamical system forecasting, meta-learning, and invariant learning.


Deep learning models are widely applied in many physical applications~\citep{lusch2018deep,yeo2019deep,kochkov2021machine,chen2018neural} including partial differential equations (PDEs) with the focus on the multi-scale~\citep{li2020fourier,stachenfeld2021learned}, multi-resolution~\citep{kochkov2021machine,wu2022learning}, and long-term stability~\citep{li2021learning,lippe2023pde} issues. 
Operator learning and neural operators~\citep{gupta2021multiwavelet,kovachki2023neural} are popular for PDE estimations. Although the ODE dynamical system does not contain the multi-scale problem that Fourier neural operator~\citep{kovachki2023neural} tried to solve, our framework can be considered as a kind of operator learning. 
% In this case, regarding the discrete ODE system states as PDE functions, our framework decomposes the function-to-function operator learning into two stages, \ie, a function to derivative function learning, and a derivative function to function forecasting. This separation not only facilitates flexible forecasting but also enables the isolation of function representations, laying the foundation for discovering invariant mechanisms. It is worth noting that, different to our work, these approaches typically emphasize fitting specific dynamical systems without attempting to uncover invariant mechanisms. 



% Physics-informed machine learning methods are proposed to incorporate physical inductive bias into the fitting of neural networks in order to avoid the prediction of physically implausible results, which are often related to symbolic regression. Though it is important direction, it is not related to this paper.
Meta-learning methods~\citep{finn2017model,rusu2018meta,li2017meta,zintgraf2019fast,perez2018film} aim to learn meta-parameters that can be used across multiple tasks, where the meta parameters are generally learned to make rapid adaptations. In previous meta-learning studies on dynamical systems~\citep{Kirchmeyer2022coda,wang2022meta, yin2021leads}, the objective was to find a meta-function that could quickly adapt to multiple new systems, where hypernetworks are only employed as low-rank adaptors for new dynamical system trajectories, similar to the idea of LoRA~\citep{hu2021lora}. Our work differs from such meta-learning approaches in two key ways. First, we focus on discovering invariant functions rather than quickly adaptable ones. Second, while meta-learning methods seek to learn a single meta-function, our framework learns multiple functions, represented by an invariant function random variable $\rf_c$. This distinction stems from our more complex environment definition, detailed in Sec.~\ref{sec:invariant_function_learning}. From another aspect, learning an invariant function distribution instead of a single function can be considered as generalized meta-learning with an invariant function learning goal. 


Current invariant learning methods~\citep{arjovsky2019invariant,lu2021invariant,rosenfeld2020risks,krueger2021out,sagawa2019distributionally} follow the framework of invariant risk minimization (IRM)~\citep{arjovsky2019invariant}, which was inspired by invariant causal predictor~\citep{peters2016causal}. This invariant learning framework aims to learn a hidden invariant representation that generalizes across multiple environments, ensuring out-of-distribution performance. However, this approach cannot work on dynamical forecast tasks due to the lack of invariant function definition and the violation of the categorical data assumption. To be more specific, first, invariant functions cannot be naturally defined in the real number vector space. Second, invariant learning commonly assumes the prediction results are categorical, where a single invariant representation can fully determine the corresponding label. However, this assumption is violated in dynamical system forecasting, where the invariant mechanism is only partially responsible for the output. In this case, the IRM principle can not hold even when the invariant function ground truth is provided. To address the issues, we introduce the causal assumption (see Fig.~\ref{fig:causal_graph}) that defines the invariant function space, and propose the corresponding invariant function learning principle and implementation.
% inspired by the philosophy of information bottlenecks~\citep{ahuja2021invariance} and domain adversarial training~\citep{}.

{
A related line of research involves symbolic regression for ordinary differential equations (ODEs), where transformer models have shown significant success~\citep{becker2023predicting,d2023odeformer,seifner2024foundationalinferencemodelsdynamical}. While these approaches primarily focus on deriving symbolic expressions for individual trajectories, rather than identifying invariant functions across groups of trajectories, exploring the interplay between these two directions presents an exciting avenue for future research.}

\vspace{-0.2cm}