%%%%%%%% ICML 2025 EXAMPLE LATEX SUBMISSION FILE %%%%%%%%%%%%%%%%%

\documentclass{article}

% Recommended, but optional, packages for figures and better typesetting:

% hyperref makes hyperlinks in the resulting PDF.
% If your build breaks (sometimes temporarily if a hyperlink spans a page)
% please comment out the following usepackage line and replace
% \usepackage{icml2025} with \usepackage[nohyperref]{icml2025} above.


% Attempt to make hyperref and algorithmic work together better:
\newcommand{\theHalgorithm}{\arabic{algorithm}}

% Use the following line for the initial blind version submitted for review:

\usepackage[preprint]{icml2025}
% \usepackage{icml2025} % For ICML anonymous submission
% If accepted, instead use the following line for the camera-ready submission:
% \usepackage[accepted]{icml2025}

\input{math_commands.tex}



% For theorems and such
\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}
\usepackage{url}
\usepackage{booktabs}       % professional-quality tables
% \usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{xcolor}         % colors
\usepackage{amsmath,amssymb}
% \usepackage{amsthm,latexsym,paralist}
\usepackage{amsthm,mathtools}
\usepackage{graphics}
% \usepackage{algorithm}
% \usepackage{algpseudocode}
\usepackage{multirow}
\usepackage{bm}
\usepackage{subcaption}
\usepackage{tcolorbox}
\usepackage{placeins}

\usepackage{xspace}
\urlstyle{same}
\usepackage{soul}
\usepackage{wrapfig}
\usepackage[english]{babel}



% For TOC

% \usepackage[toc,page,header]{appendix}
% \usepackage{minitoc}
\usepackage{etoc}
\etocdepthtag.toc{mtchapter}
\etocsettagdepth{mtchapter}{subsection}
\etocsettagdepth{mtappendix}{none}

\newcommand{\indep}{\perp \!\!\! \perp}

% For TODO note:
\usepackage{todonotes}
\newcommand{\tododone}[2][]
{\todo[color=cyan, #1]{#2}}
\setlength{\marginparwidth}{3cm}

\NewDocumentCommand{\shurui}
{ mO{} }{\textcolor{orange}{\textsuperscript{\textit{Shurui}}\textsf{\textbf{\small[#1]}}}}

\NewDocumentCommand{\xiner}
{ mO{} }{\textcolor{red}{\textsuperscript{\textit{Xiner}}\textsf{\textbf{\small[#1]}}}}

% \NewDocumentCommand{\rebuttal}
% { mO{} }{{\color{blue} #1}}

\NewDocumentCommand{\rebuttal}
{ mO{} }{#1}

% if you use cleveref..
\usepackage[capitalize,noabbrev]{cleveref}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% THEOREMS
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\theoremstyle{plain}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{assumption}[theorem]{Assumption}
\theoremstyle{remark}
\newtheorem{remark}[theorem]{Remark}

% Todonotes is useful during development; simply uncomment the next line
%    and comment out the line below the next line to turn off comments
%\usepackage[disable,textsize=tiny]{todonotes}
% \usepackage[textsize=tiny]{todonotes}

\makeatletter
\DeclareRobustCommand\onedot{\futurelet\@let@token\@onedot}
\def\@onedot{\ifx\@let@token.\else.\null\fi\xspace}

\def\eg{\emph{e.g}\onedot} \def\Eg{\emph{E.g}\onedot}
\def\ie{\emph{i.e}\onedot} \def\Ie{\emph{I.e}\onedot}
\def\cf{\emph{c.f}\onedot} \def\Cf{\emph{C.f}\onedot}
\def\etc{\emph{etc}\onedot} \def\vs{\emph{vs}\onedot}
\def\wrt{w.r.t\onedot} \def\dof{d.o.f\onedot}
\def\etal{\emph{et al}\onedot}
\newcommand{\printfnsymbol}[1]{%
  \textsuperscript{\@fnsymbol{#1}}%
}
\makeatother

\usepackage{pifont}
\newcommand{\cmark}{\ding{51}}  
\newcommand{\xmark}{\ding{55}}  
\newcommand{\dmark}{\textbf{!}}  
\NewDocumentCommand{\shuiwang}
{ mO{} }{\textcolor{blue}{\textsuperscript{\textit{Shuiwang Ji}}\textsf{\textbf{\small[#1]}}}}


% The \icmltitle you define below is probably too long as a header.
% Therefore, a short form for the running title is supplied here:
\icmltitlerunning{Discovering Physics Laws of Dynamical Systems via Invariant Function Learning}

\begin{document}

\twocolumn[
\icmltitle{Discovering Physics Laws of Dynamical Systems via Invariant Function Learning}

% It is OKAY to include author information, even for blind
% submissions: the style file will automatically remove it for you
% unless you've provided the [accepted] option to the icml2025
% package.

% List of affiliations: The first argument should be a (short)
% identifier you will use later to specify author affiliations
% Academic affiliations should list Department, University, City, Region, Country
% Industry affiliations should list Company, City, Region, Country

% You can specify symbols, otherwise they are numbered in order.
% Ideally, you should not use this facility. Affiliations will be numbered
% in order of appearance and this is the preferred way.
\icmlsetsymbol{equal}{*}

\begin{icmlauthorlist}
\icmlauthor{Shurui Gui}{tamu}
\icmlauthor{Xiner Li}{tamu}
\icmlauthor{Shuiwang Ji}{tamu}
%\icmlauthor{}{sch}
%\icmlauthor{}{sch}
\end{icmlauthorlist}

\icmlaffiliation{tamu}{Department of Computer Science 
  \& Engineering,
  Texas A\&M University,
  College Station, USA}
% \icmlaffiliation{comp}{Company Name, Location, Country}
% \icmlaffiliation{sch}{School of ZZZ, Institute of WWW, Location, Country}

\icmlcorrespondingauthor{Shuiwang Ji}{sji@tamu.edu}
% \icmlcorrespondingauthor{Firstname2 Lastname2}{first2.last2@www.uk}

% You may provide any keywords that you
% find helpful for describing your paper; these are used to populate
% the "keywords" metadata in the PDF but will not be shown in the document
\icmlkeywords{Machine Learning, ICML}

\vskip 0.3in
]

% this must go after the closing bracket ] following \twocolumn[ ...

% This command actually creates the footnote in the first column
% listing the affiliations and the copyright notice.
% The command takes one argument, which is text to display at the start of the footnote.
% The \icmlEqualContribution command is standard text for equal contribution.
% Remove it (just {}) if you do not need this facility.

\printAffiliationsAndNotice{}  % leave blank if no need to mention equal contribution
% \printAffiliationsAndNotice{\icmlEqualContribution} % otherwise use the standard text.

\begin{abstract}

We consider learning underlying laws of dynamical systems governed by ordinary differential equations (ODE). A key challenge is how to discover intrinsic dynamics across multiple environments while circumventing environment-specific mechanisms. Unlike prior work, we tackle more complex environments where changes extend beyond function coefficients to entirely different function forms. For example, we demonstrate the discovery of ideal pendulum's natural motion $\alpha^2 \sin{\theta_t}$ by observing pendulum dynamics in different environments, such as the damped environment $\alpha^2 \sin(\theta_t) - \rho \omega_t$ and powered environment $\alpha^2 \sin(\theta_t) + \rho \frac{\omega_t}{\left|\omega_t\right|}$. Here, we formulate this problem as an \emph{invariant function learning} task and propose a new method, known as \textbf{D}isentanglement of \textbf{I}nvariant \textbf{F}unctions (DIF), that is grounded in causal analysis. We propose a causal graph and design an encoder-decoder hypernetwork that explicitly disentangles invariant functions from environment-specific dynamics. The discovery of invariant functions is guaranteed by our information-based principle that enforces the independence between extracted invariant functions and environments. Quantitative comparisons with meta-learning and invariant learning baselines on three ODE systems demonstrate the effectiveness and efficiency of our method. Furthermore, symbolic regression explanation results highlight the ability of our framework to uncover intrinsic laws.

\end{abstract}

% This task aims at isolating and extracting invariant functions that govern underlying mechanisms across multiple observation environments. In this work, we propose the first framework for invariant function learning, grounded in a causal graph and implemented via an encoder-decoder hypernetwork. Our approach disentangles invariant mechanisms from varying environmental conditions and system parameters, facilitating the discovery of true dynamical patterns. Additionally, we design several baseline methods by adapting existing meta-learning and invariant learning techniques to the function space, and evaluate them on three representative ODE systems. Empirical results demonstrate the effectiveness and efficiency of the proposed methods. Further analysis highlights the limitations of current approaches and suggests future directions for advancing invariant function learning.

% Real-world physical systems are often entangled with various environmental factors, posing significant challenges to the discovery of underlying scientific laws. While deep neural networks have been widely applied to forecast dynamical systems, they struggle to capture invariant mechanisms across different environments due to sensitivity to spurious correlations and extraneous information. In this paper, we focus on dynamical systems modeled by ordinary differential equations (ODEs) and address the problem of learning invariant mechanisms. We formulate the task of \emph{invariant function learning} (IFL) to extract true physical mechanisms from ODE trajectories observed under varied conditions. We also propose the first IFL method, \textbf{D}isentanglement of \textbf{I}nvariant \textbf{F}unction (DIF), grounded on causal analysis and implemented via an efficient hypernetwork. Quantitative comparisons with meta-learning and invariant learning baselines on three ODE systems demonstrate the effectiveness and efficiency of our framework. Symbolic regression explanations further highlight our framework's capability to uncover intrinsic physics laws.
\vspace{-0.3cm}
\section{Introduction}
\vspace{-0.2cm}

Deep neural networks~\citep{goodfellow2016deep} have been widely used for predicting dynamical systems~\citep{aussem1999dynamical,singh2012predictive,wang2016data,lusch2018deep,yeo2019deep,giannakis2019data}. Numerous efforts~\citep{Kirchmeyer2022coda,wang2022meta,moulimetaphysica} have focused on modeling dynamical systems by forecasting future states from past observations, often emphasizing rapid adaptation to new systems or improved model architectures. However, an important scenario in scientific discovery has been overlooked, \ie, learning invariant mechanisms, which aims to identify shared motion patterns across dynamical systems observed in multiple environments. This task not only facilitates scientific equation discovery but also holds potential for advancing the understanding and extraction of physical laws from observational data—such as images and videos—where physical laws are highly entangled. As the first step in invariant function learning, this paper focuses on ordinary differential equation (ODE) systems.\footnote{This work introduces a new perspective and focuses only on ODE systems. While the method has the potential to be extended to PDE systems, it is not directly applicable to them due to their continuous nature and the absence of multi-environment datasets designed by domain experts currently. Please refer to Appx.~\ref{app:faq} for frequently asked questions.}

The need for invariant function learning arises because data collected is often observed under varying environments and entangled with multiple factors. For instance, the oscillation of a simple pendulum~\citep{yin2021augmenting} is commonly influenced by air friction; a prey-predator system~\citep{ahmad1993nonautonomous} can be affected by limited resources. These factors significantly hinder deep models from learning the true and invariant dynamics. Instead of capturing invariant dynamical patterns, deep models tend to be sensitive to trivial information and spurious correlations, leading to failures in identifying the true and isolated mechanisms. In light of this challenge, we explore an innovative setting called \emph{invariant function learning}, which aims to extract intrinsic mechanisms from data observed in multiple environments. Unlike prior work, we aim to tackle broader and more complex environments where changes extend beyond function coefficients to entirely different function forms. For example, we target the discovery of ideal pendulum's natural motion $\alpha^2 \sin{\theta_t}$ by observing pendulum dynamics in different environments such as the damped environment $\alpha^2 \sin(\theta_t) - \rho \omega_t$ and powered environment $\alpha^2 \sin(\theta_t) + \rho \frac{\omega_t}{\left|\omega_t\right|}$, as shown in our motivation example~\ref{fig:pendulum_example}. 

\begin{figure*}[t]
    \centering
    \resizebox{0.8\textwidth}{!}{\includegraphics[width=1\textwidth]{figures/example_precise.pdf}}
    % \caption{\textbf{\textcolor{blue}{Multi-environment Pendulum ODE systems}}}
    \caption{ \textbf{Multi-environment Pendulum ODE systems.} In this example, ODEs with different coefficients and function forms are used to extract their corresponding invariant functions (green).}\label{fig:pendulum_example}
    % \vspace{-0.3cm}
\end{figure*}

Invariant function learning presents two key challenges. Firstly, invariant mechanisms are not isolated entities, and being intertwined with varying initial conditions, system parameters, and time makes them difficult to define or disentangle. Secondly, existing invariant learning techniques~\citep{arjovsky2019invariant,lu2021invariant,rosenfeld2020risks, peters2016causal}, which are primarily designed for categorical tasks, do not extend effortlessly to dynamical systems, requiring the design of new invariant principles.
To overcome these challenges, we formulate our invariant function learning framework as a causal graph where functions are parameterized and isolated to be learned and disentangled. Furthermore, we propose an invariant function learning principle and the implementation of an encoder-decoder hypernetwork~\citep{ha2016hypernetworks} to identify the true invariant mechanism. Specifically, our contributions are listed as follows. (1) We introduce a new task, invariant function learning, aimed at scientific discovery. (2) We formulate its framework with causal foundations. (3) We propose an invariant function learning principle to identify true invariant mechanisms and design a method, Disentanglement of Invariant Function (DIF), with hypernetwork implementation. (4) To facilitate comprehensive benchmarking, we propose multi-environment ODE datasets and design several new baselines by adapting existing meta-learning and invariant learning techniques to our function learning framework.

% To solve the first challenges, Learning invariant representation~\citep{arjovsky2019invariant,lu2021invariant,rosenfeld2020risks, peters2016causal,gulrajani2020search} is a long-standing topic. However, different from general invariant learning, invariant function learning is designed for dynamical system forecasting that does not contain any invariant system state representation. This attributes to the intrinsic that the dynamical system states are not static, instead, they change along with time. The lack of invariant state representation poses a question, \textit{what is invariant in dynamical systems?} To answer this question, in this paper, we focus on ODE dynamical systems, where each dynamical system is controlled by a single set of derivative functions, which implies that the invariant factors in dynamical systems are not in real number space but in function space, which naturally requires us to define invariant function representations.

\vspace{-0.1cm}
\section{Invariant Function Learning for Dynamical System}
\vspace{-0.1cm}
\label{background_and_problem_formulation}
In this section, we first provide the background on ODEs, followed by the introduction and formulation of our invariant function learning task, along with the causal analyses that underpin our proposal.
\subsection{Ordinary Differential Equation Dynamical System}

% \begin{figure}[t]
%     \centering
%     \begin{subfigure}[b]{0.64\linewidth}
%         \centering
%         \includegraphics[width=1\textwidth]{figures/example_precise.pdf}
%         \caption{\textbf{\textcolor{blue}{Multi-environment Pendulum ODE systems}}}
%         \label{fig:pendulum_example}
%     \end{subfigure}
%     \hfill
%     \begin{subfigure}[b]{0.33\linewidth}
%         \centering
%         \includegraphics[width=1\textwidth]{figures/causal_graph.pdf}
%         \caption{\textbf{\textcolor{blue}{Structural causal model}}}
%         \label{fig:causal_graph}
%     \end{subfigure}
%     \caption{\textbf{(a) Our motivation example. (b) Causal modeling.} {\color{blue} The causal data generation process includes two phases: function generation and trajectory generation, which correspond to our two learning phases in parentheses, namely, function prediction and forecasting. The gray nodes in the causal graph indicate observable variables.}}
%     \vspace{-0.6cm}
% \end{figure}



% \textcolor{red}{below should be a new subsection or go into 2.1}
We describe a dynamical system using an ordinary differential equation (ODE) as:
\begin{equation}
\vspace{-0.05cm}
    \frac{d\vx_t}{dt} = f(\vx_t),
    \vspace{-0.05cm}
\end{equation}
where $\vx_t\in \mathcal{X} \subseteq \R ^{d}$ includes $d$ hidden states of the system at time $t$. $f \in \mathcal{F}: \mathcal{X} \mapsto T\mathcal{X}$ is the derivative function of the dynamical system mapping the hidden states to their tangent space, where $\mathcal{F}$ is the function space containing functions that describe all dynamical systems with $d$ hidden states.
% \textcolor{red}{do we need to explicitly state ($\mathcal{X}$ and) $T\mathcal{X}$ spaces?} \shurui{I think some reviewers may prefer complete definitions, and it definitely reminds readers about the what is a function $f$.}

Given proper time discretization, we consider $T$ time steps denoted as $t=t_0, t_1, \ldots, t_{T-1}$. The corresponding $T$-length trajectory can be written as a matrix $X=[\vx_{t_0}, \vx_{t_1}, \ldots, \vx_{t_{T-1}}]\in \R^{d\times T}$. Given the system hidden states before a certain time step $T_c\in \mathbb{N}$, denoted as $X_p=X_{:,0:T_c}=[\vx_{t_0}, \ldots, \vx_{t_{T_c - 1}}]\in \R^{d\times T_c}$, the forecasting task aims to predict the future trajectory $X_{:, T_c:T}=[\vx_{t_{T_c}}, \ldots, \vx_{t_{T-1}}]$. 
% We denote $X_p=X_{:,0:T_c}$ as the previously observed trajectory. 
For theoretical analysis, we represent random variables in boldface, \eg, the matrix-valued random variable corresponding to $X$ is denoted as $\rmX$; the function $f$ is a realization of the function variable $\rf$ from a given function space. Full notations are detailed in \rebuttal{Tab.~\ref{tab:notation}.}

% \subsection{Function Learning}

% We formulate the dynamical system forecasting problem as a function learning problem. Specifically, the optimization goal of the forecasting task is to approximate the true distribution $p(\rmX_{:, T_c:T}|\rmX_p)$, which is also equivalent to approximate $p(\rmX|\rmX_p)$. In our function learning framework, we consider two phases: function prediction and forecasting. For function prediction phase, given an observed trajectory $X_p$, our target is to predict a derivative function that can represent the dynamics of the system, \ie, the fitting of $p(\rf|\rmX_p)$. Intuitively, taking Fig.~\ref{fig:pendulum_example} as an example, this phase aims at the reasoning of the function basis $\sin (\theta_t)$, $-\omega_t$, $\frac{\omega_t}{|\omega_t|}$, and the coefficients $\alpha, \rho$. With the predicted derivative function, the forecasting phase feeds it into a numerical integrator with the initial condition $X_0$ for the $X$ forecasting. This two-fold function learning exposes the derivative function prediction directly, which enables us to isolate the function learning process and further disentangle the learning of invariant function.

\subsection{Invariant Function Learning}
\label{sec:invariant_function_learning}

\begin{table*}[t!]
    \centering
    \caption{\textbf{Comparison of environments with previous works in the pendulum ODE system.} We list examples from 2 environments, the pendulum states and coefficients distribution within one environment, and inference time targets. The \textcolor{blue}{blue} factors are changing across different environments. Full function environments are provided in Appx.~\ref{app:datasets}.}
    \resizebox{0.8\textwidth}{!}{
    \begin{tabular}{l|ccccc}
    \toprule
        Type & Environment $e=1$ & Environment $e=2$ & Distribution & Inference \\ %& Adaptation
    \midrule
        Coefficient environment & $f_1 = - \textcolor{blue}{\alpha^2_1} \sin(\theta_t) - \textcolor{blue}{\rho_1} \omega_t$ & $f_2 = - \textcolor{blue}{\alpha^2_2} \sin(\theta_t) - \textcolor{blue}{\rho_2} \omega_t$ & $p(\bm{\theta}_0, \bm{\omega}_0)$ & $f_3 = - \textcolor{blue}{\alpha^2_3} \sin(\theta_t) - \textcolor{blue}{\rho_3} \omega_t$  \\ %& \cmark
        Function environment & $\rf_1 = - \bm{\alpha}^2 \sin(\theta_t) \textcolor{blue}{- \bm{\rho} \omega_t}$ & $\rf_2 = - \bm{\alpha}^2 \sin(\theta_t) \textcolor{blue}{+ \bm{\rho} \frac{\omega_t}{\left|\omega_t\right|}}$ & $p(\bm{\theta}_0, \bm{\omega}_0, \bm{\alpha}, \bm{\rho})$ & $\rf_c = - \bm{\alpha}^2 \sin(\theta_t)$  \\ %& \xmark
    \bottomrule
    \end{tabular}}
    % \vspace{-0.3cm}
    \label{tab:environment_comparison}
\end{table*}

In this paper, we introduce a new task, \textbf{invariant function learning (IFL)}. Specifically, given the prior distribution of trajectories, denoted as $p(\rmX)$, we consider a scenario where trajectories are observed under multiple environments. 
% \textcolor{red}{need to explain/define what environment is before use?} \shurui{Can you add some explanations? You may refer IRM and the Figure 1. I can also add other multi-env meta learning references.}
The trajectories observed in an environment $e \in \gE$ are sampled from the conditional distribution $p(\rmX|\re=e)$. Given a trajectory $X$ from environment $e$, our goal is to discover its invariant function $f_c$, which generates the invariant trajectory $X^c$. $f_c$ and $X^c$ only include the shared mechanisms across all environments, thus capturing the underlying natural laws unaffected by environmental factors. For instance, as illustrated in Fig.~\ref{fig:pendulum_example}, in the case of a pendulum system with varying environmental effects such as frictions or power, the goal is to extract the natural motion of an ideal pendulum when excluding these external influences. A set of specific examples for the task is provided below, more examples are available in Appx.~\ref{app:datasets}.
% \begin{tcolorbox}[boxsep=0mm,left=2.5mm,right=2.5mm]
% $$
% \begin{array}{rll}
%         \mbox{Input - 4 environments:} 
%     &
%         \left\{    
%         \begin{aligned} 
%         & - 1.9^2 \sin \theta_t-0.21 \omega_t \\ 
%         & - 1.1^2 \sin \theta_t+0.40 \frac{\omega_t}{\left|\omega_t\right|} \\
%         & - 2.0^2 \sin \theta_t-0.26 \left|\omega_t\right| \omega_t \\ 
%         & - 1.5^2 \sin \theta_t-0.33 \theta_t \\ 
%         \end{aligned} 
%         \right.
%     &   \Rightarrow 
%         \mbox{Output:}
%         \left\{
%         \begin{aligned} 
%         & - 1.9^2 \sin \theta_t \\
%         & - 1.1^2 \sin \theta_t \\
%         & - 2.0^2 \sin \theta_t \\
%         & - 1.5^2 \sin \theta_t \\
%         \end{aligned}
%         \right.
%     \\
    
%      \midrule
     
%         \mbox{Target invariant function:}
%     &
%         - \bm{\alpha}^2 \sin{\theta_t} \ \ \ \mbox{where}\ \ \  \bm{\alpha} \sim p(\bm{\alpha})
%     & 
% \end{array}
% $$
%  \end{tcolorbox}


% Our environment definition follows invariant learning literatures~\citep{peters2016causal,arjovsky2019invariant}, defining environments as discrete interventions~\citep{pearl2009causality} and model them as a categorical environment variable. 


\textbf{\rebuttal{Function environments.}} The environments in this paper are different from those defined in CoDA~\citep{Kirchmeyer2022coda}, LEAD~\citep{yin2021leads}, and MetaphysiCa~\citep{moulimetaphysica}. As shown in Tab.~\ref{tab:environment_comparison}, the environments/tasks used in previous works, namely, coefficient environments, are defined by the changes on the function coefficients $\alpha$ and $\rho$, { \ie, each environment contains only one function while different environments include functions with different coefficients. In contrast, we consider more complex cases and define environments as the interventions on function forms, \ie, each environment can contain functions with the same function form and different coefficients, while different environments differ in function forms. Specifically, in Tab.~\ref{tab:environment_comparison}, while coefficient environment 1 consists of a single function $f_1$, our function environment 1 includes all the functions in form of $- \bm{\alpha}^2 \sin(\theta_t) - \bm{\rho} \omega_t$ where $\bm{\alpha}\sim p(\bm{\alpha})$, $\bm{\rho}\sim p(\bm{\rho})$. Here, we model these functions as a function random variable $\rf_1$.}
% Our environments cover changes beyond function coefficients and extend to entirely different functional forms. Thus the coefficient-environment in previous works can be regarded as a simplified special case under our setting.

\textbf{\rebuttal{Challenges.}} However, extracting such invariant dynamics presents two significant challenges. Firstly, invariant mechanisms are intertwined with varying initial conditions, system parameters, and time, making them particularly difficult to isolate or define. Secondly, in dynamical systems, state values and their derivatives evolve over time, meaning that there is no single \emph{invariant representation} fixed across time steps. This requires defining invariant factors in a function space, where functions can cover dynamical states. Conventional invariant learning techniques are not directly applicable in this context, as they are typically not designed to capture invariance in function spaces. These challenges necessitate the development of new function representations and the development of a novel invariant learning principle tailored to dynamical systems.



% \begin{table}[t!]
%     \centering
%     \caption{\textbf{Different types of environments.} We compare environment definitions by providing 2-environment examples for each definitions, data distribution among one environment, and the number of functions in each environment. The \underline{underlined} factors are intervened, generating different environments. The function variable $\rf$ indicates any function from a given function space, while $f$ is a realization.}
%     \scriptsize
%     \resizebox{0.7\textwidth}{!}{
%     \begin{tabular}{l|cc}
%     \toprule
%         & Micro-environment & Our environment \\
%     \midrule
%         Environment 1 & $f_1 = \underline{\alpha_1} \sin(\theta_t) - \underline{\rho_1} \omega_t$ & $\rf_1 = \bm{\alpha} \sin(\theta_t) \underline{- \bm{\rho} \omega_t}$ \\
%         Environment 2 & $f_2 = \underline{\alpha_2} \sin(\theta_t) - \underline{\rho_2} \omega_t$ & $\rf_2 = \bm{\alpha} \sin(\theta_t) \underline{+ \bm{\rho} \frac{\omega_t}{\left|\omega_t\right|}}$ \\
%         Distribution & $p(\bm{\theta}_0, \bm{\omega}_0)$ & $p(\bm{\theta}_0, \bm{\omega}_0, \bm{\alpha}, \bm{\rho})$ \\
%         \# functions & 1 & \# samples \\
%         Inference & $f_3 = \underline{\alpha_3} \sin(\theta_t) - \underline{\rho_3} \omega_t$ & $\rf_c = \bm{\alpha} \sin(\theta_t)$ \\
%     \bottomrule
% \end{tabular}}
%     \label{tab:environment_comparison}
% \end{table}


% \shurui{New explanatiaons: }

% Despite of the development of invariant techniques, none of these methods can be applied to dynamical systems because of the failure to answer the question: what is the invariant mechanism. In dynamical systems, everything is changing, the hidden states change along with time, the coefficients change across different trajectories, even part of the function basis can change across environments. The only invariant part is an invariant function basis. However, general invariant learning techniques aim to learn an invariant representation $\Phi = g(X)$ and only use the invariant representation $\Phi$ to make the final prediction $Y=f(\Phi)$. 
% Furthermore, the invariant learning optimization goal is still to obtain single function $f: \gX \mapsto \gY$ with intermediate invariant representation, which, however, does not fit the need of the dynamical system forecasting. Instead of one function, the forecasting task requires the learning of multiple functions.\footnote{Excluding the adaptation based meta-learning methods that requires test-time adaptation in the inference phase.} Therefore, the primary challenge of invariant mechanism extraction is to formulate the dynamical function learning problem.

\subsubsection{Causality-based Definitions} %Invariant Function 
\label{subsec:causal_perspective}

% \shurui{Do you think it is a good idea to have anther explanable exmaple as follows.
% function learning is conceptually to do an inverse reasoning, 
% encoder:
% $[0.13, 0.26, 0.54, 0.34, 0.21] \Rightarrow$ $assignment: \{func: \alpha^2 \rightarrow 0.22, \}$ 
% }
% \xiner{can add as figure}

\begin{figure}[t!]
    \centering
    \resizebox{0.8\linewidth}{!}{\includegraphics[width=1\textwidth]{figures/causal_graph.pdf}}
    % \caption{\textbf{\textcolor{blue}{Structural causal model}}}
    \caption{\textbf{Structural causal model.} { The causal data generation process includes two phases: function generation and trajectory generation, which correspond to our two learning phases in parentheses, namely, function prediction and forecasting. The gray nodes in the causal graph indicate observable variables.}}
    \label{fig:causal_graph}
    \vspace{-0.4cm}
\end{figure}

\begin{figure*}[!t]
% \vspace{-0.5cm}
    \centering
    \includegraphics[width=0.8\linewidth]{figures/framework.pdf}
    \caption{\textbf{DIF framework.}  $\hat{e}$ denotes outputs of the discriminator $g_\phi$ introduced in Sec.~\ref{subsubsec:implementation_of_invariant_function_learning_principle}.}
    \label{fig:method_framework}
    \vspace{-0.2cm}
\end{figure*}

\textbf{First challenge: causal formulation.} In light of the first challenge, we aim to formulate the invariant function learning and the dynamical system forecasting problem from a causal perspective. { As shown in Fig.~\ref{fig:causal_graph}, we formulate the trajectory data generation process as a Structural Causal Model (SCM)~\citep{pearl2009causality}, where $\rc$, $\re$, and $\rmX_0$ are exogenous variables. All endogenous variables, except the function composition step $\rf_c, \rf_e \rightarrow \rf$, are generated with extra random noises to model complex real-world scenarios. Please refer to Appx.~\ref{app:structural_causal_model} for more details.} The optimization goal of the forecasting task is to estimate the true distribution $p(\rmX)$. As can be observed from the causal graph, our function learning framework can be considered as two phases, namely, \rebuttal{\textit{function prediction} and \textit{forecasting}}. For the function prediction phase, similar to inverse problems~\citep{lu2021physics}, given the observed trajectory $\rmX_p$, the target is to reversely infer the invariant derivative function $\rf$ that can represent the dynamics of the system, \ie, fitting $p(\rf|\rmX_p)$. 
% This reverse inference requires the causal mapping $\rf \rightarrow \rmX$ to be injective. 
Intuitively, taking Fig.~\ref{fig:pendulum_example} as an example, this phase aims at the reasoning of the function basis $\sin (\theta_t)$, $-\omega_t$, $\frac{\omega_t}{|\omega_t|}$, and the coefficients $\alpha, \rho$. After obtaining the derivative function $\hat{\rf}$, the forecasting phase feeds it into a numerical integrator with the initial condition $\rmX_0$ for the $\rmX$ forecasting, which can be demonstrated as $p(\rmX|\hat{\rf}, \rmX_0)$. Note that the bold font variables are random variables instead of individual realizations.

% \begin{wrapfigure}{r}{0.4\textwidth}
%     \centering
%     \resizebox{1\linewidth}{!}{\includegraphics[width=1\textwidth]{figures/causal_graph.pdf}}
%     % \caption{\textbf{\textcolor{blue}{Structural causal model}}}
%     \caption{\textbf{Structural causal model.} {\color{blue} The causal data generation process includes two phases: function generation and trajectory generation, which correspond to our two learning phases in parentheses, namely, function prediction and forecasting. The gray nodes in the causal graph indicate observable variables.}}
%     \label{fig:causal_graph}
% \end{wrapfigure}



\rebuttal{\textbf{Second challenge: function disentanglement.}} {To handle the second challenge of extracting invariant mechanisms in dynamical systems, we aim to define invariant representation in the function space, which requires the access to intermediate functions and the disentanglement formulations.
With the two-phase function prediction to forecasting process, we explicitly expose the derivative function, allowing us to isolate the function prediction process and disentangle the learning of invariant functions. In the invariant function learning problem, we decompose the exposed function variable $\rf$ into an invariant function variable $\rf_c$ and an environment-specific function variable $\rf_e$, as in Fig.~\ref{fig:causal_graph}.} These two functions are caused by the exogenous factors $\rc$ and $\re$, respectively. Intuitively, the $\rc$ variable includes the common and invariant mechanism, while $\re$ is the environment variable determined by the observational environment of the trajectory, \eg, the pendulum dynamics can be observed in different mediums (environments), such as air and water. 

Formally, the invariant function learning target is the discovery of $\rf_c$, which eliminates the effect of environments and obtains the invariant mechanism. However, from the d-separation perspective~\citep{pearl2009causality}, since $X_p$ is the descendent of the collider $\rf$ of $\rf_c$ and $\rf_e$, given $X_p$, $\rf_c$ and $\rf_e$ are correlated/biased and not distinguishable, preventing the direct fitting of $\rf_c$ given $X_p$. Therefore, we aim to identify this intermediate hidden variable by characterizing the unique properties of the prior distribution of $p(\rf_c)$. Theoretically, since $\rf$ is the collider between $\rf_c$ and $\re$, $\rf_c$ is expected to be independent of $\re$. In addition, among all the functions that are independent of $\re$, $\rf_c$ should be the most informative with respect to the observed trajectories, which will be proved in Thm.~\ref{thm:invariant_function_learning_principle}. This forms the foundation of invariant function learning.



\section{A method: Disentanglement of Invariant Function}

Following the two-phase function learning framework, we now propose the first method for IFL, \textbf{D}isentanglement of \textbf{I}nvariant \textbf{F}unction (DIF), with hypernetwork-based implementations of the two corresponding networks. For the forecasting network, similar to traditional representation learning tasks, we aim to learn a function $f\in\gF: \R^d \mapsto \R^d$. For the function prediction, however, it requires learning a function that returns a function, $h\in \gH: \R^{d\times T_c} \mapsto \gF$, \ie, learning a hyper-function, which is enabled using a hypernetwork.

\subsection{Hypernetwork Design}\label{subsection:hypernetwork_design}


\rebuttal{\textbf{Function prediction.}} To quantify the objective of the hyper-function, we approximate its output function as a neural network with $m$ parameters. The function space $\gF$ consists of all possible neural networks with $m$ parameters, and a function $f\in \gF$ can be represented as a vector in $\R^m$. Thus this parameterization process introduces a hypernetwork structure~\citep{ha2016hypernetworks} into the implementation, as shown in Fig.~\ref{fig:method_framework}. Note that since our parameterization transfers functions into the real number space, it is now possible to apply invariant learning techniques such as IRM~\citep{arjovsky2019invariant} and VREx~\citep{krueger2021out}, where invariant (function) representations need to be extracted. In the following sections, we use $\gF$ and omit $\R^m$ for simplicity.

% Different to previous dynamical system meta-learning studies where hypernetworks are only used as adaptors, the basic building block of our framework, hypernetwork, is applied directly as the function of functions. 

Practically, since the number of parameters in a network is generally large, we propose to compress the invariant function representations into hidden representations, thus forming an encoder-decoder framework. Specifically, as shown in the Fig.~\ref{fig:method_framework}, the trajectory encoder is a transformer-based network with positional embedding design, denoted as $h_{\theta_\text{enc}}:\R^{d\times T_c} \mapsto \gZ$. Given the hidden representation from the encoder, we further encode an invariant function embedding $\hat{z}_c\in \gZ$ and an environment function embedding $\hat{z}_e \in \gZ$ using two multilayer perceptrons (MLPs), denoted as $h_{\theta_{inv}}$ and $h_{\theta_{env}}$, respectively. Then, aligning with our causal graph as Fig.~\ref{fig:causal_graph}, we combine $\hat{z}_c$ and $\hat{z}_e$ by summing them as the function representation $\hat{z} \in \gZ$, which can be used for full dynamics prediction. 
% This function representation summation is equivalent to feeding the concatenation of the two representations into a linear layer without the bias term. 
Finally, we learn an decoder MLP $h_{\theta_{dec}}$ to decode the function representation into $m$-dimensional neural network parameters, \ie, $\hat{f}_c, \hat{f} \in \gF$. 

To facilitate theoretical analysis, we simplify the notations and denote the function prediction process in the hypernetwork as two functions $\hat{f}=h_\theta(X_p)$ and $\hat{f}_c=h_{\theta_c}(X_p)$, where $h_\theta, h_{\theta_c}: \R^{d\times T_c} \mapsto \gF$; $\theta=\{\theta_{enc}, \theta_{inv}, \theta_{env}, \theta_{dec}\}$; $\theta_c=\{\theta_{enc}, \theta_{inv}, \theta_{dec}\}$. In addition, we slightly abuse the notations of $h_\theta$ and $h_{\theta_c}$ on random variables $\rmX_p$ for simplicity, producing $\hat{\rf}=h_\theta(\rmX_p)$ and $\hat{\rf}_c=h_{\theta_c}(\rmX_p)$, respectively. 
% The hypernetwork learns a hyper-function instead of a function, which can be considered as the generalized meta-learning.\footnote{The target of generalized meta-learning here does not align with the original goal and definition of meta-learning, that is, automatic learning with flexibility for fast adaptations.} 
% For clear optimization introductions in the following sections, when we try to emphasize and expose the trainable parameters $\theta$, we use $h_\theta(\rmX_p)$ instead of $\hat{\rf}$.

\rebuttal{\textbf{Forecasting.} Given the produced neural network function $\hat{f}$, we apply a numerical integrator as our forecastor, a function $g_{int}$ that takes a derivative function $\hat{f}$ and initial states $X_0$ as inputs, to obtain $\hat{X}=g_{int}(\hat{f}, X_0) + \epsilon$ where $\epsilon$ is sampled from a Gaussian noise $\mathcal{N}\left(\mathbf{X} ; 0, \sigma^2 I\right)$ introduced by calculation deviations. This forecasting formulation enables the following probability modeling, where we obtain the forecasting given realizations $X_0$ and $\hat{f}$ as a Gaussian distribution $\mathcal{N}\left(\mathbf{X} ; g_{int}(\hat{f}, X_0), \sigma^2 I\right)$ denoted as $p(\rmX|\hat{f}, X_0)$. Therefore, in probability modeling, $\hat{X}$ is sampled from $p(\rmX|\hat{f}, X_0)$. It is worth noting that unlike in inference time and analyses, it is time-consuming to use numerical integrators during training; therefore, we follow~\cite{moulimetaphysica} and train the model by fitting the derivative $\hat{\frac{d X_t}{dt}}=\hat{f}(X_t)$ with numerical derivatives from the ground-truth $X$ instead. For simplicity, we denote $\hat{f}(\cdot)$ as a neural network based derivative function with parameters $\hat{f}\in \R^m$.} 

\subsection{Discovery of Invariant Function}
With the above hypernetwork design, we can now propose the discovery of the invariant function $\rf_c$. Following the independence and information properties of $\rf_c$ discussed in Sec.~\ref{subsec:causal_perspective}, we achieve invariant function learning with the following theorem, which is our main theoretical result.

{\begin{theorem}[Invariant function learning principle]\label{thm:invariant_function_learning_principle}
    Given the causal graph in Fig.~\ref{fig:causal_graph}, and the predicted function random variable $\hat{\rf}_c=h_{\theta_c}(\rmX_p)$, it follows that the true invariant function random variable $\rf_c$ can be inferred from $h_{\theta^*_c}(\rmX_p)$, where the optimal solution $\theta^*_c$ is obtained through the following optimization:
    \begin{equation}
        \theta^*_c = \argmax_{\theta_c} I(h_{\theta_c}(\rmX_p); \rf|\rmX_0)\ \ s.t.\ \ h_{\theta_c}(\rmX_p) \indep \re,
    \end{equation}
    where $I(\cdot;\cdot)$ is mutual information that measures the information overlap between the predicted invariant function random variable $h_{\theta_c}(\rmX_p)$ and the true full-dynamics function random variable $\rf$.
\end{theorem}}
The proof in Appx.~\ref{proof:invariant_function_learning_principle} shows that the optimal solution is both necessary and sufficient to identify the true invariant function variable available. Therefore, Thm.~\ref{thm:invariant_function_learning_principle} establish guarantees and conditions for the function output $\hat{\rf}_c$ of the hypernetwork to be the invariant function $\rf_c$, fulfilling the goal of the IFL task. 

% \begin{figure}[t]
%     \centering
%     \begin{subfigure}[b]{0.73\linewidth}
%         \includegraphics[width=1\linewidth]{figures/framework.pdf}
%         \caption{\textbf{Invariant function representation learning framework.}}
%         \label{fig:method_framework}
%     \end{subfigure}
%     \begin{subfigure}[b]{0.25\linewidth}
%         \includegraphics[width=1\linewidth]{figures/hypernetwork_trick.pdf}
%         \caption{\textbf{Efficient hypernetwork.}}
%         \label{fig:hypernetwork_trick}
%     \end{subfigure}
% \end{figure}


% Having discovered the invariant function, 


% \begin{proof}
% \begin{equation}
% \begin{aligned}
%     \E_{X\sim P}\left[\log p(\rmX=X|\rmX_p=X_p)\right]& =\iint p(X, X_p) \left[\log p(\rmX=X|\rmX_p=X_p)\right] dX dX_p \\
%     &=\int p(X_p) \int p(X|X_p) \left[\log p(\rmX=X|\rmX_p=X_p)\right] dX dX_p \\
%     &= \int p(X_p) p(g(X_p)|X_p) \left[\log p(\rmX=g(X_p)|\rmX_p=X_p)\right] dX_p \\
%     &= \int p(g(X_p), X_p) \left[\log p(\rmX=g(X_p)|\rmX_p=X_p)\right] dX_p \\
%     &=\E_{X\sim P}\left[\log p(\rmX=g(X_p)|\rmX_p=X_p)\right]
% \end{aligned}
% \end{equation}
% \end{proof}


\subsubsection{Implementation of Invariant Function Learning Principle} \label{subsubsec:implementation_of_invariant_function_learning_principle}
Following Thm.~\ref{thm:invariant_function_learning_principle}, next we introduce the implementation and optimization process of our proposed networks.
We first train the encoder and decoder of our hypernetwork by approximating the trajectory distribution $p(\rmX)$, parameterized as $p(\rmX|h_\theta(\rmX_p), \rmX_0)$, where we apply the cross-entropy minimization. Given that our supervision signals only come from the ground-truth trajectories, we introduce a simple lemma for our optimization processes. The proof is provided in Appx.~\ref{proof:ODE_cross_entropy_minimizaion}.
{\begin{lemma}[ODE cross-entropy minimization\label{lemma:ODE_cross_entropy_minimizaion}]
    Given forecasting model $p(\rmX|h_\theta(\rmX_p), \rmX_0)$, it follows that the cross-entropy minimization between the data distribution $p(\rmX)$ and $p(\rmX|h_\theta(\rmX_p), \rmX_0)$ is equivalent to minimizing mean square error $\min_\theta \E_{X\sim p}\|X - \hat{X}\|^2_2$, where $\hat{X}$ is sampled from $p(\rmX|h_\theta(X_p), X_0)$.
\end{lemma}}
In order to discover invariant functions, we apply the invariant function learning principle, which requires maximizing the conditional mutual information between the predicted function {random variable} $\hat{\rf}_c=h_{\theta_c}(\rmX_p)$ and $\rf$. Based on the derivation of Lemma~\ref{lemma:ODE_cross_entropy_minimizaion}, we have the following proposition. 

{\begin{proposition}[ODE conditional mutual information maximization]
\label{proposition:ODE_conditional_information_maximization}
    Given forecasting model $p(\rmX|h_{\theta_c}(X_p), X_0)$, it follows that the conditional mutual information maximization $\max_{\theta_c} I(h_{\theta_c}(\rmX_p); \rf|\rmX_0)$ is equivalent to minimizing mean square error $\min_{\theta_c} \E_{\rmX\sim p}\|X - \hat{X}^c\|^2_2$, where $\hat{X}^c$ is the predicted trajectory sampled from $p(\rmX|h_{\theta_c}(X_p), X_0)$.
\end{proposition}}
The proof is provided in Appx.~\ref{proof:ODE_conditional_information_maximization}. Lemma~\ref{lemma:ODE_cross_entropy_minimizaion} and Prop.~\ref{proposition:ODE_conditional_information_maximization} essentially transfers the mutual information maximization of Thm.~\ref{thm:invariant_function_learning_principle} into an implementable optimization of mean square error (MSE) loss, enabling the practical use of the invariant function learning principle.
% , which can be deduced as the minimization of the negative log-likelihood (NLL) of $p(\rmX|\rf_c, \rmX_p)$, \ie, $- \E_{X, f_c\sim P} \log p(X|f_c, X_p)$. When using hypernetwork to approximate $f_c$, we have $\hat{f}_c=h_\theta(X_p)$, the approximation of $p(\rmX|\rmX_p)$, \ie, $p(\rmX|\rmX_p)$ as mentioned at the end of the last subsection, can be written as $p(X|h_\theta(X_p), X_p)$ since $p(h_\theta(X_p)|X_p)=1$ is deterministic. Here, $p(X|h_\theta(X_p), X_p)$ is the corresponding parameterized approximation of $p(X|f_c, X_p)$. Therefore, the negative log-likelihood optimization of $p(\rmX|\rmX_p)$ has already included this mutual information maximization. 
In addition, the independence constraint in Thm.~\ref{thm:invariant_function_learning_principle} requires that the extracted functions should not be over-informative or contain biased information from environments $\re$. This independence constraint can be implemented in an adversarial way, where we require the environment prediction $P(\re|\hat{\rf}_c)$ to be as less informative as possible, \ie, minimizing the mutual information of $I(\re;\hat{\rf}_c)$. Thus we introduce an environment discriminator $g_\phi$, a.k.a., $P_\phi(\re|\hat{\rf})$, which aims to distinguish the environment of any function from $\gF$. The hypernetwork is trained adversarially to enforce $\hat{\rf}_c$ as indistinguishable as possible. { The theoretically analysis of this independence training is provided in Appx.~\ref{app:independence_training}.}

% The information maximization can be reduced to the form similar to Eq.~\ref{eq:mse}, \ie, $\E_{\rmX\sim p}[\|X - \hat{X}^c\|^2_2]$, where the deduction process is detailed in App.~\ref{app:mutual_information_maximazation}. 
{ \textbf{Objective.} The overall optimization objective can be obtained with three training strategies. First, the training of the discriminator is conducted on both $\hat{f}_c$ and $\hat{f}_e$ to fully capture environment patterns. Second, we use the corresponding hidden representations of $\hat{f}_c$ and $\hat{f}_e$, $\hat{z}_c$ and $\hat{z}_e$, as the input of the discriminator.
Third, as we mentioned in Sec.\ref{subsection:hypernetwork_design}, during training, we fit derivatives instead of using an integrator.

\vspace{-0.1cm}
\begin{equation}\label{eq:objective}
\resizebox{0.9\linewidth}{!}{$
\begin{aligned}
    &\min_{\theta} \E_{\rmX \sim p}\sum_t\left\|\frac{d X_t}{dt} - \hat{f}(X_t)\right\|^2_2 \\
    & + \lambda_c \cdot \min_{\theta_c} \E_{\rmX \sim p}\sum_t\left\|\frac{d X_t}{dt} - \hat{f}_c(X_t)\right\|^2_2  \\
    &  
    + \lambda_{dis} \cdot \left[\min_{\phi} - \E_{\rmX\sim p} \log g_\phi(\hat{z}_c) + \min_{\phi, \bar{\theta}_{e}} - \E_{\rmX\sim p} \log g_\phi(\hat{z}_e)\right] \\
    & + \lambda_{adv} \cdot \max_{\bar{\theta}_c} - \E_{\rmX\sim p}\log g_\phi(\hat{z}_c)
\end{aligned}$}
\end{equation}
% \begin{equation}\label{eq:discriminator}
%     \min_{\phi} - \E_{\rmX, \re\sim p}\left[\log P_\phi(e | \hat{f}_c) + \log P_\phi(e | \hat{f}_e)\right],
% \end{equation}

% \begin{subequations}
% \begin{equation}\label{eq:invariant_adv}
%     \min_{\theta_c} \E_{\rmX, \re \sim p}\left[\|X - \hat{X}^c\|^2_2 + \log P_\phi(e | \hat{f}_c)\right], 
% \end{equation}
% \begin{equation}\label{eq:discriminator}
%     \min_{\phi} - \E_{\rmX, \re\sim p}\left[\log P_\phi(e | \hat{f}_c)\right],
% \end{equation}
% \end{subequations}
where we denote $\bar{\theta}_e=\{\theta_{enc}, \theta_{env}\}$; $\bar{\theta}_c=\{\theta_{enc}, \theta_{inv}\}$. Please refer to Appx.~\ref{app:training_objectives} for more details.}

% In practice, we apply three additional strategies to stabilize the training process.
% % Firstly, the adversarial training of $\hat{\rf}_c$ will cause the loss of environment information in $\hat{\rf}_c$, leading to the training difficulty of the discriminator; therefore, this discriminator is not only trained on $\hat{\rf}_c$ but also on $\hat{\rf}_e$ with the same hyper-parameter $\lambda_{dis}$, \ie, $\min_{\phi, \theta} - \E_{\rmX, \re\sim P}\left[\log P_\phi(e | \hat{f}_e)\right]$.
% Firstly, the training of discriminator is conducted on both $\hat{f}_c$ and $\hat{f}_e$ to fully capture environment patterns. And the training of $\hat{\rf}_e$ also requires incorporating the optimization in Eq.~\ref{eq:mse}. 
% % Secondly, instead of using the large $\hat{f}_c$ and $\hat{f}_e$ as the input of the discriminator, we input the the corresponding embeddings $\vz_c$ and $\vz_e$. Conceptually, $\hat{f}_c$ and $\vz_c$ contains the same information when the decoder transformation is injective. 
% Second, we use the corresponding hidden representations of $\hat{f}_c$ and $\hat{f}_e$, $\hat{z}_c$ and $\hat{z}_e$, as the input of the discriminator.
% Thirdly, to avoid the use of a numerical or neural integrator which causes long training time, we follow~\cite{moulimetaphysica} to fit derivatives only. 
% % That is, instead of using the inference forecaster $p(X|h_{\theta_c}(X_p), X_0)$, we calculate the derivatives of $X$ using $\hat{f}$ and $\hat{f}_c$, and replace the MSE over trajectory matrices with the MSE over derivative matrices. Note that this modification only eliminates the use of integrator for stability during training and thus does not affect our analysis and optimization goal. 


\textbf{Efficient hypernetwork implementation.} Last but not least, one of the major challenges that limits the usage of hypernetworks is the implementation complexity. \rebuttal{In this work, we propose a reference-based hypernetwork implementation to accelerate the running speed using only PyTorch~\cite{paszke2019pytorch} without re-implementing basic neural networks. The speedup compared to the naïve implementation and the vectorized functional implementation are 16.8x and 2x, respectively. Please refer to Appx.~\ref{app:efficient_hypernetwork_implementation} for implementation and experimental details.}

\vspace{-0.05cm}
\section{Related Work}\label{sec:related_work}
\vspace{-0.05cm}
This work is inspired by the ideas and limitations of previous research in dynamical system forecasting, meta-learning, and invariant learning.


Deep learning models are widely applied in many physical applications~\citep{lusch2018deep,yeo2019deep,kochkov2021machine,chen2018neural} including partial differential equations (PDEs) with the focus on the multi-scale~\citep{li2020fourier,stachenfeld2021learned}, multi-resolution~\citep{kochkov2021machine,wu2022learning}, and long-term stability~\citep{li2021learning,lippe2023pde} issues. 
Operator learning and neural operators~\citep{gupta2021multiwavelet,kovachki2023neural} are popular for PDE estimations. Although the ODE dynamical system does not contain the multi-scale problem that Fourier neural operator~\citep{kovachki2023neural} tried to solve, our framework can be considered as a kind of operator learning. 
% In this case, regarding the discrete ODE system states as PDE functions, our framework decomposes the function-to-function operator learning into two stages, \ie, a function to derivative function learning, and a derivative function to function forecasting. This separation not only facilitates flexible forecasting but also enables the isolation of function representations, laying the foundation for discovering invariant mechanisms. It is worth noting that, different to our work, these approaches typically emphasize fitting specific dynamical systems without attempting to uncover invariant mechanisms. 



% Physics-informed machine learning methods are proposed to incorporate physical inductive bias into the fitting of neural networks in order to avoid the prediction of physically implausible results, which are often related to symbolic regression. Though it is important direction, it is not related to this paper.
Meta-learning methods~\citep{finn2017model,rusu2018meta,li2017meta,zintgraf2019fast,perez2018film} aim to learn meta-parameters that can be used across multiple tasks, where the meta parameters are generally learned to make rapid adaptations. In previous meta-learning studies on dynamical systems~\citep{Kirchmeyer2022coda,wang2022meta, yin2021leads}, the objective was to find a meta-function that could quickly adapt to multiple new systems, where hypernetworks are only employed as low-rank adaptors for new dynamical system trajectories, similar to the idea of LoRA~\citep{hu2021lora}. Our work differs from such meta-learning approaches in two key ways. First, we focus on discovering invariant functions rather than quickly adaptable ones. Second, while meta-learning methods seek to learn a single meta-function, our framework learns multiple functions, represented by an invariant function random variable $\rf_c$. This distinction stems from our more complex environment definition, detailed in Sec.~\ref{sec:invariant_function_learning}. From another aspect, learning an invariant function distribution instead of a single function can be considered as generalized meta-learning with an invariant function learning goal. 


Current invariant learning methods~\citep{arjovsky2019invariant,lu2021invariant,rosenfeld2020risks,krueger2021out,sagawa2019distributionally} follow the framework of invariant risk minimization (IRM)~\citep{arjovsky2019invariant}, which was inspired by invariant causal predictor~\citep{peters2016causal}. This invariant learning framework aims to learn a hidden invariant representation that generalizes across multiple environments, ensuring out-of-distribution performance. However, this approach cannot work on dynamical forecast tasks due to the lack of invariant function definition and the violation of the categorical data assumption. To be more specific, first, invariant functions cannot be naturally defined in the real number vector space. Second, invariant learning commonly assumes the prediction results are categorical, where a single invariant representation can fully determine the corresponding label. However, this assumption is violated in dynamical system forecasting, where the invariant mechanism is only partially responsible for the output. In this case, the IRM principle can not hold even when the invariant function ground truth is provided. To address the issues, we introduce the causal assumption (see Fig.~\ref{fig:causal_graph}) that defines the invariant function space, and propose the corresponding invariant function learning principle and implementation.
% inspired by the philosophy of information bottlenecks~\citep{ahuja2021invariance} and domain adversarial training~\citep{}.

{
A related line of research involves symbolic regression for ordinary differential equations (ODEs), where transformer models have shown significant success~\citep{becker2023predicting,d2023odeformer,seifner2024foundationalinferencemodelsdynamical}. While these approaches primarily focus on deriving symbolic expressions for individual trajectories, rather than identifying invariant functions across groups of trajectories, exploring the interplay between these two directions presents an exciting avenue for future research.}

\vspace{-0.2cm}
\section{Experiments}
\label{sec:experiments}
\vspace{-0.1cm}

We conduct experiments to address the following research questions. \textbf{RQ1}: Are existing meta-learning and invariant learning techniques effective for extracting invariant functions? \textbf{RQ2}: Can the proposed invariant function learning principle outperform baseline techniques? \textbf{RQ3}: How do the full functions $\rf$ and the invariant functions $\rf_c$ differ in performance? (See Appx.~\ref{subsec:full_vs_inv}) \textbf{RQ4}: Are the extracted invariant functions explainable and aligned with the true invariant mechanisms? (See Appx.~\ref{subsec:symbolic_regression_explanation}) \textbf{RQ5}: How will performance change given different lengths of inputs and types of environments? (See Appx.~\ref{app:input_length_and_environment_analysis}) \textbf{RQ6}: Is the proposed hypernetwork implementation more efficient than previous implementations? (See Appx.~\ref{app:efficient_hypernetwork_implementation})

\subsection{Datasets}

In our experiments, we introduce three multi-environment (ME) datasets, ME-Pendulum, ME-Lotka-Volterra, and ME-SIREpidemic. These three datasets are generated by simulators modified from the DampedPendulum~\citep{yin2021augmenting}, Lotka-Volterra~\citep{ahmad1993nonautonomous}, and SIREpidemic~\citep{wang2021bridging}. Specifically, each of the dataset's training sets includes four environments with 200 samples for each environment. Specifically, each environment corresponds to one specific environmental effect. ME-Pendulum contains three types of friction and one effect with external energy. ME-Lotka-Volterra modified the common predatory relationship into four modified relationships, \eg, adding resource limits. ME-SIREpidemic produces four conceptual epidemiology models with the same susceptible population to infected population relationship. In addition to the training set, we generate 200 samples with 50 samples for each environment as an in-distribution test set. Please refer to Appx.~\ref{app:datasets} for more details.

\subsection{Experimental Setup}

To quantitatively evaluate the invariant function extraction performance, we need to remove the environment-related effects to generate invariant trajectories $X^c$ as the invariant function ground-truth, \eg, we simulate new data by eliminating $-\rho \omega_t$ from $-\alpha^2 \sin (\theta_t) - \rho \omega_t$ in the ME-Pendulum dataset (Fig.~\ref{fig:pendulum_example}). To be more specific, a generated invariant trajectory $X^c$, aligning the causal graph, has the same system parameters as the corresponding biased trajectory $X$ for the invariant part controlled by $\rc$, \ie, they have the same $\alpha$ in the pendulum example. This invariant trajectory generation is being done on the in-distribution test set so that each trajectory $X$ in this test set has its special corresponding invariant trajectory ground truth $X^c$.

This test set design enables us to mimic the situation of scientific discoveries, where we only observe environment-biased data but are required to find and evaluate invariant function candidates. Specifically, given the biased $X_p$, the hypernetwork is supposed to predict the corresponding invariant derivative function $\hat{f}_c$. Then, with a numeral integrator, the output of this invariant forecaster $\hat{X}^c$ will be evaluated by comparing with the corresponding invariant trajectory ground truth $X^c$ using normalized root mean square error (NRMSE). 

% Note that if we input $X^c_p$ into the hypernetwork, the hypernetwork's generalization ability rather than the invariant function learning ability is evaluated, which also indicates an undesired test data leaking issue for the invariant function learning task. 
% the hypernetwork also need to output both $\hat{f}_c$ and $\hat{f}$ instead of $\hat{f}_c$ only.

\subsection{Proposed Meta-learning and Invariant Learning Baselines}

General dynamical system forecasting is different from invariant function learning significantly, where they focus on how to adapt to new trajectories, which commonly requires further optimization, \eg, test-time adaptation~\citep{moulimetaphysica} or adaptations with meta information~\citep{wang2022meta,Kirchmeyer2022coda}. Unfortunately, under our scientific discovery setting, there is no extra information provided at test time, making them inapplicable to this setting. Therefore, we construct 4 new baseline settings by transplanting the techniques of previous meta-learning and invariant learning to our proposed framework detailed in Appx.~\ref{app:baselines}.

We first adopt the meta-learning baseline MAML~\citep{finn2017model}, where we use its learned meta-parameters for invariant learning to evaluate whether the fastest adapted parameter is the invariant function parameter. Our second meta-learning baseline is CoDA~\citep{Kirchmeyer2022coda}, where we replace its hypernetwork decoder with the full encoder-decoder hypernetwork in our framework to fit in our task. Aligning with the original CoDA paper, we set the dimension of the hidden representation to be 2. Similar to MAML, we eliminate the adaptation part and use only the learned meta-parameter for invariant state prediction.

For invariant learning baselines, we adopted the two most typical techniques, IRM~\citep{ahuja2021invariance} and VREx~\citep{krueger2021out}. These two techniques are applied to the proposed framework, where IRM stands for the most typical definition of invariant learning, while VREx stands for the distributionally robust optimization baseline, which can be considered as the generalization of GroupDRO~\citep{sagawa2019distributionally}.

\subsection{Quantitative Results}

\begin{figure*}[t]
% \vspace{-0.3cm}
    \centering
    \resizebox{0.8\linewidth}{!}{
    $
    \begin{array}{ccc}
         \includegraphics[width=0.5\linewidth]{figures/pendulum.png} &  
         \includegraphics[width=0.5\linewidth]{figures/LV.png} & 
         \includegraphics[width=0.5\linewidth]{figures/SIREpidemic.png}
    \end{array}
    $}
    \caption{\textbf{Invariant trajectory prediction errors} on 5 methods under 3 multi-environment ODE systems. For each method, we provide model candidates with 80+ random hyper-parameter selections in their searching spaces, \ie, more than 1200 models in the figure.}
    \label{fig:quantitative_results}
\end{figure*}

\begin{figure*}[t]
    \centering
    \begin{subfigure}[b]{0.4\linewidth}
        \includegraphics[width=1\linewidth]{figures/result_combine_pendulum.png}
        \caption{$X$ predictions using $\hat{f}$}
        \label{fig:results_combine_me_pendulum}
    \end{subfigure}
    \begin{subfigure}[b]{0.4\linewidth}
        \includegraphics[width=1\linewidth]{figures/result_inv_pendulum.png}
        \caption{$X^c$ predictions using $\hat{f}_c$}
        \label{fig:results_inv_me_pendulum}
    \end{subfigure}
    \caption{\textbf{Visualization of trajectory predictions on ME-Pendulum}}
    \label{fig:results_me_pendulum}
    \vspace{-0.5cm}
\end{figure*}


Similar to other scientific discovery tasks, such as drug discovery, constructing a proper validation set is challenging. Instead, with only observational data available, we generate invariant function candidates that can be further validated in real experimental settings, \eg, through the introduction of interventions~\citep{pearl2009causality}.

To quantitatively compare these methods, we provide the corresponding hyper-parameter search spaces for each technique in Appx.~\ref{app:experimental_details} and plot the results of random hyper-parameter sampling as distributions using Boxen plots. As shown in Fig.~\ref{fig:quantitative_results}, we compare the quality of the invariant function candidates based on their median, best result, and quantiles. Specifically, the median performance of our proposed method surpasses the middle candidates of all other approaches. The performance gaps are particularly notable on the ME-Pendulum and ME-SIR-Epidemic datasets. For example, on ME-Pendulum, over 75\% of our method’s candidates outperform the best results of MAML and CoDA, and more than 93.75\% candidates of IRM and VREx. On ME-Lotka-Volterra, the median of our candidates still outperforms nearly all candidates from other methods. 
% Furthermore, our best results significantly outperform the best results of competing approaches, demonstrating the superior capability of our method in extracting invariant functions (RQ2). 
In addition, as shown in the visualizations on ME-Pendulum~\ref{fig:results_me_pendulum}, our learned invariant function $\hat{f}_c$ eliminates environmental resistances from the original trajectory (Fig.~\ref{fig:results_combine_me_pendulum}) and obtain a simple pendulum motion without attenuation (Fig.~\ref{fig:results_inv_me_pendulum}). Both quantitative and visualization results demonstrate the superior capability of our method in extracting invariant functions (RQ2).



To address the first research question (RQ1), we observe that the invariant learning techniques, IRM and VREx, are generally more stable than the meta-learning baselines. Although IRM and VREx do not surpass MAML on ME-Lotka-Volterra, they outperform MAML on 2 out of 3 datasets and are consistently better than CoDA. However, when compared to our proposed method, the best function candidates from these invariant learning techniques are suboptimal. This confirms that the general invariant learning principles fall short in the context of invariant function extraction, aligning with the discussions in Sec.~\ref{sec:related_work}.

% There are actually two reasons behind. First, the true invariant $\rf_c$ might not be discovered given the data generation process only provide limited distributions and four environments. In fact, it is infeasible to generate data that can cover the whole distribution. Second, the PySR is not designed specifically for the challenges that may be introduced by learning multiple functions where all these variables are correlated and interacted. Therefore, we useinformation might not be However, for this symbolic regression phase, it is possible to inject strong inductive bias similar to the effect of physical-information machine learning, which can be further explored in the future.



% \subsection{Invariant Function Extraction Trade-off: Conditional Mutual Information}

% Randomly selecting possible invariant function candidates can be ineffective, fortunately, our constrained mutual information minimization optimization process can be considered as an information bottleneck. That means we can control the information $\rf_c$ left about $\rmX$. This controllability is important for generalizing to the situations that do not perfectly obey the theory because inevitable non-perfect data sampling and training process. When the information bottleneck is too narrow, \ie, low $I(\rf_c;\rmX)$, the extracted $\hat{f}_c$ be a subset of the ground truth. Therefore, selecting candidates with different information bottlenecks can be a better strategy. For different datasets, the perfect information bottlenecks are different.

% \subsection{Ablation Study}

% In this ablation study, we validate the importance of $F$ and $F_E$ network branches. Specifically, these two branches are used to facilitate the training of the hypernetwork decoder and the environment discriminator. Intuitively, since $\rf_c$ cannot produce perfect $\rmX$, the training of hypernetwork decoder can be problematic, furthermore, capturing all information including the environment information aligns with the rich feature learning principle, beneficial to the training of hypernetwork encoder. Notice that during training, the distribution of $p(\hat{f}_c)$ is not stable, which affects the training of discriminator, therefore, input $\hat{f}_e$ to the discriminator can help to train the discriminator stably.

% As shown in Tab.~\ref{tab:ablation_study}, we provide results without using $F$ and $F_E$, where we can see the significant performance drop. In addition, we further test the $F$ on the invariant states, which demonstrates the non-invariant baseline.


\vspace{-0.1cm}
\section{Limitations}\label{sec:limitations}
\vspace{-0.1cm}

While this work provides a foundation for invariant function learning in dynamical systems, several limitations and opportunities for future exploration remain. These include extending the framework to more complex entanglements, exploring applications in PDE systems and developing comprehensive benchmarks. Additionally, broader applications such as generalizable physics learning and foundational model development represent exciting directions for further research. Please refer to Q5 in Appx.~\ref{app:limitations_and_future_work} for more discussions.

\vspace{-0.1cm}
\section{Conclusion}
\vspace{-0.1cm}

In this work, we target addressing the challenge of the invariant mechanism discovery in ODE dynamical systems by extending invariant learning into function spaces. We introduce a new task, invariant function learning, which aims to extract the invariant dynamics across all environments with different environment-specific function forms. We design a causal analysis based disentanglement framework DIF to expose the underlying invariant functions. Additionally, we propose an invariant function learning principle with theoretical guarantees to optimize the framework and ensure effective invariant function discovery. Our experiments, including invariant trajectory validations, visualizations, ablation studies, and symbolic regression analyses, demonstrate the effectiveness of our method. Finally, as discussed in Sec.~\ref{sec:limitations}, the introduced invariant function learning task has wide application scenarios and many challenges remain to be addressed. We expect that our work will shed light on numerous future explorations in this field.

% In an attempt to encourage standardized notation, we have included the
% notation file from the textbook, \textit{Deep Learning}
% \citep{goodfellow2016deep} available at
% \url{https://github.com/goodfeli/dlbook_notation/}.  Use of this style
% is not required and can be disabled by commenting out
% \texttt{math\_commands.tex}.


% \centerline{\bf Numbers and Arrays}
% \bgroup
% \def\arraystretch{1.5}
% \begin{tabular}{p{1in}p{3.25in}}
% $\displaystyle a$ & A scalar (integer or real)\\
% $\displaystyle \va$ & A vector\\
% $\displaystyle \mA$ & A matrix\\
% $\displaystyle \tA$ & A tensor\\
% $\displaystyle \mI_n$ & Identity matrix with $n$ rows and $n$ columns\\
% $\displaystyle \mI$ & Identity matrix with dimensionality implied by context\\
% $\displaystyle \ve^{(i)}$ & Standard basis vector $[0,\dots,0,1,0,\dots,0]$ with a 1 at position $i$\\
% $\displaystyle \text{diag}(\va)$ & A square, diagonal matrix with diagonal entries given by $\va$\\
% $\displaystyle \ra$ & A scalar random variable\\
% $\displaystyle \rva$ & A vector-valued random variable\\
% $\displaystyle \rmA$ & A matrix-valued random variable\\
% \end{tabular}
% \egroup
% \vspace{0.25cm}

% \centerline{\bf Sets and Graphs}
% \bgroup
% \def\arraystretch{1.5}

% \begin{tabular}{p{1.25in}p{3.25in}}
% $\displaystyle \sA$ & A set\\
% $\displaystyle \R$ & The set of real numbers \\
% $\displaystyle \{0, 1\}$ & The set containing 0 and 1 \\
% $\displaystyle \{0, 1, \dots, n \}$ & The set of all integers between $0$ and $n$\\
% $\displaystyle [a, b]$ & The real interval including $a$ and $b$\\
% $\displaystyle (a, b]$ & The real interval excluding $a$ but including $b$\\
% $\displaystyle \sA \backslash \sB$ & Set subtraction, i.e., the set containing the elements of $\sA$ that are not in $\sB$\\
% $\displaystyle \gG$ & A graph\\
% $\displaystyle \parents_\gG(\ervx_i)$ & The parents of $\ervx_i$ in $\gG$
% \end{tabular}
% \vspace{0.25cm}


% \centerline{\bf Indexing}
% \bgroup
% \def\arraystretch{1.5}

% \begin{tabular}{p{1.25in}p{3.25in}}
% $\displaystyle \eva_i$ & Element $i$ of vector $\va$, with indexing starting at 1 \\
% $\displaystyle \eva_{-i}$ & All elements of vector $\va$ except for element $i$ \\
% $\displaystyle \emA_{i,j}$ & Element $i, j$ of matrix $\mA$ \\
% $\displaystyle \mA_{i, :}$ & Row $i$ of matrix $\mA$ \\
% $\displaystyle \mA_{:, i}$ & Column $i$ of matrix $\mA$ \\
% $\displaystyle \etA_{i, j, k}$ & Element $(i, j, k)$ of a 3-D tensor $\tA$\\
% $\displaystyle \tA_{:, :, i}$ & 2-D slice of a 3-D tensor\\
% $\displaystyle \erva_i$ & Element $i$ of the random vector $\rva$ \\
% \end{tabular}
% \egroup
% \vspace{0.25cm}


% \centerline{\bf Calculus}
% \bgroup
% \def\arraystretch{1.5}
% \begin{tabular}{p{1.25in}p{3.25in}}
% % NOTE: the [2ex] on the next line adds extra height to that row of the table.
% % Without that command, the fraction on the first line is too tall and collides
% % with the fraction on the second line.
% $\displaystyle\frac{d y} {d x}$ & Derivative of $y$ with respect to $x$\\ [2ex]
% $\displaystyle \frac{\partial y} {\partial x} $ & Partial derivative of $y$ with respect to $x$ \\
% $\displaystyle \nabla_\vx y $ & Gradient of $y$ with respect to $\vx$ \\
% $\displaystyle \nabla_\mX y $ & Matrix derivatives of $y$ with respect to $\mX$ \\
% $\displaystyle \nabla_\tX y $ & Tensor containing derivatives of $y$ with respect to $\tX$ \\
% $\displaystyle \frac{\partial f}{\partial \vx} $ & Jacobian matrix $\mJ \in \R^{m\times n}$ of $f: \R^n \rightarrow \R^m$\\
% $\displaystyle \nabla_\vx^2 f(\vx)\text{ or }\mH( f)(\vx)$ & The Hessian matrix of $f$ at input point $\vx$\\
% $\displaystyle \int f(\vx) d\vx $ & Definite integral over the entire domain of $\vx$ \\
% $\displaystyle \int_\sS f(\vx) d\vx$ & Definite integral with respect to $\vx$ over the set $\sS$ \\
% \end{tabular}
% \egroup
% \vspace{0.25cm}

% \centerline{\bf Probability and Information Theory}
% \bgroup
% \def\arraystretch{1.5}
% \begin{tabular}{p{1.25in}p{3.25in}}
% $\displaystyle p(\ra)$ & A probability distribution over a discrete variable\\
% $\displaystyle p(\ra)$ & A probability distribution over a continuous variable, or over
% a variable whose type has not been specified\\
% $\displaystyle \ra \sim P$ & Random variable $\ra$ has distribution $P$\\% so thing on left of \sim should always be a random variable, with name beginning with \r
% $\displaystyle  \E_{\rx\sim P} [ f(x) ]\text{ or } \E f(x)$ & Expectation of $f(x)$ with respect to $p(\rx)$ \\
% $\displaystyle \Var(f(x)) $ &  Variance of $f(x)$ under $p(\rx)$ \\
% $\displaystyle \Cov(f(x),g(x)) $ & Covariance of $f(x)$ and $g(x)$ under $p(\rx)$\\
% $\displaystyle H(\rx) $ & Shannon entropy of the random variable $\rx$\\
% $\displaystyle \KL ( P \Vert Q ) $ & Kullback-Leibler divergence of P and Q \\
% $\displaystyle \mathcal{N} ( \vx ; \vmu , \mSigma)$ & Gaussian distribution %
% over $\vx$ with mean $\vmu$ and covariance $\mSigma$ \\
% \end{tabular}
% \egroup
% \vspace{0.25cm}

% \centerline{\bf Functions}
% \bgroup
% \def\arraystretch{1.5}
% \begin{tabular}{p{1.25in}p{3.25in}}
% $\displaystyle f: \sA \rightarrow \sB$ & The function $f$ with domain $\sA$ and range $\sB$\\
% $\displaystyle f \circ g $ & Composition of the functions $f$ and $g$ \\
%   $\displaystyle f(\vx ; \vtheta) $ & A function of $\vx$ parametrized by $\vtheta$.
%   (Sometimes we write $f(\vx)$ and omit the argument $\vtheta$ to lighten notation) \\
% $\displaystyle \log x$ & Natural logarithm of $x$ \\
% $\displaystyle \sigma(x)$ & Logistic sigmoid, $\displaystyle \frac{1} {1 + \exp(-x)}$ \\
% $\displaystyle \zeta(x)$ & Softplus, $\log(1 + \exp(x))$ \\
% $\displaystyle || \vx ||_p $ & $\normlp$ norm of $\vx$ \\
% $\displaystyle || \vx || $ & $\normltwo$ norm of $\vx$ \\
% $\displaystyle x^+$ & Positive part of $x$, i.e., $\max(0,x)$\\
% $\displaystyle \1_\mathrm{condition}$ & is 1 if the condition is true, 0 otherwise\\
% \end{tabular}
% \egroup
% \vspace{0.25cm}



% \section{Final instructions}
% Do not change any aspects of the formatting parameters in the style files.
% In particular, do not modify the width or length of the rectangle the text
% should fit into, and do not change font sizes (except perhaps in the
% \textsc{References} section; see below). Please note that pages should be
% numbered.

% \section{Preparing PostScript or PDF files}

% Please prepare PostScript or PDF files with paper size ``US Letter'', and
% not, for example, ``A4''. The -t
% letter option on dvips will produce US Letter files.

% Consider directly generating PDF files using \verb+pdflatex+
% (especially if you are a MiKTeX user).
% PDF figures must be substituted for EPS figures, however.

% Otherwise, please generate your PostScript and PDF files with the following commands:
% \begin{verbatim}
% dvips mypaper.dvi -t letter -Ppdf -G0 -o mypaper.ps
% ps2pdf mypaper.ps mypaper.pdf
% \end{verbatim}

% \subsection{Margins in LaTeX}

% Most of the margin problems come from figures positioned by hand using
% \verb+\special+ or other commands. We suggest using the command
% \verb+\includegraphics+
% from the graphicx package. Always specify the figure width as a multiple of
% the line width as in the example below using .eps graphics
% \begin{verbatim}
%    \usepackage[dvips]{graphicx} ...
%    \includegraphics[width=0.8\linewidth]{myfile.eps}
% \end{verbatim}
% or % Apr 2009 addition
% \begin{verbatim}
%    \usepackage[pdftex]{graphicx} ...
%    \includegraphics[width=0.8\linewidth]{myfile.pdf}
% \end{verbatim}
% for .pdf graphics.
% See section~4.4 in the graphics bundle documentation (\url{http://www.ctan.org/tex-archive/macros/latex/required/graphics/grfguide.ps})

% A number of width problems arise when LaTeX cannot properly hyphenate a
% line. Please give LaTeX hyphenation hints using the \verb+\-+ command.

\clearpage

% \subsubsection*{Author Contributions}
% If you'd like to, you may include  a section for author contributions as is done
% in many journals. This is optional and at the discretion of the authors.

% \subsubsection*{Acknowledgments}
% Use unnumbered third level headings for the acknowledgments. All
% acknowledgments, including those to funding agencies, go at the end of the paper.

% \subsubsection*{Reproducibility Statement}

% To make sure the reproducibility of this work, for our theoretical results, all assumptions and proofs are included in Appx.~\ref{app:invariant_function_learning_function}. For the datasets, the system parameters, simulator expressions, and datasets visualizations are provided in Appx.~\ref{app:datasets}. For the model, we provide full details including all the training setup, architecture, objectives, and hyper-parameter searching spaces in Appx.~\ref{app:experimental_details}. The finalized code will be released upon acceptance.

\section*{Acknowledgments}
This work was supported in part by National Science Foundation under grant CNS-2328395 and ARPA-H under grant 1AY1AX000053.

%\section*{Impact Statement}

%This paper presents work whose goal is to advance the field
%of Machine Learning. There are many potential societal
%consequences of our work, none which we feel must be
%specifically highlighted here.

\bibliography{reference}
\bibliographystyle{icml2025}

\clearpage


\appendix
\onecolumn

{
\centering
{\LARGE Appendix of Discovering Physics Laws of Dynamical Systems via Invariant Function Learning\par}
}

\etocdepthtag.toc{mtappendix}
\etocsettagdepth{mtchapter}{none}
\etocsettagdepth{mtappendix}{subsubsection}
\tableofcontents

\clearpage

\FloatBarrier  % prevent floats from being moved past this point

\section{Notations}\label{app:notations}

For the ease of reading, we providing a table including major notations below for reference.


\begin{table}[h]
    \centering
    \caption{\textbf{Notation table}}
    \label{tab:notation}
    \begin{tabular}{p{1.25in}p{3.25in}}
    \toprule[1pt]
    \textbf{Notation} & \textbf{Explanation} \\
    \midrule
    $\displaystyle \vx_t$ & Dynamical system states at time $t$ \\
    $\displaystyle \gX$ & Dynamical system state space \\
    $\displaystyle T\gX$ & Tangent state space \\
    $\displaystyle \R$ & Real number space \\
    $\displaystyle d$ & The number of state \\
    $\displaystyle t$ & Time \\
    $\displaystyle T_c$ & The first future time step \\
    $\displaystyle X$ & A trajectory; A state matrix with T-step states \\
    $\displaystyle X_p$ & Past states before time step $T_c$ \\
    $\displaystyle X^c$ & An invariant trajectory\\
    $\displaystyle \hat{X}, \hat{X}_p, \hat{X}^c$  & The predicted trajectory of $X$, $X_p$, and $X^c$ (by a model) \\
    $\displaystyle \rmX, \rmX_p, \rmX^c$ & The matrix-valued random variable of $X$, $X_p$, and $X^c$ \\
    $\displaystyle \hat{\rmX}, \hat{\rmX}_p, \hat{\rmX}^c$  & The predicted matrix-valued random variable of $X$, $X_p$, and $X^c$ \\
    $\displaystyle f$ & A derivative function (of an underlying dynamical system) \\
    $\displaystyle f_c$ & An invariant derivative function\\
    $\displaystyle f_e$ & An environment derivative function \\
    $\displaystyle \hat{f}, \hat{f}_c, \hat{f}_e$ & The predicted functions of $f$, $f_c$, and $f_e$ (by a model) \\
    $\displaystyle \rf, \rf_c, \rf_e$ & The derivative function random variable of $f$, $f_c$, and $f_e$ \\
    $\displaystyle \hat{\rf}, \hat{\rf}_c, \hat{\rf}_e$ & The predicted derivative function random variable of $f$, $f_c$, and $f_e$ \\
    $\displaystyle \gF$ & Function space/Functional vector space \\
    $\displaystyle \hat{z}/\hat{z}_c/\hat{z}_e$ & A predicted full/invariant/environment hidden function representation \\
    $\displaystyle \gZ$ & Hidden function space/Hidden functional vector space \\
    $\displaystyle h$ & A hypernetwork \\
    $\displaystyle \gH$ & Hypernetwork function space \\
    $\displaystyle p(\cdot)$ & A probability distribution over a random variable \\
    $\displaystyle I(\cdot;\cdot)$ & Mutual information between random variables \\
    $\displaystyle H(\rmX) $ & Shannon entropy of the matrix-valued random variable $\rmX$\\
    $\displaystyle  \E_{\rmX\sim P} [ f(X) ]\text{ or } \E f(X)$ & Expectation of $f(X)$ with respect to $p(\rmX)$ \\
    $\displaystyle \rmX \sim P$ & Random variable $\rmX$ has distribution $P$\\% so thing on left of \sim should always be a random variable, with name beginning with \r
    $\displaystyle X \sim p(\rmX)$ & $X$ is sampled from distribution $p(\rmX)$ \\
    $\displaystyle  \E_{\rmX\sim P} [ f(\rmX) ]\text{ or } \E f(\rmX)$ & Expectation of $f(\rmX)$ with respect to $p(\rmX)$ \\
    $\displaystyle \{\cdot\}$ & An assignment/A substitution rule \\
    $\displaystyle \{\alpha \rightarrow \}$ & A symbol without an assigned value \\
    $\displaystyle \{\alpha \rightarrow a\}$ & A symbol with an assigned value $a$\\
    % $\displaystyle \mathcal{N} ( \vx ; \vmu , \mSigma)$ & Gaussian distribution %
    % over $\vx$ with mean $\vmu$ and covariance $\mSigma$ \\
    \bottomrule[1pt]
    \end{tabular}
\end{table}
% \centerline{\bf Probability and Information Theory}
% \bgroup
% \def\arraystretch{1.5}

% \egroup
% \vspace{0.25cm}

\clearpage
\section{FAQ \& Discussions}\label{app:faq}
\label{app:limitations_and_future_work}

To facilitate the reviewing process, we summarize the answers to the questions that arose during the discussion of an earlier version of this paper.

The major updates of this version are reorganized theoretical studies, causal graph details, more experimental analyses. We include more related field comparisons to distinguish different settings. We also cover the position of this paper in literature and the main claims of this paper. Finally, we will frankly acknowledge the limitations of this paper, explain and justify the scope of coverage, and provide possible future directions.

\noindent\textbf{Q1: Can this method be applied to PDE systems?}

\noindent\textbf{A:} While this method has the potential to be extended to PDE systems, three critical challenges currently prevent its direct application:

\begin{enumerate}
    \item \textbf{Lack of multi-environment PDE datasets}: Unlike domain adaptation and generalization tasks, multi-environment datasets for PDE systems are not yet available. Although we constructed multi-environment datasets for ODE systems, extending this to PDEs is significantly more complex and requires domain-specific expertise. Unfortunately, designing such datasets is beyond the scope of this paper.
    
    \item \textbf{PDE-specific challenges}: Due to the continuous nature of PDEs, applying invariant function learning to PDE systems requires addressing multi-scale and multi-resolution problems. Scaling up to PDEs also introduces different training dynamics, which may necessitate additional techniques to stabilize and accelerate training.
    
    \item \textbf{Interpretability challenges}: As demonstrated in Appendix~\ref{app:symbolic_explanation}, we employ symbolic regression to provide conceptually understandable explanations for sanity checks. However, this approach does not extend naturally to PDE systems, which would require the development of new interpretability methods tailored to invariant function learning in PDEs.
\end{enumerate}

\noindent\textbf{Q2: Is $\rf_c$ one deterministic true function? What is the difference between $X$ and $\rmX$; $f$ and $\rf$? }

\noindent\textbf{A:} 
\begin{enumerate}
    \item \textbf{Is $\rf_c$ a deterministic function?} No. $\rf_c$ is a random variable sampled from the structural causal model (SCM) (see Fig.~\ref{fig:app_causal_model}), obeying the Markov property of graphical causal models. Consequently, all information-theoretic analyses in this work are non-trivial.
    
    \item \textbf{Difference between $X$ and $\rmX$; $f$ and $\rf$.} $X \in \mathbb{R}^{d \times T}$ represents a single realization sampled from the matrix-shaped random variable $\rmX$, \ie, one trajectory. Similarly, $f$ is a realization of the random function $\rf$.
    
    \item \textbf{Other notation-related questions.} Our notation follows ICLR standards. Please refer to our notation table (Table~\ref{tab:notation}). It is crucial to distinguish between random variables and their realizations.
\end{enumerate}

\noindent\textbf{Q3: What are the differences between coefficient environments and function environments? }

\noindent\textbf{A:} The primary difference lies in which factors vary across multiple environments.

\begin{enumerate}
    \item \textbf{Example}: Consider the pendulum motion as an example. A \textbf{coefficient environment} includes only a \textbf{single} function. Any change in the rope length or friction coefficient defines a new environment. In contrast, a \textbf{function environment} encompasses all functions that share the same functional form but differ in parameters such as rope length and friction coefficient. As long as these pendulums experience the same \textbf{type} of friction, they belong to the same function environment.
    
    \item \textbf{Notation}: Since a coefficient environment contains only one function, we use $f$ to represent that function. Conversely, a function environment consists of multiple functions. Theoretically, the number of functions within a function environment is \textbf{infinite}. Thus, we use the random variable $\rf$ to capture the function distribution within a function environment.
\end{enumerate}

It is important to emphasize that a single function does not correspond to a single trajectory. Even with the same $T$-step discretization, a function can generate infinitely many trajectories depending on different initial conditions.

\noindent\textbf{Q4: What is the position of this paper? What can be covered by this paper?}

\noindent\textbf{A:}  

\textbf{Position}: This paper introduces the concept of invariant function learning, motivated by the function learning requirements in physical systems.

\textbf{Scope of this paper}: The primary focus of this work is to establish invariant function learning in ODE systems from multiple perspectives.

\begin{enumerate}
    \item \textbf{Model design}: We propose the first invariant function learning method.
    
    \item \textbf{Theoretical analysis}: This paper builds upon the well-known invariant learning principle. We establish the foundation of invariant function learning, propose an ODE-based invariant learning structural causal model (SCM) with minimal assumptions, and provide theoretical guarantees for invariant function discovery.
    
    \item \textbf{Function environments and datasets}: We introduce the concept of function environments. To systematically evaluate invariant function learning, we construct multiple multi-environment ODE systems.
    
    \item \textbf{Adapted baselines}: We detail the design and implementation of adapted baselines from invariant learning and meta-learning frameworks.
    
    \item \textbf{Empirical analysis}: We conduct extensive empirical studies, including quantitative comparisons, visual analyses, interpretability assessments, and multi-perspective ablation studies on ODE systems (see Appx.~\ref{app:supplementary_experiments}).
\end{enumerate}

\noindent\textbf{Q5: What cannot be covered by this paper? What are the limitations of this paper?}

\noindent\textbf{A:} This paper aims to establish a solid foundation for invariant function learning from multiple perspectives, as described in Q4. However, exploring invariant function learning is a complex and expansive task that cannot be fully addressed in a single work. Below, we outline the key limitations of this paper to guide future research directions.

\begin{enumerate}
    \item \textbf{Invariant function learning in PDE systems}: This work focuses solely on invariant function learning in ODE systems. Extending this approach to PDE systems remains an open challenge. The reasons and current obstacles in this setting have been elaborated in FAQ Q1.
    
    \item \textbf{Broader range of applications}: Unlike meta-parameters, the learned invariant functions exhibit broader adaptability. For instance, a discovered physical law can generalize across various systems. Future research could explore applications of invariant function learning, such as extracting generalizable physics laws from videos or designing physics-aware agents. Additionally, it would be interesting to investigate whether invariant function learning can contribute to the development of foundational models in physics.
\end{enumerate}

\clearpage

\section{Invariant Function Learning foundation}\label{app:invariant_function_learning_function}

\subsection{Structural Causal Model}\label{app:structural_causal_model}

\begin{figure}[t]
    \centering
    \resizebox{0.4\textwidth}{!}{\includegraphics[width=1\textwidth]{figures/causal_graph.pdf}}
    \caption{Structural causal model.}
    \label{fig:app_causal_model}
\end{figure}

{ In this section, we discuss the trajectory generation process under the Structural Causal Model (SCM) assumption in Fig.~\ref{fig:app_causal_model}. To begin with, this SCM is a directed acyclic graph (DAG) with the following components:

\begin{itemize}
    \item Exogenous variables $U=\{\rc, \re, \rmX_0, \epsilon_c, \epsilon_e, \epsilon_{p}, \epsilon\}$ are not caused by any variables within the model and are from their own independent distributions. Here $\epsilon_c, \epsilon_e, \epsilon_{p}, \epsilon$ are noise terms introduced during the function generation and trajectory integral.
    \item Endogenous variables $V=\{\rf_c, \rf_e, \rf, \rmX^c, \rmX_p, \rmX\}$ are caused by the causal mappings within the model.
    \item Structural equations define the direct causation in the model.
    \begin{itemize}
        \item $\rf_c:=g_c(\rc, \epsilon_c)$
        \item $\rf_e:=g_e(\re, \epsilon_e)$
        \item $\rf:=g_{comp}(\rf_c, \rf_e)$
        \item $\rmX^c:=g_{int}(\rf_c, \rmX_0) + \epsilon$
        \item $\rmX_p:=g^{T_c}_{int}(\rf, \rmX_0) + \epsilon_{p}$
        \item $\rmX:=g_{int}(\rf, \rmX_0) + \epsilon$
    \end{itemize}
    where $g_{int}$ is a $T$-step integrator, while $g^{T_c}_{int}$ only applies integral for $T_c$ steps. While $g_c$, $g_e$, $g_{comp}$ are assumed to be unknown, $g_{int}$ is assumed to be an ideal integrator with an extra random noise $\epsilon$ to model the real world situations. Note that the cause of $\rmX$ can be written as $\rf_c$ and $\rf_e$ without side effects, so $\rf$ is used for analytical purposes. Therefore, $g_{comp}$ is a conceptual function composition without introducing noises.
\end{itemize}
}

\subsection{Proof of Invariant Function Learning Principle}\label{proof:invariant_function_learning_principle}

% \begin{theorem}[Invariant function learning principle~\ref{thm:invariant_function_learning_principle}]
%     Given the causal graph in Fig.~\ref{fig:causal_graph}, the predicted function  variable $\hat{\rf}_c=h_{\theta_c}(\rmX_p)$ is equivalent to the true $\rf_c$ if and only if the following invariant function learning principle is satisfied:
%     \begin{equation}
%         \max_{\theta_c} I(h_{\theta_c}(\rmX_p); \rf|\rmX_0)\ \ s.t.\ \ h_{\theta_c}(\rmX_p) \indep \re,
%     \end{equation}
%     where $I(\cdot;\cdot)$ is mutual information that measure the information overlap between the the predicted invariant function random variable $h_{\theta_c}(\rmX_p)$ and the true full-dynamics function $\rf$.
% \end{theorem}

{\begin{theorem}[Invariant Function Learning Principle~\ref{thm:invariant_function_learning_principle}]
    Given the causal graph in Fig.~\ref{fig:causal_graph} and the predicted function random variable $\hat{\rf}_c = h_{\theta_c}(\rmX_p)$, the true invariant function random variable is $\rf_c = h_{\theta^*_c}(\rmX_p)$, where $\theta^*_c$ is the solution to the following optimization problem:
    \begin{equation}
        \theta^*_c = \argmax_{\theta_c} I(h_{\theta_c}(\rmX_p); \rf | \rmX_0) \quad \text{subject to} \quad h_{\theta_c}(\rmX_p) \indep \re,
    \end{equation}
    where $I(\cdot;\cdot)$ denotes mutual information, which quantifies the informational overlap between the predicted invariant function $h_{\theta_c}(\rmX_p)$ and the true full-dynamics function $\rf$.
\end{theorem}

\begin{proof}
    \emph{Existence:}

    We first prove the existence of a solution $\theta_c^*$ to the optimization problem, such that $\rf_c = h_{\theta_c^*}(\rmX_p)$. To establish this, we proceed by contradiction. Assume no such $\theta_c^*$ exists, implying that $I(\rf_c; \rf | \rmX_0)$ is not maximized. Then, there must exist some $\rf_c^\prime$ such that $\rf_c^\prime \indep \re$ and $I(\rf_c^\prime; \rf | \rmX_0) > I(\rf_c; \rf | \rmX_0)$. Given the mutual information expression $I(\rf_c; \rf | \rmX_0) = H(\rf | \rmX_0) - H(\rf | \rf_c, \rmX_0)$, this inequality implies:
    \begin{equation}\label{eq:compare_entropy}
        H(\rf | \rf_c) > H(\rf | \rf_c^\prime).
    \end{equation}

    Since $\rmX_0$ is independent of $\rf$ and $\rf_c$ (as per Fig.~\ref{fig:causal_graph}), and given that $\rf = g_\rf(\rf_c, \rf_e)$ implies $H(\rf | \rf_c, \rf_e) = 0$, we derive:
    \begin{equation}
        H(\rf | \rf_c) = H(\rf_e).
    \end{equation}

    Similarly, for $\rf_c^\prime$, we have:
    \begin{equation}
        H(\rf | \rf_c^\prime) \geq H(\rf_e | \rf_c^\prime).
    \end{equation}

    Combining these results with Eq.~\ref{eq:compare_entropy}, we obtain:
    \begin{equation}
        H(\rf_e) > H(\rf_e | \rf_c^\prime),
    \end{equation}
    which contradicts the independence condition $\rf_c^\prime \indep \re$, as this would require $H(\rf_e) = H(\rf_e | \rf_c^\prime)$. Therefore, a solution $\theta_c^*$ exists, satisfying $\rf_c = h_{\theta_c^*}(\rmX_p)$.

    \emph{Uniqueness:} We now prove that for any solution $\theta_c^*$ of the optimization process, it holds that $\rf_c = h_{\theta_c^*}(\rmX_p)$. 

We use a proof by contradiction. Assume that there exists another solution $\rf_c^\prime \neq \rf_c$ that satisfies the independence constraint and achieves the maximum mutual information. By assumption, we have $H(\re) = H(\re | \rf_c^\prime)$ and $I(\rf_c^\prime; \rf | \rmX_0) = I(\rf_c; \rf | \rmX_0)$, which implies $H(\rf | \rf_c) = H(\rf | \rf_c^\prime)$. 

Since $\rf = g_\rf(\rf_c, \rf_e)$, we expand the entropy terms:
\begin{equation}
    H(\rf | \rf_c) = H(\rf_c, \rf_e | \rf_c) = H(\rf_e),
\end{equation}
and
\begin{equation}
    H(\rf | \rf_c^\prime) = H(\rf_c, \rf_e | \rf_c^\prime) = H(\rf_c | \rf_c^\prime) + H(\rf_e | \rf_c^\prime) = H(\rf_c | \rf_c^\prime) + H(\rf_e).
\end{equation}

Substituting $H(\rf | \rf_c) = H(\rf | \rf_c^\prime)$ into these equations, we find:
\begin{equation}
    H(\rf_c | \rf_c^\prime) = 0.
\end{equation}

Based on this, we now aim to prove that $\rf_c = \rf_c^\prime$. First, given $H(\rf_c^\prime | \rf_c) \geq 0$, we need to show that $H(\rf_c^\prime | \rf_c) = 0$. Assume that $H(\rf_c^\prime | \rf_c) > 0$. In this case, $\rf_c^\prime$ can determine $\rf_c$ while containing more information than $\rf_c$, all while remaining independent of $\rf_e$. This would imply that $H(\rf^\prime | \rf) > 0$, where $\rf^\prime = g_{\rf}(\rf^\prime, \rf_e)$, which would in turn affect the corresponding prediction, leading to $H(\rmX^\prime | \rmX) > 0$. Such a situation would violate the MSE minimization condition. 

Therefore, we must have both $H(\rf_c^\prime | \rf_c) = 0$ and $H(\rf_c | \rf_c^\prime) = 0$. This implies that $\rf_c$ and $\rf_c^\prime$ are isomorphic functions, i.e., there exists a bijective function $g_b$ such that $\rf_c = g_b(\rf_c^\prime)$. 

Furthermore, since minimizing the MSE of $X$ is equivalent to minimizing the MSE of $\frac{dX}{dt}$ given $X_0$, this minimization ensures that, for the same $\rf_e$, $\rf_c(X) = \rf_c^\prime(X)$ for all $X$. As a result, the bijective function $g_b$ must be the identity mapping, and we conclude that $\rf_c^\prime = \rf_c$ in the support of $p(\rmX)$. 

Thus, for any solution $\theta_c^*$ of the optimization process, it follows that $\rf_c = h_{\theta_c^*}(\rmX_p)$.

\end{proof}}


% {\color{blue}\begin{theorem}[Invariant function learning principle~\ref{thm:invariant_function_learning_principle}]
%     Given the causal graph in Fig.~\ref{fig:causal_graph}, and the predicted function random variable $\hat{\rf}_c=h_{\theta_c}(\rmX_p)$, it follows that the true invariant function random variable $\rf_c=h_{\theta^*_c}(\rmX_p)$, where $\theta^*_c$ is the solution of the following optimization process, described as
%     \begin{equation}
%         \theta^*_c = \argmax_{\theta_c} I(h_{\theta_c}(\rmX_p); \rf|\rmX_0)\ \ s.t.\ \ h_{\theta_c}(\rmX_p) \indep \re,
%     \end{equation}
%     where $I(\cdot;\cdot)$ is mutual information that measure the information overlap between the the predicted invariant function random variable $h_{\theta_c}(\rmX_p)$ and the true full-dynamics function random variable $\rf$.
% \end{theorem}}

% \begin{proof}

%     \emph{Existence:}
    
%     {\color{blue}We first prove that there exists a solution $\theta_c^*$ of the optimization process, $\rf_c=h_{\theta_c^*}(\rmX_p)$.}
%     Given $\hat{\rf}_c=h_{\theta_c}(\rmX_p)=\rf_c$, we first have $\rf_c \indep \re | \rmX_0$ since $\rf$ is the collider between $\rf_c$ and $\re$. Then, we target to prove that $\rf_c=h_{\theta_c}(\rmX_p)$ is one of the solution of the invariant function learning principle:
%     \begin{equation}
%         \max_{\theta_c} I(h_{\theta_c}(\rmX_p); \rf|\rmX_0)\ \ s.t.\ \ h_{\theta_c}(\rmX_p) \indep \re .
%     \end{equation}
%     We use contradiction to prove it. If such $\theta_c^*$ does not exist, then $I(\rf_c;\rf|\rmX_0)$ is not a maximum, so we have $\exists \rf_c^\prime \ \ s.t. \ \ \rf_c^\prime \indep \re $ and $I(\rf_c^\prime;\rf|\rmX_0) > I(\rf_c;\rf|\rmX_0)$. Since $I(\rf_c;\rf|\rmX_0)=H(\rf|\rmX_0) - H(\rf|\rf_c, \rmX_0)$, we have $H(\rf|\rf_c, \rmX_0) > H(\rf|\rf_c^\prime, \rmX_0)$, that is

%     \begin{equation}\label{eq:compare_entropy}
%         H(\rf|\rf_c) > H(\rf|\rf_c^\prime),
%     \end{equation}
    
%     since $\rmX_0$ is independent of $\rf$ and $\rf_c$ according to the causal graph in Fig.~\ref{fig:causal_graph}. Furthermore, since $\rf=g_\rf(\rf_c, \rf_e)$, we have $H(\rf|\rf_c, \rf_e)=0$. Then we have
%     \begin{equation}
%         \begin{array}{rl}
%             H(\rf|\rf_c, \rf_e) & =0 \\
%             H(\rf, \rf_e| \rf_c) - H(\rf_e| \rf_c) & =0 \\
%             H(\rf|\rf_c) + H(\rf_e | \rf, \rf_c) - H(\rf_e | \rf_c) & =0 \\
%         \end{array}
%     \end{equation}
    
%     % Assume the projection from $\re$, $\rc$ to $\rf$ is injective, so that we can always determine which environment $\rf$ comes from, \ie, $H(\re | \rf, \rf_c) \le H(\re | \rf)=0$. Furthermore, s
%     Since $\rf_c \indep \rf_e$, $H(\rf_e | \rf_c)=H(\rf_e)$, then we have
%     \begin{equation}\label{eq:left_entropy}
%         H(\rf|\rf_c) \le H(\rf|\rf_c) + H(\rf_e | \rf, \rf_c) = H(\rf_e) = H(\rf_e | \rf_c),
%     \end{equation}

%     % which can be intuitive understood as that the left uncertainty of $\rf$ given $\rf_c$ comes from $\re$.
    
%     % Besides, since any entropy is bounded by 0, $H(\rf|\rf_c^\prime, \re) \ge 0$. Similarly, we have
%     Furthermore
%     \begin{equation}\label{eq:entropy_ineq}
%     \begin{array}{rl}
%          H(\rf|\rf_c^\prime) = H(\rf_c, \rf_e|\rf_c^\prime) & \ge H(\rf_e | \rf_c^\prime). \\
%     \end{array}
%     \end{equation}

%     According to Eq.~\ref{eq:entropy_ineq}, Eq.~\ref{eq:left_entropy}, and Eq.~\ref{eq:compare_entropy}, we have

%     \begin{equation}
%     \begin{array}{rl}
%          H(\rf_e) \ge H(\rf|\rf_c) &> H(\rf|\rf_c^\prime) \ge H(\rf_e|\rf_c^\prime)  \\
%          H(\rf_e) &  > H(\rf_e|\rf_c^\prime),
%     \end{array}
%     \end{equation}
%     which contradicts the assumption condition $\rf_c^\prime \indep \re$, \ie, $H(\re) = H(\re|\rf_c^\prime)$ and $H(\rf_e) = H(\rf_e|\rf_c^\prime)$, since $\rf_e$ is only caused by $\re$. Therefore, there exists a solution $\theta_c^*$ of the optimization process, $\rf_c=h_{\theta_c^*}(\rmX_p)$.

%     \emph{Comprehension:} We now prove that for any solution $\theta_c^*$ of the optimization process, we have $\rf_c=h_{\theta_c^*}(\rmX_p)$. 
%     % Since we have already proved that $\rf_c$ is one of the solutions of the invariant function learning principle, our target becomes to prove that $\rf_c$ is the only solution. 
%     Similarly, we use contradiction to prove. Assume that we have another solution $\rf_c^\prime\ne \rf_c$, that satisfies the independent constraint and achieves the maximum mutual information. Therefore, we have $H(\re) = H(\re|\rf_c^\prime)$ and $I(\rf_c^\prime;\rf|\rmX_0) = I(\rf_c;\rf|\rmX_0)$, \ie, $H(\rf|\rf_c) = H(\rf|\rf_c^\prime)$ accordingly. 
%     % According to the causal graph, $\rf_c$ is defined by the unique injective causal mapping from $\rc$ and can fully delegate the effects on $\rf$ from $\rc$. $\rf_c^\prime \neq \rf_c$ indicates $\rf_c^\prime$ is not the functional representation of $\rc$, \ie, $\rf_c$; thus cannot delegate $\rc$ to fully determine $\rf$ with $\re$. 
%     Since $\rf = g_\rf(\rf_c, \rf_e)$, we have $H(\rf|\rf_c) = H(\rf_c, \rf_e|\rf_c) = H(\rf_e)$ and $H(\rf|\rf_c^\prime) = H(\rf_c , \rf_e|\rf_c^\prime) = H(\rf_c |\rf_c^\prime) + H(\rf_e|\rf_c , \rf_c^\prime) = H(\rf_c |\rf_c^\prime) + H(\rf_e|\rf_c^\prime) = H(\rf_c |\rf_c^\prime) + H(\rf_e)$. Because $H(\rf|\rf_c) = H(\rf|\rf_c^\prime)$, we have $H(\rf_c |\rf_c^\prime)=0$. 
    
%     Base on that we now want to prove that $\rf_c=\rf_c^\prime$. First, given $H(\rf_c^\prime |\rf_c)\ge 0$, we target proving that $H(\rf_c^\prime |\rf_c)= 0$. Specifically, if $H(\rf_c^\prime |\rf_c)> 0$, $\rf_c^\prime$ can determine $\rf_c$ and contain more information than $\rf_c$ while keeping independent of $\rf_e$, then $H(\rf^\prime|\rf) > 0$ where $\rf^\prime=g_{\rf}(\rf^\prime, \rf_e)$, which will further affect the corresponding prediction so that $H(\rmX^\prime|\rmX) > 0$, violating the MSE minimization optimization. Therefore, we have both $H(\rf_c^\prime |\rf_c)= 0$ and $H(\rf_c |\rf_c^\prime)= 0$. That means $\rf_c$ and $\rf_c^\prime$ are isomorphic functions, \ie, $\exists \mbox{bijective funcion} g_b, \rf_c = g_b(\rf_c^\prime)$. Since the $X$ MSE minimization infers the $\frac{dX}{dt}$ MSE minimization given $X_0$, the minimization indicates that given the same $\rf_e$, $\forall X, \rf_c(X)=\rf_c^\prime(X)$, therefore $g_b$ is an identical mapping and $\rf_c^\prime = \rf_c$. Therefore, given any solution $\theta_c^*$ of the optimization process, we have $\rf_c=h_{\theta_c^*}(\rmX_p)$.
    
%     % Given that $\rf_c^\prime$ is independent of $\rf_e$, $H(\rf|\rf_c^\prime, \re) > 0$. Similarly, we will obtain $H(\rf|\rf_c^\prime) > H(\re | \rf_c^\prime)$ and have
%     % \begin{equation}
%     % \begin{array}{rl}
%     %      H(\re) = H(\rf|\rf_c) &= H(\rf|\rf_c^\prime) > H(\re|\rf_c^\prime)  \\
%     %      H(\re) &  > H(\re|\rf_c^\prime),
%     % \end{array}
%     % \end{equation}
%     % which contradicts to its independence condition. Therefore, $\rf_c$ is the only solution for the invariant function learning principle. That means when the invariant function learning principle is satisfied, $\hat{\rf_c}=\rf_c$.
% \end{proof}

\subsection{Proof of ODE Cross-entropy Minimization}\label{proof:ODE_cross_entropy_minimizaion}

{\begin{lemma}[ODE cross-entropy minimization~\ref{lemma:ODE_cross_entropy_minimizaion}]
    Given our forecasting model $p(\rmX|h_\theta(\rmX_p), \rmX_0)$, it follows that the cross-entropy minimization between the data distribution $p(\rmX)$ and $p(\rmX|h_\theta(\rmX_p), \rmX_0)$ is equivalent to minimizing mean square error $\min_\theta \E_{X\sim p}\|X - \hat{X}\|^2_2$, where $\hat{X}$ is sampled from $p(\rmX|h_\theta(X_p), X_0)$.
\end{lemma}

\begin{proof}
    The forecasting optimization goal is to use our framework to approximate the data distribution $p(\rmX)$, parameterized as $p(\rmX|h_\theta(\rmX_p), \rmX_0)$, where we apply the cross-entropy minimization, \ie,$H(p(\rmX), p(\rmX|h_\theta(\rmX_p), \rmX_0)) = -\E_{\rmX\sim p}\left[\log p(X|h_\theta(X_p), X_0)\right]$. Furthermore, this negative log-likelihood optimization can be further reduced to the common mean squared error (MSE).

    \begin{equation}
        \min_\theta \E_{\rmX\sim p}\|X - \hat{X}\|^2_2,
    \end{equation}

    where $\hat{X}\sim p(\rmX|h_\theta(X_p), X_0)$. Since the distribution $p(\rmX|h_\theta(X_p), X_0)$ is modeled as a Gaussian $\mathcal{N}\left(\mathbf{X} ; g_{int}(h_\theta(X_p), X_0), \sigma^2 I\right)$ (Sec.~\ref{subsection:hypernetwork_design}), we have $\hat{X}=\mu + \epsilon$, where $\epsilon \sim \mathcal{N}\left(0, \sigma^2 I\right)$.
    \begin{equation}
        \begin{aligned}
        &\min_\theta -\mathbb{E}_{\mathbf{X}\sim p}[\log p(X|h_\theta(X_p), X_0)] \\
        =& \min_\theta \mathbb{E}_{\mathbf{X}\sim p}\left[\frac{n}{2}\log(2\pi\sigma^2) + \frac{1}{2\sigma^2}\|X-\mu\|_2^2\right] \\
        =& \min_\theta \mathbb{E}_{\mathbf{X}\sim p}\left[\frac{1}{2\sigma^2}\|X-\mu\|_2^2\right] + \frac{n}{2}\log(2\pi\sigma^2) \\
        \end{aligned}
    \end{equation}

    Since $\frac{n}{2}\log(2\pi\sigma^2)$ is a constant, we ignore it in the minimization process.
    \begin{equation}
        \begin{aligned}
        &\min_\theta \mathbb{E}_{\mathbf{X}\sim p}\left[\frac{1}{2\sigma^2}\|X-\mu\|_2^2\right] + \frac{n}{2}\log(2\pi\sigma^2) \\
        =& \min_\theta \mathbb{E}_{\mathbf{X}\sim p}\left[\frac{1}{2\sigma^2}\|X-(\hat{X}-\epsilon)\|_2^2\right] \\
        =& \min_\theta \mathbb{E}_{\mathbf{X}\sim p}\left[\frac{1}{2\sigma^2}\|X-\hat{X}+\epsilon\|_2^2\right]\\
        =& \min_\theta \mathbb{E}_{\mathbf{X}\sim p}\left[\frac{1}{2\sigma^2}(\|X-\hat{X}\|_2^2 + \|\epsilon\|_2^2 + 2(X-\hat{X})^T\epsilon)\right]\\
        =& \min_\theta \mathbb{E}_{\mathbf{X}\sim p}\left[\frac{1}{2\sigma^2}\|X-\hat{X}\|_2^2  \right] + \frac{1}{2\sigma^2}\mathbb{E}_{\epsilon}[\|\epsilon\|_2^2] + \frac{1}{\sigma^2}\mathbb{E}_{\mathbf{X}\sim p}(X-\hat{X})^T\mathbb{E}_{\epsilon}[\epsilon]\\
        \end{aligned}
    \end{equation}
    Here, $\frac{1}{2\sigma^2}\mathbb{E}_{\rmX\sim p}[\|\epsilon\|_2^2]$ is a constant; $\mathbb{E}_{\mathbf{X}\sim p}\left[\frac{1}{\sigma^2}(X-\hat{X})^T\epsilon\right]=\frac{1}{\sigma^2}\mathbb{E}_{\mathbf{X}\sim p}(X-\hat{X})^T\mathbb{E}_{\epsilon}[\epsilon]=0$ since $\epsilon$ is independently sampled with a zero mean. Therefore,
    \begin{equation}
        \begin{aligned}
        &\min_\theta \mathbb{E}_{\mathbf{X}\sim p}\left[\frac{1}{2\sigma^2}\|X-\hat{X}\|_2^2  \right] + \frac{1}{2\sigma^2}\mathbb{E}_{\epsilon}[\|\epsilon\|_2^2] + \frac{1}{\sigma^2}\mathbb{E}_{\mathbf{X}\sim p}(X-\hat{X})^T\mathbb{E}_{\epsilon}[\epsilon]\\
        =& \min_\theta \mathbb{E}_{\mathbf{X}\sim p}\left[\frac{1}{2\sigma^2}\|X-\hat{X}\|_2^2\right] \\
        =& \min_\theta \mathbb{E}_{\mathbf{X}\sim p}\left[\|X-\hat{X}\|_2^2\right]
        \end{aligned}
    \end{equation}

This reduction connects the information theory and the practical MSE optimization, which further helps us to transform the mutual information maximization into a similar MSE optimization below.
\end{proof}}

\subsection{Proof of ODE Conditional Mutual Information Maximization}\label{proof:ODE_conditional_information_maximization}

{\begin{proposition}[Proof of ODE conditional mutual information maximization~\ref{proposition:ODE_conditional_information_maximization}]
    Given forecasting model $p(\rmX|h_{\theta_c}(X_p), X_0)$, it follows that the conditional mutual information maximization $\max_{\theta_c} I(h_{\theta_c}(\rmX_p); \rf|\rmX_0)$ is equivalent to minimizing mean square error $\min_{\theta_c} \E_{\rmX\sim p}\|X - \hat{X}^c\|^2_2$, where $\hat{X}^c$ is the predicted trajectory sampled from $p(\rmX|h_{\theta_c}(X_p), X_0)$.
\end{proposition}
\begin{proof}
    According to Lemma~\ref{lemma:ODE_cross_entropy_minimizaion}, we can reduce a negative log-likelihood minimization $\min_{\theta_c} - \E_{\rmX \sim p} \log{p(X|h_{\theta_c}(X_p),X_0)}$ to $\min_{\theta_c} \E_{\rmX\sim p}\|X - \hat{X}^c\|^2_2$.

    Therefore, we only need to prove the equivalence between the $- \E_{\rmX \sim p} \log{p(X|h_{\theta_c}(X_p),X_0)}$ minimization and the $I(h_{\theta_c}(\rmX_p); \rf|\rmX_0)$ maximization. It follows that
    \begin{equation}
        \begin{aligned}
            & \max_{\theta_c} I(h_{\theta_c}(\rmX_p); \rf|\rmX_0) \\
            =&\max_{\theta_c} H(\rf|\rmX_0) - H(\rf|h_{\theta_c}(\rmX_p),\rmX_0).
        \end{aligned}
    \end{equation}
    Since $H(\rf|\rmX_0)$ is a constant, we ignore it in the maximization process and obtain
    \begin{equation}
        \begin{aligned}
             &\max_{\theta_c} H(\rf|\rmX_0) - H(\rf|h_{\theta_c}(\rmX_p),\rmX_0) \\
            =&\max_{\theta_c} - H(\rf|h_{\theta_c}(\rmX_p),\rmX_0) \\
            =&\min_{\theta_c} H(\rf|h_{\theta_c}(\rmX_p),\rmX_0). \\
        \end{aligned}
    \end{equation}
    Since $\rmX=g_{int}(\rf, \rmX_0) + \epsilon$ where $\epsilon$ is an indepedent random noise, $H(\rmX|h_{\theta_c}(\rmX_p),\rmX_0, \rf)=H(\epsilon|h_{\theta_c}(\rmX_p))=H(\epsilon)$.
    Since given specific $\theta_c$, $H(h_{\theta_c}(\rmX_p)| \rmX)=0$, with the conditional entropy chain rule, it follows that
    \begin{equation}
        \begin{aligned}
            &\min_{\theta_c} H(\rf|h_{\theta_c}(\rmX_p),\rmX_0) \\
            =&\min_{\theta_c} H(\rf, \rmX|h_{\theta_c}(\rmX_p),\rmX_0) - H(\rmX|h_{\theta_c}(\rmX_p),\rmX_0, \rf) \\
            =&\min_{\theta_c} H(\rf, \rmX|h_{\theta_c}(\rmX_p),\rmX_0) - H(\epsilon)\\
            =&\min_{\theta_c} H(\rf, \rmX|h_{\theta_c}(\rmX_p),\rmX_0)\\
            =&\min_{\theta_c} H(\rf|h_{\theta_c}(\rmX_p),\rmX_0, \rmX) +  H(\rmX|h_{\theta_c}(\rmX_p),\rmX_0)\\
            =&\min_{\theta_c} H(\rf|\rmX_0, \rmX) +  H(\rmX|h_{\theta_c}(\rmX_p),\rmX_0)\\
            =&\min_{\theta_c} H(\rmX|h_{\theta_c}(\rmX_p),\rmX_0) \\
            =&\min_{\theta_c} - \E_{\rmX, \rmX_p, \rmX_0 \sim p} \log{p(X|h_{\theta_c}(X_p),X_0)} \\
            =&\min_{\theta_c} - \E_{\rmX \sim p} \log{p(X|h_{\theta_c}(X_p),X_0)} \\
        \end{aligned}
    \end{equation}
    where $H(\rf|\rmX_0, \rmX)$ is a constant.
    Here, we finish building the equivalence between the function conditional mutual information maximization and the trajectory negative log-likelihood minimization. Using the proof in Lemma~\ref{lemma:ODE_cross_entropy_minimizaion}, our final optimization goal can be reduced to
    \begin{equation}
        \min_{\theta_c} \E_{\rmX\sim p}\|X - \hat{X}^c\|^2_2.
    \end{equation}
\end{proof}

\subsection{Theoretical Justification for Adversarial Training}\label{app:independence_training}

To incorporate the independence constraint, we enforce the condition \( \hat{\rf}_c \indep \re \), where $\hat{\rf}_c=h_{\theta_c}(\rmX_p)$ is the predicted function random variable, not a realization. Since \( \hat{\rf}_c \indep \re \) is equivalent to \( I(\re;\hat{\rf}_c)=0 \), and \( I(\re;\hat{\rf}_c)\ge 0 \), the objective \( I(\re;\hat{\rf}_c) \) reaches its minimum when and only when \( \hat{\rf}_c \indep \re \). This leads us to define minimizing \( I(\re;\hat{\rf}_c) \) as our training criterion.

\begin{definition}
    The environment independence training criterion is
    \begin{equation}
        \theta^*_c = \argmin_{\theta_c} I(\re;\hat{\rf}_c).
    \end{equation}
\end{definition}

This training criterion can be used directly, since the mutual information \( I(\re;\hat{\rf}_c) = \mathbb{E} \left[ \log{\frac{P(\re|\hat{\rf}_c)}{P(\re)}} \right] \) while \( P(\re|\hat{\rf}_c) \) is unknown. Following Proposition 1 in GAN~\cite{goodfellow2020generative}, we introduce an optimal discriminator \( g_\phi: \mathcal{F} \mapsto \mathcal{E} \) with parameters \( \phi \) to approximate the unknown \( P(\re|\hat{\rf}_c) \) as \( P_{\phi}(\re|\hat{\rf}_c) \), minimizing the negative log-likelihood \( -\mathbb{E} \left[ \log{P_{\phi}(\re|\hat{\rf}_c)} \right] \). We then have the following two propositions:

\begin{proposition}\label{proposition:1}
    For \( \theta_c \) fixed, the optimal discriminator \( \phi \) is
    \begin{equation}
        \phi^* = \argmin_{\phi} -\mathbb{E} \left[ \log{P_{\phi}(\re|\hat{\rf}_c)} \right].
    \end{equation}
\end{proposition}

This proposition can be proved straightforwardly by applying the cross-entropy training criterion.

\begin{proposition}\label{proposition:2}
    Denoting KL-divergence as \( \mathrm{KL}[\cdot\|\cdot] \), for \( \theta_c \) fixed, the optimal discriminator \( \phi \) is \( \phi^* \), such that
    \begin{equation}
        \mathrm{KL}\left[P(\re|\hat{\rf}_c) \| P_{\phi^*}(\re|\hat{\rf}_c)\right] = 0.
    \end{equation}
\end{proposition}
\begin{proof}

Given a fixed \( \theta_c \), both \( I(\re;\hat{\rf}_c) \) and \( H(\re) \) are constants. Therefore, we have:

\begin{equation}
    \begin{aligned}
        \phi^* &= \argmin_{\phi} - \mathbb{E} \left[ \log{P_{\phi}(\re|\hat{\rf}_c)} \right] \\
        &= \argmin_{\phi} I(\re;\hat{\rf}_c) - \mathbb{E} \left[ \log{P_{\phi}(\re|\hat{\rf}_c)} \right] - H(\re) \\
        &= \argmin_{\phi} \mathrm{KL}\left[P(\re|\hat{\rf}_c) \| P_{\phi}(\re|\hat{\rf}_c)\right].
    \end{aligned}
\end{equation}

Thus, minimizing the negative log-likelihood of \( P_{\phi}(\re|\hat{\rf}_c) \) is equivalent to minimizing the KL divergence between \( P(\re|\hat{\rf}_c) \) and its approximation \( P_{\phi}(\re|\hat{\rf}_c) \). Since KL divergence is bounded by 0, we have $\mathrm{KL}\left[P(\re|\hat{\rf}_c) \| P_{\phi^*}(\re|\hat{\rf}_c)\right] = 0$. This concludes the proof.
\end{proof}


With these propositions, the mutual information can be computed with the help of the optimal discriminator \( \phi^* \). According to Proposition~\ref{proposition:2}, we have:
\begin{equation}
    \begin{aligned}
        I(\re;\hat{\rf}_c) &= \mathbb{E} \left[ \log{P_{\phi^*}(\re|\hat{\rf}_c)} \right] + H(\re) + \mathrm{KL}\left[P(\re|\hat{\rf}_c) \| P_{\phi^*}(\re|\hat{\rf}_c)\right] \\
        &= \mathbb{E} \left[ \log{P_{\phi^*}(\re|\hat{\rf}_c)} \right] + H(\re) + 0.
    \end{aligned}
\end{equation}

Thus, by disregarding the constant \( H(\re) \), the training criterion becomes:
\begin{equation}\label{eq:re_indep}
    \begin{aligned}
        \theta^*_c &= \argmin_{\theta_c} I(\re;\hat{\rf}_c) \\
        &= \argmin_{\theta_c} \mathbb{E} \left[ \log{P_{\phi^*}(\re|\hat{\rf}_c)} \right] \\
        &= \argmin_{\theta_c} \left\{ \max_{\phi} \mathbb{E} \left[ \log{P_{\phi}(\re|\hat{\rf}_c)} \right] \right\},
    \end{aligned}
\end{equation}

where $P_{\phi}(\re|\hat{\rf}_c)$ is the probability modeling of $g_\phi$. Therefore, the log-likelihood adversarial training can enforce the independence $h_{\theta_c}(\rmX_p) \indep \re$.


}

\clearpage

\section{Datasets}\label{app:datasets}

\subsection{Basic Setup}

We conduct experiments on the proposed three multi-environment datasets ME-Pendulum, ME-Lotka-Volterra, and ME-SIREpidemic. Each of these datasets includes 1000 samples, where 800 and 200 samples are assigned to training set and test set, respectively. Each training set has 4 environments where 200 samples are generated in each environment. Each sample is observed over 10 units of time, and each time is discretized by regularly-spaced discrete time steps from $t_0$ to $t_T$, where $T=99$, \ie, there are 100 time intervals of 0.1 unit of time each. A sample generation process is controlled by a set of ODEs with common parameters $\rmW^c\sim\gU(W^c_{low}, W^c_{high})$ and environment-specific parameters $\rmW^e\sim\gU(W^e_{low}, W^e_{high})$ sampled from their uniform distributions, \eg, in the pendulum system, $\rmW^c=\{\bm{\alpha}\}$ and $\rmW^e=\{\bm{\rho}\}$, where $\bm{\alpha}\sim\gU(\alpha_{low}, \alpha_{high})$ and $\bm{\rho}\sim\gU(\rho_{low}, \rho_{high})$. Note that each environment has its specific function form with its environment-specific parameters $\rmW^e$.


The environment split is the same in the test set. The only difference is that in the test set each sample $X$ has one additional prediction target $X^c$ with only the invariant dynamics. For example, the invariant trajectory $X^c$ of the one generated by $-\alpha^2 \sin{\theta_t} - \rho \omega_t$ will be created by $-\alpha^2 \sin{\theta_t}$.

\subsection{ME-Pendulum}

ME-Pendulum is motivated by the DampedPendulum system~\citep{yin2021augmenting}. The state $X_t=[\theta_t, \omega_t] \in \R^2$ are the angle and angular velocity of the pendulum at time $t$, where we have $\rmW^c=\{\bm{\alpha}\}$, $\rmW^e=\{\bm{\rho}\}$, and $T_C=\frac{T}{3}$. The underlying invariant ODE is $\frac{d \theta_t}{d t}=\omega_t, \frac{d \omega_t}{d t}=-\bm{\alpha}^{2} \sin \left(\theta_t\right)$. As shown in Tab.~\ref{tab:me-pendulum}, this invariant ODE is entangled with different environmental factors, forming four environments, namely, \emph{damped}, \emph{powered}, \emph{spring}, \emph{air}.

\begin{table}[h!]
\centering
\caption{\textbf{ME-Pendulum ODEs} with $\theta_0\sim \gU(0, \frac{\pi}{2})$ and $\omega_0\sim \gU(-1, 0)$.}\label{tab:me-pendulum}
\resizebox{1\textwidth}{!}{
\begin{tabular}{c|c|c|c}
\toprule
\textbf{Environment} & \textbf{ODE for} $\theta_t$ & \textbf{ODE for} $\omega_t$ & \textbf{Distribution of Parameters} \\ 
\midrule
Damped & $\frac{d \theta_t}{d t} = \omega_t$ & $\frac{d \omega_t}{d t} = -\bm{\alpha}^{2} \sin \left(\theta_t\right) - \bm{\rho} \omega_t$ & \multirow{5}{*}{
    \begin{tabular}{l}
        $\bm{\alpha} \sim \gU(1.0, 2.0)$ \\
        $\bm{\rho} \sim \gU(0.2, 0.4)$
    \end{tabular}} \\ 
Powered & $\frac{d \theta_t}{d t} = \omega_t$ & $\frac{d \omega_t}{d t} = -\bm{\alpha}^{2} \sin \left(\theta_t\right) + \bm{\rho} \frac{\omega_t}{|\omega_t|}$ &  \\ 
Spring & $\frac{d \theta_t}{d t} = \omega_t$ & $\frac{d \omega_t}{d t} = -\bm{\alpha}^{2} \sin \left(\theta_t\right) - \bm{\rho} \theta_t$ &  \\ 
Air & $\frac{d \theta_t}{d t} = \omega_t$ & $\frac{d \omega_t}{d t} = -\bm{\alpha}^{2} \sin \left(\theta_t\right) - \bm{\rho} |\omega_t| \omega_t$ &  \\ 
\cmidrule{1-3}
Invariant & $\frac{d \theta_t}{d t} = \omega_t$ & $\frac{d \omega_t}{d t} = -\bm{\alpha}^{2} \sin \left(\theta_t\right)$ &  \\ 
\bottomrule
\end{tabular}
}
\end{table}

\subsection{ME-Lotka-Volterra}

Motivated by the Lotka-Volterra system~\citep{ahmad1993nonautonomous}, the state $X_t=[p_t, q_t] \in \R^2$ are the population of preys and predators at time $t$, where we have $\rmW^c=\{\bm{\alpha}, \bm{\beta}, \bm{\gamma}, \bm{\delta}\}$, $\rmW^e=\{\bm{\alpha}', \bm{\beta}', \bm{\gamma}', \bm{\delta}'\}$, and $T_C=\frac{T}{2}$. The underlying invariant ODEs are $\frac{d p}{d t}=\bm{\alpha} p-\bm{\beta} p q, \frac{d q}{d t}=\bm{\delta} p q-\bm{\gamma} q$. As shown in Tab.~\ref{tab:me_lotka_volterra}, these invariant ODEs are entangled in 4 environments, \ie, \emph{save}, \emph{fight}, \emph{resource}, \emph{omnivore}. The save environment ODEs simulate the decrease of food wastage along with the increase of predators. The fight environment ODEs simulate the decrease of hunting efficiency along with the increase of predators. The resource environment ODEs limit the increase rate of the prey population. In the omnivore environment ODEs, the predators are omnivores that can build the population without preys under certain resource limits.

We plot the trajectories $X$ in the training set, and the invariant trajectories $X^c$ in the test set in Fig.~\ref{fig:me_pendulum}.

\begin{figure}[!h]
    \centering
    \begin{subfigure}[b]{0.794\linewidth}
        \includegraphics[width=1\linewidth]{figures/pendulum_train.png}
    \end{subfigure}
    \begin{subfigure}[b]{0.196\linewidth}
        \includegraphics[width=1\linewidth]{figures/pendulum_test.png}
    \end{subfigure}
    \caption{\textbf{ME-Pendulum trajectories.}}
        \label{fig:me_pendulum}
\end{figure}


\begin{table}[h!]
\centering
\caption{\textbf{ME-Lotka-Volterra ODEs} with $p_0\sim \gU(1000, 2000)$ and $q_0\sim \gU(10, 20).$}
\label{tab:me_lotka_volterra}
\resizebox{1\textwidth}{!}{
\begin{tabular}{c|c|c|c|c}
\toprule
\textbf{Environment} & \textbf{ODE for} $p_t$ & \textbf{ODE for} $q_t$ & \textbf{Distribution of Parameters} \\ 
\midrule
Save & $\frac{d p}{d t} = \bm{\alpha} p - \bm{\beta} p q - \bm{\beta}' p q \cdot 10 \exp{\left(-\frac{q}{10}\right)}$ & $\frac{d q}{d t} = \bm{\delta} p q - \bm{\gamma} q$ & \multirow{5}{*}{
    \begin{tabular}{l}
        $\bm{\alpha}, \bm{\alpha}' \sim \gU(1.2, 2.4)$ \\
        $\bm{\beta}, \bm{\beta}' \sim \gU(6e-2, 1.2e-1)$ \\
        $\bm{\gamma}, \bm{\gamma}' \sim \gU(0.48, 0.96)$ \\
        $\bm{\delta}, \bm{\delta}' \sim \gU(4.8e-4, 9.6e-4)$
    \end{tabular}} \\ 
Fight & $\frac{d p}{d t} = \bm{\alpha} p - \bm{\beta} p q$ & $\frac{d q}{d t} = \bm{\delta} p q + \bm{\delta}' p q \cdot 10 \exp{\left(-\frac{q}{10}\right)} - \bm{\gamma} q$ &  \\ 
Resource & $\frac{d p}{d t} = \bm{\alpha} p - \bm{\alpha}' \frac{p^2}{2000} - \bm{\beta} p q$ & $\frac{d q}{d t} = \bm{\delta} p q - \bm{\gamma} q$ &  \\ 
Omnivore & $\frac{d p}{d t} = \bm{\alpha} p - \bm{\beta} p q$ & $\frac{d q}{d t} = \bm{\delta} p q + 20 \bm{\gamma}' \left(1 - \frac{q}{100}\right) - \bm{\gamma} q$ &  \\ 
\cmidrule{1-3}
Invariant & $\frac{d p}{d t} = \bm{\alpha} p - \bm{\beta} p q$ & $\frac{d q}{d t} = \bm{\delta} p q - \bm{\gamma} q$ &  \\ 
\bottomrule
\end{tabular}}
\end{table}

We plot the trajectories $X$ in the training set, and the invariant trajectories $X^c$ in the test set in Fig.~\ref{fig:me_lotka_volterra}.

\begin{figure}[!h]
    \centering
    \begin{subfigure}[b]{0.794\linewidth}
        \includegraphics[width=1\linewidth]{figures/lv_train.png}
    \end{subfigure}
    \begin{subfigure}[b]{0.196\linewidth}
        \includegraphics[width=1\linewidth]{figures/lv_test.png}
    \end{subfigure}
    \caption{\textbf{ME-Lotka-Volterra trajectories.}}
        \label{fig:me_lotka_volterra}
\end{figure}


\subsection{ME-SIREpidemic}

In the SIREpidemic~\citep{wang2021bridging} system, the states $X_t=[S_t, I_t, R_t]\in \R^3$ are the susceptible, infected, and recovered individuals at time $t$, respectively. In this adapted ME-SIREpidemic system, we have $\rmW^c=\{\bm{\beta}\}$, $\rmW^e=\{\bm{\gamma}\}$, and $T_c=\frac{T}{2}$. The underlying invariant ODEs are $\frac{d S}{d t}=-\beta \frac{S I}{S+I+R}, \frac{d I}{d t}=\beta \frac{S I}{S+I+R}, \frac{d R}{d t}=0$, where we only care about the $S$ to $I$ transformation relationship. As shown in Tab.~\ref{tab:me-sirepidemic}, we introduce 4 environments, \emph{origin}, \emph{enlarge}, \emph{loop}, \emph{negative}. These four environments describe four different models, where some of them are only for math modeling. The origin environment is the same as the original SIREpidemic model. The enlarge environment ODEs expand the epidemic range. The loop environment ODEs include deaths and second-time infections. The negative environment ODEs is a pure math model allowing negative numbers.

\begin{table}[h!]
\centering
\caption{\textbf{ME-SIREpidemic ODEs} with $S_0\sim\gU(9, 10)$, $I_0\sim\gU(1, 5)$, and $R_0=0$.}
\label{tab:me-sirepidemic}
\resizebox{1\textwidth}{!}{
\begin{tabular}{c|c|c|c|c}
\toprule
\textbf{Environment} & \textbf{ODE for} $S_t$ & \textbf{ODE for} $I_t$ & \textbf{ODE for} $R_t$ & \textbf{Distribution of Parameters} \\ 
\midrule
Origin & $\frac{d S}{d t} = -\beta \frac{S I}{S+I+R}$ & $\frac{d I}{d t} = \beta \frac{S I}{S+I+R} - \bm{\gamma} I$ & $\frac{d R}{d t} = \bm{\gamma} I$ & \multirow{5}{*}{
    \begin{tabular}{l}
        $\bm{\beta} \sim \gU(4, 8)$ \\
        $\bm{\gamma} \sim \gU(0.4, 0.8)$
    \end{tabular}} \\ 
Enlarge & $\frac{d S}{d t} = -\beta \frac{S I}{S+I+R} + \bm{\gamma} I$ & $\frac{d I}{d t} = \beta \frac{S I}{S+I+R} - \bm{\gamma} I$ & $\frac{d R}{d t} = \bm{\gamma} I$ &  \\ 
Loop & $\frac{d S}{d t} = -\beta \frac{S I}{S+I+R} + \bm{\gamma} I + \bm{\gamma} R$ & $\frac{d I}{d t} = \beta \frac{S I}{S+I+R} - 2 \bm{\gamma} I$ & $\frac{d R}{d t} = \bm{\gamma} I - \bm{\gamma} R$ &  \\ 
Negative & $\frac{d S}{d t} = -\beta \frac{S I}{S+I+R}$ & $\frac{d I}{d t} = \beta \frac{S I}{S+I+R} + \bm{\gamma} \log{I}$ & $\frac{d R}{d t} = -\bm{\gamma} \log{I}$ &  \\ 
\cmidrule{1-4}
Invariant & $\frac{d S}{d t} = -\beta \frac{S I}{S+I+R}$ & $\frac{d I}{d t} = \beta \frac{S I}{S+I+R}$ & $\frac{d R}{d t} = 0$ &  \\ 
\bottomrule
\end{tabular}
}
\end{table}

We plot the trajectories $X$ in the training set, and the invariant trajectories $X^c$ in the test set in Fig.~\ref{fig:me_sirepidemic}.

\begin{figure}[!h]
    \centering
    \begin{subfigure}[b]{0.794\linewidth}
        \includegraphics[width=1\linewidth]{figures/sirepidemic_train.png}
    \end{subfigure}
    \begin{subfigure}[b]{0.196\linewidth}
        \includegraphics[width=1\linewidth]{figures/sirepidemic_test.png}
    \end{subfigure}
    \caption{\textbf{ME-SIREpidemic trajectories.}}
        \label{fig:me_sirepidemic}
\end{figure}

\section{Experimental Details}\label{app:experimental_details}

We conduct experiments on 800-sample training sets with a training batch size of 32, which leads to 25 iterations per epoch. For each run, we optimize the neural network with 2,000 epochs, which is equivalent to 50,000 iterations. Given fixed learning iterations, the learning rate is selected from $\gU(1e-4, 1e-3)$. 

\subsection{Baselines}\label{app:baselines}

In our experiments, we design four adapted baselines, since this new task has never been explored before. The selection of baselines is based on the following two questions.

\begin{itemize}
    \item As invariant learning has been successfully applied in many representation learning tasks, can general invariant learning principle still work for invariant function learning? 
    \item Meta-learning techniques has been well designed to solved problems in dynamical systems due to their quick adaptation characteristics. However, there is no evidence that the quick adaptable functions are the invariant function that shares across environments. Are they?
\end{itemize}

For the first question, since our method is based on enforcing independence which shares a similar philosophy as domain adversarial neural network~\cite{ganin2016domain}, we consider other invariant methods going different ways. There are many methods well-known in the field of invariant learning, but considering our invariant function learning formulation, we found the discovery of invariant function requires independence that is different from the most typical "well performed across all environments" invariant learning requirements. Therefore, as a validation of our guess, we adapt two widely used and known invariant learning methods, IRM~\cite{arjovsky2019invariant} and VREx~\cite{krueger2021out}. These two techniques are directly applied to our framework with the same architecture and their typical hyper-parameters searching spaces:

\begin{itemize}
    \item $\lambda_{irm} \sim \gU(1e-2, 1e2)$
    \item $\lambda_{vrex} \sim \gU(1e-1, 1e3)$
\end{itemize}

As shown in our result plot~\ref{fig:quantitative_results}, the results are not surprising. That means that the function can perform well across multiple environments is not the invariant function.

For the second question, our initial guess is that meta-learning is very sensitive to the distribution of the training set. If certain pattern exists in multiple environments (not all), the meta-learning methods are prone to capture it as a part of the meta-function, which satisfies their quick adaptation goals. In our experiments, we choose MAML~\cite{finn2017model} and CoDA~\cite{Kirchmeyer2022coda} as our adapted baselines. CoDA is adapted since it uses hypernetwork as a full network adaptor which is similar to our framework. However, CoDA is applied on coefficient-environments and requires test-time adaptation without a trajectory encoder, leading to significant architecture differences. Therefore, we apply CoDA as meta-learning techniques focusing on its low-dimension (2-dimension) environment representation and regularization. MAML is selected as the most typical meta-learning baseline, which does not require the use of hypernetwork, and only learns a meta function used to predict invariant trajectories. Their typical hyper-parameters searching spaces are shown as follows.

\begin{itemize}
    \item $\lambda_{coda}\sim \gU(1e-5, 1e-3)$
    \item $\lambda_{maml}\sim \gU(1e-3, 1)\ \ \ \ \ \ \ \mbox{(Meta learning rate)}$
\end{itemize}

\subsection{Disentanglement of Invariant Function Setup}

\subsubsection{Architecture}\label{app:architecture}

\textbf{Transformer.} For the trajectory encoder in our hypernetwork, we apply a 6-layer 8-head 256-dimension FFN transformer~\cite{vaswani2017attention} with frequency positional encoding~\cite{gehring2017convolutional}. We tried different architectures like GRUs~\cite{cho2014learning}, but the transformer encoder can provide the best in-domain test performance easily. We also sweeped to the depth, width, and number of heads, and found that 6-layer 8-head 256-dimension FFN transformer is strong enough for our ODE systems without making training difficult.

\textbf{Function embedding.} In the hidden function embedding space, we select the function embedding dimension to be 32 or 64, while these two selections perform quite similar. For the MLPs used to disentangle and decode hidden function embedding, we use 3-layer MLPs with ReLU as the activations. While for the decoder, the last layer projects the hidden function embedding to parameterized function space $\R^m$ where $m$ is the number of parameters in the derivative neural network.

\textbf{Derivative function network.} The derivative neural network is a 4-layer or 5-layer MLP with width 16 or 32, which takes $X_t\in \R^d$ as input and output $\frac{d X_t}{d t} \in \R^d$. This neural network is transformed to be a functional in our implementation.

\textbf{Discriminator.} Our discriminator is a 3 to 6 layer MLP with width 64 or 128. The size of discriminator can be easily chosen since the main goal of it is to discriminate the environment information in the hidden function space. Therefore, the simplest way to filter non-qualified discriminators is using the prediction entropy of $P_\phi(\re | \hat{\rf}_e)$, since $\hat{\rf}_e$ should contain rich environment information, different to the prediction from $\hat{\rf}_c$. Our experiments also validate that the failure to distinguish $\hat{\rf}_e$ will always cause the failure of invariant function discovery, which is natural, since the adversarial training is based on the optimal discriminators~\cite{goodfellow2020generative}.

Note that for all our MLPs, we apply one LayerNorm before each activation.\footnote{The code will be released upon acceptance.}

\subsubsection{Training Objectives}~\label{app:training_objectives}

We restate our three additional strategies here.
Firstly, the adversarial training of $\hat{\rf}_c$ will cause the loss of environment information in $\hat{\rf}_c$, leading to the training difficulty of the discriminator; therefore, this discriminator is not only trained on $\hat{\rf}_c$ but also on $\hat{\rf}_e$ with the same hyper-parameter $\lambda_{dis}$, \ie, $\min_{\phi, \theta} - \E_{\rmX, \re\sim P}\left[\log P_\phi(e | \hat{f}_e)\right]$.
Secondly, instead of using the large $\hat{f}_c$ and $\hat{f}_e$ as the input of the discriminator, we input the the corresponding embeddings $\vz_c$ and $\vz_e$. 
Thirdly, to avoid the use of a numerical or neural integrator which causes long training time, we follow~\cite{moulimetaphysica} to fit derivatives only. 
That is, instead of using the inference forecaster $p(X|h_{\theta_c}(X_p), X_0)$, we calculate the derivatives of $X$ using $\hat{f}$ and $\hat{f}_c$, and replace the MSE over trajectory matrices with the MSE over derivative matrices. Note that this modification only eliminates the use of integrator for stability during training and thus does not affect our analysis and optimization goal. 

% According to our optimization objectives~\ref{lemma:ODE_cross_entropy_minimizaion}, \ref{eq:invariant_adv} and \ref{eq:discriminator}, and the three practical training tricks, we now introduce our full implementation losses.

% First, for the encoder-decoder training and the derivative training trick, the training goal~\ref{lemma:ODE_cross_entropy_minimizaion} introduces the main forecasting loss.
% \begin{equation}
%     \gL_{f} = \frac{1}{N} \sum_{X_t} \left\|\frac{dX_t}{dt} - \frac{d\hat{X}_t}{dt}\right\|_2^2,
% \end{equation}
% where $N$ is the sample number, $\frac{d \hat{X}_t}{dt}=dF_{\hat{f}}(X_t)$. Here, to avoid the abuse of notation $\hat{f}$, we use $dF$ to denote the derivative functional that takes $\hat{f}$ as parameters. As a function representation, $\hat{f}=h_\theta(X_p)$. Note that for each $X$, we use all its 100 time steps in the training, and we still call it 1 sample. That means, for a batch size of 32, we calculate $32 \times 100 \times d$ derivatives for each iteration.

% For the first term in Eq.~\ref{eq:invariant_adv}, we have the conditional mutual information maximization as a loss.
% \begin{equation}
%     \gL_{f_c} = \lambda_{c} \cdot \frac{1}{N} \sum_{X_t} \left\|\frac{dX_t}{dt} - \frac{d\hat{X}^c_t}{dt}\right\|_2^2,
% \end{equation}
% where $\frac{d \hat{X}^c_t}{dt}=dF_{\hat{f}_c}(X_t)$; the function representation $\hat{f_c}=h_{\theta_c}(X_p)$.

% For the training of discriminator, we have input both $\hat{f}_c$ and $\hat{f}_e$ as described in the practical tricks, and we use the hidden representations $\hat{z}_c$ and $\hat{z}_e$ instead of the decoded representation $\hat{f}_c$ and $\hat{f}_e$. Thus, we introduce two losses.
% \begin{subequations}
%     \begin{equation}
%         \gL_{dis}^c = \lambda_{dis} \cdot \frac{1}{N} \sum_{X_t} - \log{g_\phi(\hat{z}_c)}
%     \end{equation}
%     \begin{equation}
%         \gL_{dis}^e = \lambda_{dis} \cdot\frac{1}{N} \sum_{X_t} - \log{g_\phi(\hat{z}_e)}
%     \end{equation}
% \end{subequations}

% Last, for the adversarial training, the second term in Eq.~\ref{eq:invariant_adv} can be written as follows.

% \begin{equation}
%     \gL_{adv} = \lambda_{adv} \cdot \frac{1}{N} \sum_{X_t} \log{g_\phi(\hat{z}_c)}
% \end{equation}

% Then the final optimization can be described as follows.

% \begin{subequations}
%     \begin{equation}
%         \theta_{enc} = \argmin_{\theta_{enc}} \gL_{f} + \gL_{f_c} + \gL_{dis}^e + \gL_{adv}
%     \end{equation}
%     \begin{equation}
%         \theta_{inv} = \argmin_{\theta_{inv}} \gL_{f} + \gL_{f_c} + \gL_{adv}
%     \end{equation}
%     \begin{equation}
%         \theta_{env} = \argmin_{\theta_{env}} \gL_{f} + \gL_{dis}^e
%     \end{equation}
%     \begin{equation}
%         \theta_{dec} = \argmin_{\theta_{dec}} \gL_{f} + \gL_{f_c}
%     \end{equation}
%     \begin{equation}
%         \phi = \argmin_{phi} \gL_{dis}^c + \gL_{dis}^e
%     \end{equation}
% \end{subequations}

% It worth noting that $\gL_{dis}^c$ is not back-propagated to $\theta_{inv}$ and $\theta_{enc}$ to prevent introducing environment information into $\hat{f}_c$.

We introduce our hyper-parameter searching space as follows.

\begin{itemize}
    \item $\lambda_{c} \sim \gU(1e-7, 1e-4)$
    \item $\lambda_{dis} \sim \gU(1e-1, 1)$
    \item $\lambda'_{adv} \sim \gU(1e2, 1e6)$
    \item $\lambda_{adv} = \lambda_c \cdot \lambda'_{adv}$
\end{itemize}

The most critical hyper-parameters are $\lambda_{c}$ and $\lambda_{adv}$ which control the information overlap between $\rf_c$ and $\rf$. Conceptually, $\lambda_{c}$ controls the conditional mutual information (MI) maximization in our invariant function learning principle, while $\lambda_{adv}$ enforces the independence constraint. The intensity of the independence enforcing $\lambda_{adv}$ is dependent on the intensity of MI maximization $\lambda_{c}$; thus, we set $\lambda_{adv}$ according to $\lambda_{c}$, leading to $\lambda_{adv} = \lambda_c \cdot \lambda'_{adv}$.

$\lambda_{dis}$ is only discriminator training, which is relatively trivial according to our discriminator descriptions in Appx.~\ref{app:architecture}.
{
\subsubsection{Metric}

Root mean square deviation (RMSE) is a commonly used metric, but it suffers difficulties when comparing datasets with different value scales. Therefore, we normalize it using its standard deviation.

\begin{equation}
    \mbox{NRMSE} = \frac{\sqrt{\E_{\rmX \sim p} \|X - \hat{X}\|^2_2}}{\mbox{Std}(\rmX)}
\end{equation}
}

\subsection{Software and Hardware}

Our implementation is under the architecture of PyTorch~\cite{paszke2019pytorch}. The deployment environments are Ubuntu 20.04 with 48 Intel(R) Xeon(R) Silver, 4214R CPU @ 2.40GHz, 755GB RAM, and graphics cards NVIDIA RTX 2080Ti.


\section{Supplementary Experiments}\label{app:supplementary_experiments}

\subsection{Full Function v.s. Invariant Function}\label{subsec:full_vs_inv}

\begin{table}[!t]\centering
\caption{\textbf{Invariant function validation and symbolic regression.} NAN denotes that the result is not applicable or not of interest.}\label{tab:symbolic_regression}
\resizebox{1\linewidth}{!}{
\begin{tabular}{c|c|l|l|l|l|l|l}\toprule
\multirow{2}{*}{Target} &\multirow{2}{*}{Function} & \multicolumn{2}{c|}{ME-Pendulum} & \multicolumn{2}{c|}{ME-Lotka-Volterra} & \multicolumn{2}{c}{ME-SIREpidemic} \\\cmidrule{3-8}
& &NRMSE &SR Explanation &NRMSE &SR Explanation &NRMSE &SR Explanation \\
\midrule
\multirow{7}{*}{$\rmX^c$} &$\hat{\rf}_c$ &0.3561 &$\begin{aligned} \frac{d\theta_t}{dt} &= 0.99 \omega_{t} \\ \frac{d\omega_t}{dt} &= -0.97 \alpha^2 \sin{\left(\theta_t \right)}  \end{aligned}$ &0.6194 &$
\begin{aligned}
\frac{dp_t}{dt} &= 1.254 p_t -0.38 q_t p_t  \\
\frac{dq_t}{dt} &= 4.1 p_t - 0.30 q_t - \gamma
\end{aligned}
$ &0.0652 &$\begin{aligned} \frac{dS_t}{dt} &= - 1.7 S_t I_{t} \\ \frac{dI_t}{dt} &= 0.42 S_t I_{t} \\ \frac{dR_t}{dt} &= -0.0088 \end{aligned}$ \\ 
\cmidrule{2-8}
&$\hat{\rf}$ &0.7884 &$\begin{aligned} \frac{d\theta_t}{dt} &= \omega_{t} \cos{\left(\frac{\omega_{t}}{e^{\alpha}} \right)} \\ \frac{d\omega_t}{dt} &= \theta_t \alpha \left(- \alpha + \rho\right) \end{aligned}$ &0.7919 &$
\begin{aligned}
\frac{dp_t}{dt} &= -0.76 p_t \\
\frac{dq_t}{dt} &= \frac{p_t}{0.36} - \gamma
\end{aligned}
$ &0.9867 &$\begin{aligned} \frac{dS_t}{dt} &= -0.24 \beta I_{t} S_t - 1.2 \\ \frac{dI_t}{dt} &= 0.40 S_t \\ \frac{dR_t}{dt} &= 0.66 \gamma \end{aligned}$ \\ 
\midrule
\multirow{2}{*}{$\rmX$ } &$\hat{\rf}_c$ &0.7994 &NAN &0.6912 &NAN &0.7641 &NAN \\
\cmidrule{2-8}
&$\hat{\rf}$ &0.1700 &NAN &0.3881 &NAN &0.0212 &NAN \\
\midrule
\multicolumn{2}{c|}{$\rf_c$ GT} & NAN & $\begin{aligned}
    \frac{d\theta_t}{dt} &= \omega_{t}  \\ 
    \frac{d\omega_t}{dt} &= - \alpha^2 \sin{\left(\theta_t \right)}
    \end{aligned}$ & NAN& $\begin{aligned}
        \frac{dp_t}{dt} &= \alpha p_t - \beta p_t q_t \\ 
        \frac{dq_t}{dt} &= \delta p_t q_t - \gamma q_t
        \end{aligned}$ & NAN& $\begin{aligned} \frac{dS_t}{dt} &= - \beta \frac{S_t I_{t}}{S_t + I_t + R_t} \\ \frac{dI_t}{dt} &= \beta \frac{S_t I_{t}}{S_t + I_t + R_t} \\ \frac{dR_t}{dt} &= 0 \end{aligned}$  \\
\bottomrule
\end{tabular}}
\vspace{-0.4cm}
\end{table}

To analyze \textbf{RQ3}, we benchmark our best invariant function learning models on the invariant state ground truth $\rmX^c$ and the multi-environment state ground truth $\rmX$, comparing their results using the predicted invariant function $\hat{\rf}_c$ and the full function $\hat{\rf}$. As shown in Tab.~\ref{tab:symbolic_regression}, the performance on $\rmX^c$ using $\hat{\rf}_c$ represents the core results of our invariant function learning approach. In contrast, the predictions on $\rmX^c$ using $\hat{\rf}$ serve as a baseline for the \emph{ablation study}, where no invariant function learning principle is applied. Additionally, the prediction error on $\rmX$ using $\hat{\rf}_c$ implies the environmental information eliminated by the invariant function learning principle, while the NRMSEs on $\rmX$ using $\hat{\rf}$ reflect standard in-distribution (ID) test errors. We observe that the ME-Lotka-Volterra dataset is the most challenging, with an NRMSE of 0.3881 in the ID test. This result is consistent with general deep learning outcomes in \cite{moulimetaphysica}, given that ME-Lotka-Volterra is more complex than its original version.

As expected, $\hat{\rf}$ performs well on $\rmX$, while $\hat{\rf}_c$ excels in predicting $\rmX^c$. In our ablation study, we compare the performance of invariant state predictions $\rmX^c$ across all datasets. The predicted invariant functions $\hat{\rf}_c$ significantly outperform the full predicted functions $\hat{\rf}$ in terms of NRMSE, validating the effectiveness of the proposed invariant function learning principle.\rebuttal{ Furthermore, we conduct more strict ablation study with independent $\hat{\rf}$ and $\hat{\rf}_c$ training in Appx.~\ref{app:ablation_study}.}

\subsection{Symbolic Regression Explanation}\label{subsec:symbolic_regression_explanation}

% Furthermore, we analyze the extracted invariant functions $\hat{\rf}_c$ by applying symbolic regression using PySR~\citep{cranmer2023interpretable} to address the RQ4. As shown in the Tab.~\ref{tab:symbolic_regression}, we compare the SR explanations of the invariant functions $\hat{\rf}_c$ and the true $\rf_c$. On ME-Pendulum, the friction-less pendulum function is extracted nearly perfectly, where $0.99 \omega_{t}$ matches $\omega_{t}$, and $-0.97 \alpha^2 \sin{\left(\theta_t \right)}$ is quite close to $- \alpha^2 \sin{\left(\theta_t \right)}$. Given the difficulty of the ME-Lotka-Volterra dataset, the invariant functions $\hat{\rf}_c$ extracted are prominent and significantly better than the full function $\hat{\rf}$ results. For ME-SIREpidemic, the invariant state NRMSE indicates a nearly perfect invariant prediction, which indicates that the invariant function must be extracted correctly. However, although the extracted functions on $\frac{dR_t}{dt}$ is correct but the extracted expressions of $\frac{dS_t}{dt}$ and $\frac{dI_t}{dt}$ do not exactly match the $\rf_c$ we expected, where the coefficient from $\frac{dS_t}{dt}$ does not equal to the inverse coefficient from $\frac{dI_t}{dt}$, regardless the denominator because it should be a constant. 
% These mismatches may attribute to the fact that PySR is not designed specifically for the challenges that may be introduced by learning multiple functions where all these variables are correlated and interacted. Fortunately, for this symbolic regression phase, PySR is not the only choice. It is possible to inject strong inductive bias similar to PIML methods~\citep{moulimetaphysica,yin2021augmenting,cranmer2020discovering}, which can be further explored in the future.

Furthermore, to address \textbf{RQ4}, we analyze the extracted invariant functions $\hat{\rf}_c$ by applying symbolic regression using PySR~\citep{cranmer2023interpretable}. As shown in Tab.~\ref{tab:symbolic_regression}, we compare the symbolic regression (SR) explanations of the extracted invariant functions $\hat{\rf}_c$ with the true invariant functions $\rf_c$. On the ME-Pendulum dataset, the frictionless pendulum function is nearly perfectly extracted, with $0.99 \omega_{t}$ matching $\omega_{t}$ and $-0.97 \alpha^2 \sin{\left(\theta_t \right)}$ closely approximating the true $- \alpha^2 \sin{\left(\theta_t \right)}$. Given the complexity of the ME-Lotka-Volterra dataset, the extracted invariant functions $\hat{\rf}_c$ are non-trivial and significantly outperform the full function $\hat{\rf}$. On the ME-SIREpidemic dataset, the near-perfect NRMSE for the invariant state indicates that the invariant function must have been correctly extracted. However, although the expression for $\frac{dR_t}{dt}$ is correct, the extracted expressions for $\frac{dS_t}{dt}$ and $\frac{dI_t}{dt}$ do not precisely match the expected $\rf_c$. Specifically, the coefficient from $\frac{dS_t}{dt}$ does not equal the inverse of the coefficient from $\frac{dI_t}{dt}$, though this discrepancy should be constant. These mismatches attribute to the limitations of PySR given the large number of variables and samples.\footnote{\rebuttal{Superior PySR explanations indicate great invariant function learning results, but effective invariant function learning results might not lead to good PySR explanations.}} Future work could explore incorporating stronger inductive biases, similar to physics-informed machine learning (PIML) methods~\citep{moulimetaphysica,yin2021augmenting,cranmer2020discovering}, to address these challenges. {Please refer to Appx.~\ref{app:symbolic_explanation} for symbolic comparisons with all baseline.}

\subsection{Ablation Study}\label{app:ablation_study}

\begin{figure}
% \vspace{-0.3cm}
    \centering
    \resizebox{1\linewidth}{!}{
    $
    \begin{array}{ccc}
         \includegraphics[width=0.5\linewidth]{figures/ablation_pendulum_single_branch.png} &  
         \includegraphics[width=0.5\linewidth]{figures/LV_single_branch.png} & 
         \includegraphics[width=0.5\linewidth]{figures/SIREpidemic_single_branch.png}
    \end{array}
    $}
    \caption{\textbf{Ablation study} on 3 DIF variants under 3 multi-environment ODE systems. For each pipeline, we provide model candidates with 50+ random hyper-parameter selections in their searching spaces, \ie, more than 450 models in the figure.}
    \label{fig:ablation_study}
\end{figure}

As a complementary ablation study of Sec.~\ref{subsec:full_vs_inv}, we train two models by eliminating two important components from the original model, namely, $f$ pipeline and $f_c$ pipeline. The $f$ pipeline removes the discriminator and only output $\hat{f}$ to the forecasting, which neglects the disentanglement process. The $f_c$ pipeline prunes the $\hat{f}$ output while maintaining the adversarial training process. As shown in Fig.~\ref{fig:ablation_study}, both $f$ and $f_c$ pipelines fail to perform valid invariant function learning aligning with our theoretical results. Specifically, the $f$ pipeline faces difficulties in extracting invariant functions without environment information. The unsatisfactory performance of the $f_c$ pipeline is attributed to the discriminator's training failure. This is because the training of the discriminator requires the capture of environment information, but the elimination of the $\hat{f}$ part also removes the training of $\hat{z}_e$, the critical environment information captor. Therefore, the discriminator loses the most important environment information input, leading to training failure.

\subsection{Input Length and Environment Analysis}\label{app:input_length_and_environment_analysis}

\begin{figure}
% \vspace{-0.3cm}
    \centering
    \resizebox{1\linewidth}{!}{
    $
    \begin{array}{ccc}
         \includegraphics[width=0.5\linewidth]{figures/ablation_pendulum_input_length.png} &  
         \includegraphics[width=0.5\linewidth]{figures/LV_input_length.png} & 
         \includegraphics[width=0.5\linewidth]{figures/SIREpidemic_input_length.png}
    \end{array}
    $}
    \caption{\textbf{Trajectory input length study} on models trained with different training input length factors under 3 multi-environment ODE systems.}
    \label{fig:input_length}
\end{figure}

\begin{figure}
% \vspace{-0.3cm}
    \centering
    \resizebox{1\linewidth}{!}{
    $
    \begin{array}{ccc}
         \includegraphics[width=0.5\linewidth]{figures/ablation_pendulum_types.png} &  
         \includegraphics[width=0.5\linewidth]{figures/LV_types.png} & 
         \includegraphics[width=0.5\linewidth]{figures/SIREpidemic_types.png}
    \end{array}
    $}
    \caption{\textbf{Environment analysis} on models trained with different numbers of training environments under 3 multi-environment ODE systems.}
    \label{fig:types_environment}
\end{figure}

In order to ensure fairness, we fix the input length and the number of environments in our experiments. However, it is also interesting to figure out the effects of the input trajectory length $T_c$ and the number of environments on the model performance. Fig.~\ref{fig:input_length} shows the performance of DIF given different input length factor $l_t$, where $T_c= \frac{T}{l_t}$. The results indicate the input length does not affect the performance significantly, where the only variances are attributed to the training difficulty of the transformer given different input lengths. Therefore, a shorter input length can perform slightly better given the same training steps.

For the environment analysis, in addition to evaluations on the full set of environments, we benchmark model performance on datasets with three and two training environments. Specifically, we select [Powered, Air, Spring] and [Powered, Air] for ME-Pendulum; [Save, Fight, Resource] and [Save, Fight] for ME-Lotka-Volterra; and [Negative, Origin, Enlarge] and [Negative, Origin] for ME-SIREpidemic. While not all possible environment combinations are evaluated, these selections provide intriguing insights. As illustrated in Fig.~\ref{fig:types_environment}, changes in the set of environments weakly affect model performance on ME-Lotka-Volterra. For ME-Pendulum, however, the inclusion of each additional environment consistently improves model performance. On ME-SIREpidemic, the performance boost observed with "3 envs" underscores the critical role of the environment Enlarge. 

Two key observations regarding the ME-SIREpidemic are worth noting. First, the average performance degradation on "4 envs" suggests a reduced focus on the important environment Enlarge due to the addition of the final environment Loop. Second, the improvement in the best performance candidate demonstrates the additional benefits of the environment Loop. These findings illustrate that while adding environments can enhance the best possible discovery of invariant functions, it also increases the average training complexity that may cause average performance degradations.



\begin{table}[!t]\centering

\caption{\textbf{Symbolic regression explanation comparisons.}}\label{tab:symbolic_regression_comparisons}
\resizebox{1\linewidth}{!}{
\begin{tabular}{c|l|l|l|l|l|l}\toprule
\multirow{2}{*}{Method} & \multicolumn{2}{c|}{ME-Pendulum} & \multicolumn{2}{c|}{ME-Lotka-Volterra} & \multicolumn{2}{c}{ME-SIREpidemic} \\\cmidrule{2-7}
&NRMSE &SR Explanation &NRMSE &SR Explanation &NRMSE &SR Explanation \\
\midrule
MAML &0.9704 & $\begin{aligned}
        \frac{d\theta_t}{dt} &= \omega_{t} - \frac{0.036 \left(\theta_t + \omega_{t}\right)}{e^{\omega_{t}}} \\
        \frac{d\omega_t}{dt} &= \frac{\sin{\left(\theta_t \right)}}{-0.67} - 0.48 \sin{\left(\left(\theta_t + 0.68\right) \sin{\left(\omega_{t} \right)} \right)}
        \end{aligned}$ &0.6774 &$\begin{aligned}
        \frac{dp_t}{dt} &= q_{t} \frac{1}{p_t + 0.63} \left(-0.022\right) \\
        \frac{dq_t}{dt} &= \frac{p_t}{\frac{p_t}{0.17} - q_{t} + q_{t} q_{t}} + \delta
        \end{aligned}$ &0.2673 &$\begin{aligned}
        \frac{dS_t}{dt} &= \frac{- S_t I_{t} - I_{t} + \cos{\left(S_t \right)}}{0.69} \\
        \frac{dI_t}{dt} &= 0.49 S_t + e^{\sin{\left(0.45 S_t \right)}} - 0.11 \\
        \frac{dR_t}{dt} &= \cos{\left(I_{t} \sin{\left(S_t \right)} \right)} \left(-0.087\right)
        \end{aligned}$ \\ 
\midrule
CoDA &0.9695 &$\begin{aligned}
        \frac{d\theta_t}{dt} &= \omega_{t} \rho \sin{\left(\theta_t + \omega_{t} + 0.94 \right)} + \omega_{t} + 0.074 \\
        \frac{d\omega_t}{dt} &= \frac{\sin{\left(- \theta_t + \omega_{t} \left(-0.20\right) + 0.37 \right)}}{0.49} - 0.57
        \end{aligned}$ &0.7097 &$\begin{aligned}
        \frac{dp_t}{dt} &= \left(1.1 - p_t p_t\right) 0.54 \\
        \frac{dq_t}{dt} &= \left(p_t + p_t\right) \left(p_t + q_{t} \left(-0.26\right)\right) - 0.66
        \end{aligned}$ &0.3184 &$\begin{aligned}
        \frac{dS_t}{dt} &= \frac{- S_t + \sin{\left(S_t \cdot 0.44 \right)} \left(-2.1\right)}{0.44} \\
        \frac{dI_t}{dt} &= \frac{3.5 S_t}{S_t + \frac{I_{t}}{S_t - 0.54}} + 0.097 \\
        \frac{dR_t}{dt} &= \frac{S_t}{I_{t} + I_{t} + \frac{e^{S_t}}{I_{t}}} + 0.029
        \end{aligned}$ \\ 
\midrule
IRM &0.7042 &$\begin{aligned}
        \frac{d\theta_t}{dt} &= \omega_{t} \cdot 0.93 \\
        \frac{d\omega_t}{dt} &= - \theta_t \alpha + \rho
        \end{aligned}$&0.6989 &$\begin{aligned}
        \frac{dp_t}{dt} &= -0.012 \\
        \frac{dq_t}{dt} &= 0.083
        \end{aligned}$ &0.9768 &$\begin{aligned}
        \frac{dS_t}{dt} &= e^{- S_t + \sin{\left(S_t \right)}} - 1.7 \\
        \frac{dI_t}{dt} &= \sin{\left(\frac{S_t}{S_t + S_t + \frac{\beta}{S_t}} - -0.12 \right)} \\
        \frac{dR_t}{dt} &= 0.33 - 0.021 S_t
        \end{aligned}$ \\ 
\midrule
VREx &0.7274 &$\begin{aligned}
        \frac{d\theta_t}{dt} &= \omega_{t} \cdot 0.92 \\
        \frac{d\omega_t}{dt} &= \alpha \left(-1.1\right) \theta_t
        \end{aligned}$&0.6877 &$\begin{aligned}
        \frac{dp_t}{dt} &= -0.032 \\
        \frac{dq_t}{dt} &= 0.12
        \end{aligned}$ &0.4652 &$\begin{aligned}
        \frac{dS_t}{dt} &= S_t \left(- I_{t} - 0.10 \beta\right) \\
        \frac{dI_t}{dt} &= \frac{S_t \beta}{S_t + S_t + \frac{\beta}{S_t}} + 0.078 \\
        \frac{dR_t}{dt} &= 0.070 + \frac{\sin{\left(\beta \right)}}{e^{I_{t}}}
        \end{aligned}$ \\ 
\midrule
Ours &0.3561 &$\begin{aligned} \frac{d\theta_t}{dt} &= 0.99 \omega_{t} \\ \frac{d\omega_t}{dt} &= -0.97 \alpha^2 \sin{\left(\theta_t \right)}  \end{aligned}$ &0.6194 &$
\begin{aligned}
\frac{dp_t}{dt} &= 1.254 p_t -0.38 q_t p_t  \\
\frac{dq_t}{dt} &= 4.1 p_t - 0.30 q_t - \gamma
\end{aligned}
$ &0.0652 &$\begin{aligned} \frac{dS_t}{dt} &= - 1.7 S_t I_{t} \\ \frac{dI_t}{dt} &= 0.42 S_t I_{t} \\ \frac{dR_t}{dt} &= -0.0088 \end{aligned}$ \\ 
\midrule
GT & NAN & $\begin{aligned}
    \frac{d\theta_t}{dt} &= \omega_{t}  \\ 
    \frac{d\omega_t}{dt} &= - \alpha^2 \sin{\left(\theta_t \right)}
    \end{aligned}$ & NAN& $\begin{aligned}
        \frac{dp_t}{dt} &= \alpha p_t - \beta p_t q_t \\ 
        \frac{dq_t}{dt} &= \delta p_t q_t - \gamma q_t
        \end{aligned}$ & NAN& $\begin{aligned} \frac{dS_t}{dt} &= - \beta \frac{S_t I_{t}}{S_t + I_t + R_t} \\ \frac{dI_t}{dt} &= \beta \frac{S_t I_{t}}{S_t + I_t + R_t} \\ \frac{dR_t}{dt} &= 0 \end{aligned}$  \\
\bottomrule
\end{tabular}}
\vspace{-0.4cm}
\end{table}


\subsection{Symbolic Regression Explanation Comparisons}\label{app:symbolic_explanation}

To further evaluate the performance differences between the proposed method and the baselines, we apply PySR to the four baseline methods and obtain analytical explanations, as summarized in Tab.~\ref{tab:symbolic_regression_comparisons}. The symbolic regression results provide an intuitive understanding of performance in relation to different NRMSE values. Specifically, when the NRMSE approaches 1, the resulting explanations are largely meaningless. As the NRMSE decreases to around 0.7, the explanations become more interpretable but may sometimes converge to oversimplified expressions, such as the IRM and VREx results on ME-Lotka-Volterra. When the NRMSE approaches zero, the expressions become more reasonable but are not always ideal due to the inherent limitations of PySR. This suggests that strong model performance does not necessarily guarantee high-quality explanations, highlighting the performance constraints of the explainer (PySR).

\subsection{Extra Visualizations}

\begin{figure}[h]
    \centering
    \begin{subfigure}[b]{0.49\linewidth}
        \includegraphics[width=1\linewidth]{figures/result_combine_lv.png}
        \caption{$X$ predictions using $\hat{f}$}
        \label{fig:results_combine_me_lv}
    \end{subfigure}
    \begin{subfigure}[b]{0.49\linewidth}
        \includegraphics[width=1\linewidth]{figures/result_inv_lv.png}
        \caption{$X^c$ predictions using $\hat{f}_c$}
        \label{fig:results_inv_me_lv}
    \end{subfigure}
    \caption{\textbf{Visualization of trajectory predictions on ME-Lotka-Volterra}}
    \label{fig:results_me_lv}
    \vspace{-0.5cm}
\end{figure}

\begin{figure}[h]
    \centering
    \begin{subfigure}[b]{0.49\linewidth}
        \includegraphics[width=1\linewidth]{figures/result_combine_sir.png}
        \caption{$X$ predictions using $\hat{f}$}
        \label{fig:results_combine_me_sir}
    \end{subfigure}
    \begin{subfigure}[b]{0.49\linewidth}
        \includegraphics[width=1\linewidth]{figures/result_inv_sir.png}
        \caption{$X^c$ predictions using $\hat{f}_c$}
        \label{fig:results_inv_me_sir}
    \end{subfigure}
    \caption{\textbf{Visualization of trajectory predictions on ME-SIREpidemic}}
    \label{fig:results_me_sir}
    \vspace{-0.5cm}
\end{figure}

In this section, we present visualization comparisons for ME-Lotka-Volterra (Fig.~\ref{fig:results_me_lv}) and ME-SIREpidemic (Fig.~\ref{fig:results_me_sir}). The results for ME-SIREpidemic closely align with its quantitative findings. For the more challenging task of ME-Lotka-Volterra, our method's predicted trajectories remain closer to the ground truth. In the $X^c$ predictions, where most methods fail, our predicted trajectory has turning points closest to the ground truth in terms of timing, although there are deviations in magnitude (Fig.~\ref{fig:results_inv_me_lv}). The complexity of the ME-Lotka-Volterra task arises from several factors, including the introduction of exponential functions within environments, the distribution of environments, and the limited number of samples in each environment. Addressing these challenges requires carefully designed benchmarks by domain experts, which we will discuss further in the limitations section.


\section{Efficient Hypernetwork Implementation}\label{app:efficient_hypernetwork_implementation}

\begin{wrapfigure}{R}{0.3\textwidth}
    \includegraphics[width=1\linewidth]{figures/hypernetwork_trick.pdf}
    \caption{\textbf{Reference-based hypernetwork implementation.}}\label{fig:hypernetwork_trick}
\end{wrapfigure}
One of the major challenges that limits the usage of hypernetworks is the implementation complexity. Most current implementations requires either re-implementing basic neural networks~\citep{oshg2019hypercl} or assigning predicted weights to the main function (forecaster) one by one for each forward pass~\citep{ortiz2023nonproportional, sudhakaran2022, Kirchmeyer2022coda}. To overcome these issues, we propose a \emph{Reference-based} hypernetwork implementation technique that uses pure PyTorch without introducing any new modules or CUDA kernels. Our proposed technique does not require reassigning weights for each sample in one forward pass, \ie, for any continuous $N$ training iterations with batch size of $B$ and a forecaster with $M$ parameter variables, our computation complexity is $O(NM + BM)$, instead of $O(BMN)$ as previous implementations.

Specifically, for every forward pass, we create a function parameter vector buffer $\in \R^m$ with fixed storage space, instead of reshaping and assigning the predicted function parameters with complexity $O(BM)$. As shown in Fig.~\ref{fig:hypernetwork_trick}, we consider the derivative neural network parameter variables as storage space pointers, \ie, the network stores references instead of matrices. The fractions of function parameter vector buffer are pointed by these pointers; thus, once the buffer's values change by the predicted function parameters, \eg, $\hat{f}$, the derivative network's parameters will be changed automatically without any assignment operators. {To maintain the buffer's fixed storage space, several in-place operations are applied to maintains computational graphs and gradients.}

\subsection{Efficiency Comparisons}

% \begin{wraptable}[14]{R}{1.0\textwidth}
%     \centering
%     \begin{tabular}{l|ccc|cc}
%         \toprule
%         Method & Vectorization & Copy & Reference & First Step Time (s) & Avg Time $\pm$ Std (s) \\
%         \midrule
%         Naïve & \xmark & \cmark & \xmark & 0.2466 & 0.1818 $\pm$ 0.0601 \\
%         Module-based & \cmark & \cmark & \xmark & 0.1768 & 0.1513 $\pm$ 0.0737 \\
%         Functional-based & \cmark & \xmark & \xmark & 0.2013 & 0.0198 $\pm$ 0.0007 \\
%         Ours & \cmark & \xmark & \cmark & 0.1805 & 0.0108 $\pm$ 0.0006 \\
%         \bottomrule
%     \end{tabular}
%     \caption{Caption}
%     \label{tab:my_label}
% \end{wraptable}

\begin{table}[tb]
    \centering
    \caption{\textbf{Hypernetwork implementation efficiency comparisons}}
    \resizebox{1\textwidth}{!}{
    \begin{tabular}{l|ccc|ccc}
        \toprule
        Implementation & Vectorization & Copy & Reference & First Step Time (s) & Avg Time $\pm$ Std (s) & Speedup\\
        \midrule
        Non-vectorized & \xmark & \cmark & \xmark & 0.2466 & 0.1818 $\pm$ 0.0601 & 1x\\
        Module-based & \cmark & \cmark & \xmark & 0.1768 & 0.1513 $\pm$ 0.0737 & 1.2x\\
        Functional-based & \cmark & \xmark & \xmark & 0.2013 & 0.0198 $\pm$ 0.0007 & 9.2x\\
        Ours & \cmark & \xmark & \cmark & 0.1805 & 0.0108 $\pm$ 0.0006 & 16.8x\\
        \bottomrule
    \end{tabular}}
    \label{tab:hypernetwork_efficiency}
\end{table}

% To validate the effectiveness of our hypernetwork implementation trick, we compare our implementation with several common types of implementations. Specifically, we run our model's forward pass for continuous 200 iterations in traning mode, and compare the time of the first iteration/step and the average time of the rest iterations. Generally, every implementation takes similar time for the first iteration, but they perform significantly different after the first step. As shown in the Tab~\ref{tab:hypernetwork_efficiency}, the Non-vectorized type represents all the methods~\citep{ortiz2023nonproportional} that do not vectorize the derivative function, and require running different derivative functions sequentially. Methods like CoDA~\citep{Kirchmeyer2022coda} vectorize model by using group-based convolution networks. But these module-based implementations use stateful PyTorch modules and require to replicate the derivative function module each forward pass, slowing down the speed. The Module-based implementation slightly outperform Non-vectorized ones attributing to the speedup of the vectorization.  In addition, we implement a vectorized functional-based implementation based on the PyTorch functional methods and gains about 10x speedup, where we may also vectorize the modules in libraries like hypnettorch~\citep{oshg2019hypercl} to achieve the same speedup. Finally, with our reference-based implementation that fully eliminate the parameter assignment after the first iteration, the hypernetwork forwarding speed can be double of the ones requiring parameter assignments. It is worth noting that this trick is still valid for possible future CUDA-based parallel hypernetwork implementations.

To evaluate the efficiency of our hypernetwork implementation, we compare it against several common implementation approaches. Specifically, we measure the forward pass time of our model over 200 continuous iterations in training mode, recording both the time for the first iteration and the average time for the subsequent iterations. While the first iteration typically takes a similar amount of time across all implementations, their performance diverges significantly in the subsequent iterations. As shown in Tab.~\ref{tab:hypernetwork_efficiency}, the \textit{Non-vectorized} implementation
% , as seen in previous works~\citep{ortiz2023nonproportional}, 
represents methods that do not vectorize the derivative function and therefore must run different derivative functions sequentially. Approaches like CoDA~\citep{Kirchmeyer2022coda} attempt to vectorize the model by employing group-based convolution networks. However, these module-based implementations rely on stateful PyTorch modules, requiring the derivative function module to be replicated during each forward pass, which slows down the process. While these \textit{Module-based} implementations offer a slight improvement over non-vectorized methods due to the vectorization benefits, the performance gain is limited. 

In contrast, our vectorized \textit{Functional-based} implementation leverages PyTorch's functional methods, achieving 9.2x speedup by avoiding the overhead associated with stateful modules. Note that vectorizing hypernetworks using libraries such as hypnettorch~\citep{oshg2019hypercl} can deliver similar speedups. Finally, our \textit{Reference-based} implementation, which eliminates parameter assignment after the first iteration, nearly doubles the forward pass speed (16.8x) compared to implementations that require such assignments. Notably, this optimization remains applicable for potential future CUDA-based parallel hypernetwork implementations. 

% \section{Supplementary Explanations}

% \begin{figure}[!t]
% % \vspace{-0.5cm}
%     \centering
%     \includegraphics[width=0.9\linewidth]{figures/explanation.pdf}
%     \caption{A function learning example. }
%     \label{fig:explanation}
%     \vspace{-0.5cm}
% \end{figure}

% The diagram~\ref{fig:explanation} illustrates the conceptual function learning process associated with the DIF framework, exemplified by the prediction of \(\frac{d \omega}{dt}\). Each (hidden) function representation, such as \(\hat{z}_c\), \(\hat{z}_e\), \(\hat{f}_c\), and \(\hat{f}_e\), \textit{implicitly} encodes both a symbolic function form and a value-assignment/substitution rule. For instance, \(\hat{f}_c\) represents two types of information: (1) the function form, e.g., \(-\alpha^2 \sin(\theta_t)\), and (2) a partial assignment of values, such as \(\{\alpha \rightarrow 1.5, \theta_t \rightarrow \}\), where \(\alpha\) is assigned a value of 1.5 while \(\theta_t\) remains a symbolic variable. This implies that \(\hat{f}_c\) represents the function \(-1.5^2 \sin(\theta_t)\), which can accept input values in the form \(\{\theta_t \rightarrow ?\}\), effectively evaluated as \(\hat{f}_c(?)\). Similarly, \(\hat{f}\) is a function defined by two symbolic variables and accepts inputs in the form \(\{\theta_t \rightarrow ?, \omega_t \rightarrow ?\}\). In this specific example, the input to \(\hat{f}\) is \(\{\theta_t \rightarrow 1.2, \omega_t \rightarrow 0.4\}\), resulting in the evaluation \(\hat{f}(1.2, 0.4) = -2.23\). 



% \section{Limitations and Future Work}\label{app:limitations_and_future_work}

% We discuss the limitations and the cover scope of this work; thus, shedding light on potential future directions accordingly. 

% % First, in this invariant function learning tasks, we only target the function combinations like $f(x) + g(x)$ and $f(x)(1+g(x))=f(x) + f(x)g(x)$ where the invariant and the environment functions are entangled by additions. We ignore the scenarios that the invariant and environment functions are entangled in forms like multiplications. Therefore, it may be of interest to disentangle more complex scenarios. However, for physical systems, given proper derivatives, factors are generally combined by additions, making the addition situations we considered important and widely applicable.

% Second, in this work, we only consider invariant function learning on ODE systems; thus, it will be of interest to explore invariant function in PDE systems.

% % Third, although we prove the invariant function learning principle using information theory, it would still be quite significant to further build invariant error bounds for this invariant function learning task using VC-Dimension~\cite{vapnik2015uniform} or PAC-Bayesian~\cite{mcallester1998some,catoni2007pac}, which would provide more in-depth insights.

% % Forth, in this work, we propose 4 multi-environment ODE systems, but we are still lacking comprehensive benchmarks that built by domain experts. There are many questions remaining, such as how many environments are sufficient to infer invariant function, what are the requirements of the environment distributions to make it possible for invariant function learning.

% Last but not least, unlike meta-parameters, the learned invariant functions have broader adaptation capabilities. For instance, a learned physical law can generalize to any system. It would be fascinating to explore applications of invariant function learning, such as learning generalizable physics from videos or developing physics-aware agents. Additionally, it would be interesting to investigate whether invariant function learning can contribute to the development of foundational models in physics.

\end{document}


% This document was modified from the file originally made available by
% Pat Langley and Andrea Danyluk for ICML-2K. This version was created
% by Iain Murray in 2018, and modified by Alexandre Bouchard in
% 2019 and 2021 and by Csaba Szepesvari, Gang Niu and Sivan Sabato in 2022.
% Modified again in 2023 and 2024 by Sivan Sabato and Jonathan Scarlett.
% Previous contributors include Dan Roy, Lise Getoor and Tobias
% Scheffer, which was slightly modified from the 2010 version by
% Thorsten Joachims & Johannes Fuernkranz, slightly modified from the
% 2009 version by Kiri Wagstaff and Sam Roweis's 2008 version, which is
% slightly modified from Prasad Tadepalli's 2007 version which is a
% lightly changed version of the previous year's version by Andrew
% Moore, which was in turn edited from those of Kristian Kersting and
% Codrina Lauth. Alex Smola contributed to the algorithmic style files.
