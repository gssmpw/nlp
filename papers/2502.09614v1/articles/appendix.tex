


% \todo{complete the intro part}



\noindent\textbf{Overview.} 
The \textbf{Appendix} provides a list of materials to support the main paper. 
\begin{itemize}
    \item \textbf{Additional Technical Explanations (Sec.~\ref{sec:supp_method}).} We give additional explanations to complement the main paper. 
    \begin{itemize}
        \item \textit{Data Preprocessing (Sec.~\ref{sec:supp_method_dta_preprocess})}.  We present details of the kinematics retargeting strategy we leverage to create a dexterous kinematic robot hand manipulation dataset from human references. 
        \item \textit{Tracking Controller Training (Sec.~\ref{sec:supp_method_tracking_controller_train})}. We explain additional details in the RL-based training scheme design, including the observation space and the reward. We also explain the control strategy for a floating base dexterous hand.  
        \item \textit{Homotopy Generator Learning (Sec.~\ref{sec:supp_method_homotopy_generator})}. We explain details in the homotopy generator learning.
        % , including the homotopy path-searching process, 
        % We explain additional details in the RL-based training scheme design, including the observation space and the reward. We also explain the control strategy for a floating base dexterous hand.  
        % \item  \textit{Mining valid tracking curricula}. We discuss details of the searching-based strategy to mine valid tracking curricula from the dataset. 
        % \item  \textit{Object features.} We present details w.r.t. how we get object features which is a part of the observation of the tracking policy. 
        % \item  \textit{Reward.} We discuss an additional bonus reward design which is leveraged to encourage task success. 
        \item  \textit{Additional Details (Sec.~\ref{sec:supp_method_additional_details})}. We present additional details w.r.t. the techniques. 
    \end{itemize}
    % % on some details unstated in the main paper, including 
    %     \begin{itemize}
    %         \item \textit{The \rep representation (Sec.~\ref{sec_appen_rep_design})}.  We discuss more insights into the representation design, why \rep can highlight errors, and how it compares to representations designed in previous works. 
    %         \item \textit{\rep Diffusion (Sec.~\ref{sec_genenoh_diffusion_details}).} We talk more about the whitened noise space, the diffusion process, the multi-step stochastic denoising process, the ``denoising via diffusion'' strategy, and the \textit{progressive denoising}, together with a discussion on why each denoising stage can successfully clean the input without breaking the naturalness achieved after previous stages. 
    %         \item  \textit{Fitting for a hand mesh trajectory (Sec.~\ref{sec_method_details_fitting}).} We provide details of the fitting process. 
    %     \end{itemize}
    \item  \textbf{Additional Experimental Results (Sec.~\ref{sec:supp_exp}).}  We include more experimental results in this section to support the effectiveness of the method, including 
    \begin{itemize}
        \item \textit{Dexterous Manipulation Tracking Control (Sec.~\ref{sec:supp_exp_dex_manip_control}).} We present additional experiments of our methods as well as additional comparisons, including results achieved by using different training settings and additional qualitative results. We will also discuss more generalization ability evaluation experiments. 
        \item \textit{Real-World Evaluations (Sec.~\ref{sec:supp_real_world})}.  We include more results of the real-world evaluations. 
        \item \textit{Analysis on the Homotopy Optimization Scheme (Sec.~\ref{sec:supp_exp_curriculum}).} We present qualitative results achieved by the proposed homotopy optimization method and comparisons to demonstrate the capability of the homotopy optimization as well as the effectiveness of the homotopy optimization path generator. 
        % \todo{Add the results}
        \item  \textit{Failure Cases (Sec.~\ref{sec:failurecase}). } We discuss the failure cases for a comprehensive evaluation and understanding w.r.t. the ability of our method. 
        % \item \tood
    \end{itemize}
        % \begin{itemize}
        %     \item \textit{HOI denoising results (Sec.~\ref{sec_appen_hoi_denoising_exp})}. We include more denoising results on GRAB, GRAB (Beta), HOI4D, and the ARCTIC dataset, including \textit{long sequences with bimanual manipulations}. Besides, we discuss the results of \textit{another series of experiments} where the training dataset is changed to the training set of the \textit{ARCTIC dataset}.  
        %     % Besides, we conduct another series of experiments by using the training set of the ARCTIC dataset as the training data. 
        %     \item  \textit{Ablation studies (Sec.~\ref{seq_abalations_redudies})}. We provide more quantitative and qualitative results of the ablation studies. 
        %     \item \textit{Applications (Sec.~\ref{sec_appen_applications})}. We provide more results on the applications that our model can support. 
        %      % \item \textit{More HOI denoising results (Sec.~\ref{sec_appen_hoi_denoising_exp})} on GRAB, GRAB (Beta), HOI4D and the ARCTIC dataset. 
        %      \item \textit{Failure cases (Sec.~\ref{sec_supp_faliure_cases})}. We discuss the limitations and failure cases of our method. 
        %      % \subsection{Analyzing the Distinction between Noise in Real Hand-Object Interaction Trajectories and Artificial Noise} \label{sec_appen_noise_real_arti}
        %      % \textcolor{myblue}{
        %      \item  \textit{Analyzing the distinction between noise in real hand-object interaction trajectories and artificial noise (Sec.~\ref{sec_appen_noise_real_arti})}. We discuss the differences between the real noise patterns and the artificial noise. 
        %      \item  \textit{User study (Sec.~\ref{sec_appen_user_study})}. We additionally include a user study to further assess the quality of our denoised results. 
        %      % }
        % \end{itemize}
    \item  \textbf{Experimental Details (Sec.~\ref{sec:supp_exp_details})}. We illustrate details of datasets, models, training and evaluation settings, simulation settings, real-world evaluation settings, and the running time as well as the complexity analysis. 
    % metrics, baselines, models, the training and evaluation settings, and the running time as well as the complexity analysis. 
\end{itemize}

% \todo{complexity analysis?}

We include a \href{https://projectwebsite7.github.io/gene-dex-manip/static/videos-lowres/video_7.mp4}{\textbf{video}} and an \href{https://projectwebsite7.github.io/gene-dex-manip/}{\textbf{website}} to introduce our work. The website and the video contain animated results. We highly recommend exploring these resources for an intuitive understanding of the challenges, the effectiveness of our model, and its superiority over prior approaches.




\section{Additional Technical Explanations} \label{sec:supp_method}

\subsection{Data Preprocessing} \label{sec:supp_method_dta_preprocess}


% \todo{kinematics retargeting}

\noindent\textbf{Kinematic retargeting. } 
We curate kinematic robot-object interaction data from human references by retargeting robot hand manipulation sequences from human hand trajectories. For instance, given a human hand-object interaction trajectory describing the human hand pose sequences represented in MANO and the object pose sequences $(\mathbf{H}^{\text{human}}), \mathbf{O}$ as well as the description of the articulated robot hand, we retarget $\mathbf{H}^{\text{human}}$ to acquire the robot hand trajectory $\mathbf{H}$. We manually define correspondences between the robot hand mesh and the MNAO hand mesh. After that, the sequence of the robot hand DoF positions is optimized so that the resulting robot hand mesh sequence is close to the human hand sequence. Formally, let $\mathbf{K}^{\text{human}}$ and $\mathbf{K}$ denote the human hand keypoint sequence and the robot hand keypoint sequence respectively, the optimization objective is:
\begin{align}
    \text{minimize} \Vert \mathbf{K} - \mathbf{K}^{\text{human}} \Vert. 
\end{align}
We use PyTorch\_Kinematics~\citep{Zhong_PyTorch_Kinematics_2024} to calculate the forward kinematics. Specifically, given the robot hand per-joint DoF position $\mathbf{\theta}_n$ at the timestep $n$, we calculate $\mathbf{h}_n$ and $\mathbf{k}_n$ as follows:
\begin{align}
    \mathbf{h}_n &= \text{Forward\_Kinematics}(\mathbf{\theta}_n), \\
    \mathbf{k}_n &= \text{KeyPoints}(\text{Forward\_Kinematics}(\mathbf{\theta}_n)),
\end{align}
where $\text{Forward\_Kinematics}(\cdot)$ computes the forward kinematics using the function provided in PyTorch\_Kinematics, $\text{KeyPoints}(\cdot)$ reads out keypoints from the converted articulated mesh. 
% To track human references, we 
% Instead of direct
% To curate kinematic robot-object interaction data from human reference




\subsection{Tracking Controller Training} \label{sec:supp_method_tracking_controller_train}




\noindent\textbf{Controlling a floating-base articulated hand.} The articulated hand is represented in the reduced coordinate $\mathbf{\theta}^{\text{finger}}$. We additionally add three translation joints and three revolute joints to control the global position and orientation of the hand, resulting in $\mathbf{\theta} = (\mathbf{\theta})^{\text{trans}}, \mathbf{\theta}^{\text{rot}}, \mathbf{\theta}^{\text{rot}})$. For the Allegro hand and the LEAP hand that we use in our experiments, $\mathbf{\theta}^{\text{finger}}$  is a 16-dimensional vector. Therefore, $\mathbf{\theta}$ is a 22-dimensional vector. 



\noindent\textbf{Observations.} 
The observation at each timestep $n$ encodes the current hand and object state, the next goal state, baseline trajectory, actions, and the object geometry: 
\begin{align}
    \mathbf{o}_n = \{ \mathbf{s}_n, \dot{\mathbf{s}}_n,  \hat{\mathbf{s}}_{n+1}, \mathbf{s}^b_n, \mathbf{a}_n, \text{feat}_{\text{obj}}, \text{aux}_n \}. \label{eq_obs}
\end{align}
where $\text{aux}_n$ is the auxiliary features, computed as follows:
\begin{align}
    \text{aux}_n = \{ \hat{\mathbf{s}}_{n+1}, \mathbf{f}_n, \hat{\mathbf{s}}_{n+1} \ominus {\mathbf{s}}_n,    \}, \label{eq:observations_aux}
\end{align}
where $\hat{\mathbf{s}}_{n+1}\ominus \ominus {\mathbf{s}}_n$  calculates the difference between two states, including the hand state difference and the object state difference, $\mathbf{f}_n$ indicates the hand finger positions in the world space.



% \todo{details of the observation} 

\noindent\textbf{Reward.} Our reward for manipulating tracking encourages the transited hand state and the object state to be close to their corresponding reference states and the hand-object affinity: 
\begin{align}
    r = w_{o, p} r_{o, p} + w_{o, q} r_{o, q} + w_{\text{wrist}} r_{\text{wrist}} + w_{\text{finger}} r_{\text{finger}} + w_{\text{affinity}} r_{\text{affinity}}, \label{eq_rew}
\end{align}
where $r_{o,p},r_{o, q},r_{\text{wrist}},r_{\text{finger}}$ are rewards for tracking object position, object orientation, hand wrist, hand fingers, $w_{o, p},  w_{o, q}, w_{\text{wrist}}, w_{\text{finger}},  w_{\text{affinity}}$ are their weights. 
 $r_{o,p},r_{o, q},r_{\text{wrist}},r_{\text{finger}}$ are computed as follows:
\begin{align}
    r_{o,p} &= 0.9 - \Vert \mathbf{p}_n^o - \hat{\mathbf{p}}_n^o \Vert_2, \\
    r_{o,q} &= \text{np.pi} - \text{Diff\_Angle}(\mathbf{q}_n^o - \hat{\mathbf{q}}_n^o )), \\
    r_{\text{wrist}} &= -(w_{\text{trans}} \Vert \mathbf{s}_n^h[:3] - \hat{\mathbf{s}}_n^h[:3] \Vert_1 + w_{\text{ornt}} \Vert \mathbf{s}_n^h[3:6] - \hat{\mathbf{s}}_n^h[3:6] \Vert_1 \\
    r_{\text{finger}} &= -w_{\text{finger}} \Vert \mathbf{s}_n^h[6:] - \hat{\mathbf{s}}_n^h[6:] \Vert_1 
    % \label{eq:rew_obj} \\
\end{align}
where $\mathbf{p}^o_n$ and $\mathbf{q}_n^o$ denote the position and the orientation, represented in quaternion, of the current object, $\mathbf{s}_n^h$ denotes the current hand state. In addition to these rewards, we would add an additional bonus reward $1$ if the object is accurately tracked, \emph{i.e.,} with the rotation error kept in $5$-degree and the translation error kept in $5$-cm. 


\noindent\textbf{Pre-processing object features.} We train PonintNet-based auto-encoder on all objects from the two datasets we considered, namely GRAB and TACO. After that, we use the latent embedding of each object as its latent feature feeding into the observation of the tracking controller. The object feature dimension is 256 in our experiments. 


% \todo{deteails of the reward}


\subsection{Homotopy Generator Learning} \label{sec:supp_method_homotopy_generator}

% \todo{how to set the iteration K}



% Our reward for manipulating tracking comprises three components: the hand reward $r_{h}$, the object reward $r_o$, and the hand-object affinity reward $r_d$. The object reward encourages the transited object position and the orientation close to their kinematic references: $r_o = w_p r_{o,p} + w_q r_{o,q}$. 
% The object reward includes both position and orientation rewards, defined as: 
% \begin{align}
%     r_o = w_p(0.9 - \Vert \mathbf{s}_n^p - \hat{\mathbf{o}}_n^p \Vert_2 ) + w_q(\text{np.pi} - \text{Diff\_Angle}(\mathbf{o}_n^q - \hat{\mathbf{o}}_n^q )), \label{eq:rew_obj}
% \end{align}
% where $w_p$ and $w_q$ are the weights for position and orientation rewards, respectively, and $\text{Diff\_Angle}(\cdot, \cdot)$  computes the rotation angle difference between two quaternions. The hand reward is calculated as the negative weighted sum of the current hand joint positions and the reference positions:
% \begin{align}
%     r_h = -(w_{\text{trans}} \Vert \mathbf{h}_n[:3] - \hat{\mathbf{h}}_n[:3] \Vert_1 + w_{\text{ornt}} \Vert \mathbf{h}_n[3:6] - \hat{\mathbf{h}}_n[3:6] \Vert_1 + w_{\text{finger}} \Vert \mathbf{h}_n[6:] - \hat{\mathbf{h}}_n[6:] \Vert_1  ), \label{eq:rew_hand}
% \end{align}
% we include in the observation, 
% The tracking controller for manipulation should be aware of the geometry of the object. So we include the object feature, produced by a pre-trained object point cloud encoder, into the state. The final state contains 



% \todo{deteails of the reward}




% The BFS-based approach for constructing the task relational space and the conditional diffusion model training are detailed as follows. 

% \todo{ add a picture for this structural space construction}

% \todo{add the retargeted motions such as same trajectory different objects}

% \todo{details w.r.t. how we identify the cross task affinities? }

% \todo{extended task set?}




\noindent\textbf{Mining effective homotopy optimization paths.} The maximum number of iterations $K$ is set to $3$ in our method to balance between the time cost and the effectiveness. 

We need to identify neighbors for each tracking task so that we can avoid iterating over all tasks and reduce the time cost. We use the cross-kinematic trajectory similarity to filter neighboring tasks. We pre-select $K_{\text{nei}}=$10 neighbouring tasks for each tracking task. 
% Given all manipulation trajectories from the training dataset $\mathcal{S}$, the tracking curricula mining wants to find valid curricula from them. A curriculum ($B\rightarrow A$) is regarded as valid if we can solve the tracking problem of trajectory $A$ better by initializing its basic trajectory as the successfully tracked action trajectory of $B$. To reduce the time consumption, we propose to first identify a potential neighbor set containing $K_{\text{nei}}$ trajectories for each trajectory via cross-trajectory similarities. After that, during each searching iteration, for each task $A$, we only try to test trajectories in its neighborhood set to see whether they can form a valid curriculum in that iteration. Such a strategy reduces the time complexity to $N_{\text{iter}} K_{\text{nei}} \vert \mathcal{S} \vert$, where $N_{\text{iter}}$ is the number of the searching iterations. 

% We simply calculate the per-frame average hand DoF position differences and the

% The structural task relational space wants to quantify the cross-task relations for each two different tasks in the expanded task set. One can imagine a most ideal way that is trying to test each pair of two manipulation trajectories $(A, B)$ to test whether one task, for instance, task $B$ can benefit from task $A$. However, such a strategy is too expensive with the time complexity $\vert \mathcal{S}\vert^2$. If we wish not to be restricted to the first-order cross-task relations only and also want to explore the high-order relations, for instance, the current task $C$ can benefit from the optimization result of task $B$ with the initialization of task $A$, the brute-force searching would cost $K\vert \mathcal{S}\vert^2$. Therefore, we adopt a heuristic-guided searching strategy to reduce the time consumption to an affordable amount. 
% we first construct an expanded task set by augmenting each single task $(\mathbf{H}_A, \mathbf{O}_A, m_A)$ into $(\mathbf{H}_C, \mathbf{O}_A, m_A)$ and $(\mathbf{H}_A, \mathbf{O}_A, m_C)$. 

% Specifically, we first identify potential neighbors for each task via trajectory similarities and object geometry similarities. Having restricted the number of potential neighbors to a reasonable value, \emph{i.e.,} $K_{\text{nei}}$, we only conduct the affinity test between each potential neighbor. With this strategy, we can reduce the time consumption to $K K_{\text{nei}} \vert \mathcal{S}\vert$. By tuning $K$ and $K_{\text{nei}}$, we can balance the accuracy and the efficiency of the task relation space construction process. Please refer to the Supp. For more details. 
% Besides, it may overlook the high-order task relations, where the current task $C$ can benefit from the optimization result of task $B$ with the initialization of task $A$.  

% Specifically, \textcolor{red}{(TODO: add the description for the BFS part)}

% \noindent\textbf{Conditional diffusion model for task relation modeling.} 
% \todo{how we obtain the pairs to train the diffusion model -- both the high order and the first order} 
% \todo{how to train the model}




% \todo{detals w.r.t. obj features}

% \todo{add the bonus reward}

% \noindent\textbf{Additional bonus rewards.} In addition to the hand tracking reward $r_h$, object tracking reward $r_o$, and the affinity reward $r_d$, we would add an additional bonus reward $1$ if the object is accurately tracked, \emph{i.e.,} with the rotation error kept in $5-$degree and the translation error kept in $5$-cm. 
% % We would add an additional bonus reward $1$ to the 

% \todo{re-check this part}

\subsection{Additional Explanations} \label{sec:supp_method_additional_details}

% \noindent\textbf{Additional explanations.} 
In the reward design, we do not include the velocity-related terms since it is impossible for us to get accurate velocities from kinematic references. One can imagine calculating the finite differences between adjacent two frames as the velocities. However, it may not be accurate. Therefore, we do not use them to avoid unnecessary noise. 


 
\section{Additional Experiments} \label{sec:supp_exp}

\subsection{Dexterous Manipulation Tracking Control} \label{sec:supp_exp_dex_manip_control}




\begin{table*}[t]
    \centering
    \caption{ 
    \textbf{Quantitative evaluations and comparisons.}  \bred{Bold red} numbers for best values.
    % and \iblue{italic blue} values for the second best-performed ones. 
    Models are trained on training tracking tasks from both the GRAB and the TACO datasets. 
    % using 
    % action-labeled trajectories from both the GRAB and the TACO datasets. 
    } 
    % \vspace{-10pt}
    \resizebox{1.0\textwidth}{!}{%
\begin{tabular}{@{\;}llccccc@{\;}}
        \toprule
        %         
        Dataset & Method & \makecell[c]{$R_{\text{err}}$ ($\text{rad}, \downarrow$)} & \makecell[c]{$T_{\text{err}}$ (${cm}, \downarrow$)}  &  $E_{\text{wrist}}$ ($ \downarrow$)  & $E_{\text{finger}}$ ($\text{rad}, \downarrow$) & Success Rate ($\%, \uparrow$) \\

        
        \cmidrule(l{0pt}r{1pt}){1-2}
        \cmidrule(l{2pt}r{2pt}){3-7}

        % \multirow{2}{*}{ GRAB } & DGrasp & - & -  & -  & - & - 
        % \\ 
        % ~ & PPO (OmniGrasp rew.) & 0.4404 & 6.69  & 0.1722 & 0.6418  & 35.97/54.94
        % \\ 
        \multirow{2}{*}{ GRAB } & PPO (w/o sup., tracking rew.)  &  0.5813 & 6.03  & {0.1730} & 0.5439  & 36.04/55.84 % & 
        \\ 
        
        \cmidrule(l{0pt}r{1pt}){2-2}
        \cmidrule(l{2pt}r{2pt}){3-7}
        

        
        ~ & \model  & \bred{0.4515} & \bred{4.82} & \bred{0.14574} & \bred{0.4574} & \bred{42.64}/\bred{61.42}
         \\ 
        
        \cmidrule(l{0pt}r{1pt}){1-2}
        \cmidrule(l{2pt}r{2pt}){3-7}


        \multirow{2}{*}{ TACO }  & PPO (w/o sup., tracking rew.)  & {0.6751} & 6.37  & \bred{0.1264}  & {0.5443} & 21.67/50.25
        \\ 
        
        \cmidrule(l{0pt}r{1pt}){2-2}
        \cmidrule(l{2pt}r{2pt}){3-7}



        ~ & \model  &  \bred{0.4782} &  \bred{3.94} & {0.1329 } & \bred{0.4228 } &  \bred{32.02}/\bred{62.07}
        \\ 

        \bottomrule
 
    \end{tabular}
    }
    \vspace{-10pt}
    \label{tb_exp_additional_jointly_trained_model}
\end{table*} 




\noindent\textbf{Training the tracking controller on two datasets.} In the main experiments, the training data and the test data come from the same dataset. We adopt such a setting considering the large cross-dataset trajectory differences. Specifically, GRAB mainly contains manipulation trajectories with daily objects, while TACO mainly covers functional tool-using trajectories. However, jointly using the trajectories from such two datasets to train the model can potentially offer us a stronger controller considering the increased labeled data coverage. Therefore, we additionally conduct this experiment where we train a single model using trajectories provided by the two datasets and test the performance on their test sets respectively. Results are summarized in Table~\ref{tb_exp_additional_jointly_trained_model}, which can still demonstrate the effectiveness of our approach. 
% However, using trajectories from two datasets 

% \todo{add the labeling details, the number of data, and how we select trajectories to train}

\noindent\textbf{Comparisons with labeling the whole dataset for training the controller. } In our method we only try to label a small fraction of data in a progressive way leveraging the power of the tracking prior provided by the tracking controller and the homotopy paths proposed by the homotopy generator so 
% curriculum scheduled by the curriculum scheduler so 
that we can get high-quality demonstrations within an affordable time budget. One may wonder whether it is possible to label all the training dataset trajectories and train the tracking controller using those demonstrations. And if possible, what's the performance of the model trained via this approach? We therefore manage to label each trajectory in the training dataset of the GRAB dataset by running the per-trajectory tracking experiments in parallel in 16 GPUs using two machines. 
We complete the optimization within one week. 
% labeling was completed within one week. 
After that, we use the resulting demonstrations to train 
% action-labeled trajectories to train 
the trajectory controller. The final model trained in this way achieves 42.13\% and 60.41\% success rates under two thresholds. The performance can still not reach our original method where we only try to optimize high-quality demonstrations for part of the data (see Table~\ref{tb_exp_main} for details). We suppose it is the quality difference between the two labeled datasets that causes the discrepancy. This further validates our assumption that both the quantity and the quality of the labeled dataset matter to train a good tracking controller. 
% Only part of the data is labeled 
% \todo{xxx}




\begin{wraptable}[5]{r}{0.6\textwidth}
\vspace{-23pt}
    \centering
    \caption{ 
    % \textbf{Ablation studies} on the GRAB dataset. 
    % \textbf{Generalization ability evaluations} on the TACO dataset. 
    % \textbf{Gen} on the TACO dataset. 
    \footnotesize
    Generalizability evaluations on the TACO dataset. 
    } 
    \vspace{5pt}
	% \resizebox{\linewidth}{!}{%
    \resizebox{0.6\textwidth}{!}{%
\begin{tabular}{@{\;}lccccc@{\;}}
        \toprule
        Test set & \makecell[c]{$R_{\text{err}}$ ($\text{rad}, \downarrow$)} & \makecell[c]{$T_{\text{err}}$ (${cm}, \downarrow$)}  &  $E_{\text{wrist}}$ ($ \downarrow$)  & $E_{\text{finger}}$ ($\text{rad}, \downarrow$) & Success Rate ($\%, \uparrow$)   \\

        % \\
        \midrule % 


        % \\
        % \cmidrule(l{15pt}r{17pt}){2-4}
        
        % \model (w/o SpatialDiff) & 2.94 & 3.45  & 31.67
        % \\ 

        % \model (w/o TemporalDiff) & \bred{1.72} & \bred{1.90} & 34.25 % 18.65 %
        % \\

        % \model (w/o Diffusion) & 3.16 & 3.83  & 18.65 

        % S0 & {0.3443} & {7.81} & {0.1225} & {0.5218} & 39.79/58.13/66.78
        % \\
        
        S1 & 0.5787 & {2.43} & {0.1481} & {0.4703} & 35.97/67.63
        \\ 

        % 
        % \model (w/o Canon.) & {2.36} & {3.57} & \iblue{13.26}
        S2 & {0.6026} & {2.46} & { 0.1455} & {0.4709} & 30.83/65.00
        \\ 

        S3  & {0.6508} & {8.06} & {0.1513} & {0.4683} & 10.18/{46.32}
        \\ 
        
        \bottomrule
 
    \end{tabular}
    }
    % }
    % \vspace{-16pt}
    \label{tb_exp_taco_generalization_test}
\end{wraptable}


\begin{figure}[ht]
  \centering
  % \vspace{-10pt}
  \includegraphics[width=\textwidth]{figs/robustness_newobj.pdf}
  % \vspace{-20pt}
  \caption{
  \textbf{Robustness towards out-of-distribution objects and manipulations. }
  Please refer to \textbf{\href{https://projectwebsite7.github.io/gene-dex-manip/}{our website} and {\href{https://projectwebsite7.github.io/gene-dex-manip/static/videos-lowres/video_7.mp4}{\textcolor{orange}{the accompanying video}}}} for animated results.
  }
  \label{fig_res_robustness_newobj}
  % \vspace{-10pt}
\end{figure}


\noindent\textbf{Further generalization ability evaluations on TACO dataset.}
We further evaluate the model's generalization ability across various test sets within the TACO dataset. As shown in Table~\ref{tb_exp_taco_generalization_test}, the controller performs well in the category-level generalization setting (S1), where object categories are known but manipulation trajectories and object geometries are novel. Performance on S2, involving novel interaction triplets, is satisfactory, demonstrating the controller's capacity to handle new manipulation sequences. However, results from S3 reveal challenges when dealing with new object categories and unseen interaction triplets. For instance, generalizing from interactions with shovels and spoons to using bowls for holding objects is particularly difficult. As shown in Figure~\ref{fig_res_robustness_newobj}, despite unfamiliar objects and interactions, we successfully lift the knife and mimic the motion, though execution is imperfect, highlighting areas for improvement and adaptability in challenging scenarios.

\noindent\textbf{Additional results.} We present additional qualitatively results in Figure~\ref{fig_additional_res} to further demonstrate the capability of our method. 


\begin{figure}[h]
  \centering
  % \vspace{-10pt}
  \includegraphics[width=\textwidth]{figs/additional_res_2.pdf}
  % \vspace{-20pt}
  \caption{
  \textbf{Additional qualitative comparisons. }
  Please refer to \textbf{\href{https://projectwebsite7.github.io/gene-dex-manip/}{our website} and {\href{https://projectwebsite7.github.io/gene-dex-manip/static/videos-lowres/video_7.mp4}{\textcolor{orange}{the accompanying video}}}} for animated results.
  }
  \label{fig_additional_res}
  % \vspace{-10pt}
\end{figure}





\subsection{Real-World Evaluations} \label{sec:supp_real_world}


\begin{table*}[t]
    \centering
    \caption{ \footnotesize
    \textbf{Real-world quantitative comparisons (GRAB dataset).}   \bred{Bold red} numbers for best values.
    % and \iblue{italic blue} values for the second best-performed ones. 
    } 
    % \vspace{-10pt}
        \resizebox{\textwidth}{!}{%
\begin{tabular}{@{\;}lcccccccccc@{\;}}
        \toprule
       Method & \texttt{apple} & \texttt{banana} & \texttt{duck}  & \texttt{elephant} & \texttt{flashlight}    & \texttt{flute}   & \texttt{hammer}  & \texttt{hand}  & \texttt{phone}  & \texttt{waterbottle}     \\

        % \\
        \midrule % 

        PPO (w/o sup., tracking rew) & 0/0/0 & 25.0/25.0/0.0 & 50.0/50.0/0 & 25.0/0.0/0.0 & 50.0/25.0/0 & 0/0/0 & 25.0/25.0/0 & 33.3/33.3/0 & 25.0/25.0/0 & 33.3/0/0
        \\ 

        % 
        \model & \bred{50.0}/\bred{50.0}/\bred{25.0} & \bred{50.0}/\bred{50.0}/\bred{25.0} & \bred{75.0}/{50.0}/\bred{50.0} & \bred{50.0}/\bred{50.0}/\bred{50.0} & \bred{75.0}/\bred{50.0}/\bred{25.0}  & \bred{25.0}/\bred{25.0}/{0.0} & \bred{50.0}/\bred{25.0}/\bred{25.0} & \bred{66.7}/\bred{66.7}/\bred{66.7} & \bred{25.0}/\bred{25.0}/\bred{25.0} & \bred{50.0}/\bred{50.0}/\bred{50.0} 
        \\ 

        % S3  & {0.6508} & {8.06} & {0.1513} & {0.4683} & 10.18/{46.32}
        % \\ 
        
        \bottomrule
 
    \end{tabular}
    }
    \vspace{-20pt}
    \label{tb_supp_real_world_results_open_loop}
\end{table*} 


% \begin{wraptable}[7]{r}{0.8\textwidth}
\begin{table*}[t]
    \centering
    \caption{ \footnotesize
    \textbf{Real-world quantitative comparisons (TACO dataset).}  \bred{Bold red} numbers for best values.
    % and \iblue{italic blue} values for the second best-performed ones. 
    } 
    % \vspace{-10pt}
        \resizebox{0.8\textwidth}{!}{%
\begin{tabular}{@{\;}lcccccc@{\;}}
        \toprule
       Method & \texttt{soap} & \texttt{shovel} & \texttt{brush}  & \texttt{roller} & \texttt{knife}    & \texttt{spoon}    \\

        % \\
        \midrule % 

        PPO (w/o sup., tracking rew) & 33.3/0/0 & 25.0/0.0/0.0 & 25.0/0/0 & 25.0/25.0/0.0 & 0/0/0 & 25.0/0/0 
        \\ 

        % 
        \model & \bred{100.0}/\bred{66.7}/\bred{66.7} & \bred{50.0}/\bred{25.0}/\bred{25.0} & \bred{25.0}/\bred{25.0}/{0.0} & \bred{50.0}/\bred{25.0}/\bred{25.0} & \bred{25.0}/\bred{25.0}/{0.0}  & \bred{50.0}/\bred{50.0}/\bred{25.0} 
        \\ 

        % S3  & {0.6508} & {8.06} & {0.1513} & {0.4683} & 10.18/{46.32}
        % \\ 
        
        \bottomrule
 
    \end{tabular}
    }
    % \vspace{-20pt}
    \label{tb_supp_real_world_results_open_loop_taco}
\end{table*} 
% \end{wraptable}


\begin{figure}[h]
  \centering
  % \vspace{-10pt}
  \includegraphics[width=\textwidth]{figs/real_res.pdf}
  % \vspace{-20pt}
  \caption{
  \textbf{Additional real-world qualitative results. }
  Please refer to \textbf{\href{https://projectwebsite7.github.io/gene-dex-manip/}{our website} and {\href{https://projectwebsite7.github.io/gene-dex-manip/static/videos-lowres/video_7.mp4}{\textcolor{orange}{the accompanying video}}}} for animated results.
  }
  \label{fig_additional_real_res}
  % \vspace{-10pt}
\end{figure}

\noindent\textbf{Success thresholds.} We define three levels of success rates. The first level of success is defined as reaching the object, finding a good grasp pose, and exhibiting the potential movements to lift the object up, \emph{i.e.,} one side of the object is successfully lifted up from the table. The second level of success is defined as finding a way to manage to lift the whole object up from the table. The third level of success is lifting the object up, followed by keeping tracking the object's trajectory for more than 100 timesteps. 

\noindent\textbf{More results.} For direct tracking results transferring setting, we present the quantitative success rates evaluated on our method and the best-performed baseline in Table~\ref{tb_supp_real_world_results_open_loop} (for the dataset GRAB) and Table~\ref{tb_supp_real_world_results_open_loop_taco} (for the dataset TACO). 
As observed in the table, the tracking results achieved by our method can be well transferred to the real-world robot, helping us achieve obviously better results than the baseline methods. It validates the real-world applicability of our tracking results. 

Please refer to the main text (Sec.~\ref{sec:exp}) for the quantitative comparisons between the transferred controllers. 
We include more qualitative results in Figure~\ref{fig_additional_real_res} to demonstrate the real-world application value of our method. 



\subsection{Analysis on the Homotopy Optimization Scheme} \label{sec:supp_exp_curriculum}


\begin{figure}[h]
  \centering
  % \vspace{-10pt}
  \includegraphics[width=\textwidth]{figs/curriculum_4.pdf}
  % \vspace{-20pt}
  \caption{
  \textbf{Effectiveness of the homotopy optimization scheme. }
  Please refer to \textbf{\href{https://projectwebsite7.github.io/gene-dex-manip/}{our website} and {\href{https://projectwebsite7.github.io/gene-dex-manip/static/videos-lowres/video_7.mp4}{\textcolor{orange}{the accompanying video}}}} for animated results.
  }
  \label{fig_curriculum}
  % \vspace{-10pt}
\end{figure}

We conduct further analysis of the proposed homotopy optimization scheme and the homotopy path generator to demonstrate their effectiveness. As shown in Figure~\ref{fig_curriculum}, by optimizing through the homotopy optimization path, we can get better results in per-trajectory tracking. 



\noindent\textbf{Lifting thin objects. } 
As demonstrated in Figure~\ref{fig_curriculum}a, for the originally unsolvable tracking problem where we should manage to lift a very thin flute up from the table, we can finally ease the tracking difficulty by gradually solving each tracking problem in the homotopy optimization path proposed by the generator. 
% in the curriculum proposed by the scheduler. 

\noindent\textbf{Grasping small objects. }
As shown in Figure~\ref{fig_curriculum}b, the original per-trajectory tracker fails to find a proper way to grasp the small sphere and lift it up from the table. However, empowered by the homotopy optimization, we can finally find a way to lift it up from the table. 
% tracking curriculum, 


\noindent\textbf{Lifting a round apple.} Figure~\ref{fig_curriculum}c demonstrates an effective homotopy optimization path that can let us lift an apple up from the table which would previously challenge the policy due to the round surface. 
% correct curriculum schedule can 

% \todo{some analysis on the curriculums}
% \todo{effectiveness of the curriculum}


% \todo{more examples}


% \todo{one single model training and testing}




% \todo{exp w.r.t. the coefficients of the supervised loss}

% \todo{add the details about the dataset}



\begin{figure}[h]
  \centering
  % \vspace{-10pt}
  \includegraphics[width=\textwidth]{figs/faliurecases.pdf}
  % \vspace{-20pt}
  \caption{
  \textbf{Failure Cases. }
  Please refer to \textbf{\href{https://projectwebsite7.github.io/gene-dex-manip/}{our website} and {\href{https://projectwebsite7.github.io/gene-dex-manip/static/videos-lowres/video_7.mp4}{\textcolor{orange}{the accompanying video}}}} for animated results.
  }
  \label{fig_failure_cases}
  % \vspace{-10pt}
\end{figure}

\subsection{Failure Cases} \label{sec:failurecase}

Our method may fail to perform well in some cases where the object is from a brand new category with challenging thin geometry, as demonstrated in Figure~\ref{fig_failure_cases}. 



\section{Additional Experimental Details} \label{sec:supp_exp_details}


% \todo{hardware setting and additional exp details}


% \todo{training time consumptions}


\begin{figure}[h]
  \centering
  % \vspace{-10pt}
  \includegraphics[width=0.7\textwidth]{figs/seen_obj_cat.pdf}
  % \vspace{-20pt}
  \caption{
  Examples of novel objects from the seen object category (TACO). 
  % \textbf{Additional qualitative comparisons. }
  % Please refer to \textbf{\href{https://projectwebsite7.github.io/gene-dex-manip/}{our website} and {\href{}{\textcolor{orange}{the accompanying video}}}} for animated results.
  }
  \label{fig_seen_obj_cat}
  % \vspace{-10pt}
\end{figure}

\begin{figure}[h]
  \centering
  % \vspace{-10pt}
  \includegraphics[width=0.7\textwidth]{figs/unseen_obj_cat.pdf}
  % \vspace{-20pt}
  \caption{
  Examples of objects from new object categories (TACO). 
  % \textbf{Additional qualitative comparisons. }
  % Please refer to \textbf{\href{https://projectwebsite7.github.io/gene-dex-manip/}{our website} and {\href{}{\textcolor{orange}{the accompanying video}}}} for animated results.
  }
  \label{fig_unseen_obj_cat}
  % \vspace{-10pt}
\end{figure}

\noindent\textbf{Datasets.} Our dexterous robot hand-object manipulation dataset is created by retargeting two public human-object datasets, namely GRAB~\cite{taheri2020grab}, containing single-hand interactions with daily objects, and TACO~\cite{liu2024taco}, featured by functional tool using interactions. We retarget the full GRAB dataset and the fully released TACO dataset, obtaining 1269 and 2316 robot hand manipulation sequences respectively. 
% robot hand manipulation sequences and 2316 sequences
% obtaining 1269 manipulation sequences in total, and the full released TACO data contains 2316 sequences in total. 
% For TACO dataset, to balance the time consumption and the final data amount, we randomly select 1800 sequences from 2316 sequences in total for retargeting.
For GRAB, we use sequences of the \texttt{subject s1}, with  197 sequences in total, as the test dataset.  The training dataset is constructed by remaining sequences from other subjects. 
For the TACO dataset, we create one training set with four distinct test sets with different generalization levels for a detailed evaluation of the model's generalization performance.
Specifically, the whole dataset is split into 1) a training dataset, containing 1565 trajectories, 2) test set S0 where both the tool object geometries and the interaction triplets are seen during training containing 207 trajectories in total, 3) test set S1 where the tool geometry is novel but the interaction triplets are seen during training with 139 trajectories, 3) test set S2 with novel interaction triplets but seen object categories and geometries, containing 120 trajectories in total, and 4) test set S3 with 285 trajectories where both the object category and interaction triplets are new to the training dataset. 
Figure~\ref{fig_seen_obj_cat} and Figure~\ref{fig_unseen_obj_cat} draw the examples of unseen objects from seen categories and the objects from new categories respectively. 
The original data presented in TACO often contains noisy initial frames where the hand penetrates through the table or the object. Such noise, though seems subtle, would affect the initial dynamics, however. For instance, if the hand initially penetrates through the table, a large force would be applied to the hand at the beginning, which would severely affect the simulation in subsequent steps. Moreover, if the hand initially penetrates through the object, the object would be bounced away at the start of the simulation. To get rid of such phenomena, we make small modifications to the original sequences. Specifically, we interpolate the \texttt{phone pass} sequence of the subject \texttt{s2} from the GRAB dataset with such TACO sequences as the final modified sequence. Specifically, we take hand poses from the initial 60 frames of the GRAB sequence. We then linearly interpolate the hand pose in the 60th frame of the GRAB sequence with the hand pose in the 60th frame of the TACO sequence. For details, please refer to code in the supplementary material (refer ``README.md'' for instructions). 
% Please check the 
% pecifically, we take the initial 60 frames from the 
% we make small modifications on the original sequences by interpolating the sequence with the \texttt{phone pass} sequence form subject \texttt{s2} in the GRAB dataset so that the 
% , the test set S1 where the tool geometry is novel but the interaction triplets are seen during training, the test set S2 with novel interaction triplets but seen object categories and geometries, and the test set S4 with new object geometries and interaction triplets. Details w.r.t. the dataset splitting and visualizations are deferred to Appendix~\ref{}. 

% create four different test sets to evaluate the generalization 
% For GRAB, we mainly evaluate the model's generalization ability towards novel interaction sequences considering the large distinctions between their object geometries. For 
% To evaluate the model's generalization capability towards novel manipulation trajectories, we further split them into train and test splits. For the GRAB dataset, we use sequences of the \texttt{subject s1}, with 197 sequences in total, as the test dataset. The train dataset is constructed by remaining sequences from other subjects. For the TACO dataset, we 190 trajectories randomly selected from 1800 sequences for test, while the remaining trajectories as the training data. 
% \todo{unboserved objects? a different splitting method for TACO that considers the object category?}  For evaluating the model's generalization ability towards unseen geometry, we additionally construct a subset from the TACO's training dataset and a subset from TACO's test dataset as a new test split. \todo{details about that}
% For the GRAB datset, although there are existing previous splitting routines~\citep{Zhou2022TOCHSO,Liu2024GeneOHDT}, they mainly focus on cross object generalization. 



% \noindent\textbf{Datasets.} 


% \todo{train and test categories  -- objects and motions in different settings }

% \todo{details about different test settings}


% \todo{ Details w.r.t. the dataset splitting and visualizations are deferred to Appendix~\ref{}. }


% \todo{some modifications on TACO}





\begin{wraptable}[6]{r}{0.5\textwidth}
\vspace{-30pt}
    \centering
    \caption{ 
    Total training time consumption (TACO dataset). 
    } 
    % \vspace{5pt}
	% \resizebox{\linewidth}{!}{%
    \resizebox{0.5\textwidth}{!}{%
\begin{tabular}{@{\;}lcccc@{\;}}
        \toprule
        ~ & \makecell[c]{PPO \\ (w/o sup)} & \makecell[c]{Ours \\ (w/o prior., \\ w/o curri.)}  & \makecell[c]{Ours \\ (w/o prior)} & Ours   \\

        \midrule % 

        Time  & $\sim$1 day & $\sim$2 days & $\sim$4 days & $\sim$4 days
        \\ 
        
        \bottomrule
 
    \end{tabular}
    }
    % \vspace{-16pt}
    \label{tb_exp_timeconsumption}
\end{wraptable}



% \noindent\textbf{Training and evaluating settings.} The general policy training is composed of three stages as we detailed in the method section. In each general tracking policy training stage, we re-train the model with different labeled datasets without transferring weights from the previously trained model. It is because we observe that weight transferring would make the model even harder to train, other than accelerating the training process. We leverage PPO~\citep{Schulman2017ProximalPO} implemented in {rl\_games}~\citep{rlgames2021} as our RL algorithm. We use the widely appreciated Isaac Gym~\citep{makoviychuk2021isaac} as the simulated physical environment. For both the single trajectory specialist tracking policy training and the general tracking policy optimization, we use 8192 parallel environments to train RL. For articulated dexterous hands, the position gain and the damping coefficient are set to 20 and 1 respectively for each finger joint. During the evaluation phase, we rollout the policy 1000 times (using 1000 parallel execution environments) and average their metrics as the evaluation result of the current trajectory. More details such as the simulation environment setting are deferred to Appendix~\ref{sec:supp_exp_details}. 



\noindent\textbf{Training and evaluation settings.} 
For both GRAB and TACO, in the first stage, we first sample 100 trajectories from the training dataset. We train their per-trajectory trackers to obtain their action-labeled data to construct our first version of the labeled dataset. After that, the first tracking controller is trained and evaluated on all trajectories. We then additionally sample 100 trajectories from the remaining trajectories using the weights positively proportional to the tracking object position error. These sampled trajectories and those sampled in the first stage then form our second version of the dataset to label. We leverage both per-trajectory tracker optimization and the tracking prior from the first version of the trained tracking controller to label the data aiming to get high-quality labeled trajectories. After that, we search tracking curricula from such 200 trajectories and train a tracking curriculum scheduler using the mined curricula. We then construct the second version of the action-labeled dataset using the final best-optimized trajectories. We then re-train the tracking controller and evaluate its performance on each trajectory. In the third stage, we sample additionally 200 trajectories from the remaining not selected trajectories. We then label them using the joint power of per-trajectory tracking optimization, tracking prior from the tracking controller, and the curriculum scheduled by the curriculum scheduler. After that, our third version is constructed using best-optimized labeled trajectories. We then re-train the tracking controller using this version of the labeled dataset. When training the tracking controller, we set a threshold in the reward, \emph{i.e.,} 50. Only trajectories with a reward above the threshold is used to provide supervision. 
% sim settings
Both the simulation and the policy run at 60Hz. The hand's gravity is ignored in the simulation. 
Please refer to the code in the supplementary materials for detailed settings. 
We train all models in a Ubuntu 20.04.6 LTS with eight NVIDIA A10 cards and CUDA version 12.5. All the models are trained in a single card without multi-gpu parallelization. 


\begin{figure}[h]
  \centering
  % \vspace{-10pt}
  \includegraphics[width=0.7\textwidth]{figs/real_world_setting.pdf}
  % \vspace{-20pt}
  \caption{
  Real-world experiment setup.
  % \textbf{Additional qualitative comparisons. }
  % Please refer to \textbf{\href{https://projectwebsite7.github.io/gene-dex-manip/}{our website} and {\href{}{\textcolor{orange}{the accompanying video}}}} for animated results.
  }
  \label{fig_real_world_setup}
  % \vspace{-10pt}
\end{figure}

\noindent\textbf{Real world experiment setup.} 
We use the Franka arm and LEAP hand to conduct real-world evaluations (Figure~\ref{fig_real_world_setup}). When transferring the state-based policy, we use FoundationPose~\cite{Wen2023FoundationPoseU6} to estimate object poses. We use finite difference to estimate both the hand joint velocities as well as the object linear and angular velocities. Considering the gap between the control strategy we use in the simulator and the control of the Franka arm and the LEAP hand, we devise a strategy to mitigate the discrepancy between these two control methods. Specifically, instead of directly applying the control signal to the LEAP hand and the Franka arm, we set up a simulator with physical and control-related parameters same as our simulation settings during training. Then, in each timestep, we first apply the control commands to the simulated LEAP hand. We then simulate the hand. After that, we read the simulated state out from the simulator. We then calculate the positional target signal that should be applied to the real LEAP hand using the current obtained state from the simulator and the current state of the real LEAP hand. The calculated positional target signal is then directly fed to the real LEAP hand controller. In our observation, once the real hand has been commanded, it can almost reach exactly the same position in the command. Thereby, in practice, we directly use the state obtained from the simulator as the positional target command fed to the real LEAP controller. Experiments demonstrate the effectiveness of this control strategy. 
% and the 


% The final best labeled trajectories 
% where the model cannot perform well 

\noindent\textbf{Time consumption and time complexity.} Table~\ref{tb_exp_timeconsumption} summarizes the total time consumption of different methods on the TACO dataset. Directly training PPO without any supervision is the most efficient approach while the performance lagged behind due to no proper guidance. Solving the per-trajectory tracking problem for providing high-quality data for training the general tracking controller would additionally increase the time consumption due to the requirement in optimizing per-trajectory trackers. Since we only select a subset from the whole training dataset, the time consumption is still affordable. Improving the per-trajectory trackers via mining tracing curricula would introduce additional time costs. Since the number of trajectories we consider for learning the curriculum scheduler is still controlled to a relatively small value. The final time cost is still relatively affordable. Experiments are conducted on a Ubuntu 20.04 machine with eight A10 GPU cards. For per-trajectory tracker optimization, we train eight trackers in parallel at one time. 

The overall time complexity of the  training process is $\mathcal{O}(\vert \mathcal{S}\vert + KK_{\text{nei}}\vert \mathcal{S}\vert)$. $\mathcal{S}$ denotes the training dataset. 
% Exact curriculum learning for acquiring high-quality per-trajectory trackers, though as we observe in Table~\ref{tb_exp_main}, is an effective strategy, which also aligns with conclusions in previous approaches, is extremely time-consuming. For 400 trajectories that we finally selected to optimize to produce an action-labeled dataset, searching the exact curriculum costs about 13 days to complete. However, our method requires only slightly more time than the version without tracking prior or curriculum scheduling (Ours w/ sup.). However, we can achieve a much higher success rate than this version, taking advantage of the increased diversity and the quality of action-labeled data. 






\noindent\textbf{Additional details.} For tracking error metrics, we report the medium value of per-trajectory result in the test set, under the consideration that the average value may be affected by outliers. 



