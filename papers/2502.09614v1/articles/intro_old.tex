\section{Introduction}

% \todo{}

% \todo{problem and the motivation -- problem is still the general tracker but to motivate the method --- should not assume the single trajectory tracking is solved, especially efficiently tracking}

% \todo{method -- it seems that the current statement is a little bit uncesary complex..?}


% \todo{how to describe the task? -- the control the robot hand to accomplish what?}
% \todo{llm for helping?}


% \eric{1. What problem you are focusing on and why it is important?
% \begin{itemize}
%     \item The definition of dexterous manipulation.
%     \item The importance of dexterous manipulation.
%     \item Since developing dexterous robotic hands aims to achieve versatility similar to that of human hands, it becomes essential to develop robotic systems with diverse dexterous manipulation skills, which is also our goal in this paper.
% \end{itemize}}

% Advancing an embodied agent's ability to interact with the physical world is a critical step toward achieving powerful artificial intelligence. 
% Dexterous manipulation i
% Dexterous manipulation involves manipulating objects to change the object state for specific purposes. 
Robotic dexterous manipulation refers to the ability of a robot hand to skillfully handle and manipulate objects for various target states with precision and adaptability.
% similar to human hand movements.
Empowering an embodied agent with dexterous manipulation skills has attracted lots of attention since smartly and adeptly manipulating objects to fulfill goals such as tool-using is a crucial part of interacting with the world. 
% is a crucial part of 
% interacting with objects with dexterous hand is the most basic and effective way to change the object states to fulfill goals such as tool-using. 
The ultimate goal of developing robotic dexterous hand skills is letting it achieve human-level dexterity and versatility~\citep{rajeswaran2017learning,chen2023visual,chen2021system,akkaya2019solving,christen2022d,zhang2023artigrasp,qin2022dexmv,liu2022herd,wu2023learning,gupta2016learning,wang2023physhoi,mordatch2012contact,liu2024quasisim,Li2024ReinforcementLF}. To fulfill this ambitious yet essential vision, an essential milestone is equipping robotic system with diverse manipulation skills, which is also our goal in this work. 

% so that it has the capability to accomplish a wide range of manipulation tasks, wh
% Fulfilling this ambitious yet essential vision 
% It is an important topic nowadays since interacting with object using dexterous hand is a most basic way of using objects to fullfilll our goals. 
% aims at equipping a multi-finger robotic dexterous hand with the ability to interact with the 
% Equipping an embodied agent with human-level dexterity is an ambitious yet essential milestone in developing future super-intelligent robots capable of smart interactions with the environment~\citep{rajeswaran2017learning,chen2023visual,chen2021system,akkaya2019solving,christen2022d,zhang2023artigrasp,qin2022dexmv,liu2022herd,wu2023learning,gupta2016learning,yao2022controlvae,fussell2021supertrack,grandia2023doc,peng2018deepmimic,wang2023physhoi,mordatch2012contact,liu2024quasisim,Jenelten2023DTCDT,Li2024ReinforcementLF}. 
% With this as the ultimate goal of developing dexterous robot hands, it becomes essential to equip robotic systems with diverse dexterous manipulation skills, which is also our goal in this work. 

% Developing dexterous hands with such dexterity 
% As the final goal of developing dexterous robot hands aims to achieve human
% Learning expressive and dexterous skills from human demonstrations is a promising and increasingly popular approach to advancing robot capabilities toward human-level dexterity. Human-object interactions (HOI) offer rich, semantically meaningful manipulations, supported by diverse data sources such as motion capture datasets~\citep{taheri2020grab,fan2023arctic,liu2024taco} and large-scale online video collections~\citep{Wu2024ReconstructingHO}. However, existing approaches predominantly focus on learning goal-driven or task-specific skill acquisitions~\citep{rajeswaran2017learning,qin2022dexmv,Xiong2022RoboTubeLH,liu2024quasisim} falling short of delivering a versatile controller capable of handling a broad range of complex manipulations.
% In this work, we aim to develop a general-purpose tracking controller from human manipulation references that can universally control a robotic hand to perform a wide variety of dexterous tasks. 

% task-specific, goal-driven 
% task-specific, goal-driven skills~\citep{rajeswaran2017learning,qin2022dexmv,Xiong2022RoboTubeLH}, or are constrained by single-trajectory optimization~\citep{liu2024quasisim}, 

% Advancing an embodied agent's capability to interact with the world is undoubtedly a significant step towards powerful artificial intelligence. Equipping an agent with high eligibility and human-level dexterity is an ambitious yet extremely vital waypoint towards future super-intelligent robot to smartly interact with the physical world~\citep{rajeswaran2017learning,chen2023visual,chen2021system,akkaya2019solving,christen2022d,zhang2023artigrasp,qin2022dexmv,liu2022herd,wu2023learning,gupta2016learning,yao2022controlvae,fussell2021supertrack,grandia2023doc,peng2018deepmimic,wang2023physhoi,mordatch2012contact,liu2024quasisim,Jenelten2023DTCDT,Li2024ReinforcementLF}. 
% % \todo{cite some locomotion related papers}
% Learning expressive and dexterous skills from human references is a promising a popular trend to push the boundary of the robot library towards human-level dexterity taking advantage of highly dexterous, semantically meaningful and expressive manipulations contained in human-object interactions (HOI) and abundant data sources covering MoCap datasets~\citep{fan2023arctic,liu2024taco} and internet scale YouTube videos~\citep{Wu2024ReconstructingHO}. However, previous efforts are mainly limited to acquiring specific and relatively simple goal-driven skills~\citep{rajeswaran2017learning,qin2022dexmv,Xiong2022RoboTubeLH} or are restricted to single trajectory optimization~\citep{liu2024quasisim}, still a far way from a versatile controller wiht the ability to accomplish a wide range of challenging maipulations. 
% % are restricted to single trajectory offline tracking\todo{need to say their limitations here?} 
% In this work, we wish to learn a general tracking controller from human manipulation reference that can universally control the robot hand to accomplish a wide and versatile range of dexterous manipulations. 


% In this work, we investigate this issue and illustrate how optimizing the simulator concurrently with skill acquisition can benefit a popular yet challenging task in robot manipulation -- dexterous manipulation transfer.

% ~\citep{rajeswaran2017learning,Xiong2022RoboTubeLH,qin2022dexmv,liu2024quasisim}. \todo{more? } 
% Considering the complex and changing environments as well as novel tasks that an embodied agent would frequently encounter during real execution, a general policy capable of 
% a general policy capable of tackling a wide range of tasks with high robustness and adaptivity is more appealing and more practically useful. Despite the impressive achievement in general and robust local motion skill learning \todo{cite the robust tracker..?}, what's the best strategy to acquire a dexterous manipulation generalist that still lacks a mature and truly workable solution? In this work, we aim to explore the possibility of training a dexterous manipulation general police for a popular yet challenging family of tasks -- dexterous manipulation tracking. 





% \eric{2. What are the challenges and why previous methods are not good enough?
% \begin{itemize}
%     \item Dexterous manipulation is challenging due to …
%     \item RL based methods require task specific reward designs and are usually restricted to a specific manipulation skill. TO based methods require accurate dynamic models with known contact states, preventing it from generalizing to new objects and skills.
%     \item A seemingly attractive idea to tackle the above issues is to leverage reference motions of human hands manipulating objects, e.g., from videos or from motion synthesis methods. When available, these data can be tracked via a controller, allowing dexterous hands to quickly perform very diverse manipulation skills.
%     \item However, dexterous manipulation tracking is also challenging due to noises, complex dynamic contacts, diverse object geometry and manipulation trajectory… and existing methods fail…
%     \item Our work aims to tackle the above challenges and introduce …
% \end{itemize}}

This vision is super-challenging due to several inherent difficulties coming from 1) the intricate dynamics involved in contact-rich dexterous manipulation, which often challenges the optimization~\citep{pang2021convex,pang2023global,liu2024quasisim,Jin2024ComplementarityFreeMM}, and 2) the wish for letting the robot master versatile manipulation skills, not limited to single tasks or specific problems. Previous works mainly resort to model-free reinforcement learning (RL) techniques~\citep{chen2023visual,chen2021system,akkaya2019solving,christen2022d,zhang2023artigrasp,qin2022dexmv,liu2022herd,wu2023learning,gupta2016learning,wang2023physhoi} and model-based trajectory optimization (TO) approaches~\citep{pang2021convex,pang2023global,Jin2024ComplementarityFreeMM,hwangbo2018per} to empower the dexterous hand with fantastic manipulation skills. Despite their impressive achievements, solely leveraging these methods can hardly let us arrive at a universal dexterous skill. RL requires methods that require task-specific reward designs and are usually restricted to a specific manipulation skill. Trajectory optimization, though free of designing rewards, relies on accurate dynamics models with known contact states. These assumptions severely prevent it from generalizing to new objects and skills. Jumping out of previous common routines, a seemingly attractive idea to tackle the above issues is to leverage references trajectories of human hands manipulating objects, which have abundant resources such as videos or motion synthesis methods, and convert the universal dexterous skill acquisition puzzle into a tracking control problem. In this way, we can isolate the high-level task planning and the low-level controlling. It further frees us from designing a general skill for tackling heterogeneous tasks by unifying diverse dexterous manipulation skill learning into a tracking control task.
% offers us the possibility of getting rid of a general
% by unifying diverse dexterous manipulation skill learning into a tracking control task.
% to isolate the high-level task planning and the low-level control problem, leverage references motions of human hands manipulating objects, which have abundant resources such as videos or motions synthesis methods, and convert the universal dexterous skill acquisition problem into a tracking control problem. 
With a controller that can track abundant and diverse human object manipulations with high generalization ability and robustness, we enable a dexterous hand to perform very diverse manipulation skills via high-level kinematics commands.
However, dexterous manipulation tracking control is also extremely challenging due to noisy kinematic references resulting from inaccurate capturing systems and the morphology difference between the robot hand and the human hand, the complex and dynamic, rich hand-object contacts, diverse and even challenging objects (\textit{e.g.,} very thin objects) and manipulation. Existing methods always fail to fully resolve these challenges, restricting their achievements in learning from sparse human references~\citep{christen2022d,zhang2023artigrasp,wu2023learning}, demonstrating versatility in relatively simple tasks such as grasping, trajectory following, and re-locating~\citep{xu2023unidexgrasp,Luo2024GraspingDO,Singh2024HandObjectIP,Chen2024ViViDexLV}, and task-specific or single skill acquisition~\citep{qin2022dexmv,liu2024quasisim,rajeswaran2017learning}. 
% \todo{more refs}
% Existing methods always fail to fully resolve these challenges, restricting their achievement in specific skill acquisition or the versatility only in relatively simple tasks such as grasping. \todo{add ref} 
In this work, we try to tackle these challenges and introduce a learning-based versatile neural tracking controller for dexterous manipulation from human references. 
% \todo{check this}

% \eric{3. Add a paragraph talking about the specific problem formulation. Train time, test time, input, output.}

The tracking control problem for dexterous manipulations from human references aims at developing a versatile and generalizable tracking controller leveraging abundant human-object interaction data. 
% at leveraging abundant kinematics-only human-object manipulation trajectories to learn a versatile and generalizable tracking controller.
Given a collection of kinematics-only human hand-object manipulation trajectories, the controller is optimized to drive a robotic dexterous hand to manipulate the object so that the resulting hand and object trajectories can closely mimic their corresponding kinematic sequences. 
% the robotic dexterous 
% track every trajectory in the dataset.
% At the test time, w
We expect the tracking controller to exhibit strong versatility, generalize well to precisely track novel manipulations, and have strong robustness towards large kinematics noises and unexpected reference states. 
% generalization ability to novel manipulations, and robustness towards large kinematics noise. Given an unseen manipulation trajectory with new and challenging interactions and unobserved object geometry, we wish it to generalize well and can precisely track these sequences. 
% to be able to generalize well to unseen manipulation trajectories and precisely track novel and challenging interactions such as complex object movements and subtle in-hand re-orientations. 
% with novel and difficult objects
% to track novel and even challenging interactions involving complex object movements with difficult, subtle, and detailed in-hand manipulations, 
% novel and even challenging interactions with
% The neural tracking control for dexterous manipulations aims to leverage human hands manipulating object trajectories, which have abundant data sources covering the internet videos and motion synthesis models, to learn a versatile and generalizable neural tracking controller. Given a dataset containing diverse human-object interaction trajectories, the controller is optimized aiming to track every manipulation trajectory in the dataset. Specifically, with the kinematics-only human object manipulation trajectory as input, the tracking controller aims to output an action trajectory that can drive a dexterous robotic hand to manipulate the object with the resulting hand trajectory and the object sequence closely mimicking their corresponding kinematic references. 
% % mimic the kinematic trajectory -- when letting a dexterous robot hand execute the output action trajectory, the resulting hand states and object states should be close to their corresponding kinematic trajectories in the input sequence. 
% A versatile and generalizable tracking controller ought to be able to generalize to novel and even challenging interactions with new and difficult objects. At the test time, given a manipulation sequence where both the hand motion and the object movements and the object geometry are unseen during the training, the controller is expected to precisely track the manipulation trajectory. 
% to give high-quality action outputs d
% that can drive the dexterous robot hand to precisely control the object so that the resulting hand sequence and the object sequences are close to the input reference. 




%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% . When executing the action sequence 
% or each manipulation sequence, the tracking controller aims at output 
% In the test time, the optimized controller 
% to produce proper actions at each timestep aiming to track 
% to track every manipulating trajectory in the dataset. 
% At the train time, the controller is optimized to track every 
% human hands manipulating objects 
% learning a versatile and generalizable tracing controller from
% is to leverage reference motions of human hands manipulating objects, \textit{e.g.}, from videos or from motion synthesis methods. When available, these data can be tracked via a controller, allowing dexterous hands to quickly perform very diverse manipulation skills.
% preventing it from generalizing to new objects and skills. 
% in dexterous skill acquisition mainly resort to model-free reinforcement learning techniques and model-based trajectory optimizatons
% 1) the intricate dynamics involved in the manipulation resulting from the complex object motions including difficult and subtle in-hand re-orientations and the frequent rich hand-object variations which even frequently challenge the single trajectory tracking problem~\citep{pang2021convex,pang2023global,liu2024quasisim,Jin2024ComplementarityFreeMM}, 2) the requirement of controlling a high DoF dexterous hand to densely track each frame in the demonstration,  3) the wish for a general controller that generalizes well to control the hand to manipulate new objects to track new manipulation references. 
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


% Given a set of kinematics-only human hand-object interaction trajectories, taken from MoCap datasets~\citep{fan2023arctic,taheri2020grab,liu2024taco} or obtained via reconstructions from videos~\citep{Wu2024ReconstructingHO}, the goal of dexterous tracking control is to develop a single, general controller capable of mimicking a diverse range of manipulation sequences with both the resulting hand and object trajectories close to their corresponding reference sequences. 
% % trajectory and the object sequence close to those in
% % executing diverse manipulations specified by kinematic reference commands.
% This controller should generalize to unseen motion trajectories, adapt to novel object geometries, and exhibit high robustness towards large noise in the kinematic motions such as unreasonable states, severe penetrations, as well as out-of-distribution objects and interactions. 
% % such as those from MoCap dataset
% % hand-object interaction dataset from a simulated dexterous hand, such as those obtained via retargeting from human HOI datasets~\citep{fan2023arctic,taheri2020grab,liu2024taco} or reconstructed from videos~\citep{Wu2024ReconstructingHO}, the goal of 
% % with strong recovery behavior. 
% Solving this problem poses several significant challenges: 1) the intricate dynamics involved in the manipulation resulting from the complex object motions including difficult and subtle in-hand re-orientations and the frequent rich hand-object variations which even frequently challenge the single trajectory tracking problem~\citep{pang2021convex,pang2023global,liu2024quasisim,Jin2024ComplementarityFreeMM}, 2) the requirement of controlling a high DoF dexterous hand to densely track each frame in the demonstration,  3) the wish for a general controller that generalizes well to control the hand to manipulate new objects to track new manipulation references. 
% Previous works largely focus on solving per-trajectory tracking problems or learning task-specific skills, such as pouring, relocating~\citep{rajeswaran2017learning,qin2022dexmv,christen2022d,zhang2023artigrasp}, in-hand re-orientation, flipping, and spinning~\citep{chen2023visual,andrychowicz2020learning} with a fixed-base robot hand. Other works demonstrate versatility only in relatively simple tasks such as trajectory following~\citep{Luo2024GraspingDO} and dexterous grasping~\citep{xu2023unidexgrasp,wan2023unidexgrasp2}. Recent advances in humanoid tracking control suggest the potential of leveraging reinforcement learning (RL) to develop versatile tracking policies by carefully designing rewards and training strategies~\citep{Luo2023PerpetualHC,Luo2023UniversalHM}. However, dexterous manipulation poses additional difficulties. Training policies to handle various manipulations remains challenging.
% % Training policies to handle various manipulations, even with carefully crafted reward functions, remains challenging. 
% % This may stem from the unstructured and heterogeneous nature of reference motions, which further expose RL’s limitations in multi-task learning. 
% Model-based techniques that assume known system dynamics face significant challenges as well due to the difficulty in accurate system modeling in the dexterous manipulation scenario. 
% The correct path toward developing a universal tracking controller remains elusive, with few attempts documented in the literature.
% % simpler tasks, such as trajectory following~\citep{Luo2024GraspingDO} and dexterous grasping~\citep{xu2023unidexgrasp,wan2023unidexgrasp2}.



% specific task-oriented 


% 1) The intricate dynamics of manipulation, characterized by complex object motions, subtle in-hand re-orientations, and frequent variations in hand-object interactions, which can be difficult even for single-trajectory tracking~\citep{liu2024quasisim,Jin2024ComplementarityFreeMM}.
% 2) The need to control a high-DoF dexterous hand to densely track each frame in the reference demonstration.
% 3) The desire for a general controller that can adapt to new objects and manipulation tasks beyond those in the training data.





% general dexterous manipulation tracking aims at optimizing for a single controller (a ``tracker'' ) that 1) can control the hand to accomplish manipulations as demonstrated in each given kinematic motions and 2) can generalize to track novel manipulation trajectories involving novel interaction trajectories and unobserved object geometries. Solving this problem poses several significant challenges: 1) the intricate dynamics involved in the manipulation 
% Given the kinematic-only hand-object interaction dataset of a simulated dexterous hand, \emph{e.g.,} acquired via retargeting from human HOI datasets~\citep{fan2023arctic,taheri2020grab,liu2024taco} or lifting from videos~\citep{Wu2024ReconstructingHO}, dexterous tracking control problem aims at learning a single yet general tracking controller that can control the robot hand accomplish versatile manipulations specified by the kinematic reference commands. The tracking controller is expected to generalize well to unseen motion trajectories, and novel object geometries, and exhibit high robustness and recovery behaviors. Solving this problem poses several significant challenges: 1) the intricate dynamics involved in the manipulation resulting from the complex object motions including difficult and subtle in-hand re-orientations and the frequent rich hand-object variations which even frequently challenge the single trajectory tracking problem~\citep{liu2024quasisim,Jin2024ComplementarityFreeMM}, 2) the requirement of controlling a high DoF dexterous hand to densely track each frame in the demonstration,  3) the wish for a general controller that generalizes well to control the hand to manipulate new objects to track new manipulation references. Previous works mainly focus on solving per-trajectory tracking problems, learning specific goal-driven task skills such as pouring and re-locating~\citep{rajeswaran2017learning,qin2022dexmv,christen2022d,zhang2023artigrasp}, in-hand re-orientation, flipping and spinning~\citep{chen2023visual,andrychowicz2020learning} with a fixed-root robot hand, or only demonstrates the versatility in relatively simple tasks such as trajectory following~\citep{Luo2024GraspingDO} and dexterous grasping~\citep{xu2023unidexgrasp,wan2023unidexgrasp2}. Recent sucssess in humanoid tracking control has demonstrated the potential of leveraging advanced reinforcement learning (RL) to learn a versatile tracking policy by carefully design rewards and the training strategy~\citep{Luo2023PerpetualHC,Luo2023UniversalHM}. However, manipulation tracking control exhibits increasing difficulties. Directly training a policy to track various manipulations, even with dedicatedly designed rewards, can hardly work, perhaps due to unstructured heterogeneous reference motions that put RL which is notoriously in short of the multi-task ability, further into a difficult situation. What's the correct way to a universal tracking controller remains extremely unclear and difficult with very rare attempts in the literature. 


% Directly adapting previous methods to solve our problem,  such as the delicately designing rewards and leveraging advanced reinforcement learning (RL) with carefully designed training scheme to train a policy aiming to track every manipulation trajectory in the dataset, though proved effective in training humanoid locomotion controller~\citep{Luo2023PerpetualHC}, can hardly work in our problem scope, 

% Model-free reinforcement learning (RL), with its no assumption on the dynamic system and the strong optimization capability, has been widely appreciated and proved effective in previous literature~\citep{qin2022dexmv,christen2022d,zhang2023artigrasp,Luo2024GraspingDO,Luo2023PerpetualHC}. 
% Reinforcement learning (RL) is a widely appreciated in previous literature 
% which does not assume prior knowledge about the dynamic system, 
% In the current situation where optimizing for an effective single trajectory tracker still has no mature solutions, what's the correct way to a universal tracking controller is extremely unclear and difficult with very rare attempts in the literature. 
% In this work, 
% well to novel manipulations and new object geometrices. 
% the wish for a universal controller that can well track every manipulation demonstrations other than over-fitting to the single trajectory, and 3) the expectation in generalizing to unseen manipulations including novel hand-object trajectories and unobserved object geometries. 


% \todo{need to mention previous works in the universal humanoid tracking?}

% What's the correct way to a universal tracking controller is extremely unclear and lacking 
% How to optimize for 
% Obtaining single trajectory tracker is already very challenging and time-consuming~\citep{liu2024quasisim}, letting alone a universal tracking controller. 
% due to the difficult dynamics that are unfriendly for an optimizer. 
% or only demonstrates the effectiveness in relatively easy interact
% Most previous works focus on single trajectory tracking, where the optimized controller is only expected to be effective for one single trajectory~\citep{pang2023global,zhang2023artigrasp,liu2024quasisim,Jin2024ComplementarityFreeMM}, or is only effective in relatively easy interactions such as trajectory following~\citep{Luo2024GraspingDO}, goal-driven manipulations such as pouring and re-locating~\citep{rajeswaran2017learning,qin2022dexmv,christen2022d,zhang2023artigrasp}, in-hand re-orientation, flipping and spinning~\citep{chen2023visual,andrychowicz2020learning} with a fixed-root robot hand, and dexterous grasping~\citep{xu2023unidexgrasp,wan2023unidexgrasp2}. In this work, we take one step further and wish for a universal controller that can effectively track every manipulation involving expressive and difficult interactions with complex and challenging object movements, and frequent contact variations, and can also generalize to novel manipulations.\todo{rewrite this} 


% \eric{4. What are your key observations and/or high-level ideas
% \begin{itemize}
%     \item Learning is essential to handle reference noises and transfer data priors to novel cases.
%     \item Imitating from large amount of high-quality tracking results can make learning especially powerful as evidenced by data-scaling law in CV and NLP. % can make the learning essential powerful as evidenced by datascaling law 
%     \item Obtaining useful data could be tricky, but we can leverage data flywheel to improve the tracking controller and the amount of high-quality data in a bootstrapping manner.
% \end{itemize}
% }


To tackle challenges in developing a versatile tracking controller, our key observations lie in three aspects: 1) Compared to model-based approaches which primarily leverage trajectory optimization to develop the controller, learning plays a crucial role in handling reference motion noises and transferring data prior to novel cases to achieve robust and generalizable tracking control. 2) Learning from a large amount of high-quality imitation-ready data with paired data containing both the kinematic reference and the action sequence is a promising approach to make the neural controller essentially powerful as evidenced by the data-scaling law in CV and NLP spaces~\citep{Achiam2023GPT4TR,Brown2020LanguageMA}. 3) Due to the inherent difficulty in solving dexterous manipulation tracking control problems, obtaining useful imitation-ready data for tracking control could be very tricky. However, we can leverage the data flywheel~\citep{Chiang2024ChatbotAA,Bai2023QwenTR,Ouyang2022TrainingLM} to improve the tracking controller and the amount of high-quality data in a bootstrapping manner. 


% Obtaining high-quality imitation-ready data for dexterous manipulation tracking control could be very tricky. 
% generalizable and 
% strong generalizations
% generalizable and 
% traditional trajectory optimization, learning plays a crucial role in handling reference motion noises and transferring data prior to novel cases. Training a neural controller is a more promising technique for us to get to a universal and generalizable tracking controller capable of handling various situations. 2) Learning from a large amount of high-quality tracking results can make the neural controller essentially powerful as evidenced by the data-scaling law in CV and NLP. 3) Obtaining useful data could be tricky, but we can leverage the data flywheel to improve the neural tracking controller and the amount of high-quality data in a bootstrapping manner.
% A learning based neutral controller is 


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Inspired by the data scaling law in the vision and NLP space~\citep{Brown2020LanguageMA,Achiam2023GPT4TR}, we envision a large number of imitation-ready data with reference and tracking result pairs could facilitate the learning of a general tracking controller. However, obtaining imitation-ready data itself is challenging since even tracking a single manipulation sequence is both costly and difficult~\citep{christen2022d,zhang2023artigrasp,pang2021convex,pang2023global,liu2024quasisim,Jin2024ComplementarityFreeMM}. In the general tracking problem scope, we envision the possibility of letting per-trajectory tracking benefit from the tracking data prior and the tracking task structure prior, which tells us cross-task relations that we can leverage to improve single task solving via the homotopy optimization. 
% % With the general tracking controller, we envision the possibility of letting the per-trajectory tracking benefit from general data prior in tracking encoded in the tracking controller and the task structure prior where the 
% % and also let the model 
% % one way that could let the per-trajectory tracking benefit from general data prior from the tracking controller and also l
% % We envision one way where we can let per-trajectory tracking benefit from tracking prior encoded in hte
% Therefore, we propose to grow the imitation-ready data and the tracking controller together in a bootstrapping manner. Starting from a small amount of imitation-ready data with action-labeled manipulation trajectories, we learn a tracking controller by letting the policy explore and evolve via RL and imitate high-quality action sequences; and also construct the tracking task structure encoding beneficial optimization paths which we can leverage to solve difficult tracking problems in a homotopy optimization manner. After that, we could leverage the data prior from the tracking controller and optimization paths proposed by the tracking task structure prior to enhance  per-trajectory tracking optimization. With these above considerations, we propose an iterative approach to gradually improve each side and finally arrive at a strong general tracking controller. 
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% the tracking task structure prio
% the data prior encoded in the learned tracking controller and the inference time structure prior 
% After that, we could leverage the data prior encoded in the learned tracking controller and the inference time structure prior to planning a correct optimization path which can finally ease the difficulty of per-trajectory optimization. Considering the inter-dependencies between the tracking controller optimization and per-trajectory trackers, we then propose an iterative approach to solve the ``chicken-and-egg'' puzzle where we gradually improve each side and finally arrive at a strong general tracking controller. 
% facilitate difficult
% where we can facilitate difficult task sol
% prior indicating how the per-trajectory tracking task could benefit from solving the other tracking tasks in a homotopy optimization manner. 
% the labeled data with high-quality action trajectories; 
% letting the policy explore and 
% trajectory in generic manipulation cases could be both costly and difficult -> 
% % Our key observation is that although RL would fall short in training a general tracking controller, it can indeed benefit from the guidance provided by successful trajectories with high-quality action sequences. 
% % successfully tracking high-quality trajectories, which also aligns with observations from the success of foundation models~\citep{Brown2020LanguageMA,Achiam2023GPT4TR}. \todo{add more refs} 
% However, how to effectively acquire a large amount of high-quality action-labeled manipulation trajectories itself is an unsolved problem. Existing methods for single trajectory tracking either fall short in quality or an unaffordable time requirement. Difficult trajectories involving challenging dynamics and tough object geometries are even harder to mimic. Optimizing a large scale of action-labeled trajectories and leveraging them to improve the general tracking controller's performance is not feasible. Restricted to a limited number of successful trajectories to provide guidance would potentially bias the tracking controller overfit to easy trajectories, hindering the capability and the generalization ability. How to improve the diversity and the quality of action-labeled trajectories is itself an urgent question. Observing the ability to leverage foundation models to solve specific tasks, we naturally seek the opportunity to improve per-trajectory tracking quality via borrowing tracking priros encoded in the general tracking controller. However, the tracking controller is learned from successfully optimized trajectories. Therefore, leveraging its tracking prior to resolve difficult problems may not be sufficient to solve very difficult problems.  Inspired by the recent advances in leveraging curriculum learning to solve difficult tasks~\citep{zhang2023learning,Zhuang2023RobotPL,liu2024quasisim}, we propose to find correct tracking curricula to improve the per-trajectory tracking. Identifying correct curriculum for each trajectory, which may require trail-and-error, is very time-consuming. This naturally lets us consider learning a curriculum scheduler that can schedule correct curricula to finally track difficult trajectories. The requirement of optimizing action-labeled datasets for training the general tracking controller and mining useful curricula for learning the curriculum scheduler further poses us with a ``chicken-and-egg'' puzzle. We consider an interactive approach where we can progressively improve the quality and diversity of action-labeled trajectories and train stronger tracking controller.
% To solve difficult per-trajectory
% better solve difficult per-trajectory tracking problems, where the tracking controller can hardly give us correct prior due to the limited coverage in its labeled training data, we further think about finding correct curricula to improve per-trajectory tracking, inspired by the recent advances in leveraging curriculum learning to solve difficult tasks~\citep{zhang2023learning,Zhuang2023RobotPL,liu2024quasisim}. 
% speed up and improve the per-trajectory tracking via tracking priors encoded in the general tracking controller. 
% or efficiency. 

% Therefore, to improve the diversity, coverage and quality of the action labeled trajectories, we naturally ask for the opportunity to improve the single-trajectory tracking via learned tracking policy priors as well as smartly design other techniques to speed up per-trajectory trackers' optimization as well as improve their performance. Observing the potential of leveraging general tracking knowldge encoded in the tracking controller and utilizing cross-task synergies for improving per-tracking trackers, we propose an interactive optimization scheme where we progressively improve the tracking controller via actionlabeled data and leveraging the tracking controller and cross-task synergies. 
% in one-shot 



% the general tracking knowledge encoded in the tracking controller that would be potentially beneficial for per-trajectory tracking and the 
% By exploring the possibilities of improving per-trajectories trackers 
% With key insights in 
% improving single per-trajectory tracking via 


% This naturally leads to a question: can we improve the single-trajectory tracking via learned tracking policy priors as well as smartly design other techniques to speed up per-trajectory trackers' optimization as well improve their performance? 
% Observing the role or curriculum in solving difficult control problems~\citep{Zhuang2023RobotPL,Luo2023PerpetualHC,liu2024quasisim,zhang2023learning}, we wish 
% % Recent advances in robotics and RL have witness the role 
% This naturally leads to a question: can we improve the single-trajectory tracking via the 
% either fall short in the efficiency or the tracking quality and can hardly equip us with lots of successful trajectories in a short 
% letting the tracking controller benefit from knowledge from per-trajectory tracking outcomes is a ``chitchen-and-egg'' problem in nature. 

% instead of model-free 
% Our key insight is iteratively letting the general controller optimization benefit from the guidance provided by trajectory-specific experts and improving each single trajectory tracking specialist via explicit and implicit cross-task transferrable knowledge. Instead of optimizing the general controller alone, where even the most advanced RL would be frequently challenged considering the requirement to jointly track every manipulation with distinct hand-object motion and object geometries, we wish to start from the relatively easy-to-solve single trajectory-specific controller optimization and propose an effective strategy that can let the general controller benefit from them. However, considering the inherent difficulties of dexterous manipulation tracking, single trajectory tracking specialists are hard to optimize to a satisfactory extent, especially for challenging interaction trajectories, and might require a long time. We therefore consider benefiting from single trajectory tracking from cross-task universal knowledge by modeling cross-task relations explicitly and exploring a structural task space that models cross-task relations. By iteratively solving the general tracker optimization and refining the trajectory-specific policies, we can finally arrive at a powerful general tracker via this ``specialist-generalist-specialist'' paradigm. 
% \todo{rewrite this --- should not assume that the single trajectory tracking is a solved problem --- especially the efficient single trajectory tracking} 
% % even the single trajectory tracking is difficult to optimize to a satisfactory extent. 

% is optimizing for the general controller that can properly benefit from the guidance provided by trajectory-specific trackers. 

% \eric{5. Explain your methods briefly
% \begin{itemize}
%     \item An overall pipeline explanation like the following. At train time, given a collection of human hand object manipulation trajectories, we first retarget human hands to robotic dexterous hands in the data preparation stage, resulting in a reference motion set. Our method then alternates between mining useful imitation-ready data from the reference motion set and training the tracking controller with the mined data. We have three key designs to enable the data flywheel to function effectively. 
%     \item First, we leverage an imitation-augmented reinforcement learning scheme to make sure the tracking controller can benefit from imitation-ready data while still being robust to state disturbance. ...
%     \item Second, we develop a per-trajectory tracking scheme which can leverage the general tracking controller to produce better imitation-ready data. ...
%     \item Third, we design a homotopy path generator, mimicking the chain of thought, to better leverage the general tracking controller for mining imitation-ready data from challenging manipulation skills. ...
% \end{itemize}
% }


We propose \modelnamenspace, a learning-based neural tracking controller designed for dexterous manipulations guided by human references. The key idea is to combine the reinforcement learning and imitation learning to train a robust and generalizable tracking controller  utilizing abundant high-quality imitation-ready data, while employing the data flywheel to curate high-quality data and improve the tracking controller in a bootstrapping manner. 
% employing a data flywheel to continually curate and enhance the dataset, thereby improving the tracking controller in a bootstrapping manner. 
% train a robust and generalizable tracking controller through a combination of reinforcement learning and imitation learning, utilizing abundant high-quality imitation-ready data while employing a data flywheel to continually curate and enhance the dataset, thereby improving the tracking controller in a bootstrapping manner. 
Given a collection of human hand-object manipulation trajectories, we first retarget human hand trajectories to robotic dexterous hand sequences in the data preparation stage. 
% In the data preparation stage, we begin by retargeting human hand-object manipulation trajectories into robotic dexterous hand sequences. 
% Our approach alternates between mining useful imitation-ready data from the reference motion set and training the tracking controller with this mined data. 
Our method then alternates between mining useful imitation-ready data from the reference motion set and training the tracking controller with the mined data. 
We propose two key designs to enable the data flywheel to function effectively. 
% We introduce two key designs to ensure the effective operation of the data flywheel. 
First, we learn the neural tracking controller by carefully integrating reinforcement learning and imitation learning techniques. 
By training the tracking controller to imitate a wealth of tracking results, we enhance its versatility in mastering a wide range of manipulation skills and generalization ability to novel interactions. At the same time, by employing the power of reinforcement learning to handle abnormal situations effectively, we can effectively improve the controller's robustness towards unexpected states and noise. 
% By combining the strengths of these approaches, we enable the tracking controller to imitate a wealth of tracking results, thereby enhancing its versatility in mastering a wide range of manipulation skills and generalization ability to novel interactions. 
% generalization ability and 
% we achieve a controller with high generalization ability and versatility. This enables the tracking controller to imitate a wealth of tracking results while also enhancing its robustness to unexpected states and noise, leveraging the power of reinforcement learning to handle abnormal situations effectively.
Second, we develop a per-trajectory tracking scheme that utilizes the neural tracking controller to mine higher-quality and more diverse imitation-ready data through a homotopy optimization scheme. To prepare this imitation-ready data, a basic solution is obtaining paired action sequences for kinematic reference trajectories via tracking trajectory-specific tracking policies to accurately follow the kinematic reference motions. 
However, the quality and diversity of the data can be limited since RL may face challenges when addressing difficult per-trajectory tracking problems. 
% due to the challenges that reinforcement learning (RL) faces when addressing difficult per-trajectory tracking problems. 
However, data quality and diversity can be limited due to the difficulty of RL in solving very challenging per-trajectory tracking problems. To enhance the effectiveness of per-trajectory tracking and ensure the data flywheel operates efficiently, we propose two techniques: 1) transferring tracking data from the tracking controller to the per-trajectory tracker to aid in solving complex problems; and 2) developing an innovative homotopy path generator that mimics a chain of thought, allowing us to plan beneficial optimization paths to address previously unsolvable per-trajectory tracking challenges through a homotopy optimization method. 

% However, this strategy frequently falls short in tracking difficult trajectories. Therefore, we propose two techniques to improve the quality of per-trajectory tracking results so that the data flywheel can work effectively: 1) transferring tracking data prior from the tracking controller to the per-trajectory tracker to facilitate difficult problem-solving; and 2) learning an innovative homotopy path generator, mimicking chain of thought, which can plan a beneficial homotopy optimization path that we can leverage to tackle previously unsolvable per-trajectory tracking problem via a homotopy method. 
% With the above designs, we propose an iterative optimization scheme where we alternately train the tracking controller and mine useful imitation-ready data, arriving us at a powerful tracking controller finally. 



% With these designs, we propose an iterative optimization scheme that alternates between: 1) training the tracking controller using high-quality imitation-ready data, and 2) mining higher-quality and more diverse data with the tracking controller via a homotopy optimization process.

% where we alternately 1) train the tracking controller via high-quality imitation-ready data, and 2) mine higher-quality and more diverse data using the tracking controller through a homotopy optimization scheme. 

% Therefore we propose to employ the power of tracking controller 
% for a kinematic reference trajectory involves training a trajectory-specific tracking policy to accurately follow the kinematic reference motions.
% Preparing the imitation-ready data by using RL to train per-trajectory tracking policies would fall 
% To prepare this imitation-ready data, our primary strategy for obtaining paired action sequences for a kinematic reference trajectory involves training a trajectory-specific tracking policy to accurately follow the kinematic reference motions.

% We propose \modelnamenspace, a learning-based neural tracking controller for dexterous manipulations from human references. The key idea lies in training a robust and generalizable tracking controller via reinforcement learning and imitation learning leveraging high-quality abundant imitation-ready data and leveraging the data flywheel to curate high-quality data and improve the tracking controller in a bootstrapping manner. Given a collection of human hand-object manipulation trajectories, we first retarget human hand trajectories to robotic dexterous hand sequences in the data preparation stage. 
% Our method then alternates between mining useful imitation-ready data from the reference motion set and training the tracking controller with the mined data. We propose two key designs to enable the data flywheel to function effectively. 
% First, we learn a neural tracking controller by carefully combining reinforcement learning and imitation learning techniques. Taking the best worlds of such two techniques, we can ensure the tracking controller have high generalization ability and versatility, taking advantage of imitating abundant tracking results, and also empowering it with high robustness towards unexpected states and noise, leveraging the power of reinforcement learning in handling abnormal situations. 
% Second, we develop a per-trajectory tracking scheme that can leverage the neural tracking controller to mine higher-quality and more diverse imitation-ready data through a homotopy optimization scheme. 
% To prepare imitation-ready data, our basic strategy to obtain the paired action sequence for a kinematic reference trajectory lies in training a trajectory-specific tracking policy to track the kinematic reference motions. 
% % At train time, given a collection of human hand-object manipulation trajectories, we first retarget human hands to robotic dexterous hands in the data preparation stage, resulting in a reference manipulation set containing retargeted hand trajectories and the original object pose sequences. 
% % To using model-free 
% % lies in training a trajectory-specific tracking policy for each trajectory. 
% % is using model-free RL to train a per-trajectory tracking policy for each trajectory.
% % However, the data quality and diversity would be restricted. 
% However, the data quality and diversity would be restricted. Therefore, we propose two techniques to improve the quality of per-trajectory tracking results so that the data flywheel can work effectively: 1) transferring tracking data prior from the tracking controller to the per-trajectory tracker to facilitate difficult problems solving; and 2) learning an innovative homotopy path generator, mimicking chain of thought, which can plan a beneficial homotopy optimization path that we can leverage to tackle previously unsolvable per-trajectory tracking problem via a homotopy method. 
% With the above designs, we propose an iterative optimization scheme where we alternately 1) train the tracking controller via high-quality imitation-ready data, and 2) mine higher-quality and more diverse data using the tracking controller through a homotopy optimization scheme. 


% high generalization ability and versatility 
% the tracking controller 
% let the tracking controller benefit from imitation-ready data, obtaining high generalization ability
% to make sure the tracking controller can benefit from imitation-ready data while still being robust to state disturbance. 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% We have three key designs to enable the data flywheel to function effectively. 
% neural tracking controller via reinforcement learning the imitation 
% leveraging high-quality abundant imitation-ready data to train a tracking controller jointly with RL and tackling the difficulties in acquiring such data by leveraging the data flywheel to curate high-quality data and improve the tracking controller in a bootstrapping manner. At train time, given a collection of human hand object manipulation trajectories, we first retarget human hands to robotic dexterous hands in the data preparation stage, resulting in a reference manipulation set containing retargeted hand trajectories and the original object pose sequences.  Our method then alternates between mining useful imitation-ready data from the reference motion set and training the tracking controller with the mined data. We have three key designs to enable the data flywheel to function effectively. 
% First, we learn a neural tracking controller by carefully combining reinforcement learning and imitation learning techniques to make sure the tracking controller can benefit from imitation-ready data while still being robust to state disturbance. 
% % leverage both imitation learning and reinforcement learning techniques to train the neural tracking controller to 
% To combine the advantages from both sides, we devise an RL-based method for solving the general dexterous manipulation tracking problem. We then introduce an action supervision loss which encourages the policy to imitate high-quality imitation-ready data with action sequences. 
% Second, we develop a per-trajectory tracking scheme that can leverage the neural tracking controller to mine higher-quality and more diverse imitation-ready data through a homotopy optimization scheme. 
% % imitation-ready data. 
% % high-quality imitation-ready data. 
% % leverage the general tracking controller to produce better imitation-ready data.
% To prepare imitation-ready data, our basic strategy is using model-free RL to train a per-trajectory tracking policy for each trajectory. However, the data quality and diversity would be restricted due to the limited capability of reinforcement learning in solving difficult tracking problems. Therefore, we propose two techniques to improve the quality of per-trajectory tracking results to ensure the data flywheel works effectively: 1) transferring tracking data prior from the tracking controller to the per-trajectory tracker to facilitate difficult problems solving; and 2) learning an innovative homotopy path generator, mimicking chain of thought, which can plan a beneficial homotopy optimization path that we can leverage to tackle previously unsolvable per-trajectory tracking problem via a homotopy method. 
% With the above designs, we propose an iterative optimization scheme where we alternately 1) train the tracking controller via high-quality imitation-ready data, and 2) mine higher-quality and more diverse data using the tracking controller through a homotopy optimization scheme. 
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% for a difficult-to-track trajectory, 
% plans a homotopy optimization path, consisting of a series of tracking tasks, 
% We solve the per-trajectory tracking problem to prepare data via three techniques. 
% To obtain high-quality imitation data with action trajectories that can successfully drive the dexterous robotic hand to track the correspondence hand and object sequence, we leverage the popular model-free RL-based method by training a per-trajectory tracking policy using RL and use the tracking result as the optimized imitation-ready data. Despite no expectation in the versatility and generalization ability in the per-trajectory tracker, tracking difficult tracking problem is still very challenging. to improve the data quality and include more diverse imitation-ready data into the training of tracking controller, we propose to leverage the general tracking prior encoded in the tracking controller to improve the per-trajectory tracker optimization. Third, we design a homotopy path generator, mimicking the chain of thought, to better leverage the general tracking controller for mining imitation-ready data from challenging manipulation skills. Soley relying on tracking prior encoded in the tracking controller to improve the per-trajectory tracker is still not enough to solve a very difficult per-trajectory tracking problem. It is because that the hard-to-track trajectories are too out-of-distribution to the data prior encoded in the tracking controller, trained with imitation-ready data that has been successfully tracked. Thus such trajectory problems can hardly benefit from the tracking prior from the current tracking controller. To resolve these challenges, we propose to solve these problems in a homotopy method. Specifically, instead of solving the difficult per-trajectory tracking problem directly, we start from tracking a different trajectory to obtain its tracking result. After that we design a technique which allows us to let the difficult per-trajectory tracking problem benefit from the tracking result. In this way we are able to solve the difficult tracking task by starting from an easy task and gradually solve a series of tsks with the tracking results transferred to the next one. Finding such beneficial optimization paths, however, is very time-consuming. To further improve the efficiency, we propose to learn a tracking task structure prior model, which, in the inference time, can efficiently propose beneficial optimization paths for a tough single trajectory tracking problem. With the above three designs, we alternately 1) train the tracking controller and learn the tracking task structure prior model with high-quality imitation-ready data and 2) improve the data via the tracking controller and the tracking task structure prior model. The data flywheel can then function well, making it possible for us to train a strong tracking controller finally. 
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


% solving the manipulation 
% , with delicately designed action space, observations, and rewards, for solving the manipulation tracking problem. 
% Second, we develop a per-trajectory tracking scheme which can 
% to train the tracking controller with
% a learning from demonstration sch
% leverage both the reinforcement learning and the imitation learning to learn  
% both reinforcement learning and imitation learning to optimize the tracking 
% imitation learning 
% leverage an imitation-augmented reinforcement learning scheme to make sure the tracking controller can benefit from imitation-ready data while still being robust to state disturbance. We devise a basic RL-based method to train the tracking controller with delicately designed action space, observations, and rewards for the general manipulation tracking problem. To let the policy benefit from the imitation-ready data, we add an additional supervised loss which, at each time, encourages the policy to imitate the action of the corresponding timestep in the trajectory.  Second, we develop a per-trajectory tracking 
% to first mine useful optimization paths from the training dataset and 
% to transfer the tracking result 
% propose to find an optimization path, 
% With the power of tracking prior 
% Per-trajectory tracking problem where the trackin
% encourage the policy t


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% To realize this vision, our method consists of three key designs: 1) an imitation augmented reinforcement learning technique that can effectively leverage high-quality action labeled trajectories to improve the tracking controller, 2) a homotopy method which effectively resolves the difficulty in challenging per-trajectory tracking problems, an tracking task structure prior model which encodes beneficial homotopy optimization paths, and 3) an iterative approach where per-trajectory trackers are improved via the tracking data prior and the task structure prior, together with the tracking controller getting stronger. 
% % tracking data prior from the tracking controller and the task structure prior 
% % automatic tracking task structure construction 
% % leveraging beneficial optimization paths
% % leveraging cross-tracking task relations, an automatic tracking task structure construction strategy where we identify cross-task structural relations in homotopy optimization, and a tracking task structure model for describing such relations to plan correct optimization paths for better solving per-trajectory tracking problem, and 3) an interactive optimization approach where per-trajectory trackers are gradually improved via tracking prior from the general tracking controller and effective optimization path scheduling, together with the tracking controller progressively getting stronger. 
% The imitation-augmented RL effectively leverages high-quality imitation-ready data with successful action trajectories to ease the training. Considering the difficulty in solving challenging single trajectory tracking problems, we propose two techniques to improve the single trajectory tracking optimization. The first one is a homotopy method that improves single trajectory tracking problem's solving via gradually solving each tracking problem in an optimization path. Finding correct optimization paths is very time-consuming. Therefore, we propose to learn a tracking task structure prior model that encodes beneficial optimization paths. During the inference, the structure prior model can propose beneficial optimization paths directly. Secondly, we consider leveraging general tracking prior encoded in the tracking controller to facilitate per-trajectory trackers' optimization. In more detail, we initialize the per-trajectory tracking policy via the action sequence proposed  by the tracking controller, followed by re-optimizing the per-trajectory tracker. 
% % transfer general tracking data prior encoded in the tracking 
% % To efficiently find correct o
% % in exploring the tracking task structure 
% % and improve single trajectory optimizing via correct optimization paths. Specifically, for a trajectory $A$ which is hard to track, we propose to leverage the homotopy optimization to finally solve it by gradually solving each tracking problem in an optimization path. Searching for correct optimization paths for each trajectory, however, is very expensive. Therefore, we propose to learn a model that can encode such structural tracking problem relations and use it as a structure prior to provide us with correct optimization paths. 
% % Secondly, we transfer general tracking prior to encoding in the tracking controller to speed up and enhance the per-trajectory trackers' optimization. 
% With the above designs, our method leverages an iterative approach that alternately 1) solves the per-trajectory tracking problem with the help of the tracking controller and the task structure prior model, and 2) trains a stronger tracking controller via supervision from imitation-ready data with high-quality action sequences. Specifically, in the beginning, we first sample a batch of samples, \emph{i.e.,} one hundred, to optimize for their per-trajectory tracking controllers. After that, the successful tracking results are collected to construct the first version of the action-labeled dataset. We then train the general controller via supervision provided by these action sequences. In the second stage, we sample another batch of trajectories from those that the current general policy cannot perform well. By optimizing their per-trajectory trackers using the initialization from the tracking controller, we obtain the second version of the action-label dataset. We then re-train the general tracking policy via guidance provided by the labeled dataset. In this stage, we find beneficial homotopy optimization paths via a heuristic-guided breadth-first searching strategy. The searched results can improve per-trajectory tracking results and can serve as the training from which we construct our tracking task structure prior model. In the third stage, we resample trajectories to grow the imitation-ready action-labeled trajectory dataset. We conduct per-trajectory tracking and try to improve previous tracking results via the guidance from the tracking data prior encoded in the tracking controller and the homotopy method leveraging optimization paths proposed by the tracking task structure prior model.  After that, we then re-train the general controller via these successful tracking results. 
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% high-quality labeled trajectories. 
% the third part of trajectories which are tracked via RL under the guidance of the tracking controller and curricula proposed by the learned curriculum scheduler.
% mine task curricula for improving difficult trajectories' tracking and learn the curriculum scheduler to model the curriculum prior. 
% rollout trajectories of such initial samples 
% 
% progressively improve the capability of the tracking controller. In the beginning, we 
% which gradually 
% solve a series of per-trajectory tracking optimization problem 
% tracking curriculum which can effectively solve difficult per-trajectory tracking problems, an automatic curricula mining strategy, and curriculum scheduler that models tracking curricula for efficient curriculum scheduling, and 3) an interactive optimization approach where per-trajectory trackers are gradually improved via tracking prior from the general tracking controller and effective curriculum scheduling, together with the tracking controller progressively getting stronger. 


% Moreover, the tracking prior encoded in the 
% Considering the limitations of the current advanced RL in solving single trajectory tracking problems, we design two strategies to improve per-trajectory trackers and optimize for raising the quality and diversity of the action-labeled dataset. The 
% % action-labeled trajectories produced by per-trajectory trackers to ease the training. 
% Considering the limitations of the current advanced RL in solving single trajectory tracking problems, we design two strategies to improve per-trajectory trackers and optimize for raising the quality and diversity of the action-labeled dataset. The first one lies in transferring general tracking prior to encoding in the tracking controller to speed up and enhance the per-trajectory trackers' optimization. Instead of optimizing the single trajectory tracker directly, we propose to use the data prior provided by the tracking controller which can offer us with a rough action trajectory to initialize the per-trajectory tracking optimization. 
% Secondly, instead of tracking each single trajectory along, we try to let the tracking task structural prior to plan optimization paths for solving tough single-trajectory tracking problems. 
% % and the inference time prior from the tracking task structural prior model to provide us with correct optimization paths to solve tough tasks finally. 
% % % use the action trajectory rollout from the tracking controller to initialize the policy so that the per-trajectory tracker can benefit from the general tracking prior, avoiding getting stuck into local optima and with the potential to converge faster. Secondly, instead of tracking each single trajectory along, we try to explore the potential of benefiting the single trajectory tracking problem, which is difficult to solve, from the tracking results of a different trajectory, formulating a tracking-curriculum where we start from simple problems and gradually solve the tough task finally. 
% Specifically, for a trajectory $A$ which is hard to optimize, the tracking curriculum tries to transfer the optimized action trajectory for task $B$ to initialize its policy to solve trajectory $A$'s tracking problem better finally. Since not every trajectory would benefit the tracking of trajectory $A$, leveraging this approach without to improve the per-trajectory tracking requires trail-and-errors for each trajectory, if without any prior guidance. Therefore, we further propose to learn a curriculum scheduler that can automatically plan a task curriculum to solve a difficult tracking problem. In more detail, we first mine such task curricula via testing such task relations from a small amount of data. After that, we train a model to describe such relations. 
% With the above designs, our method leverages an iterative approach that progressively improve the capability of the tracking controller. In the beginning, we first sample a batch of samples, \emph{i.e.,} one hundred, to optimize for their per-trajectory tracking controllers. After that, the successful rollout trajectories of such initial samples are collected to construct the first version of the action-labeled dataset. We then train the general controller via supervision provided by this dataset. In the second stage, we sample another batch of trajectories from those that the current general policy cannot perform well. By optimizing their per-trajectory trackers using the initialization from the tracking controller, we obtain the second version of the action-label dataset. We then re-train the general tracking policy via guidance provided by the labeled dataset. In this stage, we mine task curricula for improving difficult trajectories' tracking and learn the curriculum scheduler to model the curriculum prior. In the third stage, we resample for the third part of trajectories which are tracked via RL under the guidance of the tracking controller and curricula proposed by the learned curriculum scheduler. After that, we then re-train the general controller via all high-quality labeled trajectories. 


% where the per-trajectory trackers are efficiently and effectively improved together with a tracking controller that gradually become stronger taking advantage of tracking prior from the tracking controller and the curriculum scheduler. 
% an automatic curricula mining strategy that 
% breadth-first-searching that effecitlvye mines cross-task synergy relations and a curriculum schduler that models such relations for efficient curriculum schedule for improving per-trajectory trackers, and 3) an interactive approach where the per-trajectory trackers are efficiently and effectively improved together with a tracking controller that gradually become stronger taking advantage of tracking prior from the tracking controller and the curriculum scheduler. 
% a curriculum scheduler that models 
% an imitation augmented RL for improving the general tacker's performance with guidance from action-labeled trajectories produced by well-optimized single trajectory tracking specialists and a structural task relation space modeling as well as a task curriculum based on the molded task relations for improving single trajectory specialist controllers.
% from well-optimized trajectory-specific controllers  
% single trajectory tracking controllers to 

% current general tracking policy and the the structural cross-task relational model. 
% we search for cross-task relations, construct the structural task space and also distill them into a trajectory transformation model. 
% we try to transfer the optimization result from task $B$ to initialize its policy 
% Similar to the way of transferring tracking prior to improve the single trajectory tracking, for 
% Considering the difficulty and challenges in the dexterous manipulation tracking problem, single trajectory tracking controller optimization may still pose a significant challenges even for the current advanced RL algorithms, especially for very difficult manipulations. Therefore, to solve the manipulation tracking problem in such extremely challenging trajectories and to continuously improve the quality of specialist controllers, we propose to explore cross-task synergies in the manipulation tracking problem and improve each single trajectory tracker's performance leveraging cross-task synergies. Specifically, for each current task $A$, we wish to explore its ``parent tasks'' $\mathcal{P}_A$, so that task $A$'s optimization can benefit from the optimization result of its parent task $B\in \mathcal{P}_A$. By exploring such cross-task relations via bread-first-search, we can construct a structural task relation space from the given dataset. Each task pair is connected by a direct edge indicating which one can benefit from the other one and to what extent. We further propose to distill such searched cross-task relations into a trajectory transformation model, realized by a conditional diffusion model. This model can be further used to transform one given hard-to-optimize task to its ``parent task'', formulating a task curriculum to solve the current difficult task. We therefore train the general tracking controller by iteratively solving a single trajectory tracking problem for constructing a better action-labeled dataset and training the general tracker via the imitation augmented RL to let it benefit from such labeled guidance. Balancing the efficiency and the quantity as well as the quality of the action labeled dataset, our method for training the general tracking policy comprises three stages. At the beginning, we first sample a batch of samples, \emph{i.e.,} one hundred, to optimize for their trajectory-specific specialist tracking policies. After that, the successful rollout trajectories of such initial samples are collected to construct the first version of the action-labeled dataset. We then train the general tracking policy together via the imitation augmented RL via the labeled dataset. In the second stage, we sample another batch of trajectories from those that the current general policy cannot perform well. By conducting the trajectory-specific optimization on those trajectories, we obtain the second version of the action-label dataset. We then re-train the general tracking policy via guidance provided by the labeled dataset. In this stage, we search for cross-task relations, construct the structural task space and also distill them into a trajectory transformation model. In the third stage, we resample for the third part of trajectories which are tracked via RL under the guidance of the current general tracking policy and the the structural cross-task relational model. After that, we then re-train the general controller via all high-quality labeled trajectories till now. 

% demonstrate 

We demonstrate the superiority of our method and compare it with previous methods on challenging manipulation tracking tasks in two datasets, describing expressive hand-object interactions in daily and functional tool-using scenarios, involving complex object movements, difficult and subtle in-hand re-orientations, interactions with thin objects, and frequent hand-object rich contact variations. We conduct experiments in both the simulator, \emph{i.e.,} Isaac Gym~\citep{makoviychuk2021isaac},  and the real world to demonstrate the efficacy, generalization ability, and robustness of our general tracker to accomplish a wide range of manipulation tracking tasks and even excellently track novel manipulation trajectories (Figure~\ref{fig_intro_teaser}). We can track difficult manipulations involving expressive and complex object movements, detailed and subtle in-hand re-orientations, and frequent rich hand-object contact variations. Our approach successfully surpasses the previous methods both quantitatively and qualitatively, achieving more than 10\% success rate than the previous best-performed method. Besides, we conduct further analysis and demonstrate the various recovery behaviors of our controller, demonstrating its robustness to unexpected situations. Thorough ablations are conducted to validate the efficacy of our designs.

Our contributions are three-fold:
\begin{itemize}
    \item A scheme that combines reinforcement learning and imitation learning to effectively train a generalizable, versatile, and robust tracking controller, utilizing abundant high-quality imitation-ready data.
    % \item 
    % for effectively learning a generalizable, versatile, and robust tracking controller by leveraging reinforcement learning and imitation learning, utilizing abundant high-quality imitation-ready data.
    \item A per-trajectory optimization scheme for addressing challenging tracking problems using the tracking controller within a homotopy optimization framework.
    \item A homotopy optimization method for effectively solving difficult single trajectory tracking problems, an automatic homotopy optimization path exploration strategy, and a homotopy path generator that efficiently proposes beneficial optimization paths during inference.
    % \item A scheme for effectively learning a generalizable, versatile, and robust tracking controller taking advantage of reinfrmcemetn learning and imitation learning, leveraging abundant high-quality of imitation-ready data. 
    % \item A per-trajectory optimization scheme for solving difficult tracking problems using the tracking controller through a homotopy optimization paradigm. 
    % \item A homotopy method for solving very challenging single trajectory tracking problems effectively, an automatic homotopy optimization path exploration strategy, and a homotopy path generator that can efficiently propose beneficial optimization paths in the inference time. 
    % imitation-augmented RL technique that effectively facilitates tracking controller optimization via supervision from labeled trajectories with high-quality action sequences; 
    % eases the optimization difficulty of the tracking controller via supervision provided by high-quality action-labeled trajectories; 
    % A homotopy method for solving difficult tracking problems, an automatic tracking task optimization path searching strategy and a tracking task structure prior  model that can efficiently propose optimization paths for solving challenging optimization problems efficiently; 
    % An automatic 
    % tracking-curricula mining strategy and the design of a curriculum scheduler that expressively models the curriculum priors; 
    % An iterative approach that effectively improves per-trajectory trackers with the help from the tracking controller and the task structure prior model and gradually train stronger tracking controller. 
    % via tracking prior from the general tracking controller and the homotopy optimization path from the tracking task structure prior to solve challenging problems. 
    % the curriculum proposed by the curriculum scheduler. 
    % progressively improves the tracking controller via gradually enlarged and improved action-labeled data. 
\end{itemize}


% our tracker exhibits various recovery behaviours, 
% we also demonstrate its 
% Besides, optimizing through the physics curriculum can significantly enhance the performance of previously under-performed RL-based methods, almost completing the tracking problem from failure, as demonstrated in Fig.~\ref{fig_intro_teaser}. This indicates the universality of our approach to embodied AI through optimization via a physics curriculum. Thorough ablations are conducted to validate the efficacy of our designs.
% best-performed method 
% such as large rotations and complicated tool-using such as using a spoon to bring the water back and forth. Our approach successfully surpasses the previous best-performed method both quantitatively and qualitatively, achieving more than 11\% success rate than the previous best-performed method. Besides, optimizing through the physics curriculum can significantly enhance the performance of previously under-performed RL-based methods, almost completing the tracking problem from failure, as demonstrated in Fig.~\ref{fig_intro_teaser}. This indicates the universality of our approach to embodied AI through optimization via a physics curriculum. Thorough ablations are conducted to validate the efficacy of our designs.




% To be more specific, our method for training the general control policy compresises three stages. In the f
% % from which we can transfer the corresponding optimization results 


% expressive and difficult interactions 
% general controller that can universally track every mnaipulation trajectories in the dataset and can well generalize to unseen manipulations. \todo{rewrite this} 
% and grasping~\cite{xu2023unidexgrasp,wan2023unidexgrasp2}. 
% In this work we wish for a general controller 
% The task is inherently difficult 

% Some existing works rely on high-fidelity black-box simulators, where a small difference in robot control can result in dramatically different manipulation outcomes due to abrupt contact changes, making the tracking objective highly non-smooth and hard to optimize~\cite{qin2022dexmv,chen2023visual,christen2022d,rajeswaran2017learning,andrychowicz2020learning}. In this way, their tasks are restricted to relatively simple goal-driven manipulations such as pouring and re-locating~\cite{rajeswaran2017learning,qin2022dexmv,christen2022d,zhang2023artigrasp}, in-hand re-orientation, flipping and spinning~\cite{chen2023visual,andrychowicz2020learning} with a fixed-root robot hand, or manipulating objects with simple geometry such as balls~\cite{mordatch2012contact}. 



% focusing on a wide range of manipulation tasks -- dexterous manipulations tracking, we aim to explore
% we aim to explore the possibility of training a dexterous manipulation general policy. 

% Advancing an embodied agent's capacity to interact with the world represents a significant stride toward achieving general artificial intelligence. Due to the substantial costs and potential hazards of setting up real robots to do trial and error, the standard approach for developing embodied algorithms involves learning in physical simulators~\cite{coumans2016pybullet,makoviychuk2021isaac,todorov2012mujoco,wang2019redmax,hwangbo2018per,freeman2021brax,howell2022dojo} before transitioning to real-world deployment. In most cases, physical simulators are treated as black boxes, and extensive efforts have been devoted to developing learning and optimization methods for embodied skills within these black boxes. Despite the considerable progress~\cite{rajeswaran2017learning,chen2023visual,chen2021system,akkaya2019solving,christen2022d,zhang2023artigrasp,qin2022dexmv,liu2022herd,wu2023learning,gupta2016learning,yao2022controlvae,fussell2021supertrack,grandia2023doc,peng2018deepmimic,wang2023physhoi,mordatch2012contact}, the question like whether the simulators used are the most suitable ones is rarely discussed.
% In this work, we investigate this issue and illustrate how optimizing the simulator concurrently with skill acquisition can benefit a popular yet challenging task in robot manipulation -- dexterous manipulation transfer.



