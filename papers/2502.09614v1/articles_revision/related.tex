
% \usepa

\vspace{-10pt}

\section{Related Work}

\vspace{-5pt}

% \noindent\textbf{Dexterous manipulation skill acquisition.} 

Equipping robots with human-level dexterous manipulation skills is crucial for future advancements. Previous approaches either rely on model-based trajectory optimization or model-free reinforcement learning (RL). Model-based methods face challenges due to the complexity of dynamics, often requiring approximations~\citep{pang2023global,Jin2024ComplementarityFreeMM,pang2021convex}. Model-free approaches, using RL~\citep{rajeswaran2017learning,chen2023visual,chen2021system,christen2022d,zhang2023artigrasp,qin2022dexmv,liu2022herd,wu2023learning,gupta2016learning,wang2023physhoi,mordatch2012contact}, focus on goal-driven tasks with task-specific rewards, limiting their generalization across diverse tasks.
% limiting generalization to diverse tasks.
Our work explores a general controller for dexterous manipulations. Besides, learning via mimicking kinematic trajectories has recently become a popular train to equip the agent with various expressive skills~\citep{Jenelten2023DTCDT,Luo2023PerpetualHC,Luo2023UniversalHM}. DTC~\citep{Jenelten2023DTCDT} proposes a strategy that can combine the power of model-based motion planning and RL to overcome the sample inefficiency of RL. 
% overcome the deficiency of the sample inefficiency of RL. 
In the humanoid motion tracking space, PHC~\citep{Luo2023PerpetualHC} proposes an effective RL-based training strategy to develop a general humanoid motion tracker. Recently, OmniGrasp~\citep{Luo2024GraspingDO} proposes to train a universal grasping and trajectory following policy. The policy can generalize to unseen objects as well as track novel motions. However, their considered motions are still restricted in grasping and trajectory following, leaving the problem of tracking more interesting and difficult trajectories such as those with subtle in-hand manipulations largely not explored. In this paper, we focus on these difficult and challenging manipulations. Moreover, we are also related to recent trials on combining RL with imitation learning. To overcome the sample inefficiency problem of RL and to facilitate the convergence, various approaches have been developed aiming to augment RL training with demonstrations~\citep{Sun2018TruncatedHP,Hester2017DeepQF,Booher2024CIMRLCI,Liu2023BlendingIA}. In our work, we wish to leverage high-quality demonstrations to guide the agent's explorations. Unlike previous work where demonstrations are readily available, acquiring a sufficient amount of high-quality robot tracking demonstrations remains a significant challenge in our task.
% Different from previous literature where demonstrations are assumed available, how to acquire abundant high-quality robot tracking demonstrations is a challenging problem in our task. 





% , however, struggle with the complexity of dynamics and often require approximations
% Equipping robots with human-level dexterous manipulation skills is critical for advancing robotics. Previous approaches largely rely on either model-based trajectory optimization or model-free reinforcement learning (RL). Model-based methods face challenges due to the complexity of dynamics, often requiring approximations~\citep{pang2023global,Jin2024ComplementarityFreeMM,pang2021convex}. On the other hand, model-free methods, using RL~\citep{rajeswaran2017learning,chen2023visual,chen2021system,christen2022d,zhang2023artigrasp,qin2022dexmv,liu2022herd,wu2023learning,gupta2016learning,wang2023physhoi,mordatch2012contact}, focus on task-specific rewards, limiting their generalization across diverse tasks. In our work, we aim to develop an agent with versatile manipulation skills. Recently, learning by mimicking kinematic trajectories has gained popularity as a way to equip robots with a wider range of expressive skills~\citep{Jenelten2023DTCDT,Luo2023PerpetualHC,Luo2023UniversalHM}. For instance, DTC~\citep{Jenelten2023DTCDT} presents a hybrid strategy that combines model-based motion planning with RL, addressing RL's sample inefficiency. In humanoid motion tracking, PHC~\citep{Luo2023PerpetualHC} introduces an effective RL-based training approach to develop a generalized humanoid motion tracker. Recently, OmniGrasp~\citep{Luo2024GraspingDO} has trained a universal policy for grasping and trajectory tracking that generalizes to unseen objects and novel motions. However, the motions considered are limited to grasping and trajectory following, with more complex tasks remaining largely unexplored. Our paper focuses on these challenging manipulation tasks. Additionally, we explore the integration of RL with IL to overcome the sample inefficiency problem and enhance convergence. Several methods have been developed to augment RL with demonstrations~\citep{Sun2018TruncatedHP,Hester2017DeepQF,Booher2024CIMRLCI,Liu2023BlendingIA}. Unlike previous work where demonstrations are readily available, acquiring a sufficient amount of high-quality robot tracking demonstrations remains a significant challenge in our task.
% remains a significant challenge


% Recent methods combining RL with motion imitation~\citep{Jenelten2023DTCDT,Luo2023PerpetualHC} enhance expressiveness, yet they focus on simpler motions like grasping. In contrast, we target complex manipulations involving subtle in-hand re-orientations. To improve RL efficiency, imitation learning with demonstrations~\citep{Sun2018TruncatedHP,Liu2023BlendingIA} has been explored, but obtaining high-quality robot tracking demonstrations remains a challenge in our task.

% Equipping an embodied agent with dexterous manipulation skills is a vital step towards a future robot. Previous methods either adopt a model-based trajectory optimization approach or resort to a model-free method aiming to train a policy without assumptions on prior knowledge about the system. Developing a model-based method to solve dexterous manipulation is extremely challenging due to the complexity of the underlying dynamics. Therefore, previous methods would typically develop an approximated model~\citep{pang2023global,Jin2024ComplementarityFreeMM,pang2021convex}. A more popular approach is assuming the system dynamics is unknown and training a policy using advanced reinforcement learning~\citep{rajeswaran2017learning,chen2023visual,chen2021system,christen2022d,zhang2023artigrasp,qin2022dexmv,liu2022herd,wu2023learning,gupta2016learning,wang2023physhoi,mordatch2012contact}. Despite recent promising advances that have been demonstrated in the literature, they typically focus on learning goal-driven skills and require task-specific reward designs~\citep{chen2023visual,chen2021system,rajeswaran2017learning,christen2022d,zhang2023artigrasp,qin2022dexmv}, preventing them from generalizing to a wide range of diverse and novel tasks. In this work, we take a step further and explore the problem of learning a general controller for dexterous manipulations. Besides, learning a controller via mimicking kinematic trajectories has recently become a popular train to equip the controller with various expressive skills~\citep{Jenelten2023DTCDT,Luo2023PerpetualHC,Luo2023UniversalHM}. DTC~\citep{Jenelten2023DTCDT} proposes a strategy that can combine the power of model-based motion planning and RL to overcome the deficiency of the sample inefficiency of RL. In the humanoid motion tracking space, PHC~\citep{Luo2023PerpetualHC} proposes an effective RL-based training strategy to develop a general humanoid motion tracker. Recently, OmniGrasp~\citep{Luo2024GraspingDO} proposes to train a universal grasping and trajectory following policy. The policy can generalize to unseen objects as well as track novel motions. However, their considered motions are still restricted in grasping and trajectory following, leaving the problem of tracking more interesting and difficult trajectories such as those with subtle in-hand re-orientations largely not explored. In this paper, we focus on these difficult and challenging manipulations. Moreover, we are also related to recent trials on combining RL with imitation learning. To overcome the sample inefficiency problem of RL and to facilitate the convergence, various approaches have been developed aiming to augment RL training with demonstrations~\citep{Sun2018TruncatedHP,Hester2017DeepQF,Booher2024CIMRLCI,Liu2023BlendingIA}. In our work, we wish to leverage high-quality demonstrations to guide the agent's explorations. Different from previous literature where demonstrations are assumed available, how to acquire abundant high-quality robot tracking demonstrations is a challenging problem in our task. 
% such as in-hand re-orientation using the hand with a fixed base~\citep{chen2023visual,chen2021system}~\citep{chen2023visual,chen2021system}, grasping, re-locating, relatively simple tool-using~\citep{rajeswaran2017learning,christen2022d,zhang2023artigrasp,qin2022dexmv}, or single manipulation trajectory mimicking~\citep{liu2024quasisim}. 
% Recent trials only learning general skills are still restricted to relatively simple tasks such as grasping~\citep{xu2023unidexgrasp,wan2023unidexgrasp2}. 

% want to take a step further and explore the possibility of learning a general tracking controller that can accomplish a wide range of challenging manipulation tasks. 

% via using planned motions to give dense rewards, and also at the same time keep the robustness of RL in the final controller.

% where RL is leveraged to train a tracking policy with hard mining strategies. 

% action-labeled data is itself a difficult question in our problem. 
% in training a general tracking controller using RL. 
% such as complex tool-using with non-trivial object motions and subtle manipulations using fingers. 
% even the MoCap trajectories that are never seen during the training. 
% Leveraing 
% Training a general tracking 
% Our method is also related 
% \todo{learning from humans? }
% mimicking a diverse range of challenging 
% Although there are recent trails that try to learn general controller, they are still restricted to 
% Althrough there are recent trails on learning general controllers 
% is leveraging a model-free 
% Despite impressive achievements they have demonstrated, 
% a simplified model such as using a relaxed soft contact model~\citep{pang2023global,drake,Jin2024ComplementarityFreeMM}.

% \noindent\textbf{Tracking control.} Learning a controller via mimicking kinematic trajectories has recently become a popular train to equipe the controller with various expressive skills~\citep{Jenelten2023DTCDT,Luo2023PerpetualHC,Luo2023UniversalHM}. DTC~\citep{Jenelten2023DTCDT} proposes a strategy that can combine the power of model-based motion planning and RL to overcome the deficiency of the sample inefficiency of RL via using planned motions to give dense rewards, and also at the same time keep the robustness of RL in the final controller. In the humanoid motion tracking space, PHC~\cite{Luo2023PerpetualHC} proposes an effective training strategy where RL is leveraged to train a tracking policy with hard mining strategies. Recently, OmniGrasp~\citep{Luo2024GraspingDO} proposes to train a universal grasping and trajectory following policy. The policy can generalize to unseen objects as well as track novel motions, even the MoCap trajectories that are never seen during the training. However, the difficulty of their considered motion is still restricted in grasping and trajectory following, leaving the problem of tracking more interesting and difficult trajectories such as those with complex object movements with subtle in-hand re-orientations largely not explored. In this paper, we focus on difficult and challenging manipulations such as complex tool-using with non-trivial object motions and subtle manipulations using fingers. 

% \noindent\textbf{Reinforcement learning with imitation learning.} 
% How to combine RL with imitation learning has attracted lots of attention. To overcome the sample inefficiency problem of RL and to facilitate the convergence, various approaches are developed aiming to augment RL training with demonstrations~\citep{Sun2018TruncatedHP,Hester2017DeepQF,Booher2024CIMRLCI,Liu2023BlendingIA}. In our work, we wish to leverage high-quality offline data to ease the difficulty in training a general tracking controller using RL. Different from previous literature where the labeled dataset is assumed available, how to acquire enough and high-quality action-labeled data is itself a difficult question in our problem. \todo{reifne this para.}
% the training difficulty of RL in solving the 
% the difficulty of training a general tracking controller 
% Despite their achievements, what's a proper way to combine the power of both sides in the dexterous manipulation space remains largely unexplored. Moreover, 

\vspace{-5pt}