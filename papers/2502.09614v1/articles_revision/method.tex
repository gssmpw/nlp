
% \section{Generalizable Dexterous Manipulation Tracking}
% \section{Dexterous Manipulation Tracking Control}
% \section{Generalizable Neural Tracking Control for Dexterous Manipulation}
\section{Method} \label{sec:method_sec}

\vspace{-5pt}

% \todo{add the retargeting part}
 

\begin{figure}[h]
  \centering
  \vspace{-10pt}
  \includegraphics[width=\textwidth]{figs/methdo_rb_8.pdf}
  \vspace{-20pt}
  \caption{\footnotesize
  % cks the potential for tr
  % Generalizable Dexterous Manipulation Tracking with Imitation Augmented RL and Structural Task Space Modeling. 
  % Dexterous Manipulation Tracking Control.
  % Iterative optimization to train a universal dexterous manipulation tracking controller. 
  % By augmenting RL with guidance from high-quality action-labeled data, \modelname unleashes the possibility of training a universal manipulation controller from human references. With the curriculum proposed by a curriculum scheduler that models the tracking curricula and the tracking prior provided by the tracking controller, we can in turn improve the quality of action-labeled trajectories. We adopt an interactive approach which finally offers us a universal tracking controller. 
  % \modelname unleashes the possibility of training a universal manipulation controller from human references by iteratively optimizing high-quality action-labeled trajectories and training the tracking controller via imitation-augmented RL with supervision from the labeled trajectories. Guiding RL with additional supervision from high-quality action-labeled data, \modelname effectively eases the difficulty in learning a universal tracking controller. Leveraging the tracking prior from the controller as well as the tracking curriculum proposed by a learned curriculum scheduler, we can effectively improve the quality and diversity of the action-labeled trajectories. 
  % \eric{The figure is not quite intuitive so far at the tracking prior part and the homotopy optimization part. I would suggest using a flow chart instead. I do not like the way how we visualize learning a neural network with PPO since it is not common and can not be understood immediately. We should consider checking other papers and borrowing their ways of illustration.}
  \textcolor{myblue}{\href{https://projectwebsite7.github.io/gene-dex-manip/}{\modelname} }
  % \href{https://projectwebsite7.github.io/gene-dex-manip/}{\modelname}
  learns a generalizable neural tracking controller for dexterous manipulation from human references. It alternates between training the tracking controller using abundant and high-quality robot tracking demonstrations and improving the data
  % the data
  % the data diversity and quality
  via the tracking controller through a homotopy optimization scheme.
  % By gradually optimizing the tracking controller and curating high-quality demonstration data in a bootstrapping manner, we can get a versatile, generalizable, and robust tracking controller finally. 
  % imitation-ready data 
  % unlocks the potential for training a universal manipulation controller from human references by iteratively curating high-quality, action-labeled trajectories and training the tracking controller through imitation-augmented reinforcement learning (RL) with supervision from these labeled trajectories. By guiding RL with supervision from the action-labeled data, we significantly reduce the challenges associated with learning a universal tracking controller. Utilizing the tracking prior from the controller and a tracking curriculum designed by a learned curriculum scheduler, we effectively enhance the quality and diversity of the action-labeled trajectories, driving improved performance.
  % The \textbf{parameterized quasi-physical simulator} relaxes the articulated multi rigid body dynamics as the \textit{parameterized point set dynamics}, 
  % controls the contact behavior via an unconstrained \textit{parameterized spring-damper contact model},
  % and compensates for unmodeled effects via \textit{parameterized residual physics networks}. 
  % We tackle the difficult dexterous manipulation transfer problem via \textbf{a physics curriculum}. 
  }
  \vspace{-15pt}
  \label{fig_method_pipeline}
\end{figure}

% \todo{change t to n}

% \todo{robot hand-obj kinematics dataset}

% (\emph{e.g.,} Allegro, LEAP, Shadow hand, etc.)

% The dexterous manipulation tracking control problem wishes to learn a tracking controller for a dexterous robot hand from human hand-object manipulation references. Given a manipulation sequence $(\mathbf{H}, \mathbf{O})$ describing the dexterous robot hand interacting with an object, \emph{e.g.,} obtained by kinematic retargeting from a human-object manipulation sequence, we expect the controller to robustly control the robot hand to manipulate the object so that both the resulting hand poses $\hat{\mathbf{H}}$ and the object poses $\hat{\mathbf{O}}$ can properly mimic their corresponding reference motions. We expect the controller to be able to track novel reference motions with unseen yet still challenging manipulation sequences and new objects. Such an ambitious vision is challenged by 1) the inherent difficulty underlying the tracking control problem resulting from intricate dynamics coming from frequently varying rich hand-object contacts and the complicated object movements, 2) the challenges in controlling a high-DoF dexterous robot hand to densely follow references, 3) the wish for a general tracking controller that can universally track every reference manipulations covering extremely challenging cases, and 4) the wish in high robustness and generalization ability side so that the controller can adapted to mimic unobserved references. 
% 

% The generalizable neural tracking control for dexterous manipulations from human references aims to leverage abundant and high-quality human-object interaction data to learn a generalizable manipulation tracking controller to equip a dexterous robot hand with versatile dexterous manipulation ability. Given a human-object interaction dataset containing, the tracking controller learns to track every manipulation sequence. At the test time, the tracking controller ought to generalize well to track novel, challenging human manipulation sequences with new objects, unseen hand, and object reference trajectories. Both advanced model-free RL based techniques and the model-based trajectory optimization would usually face significant challenges from the wish in the versatility and the generalization ability, the complex system, and the intricate dynamics from dynamic, rich contacts. 


% The problem is inherently difficult due to 1) the intricate dynamics involved in the dexterous manipulation that frequently challenges the optimization, 2) the high expectation in the versatility, high generalization ability, and the robustness of the controller. 

\textcolor{myblue}{\noindent\textbf{Terminologies and notations.} 
% Assuming a kinematic-only hand object state sequence, dexterous manipulation ``tracking'' means physically controlling a dexterous hand to interact with an object where the resulting state trajectory should be similar to the given goal trajectory. We use ``kinematic reference(s) (trajectories/sequences)'' to denote the kinematic-only robot hand-object state sequences retargeted from human hand-object manipulation trajectories. Each kinematic reference trajectory, denoted as  $\{ \hat{\mathbf{s}}_n \}_{n=0}^{N}$, is composed of the robot hand joint state and the object pose at each timestep $n$, \emph{i.e.,} $\hat{\mathbf{s}}_n$. A robot ``tracking demonstration'' is composed of a kinematic reference trajectory $\{ \hat{\mathbf{s}}_n \}$ and an expert action sequence for the robot hand $\{ \mathbf{a}_n^L \}$. By starting from the first frame $\mathbf{s}_0=\hat{\mathbf{s}}_0$ and controlling the robot hand using the corresponding action at each frame $\mathbf{a}_n^L$, the robot hand and object transit to the next state $\mathbf{s}_{n+1}$ according to the environment dynamics. An expert action trajectory should make sure the resulting transited state sequence $\{ \mathbf{s}_n \}_{n=0}^N$ close to the kinematic reference $\{\hat{\mathbf{s}}_n\}_{n=0}^N$. We use ``single trajectory tracking policy'' and ``trajectory-specific tracking policy'' to represent the policy that is only responsible for tracking one single trajectory. While ``neural tracking controller'' means a general tracking policy that is expected to track a diverse set of trajectories and can generalize to track new ones. 
% We say a neural controller is ``robust'' if it is resilliance towards distrubances such as large noises in the kinematic references like penetrations and unreachiable states~\citep{Li2024ReinforcementLF,Serifi2024VMPVM}. The neural tracking controller can ``generalize'' if it remains effective on unseen test scenarios involving new objects as well as unseen hand and object motions~\citep{Luo2024GraspingDO,xu2023unidexgrasp,wan2023unidexgrasp2}. The controller is with high ``adaptivity'' if it can still perform well as time-varying or time-invariant context has shifted, such as the changing contact events and varied dynamics parameters~\citep{Li2024ReinforcementLF,Qi2022InHandOR}.
% Assuming a kinematic-only hand-object state sequence, dexterous manipulation ``tracking'' involves controlling a dexterous hand to interact with an object such that the resulting state trajectory matches the goal trajectory. ``Kinematic references'' refer to robot hand-object state sequences retargeted from human manipulation trajectories, denoted as $\{\hat{\mathbf{s}}_n\}_{n=0}^N$, where $\hat{\mathbf{s}}_n$ includes the robot hand joint state and object pose at each timestep $n$. 
% A ``tracking demonstration'' consists of a kinematic reference $\{\hat{\mathbf{s}}_n\}$ and an expert action sequence $\{\mathbf{a}_n^L\}$. Starting from $\mathbf{s}_0=\hat{\mathbf{s}}_0$, the robot uses $\mathbf{a}_n^L$ at each step to transition states $\mathbf{s}_{n+1}$. 
% An ``expert action sequence'' should ensure the resulting state sequence $\{\mathbf{s}_n\}_{n=0}^N$ aligns with $\{\hat{\mathbf{s}}_n\}_{n=0}^N$. 
% A robust controller tolerates disturbances like kinematics noise and unreachable states~\citep{Li2024ReinforcementLF,Serifi2024VMPVM}. It demonstrates generalization if it performs well on unseen objects, motions, and scenes~\citep{Luo2024GraspingDO,xu2023unidexgrasp,wan2023unidexgrasp2}, and high adaptivity if it maintains performance despite shifting contexts, such as changing contact events or dynamics parameters~\citep{Li2024ReinforcementLF,Qi2022InHandOR}.
% A ``single trajectory tracking policy'' tracks one specific trajectory, while a ``neural tracking controller'' is a general policy capable of handling diverse trajectories and generalizing to new ones. 
Dexterous manipulation ``tracking'' involves controlling a robotic hand to mimic a kinematic hand-object state sequence, the goal trajectory, denoted as $\{\hat{\mathbf{s}}_n\}_{n=0}^N$. These ``kinematic references''  are retargeted from human manipulation trajectories, with $\hat{\mathbf{s}}_n$ representing the robot hand state and object pose at timestep $n$. A ``tracking demonstration'' pairs a kinematic reference $\{\hat{\mathbf{s}}_n\}$ with an expert action sequence $\{\mathbf{a}_n^L\}$, guiding the robot from $\mathbf{s}_0=\hat{\mathbf{s}}_0$ to achieve a state sequence $\{\mathbf{s}_n\}_{n=0}^N$ aligned with $\{\hat{\mathbf{s}}_n\}_{n=0}^N$. A ``robust'' controller can tolerate disturbances like kinematics noise and unreachable states. It demonstrates high ``generalization ability'' if it performs well on unseen scenarios such as new objects and novel motions, and has ``adaptivity'' if it maintains effectiveness despite shifting contexts, such as changing contacts and dynamics.
% generalize to unseen scenarios, and adapt to context shifts such as changing contact events or dynamics parameters.
}

% We use tracking prior to denote the 
% Starting from the first frame $\mathbf{s}_n$ and rollouting 
% is composed of a robot hand joint state sequence and an object pose trajectory. We use $\{ \hat{\mathbf{s}}_n \}_{n=0}^{N}$ to denote the a kinematic reference 
% a robot hand state sequence in the joint space and an object pose 
% In each tiemstep $n$, 
% we use $\mathbf{s}_n$ to denote the current state, including the robot joint states and the object pose, 
% current robot hand state (in the joint space) and the object pose 
% In each tiemstep $n$, the tracking controller/policy observes the current state $\mathbf{s}_n$, composed of the robot hand joint states and the object pose, and the goal state that we expect the hand and object to achieve in the next timestep, represented as $\hat{\mathbf{s}}_{n+1}$. It then output  
% robot hand and object state, denoted as $\mathbf{s}_n$, and the goal state that we expect the hand and object to achieve in the next timestep, represented as $\mathbf{}$ 

Given a collection of kinematic human-object manipulation trajectories, we wish to learn a generalizable neural tracking controller for a dexterous robotic hand. 
% and robust 
% versatile, 
% from these references.
% We expect the controller to equip the robot hand with a wide range of manipulation skills and also generalize well to robustly track novel and challenging manipulation tasks.
% a wide range of manipulation skills and also generalize well to robustly track novel and challenging manipulation tasks.
% tackle  novel and challenging manipulation tasks.
% track novel and challenging manipulations. 
% track 
% novel manipulation sequences with unseen object geometries and challenging, subtle in-hand manipulations. 
% to be able to 
% The problem is inherently difficult because: 1) the complex dynamics of dexterous manipulation challenge precise control, which is crucial for delicate motions; 2) there are high expectations for the controller's adaptability, generalizability, and robustness.
\textcolor{myblue}{
The problem is challenging due to the difficulty in precise dexterous manipulation, which is challenged by the underlying complex dynamics, and the high demands for the controller's generalizability and robustness.
}

% controller adaptability, generalizability, and robustness.
% diverse 

% given a collection %


% \todo{exisitng methods?}
% dyanmcis 
% The problem is challenged by the 
% Either previous model-free techniques or model-based approaches 
% various novel manipulation 
% should be able to track 
% We expect the controller can not only track every manipulation sequence in the c
% with the aim to equipping a dexterous robot hand with versatile dexterous manipulation skills. We expect the tracking controller 
% generalizable neural tracking control 
% human-object interaction dataset c


% To overcome these challenges, the key of our method lies in 
% Our method 

% We propose a method that combines reinforcement learning (RL) with imitation learning (IL) to train a generalizable tracking controller using robot tracking demonstrations. Since acquiring high-quality demonstrations with paired kinematics and action sequences is challenging, we introduce a per-trajectory tracking scheme that enhances demonstration quality and diversity via homotopy optimization. By alternating between training the controller with high-quality data and generating improved demonstrations in a bootstrapping process, we develop an effective tracking controller. Detailed explanations of these steps are provided in Sections~\ref{sec:method_il_rl},~\ref{sec:method_data}, and~\ref{sec:method_iteration}.


% To address these challenges, we propose to combine reinforcement learning (RL) and imitation learning (IL) to train a generalizable tracking controller, 
% % Since acquiring 
% % guiding the policy exploration via supervision from high-quality and 
% % empowering it with both the robustness to external disturbances benefiting from RL and
% % jointly via RL objectives and supervision from high-quality and diverse 
% % using robot tracking demonstrations. 
% \textcolor{myblue}{jointly easing the difficulty in solving the complex problem via supervision from high-quality and diverse robot tracking demonstrations and improving the policy's robustness via RL explorations. Since acquiring high-quality demonstrations with paired kinematics and action sequences is challenging, we introduce a tracking demonstration mining scheme. For each single kinematic reference trajectory, we basically leverage RL to train a trajectory-specific tracking policy, aiming to infer an action trajectory that can control the robotic hand to track the reference trajectory. Since the capability of single trajectory tracker would be limited by the RL's ability, we propose to leverage the tracking controller through a homotopy optimization scheme to enhance the demonstration quality and diversity. By iteratively mining high-quality and diverse demonstrations and training the controller with demonstrations that are gradually getting improved in a bootstrapping manner, we eventually develop an effective tracking controller. }

\textcolor{myblue}{
We address these challenges by combining reinforcement learning (RL) and imitation learning (IL) to train a generalizable tracking controller, jointly easing the difficulty in solving the complex problem via supervision from high-quality and diverse robot tracking demonstrations and improving the policy's robustness via RL explorations. We introduce a single trajectory tracking scheme to mine tracking demonstrations, composed of paired kinematic references and action sequences.
For each kinematic reference, we use RL to train a trajectory-specific policy that generates actions to track the reference. To overcome RL's limitations, we propose to leverage the tracking controller through a homotopy optimization scheme to enhance the demonstration quality and diversity. By iteratively mining better demonstrations and refining the controller in a bootstrapping manner, we develop an effective, generalizable tracking controller.
% To obtain paired kinematics and action sequences in the tracking demonstrations, we introduce a tracking demonstration mining scheme. 
}
% \textcolor{myblue}{}
% 0enhance demonstration quality and diversity via the tracking controller through a homotopy optimization scheme. 
% We basically leverage RL to train a 
% per-trajectory tracking demonstration mining sche
% introduce a per-trajectory tracking scheme that enhances demonstration quality and diversity via the tracking controller through a homotopy optimization scheme. 
% By alternating between training the controller with high-quality data and generating improved demonstrations in a bootstrapping process, we develop an effective tracking controller.
We will explain how we learn the neural tracking controller from demonstrations in Section~\ref{sec:method_il_rl}, how we mine high-quality demonstrations in Section~\ref{sec:method_data}, and how we iterate between learning controller and mining demonstrations in Section~\ref{sec:method_iteration}. 
% We will discuss how we learn the neural tracking controller from demonstrations in Section~\ref{sec:method_il_rl}, mining high-quality demonstrations in Section~\ref{sec:method_data}, and iterating between learning and mining in Section~\ref{sec:method_iteration}.
% we propose to learn from a large number of robot tracking demonstrations and combine reinforcement learning (RL) with imitation learning (IL) to effectively train the tracking controller.

% a method that combines 
% in detail 
% homotopy optimization.
% Since obtaining high-quality dexterous manipulation tracking demonstrations with paired kinematics references and action sequences is difficult, we devise a per-trajectory tracking scheme where we improve the quality and diversity of demonstration data via the tracking controller through a homotopy optimization scheme. 
% We then alternate between training the tracking controller using high-quality demonstrations and curating more diverse and higher-quality demonstrations in a bootstrapping manner, ultimately developing an effective and generalizable tracking controller. We will explain in detail how we learn the neural tracking controller from demonstrations in Section~\ref{sec:method_il_rl}, how we mine high-quality demonstrations in Section~\ref{sec:method_data}, and how we iterate between learning controller and mining demonstrations in Section~\ref{sec:method_iteration}. 
% of a generalizable, versatile, and robust neural tracking controller with a wide range of high-quality robot tracking demonstrations. 

% due to inherent challenges in dexterous manipulation tracking,

% the demonstration data
% that can benefit from a tracking controller while effectively tackling the tracking challenge via homotopy optimization.
% to ensure the data flywheel functions well.

% imitation ready data 
% high-quality imitation-ready data 
% imitation-ready data. 
% which combines the 
% Our method tackles these challenges by training a tracking controller jointly using reinforcement learning and imitation learning to empower the controller with high generalization ability and versatility by learning abundant and high-quality imitation-ready data and also maintaining robustness towards unexpected situations taking advantage of interactions with the environment via trial-and-errors. However, acquiring a large number of imitation-ready data itself is costly and difficult due to challenges in the optimization side for single trajectory tracking. Therefore, to make sure the data flywheel functions properly, we alternate between training the tracking controller and mining more diverse and higher-quality imitation-ready data leveraging the tracking controller. 
% , using the tracking controller. 
% we wish to leverage high-quality and abundant imitation-ready data to train a tracking controller jointly with RL to let the tracking controller benefit from these successful to improve the versatility and the generalization ability. Due to the inherent difficulty in dexterous manipulation, we propose an iterative optimization approach where we alternately train the tracking controller and improve the quality and diversity of the imitation-ready data so that the data flywheel can function properly. 
% Our method wishes to 

% Our method consists of two key designs to realize this vision: 1) A learning-from-demonstration strategy that trains the tracking controller jointly via imitation learning and reinforcement learning, leveraging a large number of high-quality robot tracking demonstrations; 2) A per-trajectory tracking scheme that can effectively solve difficult single trajectory tracking tasks using a tracking controller through a homotopy optimization scheme, continuously providing tracking demonstrations.
% and a homotopy path generator, by transferring tracking prior in a homotopy manner. 
% imitation-ready data; 
% 
% which gradually mines 
% imitation-augmented reinforcement learning scheme to make sure the tracking controller can benefit from imitation-ready data while still being robust to state disturbance; 2) a per-trajectory tracking scheme that can leverage the general tracking controller to produce better imitation-ready data; and 3) a homotopy method and a homotopy generator that can better leverage the tracking controller to solve difficult per-trajectory tracking problem for mining imitation-ready data from challenging manipulation skills. 
% to overcome these challenges: 
% that can better leverage the general tracking controller to solve 
% effective for solving difficult per-trajectory tracking problem 
% mimicking the chain of thought, to better leverage the general tracking controller for mining imitation-ready data from challenging manipulation skills. 


% We devise a basic RL-based method to train the tracking controller with delicately design action space, observations, and rewards for the general manipulation tracking problem. To let the policy benefit from the imitation-ready data, we add an additional supervised loss which, at each time, 


% First, we leverage an imitation-augmented reinforcement learning scheme to make sure the tracking controller can benefit from imitation-ready data while still being robust to state disturbance. We devise a basic RL-based method to train the tracking controller with delicately design action space, observations, and rewards for the general manipulation tracking problem. To let the policy benefit from the imitation-ready data, we add an additional supervised loss which, at each time, encourages the policy to imitate the action of the corresponding timestep in the trajectory.  
% Second, we develop a per-trajectory tracking scheme which can leverage the general tracking controller to produce better imitation-ready data. To obtain high-quality imitation data with action trajectories that can successfully drive the dexterous robotic hand to track the correspondence hand and object sequence, we leverage the popular modle-free RL-based method by training a per-trjaectory tracking policy using RL and use the tracking result as the optimized imitation-ready data. Desptie no expectation in the versatility and generalization ability in the per-trajectory tracker, tracking difficult tracking problem is still very challenging. to improve the data quality and include more diverse imitation-ready data into the training of tracking controller, we propose to leverage the general tracking prior encoded in the tracking controller to improve the per-trajectory tracker optimization. Third, we design a homotopy path generator, mimicking the chain of thought, to better leverage the general tracking controller for mining imitation-ready data from challenging manipulation skills. Soley relying on tracking prior encoded in the tracking controller to improve the per-trajectory tracker is still not enough to solve very difficult per-trajecotyr tracking problem. It is because that the hard-to-track trajectories are too out-of-distribution to the data prior encoded in the tracking controller, trained with mitation-ready data that has been successfully tracked. Thus such trajectory problems can hardly benefit from the tracking prior from the current tracking controller. To resolve these challenges, we propose to solve these problems in a homotopy method. Specifically, instraed of solving the difficult per-trajectyr tracking problem directly, we start from tracking a different trajectory to obtain its tracking result. After that we design a technique which allows us to let the difficult per-trajectory tracking problem benefit from the tracking result. In this way we are able to solve the difficult tracking task by starting from an easy task and gradually solve a series of tsks with the tracking results transferred to the next one. Finding such beneficial optimization paths, however, is very time-consuming. To further improve the efficiency, we propose to learn a tracking task structure prior model, which, in the inference time, can efficiently propose beneficial optimization paths for a tough single trajectory tracking problem. With the above three designs, we alternately 1) train the tracking controller and learn the tracking task structure prior model with high-quality imitation-ready data and 2) improve the data via the tracking controller and the tracking task structure prior model. The data flywheel can then function well, making it possible for us to train a strong tracking controller finally. 




% we propose to alternately train strong tracking controller and improve the imitation ready data 
% as well as the intriacat dynamics from dynamic and rich conta

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%5
% The key idea idea lies in leveraging high-qualtiy abutndaten imitation-ready data to train a tracking controller jointly with RL and tackling the difficulties in acquiring such data by leveraging the data flywheel to curate high-quality data and improve the tracking controller in a bootstrapping manner. At train time, given a collection of human hand object manipulation trajectories, we first retarget human hands to robotic dexterous hands in the data preparation stage, resulting in a reference manipulation  set containing retareted hand trajecotires and the original object pose sequences.  Our method then alternates between mining useful imitation-ready data from the reference motion set and training the tracking controller with the mined data. W
% e have three key designs to enable the data flywheel to function effectively. First, we leverage an imitation-augmented reinforcement learning scheme to make sure the tracking controller can benefit from imitation-ready data while still being robust to state disturbance. We devise a basic RL-based method to train the tracking controller with delicately design action space, observations, and rewards for the general manipulation tracking problem. To let the policy benefit from the imitation-ready data, we add an additional supervised loss which, at each time, 
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% The wish in the generalization ability and versatility a
% adn versatility usually challenge popular advanced RL approaches which often requires task-specificd reward designs
% The problem is inherently chall
% The learned neural tracking controller is expected to be able to 
% At the train time, the tracking controller is optimized to track every manipulation trajectory in the training 
% Given a datset containing 
% The neural tracking control for dexterous manipulations aims at leverage human hands manipulating objects trajectories, which have abundant data sources covering the internet videos and motion synthesis models, to learn a versatile and generalizable tracking controller. Given a dataset containing diverse human-object interaction trajecotires, the controller is optimized aiming to track every manipulation trajectory in the dataset. Specifically, with the kinematics only human object manipulation trajectory as input, the tracking controller aims at output an action trajectory that can mimic the kinematic trajectory -- when letting a dexterous robot hand execute the output action trajectory, the resulting hand states and object states should be close to their corresponding kinematic trajecotires in the input sequence. A versatile and genralizable tracking controller ought be able to generalize to novel and enve challenging interactions wit new and difficult objects. At the test time, given an manipulation sequence where both the hand motion adn the object movements and the object geometry are usnee during the training, the controller is epxcted to give proper action outputs that can drive the dexterous roobt hand to precisely control the object so that the resulting hand sequence adn the object sequences are close to the input reference. 
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% The problem of dexterous manipulation tracking control aims to develop a tracking controller for a dexterous robot hand based on human hand-object manipulation references. Given a manipulation sequence $(\mathbf{H}, \mathbf{O})$, we expect the controller to robustly control the robot hand to manipulate the object to mimic the reference motions closely. Specifically, the resulting hand poses $\hat{\mathbf{H}}$ and the object poses $\hat{\mathbf{O}}$ should accurately follow their respective reference motions. Furthermore, the controller should be capable of tracking novel reference motions involving unseen, complex manipulation sequences and new objects. This ambitious vision is challenged by several difficulties: 1) The inherent difficulty of the tracking control problem, coming from the intricate dynamics arising from frequently changing hand-object contacts and complex object movements; 2) The challenge of controlling a high-degree-of-freedom (DoF) robot hand to accurately follow dense reference points; 3) The need for a general tracking controller that can universally track diverse and highly challenging manipulation cases; 4) The requirement for high robustness and generalization, enabling the controller to adapt and accurately mimic previously unobserved reference motions.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% due to the intricate dynamics arising from 
% \(\hat{\mathbf{H}}\) and object poses \(\hat{\mathbf{O}}\) 

% so that the resulting hand and object trajectories could closely mimic the reference motions. 
% in a way that closely mimics the reference motions.
% which describes the interaction between the robot hand and an object (\textit{e.g.}, obtained via kinematic retargeting from a human-object manipulation sequence), the goal is for the controller to robustly manage the robot hand to manipulate the object in a way that closely mimics the reference motions. 

% hindered by several challenges: 
% 1. The inherent difficulty of the tracking control problem due to the intricate dynamics arising from frequently changing hand-object contacts and complex object movements.
% 2. The challenge of controlling a high-degree-of-freedom (DoF) robot hand to accurately follow dense reference points.
% 3. The need for a general tracking controller that can universally track diverse and highly challenging manipulation cases.
% 4. The requirement for high robustness and generalization, enabling the controller to adapt and accurately mimic previously unobserved reference motions.

% The manipulation sequence may contain novel manipulation sequences and new object geometries that the controller is never aware of. We still expect the controller to be able to deal with such novel tracking targets. 

% inherent difficulty of controlling a dexterous hand to densely track a manipulation trajectory with complex object motions, the intricate dynamics resulting from the rich and changing contacts between the hand and the object, the difficulty of controlling a high DoF dexterous hand, and the morphology difference, 2) the vision of learning a single policy that can universally track all trajectories, and 3) the wish of the high generalization ability to unseen and difficult manipulations for the policy. 

% Given a set of human hand-object manipulation demonstrations $\mathcal{S}=\{ ( \mathbf{H}, \mathbf{O} ) \}$, the dexterous manipulation tracking control problem aims at learning a single tracking controller that can control a dexterous robot hand to manipulate the object so that both resulting hand poses and the object poses can accurately follow their corresponding references in the kinematic demonstration. Given new manipulations involving novel interactions and unseen object geometries, we expect the controller to generalize well and robustly track those unobserved manipulations. 
% % the generalizable dexterous manipulation tracking aims at learn a single general tracking policy that 1) can successfully track each trajectory in the dataset and 2) can generalize well to novel manipulation trajectories unseen during training.
% Formally, given a multi-fingered dexterous robot hand (\emph{e.g.,} Allegro, LEAP, Shadow hand, etc.) we wish to learn a tracking controller $\mathbf{\pi}(\cdot\vert \cdot)$ from the given kinematic-only human-object interaction demonstrations $\mathcal{S}$. 
% For each manipulation trajectory $(\mathbf{H}, \mathbf{O})$, $\mathbf{\pi}(\cdot\vert \cdot)$ should be able to output a physically plausible control sequence $\mathbf{A}$ that is able to drive the dexterous robot hand to manipulate the object so that the resulting hand trajectory $\hat{\mathbf{H}}$ and the object trajectory $\hat{\mathbf{O}}$ are close to the reference motion $(\mathbf{H}, \mathbf{O})$. 
% % the object according to the kinematic demonstration $(\mathbf{H}, \mathbf{O})$, where both the resulting hand sequence and the object sequence should be close to their corresponding target trajectories. 
% Besides, given a novel manipulation trajectory unseen during the training, we wish $\mathbf{\pi}(\cdot\vert \cdot)$ can generalize well and output a reasonable manipulation trajectory that can physically track the demonstration accordingly. The vision of learning a generalizable tracker is challenged by 1) the inherent difficulty of controlling a dexterous hand to densely track a manipulation trajectory with complex object motions, the intricate dynamics resulting from the rich and changing contacts between the hand and the object, the difficulty of controlling a high DoF dexterous hand, and the morphology difference, 2) the vision of learning a single policy that can universally track all trajectories, and 3) the wish of the high generalization ability to unseen and difficult manipulations for the policy. 

% primarily in the locomotion domain, % 
% Considering the above huge challenges, directly training a policy aiming to track each manipulation trajectory would challenge both current advanced model-based and model-free techniques. Inspired by the data scaling law in other spaces, we propose to leverage a large number of imitation-ready data with both demonstrations and action sequences that can correctly drive the hand to precisely mimic the reference trajectory to train a general tracking controller. However, optimizing a large number of high-quality imitation-ready data itself is very challenging. Therefore, we propose to facilitate and enhance the per-trajectory tracking optimization via data prior from the tracking controller and the task structure. Specifically, instead of solving each tracking problem alone, we propose to solve difficult tasks via a homotopy method. It tries to solve a difficult problem by gradually solving each task in the optimization path and transferring results to the next. To efficiently find correct optimization paths, we further propose to encode such structural relations into a model, which is leveraged to propose optimization paths directly during the inference. We then present an iterative optimization strategy that gradually improves each side and finally lets us arrive at a general tracking controller. 
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


% could let us arrive at a general tracking controller finally. 
% Letting the model propoe correct optimization paths could spare us lots of time cost in the trail and error in finding optimization paths. We then present an iterative optimization strategy that could let us arrive at a general tracking controller finally. 
% through an optimization path. This path contains a series of per-trajectory tracking problems. A good path can let us solve the final problem better by solving each problem and transferring results to the next task one-by-one. 

% interactive strategy that could let us arrive at a general 
% Consideirng the dependencies between the tracking controller, tracking task structural prior, 

% To improve the efficiency of finding correct optimization paths, we further propose to search such beneficial optimization paths 
% The task structural prior 
% The homotopy method optimizes a tracking task of interest 
% tracking task structure prior. The tracking task structure encodes cross-task relations 
% The tracking task structural prior 
% Specifically, we leverage RL to train per-trajectory tracking policies. 
% We further propose to use tracking prior from the tracking controller 
% to solve difficult pre-trajectory tracking problem and impro
% is not feasible 
% We adopt a model-free approach to solve the problem considering the recent success achieved by model-free RL in developing strong and even universal controllers~\citep{Luo2024GraspingDO,xu2023unidexgrasp,wan2023unidexgrasp2,Luo2023PerpetualHC,Luo2023UniversalHM}. However, despite the strong capability of RL demonstrated in other tasks, directly training a policy aiming to track each manipulation trajectory is not feasible due to the above huge challenges. Compared to using a universal tracking controller, single trajectory tracking is an easier problem to solve. Observing recent trials and success in combining expert demonstrations or policies with RL, \todo{add refs}, we explore the possibility of learning a tracking controller with the joint effort of RL and guidance from high-quality action-labeled data. However, despite the eased difficulty in training per-trajectory trackers, advanced solutions would still fall short in resolving very difficult tracking problems or are extremely time-consuming~\citep{zhang2023learning,Zhuang2023RobotPL,liu2024quasisim}. This makes optimizing for abundant and high-quality action-labeled trajectories itself a very difficult problem. Observing the power of the foundation model in solving downstream tasks effectively and inspired by the recent success in leveraging curriculum learning to tackle difficult tasks~\citep{zhang2023learning,Zhuang2023RobotPL,liu2024quasisim}, we propose to improve the quality and diversity of action-labeled dataset via tracking prior from the tracking controller and correct tracking curriculum. Since finding correct curricula will cost a lot of time and requires trial and error, we present an automatic tracking curricula mining strategy and propose to learn a curriculum scheduler, which can be used to propose good curricula, from the mined valid curricula. The dependencies between high-quality per-trajectory trackers and the general tracking controller as well as the curriculum scheduler pose us a ``chicken-and-egg''  puzzle. We therefore propose an iterative approach that progressively enhances each side and finally lets us arrive at a universal tracking controller. 


% We adopt a model-free approach to tackle this problem, motivated by the recent success of model-free reinforcement learning (RL) in developing robust and even universal controllers~\citep{Luo2024GraspingDO,xu2023unidexgrasp,wan2023unidexgrasp2,Luo2023PerpetualHC,Luo2023UniversalHM}. However, despite the demonstrated strength of RL in other tasks, directly training a policy to track individual manipulation trajectories is not feasible due to the significant challenges outlined earlier. While using a universal tracking controller is highly complex, tracking a single trajectory presents a more manageable problem. 
% Recent advancements combining expert demonstrations or policies with RL have shown promise in addressing these challenges \todo{add refs}. Building on this, we explore the potential of developing a tracking controller by integrating RL with high-quality, action-labeled data. Although training per-trajectory trackers is more straightforward, even advanced solutions often fall short when faced with exceptionally difficult tracking problems, or they become prohibitively time-consuming~\citep{zhang2023learning,Zhuang2023RobotPL,liu2024quasisim}. As a result, optimizing a large, high-quality action-labeled dataset remains a complex task.
% Inspired by the power of foundation models to solve downstream tasks effectively, and drawing from recent successes in curriculum learning for tackling difficult challenges~\citep{zhang2023learning,Zhuang2023RobotPL,liu2024quasisim}, we propose enhancing the quality and diversity of the action-labeled dataset by leveraging a tracking prior from the controller and applying a refined tracking curriculum. Given the trial-and-error nature and time costs associated with identifying the correct curricula, we introduce an automatic tracking curriculum mining strategy. Additionally, we propose learning a curriculum scheduler that can suggest effective curricula based on the mined valid ones.
% The interdependencies between high-quality per-trajectory trackers, the general tracking controller, and the curriculum scheduler create a “chicken-and-egg” problem. To address this, we propose an iterative approach that progressively improves both components, ultimately leading to the development of a universal tracking controller.


% to effecitly improving the quality and diversity of action-labeld trajecotries taking advantage the joint power of per-trajectory tracking policy optimization, 
% due to the inherent difficulty in the manipulation tracking problem and the vision of tackling multiple and heterogeneous trajectory tracking problems at once.
% Considering the recent success achieved by model-free reinforcement learning in developing strong and even universal controllers~\citep{Luo2024GraspingDO,xu2023unidexgrasp,wan2023unidexgrasp2,Luo2023PerpetualHC,Luo2023UniversalHM}, and the little assumption in the known system dynamics, which may be very difficult to model to model for dexterous manipulations, we adopt a model-free approach and wish to leverage the reinforcement learning to learn a tracking policy for solving the problem. Directly training a policy aiming to track each manipulation trajectory in the dataset is not a feasible solution due to the inherent difficulty in the manipulation tracking problem and the vision of tackling multiple and heterogeneous trajectory tracking problems at once.
% where 
% \todo{some descriptions w.r.t. the distinctions and heter. within trajs?} 
% Based on the key observation in the benefit brought by the supervision provided by successful tracking trajectories\todo{rewrite this}, we pursue a strategy that can leverage the power of action-labeled manipulation trajectories to ease the difficulty in learning a universal tracking controller\todo{rewrite this -- unleash the power of data in xxx}. This further leads us to the problem of efficiently acquiring abundant high-quality action-label trajectories in an affordable time budget. Recent advances in model-free or model-based strategies to optimize an action trajectory that can successfully track a single trajectory either fall short in quality or is highly inefficient~\citep{christen2022d,zhang2023artigrasp,liu2024quasisim}. Successful trials in curriculum learning to solve difficult problems~\citep{zhang2023learning,Zhuang2023RobotPL,liu2024quasisim} let us explore what's good structural relations we can explore in this problem that is beneficial to improve single trajectory tracking. However, despite the effectiveness, curriculum learning itself is time-expensive with additional requirements in the trial-and-error to find correct curricula. With the observation of the general knowledge encoded in the foundation model to zero-shot solving specific problems, we wonder about the possibility of leveraging the tracking knowledge learned by the tracking controller to help single trajectory tracking. This further led us to wonder if can we learn a curriculum scheduler from cross-trajectory relations encoding cross-task optimization synergies so that the scheduler can efficiently plan a task curriculum for us to solve each difficult tracking problems? The dependencies between high-quality per-trajectory trackers and the general tracking controller as well as the curriculum scheduler pose us a ``chicken-and-egg''  puzzle. We therefore propose an iterative approach that progressively enhances each side and finally lets us arrive at a universal tracking controller. 

% However we face a ``chicken-and-egg'' puzzle since acquiring  high-quality abundant action-labeled data efficiently is itself not feasible due to restrictions of current techniques.  Therefore, 
% However, despite the recent achievements, 
% Optimizing for an effective policy that can track a single manipulation trajectory is already extremely challenging considering the difficulties of the non-smooth dynamics and frequent rich contact variations. Training a generalist policy that can solve such a tough manipulation tracking problem is even more difficult. Training a single controller capable of tracking diverse motions often challenges advanced RL techniques. However, optimizing out a reasonable control trajectory for each not-too-difficult single manipulation sequence is feasible under the support of current techniques. Therefore, a natural question is can we design a strategy that can resolve difficulties in single trajectory tracking and manage to get a specialist policy and then leverage such optimized specialist policies to train the generalist policy? However, acquiring such as generalist for each single trajectory is too expensive and almost unaffordable due to the heavy time cost. Moreover, optimizing each trajectory independently may still struggle with solving extremely challenging problems. This naturally leads us to find cross-tracking task synergies to enhance each single task's performance. With the above considerations, we propose a specialist-generalist-specialist training scheme and a task curriculum, powered by a heuristic-guided task relational space exploration and modeling, for iteratively improving the capability of the generalist policy and enhancing the single manipulation trajectory tracking performance. 
% \todo{rewrite this paragraph.}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Our method comprises three key designs: 1) an imitation augmented RL algorithm that can effectively optimize a tracking controller taking the joint effort from RL and guidance from action-labeled trajectories (Sec.~\ref{sec:method_il_rl}), 2) a homotopy optimization scheme for solving difficult per-trajectory tracking problem, an optimization path searching strategy, and a tracking task structure prior model that encode optimization paths (Sec.~\ref{sec:method_task_space_modeling}), and 3) an iterative optimization scheme which alternately improve the imitation-ready data and train the tracking controller for optimizing a powerful tracking controller finally (Sec.~\ref{sec:method_sgs}).
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


% a tracking task structure modeling strategy, and tracking task structure prior model that can efficiently propose correct optimization paths in the inference time to solve difficult problems via a homotopy method (Sec.~\ref{sec:method_task_space_modeling}), and 3) an iterative optimization scheme which alternately improve the imitation-ready data and train the tracking controller for optimizing a powerful tracking controller finally (Sec.~\ref{sec:method_sgs}).
% n automatic tracking curriculum searching strategy and a curriculum scheduler that models valid tracking curricula, which can be used to propose correct curricula for tracking very difficult trajectories  (Sec.~\ref{sec:method_task_space_modeling}), 3) an interactive optimization approach that progressive 1) improve the universal tracking controller, and 2) enalarge and improve the action-labeled dataset via tracking prior from the tracking controller as well as the curriculum scheduling (Sec.~\ref{sec:method_sgs}).
% such relations via a conditional diffusion model
% could benef
% general tracking controller's training from abundant high-quality action-labeled data 

% ``specialist-generalist-specialist'' training strategy which iteratively optimizing out better specialists, leveraging the cross-task synergies from the task relational model and the generalist policy, and improving the generalist performance by training with more and better specialists 
% task relational space modeling scheme with a heuristic-guided searching algorithm for constructing the task relational space and a conditional diffusion model for describing such relations 
% that could leverage action-labeled data to 
% enhance the performance of the generalist policy 
% % cross-task synergy modeling scheme 

% effectively let the generalist policy benefit from more and better action-labeled single trajectory manipulation's optimization result and 
% a heuristic-guided searching method for building the structural task relational space, which summarizes the cross-task optimization synergies in the manipulation tracking problem, 
% 2) an effective optimization scheme that 

% as well as an offline action trajectory and then leveraging such 
% that can manage to optimize for a reasonable control sequence for each manipulation trajectory 
% Directly leveraging davnaced RL techniques to train a single tracker capable of tracking diverse motions 

% \textcolor{red}{(TODO: explain why we employ a generalist-specialist-generalist approach, why we start from the specialist training; Explain the overall high level motivations at first and then continue with the key designs)}
% Our method comprises two key designs to tackle these challenges: 1) an imitation augmented RL algorithm that could leverage action-labeled data to learn a general tracking policy, 2) an effective optimization scheme for solving difficult tracking problems with an automatically explored hierarchical trajectory space, a distilled task translation model, and a task curriculum, and 3) an interactive model training and action-labeled data acquisition and improvement scheme leveraging the universal policy and the distilled task translation model. 
% \textcolor{red}{(TODO: Add section labels)}
% \textcolor{red}{(TODO: may be swap 1 and 2)}

% \todo{action labeled dataset for training the tracking controller}

% \todo{curriculum scheduler}

% \todo{using the above two tools to improve per-trajectory trackers}


% Consider %

% specialist formulation
% specialist formulation --- 
% d

% \todo{add some explanations on the kinematic retargeting}

% \todo{though the retargeted trajectories have noise, actually we would not be restricted by such noise}

% \todo{add the residual policy --- actually it is somewhat interesting and would not restrict us into adjacent part of the original kinematic trajectory}



% \subsection{Generalist Training via Imitation Augmented RL} \label{sec:method_il_rl}
% \subsection{Tracking Controller Training via Imitation Augmented RL} \label{sec:method_il_rl} 
% \subsection{Tracking Controller Training via Imitation Augmented RL} \label{sec:method_il_rl} 

\vspace{-5pt}
\subsection{Learning a Neural Tracking Controller from Demonstrations} \label{sec:method_il_rl} 
\vspace{-5pt}
% \eric{Imitation-augmented RL refers to a pretty broad range of methods and there are already a lot of similar algorithms in the literature. We don't necessarily need to come up with a name for this algorithm since it is probably not a very big deal. We could simply use ``learning a neural tracking controller from demonstrations'' as the subsection title. We could first motivate the usage of RL and IL together since RL is better at coping with dynamic state disturbance while IL can benefit from a large number of demonstrations. Then describe the RL part. Later, incorporate the IL part. Finally, conceptually analyze the framework a bit. see below.}

% \eric{We should conceptually compare with the old DAPG paper (Learning Complex Dexterous Manipulation with
% Deep Reinforcement Learning and Demonstrations) and also add ablations in the experiment section to motivate the necessity of coming up with the current approach. 
% For me, the difference sounds to be that we are doing tracking so even using RL the explored states will not deviate from the given trajectory too much, making the current design possible. In the current design, from RL's perspective, the imitation loss biases RL policy's action toward the imitation source to improve sampling efficiency. From IL's perspective, RL is injecting noises into the states to make learning more robust similar to DART.}

% We could first motivate the usage of RL and IL together since RL is better at coping with dynamic state disturbance while IL can benefit from a large number of demonstrations. Then describe the RL part. Later, incorporate the IL part. Finally, conceptually analyze the framework a bit. see below.

% \eric{Imitation-augmented RL refers to a pretty broad range of methods and there are already a lot of similar algorithms in the literature. We don't necessarily need to come up with a name for this algorithm since it is probably not a very big deal. We could simply use ``learning a neural tracking controller from demonstrations'' as the subsection title. We could first motivate the usage of RL and IL together since RL is better at coping with dynamic state disturbance while IL can benefit from a large number of demonstrations. Then describe the RL part. Later, incorporate the IL part. Finally, conceptually analyze the framework a bit. see below.}

% To train a tracking controller that can benefit from a large number of imitation-ready data and also be robust to dynamic state disturbances, we leverage reinforcement learning and imitation learning to train a tracking controller. Specifically, we propose an RL-based training strategy with carefully designed action apace, observations and rewards for the manipulation tracking problem. We further present an additional imitation loss so that we can train the tracking controller with additional supervision from the imitation-ready data with high-quality action sequences. 

% Given a collection of human-object manipulation trajectories and a set of high-quality robot tracking demonstrations, we wish to learn an effective and generalizable neural tracking controller. 
% In the beginning, we retarget human hand-object manipulation to a robotic hand as a data pre-processing step. 
% We combine RL and IL to develop a generalizable and robust neural tracking controller. 
% Taking advantage of imitating diverse and high-quality robot tracking demonstrations, we can effectively let the tracking controller master diverse manipulation skills and also equip it with high generalization ability. Jointly leveraging the power of RL, the controller would not get overfit to imitating successful tracking results covering a narrow distribution, maintaining strong robustness to dynamic state disturbances. Specifically, we design an RL-based learning scheme for training the tracking controller, including the carefully-designed action space, observations, and the reward tailored for the manipulation tracking task. We also introduce an IL-based strategy that lets the tracking controller benefit from imitating high-quality demonstration data. 
% By integrating these two approaches, we effectively address the complex problem of generalizable tracking control.


Given a collection of human-object manipulation trajectories and a set of high-quality robot tracking demonstrations, our goal is to learn an effective and generalizable neural tracking controller. 
In the beginning, we retarget human hand-object manipulation to a robotic hand as a data preprocessing step. We combine RL and IL to develop a generalizable and robust neural tracking controller. 
Taking advantage of imitating diverse and high-quality robot tracking demonstrations, we can effectively let the tracking controller master diverse manipulation skills and also equip it with high generalization ability. 
% Jointly leveraging the power of RL, the controller would not get overfit to imitating successful tracking results covering a narrow distribution, therefore maintaining strong robustness to dynamic state disturbances. 
Jointly leveraging the power of RL, the controller avoids overfitting to successful tracking results limited to a narrow distribution, thereby maintaining robust performance in the face of dynamic state disturbances.
Specifically, we design an RL-based learning scheme for training the tracking controller, including the carefully-designed action space, observations, and the reward tailored for the manipulation tracking task. We also introduce an IL-based strategy that lets the tracking controller benefit from imitating high-quality demonstration data. 
By integrating these two approaches, we effectively address the complex problem of generalizable tracking control.


% Combining the best of the two worlds, we finally succeed in the complex generalizable tracking control problem. 
% from the trajectories. 

% Then, in our formulation, the neural tracking controller is an agent that interacts with the environment according to a tracking policy $\pi$. At each timestep $n$, the policy observes the state $\hat{\mathbf{s}}_n$, consisting of the current state of the robotic dexterous hand and the object state, and the next goal $\mathbf{s}_{n+1}$, commanded as the next state of the robotic hand in the retargeted sequence and the next goal object state. Then the agent then samples an action $\mathbf{a}_n$ from the policy $\mathbf{a}_n\sim \pi(\cdot \vert \mathbf{s}_n, \mathbf{s}_{n+1})$. After applying action to the robotic dexterous hand, the hand transits and physically interacts with the object. The resulting hand state and the object state should be close to their corresponding nextstep state in the reference trajectory. 
% tracking policy $\pi_{\theta}$. At 
% We then combine reinforcement learning (RL) and imitation learning (IL) to develop a generalizable, versatile~\eric{we are using versatile a lot from the very beginning but it is not quite clear what it indeed refers to.}, and robust neural tracking controller. 

% reinforcement learning (RL) and imitation learning (IL) 
% leveraging knowledge learned 
% from the abundant data. 
% imitation-ready data, 
% Jointly leveraging the power of the reinforcement learning, we 
% equip the tracking controller with 
% we let the tracking controller 
% so that it can benefit from abundant imitation-ready data while maintaining robustness to dynamic state disturbances. 

% a well-defined action space, observations, and the reward 
% We design an RL-based strategy for solving the tracking controller problem, including a well-defined action space, observation framework, and reward structure tailored for the manipulation tracking task. 
% We begin by motivating the integration of RL, which excels in handling dynamic challenges, with IL, which benefits from extensive demonstration data. 
% Our RL-based strategy includes a well-defined action space, observation framework, and reward structure tailored for the manipulation tracking task. 

% let the 
% To guide the learning of the tracking controller, we introduce an imitation learning scheme to jointly train the controller with RL. 

% imitation loss 
% exploration of the RL's agent, we introduce an
% and to let us succeed in the generalizable tracking control problem, we 

% We will illustrate details as follows. 
% benefit the tracking controller 
% to enhance training by adding additional supervision from high-quality demonstration data. 
% with high-quality action sequences from the demonstration data, allowing for additional supervision. Finally, we will conceptually analyze this framework to highlight its effectiveness in training a neural tracking controller from demonstrations.
% which adds additional supervision 
% as an additional super


% To develop a generalizable and versatile neural tracking controller that can benefit from abundant imitation-ready data while maintaining robustness to dynamic state disturbances, we combine reinforcement learning (RL) and imitation learning (IL). 
% benefiting from abundant imitation-ready data while 
% that is versatile 
% utilizes abundant imitation-ready data 

% consider an approach where

% We design an imitation augmented RL algorithm to train the tracking controller. It is composed of a basic tracking policy and an action supervised loss. Given an action-labeled dataset composed of trajectories with high-quality action sequences, we can overcome the difficulty of training the general controller solely via RL leveraging the additional supervision provided by the action-labeled trajectories. 
% can benefit the tracking controller's learning via this strategy. 
% Optimizing the tracking controller aiming to solve multiple tracking problems is difficult considering the inherent poor multi-task ability of RL. \todo{add refs}
% Directly using advanced RL to train a tracking controller is an extremely challenging problem considering the inherent difficulty of dexterous manipulations and the high demand for generalization ability, versatility, and robustness. In comparison, single trajectory tracking, expecting no generalization ability or versatility, is an easier problem. Inspired by recent exploration and achievements on introducing supervision, either from the offline dataset, online trajectory optimization, or experts, into RL~\citep{rajeswaran2017learning,Liu2023BlendingIA}, we design an imitation augmented RL algorithm, composed of a basic tracking policy and an action supervised loss. Given an action-labeled dataset composed of trajectories with  high-quality action sequences, we can benefit the tracking controller's learning via this strategy. 
% with dedicated   to train the tacking controller, composed of a basic tracking 
% Single trajectory tracking is an 
% Inspired by 
% % Training a general tracking controller with the expectation of tracking all trajectories directly always turns out to be a problem that is too difficult to solve by current reinforcement learning techniques. 
% Therefore, instead of optimizing the general tracker directly,  we propose to augment the training with supervision from the action-labeled dataset to ease the training~\citep{Jenelten2023DTCDT,rajeswaran2017learning,Liu2023BlendingIA}. We delicately design the observations and rewards so that by leveraging advanced RL as the base approach we can solve many single trajectory tracking problems. With high-quality action-labeled trajectories rollout by their optimized per-trajectory trackers, we then propose an imitation augmented RL training scheme to ease the training and improve the general tracking controller. 
% How to leverage experts or offline datasets to improve RL training is still an active research topic currently. 
% Despite previous successful trials in other scenarios~\citep{Jenelten2023DTCDT,rajeswaran2017learning,Liu2023BlendingIA}, how to unleash the power of this trend of methods for dexterous manipulation tracking generalist poses many unique challenges. 
% Instead of directly training a generalist policy aiming to track all trajectories directly, which always turns out to be a problem that is too difficult to solve by current RL techniques, we propose to augment the training with the supervision from action labeled dataset for ease and speed up the policy learning. \todo{some references w.r.t. this approach} 
% \todo{rewrite the next sentence}
% To fulfill this vision, we need to curate action-labeled trajectories. Therefore, we again propose to leverage RL to train a trajectory-specific tracking policy to infer actions for the trajectory. 

\noindent\textbf{Neural tracking controller.} 
% In our formulation, the neural tracking controller is an agent that interacts with the environment according to a tracking policy $\pi$. At each timestep $n$, the policy observes the observation $\mathbf{o}_n$. $\mathbf{o}_n$ is computed based upon the current state $\mathbf{s}_n$, the next goal $\hat{\mathbf{s}}_{n+1}$ which is commanded as the next goal state of the robotic hand and the next goal object state, and the object geometry.
% Then, the agent samples an action $\mathbf{a}_n$ from the policy $\mathbf{a}_n\sim \pi(\cdot \vert \mathbf{o}_n, \hat{\mathbf{s}}_{n+1})$. After applying action to the robotic dexterous hand, the hand moves and physically interacts with the object. After that, both the hand and the object transit into the next state according to the environment dynamics, \emph{i.e.,} ${\mathbf{s}}_{n+1} \sim p(\cdot \vert {\mathbf{s}}_n, \mathbf{a}_n)$. 
% The resulting hand state and object state should closely match their respective next goal states.
In our formulation, the neural tracking controller acts as an agent that interacts with the environment according to a tracking policy \(\pi\). At each timestep \(n\), the policy observes the observation \(\mathbf{o}_n\) and the next goal \(\hat{\mathbf{s}}_{n+1}\) (designated as the target state for the robotic hand and the object). The policy then computes the distribution of the action. Then the agent sample an action $\mathbf{a}_n$ from the policy, \emph{i.e.,} \(\mathbf{a}_n \sim \pi(\cdot \vert \mathbf{o}_n, \hat{\mathbf{s}}_{n+1})\). The observation $\mathbf{o}_n$ primarily contains the current state $\mathbf{s}_n$ and the object geometry. 
% which is primarily computed based on the current state \(\mathbf{s}_n\) and the next goal \(\hat{\mathbf{s}}_{n+1}\) (designated as the target state for the robotic hand and the object). The policy then computes the distribution of the action. Then the agent sample an action $\mathbf{a}_n$ from the policy, \emph{i.e.,} \(\mathbf{a}_n \sim \pi(\cdot \vert \mathbf{o}_n, \hat{\mathbf{s}}_{n+1})\). The observation $\mathbf{o}_n$ primarily contains the current state $\mathbf{s}_n$ and the object geometry. 
% Then the agent 
% and the object geometry. 
% computed based on the current state \(\mathbf{s}_n\), the next goal \(\hat{\mathbf{s}}_{n+1}\) (designated as the target state for the robotic hand and the object), and the object geometry.
% The agent then samples an action \(\mathbf{a}_n\) from the policy, \emph{i.e.,} \(\mathbf{a}_n \sim \pi(\cdot \vert \mathbf{o}_n, \hat{\mathbf{s}}_{n+1})\). 
Upon executing the action with the robotic dexterous hand, the hand physically interacts with the object, leading both the hand and the object to transition into the next state according to the environment dynamics, represented as \(\mathbf{s}_{n+1} \sim p(\cdot \vert \mathbf{s}_n, \mathbf{a}_n)\). 
% An effective tracking controller should let the resulting hand and object states should closely align with their respective next goal states.
An effective tracking controller should ensure that the resulting hand and object states closely align with their respective next goal states.
% The resulting hand and object states should closely align with their respective next goal states.

% which is computed based upon the current state $\mathbf{s}_n$, the next goal $\hat{\mathbf{s}}_{n+1}$, commanded as the next state of the robotic hand in the retargeted sequence and the next goal object state, and the object geometry.
% ~\eric{the writing is a bit confusing here. Does $\mathbf{o}_n$ include $\hat{\mathbf{s}}_{n+1}$ and the object geometry or it only includes $\mathbf{s}_n$? Why the object geometry is not a part of the policy input in the next formula?} % state-goal pair $(\mathbf{s}_n, \hat{\mathbf{s}}_{n+1})$ 
% primarily encodes the current hand and object state, object geometry, 
% the policy observes the state ${\mathbf{s}}_n$, consisting of the current state of the robotic dexterous hand and the object state, 
% and the next goal $\hat{\mathbf{s}}_{n+1}$, 
% commanded as the next state of the robotic hand in the retargeted sequence and the next goal object state. 

% The resulting hand state and the object state should be close to their corresponding next goal state. 
% nextstep state in the reference trajectory. 




% \todo{adapt to the tracking controller}
% \noindent\textbf{Per-trajectory tracker training.} 
% \noindent\textbf{Tracking controller optimization via RL.} 
% \noindent\textbf{Basic RL for training the tracking controller.} 
% \noindent\textbf{Training the tracking controller via RL.} 
\noindent\textbf{Reinforcement learning.}  
% Our single trajectory specialist training scheme leverages PPO to optimize for a tracking policy $\mathbf{\pi}(\cdot\vert \cdot)$. 
% Given a kinematic manipulation trajectory $(\mathbf{H}, \mathbf{O})$, we aim at optimizing a tracking policy $\mathbf{\pi}(\cdot\vert\cdot )$ that can control a dexterous robot hand to track the reference kinematic trajectory. 
% We use PD control to drive the hand. % links and joints %
% We carefully design the action space, and observation space as well as the reward and develop an RL-based optimization scheme for learning a tracking policy. Considering the poor sample efficiency of RL, instead of letting the tracking policy learn absolute positional target $\mathbf{a}_n$ at each timestep $n$, we assume there exists a basic trajectory, which is usually set to the kinematic reference trajectory, and let RL learn residual relative target $\Delta \mathbf{a}_n$ at each timestep $n$. For each timestep $n$, denote the hand state in the basic trajectory as $\mathbf{h}_n^b$, we calculate the final positional target as follows: 
% \begin{align}
%     \mathbf{a}_n = \mathbf{h}_n^b + \sum_{k=1}^n \Delta \mathbf{a}_k. 
% \end{align}
% In addition to the possibility in speeding up the training, the basic trajectory $\mathbf{H}^b=\{ \mathbf{h}_n^b \}$ gives us an additional dimension for optimizing the single trajectory tracking policy. For instance, we can initialize the basic trajectory as the tracking result of a different tracking task and re-optimize the residual policy to try to improve the single trajectory tracking. 
% \eric{The explanation should be more logical. We should start from introducing the state space, and then explain the common action space, e.g., joint positional targets, followed by mentioning our residual design. Finally rewards. I like how the \href{https://research.nvidia.com/labs/par/maskedmimic/assets/SIGGRAPHAsia2024_MaskedMimic.pdf}{MaskedMimic} paper introduces these.}
% We design 
% leverage 
% reinforcement learning scheme to 
% In the RL-based training scheme, the agent receives a reward $r_{n} = r({\mathbf{s}}_n, \mathbf{a}_n, \hat{\mathbf{s}}_{n+1}, \mathbf{s}_{n+1})$ after each transition. The training objective is maximizing the discounted cumulative reward: 
% \begin{align}
%     J = \mathbb{E}_{p(\tau\vert \pi)} \left[  \sum_{n=0}^{N} \gamma^{n}r_n  \right],
% \end{align}
% where $p(\tau\vert \pi) = p({\mathbf{s}}_0) \Pi_{n=0}^{N-1}p(\mathbf{s}_{n+1}\vert \mathbf{o}_n, \mathbf{a}_n)\pi(\mathbf{a}_n\vert \mathbf{s}_n,\hat{\mathbf{s}}_{n+1})$ is the likelihood of a transition trajectory of the agent $\tau = (\mathbf{s}_0,\mathbf{a}_0,r_0,...,\mathbf{s}_{N-1},\mathbf{a}_{N-1},r_{N-1}, \mathbf{s}_N)$. The discount factor $\gamma \in [0,1)$ determines the effective horizon length of the policy. 
% In the tracking control problem, the next goal state $\hat{\mathbf{s}}_{n+1}$ usually consists of the next step hand state and the object state in the kinematic reference sequence. 
% We control the robotic hand via a proportional derivative (PD) controller, following previous literature~\citep{Luo2024GraspingDO,Luo2023PerpetualHC,christen2022d,zhang2023artigrasp}. The action $\mathbf{a}_n$ then contains the target position commands of all hand joints. To improve the sample efficiency of RL, instead of letting the tracking policy learn the absolute positional target, we introduce a residual action space. Specifically, we introduce a baseline hand trajectory and train the policy to learn the residual relative target  $\Delta\mathbf{a}_n$ in each timestep. 
% The baseline trajectory is always available for the tracking problem -- one can trivially set it to the kinematic reference trajectory. 
% In each timestep $n$, we compute the position target via $\mathbf{a}_n = \mathbf{s}_n^{b} + \sum_{k=0}^{n}\Delta \mathbf{a}_k$, where $\mathbf{s}_n^{b}$ is the $n$-th step hand state in the baseline trajectory. 
% The observation at each timestep $n$ encodes the current hand and object state, the next goal state, baseline trajectory, actions, velocities, and the object geometry: 
% \begin{align}
%     \mathbf{o}_n = \{ \mathbf{s}_n, \dot{\mathbf{s}}_n, \hat{\mathbf{s}}_{n+1}, \mathbf{s}^b_n, \mathbf{a}_n, \text{feat}_{\text{obj}}, \text{aux}_n \}, \label{eq_obs}
% \end{align}
% where $\text{feat}_{\text{obj}}$ is the object feature produced by a pre-trained object point cloud encoder. We additionally introduce an auxiliary feature $\text{aux}_n$, computed based upon available states, to expose the agent with more informative information. We will explain in detail in Appendix~\ref{sec:supp_method}. 
% Our reward for manipulation tracking encourages the transited hand state and the object state to be close to their corresponding reference states and the hand-object affinity: 
% \begin{align}
%     r = w_{o, p} r_{o, p} + w_{o, q} r_{o, q} + w_{\text{wrist}} r_{\text{wrist}} + w_{\text{finger}} r_{\text{finger}} + w_{\text{affinity}} r_{\text{affinity}}, \label{eq_rew}
% \end{align}
% where $r_{o,p},r_{o, q},r_{\text{wrist}},r_{\text{finger}},r_{\text{affinity}}$ are rewards for object position, object orientation, hand wrist, hand fingers, and hand-object affinity, $w_{o, p},  w_{o, q}, w_{\text{wrist}}, w_{\text{finger}},  w_{\text{affinity}}$ are their weights. Details of the reward computation are deferred to the Appendix~\ref{sec:supp_method}. 
In the RL-based training scheme, the agent receives a reward \(r_{n} = r({\mathbf{s}}_n, \mathbf{a}_n, \hat{\mathbf{s}}_{n+1}, \mathbf{s}_{n+1})\) after each transition. The training objective is to maximize the discounted cumulative reward:
\begin{align}
    J = \mathbb{E}_{p(\tau\vert \pi)} \left[  \sum_{n=0}^{N-1} \gamma^{n}r_n  \right],
\end{align}
where \(p(\tau\vert \pi) = p({\mathbf{s}}_0) \prod_{n=0}^{N-1} p(\mathbf{s}_{n+1}\vert \mathbf{o}_n, \mathbf{a}_n)\pi(\mathbf{a}_n\vert \mathbf{s}_n,\hat{\mathbf{s}}_{n+1})\) is the likelihood of a transition trajectory of the agent \(\tau = (\mathbf{s}_0,\mathbf{a}_0,r_0,...,\mathbf{s}_{N-1},\mathbf{a}_{N-1},r_{N-1}, \mathbf{s}_N)\). The discount factor \(\gamma \in [0,1)\) determines the effective horizon length of the policy.
In the tracking control problem, the next goal state \(\hat{\mathbf{s}}_{n+1}\) typically comprises the subsequent hand state and the object state from the kinematic reference sequence. We control the robotic hand using a proportional derivative (PD) controller, following previous literature~\citep{Luo2024GraspingDO,Luo2023PerpetualHC,christen2022d,zhang2023artigrasp}. The action \(\mathbf{a}_n\) contains the target position commands for all hand joints. 
To enhance the sample efficiency of RL, rather than allowing the tracking policy to learn the absolute positional targets, we introduce a residual action space. Specifically, we introduce a baseline hand trajectory and train the policy to learn the residual relative target \(\Delta\mathbf{a}_n\) at each timestep. The baseline trajectory is consistently available for the tracking problem and can be trivially set to the kinematic reference trajectory. 
In each timestep $n$, we compute the position target via $\mathbf{a}_n = \mathbf{s}_n^{b} + \sum_{k=0}^{n}\Delta \mathbf{a}_k$, where $\mathbf{s}_n^{b}$ is the $n$-th step hand state in the baseline trajectory. 
% The observation at each timestep \(n\) encodes the current hand and object states, the next goal state, the baseline trajectory, actions, velocities, and the object geometry:
The observation at each timestep $n$ encodes the current hand and object state, the baseline trajectory, actions, velocities, and the object geometry:
% the next goal state, 
% baseline trajectory, actions, velocities, and the object geometry: 
\begin{align}
    % \mathbf{o}_n = \{ \mathbf{s}_n, \dot{\mathbf{s}}_n, \hat{\mathbf{s}}_{n+1}, \mathbf{s}^b_n, \mathbf{a}_n, \text{feat}_{\text{obj}}, \text{aux}_n \}, \label{eq_obs}
    \mathbf{o}_n = \{ \mathbf{s}_n, \dot{\mathbf{s}}_n, \mathbf{s}^b_n, \mathbf{a}_n, \text{feat}_{\text{obj}}, \text{aux}_n \}, \label{eq_obs}
\end{align}
where \(\text{feat}_{\text{obj}}\) is the object feature generated by a pre-trained object point cloud encoder. We also introduce an auxiliary feature \(\text{aux}_n\), computed based on available states, to provide the agent with more informative context. Further details will be explained in Appendix~\ref{sec:supp_method}.
Our reward for manipulation tracking encourages the transited hand state and the object state to closely match their respective reference states, as well as promoting hand-object affinity:
\begin{align}
    r = w_{o, p} r_{o, p} + w_{o, q} r_{o, q} + w_{\text{wrist}} r_{\text{wrist}} + w_{\text{finger}} r_{\text{finger}} + w_{\text{affinity}} r_{\text{affinity}}, \label{eq_rew}
\end{align}
where \(r_{o,p}, r_{o, q}, r_{\text{wrist}}, r_{\text{finger}}, r_{\text{affinity}}\) represent rewards for object position, object orientation, hand wrist, hand fingers, and hand-object affinity, respectively, while \(w_{o, p}, w_{o, q}, w_{\text{wrist}}, w_{\text{finger}}, w_{\text{affinity}}\) are their corresponding weights. Details regarding the reward computation are deferred to Appendix~\ref{sec:supp_method}.



% ~\eric{should be ``trivially''. Please use Grammarly to carefully check the whole paper to avoid grammar errors.} 
% $\mathbf{b}_n$ 

% , $\text{aux}_n$ is the auxiliary features, computed based upon available states~\eric{at least we should explain the purpose of including some auxiliary features?}, which we will explain in detail in Appendix~\ref{sec:supp_method}. 

% Our reward for manipulating tracking comprises three components: the hand reward $r_{h}$, the object reward $r_o$, and the hand-object affinity reward $r_d$. The object reward encourage the transited object position and the orientation close to their kinematic references: $r_o = w_p r_{o,p} + w_q r_{o,q}$. 
% The object reward includes both position and orientation rewards, defined as: 
% \begin{align}
%     r_o = w_p(0.9 - \Vert \mathbf{s}_n^p - \hat{\mathbf{o}}_n^p \Vert_2 ) + w_q(\text{np.pi} - \text{Diff\_Angle}(\mathbf{o}_n^q - \hat{\mathbf{o}}_n^q )), \label{eq:rew_obj}
% \end{align}
% where $w_p$ and $w_q$ are the weights for position and orientation rewards, respectively, and $\text{Diff\_Angle}(\cdot, \cdot)$  computes the rotation angle difference between two quaternions. The hand reward is calculated as the negative weighted sum of the current hand joint positions and the reference positions:
% \begin{align}
%     r_h = -(w_{\text{trans}} \Vert \mathbf{h}_n[:3] - \hat{\mathbf{h}}_n[:3] \Vert_1 + w_{\text{ornt}} \Vert \mathbf{h}_n[3:6] - \hat{\mathbf{h}}_n[3:6] \Vert_1 + w_{\text{finger}} \Vert \mathbf{h}_n[6:] - \hat{\mathbf{h}}_n[6:] \Vert_1  ), \label{eq:rew_hand}
% \end{align}
% % we include in the observation, 
% % The tracking controller for manipulation should be aware of the geometry of the object. So we include the object feature, produced by a pre-trainedd object point cloud encoder, into the state. The final state contains 

%%%%%%%%%%%%%%%%%%%%%% baseline trajs %%%%%%%%%%%%%%%%%%%%%%
% This approach benefits us from two ways. First, the RL exploration needs to explore around the baseline trajectory only, thereby accelerating the convergence. Second, it offers us an additional dimension for optimizing the tracking policy -- 
% needs to explore around a baseline hand trajectory. 
% it can accelerate the training 
% we introduce a baseline hand trajectory
% The action $\mathbf{a}_n$ contains the joint torque commands applied to all hand joints. We follow the literature and adopt 
% contains the positional target of each hand joint. 
% is the positional target of each hand joint
% in the kinematic reference sequence
% aims at optimizing $\pi$ 
% Our RL-based scheme for training the tracking controller aims at optimizing the tracking policy $\pi$ with the objective to maximize the discounted cumulative reward: 
% \begin{align}
%     J = \mathbb{E}_{p(\tau\vert )}
% \end{align}
% We carefully design the action space, observation space, and reward to develop an RL-based optimization scheme for learning a tracking controller. To address the poor sample efficiency of RL, instead of letting the tracking policy learn the absolute positional target $\mathbf{a}_n$ at each timestep $n$, we introduce a basic trajectory~\eric{base trajectory or basic trajectory? I would suggest the former. Please make sure we modify all terms together.}, commonly the kinematic reference trajectory, and allow RL to learn the residual relative target $\Delta \mathbf{a}_n$ at each timestep. For each timestep $n$, denote the hand state in the basic trajectory as $\mathbf{h}_n^b$, we compute the final positional target as follows: 
% % we denote the hand state in the basic trajectory as $\mathbf{h}_n^b$ and compute the final positional target as follows:
% \begin{align}
%     \mathbf{a}_n = \mathbf{h}_n^b + \sum_{k=1}^n \Delta \mathbf{a}_k. 
% \end{align}
% This approach not only accelerates training but also provides an additional dimension for optimizing the single trajectory tracking policy. For instance, we can initialize the basic trajectory using the tracking results from a different task and then re-optimize the residual policy to enhance the single trajectory tracking performance.
%%%%%%%%%%%%%%%%%%%%%% baseline trajs %%%%%%%%%%%%%%%%%%%%%%


% using the action trajectory sampled 
% In our method, we try to improve the single trajectory tracking by varying the basic trajectory from its own kinematic reference motion to the rollout action trajectory produced by a well-trained expert for a different task or from the general tracking policy, followed by re-optimizing the residual policy. 

%%%%%%%%%%%%%%%%%%%%%% observation space %%%%%%%%%%%%%%%%%%%%%%
% At each timestep $n$, the observation includes information about the current hand and object states, the next frame tracking target, the current cumulated residual targets, the next step hand state in the basic trajectory, and the pre-processed object feature $\text{feat}_{\text{obj}}$. The $n$-th step observation, denoted as $\mathbf{o}\mathbf{b}\mathbf{s}_n$, is calculated as follows:
% \begin{align}
%     \mathbf{o}\mathbf{b}\mathbf{s}_n = \{ \hat{\mathbf{h}}_n, \hat{\mathbf{o}}_n,  \hat{\dot{\mathbf{h}}}_n, \hat{\dot{\mathbf{o}}}_n, \mathbf{f}_n,  \mathbf{h}_{n+1}, \mathbf{o}_{n+1}, \mathbf{h}_{n+1} - \hat{\mathbf{h}}_n, \mathbf{o}_{n+1} \ominus \hat{\mathbf{o}}_n, \sum_{k=1}^n\Delta\mathbf{a}_k, \mathbf{h}_{n+1}^b, \text{feat}_{\text{obj}}   \}, \label{eq:observations}
% \end{align}
% where $\hat{\mathbf{h}}_n$ and $\hat{\mathbf{o}}_n$ represent the observed positions of hand joints and object orientation at timestep $n$, $\hat{\dot{\mathbf{h}}}_n$ and $\hat{\dot{\mathbf{o}}}_n$ denote the velocities of hand joints and object linear and angular velocities, respectively, $\mathbf{f}_n$ indicates the hand finger positions in the world space.
%%%%%%%%%%%%%%%%%%%%%% observation space %%%%%%%%%%%%%%%%%%%%%%






% At each timestep $n$, the observation comprises the current hand and object state information, the next frame tracking target, the current cumulated residual targets, the next step basic hand state, and the pre-processed object feature $\text{feat}_{\text{obj}}$ as input and output the hand action for the hand to achieve the next state. Specifically, at each timestep $n$, we calculate the observation $\mathbf{o}\mathbf{b}\mathbf{s}_n$ as follows: 
% \begin{align}
%     \mathbf{o}\mathbf{b}\mathbf{s}_n = \{ \hat{\mathbf{h}}_n, \hat{\mathbf{o}}_n,  \hat{\dot{\mathbf{h}}}_n, \hat{\dot{\mathbf{o}}}_n, \mathbf{f}_n,  \mathbf{h}_{n+1}, \mathbf{o}_{n+1}, \mathbf{h}_{n+1} - \hat{\mathbf{h}}_n, \mathbf{o}_{n+!} \ominus \hat{\mathbf{o}}_n, \sum_{k=1}^n\Delta\mathbf{a}_k, \mathbf{h}_{n+1}^b, \text{feat}_{\text{obj}}   \}, \label{eq:observations}
% \end{align}
% where $\hat{\mathbf{h}}_n$ and $\hat{\mathbf{o}}_n$ are observed hand joint DoF positions and the object orientation as well as positions at the timestep $t$, $\hat{\dot{\mathbf{h}}}_n$ and $\hat{\dot{\mathbf{o}}}_n$ are hand DoF velocities and object linear as well as angular velocities respectively, $\mathbf{f}_n$ denotes the hand finger positions in the world space. 

% $\mathbf{h}_{n+1}$ and $\mathbf{o}_{n+1}$ are the kinematic targets of the hand DoF positions and the object state in the next timestep. 

% The reward consists a hand tracking reward $r_{h}$, object tracking reward $r_o$, and the hand-object affinity reward $r_d$. Specifically, the object reward contains the object position reward and the orientation reward: 
% \begin{align}
%     r_o = w_p(0.9 - \Vert \mathbf{o}_n^p - \hat{\mathbf{o}}_n^p \Vert_2 ) + w_q(\text{np.pi} - \text{Diff\_Angle}(\mathbf{o}_n^q - \hat{\mathbf{o}}_n^q )), \label{eq:rew_obj}
% \end{align}
% where $w_p$ and $w_q$ are weights of the position reward and the orientation reward respectively, $\text{Diff\_Angle}(\cdot, \cdot)$ calculates the rotation angle difference of two rotations represented as quaternions. The hand reward is the negative of the weighted sum of the current hand DoF positions and the reference positions: 
% \begin{align}
%     r_h = -(w_{\text{trans}} \Vert \mathbf{h}_n[:3] - \hat{\mathbf{h}}_n[:3] \Vert_1 + w_{\text{ornt}} \Vert \mathbf{h}_n[3:6] - \hat{\mathbf{h}}_n[3:6] \Vert_1 + w_{\text{finger}} \Vert \mathbf{h}_n[6:] - \hat{\mathbf{h}}_n[6:] \Vert_1  ), \label{eq:rew_hand}
% \end{align}
% where $w_{\text{trans}}, w_{\text{ornt}}$, and $w_{\text{finger}}$ are weights. The hand-object affinity reward encourages the hand palm and fingers to get close to the object: 
% \begin{align}
%     r_d = -(w_{\text{palm}} \Vert \mathbf{p}_n^{\text{palm}} - \hat{\mathbf{o}}_n^p \Vert + w_{\text{finger}} \sum_{i=1}^{n_f} \Vert \mathbf{f}_{n,i} - \hat{\mathbf{o}}_n^p \Vert),  \label{eq:rew_affinity}
% \end{align}
% where $w_{\text{palm}}$ and $w_{\text{finger}}$ are weights of the palm-object and finger-object affinities respectively, $n_f$ denotes the number of fingers, $\mathbf{f}_{n,i}$ represents the position of the $i$-th finger in the current frame $n$. 
% Finally, the overall reward is composed as the sum of these three components: $r = r_h + r_o + r_d$. 
% We leverage PPO~\citep{Schulman2017ProximalPO} implemented in rl\_games~\citep{rlgames2021} to train our policy. 


%%%%%%%%%%%%%%%%%%%%%% reward function %%%%%%%%%%%%%%%%%%%%%%
% The reward function comprises three components: the hand-tracking reward $r_{h}$, the object-tracking reward $r_o$, and the hand-object affinity reward $r_d$. The object reward includes both position and orientation rewards, defined as:
% \begin{align}
%     r_o = w_p(0.9 - \Vert \mathbf{o}_n^p - \hat{\mathbf{o}}_n^p \Vert_2 ) + w_q(\text{np.pi} - \text{Diff\_Angle}(\mathbf{o}_n^q - \hat{\mathbf{o}}_n^q )), \label{eq:rew_obj}
% \end{align}
% where $w_p$ and $w_q$ are the weights for position and orientation rewards, respectively, and $\text{Diff\_Angle}(\cdot, \cdot)$  computes the rotation angle difference between two quaternions. The hand reward is calculated as the negative weighted sum of the current hand joint positions and the reference positions:
% \begin{align}
%     r_h = -(w_{\text{trans}} \Vert \mathbf{h}_n[:3] - \hat{\mathbf{h}}_n[:3] \Vert_1 + w_{\text{ornt}} \Vert \mathbf{h}_n[3:6] - \hat{\mathbf{h}}_n[3:6] \Vert_1 + w_{\text{finger}} \Vert \mathbf{h}_n[6:] - \hat{\mathbf{h}}_n[6:] \Vert_1  ), \label{eq:rew_hand}
% \end{align}
% % degrees of freedom (DoF) 
% % \[
% % r_h = -\left(w_{\text{trans}} \|\mathbf{h}_n[:3] - \hat{\mathbf{h}}_n[:3]\|_1 + w_{\text{ornt}} \|\mathbf{h}_n[3:6] - \hat{\mathbf{h}}_n[3:6]\|_1 + w_{\text{finger}} \|\mathbf{h}_n[6:] - \hat{\mathbf{h}}_n[6:]\|_1\right), \label{eq:rew_hand}
% % \]
% with weights $w_{\text{trans}}, w_{\text{ornt}}$, and $w_{\text{finger}}$. The hand-object affinity reward encourages proximity between the hand palm and fingers and the object:
% \begin{align}
%     r_d = -(w_{\text{palm}} \Vert \mathbf{p}_n^{\text{palm}} - \hat{\mathbf{o}}_n^p \Vert + w_{\text{finger}} \sum_{i=1}^{n_f} \Vert \mathbf{f}_{n,i} - \hat{\mathbf{o}}_n^p \Vert),  \label{eq:rew_affinity}
% \end{align}
% % \[
% % r_d = -\left(w_{\text{palm}} \|\mathbf{p}_n^{\text{palm}} - \hat{\mathbf{o}}_n^p\| + w_{\text{finger}} \sum_{i=1}^{n_f} \|\mathbf{f}_{n,i} - \hat{\mathbf{o}}_n^p\|\right), \label{eq:rew_affinity}
% % \]
% where $w_{\text{palm}}$ and $w_{\text{finger}}$ are the weights for the palm-object and finger-object affinities, respectively, $n_f$ is the number of fingers, and $\mathbf{f}_{n,i}$ denotes the position of the $i$-th finger at timestep $n$. The overall reward is then computed as the sum of these three components: $r = r_h + r_o + r_d$.  
% % We utilize PPO~\citep{Schulman2017ProximalPO} as implemented in rl\_games~\citep{rlgames2021} to train our policy.
%%%%%%%%%%%%%%%%%%%%%% reward function %%%%%%%%%%%%%%%%%%%%%%


% \todo{bonus reward? }

% \textcolor{red}{(TODO: add the reward design here)}
% \textcolor{red}{(TODO: add the residual policy here --- it is important since we need to justify the task curriculum here)}

% By re-optimizng the residual policy, we can possibly arrive at a better 
% For instance, by setting the base trajectory to an action trajectory produced by an expert trained for a different trajectory, 
% It opens the opportunity to transfer 
% This strategy natrualy afford us an additional dimension in optimizing the single trajectory tracking policy. 
% we train the tracker to output a residual relative target $\Delta \mathbf{a}_n$. 
% letting the policy learn absolute positional targets $\mathbf{a}_n$, we 
% residual policies whose outputs are treated as residual positional targets added to a base trajectory. Formally, 
% given a base trajectory containing the hand DoF positos at each  timestep, \emph{i.e.,} $\mathbf{H}^b = \{ \mathbf{h}_n^b \}_{n=1}^N$, we let the policy to learn a residual positional 
% To further improve the RL's sample efficiency, instead of letting the policy to output abs

% \todo{residual policy here!!!}

% \noindent\textbf{Single manipulation trajectory specialist tracking policy.}  Given a kinematic manipulation trajectory $(\mathbf{H}, \mathbf{O})$, training a specialist policy aims at optimizing a tracking policy $\mathbf{\pi}(\cdot\vert\cdot )$ that can control a dexterous robot hand to track the reference kinematic trajectory. 
% We train a specialist policy leveraging RL to solve the problem. 
% Considering the intricate dynamics involved in the dexterous manipulation resulting from the high DoF of a dexterous hand and the rich and changing hand-object contacts, we resort to model free approaches and leverage RL to optimize the manipulation tracking policy. 
% \noindent\textbf{Trajectory specific specialist policy.} Given a kinematic manipulation trajectory $(\mathbf{H}, \mathbf{O})$, training a specialist policy aims at optimizing a tracking policy $\mathbf{\pi}(\cdot\vert\cdot )$ that can control a dexterous robot hand to track the reference kinematic trajectory. 
% Assuming the physical model is unknown and non-differentiable, we leverage reinforcement learning to optimize the policy. 

% \noindent\textbf{Imitation augmented RL for learning a general tracking controller.} 
% \noindent\textbf{Imitation augmented RL.} 

% \noindent\textbf{Additional Imitation Loss.}  

% \noindent\textbf{Imitation loss.}
% \noindent\textbf{Learning from tracking demonstration.} 
\noindent\textbf{Imitation learning.} 
% To let the tracking controller benefit from a large number of robot tracking demonstration data, we train 
% of imitation-ready data, we train the actor to imitate successful tracking results, aiming at distill ``tracking knowledge'' from these trajectories into the tracking controller. 
% Leveraging the powe
% Relying on RL solely can hardly let us train a strong tracking controller due to its restriction 
% The RL-based learning scheme, restricted by sample inefficiency and the incapability to master multiple tracking problems, can hardly solve the generalizable tracking control problem, as evidenced by our early experiments. Therefore, we propose an IL-based strategy and try to distill successful, abundant, and diverse ``tracking knowledge'' to the tracking controller. Specifically, we train the tracking agent to imitate a large number of high-quality robot tracking demonstrations. 
% In this way, we can effectively guide the agent to produce ``expert actions'' that can successfully track the reference state. Besides, by imitating diverse tracking demonstrations, the agent can avoid repeatedly experiencing low rewards in difficult tracking scenarios, while also preventing over-exploitation of easier tracking tasks. 
% Formally, a robot tracking demonstration of length $N$ contains a kinematic reference sequence $(\hat{\mathbf{s}}_0, ..., \hat{\mathbf{s}}_{N})$, and an expert's state-action trajectory $(\mathbf{s}_0^L, \mathbf{a}_0^L, ..., \mathbf{s}_{N-1}^L, \mathbf{a}_{N-1}^L, \mathbf{s}_N^L)$.
% In addition to the actor loss, we add an action supervision loss to bias the policy's prediction at each timestep towards its corresponding expert action in the demonstration: 
% \begin{align}
%     \mathcal{L}_a = \mathbb{E}_{\mathbf{a}_n\sim \pi(\cdot \vert \mathbf{o}_n, \hat{\mathbf{s}}_{n+1})} \Vert \mathbf{a_n} - \mathbf{a}^L_{n}  \Vert. 
% \end{align}
% In this way, we can guide the policy’s exploration via these demonstrations, ultimately speeding up convergence and improving the performance in the complex problem.  From the IL's perspective, the RL exploration adds noise to the states to make the imitation more robust to noise in a similar flavor to DART~\citep{laskey2017dart}. 
% Since the agent should not and will not explore states too far from the reference states in the tracking control task, it is possible to optimize the policy via both the imitation loss and RL reward simultaneously.
The RL-based learning scheme, hindered by sample inefficiency and its inability to handle multiple tracking problems, struggles to solve the generalizable tracking control problem, as demonstrated by our early experiments. Therefore, we propose an IL-based strategy to distill successful, abundant, and diverse ``tracking knowledge'' into the tracking controller. Specifically, we train the tracking agent to imitate a large number of high-quality robot tracking demonstrations. 
This approach effectively guides the agent to produce ``expert actions'' that can successfully track the reference state. Additionally, by imitating diverse tracking demonstrations, the agent can avoid repeatedly encountering low rewards in challenging tracking scenarios, while also preventing over-exploitation of easier tasks.
Formally, a robot tracking demonstration of length \(N\) consists of a kinematic reference sequence \((\hat{\mathbf{s}}_0, ..., \hat{\mathbf{s}}_{N})\) and an expert's state-action trajectory \((\mathbf{s}_0^L, \mathbf{a}_0^L, ..., \mathbf{s}_{N-1}^L, \mathbf{a}_{N-1}^L, \mathbf{s}_N^L)\). 
In addition to the actor loss, we incorporate an action supervision loss to bias the policy's predictions at each timestep towards the corresponding expert action in the demonstration:
\begin{align}
    \mathcal{L}_a = \mathbb{E}_{\mathbf{a}_n\sim \pi(\cdot \vert \mathbf{o}_n, \hat{\mathbf{s}}_{n+1})} \Vert \mathbf{a_n} - \mathbf{a}^L_{n}  \Vert. 
\end{align}
This guidance allows the policy’s exploration to be informed by these demonstrations, ultimately speeding up convergence and enhancing performance in complex problems. From the perspective of IL, the RL exploration introduces noise into the states, making the imitation more robust, in a similar flavor to DART~\citep{laskey2017dart}. 
Since the agent should not and will not explore states too far from the reference states in the tracking control task, it is feasible to optimize the policy using both the imitation loss and RL reward simultaneously.


% the agent can avoid continuously experiencing low rewards in hard tracking cases, thereby free of exploiting easy tracking problems. 
% getting stuck in exploiting easy tracking problems due to low positive reward from hard tracking cases.
% ~\eric{due to low positive reward from hard tracking cases?}. 

% ~\eric{In addition to the actor loss,} 

% action prediction for the state-goal pair $(\mathbf{s}_n, \hat{\mathbf{s}}_{n+1})$ towards its corresponding expert action in the demonstration: 

% encourage the policy predict 
% when training the agent to solve the tracking problem with a kinematic sequence $(\hat{\mathbf{s}}_0, .., \hat{\mathbf{s}}_N)$ and the expert action trajectory $(\mathbf{a}^L_0, ..., \mathbf{a}^L_{N-1})$ from the demonstration, we add an action supervision loss to the policy's predicted action $\mathbf{a}_n$ at each timestep: 
% \begin{align}
%     \mathcal{L}_a = \sum_{n=0}^{N-1} \mathbb{E}_{\mathbf{a}_n\sim \pi(\cdot\vert \mathbf{o}_n, \hat{\mathbf{s}}_{n+1})} \Vert \mathbf{a_n} - \mathbf{a}^L_{n}  \Vert.  
% \end{align}

% making it possible to optimize with both the imitation loss and RL reward simultaneously.
% In the tracking control task, where the RL should not and will not explore states too far from the reference states, making it possible to optimize with both the imitation loss and RL reward simultaneously.
% the demonstrations can guide 
% The demonstrations bias the policy toward the supervision data. It can thereby 

% for each tracking demonstration containing the expert action sequence $(\mathbf{a}^L_0, ..., \mathbf{a}^L_{N-1})$ for tracking the kinematic sequence $(\hat{\mathbf{s}}_0, .., \hat{\mathbf{s}}_N)$, we add an action supervision loss to the policy's predicted action $\mathbf{a}_n$ at each timestep
% hardly offer us a strong tracking controller. 
% can hardly let us get a strong tracking controller. 
% To facilitate the learning of the tracking controller, we propose to distill successful ``tracking knowledge'' from high-quality robot tracking demonstrations. 
% % Specifically, we propose to train the  tracking controller to imitate high-quality robot tracking demonstrations. 
% % propose to leverage the joint power of RL and IL and train the tracking 
% Specifically, instead of relying on the RL-based training scheme solely, we additionally train the tracking controller to imitate high-quality robot tracking demonstrations. 
% % in addition to the RL-based training scheme, 
% Formally, for each trajectory with the action sequence $(\mathbf{H}_L, \mathbf{O}_L, \mathbf{A}_L)$, we incorporate the following action supervision loss into the policy's prediction at each timestep \((\mathbf{\mu}_n, \mathbf{\sigma}_n)\) to  encourage the actor to mimic this trajectory: 
% % encourage the actor to mimic this trajectory at each timestep by incorporating the following action supervision loss into the policy's prediction at each timestep \((\mathbf{\mu}_n, \mathbf{\sigma}_n)\):
% \begin{align}
%     \mathcal{L}_a = \Vert \mathbf{\mu}_n -  \mathbf{a}_n \Vert, 
% \end{align} 
% where \(\mathbf{a}_n\) denotes the action taken at the \(n\)-th timestep from the action trajectory $\mathbf{A}_L$.


% This strategy aims to align the action trajectory produced by the current policy with the optimized successful action sequence from the dataset, resembling the approach used in DART~\citep{laskey2017dart}~\eric{from RL's perspective, the demonstrations bias the policy toward the supervision data. from IL's perspective, the RL exploration adds noise to the states to make the imitation more robust to noise in a similar flavor to DART}.~\eric{Also mention that since this is the tracking task, the RL policy should not and will not explore states too far from the reference states, making it possible to optimize with both the imitation loss and RL reward simultaneously.}


% s at each timestep \((\mathbf{\mu}_n, \mathbf{\sigma}_n)\):
% an imitation loss to the 
% Given a dataset containing paired kinematic trajectories and action sequences $\mathcal{S}_L=\{ (\mathbf{H}_L, \mathbf{O}_L, \mathbf{A}_L) \}$, we introduce an additional action supervision loss to the actor loss, aimed at distilling successful ``tracking knowledge'' from these trajectories into the tracking controller. 

% Formally, for each labeled trajectory \((\mathbf{H}_L, \mathbf{O}_L, \mathbf{A}_L)\), we incorporate the following action supervision loss into the policy's predictions at each timestep \((\mathbf{\mu}_n, \mathbf{\sigma}_n)\):
% \begin{align}
%     \mathcal{L}_a = \Vert \mathbf{\mu}_n -  \mathbf{a}_n \Vert, 
% \end{align}
% where \(\mathbf{a}_n\) denotes the action taken at the \(n\)-th timestep from the optimized action trajectory. This strategy aims to align the current trajectory with the optimized successful trajectory from the dataset, resembling the approach used in DART~\citep{laskey2017dart}.

% \noindent\textbf{Additional imitation loss.} 
% Given a dataset containing paired kinematic trajectories and action sequences $\mathcal{S}_L=\{ (\mathbf{H}_L, \mathbf{O}_L, \mathbf{A}_L) \}$, we add an additional action supervision loss to the actor loss to distill successful ``tracking knowledge'' from these trajectories to the tracking controller. 
% Formally, for each labeled trajectory $(\mathbf{H}_L, \mathbf{O}_L, \mathbf{A}_L)$, the following action supervised loss is additionally added on the policy's prediction in each timestep $(\mathbf{\mu}_n, \mathbf{\sigma}_n))$ to train the policy:
% \begin{align}
%     \mathcal{L}_a = \Vert \mathbf{\mu}_n -  \mathbf{a}_n \Vert, 
% \end{align}
% where $\mathbf{a}_n$ denotes $n$-th step action taken from the optimized action trajectory. This strategy tries to bias the current trajectory towards the optimized successful trajectory in the dataset, in a similar flavor to DART~\citep{laskey2017dart}. 


% With the carefully designed specialist policy training strategy and the task relational space exploration and modeling scheme, it is possible for us to optimize for a satisfactory result for each single manipulation trajectory tracking task. However, it is still not enough to acquire a generalist that can not only achieve good performance in each sequence but even generalize to unseen trajectories. Simply following the above RL scheme and training a single policy on multiple tasks is not feasible as demonstrated in our early experiments. As the number of training tasks increases, the RL would even struggle with effectively optimizing a strong policy that can solve every training task, letting alone generalize to unseen trajectories and resolve the novel manipulation sequence tracking challenge. We suppose such pool fitting phenomenon comes from the inherent deficiency of RL in solving multiple tasks. We therefore propose to introduce labeled single trajectory optimization results into the training process of RL, \textit{i.e.,} providing additional action labels as the supervision signals to train the policy together with RL. Specifically, 
% n action-labeled dataset 

% wthe imitation augmented RL involves adding an additional action supervision loss to the actor loss to distill successful ``tracking knowledge'' from these trajectories to the tracking controller. 
% where each trajectory $(\mathbf{H}_L, \mathbf{O}_L)$ is augmented with an additional action sequence $\mathbf{A}_L$, 
% we add an action supervision loss to the actor loss. 
% produced by well-trained per-trajectory trackers,
% Intuitively, this strategy tries to distill knowledge form the action-labeled trajectories to the tracking controller. 
% optimized knowledge in per-trajectory trackers to the general tracking controller. 

% $(\mathbf{\mu}_n, \mathbf{\sigma}_n) = \mathbf{\pi}(\cdot \vert \hat{\mathbf{s}}_n)$ to train the policy:

% to train the policy $\mathbf{\pi}(\cdot\vert \cdot)$  


% \todo{need to say we usthe same model-free tacking algo as that for the tracking controller}
\vspace{-5pt}
\subsection{Mining High-Quality Robot Tracking Demonstrations Using Neural Controller through a Homotopy Optimization Scheme} 
\label{sec:method_data}
\vspace{-5pt}
% To prepare demonstrations to train the tracking controller, we need to infer action sequences from kinematic-only trajectories to create paired data with kinematic references and the action sequence. We primarily leverage model-free reinforcement learning techniques to train the tracking policy for each single trajectory to acquire its corresponding action sequence.~\eric{we should briefly explain the naive way of doing per-trajectory tracking. No one knows its relation with the controller at this moment and we should not assume that is straightforward to understand.} Though this straightforward solution can successfully track lots of trajectories~\eric{I think we should mention that a lot of trajectories can not be tracked by this naive method instead of emphasizing that it is already quite successful?}, relying solely on optimizing per-trajectory tracking policy independently presents challenges for complex tracking scenarios. To resolve difficulties in tracking challenging trajectories to improve the diversity and the quality of tracking demonstrations, we then explore the possibility of~\eric{this expression sounds weak} using the tracking controller through a homotopy optimization scheme to improve per-trajectory tracking by 1) transferring tracking prior from the tracking controller and 2) learning a homotopy path generator which can propose useful homotopy paths that we can leverage to track previously difficult-to-track trajectories through a homotopy optimization method. 
% % directly training a policy leveraging RL with the above designs 

% Preparing demonstrations to train the training controller

% Preparing demonstrations to train the tracking controller requires us to infer the action 
% To prepare a demonstration to train the tracking controller, we need to infer the action sequence $(\mathbf{a}^L_0,...,\mathbf{a}^L_{N-1})$ that can successfully track the kinematic reference trajectory $(\hat{\mathbf{s}}_0, ..., \hat{\mathbf{s}}_N)$. A naive way is leveraging RL to train a single trajectory tracking policy $\pi$ and take its resulting action sequence as the demonstration. However, leveraging this strategy alone can hardly provide us with a diverse and high-quality tracking demonstration dataset because RL would be challenged by the inherent difficulties in dexterous manipulation. To improve the diversity and the quality of robot tracking demonstrations, we propose to leverage the tracking controller and a homotopy optimization scheme to improve the per-trajectory tracking results. 

To prepare a demonstration from a kinematic reference trajectory \((\hat{\mathbf{s}}_0, ..., \hat{\mathbf{s}}_N)\) for training the tracking controller, we need to infer the action sequence \((\mathbf{a}^L_0, ..., \mathbf{a}^L_{N-1})\) that successfully tracks the reference sequence.  
% trajectory \((\hat{\mathbf{s}}_0, ..., \hat{\mathbf{s}}_N)\). 
A straightforward approach is to leverage RL to train a single-trajectory tracking policy \(\pi\) and use its resulting action sequence directly. 
% as the demonstration. 
However, relying solely on this strategy often fails to provide a diverse and high-quality dataset of tracking demonstrations, as RL struggles with the inherent challenges of dexterous manipulation.
To enhance both the diversity and quality of robot tracking demonstrations, we propose utilizing the tracking controller in conjunction with a homotopy optimization scheme to improve the per-trajectory tracking results.

% We first think present a naive solution 

% ~\eric{train a single policy or train a per-trajectory tracking policy?
% }

% of the inherent difficulty of dexterous manipulation tracking that would 
% which will challenge the optimization of RL, especially in difficult manipulation tracking problems.

% this strategy can hardly provide 
% Specifically, we can formulate is as a single trajectory tracking problem. Given the kinematic references, we aim at optimizing a tracking policy $\pi$ that can track the reference trajectory. In each tiemstep, $\pi$ observes the current state $\mathbf{s}_n$ and the next goal state $\hat{\mathbf{s}}_{n+1}$; and predict the distribution of the current action $\mathbf{a}_n$. $\pi$ is optimized to minimize the difference between the transited state $\mathbf{s}_{n+1} \sim p(\cdot \vert \mathbf{s}_n, \mathbf{a}_n)$ in each timestep $n$ is close to the goal state $\hat{\mathbf{s}}_{n+1}$. 
% By sampling from the $\pi$ in each step and transiting to the next state, we can  
% However, this way of doing per-trajectory tracking cannot provide us with 

\noindent\textbf{RL-based single trajectory tracking.} 
% A basic solution to acquire the demonstration is leveraging RL to solve the per-trajectory tracking problem and taking the resulting action sequence as the demonstration. Given a kinematic reference trajectory, we aim at optimizing a tracking policy $\pi$ that can track the reference trajectory. In each timestep, $\pi$ observes the current state $\mathbf{s}_n$ and the next goal state $\hat{\mathbf{s}}_{n+1}$. It then predicts the distribution of the current action $\mathbf{a}_n$. 
% The policy $\pi$ is optimized to minimize the difference between the transited state $\mathbf{s}_{n+1} \sim p(\cdot \vert \mathbf{s}_n, \mathbf{a}_n)$ at each timestep $n$ and the goal state $\hat{\mathbf{s}}_{n+1}$. Similar to our design for the RL-based learning scheme for the tracking controller, we adopt a residual action space. We introduce a baseline trajectory $(\mathbf{s}^b_0,...,\mathbf{s}^b_N)$ for each tracking task, while only leveraging RL to learn a residual policy. Observations and reward are the same to our design for tracking controller (see Eq.~\ref{eq_obs}~\ref{eq_rew}). Once $\pi$ has been optimized, we can sample the action $\mathbf{a}_n$ from $\pi$ in each timestep. By iteratively transferring to the next state using the predicted action and querying the policy $\pi$ to generate a new action, we can get the expert action sequence $(\mathbf{a}_0^L, ..., \mathbf{a}_{N-1}^L)$ for the input kinematic reference trajectory. 
A basic approach to acquiring demonstrations is to leverage RL to address the per-trajectory tracking problem, using the resulting action sequence as the demonstration. Given a kinematic reference trajectory, our goal is to optimize a tracking policy \(\pi\) that can accurately track this reference trajectory. At each timestep, \(\pi\) observes the current state \(\mathbf{s}_n\) and the next goal state \(\hat{\mathbf{s}}_{n+1}\), and it predicts the distribution of the current action \(\mathbf{a}_n\).
The policy \(\pi\) is optimized to minimize the difference between the transited state \(\mathbf{s}_{n+1} \sim p(\cdot \vert \mathbf{s}_n, \mathbf{a}_n)\) at each timestep \(n\) and the goal state \(\hat{\mathbf{s}}_{n+1}\). Similar to our design for the RL-based learning scheme for the tracking controller, we adopt a residual action space. For each tracking task, we introduce a baseline trajectory \((\mathbf{s}^b_0, ..., \mathbf{s}^b_N)\) and only learn a residual policy using RL. The observations and rewards follow the same design as for the tracking controller (see Eq.~\ref{eq_obs} and Eq.~\ref{eq_rew}).
Once \(\pi\) has been optimized, we can sample the action \(\mathbf{a}_n\) from \(\pi\) at each timestep. By iteratively transitioning to the next state using the predicted action and querying the policy \(\pi\) to generate a new action, we can obtain the expert action sequence \((\mathbf{a}_0^L, ..., \mathbf{a}_{N-1}^L)\) for the input kinematic reference trajectory.


% $\pi$ is optimized to minimize the difference between the transited state $\mathbf{s}_{n+1} \sim p(\cdot \vert \mathbf{s}_n, \mathbf{a}_n)$ in each timestep $n$ is close to the goal state $\hat{\mathbf{s}}_{n+1}$.~\eric{this sentence is grammarly incorrect.}

% with those designed for the tracking controller (see Eq.~\ref{eq_obs}~\ref{eq_rew}). 
% each kinematic trajectory, while 



% naive per-trajectory tracking 

% \noindent\textbf{Transferring tracking prior.} 
\noindent\textbf{Transferring \textcolor{myblue}{``tracking prior''}.} 
% To improve the quality and diversity of the demonstrations, we devise a strategy that utilizes the general ``tracking knowledge'' encoded in the universal tracking controller to improve trajectory-specific tracking policies. Specifically, to track a kinematic reference trajectory $(\hat{\mathbf{s}}_0, ..., \hat{\mathbf{s}}_N)$, we first track it via the tracking controller with the baseline trajectory set to the kinematic reference. We then set the baseline trajectory to the resulting action sequence and re-optimize the residual policy leveraging the RL-based single trajectory tracking method. The tracking controller can help us find a better baseline trajectory, which eases policy learning and improves the single trajectory tracking result. 
To \textcolor{myblue}{improve the demonstrations, we devise a strategy that takes advantage of the tracking controller, which has already encoded ``knowledge'' that can track lots of trajectories (named as ``tracking prior''), to improve trajectory-specific tracking policies}.
% improve the quality and diversity of demonstrations, we devise a strategy that takes advantage of the general ``tracking knowledge'' encoded in the universal tracking controller to improve trajectory-specific tracking policies. 
Specifically, to track a reference trajectory \((\hat{\mathbf{s}}_0, ..., \hat{\mathbf{s}}_N)\), we first utilize the tracking controller to track it, with the baseline trajectory set to thew reference sequence. We then adjust the baseline trajectory to the resulting action sequence and re-optimize the residual policy using the RL-based single trajectory tracking method. This approach can help us find a better baseline trajectory, facilitating single-trajectory tracking policy learning and improving the per-trajectory tracking results. 
% the result. 
% the single trajectory tracking result. 
% allows the tracking controller to find a better baseline trajectory, facilitating policy learning and improving the single trajectory tracking result. 

% ~\eric{it might confuse the reader here since the tracking policy refers to a per-trajectory tracker while the tracking controller refers to the general tracker. I think it would be better if we could use a more distinctive term to differentiate in this whole section. Maybe trajectory-specific tracking policy v.s. universal tracking controller?} 

% per-trajectory tracking. 

% The tracking controller can help us find a better baseline trajectory that would ease the policy learning, thereby facilitating the per-trajectory tracking. 

% per-trajectory tracking.
% Then we set the baseline trajectory as the
% use the tracking controller to track 
% to track a 
% % for a 
% kinematic reference trajectory $(\hat{\mathbf{s}}_0, ..., \hat{\mathbf{s}}_N)$, we first query the tracking controller to generate an action sequence $(\mathbf{a}^{\text{prior}}_0, ..., \mathbf{a}^{\text{prior}}_{N-1})$. After that, we set the baseline trajectory as the sampled actions and optimize the residual policy leveraging the RL-based per-trajectory tracking scheme. The tracking controller can help us find a better baseline trajectory that would ease the policy learning, thereby facilitating the per-trajectory tracking. 
% If the action sequence we've sampled from the tracking controller proved to serve as a better baseline trajectory for the single trajectory tracking policy, we then take the new tracking result as its action sequence and improve the demonstration. 
% Since the tracking controller 
% Once the residual policy have been optimized, we can get its tracking results. If we can better track the kinematic references in this way than adopting the original approach, 
% without transferring the tracking prior, 
% the resulting action 
% If the action sequence 

% can get a better tracking result. 
% baseline trajectory that we've sampled from the tracking controller proves to serve asa 
% transfer the general knowledge encoded in the tracking controller to improve the per
% Considering the 
% the limitations of leveraging RL to solve the per-trajectory tracking 
% With the tracking controller, which has already been optimized to be able to track lots of manipulation trajectories, we devise a method that can let the single trajectory tracking benefit from the tracking controller. Specifically, for a tracking task $T$ that is difficult to solve, we first query the tracking controller for an action sequence $\mathbf{A}$. After that, we initialize $\mathbf{A}$ as the basic trajectory in the single trajectory tracking policy~\eric{since we did not explain how to do the per-trajectory tracking above, it is weird here to talk about the base trajectory.}. We then optimize the residual policy aiming to track the trajectory $T$.  Taking advantage of the general tracking prior encoded in the tracking controller, we can potentially~\eric{do not use these very vague term} get better tracking results than directly tracking $T$ with the kinematic sequence as the basic trajectory. 
% After that, we optimize the residual policy aiming to track the trajectory $T$.



% \noindent\textbf{Learning a homotopy path generator and tracking via a homotopy method.} 

% \noindent\textbf{Solving difficult tracking problem via a homotopy optimization scheme.} 
\noindent\textbf{A homotopy optimization scheme.}
% The tracking controller's training relies on the results from previously successfully tracked trajectories, which would limit its ability.~\eric{this sentence seems redundant. maybe remove it and just keep the next sentence.} 
Training the controller with data mined from itself can introduce biases and reduce diversity, hindering its generalization capabilities. To address this issue, we propose a homotopy optimization scheme to improve the per-trajectory tracking performance and to tackle previously unsolvable single-trajectory tracking problems. For the tracking problem $T_0$, instead of solving it directly, the homotopy optimization iteratively solves each tracking task in an optimization path, \emph{e.g.,} $(T_K, T_{K-1}, ..., T_0)$, and finally solves $T_0$, in a similar flavor to ``chain-of-thought''~\citep{Wei2022ChainOT}. In the beginning, we leverage the RL-based tracking method to solve $T_K$. After that, we solve each task $T_m$ via the RL-based tracking method with the baseline trajectory set to the tracking result of $T_{m+1}$. This transfer of tracking results from other tasks helps us establish better baseline trajectories, ultimately yielding higher-quality tracking outcomes.
% solves it via iteratively solving each tracking task in an optimization path, \emph{e.g.,} $(T_K, T_{K-1}, ..., T_0)$, 

% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Transferring the tracking results of other tasks can help us find better baseline trajectories, thereby offering us better tracking results with improved quality. 
% an optimization path,  \emph{e.g.,} $(T_K, T_{K-1}, ..., T_0)$. 
% Instead of tracking a trajectory directly, the homotopy optimization 
% solving each single-trajectory tracking problem directly, 
% % a strategy that utilizes RL and transfers tracking knowledge to tackle previously unsolvable single-trajectory tracking problems. 
% Instead of addressing each single-trajectory problem directly, we employ a homotopy optimization method, inspired by the "chain-of-thought" approach~\citep{Wei2022ChainOT}, to enhance tracking results. Specifically, the homotopy optimization addresses the tracking problem for task \(T_0\) through an optimization path, \((T_K, T_{K-1}, ..., T_0)\). Initially, we use RL to solve for \(T_K\). Subsequently, we address each task \(T_m\) by applying the RL-based tracking method, using the tracking result of \(T_{m+1}\) as the baseline trajectory. This transfer of tracking results from other tasks enables us to establish better baseline trajectories, ultimately yielding higher-quality tracking outcomes.

% Considering the fact that the tracking controller is trained with the supervision from the tracking results of the already successfully tracked trajectories, its ability would be restricted.  Using data mined by the controller to train the controller could suffer from data biases and diversity issues, which may hinder the generalization ability of the tracking controller. To mitigate this issue, we need a strategy that can solve previously unsolvable single trajectory tracking problems by leveraging RL and even transferring the tracking prior. Therefore, instead of solving each single-trajectory tracking problem directly, we propose to leverage a homotopy optimization method, mimicking ``chain-of-thought''~\citep{Wei2022ChainOT}, to improve the tracking results. Specifically, the homotopy optimization solves the racking problem of task $T_0$ via an optimization path,  \emph{e.g.,} $(T_K, T_{K-1}, ..., T_0)$. At the beginning, we leverage the RL to solve $T_K$. After that, we solve each task $T_m$ via the RL-based tracking method with the baseline trajectory set to the tracking result of $T_{m+1}$. Transferring the tracking results of other tasks can help us find better baseline trajectories, thereby offering us better tracking results with improved quality. 
% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%




% may be restricted in the ability to solve 
% relatively simple tracking problems.
% Therefore, using the data mined 
% that can effectively improve the diversity and quality 
% ~\eric{Need to mention that the using the data mined by the controller to train the controller could suffer from data biases and diversity issues, which could hinder the generalizability of the controller to novel and challenging cases. Therefore we need bias mitigation techniques.} Solving very difficult single trajectory tracking problems would still pose lots of challenges. 

% optimizing the per-trajectory tracker alone, we propose to leverage a 

% to improve the tracking results of very difficult trajectories~\eric{very difficult trajectories sound like just a very small portion of the data? it is better to give the feeling that the method is generally helpful}.

% higher-quality
% by the controller to train the controller could suffer from data biases and diversity issues, which may hinder the generalization ability of the tracking controller. To mitigate this issue, we need a strategy that can solve previously unsolvable single trajectory tracking problems by leveraging RL and even transferring the tracking prior. 
% by transferring the tracking result of $T_{m+1}$ as the baseline trajectory. x

% In this way, we leverage the tracking knowled
% If the tracking result of $T_0$ can achieve can achieve better tracking performance
% Iteratively solving each problem 
% transfer the tracking results of $T_{m+1}$ as the baseline trajectory of the tracking task $T_m$  
% solve the per-
% for a tracking task $T_0$, 
% $T_0$ that is hard to solve, we propose to solve $T_0$ via a homotopy path, \emph{e.g.,} $(T_K, T_{K-1}, ..., T_0)$. Starting from solving the first task $T_K$, we transfer the tracking result obtained in the current task to initialize the basic trajectory of the next tracking task. By gradually transferring tracking results from the previous task and solving each problem gradually in this manner, we could potentially~\eric{again, this is a very weak term and should be avoided} get better tracking results of task $T_0$. 

\noindent\textbf{Finding effective homotopy optimization paths.}
% Although leveraging the homotopy optimization scheme to solve previously unsolvable tracking problems proves to be an effective strategy for many tracking tasks, it relies on effective optimization paths. A naive way to find an effective homotopy path is brute-force searching. Specifically, given a collection of kinematic references, we optimize their per-trajectory tracking results at first. We also identify neighbors for each tracking task, \emph{e.g.,} based upon the similarity between every two kinematic reference trajectories. After that, we iteratively transfer the optimization results from neighbors and re-optimize the residual policy for each tracking task. We record the neighbor that can give the current tracking task a better baseline trajectory than its kinematic reference sequence as an effective ``parent task''. After we've reached the maximum number of iterations, \emph{i.e.,} $K$, to find effective homotopy paths for the tracking task $T_0$, we can start from $T_0$ and backtrace effective ``parent tasks''. We regard $(T_{K}, ..., T_0)$ as an effective homotopy path of $T_0$ if for each $0\le m \le K$, $T_{m+1}$ is an effective parent task of $T_m$. 
While leveraging the homotopy optimization scheme to solve previously unsolvable problems has proven effective for many tasks, it depends on identifying effective optimization paths. A straightforward approach is brute-force searching. Specifically, given a set of kinematic references, we first optimize their per-trajectory tracking outcomes. We then identify neighbors for each task based on the similarity between pairs of kinematic reference trajectories. Next, we iteratively transfer optimization results from neighboring tasks and re-optimize the residual policy for each tracking task. For a specific task, we consider a neighbor that provides a better baseline trajectory than its kinematic trajectory as an effective ``parent task''.
% We consider a neighbor that provides a better baseline trajectory than the original kinematic reference as an effective ``parent task''. 
After reaching the maximum number of iterations, $K$, we can find effective homotopy paths for a task, \emph{i.e.,} $T_0$, by starting from $T_0$ and backtracing
% begin from the target task to optimize $T_0$ and backtrace 
% the
effective ``parent tasks''. We define $(T_{K}, ..., T_0)$ as an effective homotopy path for $T_0$ if, for each $0 \leq m \leq K$, $T_{m+1}$ is 
% serves as 
an effective parent task for $T_m$.


% In each iteration, for each tracking task, we try to transfer the tracking result of each of its neighbors as its baseline trajectory and re-optimize the residual tracking policy. 

% suppose we wish to 
% several iterations, 

% the best initialization as the best ``parent task''. After several iterations, \emph{i.e.,} $K$, if we can solve the current target tracking task successfully (\emph{i.e.,} achieving the tracking error smaller than a success threshold), we then start from the current tracking task and backtrace to find an effective homotopy optimization path. In more detail, starting from the task $T_0$, we can find its best ``parent task'' $T_1$. Then for each $1\le m\le K-1$, we find its best ``parent task'' $T_{m+1}$. Finally, we get an effective optimization path $(T_{K}, ..., T_0)$ for the task $T_0$. 

% If we can get better tracking result, we record the 
% every tracking task, we try to transfer th 


% \noindent\textbf{Learning a homotopy generator by encoding effective optimization paths.} 
\noindent\textbf{Learning a homotopy generator for efficient homotopy path planning.} 
% Leveraging the homotopy optimization scheme to 
% Since finding effective homotopy optimization paths is costly, which is unaffordable during the inference time 
% Finding effective homotopy optimization paths for tracking each trajectory is extremely costly and unaffordable during inference time. Therefore, we propose to learn a homotopy path generator $\mathcal{M}$ from a small set. Then, we can use the homotopy generator to propose effective homotopy paths for a tracking task efficiently. The most critical part of finding a homotopy path is finding effective ``parent tasks''. We re-formulate the homotopy path generator learning problem as a tracking task transformation problem. For each tracking task $T_0$, we wish the generator $\mathcal{M}$ to give us a distribution of its effective ``parent tasks'': $\mathcal{M}(\cdot\vert T_0)$, considering the fact that a single tracking task may have multiple effective ``parent tasks''. Once $\mathcal{M}$ has been trained, for $T_0$, we can iteratively query $\mathcal{M}$ to generate a homotopy path: $(T_K, ..., T_0)$, where $T_{m+1}\sim \mathcal{M}(\cdot \vert T_m), \forall 0\le m\le K-1$. 
% We propose to learn a conditional diffusion model as the tracking task transformer, taking advantage of its strong distribution modeling ability. Given a collection of tracking tasks, described using the kinematic reference trajectories of the hand and the object, and the object geometry, we first train a diffusion model to describe the distribution of tracking tasks. 
% To prepare data to fine-tune the diffusion model into a conditional diffusion model, we first search effective homotopy paths from the tracking task collection. 
% Having completed the search, we can obtain a set of paired data of (tracking task $T_c$, effective ``parent task'' $T_p$). 
% We then use these data to tune the diffusion model into a conditional diffusion model, \emph{i.e.,} $T_p \sim \mathcal{M}(\cdot\vert T_c)$. A well-trained $\mathcal{M}$ can propose an effective ``homotopy path'' by recursively finding the parent: $(T_K,...,T_0)$, where $T_{m+1}\sim \mathcal{M}(\cdot \vert \mathcal{T}_m),\forall 0\le m\le K-1$.
Finding effective homotopy optimization paths for tracking each trajectory is computationally expensive and impractical during inference. To address this, we propose learning a homotopy path generator $\mathcal{M}$ from a small dataset, enabling efficient generation of effective homotopy paths for other tracking tasks. The key problem in identifying homotopy paths lies in finding  effective ``parent tasks''. We reformulate this problem as a tracking task transformation problem,
% , 
aiming for a generator $\mathcal{M}$ that provides a distribution of effective ``parent tasks'' for each tracking task $T_0$: $\mathcal{M}(\cdot \vert T_0)$, considering the fact that a single tracking task may have multiple effective ``parent tasks''.
Once $\mathcal{M}$ is trained, we can find a homotopy path by iteratively finding parent tasks.
% iteratively query it to generate a homotopy path for a tracking task. 
% for $T_0$: $(T_K, ..., T_0)$, where $T_{m+1} \sim \mathcal{M}(\cdot \vert T_m)$ for all $0 \le m \le K-1$.
We propose training a conditional diffusion model as the tracking task transformer, leveraging its strong distribution modeling capability. Given a set of tracking tasks, characterized by the kinematic reference trajectories of the hand and object, as well as object geometry, we first train a diffusion model to capture the distribution of tracking tasks.
To fine-tune this diffusion model into a conditional model, we first search for effective homotopy paths within the tracking task dataset. This yields a set of paired data of the tracking task $T_c$ and their corresponding effective ``parent task'' $T_p$. We then use this data to tune the diffusion model into a conditional diffusion model, such that $T_p \sim \mathcal{M}(\cdot \vert T_c)$. A well-trained $\mathcal{M}$ can efficiently propose an effective homotopy path for a tracking task, \emph{i.e.,} $T_0$, by starting from $T_0$ and recursively finding the parent task, resulting in  $(T_K, ..., T_0)$, where $T_{m+1} \sim \mathcal{M}(\cdot \vert T_m)$ for all $0 \le m \le K-1$.
% propose an effective homotopy path by recursively finding parent tasks: $(T_K, ..., T_0)$, where $T_{m+1} \sim \mathcal{M}(\cdot \vert T_m)$ for all $0 \le m \le K-1$.


% that can efficiently generate an effective homotopy path $(T_K, ...., T_0)$ for a tracking task $T_0$. 
% With the learned generator $\mathcal{M}$, we can leverage it to efficiently
% With the learned generator $\mathcal{M}$, 
% Then we can transfer the knowledge of ``homotopy paths'' encoded in $\mathcal{M}$ to more diverse trajectories and plan effective homotopy paths for them using $\mathcal{M}$ in an efficient manner. 

% apply the homotopy generator 
% effectively transfer the 
% Since t


% With a well-trained $\mathcal{M}$, we can recursively find 
% ~\eric{Need a summary sentence describing the inference time behavior?} 

% the collection of tracking tasks. 
% a collection of 
% We first train a diffusion model to describe the distribution of 
% To prepare data, we first search effective homotopy paths from a collection of tracking tasks. Having completed the search, we can obtain a set of paired data of (tracking task, effective ``parent task''). 
% After the search 
% Each searched path can give us a collection of paried tracking 

% can be leveraged to propose effective homotopy optimization paths efficiently during the inference. 



% Taking the tracking task $T_0$ as the example, in each iteration, we try to transfer the optimization result of each  
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% % \noindent\textbf{Learning a homotopy generator by encoding beneficial optimization paths.} 
% \noindent\textbf{Learning a homotopy generator by encoding beneficial optimization paths.} 
% Solving each difficult tracking task in this fashion~\eric{what fashion? make sure every time when you use pronoun, it is clear}, though proved to enough effective, relies on correct~\eric{effective or correct? there does not seem to be a only correct answer for homotopy path} homotopy paths.~\eric{need to explain the naive way to build these paths first (the basic trial and error stuff) before you comment on the cost below.} Searching for correct paths for each difficult-to-track trajectory from scratch, however, is very costly. Therefore, we propose to learn a homotopy generator that can directly propose beneficial homotopy paths at the inference time.~\eric{comment on its connection of chain of though}
% Learning a homotopy generator requires valid optimization paths.~\eric{it is not clear since people dont know you are learning in a supervised manner. You assume too much context here.} Thus we first design an automatic tracking task structure exploration strategy for finding beneficial homotopy paths. We then train a homotopy generator to encode the mined paths~\eric{for efficient inference}.

% Specifically, we try to explore beneficial homotopy optimization paths from the trajectories in the training dataset via a breadth-first-searching strategy. 
% % Specifically, we iteratively explore beneficial cross-tracking task relations from a set of kinematic trajectories
% % We iteratively explore  beneficial cross-.
% % We explore beneficial homotopy paths from the trajectories in the training dataset via a 
% In each searching iteration, we iteratively explore beneficial cross-tracking task relations~\eric{this term is weird and not intuitive. An easier way to introduce what you what to do is to define the term first and then use it. It seems we are using terms first and deine later...Here for every two trajectories, we can evaluate whether their tracking results can benefit the tracking process of each other and we use directional edges to encode such relations.} from a set of kinematic trajectories. For each trajectory, we first identify its neighborhood from other kinematic trajectories via the similarity between two kinematic sequences. After that, we first run the per-trajectory tracking optimization to get their tracking results. Then we try to construct cross-tracking task relations via an iterative BFS approach. 
% In each iteration, for each tracking task $T$, we try to transfer the tracking result of its neighbour $T_p$ to initialize its basic trajectory and re-train the residual policy. If we can get better tracking results using the initialization from $T_p$, we say $T_p\rightarrow T$ a beneficial cross-task relation. We regard $T_p$ as the ``parent trajectory'' of $T$. We record the best tracking result for each trajectory obtained at this iteration.
% After severe iterations, we have obtained a beneficial cross-task relation set $\mathcal{C} = \{ (T_p\rightarrow T) \}$ which contains each beneficial cross-task relation. With $\mathcal{C}$, we regard $(T_{K}, T_{K-1}, ..., T_0)$ is a valid homotopy path if $T_{m+!}$ is the parent task of $T_m$ for each $0\le m \le K - 1$. 



% We propose to train a model that transforms each trajectory $T_c$ to the parent trajectory $T_p$ to model the tracking task structure.~\eric{need to talk about the motivation. the previous mined relationship is only a small set due to the heavy computation cost and we need to use the model to cover more diverse trajectories} 
% Considering the fact that a single child trajectory $T_c$ may have multiple different parent trajectories $T_p$, we formulate it as a conditional trajectory generation problem where the goal is to generate the parent trajectories conditioned on the child $T_c$. Therefore, we train a conditional diffusion model to accomplish this vision. Specifically, we first train a trajectory diffusion model $\mathcal{M}$ which models the distribution of all kinematic trajectories. After that, we fine-tune it to a conditional generative model that describes the distribution of the parent trajectory of each child trajectory. By sampling from the conditional generative model, we transform each tracking task to its parent tracking task, \emph{i.e.,} $T_p\sim \mathcal{M}(\cdot \vert T_c)$. We can construct a homotopy path by iteratively querying the conditional generative model to sample the parent tasks, \emph{i.e.,} $(T_K, T_{K-1}, ..., T_0)$ where $T_{m+1}\sim \mathcal{M}(\cdot \vert T_m)$, $0\le m \le  K - 1$. 

% At the inference time, we let the homotopy generator propose a homotopy path for a hard-to-solve task $T$.~\eric{when to stop?} We gradually solve each tracking task in the path and transfer tracking results to the next task to solve the problem finally. 


% These tracking results are regarded as 
% For each trajectory, we first identify a neighborhood for it from other kinematic trajectories via the similarity between two kinematic trajectories. We first run the per-trajectory tracking optimization to get their individual tracking results. Then we try to construct cross-task relations using an iterative BFS approach. 
% % \todo{all trajs with the initially optimized results} \todo{at first jwe identify ja neighborhood set } 
% In each iteration, for each 





% homotopy path exploration strategy for 
% automatic tracking task structure exploration strategy for finding beneficial homotopy paths. Then we train a homotopy generator to encode the mined paths.


% previous tracking task to initialize the basic trajectory of the next tracking task. By gradually solving each problem in this manner in the series, we could potentially get better results in the task $T$, taking advantage of the basic trajectory initialized from other tracking tasks. 

% % to explore the tracking task
% % explore the tracking task structure and leverage a homotopy method to resolve these challenges by borrowing knowledge from other problems. 
% % Specifically, for a tracking task 
% % to explore


% The homotopy method, though proved to enough effective in resolving challenges in very difficult tracking tasks, relies on correct homotopy paths. 




\vspace{-5pt}
\subsection{Improving the Tracking Controller via Iterative Optimization}
\label{sec:method_iteration}
\vspace{-5pt}

% We employ an iterative approach that alternates between training the tracking controller using abundant robot tracking demonstrations and curating more diverse and higher-quality tracking demonstrations using the controller. Our method consists of three stages. 
% In the first stage, we sample a small set of tracking tasks and generate a demonstration set by applying reinforcement learning (RL) to obtain single trajectory tracking results for each task.  With these demonstrations, we train a tracking controller using RL and IL. We do not learn the homotopy path generator at this stage because the generalization ability of the learned model will be restricted by the number of effective homotopy paths used for training. 
% In the second stage, we sample a trajectory dataset from the remaining tasks, weighted by the tracking error of the controller. We use RL, with attempts to transfer tracking prior, to obtain per-trajectory tracking results, search for homotopy paths, and learn a homotopy generator based on these results. The best tracking results of all successfully tracked trajectories are then curated into a new demonstration set, which we use to re-train the tracking controller.

We adopt an iterative approach that alternates between training the tracking controller with abundant robot tracking demonstrations and curating more diverse, higher-quality demonstrations using the controller. Our method is divided into three stages.
In the first stage, we sample a small set of tracking tasks and generate an initial demonstration set by applying RL to obtain single-trajectory tracking results for each task. Using these demonstrations, we train the tracking controller with both RL and IL. At this stage, we do not train the homotopy path generator, as the model's generalization ability would be limited by the small number of effective homotopy paths available for training.
In the second stage, we sample a dataset of trajectories from the remaining tasks, weighted according to the controller's tracking error. We then use RL, incorporating tracking priors, to optimize per-trajectory tracking, search for homotopy paths, and train a homotopy path generator based on the resulting data. The best tracking results from all successfully tracked trajectories are curated into a new demonstration set, which is used to re-train the tracking controller.
In the third stage, we resample tracking tasks from the remaining set and leverage RL, the tracking controller, and the homotopy generator to curate another set of tracking demonstrations. This final set is used to optimize the tracking controller, resulting in our final model.

% With the above designs, we then adopt an iterative approach where we alternate between 1) training the tracking controller using abundant robot tracking demonstrations and 2) curating a large number of tracking demonstrations using the tracking controller. 
% To balance between the effectiveness of the tracking controller and the time cost in its training, our method consists of three stages as detailed as follows. In the first stage, we sample a small amount of tracking tasks $\mathcal{S}_1$ from all training tasks. 
% Since we do not have the tracking controller or the homotopy generator at the beginning, we create a robot tracking demonstration set by directly leveraging RL to obtain the single trajectory tracking result for each tracking task in $\mathcal{S}_1$. With these demonstrations, we train a tracking controller using RL and IL. The homotopy path generator is not learned at this stage because the generalizaton ability of the learned model will berestricted by the number of effective homotopy paths used for training. 
% In the second stage, we additionally sample a trajectory dataset from the remaining training tracking tasks, with weight positively proportional to the tracking error of the tracking controller. We then jointly leverage RL and the tracking controller to obtain per-trajectory tracking results. We also search the homotopy paths and learn a homotopy generator using searched results. We then curate a demonstration set from the best per-trajectory tracking results we've obtained. 
% With these demonstrations, we re-train a tracking controller.  
% In the third stage, we re-sample tracking tasks from the remaining set. Jointly leveraging RL, the tracking controller, and the homotopy generator, we curate a new robot tracking demonstration set. We then re-train the tracking controller with the supervision provided by these demonstrations. We take it as the final optimized tracking controller. 
% Specifically, 

% data from the whole training dataset. 


% Denote the total tracking task set sampled till now as $\mathcal{S}_2$, 




% We then try to get the per-trajectory tracking results taking the joint effort of per-trajectory tracking optimization, tracking data prior from the tracking controller and the homotopy generator. Having obtained current action-labeled trajectories, we re-train the tracking controller with the supervision provided by those trajectories. We then take it as the final optimized tracking controller.~\eric{Here we only introduce what we do but do not explain the rationality. Why only homotopy in the third stage but not the second stage? Why only three stages in total but not more? At least we should comment a bit on this since with high chance reviewers will ask about it as well. It is better to say something before the others ask.}

% To learn a homotopy generator, we search effective homotopy optimization paths and j
% to get single trajectory tracking result for each 
% We then try~\eric{avoid using these weak terms} to get the per-trajectory tracking result for each trajectory in both the re-sampled dataset and the original dataset taking the joint effort of both the per-trajectory tracking optimization and the tracking data prior provided by the tracking controller. After that, we obtained the second version of the demonstration data. We then re-train the trajectory controller with the supervision from action sequences in the dataset. In this stage, we construct the tracking task structure by searching homotopy optimization paths from trajectories in the dataset. We train a homotopy generator that encodes the searched optimization paths. In the third stage, we re-train the tracking controller via labeled trajectories obtained till now. In the third stage, 
% By optimizing the per-trajectory tracker for each trajectory in the dataset, we get the first version of demonstration data containing trajectories with high-quality action sequences. After that, the labeled dataset is leveraged to train the tracking controller. Then in the second stage, 

% and the tracking task structure prior provided by the tracking task structure prior model. 
% a tracking task structure prior model which encodes the optimization paths. 
% via the imitation-augmented RL. 
% is composed of three stages. 
% e then adopt an interactive approach to gradually get a strong tracking controller. 











% Considering the fact that the tracking controller is trained with the supervision from the tracking results of the already successfully tracked trajectories, it may be restricted in the ability to solve relatively simple tracking problems. Solving very difficult single trajectory tracking problems would still pose lots of challenges. Therefore, instead of optimizing the per-trajectory tracker alone, we propose to explore the tracking task structure and leverage a homotopy method to resolve these challenges by borrowing knowledge from other problems. 
% Specifically, for a tracking task $T$ that is hard to solve, we propose to solve $T$ via a homotopy path, \emph{e.g.,} $(T_A, T_B, ..., T)$. Starting from solving the first task $T_A$, we transfer the tracking result obtained in the previous tracking task to initialize the basic trajectory of the next tracking task. By gradually solving each problem in this manner in the series, we could potentially get better results in the task $T$, taking advantage of the basic trajectory initialized from other tracking tasks. 


% for difficult-to-solve problems 
% to facilitate the per-trajectory tracking problem solving, and 2) 
% introduce two techniques to improve the per-trajectory tracking policy
% with the  tracking controller, we try to 
% With the tracking controller, we try

% th successful action sequences, we need to infer action 
% we need to infer action sequences for kinematics
% We devise a per-trajectory tracking scheme to extract a large volume of high-quality imitation-ready data from kinematic-only trajectories. Initially, single trajectory tracking simplifies the problem by not requiring versatility or generalization, making it more manageable than training a general tracking controller. For various straightforward manipulation trajectories, per-trajectory tracking can effectively leverage the basic RL-based training scheme outlined earlier. However, relying solely on RL to track each trajectory independently presents challenges for more complex tracking scenarios. To address this, we introduce two techniques that enable single trajectory tracking to benefit from the neural controller and the structure of the tracking tasks. The first technique involves transferring tracking data to the controller to enhance single trajectory performance. The second explores the tracking task structure, employing a homotopy method to mitigate the difficulties associated with challenging single trajectory tracking problems.
% % We devise a per-trajectory tracking scheme to mine a large number of high-quality imitation-ready data from kinematic-only trajectories. First, single trajectory tracking, without the demand on versatility or the generalization ability, reduced to a simpler problem than training a general tracking controller. For various manipulation trajectories that are not very difficult, per-trajectory tracking is a solvable problem leveraging the basic RL-based training scheme detailed above. However, solely relying on tracking every single trajectory independently via RL would still face challenges in resolving very difficult tracking problems. Therefore, we propose two techniques to let the single trajectory tracking benefit from the neural controller and the tracking task structure. The first one involves transferring the tracking data prior to the tracking controller to facilitate the single trajectory tracking. The second one tries to explore the tracking task structure and leverage a homotopy method to ease the difficulty in challenging single trajectory tracking problems. 
% % if leveraging advanced model-free RL techniques. 
% % By leveraging the RL to train a single trajectory tracking policy, leveraging the above designed action space, observations, and the reward, we can already get high-quality tracking results for lots of manipulation trajectoreis. 


% \noindent\textbf{Improving per-trajectory tracking via tracking data prior.} With the tracking controller, which has already been optimized to be able to track lots of manipulation trajectories, we devise a method that can let the single trajectory tracking benefit from the tracking controller's tracking capability. Specifically, for a tracking task $T$ that is difficult to solve, we first query the tracking controller for a reasonable action sequence $\mathbf{A}$. After that, we initialize $\mathbf{A}$ as the basic trajectory in the single trajectory tracking policy. After that, we optimize the residual policy aiming to track the trajectory $T$. Taking advantage of the general tracking prior encoded in the tracking controller, we can potentially get better tracking results than directly tracking $T$ with the kinematic sequence as the basic trajectory. 

% \noindent\textbf{A homotopy method for solving difficult per-trajectory tracking.} Considering the fact that the tracking controller is trained with the supervision from the tracking results of the already successfully tracked trajectories, it may be restricted in the ability to solve relatively simple tracking problems. Solving very difficult single trajectory tracking problems would still pose lots of challenges. Therefore, instead of optimizing the per-trajectory tracker alone, we propose to explore the tracking task structure and leverage a homotopy method to resolve these challenges by borrowing knowledge from other problems. 
% Specifically, for a tracking task $T$ that is hard to solve, we propose to solve $T$ via a homotopy path, \emph{e.g.,} $(T_A, T_B, ..., T)$. Starting from solving the first task $T_A$, we transfer the tracking result obtained in the previous tracking task to initialize the basic trajectory of the next tracking task. By gradually solving each problem in this manner in the series, we could potentially get better results in the task $T$, taking advantage of the basic trajectory initialized from other tracking tasks. 

% \noindent\textbf{Learning a homotopy generator by mining beneficial optimization paths.} The homotopy method, though proved to enough effective in resolving challenges in very difficult tracking tasks, relies on correct homotopy paths. Searching for correct paths for each difficult-to-track trajectory from scratch, however, is very costly. Therefore, we propose to learn a homotopy generator that can directly propose beneficial homotopy paths at the inference time. 
% Learning a homotopy generator requires valid homotopy paths. Thus we first design an automatic tracking task structure exploration strategy for finding beneficial homotopy paths. Then we train a homotopy generator to encode the mined paths.

% We explore beneficial homotopy paths from the trajectories in the training dataset via a breadth-first-searching strategy. Specifically, we iteratively explore beneficial cross-tracking task relations from a set of kinematic trajectories.
% For each trajectory, we first identify a neighborhood for it from other kinematic trajectories via the similarity between two kinematic trajectories. We first run the per-trajectory tracking optimization to get their individual tracking results. Then we try to construct cross-task relations using an iterative BFS approach. 
% % \todo{all trajs with the initially optimized results} \todo{at first jwe identify ja neighborhood set } 
% In each iteration, for each tracking task $T$, we try to transfer the tracking result of its neighboring tracking task $T_p$ to initialize its basic trajectory and re-train the residual policy. If we can get better tracking results in this manner, we say $T_p\rightarrow T$ a beneficial cross-task relation. We regard $T_p$ as the ``parent trajectory'' of $T$. We record the best tracking result for each trajectory obtained at this iteration After severe iterations, we have obtained a beneficial cross-task relation set $\mathcal{C} = \{ (T_p\rightarrow T) \}$ which contains each beneficial cross-task relation. With $\mathcal{C}$, we regard $(T_{K}, T_{K-1}, ..., T_0)$ is a valid homotopy path if $T_{m+!}$ is the parent task of $T_m$ for each $0\le m \le K - 1$. 

% We propose to train a model that transforms each trajectory $T_c$ to the parent trajectory $T_p$ to model the tracking task structure. 
% Considering the fact that a single child trajectory $T_c$ may have multiple different parent trajectories $T_p$, we formulate it as a conditional trajectory generation problem where the goal is to generate the parent trajectories conditioned on the child $T_c$. Therefore, we train a conditional diffusion model to accomplish this vision. Specifically, we first train a trajectory diffusion model $\mathcal{M}$ which models the distribution of all kinematic trajectories. After that, we fine-tune it to a conditional generative model that describes the distribution of the parent trajectory of each child trajectory. By sampling from the conditional generative model, we transform each tracking task to its parent tracking task, \emph{i.e.,} $T_p\sim \mathcal{M}(\cdot \vert T_c)$. We can construct a homotopy path by iteratively querying the conditional generative model to sample the parent tasks, \emph{i.e.,} $(T_1, T_2, ..., T_k)$ where $T_{m+1}\sim \mathcal{M}(\cdot \vert T_m)$, $1\le m \le k-1$. 

% At the inference time, we let the homotopy generator propose a homotopy path for a hard-to-solve task $T$. We gradually solve each tracking task in the path and transfer tracking results to the next task to solve the problem finally. 



% \noindent\textbf{Iterative optimization for progressively improving the tracking controller. } We then adopt an interactive approach to gradually get a strong tracking controller. Specifically, our method is composed of three stages. In the first stage, we sample a small amount of data from the whole training dataset. By optimizing the per-trajectory tracker for each trajectory in the dataset, we get the first version of imitation-ready data containing trajectories with high-quality action sequences. After that, the labeled dataset is leveraged to train the tracking controller via the imitation-augmented RL. Then in the second stage, we additionally resample a trajectory dataset from the remaining training trajectories with weight positively proportional to the tracking error of the first version controller. We then try to get the per-trajectory tracking result for each trajectory in both the re-sampled dataset and the original dataset taking the joint effort of both the per-trajectory tracking optimization and the tracking data prior provided by the tracking controller. After that, we obtained the second version of the imitation-ready data. We then re-train the trajectory controller with the supervision from action sequences in the dataset. In this stage, we construct the tracking task structure by searching homotopy optimization paths from trajectories in the dataset. We train a tracking task structure prior model which encodes the optimization paths. In the thrid stage, we re-train the tracking controller via labeled trajectories obtained till now. In the third stage, we re-sample a subste from the remaining trajectories. We then try to get the per-trajectory tracking results taking the joint effort of per-trajectory tracking optimization, tracking data prior from the tracking controller, and the tracking task structure prior provided by the tracking task structure prior model. Having obtained current action-labeled trajectories, we re-train the tracking controller with the supervision provided by those trajectories. We then take it as the final optimized tracking controller. 


% e further propose a homotopy method to resolve challenges in those problems. 
% propose to tackle difficult
% tackle difficult per-trajectory trac
% it may still be very hard for us to get good results in
% % Leveraging advanced RL with our dedicated designs for tracking, we can solve lots of per-trajectory tracking problems to a satisfactory extent. However, this strategy still struggles to solve very difficult tracking problems considering the inherent challenges of manipulation tracking. 
% % restrictions of RL. 
% Therefore, we propose to tackle difficult per-trajectory tracking problems via a homotopy method. 
% % a tracking curriculum to tackle difficult per-trajectory tracking problems. 
% % Therefore, we propose a tracking curriculum to tackle difficult per-trajectory tracking problems. 
% Specifically, for the trajectory $A=(\mathbf{H}_A, \mathbf{O}_A)$ that is difficult to track directly, instead of directly tracking the trajectory $A$, we propose to find an optimization path consisting of a series of tracking tasks that can help us solve the tracking problem of $A$. We then try to track $A$ by tracking each trajectory in the path and transfer results to the next in a one-by-one manner. If in this way we can get better tracking results for $A$ than directly solving $A$, we regard it as a good optimization path for $A$. We then further propose to encode such tracking task structure into a model so that it can propose useful optimization paths during the inference time efficiently. To fulfill this vision, we propose to first mine useful optimization paths from trajectories in the training dataset. We then encode the searched optimization paths into a model so that it can provide us with good optimization paths during the inference period efficiently. 



% to benefit single trajectory tracking from 
% the demand on the tracking
% resuded as a simpler problem than training a general racking controller, 
% To acquire a large number of high-quality tracking results, we devise a per-trajectory tracking scheme
% acquire high-quality per-trajectory tracking results 
% Training a high quality tracking controller requires 
% Training the tracking controller requires a large j
% Single tracjectoyr tracking control, tho
% Solving the per-trajectory tracking problem directly would still make it hard to address the challenge in tracking very difficult trajectories. To improve tracking results in these so that we can get more diverse imitation ready data with high quality action sequences in more tracking cases, we design a strategy where we can leverage the general tracking controller and the tracking task structure prior to better solve per-trajectory tracking problem. With the tracking controller, we devise two strategies to improve per-trajectory tracking performance. First, we try to leverage the general tracking prior encoded in the tracking controller to facilitate difficult per-trajectory tracking problem solving. The tracking controller trained to track lots of manipulation trajectories, with the supervision from high-quality imitation-ready data, already has the capability to give reasonable tracking results for various manipulation trajectories. So we design a strategy where we can let difficult per-trajectory tracking benefit from such tracking prior from the tracking controller. Second, instead of solving the per-trajectory tracking problem alone, we propose a homotopy method whih can effectively solve a difficult tracking problem by solving the task via an optimization path, and a homotopy generator which can propose beneficial homotopy optimization paths efficiently for a problem that is difficult to solve. We will detail them as follows. 


% \noindent\textbf{}



% % \noindent\textbf{Improving per-trajectory trackers via tracking prior and curriculum scheduling.} 
% \noindent\textbf{Improving per-trajectory trackers via tracking data prior and task structure prior.} The general tracking controller and the tracking task structure prior offer us two ways to improve the per-trajectory tracking result of a specific trajectory that is difficult to track, denoted as $A$. Firstly, we sample the action sequence from the general tracking controller for the current tracking task. We then initialize the basic trajectory as the sampled action sequence and re-train the residual policy to try to get better tracking results. Secondly, we let the tracking task structure prior model propose an optimization path consisting of a series of tracking tasks. After that, we solve the tracking problem of $A$ by gradually solving each task in the optimization path and transferring the results to the next one. 


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% \noindent\textbf{A homotopy method for solving difficult per-trajectory tracking problem.} 
% % We propose 
% Leveraging advanced RL with our dedicated designs for tracking, we can solve lots of per-trajectory tracking problems to a satisfactory extent. However, this strategy still struggles to solve very difficult tracking problems considering the inherent challenges of manipulation tracking. 
% % restrictions of RL. 
% Therefore, we propose to tackle difficult per-trajectory tracking problems via a homotopy method. 
% % a tracking curriculum to tackle difficult per-trajectory tracking problems. 
% % Therefore, we propose a tracking curriculum to tackle difficult per-trajectory tracking problems. 
% Specifically, for the trajectory $A=(\mathbf{H}_A, \mathbf{O}_A)$ that is difficult to track directly, instead of directly tracking the trajectory $A$, we propose to find an optimization path consisting of a series of tracking tasks that can help us solve the tracking problem of $A$. We then try to track $A$ by tracking each trajectory in the path and transfer results to the next in a one-by-one manner. If in this way we can get better tracking results for $A$ than directly solving $A$, we regard it as a good optimization path for $A$. We then further propose to encode such tracking task structure into a model so that it can propose useful optimization paths during the inference time efficiently. To fulfill this vision, we propose to first mine useful optimization paths from trajectories in the training dataset. We then encode the searched optimization paths into a model so that it can provide us with good optimization paths during the inference period efficiently. 


% \noindent\textbf{Learning a homotopy generator by mining beneficial optimization paths. } 
% Given a trajectory set containing $K$ trajectories, we find useful tracking task optimization paths via breadth-first search. For each trajectory, we first identify its potential parent set via cross-trajectory similarity. Then in each searching iteration, for each task $A$, we try to use the best-tracked action trajectory of each of its parent task $B$ to initialize its tracking policy and see whether the policy optimization could benefit from the initialization. If the tracking result of trajectory $B$ can help us track the trajectory $A$, we regard $B$ as a beneficial parent tracking task of $A$. After several iterations, we identify a set of optimization paths $\mathcal{C} = \{ (T_n, ..., T_1) \} $, where for each  $1\le k \le n-1$, $T_{k+1}$ is the parent task of $T_k$.  

% Having obtained the optimization path set $\mathcal{C}$, we propose to train a model that transform each trajectory $T_c$ to the parent trajectory $T_p$ to model the tracking task structure. 
% % to model the tracking task structure. 
% Considering the fact that a single child trajectory $T_c$ may have multiple different parent trajectories $T_p$, we formulate it as a conditional trajectory generation problem where the goal is to generate the parent trajectories conditioned on the child $T_c$. Therefore, we train a conditional diffusion model to accomplish this vision. Specifically, we first train a trajectory diffusion model $\mathcal{M}$ which models the distribution of all kinematic trajectories. After that, we fine-tune it to a conditional generative model that describes the distribution of the parent trajectory of each child trajectory. By sampling from the conditional generative model, we transform each tracking task to its parent tracking task, \emph{i.e.,} $T_p\sim \mathcal{M}(\cdot \vert T_c)$. We can construct an optimization path by iteratively querying the conditional generative model to sample the parent tasks, \emph{i.e.,} $(T_1, T_2, ..., T_k)$ where $T_{m+1}\sim \mathcal{M}(\cdot \vert T_m)$, $1\le m \le k-1$. 


% % \noindent\textbf{Planning the homotopy paths for solving difficult problems. } 


% \noindent\textbf{Iterative optimization for progressively improving the tracking controller. } We then adopt an interactive approach to gradually get a strong tracking controller. Specifically, our method is composed of three stages. In the first stage, we sample a small amount of data from the whole training dataset. By optimizing the per-trajectory tracker for each trajectory in the dataset, we get the first version of imitation-ready data containing trajectories with high-quality action sequences. After that, the labeled dataset is leveraged to train the tracking controller via the imitation-augmented RL. Then in the second stage, we additionally resample a trajectory dataset from the remaining training trajectories with weight positively proportional to the tracking error of the first version controller. We then try to get the per-trajectory tracking result for each trajectory in both the re-sampled dataset and the original dataset taking the joint effort of both the per-trajectory tracking optimization and the tracking data prior provided by the tracking controller. After that, we obtained the second version of the imitation-ready data. We then re-train the trajectory controller with the supervision from action sequences in the dataset. In this stage, we construct the tracking task structure by searching homotopy optimization paths from trajectories in the dataset. We train a tracking task structure prior model which encodes the optimization paths. In the thrid stage, we re-train the tracking controller via labeled trajectories obtained till now. In the third stage, we re-sample a subste from the remaining trajectories. We then try to get per-trajectory tracking result taking the joint effort of per-trajectory tracking optimization, tracking data prior from the tracking controller, and the tracking task structure prior provided by the tracking task structure prior model. Having obtained current action-labeled trajectories, we re-train the tracking controller with the supervision provided by those trajectories. We then take it as the final optimized tracking controller. 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


% propose to build the tracking task structure
% lots of imitation-ready data 
% the tracking controller is trained to mimic 
% with the tracking controller which already has the ability to track various manipulation 
% transfer tracking data prior encoded in the tracking controller to better solve per-trajectory tracking problem. Specifically, to solve the problem of tracking the trajectory $A$, we first try to query the trajectory controller to obtain the tracking result, an action sequence, of $A$. After that, we try to use the sampled action sequence to initialize the per-trajectory tracking 
% can leverage the tracking prior encoded in the tracking 


% \subsection{Per-Trajectory Tracking via Tracking Data Prior}
% \todo{todo} % 



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Solving per-trajectory tracking directly would still make it hard to address the challenge in tracking very difficult trajectories. To improve tracking results in these difficult cases so that we can get more diverse imitation ready data with high quality action sequences in more tracking cases, we design a strategy where we can transfer tracking data prior encoded in the tracking controller to better solve per-trajectory tracking problem. Specifically, to solve the problem of tracking the trajectory $A$, we first try to query the trajectory controller to obtain the tracking result, an action sequence, of $A$. After that, we try to use the sampled action sequence to initialize the per-trajectory tracking 
% can leverage the tracking prior encoded in the tracking 
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


% % \subsection{Homotopy Path Generator Learning} 

% % tracking task structural modeling 

% % % \subsection{Structural Cross-Task Synergy Modeling}
% % \subsection{Structural Task Relational Space Modeling} \label{sec:method_task_space_modeling}
% % \subsection{Curriculum Scheduler Modeling via Mining and Modeling Cross-Trajectory Relations} \label{sec:method_task_space_modeling}
% % \subsection{Curriculum Scheduler Modeling} \label{sec:method_task_space_modeling}
% % \subsection{Learning Curriculum Scheduler via Modeling Tracking Curricula} \label{sec:method_task_space_modeling}
% % \subsection{Modeling Tracking Task Structure via Encoding Beneficial Optimization Paths} \label{sec:method_task_space_modeling}
% \subsection{Tracking Task Structure Modeling via Encoding Homotopy Optimization Paths} \label{sec:method_task_space_modeling}
% Leveraging advanced RL with our dedicated designs for tracking, we can solve lots of per-trajectory tracking problems to a satisfactory extent. However, this strategy still struggles to solve very difficult tracking problems considering the inherent challenges of manipulation tracking. 
% % restrictions of RL. 
% Therefore, we propose to tackle difficult per-trajectory tracking problems via a homotopy method. 
% % a tracking curriculum to tackle difficult per-trajectory tracking problems. 
% % Therefore, we propose a tracking curriculum to tackle difficult per-trajectory tracking problems. 
% Specifically, for the trajectory $A=(\mathbf{H}_A, \mathbf{O}_A)$ that is difficult to track directly, instead of directly tracking the trajectory $A$, we propose to find an optimization path consisting of a series of tracking tasks that can help us solve the tracking problem of $A$. We then try to track $A$ by tracking each trajectory in the path and transfer results to the next in a one-by-one manner. If in this way we can get better tracking results for $A$ than directly solving $A$, we regard it as a good optimization path for $A$. We then further propose to encode such tracking task structure into a model so that it can propose useful optimization paths during the inference time efficiently. To fulfill this vision, we propose to first mine useful optimization paths from trajectories in the training dataset. We then encode the searched optimization paths into a model so that it can provide us with good optimization paths during the inference period efficiently. 
% % inference-time tracking task prior 
% % tracking task structure into a model so that it can provide us with inference-time tracking task structure prior efficiently. 
% % useful optimization paths into a model so that it 

% % and  and track $A$ by 
% % propose to track $A$ 
% % to solve the problem by solving each tracking 
% % the tracking curriculum tries to transfer the optimized action trajectory of a different tracking task, \emph{e.g.,} $B=(\mathbf{H}_B, \mathbf{O}_B)$ to initialize the tracking policy of $A$. If the re-optimized tracker can achieve better results than the original one, such a trail ($B\rightarrow A$) is regarded as a good curriculum. We say trajectory $B$ is the ``parent'' trajectory of $A$. Although the tracking curriculum is provided highly effective in our experiments, finding a good curriculum for solving each hard-to-track problem requires lots of trial and error and is thus very time-inefficient. Therefore, we propose to use a curriculum scheduler to expressively model tracking curricula so that that scheduler can be leveraged to propose tracking curricula efficiently. To fulfill this vision, we propose to first mine such tracking curricula from a batch of data, followed by learning a curriculum scheduler to model such tested effective curricula. 
% % rollout by a successful tracker 
% % We propose a tracking curriculum to tackle difficult per-trajectory tracking problems that continuously challenge RL when being optimized alone. 

% % \noindent\textbf{Constructing the tracking task structure.} 
% \noindent\textbf{Searching optimization paths.}
% Given a trajectory set containing $K$ trajectories, we find useful tracking task optimization paths via breadth-first search. For each trajectory, we first identify its potential parent set via cross-trajectory similarity. Then in each searching iteration, for each task $A$, we try to use the best-tracked action trajectory of each of its parent task $B$ to initialize its tracking policy and see whether the policy optimization could benefit from the initialization. If the tracking result of trajectory $B$ can help us track the trajectory $A$, we regard $B$ as a beneficial parent tracking task of $A$. After several iterations, we identify a set of optimization paths $\mathcal{C} = \{ (T_n, ..., T_1) \} $, where for each  $1\le k \le n-1$, $T_{k+1}$ is the parent task of $T_k$.  
% % we identify a set of parent tasks for each task, which further construct our tracking task structure. 
% % Such relations then form our tracking task structure. 
% % we regard $B\rightarrow A$ 
% % as a valid curriculum. After several iterations, two times in our experiments, we can identify a set of valid tracking curricula $\mathcal{C} = \{ T_p \rightarrow T_c \}$, where $T_p$ denotes the parent trajectory and $T_c$ represents the child trajectory. 
% % we find tracking curricula via breadth-first search. For each trajectory, we first identify its potential parent set via 

% % \noindent\textbf{Searching tracking curricula.} 

% \noindent\textbf{Modeling tracking task structure.}  Having obtained the optimization path set $\mathcal{C}$, we propose to train a model that transform each trajectory $T_c$ to the parent trajectory $T_p$ to model the tracking task structure. 
% % to model the tracking task structure. 
% Considering the fact that a single child trajectory $T_c$ may have multiple different parent trajectories $T_p$, we formulate it as a conditional trajectory generation problem where the goal is to generate the parent trajectories conditioned on the child $T_c$. Therefore, we train a conditional diffusion model to accomplish this vision. Specifically, we first train a trajectory diffusion model $\mathcal{M}$ which models the distribution of all kinematic trajectories. After that, we fine-tune it to a conditional generative model that describes the distribution of the parent trajectory of each child trajectory. By sampling from the conditional generative model, we transform each tracking task to its parent tracking task, \emph{i.e.,} $T_p\sim \mathcal{M}(\cdot \vert T_c)$. We can construct an optimization path by iteratively querying the conditional generative model to sample the parent tasks, \emph{i.e.,} $(T_1, T_2, ..., T_k)$ where $T_{m+1}\sim \mathcal{M}(\cdot \vert T_m)$, $1\le m \le k-1$. 
% % we then complete the trajectory transformation 
% % complete the trajectory transformation problem, \emph{i.e.,} $T_p\sim \mathcal{M}(\cdot \vert T_c)$. 
% % % \noindent\textbf{Learning curriculum scheduler via modeling tracking curricula.}
% % $\mathcal{M}_{\mathcal{C}}: T_c $


% % With our carefully designed reward and observation space, the PPO can effectively optimize a specialist policy that achieves satisfactory performance on easy or medium manipulation tracking tasks. However, it would still struggle with achieving a reasonable solution for very difficult tasks involving intricate hand-object interactions such as subtle in-hand re-orientations and frequent contact shifting. Therefore, to further enhance the single trajectory tracking performance for providing better supervision for the tracking controller's training, we propose to explore trajectory curricula to better solve difficult tracking problems. Specifically, if we can transfer the optimized action sequence from tracking problem $A=(\mathbf{H}_A, \mathbf{O}_A)$ to $B=(\mathbf{H}_B, \mathbf{O}_B)$ as the start point of the task $B$'s optimization (\emph{i.e.,} setting the action sequence $\mathbf{A}_B$ that can successfully solve $B$ as the basic trajectory of $A$ and re-optimize the residual policy), which we also observe can achieve better performance than directly tracking $B$, we regard the problem of tracking $B$ can be benefit from tracking $A$, \emph{i.e.,} saying task $A$ is the ``parent'' trajectory of $B$. We then aim to leverage such cross-trajectory relations to construct a curriculum for each trajectory that is hard to optimize. To efficiently and effectively find such relations, we wish to learn a model that can tell us what's the most suitable ``parents'' for the current trajectory. It is hard to heuristically define a rule-based criterion such as based on the kinematic similarities or object geometry affinities, that exactly refect such relations. Therefore, we formulate this problem as a trajectory transformation and aim at learning  model $\mathcal{M}_{\mathcal{S}}$ from the trajectory dataset $\mathcal{S}$ to solve it. In detail, given a difficult-to-track trajectory $(\mathbf{H}, \mathbf{O})$, we want the model $\mathcal{M}_{\mathcal{S}}$ to transform it to its ``parent'' $(\mathbf{H}^p, \mathbf{O}^p)$. Considering the fact that there may be multiple instances that can serve as the ``parent'' for a single trajectory, the trajectory transformation ought to be a stochastic process where $\mathcal{M}_{\mathcal{S}}$ models the distribution of the parent, \textit{i.e.,} $(\mathbf{H}^p, \mathbf{O}^p) \sim \mathcal{M}_{\mathcal{S}}(\cdot\vert (\mathbf{H}, \mathbf{O}))$.

% % cross-task synergies to improve the training of a difficult task. 

% % To fulfill this vision, we further formulate the learning problem of the model $\mathcal{M}_{\mathcal{S}}$ as a conditional generative model training problem. Specifically, $\mathcal{M}_{\mathcal{S}}$ takes a trajectory $(\mathbf{H}, \mathbf{O})$ as the input and generates its parent trajectory. Therefore, we first train a diffusion model to describe the distribution of all kinematic trajectories contained in the dataset $\mathcal{S}$. After that, to acquire data for tuning it to a conditional generative model for the task transformation purpose, we propose a BFS approach to mine such ``parent-child'' paired data from the dataset. Detailed steps regarding the curriculum searching and the curriculum scheduler's training are deferred to Appendix~\ref{}. 


% %%%%%%%%%%%%%%%% move to appendix %%%%%%%%%%%%%%%
% % The BFS-based approach for constructing the task relational space and the conditional diffusion model training are detailed as follows. 

% % \todo{ add a picture for this structural space construction}

% % \todo{add the retargeted motions such as same trajectory different objects}

% % \todo{details w.r.t. how we identify the cross task affinities? }

% % \todo{extended task set?}

% % \noindent\textbf{Heuristic guided BFS for structural task relational space construction.} Given all manipulation trajectories from the training dataset $\mathcal{S}$, we first construct an expanded task set by augmenting each single task $(\mathbf{H}_A, \mathbf{O}_A, m_A)$ into $(\mathbf{H}_C, \mathbf{O}_A, m_A)$ and $(\mathbf{H}_A, \mathbf{O}_A, m_C)$. The structural task relational space wants to quantify the cross-task relations for each two different tasks in the expanded task set. One can imagine a most ideal way that is trying to test each pair of two manipulation trajectories $(A, B)$ to test whether one task, for instance task $B$ can benefit from task $A$. However, such a strategy is too expensive with the time complexity $\vert \mathcal{S}\vert^2$. If we wish not to be restricted to the first order cross-task relations only and also want to explore the high-order relations, for instance, the current task $C$ can benefit from the optimization result of task $B$ with the initialization of task $A$, the brute-force searching would cost $K\vert \mathcal{S}\vert^2$. Therefore, we adopt a heuristic guided searching strategy to reduce the time consumption to an affordable amount. 

% % Specifically, we first identify potential neighbours for each task via trajectory similarities and object geometry similarities. Having restricted the number of potential neighbours to a reasonable value, \emph{i.e.,} $K_{\text{nei}}$, we only conduct the affinity test between each potential neighbours. With this strategy, we can reduce the timeconsumption to $K K_{\text{nei}} \vert \mathcal{S}\vert$. By tuning $K$ and $K_{\text{nei}}$, we can balance the accuracy and the efficiency of the task relation space construction process. Please refer to the Supp. for more details. 
% % % Besides, it may overlook the high-order task relations, where the current task $C$ can benefit from the optimization result of task $B$ with the initialization of task $A$.  

% % % Specifically, \textcolor{red}{(TODO: add the description for the BFS part)}

% % \noindent\textbf{Conditional diffusion model for task relation modeling.} 
% % \todo{how we obtain the pairs to train the diffusion model -- both the high order and the first order} 
% % \todo{how to train the model}

% % \todo{re-check this part}
% %%%%%%%%%%%%%%%% move to appendix %%%%%%%%%%%%%%%

% % \todo{details w.r.t. how to improve per-trajectory tracker via tracking prior and the curriculum scheduler}


% % \textcolor{red}{(TODO: pharse it into a specialist-generalist-specialist iteration? )}
% % \noindent\textbf{Progressive generalist model training and labeled dataset improvement.}
% % \subsection{Progressive Generalist Model Training via ``Specialist-Generalist-Specialist'' Iterations} \label{sec:method_sgs}

% % \subsection{Progressively Improving the Tracking Controller via Iterative Optimization} \label{sec:method_sgs}

% \subsection{Iterative Optimization for Progressively Improving Tracking Controller} \label{sec:method_sgs}
% Based on the above designs, we then propose an iterative approach where the tracking controller is progressively improved by alternately executing the following two steps: 1) improving per-trajectory trackers with the joint effort of per-trajectory tracking optimization, data prior from the tracking controller, and the homotopy optimization path from the tracking task structure prior model; 2) training better tracking controller with the supervision from the improved imitation-ready data with action sequences. 
% % improving the action-labeled dataset by training better per-trajectory trackers on more trajectories leveraging the power of tracking prior provided by the general tracking controller and the optimization path from the tracking task structure prior; 2) training better tracking controller via supervision from the improved action-labeled dataset. 
% % the trajectory curriculum proposed by the curriculum scheduler; 

% % \noindent\textbf{Improving per-trajectory trackers via tracking prior and curriculum scheduling.} 
% \noindent\textbf{Improving per-trajectory trackers via tracking data prior and task structure prior.} The general tracking controller and the tracking task structure prior offer us two ways to improve the per-trajectory tracking result of a specific trajectory that is difficult to track, denoted as $A$. Firstly, we sample the action sequence from the general tracking controller for the current tracking task. We then initialize the basic trajectory as the sampled action sequence and re-train the residual policy to try to get better tracking results. Secondly, we let the tracking task structure prior model propose an optimization path consisting of a series of tracking tasks. After that, we solve the tracking problem of $A$ by gradually solving each task in the optimization path and transferring the results to the next one. 
% % tracker of a specific trajectory. Firstly, we can sample 
% % The sampled action sequence is init %
% % % , \emph{e.g.,} $A$. 
% % Firstly, we can use the action trajectory sampled from the general tracking controller on the current trajectory as the basic trajectory in its policy. By re-optimizing the residual policy, we may get better results than the original optimization. Secondly, by transforming trajectory $A$ into its parent trajectory $B$ via the curriculum scheduler and using the successful rollout action trajectory on the trajectory $B$ as the basic trajectory of $A$'s tracking policy, we re-optimize the residual policy for $A$ to try to get a better tracker for $A$. 
% % the curriculum scheduler
% % curriculum scheduler %

% \noindent\textbf{Iteratively improving tracking controller and the action-labeled dataset.} We then adopt an interactive approach to gradually get a strong tracking controller. Specifically, our method is composed of three stages. In the first stage, we sample a small amount of data from the whole training dataset. By optimizing the per-trajectory tracker for each trajectory in the dataset, we get the first version of imitation-ready data containing trajectories with high-quality action sequences. After that, the labeled dataset is leveraged to train the tracking controller via the imitation-augmented RL. Then in the second stage, we additionally resample a trajectory dataset from the remaining training trajectories with weight positively proportional to the tracking error of the first version controller. We then try to get the per-trajectory tracking result for each trajectory in both the re-sampled dataset and the original dataset taking the joint effort of both the per-trajectory tracking optimization and the tracking data prior provided by the tracking controller. After that, we have obtained the second version of the imitation ready data. We then re-train the trajectory controller with the supervision from action sequences in the dataset. In this stage, we construct the tracking task structure by searching homotopy optimization paths from trajectories in the dataset. We train a tracking task structure prior model which encodes the optimization paths. In the thrid stage, we re-train the tracking controller via labeled trajectories obtained till now. In the third stage, we re-sample a subste from the remaining trajectories. We then try to get per-trajectory tracking result taking the joint effort of per-trajectory tracking optimization, tracking data prior from the tracking controller, and the tracking task structure prior provided by the tracking task structure prior model. Having obtained current action-labeled trajectories, we re-train the tracking controller with the supervision provided by those trajectories. We then take it as the final optimized tracking controller. 
% % The tracking controller 
% % by encoding such optimization paths. 
% % The resampled dataset is leveraged the action-labeled dataset, \emph{i.e.,} $\mathcal{A}_1$. After that, the labeled dataset is leveraged to train the tracking controller. With the trained general controller, we evaluate it on all training tasks. Then in the second stage, we additionally resample a trajectory dataset from the remaining training trajectories where the weight is positively proportional to the tracking error of the first version controller. The resampled dataset is then labeled by optimizing their per-trajectory trackers. 
% % Combining these optimized trajectories and labeled trajectories in the first stage, we obtain the second version of the labeled dataset $\mathcal{A}_2$. The tracking controller is then trained via the supervision provided by $\mathcal{A}_2$. We mine trajectory curricula to improve tracking results of the currently selected trajectory. With the resulting tracking curricula, we then train a curriculum scheduler. In the third stage, we re-sample a subset from the remaining trajectories and use the tracking controller as well as the curriculum scheduler to label them, resulting in the third version of labeled dataset $\mathcal{A}_3$. After that, we re-train the tracking controller with supervision provided by $\mathcal{A}_3$. 
% % In the third stage, 
% % with the re-trained controller, we then construct the 
% % task relations space and try to improve the single trajectory optimization results leveraging the task relations space and the knowledge encoded in the generlaist model. Denote the improved labeled dataset as $\mathcal{A}_3$, the final controller is then trained with the supervision guidance from $\mathcal{A}_3$. 
% % % a specialist policy for each single trajectory in the initially sampled small dataset, we obtain the first version of the labeled dataset $\mathcal{A}_1$. 


% % \begin{itemize}
% %     \item 
% % \end{itemize}

% % Despite the effectiveness of the above single trajectory specialist policy optimization strategy leveraging RL and the task curriculum, it is infeasible for us to optimizing out a tracking policy for each individual trajectory in the training dataset considering the unaffordable computation and time consumption. Moreover, the task relation modeling may fail to explore valuable cross-trajectory relations when we only have a limited number of labeled data. Therefore the single trajectory optimization may still not that perfect due to the under-explored task relations. To leverage high-quality labeled data for training the tracking controller and to balance the amount of the labeled data with the time consumption, we propose a progressive policy training and labeled data improvement strategy. Specifically, the model is gradually trained via three stages. In the first stage, we sample a small amount of data from the whole training dataset. By optimizing a specialist policy for each single trajectory in the initially sampled small dataset, we obtain the first version of the labeled dataset $\mathcal{A}_1$. After that, the labeled dataset is leveraged to train the tracking controller via the above imitation augmented RL training strategy. With the trained tracking controller, we evaluate it on all training tasks. Then in the second stage, we additionally resample a trajectory dataset from the remaining training trajectories where the weight is positively proportional to the tracking error of the first version policy. The resampled dataset is then labeled via the specialist training. Combining theses optimized trajectories and labeled trajectories in the first stage, we obtain the second version of the labeled dataset $\mathcal{A}_2$. The tracking controller is then trained via the additional supervision provided by $\mathcal{A}_2$. In the third stage, with the re-trained controller, we then construct the task relations space and try to improve the single trajectory optimization results leveraging the task relations space and the knowledge encoded in the generlaist model. Denote the improved labeled dataset as $\mathcal{A}_3$, the final controller is then trained with the supervision guidance from $\mathcal{A}_3$. 




% % \subsection{Structural Cross-Task Synergy Modeling}







% % Following the above specialist policy training strategy, we can optimize a trajectory specific policy the inferred action sequence for each single manipulation trajectory. With such action-labeled trajectories $\mathcal{A}$, we train the generalist policy together with the imitation supervision provided by the action labeled trajectories and the RL. 

% % for each single manipulation trajectory


% % To efficiently optimizing single action-labeled trajectory with minimal hand tuning effort, we wish for a single suit of reward and observation design with which we can effectively optimize for a specialist policy for each single single trajectory. 
% % However, considering the inherent difficulties underlying the dexterous manipulation tracking, the RL algorithm may struggle with tracking some very difficult manipulation trajectories, \textit{e.g.,} with difficult object movements such as subtle re-orientations or involving grasping extremely thin objects. Therefore, we propose to ease the difficult single trajectory optimization problem by transferring knowledge from other trajectories. Specifically, we propose to explore cross-trajectory relations. That is, for each single trajectory that is hard to track, we aim to find its ``parent tasks'' whose optimization results can benefit its optimization. 
% % % employ model-free approach 


% % \textcolor{red}{(TODO: in this paragraph, the focus is how to train the specialist policy -- we can directly use the specialist policy (exactly the term) here )}

% % \subsection{Dexterous Manipulation Tracking via RL and Automatic Task Curriculum}

% % \textcolor{red}{(TODO: move the MDP and the model-free approach training and learning --- the model-free approach, the model unknown assumption, the MDP formulation to the beginning of the paragraph)}
% % % Automatic task
% % Considering the intricate dynamics involved in the dexterous manipulation resulting from the high DoF of a dexterous hand and the rich and changing hand-object contacts, we employ a model-free approach for tackle the problem. 
% % \textcolor{red}{(TODO: add the MDP formulation that frequently appear in other papers)}
% % Given a single kinematic manipulation trajectory $(\mathbf{H}, \mathbf{O})$, the trajectory specific specialist training aims at optimizing a tracking policy $\mathbf{\pi}(\cdot\vert\cdot )$ that can control a dexterous robot hand to track the reference kinematic trajectory. Assuming the physical model is unknown and non-differentiable, we leverage reinforcement learning to optimize the policy. To efficiently optimizing single action-labeled trajectory with minimal hand tuning effort, we wish for a single suit of reward and observation design with which we can effectively optimize for a specialist policy for each single single trajectory. However, considering the inherent difficulties underlying the dexterous manipulation tracking, the RL algorithm may struggle with tracking some very difficult manipulation trajectories, \textit{e.g.,} with difficult object movements such as subtle re-orientations or involving grasping extremely thin objects. Therefore, we propose to ease the difficult single trajectory optimization problem by transferring knowledge from other trajectories. Specifically, we propose to explore cross-trajectory relations. That is, for each single trajectory that is hard to track, we aim to find its ``parent tasks'' whose optimization results can benefit its optimization. \textcolor{red}{(TODO: rephrase this sentence, given xxx , we aim at xxx )}
% % Specifically, we propose a BFS based searching strategy that can automatically explore the trajectory relational space, quantifying relations between each trajectory pair from the datasets. After acquiring cross-trajectory relations, we propose to distill the explored trajectory relational space into a trajectory relational model. The trajectory relational model can then serve as a task relational prior model for providing knowledge w.r.t. cross-trajectory affinities. Then for each hard to track trajectory, we can leverage this model to transform it into a different trajectory, saying  the ``parent trajectory''. By first tracking the ``parent trajectory'' and then optimizing based on the tracked sequence of the parent trajectory, we can better track the original difficult trajectory finally. 



% % \noindent\textbf{Specialist policy training.} Our single trajectory specialist training scheme leverages PPO to optimize for a tracking policy $\mathbf{\pi}(\cdot\vert \cdot)$. The policy takes the current observation, comprising the current hand and object state information and the next frame tracking target as input and output the hand action for the hand to achieve the next state. Specifically, at each timestep $t$, we calculate the observation $\mathbf{o}\mathbf{b}\mathbf{s}_t$ as follows: 
% % \begin{align}
% %     \mathbf{o}\mathbf{b}\mathbf{s}_t = \{ \hat{\mathbf{h}}_t, \hat{\mathbf{o}}_t, \hat{\dot{\mathbf{h}}}_t, \hat{\dot{\mathbf{o}}}_t, \mathbf{f}_t,  \mathbf{h}_{t+1}, \mathbf{o}_{t+1}, \mathbf{h}_{t+1} - \hat{\mathbf{h}}_t, \mathbf{o}_{t+!} \ominus \hat{\mathbf{o}}_t   \}, 
% % \end{align}
% % where $\hat{\mathbf{h}}_t$ and $\hat{\mathbf{o}}_t$ are observed hand joint DoF positions and the object orientation as well as positions at the timestep $t$, $\hat{\dot{\mathbf{h}}}_t$ and $\hat{\dot{\mathbf{o}}}_t$ are hand DoF velocities and object linear as well as angular velocities respectively, $\mathbf{f}_t$ denotes the hand finger positions in the world space, $\mathbf{h}_{t+1}$ and $\mathbf{o}_{t+1}$ are the kinematic targets of the hand DoF positions and the object state in the next timestep. 

% % The reward consists a hand tracking reward $r_{h}$, object tracking reward $r_o$, and the hand-object affinity reward $r_d$. Specifically, xxx 
% % \textcolor{red}{(TODO: add the reward design here)}
% % \textcolor{red}{(TODO: add the residual policy here --- it is important since we need to justify the task curriculum here)}
    
% % % the observation $\mathbf{o}_t$ 


% % % \noindent\textbf{Trajectory relational space exploration via BFS.} 
% % \noindent\textbf{Trajectory relational space exploration and modeling.} 
% % We propose an automatic task curriculum to tackle difficult manipulation trajectories that are hard to track directly. Treating each single trajectory tracking problem as a task, for a tough task, the task curriculum tries to find its affinity tasks and transferring the optimized control trajectories for these tasks as the initialization for the current task to improve the tracking performance. \textcolor{red}{(TODO: add details here)} Such an approach requires us to find affinity tasks, which act as ``parent tasks'' for the current hard to solve problem. The current difficult task should be able to benefit from the optimization results of the ``parent tasks''. It is hard to heuristically define a rule-based criterion such as based on the kinematic similarities or object geometry affinities, that exactly refect such relations. Therefore, we formulate this problem as a trajectory transformation and aim at learning  model $\mathcal{M}_{\mathcal{S}}$ from the trajectory dataset $\mathcal{S}$ to solve it. In detail, given a difficult task, described as a kinematic trajectory $(\mathbf{H}, \mathbf{O})$, we want the model $\mathcal{M}_{\mathcal{S}}$ to transform it to its ``parent task'', described via a kinematic trajectory $(\mathbf{H}^p, \mathbf{O}^p)$ as well. Considering the fact that there may be multiple tasks that can serve as the ``parent task'' for a single difficult task, the trajectory transformation ought to be a stochastic process where $\mathcal{M}_{\mathcal{S}}$ models the distribution of the parent, \textit{i.e.,} $(\mathbf{H}^p, \mathbf{O}^p) \sim \mathcal{M}_{\mathcal{S}}(\cdot\vert (\mathbf{H}, \mathbf{O}))$.

% % To fulfill this vision, we further formulate the learning problem of the model $\mathcal{M}_{\mathcal{S}}$ as a conditional generative model training problem. Specifically, $\mathcal{M}_{\mathcal{S}}$ takes a trajectory $(\mathbf{H}, \mathbf{O})$ as the input and generates its parent tasks. Therefore, we first train a diffusion model to describe the distribution of all kinematic trajectories contained in the dataset $\mathcal{S}$. After that, to acquire data for tuning it to a conditional generative model for the task transformation purpose, we propose a BFS approach to mine such ``parent-child'' paired data from the dataset. Specifically, \textcolor{red}{(TODO: add the description for the BFS part)}




% % % Given a tracking task $(\mathbf{H}, \mathbf{O})$, 
% % % model outhgt to model a distribution of the parent.
% % % we aim at learning a model $\mathcal{M}_{\mathcal{S}}$ from the trajectory dataset $\mathcal{S}$ 
% % % We therefore propose a BFS based approach for mining cross-task relations for constructing the task relational space from the given trajectory dataset $\mathcal{S}$. 
% % % BFS based approach for mining cross-task relations 

% % % the task curriculum tries to fi


% % % \noindent\textbf{Trajectory relation modeling via conditional diffusion.}  

% % % aiming to find ``parent tasks'' 
% % % with difficult object movements such as subtle re-orientations or involving grasping thin objects, 
% % % Considering the vision to efficiently optimizing single action-labeled trajectory with minimal hand tuning effort, we wish for a single suit of reward and observation design for each trajectory. 

% % % For each manipulation reference trajectory, we wish to d


% % % Given the unknown model assumption, we leverage reinforcement learning to optimize the policy. 


% % % For single trajectory specific specialist policy training, 


% % \subsection{General Tracking Policy via Imitation Augmented RL}
% % With the carefully designed specialist policy training strategy and the task relational space exploration and modeling scheme, it is possible for us to optimize for a satisfactory result for each single manipulation trajectory tracking task. However, it is still not enough to acquire a generalist that can not only achieve good performance in each sequence but even generalize to unseen trajectories. Simply following the above RL scheme and training a single policy on multiple tasks is not feasible as demonstrated in our early experiments. As the number of training tasks increases, the RL would even struggle with effectively optimizing a strong policy that can solve every training task, letting alone generalize to unseen trajectories and resolve the novel manipulation sequence tracking challenge. We suppose such pool fitting phenomenon comes from the inherent deficiency of RL in solving multiple tasks. We therefore propose to introduce labeled single trajectory optimization results into the training process of RL, \textit{i.e.,} providing additional action labels as the supervision signals to train the policy together with RL. Specifically, given an action-labeled  dataset $\mathcal{S}_L=\{ (\mathbf{H}_L, \mathbf{O}_L, \mathbf{A}_L) \}$, where each trajectory is augmented with an additional action sequence $\mathbf{A}$, we add an additional action supervision loss to train the policy together with RL, trying to distill optimized knowledge to the policy. Formally, for each labeled trajectory $(\mathbf{H}_L, \mathbf{O}_L, \mathbf{A}_L)$, the following action supervised loss is additionally added on the policy's prediction in each timestep $(\mathbf{\mu}_t, \mathbf{\sigma}_t) = \mathbf{\pi}(\cdot \vert \hat{\mathbf{s}}_t)$ to train the policy:
% % % to train the policy $\mathbf{\pi}(\cdot\vert \cdot)$  
% % \begin{align}
% %     \mathcal{L}_a = \Vert \mathbf{\mu}_t -  \mathbf{a}_t \Vert, 
% % \end{align}
% % where $\mathbf{a}_t$ denotes $t-$th step action taken from the optimized action trajectory. This strategy tries to bias the current trajectory towards the optimized successful trajectory in the dataset, in a similar flavor to DART~\citep{laskey2017dart}. 
% % % the action superv ised loss -- 



% % \textcolor{red}{(TODO: pharse it into a specialist-generalist-specialist iteration? )}
% % \noindent\textbf{Progressive generalist model training and labeled dataset improvement.}
% % Despite the effectiveness of the above single trajectory specialist policy optimization strategy leveraging RL and the task curriculum, it is infeasible for us to optimizing out a tracking policy for each individual trajectory in the training dataset considering the unaffordable computation and time consumption. Moreover, the task relation modeling may fail to explore valuable cross trajectory relations when we only have a limited number of labeled data. Therefore the single trajectory optimization may still not that perfect due to the under-explored task relations. To leverage high-quality labeled data for training the generalist policy and to balance the amount of the labeled data with the time consumption, we propose a progressive policy training and labeled data improvement strategy. Specifically, the model is gradually trained via three stages. In the first stage, we sample a small amount of data from the whole training dataset. By optimizing a specialist policy for each single trajectory in the initially sampled small dataset, we obtain the first version of the labeled dataset $\mathcal{A}_1$. After that, the labeled dataset is leveraged to train the generalist policy via the above imitation augmented RL training strategy. With the trained generalist, we evaluate it on all training tasks. Then in the second stage, we additionally resample a trajectory dataset from the remaining training trajectories where the weight is positively proportional to the tracking error of the first version policy. The resampled dataset is then labeled via the specialist training. Combining theses optimized trajectories and labeled trajectories in the first stage, we obtain the second version of the labeled dataset $\mathcal{A}_2$. The generalist is then trained via the additional supervision provided by $\mathcal{A}_2$. In the third stage, with the re-trained generalist, we then construct the task relations space and try to improve the single trajectory optimization results leveraging the task relations space and the knowledge encoded in the generlaist model. Denote the improved labeled dataset as $\mathcal{A}_3$, the final generalist model is then trained with the supervision guidance from $\mathcal{A}_3$. 




% % % maximize the potential benefit of the labeled data for training the generalist tracking policy and to 
% % % To tackle theses issues and 
% % % o fully exploit the potential of leveraging labeled 
% % % have a limited number of labeled dataset
% % % task relation modeling may fail to explore valuable trajectory 




% % % The RL would even struggle to optimize an effective policy that can solve every task when the size of the training task set
% % % us to obtain good result in each single trajectory tracking 


% % % for enhancing the multi-task performance of RL to learn a universal 

% % % each manipulation trajectory 
% % % that can output a physically plausible control sequence $\mathbf{A}$ for each manipulation trajectory $(\mathbf{H}, \mathbf{O})$ which is able to drive a dexterous robot 

