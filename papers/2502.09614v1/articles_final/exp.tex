
\vspace{-5pt}
\section{Experiments} \label{sec:exp}
\vspace{-5pt}
% TODO: overview part




% and adaptability 
% We conduct extensive experiments to demonstrate the effectiveness, generalization ability, and robustness, of our tracking controller. We evaluate our method on two HOI datasets containing complex functional daily manipulation featured by difficult trajectories with non-trivial object movements and subtle in-hand re-orientations, challenging object geometries, as well as rich and changing contacts. 
% We train our model in the Isaac Gym environment and conduct evaluations both in the simulator and the real world (see Section~\ref{sec:exp_setting}). We compare strong baselines with dedicated designs for dexterous manipulations to demonstrate the superiority of our method. Our tracking controller can track diverse, difficult, and novel hand-object interactions involving challenging and functional movements, extremely thin objects, as well as rich and changing contacts (see Section~\ref{sec:exp_results}).  However, previous approaches either fail to generalize or behave unsatisfactorily in challenging cases. On average, we can boost the tracking success rate by 10\% from the previously best-performed method. We make further analysis of our method on its robustness and the generalization ability to out-of-distribution test cases (see Section~\ref{sec:exp_analysis}) to illustrate its capability and utility (see Section~\ref{sec:exp_analysis}).  

% We conduct extensive experiments to evaluate the effectiveness, generalization ability, and robustness of our tracking controller. Our method is tested on two HOI datasets featuring complex, functional, daily manipulation tasks. We conduct both through experiments both in the simulation and evaluations in the real world (see Section~\ref{sec:exp_setting}). We compare our method against strong baselines to demonstrate its superiority. Our tracking controller successfully handles novel, difficult, and intriguing manipulations, including those involving intricate functional movements, extremely thin objects, and dynamic contacts (see Section~\ref{sec:exp_results}). In contrast, previous approaches fail to generalize well. On average, our method improves the tracking success rate by more than 10\% over the best prior methods across both datasets. We further analyze our controller's robustness towards significant kinematic noise such as unrealistic states and large penetrations (see Section~\ref{sec:exp_analysis}). 
We conduct extensive experiments to evaluate the effectiveness, generalizability, and robustness of our tracking controller. Tested on two HOI datasets featuring complex daily manipulation tasks, our method is assessed through both simulation and real-world evaluations (see Sec.~\ref{sec:exp_setting}). We compare our approach to strong baselines, showing its superiority. Our controller successfully handles novel manipulations, including intricate movements, thin objects, and dynamic contacts (see Sec.~\ref{sec:exp_results}), while previous methods fail to generalize well. On average, our method improves the tracking success rate by over 10\% compared to the best prior methods. Additionally, we analyze its robustness to significant kinematic noise, such as unrealistic states and large penetrations (see Sec.~\ref{sec:exp_analysis}).
% showcasing its versatility and practical utility.
% , as well as the generalization capabilities towards out-of-distribution test cases (see Section~\ref{sec:exp_analysis}), 
% , specifically designed for dexterous manipulations, 
% Empowered by our proposed training pipeline, we demonstrate that our tracking controller can effectively generalize to novel and challenging interaction trajectories involving unseen object geometry and unobserved manipulations. 
% , demonstrating strong capability and robustness. 


% , which are crucial properties that we expect for a general controller, to demonstrate its capability and utility (see Section~\ref{sec:exp_analysis}). 
% With a single policy, we 





% \textcolor{red}{TODO: track some interesting and long motions?}

\vspace{-10pt}
\subsection{Experimental Settings} \label{sec:exp_setting}
\vspace{-5pt}


% \todo{add the retargeting to the appendix} 

\noindent\textbf{Datasets.}
We test our method on two public human-object interaction  datasets:
% Our dexterous robot hand-object manipulation dataset is built by retargeting two public human-object interaction datasets: 
GRAB~\citep{taheri2020grab}, featuring daily interactions, and 
% single-hand interaction and 
TACO~\citep{liu2024taco}, containing functional tool-use interactions. 
% (single-hand interactions) and TACO~\citep{liu2024taco} (functional tool-use interactions). 
In simulation, we use the Allegro hand, with URDF adapted from IsaacGymEnvs~\citep{makoviychuk2021isaac}, and in real-world experiments, the LEAP hand~\citep{Shaw2023LEAPHL}, due to hardware constraints. Human-object interaction trajectories are retargeted to create robot hand-object sequences using PyTorch\_Kinematics~\citep{Zhong_PyTorch_Kinematics_2024}. We fully retargeted the GRAB and TACO datasets, producing 1,269 and 2,316 robot hand manipulation sequences, respectively. The evaluation on GRAB focus on testing the model’s generalization to unseen interaction sequences. 
Specifically, we use sequences from subject \texttt{s1} (197 sequences) as the test data while the remaining trajectories as the training set. 
% sequences from subject \texttt{s1} (197 sequences) are used as the test dataset. 
% while the training dataset includes the remaining sequences from other subjects. 
For the TACO dataset, we follow the generalization evaluating setting suggested by the authors~\citep{liu2024taco} and split the dataset into a training set with 1,565 trajectories and four distinct test sets at different difficulty levels. The primary quantitative results are reported on the first-level set. More details are provided in Appendix~\ref{sec:supp_exp_details}. 
% and results are provided in Appendix~\ref{sec:supp_exp_details}. 
% More details on the dataset splitting and our results on remaining test sets are provided in Appendix~\ref{sec:supp_exp_details}. 
% to assess generalization at different levels. 
% evaluation setting suggested by the authors~\citep{liu2024taco}. The dataset is split into a training set with 

% GRAB focuses on testing generalization to unseen sequences, while TACO evaluates generalization across four distinct test sets. More details on dataset splitting and results are provided in Appendix~\ref{sec:supp_exp_details}.

% Our dexterous robot hand-object manipulation dataset is constructed by retargeting two public human-object interaction datasets: GRAB~\citep{taheri2020grab}, which focuses on single-hand interactions with everyday objects, and TACO~\citep{liu2024taco}, which features functional tool-use interactions. For simulation experiments, we use the Allegro hand, while in real-world experiments, the LEAP hand~\citep{Shaw2023LEAPHL} is employed due to constraints in the hardware resources. In simulation, the URDF file for the Allegro hand is adapted from IsaacGymEnvs~\citep{makoviychuk2021isaac}, while the LEAP hand URDF is adapted from dex\_urdf~\citep{bunnyvisionpro}. We retarget human-object interaction (HOI) trajectories to simulated robot hand to create the robot hand-object interaction sequences. We use the forward kinematics implemented in PyTorch\_Kinematics~\citep{Zhong_PyTorch_Kinematics_2024} in the retargeting method. Further details are provided in Appendix~\ref{sec:supp_method}. We fully retargeted the GRAB and released TACO datasets, yielding 1,269 and 2,316 robot hand manipulation sequences, respectively. The GRAB dataset evaluation focuses on testing the model’s generalization to unseen interaction sequences, given the large variations in object geometries. Specifically, sequences from subject \texttt{s1} (197 sequences) are used as the test dataset, while the training dataset includes the remaining sequences from other subjects. For the TACO dataset, we follow the generalization evaluation setting suggested by the authors~\citep{liu2024taco}. The dataset is split into a training set with 1,565 trajectories and four distinct test sets to assess generalization at different levels. The primary quantitative results are reported on the first-level test set. More details on the dataset splitting and our results on remaining test sets are provided in Appendix~\ref{sec:supp_exp_details}. 

% The forward kinematics algorithm in  the retargeting method is
% We describe these hands in simulation using URDF files from dex\_urdf~\citep{bunnyvisionpro} and retarget human-object interaction (HOI) trajectories to both hands, generating corresponding kinematic trajectories through PyTorch\_Kinematics~\citep{Zhong_PyTorch_Kinematics_2024}. 

% the test set S0. More details on the dataset splitting and visualizations are provided in Appendix~\ref{sec:supp_exp_details}. 
% : 1) S0: Both the tool object geometries and interaction triplets are seen during training; 2) S1: Novel tool geometries, but the interaction triplets are seen during training; 3) S2: Novel interaction triplets, but familiar object categories and geometries; 4) S3: Both object geometries and interaction triplets are novel. The primary quantitative results are reported on the test set S0. More details on the dataset splitting and visualizations are provided in Appendix~\ref{sec:supp_exp_details}. 

% Our dexterous robot hand-object manipulation dataset is created by retargeting two public human-object datasets, namely GRAB~\citep{taheri2020grab}, containing single-hand interactions with daily objects, and TACO~\citep{liu2024taco}, featured by functional tool using interactions. We use simulated Allegro hand in our simulation experiments while LEAP hand~\citep{Shaw2023LEAPHL} in the real world experiments due to the hardware constraints. We use URDF files from dex\_urdf~\citep{bunnyvisionpro} as their descriptions in the simulation. We retarget HOI trajectories to these two hands to create their corresponding kinematic trajectories leveraging PyTorch\_Kinematics~\citep{Zhong_PyTorch_Kinematics_2024}. 
% For details of the retargeting method that we leverage in our method, please refer to Appendix~\ref{sec:supp_method}. 
% We retarget the full GRAB dataset and the full released TACO dataset, obtaining 1269 and 2316 robot hand manipulation sequences respectively. The evaluation on GRAB mainly aims at testing the model's generalization ability toward unseen interaction sequences considering the large distinctions between their object geometries. Specifically, we use sequences of subject \texttt{s1}, with  197 sequences in total, as the test dataset.  The training dataset is constructed by remaining sequences from other subjects. 
% For the TACO dataset, we create one training set with four distinct test sets with different generalization levels for a detailed evaluation of the model's generalization performance, as suggested by authors~\citep{liu2024taco}. Specifically, the whole dataset is split into a training dataset, containing 1565 trajectories, and the test set S0 where both the tool object geometries and the interaction triplets are seen during training, the test set S1 where the tool geometry is novel but the interaction triplets are seen during training, the test set S2 with novel interaction triplets but seen object categories and geometries, and the test set S4 with new object geometries and interaction triplets. Main quantitative results are reported on the test set S0.  Details w.r.t. the dataset splitting and visualizations are deferred to Appendix~\ref{sec:supp_exp_details}. 
% robot hand manipulation sequences and 2316 sequences
% obtaining 1269 manipulation sequences in total, and the full released TACO data which contains 2316 sequences in total. 
% For TACO dataset, to balance the time consumption and the final data amount, we randomly select 1800 sequences from 2316 sequences in total for retargeting.


% create four different test sets to evaluate the generalization 
% For GRAB, we mainly evaluate the model's generalization ability towards novel interaction sequences considering the large distinctions between their object geometries. For 
% To evaluate the model's generalization capability towards novel manipulation trajectories, we further split them into train and test splits. For the GRAB dataset, we use sequences of the \texttt{subject s1}, with 197 sequences in total, as the test dataset. The train dataset is constructed by remaining sequences from other subjects. For the TACO dataset, we 190 trajectories randomly selected from 1800 sequences for test, while the remaining trajectories as the training data. 
% \todo{unboserved objects? a different splitting method for TACO that considers the object category?}  For evaluating the model's generalization ability towards unseen geometry, we additionally construct a subset from the TACO's training dataset and a subset from TACO's test dataset as a new test split. \todo{details about that}
% For the GRAB datset, although there are existing previous splitting routines~\citep{Zhou2022TOCHSO,Liu2024GeneOHDT}, they mainly focus on cross object generalization. 








% so many todos 
\vspace{-5pt}
\noindent\textbf{Metrics.} 
We introduce five metrics to evaluate the tracking accuracy and task success: 1) Per-frame average object rotation error: \( R_{\text{err}} = \frac{1}{N+1} \sum_{n=0}^N \text{Diff\_Angle}(\mathbf{q}_n, \hat{\mathbf{q}}_n) \), where \( \hat{\mathbf{q}}_n \) and  \( \mathbf{q}_n \)  are reference and tracked orientation.
2) Per-frame average object translation error: \( T_{\text{err}} = \frac{1}{N+1} \sum_{n=0}^N \Vert \mathbf{t}_n - \hat{\mathbf{t}}_n \Vert \), where \( \mathbf{t}_n \) and \( \hat{\mathbf{t}}_n \) are the tracked and reference translations. 3) Per-frame average wrist position and rotation error: \( E_{\text{wrist}} = \frac{1}{N+1} \sum_{n=0}^N \left( 0.5 \, \text{Diff\_Angle}(\mathbf{q}_n^{\text{wrist}}, \hat{\mathbf{q}}_n^{\text{wrist}}) + 0.5 \Vert \mathbf{t}_n^{\text{wrist}} - \hat{\mathbf{t}}_n^{\text{wrist}} \Vert \right) \), where \( \mathbf{q}_n^{\text{wrist}} \) and \( \hat{\mathbf{q}}_n^{\text{wrist}} \) are wrist orientations, and \( \mathbf{t}_n^{\text{wrist}} \) and \( \hat{\mathbf{t}}_n^{\text{wrist}} \) are translations. 4) Per-frame per-joint average position error: \( E_{\text{finger}} = \frac{1}{N+1} \sum_{n=0}^N \left( \frac{1}{d} \Vert \mathbf{\theta}_n^{\text{finger}} - \hat{\mathbf{\theta}}_n^{\text{finger}} \Vert_1 \right) \), where $\mathbf{\theta}$ denotes finger joint positions, \( d \) is the degrees of freedom.
% (16 for both Allegro and LEAP hands).  
% and \( \Vert \cdot \Vert_1 \) denotes the 1-norm. 
5) Success rate: A tracking attempt is successful if \( T_{\text{err}} \), \( R_{\text{err}} \), and \( 0.5E_{\text{wrist}} + 0.5E_{\text{finger}} \) are all below the thresholds. Success is calculated with two thresholds: \( 10\text{cm} \)-\(20^\circ\)-\(0.8\) and \( 10\text{cm} \)-\(40^\circ\)-\(1.2\).
% object tracking quality, hand tracking accuracy, and 
% We introduce five metrics to evaluate the quality of object tracking, hand tracking accuracy, and overall success in the tracking task: 
% 1) Per-frame average object rotation error: $R_{\text{err}} = \frac{1}{N} \sum_{n=1}^N \text{Diff\_Angle}(\mathbf{q}_n, \hat{\mathbf{q}}_n) $, where $\mathbf{q}_n$ is the ground-truth orientation, $\hat{\mathbf{q}}_n$ is the tracked result, represented in quaternion, $\text{Diff\_Angle}(\cdot, \cdot)$ computes the angular difference between two quaternions.
% 2) Per-frame average object translation error: $T_{\text{err}} = \frac{1}{N} \sum_{n=1}^N \Vert \mathbf{t}_n - \hat{\mathbf{t}}_n \Vert$, where $\mathbf{t}_n$ and $\hat{\mathbf{t}}_n$  represent the ground-truth and tracked translations, respectively.
% 3) Per-frame average wrist position and rotation error: $E_{\text{wrist}}=\frac{1}{N}  \sum_{n=1}^N (0.5 \text{Diff\_Angle}(\mathbf{q}_n^{\text{wrist}}, \hat{\mathbf{q}}_n^{\text{wrist}})  + 0.5 \Vert \mathbf{t}_n^{\text{wrist}} - \hat{\mathbf{t}}_n^{\text{wrist}} \Vert )$, where $\mathbf{q}_n^{\text{wrist}}$ and $\hat{\mathbf{q}}_n^{\text{wrist}}$ represent the orientation of the wrist in the reference motion and the tracking result, respectively, while $\mathbf{t}^{\text{wrist}}_n $ and $\hat{\mathbf{t}}_n^{\text{wrist}}$ denote the wrist translation in the reference trajectory and the tracking result, respectively. 
% 4) Per-frame per-joint average position error: 
% $E_{\text{finger}}= \frac{1}{N}  \sum_{n=1}^N (\frac{1}{d} \Vert \mathbf{\theta}_n^{\text{finger}} - \hat{\mathbf{\theta}}_n^{\text{finger}} \Vert_1 )$, where $d$ is the number of degree-of-freedom (DoF) of the dexterous robot hand, \emph{i.e.,} 16 for Allegro hand and the LEAP hand used in our experiments, $\Vert \cdot \Vert_1$ denote the $1$-norm of the vector. 
% 5) Success rate: A tracking attempt is deemed successful if the object translation error $T_{\text{err}}$, object rotation error $R_{\text{err}}$, and the hand tracking error $0.5E_{\text{wrist}} + 0.5E_{\text{finger}}$ are all below their specified thresholds. Success rates are calculated using two thresholds: $10\text{cm}$-$20^\circ$-$0.8$, $10\text{cm}$-$40^\circ$-$1.2$. 

% are smaller than their corresponding threshold. Three success rates are calculated using two different thresholds, namely $10\text{cm}-20^\circ-0.8$, $10\text{cm}-40^\circ-1.2$. 
% a tracking is regarded as successful if the object rotation error object translation error 
% $15\text{cm}-40^\circ-1.6$.


\vspace{-5pt}
\noindent\textbf{Baselines.} 
To our knowledge, no prior model-based methods have directly tackled tracking control for dexterous manipulation. Most existing approaches focus on single goal-driven trajectory optimization with simplified dynamics models~\citep{Jin2024ComplementarityFreeMM,pang2023global,pang2021convex}, limiting their adaptability for our framework's generalizable tracking controller. Thus, we primarily compare our method with model-free approaches: 1) DGrasp~\citep{christen2022d}: Adapted to track by dividing sequences into subsequences of 10 frames, with each subsequence solved incrementally. 2) PPO (OmniGrasp rew.): We re-implemented OmniGrasp's reward~\citep{Luo2024GraspingDO} to train a policy for tracking object trajectories. 3) PPO (w/o sup., tracking rew.): We trained a policy using PPO with our proposed tracking reward and observation design.
% To the best of our knowledge, no prior model-based methods have specifically addressed tasks identical or comparable to the tracking control for dexterous manipulation. 
% Most existing approaches are constrained to single goal-driven trajectory optimization~\citep{Jin2024ComplementarityFreeMM,pang2023global,pang2021convex} with a simplified dynamics model, making them difficult to adapt for optimizing a versatile tracking controller as required by our framework.
% Therefore, we primarily compare our method against model-free approaches: 1) DGrasp~\citep{christen2022d}: We adapt DGrasp to address the tracking problem by dividing the entire sequence into several subsequences, each consisting of 10 frames. The last frame of each subsequence is defined as its reference frame. This transforms each subsequence tracking task into a solvable problem for DGrasp. We then train a policy to incrementally solve each subproblem. 2) PPO (OmniGrasp rew.): OmniGrasp proposes a universal controller for grasping objects from a table and following the given object trajectory. Since the original implementation has not been open-sourced, we re-implemented the reward as described in the paper~\citep{Luo2024GraspingDO}. Using this reward and PPO, we train a policy aimed at tracking each training trajectory. 3) PPO (w/o sup., tracking rew.): We directly train a policy using PPO and our proposed tracking reward, along with our observation design (see Section~\ref{sec:method_il_rl}). 
% To our best knowledge, there is no previous model-based methods aiming to solve the task same or similar to our dexterous manipulation tracking control. Most of them are restricted in single goal-driven trajectory optimization~\citep{Jin2024ComplementarityFreeMM,pang2023global,pang2021convex} which is hard to be adapted to align with our goal of  optimizing a tracking controller.
% which does not and can hardly be adapted to align with our goal in optimizing a tracking controller. 

% Therefore, we mainly compare model-free approaches: 1) DGrasp~\citep{christen2022d}. We adapt DGrasp to solve the tracking problem. We divide the whole sequence into several subsequences with a length of 10 frames. We then define the last frame of each subsequence as its reference frame. After that, each subsequence tracking becomes a problem that DGrasp can solve. We then train a policy to solve each subproblem gradually. 2) PPO (OmniGrasp rew.). OmniGrasp proposes a universal controller for grasping objects from the table and following the given object trajectory. Since it has not been open-sourced yet, we re-implement the reward as described in the paper~\citep{Luo2024GraspingDO}. We then train a policy using the reward and PPO, with the goal of tracking each trajectory in the training dataset. 3) PPO (w/o sup., tracking rew.). We directly train a policy using PPO and our proposed tracking reward as well as the observation design (see Equation~\ref{eq:observations}~\ref{eq:rew_obj}~\ref{eq:rew_hand}~\ref{eq:rew_affinity}). 

% Since in the tracking phase, 

% We re-implement the reward as described in the paper~\citep{Luo2024GraspingDO}. A PPO is trained 
% divide the whole sequence into several subsequences, each of which has 10 frames, and define the end frame of the subsequence as the reference frame. Then the grasping policy is used to guide the hand and gradually track the object according to the hand and the object pose of each reference frame. 
% We compare our method with previous model-free approaches. 
% omni grasp reward o not considerthe 


\vspace{-5pt}
\noindent\textbf{Training and evaluation settings.} 
% The training process involves three stages, each using different tracking demonstrations. 
We use PPO~\citep{Schulman2017ProximalPO}, implemented in {rl\_games}~\citep{rlgames2021}, with Isaac Gym~\citep{makoviychuk2021isaac} for simulation. Training is performed with 8192 parallel environments for both the per-trajectory tracker and the tracking controller. For the dexterous hand, the position gain and damping coefficient are set to 20 and 1 per finger joint. Evaluation results are averaged across 1000 parallel environments, and for real-world evaluations, we use LEAP~\citep{Shaw2023LEAPHL} with a Franka arm and FoundationPose~\citep{Wen2023FoundationPoseU6} for object state estimation. Additional details are provided in Appendix~\ref{sec:supp_exp_details}.
% for the tracking controller 
% without transferring weights between models.
% The training process for the tracking controller consists of three stages, as outlined in the method section. In each stage, we retrain the model using different labeled datasets, without transferring weights from the previous model.  We use PPO~\citep{Schulman2017ProximalPO}, implemented in {rl\_games}~\citep{rlgames2021}, as our reinforcement learning algorithm. Isaac Gym~\citep{makoviychuk2021isaac}, a widely-used simulated physical environment, is utilized for training. We employ 8192 parallel environments to train both the per-trajectory tracker and the tracking controller. For the articulated dexterous hand, the position gain and damping coefficient are set to 20 and 1, respectively, for each finger joint. During the evaluation phase, we average evaluation results obtained in 1000 parallel execution environments as the final result.  For real-world evaluations, we use LEAP~\citep{Shaw2023LEAPHL} with Franka arm. We use FoundationPose~\citep{Wen2023FoundationPoseU6} to estimate the object states. 
% Further details, such as the simulation environment and the real setup, are provided in Appendix~\ref{sec:supp_exp_details}. 
% During the evaluation phase, we sample the policy 1000 times across 1000 parallel execution environments and average the metrics to obtain the evaluation result for the current trajectory. 

% such as the simulation  setup, are provided in Appendix~\ref{sec:supp_exp_details}. 
% including the simulation environment setup, 
% This is because we found that transferring weights makes the model more difficult to train, rather than speeding up the training process.
% The tracking controller training is composed of three stages as we detailed in the method section. In each stage, we re-train the model with different labeled datasets without transferring weights from the previously trained model. It is because we observe that weight transferring would make the model even harder to train, other than accelerating the training process. We leverage PPO~\citep{Schulman2017ProximalPO} implemented in {rl\_games}~\citep{rlgames2021} as our RL algorithm. We use the widely appreciated Isaac Gym~\citep{makoviychuk2021isaac} as the simulated physical environment. We use 8192 parallel environments to train both the per-trajectory tracker and the tracking controller. For articulated dexterous hands, the position gain and the damping coefficient are set to 20 and 1 respectively for each finger joint. During the evaluation phase, we sample from the policy 1000 times (using 1000 parallel execution environments) and average their metrics as the evaluation result of the current trajectory. More details such as the simulation environment setting are deferred to Appendix~\ref{sec:supp_exp_details}. 
% For both the per-trajectory tracker and the tracking controller, 
% the single trajectory specialist tracking policy training and the general tracking policy optimization, we use 8192 parallel environments to train RL. 






% take the widely appreciated Isaac Gym as our simulated environment where we train our 


% rich and changing contacts, j
\vspace{-5pt}
% \subsection{Generalizable Dexterous Manipulation Tracking Control } \label{sec:exp_results}
\subsection{Generalizable Tracking Control for Dexteroous Manipulation } \label{sec:exp_results}
\vspace{-5pt}


% \todo{add the time consumption}

\begin{figure}[ht]
  \centering
  \vspace{-10pt}
  \includegraphics[width=\textwidth]{figs/robustness_noise.pdf}
  \vspace{-20pt}
  \caption{\footnotesize
  \textbf{Robustness w.r.t. unreasonable states. }
  Please check \textbf{\href{https://projectwebsite7.github.io/gene-dex-manip/}{our website} and {\href{https://projectwebsite7.github.io/gene-dex-manip/static/videos-lowres/video_7.mp4}{\textcolor{orange}{video}}}} for animated results.
  % ~\eric{the subsection title has been occluded by the figure... please adjust.}
  }
  \label{fig_res_robustness}
  \vspace{-10pt}
\end{figure}

% \todo{change the w/o prior, w/o curri. to w/o data, w/o structure}

\begin{figure}[ht]
  \centering
  % \vspace{-10pt}
  % \includegraphics[width=\textwidth]{figs/res_5.pdf}
  \includegraphics[width=\textwidth]{figs/eval_res_all_v2_compressed.pdf}
  % \vspace{-20pt}
  \caption{\footnotesize
  \textbf{Qualitative comparisons. }
  Please check \textbf{\href{https://projectwebsite7.github.io/gene-dex-manip/}{our website} and {\href{https://projectwebsite7.github.io/gene-dex-manip/static/videos-lowres/video_7.mp4}{\textcolor{orange}{the accompanying video}}}} for animated results.
  }
  \label{fig_res}
  \vspace{-10pt}
\end{figure}




% \begin{table*}[t]
%     \centering
%     \caption{ 
%     \textbf{Quantitative evaluations and comparisons to baselines.}  \bred{Bold red} numbers for best values and \iblue{italic blue} values for the second best-performed ones. 
%     } 
%     % \vspace{-10pt}
%     \resizebox{1.0\textwidth}{!}{%
% \begin{tabular}{@{\;}llccccc@{\;}}
%         \toprule
%         %         
%         Dataset & Method & \makecell[c]{$R_{\text{err}}$ ($\text{rad}, \downarrow$)} & \makecell[c]{$T_{\text{err}}$ (${cm}, \downarrow$)}  &  $E_{\text{wrist}}$ ($ \downarrow$)  & $E_{\text{finger}}$ ($\text{rad}, \downarrow$) & Success Rate ($\%, \uparrow$)   \\

        
%         \cmidrule(l{0pt}r{1pt}){1-2}
%         \cmidrule(l{2pt}r{2pt}){3-7}

%         % \multirow{5}{*}{ GRAB } & ControlVAE & - & -  & -  & - 
%         % \\ 
%         % ~ & MPC (w/ QuasiSim Base) & - & -  & -  & - 
%         % \\ 
%         % % \cmidrule(l{15pt}r{17pt}){3-9}

%         % % \cmidrule(l{15pt}r{17pt}){3-9}
%         % \cmidrule(l{1pt}r{1pt}){2-3}
%         % % \cmidrule(l{2pt}r{2pt}){8-12}


%         % ~ & \multirow{3}{*}{ \makecell[c]{Model \\ Free} } & DGrasp-Tracking & - & -  & -  & - 
%         % \\ 
%         \multirow{2}{*}{ GRAB } & DGrasp & - & -  & -  & - & - 
%         \\ 
%         ~ & PPO (OmniGrasp rew.) & - & -  & -  & - & - 
%         \\ 
%         ~ & PPO (w/o sup., tracking rew.)  & 0.3945 & 6.11  & 0.1076 & 0.5899 & 38.78/55.05/60.58 
%         \\ 
        
%         \cmidrule(l{0pt}r{1pt}){2-2}
%         \cmidrule(l{2pt}r{2pt}){3-7}
        
        
%         % ~ & ~ & Ours (Stage I) & {0.3443} & {7.81} & {0.1225} & {0.5218} & 39.79/58.13/66.78
%         % \\ 
%         % ~ & ~ & Ours (Stage II) & {0.3415} & {4.97} & { 0.1483} & {0.5264} & 43.25/58.13/68.51
%         % \\ 
%         ~ & \model~(w/ sup.)  & {0.3443} & {7.81} & {0.1225} & {0.5218} & 39.79/58.13/66.78
%         \\ 
%         ~ & \model~(w/ sup., w/ curriculum)  & {0.3415} & {4.97} & { 0.1483} & {0.5264} & 43.25/58.13/68.51
%         \\ 
%         ~ & \model  & \bred{0.3303} & \bred{4.53} & {0.1118} & \bred{0.5048} & \bred{47.12}/\bred{65.52}/\bred{72.41}
%         \\ 
        
%         \cmidrule(l{0pt}r{1pt}){1-2}
%         \cmidrule(l{2pt}r{2pt}){3-7}


%         % \multirow{5}{*}{ TACO } & ControlVAE & - & -  & -  & - 
%         % \\ 
%         % ~ & MPC (w/ QuasiSim Base) & - & -  & -  & - 
%         % \\ 
%         % % \cmidrule(l{15pt}r{17pt}){3-9}

%         % % \cmidrule(l{15pt}r{17pt}){3-9}
%         % \cmidrule(l{1pt}r{1pt}){2-3}
%         % % \cmidrule(l{2pt}r{2pt}){8-12}


%         % ~ & \multirow{3}{*}{ \makecell[c]{Model \\ Free} } & PPO & - & -  & -  & - 
%         % \\ 
%         % ~ & ~ & DGrasp-Tracking & - & -  & -  & - 
%         % \\ 
%         % ~ & ~ & \makecell[l]{OmniGrasp} & - & -  & -  & - 
%         % \\ 

%         \multirow{2}{*}{ TACO } & DGrasp & - & -  & -  & - & - 
%         \\ 
%         ~ & PPO (OmniGrasp rew.) & - & -  & -  & - & - 
%         \\ 
%         ~ & PPO (Tracking rew.)  & 0.4815 & 7.82  & 0.1195  & 0.4682 & {35.13}/{47.69}/61.54
%         \\ 
        
%         \cmidrule(l{0pt}r{1pt}){2-2}
%         \cmidrule(l{2pt}r{2pt}){3-7}
%         % \cmidrule(l{2pt}r{2pt}){3-7}
%         % \cmidrule(l{2pt}r{2pt}){8-12}
%         % \cmidrule(l{2pt}r{2pt}){1-2}
%         % ~ & ~ & Ours (Stage I) & \bred{ 0.5012} & \bred{4.10} & {0.2461} & {0.5771} & 5.12/5.12/61.53
%         % \\ 
%         % ~ & ~ & Ours (Stage II) & {0.4853} & {1.99} & \bred{ 0.1298} & \bred{0.4771} & 44.90/72.54/79.73
%         % \\ 
%         ~ & \model~(w/ sup.)  &  0.4444 & 2.33  & 0.1782  & 0.5438 & 47.97/67.12/73.66
%         \\ 
%         ~ & \model~(w/ sup., w/ curriculum)  & {0.4854} & {1.99} & { 0.1298} & {0.4772} & 54.90/72.55/79.74
%         \\ 
%         ~ & \model & {0.4953} &  {2.10} & {0.1510} & {0.4661} & \bred{49.02}/\bred{74.50}/\bred{81.05}
%         \\ 

%         \bottomrule
 
%     \end{tabular}
%     }
%     \vspace{-10pt}
%     \label{tb_exp_main}
% \end{table*} 




% \todo{change the w/o prior, w/o curri. to w/o data, w/o structure}
\begin{table*}[t]
    \centering
    \caption{ 
    \footnotesize
    \textcolor{myblue}{\textbf{Quantitative evaluations.}}  \bred{Bold red} and \iblue{italic blue} \textcolor{myblue}{values for best and the second best-performed ones respectively}. 
    \textcolor{myblue}{``Ours (w/o) data'' and ``Ours (w/o data, w/o homotopy)'' are two ablated versions w.r.t. quality of robot tracking demonstrations used in imitation learning (see Section~\ref{sec:ablation_studies} for details). }
    % Demonstrations used in ``Ours (w/o) data'' are not improved by 
    % ``Ours (w/o) data'' uses demonstrations that are not improved by transferring ``tracking prior'', while ``Ours (w/o data, w/o homotopy)'' uses demonstrations neither improved by transferring tracking prior nor via the homotopy optimization scheme.
    % is the ablated version where demonstrations are curated without transferring ``tracking prior'', while ``Ours (w/o data, w/o homotopy)'' uses demonstrations neither improved by transferring tracking prior nor via the homotopy optimization scheme.}
    % numbers for best values and  for the second best-performed ones. 
    } 
    % \vspace{-10pt}
    \resizebox{1.0\textwidth}{!}{%
\begin{tabular}{@{\;}llccccc@{\;}}
        \toprule
        %         
        Dataset & Method & \makecell[c]{$R_{\text{err}}$ ($\text{rad}, \downarrow$)} & \makecell[c]{$T_{\text{err}}$ (${cm}, \downarrow$)}  &  $E_{\text{wrist}}$ ($ \downarrow$)  & $E_{\text{finger}}$ ($\text{rad}, \downarrow$) & Success Rate ($\%, \uparrow$) \\

        
        \cmidrule(l{0pt}r{1pt}){1-2}
        \cmidrule(l{2pt}r{2pt}){3-7}

        

        % ~ & \multirow{3}{*}{ \makecell[c]{Model \\ Free} } & DGrasp-Tracking & - & -  & -  & - 
        % \\ 
        \multirow{6}{*}{ GRAB } & DGrasp & 0.4493 & 6.75  & 0.1372  & 0.6039 & 34.52/52.79
        \\ 
        ~ & PPO (OmniGrasp rew.) & 0.4404 & 6.69  & 0.1722 & 0.6418  & 35.53/54.82
        \\ 
        ~ & PPO (w/o sup., tracking rew.)  & 0.3945 & 6.11  & \bred{0.1076} & 0.5899 & 38.58/54.82 % & 
        \\ 
        
        \cmidrule(l{0pt}r{1pt}){2-2}
        \cmidrule(l{2pt}r{2pt}){3-7}
        
        
        % ~ & ~ & Ours (Stage I) & {0.3443} & {7.81} & {0.1225} & {0.5218} & 39.79/58.13/66.78
        % \\ 
        % ~ & ~ & Ours (Stage II) & {0.3415} & {4.97} & { 0.1483} & {0.5264} & 43.25/58.13/68.51
        % \\ 
        ~ & \model~(w/o data, w/o homotopy)  & {0.3443} & {7.81} & {0.1225} & \iblue{0.5218} & 39.59/57.87 
        \\ 
        ~ & \model~(w/o data)  & \iblue{0.3415} & \iblue{4.97} & { 0.1483} & {0.5264} & \iblue{43.15}/\iblue{62.44} % 
        \\ 
        ~ & \model  & \bred{0.3303} & \bred{4.53} & \iblue{0.1118} & \bred{0.5048} & \bred{46.70}/\bred{65.48} 
         \\ 
        
        \cmidrule(l{0pt}r{1pt}){1-2}
        \cmidrule(l{2pt}r{2pt}){3-7}


        % \multirow{5}{*}{ TACO } & ControlVAE & - & -  & -  & - 
        % \\ 
        % ~ & MPC (w/ QuasiSim Base) & - & -  & -  & - 
        % \\ 
        % % \cmidrule(l{15pt}r{17pt}){3-9}

        % % \cmidrule(l{15pt}r{17pt}){3-9}
        % \cmidrule(l{1pt}r{1pt}){2-3}
        % % \cmidrule(l{2pt}r{2pt}){8-12}


        % ~ & \multirow{3}{*}{ \makecell[c]{Model \\ Free} } & PPO & - & -  & -  & - 
        % \\ 
        % ~ & ~ & DGrasp-Tracking & - & -  & -  & - 
        % \\ 
        % ~ & ~ & \makecell[l]{OmniGrasp} & - & -  & -  & - 
        % \\ 

        \multirow{6}{*}{ TACO } & DGrasp & 0.5021 & 5.04  & \bred{0.1129}  &  0.4737 & 38.42/47.78
        \\ 
        ~ & PPO (OmniGrasp rew.) & 0.5174 & 5.43  & 0.1279  & 0.4945 & 33.5/46.31
        \\ 
        ~ & PPO (w/o sup., tracking rew.)  & \iblue{0.4815} & 4.82  & \iblue{0.1195}  & \iblue{0.4682} & {34.98}/{57.64} 
        \\ 
        
        \cmidrule(l{0pt}r{1pt}){2-2}
        \cmidrule(l{2pt}r{2pt}){3-7}
        % \cmidrule(l{2pt}r{2pt}){3-7}
        % \cmidrule(l{2pt}r{2pt}){8-12}
        % \cmidrule(l{2pt}r{2pt}){1-2}
        % ~ & ~ & Ours (Stage I) & \bred{ 0.5012} & \bred{4.10} & {0.2461} & {0.5771} & 5.12/5.12/61.53
        % \\ 
        % ~ & ~ & Ours (Stage II) & {0.4853} & {1.99} & \bred{ 0.1298} & \bred{0.4771} & 44.90/72.54/79.73
        % \\ 
        ~ & \model~(w/o data, w/o homotopy)  &  \bred{0.4444} & 2.33  & 0.1782  & 0.5438 & 44.83/67.00
        \\ 
        ~ & \model~(w/o data)  & {0.4854} & \iblue{2.21} & { 0.1698} & {0.4772} & \iblue{47.78}/\iblue{72.41}
        \\  
        ~ & \model & {0.4953} &  \bred{2.10} & {0.1510} & \bred{0.4661} & \bred{48.77}/\bred{74.38}
        \\ 

        \bottomrule
 
    \end{tabular}
    }
    \vspace{-10pt}
    \label{tb_exp_main}
\end{table*} 


% \todo{do not claim the efficiency and jmove the timeconsumption to the appendix}



% \begin{table*}[t]
%     \centering
%     \caption{ 
%     \textbf{Quantitative evaluations and comparisons to baselines.}  \bred{Bold red} numbers for best values and \iblue{italic blue} values for the second best-performed ones. 
%     } 
%     % \vspace{-10pt}
%     \resizebox{1.0\textwidth}{!}{%
% \begin{tabular}{@{\;}lllccccc@{\;}}
%         \toprule
%         %         
%         Dataset & ~ & Method & \makecell[c]{$R_{\text{err}}$ ($\text{rad}, \downarrow$)} & \makecell[c]{$T_{\text{err}}$ (${cm}, \downarrow$)}  &  $E_{\text{wrist}}$ ($ \downarrow$)  & $E_{\text{finger}}$ ($\text{rad}, \downarrow$) & Success Rate ($\%, \uparrow$)   \\

        
%         \cmidrule(l{0pt}r{1pt}){1-3}
%         \cmidrule(l{2pt}r{2pt}){4-8}

%         \multirow{5}{*}{ GRAB } & \multirow{2}{*}{ \makecell[c]{Model \\ Based} } & ControlVAE & - & -  & -  & - 
%         \\ 
%         ~ & ~ & MPC (w/ QuasiSim Base) & - & -  & -  & - 
%         \\ 
%         % \cmidrule(l{15pt}r{17pt}){3-9}

%         % \cmidrule(l{15pt}r{17pt}){3-9}
%         \cmidrule(l{1pt}r{1pt}){2-3}
%         % \cmidrule(l{2pt}r{2pt}){8-12}


%         % ~ & \multirow{3}{*}{ \makecell[c]{Model \\ Free} } & DGrasp-Tracking & - & -  & -  & - 
%         % \\ 
%         ~ & \multirow{2}{*}{ \makecell[c]{Model \\ Free} }  & PPO (OmniGrasp rew.) & - & -  & -  & - & - 
%         \\ 
%         ~ & ~ & PPO (Tracking rew.)  & 0.3945 & 6.11  & 0.1076 & 0.5899 & 38.78/55.05/60.58 
%         \\ 
        
%         \cmidrule(l{1pt}r{1pt}){2-3}
%         \cmidrule(l{1pt}r{1pt}){4-8}
%         % \cmidrule(l{2pt}r{2pt}){3-7}
%         % \cmidrule(l{2pt}r{2pt}){8-12}
%         % \cmidrule(l{2pt}r{2pt}){1-2}

%         % ~ & ~ & Ours (Stage I) & {0.3443} & {7.81} & {0.1225} & {0.5218} & 39.79/58.13/66.78
%         % \\ 
%         % ~ & ~ & Ours (Stage II) & {0.3415} & {4.97} & { 0.1483} & {0.5264} & 43.25/58.13/68.51
%         % \\ 
%         ~ & ~ & Ours  & \bred{0.3303} & \bred{4.53} & {0.1118} & \bred{0.5048} & \bred{47.12}/\bred{65.52}/\bred{72.41}
%         \\ 
        
%         \cmidrule(l{1pt}r{1pt}){1-3}
%         \cmidrule(l{1pt}r{1pt}){4-8}


%         \multirow{5}{*}{ TACO } & \multirow{2}{*}{ \makecell[c]{Model \\ Based} } & ControlVAE & - & -  & -  & - 
%         \\ 
%         ~ & ~ & MPC (w/ QuasiSim Base) & - & -  & -  & - 
%         \\ 
%         % \cmidrule(l{15pt}r{17pt}){3-9}

%         % \cmidrule(l{15pt}r{17pt}){3-9}
%         \cmidrule(l{1pt}r{1pt}){2-3}
%         % \cmidrule(l{2pt}r{2pt}){8-12}


%         % ~ & \multirow{3}{*}{ \makecell[c]{Model \\ Free} } & PPO & - & -  & -  & - 
%         % \\ 
%         % ~ & ~ & DGrasp-Tracking & - & -  & -  & - 
%         % \\ 
%         % ~ & ~ & \makecell[l]{OmniGrasp} & - & -  & -  & - 
%         % \\ 

%         ~ & \multirow{2}{*}{ \makecell[c]{Model \\ Free} }  & PPO (OmniGrasp rew.) & - & -  & -  & - 
%         \\ 
%         ~ & ~ & PPO (Tracking rew.)  & 0.4815 & 10.11  & 0.1195  & 0.4682 & {5.13}/{5.13}/61.54
%         \\ 
        
%         \cmidrule(l{1pt}r{1pt}){2-3}
%         \cmidrule(l{1pt}r{1pt}){4-8}
%         % \cmidrule(l{2pt}r{2pt}){3-7}
%         % \cmidrule(l{2pt}r{2pt}){8-12}
%         % \cmidrule(l{2pt}r{2pt}){1-2}
%         % ~ & ~ & Ours (Stage I) & \bred{ 0.5012} & \bred{4.10} & {0.2461} & {0.5771} & 5.12/5.12/61.53
%         % \\ 
%         % ~ & ~ & Ours (Stage II) & {0.4853} & {1.99} & \bred{ 0.1298} & \bred{0.4771} & 44.90/72.54/79.73
%         % \\ 
%         ~ & ~ & Ours & {0.4953} &  {2.10} & {0.1510} & {0.4661} & \bred{49.02}/\bred{74.50}/\bred{81.05}
%         \\ 

%         \bottomrule
 
%     \end{tabular}
%     }
%     \vspace{-10pt}
%     \label{tb_exp_main}
% \end{table*} 




% \begin{figure}[h]
%   \centering
%   % \vspace{-10pt}
%   \includegraphics[width=\textwidth]{figs/res.pdf}
%   % \vspace{-20pt}
%   \caption{
%   \textbf{Qualitative comparisons. }
%   Please refer to \textbf{\href{https://projectwebsite7.github.io/gene-dex-manip/}{our website} and {\href{}{\textcolor{orange}{the accompanying video}}}} for animated results.
%   }
%   \label{fig_res_bak}
%   % \vspace{-10pt}
% \end{figure}




% \begin{wraptable}[6]{r}{0.5\textwidth}
% \vspace{-30pt}
%     \centering
%     \caption{ 
%     % \textbf{Ablation studies} on the GRAB dataset. 
%     Total training time consumption (GRAB dataset). 
%     } 
%     % \vspace{5pt}
% 	% \resizebox{\linewidth}{!}{%
%     \resizebox{0.5\textwidth}{!}{%
% \begin{tabular}{@{\;}lcccc@{\;}}
%         \toprule
%         ~ & \makecell[c]{PPO \\ (w/o sup)} & \makecell[c]{Ours \\ (w/ sup.)}  & \makecell[c]{Ours \\ (w/ sup., \\ w/ curri.)} & Ours   \\

%         \midrule % 

%         Time  &  $\sim$1 day & $\sim$4.5 days & $\sim$9 days & $\sim$5 days
%         \\ 
        
%         \bottomrule
 
%     \end{tabular}
%     }
%     % \vspace{-16pt}
%     \label{tb_exp_timeconsumption}
% \end{wraptable}





% decompose 

% \todo{which card and the hardware settings}

% We demonstrate the generalization ability of our tracking controller to unseen trajectories involving difficult manipulations and novel, thin objects. Our controller has no difficulty in dealing with very intricate motions involving thin objects, subtle in-hand re-orientations, and expressive functional manipulations. As presented in Table~\ref{tb_exp_main}, we can achieve significantly higher success rates calculated under two thresholds than the best-performed baseline in both two datasets. Figure~\ref{fig_res} illustrates qualitative examples and comparisons. Please check out our website and video for animated results. 
We demonstrate the generalization ability and robustness of our tracking controller on unseen trajectories involving challenging manipulations and novel, thin objects. Our controller has no difficulty in handling intricate motions, subtle in-hand re-orientations, and expressive functional manipulations, even when dealing with thin objects. 
As shown in Table~\ref{tb_exp_main}, we achieve significantly higher success rates, calculated under two different thresholds, compared to the best-performing baseline across both datasets. Figure~\ref{fig_res} provides qualitative examples and comparisons. We show the real-world effectiveness of our method and the superiority over best-performing baselines (Figure~\ref{fig_res},Table~\ref{tb_real_world_results}). For animated results, please visit our \href{https://projectwebsite7.github.io/gene-dex-manip/}{project website} and the \href{https://projectwebsite7.github.io/gene-dex-manip/static/videos-lowres/video_7.mp4}{accompanying video}.
% in two HOI datasets. Empowered by xxx, our policy can generalize well and robustly track challenging manipulations unseen during training. Most impressively and perhaps even surprising to us, our policy has no difficulty in dealing with very intricate motions involving thin objects, subtle in-hand re-orientations, and expressive functional manipulations. 



% \textcolor{red}{(TODO: cross-object generalization test? and that should be delivered in a small table)}


% \noindent\textbf{Expressive and functional motions with subtle in-hand re-orientations.} 

\noindent\textbf{Intriguing in-hand manipulations.}
% Expressive motions with s
% ubtle in-hand manipulations.} 
Our method effectively generalizes to novel, complex, and challenging functional manipulations, featured by subtle in-hand re-orientations, essential for precise tool-use tasks. For instance, in Figure~\ref{fig_res}a, the shovel is lifted, tilted, and reoriented with intricate finger movements to complete a {stirring} motion. Similarly, in Figure~\ref{fig_res}c, the small shovel is reoriented using minimal wrist adjustments. These results demonstrate the robustness and generalization of our controller, outperforming the PPO baseline, which struggles with basic lifting. 
% tasks like lifting thin tools.
% including
% complex object movements and 
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Our method effectively generalizes to unseen, challenging manipulations that involve complex functional object movements and subtle in-hand re-orientations, which are crucial for achieving precise reorientations during tool-use tasks. For example, in Figure~\ref{fig_res}a, the shovel must first be lifted with the inner side facing down, then tilted so the inner part faces upward, while the fingers control the handle to complete a \texttt{stirring} motion. Our results nearly achieve this goal, despite the intricate finger movements required to re-orient the object.
% In Figure~\ref{fig_res}c, the trajectory demands that the small shovel be re-oriented to face right so it can be used to hold things. Although our method initially lifts the shovel with the inner side facing left (as shown in the third frame), it successfully re-orients the tool using detailed finger adjustments, with minimal change in wrist orientation (as shown in the fourth frame).
% These results highlight the robust performance and exceptional generalization ability of our controller. In contrast, the best-performing baseline, PPO (w/o sup.), which attempts to train a general controller using our designed rewards, struggles even with basic tasks like lifting thin tools from the table.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Our method generalizes well to unseen challenging manipulations involving complex functional object movements, and detailed and subtle in-hand re-orientations that are crucial to achieve correct orientations for accurately mimicking tool-using motions. In Figure~\ref{fig_res}a, the shovel should be lifted at first with the inner side facing towards and then tiled down with the inner part facing upwards and fingers controlling the handle to complete the \texttt{stirring} motion. Our results can almost fulfill the goal despite the need to use detailed finger movements to re-orient the object. As demonstrated in Figure~\ref{fig_res}c, the trajectory requires the hand to make the small shovel face towards the right side so that it can use it to hold things. Despite the fact that our method manage to lift the object up from the table with the object facing towards the left side (shown in the third frame), it can then successfully re-orient the small shovel so that it faces towards the right side mainly using fingers without varying the wrist orientation a lot (as shown in the forth frame). 
% These great performance convincingly demonstrate the strong capability and notable generalization ability of our controller. 
% In comparison to ours' adeptly tracking functional interactions, the best-performed baseline PPO (w/o sup.), which attempts to directly train a general controller using PPO with our carefully designed rewards but without additional guidance, even faces challenges in lifting such thin tools up from the table. 
% However xxx (baseline methods). \todo{baseline's performance}
% Such tool using method demonstrates the high generalizable dexterity enabled by our general tracker. 



% \begin{wraptable}[5]{r}{0.6\textwidth}
% \vspace{-23pt}
%     \centering
%     \caption{ 
%     % \textbf{Ablation studies} on the GRAB dataset. 
%     % \textbf{Generalization ability evaluations} on the TACO dataset. 
%     % \textbf{Gen} on the TACO dataset. 
%     \footnotesize
%     Generalizability evaluations on the TACO dataset. 
%     } 
%     \vspace{5pt}
% 	% \resizebox{\linewidth}{!}{%
%     \resizebox{0.6\textwidth}{!}{%
% \begin{tabular}{@{\;}lccccc@{\;}}
%         \toprule
%         Test set & \makecell[c]{$R_{\text{err}}$ ($\text{rad}, \downarrow$)} & \makecell[c]{$T_{\text{err}}$ (${cm}, \downarrow$)}  &  $E_{\text{wrist}}$ ($ \downarrow$)  & $E_{\text{finger}}$ ($\text{rad}, \downarrow$) & Success Rate ($\%, \uparrow$)   \\

%         % \\
%         \midrule % 


%         % \\
%         % \cmidrule(l{15pt}r{17pt}){2-4}
        
%         % \model (w/o SpatialDiff) & 2.94 & 3.45  & 31.67
%         % \\ 

%         % \model (w/o TemporalDiff) & \bred{1.72} & \bred{1.90} & 34.25 % 18.65 %
%         % \\

%         % \model (w/o Diffusion) & 3.16 & 3.83  & 18.65 

%         % S0 & {0.3443} & {7.81} & {0.1225} & {0.5218} & 39.79/58.13/66.78
%         % \\
        
%         S1 & 0.5787 & {2.43} & {0.1481} & {0.4703} & 35.97/67.63
%         \\ 

%         % 
%         % \model (w/o Canon.) & {2.36} & {3.57} & \iblue{13.26}
%         S2 & {0.6026} & {2.46} & { 0.1455} & {0.4709} & 30.83/65.00
%         \\ 

%         S3  & {0.6508} & {8.06} & {0.1513} & {0.4683} & 10.18/{46.32}
%         \\ 
        
%         \bottomrule
 
%     \end{tabular}
%     }
%     % }
%     % \vspace{-16pt}
%     \label{tb_exp_taco_generalization_test}
% \end{wraptable}



\noindent\textbf{Intricate manipulations with thin objects. } 
Our method also generalizes well to challenging manipulations involving thin objects. In Figure~\ref{fig_res}b, despite the thin \texttt{shovel}'s complexity and missing CAD model parts, our method successfully lifts and controls it using a firm grasp with the second and third fingers. Similarly, in Figure~\ref{fig_res}e, our controller adeptly lifts and manipulates a thin \texttt{flute}, while the best-performing baseline struggles with the initial grasp. These results highlight the advantages of our approach in handling complex and delicate manipulations.
% that are difficult to grasp and control. 
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Our method generalizes effectively to unseen, challenging manipulations involving extremely thin objects that are both difficult to grasp and control. As shown in Figure~\ref{fig_res}b, the shovel is extremely thin, with some triangles missing from the CAD model, which adds to the complexity. Additionally, controlling such a long and thin object from the end further increases the difficulty. Despite these challenges, our method performs well, successfully lifting the object using the second and third fingers while maintaining a firm grip to follow the trajectory, even with this unconventional grasping pose.
% In the scenario depicted in Figure~\ref{fig_res}e, the task involves lifting a notably thin flute from the table and manipulating it. Our controller demonstrates remarkable proficiency in interacting with such objects. In contrast, the best-performing baseline struggles significantly during the initial grasping phase, highlighting the advantages of our approach in handling complex and delicate manipulations.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Our method generalize well to unseen challenging manipulations involving extremely thin objects that are both difficult to grasp and control. As demonstrated in Figure~\ref{fig_res}b, the shovel is very thin here even with some triangles missing from the CAD model. Besides, controlling such a long and thin object from the end side further adds huge difficulties. Despite such challenges, our method can still generalize well and achieve good performance in this case. It manages to lift the object up using the second and the third fingers and control the hand to firmly hold the objects to follow its trajectory, using this unfriendly grasping pose. 
% In the case drawn in Figure~\ref{fig_res}f, the challenging task involves lifting an extremely thin flute from the table and playing with it. Our controller has no difficulty in interacting with this kind of object. 
% In comparison, the best-performed baseline faces significant challenges in the initial grasping stage. 
% Our controller can even perform robustly in this unfriendly grasping pose, controlling the hand to firmly hold the object and follow its trajectory. 

% complex object movements, extremely thin objects that are very difficult to grasp
% has no difficult in resolving challenging manipulation

% \begin{wrapfigure}[6]{r}{0.45\textwidth} % {0pt}
% \vspace{-4ex}
% \includegraphics[width = 0.45\textwidth]{./figs/real_eval_figs.pdf}
% \vspace{-24pt}
%     \caption{\footnotesize
%     Qualitative results in the real world.
%     }
%     \label{fig:fig_real_eval_figs}
% \vspace{-20pt}
% \end{wrapfigure}


% \todo{add this part}







\begin{table*}[t]
    \centering
    \caption{ \footnotesize
    \textbf{Real-world quantitative comparisons.}  \bred{Bold red} numbers for best values.
    % and \iblue{italic blue} values for the second best-performed ones. 
    } 
    % \vspace{-10pt}
        \resizebox{1.0\textwidth}{!}{%
\begin{tabular}{@{\;}lcccccccccc@{\;}}
        \toprule
       Method & \texttt{apple} & \texttt{banana} & \texttt{duck}  & \texttt{elephant} & \texttt{flashlight}    & \texttt{flute}   & \texttt{hammer}  & \texttt{hand}  & \texttt{phone}  & \texttt{waterbottle}     \\

        % \\
        \midrule % 

        PPO (w/o sup., tracking rew) & 0/0/0 & 25.0/25.0/0.0 & 50.0/25.0/0 & 50.0/0.0/0.0 & 50.0/0/0 & 0/0/0 & 25.0/0/0 & 66.7/33.3/0 & 25.0/0/0 & 33.3/33.3/0
        \\ 

        % 
        \model & \bred{25.0}/0/0 & \bred{50.0}/\bred{50.0}/\bred{25.0} & \bred{75.0}/\bred{50.0}/\bred{25.0} & \bred{75.0}/\bred{50.0}/\bred{50.0} & \bred{50.0}/\bred{25.0}/\bred{25.0}  & \bred{25.0}/\bred{25.0}/\bred{25.0} & \bred{50.0}/\bred{50.0}/\bred{50.0} & \bred{66.7}/\bred{33.3}/\bred{33.3} & \bred{50.0}/\bred{50.0}/\bred{25.0} & \bred{50.0}/\bred{33.3}/\bred{33.3} 
        \\ 

        % S3  & {0.6508} & {8.06} & {0.1513} & {0.4683} & 10.18/{46.32}
        % \\ 
        
        \bottomrule
 
    \end{tabular}
    }
    \vspace{-20pt}
    \label{tb_real_world_results}
\end{table*} 

% % \begin{wraptable}[5]{r}{0.6\textwidth}
% \begin{table*}[t]
% \vspace{-23pt}
%     \centering
%     \caption{ 
%     \footnotesize
%     Generalizability evaluations on the TACO dataset. 
%     } 
%     \vspace{5pt}
% 	% \resizebox{\linewidth}{!}{%
%     \resizebox{\textwidth}{!}{%
% \begin{tabular}{@{\;}lcccccccccc@{\;}}
%         \toprule
%        Method & apple & banana & duck  & elephant & flashlight    & flute   & hammer  & hand  & phone  & waterbottle     \\

%         % \\
%         \midrule % 

        
%         PPO (w/o sup., tracking rew) & 0.5787 & {2.43} & {0.1481} & {0.4703} & 35.97/67.63
%         \\ 

%         % 
%         % \model (w/o Canon.) & {2.36} & {3.57} & \iblue{13.26}
%         \model & {0.6026} & {2.46} & { 0.1455} & {0.4709} & 30.83/65.00
%         % \\ 

%         % S3  & {0.6508} & {8.06} & {0.1513} & {0.4683} & 10.18/{46.32}
%         % \\ 
        
%         \bottomrule
 
%     \end{tabular}
%     }
%     % \vspace{-16pt}
%     \label{tb_real_world_results}
% % \end{wraptable}
% \end{table*}


% \begin{figure}[ht]
%   \centering
%   \vspace{-10pt}
%   \includegraphics[width=0.8\textwidth]{figs/real_exp_cmp_3.pdf}
%   \vspace{-20pt}
%   \caption{\footnotesize
%   \textbf{Qualitative real-world evaluations and comparisons. }
%   % Please refer to \textbf{\href{https://projectwebsite7.github.io/gene-dex-manip/}{our website} and {\href{https://projectwebsite7.github.io/gene-dex-manip/static/videos-lowres/video_7.mp4}{\textcolor{orange}{the accompanying video}}}} for animated results.
%   }
%   \label{fig:fig_real_eval_figs}
%   \vspace{-10pt}
% \end{figure}
% \vspace{-5pt}

\noindent\textbf{Real-world evaluations and comparisons.} 
% We transfer tracking results directly to the real world to assess tracking quality and also transfer the state-based controller to evaluate its robustness to noises in the state estimator. Success rates are measured under three thresholds and compared with the best baseline. Table~\ref{tb_real_world_results} summarizes per-object success rates averaged over their manipulation trajectories (transferred controller setting). Demonstrated in Figure~\ref{fig_res}f~\ref{fig_res}g, we enable the robot to track complex object movements in real-world scenarios. Further details are in Appendix~\ref{sec:supp_real_world}.
We directly transfer tracking results to the real world to assess tracking quality and evaluate the robustness of the state-based controller against noise in the state estimator. Success rates are measured under three thresholds and compared with the best baseline. Table~\ref{tb_real_world_results} summarizes the per-object success rates averaged over their manipulation trajectories in the transferred controller setting. As demonstrated in Figures~\ref{fig_res}f and \ref{fig_res}g, we enable the robot to track complex object movements and successfully lift a hard-to-grasp round apple in real-world scenarios. While the baseline fails. Further details can be found in Appendix~\ref{sec:supp_real_world}.

% We transfer tracking results directly to the real world to assess tracking quality and evaluate the robustness of the state-based tracker to noise in the state estimator. We measure success rates under three success thresholds for both settings and compare them with the best-performing baseline. Table~\ref{tb_real_world_results} summarizes the performance of the transferred tracker. We report per-object success rates averaged over their corresponding manipulation trajectories. As shown in Figure~\ref{fig_res}f,g, our method enables the robot to successfully track complex object movements in real-world scenarios. Additional details and results are provided in Appendix~\ref{sec:supp_real_world}.
% reports the 
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% We directly transfer tracking results to the real world to demonstrate the tracking quality and also transfer the state-based tracker to the real world to evaluate its robustness to noises in the state estimator. We evaluate the success rate under three types of success thresholds for both settings and compare it with the best-performed baseline method. Values reported in Table~\ref{tb_real_world_results} are the per-object success rate averaged over their corresponding manipulation trajectories, achieved via the transferred tracker. As shown in Figure~\ref{fig_res}f,g, our results executed in the real world can successfully let the robot track difficult manipulations with complex object movements. More details and results are deferred to the Appendix~\ref{sec:supp_real_world}. 
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% success rate of each object averaged over the number of manipulation trajectories, 
% and evaluate the success rate. Moreover, we also transfer the state-based tracker to the real world to evaluate its robustness to noises in the state estimator. 




\vspace{-10pt}
\subsection{Further Analysis and Discussions} \label{sec:exp_analysis}
\vspace{-5pt}


\noindent\textbf{Robustness to noise in the kinematic reference motions.} 
% Our method is robust to significant kinematic noise. 
Despite severe hand-object penetrations in Figure~\ref{fig_res}c (frames 2 and 3) and Figure~\ref{fig_res}a (frame 2), the hand still interacts effectively with the object, highlighting the resilience of our tracking controller in challenging scenarios.
% Our method is robust to significant noise in the kinematic motions. For example, even with severe hand-object penetrations occurring in the motions depicted in Figure~\ref{fig_res}c (in the second and third frames) and Figure~\ref{fig_res}a (in the second frame), we can still effectively control the hand to interact correctly with the object. This resilience underscores the effectiveness of our tracking controller in handling challenging scenarios.
% We would not be affected by obvious noise in the kinematic motions. For instance, although there are severe hand-object penetrations in the motion shown in Figure~\ref{fig_res}c (the second and the third frame) and Figure~\ref{fig_res}a (the second frame), we can successfully control the hand to correctly interact with the object. 



% \begin{figure}[ht]
%   \centering
%   \vspace{-10pt}
%   \includegraphics[width=\textwidth]{figs/robustness_2.pdf}
%   \vspace{-20pt}
%   \caption{
%   \textbf{Robustness towards unreasonable references and novel object categories. }
%   Please refer to \textbf{\href{https://projectwebsite7.github.io/gene-dex-manip/}{our website} and {\href{https://projectwebsite7.github.io/gene-dex-manip/static/videos-lowres/video_7.mp4}{\textcolor{orange}{the accompanying video}}}} for animated results.
%   }
%   \label{fig_res_robustness}
%   \vspace{-10pt}
% \end{figure}





\noindent\textbf{Robustness to unreasonable references. }
As shown in Figure~\ref{fig_res_robustness}, our method remains unaffected by significant noise in the kinematic references with unreasonable states.
% , even when they include unreasonable or unreachable states.
We effectively track the entire motion trajectory, demonstrating the controller's robustness in handling unexpected noise.
% As shown in Figure~\ref{fig_res_robustness}a, our method remains unaffected by significant noise in the kinematic references, even when they include unreasonable or unreachable states. We can effectively track the entire motion trajectory despite these challenges, demonstrating our controller's robustness in handling unexpected noise. 
% The kinematic reference trajectory is obtained by interpolating a different trajectory from the TACO dataset, demonstrating our controller's robustness in handling complex and noisy inputs.
% As demonstrated in Figure~\ref{fig_res_robustness}a, we would not be affected by the large noise in the kinematics references containing unreasonable and unreachable states and can still manage to track the whole motion trajectory. The kinematic reference trajectory is obtained by interpolating a different trajectory with a sequence from TACO. 
% \todo{xxx}




% \noindent\textbf{Further generalization ability evaluations.}
% We further evaluate the model's generalization ability across various test sets within the TACO dataset. As shown in Table~\ref{tb_exp_taco_generalization_test}, the controller performs well in the category-level generalization setting (S1), where object categories are known but manipulation trajectories and object geometries are novel. Performance on S2, involving novel interaction triplets, is satisfactory, demonstrating the controller's capacity to handle new manipulation sequences. However, results from S3 reveal challenges when dealing with new object categories and unseen interaction triplets. For instance, generalizing from interactions with shovels and spoons to using bowls for holding objects is particularly difficult. As shown in Figure~\ref{fig_res_robustness}b, despite unfamiliar objects and interactions, we successfully lift the knife and mimic the motion, though execution is imperfect, highlighting areas for improvement and adaptability in challenging scenarios.

% We further evaluate the model's generalization ability across different levels of test sets within the TACO dataset. As shown in Table~\ref{tb_exp_taco_generalization_test}, the controller performs well in the category-level generalization setting, where the object category is known during training, but the manipulation trajectories and object geometries are novel, as evidenced by its strong performance on test set S1.
% The performance on S2, which involves novel interaction triplets, remains relatively satisfactory, demonstrating the controller's capacity to generalize to new manipulation sequences. However, results from S3 highlight the challenges faced when dealing with novel object categories and unseen interaction triplets. For instance, generalizing from interactions that involve lifting and waving shovels and spoons to using bowls for holding objects proves to be too difficult for the current design. 
% As illustrated in Figure~\ref{fig_res_robustness}b, despite the unfamiliar object category and interaction triplet, we still manage to lift the knife and mimic the intended motion, although the execution is not perfect. This indicates potential areas for improvement while showcasing the model's adaptability in challenging scenarios.

% We further test the model's generalization ability on different levels of test sets on the TACO dataset. As indicated in Table~\ref{tb_exp_taco_generalization_test}, the controller can perform well in the category-level generalization setting, where the object category is observed during the training while the manipulation trajectories and the object geometry are novel, as indicated by its good performance in the test set S1. 
% The performance on S2 contains novel interaction triples is still relatively satisfactory, indicating its ability to generalize to control novel manipulation sequences. However, results on S3 indicate the model's being challenged by novel object categories and unseen interaction triplets. Generalizing from interactions demonstrating lifting and waving shovels and spoons to using bowls to hold things is still too challenging for the current design. As shown in Figure~\ref{fig_res_robustness}b, given the unobserved object category and the interaction triplet, though not perfect, we can still manage to lift the knife up and mimic the motion. 
% \todo{add the qualitative evals}
% The object position error is still kept to a small value, indicating 

% Specifically, the whole dataset is split into a training dataset, containing 1565 trajectories, and the test set S0 where both the tool object geometry and the interaction triplets are seen during training, the test set S1 where the tool geometry is novel but the interaction triplets are seen during training, the test set S2 with novel interaction triplets but seen object categories and geometries, and the test set S4 with new object geometries and interaction triplets. Details w.r.t. the dataset splitting and visualizations are deferred to Appendix~\ref{}. 

% \begin{wraptable}[6]{r}{0.5\textwidth}
% \vspace{-30pt}
%     \centering
%     \caption{ 
%     Total training time consumption (TACO dataset). 
%     } 
%     % \vspace{5pt}
% 	% \resizebox{\linewidth}{!}{%
%     \resizebox{0.5\textwidth}{!}{%
% \begin{tabular}{@{\;}lcccc@{\;}}
%         \toprule
%         ~ & \makecell[c]{PPO \\ (w/o sup)} & \makecell[c]{Ours \\ (w/ sup.)}  & \makecell[c]{Ours \\ (w/ sup., \\ w/ curri.)} & Ours   \\

%         \midrule % 

%         Time  &  $\sim$1 day & $\sim$5 days & $\sim$13 days & $\sim$4 days
%         \\ 
        
%         \bottomrule
 
%     \end{tabular}
%     }
%     % \vspace{-16pt}
%     \label{tb_exp_timeconsumption}
% \end{wraptable}


% \noindent\textbf{Time consumption.} Table~\ref{tb_exp_timeconsumption} summarizes the total time consumption of different methods on the TACO dataset. Directly training PPO without any supervision is the most efficient approach while the performance lagged behind due to no proper guidance. Exact curriculum learning for acquiring high-quality per-trajectory trackers, though as we observe in Table~\ref{tb_exp_main}, is an effective strategy, which also aligns with conclusions in previous approaches, is extremely time-consuming. For 400 trajectories that we finally select to optimize to produce an action-labeled dataset, searching the exact curriculum costs about 13 days to complete. However, our method requires only slightly more time than the version without tracking prior or curriculum scheduling (Ours w/ sup.). However we can achieve a much higher success rate than this version, taking advantage of the increased diversity and the quality of action-labeled data. 




\vspace{-10pt}
\section{Ablation Studies} \label{sec:ablation_studies}
\vspace{-5pt}


\noindent\textbf{Diversity and quality of robot tracking demonstrations.} 
We propose enhancing the diversity and quality of tracking demonstrations using the tracking controller and homotopy generator. We ablate these strategies by creating two variants: ``Ours (w/o data, w/o homotopy)'', where the dataset is built by optimizing each trajectory without prior knowledge, and ``Ours (w/o data)'', which uses only the homotopy optimization scheme to improve demonstrations. Despite using the same number of demonstrations, both variants produce lower-quality data. As shown in Table~\ref{tb_exp_main}, they underperform compared to our full method, underscoring the importance of data quality in training the controller.
% We propose to improve the diversity and the quality of the robot tracking demonstrations via the tracking controller and the homotopy generator. We ablate these two strategies by creating two variants: ``Ours (w/o data, w/o homotopy)'', where the dataset is built by directly optimizing each trajectory without prior knowledge, and ``Ours (w/o data)'', where we only leverage the homotopy optimization scheme to improve demonstrations. Both strategies, despite using the same amount of demonstrations, result in lower data quality. As shown in Table~\ref{tb_exp_main}, they underperform compared to our full method, highlighting the importance of data quality in training the controller.
% 
% where optimization combines trajectory-specific efforts with the controller's tracking prior.
% We propose transferring tracking knowledge from the controller and curriculum before optimization to handle more difficult and diverse trajectories in the dataset. We ablate these strategies 
% We propose to transfer general tracking knowledge encoded in the tracking controller and the curriculum prior to the optimization process for solving difficult-to-solve problems so that we can include more difficult and diverse labeled trajectories into the dataset. We've tried to ablate such two strategies and create ``Ours (w/o data, w/o structure)'' where the final labeled dataset is created by directly optimizing each trajectory without any prior from the tracking controller or the curriculum scheduler, and ``Ours (w/o data)'' where each trajectory is optimized under the joint effort of the per-trajectory tracking optimization and the tracking prior provided by the tracking controller. Such two strategies, though using the same amount of labeled data as our full method to provide supervision, may fall short in the data quality aspect. As shown in Table~\ref{tb_exp_main}, they would lag behind our full version, illustrating the role of data quality in training the tracking controller. 
% This approach can potentially let us get better per-trajectory tracker under expansive tracking curriculum trail-and-error. However, it is extremely time-consuming and unaffordable. As presented in Table~\ref{tb_exp_main}, Ours (w/ sup.), as restricted by the quality and the diversity of the action-labeled data, typically lags behind our full method on both the GRAB dataset and the TACO dataset. It validates the importance of data quality and coverage used to train the tracking controller. Ours (w/ sup., w/ curriculum) leverages the power of curriculum learning and the benefit of per-trajectory searched accurate curriculum, and can sometimes outperform out proposed strategy, which uses the tracking controller to provide tracking priors and the curriculum scheduler to propose the tracking schedule for tracking difficult trajectories. However, leveraging this approach is highly time-consuming, and would become extremely unaffordable when the number of data increases. Our proposed strategy can not only effectively improve the data quality, but smartly reduces the time consumption. 
% using per-trajectory tracking optimization and the tracking prior from the tracking controller. 
% optimized with per-trajectory searched curriculum, other than the curriculum planned by the scheduler. 
% This demonstrate the effectiveness of the tracking curriculum and the quality of our mined curricula. 
% typically lags behind our full method,
% \todo{xxxx} 
% \todo{for the curriculum, say that the trajectories between TACO is a little bit similar so we can observe the the one with exact curriculum can achieve better performance some times}


\begin{wrapfigure}[13]{r}{0.45\textwidth} % {0pt}
\vspace{-3ex}
\includegraphics[width = 0.45\textwidth]{./figs/scaling.pdf}
\vspace{-30pt}
    \caption{\footnotesize
    Scaling the amount of demonstrations.
    }
    \label{fig:fig_scaling}
% \vspace{-30pt}
\end{wrapfigure}

\vspace{-5pt}
\noindent\textbf{Scaling the number of demonstrations.} 
To investigate the relationship between the tracking controller's performance and the number of demonstrations, we vary the size of the demonstration dataset during training and tested performance on the TACO dataset. Specifically, in the final training iteration, we down-sampled the dataset to 0.1, 0.3, 0.5, and 0.9 of its original size and re-trained the model. As shown in Figure~\ref{fig:fig_scaling}, there is a clear correlation between the amount of demonstrations and model performance. Since the curve has not plateaued, we hypothesize that increasing the amount of high-quality data could further improve performance.
% general tracking controller's performance and the amount of labeled data, 
% To explore the relation between the performance of the general tracking controller and the number of labeled data, we try to vary the number of final labeled data to train the model and test the model's performance on the TACO dataset. To be more specific, in the last training iteration, we try to down-sample the \emph{final action-labeled dataset} into $0.1, 0.3, 0.5, 0.9$ of its original size and re-train the model. As summarized in Figure~\ref{fig:fig_scaling}, there exists an obvious relation between the amount of labeled data used for training and the model's performance. Besides, the curve has not yet reached a flat raising mode in our trails. So we hypothesize that our method's performance could still be enhanced if we can manage to create more high-quality data to train the general tracking controller. 
 
% \todo{a scaling law that demonstrate the role of the data } 

% \todo{time consumption and the time efficiency}


\vspace{-10pt}
% \todo{repro statement}

