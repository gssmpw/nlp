\subsection{Ensemble}
\label{s34}

\subsubsection{Bagging-based ensemble}\
\label{s341}
% In recent years, bagging-based methods have emerged as effective techniques to deal with imbalanced data by manipulating the training set and creating diverse classifiers. 

 
% is an ensemble learning technique that combines multiple base classifiers to form a stronger ensemble learner.
% The concept of bagging 
In the context of imbalanced data learning, Bagging, short for Bootstrap Aggregating, involves generating multiple balanced subsets of the imbalanced dataset by resampling techniques like random undersampling and SMOTE, and training base classifiers on these subsets independently \cite{breiman1996bagging}. The final prediction is obtained through the aggregation of predictions made by each base classifier using majority voting or weighted voting. Apart from rebalancing the class distribution, it aims to counterbalance the bias towards the majority class by introducing diversity among the base classifiers.

Over-Bagging~\cite{ref_22} algorithm replaces the random sampling technique with the random oversampling technique in Bagging algorithm to deal with the imbalance of data. Data balance is achieved by over-sampling the minority class samples. However, it may generate overfitting and redundant classifiers due to the high similarity between and within bags. This is because random oversampling can create duplicate instances. Hence, SMOTE-Bagging was proposed to alleviate the replication issue. 

SMOTE-Bagging~\cite{ref_22} overcomes this disadvantage by using SMOTE to generate synthetic minority class via interpolating between existing ones. It ensures that the synthetic instances are different from the original ones and promote the diversity of bags. By using a varying resampling rate in each iteration, SMOTE-Bagging also ensures that each bag is different from the others, which further increases the diversity of the ensemble. Specifically, in each bag, the minority class instances consist of synthetic instances and resampled instances, where the resampling rate starts at 10\% in the first iteration and increases to 100\% in the last iteration. Therefore, SMOTE-Bagging can generate a more diverse ensemble than Over-Bagging, which can lead to better classification performance on the minority class.

% However, the training set used by the base classifier will be too large as each iteration oversampling is faced with all the majority class samples, this will affect the efficiency of classification learning. The SMOTE Bagging algorithm proposed to solve this problem. 

Similarly, some papers suggest that the Bagging algorithm can balance data by undersampling. Thus, the Under-Bagging \cite{ref_22} algorithm is proposed. On contrary to Over-Bagging, it undersamples the majority class samples in the process of iteratively generating training sets for base classifiers. Repeated sampling with the majority class samples can also obtain a large difference in the base classifier, and compared with Over-Bagging, the number of samples per iteration will be less and the efficiency will be higher. However, in the process of under-sampling, it is easy to neglect most of the useful samples, resulting in inaccurate classification results.

% As we know, The Bagging algorithm is simple to implement and has strong generalization ability, thus some studies have proposed using Bagging to deal with data imbalance. Bagging-based classification algorithm with data processing is simpler than Boosting-based algorithm as it does not need to calculate and update weight. In these algorithms, the over-sampling and under-sampling techniques are introduced to rebalance the data, which ensures the difference of the base classifier and the learning accuracy of the ensemble classifier.

Following this research line, numerous variants have been proposed from different viewpoints. For example, Roughly Balanced Bagging (RBBag) was proposed to generate more diverse bags \cite{hido2009roughly}. 
% That is, in each iteration of RBB, a new bag is created by randomly sampling with replacement from the majority class and sampling without replacement from the minority class. The number of positive examples is kept fixed, while the number of negative examples is drawn from a negative binomial distribution. 
That is, in each iteration of RBBag, the number of positive samples is kept fixed, while the number of negative samples  satisfies negative binomial distribution. The diversity of the bags is achieved by varying the number of negative samples, while the rough balance of the bags ensures both the majority and minority classes are equally emphasized. 
From another perspective, Neighbourhood Balanced Bagging (NBBag) \cite{blaszczynski2015neighbourhood} was designed to modify the sampling probabilities of different instances in a way that focuses on the minority samples that are hard to learn while decreasing the probabilities of selecting samples from the majority class. This is achieved by analyzing the neighbourhood of samples, and taking into account the local minority class distribution. It is still a open question that how to resample instances for more accurate and diverse base classifiers and consequently better overall performance. 

\subsubsection{Boosting-based ensemble}\
\label{s342}

Boosting-based ensemble methods aim to enhance the performance of classifiers on imbalanced data through iterative re-weighting of the training samples, to focus on the minority class. The most widely used boosting-based ensemble methods are AdaBoost \cite{hastie2009multi}, Gradient Boosting \cite{friedman2001greedy}, and XGBoost \cite{chen2016xgboost}. Especially, in AdaBoost, each base classifier is trained on weighted samples. The weights of misclassified samples are increased with each iteration, while the weights of correctly classified samples are decreased. The final classifier is a weighted combination of base classifiers, where the weights are determined by their classification accuracy.

Based on the Adaboost and the SMOTE algorithm, the SMOTE-Boost~\cite{ref_3} method is proposed. The SMOTE oversampling technique is introduced in each iteration. By adding synthetic minority class examples, each base classifier can pay more attention to the minority class. 
% In the SMOTE-Boost algorithm, data balance is achieved by iterating training sets with different weights of the base classifier to improve the difference of the base classifier and the final classification accuracy. 
However, SMOTE-Boost fails to consider the distribution of minority classes and latent noises when it generates synthetic examples, which can be blind and harmful to classification accuracy. To alleviate this problem, the MSMOTE-Boost~\cite{ref_8} algorithm is proposed as improvement. Instead, it uses the MSMOTE algorithm to process the unbalanced data. In MSMOTE, by the distance between samples, the minority class samples are divided into three groups: safety samples, boundary samples, and potential noise samples. For security samples, the synthesis algorithm is the same as that of SMOTE; for boundary samples, the nearest sample is chosen; and for potential noise samples, no operation is performed. From another perspective, RAMOBoost \cite{chen2010ramoboost} emphasizes to consider the learnability of minority class instances. RAMOBoost employs an adaptive sampling strategy at each boosting iteration, to focus on difficult-to-learn minority class instances. The sampling distribution is updated based on the pseudo-loss of the current hypothesis, which is a measure of the error rate of the hypothesis on the training data. The updated sampling distribution gives more weight to examples that are misclassified by the current hypothesis, which can help the algorithm to focus on difficult-to-learn examples.

Corresponding to the over-sampling boosting algorithms mentioned above is the boosting algorithms combined with under-sampling. The key idea of the RUS-Boost algorithm~\cite{ref_20} is to select samples randomly from the majority class samples by using the random undersampling technique (RUS) in the iteration process of the AdaBoost algorithm. Compared with the SMOTE-Boost, this algorithm has the advantages of simple implementation and short training time, but it is possible to remove many potentially useful samples in under-sampling. To avoid this problem, the EusBoost~\cite{ref_21} algorithm is proposed. This algorithm uses the evolutionary under-sampling technique, which chooses the most representative samples in the majority class samples to get more informative dataset, and it introduces the fitness function to ensure the difference of the base classifier.
Another improvement to the RUSBoost algorithm is RHSBoost. 
The key idea of RHSBoost \cite{gong2017rhsboost} is to combine the advantages of RUS and ROSE (Random Over-Sampling Examples) \cite{lunardon2014rose} sampling to generate more balanced and diverse subsets. Similar to SMOTE, at each boosting iteration, a weighted ROSE sampling algorithm is first used to oversample the minority class. Then, RUS is used to further balance the class distribution by randomly removing some instances from the majority class.
Note that ROSE and SMOTE differ in the way they generate synthetic examples. SMOTE generates synthetic examples by interpolating, while ROSE samples from a multivariate kernel density estimate of the minority class, which can maintain the characteristics of the minority class and help avoid overfitting. 
Apart from those algorithms mentioned above, there are also many boosting-based methods proposed in recent years, such as LIUBoost \cite{ahmed2019liuboost} and HUSBoost \cite{popel2018hybrid}, showing boosting-based methods are attractive and promising for imbalance data learning especially with undersampling technique.



\subsubsection{Cost-sensitive ensemble}\
\label{s343}

% Cost-sensitive ensemble methods are a family of techniques that aim to address the class imbalance problem by incorporating the cost of misclassification into the ensemble learning process. 
In scenarios such as fraud detection and medical diagnosis, the misclassification cost of different classes could vary significantly, leading to potential risks or financial losses when traditional methods applied, which simply minimizes the average error rate or maximizes overall accuracy \cite{ling2008cost}.
% Therefore, it becomes crucial to develop techniques that can take into account these costs and make more informed decisions during the classification process.
The key idea of cost-sensitive ensemble methods is to incorporate cost considerations into the construction and combination of base classifiers. To be more specific, these proposals usually differ in the way that they modify the weight update rule.

Among this family, AdaCost \cite{fan1999adacost}, CSB0, CSB1, CSB2 \cite{ting2000comparative}, AdaC1, AdaC2, and AdaC3 \cite{sun2007cost} are the most representative approaches. 
AdaCost introduces a cost adjustment function into the weight updating rule of AdaBoost. This adjustment function increases the weights of costly misclassifications more aggressively, and decreases the weights of costly correct classifications more conservatively. Theoretical analysis also provides an tighter upper bound on the cumulative training misclassification cost than AdaBoost.

CSB0 modifies the weight update rule by multiplying the weight update term by a dedicated cost function parameterized by both classifier prediction and groundtruth. CSB1 improves this vanilla version by taking the confidence of the base learner's prediction into account. 
CSB2, on the other hand, incorporates an additional factor into the weight update rule. This factor is a function of the misclassification cost and the prediction made by the base learner, i.e. the weight of current base learner in cost-sensitive scenario. By utilizing more informative factors, CSB2 is able to further minimize high cost errors.

Likewise the CSB family, AdaC1, AdaC2 and AdaC3 mainly differ in their weight update rules. In AdaC1, the cost function is introduced within the exponent part, multiplying by the weight of base classifier.
But in AdaC2, the cost function is introduced outside the exponent part as another possible attempt. In AdaC3, this modification considers the idea of AdaC1 and AdaC2 at the same time. The weight update formula is modified by introducing the cost function both inside and outside the exponent part. Note that with the change of the cost function, the weight assignment rule for base classifiers is also changing. These details are omitted for simplicity.

% These methods aim to optimize the ensemble such that it minimizes the expected cost of misclassification over the dataset, rather than simply minimizing the error rate or maximizing accuracy.


\subsubsection{Ensemble knowledge distillation} \
\label{s344}

In the era of deep learning, numerous conventional machine learning models such as SVM, KNN, and DT have been replaced by deep learning models, which offer higher accuracy. However, these deep learning models come with a significant drawback: they require substantial computational resources and storage consumption. To address this issue, knowledge distillation was introduced as a compression technique \cite{gou2021knowledge}. 
% Ensemble knowledge distillation, an extension of traditional knowledge distillation, was further proposed in the context of ensemble learning.
Ensemble knowledge distillation extends the concept of traditional knowledge distillation \cite{asif2019ensemble}. Instead of transferring knowledge from a single teacher model, ensemble knowledge distillation transfers knowledge from an ensemble of teacher models. By leveraging the collective knowledge of the ensemble, the student model learns to mimic the collective behavior, resulting in improved robustness and accuracy.

Supervision signals from teacher models can include both high-level and intermediate-level information, namely, soft labels and intermediate layer representations, where the latter is generally considered more informative. 
The diverse capabilities of teachers necessitate selecting the most suitable supervision signals or assigning importance weights based on their expertise. 

For example, in \cite{chebotar2016distilling}, student model is simply distilled with the weighted average of teacher models, where weights are fixed as hyperparameters.  And in \cite{fukuda2017efficient}, two strategies are investigated for utilizing labels from multiple teachers. The first one keeps the same with \cite{chebotar2016distilling}, while the second randomly selects a teacher model at the mini-batch level. For more flexible and finer grained weights, AMTML-KD \cite{liu2020adaptive} associates each teacher with a latent representation to adaptively learn instance-level weights, and gather intermediate-level hints from teachers using a multi-group based strategy. Concurrently, RL-KD also proposes to assign instance-level weights through a reinforcement learning process \cite{yuan2021reinforced}. In RL-KD, the agent receives training instances and applies the teacher selection policy for distillation, where the reward is defined as the performance of the trained student model. Apart from weight assignment, CTKD \cite{zhao2020highlight} designs two dedicated teachers in a collaborative manner. Specifically, one teacher trained from scratch assists the student step by step using its temporary outputs, and the other pre-trained expert teacher guides the student to focus on a critical region. Inspired by the pre-training and fine-tunning paradigm, TMKD \cite{yang2020model} first pre-trains the student, and further fine-tune the pre-trained model on downstream tasks. Both stages reduce the overfitting bias and transfer general knowledge to the student model. 
Note that ensemble knowledge distillation can be also combined with other technique such as parameter pruning and network quantization for more compact model. For instance, \cite{pham2023collaborative} proposes a novel framework that leverages both multi-teacher knowledge distillation and network quantization for learning low bit-width DNNs.

% In recent years, ensemble of classiﬁers have arisen as a possible solution to the class imbalance problem, it attracts great interest among researchers. In this section, our aim is to review the application of ensemble learning methods to deal with the unbalanced data classification. According to the techniques they used, we can distinguish three different families among ensemble approaches for imbalanced learning. we consider boosting- and bagging-based ensembles, and the last family is formed by hybrids ensembles. That is, ensemble methods that apart from combining an ensemble learning algorithm and a preprocessing technique, make use of both boosting and bagging, one inside the other, together with a preprocessing technique.



% As we know, The Bagging algorithm is simple to implement and has strong generalization ability, thus some studies have proposed using Bagging to deal with data imbalance. Bagging-based classification algorithm with data processing is simpler than Boosting-based algorithm as it does not need to calculate and update weight. In these algorithms, the over-sampling and under-sampling techniques are introduced to rebalance the data, which ensures the difference of the base classifier and the learning accuracy of the ensemble classifier.

% Over-Bagging~\cite{ref_22} algorithm replaces the random sampling technique with the random oversampling technique in Bagging algorithm to deal with the imbalance of data. Data balance is achieved by over-sampling the minority class samples. However, the training set used by the base classifier will be too large as each iteration oversampling is faced with all the majority class samples, this will affect the efficiency of classification learning. The SMOTE Bagging algorithm proposed to solve this problem.

% Similarly, some papers suggest that the Bagging algorithm can balance data by undersampling. Thus, the Under-Bagging algorithm is proposed. On contrary to Over-Bagging, it under-sampling the majority class samples in the process of iteratively generating training sets for base classifiers. Repeated sampling with the minority class samples can also obtain a large difference in the base classifier, and compared with Over Bagging, the number of samples per iteration will be less and the efficiency will be higher. However, in the process of under-sampling, it is easy to neglect most of the useful samples, resulting in inaccurate classification results.

% Following Bagging's research path

% Some hybrid ensemble methods have also been proposed to solve the unbalanced data classification, that is, they combine both bagging and boosting (also with a preprocessing technique). Easy Ensemble and Balance Cascade~\cite{ref_7} Use Bagging as the main ensemble learning method, but despite training a classiﬁer for each new bag, they train each bag using AdaBoost. Hence, the ﬁnal classiﬁer is an ensemble of ensembles. In the same manner as Under Bagging, each balanced bag in these two methods is constructed by randomly under sampling instances from the majority class and by the usage of all the instances from the minority class.

% The number of the balanced dataset generated in Easy Ensemble algorithm is constant in the iteration process, thus it can be trained in parallel. while in Balance Cascade algorithm, most of the samples correctly classified by the previous round of base classifiers are deleted, thus the serial training of base classifiers is carried out. Because of this deletion mechanism, we can avoid ignoring most of the useful samples in random under-sampling. Besides, we can also use the advantages of the AdaBoost algorithm to effectively reduce model deviation and Bagging algorithm to effectively reduce model variance.
