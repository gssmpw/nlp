\subsection{Feature representation}
\label{s32}
In addition to rebalancing classifier training, obtaining a high-quality and distinctive feature representation for data is essential for mitigating the impact of class imbalance on model performance. Several techniques can be employed to enhance feature representation in imbalanced learning, such as cost-sensitive learning, metric learning, supervised contrastive learning, prototype learning, transfer learning, and meta-learning.

\subsubsection{Cost-sensitive learning}\
\label{s321}

Cost-sensitive learning has emerged as a highly effective approach for tackling the imbalance challenges. 
The foundation of cost-sensitive learning is rooted in the concept of cost matrices~\cite{ref_A}. In contrast to conventional machine learning algorithms that treat all classification errors equally, cost matrices introduce the pivotal notion of assigning predefined costs to misclassified samples, i.e., false positives and false negatives. These assigned costs effectively function as weight factors, elevating certain misclassification errors above others in terms of importance. The overarching objective is to minimize the overall expected cost, thereby steering models to place heightened emphasis on minority classes, transcending the mere fixation on average classification errors. In essence, the concept of "costs" embodies the role of "weights" in the context of reweighting errors, endowing models with the capacity to adapt their decision boundaries and accord priority to specific classes based on the associated costs {\footnote{In this survey, we utilize the terms ``cost'' and ``weight'' concurrently, following the method used in the original paper.}}.


In the early stages, methods focus on class-specific costs and entail assigning distinct costs or weights to different classes, with these predefined costs typically established based on experiential knowledge or domain expertise~\cite{ref_A, ref_C}. The representative method is MetaCost~\cite{ref_B}, a general approach aimed at enabling non-cost-sensitive algorithms to incorporate cost-sensitive considerations. The key innovation lies in the re-labeling procedure, which begins by training multiple models on bootstrap replicates of the training dataset. Subsequently, the samples are re-labeled based on the minimized cost function. Finally, the classifier is trained using these refreshed labels. Another approach, threshold-moving~\cite{zhou2005training}, involves adjusting the output threshold towards the less expensive classes. This adjustment makes it more challenging to misclassify examples with higher associated costs. This method employs the training set to train a neural network, and cost sensitivity is introduced during the test phase by directly modifying the output logits based on the associated costs. In addition to binary classification, Zhou et al.~\cite{zhou2010multi} explored the application of cost-sensitive learning in multi-class classification tasks. They observed that the consistency of cost matrices has a significant impact on classification performance. In cases where the cost matrix lacks consistency, rescaling is recommended, particularly after decomposing the multi-class problem into a series of two-class problems. CSMLP~\cite{castro2013novel} presents a novel approach involving the design of a joint objective function for Multilayer Perceptrons. This method utilizes a single cost parameter to differentiate the importance of class errors. The class-balanced loss~\cite{cui2019class} emphasizes that as the number of samples increases, the additional benefit of adding a new data point diminishes. To address this, it introduces the concept of the effective number to approximate the expected class size and leverages this effective number to calculate weights, rather than relying solely on the sample quantity.



Besides the class-specific costs, cost-sensitive learning extends its scope to encompass the weighting of individual samples. Focal loss~\cite{lin2017focal} offers a perspective on addressing imbalance problems at the instance level, introducing varying weights for each training sample. Given that class imbalance often amplifies the difficulty of predicting minority classes, focal loss assigns higher costs to more challenging samples while applying lower costs to those that are easier to classify during training. 
Influence-balanced loss~\cite{park2021influence} takes a different approach by employing influence functions to gauge the impact of individual samples on intricate and biased decision boundaries. The loss associated with each sample is proportionally weighted based on the inverse of its influence.
Balanced softmax~\cite{ren2020balanced} extends the conventional softmax function to address imbalanced distributions. It accomplishes this by incorporating the imbalanced conditional prediction probability. This generalized softmax function serves as a form of logit calibration, effectively adjusting each class's logit value in proportion to its label frequencies, thus providing a more balanced representation in the feature space.


Another critical aspect of cost-sensitive learning revolves around the challenge of determining appropriate weights. Predefined weights often necessitate careful tuning of additional weight hyperparameters, introducing complexity to the process. To address this issue, researchers have explored diverse avenues for automatically learning the weights, considering various perspectives, including evolutionary search, gradient-based techniques, distribution transport, and meta-learning.
EDS~\cite{cao2013novel} employs an evolutionary search method to effectively seek optimal misclassification costs in multiclass imbalance scenarios based on an objective function defined with G-mean. This approach optimizes cost parameters through an evolutionary strategy.
ECS-DBN~\cite{zhang2018cost} utilizes adaptive differential evolution to optimize misclassification costs based on the training data. This enhances the effectiveness of deep belief networks by incorporating evaluation measures into the objective function.
% CoSen~\cite{khan2017cost} jointly optimizes the class-dependent costs and the neural network parameters to automatically learn robust feature representations for both the majority and minority classes. 
For gradients-based methods, GDW~\cite{chen2021generalized} addresses the imbalance problem through class-level gradient manipulation. It efficiently obtains class-level weights by developing a two-stage weight generation scheme that involves storing intermediate gradients. 
Equalization Loss~\cite{tan2020equalization} and Equalization Loss v2~\cite{tan2021equalization} determine weights from gradients as well. While Equalization Loss protects the learning of minority classes by blocking some negative gradients, it acknowledges that positive gradients may still be overwhelmed by the negative ones. Therefore, Equalization Loss v2 proposes balancing the positive-to-negative gradient ratio, using the accumulated gradient ratio as an indicator to up-weight positive gradients and down-weight negative gradients.
OT~\cite{guo2022learning} approaches the issue by viewing the training set as an imbalanced distribution over its samples. It transports this distribution to a balanced one using optimal transport. The weights of the training samples are determined as the probability mass of the imbalanced distribution, learned by minimizing the optimal transport distance between the two distributions. 
% AREA~\cite{chen2023area} argues that the number of samples in a class can not accurately measure the size of its spanned space and uses the estimated size of the spanned space for each category to adjust the training weight.
Besides, meta-learning methods are utilized to learn optimal weights through bi-level optimization as well. For instance, LRE~\cite{ren2018learning} presents a novel meta-learning algorithm that learns to assign weights to training examples based on their gradient directions. Meta-Weight-Net~\cite{shu2019meta} automatically learns an explicit loss-weight function, parameterized by a multilayer perceptron, from the data.



\subsubsection{Metric learning}\
\label{s322}
% \cite{Ren_2022_CVPR}
% \cite{Li_2022_CVPR}
% \cite{park2021influence}

Metric learning aims to learn a discriminative feature space by designing task-specific distance metrics between samples, which makes samples of the same class closer together, while samples of different classes are further apart \cite{duan2017deep}. It achieves great success in feature representation due to the powerful capacity of understanding the similarity relationship among samples. Thus, metric learning methods has been wildly applied to mitigate the imbalanced classification problems to increase the separability between minority and majority classes and achieve a better classification performance on minorities. 

\textbf{Sample selection.}
In metric learning methods, sample selection is the most important role because it directly influences the distances calculation and the quality of the extracted similarity relationship. Therefore, the sample selection of metric learning methods for imbalanced data always emphasizes the minority class \cite{huang2016learning, dong2017class, wang2018iterative, gui2022quadruplet, yan2023borderline} and carefully selects samples to design the distance metrics.  

To balance the training samples used in the training procedure, LMLE \cite{huang2016learning} clusters samples in each class and construct the sample sets by selecting four representative samples from neighboring clusters and classes for each anchor sample. Benefiting to clustering, LMLE captures the significant data variability within each class and greatly enriches the diversity of minority sample sets for distance evaluation. According to four distances defined on the selected sample sets, the triple-header hinge loss constrains the feature representation and enforces a local margin while handling class imbalance problems. 
However, the clustering is sensitive to the entire training data and class size and usually gets limited results for extremely sparse minority classes. 
To address this problem, class rectification loss \cite{dong2017class} construct the triplets by identifying hard-positive and hard-negative samples for minorities in each training batch and designs a batch-wise hard mining metric based on relative, absolute, and distribution information among selected triplets. By constraining hard-positive samples, it can discover and expand sparsely sampled minority class boundaries, while mining hard-negative samples can improve the margins of minority class boundaries that are corrupted by similar imposter classes, thereby mitigating both intra-class compactness and inter-class sparsity in feature space.
LSTM-QDM \cite{gui2022quadruplet} utilizes the minority sample to expend the data triplet in the triplet network \cite{cheng2016person} and constructs a quadruplet loss to balance the relationship among anchor, positive, negative and minority samples. Benefiting from the extra minority samples in quadruplet loss and optimizing the softmax loss jointly, LSTM-QDM achieves a stronger representation ability for imbalanced imbalanced time-series data.
Inspired by Borderline-SMOTE, DMFBML \cite{yan2023borderline} separates the samples into three categories: danger, safe and noise based on the ratio of neighbors from the different classes and generates the triplets by the guidance of three categories. Afterward, DMFBML designs a borderline margin loss function to customize the margins among different categories and assign more metric constraints for minority samples, thereby the minority and majority samples in overlapping regions could be separated.


\textbf{Distance calculation.}
Besides introducing minority samples in the sample selection strategy, other works handle the imbalance data and constrain the minority class from distance calculation.
Range loss \cite{zhang2017range} explicitly defines intra-class loss as the harmonic mean of the k-largest distances between samples within each class, and constructs inter-class loss by measuring the minimum distance among class centers. By optimizing these two components, range loss can simultaneously reduce the intra-class variations of different classes and enlarge the inter-class distance.
To treat different classes equally and separate them balanced, DMBK \cite{feng2018learning} first assumes that different classes are sampled from Gaussian densities with different expected values but one identical covariance, and then defines normalized divergence using KL-divergence to represent inter-class divergence. Afterword, inspired by arithmetic-geometric average inequality, DMBK makes use of the geometric mean to constrain the normalized divergences and make all inter-class divergences balanced. This procedure separates all classes in a balanced way and avoids neglected important minorities and inaccurate similarities incurred by imbalanced class distributions.
The Affinity loss function \cite{hayat2019gaussian} proposes to measure class similarities for an input feature in Euclidean space using a Gaussian similarity measure, specifically in terms of Bergman divergence. Additionally, it utilizes a diversity regularizer to constrain all class centers to be uniformly distributed throughout the feature space. This approach helps the model avoid the majority classes from occupying a disproportionately larger angular space compared to the minority classes, leading to a more balanced embedding space.
Imbalanced metric learning \cite{gautheron2020metric} designs a new weighting method for Mahalanobis distance-base metric learning methods by decomposing the pair of samples based on their labels and introduces the hyper-parameters to control the negative effect of the imbalance. Moreover, imbalanced metric learning derives a generalization bound theoretically involving the proportion of positive samples using the uniform stability framework. 



% \textbf{Margin adjustment}
% LDAM \cite{cao2019learning} extends the existing soft margin loss to imbalanced data scenarios by encouraging the minority classes to have larger margins so that the model can improve the generalization error of minority classes. It starts from the classification error bound and theoretically obtain the optimal trade-off of margins among multiple classes which is inversely proportional to the fourth root of the number of examples in each class. With the optimal margins, the LDAM can be extend to any sort of classification loss functions, such as Hinge loss and Cross entropy loss.
% NSL-AAM \cite{} analyses the softmax loss in imbalance setting and proposes an adaptive angle margin penalty which is added to the angle between sample embeddings and the weights of the last linear layer. The angle margin penalty of the class is proportional to the effective sample number of the class, and the larger effective numbers of the class is, the larger angle margin penalty the class suffers.



\subsubsection{Supervised contrastive learning}\
\label{s323}
% BaCon \cite{kang2022comprehensive} 
% PaCo \cite{cui2021parametric} state of the art

Supervised contrastive learning has emerged as a promising research direction in recent years, combining metric learning and self-supervised contrastive learning to leverage the advantages of both methods while overcoming their respective limitations \cite{khosla2020supervised}. 
On the one hand, conventional metric learning methods (e.g., the triplet loss \cite{weinberger2009distance} and N-pair loss \cite{sohn2016improved}) and self-supervised contrastive learning \cite{tian2020contrastive} limit the number of positive samples for each anchor to one, which places more emphasis on optimizing individual pairs and hinders the extraction of richer inter-sample information \cite{khosla2020supervised}. 
On the other hand, while contrastive learning methods have been found to generate a balanced feature space and maintain stable performance, even when the degree of imbalance becomes increasingly severe \cite{yang2020rethinking, kang2021exploring}, the features learned through contrastive learning lack semantic separability since they are learned without class label guidance, leading to poor classification performance. 
To address these limitations and handle the imbalance problem, supervised contrastive learning methods incorporate label information into the contrastive learning process and extend positive examples by including other samples from the same class along with data augmentations. This encourages the model to produce closely aligned representations of all samples from the same class and results in a more robust clustering of the representation space. 

SupCon \cite{khosla2020supervised} first extends contrastive learning to the fully-supervised setting and surpasses the performance of the traditional supervised cross-entropy loss by leveraging class labels with a contrastive loss. Although achieving performance improvement, researchers discover that the supervised contrastive loss still suffers from the imbalance of positive/negative pairs. To mitigate this issue and further enhance supervised contrastive learning for imbalanced data, the improved methods can be categorised into three classes, including {re-balanced sampling, re-weighting, and pair reconstruction}.


\textbf{Re-balanced sampling} methods aim to address the issue of imbalanced positive/negative pairs by sampling techniques. 
KCL \cite{kang2021exploring} improves upon the positive set construction rule in SupCon by randomly selecting k instances for each anchor from the same class to form the positive set, instead of using all the instances from the same class. This modification ensures that there is an equal number of positive samples for each anchor sample, thereby avoiding the dominance of instance-rich classes in the representation learning process.
CA-SupCon \cite{zhang2022class} argues that due to the scarcity of minority samples, SupCon cannot guarantee the separability of minority classes in one mini-batch. To address this issue, CA-SupCon introduces a class-aware sampler that re-balances the data distribution within each mini-batch during training, thus allowing each minority class to have a fair chance to contribute to the feature distance optimization.
GPCL \cite{jiang2021guided} introduces self-training to supervised contrastive learning to provide pseudo-labels for unlabeled data. By incorporating these pseudo-labels with confidence scores, GPCL can make full use of unlabeled data to enrich the contrastive data while filtering out unreliable negative and positive pairs with the same pseudo-labels and low confidence scores in the contrastive loss, respectively.


\textbf{Re-weighting} methods aim to increase the importance of minority pairs in the contrastive loss. For example, PaCo \cite{cui2021parametric} introduces a set of parametric class-wise learnable centers into the contrastive loss. This approach adaptively weighs the samples from different classes and enhances the intensity of pushing samples of the same class closer to their corresponding centers, effectively giving more weight to the minority class pairs.
BCL and TSC \cite{zhu2022balanced, li2022targeted} argue that supervised contrastive learning should ideally result in an embedding where different classes are uniformly distributed on a hypersphere. However, applying the contrastive loss to imbalanced data can lead to poor uniformity, with minority classes having poor separability in the feature space. 
BCL provides evidence for this claim through a theoretical analysis of the lower bound of the contrastive loss. The analysis shows that imbalanced data distribution heavily affects the model and the resulting representations of classes no longer attain a regular simplex configuration, which ultimately hinders the final classification performance.
Therefore, to address this issue, BCL proposes three class-averaging solutions to re-weight the negative pairs in the supervised contrastive loss. This helps to reduce the gradients from negative majority classes and contributes to an asymmetrical geometry configuration. Additionally, to ensure that all classes appear in every mini-batch, BCL introduces class-center representations in the positive and negative pairing, compensating for the batch sampling bias of high-frequency classes.Although motivated by the same insight, it is worth noting that TSC proposes a distinct solution from BCL. TSC aims to maintain a uniform distribution in the feature space for all classes by urging the features of classes closer to the uniformly distributed target features on the vertices of a regular simplex. To maintain semantic relations among classes, TSC proposes an online matching method to ensure that classes that are semantically close to each other are assigned to targets that are also close to each other.

 
\textbf{Pair reconstruction.} 
In addition to re-balanced sampling and re-weighting techniques, researchers also attempt to leverage the {pair reconstruction} in supervised contrastive learning to mitigate the data imbalance.
SNP-ECC \cite{gao2022ensemble} and MLCC-IBC \cite{gao2023imbalanced} extend the point-to-point contrastive learning approach to a point-to-group approach, and mitigate the imbalance problem by enriching the training samples. They construct contrastive sample groups by randomly sampling from the k-nearest majority/minority neighbors of the anchor sample, respectively. Afterwards, the new-format training data are generated by pairing the anchor sample with various contrastive sample groups. Due to the increased diversity of the contrastive sample groups, the new training data pairs are balanced and are learned using a novel label matching task.
ProCo \cite{yang2022proco} proposes to utilize the category prototype to augment representative samples and generate the adversarial proto-instance via linear interpolation in the feature space. The adversarial proto-instance is designed as a special outlier that can balance the data and encourage ProCo to rectify the decision boundaries of the minority categories during contrastive learning.



\subsubsection{Prototype learning}\
\label{s324}

In addition to exploring correlations among extensive and complex samples via metric learning and supervised contrastive learning, researchers have incorporated the idea of prototype learning \cite{snell2017prototypical} from few-shot learning into the imbalanced data scenario to enhance feature representations of minority classes.
A prototype explicitly represents a summary of all instances belonging to a specific class or intrinsic sample characteristics. In the context of imbalanced data, prototypes can improve minority representation from three aspects: {facilitating the generation of a more discriminative feature space}, {compensating for class information compensation}, and {aiding in the extraction of intrinsic sample information}.


\textbf{Discriminative embedding generation.}
Class-specific prototypes provide a central target for each class in the feature space. This enables sample embeddings explicitly converge toward the class center, contributing to a more distinctive feature space for classification. 
Hybrid-PSC \cite{wang2021contrastive} improves the supervised contrastive learning method by treating the prototypes as the pairing object for positive/negative pairs, forcing each sample to be pulled towards the prototypes of its class and pushed away from prototypes of all the other classes. This approach allows for more robust and efficient data sampling when dealing with large-scale datasets under a limited memory budget 
The Affinity loss function \cite{hayat2019gaussian} provides a theoretical analysis of the Softmax loss with a linear prediction layer and treats the linear weight vectors as the class prototypes. Based on this, the affinity loss function can automatically learns the prototypes for each class and conveniently measures feature similarities with class prototypes in Euclidean space using Bergman divergence.
Instead of calculating prototypes from labeled data, TSC \cite{li2022targeted} manually generates a set of targets uniformly distributed on a hyper-sphere as the class prototypes and forces all classes to converge to these distinct distributed targets during training for a uniform distributed feature space.
BCL \cite{zhu2022balanced} uses supervised contrastive loss to train the model, and specifically ensures that each sample is paired with all other class prototypes in the negative pairing. By doing so, BCL compensates for the batch sampling bias of high-frequency classes, which in turn enables comprehensive contractive information from each class to be captured during training.
It is worth noting that the number of class prototypes is not limited to one, and studies \cite{wang2021contrastive, hayat2019gaussian} have shown that adopting a multi-prototype paradigm can capture richer within-class data distribution and promote diversity and discriminability in the feature space.

\textbf{Class information compensation.}
As a summary of classes, class prototypes contain basic class information that can help compensate for the information shortage of minority classes by transferring knowledge across classes or generating minority samples directly through data augmentation.
OLTR \cite{liu2019large} utilizes an attention mechanism to adaptively merge all prototypes with each sample embedding. This helps transfer basic class concepts among all classes to the feature embeddings, thereby enhancing the minority embeddings.
PAN \cite{zeng2021pan} addresses the modality-imbalance problem, where the available data for text, image, and audio are all considerably imbalanced, by generating synthetic samples for the minority modalities. It learns semantically consistent prototypes that capture the semantic consistency of the same class across different modalities and uses a gated recurrent unit to fuse the nearest neighbors of the majority modality to generate new samples. 
MPCL \cite{fu2022meta} argues that empirical prototypes are severely biased due to the scarce data in minority classes and proposes a meta-learning method to calibrate the prototypes by contrastive loss. With the assumption that feature embeddings for each class lie in a Gaussian distribution, MPCL estimates the mean and covariance of class distribution by the calibrated prototypes. Then, MPCL samples new samples from the distribution as synthetic minorities to balance the classes.
Similarly, ProCo \cite{yang2022proco} proposes to utilize the class prototype to augment representative samples and generate the adversarial proto-instance to balance the data. This encourages ProCo to rectify the decision boundaries of the minority classes.

\textbf{Critical information extraction.}
Prototypes not only represent entire classes but can also be leveraged to extract intrinsic information from individual samples, which is crucial in handling typical characteristics of different classes. 
For instance, IEM \cite{zhu2020inflated} proposes to use the attention mechanism to learn multiple discriminative feature prototypes for each class, considering both local and global features, as a single prototype may not be sufficient to represent a class. By gradually updating these prototypes during the training procedure, IEM captures the representative local and global semantics of each class and incorporates more discriminative features to improve generalization on minority classes.
MPNet \cite{he2020learning} generates both instance and semantic prototypes shared by all samples, representing local representative features and semantic features, respectively. The learned prototypes are recorded in a memory module and used to represent each sample, addressing the issue of forgetting minority classes during training. Additionally, semantic memory regularization is applied for a separately distributed centroid space.




\subsubsection{Transfer learning}\
\label{s325}

Transfer learning is a technique that leverages salient features and patterns identified in a source domain to enhance performance in a target domain. In the context of imbalanced datasets, minority classes often suffer from a scarcity of labeled data, while majority classes benefit from an abundance of such data. Consequently, researchers concentrate on identifying shared characteristics or attributes among multiple classes and employ transfer learning to transfer common knowledge from well-represented majority classes to underrepresented minority classes.

\textbf{Sample synthesis.}
A substantial portion of transfer learning methods focus on transferring the majority class diversity to minority classes by synthesizing new minority samples.
For instance, FTL \cite{yin2019feature} presents a prototype-based transfer framework, predicated on a Gaussian prior encompassing all classes. Under this assumption, each class is partitioned into mean and variance, representing class-specific and class-generic factors, respectively. The shared variance of major classes, indicating commonalities between categories such as varying poses or lighting conditions, is conveyed to minor classes through the generation of synthetic samples.
LEAP \cite{liu2020deep}models the distribution of intra-class features based on the distribution of angles between features and their corresponding class centers. By assuming that these angles adhere to a Gaussian distribution, the angular variance learned from the majority class is transferred to each minority class, and synthetic samples for minority classes are drawn from the distribution with enhanced intra-class angular diversity.
Similarly, FSA \cite{chu2020feature} employs class activation maps to identify class-specific features from underrepresented classes and fuses them with class-generic features to generate minority samples. RSG \cite{wang2021rsg} devises a rare-class sample generator module, compatible with any backbone model, to create new samples based on the variation information of majority classes.
In contrast to the aforementioned methods that generate samples from minority instances, M2m \cite{kim2020m2m} directly transfers the diversity of majority classes by translating majority samples into minority samples. By referring to the output of the pre-trained classifier, a small optimized noise is added to the majority sample to facilitate the translation. In this manner, the richer information of majority samples is effectively leveraged to augment the minority classes.

In addition to sample synthesis, several methods endeavor to transfer knowledge in alternative ways.

MetaModelNet \cite{wang2017learning}, rather than transferring majority class knowledge directly, conveys the changing trend of model parameters and introduces a meta-learning method to learn the model dynamics, predicting how model parameters will alter as more training data is progressively incorporated. Specifically, MetaModelNet is trained to forecast many-shot model parameters from few-shot model parameters, which are trained on abundant and limited samples, respectively. By employing the learned model dynamics, MetaModelNet can directly enhance the model parameters trained on minority instances by predicting the parameters when additional minority samples are included.

OLTR \cite{liu2019large} explicitly constructs class centroids and exploits the attention mechanism to amalgamate sample features with class information from various classes, yielding more comprehensive feature representations. This information aggregation not only encompasses minority class information but also transfers basic class concepts among majority classes. Furthermore, the attention mechanism enables the model to adaptively tailor the importance of class information for different samples during end-to-end training, significantly enriching the minority features that lack adequate supervision signals.


GistNet \cite{liu2021gistnet} posits that all classes share a common geometric structure in the feature space. With numerous samples in the majority class, GistNet can encode the class geometry into a set of structure parameters shared by all classes, and the geometric structure of the minority class is restricted via learned structure parameters.

RVGAN-TL \cite{ding2023rvgan} initially generates minority samples using GAN and subsequently applies the TrAdaboost algorithm \cite{tradaboost} to assign different weights to the training samples. Synthetic minority samples that deviate from the real sample distribution are assigned extremely low weights, effectively transferring the distribution of real samples to the minority class and reducing the impact of noisy data.

% \cite{li2021self}

% Add this as the applocation or future work
% Note that some related works focus on addressing the domain class imbalance problem in transfer learning \cite{9655605, 8215613, 8260654}, which occurs when the class distributions between the source and target domains are significantly different. It is important to distinguish the domain class imbalance problem from the class imbalance issue studied in this survey.



\subsubsection{Meta-learning}\
\label{s326}

Meta-learning, often referred to as the process of learning to learn, aims to create a model capable of effectively generalizing across a variety of tasks rather than concentrating on instances from a single task. This approach leverages acquired meta-knowledge to address the shortage of training data for individual tasks and has been investigated for adapting models to imbalanced data distributions. Meta-learning methods achieve this goal through bi-level optimization, which consists of both inner and outer loops. The inner loop focuses on task-specific learning by training a model to excel at individual tasks. Meanwhile, the outer loop emphasizes learning across tasks and updates the model based on its performance on the inner loop tasks. To enhance the model's generalization and adaptability to imbalanced data distributions, researchers propose to optimize the model from multiple perspectives, which includes {weights, sampling method, model dynamics, data augmentation and class prototypes}.

\textbf{Weights.}
The majority of meta-learning-based methods \cite{ren2018learning, shu2019meta, lee2020l2b, jamal2020rethinking, zhang2021learning, liu2021improving} concentrate on re-weighting methods for imbalanced data. These studies highlight the limitations of infeasible manually designed weighting functions and the challenges associated with applying hyper-parameters in loss functions. Consequently, these methods treat the weights for training samples as learnable parameters and optimize them accordingly.

LRE \cite{ren2018learning} first uses a clean and unbiased dataset as the meta set (i.e., development set) to guide the training procedure. By performing validation at every training iteration, LRE dynamically assigns importance weights for each batch sample.
Meta-Weight-Net \cite{shu2019meta} automatically learns an explicit loss-weight function, parameterized by a multi-layer perceptron (MLP) from data, resulting in more stable learning performance. 
Bayesian TAML \cite{lee2020l2b} learns a set of class-specific scalars to weight the learning rate of class-specific gradients for each inner loop optimization step, thus giving more attention to minority classes. 
MLCW \cite{jamal2020rethinking} analyzes the imbalance problem from a domain adaptation perspective. They assume that the class-conditioned distribution of training data and test data is not the same, especially for minority classes with insufficient training data. Therefore, MLCW proposes to explicitly account for the differences between conditioned distributions and utilizes meta-learning to estimate the conditional weights for each training example. 
FSR \cite{zhang2021learning} argued against the expensive second-order computation of nested optimizations in meta-learning and proposes a fast sample re-weighting method. By learning from history to build proxy reward data and employing feature sharing, FSR effectively reduces optimization costs.

\textbf{Sampling method.}
Besides learning adaptive weights for training sets, meta-learning method is deployed to learn a sampling method to re-balance the training data. Balanced Meta-Softmax \cite{ren2020balanced} introduces a balanced softmax method and argues that balance sampling for the mini-batch sampling process might lead to an over-balancing problem. In this scenario, minority classes dominate the optimization process, resulting in local minimums that favor these minority classes. To address this issue, Balanced Meta-Softmax introduces a meta sampler to explicitly learn the optimal sample rate for each class under the guidance of class-balanced meta set.
Instead of co-optimized with the classification model, MESA \cite{liu2020mesa} decouples the model-training and meta-training process and proposes a meta sampler that serves as an adaptive under-sampling solution embedded in the iterative ensemble training process. By incorporating the classification error distribution of ensemble training as its input, MESA achieves high compatibility and is applicable to various statistical and non-statistical classification models, such as decision trees, Naïve Bayes, and k-nearest neighbor classifiers.

\textbf{Model dynamics.}
MetaModelNet \cite{wang2017learning} utilizes meta-learning to learn model dynamics, predicting how model parameters will change as more training data is progressively incorporated. As a result, MetaModelNet can directly enhance model parameters trained on minority instances by predicting the parameters when additional minority samples are included.

\textbf{Data augmentation.}
MetaSAug \cite{li2021metasaug} employs meta-learning for data augmentation. Inspired by ISDA \cite{wang2019implicit}, it calculates class-wise covariance matrices to extract semantic directions and generates synthesized minority instances along these semantically meaningful directions. However, the scarcity of data for minority classes results in inaccurate covariance matrices. To address this issue, MetaSAug incorporates meta-learning and learns the most meaningful class-wise covariance by minimizing the loss on a balanced meta set. Consequently, MetaSAug trains models on an augmentation set enriched with semantically augmented samples, effectively enhancing their performance.

\textbf{Class prototypes.}
MPDT \cite{fu2022meta} focuses on calibrating empirical prototypes that are significantly biased due to scarce data in minority classes. MPDT first introduces meta-prototype contrastive learning to automatically learn the reliable representativeness of prototypes and predict the target distribution of balanced training data based on prototypes. Subsequently, additional features are sampled from the predicted distribution to further balance the overwhelming dominance of majority classes.








% \subsubsection{One-class learning}

% \subsubsection{Feature selection}
% The performance of machine learning classification methods significantly relies on feature quality. However, the high dimensionality of features in real world scenarios presents a challenging problem to classifiers and the class imbalance problems become even more severe when irrelevant features are added in the feature space. For example, in text classification, the number of features in a bag of words is often more than an order of magnitude compared to the number of training documents \cite{xiong2006kernel}. In microarray-based cancer classification, the number of features is typically tens of thousands \cite{forman2003extensive}. In these problems with high-dimensional features, the feature selection methods are designed to select the most useful features for classification and improve the performance of minority classes. 
% When employing feature selection to imbalanced data, the main challenge is a trade-off between removing irrelevant features and keeping useful features. Removing somewhat irrelevant features may also risk losing potentially useful information because the original data distribution may be altered by feature selection.
% Specifically, the feature selection methods can be categorized into filter methods \cite{zheng2004feature, grobelnik1999feature} and wrapper methods.
% For example, FAST \cite{chen2008fast} directly takes the classification metric into account and construct a feature selection metric based on an ROC curve. By selecting features with the highest area under the curve as the most relevant, the classification performance of minority class are improved.   