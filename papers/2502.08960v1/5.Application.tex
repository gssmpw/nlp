\section{Applications}
\label{sec:application}
\subsection{Imbalanced data learning in natural sciences}
\subsection{Imbalanced data learning in recommender system}
\subsection{Outlier detection}
\noindent Outlier detection has been used for centuries to detect and, where appropriate, remove anomalous observations from data. Outliers arise due to mechanical faults, changes in system behaviour, fraudulent behaviour, human error, instrument error or simply through natural deviations in populations. Their detection can identify system faults and fraud before they escalate with potentially catastrophic consequences. It can identify errors and remove their contaminating effect on the data set and as such to purify the data for processing. The original outlier detection methods were arbitrary but now, principled and systematic techniques are used, drawn from the full gamut of Computer Science and Statistics.~\cite{ref_a1} 

Outlier detection encompasses aspects of a broad spectrum of techniques. Many techniques employed for detecting outliers are fundamentally identical but with different names chosen by the authors. For example, authors describe their various approaches as outlier detection, novelty detection, anomaly detection, noise detection, deviation detection or exception mining. In this paper, we have chosen to call the technique outlier detection. Aggarwal and Yu (2001) ~\cite{ref_c1}note that outliers may be considered as noise points lying outside a set of defined clusters or alternatively outliers may be defined as the points that lie outside of the set of clusters but are also separated from the noise. A further outlier definition from Barnett and Lewis (1994)~\cite{ref_b1} is:
An observation (or subset of observations) which appears to be inconsistent with the remainder of that set of data. In this paper, we focus on the two definitions quoted from Barnett and Lewis (1994)~\cite{ref_b1} above and do not consider the dual class-membership problem or separating noise and outliers.

Outlier detection is a critical task in many safety critical environments as the outlier indicates abnormal running conditions from which significant performance degradation may well result, such as an aircraft engine rotation defect or a flow problem in a pipeline. An outlier can denote an anomalous object in an image such as a land mine. An outlier may pinpoint an intruder inside a system with malicious intentions so rapid detection is essential. Outlier detection can detect a fault on a factory production line by constantly monitoring specific features of the products and comparing the real-time data with either the features of normal products or those for faults. It is imperative in tasks such as credit card usage monitoring or mobile phone monitoring to detect a sudden change in the usage pattern which may indicate fraudulent usage such as stolen card or stolen phone airtime. Outlier detection accomplishes this by analysing and comparing the time series of usage statistics. For application processing, such as loan application processing or social security benefit payments, an outlier detection system can detect any anomalies in the application before approval or payment. Outlier detection can additionally monitor the circumstances of a benefit claimant over time to ensure the payment has not slipped into fraud. Equity or commodity traders can use outlier detection methods to monitor individual shares or markets and detect novel trends which may indicate buying or selling opportunities. A news delivery system can detect changing news stories and ensure the supplier is first with the breaking news. In a database, outliers may indicate fraudulent cases or they may just denote an error by the entry clerk or a misinterpretation of a missing value code, either way detection of the anomaly is vital for data base consistency and integrity.

There are three fundamental approaches to the problem of outlier detection:

\textbf{Type 1} Determine the outliers with no prior knowledge of the data. This is essentially a learning approach analogous to unsupervised clustering. The approach processes the data as a static distribution, pinpoints the most remote points, and flags them as potential outliers. Type 1 assumes that errors or faults are separated from the ‘normal’ data and will thus appear as outliers. It requires that all data be available before processing and that the data is static. There are two sub-techniques commonly employed, diagnosis and accommodation (Rous- seeuw and Leroy, 1996)~\cite{ref_d1}. An outlier diagnostic approach highlights the potential outlying points. Once detected, the system may remove these outlier points from future processing of the data distribution. An alternative methodology is accommodation that incorporates the outliers into the distribution model generated and employs a robust classification method. These robust approaches can withstand outliers in the data and generally induce a boundary of normality around the majority of the data which thus represents normal behaviour. In contrast, non-robust classifier methods produce representations which are skewed when outliers are left in. Non-robust methods are best suited when there are only a few outliers in the data set as they are computationally cheaper than the robust methods but a robust method must be used if there are a large number of outliers to prevent this distortion. 

\textbf{Type 2} Model both normality and abnormality. This approach is analogous to supervised classification and requires pre-labelled data, tagged as normal or abnormal. The normal points could be classified as a single class or subdivided into the three distinct classes according to the requirements of the system to provide a simple normal/abnormal classification or to provide an abnormal and 3-classes of normally classifier.
Classifiers are best suited to static data as the classification needs to be rebuilt from first principles if the data distribution shifts unless the system uses an incremental classifier such as an evolutionary neural network. A type 2 approach can be used for on-line classification, where the classifier learns the classification model and then classifies new exemplars as and when required against the learned model. If the new exemplar lies in a region of normality it is classified as normal, otherwise it is flagged as an outlier. Classification algorithms require a good spread of both normal and abnormal data, i.e., the data should cover the entire distribution to allow generalisation by the classifier. New exemplars may then be classified correctly as classification is limited to a ‘known’ distribution and a new exemplar derived from a previously unseen region of the distribution may not be classified correctly unless the generalisation capabilities of the underlying classification algorithm are good.

\textbf{Type 3} Model only normality or in a very few cases model abnormality (Japkowicz et al., 1995; Fawcett and Provost 1999)~\cite{ref_e1}. Authors generally name this technique novelty detection or novelty recognition. It is analogous to a semi-supervised recognition or detection task and can be considered semi-supervised as the normal class is taught but the algorithm learns to recognise abnormality. The approach needs pre-classified data but only learns data marked normal. It is suitable for static or dynamic data as it only learns one class which provides the model of normality. It can learn the model incrementally as new data arrives, tuning the model to improve the fit as each new exemplar becomes available. It aims to define a boundary of normality.
A type 3 system recognises a new exemplar as normal if it lies within the boundary and recognises the new exemplar as novel otherwise. This boundary may be hard where a point lies wholly within or wholly outside the boundary or soft where the boundary is graduated depending on the underlying detection algorithm. A soft bounded algorithm can estimate the degree of ‘outlierness’.
It requires the full gamut of normality to be available for training to permit generalisation. However, it requires no abnormal data for training unlike type 2. Abnormal data is often difficult to obtain or expensive in many fault detection domains such as aircraft engine monitoring. Another problem with type 2 is it cannot always handle outliers from unexpected regions, for example, in fraud detection a new method of fraud never previously encountered or previously unseen fault in a machine may not be handled correctly by the classifier unless generalisation is very good. In this method, as long as the new fraud lies outside the boundary of normality then the system will be correctly detect the fraud. If normality shifts then the normal class modelled by the system may be shifted by re-learning the data model or shifting the model if the underlying modelling technique permits such as evolutionary neural networks.

According to observation, outlier detection methods are derived from three fields of computing: statistics (proximity-based, parametric, non-parametric and semi-parametric), neural networks (supervised and unsupervised) and machine learning. n the next four sections, we describe and analyse techniques from all three fields and a collection of hybrid techniques that utilise algorithms from multiple fields:

\subsubsection{Statistical Models}
Statistical approaches were the earliest algorithms used for outlier detection. Some of the earliest are applicable only for single dimensional data sets. 

\textbf{Proximity-based techniques} 
Proximity-based techniques are simple to implement and make no prior assumptions about the data distribution model. They are suitable for both type 1 and type 2 outlier detection. However, they suffer exponential computational growth as they are founded on the calculation of the distances between all records. The computational complexity is directly proportional to both the dimensionality of the data m and the number of records n. Hence, methods such as k-nearest neighbour (also known as instance-based learning and described next) with Oðn2mÞ runtime are not feasible for high dimensionality data sets unless the running time can be improved. There are various flavours of k-nearest neighbour (k-NN) algorithm for outlier detection but all calculate the nearest neighbours of a record using a suitable distance calculation metric such as Euclidean distance or Mahalanobis distance.

\textbf{Parametric methods}
Many of the methods we have just described do not scale well unless modifications and optimisations are made to the standard algorithm. Parametric methods allow the model to be evaluated very rapidly for new instances and are suitable for large data sets; the model grows only with model complexity not data size. However, they limit their applicability by enforcing a pre-selected distribution model to fit the data. If the user knows their data fits such a distribution model then these approaches are highly accurate but many data sets do not fit one particular model.

\textbf{Non-parametric methods}
Many statistical methods described in this section have data-specific parameters ranging from the k values of k-NN and k-means to distance thresholds for the proximity-based approaches to complex model parameters. Other techniques such as those based around convex hulls and regression and the PCA approaches assume the data follows a specific model. These all require a priori data knowledge. Such infor- mation is often not available or is expensive to compute. Many data sets simply do not follow one specific distribution model and are often randomly distributed. Hence, these approaches may be applicable for an outlier detector where all data is accumulated beforehand and may be pre-processed to determine parameter settings or for data where the distribution model is known. Non-parametric approaches, in contrast are more flexible and autonomous.

\textbf{Semi-parametric methods}
Semi-parametric methods apply local kernel models rather than a single global distribution model. They aim to combine the speed and com- plexity growth advantage of parametric methods with the model flexi- bility of non-parametric methods.

\subsubsection{Neural Networks}
Neural network approaches are generally non-parametric and model- based, they generalise well to unseen patterns and are capable of learning complex class boundaries. After training the neural network forms a classifier. However, the entire data set has to be traversed numerous times to allow the network to settle and model the data correctly. They also require both training and testing to fine tune the network and determine threshold settings before they are ready for the classification of new data. Many neural networks are susceptible to the curse of dimensionality though less so than the statistical techniques. The neural networks attempt to fit a surface over the data and there must be sufficient data density to discern the surface. Most neural networks automatically reduce the input features to focus on the key attributes. But nevertheless, they still benefit from feature selection or lower dimensionality data projections. 
\textbf{Supervised neural methods}
Supervised neural networks use the classification of the data to drive the learning process. The neural network uses the class to adjust the weights and thresholds to ensure the network can correctly classify the input. The input data is effectively modelled by the whole network with each point distributed across all nodes and the output representing the classification. For example, the data in Figure 4 is represented by the weights and connections of the entire network.

\textbf{Unsupervised neural methods}
Supervised networks require a pre-classified data set to permit learning. If this pre-classification is unavailable then an unsupervised neural network is desirable. Unsupervised neural networks contain nodes which compete to represent portions of the data set. As with perceptron- based neural networks, decision trees or k-means, they require a training data set to allow the network to learn. They autonomously cluster the input vectors through node placement to allow the underlying data distribution to be modelled and the normal/abnormal classes differentiated. They assume that related vectors have common feature values and rely on identifying these features and their values to topologically model the data distribution.

\subsubsection{Machine Learning}
Much outlier detection has only focused on continuous real-valued data attributes there has been little focus on categorical data. Most statistical and neural approaches require cardinal or at the least ordinal data to allow vector distances to be calculated and have no mechanism for processing categorical data with no implicit ordering. John (1995)~\cite{ref_f1} and Skalak and Rissland (1990) ~\cite{ref_g1}use a C4.5 decision tree to detect outliers in categorical data and thus identify errors and unexpected entries in databases. Decision trees do not require any prior knowledge of the data unlike many statistical methods or neural methods that require parameters or distribution models derived from the data set. Conversely, decision trees have simple class boundaries compared with the complex class boundaries yielded by neural or SVM approaches. Decision trees are robust, do not suffer the curse of dimensionality as they focus on the salient attributes, and work well on noisy data. C4.5 is scalable and can accommodate large data sets and high dimensional data with acceptable running times as they only require a single training phase. C4.5 is suited to type 2 classifier systems but not to novelty recognisers which require algorithms such as k-means which can define a boundary of normality and recognise whether new data lies inside or outside the boundary. However, decision trees are dependent on the coverage of the training data as with many classifiers. They are also susceptible to over fitting where they do not generalise well to completely novel instances which is also a problem for many neural network methods. 

\subsubsection{Hybrid Systems} 
The most recent development in outlier detection technology is hybrid systems. Hybridisation is used variously to overcome deficiencies with one particular classification algorithm, to exploit the advantages of multiple approaches while overcoming their weaknesses or using a meta-classifier to reconcile the outputs from multiple classifiers to handle all situations.
Bishop (1994) ~\cite{ref_h1}augments his MLP(we have introduce it above) with a Parzen window novelty recogniser. The MLP classifies new items similar to those already seen with high confidence. The Parzen window provides an estimate of the probability density to induce a confidence estimate. Any completely novel instances will lie in a low density and the MLP prediction confidence will be low. Parzen windows are kernel-based algorithms which use various kernel functions to rep- resent the data. Bishop uses one Gaussian kernel (as defined by equation (8)) per input vector with the kernel centred on the attribute values of the vector. The Parzen window method can provide a hard boundary for outlier detection by classifying new exemplar as normal if they be- long to a kernel and an outlier otherwise. It can also provide a soft boundary by calculating the probability that the new exemplar belongs to a Gaussian kernel. Nairac et al. (1999)~\cite{ref_i1} similarly incorporate a k-means module with the MLP module in an aircraft engine fault diagnosis system. The k-means module partitions the data and models graph shape normality. It can detect anomalies in the overall vibration shape of new signatures and the MLP detects transitions within the vibration graphs through one-step ahead prediction described above
Smyth (1994)~\cite{ref_j1}  stabilises the output from an MLP to monitor space probe data for fault detection using a hidden Markov model (HMM) as the MLP is susceptible to rapid prediction changes many of which are extraneous. The approach uses time-series data, a snapshot of the space probe transmitted periodically with equal intervals between transmis- sions. The feed-forward MLP trained using back propagation aims to model the probe and predicts the next state of the probe. However, the MLP has a tendency to switch between states at an artificially high rate so Smyth uses a HMM to correlate the state estimations and smooth the outputs producing a predictor with high accuracy. Hollmen and Tresp (1999)~\cite{ref_k1} incorporate hierarchical HMMs with EM which helps determine the parameters for time series analysis in the task of cell-phone fraud detection. The authors use the HMM to predict one-step ahead values for a fraud variable in the time series given the current and previous stataes of a user’s account and the EM algorithm is used to provide optimal parameter settings for the HMM to enable accurate one-step ahead prediction..
