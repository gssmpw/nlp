\section{Evaluation Metrics}
\label{sec:eval}

Evaluating models for imbalanced datasets requires careful consideration of the metrics used to assess the performance. Traditional metrics like accuracy can not provide a reliable measure in this imbalanced contexts because they are biased toward the majority. In datasets where one class significantly outnumbers another, a model can achieve high accuracy simply by favoring the majority, often at the expense of poorly predicting the minority. For example, in a medical diagnosis setting where 95\% of the samples are negative for a disease, a model that always predicts ``negative'' will achieve 95\% accuracy, despite failing to identify any positive cases, which are often the most critical to detect. To counter this bias and accurately assess model effectiveness, it's crucial to use evaluation metrics that accurately reflect performance across all classes, especially when misclassification costs are high. Below, we first describe the fundamental metrics for classification and then introduce the metrics for imbalance scenarios:

\noindent\textbf{{Confusion Matrix}} is a fundamental tool that provides a detailed breakdown of classification results across the actual and predicted classes, and the prediction results are categories into 4 types:

\begin{itemize}
    \item True Positives (TP): Correctly predicted positive observations.
    \item True Negatives (TN): Correctly predicted negative observations.
    \item False Positives (FP): Incorrectly predicted as positive.
    \item False Negatives (FN): Incorrectly predicted as negative.
\end{itemize}

\noindent\textbf{Precision} provides insights into the accuracy of positive predictions.
\begin{equation}
\text{Precision} = \frac{\text{TP}}{\text{TP} + \text{FP}}.
\end{equation}


\noindent\textbf{Recall}, also known as \textbf{Sensitivity}, measures the proportion of actual positives that are correctly identified by the model.
\begin{equation}
\text{Recall} = \frac{\text{TP}}{\text{TP} + \text{FN}}.
\end{equation}
It indicates how effective the model is at detecting positive instances from the data set.

\noindent\textbf{Specificity} measures the proportion of actual negatives that are correctly identified as such. In other words, it is the ability of the test to identify negative results.
\begin{equation}
\text{Specificity} = \frac{\text{TN}}{\text{TN} + \text{FP}}.
\end{equation}

With these fundamental metrics, we can derive more advanced measures to enhance the evaluation of imbalanced datasets:

\noindent\textbf{F1-score} combines precision and recall metrics to balance their influence:
\begin{equation}
\text{F1} = 2 \times \frac{\text{Precision} \times \text{Recall}}{\text{Precision} + \text{Recall}}.
\end{equation}
Precision and recall often have a trade-off relationship: improving precision typically reduces recall and vice versa. This occurs because increasing the threshold for classifying positives can decrease the number of false positives (increasing precision) but also miss more actual positives (decreasing recall). By combining these two metrics, the F1-score offers a single metric that balances this trade-off, providing a holistic view of the model performance.

In multi-class classification scenarios, the F1 score can be calculated in distinct ways, specifically through \textbf{micro} and \textbf{macro} averaging. Micro combines the outcomes of all classes to compute a single average metric, which is particularly useful in imbalanced datasets, as it emphasizes performance on rare classes. Conversely, macro calculates the metric separately for each class and then averages these results, ensuring that all classes are given equal importance regardless of their size in the dataset.



\noindent\textbf{Balanced accuracy (BA)} adjusts accuracy to reflect equitable performance across classes:
\begin{equation}
\text{BA} = \frac{\text{Sensitivity} + \text{Specificity}}{2}.
\end{equation}

\noindent\noindent\textbf{G-Mean} is defined as the geometric mean of sensitivity and specificity, and is sensitive to the performance on both classes:
\begin{equation}
\text{G-Mean} = \sqrt{\text{Sensitivity} \times \text{Specificity}}.
\end{equation}

\noindent\textbf{Matthews Correlation Coefficient (MCC)} considers all four quadrants of the confusion matrix and is highly effective for binary classification problems:
\begin{equation}
\text{MCC} = \frac{\text{TP} \times \text{TN} - \text{FP} \times \text{FN}}{\sqrt{(\text{TP}+\text{FP})(\text{TP}+\text{FN})(\text{TN}+\text{FP})(\text{TN}+\text{FN})}}.
\end{equation}

\noindent\textbf{Area Under the Receiver Operating Characteristic Curve (AUC-ROC)} is a performance measurement for classification models at various threshold settings. It represents the probability that a classifier will rank a randomly chosen positive instance higher than a randomly chosen negative instance. The ROC curve plots the True Positive Rate (TPR, also known as recall or sensitivity) against the False Positive Rate (FPR) at different threshold levels.
\begin{equation}
\text{AUC-ROC} = \int_{0}^{1} \text{TPR}(\tau) \, d\text{FPR}(\tau).
\end{equation}
A model with perfect predictive accuracy would have an AUC-ROC of 1, indicating it perfectly ranks all positive instances higher than negative ones.

These metrics provide detailed and varied perspectives for assessing the performance of models on imbalanced datasets, enabling the selection of the appropriate measures based on specific evaluation needs.
