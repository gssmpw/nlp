\section{Methodology}
\label{s3}

\input{tree}

In this section, we delve into the specifics of methods for handling imbalanced data, conducting a systematic analysis across four distinct categories: data re-balancing, feature representation, training strategy, and ensemble learning. We explore the various techniques employed within each category to achieve their respective objectives. Additionally, we provide a detailed overview in Figure \ref{fig:tax}, which serves to encapsulate the scope and relationships among the methods discussed.





\subsection{Data re-balancing}
\label{s31}

\subsubsection{Data generation}\
\label{s311}

One straightforward approach to address class imbalance and augment minority samples is random oversampling, which involves duplicating randomly selected minority class instances and adding them to the dataset. However, this fundamental method, while effective in balancing class distributions, can inadvertently lead to classifier overfitting. This occurs because random oversampling generates multiple identical instances, resulting in overly specific rules. To overcome the limitations of random oversampling, more sophisticated techniques have been introduced. Linear generative methods and deep generative models offer a refined solution by creating diverse synthetic samples for minority classes. These advanced techniques enhance the learning process for machine learning models, enabling them to make more accurate predictions for minority class instances.



\textbf{Linear generative methods} 
Linear generative methods generate synthetic minority samples by interpolating between samples or feature vectors, which includes the Synthetic Minority Oversampling Technique (SMOTE)~\cite{ref_2} and Mixup~\cite{zhang2017mixup}.



SMOTE selects k pairs of nearest minority neighbors and performs linear interpolation to create new synthetic minority samples. This method effectively mitigates overfitting and enhances classifier learning. Nonetheless, SMOTE generates an equal number of synthetic samples for each original minority sample, disregarding neighboring sample distribution characteristics and potentially causing over-representation of certain classes. To address this, recent advancements include D-SMOTE~\cite{ref_10}, which generates synthetic samples using mean points of nearest neighbors, and N-SMOTE~\cite{ref_11}, which employs structural information in neighbor calculations. Adaptive techniques like Borderline-SMOTE~\cite{ref_4} only generate synthetic samples for boundary minority samples, and ADASYN~\cite{ref_5} adjusts sample generation based on density distribution. MSMOTE~\cite{ref_12} categorizes minority class instances into safe, border, and latent noise groups and selectively generates samples for border instances. With over 100 variants developed to date, SMOTE stands out as one of the most influential and effective methods in the realm of data generation for addressing class imbalance. Given the focus of our paper on a comprehensive survey for imbalanced data, we recommend readers who are particularly interested in SMOTE and its extensive family of variants to refer to a SMOTE survey~\cite{smote-comparison}.

In contrast to SMOTE, Mixup introduces a distinct approach, allowing for interpolations between samples from different classes while simultaneously considering linear interpolations of associated labels using a consistent mixing factor. However, it's important to note that the vanilla Mixup method may significantly disrupt the minority class distribution, potentially blurring the decision boundary. Recognizing this challenge, Remix~\cite{chou2020remix} offers a flexible solution by relaxing the constraint of employing the same mixing factor for both features and labels when generating virtual mixed samples for imbalanced data. In Remix, the mixing factors for labels are thoughtfully determined, accounting for the class sizes of interpolated samples and bestowing greater importance upon minority samples. This strategic choice serves the purpose of shifting the decision boundary toward the majority class and enriching the diversity of information pertaining to the minority class through the introduction of majority samples. 
Nonetheless, it is imperative to acknowledge that the random sampling of interpolated data may exacerbate data imbalance, potentially resulting in suboptimal performance. To address this concern, several sampling methods have been introduced to further bolster Remix's efficacy. MixBoost~\cite{kabra2020mixboost} incorporates an information-theoretic measure in conjunction with the trained classifier, facilitating the selection of instances in pairs. This process ensures that one instance in each pair is in close proximity to the classifier's decision boundary, while the other is positioned further away, contributing to the iterative enhancement of classifier performance. Balanced-MixUp~\cite{galdran2021balanced} employs a dual-strategy approach by concurrently sampling imbalanced instances and balanced classes from the training data. This facilitates the creation of a balanced training distribution, effectively mitigating the risk of under-fitting in minority classes. Label Occurrence-Balanced Mixup~\cite{9746299} incorporates two independent class-balanced samplers, thereby upholding statistically balanced label occurrences for each class. This meticulous approach ensures a more equitable representation of classes in mixed data pairs.



\textbf{Deep generative models.}
Conventional data generation methods typically produce synthetic samples positioned along the line segment connecting minority class instances. Consequently, these techniques rely on local information and may not capture the overall distribution of the minority class effectively. Deep generative models, celebrated for their capacity to model the probability distribution of high-dimensional data, have gained widespread recognition for their ability to generate high-quality synthetic samples for minority classes.

Wan et al.~\cite{8285168} is a pioneering in the utilization of the generative models. Their work is dedicated to the precise modeling of high-dimensional data distributions via Variational Autoencoder (VAE) framework, with a specific focus on generating synthetic samples for minority classes. VAE maps observed data points into a latent space while simultaneously learning the underlying data structure and probabilistic characteristics. The latent variable is represented by a probability distribution over possible values rather than a single fixed value, which is able to enforce the uncertainty of the model and lead to stable prediction. By employing VAE to generate synthetic samples through the utilization of random noise conforming to a standard normal distribution, this methodology not only ensures exceptional sample diversity but also consistently outperforms conventional data generation methods.

In addition to VAE, another noteworthy contribution to data generation methods is found in the realm of Generative Adversarial Networks (GANs)~\cite{goodfellow2020generative}. Douzas et al.~\cite{douzas2018effective} introduced an innovative approach that harnesses the power of conditional Generative Adversarial Networks (cGAN)~\cite{mirza2014conditional} to approximate the genuine data distribution and produce synthetic data specifically tailored for the minority class. A typical cGAN comprises two primary components: a generator and a discriminator. The generator's role is to generate synthetic data based on provided labels, while the discriminator is tasked with distinguishing real data from artificially generated data. The application of GANs to data rebalancing involves an iterative training process. Initially, both the generator and discriminator undergo training to ensure that the discriminator becomes proficient at differentiating between various data samples, and the generator becomes capable of producing synthetic data corresponding to specific class labels. Subsequently, the trained generator is employed to conduct data augmentation for the minority class. This adversarial training procedure enables the network to capture a more accurate data distribution, resulting in the production of more reliable and representative synthetic samples. 

However, training a GAN directly for minority data generation poses significant challenges, primarily due to the scarcity of minority-class samples, which may be insufficient to effectively train a GAN model. In response to this challenge, BAGAN~\cite{mariani2018bagan} is proposed to address this issue by proposing an alternative training strategy that incorporates data from both minority and majority classes simultaneously during the adversarial training process. The key innovation lies in class conditioning, which is applied within the latent space. Moreover, BAGAN leverages the discriminator not only for its traditional role of distinguishing fake samples from real ones but also as a classifier. This dual role enables the discriminator to not only discern the authenticity of generated samples but also identify the category of real samples. Consequently, BAGAN learns the underlying features of the specific classification problem by initially considering all available samples across both minority and majority classes. These shared features can then be applied universally across all classes, thereby facilitating the generation of high-quality minorities. Similarly, SCDL-GAN~\cite{cai2019supervised} and RVGAN-TL~\cite{ding2023rvgan} employ the Wasserstein auto-encoder~\cite{tolstikhin2017wasserstein} and VAE, respectively, to capture the distributive characteristics of all classes with guidance from label information before generating samples. EWGAN~\cite{ren2019ewgan} constructs entropy-weighted label vectors for each class to characterize data imbalance and employs the Wasserstein Generative Adversarial Network~\cite{arjovsky2017wasserstein} for stable training. EID-GANs~\cite{li2022eid} tackle extreme data imbalance by designing a novel penalty function that guides the generator to learn features of extreme minorities. Meanwhile, SMOTified-GAN~\cite{sharma2022smotified} combines SMOTE with GAN, deploying GAN on pre-sampled minority data produced by SMOTE to enhance sample quality in minority classes. In contrast to the aforementioned GAN framework, GAMO ~\cite{mullick2019generative} offers an alternative solution by integrating a convex generator, classifier, and discriminator into a three-player adversarial game. GAMO's convex generator creates new samples for minority classes through convex combinations of existing instances. Its primary objective is to perplex both the discriminator and the classifier, generating samples that challenge accurate classification. These synthetic samples are strategically positioned near class boundaries, thus aiding in adjusting the classifier's decision boundaries to reduce misclassification of minority class data.

Alongside GANs, diffusion models~\cite{ho2020denoising,wang2022semantic} have gained attention for their robust representation capabilities. These models present a compelling alternative to the intricate adversarial training procedures. They operate on a fundamentally distinct principle—sequentially denoising data—a methodology that has proven to be remarkably stable and effective in generating synthetic samples. Within the context of addressing data imbalance, several pioneering works have harnessed diffusion models to excellent effect.
DiffMix~\cite{oh2023diffmix} extends the utility of diffusion models to nuclei segmentation and classification tasks, which demand precise semantic image and label map pairs, alongside realistic pathology image generation. It diversifies synthetic samples by creating virtual patches and strategically shifting instance mask locations to balance class variances. Additionally, SDM~\cite{wang2022semantic} is employed within DiffMix to synthesize diverse, semantically realistic, and balanced pathology nuclei data, conditioned on generated label maps.
DiffuASR~\cite{liu2023diffusion} addresses data sparsity and the long-tail user problem in sequential recommendation by leveraging diffusion-based pseudo sequence generation. This approach introduces classifier-guided and classifier-free strategies to maintain the original intent of users throughout the diffusion process.
MosaicFusion~\cite{xie2023mosaicfusion} introduces a diffusion-based data augmentation pipeline that simultaneously generates images and masks, particularly beneficial for large vocabulary instance segmentation. This method effectively mitigates distribution imbalances common in long-tailed and open-set scenarios. By running the diffusion process simultaneously on different image canvas regions and conditioning it on various text prompts, MosaicFusion controls the diffusion model to generate multiple objects at specific locations within a single image.








\subsubsection{Adaptive under-sampling}\
\label{s312}

In contrast to data generation techniques that augment the original dataset, under-sampling methods involve the reduction of data within the original dataset. Random under-sampling, as the most fundamental under-sampling approach, entails the random reduction of the majority class samples to achieve dataset balance. However, this method may result in the discarding of potentially valuable data, which could bear significance for the inductive process. To enhance the efficacy of data under-sampling, researchers have explored strategies aimed at retaining the most representative majority class samples. These strategies can be broadly classified into neighbor-based, clustering-based, and evolutionary-based methods.

\textbf{Neighbor-based methods.} Neighbor-based under-sampling methods revolve around the selection of samples based on their proximity to other samples. One notable technique is Tomek Link~\cite{kubat1997addressing}, which identifies pairs of examples as Tomek links if they belong to different classes and are each other's nearest neighbors. under-sampling using Tomek Link can be executed by either removing all Tomek links from the dataset or specifically targeting the majority class samples involved in these links. The removal of majority class nearest neighbors serves to reposition the learned decision boundary towards the majority class, thereby fostering the generation of minority class samples.
One-sided selection (OSS)~\cite{kubat1997addressing} extends the concept of Tomek Link to categorize majority class samples into three groups: noise samples, boundary samples, and safety samples. Subsequently, it eliminates boundary and noise samples from the majority class.
Condensed Nearest Neighbor (CNN)~\cite{ref_6}, on the other hand, leverages the nearest neighbor rule to iteratively decide whether a sample should be retained or removed. Central samples that can be accurately classified by the nearest neighbor rule are discarded. However, this method can be relatively slow due to its need for multiple passes over the training data, and it is sensitive to noise, potentially incorporating noisy samples into the retained subset.
Edited Nearest Neighbor (ENN)\cite{ref_17} operates by removing samples whose class labels differ from the majority of their nearest neighbors. However, its efficacy is limited due to the prevalence of majority class samples in close proximity to each other. Building upon ENN, the Neighborhood Cleaning Rule (NCR)~\cite{ref_18} combines ENN with k-NN~\cite{guo2003knn} to enhance the removal of noisy samples from the datasets.
The NearMiss family of methods~\cite{ref_19} introduces under-sampling techniques based on the distances between points in the majority class. NearMiss-1 retains majority class samples whose mean distance to the k nearest minority class points is the lowest. In contrast, NearMiss-2 retains majority class samples whose mean distance to the k farthest minority class points is the lowest. Finally, NearMiss-3 selects k nearest neighbors in the majority class for each point in the minority class. This approach directly controls the under-sampling ratio through the parameter k, eliminating the need for separate tuning.
NB-Rec~\cite{vuttipittayamongkol2020neighbourhood} investigates the presence of overlapping instances from various classes and introduces the utilization of the k-NN rule to investigate the local neighborhoods of individual samples. It selectively eliminates majority class samples from the overlapping region without impacting any minority samples, leading to enhanced detection of overlapped minority instances. 




\textbf{Clustering-based methods.} Clustering-based under-sampling methods employ a clustering approach to select samples for under-sampling. The underlying concept is that datasets often comprise various clusters, each exhibiting distinct characteristics. This clustering strategy aims to capture both global information and sub-concepts within the dataset, thereby enhancing sampling quality.
SBC~\cite{yen2009cluster} initiates the process by clustering the dataset into several clusters, each containing both majority and minority class samples. When a cluster exhibits a higher concentration of minority class samples and a reduced presence of majority class samples, it signifies that this cluster aligns more with the characteristics of the minority class. Consequently, SBC identifies an appropriate number of majority class samples to retain from each cluster, taking into account the ratio of majority class samples to minority class samples within the cluster.
In contrast to clustering multiclass samples, CBMP~\cite{zhang2010cluster} takes a direct approach by clustering the majority of samples and subsequently selecting samples that are either nearest to the centroid or chosen randomly. CentersNN~\cite{lin2017clustering} clusters the majority class with the number of clusters equal to the number of data points in the minority class, using cluster centers to represent the majority class. 
DSUS~\cite{ng2014diversified} introduces diversified sensitivity-based under-sampling, which selects valuable samples with high stochastic sensitivity concerning the current trained classifier. To ensure that samples are not exclusively chosen near the decision boundary and do not distort the data distribution, DSUS clusters the majority class and selects only one representative sample from each cluster for inclusion in the selection process. 

However, clustering the majority class used in these methods is a computationally intensive process, which poses a significant challenge. To address this issue, Fast-CBUS~\cite{ofek2017fast} presents an efficient clustering-based under-sampling method that narrows its focus to clustering only the minority class instances. Within each of these minority class clusters, Fast-CBUS selects majority class instances that closely surround the minority instances and proceeds to create a dedicated model for each cluster. When classifying a new sample, the method prioritizes predictions made by models associated with clusters that exhibit the closest proximity to the sample. This approach is grounded in the idea that it's beneficial to train models using instances that are challenging to classify, particularly those majority class instances that are situated near minority instances.


\textbf{Evolutionary-based methods.}
EUS~\cite{garcia2009evolutionary} presents a collection of techniques known as evolutionary under-sampling. These methods incorporate evolutionary algorithms into the process of under-sampling, with the primary aim of enhancing classifier accuracy by reducing instances predominantly from the majority class. To achieve this, EUS employs meticulously designed fitness functions, carefully striking a balance between data reduction, class balance, and classification accuracy.
Following EUS, MapReduce~\cite{triguero2015evolutionary} enhances it to make it feasible for large-scale datasets, even in the presence of extreme data imbalance. The approach involves partitioning the entire training dataset into manageable chunks. For each chunk containing minority data, the goal is to create a balanced subset using EUS, thus utilizing a representative sample from the majority set.
EUSC~\cite{le2021eusc} introduces a two-stage clustering-based surrogate model within the framework of evolutionary under-sampling, facilitating the expedited computation of fitness values. A key innovation is the creation of a surrogate model for binary optimization which is based on the meaning rather than their binary representation.
EUSBoost~\cite{galar2013eusboost, krawczyk2016evolutionary} further enhances the performance of base classifiers by integrating the evolutionary under-sampling approach with Boosting algorithms. This evolutionary strategy prioritizes the selection of the most consequential samples for the classifier learning phase, based on accuracy and an additional diversity term incorporated into the fitness function. Consequently, it effectively addresses the challenges posed by imbalanced scenarios.


In addition to the three categories mentioned above, several alternative approaches have been developed from other perspectives. 
TU~\cite{peng2019trainable} introduces a meta-learning technique to parameterize the data sampler and trains it to optimize classification performance based on the evaluation metric. By incorporating evaluation metric optimization into the data sampling process, this method effectively determines which instances should be retained or discarded for a given classifier and evaluation metric.
SDUS~\cite{9741366} utilizes supervised constructive learning, focusing on sphere neighborhoods (SPN) within the majority class. It offers two strategies: a top-down approach, treating SPN regions equally and favoring regions with smaller sample sizes, and a bottom-up approach, selecting samples proportionally based on SPN inner distribution for global pattern preservation.
RIUS~\cite{hoyos2021relevant} leverages the information-preservation principle and calculates cross-information information potential. It then selects the most relevant examples from the majority class, allowing for the extraction of the underlying structure of the majority class with a reduced number of samples.
ECUBoost~\cite{8968753} aims to prevent the loss of informative samples during data pre-processing for boosting-based ensembles. This method employs both confidence and entropy as benchmarks to ensure the validity and structural distribution of the majority class samples during under-sampling.




\subsubsection{Hybrid sampling}\
\label{s313}

Data generation and under-sampling share the common goal of achieving a balanced class distribution, but each method comes with its own set of limitations. Under-sampling can lead to information loss due to the removal of examples, and data generation techniques can increase the dataset size with potentially synthetic and less informative samples. To achieve a balanced class distribution and optimize classifier performance, researchers have introduced hybrid sampling methods. These approaches often leverage linear generative techniques to over-sample the minority class. However, linear generative methods may fall short in capturing the complex structure of minority samples, particularly in extremely imbalanced scenarios characterized by significant class overlap. To address this challenge, some methods aim to enhance the effectiveness of linear generative techniques by integrating them with adaptive under-sampling strategies. 
For instance, SMOTE+Tomek Links and SMOTE+ENN~\cite{batista2004study} initially generate minority samples using SMOTE and subsequently identify and eliminate instances involved in Tomek links or those that have been misclassified.
CUSS~\cite{feng2020cluster} combines SMOTE with the clustering-based under-sampling method to minimize the risk of discarding valuable majority class instances.
SMOTE-RSB~\cite{ramentol2012smote} and SMOTE-IPF~\cite{saez2015smote} incorporate rough set theory and iterative ensemble-based noise filter known as the iterative-partitioning filter, respectively to prevent the generation of noisy samples that might disrupt the overall data distribution.
BMW-SMOTE~\cite{gao2020ensemble} controls the number of synthetic instances generated based on a weight calculated from the ratio of neighboring majority samples, which effectively reinforces the recognition of the minority class within local regions.

In addition to the approach that combines data generation and sample selection to achieve a balanced data distribution, advanced methods delve into the intricacies of sampling strategies to create diverse training data distributions. For example, BBN~\cite{zhou2020bbn}, SimCAL~\cite{wang2020devil} and TLML~\cite{guo2021long} explore both the original imbalanced distribution and balanced distribution independently. 
Research endeavors~\cite{li2020overcoming, xiang2020learning, cai2021ace, cui2022reslt} categorize classes into groups, each characterized by a balanced distribution. SADE~\cite{zhang2022self} takes an innovative step by introducing the inversely imbalanced distribution to enhance the representation of minority classes in the dataset.
Potential Anchoring~\cite{koziarski2021potential} employs radial basis functions to measure class distribution density and maintains the original shape of class distributions by merging over- and under-sampling techniques. 
DLSA~\cite{xu2022constructing} introduces a streamlined approach by progressively adapting the label space, dynamically transitioning from imbalance to balance to facilitate classification. 
EHSO~\cite{zhu2020ehso} incorporates an evolutionary algorithm to identify the optimal balance between classification performance and the replication ratio of random oversampling. 
Dynamic Sampling~\cite{pouyanfar2018dynamic} leverages sample upgrading, making real-time adjustments to the dataset based on the training results. It randomly removes samples from well-performing classes and duplicates samples from poorly performing classes, ensuring that the classification model learns relevant information at each iteration. 




\subsubsection{Re-labeling}\
\label{s314}

Apart from data generation and under-sampling, researchers have recognized the untapped potential of unlabeled data as a valuable resource. By incorporating self-training and active learning strategies, these unlabeled samples can be effectively labeled to compensate for the lack of imbalanced category supervision information and improve classification ability. 

\textbf{Self-training.}
Self-training is a widely used technique where a model is initially trained on a labeled dataset and then iteratively improves itself by using its predictions (i.e., pseudo-labels) on unlabeled data to generate more training samples. In line with this approach, Yang et al.~\cite{yang2020rethinking} initially experimented with using pseudo-labels from minority classes to address class imbalance and subsequently retrained the model.
Expanding upon this concept, CReST~\cite{wei2021crest} made an important observation that models trained on imbalanced datasets often achieve low recall but high precision on minority classes. This suggests that these models are proficient at generating high-precision pseudo-labels for minority classes. To leverage this insight, CReST introduced a stochastic update strategy for pseudo-label selection, assigning higher selection probabilities to samples predicted as minority classes, which are more likely to be accurate.
To enhance the quality of pseudo-labels, SaR~\cite{Lai_2022_CVPR} introduced a calibration step that adjusts soft labels based on prediction bias before generating one-hot pseudo-labels. This calibration process aims to simultaneously enhance the recall of minority classes and the precision of majority classes.
In addition to utilizing linear classifiers for generating pseudo-labels, DASO~\cite{oh2022daso} explored the use of semantic pseudo-labels. These labels are derived by measuring the similarity between a given class center and unlabeled samples in feature space. DASO's research demonstrated that semantic pseudo-labels excel in predicting minority classes with high recall. As a result, the combination of linear and semantic pseudo-labels offers a promising approach to mitigating overall dataset bias.


\textbf{Active learning.}
Active learning is a technique that seeks to minimize the human labeling effort by automating the selection of the most informative unlabeled samples for human annotation. In the context of imbalanced data, active learning becomes a vital tool for addressing class imbalance and mitigating the potential error labels introduced by self-training. Various active learning strategies have emerged, each focusing on different aspects of informativeness, such as margin, uncertainty, and diversity.

Margin-based selection methods, as exemplified by SVMAL~\cite{ertekin2007active} and AI-WSELM~\cite{qin2021active}, choose the unlabeled instance closest to the classifier's decision boundary as the most informative sample. This approach is rooted in the concept that uncertain or ambiguous samples hold essential information critical for shaping the decision boundaries of a classifier. Hence, they should receive the highest priority for inclusion in the training set.

Uncertainty-based strategies for active learning rely on quantifying sample uncertainty using prediction entropy or confidence metrics. For instance, BootOS~\cite{zhu2007active} employs the entropy of prediction as an indicator of uncertainty for sample selection. It further introduces a novel concept of using max-confidence as an upper bound and min-error as a lower bound for halting the active learning process. Building upon this foundation, several other works have extended the uncertainty-based sampling approach, as evidenced in~\cite{aggarwal2021minority, liu2021comprehensive, dong2020cost, jin2022deep}.

In contrast to these uncertainty-based approaches, CEAL~\cite{fu2011certainty} acknowledges a unique challenge when applying active learning in imbalanced datasets. In such scenarios, the minority class often becomes overwhelmed by the majority class during the labeling process. To mitigate this imbalance, CEAL introduces the concept of "certainty-enhanced neighborhoods." For each unlabeled sample, it recommends considering only the local context where the majority class density is low. This strategic selection process reduces the likelihood of querying majority samples, offering a solution to the issue of minority class overshadowing during active learning.

However, these methods relying solely on a single model's output may not always perform optimally. This is particularly evident in cases where the initial model was trained on limited data and may produce biased sample selections throughout the learning process. To address these limitations, other sampling strategies focus on the diversity~\cite{8595005, dong2020cost, aggarwal2021minority} of selected samples. For example, SAL~\cite{8595005} employs active learning by training a similarity model that recommends unlabeled rare class samples for manual labeling.

