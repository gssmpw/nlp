\section{Methods}
\label{sec:methods}

\subsection{Re-balancing}


\subsubsection{Re-sampling}\

\textbf{Synthetic sampling} Random oversampling is the most basic method of dealing with unbalanced data. The algorithm first copies randomly selected samples from the minority class samples and then adds the generated sample set to it. Although it simply adds the copied data to the original data set, and multiple instances of some samples are same, it is also possible that the classifier learned from this dataset may be overfitting. Especially, the random oversampling algorithm causes the classifier to generate multiple rules for multiple copies of the same sample, making these rules too specific.

The Synthetic minority oversampling technique is proposed to solve the problem above. The basic idea of this algorithm is: First, looking for $k$ homogeneous nearest neighbor samples of each minority class sample (where $k$ is usually an odd number greater than 1), then randomly select one of the $k$ nearest neighbors, and the linear interpolation was performed in these 2 samples to randomly construct a new artificial minority sample. This method can effectively solve the problem of classification overfitting and can significantly improve the learning ability of the classifier. 

However, the SMOTE algorithm \cite{ref_2} produces the same number of synthetic data samples for each of the original minority samples, without considering the distribution characteristics of its neighboring samples, making it more likely to repeat in the same class. In order to solve the above problem, many improved algorithms have been proposed in recent years. 

The D-SMOTE~\cite{ref_10} algorithm is proposed to generate the artificial samples by using the nearest neighbor sample mean points; The N-SMOTE~\cite{ref_11} is based on neighbor calculation formula of surrounding space structure information; In addition, some adaptive sampling methods have also been proposed. Representative algorithms include the Borderline-SMOTE algorithm~\cite{ref_4} and adaptive synthetic sampling (ADASYN)~\cite{ref_5}. SMOTE algorithm generates synthetic samples for each minority sample, while Borderline-SMOTE only generates synthetic samples for those minority samples close to the boundary. The main idea of ADASYN algorithm is to use density distribution as the criterion for automatically determining the number of synthetic samples, and to change the number of samples by self-adapting. Furthermore, MSMOTE~\cite{ref_12} divides the instances of the minority class into three groups, safe, border and latent noise instances by the calculation of the distances among all examples. When MSMOTE generates new samples, it only selects the nearest neighbor for border instances and does nothing for latent noise instances.

In addition, some data cleaning technologies have been widely used to solve the problem of noise samples caused by SMOTE technology. The most representative one is SMOTE + Tomek~\cite{ref_13} algorithm, which combines SMOTE with Tomek algorithm. This algorithm first uses SMOTE algorithm to generate synthetic samples and then uses the Tomek algorithm to clean the synthetic samples from these two sources, which can overcome the noise problem caused by SMOTE.

In addition to the SMOTE algorithm, some scholars also proposed the method of generating the second kind of artificial data by using probability density. For example, Normal distribution~\cite{ref_14} and Gaussian distribution~\cite{ref_15} are the two commonly used methods, they use the appropriate probability distribution to generate heterosexual samples, and then transforms the problem of unbalanced data into two kinds of problems of balanced data to solve the problem of unbalanced data. 

\textbf{Adaptive down-sampling} Unlike over-sampling, which adds data to the original data set, under-sampling removes data from the original data set. The most basic under sampling technology is random undersampling, that is, to reduce the size of the majority sample by randomly reducing the number of the majority sample, to get a balanced dataset. But this method may discard potentially useful data, which could be important for the induction process.

A more efficient way to use the under-sampling technique is Tomek Link, A pair of examples is called a Tomek link if they belong to diﬀerent classes and are each other’s nearest neighbors~\cite{ref_16}. Undersampling can be done by removing all Tomek links from the dataset. An alternate method is to only remove the majority class samples that are part of a Tomek link.

Another type of undersampling method is Condensed Nearest Neighbor (CNN)~\cite{ref_6}, it chooses a subset U of the training set T such that for every point in T its nearest neighbor in U is of the same class. Such a method can be slower compared to other methods since it requires many passes over the training data. Further, because of the randomness involved in the selection of points at each iteration, the subset selected can vary signiﬁcantly. Besides, the edited nearest neighbor (ENN)~\cite{ref_17} is the nearest neighbor rule under-sampling method, it removes points whose class label differs from a majority of its$k$nearest neighbors. However, most of the samples near the majority class samples are the majority class, so the samples can be deleted by this method are very limited.

Based on ENN, the Neighborhood Cleaning Rule (NCL)~\cite{ref_18} are proposed. The core idea of it is to find the$k$nearest neighbor samples for each sample in the training sample set. For example, if the sample is a majority class sample and there are more than two of its three nearest neighbors are minority class samples such sample will be deleted. Besides, if the sample is a minority class, it will be deleted; otherwise, when the sample is a minority class and more than two of its three nearest neighbors are majority class samples, the majority class samples in the nearest neighbors will be removed. However, the noise samples in the minority class samples cannot be taken into consideration in this method, and most of the samples deleted by the second method belong to boundary samples, which will have a great negative impact on the classification of subsequent classifiers.

For the reason above, The Near Miss family of methods~\cite{ref_19} are proposed and they perform under-sampling of points in the majority class based on their distance to other points in the same class. In NearMiss-1, those points from the majority class samples are retained whose mean distance to the$k$nearest points in the minority class samples is lowest. In contrast to NearMiss-1, NearMiss-2 keeps those points from the majority class samples whose mean distance to the$k$farthest points in the minority class samples is lowest. The ﬁnal Near Miss variant, NearMiss-3 selects$k$nearest neighbors in the majority class samples for every point in the minority class samples. In this case, the under-sampling ratio is directly controlled by$k$and is not separately tuned.

In addition to the above methods, there are other types of under-sampling methods, such as one-sided selection (OSS) ~\cite{ref_9}. This method divides majority class samples into "noise samples", "boundary samples" and "safety samples", and then removes boundary samples and noise samples from the majority. It aims to keep the samples which have a certain amount of information and a certain spatial representativeness. Based on the above ideas, other methods proposes to use a clustering method to obtain spatially representative samples. Firstly, majority class samples are clustered, and the number of clusters is the same as that of the minority class samples. Then, the center of each cluster is extracted as the majority class samples, so that the selected samples of most classes can have a certain spatial representativeness. The typical clustering methods are spectral clustering and kernel clustering.

\textbf{Hybrid sampling} \
The method of hybrid sampling is mainly based on SMOTE algorithm for fusion and improvement. SMOTE usually lacks a good representation or clear structure of the minority samples under extremely unbalanced conditions due to the severe overlapping (the few minority samples generally present simultaneously). Oversampling methods that rely directly on analyzing relationships between a few class samples to produce new data in this situation (such as SMOTE and its variants) will usually fail or even reverse optimizing.

There are some variations combining SMOTE with other methods that attempt to improve the failure of SMOTE algorithms in high imbalance ratio: (1)SMOTE+Tomek Links and SMOTE + ENN\cite{batista2004study}: Tomek linkage is a data cleaning method used to identify the nearest neighbor pairs in a data set that have different categories. Removing one or both of these pairs (for example, the samples in most classes) reduces noise or ambiguity in the decision boundaries of the training data set. The method will be done by oversampling a few classes using the SMOTE method first to achieve balance, then identifying and removing the example in the Tomek link from most classes. Another popular undersampling method is the "edit nearest neighbor" or "ENN" rule. This rule involves using the k = 3 nearest neighbors to locate those examples in the dataset that are misclassified and subsequently deleted. It can be applied to all classes, and those examples in most classes.
(2)SMOTE-RSB\cite{ramentol2012smote}: This paper uses the Rough set theory to clear the artificial data points produced by SMOTE and avoid creating noisy samples or those that cause discontinuity in the general distribution. In this way, SMOTE can avoid reverse optimization under high imbalance. (3)SMOTE-IPF\cite{saez2015smote}: Intrinsic limitations of SOMTE can aggravate the data distribution problem produced by noisy and borderline examples.
This paper proposes an iterative ensemble-based noise filter called Iterative-Partitioning Filter(IPF) as the extension of SMOTE, which can overcome the noise and borderline examples in imbalanced datasets.

In addition to improving the SMOTE algorithm, \cite{pouyanfar2018dynamic} proposes a dynamic sampling method. This method uses the idea of sample upgrading. It dynamically adjusts the data set according to the training results:  samples of the class with good results are randomly deleted, and samples of the class with poor results are randomly copied to ensure that the classification model can learn relevant information every time.

\textbf{Semi-supervised learning} \
In the case of imbalanced data, the value of label information needs to be considered. On the one hand, supervised learning with labels can get better results than unsupervised learning, that is, the label is positive and valuable.
On the other hand, imbalanced training data categories will cause the classification decision surface of the classifier to favor categories with more data, that is, the label is negative.
In this regard, semi-supervised learning can be introduced to make good use of labeled and unlabeled data and synthesize the value of labels to alleviate the problem of data imbalance.

The problem of data imbalance can be understood and promoted from two aspects: semi-supervised and self-supervised \cite{yang2020rethinking}. On the one hand, more unlabeled data can be used for semi-supervised learning to compensate for the lack of imbalanced category supervision information and improve classification ability. Specifically, the model can be trained with labeled data first. Then, the trained model can be used to predict the unlabeled data, and the predicted results will be used as Pseudo-Labels. Finally, the unlabeled data marked with Pseudo-Labels will be put into the labeled data set to continue the supervised learning. On the other hand, imbalanced labels can lead to label bias, which can be overcome by self-supervised learning of the pre-training model initialized by self-supervised pre-training(SSP) to learn irrelevant feature information and then add label information to continue training at the later stage of training.

Aiming at label quality degradation caused by category imbalance, \cite{kim2020distribution} proposes a convex optimization method to refine the pseudo-labels generated by biased models gently. An algorithm called Distribution Aligning Refinery of Pseudo-label (DARP) is developed to solve the problem effectively.

% generate minority 
ECRT \cite{chen2021supercharging} argues that unlike standard empirical risk minimization (ERM) schemes, causally-inspired learning embraces robustness against potential perturbations, and can effectively block or attenuate contributions from spurious, unstable features. To achieve this, ECRT leverages the causal representation encoder, which is based on generalized contrastive learning, to perform independent component analysis and further disentangle learned features into the source space. By learning from all labeled data, the components of the source space become statistically independent, leading to better causal generalization and facilitating the synthesis of minority samples.




\subsubsection{Cost sensitive learning (re-weighting)}\


Generally, the classification algorithms in machine learning aim at getting the maximum accuracy, but the cost-sensitive algorithm aims at getting the minimum cost. The difference is showed in the following tables.


\begin{table} 
\centering
\caption{General algorithm based on accuracy}  
\begin{tabular}{|c|c|c|}
\hline  
 & Actual positive  & Actual negative \\  
\hline  
Predict positive  & TP (true positive) & FP (false positive) \\  
Predict negative  & FN (false negative) & TN (true negative) \\  
\hline  
\end{tabular}  
\end{table}

\begin{table} 
\centering
\caption{Cost-sensitive algorithm based on cost}  
\begin{tabular}{|c|c|c|}
\hline  
 & Actual positive  & Actual negative \\  
\hline  
\centering
Predict positive  & TP $C_{11}$ & FP $C_{10}$ \\  
Predict negative  & FN $C_{01}$ & TN $C_{00}$ \\  
\hline  
\end{tabular}  
\end{table}

In traditional algorithms, which based on accuracy as the Table 1 shows, the objective is to maximize the true parts (i.e., TP+TN) which is the same as minimize the false parts (i.e., FP+FN). However, traditional algorithms think FP and FN are both wrong and do not care the difference of FP and FN. For example, in the problem of bank loan classification, the FP, which means the bank thinks the lender can pay the loan while actually the lender can not pay the loan, is more harmful than the FN. The cost-sensitive algorithm is more suitable to the bank loan classification problem. The cost-sensitive algorithm assigns different cost value at different part as the Table 2 shows. The objective of the cost-sensitive algorithm is to minimize $|FP|*C_{10}+|FN|*C_{01}$. The cost-sensitive algorithm is more suitable in many real world problems such as risky behaviors recognition and sever disease diagnosis than the traditional algorithm 
The loss function of the cost-sensitive algorithm is as follows.

\begin{equation} R(x,i)=\sum_{j}P(j|x)c(i,j) \end{equation}

For a classification algorithm A, it assign an instance x to class i, while the instance x is in class j. $P(j |x)$ means the probability of the instance x to be predicted as class j, and $ c(i,j)$ means actual $i$ is predicted as class $j$.
There are many ways to implement cost sensitive learning which can be categorized into three kinds. The first kind of techniques applies misclassification cost to data set as a form of data space weighting, which we call it meta-learning because it changes the scale of every class in data preprocessing procedure. The second kind of techniques incorporates cost sensitive features directly into classification paradigms to essentially fit the cost sensitive framework into these classifiers which we call it direct methods.

\textbf{ICET}
ICET  (Inexpensive Classification with Expensive Tests)\cite{ref_A} combined a decision tree algorithm RG2 and a genetic algorithm GENESIS. ICET uses a genetic algorithm (Grefenstette, 1986) to evolve a population of biases for a decision tree induction algorithm (a modified version of C4.5, Quinlan, 1992). The fitness function of the genetic algorithm is the average cost of classification when using the decision tree, including both the costs of tests (features, measurements) and the costs of classification errors. ICET has the following features: (1) It is sensitive to test costs. (2) It is sensitive to classification error costs. (3) It combines a greedy search heuristic with a genetic search algorithm. (4) It can handle conditional costs, where the cost of one test is conditional on whether a second test has been selected yet. (5) It distinguishes tests with immediate results from tests with delayed results
ICET uses a two-tiered search strategy. On the bottom tier, EG2 performs a greedy search through the space of decision trees, using the standard TDIDT strategy. On the top tier, GENESIS performs a genetic search through a space of biases. The biases are used to modify the behavior of EG2. In other words, GENESIS controls EG2’s preference for one type of decision tree over another.

\textbf{Cost-senstive decision trees}
Incorporating cost into decision tree classification algorithm is another example of directly methods. Cost can be incorporated into it in various ways. First way is cost can be applied to adjust the decision threshold, second way is cost can be used in splitting attribute selection during decision tree construction and the other way is cost sensitive pruning schemes can be applied to the tree.

\textbf{Meta-cost} 
Meta-cost algorithm \cite{ref_B} is a general method to make non-cost-sensitive algorithms to be cost-sensitive. The key concept is to change the golden class label according to the minimized cost function $R(x,i)=\sum_{j}P(j|x)c(i,j)$ and use the refreshed labels to train a new classifier. Generally, first using bootstrap resample to generate n examples, then on each example train a classifier, therefore we get n different classifiers. Then using bagging voting to generate the class probability for each instance, then using the formula to get the new label.
\begin{equation}
    i= \arg\min_{i} R(x,i)=\arg\min_{i} \sum_{j} P(j|x)c(i,j)
\end{equation}  

Then change the label and redo the procedure.
Meta-cost assumed that the cost for different instance is the same, that is $C(i,j,x)=C(i,j)$ which is not suitable in real word. Also bagging is not suitable for computing the $P(j|x)$.

\textbf{Empirical Thresholding}
Empirical Thresholding \cite{ref_C} selects a proper threshold from training instances according to the misclassification cost. Similar to other cost-sensitive meta-learning methods, Thresholding can convert any existing (and future) cost-insensitive learning algorithms and techniques into cost-sensitive ones.
Thresholding can learn the threshold from samples. The threshold is a representative of different cost as well as sample distribution. Almost all classification methods can produce probability estimates on instances (both training instances and test instances). Thresholding simply finds the best probability from the training instances as the threshold, and use it to predict the class label of test instances: a test example with predicted probability above or equal to this threshold is predicted as positive; otherwise as negative. Thus, for a given threshold, the total misclassification cost for a set of examples can be calculated, and the total misclassification cost($M_c$) is a function of threshold($T$). The curve of this function can be obtained after computing misclassification costs for each possible threshold. Then we choose the best threshold that minimizes the total misclassification cost on training data. There are three types of the curves for the function $M_c=f(T)$ as shown in Figure 4.


Figure 4(a) is the ideal case, then the T is chosen. In figure4(b), the smaller cost is chosen then T1 is chosen. In figure4(c), T1 and T2 have the same cost, then we use a heuristic to resolve the tie: choose the less steep one, because we prefer a local minimum that is less sensitive to small changes in the threshold selection. The in figure4(c), we choose T2.
Another problem is that the best threshold obtained directly from the training instances may not generalize well for the test instances. To reduce overfitting, Thresholding searches for the best probability as threshold from the validation sets. More specifically, an m-fold cross-validation is applied, and the base learning algorithm predicts the probability estimates on the validation sets. After this, the probability estimate of each training example is obtained (as it was in the validation set). Thresholding then simply picks up the best threshold that yields the minimum total misclassification cost (with the tie breaking heuristic described earlier), and use it for the test instances. Note that the test instances are not used for searching the best threshold.

\textbf{Costing}
Sampling convert the classifier learning algorithms and classification theory into cost-sensitive algorithms and theory based on cost-proportionate weighting of the training examples, which can be realized either by feeding the weights to the classification algorithm (as often done in boosting), or by careful subsampling. In many sampling methods, costing achieves better predictive performance.
Costing \cite{ref_D} based on cost-proportionate rejection sampling and ensemble aggregation. Costing based on the following theorem,

\begin{equation}
    \hat{D}(x,y,c)=\frac{c}{E_{x,y,c \sim D}[c]}D(x,y,c)
\end{equation}

We assume that examples are drawn independently from a distribution D with domain $X \times Y \times C$ where X is the input space to a classifier, Y is a (binary) output space and $C\in [0,\infty]$ is the importance (extra cost) associated with mislabeling that examples.  The goal is to learn a classifier$ h : X \to Y $which minimizes the expected cost: $E_{x,y,c \sim D} [c I(h(x) \ne y)]$. The theorem is the optimal rate classifier for $\hat{D}$ are optimal cost minimizers for D.

Rejection sampling allows us to draw examples independently from the distribution $\hat{D}$, given examples drawn independently from D.  In rejection sampling, examples from $\hat{D}$ are obtained by first drawing examples from D, and then keeping (or accepting) the sample with probability proportional to $\frac{\hat{D}}{D}$  . Here, we have $\frac{\hat{D}}{D} \propto c$, so we accept an example with probability c/Z, where Z is some constant chosen so that $max_{x,y,c \in S } c \le Z$ leading to the name cost-proportionate rejection sampling. Rejection sampling results in a set S’ which is generally smaller than S. Furthermore, because inclusion of an example in S’ is independent
of other examples, and the examples in S are drawn independently, we know that the examples in S’ are distributed independently according to $\hat{D}$.

\textbf{Weighting}
In traditional process, each instance is assigned 1 and the total weight of the dataset is N if there are N instance in the dataset. We can reweight the instance to let the weight distribution consistents with the cost distribution. The forllowing reweighting approch is used.

\begin{equation}
    w(j)=C(j)\frac{N}{\sum_{i}C(i)N_i}
\end{equation}
such that the sum of all instances weights is $ \sum_{j}w(j)N_j = N $

Let N to be the number of instances of the dataset. $N_j$ be the number of instances that are in class j. Let C(j) be the cost of misclassifying a class j instance. C(j) is computed according to
the cost matric which is discribed in the previous section.


\subsubsection{Logit adjustment}
Logit compensation aims to eliminate the bias caused by the imbalance of data and learn the rectification of the boundary. The compensation can be applied during either training or testing.
Logit adjustment can be categoried into two classes, which includes: posterior re-calibration and margin adjustment.

\textbf{Posterior re-calibration}

\textbf{Margin adjustment}
LDAM \cite{cao2019learning} extends the existing soft margin loss to imbalanced data scenarios by encouraging the minority classes to have larger margins so that the model can improve the generalization error of minority classes. It starts from the classification error bound and theoretically obtain the optimal trade-off of margins among multiple classes which is inversely proportional to the fourth root of the number of examples in each class. With the optimal margins, the LDAM can be extend to any sort of classification loss functions, such as Hinge loss and Cross entropy loss.
NSL-AAM \cite{} analyses the softmax loss in imbalance setting and proposes an adaptive angle margin penalty which is added to the angle between sample embeddings and the weights of the last linear layer. The angle margin penalty of the class is proportional to the effective sample number of the class, and the larger effective numbers of the class is, the larger angle margin penalty the class suffers.



\subsection{Feature representation}

% \subsubsection{Feature selection}

The performance of machine learning classification methods significantly relies on feature quality. However, the high dimensionality of features in real world scenarios presents a challenging problem to classifiers and the class imbalance problems become even more severe when irrelevant features are added in the feature space. For example, in text classification, the number of features in a bag of words is often more than an order of magnitude compared to the number of training documents \cite{xiong2006kernel}. In microarray-based cancer classification, the number of features is typically tens of thousands \cite{forman2003extensive}. In these problems with high-dimensional features, the feature selection methods are designed to select the most useful features for classification and improve the performance of minority classes. 
When employing feature selection to imbalanced data, the main challenge is a trade-off between removing irrelevant features and keeping useful features. Removing somewhat irrelevant features may also risk losing potentially useful information because the original data distribution may be altered by feature selection.
Specifically, the feature selection methods can be categorized into filter methods \cite{zheng2004feature, grobelnik1999feature} and wrapper methods.
For example, FAST \cite{chen2008fast} directly takes the classification metric into account and construct a feature selection metric based on an ROC curve. By selecting features with the highest area under the curve as the most relevant, the classification performance of minority class are improved.   


\subsubsection{Metric learning}
\cite{Ren_2022_CVPR}
\cite{Li_2022_CVPR}
\cite{park2021influence}

Metric learning aims to learn a discriminative feature space by designing task-specific distance metrics between samples, which makes samples of the same class closer together, while samples of different classes are further apart \cite{duan2017deep}. It achieves great success in feature representation due to the powerful capacity of understanding the similarity relationship among samples. Thus, metric learning methods has been wildly applied to mitigate the imbalanced classification problems to increase the separability between minority and majority classes and achieve a better classification performance on minorities. 

\textbf{Sample selection.}
In metric learning methods, sample selection is the most important role because it directly influences the distances calculation and the quality of the extracted similarity relationship. Therefore, the sample selection of metric learning methods for imbalanced data always emphasizes the minority class \cite{huang2016learning, dong2017class, wang2018iterative, gui2022quadruplet, yan2023borderline} and carefully selects samples to design the distance metrics.  

To balance the training samples used in the training procedure, LMLE \cite{huang2016learning} clusters samples in each class and construct the sample sets by selecting four representative samples from neighboring clusters and classes for each anchor sample. Benefiting to clustering, LMLE captures the significant data variability within each class and greatly enriches the diversity of minority sample sets for distance evaluation. According to four distances defined on the selected sample sets, the triple-header hinge loss constrains the feature representation and enforces a local margin while handling class imbalance problems. 
However, the clustering is sensitive to the entire training data and class size and usually gets limited results for extremely sparse minority classes. 
To address this problem, class rectification loss \cite{dong2017class} construct the triplets by identifying hard-positive and hard-negative samples for minorities in each training batch and designs a batch-wise hard mining metric based on relative, absolute, and distribution information among selected triplets. By constraining hard-positive samples, it can discover and expand sparsely sampled minority class boundaries, while mining hard-negative samples can improve the margins of minority class boundaries that are corrupted by similar imposter classes, thereby mitigating both intra-class compactness and inter-class sparsity in feature space.
LSTM-QDM \cite{gui2022quadruplet} utilizes the minority sample to expend the data triplet in the triplet network \cite{cheng2016person} and constructs a quadruplet loss to balance the relationship among anchor, positive, negative and minority samples. Benefiting from the extra minority samples in quadruplet loss and optimizing the softmax loss jointly, LSTM-QDM achieves a stronger representation ability for imbalanced imbalanced time-series data.
Inspired by Borderline-SMOTE, DMFBML \cite{yan2023borderline} separates the samples into three categories: danger, safe and noise based on the ratio of neighbors from the different classes and generates the triplets by the guidance of three categories. 
Afterward, DMFBML designs a borderline margin loss function to customize the margins among different categories and assign more metric constraints for minority samples, thereby the minority and majority samples in overlapping regions could be separated.

% Iterative metric learning \cite{wang2018iterative} argues that an high-quality data space in which the boundaries between different classes become more clearly is essential for classification especially for imbalanced data. Thus, instead of using all training data, it selects the most related subset of training samples form positive and negative training data classes to mitigate the imbalance problem and utilizes the benchmark metric learning method: Large Margin Nearest Neighbor method \cite{weinberger2009distance} to transform the data space iteratively until achieving a stable and effective data space.

\textbf{Distance calculation.}
Besides introducing minority samples in the sample selection strategy, other works handle the imbalance data and constrain the minority class from distance calculation.
Range loss \cite{zhang2017range} explicitly defines intra-class loss as the harmonic mean of the k-largest distances between samples within each class, and constructs inter-class loss by measuring the minimum distance among class centers. By optimizing these two components, range loss can simultaneously reduce the intra-class variations of different classes and enlarge the inter-class distance.
To treat different classes equally and separate them balanced, DMBK \cite{feng2018learning} first assumes that different classes are sampled from Gaussian densities with different expected values but one identical covariance, and then defines normalized divergence using KL-divergence to represent inter-class divergence. Afterword, inspired by arithmetic-geometric average inequality, DMBK makes use of the geometric mean to constrain the normalized divergences and make all inter-class divergences balanced. This procedure separates all classes in a balanced way and avoids neglected important minorities and inaccurate similarities incurred by imbalanced class distributions.
The Affinity loss function \cite{hayat2019gaussian} proposes to measure class similarities for an input feature in Euclidean space using a Gaussian similarity measure, specifically in terms of Bergman divergence. Additionally, it utilizes a diversity regularizer to constrain all class centers to be uniformly distributed throughout the feature space. This approach helps the model avoid the majority classes from occupying a disproportionately larger angular space compared to the minority classes, leading to a more balanced embedding space.
Imbalanced metric learning \cite{gautheron2020metric} designs a new weighting method for Mahalanobis distance-base metric learning methods by decomposing the pair of samples based on their labels and introduces the hyper-parameters to control the negative effect of the imbalance. Moreover, imbalanced metric learning derives a generalization bound theoretically involving the proportion of positive samples using the uniform stability framework. 

% explores the correlations among the imbalance data and aims to find more stable neighborhoods of training data for the testing data, to construct an neighbor-based classification in an effective data space. Specifically, Iterative metric learning chooses the benchmark metric learning method: Large Margin Nearest Neighbor method \cite{weinberger2009distance} to transform the data space. This can simultaneously separate the samples from different classes by a large margin and minimize pairwise distances between samples from the same class. To get more reliable results, Iterative metric learning proposes to employ the metric learning iteratively and embeds the original data space into a more stable and effective data space, in which the boundaries between different classes become more clearly, and then select the most related training samples for each test sample to solve imbalanced issue among training data. 
%For most traditional DML methods that have natural tendencies to favor the majority classes, they care more about the distinctions between majority classes with different labels. Distances between samples from those majority classes will be further emphasized while minority classes may be neglected easily. Therefore, it is of vital importance for DMBK to treat samples from different classes (both majority classes and minority classes) equally and separate them balanced.
% In addition to directly defining intra-class and inter-class distances, some metric learning methods, such as those proposed by \cite{huang2016learning} and \cite{dong2017class}, achieve this by carefully selecting and grouping samples.
% LMLE \cite{huang2016learning} clusters samples in each class using k-means and then selects four representative samples from different clusters and classes for each anchor sample. Instead of using all training data, LMLE defines four distances among these selected samples and constructs a triple-header hinge loss to constrain the feature representation and balance the training samples used in the training procedure. By leveraging clustering and a distance-based loss function, LMLE captures the significant data variability within each class and enforces a local margin while handling class imbalance problems. However, the clustering is sensitive to the entire training data and class size and usually gets limited results for extremely sparse minority classes. 
% To address this problem, class rectification loss \cite{dong2017class} identifies hard-positive and hard-negative samples for minorities in each training batch and designs a batch-wise hard mining metric based on relative, absolute, and distribution information among selected triplets. By constraining hard-positive samples, it can discover and expand sparsely sampled minority class boundaries, while mining hard-negative samples can improve the margins of minority class boundaries that are corrupted by similar imposter classes, thereby mitigating both intra-class compactness and inter-class sparsity.
 




\subsubsection{Supervised contrastive learning}

% The core idea of contrastive learning is to align the positive sample pairs and repulse the negative sample pairs.
Supervised contrastive learning has emerged as a promising research direction in recent years, combining metric learning and self-supervised contrastive learning to leverage the advantages of both methods while overcoming their respective limitations \cite{khosla2020supervised}. 
On the one hand, conventional metric learning methods (e.g., the triplet loss \cite{weinberger2009distance} and N-pair loss \cite{sohn2016improved}) and self-supervised contrastive learning \cite{tian2020contrastive} limit the number of positive samples for each anchor to one, which places more emphasis on optimizing individual pairs and hinders the extraction of richer inter-sample information \cite{khosla2020supervised}. 
On the other hand, while contrastive learning methods have been found to generate a balanced feature space and maintain stable performance, even when the degree of imbalance becomes increasingly severe \cite{yang2020rethinking, kang2021exploring}, the features learned through contrastive learning lack semantic separability since they are learned without class label guidance, leading to poor classification performance. 
To address these limitations and handle the imbalance problem, supervised contrastive learning methods incorporate label information into the contrastive learning process and extend positive examples by including other samples from the same class along with data augmentations. This encourages the model to produce closely aligned representations of all samples from the same class and results in a more robust clustering of the representation space. 

SupCon \cite{khosla2020supervised} first extends contrastive learning to the fully-supervised setting and surpasses the performance of the traditional supervised cross-entropy loss by leveraging class labels with a contrastive loss. Although achieving performance improvement, researchers discover that the supervised contrastive loss still suffers from the imbalance of positive/negative pairs. To mitigate this issue and further enhance supervised contrastive learning for imbalanced data, the improved methods can be categorised into three classes, including {re-balanced sampling, re-weighting, and pair reconstruction}.
% As a special type of metric learning method, This approach extends self-supervised contrastive learning to the fully-supervised setting, allowing for the utilization of label information. The superiority of supervised contrastive learning can be attributed to its ability to consider multiple positives per anchor, in contrast to traditional metric learning methods (e.g., the triplet loss \cite{weinberger2009distance} and N-pair loss \cite{sohn2016improved}) and self-supervised contrastive learning \cite{tian2020contrastive} that only use a single positive sample. By sampling positive examples from the same class as the anchor sample, along with data augmentations, the supervised contrastive loss enables implicit hard positive/negative mining, and encourages the model to produce closely aligned representations of all samples from the same class, resulting in a more robust clustering of the representation space.
% Contrastive learning is a self-supervised learning technique that maps inputs to a feature space, where similar inputs are grouped together without relying on label information. The model is trained to maximize the similarity between positive samples and minimize the similarity between negative samples. 
% In contrast to supervised methods that demonstrate a significant performance drop in scenarios with imbalanced data, Kang et al. \cite{kang2021exploring} discovered that contrastive learning methods can generate a balanced feature space and maintain stable performance, even when the degree of imbalance becomes increasingly severe.
% However, the features learned through contrastive learning lack semantic discriminativeness because they are learned without the guidance of class labels and tend to perform poorly in classification tasks. 
% To address this limitation, several recent works \cite{kang2021exploring, cui2021parametric, wang2021contrastive} propose introducing the contrastive learning method into supervised tasks to learn discriminative and balanced representations for imbalanced data.

\textbf{Re-balanced sampling} methods aim to address the issue of imbalanced positive/negative pairs by sampling techniques. 
KCL \cite{kang2021exploring} improves upon the positive set construction rule in SupCon by randomly selecting k instances for each anchor from the same class to form the positive set, instead of using all the instances from the same class. This modification ensures that there is an equal number of positive samples for each anchor sample, thereby avoiding the dominance of instance-rich classes in the representation learning process.
CA-SupCon \cite{zhang2022class} argues that due to the scarcity of minority samples, SupCon cannot guarantee the separability of minority classes in one mini-batch. To address this issue, CA-SupCon introduces a class-aware sampler that re-balances the data distribution within each mini-batch during training, thus allowing each minority class to have a fair chance to contribute to the feature distance optimization.
GPCL \cite{jiang2021guided} introduces self-training to supervised contrastive learning to provide pseudo-labels for unlabeled data. By incorporating these pseudo-labels with confidence scores, GPCL can make full use of unlabeled data to enrich the contrastive data while filtering out unreliable negative and positive pairs with the same pseudo-labels and low confidence scores in the contrastive loss, respectively.


\textbf{Re-weighting} methods aim to increase the importance of minority pairs in the contrastive loss. For example, PaCo \cite{cui2021parametric} introduces a set of parametric class-wise learnable centers into the contrastive loss. This approach adaptively weighs the samples from different classes and enhances the intensity of pushing samples of the same class closer to their corresponding centers, effectively giving more weight to the minority class pairs.
BCL and TSC \cite{zhu2022balanced, li2022targeted} argue that supervised contrastive learning should ideally result in an embedding where different classes are uniformly distributed on a hypersphere. However, applying the contrastive loss to imbalanced data can lead to poor uniformity, with minority classes having poor separability in the feature space. 
BCL provides evidence for this claim through a theoretical analysis of the lower bound of the contrastive loss. The analysis shows that imbalanced data distribution heavily affects the model and the resulting representations of classes no longer attain a regular simplex configuration, which ultimately hinders the final classification performance.
Therefore, to address this issue, BCL proposes three class-averaging solutions to re-weight the negative pairs in the supervised contrastive loss. This helps to reduce the gradients from negative majority classes and contributes to an asymmetrical geometry configuration. Additionally, to ensure that all classes appear in every mini-batch, BCL introduces class-center representations in the positive and negative pairing, compensating for the batch sampling bias of high-frequency classes.Although motivated by the same insight, it is worth noting that TSC proposes a distinct solution from BCL. TSC aims to maintain a uniform distribution in the feature space for all classes by urging the features of classes closer to the uniformly distributed target features on the vertices of a regular simplex. To maintain semantic relations among classes, TSC proposes an online matching method to ensure that classes that are semantically close to each other are assigned to targets that are also close to each other.

 
\textbf{Pair reconstruction.} 
In addition to re-balanced sampling and re-weighting techniques, researchers also attempt to leverage the {pair reconstruction} in supervised contrastive learning to mitigate the data imbalance.
SNP-ECC \cite{gao2022ensemble} and MLCC-IBC \cite{gao2023imbalanced} extend the point-to-point contrastive learning approach to a point-to-group approach, and mitigate the imbalance problem by enriching the training samples. They construct contrastive sample groups by randomly sampling from the k-nearest majority/minority neighbors of the anchor sample, respectively. Afterwards, the new-format training data are generated by pairing the anchor sample with various contrastive sample groups. Due to the increased diversity of the contrastive sample groups, the new training data pairs are balanced and are learned using a novel label matching task.
ProCo \cite{yang2022proco} proposes to utilize the category prototype to augment representative samples and generate the adversarial proto-instance via linear interpolation in the feature space. The adversarial proto-instance is designed as a special outlier that can balance the data and encourage ProCo to rectify the decision boundaries of the minority categories during contrastive learning.



\subsubsection{Prototype learning}
In addition to exploring correlations among extensive and complex samples via metric learning and supervised contrastive learning, researchers have incorporated the idea of prototype learning \cite{snell2017prototypical} from few-shot learning into the imbalanced data scenario to enhance feature representations of minority classes.
A prototype explicitly represents a summary of all instances belonging to a specific class or intrinsic sample characteristics. In the context of imbalanced data, prototypes can improve minority representation from three aspects: {facilitating the generation of a more discriminative feature space}, {compensating for class information compensation}, and {aiding in the extraction of intrinsic sample information}.


\textbf{Discriminative feature space generation.}
Class-specific prototypes provide a central target for each class in the feature space. This enables sample embeddings explicitly converge toward the class center, contributing to a more distinctive feature space for classification. 
Hybrid-PSC \cite{wang2021contrastive} improves the supervised contrastive learning method by treating the prototypes as the pairing object for positive/negative pairs, forcing each sample to be pulled towards the prototypes of its class and pushed away from prototypes of all the other classes. This approach allows for more robust and efficient data sampling when dealing with large-scale datasets under a limited memory budget 
The Affinity loss function \cite{hayat2019gaussian} provides a theoretical analysis of the Softmax loss with a linear prediction layer and treats the linear weight vectors as the class prototypes. Based on this, the affinity loss function can automatically learns the prototypes for each class and conveniently measures feature similarities with class prototypes in Euclidean space using Bergman divergence.
Instead of calculating prototypes from labeled data, TSC \cite{li2022targeted} manually generates a set of targets uniformly distributed on a hyper-sphere as the class prototypes and forces all classes to converge to these distinct distributed targets during training for a uniform distributed feature space.
BCL \cite{zhu2022balanced} uses supervised contrastive loss to train the model, and specifically ensures that each sample is paired with all other class prototypes in the negative pairing. By doing so, BCL compensates for the batch sampling bias of high-frequency classes, which in turn enables comprehensive contractive information from each class to be captured during training.
It is worth noting that the number of class prototypes is not limited to one, and studies \cite{wang2021contrastive, hayat2019gaussian} have shown that adopting a multi-prototype paradigm can capture richer within-class data distribution and promote diversity and discriminability in the feature space.

\textbf{Class information compensation.}
As a summary of classes, class prototypes contain basic class information that can help compensate for the information shortage of minority classes by transferring knowledge across classes or generating minority samples directly through data augmentation.
OLTR \cite{liu2019large} utilizes an attention mechanism to adaptively merge all prototypes with each sample embedding. This helps transfer basic class concepts among all classes to the feature embeddings, thereby enhancing the minority embeddings.
PAN \cite{zeng2021pan} addresses the modality-imbalance problem, where the available data for text, image, and audio are all considerably imbalanced, by generating synthetic samples for the minority modalities. It learns semantically consistent prototypes that capture the semantic consistency of the same class across different modalities and uses a gated recurrent unit to fuse the nearest neighbors of the majority modality to generate new samples. 
MPCL \cite{fu2022meta} argues that empirical prototypes are severely biased due to the scarce data in minority classes and proposes a meta-learning method to calibrate the prototypes by contrastive loss. With the assumption that feature embeddings for each class lie in a Gaussian distribution, MPCL estimates the mean and covariance of class distribution by the calibrated prototypes. Then, MPCL samples new samples from the distribution as synthetic minorities to balance the classes.
Similarly, ProCo \cite{yang2022proco} proposes to utilize the class prototype to augment representative samples and generate the adversarial proto-instance to balance the data. This encourages ProCo to rectify the decision boundaries of the minority classes.

\textbf{Critical information extraction.}
Prototypes not only represent entire classes but can also be leveraged to extract intrinsic information from individual samples, which is crucial in handling typical characteristics of different classes. 
For instance, IEM \cite{zhu2020inflated} proposes to use the attention mechanism to learn multiple discriminative feature prototypes for each class, considering both local and global features, as a single prototype may not be sufficient to represent a class. By gradually updating these prototypes during the training procedure, IEM captures the representative local and global semantics of each class and incorporates more discriminative features to improve generalization on minority classes.
MPNet \cite{he2020learning} generates both instance and semantic prototypes shared by all samples, representing local representative features and semantic features, respectively. The learned prototypes are recorded in a memory module and used to represent each sample, addressing the issue of forgetting minority classes during training. Additionally, semantic memory regularization is applied for a separately distributed centroid space.




\subsubsection{Transfer learning}

Transfer learning is a technique that leverages salient features and patterns identified in a source domain to enhance performance in a target domain. In the context of imbalanced datasets, minority classes often suffer from a scarcity of labeled data, while majority classes benefit from an abundance of such data. Consequently, researchers concentrate on identifying shared characteristics or attributes among multiple classes and employ transfer learning to transfer common knowledge from well-represented majority classes to underrepresented minority classes.

\textbf{Sample synthesis.}
A substantial portion of transfer learning methods focus on transferring the majority class diversity to minority classes by synthesizing new minority samples.
For instance, FTL \cite{yin2019feature} presents a prototype-based transfer framework, predicated on a Gaussian prior encompassing all classes. Under this assumption, each class is partitioned into mean and variance, representing class-specific and class-generic factors, respectively. The shared variance of major classes, indicating commonalities between categories such as varying poses or lighting conditions, is conveyed to minor classes through the generation of synthetic samples.
LEAP \cite{liu2020deep}models the distribution of intra-class features based on the distribution of angles between features and their corresponding class centers. By assuming that these angles adhere to a Gaussian distribution, the angular variance learned from the majority class is transferred to each minority class, and synthetic samples for minority classes are drawn from the distribution with enhanced intra-class angular diversity.
Similarly, FSA \cite{chu2020feature} employs class activation maps to identify class-specific features from underrepresented classes and fuses them with class-generic features to generate minority samples. RSG \cite{wang2021rsg} devises a rare-class sample generator module, compatible with any backbone model, to create new samples based on the variation information of majority classes.
In contrast to the aforementioned methods that generate samples from minority instances, M2m \cite{kim2020m2m} directly transfers the diversity of majority classes by translating majority samples into minority samples. By referring to the output of the pre-trained classifier, a small optimized noise is added to the majority sample to facilitate the translation. In this manner, the richer information of majority samples is effectively leveraged to augment the minority classes.

In addition to sample synthesis, several methods endeavor to transfer knowledge in alternative ways.

MetaModelNet \cite{wang2017learning}, rather than transferring majority class knowledge directly, conveys the changing trend of model parameters and introduces a meta-learning method to learn the model dynamics, predicting how model parameters will alter as more training data is progressively incorporated. Specifically, MetaModelNet is trained to forecast many-shot model parameters from few-shot model parameters, which are trained on abundant and limited samples, respectively. By employing the learned model dynamics, MetaModelNet can directly enhance the model parameters trained on minority instances by predicting the parameters when additional minority samples are included.

OLTR \cite{liu2019large} explicitly constructs class centroids and exploits the attention mechanism to amalgamate sample features with class information from various classes, yielding more comprehensive feature representations. This information aggregation not only encompasses minority class information but also transfers basic class concepts among majority classes. Furthermore, the attention mechanism enables the model to adaptively tailor the importance of class information for different samples during end-to-end training, significantly enriching the minority features that lack adequate supervision signals.


GistNet \cite{liu2021gistnet} posits that all classes share a common geometric structure in the feature space. With numerous samples in the majority class, GistNet can encode the class geometry into a set of structure parameters shared by all classes, and the geometric structure of the minority class is restricted via learned structure parameters.

RVGAN-TL \cite{ding2023rvgan} initially generates minority samples using GAN and subsequently applies the TrAdaboost algorithm \cite{tradaboost} to assign different weights to the training samples. Synthetic minority samples that deviate from the real sample distribution are assigned extremely low weights, effectively transferring the distribution of real samples to the minority class and reducing the impact of noisy data.


% Add this as the applocation or future work
% Note that some related works focus on addressing the domain class imbalance problem in transfer learning \cite{9655605, 8215613, 8260654}, which occurs when the class distributions between the source and target domains are significantly different. It is important to distinguish the domain class imbalance problem from the class imbalance issue studied in this survey.

% \subsubsection{One-class learning}

\subsubsection{Meta-learning}

Meta-learning, often referred to as the process of learning to learn, aims to create a model capable of effectively generalizing across a variety of tasks rather than concentrating on instances from a single task. This approach leverages acquired meta-knowledge to address the shortage of training data for individual tasks and has been investigated for adapting models to imbalanced data distributions. Meta-learning methods achieve this goal through bi-level optimization, which consists of both inner and outer loops. The inner loop focuses on task-specific learning by training a model to excel at individual tasks. Meanwhile, the outer loop emphasizes learning across tasks and updates the model based on its performance on the inner loop tasks. To enhance the model's generalization and adaptability to imbalanced data distributions, researchers propose to optimize the model from multiple perspectives, which includes {weights, sampling method, model dynamics, data augmentation and class prototypes}.

\textbf{Weights.}
The majority of meta-learning-based methods \cite{ren2018learning, shu2019meta, lee2020l2b, jamal2020rethinking, zhang2021learning} concentrate on re-weighting methods for imbalanced data. These studies highlight the limitations of infeasible manually designed weighting functions and the challenges associated with applying hyper-parameters in loss functions. Consequently, these methods treat the weights for training samples as learnable parameters and optimize them accordingly.

LRE \cite{ren2018learning} first uses a clean and unbiased dataset as the meta set (i.e., development set) to guide the training procedure. By performing validation at every training iteration, LRE dynamically assigns importance weights for each batch sample.
Meta-Weight-Net \cite{shu2019meta} automatically learns an explicit loss-weight function, parameterized by a multi-layer perceptron (MLP) from data, resulting in more stable learning performance. 
Bayesian TAML \cite{lee2020l2b} learns a set of class-specific scalars to weight the learning rate of class-specific gradients for each inner loop optimization step, thus giving more attention to minority classes. 
MLCW \cite{jamal2020rethinking} analyzes the imbalance problem from a domain adaptation perspective. They assume that the class-conditioned distribution of training data and test data is not the same, especially for minority classes with insufficient training data. Therefore, MLCW proposes to explicitly account for the differences between conditioned distributions and utilizes meta-learning to estimate the conditional weights for each training example. 
FSR \cite{zhang2021learning} argued against the expensive second-order computation of nested optimizations in meta-learning and proposes a fast sample re-weighting method. By learning from history to build proxy reward data and employing feature sharing, FSR effectively reduces optimization costs.

\textbf{Sampling method.}
Besides learning adaptive weights for training sets, meta-learning method is deployed to learn a sampling method to re-balance the training data. Balanced Meta-Softmax \cite{ren2020balanced} introduces a balanced softmax method and argues that balance sampling for the mini-batch sampling process might lead to an over-balancing problem. In this scenario, minority classes dominate the optimization process, resulting in local minimums that favor these minority classes. To address this issue, Balanced Meta-Softmax introduces a meta sampler to explicitly learn the optimal sample rate for each class under the guidance of class-balanced meta set.
Instead of co-optimized with the classification model, MESA \cite{liu2020mesa} decouples the model-training and meta-training process and proposes a meta sampler that serves as an adaptive under-sampling solution embedded in the iterative ensemble training process. By incorporating the classification error distribution of ensemble training as its input, MESA achieves high compatibility and is applicable to various statistical and non-statistical classification models, such as decision trees, Naïve Bayes, and k-nearest neighbor classifiers.

\textbf{Model dynamics.}
MetaModelNet \cite{wang2017learning} utilizes meta-learning to learn model dynamics, predicting how model parameters will change as more training data is progressively incorporated. As a result, MetaModelNet can directly enhance model parameters trained on minority instances by predicting the parameters when additional minority samples are included.

\textbf{Data augmentation.}
MetaSAug \cite{li2021metasaug} employs meta-learning for data augmentation. Inspired by ISDA \cite{wang2019implicit}, it calculates class-wise covariance matrices to extract semantic directions and generates synthesized minority instances along these semantically meaningful directions. However, the scarcity of data for minority classes results in inaccurate covariance matrices. To address this issue, MetaSAug incorporates meta-learning and learns the most meaningful class-wise covariance by minimizing the loss on a balanced meta set. Consequently, MetaSAug trains models on an augmentation set enriched with semantically augmented samples, effectively enhancing their performance.

\textbf{Class prototypes.}
MPDT \cite{fu2022meta} focuses on calibrating empirical prototypes that are significantly biased due to scarce data in minority classes. MPDT first introduces meta-prototype contrastive learning to automatically learn the reliable representativeness of prototypes and predict the target distribution of balanced training data based on prototypes. Subsequently, additional features are sampled from the predicted distribution to further balance the overwhelming dominance of majority classes.


% \subsection{Classifier design}
\subsection{Training strategy}
\subsubsection{Pre-training}
SCDL-GAN \cite{cai2019supervised} constructs a two-stages framework and leverages the self-supervised learning method to represent the samples distributions. By using wasserstein auto-encoder architecture, SCDL-GAN can represent the distributive characteristics of all classes under the guideline of label information, avoiding the biased minority representation and contributing to a well performed GAN-based minority synthesis.

Besides ustilising auto-encoder to generate the feature for each sample, contrastive learning \cite{kang2021exploring} also is not affected by the number of labels and verified the powerful representation ability for imbalanced data, where the feature representations present similar linear separability w.r.t. all the classes. 

\subsubsection{Curriculum learning}
\subsubsection{Decoupling}

LDAM \cite{cao2019learning} also observe empirically that re-weighting and re-sampling are both inferior to the vanilla empirical risk minimization (ERM) algorithm (where all training examples have the same weight) before annealing the learning rate. The features produced before annealing the learning rate by re-weighting and re-sampling are worse than those produced by ERM. Thus, LDAM proposes to decouple the feature representation and classification and defers the re-balancing or re-sampling training procedure to the classification procedure. It trains the model using vanilla ERM and anneals the learning rate in the first stage. In the second stage, LDAM deploys the re-weighting or re-sampling strategy with a smaller learning rate to adjust the decision boundary and locally fine-tune the features, resulting in a better performance than one-stage training.




\subsection{Ensemble}
In recent years, ensemble of classiﬁers have arisen as a possible solution to the class imbalance problem, it attracts great interest among researchers. In this section, our aim is to review the application of ensemble learning methods to deal with the unbalanced data classification. According to the techniques they used, we can distinguish three different families among ensemble approaches for imbalanced learning. we consider boosting- and bagging-based ensembles, and the last family is formed by hybrids ensembles. That is, ensemble methods that apart from combining an ensemble learning algorithm and a preprocessing technique, make use of both boosting and bagging, one inside the other, together with a preprocessing technique.

Based on the Adaboost.M2 and the Smote algorithm, the SMOTE-Boost~\cite{ref_3} method is proposed. The SMOTE oversampling technique is introduced in each iteration. By adding synthetic minority class examples, each base classifier can pay more attention to the minority class. In the SMOTE-Boost algorithm, data balance is achieved by iterating training sets with different weights of the base classifier to improve the difference of the base classifier and the final classification accuracy. However, the algorithm adds synthetic samples by interpolation, so that the newly added synthetic samples are only distributed on the connection of the original samples, which cannot reflect the actual distribution of data very well and may be easily lead to overgeneralization. To solve this problem, the MSMOTE-Boost~\cite{ref_8} algorithm is proposed to improve SMOTE-Boost. It uses the MSMOTE algorithm to process the unbalanced data in the iteration process. According to the distance, the majority class samples are divided into three groups: safety samples, boundary samples, and potential noise samples. In MSOTE, the synthesis algorithm of security samples is the same as that of SMOTE; for boundary samples, the nearest sample is chosen; and for potential noise samples, no operation is performed.

Corresponding to the over-sampling boosting algorithm mentioned above is the boosting algorithm combined with under-sampling. The key idea of the RUS-Boost algorithm~\cite{ref_20} is to select samples randomly from the majority class samples by using the random undersampling technique (RUS) in the iteration process of the AdaBoost algorithm. Compared with the SMOTE-Boost, this algorithm has the advantages of simple implementation and short training time, but it is possible to remove most of the potential useful samples in under-sampling. To avoid this problem, the EusBoost~\cite{ref_21} algorithm is proposed. This algorithm uses the evolutionary under-sampling technique, it chooses the most representative samples in the majority class samples to get the balanced dataset, and it introduces the fitness function to ensure the difference of the base classifier.

As we know, The Bagging algorithm is simple to implement and has strong generalization ability, thus some studies have proposed using Bagging to deal with data imbalance. Bagging-based classification algorithm with data processing is simpler than Boosting-based algorithm as it does not need to calculate and update weight. In these algorithms, the over-sampling and under-sampling techniques are introduced to rebalance the data, which ensures the difference of the base classifier and the learning accuracy of the ensemble classifier.

Over-Bagging~\cite{ref_22} algorithm replaces the random sampling technique with the random oversampling technique in Bagging algorithm to deal with the imbalance of data. Data balance is achieved by over-sampling the minority class samples. However, the training set used by the base classifier will be too large as each iteration oversampling is faced with all the majority class samples, this will affect the efficiency of classification learning. The SMOTE Bagging algorithm proposed to solve this problem.

Similarly, some papers suggest that the Bagging algorithm can balance data by undersampling. Thus, the Under-Bagging algorithm is proposed. On contrary to Over Bagging, it under-sampling the majority class samples in the process of iteratively generating training sets for base classifiers. Repeated sampling with the minority class samples can also obtain a large difference in the base classifier, and compared with Over Bagging, the number of samples per iteration will be less and the efficiency will be higher. However, in the process of under-sampling, it is easy to neglect most of the useful samples, resulting in inaccurate classification results. 

Some hybrid ensemble methods have also been proposed to solve the unbalanced data classification, that is, they combine both bagging and boosting (also with a preprocessing technique). Easy Ensemble and Balance Cascade~\cite{ref_7} Use Bagging as the main ensemble learning method, but despite training a classiﬁer for each new bag, they train each bag using AdaBoost. Hence, the ﬁnal classiﬁer is an ensemble of ensembles. In the same manner as Under Bagging, each balanced bag in these two methods is constructed by randomly under sampling instances from the majority class and by the usage of all the instances from the minority class.

The number of the balanced dataset generated in Easy Ensemble algorithm is constant in the iteration process, thus it can be trained in parallel. while in Balance Cascade algorithm, most of the samples correctly classified by the previous round of base classifiers are deleted, thus the serial training of base classifiers is carried out. Because of this deletion mechanism, we can avoid ignoring most of the useful samples in random under-sampling. Besides, we can also use the advantages of the AdaBoost algorithm to effectively reduce model deviation and Bagging algorithm to effectively reduce model variance.


% \subsection{Semi/self-supervised learning methods}
