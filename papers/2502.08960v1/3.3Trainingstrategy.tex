\subsection{Training strategy}
\label{s33}

\subsubsection{Decoupling training}\
\label{s331}

The concept of decoupling training was first formally evaluated by cRT \cite{kang2019decoupling}, which separates the model learning procedure into two distinct processes: representation learning and classification. They systematically explored how imbalanced data affects each process individually and found that data imbalance might not be a significant issue in learning high-quality representations. Moreover, they demonstrated that the model could achieve robust classification performance by only retraining the classifier using class-balanced sampling. 
Similarly, LDAM \cite{cao2019learning} observes empirically that re-weighting and re-sampling are both inferior to the vanilla empirical risk minimization (ERM) algorithm (where all training examples have the same weight) before annealing the learning rate. Thus, LDAM defers the re-balancing or re-sampling training procedure to the classification stage. It initially trains the model using vanilla ERM and anneals the learning rate to obtain a better representation. Subsequently, re-weighting or re-sampling strategies are implemented with a smaller learning rate to adjust the decision boundary and fine-tune the features locally, leading to improved performance compared to one-stage training.

Following the work of cRT and LDAM, numerous methods have adopted the decoupling scheme and focused on enhancing feature representation through supervised and self-supervised learning approaches.
KCL \cite{kang2021exploring} leverages the superiority of supervised contrastive learning and further improve it by balancing the positive pairs, ensuring that there is an equal number of positive samples for each anchor sample. 
In contrast, BaCon \cite{kang2022comprehensive} studies introduce a balance term in the supervised contrastive loss to re-weight the negative pairs. During the classifier fine-tuning phase, they propose a generalized normalization classifier, which is more effective in preserving the length information of feature vectors and achieves more powerful classification ability.

Instead of utilizing the label information in representation learning, SSP \cite{yang2020rethinking} finds that self-supervised method could yields a great initialization that is more label-agnostic from the imbalanced dataset.  
rwSAM \cite{liuself} systematically compares features pre-trained by self-supervised learning and supervised learning, both theoretically and empirically and finds that self-supervised learning can learn label-irrelevant-but-transferable features that aid in classifying minority classes better than supervised pre-training. To further reduce the influence of biased data distribution on self-supervised pre-training, rwSAM adapts sharpness-aware minimization to improve model generalization and calculates loss weight using kernel density estimation, which does not require training data labels.
SCDL-GAN \cite{cai2019supervised} employs the Wasserstein auto-encoder architecture to represent sample distributions previously. Guided by label information, SCDL-GAN can represent the distributive characteristics of all classes, avoiding biased minority representation and contributing to a well-performing GAN-based minority synthesis.
SSD \cite{li2021self} proposes training an initial model under label supervision and self-supervision simultaneously using instance-balanced sampling. The pre-trained model is then used to provide informative soft labels as supervision signals during the classifier training procedure. By using self-distillation, the inter-class information is integrated into the soft labels and transferred to the minority classes.





\subsubsection{Fine-tuning pre-trained model}\
\label{s332}

In contrast to the decoupling approach, which trains feature representations and classifiers on the same dataset, fine-tuning from a well pre-trained deep model, which is also called transfer learning, has been found to yield impressive performance for many downstream tasks. This method has been proven to improve model robustness and uncertainty estimates \cite{hendrycks2019using}, while also benefiting imbalanced data representation. Specifically, the model is first pre-trained on a large, related dataset (source domain) to learn rich, unbiased feature representations. Subsequently, fine-tuning is conducted on the target dataset (target domain) to better adapt to the specific task.

FDM \cite{ouyang2016factors} initially examines the impact of imbalance in this paradigm, finding that majority classes have a more significant influence on feature learning, and achieving a more uniform distribution of sample numbers across classes is preferable. To counteract the effects of imbalance, FDM proposes a hierarchical feature learning approach that clusters objects into visually similar class groups and learns deep representations for these groups separately. This facilitates learning specific feature representations for individual classes.
Similarly, taking into account the heavy computation and irrelevant features learned from large-scale datasets, DSTL \cite{cui2018large} proposes to first measure the similarity between the target domain and source domain, selecting the most relevant samples from the source domain for pre-training the model. Subsequently, fine-tuning is performed on a more evenly-distributed subset of the target dataset, balancing the network's efforts among all categories and facilitating the transfer of learned features.
It is worth noting that the source domain and target domain do not require a strict or explicit correlation. By utilizing a large-scale dataset (i.e., ImageNet) as the source domain, IBCC \cite{singh2020imbalanced} can apply the learned basic texture knowledge to a target domain consisting of histopathological images, achieving impressive performance in handling imbalanced medical data.



\subsubsection{Curriculum learning}\
\label{s333}

Curriculum learning \cite{bengio2009curriculum} is the strategy of learning from easy to hard, which can significantly improve the generalization of the deep model. To achieve this, the curriculum schedules focus on sampling, optimization, and re-weighting strategies.
CASED \cite{jesson2017cased} introduces a curriculum sampling method for the task of extreme imbalanced lung nodule detection in chest CT scans. Initially, CASED focuses on learning how to distinguish nodules from their immediate surroundings and gradually incorporates a greater proportion of difficult-to-classify global context until it reaches uniform sampling from the empirical data distribution. The curriculum sampling strategy effectively addresses the imbalance problem but also enhances the model's robustness against nodule annotation quality issues.
CurriculumNet \cite{guo2018curriculumnet} designs the sampling curriculum by measuring the complexity of data using its distribution density in a feature space, and then ranks the complexity in an unsupervised manner. Then the model capability is improved gradually by continuously adding the data with increasing complexity into the training process.
BBN \cite{zhou2020bbn} proposes a bilateral-branch network structure and utilizes each branch to capture the imbalanced distribution and reverse distribution respectively. In the training procedure, a cumulative learning strategy is equipped to first learn the universal patterns of whole dataset and then pay attention to the minority class gradually.

Beside the sampling curriculum from imbalance to balance, DCL \cite{wang2019dynamic} further incorporates metric learning, which emphasizes learning a soft feature embedding to separate different samples in the feature space. With the expectation that the model will first learn suitable feature representations before classifying samples into correct labels, a loss scheduler is proposed to shift the learning objective from metric learning loss to classification loss.

SEDC \cite{zhao2022diagnosing} introduces a dual-curriculum learning strategy that performs feature re-weighting using adaptively updated weighting factors for each sample. By assigning different feature weight and sample weight, SEDC learns the optimal decision boundary by balancing the training contributions between majority and minority classes.



\subsubsection{Posterior re-calibration}\
\label{s334}

Posterior re-calibration refers to techniques that are designed from statistical perspectives to apply post-hoc adjustments to model outputs. The idea of calibrating model outputs is first leveraged in probability estimation where models are expected to generate reliable probability estimates of events (e.g. the probability of a sample falling into a certain class) \cite{wallace2012class}. Recently, significant strides have emerged in adapting posterior calibration techniques for discriminative tasks, specifically addressing challenges posed by imbalanced or long-tailed data. We categorize the existing model calibration methods into two main categories: Weight normalization and Logit re-balancing.

\textbf{Classifier weight normalization.} The weight normalization technique is initially introduced along with decoupling training in \cite{kang2019decoupling}. In their two-stage training approach, they observed that following joint training with instance-balanced sampling, the classifier's weight norms correlate with the cardinality of the classes. Inspired by this observation, they proposed the $\tau$-normalization technique to re-scale the weight matrix in the second traing phase or the inference phase. Following this work, DisAlign \cite{zhang2021disalign} was proposed to address the performance bottleneck in the biased decision boundary of the existing long-tail methods. It extended the second training phase by introducing additional learnable parameters to the classifier and utilizing a KL divergence-based re-weighted loss. Gaussian Clouded Logit Adjustment \cite{li2022long} discovered that the learned representations within imbalanced datasets remain insufficient, particularly noting a severe compression of embedding spaces associated with the tail classes. To this end, it provided another solution to calibrate model output which combines normalization and random perturbation.


\textbf{Logit re-balancing.} This type of methods are based on one common probabilistic depiction of data imbalance: label prior shift. It signifies a difference between the prior label distributions of the training set, denoted as $p_s(y)$, and those of the test set, $p_t(y)$. This difference exists while maintaining identical conditional probabilities of
samples given the class label, expressed as $p_s(x|y) = p_t(x|y)$. Based on this depiction, we may consider applying an additive adjustment to the logit output during inference: $f_y(x) \leftarrow f_y(x) + \log p_t(y) -\log p_s(y)$, which forms the main idea of balanced softmax \cite{hong2021disentangling, ren2020balanced}. For the setting where labels in test set are uniformly distributed, \cite{menon2021long} derived a similar logit adjustment formulation from another perspective. They draw inspiration from an alternative optimization objective, balanced error, which is the average of per-class error rates. Given that the target label distribution may also be long-tailed in practice, LADE \cite{hong2021disentangling} considered that Balanced Softmax does not involve the disentanglement of $p_s(y)$ during the training phase. It extended the original balanced softmax by utilizing regularized DV representation and importance sampling and proposed a novel loss function to further improve the performance. \cite{ye2022identifying} and \cite{tian2020posterior} extended the balanced softmax by adding a hyper-parameter as the scale of adjustment. UNO-IC \cite{tian2020posterior} further combines this parameterized calibration method with temperature scaling and the Noisy-Or model, forming an effective multi-modal fusion algorithm.

Recently, \cite{wang2024unified} conducted a systematical analysis which succeeds in explaining the role of re-weighting and logit-adjustment in a unified manner, as well as some empirical results that are out of the scope of existing theories. As no additional parameters are involved, posterior recalibration manages to provide decent performance gains at a low computational cost, especially in the setting of long-tailed classification.




