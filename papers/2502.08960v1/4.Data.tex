\section{Imbalance Across Diverse Data Formats}
\label{sec:data}
\subsection{Image data}
\quad The rapid advancement of computer vision benefits greatly from the expansion of datasets. However, keeping data balanced becomes hard as data size grows in real-world image collection efforts. Consequently, data imbalance, especially in the form of long-tailed distribution, emerges as a widespread challenge across various computer vision tasks such as image classification, object detection and segmentation, leading to a decline in model performance. In the following sections, we will outline commonly used methods to mitigate the data imbalance problem within these tasks.

\subsubsection{Image Classification}\ 

Image classification stands as one of the foundational tasks in the realm of computer vision. Many image classification datasets as well as various domain-specific collections, exhibit long-tailed distribution patterns. In this scenario, models tend to show bias towards the head classes, resulting in lower predicting accuracy in tail classes. Numerous studies have delved into resolving the issue of class imbalance in image classification, spanning across the categories we previously discussed. In terms of improving data qualities, resampling \cite{kang2019decoupling, ren2020balanced, wang2019dynamic} and data augmentation \cite{kim2020m2m, tan2021equalization, chu2020feature} are frequently employed, mainly to balance the amount of data in different classes. \cite{kang2019decoupling} evaluates the performance of various sampling strategies for long-tailed recognition under both joint and decoupled learning schemes. Dynamic Curriculum Learning (DCL) \cite{wang2019dynamic} integrates the idea of curriculum learning into a sampling scheduler to train the model dynamically from imbalanced to balanced and from easy to hard. Major-to-Minor translation (M2m)  \cite{kim2020m2m} expands the oversampling approach by translating majority samples to minority ones using perturbation-based optimization. Regarding model enhancements, a variety of techniques spanning model design \cite{wu2020solving, wu2021adversarial}, loss function \cite{huang2016learning, dong2017class, zhang2017range}, and training strategy \cite{kang2021exploring, kang2019decoupling, chu2020feature} have been proposed to improve model performance when dealing with imbalanced data. GLMC \cite{du2023global} effectively improve the generalization of the backbone for long-tailed visual recognition by combining curated self-supervised and supervised loss functions.  RoBal \cite{wu2021adversarial} succeed in tackle adversarial robustness underlong-tailed distribution by leveraging a scale-invariant classifier and data re-balancing via both margin engineering at training stage and boundary adjustment during inference.
Recently, studies like BALLAD \cite{ma2021simple} and PEL \cite{shi2023parameter} employed pretrained contrastive vision-language models such as CLIP to achieve state-of-the-art performance on imbalanced datasets.

\subsubsection{Object Detection}\ 

Object detection involves simultaneously estimating the categories and locations of object instances within a given image. The imbalance problems in object detection can be grouped into four main categories: class imbalance, scale imbalance, spatial imbalance and objective imbalance \cite{oksuz2020imbalance}. Class imbalance refers to the unequal distribution of examples among classes, which can occur both between foreground classes and between foreground and background (negative) classes. To handle this problem, sampling methods are often utilized which re-weights bounding boxes according to their types (i.e. positive or negative)~\cite{girshick2014rich, liu2016ssd}, the prediction hardness~\cite{ross2017focal, li2019gradient} or the IoU metric~\cite{cao2020prime}. In addition to direct sampling methods, some studies introduce additional model architectures to improve the detection criterion~\cite{chen2022residual, chen2019towards}, and some others address imbalance by producing artificial samples within the training dataset~\cite{wang2017fast,tripathi2019learning}.  Scale imbalance arises from the unequal distribution of object scales and input bounding box sizes. Solutions to these issues often involve utilizing models and features with a pyramidal structure. For example, Single Shot Detector (SSD)~\cite{liu2016ssd} is able to exploits features at different levels through a feed-forward pyramidal convolutional network. Its key approach to addressing scale imbalance is the use of multi-scale convolutional bounding box outputs attached to multiple feature maps at the top of the network. Feature Pyramid Networks (FPNs)~\cite{lin2017feature} offer a generic solution for constructing feature pyramids within deep convolutional networks. The construction includes a bottom-up pathway, a top-down pathway, and lateral connections. By integrating these three components, FPNs empower the network to generate feature representations that are both semantically rich and spatially detailed across multiple scales.


\subsubsection{Image Segmentation}\ 

The goal of image segmentation is to divide an image into multiple regions and map them into classes or objects. Segmentation models often suffer from data imbalance, as real-world image segmentation datasets typically exhibit long-tailed label distributions, which can lead to a bias towards the majority classes. To mitigate this issue, some studies investigate into class-balancing loss functions. Sudre et al.~\cite{sudre2017generalised} studied the use of different loss functions in deep learning frameworks for segmenting medical images, particularly when dealing with highly unbalanced data. The authors propose using the Generalized Dice overlap~\cite{crum2006generalized} as a robust loss function for such tasks. Bulo et al.~\cite{rota2017loss} introduces a novel concept called loss max-pooling for handling imbalanced training data distributions in semantic image segmentation. This approach adaptively re-weights the contributions of each pixel based on their observed losses, targeting under-performing classification results often encountered for under-represented object classes. Taghanaki et al.~\cite{taghanaki2019combo} propose a new curriculum learning based loss function named Combo loss to tackle the imbalance problem in segmenting multiple organs from medical images. The Combo loss leverage Dice similarity coefficient to deter model parameters from being held at bad local minima and gradually learn better model parameters by penalizing for false positives/negatives using a cross entropy term. Zhong et al.~\cite{zhong2023understanding} explored imbalanced semantic segmentation from the perspective of neural collapse. They introduced a regularization loss on feature centers called Center Collapse Regularizer to encourage the network to learn features closer to the appealing structure.


\subsection{Text data}
Data imbalance also causes problem for many NLP tasks~\cite{henning2022survey,zhao2024retrieval}, such as sentiment analysis, news classification, and text generation. It can cause the models trained on imbalanced data to be biased towards the majority classes and ignore the minority classes.

\subsubsection{Sentiment Analysis}\

Sentiment analysis is a task that aims to identify the emotional attitude of a text, such as positive, negative, neutral, etc. However, some classes may be more frequent than others in the data, such as positive reviews being more common than negative ones. This creates a problem of class imbalance, which can affect the performance of sentiment classifiers.
For example, the sentiment of tweets collected from Twitter on majority of keywords related to social events, people, organization is highly imbalanced. To alleviate bias, 
re-sampling techniques such as SMOTE are widely applied in \cite{ghosh2019imbalanced,flores2018evaluation,ardianto2020sentiment} to over-sample the minority class. 
% They found that the SMOTE method significantly improved the classifier performance as revealed by precision and recall values.
In addition to social psychology, there are several other factors that can greatly skew sentiment analysis results. One example comes from the work of Al-Azani et al. \cite{al2017using}, where they examined the sentiment analysis of the Syria Tweets dataset. This dataset consisted of 1350 negative and 448 positive tweets, as reported by Mohammad et al. \cite{mohammad2016translation}. It is important to note that this dataset was highly imbalanced due to the ongoing regional instability in Syria.
Compared with static sentiment analysis, real-time sentiment analysis is more challenging due to limited labeled data and sudden sentiment changes caused by real-world events (concept shift). To overcome this, Guerra et al. \cite{guerra2014sentiment} acquire labels and handle concept drift using insights from self-report imbalances. That is, a preference for reporting positive feelings over negative feelings and a preference for reporting extreme feelings over average feelings, which helps generate labeled data on polarizing topics and informs a spike-based feature representation strategy. 
% Their work achieves accuracy of up to 84\% in analyzing live reactions on Twitter, without manual labeling. 
Apart from sentiment analysis of social network, another example can be product reviews. In product reviews, satisfied customers may be less likely to leave a review, while dissatisfied customers are more likely to voice their opinions, leading to a higher number of negative reviews. For instance, Obiedat et al. \cite{obiedat2022sentiment} proposed PSO-SVM with particle swarm optimization (PSO) to find the best feature weights and the best $k$ value for each oversampling technique, and then applied SVM to classify the oversampled and weighted reviews.

\subsubsection{News Classification}\ 

News Classification involves assigning a topic or category to a news article, such as politics, sports, entertainment, etc. The class imbalance phenomenon is the situation where the number of news articles that belong to different categories is not equal. 
Some topics may be more popular or frequent than others, such as sports news being more common than science news. For example, Dogra et al. \cite{dogra2022comparative} aim to extract the banking news and its most correlated news articles from the pool of financial news articles, and divide financial news articles into four classes: banking, non-banking, governmental, and global. The distribution of news articles amongst four classes are 9.16\%, 70.54\%, 4.21\% and 16.09\% respectively, due to the different frequency and level of attention of news events. 
Sometimes, class imbalance phenomenon happens because the class of interest is quite rare than other classes, especially in binary classification problem. For instance, Rivera et al. \cite{rivera2020news} need to classify the news from a local newspaper based on whether they are about a traffic incident or not. However, the class of interest, which is ‘traffic accident’, only represents 10.30\% of the instances, while the rest are other types of news. 
Another binary classification example can be fake news classification, in this scenario, the class imbalance refers to the number of fake news data is much smaller than the number of true news data. This happens because manual labeling of fake news data is time-consuming and difficult, while true news data is more abundant and accessible. For example, the BanFakeNews dataset \cite{hossain2020banfakenews} contains about 50,000 Bengali news articles, of which only 1,299 are labeled as fake and the rest are labeled as true. Hence, Keya et al. \cite{keya2022augfake} proposed AugFake-BERT, which uses the pre-trained BERT model to insert and substitute words for data augmentation, and train a smaller BERT model from scratch with the augmented dataset for embedding and classification. For multi-class classification problem, the long-tailed phenomenon is also evident, for example, Hasib et al. \cite{hasib2023strategies} study Bangla news classification with Bangla Newspaper Dataset \cite{zabir_al_nazi_2020}. The sample of 437,948 news articles are organized into high imbalanced 32 categories, where the 'bangladesh' category has 232,504 amounts of article and the percentage is 53.09\% which is the highest and the lowest one are 'askeditor' and 'bs-events' categories which had only one article respectively.


% \subsection{Language modeling}

\subsubsection{Text Generation}\ 


In addition to the aforementioned categories, the growing prevalence of large language models (LLMs) has brought the issue of data imbalance in text generation tasks to the forefront. To address this challenge, researchers have proposed several strategies. One prominent approach involves adjusting the composition of pre-training datasets to balance the proportions of data across tasks, languages, or domains. This method enhances the model's learning efficiency, ensures a comprehensive representation of language features, and aligns the model's capabilities with the specific requirements of the target tasks, thereby achieving balanced performance across diverse scenarios. Raffel et al.~\cite{raffel2020exploring} demonstrated the effectiveness of two such strategies: mixing proportionally to dataset size and temperature-scaled mixing. In the former, datasets are weighted based on their sample sizes, while in the latter, a temperature parameter $T$ is introduced to adjust task-level data distribution. This parameter increases the relative weight of smaller tasks, offering a more balanced mixture. Another promising strategy is leveraging fine-tuned models with limited samples as proxies for data selection. For example, Li et al.~\cite{li2023from} proposed the ``cherry model,'' which operates through three fine-tuning stages: learning from limited experience, evaluation, and retraining based on self-guided insights. This model autonomously identifies and selects high-quality ``cherry samples'' from open-source datasets. Remarkably, on the Alpaca dataset, this method achieved superior performance using only approximately 5\% of the data compared to models trained on the entire dataset. Custom dataset construction and task-specific fine-tuning also serve as effective methods to mitigate data imbalance. For instance, Li et al.~\cite{li2024numinamath} segmented raw data into question-answer pairs, translated and realigned them, and incorporated chain-of-thought (CoT) reasoning and tool-integrated reasoning (TIR) annotations. The resulting mathematical problem dataset underwent two stages of fine-tuning: one on diverse CoT-annotated datasets and another on synthetic TIR datasets, leading to significant improvements in mathematical reasoning tasks. Furthermore, iterative trial-and-error processes have proven effective in addressing data imbalance. An et al.~\cite{an2023learning} utilized multiple LLMs to collect inaccurate reasoning paths, which were subsequently corrected by GPT-4. This Correction-Centric Evolution strategy facilitated fine-tuning, enabling consistent performance improvements across various LLMs and tasks.

% \subsubsection{Relation Extraction} Relation extraction involves identifying and classifying relationships between entities mentioned in text. It aims to extract structured information from unstructured text by determining the type of relationship between entities, such as the interaction between a person and an organization mentioned in a news article. The data imbalance phenomenon of relation extraction is also widely reported in \cite{ye2019exploiting,song2021classifier,mitra2020multi,zhang2019long,yuan2017one,xie2021revisiting,nguyen2014robust}. For example, relation extraction with distant supervision is widely adopted, which automatically generates relation labels by aligning the text with the existing knowledge base. 

% \begin{comment}
% \subsection{Topic Classification}
% \subsubsection{Neural Machine Translation}
% \subsubsection{Event type detection} This task involves determining the type of an event in a text, such as traffic accident, terrorist attack, natural disaster, etc. Some events may be more frequent or salient than others, such as social events being more common than military events.
% \end{comment}

% \subsection{Time series data}

\subsection{Graph data}
Recently, graph data has gained significant attention for its ability to represent intricate structural relationships among diverse entities, facilitating applications across fields such as chemical analysis, social network modeling, and recommendation systems. The extensive use of graph structures has driven considerable advancements in graph analytics, typically categorized into node-level, edge-level, and graph-level tasks based on the analytical objectives. Given these inherent objectives within graph structures, imbalance issues are prevalent across all analytical levels, manifesting in various forms. In the following, we will formulate these distinct imbalance challenges in alignment with each graph objective.

\subsubsection{Node-level imbalance}\ 

\textbf{Node class imbalance.} Node class imbalance refers to the uneven distribution of class labels among nodes within a graph, where certain classes contain fewer labels compared to others. More importantly, the non-IID nature of graph data causes nodes to influence their neighbors through the message-passing paradigm in graph neural networks, further exacerbating the under-representation of minority classes. This interconnectedness intensifies the challenges posed by imbalanced data, as classes with fewer labels struggle to propagate meaningful signals across the graph, potentially diminishing model performance on these underrepresented classes.

To supplement the limited labels of minority classes, data generation has emerged as a widely adopted approach. In linear generative methods, for example, GraphSMOTE applies the SMOTE technique within the graph domain to synthesize new nodes for minority classes and establish edges among them. Other methods, such as GraphMixup, GraphENS, SNS, and GraphSHA, employ mixup techniques to create more challenging samples by interpolating nodes from different classes, thereby enhancing the generalization capacity of models. In contrast, deep generative methods such as ImGAGN leverage GAN architectures, where a generator and discriminator work simultaneously to perform data generation and classification. Additionally, methods like GraphSR and BIM enhance minority class representation through self-training, expanding the labeled dataset by annotating unlabeled nodes within the original graph. This approach helps mitigate noise introduced by generative models, leading to improved and more robust performance.

\textbf{Node degree imbalance.}
Node degrees in graphs often follow a long-tail distribution, i.e., a significant fraction of nodes are tail nodes with
a small degree. For instance, in social networks, a small number of users may have thousands of connections, while the majority maintain only a few. Similarly, in e-commerce recommendation systems, popular items frequently appear in user histories, whereas niche items receive minimal attention. This disparity, referred to as node degree imbalance, creates challenges in effectively exploring the information from low-degree tail nodes. Specifically, head nodes with high degrees benefit from rich structural information and neighbors, allowing them to achieve better representations and stronger performance in downstream tasks. In contrast, "tail" nodes with low degrees have limited topological information, which constrains their effectiveness.

To mitigate the information scarcity in tail nodes, researchers have developed methods to transfer structural patterns from head nodes to tail nodes. LTE4G trains multiple expert models to handle different types of nodes, using knowledge distillation to transfer knowledge from head-node experts to those focused on tail nodes. Similarly, Cold Brew leverages a strong teacher model with node-wise structural embeddings to guide a student model, enabling it to uncover missing latent neighborhoods for tail nodes. Tail-GNN models variable connections between a target node and its neighbors and learns the neighborhood translation from head nodes to enhance the structurally limited tail nodes. SAILOR proposed a structural augmentation method to enhance node homophily by adding pseudo-homophilic edges to tail nodes, allowing them to acquire more task-relevant information from neighboring nodes.

\textbf{Node topology imbalance.}
Distinct from other imbalance issues that focus on disparities in node or degree quantity, node topology imbalance highlights the positional distribution of labeled nodes within the graph. This problem is rooted in the homophily assumption that influence boundaries created through label propagation should ideally align with true class boundaries. However, the actual placement of labeled nodes often shifts these influence boundaries away from true boundaries, leading to skewed model decision boundaries and degraded performance.

To address topology imbalance, ReNode introduces a metric that quantifies this imbalance by measuring influence conflict. It then reweights labeled nodes based on their proximity to class boundaries, thereby improving the performance for nodes located near boundaries. Alternatively, PASTEL proposes an anchor-based position encoding mechanism that directly optimizes information propagation paths, enhancing the connectivity among nodes within the same class to strengthen the flow of supervisory information.

\subsubsection{Edge-level imbalance}\ 

Besides the node-level imbalances, real-world graphs also exhibit imbalanced edge distributions, which significantly impact various downstream tasks, including inductive recommendation and anomalous transaction detection on financial networks. Specifically, new users or items are continuously added to the system as novel inductive nodes by linking with the existing graph. However, these novel nodes, situated within a few-shot context, typically possess limited edges due to scarce information, posing challenges for effective node representation and accurate link prediction. To address this issue, a recent study \cite{zhu2023few} employs episodic learning to explore transferable knowledge and augment the representation of nodes with sparse connections. Additionally, imbalanced edge distribution prominently affects edge anomaly detection tasks, where anomalies such as abnormal transactions are modeled as minorities within the graph. This challenge is generally addressed by characterizing the normal edge distributions \cite{zhu2023few} with auxiliary information \cite{liu2022deep} and identifying deviations from these norms as indicators of anomalies \cite{li2021live}.


\subsubsection{Graph-level imbalance}\ 

Graph-level datasets, frequently employed in modeling molecular or drug graphs, inherently exhibit an imbalanced distribution. This imbalance is primarily observed in datasets featuring naturally occurring distributions, such as those associated with rare diseases or specific chemical compounds. This scenario poses significant challenges for machine learning models, which may struggle to accurately predict properties or classifications for these infrequent but critical instances. To address these challenges, recent research \cite{wang2022imbalanced} constructs a global graph structure that aggregates information among similar samples to enhance representation. This technique effectively boosts the predictive capabilities of models dealing with rare or infrequent graph types by pooling collective information from analogous instances. Additionally, another study \cite{liu2023semi} focuses on regression tasks and employs additional real, yet unlabeled data to compensate for minorities. By adopting an iterative training process, this model gradually annotates the unlabeled data and includes these data for model training, thereby enhancing model performance systematically over time.