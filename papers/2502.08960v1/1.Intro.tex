\section{Introduction}
\label{sec:intro}

The advent of expansive data availability has propelled machine learning (ML) to the forefront of technological advancements in both academia and industry. These ML models are intricately designed to fit specific data distributions and are subsequently employed in a variety of downstream tasks, ranging from predictive analytics to automated decision-making systems. Therefore, the performance of ML models is profoundly influenced by the quality and distribution of the data used during training. Data that is representative, diverse, and meticulously preprocessed ensures that models are not only accurate but also robust and generalizable across different settings and challenges.

However, natural data distributions are inherently complex and frequently flawed. Among the myriad challenges they present, the issue of imbalanced data distribution is particularly prominent, reflecting the widespread and natural disparities encountered across various fields. In finance, for example, instances of fraud are relatively rare compared to legitimate transactions, making it difficult for models to learn to detect such anomalies accurately. In the domain of healthcare, rare diseases may be underrepresented in medical datasets, posing significant challenges in developing robust diagnostic models. In industrial settings, quality control systems often need to identify infrequent product defects, which can be obscured by the overwhelming majority of conforming products. These scenarios not only complicate ML model training but also place significant demands on the robustness of the systems.

Generally, imbalanced data distributions significantly impact the performance and utility of ML models. These models typically perform well on high-resource groups that have ample data but under-perform on low-resource groups where data is scarce, resulting in unclear distribution boundaries. Consequently, although ML models may exhibit satisfactory overall performance, their effectiveness is notably reduced within these low-resource groups. However, these low-resource groups often hold greater significance in real-world scenarios. For example, in medical diagnostics, the failure to detect a rare disease due to insufficient data can lead to missed diagnoses and inadequate patient care. Similarly, in financial systems, the inability to identify infrequent instances of fraud can result in significant financial losses and compromised security. This tendency of ML models to overlook rare but critical instances diminishes the utility and safety of automated decision-making systems in practical applications.

In response to these challenges, the ML community has devised a range of methods and we organize them into four fundamental categories—data re-balancing, feature representation, training strategy, and ensemble learning—each corresponding to crucial aspects of the ML process.
Data re-balancing techniques are crucial for adjusting the data distribution to better representation, utilizing approaches such as over-sampling the minority class and under-sampling the majority class. This adjustment is critical to prevent the model from disproportionately favoring the majority samples, aligning with the data preparation phase of ML.
Feature representation strategies enhance the ability to accurately capture and represent the information relevant to minority samples. This improvement is pivotal in the feature engineering phase, enabling models to effectively learn from and make predictions for all samples.
Advanced training strategies adjust the learning algorithms to minimize their inherent bias towards the majority samples. This key adjustment during the training phase ensures that the learning process is inclusive, giving equitable consideration to all samples.
Lastly, ensemble methods, which involve combining several models, correspond to the model integration part of the ML process. These methods leverage the strengths of multiple algorithms to potentially reduce biases that may arise from imbalanced data, thereby enhancing the robustness and accuracy of the final model outputs.
By categorizing methods according to the foundational processes of ML, this taxonomy not only facilitates comprehensive field surveys but also clarifies the motivations behind these strategies, aiding in the pursuit of specific objectives for specific method designation.
Additionally, this survey explores the manifestation of imbalances across various data formats, including images, text, and graphs, highlighting the differences, unique challenges and required adaptations for each format. This exploration is critically important as it deepens the understanding of each data format and facilities developing targeted ML strategies for complex data format scenarios.


The contributions of this survey are summarized as follows:
\begin{itemize}
\item We provide a comprehensive survey of the literature addressing imbalanced data learning, offering a structured overview of methods that are rooted in the foundational processes of ML.
\item We conduct a thorough analysis of imbalances across various data formats, including images, text, and graphs, providing an in-depth exploration of the challenges and methodologies specific to each format.
\item We highlight resources available for tackling imbalanced data and explore current challenges and future directions. This discussion is designed to guide researchers who are struggled with imbalance issues, aiding them in developing strategies effectively and efficiently.
\end{itemize}

The structure of this survey is outlined as follows: Section II provides a thorough investigation of methodologies for handling imbalances, organized according to our taxonomy. Section III extensively discusses the manifestation of imbalances across various data formats. 
Section IV offers a detailed investigation of evaluation metrics specific to imbalanced data methods. 
Section V is dedicated to summarizing the resources available for learning from imbalanced data. Finally, Section VI presents the challenges and future directions in this field.





















% Imbalanced datasets, that is, the number of samples of one class in the data set is much larger than that of other classes. However, such a minority class is usually the one we are interested in, and its information may be underrepresented due to the imbalance. In this way, classifiers can have good accuracy on the majority class but very poor accuracy on the minority classes.

% As we know, classification is one of the important research contents in the field of data mining. Traditional classification methods have achieved good performance in balanced dataset classification. However, the actual data sets are often unbalanced, For the traditional classifier which aims at improving the overall classification accuracy, this imbalance will inevitably lead to the classifier paying too much attention to the majority class samples, thus reducing the classification performance of minority class samples. In practical applications, such as text categorization, fault detection, fraud detection, and oil-spills detection, people may be more concerned about the few classes in the data set, and the cost of misclassification of these minority classes is usually greater than that of most classes.

% Unlike the classification of balanced data, the classification of unbalanced data is relatively difficult to solve. On the one hand, the classical classification accuracy evaluation criteria are not suitable for unbalanced data. The proportion of instance plays an important role in the classification, when learning unbalanced data, the impact of the minority class samples on classification accuracy may be much less than that of the majority class samples. Accuracy-driven classification method usually results in a low recognition rate of the majority class samples, as such classifiers tend to predict a sample as a majority class. Thus, aiming at class accuracy for unbalanced data is inappropriate, and more reasonable evaluation criteria need to be introduced. On the other hand, as the information contained in the minority class samples will be very limited, it is difficult to determine the distribution of a few classes of data, in this way, it is difficult to find rules within them, which results in a low accuracy rate of these data.

% Lots of methods for solving class imbalance problems have been proposed to solve this problem. These methods can be categorized into five groups, which depend on how they deal with class imbalance. Probably, the most common methods are sampling, which includes oversampling and under-sampling. These two methods rebalance the datasets through sampling techniques. Besides, class-imbalance is closely related to cost-sensitive learning, which deals with class imbalance by incurring different costs for the two classes. Furthermore, if the class imbalance is serious, we can treat this problem as an anomaly detection problem. In addition to these approaches, another group of techniques emerges when the use of ensembles of classiﬁers is considered. The next sections focus on these methods in detail.

% Contributions of each work described falls under one or more aforementioned categories.