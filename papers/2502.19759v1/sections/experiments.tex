\begin{table*}[t]
\small
\centering
\renewcommand{\arraystretch}{1.2} 
\setlength{\tabcolsep}{6pt}
\begin{tabular}{l c c c c c c}
\toprule
\multirow{2}{*}{\textbf{Model}} & \multirow{2}{*}{\textbf{LLM FT}} & \multirow{2}{*}{\textbf{Modality}} & \multicolumn{3}{c}{\textbf{GPT Score}} & \multirow{2}{*}{\textbf{WER}} \\ 
\cmidrule(lr){4-6}
  & & & \textbf{User} & \textbf{System} & \textbf{Overall} & \\ 
\midrule
\rowcolor{gray!10}  &   & $\mathcal{S\rightarrow T,\colorbox{yellow!40}{\hspace{-2pt}$\mathcal{S}$\hspace{-2pt}}}$ & $1.94 \scalebox{0.7}{$\pm 0.07$}$   & $2.76 \scalebox{0.7}{$\pm 0.08$}$  & $2.35 \scalebox{0.7}{$\pm 0.05$}$  & 8.36\% \\ 
\rowcolor{gray!10} \multirow{-2}{*}{GLM-4-Voice \cite{glm2024chatglm}}  & \multirow{-2}{*}{\cmark} & $\mathcal{S\rightarrow \colorbox{yellow!40}{\hspace{-2pt}$\mathcal{T}$\hspace{-2pt}},S}$ & $2.04 \scalebox{0.7}{$\pm 0.07$}$   & $2.97 \scalebox{0.7}{$\pm 0.08$}$  & $2.50 \scalebox{0.7}{$\pm 0.06$}$  & $-$ \\ 
glm-4-9b-chat \cite{zeng2024glm4}  &   $-$    & $\mathcal{T\rightarrow \colorbox{yellow!40}{\hspace{-2pt}$\mathcal{T}$\hspace{-2pt}}}$ & $4.30 \scalebox{0.7}{$\pm 0.05$}$   & $3.90 \scalebox{0.7}{$\pm 0.06$}$  & $4.10 \scalebox{0.7}{$\pm 0.04$}$  & $-$ \\ 
\midrule
\rowcolor{gray!10}         &   & $\mathcal{S\rightarrow T,\colorbox{yellow!40}{\hspace{-2pt}$\mathcal{S}$\hspace{-2pt}}}$ & $2.51 \scalebox{0.7}{$\pm 0.09$}$   & $3.16 \scalebox{0.7}{$\pm 0.09$}$  & $2.83 \scalebox{0.7}{$\pm 0.06$}$  & 5.90\% \\ 
\rowcolor{gray!10}   \multirow{-2}{*}{Lyra \cite{zhong2024lyra}}        & \multirow{-2}{*}{\cmark}  & $\mathcal{S\rightarrow \colorbox{yellow!40}{\hspace{-2pt}$\mathcal{T}$\hspace{-2pt}},S}$ & $2.67 \scalebox{0.7}{$\pm 0.09$}$   & $3.38 \scalebox{0.7}{$\pm 0.09$}$  & $3.03 \scalebox{0.7}{$\pm 0.07$}$  & $-$ \\ 
Qwen2-VL-7B-Instruct \cite{Qwen2-VL}  &    $-$   & $\mathcal{T\rightarrow \colorbox{yellow!40}{\hspace{-2pt}$\mathcal{T}$\hspace{-2pt}}}$ & $3.80 \scalebox{0.7}{$\pm 0.08$}$   & $3.88 \scalebox{0.7}{$\pm 0.08$}$  & $3.84 \scalebox{0.7}{$\pm 0.06$}$  & $-$ \\ 
\midrule
\rowcolor{gray!10} &  & $\mathcal{S\rightarrow T,\colorbox{yellow!40}{\hspace{-2pt}$\mathcal{S}$\hspace{-2pt}}}$ & $1.73 \scalebox{0.7}{$\pm 0.06$}$   & $2.28 \scalebox{0.7}{$\pm 0.07$}$  & $2.00 \scalebox{0.7}{$\pm 0.05$}$  & 12.36\% \\ 
\rowcolor{gray!10}  \multirow{-2}{*}{Freeze-Omni \cite{xiong2024freeze}} & \multirow{-2}{*}{\xmark}   & $\mathcal{S\rightarrow \colorbox{yellow!40}{\hspace{-2pt}$\mathcal{T}$\hspace{-2pt}},S}$ & $2.09 \scalebox{0.7}{$\pm 0.07$}$   & $3.06 \scalebox{0.7}{$\pm 0.08$}$  & $2.57 \scalebox{0.7}{$\pm 0.06$}$  & $-$ \\ 
Qwen2-7B-Instruct \cite{qwen2}  &   $-$    & $\mathcal{T\rightarrow \colorbox{yellow!40}{\hspace{-2pt}$\mathcal{T}$\hspace{-2pt}}}$ & $4.26 \scalebox{0.7}{$\pm 0.06$}$   & $3.81 \scalebox{0.7}{$\pm 0.07$}$  & $4.03 \scalebox{0.7}{$\pm 0.05$}$  & $-$ \\ 
\midrule
\rowcolor{gray!10} &  & $\mathcal{S\rightarrow T,\colorbox{yellow!40}{\hspace{-2pt}$\mathcal{S}$\hspace{-2pt}}}$ & $2.44 \scalebox{0.7}{$\pm 0.09$}$   & $2.84 \scalebox{0.7}{$\pm 0.09$}$  & $2.64 \scalebox{0.7}{$\pm 0.06$}$  & 24.90\% \\ 
\rowcolor{gray!10}    \multirow{-2}{*}{MiniCPM-o \cite{yao2024minicpm}}   & \multirow{-2}{*}{\cmark}   & $\mathcal{S\rightarrow \colorbox{yellow!40}{\hspace{-2pt}$\mathcal{T}$\hspace{-2pt}},S}$ & $3.22 \scalebox{0.7}{$\pm 0.09$}$   & $3.93 \scalebox{0.7}{$\pm 0.08$}$  & $3.58 \scalebox{0.7}{$\pm 0.06$}$  & $-$ \\ 
Qwen2.5-7B-Instruct \cite{qwen2.5}  &  $-$     & $\mathcal{T\rightarrow \colorbox{yellow!40}{\hspace{-2pt}$\mathcal{T}$\hspace{-2pt}}}$ & $4.28 \scalebox{0.7}{$\pm 0.05$}$   & $3.84 \scalebox{0.7}{$\pm 0.06$}$  & $4.06 \scalebox{0.7}{$\pm 0.04$}$  & $-$ \\ 
\bottomrule
\end{tabular}
\vskip -0.05in
\caption{Evaluation results for voice interaction models, including the instruct fine-tuned version of each model's backbone LLM. $\mathcal{S}$ and $\mathcal{T}$ represent speech and text, respectively. ``Modality'' indicates input → output data type. ``LLM FT'' shows whether the backbone LLM was fine-tuned during training. ``User'' and ``System'' represent scores for responses to past user and model utterances, respectively. ``Overall'' denotes the score across all responses. ``WER'' refers to the word error rate between the model’s intermediate text response and the transcribed spoken response, highlighting degradation from speech synthesis. GPT Scores are reported with a 95\% confidence interval.}
\label{tab:recall}
\vskip -0.2in
\end{table*}


\section{Experiments}
\label{sec:experiments}

\begin{figure*}
    \centering
    \includegraphics[width=0.95\linewidth]{figures/Figure7.pdf}    
    \vskip -0.05in
    \caption{Attention maps for ground truth answers given each model's past dialog and question. In each subfigure, the left side represents questions about past user utterances, and the right side represents questions about past model utterances. Red boxes indicate the positions of supporting utterances.}
    \label{fig_attn_map}
    \vskip -0.2in
\end{figure*}

In this section, we present the results of two experiments and analyses using ContextDialog. In Section \ref{subsec:recall}, we demonstrate that open-source voice interaction models struggle to recall past information on their own, particularly user-specific information that exists solely in spoken form. Then, in Section \ref{subsec:retrieval}, we show that even when leveraging a more advanced dedicated text retriever, models fail to respond robustly given the retrieved information, yielding limited improvements in spoken response generation. These two analyses highlight a critical yet often overlooked aspect of voice interaction models, their ability to remember past interactions, which is essential for real-world deployment.

For our experiments, we select four open-source multi-turn voice interaction models: GLM-4-Voice \cite{zeng2024glm4}, MiniCPM-o 2.6 \cite{yao2024minicpm}, Freeze-Omni \cite{xiong2024freeze}, and Lyra \cite{zhong2024lyra}. To support real-time generation and minimize latency, these models generate responses directly from the input speech without an intermediate speech-to-text conversion. To mitigate semantic degradation in speech-only generation, these models generate text responses alongside spoken responses: GLM-4-Voice employs an interleaved token generation approach, alternating between text and speech tokens (Figure \ref{fig_decoder}(a)), while MiniCPM-o, Freeze-Omni, and Lyra generate text responses while simultaneously synthesizing speech using real-time generated text tokens and the LLM’s hidden states (Figure \ref{fig_decoder}(b)). 

In all experiments, we evaluate each model's spoken response using the LLM-as-a-judge approach \cite{NEURIPS2023_91f18a12}, following previous voice interaction models \cite{chen2024slamomnitimbrecontrollablevoiceinteraction,fang2025llamaomni, zeng2025scaling}. We employ \textit{gpt-4o-mini} for evaluation, referred to as the GPT Score in this paper, using a five-point scale, and design prompts to assess recall by measuring how well the generated responses contain the ground truth information relevant to the given question, as detailed in Appendix \ref{subapp:evaluation}. Since \textit{gpt-4o-mini} is tailored to text inputs, we first convert the spoken responses into text using \textit{whisper-large-v3} \cite{pmlr-v202-radford23a} ($\mathcal{S\rightarrow T,\colorbox{yellow!40}{\hspace{-2pt}$\mathcal{S}$\hspace{-2pt}}}$).

Additionally, considering that each model also generates an intermediate text response corresponding to the spoken output, we also evaluate it ($\mathcal{S\rightarrow}\colorbox{yellow!40}{\hspace{-2pt}$\mathcal{T}$\hspace{-2pt}}\mathcal{,S}$). By analyzing the evaluation results along with the word error rate (WER) between the text response and the transcribed spoken response, we can disentangle recall ability from speech synthesis capability, allowing us to identify cases where the model successfully recalls information but fails in speech synthesis.

We use the official implementations, hyperparameters, and checkpoints for all four models (Section \ref{subapp:licenses}), running experiments on a single NVIDIA A40 GPU. Detailed model descriptions and additional analyses are provided in Appendix \ref{subapp:models}.

\subsection{Does Your Model Truly Recall Past Information?}
\label{subsec:recall}

Using ContextDialog, we examine whether these models can recall or remind users of past utterances, either from the user or the model itself. To assess differences in question difficulty, we additionally evaluate the chat versions of each model’s backbone LLM \cite{glm2024chatglm, qwen2, qwen2.5, Qwen2-VL}, providing a basis for comparing the difficulty of questions based on user-spoken versus model-generated utterances. 

The scores for each model on questions about past user utterances and the model’s own responses, along with their 95\% confidence intervals and averages, are presented in Table \ref{tab:recall}. In this table, we observe two key patterns. First, in multi-turn dialogs requiring past context, all voice interaction models (shaded in gray) show significant performance drop compared to text-based counterparts (unshaded), regardless of whether evaluation is on the intermediate text response ($\mathcal{S\rightarrow}\colorbox{yellow!40}{\hspace{-2pt}$\mathcal{T}$\hspace{-2pt}}\mathcal{,S}$) or the transcribed response ($\mathcal{S\rightarrow T,\colorbox{yellow!40}{\hspace{-2pt}$\mathcal{S}$\hspace{-2pt}}}$). This degradation is particularly pronounced in Freeze-Omni, where the LLM is frozen during speech model training (LLM FT: \xmark). These results indicate that expanding a pre-trained LLM to speech significantly weakens its ability to process long contexts.

Secondly, unlike their text-based counterparts (unshaded), voice interaction models (shaded in gray) perform consistently better when recalling their own past utterances than the user’s ($p<0.01$). This stems from the generation mechanism of recent voice interaction models. Since speech-only output degrades semantic modeling, most modern models generate text alongside speech—either simultaneously or alternately—to leverage the backbone LLM’s text capability. Consequently, when responding to questions about the model’s past utterances, both text and speech are utilized (Figure \ref{fig_recall}(b)), whereas for user utterances, the model must rely solely on speech (Figure \ref{fig_recall}(a)).

% each turn includes user speech and the model’s speech and text outputs, making it easier to use text-based capabilities for responses (Figure \ref{fig_recall}(b)) than for user-spoken content (Figure \ref{fig_recall}(a)).

We further analyze how models respond to questions about past user and model utterances by examining their attention maps during response generation as shown in Figure \ref{fig_attn_map}. The horizontal axis represents the turn index (``U'' for user, ``M'' for model), and the vertical axis represents the attention layer index. We sum attention weights over all tokens in each utterance. As shown, models attend less to supporting utterances when answering questions about past user utterances than model utterances. This suggests that an inherent bias, where models allocate less attention to user-spoken content, contributes to the recall gap and highlights the need for improved modeling capabilities.

The findings in this section reveal modality-specific differences, both compared to text interaction models and within speech models. They underscore the need to improve voice interaction models by introducing training and generation methods to better utilize long-range conversational history. In the following section, we validate a practical approach to enhancing past information utilization with minimal computational cost when the model fails to recall relevant details. Specifically, we examine the effectiveness of retrieval-augmented generation (RAG) in voice interaction models.
 
\subsection{Does Your Model Reliably Augment Retrieved Information into Generation?}
\label{subsec:retrieval}

\begin{figure*}
    \centering
    \includegraphics[width=0.98\linewidth]{figures/Figure3.pdf}    
    \vskip -0.05in
    \caption{The results of applying a RAG method to each model are shown. The red dashed line indicates the results generated without RAG (Section \ref{subsec:recall}). The evaluation is based on the transcribed spoken response $\mathcal{S\rightarrow T,\colorbox{yellow!40}{\hspace{-2pt}$\mathcal{S}$\hspace{-2pt}}}$.}
    \label{fig_rag_results}
    \vskip -0.2in
\end{figure*} 

Leveraging RAG methods from the NLP domain \cite{JMLR:v24:23-0037, NEURIPS2020_6b493230}, we assess whether voice interaction models can effectively utilize past utterances when retrieved by a dedicated module, as illustrated in Figure \ref{fig_recall}(c). Given our observation in Section \ref{subsec:recall} that models struggle more with speech than text, we transcribe past user and model utterances at the end of each speech segment using a separate ASR model. These transcriptions are stored with their corresponding text embeddings, extracted via a separate retriever.

Once stored, these transcriptions serve as passages from which relevant information can be retrieved when a user query arrives. Upon receiving input speech, we convert it into text using the same ASR model, extract its embedding, and retrieve the top-\textit{k} most relevant past utterances by comparing cosine similarity. These retrieved utterances are then augmented into spoken response generation. We use \textit{whisper-large-v3-turbo} \cite{pmlr-v202-radford23a} for the ASR model and \textit{e5-large-v2} \cite{Wang2022TextEB}, a widely used retrieval model in NLP.

The retrieved texts are incorporated into the generation stage using the following format:  
\texttt{Based on your/my statement: ``RETRIEVED TRANSCRIBED TEXT1'', your/my statement: ``RETRIEVED TRANSCRIBED TEXT2'' ...}. The choice between \texttt{your} and \texttt{my} depends on the speaker of the retrieved utterance, ensuring clear integration into the prompt. The model then utilizes this prompt to generate spoken responses, as shown in Figure \ref{fig_recall}(c). Details on how each model incorporates this prompt into spoken response generation are in Appendix \ref{subsubapp:models_for_analysis}, while experiments with various other prompts are discussed in Appendix \ref{subsubapp:retrieval_prompt}.

The experimental results on integrating RAG into voice interaction models are presented in Figure \ref{fig_rag_results}, where (a)–(d) correspond to the four evaluated models. The red dashed line indicates baseline performance when models generate responses based solely on intrinsic recall without RAG (Section \ref{subsec:recall}). These results are measured using the ASR transcript of the spoken response ($\mathcal{S\rightarrow T,\colorbox{yellow!40}{\hspace{-2pt}$\mathcal{S}$\hspace{-2pt}}}$) for all QA pairs, and trends in the intermediate text response are similar, as detailed in Appendix \ref{subsubapp:additional_results}.

As shown in the figure, all models perform similarly or worse with RAG, showing little to no improvement as the number of retrieved utterances increases. We attribute this to two main factors. First, while RAG increases the chances of retrieving and using supporting utterances, retrieval failures introduce irrelevant sentences that add noise and disrupt generation. Second, unlike text-based models, voice interaction models are generally trained to avoid long responses, as users do not expect lengthy monologs. However, RAG adds prompts to the generation process, leading to longer responses that contradict the models' training tendencies.


\subsubsection{Analyses}
We observe that incorporating utterances retrieved by a dedicated retrieval module \cite{Wang2022TextEB} into spoken response generation has little effect on voice interaction models. To further investigate this phenomenon, we conduct various experiments.

To determine whether prompting itself is ineffective for voice interaction models, we conduct two experiments: (1) providing the supporting utterance from the ContextDialog QA pair as a prompt instead of retrieved utterances and (2) using an unrelated utterance as a prompt to generate the spoken response. As in previous evaluations, we assess the spoken response based on its transcribed text ($\mathcal{S\rightarrow T,\colorbox{yellow!40}{\hspace{-2pt}$\mathcal{S}$\hspace{-2pt}}}$), with results shown in Table \ref{tab:rag_support}.

\begin{table}[t]
\small
\centering
\begin{tabular}{l|c|c}
\toprule
\textbf{Model}               & \textbf{Prompt} & \textbf{GPT Score}                                \\ \midrule
\multirow{3}{*}{GLM-4-Voice} &  \xmark             & $2.35\pm 0.05$ \\ \cmidrule{2-3} 
                             & Supporting      & $\mathbf{2.60\pm 0.05}$ \\ \cmidrule{2-3} 
                             & Irrelevant      & $1.87\pm 0.05$ \\ \midrule
\multirow{3}{*}{Lyra}        & \xmark               & $2.83\pm 0.06$ \\ \cmidrule{2-3} 
                             & Supporting      & $\mathbf{3.44\pm 0.05}$ \\ \cmidrule{2-3} 
                             & Irrelevant      & $1.96\pm 0.05$ \\ \midrule
\multirow{3}{*}{Freeze-Omni} &\xmark              & $2.00\pm 0.05$ \\ \cmidrule{2-3} 
                             & Supporting      & $\mathbf{2.38\pm 0.04}$ \\ \cmidrule{2-3} 
                             & Irrelevant      & $1.54\pm 0.04$ \\ \midrule
\multirow{3}{*}{MiniCPM-o}   & \xmark               & $\mathbf{2.64\pm 0.06}$ \\ \cmidrule{2-3} 
                             & Supporting      & $2.49\pm 0.06$ \\ \cmidrule{2-3} 
                             & Irrelevant      & $1.63\pm 0.05$ \\
                             \bottomrule
\end{tabular}
\vskip -0.05in
\caption{GPT Score results when augmenting spoken response generation with either the ground-truth supporting utterance (``Supporting'') or an entirely unrelated utterance (``Irrelevant'') as prompts.}
\label{tab:rag_support}
\vskip -0.2in
\end{table}


\begin{table*}[t]
\small
\centering
\renewcommand{\arraystretch}{1.2} 
\setlength{\tabcolsep}{6pt}
\begin{tabular}{l c c c c c}
\toprule
\multirow{2}{*}{\textbf{Model}} & \multirow{2}{*}{\textbf{Retriever}} & \multirow{2}{*}{\textbf{ASR}} & \multicolumn{3}{c}{\textbf{GPT Score}} \\ 
\cmidrule(lr){4-6}
  & & & \textbf{top-\textit{1}} & \textbf{top-\textit{2}} & \textbf{top-\textit{3}} \\ 
\midrule
\rowcolor{gray!10}  &   & \cmark & $2.34 \scalebox{0.7}{$\pm 0.05$}$   & $2.30 \scalebox{0.7}{$\pm 0.05$}$  & $2.09 \scalebox{0.7}{$\pm 0.05$}$  \\ 
\rowcolor{gray!10} GLM-4-Voice \cite{glm2024chatglm}  & \multirow{-2}{*}{\textit{e5-large-v2} \cite{Wang2022TextEB}} & \xmark & $2.42 \scalebox{0.7}{$\pm 0.05$}$   & $2.40 \scalebox{0.7}{$\pm 0.05$}$  & $2.15 \scalebox{0.7}{$\pm 0.05$}$  \\ 
  &   SONAR \cite{Duquenne:2023:sonar_arxiv}    & $-$ & $2.24 \scalebox{0.7}{$\pm 0.05$}$   & $2.15 \scalebox{0.7}{$\pm 0.05$}$  & $1.97 \scalebox{0.7}{$\pm 0.05$}$  \\ 
\midrule
\rowcolor{gray!10}         &   & \cmark & $2.83 \scalebox{0.7}{$\pm 0.06$}$   & $2.68 \scalebox{0.7}{$\pm 0.06$}$  & $2.52 \scalebox{0.7}{$\pm 0.06$}$  \\ 
\rowcolor{gray!10}   Lyra \cite{zhong2024lyra}       & \multirow{-2}{*}{\textit{e5-large-v2} \cite{Wang2022TextEB}}  & \xmark & $2.94 \scalebox{0.7}{$\pm 0.06$}$   & $2.78 \scalebox{0.7}{$\pm 0.06$}$  & $2.68 \scalebox{0.7}{$\pm 0.06$}$  \\ 
  &    SONAR \cite{Duquenne:2023:sonar_arxiv}   & $-$ & $2.48 \scalebox{0.7}{$\pm 0.06$}$   & $2.39 \scalebox{0.7}{$\pm 0.06$}$  & $2.25 \scalebox{0.7}{$\pm 0.06$}$  \\ 
\midrule
\rowcolor{gray!10} &  & \cmark & $2.02 \scalebox{0.7}{$\pm 0.04$}$   & $1.98 \scalebox{0.7}{$\pm 0.04$}$  & $1.80 \scalebox{0.7}{$\pm 0.04$}$  \\ 
\rowcolor{gray!10}  Freeze-Omni \cite{xiong2024freeze} & \multirow{-2}{*}{\textit{e5-large-v2} \cite{Wang2022TextEB}}   & \xmark & $2.08 \scalebox{0.7}{$\pm 0.04$}$   & $2.03 \scalebox{0.7}{$\pm 0.04$}$  & $1.90 \scalebox{0.7}{$\pm 0.04$}$  \\ 
  &   SONAR \cite{Duquenne:2023:sonar_arxiv}    & $-$ & $1.83 \scalebox{0.7}{$\pm 0.04$}$   & $1.77 \scalebox{0.7}{$\pm 0.04$}$  & $1.67 \scalebox{0.7}{$\pm 0.04$}$  \\ 
\midrule
\rowcolor{gray!10} &  & \cmark & $2.10 \scalebox{0.7}{$\pm 0.05$}$   & $1.91 \scalebox{0.7}{$\pm 0.05$}$  & $1.81 \scalebox{0.7}{$\pm 0.05$}$  \\ 
\rowcolor{gray!10}    MiniCPM-o \cite{yao2024minicpm}   & \multirow{-2}{*}{\textit{e5-large-v2} \cite{Wang2022TextEB}}   & \xmark & $2.16 \scalebox{0.7}{$\pm 0.06$}$   & $1.98 \scalebox{0.7}{$\pm 0.05$}$  & $1.86 \scalebox{0.7}{$\pm 0.05$}$  \\ 
  &  SONAR \cite{Duquenne:2023:sonar_arxiv}     & $-$ & $2.01 \scalebox{0.7}{$\pm 0.05$}$   & $1.82 \scalebox{0.7}{$\pm 0.05$}$  & $1.78 \scalebox{0.7}{$\pm 0.05$}$  \\ 
\bottomrule
\end{tabular}
\vskip -0.05in
\caption{Evaluation results for RAG with voice interaction models. ``ASR'' indicates whether RAG is performed using ASR-transcribed text (\cmark) or ground-truth text (\xmark). The scores are reported with a 95\% confidence interval.}
\label{tab:rag_wo_asr}
\vskip -0.1in
\end{table*}


\begin{table}[]
\small
\centering
\begin{tabular}{l|c|ccc}
\toprule
\multirow{2}{*}{\textbf{Retriever}}                    & \multirow{2}{*}{\textbf{ASR}} & \multicolumn{3}{c}{\textbf{Recall}}                                                                                                                                      \\ \cmidrule{3-5} 
                                                       &                               & \multicolumn{1}{c|}{\textbf{top-\textit{1}}} & \multicolumn{1}{c|}{\textbf{top-\textit{2}}} & \textbf{top-\textit{3}} \\ \midrule
\multirow{2}{*}{\textit{e5-large-v2}} & \cmark         & \multicolumn{1}{c|}{0.5773}                                   & \multicolumn{1}{c|}{0.7339}                                   & 0.7959                                   \\ \cmidrule{2-5} 
                                                       & \xmark         & \multicolumn{1}{c|}{0.5827}                                   & \multicolumn{1}{c|}{0.7561}                                   & 0.8212                                   \\ \midrule
SONAR                                                  & $-$                           & \multicolumn{1}{c|}{0.3955}                                   & \multicolumn{1}{c|}{0.5306}                                   & 0.6087    \\
\bottomrule
\end{tabular}
\vskip -0.05in
\caption{Retrieval performance for each model used in the analysis, measuring the probability of the supporting utterance being included in the top-\textit{k} utterances. ``ASR'' indicates that retrieval is performed using transcripts obtained from the ASR model.}
\label{tab:retriever_recall}
\vskip -0.2in
\end{table}

For models other than MiniCPM-o, we observe that providing the correct supporting utterance improves performance on QA requiring past information, while using an incorrect utterance as a prompt degrades performance. This suggests that for most models, the primary obstacles to using RAG for remembering past conversations in voice interaction models lie not in the act of augmentation itself, but in factors beyond incorporating relevant information, such as retrieval errors.

To examine whether the limited effectiveness of RAG is primarily due to ASR errors, we analyze the impact of recognition errors in retrieving past utterances. Specifically, we compare two approaches: (1) retrieving using the ground-truth text of past conversations and the ground-truth transcript of the input speech and (2) retrieving directly from speech with a speech retriever module, bypassing the recognition process.

Since no suitable open-source speech retriever module is available, we use SONAR \cite{Duquenne:2023:sonar_arxiv}, which, while not primarily designed for retrieval, extracts semantic embeddings from speech and retrieves past utterances based on cosine similarity. Note that since the voice interaction models rely on text for spoken response generation, retrieved information is provided in text form regardless of retriever modality.

As shown in Table \ref{tab:rag_wo_asr}, ASR has minimal impact on RAG performance for text-based retrievers (``ASR'' \cmark vs. \xmark). In contrast, using a speech retriever leads to a relatively significant performance drop. These results align with the retrieval performance in Table \ref{tab:retriever_recall}, where ASR does not substantially affect the retriever’s ability to include the supporting utterance in the top-\textit{k} results. Additionally, the speech retriever is not originally designed for retrieval, and training challenges—such as longer audio sequences and limited data—contribute to recall degradation, leading to performance decline.

The observations in Section \ref{subsec:recall} highlight the recall difficulty of speech being substantial compared to text. The findings in this section show that even when models retrieve information through an external module and augment it into generation, they fail to use it effectively, suggesting two key areas for improvement. First, even when explicitly provided with the supporting utterance, current models underperform compared to text-based counterparts, underscoring the need for stronger conversational capabilities in voice interaction models. Second, while several methods were developed to ensure robustness against retrieval noise in the NLP domain \cite{Chen_Lin_Han_Sun_2024, yoran2024making}, voice interaction models require better training and inference strategies to enhance resilience to retrieval noise alongside general modeling improvements.