\section{ContextDialog}
\label{sec:dataset}

We propose ContextDialog, a comprehensive benchmark designed to evaluate a voice interaction model’s ability to engage in, retain, and leverage relevant information throughout multi-turn conversations, reflecting real-world scenarios where people often forget and revisit past exchanges. ContextDialog is constructed using MultiDialog \cite{park-etal-2024-lets}, a spoken dialog corpus featuring conversations between two speakers, comprising approximately 340 hours of data with at least 10 turns per conversation from 12 speakers. 
We use the \texttt{test\_freq} and \texttt{test\_rare} splits from MultiDialog, consisting of 450 and 381 spoken dialogs, respectively. Some data are filtered during generation and validation, with the final statistics of ContextDialog shown in Table \ref{tab:contextdialog_stats} and the data generation pipeline illustrated in Figure \ref{fig_contextdialog}.

\begin{table}[t]
    \centering
    \small
    \begin{tabular}{l|c|cc}
        \toprule
        \textbf{} & \textbf{Statistics} & \textbf{\texttt{test\_freq}} & \textbf{\texttt{test\_rare}} \\
        \midrule
        \multirow{4}{*}{\shortstack[l]{Dialog\\History}} 
            & \# dialogs       & \multicolumn{1}{c}{363}   & \multicolumn{1}{c}{290}   \\
            & max turn         & \multicolumn{1}{c}{16}    & \multicolumn{1}{c}{24}    \\
            & min turn         & \multicolumn{1}{c}{10}    & \multicolumn{1}{c}{10}    \\
            & avg turn         & \multicolumn{1}{c}{10.57} & \multicolumn{1}{c}{10.61} \\
        \midrule
        \multirow{4}{*}{\shortstack[l]{Question /\\Answer}}       
            & \# QA pairs     &  1,452            & 1,160              \\  
            & max dur(s)     & 13.19 / 24.80             & 19.23 / 22.11              \\
            & min dur(s)     & 2.60 / 1.11               & 2.14 / 1.30               \\
            & avg dur(s)     & 5.97 / 6.78               & 5.90 / 6.59               \\
        \bottomrule
    \end{tabular}
    \caption{Statistics of ContextDialog for Dialog History and generated QA on \texttt{test\_freq} and \texttt{test\_rare} splits. The numbers on the left and right are related to the question and answer, respectively. The term dur refers to the duration of the generated question and answer.}
    \label{tab:contextdialog_stats}
    \vskip -0.2in
\end{table}


\begin{figure*}
    \centering
    \includegraphics[width=0.90\linewidth]{figures/Figure2.pdf}    
    \caption{Overview of our analyses. In Section \ref{subsec:recall}, we evaluate model recall by analyzing responses to questions about (a) past user and (b) past model utterances. In Section \ref{subsec:retrieval}, we examine whether (c) augmenting spoken response generation with separately retrieved utterances improves responses to questions about past utterances.}
    \label{fig_recall}
    \vskip -0.2in
\end{figure*}

\subsection{Text Question-Answer Generation}
\label{subsec:generation}
We first construct a dataset of context-recall QA pairs using \textit{gpt-4o}. Given the transcripts of MultiDialog, the model is prompted to generate questions and answers based solely on information that appeared only once in the conversation. To ensure diversity and broad applicability, we generate questions based on both user and system utterances, selecting information from either the first or second half of the conversation. This results in four QA pairs per spoken dialog. Additionally, the model is requested to output the \textit{supporting utterance}—the utterance in the conversation that serves as the clue for the answer—for each pair to enhance both data quality and usability. For a more realistic setting, the questions are designed to require detailed answers rather than simple Yes/No responses.

After generating the QA pairs, we validate each question, answer, and supporting utterance using \textit{o1-mini} \cite{openai2024openaio1card}. A validation prompt assesses their appropriateness within the dialog context through three rounds of evaluation: (1) dialog context up to just before the supporting utterance, (2) up to and including the supporting utterance, and (3) the entire conversation. The first step is to check whether the answer can be inferred without the supporting utterance, requiring a NO response, while the second and third ensure consistency across different context levels, requiring YES responses. Failed QA pairs are filtered out, and the validated pairs are used to construct the spoken QA dataset. The prompts used are in Appendix \ref{subapp:contextdialog}.

\subsection{Spoken Question-Answer Generation}
\label{subsec:tts}
To ensure the user and model to continue naturally in the given spoken dialog, the voice of the spoken QA pair must seamlessly match that of the original conversation. To achieve this, we use Fish Speech \cite{fish-speech-v1.4}, a speaker-adaptive TTS model that generates speech in the target speaker’s timbre using a reference speech. For each QA pair, we select the longest speech segment from the original dialog for each speaker as the reference to maximize speaker similarity. To ensure accurate pronunciation, each spoken QA pair is generated five times, and the sample with the lowest word error rate (WER)—measured using a separate ASR model \cite{pmlr-v202-radford23a}—is selected. If the selected sample has a nonzero WER, it is manually reviewed, and mispronounced samples are filtered out. This process ensures that ContextDialog maintains both speaker identity and clear pronunciation in the final spoken QA pairs.
