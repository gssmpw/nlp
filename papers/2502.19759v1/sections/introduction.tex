\section{Introduction}

Voice assistants such as Apple Siri and Amazon Alexa have become an irreplaceable element of daily life, enabling natural and efficient speech-based interactions. In early systems, a cascaded pipeline is employed where speech is first transcribed using automatic speech recognition (ASR), then processed as text, and finally converted back to speech via text-to-speech (TTS) \cite{lin-etal-2024-advancing}. With the advent of large language models (LLMs) \cite{NEURIPS2020_1457c0d6, grattafiori2024llama3herdmodels}, however, the research community has shifted greatly towards end-to-end approaches. These models integrate ASR, text processing, and TTS into a unified multimodal framework \cite{zhang-etal-2023-speechgpt}, which not only reduces latency \cite{xie2024miniomnilanguagemodelshear} but also better preserves the richness of vocal cues \cite{kim2024paralinguisticsaware}. In line with this trend, GPT-4o \cite{openai2024gpt4ocard} has demonstrated impressive capabilities by processing visual, speech, and text data in an end-to-end manner, where various voice interaction models, datasets, and benchmarks have rapidly emerged alongside. \cite{cheng2025omnichatenhancingspokendialogue, cheng2025voxdialogue, fang2025llamaomni, xie2024miniomni2opensourcegpt4ovision}.

Despite these advances, most current models excel only in single-turn interactions. In practical applications, however, users engage in multi-turn dialogs where a one-off response is insufficient. Specifically, models must continuously retain and leverage contextual information from previous turns. For example, Gemini 2.0 \cite{google2024gemini} demonstrates the ability to remember preceding details—for instance when a user provides an apartment door code during interaction and inquires about it later—thereby showcasing robust context-maintenance. Notably, other closed-source solutions, such as OpenAI’s Advanced Voice Mode \cite{openai2024twelve}, have also showcased similar capabilities by referencing past interactions.

In parallel, the open-source community has also intensified its efforts to develop voice interaction models that support multi-round communications \cite{kyutai2024moshi, yao2024minicpm, zeng2024glm4}. Typically, these models take speech as input and generate both text and speech outputs, rather than producing spoken responses alone, to leverage the strengths of LLMs and ensure coherent, multi-turn responses. However, it remains unclear whether open-source systems can effectively retain and utilize long-range interaction histories. Furthermore, there are no benchmarks that explicitly require using dialog history to generate responses.

In this work, we systematically investigate the ability of open-source voice interaction models to maintain and utilize conversational context through two key experiments. We evaluate (1) whether models can recall and generate spoken responses based on previous dialog and (2) their robustness in incorporating externally retrieved utterances. To support this evaluation, we introduce ContextDialog—a speech-to-speech benchmark that focus on assessing recall via spoken question-answer (QA) pairs derived from existing spoken dialogs, prompting one speaker to reference earlier information.

Our findings reveal that open-source models struggle with past speech in two key aspects. \textbf{Performance gap with text-based systems} – Speech models generally perform worse than their text-based counterparts, and \textbf{Modality-based recall gap} – Within speech models, recalling speech-based information is less accurate than retrieving text, likely due to weaker speech processing capabilities.
Additionally, our investigation of retrieval-augmented generation (RAG) shows that it fails to compensate for the model's inability to recall past information. We identify a major challenge: \textbf{Sensitivity to retrieval errors} – Models are highly susceptible to retrieval mistakes, leading to unchanged or even degraded performance.
Through these findings, we highlight the challenges models face in processing past conversational context and their sensitivity to noise in retrieved information, drawing attention to a fundamental, yet often overlooked, capability within the open-source community. Our contributions are as follows: 

\begin{itemize} 
    \item 
     We introduce ContextDialog, a benchmark designed to evaluate the models' ability to utilize dialog history in multi-turn conversations.\footnote{Project Page: \href{https://contextdialog.github.io/}{https://contextdialog.github.io/}}
    \item We show that most open-source models struggle with recalling past dialogs and fail to effectively incorporate retrieved information, even when augmented with external retriever.
    \item Through extensive evaluation and analysis, we uncover overlooked limitations in current models that restrict their applicability and propose directions for future improvements.
\end{itemize}