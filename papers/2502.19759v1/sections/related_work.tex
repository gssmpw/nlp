\section{Related Works}
\textbf{Voice Interaction Models} 
Early voice interaction models follow a cascaded approach \cite{lin-etal-2024-advancing}, transcribing speech into text, processing the transcription, and then synthesizing the output speech. Recently, end-to-end pipelines have emerged, performing these steps within a single model \cite{zhang-etal-2023-speechgpt}. Although some models generate spoken responses without relying on text \cite{nguyen-etal-2023-generative}, the inherent length and data scarcity of speech hinder semantic  modeling \cite{kyutai2024moshi}. Recent approaches integrate text generation within speech modeling to mitigate such problem, leveraging pre-trained LLMs by incorporating text as an intermediate representation \cite{kim2024paralinguisticsaware, zhang-etal-2023-speechgpt}, generating it alongside speech \cite{fang2025llamaomni}, or interleaving it with speech tokens \cite{zeng2024glm4}.

Many prior works focus on single-turn interaction \cite{fang2025llamaomni, kim2024paralinguisticsaware, xie2024miniomnilanguagemodelshear, xie2024miniomni2opensourcegpt4ovision, zeng2025scaling, zhang-etal-2023-speechgpt, zhao2024advancingspeechlanguagemodels}. As an extension of these studies, multi-round voice interaction models have also emerged \cite{chen2024slamomnitimbrecontrollablevoiceinteraction, chen2025minmomultimodallargelanguage, kyutai2024moshi, fu2025vita, li2025baichuanomni15technicalreport, mai2025realtimetextlessdialoguegeneration, mitsui-etal-2024-pslm, park-etal-2024-lets, veluri-etal-2024-beyond, xiong2024freeze, yao2024minicpm, zeng2024glm4, zhang2024intrinsicvoiceempoweringllmsintrinsic, zhang2025omniflattenendtoendgptmodel, zhong2024lyra}. However, whether these models can effectively handle past conversation history in real-world multi-turn dialog scenarios remains unexplored. For models to function as effective voice assistants, it is crucial to assess their ability to retain and leverage past utterances to generate contextually appropriate responses.
 
\begin{figure*}
    \centering
    \includegraphics[width=0.98\linewidth]{figures/Figure1.pdf}    
    \caption{Overview of the ContextDialog generation process. Past-recall QA pairs are first generated and validated (Section \ref{subsec:generation}), then converted to speech via adaptive TTS and verified both automatically and manually (Section \ref{subsec:tts}).}
    \label{fig_contextdialog}
    \vskip -0.2in
\end{figure*}

\textbf{Benchmarks} Numerous datasets and benchmarks for audio foundation models have emerged \cite{sakshi2025mmau, wang2024audiobench, yang-etal-2024-air}, particularly for voice interaction models \cite{chen2024emovaempoweringlanguagemodels, fang2025llamaomni, park-etal-2024-lets, xie2024miniomnilanguagemodelshear}. For example, in task-oriented spoken dialogs, benchmarks assess a modelâ€™s ability to recognize entities and dialog states from past utterances \cite{henderson-etal-2014-second, NEURIPS2023_7b16688a, spithourakis-etal-2022-evi}, while in open-domain dialogs, they focus on modeling and evaluating response coherence \cite{Busso2008, cieri-etal-2004-fisher, cheng2025omnichatenhancingspokendialogue, park-etal-2024-lets}. Beyond semantic relevance, recent works propose benchmarks targeting non-verbal components crucial for voice interaction models, such as gender, emotion, and background noise \cite{NEURIPS2024_681fe4ec, chen2024voicebenchbenchmarkingllmbasedvoice, cheng2025voxdialogue}. Unlike existing benchmarks that evaluate multi-turn semantics without ensuring past information is necessary for responses, ContextDialog explicitly requires models to retrieve and utilize relevant past utterances, enabling a systematic assessment of recall ability.

\textbf{Retrieval in Voice Interaction Model} 
With advancements in RAG techniques in natural language processing (NLP) \cite{JMLR:v24:23-0037, NEURIPS2020_6b493230}, efforts to integrate RAG into spoken dialog models have emerged \cite{10448210, Min2025, 10447448}. Prior works have primarily focused on task-oriented dialog for entity extraction \cite{10447448} or spoken question answering \cite{10448210, Min2025}, retrieving information from long speech documents \cite{lee2018spoken}. In contrast, we focus on multi-turn voice interactions, examining whether relevant data retrieved via an external module can be effectively utilized in the generation, specifically tailored for recent open-source interaction models.

