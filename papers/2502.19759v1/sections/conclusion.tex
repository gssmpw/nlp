\section{Conclusion}
In this work, we conducted an in-depth analysis of a critical yet underexplored challenge in open-source voice interaction models: maintaining and utilizing past utterances. To address the lack of benchmarks that explicitly require accurate reference to past dialog, we introduced ContextDialog, a speech-to-speech benchmark designed to systematically evaluate a modelâ€™s ability to recall utterances from previous turns. Using this benchmark, our experiments revealed that models struggle with recalling past utterances and remain highly sensitive to retrieval errors, limiting improvements even with dedicated retriever. These findings highlight a crucial gap in memory retention for open-source models, emphasizing the need for stronger conversational memory methods, such as improved long-context modeling, robust RAG techniques, or dedicated memory modules. We hope that our work may act as a trigger to raise awareness to this overlooked challenge and encourage future research to further enhance the usability and effectiveness of voice interaction models.