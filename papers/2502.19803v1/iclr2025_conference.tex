
\documentclass{article} % For LaTeX2e
\usepackage{iclr2025_conference,times}

\input{math_commands.tex}

\usepackage{hyperref}
\usepackage{url}
\usepackage{graphicx}
\usepackage[T1]{fontenc}
\usepackage{amsmath}
\usepackage{caption}
\usepackage{lipsum}
\usepackage{wrapfig}
\usepackage[ruled,vlined]{algorithm2e}
\usepackage{tabularx}
\title{UIFace: Unleashing Inherent Model Capability to Enhance Intra-Class Diversity in \\ Synthetic Face Recognition}

% Authors must not appear in the submitted version. They should be hidden
% as long as the \iclrfinalcopy macro remains commented out below.
% Non-anonymous submissions will be rejected without review.

\author{Xiao Lin$^{1,}$\footnotemark[1]\, , Yuge Huang$^{1,}$\footnotemark[1]\, , Jianqing Xu$^{1}$, Yuxi Mi$^{2}$, Shuigeng Zhou$^{2}$, Shouhong Ding$^{1,}$\footnotemark[2]  \\
\centerline{$^1$Tencent Youtu Lab \quad $^2$Fudan University} \\
\centerline{\texttt{\{xiaolin,yugehuang,joejqxu,ericshding\}@tencent.com}} \\ 
\centerline{\texttt{\{yxmi20,sgzhou\}@fudan.edu.cn}} \\ 
}
\renewcommand{\thefootnote}{\fnsymbol{footnote}}
\footnotetext[1]{Equal contribution}
\footnotetext[2]{Corresponding author}

\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}

\iclrfinalcopy % Uncomment for camera-ready version, but NOT for submission.
\begin{document}

\maketitle
\begin{abstract}
Face recognition (FR) stands as one of the most crucial applications in computer vision. The accuracy of FR models has significantly improved in recent years due to the availability of large-scale human face datasets. However, directly using these datasets can inevitably lead to privacy and legal problems. Generating synthetic data to train FR models is a feasible solution to circumvent these issues. While existing synthetic-based face recognition methods have made significant progress in generating identity-preserving images, they are severely plagued by context overfitting, resulting in a lack of intra-class diversity of generated images and poor face recognition performance. In this paper, we propose a framework to \textbf{U}nleash \textbf{I}nherent capability of the model to enhance intra-class diversity for synthetic face recognition, shortened as \textbf{UIFace}. Our framework first trains a diffusion model that can perform sampling conditioned on either identity contexts or a learnable empty context. The former generates identity-preserving images but lacks variations, while the latter exploits the model's intrinsic ability to synthesize intra-class-diversified images but with random identities. Then we adopt a novel two-stage sampling strategy during inference to fully leverage the strengths of both types of contexts, resulting in images that are diverse as well as identity-preserving. Moreover, an attention injection module is introduced to further augment the intra-class variations by utilizing attention maps from the empty context to guide the sampling process in ID-conditioned generation. Experiments show that our method significantly surpasses previous approaches with even less training data and half the size of synthetic dataset. The proposed \textbf{UIFace} even achieves comparable performance with FR models trained on real datasets when we further increase the number of synthetic identities. Code will be released at \url{https://github.com/Tencent/TFace}.

\end{abstract}

\vspace{-1.9mm}
\section{INTRODUCTION}
\label{INTRODUCTION}

Recent years have witnessed incredible improvements in the accuracy of FR models. This can be attributed to the advancements in model architectures \citep{boutros2022pocketnet, huang2017densely}, margin-based loss functions \citep{deng2019arcface, huang2020curricularface}, and more importantly, the availability of large-scale face datasets \citep{huang2008labeled, kemelmacher2016megaface, zhu2021webface260m}, which contain millions of identities with rich variations in age, pose and expression.

However, these large-scale datasets are often collected directly from the Internet, without the explicit consent of individuals, which inevitably leads to privacy and legal issues \citep{regulation2016regulation}. Moreover, these datasets suffer from challenges about noisy labels \citep{wang2018devil, wang2019co} and imbalanced class distribution \citep{liu2019large, yi2014learning}. In other words, images with the same label may belong to different individuals, and there is a significant disparity in the number of images among different identities. These drawbacks limit the further application of real face datasets.

\begin{figure}[ht]
\begin{center}
\includegraphics[width=1\linewidth]{fig/motivation/figure.pdf}
\caption{
a) \textbf{Visualization of samples from different datasets.} Samples from the real face dataset CASIA exhibit variations in attributes such as pose, expression and illumination. However, when we synthesize face data using previous method IDiff-Face \citep{boutros2023idiff}, the generated images show poor diversity, more specifically, similar expressions and poses, which is caused by identity overfitting. In contrast, our method can generate a wider variety of images, thereby enhancing the accuracy of the trained FR model.
b) \textbf{Quantitative comparison of dataset diversity.} We apply LPIPS and Improved Recall to measure the diversity of different datasets. A higher value indicates better diversity.
c) \textbf{Quantitative comparison of final accuracy of the FR models.} 
}
\label{fig:motivation} 
\end{center}
\end{figure} 

Generating synthetic data of non-existent identities to train FR model, also known as synthetic face recognition, is a feasible solution to the above issues as suggested by \cite{deandres2024frcsyn}. There have already been some explorations in this field. The initial methods \citep{shen2018faceid, deng2020disentangled} employ Generative Adversarial Networks \citep{goodfellow2020generative} to generate synthetic face data and utilize the disentangled latent space to enhance the controllability. However, GAN-based methods have been shown to generate only a limited number of unique identities \citep{kim2023dcface}, leading to poor generalization of the trained FR models. Recently, diffusion models have made significant advancements in image generation \citep{song2020denoising, dhariwal2021diffusion, song2020score, rombach2022high}. Some diffusion-based methods \citep{wang2024instantid, li2024photomaker, boutros2023idiff, papantoniou2024arc2face, zhang2024flashface} have been proposed for identity-preserving face generation, which is achieved by conditioning the generation process with identity contexts, i.e., identity features extracted from the pretrained FR models \citep{deng2019arcface, Boutros_2022_CVPR}. It has been demonstrated that diffusion-based methods are capable of generating a greater variety of unique identities compared to GAN-based methods \citep{kim2023dcface}, showing a highly promising potential for real applications. Yet, a high-quality human face dataset does not just imply a large number of identities but also requires good intra-class diversity. Specifically, the synthetic face images need to exhibit variations in attributes such as expression, illumination and pose.

While existing methods have made significant progress, they still suffer from context overfitting \citep{boutros2023idiff}. More specifically, a fixed identity context not only determines the identity of generated images but also limits their the variations of identity-irrelevant attributes, resulting in insufficient intra-class diversity. 
As illustrated in Figure \ref{fig:motivation}a, previous methods often exhibit a reduced diversity of synthetic images, i.e., lack of variations in expression and pose, compared to real-world dataset. To further demonstrate this issue, following \citet{kim2023dcface} and \citet{papantoniou2024arc2face}, we utilize LPIPS \citep{zhang2018unreasonable} and Improved Recall \citep{kynkaanniemi2019improved} to quantitatively measure the intra-class diversity of these datasets. As shown in Figure \ref{fig:motivation}b, the diversity of the synthetic dataset generated by previous approach significantly lags behind that of the real dataset CASIA-Webface \citep{yi2014learning}, resulting in poor performance of the trained FR model (Figure \ref{fig:motivation}c). Although methods such as Contextual Partial Dropout \citep{boutros2023idiff}, style condition extractor \citep{kim2023dcface} and paired data generation \citep{he2024imagineyourself} have been proposed to alleviate this issue, they either rely on complex network designs or require the introduction of additional training data, and still have a significant performance gap compared to real data-based methods.  

Intuitively, the model inherently possesses the ability to generate diverse images because the real data used to train the generative model contain rich intra-class variations. However, when it comes to a specific identity context, such inherent capability is limited because of context overfitting. Therefore, in this paper, we propose a novel framework to unleash model inherent capability to enhance intra-class diversity for synthetic face recognition, shortened as \textbf{UIFace}. Specifically, we first train a diffusion model that can perform sampling conditioned on either specific identity contexts or a learnable empty context. When conditioned on a specific identity context, the model generates identity-preserving images but with poor diversity due to context overfitting. On the other hand, when conducting generation conditioned on the empty context, the model can fully leverage its inherent capability to synthesize various images but with random identities. This is because during training, we allow the empty context to generate all the images in the dataset. To exploit the strength of both types of contexts to synthesize images that are diverse as well as identity-preserving, we adopt a novel two-stage generation strategy during inference. Our key observation is that in the early stages of sampling, the model restores identity-irrelevant contents, while in the later stage the model recovers the identity-relevant details (Section \ref{timesteps}). Thus, in the first stage, the diffusion model performs sampling conditioned on the empty context for unleashing its intrinsic ability to generate large intra-class variations. In the second stage, the model generates identity-preserving details based on given identity conditions. Such a two-stage strategy takes advantage of model's inherent ability to enhance the diversity of conditional generation. Moreover, we propose an adaptive partitioning strategy to adaptively determine the boundary of these two stages for each sample based on the difference between adjacent cross-attention maps. To fully harness the diversity of empty context-conditioned generation, we propose an attention injection module to use the attention maps from unconditional generation to guide the conditional generation process, which further leverages the model's inherent ability while maintaining ID-consistency. 

In summary, our contributions are as follows.
\begin{itemize}
    \item We propose a novel two-stage synthetic face recognition framework. By allowing the learnable empty context and identity contexts to dominate different stages of the face generation, our method can fully leverage model's inherent capability to achieve intra-class-diversified image generation while maintaining identity-preserving.
    \item We propose an adaptive partitioning strategy to adaptively determine the boundaries of two stages for different samples and an attention injection module to utilize attention maps from unconditional generation to guide conditional generation. These designs can further unleash model's inherent ability to enhance the intra-class variations of synthesized images.
    \item Experimental results show that our method outperforms existing state-of-the-art methods by a significant margin by using fewer training data. We surpass current state-of-the-art methods even when synthesizing fewer than half the number of synthetic identities. When further increasing the number of identities, the proposed UIFace can surprisingly achieve comparable performance with those FR models trained on real datasets.
\end{itemize}

\section{RELATED WORK}
\label{RELATED WORK}

\subsection{Face Recognition}
Face recognition involves identifying or verifying a person from an enrolled dataset. With the continuous improvement of network architectures \citep{he2016deep, boutros2022pocketnet, huang2017densely} and the introduction of novel loss functions \citep{Boutros_2022_CVPR, deng2019arcface, huang2020curricularface, wang2018cosface, Kim_2022_CVPR}, the accuracy of face recognition models has made remarkable advancements in recent years. More importantly, the improvement in performance is also attributed to large-scale face datasets \citep{huang2008labeled, cao2018vggface2, zhu2021webface260m, guo2016ms, kemelmacher2016megaface} as well as datasets tailored to address specific challenges \citep{zheng2018cross, zheng2017cross, sengupta2016frontal, moschoglou2017agedb}. Nevertheless, these datasets are collected directly from the Internet without explicit individual consent, leading to inevitable privacy and legal concerns. Moreover, they are suffered from noisy labels and long-tail problem \citep{yi2014learning}. The above issues need to be carefully considered before using these datasets.

\subsection{Face Image Synthesis}
Image generation models have made remarkable progress, allowing the synthesis of high-quality human face images. GAN-based methods \citep{brock2018large, choi2018stargan, karras2017progressive, karras2019style, karras2020analyzing, yin2017towards} are successful pioneers among them. They achieve identity-preserving face generation by decoupling identity and attributes \citep{bao2018towards}, or introducing additional classifiers and discriminators \citep{shen2018faceid}. DiscoFaceGAN \citep{deng2020disentangled} introduces a more fine-grained decoupled latent space to enable precise control of synthesized faces. Recently, diffusion models \citep{song2020denoising, ho2020denoising, dhariwal2021diffusion, song2020score, rombach2022high} have made significant advances in the field of image synthesis. Some methods \citep{zhang2024flashface, wang2024instantid, li2024photomaker} have achieved high-fidelity identity-preserving image generation with text and ID-conditioned diffusion model.

\subsection{Face Recognition with Synthetic Dataset}
Replacing real datasets with synthetic face datasets can address the legal and class imbalance issues. SynFace \citep{qiu2021synface} applies DiscoFaceGAN \citep{deng2020disentangled} to synthesis identity-consistent data for training FR models. 
DCFace \citep{kim2023dcface} proposes an diffusion-based method that employs decoupled style and identity encoders to generate dual conditions and demonstrates that DDPM \citep{ho2020denoising} can generate more unique identities than GAN-based methods, which is crucial for improving the accuracy of FR models. Some works \citep{boutros2023idiff, papantoniou2024arc2face} continue to enhance the quality of synthetic datasets, narrowing the performance gap between FR models trained on synthetic and real data. Nevertheless, as depicted in Figure \ref{fig:motivation}, existing methods still suffer from context overfitting, resulting in insufficient diversity in synthetic datasets. While some approaches \citep{boutros2023idiff,kim2023dcface,he2024imagineyourself} have been suggested to address this challenge, they often depend on intricate network architectures or necessitate extra training data. In contrast, this paper explores to unleash model's inherent capability to intra-class-diversified image generation for synthetic-based face recognition.

\begin{figure}
    \begin{center}
    \includegraphics[width=1\linewidth]{fig/observation/figure.pdf}
    \caption{
    \textbf{Effects of different sampling timesteps on identity.} The x-axis represents timestep intervals where identity contexts are used as conditions. The empty context is used as a substitute in timesteps that not covered in intervals. The y-axis represents the intra-class similarity of the generated face images. The maximum sampling timestep $T$ is set to 1000. 
    }
    \label{fig:observation} 
    \end{center}
 \end{figure} 

\begin{figure}[t]
   \begin{center}
   \includegraphics[width=1\linewidth]{fig/method/method_v2.pdf}
   \caption{
   \textbf{Overview of proposed UIFace.} We propose a two-stage sampling strategy to unleash the intrinsic capability of the model to generate diverse images. In the first stage, the model performs unconditional generation based on the empty context $c_e$. In the second stage, the model restores identity-relevant details conditioned on specific identity contexts $c$. We further propose an adaptive stage partition strategy to determine the boundary of these two stages $t_0$ and an attention injection module to enhance diversity of synthetic dataset while maintaining identities. 
   }
   \label{fig:method} 
   \end{center}
\end{figure} 

\section{METHOD}
\label{METHOD}
\textbf{Overview.} The aim of synthetic face recognition is to generate synthetic face data for training FR models, thereby addressing the inherent issues of real datasets collected directly from the Internet, as discussed in Section \ref{INTRODUCTION}. Formally, given the training dataset $\{x_{i}\}_{i=1}^{N}$, we follow previous works \citep{papantoniou2024arc2face,boutros2023idiff} to utilize a pre-trained FR model $\mathcal{F}$ to extract the identity context $c_{i} = \mathcal{F}(x_{i})$ for each image, resulting in extended dataset $\{x_{i}, c_{i}\}_{i=1}^{N}$. Subsequently, we train a conditional diffusion model $\mathcal{G}$ that can perform face generation conditioned on either specific identity contexts $c_{i}$ or en empty context $c_{e}$ (Section \ref{Preliminary}). Then we elucidate the crucial observation that in the early sampling stage, the model restores identity-irrelevant contents, whereas in the later stage it recovers details that determine the identity (\ref{timesteps}). Next, we introduce the two-stage sampling strategy to generate synthetic face images that simultaneously preserve identity and exhibit diversity (Section \ref{Two-stage}). Then, we propose an attention injection mechanism to further enhance the quality of the synthetic images (Section \ref{Attention injection}). Lastly, we present details about synthetic dataset generation (Section \ref{synthetic dataset generation}). The framework overview is shown in Figure \ref{fig:method}.

\subsection{Preliminary}
\label{Preliminary}
Latent Diffusion Model \citep{rombach2022high} employs a denoising
process to approximate the distribution of latent representations $z$ of real images $x$. A Variational Autoencoder $\mathcal{E}$ is first applied to map the images to a lower-dimensional latent space $z = \mathcal{E}(x)$. The images are progressively corrupted by adding Gaussian noise according to a predefined schedule during training. Formally, 
\begin{equation}
    z_{t} = \sqrt{\bar{\alpha}_{t}}z_{0} + \sqrt{1-\bar{\alpha}_{t}}\epsilon,
\end{equation}
where $t$ and $z_{0}$ stand for the diffusion timestep and clean latent representation respectively. The reverse process (also known as the sampling process) is defined by the following equation
\begin{equation}
z_{t-1} = \mu_\theta(z_t, t, c) \\
= \frac{1}{\sqrt{\alpha_t}} \left( z_t - \frac{1 - \alpha_t}{\sqrt{1 - \bar{\alpha}_t}} \epsilon_\theta(z_t, t, c) \right).
\end{equation}
where $c$ is the textual condition. The optimization of the UNet \citep{ronneberger2015u} backbone $\epsilon_\theta$ is achieved by recovering the random noise $\epsilon$ and minimizing the following loss
\begin{equation}
    \mathcal{L} = \mathbb{E}_{z_t, t, c, \epsilon \sim \mathcal{N}(0, \mathbf{I})} \left[ \left\| \epsilon - \epsilon_\theta(z_t, t, c) \right\|_2^2 \right].
\end{equation}
In synthetic-based face recognition, $c$ is the identity context extracted from a pre-trained FR model. Our method introduce an additional learnable empty context $c_{e}$. During training iterations, the identity context $c$ is probabilistically replaced with $c_{e}$. Thus, $c_e$ can be used to generate images of any identities present in the training data. When training finished, the model can generate identity-preserving images conditioned on any given $c$, or random face images conditioned on $c_e$. Our training process is consistent with previous ID-conditioned diffusion methods except for the introduction of empty context. Next, we will describe the inference process of our method.

\subsection{Effects of different sampling timesteps on identity}
\label{timesteps}
In this subsection, we present our observations about effects of different sampling timesteps on the identity of generated images. For this purpose, we inject identity contexts as generation conditions in different timestep intervals and calculate the intra-class similarity of generated images, which is based on the distance of features from a pre-trained FR model just as the standard metric in this field. As depicted in Figure \ref{fig:observation}, during the initial generation phase (for larger $t$ values), employing identity contexts as conditions exhibits minimal enhancement in the intra-class similarity of synthesized images, while condition the generation process in the later timesteps (for smaller $t$ values) can rapidly improve the intra-class similarity. It implies that the model recovers identity-irrelevant contents at early sampling stage and restores identity-relevant details in later stage. Based on these observations, we propose a two-stage sampling strategy to unleash the inherent diversity of the model while maintaining the identity consistency of generated images. 
\subsection{Two-stage sampling strategy}
\label{Two-stage}
As mentioned in the Section \ref{INTRODUCTION}, previous synthetic-based methods suffer from the overfitting of identity context $c$, leading to poor diversity of generated face images and degenerated face recognition performance. To address this issue, we propose a two-stage sampling strategy during inference to unleash the model's inherent capability to enhance intra-class diversity. According to the observations from Section \ref{timesteps}, we instruct the model to perform sampling conditioned on the empty identity $c_e$ in the early stage, which helps to introduce greater intra-class variations. In the later stage, we condition the generation process with specific identity context $c$ to generate identity-relevant details. This strategy leads to  both diverse and identity-preserving images. The overall formula is as follows, 
\begin{equation}
z_{t-1} = 
\begin{cases} 
\frac{1}{\sqrt{\alpha_t}} \left( z_t - \frac{1 - \alpha_t}{\sqrt{1 - \bar{\alpha}_t}} \epsilon_\theta(z_t, t, c_{e}) \right) & \text{if } t \in [t_0, T], \\
\frac{1}{\sqrt{\alpha_t}} \left( z_t - \frac{1 - \alpha_t}{\sqrt{1 - \bar{\alpha}_t}} \left((w+1)\epsilon_\theta(z_t, t, c) - w\epsilon_\theta(z_t, t, c_e)\right)\right) & \text{if } t \in [0, t_0],
\end{cases}
\end{equation}
where $t_0$ serves as the boundary between the two stages during inference and $w$ is the scale of classifier-free guidance \citep{ho2022classifier}. Next we further introduce an adaptive partitioning strategy to assign a unique boundary $t_0$ for each sample.

\textbf{Adaptive partitioning.} We observed that the cross-attention maps between UNet features and identity context exhibit significant variations during the early sampling timesteps when the model has not yet started focusing on details that determine the identity. And a decrease of temporal difference in cross-attention maps signifies the focus of sampling process shifting towards identity-relevant details in images (See Appendix for further details). Thus we propose an adaptive partition strategy based on the temporal differences of cross-attention maps. Specifically, let $h_t$ denote the normalized cross-attention map between the learnable empty context $c_e$ and the UNet features at timestep $t$ and $d_t$ denote the difference of adjacent cross-attention maps. The boundary $t_0$ for each sample is determined as follows,
\begin{equation}
    d_t = \|h_{t+1} - h_{t}\|_{2}, \quad t_0 = \min \{ t : (d_{t+1} > th) \; and \; (d_{t} < th) \}
\end{equation}
where $th$ is a hyperparameter. We interpolate the cross-attention maps of each layer to the same resolution, then average and normalize them across the spatial dimensions to get the final $h_t$. The attention maps mentioned later in this paper are obtained in a similar manner.

\subsection{Attention injection}
\label{Attention injection}
To further enhance the quality of synthesized images by leveraging the inherent diversity of the empty context $c_e$, we propose an attention injection module in the second sampling stage to directly use the attention maps of $c_e$ to guide the conditional generation. Let $h_{self}^{c}$ and $h_{cross}^{c}$ represent the self-attention map of UNet features in conditional sampling and cross-attention map between UNet features and identity context $c$ respectively. Similarly, let $h_{self}^{c_e}$ and $h_{cross}^{c_e}$ denote the attention maps in unconditional generation. A naive approach is to directly copy the corresponding attention maps. Nevertheless, experimental results indicate that directly substituting the cross-attention map has a considerable impact on identity and quality of the generated images (Section \ref{Ablation studies}). Thus, we introduce a novel injection strategy to leverage the cross-attention map $h_{cross}^{c_e}$ from $c_e$. Specifically, we normalize $h_{cross}^{c}$ using the mean and variance of $h_{cross}^{c_e}$ along the spatial dimensions. The formula is as follows,
\begin{align}
    \mu_c, \sigma_c = \operatorname{mean}(h_{cross}^{c}), &\; \operatorname{std}(h_{cross}^{c}), \quad \mu_{c_e}, \sigma_{c_e} = \operatorname{mean}(h_{cross}^{c_e}), \; \operatorname{std}(h_{cross}^{c_e}), \\
    &h_{cross}^{c} = \frac{h_{cross}^{c} - \mu_c}{\sigma_c} * \sigma_{c_e} + \mu_{c_e}. 
\end{align}
As for the self-attention map, we adopt direct replacement as $h_{self}^{c} = h_{self}^{c_e}$ because we found that the self-attention of UNet features affects identity-irrelevant attributes of the synthesized images. Experimental results demonstrate that the proposed attention injection module, which handles self and cross-attention differently, achieves identity preservation while further harnessing the inherent ability of diffusion model to enhance the diversity of the synthesized images.
\subsection{synthetic-based face recognition}
\label{synthetic dataset generation}
To generate synthetic dataset with non-existent identities, first we follow the previous method \citep{boutros2023idiff} to generate non-existent face images using an additional unconditional face generation model and extract their identity contexts using the pre-trained FR model \citep{Boutros_2022_CVPR}. Then we adopt the strategy from \citet{kim2023dcface} to filter out similar identity contexts using cosine distances to ensure inter-class discrepancy. Subsequently, we utilize these identity contexts as conditions to generate face dataset using our method. Last, we train new FR models on these synthetic data from scratch and report the final FR performance on real datasets.

\section{EXPERIMENTS}
\label{EXPERIMENTS}

\subsection{Experimental setup}
\label{Experimental setup}

\textbf{Training and testing dataset.} We train our diffusion model on CAISA-Webface \citep{yi2014learning} dataset containing about 500k face images of 10575 real identities of celebrities directly grabbed from the web, which show large variations of attributes such as pose, illumination, and facial expressions. We evaluate the final synthetic-based FR models on the five most commonly used real datasets, LFW \citep{huang2008labeled}, CFP-FP \citep{sengupta2016frontal}, CPLFW \citep{zheng2018cross}, AgeDB \citep{moschoglou2017agedb} and CALFW \citep{zheng2017cross}. These datasets include images with varying ages, poses, and facial expressions, which allow to comprehensively measure the generalization of FR models.

\textbf{Implementation details.}
Our diffusion model is trained using an Adam \citep{kingma2014adam} optimizer with a learning rate of 1e-4 and a batch size of 64. The training is conducted for a total of 250k iterations. During training process, we randomly replaced $c$ with the empty context $c_e$ at a probability of 20\%. As for training synthetic-based FR models, We adopt an IR50 as backbone with ArcFace loss \citep{deng2019arcface} for 40 epochs. The scale of classifier-free guidance is set to 1 and $th$ is 0.005. 

\input{table/table1.tex}

\subsection{Comparison with state-of-the-art methods}
In this subsection, we compare our method with other synthetic-based face recognition methods. The results are shown in Table \ref{tab:sota}. As shown in the table, our method significantly outperforms previous approaches, including the latest methods \citep{papantoniou2024arc2face, sun2024cemiface, li2024id} in the average metrics across the five datasets. Moreover, our method maintains superior performance even when synthesizing fewer than half the number of face images compared to previous methods. Notably, previous state-of-the-art method Arc2Face \citep{papantoniou2024arc2face} even uses more datasets and higher resolution images to train the generative model. We attribute the superiority of our method to our two-stage sampling strategy and attention injection mechanism, which can unleash the potential of diffusion model to synthesize images with enhanced diversity, thereby enabling the FR model to have better generalization capabilities on real datasets. By increasing the number of identities, our approach even outperforms the FR model trained on CAISA-Webface on the CPLFW dataset and achieves comparable performance in the average accuracy.


\subsection{Ablation studies}
\label{Ablation studies}
In this subsection we further investigate the effectiveness of each design of proposed \textbf{UIFace} in synthetic face recognition. The results are shown in Table \ref{tab:ablation}. 


\textbf{Impact of two-stage sampling.} We first build a baseline method based on the vanilla single-stage sampling strategy (baseline) for synthetic-based face recognition. To validate the effectiveness of proposed two-stage sampling, we first employ a naive two-stage sampling process based on fixed stage partition ($t_0=500$, 2-stage-fixed). As shown in Table \ref{tab:ablation}, the naive two-stage design with fixed partition has already demonstrated a significant performance improvement compared to the baseline in both the diversity of the generated dataset and the accuracy of the FR model. We attribute this to our observations in Section \ref{timesteps} so that such two-stage strategy can unleash the model's potential for intra-class-diversified image generation, leading to improved face recognition accuracy.



\textbf{Impact of adaptive partitioning strategy.} Moreover, we validate the effectiveness of the proposed adaptive partition by incorporating it into the baseline method (2-stage-adaptive). As shown in Table \ref{tab:ablation}, the proposed adaptive partitioning significantly outperforms the fixed partitioning method. Such adaptive partition strategy not only eliminates the need of manual hyperparameter tuning but also allows for the allocation of different stage boundaries for each sample, especially considering that the partition preferences may vary across different samples.



\begin{figure}[ht]
    \begin{center}
    \includegraphics[width=1\linewidth]{fig/attn/figure.pdf}
    \caption{
    visualization results of different attention injection strategies (\textbf{Top}: unconditional generation; \textbf{Middle}: attention injection with vanilla replacement; \textbf{Bottom}: the proposed attention inject). 
    }
    \label{fig:attn} 
    \end{center}
\end{figure} 

\begin{figure}[ht]
    \begin{center}
    \includegraphics[width=1\linewidth]{fig/discussion/figure_v2.pdf}
    \caption{
    Genuine and imposter comparisons. (\textbf{Left}: baseline; \textbf{Mid}: UIFace; \textbf{Right}: CASIA).
    }
    \label{fig:discussion} 
    \end{center}
\end{figure} 

\input{table/table2.tex}

\textbf{Impact of the proposed attention injection.} We first visualize the synthesized images using the proposed attention injection compared to images generated using vanilla attention map replacement. As shown in Figure \ref{fig:attn}, directly replacing the attention map will greatly degenerate the quality and identity of the synthesized images, while the proposed attention injection can effectively utilize the diversity from unconditional generation as well as maintain identity-preserving. Then we augment the baseline model with the proposed attention injection method (attn). A significant improvement can be observed in Figure \ref{tab:ablation}, in both the diversity of synthetic datasets and the accuracy of the FR model. This is because we enhance the intra-class variations with the model's inherent ability in unconditional diverse image generation. Combining our adaptive two-stage strategy and attention injection leads to the best diversity and performance improvement. 

\input{table/table5.tex}

\begin{figure}[ht]
    \begin{center}
    \includegraphics[width=1\linewidth]{fig/visualization/figure.pdf}
    \caption{
    Visualization results of IDiff-Face (\textbf{odd rows}) and UIFace (\textbf{even rows}).
    }
    \label{fig:visualization} 
    \end{center}
 \end{figure} 
 
\textbf{Discussion about identity consistency.} Intuitively, the two-stage sampling strategy proposed in this paper may reduce the intra-class identity similarity of the generated images since the empty context $c_e$ stands for "random identities", which is also of great significance in synthetic face recognition. In fact, there is a trade-off between intra-class diversity and intra-class identity consistency in this task. And our designs, including the discussion in Section \ref{timesteps}, the use of classifier-free guidance in the second stage (Section \ref{Two-stage}) and the different treatment of self and cross-attention maps in Section \ref{Attention injection}, have sought to maximize intra-class diversity while preserving identity consistency as much as possible. To demonstrate this, we conduct genuine and imposter comparisons, a common metric in face recognition that measures the similarity between data points of the same individual and different individuals. As shown in Figure \ref{fig:discussion} and Table \ref{tab:ablation}, our method has improved both intra-class identity similarity and style diversity compared to the baseline method. 

\subsection{UIFace with different diffusion sampling methods}
Since our motivation lies in different sampling preferences in different stages during inference, which is agnostic to any specific diffusion sampling method, our method can be applied to various diffusion-based generation methods. Here we choose FPNDM \citep{liu2022pseudo} as a representative. As demonstrated in Table\ref{tab:generalization}, UIFace achieves consistent improvements on both DDIM and FPNDM sampling methods, showing the effectiveness and generalization of our method. The baseline is the same as Section\ref{Ablation studies} and the numbers of synthetic images are all 0.5M.

\subsection{Qualitative results}
The qualitative results of IDiff-Face \citep{boutros2023idiff} and proposed UIFace are shown in Figure \ref{fig:visualization}. We randomly sample identity contexts and used both methods to generate synthetic face images. It is evident from the figure that the images generated by IDiff-Face exhibit a lack of variations in attributes such as expression, illumination and pose, which is caused by context overfitting as discussed in Section \ref{INTRODUCTION}. In contrast, our proposed \textbf{UIFace} employs a two-stage sampling strategy to unleash the model's inherent capability, thereby enhancing intra-class diversity in the generated images (more expression changes, facial rotations and lighting variations). 

\section{Conclusion}
We introduce \textbf{UIFace}, a novel synthetic face recognition framework that leverages a two-stage sampling strategy to unleash inherent model capability to enhance intra-class diversity of synthetic face dataset. Moreover, we propose an adaptive partitioning strategy and an attention injection method to further improve intra-class diversity while maintaining identity-preserving. Extensive experiments demonstrate that \textbf{UIFace} outperforms existing methods in multiple benchmarks even using less training data and fewer identity number. 

\newpage
\bibliography{iclr2025_conference}
\bibliographystyle{iclr2025_conference}

\end{document}
