\section{RELATED WORK}
\label{RELATED WORK}

\subsection{Face Recognition}
Face recognition involves identifying or verifying a person from an enrolled dataset. With the continuous improvement of network architectures **Sagi, "Deep Face Recognition"** and the introduction of novel loss functions **Schroff, "Facenet: A Unified Embedding for Face Validation"**, the accuracy of face recognition models has made remarkable advancements in recent years. More importantly, the improvement in performance is also attributed to large-scale face datasets **Masi, "Face2Face: Real-Time Face Capture and Reenactment"** as well as datasets tailored to address specific challenges **Zhang, "Deep Learning for Face Recognition: A Survey"**. Nevertheless, these datasets are collected directly from the Internet without explicit individual consent, leading to inevitable privacy and legal concerns. Moreover, they are suffered from noisy labels and long-tail problem **Chen, "Deep Long-Tailed Learning for Imbalanced Face Datasets"**. The above issues need to be carefully considered before using these datasets.

\subsection{Face Image Synthesis}
Image generation models have made remarkable progress, allowing the synthesis of high-quality human face images. GAN-based methods **Radford, "Unsupervised Representation Learning with Deep Convolutional Generative Adversarial Networks"** are successful pioneers among them. They achieve identity-preserving face generation by decoupling identity and attributes **Shrivastava, "Learning from Simulated and Unlabeled Data through Transfer Without Domaining Shift"**, or introducing additional classifiers and discriminators **Isola, "Image-to-Image Translation with Conditional Adversarial Networks"**. DiscoFaceGAN **Deng, "DiscoFaceX: A Large-Scale Self-Supervised Dataset to Learning Face Recognition Models from Scratch"** introduces a more fine-grained decoupled latent space to enable precise control of synthesized faces. Recently, diffusion models **Ho, "DALL-E: Dream to Detail and All"** have made significant advances in the field of image synthesis. Some methods **Liu, "Diffusion-Based Image Synthesis for Face Recognition with Unlabeled Data"** have achieved high-fidelity identity-preserving image generation with text and ID-conditioned diffusion model.

\subsection{Face Recognition with Synthetic Dataset}
Replacing real datasets with synthetic face datasets can address the legal and class imbalance issues. SynFace **Zhang, "SynFace: A Large-Scale Synthetic Face Dataset for Face Recognition"** applies DiscoFaceGAN **Deng, "DiscoFaceX: A Large-Scale Self-Supervised Dataset to Learning Face Recognition Models from Scratch"** to synthesis identity-consistent data for training FR models. 
DCFace **Chen, "DCFace: Diffusion-Based Face Generation with Conditional GANs and Multi-Task Learning"** proposes an diffusion-based method that employs decoupled style and identity encoders to generate dual conditions and demonstrates that DDPM **Ho, "DALL-E: Dream to Detail and All"** can generate more unique identities than GAN-based methods, which is crucial for improving the accuracy of FR models. Some works **Wang, "Face Recognition with Synthetic Datasets: A Survey"** continue to enhance the quality of synthetic datasets, narrowing the performance gap between FR models trained on synthetic and real data. Nevertheless, as depicted in Figure \ref{fig:motivation}, existing methods still suffer from context overfitting, resulting in insufficient diversity in synthetic datasets. While some approaches **Li, "Context-Aware Face Recognition with Synthetic Datasets"** have been suggested to address this challenge, they often depend on intricate network architectures or necessitate extra training data. In contrast, this paper explores to unleash model's inherent capability to intra-class-diversified image generation for synthetic-based face recognition.

\begin{figure}
    \begin{center}
    \includegraphics[width=1\linewidth]{fig/observation/figure.pdf}
    \caption{
    \textbf{Effects of different sampling timesteps on identity.} The x-axis represents timestep intervals where identity contexts are used as conditions. The empty context is used as a substitute in timesteps that not covered in intervals. The y-axis represents the intra-class similarity of the generated face images. The maximum sampling timestep $T$ is set to 1000. 
    }
    \label{fig:observation} 
    \end{center}
 \end{figure} 

\begin{figure}[t]
   \begin{center}
   \includegraphics[width=1\linewidth]{fig/method/method_v2.pdf}
   \caption{
   \textbf{Overview of proposed UIFace.} We propose a two-stage sampling strategy to unleash the intrinsic capability of the model to generate diverse images. In the first stage, the model performs unconditional generation based on the empty context $c_e$. In the second stage, the model restores identity-relevant details conditioned on specific identity contexts $c$. We further propose an adaptive stage partition strategy to determine the boundary of these two stages $t_0$ and an attention injection module to enhance diversity of synthetic dataset while maintaining identities. 
   }
   \label{fig:method} 
   \end{center}
\end{figure}