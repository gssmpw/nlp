\section{RELATED WORK}
\label{RELATED WORK}

\subsection{Face Recognition}
Face recognition involves identifying or verifying a person from an enrolled dataset. With the continuous improvement of network architectures \citep{he2016deep, boutros2022pocketnet, huang2017densely} and the introduction of novel loss functions \citep{Boutros_2022_CVPR, deng2019arcface, huang2020curricularface, wang2018cosface, Kim_2022_CVPR}, the accuracy of face recognition models has made remarkable advancements in recent years. More importantly, the improvement in performance is also attributed to large-scale face datasets \citep{huang2008labeled, cao2018vggface2, zhu2021webface260m, guo2016ms, kemelmacher2016megaface} as well as datasets tailored to address specific challenges \citep{zheng2018cross, zheng2017cross, sengupta2016frontal, moschoglou2017agedb}. Nevertheless, these datasets are collected directly from the Internet without explicit individual consent, leading to inevitable privacy and legal concerns. Moreover, they are suffered from noisy labels and long-tail problem \citep{yi2014learning}. The above issues need to be carefully considered before using these datasets.

\subsection{Face Image Synthesis}
Image generation models have made remarkable progress, allowing the synthesis of high-quality human face images. GAN-based methods \citep{brock2018large, choi2018stargan, karras2017progressive, karras2019style, karras2020analyzing, yin2017towards} are successful pioneers among them. They achieve identity-preserving face generation by decoupling identity and attributes \citep{bao2018towards}, or introducing additional classifiers and discriminators \citep{shen2018faceid}. DiscoFaceGAN \citep{deng2020disentangled} introduces a more fine-grained decoupled latent space to enable precise control of synthesized faces. Recently, diffusion models \citep{song2020denoising, ho2020denoising, dhariwal2021diffusion, song2020score, rombach2022high} have made significant advances in the field of image synthesis. Some methods \citep{zhang2024flashface, wang2024instantid, li2024photomaker} have achieved high-fidelity identity-preserving image generation with text and ID-conditioned diffusion model.

\subsection{Face Recognition with Synthetic Dataset}
Replacing real datasets with synthetic face datasets can address the legal and class imbalance issues. SynFace \citep{qiu2021synface} applies DiscoFaceGAN \citep{deng2020disentangled} to synthesis identity-consistent data for training FR models. 
DCFace \citep{kim2023dcface} proposes an diffusion-based method that employs decoupled style and identity encoders to generate dual conditions and demonstrates that DDPM \citep{ho2020denoising} can generate more unique identities than GAN-based methods, which is crucial for improving the accuracy of FR models. Some works \citep{boutros2023idiff, papantoniou2024arc2face} continue to enhance the quality of synthetic datasets, narrowing the performance gap between FR models trained on synthetic and real data. Nevertheless, as depicted in Figure \ref{fig:motivation}, existing methods still suffer from context overfitting, resulting in insufficient diversity in synthetic datasets. While some approaches \citep{boutros2023idiff,kim2023dcface,he2024imagineyourself} have been suggested to address this challenge, they often depend on intricate network architectures or necessitate extra training data. In contrast, this paper explores to unleash model's inherent capability to intra-class-diversified image generation for synthetic-based face recognition.

\begin{figure}
    \begin{center}
    \includegraphics[width=1\linewidth]{fig/observation/figure.pdf}
    \caption{
    \textbf{Effects of different sampling timesteps on identity.} The x-axis represents timestep intervals where identity contexts are used as conditions. The empty context is used as a substitute in timesteps that not covered in intervals. The y-axis represents the intra-class similarity of the generated face images. The maximum sampling timestep $T$ is set to 1000. 
    }
    \label{fig:observation} 
    \end{center}
 \end{figure} 

\begin{figure}[t]
   \begin{center}
   \includegraphics[width=1\linewidth]{fig/method/method_v2.pdf}
   \caption{
   \textbf{Overview of proposed UIFace.} We propose a two-stage sampling strategy to unleash the intrinsic capability of the model to generate diverse images. In the first stage, the model performs unconditional generation based on the empty context $c_e$. In the second stage, the model restores identity-relevant details conditioned on specific identity contexts $c$. We further propose an adaptive stage partition strategy to determine the boundary of these two stages $t_0$ and an attention injection module to enhance diversity of synthetic dataset while maintaining identities. 
   }
   \label{fig:method} 
   \end{center}
\end{figure}