
\documentclass{article} % For LaTeX2e
\usepackage{iclr2025_conference,times}

% Optional math commands from https://github.com/goodfeli/dlbook_notation.
\input{math_commands.tex}
\usepackage{booktabs}
\usepackage{hyperref}
\usepackage{url}
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{geometry}
\usepackage{graphicx} 
\usepackage{float}

\title{The Empirical Impact of Reducing Symmetries on the Performance of Deep Ensembles and MoE}

% Authors must not appear in the submitted version. They should be hidden
% as long as the \iclrfinalcopy macro remains commented out below.
% Non-anonymous submissions will be rejected without review.

\author{Andrei Chernov \\
Independent Researcher \\
\texttt{chernov.andrey.998@gmail.com} \\
\And
Oleg Novitskij  \\
HSE University \\
\texttt{oanovitskii@edu.hse.ru} \\
% \AND
% Coauthor \\
% Affiliation \\
% Address \\
% \texttt{email}
}

% The \author macro works with any number of authors. There are two commands
% used to separate the names and addresses of multiple authors: \And and \AND.
%
% Using \And between authors leaves it to \LaTeX{} to determine where to break
% the lines. Using \AND forces a linebreak at that point. So, if \LaTeX{}
% puts 3 of 4 authors names on the first line, and the last on the second
% line, try using \AND instead of \And before the third author name.

\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}

%TODO: do not forget to change sty!
\iclrfinalcopy % Uncomment for camera-ready version, but NOT for submission.
\begin{document}


\maketitle

\begin{abstract}
Recent studies have shown that reducing symmetries in neural networks enhances linear mode connectivity between networks without requiring parameter space alignment, leading to improved performance in linearly interpolated neural networks. However, in practical applications, neural network interpolation is rarely used; instead, ensembles of networks are more common. In this paper, we empirically investigate the impact of reducing symmetries on the performance of deep ensembles and Mixture of Experts (MoE) across five datasets. Additionally, to explore deeper linear mode connectivity, we introduce the Mixture of Interpolated Experts (MoIE). Our results show that deep ensembles built on asymmetric neural networks achieve significantly better performance as ensemble size increases compared to their symmetric counterparts. In contrast, our experiments do not provide conclusive evidence on whether reducing symmetries affects both MoE and MoIE architectures. The code is available on GitHub\footnote{\url{https://github.com/krds00/asym_ensembles/}}.
\end{abstract}

\section{Introduction}
In the last decade, neural networks have proven to be one of the most important algorithms in the field of machine learning. Despite their undeniable empirical success, many fundamental questions remain unanswered. One such question concerns parameter space symmetries: for any given set of neural network parameters, there exist numerous ‘twins’ that produce exactly the same output for every input while having different parameter values.

There are multiple sources of symmetry in neural network architectures. One prominent example is permutation symmetry in fully connected layers. Consider a standard multi-layer perceptron (MLP). If we swap two neurons in a hidden layer along with their incoming and outgoing weights, the network will produce the exact same output. As a result, any hidden layer of size $n$ has $n!$ different sets of parameters that yield identical outputs. Activation functions such as ReLU \cite{nair2010rectified} can also produce symmetries \cite{wiese2023towards}.

The effects of parameter symmetries have been studied in various areas, including neuron interpretability \cite{godfrey2022symmetries}, optimization \cite{neyshabur2015path}, and Bayesian deep learning \cite{kurle2022detrimental}. In this work, we primarily focus on the impact of symmetries on model accuracy. It has been shown in \cite{lim2024empirical} that eliminating redundant parameters in neural networks improves linear mode connectivity, thereby enhancing the performance of networks whose parameters are obtained by interpolating between two trained models. In this study, we present the Mixture of Interpolated Experts model (see Section \ref{sec:MoIE} for details) and investigate how parameter symmetries influence its performance.

However, the practical application of interpolated neural networks remains controversial, as such architectures usually do not provide a performance boost. The most common approach to leveraging multiple neural networks is through ensembles, where outputs from several models are aggregated to form a final prediction—examples include Deep Ensembles \cite{lakshminarayanan2017simple} and Mixture of Experts (MoE) \cite{fedus2022review}. In this paper, we empirically investigate how reducing symmetry impacts the performance of Deep Ensembles and MoE on $5$ datasets.

\section{Related Work}

\subsection{W-Asymmetric MLP}
Various methods for breaking parameter symmetries in neural networks have been studied, including approaches to removing permutation symmetries \cite{pourzanjani2017improving, pittorino2022deep}, scaling symmetries \cite{badrinarayanan2015understanding}, and sign symmetries \cite{wiese2023towards}. However, in most of these approaches, the neural network architectures or training processes deviate from standard practices, making them difficult to apply in practice. In this work, we fully adopt the approach from \cite{lim2024empirical} to break symmetries in neural networks. This method randomly freezes a portion of the neural network’s weights before training, keeping them unchanged throughout training (see Section \ref{sec:wmlp} for details). Notably, it does not require any special modifications to the training process. Authors of \cite{lim2024empirical} showed that breaking symmetries improves linear mode connectivity between two independently trained neural networks. In this paper, we investigate the empirical impact of reducing symmetries on the performance of Deep Ensembles and Mixture of Experts.

\subsection{Neural Network Ensembles}
In this study, we employ two different approaches for ensembling neural networks. The first approach, known as Deep Ensembles \cite{lakshminarayanan2017simple}, trains $k$ neural networks independently and averages their outputs to obtain the final prediction.

The second approach is the Mixture of Experts (MoE) \cite{yuksel2012twenty}, which consists of two main components: experts and a gating network. Each expert generates an output, but unlike Deep Ensembles, the final prediction is obtained through a weighted average of the experts' outputs. The weights for each expert are dynamically predicted by the gating network rather than being fixed. Recently, MoE architectures utilizing MLP models as experts have gained popularity \cite{fedus2022review} especially in NLP \cite{du2022glam}
and CV domains \cite{puigcerver2023sparse,
riquelme2021scaling}. In this work, we adapt MoE architectures for tabular data from \cite{chernov2025moe}. We cover it in detail in Section \ref{sec:MoE}.

\section{Datasets}

For our work, we selected five datasets to cover different problems:  
\begin{itemize}
    \item \textbf{Regression:} California Housing Prices dataset \cite{pace1997sparse}.
    \item \textbf{Binary classification:} Churn Modeling\footnote{https://www.kaggle.com/shrutimechlearn/churn-modelling} and Adult Income \cite{kohavi1996scaling}.
    \item \textbf{Multi-class classification:} MNIST \cite{deng2012mnist} and Otto Group Product\footnote{https://www.kaggle.com/c/otto-group-product-classification-challenge/data}.
\end{itemize}

Appendix \ref{app:datasets} summarizes the key attributes of these datasets. To ensure consistency, we applied a standardized preprocessing pipeline. Each dataset was split into training, validation, and testing sets with an overall partitioning of 64\% for training, 16\% for validation, and 20\% for testing. Real-valued features were scaled using a \texttt{StandardScaler}, and for classification tasks, the splits were stratified by the target variable. Additional preprocessing steps were applied to each dataset as follows:
\begin{itemize}
    \item \textbf{Churn Modeling dataset:} Non-informative columns such as \texttt{RowNumber}, \texttt{CustomerId}, and \texttt{Surname} were removed.
    \item \textbf{Otto Group Product dataset:} The \texttt{id} column was dropped, and the target variable was encoded using a \texttt{LabelEncoder}.
    \item \textbf{Adult Income dataset:} The target variable was transformed by mapping \texttt{<=\$50K} to $0$ and \texttt{>\$50K} to $1$. Categorical features were processed using a \texttt{OneHotEncoder}, with missing values imputed as \texttt{MissingValue}, while numerical missing values were filled with $0$.
    \item \textbf{California Housing Prices dataset:} Since this dataset contains no missing values, its numerical features were simply scaled.
    \item \textbf{MNIST dataset:} Grayscale images were preprocessed by normalizing and centering pixel values.
\end{itemize}.  

\section{Models}

\subsection{W-Asymmetric MLP}
\label{sec:wmlp}

In this paper, we fully adopt the implementation of W-Asymmetric MLP (WMLP) from \cite{lim2024empirical}, where it was theoretically proven that this approach significantly reduces parameter symmetries. This is achieved by freezing a small portion of the weights, approximately $\mathcal{O}(n^{1/4})$ for details see Algorithm \ref{alg:WMLP}

It is important to emphasize that in ensemble networks utilizing different WMLP models, the frozen neurons—both in value and position—remain identical across all instances. For the hidden layers, we use the GeLU activation function from \cite{hendrycks2016gaussian} in both MLP and WMLP.

\begin{algorithm}[t]
\caption{WMLP Weight and Bias Initialization with Masking}
\label{alg:WMLP}
\begin{algorithmic}[1]
\Require Number of layers \(L = 4\), hidden dimension \(d \in \{64,128,256\}\), and mask seeding parameter \(\mathtt{mask\_num}\).
\State \textbf{Define fixed weights per output unit:}
\[
n_{\text{fix}}^{(1)} = 2,\quad
n_{\text{fix}}^{(l)} =
\begin{cases}
4, & \text{if } d = 256, \\
3, & \text{otherwise}
\end{cases} \quad \text{for } l > 1.
\]
\For{\(l = 1,\dots,L\)}
    \State Let \(W^{(l)} \in \mathbb{R}^{\text{out}_l \times \text{in}_l}\) be the weight matrix.
    \For{\(i = 1,\dots,\text{out}_l\)}
        \State \textbf{Generate Mask:}
        \State For each output unit, select a random subset of \(n_{\text{fix}}^{(l)}\) input indices to  be fixed.The fixed \State positions are determined using a seed based on \(l\) and the output unit index, ensuring \State reproducibility within an ensemble.
        \For{\(j = 1,\dots,\text{in}_l\)}
            \If{\(j\) is in the fixed subset for unit \(i\)}
                \State Set \(W^{(l)}_{ij} \sim \mathcal{N}(0,1)\). The random seed for fixed weights depends on the layer \(l\) and \State weight position. These weights are then frozen.
            \Else
                \State Initialize \(W^{(l)}_{ij}\) using Kaiming Uniform Initialization with parameter \(\sqrt{5}\). The random \State seed for non-frozen weights depends on the repetition number, the estimator index in the \State ensemble, \(l\), and the weight's position.
            \EndIf
        \EndFor
    \EndFor
    \State \textbf{Initialize Bias:} \\
    \hspace*{1em} Set \(b^{(l)}\) uniformly in 
    \[
      \left[-\frac{1}{\sqrt{\text{in}_l}},\ \frac{1}{\sqrt{\text{in}_l}}\right],
    \]
    \Comment{The random seed for bias initialization depends on the repetition number, the estimator index, \(l\), and the bias position.}
\EndFor
\end{algorithmic}
\end{algorithm}

\subsection{Mixture of Experts}
\label{sec:MoE}

In \cite{chernov2025moe}, it was shown that MoE performs at least as well as a vanilla MLP on tabular data while requiring significantly fewer parameters. In this paper, we compare the performance of MoE with MLP as experts against MoE with WMLP experts. 

From \cite{chernov2025moe}, we utilize both the vanilla MoE, where logistic regression is used as a gating neural network, and the Gumbel Gating MoE (GG MoE), which employs the Gumbel-softmax function instead of the standard Softmax activation for logistic regression. Following the original paper, we use $10$ samples from the Gumbel-softmax distribution during inference.

\subsection{Mixture of Interpolated Experts}
\label{sec:MoIE}

Since \cite{lim2024empirical} demonstrated that reducing symmetries improves the performance of linearly interpolated neural networks, we evaluate the performance of the Mixture of Interpolated Experts (MOIE). MOIE uses the same gating function as MoE but, instead of computing a weighted average of the final outputs, it linearly interpolates the weights of the experts to produce an output:

$$\hat{y} = \text{Expert architecture} \left( \sum_{i}^{k} \alpha_i(x) W_i(x) \right),$$

where $\hat{y}$ is the final prediction, $k$ is the number of experts, $\alpha$ is an output from a gating network, and $W_i$ represents the model parameters of each expert. The expert architecture is selected from ${\text{MLP}, \text{WMLP}}$.

\section{Experiments}
\label{Experiments}

\subsection{Setup}
In this section, we describe the details of the training and evaluation procedures applied to Deep Ensembles (Section \ref{Deep ensemble}), MoE and MoIE (Section \ref{MOE MOIE}).

\subsubsection{Deep Ensemble}
\label{Deep ensemble}
We trained models with a batch size of $256$. For constructing Deep Ensembles, we trained $64$ instances of both the MLP and WMLP models, each initialized with a different random seed to ensure variability in the free weights. 

For WMLP, a fixed number of random weights per row, denoted as \(n_{\text{fix}}\), was selected and frozen in each layer. These frozen weights were sampled from a \(\mathcal{N}(0,I)\) distribution. To reduce variance in the final metrics, we repeated training and evaluation $10$ times independently and reported the average evaluation metrics on the test sets.

For Deep Ensembles, we utilized MLP and WMLP blocks. Their structure consisted of an input layer that mapped the number of dataset features to a hidden dimension (\textit{hidden\_dim}), followed by two hidden layers of size \textit{hidden\_dim} \(\times\) \textit{hidden\_dim}, and an output layer of size \textit{hidden\_dim} \(\times\) \textit{out\_features}, where \textit{out\_features} was set to $1$ for regression and to the number of classes for classification. Experiments for Deep Ensembles were conducted for \textit{hidden\_dim} values of $64$, $128$, and $256$.

Loss functions were selected based on the task: \texttt{MSELoss} for regression and \texttt{CrossEntropyLoss} for classification, with \texttt{RMSE} and \texttt{accuracy} serving as the evaluation metrics, respectively. Optimization was carried out using the AdamW optimizer with a learning rate of \(1\times10^{-3}\) and a weight decay of \(3\times10^{-2}\). Each network was trained for up to $1000$ epochs, with $batch\_size=256$ and early stopping triggered if the validation loss did not improve for $16$ consecutive epochs. Training was performed in parallel on 64 CPUs. After each training iteration, we logged the training time, the number of epochs executed, and the performance metric for each of the $64$ MLP and WMLP models. Finally, the individual models were aggregated into Deep Ensembles of $2$, $4$, $8$, $16$, $32$, and $64$ networks. For regression tasks, ensemble predictions were computed as the mean of the individual outputs, while for classification tasks the logits were averaged and the final prediction was determined via the \texttt{argmax} function. For each ensemble, both the ensemble performance metric and an interpolation metric—derived from averaging the model weights—were recorded.

\subsubsection{MoE and MOIE}
\label{MOE MOIE}
In experiments with MoE and MoIE, we used both MLP and WMLP architectures, along with the same loss functions, evaluation metrics, training procedures, and optimizer parameters as described in Section \ref{Deep ensemble}. For these experiments, the expert hidden dimension was fixed at $64$. In the WMLP architecture, the number of fixed weights per output unit, \(n_{\text{fix}}\), was set to $2$ for the input layer and $3$ for subsequent layers. The number of experts was varied among \([2,\,4,\,8,\,16,\,32,\,64]\). We conducted experiments for all models described in Sections \ref{sec:MoE} and \ref{sec:MoIE}.

\subsection{Results}
\label{results}

Figure \ref{fig:deep_relative} presents the experimental results, showing the average performance improvements in test metrics across different random seeds. Specifically, we report accuracy for classification tasks and RMSE for regression tasks, measuring improvement relative to the average performance of a single neural network. The results are presented for each dataset and hidden dimension configuration and indicate that Deep Ensembles with WMLP models improve significantly more than with MLP models and this improvement increases as the ensemble size increases.

A possible explanation for this behavior could be that WMLP deep ensembles perform worse than MLP ensembles in terms of absolute test metric values. However, this is not the case, as demonstrated in Appendix \ref{app:performance}. Given that WMLP models retain the universal approximation property, as shown in \cite{lim2024empirical}, we believe this is a promising finding that could encourage the adoption of asymmetric neural networks in ensembles for practical applications.  

Likewise, Figure \ref{fig:moe_relative} shows the relative performance of MoE and MoIE with a varying number of experts compared to their corresponding models with two experts. As discussed in Section \ref{MOE MOIE}, the hidden size of each expert remains constant. Although MoE with WMLP experts and MoIE with WMLP experts tend to outperform their counterparts with MLP experts on $4$ out of $5$ datasets, the improvements are less convincing compared to Deep Ensembles. We also report absolute metrics in Appendix \ref{app:performance}.

One potential reason for unclear results in MoE and MoIE might be that the models tend to overfit, meaning that as the number of parameters increases, test metrics deteriorate. We did not apply any regularization techniques to the neural network architectures to avoid overcomplicating the analysis. However, addressing this issue is essential, and the experimental setup for MoE and MoIE should be adjusted in future work.

\section{Conclusion}
In this paper, we empirically demonstrated that the performance of Deep Ensembles improves significantly with increasing ensemble size when using W-Asymmetric MLP models compared to vanilla MLP models. This result may serve as a first step toward understanding the practical impact of reducing symmetry in neural networks.

However, based on our experiments, we cannot conclude that W-Asymmetric MLP improves the performance of either the Mixture of Experts (MoE) or the Mixture of Interpolated Experts (MoIE) models. As discussed in Section \ref{results}, the experimental setup for MoE should be refined in future work
\newpage

% Figure 5: Deep Ensemble Relative Improvement (All Datasets)
\begin{figure}[H]
  \begin{center}
    \includegraphics[width=\linewidth, height=0.8\textheight, keepaspectratio]{figures/deep_ensemble_relative_improvement_all.pdf}
  \end{center}
  \vspace{1em}
  \caption{Deep ensembles’ relative improvement in performance. The graphics depicts the relative improvement in performance of both MLP and WMLP models compared to a single MLP and WMLP neural network, respectively.}
  \label{fig:deep_relative}
\end{figure}

% Figure 6: MoE Relative Improvement (All Datasets)
\begin{figure}[H]
  \begin{center}
    \includegraphics[width=\linewidth, height=0.8\textheight, keepaspectratio]{figures/moe_relative_improvement_all.pdf}
  \end{center}
  \vspace{1em}
  \caption{MoE and MoIE relative improvement. In these graphics, MLP represents MoE with vanilla MLP experts, WMLP denotes MoE with WMLP experts, IMLP corresponds to MoIE with vanilla MLP experts, and IWMLP refers to MoIE with WMLP experts. The relative improvement of all models is shown in comparison to their corresponding model architectures with two experts.}
  \label{fig:moe_relative}
\end{figure}

\bibliography{./iclr2025_conference}
\bibliographystyle{iclr2025_conference}

\appendix
\section{Appendix}
\subsection{Summary of Datasets}
\label{app:datasets}
Table \ref{table:datasets} provides a summary of the datasets used in this paper.




\begin{table}[ht]
\centering
\caption{Datasets description}
\label{table:datasets}
\resizebox{\textwidth}{!}{%
\begin{tabular}{@{}p{2.5cm}p{2.5cm}p{2cm}p{3.5cm}p{2.5cm}@{}}
\toprule
\textbf{Dataset} & \textbf{Task} & \textbf{Instances} & \textbf{Feature Details} & \textbf{Target Variable}  \\ \midrule
Churn Modelling & Binary Classification & 10,000 & Customer attributes (e.g., CreditScore, flagGeography, Gender, Age, Tenure, Balance, NumOfProducts) & Customer churn (Exited: yes/no) \\[1ex]
Otto Group Product & Multi-class Classification & 61,878 & 93 anonymized numerical features (feat\_1 to feat\_93) & Product category (9 classes) \\[1ex]
Adult Income & Binary Classification & 48,842 & Mixed continuous and categorical variables (e.g., age, workclass, education, occupation, etc.) & Income level (``\texttt{>50K}'' as 1, ``\texttt{<=50K}'' as 0) \\[1ex]
California Housing Prices & Regression & 20,640 & 8 numerical predictors (MedInc, HouseAge, AveRooms, AveBedrms, Population, AveOccup, Latitude, Longitude) & Median house value (in \$100K units) \\[1ex]
MNIST & Multi-class Classification & 70,000 & Preprocessed rectified into a single vector 28$\times$28 pixel grayscale images (784 features per image) & Handwritten digit (10 classes: 0--9) \\ \bottomrule
\end{tabular}
}
\end{table}


\subsection{Absolute Metrics for all Models on Test Dataset}
\label{app:performance}

Figures~\ref{fig:deep_absolute} and~\ref{fig:moe_absolute} present the absolute results of the experiments described in Section~\ref{Experiments}. In Figure~\ref{fig:deep_absolute}, the change of the relevant metric for deep ensembles using both MLP and WMLP models is shown as a function of ensemble size. The bold lines indicate the mean performance across different random seeds, while the shaded regions represent the $\pm$ one standard deviation intervals. Additionally, the figure displays the mean metric values and intervals for a baseline, which were calculated by aggregating the test metrics of $64$ single MLP and $64$ single WMLP models; these baseline values were subsequently used in Figure~\ref{fig:deep_relative}. It can be observed that the metric improves as the ensemble size increases. Notably, although WMLP models may yield inferior performance when used individually, the WMLP ensemble tends to outperform the MLP ensemble.

% Figure 3: Deep ensemble absolute metrics
\begin{figure}[H]
  \begin{center}
    \includegraphics[width=\linewidth]{figures/deep_ensemble_absolute_composite.pdf}
  \end{center}
  \vspace{1em}
  \caption{Deep ensemble absolute metrics.}
  \label{fig:deep_absolute}
\end{figure}


Similarly, Figure~\ref{fig:moe_absolute} illustrates the change of the corresponding metric for MoE and MoIE models as a function of the number of experts. Analogous to Figure~\ref{fig:deep_absolute}, the plot shows the mean performance along with the $\pm$ standard deviation intervals obtained from aggregating results over various random seeds. The baseline values corresponding to the case of two experts were used to compute the relative improvements shown in Figure~\ref{fig:moe_relative}. It is evident that the use of Gumbel-softmax leads to better performance compared to the standard softmax, and, in most cases, MoE with WMLP experts or MoIE with WMLP (IWMLP) achieves higher quality than MoE with MLP experts or MoIE with MLP/WMLP, respectively.

% Figure 4: MoE/MoIE absolute metrics
\begin{figure}[H]
  \begin{center}
    \includegraphics[width=\linewidth]{figures/moe_absolute_composite.pdf}
  \end{center}
  \vspace{1em}
  \caption{MoE/MoIE absolute metrics.}
  \label{fig:moe_absolute}
\end{figure}
\end{document}

% \subsection{Models}
% mlp vs wmlp; describe the hidden_size, etc.



% \section{Results}
% TBD.
% \section{Conclusion}

% \section{Submission of papers to ICLR Workshop on Neural Network Weights as a New Data Modality 2025}

The workshop requires electronic submissions, see the workshop's website for more instructions \url{https://weight-space-learning.github.io/cfp/}. 

If your paper is ultimately accepted, the statement {\tt
  {\textbackslash}iclrfinalcopy} should be inserted to adjust the
format to the camera ready requirements.

The format for the submissions is a variant of the NeurIPS format.
Please read carefully the instructions below, and follow them
faithfully. 
The workshop follows the ICLR 2025 conference's formatting instructions.

\subsection{Style}

Papers to be submitted to ICLR Workshop on Neural Network Weights as a New Data Modality 2025 must be prepared according to the
instructions presented here.

%% Please note that we have introduced automatic line number generation
%% into the style file for \LaTeXe. This is to help reviewers
%% refer to specific lines of the paper when they make their comments. Please do
%% NOT refer to these line numbers in your paper as they will be removed from the
%% style file for the final version of accepted papers.

Authors are required to use the ICLR Workshop on Neural Network Weights as a New Data Modality \LaTeX{} style files obtainable at the
workshop website. Please make sure you use the current files and
not previous versions. Tweaking the style files may be grounds for rejection.

\subsection{Retrieval of style files}

The style files for ICLR and other conference information are available online at:
\begin{center}
   \url{https://weight-space-learning.github.io/}
\end{center}
Submissions must be made using \LaTeX{} and the style files
\verb+iclr2025_conference.sty+ and \verb+iclr2025_conference.bst+ (to be used with \LaTeX{}2e). The file
\verb+iclr2025_conference.tex+ may be used as a ``shell'' for writing your paper. All you
have to do is replace the author, title, abstract, and text of the paper with
your own.

The formatting instructions contained in these style files are summarized in
sections \ref{gen_inst}, \ref{headings}, and \ref{others} below.

\section{General formatting instructions}
\label{gen_inst}

The text must be confined within a rectangle 5.5~inches (33~picas) wide and
9~inches (54~picas) long. The left margin is 1.5~inch (9~picas).
Use 10~point type with a vertical spacing of 11~points. Times New Roman is the
preferred typeface throughout. Paragraphs are separated by 1/2~line space,
with no indentation.

Paper title is 17~point, in small caps and left-aligned.
All pages should start at 1~inch (6~picas) from the top of the page.

Authors' names are
set in boldface, and each name is placed above its corresponding
address. The lead author's name is to be listed first, and
the co-authors' names are set to follow. Authors sharing the
same address can be on the same line.

Please pay special attention to the instructions in section \ref{others}
regarding figures, tables, acknowledgments, and references.


There will be a strict upper limit for the main text of the initial submission, with unlimited additional pages for citations. 
Please note that the workshop has two tracks. Full papers have a page limit of 8 pages, while extended abstracts (tiny papers track) have a page limit of 4 pages.

\section{Headings: first level}
\label{headings}

First level headings are in small caps,
flush left and in point size 12. One line space before the first level
heading and 1/2~line space after the first level heading.

\subsection{Headings: second level}

Second level headings are in small caps,
flush left and in point size 10. One line space before the second level
heading and 1/2~line space after the second level heading.

\subsubsection{Headings: third level}

Third level headings are in small caps,
flush left and in point size 10. One line space before the third level
heading and 1/2~line space after the third level heading.

\section{Citations, figures, tables, references}
\label{others}

These instructions apply to everyone, regardless of the formatter being used.

\subsection{Citations within the text}

Citations within the text should be based on the \texttt{natbib} package
and include the authors' last names and year (with the ``et~al.'' construct
for more than two authors). When the authors or the publication are
included in the sentence, the citation should not be in parenthesis using \verb|\citet{}| (as
in ``See \citet{Hinton06} for more information.''). Otherwise, the citation
should be in parenthesis using \verb|\citep{}| (as in ``Deep learning shows promise to make progress
towards AI~\citep{Bengio+chapter2007}.'').

The corresponding references are to be listed in alphabetical order of
authors, in the \textsc{References} section. As to the format of the
references themselves, any style is acceptable as long as it is used
consistently.

\subsection{Footnotes}

Indicate footnotes with a number\footnote{Sample of the first footnote} in the
text. Place the footnotes at the bottom of the page on which they appear.
Precede the footnote with a horizontal rule of 2~inches
(12~picas).\footnote{Sample of the second footnote}

\subsection{Figures}

All artwork must be neat, clean, and legible. Lines should be dark
enough for purposes of reproduction; art work should not be
hand-drawn. The figure number and caption always appear after the
figure. Place one line space before the figure caption, and one line
space after the figure. The figure caption is lower case (except for
first word and proper nouns); figures are numbered consecutively.

Make sure the figure caption does not get separated from the figure.
Leave sufficient space to avoid splitting the figure and figure caption.

You may use color figures.
However, it is best for the
figure captions and the paper body to make sense if the paper is printed
either in black/white or in color.
\begin{figure}[h]
\begin{center}
%\framebox[4.0in]{$\;$}
\fbox{\rule[-.5cm]{0cm}{4cm} \rule[-.5cm]{4cm}{0cm}}
\end{center}
\caption{Sample figure caption.}
\end{figure}

\subsection{Tables}

All tables must be centered, neat, clean and legible. Do not use hand-drawn
tables. The table number and title always appear before the table. See
Table~\ref{sample-table}.

Place one line space before the table title, one line space after the table
title, and one line space after the table. The table title must be lower case
(except for first word and proper nouns); tables are numbered consecutively.

\begin{table}[t]
\caption{Sample table title}
\label{sample-table}
\begin{center}
\begin{tabular}{ll}
\multicolumn{1}{c}{\bf PART}  &\multicolumn{1}{c}{\bf DESCRIPTION}
\\ \hline \\
Dendrite         &Input terminal \\
Axon             &Output terminal \\
Soma             &Cell body (contains cell nucleus) \\
\end{tabular}
\end{center}
\end{table}

\section{Default Notation}

In an attempt to encourage standardized notation, we have included the
notation file from the textbook, \textit{Deep Learning}
\cite{goodfellow2016deep} available at
\url{https://github.com/goodfeli/dlbook_notation/}.  Use of this style
is not required and can be disabled by commenting out
\texttt{math\_commands.tex}.


\centerline{\bf Numbers and Arrays}
\bgroup
\def\arraystretch{1.5}
\begin{tabular}{p{1in}p{3.25in}}
$\displaystyle a$ & A scalar (integer or real)\\
$\displaystyle \va$ & A vector\\
$\displaystyle \mA$ & A matrix\\
$\displaystyle \tA$ & A tensor\\
$\displaystyle \mI_n$ & Identity matrix with $n$ rows and $n$ columns\\
$\displaystyle \mI$ & Identity matrix with dimensionality implied by context\\
$\displaystyle \ve^{(i)}$ & Standard basis vector $[0,\dots,0,1,0,\dots,0]$ with a 1 at position $i$\\
$\displaystyle \text{diag}(\va)$ & A square, diagonal matrix with diagonal entries given by $\va$\\
$\displaystyle \ra$ & A scalar random variable\\
$\displaystyle \rva$ & A vector-valued random variable\\
$\displaystyle \rmA$ & A matrix-valued random variable\\
\end{tabular}
\egroup
\vspace{0.25cm}

\centerline{\bf Sets and Graphs}
\bgroup
\def\arraystretch{1.5}

\begin{tabular}{p{1.25in}p{3.25in}}
$\displaystyle \sA$ & A set\\
$\displaystyle \R$ & The set of real numbers \\
$\displaystyle \{0, 1\}$ & The set containing 0 and 1 \\
$\displaystyle \{0, 1, \dots, n \}$ & The set of all integers between $0$ and $n$\\
$\displaystyle [a, b]$ & The real interval including $a$ and $b$\\
$\displaystyle (a, b]$ & The real interval excluding $a$ but including $b$\\
$\displaystyle \sA \backslash \sB$ & Set subtraction, i.e., the set containing the elements of $\sA$ that are not in $\sB$\\
$\displaystyle \gG$ & A graph\\
$\displaystyle \parents_\gG(\ervx_i)$ & The parents of $\ervx_i$ in $\gG$
\end{tabular}
\vspace{0.25cm}


\centerline{\bf Indexing}
\bgroup
\def\arraystretch{1.5}

\begin{tabular}{p{1.25in}p{3.25in}}
$\displaystyle \eva_i$ & Element $i$ of vector $\va$, with indexing starting at 1 \\
$\displaystyle \eva_{-i}$ & All elements of vector $\va$ except for element $i$ \\
$\displaystyle \emA_{i,j}$ & Element $i, j$ of matrix $\mA$ \\
$\displaystyle \mA_{i, :}$ & Row $i$ of matrix $\mA$ \\
$\displaystyle \mA_{:, i}$ & Column $i$ of matrix $\mA$ \\
$\displaystyle \etA_{i, j, k}$ & Element $(i, j, k)$ of a 3-D tensor $\tA$\\
$\displaystyle \tA_{:, :, i}$ & 2-D slice of a 3-D tensor\\
$\displaystyle \erva_i$ & Element $i$ of the random vector $\rva$ \\
\end{tabular}
\egroup
\vspace{0.25cm}


\centerline{\bf Calculus}
\bgroup
\def\arraystretch{1.5}
\begin{tabular}{p{1.25in}p{3.25in}}
% NOTE: the [2ex] on the next line adds extra height to that row of the table.
% Without that command, the fraction on the first line is too tall and collides
% with the fraction on the second line.
$\displaystyle\frac{d y} {d x}$ & Derivative of $y$ with respect to $x$\\ [2ex]
$\displaystyle \frac{\partial y} {\partial x} $ & Partial derivative of $y$ with respect to $x$ \\
$\displaystyle \nabla_\vx y $ & Gradient of $y$ with respect to $\vx$ \\
$\displaystyle \nabla_\mX y $ & Matrix derivatives of $y$ with respect to $\mX$ \\
$\displaystyle \nabla_\tX y $ & Tensor containing derivatives of $y$ with respect to $\tX$ \\
$\displaystyle \frac{\partial f}{\partial \vx} $ & Jacobian matrix $\mJ \in \R^{m\times n}$ of $f: \R^n \rightarrow \R^m$\\
$\displaystyle \nabla_\vx^2 f(\vx)\text{ or }\mH( f)(\vx)$ & The Hessian matrix of $f$ at input point $\vx$\\
$\displaystyle \int f(\vx) d\vx $ & Definite integral over the entire domain of $\vx$ \\
$\displaystyle \int_\sS f(\vx) d\vx$ & Definite integral with respect to $\vx$ over the set $\sS$ \\
\end{tabular}
\egroup
\vspace{0.25cm}

\centerline{\bf Probability and Information Theory}
\bgroup
\def\arraystretch{1.5}
\begin{tabular}{p{1.25in}p{3.25in}}
$\displaystyle P(\ra)$ & A probability distribution over a discrete variable\\
$\displaystyle p(\ra)$ & A probability distribution over a continuous variable, or over
a variable whose type has not been specified\\
$\displaystyle \ra \sim P$ & Random variable $\ra$ has distribution $P$\\% so thing on left of \sim should always be a random variable, with name beginning with \r
$\displaystyle  \E_{\rx\sim P} [ f(x) ]\text{ or } \E f(x)$ & Expectation of $f(x)$ with respect to $P(\rx)$ \\
$\displaystyle \Var(f(x)) $ &  Variance of $f(x)$ under $P(\rx)$ \\
$\displaystyle \Cov(f(x),g(x)) $ & Covariance of $f(x)$ and $g(x)$ under $P(\rx)$\\
$\displaystyle H(\rx) $ & Shannon entropy of the random variable $\rx$\\
$\displaystyle \KL ( P \Vert Q ) $ & Kullback-Leibler divergence of P and Q \\
$\displaystyle \mathcal{N} ( \vx ; \vmu , \mSigma)$ & Gaussian distribution %
over $\vx$ with mean $\vmu$ and covariance $\mSigma$ \\
\end{tabular}
\egroup
\vspace{0.25cm}

\centerline{\bf Functions}
\bgroup
\def\arraystretch{1.5}
\begin{tabular}{p{1.25in}p{3.25in}}
$\displaystyle f: \sA \rightarrow \sB$ & The function $f$ with domain $\sA$ and range $\sB$\\
$\displaystyle f \circ g $ & Composition of the functions $f$ and $g$ \\
  $\displaystyle f(\vx ; \vtheta) $ & A function of $\vx$ parametrized by $\vtheta$.
  (Sometimes we write $f(\vx)$ and omit the argument $\vtheta$ to lighten notation) \\
$\displaystyle \log x$ & Natural logarithm of $x$ \\
$\displaystyle \sigma(x)$ & Logistic sigmoid, $\displaystyle \frac{1} {1 + \exp(-x)}$ \\
$\displaystyle \zeta(x)$ & Softplus, $\log(1 + \exp(x))$ \\
$\displaystyle || \vx ||_p $ & $\normlp$ norm of $\vx$ \\
$\displaystyle || \vx || $ & $\normltwo$ norm of $\vx$ \\
$\displaystyle x^+$ & Positive part of $x$, i.e., $\max(0,x)$\\
$\displaystyle \1_\mathrm{condition}$ & is 1 if the condition is true, 0 otherwise\\
\end{tabular}
\egroup
\vspace{0.25cm}



\section{Final instructions}
Do not change any aspects of the formatting parameters in the style files.
In particular, do not modify the width or length of the rectangle the text
should fit into, and do not change font sizes (except perhaps in the
\textsc{References} section; see below). Please note that pages should be
numbered.

\section{Preparing PostScript or PDF files}

Please prepare PostScript or PDF files with paper size ``US Letter'', and
not, for example, ``A4''. The -t
letter option on dvips will produce US Letter files.

Consider directly generating PDF files using \verb+pdflatex+
(especially if you are a MiKTeX user).
PDF figures must be substituted for EPS figures, however.

Otherwise, please generate your PostScript and PDF files with the following commands:
\begin{verbatim}
dvips mypaper.dvi -t letter -Ppdf -G0 -o mypaper.ps
ps2pdf mypaper.ps mypaper.pdf
\end{verbatim}

\subsection{Margins in LaTeX}

Most of the margin problems come from figures positioned by hand using
\verb+\special+ or other commands. We suggest using the command
\verb+\includegraphics+
from the graphicx package. Always specify the figure width as a multiple of
the line width as in the example below using .eps graphics
\begin{verbatim}
   \usepackage[dvips]{graphicx} ...
   \includegraphics[width=0.8\linewidth]{myfile.eps}
\end{verbatim}
or % Apr 2009 addition
\begin{verbatim}
   \usepackage[pdftex]{graphicx} ...
   \includegraphics[width=0.8\linewidth]{myfile.pdf}
\end{verbatim}
for .pdf graphics.
See section~4.4 in the graphics bundle documentation (\url{http://www.ctan.org/tex-archive/macros/latex/required/graphics/grfguide.ps})

A number of width problems arise when LaTeX cannot properly hyphenate a
line. Please give LaTeX hyphenation hints using the \verb+\-+ command.

\subsubsection*{Author Contributions}
If you'd like to, you may include  a section for author contributions as is done
in many journals. This is optional and at the discretion of the authors.

\subsubsection*{Acknowledgments}
Use unnumbered third level headings for the acknowledgments. All
acknowledgments, including those to funding agencies, go at the end of the paper.


\bibliography{./iclr2025_conference}
\bibliographystyle{iclr2025_conference}


You may include other additional sections here.


\end{document}
