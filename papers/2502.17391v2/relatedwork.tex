\section{Related Work}
\subsection{W-Asymmetric MLP}
Various methods for breaking parameter symmetries in neural networks have been studied, including approaches to removing permutation symmetries \cite{pourzanjani2017improving, pittorino2022deep}, scaling symmetries \cite{badrinarayanan2015understanding}, and sign symmetries \cite{wiese2023towards}. However, in most of these approaches, the neural network architectures or training processes deviate from standard practices, making them difficult to apply in practice. In this work, we fully adopt the approach from \cite{lim2024empirical} to break symmetries in neural networks. This method randomly freezes a portion of the neural networkâ€™s weights before training, keeping them unchanged throughout training (see Section \ref{sec:wmlp} for details). Notably, it does not require any special modifications to the training process. Authors of \cite{lim2024empirical} showed that breaking symmetries improves linear mode connectivity between two independently trained neural networks. In this paper, we investigate the empirical impact of reducing symmetries on the performance of Deep Ensembles and Mixture of Experts.

\subsection{Neural Network Ensembles}
In this study, we employ two different approaches for ensembling neural networks. The first approach, known as Deep Ensembles \cite{lakshminarayanan2017simple}, trains $k$ neural networks independently and averages their outputs to obtain the final prediction.

The second approach is the Mixture of Experts (MoE) \cite{yuksel2012twenty}, which consists of two main components: experts and a gating network. Each expert generates an output, but unlike Deep Ensembles, the final prediction is obtained through a weighted average of the experts' outputs. The weights for each expert are dynamically predicted by the gating network rather than being fixed. Recently, MoE architectures utilizing MLP models as experts have gained popularity \cite{fedus2022review} especially in NLP \cite{du2022glam}
and CV domains \cite{puigcerver2023sparse,
riquelme2021scaling}. In this work, we adapt MoE architectures for tabular data from \cite{chernov2025moe}. We cover it in detail in Section \ref{sec:MoE}.