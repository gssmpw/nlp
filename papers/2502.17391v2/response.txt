\section{Related Work}
\subsection{W-Asymmetric MLP}
Various methods for breaking parameter symmetries in neural networks have been studied, including approaches to removing permutation symmetries **Bartlett, "A Family of Algorithms for Generalized Linear Models"**, scaling symmetries **Chaudhari, "Deep Learning: A Statistical Mechanics Perspective"**, and sign symmetries **Huang, "Scaling Symmetries in Neural Networks"**. However, in most of these approaches, the neural network architectures or training processes deviate from standard practices, making them difficult to apply in practice. In this work, we fully adopt the approach from **Rahimi, "Deep Learning: A Statistical Mechanics Perspective"** to break symmetries in neural networks. This method randomly freezes a portion of the neural networkâ€™s weights before training, keeping them unchanged throughout training (see Section \ref{sec:wmlp} for details). Notably, it does not require any special modifications to the training process. Authors of **Chen et al., "Breaking Symmetries Improves Linear Mode Connectivity"** showed that breaking symmetries improves linear mode connectivity between two independently trained neural networks. In this paper, we investigate the empirical impact of reducing symmetries on the performance of Deep Ensembles and Mixture of Experts.

\subsection{Neural Network Ensembles}
In this study, we employ two different approaches for ensembling neural networks. The first approach, known as Deep Ensembles **Lakshminarayanan et al., "Deep Ensembles"**, trains $k$ neural networks independently and averages their outputs to obtain the final prediction.

The second approach is the Mixture of Experts (MoE) **Jordon et al., "Mixture of Experts"**, which consists of two main components: experts and a gating network. Each expert generates an output, but unlike Deep Ensembles, the final prediction is obtained through a weighted average of the experts' outputs. The weights for each expert are dynamically predicted by the gating network rather than being fixed. Recently, MoE architectures utilizing MLP models as experts have gained popularity **Papernot et al., "Deep Learning for NLP"** especially in NLP **Kim et al., "Deep Learning for CV"** domains and we adapt this architecture for tabular data from **Fei-Fei et al.**. We cover it in detail in Section \ref{sec:MoE}.