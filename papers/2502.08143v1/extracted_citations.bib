@InProceedings{Bubeck2018ALT,
  title = 	 {Sparsity, variance and curvature in multi-armed bandits},
  author = 	 {Bubeck, Sébastien and Cohen, Michael and Li, Yuanzhi},
  booktitle = 	 {Proceedings of Algorithmic Learning Theory},
  pages = 	 {111--127},
  year = 	 {2018},
  editor = 	 {Janoos, Firdaus and Mohri, Mehryar and Sridharan, Karthik},
  volume = 	 {83},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {07--09 Apr},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v83/bubeck18a/bubeck18a.pdf},
  url = 	 {https://proceedings.mlr.press/v83/bubeck18a.html},
  abstract = 	 {In (online) learning theory the concepts of sparsity, variance and curvature are well-understood and are routinely used to obtain refined regret and generalization bounds. In this paper we further our understanding of these concepts in the more challenging limited feedback scenario. We consider the adversarial multi-armed bandit and linear bandit settings and solve several open problems pertaining to the existence of algorithms with favorable regret bounds under the following assumptions: (i) sparsity of the individual losses, (ii) small variation of the loss sequence, and (iii) curvature of the action set. Specifically we show that (i) for $s$-sparse losses one can obtain $\tilde{O}(\sqrt{s T})$-regret (solving an open problem by Kwon and Perchet), (ii) for loss sequences with variation bounded by $Q$ one can obtain $\tilde{O}(\sqrt{Q})$-regret (solving an open problem by Kale and Hazan), and (iii) for linear bandit on an $\ell_p^n$ ball one can obtain $\tilde{O}(\sqrt{n T})$-regret for $p ∈[1,2]$ and one has $\tilde{Ω}(n \sqrt{T})$-regret for $p&gt;2$ (solving an open problem by Bubeck, Cesa-Bianchi and Kakade). A key new insight to obtain these results is to use regularizers satisfying more refined conditions than general self-concordance.}
}

@article{HazanAndKale11a,
  author  = {Elad Hazan and Satyen Kale},
  title   = {Better Algorithms for Benign Bandits},
  journal = {Journal of Machine Learning Research},
  year    = {2011},
  volume  = {12},
  number  = {35},
  pages   = {1287-1311},
  url     = {http://jmlr.org/papers/v12/hazan11a.html}
}

@InProceedings{Ito2021HybridDataMABBound,
  title = 	 {Parameter-Free Multi-Armed Bandit Algorithms with Hybrid Data-Dependent Regret Bounds},
  author =       {Ito, Shinji},
  booktitle = 	 {Proceedings of Thirty Fourth Conference on Learning Theory},
  year = 	 {2021},
  pdf = 	 {http://proceedings.mlr.press/v134/ito21a/ito21a.pdf}
}

@InProceedings{ItoCOLT2022aVariance,
  title = 	 {Adversarially Robust Multi-Armed Bandit Algorithm with Variance-Dependent Regret Bounds},
  author =       {Ito, Shinji and Tsuchiya, Taira and Honda, Junya},
  booktitle = 	 {Proceedings of Thirty Fifth Conference on Learning Theory},
  pages = 	 {1421--1422},
  year = 	 {2022},
  editor = 	 {Loh, Po-Ling and Raginsky, Maxim},
  volume = 	 {178},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {02--05 Jul},
  publisher =    {PMLR},
  pdf = 	 {https://proceedings.mlr.press/v178/ito22a/ito22a.pdf},
  url = 	 {https://proceedings.mlr.press/v178/ito22a.html}
}

@InProceedings{ItoCOLT2024,
  title = 	 {Adaptive Learning Rate for Follow-the-Regularized-Leader: Competitive Analysis and Best-of-Both-Worlds},
  author =       {Ito, Shinji and Tsuchiya, Taira and Honda, Junya},
  booktitle = 	 {Proceedings of Thirty Seventh Conference on Learning Theory},
  pages = 	 {2522--2563},
  year = 	 {2024},
  editor = 	 {Agrawal, Shipra and Roth, Aaron},
  volume = 	 {247},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {30 Jun--03 Jul},
  publisher =    {PMLR},
  pdf = 	 {https://proceedings.mlr.press/v247/ito24a/ito24a.pdf},
  url = 	 {https://proceedings.mlr.press/v247/ito24a.html},
  abstract = 	 {Follow-The-Regularized-Leader (FTRL) is known as an effective and versatile approach in online learning, where appropriate choice of the learning rate is crucial for smaller regret. To this end, we formulate the problem of adjusting FTRL’s learning rate as a sequential decision-making problem and introduce the framework of competitive analysis. We establish a lower bound for the competitive ratio and propose update rules for the learning rate that achieves an upper bound within a constant factor of this lower bound. Specifically, we illustrate that the optimal competitive ratio is characterized by the (approximate) monotonicity of components of the penalty term, showing that a constant competitive ratio is achievable if the components of the penalty term form a monotone non-increasing sequence, and derive a tight competitive ratio when penalty terms are $\xi$-approximately monotone non-increasing. Our proposed update rule, referred to as \textit{stability-penalty matching}, also facilitates the construction of Best-Of-Both-Worlds (BOBW) algorithms for stochastic and adversarial environments. In these environments our results contribute to achieving tighter regret bound and broaden the applicability of algorithms for various settings such as multi-armed bandits, graph bandits, linear bandits, and contextual bandits.}
}

@inproceedings{Tsuchiya2023stabilitypenaltyadaptive,
title={Stability-penalty-adaptive follow-the-regularized-leader: Sparsity, game-dependency, and best-of-both-worlds},
author={Taira Tsuchiya and Shinji Ito and Junya Honda},
booktitle={Thirty-seventh Conference on Neural Information Processing Systems},
year={2023},
url={https://openreview.net/forum?id=J3taqrzyyA}
}

@InProceedings{WeiAndLuo2018aBroadOMD,
  title = 	 {More Adaptive Algorithms for Adversarial Bandits},
  author =       {Wei, Chen-Yu and Luo, Haipeng},
  booktitle = 	 {Proceedings of the 31st  Conference On Learning Theory},
  pages = 	 {1263--1291},
  year = 	 {2018},
  editor = 	 {Bubeck, Sébastien and Perchet, Vianney and Rigollet, Philippe},
  volume = 	 {75},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {06--09 Jul},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v75/wei18a/wei18a.pdf},
  url = 	 {https://proceedings.mlr.press/v75/wei18a.html},
  abstract = 	 {We develop a novel and generic algorithm for the adversarial multi-armed bandit problem (or more generally the combinatorial semi-bandit problem). When instantiated differently, our algorithm achieves various new data-dependent regret bounds improving previous work. Examples include: 1) a regret bound depending on the variance of only the best arm; 2) a regret bound depending on the first-order path-length of only the best arm; 3) a regret bound depending on the sum of the first-order path-lengths of all arms as well as an important negative term, which together lead to faster convergence rates for some normal form games with partial feedback; 4) a regret bound that simultaneously implies small regret when the best arm has small loss {\it and} logarithmic regret when there exists an arm whose expected loss is always smaller than those of other arms by a fixed gap (e.g. the classic i.i.d. setting). In some cases, such as the last two results, our algorithm is completely parameter-free. The main idea of our algorithm is to apply the optimism and adaptivity techniques to the well-known Online Mirror Descent framework with a special log-barrier regularizer. The challenges are to come up with appropriate optimistic predictions and correction terms in this framework. Some of our results also crucially rely on using a sophisticated increasing learning rate schedule.}
}

@article{Zimmert2021TsallisINF,
author = {Zimmert, Julian and Seldin, Yevgeny},
title = {Tsallis-{INF}: An optimal algorithm for stochastic and adversarial bandits},
year = {2021},
issue_date = {January 2021},
publisher = {JMLR.org},
volume = {22},
number = {1},
issn = {1532-4435},
abstract = {We derive an algorithm that achieves the optimal (within constants) pseudo-regret in both adversarial and stochastic multi-armed bandits without prior knowledge of the regime and time horizon. The algorithm is based on online mirror descent (OMD) with Tsallis entropy regularization with power α = 1/2 and reduced-variance loss estimators. More generally, we define an adversarial regime with a self-bounding constraint, which includes stochastic regime, stochastically constrained adversarial regime (Wei and Luo, 2018), and stochastic regime with adversarial corruptions (Lykouris et al., 2018) as special cases, and show that the algorithm achieves logarithmic regret guarantee in this regime and all of its special cases simultaneously with the optimal regret guarantee in the adversarial regime. The algorithm also achieves adversarial and stochastic optimality in the utility-based dueling bandit setting. We provide empirical evaluation of the algorithm demonstrating that it significantly outperforms Ucb1 and Exp3 in stochastic environments. We also provide examples of adversarial environments, where Ucb1 and Thompson Sampling exhibit almost linear regret, whereas our algorithm suffers only logarithmic regret. To the best of our knowledge, this is the first example demonstrating vulnerability of Thompson Sampling in adversarial environments. Last but not least, we present a general stochastic analysis and a general adversarial analysis of OMD algorithms with Tsallis entropy regularization for α ε [0; 1] and explain the reason why α = 1/2 works best.},
journal = {J. Mach. Learn. Res.},
month = jan,
articleno = {28},
numpages = {49},
keywords = {bandits, online learning, best of both worlds, online mirror descent, tsallis entropy, multi-armed bandits, stochastic, adversarial, I.I.D.}
}

