@Article{LiKWIK2010,
author={Li, Lihong
and Littman, Michael L.
and Walsh, Thomas J.
and Strehl, Alexander L.},
title={Knows what it knows: a framework for self-aware learning},
journal={Machine Learning},
year={2011},
month={Mar},
day={01},
volume={82},
number={3},
pages={399-443},
abstract={We introduce a learning framework that combines elements of the well-known PAC and mistake-bound models. The KWIK (knows what it knows) framework was designed particularly for its utility in learning settings where active exploration can impact the training examples the learner is exposed to, as is true in reinforcement-learning and active-learning problems. We catalog several KWIK-learnable classes as well as open problems, and demonstrate their applications in experience-efficient reinforcement learning.},
issn={1573-0565},
doi={10.1007/s10994-010-5225-4},
url={https://doi.org/10.1007/s10994-010-5225-4}
}



@InProceedings{Azar2017,
  title = 	 {Minimax Regret Bounds for Reinforcement Learning},
  author =       {Mohammad Gheshlaghi Azar and Ian Osband and R{\'e}mi Munos},
  booktitle = 	 {Proceedings of the 34th International Conference on Machine Learning},
  pages = 	 {263--272},
  year = 	 {2017},
  editor = 	 {Precup, Doina and Teh, Yee Whye},
  volume = 	 {70},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {06--11 Aug},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v70/azar17a/azar17a.pdf},
  abstract = 	 {We consider the problem of provably optimal exploration in reinforcement learning for finite horizon MDPs. We show that an optimistic modification to value iteration achieves a regret bound of $\tilde {O}( \sqrt{HSAT} + H^2S^2A+H\sqrt{T})$ where $H$ is the time horizon, $S$ the number of states, $A$ the number of actions and $T$ the number of time-steps. This result improves over the best previous known bound $\tilde {O}(HS \sqrt{AT})$ achieved by the UCRL2 algorithm. The key significance of our new results is that when $T\geq H^3S^3A$ and $SA\geq H$, it leads to a regret of $\tilde{O}(\sqrt{HSAT})$ that matches the established lower bound of $\Omega(\sqrt{HSAT})$ up to a logarithmic factor. Our analysis contain two key insights. We use careful application of concentration inequalities to the optimal value function as a whole, rather than to the transitions probabilities (to improve scaling in $S$), and we define Bernstein-based “exploration bonuses” that use the empirical variance of the estimated values at the next states (to improve scaling in $H$).}
}


@inproceedings{Dann2015,
author = {Dann, Christoph and Brunskill, Emma},
title = {Sample Complexity of Episodic Fixed-Horizon Reinforcement Learning},
year = {2015},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {Recently, there has been significant progress in understanding reinforcement learning
in discounted infinite-horizon Markov decision processes (MDPs) by deriving tight
sample complexity bounds. However, in many real-world applications, an interactive
learning agent operates for a fixed or bounded period of time, for example tutoring
students for exams or handling customer service requests. Such scenarios can often
be better treated as episodic fixed-horizon MDPs, for which only looser bounds on
the sample complexity exist. A natural notion of sample complexity in this setting
is the number of episodes required to guarantee a certain performance with high probability
(PAC guarantee). In this paper, we derive an upper PAC bound \~{O}(|S|2|A|H2/∊2 ln 1/δ)
and a lower PAC bound Ω(|S||A|H2/∊2 ln 1/δ+c) that match up to log-terms and an additional
linear dependency on the number of states |S|. The lower bound is the first of its
kind for this setting. Our upper bound leverages Bernstein's inequality to improve
on previous bounds for episodic finite-horizon MDPs which have a time-horizon dependency
of at least H3.},
booktitle = {Proceedings of the 28th International Conference on Neural Information Processing Systems - Volume 2},
pages = {2818–2826},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'15}
}

@inproceedings{Tewari2007,
 author = {Tewari, Ambuj and Bartlett, Peter},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Platt and D. Koller and Y. Singer and S. Roweis},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {Optimistic Linear Programming gives Logarithmic Regret for Irreducible MDPs},
 volume = {20},
 year = {2008}
}



@inproceedings{Sun2020,
author = {Sun, Yanchao and Huang, Furong},
title = {Can Agents Learn by Analogy? An Inferable Model for PAC Reinforcement Learning},
year = {2020},
isbn = {9781450375184},
publisher = {International Foundation for Autonomous Agents and Multiagent Systems},
address = {Richland, SC},
abstract = {Model-based reinforcement learning algorithms make decisions by building and utilizing
a model of the environment. However, none of the existing algorithms attempts to infer
the dynamics of any state-action pair from known state-action pairs before meeting
it for sufficient times. We propose a new model-based method called Greedy Inference
Model (GIM) that infers the unknown dynamics from known dynamics based on the internal
spectral properties of the environment. In other words, GIM can "learn by analogy".
We further introduce a new exploration strategy which ensures that the agent rapidly
and evenly visits unknown state-action pairs. GIM is much more computationally efficient
than state-of-the-art model-based algorithms, as the number of dynamic programming
operations is independent of the environment size. Lower sample complexity could also
be achieved under mild conditions compared against methods without inferring. Experimental
results demonstrate the effectiveness and efficiency of GIM in a variety of real-world
tasks.},
booktitle = {Proceedings of the 19th International Conference on Autonomous Agents and MultiAgent Systems},
pages = {1332–1340},
numpages = {9},
keywords = {model-based reinforcement learning, computational complexity, sample complexity, spectral method},
location = {Auckland, New Zealand},
series = {AAMAS '20}
}

@inproceedings{BrunskillAndLi2013,
author = {Brunskill, Emma and Li, Lihong},
title = {Sample Complexity of Multi-Task Reinforcement Learning},
year = {2013},
publisher = {AUAI Press},
address = {Arlington, Virginia, USA},
abstract = {Transferring knowledge across a sequence of reinforcement-learning tasks is challenging,
and has a number of important applications. Though there is encouraging empirical
evidence that transfer can improve performance in subsequent reinforcement-learning
tasks, there has been very little theoretical analysis. In this paper, we introduce
a new multi-task algorithm for a sequence of reinforcement-learning tasks when each
task is sampled independently from (an unknown) distribution over a finite set of
Markov decision processes whose parameters are initially unknown. For this setting,
we prove under certain assumptions that the per-task sample complexity of exploration
is reduced significantly due to transfer compared to standard single-task algorithms.
Our multi-task algorithm also has the desired characteristic that it is guaranteed
not to exhibit negative transfer: in the worst case its per-task sample complexity
is comparable to the corresponding single-task algorithm.},
booktitle = {Proceedings of the Twenty-Ninth Conference on Uncertainty in Artificial Intelligence},
pages = {122–131},
numpages = {10},
location = {Bellevue, WA},
series = {UAI'13}
}

@article{Agarwal2021MarkovDP,
  title={Markov Decision Processes with Long-Term Average Constraints},
  author={Mridul Agarwal and Qinbo Bai and Vaneet Aggarwal},
  journal={ArXiv},
  year={2021},
  volume={abs/2106.06680}
}

@article{UCRL2,
  author  = {Thomas Jaksch and Ronald Ortner and Peter Auer},
  title   = {Near-optimal Regret Bounds for Reinforcement Learning},
  journal = {Journal of Machine Learning Research},
  year    = {2010},
  volume  = {11},
  number  = {51},
  pages   = {1563-1600}
}



@inproceedings{UCRL2006,
 author = {Auer, Peter and Ortner, Ronald},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {B. Sch\"{o}lkopf and J. Platt and T. Hoffman},
 pages = {},
 publisher = {MIT Press},
 title = {Logarithmic Online Regret Bounds for Undiscounted Reinforcement Learning},
 volume = {19},
 year = {2007}
}







@article{EXP3Auer2002b,
author = {Auer, Peter and Cesa-Bianchi, Nicol\`{o} and Freund, Yoav and Schapire, Robert E.},
title = {The Nonstochastic Multiarmed Bandit Problem},
journal = {SIAM Journal on Computing},
volume = {32},
number = {1},
pages = {48-77},
year = {2002},
doi = {10.1137/S0097539701398375},

URL = { 
        https://doi.org/10.1137/S0097539701398375
    
},
eprint = { 
        https://doi.org/10.1137/S0097539701398375
    
}
,
    abstract = { In the multiarmed bandit problem, a gambler must decide which arm of K nonidentical slot machines to play in a sequence of trials so as to maximize his reward. This classical problem has received much attention because of the simple model it provides of the trade-off between exploration (trying out each arm to find the best one) and exploitation (playing the arm believed to give the best payoff). Past solutions for the bandit problem have almost always relied on assumptions about the statistics of the slot machines.In this work, we make no statistical assumptions whatsoever about the nature of the process generating the payoffs of the slot machines. We give a solution to the bandit problem in which an adversary, rather than a well-behaved stochastic process, has complete control over the payoffs. In a sequence of T plays, we prove that the per-round payoff of our algorithm approaches that of the best arm at the rate O(T-1/2 ). We show by a matching lower bound that this is the best possible.We also prove that our algorithm approaches the per-round payoff of any set of strategies at a similar rate: if the best strategy is chosen from a pool of N strategies, then our algorithm approaches the per-round payoff of the strategy at the rate O((log N1/2T-1/2 ). Finally, we apply our results to the problem of playing an unknown repeated matrix game. We show that our algorithm approaches the minimax payoff of the unknown game at the rate O(T-1/2 ). }
}






@inproceedings{REGAL2009,
author = {Bartlett, Peter L. and Tewari, Ambuj},
title = {REGAL: A Regularization Based Algorithm for Reinforcement Learning in Weakly Communicating MDPs},
year = {2009},
isbn = {9780974903958},
publisher = {AUAI Press},
address = {Arlington, Virginia, USA},
abstract = {We provide an algorithm that achieves the optimal regret rate in an unknown weakly
communicating Markov Decision Process (MDP). The algorithm proceeds in episodes where,
in each episode, it picks a policy using regularization based on the span of the optimal
bias vector. For an MDP with S states and A actions whose optimal bias vector has
span bounded by H, we show a regret bound of \~{O}(HS√AT). We also relate the span to
various diameter-like quantities associated with the MDP, demonstrating how our results
improve on previous regret bounds.},
booktitle = {Proceedings of the Twenty-Fifth Conference on Uncertainty in Artificial Intelligence},
pages = {35–42},
numpages = {8},
location = {Montreal, Quebec, Canada},
series = {UAI '09}
}

@inproceedings{Azar2013,
 author = {Azar, Mohammad Gheshlaghi and Lazaric, Alessandro and Brunskill, Emma},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {C. J. C. Burges and L. Bottou and M. Welling and Z. Ghahramani and K. Q. Weinberger},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {Sequential Transfer in Multi-armed Bandit with Finite Set of Models},
 volume = {26},
 year = {2013}
}

@InProceedings{Azar2013PolicyAdvice,
author="Azar, Mohammad Gheshlaghi
and Lazaric, Alessandro
and Brunskill, Emma",
editor="Blockeel, Hendrik
and Kersting, Kristian
and Nijssen, Siegfried
and {\v{Z}}elezn{\'y}, Filip",
title="Regret Bounds for Reinforcement Learning with Policy Advice",
booktitle="Machine Learning and Knowledge Discovery in Databases",
year="2013",
publisher="Springer Berlin Heidelberg",
address="Berlin, Heidelberg",
pages="97--112",
abstract="In some reinforcement learning problems an agent may be provided with a set of input policies, perhaps learned from prior experience or provided by advisors. We present a reinforcement learning with policy advice (RLPA) algorithm which leverages this input set and learns to use the best policy in the set for the reinforcement learning task at hand. We prove that RLPA has a sub-linear regret of {\$}{\backslash}widetilde O({\backslash}sqrt{\{}T{\}}){\$}relative to the best input policy, and that both this regret and its computational complexity are independent of the size of the state and action space. Our empirical simulations support our theoretical analysis. This suggests RLPA may offer significant advantages in large domains where some prior good policies are provided.",
isbn="978-3-642-40988-2"
}



@inproceedings{ChiJin2020,
  author    = {Chi Jin and
               Zhuoran Yang and
               Zhaoran Wang and
               Michael I. Jordan},
  editor    = {Jacob D. Abernethy and
               Shivani Agarwal},
  title     = {Provably efficient reinforcement learning with linear function approximation},
  booktitle = {Conference on Learning Theory, {COLT} 2020, 9-12 July 2020, Virtual
               Event [Graz, Austria]},
  series    = {Proceedings of Machine Learning Research},
  volume    = {125},
  pages     = {2137--2143},
  publisher = {{PMLR}},
  year      = {2020},
  timestamp = {Fri, 27 Nov 2020 16:13:27 +0100},
  biburl    = {https://dblp.org/rec/conf/colt/JinYWJ20.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{Tropp2011FREEDMANSIF,
  title={FREEDMAN'S INEQUALITY FOR MATRIX MARTINGALES},
  author={J. Tropp},
  journal={Electronic Communications in Probability},
  year={2011},
  volume={16},
  pages={262-270}
}

@inproceedings{Simchowitz2019,
 author = {Simchowitz, Max and Jamieson, Kevin G},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {H. Wallach and H. Larochelle and A. Beygelzimer and F. d\textquotesingle Alch\'{e}-Buc and E. Fox and R. Garnett},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {Non-Asymptotic Gap-Dependent Regret Bounds for Tabular MDPs},
 volume = {32},
 year = {2019}
}


@inproceedings{ChiJin2018,
 author = {Jin, Chi and Allen-Zhu, Zeyuan and Bubeck, Sebastien and Jordan, Michael I},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {S. Bengio and H. Wallach and H. Larochelle and K. Grauman and N. Cesa-Bianchi and R. Garnett},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {Is Q-Learning Provably Efficient?},
 volume = {31},
 year = {2018}
}

@book{goodfellow2016deep,
title={Deep learning},
author={Goodfellow, Ian and Bengio, Yoshua and Courville, Aaron and Bengio, Yoshua},
volume={1},
year={2016},
publisher={MIT Press}
}


@InProceedings{YangAISTATS2021,
  title = 	 { Q-learning with Logarithmic Regret },
  author =       {Yang, Kunhe and Yang, Lin and Du, Simon},
  booktitle = 	 {Proceedings of The 24th International Conference on Artificial Intelligence and Statistics},
  pages = 	 {1576--1584},
  year = 	 {2021},
  editor = 	 {Banerjee, Arindam and Fukumizu, Kenji},
  volume = 	 {130},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {13--15 Apr},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v130/yang21b/yang21b.pdf},
  url = 	 {http://proceedings.mlr.press/v130/yang21b.html},
  abstract = 	 { This paper presents the first non-asymptotic result showing a model-free algorithm can achieve logarithmic cumulative regret for episodic tabular reinforcement learning if there exists a strictly positive sub-optimality gap. We prove that the optimistic Q-learning studied in [Jin et al. 2018] enjoys a ${\mathcal{O}}\!\left(\frac{SA\cdot \mathrm{poly}\left(H\right)}{\Delta_{\min}}\log\left(SAT\right)\right)$ cumulative regret bound where $S$ is the number of states, $A$ is the number of actions, $H$ is the planning horizon, $T$ is the total number of steps, and $\Delta_{\min}$ is the minimum sub-optimality gap of the optimal Q-function. This bound matches the information theoretical lower bound in terms of $S,A,T$ up to a $\log\left(SA\right)$ factor. We further extend our analysis to the discounted setting and obtain a similar logarithmic cumulative regret bound. }
}



@InProceedings{Mao2021b,
  title = 	 {Near-Optimal Model-Free Reinforcement Learning in Non-Stationary Episodic MDPs},
  author =       {Mao, Weichao and Zhang, Kaiqing and Zhu, Ruihao and Simchi-Levi, David and Basar, Tamer},
  booktitle = 	 {Proceedings of the 38th International Conference on Machine Learning},
  pages = 	 {7447--7458},
  year = 	 {2021},
  editor = 	 {Meila, Marina and Zhang, Tong},
  volume = 	 {139},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {18--24 Jul},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v139/mao21b/mao21b.pdf},
  url = 	 {http://proceedings.mlr.press/v139/mao21b.html},
  abstract = 	 {We consider model-free reinforcement learning (RL) in non-stationary Markov decision processes. Both the reward functions and the state transition functions are allowed to vary arbitrarily over time as long as their cumulative variations do not exceed certain variation budgets. We propose Restarted Q-Learning with Upper Confidence Bounds (RestartQ-UCB), the first model-free algorithm for non-stationary RL, and show that it outperforms existing solutions in terms of dynamic regret. Specifically, RestartQ-UCB with Freedman-type bonus terms achieves a dynamic regret bound of $\widetilde{O}(S^{\frac{1}{3}} A^{\frac{1}{3}} \Delta^{\frac{1}{3}} H T^{\frac{2}{3}})$, where $S$ and $A$ are the numbers of states and actions, respectively, $\Delta&gt;0$ is the variation budget, $H$ is the number of time steps per episode, and $T$ is the total number of time steps. We further show that our algorithm is \emph{nearly optimal} by establishing an information-theoretical lower bound of $\Omega(S^{\frac{1}{3}} A^{\frac{1}{3}} \Delta^{\frac{1}{3}} H^{\frac{2}{3}} T^{\frac{2}{3}})$, the first lower bound in non-stationary RL. Numerical experiments validate the advantages of RestartQ-UCB in terms of both cumulative rewards and computational efficiency. We further demonstrate the power of our results in the context of multi-agent RL, where non-stationarity is a key challenge.}
}

@book{RLBook2018,
  added-at = {2019-07-13T10:11:53.000+0200},
  author = {Sutton, Richard S. and Barto, Andrew G.},
  edition = {Second},
  interhash = {ac6b144aaec1819919a2fba9f705c852},
  intrahash = {f46601cf8b13d39d1378af0d79438b12},
  keywords = {},
  publisher = {The MIT Press},
  timestamp = {2019-07-13T10:11:53.000+0200},
  title = {Reinforcement Learning: An Introduction},
  url = {http://incompleteideas.net/book/the-book-2nd.html},
  year = {2018 }
}



@misc{Hallak2015contextual,
      title={Contextual Markov Decision Processes}, 
      author={Assaf Hallak and Dotan Di Castro and Shie Mannor},
      year={2015},
      eprint={1502.02259},
      archivePrefix={arXiv},
      primaryClass={stat.ML}
}

@inproceedings{LatentMDPs2021,
title={{RL} for Latent {MDP}s: Regret Guarantees and a Lower Bound},
author={Jeongyeol Kwon and Yonathan Efroni and Constantine Caramanis and Shie Mannor},
booktitle={Thirty-Fifth Conference on Neural Information Processing Systems},
year={2021}
}

﻿@article{Herbster1998FixedShare,
author={Herbster, Mark
and Warmuth, Manfred K.},
title={Tracking the Best Expert},
journal={Machine Learning},
year={1998},
month={Aug},
day={01},
volume={32},
number={2},
pages={151-178},
issn={1573-0565},
doi={10.1023/A:1007424614876},
url={https://doi.org/10.1023/A:1007424614876}
}

@inproceedings{CesaBianchi2012MirrorDM,
  title={Mirror Descent Meets Fixed Share (and feels no regret)},
  author={Nicol{\`o} Cesa-Bianchi and Pierre Gaillard and G{\'a}bor Lugosi and Gilles Stoltz},
  booktitle={NIPS},
  year={2012}
}

@article{Adamskiy2016,
  author  = {Dmitry Adamskiy and Wouter M. Koolen and Alexey Chernov and Vladimir Vovk},
  title   = {A Closer Look at Adaptive Regret},
  journal = {Journal of Machine Learning Research},
  year    = {2016},
  volume  = {17},
  number  = {23},
  pages   = {1-21},
  url     = {http://jmlr.org/papers/v17/13-533.html}
}

@inproceedings{HazanAdaptiveRegret2009,
author = {Hazan, Elad and Seshadhri, C.},
title = {Efficient Learning Algorithms for Changing Environments},
year = {2009},
isbn = {9781605585161},
address = {New York, NY, USA},
doi = {10.1145/1553374.1553425},
abstract = {We study online learning in an oblivious changing environment. The standard measure of regret bounds the difference between the cost of the online learner and the best decision in hindsight. Hence, regret minimizing algorithms tend to converge to the static best optimum, clearly a suboptimal behavior in changing environments. On the other hand, various metrics proposed to strengthen regret and allow for more dynamic algorithms produce inefficient algorithms.We propose a different performance metric which strengthens the standard metric of regret and measures performance with respect to a changing comparator. We then describe a series of data-streaming-based reductions which transform algorithms for minimizing (standard) regret into adaptive algorithms albeit incurring only poly-logarithmic computational overhead.Using this reduction, we obtain efficient low adaptive-regret algorithms for the problem of online convex optimization. This can be applied to various learning scenarios, i.e. online portfolio selection, for which we describe experimental results showing the advantage of adaptivity.},
booktitle = {Proceedings of the 26th Annual International Conference on Machine Learning},
pages = {393–400},
numpages = {8},
location = {Montreal, Quebec, Canada},
series = {ICML '09}
}

@inproceedings{SunTempLe2021,
  author={Yanchao Sun and Xiangyu Yin and Furong Huang},
  title={TempLe: Learning Template of Transitions for Sample Efficient Multi-task RL},
  year={2021},
  cdate={1609459200000},
  pages={9765-9773},
  url={https://ojs.aaai.org/index.php/AAAI/article/view/17174},
  booktitle={AAAI}
}

@inproceedings{DannUnifyingPACandRegret2017,
author = {Dann, Christoph and Lattimore, Tor and Brunskill, Emma},
title = {Unifying PAC and Regret: Uniform PAC Bounds for Episodic Reinforcement Learning},
year = {2017},
isbn = {9781510860964},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
booktitle = {Proceedings of the 31st International Conference on Neural Information Processing Systems},
pages = {5717–5727},
numpages = {11},
location = {Long Beach, California, USA},
series = {NIPS'17}
}

@article{weissman2003inequalities,
  title={Inequalities for the L1 deviation of the empirical distribution},
  author={Weissman, Tsachy and Ordentlich, Erik and Seroussi, Gadiel and Verdu, Sergio and Weinberger, Marcelo J},
  journal={Hewlett-Packard Labs, Tech. Rep},
  year={2003}
}

@inproceedings{Tarbouriech2021a,
title={A Provably Efficient Sample Collection Strategy for Reinforcement Learning},
author={Jean Tarbouriech and Matteo Pirotta and Michal Valko and Alessandro Lazaric},
booktitle={Advances in Neural Information Processing Systems},
editor={A. Beygelzimer and Y. Dauphin and P. Liang and J. Wortman Vaughan},
year={2021},
url={https://openreview.net/forum?id=AvVDR8R-kQX}
}

@inproceedings{Tarbouriech2020,
title={Active Model Estimation in Markov Decision Processes },
author={Jean Tarbouriech and Shubhanshu Shekhar and Matteo Pirotta and Mohammad Ghavamzadeh and Alessandro Lazaric},
booktitle={Uncertainty in Artificial Intelligence},
year={2020},
url={http://proceedings.mlr.press/v124/tarbouriech20a/tarbouriech20a-supp.pdf}
}

@inproceedings{GuoAndBrunskill2015,
author = {Guo, Zhaohan and Brunskill, Emma},
title = {Concurrent PAC RL},
year = {2015},
isbn = {0262511290},
publisher = {AAAI Press},
abstract = {In many real-world situations a decision maker may make decisions across many separate reinforcement learning tasks in parallel, yet there has been very little work on concurrent RL. Building on the efficient exploration RL literature, we introduce two new concurrent RL algorithms and bound their sample complexity. We show that under some mild conditions, both when the agent is known to be acting in many copies of the same MDP, and when they are not the same but are taken from a finite set, we can gain linear improvements in the sample complexity over not sharing information. This is quite exciting as a linear speedup is the most one might hope to gain. Our preliminary experiments confirm this result and show empirical benefits.},
booktitle = {Proceedings of the Twenty-Ninth AAAI Conference on Artificial Intelligence},
pages = {2624–2630},
numpages = {7},
location = {Austin, Texas},
series = {AAAI'15}
}

@article{Steimle2021,
author = {Lauren N. Steimle and David L. Kaufman and Brian T. Denton},
title = {Multi-model Markov decision processes},
journal = {IISE Transactions},
volume = {53},
number = {10},
pages = {1124-1139},
year  = {2021},
publisher = {Taylor & Francis},
doi = {10.1080/24725854.2021.1895454},
eprint = { 
        https://doi.org/10.1080/24725854.2021.1895454
    
}
}


@InProceedings{AbelLLRL2018,
  title = 	 {Policy and Value Transfer in Lifelong Reinforcement Learning},
  author =       {Abel, David and Jinnai, Yuu and Guo, Sophie Yue and Konidaris, George and Littman, Michael},
  booktitle = 	 {Proceedings of the 35th International Conference on Machine Learning},
  pages = 	 {20--29},
  year = 	 {2018},
  editor = 	 {Dy, Jennifer and Krause, Andreas},
  volume = 	 {80},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {10--15 Jul},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v80/abel18b/abel18b.pdf},
  abstract = 	 {We consider the problem of how best to use prior experience to bootstrap lifelong learning, where an agent faces a series of task instances drawn from some task distribution. First, we identify the initial policy that optimizes expected performance over the distribution of tasks for increasingly complex classes of policy and task distributions. We empirically demonstrate the relative performance of each policy class’ optimal element in a variety of simple task distributions. We then consider value-function initialization methods that preserve PAC guarantees while simultaneously minimizing the learning required in two learning algorithms, yielding MaxQInit, a practical new method for value-function-based transfer. We show that MaxQInit performs well in simple lifelong RL experiments.}
}


@inproceedings{Lecarpentier2021LipschitzLR,
  title={Lipschitz Lifelong Reinforcement Learning},
  author={Erwan Lecarpentier and David Abel and Kavosh Asadi and Yuu Jinnai and Emmanuel Rachelson and Michael L. Littman},
  booktitle={AAAI},
  year={2021}
}

@article{BrunskillAndLi2015,
  author    = {Emma Brunskill and
               Lihong Li},
  title     = {The Online Discovery Problem and Its Application to Lifelong Reinforcement
               Learning},
  journal   = {CoRR},
  volume    = {abs/1506.03379},
  year      = {2015},
  eprinttype = {arXiv},
  eprint    = {1506.03379},
  timestamp = {Mon, 13 Aug 2018 16:46:43 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/BrunskillL15.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{Xu2019WorstCaseRB,
  title={Worst-Case Regret Bound for Perturbation Based Exploration in Reinforcement Learning},
  author={Ziping Xu and Ambuj Tewari},
  booktitle={NeurIPS},
  year={2019}
}

@misc{rlberry,
    author = {Domingues, Omar Darwiche and Flet-Berliac, Yannis and Leurent, Edouard and M{\'e}nard, Pierre and Shang, Xuedong and Valko, Michal},
    doi = {10.5281/zenodo.5544540},
    month = {10},
    title = {{rlberry - A Reinforcement Learning Library for Research and Education}},
    url = {https://github.com/rlberry-py/rlberry},
    year = {2021}
}

@inproceedings{Zinkevich2003,
author = {Zinkevich, Martin},
title = {Online Convex Programming and Generalized Infinitesimal Gradient Ascent},
year = {2003},
isbn = {1577351894},
publisher = {AAAI Press},
abstract = {Convex programming involves a convex set F ⊆ Rn and a convex cost function c : F → R. The goal of convex programming is to find a point in F which minimizes c. In online convex programming, the convex set is known in advance, but in each step of some repeated optimization problem, one must select a point in F before seeing the cost function for that step. This can be used to model factory production, farm production, and many other industrial optimization problems where one is unaware of the value of the items produced until they have already been constructed. We introduce an algorithm for this domain. We also apply this algorithm to repeated games, and show that it is really a generalization of infinitesimal gradient ascent, and the results here imply that generalized infinitesimal gradient ascent (GIGA) is universally consistent.},
booktitle = {Proceedings of the Twentieth International Conference on International Conference on Machine Learning},
pages = {928–935},
numpages = {8},
location = {Washington, DC, USA},
series = {ICML'03}
}

@inproceedings{HallAndWillet2013,
author = {Hall, Eric C. and Willett, Rebecca M.},
title = {Dynamical Models and Tracking Regret in Online Convex Programming},
year = {2013},
publisher = {JMLR.org},
abstract = {This paper describes a new online convex optimization method which incorporates a family of candidate dynamical models and establishes novel tracking regret bounds that scale with the comparator's deviation from the best dynamical model in this family. Previous online optimization methods are designed to have a total accumulated loss comparable to that of the best comparator sequence, and existing tracking or shifting regret bounds scale with the overall variation of the comparator sequence. In many practical scenarios, however, the environment is non-stationary and comparator sequences with small variation are quite weak, resulting in large losses. The proposed Dynamic Mirror Descent method, in contrast, can yield low regret relative to highly variable comparator sequences by both tracking the best dynamical model and forming predictions based on that model. This concept is demonstrated empirically in the context of sequential compressive observations of a dynamic scene and tracking a dynamic social network.},
booktitle = {Proceedings of the 30th International Conference on International Conference on Machine Learning - Volume 28},
pages = {I–579–I–587},
location = {Atlanta, GA, USA},
series = {ICML'13}
}

@inproceedings{Zhang2018a,
author = {Zhang, Lijun and Lu, Shiyin and Zhou, Zhi-Hua},
title = {Adaptive Online Learning in Dynamic Environments},
year = {2018},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {In this paper, we study online convex optimization in dynamic environments, and aim to bound the dynamic regret with respect to any sequence of comparators. Existing work have shown that online gradient descent enjoys an O(√T(1 + PT)) dynamic regret, where T is the number of iterations and PT is the path-length of the comparator sequence. However, this result is unsatisfactory, as there exists a large gap from the Ω(√T(1 + PT)) lower bound established in our paper. To address this limitation, we develop a novel online method, namely adaptive learning for dynamic environment (Ader), which achieves an optimal O(√T(1 + PT)) dynamic regret. The basic idea is to maintain a set of experts, each attaining an optimal dynamic regret for a specific path-length, and combines them with an expert-tracking algorithm. Furthermore, we propose an improved Ader based on the surrogate loss, and in this way the number of gradient evaluations per round is reduced from O(log T) to 1. Finally, we extend Ader to the setting that a sequence of dynamical models is available to characterize the comparators.},
booktitle = {Proceedings of the 32nd International Conference on Neural Information Processing Systems},
pages = {1330–1340},
numpages = {11},
location = {Montr\'{e}al, Canada},
series = {NIPS'18}
}

@article{Zhao2020switchingCost,
author = {Zhao, Yawei and Zhao, Qian and Zhang, Xingxing and Zhu, En and Liu, Xinwang and Yin, Jianping},
title = {Understand Dynamic Regret with Switching Cost for Online Decision Making},
year = {2020},
issue_date = {June 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {11},
number = {3},
issn = {2157-6904},
url = {https://doi.org/10.1145/3375788},
doi = {10.1145/3375788},
abstract = {As a metric to measure the performance of an online method, dynamic regret with switching cost has drawn much attention for online decision making problems. Although the sublinear regret has been provided in much previous research, we still have little knowledge about the relation between the dynamic regret and the switching cost. In the article, we investigate the relation for two classic online settings: Online Algorithms (OA) and Online Convex Optimization (OCO). We provide a new theoretical analysis framework that shows an interesting observation; that is, the relation between the switching cost and the dynamic regret is different for settings of OA and OCO. Specifically, the switching cost has significant impact on the dynamic regret in the setting of OA. But it does not have an impact on the dynamic regret in the setting of OCO. Furthermore, we provide a lower bound of regret for the setting of OCO, which is same with the lower bound in the case of no switching cost. It shows that the switching cost does not change the difficulty of online decision making problems in the setting of OCO.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = {apr},
articleno = {28},
numpages = {21},
keywords = {switching cost, online algorithms, online mirror descent, dynamic regret, online convex optimization, Online decision making}
}

@inproceedings{Yang2016movingClairvoyant,
author = {Yang, Tianbao and Zhang, Lijun and Jin, Rong and Yi, Jinfeng},
title = {Tracking Slowly Moving Clairvoyant: Optimal Dynamic Regret of Online Learning with True and Noisy Gradient},
year = {2016},
publisher = {JMLR.org},
abstract = {This work focuses on dynamic regret of online convex optimization that compares the performance of online learning to a clairvoyant who knows the sequence of loss functions in advance and hence selects the minimizer of the loss function at each step. By assuming that the clairvoyant moves slowly (i.e., the minimizers change slowly), we present several improved variation-based upper bounds of the dynamic regret under the true and noisy gradient feedback, which are optimal in light of the presented lower bounds. The key to our analysis is to explore a regularity metric that measures the temporal changes in the clairvoyant's minimizers, to which we refer as path variation. Firstly, we present a general lower bound in terms of the path variation, and then show that under full information or gradient feedback we are able to achieve an optimal dynamic regret. Secondly, we present a lower bound with noisy gradient feedback and then show that we can achieve optimal dynamic regrets under a stochastic gradient feedback and two-point bandit feedback. Moreover, for a sequence of smooth loss functions that admit a small variation in the gradients, our dynamic regret under the two-point bandit feedback matches what is achieved with full information.},
booktitle = {Proceedings of the 33rd International Conference on International Conference on Machine Learning - Volume 48},
pages = {449–457},
numpages = {9},
location = {New York, NY, USA},
series = {ICML'16}
}


@article{TrackingLinearPredictor2001,
author = {Herbster, Mark and Warmuth, Manfred K.},
title = {Tracking the Best Linear Predictor},
year = {2001},
issue_date = {9/1/2001},
publisher = {JMLR.org},
volume = {1},
issn = {1532-4435},
url = {https://doi.org/10.1162/153244301753683726},
doi = {10.1162/153244301753683726},
abstract = {In most on-line learning research the total on-line loss of the algorithm is compared to the total loss of the best off-line predictor u from a comparison class of predictors. We call such bounds static bounds. The interesting feature of these bounds is that they hold for an arbitrary sequence of examples. Recently some work has been done where the predictor ut at each trial t is allowed to change with time, and the total on-line loss of the algorithm is compared to the sum of the losses of ut at each trial plus the total "cost" for shifting to successive predictors. This is to model situations in which the examples change over time, and different predictors from the comparison class are best for different segments of the sequence of examples. We call such bounds shifting bounds. They hold for arbitrary sequences of examples and arbitrary sequences of predictors.Naturally shifting bounds are much harder to prove. The only known bounds are for the case when the comparison class consists of a sequences of experts or boolean disjunctions. In this paper we develop the methodology for lifting known static bounds to the shifting case. In particular we obtain bounds when the comparison class consists of linear neurons (linear combinations of experts). Our essential technique is to project the hypothesis of the static algorithm at the end of each trial into a suitably chosen convex region. This keeps the hypothesis of the algorithm well-behaved and the static bounds can be converted to shifting bounds.},
journal = {J. Mach. Learn. Res.},
month = {sep},
pages = {281–309},
numpages = {29}
}

@article{CampolongoAndOrabona2021TemporalVariability,
  author    = {Nicol{\`{o}} Campolongo and
               Francesco Orabona},
  title     = {A closer look at temporal variability in dynamic online learning},
  journal   = {CoRR},
  volume    = {abs/2102.07666},
  year      = {2021},
  url       = {https://arxiv.org/abs/2102.07666},
  eprinttype = {arXiv},
  eprint    = {2102.07666},
  timestamp = {Thu, 18 Feb 2021 15:26:00 +0100},
  biburl    = {https://dblp.org/rec/journals/corr/abs-2102-07666.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@misc{CanonneShortNote2022,
  doi = {10.48550/ARXIV.2202.07198},  
  url = {https://arxiv.org/abs/2202.07198},  
  author = {Canonne, Clément L.},  
  keywords = {Probability (math.PR), Statistics Theory (math.ST), FOS: Mathematics, FOS: Mathematics},  
  title = {A short note on an inequality between KL and TV},  
  publisher = {arXiv},  
  year = {2022},  
  copyright = {Creative Commons Attribution Non Commercial Share Alike 4.0 International}
}

@book{CalculusSpivak,
  added-at = {2015-07-17T04:33:01.000+0200},
  author = {Spivak, Michael},
  biburl = {https://www.bibsonomy.org/bibtex/23d735440f9a1bda2ceac8d0326ff7420/ytyoun},
  edition = {Fourth},
  interhash = {7583e78dfc243481b3fd98656f8b8f8c},
  intrahash = {3d735440f9a1bda2ceac8d0326ff7420},
  keywords = {calculus textbook},
  publisher = {Publish or Perish},
  timestamp = {2015-07-17T04:33:01.000+0200},
  title = {Calculus},
  year = 2008
}

@inproceedings{MacWilliams1977TheTO,
  title={The Theory of Error-Correcting Codes},
  author={F. Jessie MacWilliams and N. J. A. Sloane},
  year={1977}
}

@article{Lattimore2018,
  author  = {Tor Lattimore},
  title   = {Refining the Confidence Level for Optimistic Bandit Strategies},
  journal = {Journal of Machine Learning Research},
  year    = {2018},
  volume  = {19},
  number  = {20},
  pages   = {1--32},
  url     = {http://jmlr.org/papers/v19/17-513.html}
}

@InProceedings{Domingues2021,
  title = 	 {Episodic Reinforcement Learning in Finite MDPs: Minimax Lower Bounds Revisited},
  author =       {Domingues, Omar Darwiche and M{\'e}nard, Pierre and Kaufmann, Emilie and Valko, Michal},
  booktitle = 	 {Proceedings of the 32nd International Conference on Algorithmic Learning Theory},
  pages = 	 {578--598},
  year = 	 {2021},
  editor = 	 {Feldman, Vitaly and Ligett, Katrina and Sabato, Sivan},
  volume = 	 {132},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {16--19 Mar},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v132/domingues21a/domingues21a.pdf},
  url = 	 {https://proceedings.mlr.press/v132/domingues21a.html},
  abstract = 	 {In this paper, we propose new problem-independent lower bounds on the sample complexity and regret in episodic MDPs, with a particular focus on the \emph{non-stationary case} in which the transition kernel is allowed to change in each stage of the episode. Our main contribution is a lower bound of $\Omega((H^3SA/\epsilon^2)\log(1/\delta))$ on the sample complexity of an $(\varepsilon,\delta)$-PAC algorithm for best policy identification in a non-stationary MDP, relying on a construction of “hard MDPs” which is different from the ones previously used in the literature. Using this same class of MDPs, we also provide a rigorous proof of the $\Omega(\sqrt{H^3SAT})$ regret bound for non-stationary MDPs. Finally, we discuss connections to PAC-MDP lower bounds.}
}

@book{MarkovChainAndMixingTime2008,
  added-at = {2015-06-15T03:55:43.000+0200},
  author = {Levin, D.A. and Peres, Y. and Wilmer, E.L.},
  biburl = {https://www.bibsonomy.org/bibtex/267c39e68955ff473c9d292c6fbb7d351/peter.ralph},
  interhash = {2986988f00d006bf25613c7942d89ead},
  intrahash = {67c39e68955ff473c9d292c6fbb7d351},
  isbn = {9780821886274},
  keywords = {Markov_chain hitting_times mixing_times reference resistance_distance spectral_decomposition},
  publisher = {American Mathematical Soc.},
  timestamp = {2015-06-15T03:55:43.000+0200},
  title = {Markov Chains and Mixing Times},
  url = {http://pages.uoregon.edu/dlevin/MARKOV/},
  year = 2008
}


@misc{Tulsiani2014L6,
  author        = {Madhur Tulsiani},
  title         = {Lecture 6, Lecture notes in Information and Coding Theory},
  month         = {October},
  year          = {2014},
  url           = {https://home.ttic.edu/~madhurt/courses/infotheory2014/l6.pdf},
  publisher={Toyota Technological Institute at Chicago}
}

@misc{Tulsiani2014L5,
  author        = {Madhur Tulsiani},
  title         = {Lecture 5, Lecture notes in Information and Coding Theory},
  month         = {October},
  year          = {2014},
  url           = {https://home.ttic.edu/~madhurt/courses/infotheory2014/l6.pdf},
  publisher={Toyota Technological Institute at Chicago}
}

@book{BanditAlgorithmsBook2020, 
place={Cambridge}, 
title={Bandit Algorithms}, 
DOI={10.1017/9781108571401}, 
publisher={Cambridge University Press}, 
author={Lattimore, Tor and Szepesvári, Csaba}, 
year={2020}}

@inproceedings{Dann2021MDPLowerBound,
 author = {Dann, Christoph and Marinov, Teodor Vanislavov and Mohri, Mehryar and Zimmert, Julian},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {M. Ranzato and A. Beygelzimer and Y. Dauphin and P.S. Liang and J. Wortman Vaughan},
 pages = {1--12},
 publisher = {Curran Associates, Inc.},
 title = {Beyond Value-Function Gaps: Improved Instance-Dependent Regret Bounds for Episodic Reinforcement Learning},
 url = {https://proceedings.neurips.cc/paper/2021/file/000c076c390a4c357313fca29e390ece-Paper.pdf},
 volume = {34},
 year = {2021}
}

@book{CoverAndThomas2006,
author = {Cover, Thomas M. and Thomas, Joy A.},
title = {Elements of Information Theory (Wiley Series in Telecommunications and Signal Processing)},
year = {2006},
isbn = {0471241954},
publisher = {Wiley-Interscience},
address = {USA}
}

@article{sason2015reverse,
  title={On reverse {P}insker inequalities},
  author={Sason, Igal},
  journal={arXiv preprint arXiv:1503.07118},
  year={2015}
}

@inproceedings{Azar2012,
author = {Azar, Mohammad Gheshlaghi and Munos, R\'{e}mi and Kappen, Hilbert J.},
title = {On the Sample Complexity of Reinforcement Learning with a Generative Model},
year = {2012},
isbn = {9781450312851},
publisher = {Omnipress},
address = {Madison, WI, USA},
abstract = {We consider the problem of learning the optimal action-value function in the discounted-reward Markov decision processes (MDPs). We prove a new PAC bound on the sample-complexity of model-based value iteration algorithm in the presence of the generative model, which indicates that for an MDP with N state-action pairs and the discount factor γ ∈ [0, 1) only O(N log(N/δ)/(1 - γ)3ε2)) samples are required to find an ε-optimal estimation of the action-value function with the probability 1 - δ. We also prove a matching lower bound of Θ(N log(N/δ)/((1 - γ)3ε2)) on the sample complexity of estimating the optimal action-value function by every RL algorithm. To the best of our knowledge, this is the first matching result on the sample complexity of estimating the optimal (action-) value function in which the upper bound matches the lower bound of RL in terms of N, ε, δ and 1/(1-γ). Also, both our lower bound and our upper bound significantly improve on the state-of-the-art in terms of 1/(1 - γ).},
booktitle = {Proceedings of the 29th International Coference on International Conference on Machine Learning},
pages = {1707–1714},
numpages = {8},
location = {Edinburgh, Scotland},
series = {ICML'12}
}

@article{LaiAndRobbins1985,
  added-at = {2007-07-05T16:17:35.000+0200},
  author = {Lai, T.L. and Robbins, H.},
  biburl = {https://www.bibsonomy.org/bibtex/243d5e28aa6ae3446e548319c7f964b7f/jleny},
  description = {bandit problems},
  interhash = {c33edf59c35ee99dbaa6f1ce8835b782},
  intrahash = {43d5e28aa6ae3446e548319c7f964b7f},
  journal = {Advances in Applied Mathematics},
  keywords = {imported},
  pages = {4--22},
  timestamp = {2007-07-05T16:17:37.000+0200},
  title = {Asymptotically Efficient Adaptive Allocation Rules},
  volume = 6,
  year = 1985
}

@article{Kaufmann2016,
author = {Kaufmann, Emilie and Capp\'{e}, Olivier and Garivier, Aur\'{e}lien},
title = {On the Complexity of Best-Arm Identification in Multi-Armed Bandit Models},
year = {2016},
issue_date = {January 2016},
publisher = {JMLR.org},
volume = {17},
number = {1},
issn = {1532-4435},
abstract = {The stochastic multi-armed bandit model is a simple abstraction that has proven useful in many different contexts in statistics and machine learning. Whereas the achievable limit in terms of regret minimization is now well known, our aim is to contribute to a better understanding of the performance in terms of identifying the m best arms. We introduce generic notions of complexity for the two dominant frameworks considered in the literature: fixed-budget and fixed-confidence settings. In the fixed-confidence setting, we provide the first known distribution-dependent lower bound on the complexity that involves information-theoretic quantities and holds when m ≥ 1 under general assumptions. In the specific case of two armed-bandits, we derive refined lower bounds in both the fixedcon fidence and fixed-budget settings, along with matching algorithms for Gaussian and Bernoulli bandit models. These results show in particular that the complexity of the fixed-budget setting may be smaller than the complexity of the fixed-confidence setting, contradicting the familiar behavior observed when testing fully specified alternatives. In addition, we also provide improved sequential stopping rules that have guaranteed error probabilities and shorter average running times. The proofs rely on two technical results that are of independent interest: a deviation lemma for self-normalized sums (Lemma 7) and a novel change of measure inequality for bandit models (Lemma 1).},
journal = {J. Mach. Learn. Res.},
month = {jan},
pages = {1–42},
numpages = {42},
keywords = {multi-armed bandit, pure exploration, best-arm identification, information-theoretic divergences, sequential testing}
}

@inproceedings{Diakonikolas2021EfficientMF,
  title={Efficient Methods for Structured Nonconvex-Nonconcave Min-Max Optimization},
  author={Jelena Diakonikolas and Constantinos Daskalakis and Michael I. Jordan},
  booktitle={International Conference on Artificial Intelligence and Statistics},
  year={2021}
}

@inproceedings{Daskalakis2021,
author = {Daskalakis, Constantinos and Skoulakis, Stratis and Zampetakis, Manolis},
title = {The Complexity of Constrained Min-Max Optimization},
year = {2021},
isbn = {9781450380539},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3406325.3451125},
doi = {10.1145/3406325.3451125},
abstract = {Despite its important applications in Machine Learning, min-max optimization of objective functions that are nonconvex-nonconcave remains elusive. Not only are there no known first-order methods converging to even approximate local min-max equilibria (a.k.a.&nbsp;approximate saddle points), but the computational complexity of identifying them is also poorly understood. In this paper, we provide a characterization of the computational complexity as well as of the limitations of first-order methods in this problem. Specifically, we show that in linearly constrained min-max optimization problems with nonconvex-nonconcave objectives an approximate local min-max equilibrium of large enough approximation is guaranteed to exist, but computing such a point is PPAD-complete. The same is true of computing an approximate fixed point of the (Projected) Gradient Descent/Ascent update dynamics, which is computationally equivalent to computing approximate local min-max equilibria. An important byproduct of our proof is to establish an unconditional hardness result in the&nbsp;Nemirovsky-Yudin 1983 oracle optimization model, where we are given oracle access to the values of some function f : P → [−1, 1] and its gradient ∇ f, where P ⊆ [0, 1]d is a known convex polytope. We show that any algorithm that uses such first-order oracle access to f and finds an ε-approximate local min-max equilibrium needs to make a number of oracle queries that is exponential in at least one of 1/ε, L, G, or d, where L and G are respectively the smoothness and Lipschitzness of f. This comes in sharp contrast to minimization problems, where finding approximate local minima in the same setting can be done with Projected Gradient Descent using O(L/ε) many queries. Our result is the first to show an exponential separation between these two fundamental optimization problems in the oracle model.},
booktitle = {Proceedings of the 53rd Annual ACM SIGACT Symposium on Theory of Computing},
pages = {1466–1478},
numpages = {13},
keywords = {black-box lower bounds, min-max optimization, computational complexity, PPAD, TFNP},
location = {Virtual, Italy},
series = {STOC 2021}
}

@inproceedings{Abernethy2022MinMaxFairness,
  title={Active Sampling for Min-Max Fairness},
  author={Jacob D. Abernethy and Pranjal Awasthi and Matth{\"a}us Kleindessner and Jamie H. Morgenstern and Chris Russell and Jie Zhang},
  booktitle={International Conference on Machine Learning},
  year={2022}
}

@article{Duchi2018Lectures,
  title={Introductory lectures on stochastic
 optimization},
  author={John C. Duchi},
  journal={IAS/Park City Mathematics Series},
  year={2018}
}


@InProceedings{Carmon2021a,
  title = 	 {Thinking Inside the Ball: Near-Optimal Minimization of the Maximal Loss},
  author =       {Carmon, Yair and Jambulapati, Arun and Jin, Yujia and Sidford, Aaron},
  booktitle = 	 {Proceedings of Thirty Fourth Conference on Learning Theory},
  pages = 	 {866--882},
  year = 	 {2021},
  editor = 	 {Belkin, Mikhail and Kpotufe, Samory},
  volume = 	 {134},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {15--19 Aug},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v134/carmon21a/carmon21a.pdf},
  url = 	 {https://proceedings.mlr.press/v134/carmon21a.html},
  abstract = 	 {We characterize the complexity of minimizing $\max_{i\in[N]} f_i(x)$ for convex, Lipschitz functions $f_1,\ldots, f_N$. For non-smooth functions, existing methods require $O(N\epsilon^{-2})$ queries to a first-order oracle to compute an $\epsilon$-suboptimal point and $\widetilde{O}(N\epsilon^{-1})$ queries if the $f_i$ are $O(1/\epsilon)$-smooth.  We develop methods with improved complexity bounds of $\widetilde{O}(N\epsilon^{-2/3} + \epsilon^{-8/3})$ in the non-smooth case and $\widetilde{O}(N\epsilon^{-2/3} + \sqrt{N}\epsilon^{-1})$ in the $O(1/\epsilon)$-smooth case.  Our methods consist of a recently proposed ball optimization oracle acceleration algorithm (which we refine) and a careful implementation of said oracle for the softmax function.  We also prove an oracle complexity lower bound scaling as $\Omega(N\epsilon^{-2/3})$, showing that our dependence on $N$ is optimal up to polylogarithmic factors.}
}

@article{Nemirovski2009,
author = {Nemirovski, A. and Juditsky, A. and Lan, G. and Shapiro, A.},
title = {Robust Stochastic Approximation Approach to Stochastic Programming},
journal = {SIAM Journal on Optimization},
volume = {19},
number = {4},
pages = {1574-1609},
year = {2009},
doi = {10.1137/070704277},

URL = { 
    
        https://doi.org/10.1137/070704277
    
    

},
eprint = { 
    
        https://doi.org/10.1137/070704277
    
    

}
,
    abstract = { In this paper we consider optimization problems where the objective function is given in a form of the expectation. A basic difficulty of solving such stochastic optimization problems is that the involved multidimensional integrals (expectations) cannot be computed with high accuracy. The aim of this paper is to compare two computational approaches based on Monte Carlo sampling techniques, namely, the stochastic approximation (SA) and the sample average approximation (SAA) methods. Both approaches, the SA and SAA methods, have a long history. Current opinion is that the SAA method can efficiently use a specific (say, linear) structure of the considered problem, while the SA approach is a crude subgradient method, which often performs poorly in practice. We intend to demonstrate that a properly modified SA approach can be competitive and even significantly outperform the SAA method for a certain class of convex stochastic problems. We extend the analysis to the case of convex-concave stochastic saddle point problems and present (in our opinion highly encouraging) results of numerical experiments. }
}

@inproceedings{Carmon2020,
 author = {Carmon, Yair and Jambulapati, Arun and Jiang, Qijia and Jin, Yujia and Lee, Yin Tat and Sidford, Aaron and Tian, Kevin},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {H. Larochelle and M. Ranzato and R. Hadsell and M.F. Balcan and H. Lin},
 pages = {19052--19063},
 publisher = {Curran Associates, Inc.},
 title = {Acceleration with a Ball Optimization Oracle},
 url = {https://proceedings.neurips.cc/paper/2020/file/dba4c1a117472f6aca95211285d0587e-Paper.pdf},
 volume = {33},
 year = {2020}
}


@article{Shalev-Shwartz10a,
  author  = {Shai Shalev-Shwartz and Ohad Shamir and Nathan Srebro and Karthik Sridharan},
  title   = {Learnability, Stability and Uniform Convergence},
  journal = {Journal of Machine Learning Research},
  year    = {2010},
  volume  = {11},
  number  = {90},
  pages   = {2635--2670},
  url     = {http://jmlr.org/papers/v11/shalev-shwartz10a.html}
}
 
 @book{Lan2020, 
 place={S.l.}, 
 edition={1}, 
 series={Springer Series in the Data Sciences}, 
 title={First-order and stochastic optimization methods for machine learning}, 
 publisher={SPRINGER}, 
 author={LAN, GUANGHUI}, 
 year={2020}, 
 collection={Springer Series in the Data Sciences}} 
 
 
@book{PLGbook,
  added-at = {2019-07-29T00:00:00.000+0200},
  author = {Cesa-Bianchi, Nicolò and Lugosi, Gábor},
  biburl = {https://www.bibsonomy.org/bibtex/262edbe6d002d919a93e22e6e36ed3c78/dblp},
  ee = {https://www.wikidata.org/entity/Q59538584},
  interhash = {1170748dfb0bbdd1a80fce936f0fe46b},
  intrahash = {62edbe6d002d919a93e22e6e36ed3c78},
  isbn = {978-0-511-54692-1},
  keywords = {dblp},
  pages = {I-XII, 1-394},
  publisher = {Cambridge University Press},
  timestamp = {2019-07-30T11:47:45.000+0200},
  title = {Prediction, {L}earning, and {G}ames.},
  year = 2006
}

@misc{Soma2022,
  doi = {10.48550/ARXIV.2212.13669},
  
  url = {https://arxiv.org/abs/2212.13669},
  
  author = {Soma, Tasuku and Gatmiry, Khashayar and Jegelka, Stefanie},
  
  keywords = {Machine Learning (cs.LG), Optimization and Control (math.OC), FOS: Computer and information sciences, FOS: Computer and information sciences, FOS: Mathematics, FOS: Mathematics},
  
  title = {Optimal algorithms for group distributionally robust optimization and beyond},
  
  publisher = {arXiv},
  
  year = {2022},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}







@article{Ben-Tal2013Robust,
author = {Ben-Tal, Aharon and den Hertog, Dick and De Waegenaere, Anja and Melenberg, Bertrand and Rennen, Gijs},
title = {Robust Solutions of Optimization Problems Affected by Uncertain Probabilities},
journal = {Management Science},
volume = {59},
number = {2},
pages = {341-357},
year = {2013},
doi = {10.1287/mnsc.1120.1641},

URL = { 
    
        https://doi.org/10.1287/mnsc.1120.1641
    
    

},
eprint = { 
    
        https://doi.org/10.1287/mnsc.1120.1641
    
    

}
,
    abstract = { In this paper we focus on robust linear optimization problems with uncertainty regions defined by ϕ-divergences (for example, chi-squared, Hellinger, Kullback–Leibler). We show how uncertainty regions based on ϕ-divergences arise in a natural way as confidence sets if the uncertain parameters contain elements of a probability vector. Such problems frequently occur in, for example, optimization problems in inventory control or finance that involve terms containing moments of random variables, expected utility, etc. We show that the robust counterpart of a linear optimization problem with ϕ-divergence uncertainty is tractable for most of the choices of ϕ typically considered in the literature. We extend the results to problems that are nonlinear in the optimization variables. Several applications, including an asset pricing example and a numerical multi-item newsvendor example, illustrate the relevance of the proposed approach. This paper was accepted by Gérard P. Cachon, optimization. }
}


@inproceedings{Shalev-Shwartz2016MinimizingMaxalLossHowWhy,
author = {Shalev-Shwartz, Shai and Wexler, Yonatan},
title = {Minimizing the Maximal Loss: How and Why},
year = {2016},
publisher = {JMLR.org},
abstract = {A commonly used learning rule is to approximately minimize the average loss over the training set. Other learning algorithms, such as AdaBoost and hard-SVM, aim at minimizing the maximal loss over the training set. The average loss is more popular, particularly in deep learning, due to three main reasons. First, it can be conveniently minimized using online algorithms, that process few examples at each iteration. Second, it is often argued that there is no sense to minimize the loss on the training set too much, as it will not be reflected in the generalization loss. Last, the maximal loss is not robust to outliers. In this paper we describe and analyze an algorithm that can convert any online algorithm to a minimizer of the maximal loss. We prove that in some situations better accuracy on the training set is crucial to obtain good performance on unseen examples. Last, we propose robust versions of the approach that can handle outliers.},
booktitle = {Proceedings of the 33rd International Conference on International Conference on Machine Learning - Volume 48},
pages = {793–801},
numpages = {9},
location = {New York, NY, USA},
series = {ICML'16}
}

@inproceedings{NguyenAndMehta2023AOMTRL,
  author       = {Quan Nguyen and
                  Nishant A. Mehta},
  title        = {Adversarial Online Multi-Task Reinforcement Learning},
  booktitle    = {International Conference on Algorithmic Learning Theory, February
                  20-23, 2023, Singapore},
  publisher    = {{PMLR}},
  year         = {2023},
  timestamp    = {Wed, 15 Mar 2023 16:51:17 +0100},
  biburl       = {https://dblp.org/rec/conf/alt/NguyenM23.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@misc{Zhang2023stochastic,
      title={Stochastic Approximation Approaches to Group Distributionally Robust Optimization}, 
      author={Lijun Zhang and Peng Zhao and Tianbao Yang and Zhi-Hua Zhou},
      year={2023},
      eprint={2302.09267},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@inproceedings{Li2018VisualizeLossLandscape,
 author = {Li, Hao and Xu, Zheng and Taylor, Gavin and Studer, Christoph and Goldstein, Tom},
 booktitle = {Advances in Neural Information Processing Systems},
 title = {Visualizing the Loss Landscape of Neural Nets},
 year = {2018}
}

@article{Vapnik1971,
author = {Vapnik, V. N. and Chervonenkis, A. Ya.},
title = {On the Uniform Convergence of Relative Frequencies of Events to Their Probabilities},
journal = {Theory of Probability \& Its Applications},
volume = {16},
number = {2},
pages = {264-280},
year = {1971},
doi = {10.1137/1116025},
}

@article{Bellman1957markovian,
  added-at = {2017-04-07T12:00:35.000+0200},
  author = {Bellman, Richard},
  biburl = {https://www.bibsonomy.org/bibtex/2c04c5f89b4e8445651eded5b56c67342/becker},
  interhash = {d7aa065c075b248c9980b4c45d635b66},
  intrahash = {c04c5f89b4e8445651eded5b56c67342},
  journal = {Journal of Mathematics and Mechanics},
  keywords = {chain citedby:scholar:count:987 citedby:scholar:timestamp:2017-4-7 decision diss inthesis markov process},
  number = 5,
  pages = {679--684},
  timestamp = {2017-12-20T14:47:54.000+0100},
  title = {A Markovian decision process},
  url = {http://www.jstor.org/stable/24900506},
  volume = 6,
  year = 1957
}


@inproceedings{Sagawa2020Distributionally,
title={Distributionally Robust Neural Networks},
author={Shiori Sagawa and Pang Wei Koh and Tatsunori B. Hashimoto and Percy Liang},
booktitle={International Conference on Learning Representations},
year={2020}
}

@inproceedings{Nika2022OnDemandSampling,
 author = {Haghtalab, Nika and Jordan, Michael and Zhao, Eric},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {S. Koyejo and S. Mohamed and A. Agarwal and D. Belgrave and K. Cho and A. Oh},
 pages = {406--419},
 publisher = {Curran Associates, Inc.},
 title = {On-Demand Sampling: Learning Optimally from Multiple Distributions},
 volume = {35},
 year = {2022}
}

@Inproceedings{Ivkin2021,
 author = {Nikita Ivkin and Zohar Karnin and Valerio Perrone and Giovanni Zappella},
 title = {Cost-aware adversarial best arm identification},
 year = {2021},
 url = {https://www.amazon.science/publications/cost-aware-adversarial-best-arm-identification},
 booktitle = {ICLR NAS Workshop 2021},
}

@inproceedings{Xu2022,
author = {Xu, Mengdi and Huang, Peide and Niu, Yaru and Kumar, Visak and Qiu, Jielin and Fang, Chao and Lee, Kuan-Hui and Qi, Xuewei and Lam, Henry and Li, Bo and Zhao, Ding},
year = {2022},
month = {10},
pages = {},
title = {Group Distributionally Robust Reinforcement Learning with Hierarchical Latent Variables},
doi = {10.48550/arXiv.2210.12262},
booktitle={International Conference on Artificial Intelligence and Statistics},
}

@article{Bhandari2019,
  author       = {Jalaj Bhandari and
                  Daniel Russo},
  title        = {Global Optimality Guarantees For Policy Gradient Methods},
  journal      = {CoRR},
  volume       = {abs/1906.01786},
  year         = {2019},
  url          = {http://arxiv.org/abs/1906.01786},
  eprinttype    = {arXiv},
  eprint       = {1906.01786},
  timestamp    = {Thu, 13 Jun 2019 13:36:00 +0200},
  biburl       = {https://dblp.org/rec/journals/corr/abs-1906-01786.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{Xu2010DRoRL,
 author = {Xu, Huan and Mannor, Shie},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Lafferty and C. Williams and J. Shawe-Taylor and R. Zemel and A. Culotta},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {Distributionally Robust Markov Decision Processes},
 url = {https://proceedings.neurips.cc/paper_files/paper/2010/file/19f3cd308f1455b3fa09a282e0d496f4-Paper.pdf},
 volume = {23},
 year = {2010}
}

@misc{Smirnova2019distributionally,
      title={Distributionally Robust Reinforcement Learning}, 
      author={Elena Smirnova and Elvis Dohmatob and Jérémie Mary},
      year={2019},
      eprint={1902.08708},
      archivePrefix={arXiv},
      primaryClass={stat.ML}
}


@InProceedings{Clement2021,
  title = 	 {First-Order Methods for Wasserstein Distributionally Robust MDP},
  author =       {Clement, Julien Grand and Kroer, Christian},
  booktitle = 	 {Proceedings of the 38th International Conference on Machine Learning},
  pages = 	 {2010--2019},
  year = 	 {2021},
  editor = 	 {Meila, Marina and Zhang, Tong},
  volume = 	 {139},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {18--24 Jul},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v139/clement21a/clement21a.pdf},
  url = 	 {https://proceedings.mlr.press/v139/clement21a.html},
  abstract = 	 {Markov decision processes (MDPs) are known to be sensitive to parameter specification. Distributionally robust MDPs alleviate this issue by allowing for \textit{ambiguity sets} which give a set of possible distributions over parameter sets. The goal is to find an optimal policy with respect to the worst-case parameter distribution. We propose a framework for solving Distributionally robust MDPs via first-order methods, and instantiate it for several types of Wasserstein ambiguity sets. By developing efficient proximal updates, our algorithms achieve a convergence rate of $O\left(NA^{2.5}S^{3.5}\log(S)\log(\epsilon^{-1})\epsilon^{-1.5} \right)$ for the number of kernels $N$ in the support of the nominal distribution, states $S$, and actions $A$; this rate varies slightly based on the Wasserstein setup. Our dependence on $N,A$ and $S$ is significantly better than existing methods, which have a complexity of $O\left(N^{3.5}A^{3.5}S^{4.5}\log^{2}(\epsilon^{-1}) \right)$. Numerical experiments show that our algorithm is significantly more scalable than state-of-the-art approaches across several domains.}
}


@InProceedings{Zhou2021OfflineRobustRL,
  title = 	 { Finite-Sample Regret Bound for Distributionally Robust Offline Tabular Reinforcement Learning },
  author =       {Zhou, Zhengqing and Zhou, Zhengyuan and Bai, Qinxun and Qiu, Linhai and Blanchet, Jose and Glynn, Peter},
  booktitle = 	 {Proceedings of The 24th International Conference on Artificial Intelligence and Statistics},
  pages = 	 {3331--3339},
  year = 	 {2021},
  editor = 	 {Banerjee, Arindam and Fukumizu, Kenji},
  volume = 	 {130},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {13--15 Apr},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v130/zhou21d/zhou21d.pdf},
  url = 	 {https://proceedings.mlr.press/v130/zhou21d.html},
  abstract = 	 { While reinforcement learning has witnessed tremendous success recently in a wide range of domains, robustness–or the lack thereof–remains an important issue that remains inadequately addressed. In this paper, we provide a distributionally robust formulation of offline learning policy in tabular RL that aims to learn a policy from historical data (collected by some other behavior policy) that is robust to the future environment arising as a perturbation of the training environment. We first develop a novel policy evaluation scheme that accurately estimates the robust value (i.e. how robust it is in a perturbed environment) of any given policy and establish its finite-sample estimation error. Building on this, we then develop a novel and minimax-optimal distributionally robust learning algorithm that achieves $O_P\left(1/\sqrt{n}\right)$ regret, meaning that with high probability, the policy learned from using $n$ training data points will be $O\left(1/\sqrt{n}\right)$ close to the optimal distributionally robust policy. Finally, our simulation results demonstrate the superiority of our distributionally robust approach compared to non-robust RL algorithms. }
}


@mastersthesis{Scinocca2022,
  author       = {Stephen Scinocca}, 
  title        = {Group-Envy Fairness in the Stochastic Bandit Setting},
  school       = {University of Victoria},
  year         = {2022},
  month        = {09},
  url         = {https://dspace.library.uvic.ca/handle/1828/14279}
}

@article{Freund1997,
    author = "Freund, Yoav and Schapire, Robert E.",
    title = "{A Decision-Theoretic Generalization of On-Line Learning and an Application to Boosting}",
    doi = "10.1006/jcss.1997.1504",
    journal = "Journal of Computer and System Sciences",
    volume = "55",
    number = "1",
    pages = "119--139",
    year = "1997"
}


@article{Freund1999,
title = "Adaptive Game Playing Using Multiplicative Weights",
abstract = "We present a simple algorithm for playing a repeated game. We show that a player using this algorithm suffers average loss that is guaranteed to come close to the minimum loss achievable by any fixed strategy. Our bounds are nonasymptotic and hold for any opponent. The algorithm, which uses the multiplicative-weight methods of Littlestone and Warmuth, is analyzed using the Kullback-Liebler divergence. This analysis yields a new, simple proof of the min-max theorem, as well as a provable method of approximately solving a game. A variant of our game-playing algorithm is proved to be optimal in a very strong sense. Journal of Economic Literature Classification Numbers: C44, C70, D83.",
author = "Yoav Freund and Schapire, {Robert E.}",
year = "1999",
month = oct,
doi = "10.1006/game.1999.0738",
language = "English (US)",
volume = "29",
pages = "79--103",
journal = "Games and Economic Behavior",
issn = "0899-8256",
publisher = "Academic Press Inc.",
number = "1",

}

@article{Kivinen1997EG,
author = {Kivinen, Jyrki and Warmuth, Manfred K.},
title = {Exponentiated Gradient versus Gradient Descent for Linear Predictors},
year = {1997},
issue_date = {Jan. 10, 1997},
publisher = {Academic Press, Inc.},
address = {USA},
volume = {132},
number = {1},
issn = {0890-5401},
url = {https://doi.org/10.1006/inco.1996.2612},
doi = {10.1006/inco.1996.2612},
journal = {Inf. Comput.},
month = {jan},
pages = {1–63},
numpages = {63}
}

@book{ShaiAndShaiBook2014,
author = {Shalev-Shwartz, Shai and Ben-David, Shai},
title = {Understanding Machine Learning: From Theory to Algorithms},
year = {2014},
isbn = {1107057132},
publisher = {Cambridge University Press},
address = {USA},
abstract = {Machine learning is one of the fastest growing areas of computer science, with far-reaching applications. The aim of this textbook is to introduce machine learning, and the algorithmic paradigms it offers, in a principled way. The book provides an extensive theoretical account of the fundamental ideas underlying machine learning and the mathematical derivations that transform these principles into practical algorithms. Following a presentation of the basics of the field, the book covers a wide array of central topics that have not been addressed by previous textbooks. These include a discussion of the computational complexity of learning and the concepts of convexity and stability; important algorithmic paradigms including stochastic gradient descent, neural networks, and structured output learning; and emerging theoretical concepts such as the PAC-Bayes approach and compression-based bounds. Designed for an advanced undergraduate or beginning graduate course, the text makes the fundamentals and algorithms of machine learning accessible to students and non-expert readers in statistics, computer science, mathematics, and engineering.}
}
@inproceedings{Neu2015ExploreNM,
 author = {Neu, Gergely},
 booktitle = {Advances in Neural Information Processing Systems},
 pages = {},
 title = {Explore no more: Improved high-probability regret bounds for non-stochastic bandits},
 url = {https://proceedings.neurips.cc/paper_files/paper/2015/file/e5a4d6bf330f23a8707bb0d6001dfbe8-Paper.pdf},
 volume = {28},
 year = {2015}
}
@inproceedings{Neu2015ExploreNMWorkshop,
 author = {Neu, Gergely},
 booktitle = {European Workshop on Reinforcement Learning (EWRL)},
 pages = {},
 title = {Explore no more: Simple and tight high-probability bounds for non-stochastic bandits},
 url = {https://ewrl.wordpress.com/wp-content/uploads/2015/02/ewrl12_2015_submission_31.pdf},
 volume = {28},
 year = {2015}
}


@article{BubeckAndCesaBianchi2012BanditMonograph,
url = {http://dx.doi.org/10.1561/2200000024},
year = {2012},
volume = {5},
journal = {Foundations and Trends® in Machine Learning},
title = {Regret Analysis of Stochastic and Nonstochastic Multi-armed Bandit Problems},
doi = {10.1561/2200000024},
issn = {1935-8237},
number = {1},
pages = {1-122},
author = {Sébastien Bubeck and Nicolò Cesa-Bianchi}
}

@INPROCEEDINGS{Nguyen2020ICPR,

  author={Nguyen, Quan and Richter, Julius and Lauri, Mikko and Gerkmann, Timo and Frintrop, Simone},

  booktitle={2020 25th International Conference on Pattern Recognition (ICPR)}, 

  title={Improving mix-and-separate training in audio-visual sound source separation with an object prior}, 

  year={2020},

  volume={},

  number={},

  pages={5844-5851},

  doi={10.1109/ICPR48806.2021.9412174}}
  
 @InProceedings{Nguyen2019DDMMDPM,
author="Nguyen, Quan
and Lauri, Mikko
and Frintrop, Simone",
editor="Nayak, Abhaya C.
and Sharma, Alok",
title="Distance Dependent Maximum Margin Dirichlet Process Mixture",
booktitle="PRICAI 2019: Trends in Artificial Intelligence",
year="2019",
publisher="Springer International Publishing",
address="Cham",
pages="188--200",
abstract="We propose distance dependent maximum margin Dirichlet Process Mixture (STANDPM), a nonparametric Bayesian clustering model that combines distance-based priors with the discriminatively learned likelihood of the Maximum Margin Dirichlet Process Mixture. STANDPM generalizes the distance-based prior introduced in the distance dependent Chinese Restaurant Process for non-sequential distances and allows modeling of complex dependencies between data points and clusters. The generalized distance-based prior is formulated as an abstract similarity measurement between a data point and a cluster. Empirical results show that the STANDPM model with abstract similarity achieves state-of-the-art performances on a number of challenging clustering datasets.",
isbn="978-3-030-29911-8"
}


@article{BlumAndMansour2007a,
author = {Blum, Avrim and Mansour, Yishay},
title = {From External to Internal Regret},
year = {2007},
issue_date = {12/1/2007},
publisher = {JMLR.org},
volume = {8},
issn = {1532-4435},
abstract = {External regret compares the performance of an online algorithm, selecting among N actions, to the performance of the best of those actions in hindsight. Internal regret compares the loss of an online algorithm to the loss of a modified online algorithm, which consistently replaces one action by another.In this paper we give a simple generic reduction that, given an algorithm for the external regret problem, converts it to an efficient online algorithm for the internal regret problem. We provide methods that work both in the full information model, in which the loss of every action is observed at each time step, and the partial information (bandit) model, where at each time step only the loss of the selected action is observed. The importance of internal regret in game theory is due to the fact that in a general game, if each player has sublinear internal regret, then the empirical frequencies converge to a correlated equilibrium.For external regret we also derive a quantitative regret bound for a very general setting of regret, which includes an arbitrary set of modification rules (that possibly modify the online algorithm) and an arbitrary set of time selection functions (each giving different weight to each time step). The regret for a given time selection and modification rule is the difference between the cost of the online algorithm and the cost of the modified online algorithm, where the costs are weighted by the time selection function. This can be viewed as a generalization of the previously-studied sleeping experts setting.},
journal = {Journal of Machine Learning Research},
month = {dec},
pages = {1307–1324},
numpages = {18}
}


@article{Cesa-Bianchi2007Prod, 
title={Improved second-order bounds for prediction with expert advice}, 
volume={66}, 
url={https://doi.org/10.1007/s10994-006-5001-7}, 
DOI={10.1007/s10994-006-5001-7}, 
number={2–3}, 
journal={Machine Learning}, 
publisher={Springer Science+Business Media}, 
author={Cesa-Bianchi, Nicolò and Mansour, Yishay and Stoltz, Gilles}, 
year={2007}, 
month={Mar}, 
pages={321–352} 
}

@article{Gaillard2014SecondOrderBoundExcessLosses,
author = {Gaillard, Pierre and Stoltz, Gilles and Erven, Tim},
year = {2014},
month = {02},
pages = {},
title = {A Second-order Bound with Excess Losses},
volume = {35},
journal = {Journal of Machine Learning Research}
}

@inproceedings{Audibert2009minimax,
author = {Audibert, Jean-Yves and Bubeck, Sébastien},
title = {Minimax Policies for Adversarial and Stochastic Bandits},
booktitle = {Proceedings of the 22nd Annual Conference on Learning Theory (COLT)},
year = {2009},
month = {January},
abstract = {We fill in a long open gap in the characterization of the minimax rate for the multi-armed bandit problem. Concretely, we remove an extraneous logarithmic factor in the previously known upper bound and propose a new family of randomized algorithms based on an implicit normalization, as well as a new analysis. We also consider the stochastic case, and prove that an appropriate modification of the upper confidence bound policy UCB1 (Auer et al., 2002) achieves the distribution-free optimal rate while still having a distribution-dependent rate logarithmic in the number of plays.},
url = {https://www.microsoft.com/en-us/research/publication/minimax-policies-adversarial-stochastic-bandits/},
edition = {Proceedings of the 22nd Annual Conference on Learning Theory (COLT)}
}

@article{OrabonaIntroToOnlineLearningBook,
  author       = {Francesco Orabona},
  title        = {A Modern Introduction to Online Learning},
  journal      = {CoRR},
  volume       = {abs/1912.13213},
  year         = {2023},
  url          = {http://arxiv.org/abs/1912.13213},
  eprinttype    = {arXiv},
  eprint       = {1912.13213},
  timestamp    = {Sat, 04 Jan 2020 19:40:16 +0100},
  biburl       = {https://dblp.org/rec/journals/corr/abs-1912-13213.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{Abernethy2015Fighting,
 author = {Abernethy, Jacob D and Lee, Chansoo and Tewari, Ambuj},
 booktitle = {Advances in Neural Information Processing Systems},
 pages = {},
 title = {Fighting Bandits with a New Kind of Smoothness},
 url = {https://proceedings.neurips.cc/paper_files/paper/2015/file/5caf41d62364d5b41a893adc1a9dd5d4-Paper.pdf},
 volume = {28},
 year = {2015}
}

@inproceedings{Luo2015AdaNormalHedge,
  title={Achieving All with No Parameters: Ada{N}ormal{H}edge},
  author={Haipeng Luo and Robert E. Schapire},
  booktitle={Annual Conference Computational Learning Theory},
  year={2015},
  url={https://api.semanticscholar.org/CorpusID:11088893}
}
@misc{Luo2017CSCI699LectureNote13,
  author        = {Heipeng Luo},
  title         = {Lecture 13, {I}ntroduction to {O}nline {L}earning},
  month         = {October},
  year          = {2017},
  howpublished           = {\url{https://haipeng-luo.net/courses/CSCI699/lecture13.pdf}},
  publisher={University of Southern California}
}


@InProceedings{Daniely2015StronglyAdaptiveOL,
  title = 	 {Strongly Adaptive Online Learning},
  author = 	 {Daniely, Amit and Gonen, Alon and Shalev-Shwartz, Shai},
  booktitle = 	 {Proceedings of the 32nd International Conference on Machine Learning},
  pages = 	 {1405--1411},
  year = 	 {2015},
  volume = 	 {37},
  address = 	 {Lille, France},
  month = 	 {07--09 Jul},
  pdf = 	 {http://proceedings.mlr.press/v37/daniely15.pdf},
  url = 	 {https://proceedings.mlr.press/v37/daniely15.html},
  abstract = 	 {Strongly adaptive algorithms are algorithms whose performance on every time interval is close to optimal. We present a reduction that can transform standard low-regret algorithms to strongly adaptive. As a consequence, we derive simple, yet efficient, strongly adaptive algorithms for a handful of problems.}
}


@INPROCEEDINGS{Bouneffouf2020MABApplication,

  author={Bouneffouf, Djallel and Rish, Irina and Aggarwal, Charu},

  booktitle={2020 IEEE Congress on Evolutionary Computation (CEC)}, 

  title={Survey on Applications of Multi-Armed and Contextual Bandits}, 

  year={2020},

  volume={},

  number={},

  pages={1-8},

  doi={10.1109/CEC48606.2020.9185782}}


@article{Kleinberg2010Sleeping,
author = {Kleinberg, Robert and Niculescu-Mizil, Alexandru and Sharma, Yogeshwer},
title = {Regret Bounds for Sleeping Experts and Bandits},
year = {2010},
issue_date = {September 2010},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {80},
number = {2–3},
issn = {0885-6125},
url = {https://doi.org/10.1007/s10994-010-5178-7},
doi = {10.1007/s10994-010-5178-7},
abstract = {We study on-line decision problems where the set of actions that are available to the decision algorithm varies over time. With a few notable exceptions, such problems remained largely unaddressed in the literature, despite their applicability to a large number of practical problems. Departing from previous work on this "Sleeping Experts" problem, we compare algorithms against the payoff obtained by the best ordering of the actions, which is a natural benchmark for this type of problem. We study both the full-information (best expert) and partial-information (multi-armed bandit) settings and consider both stochastic and adversarial rewards models. For all settings we give algorithms achieving (almost) information-theoretically optimal regret bounds (up to a constant or a sub-logarithmic factor) with respect to the best-ordering benchmark.},
journal = {Machine Learning},
month = {sep},
pages = {245–272},
numpages = {28},
keywords = {Regret, Online algorithms, Computational learning theory}
}

@inproceedings{KaleNIPS2016SleepingCombinatorialOpt,
 author = {Kale, Satyen and Lee, Chansoo and Pal, David},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {D. Lee and M. Sugiyama and U. Luxburg and I. Guyon and R. Garnett},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {Hardness of Online Sleeping Combinatorial Optimization Problems},
 url = {https://proceedings.neurips.cc/paper_files/paper/2016/file/184260348236f9554fe9375772ff966e-Paper.pdf},
 volume = {29},
 year = {2016}
}

@InProceedings{Gaillard2023OneArrowTwoKills,
  title = 	 {One Arrow, Two Kills: A Unified Framework for Achieving Optimal Regret Guarantees in Sleeping Bandits},
  author =       {Gaillard, Pierre and Saha, Aadirupa and Dan, Soham},
  booktitle = 	 {Proceedings of The 26th International Conference on Artificial Intelligence and Statistics},
  pages = 	 {7755--7773},
  year = 	 {2023},
  volume = 	 {206},
  month = 	 {25--27 Apr},
  pdf = 	 {https://proceedings.mlr.press/v206/gaillard23a/gaillard23a.pdf},
  url = 	 {https://proceedings.mlr.press/v206/gaillard23a.html},
  abstract = 	 {We address the problem of Internal Regret in adversarial Sleeping Bandits and the relationship between different notions of sleeping regrets in multi-armed bandits. We propose a new concept called Internal Regret for sleeping multi-armed bandits (MAB) and present an algorithm that achieves sublinear Internal Regret, even when losses and availabilities are both adversarial. We demonstrate that a low internal regret leads to both low external regret and low policy regret for i.i.d. losses. Our contribution is unifying existing notions of regret in sleeping bandits and exploring their implications for each other. In addition, we extend our results to Dueling Bandits (DB), a preference feedback version of multi-armed bandits, and design a low-regret algorithm for sleeping dueling bandits with stochastic preferences and adversarial availabilities. We validate the effectiveness of our algorithms through empirical evaluations.}
}

@article{Slivkins2014,
author = {Slivkins, Aleksandrs},
title = {Contextual Bandits with Similarity Information},
year = {2014},
issue_date = {January 2014},
publisher = {JMLR.org},
volume = {15},
number = {1},
issn = {1532-4435},
abstract = {In a multi-armed bandit (MAB) problem, an online algorithm makes a sequence of choices. In each round it chooses from a time-invariant set of alternatives and receives the payoff associated with this alternative. While the case of small strategy sets is by now well-understood, a lot of recent work has focused on MAB problems with exponentially or infinitely large strategy sets, where one needs to assume extra structure in order to make the problem tractable. In particular, recent literature considered information on similarity between arms.We consider similarity information in the setting of contextual bandits, a natural extension of the basic MAB problem where before each round an algorithm is given the context--a hint about the payoffs in this round. Contextual bandits are directly motivated by placing advertisements on web pages, one of the crucial problems in sponsored search. A particularly simple way to represent similarity information in the contextual bandit setting is via a similarity distance between the context-arm pairs which bounds from above the difference between the respective expected payoffs.Prior work on contextual bandits with similarity uses "uniform" partitions of the similarity space, so that each context-arm pair is approximated by the closest pair in the partition. Algorithms based on "uniform" partitions disregard the structure of the payoffs and the context arrivals, which is potentially wasteful. We present algorithms that are based on adaptive partitions, and take advantage of "benign" payoffs and context arrivals without sacrificing the worst-case performance. The central idea is to maintain a finer partition in high-payoff regions of the similarity space and in popular regions of the context space. Our results apply to several other settings, e.g., MAB with constrained temporal change (Slivkins and Upfal, 2008) and sleeping bandits (Kleinberg et al., 2008a).},
journal = {Journal of Machine Learning Research},
month = {jan},
pages = {2533–2568},
numpages = {36},
keywords = {metric space, contextual bandits, regret, Lipschitz-continuity, multi-armed bandits}
}

@article{Slivkins2013,
  author       = {Aleksandrs Slivkins},
  title        = {Dynamic Ad Allocation: Bandits with Budgets},
  journal      = {CoRR},
  volume       = {abs/1306.0155},
  year         = {2013},
  url          = {http://arxiv.org/abs/1306.0155},
  eprinttype    = {arXiv},
  eprint       = {1306.0155},
  timestamp    = {Mon, 13 Aug 2018 16:47:27 +0200},
  biburl       = {https://dblp.org/rec/journals/corr/Slivkins13.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{Freund1997SpecializedExperts,
author = {Freund, Yoav and Schapire, Robert E. and Singer, Yoram and Warmuth, Manfred K.},
title = {Using and Combining Predictors That Specialize},
year = {1997},
isbn = {0897918886},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/258533.258616},
doi = {10.1145/258533.258616},
booktitle = {Proceedings of the Twenty-Ninth Annual ACM Symposium on Theory of Computing},
pages = {334–343},
numpages = {10},
location = {El Paso, Texas, USA},
series = {STOC '97}
}

@article{Blum1997CalendarScheduling,
author={Blum, Avrim},
title={Empirical Support for Winnow and Weighted-Majority Algorithms: Results on a Calendar Scheduling Domain},
journal={Machine Learning},
year={1997},
month={Jan},
day={01},
volume={26},
number={1},
pages={5-23},
abstract={This paper describes experimental results on using Winnow and Weighted-Majority based algorithms on a real-world calendar scheduling domain. These two algorithms have been highly studied in the theoretical machine learning literature. We show here that these algorithms can be quite competitive practically, outperforming the decision-tree approach currently in use in the Calendar Apprentice system in terms of both accuracy and speed. One of the contributions of this paper is a new variant on the Winnow algorithm (used in the experiments) that is especially suited to conditions with string-valued classifications, and we give a theoretical analysis of its performance. In addition we show how Winnow can be applied to achieve a good accuracy/coverage tradeoff and explore issues that arise such as concept drift. We also provide an analysis of a policy for discarding predictors in Weighted-Majority that allows it to speed up as it learns.},
issn={1573-0565},
doi={10.1023/A:1007335615132},
url={https://doi.org/10.1023/A:1007335615132}
}

@InProceedings{Vovk2005SecondGuessing,
author="Vovk, Vladimir",
editor="Jain, Sanjay
and Simon, Hans Ulrich
and Tomita, Etsuji",
title="Defensive Prediction with Expert Advice",
booktitle="Algorithmic Learning Theory",
year="2005",
publisher="Springer Berlin Heidelberg",
address="Berlin, Heidelberg",
pages="444--458",
abstract="The theory of prediction with expert advice usually deals with countable or finite-dimensional pools of experts. In this paper we give similar results for pools of decision rules belonging to an infinite-dimensional functional space which we call the Fermi--Sobolev space. For example, it is shown that for a wide class of loss functions (including the standard square, absolute, and log loss functions) the average loss of the master algorithm, over the first N steps, does not exceed the average loss of the best decision rule with a bounded Fermi--Sobolev norm plus O(N−{\thinspace}−{\thinspace}1/2). Our proof techniques are very different from the standard ones and are based on recent results about defensive forecasting. Given the probabilities produced by a defensive forecasting algorithm, which are known to be well calibrated and to have high resolution in the long run, we use the Expected Loss Minimization principle to find a suitable decision.",
isbn="978-3-540-31696-1"
}


@inproceedings{Saha2020,
author = {Saha, Aadirupa and Gaillard, Pierre and Valko, Michal},
title = {Improved Sleeping Bandits with Stochastic Actions Sets and Adversarial Rewards},
year = {2020},
booktitle = {Proceedings of the 37th International Conference on Machine Learning},
articleno = {774},
numpages = {10},
series = {ICML'20}
}

@article{Kanade2014SleepingExperts,
author = {Kanade, Varun and Steinke, Thomas},
title = {Learning Hurdles for Sleeping Experts},
year = {2014},
issue_date = {July 2014},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {6},
number = {3},
issn = {1942-3454},
url = {https://doi.org/10.1145/2505983},
doi = {10.1145/2505983},
abstract = {We study the online decision problem in which the set of available actions varies over time, also called the sleeping experts problem. We consider the setting in which the performance comparison is made with respect to the best ordering of actions in hindsight. In this article, both the payoff function and the availability of actions are adversarial. Kleinberg et al. [2010] gave a computationally efficient no-regret algorithm in the setting in which payoffs are stochastic. Kanade et al. [2009] gave an efficient no-regret algorithm in the setting in which action availability is stochastic.However, the question of whether there exists a computationally efficient no-regret algorithm in the adversarial setting was posed as an open problem by Kleinberg et al. [2010]. We show that such an algorithm would imply an algorithm for PAC learning DNF, a long-standing important open problem. We also consider the setting in which the number of available actions is restricted and study its relation to agnostic-learning monotone disjunctions over examples with bounded Hamming weight.},
journal = {ACM Trans. Comput. Theory},
month = {jul},
articleno = {11},
numpages = {16},
keywords = {lower bounds, sleeping experts, Online learning}
}

@inproceedings{Neu2014CombinatorialSleepingPolicyRegret,
 author = {Neu, Gergely and Valko, Michal},
 booktitle = {Advances in Neural Information Processing Systems},
 pages = {},
 title = {Online combinatorial optimization with stochastic decision sets and adversarial losses},
 url = {https://proceedings.neurips.cc/paper_files/paper/2014/file/c930eecd01935feef55942cc445f708f-Paper.pdf},
 volume = {27},
 year = {2014}
}

@article{ChernovVovk2010UnknownNoExperts,
  author       = {Alexey V. Chernov and
                  Vladimir Vovk},
  title        = {Prediction with Advice of Unknown Number of Experts},
  journal      = {CoRR},
  volume       = {abs/1006.0475},
  year         = {2010},
  url          = {http://arxiv.org/abs/1006.0475},
  eprinttype    = {arXiv},
  eprint       = {1006.0475},
  timestamp    = {Mon, 13 Aug 2018 16:49:03 +0200},
  biburl       = {https://dblp.org/rec/journals/corr/abs-1006-0475.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@InProceedings{ChernovVovk2009ExpertEvaluatorsAdvice,
author="Chernov, Alexey
and Vovk, Vladimir",
title="Prediction with Expert Evaluators' Advice",
booktitle="Algorithmic Learning Theory",
year="2009",
address="Berlin, Heidelberg",
pages="8--22",
abstract="We introduce a new protocol for prediction with expert advice in which each expert evaluates the learner's and his own performance using a loss function that may change over time and may be different from the loss functions used by the other experts. The learner's goal is to perform better or not much worse than each expert, as evaluated by that expert, for all experts simultaneously. If the loss functions used by the experts are all proper scoring rules and all mixable, we show that the defensive forecasting algorithm enjoys the same performance guarantee as that attainable by the Aggregating Algorithm in the standard setting and known to be optimal. This result is also applied to the case of ``specialist'' experts. In this case, the defensive forecasting algorithm reduces to a simple modification of the Aggregating Algorithm.",
isbn="978-3-642-04414-4"
}


@InProceedings{Luo2018ContextualNonstationary,
  title = 	 {Efficient Contextual Bandits in Non-stationary Worlds},
  author =       {Luo, Haipeng and Wei, Chen-Yu and Agarwal, Alekh and Langford, John},
  booktitle = 	 {Proceedings of the 31st  Conference On Learning Theory},
  pages = 	 {1739--1776},
  year = 	 {2018},
  editor = 	 {Bubeck, Sébastien and Perchet, Vianney and Rigollet, Philippe},
  volume = 	 {75},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {06--09 Jul},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v75/luo18a/luo18a.pdf},
  url = 	 {https://proceedings.mlr.press/v75/luo18a.html},
  abstract = 	 {Most contextual bandit algorithms minimize regret against the best fixed policy, a questionable benchmark for non-stationary environments that are ubiquitous in applications.  In this work, we develop several efficient contextual bandit algorithms for non-stationary environments by equipping existing methods for i.i.d. problems with sophisticated statistical tests so as to dynamically adapt to a change in distribution.   We analyze various standard notions of regret suited to non-stationary environments for these algorithms, including interval regret, switching regret, and dynamic regret. When competing with the best policy at each time, one of our algorithms achieves regret $\mathcal{O}(\sqrt{ST})$ if there are $T$ rounds with $S$ stationary periods, or more generally $\mathcal{O}(\Delta^{1/3}T^{2/3})$ where $\Delta$ is some non-stationarity measure. These results almost match the optimal guarantees achieved by an inefficient baseline that is a variant of the classic Exp4 algorithm. The dynamic regret result is also the first one for efficient and fully adversarial contextual bandit. Furthermore, while the results above require tuning a parameter based on the unknown quantity $S$ or $\Delta$, we also develop a parameter free algorithm achieving regret $\min\{S^{1/4}T^{3/4}, \Delta^{1/5}T^{4/5}\}$. This improves and generalizes the best existing result $\Delta^{0.18}T^{0.82}$ by Karnin and Anava (2016) which only holds for the two-armed bandit problem.}
}


@InProceedings{Williamson2019FairnessRM,
  title = 	 {Fairness risk measures},
  author =       {Williamson, Robert and Menon, Aditya},
  booktitle = 	 {Proceedings of the 36th International Conference on Machine Learning},
  pages = 	 {6786--6797},
  year = 	 {2019},
  editor = 	 {Chaudhuri, Kamalika and Salakhutdinov, Ruslan},
  volume = 	 {97},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {09--15 Jun},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v97/williamson19a/williamson19a.pdf},
  abstract = 	 {Ensuring that classifiers are non-discriminatory or fair with respect to a sensitive feature (e.g., race or gender) is a topical problem. Progress in this task requires fixing a definition of fairness, and there have been several proposals in this regard over the past few years. Several of these, however, assume either binary sensitive features (thus precluding categorical or real-valued sensitive groups), or result in non-convex objectives (thus adversely affecting the optimisation landscape). In this paper, we propose a new definition of fairness that generalises some existing proposals, while allowing for generic sensitive features and resulting in a convex objective. The key idea is to enforce that the expected losses (or risks) across each subgroup induced by the sensitive feature are commensurate. We show how this relates to the rich literature on risk measures from mathematical finance. As a special case, this leads to a new convex fairness-aware objective based on minimising the conditional value at risk (CVaR).}
}



@InProceedings{Mohri2019,
  title = 	 {Agnostic Federated Learning},
  author =       {Mohri, Mehryar and Sivek, Gary and Suresh, Ananda Theertha},
  booktitle = 	 {Proceedings of the 36th International Conference on Machine Learning},
  pages = 	 {4615--4625},
  year = 	 {2019},
  editor = 	 {Chaudhuri, Kamalika and Salakhutdinov, Ruslan},
  volume = 	 {97},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {09--15 Jun},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v97/mohri19a/mohri19a.pdf},
  abstract = 	 {A key learning scenario in large-scale applications is that of federated learning, where a centralized model is trained based on data originating from a large number of clients. We argue that, with the existing training and inference, federated models can be biased towards different clients. Instead, we propose a new framework of agnostic federated learning, where the centralized model is optimized for any target distribution formed by a mixture of the client distributions. We further show that this framework naturally yields a notion of fairness. We present data-dependent Rademacher complexity guarantees for learning with this objective, which guide the definition of an algorithm for agnostic federated learning. We also give a fast stochastic optimization algorithm for solving the corresponding optimization problem, for which we prove convergence bounds, assuming a convex loss function and a convex hypothesis set. We further empirically demonstrate the benefits of our approach in several datasets. Beyond federated learning, our framework and algorithm can be of interest to other learning scenarios such as cloud computing, domain adaptation, drifting, and other contexts where the training and test distributions do not coincide.}
}

@book{Rawls1971theory,
 author = "Rawls, J.",
 title = "A Theory of Justice",
 publisher = "Harvard University Press",
 year = 1971
}

@inproceedings{NguyenAndMehta2024SBEXP3,
  title={Near-optimal per-action regret bounds for sleeping bandits},
  author={Nguyen, Quan M and Mehta, Nishant},
  booktitle={International Conference on Artificial Intelligence and Statistics},
  pages={2827--2835},
  year={2024},
  organization={PMLR}
}


@inproceedings{NguyenAndMehta2024GDRO,
  title={Beyond Minimax Rates in Group Distributionally Robust Optimization via a Novel Notion of Sparsity},
  author={Quan Nguyen and Nishant A. Mehta},
  booktitle={AAAI Workshop on AI for Biased or Scarce Data},
  year={2024}
}

@inproceedings{Jin2023BOBW,
	title={Improved Best-of-Both-Worlds Guarantees for Multi-Armed Bandits: {FTRL} with General Regularizers and Multiple Optimal Arms},
	author={Tiancheng Jin and Junyan Liu and Haipeng Luo},
	booktitle={Thirty-seventh Conference on Neural Information Processing Systems},
	year={2023},
	url={https://openreview.net/forum?id=wbg4JEM5Jp}
}


@InProceedings{Ito2021HybridDataMABBound,
  title = 	 {Parameter-Free Multi-Armed Bandit Algorithms with Hybrid Data-Dependent Regret Bounds},
  author =       {Ito, Shinji},
  booktitle = 	 {Proceedings of Thirty Fourth Conference on Learning Theory},
  year = 	 {2021},
  pdf = 	 {http://proceedings.mlr.press/v134/ito21a/ito21a.pdf}
}

@article{Auer2002a,
author={Auer, Peter
and Cesa-Bianchi, Nicol{\`o}
and Fischer, Paul},
title={Finite-time Analysis of the Multiarmed Bandit Problem},
journal={Machine Learning},
year={2002},
month={May},
day={01},
volume={47},
number={2},
pages={235-256},
abstract={Reinforcement learning policies face the exploration versus exploitation dilemma, i.e. the search for a balance between exploring the environment to find profitable actions while taking the empirically best action as often as possible. A popular measure of a policy's success in addressing this dilemma is the regret, that is the loss due to the fact that the globally optimal policy is not followed all the times. One of the simplest examples of the exploration/exploitation dilemma is the multi-armed bandit problem. Lai and Robbins were the first ones to show that the regret for this problem has to grow at least logarithmically in the number of plays. Since then, policies which asymptotically achieve this regret have been devised by Lai and Robbins and many others. In this work we show that the optimal logarithmic regret is also achievable uniformly over time, with simple and efficient policies, and for all reward distributions with bounded support.},
issn={1573-0565},
doi={10.1023/A:1013689704352}
}

@inproceedings{Kuroki2024bestofbothworlds,
      title={Best-of-Both-Worlds Algorithms for Linear Contextual Bandits}, 
      author={Yuko Kuroki and Alberto Rumi and Taira Tsuchiya and Fabio Vitale and Nicolò Cesa-Bianchi},
      year={2024},
	booktitle={International Conference on Artificial Intelligence and Statistics (AISTATS)}
}

@inproceedings{Wang2022TwoLayerReLUHiddenConvex,
	title={The Hidden Convex Optimization Landscape of Regularized Two-Layer {ReLU} Networks: an Exact Characterization of Optimal Solutions},
	author={Yifei Wang and Jonathan Lacotte and Mert Pilanci},
	booktitle={International Conference on Learning Representations},
	year={2022},
	url={https://openreview.net/forum?id=Z7Lk2cQEG8a}
}


@InProceedings{Mishkin2022FastOptTwoLayerReLU,
  title = 	 {Fast Convex Optimization for Two-Layer {R}e{LU} Networks: Equivalent Model Classes and Cone Decompositions},
  author =       {Mishkin, Aaron and Sahiner, Arda and Pilanci, Mert},
  booktitle = 	 {Proceedings of the 39th International Conference on Machine Learning},
  year = 	 {2022},
  pdf = 	 {https://proceedings.mlr.press/v162/mishkin22a/mishkin22a.pdf},
  abstract = 	 {We develop fast algorithms and robust software for convex optimization of two-layer neural networks with ReLU activation functions. Our work leverages a convex re-formulation of the standard weight-decay penalized training problem as a set of group-l1-regularized data-local models, where locality is enforced by polyhedral cone constraints. In the special case of zero-regularization, we show that this problem is exactly equivalent to unconstrained optimization of a convex "gated ReLU" network. For problems with non-zero regularization, we show that convex gated ReLU models obtain data-dependent approximation bounds for the ReLU training problem. To optimize the convex re-formulations, we develop an accelerated proximal gradient method and a practical augmented Lagrangian solver. We show that these approaches are faster than standard training heuristics for the non-convex problem, such as SGD, and outperform commercial interior-point solvers. Experimentally, we verify our theoretical results, explore the group-l1 regularization path, and scale convex optimization for neural networks to image classification on MNIST and CIFAR-10.}
}

@misc{Chen2024DynamicsThreeLayer,
      title={On the dynamics of three-layer neural networks: initial condensation}, 
      author={Zheng-An Chen and Tao Luo},
      year={2024},
      eprint={2402.15958},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@inproceedings{Kawaguchi2016DLwithoutPoorMinima,
title={Deep Learning without Poor Local Minima},
author={Kawaguchi, Kenji},
booktitle={Advances in neural information processing systems (NeurIPS)},
pages={586--594},
year={2016}
}

@inproceedings{Vardi2022SampleComplexityForTwoLayerNets,
title={The Sample Complexity of One-Hidden-Layer Neural Networks},
author={Gal Vardi and Ohad Shamir and Nathan Srebro},
booktitle={Advances in Neural Information Processing Systems},
editor={Alice H. Oh and Alekh Agarwal and Danielle Belgrave and Kyunghyun Cho},
year={2022},
url={https://openreview.net/forum?id=DI3hGYPwfT}
}

@ARTICLE{Chang2011BoundGaussianError,

  author={Chang, Seok-Ho and Cosman, Pamela C. and Milstein, Laurence B.},

  journal={IEEE Transactions on Communications}, 

  title={Chernoff-Type Bounds for the Gaussian Error Function}, 

  year={2011},

  volume={59},

  number={11},

  pages={2939-2944},

  keywords={Upper bound;Fading channels;Approximation methods;Error probability;Signal to noise ratio;Communication systems;Bounds;error function;exponential;Gaussian Q-function},

  doi={10.1109/TCOMM.2011.072011.100049}}


@InProceedings{Ding2022GOCofMomentumPG,
  title = 	 { On the Global Optimum Convergence of Momentum-based Policy Gradient },
  author =       {Ding, Yuhao and Zhang, Junzi and Lavaei, Javad},
  booktitle = 	 {Proceedings of The 25th International Conference on Artificial Intelligence and Statistics},
  pages = 	 {1910--1934},
  year = 	 {2022},
  editor = 	 {Camps-Valls, Gustau and Ruiz, Francisco J. R. and Valera, Isabel},
  volume = 	 {151},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {28--30 Mar},
  publisher =    {PMLR},
  pdf = 	 {https://proceedings.mlr.press/v151/ding22a/ding22a.pdf},
  url = 	 {https://proceedings.mlr.press/v151/ding22a.html},
  abstract = 	 { Policy gradient (PG) methods are popular and efficient for large-scale reinforcement learning due to their relative stability and incremental nature. In recent years, the empirical success of PG methods has led to the development of a theoretical foundation for these methods. In this work, we generalize this line of research by establishing the first set of global convergence results of stochastic PG methods with momentum terms, which have been demonstrated to be efficient recipes for improving PG methods. We study both the soft-max and the Fisher-non-degenerate policy parametrizations, and show that adding a momentum term improves the global optimality sample complexities of vanilla PG methods by $\tilde{\mathcal{O}}(\epsilon^{-1.5})$ and $\tilde{\mathcal{O}}(\epsilon^{-1})$, respectively, where $\epsilon&gt;0$ is the target tolerance. Our results for the generic Fisher-non-degenerate policy parametrizations also provide the first single-loop and finite-batch PG algorithm achieving an $\tilde{O}(\epsilon^{-3})$ global optimality sample complexity. Finally, as a by-product, our analyses provide general tools for deriving the global convergence rates of stochastic PG methods, which can be readily applied and extended to other PG estimators under the two parametrizations. }
}

@misc{Chen2023MABUnbounded,
      title={Improved Algorithms for Adversarial Bandits with Unbounded Losses}, 
      author={Mingyu Chen and Xuezhou Zhang},
      year={2023},
      eprint={2310.01756},
      archivePrefix={arXiv},
      primaryClass={stat.ML},
      url={https://arxiv.org/abs/2310.01756}, 
}

@article{Audibert10a,
  author  = {Jean-Yves Audibert and S{{\'e}}bastien Bubeck},
  title   = {Regret Bounds and Minimax Policies under Partial Monitoring},
  journal = {Journal of Machine Learning Research},
  year    = {2010},
  volume  = {11},
  number  = {94},
  pages   = {2785--2836},
  url     = {http://jmlr.org/papers/v11/audibert10a.html}
}


@InProceedings{Bubeck2018ALT,
  title = 	 {Sparsity, variance and curvature in multi-armed bandits},
  author = 	 {Bubeck, Sébastien and Cohen, Michael and Li, Yuanzhi},
  booktitle = 	 {Proceedings of Algorithmic Learning Theory},
  pages = 	 {111--127},
  year = 	 {2018},
  editor = 	 {Janoos, Firdaus and Mohri, Mehryar and Sridharan, Karthik},
  volume = 	 {83},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {07--09 Apr},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v83/bubeck18a/bubeck18a.pdf},
  url = 	 {https://proceedings.mlr.press/v83/bubeck18a.html},
  abstract = 	 {In (online) learning theory the concepts of sparsity, variance and curvature are well-understood and are routinely used to obtain refined regret and generalization bounds. In this paper we further our understanding of these concepts in the more challenging limited feedback scenario. We consider the adversarial multi-armed bandit and linear bandit settings and solve several open problems pertaining to the existence of algorithms with favorable regret bounds under the following assumptions: (i) sparsity of the individual losses, (ii) small variation of the loss sequence, and (iii) curvature of the action set. Specifically we show that (i) for $s$-sparse losses one can obtain $\tilde{O}(\sqrt{s T})$-regret (solving an open problem by Kwon and Perchet), (ii) for loss sequences with variation bounded by $Q$ one can obtain $\tilde{O}(\sqrt{Q})$-regret (solving an open problem by Kale and Hazan), and (iii) for linear bandit on an $\ell_p^n$ ball one can obtain $\tilde{O}(\sqrt{n T})$-regret for $p ∈[1,2]$ and one has $\tilde{Ω}(n \sqrt{T})$-regret for $p&gt;2$ (solving an open problem by Bubeck, Cesa-Bianchi and Kakade). A key new insight to obtain these results is to use regularizers satisfying more refined conditions than general self-concordance.}
}

@article{KwonAndPerchet2016JMLRSparsity,
  author  = {Joon Kwon and Vianney Perchet},
  title   = {Gains and Losses are Fundamentally Different in Regret Minimization: The Sparse Case},
  journal = {Journal of Machine Learning Research},
  year    = {2016},
  volume  = {17},
  number  = {227},
  pages   = {1--32},
  url     = {http://jmlr.org/papers/v17/15-503.html}
}

@InProceedings{Beygelzimer2015SVR,
  title = 	 {Contextual Bandit Algorithms with Supervised Learning Guarantees},
  author = 	 {Beygelzimer, Alina and Langford, John and Li, Lihong and Reyzin, Lev and Schapire, Robert},
  booktitle = 	 {Proceedings of the Fourteenth International Conference on Artificial Intelligence and Statistics},
  pages = 	 {19--26},
  year = 	 {2011},
  editor = 	 {Gordon, Geoffrey and Dunson, David and Dudík, Miroslav},
  volume = 	 {15},
  series = 	 {Proceedings of Machine Learning Research},
  address = 	 {Fort Lauderdale, FL, USA},
  month = 	 {11--13 Apr},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v15/beygelzimer11a/beygelzimer11a.pdf},
  url = 	 {https://proceedings.mlr.press/v15/beygelzimer11a.html},
  abstract = 	 {We address the problem of competing with any large set of $N$ policies in the non-stochastic bandit setting, where the learner must repeatedly select among $K$ actions but observes only the reward of the chosen action. We present a modification of the Exp4 algorithm of Auer et al., called Exp4.P, which with high probability incurs regret at most $O(\sqrt{KT\ln N})$.  Such a bound does not hold for Exp4 due to the large variance of the importance-weighted estimates used in the algorithm. The new algorithm is tested empirically in a large-scale, real-world dataset.  For the stochastic version of the problem, we can use Exp4.P as a subroutine to compete with a possibly infinite set of policies of VC-dimension d while incurring regret at most $O(\sqrt{Td\ln T})$ with high probability.  These guarantees improve on those of all previous algorithms, whether in a stochastic or adversarial environment, and bring us closer to providing guarantees for this setting that are comparable to those in standard supervised learning.}
}

@inproceedings{Lee2020BiasNoMore,
 author = {Lee, Chung-Wei and Luo, Haipeng and Wei, Chen-Yu and Zhang, Mengxiao},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {H. Larochelle and M. Ranzato and R. Hadsell and M.F. Balcan and H. Lin},
 pages = {15522--15533},
 publisher = {Curran Associates, Inc.},
 title = {Bias no more: high-probability data-dependent regret bounds for adversarial bandits and MDPs},
 url = {https://proceedings.neurips.cc/paper_files/paper/2020/file/b2ea5e977c5fc1ccfa74171a9723dd61-Paper.pdf},
 volume = {33},
 year = {2020}
}


@InProceedings{ItoCOLT2024,
  title = 	 {Adaptive Learning Rate for Follow-the-Regularized-Leader: Competitive Analysis and Best-of-Both-Worlds},
  author =       {Ito, Shinji and Tsuchiya, Taira and Honda, Junya},
  booktitle = 	 {Proceedings of Thirty Seventh Conference on Learning Theory},
  pages = 	 {2522--2563},
  year = 	 {2024},
  editor = 	 {Agrawal, Shipra and Roth, Aaron},
  volume = 	 {247},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {30 Jun--03 Jul},
  publisher =    {PMLR},
  pdf = 	 {https://proceedings.mlr.press/v247/ito24a/ito24a.pdf},
  url = 	 {https://proceedings.mlr.press/v247/ito24a.html},
  abstract = 	 {Follow-The-Regularized-Leader (FTRL) is known as an effective and versatile approach in online learning, where appropriate choice of the learning rate is crucial for smaller regret. To this end, we formulate the problem of adjusting FTRL’s learning rate as a sequential decision-making problem and introduce the framework of competitive analysis. We establish a lower bound for the competitive ratio and propose update rules for the learning rate that achieves an upper bound within a constant factor of this lower bound. Specifically, we illustrate that the optimal competitive ratio is characterized by the (approximate) monotonicity of components of the penalty term, showing that a constant competitive ratio is achievable if the components of the penalty term form a monotone non-increasing sequence, and derive a tight competitive ratio when penalty terms are $\xi$-approximately monotone non-increasing. Our proposed update rule, referred to as \textit{stability-penalty matching}, also facilitates the construction of Best-Of-Both-Worlds (BOBW) algorithms for stochastic and adversarial environments. In these environments our results contribute to achieving tighter regret bound and broaden the applicability of algorithms for various settings such as multi-armed bandits, graph bandits, linear bandits, and contextual bandits.}
}

@inproceedings{Tsuchiya2023stabilitypenaltyadaptive,
title={Stability-penalty-adaptive follow-the-regularized-leader: Sparsity, game-dependency, and best-of-both-worlds},
author={Taira Tsuchiya and Shinji Ito and Junya Honda},
booktitle={Thirty-seventh Conference on Neural Information Processing Systems},
year={2023},
url={https://openreview.net/forum?id=J3taqrzyyA}
}


@article{Zimmert2021TsallisINF,
author = {Zimmert, Julian and Seldin, Yevgeny},
title = {Tsallis-{INF}: An optimal algorithm for stochastic and adversarial bandits},
year = {2021},
issue_date = {January 2021},
publisher = {JMLR.org},
volume = {22},
number = {1},
issn = {1532-4435},
abstract = {We derive an algorithm that achieves the optimal (within constants) pseudo-regret in both adversarial and stochastic multi-armed bandits without prior knowledge of the regime and time horizon. The algorithm is based on online mirror descent (OMD) with Tsallis entropy regularization with power α = 1/2 and reduced-variance loss estimators. More generally, we define an adversarial regime with a self-bounding constraint, which includes stochastic regime, stochastically constrained adversarial regime (Wei and Luo, 2018), and stochastic regime with adversarial corruptions (Lykouris et al., 2018) as special cases, and show that the algorithm achieves logarithmic regret guarantee in this regime and all of its special cases simultaneously with the optimal regret guarantee in the adversarial regime. The algorithm also achieves adversarial and stochastic optimality in the utility-based dueling bandit setting. We provide empirical evaluation of the algorithm demonstrating that it significantly outperforms Ucb1 and Exp3 in stochastic environments. We also provide examples of adversarial environments, where Ucb1 and Thompson Sampling exhibit almost linear regret, whereas our algorithm suffers only logarithmic regret. To the best of our knowledge, this is the first example demonstrating vulnerability of Thompson Sampling in adversarial environments. Last but not least, we present a general stochastic analysis and a general adversarial analysis of OMD algorithms with Tsallis entropy regularization for α ε [0; 1] and explain the reason why α = 1/2 works best.},
journal = {J. Mach. Learn. Res.},
month = jan,
articleno = {28},
numpages = {49},
keywords = {bandits, online learning, best of both worlds, online mirror descent, tsallis entropy, multi-armed bandits, stochastic, adversarial, I.I.D.}
}


@InProceedings{Ashutosh21a,
  title = 	 { Bandit algorithms: Letting go of logarithmic regret for statistical robustness },
  author =       {Ashutosh, Kumar and Nair, Jayakrishnan and Kagrecha, Anmol and Jagannathan, Krishna},
  booktitle = 	 {Proceedings of The 24th International Conference on Artificial Intelligence and Statistics},
  pages = 	 {622--630},
  year = 	 {2021},
  editor = 	 {Banerjee, Arindam and Fukumizu, Kenji},
  volume = 	 {130},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {13--15 Apr},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v130/ashutosh21a/ashutosh21a.pdf},
  url = 	 {https://proceedings.mlr.press/v130/ashutosh21a.html},
  abstract = 	 { We study regret minimization in a stochastic multi-armed bandit setting, and establish a fundamental trade-off between the regret suffered under an algorithm, and its statistical robustness. Considering broad classes of underlying arms’ distributions, we show that bandit learning algorithms with logarithmic regret are always inconsistent and that consistent learning algorithms always suffer a super-logarithmic regret. This result highlights the inevitable statistical fragility of all ‘logarithmic regret’ bandit algorithms available in the literature - for instance, if a UCB algorithm designed for 1-subGaussian distributions is used in a subGaussian setting with a mismatched variance parameter, the learning performance could be inconsistent. Next, we show a positive result: statistically robust and consistent learning performance is attainable if we allow the regret to be slightly worse than logarithmic. Specifically, we propose three classes of distribution oblivious algorithms that achieve an asymptotic regret that is arbitrarily close to logarithmic. }
}


@InProceedings{Genalti24a,
  title = 	 {$(\epsilon, u)$-Adaptive Regret Minimization in Heavy-Tailed Bandits},
  author =       {Genalti, Gianmarco and Marsigli, Lupo and Gatti, Nicola and Metelli, Alberto Maria},
  booktitle = 	 {Proceedings of Thirty Seventh Conference on Learning Theory},
  pages = 	 {1882--1915},
  year = 	 {2024},
  editor = 	 {Agrawal, Shipra and Roth, Aaron},
  volume = 	 {247},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {30 Jun--03 Jul},
  publisher =    {PMLR},
  pdf = 	 {https://proceedings.mlr.press/v247/genalti24a/genalti24a.pdf},
  url = 	 {https://proceedings.mlr.press/v247/genalti24a.html},
  abstract = 	 {Heavy-tailed distributions naturally arise in several settings, from finance to telecommunications. While regret minimization under subgaussian or bounded rewards has been widely studied, learning with heavy-tailed distributions only gained popularity over the last decade. In this paper, we consider the setting in which the reward distributions have finite absolute raw moments of maximum order $1+\epsilon$, uniformly bounded by a constant $u&lt;+\infty$, for some $\epsilon \in (0,1]$. In this setting, we study the regret minimization problem when $\epsilon$ and $u$ are unknown to the learner and it has to adapt. First, we show that adaptation comes at a cost and derive two negative results proving that the same regret guarantees of the non-adaptive case cannot be achieved with no further assumptions. Then, we devise and analyze a fully data-driven trimmed mean estimator and propose a novel adaptive regret minimization algorithm, \texttt{AdaR-UCB}, that leverages such an estimator. Finally, we show that \texttt{AdaR-UCB} is the first algorithm that, under a known distributional assumption, enjoys regret guarantees nearly matching those of the non-adaptive heavy-tailed case.}
}

@article{HadijiAndStoltz2023,
author = {Hadiji, H\'{e}di and Stoltz, Gilles},
title = {Adaptation to the range in K-armed bandits},
year = {2024},
issue_date = {January 2023},
publisher = {JMLR.org},
volume = {24},
number = {1},
issn = {1532-4435},
abstract = {We consider stochastic bandit problems with K arms, each associated with a distribution supported on a given finite range [m,M]. We do not assume that the range [m,M] is known and show that there is a cost for learning this range. Indeed, a new trade-off between distribution-dependent and distribution-free regret bounds arises, which prevents from simultaneously achieving the typical ln T and √T bounds. For instance, a √T distribution-free regret bound may only be achieved if the distribution-dependent regret bounds are at least of order √T. We exhibit a strategy achieving the rates for regret imposed by the new trade-off.},
journal = {J. Mach. Learn. Res.},
month = mar,
articleno = {13},
numpages = {33},
keywords = {multiarmed bandits, adversarial learning, cumulative regret, information-theoretic proof techniques}
}


@InProceedings{ItoCOLT2022aVariance,
  title = 	 {Adversarially Robust Multi-Armed Bandit Algorithm with Variance-Dependent Regret Bounds},
  author =       {Ito, Shinji and Tsuchiya, Taira and Honda, Junya},
  booktitle = 	 {Proceedings of Thirty Fifth Conference on Learning Theory},
  pages = 	 {1421--1422},
  year = 	 {2022},
  editor = 	 {Loh, Po-Ling and Raginsky, Maxim},
  volume = 	 {178},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {02--05 Jul},
  publisher =    {PMLR},
  pdf = 	 {https://proceedings.mlr.press/v178/ito22a/ito22a.pdf},
  url = 	 {https://proceedings.mlr.press/v178/ito22a.html}
}


@InProceedings{Bubeck2019ImprovedPathlength,
  title = 	 {Improved Path-length Regret Bounds for Bandits},
  author =       {Bubeck, S{\'e}bastien and Li, Yuanzhi and Luo, Haipeng and Wei, Chen-Yu},
  booktitle = 	 {Proceedings of the Thirty-Second Conference on Learning Theory},
  pages = 	 {508--528},
  year = 	 {2019},
  editor = 	 {Beygelzimer, Alina and Hsu, Daniel},
  volume = 	 {99},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {25--28 Jun},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v99/bubeck19b/bubeck19b.pdf},
  url = 	 {https://proceedings.mlr.press/v99/bubeck19b.html},
  abstract = 	 {We study adaptive regret bounds in terms of the variation of the losses (the so-called path-length bounds) for both multi-armed bandit and more generally linear bandit. We first show that the seemingly suboptimal path-length bound of (Wei and Luo, 2018) is in fact not improvable for adaptive adversary. Despite this negative result, we then develop two new algorithms, one that strictly improves over (Wei and Luo, 2018) with a smaller path-length measure, and the other which improves over (Wei and Luo, 2018) for oblivious adversary when the path-length is large. Our algorithms are based on the well-studied optimistic mirror descent framework, but importantly with several novel techniques, including new optimistic predictions, a slight bias towards recently selected arms, and the use of a hybrid regularizer similar to that of (Bubeck et al., 2018). Furthermore, we extend our results to linear bandit by showing a reduction to obtaining dynamic regret for a full-information problem, followed by a further reduction to convex body chasing. As a consequence we obtain new dynamic regret results as well as the first path-length regret bounds for general linear bandit.}
}

@article{HazanAndKale11a,
  author  = {Elad Hazan and Satyen Kale},
  title   = {Better Algorithms for Benign Bandits},
  journal = {Journal of Machine Learning Research},
  year    = {2011},
  volume  = {12},
  number  = {35},
  pages   = {1287-1311},
  url     = {http://jmlr.org/papers/v12/hazan11a.html}
}

@inproceedings{Rakhlin2012,
author = {Rakhlin, Alexander and Shamir, Ohad and Sridharan, Karthik},
title = {Making gradient descent optimal for strongly convex stochastic optimization},
year = {2012},
isbn = {9781450312851},
publisher = {Omnipress},
address = {Madison, WI, USA},
abstract = {Stochastic gradient descent (SGD) is a simple and popular method to solve stochastic optimization problems which arise in machine learning. For strongly convex problems, its convergence rate was known to be O(log(T)/T), by running SGD for T iterations and returning the average point. However, recent results showed that using a different algorithm, one can get an optimal O(1/T) rate. This might lead one to believe that standard SGD is suboptimal, and maybe should even be replaced as a method of choice. In this paper, we investigate the optimality of SGD in a stochastic setting. We show that for smooth problems, the algorithm attains the optimal O(1/T) rate. However, for non-smooth problems, the convergence rate with averaging might really be Ω(log(T)/T), and this is not just an artifact of the analysis. On the flip side, we show that a simple modification of the averaging step suffices to recover the O(1/T) rate, and no other change of the algorithm is necessary. We also present experimental results which support our findings, and point out open problems.},
booktitle = {Proceedings of the 29th International Coference on International Conference on Machine Learning},
pages = {1571–1578},
numpages = {8},
location = {Edinburgh, Scotland},
series = {ICML'12}
}

@InProceedings{WeiAndLuo2018aBroadOMD,
  title = 	 {More Adaptive Algorithms for Adversarial Bandits},
  author =       {Wei, Chen-Yu and Luo, Haipeng},
  booktitle = 	 {Proceedings of the 31st  Conference On Learning Theory},
  pages = 	 {1263--1291},
  year = 	 {2018},
  editor = 	 {Bubeck, Sébastien and Perchet, Vianney and Rigollet, Philippe},
  volume = 	 {75},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {06--09 Jul},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v75/wei18a/wei18a.pdf},
  url = 	 {https://proceedings.mlr.press/v75/wei18a.html},
  abstract = 	 {We develop a novel and generic algorithm for the adversarial multi-armed bandit problem (or more generally the combinatorial semi-bandit problem). When instantiated differently, our algorithm achieves various new data-dependent regret bounds improving previous work. Examples include: 1) a regret bound depending on the variance of only the best arm; 2) a regret bound depending on the first-order path-length of only the best arm; 3) a regret bound depending on the sum of the first-order path-lengths of all arms as well as an important negative term, which together lead to faster convergence rates for some normal form games with partial feedback; 4) a regret bound that simultaneously implies small regret when the best arm has small loss {\it and} logarithmic regret when there exists an arm whose expected loss is always smaller than those of other arms by a fixed gap (e.g. the classic i.i.d. setting). In some cases, such as the last two results, our algorithm is completely parameter-free. The main idea of our algorithm is to apply the optimism and adaptivity techniques to the well-known Online Mirror Descent framework with a special log-barrier regularizer. The challenges are to come up with appropriate optimistic predictions and correction terms in this framework. Some of our results also crucially rely on using a sophisticated increasing learning rate schedule.}
}


@InProceedings{Rakhlin2013OptimisticFTRL,
  title = 	 {Online Learning with Predictable Sequences},
  author = 	 {Rakhlin, Alexander and Sridharan, Karthik},
  booktitle = 	 {Proceedings of the 26th Annual Conference on Learning Theory},
  pages = 	 {993--1019},
  year = 	 {2013},
  editor = 	 {Shalev-Shwartz, Shai and Steinwart, Ingo},
  volume = 	 {30},
  series = 	 {Proceedings of Machine Learning Research},
  address = 	 {Princeton, NJ, USA},
  month = 	 {12--14 Jun},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v30/Rakhlin13.pdf},
  url = 	 {https://proceedings.mlr.press/v30/Rakhlin13.html},
  abstract = 	 {We present methods for online linear optimization that take advantage of benign (as opposed to worst-case) sequences. Specifically if the sequence encountered by the learner is described well by a known “predictable process”, the algorithms presented enjoy tighter bounds as compared to the typical worst case bounds. Additionally, the methods achieve the usual worst-case regret bounds if the sequence is not benign. Our approach can be seen as a way of adding \emphprior knowledge about the sequence within the paradigm of online learning. The setting is shown to encompass partial and side information. Variance and path-length bounds can be seen as particular examples of online learning with simple predictable sequences.We further extend our methods to include competing with a set of possible predictable processes (models), that is “learning” the predictable process itself concurrently with using it to obtain better regret guarantees. We show that such model selection is possible under various assumptions on the available feedback. }
}


@InProceedings{Bubeck2012bSAO,
  title = 	 {The Best of Both Worlds: Stochastic and Adversarial Bandits},
  author = 	 {Bubeck, Sébastien and Slivkins, Aleksandrs},
  booktitle = 	 {Proceedings of the 25th Annual Conference on Learning Theory},
  pages = 	 {42.1--42.23},
  year = 	 {2012},
  editor = 	 {Mannor, Shie and Srebro, Nathan and Williamson, Robert C.},
  volume = 	 {23},
  series = 	 {Proceedings of Machine Learning Research},
  address = 	 {Edinburgh, Scotland},
  month = 	 {25--27 Jun},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v23/bubeck12b/bubeck12b.pdf},
  url = 	 {https://proceedings.mlr.press/v23/bubeck12b.html},
  abstract = 	 {We present a new bandit algorithm, SAO (Stochastic and Adversarial Optimal) whose regret is (essentially) optimal both for adversarial rewards and for stochastic rewards. Specifically, SAO combines the \emphO(√\emphn) worst-case regret of Exp3 (Auer et al., 2002b) and the (poly)logarithmic regret of UCB1 (Auer et al., 2002a) for stochastic rewards. Adversarial rewards and stochastic rewards are the two main settings in the literature on multi-armed bandits (MAB). Prior work on MAB treats them separately, and does not attempt to jointly optimize for both. This result falls into the general agenda to design algorithms that combine the optimal worst-case performance with improved guarantees for “nice” problem instances.}
}


@InProceedings{DannCOLT2023aBlackbox,
  title = 	 {A Blackbox Approach to Best of Both Worlds in Bandits and Beyond},
  author =       {Dann, Chris and Wei, Chen-Yu and Zimmert, Julian},
  booktitle = 	 {Proceedings of Thirty Sixth Conference on Learning Theory},
  pages = 	 {5503--5570},
  year = 	 {2023},
  editor = 	 {Neu, Gergely and Rosasco, Lorenzo},
  volume = 	 {195},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {12--15 Jul},
  publisher =    {PMLR},
  pdf = 	 {https://proceedings.mlr.press/v195/dann23a/dann23a.pdf},
  url = 	 {https://proceedings.mlr.press/v195/dann23a.html},
  abstract = 	 {Best-of-both-worlds algorithms for online learning which achieve near-optimal regret in both the adversarial and the stochastic regimes have received growing attention recently. Existing techniques often require careful adaptation to every new problem setup, including specialized potentials and careful tuning of algorithm parameters. Yet, in domains such as linear bandits, it is still unknown if there exists an algorithm that can obtain $O(\log(T))$ regret in the stochastic regime and $\tilde{O}(\sqrt{T})$ regret in the adversarial regime. In this work, we resolve this question positively and present a generally applicable reduction from best of both worlds to a wide family of follow-the-regularized-leader (FTRL) algorithms. We showcase the capability of this reduction by transforming existing algorithms that only achieve worst-case guarantees into new best-of-both-worlds algorithms in the setting of contextual bandits, graph bandits and tabular Markov decision processes.}
}
