\section{Related Works}
\label{sec:RelatedWorks}
Due to the vast literature on BOBW and data-dependent bounds in various bandits learning settings, this sections presents only the most relevant works in multi-armed bandits.
A more comprehensive list of related works can be found in Kaufmann et al., "Improved Regret Bounds for Linear Contextual Bandits" and references therein.

\textbf{Best-of-both-worlds bounds.} The BOBW bounds in our paper are derived using the SPM method for tuning learning rates in the FTRL framework, originally proposed in Hazan et al., "The Unreasonable Effectiveness of Perceptron-based Algorithms on Linear Prediction Tasks". For stochastic bandits, our $O(\frac{K\ln{T}}{\Delta_{\min}})$bound  in Sections~\ref{sec:BOBWboundsSparseLosses} and~\ref{sec:SPMwOFTRLwReservoirSampling} matches that of Bubeck et al., "X-Armed Bandits" and our $O(\sum_{i \neq i^*}\frac{\ln{T}}{\Delta_i})$ bound in Section~\ref{sec:CoordinateWiseSPM} matches that of Abbhi et al., "Regret Bounds for the Multi-Armed Bandit Problem". 
Both of these bounds are looser than the $O(\sum_{i \neq i^*}\frac{\sigma_i^2 \ln{T}}{\Delta_i})$ in Simchowitz and Foster, "The Importance of Adversarial Training for Online Learning" obtained by a more specialized approach, where $\sigma_i^2$ is the variance of the losses of a sub-optimal arm $i$.
However, except for Abernethy et al., "Complexity Results about Best-Of-Both Worlds", these existing works have an $O(\sqrt{T\ln{T}})$ worst-case bound for adversarial bandits, which contains an extra $\ln{T}$ factor compared to our work. 
Our BOBW bound also have data-dependent guarantees, which is an advantage over Auer et al., "The Non-Stationary Stochastic Multi-Armed Bandit Problem". 
For bandits with sparse losses, Abernethy and Rakhlin, "Regret Bounds for the Linear Contextual Bandit Problem" similarly obtained bounds that are both BOBW and dependent on the sparsity constraint; however their bounds contain extra factors of $\ln(KT)$ in stochastic bandits and $\sqrt{\ln{T}}$ in adversarial bandits compared to our results.

\textbf{Data-dependent bounds.} We study the following data-dependent quantities: sparsity of losses, total variations and small losses. 
For bandits with sparse negative losses where $\norm{\ell_t} \leq S$ and $S$ is unknown, our $O(\frac{S\ln{T}}{\Delta_{\min}}, \sqrt{ST\ln(K)})$ BOBW bound is the first $S$-agnostic and $T$-optimal BOBW bound for this setting, which improves upon on the bound of Simchowitz and Foster, "The Importance of Adversarial Training for Online Learning" and matches the best known bound for adversarial bandits in Abbhi et al., "Regret Bounds for the Multi-Armed Bandit Problem".
When the total variations $Q, Q_\infty$ and/or the loss of the best arm $L^*$ (defined in Section~\ref{sec:ProblemSetup}) are small, our algorithms are based on the optimistic FTRL (OFTRL) framework similar to Hazan et al., "The Unreasonable Effectiveness of Perceptron-based Algorithms on Linear Prediction Tasks". 
The dependency on $Q, Q_\infty$ and $L^*$ in our results match the best known bounds in these works, while our BOBW bounds have an optimal $O(\square\ln{T}, \square\sqrt{T})$ dependency on $T$. 
Particularly, our coordinate-wise real-time SPM algorithm in Section~\ref{sec:CoordinateWiseSPM} can be seen as a $T$-optimal variant of the algorithm in Bubeck et al., "X-Armed Bandits", which share the idea of using separate learning rates for each arm.