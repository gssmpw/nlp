\PassOptionsToPackage{table}{xcolor}
% \documentclass[anon, 12pt]{colt2025} % Anonymized submission
\documentclass[final,12pt]{colt2025} % Include author names

% The following packages will be automatically loaded:
% amsmath, amssymb, natbib, graphicx, url, algorithm2e

\title[Data-dependent Bounds in MABs with BOBW using SPM]{Data-dependent Bounds with $T$-Optimal Best-of-Both-Worlds Guarantees in Multi-Armed Bandits using Stability-Penalty Matching}
\usepackage{times}

% For math writing
\usepackage[utf8]{inputenc}
\usepackage{import}
\usepackage{amsfonts}
\usepackage{amsmath, bm, amssymb}
\allowdisplaybreaks
\usepackage[framemethod=TikZ]{mdframed}
\usepackage{multirow, multicol, array}
\usepackage{booktabs}
\usepackage{commath}
\usepackage{mathtools}
\usepackage{tikz}
\usetikzlibrary{shapes,arrows,positioning,intersections}
\usepackage[bb=dsserif]{mathalpha}

\usepackage{thm-restate}
\usepackage{mwe}
%\usepackage{algorithm} 
% \PassOptionsToPackage{noend}{algorithm2e}
% \usepackage[noend]{algorithm2e}

% \usepackage[table, dvipsnames]{xcolor}
%% REMOVE BEFORE SUBMISSION, JUST IN CASE
% \setlength{\marginparwidth}{27mm}

% \usepackage[draft,multiuser]{fixme}
% %\fxsetup{theme=color,mode=multiuser}
% \fxusetheme{colorsig}

% \FXRegisterAuthor{quan}{Quan}{\color{red}QN}
% \FXRegisterAuthor{shinji}{Shinji}{\color{green}SI}
% \FXRegisterAuthor{junpei}{Junpei}{\color{black}JK}
% \FXRegisterAuthor{niche}{Niche}{\color{blue}NM}
% for a note in the margin: \shinjinote{...} 
% for a note in the main text with "hi" appearing in the margin:
%     \begin{Shinjinote}{hi} ...\end{Shinjiinote} 



% \newcommand{\argmax}{\operatornamewithlimits{argmax}}
% \newcommand{\argmin}{\operatornamewithlimits{argmin}}
\providecommand{\P}{}
\renewcommand{\P}{\mathbb{P}}
%\DeclareMathOperator*{\argmax}{arg\,max}
%\DeclareMathOperator*{\argmin}{arg\,min}

% \newtheorem{theorem}{Theorem}[chapter]
% \newtheorem{mylemma}[theorem]{Lemma}
% \newtheorem{corollary}[theorem]{Corollary}
% \newtheorem{proposition}{Proposition}[theorem]
% \newtheorem{remark}{Remark}[chapter]
% \newtheorem{definition}{Definition}[chapter]
% \newtheorem{example}{Example}[chapter]
% \newtheorem{problem}{Problem}
% \newtheorem{assumption}{Assumption}[chapter]

%%%%%%%%%%%%%%%%%%%%%%%%%%

% \newcommand{\junpei}[1]{{\textcolor{red}{Junpei: #1}}}

% \newcommand{\niche}[1]{{\textcolor{red}{Nishant: #1}}}
    
\input{math_commands.tex}
\newcommand\scalemath[2]{\scalebox{#1}{\mbox{\ensuremath{\displaystyle #2}}}}
% Use \Name{Author Name} to specify the name.
% If the surname contains spaces, enclose the surname
% in braces, e.g. \Name{John {Smith Jones}} similarly
% if the name has a "von" part, e.g \Name{Jane {de Winter}}.
% If the first letter in the forenames is a diacritic
% enclose the diacritic in braces, e.g. \Name{{\'E}louise Smith}

% Two authors with the same address
% \coltauthor{\Name{Author Name1} \Email{abc@sample.com}\and
%  \Name{Author Name2} \Email{xyz@sample.com}\\
%  \addr Address}

% Three or more authors with the same address:
% \coltauthor{\Name{Author Name1} \Email{an1@sample.com}\\
%  \Name{Author Name2} \Email{an2@sample.com}\\
%  \Name{Author Name3} \Email{an3@sample.com}\\
%  \addr Address}

% Authors with different addresses:
\coltauthor{%
 \Name{Quan Nguyen} \Email{manhquan233@gmail.com}\\
 \addr University of Victoria\footnote{The majority of this work was done when Quan Nguyen was at RIKEN AIP.}
 \AND
 \Name{Shinji Ito} \Email{shinji@mist.i.u-tokyo.ac.jp}\\
 \addr University of Tokyo and RIKEN AIP
 \AND
 \Name{Junpei Komiyama} \Email{junpei@komiyama.info} \\
 \addr New York University and RIKEN AIP
 \AND
 \Name{Nishant A. Mehta} \Email{nishantmehta.x@gmail.com} \\
 \addr University of Victoria
}

\begin{document}

\maketitle

\begin{abstract}%
  Existing data-dependent and best-of-both-worlds regret bounds for multi-armed bandits problems have limited adaptivity as they are either data-dependent but not best-of-both-worlds (BOBW), BOBW but not data-dependent or have sub-optimal $O(\sqrt{T\ln{T}})$ worst-case guarantee in the adversarial regime.
    To overcome these limitations, we propose real-time stability-penalty matching (SPM), a new method for obtaining regret bounds that are simultaneously data-dependent, best-of-both-worlds and $T$-optimal for multi-armed bandits problems.
    In particular, we show that real-time SPM obtains bounds with worst-case guarantees of order $O(\sqrt{T})$ in the adversarial regime and $O(\ln{T})$ in the stochastic regime while simultaneously being adaptive to data-dependent quantities such as sparsity, variations, and small losses.
    Our results are obtained by extending the SPM technique for tuning the learning rates in the follow-the-regularized-leader (FTRL) framework, which further indicates that the combination of SPM and FTRL is a promising approach for proving new adaptive bounds in online learning problems.
\end{abstract}

\begin{keywords}%
    multi-armed bandits, adaptive bounds, best-of-both-worlds, stability-penalty matching
\end{keywords}

\section{Introduction}
The multi-armed bandits problem~\citep{LaiAndRobbins1985,Auer2002a} is one of the most fundamental frameworks for modeling sequential decision making problems under limited feedback.
In this problem, a learner sequentially interacts with the environment in $T$ rounds. In round $t = 1, 2, \dots$, the learner chooses an action $I_t$ from a set of $K$ available actions and observes a numerical feedback $\ell_{t,I_t} \in \R$. 
This $\ell_{t,I_t}$ is an element of a hidden vector $\ell_t \in \R^K$ chosen at the beginning of round $t$ by an oblivious adversary. 
The performance of the learner is its \emph{pseudo-regret}
\begin{align}
    R_T = \max_{a \in [K]}R_{T, a} = \max_{a \in [K]}\E\left[\sum_{t=1}^T \ell_{t,I_t} - \ell_{t,a}\right],
    \label{eq:regretDefinition}
\end{align}
where $\E$ denote the expectation taken over all randomness from all $T$ rounds.
Existing works have constructed algorithms with \textit{worst-case} regret bounds that hold under the assumption on whether the adversary is adversarial (i.e., $(\ell_t)_t$ are arbitrary) or stochastic (i.e., $(\ell_{t})_t$ are drawn i.i.d. from some distribution)~\citep{LaiAndRobbins1985,Auer2002a,EXP3Auer2002b}, \textit{best-of-both-worlds} (BOBW) bounds that have worst-case guarantees simultaneously for adversarial and stochastic adversaries~\citep[e.g.][]{Bubeck2012bSAO,Zimmert2021TsallisINF,DannCOLT2023aBlackbox,ItoCOLT2024}, or \textit{data-dependent} bounds that are adaptive to the sequence $(\ell_t)_t$~\citep[e.g.][]{WeiAndLuo2018aBroadOMD,Bubeck2018ALT, Ito2021HybridDataMABBound,ItoCOLT2022aVariance,Tsuchiya2023stabilitypenaltyadaptive}.
Despite this vast amount of literature on different types of worst-case and adaptive bounds for multi-armed bandits, we are not aware of any works that establish bounds that are \emph{simultaneously} data-dependent, best-of-both-worlds \emph{and} have optimal dependency on $T$. 
In particular, existing works suffer from at least one of three limitations: being data-dependent but not BOBW~\citep{HazanAndKale11a,Bubeck2018ALT,WeiAndLuo2018aBroadOMD}, being BOBW but not data-dependent~\citep{Zimmert2021TsallisINF, DannCOLT2023aBlackbox} or having sub-optimal dependency on $T$~\citep{HazanAndKale11a,WeiAndLuo2018aBroadOMD,Tsuchiya2023stabilitypenaltyadaptive, ItoCOLT2024}. In this work, we close this gap in the literature by introducing novel algorithms with regret bounds that are simultaneously BOBW, data-dependent and $T$-optimal.

All of our algorithms are established in the Follow-the-Regularized-Leader (FTRL) framework~\citep[see e.g.][]{BanditAlgorithmsBook2020}, in which the time-varying learning rates are tuned by the Stability-Penalty Matching (SPM) method. 
SPM was originally proposed by~\cite{ItoCOLT2024} as a principled method for tuning learning rates in FTRL using both the \emph{penalty} and \emph{stability} terms.
More specifically, in round $t$, our algorithms compute a probability vector
\begin{align}
    q_t = \argmin_{x \in \Delta_K} \inp{L_{t-1}}{x} + \phi_t(x),
\end{align}
where $\Delta_K = \{x \in \R^K_+: \sum_{i=1}^K x_i = 1\}$ denotes the $K$-dimensional simplex, $L_{t-1} \in \R^K$ is the estimated cumulative loss vector up to round $t-1$ and $\phi_t(x): \Delta_K \to \R$ is the regularization function. We use the following specific form for $\phi_t$:
\begin{align}
    \phi_t(x) = \beta_t f(x) + \gamma u(x),
    \label{eq:formOfPhiT}
\end{align}
where $f(x): \Delta_K \to \R_{-}, u(x): \Delta_K \to \R_{+}$ are convex, $\beta_t > 0$ is the learning rate in round $t$ and $\gamma$ is a constant. Then, the learner draws an arm $I_t \sim q_t$ according to $q_t$ (or some $p_t \in \Delta_K$ derived from $q_t$) and computes an estimated loss vector $\hat{\ell}_t$.
Let $D_t(x, y) = \phi_t(x) - \phi_t(y) - \inp{\nabla \phi_t(y)}{x - y}$ denote the Bregman divergence associated with $\phi_t$.
The standard analysis of FTRL~\citep[e.g.][Exercise 28.12]{BanditAlgorithmsBook2020} implies that
\begin{align*}
    R_{T,a} 
     &\lesssim \phi_{T+1}(e_a) - \phi_1(q_1) + \scalemath{0.99}{
     \E\left[\sum_{t=1}^T (\phi_t(q_{t+1}) - \phi_{t+1}(q_{t+1})) + \sum_{t=1}^T (\inp{\hat{\ell}_t}{q_t - q_{t+1}} - D_{t}(q_{t+1}, q_t))\right]
     } \\
    &\lesssim \gamma u(e_a) - \beta_1f(q_1) + \underbrace{
    \E\left[\sum_{t=1}^T (\beta_{t+1} - \beta_t)h_{t+1}\right]
    }_{\text{penalty term}} + \underbrace{
    \E\left[\sum_{t=1}^T\frac{z_t}{\beta_t}\right]
    }_{\text{stability term}}
\end{align*}
where $e_a$ is the $a$-th vector in the standard basis of $\R^K$, $h_{t+1}$ satisfies $ (-f(q_{t+1})) \lesssim h_{t+1}$ and $z_t$ satisfies $\beta_t \E[\inp{\hat{\ell}_t}{q_t - q_{t+1}} - D_{t}(q_{t+1}, q_t)] \lesssim \E[z_t]$. 
SPM carefully chooses $\beta_1, z_t$ and $h_t$ so that $h_{t+1} \leq O(h_t)$ and sets the learning rate of the next round to be
\begin{align}
    \beta_{t+1} = \beta_t + \frac{z_t}{\beta_t h_t}.
    \label{eq:SPM}
\end{align}
This makes $(\beta_{t+1} - \beta_t)h_{t+1}$ match with $\frac{z_t}{\beta_t}$ and implies 
$
    R_{T,a} \lesssim \gamma u(e_a) - \beta_1 f(q_1) + \E\left[\sum_{t=1}^T\frac{z_t}{\beta_t}\right].
$
An important insight in SPM is that by picking $f(x)$ and $u(x)$ appropriately, $\E\left[\sum_{t=1}^T \frac{z_t}{\beta_t}\right] $ is naturally adaptive to the adversarial or stochastic nature of the environment~\citep{ItoCOLT2024}. 
In our work, we will show that SPM can be made adaptive not only to the nature of the environment but also to the underlying structure of the sequence of losses such as sparsity and total variation.

\subsection{Main Contributions and Techniques}
Throughout the paper, we will write $O(\square \ln(T), \square \sqrt{T})$ to denote a BOBW bound that holds for stochastic and adversarial regimes, respectively, where $\square$ contains problem-dependent terms.
The original SPM method~\citep{ItoCOLT2024} used $z_t = \Omega(\beta_t \E_{I_t}[\inp{\hat{\ell}_t}{q_t - q_{t+1}} - D_{t}(q_{t+1}, q_t)])$, where $\E_{I_t}$ denotes an expectation taken over $I_t$. 
Because only one out of $K$ arms is observable in each round $t$, this in-expectation form of $z_t$ inevitably requires taking the trivial bounds (e.g. $1$) of the losses into its computation, thus limiting its adaptivity to $(\ell_t)_{t}$. Our work overcomes this limitation by setting
\begin{align}
    z_t = \Omega(\beta_t (\inp{\hat{\ell}_t}{q_t - q_{t+1}} - D_{t}(q_{t+1}, q_t))).
    \label{eq:realtimeZt}
\end{align}
We call this \emph{real-time SPM}, since $z_t$ depends on the observed arm $I_t$.
The main technical challenge is now $z_t$ can be very large since it grows with $\mathrm{poly}(\frac{1}{p_{t,I_t}})$. 
At the same time, we need to limit the amount of explicit exploration to obtain a BOBW bound for stochastic bandits. 
Table~\ref{table:results} summarizes our main results, showing that real-time SPM can be controlled effectively to give BOBW \emph{and} data-dependent bounds with optimal dependency on $T$.
Our results also hold for the more general adversarial regime with self-bounding constraint setting~\citep{Zimmert2021TsallisINF}.
Appendix~\ref{sec:RelatedWorks} gives a more detailed discussion on related works.
Our paper is organized as follows: 
\begin{itemize}
    \item Section~\ref{sec:SPMwithRealTimeStability} introduces the real-time SPM method and states Lemma~\ref{lemma:refinedBoundF}, a key technical lemma for bounding $\E\left[\sum_{t=1}^T \frac{z_t}{\beta_t} \right]$.
    While the original analysis of SPM~\citep{ItoCOLT2024} relies on having a small $\max_{t \in [T]}z_t$ and thus cannot be applied to real-time SPM, our Lemma~\ref{lemma:refinedBoundF} instead shows that real-time SPM incurs an additional regret of at most $O\left(\max_{t \in [T]}\frac{z_t}{\beta_t}\ln{\sum_{t=1}^T \frac{z_t}{h_t}}\right)$. 
    Moreover, both $\frac{z_t}{\beta_t}$ and $\frac{z_t}{h_t}$ can be effectively controlled by appropriate choices of $\phi_t(x)$.
    \item Section~\ref{sec:BOBWboundsSparseLosses} considers the bandits problems with signed sparse losses, where $\ell_{t,i} \in [-1, 1]$ and $\norm{\ell_t}_0 \leq S$. We show that using $\alpha$-Tsallis entropy and log-barrier functions in place of $f$ and $g$ in~\eqref{eq:formOfPhiT} leads to an $O\left(\frac{(K^{1-\alpha}-1)S^\alpha \ln(T)}{\alpha (1-\alpha) \Delta_{\min}}, \left(\sqrt{\frac{(K^{1-\alpha} - 1)S^\alpha T}{\alpha(1-\alpha)}}\right)\right)$. 
    This bound is $T$-optimal and improves upon the best known bound for this setting established by~\citet{Tsuchiya2023stabilitypenaltyadaptive}.
    When $S$ is known, we show that the adversarial bound is improved to $O(\sqrt{ST\ln(K/S)})$, resolving an open question in~\citet{KwonAndPerchet2016JMLRSparsity}. 
    Furthermore, we prove a near-matching lower bound for problems in which the sparsity constraint holds in expectation. 
    % In Appendix~\ref{appendix:SPMSleepingBandits}, we also show that the same algorithm for sparse bandits can be applied in the related setting of adversarial sleeping bandits and obtain a regret bound that matches the best known bound in~\citet{NguyenAndMehta2024SBEXP3} with fewer assumptions.
    \item \looseness=-1 Section~\ref{sec:SPMwOFTRLwReservoirSampling} considers problems with small total variation $Q$ (defined in Section~\ref{sec:ProblemSetup}) and presents a new algorithm obtaining a $O\left(\frac{(K- 1)^{1-\alpha} K^\alpha \ln(T)}{\alpha(1-\alpha)\Delta_{\min}}, \sqrt{Q\ln(K)}\right)$ BOBW bound. 
    In the adversarial regime, the $O(\sqrt{Q\ln(K)})$ bound matches the best known bound in~\citet{Bubeck2018ALT} while having the advantage of not requiring knowledge of $Q$.
    \item Section~\ref{sec:CoordinateWiseSPM} introduces a new SPM method called coordinate-wise SPM, which maintain arm-dependent learning rates $\beta_{t,i}$ and performs real-time SPM on each arm separately. 
    We show that coordinate-wise SPM achieves a BOBW bound with order $O\left(\frac{1}{\alpha (1-\alpha)}\sum_{i \neq i^*} \frac{\ln(T)}{\Delta_i}\right)$ in stochastic bandits and $O\left(\min\left\{ \sqrt{K\ln(T)\min(Q_\infty, L^*, T-L^*)}, K^{\frac{\alpha}{2}}\sqrt{KT}\right\}\right)$ in adversarial bandits,
    where $Q_\infty$ and $L^*$ are $\ell_\infty$-norm total variation and total loss of the best arm, respectively (see Section~\ref{sec:ProblemSetup} for their formal definitions).
\end{itemize}
\begin{table}[htbp]
    \setlength{\tabcolsep}{1.5pt}
    \caption{Summary of data-dependent results in existing and ours works.
    The three blocks of rows show bounds dependent on sparsity $S$, total variation $Q$ and a combination of $Q_\infty$ and $L^*$, respectively (formal definitions are in Section~\ref{sec:ProblemSetup}).
    We use $H^*_{\infty}= \min(Q_\infty, L^*, T-L^*)$.
    ``$T$-opt BOBW'' denote whether a bound is BOBW and $T$-optimal.
    ``Param-free'' denote whether a bound requires knowledge of the data-dependent quantities.
    } 
    \label{table:results}
    \centering
    \begin{tabular}{lllcc}
       \toprule      
      % \multirow{2}{*}{Algorithms} & \multicolumn{2}{c}{Data-dependent Bounds}  & \multirow{2}{*}{$T$-opt BOBW?} & \multirow{2}{*}{P-free?}\\    
      Algorithms & Stochastic & Adversarial & $T$-opt BOBW? & Param-free? \\
      \midrule
    \citet{Bubeck2018ALT} & $-$ & $\sqrt{ST\ln{K}}$ & $\times$ & $\times$ \\    
    \citet{Tsuchiya2023stabilitypenaltyadaptive} & $\frac{S\ln(T)\ln(KT)}{\Delta_{\min}} $ & $\sqrt{ST\ln{T}\ln{K}}$ & $\times$ & $\checkmark$ \\
    \cellcolor{lightgray}
    {Theorem~\ref{thm:BOBWbounds}} & $\frac{S\ln{T}\ln{K}}{\Delta_{\min}} $ & $\sqrt{ST\ln{K}}$ & $\checkmark$ &$\checkmark$ \\
    \midrule
    \citet{HazanAndKale11a} & $-$ & $\sqrt{Q\ln{T}\ln{K}}$ & $\times$ & $\checkmark$ \\
    \citet{Bubeck2018ALT} & $-$ & $\sqrt{Q\ln{K}}$  & $\times$ & $\times$ \\
    \cellcolor{lightgray}
    {Theorem~\ref{thm:BOBWsqrtQLnKBound}} & $\frac{K\ln{T}}{\Delta_{\min}}$ & $\sqrt{Q\ln{K}}$ & $\checkmark$ & $\checkmark$ \\
    \midrule
    \citet{WeiAndLuo2018aBroadOMD} & $\frac{K\ln{T}}{\Delta_{\min}}$ & $\sqrt{KL^*\ln{T}}$ & $\times$ &  $\checkmark$\\
    \citet{Ito2021HybridDataMABBound} & $\sum\limits_{i \neq i^*}\frac{\ln{T}}{\Delta_i}$ & $\sqrt{K\min(Q_\infty, L^*)\ln{T}}$ & $\times$ &  $\checkmark$\\
    \citet{ItoCOLT2022aVariance} & $\sum\limits_{i \neq i^*}(\frac{\sigma^2_i}{\Delta_i} + 1)\ln{T}$ & $\sqrt{KH^*_{\infty}\ln{T}}$ & $\times$ &  $\checkmark$\\
    \cellcolor{lightgray}
    {Theorem~\ref{thm:CWSPMbounds}} & $\sum\limits_{i \neq i^*}\frac{\ln{T}}{\Delta_i}$ & $\min(\sqrt{KH^*_{\infty}\ln{T}},\sqrt{K^{1+\alpha}T})$ & $\checkmark$ & $\checkmark$\\
      \bottomrule
    \end{tabular}
  \end{table}

\subsection{Problem Setup}
\label{sec:ProblemSetup}
For an integer $N$, let $[N] = \{1, 2, \dots, N\}$ denote the set of integers from $1$ to $N$. 
% Let $\Delta_K = \{x \in \R^K_+: \sum_{i=1}^K x_i = 1\}$ denote the $K$-dimensional simplex. 
% Expectations and probabilities of events are denoted by $\E$ and $\P$, respectively.
%
We study the multi-armed bandits problem~\citep{LaiAndRobbins1985,Auer2002a} in which a learner is given $K$ arms and interacts with the environment in $T$ rounds.  
In each round $t$, an adversary selects a hidden vector $\ell_{t} = (\ell_{t, 1}, \ell_{t,2}, \dots, \ell_{t,K})^\top$. 
The learner chooses one arm $I_t \in [K]$ and observes its loss $\ell_{t,I_t}$. We assume $\abs{\ell_{t,i}} \leq 1$ for all $t \in [T], i \in [K]$.
The learner aims to minimize its 
{regret} $R_T$ over $T$ rounds, defined by Equation~\eqref{eq:regretDefinition}.

We are interested in developing learning algorithms with provable upper bounds on $R_T$ that hold simultaneously for two regimes: adversarial~\citep{Auer2002a} and adversarial with a $(\Delta, C, T)$ self-bounding constraint~\citep{Zimmert2021TsallisINF}. In the \textit{adversarial regime}, no assumption is made on how the adversary generates $(\ell_t)_{t \in [T]}$. The adversarial regime with a $(\Delta, C, T)$ self-bounding constraint~\citep{Zimmert2021TsallisINF} is given below.
\begin{definition} (Adversarial regime with a self-bounding constraint)
    For $T \geq 1, \Delta \in [0,1]^K$ and $C \geq 0$, the problem is in adversarial regime with a $(\Delta, C, T)$ self-bounding constraint if the regret of any algorithm at time $T$ satisfies
    $
        R_T \geq \sum_{t=1}^T\sum_{i=1}^K \Delta_i \P(I_t = i) - C.
    $
    \label{def:selfboundingconstraint}
\end{definition}
As noted in~\citet{Zimmert2021TsallisINF}, the stochastic bandits setting~\citep{LaiAndRobbins1985} satisfies Definition~\ref{def:selfboundingconstraint}.
We also use the common assumption that there exists an optimal arm $i^*$ such that $\Delta_i > 0$ for all $i \neq i^*$, that is, the optimal arm is unique. Let $\Delta_{\min} = \min_{i \in [K]}\{\Delta_i: \Delta_i > 0\}$.

We focus on obtaining bounds that are adaptive not only to the adversary's regime but also to the data-dependent properties of the loss sequence $(\ell_t)_{t \in [T]}$.
The following data-dependent quantities are considered in our work.
\begin{itemize}
    \item \textbf{Sparsity of losses}~\citep{KwonAndPerchet2016JMLRSparsity}. All loss vectors have at most $1 \leq S \leq K$ non-zero elements, i.e., $\norm{\ell_t}_0 \leq S$, where $S$ is unknown.
    \item \textbf{Variation of losses}~\citep{HazanAndKale11a,ItoCOLT2022aVariance} The total variation of the sequence $(\ell_t)_t$ is $Q = \sum_{t=1}^T \norm{\ell_t - \frac{1}{T}\sum_{s=1}^T \ell_s}^2_2$. The $\ell_\infty$-norm total variation is $Q_\infty = \min_{\bar{\ell} \in [0,1]^K}\sum_{t=1}^T \norm{\ell_t - \bar{\ell}}^2_\infty$.
    \item \textbf{Best-arm loss}. For non-negative losses, we consider the cumulative loss of the best arm $L_{*} = \min_{i \in [K]}\sum_{t=1}^T \ell_{t,i}$.
\end{itemize}
% All of our algorithms follow the FTRL framework using SPM for tunning the learning rates.

\section{Stability-Penalty Matching with Real-Time Stability Term}
\label{sec:SPMwithRealTimeStability}
Let $\tilde{p} = \min(1 - p, p)$ for $p \in [0, 1]$. We use the notation $f \lesssim g$ to denote $f = O(g)$.
To obtain data-dependent bounds using SPM, we use SPM where the stability term is a function of the \emph{observed} loss, i.e., $z_t$ satisfies Equation~\eqref{eq:realtimeZt}.
Note that $z_t$ grows with $\ell_{t,I_t}^2$ and $\frac{1}{p_{t,I_t}}$.
The benefit of this real-time $z_t$ is that data-dependent quantities 
such as sparsity and total variation naturally come out of $\E[z_t]$.
For example, in Algorithm~\ref{algo:optimalBOBWSparsity} for bandits with sparse losses $\norm{\ell_t}_0 \leq S$, we use $z_t = O(\tilde{p}_{t,I_t}^{2-\alpha}\frac{\ell_{t,I_t}^2}{p_{t,I_t}^2})$ for some $\alpha \in (0,1)$.
It follows that $\E[z_t] = O( \sum_{i: \ell_{t,i} \neq 0} \tilde{p}_{t,i}^{1-\alpha}\ell_{t,i}^2) \leq O(S^\alpha)$, leading to the $O(\sqrt{ST\ln{K}})$ bound.
%
The main challenge in using the real-time $z_t$ is the value of $z_t$ can be unbounded whenever $p_{t,I_t}$ is very small.
It follows that $z_{\max} = \max_{t \in [T]} z_t$ can be unbounded, which makes it difficult to apply existing techniques in~\citet[Lemma 10]{ItoCOLT2024} that bounds $\E[\sum_{t=1}^T \frac{z_t}{\beta_t}]$ by a quantity that grows with $\E[z_{\max}]$.
We resolve this challenge by using the following technical lemma.
\begin{lemma}
    For any $T \geq 1, z_{1:T} \geq 0, h_{1:T} > 0$ and a sequence $\beta_{1:T}$ defined by Equation~\eqref{eq:SPM}, let $F(z_{1:T}, h_{1:T}) = \sum_{t=1}^T \frac{z_t}{\beta_t}$ and $G(z_{1:T}, h_{1:T}) = \sum_{t=1}^T \frac{z_t}{\sqrt{\sum_{s=1}^t \frac{z_s}{h_s}}}$. We have
    \begin{align}
          F(z_{1:T}, h_{1:T}) \lesssim G(z_{1:T}, h_{1:T}) + \left(\max_{t \in [T]}\frac{z_t}{\beta_t}\right)\ln\left(\sum_{t=1}^T \frac{z_t}{h_t}\right).
    \end{align}
    \label{lemma:refinedBoundF}
\end{lemma}
\begin{proof}(Sketch)
    Our proof extends from the proof of~\citet[Lemma 3]{ItoCOLT2024}.
    Similar to the proof of~\citet[Lemma 10]{ItoCOLT2024}, we define a new sequence $\beta'_t = \sqrt{\beta_1^2 + 2\sum_{s=1}^{t-1}\frac{z_s}{h_s}}$ and consider the set of rounds $E = \{t \in [T]: \beta'_{t+1} \geq \sqrt{2}\beta'_t\}$. 
    The complement of $E$ is $E^c = [T] \setminus E$. We have
    \begin{align*}
        F(z_{1:T}, h_{1:T}) &= \underbrace{\sum_{t \in E^c}\frac{z_t}{\beta_t}}_{(a)} + \underbrace{\sum_{t \in E}\frac{z_t}{\beta_t}}_{(b)},
    \end{align*}
    where $(a)$ is bounded by $G(z_{1:T}, h_{1:T})$ as in~\citet[Lemma 10]{ItoCOLT2024}, and $(b)$ is bounded by
    \begin{align*}
        (b) \leq \left(\max_{t \in [T]}\frac{z_t}{\beta_t}\right)\abs{E} \leq \left(\max_{t \in [T]}\frac{z_t}{\beta_t}\right)\log_{\sqrt{2}}\left(\frac{\beta'_{T+1}}{\beta'_1}\right) \lesssim \left(\max_{t \in [T]}\frac{z_t}{\beta_t}\right)\ln\left(\sum_{t=1}^T \frac{z_t}{h_t}\right),
    \end{align*}
    where the second inequality is from the fact that $\beta'_t$ is multiplied by at least $\sqrt{2}$ after every round in $E$; thus there can be at most $\log_{\sqrt{2}}\frac{\beta'_{T+1}}{\beta'_1}$ such multiplications.
\end{proof}
%%%%%%%%%%%%%%%%%%%%%
Lemma~\ref{lemma:refinedBoundF} implies that if (I) the sum $\E[\sum_{t=1}^T \frac{z_t}{h_t}]$ grows with $\mathrm{poly}(T)$ and (II) $\max_{t}\frac{z_t}{\beta_t}$ is small, then $\E[F(z_{1:T}, h_{1:T})]$ grows dominantly with $\E[G(z_{1:T}, h_{1:T})]$ plus an $O(\ln(T))$ term. Hence, we can safely ignore other terms and focus only on bounding $G(z_{1:T}, h_{1:T})$. The proof of~\citet[Lemma 10]{ItoCOLT2024} already showed that \begin{align}
    G(z_{1:T}, h_{1:T}) \lesssim \min\left\{ \sqrt{\ln(T)\sum_{t=1}^T h_tz_t} + \sqrt{\frac{1}{T}h_{\max}\sum_{t=1}^T z_t},\sqrt{h_{\max}\sum_{t=1}^T z_t}\right\}.
    \label{eq:boundG}
\end{align}
In the rest of the paper, we will show that different choices of the (hybrid) regularization function lead to specific forms of $z_t$ and $h_t$ such that not only do both conditions (I) and (II) hold but also they 
{imply} BOBW data-dependent bounds with optimal dependency on $T$ from~\eqref{eq:boundG}.

\section{Application I: BOBW Bounds for Bandits with Sparse Losses}
\label{sec:BOBWboundsSparseLosses}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{algorithm}[t]
	\KwIn{$K \geq 3, T \geq 4K, \alpha \in (0, 1), \beta_1 = \frac{8K}{1-\alpha}, \gamma = \max(6,48\sqrtfrac{\alpha}{1-\alpha}), d = 2$.}
    % \KwIn{Constant $c \geq 1$.}
    Initialize $L_{0,i} = 0$ for $i \in [K]$ \;
	% Initialize $q_{i, 1} = 1$ for $i = 1, 2, \dots, K$\; 
	
    \For{each round $t = 1, \dots, T$}{        
        Compute $q_t = \argmin_{p \in \Delta_K} \inp{L_{t-1}}{p} + \beta_t\left(\frac{1}{\alpha}(1-\sum_{i=1}^K p_i^\alpha)\right) - \gamma\sum_{i=1}^K \ln(p_i)$\;

        Compute $p_t = \left(1 - \frac{K}{T}\right)q_t + \frac{1}{T}\1$\;

		Draw $I_t \sim p_t$ and observe $\ell_{t, I_t}$\; %% Nishant says: I think it should be p_t instead of q_t

		Compute loss estimate $\hat{\ell}_{t, i} = \frac{\ell_{t,i}\I{I_t = i}}{p_{t,i}}$ and update $L_{t,i} = L_{t-1,i} + \hat{\ell}_{t,i}$\;

        Compute $z_t =  \min\left( \frac{(6d)^{2-\alpha}}{2(1-\alpha)}\min(p_{t,I_t}, 1 - p_{t,I_t})^{2-\alpha}\hat{\ell}^2_{t,I_t}, \frac{\beta_t18d^2}{\gamma} \ell_{t,I_t}^2  \right)$ \;

        Compute $h_t = \left(\frac{1}{\alpha}(\sum_{i=1}^K p_{t,i}^\alpha - 1)\right)$\;

        Compute $\beta_{t+1} = \beta_t + \frac{z_t}{\beta_t h_t}$\;
        % such that $\beta_t \leq \beta_{t+1} \leq c\beta_t$\;
	}
	\caption{Real-time SPM with hybrid regularization for losses in $[-1,1]$.}
	\label{algo:optimalBOBWSparsity}
\end{algorithm}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
We consider the multi-armed bandits setting with sparse losses~\citep{KwonAndPerchet2016JMLRSparsity}, in which the loss vector $\ell_t \in [-1, 1]^K$ has at most $S$ non-zero elements, i.e., $\max_{t \in [T]}\norm{\ell_t}_0 \leq S$. 
Note that $S$ is unknown to the learner.
Let $\psi_{TE}(p) = \frac{1}{\alpha}(1-\sum_{i=1}^K p_i^\alpha)$
be the $\alpha$-Tsallis entropy with some $\alpha \in (0, 1)$ and $\psi_{LB}(p) = -\sum_{i=1}^K \ln(p_i)$ be the log-barrier function.
Our approach for this setting is in Algorithm~\ref{algo:optimalBOBWSparsity}, in which we use the hybrid regularizer $\phi_t(p) = \beta_t\psi_{TE}(p) + \gamma\psi_{LB}(p)$ to obtain 
\begin{align*}
    q_{t} = \argmin_{p \in \Delta_K}\{\inp{L_{t-1}}{p} + \beta_t\psi_{TE}(p) + \gamma\psi_{LB}(p)\},
\end{align*}
%
Then, we mix $q_t$ with $\frac{1}{T}$-uniform exploration to obtain the sampling probability $p_t$, i.e., $ p_{t} = \left(1 - \frac{K}{T}\right)q_t + \frac{1}{T}\1$.
%
The learning rates $(\beta_t)_t$ are set by the SPM rule by~\cite{ItoCOLT2024} as
\begin{align}
    \beta_1 = \frac{8K}{1-\alpha}, \,\, \beta_{t+1} = \beta_{t} + \frac{z_t}{\beta_t h_t},
    \label{eq:betatplus1}
\end{align}
where 
\begin{align}
    z_t =  \min\left( \frac{(6d)^{2-\alpha}}{2(1-\alpha)}\min(p_{t,I_t}, 1 - p_{t,I_t})^{2-\alpha}\hat{\ell}^2_{t,I_t}, \beta_t\frac{18d^2}{\gamma} \ell_{t,I_t}^2  \right), \,\, h_t = (-\psi_{TE}(p_{t})),
    \label{eq:setzthtforsparsebandits}
\end{align}
and $\gamma = \max(6,48\sqrtfrac{\alpha}{1-\alpha}), d = 2$.
Note that $\beta_1 \geq \frac{4K}{(\omega -1)(1 - \omega^{\alpha-1})}$ for $\omega = 2$.
The following theorem states the BOBW bounds of Algorithm~\ref{algo:optimalBOBWSparsity}.
\begin{theorem}
    For any $K \geq 4, T \geq 4k$, Algorithm~\ref{algo:optimalBOBWSparsity} guarantees the following bounds simultaneously
    \begin{itemize}
        \item In the adversarial regime: 
        \begin{align}
            R_T \leq O\left(\sqrt{\frac{(K^{1-\alpha} - 1)S^\alpha T}{\alpha(1-\alpha)}}\right)
            \label{eq:boundSparseBanditsAdversarial}
        \end{align}
        \item In the adversarial regime with a self-bounding constraint:
        \begin{align}
            R_T \leq O\left(\frac{(K-1)^{1-\alpha}S^\alpha \ln(T)}{\alpha(1-\alpha)\Delta_{\min}} + \sqrt{C\frac{(K-1)^{1-\alpha}S^\alpha \ln(T)}{\alpha(1-\alpha)\Delta_{\min}}} + \sqrt{\frac{(K-1)^{1-\alpha}S^{\alpha}}{\alpha(1-\alpha)}}\right)
            \label{eq:boundSparseBanditsAdversarialWithSelfBounding}
        \end{align}
    \end{itemize}
    \label{thm:BOBWbounds}
\end{theorem}
%
\looseness=-1 In Appendix~\ref{appendix:setAlphaCloseto1}, we show that by setting $\alpha = 1 - \frac{1}{2\ln(K)}$, we obtain $\frac{(K^{1-\alpha} - 1)S^{\alpha}}{\alpha(1-\alpha)} \lesssim S^\alpha\ln(K)$ and $\frac{(K-1)^{1-\alpha}S^{\alpha}}{\alpha(1-\alpha)} \lesssim S^\alpha\ln(K)$. 
In the adversarial regime, Theorem~\ref{thm:BOBWbounds} recovers the $O(\sqrt{ST\ln(K)})$ bound in~\cite{Bubeck2018ALT} and~\cite{Tsuchiya2023stabilitypenaltyadaptive} while still being $S$-agnostic. 
In the adversarial regime with a self-bounding constraint, the bound becomes $O(\frac{S\ln(K)\ln(T)}{\Delta_{\min}})$ which has an optimal dependency on $T$. To the best of our knowledge, Theorem~\ref{thm:BOBWbounds} is the first result for bandits with sparse signed losses that is simultaneously $S$-agnostic, $T$-optimal and BOBW.
Also, our approach is more computationally efficient than that of~\cite{Tsuchiya2023stabilitypenaltyadaptive} as we do not need to solve any additional optimization problems to compute the learning rates $(\beta_t)_t$.
%
\begin{remark}
    When $S$ is known, then $\frac{(K^{1-\alpha} - 1)S^{\alpha}}{\alpha(1-\alpha)}$ 
    can be further bounded by $6S\ln(\frac{K}{S})$. Consider only the case where $S$ is sufficiently small so that $e^2S \leq K$ (the other direction trivially leads to $O(\frac{S}{\alpha(1-\alpha)})$). 
    Letting $\alpha = 1- \frac{1}{\ln(K/S)}$, then $(\frac{K-1}{S})^{1-\alpha} \leq (\frac{K}{S})^{1-\alpha} = e$.  Since $\ln(K/S) \geq 2$, we have $\alpha \geq \frac{1}{2}$. Therefore,
        \begin{align*}
            \frac{(K^{1-\alpha} - 1)S^{\alpha}}{\alpha(1-\alpha)} &\leq \frac{K^{1-\alpha}S^{\alpha}}{\alpha(1-\alpha)} = S \left(\frac{K}{S}\right)^{1-\alpha} \frac{\ln(K/S)}{\alpha} = \frac{eS \ln(K/S)}{\alpha} \leq 6S\ln(K/S).
        \end{align*}
        This result shows that an $O(\sqrt{ST\ln(K/S)})$ upper bound is attainable even for signed losses, which resolves an open question posed in~\cite[Remark 12]{KwonAndPerchet2016JMLRSparsity}.
    \end{remark} 
% \begin{remark}
%     In Appendix~\ref{appendix:SPMSleepingBandits}, we also show that Algorithm~\ref{algo:optimalBOBWSparsity} can be applied in the related setting of adversarial sleeping bandits and obtain a regret bound that matches the best known bound in~\citet{NguyenAndMehta2024SBEXP3} despite using fewer assumptions.
% \end{remark}
\subsection{Proof Sketch for Theorem~\ref{thm:BOBWbounds}}
As mentioned in Section~\ref{sec:SPMwithRealTimeStability}, we first show that $z_t$ and $h_t$ in~\eqref{eq:setzthtforsparsebandits} satisfy the two conditions (I) $\E[\sum_{t=1}^T \frac{z_t}{h_t}] = O(\mathrm{poly}(T))$ and (II) $\max_t\frac{z_t}{\beta_t}$ is small. The second condition is straightforward from the definition of $z_t, \gamma$ and $d$, since $
    \frac{z_t}{\beta_t} \leq \frac{18d^2}{\gamma} \leq 6d^2 = 24$
which is a constant. To see that (I) is true, note that $h_t$ is fixed with respect to $I_t$. Hence,
\begin{align*}
    \E\left[\sum_{t=1}^T \frac{z_t}{h_t}\right] &= \E\left[\sum_{t=1}^T \frac{\E_{I_t}[z_t]}{h_t}\right] \leq T\E\left[\frac{\max_{t \in [T]}\E_{I_t}[z_t]}{\min_{t \in [T]}h_t}\right].
\end{align*}
Then, the condition (I) follows from Lemma~\ref{lemma:minhtAndmaxEzt}, which shows that $h_t \geq \frac{1-\alpha}{4\alpha}T^{-\alpha}$ and $E_{I_t}[z_t] \leq \frac{(6d)^{2-\alpha}}{2(1-\alpha)} S^{\alpha}$.
Jensen's inequality implies that $\E\left[\ln\left(\sum_{t=1}^T \frac{z_t}{h_t}\right)\right] \leq \ln\left(\E\left[\sum_{t=1}^T \frac{z_t}{h_t}\right]\right)$. 
Combining this with Lemma~\ref{lemma:minhtAndmaxEzt}, we conclude that $\E[F(z_{1:T}, h_{1:T})]$ grows dominantly with $\E[G(z_{1:T}, h_{1:T})]$. 
The last part of the proof is showing that plugging $z_t$ and $h_t$ from~\eqref{eq:setzthtforsparsebandits} into~\eqref{eq:boundG} yields the desired bounds.
In the adversarial regime, the bound~\eqref{eq:boundSparseBanditsAdversarial} follows directly from~\eqref{eq:boundG}, Lemma~\ref{lemma:minhtAndmaxEzt}, $h_t \leq \frac{K^{1-\alpha}-1}{\alpha}$ and Jensen's inequality $\E[\sqrt{X}] \leq \sqrt{\E[X]}$.
In the adversarial regime with a self-bounding constraint, we can prove~\eqref{eq:boundSparseBanditsAdversarialWithSelfBounding} by first showing that
\begin{align*}
    \E[h_tz_t] &\leq \frac{(6d)^{2-\alpha}}{\alpha(1-\alpha)\Delta_{\min}}{(K-1)^{1-\alpha}S^{\alpha}}\E\left[\sum_{i=1}^K p_{t,i}\Delta_i\right].    
\end{align*}
and then following the same argument as in~\citet{ItoCOLT2024}.

\subsection{A Lower Bound for Problems with Soft Sparsity Constraint}
\looseness=-1 It remains an open question whether the BOBW bounds in Theorem~\ref{thm:BOBWbounds} are tight under the hard constraint $\norm{\ell_t}_0 \leq S$. 
This hard-constraint problem belongs to a broader class of settings with a more relaxed constraint, in which there exists an $\alpha \in (0,1)$ and $1 \leq U \leq K^\alpha$ such that for all $t \in [T]$,
\begin{align}
    \E\left[\left(\sum_{i=1}^K \abs{\ell_{t,i}}^{2/\alpha}\right)^\alpha\right] \leq U.
    \label{eq:softconstraint}
\end{align}
In other words, the sparsity constraint holds in expectation. Obviously, the hard-constraint setting with $\norm{\ell_t}_0 \leq S$ satisfies~\eqref{eq:softconstraint} for any $\alpha \in (0,1)$ and $U = S^\alpha$.
Moreover, by using the same Algorithm~\ref{algo:optimalBOBWSparsity} and straightforward modifications in its proof, we can obtain the corresponding $O(\frac{K^{1-\alpha}U}{\alpha(1-\alpha)\Delta_{\min}}\ln{T})$ and $O(\sqrt{\frac{K^{1-\alpha}}{\alpha (1-\alpha)}UT})$ BOBW bounds for stochastic and adversarial regimes, respectively.
The following theorem, whose proof is in Section~\ref{sec:lowerboundproofs}, shows near-matching lower bounds for problems with soft sparsity constraint defined in~\eqref{eq:softconstraint}.
\begin{theorem}
    (Instance-Dependent Lower Bound) For any consistent algorithm, for any $K \geq 4, \alpha \in (0,1)$ and $1 \leq U \leq \frac{K^\alpha}{4}$, there exists a $K$-armed stochastic bandit instance with $\Delta_{\min} \in (0,1)$ and loss distribution satisfying~\eqref{eq:softconstraint} such that
    \begin{align*}
        \lim_{T \to \infty} \frac{R_T}{\ln(T)} = \Omega\left(\frac{K^{1-\alpha}U}{\Delta_{\min}}\right).
    \end{align*}
    (Minimax Lower Bound) For any algorithm, for any $K \geq 4, \alpha \in (0,1)$ and $U \leq K^\alpha$, there exists an adversarial bandit instance with $K$ arms and loss distribution satisfying~\eqref{eq:softconstraint} such that
    \begin{align*}
        R_T = \Omega(\sqrt{K^{1-\alpha}UT}).
    \end{align*}
    \label{thm:LowerBounds}
\end{theorem}

\subsection{Implications for Bandits with Adversarially Changing Action Sets}
Intuitively, the sparsity constraint $\norm{\ell_t}_0 \leq S$ indicates that there are at most $S$ arms containing non-trivial information in each round, however the learner 
{does not know the} arms with non-trivial information.
In this sense, sparse bandits is conceptually more difficult than adversarial sleeping bandits~\citep{Kleinberg2010Sleeping}, where in each round $t$ the learner is given, by an adversary, a set $\sA_t \subseteq [K]$ of active arms to choose from. 
Note that the learner is not allowed to choose an arm in $[K] \setminus \sA_t$.
The performance of the learner is measured by its per-action regret
\begin{align*}
    R_{T,a} = \sum_{t=1}^T \I{a \in \sA_t}(\ell_{t,I_t} - \ell_{t,a}).
\end{align*}
A natural question is whether Algorithm~\ref{algo:optimalBOBWSparsity} can be extended to this adversarial sleeping bandits setting.
The following theorem answers this question in the positive.
\begin{theorem}
    For any $K \geq 4, T \geq 4k$, Algorithm~\ref{algo:SPMSleepingBandit} (in Appendix~\ref{appendix:SPMSleepingBandits}) guarantees that for all $a \in [K]$,
    \begin{align*}
      \E[R_{T,a}] \leq O\left(\sqrt{\frac{(K^{1-\alpha} - 1)(\max_{t \in [T]}\abs{\sA_t})^\alpha}{\alpha(1-\alpha)}T}\right),
  \end{align*}
  \label{thm:SPMSleepingBanditBound}
\end{theorem}
Our Algorithm~\ref{algo:SPMSleepingBandit} is a combination of Algorithm~\ref{algo:optimalBOBWSparsity} and the SB-EXP3 algorithm in~\cite{NguyenAndMehta2024SBEXP3}. More specifically, Algorithm~\ref{algo:SPMSleepingBandit} uses the estimated cumulative \emph{regret} (instead of losses) to compute $q_t$ in the FTRL update. Then, the sampling probability vector $p_t$ is obtained by a filtering step $p_{t,i} = \frac{q_{t,i}\I{i \in \sA_t}}{\sum_{j=1}^K q_{t,j}\I{j \in \sA_t}}$.
While the bound in Theorem~\ref{thm:SPMSleepingBanditBound} is of the same order as in~\citet[][Theorem 2]{NguyenAndMehta2024SBEXP3}, it has the advantage of not requiring the knowledge of $\max_{t}\abs{\sA_t}$ in advance nor any complicated two-level doubling trick.

\section{Application II: $\sqrt{Q\ln(K)}$ Upper Bound with Unknown $Q$ using Optimistic FTRL}
\label{sec:SPMwOFTRLwReservoirSampling}
\looseness=-1 In this section, we propose a new approach for obtaining a BOBW $O(\frac{K\ln{T}}{\Delta_{\min}}, \sqrt{Q\ln{K}})$-bound with unknown $Q$. 
For ease of exposition, we assume losses are in $[0,1]$ and note that the analysis can be easily extended for losses in $[-1,1]$.
The new approach is based on applying real-time SPM on the Optimistic FTRL framework~\citep{Rakhlin2013OptimisticFTRL}, and then combining with the Reservoir Sampling algorithm~\citep{HazanAndKale11a}.

 In principle, our algorithm follows the same framework as~\citet{HazanAndKale11a,Bubeck2018ALT} where the learner maintains a reservoir $\sS_i$ of observed losses for each arm $i \in [K]$ and then uses the estimated mean $m_{t,i} = \tilde{\mu}_{t,i}$ of these reservoirs as the optimistic vector $m_t$ in Optimistic FTRL. 
In each round $t$, the learner chooses to perform either a reservoir sampling step for updating the reservoir $\sS_i$, or a FTRL learning step for minimizing the regret. 
When the FTRL learning step is performed in round $t$, the vector $q_t$ is computed by
\begin{align}
    q_t = \argmin_{x \in \Delta_K} \inp{m_t + L_{t-1}}{x} + \beta_t\left(\frac{1}{\alpha}(1-\sum_{i=1}^K x_i^\alpha)\right) - \gamma\sum_{i=1}^K \ln(x_i).
    \label{eq:qtInOFTRL}
\end{align}
Similar to Algorithm~\ref{algo:optimalBOBWSparsity}, the sampling probability vector $p_t$ is obtained by mixing with $\frac{1}{T}$, i.e, $p_t = \left(1 - \frac{K}{T}\right)q_t + \frac{1}{T}\1$. 
After an arm $I_t \sim p_t$ is drawn, the loss estimates are $\hat{\ell}_{t, i} = m_{t,i} + \frac{(\ell_{t,i} - m_{t,i})\I{I_t = i}}{p_{t,i}}$.
%
The learning rates $(\beta_t)_t$ are computed by real-time SPM, with $z_t$ and $h_t$ defined as
\begin{align*}
    z_t =  \min\left( \frac{(6d)^{2-\alpha}}{2(1-\alpha)}\tilde{p}_{t,I_t}^{2-\alpha}(\hat{\ell}_{t,I_t} - m_{t,I_t})^2, \frac{\beta_t18d^2}{\gamma}(\ell_{t,I_t} - m_{t,I_t})^2  \right), \quad h_t =\frac{1}{\alpha}\left(\sum_{i=1}^K p_{t,i}^\alpha - 1\right).
\end{align*}
The full procedure is given in~Algorithm~\ref{algo:OFTRLReservoirSampling} in Appendix~\ref{appendix:proofsforSectionOFTRLReservoirSampling}.
The following theorem states the BOBW bound for this approach.
\begin{theorem}
    Algorithm~\ref{algo:OFTRLReservoirSampling} (in Appendix~\ref{appendix:proofsforSectionOFTRLReservoirSampling}) guarantees the following bounds simultaneously
    \begin{itemize}
        \item In the adversarial regime:
        \begin{align}
            R_T \leq O\left( \sqrtfrac{(K^{1-\alpha} - 1)Q}{\alpha (1-\alpha)}\right).
            \label{eq:BOBWsqrtQLnKBoundAdv}
        \end{align}
        \item In the adversarial regime with a self-bounding constraint:
        \begin{align}
            R_T \leq O\left(\frac{(K-1)^{1-\alpha}K^\alpha \ln(T)}{\alpha(1-\alpha)\Delta_{\min}} + \sqrt{C\frac{(K-1)^{1-\alpha}K^\alpha \ln(T)}{\alpha(1-\alpha)\Delta_{\min}}} + \sqrt{\frac{(K-1)^{1-\alpha}K^{\alpha}}{\alpha(1-\alpha)}}\right)
            \label{eq:BOBWsqrtQLnKBoundSto}
        \end{align}
    \end{itemize}
    \label{thm:BOBWsqrtQLnKBound}
\end{theorem}
\begin{proof}(Sketch)
    Our analysis follows from the analysis of Algorithm~\ref{algo:optimalBOBWSparsity} and the observation by~\citet{HazanAndKale11a} that the reservoir sampling steps only add an $O(\ln(T)^2)$ amount to the regret bound.
    As a result, the bound~\eqref{eq:BOBWsqrtQLnKBoundSto} for adversarial regime with a self-bounding constraint follows almost identically to that of Algorithm~\ref{algo:optimalBOBWSparsity}. 
    For the bound~\eqref{eq:BOBWsqrtQLnKBoundAdv} in the adversarial regime, the total variation $Q$ naturally comes out of $\sum_{t=1}^T z_t$  as follows:
    \begin{align*}
        \E\left[\sum_{t=1}^T z_t\right] &\lesssim \frac{1}{1-\alpha}\E\left[\sum_{t=1}^T \sum_{i=1}^K (\ell_{t,i} - m_{t,i})^2\right] =  \frac{1}{1-\alpha} \E\left[\sum_{t=1}^T \norm{\ell_{t} - \tilde{\mu}_{t}}_2^2 \right] \qquad(\text{since } m_t = \tilde{\mu}_{t})\\
        % &\leq \frac{1}{1-\alpha}\left(\E\left[\sum_{t=1}^T \norm{\ell_{t} - \mu_{t}}_2^2 \right] + \E\left[\sum_{t=1}^T \norm{\tilde{\mu}_t - \mu_{t}}_2^2 \right]\right) \\
        &\leq \frac{1}{1-\alpha}\left(\E\left[\sum_{t=1}^T \norm{\ell_{t} - \mu_{T}}_2^2 \right] + \E\left[\sum_{t=1}^T \norm{\tilde{\mu}_t - \mu_{t}}_2^2 \right]\right) \\
        &\leq \frac{1}{1-\alpha}\left(Q + \sum_{t=1}^T \frac{Q}{t\ln(T)}\right) \leq O\left(\frac{Q}{1-\alpha}\right),
    \end{align*}
     where the second inequality follows from triangle inequality and 
     Lemma 10 in~\cite{HazanAndKale11a}, the third inequality is by Lemma 11 in~\cite{HazanAndKale11a}, and the last inequality is due to $\sum_{t=1}^T \frac{1}{t\ln{T}} \leq O(1)$. Together with~\eqref{eq:boundG} and $h_{\max} \leq \frac{K^{1-\alpha}-1}{\alpha}$, this implies~\eqref{eq:BOBWsqrtQLnKBoundAdv}.
\end{proof}
\begin{remark}
    While existing works~\citep{HazanAndKale11a, Bubeck2018ALT} require either the knowledge of $Q$ or sophisticated doubling tricks to estimate $Q$, our Algorithm~\ref{algo:OFTRLReservoirSampling} does not require such knowledge or any tricks.
    When $\alpha \to 1$, the bound in~\eqref{eq:BOBWsqrtQLnKBoundAdv} becomes $O(\sqrt{Q\ln(K)})$. This bound matches the best known upper bound in~\citet{Bubeck2018ALT} and never exceeds $O(\sqrt{TK\ln(K)})$ in the worst case, all while simultaneously having a $T$-optimal best-of-both-worlds guarantee.
\end{remark}

\section{Coordinate-Wise Stability-Penalty Matching}
\label{sec:CoordinateWiseSPM}
%%%%%%%%%%%%%%%%% Coordinate Wise Algorithm %%%%%%%%%%%%%%%%%%%%%%
\begin{algorithm}[t]
	\KwIn{$K \geq 3, T \geq 4K, \alpha \in (0,1), \beta_1 = \frac{8K}{1-\alpha}\1, \gamma = \max(6,48\sqrtfrac{\alpha}{1-\alpha}), d = 2$.}
    % \KwIn{Constant $c \geq 1$.}
    Initialize $L_{0,i} = 0$ for $i \in [K]$ \;
	% Initialize $q_{i, 1} = 1$ for $i = 1, 2, \dots, K$\; 

	\For{each round $t = 1, \dots, T$}{        
        Compute $m_t \in [0,1]^K$ where $m_{t,i} = \frac{1}{1 + \sum_{s=1}^{t-1}\I{I_s = i}}\left(\frac{1}{2} + \sum_{s=1}^{t-1} \I{I_s = i} \ell_{t,i}\right)$\;

        Compute $q_t$ by Equation~\eqref{eq:qtInOFTRL}\;

        Compute $p_t = (1-\frac{K}{T})q_t + \frac{1}{T}\1$\;
		
        Draw $I_t \sim p_t$ and observe $\ell_{t, I_t}$\;

		Compute loss estimate $\hat{\ell}_{t, i} = m_{t,i} + \frac{(\ell_{t,i} - m_{t,i})\I{I_t = i}}{p_{t,i}}$ and update $L_{t,i} = L_{t-1,i} + \hat{\ell}_{t,i}$\;
        
        Compute $z_{t,i} =  \I{i = I_t}(\ell_{t,I_t} - m_{t,I_t})^2\min\left( \frac{(6d)^{2-\alpha}}{2(1-\alpha)}\min\left\{p_{t,I_t}^{-\alpha}, \frac{1 - p_{t,I_t}}{p_{t,I_t}^2}\right\}, \frac{\beta_{t,i}18d^2}{\gamma}  \right)$ \;
        
        Compute $h_{t,i} = \frac{1}{\alpha}p_{t,i}^\alpha$\;
        
        Compute $\beta_{t+1, i} = \beta_{t,i} + \frac{z_{t,i}}{\beta_{t,i}h_{t,i}}$\;
        % such that $\beta_t \leq \beta_{t+1} \leq c\beta_t$\;
	}
	\caption{Coordinate-wise SPM with hybrid regularization for losses in $[0,1]$.}
	\label{algo:CoordinateWiseSPM}
\end{algorithm}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
We further generalize the SPM framework by introducing a new technique called coordinate-wise SPM (\texttt{CoWSPM}).
As the name suggests,~\texttt{CoWSPM} maintains separate learning rate $\beta_{t,i}$, stability term $z_{t,i}$ and penalty term $h_{t,i}$ for each arm $i \in [K]$.
In each round $t$,~\texttt{CoWSPM} updates the learning rates for each arm using the SPM update formula~\eqref{eq:SPM}, i.e.,
\begin{align}
    \beta_{t+1,i} = \beta_{t,i} + \frac{z_{t,i}}{\beta_{t,i}h_{t,i}}.
    \label{eq:CWSPMupdate}
\end{align}
Obviously, if $(z_{t,i})_{i \in [K]}$ and $(h_{t,i})_{i \in [K]}$ take the same values across all arms, this this approach recovers Algorithm~\ref{algo:optimalBOBWSparsity}.
{Instead,} we adopt a different approach where $z_{t,i} = 0$ for all $i \neq I_t$ so that only the learning rate $\beta_{t,I_t}$ of the observed arm $I_t$ is updated in round $t$. 
The full procedure of~\texttt{CoWSPM} is given in Algorithm~\ref{algo:CoordinateWiseSPM}, which uses the Optimistic FTRL framework with
\begin{align}
    \phi_t(x) = \sum_{i=1}^K \beta_{t,i}\left(\frac{-x_i^\alpha}{\alpha} + (1-x_{i})\ln(1-x_{i}) + x_{i}\right) - \gamma\sum_{i=1}^K \ln(x_i).
    \label{eq:CWSPMregularizer}
\end{align}
% Note that the negative part takes the form 
% \begin{align}
%     f(x_{i}) = \frac{-x_i^\alpha}{\alpha} + (1-x_{i})\ln(1-x_{i}) + x_{i}.
%     \label{eq:CWSPMregularizerFpart}
% \end{align}
This regularization function contains not only the $\alpha$-Tsallis entropy, but also a part of the Shannon entropy and a linear term. 
The addition of these terms into the regularizer has been done in~\citet{ItoCOLT2022aVariance} in order to have a regret bound containing the quantity $\tilde{p}_{t,i} = \min(p_{t,i}, 1-p_{t,i})$ for the stochastic setting.
This technique has a similar impact in our work, where it allows us to bound $z_{t,i}$ by a quantity containing $\tilde{p}_{t,i}$.
However, while we use the same technique to introduce $\tilde{p}_{t,i}$ into our bounds, our analysis develops fundamentally different technical lemmas from that of~\cite{ItoCOLT2022aVariance} in order to use this new regularizer in the real-time SPM framework. 
%
% For each arm $i \in [K]$, the penalty term is $h_{t,i} = \frac{1}{\alpha} p_{t,i}^\alpha$.
Next, to ensure that only $\beta_{t,I_t}$ is updated, we set $z_{t,i}$ by
\begin{align}
    z_{t,i} =  \I{i = I_t}(\ell_{t,I_t} - m_{t,I_t})^2\min\left( \frac{(6d)^{2-\alpha}}{2(1-\alpha)}\min\left\{p_{t,I_t}^{-\alpha}, \frac{1 - p_{t,I_t}}{p_{t,I_t}^2}\right\}, \frac{\beta_{t,i}18d^2}{\gamma}  \right),
    \label{eq:CoordinateWiseSPMzti}
\end{align}
so that $z_{t,I_t} \geq 0$ and $z_{t,i} = 0$ for $i \neq I_t$. 
The following theorem states the BOBW data-dependent bound of Algorithm~\ref{algo:CoordinateWiseSPM}, whose full proof is given in Appendix~\ref{appendix:ProofsForCoordinateWiseSPM}. 
The proof sketch outlines the main technical challenges in the analysis of Algorithm~\ref{algo:CoordinateWiseSPM}.
\begin{theorem}
    For any $K \geq 4, T \geq 4k$, \texttt{CoWSPM} (Algorithm~\ref{algo:CoordinateWiseSPM}) with $\alpha \in (0,1)$ guarantees the following bounds simultaneously
    \begin{itemize}
        \item In the adversarial regime: 
        \begin{align*}
            R_T \lesssim\min\left\{ \sqrt{K\ln(T)\min(Q_\infty, L^*, T-L^*)}, K^{\frac{\alpha}{2}}\sqrt{KT}\right\}.
        \end{align*}
        \item In the stochastic regime:
        \begin{align*}
            R_T \lesssim \frac{1}{\alpha (1-\alpha)}\sum_{i \neq i^*} \frac{\ln(T)}{\Delta_i}.
        \end{align*}
    \end{itemize}
    \label{thm:CWSPMbounds}
\end{theorem}
% \begin{itemize}
%     \item The analysis of SPM for stochastic regime relies on bounding $z_t$ by a term containing $\min(p_{t,i}, 1-p_{t,i})$. 
%     In coordinate-wise SPM, it is difficult to have a bound $z_{t,i}$ that contains $\tilde{p}_{t,i}$. 
%     We resolve this by adding $(1-p_{i})\ln(1-p_i)$ to the regularization function, similar to~\cite{ItoCOLT2022aVariance}. 
%     This additional term has the two nice properties: it does not dominate $p_i^\alpha$ (see Lemma~\ref{lemma:xalphadominatesxminus1lnxminus1}) and at the same time, it provides a bound on $z_{t,i}$ that contains $\tilde{p}_{t,i^*}$ where $i^*$ is the optimal arm.

%     Note that while we use the same technique to introduce $1-p_{t,i^*}$ into our bound, our analysis develops fundamentally different technical lemmas from that of~\cite{ItoCOLT2022aVariance} in order to use this new regularizer in the SPM framework.
% \end{itemize}


\begin{proof}(Sketch)
    Intuitively, coordinate-wise SPM consists of $K$ separate real-time SPM processes, one for each arm. 
Similar to~\citet{ItoCOLT2022aVariance}, we find that this more refined approach enables deriving a bound (for the adversarial regime) that is adaptive to simultaneously different data-dependent quantities such as $Q_\infty$ and $L^*$.
However, having separate learning rates introduces several new technical challenges. First, the analysis developed for Algorithm~\ref{algo:optimalBOBWSparsity} that bounds $q_{t+1, i} = O(q_{t, i})$ for all $i \in [K]$ no longer applies because in each round $t$, the learning rates $(\beta_{t,i})_{i \in [K]}$ can be arbitrarily different from each other. 
The \texttt{CoWSPM} algorithm resolves this by using $\beta_{t+1, i} = \beta_{t,i}$ for $i \neq I_T$ so that it only need $q_{t+1, i} = O(q_{t,i})$ to hold for $i = I_t$ since
\begin{align*}
    \phi_t(q_{t+1}) - \phi_{t+1}(q_{t+1}) &= \sum_{i=1}^K (\beta_{t+1,i} - \beta_{t,i})(-f(q_{t+1, i})) = (\beta_{t+1, I_t} - \beta_{t,I_t})f(q_{t+1, I_t}).
\end{align*}

The second and also more important challenge is that even if $q_{t+1, i} = O(q_{t, i})$, the naive decomposition of the $\alpha$-Tsallis entropy into its coordinate-wise form $-\psi_{TE}(x) = \frac{1}{\alpha}\sum_{i=1}^K (x_i^\alpha - x_i)$ and then assigning $h_{t,i} = \frac{1}{\alpha} (x_i^\alpha - x_i)$ does \emph{not} guarantee that $h_{t+1, i} = O(h_{t,i})$. This is because the function $x \mapsto x^\alpha - x$ gets arbitrarily close to $0$ when $x$ gets close to $1$.
% Note that this observation also applies to the function $-f(x)$ with $f(x)$ defined in~\eqref{eq:CWSPMregularizerFpart}.
This prompts a different choice for $h_{t,i}$ rather than $-f(p_{t,i})$. 
Algorithm~\ref{algo:CoordinateWiseSPM} uses $h_{t,i} = \frac{1}{\alpha}p_{t,i}^\alpha$, which is monotonically increasing and ensures that $h_{t+1,I_t} = O(h_{t,I_t})$ for $q_{t+1,i} = O(q_{t,i})$. 
This choice of $h_{t,i}$ is justified by the technical Lemma~\ref{lemma:xalphadominatesxminus1lnxminus1}, which states that $(x-1)\ln(1-x) \leq x^\alpha$ for any $x, \alpha \in [0,1]$.

% The last part of the proof is ensure that the product $\E[h_{t,i}z_{t,i}]$, where the new stability term $z_{t,i}$ defined in~\eqref{eq:CoordinateWiseSPMzti}, contains $\tilde{p}_{t,i}$ instead of just $p_{t,i}$. 
% This is proved in Lemma~\ref{lemma:CoordinateWiseSPMztiIsGoodForStochasticBandits}, which shows that $\E_{I \sim p}\left[\I{I = i}p_i^\alpha \min\left( p_i^{-\alpha}, \frac{1-p_i}{p_i^2} \right)\right] \leq 2\min(p_i, 1-p_i)$ holds for any $\alpha \in (0,1)$ and $p \in \Delta_K$.
% \end{proof}
Finally, we prove that with $z_{t,i}$ defined in~\eqref{eq:CoordinateWiseSPMzti}, the product $\E[h_{t,i}z_{t,i}]$ is upper bounded by a quantity containing $\tilde{p}_{t,i}$ and thus an $O(\sum_{i \neq i^*}\frac{\ln{T}}{\Delta_i})$ regret bound holds for stochastic bandits.
This is handled by Lemma~\ref{lemma:CoordinateWiseSPMztiIsGoodForStochasticBandits}, which shows that $\E_{I_t}\left[z_{t,i}\right] \leq 2\min(p_{t,i}, 1-p_{t,i})$.
\end{proof}

\begin{remark}
    Theorem~\ref{thm:CWSPMbounds} holds for all $\alpha \in (0,1)$. In particular, for $\alpha \neq \frac{1}{2}$,  we do not require any additional assumptions such as the $\Delta_i$ being known in order to get the $T$-optimal BOBW bound.
    This is a major difference compared to the Tsallis-INF algorithm~\citep{Zimmert2021TsallisINF}.
    On the other hand, the adversarial bound in Theorem~\ref{thm:CWSPMbounds} has an extra factor $\sqrt{K^\alpha}$. It is unclear to us whether this extra factor is a fundamental limitation of~\texttt{CoWSPM} or an artifact of our analysis.
\end{remark}

\section{Conclusion and Future Works}
\label{sec:conclusion}
\looseness=-1 We developed real-time SPM, an extension of the SPM method originally developed for obtaining best-of-both-worlds bounds in bandits problems.
We showed that real-time SPM algorithms achieve novel bounds that are simultaneously best-of-both-worlds, data-dependent and have optimal dependency on $T$ in both stochastic and adversarial regimes. Our bounds also have optimal dependency on the data-dependent quantities such as sparsity or total variation of the loss sequence without knowing them nor using sophisticated estimation tricks. Future work includes applying real-time SPM on other bandits problems, such as contextual linear bandits, and making real-time SPM adaptive towards other challenging data-dependent quantities like $\ell_1$ and $\ell_2$-norm path-length bounds.

% Acknowledgments---Will not appear in anonymized version
% \acks{We thank a bunch of people and funding agency.}

\bibliography{colt2025-SPMDataDependentBoundsMAB}
\input{appendix.tex}

\end{document}
