%%%%%%%% ICML 2025 EXAMPLE LATEX SUBMISSION FILE %%%%%%%%%%%%%%%%%

\documentclass{article}
\usepackage{adjustbox}
% Recommended, but optional, packages for figures and better typesetting:
\usepackage{microtype}
\usepackage{graphicx}
\usepackage{booktabs} % for professional tables
\usepackage{dblfloatfix}
\usepackage{colortbl}
% hyperref makes hyperlinks in the resulting PDF.
% If your build breaks (sometimes temporarily if a hyperlink spans a page)
% please comment out the following usepackage line and replace
% \usepackage{icml2025} with \usepackage[nohyperref]{icml2025} above.
% \usepackage{hyperref}
\usepackage{times}             % Times font
\usepackage{booktabs}          % Professional tables
\usepackage{amsmath,amssymb,amsfonts} % Math packages
\usepackage{graphicx}          % For including graphics
\usepackage{caption}           % Better captions
\usepackage{xcolor}            % Colors
\usepackage{float}             % Floating environments
\usepackage[numbers]{natbib}   % Numerical references (compact)
\usepackage{multicol}          % Multiple columns
\usepackage[bookmarks=true]{hyperref} % Hyperlinks
\usepackage{multirow}          % Multi-row cells in tables
\usepackage{array}             % Array and tabular tweaks
\usepackage{subcaption}        % Sub-figures
\usepackage{booktabs}          % Extra commands for tables (already loaded, repeated for clarity)
\usepackage{wrapfig}
\usepackage[misc]{ifsym}

% ExplSyntaxOn macros
\ExplSyntaxOn
\newcommand{\plus}[1]{
  \fp_compare:nTF { #1 >= 10 }
    {({\color{blue} \textbf{+#1}})} % If #1 >= 10
    {({\color{blue} +#1})}         % Else
}
\ExplSyntaxOff

\ExplSyntaxOn
\newcommand{\minus}[1]{
  \fp_compare:nTF { #1 >= 10 }
    {({\color{red} \textbf{-#1}})} % If #1 >= 10
    {({\color{red} -#1})}         % Else
}
\ExplSyntaxOff

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% CUSTOM COMMANDS
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newcommand{\SE}{\mathrm{SE}}
\newcommand{\SO}{\mathrm{SO}}
\newcommand{\OO}{\mathrm{O}}
\newcommand{\E}{\mathrm{E}}
\newcommand{\T}{\mathrm{T}}
\newcommand{\inv}{\mathrm{inv}}
\newcommand{\reg}{\mathrm{reg}}
\newcommand{\high}{\text{high}}
\newcommand{\low}{\text{low}}
\newcommand{\equi}{\mathrm{equiv}}
\newcommand{\ac}{\mathbf{a}}
\newcommand{\grip}{\mathbf{c}}
\newcommand{\ob}{\mathbf{o}}
\newcommand{\am}{\mathbf{A}}
\newcommand{\lt}{\mathbf{l}}
\newcommand{\vc}{\mathbf{V}}
\newcommand{\ti}{\mathbf{t}}
\newcommand{\obj}{\mathbf{obj}}
\newcommand{\num}{\mathbf{nums}}
\newcommand{\C}{\mathbf{C}}
\newcommand{\g}{\mathbf{g}}
\newcommand{\Cu}{C_{u}}

\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}

\usepackage{xparse}
\ExplSyntaxOn
\NewDocumentCommand{\definealphabet}{mmmm}
 {% #1 = prefix, #2 = command, #3 = start, #4 = end
  \int_step_inline:nnn { `#3 } { `#4 }
   {
    \cs_new_protected:cpx { #1 \char_generate:nn { ##1 }{ 11 } }
     {
      \exp_not:N #2 { \char_generate:nn { ##1 } { 11 } }
     }
   }
 }
\ExplSyntaxOff
\definealphabet{bb}{\mathbb}{A}{Z}
\definealphabet{bf}{\mathbf}{A}{Z}
\definealphabet{mc}{\mathcal}{A}{Z}
\definealphabet{mf}{\mathfrak}{A}{Z}
\definealphabet{mf}{\mathfrak}{a}{z}


% Attempt to make hyperref and algorithmic work together better:
\newcommand{\theHalgorithm}{\arabic{algorithm}}

% Use the following line for the initial blind version submitted for review:
% \usepackage{icml2025}

% If accepted, instead use the following line for the camera-ready submission:
\usepackage[accepted]{arxiv}

% For theorems and such
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{amsthm}

% if you use cleveref..
\usepackage[capitalize,noabbrev]{cleveref}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% THEOREMS
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\theoremstyle{plain}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{assumption}[theorem]{Assumption}
\theoremstyle{remark}
\newtheorem{remark}[theorem]{Remark}

% Todonotes is useful during development; simply uncomment the next line
%    and comment out the line below the next line to turn off comments
%\usepackage[disable,textsize=tiny]{todonotes}
\usepackage[textsize=tiny]{todonotes}


% The \icmltitle you define below is probably too long as a header.
% Therefore, a short form for the running title is supplied here:
\icmltitlerunning{Hierarchical Equivariant Policy via Frame Transfer}

\begin{document}

\twocolumn[
\icmltitle{Hierarchical Equivariant Policy via Frame Transfer}

% It is OKAY to include author information, even for blind
% submissions: the style file will automatically remove it for you
% unless you've provided the [accepted] option to the icml2025
% package.

% List of affiliations: The first argument should be a (short)
% identifier you will use later to specify author affiliations
% Academic affiliations should list Department, University, City, Region, Country
% Industry affiliations should list Company, City, Region, Country

% You can specify symbols, otherwise they are numbered in order.
% Ideally, you should not use this facility. Affiliations will be numbered
% in order of appearance and this is the preferred way.
% \icmlsetsymbol{equal}{*}

\begin{icmlauthorlist}
\icmlauthor{Haibo Zhao\textsuperscript{*}}{neu}
\icmlauthor{Dian Wang\textsuperscript{* \Letter}}{neu}
\icmlauthor{Yizhe Zhu}{neu}
\icmlauthor{Xupeng Zhu}{neu}
\icmlauthor{Owen Howell}{neu,comp}
\icmlauthor{Linfeng Zhao}{neu}
\icmlauthor{Yaoyao Qian}{neu}
% \icmlauthor{Firstname5 Lastname5}{yyy}
% \icmlauthor{Firstname6 Lastname6}{sch,yyy,comp}
% \icmlauthor{Firstname7 Lastname7}{comp}
%\icmlauthor{}{sch}
\icmlauthor{Robin Walters}{neu}
\icmlauthor{Robert Platt}{neu,comp}
%\icmlauthor{}{sch}
%\icmlauthor{}{sch}
\\
\small{\texttt{\{zhao.haib,wang.dian,zhu.xup,howell.o,zhao.linf,qian.ya,r.walters,r.platt\}@northeastern.edu}}
\end{icmlauthorlist}

\icmlaffiliation{neu}{Northeastern University}
\icmlaffiliation{comp}{Boston Dynamics AI Institute}
\icmlcorrespondingauthor{Dian Wang}{wang.dian@northeastern.edu}
% \icmlcorrespondingauthor{Haibo Zhao}{zhao.haib@northeastern.edu}

% \icmlcorrespondingauthor{Firstname2 Lastname2}{first2.last2@www.uk}

% You may provide any keywords that you
% find helpful for describing your paper; these are used to populate
% the "keywords" metadata in the PDF but will not be shown in the document
\icmlkeywords{Machine Learning, ICML}

\vskip 0.3in
]

% this must go after the closing bracket ] following \twocolumn[ ...

% This command actually creates the footnote in the first column
% listing the affiliations and the copyright notice.
% The command takes one argument, which is text to display at the start of the footnote.
% The \icmlEqualContribution command is standard text for equal contribution.
% Remove it (just {}) if you do not need this facility.

%\printAffiliationsAndNotice{}  % leave blank if no need to mention equal contribution
\printAffiliationsAndNotice{\icmlEqualContribution} % otherwise use the standard text.

\begin{abstract}

% Recent advances in hierarchical policy learning highlight the advantages of decomposing systems into high-level and low-level agents, enabling efficient long-horizon reasoning and precise fine-grained control. However, the interface between these hierarchy levels remains underexplored, and existing hierarchical methods often need extensive demonstrations to achieve robust performance. To address these issues, we propose a novel Hierarchical Equivariant Policy (HEP). One of our main contributions is a Frame Transfer interface for hierarchical policy learning, which uses the high-level agent's output as a coordinate frame for the low-level agent, providing a strong inductive bias while retaining flexibility. Additionally, we integrate domain symmetries into both levels to improve sample efficiency, and theoretically demonstrate the system's overall equivariance. A key insight of our approach is that hierarchical and equivariant policy learning reinforce each other naturally: hierarchical decomposition focuses each sub-policy on a smaller sub-problem, while equivariance ensures consistent behavior under symmetric transformations at all hierarchy levels. HEP achieves state-of-the-art performance in complex robotic manipulation tasks, demonstrating significant improvements in both simulation and real-world settings.

Recent advances in hierarchical policy learning highlight the advantages of decomposing systems into high-level and low-level agents, enabling efficient long-horizon reasoning and precise fine-grained control. However, the interface between these hierarchy levels remains underexplored, and existing hierarchical methods often ignore domain symmetry, resulting in the need for extensive demonstrations to achieve robust performance. To address these issues, we propose Hierarchical Equivariant Policy (HEP), a novel hierarchical policy framework. We propose a frame transfer interface for hierarchical policy learning, which uses the high-level agent's output as a coordinate frame for the low-level agent, providing a strong inductive bias while retaining flexibility. Additionally, we integrate domain symmetries into both levels and theoretically demonstrate the system's overall equivariance. HEP achieves state-of-the-art performance in complex robotic manipulation tasks, demonstrating significant improvements in both simulation and real-world settings.

% We introduce Frame Transfer, a novel interface for hierarchical policy learning, which adapts the high-level agent's output as a coordinate frame for the low-level agent, providing a strong inductive bias while retaining flexibility. Additionally, we integrate domain symmetries into both levels and theoretically demonstrate that Frame Transfer preserves the system's overall symmetry. Our resulting model, the Hierarchical Equivariant Policy (HEP), achieves state-of-the-art performance in complex robotic manipulation tasks, demonstrating significant improvements in both simulation and real-world settings.
% Recent advances in hierarchical policy learning have demonstrated the benefits of decomposing systems into high-level and low-level agents, outperforming end-to-end models by delegating long-horizon reasoning and fine-grained control to different levels. However, the interface between different hierarchy levels remains underexplored, and existing methods still require a large number of demonstrations despite their improvements. In this work, we propose Frame Transfer, a novel interface in hierarchical policy learning which adapts the high-level output as a coordinate frame for the low-level agent to provide both strong inductive bias and flexibility. Moreover, we integrate domain symmetry in both levels, and theoretically analyze that Frame Transfer maintains the overall symmetrical structure. Our complete model, Hierarchical Equivariant Policy (HEP), significantly outperforms state-of-the-art baselines in complex robotic manipulation tasks in both simulation and the real-world.

% In this work, we propose a novel Hierarchical Equivariant Policy (HEP) that further enhances efficiency by integrating domain symmetry, a more flexible Frame Transfer Interface, and a stacked voxel representation. We also theoretically analyze how the Frame Transfer Interface augments our SO(2)-equivariant low-level diffuser with T(3) translation equivariance. Simulation results across 30 simulation tasks demonstrate better sample efficiency compared to state-of-the-art baselines. Additionally, we validate HEP’s generalization and efficiency in real-world experiments, including a one-shot pick-and-place task and three challenging long-horizon tasks that other baselines fail to solve.

% Recent advances in hierarchical policy learning have demonstrated the benefits of decomposing systems into high-level and low-level agents, outperforming end-to-end models in terms of efficiency. However, the interface between these agents remains underexplored, and existing methods still require a large number of demonstrations despite their improvements. We propose a novel $\SO(2)\times\T(3)$-equivariant Hierarchical Equivariant Policy (HEP) that further enhances efficiency by integrating domain symmetry, a more flexible Frame Transfer Interface, and a stacked voxel representation. We also theoretically analyze how the Frame Transfer Interface augments our SO(2)-equivariant low-level diffuser with T(3) translation equivariance. Simulation results across 30 simulation tasks demonstrate better sample efficiency compared to state-of-the-art baselines. Additionally, we validate HEP’s generalization and efficiency in real-world experiments, including a one-shot pick-and-place task and three challenging long-horizon tasks that other baselines fail to solve.
\end{abstract}

\section{Introduction}
\label{intro}

Learning-based approaches have emerged as a powerful paradigm for developing control policies in sequential decision-making tasks, such as robotic manipulation. By leveraging data-driven methods, policy learning provides a scalable framework for addressing tasks with complex dynamics and high-dimensional observation spaces. Recent advancements in end-to-end policy learning~\cite{zhao2023learning,chi2023diffusionpolicy} have shown promising results in mapping raw sensory inputs to low-level actions such as end-effector trajectories. 
% Imitation Learning (IL) has emerged as a powerful paradigm for training control policies in sequential decision-making tasks like robotic manipulation. By learning directly from expert demonstrations, IL provides a scalable framework for solving tasks with complex dynamics and high-dimensional observation spaces. Recent advancements in end-to-end imitation learning~\cite{zhao2023learning,chi2023diffusionpolicy} have shown promising results in mapping raw sensory inputs to low-level actions such as end-effector trajectories. 
While these methods exhibit state-of-the-art performance when large amounts of training data are available, they struggle in scenarios with only limited data,
% in scenarios where training data is limited, 
due to 
% they struggle to generalize on tasks with few expert examples. This is due to
the large function space required to parameterize complex end-to-end mappings.

\begin{figure}[t]
    \centering
    \includegraphics[width=\linewidth]{img/fig1_dian_v5.png}
    \caption{\textbf{Hierarchical Equivariant Policy (HEP)} is composed of a high-level agent that predicts a coarse translation, a low-level agent that predicts the fine-grained trajectory, and a novel Frame Transfer interface that transfers the coordinate frame of the low-level to the predicted keypose frame from the high-level.}
    \label{fig:frame_transfer}
\end{figure}
A promising alternative strategy is to employ a hierarchical structural prior that decomposes the policy into different levels, e.g., a high-level agent responsible for identifying a goal pose 
% (e.g., an SE(3) pose),
and a low-level agent for trajectory refinement. Hierarchical methods can reduce the complexity of the policy function space by delegating long-horizon reasoning to the high-level module and fine-grained control to the low-level module,
% This decomposition reduces the complexity of each policy, 
enabling efficient learning and execution. Despite their promise, one underexplored question in hierarchical policy learning is what is the right interface between different levels. For example, in robotic manipulation, existing hierarchical methods~\cite{ma2024hierarchical,xian2023chaineddiffuser} often impose rigid constraints on the interface between the high-level and low-level agents, where the high-level action is used as the last pose in the low-level trajectory. 
% where the high-level output acts as a fixed guide for the low-level policy. 
This constraint limits flexibility and often requires both levels to perform fine-grained reasoning in high-dimensional spaces, negating some of the potential benefits of the hierarchical design. Moreover, prior hierarchical methods focus solely on the hierarchical decomposition and do not exploit the domain symmetries often present in robotic tasks, missing an opportunity to further improve generalization and efficiency.

% , where recent works like Hierarchical Diffusion Policy~\cite{ma2024hierarchical} and ChainedDiffuser~\cite{xian2023chaineddiffuser} have shown that this decomposition allows for efficient learning and execution of manipulation policies.






% Recently, end-to-end policy learning methods have emerged as a powerful approach in robotic manipulation, enabling robots to learn direct mappings from raw visual inputs to low-level actions such as end-effector trajectories. For instance, works like Diffusion Policy~\cite{chi2023diffusionpolicy} and ACT~\cite{zhao2023learning} have achieved impressive task performance. However, while these methods exhibit state-of-the-art performance in handling diverse and high-dimensional action spaces, they are typically not efficient enough due to the large function space required to handle such complex end-to-end mappings, thus demanding large datasets of demonstrations for training. 

% A promising alternative paradigm is to insert a hierarchical structural prior that decomposes the policy into two levels: a high-level agent responsible for a coarse action (e.g., an SE(3) pose), and a low-level agent for trajectory refinement. Hierarchical methods can reduce the complexity of the policy function space by delegating long-horizon reasoning to the high-level module and fine-grained control to the low-level module, where recent works like Hierarchical Diffusion Policy~\cite{ma2024hierarchical} and ChainedDiffuser~\cite{xian2023chaineddiffuser} have shown that this decomposition allows for efficient learning and execution of manipulation policies. 

% Despite its advantages, hierarchical structure in robotic policy learning remains an underexplored area; moreover, existing hierarchical methods often impose rigid constraints on the interface between the high-level and low-level agents. For example, the high-level action is used as the last pose in the low-level trajectory. 
% This constraint limits flexibility and often requires both levels to perform fine-grained reasoning in high-dimensional spaces, negating some of the potential benefits of the hierarchical design.

In this paper, we propose a novel hierarchical policy learning framework that overcomes these limitations by introducing a more flexible and efficient interface between the high-level and low-level agents. Specifically, our high-level agent predicts a \emph{keypose} in the form of a coarse 3D location representing a subgoal of the task.
% 3D translation using a 3D fully convolutional network (FCN). 
This location is then used to construct the coordinate frame for the low-level policy, enabling it to predict trajectories relative to this keypose frame, as shown in~\autoref{fig:frame_transfer}. This \emph{Frame Transfer} interface maintains a strong inductive bias (by anchoring the low-level policy to a subgoal) yet offers structural flexibility (allowing the low-level policy to refine trajectories locally). Furthermore, Frame Transfer offers a natural fit for integrating domain symmetry by decomposing it into the global symmetry of the subgoal (i.e., the subgoal should transform with the scene) and a local symmetry of the low-level policy (i.e., it should behave consistently in the local keypose frame).
% Furthermore, Frame Transfer offers a natural fit for integrating domain symmetry into the policy: if the scene is translated or rotated, the subgoal (keypose) should transform accordingly, and the low-level policy should behave consistently in its keypose frame. 
By incorporating equivariant structures at both levels, our entire hierarchical system becomes more robust to spatial variations, resulting in significantly improved sample efficiency. 
% Furthermore, our framework leverages group symmetries by incorporating equivariant structures in both the high-level and low-level agents, resulting in significantly improved sample efficiency. 
Lastly, to better encode 3D sensory information, we adopt a stacked voxel representation~\cite{zhou2018voxelnet}, ensuring rich visual features and fast computation.
% an equivariant 3D FCN in the high level and the recent advancements in equivariant diffusion policy at the low level, resulting in significantly improved sample efficiency. Additionally, we adopt a stacked voxel representation at both levels, ensuring rich visual features even at limited voxel resolutions.

We summarize our contributions as follows: 
\begin{itemize}
\item We propose Hierarchical Equivariant Policy (HEP), a novel, sample-efficient hierarchical policy learning framework.
\item We introduce Frame Transfer as an interface for hierarchical policy learning, providing effective and flexible policy decomposition.
\item We theoretically demonstrate the equivariance of HEP, showing its spatial generalizability. Although equivariance has been used in policy learning, our work is the first to study it in a hierarchical policy.
    % \item We propose Hierarchical Equivariant Policy (HEP), a novel policy learning framework with three main components: equivariant backbone, Frame Transfer interface, and stacked voxel encoder. 
    % \item We theoretically demonstrate the equivariance of HEP, and prove that both Frame Transfer and the stacked voxel encoder preserve the symmetry. Although equivariance has been used in policy learning, our work is the first to study it in a hierarchical policy.
    % \item We propose a novel Frame Transfer interface for hierarchical policy learning, and a Stacked Voxel encoder  
    \item We provide a thorough evaluation of our method in both simulation and the real-world. Among 30 RLBench~\cite{9001253} tasks, HEP outperforms state-of-the-art baselines by an average of 10\% to 23\% in different settings, 
    % in 28 out of 30 tasks in RLBench~\cite{9001253},
    % by $ \sim 10-30 \% $ on many tasks, 
    with particular improvement on tasks requiring fine control or long-horizon reasoning.
    % \item We demonstrate HEP can do accurate single-shot generalization and is robust to changes in background/environment.
\end{itemize}


% where the low level action prediction (e.g., end-effector trajectory) is conditioned on a high-level action (e.g., an SE(3) pose that the trajectory is trying to reach). This type of approach effectively decompose the low-level action prediction into two steps, thereby improving the efficiency.

% However, existing hierarchical models often formulate the high-level action as a constraint in the low-level action prediction. As a result of this constraint, both the high-level and the low-level need to perform fine 6DoF reasoning, hindering the efficiency of the hierarchical structure. 

% - end to end policy learning became very powerful recently
% - those methods are not efficient enough, 
% - alternatively, we can use hierarchical architectures to 
% - key questions in hierarchical policy learning: 

% Contributions:
% - sample efficient, SE(2)-equivariant hierarchical policy learning framework
% - novel interface between high level and low level, predicts low level action relative to a 3D point predicted by high level
% - Stacked voxel representation for policy learning


\input{related_work.tex}

\section{Background}

\subsection{Problem Definition}
In this paper, we focus on visuomotor policy learning via Behavior Cloning (BC) in robotic manipulation. We aim to learn a policy $\pi: O\to A$ to map from the observation space $O$ to the action space $A$.
% We aim to learn a policy $\pi$ for visuomotor control, where $\pi$ is a map from the observation space $O$ to the action space $A$.

To define the observation and action spaces, let $s=(x, y, z, q, c)\in S=\bbR^3 \times \SO(3) \times \bbR$ be the space of gripper states where \((x,y,z)\) is a 3D position, \(q \in SO(3)\) is an orientation, \(c\) is a gripper aperature (open width). The observation space is 
% at time $t$, 
$o\in O=\bbR^{n\times(3+k)}\times S^t$ including both a point cloud $P=\{p_i : p_i=(x_i, y_i, z_i, f_i)\in\bbR^{3+k}\}$ with $k$ dimensional point features (e.g., $k=3$ for RGB) and $t$ history steps of the gripper state. The action $a=\{a_1, a_2, \dots, a_m\}\in A=S^m$ contains $m$ control steps of the gripper state.
% $a=\{a_m|a_m\in\bbR^3 \times \SO(3) \times \bbR\}$ where each individual action $a_m \;=\; (\,x,\,y,\,z,\,q,\,c\,)$
% where \((x,y,z)\) is a 3D translation of the end effector, \(q \in SO(3)\) is its orientation, \(c\) is a gripper command (e.g., open/close width). 

% and the history pose of the gripper. Our system will predict an action $\mathbf{a} \in A$ containing a number of control steps: $\mathbf{a}\;=\;\{\,a^1,\; a^2,\;\dots,\;a^n\}$.
% Each individual action $a \;=\; (\,x,\,y,\,z,\,q,\,c\,)$
% where \((x,y,z)\) is a 3D translation of the end effector, \(q \in SO(3)\) is its orientation, \(c\) is a gripper command (e.g., open/close width).

% \subsubsection{Symmetry of Policy}
% To design a $T(3) \times \SO(2)$ equivariant policy
% % , i.e., translation equivariant in 3D and rotation equivariant around the $z$ axis. Compared to a full $\SE(3)$ equivariance, $T(3) \times \SO(2)$ better captures the ground truth symmetry in robotic tasks and will not waste model power on embedding unnecessary out-of-plane rotation symmetry. 
% , let $g=(t, r) \in T(3) \times \SO(2)$, 
% % where $R_\theta=\begin{bsmallmatrix}
% % \cos\theta & -\sin\theta \\
% % \sin\theta & \cos\theta
% % \end{bsmallmatrix}$, 
% our policy should satisfy the equivariant constraint
% \begin{equation}
% \pi(g \cdot a \mid g \cdot o) = \pi(a \mid o),
% \end{equation}
% where $g=(t, r)$ acts on the action $a$ by transforming the gripper pose command, $ga = (\rho_1(r) (x + t_x, y+t_y), z+t_z, {R_r}\cdot q, c)$ where $R_r = \begin{bsmallmatrix}
% \rho_1(r) & 0 \\
% 0 & 1
% \end{bsmallmatrix}$.
% $g$ acts on $o$ through transforming the point cloud $P=\{p_1, p_2, \dots\}, p_i=(x_i, y_i, z_i)\in \bbR^3$ via $gp_i=(\rho_1(r) (x_i + t_x, y_i+t_y), z_i+t_z)$, and transforming the gripper pose in the same way as $a$. 

% % Our policy is equivariant under the rotations $\g$: \begin{equation}
% % \pi(\g \cdot a \mid \g \cdot o) = \pi(a \mid o)
% % \end{equation}
% As shown in \autoref{}, a transformed observation would lead to a transformed predicted trajectory. We use escnn \citet{} to implement $SO(2)$-equivariance.


\subsection{Equivariance}\label{BBh}

A function $f$ is equivariant if it commutes with the transformations of a symmetry group $G$, where $\forall g\in G, f(g x) = g  f(x)$. This is a mathematical way of expressing that $f$ is symmetric with respect to $G$: if we evaluate $f$ for transformed versions of the same input, we should obtain transformed versions of the same output. 

Our objective is to design a policy that is symmetric (equivariant) under the group $g\in T(3)\times \SO(2)$, 
where $T(3)$ represents the group of 3D translations, and $\SO(2)$ represents the group of planar rotations around the z-axis of the world coordinate system, $
\pi(g o) = g\pi( o)
$. This symmetry captures the ground truth structure in many robotic tasks without enforcing unnecessary out-of-plane rotation equivariance (which is often invalid due to gravity and the canonical pose of objects). 
% As shown in \autoref{fig:equi}, a transformed observation should lead to a transformed predicted trajectory. 
% Leveraging this symmetry introduces an appropriate inductive bias to the model, improving sampling efficiency and generalization.

% The group $T(3)\times \SO(2)$ can be naturally decomposed into translation $T(3)$, which is handled using methods like 3D convolution, and rotation $\SO(2)$, which is addressed via network design by introducing equivariant layers~\cite{} that respect $\SO(2)$ transformations through appropriate representations of $\SO(2)$ or its subgroups. 

To define a $T(3) \times \SO(2)$ equivariant policy, we first need to define how the group element acts on the observation and the action.
% , i.e., translation equivariant in 3D and rotation equivariant around the $z$ axis. Compared to a full $\SE(3)$ equivariance, $T(3) \times \SO(2)$ better captures the ground truth symmetry in robotic tasks and will not waste model power on embedding unnecessary out-of-plane rotation symmetry. 
let $g=(t, R_\theta) \in T(3) \times \SO(2)$ where $t=(t_x, t_y, t_z)$ and $R_\theta$ is the $2\times2$ rotation matrix, 
% where $R_\theta=\begin{bsmallmatrix}
% \cos\theta & -\sin\theta \\
% \sin\theta & \cos\theta
% \end{bsmallmatrix}$, 
$g$ acts on the action $a$ by transforming the gripper pose command. Let $ \tilde{R}_\theta = 
\begin{bsmallmatrix}
R_\theta & 0 \\
0 & 1
\end{bsmallmatrix} $, $ga=\{ga_1, ga_2, \dots ga_m\}$ where 
\[
ga_i = (R_\theta (x_i + t_x, y_i+t_y), z_i+t_z, {\tilde{R}_\theta}\cdot q_i, c_i).
\]
$g$ acts on $o$ through transforming the gripper pose in the same way as $a$, and transforming the point cloud $P=\{p_i: p_i=(x_i, y_i, z_i, f_i)\in \bbR^{3+k}\}$ via $gP = \{gp_i\}$ where 
\[
gp_i=(R_\theta (x_i + t_x, y_i+t_y), z_i+t_z, f_i).
\]

% Our policy is equivariant under the rotations $\g$: \begin{equation}
% \pi(\g \cdot a \mid \g \cdot o) = \pi(a \mid o)
% \end{equation}


% \subsubsection{Group Action of $\SO(2)$}
% We focus on three particular representations of $g\in \mathrm{SO}(2)$ or its subgroup $g\in C_u$ (containing $u$ discrete rotations) that define how the group acts on different data. Specifically:

% % Our interest is mainly on the group $T(3)$ of 3D translation, as well as the group $\SO(2)$ of planar rotations (the rotation is around the
% % z-axis of the world coordinate system) and its subgroup $\C_u$ containing u discrete rotations. By introducing equivariant learning, our policy would have the correct inductive bias to improve sampling efficiency. We focus on three particular representations of $g\in \mathrm{SO}(2)$ or $g\in C_u$ in this paper:

% \underline{Trivial Representation $\rho_0$}: The trivial representation $\rho_0$ characterizes the action of $\mathrm{SO}(2)$ or $C_u$ on an invariant scalar $x \in \mathbb{R}$ such that $
% \rho_0(g) x = x.$
% This means that every group element $g$ leaves the scalar $x$ unchanged.

% \underline{Standard Representation $\rho_1$}: The standard representation $\rho_1$ defines how $\mathrm{SO}(2)$ or $C_u$ acts on a vector $v \in \mathbb{R}^2$ using a $2 \times 2$ rotation matrix. The action is given by 
% $
% \rho_\omega(g) v = \begin{psmallmatrix}
%   \cos g & -\sin g \\
%   \sin g & \cos g
% \end{psmallmatrix} v.
% $
% When $\omega = 1$, the representation $\rho_1(g)$ corresponds to the standard $2 \times 2$ rotation matrix.

% \underline{Regular Representation $\rho_{\text{reg}}$}: The regular representation $\rho_{\text{reg}}$ describes the action of $C_u$ on a vector $x \in \mathbb{R}^u$ via $u \times u$ permutation matrices. Let $g = r^m$ be an element of the cyclic group $C_u = \{1, r^1, \ldots, r^{u-1}\}$, and let $x = (x_1, x_2, \dots, x_u) \in \mathbb{R}^u$. Then the action is defined by $
% \rho_{\text{reg}}(g) x = \left( x_{u - m + 1}, x_{u - m + 2}, \dots, x_u, x_1, x_2, \dots, x_{u - m} \right). $
% This operation cyclically permutes the coordinates of $x$ in $\mathbb{R}^u$.\\
% \\
% A representation $\rho$ can also be constructed as a combination of different representations. Specifically, $\rho$ is defined as the direct sum $\rho = \rho_0^{n_0} \oplus \rho_1^{n_1} \oplus \rho_2^{n_2}$, 
% which belongs to the general linear group $GL(n_0 + 2n_1 + 2n_2)$. In this case, $\rho(g)$ is a block diagonal matrix of size $(n_0 + 2n_1 + 2n_2) \times (n_0 + 2n_1 + 2n_2)$ that acts on vectors $x \in \mathbb{R}^{n_0 + 2n_1 + 2n_2}$. 

% \subsubsection{Group Action of $T(3)$}
% The group $T(3)$ of 3D translations is an additive group, whose action is defined by shifting spatial coordinates. For example, for a point cloud $P=\{p_1, p_2, \dots\}$ where $p_i=(x_i, y_i, z_i)$, the action of $g\in T(3)$ is $t\cdot p_i = (x_i + t_x, y_i + t_y, z_i + t_z)$. Similarly, for voxel-based representations, $T(3)$ acts by shifting the spatial indices of the voxel grid. 
%Convolutions are inherently translationally invariant. <- I would say this
% Translation symmetry is naturally handled by operations such as 3D convolutions, which are inherently translation-equivariant.

\subsection{Voxel Maps as Function}\label{voxel_map_as_f}
In deep learning, voxel maps (3D volumetric data) are typically expressed
as tensors. However, it is sometimes convenient to express volumetric data in the form of functions over the 3D space. 
% However, it is sometimes convenient for reasoning the symmetry to express these as functions. 
Specifically, given a one-channel voxel map 
$V \in \mathbb{R}^{1 \times D \times H \times W}$, we may equivalently express $V$ as a continuous function
$
   \mcV : \mathbb{R}^3 \; \to \; \mathbb{R},
$
where $\mcV(x,y,z)$ describes the intensity value at the continuous world coordinate $(x,y,z)$. Notice that here the domain of $\mcV$ is the 3D world coordinate frame, not the discrete voxel indices. The relationship between the voxel indices and world coordinates is a linear map defined by the spatial resolution and the origin of the voxel grid. 

Similarly, if we have an $m$-channel voxel map
$V \in \mathbb{R}^{m \times D \times H \times W}$, we can interpret it as
$
   \mcV : \mathbb{R}^3 \; \to \; \mathbb{R}^m,
$
where each point $(x,y,z)$ in the volume maps to an $m$-dimensional feature vector. 
The group $g=(t, \theta)\in T(3)\times \SO(2)$ acts on a voxel feature map as
\begin{equation}
(g\mcV) (x, y, z) = \rho (\theta) \mcV (R_\theta^{-1}(x-t_x, y-t_y), z-t_z),
\end{equation}
where $t\in T(3)$ acts on $\mcV$ by translating the voxel location, while $\theta$ acts on $\mcV$ by both rotating the voxel location and transforming the feature vector via $\rho(\theta)\in GL(m)$, an $m\times m$ invertible matrix known as a group representation.
% is an $\SO(2)$ representation defining how the feature vector transforms under $\theta$.

\section{Hierarchical Equivariant Policy}
\label{HEP}
\begin{figure*}[t]
\centering
\includegraphics[width=\linewidth]{img/overview_dian_v5.png}
\caption{\textbf{Overview of Hierarchical Equivariant Policy (HEP).} In the highlevel (top), given a point cloud input, we first use an equivariant stacked voxel encoder (green) to process the point cloud and get a voxel feature map. The voxel feature map is then sent to an equivariant UNet (blue) to produce a high-level action probability map. After taking the argmax of the action map as the high-level action, we use Frame Transfer (yellow) to translate the coordinate frame of observation in the low-level (bottom). The translated observation is sent to the stacked voxel encoder (green, same architecture as the one used in the high-level), followed by an equivariant diffusion policy \cite{wang2024equivariant} (blue) to produce the low-level action.}
\label{fig:c2fedp-overview} % Add a label for referencing
\end{figure*}
% \subsection{Overview}\label{BB}

The main contribution of our paper is a Hierarchical Equivariant Policy that leverages equivariant learning in both the high-level and low-level agents and employs a novel frame transfer interface to connect them. In this section, we first introduce the overview of our hierarchical policy structure and the novel frame transfer interface. Then, we describe the high-level and low-level agents in detail.

The overview of our system is shown in \autoref{fig:c2fedp-overview}. We factor the policy learning problem into a two-step action prediction using a high-level agent $\pi_\text{high}$ and a low-level agent $\pi_\text{low}$, 
% To improve precision and translation invariance, 
% as well as adapt hierarchical policy learning to closed-loop control, 
% we design a new coarse-to-fine communication scheme between the high-level 
% and low-level modules:
\begin{equation}
    \pi(o) \;=\; \pi_{\text{low}}\bigl( o, t_{\text{high}}\bigr) \;
         ; t_{\high}=\pi_{\text{high}}\bigl(o\bigr) 
    \label{eq:coarse2fine}
\end{equation}
where $t_\high\in T(3)$ is a 3D translation predicted by $\pi_\high$.
% \begin{equation}
%     t_{\high}=\pi_{\text{high}}\bigl(o\bigr) 
%      ,
%     \label{eq:coarse2fine}
% \end{equation}

%, and $\circ$ denotes function composition that sends $t_\text{high}$ from the output of $\pi_\text{high}$ to the input of $\pi_\text{low}$.
% Here, the high-level agent $\pi_{\text{high}}$ generates a coarse estimation of 3D translation of the next keypose, denoted by \(t_{\text{high}} \in T(3)\). The low-level agent generate trajectories conditioned on high level prediction. 

% Instead of predicting an $\SE(3)$ pose at the high-level and using it as a constraint target for the low-level like prior works, we propose a new frame transfer interface between the two levels to improve the efficiency at the high level and flexibility at the low level. Specifically, we decompose the low level agent as: 
% \begin{equation}
% \label{eq:low_level}
% \pi_{\text{low}}\bigl(a \mid o,\,t_{\text{high}}\bigr)
% \;=\;
% f_{\text{traj}}\!\Bigl(
%   \pi_{\text{diffuser}}\bigl(f_{\text{obs}}\bigl(o,\,t_{\text{high}}\bigr)\bigr), 
%   \; t_{\text{high}}
% \Bigr),
% \end{equation}
\subsection{Frame Transfer Interface}
\label{frametsf}
The effectiveness of a hierarchical policy depends largely on the design of the high-level action output and its integration with the low-level agent. Prior approaches~\cite{xian2023chaineddiffuser,ma2024hierarchical} often constrain the high-level agent to predict an $\SE(3)$ pose, which is then treated as a rigid constraint for the low-level agent by enforcing it as the endpoint of the low-level trajectory. While this design simplifies task decomposition, it restricts flexibility and imposes computational burdens on the high-level agent, which must reason about precise pose constraints in high-dimensional spaces.

To overcome these limitations, we propose a flexible and efficient Frame Transfer interface (\autoref{fig:c2fedp-overview} middle) between the high- and low-level agents by only passing a $T(3)$ frame rather than constraining the pose. Specifically, our high-level agent predicts a 3D translation $t_\high$, which is used as a canonical reference frame for the low-level agent,
\begin{equation}
\label{eq:low_level}
\pi_{\text{low}}\bigl( o,\,t_{\text{high}}\bigr)
\;=\;
\tau\!\Bigl(
  \phi \bigl(\tau \bigl(o,\,t_{\text{high}}\bigr)\bigr), 
  \; -t_{\text{high}}
\Bigr),
\end{equation}
where $t_\high$ is the 3D translation (i.e., a keypose) predicted by the high-level agent, and $\phi$ is a trajectory generator that produces a trajectory based on the transformed observation. $\tau: (O\cup A)\times \bbR^3 \to O\cup A$ is a Frame Transfer function, which translates the $(x, y, z)$ component of the input observation or action to the input keypose frame. Specifically, we define the $+, -$ operators between $o$ or $a$ and $t_\high$ as addition and subtraction on the $(x, y, z)$ component of $o$ or $a$. For example, for $a=(x, y, z, q, c),  a+ t_\high = ((x, y, z)+t_\high, q, c)$. The Frame Transfer function $\tau$ is then defined as $\tau (o, t_\text{high}) = o - t_\text{high}, \tau(a, t_\high) = a - t_\high$.
% $\tau^{-1}$ is the inverse process that translates the input to the global coordinate frame, where $\tau^{-1} (a, t_\text{high}) = a + t_\text{high}$. 
% where $-, +$ here represents translating the $(x, y, z)$ component of the observation $o$ or action $a$. E.g., for $a=(x, y, z, q, c),  a+ t_\high = ((x, y, z)+t_\high, q, c)$.

% To improve precision and translation invariance, 
% as well as adapt hierarchical policy learning to closed-loop control, 
% we design a new coarse-to-fine communication scheme between the high-level 
% and low-level modules:
% \begin{equation}
%     \pi(a \mid o) \;=\; \pi_{\text{high}}\bigl(trans_{\text{high}} \mid o\bigr) 
%     \;\circ\; \pi_{\text{low}}\bigl(a \mid o, trans_{\text{high}}\bigr).
%     \label{eq:coarse2fine}
% \end{equation}

% The policy \(\pi_{\text{diffuser}}\) is an equivariant diffuser that produces a trajectory based on the given observation. The functions \(f_{\text{obs}}\) and \(f_{\text{traj}}\) implement frame transfer: the first translates the observation into the keypose frame via \(o_{\text{rel}} = o - t_{\text{high}}\), and the second shifts the resulting action back to the global coordinate system. 


% Given the observation in keypose frame $o_{\text{rel}}$, the low-level agent 
% and after diffuser predicts a trajectory in the keypose frame
% predicts a trajectory $\mathbf{a}_\text{rel}$ in the keypose frame (i.e., the frame centered at \(l_{\text{high}}\)). It is translated back to the world frame as the final output by the second one. Thus we call our approach Frame Transfer.
% represented as 
% \(\{ a_{t}^{\text{c2f}}, a_{t+1}^{\text{c2f}}, \dots, a_{t+(n-1)}^{\text{c2f}} \}\), 
% which defines the trajectory relative to a frame centered at \(l_{\text{high}}\). 
% Finally, the trajectory is translated back to the world frame to obtain the final action output, $\mathbf{a} = \mathbf{a}_\text{rel} + l_{\text{high}}$. This design forces the low-level agent to predict a trajectory based on 
% the high-level prediction. Essentially, it transfers the prediction frame in the low-level from the world frame to the keypose frame centered at the high-level action prediction, thus we call our approach Frame Transfer.
% coarse-to-fine trajectory is translated 
% back to the world frame to obtain the final trajectory.

The Frame Transfer interface offers several advantages. First, it provides an efficient mechanism that geometrically embeds the high-level action directly into the input of the low-level agent, ensuring seamless communication between the two levels. Second, by representing observations and trajectories in a relative frame, it introduces translation invariance to the low-level agent, simplifying its learning process and improving robustness. Third, unlike prior works~\cite{xian2023chaineddiffuser,ma2024hierarchical} which treat the high-level prediction as a rigid motion planning constraint (thus forcing the high-level agent to generate accurate $\SE(3)$ poses and limiting the policy in an open-loop manner), our approach interprets the high-level output as a flexible constraint. This flexibility reduces the computational burden on the high-level agent, as it only predicts a 3D translation, while preserving the system’s capability to operate in both open-loop and closed-loop control settings.

% Frame Transfer offers multiple advantages. First, it provides an efficient interface that geometrically embeds the high-level action in the input to the low-level agent. Second, using relative observations and trajectories provides the low-level agent with translation invariance. Third, unlike prior works~\cite{} that treat the high-level prediction as a rigid motion-planning constraint (thus requiring the high-level agent to generate accurate $\SE(3)$ poses and confining decision making to discrete keyposes in an open-loop manner), our approach treats the high-level output as a flexible constraint. Consequently, our high-level agent is more efficient as it only needs to predict translation, while the entire system remains flexible for either open-loop or close-loop control.
% Consequently, the low-level policy remains fully autonomous, enabling the entire system to operate in a closed-loop fashion.


\subsection{High-level Agent}\label{BB}
To efficiently predict the high-level action $t_\text{high}\in T(3)$, we represent it 
% Our high-level agent predicts a coarse estimate of the next keypose, following the definition of keypose in PerAct~\cite{}. An overview of the high-level pipeline is shown in Figure~\ref{fig:c2fedp-overview}. 
% The keypose estimate is represented
as a voxel map discretizing $\mcV_a: \bbR^3 \to \bbR$ where $\mcV_a(t)$ represents the probability of translation $t$ (see \autoref{voxel_map_as_f}) . 
%(here we use the function representation of a voxel map described in \autoref{voxel_map_as_f}, where the input to $\mcV_a$ is the voxel location and the output is the action probability)
This provides a dense spatial representation and naturally handles translation multi-modality~\cite{shridhar2023perceiver}. The center of the voxel with the highest predicted probability is then selected as the high-level agent's final output, $t_\high = \argmax \mcV_a$. Accordingly, the input observation is voxelized to $\mcV_o: \bbR^3\to \bbR^3$ (where the output of $\mcV_o$ is RGB), and we use an $\SO(2)$-equivariant 3D U-Net $\psi: \mcV_o \to \mcV_a$ to enforce $g\in T(3)\times \SO(2)$ symmetry, $\psi(g \mcV_o) = g \psi(\mcV_o)$. The entire high-level structure is shown in~\autoref{fig:c2fedp-overview} top. 
% produce the voxel heatmap representing action values. 


During training, the high-level agent's objective is to minimize the discrepancy between its predicted voxel heatmap $\mcV_a$ and the ground truth one-hot heatmap $\mcV_a^*$, derived from expert demonstrations, using the cross-entropy loss,
\begin{equation}
    \mathcal{L}_\high= - \sum_{x, y, z} \mcV_a^*(x, y, z)\log (\hat{\mcV}_a(x, y, z)),
\end{equation}
where $\hat{\mcV}_a(x, y, z)$ is the probability for voxel $(x, y, z)$ obtained by applying a softmax over the predicted heatmap.

% , and $H^*_{i, j, k}$ is the binary ground truth probability.
% \subsubsection{Stack Voxel Representation and Equivariant Encoder}
% \label{subsec:stacked_voxel}
% High-level agent is supposed to predict coarse estimation with high recall rate to benefit low-level refinement. Increasing the size of voxel make the coarse estimation span more space hence improve the recall rate, however, regular voxel would lose details of observation due to low resolution(larger voxel size). Stacked Voxel \citet{} in 3D vision area reserves fine detail within voxel by define it 
% as a point sets: 
% \begin{equation}
%  V = \left\{ p_i = [ r_i, g_i, b_i,x^{r}_i,y^{r}_i,z^{r}_i]^\top \in \mathbb{R}^9 \right\}_{i=1}^t 
% \end{equation} 
% as a stacked voxel containing \( t \leq T \) points,
% where $r_i, g_i, b_i$
% are the rgb values of $p_i$, and $x^{r}_i,y^{r}_i,z^{r}_i$ are its relative location towards the centroid of voxel.
% The difference of stacked voxel and regular voxel, and how stacked voxel is encoded is shown in \autoref{fig:stacked_voxel}. Unlike regular voxel which only takes one point to generate final voxel feature, the stacked voxel aggregate information of multiple points within voxel hence retain a fine-grain point interactions within a voxel and enables the final feature representation to learn descriptive shape
% information.
% \begin{figure}[H] 
%     \centering  \includegraphics[width=0.45\textwidth]{img/stacked_voxel.png}
%     \caption{Stacked Voxel Representation and Equivariant Encoder}
%     \label{fig:stacked_voxel}
% \end{figure}
% \subsubsection{Symmetry of High-level}
% Our high-level agent is designed to be $\mathrm{SO}(2)$-equivariant with respect to the 
% rotation $\mathbf{g}$. Specifically, we have 
% $\pi_{\text{high}}\bigl(\mathbf{g}\cdot l_{\text{high}} \mid \mathbf{g}\cdot o\bigr)
% = \pi_{\text{high}}\bigl(l_{\text{high}} \mid o\bigr).$
% We implement this property using escnn\cite{}, which allows us to define how 
% $\mathbf{g}$ acts on the stacked voxel as illustrated in \autoref{rt_action}

% \subsubsection{Translation Equivariance of High-level} 
% In the high-level agent, the stacked voxel represents the position of each point relative to the voxel center, ensuring invariance under global translations in T(3). Additionally, the U-Net encoder processes these voxels using convolutional operations, which are inherently translation-equivariant \cite{}. As a result, the entire high-level agent exhibits T(3)-equivariance. Detailed illustration is in appendix.

\subsection{Stacked Voxel Representation}
\label{subsec:stacked_voxel}

\begin{figure}[t] 
    \centering  \includegraphics[width=\linewidth]{img/stacked_voxel_dian_v2.png}
    \caption{\textbf{Equivariant Stacked Voxel Encoder.} Compared with the standard average pooling in point cloud voxelization (bottom), stacked voxel representation (top) can provide a richer representation of the points within the region of a voxel.}
    \label{fig:stacked_voxel}
\end{figure}

As our high-level agent uses 3D voxel grids as the visual input, the voxel encoder plays a crucial role in the policy. Standard 3D convolutional encoders impose a heavy computational burden, which often requires aggressive resolution compression that reduces the fine details in the observation. To address this limitation, we adopt Stacked Voxels~\cite{zhou2018voxelnet} from the 3D vision literature, which preserve fine-grained spatial cues by replacing voxel downsampling with a PointNet~\cite{qi2017pointnet} that aggregates information from all points within the spatial extent of each voxel.

Specifically, given a point cloud $P$, we first partition it into $H \times W \times D$ point sets, where each set $P_j\subseteq P$ corresponds to the points contained within a voxel $j$ in the $H \times W \times D$ voxel grid. Each point set $P_j$ is processed by an equivariant PointNet 
$l:P_j\mapsto \mcV (j_x, j_y, j_z)$ 
% $l: \bbR^{n\times(3+k)}\to \bbR^{c}$
to produce a $c$-dimensional aggregated feature vector for the voxel $j$. Repeating for all voxels results in a voxel grid feature map with dimensions $c \times H \times W \times D$. This feature map is then used as input to subsequent 3D convolutional networks.

This process, illustrated in \autoref{fig:stacked_voxel}, retains more nuanced shape information compared to simple voxel downsampling. 
% By encoding richer geometric details, Stacked Voxels enhance the learned representation’s ability to capture fine-grained spatial structure. 
Moreover, we prove that the stacked voxel representation maintains equivariance. (See proof in \autoref{app:stacked_voxel_proof}.)

\begin{proposition}
For $g=(t, \theta) \in T(3)\times \SO(2)$, if the PointNet $l$ is $\SO(2)$-equivariant and $T(3)$-invariant, i.e.,  $l(gP_j)=\rho(\theta)l(P_j)$, then
the stacked voxel representation $\nu:P\mapsto \mcV$ s.t. $\nu (P) (j_x, j_y, j_z) = l(P_j)$ is $\T(3)\times \SO(2)$-equivariant, i.e., $\nu(gP) = g\nu(P)$.
% a rotated point cloud $gP$ results in a rotated voxel representation $gV$, 
% when the PointNet $l$ is $\SO(2)$-equivariant and $T(3)$-invariant, i.e.,  $l(gP_j)=\rho(\theta)l(P_j)$.
\end{proposition}

In practice, we implement the $T(3)$-invariance in the PointNet by using the relative position to the center of each voxel, and implement the $\SO(2)$-equivariance using escnn~\cite{cesa2022a}.

% \begin{proof}
% \begin{align}
% \nu(P)(j_x, j_y, j_z) = l\left(m\left(j_x, j_y, j_z, P\right)\right)
% \end{align}
% Substituting $P=g^{-1}P$ and $(j_x, j_y, j_z) = g^{-1}(j_x, j_y, j_z)$,
% \begin{align}
% \nu(g^{-1}P)(g^{-1}(j_x, j_y, j_z)) &= l\left(m\left(g^{-1}(j_x, j_y, j_z), g^{-1}P\right)\right)\\
% \nu(g^{-1}P)(g^{-1}(j_x, j_y, j_z)) &= l\left(g^{-1}m\left(j_x, j_y, j_z, P\right)\right)\\
% \nu(g^{-1}P)(g^{-1}(j_x, j_y, j_z)) &= \rho^{-1}(\theta)l\left(m\left(j_x, j_y, j_z, P\right)\right)
% \end{align}
% Multiplying both sides with $\rho(\theta)$,
% \begin{align}
% \rho(\theta)\nu(g^{-1}P)(g^{-1}(j_x, j_y, j_z)) &= l\left(m\left(j_x, j_y, j_z, P\right)\right)\\
% \rho(\theta)\nu(g^{-1}P)(g^{-1}(j_x, j_y, j_z)) &= \nu(P)(j_x, j_y, j_z)
% \end{align}
% \end{proof}

\subsection{Low-level Agent}
After predicting the high-level action $t_\high$ and using Frame Transfer to canonicalize the observation, our low-level trajectory generator $\phi$ needs to create an $\SE(3)$ trajectory for the robot gripper. As shown in~\autoref{fig:c2fedp-overview} bottom, we first process the observation with a stacked voxel encoder, then leverage an equivariant diffusion policy~\cite{wang2024equivariant} to represent the policy $\phi$, which denoises the trajectory from a randomly sampled noisy trajectory. Specifically, we model a conditional noise prediction function $\varepsilon: o, a^k, k \mapsto e^k$, where the observation $o$ is the denoising conditioning, $a^k$ is a noisy action, $k$ is the denoising step, and $e^k$ is the predicted noise in $a^k$ s.t. the noise-free action $a=a^k - e^k$. The model $\varepsilon$ is implemented as an $\SO(2)$-equivariant function, $\varepsilon(go, ga^k, k) = g\varepsilon(o, a^k, k)$, to ensure the policy $\phi$ it represents is $\SO(2)$-equivariant, $\phi(go) = g\phi(o)$. See~\cite{wang2024equivariant} for more details.

During training, given an expert observation trajectory pair $(o, a)$, we first use the translation $t_n$ from the last step $a_n$ as the keypose, then apply frame transfer to get $o^*=\tau(o, t_n), a^*=\tau(a, t_n)$. The low-level loss is 
\begin{equation}
\mathcal{L}_\low = \left\|\varepsilon(o^*, a^* + e^k, k) - e^k\right\|^2,
\end{equation}
where $e^k$ is a random noise conditioned on a randomly sampled denoising step $k$.
% The overview of our Low-level is shown in \autoref{}. The output from high-level is first encoded through relative observation with a voxel level heatmap. Then the observation is sent to a $\SO(2)$ Equivariant denoising process following the design in EDP\cite{} to get a relative trajectory. Relative trajectory will then be transformed to world frame as the final output from low-level agent.
% The low-level diffuser (part of our low-level agent) generates a relative trajectory from the relative observation. We build on the Equivariant Diffusion Policy (EDP)~\cite{...}, but replace its original observation encoder with our stacked voxel encoder. EDP employs a conditional denoising process to generate actions from the observation, and by modeling an $\mathrm{SO}(2)$-equivariant gradient field, it ensures $\mathrm{SO}(2)$-equivariance.

\subsection{Symmetry of Policy} 

In this section, we describe the overall $T(3)\times \SO(2)$ symmetry of our hierarchical architecture. As is shown in \autoref{fig:equi}, a transformation 
in the observation should lead to the same transformation in both levels of HEP. Specifically, we decompose the symmetry into a rotation and translation, and prove each separately. 

Let $\pi$ be a hierarchical policy composed of a high-level agent $\pi_\high$, a low-level agent $\pi_\low$, 
and frame-transfer functions $\tau$ (see \autoref{HEP}).
% \subsubsection{Equivariance to Rotation} 
% Consider the group $g\in\SO(2)$.
\begin{proposition}[Hierarchical \(\mathrm{SO}(2)\) Equivariance]
\label{thm:hierarchical_equivariance}
$\pi$ is $\SO(2)$-equivariant when the following assumptions hold for $g\in\SO(2)$:
% Assume that for \emph{every} planar rotation ${g} \in \mathrm{SO}(2)$:
\begin{enumerate}
\item The high-level policy $\pi_\high$ is $\mathrm{SO}(2)$-equivariant, $\pi_\high(go)=g  \pi_\high(o)$
\item The low-level policy $\pi_{\text{low}}$ is $\mathrm{SO}(2)$-equivariant, $\pi_\low (go, gt_\high) = g \cdot \pi_{low}(o, t_\high)$
% \item The frame-transfer functions $\tau$ commute with ${g}$.
\item The Frame Transfer function $\tau$ is $\mathrm{SO}(2)$-equivariant.
\end{enumerate}
\end{proposition}
In \autoref{Section: Proof I} we show that the \emph{entire} hierarchical policy $\pi$ is $\mathrm{SO}(2)$-equivariant so that rotating the observation $o$ results in an action rotated in the same way.
% \subsubsection{Equivariance to Translation}
\begin{proposition}[Hierarchical \(\mathrm{T}(3)\) Equivariance]
\label{prop:t3_equivariance}
$\pi$ is $T(3)$-equivariant when the following assumptions hold for $t\in T(3)$
% Let \(\pi\) be a hierarchical policy composed of:
% \begin{itemize}
%     \item a high-level agent \(\pi_{\text{high}}\),
%     \item a low-level agent \(\pi_{\text{low}}\), and
%     \item frame-transfer functions \(f_{\text{obs}}\) and \(f_{\text{traj}}\).
% \end{itemize}
% Assume that for \emph{every} 3D translation \(\mathbf{t} \in \mathrm{T}(3)\):
\begin{enumerate}
    \item \(\pi_{\high}\) is \(\mathrm{T}(3)\)-equivariant, $\pi_\high(o+t)=t+\pi_\high(o)$
    \item The Frame Transfer function \(\tau\) is \(\mathrm{T}(3)\)-invariant, and satisfies $\tau(o,t_{\high})=\tau(o+t,t_{\high}+t)$
    % and \(\tau\) commutes with \({t}\),$\tau(a+t_1,-(t_{high}+t_2))=\tau(a,-t_{high})+t_1+t_2$
\end{enumerate}
\end{proposition}

Notably, even if the low-level policy \(\pi_{\text{low}}\) is not \(\mathrm{T}(3)\)-equivariant, the \emph{entire} hierarchical policy \(\pi\) is \(\mathrm{T}(3)\)-equivariant. This is proven in \autoref{Section: Proof II}.

\begin{figure}[t]
    \centering
    \includegraphics[width=\linewidth]{img/equivariance.png}
    \caption{\textbf{Equivariance in HEP.} When the observation is rotated and translated, the high- and low- level actions are rotated and translated accordingly.}
    \label{fig:equi}
\end{figure}

\section{Simulation Experiment}
\begin{table*}[ht]
\centering
\caption{\textbf{Performance of Different Models Across Various Tasks in Simulation.} Success rates (in percentages) are reported for each task. Bolded values indicate the best performance for each task, and improvements are shown in blue where applicable.}
\label{tab:combined_task_performance_subtables}
\vskip 0.15in
\renewcommand{\arraystretch}{0.9} % Slightly reduce row spacing for compactness
\scriptsize % Change font size to scriptsize for smaller text
\setlength{\tabcolsep}{2pt} % Reduce column spacing to 2pt
% Define column types
\newcolumntype{L}{>{\raggedright\arraybackslash}p{2.5cm}} % Adjusted width for better fit
\newcolumntype{C}{>{\centering\arraybackslash}p{1.1cm}} % Adjusted width for better fit

% ----------------------------
% Subtable 1: Open Loop Setting - Task Group 1
% ----------------------------
% ----------------------------
% Subtable 1: Open Loop Setting - Task Group 1
% ----------------------------
\begin{subtable}{\textwidth}
    \centering
    \begin{tabular}{@{}LCCCCCCCCCCC@{}}
    \toprule
    {Method \textbf{(Open-loop)}}  & \textbf{Mean} & 
    {Pick/Lift} & 
    {Push Button} & 
    {Knife on Board} & 
    {Put Money} & 
    {Reach Target} & 
    {Slide Block} & 
    {Stack Wine} & 
    {Take Money} & 
    {Take Umbrella} & 
    {Pick up Cup}\\ 
    \midrule
    {Ours} & 
    \textbf{88}\textcolor{blue}{(+10)} &  % Example mean value, replace with actual calculation
    \textbf{99}\textcolor{blue}{(+1)} & 
    \textbf{100}\textcolor{blue}{(+1)} & 
    \textbf{96}\textcolor{blue}{(+5)} & 
    98\textcolor{red}{(-1)} & 
    \textbf{100} & 
    \textbf{100}\textcolor{blue}{(+2)} & 
    \textbf{100}\textcolor{blue}{(+7)} & 
    90\textcolor{red}{(-10)} & 
    \textbf{100}\textcolor{blue}{(+1)} & 
    \textbf{98}\textcolor{blue}{(+4)} \\ 
    {Chained Diffuser} & 
    78 & % Example mean value, replace with actual calculation
    98 & 
    96 & 
    91 & 
    \textbf{99} & 
    \textbf{100} & 
    98 & 
    93 & 
    \textbf{100} & 
    96 & 
    94 \\ 
    {3D Diffuser Actor} & 
    56 & % Example mean value, replace with actual calculation
    98 & 
    99 & 
    84 & 
    88 & 
    \textbf{100} & 
    98 & 
    90 & 
    89 & 
    99 & 
    94 \\ 
    % \midrule
    % \bottomrule
        \toprule
     {Method \textbf{(Open-loop)}}&{}& 
    {Unplug Charger} & 
    {Close Door} & 
    {Open Box} & 
    {Open Fridge} & 
    {Frame off Hanger} & 
    {Open Oven} & 
    {Books on Shelf} & 
    {Wipe Desk} & 
    {Cup in Cabinet} & 
    {Shoe out of Box} \\ % Added "Microwave" and removed extra &
    \midrule
    {Ours (HEP)} & {}&
    \textbf{99}\textcolor{blue}{(+4)} & 
    \textbf{90}\textcolor{blue}{(+24)} & 
    \textbf{100}\textcolor{blue}{(+4)} & 
    \textbf{83}\textcolor{blue}{(+15)} & 
    \textbf{93}\textcolor{blue}{(+8)} & 
    \textbf{87}\textcolor{blue}{(+1)} & 
    \textbf{99}\textcolor{blue}{(+7)} & 
    \textbf{77}\textcolor{blue}{(+12)} & 
    \textbf{76}\textcolor{blue}{(+8)} & 
    \textbf{90}\textcolor{blue}{(+12)}  \\ % Added data for "Microwave"
    {Chained Diffuser} & {}&
    95 & 
    76 & 
    96 & 
    68 & 
    85 & 
    86 & 
    92 & 
    65 & 
    68 & 
    78  \\ % Added data for "Microwave"
    {3D Diffuser Actor} & {}&
    49 & 
    7 & 
    15 & 
    41 & 
    71 & 
    3 & 
    36 & 
    5 & 
    1 & 
    21\\ % Added data for "Microwave"
    % \midrule
    % \bottomrule
        \toprule
   Method \textbf{(Open-loop)} & 

    {} &{Open Microwave}&{Turn on Lamp} & 
    {Open Grill} & 
    {Stack Blocks} & 
    {Stack Cups} & 
    {Push 3 Buttons} & 
    {USB in Computer} & 
    {Open Drawer} & 
    {Put Item in Drawer} & 
    {Sort Shape}\\ 
    \midrule
    {Ours} &  
    {}&\textbf{82}\textcolor{blue}{(+26)}&
    \textbf{95}\textcolor{blue}{(+55)} & 
    \textbf{99}\textcolor{blue}{(+4)} & 
    \textbf{54}\textcolor{blue}{(+4)} & 
    \textbf{32}\textcolor{blue}{(+4)} & 
    \textbf{99}\textcolor{blue}{(+12)} & 
    \textbf{90}\textcolor{blue}{(+16)} & 
    \textbf{94}\textcolor{blue}{(+10)} & 
    \textbf{95}\textcolor{blue}{(+7)} & 
    \textbf{22}\textcolor{blue}{(+3)} \\ 
    {Chained Diffuser} &  
    {}&56&
    40 & 
    95 & 
    10 & 
    12 & 
    86 & 
    74 & 
    84 & 
    88 & 
    10 \\ 
    {3D Diffuser Actor} &  
    {}&46&20 & 
    70 & 
    50 & 
    28 & 
    87 & 
    42 & 
    71 & 
    70 & 
    19 \\ 
    \bottomrule
    \end{tabular}
    \label{tab:subtable1}
\end{subtable}
% \vspace{0pt}
% ----------------------------
% Subtable 2: Open Loop Setting - Task Group 2
% ----------------------------
% \begin{subtable}{\textwidth}
%     \centering
%     \begin{tabular}{@{}LCCCCCCCCCCC@{}} % Updated to 12 columns: 1 Left + 11 Centered
%     \toprule
%      \textbf{Method (Open-loop)}&{}& 
%     {Unplug Charger} & 
%     {Close Door} & 
%     {Open Box} & 
%     {Open Fridge} & 
%     {Frame off Hanger} & 
%     {Open Oven} & 
%     {Books on Shelf} & 
%     {Wipe Desk} & 
%     {Cup in Cabinet} & 
%     {Shoe out of Box} \\ % Added "Microwave" and removed extra &
%     \midrule
%     {Ours (HEP)} & {}&
%     \textbf{99}\textcolor{blue}{(+4)} & 
%     \textbf{90}\textcolor{blue}{(+24)} & 
%     \textbf{100}\textcolor{blue}{(+4)} & 
%     \textbf{83}\textcolor{blue}{(+15)} & 
%     \textbf{93}\textcolor{blue}{(+8)} & 
%     \textbf{87}\textcolor{blue}{(+1)} & 
%     \textbf{99}\textcolor{blue}{(+7)} & 
%     \textbf{77}\textcolor{blue}{(+12)} & 
%     \textbf{76}\textcolor{blue}{(+8)} & 
%     \textbf{90}\textcolor{blue}{(+12)}  \\ % Added data for "Microwave"
%     {Chained Diffuser} & {}&
%     95 & 
%     76 & 
%     96 & 
%     68 & 
%     85 & 
%     86 & 
%     92 & 
%     65 & 
%     68 & 
%     78  \\ % Added data for "Microwave"
%     {3D Diffuser Actor} & {}&
%     49 & 
%     7 & 
%     15 & 
%     41 & 
%     71 & 
%     3 & 
%     36 & 
%     5 & 
%     1 & 
%     21\\ % Added data for "Microwave"
%     \bottomrule
%     \end{tabular}
%     \label{tab:subtable2}
% \end{subtable}
% \vspace{0pt}
% Reduce spacing between first three subtables

% ----------------------------
% Subtable 3: Open Loop Setting - Task Group 3
% ----------------------------
% \begin{subtable}{\textwidth}
%     \centering
%     % \hspace{0.41cm}
%     \begin{tabular}{@{}LCCCCCCCCCCC@{}}
%     \toprule
%    Method \textbf{(Open-loop)} & 

%     {} &{Open Microwave}&{Turn on Lamp} & 
%     {Open Grill} & 
%     {Stack Blocks} & 
%     {Stack Cups} & 
%     {Push 3 Buttons} & 
%     {USB in Computer} & 
%     {Open Drawer} & 
%     {Put Item in Drawer} & 
%     {Sort Shape}\\ 
%     \midrule
%     {Ours} &  
%     {}&\textbf{82}\textcolor{blue}{(+28)}&
%     \textbf{95}\textcolor{blue}{(+55)} & 
%     \textbf{99}\textcolor{blue}{(+4)} & 
%     \textbf{54}\textcolor{blue}{(+4)} & 
%     \textbf{32}\textcolor{blue}{(+4)} & 
%     \textbf{99}\textcolor{blue}{(+12)} & 
%     \textbf{90}\textcolor{blue}{(+16)} & 
%     \textbf{94}\textcolor{blue}{(+10)} & 
%     \textbf{95}\textcolor{blue}{(+7)} & 
%     \textbf{22}\textcolor{blue}{(+3)} \\ 
%     {Chained Diffuser} &  
%     {}&56&
%     40 & 
%     95 & 
%     10 & 
%     12 & 
%     86 & 
%     74 & 
%     84 & 
%     88 & 
%     10 \\ 
%     {3D Diffuser Actor} &  
%     {}&46&20 & 
%     70 & 
%     50 & 
%     28 & 
%     87 & 
%     42 & 
%     71 & 
%     70 & 
%     19 \\ 
%     \bottomrule
%     \end{tabular}
%     \label{tab:subtable3}
% \end{subtable}
\vspace{8pt} % Increase spacing before Subtable 4

% ----------------------------
% Subtable 4: Close Loop Setting
% ----------------------------
\begin{subtable}{\textwidth}
    \centering
    \begin{tabular}{@{}LCCCCCCCCCCC@{}}
    \toprule
    Method \textbf{(Closed-loop)} & 
    \textbf{Mean} & 
    {Turn On Lamp} & 
    {Open Microwave} & 
    {Push 3 Buttons} & 
    {Open Drawer} & 
    {Put Item in Drawer} & 
    {Slide Block} & 
    {Stack Wine} & 
    {Take Money} & 
    {Take Umbrella} & 
    {Pick up Cup} \\ 
    \midrule
    {Ours} & 
    \textbf{79}\textcolor{blue}{(+23)} & 
    \textbf{60}\textcolor{blue}{(+32)} & 
    \textbf{64}\textcolor{blue}{(+22)} & 
    \textbf{37}\textcolor{blue}{(+36)} & 
    \textbf{95}\textcolor{blue}{(+41)} & 
    \textbf{76}\textcolor{blue}{(+28)} & 
    \textbf{95}\textcolor{blue}{(+20)} & 
    \textbf{89}\textcolor{blue}{(+10)} & 
    \textbf{94}\textcolor{blue}{(+14)} & 
    \textbf{90}\textcolor{blue}{(+9)} & 
    \textbf{93}\textcolor{blue}{(+15)} \\ 
    {EquiDiff} & 
    57 & 
    28 & 
    42 & 
    1 & 
    54 & 
    48 & 
    75 & 
    79 & 
    80 & 
    81 & 
    78 \\ 
    \bottomrule
    \end{tabular}
    \label{tab:subtable4}
\end{subtable}
\end{table*}

\subsection{Experimental Settings}
% \begin{figure*}[ht]
% \centering
% \includegraphics[width=\linewidth]{img/sim_in_body.png}
% \caption{\textbf{The Simulation Tasks from RLBench~\cite{}.} See Appendix~\ref{} for all environments.}
% \label{fig:sim_body} % Add a label for referencing
% \end{figure*}

To evaluate our policy, we first perform experiments in simulated environments in the RLBench~\cite{9001253} benchmark implemented using CoppeliaSim~\cite{6696520} and PyRep~\cite{james2019pyrep}. The simulated environments contain a 7-joint Franka Panda robot equipped with a parallel gripper, as well as four RGB-D cameras 
% positioned at the front, left shoulder, right shoulder, and wrist of the robot 
to provide the point cloud observation.
% All experiments are conducted using a 7-joint Franka Panda robot equipped with a parallel gripper. Point clouds are aggregated from four RGB-D cameras positioned at the front, left shoulder, right shoulder, and wrist or the robot. 

% The simulation environment is implemented in CoppeliaSim~\cite{} and interfaced through PyRep~\cite{}. All experiments are conducted using a 7-joint Franka Panda robot equipped with a parallel gripper. Observations are captured from four RGB-D cameras positioned at the front, left shoulder, right shoulder, and wrist. Each camera operates noiselessly with a resolution of 256 × 256.

We evaluate our model on 30 RLBench tasks, among which 20 are widely used in the prior works like~\cite{xian2023chaineddiffuser}. The remaining 10 are challenging tasks that demand precise control, such as \texttt{Lamp On}, or long-horizon planning, like \texttt{Push 3 Buttons}. A subset of the 30 simulation tasks is shown in \autoref{fig:sim_body}. Each task is trained using 100 demonstrations, more detailed task descriptions and visualizations are provided in \autoref{detail_of_sim}.
% We evaluate the models on 30 RLBench tasks, which are divided into two categories. The first category, \textbf{20 Tasks Used by Former Hierarchical Policies}, includes twenty tasks widely used in prior research, enabling direct comparisons with state-of-the-art hierarchical agents such as Chained Diffuser. These tasks range from general benchmarks to those requiring sustained interaction with the environment, such as \texttt{Wipe Desk}, where the robot follows continuous trajectories. The second category, \textbf{Hard RLBench Tasks}, comprises ten challenging tasks that demand precise control, such as \texttt{Lamp On}, or long-horizon planning, like \texttt{Push 3 Buttons}. A subset of the simulation tasks is shown in \autoref{fig:sim_body}. Detailed task descriptions and visualizations are provided in the appendix.

We consider two different control settings, open-loop and closed-loop control. In closed-loop, we use each control step in the dataset as the low-level's target, and next keyframe is used as the label for the high-level agent. In open-loop, we use the keyframe (i.e., some key actions in the entire trajectory like pick, place, etc.) defined by the prior work~\cite{shridhar2023perceiver} as the target for the high-level agent, then construct the low-level target by interpolating between the consecutive keyframes. In principle, the open-loop setting requires fewer prediction steps to finish a task, while the closed-loop setting makes the policy more responsive. Thanks to the flexibility of our Frame Transfer interface, our policy can operate in both settings, while some prior works are limited in the open-loop setting.


% We consider two different control settings, open-loop and closed-loop control. In both settings, the dataset is based on that provided in RLBench with a fixed control rate. In closed-loop control, the low-level's control frequency matches the dataset, and we use the every $m$th gripper position as the label for the high-level agent. In open-loop control, we use the keyframe (i.e., some key actions in the entire trajectory like pick, place, etc.) defined by the prior work~\cite{peract} as the target for the high-level agent, then construct the low-level target by interpolating between the consecutive keyposes. In principle, the policy in an open-loop setting requires fewer prediction steps to finish a task, while the closed-loop setting makes the policy more responsive. Thanks to the flexibility of our Frame Transfer interface, our policy can be trained in both settings, while some prior works are limited in the open-loop setting. 
% The open-loop dataset is constructed by downsampling steps between keyposes, following prior work \cite{xian2023chaineddiffuser}. 
% We evaluate our method in the open-loop setting across all 30 tasks, whereas the closed-loop setting is evaluated in 10 selected tasks that represent the full diversity and complexity of the complete task set.

% Training a closed-loop agent requires significantly more computational resources, as it involves training at every step, whereas open-loop training is restricted to keyframes. Due to our resource constraints, closed-loop evaluations are performed on a subset of 10 tasks selected from the 30 open-loop tasks. These tasks are carefully chosen from both categories to ensure they represent the full diversity and complexity of the complete task set.

\begin{figure}[t]
\centering
\begin{subfigure}[t]{0.32\linewidth}
\centering
\includegraphics[width=\linewidth]{img/1.png}
\caption{Open Microwave}
\end{subfigure}
\begin{subfigure}[t]{0.32\linewidth}
\centering
\includegraphics[width=\linewidth]{img/4.png}
\caption{Stack Wine}
\end{subfigure}
\begin{subfigure}[t]{0.32\linewidth}
\centering
\includegraphics[width=\linewidth]{img/3.png}
\caption{Shoes Out of Box}
\end{subfigure}
\caption{\textbf{The Simulation Tasks from RLBench~\cite{9001253}.} See \autoref{detail_of_sim} for all environments.}
\label{fig:sim_body}
\end{figure}

\begin{figure*}[t]
    \centering
    \begin{subfigure}{\textwidth}
        \centering
        \includegraphics[width=\textwidth]{img/new_real1.png}
        \caption{Pot Cleaning}
        \label{fig:pot_cleaning}
    \end{subfigure}

    \begin{subfigure}{\textwidth}
        \centering
        \begin{minipage}{0.59\textwidth}
            \centering
            \includegraphics[width=\textwidth]{img/block1.png}
            \caption{Blocks to Drawer}
            \label{fig:blocks_to_drawer}
        \end{minipage}
        \hfill
        \begin{minipage}{0.4\textwidth}
            \centering
            \includegraphics[width=\textwidth]{img/block2.png}
            \caption{Blocks Stacking}
            \label{fig:blocks_stacking}
        \end{minipage}
    \end{subfigure}

    \caption{\textbf{Real-world Experiment Setting.} \autoref{fig:pot_cleaning}: Pot cleaning, the robot needs to open the pot lid, pour detergent into the pot, and clean it with a sponge. \autoref{fig:blocks_to_drawer}: Blocks to drawer, the robot needs to open the drawer, place two blocks inside, and close the drawer. \autoref{fig:blocks_stacking}: Blocks stacking, the robot needs to stack three blocks one by one.}
    \label{fig:task_illustrations}
\end{figure*}

\subsection{Baseline}
We compare our method against the following baselines. 
\textbf{3D Diffuser Actor}: an open-loop agent that combines diffusion policies~\cite{chi2023diffusionpolicy} with 3D scene representations. 
\textbf{Chained Diffuser}: an open-loop hierarchical agent that uses Act3D~\cite{gervet2023act3d} in the high-level and diffusion policy in the low-level. 
% \textbf{Hierarchical Diffusion Policy}: an open-loop hierarchical agent that employs PerAct~\cite{shridhar2023perceiver} in the high level and diffusion policies in the low level to predict actions in both end-effector space and joint space. 
\textbf{Equivariant Diffusion Policy (EquiDiff)}: an $\SO(2)$-equivariant, closed-loop policy that applies equivariant denoising. 
% We evaluate the baselines in the same settings as their original experiments, i.e., open-loop for 3D Diffuser Actor, Chained Diffuser, and Hierarchical Diffusion Policy; closed-loop for EquiDiff. 

% See \autoref{detail_of_baseline} for more details of the baselines.

% For open-loop control, we evaluate our policy against 3D Diffuser Actor, Chained Diffuser, and Hierarchical Diffusion Policy. However, since other hierarchical agents are not designed to operate in a closed-loop setting, we compare our policy only with EDP, the prior state-of-the-art closed-loop agent.
% See Appendix for more details.
\subsection{Results}

\autoref{tab:combined_task_performance_subtables} presents the comparison in terms of the evaluation success rates of the last checkpoint across 100 trials. 
% Generally, our model outperforms all the baselines in the majority of the tasks in both open-loop and closed-loop settings. 
% demonstrating its better sampling efficiency and robustness.
% The results highlight the performance of our model under both open-loop and closed-loop settings, demonstrating its better sampling efficiency and robustness across a wide range of tasks.

\textbf{Open-loop Results: }Our model outperforms the baselines in 28 out of the 30 tasks, achieving an average absolute improvement of \textbf{10\%}. 
% This gain reflects the efficiency of our approach in open-loop. 
The task where HEP falls short of achieving the best results is \texttt{Take Money}. Further investigation reveals that HEP achieves 98\% success rate at earlier checkpoints but fails at the final checkpoint, likely due to overfitting. Tasks involving precise actions or long-horizon trajectories e.g., \texttt{Lamp-on} and \texttt{Push 3 Buttons} also exhibited consistently high success rates, demonstrating the adaptability of our method to diverse task requirements. We also compare our model with hierarchical diffusion policy~\cite{ma2024hierarchical} in \autoref{comp_with_hdp}

\textbf{Closed-loop Results: } Here we consider 10 selected tasks that represent the full diversity and complexity of the complete task set. The closed-loop setting requires longer-horizon trajectories, making it harder to succeed in evaluation. Despite this, our model consistently outperforms EquiDiff across all 10 tasks, achieving an average absolute improvement of \textbf{23\%}. This improvement underscores the effectiveness of HEP in handling the increased complexity of long-horizon decision-making. 
% Tasks such as \texttt{Push 3 Buttons}, which require pressing three buttons in a specific sequence, pose a significant challenge for closed-loop agents. These tasks rely heavily on keyframe-level history, as visual inputs alone are insufficient to indicate task progress. Despite this, our model achieved a 36\% improvement over EquiDiff’s 1\% success rate, demonstrating its ability to integrate historical context and visual feedback effectively, even without keyframe-level history. 
% The robustness of our hierarchical framework is particularly evident in long-horizon tasks, where it maintains high accuracy.

% These results validate the advantages of our approach in both open-loop and closed-loop settings, highlighting its superior sample efficiency, generalization capability, and robustness in complex task environments.

\subsection{Ablation Study}
\label{ab}

To validate the impact of our contributions,we perform
\begin{wraptable}{r}{0.3\linewidth}
% \vspace{-0.7cm}
\vspace{-0.1cm}
\caption{\textbf{Ablation Study Results.}}
\label{tab:ablation-results}
\vskip 0.15in
\centering
\setlength{\tabcolsep}{2pt}
\scriptsize
\begin{tabular}{@{}lc@{}}
\toprule
Method & Mean \\
\midrule
No Equi No FT & 0.60 \\
No Equi & 0.70 \\
No FT & 0.78 \\
No Stacked Voxel & 0.84 \\
\textbf{Complete Model} & \textbf{0.94} \\
\bottomrule
\end{tabular}
\end{wraptable}
  an ablation study in six tasks considering the following configurations: \textbf{No Equi}: same architecture but removes all equivariant structure. \textbf{No Stacked Voxel}: removes the stacked voxel encoder. \textbf{No FT}: removes the Frame Transfer interface and uses the high-level action as an additional conditioning in the low-level. \textbf{No Equi No FT}: combination of No Equi and No FT.

As is shown in \autoref{tab:ablation-results}, removing equivariance makes the most significant negative impact on our model, reducing the mean success rate by 24\%. The performance drop when removing Frame Transfer and stacked voxel encoder is 16\% and 10\%, respectively, demonstrating the importance of all three key pieces of our model. Moreover, the 10\% performance difference between No Equi and No Equi No FT shows the potential of Frame Transfer beyond our model. See \autoref{tab:ablation-results-all} in the Appendix for the full table.

% We ablate the impact of our design choices in Table~\ref{tab:ablation-results}. 
% All ablations are conducted on six tasks. 
% We specifically examine the following configurations: 
% \begin{itemize}
%     \item \emph{No Equi}: Use the same architecture but remove all equivariant structures,
%     \item \emph{No High Level}: Only use the low-level agent,
%     \item \emph{No Stacked Voxel}: Replace stacked voxels with standard voxels,
%     \item \emph{No Frame Translation}: Remove the frame transfer communication between high-level and low-level.
% \end{itemize}

% \begin{table}[t]
% \centering
% \setlength{\tabcolsep}{0.3pt}
% \scriptsize
% % \resizebox{\textwidth}{!}{
% \newcolumntype{C}{>{\centering\arraybackslash}p{0.9cm}} % Adjusted width for better fit
% \begin{tabular}{@{}lCCCCCCC@{}}
% \toprule
% \textbf{Method} & \textbf{Lamp on} & \textbf{Open microw.} & \textbf{Push 3 buttons} & \textbf{Push button} & \textbf{Open box} & \textbf{Insert USB} & \textbf{Mean} \\
% \midrule
% No Equi No FT & 0.21 & 0.44 & 0.53 & 0.96 & 0.99 & 0.51 & 0.60\\
% No Equi               & 0.41 & 0.53 & 0.67 & 0.98 & 0.99 & 0.64 & 0.70 \\
% No Stacked Voxel      & 0.77 & 0.65 & 0.87 & 0.99 & 0.99 & 0.79 & 0.84 \\
% No FT & 0.75 & 0.56 & 0.73 & 0.98 & 0.99 & 0.68 & 0.78 \\
% \textbf{Complete Model}        & \textbf{0.95} & \textbf{0.82} & \textbf{0.99} & \textbf{1.00} & \textbf{1.00} & \textbf{0.90} & \textbf{0.94} \\
% \bottomrule
% \end{tabular}
% % }
% \caption{Performance of different ablations on various tasks.}
% \label{tab:ablation-results}
% \end{table}

% \subsection{Open Loop Setting Experiment}

% We trained models on following 30 tasks with 100 demonstrations in single task setting. We show To generate the open loop dataset, the steps between 2 keyposes are downsampled to a fixed number following former works\citet[] The tasks contain three types: 
% \begin{itemize}
%     \item \textbf{Hiveformer Tasks:} The Auto-$\lambda$ \citet{} experimental setup includes 10 tasks commonly used in prior work, including the former state-of-the-art hierarchical agent, Chained Diffuser, enabling direct performance comparison. We also evaluate performance across 10 episodes. Since the baseline models achieve an average performance exceeding 90\% on these tasks with 100 episodes, the 10-episode evaluation provides additional insights into the sampling efficiency of all models.
% \item \textbf{10 RLBench Tasks Requiring Continuous Interaction}: These tasks, such as \texttt{wipe\_desk}, involve continuous interaction with the environment. For instance, \texttt{wipe\_desk} requires the robot to follow a wiping trajectory to remove dirt, demonstrating the advantages of a hierarchical policy. These tasks are also considered by the Chained Diffuser.

% \item \textbf{10 Hard RLBench Tasks}: We identify 10 challenging tasks from several behavior cloning works\citet{}, where models would underperform due to the inherent difficulty of the task settings. These tasks either require precise positioning, such as \texttt{lamp\_on}, where the robot must touch a very small switch, or involve long-horizon planning, such as \texttt{push\_3\_buttons}, which requires the robot to sequentially push three buttons.
% \end{itemize}
% Detail description of tasks is left in appendix.
% \subsubsection{Baseline}
% We compare our method against the following baselines:

% \begin{enumerate}
%     \item \textbf{3D Diffuser Actor} \cite{}: A state-of-the-art (SOTA) open-loop agent. We reproduced this method using the officially released code.
%     \item \textbf{Chained Diffuser} \cite{}: A previous SOTA hierarchical agent. We also reproduced this method using the official implementation.
%     \item \textbf{Equivariant Diffusion Policy} \cite{}: A SOTA closed-loop agent.
% \end{enumerate}
% \subsubsection{Result}
% Whenever possible, we reference the official performance numbers reported in the respective baseline papers. \autoref{tab:combined_task_performance_subtables} presents the experimental results, showing success rates across 100 evaluations for the final checkpoint. Our model outperforms the baselines in 28 out of 30 tasks, achieving a 10.3\% absolute improvement, which highlights its better sampling efficiency. The task where HEP does not
% achieve close to the best results is Take
% Money. Upon further investigation, we find that on Take
% Money, HEP achieves higher success rates like 98\% on earlier checkpoints than the final one. The lower performance on the final checkpoint could be an artifact of over-fitting

% We also conducted an ablation study to evaluate the contributions of stacked voxels, frame transfer, and equivariance to overall performance. The results indicate that each of these components contributes to the observed performance improvement. Also, our ablation reveals that the frame transfer interface improves performance even in the absence of equivariance. This underscores the frame transfer interface effectiveness as a communication mechanism between the high-level and low-level agents.

% \subsection{Close Loop Setting Experiment}
% Thanks to our flexible frame transfer, our model is able to control in close loop setting. The setting of close loop dataset is same as open loop setting other than it is generated without downsampling the steps  therefore has a way longer horizon than open loop setting. Making decision on action level (aka closed loop) make agent more reactive to interfere in environment. But it also makes the task more challenging due to longer horizon. It also requires more training resource since it is trained on each steps unlike on just keypose for open loop agent. Therefore we choose 10 tasks from 30 tasks in open loop setting due to our limited computational resource.
% \subsubsection{Baseline}
% Since former hierarchical policy can only performs control on the keypose level\citet{}, we compare our method against the SOTA close agent Equivariant Diffusion Policy. We reproduced this method using the official implementation.
% \subsubsection{Result}
% \autoref{tab:combined_task_performance_subtables} presents the experimental results in terms of success rates across 100 evaluations for the final checkpoint. Our model outperforms the baselines in all 10 tasks, achieving a 22.7\% absolute improvement, demonstrating better sampling efficiency.

% The "Push 3 Buttons" task, which requires the robot to press three buttons in a specific sequence, heavily relies on keypose-level history, as visual inputs alone provide insufficient information to indicate which buttons have already been pressed. This task is particularly challenging for closed-loop agents that make decisions at the action level, as their historical context is limited to action-level data. Remarkably, our model has a 36\% improvement over the 1\% baseline success rate, demonstrating that a hierarchical closed-loop agent has significantly superior sample efficiency on difficult long horizon tasks.


\section{Real-World Experiment} 
In this section, we evaluate our method on a real robot system comprised of a UR5 robot and 3 Intel Realsense~\cite{keselman2017intel} D455 RGBD sensors. Details on the experiment setting are given in \autoref{sec:realworld-exp}.

\paragraph{Baseline Comparison} 

We experiment in three tasks as shown in \autoref{fig:task_illustrations}. These tasks are challenging due to their extreme long horizon (can be divided into 6 to 9 sub-tasks) and the diverse types of manipulation involved. 
% for example, handling non-rigid objects (using sponge), performing periodic actions (dispensing detergent and scrubbing), and working with 6Dof action spaces (tilting detergent bottle). 
% We train each task with 30 demonstrations, and 
Evaluations are conducted in 20 trials: 10 with object placements similar to the training dataset's and 10 with unseen placements. 

As shown in \autoref{tab:performance_open_loop}, our model successfully completes the tasks under open-loop control. Most failures occur due to the slight misalignment between the gripper and the object, likely caused by poor depth quality of the sensors. 
% such as significant variations in the appearance of the same object in the point cloud when viewed from different locations within the workspace. 
We further evaluate our model in a closed-loop setting, where it achieves similar performance to the open-loop version in two of the three tasks. However, in Pot Cleaning, while the agent progresses further in the task, it becomes stuck in a recurrent cleaning loop. This likely results from the lack of history information in the observations, preventing the agent from recognizing when to exit the cleaning phase. In contrast, the open-loop version follows a single keypose for cleaning, facilitating a smoother transition to the next stage.

% Two tasks are successfully completed under the close-loop control with same numbers of demonstrations. The most common failure mode is due to lack of history information on high level, which sometimes leads policy to do the same subtask again and again. We leave the detailed failure analysis in appendix. This could be solved by simply add longer history to high level. We leave this as our future work.


\begin{table}[t]
\centering
\caption{\textbf{Performance of Different Models in the Real-World.}}
\label{tab:performance_open_loop}
\vskip 0.15in
\renewcommand{\arraystretch}{1.1} % 调整行高
\setlength{\tabcolsep}{4pt}       % 设置列间距
\scriptsize % 调整文字大小为稍大尺寸
\begin{tabular}{lccc}
\toprule
Task & Pot Cleaning & Blocks to Drawer & Blocks Stacking \\ 
Number of Demo & 30 & 20 & 30 \\ 
\midrule
Chained Diffuser   & 0.3  & 0.2  & 0.4  \\ 
Open-loop HEP (Ours) & \textbf{0.8} & \textbf{0.85} & \textbf{0.9} \\ 
Closed-loop HEP (Ours) & - & \textbf{0.8} & \textbf{0.9} \\
\bottomrule
\end{tabular}
\end{table}

% \begin{table}[ht]
% \centering
% \caption{\textbf{Performance of Different Models in Closed Loop.}}
% \label{tab:performance_closed_loop}
% \renewcommand{\arraystretch}{1.1} % 调整行高
% \setlength{\tabcolsep}{6pt}       % 设置列间距
% \footnotesize % 调整文字大小为稍大尺寸
% \begin{tabular}{lcc}
% \toprule
% \textbf{Models} & \textbf{Blocks to Drawer} & \textbf{Blocks Stacking} \\ 
% \midrule
% EquiDiff   & 0    & 0    \\ 
% Ours & \textbf{0.8} & \textbf{0.9} \\ 
% \bottomrule
% \end{tabular}
% \end{table}

% \begin{figure}[t] 
%     \centering  \includegraphics[width=0.45\textwidth]{img/Workspace.png}
%     \caption{\textbf{Data Collection and Testing Distribution.}Data was collected in the sampling area, while testing included 10 positions within the sampling area and 10 randomly translated positions within the workspace.}
%     \label{Woekspace}
% \end{figure}

% \begin{figure}[t]
%     \begin{minipage}{0.42\linewidth}
%         \centering
%         \scriptsize
%         % \renewcommand{\arraystretch}{1.2} 
%         \setlength{\tabcolsep}{1pt} 
%         \begin{tabular}{cc}
%             \toprule
%             Model & Success Rate \\
%             \midrule
%             Chained Diffuser & 0.05 \\ 
%             HEP (Ours) & \textbf{0.9} \\
%             \bottomrule
%         \end{tabular}
%         \captionof{table}{Results of one-shot generalization experiment.}
%         \end{minipage}
%         \begin{minipage}{0.58\linewidth}
%         \centering
%         \scriptsize
%         % \renewcommand{\arraystretch}{1.2} 
%         \setlength{\tabcolsep}{1pt} 
%         \begin{tabular}{ccc}
%             \toprule
%             {Method} & {Color Change} & {Color+Objects} \\
%             \midrule
%             Chained Diffuser & 0         & 0 \\
%             \rowcolor{cyan!10} HEP (Ours)      & \textbf{0.9} & \textbf{0.6} \\
%             \bottomrule
%         \end{tabular}
%         \captionof{table}{Results of environmental variation experiment.}
%     \end{minipage}
% \end{figure}
\begin{figure}[t]
  \centering
  \begin{minipage}[t]{0.55\linewidth} % First subfigure: 2/3 of 0.5\textwidth
  \vspace{0pt}
    \includegraphics[width=\textwidth]{img/oneshot.png}
    \captionof{figure}{\textbf{One-Shot Test.} The model is trained on a single demonstration to evaluate its generalization capability.}
    % \subcaption{One-shot Experiment} % Subcaption for first image
    \label{fig:oneshot}
  \end{minipage}% <-- Remove space between minipages
  \hfill
\begin{minipage}[t]{0.4\linewidth}
\vspace{-0.2cm}
\captionof{table}{\textbf{Results of One-Shot Generalization Experiment.}}
\label{oneshot}
\centering
\scriptsize
% \renewcommand{\arraystretch}{1.2} 
\setlength{\tabcolsep}{1pt} 
\vskip 0.15in
\begin{tabular}{cc}
\toprule
Model & Success Rate \\
\midrule
Chained Diffuser & 0.05 \\ 
HEP (Ours) & \textbf{0.8} \\
\bottomrule
\end{tabular}
\end{minipage}
      
  % \begin{minipage}[t]{0.31\linewidth} % Second subfigure: 1/3 of 0.5\textwidth
  %   \includegraphics[width=\textwidth]{img/modified.png}
  %   \captionof{figure}{\textbf{Environment Variations}} % Subcaption for second image
  %   \label{fig:robust}
  % \end{minipage}
  % \vspace{-4pt} % Adjust vertical spacing between subfigures and caption
  % % \caption{\textbf{Generalizability Evaluation.}} % Main caption
  % \label{fig:generalizability}
  % \end{minipage}
\end{figure}

\paragraph{One-Shot Generalization} 
% \begin{wraptable}{r}{0.4\linewidth}
% \vspace{-0.7cm}
% \captionof{table}{\textbf{Results of One-Shot Generalization Experiment.}}
% \label{oneshot}
% \centering
% \scriptsize
% % \renewcommand{\arraystretch}{1.2} 
% \setlength{\tabcolsep}{1pt} 
% \vskip 0.15in
% \begin{tabular}{cc}
%     \toprule
%     Model & Success Rate \\
%     \midrule
%     Chained Diffuser & 0.05 \\ 
%     HEP (Ours) & \textbf{0.8} \\
%     \bottomrule
% \end{tabular}
% \end{wraptable}
To evaluate the generalizability of our model, we perform a one-shot experiment where the model is trained to finish a pick-place task with only one demonstration. During testing, the object is placed in unseen poses, as shown in \autoref{fig:oneshot}. The results in \autoref{oneshot} demonstrate the strong generalizability of our model, achieving an 80\% success rate over 20 trials. For comparison, we evaluate Chained Diffuser under the same setting, but it only succeeded when the toy car was positioned exactly as in the demonstration. This result highlights the superior generalization ability of our approach, enabling robust execution of manipulation tasks from limited training data.

% To evaluate our model's ability to generalize across object translations from a single demonstration, we designed a toy car grasping task. The primary objective was to assess whether the model could successfully grasp and place an object when it appeared in locations different from those seen during training. To isolate the impact of generalization and prevent large dataset sizes from influencing the results, we trained the model using only a single demonstration trajectory. In this task, a toy car was randomly placed within the workspace but outside the initial demonstration sampling region, requiring the robotic arm to grasp and relocate it to a predefined target area. \\During evaluation, our model demonstrated strong generalization capabilities, achieving a 90\% success rate over 20 trials. For comparison, we tested Equivariant Diffusion Policy and Chain Diffuser under identical conditions, each trained with a single demonstration trajectory. These baseline models successfully executed the task only when the toy car was positioned exactly as in the demonstration but failed when the object's location varied, as shown in \autoref{tab:toy_car_success}. These results highlight the superior translation generalization ability of our approach, enabling robust execution of manipulation tasks from limited training data.

% \begin{figure}[h!] 
%     \centering
%     % \begin{minipage}{\linewidth} 
%         \centering
%         \includegraphics[width=0.7\linewidth]{img/real3.png} 
%     % \end{minipage}
%     % \vspace{0.5cm} 
%     % \begin{minipage}{\linewidth}
%     %     \centering
%     %     \renewcommand{\arraystretch}{1.2} 
%     %     \setlength{\tabcolsep}{8pt} 
%     %     \begin{tabular}{>{\centering\arraybackslash}p{0.4\linewidth} >{\centering\arraybackslash}p{0.4\linewidth}}
%     %         \toprule
%     %         Model & Success Rate \\
%     %         \midrule
%     %         Chained Diffuser & 0.05 \\ 
%     %         HEP(Ours) & \textbf{0.9} \\
%     %         \bottomrule
%     %     \end{tabular}
%     % \end{minipage}
%     \caption{
%     % \textbf{Success rates in the toy car grasping task under 20 trials.} 
%     \textbf{One-shot Generalization Experiment.} One trial is tested at the same object position as the demonstration, while the remaining 19 trials are tested at random object positions.}
%     \label{fig:toy_car_success}
% \end{figure}


\paragraph{Robust to Environment Variations}
% \begin{wraptable}{r}{0.5\linewidth}
% \vspace{-0.7cm}
\begin{table}[t]
\captionof{table}{\textbf{Results of Environmental Variation Experiment.}}
\label{robust}
\vspace{0.15in}
\centering
\scriptsize
% \renewcommand{\arraystretch}{1.2} 
\setlength{\tabcolsep}{4pt} 
\begin{tabular}{cccc}
    \toprule
    {Method} & No Variation & {Color} & {Color+Objects} \\
    \midrule
    Chained Diffuser & 0.4 & 0         & 0 \\
     HEP (Ours)     & \textbf{0.9} & \textbf{0.9} & \textbf{0.6} \\
    \bottomrule
\end{tabular}
\end{table}
\begin{figure}[H]
\centering
\includegraphics[width=\linewidth]{img/modified_horizontal.png}
\captionof{figure}{\textbf{Environment Variations.} Left shows the training environment. Middle and right are the test environment with variation. } % Subcaption for second image
\label{fig:robust}
% \caption{\textbf{Generalizability Evaluation.}} % Main caption
\label{fig:generalizability}
\end{figure}
\vspace{-5pt}
In this experiment, we evaluate the robustness of our trained model under environmental variations. Specifically, we introduced modifications to the Block Stacking task during test time by changing the color of the table (Color) and additionally adding unrelated objects as distractors (Color+Objects), as shown in \autoref{fig:robust}. The result is shown in \autoref{robust}. Surprisingly, our model demonstrates exceptional adaptability, achieving 90\% and 60\% success rate under those two test-time variations, whereas the baseline fails to complete the task with those distractions.

% To evaluate the robustness of our model to environmental changes, we introduced modifications to the original block stacking task, as shown in \autoref{fig:blocks_stacking}. Specifically, we replaced the white baseboard with a black one to assess whether the model could still successfully complete the task under altered visual conditions, as illustrated in \autoref{fig:background_color_modification}. This modification aimed to test whether the model could still successfully complete the task under changed environment. \\Surprisingly, our model demonstrated exceptional adaptability, achieving a 90\% success rate (9 out of 10 trials), whereas the baseline models failed to complete the task.
% Encouraged by this unexpected result, we further increased the complexity of the modified environment by introducing various unrelated objects as distractors, as shown in \autoref{fig:unrelated_objects}. This additional challenge tested whether our model could maintain its performance in a significantly different scene with increased visual clutter. Even under these conditions, our model remained robust, successfully completing 6 out of 10 trials.  This shows that HEP is not simplify over-fitting and is able to transfer learned motor manipulation actions to novel environments.
% \begin{figure}[h!]
%     \centering
%     \begin{subfigure}{0.48\linewidth}
%         \centering
%         \includegraphics[width=\linewidth]{img/modifed.png}
%         \caption{\centering{Changed Color Of Background}}
%         \label{fig:background_color_modification}
%     \end{subfigure}
%     \hfill
%     \begin{subfigure}{0.48\linewidth}
%         \centering
%         \includegraphics[width=\linewidth]{img/fur_mod.png}
%         \caption{\centering{Background Color Change With Unrelated Objects}}
%         \label{fig:unrelated_objects}
%     \end{subfigure}
    % \vspace{5pt}
    % \begin{minipage}{\linewidth}
    %     \centering
    %     \renewcommand{\arraystretch}{1.2} 
    %     \setlength{\tabcolsep}{4pt} 
    %     \begin{tabular}{>{\centering\arraybackslash}p{0.4\linewidth} >{\centering\arraybackslash}p{0.25\linewidth} >{\centering\arraybackslash}p{0.25\linewidth}}
    %         \toprule
    %         \textbf{Method} & \textbf{Color Change} & \textbf{Color+Objects} \\
    %         \midrule
    %         Chained Diffuser & 0.1         & 0 \\
    %         \rowcolor{cyan!10} Ours      & \textbf{0.9} & \textbf{0.6} \\
    %         \bottomrule
    %     \end{tabular}
    % \end{minipage}
%     \caption{\textbf{Environment Variations Introduced at Test Time.}}
%     \label{fig:example_modifications}
% \end{figure}
\section{Conclusion}


In this work, we propose an Hierarchical Equivariant Policy for visuomotor policy learning. By utilizing Frame Transfer, our architecture naturally has both translational and rotational equivariance. 
% In order for robotic policy learning that can easily generalize to real world tasks, the ability to factor actions into high-level planning and low-level actions is essential for sample efficiency. 
Experimentally, HEP achieves significantly higher performance than previous methods on behavior cloning tasks that require fine motor control. 
While our work provides a solid foundation for hierarchical policies with geometric structure, several future directions remain open for exploration. One key limitation is that our experiments focus on tabletop manipulation. Extending HEP to more complex robotic tasks, such as humanoid motion, is a promising direction. Another limitation is the lack of memory mechanisms, which can be critical for tasks requiring history information. Future work could explore integrating Transformers~\cite{vaswani2017attention} to enhance temporal reasoning. Finally, expanding Frame Transfer to incorporate both translational and rotational specification could further improve the effectiveness of hierarchical policies.

\section{Acknowledgment}
The authors would like to extend their gratitude to Boce Hu for designing the fin-ray gripper fingers and for proofreading the paper, to Heng Tian for designing and fabricating components for the robot experiments, and to Shaoming Li for collecting demonstrations for the robot experiments.
% Our work provides a key starting point for building hierarchal policies with geometric information, and can easily be generalized to a broader range of tasks, such as humanoid motion, whereas one of the limitation of this paper is that we only demonstrate our work in the context of tabletop manipulation. Another limitation is that our model sometimes struggles with tasks requiring history information, and future works could address this by adding an LSTM~\cite{graves2012long} or Transformer~\cite{vaswani2017attention} to memorize past transitions. Lastly, another promising future work direction is extending Frame Transfer to specify both translation and rotation.


% \subsection{Future Work}
% In this work, we utilized a translational $T(3)$ frame transfer function. Utilization of an $SE(3)$ frame transfer function, combined with a low-level implementation, may allow for better generalization.

% \begin{itemize}
% \color{red}
% \item I would maybe add something about high level language planning here. I.e. oftentimes humans give instructions about tasks which must be translated into concrete policy i.e. $\pi(a|o , c)$ where $c$ is some context
% \item One other idea: Having a super-low level non-equivariant policy to deal with fine grain details,
% \end{itemize}

\newpage
\section{Impact Statement}
This paper presents work whose goal is to advance the field of Machine Learning. There are many potential societal consequences of our work, none which we feel must be specifically highlighted here.
\bibliography{icml_paper}
\bibliographystyle{icml2025}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% APPENDIX
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage
\appendix
\onecolumn


\section{Proof: The Full Policy is $SO(2)$ Equivariant  }\label{Section: Proof I}
Let us prove that the policy is $SO(2)$ Equivariant and satisfies
\begin{align}
\forall g\in SO(2), \quad \pi(  g \cdot o ) = g \cdot \pi( o )
\end{align}
We will prove this in two steps.
\subsection{Low-level Equivariance}
First, let us prove that the low-level agent is $SO(2)$ equivariant. The low-level policy can be written as 
\begin{align*}
    \pi_{low}(  o , t_{high} ) = \tau( \phi( \tau(o, t_{high}) ) , -t_{high} )
\end{align*}
The frame transfer functions satisfy
\begin{align*}
& \forall g \in SO(2), \quad \tau( g \cdot o , g \cdot t  ) = g \cdot \tau( o , t  ) \\
\end{align*}
and the diffusion policy satisfies
\begin{align*}
\forall g \in SO(2), \quad \phi( g \cdot o ) = g \cdot \phi( o ) \\
\end{align*}
Thus, we have that
\begin{align*}
\forall g \in SO(2),  \pi_{low}(  g\cdot o ,  g\cdot t_{high} )= \tau( \phi( \tau( g\cdot o, g\cdot t_{high}) ) , g\cdot -t_{high} )
\end{align*}
Using the frame transfer function property $\tau( g \cdot o , g \cdot t  ) = g \cdot \tau( o , t  ))$ we have that 
\begin{align*}
\forall g \in SO(2),  \pi_{low}( g\cdot o ,  g\cdot t_{high} ) =  \tau( \phi( g\cdot \tau( o, t_{high}) ) , g\cdot -t_{high} )
\end{align*}
Using the $SO(2)$ equivariance of the diffusion policy and the properties of the frame transfer functions, we have that
\begin{align*}
 \forall g \in SO(2),   \tau( \phi( g\cdot \tau( o, t_{high}) ) , g\cdot -t_{high} ) = \tau(  g\cdot \phi( \tau( o, t_{high}) ) , g\cdot -t_{high} ) =g \cdot \tau(  \phi( \tau( o, t_{high}) ) ,  -t_{high} )
\end{align*}
Thus,
\begin{align*}
\forall g \in SO(2),  \pi_{low}( g\cdot o ,  g\cdot t_{high} ) =  g \cdot \tau( \phi( \tau( o, t_{high}) ) , -t_{high} )
% \forall g \in SO(2),  \pi_{low}( g\cdot a |  g\cdot o ,  g\cdot t_{high} ) =  g \cdot \tau( \phi( \tau( o, t_{high}) ) , -t_{high} )
\end{align*}
And by definition,
\begin{align*}
\tau( \phi( \tau( o, t_{high}) ) , -t_{high} )=\pi_{low}(  o ,  t_{high} )
% \forall g \in SO(2),  \pi_{low}( g\cdot a |  g\cdot o ,  g\cdot t_{high} ) =  g \cdot \tau( \phi( \tau( o, t_{high}) ) , -t_{high} )
\end{align*}
Thus, we have that
\begin{align*}
  \forall g \in SO(2),  \pi_{low}(  g\cdot o ,  g\cdot t_{high} ) = g \cdot \pi_{low}( o ,  t_{high} )  
\end{align*}
\qed

\subsection{Full Policy Equivariance}
Using the equivariance of the low-level policy, let us show that the full policy is $SO(2)$ equivariant. The high-level, low-level and diffusion policies satisfy
\begin{align*}
\forall g \in SO(2), \quad \pi_{high}(  g \cdot o ) = g \cdot \pi_{high}(  o ) \\
\forall g \in SO(2), \quad \pi_{low}( g \cdot o, g \cdot t_{high}  ) = g \cdot \pi_{low}(  o, t_{high} ) \\
\end{align*}
Now, combining high-level and low-level policy together we got that: 
\begin{align*}
 \pi( o ) =  \pi_{low}( \pi_{high}( o ), o )
\end{align*}
% where, conditioned on the observation and the high-level action, the output of the low level policy is
% \begin{align*}
% \pi_{low}( \pi_{high}( o ),o ) = \tau( \phi( \tau(o, \pi_{high}( o )) ) , \pi_{high}( o ) )
% \end{align*}
When g acting on both the input observation, we have that
\begin{align*}
% \forall g \in SO(2), \quad  \pi( g \cdot a | g \cdot o ) = \sum_{ t_{high} } \pi_{high}( t_{high} | g \cdot o ) \pi_{low}( g \cdot a | t_{high},  g \cdot o )
\forall g \in SO(2), \quad  \pi(g \cdot o ) = \pi_{low}( \pi_{high}(g\cdot o ), g\cdot o )
\end{align*}
% Now, via the $SO(2)$ equivariance of the high-level policy $\pi_{high}( t_{high} | g \cdot o ) = \pi_{high}( g^{-1} t_{high} | o )$ we have that
Now, via the $SO(2)$ equivariance of the high-level policy $\pi_{high}(  g \cdot o ) = g \cdot \pi_{high}(  o )$ we have that
\begin{align*}
% \forall g \in SO(2), \sum_{ t_{high} } \pi_{high}( t_{high} | g \cdot o ) \pi( g \cdot a | t_{high},  g \cdot o ) = \sum_{ t_{high} } \pi_{high}(  g^{-1} t_{high} | o ) \pi( g \cdot a | t_{high},  g \cdot o ) = \sum_{ t_{high} } \pi_{high}(  t_{high} | o ) \pi_{low}( g \cdot a |  g \cdot t_{high},  g \cdot o ) 
\forall g \in SO(2),  \pi_{low}( \pi_{high}(g\cdot o ), g\cdot o ) = \pi_{low}( g\cdot \pi_{high}(o ), g\cdot o )
\end{align*}
Thus, using the $SO(2)$ equivariance of the low-level policy $\pi_{low}( g \cdot \pi_{high}(o ),  g \cdot o ) = g \cdot\pi_{low}( \pi_{high}(o ), o ) $ we have that
\begin{align*}
\forall g \in SO(2), \quad  \pi(  g \cdot o ) = g \cdot\pi_{low}( \pi_{high}(o ), o )
\end{align*}
Note that the $\pi_{low}( \pi_{high}(o ), o )$ is just the expression for $\pi(o)$. Thus, we must have that
\begin{align*}
\forall g \in SO(2), \quad  \pi( g \cdot o ) =g \cdot \pi(  o ) 
\end{align*}
holds.
\qed

\section{Proof: The Full Policy is $T(3)$ Equivariant  }\label{Section: Proof II}
% Draft Haibo
As defined in \autoref{frametsf}, $+, -$ as operators between $o$ or $a$ and $t_\high$ as addition and subtraction on the $(x, y, z)$ component of $o$ or $a$. Similarly, we can define the translation $t \in T(3)$ acting on $o$ or $a$ as an addition to the $(x, y, z)$ component as $o+t$ or $a+t$.


First, suppose that the high-level policy $\pi_{high}(o)$ is $T(3)$-equivariant so that
\begin{align*}
\forall t \in T(3), \enspace  \pi_{high}( o + t ) =  t + \pi_{high}(  o )
\end{align*}
which is simply the statement that shifting the scene shifts the high-level policy in the same way. Now, we will show that the full hierarchical policy satisfies the equivariance condition. The low-level policy is determined by
\begin{align*}
\pi_{low}( \pi_{high}(o) , o ) = \tau( \phi( \tau(o, \pi_{high}(o) ) ) , -\pi_{high}(o) )
\end{align*}
How does the low-level policy transform under a translation? Using $\pi( o ) = \pi_{low}( \pi_{high}(o) , o )$, we have that
\begin{align*}
\pi(o+t) = \pi_{low}( o + t , \pi_{high}(o + t) ) 
\end{align*}
Using the definition of the low-level policy, we have that
\begin{align*}
 \pi(o+t) = \pi_{low}( o + t , \pi_{high}(o+t) ) = \tau( \phi( \tau(o+t,\pi_{high}(o+t) ) , -\pi_{high}(o+t) ) 
\end{align*}
Now, using the equivariance of high-level policy, we have that $\pi_{high}( o + t ) =  t + \pi_{high}(  o )$ so that
\begin{align*}
\tau( \phi( \tau(o+t,\pi_{high}(o+t) ) , -\pi_{high}(o+t) ) = \tau( \phi( \tau(o+t,\pi_{high}(o)+t ) , -\pi_{high}(o)-t ) 
\end{align*}
Now, we can simplify this expression via the fact that the frame transfer $\tau$ function is $T(3)$ invariant. We have that $\tau(o+t,\pi_{high}(o)+t) = \tau(o,t_{high})$ which implies that
\begin{align*}
\tau( \phi( \tau(o+t,\pi_{high}(o)+t ) , -\pi_{high}(o)+t ) = \tau( \phi( \tau(o,\pi_{high}(o) ) , -\pi_{high}(o)-t )
\end{align*}
Now, note that $\tau(o+t,\pi_{high}(o)+t) = \tau(o,t_{high})$ implies that $\tau(o,-\pi_{high}(o)-t) = \tau(o + t,-\pi_{high}(o))$. Using the fact that
$\tau(o + t,-\pi_{high}(o)) = \tau(o,-\pi_{high}(o)) + t$, we have that
\begin{align*}
\tau( \phi( \tau(o,\pi_{high}(o) ) , -\pi_{high}(o)-t ) = \tau( \phi( \tau(o,\pi_{high}(o) ) , -\pi_{high}(o) ) + t
\end{align*}
Thus, combining the above expressions and using the definition of the low-level policy, we have that
\begin{align*}
\pi_{low}( o + t , \pi_{high}(o+t) ) = \tau( \phi( \tau(o,\pi_{high}(o) ) , -\pi_{high}(o)-t )  = t + \tau( \phi( \tau(o,\pi_{high}(o) ) , -\pi_{high}(o) ) = t + \pi_{low}( o , \pi_{high}(o) )
\end{align*}
Thus, we have that
\begin{align*}
\pi_{low}( o + t , \pi_{high}(o + t) ) =  t + \pi_{low}( o , \pi_{high}(o) )
\end{align*}
This is just the expression for the full policy function as $\pi(o) = \pi_{low}( o , \pi_{high}(o) ) $. Ergo, we must have that
\begin{align*}
\pi( o + t ) = t + \pi( o )
\end{align*}
and the full policy is $T(3)$-Equivariant.
\qed

\section{Proof: Equivariance of the Stacked Voxel Representation}
\label{app:stacked_voxel_proof}
\begin{proof}
We aim to prove that the stacked voxel representation $\nu$ is $T(3) \times SO(2)$-equivariant, i.e., 
\[
\nu(gP) = g\nu(P),
\]
where $g \in T(3) \times SO(2)$ is a group transformation.

Define a point set selection function $m: j_x, j_y, j_z, P\mapsto P_J$ that selects the subset of points $P_j \subseteq P$ within the voxel indexed by $(j_x, j_y, j_z)$. By the definition, $m$ is an equivariant function $m(g(j_x, j_y, j_z), gP) = gm(j_x, j_y, j_z, P)$. 

The stacked voxel representation $\nu$ for a given voxel location $j_x, j_y, j_z$ can be written as:
\[
\nu(P)(j_x, j_y, j_z) = l\left(m(j_x, j_y, j_z, P)\right),
\]
where: 
\begin{itemize}
    \item $m(j_x, j_y, j_z, P)$ selects the subset of points $P_j \subseteq P$ within the voxel indexed by $(j_x, j_y, j_z)$,
    \item $l(P_j)$ maps the selected point subset $P_j$ to a feature vector representing the voxel at $(j_x, j_y, j_z)$.
\end{itemize}

Substitute $P = gP$ into $\nu(P)$. Using the definition, we have:
\[
\nu(gP)(j_x, j_y, j_z) = l\left(m(j_x, j_y, j_z, gP)\right).
\]

The point set selection function $m$ is equivariant to group transformations, so we have:
\[
m(j_x, j_y, j_z, gP) = g \, m(g^{-1}(j_x, j_y, j_z), P).
\]
% This means that selecting the points from $gP$ at voxel $(j_x, j_y, j_z)$ is equivalent to:
% \begin{enumerate}
%     \item Applying $g^{-1}$ to transform the voxel indices back to their original position under $P$,
%     \item Transforming the selected points by $g$.
% \end{enumerate}

Substitute this into the expression for $\nu(gP)$:
\[
\nu(gP)(j_x, j_y, j_z) = l\left(g \, m(g^{-1}(j_x, j_y, j_z), P)\right).
\]

% \textbf{Step 4: Equivariance of the PointNet $l$}

The PointNet $l$ is $SO(2)$-equivariant and $T(3)$-invariant, meaning:
\[
l(gP_j) = \rho(\theta)l(P_j),
\]
where $P_j = m(j_x, j_y, j_z, P)$, and $\rho(\theta)$ is the linear representation of the rotation group action.

Applying this to $l(g \, m(g^{-1}(j_x, j_y, j_z), P))$:
\[
\nu(gP)(j_x, j_y, j_z) = \rho(\theta)l\left(m(g^{-1}(j_x, j_y, j_z), P)\right).
\]

% \textbf{Step 5: Relating $\nu(gP)$ and $\nu(P)$}

From the definition of $\nu(P)$, we know:
\[
l\left(m(g^{-1}(j_x, j_y, j_z), P)\right) = \nu(P)(g^{-1}(j_x, j_y, j_z)).
\]

Substituting this into the equation:
\[
\nu(gP)(j_x, j_y, j_z) = \rho(g) \nu(P)(g^{-1}(j_x, j_y, j_z)).
\]

% \textbf{Step 6: Final Transformation to Group Action on $\nu$}

Since $\mcV = \nu(P)$ is a voxel grid (in function representation), the group action on $\nu$ is defined as:
\[
(g\nu(P))(j_x, j_y, j_z) = \rho(g)\nu(P)(g^{-1}(j_x, j_y, j_z)).
\]

Thus, we have:
\[
\nu(gP) = g\nu(P).
\]

\end{proof}

\section{Additional Background of Group Symmetry}
The group $T(3)\times \SO(2)$ can be naturally decomposed into translation $T(3)$, which is handled using methods like 3D convolution, and rotation $\SO(2)$, which is addressed via network design by introducing equivariant layers~\cite{cesa2022a} that respect $\SO(2)$ transformations through appropriate representations of $\SO(2)$ or its subgroups. 

\subsection{Group Action of $\SO(2)$}
We focus on three particular representations of $g\in \mathrm{SO}(2)$ or its subgroup $g\in C_u$ (containing $u$ discrete rotations) that define how the group acts on different data. Specifically:

% Our interest is mainly on the group $T(3)$ of 3D translation, as well as the group $\SO(2)$ of planar rotations (the rotation is around the
% z-axis of the world coordinate system) and its subgroup $\C_u$ containing u discrete rotations. By introducing equivariant learning, our policy would have the correct inductive bias to improve sampling efficiency. We focus on three particular representations of $g\in \mathrm{SO}(2)$ or $g\in C_u$ in this paper:

\underline{Trivial Representation $\rho_0$}: The trivial representation $\rho_0$ characterizes the action of $\mathrm{SO}(2)$ or $C_u$ on an invariant scalar $x \in \mathbb{R}$ such that $
\rho_0(g) x = x.$
This means that every group element $g$ leaves the scalar $x$ unchanged.

\underline{Standard Representation $\rho_1$}: The standard representation $\rho_1$ defines how $\mathrm{SO}(2)$ or $C_u$ acts on a vector $v \in \mathbb{R}^2$ using a $2 \times 2$ rotation matrix. The action is given by 
$
\rho_\omega(g) v = \begin{psmallmatrix}
  \cos g & -\sin g \\
  \sin g & \cos g
\end{psmallmatrix} v.
$
When $\omega = 1$, the representation $\rho_1(g)$ corresponds to the standard $2 \times 2$ rotation matrix.

\underline{Regular Representation $\rho_{\text{reg}}$}: The regular representation $\rho_{\text{reg}}$ describes the action of $C_u$ on a vector $x \in \mathbb{R}^u$ via $u \times u$ permutation matrices. Let $g = r^m$ be an element of the cyclic group $C_u = \{1, r^1, \ldots, r^{u-1}\}$, and let $x = (x_1, x_2, \dots, x_u) \in \mathbb{R}^u$. Then the action is defined by $
\rho_{\text{reg}}(g) x = \left( x_{u - m + 1}, x_{u - m + 2}, \dots, x_u, x_1, x_2, \dots, x_{u - m} \right). $
This operation cyclically permutes the coordinates of $x$ in $\mathbb{R}^u$.\\
\\
A representation $\rho$ can also be constructed as a combination of different representations. Specifically, $\rho$ is defined as the direct sum $\rho = \rho_0^{n_0} \oplus \rho_1^{n_1} \oplus \rho_2^{n_2}$, 
which belongs to the general linear group $GL(n_0 + 2n_1 + 2n_2)$. In this case, $\rho(g)$ is a block diagonal matrix of size $(n_0 + 2n_1 + 2n_2) \times (n_0 + 2n_1 + 2n_2)$ that acts on vectors $x \in \mathbb{R}^{n_0 + 2n_1 + 2n_2}$. 

\subsection{Group Action of $T(3)$}
Follow the definition of + -
The group $T(3)$ of 3D translations is an additive group, whose action is defined by shifting spatial coordinates. For example, for a point cloud $P=\{p_1, p_2, \dots\}$ where $p_i=(x_i, y_i, z_i)$, the action of $g\in T(3)$ is $t\cdot p_i = (x_i + t_x, y_i + t_y, z_i + t_z)$. Similarly, for voxel-based representations, $T(3)$ acts by shifting the spatial indices of the voxel grid. 
Convolutions are inherently translationally invariant. <- I would say this
Translation symmetry is naturally handled by operations such as 3D convolutions, which are inherently translation-equivariant.

\section{Training Detail}
\label{training_detail}
In the simulation experiments, we  we use a batch size of 16 for
training. Specifically, the observation contains one step of history observation, and 3 steps of history action and the output of
the denoising process is a sequence of 18 action steps. In close-loop control we use all 18 steps for training and execute 18 steps, similar to prior work (\cite{xian2023chaineddiffuser}). In close-loop control 18 steps and 9 steps are used for training and execution, similar to setting of \cite{wang2024equivariant} a. We train our models with the AdamW (\cite{loshchilov2019decoupledweightdecayregularization}) optimizer (with a
learning rate of $10^{-4}$ and weight decay of 5*$10^{-4}$). We use DDPM (\cite{ddpm}) with 100 denoising
steps for both training and evaluation. We training each tasks with 100000 iterates. 

\section{Detail of Simulation Tasks}
\label{detail_of_sim}
Here are descriptions of 30 tasks, as shown in \autoref{fig:complete_sim}, mentioned in simulation experiment:
\begin{enumerate}
    \item \textbf{Pick/Lift}: Grasp and lift a block from the table.
    \item \textbf{Push Button}: Press a button.
    \item \textbf{Knife on Board}: Place a knife onto a cutting board.
    \item \textbf{Put Money}: Put dollars in safe.
    \item \textbf{Reach Target}: Move the gripper to a specified target location.
    \item \textbf{Slide Block}: Slide a block across the table to certain area.
    \item \textbf{Stack Wine}: Put wine bottles into a shelf.
    \item \textbf{Take Money}: Take dollars from safe.
    \item \textbf{Take Umbrella}: Retrieve an umbrella from a stand.
    \item \textbf{Pick up Cup}: Grasp and lift a cup.
    \item \textbf{Unplug Charger}: Disconnect a charger from an outlet.
    \item \textbf{Close Door}: Shut a door fully.
    \item \textbf{Open Box}: Lift the lid of a box.
    \item \textbf{Open Fridge}: Pull the fridge door open.
    \item \textbf{Frame off Hanger}: Remove a frame from a hanger.
    \item \textbf{Open Oven}: Open the oven door.
    \item \textbf{Books on Shelf}: Put book on a shelf.
    \item \textbf{Wipe Desk}: Wipe a desk surface clean using a cloth.
    \item \textbf{Cup in Cabinet}: Place a cup inside a cabinet.
    \item \textbf{Shoe out of Box}: Remove a shoe from its box.
    \item \textbf{Open Microwave}: Open a microwave door.
    \item \textbf{Turn on Lamp}: Activate a lamp using its switch.
    \item \textbf{Open Grill}: Lift the lid of a grill.
    \item \textbf{Stack Blocks}: Stack blocks on top of each other.
    \item \textbf{Stack Cups}: Arrange cups in a stacked configuration.
    \item \textbf{Push 3 Buttons}: Press three buttons in a specific sequence.
    \item \textbf{Plug USB in Computer}: Insert a USB device into a port.
    \item \textbf{Open Drawer}: Pull a drawer open.
    \item \textbf{Put Item in Drawer}: Pull a drawer open and place an object inside a drawer.
    \item \textbf{Sort Shape}: Put shape in a shape sorter
\end{enumerate}


\begin{figure*}[ht]
\centering
\includegraphics[width=\linewidth]{img/complete_sim.png}
\caption{\textbf{All simulation tasks we evaluate on}}
\label{fig:complete_sim} % Add a label for referencing
\end{figure*}

\section{Real-World Experimental Settings}
\label{sec:realworld-exp}
Our real-world experiments are conducted on a UR5e robotic arm equipped with a Robotiq 2F-85 gripper and three Intel RealSense D455 cameras as shown in \autoref{fig:complete_real} . Demonstrations are collected using a 6-DoF 3DConnexion SpaceMouse at a 10 Hz rate, logging both the visual observations (from all three cameras) and the robot’s end-effector actions (position, orientation, and gripper states).
% \label{detail_of_sim}
\begin{figure*}[ht]
\centering
\includegraphics[width=\linewidth]{img/setting.png}
\caption{\textbf{Real-world Experiment Setting}}
\label{fig:complete_real} % Add a label for referencing
\end{figure*}
% temp
\section{Comparison With Hierarchical Diffusion Policy(HDP)}
We also compare our policy with another hierarchical baseline, HDP \cite{ma2024hierarchical}, by selecting seven tasks from the HDP paper that we evaluated. We then compare the success rates on these tasks, as shown in \autoref{tab:hdp-hep-results}. Our approach achieves an absolute mean improvement of 20\%, demonstrating superior sampling efficiency.
\label{comp_with_hdp}
\begin{table}[t]
\caption{\textbf{Performance of HDP and HEP on 7 Tasks.}}
\label{tab:hdp-hep-results}
\vskip 0.15in
\centering
\setlength{\tabcolsep}{0.3pt}
\scriptsize
\newcolumntype{C}{>{\centering\arraybackslash}p{1.2cm}} % Adjusted width for better fit
\begin{tabular}{@{}lCCCCCCCC@{}}
\toprule
Method\textbf{(Open-loop)} & \textbf{Mean} & Reach Target & Pick Up Cup & Open Box & Open Drawer & Open Microwave & Open Oven & Knife on Board \\
\midrule
HDP & 74 & \textbf{100} & 82 & 90 & 90 & 26 & 58 & 72 \\
HEP (Ours)  & \textbf{94}\textcolor{blue}{(+20)} & \textbf{100} & \textbf{98}\textcolor{blue}{(+16)} & \textbf{100}\textcolor{blue}{(+10)} & \textbf{94}\textcolor{blue}{(+4)} & \textbf{82}\textcolor{blue}{(+56)} & \textbf{87}\textcolor{blue}{(+29)} & \textbf{96}\textcolor{blue}{(+24)} \\
\bottomrule
\end{tabular}
\end{table}

\section{Full Result of Ablation Study}
We show the full result of ablation study (\autoref{ab}) here at \autoref{tab:ablation-results-all}. 
\begin{table}[t]
\caption{\textbf{Performance of Different Ablations on Various Tasks.}}
\label{tab:ablation-results-all}
\vskip 0.15in
\centering
\setlength{\tabcolsep}{0.3pt}
\scriptsize
% \resizebox{\textwidth}{!}{
\newcolumntype{C}{>{\centering\arraybackslash}p{0.9cm}} % Adjusted width for better fit
\begin{tabular}{@{}lCCCCCCC@{}}
\toprule
Method & Mean & Lamp on & Open microw. & Push 3 buttons & Push button & Open box & Insert USB \\
\midrule
No Equi No FT & 0.60 & 0.21 & 0.44 & 0.53 & 0.96 & 0.99 & 0.51 \\
No Equi               & 0.70 & 0.41 & 0.53 & 0.67 & 0.98 & 0.99 & 0.64 \\
No FT & 0.78 & 0.75 & 0.56 & 0.73 & 0.98 & 0.99 & 0.68 \\
No Stacked Voxel      & 0.84 & 0.77 & 0.65 & 0.87 & 0.99 & 0.99 & 0.79 \\
\textbf{Complete Model}        & \textbf{0.94} & \textbf{0.95} & \textbf{0.82} & \textbf{0.99} & \textbf{1.00} & \textbf{1.00} & \textbf{0.90} \\
\bottomrule
\end{tabular}
% }
\end{table}
\section{Voxelization Details}
We build our voxelization function based on \cite{mmdet3d2020}. The size of our voxel grid is 64*64*64 with maximum 6 points within it.
\end{document}

% This document was modified from the file originally made available by
% Pat Langley and Andrea Danyluk for ICML-2K. This version was created
% by Iain Murray in 2018, and modified by Alexandre Bouchard in
% 2019 and 2021 and by Csaba Szepesvari, Gang Niu and Sivan Sabato in 2022.
% Modified again in 2023 and 2024 by Sivan Sabato and Jonathan Scarlett.
% Previous contributors include Dan Roy, Lise Getoor and Tobias
% Scheffer, which was slightly modified from the 2010 version by
% Thorsten Joachims & Johannes Fuernkranz, slightly modified from the
% 2009 version by Kiri Wagstaff and Sam Roweis's 2008 version, which is
% slightly modified from Prasad Tadepalli's 2007 version which is a
% lightly changed version of the previous year's version by Andrew
% Moore, which was in turn edited from those of Kristian Kersting and
% Codrina Lauth. Alex Smola contributed to the algorithmic style files.


