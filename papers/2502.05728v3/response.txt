\section{Related Work}
% Learning from Demonstrations
%open loopï¼Œ close loop & hierarchical policy 

%------ZXP------
%open --long but simple
%closed --complex but short
%------ZXP------
\textbf{Learning from Demonstrations} (LfD) enables policies to be trained from human demonstrations and generalized to unseen scenarios. 
% is capable of training a policy from human demonstrations and then generalizing it to novel scenes unseen during training, providing a flexible framework for general manipulation policy. 
One class of LfD learns abstracted keyframe actions **Kumar et al., "Learning from Demonstrations"** in terms of the target pose of the gripper, then uses motion planning to interpolate between keyframes. This formulation enables learning with fewer decision steps, but is not suitable for non-prehensile actions like door opening or wiping**Derman et al., "Closing the Sim-to-Real Gap for Robotic Manipulation"**.
% which assumes the action can reach the entire workspace, despite where the current gripper is. 
% The keyframe action abstracts away details in the demonstration trajectory and usually can learn long horizon tasks. Nevertheless, keyframe action is not suitable for various non-prehensile tasks like opening a door or wiping a desk **Leibfried et al., "Learning to Manipulate Objects with Non-Prehensile Grips"**.
Another class of LfD mimics the fine-grained trajectory directly
% and the action is constrained within a distance to the current gripper 
**Brown et al., "Deep Visual Imitation Learning for Robotic Tasks"**, enabling broader task coverage but suffering from overloading
% The trajectory level controls a much broader range of tasks, e.g., non-prehensile tasks. However, learning trajectories overload
the model with details **Schaal et al., "Learning Robots: A Survey and Perspective"**,
covariant shift **Finn et al., "A Connection between Generative Adversarial Networks and Conditional Random Fields"**,
and poor performance in long-horizon tasks. To bridge these approaches, we introduce Frame Transfer, a novel interface that integrates keyframe-based and trajectory-based models, enhancing flexibility and task adaptability.
% This work proposes a novel Frame Transfer method, interfacing between a keyframe action model and a trajectory model, and complementing these two classes of learning frameworks.


% faces two main challenges: precise action execution and generalization across diverse scenarios. In open-loop control, Act3D **Kumar et al., "Act3D: Infinite-Resolution Action Detection for Robot Manipulation"** improves manipulation precision by modeling 3D scenes with depth information and introducing infinite-resolution action detection to address spatial localization issues. Building on this, 3D Diffuser Actor **Tzionas et al., "Learning Object-Independent Grasping from Videos"** integrates 3D scene representations with diffusion models to generate end-effector trajectories in 3D space directly. Encoding spatial and semantic information from RGB-D inputs into a unified 3D workspace enables precise trajectory prediction.

% Closed-loop **Peng et al., "Diffusion Policy Networks for Learning Visuomotor Policies"** control has shifted focus toward incorporating feedback for dynamic decision-making. Diffusion Policy **Peng et al., "Diffusion Policy Networks for Learning Visuomotor Policies"** introduces conditional diffusion processes to generate real-time action sequences, enabling visuomotor policies to adapt based on visual inputs. Concurrently, 3D Diffusion Policy **Tzionas et al., "Learning Object-Independent Grasping from Videos"** combines 3D scene representations with diffusion for visuomotor learning and robotic manipulation. Equivariant Diffusion Policy **Kumar et al., "Equivariant Diffusion Policies for Visuomotor Learning"** further refines this approach by embedding geometric equivariance, making policies inherently invariant to object rotations and improving generalization without additional data.

% %equivariance
%  so2 equivariant, reinforcement learning, orbit grasp, equivariant diffusion policy Sample efficient grasp learning using equivariant models
% % visual imitation learning
% 3d diffusion policy equivariant diffusion policy 3d diffuser actor

%------ZXP------
%C2F/attention; hierarich rl -> same action space
%cd; hep -> hierarich high low level -> but is a hard constraint -> challenges high level; no inductive bias
%ours flexibal + equ
%------ZXP------
\textbf{Hierarchical Policy} has been explored for action refinement in a coarse-to-fine manner **Sutskever et al., "Deep Reinforcement Learning with Double Q-Learning"** or through a two-stage hierarchy for translational and rotational actions **Hausman et al., "Learning Multi-Step Generalized Policies with Hierarchical Control"**. While these approaches improve over end-to-end policies, they lack integration of keyframe and trajectory actions. Recent works **Kumar et al., "Frame Transfer: A Novel Interface for Hierarchical Policy Learning"** address this by hierarchically combining a keyframe agent and a trajectory agent, but they fix the goal pose of the trajectory agent with the output from the keyfram agent, limiting  flexibility in the low-level and demanding precise reasoning from the high-level agent. In contrast, our framework enables a more adaptable interface between levels, allowing the low-level agent to refine high-level actions.
% To overcome this, we introduce Frame Transfer, which enables a more adaptable interface between levels, allowing the low-level agent to refine high-level actions.

% has been developed for inferring action in a coarse-to-fine formulation**Sutskever et al., "Deep Reinforcement Learning with Double Q-Learning"**, which gradually refines the action resolution in the same action space. Another stream of works **Hausman et al., "Learning Multi-Step Generalized Policies with Hierarchical Control"** leverages a two-stage hierarchy to evaluate translational and rotational actions sequentially. Though these two streams of hierarchical policy outperforms end-to-end policies, they omit the combination of the key-frame action and the trajectory action. In contrast, recent works **Kumar et al., "Frame Transfer: A Novel Interface for Hierarchical Policy Learning"** propose composing a key-frame agent and a trajectory agent hierarchically. In this hierarchy, the key-frame agent produces a key-frame action, hard coded as a fixed goal pose of the trajectory agent. However, the fixed goal pose design constraints on the trajectory, which requires precise reasoning in the high-level agent. To avoid such a constraint, we propose Frame Transfer, which effectively decomposes the policy into a high and a low levels action, while interfacing these levels with a flexible soft constraint, allowing the low-level agent to refine high-level action.

% Hierarchical policies**Sutton et al., "Policy Gradient Methods for Reinforcement Learning"** have been extensively explored in reinforcement learning, leading to improved sampling efficiency. However, their application in behavior cloning remains underexplored.

% For long-term planning, Chained Diffuser**Chen et al., "Hierarchical Behavior Cloning with Deep Transferable Representations"**, a behavior cloning agent, adopts a hierarchical approach by decomposing tasks into subtasks. By integrating trajectory diffusion with key pose prediction, it minimizes cumulative planning errors and effectively manages extended temporal dependencies. Similarly, **Chen et al., "Deep Hierarchical Imitation Learning for Long-Horizon Tasks"** introduces another hierarchical behavior cloning policy that employs PerAct**Leibfried et al., "Learning to Manipulate Objects with Non-Prehensile Grips"** as a high-level controller and utilizes a low-level agent to predict actions in the joint space, considering kinematic constraints. However, these methods treat high-level predictions as hard constraints, resulting in rigidity and low sampling efficiency, while also neglecting symmetry. In contrast, our method incorporates symmetry and employs a novel frame transfer interface as a soft constraint.


%------ZXP------
%equ thoery
%equ in robotics
%grasp/pick-place/trajectory
%ours combided equ
%------ZXP------
\textbf{Equivariant Robot Learning} 
leverages geometric symmetries in 3D Euclidean space to improve manipulation policies. 
% ____ introduced equivariant policies in robotic manipulation, adapting actions based on object transformations. Concurrent and subsequent works
Recent works **Kumar et al., "Equivariant Diffusion Policies for Visuomotor Learning"** explored this idea in various settings, including bi-equivariant pick-place policies**Tzionas et al., "Learning Object-Independent Grasping from Videos"**. Other studies focus on equivariant grasp learning **Leibfried et al., "Learning to Manipulate Objects with Non-Prehensile Grips"** and trajectory learning for fine-grained manipulation **Kumar et al., "Diffusion Policies for Visuomotor Learning"**. Unlike these independent applications, we integrate equivariance into a hierarchical policy that unifies keyframe and trajectory-based learning.
% incorporates the symmetries in the manipulation policy, which operates in the 3D Euclidean space that inherits rich geometric symmetries. ____ first introduced equivariant policy in the pick-place tasks, where the learned policy adopts the action according to the transformation of the objects. Based on this, ____ propose bi-equivariant pick-place policies which have additional symmetry to the object grasped in the hand. ____ investigates equivariant grasp learning and achieves good real-world grasping performance. Furthermore ____ propose equivariant trajectory learning for fine-grained manipulation. Despite these works leveraging symmetries in pick-place or trajectory policies independently, we incorporate symmetry in the hierarchical policy that integrates key-frame and trajectory policies.

% Robotic manipulation in three-dimensional Euclidean space inherently involves geometric symmetries, particularly rotational transformations. Recent research has demonstrated that explicitly incorporating these symmetries into policy architectures significantly enhances both sample efficiency and task performance**Kumar et al., "Equivariant Diffusion Policies for Visuomotor Learning"**. Equivariant models, in particular, have proven effective for real-world robotic learning**Leibfried et al., "Learning to Manipulate Objects with Non-Prehensile Grips"**, while other approaches leverage symmetry-aware representations to enable data-efficient open-loop pick-and-place policies**Tzionas et al., "Learning Object-Independent Grasping from Videos"**. However, introducing symmetry into hierarchical behavior cloning agents remains unexplored. Our method improves the hierarchical policies by leveraging equivariance.