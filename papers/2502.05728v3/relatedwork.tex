\section{Related Work}
% Learning from Demonstrations
%open loopï¼Œ close loop & hierarchical policy 

%------ZXP------
%open --long but simple
%closed --complex but short
%------ZXP------
\textbf{Learning from Demonstrations} (LfD) enables policies to be trained from human demonstrations and generalized to unseen scenarios. 
% is capable of training a policy from human demonstrations and then generalizing it to novel scenes unseen during training, providing a flexible framework for general manipulation policy. 
One class of LfD learns abstracted keyframe actions \cite{james2022q, james2022coarse, shridhar2023perceiver, gervet2023act3d, goyal2023rvt} in terms of the target pose of the gripper, then uses motion planning to interpolate between keyframes. This formulation enables learning with fewer decision steps, but is not suitable for non-prehensile actions like door opening or wiping~\cite{xian2023chaineddiffuser, ma2024hierarchical}.
% which assumes the action can reach the entire workspace, despite where the current gripper is. 
% The keyframe action abstracts away details in the demonstration trajectory and usually can learn long horizon tasks. Nevertheless, keyframe action is not suitable for various non-prehensile tasks like opening a door or wiping a desk \cite{xian2023chaineddiffuser, ma2024hierarchical}.
Another class of LfD mimics the fine-grained trajectory directly
% and the action is constrained within a distance to the current gripper 
\cite{song2020grasping, ye2022bagging, toyer2020magical, zhang2018deep, chi2023diffusionpolicy, zhu2023viola, mandlekar2021matters, zhao2023learning, wang2024equivariant}, enabling broader task coverage but suffering from overloading
% The trajectory level controls a much broader range of tasks, e.g., non-prehensile tasks. However, learning trajectories overload
the model with details \cite{zhao2023learning}, covariant shift \cite{ke2021grasping}, and poor performance in long-horizon tasks. To bridge these approaches, we introduce Frame Transfer, a novel interface that integrates keyframe-based and trajectory-based models, enhancing flexibility and task adaptability.
% This work proposes a novel Frame Transfer method, interfacing between a keyframe action model and a trajectory model, and complementing these two classes of learning frameworks.


% faces two main challenges: precise action execution and generalization across diverse scenarios. In open-loop control, Act3D \cite{gervet2023act3d} improves manipulation precision by modeling 3D scenes with depth information and introducing infinite-resolution action detection to address spatial localization issues. Building on this, 3D Diffuser Actor \cite{ke20243d} integrates 3D scene representations with diffusion models to generate end-effector trajectories in 3D space directly. Encoding spatial and semantic information from RGB-D inputs into a unified 3D workspace enables precise trajectory prediction.

% Closed-loop \cite{song2020grasping, ye2022bagging, toyer2020magical, zhang2018deep} control has shifted focus toward incorporating feedback for dynamic decision-making. Diffusion Policy \cite{chi2023diffusionpolicy} introduces conditional diffusion processes to generate real-time action sequences, enabling visuomotor policies to adapt based on visual inputs. Concurrently, 3D Diffusion Policy \cite{ze20243d} combines 3D scene representations with diffusion for visuomotor learning and robotic manipulation. Equivariant Diffusion Policy \cite{wang2024equivariant} further refines this approach by embedding geometric equivariance, making policies inherently invariant to object rotations and improving generalization without additional data.

% %equivariance
%  so2 equivariant, reinforcement learning, orbit grasp, equivariant diffusion policy Sample efficient grasp learning using equivariant models
% % visual imitation learning
% 3d diffusion policy equivariant diffusion policy 3d diffuser actor

%------ZXP------
%C2F/attention; hierarich rl -> same action space
%cd; hep -> hierarich high low level -> but is a hard constraint -> challenges high level; no inductive bias
%ours flexibal + equ
%------ZXP------
\textbf{Hierarchical Policy} has been explored for action refinement in a coarse-to-fine manner \cite{levy2018hierarchical, gualtieri2020learning, james2022coarse} or through a two-stage hierarchy for translational and rotational actions \cite{sharma2017learning, wang2020policy, rss22xupeng}. While these approaches improve over end-to-end policies, they lack integration of keyframe and trajectory actions. Recent works \cite{xian2023chaineddiffuser, ma2024hierarchical} address this by hierarchically combining a keyframe agent and a trajectory agent, but they fix the goal pose of the trajectory agent with the output from the keyfram agent, limiting  flexibility in the low-level and demanding precise reasoning from the high-level agent. In contrast, our framework enables a more adaptable interface between levels, allowing the low-level agent to refine high-level actions.
% To overcome this, we introduce Frame Transfer, which enables a more adaptable interface between levels, allowing the low-level agent to refine high-level actions.

% has been developed for inferring action in a coarse-to-fine formulation~\cite{levy2018hierarchical, gualtieri2020learning, james2022coarse}, which gradually refines the action resolution in the same action space. Another stream of works \cite{sharma2017learning, wang2020policy, rss22xupeng} leverages a two-stage hierarchy to evaluate translational and rotational actions sequentially. Though these two streams of hierarchical policy outperforms end-to-end policies, they omit the combination of the key-frame action and the trajectory action. In contrast, recent works \cite{xian2023chaineddiffuser,ma2024hierarchical} propose composing a key-frame agent and a trajectory agent hierarchically. In this hierarchy, the key-frame agent produces a key-frame action, hard coded as a fixed goal pose of the trajectory agent. However, the fixed goal pose design constraints on the trajectory, which requires precise reasoning in the high-level agent. To avoid such a constraint, we propose Frame Transfer, which effectively decomposes the policy into a high and a low levels action, while interfacing these levels with a flexible soft constraint, allowing the low-level agent to refine high-level action.

% Hierarchical policies~\cite{NEURIPS2018_e6384711, vezhnevets2017feudal, wen2020efficiency, florensa2017stochastic} have been extensively explored in reinforcement learning, leading to improved sampling efficiency. However, their application in behavior cloning remains underexplored.

% For long-term planning, Chained Diffuser~\cite{xian2023chaineddiffuser}, a behavior cloning agent, adopts a hierarchical approach by decomposing tasks into subtasks. By integrating trajectory diffusion with key pose prediction, it minimizes cumulative planning errors and effectively manages extended temporal dependencies. Similarly, \cite{ma2024hierarchical} introduces another hierarchical behavior cloning policy that employs PerAct~\cite{shridhar2023perceiver} as a high-level controller and utilizes a low-level agent to predict actions in the joint space, considering kinematic constraints. However, these methods treat high-level predictions as hard constraints, resulting in rigidity and low sampling efficiency, while also neglecting symmetry. In contrast, our method incorporates symmetry and employs a novel frame transfer interface as a soft constraint.


%------ZXP------
%equ thoery
%equ in robotics
%grasp/pick-place/trajectory
%ours combided equ
%------ZXP------
\textbf{Equivariant Robot Learning} 
leverages geometric symmetries in 3D Euclidean space to improve manipulation policies. 
% \cite{wangmathrm} introduced equivariant policies in robotic manipulation, adapting actions based on object transformations. Concurrent and subsequent works
Recent works ~\cite{iclr22,corl,liu2023continual,kim2023se,kohler2023symmetric,nguyen2023equivariant,nguyen2024symmetry,eisner2024deep,gao2024riemann} explored this idea in various settings, including bi-equivariant pick-place policies~\cite{neural_descriptor,ryu2023diffusion,ryu2023equivariant, pan2023tax,rss22haojie,huang2024fourier, huang2024imagination,huang2024match}. Other studies focus on equivariant grasp learning \cite{rss22xupeng, huang2023edge, huorbitgrasp,lim2024equigraspflow} and trajectory learning for fine-grained manipulation \cite{jia2023seil, wang2024equivariant,yang2024equivact,yang2024equibot}. Unlike these independent applications, we integrate equivariance into a hierarchical policy that unifies keyframe and trajectory-based learning.
% incorporates the symmetries in the manipulation policy, which operates in the 3D Euclidean space that inherits rich geometric symmetries. \cite{wang2020policy} first introduced equivariant policy in the pick-place tasks, where the learned policy adopts the action according to the transformation of the objects. Based on this, \cite{neural_descriptor, ryu2023equivariant, rss22haojie, huang2024fourier, huang2024imagination} propose bi-equivariant pick-place policies which have additional symmetry to the object grasped in the hand. \cite{rss22xupeng, huang2023edge, huorbitgrasp} investigates equivariant grasp learning and achieves good real-world grasping performance. Furthermore \cite{wangmathrm, jia2023seil, wang2024equivariant, wanggeneral, yang2024equivact} propose equivariant trajectory learning for fine-grained manipulation. Despite these works leveraging symmetries in pick-place or trajectory policies independently, we incorporate symmetry in the hierarchical policy that integrates key-frame and trajectory policies.

% Robotic manipulation in three-dimensional Euclidean space inherently involves geometric symmetries, particularly rotational transformations. Recent research has demonstrated that explicitly incorporating these symmetries into policy architectures significantly enhances both sample efficiency and task performance~\citep{iclr22, corl, huang2023leveraging, huang2023edge, simeonov2023se, pan2023tax, liu2023continual, jia2023seil, kim2023se, kohler2023symmetric, nguyen2023equivariant, nguyen2024symmetry, eisner2024deep, yang2024equivact, huorbitgrasp, gao2024riemann, lim2024equigraspflow}. Equivariant models, in particular, have proven effective for real-world robotic learning~\citep{corl22, rss22xupeng, zhu2023robot}, while other approaches leverage symmetry-aware representations to enable data-efficient open-loop pick-and-place policies~\citep{neural_descriptor, ryu2023equivariant, rss22haojie, huang2024fourier, huang2024imagination}. However, introducing symmetry into hierarchical behavior cloning agents remains unexplored. Our method improves the hierarchical policies by leveraging equivariance.