% This must be in the first 5 lines to tell arXiv to use pdfLaTeX, which is strongly recommended.
\pdfoutput=1
% In particular, the hyperref package requires pdfLaTeX in order to break URLs across lines.

\documentclass[11pt]{article}

% Change "review" to "final" to generate the final (sometimes called camera-ready) version.
% Change to "preprint" to generate a non-anonymous version with page numbers.
\usepackage[final]{acl}

% Standard package includes
\usepackage{times}
\usepackage{latexsym}

% For proper rendering and hyphenation of words containing Latin characters (including in bib files)
\usepackage[T1]{fontenc}
% For Vietnamese characters
% \usepackage[T5]{fontenc}
% See https://www.latex-project.org/help/documentation/encguide.pdf for other character sets

% This assumes your files are encoded as UTF8
\usepackage[utf8]{inputenc}

% This is not strictly necessary, and may be commented out,
% but it will improve the layout of the manuscript,
% and will typically save some space.
\usepackage{microtype}

% This is also not strictly necessary, and may be commented out.
% However, it will improve the aesthetics of text in
% the typewriter font.
\usepackage{inconsolata}

%Including images in your LaTeX document requires adding
%additional package(s)
\usepackage{graphicx}

\usepackage{algorithm}
\usepackage{algpseudocode}

% If the title and author information does not fit in the area allocated, uncomment the following
%
%\setlength\titlebox{<dim>}
%
% and set <dim> to something 5cm or larger.
\usepackage{enumitem}
\usepackage{subcaption}
\usepackage{amsmath,amsfonts,mathtools} 
\usepackage{booktabs}
\usepackage{float}
\usepackage{xcolor}
\newcommand{\ccr}[1]{\makecell{{\color{#1}\rule{1cm}{1cm}}}}
\usepackage{amsmath}
\newtheorem{theorem}{Theorem}
\newtheorem{lemma}{Lemma}
\newtheorem{proof}{Proof}[section]

\newcommand{\wy}[1]{[\textcolor{red}{wy: #1}]}

\title{Finite State Automata Inside Transformers with Chain-of-Thought:\\ A Mechanistic Study on State Tracking}

% Author information can be set in various styles:
% For several authors from the same institution:
% \author{Author 1 \and ... \and Author n \\
%         Address line \\ ... \\ Address line}
% if the names do not fit well on one line use
%         Author 1 \\ {\bf Author 2} \\ ... \\ {\bf Author n} \\
% For authors from different institutions:
% \author{Author 1 \\ Address line \\  ... \\ Address line
%         \And  ... \And
%         Author n \\ Address line \\ ... \\ Address line}
% To start a separate ``row'' of authors use \AND, as in
% \author{Author 1 \\ Address line \\  ... \\ Address line
%         \AND
%         Author 2 \\ Address line \\ ... \\ Address line \And
%         Author 3 \\ Address line \\ ... \\ Address line}

% \author{First Author \\
%   Affiliation / Address line 1 \\
%   Affiliation / Address line 2 \\
%   Affiliation / Address line 3 \\
%   \texttt{email@domain} \\\And
%   Second Author \\
%   Affiliation / Address line 1 \\
%   Affiliation / Address line 2 \\
%   Affiliation / Address line 3 \\
%   \texttt{email@domain} \\}
\renewcommand{\thefootnote}{\fnsymbol{footnote}}
\author{
 \textbf{Yifan Zhang\textsuperscript{1,2}\footnotemark[1]},
 \textbf{Wenyu Du\textsuperscript{3}},
 \textbf{Dongming Jin\textsuperscript{1,2}},
 \textbf{Jie Fu\textsuperscript{4}\footnotemark[2]},
 \textbf{Zhi Jin\textsuperscript{1,2}\footnotemark[2]}
\\
 \textsuperscript{1}Key Laboratory of High Confidence Software Technology (PKU), MOE, China
 \\
 \textsuperscript{2}School of Computer Science, Peking University, China
 \\
 \textsuperscript{3}The University of Hong Kong,
 \textsuperscript{4}Shanghai AI Lab
\\
 \texttt{yifanzhang@stu.pku.edu.cn, fujie@pjlab.org.cn, zhijin@pku.edu.cn}
}




\begin{document}
\maketitle
\footnotetext[1]{Work done during internship at Shanghai AI Lab.}
\footnotetext[2]{Corresponding author.}

\renewcommand{\thefootnote}{\arabic{footnote}}

\begin{abstract}

Chain-of-Thought (CoT) significantly enhances the performance of large language models (LLMs) across a wide range of tasks, and prior research shows that CoT can theoretically increase expressiveness. However, there is limited mechanistic understanding of the algorithms that Transformer+CoT can learn. 
In this work, we (1) evaluate the state tracking capabilities of Transformer+CoT and its variants, confirming the effectiveness of CoT. (2) Next, we identify the circuit—a subset of model components—responsible for tracking the world state, finding that late-layer MLP neurons play a key role. We propose two metrics, compression and distinction, and show that the neuron sets for each state achieve nearly 100\% accuracy, providing evidence of an implicit finite state automaton (FSA) embedded within the model. 
(3) Additionally, we explore three realistic settings: skipping intermediate steps, introducing data noise, and testing length generalization. Our results demonstrate that Transformer+CoT learns robust algorithms (FSA), highlighting its resilience in challenging scenarios.

\end{abstract}

\section{Introduction} \label{Introduction}

\begin{figure}[t] 
    \centering
    \includegraphics[width=\columnwidth]{image/motivation_v3.pdf}
    \caption{
    An illustration of one of the simplest state tracking problems, $\mathbb{Z}_2$. After training on data generated by transition rules of $\mathbb{Z}_2$, Transformer+CoT successfully recovers an implicit FSA by differentiating two states (\textcolor{orange}{$q_0$} and \textcolor{green}{$q_1$}) of the $\mathbb{Z}_2$ using two distinct sets of neurons in late-layer MLPs. 
    }
    \label{fig:motivation}
\end{figure}

Transformer-based large language models (LLMs) \citealp{touvron2023llamaopenefficientfoundation, openai2023gpt4} revolutionize natural language processing (NLP) by demonstrating significant progress across various tasks. 
However, they still face challenges with basic calculations \citep{zhou2023algorithmstransformerslearnstudy}, complex reasoning \citep{valmeekam2024llmscantplanlrms, han2024folionaturallanguagereasoning}, and regular languages \citep{bhattamishra-etal-2020-ability}. Approaches such as Chain-of-Thought (CoT) prompting \citep{wei2023chainofthoughtpromptingelicitsreasoning} and scratchpads \citep{nye2021workscratchpadsintermediatecomputation} address these limitations by generating intermediate reasoning steps. 
To understand the success of CoT, prior work has analyzed its expressiveness through the lenses of formal language theory and circuit complexity. 
Theoretical work \citep{zhang2024autoregressivechainthought, qiu2024askshallgiventuring, li2024chainthoughtempowerstransformers} demonstrates that incorporating a linear number of intermediate steps increases the expressive power of log-precision transformers\footnote{This paper refers to log-precision transformers; for simplicity, it uses ``transformers'' interchangeably.} \citep{10.5555/3666122.3668406}, enabling them to represent all \textbf{Finite State Automata (FSA)}, which are a foundational class of automata.

However, theoretical expressiveness indicates only upper and lower bounds on what an architecture can express; it does not guarantee successful learning during training. For instance, while recurrent neural networks (RNNs) are theoretically more expressive than transformers in Chomsky's computational hierarchy—where RNNs can handle regular languages and transformers are positioned lower \citep{delétang2023neuralnetworkschomskyhierarchy}—RNNs often fail to outperform transformers in practice. 
Consequently, some studies investigate the expressiveness of these architectures through the lens of learnability by measuring performance in language modeling. For example, \citet{liu2023transformerslearnshortcutsautomata} demonstrated that training transformers with recency-biased scratchpads improves sequential accuracy. However, even near-perfect next-token prediction accuracy does not imply that generative models reconstruct a true world model~\cite{vafa2024evaluatingworldmodelimplicit}. This raises a critical question: \textbf{Does CoT help Transformer recover a world model in the form of FSA, or do they merely learn shortcuts?}

To address this question, we extend the study of learnability beyond accuracy improvements, performing an internal mechanistic analysis of CoT's success. Specifically, we focus on \textit{state tracking}, a standard task for evaluating expressiveness \citep{merrill2024illusionstatestatespacemodels}. In state tracking, a sequence of updates modifies the world state, which is represented as an FSA. The goal is to determine the final state after applying all updates sequentially. State tracking is a core capability of generative models and underpins many downstream tasks—such as entity tracking \citep{kim-schuster-2023-entity}, chess \citep{toshniwal2022chesstestbedlanguagemodel}, and map navigation \citep{liu2023agentbenchevaluatingllmsagents}. Figure \ref{fig:motivation} illustrates \(\mathbb{Z}_2\)\footnote{The state tracking problem \(\mathbb{Z}_2\) is equivalent to parity, a formal language describing binary sequences with specific evenness or oddness properties.}, one of the simplest state tracking problems, along with its corresponding FSA and transition rules. 

We begin by comprehensively evaluating the state tracking capabilities of Transformer+CoT, comparing it with other models (RNNs), transformer variants (e.g., those with recurrence \citep{fan2021addressinglimitationstransformersfeedback, 9878733}), and CoT variants (e.g., implicit CoT \citep{goyal2024thinkspeaktraininglanguage}). Empirically, we show that Transformer+CoT is the only model capable of efficiently learning state tracking for sequences of arbitrary lengths across three groups: $\mathbb{Z}_{60}$, $A_4 \times \mathbb{Z}_5$, and $A_5$, in both in-domain and out-of-distribution settings. 

Next, to provide a mechanistic explanation for this success, we apply interpretability techniques to analyze the algorithms learned by Transformer+CoT. Using activation patching \citep{10.5555/3495724.3496763}, we identify the circuits (specific model components) responsible for state tracking and observe that Transformer+CoT relies heavily on late-layer MLP neurons. These neurons can be effectively grouped into states based on transition rules. To quantify this, we introduce two metrics: compression and distinction. Compression measures the similarity of representations for the same state under different input prompts, while distinction quantifies the separation between different states, even when their inputs are similar. We find nearly 100\% accuracy on both metrics at every intermediate step, providing strong evidence that the model reconstructs the world model (i.e., FSA). 
For instance, in Figure \ref{fig:motivation}, Transformer+CoT compresses inputs corresponding to two states (i.e. \textcolor{orange}{$q_0$} and \textcolor{green}{$q_1$}) by activating two distinct sets of neurons. 
% {Transformer+CoT effectively encodes inputs that correspond to the same state by selectively activating a distinct subset of neurons. Simultaneously, it maintains clear separability by ensuring that this subset of neurons is uniquely activated for different states, thereby preserving state-specific representations.}
Additionally, we observe shifts in attention patterns as the number of intermediate steps increases. 

To evaluate robustness in real-world scenarios, we test Transformer+CoT under three challenging settings: skip-step reasoning, noisy scratchpads, and length generalization. Our results show that Transformer+CoT learns robust algorithms even in noisy environments, indicating that the underlying FSA is resilient. This robustness supports the implementation of state tracking in more complex tasks, providing a theoretical foundation for downstream applications.

In summary, this work is the first to extend the study of learnability and expressiveness through mechanistic interpretation, uncovering the underlying algorithms of Transformer+CoT. Our contributions are as follows: 

\begin{enumerate}[noitemsep]
    \item We conduct a comprehensive evaluation of the state tracking capabilities of Transformer+CoT, demonstrating its unique ability to track states of arbitrary lengths across multiple groups ($\mathbb{Z}_{60}$, $A_4 \times \mathbb{Z}_5$, and $A_5$) in both in-domain and out-of-distribution settings.
    \item Using interpretability techniques, including activation patching, we analyze the learned algorithms in Transformer+CoT. We identify the activation of late-layer MLP neurons and classify them into states based on transition rules, achieving nearly 100\% accuracy in metrics of compression and distinction, which confirms the model's reconstruction of the world model (FSA).
    \item We explore Transformer+CoT in three challenging settings and find that it learns resilient algorithms capable of effective state tracking in noisy conditions. 
\end{enumerate}





% Theoretical work has placed transformers at the lower end of Chomsky’s computational hierarchy, imposing limitations on computational abilities \citep{delétang2023neuralnetworkschomskyhierarchy}. 
% In contrast to recurrent neural networks \citep{10.5555/553011, 6795963} (RNNs and LSTMs), autoregressive transformers trade recurrence for parallelism \citep{zhang2024autoregressivechainthought}. 
% \citet{merrill2023parallelismtradeofflimitationslogprecision} point out that transformers struggle to express inherently sequential computation, including state tracking. 
% Another line of work \citep{zhang2024autoregressivechainthought, qiu2024askshallgiventuring, merrill2024expressivepowertransformerschain, li2024chainthoughtempowerstransformers} refers to chain-of-thought to extend the expressive power of transformers. 
% Theoretically, chain-of-thought can enable a single finite-size Transformer to be computational universal. 

% Recently, \citet{merrill2024illusionstatestatespacemodels} show that transformers and state-space models (SSMs) \citep{gu2022efficientlymodelinglongsequences, gu2024mambalineartimesequencemodeling} can only express state tracking within the complexity class L-uniform $TC^0$. 
% On the other hand, \citet{merrill2024expressivepowertransformerschain, li2024chainthoughtempowerstransformers} point out that transformers with $O(n)$ chain-of-thought intermediate steps can express any finite state automata. 
% Existing work mainly analyzes the limitations of transformers from a theoretical perspective (algebraic formal language theory and circuit complexity), thus necessitating the significance of chain-of-thought, yet there is a lack of mechanistic interpretation on what algorithm transformers with chain-of-thought learn in terms of state tracking, which is not only related to AI security, but also establishes a theoretical underpinning for state tracking in reasoning and search tasks. 


\section{Related Work} \label{Background&Related Work}


% However, automata play a fundamental role in evaluating the sequential computational ability of models. 
% (Automata, particularly finite state automata (FSA), serve as a compact and mathematically grounded framework for representing sequential processes.They provide an essential benchmark for understanding how models handle and internalize structured, rule-based transitions—a quality central to many applications such as natural language processing, speech recognition, and control systems.)
% (Finite-state automata (FSA), in particular, provide a mathematical abstraction for capturing sequential dependencies, making them a crucial tool in understanding the capabilities of sequence models. They have been extensively used in formal language theory, natural language processing, and cognitive science, where they help analyze the expressiveness and generalization properties of learning systems.)
% Automata, particularly FSA, serve as a mathematical abstraction for capturing sequential dependencies and decision processes, offering insights into the underlying structure of learned representations in neural networks \citep{Hopcroft1979introduction}. 
% And automata theory \citep{Jurafsky2000speech} provides a rigorous framework for modeling and analyzing systems that exhibit state-dependent transitions, making it widely applicable in natural language processing, formal verification, and cognitive modeling.
% Recurrent neural networks (RNNs) have been extensively studied in this context, with prior research \citep{Siegelmann1995computational, Elman1990structure} demonstrating that RNNs can recover FSA from sequential data, effectively modeling complex transition dynamics. 
% However, despite the growing success, whether Transformer models augmented +CoT can similarly recover FSA remains an open question. 
% This gap in understanding motivates our investigation into whether Transformer models augmented +CoT can effectively learn FSA-like structures, focusing specifically on state tracking.

\subsection{FSA and State Tracking}

% To better investigate state tracking capability of transformers, it is necessary to give a formal definition first. 
% We follow the previous definition \citep{merrill2024illusionstatestatespacemodels, grazzi2024unlockingstatetrackinglinearrnns}, where the world model behind state tracking is a (deterministic) finite state automaton. 

% We use the standard definition of finite state automata, which is a tuple $\mathcal{A} = (\Sigma, Q, q_0, \delta)$ where $\Sigma$ is a finite set of characters, $Q$ is a finite set of states, $q_0 \in Q$ is the start state, and $\delta : Q \times \Sigma \to Q$ is the state-transition function mapping a state and character to the next state.
% Following the formal definition in \citet{merrill2024illusionstatestatespacemodels}, state tracking can be expressed as a word problem on a finite monoid formally. 
% Given a monoid $(M, \cdot)$ ($M$ is the set and $\cdot$ is the associative operation), we want to reduce words $m_1 \ldots m_n \in M^*$, describing the sequence of updates, to their product $m_1 \cdot m_2 \cdot \ldots \cdot m_n \in M$, representing the state of the system after the updates. 
% If $M$ is finite, there is a corresponding finite state automaton $(M, M, e, \delta)$ that solves the word problem, where the starting state is $e$ (the identity element), and the transition function is $\delta(m_1, m_2) = m_1 \cdot m_2$ for $m_1, m_2 \in M$. 
% As a generative model, transformers with chain-of-thought generate state sequences $q_1 \ldots q_n \in M^*$, given input sequences $m_1 \ldots m_n \in M^*$ , where $q_i$ equals the state of the system at $i\text{-th}$ step. 
% In this work, we focus on word problems on groups (monoids with inverses), specifically, on the cyclic group $Z_m$, i.e. addition modulo $m$, and the symmetric group $S_m$, i.e. the group of permutations on $m$ elements. \wy{paraphrase}

% We adopt the conventional definition of a finite state automaton \citep{Hopcroft1979introduction}, which is formally represented as a tuple $\mathcal{A} = (\Sigma, Q, q_0, \delta)$. Here, $\Sigma$ denotes a finite alphabet, $Q$ is a finite set of states, $q_0 \in Q$ is the initial state, and $\delta: Q \times \Sigma \to Q$ defines the transition function that maps a given state and input symbol to a subsequent state.
% According to the formalism presented in \citet{merrill2024illusionstatestatespacemodels, grazzi2024unlockingstatetrackinglinearrnns}, state tracking can be framed as solving a word problem over a finite monoid. Specifically, given a monoid $(M, \cdot)$, where $M$ represents a set equipped with an associative operation $\cdot$, the objective is to compute the product $m_1 \cdot m_2 \cdot \ldots \cdot m_n \in M$ from a sequence of elements $m_1 \ldots m_n \in M^*$, which corresponds to the system’s state after processing the updates sequentially. When $M$ is finite, this computation can be performed by a corresponding finite state automaton $(M, M, e, \delta)$, where the identity element $e$ serves as the initial state, and the transition function is defined as $\delta(m_1, m_2) = m_1 \cdot m_2$ for all $m_1, m_2 \in M$.
% In the generative modeling setting, transformers augmented with chain-of-thought generate state sequences $q_1 \ldots q_n \in M^*$ based on input sequences $m_1 \ldots m_n \in M^*$. Here, each generated state $q_i$ corresponds to the system’s state at the $i$-th step. Our work specifically investigates word problems over groups, which are monoids with inverses, focusing on two particular structures: the cyclic group $\mathbb{Z}_m$, characterized by addition modulo $m$, and the symmetric group $S_m$, which represents the set of all permutations on $m$ elements.

We adopt the conventional definition of a finite state automaton (FSA) as a tuple $\mathcal{A} = (\Sigma, Q, q_0, \delta)$, where $\Sigma$ is the alphabet, $Q$ is a set of states, $q_0$ is the initial state, and $\delta$ is the transition function\citep{Hopcroft1979introduction}. State tracking can be framed as solving a word problem over a finite monoid $(M, \cdot)$, where the objective is to compute the product $m_1 \cdot m_2 \cdot \ldots \cdot m_n \in M$\citep{merrill2024illusionstatestatespacemodels}. This computation can be performed by a finite state automaton. As generative models, transformers augmented with chain-of-thought generate state sequences $q_1 \ldots q_n \in M^*$ based on input sequences $m_1 \ldots m_n \in M^*$. Our work focuses on word problems over groups, specifically the cyclic group $\mathbb{Z}_m$ and the symmetric group $S_m$.

\subsection{Mechanistic Interpretability} 
The use of mechanistic interpretability \citep{rai2024practicalreviewmechanisticinterpretability, ferrando2024primerinnerworkingstransformerbased} to explain LLMs is an emerging research direction. 
This approach employs various techniques, including logit lens\citep{geva2021transformerfeedforwardlayerskeyvalue}, probing\citep{gurnee2023findingneuronshaystackcase}, causal mediation analysis\citep{wang2022interpretabilitywildcircuitindirect}, sparse autoencoders\citep{cunningham2023sparseautoencodershighlyinterpretable}, and visualization\citep{cooney2023circuitsvis}, to identify and analyze the features and circuits of LLMs. 
% \citet{nikankin2024arithmeticalgorithmslanguagemodels} provide a high-resolution understanding of the mechanism that LLMs use to answer arithmetic prompts using a set of techniques including activation patching, logit lens, etc. 
% And \citet{saparov2024transformersstrugglelearnsearch} designs a novel mechanistic interpretability technique to reconstruct the computational graph, through utilizing activation patching. 
% In this work, we aim to provide a mechanistic interpretation on the algorithm transformers with chain-of-thought have learned, to determine whether and to what extent the model is utilizing a correct algorithm to keep track of world state, rather than based on heuristics. 

\section{Evaluating State Tracking Capability Across Architectures} \label{sec:model performance}

% Previous work \citep{liu2023transformerslearnshortcutsautomata, merrill2024illusionstatestatespacemodels, grazzi2024unlockingstatetrackinglinearrnns} mainly model the state tracking problem as token-tagging task, where the sequence-to-sequence neural network (encoder) maps the input sequences $m_1 \ldots m_n \in M^*$ to corresponding state sequences $q_1 \ldots q_n \in M^*$. 
% However, the dominant large language models accept transformers as generative model (decoder), and whether transformers with chain-of-thought can accomplish this task remains a subject of further investigation. 

Besides standard transformer and CoT, there are various theoretical works \citep{zhang2024autoregressivechainthought, fan2024loopedtransformerslengthgeneralization, 9878733, fan2021addressinglimitationstransformersfeedback} attempting to inject recurrence into transformers, while another line of work \citep{goyal2024thinkspeaktraininglanguage, hao2024traininglargelanguagemodels} proposing modifications of chain-of-thought (implicit chain-of-thought in contrast to explicit chain-of-thought). 
In this section, we will explore the state tracking capability with an empirical lens: can standard transformer with/without CoT and these variants successfully learn state tracking?

\paragraph{Dataset: $A_5,A_4 \times \mathbb{Z}_5, \mathbb{Z}_{60}.$}

Following the formal definition of state tracking in \citep{merrill2024illusionstatestatespacemodels, grazzi2024unlockingstatetrackinglinearrnns}, we model state tracking as word problems, and consider three kinds of groups with increasing difficulty: $\mathbb{Z}_{60}$, an abelian group encoding mod-60 addition, $A_4 \times \mathbb{Z}_5$, a non-abelian but solvable group, which is a direct product group of one alternating group $A_4$ (a subgroup of the symmetric group $S_4$ containing only even permutations) and one cyclic group $\mathbb{Z}_5$, and $A_5$, the alternating group on five elements, which is the smallest non-solvable subgroup. 
With the same number of elements 60, the three groups belong to $TC^0$, $TC^0$, and $NC^1$-complete respectively, with varying difficulty mainly deriving from the complexity of learning the group multiplication operation.

\paragraph{Architectures.}

We choose a GPT2-like \citep{radford2019language} transformer with chain-of-thought (denoted as Transformer+CoT), choose the realistic setting with a bounded number of layers, and log-precision. 
To compare Transformer+CoT  with other models, we also consider recurrent neural networks (RNNs \citealp{10.5555/553011} and LSTMs \citealp{6795963}), S4 \citep{gu2022efficientlymodelinglongsequences}, Mamba \citep{gu2024mambalineartimesequencemodeling}, implicit chain-of-thought: transformers with pause (denoted as Pause) \citep{goyal2024thinkspeaktraininglanguage} and other variants of transformers: standard recurrent transformer (denoted as Recurrent) \citep{9878733}, looped transformer (denoted as Looped) \citep{fan2024loopedtransformerslengthgeneralization}. 
In order to disentangle recursion introduced by CoT from other variants, we classify different models into encoder and decoder and adopt the Token-Tagging task (TT) and Language Modeling task (LM), respectively.
Only Transformer+CoT and implicit CoT (Pause) are designed as decoders, while other models or variants encoder. 
The reason we design some models as encoders rather than decoders without CoT is that, we use the same labels whether the model is an encoder or decoder, in order to eliminate the influence of labels on supervised training, like the hint setting in \citep{li2024chainthoughtempowerstransformers}.
So that we can contribute the performance improvement to recurrence in the structure for variants without chain-of-thought.
Model details refer to Appendix~\ref{appendix models}. 
% The reason we design some models as encoder, 
% In addition, to eliminate the influence of labels on supervised training, we use the same labels whether the model is an encoder or decoder.
% 补充为什么设计成encoder的citation: like the hint setting in \citep{li2024chainthoughtempowerstransformers}
% All models and details are listed in Table~\ref{tab:model_complexities}.

% \subsection{Time and Depth Complexity}

% Theoretical work \citep{merrill2024illusionstatestatespacemodels, liu2023transformerslearnshortcutsautomata} combines circuit complexity and algebraic formal language theory to divide the model and state tracking tasks into different complexity classes $TC^0, NC^1$. 
% And \citet{merrill2024illusionstatestatespacemodels} point out that $NC^1$-complete word problems can not be expressed by $TC^0$ models. 
% This work, we employ two complexity metrics: time complexity and depth complexity \citep{zhang2024autoregressivechainthought} to measure different variants' sequential computation capability. 
% In contrast to time complexity, which corresponds to the cumulative count of computational steps, depth complexity corresponds to the longest chain of dependent steps, and evaluates the number of sequential steps.
% It is worth noting that higher time complexity does not necessarily imply an increase in depth complexity (e.g. Pause). 
% According to the definition, $NC^1$ is the class of problems solvable by uniform Boolean circuits with depth $O(logN)$ and polynomial size in the input length $N$, and $NC^1$-complete word problems (e.g. $A_5$) can not be expressed by models with $O(1)$ depth complexity in log-precision. 
% Different models' time and depth complexity are listed in Table~\ref{tab:model_complexities}. 

\paragraph{Performance (In-Domain).} \label{Performance in Distribution}

We hold a comprehensive evaluation of models' state tracking capability on word problems, with the same model depth and dimension. 
We fix the same number of layers and same model dimension for all models of log-precision.
We use sequence accuracy as a metric, where the generated sequence is true only when same to ground truth sequence. 
Figure~\ref{fig:model_accuracy} gives different models' performance across different input sequence length and different groups. 
We draw several conclusions:

\begin{enumerate}[noitemsep]
    \item Consistent with theoretical study \citep{liu2023transformerslearnshortcutsautomata, merrill2024illusionstatestatespacemodels}, transformers and state-space models can not express arbitrary length $A_5$ word problems, in contrast to RNN and LSTM.
    \item $O(N)$ intermediate steps extend the expressive power of transformers. 
    In particular, Transformer+CoT can empirically converge to a nearly perfect generative model on word problems of arbitrary length, which allows for parallel training and a smoother flow of gradients like Transformer due to no architecture change.
    % \item which allows for parallel training and a smoother flow of gradients like Transformer due to no architecture change.
    However, other variants of transformers (Pause, Looped, and Recurrent) fail to converge on longer sequence lengths, despite postponing the accuracy drop to longer lengths compared to the standard transformer.
    \item Expressiveness doesn't equal learnability. 
    Although $A_4 \times \mathbb{Z}_5$ belongs to $TC^0$, it can not be learned by models with circuit complexity $TC^0$ on sequences of arbitrary length. 
    % Moreover, although some models (RNN, Recurrent and Looped) are theoretically capable of expression, they exhibit instability or even failure during training on longer word problems. 
    \item Transformer+CoT achieve a dual win in both expressiveness and learnability. 
    The model not only achieves higher expressiveness than the vanilla transformer but also successfully converges to sequences that are 100 times the number of layers.
\end{enumerate}

\begin{figure*}[ht] 
    \centering
    \includegraphics[width=\textwidth]{image/model_performance_v3.pdf}
    \caption{Model accuracy across sequence lengths for $\mathbb{Z}_{60}, A_4 \times \mathbb{Z}_5 $ and $A_5$.}
    \label{fig:model_accuracy}
\end{figure*}

\paragraph{Performance (Out-of-Distribution).} \label{subsec: ood}

\citep{liu2023transformerslearnshortcutsautomata} analyze transformers' failure in out-of-distribution of parity, and argue that transformers learn a shortcut solution, which compute the parity by counting the number of 1s in the input sequences and compute mod-$2$, thus failing to generalize to sequences with unseen sum of 1s.
\citep{zhang2024autoregressivechainthought} discuss the role of chain-of-thought in computability, and point that CoT simulates the recurrent connection by iteratively encode and decode back and forth between states and tokens. 
\emph{The key of Transformer+CoT's expressiveness on state tracking is that previous computation can be retrieved, through concatenating previous step state to the end of scratchpad}.

To investigate whether Transformer+CoT learns an algorithm based solely on the input sequences $m_1 \ldots m_n$ or combines input and scratchpad as theoretical work expects, We divide the elements of the three groups into proper subsets.
% The designed distribution shift experiments train the model on sequences where each input sequence $m_1 \ldots m_n$ belongs to a specific proper subset, and different sequences may belong to different subsets.
% The evaluation is conducted on the sequences sampled from the full set.
% The designed distribution shift experiments train the model on sequences, where each sequence $m_1 \ldots m_n$ belongs to any proper subset, with different sequences potentially belonging to different subsets. 
% Evaluation is performed on sequences $m_1 \ldots m_n$ sampled from the full set.
% We set up the following training set: $m$ belongs to one proper subset for one sequence, but different sequences may belong to different subsets, and evaluation is performed on the full set.
And we train the model on sequences with $m$ belonging to one proper subset, but evaluate on the full set. 
We stress that, through restricting the input sequences into separate subsets, the possible state sequences remain the same, for the reason that any subset can express the whole group through group operation.
If the model learns a shortcut solution, that attends to only input, it can not generalize to sequences with group elements sampled from the full set, because the model has not seen mixed input sequences in the training set.
As outlined in Section~\ref{Performance in Distribution}, both LSTM and Transformer+CoT successfully learn all word problems in-distribution, and we test their performance in out-of-distribution.
Results in Figure~\ref{fig:model_accuracy} show that Transformer+CoT achieves perfect generalization on three groups $\mathbb{Z}_{60}, A_4 \times \mathbb{Z}_5, A_5$ in contrast to LSTM, implying that the model attends to not only input but also scratchpad.
We provide more experimental settings in Appendix~\ref{groups}.
The out-of-distribution performance eliminates the possibility that the model learns specific shortcuts similar to those \citet{liu2023transformerslearnshortcutsautomata} found on $Z_2$ group, but this does not rule out the possibility that other shortcuts exist, which necessitates an interpretation on the mechanism.

\section{Mechanism Inside Transformer+CoT: FSA} \label{sec:mechanism}

Both transformers with chain-of-thought and recurrent neural network achieve perfect performance within distribution, and the former even generalize well in out-of-distribution. 
Due to the transformer's black-box nature, the mechanism behind its implementation of state tracking remains unknown, and we cannot answer the questions that what algorithm the model has learned to keep track of world state, and to what extent the model can generalize. 
Considering that the word problems involves a series of state transitions, the model's state computation is dynamic and consecutive while generating intermediate steps. 
In this section, we try to analyze the circuit transformers with chain-of-thought use to keep track of world state first, and then hold deep component analysis to interpret the mechanism.

\subsection{Circuit Localization} \label{subsec: circuit}

To understand the mechanism of state tracking, we will first localize the circuit (a subset of components) using activation patching \citep{10.5555/3495724.3496763}. 
We format the word problems as prompt $p_i = m_1 \ldots m_n | q_i \ldots q_{i-1}$ and result state token $q_i$ at $i\text{-th}$ step.
At each intermediate step, we sample a prompt $p_i$ with its corresponding result state $q_i$, and then sample a counterfactual prompt $p_i'$, resulting in a different result state $q_i'$. 
We hold intervention experiments, by replacing the activation of a specific MLP layer or attention head with the pre-computed for $p_i'$, and then assessing how this impacts the probabilities of answer tokens. 
To access the importance of one component at $i\text{-th}$ step, we average the following intervention effect (IE) metric \citep{nikankin2024arithmeticalgorithmslanguagemodels} across all prompts, which is the mean of impacts of $q_i$ and $q_i'$: 
\[
    \text{IE}(q_i, q_i') = 
    \frac{1}{2} \left[ 
        \frac{\text{P}^*(q_i') - \text{P}(q_i')}{\text{P}(q_i')} 
        + \frac{\text{P}(q_i) - \text{P}^*(q_i)}{\text{P}^*(q_i)} 
    \right]
\]
where $\text{P}$ and $\text{P}^*$ are the pre- and post-intervention probability distributions. 
% Having identified the component of the circuit, we have to know in which position the model promotes the correct answer token. 
% To understand in which position the model promote the correct answer, we hold linear probing in each layer at each position. 
% We train a linear classifier at each position and layer, which takes corresponding model activation as input and generates a probability distribution over all group elements. 
% The classifier's performance is then assessed on a separate test set, measuring to what extent the correct answer can be derived from the output representation at the layer and position. 

We localize the circuit Transformer+CoT use to keep track of world state at each intermediate step in A5 word problems.
As Figure~\ref{fig:ie&probe} shows, we find that the circuit across each intermediate step mainly consists of MLPs, with attention heads few impacts.
And across each intermediate step, the circuit has hardly changed, that is, the first MLP (MLP0 \citealp{mcdougall2023copysuppressioncomprehensivelyunderstanding}) in position $m_i$ and late-layer MLPs in last position play a significant role in state tracking at the $i\text{-th}$ step.
% Moreover, at the last position, the late-layer MLPs promote the correct world state into the residual stream, resulting in successful state transitions.
% For example, according to the results in Figure~\ref{fig:ie&probe} at $4\text{-th}$ step, we can know that in the last position $q_4$, the late-layer MLPs promote the correct world state in the residual stream. 
Above all, given input sequences $m_1 \ldots m_n \in M^*$ concatenated with scratchpad $q_1 \ldots q_{i-1} \in M^*$, the MLPs mainly implement state transition, and the late-layer MLPs in the last position $q_{i-1}$ promote the correct token $q_i$. 

\begin{figure} [ht]
    \centering
    \includegraphics[width=1.0\linewidth]{image/ie_v2.pdf}
    \caption{Activation patching results for A5 word problems of length 10 at $4\text{-th}$ step, with the prompt $p_5 = m_1 \ldots m_{10} | q_1 \ldots q_4$ and ground truth $q_5$.}
    \label{fig:ie&probe}
\end{figure}

% \begin{figure*}[t]
%  \centering
%     \begin{subfigure}[b]{0.555\textwidth}
%         \centering
%         \includegraphics[width=\textwidth]{image/ie_v2.pdf}
%         \caption{Activation patching results in log scale.}
%         \label{fig:ie}
%     \end{subfigure}
%     \hspace{0pt} % Adjust this value to control the space between the images
%     \begin{subfigure}[b]{0.430\textwidth}
%         \centering
%         \includegraphics[width=\textwidth]{image/probe.pdf}
%         \caption{Probing results.}
%         \label{fig:probe}
%     \end{subfigure}
%     \caption {An example for A5 word problem of length 10 at $4\text{-th}$ step, with the prompt $p_5 = m_1 \ldots m_{10} | q_1 \ldots q_4$ and ground truth $q_5$.
%     (a): MLP0 at $m_5$ and late-layer MLPs at $q_4$ mainly compose the circuit. 
%     (b): Late-layer MLPs implement the state transition.
%     }
%     \label{fig:ie&probe}
% \end{figure*}

\subsection{MLP Neuron Analysis} \label{subsec: MLP Neuron Analysis}

Having identified the circuit, we then hold deeper component analysis on how late-layer MLPs implement state tracking. 
\citet{geva-etal-2022-transformer} point out that the MLP contributes additive updates to the residual stream, which can be further decomposed into weighted collections of sub-updates. 
In particular, given input $x^l$ at layer $l$, MLP can be expressed using the parameters $K^l, V^l \in \mathbb{R}^{d_{mlp} \times d_m}
$, where $d_{mlp}$ is the MLP intermediate dimension and $d_m$ is the model dimension.
Additionally, a non-linear activation function $f$ is applied:
\[MLP^l(x^l) = f(K^lx^l)V^l \]
Expanding this further, it can be decomposed as: 
\[MLP^l(x^l) = \sum_{j=1}^{d_{mlp}} f(x^l \cdot k_j^l) v_j^l = \sum_{j=1}^{d_{mlp}} m_{j}^l v_j^l
\]
where $k^l_j \in \mathbb{R}^d$ and $v^l_j \in \mathbb{R}^d$ correspond to the $j\text{-th}$ row vectors of $K^l$ and $V^l$, respectively. 
The scalar $m_{j}^l = f(x^l \cdot k_j^l)$ represents the activation coefficient associated with the neuron $v_j^l$. 
Notably, when mapped into the vocabulary space, these individual sub-updates $m_{j}^l v_j^l$ can be interpreted in a human-understandable manner \citep{geva-etal-2022-transformer}.
% \citet{nikankin2024arithmeticalgorithmslanguagemodels} find that the top MLP neurons in Llama3 act as key-value memories in handling arithmetic tasks, where activation patterns correspond to numerical heuristics, and the associated value neurons promote corresponding numbers.
% Based on results in Section~\ref{subsec: circuit}, we hypothesize that: in state tracking, the activation pattern corresponds to the state transition rule, where top-$K$ neurons get activated on a prompt subset deducing to the same state, and the activated neurons promote the corresponding state. 

% Regarding MLP neurons as separate components, we run similar activation patching experiments as in \ref{subsec: circuit}. 
% Across all prompts, we calculate different late-layer MLP neurons' intervention effects at last position. 
% Like one sampled MLP in Figure~\ref{fig:mlp}, the distribution of intervention effects of MLP neurons exhibits an excessive number of peaks. 
We first group all possible prompts into prompt subsets according to the resulting world state. 
Specifically, any prompt $m_1 \ldots m_n | q_i \ldots q_{i-1}$ at $i\text{-th}$ step in one subset should reduce to the same state $q_i$ under group operation (for simplicity, denoted as $\textbf{q}$ prompt subset\footnote{We use $\textbf{q}$ to represent $q_i$}). 
On each $\textbf{q}$ prompt subset, we calculate the contribution of each late-layer MLP to $\textbf{q}$ in the layer representation using logit lens \citep{geva2021transformerfeedforwardlayerskeyvalue} and average on all intermediate steps.
The results in Figure~\ref{fig:increase} show that, among late-layer MLPs, the last three layers of MLP play a significant role and MLP11 mainly implements the state transition, accounting for the 72\% logit increase.

\begin{figure}[ht]
  \includegraphics[width=1.05\columnwidth]{image/increase_v2.pdf}
  \caption{
  Average state $\textbf{q}$ logit increase in the layer representation for different late-layer MLPs.
  }
  \label{fig:increase}
\end{figure}

% Then we hold causal mediation analysis of late-layer MLP neurons in last position on $\textbf{q}$ prompt subset, but with modified intervention effects \footnote{\(\text{IE}(\textbf{q}) =  \frac{\text{P}(\textbf{q}) - \text{P}^*(\textbf{q})}{\text{P}^*(\textbf{q})}\)}, for the reason that we only consider the impact of state $\textbf{q}$.
% Interestingly, the intervention effect distribution has only a few prominent peaks, which means that only several top-$K$ neurons play significant role in updating the world state to $\textbf{q}$; 
% an example for layer $l=11$ in $\textbf{q}=0$ prompt subset is shown in Figure~\ref{fig:mlpwrtstate}.

% \citet{nikankin2024arithmeticalgorithmslanguagemodels} find that the top MLP neurons in Llama3 \citep{grattafiori2024llama3herdmodels} act as key-value memories in handling arithmetic tasks, where keys (coefficient) correspond to numerical patterns, and the associated value neurons promote correct numbers. 
% Based on this, we hypothesize that: 
% (i) in state tracking, the activation pattern corresponds to the state transition rule, and a top-$K$ neuron gets activated on a prompt subgroup, where all the prompts deduce to the same state. 
% (ii) the associated neuron promote the correct state according to the state transition rules. 

To analyze what the MLP neurons' activation pattern is, we divide the prompts into different ($m_i,q_{i-1}$) pairs (for simplicity, ($m, q$) pairs\footnote{We use $q$ to represent $q_{i-1}$.}), where $m_i$ is the $i\text{-th}$ input element, and $q_{i-1}$ is the previous world state. 
We analyze the activation pattern of MLP neurons in layer $l$ across all ($m, q$) pairs. 
Interestingly, late-layer MLP neurons get activated only on a specific subset of all ($m, q$) pairs (denoted as \emph{predictions}). 
More specifically, the top 60 ($m,q$) pairs are distributed across each row or column. 
Interestingly, according to the transition rules, there are 60 ($m,q$) pairs deducing to state $\textbf{q}$ (corresponding to $\textbf{q}$ prompt subset), distributed across each row and column (denoted labels). 
Based on this, we can ask the following questions: 
\begin{itemize}
    \item Q1: Does the activation pattern correspond to a bag of MLP neurons being activated on the $\textbf{q}$ prompt subset, which deduces to state $\textbf{q}$ according to transition rules?
    \item Q2: Can we classify the MLP neurons according to the resulting states? 
\end{itemize}




% \begin{algorithm}
% \caption{Classify MLP Neurons at Layer $l$}
% \begin{algorithmic}[1]
% \State \textbf{Input:} Prior transition rules, activation coefficients $m_j^l$, value vector $v_j^l$, threshold $\sigma$
% \State \textbf{Output:} Classified neurons with F1 score $\geq \sigma$

% \For{each possible resulting state}
%     \State Compute labels based on priori transition rules
% \EndFor

% \For{each neuron in layer $l$}
%     \State Measure $m_j^l$ for all $(m, q)$ pairs
% \EndFor

% \For{each token embedded in $v_j^l$}
%     \State Calculate logits of the token using logit lens
%     \State Convert logits into a 2D pattern for each $(m, q)$ pair
% \EndFor

% \For{each pair $(m, q)$}
%     \State Multiply the activation coefficient $m_j^l$ with the corresponding logit from the 2D pattern
%     \State Store the result as the effective logit contribution for each $(m, q)$ pair
% \EndFor

% \State Sort the effective logit contributions in descending order
% \State Select the top 60 $(m, q)$ pairs as predictions

% \For{each pair $(m, q)$ in predictions}
%     \State Compare the predictions with the labels
%     \State Compute the F1 score between the prediction and label
% \EndFor

% \For{each neuron}
%     \If{F1 score $\geq \sigma$}
%         \State Classify the neuron to the corresponding state
%     \Else
%         \State Do not classify the neuron
%     \EndIf
% \EndFor

% \end{algorithmic}
% \end{algorithm}


% \begin{figure*}[t]
%  \centering
%     \begin{subfigure}[t]{0.588\textwidth}
%         \centering
%         \includegraphics[width=\textwidth]{image/mlp wrt state.pdf}
%         \caption{}
%         \label{fig:mlpwrtstate}
%     \end{subfigure}
%     \hspace{0pt} % Adjust this value to control the space between the images
%     \begin{subfigure}[t]{0.392\textwidth}
%         \centering
%         \includegraphics[width=\textwidth]{image/pattern.pdf}
%         \caption{}
%         \label{fig:pattern}
%     \end{subfigure}
%     \caption {
%     (a): Activation patching results of MLP on one sampled prompt subset. 
%     The y-axis is the intervention effect, and the x-axis is the MLP neuron index.
%     (b): Activation pattern. 
%     The y-axis is the previous state, and the x-axis is $i\text{-th}$ group element. 
%     Each grid color represents the scalar of the coefficient, and the darker, the bigger.
%     }
%     \label{fig:wrtstate}
% \end{figure*}
To answer the questions, we hold neurons classification experiments (with the detailed procedures provided in Appendix~\ref{appendix algorithm}) to classify all MLP neurons at layer $l$.
Through this experiment, we successfully mapped the MLP11, MLP10 and MLP9 neurons to states with successful ratio 90.0\%, 79.6\% and 38.7\%.
Furthermore, nearly 15.5\% MLP11 neurons even have perfect f1 score with the ground truth subset of ($m, q$) pairs, meaning the neuron to state mapping  N to 1. 
After classifying neurons into states, we can compute the accuracy and recall of the activated neurons, refer to Appendix~\ref{precision and recall} for details.


For Q1, most neurons have high f1 score with ground truth subset of ($m, q$) pairs of state $\textbf{q}$, which means that the neurons get activated on $\textbf{q}$ prompt subset, which deduce to state $\textbf{q}$ according to transition rules. 
For Q2, we can classify all activated neurons at layer $l$ across intermediate steps, and even 15.5\% MLP11 neurons satisfy N to 1 mapping.
We can conclude that transformers with chain-of-thought keep track of world state through a bag of late-layer MLP neurons, which get activated in specific subset of ($m, q$) pairs, and promote correct state token $\textbf{q}$ to the residual stream. 
Moreover, a correct state transition is achieved through the collective sub-updates of multiple neurons, that belong to the state $\textbf{q}$. 
Beyond the late-layer MLPs, we also analyze another significant component of the circuit, MLP0, which primarily achieves effective embedding of $m_i$ for subsequent state transitions. 
Further details can be found in Appendix~\ref{appendix mlp0}.



\subsection{Transformer+CoT Recovers FSA } \label{subsec: automata}

Nearly perfect in next-token prediction does not necessarily imply that the generative model has reconstructed the world model to the same extent, for example, cumulative Connect-4 in \citep{vafa2024evaluatingworldmodelimplicit}.
In terms of state tracking, the world model FSA can compress different sequences as long as they deduce to the same state, and distinguish sequences as long as they deduce to different states, no matter how similar the sequences are.
As a generative model, Transformer+CoT has achieved nearly perfect sequence accuracy in word problems, while the learned structure remains under-researched.
% Despite of nearly perfect performance in predicting next token $q_i$ on word problems, we need to explore whether Transformer+CoT recover the world model finite state automata.
Results in Section~\ref{subsec: MLP Neuron Analysis} imply that the model implements state tracking, through activating a bag of neurons.
Based on this, we can measure the generative model's FSA recovery, from the perspective of sequence compression and distinction at the granularity of the MLP neurons.
For activated neurons on two prompts $p_i, p_i'$ deducing to the same state, we design the following compression metric:
\[
\text{Compression} = \frac{ |\text{N}_{p_i} \cap \text{N}_{p_i'}| }{|\text{N}_{p_i} \cup \text{N}_{p_i'}|}
\]
where $\text{N}_{p_i}, \text{N}_{p_i'}$ represent activated neurons on $p_i, p_i'$.
And for activated neurons on two prompts $p_i, p_i'$ deducing to different states, we design the following distinction metric:
\[
\text{Distinction} = 1 - \frac{ |\text{N}_{p_i} \cap \text{N}_{p_i'}| }{|\text{N}_{p_i} \cup \text{N}_{p_i'}|}
\]
For each intermediate step, we calculate compression and distinction pairwise in all $m, q$ pairs.
We find that Transformer+CoT can not only compress prompts sampled from the $\textbf{q}$ prompt subset, but also distinguish prompts from different subsets with average metric close to 1, at each intermediate step.
Beyond $A_5$, we also find the similar FSA existing on $\mathbb{Z}_{60}$ and $A_4 \times \mathbb{Z}_5$, and large model scales, with details referring to Appendix~\ref{appendix other groups}.
And we conclude that \emph{Transformer+CoT recovers FSA on state tracking even at the granularity of MLP neurons}.

% \begin{figure}[t]
%   \includegraphics[width=\columnwidth]{image/metric.pdf}
%   \caption{
%   Compression and distinction metrics for prompts pairwise at $1\text{-th}$ step.
%   The model achieves nearly perfect compression or distinction for every pair of prompts.
%   }
%   \label{fig:C&D}
% \end{figure}


% \subsection{Knockout Experiments} 

% To test the hypothesis, we knock out the top-$K$ MLP neurons with respect to states, and test the performance drop on the corresponding $q_i$ prompt subgroup.

\subsection{Dynamic Circuit Analysis: Attention Patterns} \label{subsec: attention}

In the context of state tracking, the model's circuit is dynamic and consecutive during inferring each state $q_i$ in the scratchpad. 
Previous sections mainly analyze the MLP circuit as static circuit composed of late-layer MLPs, in this section we focus on the dynamic attention patterns across intermediate steps.
We first obtain the principal attention heads based on the activation patching results in Section~\ref{subsec: circuit} and analyze their activation patterns.
Results in Figure~\ref{fig:attention} show that there are two kinds of attention patterns:
\begin{enumerate}[noitemsep]
    \item Across all steps, specific attention heads transfer information from position $m_i$ to position $q_{i-1}$.
    \item As the sequence length increases, the model has developed another attention pattern: the model passes the information from $m_1 \ldots m_n$ backwards until the delay position ``:'', which splits input and scratchpad and marks the beginning of state tracking, so that the model can attend to the delay position during inference.
\end{enumerate}
We discuss more about the two attention patterns in Appendix~\ref{appendix attention}.

% In terms of the second attention pattern, a natural question is how the attention heads extract $m_i$ from the numerous input $m_1 \ldots m_n$ embedded in the residual stream at the delay position.
% We hypothesize that this extraction relies on the first type of attention pattern, which has injected the input information $m_i$ into the current position, facilitating the second type attention heads to differentiate the mixed information embedded in delay position.
% Furthermore, we validate this hypothesis by masking position $m_i$ at the $q_{i-1}$ position and probing the changes of retrieving input $m_i$ at the current step.
% When there is no mask, the average accuracy of probing $m_i$ in the representation of the middle and later layers, except for the last layer (high results for $q_i$), is greater than 0.9. 
% However, when the mask is applied, it drops to nearly 0.
% We conclude that, the model uses two interrelated dynamic attention patterns to focus on the correct input $m_i$ at $i\text{-th}$ step, corresponding to the automata accepting each step of the input.

\begin{figure}[ht]
  \includegraphics[width=1.05\columnwidth]{image/attention_v3.pdf}
  \caption{
  Two attention patterns.
  }
  \label{fig:attention}
\end{figure}






% \begin{table}
%   \centering
%   \begin{tabular}{lc}
%     \hline
%     \textbf{Command} & \textbf{Output} \\
%     \hline
%     \verb|{\"a}|     & {\"a}           \\
%     \verb|{\^e}|     & {\^e}           \\
%     \verb|{\`i}|     & {\`i}           \\
%     \verb|{\.I}|     & {\.I}           \\
%     \verb|{\o}|      & {\o}            \\
%     \verb|{\'u}|     & {\'u}           \\
%     \verb|{\aa}|     & {\aa}           \\\hline
%   \end{tabular}
%   \begin{tabular}{lc}
%     \hline
%     \textbf{Command} & \textbf{Output} \\
%     \hline
%     \verb|{\c c}|    & {\c c}          \\
%     \verb|{\u g}|    & {\u g}          \\
%     \verb|{\l}|      & {\l}            \\
%     \verb|{\~n}|     & {\~n}           \\
%     \verb|{\H o}|    & {\H o}          \\
%     \verb|{\v r}|    & {\v r}          \\
%     \verb|{\ss}|     & {\ss}           \\
%     \hline
%   \end{tabular}
%   \caption{Example commands for accented characters, to be used in, \emph{e.g.}, Bib\TeX{} entries.}
%   \label{tab:accents}
% \end{table}



% \begin{figure}[t]
%   \includegraphics[width=\columnwidth]{example-image-golden}
%   \caption{A figure with a caption that runs for more than one line.
%     Example image is usually available through the \texttt{mwe} package
%     without even mentioning it in the preamble.}
%   \label{fig:experiments}
% \end{figure}

% \begin{figure*}[t]
%   \includegraphics[width=0.48\linewidth]{example-image-a} \hfill
%   \includegraphics[width=0.48\linewidth]{example-image-b}
%   \caption {A minimal working example to demonstrate how to place
%     two images side-by-side.}
% \end{figure*}





% \begin{table*}
%   \centering
%   \begin{tabular}{lll}
%     \hline
%     \textbf{Output}           & \textbf{natbib command} & \textbf{ACL only command} \\
%     \hline
%     \citep{Gusfield:97}       & \verb|\citep|           &                           \\
%     \citealp{Gusfield:97}     & \verb|\citealp|         &                           \\
%     \citet{Gusfield:97}       & \verb|\citet|           &                           \\
%     \citeyearpar{Gusfield:97} & \verb|\citeyearpar|     &                           \\
%     \citeposs{Gusfield:97}    &                         & \verb|\citeposs|          \\
%     \hline
%   \end{tabular}
%   \caption{\label{citation-guide}
%     Citation commands supported by the style file.
%     The style is based on the natbib package and supports all natbib citation commands.
%     It also supports commands defined in previous ACL style files for compatibility.
%   }
% \end{table*}


\section{Robustness and Generalizability of FSA} \label{sec: robustness}

Sections~\ref{sec:model performance} and~\ref{sec:mechanism} have demonstrated positive results in FSA recovery. 
However, the word problems considered are too idealistic, with sequence length controlled and noise excluded.
And the actual data distribution may differ from this.
In this section, we will investigate the robustness and generalizability of FSA inside Transformer+CoT, considering intermediate step jumps, scratchpad noise, and length generalization.
We found that optimizing the distribution of training set data can guide the learned model to adapt to the above scenarios, except for length generalization.
And solving the latter may necessitate exploring different perspectives, such as modifying the model architecture, improving training strategies, or other.

\subsection{Allow Skipping} \label{Skipping}

In practical training data, intermediate step jumps are very common.
For example, the length of $q_1 \ldots q_n$ may be less than $n$, with some intermediate steps skipped.
In our experiment setting, we skip each intermediate state $q_i$ except $q_n$ with skipping probability, and train Transformer+CoT in the dataset. 
In order to study the extent of the model's step jumps, we use a linear classifier to probe the states embedded in the residual stream in the last position.
Specifically, we probe the token $q_i$ in the last position, given the prompt $m_1 \ldots m_n | q_1 \ldots q_{i-1}$.
From the results in Figure~\ref{fig:multi_step_probe}, we can find that $q_i$, $q_{i+1}$, and $q_{i+2}$ are all embedded in the residual stream, suggesting that the model has learned single-step reasoning, two-step reasoning (skipping one step), as well as three-step reasoning (skipping two steps).
Moreover, we find that the index of layers with sufficiently high probing results for $q_{i+2}$ is greater than that for $q_{i+1}$, which is greater than that for $q_{i}$ in turn.
Moreover, we propose possible mechanisms for skipping and design experiments for analysis as in the appendix~\ref{appendix skipping}.
Finally, we conclude that adding skipping to the training set can guide the FSA inside Transformer+CoT to adapt to the scenario.
% In terms of skipping two steps, we stress that in the distribution of the training data set, there is a probability of only 0.038 that there is a two-step skipping (refer to Appendix~\ref{sec:appendix} for calculation details).
% A small percentage of the training data has taught the model to learn two-step reasoning.

\begin{figure}[ht]
  \includegraphics[width=\columnwidth]{image/multi_step_probe.pdf}
  \caption{
  Probing results for state $q_i$ at $4\text{-th}$ step.
  We can find the positive results appearing two positions earlier.
  }
  \label{fig:multi_step_probe}
\end{figure}

% There are two possible mechanisms for skipping in Transformer+CoT: one is to use continuous layers to achieve multi-step state transitions, and the other is to use a single layer to achieve multi-step transitions.
% The key difference is that the former depends on the skipped states, while the latter does not.
% To explore what algorithms the model uses in skipping, we design intervention experiments by suppressing skipped tokens.
% Specifically, given the prompt $m_1 \ldots m_n | q_1 \ldots q_{i-1}$, we evaluate the probing changes for $q_{i+1}$ in the last position, while suppressing token $q_{i}$.
% % (Refer to Appendix~\ref{appendix skipping} for more details).
% We find that suppressing $q_{i}$ has caused the positive probing results to drop almost to zero in the last position with skipping probability ranging between 0 and 1, suggesting that Transformer+CoT implements skipping through continuous layers.
% This also interprets that why the index of layers with relative high probing results satisfies \( q_{i+2} > q_{i+1} > q_{i-1} \), as show in Figure~\ref{fig:multi_step_probe}.
% There are two mechanisms for skipping in Transformer+CoT: one involves using multiple layers for state transitions, and the other uses a single layer for the same purpose. 
% The key difference is that the former relies on skipped states, while the latter does not. 
% To investigate the model's skipping behavior, we design experiments by suppressing skipped tokens(Refer to Appendix~\ref{appendix skipping} for more details).
% We find that suppressing a token leads to a significant drop in probing results, indicating that Transformer+CoT uses multiple layers for skipping. 
% This also explains the observed pattern where probing results follow a specific order in the layers.



% \subsection{Superposition}

\subsection{Noise in Scratchpad} \label{subsec: noise}

In this section, we consider injecting noise into scratchpad $q_1 \ldots q_{i_1}$, where some previous state is false.
We divide the noise into the following two types: any false state except $q_{i-1}$ and false $q_{i-1}$.
We have tested the accuracy of the model in predicting the next token with noise in scratchpad (we use token accuracy as metric here).
For the first type noise, the model still achieves almost perfect predictions.
The reason is obvious: when the model implements state transition in the $i\text{-th}$ step, it mainly focuses on $m_i$ and the previous state $q_{i-1}$, while ignoring intermediate states prior to $q_{i-1}$.
And for the second type of noise, the model's performance dramatically decreases to an average accuracy of nearly zero (0.017).

Here, we attempt once again to deal with the scenario by adjusting the dataset distribution.
Specifically, we train the model using the data with the second type of noise in scratchpad and successfully enhanced the robustness of the model, with the second type noise accuracy increased from 0.017 to 0.896.
Furthermore, we probe the layer representation in the last position to investigate whether the model has retrieved correct state tracking.
Specifically, given the corrupt prompt $m_1 \ldots m_n | q_1 \ldots \hat{q_{i-1}}$ ($\hat{q_{i-1}}$ stands for the error state), we probe the correct next state $q_{i}$, false previous state $\hat{q_{i-1}}$ and true previous state $q_{i-1}$.
We find that the layer representation maintains positive probing results for $\hat{q_{i-1}}$ until the MLP11, which exhibits high accuracy for $q_{i}$ , and low for $q_{i-1}$.
Based on this, we propose one possible mechanism that the model attends to intermediate representation at previous step, thus retrieving the correct input state $q_{i-1}$ and updating the state to $q_{i}$ successfully.
And we have designed experiments to analyze the hypothesis as in Appendix~\ref{appendix noise}.
% According to the probing results, we propose one possible mechanism that at $i\text{-th}$ step, the model suppresses the wrong input state $\hat{q_{i-1}}$, and attends to the intermediate representation at previous step, thus retrieving the correct input state $q_{i-1}$ and updating the state to $q_{i}$ successfully.
% We validate this by masking the position ${q_{i-2}}$ in the last layer and evaluate the probing accuracy change for state $q_{i}$ in the last position.
% The results indicate that after masking, the MLP11 probing results dropped from greater than 0.9 to nearly zero.
% Moreover, the model has also implement correct state transition this step, which means the model has implemented two-step reasoning like Section~\ref{Skipping}.
We conclude that optimizing dataset distribution can help in maintaining the accuracy and reliability of the FSA inside Transformer+CoT.

\subsection{Length Generalization and Error Analysis}

Furthermore, we investigate length generalization in out-of-distribution setting, which is a critical challenge in the development of transformer-based language models. 
We replace GPT2's original absolute positional embedding with NoPE (without explicit positional embedding) for better length generalization performance, which has been proven to resemble T5’s relative positional embedding \citep{10.5555/3666122.3667204}.
And we choose LSTM for comparison for the reason that it learns any word problems on sequences of arbitrary length. 
Table~\ref{tab:ood} shows that on sequences of lengths that it has not ever seen during training, different models exhibit varying levels of length generalization ability: LSTM perfect, while transformers with chain-of-thought weak. 
We analyze the failure from the perspective of MLP neurons, and find an interesting ``U-turn'' phenomenon in activated neurons precision during the last few steps of inference.
More analysis on this phenomenon refers to the Appendix~\ref{appendix length}.
Finally, we conclude that improving the length generalization ability of the model requires other methods, perhaps the model structure, loss function or optimization algorithm.



\begin{table}[ht]
\centering
\resizebox{\columnwidth}{!}{
\begin{tabular}{lcccccccccccccccc}
\toprule
Models & 20 & 21 & 22 & 23 & 24 & 25 & 30 \\
\midrule
LSTM & 1 & 1 & 1 & 1 & 1 & 1 & 1  \\
Transformer+CoT & 0.99 & 0.983 & 0.868 & 0.527 & 0.24 & 0.088 & 0 \\
\bottomrule
\end{tabular}
}
\caption{Length generalization performance of LSTM and Transformer+CoT. 
The models are trained on sequences of length up to 20.}
\label{tab:ood}
\end{table}

\section{Conclusions}
In this work, we investigate the learnability of Transformer+CoT through the lens of mechanistic interpretability in state tracking. We begin by reaffirming the success of Transformer+CoT in both in-domain and out-of-domain settings. Next, we identify the key components of the circuit, specifically the neurons in the last layers of the MLP, which play a critical role. We find evidence of an implicit FSA within the model, where each state is compressed by a distinct set of neurons. 
Finally, we evaluate the model in three realistic settings and show that the learned FSA is robust to noise and sequence skipping but struggles with generalization to significantly longer sequences. This suggests the need for architectural or optimization improvements. 
Our findings reveal that Transformer+CoT achieves near-perfect performance in next-token prediction while internalizing an FSA-like structured state representation, bridging the gap between expressiveness and learnability. This work provides insights into structured reasoning for sequential tasks.



% Despite these promising results, our study is limited to GPT2-style architectures. Future research should explore more diverse models, including Mixture-of-Experts (MoE) architectures and advanced memory-augmented Transformers. Additionally, disentangling state tracking from other emergent capabilities remains a challenge in practical applications.



\section*{Limitations}

In this paper, we define state tracking as word problems involving cyclic and symmetric groups, which form the basis of our conclusions. Our study focuses solely on GPT2-like models, leaving the exploration of additional models, such as Llama, for future research. Furthermore, while we have made efforts to address more realistic word problems, state tracking in real-world tasks is intricate, making it challenging to separate state tracing abilities from other skills.





% Bibliography entries for the entire Anthology, followed by custom entries
% \bibliography{anthology,custom}
% Custom bibliography entries only
\bibliography{acl_latex}

\appendix

\section{Symbol Description} \label{sec:appendix}

In this section, we briefly introduce the symbols used in this article as in Table~\ref{tab:symbols}.

\begin{table*}[ht]
    \centering
    \caption{Symbols used in this paper}
    \begin{tabular}{lp{12cm}}
\toprule
\textbf{Symbol} & \textbf{Description} \\ \midrule
$\Sigma$               & Finite set of characters                     \\
$Q$               & Finite set of states in the automaton             \\
$q_0$               & Start state, an element of the state set Q      \\
$\delta$               & State-transition function                  \\
$(M, \cdot)  $             & A monoid                    \\
$M^*$               & Set of all possible sequences from $M$      \\
$m_i$               & The $i$\-th input of in word problems        \\
$e$               & The starting state                    \\
$Z_m$               & Cyclic group of order m             \\ 
$S_m$              & Symmetric group of order m           \\ 
$TC^0$            &  A complexity class in computational complexity theory, consisting of problems solvable by uniform constant-depth threshold circuits with a polynomial number of gates.     \\
$NC^1$            &  A complexity class containing problems solvable by uniform logarithmic-depth Boolean circuits with a polynomial number of gates and bounded fan-in.     \\
$\mathbb{Z}_{60}$          & an abelian group encoding mod-60 addition      \\
$A_5$ & the alternating group on five elements \\
$A_4 \times \mathbb{Z}_5$ & a non-abelian but solvable group \\
$p_i$ & Prompt at the $i$\-th step \\
$q_i$ & Result state at the $i$\-th step \\
$\hat{q_i}$ & Error state \\
$p_i'$ & Counterfactual prompt at the $i$\-th step \\
$q_i'$ & Counterfactual result state at the $i$\-th step \\
$\text{P}$ & Pre-intervention probability distribution\\
$\text{P}^*$ & Post-intervention probability distribution\\
$x^l$ & Input at layer $l$\\
$K^l$ & The weight matrix at layer $l$\\
$V^l$ & The bias vector at layer $l$ \\
$d_{mlp}$ & the MLP intermediate dimension \\
$d_m$ & the model dimension \\
$f$ & Non-linear activation function \\
$k^l_j$ & the $j\text{-th}$ row vectors of $K^l$ \\
$v^l_j$ & the $j\text{-th}$ row vectors of $V^l$ \\
$m_{j}^l$ & Activation coefficient at the $j$\-th neuron \\
$\text{N}_{p_i}$ & Set of activated neurons on prompt $p_i$ \\
$\text{N}_{p_i'}$ & Set of activated neurons on prompt $p_i'$ \\
$\mathcal{L}_{n-1}$ & Length of the state sequence $q_1 \ldots q_{n-1}$ except last state \\
$\mathcal{L}_{n}$ & Length of the state sequence $q_1 \ldots q_{n}$ \\
\bottomrule
\end{tabular}
    \label{tab:symbols}
\end{table*}


% 276行
\section{Model used for Performance Evaluation} \label{appendix models} 

In Section~\ref{sec:model performance}, we hold a comprehensive evaluation on various model vatiants and implicit CoT. 
The models and task designs are listed as Table~\ref{tab:model_complexities}.

\begin{table}[ht]
\centering
\begin{tabular}{@{}lcc@{}}
\toprule
\textbf{Model} & \textbf{Architecture} & \textbf{Task} \\ 
\midrule
\textbf{RNN} & Encoder & TT \\  
\textbf{LSTM} & Encoder & TT \\  
\textbf{S4} & Encoder & TT \\  
\textbf{Mamba} & Encoder & TT \\  
\textbf{Transformer} & Encoder & TT \\  
\textbf{Transformer+CoT} & Decoder & LM \\  
\textbf{Recurrent} & Encoder & TT \\  
\textbf{Looped} & Encoder & TT \\  
\textbf{Pause} & Decoder & LM \\  
\bottomrule
\end{tabular}
\caption{Depth and time complexity of different models.
TT and LM represent Token-Tagging task and Language Modeling task, respectively.}
\label{tab:model_complexities}
\end{table}


% 342行
\section{Separating Elements in Groups} \label{groups}

In Section~\ref{subsec: ood}, we separate all $m$ into proper subsets to explore the models' performance.
As Table~\ref{tab: division} shows, we divide $m$ in $\mathbb{Z}_{60}$, $A4\_\text{x}\_Z5$ and $A_5$ according to values, orders and permutation types, respectively.
We then prove that the division is reasonable, for the reason that every proper subset can generate the whole group.

\begin{theorem}
    Every sepated proper subset can generate the whole group under group operation.
\end{theorem}

\begin{proof}

    For Cyclic group, every element is a generator, so that proper subsets can generate \( h \in \mathbb{Z}_{60} \).
    
    For alternating group, the 3-cycle and 5-cycle in \( A_5 \) can both generate the group.

    For direct product group, we continue the proof as follows:

    The elements of order 15 in the direct product group \( A_4 \times \mathbb{Z}_5 \) are those elements of the form \( (g, h) \), where \( g \in A_4 \) is a 3-cycle (order 3) and \( h \in \mathbb{Z}_5 \) is a nonzero element (order 5).
    Although these elements of order 15 form a proper subset of the entire group, their projections onto the factors \( A_4 \) and \( \mathbb{Z}_5 \) separately generate the full groups:
    \begin{itemize}
        \item The 3-cycle in \( A_4 \) can generate \( A_4 \) (since \( A_4 \) is generated by its 3-cycles).
        \item Any nonzero element in \( \mathbb{Z}_5 \) is a generator, thus can generate the entire \( \mathbb{Z}_5 \).
    \end{itemize}
    By the generation theorem of direct product groups, if a subset has surjective projections onto each factor, then the group it generates must be the entire direct product.
    Thus, all elements of order 15 can generate the entire group \( A_4 \times \mathbb{Z}_5 \).
\end{proof}

We train the model with sequences sampled from one subset, and evaluate on sequences with $m$ sampled from the group.
We emphasize that Although the model did not encounter mixed sequences during training, the proper subset is able to generate the entire group. 
Therefore, all possible state transitions are included in the data.
Transformer+CoT success in out-of-distribution shows that the learned algorithm does not rely on finding a certain pattern in the input sequence.

% Surprisingly, LSTM fails to generalize as an encoder, and we have also explored its performance as a decoder, where LSTM fails to converge, however.
Surprisingly, LSTM fails to generalize when used as an encoder. 
We have also explored its performance as a decoder, but here, LSTM even fails to converge.
% We The failure of the LSTM may be due to its design as an encoder, while CoT can include all elements of the group within the entire sequence, by concatenating every possible states after the input sequence.
We hypothesize that the disparity in out-of-distribution performance stems from LSTM being used as an encoder, whereas Transformer+CoT, as a decoder, can integrate all elements of the group into the sequence by concatenating each possible state after the input sequence.

\begin{table*}
  \centering
  \begin{tabular}{lp{10cm}}  
    \hline
    \textbf{Group} & \textbf{Proper Subsets} \\
    \hline
    $A_{5}$ & \textbf{Identity and Double Transpositions:} \\  
            & 0, 3, 8, 11, 12, 13, 14, 27, 30, 33, 41, 43, 47, 53, 55, 59 \tabularnewline
            & \textbf{3-Cycles:} \\  
            & 1, 2, 4, 5, 6, 7, 9, 10, 15, 19, 22, 24, 28, 29, 37, 39, 40, 49, 51, 52 \tabularnewline
            & \textbf{5-Cycles:} \\  
            & 16, 17, 18, 20, 21, 23, 25, 26, 31, 32, 34, 35, 36, 38, 42, 44, 45, 46, 48, 50, 54, 56, 57, 58 \\
    \hline
    $A4\_\text{x}\_Z5$ & \textbf{Order 15:} \\  
            & 6, 7, 8, 9, 11, 12, 13, 14, 21, 22, 23, 24, 26, 27, 28, 29, 31, 32, 33, 34, 36, 37, 38, 39, 46, 47, 48, 49, 51, 52, 53, 54 \tabularnewline
            & \textbf{Other Orders:} \\  
            & 0, 15, 40, 55, 5, 10, 20, 25, 30, 35, 45, 50, 1, 2, 3, 4, 16, 17, 18, 19, 41, 42, 43, 44, 56, 57, 58, 59 \\
    \hline
    $\mathbb{Z}_{60}$ & \textbf{$<30$}: \\  
            & 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29 \tabularnewline
            & \textbf{$\ge 30$}: \\  
            & 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59 \\
    \hline
  \end{tabular}
  \caption{Division of elements for three groups in out-of-distribution.}
  \label{tab: division}
\end{table*}


% 469行
\section{Classification Algorithms} \label{appendix algorithm}

We provide the late-layer MLP neurons classification procedures and pseudocode Algorithm ~\ref{al: classify} as follows:

\begin{enumerate}[noitemsep]
    \item Utilize the priori transition rules to pre-compute all labels across all possible resulting states. 
    \item Measure all activation coefficient $m_{j}^l$ across all ($m, q$) pairs. 
    \item Utilize the logit lens to calculate the logits of tokens embedded in $v_j^l$, and convert to a 2D pattern, where the cell in index ($m, q$) is the logit of the result state $\textbf{q} = m \cdot q$. 
    \item Multiply the intermediate results of the previous two steps element-wise, resulting in effective logit contribution of the neuron to the corresponding state for each ($m, q$) pair. 
    \item  Extract the top 60 ($m, q$) pairs from the activation pattern as the prediction. 
    \item Compute the f1 scores between prediction and all labels. 
    The neuron can be classified to state with f1 score no less than threshold $\theta=0.2$\footnote{Here the value of $\theta$ is an empirical setting, and the random value is 0.017, refer to Appendix~\ref{f1} for the computation of the random value.}.
\end{enumerate}


\begin{algorithm}
\caption{Classify MLP Neurons at Layer $l$}
\begin{algorithmic}[1]
\State \textbf{Input:} Prior transition rules $T$, activation coefficients matrix $M_j^l$ for all $(m, q)$ pairs, value vector $v_j^l$, threshold $\theta$
\State \textbf{Output:} Classified neurons
\State Compute all labels $L$ from $T$
\For{each neuron $j$ in layer $l$}
    \State Compute logit matrix $Z_j$ for all $(m, q)$ pairs from $v_j^l$ using logit lens
    \State Compute effective logit contribution $C_j = M_j^l \odot Z_j$
    \State Select top 60 $(m, q)$ pairs from $C_j$ as prediction $P_j$
    \State Compute F1 score between $P_j$ and $L$
    \If{F1 score $\geq \theta$} 
        \State Classify neuron $j$ as relevant
    \Else
        \State Classify neuron $j$ as irrelevant
    \EndIf
\EndFor
\State \Return Classified neurons
\end{algorithmic}
\label{al: classify}
\end{algorithm}


% 477行
\section{Expected F1 Score of Randomly Activated Neurons} \label{f1}

In Section~\ref{subsec: MLP Neuron Analysis}, we empirically set the value of $\theta$ as 0.2 to classify MLP neurons, which is non-trivial compared with random F1 score 0.017.
And we provide detailed calculation process of randomly activated neurons' F1 score as follows.

Let \( S \) be a set containing 3600 elements: \(S = \{0, 1, 2, \ldots, 3599\}\), and let \( L \) be the label set consisting of the first 60 elements: \(L = \{0, 1, 2, \ldots, 59\}\).
We randomly sample 60 elements from \( S \) to form a subset \( x \): \(x \subseteq S\) and \(|x| = 60\).
Our goal is to compute the expected values of precision, recall, and F1 score for the subset \( x \) with respect to the label set \( L \).

Define \(T = |x \cap L|\) as the number of correctly selected elements. 
Since \( x \) is chosen uniformly at random from \( S \), and given that \( L \) contains 60 elements, \( T \) follows a hypergeometric distribution. 
Its expected value is:
\[
\mathbb{E}[T] = \frac{|x| \cdot |L|}{|S|} = \frac{60 \times 60}{3600} = 1.
\]
The precision and recall are defined by
\[
\text{Precision} = \frac{|x \cap L|}{|x|} = \frac{T}{60} 
\]
\[ \text{Recall} = \frac{|x \cap L|}{|L|} = \frac{T}{60}
\]
Thus, their expected values are
\[
\mathbb{E}[\text{Precision}] = \mathbb{E}\left[\frac{T}{60}\right] = \frac{\mathbb{E}[T]}{60} = \frac{1}{60},
\]
\[
\mathbb{E}[\text{Recall}] = \frac{1}{60}.
\]
The F1 score is the harmonic mean of precision and recall:
\[
\text{F1} = \frac{2 \cdot \text{Precision} \cdot \text{Recall}}{\text{Precision} + \text{Recall}}.
\]
Since \(\text{Precision} = \text{Recall} = \frac{T}{60}\), we substitute to obtain
\[
\text{F1} = \frac{2 \cdot \frac{T}{60} \cdot \frac{T}{60}}{\frac{T}{60} + \frac{T}{60}} = \frac{2 \cdot \frac{T^2}{3600}}{\frac{2T}{60}} = \frac{T}{60}.
\]
Hence, the expected F1 score is
\[
\mathbb{E}[\text{F1}] = \mathbb{E}\left[\frac{T}{60}\right] = \frac{1}{60} \approx 0.01667.
\]



% 549行
\section{Activated Neurons Analysis} \label{precision and recall}

Having classified neurons into states as in Section~\ref{subsec: MLP Neuron Analysis}, We can compute the accuracy and recall of the activated neurons, averaged over all $\textbf{q}$ prompt subsets, during the single-step state transition of the model.
We acquire the top-$K$ activated neurons according to the activation coefficient $m_{j}^l$, and compute the precision and recall with classified neurons as labels.
Results in Figure~\ref{fig:p&r} show that across intermediate steps, the model successfully activate MLP11 neurons with high precision 0.797 but low recall 0.253.
We stress that the high precision explains the accuracy on word problems.
The low recall imply that, the eurons activated at different steps are also distinguished.




\begin{figure}[ht]
  \includegraphics[width=\columnwidth]{image/precision_recall_v3.pdf}
  \caption{
  Averaged precision and recall across different steps.
  }
  \label{fig:p&r}
\end{figure}



% 560行
\section{MLP0 in Circuit} \label{appendix mlp0}

Activation patching results in Section~\ref{subsec: circuit} show that the circuit mainly consists of MLP0 and late-layer MLPs.
Given a prompt $m_1 \ldots m_n | q_1 \ldots q_{i-1}$, the late-layer MLPs implement state transition in the postion $q_{i-1}$, while the MLP0 mainly achieves effective word embedding in position $m_{i}$.
As Figure\ref{fig:mlp0} shows, the MLP0 promotes input $m_i$ into the residual stream, which will then be transferred to the last position $q_{i-1}$ by attention heads for subsequent state transitions.

\begin{figure}[ht]

\includegraphics[width=\columnwidth]{image/A5increase_v2.pdf}
  \caption{
  Average input $m$ logit increase in the layer representation for MLP0.
  The MLP0 mainly implements effective word embedding in the circuit.
  }
  \label{fig:mlp0}
  
\end{figure}



% 580行
\section{Automta still exists on other groups with large model scale.}  \label{appendix other groups}

We have found FSA with GPT2-small on $A_5$ as in Section\ref{subsec: automata}, in this section, we will extrapolate this conclusion to other problems and models.
We first conduct similar experiments on $A_4 \times \mathbb{Z}_5$ and $ \mathbb{Z}_{60}$.
We find that MLP11 neurons can still be classified according to transition rules, and the model can still compress and distinguish different input sequences.
Moreover, we explore whether FSA still exists with model scale larger.
We have repeated the experiments, increasing the model scale from GPT2 small (124M) to GPT2 large (744M), and found FSA still exists.
Results are shown in Figure~\ref{fig: stillexists}.

\begin{figure*}[ht]
    \centering
    \begin{subfigure}{0.45\textwidth}
        \centering
        \includegraphics[width=\textwidth]{image/metric_mlp10.pdf}
        \caption{}
    \end{subfigure}
    \begin{subfigure}{0.45\textwidth}
        \centering
        \includegraphics[width=\textwidth]{image/A4_x_Z5metric_mlp11.pdf}
        \caption{}
    \end{subfigure}
    \begin{subfigure}{0.45\textwidth}
        \centering
        \includegraphics[width=\textwidth]{image/Z60metric_mlp11.pdf}
        \caption{}
    \end{subfigure}
    \begin{subfigure}{0.45\textwidth}
        \centering
        \includegraphics[width=\textwidth]{image/largeA5metric_mlp35.pdf}
        \caption{}
    \end{subfigure}
    \caption{(a): Compression and distinction metrics for MLP10 on $A_5$.
    (b): Compression and distinction metrics for MLP11 on $A_4 \times \mathbb{Z}_5$.
    (c): Compression and distinction metrics for MLP11 on $\mathbb{Z}_{60}$.
    (d): Compression and distinction metrics for MLP10 on $A_5$ with larger model scale.
    % The entire square's dark color represents that the metric of any pairwise combination of prompts is nearly 1.
    The entire square's dark color can be interpreted as the metric being nearly 1 for any pairwise combination of prompts.
    }
    \label{fig: stillexists}
\end{figure*}


% 607行
\section{Dynamic Attention Pattern} \label{appendix attention}

In terms of the attention patterns in Section~\ref{subsec: attention}, a natural question is how the attention heads extract $m_i$ from the numerous input $m_1 \ldots m_n$ embedded in the residual stream at the delay position.
% a natural question is how the two patterns interact.
% We hold intervention experiments to provide some insights.
% Specifically, we separately mask position $m_i$ and the dalay position, and probe the changes of retrieving input $m_i$ at the current step.

We hypothesize that this extraction depends on the first type of attention pattern, which has injected the input information $m_i$ into the current position, facilitating the second type attention heads to differentiate the mixed information embedded in delay position.
Furthermore, we validate this hypothesis by masking position $m_i$ at the $q_{i-1}$ position and probing the changes of retrieving input $m_i$ at the current step.
When there is no mask, the average accuracy of probing $m_i$ in the representation of the middle and later layers, except for the last layer (high results for $q_i$), is greater than 0.9. 
However, when the mask is applied, it drops to nearly 0.
We conclude that, the model uses two interrelated dynamic attention patterns to focus on the correct input $m_i$ at $i\text{-th}$ step, corresponding to the FSA accepting each step of the input.

% \section{Skipping} \label{appendix skipping}

% There are two possible mechanisms for skipping in Transformer+CoT: one is to use continuous layers to achieve multi-step state transitions, and the other is to use a single layer to achieve multi-step transitions.
% The key difference is that the former depends on the skipped states, while the latter does not.
% To explore what algorithms the model uses in skipping, we design intervention experiments by suppressing skipped tokens.
% Specifically, given the prompt $m_1 \ldots m_n | q_1 \ldots q_{i-1}$, we probe the token $q_{i+1}$ in the last position, while suppressing token $q_{i}$.
% We find that suppressing $q_{i}$ has caused the positive probing results to drop almost to zero in the last position with $\sigma$ ranging between 0 and 1, suggesting that skipping necessitates the skipped state $q_{i}$ in the previous layers.
% This suggests that the possible mechanism is the model performs single-step reasoning in continuous layers, resulting predicting the token $q_{i+1}$.
% This also interprets that why the index of layers with relative high probing results satisfies \( q_{i+2} > q_{i+1} > q_{i-1} \), as show in Figure~\ref{fig:multi_step_probe}.

\section{Allow Skipping } \label{appendix skipping}

% There are two mechanisms for skipping in Transformer+CoT: one involves using multiple layers for state transitions, and the other uses a single layer for the same purpose. 
% The key difference is that the former relies on skipped states, while the latter does not. 
% To investigate the model's skipping behavior, we design experiments by suppressing skipped tokens(Refer to Appendix~\ref{appendix skipping} for more details).
% We find that suppressing a token leads to a significant drop in probing results, indicating that Transformer+CoT uses multiple layers for skipping. 
% This also explains the observed pattern where probing results follow a specific order in the layers.

According to the probing results in Figure~\ref{fig:multi_step_probe}, we propose two possible mechanisms for skipping in Transformer+CoT: one is to use continuous layers to achieve multi-step state transitions, and the other is to use a single layer to achieve multi-step transitions.
The key difference is that the former depends on the skipped states, while the latter does not.
To explore what algorithms the model uses in skipping, we design intervention experiments by suppressing skipped tokens.
Specifically, given the prompt $m_1 \ldots m_n | q_1 \ldots q_{i-1}$, we evaluate the probing changes for $q_{i+1}$ in the last position, while suppressing token $q_{i}$.
We find that suppressing $q_{i}$ has caused the positive probing results to drop almost to zero in the last position with skipping probability ranging between 0 and 1, suggesting that Transformer+CoT implements skipping through continuous layers.
% This interprets that why the index of layers with relative high probing results satisfies \( q_{i+2} > q_{i+1} > q_{i-1} \), as show in Figure~\ref{fig:multi_step_probe}.

% 774行
\section{Noise in Scratchpad} \label{appendix noise}


% According to the probing results in Section~\ref{subsec: noise}, we hypothesize that at $i\text{-th}$ step, the model suppresses the wrong input state $\hat{q_{i-1}}$ in the last layer, and attends to the intermediate representation at previous step, thus retrieving the correct input state $q_{i-1}$ and updating the state to $q_{i}$ successfully.
% We test this by masking the position ${q_{i-2}}$ in the last layer and evaluate the probing accuracy change for state $q_{i}$ in the last position.
% The results indicate that after masking, the MLP11 probing results dropped from greater than 0.9 to nearly zero.
% This suggests that attending to position ${q_{i-2}}$ in the last layer is necessary for correct state transition at current step.
% Moreover, we hold ablation experiments for model trained without noise, and find that probing accuracy change for state $q_{i}$ in the last position does not change much.
% This may suggest that the automata may develop adaptive attention patterns to deal with noise in scratchpad.

Based on the probing results in Section~\ref{subsec: noise}, we propose that at the $i\text{-th}$ step, the model mitigates the influence of the incorrect input state $\hat{q_{i-1}}$ in the final layer while focusing on the intermediate representation from the previous step. This allows it to recover the correct input state $q_{i-1}$ and accurately update to $q_{i}$.

To verify this, we mask the position of ${q_{i-2}}$ in the final layer and analyze the resulting changes in probing accuracy for state $q_{i}$ at the last position. The results show that after masking, the MLP11 probing score drops from above 0.9 to nearly zero, indicating that attending to ${q_{i-2}}$ in the last layer is crucial for accurate state transitions at the current step.
In contrast, we hold ablation experiments with a model trained without noise, and we observe minimal change in the probing accuracy for state $q_{i}$ while masking. 
This suggests that the automaton may develop adaptive attention mechanisms to compensate for noise in the scratchpad.

\section{Generalization Performance for Other Length Ranges} \label{appendix length}

There are many factors contributing to weak length generalization. 
Here, we explain the model's failure in length generalization from the perspective of MLP neurons.
We calculate neurons precision and recall (Section~\ref{subsec: MLP Neuron Analysis}) on word problems with one length exceeding the training set and another below the training set.
Results in Figure~\ref{fig:length_generalization} show that neurons' precision and recall decrease sharply, with the inference step larger than the maximum length seen before.
Interestingly, we have found that when the sequence length exceeds the training data by a small margin, there is a ``U-turn'' phenomenon in precision during the last few steps of inference, which will yet disappear with the length increasing.
To explore the ``U-turn'' phenomenon, we conduct experiments with models train on various length ranges and find that the margin is roughly between 125\% and 150\%, beyond which the phenomenon disappears. 
Additionally, this phenomenon also exists when the test length is slightly shorter than the training length.
This may indicate that activating correct neurons is not the determining factor limiting length generalization.
Finally, we conclude that improving the length generalization ability of the model requires other methods, perhaps the model structure, loss function or optimization algorithm.

\begin{figure}[ht]
  \includegraphics[width=\columnwidth]{image/length_generalization_v4.pdf}
  \caption{
  MLP11 neurons precision and recall across intermediate steps on A5 with length $8$ and $25$.
  The model is trained on sequences with length ranging from 10 to 20.
  }
  \label{fig:length_generalization}
\end{figure}


% We conduct experiments with models train on various length ranges and find that the margin is roughly between 125\% and 150\%, beyond which the phenomenon disappears. 
% Additionally, this phenomenon also exists when the test length is slightly shorter than the training length.
% This may indicate that activating correct neurons is not the determining factor limiting length generalization.

\end{document}

