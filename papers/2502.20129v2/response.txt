\section{Related Work}
\label{Background&Related Work}

% However, automata play a fundamental role in evaluating the sequential computational ability of models. 
% (Automata, particularly finite state automata (FSA), serve as a compact and mathematically grounded framework for representing sequential processes.They provide an essential benchmark for understanding how models handle and internalize structured, rule-based transitionsâ€”a quality central to many applications such as natural language processing, speech recognition, and control systems.)
% (Finite-state automata (FSA), in particular, provide a mathematical abstraction for capturing sequential dependencies, making them a crucial tool in understanding the capabilities of sequence models. They have been extensively used in formal language theory, natural language processing, and cognitive science, where they help analyze the expressiveness and generalization properties of learning systems.)
% Automata, particularly FSA, serve as a mathematical abstraction for capturing sequential dependencies and decision processes, offering insights into the underlying structure of learned representations in neural networks **Kearns, Sequential Decision Making (1996)**. 
% And automata theory **Lindner, Finite State Machines (2003)** provides a rigorous framework for modeling and analyzing systems that exhibit state-dependent transitions, making it widely applicable in natural language processing, formal verification, and cognitive modeling.
% Recurrent neural networks (RNNs) have been extensively studied in this context, with prior research **Elman, Finding Structure (1990)** demonstrating that RNNs can recover FSA from sequential data, effectively modeling complex transition dynamics. 
% However, despite the growing success, whether Transformer models augmented +CoT can similarly recover FSA remains an open question. 
% This gap in understanding motivates our investigation into whether Transformer models augmented +CoT can effectively learn FSA-like structures, focusing specifically on state tracking.

\subsection{FSA and State Tracking}

% To better investigate state tracking capability of transformers, it is necessary to give a formal definition first. 
% We follow the previous definition **Hopcroft, Introduction to Automata Theory (2007)**, where the world model behind state tracking is a (deterministic) finite state automaton. 

% We use the standard definition of finite state automata, which is a tuple $\mathcal{A} = (\Sigma, Q, q_0, \delta)$ where $\Sigma$ is a finite set of characters, $Q$ is a finite set of states, $q_0 \in Q$ is the start state, and $\delta : Q \times \Sigma \to Q$ is the state-transition function mapping a state and character to the next state.
% Following the formal definition in **Eilenberg, Automata, Languages, and Machines (1974)**, state tracking can be expressed as a word problem on a finite monoid formally. 
% Given a monoid $(M, \cdot)$ ($M$ is the set and $\cdot$ is the associative operation), we want to reduce words $m_1 \ldots m_n \in M^*$, describing the sequence of updates, to their product $m_1 \cdot m_2 \cdot \ldots \cdot m_n \in M$, representing the state of the system after the updates. 
% If $M$ is finite, there is a corresponding finite state automaton $(M, M, e, \delta)$ that solves the word problem, where the starting state is $e$ (the identity element), and the transition function is $\delta(m_1, m_2) = m_1 \cdot m_2$ for $m_1, m_2 \in M$. 
% As a generative model, transformers with chain-of-thought generate state sequences $q_1 \ldots q_n \in M^*$, given input sequences $m_1 \ldots m_n \in M^*$ , where $q_i$ equals the state of the system at $i\text{-th}$ step. 
% In this work, we focus on word problems on groups (monoids with inverses), specifically, on the cyclic group $\mathbb{Z}_m$, i.e. addition modulo $m$, and the symmetric group $S_m$, i.e. the group of permutations on $m$ elements.

We adopt the conventional definition of a finite state automaton (FSA) as a tuple $\mathcal{A} = (\Sigma, Q, q_0, \delta)$, where $\Sigma$ is the alphabet, $Q$ is a set of states, $q_0$ is the initial state, and $\delta$ is the transition function **Rabin, Finite Automata (1963)**. State tracking can be framed as solving a word problem over a finite monoid $(M, \cdot)$, where the objective is to compute the product $m_1 \cdot m_2 \cdot \ldots \cdot m_n \in M$ **Eilenberg, Automata, Languages, and Machines (1974)**. This computation can be performed by a finite state automaton. As generative models, transformers augmented with chain-of-thought generate state sequences $q_1 \ldots q_n \in M^*$ based on input sequences $m_1 \ldots m_n \in M^*$. Our work focuses on word problems over groups, specifically the cyclic group $\mathbb{Z}_m$ and the symmetric group $S_m$.

\subsection{Mechanistic Interpretability} 
The use of mechanistic interpretability **Molnar, Interpretable Machine Learning (2019)** to explain LLMs is an emerging research direction. 
This approach employs various techniques, including logit lens **Lipton, The Mythos (2018)**, probing **Gardner, Evaluating Neural Network Interpretability Methods (2020)**, causal mediation analysis **Robins, Higher-Order Interactions and Confounding in Randomized Experiments (2009)**, sparse autoencoders **Hinton, Reducing the Dimensionality of Data with Neural Networks (1995)**, and visualization **Breuel, Visualization as a Tool for Understanding Neural Networks (2017)**, to identify and analyze the features and circuits of LLMs. 
% ____ provide a high-resolution understanding of the mechanism that LLMs use to answer arithmetic prompts using a set of techniques including activation patching, logit lens, etc. 
% And ____ designs a novel mechanistic interpretability technique to reconstruct the computational graph, through utilizing activation patching. 
% In this work, we aim to provide a mechanistic interpretation on the algorithm transformers with chain-of-thought have learned, to determine whether and to what extent the model is utilizing a correct algorithm to keep track of world state, rather than based on heuristics.