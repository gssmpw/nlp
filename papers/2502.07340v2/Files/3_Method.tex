\section{Methodology}
\label{section:method}
In this section, we will detail our proposed framework \textbf{\OURS} as shown in Figure \ref{figure:model}.
Previous studies \citep{lin2024flame, kang2024unfamiliarfinetuningexamplescontrol,gekhman-etal-2024-fine} find that tuning LLMs on data containing new or unfamiliar knowledge can encourage models to be overconfident and further lead to hallucinations.
% Based on this finding, \OURS~aims to filter out the unfamiliar instruction data and retain data that aligns well with LLM’s learned knowledge for the instruction tuning, thereby aligning the LLM to follow instructions and hallucinate less.
Inspired by this finding, \OURS~aims to filter out the unfamiliar instruction data for the instruction tuning, thereby aligning the LLM to follow instructions and hallucinate less.



\begin{figure*}[t]
    \centering
    % for arxiv
    \vspace{-6.5mm}
    \includegraphics[width=0.97\linewidth]{Figures/model.pdf}
    \caption{
     The process of \OURS.
     \OURS~identifies and selects high-quality instruction data that aligns well with the LLM’s learned knowledge to reduce hallucination.
     Then it uses selected instruction data for training LLMs.
    }
    \label{figure:model}
\end{figure*}


% To comprehensively explore the knowledge within the instruction data, we propose Internal Knowledge Probing (ICP, Section \ref{section:ICP}) and Semantic Equivalence Identification (SEI, Section \ref{section:SEI}) to separately measure how well the LLM understands the knowledge within the given instruction and target response.
% We further introduce an expert-aligned scoring model to measure the data quality, ensuring the difficulty and complexity levels of the selected samples.
% By both considering data quality and avoiding unfamiliar data (Sec. \ref{section:ranking}), we can use the selected data to effectively align LLMs to follow instructions and hallucinate less during the instruction tuning stage.

% \subsection{Overview}
% \label{sec:overview}
% \OURS~generally operates in the following four steps, as shown in Figure \ref{}.

% \noindent
% \textbf{Step 1: Generating Initial Responses for Knowledge Estimation.}
% \
% For a given instruction data $s=(q, r)$, $q$ denotes the instruction, and $r$ denotes the target response.
% For instruction $q$, we sample multiple responses $[r'_1,r'_2,...,r'_K]$, where $K$ represents the number of samples.
% These are produced from a base LLM and we employ few-shot examples \citep{lin2024the} to ensure the generation of coherent responses to follow instructions.

% \noindent
% \textbf{Step 2: Estimating the Knowledge in the Given Instruction via Internal Knowledge Probing.}
% \
% We obtain sentence embeddings from LLM's internal states and utilize differential entropy in embedding space to measure the semantic consistency of the generated responses $[r'_1,...,r'_K]$ for instruction $q$.
% % In this way, we can better exploit the dense semantic information and capture fine-grained semantic relationships among different responses.
% This approach helps us better exploit the dense semantic information and capture fine-grained semantic relationships among different responses.

% \noindent
% \textbf{Step 3: Estimating the Knowledge in Target Response via Semantic Equivalence Identification.}
% \
% We further identify whether the knowledge in the target response aligns well with LLM’s knowledge.
% To mitigate the impact of the linguistic invariance problem in estimating knowledge, we use a well-trained model to cluster the generated responses that convey the same thing.
% Then we apply a designed voting strategy to determine which semantic cluster the target response $r$ belongs to and measure the LLM's familiarity with it.

% % \paragraph{Step 4: Estimating the Quality of Instruction Data.}

% \noindent
% \textbf{Step 4: Ranking, Selecting, and Training.}
% \
% In this step, we first introduce an expert-aligned reward model to measure data quality.
% This is because considering only the LLM's familiarity ignores other characteristics of instruction data (e.g., the complexity of the instruction and the fluency of the response), which in turn limits the quality of selected data.
% By both considering data quality and avoiding unfamiliar data, we can select the most suitable data used for training and align LLMs to follow instructions better and hallucinate less.

% \noindent
%







\subsection{Internal Consistency Probing}
\label{section:IKP}
% To comprehensively measure how familiar the LLM is with instruction data, the first challenge is to measure how well the LLM understands the knowledge within the instruction.
To comprehensively measure the LLM's familiarity with instruction data, the first challenge is to evaluate how well the LLM understands the knowledge within the instructions.
Prompting LLMs to generate multiple responses to the same instruction and measuring how consistent those responses are has been proven to be an effective way \citep{wang2023selfconsistency, chen2024inside}.
This is because if LLMs understand the question and are confident in their answers, they will produce similar responses.
A practical way to measure the consistency of free-form responses is to utilize lexical metrics (e.g., Rouge-L) \citep{DBLP:journals/tmlr/LinT024} or sentence-level confidence scores (e.g., perplexity) \citep{ren2023outofdistribution}.
However, these straightforward strategies neglect highly concentrated semantic information within the internal states of LLMs, and thus fail to capture the fine-grained differences between responses.

Hence, we propose \textbf{Internal Consistency Probing (ICP)} to measure the semantic consistency in the dense embedding space.
For an instruction data $s=(q, r)$, $q$ denotes the instruction, and $r$ denotes the target response.
For instruction $q$, we first sample $K$ responses $[r'_1,...,r'_K]$ from a base LLM and apply few-shot demonstrations \citep{lin2024the} to ensure the coherence of generated responses.
For $K$ generated responses, we use the internal states of the last token of each response in the last layer as the final sentence embeddings $E=[e_1,e_2,...,e_K]$, as it effectively captures the sentence semantics \citep{azaria2023the}.
We further utilize differential entropy (DE) to assess the semantic consistency in continuous embedding space, which is the extension of discrete Shannon entropy:
\begin{align}
{\rm DE}(X)=-{\rm \int}_xf(x)~{\rm log}(f(x))dx.
\end{align}

% For sentence embeddings $E$, we process it as a multivariate Gaussian Distribution $E\sim N(\mu, \Sigma)$.  
We process and treat sentence embeddings $E$ as a multivariate Gaussian Distribution $E\sim N(\mu, \Sigma)$.
Then, the differential entropy can be expressed as:
% \citep{zhouyin2021understandingneuralnetworkslogarithm}, 
% \begin{align}
% \label{eq:de}
% {\rm DE}(E) &= \frac{1}{2}{\rm log}((2\pi e)^d{\rm det} (\Sigma)) \notag \\
% =\frac{1}{2} {\rm log}~&({\rm det} (\Sigma)) + \frac{d}{2}{\rm log}(2\pi+1)
% \end{align}
\begin{align}
\label{eq:de}
{\rm DE}(E) &= \frac{1}{2}{\rm log}((2\pi e)^d{\rm det} (\Sigma)),
\end{align}
where ${\rm det}(\Sigma)$ represents the determinant of the covariance matrix $\Sigma$, $d$ is the dimension of the sentence embedding, and $e$ is the natural constant.
$\Sigma$ denotes the covariance matrix that captures the relationship between $K$ different sentence embeddings, which takes the form:
\begin{align}
\Sigma = \frac{1}{K-1} \sum_{i=1}^{K}(e_i-\mu)(e_i-\mu)^T.
\end{align}

Finally, we measure semantic consistency using ${\rm DE}(E)$, term as \bm{$F_{ins}(q)$} for a given instruction $q$ in data $s$.
Also, ${\rm DE}(E)$ in Eq.(\ref{eq:de}) simplifies to:
{
\small
\begin{align}
\label{eq:de_final}
F_{ins}(q) = \frac{1}{2}{\rm log}{\rm det}(\Sigma) + \frac{d}{2}({\rm log}2\pi+1) = \frac{1}{2}\sum_{i=1}^{d}\lambda_i + G,
\end{align}
}
% where $[\lambda_1,...,\lambda_d]$ denotes the eigenvalues of the covariance matrix $\Sigma$, which can be easily calculated by singular value decomposition.

\noindent
where $\lambda_i$ denotes the $i$-th eigenvalue of the covariance matrix $\Sigma$, which can be easily calculated by singular value decomposition.
$G$ is a constant.
% Finally, we measure semantic consistency via Eq.(\ref{eq:de_final}), term as \bm{$F_{ins}(q)$} for a given instruction $q$ in data $s$:
% Finally, we measure semantic consistency via Eq.(\ref{eq:de_final}), term as \bm{$F_{ins}(q)$} for a given instruction $q$ in data $s$.

% \begin{align}
% % \label{eq:de_final}
% F_{ins}(q) = {\rm DE}(E)
% \end{align}
If the LLM is familiar with the given instruction, the sentence embeddings of generated responses will be highly correlated and the value of $F_{ins}(q)$ will be close to $G$.
On the contrary, when the LLM is indecisive, the model will generate multiple responses with different meanings leading to a significant value of $F_{ins}(q)$. 
In this way, we can exploit the dense semantic information to effectively measure the LLM’s familiarity with the instruction.


\subsection{Semantic Equivalence Identification}
\label{section:SEI}
Another challenge is to estimate the knowledge in the target response and measure the LLM's familiarity with it, since the target response can contain expert-level and unfamiliar knowledge for the LLM.
Training LLMs on such data can encourage hallucinations.
Therefore, we propose \textbf{Semantic Equivalence Identification (SEI)} to measure the LLM’s familiarity with the target response by calculating how many generated responses are semantically equivalent to the target response.
If the target response and more generated responses convey the same meaning, it indicates that the LLM is more familiar with it, thereby training the LLM on this target response will reduce hallucinations.


% Since the internal states of LLMs cannot efficiently represent the target response, as this response is manually labeled or generated by other advanced LLMs instead of generated by the LLM itself.
As the target response is manually labeled or derived from advanced LLMs (e.g., GPT-4) instead of generated by the LLM itself, the internal states of the LLM cannot effectively represent the target response.
Thus, unlike utilizing internal states as the proposed ICP, we calculate LLM's familiarity with target responses using the proposed semantic clustering strategy.
In detail, we first cluster the generated responses that convey the same thing into a semantic cluster.
This is because these responses are often free-form, and multiple generated responses can have the same meaning in different ways.
Therefore, we employ an off-the-shelf natural language inference (NLI) model to cluster these responses.
NLI models are trained to infer the logical entailment between an arbitrary pair of sentences.
Thus, NLI models are well-suited to identify semantic equivalence, as two generated responses mean the same thing if you can entail (i.e. logically imply) each from the other \citep{kuhn2023semantic, jung-etal-2024-impossible}.
In this way, we can use an NLI model to consider two responses that can be entailed from each other as semantically equivalent responses.
Specifically, we test each pair $(r'_i, r'_j)$ of $i$-th and $j$-th generated responses as:
\begin{align}
\small
\begin{split}
    F_{\textit{equivalent}}(r'_i, r'_j) = \mathbb I \Bigl\{ & L_{\textit{NLI}}(r'_i \Rightarrow r'_j) = L_{\textit{entailment}} \,\, \land \\
    & L_{\textit{NLI}}(r'_j \Rightarrow r'_i) =L_{\textit{entailment}} \Bigr\},
\end{split}
\end{align}
where $L_{NLI}$ represents the predictions of the NLI model, $L_{entailment}$ means the label of entailment relation.
$\mathbb I$ is the indicator function.

In this way, we can identify the semantic equivalence of each pair of generated responses and then cluster these generated responses $[r'_1,...,r'_K]$ into $M$ different semantic clusters $[c_1,...,c_M]$, where $m$-th semantic cluster $c_m$ contains $k_m$ generated responses.
Each semantic cluster $c$ is a set of generated responses that convey the same thing.
We further apply the NLI model to determine which semantic cluster the target response $r$ fits in.
Specifically, we use the model to test the target response $r$ and each generated response $r'_i \in [r'_1,...,r'_K]$: 
\begin{align}
\small
\begin{split}
    F_{\textit{equivalent}}(r, r'_i) = \mathbb I \Bigl\{ & L_{\textit{NLI}}(r \Rightarrow r'_i) = L_{\textit{entailment}} \,\, \land \\
    & L_{\textit{NLI}}(r'_i \Rightarrow r) =L_{\textit{entailment}} \Bigr\}.
\end{split}
\end{align}


Using this method, we can determine how many generated responses in a semantic cluster are semantically equivalent to the target response $r$.
For semantic clusters $[c_1,...,c_M]$, the counts of such generated responses are $[k'_1,k'_2,...,k'_M]$.
We use the votes in each semantic cluster to decide which cluster the target response belongs to:
\begin{align}
{\rm Index}(c_{target}) = \mathop{\arg\max}([\frac{k'_1}{k_1}, \frac{k'_2}{k_2},..., \frac{k'_M}{k_M}]).
\end{align}

We calculate the ratio of the number of responses $k_{target}$ in the target cluster $c_{target}$ to the total number of generated responses as \bm{$F_{res}(r)$}:
\begin{align}
\label{eq:res}
F_{res}(r) = \frac{k_{target}}{\sum_{m=1}^M k_m}.
\end{align}

According to Eq.(\ref{eq:res}), when the LLM is familiar with the knowledge within the target response $r$, most of the generated responses will have the same meaning as target response $r$, thus the value of $F_{res}(r)$ will be close to 1.
On the contrary, if the target response contains unseen knowledge, i.e., none of the generated responses have the same meaning as it, the value of $F_{res}(r)$ will be close to 0. 
% To this end, we can effectively estimate the knowledge in the target response.
To this end, we can effectively measure the LLM’s familiarity with the target response.


% comprehensively measure the LLM’s familiarity with instruction data

\subsection{Ranking, Selecting, and Training}
To comprehensively estimate the knowledge and consider both the LLM’s familiarity with the instruction and the target response, we calculate the ratio between $F_{ins}(q)$ and $F_{res}(r)$ for an instruction data $(q,r)$ as the final score:
\begin{align}
F_{familiarity}(q,r) = \frac{F_{res}(q)}{F_{ins}(r)}.
\end{align}

This score effectively measures how well the LLM understands the knowledge in instruction data. 
High $F_{familiarity}$ values indicate that the knowledge in the data aligns well with the LLM, as they show that the generated responses are very consistent for a given instruction (i.e., low $F_{ins}(q)$ values) and the generated responses are very semantically similar to the target response (i.e., high $F_{res} (r)$ values).
Based on the principle of filtering unfamiliar instruction data, the data with high $F_{familiarity}$ should be selected to train the LLM.

% However, our early experiments observed that ranking instruction data solely based on the LLM's familiarity $F_{familiarity}$ leads to introducing only very simple instructions, which in turn hinders the model's ability to follow instructions.
However, our early experiments observed that selecting instruction data solely based on the LLM's familiarity $F_{familiarity}$ significantly reduces hallucinations but hinders the model's ability to follow instructions.
This is because considering only familiarity ignores other important characteristics of instruction data, e.g., the complexity of the instruction and the fluency of the response.
Therefore, we further introduce an expert-aligned quality reward model to measure the data quality.
% We use an expert-labeled preference dataset \citep{DBLP:conf/icde/LiuTZZMZ0HZZMZY24} which contains 3,751 instruction data, and apply the ranking loss to train a scoring model \citep{rei-etal-2020-comet}.
We use an expert-labeled preference dataset \citep{DBLP:conf/icde/LiuTZZMZ0HZZMZY24} which contains 3,751 instruction data to train a reward model (more details are shown in Appendix \ref{appendix:id}).
To take both familiarity $F_{familiarity}(q,r)$ and quality $F_{quality}(q,r)$ into consideration, we define the mixed rank $R^{(i)}_{final}$ for $i$-th data as the average of the two ranks corresponding to the two metrics:
\begin{align}
\small
\label{eq:final_r}
R^{(i)}_{final} = \frac{1}{2}(R^{(i)}_{familiarity} + R^{(i)}_{quality}),
\end{align}
where $R^{(i)}_{familiarity}$ and $R^{(i)}_{quality}$ refer to the ranks of the $i$-th data point in the degree of familiarity and quality.
In this way, we can effectively consider data quality and avoid unfamiliar data.
% , we can use the selected data to effectively align LLMs to follow instructions and hallucinate less during the instruction tuning stage.

Finally, we rank all the instruction data with their corresponding mixed rank $R_{final}$ to select the top-ranked data, e.g., selecting the top 5\% data to apply the supervised finetuning on the LLM.
Based on the proposed \OURS, we can use the suitable data to effectively align LLMs to follow instructions and hallucinate less during the instruction tuning stage.



\label{section:ranking}
