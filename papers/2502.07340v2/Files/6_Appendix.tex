\appendix
\section*{Appendix}
% \noindent This appendix is organized as follows.  

% \begin{itemize}  
%     \item In Section~\ref{appendix:training}, we report the training details, e.g., training datasets and hyperparameters.  
%     \item  In Section~\ref{appendix:baselines}, we go into detail about the baselines used in our experiments.
%     \item In Section~\ref{appendix:evaluations}, we show the details of evaluations, e.g., the introduction of the used benchmarks and evaluation prompts.
%     \item In Section~\ref{char_section}, we list the details of the general characteristics of selected samples.
%     \item In Section~\ref{human_section}, we show the implementation details of human evaluation.
%     \item In Section~\ref{appendix:needle}, we conduct a ``Needle in A HayStack'' experiment to test the ability to utilize information from different positions.
%     \item In Section~\ref{appendix:parameter}, we conduct experiments to explore the impact of hyperparameters.
%     \item In Section~\ref{appendix:case}, we come up with a practical case study to show the effectiveness of GATEAU.
%     \item In Section~\ref{appendix:dis}, we discuss some possible questions, including execution time (Sec. \ref{appendix:dis_time}), experiments in other LLMs (Sec. \ref{appendix:dis_other}), the diversity of selected samples (Sec. \ref{appendix:dis_diversity}), further exploration of HMG (Sec. \ref{appendix:dis_hmg}).
% \end{itemize}



\section{Evaluation}
\label{appendix:eva}

In this section, we will detail the benchmarks and evaluation metrics.

\paragraph{BioGEN. (Factuality)}
This benchmark requires generating short biographies for particular people entities, with a total of 500 samples.
The task of generating people biographies is effective, because generations consist of verifiable statements rather than debatable or subjective ones, and the scope is broad (i.e., covering diverse nationalities, professions, and levels of rarity).
To evaluate each generated response, we follow the FactScore procedure to extract the number of correct and incorrect facts.
Following \citet{min2023factscore}, we first employ GPT-3.5-Turbo-0125 to break a generation into a series of atomic facts and utilize GPT-3.5-Turbo-0125 to compute the percentage of atomic facts supported by a reliable knowledge source.
The percentage of the correct statements (\% FactScore), the number of generated statements (\# Facts), and the ratio of generations that do not abstain from responding (\% Respond) are adopted as the evaluation metrics.

\paragraph{LongFact. (Factuality)}
LongFact requests detailed descriptions for a queried entity and expects a document-level response that is typically very long, often exceeding a thousand tokens.
Specifically, LongFact consists of two subtasks: \textbf{LongFact-Concepts} and \textbf{LongFact-Objects}, separated based on whether the questions ask about concepts or objects.
Following \citet{cheng2024integrativedecodingimprovefactuality}, we use 120 samples of each task for evaluation.
The evaluation process is similar to BioGEN.
We employ GPT-3.5-Turbo-0125 and report the FactScore of LongFact-Concepts and LongFact-Objects, termed as \% Concepts and \% Objects.

\paragraph{FollowRAG. (Faithfulness and Instruction Following)}
FollowRAG aims to assess the model’s ability to follow user instructions in complex multi-document contexts, covering 22 fine-grained atomic instructions across 6 categories. 
The queries in FollowRAG are sourced from 4 QA datasets across NaturalQA \citep{47761}, TriviaQA \citep{joshi-etal-2017-triviaqa}, HotpotQA \citep{yang-etal-2018-hotpotqa}, and WebQSP \citep{Yih2016TheVO}.
It collects and verifies definitions and examples of atomic instructions using rules (e.g., code), excluding those irrelevant to retrieval-augmented generation (RAG) scenarios.
FollowRAG identifies 22 types of instruction constraints, encompassing language, length, structure, and keywords.
Thus, it is suitable to use FollowRAG to evaluate the model’s ability to follow user instructions.
Utilizing the verifiable nature of designed atomic instructions, FollowRAG automates the verification of the model’s adherence to each instruction through code validation.
We calculate the average pass rate for each atomic instruction across all samples to determine the instruction-following score and name this task as \textbf{FollowRAG-Intruction}.
Also, FollowRAG provides retrieved passages as contextual information to evaluate the model's faithfulness.
We name this task as \textbf{FollowRAG-Faithfulness}.
Under new instruction constraints, the model’s target output differs from the gold answers in the original QA dataset, rendering traditional metrics like EM ineffective.
Following \citet{dong2024generalinstructionfollowingalignmentretrievalaugmented}, we use the original gold answers as a reference and utilize GPT-4o-2024-05-13 to evaluate whether the model’s outputs address the questions.
The scoring criteria are as follows: Completely correct (1 point), Partially correct (0.5 points), Completely incorrect (0 points). 
The average score of all samples is taken as the final score for FollowRAG-Faithfulness.

\paragraph{MT-Bench. (Instruction Following)}
MT-Bench is a benchmark consisting of 80 questions, designed to test instruction-following ability, covering common use cases and challenging questions. 
It is also carefully constructed to differentiate chatbots based on their core capabilities, including writing, roleplay, extraction, reasoning, math, coding, STEM knowledge, and social science. 
For evaluation, MT-Bench prompts GPT-4 to act as judges and assess the quality of the models’ responses. 
For each turn, GPT-4 will give a score on a scale of 10. 
Notably, since we only fine-tune on single-turn instruction data (e.g., Alpaca and Alpaca-GPT4), the evaluation is restricted to Turn 1 of MTBench, similar to previous studies \citep{li2024shot}.

\section{Implementation Details}
\label{appendix:id}

\paragraph{Hyperparameters and Devices.}
We use Adam optimizer \citep{kingma2017adammethodstochasticoptimization} to train our model, with a $2\times10^{-5}$ learning rate and a batch size of 16, steers the training across three epochs.
We set the maximum input length for the models to 1024. 
To get the generated initial responses for knowledge estimation, we set the temperature as 0.7 and set hyperparameter $K$ as 10 to generate 10 responses for the given instruction $q$.
We conduct our experiments on NVIDIA A800 80G GPUs with DeepSpeed+ZeRO3 and BF16.

\paragraph{Training of NLI Model.}
Natural language inference (NLI) is a well-studied task in the NLP community.
We employ a well-trained NLI model DeBERTa-large-mnli\footnote{https://huggingface.co/microsoft/deberta-large-mnli} \citep{he2021deberta} (0.3B) as our model to conduct the experiments and report the results.
DeBERTa-large-mnli is the DeBERTa large model fine-tuned with multi-genre natural language inference (MNLI) corpus \citep{N18-1101}, which is a crowd-sourced collection of 433k sentence pairs annotated with textual entailment information.
DeBERTa-large-mnli shows advanced performance in various NLI benchmarks e.g., 91.5\% accuracy on MNLI test set.


\paragraph{Traning of Quality Reward Model.}
Our training data is derived from an expert-revised dataset \citep{DBLP:conf/icde/LiuTZZMZ0HZZMZY24}, which consists of 3,751 instruction pairs from Alpaca refined by linguistic experts to enhance fluency, accuracy, and semantic coherence between instructions and responses.
Meanwhile, \citet{DBLP:conf/icde/LiuTZZMZ0HZZMZY24} employs the edit distance metric (i.e., Levenshtein distance) to assess the quality of the original instruction pair and revised instruction pair.
Thus, we can treat this edit distance metric as the target reward value and use the point-wise loss function to train the reward model.
Specifically, following \citet{ge2024clustering}, we concatenate instruction pairs as text inputs and use the given reward value in the dataset as the target outputs.
We use the average pooling strategy and introduce the additional feed-forward layer to transform the hidden states of the model into a scalar.
Then we use Mean Squared Error as the loss function to train the reward model.
We select DeBERTa-large \citep{he2021deberta} (0.3B) as our model.
We use Adam optimizer to train our model, with a $1.5\times10^{-5}$ learning rate and a batch size of 8.
We train our model on a single NVIDIA A800.

\paragraph{Prompt Template.}
We use the prompt template from Alpaca \citep{alpaca}.
We keep the same template in training and inference.



\begin{figure}
    \centering
    \includegraphics[width=7.6cm]{Figures/number_of_responses.pdf}
    \caption{FactScore results on BioGEN with the different number of generated responses $K$. We conduct the experiments based on LLaMA-3-8B. }
    \label{fig_k}
\end{figure}



\section{Parameter Study}
\label{appendix:para}
We explore the effects of two important hyperparameters in our method: the number of generated responses $K$ and the temperature $T$ during the response generation.
As shown in Figure \ref{fig_k}, increasing the number of generated responses improves the performance of our method, but when the number of generated responses is greater than 10, the performance will be stable.
Therefore, we empirically recommend setting the number of generated responses $K$ to 10, which makes our method effective and efficient.
For the temperature $T$, we find that the performance of the model improves as long as the temperature $T$ is chosen wisely and not at an extreme value (e.g., 0, as this would result in multiple generated responses that are exactly the same).
We recommend that the temperature take a moderate value, as this ensures both that there is diversity in the responses generated and that the generated responses do indeed match the model's perceptions (rather than being too random).
Overall, our method \OURS~is robust to these hyperparameters, making our method easy to follow.

\begin{table}
\scriptsize	
\centering
\resizebox{0.75\linewidth}{!}{
\begin{tabular}{lcc}
\toprule
\textbf{Model} & \textbf{Dataset} & \textbf{BioGEN}\\
\midrule
\rowcolor{blue!5} \textbf{\OURS} & Alpaca & \textbf{50.3}   \\
- $T=0$ & Alpaca & 43.2   \\
- $T=0.2$ & Alpaca & 49.3   \\
- $T=0.7$ (Ours) & Alpaca & \textbf{50.3}   \\
- $T=1.0$ & Alpaca & 50.1   \\
- $T=1.3$ & Alpaca & 49.7   \\
\hdashline[2pt/3pt]
\rowcolor{blue!5} \textbf{\OURS } & Alpaca-GPT4 & \textbf{50.5}   \\
- $T=0$ & Alpaca-GPT4 & 43.6   \\
- $T=0.2$ & Alpaca-GPT4 & 48.9   \\
- $T=0.7$ (Ours) & Alpaca-GPT4 & \textbf{50.5}   \\
- $T=1.0$ & Alpaca-GPT4 & 49.8   \\
- $T=1.3$ & Alpaca-GPT4 & 49.5   \\
\bottomrule
\end{tabular}}
\caption{FactScore results on BioGEN with different temperature $T$ during the response generation. 
We conduct the experiments on LLaMA-3-8B and use 5\% selected instruction data from different datasets.}
\label{tb:para} 
\end{table}



\section{Transferability Study}
\label{appendix:trans}
To verify the transferability of the \OURS~method, we conducted experiments on different foundation models using the Alpaca instruction dataset shown in Table \ref{tb:1-hall} and Table \ref{tb:1-if}.
We select LLaMA \citep{llama1} and Qwen-2 \citep{yang2024qwen2technicalreport} at the 7B size as the new base models.
We aim to gain deeper insights into the applicability of the \OURS~method across different models, providing a reference for further research and applications. 
We find that the \OURS~method is also applicable to other models, showing strong transferability and robustness to other models and further research.
Compared to other baselines, \OURS~significantly reduces hallucinations and keeps a strong ability to follow instructions.




\begin{table*}
\scriptsize
\centering  
\resizebox{\textwidth}{!}{
\begin{tabular}{lcccccccccccccc}
\toprule
\multirow{2}{*}{\textbf{Model}} & \multicolumn{3}{c}{\textbf{BioGEN$^\dag$ }} & \multicolumn{3}{c}{\textbf{LongFact$^\dag$}} & \multicolumn{5}{c}{\textbf{FollowRAG - Faithfulness$^\ddag$}} \\
\cmidrule(lr){2-4} \cmidrule(lr){5-7} \cmidrule(lr){8-12} 
& \textbf{FactScore} & \textbf{Respond} & \textbf{Facts}  & \textbf{Objects} & \textbf{Concepts} & \textbf{Avg.} & \textbf{NaturalQA} & \textbf{TriviaQA} & \textbf{HotpotQA} & \textbf{WebQSP} & \textbf{Avg.}\\
\midrule
\multicolumn{12}{c}{\cellcolor{myyellow} \textbf{LLaMA-1}} \\
Vanilla - 100\% & 38.6 & 100.0 & 16.6 & 84.3 &78.2 &  81.3 & 37.5 	& 	 	50.5 &	 	 	16.0 	 &	 	47.5 	 &	 	37.9  \\
FLAME-DPO$^{fact}$ & 41.2 & 100.0 & 14.8 & 86.7 & 81.2  & 84.0  &41.5 & 			55.0 & 			21.5 		& 	52.5 	& 		42.6 \\
SELF-EVAL & 41.8 & 100.0 & 15.7 & 87.0 & 80.8 & 83.9  &42.5 	 &		56.5 		 &	22.5 		 &	53.5 		 &	43.8 \\
\midrule
IFD - 5\% & 40.2 & 100.0 & 20.1 & 83.2 & 80.4 & 81.8 &38.0 		&	53.5 		&	18.5 	&		49.0 		&	39.8 \\
CaR - 5\% & 39.6 & 100.0 & 18.2  & 85.9 & 80.1 & 83.0 &38.0 		&	53.0 	&		19.0 	&		50.5 		&	40.1 \\
Nuggets - 5\% & 39.3 & 100.0 & 19.4 & 85.1 & 77.3 & 81.2 &39.5 	&		54.5 	&		20.0 		&	50.0 	&		41.0 \\
\rowcolor{blue!5} \textbf{NOVA - 5\%} & \textbf{43.6} & 100.0 & \textbf{21.5} & \textbf{88.1} & \textbf{82.5} & \textbf{85.3}  &\textbf{44.5} 		&	\textbf{58.5} 	&		\textbf{24.0} 	&		\textbf{55.5} 	&		\textbf{45.6} \\
\hdashline[2pt/3pt]
\rowcolor{blue!5} $\Delta$ compared to Vanilla - 100\% & \textcolor[rgb]{0.7,0,0}{+5.0} & - & \textcolor[rgb]{0.7,0,0}{+4.9}  & \textcolor[rgb]{0.7,0,0}{+3.8} & \textcolor[rgb]{0.7,0,0}{+4.3} & \textcolor[rgb]{0.7,0,0}{+4.1} & \textcolor[rgb]{0.7,0,0}{+7.0} 	&		\textcolor[rgb]{0.7,0,0}{+8.0} &			\textcolor[rgb]{0.7,0,0}{+8.0} 	&		\textcolor[rgb]{0.7,0,0}{+8.0} 	&		\textcolor[rgb]{0.7,0,0}{+7.7} \\
\midrule
IFD - 10\% & 40.7 & 100.0 & 19.2 & 85.2 & 80.3  & 82.8  &40.0 	&		54.5 		&	20.0 		&	51.0 	&		41.4  \\
CaR - 10\% & 40.3 & 100.0 & \textbf{21.1} & 83.4 & 79.2  & 81.3& 41.0 	&		52.0 	&		18.0 	&		49.5 		&	40.1 \\
Nuggets - 10\% & 41.0 & 100.0 & 18.8  & 84.2 & 78.6 & 81.4 & 39.5 		& 	53.0 		& 	17.5 		& 	51.0 		& 	40.3 \\
\rowcolor{blue!5} \textbf{NOVA - 10\%} & \textbf{43.2} & 100.0 & 20.7  & \textbf{87.6} & \textbf{83.2} & \textbf{85.4}& \textbf{43.5} 	&		\textbf{59.5} 			& \textbf{22.5} 		&	\textbf{53.0} 	&		\textbf{44.6}  \\
\hdashline[2pt/3pt]
\rowcolor{blue!5} $\Delta$ compared to Vanilla - 100\% & \textcolor[rgb]{0.7,0,0}{+4.6} & - & \textcolor[rgb]{0.7,0,0}{+4.1} & \textcolor[rgb]{0.7,0,0}{+3.3} & \textcolor[rgb]{0.7,0,0}{+5.0}  & \textcolor[rgb]{0.7,0,0}{+4.2} & \textcolor[rgb]{0.7,0,0}{+6.0} 	&		\textcolor[rgb]{0.7,0,0}{+9.0} 		&	\textcolor[rgb]{0.7,0,0}{+6.5} 	&		\textcolor[rgb]{0.7,0,0}{+5.5} 	&		\textcolor[rgb]{0.7,0,0}{+6.7} \\
\midrule
IFD - 15\% & 39.2 & 100.0 & 18.7 & 86.1 & 81.1  & 83.6 & 39.5 	&		52.0 		&	17.5 	&		49.5 	&		39.6 \\
CaR - 15\% & 40.2 & 100.0 & 19.3 & 84.2 & 80.4  & 82.3 & 38.0 	&		51.5 	&		17.0 		&	48.0 		&	38.6 \\
Nuggets - 15\% & 40.9 & 100.0 & 18.1 & 83.3  & 80.0 & 81.7 &40.0 		&	52.5 		&	15.5 		&	50.5 &			39.6 \\
\rowcolor{blue!5} \textbf{NOVA - 15\%} & \textbf{44.1} & 100.0 & \textbf{19.4} & \textbf{89.6} & \textbf{83.7}  & \textbf{86.7} &\textbf{42.5} 		&	\textbf{56.5} 		&	\textbf{23.5} 	&		\textbf{54.5} 	&		\textbf{44.3}   \\
\hdashline[2pt/3pt]
\rowcolor{blue!5} $\Delta$ compared to Vanilla - 100\% & \textcolor[rgb]{0.7,0,0}{+5.5} & - & \textcolor[rgb]{0.7,0,0}{+2.8}  & \textcolor[rgb]{0.7,0,0}{+5.3} & \textcolor[rgb]{0.7,0,0}{+5.5} & \textcolor[rgb]{0.7,0,0}{+5.4} &\textcolor[rgb]{0.7,0,0}{+5.0} 	&		\textcolor[rgb]{0.7,0,0}{+6.0} 	&		\textcolor[rgb]{0.7,0,0}{+7.5} &			\textcolor[rgb]{0.7,0,0}{+7.0} 		&	\textcolor[rgb]{0.7,0,0}{+6.4} \\
\midrule
\multicolumn{12}{c}{\cellcolor{myyellow} \textbf{Qwen-2}} \\
Vanilla - 100\% & 40.3 & 100.0 & 17.3 &83.4&	80.2&	81.8 	 & 39.5 & 57.5 & 18.5 & 49.0 & 41.1  \\
FLAME-DPO$^{fact}$ & 47.1	&100.0	&16.9	&87.8&	82.7&	85.3 	& 44.5 & 58.0 & 20.5 & 53.0 & 44.0  \\
SELF-EVAL & 46.8	&100.0 	&14.2	&88.2&	81.6&	84.9  & 43.5 & 59.0 & 21.0 & 53.0 & 44.1 \\
\midrule
IFD - 5\% &44.2&	100.0 	&16.5&	85.2&	81.2	&83.2  & 42.5 & 56.5 & 20.5 & 53.5 & 43.3\\
CaR - 5\% &45.7	&100.0 &	\textbf{18.6}	&84.1	&81.5	&82.8 & 44.5 & 55.5 & 21.0 & 52.0 & 43.3 \\
Nuggets - 5\% & 46.6&	100.0 	&17.8	&84.7&	81.0 &	82.9 & 43.0 & 57.5 & 21.5 & 52.5 & 43.6 \\
\rowcolor{blue!5} \textbf{NOVA - 5\%} & \textbf{49.1}	&100.0& 	18.3 &	\textbf{90.2}&	\textbf{83.2}	& \textbf{86.7} & \textbf{46.0} & \textbf{59.6} & \textbf{23.5} & \textbf{55.5} & \textbf{46.1} \\
\hdashline[2pt/3pt]
\rowcolor{blue!5} $\Delta$ compared to Vanilla - 100\% & \textcolor[rgb]{0.7,0,0}{+8.8} &	-	& \textcolor[rgb]{0.7,0,0}{+1.0} 	& \textcolor[rgb]{0.7,0,0}{+6.8} 	& \textcolor[rgb]{0.7,0,0}{+3.0} 	& \textcolor[rgb]{0.7,0,0}{+4.9}  & \textcolor[rgb]{0.7,0,0}{+6.5} & \textcolor[rgb]{0.7,0,0}{+2.1} & \textcolor[rgb]{0.7,0,0}{+5.0} & \textcolor[rgb]{0.7,0,0}{+6.5} & \textcolor[rgb]{0.7,0,0}{+5.0}\\
\midrule
IFD - 10\% & 44.5	& 100.0 &	17.8&	84.2	&80.5&	82.4  & 41.5 & 59.5 & 19.5 & 51.0 & 42.9 \\
CaR - 10\%& 45.2&	100.0 &	20.3&	84.5	&79.8&	82.2  & 42.5 & 60.0 & 18.5 & 53.0 & 43.5 \\
Nuggets - 10\% & 46.1	&100.0& 	\textbf{23.5}&	85.2&	79.7	& 82.5  & 42.0 & 60.0 & 20.0 & 51.5 & 43.4 \\
\rowcolor{blue!5} \textbf{NOVA - 10\%} & \textbf{47.5}	& \textbf{100.0} 	 & 18.6  &	\textbf{89.6}	 & \textbf{83.5} &	\textbf{86.6}  & \textbf{45.0} & \textbf{62.0} & \textbf{21.5} & \textbf{53.5} & \textbf{45.5} \\
\hdashline[2pt/3pt]
\rowcolor{blue!5} $\Delta$ compared to Vanilla - 100\% & \textcolor[rgb]{0.7,0,0}{+7.2} &	-	& \textcolor[rgb]{0.7,0,0}{+1.3} 	& \textcolor[rgb]{0.7,0,0}{+6.2} 	& \textcolor[rgb]{0.7,0,0}{+3.3} 	& \textcolor[rgb]{0.7,0,0}{+4.7} & \textcolor[rgb]{0.7,0,0}{+5.5} & \textcolor[rgb]{0.7,0,0}{+4.5} & \textcolor[rgb]{0.7,0,0}{+3.0} & \textcolor[rgb]{0.7,0,0}{+4.5} & \textcolor[rgb]{0.7,0,0}{+4.4} \\
\midrule
IFD - 15\% & 43.7	 &100.0 &	19.2	&82.5	&79.5	&81.0  & 42.0 & 61.5 & 18.5 & 52.0 & 43.5 \\
CaR - 15\% & 44.8&	100.0 	&20.8&	81.2	&81.3	&81.3 & 43.0 & 62.5 & 19.5 & 53.0 & 44.5\\
Nuggets - 15\% & 45.7&	100.0 &	\textbf{21.7} &	80.8	&80.1	&80.5 & 40.5 & 62.5 & 20.0 & 52.5 & 43.9\\
\rowcolor{blue!5} \textbf{NOVA - 15\%} & \textbf{47.2} &	100.0 	& 19.3 &	\textbf{88.8}	& \textbf{82.9} &	\textbf{85.9} & \textbf{44.5} & \textbf{64.5} & \textbf{22.0} & \textbf{54.0} & \textbf{46.3}  \\
\hdashline[2pt/3pt]
\rowcolor{blue!5} $\Delta$ compared to Vanilla - 100\% & \textcolor[rgb]{0.7,0,0}{+6.9} &	-&	\textcolor[rgb]{0.7,0,0}{+2.0} 	& \textcolor[rgb]{0.7,0,0}{+5.4} &	\textcolor[rgb]{0.7,0,0}{+2.7} 	& \textcolor[rgb]{0.7,0,0}{+4.0}  & \textcolor[rgb]{0.7,0,0}{+5.0} & \textcolor[rgb]{0.7,0,0}{+5.0} & \textcolor[rgb]{0.7,0,0}{+3.5} & \textcolor[rgb]{0.7,0,0}{+5.0} & \textcolor[rgb]{0.7,0,0}{+5.2} \\
\bottomrule
\end{tabular}
}
\caption{Results on three hallucination benchmarks. 
$\dag$ indicates the factuality hallucination benchmark. $\ddag$ indicates the faithfulness hallucination benchmark. 
We conduct the experiments based on Alpaca dataset.}
\label{tb:1-hall}
\end{table*}

\begin{table}[t]
\scriptsize	
\centering
\resizebox{\linewidth}{!}{
\begin{tabular}{lcc}
\toprule
\textbf{Model} & \textbf{MT-Bench} & \textbf{FollowRAG-Intruction}\\
\midrule
\multicolumn{3}{c}{\cellcolor{myyellow} \textbf{LLaMA-1}}\\
Vanilla - 100\% & 47.8 & 37.7  \\
FLAME-DPO$^{fact}$ & 40.6 & 37.5 \\
SELF-EVAL & 42.2 & 38.1 \\
\midrule
IFD - 5\% & 48.3 & 37.8 \\
CaR - 5\% & \textbf{50.1} & \textbf{38.2} \\
Nuggets - 5\% & 48.6 & 38.0 \\
\rowcolor{blue!5} \textbf{NOVA - 5\%} & 49.8 & 38.1 \\
\hdashline[2pt/3pt]
\rowcolor{blue!5} $\Delta$ compared to Vanilla - 100\% & \textcolor[rgb]{0.7,0,0}{+2.0} & \textcolor[rgb]{0.7,0,0}{+0.4} \\
\midrule
IFD - 10\% & 47.9 & 38.6 \\
CaR - 10\% & \textbf{49.5} & 38.1 \\
Nuggets - 10\% & 48.4 & 38.7 \\
\rowcolor{blue!5} \textbf{NOVA - 10\%} & 49.3 & \textbf{39.0} \\
\hdashline[2pt/3pt]
\rowcolor{blue!5} $\Delta$ compared to Vanilla - 100\% & \textcolor[rgb]{0.7,0,0}{+1.5} & \textcolor[rgb]{0.7,0,0}{+1.3} \\
\midrule
IFD - 15\% & 48.5 & 38.2 \\
CaR - 15\% & \textbf{50.3} & 37.6 \\
Nuggets - 15\% & 49.5 & \textbf{38.6} \\
\rowcolor{blue!5} \textbf{NOVA - 15\%} & 48.3 & 38.0 \\
\hdashline[2pt/3pt]
\rowcolor{blue!5} $\Delta$ compared to Vanilla - 100\% & \textcolor[rgb]{0.7,0,0}{+0.5} & \textcolor[rgb]{0.7,0,0}{+0.4} \\
% \bottomrule
\midrule
\multicolumn{3}{c}{\cellcolor{myyellow} \textbf{Qwen-2}}\\
Vanilla - 100\% & 50.2 & 38.2 \\
FLAME-DPO$^{fact}$ & 47.8 & 38.7 \\
SELF-EVAL & 49.5 & 37.3 \\
\midrule
IFD - 5\% & 59.5 & 39.2 \\
CaR - 5\% & \textbf{61.2} & 39.5 \\ 
Nuggets - 5\% & 60.3 & \textbf{40.2} \\
\rowcolor{blue!5} \textbf{NOVA - 5\%} & 60.8 & 39.7  \\
\hdashline[2pt/3pt]
\rowcolor{blue!5} $\Delta$ compared to Vanilla - 100\% & \textcolor[rgb]{0.7,0,0}{+10.6} & \textcolor[rgb]{0.7,0,0}{+1.5} \\
\midrule
IFD - 10\% & 59.8 & 40.1 \\
CaR - 10\% & \textbf{60.1} & 40.5 \\
Nuggets - 10\% & 58.8 & \textbf{41.1} \\
\rowcolor{blue!5} \textbf{NOVA - 10\%} & 58.4 & 40.1 \\
\hdashline[2pt/3pt]
\rowcolor{blue!5} $\Delta$ compared to Vanilla - 100\% & \textcolor[rgb]{0.7,0,0}{+8.2} & \textcolor[rgb]{0.7,0,0}{+1.9} \\
\midrule
IFD - 15\% & \textbf{59.3} & \textbf{40.5} \\
CaR - 15\% & 57.5 & 39.8 \\
Nuggets - 15\% & 58.5 & 40.3 \\
\rowcolor{blue!5} \textbf{NOVA - 15\%} & 59.2 & 40.0 \\
\hdashline[2pt/3pt]
\rowcolor{blue!5} $\Delta$ compared to Vanilla - 100\% & \textcolor[rgb]{0.7,0,0}{+9.0} & \textcolor[rgb]{0.7,0,0}{+1.8} \\
\bottomrule
\end{tabular}}
\caption{Results on two instruction-following benchmarks based on Alpaca dataset.}
\label{tb:1-if} 
\end{table}









\section{Design Exploration}
\label{appendix:design}


\textbf{The Design of NLI Model}
\
We further explore the effects of the NLI model on the final performance of \OURS.
We first attempt to analyze the effect of the size of the model on the final results.
Specifically, we introduce DeBERTa-base-mnli\footnote{https://huggingface.co/microsoft/deberta-base-mnli}, DeBERTa-xlarge-mnli\footnote{https://huggingface.co/microsoft/deberta-xlarge-mnli} and DeBERTA-xxlarge-mnli\footnote{https://huggingface.co/microsoft/deberta-v2-xxlarge-mnli}.
As shown in Table \ref{tb:nli_size}, we can find that increasing the size of the NLI model can provide some improvement in the final result, especially when changing the DeBERTa-base-mnli to DeBERTa-large-mnli.
However, continuing to increase the model parameters did not have a significant impact on the final performance. 
Therefore, in order to balance the performance and the inference time of NLI models, we select the DeBERTa-large-mnli to report the final results in our paper.
Meanwhile, we further explore whether we use the advanced LLMs (e.g., GPT-4o and GPT-3.5-Turbo) to directly identify the semantic equivalence and get the correct semantic clusters.
Specifically, we use the prompt shown in Figure \ref{fig:prmpt} to test the generated responses and the target response by querying the advanced LLMs to identify semantic equivalence.
We use the same method as SEI, utilizing the outputs of advanced LLMs to derive semantic clusters and calculate the score of $F_{res}(r)$.
As shown in Table \ref{tb:nli_size}, the direct application of results from advanced LLMs proves effective in identifying semantic equivalence.
Nevertheless, using NLI models delivers competitive or superior final performance while avoiding API-related costs.
Consequently, employing NLI models to identify semantic equivalence is both efficient and effective, substantiating the efficacy of our designed SEI approach.



\begin{table}
\scriptsize	
\centering
\resizebox{0.72\linewidth}{!}{
\begin{tabular}{lcc}
\toprule
\textbf{Model} & \textbf{Size} & \textbf{BioGEN}\\
\midrule
\multicolumn{3}{c}{\cellcolor{myyellow} Alpaca} \\
DeBERTa-base-mnli & 0.1B & 49.7   \\
DeBERTa-large-mnli & 0.3B & 50.3   \\
DeBERTa-xlarge-mnli & 0.7B & 50.1   \\
DeBERTa-xxlarge-mnli & 1.3B & \textbf{50.5}   \\
\hdashline[2pt/3pt]
GPT-3.5-Turbo-0125 & unknown & 49.8 \\
GPT-4o-2024-05-13 & unknown & 50.2 \\
\midrule
\multicolumn{3}{c}{\cellcolor{myyellow} Alpaca- GPT4} \\
DeBERTa-base-mnli & 0.1B & 49.4   \\
DeBERTa-large-mnli & 0.3B & 50.5   \\
DeBERTa-xlarge-mnli & 0.7B & \textbf{51.2}   \\
DeBERTa-xxlarge-mnli & 1.3B & 50.3   \\
\hdashline[2pt/3pt]
GPT-3.5-Turbo-0125 & unknown & 49.2 \\
GPT-4o-2024-05-13 & unknown & 50.0 \\
\bottomrule
\end{tabular}}
\caption{FactScore results on BioGEN with different models. We conduct experiments on LLaMA-3-8B and use selected 5\% data from different datasets.}
\label{tb:nli_size} 
\end{table}

\begin{table}
\scriptsize	
\centering
\resizebox{0.7\linewidth}{!}{
\begin{tabular}{lc}
\toprule
\textbf{Model}  & \textbf{Accuracy}\\
\midrule
\rowcolor{blue!5} \textbf{Our Used Reward Model} &  \textbf{92.0}  \\
GPT-3.5-Turbo-0125  & 85.0 \\
GPT-4o-2024-05-13  & 90.0 \\
\bottomrule
\end{tabular}}
\caption{Accuracy of our used reward model and other advanced LLMs on the constructed test set.}
\label{tb:rm} 
\end{table}

\noindent
\textbf{The Design of Quality Reward Model}
\
We also explore the effectiveness of the quality reward model.
We introduce UltraFeedback \citep{cui2024ultrafeedback} and sample 100 instructions and their corresponding responses as the test set (we find that most of the selected data are in English, but some of the selected instruction types are translation tasks, so a few data contain Chinese responses).
Specifically, for each instruction, we randomly select 2 responses and determine the ranking between the responses based on their labeled scores of instruction-following, honesty, truthfulness, and helpfulness.
Only if all four scores are higher will the response be considered a high-quality response.
Meanwhile, we involve two Ph.D. students to conduct the human evaluation to ensure the correctness of the response ranking of each sample.
Afterwards, we take the instructions and the responses as inputs to each model, and let the model determine the ranking between the responses and calculate the accuracy of the model's prediction of the ranking.
We compare our used Quality Reward Model with GPT-3.5-Turbo-0125 and GPT-4o-2024-05-13.
We use the same prompt for each model as \citet{ge2024clustering}.
As shown in Table \ref{tb:rm}, our reward model achieves better performance, showing the effectiveness of our method.
Despite GPT-4o’s strong alignment with human preferences in most general tasks, our reward model trained on the expert-revised preference dataset can perform better, highlighting the subtle gap between expert preferences and advanced GPT-4o preferences.



\noindent
\textbf{The Design of Obtaining Sentence Embedding. Alpaca-GPT4}
\
For $K$ generated responses, we use the internal states of the last token of each response in the last layer as the final sentence embeddings $E=[e_1,e_2,...,e_K]$, as it effectively captures the sentence semantics \citep{azaria2023the}.
We further explore the different ways to obtain sentence embedding.
Specifically, we first average all the internal states of tokens in the sentence to obtain the sentence embedding (named Average Pooling), which is an intuitive method to get the sentence embedding for decoder-only models.
As shown in Table \ref{tb:sentence}, we can find the design of \OURS~achieves better performance in both reducing hallucinations and following instructions, showing the effectiveness of our designed SEI.
We further explore the internal states from which layer in the LLMs can be used to effectively measure the consistency.
Except for the internal states from the last layer, we select both internal states from the first layer and internal states from the middle layer (layer 16 for LLaMA-3-8B), and use the internal states of the last token to represent the sentence embeddings.
We can find that using sentence embedding in the shallow layer yields inferior performance compared to using sentence embedding in the deep layers, as the shallow layer may not effectively model the rich semantic information.
Overall, extensive experiments show that our design of \OURS~is sound and effective.

\begin{figure*}[t]
    \centering
    \begin{tcolorbox}[title = {The Prompt for Identifying the Semantic Equivalence}, size=title, colframe = white, colbacktitle = black!65!white]
    \noindent   
    Please compare the following two sentences and determine whether they are semantically the same. If they are semantically identical, respond with "Identical"; if not, respond with "Different." Consider the meaning, context, and any implicit nuances of the sentences. \\

    Sentence 1: \{Sentence 1\} \\
    Sentence 2: \{Sentence 1\} \\
    
    Provide your judgment below:
    \end{tcolorbox}
    \caption{The prompt for identifying the semantic equivalence.}
    \label{fig:prmpt}
\end{figure*}


\begin{table}
\scriptsize	
\centering
\resizebox{0.85\linewidth}{!}{
\begin{tabular}{lcc}
\toprule
\textbf{Model} & \textbf{BioGEN} & \textbf{MT-Bench}\\
\midrule
\rowcolor{blue!5} \textbf{\OURS \ - 5\%} & \textbf{50.5} & \textbf{64.6}  \\
-w. Average Pooling & 49.5 & 64.2 \\
-w. The First Layer & 48.9 & 63.7 \\
-w. The Middle Layer & 49.8 & 64.4 \\
-w. The Last Layer (Ours) & 50.5 & 64.6 \\
\bottomrule
\end{tabular}}
\caption{Evaluation results of \OURS~that employ various methods for obtaining sentence embedding. We conduct the experiments based on LLaMA-3-8B and the Alpaca-GPT4 dataset.
We report the FactScore results on BioGEN.}
\label{tb:sentence} 
\end{table}

\begin{table}
\scriptsize	
\centering
\resizebox{1\linewidth}{!}{
\begin{tabular}{lcc}
\toprule
\textbf{Model} & \textbf{BioGEN} & \textbf{MT-Bench}\\
\midrule
\rowcolor{blue!5} \textbf{\OURS~- LLaMA-3-8B - 5\%} & \textbf{50.3} & \textbf{60.5}  \\
-w/o. Few-shot Demonstrations & 50.1 & 59.8 \\
\rowcolor{blue!5} \textbf{\OURS~- LLaMA-1-7B - 5\%} & \textbf{43.6} & \textbf{49.8}  \\
-w/o. Few-shot Demonstrations & 41.9 & 49.2 \\
\bottomrule
\end{tabular}}
\caption{The effects of used few-shot demonstrations. 
We conduct the experiments based on two base models and the Alpaca dataset.
We report the FactScore results on BioGEN.}
\label{tb:de} 
\end{table}




\noindent
\textbf{The Design of Using Few-shot Demonstration.}
\
As detailed in Sec. \ref{section:IKP}, we sample $K$ responses $[r'_1,...,r'_K]$ from a base LLM with few-shot demonstrations \citep{lin2024the} to ensure the coherence of generated responses.
We use the same demonstrations as \citet{lin2024the}.
We further conduct experiments to explore the effects of these used demonstrations.
We find that using few-shot demonstrations in the process of generating responses for a given instruction allows the base LLMs to better express what they have learned in the pre-training stage.
In turn, this will enable ICP and SEI to better estimate the knowledge contained in the instruction data and thus better identify the high-quality instruction data that aligns well with the LLM’s learned knowledge to reduce hallucination and improve instruction-following ability.
At the same time, we find that this strategy improves more for base models with poor capabilities (e.g., LLaMA-1-7B), which is due to the fact that a poor base LLM may hold relevant knowledge in response to a query, yet occasionally falters in conveying accurate information \citep{zhang-etal-2024-self}.

\section{Human Evaluation}
\label{appendix:huamn}
During the human evaluation, the participants follow the principles in Figure \ref{fig:human_evaluation_principles} to make the decision.
For each comparison, three options are given (Ours Wins, Tie, and Vanilla Fine-tuning Wins) and the majority voting determines the final result. 
We invite three Ph.D. students to compare the responses generated by the models.
Before participants begin to make judgments, we describe the principles of our design in detail and ensure that each participant correctly understands the principles.
If the final result can not be determined by the majority voting, we will make the discussion among the participants and vote on the result again.


\begin{figure*}[t]
    \centering
    \begin{tcolorbox}[title = {The Principles of Human Evaluation}, size=title, colframe = white, colbacktitle = black!65!white]
    \noindent   
    You are asked to evaluate the biographies generated by different models.
    You should choose the preferred biography according to the following perspectives independently: \\

    1. \textbf{Factuality}: Whether the biography provides relatively more factual statements over the non-factual statements? \\
    2. \textbf{Helpfulness}: Whether the biography provides useful information? \\
    3. \textbf{Relevance}: Whether the statements contained in the biography relevant to the provided people entity? \\
    4. \textbf{Naturalness}: Whether the biography sound natural and fluent? \\

    Finally, please make a decision among 3 opinions, including Win, Tie, and Loss.

    \end{tcolorbox}
    \caption{The principles of human evaluation.}
    \label{fig:human_evaluation_principles}
\end{figure*}



\section{Case Study for Selected Samples}
\label{appendix:cs-ss}
To evaluate our proposed \OURS~qualitatively, we also select some instruction samples from the Alpaca dataset for case studies as shown in Figure \ref{fig:casestudy}.
Firstly, we can find that simply using $R_{familiarity}$ in Eq. (\ref{eq:final_r}) can effectively identify the simple and straightforward instruction samples that align well with LLM’s knowledge.
On the contrary, the sample ranked last according to $R_{familiarity}$ contains the open-ended instruction and the very subjective target response.
Meanwhile, further using $R_{final}$ in Eq. (\ref{eq:final_r}) that considers characteristics beyond just familiarity $R_{familiarity}$ by introducing the quality reward model further enhances data quality, e.g., the complexity of instructions.
We can also observe that the sample ranked last according to $R_{final}$ also contains the open-ended instruction and the detailed target response, which may introduce familiar knowledge to LLMs and further promote hallucinations.





\newpage
\begin{figure*} 
    % \small
    \centering
    \begin{tcolorbox}[title = {Case Study for Selected Samples}, size=title, colframe = white, colbacktitle = black!65!white]
    \noindent
    \

    \faLaughBeam~\textbf{Sample ranked first according to $R_{familiarity}$ in Eq. (\ref{eq:final_r}):}
    
    \textbf{Instruction:} Find a word that is opposite in meaning to the given word: assured.
    
    \textbf{Target Response:} Doubtful. \\ \\

    \faFrown~\textbf{Sample ranked last according to $R_{familiarity}$ in Eq. (\ref{eq:final_r}):}
    
    \textbf{Instruction:} Could you pick a set of five songs which will fit our theme: a summer night's party?
    
    \textbf{Target Response:} \\ 1. Billie Eilish – “Bad Guy” \\ 2. Ed Sheeran – “Shape of You” \\ 3. Maroon 5 – “Summer Nights” \\ 4. DNCE – “Cake by the Ocean” \\ 5. Drake – “In My Feelings”. \\ \\
    
    \faLaughBeam~\textbf{Sample ranked first according to $R_{final}$ in Eq. (\ref{eq:final_r}):}
    
    \textbf{Instruction:} Given a dictionary, create a JSON representation. dict = \{"name": "John", "age": 18, "is\_adult": True\}
    
    \textbf{Target Response:} \\ \{\\ \quad "name": "John", \\ "age": 18, \\ \quad "is\_adult": true \\ \} \\ \\

    \faFrown~\textbf{Sample ranked last according to $R_{final}$ in Eq. (\ref{eq:final_r}): }
    
    \textbf{Instruction:} Describe how to impress a customer.
    
    \textbf{Target Response:} Listen to the customer carefully, acknowledge their needs and desires, make sure to follow up on commitments, and strive to exceed expectations. \\ \\


    \end{tcolorbox}
    \caption{Case study for selected samples.}
    \label{fig:casestudy}
\end{figure*}