\section{Conclusion}
\label{subsection:conclusion}
In this paper, we introduce \OURS, a novel framework designed to identify high-quality data that aligns well with the LLMâ€™s learned knowledge to reduce hallucination.
% Our proposed method includes Internal Consistency Probing and Semantic Equivalence Identification, which are designed to separately measure the LLM's understanding of the given instruction and target response.
% In this way, we can measure the familiarity of the LLM with the instruction data and prevent the model from being trained on unfamiliar data, thereby reducing hallucinations.
NOVA includes Internal Consistency Probing and Semantic Equivalence Identification, which are designed to separately measure the LLM's familiarity with the given instruction and target response, then prevent the model from being trained on unfamiliar data, thereby reducing hallucinations.
Lastly, we introduce an expert-aligned reward model, considering characteristics beyond just familiarity to enhance data quality.
By considering data quality and avoiding unfamiliar data, we can use the selected data to effectively align LLMs to follow instructions and hallucinate less in the instruction tuning stage.
Experiments and analysis show the effectiveness of \OURS.

\section*{Limitations}
Although empirical experiments have confirmed the effectiveness of the proposed \OURS, two major limitations remain. 
Firstly, our proposed method requires LLMs to generate multiple responses for the given instruction, which introduces additional execution time.
However, it is worth noting that this additional execution time is used to perform offline data filtering, our proposed method does not introduce additional time overhead in the inference phase.
Additionally, \OURS~is primarily used for single-turn instruction data filtering, thus exploring its application in multi-turn scenarios presents an attractive direction for future research.