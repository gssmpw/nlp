\section{Introduction}
\label{section:introduction}
Alignment is a critical procedure to ensure large language models (LLMs) follow user instructions \citep{ OpenAI2023GPT4TR, yang2024qwen2technicalreport}. 
% A practical approach for alignment is to perform instruction tuning on the instruction data \citep{ouyang2022training, Bai2022TrainingAH}.
Despite significant progress in LLM alignment and instruction tuning \citep{ouyang2022training, Bai2022TrainingAH}, state-of-the-art aligned LLMs still generate statements that appear credible but are actually incorrect, referred to as hallucinations \citep{ji-survey, hit-survey}.
Such hallucinations can undermine the trustworthiness of LLMs in real-world applications \citep{si2023spokenwoz, min2023factscore, rawte2023surveyhallucinationlargefoundation, wei2024long}.


\begin{figure}
    \centering
    \includegraphics[width=8cm]{Figures/intro.pdf}
    \caption{Instruction following ability on MT-Bench vs hallucination on LongFact. \textbf{\OURS} simultaneously aligns LLMs to follow instructions and hallucinate less.}
    \label{fig_example}
\end{figure}

Previous studies \citep{kang2024unfamiliarfinetuningexamplescontrol, gekhman-etal-2024-fine, lin2024flame} indicate that tuning LLMs on instruction data that contains new or unfamiliar knowledge can encourage models to be overconfident and promote hallucinations. 
In other words, once the knowledge in the instruction data has not been learned during the pre-training stage of LLMs, the fine-tuned LLMs tend to produce more errors when generating responses.
Therefore, there is a dilemma in instruction tuning:
On the one hand, the LLMs need to learn to follow user instructions during this stage, which is crucial for user interaction in real-world applications \citep{wang2023how, chen2024alpagasus}; 
On the other hand, using high-quality data (whether manually labeled or generated by other advanced LLMs) for instruction tuning can introduce unfamiliar knowledge to LLMs, thereby encouraging hallucinations \citep{kang2024unfamiliarfinetuningexamplescontrol, lin2024flame}.
Thus, a critical question arises: \textit{\textbf{How can we align LLMs to follow instructions and hallucinate less during the instruction tuning stage?}}



Certain efforts \citep{lin2024flame, zhang-etal-2024-self, tian2024finetuning} apply reinforcement learning (RL) to teach LLMs to hallucinate less after the instruction tuning stage.
For example, \citet{zhang-etal-2024-self} leverages the self-evaluation capability of an LLM and employs GPT-3.5-turbo \citep{chatgpt2022} to create preference data, subsequently aligning the LLM with direct preference optimization (DPO) \citep{dpo}.
% However, these methods necessitate additional corpus and API costs of the close-source LLMs, making them inefficient.
% Previous work \citep{lin2024flame} also finds that such RL methods can weaken the model's ability to follow instructions.
However, \citet{lin2024flame} finds that such RL-based methods can weaken the model's ability to follow instructions.
These methods also necessitate additional preference data and API costs from the advanced LLMs, making them inefficient.
% Different from RL methods, an intuitive strategy to align LLMs to follow instructions and hallucinate less is to filter out the unfamiliar instruction data for the instruction tuning.
Different from RL-based methods, an intuitive strategy to align LLMs to follow instructions and hallucinate less is to filter out the instruction data that contains unfamiliar knowledge for the instruction tuning.
Unfortunately, previous studies \citep{ liu2024what, cao2024instructionmininginstructiondata} solely focus on selecting high-quality data to improve the instruction-following abilities of LLMs.
Even worse, these selected high-quality data may present more unknown knowledge to the LLM and further encourage hallucinations, as these data may contain responses with expert-level knowledge and often delve into advanced levels of detail.

Therefore, we introduce \textbf{\OURS}, which includes \textbf{I\underline{n}ternal C\underline{o}nsistency Probing (ICP)} and \textbf{Semantic Equi\underline{v}alence Identific\underline{a}tion (SEI)}, a framework designed to identify high-quality instruction samples that align well with LLMâ€™s knowledge, thereby aligning the LLM to follow instructions and hallucinate less.
\OURS~initially uses ICP and SEI to measure how well the LLM understands the knowledge in the given instruction and target response. 
For ICP, we prompt the LLM to generate multiple responses to demonstrate what it has learned about a specific instruction during pre-training.
Then we use the internal states produced by the LLM to assess how consistent the generated responses are. 
If the internal states of these responses exhibit greater consistency for the instruction, it indicates that the LLM has internalized the relevant knowledge during pre-training.
For SEI, we first integrate a well-trained model to classify the generated responses that convey the same thing into a semantic cluster.
Next, we employ the designed voting strategy to identify which semantic cluster the target response fits in.
This helps us find out how many generated responses are semantically equivalent to the target response, indicating how well the LLM understands the target response.
If the target response matches well with the largest cluster, it shows the LLM is familiar with its content.
Based on ICP and SEI, we can measure how well the model understands the knowledge in instruction data and avoid training it on unfamiliar data to reduce hallucinations.
Lastly, to ensure the quality of selected samples, we introduce an expert-aligned quality reward model, considering characteristics beyond just familiarity, e.g., the complexity of instructions and the fluency of responses.
% Lastly, we introduce an expert-aligned quality reward model, considering characteristics beyond just familiarity, e.g., the complexity of instructions and the fluency of responses, to enhance data quality.
By considering data quality and avoiding unfamiliar data, we can use the selected data to effectively align LLMs to follow instructions and hallucinate less.

We conduct extensive experiments to evaluate the effectiveness of \OURS~from both instruction-following and hallucination perspectives.
Experimental results demonstrate that \OURS~significantly reduces hallucinations while maintaining a competitive ability to follow instructions.








