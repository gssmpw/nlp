\section{Experiment}
\label{section:experiment}
% In this section, we evaluate the efficacy of our proposed framework \textbf{\OURS}~across different benchmarks, including the instruction-following benchmark, factuality hallucination benchmark, and faithfulness hallucination benchmark.
% In this section, we evaluate the efficacy of our proposed framework \textbf{\OURS}~across different benchmarks.

In this section, we conduct experiments and provide analyses to justify the effectiveness of \OURS.



\subsection{Setup}

\textbf{Instruction Dataset.}
\
We conduct instruction tuning with two different instruction datasets.
\textbf{Alpaca} \citep{alpaca} contains 52,002 samples that are created by employing Text-Davinci-003 model \citep{ouyang2022training} and Self-instruct framework \citep{wang-etal-2023-self-instruct}.
\textbf{Alpaca-GPT4} \citep{peng2023instructiontuninggpt4} further employs more powerful GPT-4 \citep{GPT-4} to get high-quality instruction data.


\begin{table*}[t]
\scriptsize
\centering  
\resizebox{0.97\textwidth}{!}{
% \begin{tabular}{l|ccccc|ccccc|ccccc}
\begin{tabular}{lccccccccccccccc}
\toprule
\multirow{2}{*}{\textbf{Model}} & \multicolumn{3}{c}{\textbf{BioGEN$^\dag$ }} & \multicolumn{3}{c}{\textbf{LongFact$^\dag$}} & \multicolumn{5}{c}{\textbf{FollowRAG - Faithfulness$^\ddag$}} \\
\cmidrule(lr){2-4} \cmidrule(lr){5-7} \cmidrule(lr){8-12} 
& \textbf{FactScore} & \textbf{Respond} & \textbf{Facts}  & \textbf{Objects} & \textbf{Concepts} & \textbf{Avg.} & \textbf{NaturalQA} & \textbf{TriviaQA} & \textbf{HotpotQA} & \textbf{WebQSP} & \textbf{Avg.} \\
\midrule
\rowcolor{mygray} \multicolumn{12}{c}{\cellcolor{myyellow} \textbf{Alpaca}} \\
Vanilla - 100\% & 42.4 & 100.0 & 17.1 & 85.8 & 80.3 & 83.1 & 40.5 & 53.5 & 16.0 & 49.5 & 39.9 \\
FLAME-DPO$^{fact}$ & 47.2 & 100.0 & 15.6 & 88.3 & 81.2 & 84.8 & 43.5 & 57.0 & 17.5 & 52.0 & 42.5 \\
SELF-EVAL & 48.3 & 100.0 & 16.9 & 87.8 & 81.0 & 84.4 & 43.0 & 58.0 & 16.5 & 52.5 & 42.5 \\
\midrule
IFD - 5\% & 48.1 & 100.0 & \textbf{21.0} & 87.2 & 80.5 & 83.9 & 41.5 & 57.0 & 15.5 & 51.5 & 41.4 \\
CaR - 5\% & 47.9 & 100.0 & 16.2 & 86.6 & 79.1 & 82.9 & 42.5 & 58.0 & 16.5 & 51.0 & 42.0 \\
Nuggets - 5\% & 48.2 & 100.0 & 18.3 & 88.6 & 81.2 & 84.9 & 42.5 & 56.0 & 16.5 & 51.0 & 41.5 \\
\rowcolor{blue!5} \textbf{\OURS} - 5\% & \textbf{50.3} & 100.0 & 17.9 & \textbf{92.4} & \textbf{82.7} & \textbf{87.6} & \textbf{46.5} & \textbf{60.0} & \textbf{19.0} & \textbf{53.5} & \textbf{44.8} \\
\hdashline[2pt/3pt]
\rowcolor{blue!5}  $\Delta$ compared to Vanilla - 100\%  & \textcolor[rgb]{0.7,0,0}{+7.9} & - & \textcolor[rgb]{0.7,0,0}{+0.8} & \textcolor[rgb]{0.7,0,0}{+6.6} & \textcolor[rgb]{0.7,0,0}{+2.4} & \textcolor[rgb]{0.7,0,0}{+4.5} & \textcolor[rgb]{0.7,0,0}{+6.0} & \textcolor[rgb]{0.7,0,0}{+6.5} & \textcolor[rgb]{0.7,0,0}{+3.0} & \textcolor[rgb]{0.7,0,0}{+4.0} & \textcolor[rgb]{0.7,0,0}{+4.9} \\
\midrule
IFD - 10\% & 43.2 & 100.0 & 20.5 & 86.3 & 79.2 & 82.8 & 40.5 & 60.0 & 17.5 & 53.5 & 42.9 \\
CaR - 10\% & 45.2 & 100.0 & 24.3 & 87.1 & 81.3 & 84.2 & 44.0 & 59.5 & 18.0 & 48.5 & 42.5 \\
Nuggets - 10\% & 45.8 & 100.0 & \textbf{27.1} & 86.7 & 80.4 & 83.6 & 43.0 & 58.5 & 17.0 & 52.5 & 42.8 \\
\rowcolor{blue!5} \textbf{\OURS} - 10\% & \textbf{46.8} & 100.0 & 18.4 & \textbf{89.1} & \textbf{81.6} & \textbf{85.4} & \textbf{46.0} & \textbf{63.0} & \textbf{20.0} & \textbf{59.0} & \textbf{47.0} \\
\hdashline[2pt/3pt]
\rowcolor{blue!5}  $\Delta$ compared to Vanilla - 100\%  & \textcolor[rgb]{0.7,0,0}{+4.4} & - & \textcolor[rgb]{0.7,0,0}{+1.3} & \textcolor[rgb]{0.7,0,0}{+3.3} & \textcolor[rgb]{0.7,0,0}{+1.3} & \textcolor[rgb]{0.7,0,0}{+2.3} & \textcolor[rgb]{0.7,0,0}{+5.5} & \textcolor[rgb]{0.7,0,0}{+9.5} & \textcolor[rgb]{0.7,0,0}{+4.0} & \textcolor[rgb]{0.7,0,0}{+9.5} & \textcolor[rgb]{0.7,0,0}{+7.1} \\
\midrule
IFD - 15\% & 42.2 & 100.0 & 19.4 & 84.7 & 80.7 & 82.7 & 43.5 & 63.0 & 23.0 & 50.0 & 44.9 \\
CaR - 15\% & 43.9 & 100.0 & 20.9 & 86.4 & 78.0 & 82.2 & 45.5 & 61.5 & 22.0 & 48.0 & 44.3 \\
Nuggets - 15\% & 44.3 & 100.0 & \textbf{23.4} & 86.5 & 80.1 & 83.3 & 45.0 & 62.5 & 21.0 & 49.0 & 44.4 \\
\rowcolor{blue!5} \textbf{\OURS} - 15\% & \textbf{45.9} & 100.0 & 18.7 & \textbf{88.1} & \textbf{82.1} & \textbf{85.1} & \textbf{48.5} & \textbf{68.0} & \textbf{25.0} & \textbf{52.0} & \textbf{48.4} \\
\hdashline[2pt/3pt]
\rowcolor{blue!5}  $\Delta$ compared to Vanilla - 100\%  & \textcolor[rgb]{0.7,0,0}{+3.5} & - & \textcolor[rgb]{0.7,0,0}{+1.6}  & \textcolor[rgb]{0.7,0,0}{+2.3} & \textcolor[rgb]{0.7,0,0}{+1.8} & \textcolor[rgb]{0.7,0,0}{+2.0} & \textcolor[rgb]{0.7,0,0}{+8.0} & \textcolor[rgb]{0.7,0,0}{+14.5} & \textcolor[rgb]{0.7,0,0}{+9.0} & \textcolor[rgb]{0.7,0,0}{+2.5} & \textcolor[rgb]{0.7,0,0}{+8.5} \\
\midrule
\multicolumn{16}{c}{\cellcolor{myyellow} \textbf{Alpaca - GPT4}} \\
Vanilla - 100\% & 41.9 & 100.0 & 32.0 & 84.7 & 80.4 & 82.6 & 39.5 & 49.5 & 14.5 & 49.0 & 38.1 \\
FLAME-DPO$^{fact}$ & 46.3 & 100.0 & 27.6 & 87.3 & 84.1 & 85.7 & 42.0 & 55.5 & 16.5 & 52.0 & 41.5 \\
SELF-EVAL & 47.2 & 100.0 & 31.6 & 86.7 & 83.7 & 85.2 & 43.5 & 59.0 & 15.5 & 51.5 & 42.4 \\
\midrule
IFD - 5\% & 46.7 & 100.0 & 39.2 & 84.4 & 79.6 & 82.0 & 42.5 & 58.0 & 16.5 & 52.0 & 42.3 \\
CaR - 5\% & 46.9 & 100.0 & 41.1 & 86.2 & 81.1 & 83.7 & 43.5 & 57.5 & 17.0 & 51.5 & 42.4 \\
Nuggets - 5\% & 47.2 & 100.0 & \textbf{42.3} & 87.0 & 82.3 & 84.7 & 41.0 & 56.0 & 17.0 & 52.0 & 41.5 \\
\rowcolor{blue!5} \textbf{\OURS} - 5\% & \textbf{50.5} & 100.0 & 33.8 & \textbf{90.1} & \textbf{85.2} & \textbf{87.7} & \textbf{45.0} & \textbf{62.0} & \textbf{20.5} & \textbf{53.5} & \textbf{45.3} \\
\hdashline[2pt/3pt]
\rowcolor{blue!5}  $\Delta$ compared to Vanilla - 100\%  & \textcolor[rgb]{0.7,0,0}{+8.6} & - & \textcolor[rgb]{0.7,0,0}{+1.8} & \textcolor[rgb]{0.7,0,0}{+5.4} & \textcolor[rgb]{0.7,0,0}{+4.8} & \textcolor[rgb]{0.7,0,0}{+5.1} & \textcolor[rgb]{0.7,0,0}{+5.5} & \textcolor[rgb]{0.7,0,0}{+12.5} & \textcolor[rgb]{0.7,0,0}{+6.0} & \textcolor[rgb]{0.7,0,0}{+4.5} & \textcolor[rgb]{0.7,0,0}{+7.2} \\
\midrule
IFD - 10\% & 43.6 & 100.0 & \textbf{39.2} & 86.5 & 77.8 & 82.2 & 40.5 & 56.0 & 16.0 & 49.5 & 40.5 \\
CaR - 10\% & 45.9 & 100.0 & 38.0 & 87.1 & 78.3 & 82.7 & 43.0 & 55.0 & 15.5 & 48.0 & 40.4 \\
Nuggets - 10\% & 46.8 & 100.0 & 35.7 & 88.2 & 80.1 & 84.2 & 41.5 & 54.5 & 16.5 & 50.0 & 40.6 \\
\rowcolor{blue!5} \textbf{\OURS} - 10\% & \textbf{48.1} & 100.0 & 32.3 & \textbf{90.6} & \textbf{81.8} & \textbf{86.2} & \textbf{44.5} & \textbf{59.0} & \textbf{18.0} & \textbf{51.0} & \textbf{43.1} \\
\hdashline[2pt/3pt]
\rowcolor{blue!5}  $\Delta$ compared to Vanilla - 100\%  & \textcolor[rgb]{0.7,0,0}{+6.2} & - & \textcolor[rgb]{0.7,0,0}{+0.3}  & \textcolor[rgb]{0.7,0,0}{+5.9} & \textcolor[rgb]{0.7,0,0}{+1.4} & \textcolor[rgb]{0.7,0,0}{+3.6} & \textcolor[rgb]{0.7,0,0}{+5.0} & \textcolor[rgb]{0.7,0,0}{+9.5} & \textcolor[rgb]{0.7,0,0}{+3.5} & \textcolor[rgb]{0.7,0,0}{+2.0} & \textcolor[rgb]{0.7,0,0}{+5.0} \\
\midrule
IFD - 15\% & 42.9 & 100.0 & 32.2 & 85.2 & 80.3 & 82.8 & 46.0 & 54.5 & 15.0 & 52.0 & 41.9 \\
CaR - 15\% & 44.6 & 100.0 & 33.6 & 85.8 & 81.5 & 83.7 & 43.5 & 55.0 & 18.0 & 53.5 & 42.5 \\
Nuggets - 15\% & 44.8 & 100.0 & \textbf{34.5} & 86.1 & 80.7 & 83.4 & 45.0 & 52.0 & 16.0 & 53.0 & 41.5 \\
\rowcolor{blue!5} \textbf{\OURS} - 15\% & \textbf{46.9} & 100.0 & 32.1 & \textbf{88.0} & \textbf{82.5} & \textbf{85.3} & \textbf{49.5} & \textbf{56.5} & \textbf{18.5} & \textbf{55.0} & \textbf{44.9} \\
\hdashline[2pt/3pt]
\rowcolor{blue!5}  $\Delta$ compared to Vanilla - 100\%  & \textcolor[rgb]{0.7,0,0}{+5.0} & - & \textcolor[rgb]{0.7,0,0}{+0.1}  & \textcolor[rgb]{0.7,0,0}{+3.3} & \textcolor[rgb]{0.7,0,0}{+2.1} & \textcolor[rgb]{0.7,0,0}{+2.7} & \textcolor[rgb]{0.7,0,0}{+10.0} & \textcolor[rgb]{0.7,0,0}{+7.0} & \textcolor[rgb]{0.7,0,0}{+4.0} & \textcolor[rgb]{0.7,0,0}{+6.0} & \textcolor[rgb]{0.7,0,0}{+6.8} \\
\bottomrule
\end{tabular}
}
\caption{Results on three hallucination benchmarks. 
$\dag$ indicates the factuality hallucination benchmark. $\ddag$ indicates the faithfulness hallucination benchmark. 
We conduct the experiments based on LLaMA-3-8B.}
\label{tb:main-hall}
\end{table*}


\noindent
\textbf{Evaluation.}
\
To evaluate our method comprehensively, we select widely adopted benchmarks for the targeted abilities.
(1) Factuality hallucination benchmark: BioGEN \citep{min2023factscore} and LongFact \citep{wei2024longformfactualitylargelanguage};
(2) Faithfulness hallucination benchmark: FollowRAG-Faithfulness \citep{dong2024generalinstructionfollowingalignmentretrievalaugmented}, including 4 different QA datasets;
(3) Instruction-following benchmark: MT-Bench \citep{zheng2023judgingllmasajudgemtbenchchatbot} and FollowRAG-Instruction. 
% For BioGEN, we present the FactScore percentage \citep{min2023factscore}, the Respond ratio, and the number of generated Facts.
% For LongFact, we report the FactScore percentage of two subtasks, including LongFact-Objects and LongFact-Concepts, referred to as Objects and Concepts.
% FollowRAG measures instruction-following ability (FollowRAG-Instruction) by calculating the average pass rate for each atomic instruction and assesses faithfulness (FollowRAG-Faithfulness) by utilizing GPT-4 to evaluate whether the LLMs utilize the given passages and correctly address the questions.
% For MT-Bench, we use GPT-4 to rate the responses generated by models, which evaluates instruction-following proficiency.
Comprehensive descriptions of tasks, datasets, and evaluation metrics are detailed in Appendix \ref{appendix:eva}.



\noindent
\textbf{Baselines.}
\
We compare several strong baselines, including (1) Vanilla Instruction Tuning: \textbf{Vanilla - 100\%} fine-tunes the model on the whole instruction dataset; 
(2) Instruction Data Filtering Methods: 
\textbf{IFD} \citep{li-etal-2024-quantity} proposes instruction-following difficulty to select a subset of instruction data.
\textbf{CaR} \citep{ge2024clustering} simultaneously considers the data quality and diversity by introducing two scoring methods.
\textbf{Nuggets} \citep{li2024shot}
focuses on selecting high-quality data by identifying samples that notably boost the performance of different tasks after being learned as one-shot instances;
(3) RL-based Methods: \textbf{FLAME-DPO$^{\rm fact}$} \citep{lin2024flame} introduces atomic fact decomposition and retrieval augmented claim verification to construct preference data and apply DPO.
\textbf{SELF-EVAL} \citep{zhang-etal-2024-self} leverages the self-evaluation capability of LLMs and employs GPT-3.5 to create preference data, aligning the LLM with DPO.
We apply these RL-based methods after tuning LLMs on the whole instruction dataset.

\noindent
\textbf{Implementation Details.}
\
Our main experiments are conducted on LLaMA-3-8B and LLaMA-3-70B \citep{llama3}.
% We also use LLaMA-1-7B \citep{llama1} to analyze the effectiveness of our method in the Appendix.
% More details are shown in Appendix \ref{appendix:id}, e.g., the training of quality reward model and hyperparameters.
More implementation details are shown in Appendix \ref{appendix:id}, e.g., the training of quality reward model and hyperparameters.


\begin{table}[h]
\scriptsize	
\centering
\resizebox{0.91\linewidth}{!}{
\begin{tabular}{lcc}
\toprule
\textbf{Model} & \textbf{MT-Bench} & \textbf{FollowRAG-Intruction}\\
\midrule
\multicolumn{3}{c}{\cellcolor{myyellow} \textbf{Alpaca}}\\
Vanilla - 100\% & 51.9 & 38.7  \\
FLAME-DPO$^{fact}$ & 46.7 & 39.2 \\
SELF-EVAL & 48.3 & 38.5 \\
% \hline
\midrule
IFD - 5\% & 60.1 & 39.6 \\
CaR - 5\% & 56.6 & \textbf{41.4} \\
Nuggets - 5\% & 60.0 & 40.6 \\
\rowcolor{blue!5} \textbf{\OURS \ - 5\%} & \textbf{60.5} & 39.1 \\
\hdashline[2pt/3pt]
\rowcolor{blue!5} $\Delta$ compared to Vanilla - 100\% & \textcolor[rgb]{0.7,0,0}{+8.6} & \textcolor[rgb]{0.7,0,0}{+0.4} \\
% \hline
\midrule
IFD - 10\% & 57.2 & 40.4 \\
CaR - 10\% & \textbf{58.3} & \textbf{42.3} \\
Nuggets - 10\% & 58.2 & 41.1 \\
\rowcolor{blue!5} \textbf{\OURS\ - 10\%} & 56.6 & 38.8 \\
\hdashline[2pt/3pt]
\rowcolor{blue!5} $\Delta$ compared to Vanilla - 100\% & \textcolor[rgb]{0.7,0,0}{+4.7} & \textcolor[rgb]{0.7,0,0}{+0.1} \\
% \hline
\midrule
IFD - 15\% & 56.0 & 40.2 \\
CaR - 15\% & \textbf{57.4} & \textbf{41.0} \\
Nuggets - 15\% & 57.0 & 40.6 \\
\rowcolor{blue!5} \textbf{\OURS\ - 15\%} & 57.2 & 40.1 \\
\hdashline[2pt/3pt]
\rowcolor{blue!5} $\Delta$ compared to Vanilla - 100\% & \textcolor[rgb]{0.7,0,0}{+5.3} & \textcolor[rgb]{0.7,0,0}{+1.4} \\
\midrule
\multicolumn{3}{c}{\cellcolor{myyellow} \textbf{Alpaca - GPT4}}\\
Vanilla - 100\% & 64.3 & 36.9 \\
FLAME-DPO$^{fact}$ & 56.2 & 37.2 \\
SELF-EVAL & 53.1 & 36.5 \\
% \hline
\midrule
IFD - 5\% & 65.0 & 37.0 \\
CaR - 5\% & 65.4 & 38.0 \\
Nuggets - 5\% & \textbf{66.2} & \textbf{38.5} \\
\rowcolor{blue!5} \textbf{\OURS - 5\%} & 64.6 & 37.8 \\
\hdashline[2pt/3pt]
\rowcolor{blue!5} $\Delta$ compared to Vanilla - 100\% & \textcolor[rgb]{0.7,0,0}{+0.3} & \textcolor[rgb]{0.7,0,0}{+0.9} \\
% \hline
\midrule
IFD - 10\% & 65.0 & 37.8 \\
CaR - 10\% & 65.8 & 38.0 \\
Nuggets - 10\% & \textbf{67.5} & 38.0 \\
\rowcolor{blue!5} \textbf{\OURS\ - 10\%} & 64.6 & \textbf{39.1} \\
\hdashline[2pt/3pt]
\rowcolor{blue!5} $\Delta$ compared to Vanilla - 100\% & \textcolor[rgb]{0.7,0,0}{+0.3} & \textcolor[rgb]{0.7,0,0}{+2.1} \\
% \hline
\midrule
IFD - 15\% & 62.3 & 37.9 \\
CaR - 15\% & 61.1 & \textbf{38.1} \\
Nuggets - 15\% & \textbf{66.5} & 38.0 \\
\rowcolor{blue!5} \textbf{\OURS\ - 15\%} & 64.5 & 37.5 \\
\hdashline[2pt/3pt]
\rowcolor{blue!5} $\Delta$ compared to Vanilla - 100\% & \textcolor[rgb]{0.7,0,0}{+0.2} & \textcolor[rgb]{0.7,0,0}{+0.5} \\
\bottomrule
\end{tabular}}
\caption{Results on two instruction-following benchmarks implemented on LLaMA-3-8B.}
\label{tb:if} 
\end{table}


% \begin{table}[h]
% \scriptsize	
% \centering
% \resizebox{0.97\linewidth}{!}{
% \begin{tabular}{lcccc}
% \toprule
% \textbf{Model} & \textbf{BioGEN} & \textbf{LongFact} & \textbf{FollowRAG - F} & \textbf{MT-Bench}\\
% \midrule
% \rowcolor{blue!5} \textbf{\OURS \ - 5\%} & \textbf{50.3} & \textbf{87.7} & \textbf{45.3} & \textbf{64.6}  \\
% FLAME-DPO$^{\rm fact}$ & 46.3 & 85.7 & 41.5 & 56.2 \\
% SELF-EVAL & 47.2 & 85.2 & 42.3 & 53.1 \\
% w/o. RL Stage & 41.9 & 82.6 & 38.1 & 64.3 \\
% \bottomrule
% \end{tabular}}
% \caption{Comparison with RL-based methods. 
% We report the FactScore results on BioGEN. 
% FollowRAG-F denotes the average score on FollowRAG-Faithfulness.
% We apply RL stages after training on Alpaca-GPT4.
% }
% \label{tb:rl} 
% \end{table}

\subsection{Main Results}
\textbf{\OURS~Significantly Reduces Hallucinations.}
\
As shown in Table \ref{tb:main-hall}, \OURS~shows consistent and significant improvements on three hallucination benchmarks measuring factuality and faithfulness.
Compared to indiscriminately using the whole instruction dataset (i.e., Vanilla - 100\%), using samples selected by \OURS~to train LLMs can improve \textbf{3.5-8.6\%} on BioGEN, \textbf{2.0-5.1\%} on LongFact, and \textbf{4.9-8.5\%} on FollowRAG-Faithfulness.
This is because \OURS~effectively filters out the unfamiliar instruction data and avoids training LLMs on these data thereby reducing the hallucinations.
Compared to instruction data filtering methods that focus on data quality, like IFD, our method consistently improves the performance across different selected sample ratios (5-15\%) on three benchmarks.
Meanwhile, these data selected by quality-focused methods may present unfamiliar knowledge to the LLM and encourage hallucinations on LongFact.
On the contrary, \OURS~aims to identify the samples that align well with LLMâ€™s knowledge, helping the LLM to hallucinate less.
\OURS~also achieves better performance than RL-based methods without introducing additional preference data.
These findings underline the effectiveness of our method in aligning LLMs to hallucinate less.


% \vspace{-1.05mm}
\noindent
\textbf{\OURS~Maintains a Good Balance between Following Instructions and Reducing Hallucinations.}
\
As shown in Table \ref{tb:if}, \OURS~achieves a better instruction-following ability compared to vanilla tuning methods, especially when the LLM is trained on Alpaca.
It shows that \OURS~can effectively align LLMs to follow instructions.
In some cases, our method surpasses data filtering methods that enhance instruction-following ability, demonstrating its effectiveness in identifying suitable data for LLMs.
Unlike RL-based methods that weaken the model's instruction-following ability, our method shows superior instruction-following ability while greatly reducing hallucinations.

\noindent
\textbf{\OURS~Mitigates Overconfidence Phenomenon.}
\
We select 15 samples with the lowest scores for each model from LongFact-Objects and calculate its average perplexity on these samples.
We find that \OURS~generates a high perplexity score (i.e., low sentence-level confidence score) on these bad cases as shown in Figure\ref{fig_ppl}, showing that \OURS~mitigates overconfidence in these false statements.



\begin{figure}
    \centering
    % \vspace{-8mm}
    \includegraphics[width=7cm]{Figures/ppl.pdf}
    \caption{Average perplexity score of 15 samples with the lowest scores for each model from LongFact-Objects. Models are trained on Alpaca-GPT4.}
    \label{fig_ppl}
\end{figure}

\begin{table}
\scriptsize	
\centering
\resizebox{0.7\linewidth}{!}{
\begin{tabular}{lcc}
\toprule
\textbf{Model} & \textbf{BioGEN} & \textbf{MT-Bench}\\
\midrule
\rowcolor{blue!5} \textbf{\OURS \ - 5\% - 70B} & 60.9 &  74.3 \\
-w/o. Data Filtering & {53.7} & {73.2}  \\
\hdashline[2pt/3pt]
\rowcolor{blue!5} \textbf{\OURS \ - 5\% - 8B} & 50.5 & 64.6  \\
-w/o. Data Filtering & {41.9} & {64.3}  \\
-w/o. ICP & {47.6} &  {64.1} \\
-w/o. SEI & {48.3} & {63.8} \\
-w/o. Quality RM & {55.6} & {48.6} \\
-w/o. ICP \& SEI & {43.7} & {65.2} \\
\bottomrule
\end{tabular}}
\caption{Results of ablation and scalability study. We report FactScore results on BioGEN. Models are trained on Alpaca-GPT4. RM represents the reward model.}
\label{tb:ab} 
\end{table}

\subsection{Analysis}
\textbf{Ablation Study.}
\
We conduct the ablation study in Table \ref{tb:ab}.
We can find that the proposed ICP and SEI can both help LLMs to reduce hallucinations.
% Only considering the LLM's familiarity (i.e., Quality RM) would limit the instruction-following ability of LLMs.
% Because considering only familiarity ignores other important characteristics of instruction data, thus it is necessary to introduce an expert-aligned reward model to keep a good balance between following instructions and reducing hallucination.
Also, considering only familiarity (i.e., -w/o. Quality RM) ignores other important characteristics of instruction data and limits the instruction-following ability of LLMs.
Thus, even if considering familiarity alone would greatly reduce hallucinations, it is still necessary to introduce a quality reward model to maintain a good balance between following instructions and reducing hallucinations.


\noindent
\textbf{Scalability Study.}
\
We implement our method on the LLaMA-3-70B in Table \ref{tb:ab} to explore whether \OURS~can fit in larger LLMs.
Results indicate that \OURS~scales effectively to larger-scale models.




\begin{table}[t]
    \scriptsize
    \centering
    \resizebox{\linewidth}{!}{
        \begin{tabular}{m{\hsize}}
        \specialrule{1pt}{1pt}{2pt}
        \textbf{Instruction}: 
        Who is the character Geralt of Rivia from The Witcher series? \\
        \specialrule{1pt}{0pt}{1pt}
        \textbf{Vanilla - 100\%}:
        \sethlcolor{tabcolor5}\hl{Geralt of Rivia is a character and the main protagonist of the fantasy novel series "The Witcher"} written by Andrzej Sapkowski (\cmark). 
        \sethlcolor{tabcolor1}\hl{He is a witcher}, \underline{a rare human mutation born with powers} (\xmark) to \sethlcolor{tabcolor4}\hl{fight against the monsters} that \underline{threaten the world of the living} (\xmark). 
        Geralt is known for his intelligence, skill in combat, and his use of magical artifacts, particularly his silver sword (\cmark).  
        Throughout the series, Geralt faces numerous challenges and battles as he journeys across the Continent, \sethlcolor{tabcolor3}\hl{accompanied by his friends and lovers}, in a quest to protect the world and the people he cares about (\cmark). \\

        \specialrule{0pt}{0pt}{1pt}
        \cdashline{1-1}[0.8pt/2pt]
        \specialrule{0pt}{0pt}{2pt}
        \textbf{\OURS - 5\%}: 
        \sethlcolor{tabcolor5}\hl{Geralt of Rivia is a character from The Witcher series}, a fantasy franchise that includes a series of novels, short stories, and video games (\cmark).  \sethlcolor{tabcolor1}\hl{He is a witcher}, \sethlcolor{tabcolor4}\hl{a member of a guild of professional monster hunters}, and his role is to protect the innocent from supernatural threats (\cmark). 
        Geralt is a complex and brooding character, with a strong sense of honor and duty, and a deep understanding of the world around him (\cmark). 
        He is often portrayed as a loner, but \sethlcolor{tabcolor3}\hl{he is also capable of forming close relationships with others}, particularly with those who share his beliefs and values (\cmark). \\
        \specialrule{1pt}{0.5pt}{0pt}   
        \end{tabular}}
\caption{Case study from LongFact-Objects. We highlight the statements that share the same semantics using the same color.
Models are trained on Alpaca-GPT4.}
    
\label{tab:case}
\end{table}


\begin{table}
\scriptsize	
\centering
\resizebox{0.87\linewidth}{!}{
\begin{tabular}{lcc}
\toprule
\textbf{Model} & \textbf{BioGEN} & \textbf{MT-Bench}\\
\midrule
\rowcolor{blue!5} \textbf{\OURS \ - 5\% - Alpaca-GPT4} & 50.5 & 64.6  \\
\multicolumn{3}{c}{\cellcolor{mypink} \textbf{-w/o ICP}}\\
-w. Confidence Score (Perplexity) & {48.4} & 62.2 \\
-w. Lexical Similarity (Rouge-L) & {47.9} & 61.5 \\
-w. Using Embedding Model & {49.8} & 63.9 \\
% -w. Average Pooling & - & - \\
\multicolumn{3}{c}{\cellcolor{mypink} \textbf{-w/o SEI}}\\
-w. K-means Clustering via Internal States & {47.8} & 60.2  \\
-w. K-means Clustering via Embedding Model & {48.5} & 63.2 \\
-w Voting without Semantic Clustering & {47.3} & 60.8 \\
\bottomrule
\end{tabular}}
\caption{Evaluation results of \OURS~that employ various methods for measuring the LLM's familiarity.
We report FactScore results on BioGEN.}
\label{tb:var} 
\end{table}

\noindent
\textbf{Case Study.}
\
We conduct a case study in Table \ref{tab:case} to visually show the advantages of \OURS.
Compared to using the whole training data, our method ensures the statements are correct and comprehensive, and the generated text is fluent and natural.

\noindent
\textbf{Variant Methods Testing.}
\
As shown in Table \ref{tb:var}, we further explore the variant methods in measuring the LLM's familiarity.
For ICP, we separately replace it with sentence-level confidence (Perplexity) and lexical metrics (Rouge-L).
Specifically, we use the average perplexity score of generated responses to represent sentence-level confidence and use the average Rouge-L score between each pair of two generated responses as lexical metrics.
However, these straightforward strategies neglect highly concentrated semantic information within the internal states, and thus fail to capture the fine-grained differences between responses and limit the final performance.
We also explore the effectiveness of an advanced embedding model, we use \textsc{text-embedding-3-large}\footnote{https://platform.openai.com/docs/guides/embeddings} from OpenAI and set the dimension as 4096.
We find that using the internal states achieves better performance, showing the effectiveness of our method.
This is because internal states may reflect more dense and fine-grained information from LLM itself that may have been lost in the decoding phase of the responses.
For SEI, we explore whether using k-means clustering based on internal states computed as ICP and sentence embedding from \textsc{text-embedding-3-large} can identify suitable semantic clusters.
We can find that our method achieves better performance because the k-means algorithm is not based on semantic equivalence to get the clusters.
Also, the internal states of LLMs cannot efficiently represent the target response, as this response is manually labeled or generated by other advanced LLMs instead of generated by the LLM itself.
We also find that simply voting based on the textual contents instead of semantic clustering limits the final performance, as these responses are often free-form and can have the same meaning in different ways. 
% Overall, extensive results show the effectiveness of our designed modules and motivation.




\noindent
\textbf{Discussion.}
We conduct the parameter study to test the robustness of our method in Appendix \ref{appendix:para}.
We also conduct a transferability study in Appendix \ref{appendix:trans} and find \OURS~can fit in other LLMs.
We further explore the design of our method in Appendix \ref{appendix:design} and find our design is effective.
We conduct a case study in Appendix \ref{appendix:cs-ss} to qualitatively show the difference between samples with different scores.

\noindent
\textbf{Human Evaluation.}
\
We conduct a human evaluation on the 50 generated biographies from BioGEN across four key dimensions: factuality, helpfulness, relevance, and naturalness.
For each comparison, three options are given (Ours Wins, Tie, and Vanilla Fine-tuning Wins) and the majority voting determines the final result. 
Figure \ref{fig_human} shows that our method significantly reduces hallucinations and effectively follows instructions with high-quality responses.
Details can be found in Appendix \ref{appendix:huamn}.

\begin{figure}
    \centering
    \includegraphics[width=6.4cm]{Figures/human.pdf}
    \caption{Human evaluation across four key dimensions. The models are trained on Alpaca-GPT4.}
    \label{fig_human}
\end{figure}
