\begin{abstract}
Training LLMs on data containing unfamiliar knowledge during the instruction tuning stage can encourage hallucinations.
To address this challenge, we introduce \textbf{\OURS}, a novel framework designed to identify high-quality data that aligns well with the LLM's learned knowledge to reduce hallucinations. 
\OURS~includes Internal Consistency Probing (ICP) and Semantic Equivalence Identification (SEI) to measure how familiar the LLM is with instruction data. 
Specifically, ICP evaluates the LLM's understanding of the given instruction by calculating the tailored consistency among multiple self-generated responses.
SEI further assesses the familiarity of the LLM with the target response by comparing it to the generated responses, using the proposed semantic clustering and well-designed voting strategy.
Finally, to ensure the quality of selected samples, we introduce an expert-aligned reward model, considering characteristics beyond just familiarity.
By considering data quality and avoiding unfamiliar data, we can utilize the selected data to effectively align LLMs to follow instructions and hallucinate less.
% Experiments show that \OURS~significantly reduces hallucinations while maintaining a competitive ability to follow instructions.
% % % for arxiv
Extensive experiments and analysis show that \OURS~significantly reduces hallucinations and allows LLMs to maintain a strong ability to follow instructions.\footnote{The data and code will be available at \url{https://github.com/S1s-Z/NOVA}.}
% Experiments show that \OURS~significantly reduces hallucinations while maintaining a competitive ability to follow instructions.\footnote{The data and code will be available at \url{https://github.com/S1s-Z/NOVA}.}

\end{abstract}