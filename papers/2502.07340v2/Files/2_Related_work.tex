\section{Related Work}

\textbf{Hallucinations in LLMs.}
\
Hallucinations occur when the generated content from LLMs seems believable but does not match factual or contextual knowledge \citep{ji-survey, rawte2023surveyhallucinationlargefoundation, hit-survey}.
% Recent studies \citep{lin2024flame, kang2024unfamiliarfinetuningexamplescontrol, gekhman-etal-2024-fine} attempt to analyze the causes of hallucinations in LLMs.
% \citet{lin2024flame} conducts a pilot study and finds that tuning LLMs on data containing unseen knowledge can encourage models to be overconfident, leading to hallucinations.
Recent studies \citep{lin2024flame, kang2024unfamiliarfinetuningexamplescontrol, gekhman-etal-2024-fine} attempt to analyze the causes of hallucinations in LLMs and find that tuning LLMs on data containing unseen knowledge can encourage models to be overconfident, leading to hallucinations.
Therefore, recent studies \citep{lin2024flame, zhang-etal-2024-self, tian2024finetuning} attempt to apply RL-based methods to teach LLMs to hallucinate less after the instruction tuning stage.
However, these methods are inefficient because they require additional corpus and API costs for advanced LLMs.
Even worse, such RL-based methods can weaken the instruction-following ability of LLMs \citep{lin2024flame}.
In this paper, instead of introducing the inefficient RL stage, we attempt to directly filter out the unfamiliar data during the instruction tuning stage, aligning LLMs to follow instructions and hallucinate less.






\noindent
\textbf{Data Filtering for Instruction Tuning.}
\
Data are crucial for training neural networks. \citep{van2020survey, song2022learningnoisylabelsdeep, si-etal-2022-scl, si-etal-2023-santa, zhao2024ultraedit, an2024threadlogicbaseddataorganization, si-etal-2024-improving, cai-etal-2024-unipcm}.
According to \citet{zhou2023lima}, data quality is more important than data quantity in instruction tuning.
Therefore, many works attempt to select high-quality instruction samples to improve the LLMsâ€™ instruction-following abilities.
\citet{chen2023alpagasus, liu2024what} utilize the feedback from well-aligned close-source LLMs to select samples.
\citet{cao2024instructionmininginstructiondata,li-etal-2024-quantity, ge2024clustering, si2024selecting, xia2024less,zhang2024recostexternalknowledgeguided} try to utilize the well-designed metrics (e.g., complexity) based on open-source LLMs to select the samples.
However, these high-quality data always contain expert-level responses and may contain much unfamiliar knowledge to the LLM.
Unlike focusing on data quality, we attempt to identify the samples that align well with LLM's knowledge, thereby allowing the LLM to hallucinate less.
