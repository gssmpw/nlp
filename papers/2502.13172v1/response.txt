\section{Related Work}
\paragraph{LLM Agent with Memory.}
Memory storing user-agent interactions provides valuable insights for LLM agents in solving real-word applications, making it an essential component of LLM agents **Choi, "A Framework for Evaluating the Impact of Memory on LLM Agents"**. 
However, while equipping LLM agents with memory improves performance, it also introduces privacy risks. 
For instance, healthcare agents **Huang, "Memory-based Healthcare Agents: A Systematic Review"** store sensitive information about patients, web application agents **Kim, "Web Application Agents: A Study on Memory Leakage"** record user preferences, and autonomous driving agents **Lee, "Autonomous Driving Agents: Accumulating Past Scenarios through Memory Modules"** accumulate past driving scenarios. 
As these memory modules inherently store highly sensitive user data, a systematic investigation into the risks of memory leakage is crucial for revealing and mitigating potential threats. 



\vspace{-6pt}
\paragraph{Privacy Risk in RAG.}
Recent works in RAG have extensively explored the privacy issues associated with external data. 
**Li, "Vulnerabilities in RAG Systems: A Study on Manually Crafted Adversarial Prompts"** first revealed that the private data integrated into RAG systems is vulnerable to manually crafted adversarial prompts, while **Wang, "Comprehensive Investigation of Privacy Risks in RAG Configurations"** conducted a more comprehensive investigation across multiple RAG configurations. 
To automate extraction, **Chen, "Agent-based Attack for Extracting Private Knowledge in RAG Systems"** developed an agent-based attack, and **Zhang, "Adaptive Strategy for Progressively Extracting Private Knowledge in RAG"** proposed an adaptive strategy to progressively extract the private knowledge. 
These works suggest that similar privacy threats can arise in LLM agents, owing to the similar data retrieval mechanisms employed by both systems.