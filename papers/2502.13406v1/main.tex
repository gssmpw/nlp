\documentclass[letterpaper, 10 pt]{ieeeconf}
\pagestyle{empty}

\IEEEoverridecommandlockouts
% The preceding line is only needed to identify funding in the first footnote. If that is unneeded, please comment it out.

% Packages
\input{packages}

% Custom math operators
\input{notation}

% Tighten up spacing
\setlength{\textfloatsep}{7pt}
\setlength\intextsep{7pt}

\begin{document}

\title{\LARGE \bf Generative Predictive Control:\\ Flow Matching Policies for Dynamic and Difficult-to-Demonstrate Tasks}

\author{Vince Kurtz and Joel W. Burdick}

\maketitle
\thispagestyle{empty}

\begin{abstract}
    Generative control policies have recently unlocked major progress in robotics. These methods produce action sequences via diffusion or flow matching, with training data provided by demonstrations. But despite enjoying considerable success on difficult manipulation problems, generative policies come with two key limitations. First, behavior cloning requires expert demonstrations, which can be time-consuming and expensive to obtain. Second, existing methods are limited to relatively slow, quasi-static tasks. In this paper, we leverage a tight connection between sampling-based predictive control and generative modeling to address each of these issues. In particular, we introduce \textit{generative predictive control}, a supervised learning framework for tasks with fast dynamics that are easy to simulate but difficult to demonstrate. We then show how trained flow-matching policies can be warm-started at run-time, maintaining temporal consistency and enabling fast feedback rates. We believe that generative predictive control offers a complementary approach to existing behavior cloning methods, and hope that it paves the way toward generalist policies that extend beyond quasi-static demonstration-oriented tasks.
\end{abstract}

\section{Introduction and Related Work}\label{sec:intro}

Diffusion and flow matching policies have enabled tremendous success in behavior cloning for quasi-static dexterous manipulation tasks \cite{chi2023diffusion, black2024pi_0, zhao2023learning, fu2024mobile}. Can generative policies also control systems with fast nonlinear dynamics at high control frequencies, where demonstrations are difficult to come by? We answer this question in the affirmative by introducing Generative Predictive Control (GPC), a supervised learning framework for dynamic and difficult to demonstrate tasks. 

The basic idea behind GPC is summarized in Fig.~\ref{fig:hero}. GPC alternates between data collection via Sampling-based Predictive Control (SPC) and policy training via flow matching. The flow matching model bootstraps SPC, allowing for continual performance improvements while maintaining a supervised (e.g., regression) training objective.

\subsection{Generative Policies for Behavior Cloning}\label{sec:intro:behavior_cloning}

Generative models like diffusion \cite{chi2023diffusion} and flow matching \cite{black2024pi_0} have recently gained prominence as powerful policy representations for robotics tasks. These models typically focus on behavior cloning \cite{zhao2023learning, fu2024mobile}, where expert demonstrations serve as training data.

These models offer a few key advantages over other policy representations. Diffusion and flow-matching maintain a high degree of multi-modal expressiveness, allowing for multiple ``paths'' to achieve the same task \cite{chi2023diffusion}. They are also able to consider high-dimensional image observations, which are ubiquitous in modern robotics \cite{song2020score,song2019generative,lipman2022flow}.

Importantly, generative models for behavior cloning are trained in a \textit{supervised} manner, with clearly defined regression targets. This improves training stability over unsupervised reinforcement learning methods, which can be sensitive to reward specifications, implementation details, and even the random seed used in training \cite{andrychowicz2020matters, engstrom2019implementation}.

The success of generative behavior cloning, the expressiveness of flow matching and diffusion, and the rise of large foundation models in natural language processing and computer vision have inspired a widespread push toward generalist policies that combine data from multiple tasks \cite{black2024pi_0, lbm}. This includes widespread data collection efforts \cite{o2023open} as well as creative tele-operation alternatives \cite{chi2024universal}.

Obtaining demonstration data is a key challenge, and potentially a critical bottleneck in this effort. While creative ways to obtain demonstration data is an area of much active research \cite{chi2024universal}, it is unlikely that demonstrations alone will produce the internet-scale data used to train large vision-language models any time soon. Additionally, some tasks are simply difficult to demonstrate, particularly for robots with fast nonlinear dynamics or unique morphologies. 

\begin{figure}
    \centering
    \includegraphics[width=\linewidth]{figures/cover_fig.pdf}
    \caption{Generative predictive control is a supervised learning framework for dynamic tasks that are difficult to demonstrate but easy to simulate. First, we generate training data with sampling-based predictive control \cite{li2024drop, williams2016aggressive}, leveraging advances in massively parallel GPU simulation \cite{mjx, Genesis, makoviychuk2021isaac}. We then use this data to train a flow matching policy, which is structured similarly to the behavior cloning policies that have enjoyed tremendous success on quasi-static manipulation tasks \cite{chi2023diffusion, black2024pi_0}. The flow matching policy can then be used to augment the sampling-based controller, further improving performance.
    }
    \label{fig:hero}
\end{figure}

\subsection{Sampling-based Predictive Control}\label{sec:intro:spc}

At the same time, a very different trend has been gaining traction in the nonlinear optimal control community. Sampling-based Predictive Control (SPC) is an alternative to gradient-based Model Predictive Control (MPC), where a simple sampling procedure is used in place of a sophisticated nonlinear optimizer \cite{li2024drop}.

Prominent algorithms in this family include Model Predictive Path Integral Control (MPPI) \cite{williams2016aggressive}, Predictive Sampling (PS) \cite{howell2022predictive}, and Cross-Entropy Methods (CEM) \cite{rubinstein1999cross}. SPC methods are exceedingly easy to implement, and have been studied for a long time. But recent advances in computing speed and parallel simulation \cite{makoviychuk2021isaac, mjx, Genesis} have enabled them to scale to complex problems like in-hand dexterous manipulation \cite{li2024drop}, legged locomotion \cite{xue2024full} and more \cite{kurtz2024hydrax, howell2022predictive}. On certain tasks, SPC methods offer performance competitive with state-of-the-art reinforcement learning algorithms, even without any offline policy training \cite{li2024drop}.

SPC is particularly appealing in the context of contact rich robotics tasks, where the stiff dynamics of contact present significant challenges to general-purpose non-convex optimizers \cite{posa2014direct, kurtz2023inverse, erez2012trajectory, aydinoglu2024consensus}. Even the simplest SPC algorithms can outperform specialized contact-implicit optimizers \cite{howell2022predictive}, thanks to the fact that they do not require expensive gradients through contact, and the fact that randomization smoothes out contact-related discontinuities \cite{suh2022bundled, le2024leveraging}. 

In many ways, SPC is complementary to generative behavior cloning: it is agnostic to a robot's morphology, and can be particularly effective on tasks with fast nonlinear dynamics. Behavior cloning, on the other hand, has shown particular success on tasks involving deformable objects like cloth and food \cite{chi2023diffusion, black2024pi_0, zhao2023learning, fu2024mobile}, which are difficult to simulate at speeds sufficient for SPC. 

\subsection{Contribution}

In this paper, we highlight a surprisingly deep connection generative policies and SPC algorithms. This connection was first identified by \cite{pan2024model} in the context of MPPI: here we extend this connection to a general class of SPC algorithms. 

We then leverage this connection to develop a supervised learning framework for dynamic and difficult-to-demonstrate tasks, which we call Generative Predictive Control (GPC). We show how flow-matching policies can be warm-started to encourage temporal consistency and smooth action sequences, and demonstrate how these warm-starts are critical for effective performance on systems with fast dynamics.

We provide an open-source GPC implementation in JAX/MJX \cite{mjx}, which we use to push the scalability limits of our proposed approach. We train GPC policies on systems ranging from a simple inverted pendulum to a 29 degree-of-freedom (DoF) humanoid robot. Our largest and most difficult task (humanoid standup) exposes the limits of this approach: bootstrapping SPC with a trained policy improves performance, but the GPC policy alone is not sufficient.

With this in mind, we provide an extensive discussion of the limitations of GPC and directions for future work to overcome these limitations in Section~\ref{sec:limitations}. These include value function learning, better exploration strategies, and more efficient use of non-optimal samples. 

\section{Background}\label{sec:background}

\subsection{Problem Formulation}\label{sec:background:problem}

In this paper we consider optimal control problems of the standard form
\begin{subequations}\label{eq:ocp}
\begin{align}
    \min_{u_0, u_1, \dots, u_T} ~& \phi(x_{T+1}) + \sum_{\tau=0}^{T} \ell(x_\tau, u_\tau), \\
    \mathrm{s.t.} ~& x_{\tau+1} = f(x_\tau, u_\tau), \\
                   & x_0 = x_{init},
\end{align}
\end{subequations}
where $x_\tau \in \mathbb{R}^n$ represents the system state at time step $\tau$, $u_\tau \in \mathbb{R}^m$ are control actions, $\ell : \mathbb{R}^n \times \mathbb{R}^m \to \mathbb{R} $ and $\phi: \mathbb{R}^n \to \mathbb{R}$ are running and terminal costs, and $f : \mathbb{R}^n \times \mathbb{R}^m \to \mathbb{R}^n$ captures the system dynamics. 

For ease of notation, we denote the action sequence as $U = [u_0, u_1, \dots, u_T]$ and rewrite \eqref{eq:ocp} in compact form as
\begin{equation}\label{eq:ocp_compact}
    \min_{U} ~ J(U;x_{init}),
\end{equation}
where both the costs and dynamics constraints are wrapped into the (highly non-convex) objective $J : \mathbb{R}^{m \times T} \times \mathbb{R}^n \to \mathbb{R}$.

We will approach this control problem from a Model Predictive Control (MPC) perspective. Rather than attempting to find an optimal closed-form state-feedback policy $u_\tau = \kappa(x_\tau)$, we seek numerical methods to rapidly (and if needed, approximately) solve for the action sequence $U$ from a given initial condition $x_{init}$. At run time, we continually solve from current state estimate, applying only $u_0$ before re-planning as quickly as possible.

\subsection{Sampling-based Predictive Control}\label{sec:background:spc}

MPC methods traditionally solve $\eqref{eq:ocp}$ with gradient-based non-convex optimization. But these techniques face significant challenges, particularly when it comes to the stiff and highly nonlinear contact dynamics essential for contact-rich robot locomotion and manipulation tasks \cite{wensing2023optimization}. 

In response to these challenges, Sampling-based Predictive Control (SPC) is gaining prominence as a simple and computationally efficient alternative to gradient-based MPC \cite{williams2016aggressive, williams2017model, li2024drop, xue2024full, vlahov2024mppi}. Instead of relying on the complex machinery of nonlinear optimization, SPC algorithms perform some variation of the following simple procedure:
\begin{itemize}
    \item At step $k$, sample $N$ candidate action sequences
    \begin{equation}\label{eq:spc_proposal}
        U^{(i)} \sim \mathcal{N}(\bar{U}_{k-1}, \sigma^2), \quad i \in [1, N],
    \end{equation}
    typically from a Gaussian proposal distribution\footnote{Some SPC methods use a non-isotropic proposal distribution and update the variance of the proposal distribution along with the mean. We focus on an isotropic Gaussian with fixed variance for simplicity of presentation.}.
    \item Roll out each action sequence from the latest state estimate $x_{k-1}$, recording the costs
    \begin{equation}\label{eq:spc_rollouts}
        J^{(i)} \gets J\left(U^{(i)}; x_{k-1}\right).
    \end{equation}
    \item Use the costs to update the mean action sequence,
    \begin{equation}\label{eq:spc_update}
        \bar{U}_{k} \gets \bar{U}_{k-1} + \frac{\sum_{i=1}^{N} g(J^{(i)})(U^{(i)} - \bar{U}_{k-1})}{\sum_{i=1}^N g(J^{(i)})},
    \end{equation}
    using some weighting function $g : \mathbb{R} \to \mathbb{R}^+$ .
    \item Apply the first action from $\bar{U}_{k}$, and repeat in MPC fashion from the updated state $x_k$.
\end{itemize}

The ability to parallelize the rollouts \eqref{eq:spc_rollouts} presents a key advantage of SPC over traditional gradient-based MPC. The advent of massively parallel GPU-enabled robotics simulators \cite{mjx, Genesis, makoviychuk2021isaac} only further reinforces this advantage. Sampling can also reduce the severity of local minima and smooth out stiff dynamics associated with contact \cite{suh2022bundled, le2024leveraging}.

The choice of weighting function $g$ is the key factor distinguishing different SPC algorithms. For instance, \textbf{MPPI} \cite{williams2016aggressive} uses a Bolzmann-like exponentially weighted average,
\begin{equation}\label{eq:mppi_g}
    g_{MPPI}(J) = \exp\left(- J / \lambda \right),
\end{equation}
where $\lambda > 0$ is the temperature parameter. Smaller values of $\lambda$ give more weight to the lowest-cost samples. 

In the low-temperature limit we recover \textbf{predictive sampling} \cite{howell2022predictive},
\begin{equation}\label{eq:ps_g}
    g_{PS}(J) = \lim_{\lambda \to 0} \exp\left(- J/\lambda\right),
\end{equation}
where the updated mean $\bar{U}$ is simply chosen as the lowest-cost sample. 

Another popular option is \textbf{CEM} \cite{rubinstein1999cross, li2024drop}, which weights the top performing samples equally,
\begin{equation}\label{eq:cem_g}
    g_{CEM}(J) = \begin{cases}
        1 & \mathrm{if~} J \leq \gamma \\
        0 & \mathrm{otherwise}
    \end{cases}.
\end{equation}
The threshold $\gamma$ is defined implicitly by a pre-defined number of \textit{elite samples}. CEM is typically paired with an adaptive update rule for the variance of the proposal distribution.

\textbf{Tsallis-MPPI} \cite{wang2021variational} uses a generalized exponential to provide a middle ground between MPPI and CEM,
\begin{equation}\label{eq:tsallis_g}
    g_{T}(J) = \max(1 - (r - 1)J/\lambda, 0)^{\frac{1}{r - 1}},
\end{equation}
where the original MPPI update is recovered as $r \to 1$.

High-performance implementations of these and other SPC algorithms are available in \texttt{hydrax} \cite{kurtz2024hydrax}, a JAX-based SPC software package built on top of MuJoCo MJX \cite{mjx}.

\begin{remark}
    SPC performance can benefit significantly from dimensionality reduction via spline representations, e.g., representing $U = [u_0, u_1, \dots, u_T]$ with $T' \ll T$ knot points. While our implementation focuses on only the simplest zero-order-hold interpolation, the GPC framework is compatible with more sophisticated schemes such as Fourier features or higher-order splines \cite{howell2022predictive}.
\end{remark}

\subsection{Generative Modeling}\label{sec:gen}

\subsubsection{Flow Matching}\label{sec:flow}

Flow matching generative models \cite{lipman2022flow} focus on a seemingly very different problem: produce a sample $\x$ from a probability distribution $p(\x)$. In the typical problem setting, we do not have access to $p(\x)$ in closed form, but we do have samples from this data distribution.

The basic idea is to consider a \textit{probability density path} $p_t(\x)$. This path flows from an easy-to-sample proposal distribution $p_0(\x) = \mathcal{N}(0, I)$ at $t = 0$ to the data distribution at $t = 1$. Flow matching generative models learn a time-varying vector field $v_\theta(\x, t)$ that moves samples from $p_0(\x)$ to $p_1(\x)$, where $\theta$ denote learnable parameters.

The flow matching network $v_\theta$ is trained via standard (stochastic) gradient descent methods on
\begin{equation}\label{eq:flow_objective}
    \min_{\theta} \mathbb{E}_{t, \x_0, \x_1} \left[\mathcal{L}_{FM}(\theta; \x_0, \x_1, t)\right],
\end{equation}
where
\begin{multline}\label{eq:flow_loss}
    \mathcal{L}_{FM}\left(\theta; \x_0, \x_1, t \right) = \\
    \left\| v_\theta\big(t \x_1 + (1-t)\x_0, t\big) - (\x_1 - \x_0) \right\|^2
\end{multline}
is the flow matching loss. The expectation in \eqref{eq:flow_objective} taken over $t \sim \mathcal{U}(0, 1)$, $\x_0 \sim p_0(\x)$, and $\x_1 \sim p_1(\x)$. Each of these are easy to acquire, since we already have data points $\x_1$ and $p_0(\x) = \mathcal{N}(0, I)$ is easy to sample from. Intuitively, $\mathcal{L}_{FM}$ attempts to push samples in straight line from $\x_0$ to $\x_1$.

At inference time, we generate new samples by starting with $\x \sim p_0(\x)$ and simulating the Ordinary Differential Equation (ODE)
\begin{equation}
    \dot{\x} = v_\theta(\x, t).
\end{equation}
Any off-the-shelf ODE solver can be used for this purpose, but in many cases a simple forward Euler scheme
\begin{equation}
    \x_{t+\delta t} = \x_t + \delta t v_\theta(\x_t, t),
\end{equation}
is sufficiently performant.

\subsubsection{Diffusion}\label{sec:diffusion}

Flow matching is equivalent (under some technical conditions) to diffusion-based generative modeling \cite{gao2025diffusionmeetsflow}. Diffusion models also consider a series of probability distributions flowing from an initial Gaussian to the data distribution. But rather than being parameterized by a time $t$, these are typically parameterized by additive noise $\sigma$,
\begin{equation}
    p_\sigma(\x) = \int p(\y) \mathcal{N}(\x;\y, \sigma^2I) d\y.
\end{equation}
For large $\sigma$, $p_\sigma(\x)$ is dominated by noise and approaches an easy-to-sample Gaussian. For small $\sigma$, $p_\sigma(\x)$ approaches the data distribution.

Diffusion models estimate the score
\begin{equation}
    s_\theta(\x, \sigma) \approx \nabla_{\x} \log p_\sigma(\x)
\end{equation}
at various noise levels by learning to remove noise added to the original data \cite{song2019generative, song2020score}. Once we have a trained score network, we can use it sample from $p_\sigma$ using Langevin dynamics
\begin{equation}
    \x \gets \x + \epsilon s_\theta(\x, \sigma) + \sqrt{2\epsilon} \mathbf{z} \quad \mathbf{z} \sim\mathcal{N}(0, I),
\end{equation}
with step size $\epsilon > 0$. By gradually reducing $\sigma$, we arrive at samples from the data distribution $p(\x)$.

\section{SPC is Online Generative Modeling}

In this section, we establish a formal connection between SPC and generative modeling. Specifically, we show that the SPC update \eqref{eq:spc_update} is a monte-carlo estimate of the score of a noised target distribution. This connection was first identified for the case of MPPI in \cite{pan2024model} and used to develop Dial-MPC, a multi-stage SPC algorithm for legged locomotion, in \cite{xue2024full}. Here we extend this connection to generic SPC algorithms with updates of the form \eqref{eq:spc_update}.

First, we define a target distribution conditioned on the intial state $x$:
\begin{equation}
    p(U \mid x) \propto g(J(U; x)),
\end{equation}
which is determined by the algorithm-specific weighting function $g(\cdot)$ introduced in Sec.~\ref{sec:background:spc} above. In the spirit of score-based diffusion (Sec.~\ref{sec:diffusion}) we define the noised target distribution
\begin{equation}\label{eq:noised_target}
    p_\sigma(U \mid x) \propto \mathbb{E}_{\tilde{U} \sim \mathcal{N}(U, \sigma^2)}[g(\tilde{U})].
\end{equation}
It turns out the score of this noised target is directly related to the SPC update \eqref{eq:spc_update}:

\begin{proposition}
    The score of the noised target distribution \eqref{eq:noised_target} is given by
    \begin{equation}
        \nabla_U \log p_\sigma(U \mid x) = \frac{1}{\sigma^2} \frac{\mathbb{E}_{\tilde{U} \sim \mathcal{N}(U, \sigma^2)}\left[g(\tilde{U})(\tilde{U}-U)\right]}{\mathbb{E}_{\tilde{U} \sim \mathcal{N}(U, \sigma^2)}\left[g(\tilde{U})\right]}
    \end{equation}
\end{proposition}
\begin{proof}
    For simplicity of notation, we drop the conditioning on $x$ and write the target distribution as $p_\sigma(U)$. We also denote the normal density as $\mathcal{N}(\tilde{U}; U, \sigma^2) = q_U(\tilde{U})$.

    The score is given by
    \begin{equation}
        \nabla_U \log p_\sigma(U) = \frac{\nabla_U p_\sigma(U)}{p_\sigma(U)}.
    \end{equation}
    In the numerator we have
    \begin{align}
        \nabla_U p_\sigma(U) &= \frac{1}{\eta} \nabla_U \int q_U(\tilde{U})g(\tilde{U})d\tilde{U} \\
        &= \frac{1}{\eta} \int \nabla_U q_U(\tilde{U}) g(\tilde{U})d\tilde{U} \\
        &= \frac{1}{\eta} \int q_U(\tilde{U}) \nabla_U \log q_U(\tilde{U})g(\tilde{U})d\tilde{U} \\
        &= \frac{1}{\eta} \mathbb{E}_{\tilde{U} \sim \mathcal{N}(U, \sigma^2)}\left[{g(\tilde{U})\frac{\tilde{U} - U}{\sigma^2}}\right],
    \end{align}
    where $\eta$ is a normalizing constant and we use the fact that $\nabla_U \log q_U(\tilde{U}) = (\tilde{U} - U)/\sigma^2$.

    Bringing $1 / \sigma^2$ outside the expectation, we have
    \begin{equation}
        \frac{\nabla_U p_\sigma(U)}{p_\sigma(U)} = \frac{\mathbb{E}_{\tilde{U} \sim \mathcal{N}(U, \sigma^2)}\left[g(\tilde{U})(\tilde{U}-U)\right]}{\sigma^2\mathbb{E}_{\tilde{U} \sim \mathcal{N}(U, \sigma^2)}\left[g(\tilde{U})\right]}
    \end{equation}
    and thus the proposition holds.
\end{proof}

In particular, this means that the SPC update \eqref{eq:spc_update} is nothing more than a Monte-Carlo estimate of score ascent, e.g.,

\begin{equation}
    \bar{U}_k \gets \bar{U}_{k-1} + \sigma^2 \nabla_{\bar{U}_{k-1}} \log p_\sigma(\bar{U}_{k-1} \mid x_{k-1})
\end{equation}
with
\begin{multline}
    \sigma^2 \nabla_U \log p_\sigma(U \mid x) \approx \\
    \frac{\sum_{i=1}^{N} g(J(U^{(i)}; x)(U^{(i)} - U)}{\sum_{i=1}^N g(J(U^{(i)}; x)},
\end{multline}
where $U^{(i)} \sim \mathcal{N}(U, \sigma^2I)$.

The additional $\sigma^2$ term may seem like a troublesome annoyance, but in fact Langevin step sizes $\epsilon \propto \sigma^2$ are a standard recommendation in the diffusion literature \cite[Algorithm~1]{song2019generative}. Here this choice of step size emerges naturally from the SPC update \eqref{eq:spc_update}.

This connection also sheds some light on the benefits of predictive sampling (where we merely choose the best sample). In particular, the un-noised target distribution for predictive sampling is a dirac delta concentrating all probability mass at the globally optimal solution \cite[Appendix~A]{pan2024model}, leading to a noised target distribution $p_\sigma(U \mid x)$ with a single mode at the globally optimal solution. In this sense, \textbf{predicitve sampling is monte-carlo score ascent on a probability distribution with a single mode at the global optimum}. 

For this reason, we focus our experimental results primarily on predictive sampling. A more thorough theoretical exploration of the advantages and disadvantages of other SPC algorithms is an important topic for future work. 

\section{Generative Predictive Control}

The previous section shows that we can think of the mean of the SPC sampling distribution, $\tilde{U}_k$, as a sample from a state-conditioned optimal action distribution
\begin{equation}\label{eq:gpc_target}
    \bar{U}_k \sim p(U \mid x_k) \propto g(J(U; x_k)).
\end{equation}
This leads to a natural question: can we train a generative model to produce $\bar{U}_k$? In addition to imitating the SPC update process, such a generative model
\begin{equation}\label{eq:gpc_param_model}
    p_\theta(U \mid x_k) \approx p(U \mid x_k),
\end{equation}
parameterized by network weights $\theta$, would maintain a similar structure to the flow matching and diffusion models used in behavior cloning \cite{chi2023diffusion, fu2024mobile, zhao2023learning, black2024pi_0}.

\begin{remark}
    Observant readers might note that behavior cloning methods typically condition on observations (or a history of observations) rather than a full state estimate, which is not always readily available in practice \cite{chi2023diffusion}. While we use state conditioning for notational simplicity, the GPC framework also applies to observation conditioning. In fact, our implementation conditions on observations $y = h(x)$ rather than the full state $x$. 
\end{remark}

This is the basic idea behind GPC. We use data ($\bar{U}_k, x_k$) from running SPC in simulation to train a flow matching model \eqref{eq:gpc_param_model}. This flow matching model is characterized by a time-varying state-conditional vector field
\begin{equation}
    \dot{U} = v_\theta(U, x, t)
\end{equation}
that pushes samples from $U_t \sim \mathcal{N}(0, I)$ at $t = 0$ to the target distribution \eqref{eq:gpc_target} at $t = 1$.

The simplest approach would be to define a conditional flow matching loss analogous to \eqref{eq:flow_loss},
\begin{multline}
    \mathcal{L}_{CFM}(\theta; U_0, \bar{U}_k, x_k, t) = \\
     \left\| v_\theta(t\bar{U}_k - (1 - t)U_0, x_k, t) - (\bar{U}_k - U_0) \right\|^2,
\end{multline}
where $(\bar{U}_k, x_k)$ are data points from the SPC controller, $U_0 \sim \mathcal{N}(0, I)$ is a sample from the proposal distribution, and $t \sim \mathcal{U}(0, 1)$ is sampled uniformly along the probability flow.

\begin{figure}
    \centering
    \includegraphics[width=0.6\linewidth]{figures/flow.pdf}
    \caption{The SPC update rule move samples $\bar{U}_{k-1} \to \bar{U}_k$ closer to the optimal $U^*$, but because the SPC update is a single score ascent step, samples do to always reach the optimal sampling distribution, especially in early iterations. To deal with this, we weigh the flow matching loss \eqref{eq:weights} according to cosine similarity with the update vector $\bar{U}_k - \bar{U}_{k-1}$. Shaded areas indicate regions of higher weight.}
    \label{fig:flow}
\end{figure}

The efficiency of this approach is limited, however, by the fact that while $\bar{U}_k$ move in the direction of higher probability mass, $\bar{U}_k$ are not necessarily optimal samples from the target distribution, especially in early iterations. This is illustrated in Fig.~\ref{fig:flow} for a toy 2D example. The SPC update is a monte-carlo score ascent step, but does not necessarily reach a true sample $U^*$ in a single step. This means that while the flow target (arrows) are accurate in some regions (e.g., lower right corner), they may be inaccurate in other areas (e.g., upper right corner between $\bar{U}_k$ and $U^*$).

To deal with this, we propose a small modification of the flow matching loss. We leverage the cosine similarity score
\begin{equation}
 S_C(\x, \y) = \frac{\x \cdot \y}{\|\x\|\|\y\|},
\end{equation}
which measures directional similarity between vectors $\x$ and $\y$. $S_C$ outputs a scalar in $[-1, 1]$, with high values indicating a high degree of similarity. In our case, if the update $\bar{U}_k - \bar{U}_{k-1}$ and the flow target $\bar{U}_k - U_0$ are close, then the sample $U_0 \sim \mathcal{N}(0, I)$ is more informative, so we weigh it higher. In particular we define weights
\begin{multline}\label{eq:weights}
 w(\bar{U}_k, \bar{U}_{k-1}, U_0) = \\
 \exp\left(-\gamma (1 - S_C(\bar{U}_k - \bar{U}_{k-1}, \bar{U}_k - U_0)))\right),
\end{multline}
where the hyperparameter $\gamma > 0$ defines a decay rate as the two vectors move further apart. This gives us $w = 1$ if $\bar{U}_{k-1}$ and $U_0$ are in alignment, and decays outside that range. This is illustrated with green shading in Fig.~\ref{fig:flow}. The final GPC loss is given by
\begin{multline}
 \mathcal{L}_{GPC}(\theta; U_0, \bar{U}_k, \bar{U}_{k-1}, x_k, t) = \\
 w(\bar{U}_k, \bar{U}_{k-1}, U_0) \mathcal{L}_{CFM}(\theta; U_0; \bar{U}_k, x_k, t),
\end{multline}
which adds a dependence on $\bar{U}_{k-1}$.

To further improve performance, GPC performs a cycle of SPC simulation and model fitting. This is illustrated in Fig.~\ref{fig:hero} and outlined in Algorithm~\ref{alg:gpc}. 

\begin{algorithm}
    \caption{Generative Predictive Control}
    \label{alg:gpc}
    \KwIn{SPC algorithm $g(U)$, flow matching model $p_\theta(U \mid x)$, system model $f(x, u)$.}
    \KwOut{Trained flow model parameters $\theta$.}
    
    \While{not converged}{
        \Comment{Gather simulation data}

        \For{$j = 1,...,N_E$} { \label{ae:envs}
            $x^{(j)}_0 \sim \mathcal{X}_0$

            $\bar{U}^{(j)}_0 \sim \mathcal{N}(0, \sigma^2I)$
            
            \For{$k \in [1, K]$}
            {
                \Comment{Sample action sequences}
                
                $U^{(i,j)} \sim \mathcal{N}(\bar{U}^{(j)}_{k-1}, \sigma^2 I), \quad i \in [1, N_S]$ \label{ae:spc_samples}
                
                $U^{(i,j)} \sim p_\theta(\bar{U}_{k-1}^{(j)} \mid x_{k-1}^{(j)}), \quad i \in [N_S, N]$ \label{ae:flow_samples}
                
                \Comment{Parallel rollouts}
                
                $J^{(i,j)} \gets J\left(U^{(i,j)}; x^{i,j}_{k-1}\right)$ \label{ae:rollouts}

                \Comment{Update actions via SPC}
                $\bar{U}_k^{(j)} \gets \bar{U}_{k-1}^{(j)} + \frac{\sum_{i=1}^{N} g(J^{(i,j)})(U^{(i,j)} - \bar{U}^{(j)}_{k-1})}{{\sum_{i=1}^N g(J^{(i,j)})}}$
                
                \Comment{Advance the simulations}
                % TODO: denote on-policy rollouts
                $x_{k}^{(j)} \gets f\left(x_{k-1}^{(j)}, u_k^{(j)}\right)$ \label{ae:sim}

            }
        }

        \Comment{Fit flow matching model}
        $\theta \gets \arg\min_{\theta} \mathbb{E}_{t, U_0, j, k}\left[\mathcal{L}_{GPC}\left(\theta; U_0, \bar{U}_k^{(j)}, \bar{U}_{k-1}^{(j)}, x_k^{(j)}, t\right)\right]$ \label{ae:flow}
    }
\end{algorithm}

In the simulation phase, we first sample initial states $x_0^{(j)}$ from some set of initial conditions $\mathcal{X}_0$ for $N_E$ separate simulation environments. We then proceed to perform SPC in each environment, with $N_S$ of the samples coming from the typical normal proposal distribution and the remaining $N - N_S$ samples from the flow matching model. The SPC update uses all $N$ samples to update the action sequence $\bar{U}_k^{j}$ for each initial condition $j$. 

When advancing the simulation (line \ref{ae:sim}), there are many valid choices for the action $u_k^{j}$. We could choose the initial input from $\bar{U}_k^{(j)}$, following the SPC scheme. Or we could choose one of the samples from $p_\theta$, resulting an on-policy strategy. Or we could choose some other action entirely, resulting in something akin to off-policy reinforcement learning. We found that an on-policy strategy led to more exploration of the state space in early iterations, and was generally the most effective in our examples. A more thorough evaluation of different time advancement strategies remains an important area for future research.

After collecting a dataset of states $x_k^{(j)}$ and action sequences $U_k^{(j)}$, we fit a flow matching model by minimizing $\mathcal{L}_{GPC}$ (line \ref{ae:flow}) with standard methods. The expectation in this case is over flow timesteps $t \in \mathcal{U}(0, 1)$, initial samples $U_0 \sim \mathcal{N}(0, I)$, parallel environments $j = 1,\dots, N_E$, and simulation steps $k = 1, \dots, K$.

This iterative process repeats until convergence. The inclusion of flow matching samples (line \ref{ae:flow_samples}) enables improved performance at each iteration, while the traditional SPC samples (line \ref{ae:spc_samples}) prevent distribution collapse. 

GPC benefits from parallelism throughout Algorithm~\ref{alg:gpc}. In addition to parallel rollouts in the SPC update step (line \ref{ae:rollouts}), we can parallelize over simulation environments (line \ref{ae:envs}) and in the model training step (line \ref{ae:flow}). Our implementation \cite{kurtz2025gpc} leverages the modern vectorization and parallelization tools in JAX \cite{jax2018github} together with the massively parallel robotics simulation made possible by MuJoCo MJX \cite{mjx}. 

\section{Using a Generative Policy}

In this section we discuss two ways to use a trained GPC policy: using policy samples directly, and seeding an SPC controller. 

\subsection{Warm-starts and Temporal Consistency}\label{sec:gpc_alone}

A key challenge in applying generative policies of the form $p_\theta(U \mid x)$ is the issue of \textit{temporal consistency} \cite{zhao2023learning}. In real-time feedback control, the multi-modal expressiveness of generative models presents a challenge: samples at subsequent timesteps can be drawn from different modes, leading to ``jittering'' behavior. 

The most common solution is to merely roll out several steps of the action sequence before replanning. This forces the controller to ``commit'' to a particular mode, but is not suitable for highly dynamic tasks that require rapid feedback. Other alternatives include averaging over samples produced at different timesteps \cite{zhao2023learning}, but this does not always work well in practice \cite{black2024pi_0}.

We propose a simple alternative inspired by warm-starts in MPC. Rather than starting the flow generation process from $U_0 \sim \mathcal{N}(0, I)$, we start from 
\begin{equation}
    U_0 = (1 - \alpha) \mathcal{N}(0, I) + \alpha \bar{U}_{k - 1}
\end{equation}
where $\alpha \in [0, 1]$ is the \textit{warm-start level}. With $\alpha = 1$, the flow process is started from the previous sample $\bar{U}_{k-1}$, while $\alpha = 0$ recovers the original normal proposal distribution. Because flow matching essentially defines a vector field driving samples toward a mode of the sampling distribution, flows with a high warm-start level $\alpha$ tend to stay close to the same mode as the previous sample $\bar{U}_{k-1}$.

We find that this simple warm-start procedure results in smooth and performant control signals at fast feedback rates, as shown in Sec.~\ref{sec:experiments} below.

\subsection{Seeding Sampling-based Predictive Control}\label{sec:seeding_spc}

Another possible use of a trained GPC policy is to add samples $\bar{U} \sim p_\theta(U \mid x)$ from the trained flow matching model to bootstrap an SPC controller. This also alleviates concerns with mode switching and temporal consistency, as mode switches only occur if the switch results in a lower cost, as characterized by the SPC weighting function $g(\cdot)$.

Seeding SPC with a flow matching model also alleviates many of the uni-modality concerns often raised regarding SPC algorithms \cite{zhang2024multi}, and can allow for effective performance with fewer samples and therefore faster control loops. This approach also opens the door to continual improvement after deployment, as these samples can be used to further refine the flow matching model as in Algorithm.~\ref{alg:gpc}.

We found that seeding SPC offered significantly better performance than direct flow matching on the largest examples we tested, as detailed in Sec.~\ref{sec:experiments} below. While SPC is computationally cheap, this improved performance does come with some limitations: a full state-estimate is required to perform the SPC rollouts, and conditioning on observations $y = h(x)$ directly is no longer feasible. 

\section{Domain Randomization Strategies}\label{sec:online_dr}

Domain Randomization (DR) has emerged as a key ingredient in enabling sim-to-real transfer of policies trained in simulation, particularly for reinforcement learning \cite{handa2023dextreme, tobin2017domain}. It is reasonable to expect that DR also has a critical role to play in sim-to-real transfer of GPC policies.

Fortunately, the availability of massively parallel simulators and the structure of the SPC/GPC paradigm enables a range of new DR possibilities. In particular, we can modify the SPC rollouts \eqref{eq:spc_rollouts} by simulating each action sequence $U^{(i)}$ in several domains with randomized parameters (e.g., friction coefficients, body masses, etc.). This results in cost values indexed by both sample $i$ and domain $d$, e.g.,
\begin{equation}
    J^{(i, d)} = J(U^{(i)}; x_{k-1}, d).
\end{equation}
We then aggregate this cost data across domains before performing the standard SPC update \eqref{eq:spc_update} on the net costs. 

The simplest choice would be take the average cost over all domains,
\begin{equation}\label{eq:average_dr}
    J^{(i)} = \mathbb{E}_d \left[ J^{(i, d)} \right].
\end{equation}
This is analogous to the typical RL domain randomization framework, which considers the expected reward over all domains. 

But the SPC/GPC framework allows for other possibilities as well. We can, for instance, take a more conservative approach and use the worst-case cost
\begin{equation}
    J^{(i)} = \max_d \left[ J^{(i, d)} \right].
\end{equation}

We can even use more sophisticated risk metrics like conditional value-at-risk (CVaR) \cite{rockafellar2000optimization, dixit2023risk}, which takes the expected cost in the $(1 - \beta)$ tail of the distribution:
\begin{equation}\label{eq:cvar_dr}
    J^{(i)} = \inf_{z \in \mathbb{R}} \mathbb{E}_d \left[ z + \frac{\max(J^{(i, d)} - z, 0)}{1 - \beta} \right].
\end{equation}
The parameter $\beta \in [0, 1]$ determines the degree of risk sensitivity. 

These and other risk strategies for online domain randomization are implemented in \texttt{hydrax} \cite{kurtz2024hydrax}, making them readily available for both GPC training and online deployment in a bootstrapped SPC controller. We illustrate the impact of different domain randomization strategies in Section~\ref{sec:domain_randomization} below.

\section{Simulation Experiments}\label{sec:experiments}

\begin{figure*}
    \centering
    \includegraphics[width=0.13\linewidth]{figures/pendulum.png}
    \includegraphics[width=0.13\linewidth]{figures/cart_pole.png}
    \includegraphics[width=0.13\linewidth]{figures/double_cart_pole.png}
    \includegraphics[width=0.13\linewidth]{figures/pusht.png}
    \includegraphics[width=0.13\linewidth]{figures/walker.png}
    \includegraphics[width=0.13\linewidth]{figures/crane.png}
    \includegraphics[width=0.13\linewidth]{figures/humanoid.png}
    \caption{Systems used to evaluate GPC performance, from left to right: inverted pendulum, cart-pole, double cart-pole, push-T, planar biped, luffing crane, humanoid.}
    \label{fig:systems}
\end{figure*}

In this section, we present the results from simulation experiments on each of the systems shown in Fig.~\ref{fig:systems}. We aim to answer the following questions:
\begin{enumerate}
    \item Can GPC perform tasks that require multi-modal reasoning, as well highly dynamic tasks that require high-frequency feedback (Sec.~\ref{sec:policy_performance})?
    \item Can GPC continually improve its performance over multiple iterations (Sec.~\ref{sec:training_stability})?
    \item How do different domain randomization strategies impact performance with model error (Sec.~\ref{sec:domain_randomization})?
    \item What are the scalability limits of this approach (Sec.~\ref{sec:scalability})?
\end{enumerate}

We find that GPC is effective for control systems with fast dynamics at high feedback rates, enjoys the training stability characteristic of supervised learning methods, and enables risk-aware control, but struggles to scale to our largest and most difficult example (humanoid standup). We provide further discussion of these scalability limitations, and how they might be overcome in the future, in Section~\ref{sec:limitations}.

\subsection{Example Systems}\label{sec:experiments:sys}

We test GPC on seven example systems of varying scale and difficulty. Each of these systems are introduced briefly below, with further details available in the open-source implementation \cite{kurtz2025gpc}.

\textbf{Inverted pendulum:} This simple one-dimensional system requires swinging a pendulum to the upright position and balancing it there. Torque limits prevent the pendulum from swinging directly upright: the policy must gradually pump energy into the system. 

\textbf{Cart-pole:} An unactuated pendulum is mounted on an actuated cart. Control actions are torques applied to the cart, and the task is to balance the pendulum upright. While this is a relatively simple nonlinear system, obtaining successful human demonstrations would be difficult.

\textbf{Double cart-pole:} In this extension of the cart-pole example, an unactuated double pendulum is mounted on the cart. The fast and chaotic double pendulum dynamics make this task particularly challenging. 

\textbf{Push-T:} A robotic finger pushes a T-shaped block to a target position and orientation on a table. This task has been solved with behavior cloning \cite{chi2023diffusion}, and is a standard example of a task that requires multi-modal reasoning. 

\textbf{Planar biped:} A robot walker, constrained to the sagittal plane, is tasked with moving forward at 1.5 m/s. The high dimensionality of this system would make teleoperation for behavior cloning difficult. Successful locomotion also requires fast replanning.

\textbf{Luffing crane:} A swinging payload is attached via a rope to a luffing crane. Control actions are target boom angles and rope length. This underactuated system provides a particularly useful testbed for investigating the impact of modeling errors and domain randomization strategies.

\textbf{Humanoid standup:} A Unitree G1 humanoid model is tasked with reaching a standing configuration. Initial conditions are random joint angles, joint velocities, and base orientation, so that the robot begins sprawled on the ground.

A brief summary of each of these examples is shown in Table~\ref{tab:summary}. For the smaller examples we use a simple Multi-Layer Perceptron (MLP) architecture for the flow network $v_\theta$, while for the larger examples we use a Convolutional Neural Network (CNN) with FiLM conditioning \cite{perez2018film}, as recommended in \cite{chi2023diffusion}. All experiments were performed on a desktop computer with an Nvidia RTX 4070 (12 GB) GPU.

\begin{table*}[]
    \centering
    \begin{tabular}{c|ccccccc}
         Name & Pendulum & Cart-Pole & Double Cart-Pole & Push-T & Walker & Crane & Humanoid \\
         \hline
         DoFs & 1 & 2 & 3 & 5 & 9 & 7 & 29 \\
         Num. actuators & 1 & 1 & 1 & 2 & 6 & 3 & 23 \\
         Planning horizon (sec) & 0.5 & 1.0 & 0.8 & 0.5 & 0.6 & 0.8 & 0.9 \\
         Network architecture & MLP & MLP & MLP & CNN & CNN & CNN & CNN \\
         Train time (hr:min:sec) & 0:00:30 & 0:01:20 & 0:17:27 & 0:17:22 & 0:06:25 & 0:09:41 & 2:46:55 \\
         Inference time (ms) & 1.0 & 1.3 & 1.3 & 3.8 & 8.1 & 2.4 & 4.0 \\
         GPC-seeded SPC is effective & \cmark & \cmark & \cmark & \cmark & \cmark & \cmark & \cmark   \\
         GPC alone is effective & \cmark & \cmark & \cmark & \cmark & \cmark & \cmark & \xmark   \\
    \end{tabular}
    \caption{Summary of each of the example systems. Full details and hyperparameter configurations are available at \cite{kurtz2025gpc}}
    \label{tab:summary}
\end{table*}

\subsection{Policy Performance}\label{sec:policy_performance}

Footage of closed-loop GPC performance on each of the examples is shown in the accompanying video. In all the examples, the GPC policy was effective in improving closed-loop performance when used in conjunction with SPC, as described in Sec.~\ref{sec:seeding_spc}. Applying the GPC policy directly (Sec.~\ref{sec:gpc_alone}) was effective in all cases except the humanoid.

\textbf{GPC can handle multi-modal action distributions.} This is evidenced by effectiveness on the Push-T example, which requires multi-modal reasoning to reach around the block \cite{chi2023diffusion}. Interestingly, GPC training takes under 20 minutes, while training a similar Push-T policy with diffusion-based behavior cloning (and full state observations) takes around an hour.

\begin{figure}
    \centering
    \includegraphics[width=\linewidth]{figures/warm_starts.pdf}
    \caption{Closed-loop performance for the double cart-pole with and without warm-starts. The warm-started GPC controller (right) produces smooth control actions and is able to successfully balance the double pendulum upright. Without warm-starts (left), control actions are dominated by noisy jittering between modes, and the system does not balance.}
    \label{fig:warm-starts}
\end{figure}

More importantly, \textbf{GPC can control systems with fast dynamics at high control rates}. The double cart-pole example provides a particularly clear example of this, as well as the importance of warm-starts (Sec.~\ref{sec:gpc_alone}). Fig.~\ref{fig:warm-starts} illustrates performance with and without warm-starts. Without warm-starts (left, $\alpha = 0$), the control actions are dominated by significant noise (top plot) and the system is unable to swing upright (bottom plot). With warm-starts (right, $\alpha = 1$), we obtain smoother control actions and successfully balance around the upright position. The controller is still able to respond rapidly to the chaotic system dynamics, as evidenced by the rapid changes between 1 and 2 seconds. 

\subsection{Training Stability}\label{sec:training_stability}

Training curves for each of the examples are shown in Fig.~\ref{fig:training_curves}. In addition to the average loss and average cost at each iteration, we plot the percent of states in which the flow-matching policy generates the best action sequence. Recall that some samples come from the policy, while others are from the SPC normal proposal distribution (Algorithm~\ref{alg:gpc}, lines ~\ref{ae:spc_samples}-\ref{ae:flow_samples}). A higher percentage of best samples coming from the policy is a useful indicator of policy performance, and tends to increase monotonically (modulo noise from randomized initial conditions) throughout the training process.

\begin{figure*}
    \centering
    \includegraphics[width=\linewidth]{figures/training_curves.pdf}
    \caption{Training curves showing the average loss $\mathcal{L}_{GPC}$, percent of states in which the flow-matching policy generated the best action sequence, and the average cost of the best rollout. GPC is able to leverage the training stability of supervised learning while avoiding the need for demonstrations.}
    \label{fig:training_curves}
\end{figure*}

Interestingly, this percentage does not reach 100\%, even after many iterations. We suspect that this is due to the fact that a good sample from the policy also improves SPC performance at the following time step, since this will shift the mean of the proposal distribution, $\bar{U}_k$, close to optimality. 

While we leave a systematic hyperparameter sensitivity study for future work, our empirical observation is that \textbf{GPC benefits from the training stability characteristic of supervised learning} methods. This is in contrast to reinforcement learning methods, which can exhibit high sensitivity to reward tuning, implementation details, and even the random seed used for training \cite{engstrom2019implementation, andrychowicz2020matters}. 

\subsection{Domain Randomization}\label{sec:domain_randomization}

\textbf{GPC offers an opportunity to train with risk-sensitive domain randomization schemes}, as outlined in Sec.~\ref{sec:online_dr}. We use the luffing crane example to explore the impact of different strategies on the performance of a trained policy. 

In particular, we train three GPC policies: one with no domain randomization, one with average-cost domain randomization \eqref{eq:average_dr}, and one with more conservative CVaR ($\beta = 0.25$) domain randomization \eqref{eq:cvar_dr}. For the domain randomized models, we used 8 randomized domains. Each domain uses a slightly different model with modified joint damping, payload mass, payload inertia, and actuator gains.

After training, we apply each policy directly with warm-starts (Sec.~\ref{sec:gpc_alone}). To evaluate closed-loop performance, we generate 50 random target locations that the payload must visit. The target is moved to the next location once the payload is moved within 15 cm or after 10 seconds, whichever comes first. We report the total time to visit all 50 targets in Table~\ref{tab:crane_dr}: lower times indicate better performance. The same target locations are used for each policy.

\begin{table}[h]
    \centering
    \begin{tabular}{c|ccc}
        & No DR & Avg. & CVaR(0.25) \\
        \hline
         No model error & 106 & \textbf{103} & 133   \\
        \hline
         With model error & 165 & 184 & \textbf{139}
    \end{tabular}
    \caption{Time (in seconds) for the crane to visit a sequence of 50 randomly generated payload target positions, under policies trained with various domain randomization schemes.}
    \label{tab:crane_dr}
\end{table}

We first test the policies without any model error in the simulator. In this case both the non-randomized policy and average-case domain randomization perform significantly better than the more conservative CVaR policy. When we add a small amount of model error to the simulator (lower joint damping, heavier payload mass), all methods perform worse. But the more conservative CVaR policy degrades the least, and significantly outperforms the other policies. 

\subsection{Scalability}\label{sec:scalability}

In choosing our set of example systems, which range from 1 to 29 degrees-of-freedom, we aim to push the limits of the scalability of our method. This \textbf{scalability hits a bottleneck with the largest humanoid example}: the trained policy is unable to reliably drive the robot to stand up, though using the policy to seed SPC is effective (as shown in the accompanying video).

It is likely that further cost and hyperparameter tuning, as well as the addition of more computation, would result in better performance for the humanoid example. However, state-of-the-art reinforcement learning methods are able to train similar behaviors \cite{freeman2021brax, raffin2021stable}, suggesting that further algorithmic improvements in the GPC framework are necessary to achieve reliable scalability to such systems.

\section{Limitations and Future Work}\label{sec:limitations}

Limited effectiveness on our largest (humanoid standup) example is the most severe limitation of our proposed method, as discussed above. We believe that \textbf{value function learning will be a critical component in overcoming these scalability limitations}. In addition to being a key element of state-of-the-art reinforcement learning methods like PPO, value learning would allow us to reduce the planning horizon $T$ in problem \ref{eq:ocp}. Reducing the planning horizon reduces the dimensionality of the sampling space, making planning easier while maintaining long-horizon reasoning. Methods that leverage connections between the gradient of the value function and the flow field $v_\theta$ are of particular interest.

Similarly, the sample complexity of the basic GPC framework introduced here is poor relative to state-of-the-art RL algorithms. To generate a single training data point, we run $N$ short simulations---and even more under a risk-based domain randomization strategy. While advances in computing speeds may mean that the benefits of a supervised learning framework eventually outweigh the computational costs, methods of more fully using \textit{all} of the data from SPC rollouts could significantly improve the GPC framework.

Other performance improvements could come from more benign algorithmic details. For instance, our SPC implementation \cite{kurtz2024hydrax} represents action sequences with a simple zero-order-hold spline parameterization. Higher-order splines \cite{howell2022predictive} or alternative parameterizations could be more effective. Better choices of action space, such as using task-space/end-effector coordinates rather than joint coordinates, could also be useful. Actuation limits are a critical component of many robotics tasks, but our implementation does not handle these in any particularly special way. Leveraging recent advances in constrained generative modeling \cite{fishman2024diffusion, fishman2024metropolis,kurtz2024equality} to do so is another potentially fruitful area for future work. 

Future work will also focus on hardware experiments. Hardware experiments are a critical component of evaluating any robotic control framework: this is particularly the case when it comes to domain randomization strategies. This will also provide an important platform for exploring policies conditioned on various observations, ranging from raw sensor data to images to foundation model embeddings like DINO \cite{oquab2023dinov2}. Integrated simulation and rendering in the recently-released MuJoCo playground \cite{mujoco_playground_2025} could provide a useful software platform for exploring image-conditioned policies. 

Finally, the GPC framework offers a means for including dynamic and difficult-to-demonstrate tasks in a generalist policy \cite{black2024pi_0} or large behavior model \cite{lbm} framework that combines data from multiple tasks in a single policy. Exploring this possibility remains an important topic for future work.

\section{Conclusion}

We introduced Generative Predictive Control, a framework for learning flow matching policies on dynamic tasks that are easy to simulate but difficult to demonstrate. GPC leverages tight connections between generative modeling and sampling-based predictive control to generate training data for supervised learning without expert demonstrations. We showed how warm-started GPC policies enable real-time control at over 500 Hz, ensuring temporal consistency via warm-starts. Future work will focus on validating the GPC framework on hardware, incorporating value function learning, and training generalist multi-task policies.

\bibliographystyle{IEEEtran}
\bibliography{references}

\end{document}
