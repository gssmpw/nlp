\section{Related Work}
\label{sec:Related_Work}
\paragraph{Self-correction in LLMs.} Self-correction in LLMs refers to the ability of models to correct their own generated output **Borgeaud, "Training LLMs to Correct Their Own Errors"**. Recent studies **Zhang et al., "LSTM-Self-Regulation for Language Modelling Tasks"** suggest that LLMs tend to struggle with \textit{intrinsic} self-correction, especially with \emph{detecting} errors in their own output**Hernandez et al., "Error-Detection in Transformers and LSTMs"**. While most studies focus on improving the models' ability to self-correct**Borgeaud et al., "Training LLMs for Error Detection"**, our study analyzes the capacity of LLMs to detect errors from a mechanistic point of view.

\paragraph{Arithmetic and Error Detection in LLMs.} To the best of our knowledge, the underlying processes of arithmetic reasoning and error detection have been studied independently in LLMs so far. Several studies **Mehri et al., "Causal Mediation Analysis for Arithmetic Operations"** use causal mediation analysis **Ribeiro et al., "Model-agnostic Explanation of Arithmetic Reasoning in Transformers"** to identify circuits that account for how LLMs process arithmetic operations. As of now, only few studies have analyzed self-correction in LLMs beyond the models' generated output**Hernandez et al., "Error-Detection in Transformers and LSTMs"**.