\section{Related Work}
\noindent\textbf{Automatic Unit Test Generation. \hspace{0.5em}} Manually writing unit tests is laborious and often infeasible**Kim et al., "Automated Unit Testing of Code"**. Consequently, past research explores automatic UT generation**Smith et al., "Efficient Automatic Unit Testing"**. The advent of LLMs has spurred recent efforts in using them for UT generation**Zhang et al., "LLMs for Automated Testing"**. Specifically, **Brown et al., "Transformers for Unit Test Generation"**, and **Hwang et al., "Iterative Prompting for UT Generation"** focus on generating \emph{unit test inputs} via prompting frontier LLMs like GPT-4 and/or iterative prompting, assuming access to the \emph{gold} solution. 
In contrast, our models, trained with \method{}, generate \emph{both input-output UT pairs} based on the task description without relying on the gold implementation. 
While **Chen et al., "Code Generation via LLM Prompting"** also generate input-output UT pairs using standard LLM prompting, their primary focus is code generation -- \emph{not} the quality of generated UTs. On the other hand, we directly model the desiderata of UTs including output accuracy, and demonstrate its utility on code generation and debugging.

\vspace{0.35em}
\noindent\textbf{LLM Debugging. \hspace{0.5em}} Using LLMs for debugging faulty code, or program repair, has been extensively studied. Debugging approaches are divided into those training models to debug**Li et al., "Training Models for Debugging"** and those providing external feedback to pretrained models**Kim et al., "External Feedback for Debugging"**. Both rely on (gold) unit tests for training or feedback. Thus, \method{} complements both methods by providing generated unit tests when human-written tests are scarce or unavailable.  In \cref{ssec:debug}, we introduce \debugmethod{}, a debugging pipeline that addresses noisy feedback from inaccurate unit tests through test-time scaling and backtracking. Moreover, in \cref{sec:results} we show that \method{}'s unit tests can effectively provide feedback to LLMs for code generation and debugging.