@inproceedings{wan22,
    title = "A Unified Dialogue User Simulator for Few-shot Data Augmentation",
    author = "Wan, Dazhen  and
      Zhang, Zheng  and
      Zhu, Qi  and
      Liao, Lizi  and
      Huang, Minlie",
    editor = "Goldberg, Yoav  and
      Kozareva, Zornitsa  and
      Zhang, Yue",
    booktitle = "Findings of the Association for Computational Linguistics: EMNLP 2022",
    month = dec,
    year = "2022",
    address = "Abu Dhabi, United Arab Emirates",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.findings-emnlp.277/",
    doi = "10.18653/v1/2022.findings-emnlp.277",
    pages = "3788--3799",
    abstract = "Pre-trained language models have shown superior performance in task-oriented dialogues. However, existing datasets are on limited scales, which cannot support large-scale pre-training. Fortunately, various data augmentation methods have been developed to augment large-scale task-oriented dialogue corpora. However, they heavily rely on annotated data in the target domain, which require a tremendous amount of data collection and human labeling work. In this paper, we build a unified dialogue user simulation model by pre-training on several publicly available datasets. The model can then be tuned on a target domain with few-shot data. The experiments on a target dataset across multiple domains show that our proposed model brings remarkable performance increases through data augmentation."
}
@misc{soudani24,
      title={A Survey on Recent Advances in Conversational Data Generation}, 
      author={Heydar Soudani and Roxana Petcu and Evangelos Kanoulas and Faegheh Hasibi},
      year={2024},
      eprint={2405.13003},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2405.13003}, 
}
@misc{zhang24,
      title={A Comprehensive Analysis of the Effectiveness of Large Language Models as Automatic Dialogue Evaluators}, 
      author={Chen Zhang and Luis Fernando D'Haro and Yiming Chen and Malu Zhang and Haizhou Li},
      year={2024},
      eprint={2312.15407},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2312.15407}, 
}
@misc{jia24,
      title={Leveraging LLMs for Dialogue Quality Measurement}, 
      author={Jinghan Jia and Abi Komma and Timothy Leffel and Xujun Peng and Ajay Nagesh and Tamer Soliman and Aram Galstyan and Anoop Kumar},
      year={2024},
      eprint={2406.17304},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2406.17304}, 
}
@misc{zhang20,
      title={Recent Advances and Challenges in Task-oriented Dialog System}, 
      author={Zheng Zhang and Ryuichi Takanobu and Qi Zhu and Minlie Huang and Xiaoyan Zhu},
      year={2020},
      eprint={2003.07490},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2003.07490}, 
}
@inproceedings{xu24,
    title = "Rethinking Task-Oriented Dialogue Systems: From Complex Modularity to Zero-Shot Autonomous Agent",
    author = "Xu, Heng-Da  and
      Mao, Xian-Ling  and
      Yang, Puhai  and
      Sun, Fanshu  and
      Huang, Heyan",
    editor = "Ku, Lun-Wei  and
      Martins, Andre  and
      Srikumar, Vivek",
    booktitle = "Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = aug,
    year = "2024",
    address = "Bangkok, Thailand",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2024.acl-long.152",
    doi = "10.18653/v1/2024.acl-long.152",
    pages = "2748--2763",
}
@misc{balaguer24,
      title={RAG vs Fine-tuning: Pipelines, Tradeoffs, and a Case Study on Agriculture}, 
      author={Angels Balaguer and Vinamra Benara and Renato Luiz de Freitas Cunha and Roberto de M. Estevão Filho and Todd Hendry and Daniel Holstein and Jennifer Marsman and Nick Mecklenburg and Sara Malvar and Leonardo O. Nunes and Rafael Padilha and Morris Sharp and Bruno Silva and Swati Sharma and Vijay Aski and Ranveer Chandra},
      year={2024},
      eprint={2401.08406},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2401.08406}, 
}
@misc{ovadia24,
      title={Fine-Tuning or Retrieval? Comparing Knowledge Injection in LLMs}, 
      author={Oded Ovadia and Menachem Brief and Moshik Mishaeli and Oren Elisha},
      year={2024},
      eprint={2312.05934},
      archivePrefix={arXiv},
      primaryClass={cs.AI},
      url={https://arxiv.org/abs/2312.05934}, 
}
@inproceedings{kim23,
    title = "{SODA}: Million-scale Dialogue Distillation with Social Commonsense Contextualization",
    author = "Kim, Hyunwoo  and
      Hessel, Jack  and
      Jiang, Liwei  and
      West, Peter  and
      Lu, Ximing  and
      Yu, Youngjae  and
      Zhou, Pei  and
      Bras, Ronan  and
      Alikhani, Malihe  and
      Kim, Gunhee  and
      Sap, Maarten  and
      Choi, Yejin",
    editor = "Bouamor, Houda  and
      Pino, Juan  and
      Bali, Kalika",
    booktitle = "Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing",
    month = dec,
    year = "2023",
    address = "Singapore",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2023.emnlp-main.799/",
    doi = "10.18653/v1/2023.emnlp-main.799",
    pages = "12930--12949",
}
@misc{samarinas24,
      title={Simulating Task-Oriented Dialogues with State Transition Graphs and Large Language Models}, 
      author={Chris Samarinas and Pracha Promthaw and Atharva Nijasure and Hansi Zeng and Julian Killingback and Hamed Zamani},
      year={2024},
      eprint={2404.14772},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2404.14772}, 
}
@misc{xu24-2,
      title={HR-MultiWOZ: A Task Oriented Dialogue (TOD) Dataset for HR LLM Agent}, 
      author={Weijie Xu and Zicheng Huang and Wenxiang Hu and Xi Fang and Rajesh Kumar Cherukuri and Naumaan Nayyar and Lorenzo Malandri and Srinivasan H. Sengamedu},
      year={2024},
      eprint={2402.01018},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2402.01018}, 
}
@inproceedings{duan24,
    title = "{B}ot{C}hat: Evaluating {LLM}s' Capabilities of Having Multi-Turn Dialogues",
    author = "Duan, Haodong  and
      Wei, Jueqi  and
      Wang, Chonghua  and
      Liu, Hongwei  and
      Fang, Yixiao  and
      Zhang, Songyang  and
      Lin, Dahua  and
      Chen, Kai",
    editor = "Duh, Kevin  and
      Gomez, Helena  and
      Bethard, Steven",
    booktitle = "Findings of the Association for Computational Linguistics: NAACL 2024",
    month = jun,
    year = "2024",
    address = "Mexico City, Mexico",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2024.findings-naacl.201/",
    doi = "10.18653/v1/2024.findings-naacl.201",
    pages = "3184--3200",
}
@misc{salinas25,
      title={Tuning LLM Judges Hyperparameters}, 
      author={David Salinas and Omar Swelam and Frank Hutter},
      year={2025},
      eprint={2501.17178},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2501.17178}, 
}
@misc{zheng23,
      title={Judging LLM-as-a-Judge with MT-Bench and Chatbot Arena}, 
      author={Lianmin Zheng and Wei-Lin Chiang and Ying Sheng and Siyuan Zhuang and Zhanghao Wu and Yonghao Zhuang and Zi Lin and Zhuohan Li and Dacheng Li and Eric P. Xing and Hao Zhang and Joseph E. Gonzalez and Ion Stoica},
      year={2023},
      eprint={2306.05685},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2306.05685}, 
}
@misc{dubois24,
      title={Length-Controlled AlpacaEval: A Simple Way to Debias Automatic Evaluators}, 
      author={Yann Dubois and Balázs Galambosi and Percy Liang and Tatsunori B. Hashimoto},
      year={2024},
      eprint={2404.04475},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2404.04475}, 
}
@misc{touvron23,
      title={Llama 2: Open Foundation and Fine-Tuned Chat Models}, 
      author={Hugo Touvron, Louis Martin, Kevin Stone et al.},
      year={2023},
      eprint={2307.09288},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2307.09288}, 
}
@misc{grattafiori24,
      title={The Llama 3 Herd of Models}, 
      author={Aaron Grattafiori, Abhimanyu Dubey, Abhinav Jauhri at al.},
      year={2024},
      eprint={2407.21783},
      archivePrefix={arXiv},
      primaryClass={cs.AI},
      url={https://arxiv.org/abs/2407.21783}, 
}
@misc{qin23,
      title={End-to-end Task-oriented Dialogue: A Survey of Tasks, Methods, and Future Directions}, 
      author={Libo Qin and Wenbo Pan and Qiguang Chen and Lizi Liao and Zhou Yu and Yue Zhang and Wanxiang Che and Min Li},
      year={2023},
      eprint={2311.09008},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2311.09008}, 
}
@article{kruskal52,
 ISSN = {01621459, 1537274X},
 URL = {http://www.jstor.org/stable/2280779},
 author = {William H. Kruskal and W. Allen Wallis},
 journal = {Journal of the American Statistical Association},
 number = {260},
 pages = {583--621},
 publisher = {[American Statistical Association, Taylor & Francis, Ltd.]},
 title = {Use of Ranks in One-Criterion Variance Analysis},
 urldate = {2025-02-03},
 volume = {47},
 year = {1952}
}
@article{friedman37,
author = {Milton Friedman},
title = {The Use of Ranks to Avoid the Assumption of Normality Implicit in the Analysis of Variance},
journal = {Journal of the American Statistical Association},
volume = {32},
number = {200},
pages = {675--701},
year = {1937},
publisher = {ASA Website},
doi = {10.1080/01621459.1937.10503522},
URL = {https://www.tandfonline.com/doi/abs/10.1080/01621459.1937.10503522},
eprint = { ttps://www.tandfonline.com/doi/pdf/10.1080/01621459.1937.10503522}
}
@article{wilcoxon45,
 ISSN = {00994987},
 URL = {http://www.jstor.org/stable/3001968},
 author = {Frank Wilcoxon},
 journal = {Biometrics Bulletin},
 number = {6},
 pages = {80--83},
 publisher = {[International Biometric Society, Wiley]},
 title = {Individual Comparisons by Ranking Methods},
 urldate = {2025-02-03},
 volume = {1},
 year = {1945}
}
@misc{panickssery24,
      title={LLM Evaluators Recognize and Favor Their Own Generations}, 
      author={Arjun Panickssery and Samuel R. Bowman and Shi Feng},
      year={2024},
      eprint={2404.13076},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2404.13076}, 
}
@article {bender97,
	Title = {Ordinal logistic regression in medical research},
	Author = {Bender, R and Grouven, U},
	Number = {5},
	Volume = {31},
	Year = {1997},
	Journal = {Journal of the Royal College of Physicians of London},
	ISSN = {0035-8819},
	Pages = {546—551},
	Abstract = {Medical research workers are making increasing use of logistic regression analysis for binary and ordinal data. The purpose of this paper is to give a non-technical introduction to logistic regression models for ordinal response variables. We address issues such as the global concept and interpretation of logistic models, the model building procedure from a practical point of view, and the assessment of the model adequacy. For illustrative purposes we apply these methods to real data of a study investigating the association between glycosylated haemoglobin and retinopathy. We give some recommendations for the use and assessment of ordinal logistic regression models in medical research.},
	URL = {https://europepmc.org/articles/PMC5420958},
}
@misc{fu24,
      title={Imprompter: Tricking LLM Agents into Improper Tool Use}, 
      author={Xiaohan Fu and Shuheng Li and Zihan Wang and Yihao Liu and Rajesh K. Gupta and Taylor Berg-Kirkpatrick and Earlence Fernandes},
      year={2024},
      eprint={2410.14923},
      archivePrefix={arXiv},
      primaryClass={cs.CR},
      url={https://arxiv.org/abs/2410.14923}, 
}
@misc{hu24,
      title={Explaining Length Bias in LLM-Based Preference Evaluations}, 
      author={Zhengyu Hu and Linxin Song and Jieyu Zhang and Zheyuan Xiao and Tianfu Wang and Zhengyu Chen and Nicholas Jing Yuan and Jianxun Lian and Kaize Ding and Hui Xiong},
      year={2024},
      eprint={2407.01085},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2407.01085}, 
}
@misc{salinas25-2,
      title={Tuning LLM Judge Design Decisions for 1/1000 of the Cost}, 
      author={David Salinas and Omar Swelam and Frank Hutter},
      year={2025},
      eprint={2501.17178},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2501.17178}, 
}
@misc{li25,
      title={Rethinking Mixture-of-Agents: Is Mixing Different Large Language Models Beneficial?}, 
      author={Wenzhe Li and Yong Lin and Mengzhou Xia and Chi Jin},
      year={2025},
      eprint={2502.00674},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2502.00674}, 
}
@misc{suresh25,
      title={DiaSynth: Synthetic Dialogue Generation Framework for Low Resource Dialogue Applications}, 
      author={Sathya Krishnan Suresh and Wu Mengjun and Tushar Pranav and Eng Siong Chng},
      year={2025},
      eprint={2409.19020},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2409.19020}, 
}
@misc{rafailov24,
      title={Direct Preference Optimization: Your Language Model is Secretly a Reward Model}, 
      author={Rafael Rafailov and Archit Sharma and Eric Mitchell and Stefano Ermon and Christopher D. Manning and Chelsea Finn},
      year={2024},
      eprint={2305.18290},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2305.18290}, 
}
@misc{he24,
      title={Does Prompt Formatting Have Any Impact on LLM Performance?}, 
      author={Jia He and Mukund Rungta and David Koleczek and Arshdeep Sekhon and Franklin X Wang and Sadid Hasan},
      year={2024},
      eprint={2411.10541},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2411.10541}, 
}