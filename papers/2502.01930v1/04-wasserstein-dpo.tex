\section{Distributionally Robust DPO}

In this section, we give the formulation of our Wasserstein DPO (WDPO) and Kullback-Leibler DPO (KLDPO). 

\textbf{Sampling Procedure: } As introduced in \cref{sec:prelim-rlhf}, a prompt $s\in\states$ is sampled from some initial prompt (context) distribution $\mu^o$. Then two responses are sampled independently from $\pi^o$ (empirically we will set $\pi^o=\piSFT$), i.e., $a^1,a^2 \simiid \pi^o(\cdot\mid s)$. Similar to \citet{zhu2023principled}, we introduce the variable $y\in\{0,1\}$ where $y=1$ indicates the event $a^1\succ a^2\mid s$, and $y=0$ indicates the event $a^2\succ a^1\mid s$. Lastly, we will sample a Bernoulli random variable $y$ according to the BT model $P^*$. Formally, we define the joint data-generating distribution as follows.
\begin{definition}[Joint data-generating distribution]
    Consider the product space $\cZ\coloneqq \states\times\actions\times\actions\times\{0,1\}$. We define the nominal data-generating distribution as
    \begin{align*}
        \sfP^o(s,a^1,a^2,y) = \mu^o(s)\pi^o(a^1\mid s)\pi^o(a^2\mid s)\cdot[\indic_{\{y=1\}}P^*(a^1\succ a^2\mid s) + \indic_{\{y=0\}}P^*(a^2\succ a^1\mid s)].
    \end{align*}
\end{definition}
We will also denote $z=(s,a^1,a^2,y)\in\cZ$ and $\sfP^o(z)=\sfP^o(s,a^1,a^2,y)$. We assume that  $\sfP^o$ generates the dataset $\cD=\{z_i\}_{i=1}^n$ used for learning, i.e., $z_i\sim\sfP^o$.

\subsection{Distributionally Robust DPO}
From the DPO objective (\cref{eq:dpo-loss-dataset}), we define the \textit{pointwise} DPO loss function as follows
\begin{align}\label{eq:pointwise-dpo-loss}
    l(z;\theta) = -y \log\sigma(\beta h_\theta(s,a^1,a^2)) - (1-y) \log\sigma(\beta h_\theta(s,a^2,a^1)),
\end{align}
where $h_\theta(s,a^1,a^2)\coloneqq \log\frac{\pi_\theta(a^1\mid s)}{\piref(a^1\mid s)}-\log\frac{\pi_\theta(a^2\mid s)}{\piref(a^2\mid s)}$ is the \textit{preference score} of an answer $a^1$ relative to another one $a^2$ (but parameterized in policy parameter $\theta)$. Let $\cP(\rho;\sfP^o)$ be a distributional uncertainty set centered around $\sfP^o$ with radius $\rho>0$. Following the principles of distributionally robust optimization (DRO), we formulate the distributionally robust DPO objective as:
\begin{equation}\label{eq:generic-drdro-objective}
\min_{\theta} \max_{\sfP\in\cP(\rho;\sfP^o)} \EE_{z\sim\sfP} [l(z;\theta)].
\end{equation}
Intuitively, we aim to find the best policy under the worst-case data distribution. 

When we have a Wasserstein uncertainty set $\cP_{\sfW_p}$, i.e., \cref{eq:generic-uncertainty-set} equipped with the $p$-th order Wasserstein distance, we define the Wasserstein DPO (WDPO) loss as follows
\begin{equation}\label{eq:wdpo-loss}
    \cLW(\theta;\rho) = \sup_{\sfP\in\cP_{\sfW_p}(\rho;\sfP^o)}\EE_{z\sim\sfP}[l(\theta; z)],
\end{equation}
Similarly, given a Kullback-Leibler uncertainty set $\cP_{\mathrm{KL}}(\rho;\sfP^o)$, we define the KLDPO loss function as follows
\begin{equation}\label{eq:kldpo-loss}
    \cLKL(\theta;\rho) = \sup_{\sfP\in\cP_{\mathrm{KL}}(\rho;\sfP^o)}\EE_{z\sim\sfP}[l(\theta; z)].
\end{equation}
When the nominal distribution $\sfP^o$ is replaced with its empirical counterpart, i.e., $\sfP^o_n\coloneqq(1/n)\sum_{i=1}^n\delta_{z_i}$, where $z_1,\dots,z_n$ are $n$ i.i.d. samples from $\sfP^o$, we use $\cLW_n(\theta;\rho)$ and $\cLKL_n(\theta;\rho)$ to denote the empirical WDPO and KLDPO losses incurred by the policy parameter $\theta$, respectively.


