\section{Tractable (Approximate) Algorithms}
While our distributionally robust DPO formulations enjoy finite-sample guarantees,  it is computationally challenging to solve the min-max objective of \cref{eq:generic-drdro-objective} using stochastic gradient descent methods. Though many min-max optimization problems can be solved by alternating gradient descent methods, our problem is not directly amenable to such an approach as we do not have direct control over the data distribution $\sfP\in\cP(\rho;\sfP^o)$ and they are not parameterized. Moreover, the preference data is generated according to the nominal distribution $\sfP^o$ and we do not have data samples from any other distributions in the uncertainty set $\cP(\rho;\sfP^o)$.  To overcome this challenge, we introduce principled tractable algorithms to solve WDPO and KLDPO.

\subsection{Tractable WDPO}
The connection between Wasserstein distributionally robust optimization (DRO) and regularization has been established in various settings by many \citep{esfahani2015data,shafieezadeh2019regularization,chen2018robust}. We leverage the recent progress in Wasserstein theory on connecting Wasserstein distributionally robust optimization to regularization. For $p$-Wasserstein DRO, $p\in(1,\infty]$, \citet{gao2022wasserstein} shows that for a broad class of loss functions, possibly non-convex and non-smooth, with high probability, the Wasserstein DRO is asymptotically equivalent to variation regularization. In particular, an immediate consequence of \citet[Theorem 1]{gao2022wasserstein} is that, when $p=2$,
\begin{align*}
    \min_{\theta\in\Theta}\sup_{\sfP\colon\sfW_p(\sfP,\sfP^o_n)\leq \rho_n}\EE_{z\sim\sfP}[l(z;\theta)] = \min_{\theta\in\Theta}\big\{ \EE_{z\sim\sfP^o_n}[l(z;\theta)] + \rho_n \sqrt{(1/n)
  \textstyle\sum\nolimits_{i=1}^n\normns{\grad_z(l(z_i;\theta)}_2^2} \big\}+ O_p(1/n),
\end{align*}
where $\rho_n=O(1/\sqrt{n})$. That is, one can solve the Wasserstein DRO objective by adding a gradient regularization to the empirical risk minimization (ERM) loss, $\EE_{z\sim\sfP^o_n}[l(z;\theta)]$. Based on this, we propose a tractable WDPO algorithm in \cref{algo:WDPO-with-gradient-regularizer}.

\begin{algorithm}[ht]
	\caption{WDPO Algorithm}	
	\begin{algorithmic}[1]\label{algo:WDPO-with-gradient-regularizer}
	\STATE \textbf{Input:} Dataset $\cD=\{(s_i,a^w_i,a^l_i)\}_{i=1}^n$, Reference policy $\piref$, Robustness hyperparameter $\rho_0$
		\STATE \textbf{Initialize:} Policy $\pi_\theta$
		\WHILE{$\theta$ has not converged}
		\STATE Calculate the non-robust DPO loss $\cLDPO(\pi_\theta;\cD)$ according to \cref{eq:dpo-loss-dataset}.
  
  \STATE Calculate the gradient regularizer loss
  \begin{align*}
      \cR(\pi_\theta;\cD) = \sqrt{\rho_0/n} (\EE_{z\sim\cD}\normns{\grad_z l(z; \theta)}_2^2 )^{1/2}. 
  \end{align*}
  \STATE Calculate the approximate WDPO loss 
  \begin{align*}
      \cLW(\theta,\rho_0) = \cLDPO(\pi_\theta;\cD)+\cR(\pi_\theta;\cD).
  \end{align*}
  \STATE $\theta \leftarrow \mathrm{Adam}(\grad_\theta [\cLW(\theta,\rho_0)], \theta,\alpha,\beta_1,\beta_2)$
		\ENDWHILE
		\STATE \textbf{Output:} $\pi_\theta$
	\end{algorithmic}
\end{algorithm}

\subsection{Tractable KLDPO: Approximate Dual Solution}
The following proposition shows that we can approximate the worst-case probability distribution in a KL uncertainty set w.r.t. a given loss function.  Similar results can also be found in distributionally robust reinforcement learning literature (e.g., \citet{gadot2024bring}).

\begin{proposition}[Worst-case distribution (informal)]\label{prop:KL-dual-worst-case-informal}
    Let  $\sfPunderline\in\RR^n$ be the worst-case distribution  w.r.t. a loss function  $l$ and KL uncertainty around the empirical distribution $\sfP_n^o$, defined as $\sfPunderline = \sup_{\sfP\colon\KLdiverg{\sfP}{\sfP_n^o}\leq \rho} \EE_{z\sim\sfP} [l(z;\theta)]$. Then, 
    \begin{equation}
        \sfPunderline(i) \propto \sfP_n^o(i) \cdot \exp{(\omega - l(z_i;\theta))/\tau},
    \end{equation}
    where $\omega\leq \sum_{i=1}^n \sfP_n^o(i) l(z_i;\theta)$, and $\tau>0$ is some constant.
\end{proposition}
We defer the formal proof of \cref{prop:KL-dual-worst-case-informal} to \cref{sec:proof-tractable-kldpo}. Here, $\omega$ and $\alpha$ do not have closed forms. $\omega$ can be proven to be upper bounded by the empirical DPO loss. It can be thus viewed as a re-weighting threshold: extreme losses are more biased towards the baseline empirical DPO loss. $\alpha$ works as a temperature parameter to control how much we want the re-weighting. Based on \cref{prop:KL-dual-worst-case-informal}, we propose a tractable KLDPO algorithm in \cref{algo:kldpo-dual-approximation}. 
\begin{algorithm}[ht]
	\caption{KLDPO Algorithm}	
	\begin{algorithmic}[1]\label{algo:kldpo-dual-approximation}
	\STATE \textbf{Input:} Dataset $\cD=\{(s_i,a^w_i,a^l_i)\}_{i=1}^n$, Reference policy $\piref$, Robustness temperature parameter $\tau$
		\STATE \textbf{Initialize:} Policy $\pi_\theta$
		\WHILE{$\theta$ has not converged}
        \STATE Approximate the worst-case empirical distribution as
    \begin{equation*}
        \sfPunderline(i) \propto \exp{(1/\tau)(-l(z_i;\theta) + (1/n)\textstyle\sum\nolimits_{i=1}^n l(z_i;\theta)  )}.
    \end{equation*}
        \STATE Calculate the approximate KLDPO loss as
        \begin{equation*}
            \cLKL(\theta;\cD) = \textstyle\sum\nolimits_{i=1}^n \sfPunderline(i)\cdot l(z_i;\theta).
        \end{equation*}
  \STATE $\theta \leftarrow \mathrm{Adam}(\grad_\theta [\cLKL(\theta;\cD)], \theta,\alpha,\beta_1,\beta_2)$
		\ENDWHILE
		\STATE \textbf{Output:} $\pi_\theta$
	\end{algorithmic}
\end{algorithm}

