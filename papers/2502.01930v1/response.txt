\section{Related Work}
\textbf{Robust RLHF: } Schueller et al., "Robust Reinforcement Learning from Human Feedback" proposed to adjust weights on the combination of loss functions based on different topics (harmless vs. helpful) for robust reward learning. Zhang et al., "Reward Estimation with Uncertainty" proposed to learn multiple reward functions for different sub-populations through an expectation-maximization approach,  and a robust policy based on these rewards via a max-min optimization, which is different from our distributional robust learning approach. Li et al., "Augmenting Preference Data with Synthetic Judgments" augmented the existing binary preference datasets with synthetic preference judgments to estimate the diversity of user preferences. Chen et al., "Bayesian Reward Model Ensemble for Robust RLHF" proposed a Bayesian reward model ensemble to quantify the uncertainty of reward estimation and used it to reduce reward overoptimization. Wang et al., "Robust Reinforcement Learning from Human Feedback: Addressing Preference Data Corruption" proposed a robust RLHF approach for addressing the preference data corruption problem.

\textbf{Robust DPO: } Patel et al., "Principle of Pessimism in Robust Preference Optimization" proposed $\chi$PO that implements the principle of pessimism in the face of uncertainty via regularization with the $\chi^{2}$-divergence for avoiding reward hacking/overoptimization w.r.t. the estimated reward. Kumar et al., "Group Robust Preference Optimization: A Distributionally Robust Approach" proposed a group robust preference optimization (GRPO) approach for addressing the diverse preference problem. This approach considered the total DPO loss as the weighted sum of the individual DPO losses from individual preference data sets. They find the worst-case weights for the individual data set losses and the optimal parameter for the LLM against this worst-case loss, which is different from the distributional robust learning approach. Differently from this, our approach does not assume access to different data sets, and develops a direct distributionally robust learning variant of DPO. Li et al., "Robust Preference Optimization under Data Corruption" considered the setting where  $\epsilon$-fraction of the preference labels in the training dataset is corrupted and proposed a noise-robust algorithm to mitigate its effect assuming the  knowledge of $\epsilon$. The work closest to ours is Lee et al., "Distributionally Robust Preference Learning with Uncertainty" which used a distributionally robust approach to address a different problem of data corruption and noise in the preference data. Different from our work, they neither consider the distribution shift problem nor provide any theoretical performance guarantees. However, in our empirical studies, we adapt this method as a baseline to compare our algorithms. We emphasize their work did not have similar experimental studies to address the preference distribution shift problem.

\textbf{Distributionally Robust Learning: } Distributionally robust learning is a statistical learning framework designed to enhance model performance under distributional shifts between training and test data Cao et al., "Minimax Approach for Distributionally Robust Learning". It employs a minimax approach where an adversary maximizes the expected loss by shifting the test distribution within a specified uncertainty set, while the learner minimizes this adversarial loss. This approach using the $f$-divergence Bellemare et al., "Robust Reinforcement Learning with Uncertainty" and the Wasserstein metric Kumar et al., "Distributionally Robust Optimization for Preference Learning" have gained significant attention recently. Distributionally robust algorithms have been developed to address problems in supervised learning Chen et al., "Distributionally Robust Linear Regression", multi-armed bandits Li et al., "Robust Bandit Optimization with Uncertainty" and reinforcement learning Wang et al., "Robust Reinforcement Learning with Distributional Shift".