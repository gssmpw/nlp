\section{Related Work}
\textbf{Robust RLHF: } \citet{bai2022training} proposed to adjust weights on the combination of loss functions based on different topics (harmless vs. helpful) for robust reward learning. \citet{chakraborty2024maxmin} proposed to learn multiple reward functions for different sub-populations through an expectation-maximization approach,  and a robust policy based on these rewards via a max-min optimization, which is different from our distributional robust learning approach. \citet{padmakumar2024beyond} augmented the existing binary preference datasets with synthetic preference judgments to estimate the diversity of user preferences. \citet{yan2024reward} proposed a Bayesian reward model ensemble to quantify the uncertainty of reward estimation and used it to reduce reward overoptimization. \citet{bukharin2024robust} proposed a robust RLHF approach for addressing the preference data corruption problem. 

\textbf{Robust DPO: } \citet{huang2024correcting} proposed $\chi$PO that implements the principle of pessimism in the face of uncertainty via regularization with the $\chi^{2}$-divergence for avoiding reward hacking/overoptimization w.r.t. the estimated reward. \citet{ramesh2024group} proposed a group robust preference optimization (GRPO) approach for addressing the diverse preference problem. This approach considered the total DPO loss as the weighted sum of the individual DPO losses from individual preference data sets. They find the worst-case weights for the individual data set losses and the optimal parameter for the LLM against this worst-case loss, which is different from the distributional robust learning approach. Differently from this, our approach does not assume access to different data sets, and develops a direct distributionally robust learning variant of DPO. \citet{chowdhury2024provably} considered the setting where  $\epsilon$-fraction of the preference labels in the training dataset is corrupted and proposed a noise-robust algorithm to mitigate its effect assuming the  knowledge of $\epsilon$. The work closest to ours is \citet{wu2024towards} which used a distributionally robust approach to address a different problem of data corruption and noise in the preference data. Different from our work, they neither consider the distribution shift problem nor provide any theoretical performance guarantees. However, in our empirical studies, we adapt this method as a baseline to compare our algorithms. We emphasize their work did not have similar experimental studies to address the preference distribution shift problem. 

\textbf{Distributionally Robust Learning: } Distributionally robust learning is a statistical learning framework designed to enhance model performance under distributional shifts between training and test data \cite{chen2018robust}. It employs a minimax approach where an adversary maximizes the expected loss by shifting the test distribution within a specified uncertainty set, while the learner minimizes this adversarial loss. This approach using the $f$-divergence \citep{namkoong2016stochastic,duchi2018learning, levy2020large} and the Wasserstein metric \citep{esfahani2015data,kuhn2019wasserstein,gao2022wasserstein} have gained significant attention recently. Distributionally robust algorithms have been developed to address problems in supervised learning \cite{chen2018robust, namkoong2016stochastic, levy2020large}, multi-armed bandits \cite{si2020distributionally, yang2023distributionally} and reinforcement learning \cite{panaganti-rfqi, zhou2024natural, shi2024distributionally,yang2022toward}.