\section{Preliminaries}

\textbf{Notations: } We use calligraphic letters for sets, e.g., $\states$ and $\actions$. For any vector $x$, $\normns{\cdot}$ denotes the Euclidean norm. When $\Sigma$ is some positive semi-definite matrix, we write $\normns{x}_{\Sigma}=\sqrt{x^{\top}\Sigma x}$ as a semi-norm of $x$. For any measure $\sfP$, we use $\sfP_n$ to denote the empirical distribution constructed using $n$ i.i.d. samples, $x_1,\dots,x_n$, from $\sfP$, i.e., $\sfP_n = (1/n)\sum_{i=1}^n\delta_{x_i}$, where $\delta_x$ is the Dirac measure. We use $\sigma$ to denote the sigmoid (standard logistic) function, i.e., $\sigma(x)=\frac{1}{1+e^{-x}}$. We use $l(z;\theta)$ and $l_z(\theta)$ to denote the loss incurred by sample $z$ with policy parameter $\theta$. For any set $\cZ$, $\cP(\cZ)$ is the set of all Borel measures over $\cZ$. For any positive semi-definite matrix $\Sigma$, $\lambdamin(\Sigma)$ and $\lambdamax(\Sigma)$ denote its smallest and largest eigenvalues, respectively. 

\textbf{Wasserstein Distance: } For a given set $\cZ$, the Wasserstein distance of order $p$ between two distributions $\mu,\nu\in\cP(\cZ)$ is defined as (see \citet{villani2009optimal}):
    \begin{align}\label{eq:wasserstein-distance-equation}
        \sfW_p(\mu,\nu) = \min_{\gamma\in\cP(\cZ\times\cZ)} \left\{ \int_{\cZ\times\cZ} d^p(x, x')\gamma(dx,dx') \colon \text{$\gamma$ has marginal distributions $\mu,\nu$}\right\},
    \end{align}
where $d$ is some metric defined on $\cZ$. 

\textbf{Kullback-Leibler Divergence: } For any two probability distributions $\sfP$ and $\sfQ$ defined on $\cZ$, the Kullback-Leibker (KL) divergence is defined as
\begin{align}
    \KLdiverg{\sfP}{\sfQ} = \sum_{z\in\cZ} \sfP(z) \log (\sfP(z)/\sfQ(z)).
\end{align}


\textbf{RLHF: }\label{sec:prelim-rlhf} The RLHF paradigm   consists of three steps:

\textit{Step 1: Supervised Fine-tuning (SFT).} SFT involves fine-tuning a pre-trained LLM through supervised learning on high-quality data,  curated for the downstream tasks.

\textit{Step 2: Reward Modelling.} In the second step, given any context $s \in \states$, two responses $a^1,a^2 \in \actions$ are independently sampled from the behavior policy $\pi^o$, which is usually chosen as the SFT policy $\piSFT$. Then, a (human) labeler provides a preference response between these responses. We assume that the preference responses are generated according to the  Bradley-Terry (BT) model \citep{bradley1952rank}: 
\begin{align}\label{eq:BT-model}
        P^*(a^1\succ a^2 \mid s) = \frac{\exp{r^*(s,a^1)}}{\exp{r^*(s,a^1)} + \exp{r^*(s,a^2)}},
    \end{align}
       where  $a^1\succ a^2$ denotes $a^1$ being preferred over $a^2$, and  $r^*$ is the underlying unknown reward function. We use $a^w,a^l$ to denote the preferred and dis-preferred responses, respectively. We assume access to a static dataset of comparison, $\cD=\{(s_i,a^w_i,a^l_i)\}_{i=1}^n$, where $s_i$'s are sampled from some initial prompt (context) distribution $\mu^o$, $a^1_i,a^2_i$'s are independently sampled from $\piSFT$, and the preferences responses are sampled from the BT model $P^*$. With $\cD$, we can learn a parameterized reward model $r_\phi(s,a)$ by  minimizing the  maximum likelihood estimation (MLE) loss,
\begin{equation*}
    \cLRLHF(r_\phi; \cD) = - \EE_{(s,a^w,a^l)\sim\cD} [\log\sigma(r_\phi(s,a^w)-r_\phi(s,a^l))]
\end{equation*}

\textit{Step 3: RL Fine-Tuning.} In the final step, the optimal policy $\pi^*$ under the reward $r_\phi$ is obtained by solving the KL-regularized reward maximization problem given by
\begin{align}\label{eq:rlhf-objective}
    \max_\pi \EE_{s\sim\mu} \left[ \EE_{a\sim\pi(\cdot\mid s)}[r_\phi(s,a)]   -\beta\KLdiverg{\pi(\cdot\mid s)}{\piref(\cdot\mid s)}\right],
\end{align}
where $\beta$ is a parameter controlling the deviation from the base reference policy $\piref$ (usually, $\piSFT$).

\textbf{Direct Preference Optimization (DPO): } The DPO approach \citep{rafailov2023direct} leverages the fact that the unknown reward function can be expressed in terms of the optimal policy and the reference policy. Formally,  given any reward function $r^*$, the optimal solution  of  \cref{eq:rlhf-objective} takes the form $\pi^*(a\mid s) = \frac{1}{Z^*(s)}  \piref(a\mid s)\exp{r^*(s,a)/\beta}$, where $Z^*(s)$ denotes the partition (normalizing) function. Rearranging the above, we get $ r^*(s,a) = \beta \log \frac{\pi^*(a\mid s)}{\piref(a\mid s)} + \beta\log Z^*(s)$ for all $(s,a)$. Substituting this into \cref{eq:BT-model}, the optimal RLHF policy $\pi^*$  satisfies the preference model:{
\begin{align*}
    P^*(a^1\succ a^2\mid s) = \sigma\bigg(\beta\log\frac{\pi^*(a^1\mid s)}{\piref(a^1\mid s)} - \beta\log\frac{\pi^*(a^2\mid s)}{\piref(a^2\mid s)}\bigg).
\end{align*}
}

Using the preference response dataset $\cD$, we can learn the optimal policy directly by minimizing the  MLE loss for a parameterized policy $\pi_\theta$, 
\begin{align}\label{eq:dpo-loss-dataset}
    \cLDPO(\pi_\theta;\cD)=-\EE_{(s,a^w,a^l)\sim\cD} \bigg[   \log\sigma\bigg( \beta\log\frac{\pi_\theta(a^w\mid s)}{\piref(a^w\mid s)} - \beta \log\frac{\pi_\theta(a^l\mid s)}{\piref(a^l\mid s)}\bigg)\bigg].
\end{align}
\textbf{Distributional Uncertainty Sets:} Given any $\rho>0$ and $\sfP^o\in\cP(\cZ)$, we define the distributional uncertainty set as
\begin{equation}\label{eq:generic-uncertainty-set}
    \cP(\rho;\sfP^o) \coloneqq \{\sfP\in\cP(\cZ)\colon D(\sfP,\sfP^o)\leq \rho\},
\end{equation}
where $D(\cdot,\cdot)$ is some distance metric between two probability measures, for e.g., $\sfW_p$ and $D_{\mathrm{KL}}$. 
