\section{Theoretical Analysis}
In this section, we present the sample complexity guarantees for our WDPO and KLDPO algorithms. We make the following assumptions for the rest of the papers. 

\begin{assumption}[Log-linear policy class]\label{assum:log-linear-assumption}
    Let $\psi\colon\states\times\actions\to\RR^d$ be a known $d$-dimensional feature mapping with $\max_{s,a}\normns{\psi(s,a)}_2\leq 1$. Assume a bounded policy parameter set $\Theta\coloneqq\{\theta\in\RR^d\colon \normns{\theta}_2\leq B\}$. We consider the following class of log-linear policies:
     \begin{align}\label{eq:log-linear-policy-class}
        \Pi = \bigg\{\pi_\theta\colon \pi_\theta(a\mid s)&=\frac{\exp{\theta^{\top}\psi(s,a)}}{\sum_{a'\in\actions}\exp{\theta^{\top}\psi(s,a')}} \bigg\}.
    \end{align}

\end{assumption}
\begin{remark}
    This is a standard assumption in the  theoretical analysis of the RL algorithms \citep{agarwal2021theory,modi2020sample}, RLHF \citep{zhu2023principled}, and DPO  \citep{nika2024reward,chowdhury2024provably}. Our analysis can be extended to the neural policy class where $\theta^{\top}\psi(s,a)$ is replaced $f_\theta(s,a)$, where $f_\theta$ is a neural network with twice differentiability and smoothness assumptions.

\end{remark}
We also make the following data coverage assumption on the uncertainty set $\cP(\rho;\sfP^o)$.
\begin{assumption}[Regularity condition]\label{assum:uniform-data-cov-assumption}
   There exists $\lambda>0$ such that
    \begin{align*}
        \Sigma_{\sfP}\coloneqq\EE_{(s,a^1,a^2,y)\sim\sfP}[(\psi(s,a^1)-\psi(s,a^2))(\psi(s,a^1)-\psi(s,a^2))^{\top}] \succeq \lambda I, \quad\forall \sfP\in\cP(\rho;\sfP^o).
    \end{align*}
\end{assumption}
\begin{remark}
    We note that similar assumptions on data coverage under linear architecture models are standard in the offline RL literature \citep{agarwal2019reinforcement,wang2021what,jin2021pessimism}.
    Implicitly, \cref{assum:uniform-data-cov-assumption} imposes $\lambda\leq \lambdamin(\Sigma_{\sfP^o})$, which means that the data-generating distribution $\sfP^o$ has good coverage. 

\end{remark}


\subsection{Estimation Error for WDPO}
Let $\theta^*\in\argmin_{\theta\in\Theta}\cLDPO(\theta)$ be the ground-truth optimal policy parameter with respect to the true nominal distribution and let its empirical counterpart be $\theta_n\in\argmin_{\theta\in\Theta}\cLDPO_n(\theta)$. Now for the robust policy parameters, we let $\thetaW\in\argmin_{\theta\in\Theta}\cLW(\theta;\rho)$, and let its empirical counterpart be $\thetaW_n\in\argmin_{\theta\in\Theta}\cLW_n(\theta;\rho)$. 

We first establish the strong convexity of $\cLW$. 
\begin{lemma}\label{lem:sup-dpo-strongly-convex}
    Let $l(z;\theta)$ be the DPO loss function given in  \cref{eq:pointwise-dpo-loss}. The Wasserstein DPO loss function,
    \begin{equation*}
     \cLW(\theta;\rho) = \sup_{\sfP\colon\sfW_p(\sfP,\sfP^o)\leq\rho} \EE_{z\sim\sfP}[l(z;\theta)],
    \end{equation*}
    is $\gamma\lambda$-strongly convex in $\theta$ with respect to $\normns{\cdot}_2$, where $\lambda$ is the regularity condition number defined in \cref{assum:uniform-data-cov-assumption}, and $\gamma=\frac{\beta^2e^{4\beta B}}{(1+e^{4\beta B})^2}$. 
\end{lemma}

Now, present our main result on the sample complexity result for the convergence of the robust policy parameter. 

\begin{theorem}[Estimation error of $\thetaW_n$]\label{thm:wdpo-policy-parameter-convergence}
    Let $\delta\in(0,1)$. With probability at least $1-\delta$, we have
    \begin{equation*}
        \normns{\thetaW_n-\thetaW}^2_2 \leq \sqrt{\frac{8K^2\log(2/\delta)}{\gamma^2\lambda^2 n}},
    \end{equation*}
    where $\gamma=\frac{\beta^2e^{4\beta B}}{(1+e^{4\beta B})^2}$ and $K=\absns{\log\sigma(-4\beta B)}$, $\lambda$ is the regularity condition number defined in \cref{assum:uniform-data-cov-assumption}.
\end{theorem}
\begin{proof}[Proof sketch]
    Strong duality of Wasserstein DRO (see \citet{gao-2022-distributionally} and \cref{cor:strong-duality-holds-for-dpo-loss}) helps us  reduce the difference $\abs{\cLW(\theta;\rho)-\cLW_n(\theta;\rho)}$ to the concentration $\absns{\EE_{z\sim \sfP^o}[l_\eta(z;\theta)] - \EE_{z\sim \sfP^o_n}[l_\eta(z;\theta)]}$, where $l_\eta(z;\theta)=\inf_{z\in\cZ} [\eta d^p(z,z') - l(z;\theta)]$ is called the \textit{Moreau-Yosida regularization} of $-l$ with parameter $1/\eta$. We show that, for all $\eta\geq 0$, all $l_\eta$ are uniformly bounded. We then use Hoeffding's inequality to obtain concentration. Note that this concentration is true for any policy parameter $\theta\in\Theta$. We organize this concentration result on WDPO loss function to \cref{lem:convergence-of-wdpo-loss}. Detailed proof is in \cref{sec:proof-of-loss-function-convergence}.

    Next, when \cref{assum:uniform-data-cov-assumption} is in place, we can show that $g(\theta)\coloneqq \EE_{z\sim\sfP}[l(z;\theta)]$ is $\gamma$-strongly convex w.r.t. the positive definite norm $\normns{\cdot}_{\Sigma_{\sfP}}$. Further, by the property of supremum, we can show that $\cLW$ is $\gamma\lambda$-strongly convex but w.r.t. $\normns{\cdot}_{2}$. A detailed proof is provided in \cref{sec:proof-of-sup-dpo-strongly-convex}.
    
    Decompose $\cLW(\thetaW_n)-\cLW(\thetaW)$ into three terms: $\cLW(\thetaW_n;\rho)-\cLW_n(\thetaW_n;\rho)$, $\cLW_n(\thetaW_n;\rho)-\cLW_n(\thetaW;\rho)$, and $\cLW_n(\thetaW;\rho)-\cLW(\thetaW;\rho)$. The second term is non-positive since $\thetaW_n$ is the minimizer of $\cLW_n$. Now we apply the concentration of the WDPO loss function (see \cref{lem:convergence-of-wdpo-loss} in \cref{sec:proof-of-loss-function-convergence}) to $\absns{\cLW(\thetaW_n;\rho)-\cLW_n(\thetaW_n;\rho)}$ and $\absns{\cLW_n(\thetaW;\rho)-\cLW(\thetaW;\rho)}$. Finally, we use the property of strongly convex function (\cref{lem:strongly-convex-uniqueness-of-minimizer}) on $\cLW$ to acquire the policy parameter convergence. The detailed proof is in \cref{sec:proof-of-robust-policy-param-converg}. 
\end{proof}

We state the policy parameter convergence of non-robust DPO below in order to compare it with that of robust DPO. 
\begin{proposition}[Estimation error of (non-robust) DPO]\label{prop:dpo-policy-convergence}
    Let $\delta\in(0,1)$ and $\beta>0$. With probability at least $1-\delta$, 
    \begin{equation*}
        \normns{\theta_n-\theta^*}_{\Sigma_{\cD}+\lambda I} \leq 2\sqrt{\frac{4\beta^2}{\gamma^2 n}(d+\log(1/\delta)) + 2\lambda B^2},
    \end{equation*}
    where $\gamma=\frac{\beta^2e^{4\beta B}}{(1+e^{4\beta B})^2}$, and $\Sigma_{\cD} = \frac{1}{n}\sum_{i=1}^n(\psi(s_i,a^1_i)-\psi(s_i,a_i^2))(\psi(s_i,a^1_i)-\psi(s_i,a_i^2))^{\top}$ is the sample covariance matrix.
\end{proposition}
A result of the same order can be inferred from the data-corruption robust DPO work, \citet[Theorem 4.2]{chowdhury2024provably}, as a special case. We provide an independent proof in \cref{sec:proof-of-dpo-policy-convergence} with precise constants. 

\begin{remark}
    We would like to note that the estimation error rate of convergence for WDPO is $\normns{\thetaW_n-\thetaW}_2=O(n^{-1/4})$, from \cref{thm:wdpo-policy-parameter-convergence}. The estimation error rate of convergence for (non-robust) DPO is $\normns{\theta_n-\theta^*}_{\Sigma_{\cD}+\lambda I}=O(n^{-1/2})$, from \cref{prop:dpo-policy-convergence}. So, the estimation error rate of convergence for WDPO is worse than that of  (non-robust) DPO. This arises due to significant challenges exclusive to the robust setting. For example, for the non-robust DPO, we can calculate the closed-form expression of $\grad_\theta (1/n)\sum_{i=1}^n l(z_i;\theta)$ (see \cref{eq:non-robust-dpo-grad-sample-loss}). This allows us to write $\normns{\grad_\theta (1/n)\sum_{i=1}^n l(z_i;\theta^*)}_{(\Sigma_{\cD}+\lambda I)^{-1}}$ in quadratic form and then obtain a concentration using Bernstein's inequality. However, for WDPO, we note that $\grad_\theta \cLW_n(\thetaW)\neq \sup_{\sfP\in\cP_{\sfW_p}}\grad_\theta \EE_{z\sim\sfP}[l(z;\thetaW)]$, and the non-robust approach will not work for the robust setting. Developing analysis techniques to achieve a better rate of convergence for robust DPO is an open question. 
\end{remark}


\subsection{Estimation Error for KLDPO}
Let $\thetaKL\in\argmin_{\theta\in\Theta}\cLKL(\theta;\rho)$, and let its empirical counterpart be $\thetaKL_n\in\argmin_{\theta\in\Theta}\cLKL_n(\theta;\rho)$. The proofs for the convergence of KLDPO loss function and policy parameter closely mirror those for the Wasserstein DPO. We defer the detailed proofs of KLDPO to \cref{sec:kldpo-proof-appendix} and only state the theorems in this section.

\begin{theorem}[Estimation error of $\thetaKL_n$]
    Let $\delta\in(0,1)$. With probability at least $1-\delta$, we have
    \begin{equation*}
        \normns{\thetaKL_n-\thetaKL}^2_2 \leq \sqrt{\frac{8\lambdaoverline^2\exp{L/\lambdaunderline}\log(2/\delta)}{\gamma^2\lambda^2n}},
    \end{equation*}
    where  $\gamma=\frac{\beta^2e^{4\beta B}}{(1+e^{4\beta B})^2}$. $\lambda$ is the regularity condition number defined in \cref{assum:uniform-data-cov-assumption}, $0<\lambda\leq \lambdamin(\Sigma_{\sfP^o})$. $\lambdaunderline,\lambdaoverline$ are some universal constants, and $L$ is an upper bound on the loss function $l$.
\end{theorem}

\begin{remark}
    The exponential constant in the upper bound is a characteristic of distributional robust optimization with KL  uncertainty set \citet[Proposition 2]{hu2013kullback}. Similar exponential constants appear in the theoretical analysis of the distributionally robust RL \citep{zhou2021finite,yang2022toward,panaganti22a,xu-panaganti-2023samplecomplexity}. Both WDPO and KLDPO have $O(n^{-1/4})$ policy parameter convergence. An empirical comparison is given in \cref{sec:experiments}.
\end{remark}