\section{Conclusions}
In this paper, we proposed the formalism of distributionally robust DPO,  developed two novel algorithms using this framework,  and established their theoretical guarantees. We also developed efficient approximation techniques that enable scalable implementation of these algorithms as part of the existing LLM alignment pipeline. We showed extensive empirical evaluations that validate the effectiveness of our proposed algorithms in addressing preference distribution shifts in LLM alignment. In future works, we plan to extend our distributionally robust DPO algorithms to address the challenges of reward hacking. We also plan to develop distributionally robust algorithms for other RLHF approaches. 
