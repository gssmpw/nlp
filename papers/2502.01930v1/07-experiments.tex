\section{Experiments}\label{sec:experiments}
\begin{figure*}[!ht]
    \centering 
    \includegraphics[width=\linewidth]{assets/exp3-1x3-plot.pdf}
    \caption{Evaluation of DPO, WDPO, KLDPO, and Dr. DPO. The training preference labels are generated by $r^*_1(0.1)$.}
    \label{fig:emotion-exp-convex-15kl-lineplot}
\end{figure*}
\begin{figure*}[!ht]
    \centering 
    \includegraphics[width=\linewidth]{assets/exp4-1x3-plot.pdf}
    \caption{Evaluation of DPO, WDPO, KLDPO, and Dr. DPO. The training preference labels are generated by $r^*_2(0.1)$.}
    \label{fig:emotion-exp-power-15kl-lineplot}
\end{figure*}
We use the Emotion dataset \citep{saravia-etal-2018-carer} which consists of English Twitter texts. Each text is categorized into six emotions: \textit{sadness, joy, love, anger, fear, surprise}. To ensure data quality, we excluded \textit{surprise} due to its under-representation in the dataset. We first train reward models which can accurately quantify emotions in each text. We achieve this by performing multi-label classification and adapting a small LLM with an appended classification head. We fine-tune this LLM using binary cross-entropy loss and apply sigmoid activation which allows the model to assign probabilities for multiple emotions. We denote $r_1,r_2,...,r_5$ as the trained reward functions that correspond to \textit{anger, fear, sadness, joy, love}, respectively. For the experiments in this section, we use GPT-2 \citep{radford2019language} as the base model. More details about this experiment can be found in \cref{sec:emotion-alignment-appendix}.

\textbf{Binary Emotion Alignment: } In this section, we consider a simpler setting with only \textbf{two} emotions: \textit{anger} and \textit{fear}. We consider two \textbf{mixture} reward functions classes: (1) $r^*_1(\alpha)\coloneqq\alpha\cdot r_1 + (1-\alpha)\cdot r_2$, (2) $r^*_2(\alpha)\coloneqq r_1^{\alpha}\cdot r_2^{1-\alpha}$. For both (1) and (2), we generate preference labels according to the BT model (\cref{eq:BT-model}) parameterized by $r^*_i(\alpha^o)$, where $\alpha^o\in\{0.1,0.3,0.5,0.7,0.9\}$ for both $i=1,2$. For evaluation, we use $\alpha\in\{0,0.1,0.3,0.5,0.7,0.9,1\}$, where $\alpha=0,1$ represent the cases of trained models being evaluated on \textbf{single} \textit{fear} and \textit{anger} reward functions. 


In \cref{fig:emotion-exp-convex-15kl-lineplot}, $r^*_1(0.1)$ is used to generate the training preference labels. As expected, DPO is able to outperform WDPO in the nominal data setting, since DPO is the optimal policy when there is no distribution shift between training and testing. However, when the trained models are evaluated on other mixture reward functions, e.g., $r^*_1(0.5)$ and $r^*_1(0.7)$, KLDPO is able to outperform DPO and maintain performance. WDPO notably has more robustness than DPO when the evaluation starts to favor $r_1$ (\textit{anger}), i.e., when $\alpha=1.0$. In the two plots on the right, we show that we are able to tune the robustness of WDPO and KLDPO. Ideally, we want our curve to have a flatter slope which implies that the model is able to perform similarly well on all preference combinations. WDPO with hyperparameter $\rho_0=100$ and KLDPO with parameter $\tau=1$ achieve the highest robustness, respectively. 

In \cref{fig:emotion-exp-power-15kl-lineplot}, $r^*_2(0.1)$ is used to generate the training preference labels. Again, DPO is able to outperform everyone in the nominal data setting. However, its performance precipitates when $\alpha$ increases. In contrast, our KLDPO has very moderate drop in performance when the evaluation changes from $r^*_2(0.1)$ to $r^*_2(1.0)$. In \cref{sec:emotion-alignment-appendix}, we show that $r^*_2(0.1)$ is correlated to $r_2$ (\textit{fear}) with a correlation factor of $0.97$. In other words, the training data has little information about $r_1$ (\textit{anger}). KLDPO shows superior robustness for being able to algorithmically anticipate $r_1$ and perform much better on $r^*_2(1.0)=r_1$. Similarly, our WDPO also shows robustness as it maintains performance for a wide range of $\alpha$.

\textbf{Multi-class Emotion Alignment: } Here, we consider the multi-class mixture reward function: $r^*_4 \coloneqq \frac{1}{5} \cdot r_1 + \frac{1}{5}\cdot r_2 + \frac{1}{5}\cdot r_3 + \frac{1}{5} \cdot r_4 + \frac{1}{5} \cdot r_5$. We generate preference labels according to the BT model (\cref{eq:BT-model}) parameterized by $r_4^*$. 

\begin{wrapfigure}{l}{0.5\textwidth}
    \centering
    \includegraphics[width=\linewidth]{assets/emotion-multiclass-exp14-no-love.pdf}
    \caption{Evaluation of DPO, WDPO, KLDPO, and Dr. DPO. The training preference labels are generated by $r^*_4$.}
    \label{fig:emotion-multiclass}
\end{wrapfigure}


In \cref{fig:emotion-multiclass}, DPO and WDPO are evaluated on standalone reward functions, i.e., $r_1,\dots, r_4$. In other words, the preference model shifts from $r^*_4$ to each of $r_1,\dots, r_4$. We note that our KLDPO has superior robustness against preference shift in that for all four shift scenarios, it is able to outperform DPO. In particular, when DPO is evaluated on $r_2$ (standalone \textit{fear}), it only achieves a reward of $0.4$. Our KLDPO is able to achieve a score of $0.6$. We also note that WDPO is able to outperform DPO when $r^*_4$ shifts to $r_4$ (standalone \textit{joy}) and to $r_2$ (standalone \textit{sadness}).


