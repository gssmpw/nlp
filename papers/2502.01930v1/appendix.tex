\newpage
\appendix
\section{Useful Technical Results}
\subsection{Wasserstein Theory}\label{sec:wasserstein-theory}
We rely on the following strong duality result from the Wasserstein distributionally robust optimization (WDRO) literature.
\begin{lemma}[\text{\citealp[Theorem 1]{gao-2022-distributionally}}; Strong Duality for DRO with Wasserstein Distance]\label{thm:wasser-duality}
    Consider any $p\in[1,\infty)$, any $\nu\in\cP(\Xi)$, any $\rho > 0$, and any $\Psi \in L^1(\nu)$ such that the growth rate $\kappa$ of $\Psi$ satisfies
    \begin{equation}\label{eq:growth-rate-of-obj}
        \kappa \coloneqq \inf\bigg\{ \eta \geq 0 \colon \int_\Xi \Phi(\eta, \zeta) \nu(d\zeta) > -\infty \bigg\} < \infty,
    \end{equation}
    where $\Phi(\eta,\zeta)\coloneqq \inf_{\xi\in\Xi}\{\eta d^p(\xi,\zeta)-\Psi(\xi)\}$ is a regularization operator. Then the strong duality holds with \textbf{finite optimal value} $v_p = v_D \leq \infty$, where
    \begin{align*}
        v_p &\coloneqq \sup_{\mu\in\cP(\Xi)} \bigg\{ \int_\Xi \Psi(\xi) \mu(d\xi) \colon \sfW_p(\mu,\nu)\leq \rho    \bigg\}, &&\qquad\qquad \text{(Primal)} \\
        v_D &\coloneqq \inf_{\eta \geq 0} \bigg\{  \eta \rho^p - \int_\Xi \inf_{\xi\in\Xi} [\eta d^p(\xi,\zeta) - \Psi(\xi)] \nu(d\zeta) \bigg\}. &&\qquad\qquad \text{(Dual)}
    \end{align*}
\end{lemma}

\begin{lemma}[\text{\citealp[Lemma 2.(ii)]{gao-2022-distributionally}}; Properties of the growth $\kappa$]\label{lem:growth-rate-sufficient}
    Suppose that $\nu\in\cP_p(\Xi)$. Then the growth rate $\kappa$ (as defined in \cref{eq:growth-rate-of-obj}) is finite if and only if there exists $\zeta^o\in\Xi$ and $L,M>0$ such that
    \begin{equation}\label{eq:growth-rate-bounded-iff-condition}
    \Psi(\xi)-\Psi(\zeta^o) \leq Ld^p(\xi,\zeta^o) + M, \quad\forall \xi\in\Xi.
    \end{equation}
\end{lemma}
\begin{corollary}\label{cor:strong-duality-holds-for-dpo-loss}
    Consider any bounded loss function $l$ over bounded $\Xi$. Then the duality defined in \cref{thm:wasser-duality} holds.
\end{corollary}
\begin{proof}
    It follows from \cref{lem:growth-rate-sufficient}. We can pick $L$ to be the diameter of $\Xi$ and $M$ to be the bound of $\Psi$.
\end{proof}
\subsection{Optimization}
\begin{lemma}[\text{\citealp[Theorem 1.24]{beck2014introduction}}; Linear Approximation Theorem]\label{lem:linear-approximation-theorem}
    Let $f\colon U\to\RR$ be a twice continuously differentiable function over an open set $U\subseteq \RR^n$, and let $x,y\in U$ be such that $[x,y]\subseteq U$. Then there exists $\xi\in[x,y]$ such that
    \begin{equation*}
        f(y) = f(x) + \grad f(x)^{\top}(y-x) + \frac{1}{2}(y-x)^{\top}\grad^2 f(\xi)(y-x).
    \end{equation*}
\end{lemma}
\begin{lemma}[\text{\citealp[Theorem 5.24]{beck2017first}}; First-order characterizations of strong convexity]\label{lem:first-order-character-ization-of-strong-convexity}
    Let $f\colon \EE\to(-\infty,\infty]$ be a proper closed and convex function. Then for a given $\sigma>0$, the following two claims are equivalent:
    \begin{enumerate}[(I)]
        \item For any $x,y\in\dom(f)$ and $\lambda\in[0,1]$:
        \begin{equation*}
            f(\lambda x+(1-\lambda )y) \leq \lambda f(x) + (1-\lambda)f(y) - \frac{\sigma}{2}\lambda(1-\lambda)\normns{x-y}^2.
        \end{equation*}
        \item 
        \begin{equation*}
            f(y)\geq f(x) + \inner{g, y-x} + \frac{\sigma}{2}\normns{y-x}^2,     
        \end{equation*}
        for any $x\in\dom(\partial f)$, $y\in\dom(f)$ and $g\in\partial f(x)$.
    \end{enumerate}
\end{lemma}

\begin{lemma}[\text{\citealp[Theorem 5.25]{beck2017first}}; Existence and uniqueness of a minimizer of closed strongly convex functions]\label{lem:strongly-convex-uniqueness-of-minimizer}
    Let $f\colon \EE\to (-\infty,\infty]$ be a proper closed and $\sigma$-strongly convex function $\sigma>0$. Then
    \begin{enumerate}[(I)]
        \item $f$ has a unique minimizer;
        \item $f(x)-f(x^*)\geq \frac{\sigma}{2}\normns{x-x^*}^2$ for all $x\in\dom(f)$, where $x^*$ is the unique minimizer of $f$.
    \end{enumerate}
\end{lemma}

\subsection{Distributionally Robust Optimization Results}
The Kullback-Liebler uncertainty set can be constructed with the $f$-divergence. The $f$-divergence between the distribution $\sfP$ and $\sfP^o$ is defined as
\begin{equation} \label{eq:f-divergence}
    \fdiverg{\sfP}{\sfP^o} = \int_{\cX} f\bigg(\frac{d\sfP}{d\sfP^o} \bigg)d\sfP^o,
\end{equation}
where $f$ is a convex function. $f(t)=t\log(t)$ gives us the Kullback-Liebler divergence. Let $\sfP^o$ be a distribution on the space $\cX$ and let $l\colon \cX\to\RR$ be a loss function. We have the following result from the distributionally robust optimization literature.
\begin{lemma}[\text{\citealp[Proposition 1]{duchi2018learning}}]\label{lem:dual-reformulation-f-diverg}
    Let $D_f$ be the $f$-divergence defined in \cref{eq:f-divergence}. Then,
    \begin{equation}\label{eq:dual-reformulation-f-diverg}
        \sup_{\sfP\colon\fdiverg{\sfP}{\sfP^o}\leq\rho} E_{\sfP}[l(X)] = \inf_{\lambda\geq0,\eta\in\RR} \EE_{\sfP^o} \bigg[ \lambda f^*\bigg( \frac{l(X)-\eta}{\lambda}\bigg)\bigg] + \lambda\rho+\eta,
    \end{equation}
    where $f^*(s)=\sup_{t\geq0}\{st-f(t)\}$ is the Fenchel conjugate.
\end{lemma}


\subsection{Concentration Results}
\begin{lemma}[Hoeffding's inequality \text{\citep[see][Theorem 2.8]{boucheron2013concentration}}]\label{thm:hoeffding}
 Let $X_1,\dots,X_n$ be independent random variables such that $X_i$ takes its values in $[a_i,b_i]$ almost surely for all $i\leq n$. Let
 \begin{equation*}
     S=\sum_{i=1}^n(X_i-\expect{X_i}).
 \end{equation*}
 Then for every $t>0$,
 \begin{equation*}
     \prob{S\geq t}\leq\exp{-\frac{2t^2}{\sum_{i=1}^n(b_i-a_i)^2}}.
 \end{equation*}
 Furthermore, if $X_1,\dots,X_n$ are a sequence of independent, identically distributed random variables with mean $\mu$. Let $\mean{X}_n = \frac{1}{n}\sum_{i=1}^n X_i$. Suppose that $X_i\in[a,b]$, $\forall i$. Then for all $t>0$
\begin{equation*}
     \prob{\abs{\mean{X}_n - \mu} \geq t} \leq 2\exp{-\frac{2nt^2}{(b-a)^2}}.
\end{equation*}
\end{lemma}

\begin{lemma}[\text{\citealp[Theorem 2.1]{hsu2012tail}}]\label{lem:psd-quadratic-form-concentration}
    Let $A\in\RR^{n\times n}$ be a matrix, and let $\Sigma\coloneqq A^{\top}A$. Suppose that $x=(x_1,\dots,x_n)$ is a random vector such that for some $\mu\in\RR^n$ and $\sigma\geq 0$,
    \begin{equation*}
        \EE[\expns{\alpha^{\top}(x-\mu)}] \leq \expns{\normns{\alpha}^2 \sigma^2/ 2},
    \end{equation*}
    for all $\alpha\in\RR^n$. For all $t>0$,
    \begin{equation*}
        \PP\bigg[\normns{Ax}^2 > \sigma^2 \cdot \bigg(\tr(\Sigma)+2\sqrt{\tr(\Sigma^2)t} + 2\normns{\Sigma}t \bigg) + \tr(\Sigma \mu\mu^{\top})\cdot \bigg( 1+ 2\sqrt{\frac{t\normns{\Sigma}^2}{\tr(\Sigma^2)}} \bigg)\bigg] \leq e^{-t}.
    \end{equation*}
    Moreover, if $\mu=0$ and $\sigma=1$, then the probability inequality reads
    \begin{equation*}
        \PP\bigg(\normns{Ax}^2 > \tr(\Sigma) + 2\sqrt{\tr(\Sigma^2)t} + 2\normns{\Sigma}t \bigg) \leq e^{-t}.
    \end{equation*}
\end{lemma}






\section{Proof of WDPO Sample Complexity}
Many properties of distributionally robust DPO are derived from those of the non-robust DPO. We hence start with the following proof of policy parameter convergence in the non-robust setting (\cref{prop:dpo-policy-convergence}).
\subsection{Proof of Non-robust DPO Policy Parameter Convergence}\label{sec:proof-of-dpo-policy-convergence}
Recall the pointwise DPO loss:
\begin{equation*}
    l(\theta;s,a^1,a^2,y) \coloneqq -y \log\sigma(\beta h_\theta(s,a^1,a^2)) - (1-y) \log\sigma(\beta h_\theta(s,a^2,a^1)),
\end{equation*}
where $h_\theta(s,a^1,a^2)\coloneqq \log\frac{\pi_\theta(a^1\mid s)}{\piref(a^1\mid s)}-\log\frac{\pi_\theta(a^2\mid s)}{\piref(a^2\mid s)}$. Denote this loss by $l_z(\theta)$ where $z=(s,a^1,a^2,y)$. We also denote the empirical (sample) DPO loss as
\begin{equation*}
    l_{\cD}(\theta) =\frac{1}{n}\sum_{i=1}^n l_{z_i}(\theta) = \frac{1}{n}\sum_{i=1}^n -y_i \log\sigma(\beta h_\theta(s_i,a^1_i,a^2_i)) - (1-y_i) \log\sigma(\beta h_\theta(s_i,a^2_i,a^1_i)).
\end{equation*}
We denote the MLE solution to $l_{\cD}$ by $\thetahatDPO\in\argmin_{\theta\in\Theta}l_{\cD}(\theta)$. Also, denote the true parameter which is the global minimum of the population negative log likelihood by $\theta^*$.

\paragraph{(Almost) Strong Convexity of $l$.} In order to calculate the Hessian matrix of $l_z$ w.r.t. $\theta$, we need to calculate $\grad^2_\theta \log\sigma(\beta h_\theta(s,a^1,a^2))$.

Suppose $f\colon\RR\to\RR$, $g\colon\RR^d\to\RR$. The Hessian of $f\circ g$ is, for any $x\in\RR^d$,
\begin{equation}\label{eq:chain-rule-hessian-comp-with-scalar-func}
    \grad_x^2(f\circ g)(x) = f'(g(x))\grad_x^2 g(x) + f^{''}(g(x))\grad_x g(x)\grad_x g(x)^{\top}.
\end{equation}

Recall that $\sigma$ is the sigmoid function. It has the properties: $\sigma(-x)=1-\sigma(x)$ and $\sigma'(x)=\sigma(x)(1-\sigma(x))$. Let $f(x) = \log\sigma(x)$, we have
\begin{align*}
    \frac{d}{dx}f(x) &= \frac{\sigma'(x)}{\sigma(x)} = \frac{\sigma(x)(1-\sigma(x))}{\sigma(x)} = \sigma(-x);\\
    \frac{d^2}{dx^2}f(x) &= \frac{d}{dx}[\sigma(-x)] = \frac{d}{dx}[1-\sigma(x)] = -\sigma'(x) = -\sigma(x)\sigma(-x).
\end{align*}
With $g(\theta)\coloneqq \beta h_\theta(s,a^1,a^2)$ and the Hessian chain rule for composition with a scalar function (\cref{eq:chain-rule-hessian-comp-with-scalar-func}), we have
\begin{align*}
    \grad_\theta^2\log\sigma(\beta h_\theta(s,a^1,a^2))&=\beta\sigma(-\beta h_\theta(s,a^1,a^2))\grad_\theta^2 h_\theta(s,a^1,a^2) \\
    &\quad- \beta^2\sigma(\beta h_\theta(s,a^1,a^2))\sigma(-\beta h_\theta(s,a^1,a^2))\grad_\theta h_\theta(s,a^1,a^2)\grad_\theta h_\theta(s,a^1,a^2)^{\top}.
\end{align*}
In addition, we have the following observations
\begin{align*}
    \grad_\theta h_\theta(s,a^1,a^2) &= \grad_\theta\log\pi_\theta(a^1\mid s) - \grad_\theta\log\pi_\theta(a^2\mid s) = -\grad_\theta h_\theta(s,a^2,a^1);\\
    \grad_\theta^2 h_\theta(s,a^1,a^2)  &= \grad_\theta^2\log\pi_\theta(a^1\mid s) - \grad_\theta^2\log\pi_\theta(a^2\mid s) = -\grad_\theta^2 h_\theta(s,a^2,a^1).
\end{align*}
Now, using the above observations, we can simplify $\grad_\theta^2 l_z(\theta)$ as follows
\begin{align*}
    \grad_\theta^2 l_z(\theta) &= -y\grad_\theta^2 \log\sigma(\beta h_\theta(s,a^1,a^2)) - (1-y)\grad_\theta^2 \log\sigma(\beta h_\theta(s,a^2,a^1)) \\
    &= -y \big[\beta\sigma(-\beta h_\theta(s,a^1,a^2))\grad_\theta^2 h_\theta(s,a^1,a^2) \\
    &\quad- \beta^2\sigma(\beta h_\theta(s,a^1,a^2))\sigma(-\beta h_\theta(s,a^1,a^2))\grad_\theta h_\theta(s,a^1,a^2)\grad_\theta h_\theta(s,a^1,a^2)^{\top} \big] \\
    &\quad -(1-y)\big[\beta\sigma(-\beta h_\theta(s,a^2,a^1))\grad_\theta^2 h_\theta(s,a^2,a^1) \\
    &\quad -\beta^2\sigma(\beta h_\theta(s,a^2,a^1))\sigma(-\beta h_\theta(s,a^2,a^1))\grad_\theta h_\theta(s,a^2,a^1)\grad_\theta h_\theta(s,a^2,a^1)^{\top} \big] \\
    &= -y \beta\sigma(-\beta h_\theta(s,a^1,a^2))\grad_\theta^2 h_\theta(s,a^1,a^2) \\
    &\quad+ y \beta^2\sigma(\beta h_\theta(s,a^1,a^2))\sigma(-\beta h_\theta(s,a^1,a^2))\grad_\theta h_\theta(s,a^1,a^2)\grad_\theta h_\theta(s,a^1,a^2)^{\top}  \\
    &\quad -(1-y)\beta\sigma(-\beta h_\theta(s,a^2,a^1))\grad_\theta^2 h_\theta(s,a^2,a^1) \\
    &\quad +(1-y)\beta^2\sigma(\beta h_\theta(s,a^2,a^1))\sigma(-\beta h_\theta(s,a^2,a^1))\grad_\theta h_\theta(s,a^2,a^1)\grad_\theta h_\theta(s,a^2,a^1)^{\top}  \\
    &\stackeq{(a)} -y \beta\sigma(-\beta h_\theta(s,a^1,a^2))\grad_\theta^2 h_\theta(s,a^1,a^2) \\
    &\quad+ y \beta^2\sigma(\beta h_\theta(s,a^1,a^2))\sigma(-\beta h_\theta(s,a^1,a^2))\grad_\theta h_\theta(s,a^1,a^2)\grad_\theta h_\theta(s,a^1,a^2)^{\top}  \\
    &\quad +(1-y)\beta\sigma(-\beta h_\theta(s,a^2,a^1))\grad_\theta^2 h_\theta(s,a^1,a^2) \\
    &\quad +(1-y)\beta^2\sigma(-\beta h_\theta(s,a^1,a^2))\sigma(\beta h_\theta(s,a^1,a^2))\grad_\theta h_\theta(s,a^1,a^2)\grad_\theta h_\theta(s,a^1,a^2)^{\top}  \\
    &= \beta (-y+\sigma(\beta h_\theta(s,a^1,a^2))) \grad_\theta^2 h_\theta(s,a^1,a^2) \\
    &\quad + \beta^2\sigma(\beta h_\theta(s,a^1,a^2))\sigma(-\beta h_\theta(s,a^1,a^2))\grad_\theta h_\theta(s,a^1,a^2)\grad_\theta h_\theta(s,a^1,a^2)^{\top}.
\end{align*}
where $(a)$ is due to $h_\theta(s,a^2,a^1)=-h_\theta(s,a^1,a^2)$, $\grad_\theta h_\theta(s,a^2,a^1)=-\grad_\theta h_\theta(s,a^1,a^2)$ and $\grad_\theta^2 h_\theta(s,a^2,a^1)=-\grad_\theta^2 h_\theta(s,a^1,a^2)$. It's clear that we have to calculate $\grad_\theta^2 h_\theta(s,a^1,a^2)$ and $\grad_\theta h_\theta(s,a^1,a^2)$. Observe that
\begin{equation}\label{eq:grad-h-func}
    \grad_\theta h_\theta(s,a^1,a^2) = \grad_\theta \log \pi_\theta(a^1\mid s) - \grad_\theta\log \pi_\theta(a^2\mid s) = \frac{1}{\pi_\theta(a^1\mid s)}\grad_\theta\pi_\theta(a^1\mid s) - \frac{1}{\pi_\theta(a^2\mid s)}\grad_\theta\pi_\theta(a^2\mid s).
\end{equation}
In addition, we have that $\grad_\theta^2 h_\theta(s,a^1,a^2)  = \grad_\theta^2\log\pi_\theta(a^1\mid s) - \grad_\theta^2\log\pi_\theta(a^2\mid s)$. Using the Hessian chain rule (\cref{eq:chain-rule-hessian-comp-with-scalar-func}), we have
\begin{equation*}
    \grad_\theta^2 \log\pi_\theta(a\mid s)  = \frac{1}{\pi_\theta(a\mid s)}\grad_\theta^2\pi_\theta(a\mid s) - \frac{1}{\pi_\theta(a\mid s)^2}\grad_\theta\pi_\theta(a\mid s)\grad_\theta\pi_\theta(a\mid s)^{\top}.
\end{equation*}
Now it boils down to tackling $\grad_\theta\pi_\theta(a\mid s)$ and $\grad_\theta^2\pi_\theta(a\mid s)$. Observe that
\begin{align*}
    \grad_\theta\pi_\theta(a\mid s) &= \frac{\grad_\theta \exp{\inner{\psi(s,a),\theta}}[\sum_{a'}\exp{\inner{\psi(s,a'),\theta}}] - [\sum_{a'}\grad_\theta\exp{\inner{\psi(s,a'),\theta}}]\exp{\inner{\psi(s,a),\theta}}}{(\sum_{a'}\exp{\inner{\psi(s,a'),\theta}})^2}\\
    &=\frac{\exp{\inner{\psi(s,a),\theta}}}{\sum_{a'}\exp{\inner{\psi(s,a'),\theta}}}\psi(s,a) - \frac{\exp{\inner{\psi(s,a),\theta}}}{(\sum_{a'}\exp{\inner{\psi(s,a'),\theta}})^2} \sum_{a'} \exp{\inner{\psi(s,a'),\theta}}\psi(s,a')\\
    &=\frac{\exp{\inner{\psi(s,a),\theta}}}{\sum_{a'}\exp{\inner{\psi(s,a'),\theta}}}\psi(s,a) - \frac{\exp{\inner{\psi(s,a),\theta}}}{\sum_{a'}\exp{\inner{\psi(s,a'),\theta}}} \sum_{a'} \frac{\exp{\inner{\psi(s,a),\theta}}}{\sum_{a''}\exp{\inner{\psi(s,a''),\theta}}}\psi(s,a')\\
    &= \pi_\theta(a\mid s)\psi(s,a) - \pi_\theta(a\mid s) \sum_{a'}\pi_\theta(a'\mid s)\psi(s,a') \\
    &= \pi_\theta(a\mid s)\bigg[\psi(s,a)-\sum_{a'}\pi_\theta(a'\mid s)\psi(s,a')\bigg].
\end{align*}
Then we have
\begin{align}\label{eq:grad-h-func-calculated}
    \grad_\theta h_\theta(s,a^1,a^2) &= \frac{1}{\pi_\theta(a^1\mid s)}\pi_\theta(a^1\mid s)\bigg[\psi(s,a^1) -\sum_{a'}\pi_\theta(a'\mid s)\psi(s,a')\bigg] \nonumber\\
    &\quad - \frac{1}{\pi_\theta(a^2\mid s)}\pi_\theta(a^2\mid s)\bigg[\psi(s,a^2)-\sum_{a'}\pi_\theta(a'\mid s)\psi(s,a')\bigg]  \nonumber\\
    &= \psi(s,a^1) - \psi(s,a^2).
\end{align}
Notice that $\grad_\theta h_\theta$ above does not depend on the policy parameter $\theta$. This implies that its Hessian is the zero matrix, i.e., $\grad_\theta^2 h_\theta(s,a^1,a^2) = \bfzero$. Finally, we have that
\begin{equation*}
    \grad_\theta^2 l_z(\theta) =  \beta^2\sigma(\beta h_\theta(s,a^1,a^2))\sigma(-\beta h_\theta(s,a^1,a^2))  (\psi(s,a^1) - \psi(s,a^2))(\psi(s,a^1) - \psi(s,a^2))^{\top}.
\end{equation*}
Moving from the pointwise loss to the empirical loss, we denote
\begin{equation*}
    \grad_\theta^2 l_{\cD}(\theta) =  \frac{1}{n}\sum_{i=1}^n \beta^2\sigma(\beta h_\theta(s_i,a^1_i,a^2_i))\sigma(-\beta h_\theta(s_i,a^1_i,a^2_i))  (\psi(s_i,a^1_i) - \psi(s_i,a^2_i))(\psi(s_i,a^1_i) - \psi(s_i,a^2_i))^{\top}.
\end{equation*}
Now let's focus on the function $\sigma(x)\sigma(-x)$. Our aim is to find a lower bound for this function. Observe that
\begin{align}
    \absns{h_\theta(s,a^1,a^2)} &= \absns{(\log\pi_\theta(a^1\mid s) - \log\pi_\theta(a^2\mid s)) - (\log\piref(a^1\mid s) - \log\piref(a^2\mid s))} \nonumber\\
    &= \absns{\inner{\theta, \psi(s,a^1)-\psi(s,a^2)} - \inner{\thetaref, \psi(s,a^1)-\psi(s,a^2)}} \nonumber\\
    &= \absns{\inner{\theta-\thetaref, \psi(s,a^1)-\psi(s,a^2)}} \nonumber\\
    &\stackleq{(a)} \normns{\theta-\thetaref}_2 \normns{\psi(s,a^1)-\psi(s,a^2)}_2 \nonumber \\
    &\stackleq{(b)} 4B\label{eq:h-function-bounds},
\end{align}
where $(a)$ is due to Cauchy-Schwarz inequality. $(b)$ is due to the assumptions $\normns{\theta}_2\leq B$ and $\max_{s,a}\normns{\psi(s,a)}_2\leq 1$. Now this suggests that the input to the function $\sigma(\beta h_\theta(s,a^1,a^2))\sigma(-\beta h_\theta(s,a^1,a^2))$ is bounded in $[-4\beta B,4\beta B]$. Since $\sigma(x)\sigma(-x)$ is symmetric and strictly decreasing when $x\in[0,\infty)$, we have that
\begin{equation}\label{eq:lower-bound-dpo-loss-coef}
    \beta^2\sigma(\beta h_\theta(s,a^1,a^2))\sigma(-\beta h_\theta(s,a^1,a^2)) \geq \frac{\beta^2 e^{4\beta B}}{(1+e^{4\beta B})^2}, \quad \forall \theta\in\Theta.
\end{equation}
We then have that
\begin{equation*}
    u^{\top} \grad_\theta^2 l_{\cD}(\theta)u \geq \frac{\gamma}{n}\normns{Xu}_2^2, \quad\forall u\in\RR^d,
\end{equation*}
where $\gamma=\frac{\beta^2 e^{4\beta B}}{(1+e^{4\beta B})^2}$ and $X\in\RR^{n\times d}$ has the differencing vector $x_i\coloneqq \psi(s_i,a^1_i)-\psi(s_i,a_i^2)\in\RR^d$ as its $i$-th row. Thus, if we introduce the error vector $\Delta\coloneqq \thetahatDPO-\theta^*$, then by the linear approximation theorem (\cref{lem:linear-approximation-theorem}), there exists $\alpha\in[0,1]$ and $\thetatilde=\alpha\thetahatDPO+(1-\alpha)\theta^*$ such that
 \begin{equation}\label{eq:strong-convexity-of-l}
     l_{\cD}(\theta^*+\Delta) - l_{\cD}(\theta^*) - \inner{\grad_\theta l_{\cD}(\theta^*), \Delta} = \frac{1}{2}\Delta^{\top}\grad_\theta^2 l_{\cD}(\thetatilde) \Delta \geq \frac{\gamma}{2n}\normns{X\Delta}^2_2 = \frac{\gamma}{2}\normns{\Delta}^2_{\Sigma_{\cD}},
 \end{equation}
where $\Sigma_{\cD} = \frac{1}{n}\sum_{i=1}^n(\psi(s_i,a^1_i)-\psi(s_i,a_i^2))(\psi(s_i,a^1_i)-\psi(s_i,a_i^2))^{\top}$. This implies that $l_{\cD}$ is (almost) strongly convex around $\theta^*$ with parameter $\gamma$ with respect to \textbf{semi-norm}$ \normns{\cdot}_{\Sigma_{\cD}}$. Note that we will not treat $l_{\cD}$ as a strictly strongly convex function in any part of this proof. We only need the inequality \cref{eq:strong-convexity-of-l}.

\paragraph{Bounding the estimation error.} Recall that $\thetahatDPO$ is optimal for $l_{\cD}(\theta)$ and $\Delta\coloneqq \thetahatDPO - \theta^*$. We must have $l_{\cD}(\thetahatDPO)\leq l_{\cD}(\theta^*)$. By substracting and adding $\inner{\grad_\theta l_{\cD}(\theta^*),\Delta}$ on both sides, we have
\begin{equation*}
     l_{\cD}(\theta^*+\Delta) - l_{\cD}(\theta^*) - \inner{\grad_\theta l_{\cD}(\theta^*),\Delta} \leq -\inner{\grad_\theta l_{\cD}(\theta^*),\Delta}.
\end{equation*}
For the right hand side above, we have 
\begin{equation*}
     \absns{\inner{\grad_\theta l_{\cD}(\theta^*),\Delta}} \leq \normns{\grad_\theta l_{\cD}(\theta^*)}_{(\Sigma_{\cD}+\lambda I)^{-1}}\normns{\Delta}_{\Sigma_{\cD}+\lambda I}, \quad \text{ for any } \lambda > 0.
\end{equation*}
By $\gamma$-strong convexity of $l_{\cD}$ at $\theta^*$, we have
\begin{equation*}
     l_{\cD}(\theta^*+\Delta) - l_{\cD}(\theta^*) - \inner{\grad_\theta l_{\cD}(\theta^*), \Delta} \geq \frac{\gamma}{2}\normns{\Delta}^2_{\Sigma_{\cD}}.
\end{equation*}
Combining the inequalities, we have $\frac{\gamma}{2}\normns{\Delta}^2_{\Sigma_{\cD}} \leq \normns{\grad_\theta l_{\cD}(\theta^*)}_{(\Sigma_{\cD}+\lambda I)^{-1}}\normns{\Delta}_{\Sigma_{\cD}+\lambda I}$. Now we need to bound the term $\normns{\grad_\theta l_{\cD}(\theta^*)}_{(\Sigma_{\cD}+\lambda I)^{-1}}$. We can calculate the gradient w.r.t. $\theta$ of the pointwise loss as follows
\begin{align*}
    \grad_\theta l_z(\theta) &= \grad_\theta [-y \log\sigma(\beta h_\theta(s,a^1,a^2)) - (1-y) \log\sigma(\beta h_\theta(s,a^2,a^1))] \\
    &= -y\grad_\theta \log\sigma(\beta h_\theta(s,a^1,a^2)) - (1-y)\grad_\theta \log\sigma(\beta h_\theta(s,a^2,a^1)) \\
    &= -\beta y\sigma(-\beta h_\theta(s,a^1,a^2))\grad_\theta h_\theta(s,a^1,a^2) - \beta(1-y)\sigma(\beta h_\theta(s,a^1,a^2))\grad_\theta h_\theta(s,a^2,a^1) \\
    &\stackeq{(a)} -\beta(y\sigma(\beta h_\theta(s,a^2,a^1)) - (1-y)\sigma(\beta h_\theta(s,a^1,a^2))) (\psi(s,a^1)-\psi(s,a^2)),
\end{align*}
where $(a)$ is due to $\grad_\theta h_\theta(s,a^1,a^2)=\psi(s,a^1)-\psi(s,a^2)$ calculated in \cref{eq:grad-h-func-calculated}. This implies that
\begin{equation}\label{eq:non-robust-dpo-grad-sample-loss}
    \grad_\theta l_{\cD}(\theta^*) = \frac{-\beta}{n}\sum_{i=1}^n [y_i\sigma(\beta h_{\theta^*}(s_i,a_i^2,a_i^1)) - (1-y_i)\sigma(\beta h_{\theta^*}(s_i,a_i^1,a_i^2))] x_i,
\end{equation}
where $x_i = \psi(s_i,a_i^1)-\psi(s_i,a_i^2)$. Now let's define a random vector $V\in\RR^n$ with i.i.d. components as
\begin{equation}\label{eq:non-robust-dpo-quadradic-rv}
    V_i = \begin{cases}
        \sigma(\beta h_{\theta^*}(s_i,a_i^2,a_i^1)) & \text{w.p. } \sigma(\beta h_{\theta^*}(s_i,a_i^1,a_i^2)), \\
        -\sigma(\beta h_{\theta^*}(s_i,a_i^1,a_i^2)) & \text{w.p. } \sigma(\beta h_{\theta^*}(s_i,a_i^2,a_i^1)).
    \end{cases}
\end{equation}
Then we have $\grad_\theta l_{\cD}(\theta^*)=-\frac{\beta}{n}X^{\top} V$. It's easy to verify that $\EE V_i = 0$ and $\absns{V_i}\leq 1$, for all $1\leq i\leq n$. Next, if we define the $n\times n$ matrix $M\coloneqq \frac{\beta^2}{n^2} X(\Sigma_{\cD}+\lambda I)^{-1}X^{\top}$, then we can write $\normns{\grad_\theta l_{\cD}(\theta^*)}_{(\Sigma_{\cD}+\lambda I)^{-1}}^2=V^{\top}MV$. Let the eigendecomposition of $X^{\top}X$ be $U\Lambda U^{\top}$. Observe that
\begin{equation*}
    M = \frac{\beta^2}{n^2} X(\Sigma_{\cD}+\lambda I)^{-1} X^{\top} = \frac{\beta^2}{n^2} XU(\Lambda/n+\lambda I)^{-1}U^{\top} X^{\top}.
\end{equation*}
We can bound the trace of $M$ as follows
\begin{align*}
    \tr(M) &= \tr(\frac{\beta^2}{n^2} XU(\Lambda/n+\lambda I)^{-1}U^{\top} X^{\top}) = \frac{\beta^2}{n^2} \tr(U(\Lambda/n+\lambda I)^{-1}U^{\top} U\Lambda U^{\top}) \\
    &= \frac{\beta^2}{n^2} \tr(U(\Lambda/n+\lambda I)^{-1}\Lambda U^{\top}) =\frac{\beta^2}{n^2}\tr((\Lambda/n+\lambda I)^{-1}\Lambda)  = \frac{\beta^2}{n^2} \sum_{i=1}^d \frac{n e_i}{e_i+\lambda n} \\
    &\leq \frac{\beta^2}{n^2} \cdot nd = \frac{\beta^2 d}{n},
\end{align*}
where $e_i$ is the $i$-th eigenvalue of $X^{\top}X$. Similarly, we can bound $\tr(M^2)\leq \frac{\beta^4 d}{n^2}$. Now, let $X = \Utilde \Sigma \Vtilde^{\top}$ be the singular value decomposition of $X$. Then we can show that
\begin{equation*}
    M = \frac{\beta^2}{n^2} X(X^{\top}X/n+\lambda I)^{-1}X^{\top} = \frac{\beta^2}{n^2} \Utilde \Sigma(\Sigma^{\top}\Sigma/n+\lambda I)^{-1}\Sigma \Utilde^{\top}.
\end{equation*}
Since $X(\Sigma_{\cD}+\lambda I)^{-1}X^{\top}$ is symmetric, and clearly $\Utilde \Sigma(\Sigma^{\top}\Sigma/n+\lambda I)^{-1}\Sigma \Utilde^{\top}$ diagonalizes it, the eigenvalue of it takes form $\frac{\sigma_i^2}{\sigma_i^2/n+\lambda}$, where $\sigma_i$ is the $i$-th singular value of $X$. Hence, all eigenvalues are upper bounded by $n$. Then we must have $\opnormns{M}=\lambdamax(M) \leq \frac{\beta^2}{n}$. Since the components of $V$ are i.i.d. with $\EE V_i=0$ and $\absns{V_i}\leq 1$, the elements are $1$-sub-Gaussian, we can use the Bernstein's inequality for sub-Gaussian random variables in quadratic form (see \cref{lem:psd-quadratic-form-concentration}). It implies that with probability at least $1-\delta$, 
\begin{align*}
    \normns{\grad_\theta l_{\cD}(\theta^*)}_{(\Sigma_{\cD}+\lambda I)^{-1}}^2 &= V^{\top}MV \leq \tr(M) + 2\sqrt{\tr(M^2)\log(1/\delta)} + 2\opnormns{M}\log(1/\delta) \\
    &\leq \frac{\beta^2 d}{n} + 2\sqrt{\frac{\beta^4}{n^2} d \log(1/\delta)} + 2\frac{\beta^2}{n}\log(1/\delta) = \frac{\beta^2}{n} (d + 2\sqrt{d\log(1/\delta)} + 2\log(1/\delta)).
\end{align*}
Set $a= \sqrt{d}$ and $b=\sqrt{\log(1/\delta)}$. Note that we have
\begin{align*}
    d + 2\sqrt{d\log(1/\delta)} + 2\log(1/\delta) &= (a+b)^2 + b^2 \\
    &\leq 2(a+b)^2 = 2 (a^2+b^2+2ab) \\
    &\leq 2 (a^2+b^2 + a^2+ b^2) = 4 (a^2+b^2) = 4(d + \log(1/\delta)),
\end{align*}
where the last inequality is due to AM-GM inequality. Altogether, we have $\normns{\grad_\theta l_{\cD}(\theta^*)}_{(\Sigma_{\cD}+\lambda I)^{-1}}^2 \leq \frac{4\beta^2}{n}(d+\log(1/\delta))$. 

The final assembly now begins as follows
\begin{align*}
    \frac{\gamma}{2}\normns{\Delta}^2_{\Sigma_{\cD}+\lambda I} &= \frac{\gamma}{2}\normns{\Delta}^2_{\Sigma_{\cD}} + \frac{\gamma}{2}\normns{\Delta}_{\lambda I}^2 =\frac{\gamma}{2}\normns{\Delta}^2_{\Sigma_{\cD}} + \frac{\lambda\gamma}{2}\normns{\Delta}^2 \\
    &\leq \normns{\grad_\theta l_{\cD}(\theta^*)}_{(\Sigma_{\cD}+\lambda I)^{-1}}\normns{\Delta}_{\Sigma_{\cD}+\lambda I} + \frac{\lambda\gamma}{2}\normns{\Delta}^2 \\
    &\leq \sqrt{\frac{4\beta^2}{n}(d+\log(1/\delta))}\normns{\Delta}_{\Sigma_{\cD}+\lambda I} + \frac{\lambda\gamma}{2}4B^2,
\end{align*}
where the last inequality uses triangle inequality and the assumption that $\normns{\theta} \leq B, \forall \theta\in\Theta$. This implies that
\begin{equation*}
    \normns{\Delta}^2_{\Sigma_{\cD}+\lambda I} \leq \frac{2}{\gamma}\sqrt{\frac{4\beta^2}{n}(d+\log(1/\delta))}\normns{\Delta}_{\Sigma_{\cD}+\lambda I} + 4\lambda B^2.
\end{equation*}
Now denote $\alpha=\frac{2}{\gamma}\sqrt{\frac{4\beta^2}{n}(d+\log(1/\delta))}$ and $\beta = 4\lambda B^2$, and let $x = \normns{\Delta}_{\Sigma_{\cD}+\lambda I}$. Since we have $x^2-\alpha x -\beta \leq 0$, then $x$ must be less than the bigger root, i.e.,
\begin{equation*}
    x\leq \frac{\alpha+\sqrt{\alpha^2+4\beta}}{2} \leq \sqrt{\frac{\alpha^2+\alpha^2+4\beta}{2}} = \sqrt{\alpha^2 + 2\beta},
\end{equation*}
where the second inequality is by Jensen's inequality. Finally, we have that
\begin{equation*}
    \normns{\thetahatDPO - \theta^*}_{\Sigma_{\cD}+\lambda I} = \normns{\Delta}_{\Sigma_{\cD}+\lambda I} \leq 2 \sqrt{\frac{4\beta^2}{\gamma^2 n}(d+\log(1/\delta)) + 2\lambda B^2}.
\end{equation*}

\subsection{Proof of WDPO Loss Function Convergence}\label{sec:proof-of-loss-function-convergence}
\begin{lemma}[Convergence of WDPO loss]\label{lem:convergence-of-wdpo-loss}
    Fix any $\theta\in\Theta$ and $\rho>0$. Let $\delta\in(0,1)$. With probability $1-\delta$, 
    \begin{equation*}
        \absns{\cLW(\theta;\rho)-\cLW_n(\theta;\rho)} \leq \sqrt{\frac{K^2\log(2/\delta)}{2n}},
    \end{equation*}
    where $K=\absns{\log\sigma(-4\beta B)}$.
\end{lemma}
\begin{proof}
    Recall the strong duality in \cref{thm:wasser-duality}. The term $\inf_{z\in\cZ} [\eta d^p(z,z') - l(z;\theta)]$ is called the \textit{Moreau-Yosida regularization} of $-l$ with parameter $1/\eta$. We denote it by $l_\eta(z;\theta)$. Now observe that
    \begin{align*}
        \abs{\cLW(\theta;\rho)-\cLW_n(\theta;\rho)} &= \abs{\sup_{\sfP\colon \sfW_p(\sfP,\sfP^o)\leq \rho} \EE_{z\sim\sfP} [l_z(\theta)] - \sup_{\sfP\colon \sfW_p(\sfP,\sfP^o_n)\leq \rho} \EE_{z\sim\sfP} [l_z(\theta)]} \\
        &\stackeq{(a)} \abs{\inf_{\eta\geq 0} \{\eta\rho^p - \EE_{z\sim\sfP^o}[l_\eta(z;\theta)]\} - \inf_{\eta\geq 0} \{\eta\rho^p - \EE_{z\sim\sfP_n^o}[l_\eta(z;\theta)]\}} \\
        &\stackleq{(b)} \sup_{\eta\geq 0} \abs{\EE_{z\sim \sfP^o}[l_\eta(z;\theta)] - \EE_{z\sim \sfP^o_n}[l_\eta(z;\theta)]},
    \end{align*}
    where $(a)$ is by the strong duality, and $(b)$ is due to $\absns{\inf_x f(x) - \inf_x g(x)}\leq \sup_x \absns{f(x)-g(x)}$. Next, we will show that, for any $\eta\geq 0$, the function $l_\eta$ is a bounded function. We first prove its upper bound. The negative DPO loss takes the following form:
    \begin{equation*}
        -l(z;\theta) = y\log\sigma(x) + (1-y)\log\sigma(-x)\leq 0, \quad y\in\{0,1\}.
    \end{equation*}
    The inequality is because the sigmoid function is \textit{strictly} bounded between $0$ and $1$, i.e., $\sigma\in(0,1)$. This implies that $\log\sigma$ is non-positive. Using this, we have that
    \begin{equation*}
        l_\eta(z;\theta) = \inf_{z'\in\cZ}[\eta d^p(z',z)-l(z';\theta)] \leq  \inf_{z'\in\cZ}[\eta d^p(z',z)] = 0.
    \end{equation*}
    Now we prove its lower bound. Recall that in the analysis of non-robust DPO loss, we proved that $\absns{h_\theta(s,a^1,a^2)}\leq 4B$ (see \cref{eq:h-function-bounds}). Since both $\log$ and $\sigma$ are increasing functions, we have that $\log\sigma(\beta h_\theta(s,a^1,a^2)) \geq \log \sigma(-4\beta B)$. Now observe that
    \begin{align*}
        l_\eta(z;\theta) &= \inf_{z'\in\cZ}[\eta d^p(z',z)-l(z;'\theta)] \\ &\geq \inf_{z'\in\cZ} [-l(z';\theta)] = \inf_{s,a^1,a^2,y} [y\log\sigma(\beta h_\theta(s,a^1,a^2)) + (1-y) \log\sigma(\beta h_\theta(s,a^2,a^1))] \\
        &\geq \log\sigma(-4\beta B),
    \end{align*}
    where the first inequality is because both $\eta$ and metric $d^p$ are non-negative. The last inequality is because only one of the $\log\sigma$ term will be activated and the lower bound we recalled above. Denote $K=\absns{\log\sigma(-4\beta B)}$. Since $l_\eta$ is a bounded function, by Hoeffding's inequality for bounded random variable (\cref{thm:hoeffding}), we have
    \begin{equation*}
        \PP\bigg(\abs{\EE_{z\sim \sfP^o}[l_\eta(z;\theta)] - \EE_{z\sim \sfP^o_n}[l_\eta(z;\theta)]} \geq \epsilon \bigg) \leq 2\exp{\frac{-2n\epsilon^2}{K^2}}.         
    \end{equation*}

    By picking $\delta$ to be the right hand side above, we have that, with probability at least $1-\delta$,
    \begin{equation*}
        \absns{\EE_{z\sim \sfP^o}[l_\eta(z;\theta)] - \EE_{z\sim \sfP^o_n}[l_\eta(z;\theta)]} \leq \sqrt{\frac{K^2\log(2/\delta)}{2n}}.
    \end{equation*}
    Since $K$ does not depend on $\eta$, such concentration is uniform for all functions $l_\eta, \eta\geq 0$. We have the desired result.
\end{proof}


\subsection{Proof of the Strong Convexity of WDPO Loss}\label{sec:proof-of-sup-dpo-strongly-convex}
We first prove that the function $g(\theta;\sfP)\coloneqq \EE_{z\sim\sfP}[l(z;\theta)]$ is strongly convex, for any $\sfP$, as follows:
\begin{lemma}\label{lem:expected-dpo-strongly-convex}
    Let $l(z;\theta)$ be the DPO loss function. Assume that \cref{assum:uniform-data-cov-assumption} is in place. Then $g(\theta)\coloneqq \EE_{z\sim \sfP }[l(z;\theta)]$ is $\gamma$-strongly convex with respect to norm $\normns{\cdot}_{\Sigma_{\sfP}}$, where $\Sigma_{\sfP}=\EE_{(s,a^1,a^2,y)\sim\sfP}(\psi(s,a^1) - \psi(s,a^2))(\psi(s,a^1) - \psi(s,a^2))^{\top}$, and $\gamma=\frac{\beta^2e^{4\beta B}}{(1+e^{4\beta B})^2}$.
\end{lemma}
\begin{proof}
Recall that we proved that the Hessian of the pointwise DPO loss takes the form:
    \begin{equation*}
        \grad_\theta^2 l_z(\theta) =  \beta^2\sigma(\beta h_\theta(s,a^1,a^2))\sigma(-\beta h_\theta(s,a^1,a^2))  (\psi(s,a^1) - \psi(s,a^2))(\psi(s,a^1) - \psi(s,a^2))^{\top}.
    \end{equation*}
    In addition, we also proved that (see \cref{eq:lower-bound-dpo-loss-coef})
    \begin{equation*}
    \beta^2\sigma(\beta h_\theta(s,a^1,a^2))\sigma(-\beta h_\theta(s,a^1,a^2)) \geq \frac{\beta^2 e^{4\beta B}}{(1+e^{4\beta B})^2}, \quad \forall\theta\in\Theta.
    \end{equation*}
    This implies that
    \begin{equation*}
        u^{\top}\grad_\theta^2 l_z(\theta) u \geq \gamma \normns{(\psi(s,a^1)-\psi(s,a^2))^{\top}u}^2_2, \quad \forall u\in\RR^d,
    \end{equation*}
    where $\gamma=\frac{\beta^2e^{4\beta B}}{(1+e^{4\beta B})^2}$. Thus, if we introduce the error vector $\Delta\coloneqq \theta'-\theta$, where $\theta,\theta'\in\Theta$, then by the linear approximation theorem (\cref{lem:linear-approximation-theorem}), there exists $\alpha\in[0,1]$ and $\thetatilde=\alpha\theta+(1-\alpha)\theta'$ such that
 \begin{equation}\label{eq:dpo-loss-lower-bounding-almost-strong-convex}
     l_z(\theta+\Delta) - l_z(\theta) - \inner{\grad_\theta l_z(\theta), \Delta} = \frac{1}{2}\Delta^{\top}\grad_\theta^2 l_z(\thetatilde) \Delta \geq \frac{\gamma}{2}\normns{(\psi(s,a^1)-\psi(s,a^2))^{\top}\Delta}^2_2 = \frac{\gamma}{2}\normns{\Delta}^2_{\Sigma_z},
 \end{equation}
where $\Sigma_z = (\psi(s,a^1)-\psi(s,a^2))(\psi(s,a^1)-\psi(s,a^2))^{\top}$. Note that $\Sigma_z$ is only semi-definite. Let $\alpha\in[0,1]$ and $\theta,\theta'\in\Theta$. Observe that
\begin{align*}
    g(\alpha\theta+(1-\alpha)\theta') &= \EE_{z\sim\sfP}[l(\alpha\theta+(1-\alpha)\theta'; z)] \\
    &\stackleq{(a)} \EE_{z\sim\sfP} \bigg[\alpha l(z;\theta) + (1-\alpha) l(\theta';z)-\frac{\gamma}{2}\alpha(1-\alpha)\normns{\theta-\theta'}^2_{\Sigma_z} \bigg] \\
    &= \alpha g(\theta) + (1-\alpha) g(\theta') - \frac{\gamma}{2}\alpha(1-\alpha)(\theta-\theta')^{\top} \EE_\sfP[\Sigma_z](\theta-\theta') \\
    &= \alpha g(\theta) + (1-\alpha) g(\theta') - \frac{\gamma}{2}\alpha(1-\alpha)\normns{\theta-\theta'}^2_{\Sigma_{\sfP}},
\end{align*}
where $(a)$ is by \cref{lem:first-order-character-ization-of-strong-convexity}. In particular, the equivalence between the inequalities, \cref{eq:dpo-loss-lower-bounding-almost-strong-convex} and $(a)$, can be found in the proof of \citet[Theorem 5.24]{beck2017first}, and the author would like to comment that the proof does not rely on whether $\normns{\cdot}_{\Sigma_z}$ is a semi-norm or a norm. Now, by \cref{assum:uniform-data-cov-assumption}, $\Sigma_{\sfP}$ is strictly positive definite, hence $\normns{\cdot}_{\Sigma_{\sfP}}$ is a norm. This implies that $g$ is $\gamma$-strongly convex with respect to $\normns{\cdot}_{\Sigma_{\sfP}}$.
\end{proof}
Now, we are ready to prove our main strong convexity lemma.
\begin{lemma}[Lemma 1 restated]\label{lem:proof-of-lemma-1}
    Let $l(z;\theta)$ be the DPO loss function. The Wasserstein distributionally robust DPO loss function,
    \begin{equation*}
     \cLW(\theta;\rho) \coloneqq \sup_{\sfP\colon\sfW_p(\sfP,\sfP^o)\leq\rho} \EE_{z\sim\sfP}[l(z;\theta)],
    \end{equation*}
    is $\gamma\lambda$-strongly convex in $\theta$ with respect to (non-weighted) $2$-norm $\normns{\cdot}_2$, where $\lambda$ is the regularity condition number defined in \cref{assum:uniform-data-cov-assumption}, and $\gamma=\frac{\beta^2e^{4\beta B}}{(1+e^{4\beta B})^2}$.
\end{lemma}
\begin{proof}
    Let $\alpha\in[0,1]$ and $\theta,\theta'\in\Theta$. First, we denote $h(\theta;\sfP)=\EE_{z\sim\sfP}[l(z;\theta)]$ for any $\sfP$ in the Wasserstein ball. In \cref{lem:expected-dpo-strongly-convex}, we proved that $h$ is $\gamma$-strongly convex in $\theta$ w.r.t. norm $\normns{\cdot}_{\Sigma_{\sfP}}$. Now observe that
    \begin{align*}
        \cLW(\alpha\theta+(1-\alpha)\theta';\rho) &= \sup_{\sfP\colon\sfW_p(\sfP,\sfP^o)\leq\rho} h(\alpha\theta+(1-\alpha)\theta';z)\\
        &\stackleq{(a)} \sup_{\sfP\colon\sfW_p(\sfP,\sfP^o)\leq\rho} \bigg\{  \alpha h(\theta;\sfP) + (1-\alpha)h(\theta';\sfP) -\frac{\gamma}{2}\alpha(1-\alpha)\normns{\theta-\theta'}^2_{\Sigma_\sfP} \bigg\} \\
        &\stackleq{(b)} \alpha\cLW(\theta;\rho) + (1-\alpha)\cLW(\theta';\rho) + \sup_{\sfP\colon\sfW_p(\sfP,\sfP^o)\leq\rho}-\frac{\gamma}{2}\alpha(1-\alpha)\normns{\theta-\theta'}^2_{\Sigma_{\sfP}}\\
        &= \alpha\cLW(\theta;\rho) + (1-\alpha)\cLW(\theta';\rho) - \frac{\gamma}{2}\alpha(1-\alpha)\inf_{\sfP\colon\sfW_p(\sfP,\sfP^o)\leq\rho}\normns{\theta-\theta'}^2_{\Sigma_{\sfP}} \\
        &\leq \alpha\cLW(\theta;\rho) + (1-\alpha)\cLW(\theta';\rho) - \frac{\gamma}{2}\alpha(1-\alpha)\inf_{\sfP\colon\sfW_p(\sfP,\sfP^o)\leq\rho}\lambdamin(\Sigma_{\sfP})\normns{\theta-\theta'}^2_2 \\
        &\stackleq{(c)} \alpha\cLW(\theta;\rho) + (1-\alpha)\cLW(\theta';\rho)- \frac{\gamma\lambda}{2}\alpha(1-\alpha)\normns{\theta-\theta'}^2_2.
    \end{align*}
    Note that the function $g(\theta)=\EE_{z\sim\sfP}[l(z;\theta)]$ is $\gamma$-strongly convex with respect to $\normns{\cdot}_{\Sigma_\sfP}$ by \cref{lem:expected-dpo-strongly-convex}. We use this fact in $(a)$. The inequality in $(b)$ is due to $\sup_x (f(x)+g(x))\leq \sup_x f(x) + \sup_x g(x)$. The last inequality $(c)$ is because $\lambdamin(\Sigma_{\sfP}) \geq \lambda$, for all $\sfP\in\cP_{\sfW}$ by \cref{assum:uniform-data-cov-assumption}. This implies that $\cLW$ is a $\gamma\lambda$-strongly convex function with respect to $\normns{\cdot}_2$.
\end{proof}

\subsection{Proof of Policy Parameter Convergence of WDPO}\label{sec:proof-of-robust-policy-param-converg}
By \cref{lem:convergence-of-wdpo-loss}, we have that, with probability at least $1-\delta$,
    \begin{align*}
        \cLW(\thetaW_n;\rho) &-\cLW(\thetaW;\rho) \\
        &= \cLW(\thetaW_n;\rho)-\cLW_n(\thetaW_n;\rho)+\cLW_n(\thetaW_n;\rho)-\cLW_n(\thetaW;\rho) +\cLW_n(\thetaW;\rho)-\cLW(\thetaW;\rho)\\
        &\leq \absns{\cLW(\thetaW_n;\rho)-\cLW_n(\thetaW_n;\rho)} + \absns{\cLW_n(\thetaW;\rho)-\cLW(\thetaW;\rho)} \\
        &\leq \sqrt{\frac{2K^2\log(2/\delta)}{n}},
    \end{align*}
    where the first inequality is because $\thetaW_n$ is the minimizer of $\cLW_n$. Now by the $\gamma\lambda$-strong convexity of $\cLW$ (see \cref{lem:sup-dpo-strongly-convex}) and \cref{lem:strongly-convex-uniqueness-of-minimizer}.II, we have that
    \begin{equation*}
        \normns{\thetaW_n -\thetaW}_2^2 \leq \sqrt{\frac{8K^2\log(2/\delta)}{\gamma^2\lambda^2 n}}.
    \end{equation*}

   

\section{Proof of KLDPO Sample Complexity}\label{sec:kldpo-proof-appendix}

We state a result from \citet{hu2013kullback} that proves an equivalent condition for the infimum to be achieved at $\lambda^*=0$.
\begin{proposition}[\text{\citealp[Proposition 2]{hu2013kullback}}]\label{prop:lambda-zero-equiv}
    Let $l_u(z;\theta)$ be the essential supremum of $l(z;\theta)$ under measure $\sfP^o$, i.e.,
    \begin{equation*}
        l_u(z;\theta) = \inf\{ t \in\RR\colon\PP(l(z;\theta)>t)=0\}.
    \end{equation*}
    Also let $\kappa_u=\PP(l(z;\theta)=l_u(z;\theta))$, i.e., $\kappa_u$ is the probability mass of the distribution $\sfP^o$ on the essential supremum of $l$. Then $\lambda^*=0$ if and only if $l_u(z;\theta)<+\infty$, $\kappa_u>0$, and $\log\kappa_u+\rho \geq 0$, where $\rho$ is the diameter of the KL uncertainty set.
\end{proposition}

We now make an assumption on the loss function(s) $l(\cdot;\theta),\;\theta\in\Theta$. Note that this assumption is only used in proving the dual reformulation of KLDPO objective.
\begin{assumption}\label{assum:kldpo-assumptions}
    We assume that $l(z;\theta)\leq L$ for all $\theta\in\Theta$. That is, the loss function is upper bounded by $L$. In addition, we also assume that $\Theta$ permits a uniform upper bound on $\lambda_\theta$. That is, we assume that $\sup_{\theta\in\Theta} \lambda_\theta < \lambdaoverline$.
\end{assumption}

We now prove the following dual reformulation result:
\begin{lemma}\label{lem:kldpo-dual-reformulation-complete}
    Let $l(z;\theta)$ be the DPO loss. The KLDPO loss function has the following dual reformulation
    \begin{equation*}
        \cLKL(\theta;\rho) = \sup_{\sfP\colon \KLdiverg{\sfP}{\sfP^o}\leq \rho} \EE_{z\sim\sfP} [l(z;\theta)]= \inf_{\lambda\in[\lambdaunderline,\lambdaoverline]}\bigg\{\lambda\rho + \lambda\log\bigg(\EE_{z\sim\sfP^o}\bigg[ \exp{\frac{l(z;\theta)}{\lambda}}\bigg]\bigg)\bigg\},
    \end{equation*}
    where $0<\lambdaunderline<\lambdaoverline<\infty$ are some constants.
\end{lemma}
\begin{proof}
    We include the derivation here for completeness. Previous works in optimization and distributionally robust reinforcement learning have covered the dual problem of distributionally robust optimization with KL uncertainty set (e.g., see \citet{hu2013kullback,panaganti22a,xu-panaganti-2023samplecomplexity}).

    Recall that $f(t)=t\log(t)$ corresponds to the KL divergence. The optimal $t$ for $f^*(s)=\sup_{t\geq 0}\{ st-t\log(t) \}$ is $\exp{s-1}$. This implies that the Fenchel conjugate of $f$ is $f^*(s) = \exp{s-1}$. From \cref{lem:dual-reformulation-f-diverg}, we get
    \begin{align*}
        \sup_{\sfP\colon \KLdiverg{\sfP}{\sfP^o}\leq \rho} \EE_{z\sim\sfP} [l(z;\theta)]&= \inf_{\lambda\geq 0,\eta\in\RR} \bigg\{\EE_{z\sim\sfP^o} \bigg[ \lambda f^*\bigg( \frac{l(z;\theta)-\eta}{\lambda}\bigg)\bigg] + \lambda\rho+\eta \bigg\} \\
        &= \inf_{\lambda\geq 0,\eta\in\RR}\bigg\{\EE_{z\sim\sfP^o}\bigg[ \lambda\exp{\frac{l(z;\theta)-\eta}{\lambda} - 1} \bigg] + \lambda\rho+\eta\bigg\} \\
        &= \inf_{\lambda\geq 0}\bigg\{\lambda\rho + \lambda\log\bigg(\EE_{z\sim\sfP^o}\bigg[ \exp{\frac{l(z;\theta)}{\lambda}}\bigg]\bigg)\bigg\},
    \end{align*}
    where the last equality by plugging in the optimal $\eta$, i.e., $\eta^*=\lambda\log(\EE_{z\sim\sfP^o}[\exp{l(z;\theta)/\lambda - 1}])$. Now observe that
    \begin{equation*}
        h(\lambda;\theta)\coloneqq \lambda\rho + \lambda\log\bigg(\EE_{z\sim\sfP^o}\bigg[ \exp{\frac{l(z;\theta)}{\lambda}}\bigg]\bigg) \geq \lambda\rho \eqqcolon g(\lambda).
    \end{equation*}
    The inequality is because the loss function is non-negative, i.e., $l\geq0$, and $h$ is increasing in $l$. Now $g(\lambda)$ is a strictly increasing function that lower bounds function $h(\lambda;\theta)$. Since $g(\lambda)\to\infty$ as $\lambda\to\infty$, $h(\lambda;\theta)$ cannot achieve its infimum at $\infty$. In other words, there exists $\lambdaoverline_\theta$ such that
    \begin{equation*}
        h(\lambda; \theta) \geq g(\lambda) > g(\lambdaoverline_\theta), \forall \quad \lambda > \lambdaoverline_\theta.
    \end{equation*}
    This implies that it suffices to seek the infimum in $[0,\lambdaoverline_\theta]$. Hence, we have
    \begin{equation*}
        \cLKL(\theta;\rho) = \inf_{\lambda\in[0,\lambdaoverline_\theta]}\bigg\{\lambda\rho + \lambda\log\bigg(\EE_{z\sim\sfP^o}\bigg[ \exp{\frac{l(z;\theta)}{\lambda}}\bigg]\bigg)\bigg\}.
    \end{equation*}
    Now from \cref{prop:lambda-zero-equiv}, the condition $\log\kappa_u + \rho\geq 0$ is problem-dependent due to the diameter $\rho$, which is a design choice. Note that when $\kappa_u$ is close to zero, the condition $\log\kappa_u + \rho\geq 0$ is almost never true for a reasonable $\rho$. Hence, we ignore the case where $\lambda^*=0$. By \cref{assum:kldpo-assumptions}, without loss of generality, we have that $\lambda^*\in [\lambdaunderline,\lambdaoverline]$, where $\lambdaunderline$ is some problem-specific constant. Then we have the result. In the literature of distributionally robust reinforcement learning, similar arguments can be found in \citet{zhou2021finite,panaganti22a}.
\end{proof}



\begin{lemma}\label{lem:convergence-of-kldpo-loss}
    Fix any $\theta\in\Theta$ and $\rho>0$. Let $\delta\in(0,1)$. Assume \cref{assum:kldpo-assumptions} is in place. With probability $1-\delta$, we have that
    \begin{equation*}
        \absns{\cLKL(\theta;\rho)-\cLKL_n(\theta;\rho)} \leq \lambdaoverline\sqrt{\frac{\exp{L/\lambdaunderline}\log(2/\delta)}{2n}}, \quad \forall\epsilon>0,
    \end{equation*}
    where $\lambdaunderline,\lambdaoverline$ are some constants that are independent of $\epsilon$.
\end{lemma}
\begin{proof}
Observe that
\begin{align*}
    \absns{\cLKL(\theta;\rho)-\cLKL_n(\theta;\rho)} &= \abs{\sup_{\sfP\colon \KLdiverg{\sfP}{\sfP^o}\leq\rho}\EE_{z\sim\sfP}[l(z;\theta)] - \sup_{\sfP\colon \KLdiverg{\sfP}{\sfP^o_n}\leq\rho}\EE_{z\sim\sfP}[l(z;\theta)]} \\
    &\stackeq{(a)} \bigg\lvert \inf_{\lambda\in[\lambdaunderline,\lambdaoverline]}\bigg\{\lambda\rho + \lambda\log\bigg(\EE_{z\sim\sfP^o}\bigg[ \exp{\frac{l(z;\theta)}{\lambda}}\bigg]\bigg)\bigg\}  \\
    &\quad\quad- \inf_{\lambda\in[\lambdaunderline,\lambdaoverline]}\bigg\{\lambda\rho + \lambda\log\bigg(\EE_{z\sim\sfP^o_n}\bigg[ \exp{\frac{l(z;\theta)}{\lambda}}\bigg]\bigg)\bigg\} \bigg\rvert\\
    &\stackleq{(b)} \sup_{\lambda\in[\lambdaunderline,\lambdaoverline]} \abs{\lambda\log\bigg(\EE_{z\sim\sfP^o_n}\bigg[ \exp{\frac{l(z;\theta)}{\lambda}}\bigg]\bigg) - \lambda\log\bigg(\EE_{z\sim\sfP^o}\bigg[ \exp{\frac{l(z;\theta)}{\lambda}}\bigg]\bigg)} \\
    &\stackeq{(c)} \sup_{\lambda\in[\lambdaunderline,\lambdaoverline]} \lambda \abs{\log\bigg( \frac{\EE_{z\sim\sfP^o_n}[\exp{l(z;\theta)}/\lambda]}{\EE_{z\sim\sfP^o}[\exp{l(z;\theta)}/\lambda]}\bigg)} \\
    &\leq \sup_{\lambda\in[\lambdaunderline,\lambdaoverline]} \lambda \abs{\log\bigg( \frac{\absns{\EE_{z\sim\sfP^o_n}[\exp{l(z;\theta)}/\lambda] - \EE_{z\sim\sfP^o}[\exp{l(z;\theta)}/\lambda]}}{\EE_{z\sim\sfP^o}[\exp{l(z;\theta)}/\lambda]} + 1\bigg)} \\
    &\stackleq{(d)} \sup_{\lambda\in[\lambdaunderline,\lambdaoverline]} \lambda\frac{\absns{\EE_{z\sim\sfP^o_n}[\exp{l(z;\theta)}/\lambda] - \EE_{z\sim\sfP^o}[\exp{l(z;\theta)}/\lambda]}}{\EE_{z\sim\sfP^o}[\exp{l(z;\theta)}/\lambda]} \\
    &\stackleq{(e)} \lambdaoverline\sup_{\lambda\in[\lambdaunderline,\lambdaoverline]} \absns{\EE_{z\sim\sfP^o_n}[\exp{l(z;\theta)}/\lambda] - \EE_{z\sim\sfP^o}[\exp{l(z;\theta)}/\lambda]},
\end{align*}
where $(a)$ is by \cref{lem:kldpo-dual-reformulation-complete}. $(b)$ is because $\absns{\inf_x f(x) - \inf_x g(x)}\leq \sup_x\absns{f(x)-g(x)}$. $(c)$ is by \cref{assum:kldpo-assumptions}. $(d)$ is due to $\absns{\log(1+x)}\leq \absns{x}, \forall x\geq 0$. $(e)$ is due to the fact that the loss function $l$ is non-negative, i.e., $l\geq 0$. Now by applying Hoeffding's inequality (\cref{thm:hoeffding}), we have
\begin{equation*}
    \PP(\absns{\EE_{z\sim\sfP^o_n}[\exp{l(z;\theta)}/\lambda] - \EE_{z\sim\sfP^o}[\exp{l(z;\theta)}/\lambda]} \geq \epsilon ) \leq 2\exp{-\frac{2n\epsilon^2}{\exp{L/\lambdaunderline}}}.
\end{equation*}
By choosing $\epsilon = \sqrt{\frac{\exp{L/\lambdaunderline}\log(2/\delta)}{2n}}$, we have the result.
\end{proof}
We prove a strong convexity result similar to \cref{lem:sup-dpo-strongly-convex} for KLDPO loss function.
\begin{lemma}[Strong convexity of KLDPO loss]\label{lem:sup-dpo-strongly-convex-kl}
    Let $l(z;\theta)$ be the DPO loss function. The KL distributionally robust DPO loss function,
    \begin{equation*}
     \cLKL(\theta;\rho) \coloneqq \sup_{\sfP\colon\KLdiverg{\sfP}{\sfP^o}\leq\rho} \EE_{z\sim\sfP}[l(z;\theta)],
    \end{equation*}
    is $\gamma\lambda$-strongly convex in $\theta$ with respect to (non-weighted) $2$-norm $\normns{\cdot}_2$, where $\lambda$ is the regularity condition number defined in \cref{assum:uniform-data-cov-assumption}, and $\gamma=\frac{\beta^2e^{4\beta B}}{(1+e^{4\beta B})^2}$.
\end{lemma}
\begin{proof}
    Let $\alpha\in[0,1]$ and $\theta,\theta'\in\Theta$. First, we denote $h(\theta;\sfP)=\EE_{z\sim\sfP}[l(z;\theta)]$ for any $\sfP$ in the KL ball. In \cref{lem:expected-dpo-strongly-convex}, we proved that $h$ is $\gamma$-strongly convex in $\theta$ w.r.t. norm $\normns{\cdot}_{\Sigma_{\sfP}}$. Now observe that
    \begin{align*}
        \cLKL(\alpha\theta+(1-\alpha)\theta';\rho) &= \sup_{\sfP\colon\KLdiverg{\sfP}{\sfP^o}\leq\rho} h(\alpha\theta+(1-\alpha)\theta';z)\\
        &\stackleq{(a)} \sup_{\sfP\colon\KLdiverg{\sfP}{\sfP^o}\leq\rho} \bigg\{  \alpha h(\theta;\sfP) + (1-\alpha)h(\theta';\sfP) -\frac{\gamma}{2}\alpha(1-\alpha)\normns{\theta-\theta'}^2_{\Sigma_\sfP} \bigg\} \\
        &\stackleq{(b)} \alpha\cLKL(\theta;\rho) + (1-\alpha)\cLKL(\theta';\rho) + \sup_{\sfP\colon\KLdiverg{\sfP}{\sfP^o}\leq\rho}-\frac{\gamma}{2}\alpha(1-\alpha)\normns{\theta-\theta'}^2_{\Sigma_{\sfP}}\\
        &= \alpha\cLKL(\theta;\rho) + (1-\alpha)\cLKL(\theta';\rho) - \frac{\gamma}{2}\alpha(1-\alpha)\inf_{\sfP\colon\KLdiverg{\sfP}{\sfP^o}\leq\rho}\normns{\theta-\theta'}^2_{\Sigma_{\sfP}} \\
        &\leq \alpha\cLKL(\theta;\rho) + (1-\alpha)\cLKL(\theta';\rho) - \frac{\gamma}{2}\alpha(1-\alpha)\inf_{\sfP\colon\KLdiverg{\sfP}{\sfP^o}\leq\rho}\lambdamin(\Sigma_{\sfP})\normns{\theta-\theta'}^2_2 \\
        &\stackleq{(c)} \alpha\cLKL(\theta;\rho) + (1-\alpha)\cLKL(\theta';\rho)- \frac{\gamma\lambda}{2}\alpha(1-\alpha)\normns{\theta-\theta'}^2_2.
    \end{align*}
    Note that the function $g(\theta)=\EE_{z\sim\sfP}[l(z;\theta)]$ is $\gamma$-strongly convex with respect to $\normns{\cdot}_{\Sigma_\sfP}$ by \cref{lem:expected-dpo-strongly-convex}. We use this fact in $(a)$. The inequality in $(b)$ is due to $\sup_x (f(x)+g(x))\leq \sup_x f(x) + \sup_x g(x)$. The last inequality $(c)$ is because $\lambdamin(\Sigma_{\sfP}) \geq \lambda$, for all $\sfP\in\cP_{\mathrm{KL}}$ by \cref{assum:uniform-data-cov-assumption}. This implies that $\cLKL$ is a $\gamma\lambda$-strongly convex function with respect to $\normns{\cdot}_2$.
\end{proof}

\subsection{Proof of Policy Parameter Convergence of KLDPO}\label{sec:proof-of-robust-policy-param-converg-kl}
By \cref{lem:convergence-of-kldpo-loss}, we have that, with probability at least $1-\delta$,
    \begin{align*}
        \cLKL(\thetaKL_n;\rho) &-\cLKL(\thetaKL;\rho) \\
        &= \cLKL(\thetaKL_n;\rho)-\cLKL_n(\thetaKL_n;\rho)+\cLKL_n(\thetaKL_n;\rho)-\cLKL_n(\thetaKL;\rho) +\cLKL_n(\thetaKL;\rho)-\cLKL(\thetaKL;\rho)\\
        &\leq \absns{\cLKL(\thetaKL_n;\rho)-\cLKL_n(\thetaKL_n;\rho)} + \absns{\cLKL_n(\thetaKL;\rho)-\cLKL(\thetaKL;\rho)} \\
        &\leq 2\lambdaoverline\sqrt{\frac{\exp{L/\lambdaunderline}\log(2/\delta)}{2n}}, \quad\forall\epsilon>0,
    \end{align*}
    where the first inequality is because $\thetaKL_n$ is the minimizer of $\cLKL_n$. Now by the $\gamma\lambda$-strong convexity of $\cLKL$ (see \cref{lem:sup-dpo-strongly-convex-kl}) and \cref{lem:strongly-convex-uniqueness-of-minimizer}.II, we have that
    \begin{equation*}
        \normns{\thetaKL_n -\thetaKL}_2^2 \leq \sqrt{\frac{8\lambdaoverline^2\exp{L/\lambdaunderline}\log(2/\delta)}{\gamma^2\lambda^2n}}, \quad\forall\epsilon >0.
    \end{equation*}

\section{Proof of Tractable KLDPO}\label{sec:proof-tractable-kldpo}
Next, we prove the formal version of \cref{prop:KL-dual-worst-case-informal}.
\begin{theorem}\label{thm:KL-dual-worst-case-formal}
    Suppose we have the following distributionally robust loss that corresponds to a KL uncertainty set:
    \begin{equation*}
        \sup_{\sfP\colon\KLdiverg{\sfP}{\sfP_n^o}\leq \rho} \EE_{z\sim\sfP} [l(z;\theta)].
    \end{equation*}
    A worst distribution $\sfPunderline\in\RR^n$ is related to the empirical nominal distribution $\sfP^o_n$, which is constructed using $n$ i.i.d. samples $z_1,\dots,z_n$, through
    \begin{equation}
        \sfPunderline(i) = \sfP_n^o(i) \cdot \exp{\frac{\omega - l(z_i;\theta)}{\lambda}},
    \end{equation}
    where $\sfPunderline(i)$ corresponds to the worst-case mass on the $i$-th data, and further it is subject to
    \begin{align}
        \sum_{i=1}^n \sfP_n^o(i)\cdot \exp{\frac{\omega-l(z_i;\theta)}{\lambda}} \cdot \bigg( \frac{\omega-l(z_i;\theta)}{\lambda}\bigg) &= \rho, \label{eq:constr1-of-kl-worst-case-kernel}\\
        \sum_{i=1}^n\sfP_n^o(i)\cdot \exp{\frac{\omega-l(z_i;\theta)}{\lambda}} &= 1, \label{eq:constr2-of-kl-worst-case-kernel}\\
        \lambda &\geq 0\label{eq:constr3-of-kl-worst-case-kernel}.
    \end{align}
\end{theorem}
\begin{proof}
    We re-write the objective as a convex optimization problem
    \begin{align}
        \tag{P1}\underset{p\in\RR^n}{\textbf{minimize}} \quad& \inner{p, \;l} \nonumber \\
        \textbf{subject to} \quad &\sum_{i=1}^n p_i\log\bigg( \frac{p_i}{q_i}\bigg) \leq \rho, \nonumber\\
        & \vecofone^{\top}p=1, \nonumber\\
        & p_i\geq 0, \forall i.\nonumber
    \end{align}
    First, we ignore the constraint $p_i\geq 0$ which will be automatically satisfied later. Now, the associated Lagrangian function takes the form
    \begin{equation*}
        L(p,\lambda,\mu) = \inner{p,\; l} + \lambda (\sum_{i=1}^n p_i\log(p_i/q_i)-\rho) + \mu(\vecofone^{\top} p -1).
    \end{equation*}
    We can calculate the KKT conditions as follows
    \begin{align*}
        \frac{\partial L}{\partial p_i} = l_i + \lambda(\log(p_i/q_i)+ 1) + \mu = 0.
    \end{align*}
    This implies that
    \begin{equation*}
        p_i = q_i \exp{-1}\exp{\frac{-\mu-l_i}{\lambda}}, \quad \forall i\in\{ 1,\dots, n\}.
    \end{equation*}
    In addition, we have other KKT conditions as follows
    \begin{align*}
        \sum_{i=1}^n p_i \log(p_i/q_i) - \rho &\leq 0 ,\\
        \sum_{i=1}^n p_i &= 1 ,\\
        \lambda &\geq 0, \\
        \lambda ( \sum_{i=1}^n p_i \log(p_i/q_i) -\rho) &= 0.
    \end{align*}
    From complimentary slackness, we have
    \begin{equation*}
        \sum_{i=1}^n p_i \log(p_i/q_i) = \rho.
    \end{equation*}
    Plugging in $p_i=q_i \exp{-1}\exp{\frac{-\mu-l_i}{\lambda}}$, we have
    \begin{equation*}
        \sum_{i=1}^n q_i \exp{\frac{-\mu-l_i}{\lambda}-1} \cdot \bigg( \frac{-\mu-l_i}{\lambda}-1\bigg) = \rho.
    \end{equation*}
    Also, we have $\sum_{i=1}^n q_i\exp{\frac{-\mu-l_i}{\lambda} - 1} = 1$. In addition, it is easy to see that the constraints $p_i\geq 0$, $\forall i$, are satisfied since $q_i\exp{\frac{-\mu-l_i}{\lambda}-1}\geq 0$. By setting $\omega=-\mu-\lambda$, we have the result.
\end{proof}
Here, $\omega$ and $\lambda$ are implicitly defined by the constraints (\cref{eq:constr1-of-kl-worst-case-kernel}-\cref{eq:constr3-of-kl-worst-case-kernel}). Now we prove that the threshold variable $\omega$ can be upper bounded by the empirical DPO loss.
\begin{proposition}\label{prop:bound-on-dual-var}
    $\omega$ satisfies $\omega \leq \sum_{i=1}^n \sfP_n^o(i) l(z_i;\theta)$.
\end{proposition}
\begin{proof}
    Recall the constraint
    \begin{equation*}
        \sum_{i=1}^n q_i \exp{\frac{-\mu-l(z_i;\theta)}{\lambda}-1} = 1.
    \end{equation*}
    By applying Jensen's inequality, we have
    \begin{equation*}
        \exp{\sum_{i=1}^n q_i \bigg(\frac{-\mu-l(z_i;\theta)}{\lambda} \bigg) - 1} \leq 1.
    \end{equation*}
    Some algebra give us
    \begin{equation*}
        \exp{\sum_{i=1}^n q_i \bigg( \frac{-l(z_i;\theta)}{\lambda}\bigg)} \leq \exp{\frac{\mu}{\lambda} + 1}.
    \end{equation*}
    This implies  that $-\mu-\lambda \leq \sum_{i=1}^n q_i l(z_i;\theta)$. Recall that we set $\omega = -\mu-\lambda$, and we have the result.
\end{proof}






\section{Additional Experiment Details}\label{sec:emotion-alignment-appendix}
% \subsection{Small Scale Experiments (with GPT-2): Emotion Alignment}
\paragraph{Reward Model Training:} 
The raw Emotion dataset \citep{saravia-etal-2018-carer} consists of text samples paired with multi-class labels for six different emotion classes (\textit{joy, sadness, love, anger, fear, and surprise}). This dataset was then transformed into a multi-label dataset, referred to as the Emotion Reward Dataset. To create the multi-label dataset, the \textit{surprise} class was excluded due to its limited representation in the original dataset. Following this, up to three random text samples from the raw dataset were concatenated, and their associated labels were merged. \textbf{This pre-processing step ensured that the reward model encountered text samples representing multiple emotions during training}. 

For the reward model, GPT-2 was employed, augmented with a classification head applied to the last token. The model was trained using a sigmoid activation function and binary cross-entropy loss, adhering to the standard multilabel classification framework. Training was conducted over 8 epochs with a batch size of 64, utilizing the Adam optimizer with a learning rate of $5.0 \times 10^{-5} $ and a weight decay of 0.01. The reward model achieved a test accuracy of 84\% and a test ROC-AUC score of 0.99. The emotion-specific scores predicted by this reward model were treated as the rewards for individual emotions.

\paragraph{Supervised Fine-Tuning:}
Before training the WDPO algorithm, it is essential to ensure that the model familiarize with the types of texts present in the dataset. To achieve this, we performed supervised fine-tuning (SFT). We selected GPT-2 as the base language model and trained it to predict the next token based on the text samples in the emotion dataset. The maximum length of each text sample was capped at 68 tokens. The model was trained for 10 epochs with a batch size of 64. The training used the RMSProp optimizer with a learning rate of $5.0 \times 10^{-7} $ following 12 warmup steps. Additionally, a maximum gradient norm of 10 was applied to stabilize the training.

\paragraph{Data Generation:} A preference dataset was created, consisting of a chosen and a rejected completion for each prompt in the dataset. The first four tokens from each text in the emotion dataset were used as prompts. Using the SFT model, two completions were generated for each prompt. These completions were generated with a \texttt{top-k} value of 0, \texttt{top-p} of 1, and up to 64 new tokens. The completions were then evaluated using the reward model, and the chosen and rejected completions were determined based on a combined metric derived from the predicted rewards. In the first plot of \cref{fig:corr-fig-1}, we show the correlation among $r_1$, $r_2$, and $r_1^*(0.1)$. We can see that, as designed, the training preference model is mostly influenced by $r_2$ (\textit{fear}). Recall that $r_1^*(0.1)$ is by design $1/10$ of $r_1$ (\textit{anger}) and $9/10$ of $r_2$ (\textit{fear}). The correlation heatmap verifies that we indeed have an accurate mixture training preference. In the last plot of \cref{fig:corr-fig-2}, we show the correlation among $r_1,r_2,r_3,r_4,r_5,r^*_4$. Recall that $r_4^*$ is constructed under equally-weighted influence of all five standalone reward models.
\begin{figure}[!ht]
    \centering
    % First plot
    \begin{minipage}{0.32\textwidth}
        \centering
        \includegraphics[width=\linewidth]{assets/exp3-corr.pdf}
        \label{fig:exp3-corr}
    \end{minipage}
    \hfill
    % Second plot
    \begin{minipage}{0.32\textwidth}
        \centering
        \includegraphics[width=\linewidth]{assets/exp4-corr.pdf}
        \label{fig:exp4-corr}
    \end{minipage}
    \hfill
    % Third plot
    \begin{minipage}{0.32\textwidth}
        \centering
        \includegraphics[width=\linewidth]{assets/exp8-corr.pdf}
        \label{fig:exp8-corr}
    \end{minipage}
    \caption{Correlation heatmap for $r^*_1(0.1)$, $r^*_1(0.3)$, $r^*_2(0.1)$, respectively.}
    \label{fig:corr-fig-1}
\end{figure}

\begin{figure}[!ht]
    \centering
    % First plot
    \begin{minipage}{0.32\textwidth}
        \centering
        \includegraphics[width=\linewidth]{assets/exp9-corr.pdf}
        % \caption{Correlation heatmap for $r^*_1(0.1)$.}
        \label{fig:exp9-corr}
    \end{minipage}
    \hfill
    % Second plot
    \begin{minipage}{0.32\textwidth}
        \centering
        \includegraphics[width=\linewidth]{assets/exp10-corr.pdf}
        % \caption{Correlation heatmap for $r^*_2(0.1)$.}
        \label{fig:exp10-corr}
    \end{minipage}
    \hfill
    % Third plot
    \begin{minipage}{0.32\textwidth}
        \centering
        \includegraphics[width=\linewidth]{assets/exp19-corr.pdf}
        % \caption{Correlation heatmap for $r^*_2(0.1)$.}
        \label{fig:exp19-corr}
    \end{minipage}
    \caption{Correlation heatmap for $r^*_2(0.3)$, $r^*_2(0.5)$, $r^*_4$, respectively.}
    \label{fig:corr-fig-2}
\end{figure}

\paragraph{WDPO Training:} In WDPO training, one of the main challenges is calculating the gradient penalty of the DPO loss with respect to the input. However, since the input is tokenized as integers, gradient cannot be directly calculated. To address this, gradient is calculated with respect to the first hidden state, which is typically the output of the embedding layer, where gradients are tracked. The model was trained for 40 epochs with an effective batch size of 64. We used RMSProp optimizer, with a learning rate of $5.0 \times 10^{-7} $ following 12 warmup steps. A maximum gradient norm of 10 was applied to ensure stable training. The DPO beta parameter was set to 0.1 for all training runs. Experiments were conducted on a single 40 GB A100 GPU, requiring gradient accumulation over two steps.


\paragraph{KLDPO Training:} The model was trained for 40 epochs with an effective batch size of 64. We used RMSProp optimizer, with a learning rate of $5.0 \times 10^{-7} $ following 12 warmup steps. A maximum gradient norm of 10 was applied to ensure stable training. The DPO beta parameter was set to 0.1 for all training runs. Experiments were conducted on a single 40 GB A100 GPU and gradient was accumulated over two steps to keep training consistent across all algorithms.

\paragraph{More Simulation Results:} 

In this section, we include more simulation results where models are trained on various nominal preference models. In \cref{fig:exp3-all-kl20}, models are trained on the preference labels generated according to $r^*_1(0.1),r^*_1(0.3), r^*_1(0.5),r^*_1(0.7),r^*_1(0.9)$, respectively. Starting from the third plot, we notice that the robustness of our KLDPO and WDPO (along with the baseline robust policy Dr. DPO) reduces. This is because when the training preference model is closer to the testing preference model, the preference shift diminishes. In such cases, non-robust algorithm such as DPO will not be affected much.

In \cref{fig:exp4-all-kl30}, we provide additional simulation results for function class $r^*_2$. The models are trained on the preference labels generated according to $r^*_2(0.1),r^*_2(0.3),r^*_2(0.5),r^*_2(0.7),r^*_2(0.9)$, respectively. Similar to the $r^*_1$ function class, we also observe that when the training preference model is closer to the testing preference model, the performance of non-robust DPO and the robust models, WDPO and KLDPO, is more or less homogeneous. 


We summarize the key implementation and hardware details of text generation tasks in \cref{tab:key-implementations}.
\newpage
\begin{figure*}[!ht]
    \centering 
    \includegraphics[width=\linewidth]{assets/exp3-kl20-all.pdf}
    \caption{Evaluation of DPO, WDPO, KLDPO, and Dr. DPO.}
    \label{fig:exp3-all-kl20}
\end{figure*}
\begin{figure*}[!h]
    \centering 
    \includegraphics[width=\linewidth]{assets/exp4-kl30-all.pdf}
    \caption{Evaluation of DPO, WDPO, KLDPO, and Dr. DPO. }
    \label{fig:exp4-all-kl30}
\end{figure*}
\begin{table}[!ht]
    \centering
    \resizebox{0.58\textwidth}{!}{%
    \begin{tabular}{cc}
        \toprule
        \multicolumn{2}{c}{\textbf{Model}} \\ \midrule
        Pre-training & GPT2  \\
        Hardware & NVIDIA A100 40 GB \\
        Inference Max New Tokens & 64 \\
        Inference top-k & 0 \\
        \midrule
        \multicolumn{2}{c}{\textbf{Dataset}} \\ \midrule
        Task name & \textbf{Emotion Alignment}\\
        Description & Generate text aligned for certain emotions \\
        Prompt length & 4 \\
        Completion length & 64 \\
        Dataset & dair-ai/emotion \citep{saravia-etal-2018-carer} \\
        \midrule
        \multicolumn{2}{c}{\textbf{Reward Training}} \\ \midrule
        Finetuning epochs & 8 \\
        Batch Size & 64 \\
        Optimizer & Adam \\
        Initial learning rate &  $5.0 \times 10^{-5} $\\
        Weight Decay & 0.01 \\
        Learning rate scheduler & Linear \\
        \midrule
        \multicolumn{2}{c}{\textbf{SFT}} \\ \midrule
        Finetuning epochs & 10 \\
        Batch Size & 64 \\
        Optimizer & RMSProp \\
        Initial learning rate & $5.0 \times 10^{-7}$ \\
        Warmup steps & 12 \\
        Learning rate scheduler & Constant with Warmup \\
        Max grad norm & 10.0 \\
        \midrule
        \multicolumn{2}{c}{\textbf{WDPO}} \\ \midrule
        Finetuning epochs & 40 \\
        Batch Size & 64 \\
        Optimizer & RMSProp \\
        Initial learning rate & $5.0 \times 10^{-7}$ \\
        Warmup steps & 12 \\
        Learning rate scheduler & Constant with Warmup \\
        Max grad norm & 10.0 \\
        Gradient accumulation steps & 2 \\
        DPO beta & 0.1 \\
        Gradient Penalty Lambda & 100 \\
        \midrule
        \multicolumn{2}{c}{\textbf{KLDPO}} \\ \midrule
        Finetuning epochs & 40 \\
        Batch Size & 64 \\
        Optimizer & RMSProp \\
        Initial learning rate & $5.0 \times 10^{-7}$ \\
        Warmup steps & 12 \\
        Learning rate scheduler & Constant with Warmup \\
        Max grad norm & 10.0 \\
        Gradient accumulation steps & 2 \\
        DPO beta & 0.1 \\
        Lambda & 1 \\ 
        \bottomrule
    \end{tabular}
    }
    \caption{Key implementations of the experiments.}
    \label{tab:key-implementations}
\end{table}