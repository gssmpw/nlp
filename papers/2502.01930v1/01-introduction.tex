\section{Introduction}\label{sec:introduction}
The alignment of large language models (LLMs) with human values and preferences is a central objective in machine learning, enabling these models to produce outputs that are useful, safe, and aligned with human intent. Since LLMs are trained on vast, diverse datasets using self-supervised learning, an additional alignment phase is often required to refine their behavior based on human feedback. A widely adopted approach for this is Reinforcement Learning from Human Feedback (RLHF)  \citep{christiano2017deep, ziegler2019fine, ouyang2022training}, which involves training a reward model using human preference data and optimizing the LLM  using reinforcement learning approaches, such as proximal policy optimization. More recently, Direct Preference Optimization (DPO) has emerged as an alternative that simplifies the alignment process by directly optimizing model parameters based on human preferences without requiring an explicit reward model. These alignment techniques have played a crucial role in improving the ability of LLMs to generate responses that adhere to human expectations and societal norms, leading to today's powerful chat models \citep{achiam2023gpt, touvron2023llama}. 


Despite the importance of the LLM alignment problem,  RLHF and DPO remain fundamentally challenging and fragile, mainly due to three reasons. $(i)$ \emph{Diversity of human preferences:} Standard RLHF/DPO approaches implicitly assume that human preferences can be accurately captured by a single reward function.  In reality, human preferences are highly diverse, context-dependent, and distributional, making it infeasible to represent them with a one-size-fits-all optimization framework \cite{zhao2024group, durmus2023towards}. Standard preference-learning methods tend to skew toward the preferences represented in the majority of training data, disproportionately penalizing minority opinions and reinforcing biases \cite{chakraborty2024maxmin}.  $(ii)$ \emph{Reward hacking:} The quality of human preference feedback is inherently noisy, ambiguous, and inconsistent, as they are collected from human annotators who may lack domain expertise, exhibit labeling fatigue, or hold conflicting opinions \cite{zhang2024diverging, wu2024towards}, which can often lead to misaligned preference estimation. This issue is exacerbated by reward hacking, where models learn undesirable shortcuts to maximize the estimated reward function, generating responses that appear aligned but deviate from genuine human intent \cite{amodei2016concrete,skalse2022defining,eisenstein2023helping}. $(iii)$ \emph{Distribution shift:} Alignment algorithms use static preference datasets for training, collected under controlled conditions. However, the preferences of real-world users can often be out-of-distribution from that of the training data, depending on the geographical region, demography,  linguistic patterns, and emerging social trends, among many others. A model aligned using a specific fixed dataset may fail catastrophically when deployed to users whose preference distribution does not match that of the training data \cite{casper2023open,levine2023baseline,kirk2024understanding}.
\begin{figure*}[!ht]
    \centering 
    \includegraphics[width=\linewidth]{assets/main-diagram.pdf}
    \caption{Suppose the population that generates the training preference labels has a higher presence of preference model 1 (P1 preference), the trained non-robust RLHF/DPO model tends to generate completion more aligned with Completion 1 (C1) when it sees a similar prompt. It is possible that the model is deployed to a population that has the second preference model, which dislikes Completion 1 and favors Completion 2, resulting in low reward in testing. A distributionally robust DPO model (our WDPO and KLDPO) will consider an uncertainty set of preference models and will offer a robust performance across the preference models in this uncertainty set.}
    \label{fig:main-diagram}
\end{figure*}

\begin{figure*}[!ht]
    \centering
    % First plot
    \begin{minipage}{0.32\textwidth}
        \centering
        \includegraphics[width=\linewidth]{assets/barplot-exp4.pdf}
    \end{minipage}
    \hfill
    % Second plot
    \begin{minipage}{0.32\textwidth}
        \centering
        \includegraphics[width=\linewidth]{assets/emotion-multiclass-exp14-no-both.pdf}
    \end{minipage}
    \hfill
    % Third plot
    \begin{minipage}{0.32\textwidth}
        \centering
        \includegraphics[width=\linewidth]{assets/exp19-corr.pdf}
    \end{minipage}
    \caption{\textit{Left plot:} We use two reward models, \textit{anger} and \textit{fear}, which are trained on the Emotion dataset \citep{saravia-etal-2018-carer}, and obtain the training preference data using the weighted sum of these rewards. We introduce the preference shift from ``Train" to ``Test" by changing the weight of \textit{anger} and \textit{fear} rewards. \textit{Middle plot:} Here, models are trained on the \textit{mixed} preference model which has roughly equal weight of five emotions rewards. The models are then evaluated on singular emotion reward: \textit{joy, sadness} and \textit{anger}. \textit{Right plot:} This  shows the correlation between the \textit{mixed} preference model and the five \textit{standalone} preference models. The left and middle plots illustrate the lack of robustness of the standard DPO algorithm and the superior performance of our algorithms in mitigating the preference distribution shift (see \cref{sec:experiments} for details)
    }
    \label{fig:secondary-diagram}
\end{figure*}
In this paper, we address the fragility of the LLM alignment using DPO, with a particular focus on the challenges arising from the \emph{prefence distribution shift}.  DPO reduces the alignment problem to a supervised learning problem. It is known that the performance of supervised learning algorithms degrades significantly in the out-of-distribution setting \cite{taori2020measuring,koh2021wilds}, which is exacerbated due to the realistic distribution shift scenarios arising in the LLM deployment. Distributionally robust optimization/learning framework has been recently used to address the issue of distribution shift in various settings \cite{duchi2018learning, kuhn2019wasserstein, chen2020distributionally}. This framework considers an uncertainty set of data distributions around a nominal distribution (typically the training data distribution), and solves a minimax optimization problem to minimize the expected loss, where the expectation is taken w.r.t. the distribution in the uncertainty set that maximizes the loss. The distributionally robust learning approach has been successfully applied,  with theoretical guarantees and scalable algorithms,  in supervised learning \cite{chen2018robust, namkoong2016stochastic, levy2020large}, multi-armed bandits \cite{si2020distributionally, yang2023distributionally} and reinforcement learning \cite{wang22policygradient,panaganti-rfqi, zhou2024natural}. This motivates us to address the following questions:
\begin{quote}
    \textit{Can we alleviate the problem of distribution shift in DPO-based LLM alignment using distributionally robust learning approaches? What kind of theoretical guarantees can be provided for such an approach? How do we develop tractable gradient-descent style algorithms? How do we demonstrate the performance improvement of the LLM alignment algorithms achieved through such an approach?}
\end{quote}

In this paper, we provide affirmative answers to these questions. We summarize our main contributions below. 
\begin{enumerate}
    \item First, we rigorously formulate the distributionally robust DPO framework and establish its theoretical guarantees. We show that, when the policy class is log-linear, the estimation error of the distributionally robust policy parameter converges at the rate of $O(n^{-1/4})$, for both WDPO and KLDPO.
    \item We develop tractable approximate formulations for the otherwise challenging min-max loss functions of WDPO and KLDPO, which can be minimized using gradient descent approaches. We leverage this to design practical algorithms that can be directly integrated with the existing LLM alignment pipeline.  
    \item Through extensive empirical experiments, we demonstrate the superior performance of our distributionally robust DPO algorithms in mitigating the preference distribution shift problem in LLM alignment.   
\end{enumerate}








