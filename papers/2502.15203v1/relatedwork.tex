\section{Related Works}
\subsection{Diffusion-based Text-to-Image Generation.}
Denoising diffusion models~\cite{ho2020denoising,song2020denoising} and their various extensions~\cite{rombach2022high,podell2023sdxl} have made remarkable progress in the field of text-to-image (T2I) generation. 
By leveraging powerful text encoders such as CLIP~\cite{radford2021learning}, it has become possible to synthesize high-quality images conditioned on textual prompts. 
Recently, methods specialized for high-resolution generation~\cite{rombach2022high}, classifier-free guidance for enhanced controllability~\cite{ho2022classifier}, and improved models adopting large-scale diffusion backbones~\cite{podell2023sdxl} have also been proposed.

\subsection{Image Editing with Diffusion Models.}
In an effort to harness the outstanding image-generation capabilities of diffusion models, various editing techniques have been proposed. 
For example, MasaCtrl~\cite{cao2023masactrl} demonstrates that freezing the keys and values in the self-attention layers of the denoising network~\cite{ronneberger2015u} can more faithfully preserve the original appearance of an image. 
Subsequently, methods that manipulate self-attention layers to better maintain the shape of a concept~\cite{cao2023masactrl,nam2024dreammatcher,alaluf2024cross} have also been introduced. 
Such approaches enable localized editing and global style transfer without the need to retrain the entire model, but their performance on complex scenes with multiple objects has yet to be sufficiently verified.

\subsection{Multi-Concept Personalization.}
Generating images that incorporate multiple user-specific concepts is an important yet challenging task. 
Textual Inversion~\cite{gal2022image} introduced a technique that optimizes text embeddings to represent novel concepts, and subsequent research has investigated approaches such as full~\cite{ruiz2023dreambooth} or partially fine-tuning the model parameters~\cite{kumari2023multi} or inserting rank-one edits~\cite{tewel2023key}. 
Although training multiple concepts simultaneously may lead to concept blending or partial loss, recent studies aim to mitigate these issues via weight merging~\cite{gu2024mix}. Concept Weaver~\cite{kwon2024concept} and DreamMatcher~\cite{nam2024dreammatcher} offer a training-free approach that combines multiple concepts during sampling, though certain methods may still require inversion or fine-tuning during inference. 
By proposing a novel approach that minimizes additional optimization steps while flexibly generating high-quality images of multiple concepts, our work extends the context of prior research.