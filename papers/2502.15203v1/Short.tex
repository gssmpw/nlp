\documentclass[conference]{IEEEtran}
\IEEEoverridecommandlockouts
% The preceding line is only needed to identify funding in the first footnote. If that is unneeded, please comment it out.
\usepackage{cite}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{algorithmic}
\usepackage{graphicx}
\usepackage{textcomp}
\usepackage{xcolor}
\usepackage{placeins}
\usepackage{float}

\usepackage{enumitem}
\usepackage{stfloats} % For handling two-column floats
\usepackage{afterpage} % For flushing floats to specific pages
\usepackage{booktabs}
\usepackage[utf8]{inputenc}

\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em
    T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}
\begin{document}

\title{FlipConcept: Tuning-Free Multi-Concept Personalization for Text-to-Image Generation \\
}

\author{\IEEEauthorblockN{Young-Beom Woo}
\IEEEauthorblockA{\textit{Dept. of Artificial Intelligence} \\
\textit{Korea University}\\
Seoul, Republic of Korea \\
woo\_y\_b@korea.ac.kr}
\and
\IEEEauthorblockN{Sun-Eung Kim}
\IEEEauthorblockA{\textit{Dept. of Artificial Intelligence} \\
\textit{Korea University}\\
Seoul, Republic of Korea \\
se\_kim@korea.ac.kr}
}

\maketitle
\begin{figure*}[t]
% \begin{center}
\centering
% \fbox{\rule{0pt}{2in} \rule{1\linewidth}{0pt}}
\includegraphics[width=0.8\linewidth]{main.png}
% \end{center}
  \caption{\textbf{Comparison of multi-subject personalization methods.} The first two columns illustrate the learned personalized concepts (the cat and the woman).
From left to right, we show the final generated results of Custom Diffusion, Perfusion, Mix-of-Show, and FlipConcept (Ours), all prompted with:
\textit{``A photo of a cat and a woman hugging each other, lighthouse background.''}
While existing approaches often cause some concepts to disappear, mix appearances, or distort structural details when dealing with multiple subjects (e.g., Custom Diffusion loses structural accuracy, and Perfusion/Mix-of-Show frequently fail to preserve all concepts), our method preserves each conceptâ€™s features and the overall scene context, resulting in coherent and high-quality outputs.}
\label{fig:short}
\end{figure*}

\begin{abstract}

Recently, methods that integrate multiple personalized concepts into a single image have garnered significant attention in the field of text-to-image (T2I) generation.
However, existing methods experience performance degradation in complex scenes with multiple objects due to distortions in non-personalized regions. To address this issue, we propose FlipConcept, a novel approach that seamlessly integrates multiple personalized concepts into a single image without requiring additional tuning. We introduce guided appearance attention to accurately mimic the appearance of a personalized concept as intended. Additionally, we introduce mask-guided noise mixing to protect non-personalized regions during editing. Lastly, we apply background dilution to minimize attribute leakage, which is the undesired blending of personalized concept attributes with other objects in the image. In our experiments, we demonstrate that the proposed method, despite not requiring tuning, outperforms existing models in both single and multiple personalized concept inference.

\end{abstract}

\begin{IEEEkeywords}
Text-to-Image, Diffusion, Multi-Concept Personalization, Tuning-Free, FlipConcept
\end{IEEEkeywords}

\section{Introduction}

Text-to-image (T2I) generation has made significant progress in creating images based on user-provided personalized concepts. At the same time, combining multiple concepts into a single image remains challenging, as it requires harmoniously arranging multiple concepts in line with the input text's intent while preserving their unique quality and identity. Custom Diffusion\cite{kumari2023multi}, a representative approach in personalization, fine-tunes the cross-attention layers of a pre-trained diffusion model using multiple personalized images to effectively adapt to new concepts. Nevertheless, such tuning-based methods \cite{kumari2023multi, gal2022image, ruiz2023dreambooth, tewel2023key, gu2024mix, kwon2024concept, yang2007reconstruction, hwang2006full, lee2001automatic} are prone to overfitting and face significant challenges in generalizing when tasked with combining multiple concepts into a single image. To resolve these problems, recent research has increasingly focused on exploring tuning-free approaches\cite{alaluf2024cross, cao2023masactrl, nam2024dreammatcher}.

However, despite their remarkable achievements, as shown in Figure 1, existing methods still frequently fail to manage interactions among multiple objects effectively.
In particular, in complex scenarios, such as balancing multiple objects within an image, these methods often fail to maintain the relationships between concepts or handle non-personalized regions appropriately, leading to unintended mixing, distortion, or modification.


To address these challenges, we propose a novel method called \textbf{FlipConcept}, which consistently generates images containing multiple personalized concepts using just a single reference image per concept, without requiring additional fine-tuning.  
Our framework operates in two distinct stages.  

In the first stage, we prepare the input data for the model.  
Specifically, we generate a background image and extract masks from it.  
Then, we perform Edit-Friendly DDPM inversion\cite{huberman2024edit} on the prepared background image and the personalized concept images.  
This process yields edit-friendly latent representations and masks with accurately defined regions.  
  
In the second stage, we leverage the latents and masks obtained in the previous step to generate an image that seamlessly integrates the personalized concepts with the background. 
To accomplish this effectively, we introduce three core techniques. 
During the image generation process, \textbf{Guided Appearance Attention} reconstructs the keys and values of personalized concept images by referencing those from other images, thereby amplifying the influence of personalized concepts on overall appearance. 
\textbf{Mask-Guided Noise Mixing} then selectively integrates noise predictions based on extracted masks, ensuring that only regions containing the personalized concepts are modified while non-personalized areas remain unaffected. 
Finally, \textbf{Background Dilution} applies an extended object mask to reduce the influence of regions outside the designated areas, mitigating concept leakage during self-attention operations and maintaining high image quality by minimizing interference with non-concept regions.

FlipConcept provides an efficient and modular approach for multi-concept personalization in diffusion models, enabling flexible addition or modification of personalized concepts without requiring additional tuning. It addresses the overfitting issues commonly observed in tuning-based approaches and avoids the computational overhead of retraining. Comprehensive experimental findings verify that our method effectively generates personalized images in complex scenarios while preserving non-edited areas. For example, in scenarios involving multiple characters and objects, our method ensures the coherence of each concept and maintains the integrity of the background. Furthermore, the generated images achieve high performance in CLIP \cite{radford2021learning} score evaluations, highlighting the quality and relevance of the generated content.

To summarize, the key contributions of our research are as follows:
\begin{itemize}
    \item Existing research has revealed limitations in generating multiple user-personalized concepts within a single image, particularly concerning concept leakage and difficulties in representing relationships between objects.

    \item To overcome these issues, we propose an innovative method called FlipConcept, which consistently integrates multiple personalized concepts using only a single reference image per concept, without requiring additional fine-tuning. By introducing guided appearance attention, mask-guided noise mixing, and background dilution, our method effectively addresses the challenges of multi-concept personalization, generating high-quality images while minimizing concept leakage.

    \item Our experiments demonstrate that FlipConcept maintains the coherence of each concept and preserves the background without distortion or unintended alterations, even in complex scenarios, outperforming existing methods. 
    Notably, our method achieves superior results in CLIP score evaluations, highlighting the quality and relevance of the generated content.
\end{itemize}

\section{Related Works}

\subsection{Diffusion-based Text-to-Image Generation.}
Denoising diffusion models~\cite{ho2020denoising,song2020denoising} and their various extensions~\cite{rombach2022high,podell2023sdxl} have made remarkable progress in the field of text-to-image (T2I) generation. 
By leveraging powerful text encoders such as CLIP~\cite{radford2021learning}, it has become possible to synthesize high-quality images conditioned on textual prompts. 
Recently, methods specialized for high-resolution generation~\cite{rombach2022high}, classifier-free guidance for enhanced controllability~\cite{ho2022classifier}, and improved models adopting large-scale diffusion backbones~\cite{podell2023sdxl} have also been proposed.

\subsection{Image Editing with Diffusion Models.}
In an effort to harness the outstanding image-generation capabilities of diffusion models, various editing techniques have been proposed. 
For example, MasaCtrl~\cite{cao2023masactrl} demonstrates that freezing the keys and values in the self-attention layers of the denoising network~\cite{ronneberger2015u} can more faithfully preserve the original appearance of an image. 
Subsequently, methods that manipulate self-attention layers to better maintain the shape of a concept~\cite{cao2023masactrl,nam2024dreammatcher,alaluf2024cross} have also been introduced. 
Such approaches enable localized editing and global style transfer without the need to retrain the entire model, but their performance on complex scenes with multiple objects has yet to be sufficiently verified.

\subsection{Multi-Concept Personalization.}
Generating images that incorporate multiple user-specific concepts is an important yet challenging task. 
Textual Inversion~\cite{gal2022image} introduced a technique that optimizes text embeddings to represent novel concepts, and subsequent research has investigated approaches such as full~\cite{ruiz2023dreambooth} or partially fine-tuning the model parameters~\cite{kumari2023multi} or inserting rank-one edits~\cite{tewel2023key}. 
Although training multiple concepts simultaneously may lead to concept blending or partial loss, recent studies aim to mitigate these issues via weight merging~\cite{gu2024mix}. Concept Weaver~\cite{kwon2024concept} and DreamMatcher~\cite{nam2024dreammatcher} offer a training-free approach that combines multiple concepts during sampling, though certain methods may still require inversion or fine-tuning during inference. 
By proposing a novel approach that minimizes additional optimization steps while flexibly generating high-quality images of multiple concepts, our work extends the context of prior research.

\section{Preliminary}
\subsection{Latent Diffusion Models}

In this study, we implement a text-to-image (T2I) generation model based on the Latent Diffusion Model (LDM)\cite{rombach2022high}.
It operates in a compressed latent space instead of using high-dimensional images, thereby achieving both computational efficiency and high-resolution image generation.
An input image is converted into a latent vector via an autoencoder and is subsequently reconstructed into the original image through a decoder.

To incorporate text conditions, we transform the text prompt into a latent vector using a pretrained CLIP text encoder \cite{radford2021learning}, and use it as a condition \(c\).
This ensures that the generated image aligns with the semantic meaning of the text.
The LDM minimizes the following loss function:

\[
L = \mathbb{E}_{x_0, \epsilon \sim \mathcal{N}(0,I)}\Bigl[\bigl\|\epsilon - \epsilon_\theta(x_t, t, c)\bigr\|_2^2\Bigr],
\]


\noindent where \(x_t\) is the noise-added sample at the \(t\)-th time step, and \(\epsilon_\theta\) is the diffusion model \cite{ho2020denoising} with parameters \(\theta\). By using this formulation, one can construct a text-based T2I diffusion model.

\subsection{Self-Attention in Stable Diffusion}

In this study, we adopt Stable Diffusion (SD)\cite{rombach2022high}, which is one of the LDM-based models.
Built upon a denoising U-Net\cite{ronneberger2015u, ding2024freecustom}, SD includes an encoder-decoder architecture with seven blocks and sixteen layers, featuring residual blocks, self-attention, and cross-attention modules that collectively enable efficient image restoration and generation in the latent space.
Among these modules, the self-attention mechanism contributes to generating the layout and specific details of the content.

At each time step \(t\), the noisy latent code \(z_t\) is provided as input, and query (Q), key (K), and value (V) vectors are extracted from the intermediate layer representation \(\phi_l(z_t)\).
Trainable linear weight matrices \(W_Q, W_K, W_V\) are employed in this process.

\[
Q = W_Q\bigl(\phi_l(z_t)\bigr), \quad K = W_K\bigl(\phi_l(z_t)\bigr), \quad V = W_V\bigl(\phi_l(z_t)\bigr).
\]

Each component \(q_{(i,j)}\) of the query vector \(Q\) performs an inner product operation with all key vectors to compute attention scores, which are then normalized by a softmax function to assign higher weights to more important positions.
During score computation, the results are scaled by \(\sqrt{d_k}\), the square root of the query-key dimensionality, to counteract the tendency of inner product values growing larger with increasing dimensions, thereby improving training stability and convergence speed.
Based on the normalized scores, the \(V\) vectors are weighted and summed to form the updated feature \(\Delta \phi(i,j)\).
The overall self-attention computation can be summarized as follows:

\[
A(i,j) = \mathrm{softmax}\Bigl(\frac{q_{(i,j)} \cdot K^T}{\sqrt{d_k}}\Bigr), \quad
\Delta \phi(i,j) = A(i,j) \cdot V.
\]

Through this process, the model effectively emphasizes the image regions associated with the text semantics.
Further analysis \cite{alaluf2024cross, cao2023masactrl, nam2024dreammatcher} indicates that self-attention can be divided into a structure path and an appearance path, where the structure path highlights object shapes and layouts, and the appearance path emphasizes color and texture.

\section{Method}

\begin{figure*}[htb!]
\centering
\includegraphics[width=0.8\textwidth]{Testfigure4.png}
\caption{\textbf{Overview of FlipConcept.}
Our framework leverages Grounded SAM \cite{ren2024grounded} and DDPM inversion \cite{huberman2024edit} to extract masks and latents of both background and personalized concepts, which are then combined through adaptive concept blending including Guided Appearance Attention, Mask-Guided Noise Mixing, and Background Dilution.} 
\label{fig}
\end{figure*}

In this section, we propose an overall image generation framework called \textbf{FlipConcept}, which can naturally integrate multiple user-defined personal concepts into a single image using a single model, without any additional tuning process. 
This framework organically combines various personal concepts into a single scene to ultimately produce a high-quality outcome that reflects the userâ€™s intention. 
Through this approach, it is possible to go beyond the limitations of object editing in existing methods and reflect more complex and rich scenarios or contexts in the image, thereby expanding the scope of the modelâ€™s applicability by simultaneously handling multiple concepts with a single model.

Inspired by a step-by-step image generation approach\cite{kwon2024concept}, this framework goes through two main processes in total. 
In the first stage, we create latent representations and masks for image generation. 
Initially, we obtain a background image that aligns with the intended purpose.
This image is then fed into a text-segmentation model to extract both an object mask, which defines the target object's region, and a background region mask.
Next, we transform the prepared background image and the personal concept images into latent representations and then replicate the background latent representation.
In the second stage, we feed all the latent representations and masks obtained from the previous steps into the diffusion model to generate the background image $I_{\text{out}}$ that naturally reflects the personal concepts. 
Fig.2 illustrates the overall structure of the proposed framework, and the subsequent sections will provide more detailed explanations of these key steps.

\subsection{Input Preparation}

\subsubsection{Background Image and Mask Generation}

Most existing approaches \cite{kumari2023multi, gal2022image, tewel2023key, gu2024mix} tend to randomly generate the background regions, excluding those defined by the personal concepts.
In contrast, our approach produces candidate background images and then allows the user to directly choose among them.
To achieve this, we utilize Stable Diffusion XL\cite{podell2023sdxl} model to generate a variety of background images, from which the user selects the desired scene.
At this stage, the chosen background image must clearly include the specific object that will be modified during the subsequent editing process.

Once the background image is prepared, masks for the regions of the target objects and the background excluding those objects are extracted. In this process, we use the Grounded SAM package\cite{ren2024grounded}, which combines Grounding DINO\cite{liu2025grounding} for open-set detection and Segment Anything Model (SAM) \cite{kirillov2023segment} for promptable segmentation. Specifically, Grounding DINO generates bounding boxes for the objects desired by the user based on the text prompt, which are then used as conditions to guide SAM for fine-grained object segmentation. The generated masks represent the location and extent of specific entities mentioned in the text prompt, precisely defining the object regions.

After generating the masks for each object, the background mask is defined as the region outside the combined object areas. These masks will later guide the generation process to ensure the proper object areas are manipulated.

\subsubsection{Image Inversion}

Once the background image and the personal concept images are determined, we first extract a latent representation from each of them.
In conventional approaches, DDIM Inversion based on Denoising Diffusion Implicit Models (DDIM) \cite{song2020denoising} was typically used to backtrack the generation process of a diffusion model and obtain the latent code of a given image.
However, this method required many steps, leading to reduced reconstruction quality and the loss of fine details.
To address this issue, our study utilizes Edit-Friendly DDPM Inversion \cite{huberman2024edit} to extract a noise map that directly reflects the original image, thereby preserving structural features and details while providing a foundation for various editing operations.

Edit-Friendly DDPM Inversion forms a sequence via a probabilistic path directly defined from the original image $x_0$ (e.g., $I_{\text{back}}$, $I_1$, and $I_2$ in Fig.2).
To achieve this, at each time step $t$ ($1 \le t \le T$), we independently sample noise $\tilde{\epsilon}_t \sim \mathcal{N}(0, I)$ and construct $x_t$ as follows:
\[
x_t = \sqrt{\bar{\alpha}_t} \cdot x_0 + \sqrt{1 - \bar{\alpha}_t} \cdot \tilde{\epsilon}_t,
\]

\noindent where ${\bar{\alpha}_t}$ is the cumulative noise schedule parameter used in the diffusion process.
Because $\tilde{\epsilon}_t$ is independently sampled at each time step, the noise vectors between adjacent time steps exhibit a negative correlation.
Unlike traditional DDPM sampling \cite{ho2020denoising}, this causes $x_t$ and $x_{t-1}$ to diverge more on average, thereby increasing the variance of the extracted noise map $(z_T, \dots, z_1)$ and more strongly reflecting the structure of the original image.

We then invert the auxiliary sequence $(x_1, \dots, x_T)$ obtained in this manner to extract the noise map.
At each time step $t$, given $\hat{\mu}_t(x_t)$ and $\sigma_t$, $z_t$ is computed as follows:
\[
z_t = \frac{x_{t-1} - \hat{\mu}_t(x_t)}{\sigma_t}.
\]

The edit-friendly noise map generated through this process does more than simply backtrack the modelâ€™s generation path; it provides a latent representation that more faithfully encodes the input image.
Consequently, it allows us to preserve the original image as much as possible while offering a flexible latent code for various editing tasks.
These latent representations play a crucial role in future editing steps, enabling the natural integration of new objects or concepts while maintaining structural features and fine details.

Among the acquired latent representations, the background latent representation $z_T^{\text{back}}$ extracted from the background image is replicated to be one more than the number of personal concepts (e.g., \(z_T^{\text{per}\_1}\)  and \(z_T^{\text{per}\_2}\) in Fig.2) to be edited.
One of these is initialized as \(z_T^{\text{out}}\) to generate the final output image \(I_{\text{out}}\), 
while the remaining copies are designated as \(z_T^{\text{ref}\_1}, z_T^{\text{ref}\_2}, \dots\) and treated as reference latents.
These reference latents serve as conditions for reliably embedding specific objects or concepts in subsequent editing steps.
Finally, once all latent representations and the previously generated masks are ready, we feed them into the Stable Diffusion model\cite{rombach2022high}.
At this stage, the background latent representation, reference latents, and masks organically combine to reconstruct a high-quality background image $I_{\text{out}}$ that reflects the userâ€™s intended object.
Through this process, multiple personal concepts can be seamlessly integrated into a single scene, enabling image generation that faithfully captures the intended meaning of the prompt.

\subsection{Adaptive Concept Blending}

Once all the preliminary steps have been completed, we feed the prepared inputs into the diffusion model.
At this stage, we introduce three techniques \textbf{Guided Appearance Attention}, \textbf{Mask Guided Noise Mixing}, and \textbf{Background Dilution} to address various issues that arise in multi-concept blending.
These methods maintain the structural stability of the model while enabling the precise transfer of appearance information. 
As a result, they optimize the fusion of complex concepts and background processing to maximize final image quality.

\textbf{Guided Appearance Attention} emphasizes appearance information by sharing the key and value of the personal concept image in the self-attention layer and reconstructing the key and value of the reference image.
Specifically, the key of the reference image is replaced with the key from the personal concept image, and the value adopts a value guidance inspired by classifier-free guidance\cite{ho2022classifier}, thereby enhancing both text-image alignment and overall visual quality. Value guidance is defined as follows:
\[
V_t^{\text{gui}} = V_t^{\text{per}} + \alpha \cdot \bigl(V_t^{\text{per}} - V_t^{\text{ref}}\bigr),
\]

\noindent where \(V_t^{\text{per}}\) represents the value of the personal concept image at time step $t$, \(V_t^{\text{ref}}\) is the value of the reference image at time step $t$, and \(\alpha\) is a hyperparameter that controls the strength of appearance emphasis.
In this paper, we set \(\alpha=0.15\),\ which was empirically found to balance visual quality and alignment.
By adjusting this hyperparameter, one can refine the emphasis on appearance features based on specific use cases or preferences.

The reconstructed key and value of the reference image then pass through the query and self-attention operations\cite{cao2023masactrl, alaluf2024cross}, accurately projecting the appearance information of the personal concept onto the structural layout of the reference image.
As a result, it not only achieves consistency with the text prompt but also precisely reflects visual attributes such as color and texture.

\textbf{Mask Guided Noise Mixing} is a technique that blends the noise predicted by a diffusion model at the mask unit, ensuring that each personal concept is only applied within its designated region.
Namely, the object mask (\(M_i\)) corresponds to the area of the concept to be edited, while the background mask (\(M_B\)) covers the remaining preserved region, and the noise is combined using a hadamard product (\(\odot\)) approach.
The process can be formulated as follows:

\[
\epsilon_\theta^{\text{gui}} = \epsilon_\theta(z_t^{\text{back}}) \odot M_B + \sum_{i=1}^n \epsilon_{\theta}(z_{t}^{\mathit{ref\_i}}) \odot M_i,
\]

\noindent where \(\epsilon_\theta^{\text{gui}}\) denotes the guided noise, \(\epsilon_\theta\bigl(z_t^{\text{back}}\bigr)\) represents the noise for the background, and \(M_B\) is the background mask.
Additionally, $\epsilon_{\theta}(z_{t}^{\mathit{ref\_i}})$ and $M_i$ correspond to the \(i\)-th reference noise and its mask, respectively.
Simultaneously, the noise corresponding to the \(i\)-th reference object is also updated by re-synthesizing the area outside the object's region with the background noise, using the \(i\)-th object mask. 
In addition, we utilize a swap guidance mechanism \cite{alaluf2024cross} to effectively steer the denoising process toward the target appearance distribution, thereby enhancing image plausibility and reducing distortions.
By following this process, one can clearly separate object and background regions, while naturally fusing various concepts from the text prompt into a single image.

\textbf{Background Dilution} is a technique that reduces the influence of the background in the reference latent generated during the diffusion process by leveraging object masks.
Specifically, after generating the reference latent (but before proceeding to the next step), a minimal rectangular mask is formed based on the object mask extracted in the previous stage. 
This mask is then used to mix the background latent with the reference latent, while multiplying the background latent by a weight \(\beta\) to control the blend ratio and dilute the background's influence.

The formula for updating the latent is as follows:

\[
z_t^{\text{ref}} \leftarrow z_t^{\text{back}} \odot \beta \bigl(1 - M_E\bigr) + z_t^{\text{ref}} \odot M_E,
\]

\noindent where \(z_t^{\text{back}}\) represents the background latent, \(z_t^{\text{ref}}\) denotes the reference latent, and \(\beta\) is a coefficient that adjusts their relative influence.
Empirically, \(\beta=0.8\) was found to yield the best results.
Moreover, \(M_E\) is a rectangular mask expanded from the object mask, precisely covering only the region around the target object.
This approach clearly separates the background and the object while representing only the minimal area around the object, allowing personal concepts to be reflected more accurately.

\section{Experiments}

\begin{figure*}[htb!]
\centering
\includegraphics[width=0.8\textwidth]{Qualitative.png}
\caption{\textbf{Qualitative Evaluation of Multi-Concept Personalization.} We compare our method FlipConcept against representative approaches (Custom Diffusion\cite{kumari2023multi}, Perfusion\cite{tewel2023key}, Mix-of-Show\cite{gu2024mix}) using prompts that incorporate multiple personalized concepts (e.g., cat and dog) under various backgrounds.
Our method consistently prevents visual mixing or structural collapse, producing coherent results in complex scenarios.}
\label{fig3}
\end{figure*}

In this section, we present the experimental results that verify the performance of the proposed method from various perspectives. By conducting a comprehensive evaluation based on both quantitative and qualitative metrics, comparisons with representative models, and an ablation study, we demonstrate that our approach outperforms existing methods. For this purpose, we utilized the Stable Diffusion V2.1 base model\cite{rombach2022high}, and all images were generated at a resolution of 512 Ã— 512 using two NVIDIA A40 GPUs. Nevertheless, for some recently introduced models such as Mix-of-Show\cite{gu2024mix}, we found that their proposed approaches could not be straightforwardly adapted to Stable Diffusion V2.1, leading us to rely on the official source code and configurations provided in their respective papers.

\subsection{Experimental Settings}

We utilize the Custom Concept 101 dataset \cite{kumari2023multi} as well as images generated via Stable Diffusion\cite{rombach2022high}.
The Custom Concept 101 dataset comprises multiple scenarios built from a variety of conceptual categories such as animals, humans, natural landscapes, and objects, and we prepared five sets of quantitative evaluations based on it.
To create the prompts for these experimental sets, we automatically generate sentences that capture inter-object relationships using ChatGPT\cite{achiam2023gpt}.
This process allows us to rigorously assess the robustness and generality of the proposed method.
For evaluation, we adopt two CLIP-based metrics\cite{radford2021learning} as named $T_\text{CLIP}$ and $I_\text{CLIP}$, following prior studies\cite{ruiz2023dreambooth,kumari2023multi,tewel2023key,gal2022image,gu2024mix,kwon2024concept,nam2024dreammatcher}. $T_\text{CLIP}$ (Text CLIP) quantifies prompt fidelity by calculating the cosine similarity between the CLIP embedding of the entire generated image and that of the text prompt.
On the other hand, $I_\text{CLIP}$ (Image CLIP) measures concept fidelity by computing the cosine similarity between the text embeddings of a given concept and the designated region in the image that represents that concept.
Through this approach, we can evaluate from multiple angles whether the model accurately reflects the content required by the text when generating multiple concepts simultaneously.
For a fair comparison, all experiments were conducted under the same conditions and hyperparameters, with measures taken to minimize any variability arising from randomness.
Our comparisons encompass state-of-the-art methods including Custom Diffusion\cite{kumari2023multi}, Perfusion\cite{tewel2023key}, and Mix-of-Show\cite{gu2024mix}, and the results confirm that our proposed method also excels in practical use cases.
Additionally, we conduct an ablation study on each component to quantitatively analyze how much each module contributes to the overall performance.

\begin{table}[htbp]
\caption{\textbf{Quantitative comparison of multi-personal concept generation.} Our method (d) outperforms the representative approaches in both $T_\text{CLIP}$ and $I_\text{CLIP}$, indicating superior text and concept alignment.}
\centering
\begin{tabular}{clcc}
\toprule
& \textbf{Method} & $T_\text{CLIP} \uparrow$ & $I_\text{CLIP} \uparrow$ \\
\midrule
(a) & Custom Diffusion \cite{kumari2023multi}          & 0.3480 & 0.7131 \\
(b) & Perfusion \cite{tewel2023key}                & 0.3044 & 0.6765 \\
(c) & Mix-of-show \cite{gu2024mix}      & 0.3206 & 0.7099 \\
(d) & \textbf{FlipConcept (Ours)}                           & \textbf{0.3810} & \textbf{0.7382} \\
\bottomrule
\end{tabular}
\label{tab:component_comparison}
\end{table}

\begin{table}[htbp]
\caption{\textbf{Time comparison of multi-personal concept generation.} Quantitative analysis shows that FlipConcept achieves the best performance in terms of preprocessing (such as fine-tuning or retraining) and inference time.}
\centering
\resizebox{\linewidth}{!}{%
\begin{tabular}{clccc}
\toprule
& \textbf{Method} & \textbf{Preprocessing} & \textbf{Inference} & \textbf{Total} \\
\midrule
(a) & Custom Diffusion \cite{kumari2023multi} & 814s & 13s & 827s \\
(b) & Perfusion \cite{tewel2023key} & 7115s & 7s & 7122s \\
(c) & Mix-of-Show \cite{gu2024mix} & 1509s & 16s & 1525s \\
(d) & \textbf{FlipConcept (Ours)} & \textbf{0} & \textbf{124s} & \textbf{124s} \\
\bottomrule
\end{tabular}%
}
\label{tab:time_comparison}
\end{table}

\subsection{Generation Results}

To evaluate the effectiveness of the proposed method, we conducted both quantitative and qualitative comparisons against representative approaches, including Custom Diffusion \cite{kumari2023multi}, Perfusion \cite{tewel2023key}, and Mix-of-Show \cite{gu2024mix}.  
We generated and compared approximately 50 images for each method, and the results confirm that our method consistently outperforms existing approaches in producing coherent, visually appealing, and structurally accurate multi-concept images.

Table~\ref{tab:component_comparison} presents the quantitative performance results based on CLIP-based metrics.  
Our method achieves the highest scores in both $T_\text{CLIP}$ and $I_\text{CLIP}$, indicating that the generated images exhibit a high degree of alignment with text prompts and faithfully reflect concept-specific details.  
By comparison, the representative methods show a significant decline in performance as the number of concepts increases.  
For instance, while Custom Diffusion is capable of handling multiple concepts, it suffers from a lack of structural consistency and issues with concept blending, resulting in lower $T_\text{CLIP}$ and $I_\text{CLIP}$ scores compared to our method.

Figure~\ref{fig3} illustrates qualitative comparisons of multi-concept generation results across various prompts.  
Our method demonstrates exceptional capability in handling complex interactions among multiple personalized concepts, such as "A photo of a guitar placed next to a cup, bar background."  
In such scenarios, representative methods often exhibit problems like blending appearance details or losing structural integrity.  
For example, Perfusion struggles to maintain overall image structure, while Mix-of-Show frequently merges adjacent concepts into ambiguous forms.  
In contrast, our method effectively separates and integrates individual concepts, maintaining high fidelity to the text prompts while delivering visually compelling results.

Lastly, Table~\ref{tab:time_comparison} presents a comparison of our method with existing approaches in terms of time efficiency\cite{ding2024freecustom}.
Because our approach requires no tuning, it demonstrates significantly higher time efficiency than existing methods, as shown in the results.
In particular, tuning-based models demand substantial preprocessing time, whereas our method can be readily applied to various scenarios without additional training.

\begin{figure*}[htb!]
\centering
\includegraphics[width=0.8\textwidth]{ablation.png}
\caption{\textbf{Ablation Study.} 
The first row shows images generated by projecting personal concept 1 into reference latent 1, the second row applies personal concept 2 to reference latent 2, and the third row blends both personal concepts 1 and 2 into the background image. 
(a) Results using only mask-guided noise mixing and key-value replacement in self-attention. 
(b) Results from (a) with guided appearance attention. 
(c) Results from (b) with background dilution. 
(d) Results from (c) with mask-guided noise mixing on the reference noise.}
\label{fig4}
\end{figure*}

\begin{table}[htbp]
\caption{\textbf{Ablation Study.} Quantitative analysis of each component shows that our final setting (d) achieves the best performance in both $T_\text{CLIP}$ and $I_\text{CLIP}$.}
\centering
\resizebox{\linewidth}{!}{%
\begin{tabular}{clcc}
\toprule
& \textbf{Settings} & $T_\text{CLIP} \uparrow$ & $I_\text{CLIP} \uparrow$ \\
\midrule
(a) & Only mask-guided noise mixing            & 0.3401 & 0.6899 \\
(b) & (a) + Guided appearance attention        & 0.3665 & 0.7168 \\
(c) & (b) + Background dilution                & 0.3554 & 0.7026 \\
(d) & \textbf{(c) + Mask-guided noise mixing on reference noise (Ours)} & \textbf{0.3860} & \textbf{0.7317} \\
\bottomrule
\end{tabular}%
}
\label{tab:component_comparison2}
\end{table}

\subsection{Ablation Study}

In Figure~\ref{fig4} and Table~\ref{tab:component_comparison2}, we present the ablation study results conducted to verify the contribution of each component to the performance of the proposed method.
As shown in Figure~\ref{fig4}, we can visually confirm how the generated results change when successively adding mask-based noise mixing, key-value replacement in the self-attention module, guided appearance (attention), background dilution, and mask-based noise mixing for reference noise.
In addition, Table~\ref{tab:component_comparison2} examines the quantitative changes at each step by measuring CLIP text similarity ($T_\text{CLIP}$) and image similarity ($I_\text{CLIP}$).

First, Figure~\ref{fig4} (a) applies only mask-based noise mixing and key-value replacement\cite{cao2023masactrl, alaluf2024cross} in self-attention, demonstrating that even these two techniques alone can reflect certain personalized concepts (e.g., a specific entity or object).
In Figure~\ref{fig4} (b), guided appearance attention is further introduced, thereby maintaining the target's color and texture information more clearly and simultaneously enhancing both the visual similarity and the text similarity of the generated image.
In Figure~\ref{fig4} (c), by applying background dilution, the background information outside the target is weakened, which further emphasizes the target's representation.
Although some decrease in CLIP text similarity ($T_\text{CLIP}$) is observed due to the simplified background, the segmentation and portrayal of the target itself become more distinct.
Lastly, in Figure~\ref{fig4} (d), the mask-based noise mixing for reference noise is added, allowing the model to maintain the target's fine details while minimizing confusion among multiple personalized concepts.
This ultimately achieves the highest visual and text similarity, demonstrating stable performance in multi-personal concept generation tasks.

Consequently, as confirmed in Figure~\ref{fig4} and Table~\ref{tab:component_comparison2}, the ablation study results show that each individual component such as guided appearance attention, background dilution, and mask-based noise mixing plays a crucial role in enhancing the model's multi-personal concept generation performance.

\section{Conclusion}
We presented FlipConcept, a novel approach that integrates multiple personalized concepts into a single image without additional fine-tuning. 
By leveraging a step-by-step process with our three core techniques guided appearance attention, mask-guided noise mixing, and background dilution, we achieve a more robust representation of both image structure and personalized concepts. 
Consequently, our method excels at text-guided multi-concept editing, preserving each conceptâ€™s integrity while avoiding undesired alterations to other image regions.
FlipConcept demonstrates superior performance over existing methods in combining multiple personal concepts into a single scene, minimizing concept leakage and background distortion.
Moreover, FlipConcept is broadly applicable to AR/VR, e-commerce, and entertainment. 
For instance, AR/VR environments benefit from real-time integration of personalized concepts for immersive experiences; in e-commerce, FlipConcept flexibly merges products with various backgrounds for richer visual representations; and in the entertainment domain, it enables swift and precise generation or editing of multiple characters and objects in complex scenes.
As future work, we plan to extend FlipConcept by validating and improving its performance in high-resolution image generation and complex multi-concept scenarios (e.g., scenes with overlapping multiple objects or multiple persons). 
These efforts will not only demonstrate FlipConceptâ€™s potential to surpass existing multi-concept personalized image generation methods but also establish a new benchmark for advanced text-to-image systems.

\bibliographystyle{plain}
\bibliography{reference}
\end{document}
