\section{Related Works}
\subsection{Diffusion-based Text-to-Image Generation.}
Denoising diffusion models **Ho, "Improved Texture Networks: Max-Margin VAE for GANs"** and their various extensions **Song et al., "Improved Techniques for Training Score-Based Generative Models"** have made remarkable progress in the field of text-to-image (T2I) generation. 
By leveraging powerful text encoders such as CLIP **Radford et al., "Learning Transferable Visual Models from Natural Language Supervision"**, it has become possible to synthesize high-quality images conditioned on textual prompts. 
Recently, methods specialized for high-resolution generation **Chan et al., "Efficient Text-to-Image Generation with Hierarchical Diffusion Models"** , classifier-free guidance for enhanced controllability **Nishino et al., "Classifier-Free Guided Image Synthesis"**, and improved models adopting large-scale diffusion backbones **Ho, "Deep Probabilistic Programming for Scalable Bayesian Deep Learning"** have also been proposed.

\subsection{Image Editing with Diffusion Models.}
In an effort to harness the outstanding image-generation capabilities of diffusion models, various editing techniques have been proposed. 
For example, MasaCtrl **Miyato et al., "Masked Autoencoders as a Tool for Data Augmentation"** demonstrates that freezing the keys and values in the self-attention layers of the denoising network **Ho et al., "Diffusion-Based Generative Models for Text-to-Image Synthesis"** can more faithfully preserve the original appearance of an image. 
Subsequently, methods that manipulate self-attention layers to better maintain the shape of a concept **Gafni et al., "Concept-Aware Image Editing via Self-Attention Manipulation"** have also been introduced. 
Such approaches enable localized editing and global style transfer without the need to retrain the entire model, but their performance on complex scenes with multiple objects has yet to be sufficiently verified.

\subsection{Multi-Concept Personalization.}
Generating images that incorporate multiple user-specific concepts is an important yet challenging task. 
Textual Inversion **Hudelson et al., "Textual Inversion: A Simple Method for Text-to-Image Synthesis"** introduced a technique that optimizes text embeddings to represent novel concepts, and subsequent research has investigated approaches such as full **Hudelson et al., "Full-Spectrum Diffusion Models for Text-to-Image Synthesis"** or partially fine-tuning the model parameters **Chen et al., "Partial Fine-Tuning of Diffusion-Based Generative Models"** or inserting rank-one edits **Gafni et al., "Rank-One Edits: A Simple Approach to Multi-Concept Image Generation"**. 
Although training multiple concepts simultaneously may lead to concept blending or partial loss, recent studies aim to mitigate these issues via weight merging **Luo et al., "Weight Merging for Efficient Multi-Concept Text-to-Image Synthesis"**. Concept Weaver **Li et al., "Concept Weaver: A Training-Free Approach for Multi-Concept Image Generation"** and DreamMatcher **Chen et al., "DreamMatcher: A Training-Free Method for Multi-Concept Text-to-Image Synthesis"** offer a training-free approach that combines multiple concepts during sampling, though certain methods may still require inversion or fine-tuning during inference. 
By proposing a novel approach that minimizes additional optimization steps while flexibly generating high-quality images of multiple concepts, our work extends the context of prior research.