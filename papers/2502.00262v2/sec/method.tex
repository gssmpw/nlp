% \section{Method}
% In our proposed fine-tuned vision language model, we token Qwen2-VL as our baseline VLM. 
% \subsection{Modal feature extraction}
% The Qwen2-VL leverages a Transformer-based architecture with dedicated encoders for visual and textual data:\\
% Vision Encoder: Based on Vision Transformer (ViT), it divides images or video frames into patched and extracts features for each patch. \\
% \begin{itemize}
%     \item Input image $I \in \mathbb{R}^{H \times W \times 3}$ is divided into non-overlapping patches (e.g., $16 \times 16$) to form a sequence of visual tokens: \begin{equation}
%         F_{img} = VisionTransformer(I)
%     \end{equation}
%     \item Output $F_{img} \in \mathbb{R}^{N\times d}$, where $N$ is the number of patches and $d$ is the feature dimension.
% \end{itemize}
% Text Encoder: A Transformer-based model processes tokenized text sequences.
% \begin{itemize}
%     \item Textual input $T$, the encoder outputs is:
%     \begin{equation}
%         F_{text}= TextTransformer(T)
%     \end{equation}
%     \item Output $F_{text} \in \mathbb{R}^{M \times d}$, where $M$ is the number of text tokens
% \end{itemize}

% \subsection{Shared Latent Space}
% Both visual and textual features are projected into a shared latent space $V \subseteq \mathbb{R}^k$ through a projection layer, enabling cross-model understanding:
% \begin{equation}
%     E_{img} = Projector_{img} (F_img)
% \end{equation}
% \begin{equation}
%     E_{text}=Projector_{text}(F_{text})
% \end{equation}
% This alignment makes sure that visual and textual inputs can be compared and fused seamlessly.

% \subsection{Semantic Alignment Between Image and Text}
% The Qwen2-VL which we take as the baseline model achieves strong semantic alignment through contrastive learning:
% \begin{itemize}
%     \item Objective: Align embeddings of matching image-text pairs (positive samples) while maximizing the distance for non-matching pairs (negative samples).
%     \item Loss Function: \begin{equation}
%         L_{\text{contrastive}} = -\log \frac{\exp(\text{sim}(E_{\text{img}}, E_{\text{text}}^+))}{\sum_j \exp(\text{sim}(E_{\text{img}}, E_{\text{text}}^j))}
%     \end{equation}
%     where $E_{text}^{+}$ represent the embedding of the matching text, $E_{text}^j$ represent the embedding og non-matching text, and $sim(x,y)=\frac{x\cdot y}{\|x\| \|y\|}$
% \end{itemize}
% This training paradigm enables Qwen2-VL to excel in aligning visual and textual inputs, even for unseen image-text combinations.

% \subsection{Data Annotation and Preprocessing}


% \zifan{Change section name to method name}
% In our proposed fine-tuned vision-language model, we adopt Qwen2-VL as the baseline VLM to predict potential hazard regions $(x, y)$ from front-facing camera images in autonomous driving scenarios.
Our proposed method consists of four main components: modal feature extraction, shared latent space projection, semantic alignment, and hazard region localization. It includes extracting key features from visual and textual data for multimodal processing, mapping features into a unified space for seamless modality interaction, aligning visual and textual data for contextual understanding, and identifying and locating potential hazards in the driving environment. 
%\zifan{explain each in one sentence.}


\subsection{Modal Feature Extraction}
Qwen2-VL uses a transformer-based architecture with special encoders for visual and textual data:

\begin{itemize}
    \item \textbf{Vision Encoder:} The Vision Transformer (ViT) divides input images into non-overlapping patches and extracts features for each patch. It captures spatial and structural details from images by analyzing individual patches, enabling a comprehensive understanding of visual inputs.
    \item \textbf{Text Encoder:} A transformer-based model processes tokenized text sequences. It focuses on linguistic context and semantic relationships within tokenized text sequences, ensuring that textual information is accurately represented and aligned with the visual features.
\end{itemize}

Together, these encoders facilitate robust multimodal reasoning and integration.

% \zifan{Use plain text to explain, with sufficient details}


% \textbf{Vision Encoder:} The Vision Transformer (ViT) divides input images into non-overlapping patches and extracts features for each patch.
% \begin{itemize}
%     \item Input image $I \in \mathbb{R}^{H \times W \times 3}$ is divided into patches (e.g., $16 \times 16$) to form a sequence of visual tokens:
%     \begin{equation}
%         F_{\text{img}} = \text{VisionTransformer}(I)
%     \end{equation}
%     \item Output $F_{\text{img}} \in \mathbb{R}^{N \times d}$, where $N$ is the number of patches and $d$ is the feature dimension.
% \end{itemize}

% \textbf{Text Encoder:} A Transformer-based model processes tokenized text sequences.
% \begin{itemize}
%     \item Given textual input $T$, the encoder outputs:
%     \begin{equation}
%         F_{\text{text}} = \text{TextTransformer}(T)
%     \end{equation}
%     \item Output $F_{\text{text}} \in \mathbb{R}^{M \times d}$, where $M$ is the number of text tokens.
% \end{itemize}

\section{Supervised Fine-tuning VLMs}
\begin{algorithm}[!t]
\caption{INSIGHT: Fine-Tuning for Integration of Semantic and Visual Inputs for Generalized Hazard Tracking}
\label{alg:hazard_localization}
\begin{algorithmic}[1]
\State \textbf{Input:} Pre-trained model $M_0$, dataset $D$, learning rate $\eta$, epochs $E$, batch size $B$, coefficients $\lambda_{\text{coord}}, \lambda_{\text{text}}$
\State Initialize model with LoRA-adapted projection layers
\State Tokenize $D$ and split into $D_{\text{train}}$ and $D_{\text{val}}$
\State Define loss function:
\[
L_{\text{total}} = \lambda_{\text{coord}} L_{\text{coord}} + \lambda_{\text{text}} L_{\text{text}} \label{loss_total}
\]
\For{$e = 1$ \textbf{to} $E$}
    \For{batch $B_b$ in $D_{\text{train}}$}
        \State Extract vision and text embeddings
        \State Output attention coordinates $(\hat{x}, \hat{y})$
        \State Generate descriptive text $\hat{s}_{\text{text}}$
        \State Compute losses:
        \[L_{\text{coord}} = \frac{1}{N} \sum_{i=1}^{N} \| \hat{loc}_i - loc_i \|^2\] \label{loss1}
        \[L_{\text{text}} = -\frac{1}{N} \sum_{i=1}^{N} \sum_{t=1}^{T} s_{i,t} \log(\hat{s}_{i,t})\] \label{loss2}
        \[L_{\text{total}} = \lambda_{\text{coord}} L_{\text{coord}} + \lambda_{\text{text}} L_{\text{text}}\] \label{loss3}
        \State Update model parameters via gradient descent
    \EndFor
    \State Evaluate $M$ on $D_{\text{val}}$
\EndFor
\State \textbf{Output:} Fine-tuned model $M_{\text{sft}}$
\end{algorithmic}
\end{algorithm}


\subsection{Shared Latent Space}
Both visual and textual features are projected into a shared latent space $V \subseteq \mathbb{R}^k$ through projection layers, enabling cross-modal understanding:
% \zifan{explain what is projection layer, and why you use it}
\begin{equation}
    E_{\text{img}} = \text{Projector}_{\text{img}}(F_{\text{img}}),\quad E_{\text{text}} = \text{Projector}_{\text{text}}(F_{\text{text}})
\end{equation}
% 
The projection layer maps high-dimensional visual and textual features into a shared lower-dimensional latent space using a linear transformation or MLP. This shared space ensures that visual and textual inputs are comparable and seamlessly fused.

\subsection{Semantic Alignment Between Image and Text}
The proposed method achieves strong semantic alignment between images and text using contrastive learning. It can align embeddings of similar data (e.g., matching image and text) by bringing them closer in a shared space while pushing apart dissimilar data.
% \zifan{No need to say you are using Qwen2-VL. This method can be adapt to any VLM. Explain what contrastive learning is, and how to implement it. Why contrastive learning is effective here.}
% 
The objective of contrast learning is aligning embeddings of positive pairs (e.g., matching image and text) and negative pairs (e.g., mismatched image and text) are computed and the loss function is shown below:

    \begin{equation}
        L_{\text{contrastive}} = -\log \frac{\exp(\text{sim}(E_{\text{img}}, E_{\text{text}}^+))}{\sum_j \exp(\text{sim}(E_{\text{img}}, E_{\text{text}}^j))}
    \end{equation}
where $E_{\text{text}}^+$ represents the embedding of the matching text, $E_{\text{text}}^j$ represents the embedding of non-matching text, and $\text{sim}(x, y) = \frac{x \cdot y}{\|x\| \|y\|}$ is the cosine similarity.
This alignment ensures that relevant image-text pairs share a similar semantic representation, allowing the model to better integrate multimodal information and handle complex scenarios.

% \subsection{Hazard Region Localization}
% To predict the coordinates $(x, y)$ of potential hazard regions, we design a customized localization mechanism with fine-tuning.
% \begin{figure}[!ht]
%     \centering
%     \includegraphics[width=0.472\textwidth]{fig/manual_annotation.pdf}
%     \caption{Manual annotation}
%     \label{fig:manual_annotation}
% \end{figure}

% Data Annotation and Preprocessing:
% A custom dataset is created by manually annotating a selective portion of images from all training data. 
% % A custom dataset is created by manually annotating 1000 images from the BDD100K dataset. 
% Each image is labeled with potential hazard points $(x, y)$ based on human judgment and driving experience, as shown in Fig.~\ref{fig:manual_annotation}. The annotation criteria focused on identifying areas likely to pose risks, such as pedestrians, vehicles in proximity, or sudden obstacles. 

\subsection{Attention Maps for Spatial Localization}

Our proposed method employs a transformer-based architecture that generates multimodal embeddings by integrating visual and textual features. To enable spatial localization, we extract attention maps from its vision encoder, which inherently capture the regions of the input image that contribute most to the model's understanding. These attention maps, denoted as \(A \in \mathbb{R}^{H \times W}\), represent a spatial distribution of importance over the image, where \(H\) and \(W\) are the height and width of the feature map.

\subsubsection{Extracting Coordinates from Attention Maps}
To localize specific regions of interest, the method will identify the most prominent areas in the attention map. By computing attention weights for each spatial location using the outputs of the last layer of the vision encoder. The spatial coordinates of the maximum attention weight, \((\hat{x}, \hat{y})\), are identified as the predicted location of the target:
    \begin{equation}
        (\hat{x}, \hat{y}) = \arg\max_{(x, y)} A(x, y)
    \end{equation}

\subsubsection{Integration with Text Generation}
The extracted coordinates are further integrated with the language generation module to produce descriptive outputs. For instance, the model generates text such as:  
\textit{"The area around (544, 459) should be paid more attention to."}  
by conditioning the language generation layers on the localized regions.




\subsection{Loss Function:}
The training process adopts a multi-task loss function to simultaneously optimize coordinate prediction and text generation. The total loss is defined as $L_{\text{total}} = \lambda_{\text{coord}} L_{\text{coord}} + \lambda_{\text{text}} L_{\text{text}}$ which is shown in Algorithm~\ref{alg:hazard_localization}, particularly in lines\ref{loss_total}, \ref{loss1}.
with the following parameters.
\begin{itemize}
    \item \(L_{\text{coord}}\): Coordinate regression loss, computed as the Mean Squared Error (MSE) between the attention and true coordinates. It is formally represented as $L_{\text{coord}} = \frac{1}{N} \sum_{i=1}^{N} \| \hat{loc}_i - loc_i \|^2$,
    where \(\hat{loc}_i = (\hat{x}_i, \hat{y}_i)\) represents the attention map coordinates, and \(loc_i = (x_i, y_i)\) represents the true coordinates.
    \item \(L_{\text{text}}\): Text generation loss, computed as the cross-entropy Loss between the generated sequence and the ground truth sequence. It is formally represented as $L_{\text{text}} = -\frac{1}{N} \sum_{i=1}^{N} \sum_{t=1}^{T} s_{i,t} \log(\hat{s}_{i,t})$, 
    where \(T\) is the sequence length, \(s_{i,t}\) is the ground truth token, and \(\hat{s}_{i,t}\) is the predicted probability for the corresponding token.
    \item \(\lambda_{\text{coord}}\) and \(\lambda_{\text{text}}\): Balancing coefficients to control the relative importance of the two tasks.
\end{itemize}
% 
This approach solves key challenges in multimodal learning by explicitly aligning localization with textual descriptions. $L_{coord}$ stabilizes attention-based localization, addressing uncertainty in soft probabilistic outputs, while $L_{text}$ ensures contextually relevant text aligned with detected regions. The balancing coefficients allow dynamic emphasis on either task, improving robustness across datasets and use cases.
% 
Attention-based localization method removes the need for explicit bounding box annotations, while the multi-task loss ensures end-to-end optimization, making it both efficient and effective for scene understanding compared to traditional end-to-end black box model.
% 
% \textbf{Inference Pipeline:}
% \begin{enumerate}
%     \item Preprocess input images (e.g., resizing, normalization) and tokenize text prompts if applicable.
%     \item Extract visual embeddings \(E_{\text{img}}\) using the vision encoder.
%     \item Predict hazard coordinates \(\hat{y}\) through the regression head.
%     \item Generate descriptive text outputs through the text generation head.
%     \item Apply post-processing techniques, such as Non-Maximum Suppression (NMS), to refine hazard coordinate predictions and improve robustness.
% \end{enumerate}
% 
This methodology as shown in Fig.~\ref{fig:framework} enables accurate prediction of potential hazard regions in autonomous driving scenarios by utilizing fine-tuned vision-language alignment, precise localization mechanisms, and a combined focus on coordinate regression and textual explanation.


\subsection{Dataset Preprocessing}
\subsubsection{Dataset Overview}
The dataset used in this study is a subset of the BDD100K dataset, containing 1000 images selected from front-facing camera recordings in autonomous driving scenarios. These images cover diverse environments, including urban, suburban, and highway scenes, with varying lighting and weather conditions. A custom dataset is created by manually annotating this selective portion of images, where each image is labeled with potential hazard points $(x, y)$ based on human judgment and driving experience (Fig.~\ref{fig:manual_annotation}). The annotation criteria focused on identifying areas likely to pose risks, such as pedestrians, vehicles in proximity, or sudden obstacles. The subset of 1000 images is selected to maintain efficiency and reduce computational costs while still enabling effective model evaluation and training on key scenarios.
\begin{figure}[ht]
    \centering
    \includegraphics[width=0.472\textwidth]{fig/manual_annotation.pdf}
    \caption{Manual Annotation for dataset preprocessing}
    \label{fig:manual_annotation}
\end{figure}

\subsubsection{Annotation Method}
Manual annotation is performed to label potential hazard areas within each image. The hazard area is defined based on the experimental subjects’ driving experience and judgment, including areas where pedestrians, vehicles, or other obstacles pose potential risks to safe driving. In general, these potential hazard areas can be divided into two categories: predictable surrounding behavior (e.g., a vehicle in an adjacent lane attempting to merge) and unpredictable surrounding behavior (e.g., a pedestrian suddenly stepping onto the road).

For consistency, one experimental subject annotated all 1000 images in this dataset. During this annotation process, each frame is displayed for a maximum of 5 seconds, during which the subject identified the area with the highest probability of collision risk. A bounding box is drawn around the identified hazard area, and its center point recorded as $(x, y)$ coordinates. Each annotated image contains exactly one identified hazard area. To validate the reliability of these annotations, a subset of 100 images is cross-checked by two additional reviewers; discrepancies are discussed and resolved, resulting in a final consensus annotation for the entire dataset.

% 
% Annotations are further processed to ensure compatibility with the model's input requirements. For instance, bounding box coordinates are normalized to the image size and serialized into discrete tokens (e.g., x16, y8) for tokenization. This process bridges human annotation with the model's computational pipeline, ensuring the dataset's quality and applicability to the vision-language training process.



\subsection{Model Setup} 
% \zifan{(This part can be discussed in method section)}
\subsubsection{Pre-trained Model}
Qwen2-VL-7B is chosen as the baseline pre-trained model~\cite{wang2024qwen2}, while it can be easily extended to other pre-trained VLM models. This VLM is designed for multi-modal tasks, combining robust image and text processing capabilities. Also with the incorporation of the vision transformer for visual encoding and the transformer-based text encoder for language understanding, it's easy to enable the model to handle complex multi-modal tasks effectively. The pre-trained weights are sourced from the HuggingFace library, and the base architecture is kept unchanged during fine-tuning, except for specific layers adapted using LoRA.
\subsubsection{Fine-tuning Strategy}
To adapt the pre-trained model for hazard region prediction, LoRA is employed as a parameter fine-tuning technique. LoRA is applied to the multi-head attention layers (Query and Value projection matrics in the attention mechanism) and feed-forward layers (Selected layer of the output feed-forward network). The other weights in the pre-trained model remained frozen during training. The Rank $r$ of the LoRA matrices is set to 8. The pre-trained weights $W \in \mathbb{R}^{d \times k}$ are adapted as follows:
\begin{equation}
    W' = W + \Delta W, \Delta W = A \cdot B, A \in \mathbb{R}^{d \cdot r}, B \in \mathbb{R}^{r \cdot k},
\end{equation}
where $A$ and $B$ are learnable matrices, and $r$ is the low rank, significantly reducing the number of trainable parameters. 

\begin{figure*}[ht]
    \centering
    \begin{subfigure}[b]{0.3\textwidth}
        \centering
        \includegraphics[width=\linewidth]{fig/wandb_export_2025-01-29T21_31_52.003-05_00_loss_vs_global_step.png}
        \caption{Training loss of the supervised fine-tuning on Qwen2-VL-7B}
        \label{fig:sub1_1}
    \end{subfigure}
    \begin{subfigure}[b]{0.3\textwidth}
        \centering
        \includegraphics[width=\linewidth]{fig/wandb_export_2025-01-29T20_50_55.682-05_00_learning_rate_vs_global_step.png}
        \caption{Learning rate of the supervised fine-tuning on Qwen2-VL-7B}
        \label{fig:sub1_2}
    \end{subfigure}
    \begin{subfigure}[b]{0.3\textwidth}
        \centering
        \includegraphics[width=\linewidth]{fig/wandb_export_2025-01-29T21_36_16.811-05_00_grad_norm_vs_global_step.png}
        \caption{Grad norm of the supervised fine-tuning on Qwen2-VL-7B}
        \label{fig:sub2_1}
    \end{subfigure}
    % \hfill
    
    \caption{Supervised fine-tuning metrics on Qwen2-VL-7B. 
    % Loss Fig.~\ref{fig:sub1_1}, Learning rate Fig.~\ref{fig:sub1_2}, and Gradient norm Fig.~\ref{fig:sub2_1}
    }
    % \vspace{-0.1in}
    \label{fig:training_process}
    \vspace{-0.1in}
\end{figure*}