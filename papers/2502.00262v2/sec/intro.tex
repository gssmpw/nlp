\section{Introduction}
In autonomous driving scenarios, there exists a possibility that unpredictable edge cases, such as pedestrian crossing and construction zones, will occur in the next time steps~\cite{feng2023dense}. These unforeseen edge cases can lead to dangerous consequences, including vehicle crashes, pedestrian injuries, etc.
Therefore, it is necessary to develop a precise and generalized model to interpret the current driving scenario and draw out unpredictable dangerous scenarios. By connecting end-to-end and prediction models, autonomous vehicles are able to prevent effects from possible dangerous edge cases with advanced autonomous driving algorithms~\cite{chen2024end}. 

However, connecting current autonomous driving models and edge cases prediction models is challenging~\cite{feng2023dense}. The traditional detection algorithms, such as the sensor-based autonomous driving model~\cite{ribeiro2016should}, do not perform well on edge cases due to the lack of explicit expression of any semantic concepts. 
% 
The vision-language model (VLM) is a useful tool for developing high-hierarchy auxiliary algorithms. It is built by integrating a large language model (LLM) and a vision encoder,  which can take joint visual and textual input and generate textual responses. The vision-language adapter architecture enables collaborative understandings of multi-modal datasets, such as video, image, and natural language. With the ability of generalization and extraction from higher-level semantic information, the VLMs can utilize the pretrained knowledge, support the natural language commands, and deal with various, complicated, and even unseen driving scenarios~\cite{cui2023drivellm,Pan2024VLP}.
% \zifan{Ability to Leverage Pretrained Knowledge, Support for Natural Language Commands}
% \zifan{why is it important? why suit for autonomous driving?}
% The adapter meets the gap between visual features and language representations and also facilitates multimodal information fusion effectively. 
% 
% These scenarios, such as unexpected pedestrian behavior or sudden environmental changes (e.g., debris on the road or severe weather), are often underrepresented in training datasets. Consequently, the models fail to generalize effectively in such conditions, leading to higher risks of accidents.

In this paper, we introduce a novel framework, namely INSIGHT (Integration of Semantic and Visual Inputs for Generalized Hazard Tracking). 
With the connection of visual and textual inputs, the model can draw a precise description of the current scenario and forecast the dangerous location for future scenarios. It provides sufficient generalization ability when facing unexpected pedestrian and vehicle behaviors or sudden surrounding changes (e.g., emergency vehicles or debris on the road). The model is trained by using a multi-modal approach, extracting visual features with a Vision Transformer and fusing them with text embeddings from a pre-trained language model. It is also fine-tuned on an annotated dataset to optimize potential hazard edge case detection.
% 
% \zifan{Use one or two sentences to describe the method.}
Our contribution is summarized in three folds: 
\begin{itemize}
    \item Utilize the vision-language model to combine semantic and visual inputs to enhance hazard detection and improve generalization ability in edge cases.
    \item Achieve the goal of precise location detection by using an attention maps localization for loss function of supervised fine-tuning VLMs.
    \item Simulation studies show that INSIGHT effectively focuses on detecting potential hazard edge case areas that baseline and traditional models cannot complete.
    \end{itemize}
% (list the contribution with bullet points)
