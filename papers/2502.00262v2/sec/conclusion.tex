\section{Conclusion}
This study demonstrates the effectiveness of using fine-tuned VLMs, such as Qwen2-VL, in addressing the challenges of unpredictable edge case scenarios in autonomous driving. By connecting visual and textual modalities, INSIGHT enhances situational edge-case awareness and hazard detection. Experimental results show significant improvements in accuracy for hazard region localization and text generation, outperforming baselines and approaching human-level performance. INSIGHT not only advances the robustness of autonomous systems but also provides a framework for integrating multimodal learning to handle complex and rare driving conditions effectively. 
Future research could further explore real-time implementations and expand the model's adaptability to broader datasets, CARLA simulators, and real-world scenarios.