\section{Related Works}
\begin{figure*}[!th]
    \centering
    \includegraphics[width=1\linewidth]{fig/framework.pdf}
    \caption{Integration of Semantic and Visual Inputs for Generalized Hazard Tracking through Supervised Fine-tuning VLMs}
    \label{fig:framework}
\end{figure*}
Recently, the development of autonomous driving algorithms has significantly improved with various approaches addressing scene understanding, prediction, and control\cite{min2024driveworld,zhao2024enhanced,huang2020multi}. This section reviews relevant work in edge case scenario exploration, end-to-end autonomous driving models, and VLMs in autonomous vehicles.
\subsection{Edge Case Scenario Exploration}
Edge case scenarios in autonomous driving refer to rare, unpredictable, or extreme situations that challenge the robustness and safety of autonomous driving algorithms\cite{Cai2024A, Karunakaran2023Generating}. Verifying both the vehicle and its algorithms in these edge cases is critical to ensuring safety and reliability\cite{feng2023dense}. 
Testing in such scenarios involves subjecting the vehicle and its systems to high-risk or failure-prone conditions, allowing for a comprehensive evaluation of their performance under adverse circumstances\cite{goss2021generation, Zhang2023Finding}.
These simulations play a vital role in rigorously testing and refining algorithms and vehicle systems, enabling the identification and resolution of potential weaknesses\cite{Saffary2024Developing}. After verifying and even training in such scenarios, these vehicles are able to not only enhance the robustness of autonomous systems but also accelerate their deployment in real-world settings by ensuring they meet safety and performance standards under a wide range of conditions 
% \zifan{How does it ensure safety and reliability? After training in such scenarios, these vehicles are able to ...}
% 
Extensive research has been conducted to model and simulate edge case scenarios, focusing on conditions such as extreme weather, erratic pedestrian behavior, and unconventional vehicle movements\cite{chen2023using, Luo2021Interactive, chen2024deep}. By addressing these issues in simulated environments, developers can ensure a higher level of safety and performance before deploying the systems in real-world applications\cite{Ghodsi2021Generating}. This proactive approach is essential for building trust in autonomous driving technologies and mitigating risks associated with edge cases.

\subsection{End-to-End Autonomous Driving Model}
End-to-end models for autonomous driving utilize deep learning architectures to map raw sensor inputs, such as images and LiDAR data, directly to control outputs like steering, acceleration, and braking. Convolutional neural network (CNN) is a common model employed to process visual data by extracting spatial features from camera inputs, enabling the recognition of lanes, vehicles, and other road elements\cite{sharma2023cnn, alsanwy2023cnn}. Recurrent neural network (RNN) and temporal convolutional network (TCN) handle data sequences effectively, capturing dynamic changes in the environment over time\cite{Du2021Imitation}. Moreover, transformer utilizes self-attention mechanisms to effectively model long-range dependencies and complex interactions for both visual and sequential data processing\cite{rayakota2024hybridte, chen2023detrive}, offering improved performance in tasks such as object detection, trajectory prediction, and scene understanding\cite{Li2023Lane}. While effective in many scenarios, these models often face challenges in rare or hard-predictable edge cases due to their reliance on the data used during training which is collected from daily common driving scenarios or simple AV simulator scenarios\cite{10605806}, which may not fully represent real-world variability.

\subsection{Vision-Language Model in Autonomous Vehicle}
In recent years, VLM has made breakthroughs in natural language processing and multimodal tasks\cite{cui2023drivellm, Renz2024CarLLaVA}. Applying it to autonomous driving can effectively improve scene understanding and decision-making capabilities\cite{zhou2024vision, Pan2024VLP}.
% \subsubsection{Bridge of visual and language}
VLMs enhance the systemâ€™s comprehensive perception capabilities by combining visual data, such as cameras and LiDAR, and textual data, such as traffic signs and navigation instructions\cite{Wen2023On,zheng2024simplellm4ad,zhang2022learning}. The effective integration of visual features and language representation through visual language adapters can improve the ability to understand complex driving scenarios\cite{xu2024drivegpt4}.
% \subsubsection{Knowledge Reasoning and Edge Case Prediction} 
% Zero-shot learning is a method that can use the power of pre-trained models to perform efficient scene inference in the absence of labeled data or even in the absence of labeled data\cite{wang2024solving, kojima2022large}. 
With large-scale pre-trained VLMs, the model can learn common representations and knowledge from massive multi-modal data, assisted with zero-shot learning~\cite{wang2024solving, kojima2022large}, thus having strong generalization capabilities~\cite{rocamonde2023vision, Wei2022Semantic}. The model can infer semantic information such as the meaning of the potential movement of a pulled-over vehicle with law enforcement on the side from visual features in an image\cite{Su2024To}. This ability allows the model to use pre-trained knowledge to make inferences about unknown data when encountering new scenarios, without relying on task-specific annotated data\cite{gouidis2024fusing}.
In our work, we use VLM to unify visual and language understanding in autonomous driving, enabling real-time decisions, better scene comprehension, and improved edge case handling for safer and more efficient driving automation.

