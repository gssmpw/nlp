\section{Experiments}

\subsection{Hyperparameter Configuration}
The AdamW optimizer which includes the weight decaying for regularization is used in the experiment. The initial learning rate is set to $1 \times 10^{-4}$ and is decayed based on the cosine schedule as $\eta_t = \eta_0 \cdot 0.5 \left(1 + \cos\left(\frac{t}{T} \pi\right)\right)$,
where $\eta_t$ is learning rate at step $t$, and $T$ is total number of training steps.
% 
Also due to the memory constraints, a single image per batch is processed with gradient accumulation performed over 8 steps before parameter updates. The model is trained for 3 epochs, ensuring sufficient convergence without overfitting. Gradients are clipped at a maximum norm of 1.0 to prevent exploding gradients. BF16 mixed precision is enabled to reduce memory usage and improve training speed.

\subsection{Experiment Equipment}

The proposed experiment is conducted on a lab server running Ubuntu 20.04.6 LTS (Focal Fossa). The system is equipped with a 13th Gen Intel Core i9-13900KF processor featuring 16 cores and 32 threads. For graphical computation, the server utilizes an NVIDIA GPU GeForce RTX 4090 24 GB with the proprietary NVIDIA 535.183.01 driver. The system includes 64 GB of RAM and a 1 TB NVMe SSD for primary storage, supplemented by a 14.55 TB external USB drive for dataset storage and backups.  
% This hardware configuration provides sufficient computational power to efficiently fine-tune the Qwen2-VL-7B model and handle the BDD100K dataset during training and evaluation.

\subsection{Training Result}

\subsubsection{Training Process}

The training process is monitored using key metrics, including training loss, learning rate, and Validation accuracy, to ensure effective convergence and robust model performance on unseen data. Fig.~\ref{fig:training_process} presents these metrics: the training loss consistently decreases, indicating effective learning; the learning rate follows a cosine decay schedule for smooth optimization; and the gradient normalization remains controlled, ensuring stable training.
During the training process, the loss $\mathcal{L}$ exhibits a rapid decline from approximately $2.0$ at the beginning to around $0.6$ by $35$ steps, after which it stabilizes, indicating convergence. The learning rate $\eta$ follows a scheduled warm-up and cosine annealing strategy, starting at $3 \times 10^{-5}$ and progressively decreasing to near $0$ by $300$ steps, aligning with designed learning rate function to enhance model stability. The gradient norm $\|g\|$ shows an initial high value of $\sim2.0$, decreasing over time but exhibiting fluctuations, suggesting adjustments in weight updates during training. This overall trend indicates effective convergence of the model while maintaining controlled parameter updates.






% \textbf{Summary of Metrics:} To provide a detailed summary of the training process, Table~\ref{tab:training_metrics} presents the training loss, validation loss, and validation accuracy at the end of each epoch.

% \begin{table}[ht]
%     \centering
%     \caption{Training Metrics per Epoch}
%     \label{tab:training_metrics}
%     \begin{tabular}{c|c|c|c}
%         \toprule
%         \textbf{Epoch} & \textbf{Training Loss} & \textbf{Validation Loss} & \textbf{Validation Accuracy (\%)} \\
%         \midrule
%         1 & 0.485 & 0.512 & 82.3 \\
%         2 & 0.341 & 0.372 & 84.1 \\
%         3 & 0.265 & 0.298 & 85.5 \\
%         \bottomrule
%     \end{tabular}
% \end{table}

\subsubsection{Core Indicators and Performance}

The fine-tuned model's performance is evaluated using several core indicators, including BLEU-4, ROUGE-L, ROUGE-1, ROUGE-2, and Mean Squared Error (MSE). The BLEU-4 measures the precision of up to 4-gram overlaps between a fine-tuned vision-language model's generated text and reference outputs, making it useful for tasks like image captioning comparison, ROUGE-1 captures the unigram-level overlap, indicating how well the model covers key information in text summarization tasks, ROUGE-2 extends this to bigrams, offering a finer-grained measure of contextual coverage for summarizing or describing visual content, and ROUGE-L uses the longest common subsequence to evaluate the sequence-level match, emphasizing overall recall of critical information in the model’s output. The fine-tuned model is verified in the highway and urban driving scenarios of the BDD100K human annotation dataset. Each dataset contains 250 frames of driving scenarios and the $max \_ samples$ is also 250. The model achieved outstanding performance on standard scenarios, as shown in Table~\ref{tab:core_metrics}. The model achieved outstanding performance in highway driving scenarios and also great performance in urban driving scenarios. The MSE calculation function is shown below:
\begin{equation}
    \text{MSE} = \frac{1}{n} \sum_{i=1}^n \left( (x_i - x_i^{\text{true}})^2 + (y_i - y_i^{\text{true}})^2 \right)
\end{equation}
Where $x_i$ and $y_i$ represent the prediction from the fine-tuned Qwen2-VL and the $x_i^{\text{true}}$ and $y_i^{\text{true}}$ represent the human-annotation ground truth pixels location.
% \paragraph{Quantitative Results.}
% The model achieved outstanding performance on standard scenarios, as shown in Table~\ref{tab:core_metrics}. However, in edge cases, slight performance degradation is observed due to increased complexity. The metrics for each scenario type are summarized below.
\begin{table}[]
\centering
\caption{Core metrics for hazard region prediction}
\label{tab:core_metrics}
\begin{tabular}{lcc}
\toprule
\textbf{Metric} & \textbf{Highway Driving} & \textbf{Urban Driving}  \\
\midrule
BLEU-4 (\%)     & 89.025 & 88.012    \\
ROUGE-1 (\%)    & 87.313 & 86.928    \\
ROUGE-2 (\%)    & 72.857 & 71.957   \\
ROUGE-L (\%)    & 87.324 & 86.913  \\
MSE (pixels)    & 42.27 & 45.56     \\
\bottomrule
\end{tabular}
\end{table}
% 
The model's qualitative performance is illustrated in Fig.~\ref{fig:qualitative_results}, where predicted hazard regions (yellow) closely match the ground truth annotations (red) in most cases.
\begin{figure}
    \centering
    \includegraphics[width=1\linewidth]{fig/compare.pdf}
    \caption{Comparison of ground truth and predicted coordinates}
    \label{fig:qualitative_results}
\end{figure}
% 
In scenarios involving sudden lane changes or complicated driving scenarios, the model's performance dropped, as indicated by the higher MSE (3.29 pixels). An example of such a scenario is shown in Fig.~\ref{fig:qualitative_results}, where the predicted hazard region slightly deviates from the human-annotated ground truth due to occlusion.

\paragraph{Comparison with Baseline Models.}
As shown in Table~\ref{tab:comparision}, the proposed LoRA fine-tuned model achieves the highest performance across all reported metrics.
Specifically, it obtains an $88.087\%$ BLEU-4 score, far exceeds the next best baseline’s $16.241\%$ (Qwen2-VL-72B), and similarly outperforms other Qwen2-VL variants and Llama3.2-VL-7B by large margins in ROUGE-1, ROUGE-2, and ROUGE-L.

\begin{table*}[hb]
\centering
\caption{Core Performance Metrics for Hazard Region Prediction}
\label{tab:comparision}
\begin{tabular}{lccccc}
\toprule
\textbf{Metric} & \textbf{Fine-tuned Qwen2-VL-7B}& \textbf{Qwen2-VL-2B} & \textbf{Qwen2-VL-7B} & \textbf{Qwen2-VL-72B} & \textbf{Llama3.2-VL-7B}  \\
\midrule
BLEU-4 (\%)     & 88.087 & 8.697 & 14.105 & 16.241 & 15.781    \\
ROUGE-1 (\%)    & 87.077 & 22.308 & 26.603 & 28.348 & 27.824    \\
ROUGE-2 (\%)    & 72.084 & 7.535 & 8.022 & 8.272 & 8.913   \\
ROUGE-L (\%)    & 87.013 & 13.875 & 19.0782 & 22.713 & 19.945  \\
\bottomrule
\end{tabular}
\end{table*}

\subsubsection{Edge Case Implementation}
\begin{figure}[ht]
    \centering
    \includegraphics[width=1\linewidth]{fig/example.pdf}
    \caption{Demonstration of generalization ability}
    \label{fig:example_edge_case}
\end{figure}
% The well-fine-tuned Qwen2-VL-7B model is able to conclude current scenarios, give a reasonable interpretation, and draw out the area that needs to be paid more attention to.
The well-fine-tuned Qwen2-VL-7B model proves the capability to analyze and understand complex driving scenarios. It can effectively interpret the current scene, taking into account both visual and contextual cues, to provide a comprehensive and accurate understanding of the situation. By using its advanced VLMs' thought chain, the model identifies potential hazards, highlights key areas of the current frame, and generates actionable insights. Furthermore, it draws attention to specific regions within the environment that require closer observation or immediate action, such as areas with pedestrians, abrupt vehicle movements, or unexpected obstacles. The example implementation is shown in Fig.~\ref{fig:example_edge_case} with the parameter Top-p is $0.9$ and Temperature is $0.95$.
