\section{Algorithms for Computing Nonnegative Manifold Data Factorizations.}
\label{sec:algorithms}
In this section, we present iterative algorithms for computing T-NMDF and CC-NMDF, and we propose a data-driven initial guess for CC-NMDF. We also discuss postprocessing and parameter choices to improve the interpretability of the learned factorizations.
\subsection{Tangent Space Nonnegative Manifold Data Factorization.}
To find an approximate solution of \cref{eq:tnmdf problem}, we observe that we can apply algorithms for computing Euclidean semi-NMF. Explicitly, we fix an orthonormal basis $\{\phi_\mPoint^\sumIndB\}_{\sumIndB=1}^\dimInd \subset \tangent_\mPoint \manifold$ and consider the coordinate matrix $\Matrix \in \Real^{\numData \times \dimInd}$ defined entrywise by
\begin{equation}
    \Matrix_{\sumIndA, \sumIndB} = \left ( \log_\mPoint \Tensor^\sumIndA, \phi_\mPoint^\sumIndB \right )_\mPoint
\end{equation}
so that
\begin{equation}
\label{eq:coord matrix}
    \log_{\mPoint} \Tensor^\sumIndA = \sum_{\sumIndB=1}^\dimInd \Matrix_{\sumIndA,\sumIndB} \phi_\mPoint^\sumIndB.
\end{equation}
Then, for a desired rank $\numFactors$, we compute the semi-NMF of $\Matrix$ given by
\begin{equation}
    \Matrix \approx \nonnegFactor \factorCoords
\end{equation}
where $\nonnegFactor \in \Real_{\geq 0}^{\numData \times \numFactors}$ and $\factorCoords\in\Real^{\numFactors \times \dimInd}$, using an iterative algorithm such as the one described by \cite{ding2008convex}. Then, the corresponding tangent vector factors are 
$\{\mTVectorFactor_\mPoint^\sumIndC\}_{\sumIndC=1}^\numFactors \subset \tangent_\mPoint\manifold$, where $\mTVectorFactor_\mPoint^\sumIndC = \sum_{\sumIndB=1}^\dimInd \factorCoords_{\sumIndC, \sumIndB}\phi_{\mPoint}^\sumIndB$, and the manifold-valued factors are $\{\TensorY^\sumIndC\}_{\sumIndC=1}^\numFactors \subset \manifold$ where
\begin{equation}
    \TensorY^\sumIndC = \exp_\mPoint \left ( \left(\max_{\sumIndA = 1, \dots, \numData} \nonnegFactor_{\sumIndA, \sumIndC}\right) \mTVectorFactor_\mPoint^\sumIndC \right ).
\end{equation}
The T-NMDF algorithm (using the Euclidean semi-NMF algorithm from \citealp{ding2008convex}) is summarized in \Cref{alg:t-SNMDF}. This approach is a computationally cheap way to find approximate solutions of \eqref{eq:tnmdf problem}, since it requires only standard matrix operations after the coordinate matrix $\Matrix$ is obtained. 

\begin{algorithm}[t!]
\caption{Tangent space nonnegative manifold data factorization (T-NMDF)}
\label{alg:t-SNMDF}
\begin{algorithmic}[1]
\Require{$\{\Tensor^\sumIndA\}_{\sumIndA=1}^\numData \subset \manifold$, $\mPoint\in\manifold$, $K \in \Natural$}
\For{$\sumIndA = 1, \ldots, \numData$}
\For{$\sumIndB = 1, \ldots, \dimInd$}
\State $\Matrix_{\sumIndA, \sumIndB} \gets \left ( \log_\mPoint \Tensor^\sumIndA, \phi_\mPoint^\sumIndB \right )_\mPoint$
\EndFor
\EndFor
\State Compute the Euclidean semi-NMF $\Matrix \approx \nonnegFactor \factorCoords$
\For{$\sumIndC = 1, \ldots, \numFactors$}
\State $\mTVectorFactor_\mPoint^\sumIndC \coloneqq \sum_{\sumIndB=1}^\dimInd \factorCoords_{\sumIndC, \sumIndB}\phi_{\mPoint}^\sumIndB$
\State $\TensorY^\sumIndC \coloneqq \exp_\mPoint \left ( \left(\max_{\sumIndA = 1, \dots, \numData} \nonnegFactor_{\sumIndA, \sumIndC}\right) \mTVectorFactor_\mPoint^\sumIndC \right )$
\EndFor
\State \Return $\nonnegFactor, \{\mTVectorFactor_\mPoint^\sumIndC\}_{\sumIndC=1}^\numFactors$, and $\{\TensorY^\sumIndC\}_{\sumIndC=1}^\numFactors$
\end{algorithmic}
\end{algorithm}

\subsection{Curvature Corrected Nonnegative Manifold Data Factorization.}
\label{sec:ccnmdf}
We propose an alternating iterative algorithm for solving the curvature-aware factorization problem \eqref{eq:cc snmdf prob}. First, we argue that the solution of tangent space $\numFactors$-means will give a good initialization. Then, we derive a multiplicative update for the curvature corrected algorithm.

\subsubsection{Initialization of the algorithm.}
\label{sec:initialization}
The framework developed by \cite{diepeveen2023curvature} implies that good solutions to low-rank decomposition problems such as \eqref{eq:cc snmdf prob} can be obtained by first solving a similar problem in $\tangent_\mPoint \manifold$ and subsequently applying curvature correction. In particular, we claim that CC-NMDF can be obtained as a relaxation of the manifold K-means problem. Consider the $\numFactors$-means clustering problem for data $\{ \Tensor^\sumIndA\}_{\sumIndA = 1}^\numData \subset \manifold$
\begin{equation}
\label{eq:exact kmeans}
    \argmin_{\substack{ \mPointB^1, \dots, \mPointB^\numFactors \in \manifold \\
    \nonnegFactor \in \{0,1\}^{\numData \times \numFactors} \\
    \text{s.t. } \nonnegFactor \mathbf{1}_\numFactors = 1}} \sum_{\sumIndA = 1}^{\numData} \sum_{\sumIndC = 1}^{\numFactors}  \nonnegFactor_{\sumIndA, \sumIndC}\distance_\manifold \left(\Tensor^\sumIndA, \mPointB^\sumIndC\right)^2.
\end{equation}
which assigns each data point $\Tensor^\sumIndA$ to one of $\numFactors$ clusters, whose centroids are given by the minimizers $\{\mPointB^\sumIndC\}_{\sumIndC=1}^\numFactors \subset \manifold$ of the above. Then, if $\nonnegFactor$ and $\{\mPointB^\sumIndC\}_{\sumIndC=1}^\numFactors $ are solutions of the above, we see that 

\begin{multline}
    \sum_{\sumIndA=1}^\numData  \sum_{\sumIndC=1}^\numFactors \nonnegFactor_{\sumIndA, \sumIndC} \distance_{\manifold} \left(\Tensor^\sumIndA, \mPointB^\sumIndC\right)^2 \overset{\mTVectorFactor_\mPoint^\sumIndC \coloneqq \log_{\mPoint}\mPointB^{\sumIndC}}{=} \sum_{\sumIndA=1}^\numData  \sum_{\sumIndC=1}^\numFactors \nonnegFactor_{\sumIndA, \sumIndC} \distance_{\manifold} \left(\Tensor^\sumIndA, \exp_{\mPoint}\left( \mTVectorFactor_\mPoint^\sumIndC\right)\right)^2 \\
    \overset{\text{\cite[Thm~3.4]{diepeveen2023curvature}}}{\approx} \sum_{\sumIndA=1}^\numData \sum_{\sumIndC=1}^\numFactors \nonnegFactor_{\sumIndA, \sumIndC} \sum_{\sumIndB=1}^\dimInd  \beta(\kappa_{\sumIndA,\sumIndB})^2 \left( \mTVectorFactor_\mPoint^\sumIndC - \log_{\mPoint} \Tensor^\sumIndA, \Theta_{\mPoint}^{(\sumIndA),\sumIndB}\right)_\mPoint^2\\
    \overset{\nonnegFactor_{\sumIndA, \sumIndC} \in \{0,1\}}{=} \sum_{\sumIndA=1}^\numData \sum_{\sumIndC=1}^\numFactors \sum_{\sumIndB=1}^\dimInd  \beta(\kappa_{\sumIndA,\sumIndB})^2 \left(\nonnegFactor_{\sumIndA, \sumIndC} \mTVectorFactor_\mPoint^\sumIndC -  \nonnegFactor_{\sumIndA, \sumIndC} \log_{\mPoint} \Tensor^\sumIndA, \Theta_{\mPoint}^{(\sumIndA),\sumIndB}\right)_\mPoint^2\\
    \overset{\sum_{\sumIndC=1}^\numFactors \nonnegFactor_{\sumIndA, \sumIndC}=1}{=} \sum_{\sumIndA=1}^\numData  \sum_{\sumIndB=1}^\dimInd  \beta(\kappa_{\sumIndA,\sumIndB})^2 \left(\sum_{\sumIndC=1}^\numFactors\nonnegFactor_{\sumIndA, \sumIndC} \mTVectorFactor_\mPoint^\sumIndC -  \sum_{\sumIndC=1}^\numFactors\nonnegFactor_{\sumIndA,\sumIndC} \log_{\mPoint} \Tensor^\sumIndA, \Theta_{\mPoint}^{(\sumIndA),\sumIndB}\right)_\mPoint^2 \\
\overset{\sum_{\sumIndC=1}^\numFactors \nonnegFactor_{\sumIndA, \sumIndC}=1}{=} \sum_{\sumIndA=1}^\numData  \sum_{\sumIndB=1}^\dimInd  \beta(\kappa_{\sumIndA,\sumIndB})^2 \left(\sum_{\sumIndC=1}^\numFactors\nonnegFactor_{\sumIndA, \sumIndC} \mTVectorFactor_\mPoint^\sumIndC -  \log_{\mPoint} \Tensor^\sumIndA, \Theta_{\mPoint}^{(\sumIndA),\sumIndB}\right)_\mPoint^2 .
\end{multline}
By relaxing the constraint on $\nonnegFactor$ to allow its entries to take on values in $(0, \infty)$ and removing the row sum constraint, we then obtain the optimization problem for CC-NMDF \eqref{eq:cc snmdf prob}. Hence, CC-NMDF can be viewed as a relaxation of the exact $\numFactors$-means problem on $\manifold$, which implies that a solution of manifold $\numFactors$-means is a good initialization for solving the CC-NMDF problem. To avoid the expense of direct computation on $\manifold$ for initialization, we can instead solve the tangent space $\numFactors$-means problem 
\begin{align}
     \label{eq:kmeans tpm naive}   \argmin_{\substack{\nonnegFactor \in \{0, 1\}^{\numData \times \numFactors}\\
    \mTVectorFactor_\mPoint^1, \dots, \mTVectorFactor_\mPoint^\numFactors \in \tangent_\mPoint \manifold} } & \sum_{\sumIndA=1}^\numData \sum_{\sumIndC=1}^\numFactors \nonnegFactor_{\sumIndA, \sumIndC} \left\| \log_\mPoint \Tensor^\sumIndA - \mTVectorFactor_\mPoint^\sumIndC\right\|_\mPoint^2 \\
    \text{subject to }& \nonnegFactor \mathbf{1}_\numFactors = \mathbf{1}_\numData \nonumber
\end{align}
by applying standard Euclidean $\numFactors$-means algorithms to the coordinate matrix $\Matrix$, computed as in \eqref{eq:coord matrix}. Once we obtain an initial guess for $\nonnegFactor$ by tangent space $\numFactors$-means, we relax $\nonnegFactor$ by replacing each 0 entry with $\delta$ for some $0 < \delta \ll 1$, then normalizing the rows of $\nonnegFactor$ such that $\nonnegFactor \mathbf{1}_\numFactors = \mathbf{1}_\numData$.
\subsubsection{Multiplicative Update for CC-NMDF.}
Starting from initial guesses for $\nonnegFactor \in \Real_{\geq 0}^{\numData \times \numFactors}$ and $\{\mTVectorFactor_\mPoint^\sumIndC\}_{\sumIndC = 1}^\numFactors$, we apply curvature correction to  $\nonnegFactor$ and each $\mTVectorFactor_\mPoint^\sumIndC$ in an alternating fashion. First, we correct the tangent vector factors $\mTVectorFactor_\mPoint^\sumIndC$ by solving

\begin{equation}\label{eq:cc-factor-problem}
    \argmin_{\mTVectorFactor_\mPoint^1, \ldots, \mTVectorFactor_\mPoint^\numFactors \in \tangent_\mPoint \manifold} \sum_{\sumIndA=1}^\numData  \sum_{\sumIndB=1}^\dimInd \beta(\kappa_{\sumIndA,\sumIndB})^2 \left(\sum_{\sumIndC=1}^\numFactors \nonnegFactor_{\sumIndA, \sumIndC} \mTVectorFactor_\mPoint^\sumIndC - \log_{\mPoint} \Tensor^\sumIndA, \Theta_{\mPoint}^{(\sumIndA),\sumIndB}\right)_\mPoint^2.
\end{equation}
To do this, we first expand each $\mTVectorFactor_\mPoint^\sumIndC$ and write
\begin{equation}
    \mTVectorFactor_\mPoint^\sumIndC =  \sum_{\sumIndB=1}^\dimInd \factorCoords_{\sumIndC, \sumIndB} \phi_\mPoint^\sumIndB
\end{equation}
where $\factorCoords \in \Real^{\numFactors \times \dimInd}$ is a real-valued matrix and $\{\phi_\mPoint^\sumIndB\}_{\sumIndB=1}^\dimInd \subset \tangent_\mPoint \manifold$ is an orthonormal basis. Then we can solve \eqref{eq:cc-factor-problem} by adapting the curvature correction step in \cite[Sec.~5.2]{diepeveen2023curvature}. Explicitly, we define the real-valued tensor $\TensorB \in \Real^{\numData \times \dimInd \times \numFactors \times \dimInd}$ entrywise by

\begin{equation}
    \label{eq:b tensor def}
    \TensorB_{\sumIndA_1, \sumIndB, \sumIndC_1, \sumIndA} \coloneqq \nonnegFactor_{\sumIndA_1, \sumIndC_1} \left(\phi_\mPoint ^\dimInd, \Theta_{\mPoint}^{(\sumIndA),\sumIndB}\right)_\mPoint
\end{equation}
and then define the real-valued tensor $\TensorA \in \Real^{\numFactors \times \dimInd \times \numFactors \times \dimInd}$ entrywise by

\begin{equation}
    \label{eq:a tensor def}
    \TensorA_{\sumIndC_1', \sumIndA', \sumIndC_1, \sumIndA} \coloneqq \sum_{\sumIndA_1 = 1}^\numData \sum_{\sumIndB = 1}^\dimInd \beta(\kappa_{\sumIndA_1, \sumIndB})^2 \TensorB_{\sumIndA_1, \sumIndB, \sumIndC_1, \sumIndA} \TensorB_{\sumIndA_1, \sumIndB, \sumIndC_1', \sumIndA'}.
\end{equation}
Then, solutions of the linear system
\begin{equation}
    \label{eq:f linear system}
    \sum_{\sumIndC_1 = 1}^\numFactors \sum_{\sumIndA = 1}^\dimInd \TensorA_{\sumIndC_1', \sumIndA', \sumIndC_1, \sumIndA} \factorCoords_{\sumIndC_1, \sumIndA} = \sum_{\sumIndA_1 = 1}^\numData \sum_{\sumIndB = 1}^\dimInd \beta(\kappa_{\sumIndA_1, \sumIndB})^2 \TensorB_{\sumIndA_1, \sumIndB, \sumIndC_1, \sumIndA} \left( \log_\mPoint \Tensor^\sumIndA, \Theta_{\mPoint}^{(\sumIndA),\sumIndB} \right)_\mPoint
\end{equation}
satisfy the first-order optimality conditions for \eqref{eq:cc-factor-problem}.

Next, to correct the non-negative factor $\nonnegFactor$, we seek to solve 
\begin{equation}\label{eq:cc-nonneg-problem}
    \inf_{\nonnegFactor \in \Real_{\geq 0}^{\numData \times \numFactors}} \sum_{\sumIndA=1}^\numData  \sum_{\sumIndB=1}^\dimInd \beta(\kappa_{\sumIndA,\sumIndB})^2 \left(\sum_{\sumIndC=1}^\numFactors \nonnegFactor_{\sumIndA, \sumIndC} \mTVectorFactor_\mPoint^\sumIndC - \log_{\mPoint} \Tensor^\sumIndA, \Theta_{\mPoint}^{(\sumIndA),\sumIndB}\right)_\mPoint^2.
\end{equation}
We derive a multiplicative update for this constrained optimization problem using the Karush-Kuhn-Tucker conditions. Let $f:\Real^{\numData \times \numFactors} \to \Real$ be given by
\begin{equation}
    f(\nonnegFactor) = \sum_{\sumIndA=1}^\numData  \sum_{\sumIndB=1}^\dimInd \beta(\kappa_{\sumIndA,\sumIndB})^2 \left(\sum_{\sumIndC=1}^\numFactors \nonnegFactor_{\sumIndA, \sumIndC} \mTVectorFactor_\mPoint^\sumIndC - \log_{\mPoint} \Tensor^\sumIndA, \Theta_{\mPoint}^{(\sumIndA),\sumIndB}\right)_\mPoint^2.
\end{equation}
We then form the Lagrangian function
\begin{equation}
    g(\nonnegFactor) = -\trace(\lambda \nonnegFactor^T) + f(\nonnegFactor)
\end{equation}
where $\lambda \in \Real^{\numData \times \numFactors}$, enforcing nonnegativity for each entry of $\nonnegFactor$. The gradient of the Lagrangian is given by
\begin{equation}
    \nabla g(\nonnegFactor)_{\sumIndA_1 \sumIndC_1} = -\lambda_{\sumIndA_1 \sumIndC_1} + 2\sum_{\sumIndA=1}^\numData \sum_{\sumIndB=1}^\dimInd \beta(\kappa_{\sumIndA,\sumIndB})^2\left(\sum_{\sumIndC=1}^\numFactors \nonnegFactor_{\sumIndA, \sumIndC} \mTVectorFactor_\mPoint^\sumIndC - \log_{\mPoint} \Tensor^\sumIndA, \Theta_{\mPoint}^{(\sumIndA),\sumIndB}\right)_\mPoint \left(\mTVectorFactor_\mPoint^{\sumIndC_1}, \Theta_{\mPoint}^{(\sumIndA_1),\sumIndB}\right)_\mPoint.
\end{equation}
By the complementary slackness condition, we have that $\lambda_{\sumIndA_1 \sumIndC_1} \nonnegFactor_{\sumIndA_1 \sumIndC_1}=0$ for $\sumIndA_1 = 1, \dots, \numData$ and $\sumIndC_1 = 1, \dots, \numFactors$. Setting $\nabla g(\nonnegFactor)_{\sumIndA_1 \sumIndC_1} = 0$ and using this complementary slackness condition, we obtain the $\numData\numFactors$ equations
\begin{equation}
    2 \nonnegFactor_{\sumIndA_1 \sumIndC_1} \sum_{\sumIndA=1}^\numData \sum_{\sumIndB=1}^\dimInd \beta(\kappa_{\sumIndA,\sumIndB})^2\left(\sum_{\sumIndC=1}^\numFactors \nonnegFactor_{\sumIndA, \sumIndC} \mTVectorFactor_\mPoint^\sumIndC - \log_{\mPoint} \Tensor^\sumIndA, \Theta_{\mPoint}^{(\sumIndA),\sumIndB}\right)_\mPoint \left(\mTVectorFactor_\mPoint^{\sumIndC_1}, (\Theta_{\mPoint}^{(\sumIndA_1),\sumIndB}\right)_\mPoint = 0
\end{equation}
which we equivalently write as the fixed point equations
\begin{equation}
\nonnegFactor_{\sumIndA_1 \sumIndC_1} = \nonnegFactor_{\sumIndA_1 \sumIndC_1} \sqrt{\frac{\mathbf{A}_{\sumIndA_1\sumIndC_1}^+ + \mathbf{B}_{\sumIndA_1\sumIndC_1}^-}{\mathbf{A}_{\sumIndA_1\sumIndC_1}^- + \mathbf{B}_{\sumIndA_1\sumIndC_1}^+}}
\end{equation}
where the entries of $\mathbf{A}$ and $\mathbf{B}$ are given by
\begin{equation}
\label{eq:mult update A}
    \mathbf{A}_{\sumIndA_1, \sumIndC_1} = \sum_{\sumIndA=1}^\numData \sum_{\sumIndB=1}^\dimInd \beta(\kappa_{\sumIndA,\sumIndB})^2\left(\sum_{\sumIndC=1}^\numFactors \nonnegFactor_{\sumIndA,\sumIndC} \mTVectorFactor_\mPoint^\sumIndC, \Theta_{\mPoint}^{(\sumIndA),\sumIndB}\right)_\mPoint \left(\mTVectorFactor_\mPoint^{\sumIndC_1}, (\Theta_{\mPoint}^{(\sumIndA_1),\sumIndB}\right)_\mPoint,
\end{equation}
\begin{equation}
\label{eq:mult update B}
     \mathbf{B}_{\sumIndA_1, \sumIndC_1} = \sum_{\sumIndA=1}^\numData \sum_{\sumIndB=1}^\dimInd \beta(\kappa_{\sumIndA,\sumIndB})^2\left(\log_\mPoint\Tensor^\sumIndA, \Theta_{\mPoint}^{(\sumIndA),\sumIndB}\right)_\mPoint \left(\mTVectorFactor_\mPoint^{\sumIndC_1}, (\Theta_{\mPoint}^{(\sumIndA_1),\sumIndB}\right)_\mPoint,
\end{equation}
and $\mathbf{A}_{\sumIndA_1, \sumIndC_1}^+ = \max\{0, \mathbf{A}_{\sumIndA_1, \sumIndC_1}\}$ and $\mathbf{A}_{\sumIndA_1, \sumIndC_1}^- = -\min\{0, \mathbf{A}_{\sumIndA_1, \sumIndC_1}\}$. Hence, we obtain the multiplicative update 
% \begin{equation}
%     \nonnegFactor_{\sumIndA_1, \sumIndC_1} \leftarrow \nonnegFactor_{\sumIndA_1, \sumIndC_1} \sqrt{\frac{\mathbf{A}_{\sumIndA_1,\sumIndC_1}^+ + \mathbf{B}_{\sumIndA_1,\sumIndC_1}^-}{\mathbf{A}_{\sumIndA_1,\sumIndC_1}^- + \mathbf{B}_{\sumIndA_1,\sumIndC_1}^+}}.
% \end{equation}
\begin{equation}
    \nonnegFactor \leftarrow \nonnegFactor \odot \sqrt{\frac{\mathbf{A}^+ + \mathbf{B}^-}{\mathbf{A}^- + \mathbf{B}^+}}
\end{equation}
where $\odot$ denotes pointwise multiplication, and the square root and division are also pointwise. To improve convergence, we repeat the update for $\nonnegFactor$ before re-updating $\factorCoords$. The full algorithm is summarized in \Cref{alg:CC-NMDF}. We note that $\mathbf{B}$ can be pre-computed and used throughout the updates for $\nonnegFactor$, and the computations of $\mathbf{A}$ and $\mathbf{B}$ are both entirely in $\tangent_\mPoint \manifold$.

\begin{algorithm}[t!]
\caption{Curvature corrected nonnegative manifold data factorization (CC-NMDF)}
\label{alg:CC-NMDF}
\begin{algorithmic}[1]
\Require{$\{\Tensor^\sumIndA\}_{\sumIndA=1}^\numData \subset \manifold$, $\mPoint\in\manifold$, $K \in \Natural, \delta$, \texttt{maxIter}, \texttt{maxSubIter}}
\For{$\sumIndA = 1, \ldots, \numData$}
\For{$\sumIndB = 1, \ldots, \dimInd$}
\State $\Matrix_{\sumIndA, \sumIndB} \gets \left ( \log_\mPoint \Tensor^\sumIndA, \phi_\mPoint^\sumIndB \right )_\mPoint$
\EndFor
\EndFor
\State Compute the Euclidean $\numFactors$-means clustering $\Matrix \approx \nonnegFactor \factorCoords$
\State Replace every 0 entry of $\nonnegFactor$ with $\delta$.
\State Set $\mathbf{D} = \text{diag}(\nonnegFactor \mathbf{1}_K)$,  $\nonnegFactor \gets \mathbf{D}^{-1}\nonnegFactor$
\For{$i=1$, \ldots, \texttt{maxIter}}
\State Construct linear system \eqref{eq:f linear system} and update $\factorCoords$ accordingly.
\For{$j=1$, \ldots, \texttt{maxSubIter}}
\State Compute $\mathbf{A}, \mathbf{B}$ as defined in \cref{eq:mult update A,eq:mult update B}.
\State Set $\nonnegFactor \gets \nonnegFactor \odot \nonnegFactor \sqrt{\frac{\mathbf{A}^+ + \mathbf{B}^-}{\mathbf{A}^- + \mathbf{B}^+}}$
\EndFor
\EndFor
\For{$\sumIndC = 1, \ldots, \numFactors$}
\State $\mTVectorFactor_\mPoint^\sumIndC \coloneqq \sum_{\sumIndB=1}^\dimInd \factorCoords_{\sumIndC, \sumIndB}\phi_{\mPoint}^\sumIndB$
\State $\TensorY^\sumIndC \coloneqq \exp_\mPoint \left ( \left(\max_{\sumIndA = 1, \dots, \numData} \nonnegFactor_{\sumIndA, \sumIndC}\right) \mTVectorFactor_\mPoint^\sumIndC \right )$
\EndFor
\State \Return $\nonnegFactor, \{\mTVectorFactor_\mPoint^\sumIndC\}_{\sumIndC=1}^\numFactors$, and $\{\TensorY^\sumIndC\}_{\sumIndC=1}^\numFactors$
\end{algorithmic}
\end{algorithm}


\subsection{Algorithm Considerations.}
\label{sec:algorithm-considerations}
In this section, we discuss two aspects of CC-NMDF that must be considered in practical applications. 
\subsubsection{Potential Cancellation of Tangent Vector Factors.} Because CC-NMDF does not impose restrictions on the learned tangent vector factors $\{\mTVectorFactor_\mPoint^\sumIndC\}_{\sumIndC=1}^\numFactors$, it is possible that cancellation can occur in the learned factors. That is, we may have $(\mTVectorFactor_\mPoint^\sumIndB, \mTVectorFactor_\mPoint^\sumIndC)_\mPoint < 0$ for some $\sumIndB \neq \sumIndC$. This may impact the interpretability of the factors. 

To build intuition about this effect, consider the Euclidean semi-NMF of real-valued data $\Matrix \in \Real^{\numData \times \dimInd}$ with mixed signs 
\begin{equation}
    \label{eq:eucl snmf}
    \Matrix \approx \nonnegFactor \factorCoords
\end{equation}
where $\nonnegFactor \in \Real_{\geq 0}^{\numData \times \numFactors}$ and $\factorCoords \in \Real^{\numFactors \times \dimInd}$. In particular, suppose that the $\sumIndB$th and $\sumIndC$th rows of $\factorCoords$, denoted by $\factorCoords_{\sumIndB,:}$ and $\factorCoords_{\sumIndC,:}$, satisfy $\factorCoords_{\sumIndB,:} = - \factorCoords_{\sumIndC, :}$. Then, if the $\sumIndA$th row of $\nonnegFactor$, corresponding to the $\sumIndA$th element of the dataset, has nonzero $\sumIndB$th and $\sumIndC$th entries, then the $\sumIndA$th element of the dataset could be equally well-represented by a semi-NMF
\begin{equation}
    \Matrix \approx \nonnegFactor' \factorCoords
\end{equation}
where $\nonnegFactor'_{i,j} + \nonnegFactor'_{i,k} = \nonnegFactor_{i,j} + \nonnegFactor_{i,k}$. That is, the approximation can no longer be interpreted as a parts-based decomposition due to the ambiguity about the relationship between the $\sumIndA$th element of the dataset and the $\sumIndB$th and $\sumIndC$th learned factors. Hence, this cancellation effect hinders the interpretability of the representation of the dataset learned by semi-NMF. 

In the case of manifold-valued data $\{\Tensor^\sumIndA\}_{\sumIndA=1}^\numData \subset \manifold$ and their CC-NMDF representation $\log_\mPoint \Tensor^\sumIndA \approx \sum_{\sumIndC=1}^\numFactors \nonnegFactor_{\sumIndA, \sumIndC} \mTVectorFactor_\mPoint^\sumIndC$, an analogous cancellation effect can occur when $(\mTVectorFactor_\mPoint^\sumIndB, \mTVectorFactor_\mPoint^\sumIndC)_\mPoint < 0$ for any pair of $\sumIndB$ and $\sumIndC$. In \Cref{sec:tsnmdf}, we proposed interpreting the factors $\TensorY = \exp_\mPoint \left ( \left (\max_{\sumIndA = 1, \dots, \numData} \nonnegFactor_{\sumIndA,\sumIndC}\right) \mTVectorFactor_\mPoint ^\sumIndC\right )$ as vertices of a nonlinear polyhedron in $\manifold$ that contains the original data points $\Tensor^\sumIndA$. Therefore, we argue that if $(\mTVectorFactor_\mPoint^\sumIndB, \mTVectorFactor_\mPoint^\sumIndC)_\mPoint < 0$ and the learned representation of $\log_\mPoint \Tensor^\sumIndA$ has nonzero coefficients associated with $\mTVectorFactor_\mPoint^\sumIndB$ and $\mTVectorFactor_\mPoint^\sumIndC$ (that is, the $\sumIndA$th row of $\nonnegFactor$ has nonzero $\sumIndB$ and $\sumIndC$ entries), then there exists an ambiguity in the relationship of $\Tensor^\sumIndA$ to the manifold-valued factors $\TensorY^\sumIndB$ and $\TensorY^\sumIndC$. In particular, the vertices associated to the factors may ``cancel out'' with each other, since $(\mTVectorFactor_\mPoint^\sumIndB, \mTVectorFactor_\mPoint^\sumIndC)_\mPoint < 0$. The most extreme example of this occurs when $\mTVectorFactor_\mPoint^\sumIndB = c \mTVectorFactor_\mPoint^\sumIndC$ where $c < 0$. In this case, one of the learned factors is completely redundant, which clearly impacts the effectiveness of the low-rank approximation and the interpretability of the factors. A lower-grade cancellation effect can occur whenever $(\mTVectorFactor_\mPoint^\sumIndB, \mTVectorFactor_\mPoint^\sumIndC)_\mPoint < 0$.

To mitigate this cancellation, we define $\rho:\Real \to \Real$ as 
\begin{equation}
    \rho(x) := \min\{0,x\}.
\end{equation}
Then, we define the \emph{effective} $\sumIndC$th tangent vector factor coordinate of the $\sumIndA$th data point by
\begin{align}
    \nonnegFactorCor_{\sumIndA,\sumIndC} &\coloneqq \nonnegFactor_{\sumIndA,\sumIndC} \mTVectorFactor_\mPoint^\sumIndC + \sum_{\sumIndB\neq \sumIndC}^\numFactors \rho \left( \left( \nonnegFactor_{\sumIndA,\sumIndB} \mTVectorFactor_\mPoint^\sumIndB , \frac{1}{\|\mTVectorFactor_\mPoint^\sumIndC\|_{\mPoint}} \mTVectorFactor_\mPoint^\sumIndC\right)_{\mPoint} \right) \frac{1}{\|\mTVectorFactor_\mPoint^\sumIndC\|_{\mPoint}} \mTVectorFactor_\mPoint^\sumIndC \nonumber \\
    &= \left( \nonnegFactor_{\sumIndA,\sumIndC} + \sum_{\sumIndB\neq \sumIndC}^\numFactors \nonnegFactor_{\sumIndA,\sumIndB} \frac{\rho \left( ( \mTVectorFactor_\mPoint^\sumIndB, \mTVectorFactor_\mPoint^\sumIndC )_{\mPoint} \right)}{\|\mTVectorFactor_\mPoint^\sumIndC\|_{\mPoint}^2} \right) \mTVectorFactor_\mPoint^\sumIndC.
\end{align}
So, using this adjustment, we define corrected manifold-valued factors as
\begin{equation}
    \TensorY^\sumIndC := \exp_{\mPoint}\left( (\max_{\sumIndA=1, \ldots, \numData} \nonnegFactorCor_{\sumIndA,\sumIndC}) \mTVectorFactor_\mPoint^\sumIndC \right).
\end{equation}
We apply this correction throughout our numerical experiments.

\subsubsection{Choice of Base Point of Linearization.} We can also mitigate potential cancellation effects via judicious choice of $\mPoint$, the base point of the chosen tangent space. Since CC-NMDF is a tangent space-based method, the choice of tangent space (via choice of base point $\mPoint$) can have a significant impact on many aspects of the method's performance. For any tangent space-based low rank approximation scheme, a poor choice of base point can result in lower-quality  approximations and identified by examining the exact reconstruction error along with the resulting factors. Such factors may exhibit obvious distortions from expected behavior. In the case of CC-NMDF, the factors may also suffer from cancellation effects, as described in the previous section, if the base point is not chosen carefully. 

Some tangent space-based low-rank approximations of manifold-valued data have natural choices of base points based on their Euclidean analogues; for instance, CC-SVD uses the barycenter of the data as the base point $\mPoint$ because of its relationship to Euclidean principal component analysis \citep{diepeveen2023curvature}. While there is no such analogous choice for the NMF-inspired CC-NMDF, we propose a base point motivated by the previous discussion of cancellation among the learned factors. In particular, one way to ensure that the tangent vector factors learned by CC-NMDF satisfy $(\mTVectorFactor_\mPoint^\sumIndC, \mTVectorFactor_\mPoint^\sumIndB)_\mPoint \geq 0$ is to choose a base point $\mPoint \in \manifold$ such that $(\log_\mPoint \Tensor^\sumIndC, \log_\mPoint \Tensor^\sumIndB)_\mPoint \geq 0$ for all $\sumIndC, \sumIndB \in [\numData]$, such as a point that is sufficiently distant from all of the points in the dataset. However, if such a point is used, then curvature effects can be exaggerated due to the distance between the base point and the data points. As illustrated by \eqref{eq:beta def} and \eqref{eq:relaxed-semi-nmf}, curvature effects are particularly significant for manifolds with negative curvature, since tangent space errors can be amplified on these manifolds. Therefore, in such settings, curvature correction is even more important when using base points satisfying the above heuristic.
