\section{Interpretable Factorization of Manifold-Valued Data.}
\label{sec:factorization}
We first seek to adapt the Euclidean NMF to the manifold setting in hopes of obtaining similarly interpretable factors. However, there are two main obstacles to this goal. First, the original Euclidean NMF is only defined for nonnegative input data, and there is no characteristic clearly equivalent to nonnegativity for general manifold-valued data. Second, the interpretability of NMF is derived from its additive nature, which depends on the linearity of the underlying domain, and the non-Euclidean settings that require the use of tools from Riemannian geometry are clearly nonlinear. We address the first obstacle by turning to an NMF variant known as semi-nonnegative matrix factorization \citep{ding2008convex}, which we refer to as semi-NMF, and is defined for (Euclidean) data of mixed sign. We address the second obstacle by proposing interpretation of resulting manifold-valued factors that does not rely on linear structure in the data domain. 
\subsection{Euclidean Semi-NMF and Exact Formulation on Manifolds.}
For Euclidean data of mixed sign, \cite{ding2008convex} proposed a semi-nonnegative matrix factorization that only constrains the coefficients to be nonnegative and allows the learned basis factors to be of mixed sign. Due to the nonnegativity of the coefficients, the resulting factorizations tend to retain the interpretability of standard NMF. Given a data matrix $\Matrix \in \Real^{\numData \times \dimInd}$ and a desired rank $\numFactors$, semi-NMF computes the low-rank approximation
\begin{equation}
\label{eq:eucl SNMF}
    \Matrix \approx \nonnegFactor \factorCoords
\end{equation}
where $\nonnegFactor \in \Real_{\geq 0}^{\numData \times \numFactors}$ and $\factorCoords\in\Real^{\numFactors \times \dimInd}$ and $\nonnegFactor$ and $\factorCoords$ are given by

\begin{equation}
\label{eq:eucl SNMF opt prob}
\underset{\substack{\nonnegFactor \in \Real_{\geq 0}^{\numData \times \numFactors}\\ \factorCoords\in\Real^{\numFactors \times \dimInd}}}{\argmin} \left\|\Matrix - \nonnegFactor \factorCoords\right\|_F^2.
\end{equation}

If we consider $\Matrix$ as a collection of $\numData$ points in $\Real^\dimInd$, we can use the idea of only constraining the learned coefficients for each point to be nonnegative to formulate a similar factorization of a manifold-valued dataset $\{\Tensor^\sumIndA\}_{\sumIndA = 1}^\numData \subset \manifold$ as follows. Given a desired rank $\numFactors$ and a point of linearization $\mPoint \in \manifold$, we seek to find

\begin{equation}
    \label{eq:manifold SNMF exact}
    \underset{\substack{\nonnegFactor \in \Real_{\geq 0}^{\numData \times \numFactors} \\
    \mTVectorFactor_\mPoint^1, \dots, \mTVectorFactor_\mPoint^\numFactors \in \tangent_\mPoint\manifold}}{\argmin} \sum_{i=1}^\numData \distance_\manifold \left(\Tensor^\sumIndA, \exp_\mPoint \left(\sum_{\sumIndC=1}^\numFactors \nonnegFactor_{\sumIndA,\sumIndC} \mTVectorFactor_\mPoint^\sumIndC\right)\right)^2.
\end{equation}

Defining such a factorization in this way requires that the learned factors actually lie on a chosen tangent space $\tangent_\mPoint \manifold$. Note that this formulation reduces to the Euclidean SNMF when $\manifold$ is $\Real^\dimInd$ and $\mPoint = 0$. In this case, the learned tangent vector factors $\mTVectorFactor_\mPoint^\sumIndC$ are the rows of $\factorCoords$. 
In the general case, we define the corresponding manifold-valued factors $\{\TensorY^\sumIndC\}_{\sumIndC = 1}^\numFactors$ by
\begin{equation}
\label{eq:manifold valued factors}
    \TensorY^\sumIndC \coloneqq \exp_\mPoint \left ( \left(\max_{\sumIndA = 1, \dots, \numData} \nonnegFactor_{\sumIndA, \sumIndC} \right) \mTVectorFactor_\mPoint^\sumIndC \right )
\end{equation}
and we interpret these manifold-valued factors as the vertices of a nonlinear polyhedron in $\manifold$ that approximately encapsulates the original data. Then, the learned coefficients for each of the original data points, i.e. the rows of $\nonnegFactor$, locate each data point with respect to each vertex and the base point of linearization and hence serve as geometric descriptors. 

While the formulation given in \eqref{eq:manifold SNMF exact} is a natural extension of the Euclidean semi-NMF to the setting of manifold-valued data, in practice it is computationally expensive to solve via direct optimization on $\manifold$. In the following section, we develop feasible approximations to the exact problem.

\subsection{Tangent Space and Curvature Corrected Nonnegative Manifold Data Factorizations.}
\label{sec:tsnmdf}
Because of the computational expense of direct optimization on $\manifold$, we first propose to solve an approximation to the exact problem \eqref{eq:manifold SNMF exact} in $\tangent_\mPoint \manifold$. We rewrite the exact problem as
\begin{equation}
     \label{eq:manifold snmf rewritten}   \argmin_{\substack{\nonnegFactor \in \Real_{\geq 0}^{\numData \times \numFactors}\\
    \mTVectorFactor_\mPoint^1, \dots, \mTVectorFactor_\mPoint^\numFactors \in \tangent_\mPoint \manifold} } \sum_{\sumIndA=1}^\numData  \distance_\manifold\left( \exp_\mPoint\left( \log_\mPoint \Tensor^\sumIndA\right), \exp_\mPoint \left(\sum_{\sumIndC=1}^\numFactors \nonnegFactor_{\sumIndA,\sumIndC} \mTVectorFactor_\mPoint^\sumIndC\right)\right)^2.
\end{equation}
A natural approach to approximating this formulation is to linearize the problem and solve
\begin{equation}
\label{eq:tnmdf problem}
    \argmin_{\substack{\nonnegFactor \in \Real_{\geq 0}^{\numData \times \numFactors}\\
    \mTVectorFactor_\mPoint^1, \dots, \mTVectorFactor_\mPoint^\numFactors \in \tangent_\mPoint \manifold} } \sum_{\sumIndA=1}^\numData   \left \| \log_\mPoint \Tensor^\sumIndA - \sum_{\sumIndC=1}^\numFactors \nonnegFactor_{\sumIndA, \sumIndC} \mTVectorFactor_\mPoint^\sumIndC\right\|_\mPoint^2.
\end{equation}
We refer to the loss function considered in \eqref{eq:tnmdf problem} as the \emph{tangent space error}, as it is the error of approximation in $\tangent_\mPoint \manifold$. Accordingly, we refer to the corresponding approximation 
\begin{equation}
\label{eq:tnmdf}
    \log_\mPoint \Tensor^\sumIndA \approx \sum_{\sumIndC = 1}^\numFactors \nonnegFactor_{\sumIndA, \sumIndC} \mTVectorFactor_\mPoint ^\sumIndC,
\end{equation}
where $\nonnegFactor$ and $\{\mTVectorFactor_\mPoint^\sumIndC\}_{\sumIndC = 1}^\numFactors \subset \tangent_\mPoint \manifold$ are solutions of \eqref{eq:tnmdf problem}, as \emph{tangent space nonnegative manifold data factorization} (T-NMDF), and we define associated manifold-valued factors as in \eqref{eq:manifold valued factors}. Crucially, T-NMDF can be computed solely on $\tangent_\mPoint \manifold$, which greatly speeds up computation due to linearity of $\tangent_\mPoint \manifold$. However, the linearization of the data required to work solely on $\tangent_\mPoint \manifold$ may incur additional approximation error due to the global geometry of $\manifold$ \citep{diepeveen2023curvature}. In particular, the solutions of \eqref{eq:tnmdf problem} are not guaranteed to coincide with the solutions of the exact problem \eqref{eq:manifold SNMF exact}. Hence, T-NMDF may produce reconstructions that are not as faithful to the original data as solutions of \eqref{eq:manifold SNMF exact}, and the manifold-valued factors learned by T-NMDF may suffer distortions that impede their interpretability in the original data domain. To correct these issues while retaining the computational feasibility of T-NMDF, we propose a \emph{curvature corrected nonnegative manifold data factorization} (CC-NMDF) as follows.

By theory developed by \citealp{diepeveen2023curvature}, we know that the exact reconstruction error on $\manifold$ minimized in \eqref{eq:manifold SNMF exact} is dominated by \emph{curvature corrected} tangent space error, rather than the linearized error considered in \eqref{eq:tnmdf problem}. In particular, consider a symmetric Riemannian manifold $(\manifold, (\cdot, \cdot) )$ with \emph{curvature tensor} at a point $\mPoint$ denoted by $\curvature_\mPoint(\cdot, \cdot)(\cdot):\tangent_\mPoint \manifold \times \tangent_\mPoint \manifold \times \tangent_\mPoint \manifold \to \tangent_\mPoint \manifold$. For each $\sumIndA \in [\numData]$ let $\{ \Theta_{\mPoint}^{(\sumIndA),\sumIndB} \}_{\sumIndB=1}^\dimInd \subset \tangent_\mPoint \manifold$ be an orthonormal frame that diagonalizes the operator
\begin{equation}
    \Theta_\mPoint \mapsto \curvature_\mPoint \left(\Theta_\mPoint, \log_{\mPoint} \Tensor^\sumIndA\right) \log_{\mPoint} \Tensor^\sumIndA
\end{equation}
with respective eigenvalues $\kappa_{\sumIndA,\sumIndB}$ and define $\beta: \Real \to \Real$ as
\begin{equation}
\label{eq:beta def}
    \beta(\kappa):=\left\{\begin{array}{cl}
\frac{\sinh (\sqrt{-\kappa})}{\sqrt{-\kappa}}, & \kappa<0, \\
1, & \kappa=0, \\
\frac{\sin (\sqrt{\kappa})}{\sqrt{\kappa}}, & \kappa>0 .
\end{array}\right.
\end{equation}
Then, we have by \cite{diepeveen2023curvature}, Thm.~3.4 that
\begin{equation}
    \sum_{\sumIndA=1}^\numData  \distance_{\manifold} \left(\Tensor^\sumIndA, \exp_{\mPoint}\left(\sum_{\sumIndC=1}^\numFactors \nonnegFactor_{\sumIndA,\sumIndC} \mTVectorFactor_\mPoint^\sumIndC\right)\right)^2 = \sum_{\sumIndA=1}^\numData  \sum_{\sumIndB=1}^\dimInd \beta(\kappa_{\sumIndA,\sumIndB})^2 \left(\sum_{\sumIndC=1}^\numFactors \nonnegFactor_{\sumIndA,\sumIndC} \mTVectorFactor_\mPoint^\sumIndC - \log_{\mPoint} \Tensor^\sumIndA, \Theta_{\mPoint}^{(\sumIndA),\sumIndB}\right)_\mPoint^2 + \mathcal{O}(\epsilon^3)
    \label{eq:relaxed-semi-nmf}
\end{equation}
where 
\begin{equation}
    \epsilon \coloneqq \sqrt{\sum_{\sumIndA=1}^\numData \left\|\sum_{\sumIndC=1}^\numFactors \nonnegFactor_{\sumIndA,\sumIndC} \mTVectorFactor_\mPoint^\sumIndC - \log_{\mPoint} \Tensor^\sumIndA\right\|_\mPoint^2}.
\end{equation}
Following \cite{diepeveen2023curvature}, we refer to the leading term of the right hand side of \eqref{eq:relaxed-semi-nmf} as the \emph{curvature corrected approximation error}. 
This theory implies that when $\manifold$ has nonzero curvature, the curvature corrected approximation error is a better approximation of the exact reconstruction error on $\manifold$ than the tangent space error. In fact, \cite{diepeveen2023curvature} found that for manifold-valued singular value decompositions, minimizing the curvature corrected approximation error resulted in better performance (on the basis of exact reconstruction error) than minimizing the tangent space error.  
This suggests that solving the curvature corrected problem
\begin{equation}
    \label{eq:cc snmdf prob}
    \argmin_{\substack{\nonnegFactor \in \Real_{\geq 0}^{\numData \times \numFactors} \\
        \mTVectorFactor_\mPoint^1, \dots, \mTVectorFactor_\mPoint^\numFactors \in \tangent_\mPoint \manifold}} \sum_{\sumIndA=1}^\numData  \sum_{\sumIndB=1}^\dimInd \beta(\kappa_{\sumIndA,\sumIndB})^2 \left(\sum_{\sumIndC=1}^\numFactors \nonnegFactor_{\sumIndA,\sumIndC} \mTVectorFactor_\mPoint^\sumIndC - \log_{\mPoint} \Tensor^\sumIndA, \Theta_{\mPoint}^{(\sumIndA),\sumIndB}\right)_\mPoint^2
\end{equation}
will give a geometry-aware approximate minimizer of \eqref{eq:manifold SNMF exact} that only requires working on $\tangent_\mPoint \manifold$ rather than $\manifold$ itself. We refer to the corresponding approximation as \emph{curvature corrected nonnegative manifold data factorization} (CC-NMDF). 