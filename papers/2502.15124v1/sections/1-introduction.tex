\section{Introduction.}
\label{sec:introduction}
Many scientific techniques collect data with natural representations in non-Euclidean domains. For example, scientific imaging methods such as electron backscatter diffraction imaging \citep{adams1993orientation} and diffusion tensor magnetic resonance imaging \citep{basser1994mr} generate manifold-valued data. For the former, the data live in $SO(3)$, the 3D rotation group, and for the latter, the data live in $\mathcal{P}(3)$, the space of $3 \times 3$ symmetric positive definite matrices. Molecules \citep{kearnes2016molecular}, metabolic pathways \citep{jeong2000large}, and social relationships \citep{newman2002random} are just a few examples of phenomena that can be encoded as graphs. Data collected from a variety of domains can be embedded into Riemannian manifolds to uncover latent non-Euclidean structure \citep{wilson2014spherical}. Additionally, high-dimensional Euclidean data can be modeled as instead being drawn from an unknown, lower-dimensional non-Euclidean domain, enabling new methods of visualization, compression, and further analysis \citep{bronstein2017geometric,puchkin2022structure}. 

In particular, the data setting of \emph{symmetric Riemannian manifolds} is an exciting realm in which to develop data analysis methods for two main reasons. First, the class of these symmetric spaces includes manifolds that appear in many existing scientific techniques, such as those mentioned above,  and manifolds that are popular targets for embeddings of high-dimensional Euclidean data, such as spherical and hyperbolic spaces \citep{lopez2021symmetric,keller2020hydra,wilson2014spherical}. Hence, methods developed for such manifold-valued data and embeddings can be used in a variety of application domains. Second, symmetric spaces have mathematical properties that enable the design of tractable algorithms that respect the geometry of the spaces. Therefore, practical and effective data analysis methods can be developed for these settings. 

For data that have been collected from or embedded in Riemannian manifolds, data analysis methods should account for and leverage the underlying geometry of these manifolds. At the same time, evaluating manifold-specific mappings can be computationally expensive. One approach is to exploit nice geometric properties of a specific manifold domain, such as the sphere \citep{li1999multiscale, narcowich1996nonstationary,tabaghi2023principal}, to enable computationally feasible yet geometrically faithful adaptations of Euclidean data processing techniques. Other methods utilize a \emph{tangent space} of the manifold to approximate the underlying geometry while enjoying the efficiency enabled by working on linear spaces \citep{fletcher2004principal,fletcher2004principal2,ho2013nonlinear,yin2015nonlinear}. 

A foundational family of  techniques for processing Euclidean data is that of \emph{low rank approximation}, motivated by applications such as visualization, compression, feature extraction, and improving the performance of subsequent machine learning methods. Consequently, there is growing interest in developing methods for manifold-valued data that emulate the behavior of low rank approximations and are adapted for these domains. For example, several ideas such as Principal Geodesic Analysis (PGA) \citep{fletcher2004principal,fletcher2004principal2} and the nested subspaces framework \citep{jung2012analysis,yang2021nested} are motivated by the utility of Principal Component Analysis (PCA) for Euclidean data. 

A popular low-rank approximation technique for interpretable feature extraction from nonnegative Euclidean data is \emph{nonnegative matrix factorization} (NMF). For a nonnegative data matrix $\Matrix \in \Real_{\geq 0}^{\numData \times \dimInd}$ and given rank $\numFactors$, NMF is an approximation
\begin{equation}
    \Matrix \approx \mathbf{H} \mathbf{W} 
\end{equation}
where $\mathbf{H} \in \Real_{\geq 0}^{\numData \times \numFactors}, \mathbf{W} \in \Real_{\geq 0}^{\numFactors \times \dimInd}$. By enforcing a nonnegativity constraint on $\mathbf{H}$ and $\mathbf{W}$, NMF extracts additive factors (the rows of $\mathbf{W}$) from high-dimensional Euclidean data that tend to be interpretable by human experts across a variety of data domains, which makes NMF a simple yet powerful tool for tasks such as topic modeling and image segmentation \citep{Lee1999}. One reason for developing methods adapted for manifold-valued data is to retain the structure of the original data domain, so readily interpretable low-rank approximations on manifolds, such as adaptations of NMF, are an exciting avenue for practical applications.

As previously outlined, there are two main types of methods for manifold-valued data. Typically, methods that account for global geometry are specific to particular manifolds in order to leverage known geometric information. While these methods remain faithful to the nonlinear structure of their domains, they are difficult to practically generalize to broader families of manifolds. On the other hand, methods that work on tangent spaces can more easily adapt frameworks originally defined for Euclidean data for many manifold domains. However, tangent space-based methods generally introduce large errors, since curvature information is lost when the data are linearized. These drawbacks present considerable challenges to developing a general NMF-type factorization for the manifold setting. 

In this work, we propose \emph{curvature corrected nonnegative manifold data factorization} (CC-NMDF) as a geometry-aware analogue of NMF for manifold-valued data. We formulate an interpretable factorization for data drawn from any symmetric Riemannian manifold, apply a curvature correction to a fast tangent space-based algorithm, and propose an iterative algorithm for computing CC-NMDF. We demonstrate our method on a real-world data set collected via diffusion tensor magnetic resonance imaging.

\subsection{Related Work.}
In this section, we survey relevant connections between low-rank approximation methods for manifold-valued data and NMF. As previously mentioned, PGA \citep{fletcher2004principal, fletcher2004principal2} is a general method that adapts PCA to the manifold setting, and practical algorithms for implementing PGA reduce to performing PCA on a particular tangent space of the underlying manifold. The learned geodesic submanifolds are intended to maximize reconstruction fidelity, analogous to the role of the linear subspaces learned by PCA. Other approaches construct nested submanifolds using manifold-specific properties of settings such as spherical and hyperbolic surfaces \citep{jung2012analysis,tabaghi2023principal} and Grassmann manifolds \citep{curry2019principal,yang2021nested} to achieve computationally feasible algorithms. However, these methods do not yield submanifolds that are easily interpreted in the original data domain. 

In the NMF literature, some methods have been proposed to account for possible nonlinear structure when computing otherwise standard Euclidean NMF. For instance, \cite{cai2008non}, \cite{huang2014robust}, and \cite{lu2012manifold} construct a nearest-neighbors graph using affinities between input data points (which are high-dimensional vectors in $\Real^D$) and use the corresponding graph Laplacian to regularize NMF in hopes of preserving unknown manifold structure in the data. \cite{he2020low} propose a similarly regularized NMF whose basis matrix is additionally assumed to lie on a Stiefel manifold in order to reduce redundancy in the basis factors. These methods do not have prior knowledge of the underlying manifold the data may be drawn from; instead, they seek to approximate the unknown  geometry through data-driven graphs. Hence, the resulting factors cannot be guaranteed to lie on the underlying manifold, and these methods do not incorporate known geometry of application domains. 

There has been some work on NMF-type methods for data drawn from known manifolds. For example, \cite{ho2013nonlinear} propose a version of sparse coding and dictionary learning for manifold-valued data. However, their methods enforce sparsity rather than nonnegativity for the learned coefficients, so while the dictionary atoms do lie on the manifold the data are drawn from, they cannot be easily interpreted themselves. Also, the algorithms provided require computation of geodesics, so feasible algorithms are provided for only two manifold settings. 

\subsection{Contributions.}
In this work, we adapt a framework for curvature corrected low-rank approximation of manifold-valued data \citep{diepeveen2023curvature} to develop a geometry-aware yet tangent space-based factorization for data drawn from symmetric Riemannian manifolds that is analogous to NMF. Then, we propose an interpretation for the resulting factors in the original manifold domain. We develop an iterative algorithm to compute our CC-NMDF that only requires a multiplicative update and solving a linear system. We also discuss adjustments to the algorithm to improve the interpretability of the computed factors. We demonstrate our method on a real-world dataset and compare it with other low-rank approximation methods to investigate curvature effects and the quality of the resulting factors. 

\subsection{Outline.}
In \Cref{sec:notation} we summarize notation used throughout this paper and necessary concepts from Riemannian geometry. Then, in \Cref{sec:factorization} we introduce a formulation for nonnegative factorization of manifold-valued data, and we apply a curvature correction to account for the underlying geometry of the domain, resulting in our CC-NMDF. Next, in \Cref{sec:algorithms}, we provide tractable algorithms for computing CC-NMDF for data drawn from any symmetric Riemannian manifold, and we discuss algorithm considerations such as initialization. In \Cref{sec:numerics}, we apply our method to a diffusion tensor magnetic resonance imaging dataset, evaluate the effects of algorithm parameter choices, and compare CC-NMDF to other low-rank approximation methods for manifold-valued data on the basis of reconstruction and interpretability of the resulting factors. Finally, in \Cref{sec:conclusions}, we summarize our findings. 


