\section{Numerics.}
\label{sec:numerics}
To characterize the behavior and applications of the CC-NMDF proposed in \Cref{sec:ccnmdf}, we evaluate our methods on real-world data. Our goal is to compare our scheme to other low-rank approximations for manifold-valued data on the basis of approximation error and interpretability of the resulting factors. We first investigate the effects of different choices of base point and the factor correction proposed in \Cref{sec:algorithm-considerations}. Then, we compare the performance of CC-NMDF to other low-rank approximation schemes for manifold-valued data on the basis of reconstruction quality and interpretability of manifold-valued factors. The code for our algorithms and experiments is available at \url{https://github.com/joycechew/NMDF}.

\subsection{Overview of the Data Set and Experiment Commonalities.} We use a data set of diffusion tensor magnetic resonance imaging (DTI) of the adult human brain \citep{zhang2018evaluation,qi2021regionconnect}. In these experiments, we take a subset of the entire brain to make the computation feasible, visualized in \Cref{fig:IIT_dataset}. Each voxel is a $3 \times 3$ symmetric positive definite matrix, so the data lie on $\mathcal{P}(3)$, which is a symmetric Riemannian manifold with non-positive curvature. We focus on a non-compact manifold to investigate our proposed heuristic for choosing a base point. On a compact manifold such as a sphere, it may not be possible to satisfy this heuristic and therefore may become more difficult to identify a good base point. The second reason we focus on data drawn from $\mathcal{P}(3)$ is that we expect from the results of \cite{diepeveen2023curvature} that curvature correction is most notable for non-positively curved spaces, and we want to investigate curvature effects for the proposed factorizations. 

\begin{figure}[t!]
    \centering
    \includegraphics[width=0.6\linewidth]{figures/IIT_dataset_horiz.pdf}
    \caption{Visualization of the DTI data set (left) and two ``slices'' from the data (right).}
    \label{fig:IIT_dataset}
\end{figure}

Since the data are arranged as a single mode-3 tensor, we extract and unfold $4 \times 4 \times 4$ regions of voxels and consider each unfolded region an individual data point. Hence, in our processed data set, each data point is on the power manifold $\mathcal{M} = \mathcal{P}(3)^{64}$, and we have $\numData = 117$. We choose to arrange the data in this way to explore the effect of low-rank approximations since this choice of $\mathcal{M}$ has dimension $384$, as opposed to the manifold $\mathcal{P}(3)$, which has dimension $6$. Also, processing the data this way preserves some of the spatial relationships between adjacent voxels in each region, allowing for study of spatial patterns in the data set. 

For each computation of CC-NMDF, we apply the correction described in \Cref{sec:algorithm-considerations} for rendering manifold-valued factors. We perform decompositions for 12 values of $\numFactors$ linearly spaced in $[2,35]$. We run 50 iterations of each iterative method (T-NMDF and CC-NMDF) with $\texttt{maxSubIter} = 5$ for CC-NMDF, and we take $\delta = 0.1$.


Throughout our experiments, we employ the following error metric to evaluate the quality of different low-rank approximations of the data set. First, we define the data power manifold $\manifoldB = \manifold^{N}$ and use the distance inherited from the power manifold structure
\begin{equation}
    \distance_{\manifoldB}(\Tensor, \TensorY) = \sqrt{\sum_{\sumIndA=1}^\numData \distance_{\manifold}(\Tensor^\sumIndA, \TensorY^\sumIndA)^2}
\end{equation}
as our error metric. That is, given data $\{\Tensor^\sumIndA\}_{\sumIndA=1}^\numData \subset \manifold$ and tangent space approximations $\Xi_\mPoint^\sumIndA \approx \log_\mPoint \Tensor^\sumIndA$, we take the approximation error to be
\begin{equation}
    \distance_{\manifoldB} (\Tensor, \Xi_\mPoint) = \sqrt{\sum_{\sumIndA=1}^\numData \distance_{\manifold}(\Tensor^\sumIndA, \exp_{\mPoint}(\Xi_{\mPoint}^\sumIndA))^2}.
\end{equation}


\subsection{Choice of Base Point for Linearization.}
\label{sec:numerics base point}
We first explore the effect of the base point for linearization on the resulting CC-NMDF. A natural first choice for the base point would be the barycenter of the data, since this choice is successfully used for SVD-like approximations of manifold-valued data \citep{diepeveen2023curvature}. However, as discussed in \Cref{sec:algorithm-considerations}, this choice may result in redundant factors or otherwise negatively impact the quality of the factors. Another possible choice of base point is a point $\mPoint$ on the manifold sufficiently distant from each data point such that $(\log_{\mPoint} \Tensor^\sumIndA, \log_{\mPoint} \Tensor^\sumIndB)_{\mPoint} > 0$ for all $\sumIndA \neq \sumIndB$. Often, one suitable choice of this kind of base point is a point close to zero on the manifold. In this case, we use the point on $\mathcal{P}(3)^{64}$ with every entry equal to $10^{-5} \times I_3$ as such a point, where $I_3$ is the $3 \times 3$ identity matrix, and we denote this point by $\mPointB$. For comparison, we let $\mPointC$ denote the barycenter of the data, and we compute the CC-NMDF of our data set using $\mPointB$ and $\mPointC$ as the base points of linearization. We denote these as CC-NMDF($\mPointB$) and CC-NMDF($\mPointC$), respectively, in our results. 

\begin{figure}[t!]
    \centering
    \includegraphics[width=0.48\linewidth]{figures/base_point_comparison.png}
    \caption{The error incurred by CC-NMDF using approximate zero ($\mPointB$) and the data barycenter ($\mPointC$) as the base point of linearization, plotted against approximation rank ($\numFactors$).}
    \label{fig:cc_q0_qmean_loss}
\end{figure}
\begin{figure}[t!]
    \centering
    \begin{subfigure}{0.48\linewidth}
        \includegraphics[width=\linewidth]{figures/cc_q0_factors.png}
        \caption{}
    \end{subfigure}
    \hfill
    \begin{subfigure}{0.48\linewidth}
        \includegraphics[width=\linewidth]{figures/cc_qmean_factors.png}
        \caption{}
        \label{fig:cc_qmean_factors}
    \end{subfigure}
    \caption{The factors obtained from rank-20 CC-NMDF using (a) $\mPointB$, approximately zero, and (b) $\mPointC$, the data barycenter, as the base point of linearization. (b) shows a clear deterioration in the interpretability of the factors when compared to (a) and the original data set.}
    \label{fig:cc_q0_qmean_factors}
\end{figure}

We see that using $\mPointC$ as the base point of linearization results in a better approximation error (\Cref{fig:cc_q0_qmean_loss}), but using $\mPointB$ results in much more interpretable manifold-valued factors (\Cref{fig:cc_q0_qmean_factors}). The choice of base point $\mPoint$ impacts the effect of distortion due to the geometry of $\manifold$ on the linearized data $\{\log_\mPoint \Tensor^\sumIndA\}_{\sumIndA=1}^\numData$ that is used in the computation of low-rank approximations. A base point that is further from the manifold data points will naturally induce more distortion in the linearization itself, so we expect that CC-NMDF($\mPointC$) will have a better approximation error that CC-NMDF($\mPointB$). However, the manifold-valued factors arising from CC-NMDF($\mPointC$) as the base point of linearization are subject to distortions that do not appear to be representative of actual features in the original data set, and the individual factors appear to generally be more uniform and lack detail that is present in the manifold-valued factors from CC-NMDF($\mPointB$). Hence, on the basis of interpretability of the manifold-valued factors, we use $\mPoint$ as the base point of linearization for manifold NMF methods in the following section.

\subsection{Comparison with T-NMDF and CC-SVD.}
\label{sec:numerics vs naive}
\begin{figure}[t!]
    \centering
    \begin{subfigure}{0.48\linewidth}
        \includegraphics[width=\linewidth]{figures/methods_comparison.png}
    \caption{}
    \label{fig:naive_vs_cc_loss}
    \end{subfigure}
    \hfill
    \begin{subfigure}{0.48\linewidth}
        \includegraphics[width=\linewidth]{figures/cc_q0_factors.png}
        \caption{}
        \label{fig:cc_q0_factors}
    \end{subfigure}
    \begin{subfigure}{0.48\linewidth}
        \includegraphics[width=\linewidth]{figures/naive_q0_factors.png}
        \caption{}
        \label{fig:naive_q0_factors}
    \end{subfigure}
    \hfill
    \begin{subfigure}{0.48\linewidth}
        \includegraphics[width=\linewidth]{figures/ccsvd_qmean_factors.png}
        \caption{}
        \label{fig:CC-SVD_qmean_factors}
    \end{subfigure}
    \caption{(a) Approximation error incurred by CC-NMDF, T-NMDF, and CC-SVD for approximation ranks 2-35. (b) Manifold-valued factors obtained by CC-NMDF. (c) Manifold-valued factors obtained by T-NMDF. (d) Manifold-valued factors obtained by CC-SVD.}
\end{figure}
Next, we compare CC-NMDF to T-NMDF, the uncorrected tangent space-based method described in \Cref{alg:t-SNMDF}, and CC-SVD, a curvature-corrected singular value decomposition for manifold-valued data \citep{diepeveen2023curvature}. T-NMDF is simpler to implement and requires less memory and computational time than CC-NMDF, as it does not require any manifold mappings or expansion into a basis of $\tangent_\mPoint \manifold$. However, as a purely tangent space-based method, T-NMDF does not account for the curvature of the underlying manifold. 


In \Cref{fig:naive_vs_cc_loss}, we see that the approximation error on the manifold is moderately better for CC-NMDF compared to T-NMDF. Since we expect non-negligible curvature effects on $\mathcal{P}(3)^{64}$, it is unsurprising that the curvature-corrected method results in a better approximation error. Furthermore, several factors that result from T-NMDF appear to be similar to each other (see \Cref{fig:naive_q0_factors}), indicating that T-NMDF does not capture as many features of the data set as CC-NMDF does. Additionally, some of the factors appear to suffer from distortions that do not correspond to features in the original data set, such as the factors in the upper left and upper right of \Cref{fig:naive_q0_factors}. Therefore, the curvature correction incorporated in CC-NMDF improves both the manifold approximation error and interpretability of the resulting factors. 

It is unsurprising that CC-SVD has the best approximation error on the manifold, as it is designed to emulate the behavior of the Euclidean SVD, and it is the solution of an unconstrained low-rank approximation problem. However, in \Cref{fig:CC-SVD_qmean_factors}, we see that the factors obtained via CC-SVD (\Cref{fig:CC-SVD_qmean_factors}) sharply contrast with the factors obtained via CC-NMDF (\Cref{fig:cc_q0_factors}). In general, the CC-SVD factors do not correspond well to any of the individual regions in the original data set. Furthermore, the factors with the smallest singular values (shown in the bottom two rows of \Cref{fig:CC-SVD_qmean_factors}) are extremely distorted, indicating that they capture either noise or other pathologies in the data. 
