@article{su2024conflictbank,
  title={ConflictBank: A Benchmark for Evaluating the Influence of Knowledge Conflicts in LLM},
  author={Su, Zhaochen and Zhang, Jun and Qu, Xiaoye and Zhu, Tong and Li, Yanshu and Sun, Jiashuo and Li, Juntao and Zhang, Min and Cheng, Yu},
  journal={arXiv preprint arXiv:2408.12076},
  year={2024}
}
@article{shi2023trusting,
  title={Trusting your evidence: Hallucinate less with context-aware decoding},
  author={Shi, Weijia and Han, Xiaochuang and Lewis, Mike and Tsvetkov, Yulia and Zettlemoyer, Luke and Yih, Scott Wen-tau},
  journal={arXiv preprint arXiv:2305.14739},
  year={2023}
}

@article{zhou2023context,
  title={Context-faithful prompting for large language models},
  author={Zhou, Wenxuan and Zhang, Sheng and Poon, Hoifung and Chen, Muhao},
  journal={arXiv preprint arXiv:2303.11315},
  year={2023}
}

@article{xu2024knowledge,
  title={Knowledge Conflicts for LLMs: A Survey},
  author={Xu, Rongwu and Qi, Zehan and Wang, Cunxiang and Wang, Hongru and Zhang, Yue and Xu, Wei},
  journal={arXiv preprint arXiv:2403.08319},
  year={2024}
}

@article{marjanovic2024internal,
  title={From Internal Conflict to Contextual Adaptation of Language Models},
  author={Marjanovi{\'c}, Sara Vera and Yu, Haeun and Atanasova, Pepa and Maistro, Maria and Lioma, Christina and Augenstein, Isabelle},
  journal={arXiv preprint arXiv:2407.17023},
  year={2024}
}

@article{renze2024effect,
  title={The effect of sampling temperature on problem solving in large language models},
  author={Renze, Matthew and Guven, Erhan},
  journal={arXiv preprint arXiv:2402.05201},
  year={2024}
}


@article{thakur2024verigen,
  title={Verigen: A large language model for verilog code generation},
  author={Thakur, Shailja and Ahmad, Baleegh and Pearce, Hammond and Tan, Benjamin and Dolan-Gavitt, Brendan and Karri, Ramesh and Garg, Siddharth},
  journal={ACM Transactions on Design Automation of Electronic Systems},
  volume={29},
  number={3},
  pages={1--31},
  year={2024},
  publisher={ACM New York, NY}
}

@article{wang2023study,
  title={Study the correlation between the readme file of GitHub projects and their popularity},
  author={Wang, Tianlei and Wang, Shaowei and Chen, Tse-Hsun Peter},
  journal={Journal of Systems and Software},
  volume={205},
  pages={111806},
  year={2023},
  publisher={Elsevier}
}

@article{shi2024thorough,
  title={A thorough examination of decoding methods in the era of llms},
  author={Shi, Chufan and Yang, Haoran and Cai, Deng and Zhang, Zhisong and Wang, Yifan and Yang, Yujiu and Lam, Wai},
  journal={arXiv preprint arXiv:2402.06925},
  year={2024}
}

@article{li2024dawn,
  title={The dawn after the dark: An empirical study on factuality hallucination in large language models},
  author={Li, Junyi and Chen, Jie and Ren, Ruiyang and Cheng, Xiaoxue and Zhao, Wayne Xin and Nie, Jian-Yun and Wen, Ji-Rong},
  journal={arXiv preprint arXiv:2401.03205},
  year={2024}
}

@article{wang2024self,
  title={Self-Consistency Boosts Calibration for Math Reasoning},
  author={Wang, Ante and Song, Linfeng and Tian, Ye and Peng, Baolin and Jin, Lifeng and Mi, Haitao and Su, Jinsong and Yu, Dong},
  journal={arXiv preprint arXiv:2403.09849},
  year={2024}
}

@inproceedings{ahmed2023better,
  title={Better patching using LLM prompting, via Self-Consistency},
  author={Ahmed, Toufique and Devanbu, Premkumar},
  booktitle={2023 38th IEEE/ACM International Conference on Automated Software Engineering (ASE)},
  pages={1742--1746},
  year={2023},
  organization={IEEE}
}

@article{cliff1993dominance,
  title={Dominance statistics: Ordinal analyses to answer ordinal questions.},
  author={Cliff, Norman},
  journal={Psychological bulletin},
  volume={114},
  number={3},
  pages={494},
  year={1993},
  publisher={American Psychological Association}
}

@inproceedings{santos2020predicting,
  title={Predicting software defects with explainable machine learning},
  author={Santos, Geanderson and Figueiredo, Eduardo and Veloso, Adriano and Viggiato, Markos and Ziviani, Nivio},
  booktitle={Proceedings of the XIX Brazilian Symposium on Software Quality},
  pages={1--10},
  year={2020}
}

@article{rajbahadur2021impact,
  title={The impact of feature importance methods on the interpretation of defect classifiers},
  author={Rajbahadur, Gopi Krishnan and Wang, Shaowei and Oliva, Gustavo A and Kamei, Yasutaka and Hassan, Ahmed E},
  journal={IEEE Transactions on Software Engineering},
  volume={48},
  number={7},
  pages={2245--2261},
  year={2021},
  publisher={IEEE}
}

@misc{deepseekai2024deepseekllmscalingopensource,
      title={DeepSeek LLM: Scaling Open-Source Language Models with Longtermism}, 
      author={DeepSeek-AI and : and Xiao Bi and Deli Chen and Guanting Chen and Shanhuang Chen and Damai Dai and Chengqi Deng and Honghui Ding and Kai Dong and Qiushi Du and Zhe Fu and Huazuo Gao and Kaige Gao and Wenjun Gao and Ruiqi Ge and Kang Guan and Daya Guo and Jianzhong Guo and Guangbo Hao and Zhewen Hao and Ying He and Wenjie Hu and Panpan Huang and Erhang Li and Guowei Li and Jiashi Li and Yao Li and Y. K. Li and Wenfeng Liang and Fangyun Lin and A. X. Liu and Bo Liu and Wen Liu and Xiaodong Liu and Xin Liu and Yiyuan Liu and Haoyu Lu and Shanghao Lu and Fuli Luo and Shirong Ma and Xiaotao Nie and Tian Pei and Yishi Piao and Junjie Qiu and Hui Qu and Tongzheng Ren and Zehui Ren and Chong Ruan and Zhangli Sha and Zhihong Shao and Junxiao Song and Xuecheng Su and Jingxiang Sun and Yaofeng Sun and Minghui Tang and Bingxuan Wang and Peiyi Wang and Shiyu Wang and Yaohui Wang and Yongji Wang and Tong Wu and Y. Wu and Xin Xie and Zhenda Xie and Ziwei Xie and Yiliang Xiong and Hanwei Xu and R. X. Xu and Yanhong Xu and Dejian Yang and Yuxiang You and Shuiping Yu and Xingkai Yu and B. Zhang and Haowei Zhang and Lecong Zhang and Liyue Zhang and Mingchuan Zhang and Minghua Zhang and Wentao Zhang and Yichao Zhang and Chenggang Zhao and Yao Zhao and Shangyan Zhou and Shunfeng Zhou and Qihao Zhu and Yuheng Zou},
      year={2024},
      eprint={2401.02954},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2401.02954}, 
}

@article{wei2023magicoder,
  title={Magicoder: Source Code Is All You Need},
  author={Wei, Yuxiang and Wang, Zhe and Liu, Jiawei and Ding, Yifeng and Zhang, Lingming},
  journal={arXiv preprint arXiv:2312.02120},
  year={2023}
}

@article{daneshvar2024exploring,
  title={Exploring RAG-based Vulnerability Augmentation with LLMs},
  author={Daneshvar, Seyed Shayan and Nong, Yu and Yang, Xu and Wang, Shaowei and Cai, Haipeng},
  journal={arXiv preprint arXiv:2408.04125},
  year={2024}
}

@inproceedings{jahic2024state,
  title={State of Practice: LLMs in Software Engineering and Software Architecture},
  author={Jahi{\'c}, Jasmin and Sami, Ashkan},
  booktitle={2024 IEEE 21st International Conference on Software Architecture Companion (ICSA-C)},
  pages={311--318},
  year={2024},
  organization={IEEE}
}

@inproceedings{cabrero2024exploring,
  title={Exploring Human-AI Collaboration in Agile: Customised LLM Meeting Assistants},
  author={Cabrero-Daniel, Beatriz and Herda, Tomas and Pichler, Victoria and Eder, Martin},
  booktitle={International Conference on Agile Software Development},
  pages={163--178},
  year={2024},
  organization={Springer Nature Switzerland Cham}
}

@inproceedings{ciniselli2023source,
  title={Source code recommender systems: The practitioners' perspective},
  author={Ciniselli, Matteo and Pascarella, Luca and Aghajani, Emad and Scalabrino, Simone and Oliveto, Rocco and Bavota, Gabriele},
  booktitle={2023 IEEE/ACM 45th International Conference on Software Engineering (ICSE)},
  pages={2161--2172},
  year={2023},
  organization={IEEE}
}

@article{dakhel2023github,
  title={Github copilot ai pair programmer: Asset or liability?},
  author={Dakhel, Arghavan Moradi and Majdinasab, Vahid and Nikanjam, Amin and Khomh, Foutse and Desmarais, Michel C and Jiang, Zhen Ming Jack},
  journal={Journal of Systems and Software},
  volume={203},
  pages={111734},
  year={2023},
  publisher={Elsevier}
}

@article{paredes2023chatgpt,
  title={ChatGPT API: Brief overview and integration in Software Development},
  author={Paredes, Cristian Mauricio Gallardo and Machuca, Cristian and Claudio, Yadira Maricela Semblantes},
  journal={International Journal of Engineering Insights},
  volume={1},
  number={1},
  pages={25--29},
  year={2023}
}

@article{peng2023impact,
  title={The impact of ai on developer productivity: Evidence from github copilot},
  author={Peng, Sida and Kalliamvakou, Eirini and Cihon, Peter and Demirer, Mert},
  journal={arXiv preprint arXiv:2302.06590},
  year={2023}
}

@inproceedings{chen2024code,
  title={Code search is all you need? improving code suggestions with code search},
  author={Chen, Junkai and Hu, Xing and Li, Zhenhao and Gao, Cuiyun and Xia, Xin and Lo, David},
  booktitle={Proceedings of the IEEE/ACM 46th International Conference on Software Engineering},
  pages={1--13},
  year={2024}
}

@article{parvez2021retrieval,
  title={Retrieval augmented code generation and summarization},
  author={Parvez, Md Rizwan and Ahmad, Wasi Uddin and Chakraborty, Saikat and Ray, Baishakhi and Chang, Kai-Wei},
  journal={arXiv preprint arXiv:2108.11601},
  year={2021}
}

@article{lu2022reacc,
  title={Reacc: A retrieval-augmented code completion framework},
  author={Lu, Shuai and Duan, Nan and Han, Hojae and Guo, Daya and Hwang, Seung-won and Svyatkovskiy, Alexey},
  journal={arXiv preprint arXiv:2203.07722},
  year={2022}
}

@misc{nashid2023retrieval,
  title={Retrieval-Based Prompt Selection for Code-Related Few-Shot Learning. In 2023 IEEE/ACM 45th International Conference on Software Engineering (ICSE)},
  author={Nashid, Noor and Sintaha, Mifta and Mesbah, Ali},
  year={2023},
  publisher={IEEE}
}

@article{tan2024prompt,
  title={Prompt-based Code Completion via Multi-Retrieval Augmented Generation},
  author={Tan, Hanzhuo and Luo, Qi and Jiang, Ling and Zhan, Zizheng and Li, Jing and Zhang, Haotian and Zhang, Yuqun},
  journal={arXiv preprint arXiv:2405.07530},
  year={2024}
}


@misc{kabir2024zs4czeroshotsynthesiscompilable,
      title={ZS4C: Zero-Shot Synthesis of Compilable Code for Incomplete Code Snippets using ChatGPT}, 
      author={Azmain Kabir and Shaowei Wang and Yuan Tian and Tse-Hsun and Chen and Muhammad Asaduzzaman and Wenbin Zhang},
      year={2024},
      eprint={2401.14279},
      archivePrefix={arXiv},
      primaryClass={cs.SE},
      url={https://arxiv.org/abs/2401.14279}, 
}
@article{spracklen2024we,
  title={We Have a Package for You! A Comprehensive Analysis of Package Hallucinations by Code Generating LLMs},
  author={Spracklen, Joseph and Wijewickrama, Raveen and Sakib, AHM and Maiti, Anindya and Jadliwala, Murtuza},
  journal={arXiv preprint arXiv:2406.10279},
  year={2024}
}

@article{wu2019developers,
  title={How do developers utilize source code from stack overflow?},
  author={Wu, Yuhao and Wang, Shaowei and Bezemer, Cor-Paul and Inoue, Katsuro},
  journal={Empirical Software Engineering},
  volume={24},
  pages={637--673},
  year={2019},
  publisher={Springer}
}


@article{seaman1999qualitative,
  title={Qualitative methods in empirical studies of software engineering},
  author={Seaman, Carolyn B.},
  journal={IEEE Transactions on software engineering},
  volume={25},
  number={4},
  pages={557--572},
  year={1999},
  publisher={IEEE}
}

@article{zhang2019empirical,
  title={An empirical study of obsolete answers on stack overflow},
  author={Zhang, Haoxiang and Wang, Shaowei and Chen, Tse-Hsun and Zou, Ying and Hassan, Ahmed E},
  journal={IEEE Transactions on Software Engineering},
  volume={47},
  number={4},
  pages={850--862},
  year={2019},
  publisher={IEEE}
}

@misc{javaparser,
  author       = {Anonymous},
  title        = {JavaParser},
  year         = {2019},
  howpublished = {\url{https://github.com/javaparser/javaparser}}
}



@misc{ChatGPT,
  author       = {OpenAI},
  title        = {Introducing ChatGPT},
  year         = {2022},
  howpublished = {\url{https://openai.com/blog/chatgpt}},
  note         = {Accessed: 2023-12-28}
}

@misc{GithubCopilot,
  title = {GithubCopilot},
  howpublished = {\url{https://github.com/features/copilot}},
  note = {Accessed: 2024-04-22}
}


@inproceedings{khoury2023secure,
  title={How secure is code generated by chatgpt?},
  author={Khoury, Rapha{\"e}l and Avila, Anderson R and Brunelle, Jacob and Camara, Baba Mamadou},
  booktitle={2023 IEEE International Conference on Systems, Man, and Cybernetics (SMC)},
  pages={2445--2451},
  year={2023},
  organization={IEEE}
}

@article{dou2024s,
  title={What's Wrong with Your Code Generated by Large Language Models? An Extensive Study},
  author={Dou, Shihan and Jia, Haoxiang and Wu, Shenxi and Zheng, Huiyuan and Zhou, Weikang and Wu, Muling and Chai, Mingxu and Fan, Jessica and Huang, Caishuang and Tao, Yunbo and others},
  journal={arXiv preprint arXiv:2407.06153},
  year={2024}
}

@article{liu2024exploring,
  title={Exploring and evaluating hallucinations in llm-powered code generation},
  author={Liu, Fang and Liu, Yang and Shi, Lin and Huang, Houkun and Wang, Ruifeng and Yang, Zhen and Zhang, Li},
  journal={arXiv preprint arXiv:2404.00971},
  year={2024}
}

@misc{hallucination-in-llm,
  author = {Maryna Bilan},
  title = {Hallucinations in LLMs: What You Need to Know Before Integration},
  howpublished = {\url{https://masterofcode.com/blog/hallucinations-in-llms-what-you-need-to-know-before-integration}},
  note = {Accessed: 2024-04-22}
}

@misc{understanding-hallucination,
  author = {Frank Neugebauer},
  title = {Understanding LLM Hallucinations},
  howpublished = {\url{https://towardsdatascience.com/llm-hallucinations-ec831dcd7786}},
  note = {Accessed: 2024-04-22}
}

@misc{trusting-ai,
  author = {Yehuda Gelb},
  title = {Trusting AI-Generated Code: Determining Reality from Hallucination},
  howpublished = {\url{
      https://medium.com/checkmarx-security/trusting-ai-generated-code-determining-reality-from-hallucination-4deaa595eb41}},
  note = {Accessed: 2024-04-22}
}

@misc{generative-ai,
  author = {Sascha Heyer},
  title = {Generative AI - Understand and Mitigate Hallucinations in LLMs},
  howpublished = {\url{https://medium.com/google-cloud/generative-ai-understand-and-mitigate-hallucinations-in-llms-8af7de2f17e2}},
  note = {Accessed: 2024-04-22}
}


@misc{hallucination-categorization,
  title={A Survey on Hallucination in Large Language Models:
Principles, Taxonomy, Challenges, and Open Questions},
  author={Lei Huang and Weijiang Yu, Weitao Ma and Weihong Zhong and Zhangyin Feng and Haotian Wang and Qianglong Chen and Weihua Peng and Xiaocheng Feng and Bing Qinand Ting Liu},
  journal={arXiv preprint arXiv:2311.05232},
  year={2023}
}

@article{dhuliawala2023chain,
  title={Chain-of-verification reduces hallucination in large language models},
  author={Dhuliawala, Shehzaad and Komeili, Mojtaba and Xu, Jing and Raileanu, Roberta and Li, Xian and Celikyilmaz, Asli and Weston, Jason},
  journal={arXiv preprint arXiv:2309.11495},
  year={2023}
}

@article{detecting-factual-errors,
  title={LM vs LM: Detecting Factual Errors via Cross Examination},
  author={Roi Cohen and May Hamri and Mor Geva and Amir Globerson},
  journal={arXiv preprint arXiv:2305.13281},
  year={2023}
}

@misc{chatgpt-fall-short,
  title={Why Does ChatGPT Fall Short in Providing Truthful Answers?},
  author={Shen Zheng and Jie Huang and Kevin Chen-Chuan Chang},
  journal={arXiv preprint arXiv:2304.10513},
  year={2023}
}



@book{rajaraman2011mining,
  title={Mining of massive datasets},
  author={Rajaraman, Anand and Ullman, Jeffrey D},
  year={2011},
  publisher={Autoedicion}
}

@article{chen1998evaluation,
  title={Evaluation metrics for language models},
  author={Chen, Stanley F and Beeferman, Douglas and Rosenfeld, Roni},
  year={1998},
  publisher={Carnegie Mellon University}
}

@article{wang2022self,
  title={Self-consistency improves chain of thought reasoning in language models},
  author={Wang, Xuezhi and Wei, Jason and Schuurmans, Dale and Le, Quoc and Chi, Ed and Narang, Sharan and Chowdhery, Aakanksha and Zhou, Denny},
  journal={arXiv preprint arXiv:2203.11171},
  year={2022}
}

@misc{reducing-hallucination,
  title={RHO (\textrho): Reducing Hallucination in Open-domain Dialogues with Knowledge Grounding},
  author={Ziwei Ji and Zihan Liu and Nayeon Lee and Tiezheng Yu and Bryan Wilie and Min Zeng and Pascale Fung},
  journal={arXiv preprint arXiv:2212.01588},
  year={2022}
}

@misc{java-api,
  author = {Oracle},
  title = {Java API Specification},
  howpublished = {\url{https://docs.oracle.com/javase/8/docs/api/overview-summary.html}},
  note = {Accessed: 2024-04-22}  
}

@misc{magicoder,
  title = {Magicoder},
  howpublished = {\url{https://huggingface.co/ise-uiuc/Magicoder-S-DS-6.7B}},
  note = {Accessed: 2024-04-22}  
}


@misc{googlebigquery-github,
  author = {Felipe Hoffa},
  title = {GitHub on BigQuery: Analyze all the open source code},
  howpublished = {\url{https://cloud.google.com/blog/topics/public-datasets/github-on-bigquery-analyze-all-the-open-source-code}},
  note = {Accessed: 2024-04-22}  
}

@misc{googlebigquery,
  author = {Google},
  title = {googlebigquery},
  howpublished = {\url{https://cloud.google.com/bigquery/}},
  note = {Accessed: 2024-04-22}  
}


@article{pan2023lost,
  title={Lost in Translation: A Study of Bugs Introduced by Large Language Models while Translating Code},
  author={Pan, Rangeet and Ibrahimzada, Ali Reza and Krishna, Rahul and Sankar, Divya and Wassi, Lambert Pouguem and Merler, Michele and Sobolev, Boris and Pavuluri, Raju and Sinha, Saurabh and Jabbarvand, Reyhaneh},
  journal={arXiv preprint arXiv:2308.03109},
  year={2023}
}

@article{latendresse2024chatgpt,
  title={Is ChatGPT a Good Software Librarian? An Exploratory Study on the Use of ChatGPT for Software Library Recommendations},
  author={Latendresse, Jasmine and Khatoonabadi, SayedHassan and Abdellatif, Ahmad and Shihab, Emad},
  journal={arXiv preprint arXiv:2408.05128},
  year={2024}
}


@misc{sentence-transformers,
  title = {all-MiniLM-L6-v2},
  howpublished = {\url{https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2}},
  note = {Accessed: 2024-04-22}
}


@inproceedings{rajbahadur2017impact,
  title={The impact of using regression models to build defect classifiers},
  author={Rajbahadur, Gopi Krishnan and Wang, Shaowei and Kamei, Yasutaka and Hassan, Ahmed E},
  booktitle={2017 IEEE/ACM 14th International Conference on Mining Software Repositories (MSR)},
  pages={135--145},
  year={2017},
  organization={IEEE}
}



@misc{human-eval,
  title = {Human Eval},
  howpublished = {\url{https://github.com/openai/human-eval}},
  note = {Accessed: 2024-04-22}  
}

@article{dong2023self,
  title={Self-collaboration code generation via chatgpt},
  author={Dong, Yihong and Jiang, Xue and Jin, Zhi and Li, Ge},
  journal={arXiv preprint arXiv:2304.07590},
  year={2023}
}

@article{yeticstiren2023evaluating,
  title={Evaluating the code quality of ai-assisted code generation tools: An empirical study on github copilot, amazon codewhisperer, and chatgpt},
  author={Yeti{\c{s}}tiren, Burak and {\"O}zsoy, I{\c{s}}{\i}k and Ayerdem, Miray and T{\"u}z{\"u}n, Eray},
  journal={arXiv preprint arXiv:2304.10778},
  year={2023}
}

@misc{hajipour2023codelmsecbenchmark,
      title={CodeLMSec Benchmark: Systematically Evaluating and Finding Security Vulnerabilities in Black-Box Code Language Models}, 
      author={Hossein Hajipour and Keno Hassler and Thorsten Holz and Lea Schönherr and Mario Fritz},
      year={2023},
      eprint={2302.04012},
      archivePrefix={arXiv},
      primaryClass={cs.CR},
      url={https://arxiv.org/abs/2302.04012}, 
}

@misc{hao2024empiricalstudydevelopersshared,
      title={An Empirical Study on Developers Shared Conversations with ChatGPT in GitHub Pull Requests and Issues}, 
      author={Huizi Hao and Kazi Amit Hasan and Hong Qin and Marcos Macedo and Yuan Tian and Steven H. H. Ding and Ahmed E. Hassan},
      year={2024},
      eprint={2403.10468},
      archivePrefix={arXiv},
      primaryClass={cs.SE},
      url={https://arxiv.org/abs/2403.10468}, 
}

@misc{zan2023privatelibrary,
      title={Private-Library-Oriented Code Generation with Large Language Models}, 
      author={Daoguang Zan and Bei Chen and Yongshun Gong and Junzhi Cao and Fengji Zhang and Bingchao Wu and Bei Guan and Yilong Yin and Yongji Wang},
      year={2023},
      eprint={2307.15370},
      archivePrefix={arXiv},
      primaryClass={cs.SE},
      url={https://arxiv.org/abs/2307.15370}, 
}

@misc{unixcoder,
  title = {Unixcoder},
  howpublished = {\url{https://huggingface.co/microsoft/unixcoder-base}},
  note = {Accessed: 2024-04-22}  
}

@article{codeLlama,
  title={Code llama: Open foundation models for code},
  author={Roziere, Baptiste and Gehring, Jonas and Gloeckle, Fabian and Sootla, Sten and Gat, Itai and Tan, Xiaoqing Ellen and Adi, Yossi and Liu, Jingyu and Sauvestre, Romain and Remez, Tal and others},
  journal={arXiv preprint arXiv:2308.12950},
  year={2023}
}

@misc{stableCode,
  title = {Stable Code},
  howpublished = {\url{https://huggingface.co/stabilityai/stable-code-3b}},
  note = {Accessed: 2024-04-22}  
}



@misc{beautifulsoup,
  title = {Beautifulsoup4},
  howpublished = {\url{https://pypi.org/project/beautifulsoup4/}},
  note = {Accessed: 2024-04-22}  
}



@article{qiao2020deep,
  title={Deep learning based software defect prediction},
  author={Qiao, Lei and Li, Xuesong and Umer, Qasim and Guo, Ping},
  journal={Neurocomputing},
  volume={385},
  pages={100--110},
  year={2020},
  publisher={Elsevier}
}

@article{zan2023private,
  title={Private-library-oriented code generation with large language models},
  author={Zan, Daoguang and Chen, Bei and Gong, Yongshun and Cao, Junzhi and Zhang, Fengji and Wu, Bingchao and Guan, Bei and Yin, Yilong and Wang, Yongji},
  journal={arXiv preprint arXiv:2307.15370},
  year={2023}
}

@article{menze2009comparison,
  title={A comparison of random forest and its Gini importance with standard chemometric methods for the feature selection and classification of spectral data},
  author={Menze, Bjoern H and Kelm, B Michael and Masuch, Ralf and Himmelreich, Uwe and Bachert, Peter and Petrich, Wolfgang and Hamprecht, Fred A},
  journal={BMC bioinformatics},
  volume={10},
  pages={1--16},
  year={2009},
  publisher={Springer}
}

@article{rajbahadur2019impact,
  title={Impact of discretization noise of the dependent variable on machine learning classifiers in software engineering},
  author={Rajbahadur, Gopi Krishnan and Wang, Shaowei and Kamei, Yasutaka and Hassan, Ahmed E},
  journal={IEEE Transactions on Software Engineering},
  volume={47},
  number={7},
  pages={1414--1430},
  year={2019},
  publisher={IEEE}
}

@article{cutler2012random,
  title={Random forests},
  author={Cutler, Adele and Cutler, D Richard and Stevens, John R},
  journal={Ensemble machine learning: Methods and applications},
  pages={157--175},
  year={2012},
  publisher={Springer}
}

@inproceedings{xu2017answerbot,
  title={AnswerBot: Automated generation of answer summary to developers' technical questions},
  author={Xu, Bowen and Xing, Zhenchang and Xia, Xin and Lo, David},
  booktitle={2017 32nd IEEE/ACM international conference on automated software engineering (ASE)},
  pages={706--716},
  year={2017},
  organization={IEEE}
}

@article{ahmed2024studying,
  title={Studying and recommending information highlighting in Stack Overflow answers},
  author={Ahmed, Shahla Shaan and Wang, Shaowei and Tian, Yuan and Chen, Tse-Hsun Peter and Zhang, Haoxiang},
  journal={Information and Software Technology},
  volume={172},
  pages={107478},
  year={2024},
  publisher={Elsevier}
}

@inproceedings{yang2023does,
  title={Does data sampling improve deep learning-based vulnerability detection? Yeas! and Nays!},
  author={Yang, Xu and Wang, Shaowei and Li, Yi and Wang, Shaohua},
  booktitle={2023 IEEE/ACM 45th International Conference on Software Engineering (ICSE)},
  pages={2287--2298},
  year={2023},
  organization={IEEE}
}

@article{yang2024simclone,
  title={SimClone: Detecting Tabular Data Clones using Value Similarity},
  author={Yang, Xu and Rajbahadur, Gopi Krishnan and Lin, Dayi and Wang, Shaowei and Jiang, Zhen Ming},
  journal={ACM Transactions on Software Engineering and Methodology},
  year={2024},
  publisher={ACM New York, NY}
}

@inproceedings{ghotra2015revisiting,
  title={Revisiting the impact of classification techniques on the performance of defect prediction models},
  author={Ghotra, Baljinder and McIntosh, Shane and Hassan, Ahmed E},
  booktitle={2015 IEEE/ACM 37th IEEE International Conference on Software Engineering},
  volume={1},
  pages={789--800},
  year={2015},
  organization={IEEE}
}

@article{Humanval+,
  title={Is Your Code Generated by ChatGPT Really Correct? Rigorous Evaluation of Large Language Models for Code Generation},
  author={Liu, Jiawei and Wang, Yuyao and Zhang, Lingming},
  year={2023},
  publisher={Curran Associates}
}


@article{vulner2,
  title={Generating secure hardware using chatgpt resistant to cwes},
  author={Nair, Madhav and Sadhukhan, Rajat and Mukhopadhyay, Debdeep},
  journal={Cryptology ePrint Archive},
  year={2023}
}
@article{codehalu,
  title={CodeHalu: Code Hallucinations in LLMs Driven by Execution-based Verification},
  author={Tian, Yuchen and Yan, Weixiang and Yang, Qian and Chen, Qian and Wang, Wen and Luo, Ziyang and Ma, Lei},
  journal={arXiv preprint arXiv:2405.00253},
  year={2024}
}
@article{nlgsurvey,
  title={Survey of Hallucination in Natural Language Generation},
  author={Ji, Ziwei and Lee, Nayeon and Frieske, Rita and Yu, Tiezheng and Su, Dan and Xu, Yan and Ishii, Etsuko and Bang, Ye Jin and Madotto, Andrea and Fung, Pascale},
  journal={ACM Computing Surveys},
  volume={55},
  number={12},
  pages={1--38},
  year={2023},
  publisher={Association for Computing Machinery (ACM)}
}

@inproceedings{pearce2022asleep,
  title={Asleep at the keyboard? assessing the security of github copilot’s code contributions},
  author={Pearce, Hammond and Ahmad, Baleegh and Tan, Benjamin and Dolan-Gavitt, Brendan and Karri, Ramesh},
  booktitle={2022 IEEE Symposium on Security and Privacy (SP)},
  pages={754--768},
  year={2022},
  organization={IEEE}
}

@inproceedings{fan2023large,
  title={Large language models for software engineering: Survey and open problems},
  author={Fan, Angela and Gokkaya, Beliz and Harman, Mark and Lyubarskiy, Mitya and Sengupta, Shubho and Yoo, Shin and Zhang, Jie M},
  booktitle={2023 IEEE/ACM International Conference on Software Engineering: Future of Software Engineering (ICSE-FoSE)},
  pages={31--53},
  year={2023},
  organization={IEEE}
}
@inproceedings{majdinasab2024assessing,
  title={Assessing the Security of GitHub Copilot's Generated Code-A Targeted Replication Study},
  author={Majdinasab, Vahid and Bishop, Michael Joshua and Rasheed, Shawn and Moradidakhel, Arghavan and Tahir, Amjed and Khomh, Foutse},
  booktitle={2024 IEEE International Conference on Software Analysis, Evolution and Reengineering (SANER)},
  pages={435--444},
  year={2024},
  organization={IEEE}
}

@article{fu2023security,
  title={Security weaknesses of copilot generated code in github},
  author={Fu, Yujia and Liang, Peng and Tahir, Amjed and Li, Zengyang and Shahin, Mojtaba and Yu, Jiaxin},
  journal={arXiv preprint arXiv:2310.02059},
  year={2023}
}

@article{dawn,
  title={The dawn after the dark: An empirical study on factuality hallucination in large language models},
  author={Li, Junyi and Chen, Jie and Ren, Ruiyang and Cheng, Xiaoxue and Zhao, Wayne Xin and Nie, Jian-Yun and Wen, Ji-Rong},
  journal={arXiv preprint arXiv:2401.03205},
  year={2024}
}


@article{logen,
  title={CERT: continual pre-training on sketches for library-oriented code generation},
  author={Zan, Daoguang and Chen, Bei and Yang, Dejian and Lin, Zeqi and Kim, Minsu and Guan, Bei and Wang, Yongji and Chen, Weizhu and Lou, Jian-Guang},
  journal={arXiv preprint arXiv:2206.06888},
  year={2022}
}

@inproceedings{private,
  title={When Language Model Meets Private Library},
  author={Zan, Daoguang and Chen, Bei and Lin, Zeqi and Guan, Bei and Yongji, Wang and Lou, Jian-Guang},
  booktitle={Findings of the Association for Computational Linguistics: EMNLP 2022},
  pages={277--288},
  year={2022}
}

@article{guo2022unixcoder,
  title={Unixcoder: Unified cross-modal pre-training for code representation},
  author={Guo, Daya and Lu, Shuai and Duan, Nan and Wang, Yanlin and Zhou, Ming and Yin, Jian},
  journal={arXiv preprint arXiv:2203.03850},
  year={2022}
}

@article{luo2023wizardcoder,
  title={Wizardcoder: Empowering code large language models with evol-instruct},
  author={Luo, Ziyang and Xu, Can and Zhao, Pu and Sun, Qingfeng and Geng, Xiubo and Hu, Wenxiang and Tao, Chongyang and Ma, Jing and Lin, Qingwei and Jiang, Daxin},
  journal={arXiv preprint arXiv:2306.08568},
  year={2023}
}

@article{li2023starcoder,
  title={Starcoder: may the source be with you!},
  author={Li, Raymond and Allal, Loubna Ben and Zi, Yangtian and Muennighoff, Niklas and Kocetkov, Denis and Mou, Chenghao and Marone, Marc and Akiki, Christopher and Li, Jia and Chim, Jenny and others},
  journal={arXiv preprint arXiv:2305.06161},
  year={2023}
}

@inproceedings{yu2024codereval,
  title={Codereval: A benchmark of pragmatic code generation with generative pre-trained models},
  author={Yu, Hao and Shen, Bo and Ran, Dezhi and Zhang, Jiaxin and Zhang, Qi and Ma, Yuchi and Liang, Guangtai and Li, Ying and Wang, Qianxiang and Xie, Tao},
  booktitle={Proceedings of the 46th IEEE/ACM International Conference on Software Engineering},
  pages={1--12},
  year={2024}
}

@article{zhuo2024bigcodebench,
  title={Bigcodebench: Benchmarking code generation with diverse function calls and complex instructions},
  author={Zhuo, Terry Yue and Vu, Minh Chien and Chim, Jenny and Hu, Han and Yu, Wenhao and Widyasari, Ratnadira and Yusuf, Imam Nur Bani and Zhan, Haolan and He, Junda and Paul, Indraneil and others},
  journal={arXiv preprint arXiv:2406.15877},
  year={2024}
}

@article{du2023classeval,
  title={Classeval: A manually-crafted benchmark for evaluating llms on class-level code generation},
  author={Du, Xueying and Liu, Mingwei and Wang, Kaixin and Wang, Hanlin and Liu, Junwei and Chen, Yixuan and Feng, Jiayi and Sha, Chaofeng and Peng, Xin and Lou, Yiling},
  journal={arXiv preprint arXiv:2308.01861},
  year={2023}
}

@article{zhang2023repocoder,
  title={Repocoder: Repository-level code completion through iterative retrieval and generation},
  author={Zhang, Fengji and Chen, Bei and Zhang, Yue and Keung, Jacky and Liu, Jin and Zan, Daoguang and Mao, Yi and Lou, Jian-Guang and Chen, Weizhu},
  journal={arXiv preprint arXiv:2303.12570},
  year={2023}
}


@article{li2023starcoder,
  title={Starcoder: may the source be with you!},
  author={Li, Raymond and Allal, Loubna Ben and Zi, Yangtian and Muennighoff, Niklas and Kocetkov, Denis and Mou, Chenghao and Marone, Marc and Akiki, Christopher and Li, Jia and Chim, Jenny and others},
  journal={arXiv preprint arXiv:2305.06161},
  year={2023}
}





@article{codex,
  title={Evaluating large language models trained on code},
  author={Chen, Mark and Tworek, Jerry and Jun, Heewoo and Yuan, Qiming and Pinto, Henrique Ponde De Oliveira and Kaplan, Jared and Edwards, Harri and Burda, Yuri and Joseph, Nicholas and Brockman, Greg and others},
  journal={arXiv preprint arXiv:2107.03374},
  year={2021}
}

@article{toolcoder,
  title={Toolcoder: Teach code generation models to use api search tools},
  author={Zhang, Kechi and Zhang, Huangzhao and Li, Ge and Li, Jia and Li, Zhuo and Jin, Zhi},
  journal={arXiv preprint arXiv:2305.04032},
  year={2023}
}
@misc{Librarian,
      title={Is ChatGPT a Good Software Librarian? An Exploratory Study on the Use of ChatGPT for Software Library Recommendations}, 
      author={Jasmine Latendresse and SayedHassan Khatoonabadi and Ahmad Abdellatif and Emad Shihab},
      year={2024},
      eprint={2408.05128},
      archivePrefix={arXiv},
      primaryClass={cs.SE},
      url={https://arxiv.org/abs/2408.05128}, 
}
@inproceedings{codegen4libs,
  title={Codegen4libs: A two-stage approach for library-oriented code generation},
  author={Liu, Mingwei and Yang, Tianyong and Lou, Yiling and Du, Xueying and Wang, Ying and Peng, Xin},
  booktitle={2023 38th IEEE/ACM International Conference on Automated Software Engineering (ASE)},
  pages={434--445},
  year={2023},
  organization={IEEE}
}

@article{longllmlingua,
  title={Longllmlingua: Accelerating and enhancing llms in long context scenarios via prompt compression},
  author={Jiang, Huiqiang and Wu, Qianhui and Luo, Xufang and Li, Dongsheng and Lin, Chin-Yew and Yang, Yuqing and Qiu, Lili},
  journal={arXiv preprint arXiv:2310.06839},
  year={2023}
}

@inproceedings{lingua2,
    title = "{LLML}ingua-2: Data Distillation for Efficient and Faithful Task-Agnostic Prompt Compression",
    author = {Pan, Zhuoshi  and
      Wu, Qianhui  and
      Jiang, Huiqiang  and
      Xia, Menglin  and
      Luo, Xufang  and
      Zhang, Jue  and
      Lin, Qingwei  and
      R{\"u}hle, Victor  and
      Yang, Yuqing  and
      Lin, Chin-Yew  and
      Zhao, H. Vicky  and
      Qiu, Lili  and
      Zhang, Dongmei},
    editor = "Ku, Lun-Wei  and
      Martins, Andre  and
      Srikumar, Vivek",
    booktitle = "Findings of the Association for Computational Linguistics ACL 2024",
    month = aug,
    year = "2024",
    address = "Bangkok, Thailand and virtual meeting",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2024.findings-acl.57",
    doi = "10.18653/v1/2024.findings-acl.57",
    pages = "963--981",
    abstract = "This paper focuses on task-agnostic prompt compression for better generalizability and efficiency. Considering the redundancy in natural language, existing approaches compress prompts by removing tokens or lexical units according to their information entropy obtained from a causal language model such as LLaMa-7B. The challenge is that information entropy may be a suboptimal compression metric: (i) it only leverages unidirectional context and may fail to capture all essential information needed for prompt compression; (ii) it is not aligned with the prompt compression objective.To address these issues, we propose a data distillation procedure to derive knowledge from an LLM to compress prompts without losing crucial information, and meantime, introduce an extractive text compression dataset. We formulate prompt compression as a token classification problem to guarantee the faithfulness of the compressed prompt to the original one, and use a Transformer encoder as the base architecture to capture all essential information for prompt compression from the full bidirectional context. Our approach leads to lower latency by explicitly learning the compression objective with smaller models such as XLM-RoBERTa-large and mBERT.We evaluate our method on both in-domain and out-of-domain datasets, including MeetingBank, LongBench, ZeroScrolls, GSM8K, and BBH. Despite its small size, our model shows significant performance gains over strong baselines and demonstrates robust generalization ability across different LLMs. Additionally, our model is 3x-6x faster than existing prompt compression methods, while accelerating the end-to-end latency by 1.6x-2.9x with compression ratios of 2x-5x.",
}
@inproceedings{
recomp,
title={{RECOMP}: Improving Retrieval-Augmented {LM}s with Context Compression and Selective Augmentation},
author={Fangyuan Xu and Weijia Shi and Eunsol Choi},
booktitle={The Twelfth International Conference on Learning Representations},
year={2024},
url={https://openreview.net/forum?id=mlJLVigNHp}
}

@inproceedings{gist,
 author = {Mu, Jesse and Li, Xiang and Goodman, Noah},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {A. Oh and T. Naumann and A. Globerson and K. Saenko and M. Hardt and S. Levine},
 pages = {19327--19352},
 publisher = {Curran Associates, Inc.},
 title = {Learning to Compress Prompts with Gist Tokens},
 url = {https://proceedings.neurips.cc/paper_files/paper/2023/file/3d77c6dcc7f143aa2154e7f4d5e22d68-Paper-Conference.pdf},
 volume = {36},
 year = {2023}
}

@misc{walk,
      title={Walking Down the Memory Maze: Beyond Context Limit through Interactive Reading}, 
      author={Howard Chen and Ramakanth Pasunuru and Jason Weston and Asli Celikyilmaz},
      year={2023},
      eprint={2310.05029},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2310.05029}, 
}

@misc{llmos,
      title={MemGPT: Towards LLMs as Operating Systems}, 
      author={Charles Packer and Sarah Wooders and Kevin Lin and Vivian Fang and Shishir G. Patil and Ion Stoica and Joseph E. Gonzalez},
      year={2024},
      eprint={2310.08560},
      archivePrefix={arXiv},
      primaryClass={cs.AI},
      url={https://arxiv.org/abs/2310.08560}, 
}

@inproceedings{llmlingua,
    title = "{LLML}ingua: Compressing Prompts for Accelerated Inference of Large Language Models",
    author = "Jiang, Huiqiang  and
      Wu, Qianhui  and
      Lin, Chin-Yew  and
      Yang, Yuqing  and
      Qiu, Lili",
    editor = "Bouamor, Houda  and
      Pino, Juan  and
      Bali, Kalika",
    booktitle = "Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing",
    month = dec,
    year = "2023",
    address = "Singapore",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2023.emnlp-main.825",
    doi = "10.18653/v1/2023.emnlp-main.825",
    pages = "13358--13376",
    abstract = "Large language models (LLMs) have been applied in various applications due to their astonishing capabilities. With advancements in technologies such as chain-of-thought (CoT) prompting and in-context learning (ICL), the prompts fed to LLMs are becoming increasingly lengthy, even exceeding tens of thousands of tokens. To accelerate model inference and reduce cost, this paper presents LLMLingua, a coarse-to-fine prompt compression method that involves a budget controller to maintain semantic integrity under high compression ratios, a token-level iterative compression algorithm to better model the interdependence between compressed contents, and an instruction tuning based method for distribution alignment between language models. We conduct experiments and analysis over four datasets from different scenarios, i.e., GSM8K, BBH, ShareGPT, and Arxiv-March23; showing that the proposed approach yields state-of-the-art performance and allows for up to 20x compression with little performance loss.",
}

@inproceedings{UBC,
  title={Retrieval-based prompt selection for code-related few-shot learning},
  author={Nashid, Noor and Sintaha, Mifta and Mesbah, Ali},
  booktitle={2023 IEEE/ACM 45th International Conference on Software Engineering (ICSE)},
  pages={2450--2462},
  year={2023},
  organization={IEEE}
}

@misc{he2024,
      title={Exploring Demonstration Retrievers in RAG for Coding Tasks: Yeas and Nays!}, 
      author={Pengfei He and Shaowei Wang and Shaiful Chowdhury and Tse-Hsun Chen},
      year={2024},
      eprint={2410.09662},
      archivePrefix={arXiv},
      primaryClass={cs.SE},
      url={https://arxiv.org/abs/2410.09662}, 
}

@article{repo,
  title={Crosscodeeval: A diverse and multilingual benchmark for cross-file code completion},
  author={Ding, Yangruibo and Wang, Zijian and Ahmad, Wasi and Ding, Hantian and Tan, Ming and Jain, Nihal and Ramanathan, Murali Krishna and Nallapati, Ramesh and Bhatia, Parminder and Roth, Dan and others},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2024}
}

@inproceedings{allyouneed,
  title={Code search is all you need? improving code suggestions with code search},
  author={Chen, Junkai and Hu, Xing and Li, Zhenhao and Gao, Cuiyun and Xia, Xin and Lo, David},
  booktitle={Proceedings of the IEEE/ACM 46th International Conference on Software Engineering},
  pages={1--13},
  year={2024}
}

@misc{soft,
      title={Adapting Language Models to Compress Contexts}, 
      author={Alexis Chevalier and Alexander Wettig and Anirudh Ajith and Danqi Chen},
      year={2023},
      eprint={2305.14788},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2305.14788}, 
}

@ARTICLE{rl,
  author={Jung, Hoyoun and Kim, Kyung-Joong},
  journal={IEEE Access}, 
  title={Discrete Prompt Compression With Reinforcement Learning}, 
  year={2024},
  volume={12},
  number={},
  pages={72578-72587},
  keywords={Training;Task analysis;Optimization;Closed box;Computational efficiency;Chatbots;Reinforcement learning;Large language models;Application programming interfaces;Large language models;prompt compression;reinforcement learning;computational efficiency;black-box APIs},
  doi={10.1109/ACCESS.2024.3403426}}

@article{selective,
  title={Unlocking context constraints of llms: Enhancing context efficiency of llms with self-information-based content filtering},
  author={Li, Yucheng},
  journal={arXiv preprint arXiv:2304.12102},
  year={2023}
}

@article{natural,
  title={Natural is the best: Model-agnostic code simplification for pre-trained large language models},
  author={Wang, Yan and Li, Xiaoning and Nguyen, Tien N and Wang, Shaohua and Ni, Chao and Ding, Ling},
  journal={Proceedings of the ACM on Software Engineering},
  volume={1},
  number={FSE},
  pages={586--608},
  year={2024},
  publisher={ACM New York, NY, USA}
}

@inproceedings{dietcode,
  title={Diet code is healthy: Simplifying programs for pre-trained models of code},
  author={Zhang, Zhaowei and Zhang, Hongyu and Shen, Beijun and Gu, Xiaodong},
  booktitle={Proceedings of the 30th ACM Joint European Software Engineering Conference and Symposium on the Foundations of Software Engineering},
  pages={1073--1084},
  year={2022}
}

@inproceedings{codet5,
  title={CodeT5: Identifier-aware Unified Pre-trained Encoder-Decoder Models for Code Understanding and Generation},
  author={Wang, Yue and Wang, Weishi and Joty, Shafiq and Hoi, Steven CH},
  booktitle={Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing},
  pages={8696--8708},
  year={2021}
}
@inproceedings{copy,
  title={Get To The Point: Summarization with Pointer-Generator Networks},
  author={See, Abigail and Liu, Peter J and Manning, Christopher D},
  booktitle={Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
  year={2017},
  organization={Association for Computational Linguistics}
}

@article{codexglue,
  title={Codexglue: A machine learning benchmark dataset for code understanding and generation},
  author={Lu, Shuai and Guo, Daya and Ren, Shuo and Huang, Junjie and Svyatkovskiy, Alexey and Blanco, Ambrosio and Clement, Colin and Drain, Dawn and Jiang, Daxin and Tang, Duyu and others},
  journal={arXiv preprint arXiv:2102.04664},
  year={2021}
}

@article{efficient,
  title={Efficient Prompting Methods for Large Language Models: A Survey},
  author={Chang, Kaiyan and Xu, Songcheng and Wang, Chenglong and Luo, Yingfeng and Xiao, Tong and Zhu, Jingbo},
  journal={arXiv preprint arXiv:2404.01077},
  year={2024}
}

@article{gpt4,
  title={Gpt-4 technical report},
  author={Achiam, Josh and Adler, Steven and Agarwal, Sandhini and Ahmad, Lama and Akkaya, Ilge and Aleman, Florencia Leoni and Almeida, Diogo and Altenschmidt, Janko and Altman, Sam and Anadkat, Shyamal and others},
  journal={arXiv preprint arXiv:2303.08774},
  year={2023}
}

@article{learning,
author = {Yadavally, Aashish and Li, Yi and Wang, Shaohua and Nguyen, Tien N.},
title = {A Learning-Based Approach to Static Program Slicing},
year = {2024},
issue_date = {April 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {8},
number = {OOPSLA1},
url = {https://doi.org/10.1145/3649814},
doi = {10.1145/3649814},
abstract = {Traditional program slicing techniques are crucial for early bug detection and manual/automated debugging of online code snippets. Nevertheless, their inability to handle incomplete code hinders their real-world applicability in such scenarios. To overcome these challenges, we present NS-Slicer, a novel learning-based approach that predicts static program slices for both complete and partial code Our tool leverages a pre-trained language model to exploit its understanding of fine-grained variable-statement dependencies within source code. With this knowledge, given a variable at a specific location and a statement in a code snippet, NS-Slicer determines whether the statement belongs to the backward slice or forward slice, respectively. We conducted a series of experiments to evaluate NS-Slicer's performance. On complete code, it predicts the backward and forward slices with an F1-score of 97.41\% and 95.82\%, respectively, while achieving an overall F1-score of 96.77\%. Notably, in 85.20\% of the cases, the static program slices predicted by NS-Slicer exactly match entire slices from the oracle. For partial programs, it achieved an F1-score of 96.77\%–97.49\% for backward slicing, 92.14\%–95.40\% for forward slicing, and an overall F1-score of 94.66\%–96.62\%. Furthermore, we demonstrate NS-Slicer's utility in vulnerability detection (VD), integrating its predicted slices into an automated VD tool. In this setup, the tool detected vulnerabilities in Java code with a high F1-score of 73.38\%. We also include the analyses studying NS-Slicer’s promising performance and limitations, providing insights into its understanding of intrinsic code properties such as variable aliasing, leading to better slicing.},
journal = {Proc. ACM Program. Lang.},
month = apr,
articleno = {97},
numpages = {27},
keywords = {AI4SE, Debugging, Neural Networks, Pre-Trained Language Models, Static Slicing, Vulnerability Detection}
}


@misc{lessismore,
      title={Less is More: DocString Compression in Code Generation}, 
      author={Guang Yang and Yu Zhou and Wei Cheng and Xiangyu Zhang and Xiang Chen and Terry Yue Zhuo and Ke Liu and Xin Zhou and David Lo and Taolue Chen},
      year={2024},
      eprint={2410.22793},
      archivePrefix={arXiv},
      primaryClass={cs.SE},
      url={https://arxiv.org/abs/2410.22793}, 
}
@inproceedings{copy1,
  title={Point, disambiguate and copy: Incorporating bilingual dictionaries for neural machine translation},
  author={Zhang, Tong and Zhang, Long and Ye, Wei and Li, Bo and Sun, Jinan and Zhu, Xiaoyu and Zhao, Wen and Zhang, Shikun},
  booktitle={Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)},
  pages={3970--3979},
  year={2021}
}

@article{rag,
  title={Retrieval-augmented generation for knowledge-intensive nlp tasks},
  author={Lewis, Patrick and Perez, Ethan and Piktus, Aleksandra and Petroni, Fabio and Karpukhin, Vladimir and Goyal, Naman and K{\"u}ttler, Heinrich and Lewis, Mike and Yih, Wen-tau and Rockt{\"a}schel, Tim and others},
  journal={Advances in Neural Information Processing Systems},
  volume={33},
  pages={9459--9474},
  year={2020}
}
@article{rag2,
  title={Atlas: Few-shot learning with retrieval augmented language models},
  author={Izacard, Gautier and Lewis, Patrick and Lomeli, Maria and Hosseini, Lucas and Petroni, Fabio and Schick, Timo and Dwivedi-Yu, Jane and Joulin, Armand and Riedel, Sebastian and Grave, Edouard},
  journal={Journal of Machine Learning Research},
  volume={24},
  number={251},
  pages={1--43},
  year={2023}
}

@inproceedings{summdataset,
    title = "{D}ebate{S}um: A large-scale argument mining and summarization dataset",
    author = "Roush, Allen  and
      Balaji, Arvind",
    editor = "Cabrio, Elena  and
      Villata, Serena",
    booktitle = "Proceedings of the 7th Workshop on Argument Mining",
    month = dec,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.argmining-1.1/",
    pages = "1--7",
    abstract = "Prior work in Argument Mining frequently alludes to its potential applications in automatic debating systems. Despite this focus, almost no datasets or models exist which apply natural language processing techniques to problems found within competitive formal debate. To remedy this, we present the DebateSum dataset. DebateSum consists of 187,386 unique pieces of evidence with corresponding argument and extractive summaries. DebateSum was made using data compiled by competitors within the National Speech and Debate Association over a 7year period. We train several transformer summarization models to benchmark summarization performance on DebateSum. We also introduce a set of fasttext word-vectors trained on DebateSum called debate2vec. Finally, we present a search engine for this dataset which is utilized extensively by members of the National Speech and Debate Association today. The DebateSum search engine is available to the public here: \url{http://www.debate.cards}"
}

@article{gemini,
  title={Gemini: a family of highly capable multimodal models},
  author={Team, Gemini and Anil, Rohan and Borgeaud, Sebastian and Alayrac, Jean-Baptiste and Yu, Jiahui and Soricut, Radu and Schalkwyk, Johan and Dai, Andrew M and Hauth, Anja and Millican, Katie and others},
  journal={arXiv preprint arXiv:2312.11805},
  year={2023}
}

@article{codebleu,
  title={Codebleu: a method for automatic evaluation of code synthesis},
  author={Ren, Shuo and Guo, Daya and Lu, Shuai and Zhou, Long and Liu, Shujie and Tang, Duyu and Sundaresan, Neel and Zhou, Ming and Blanco, Ambrosio and Ma, Shuai},
  journal={arXiv preprint arXiv:2009.10297},
  year={2020}
}

@article{less,
  title={Less is More: DocString Compression in Code Generation},
  author={Yang, Guang and Zhou, Yu and Cheng, Wei and Zhang, Xiangyu and Chen, Xiang and Zhuo, Terry and Liu, Ke and Zhou, Xin and Lo, David and Chen, Taolue},
  journal={arXiv preprint arXiv:2410.22793},
  year={2024}
}


@misc{reproduce,
  author       = {anonymous},
  title        = {CodePromptZip},
  year         = {2025},
  howpublished = {\url{https://anonymous.4open.science/r/CodePromptZip-6B2B}}
}

@misc{codelm,
      title={Unifying the Perspectives of NLP and Software Engineering: A Survey on Language Models for Code}, 
      author={Ziyin Zhang and Chaoyu Chen and Bingchang Liu and Cong Liao and Zi Gong and Hang Yu and Jianguo Li and Rui Wang},
      year={2024},
      eprint={2311.07989},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2311.07989}, 
}