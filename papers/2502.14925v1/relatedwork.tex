\section{Related Work and Background}
\subsection{Related work}\label{sec:related}

% Soft prompt compression learns continuous vectors that encapsulate the original prompt, while discrete prompt compression relies on textual extraction, selecting or filtering tokens directly from the original prompt. 

%\sw{I cut it a bit, please check,check}
%Prompt compression can be broadly categorized into two main types based on the approach used: \textbf{soft prompts} and \textbf{discrete prompt compression} \citep{efficient}. \textbf{Soft prompts} involve learning embeddings that encapsulate either the instructions~\citep{gist} or original documents (examples)~\citep{soft}. For instance, \citealp{gist} compress the in-task instruction of prompts into a smaller sets of ``gist'' vectors which can be cached and reused. \citealp{soft} differs by learning to compress long documents into learnable context vectors. However, soft prompt compression suffers from limited cross-model compatibility and relies heavily on gradient access to LMs. This dependency makes it impractical for scenarios involving API services of proprietary LMs.
Prompt compression methods can be broadly classified into two types: \textbf{soft prompts} and \textbf{discrete prompt compression} \citep{efficient}. \textbf{Soft prompts} learn embeddings that encode either task instructions~\citep{gist} or example documents~\citep{soft}. For example, \citet{gist} condense prompt instructions into reusable ``gist'' vectors, while \citet{soft} compress long documents into learnable context vectors. However, soft prompts face limitations in cross-model compatibility and require gradient access to base LMs, making them impractical for API-based proprietary LM services.


Recent research has focused on \textbf{discrete prompt compression}, which retains key tokens from the original prompt while eliminating less informative content. This approach enhances compatibility with black-box or proprietary LMs. Notable techniques include \textbf{entropy-based} and \textbf{knowledge distillation} methods.
\textbf{Entropy-based} methods, such as LLMlingua and LongLLMlingua \citep{llmlingua,longllmlingua}, use small LMs to estimate the information entropy of tokens, filtering out low-value content. However, they rely on heuristic metrics that may not align well with compression objectives.
\textbf{Knowledge distillation} leverages large LMs like GPT-4 \citep{gpt4} to generate compressed summaries, which are then used to fine-tune smaller LMs as compressors. For instance, \citet{recomp} trained a T5 model on GPT-3.5-turbo summaries, while \citet{lingua2} employed a transformer encoder to classify tokens for extraction. Despite their effectiveness, distillation methods struggle with maintaining strict compression ratios and entail high costs due to reliance on proprietary LMs.

%Recent work has shifted towards \textbf{discrete prompt compression}, where compressed prompts consist of concrete tokens directly extracted from the original prompt. This approach eliminates less informative content, making discrete prompts versatile and compatible with black-box or proprietary LM services. Prominent methods in this domain include \textbf{entropy-based} \citep{llmlingua,longllmlingua} and \textbf{knowledge distillation} approaches. 
%\textbf{Entropy-based} methods, such as LLMlingua and LongLLMlingua \citep{llmlingua,longllmlingua,recomp,lingua2}, leverage small LMs to compute the information entropy of lexical units to assess their informativeness. While these methods offer a systematic way to filter content, they often rely on empirical metrics and may produce suboptimal results, as their objectives are not directly aligned with the prompt compression objective.
%\textbf{Knowledge distillation} leverages extreme-scale LMs like GPT-4 \citep{gpt4} for their summarization capabilities. In this paradigm, smaller student LMs are fine-tuned as compressors using datasets distilled from the outputs of these teacher LMs. For example, \citealp{recomp} employs a T5 encoder-decoder model as the compressor, training it on summaries of textual documents generated by GPT-3.5-turbo. Similarly, \citealp{lingua2} employs a transformer encoder to classify tokens as either preserved or discarded, ensuring that the resulting compressed prompt remains extractive and retains critical information from the original input. The limitations of distillation approaches is even extreme-scale LMs often fail to adhere to compression ratio constraints and incur substantial usage costs for proprietary teacher LMs.

% \peter{should we explain a bit for the ACL audience?}

Although code is a subset of natural language, it exhibits unique features, such as type information \citep{codelm}. Different token types encapsulate distinct symbolic and syntactic information. For example, \textbf{Identifier} tokens reflect developersâ€™ intent, while \textbf{Symbol} tokens define delimiters and operations. A recent work \citep{less} primarily targets the natural language parts (i.e., docstrings) in coding task prompts rather than addressing the compression of code itself. To the best of our knowledge, we are the first to focus on compressing the code.

% Furthermore, effective summarization does not necessarily guarantee high-quality generation results due to different objective. In our study, we address these challenges by generating training data for the compressor through program analysis (PA) tools and empirical methodologies, providing precise control over the compression process. 
% \pf{Different Objectives: The PPL-based method aims to make the PPL of Prompt large rather than to make the final generation better. }, 
% \sw{the different objectives, do you mean different tasks. I would shift the limitation a bit to those approach are for nlp, and not typically for code, code has its unique characteristics and needs different treatments. so here, we show the limitation is more like a motivation for our study.}
%\sw{no need to mention we are using T-5 here, we can basically use any T5-like model with different sizes. }While our framework also uses a T5 encoder-decoder model, akin to \citealp{recomp}, our training sets are explicitly conditioned on varying compression ratios. This design enables the compressor to produce demonstrations of different lengths while ensuring alignment with the final generation tasks.
%in-task fintuning
\subsection{Problem Formulation}
Referring to \citealp{llmlingua}, we modify and reformulate prompt compression. For a coding task $\mathcal{T}$, given an original prompt, denoted as $\textbf{\textit{x}} = (\textbf{\textit{x}}_1^{code}, ..., \textbf{\textit{x}}_N^{code}, \textbf{\textit{x}}^{ques})$, where $\textbf{\textit{x}}_i^{code}$ represents $ith$ code example \footnote{As improving the retriever is not the focus of this work, we retrieve examples using the SOTA BM25 \citep{he2024}.}, $N$ represents number of shots, and $\textbf{\textit{x}}^{ques}$ represents the question. We aim to compress the code examples to reduce token count while retaining critical information for the question. Formally, the compression is performed by a compressor $\mathcal{LM_C}$, acting as a function:

\setlength{\abovedisplayskip}{7pt} % Adjust the space above
\setlength{\belowdisplayskip}{5pt} % Adjust the space below
\begin{align}
\widetilde{\textbf{\textit{x}}}_i^{code}=\mathcal{LM_C}(\textbf{\textit{x}}_i^{code},\tau_{code}, \mathcal{T})
\end{align}\label{eq:lmc}

\noindent where $\tau_{code}=1-|\widetilde{\textbf{\textit{x}}}_i^{code}|/|\textbf{\textit{x}}_i^{code}|$ is the compression ratio for a code snippet. With compressed code examples, the overall prompt is shortened as:

\begin{align}
\widetilde{\textbf{\textit{x}}} &= \ourtool(\textbf{\textit{x}}) \\
&= (\{\mathcal{LM_C}(\textbf{\textit{x}}_i^{code},\tau_{code} )\}_{i=1}^N, \textbf{\textit{x}}^{ques}) \nonumber
\end{align}

\noindent where the overall ratio is given by $\tau=1-\widetilde{\textbf{\textit{x}}}/\textbf{\textit{x}}$.
The generation of the base language model $\mathcal{\mathcal{BLM}}$ with the compressed prompt $\widetilde{\textbf{\textit{x}}}$ is expected to closely approximate the generation with the original prompt \textbf{\textit{x}}. This can be formulated as: 
\begin{align}
\min _{\widetilde{\boldsymbol{x}}, \tau} \operatorname{KL}\left(P\left(\mathcal{BLM}(\widetilde{\textbf{\textit{x}}})\mid \widetilde{\textbf{\textit{x}}}\right), P\left(\mathcal{BLM}(\textbf{\textit{x}}) \mid \textbf{\textit{x}}\right)\right)
\end{align}

% The compressor LM can be regarded as a conditional generation problem. Concretely, the probability of generating a compressed code example is conditioned on the ratio, which includes the ratio and the original code snippet. 
% \begin{align}
% Compressor(d)=P(C \mid r, d)
% \end{align}
% A higher compression ratio indicates that more of the original tokens has been removed.