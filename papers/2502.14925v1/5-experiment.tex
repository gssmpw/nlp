\section{Experimental Setting}\label{sec:experimentalsetting}

% We initialize the compressor LM with the CodeT5 and fine-tune them on the training dataset. All models use a learning rate of and batch size equivalent to 130k tokens.

% Since we have gpt-3.5-turbo as the $\mathcal{BLM}$, the tokens are all computed with tiktoken's Gpt-3.5-tokenizer. 

%\yt{wonder if we could move RQ3 model setting analysis out to save space?}
\subsection{Research Questions}
\begin{itemize}
    \setlength\itemsep{-0.07in} % Adjust this value to control spacing
    \item RQ1: How effective is \ourtool compared to NL-specific prompt compression methods on coding tasks?
    \item RQ2: What is the trade-off between compression ratio and number of shots in \ourtool's performance?
    \item RQ3: How effective is \ourtool in controlling compression ratios?
    \item RQ4: How does \ourtool perform across different $\mathcal{BLM}$s?
    \item RQ5: How does \ourtool perform on unparsable code snippets? 
\end{itemize}

In RQ1, we compare generation quality by the
$\mathcal{BLM}$ using compressed prompts from existing approaches. RQ2 examines the impact of two key hyper-parameters, $\tau_{code}$ and number of shots, analyzing the trade-off between using highly compressed examples versus fewer complete ones within a fixed budget. RQ3 examines the ability of \ourtool on controlling compression ratios. RQ4 evaluates \ourtool's performance across different $\mathcal{BLM}$ to test its generalization. RQ5 explores scenarios with unparsable code, demonstrating the robustness of \ourtool as a learning-based framework.


%In RQ1, we investigate the quality of code generation by $\mathcal{BLM}$ when using compressed prompts generated through different compression methods, aiming to evaluate how \ourtool performs compared to existing approaches. In RQ2, we focus on the impact of two critical hyper-parameters: $\tau_{code}$ and the number of shots. This analysis examines the trade-off between incorporating a greater number of highly compressed code examples vs. fewer, more detailed demonstrations, under a fixed input token budget. In RQ3, we evaluate the performance of \ourtool when integrated with different $\mathcal{BLM}$ to validate its generalization capabilities. In RQ4, we consider scenarios where high-quality parsable code snippets are unavailable. By breaking the parsability of code examples, we highlight the necessity of a learning-based compressor like \ourtool and its robustness in handling unparsable code.

 % We used the Atlas dataset~\citep{UBC}, which contains pairs of focal and test methods to generate assertion statements. Following \citep{UBC}, we employed four evaluation metrics: Exact Match, Plausible Match, Longest Common Subsequence, and Edit Distance. Exact Match rate measures the proportion of predictions that exactly match the ground truth, while Plausible Match accounts for semantically equivalent assertions (e.g., treating $assertTrue(value == 0)$ and $assertEquals(value, 0)$ as identical). Longest Common Subsequence evaluates the overlap between predicted and expected assertions, and Edit Distance calculates the number of edits needed to match the prediction to the ground truth.


\subsection{Baselines and Oracle}
We compare our approach against four state-of-the-art prompt compression baselines: LLMLingua \citep{llmlingua}, LongLLMLingua \citep{longllmlingua}, LLMLingua-2 \citep{lingua2}, and RECOMP \citep{recomp}, with detailed descriptions provided in Sec. \ref{sec:related}. For reference, we also evaluate prompts without retrieval or compression. Additionally, we include \emph{Oracle}, where we iteratively remove tokens directly based on their type-aware priority ranking, without using compressor model (as the approach used to construct training dataset in Section~\ref{sec:trainingset}), as a program analysis-based baseline for code examples.
% \sw{maybe change a word, it is not a ideal compression, which is easy to misleading, maybe say Program analysis (PA) baseline}

\subsection{Datasets and Metrics}
We evaluated the performance of \ourtool on the same three coding tasks, using the same prompt template and metrics, as presented in Sec. \ref{sec:setup}.%\sw{what is the split ratio for training and testing?, fixed in sec 3.2}

\subsection{Base LMs}
Concrete compression offers the advantage of transferability across various $\mathcal{BLM}$s \cite{recomp,rl}. For RQ1, RQ2, RQ3, and RQ5, we conducted experiments on GPT-3.5-turbo. In RQ4, to evaluate the generalization of \ourtool, we conducted experiments on two additional $\mathcal{BLM}$s: the open-source CodeLlama-13B \citep{codeLlama} and the proprietary LM service Gemini-1.0-pro \citep{gemini}.



