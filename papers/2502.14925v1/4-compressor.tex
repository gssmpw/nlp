\section{Methodology}\label{sec:framework}

\begin{figure*}
    \centering
    \includegraphics[width=1\linewidth]{figures/codezip.pdf}
    \caption{Framework of \ourtool.}
    \label{fig:framework}
    \vspace{-0.3cm}
\end{figure*}
% \sw{first, introduce the big picture of compressor. why we need this compression, something like not all code snippets retrieved are parsable. to improve the flexibility, we need this compressor to compress on complete/partial code.  what is the rationale of this design, what key contribution in the design. things like why we use copy-attention mechanism? we would like to ganrantee that each single generated word comes from the original prompt? then going into the details. second, which part controls the control rate? we need to emphasize this, it is one key contribution of our approach, which allows us to control the compression rate.}
% \sw{It seems you introduce Figure 1 in introduction section. currently, the introduction section seems too long. you should introduce the framework (Figure 1) here, providing the overall picture of \ourtool. Then, introduce each key component. The overall steps for our framework based on the figure 1, we can see is, given a program task, for the training stage, first we do type priority ranking to identify the impact of program type. 2) we construct data for training a compressor based on the identify type priority, 3) train a compressor. specificaly, we introduce a specific design for model architecture and training strategy. For the inference stage, we do ...
% Note that we need to explain more on why training a compressor rather than pruning tokens based on priority directly. I saw you mentioned unparsable code, but we need to make it clearer and stand out. }
% Although our training set construction method, Oracle,
% serves as a program analysis-based compression
% approach for code examples. 
% Our training set construction method, Oracle, serves as a program analysis-based compression approach for code examples. It employs JavaParser to construct ASTs for token labeling and removal, limiting its applicability to unparsable code examples. However, code parsability cannot always be ensured. \sw{move this to methodology for motivating our approach}
% Note that a significant portion of available code are incomplete and unparsable~\cite{}. Therefore, we aim to build a language model (i.e., $\mathcal{LM_C}$) that can compress code that satisfies a specified ratio by iteratively removing the highest-priority tokens in a greedy manner, as motivated by prior works \citep{dietcode,natural}. 

% The removal priority ranking establishes a systematic strategy to code example compression, where \textit{tokens in higher-priority types are discarded before those in lower-priority types}. 
% Then tokens are iteratively removed based on their removal priority to construct the code compression training dataset, represented as ternary groups $\big({\widetilde{\textbf{\textit{x}}}^{code_i}}, \textbf{\textit{x}}^{code_i}, \tau_{code}\big)$. 
% only 70,433 (48.9\%) of 1,444,112 code examples in the Assertion Generation dataset \citep{UBC} are parsable,

As illustrated in Figure \ref{fig:framework}, \ourtool operates in two phases.
In the training phase, we first derive a type-aware priority ranking for a specific task $\mathcal{T}$ (Sec. \ref{sec:ranking}). Using this ranking, we implement a priority-driven strategy (Algorithm \ref{alg:greedy}): \textit{tokens in higher-priority types are discarded before those in lower-priority types}. This process transforms $(\textbf{\textit{x}}i^{code}, \tau_{code}, \mathcal{T})$ into $\widetilde{\textbf{\textit{x}}}_i^{code}$. We then train $\mathcal{LM_C}$ on the constructed dataset to learn the sequence-to-sequence compression task.

The design of the learning-based $\mathcal{LM_C}$ enhances \textbf{applicability}. While Algorithm \ref{alg:greedy} can directly output compressed code examples, its implementation relies on JavaParser for token labeling and removal, restricting its use to unparsable code. However, as shown in Table \ref{tab:task_statistics},  unparsable code examples are common in coding tasks.

The $\mathcal{LM_C}$ processes code sequences as probabilistic relations \citep{recomp,lingua2} rather than relying strictly on exact syntax, enables our framework to handle both parsable and unparsable code examples while tolerating compile and parse errors \citep{learning}.

In the inference phase, given a query, the $\mathcal{LM_C}$ accepts a specified $\tau_{code}$ and the original retrieved code example to generate compressed code that retains the most critical tokens. These compressed examples are then aggregated into a prompt and passed to the $\mathcal{BLM}$ to generate the final output.


% We train $\mathcal{LM_C}$ on dataset constructed by Algorithm \ref{alg:greedy} rather than direct use Algorithm \ref{alg:greedy} to compress code example is the consideration of applicability.
% As a PA-based approach, Oracle relies on JavaParser to construct abstract syntax trees (ASTs) for token labeling and removal, making it applicable only to compress parsable code examples. However, code parsability cannot always be guaranteed. For instance, as shown in Table \ref{tab:task_statistics} in the analyzed datasets, Assertion Generation \citep{UBC} includes 1,444,112 code examples, of which only 70,433 (48.9\%) are parsable.

%The subsequent subsections detail the steps for constructing the training datasets and training the compressor.

\newcommand{\INPUT}{\item[\textbf{Input:}]}
\newcommand{\OUTPUT}{\item[\textbf{Output:}]}
\begin{algorithm}
\caption{\footnotesize Priority-driven Greedy Algorithm for Dataset Construction} 
\label{alg:greedy}
\begin{algorithmic}[1]
\footnotesize
\INPUT $\textbf{\textit{x}}_i^{code} = \{x_j\}_{j=1}^{L}$, $\tau_{code}$, type priorities of $\mathcal{T}$.
\OUTPUT $\widetilde{\textbf{\textit{x}}}_i^{code}$.
\STATE Initialize a priority queue $\textit{pq}$.
\FOR{each token $x_j \in \textbf{\textit{x}}_i^{code}$}
    \STATE Assign priority to $x_j$ (\textit{Prioritize the drop of high-frequency tokens in prioritized type}).
    \STATE Insert $x_j$ into $\textit{pq}$.
\ENDFOR
\STATE $\textit{removedTokens} \gets \emptyset$.
\STATE $L_{rm} \gets \lfloor \tau_{code} \cdot L \rfloor$.
\STATE $\widetilde{L}_{rm} \gets 0$.
\WHILE{$\widetilde{L}_{rm} < L_{rm}$}
    \STATE $x_j \gets \textit{pq.pop()}$.
    \STATE $\textit{removedTokens} \gets \textit{removedTokens} \cup \{x_j\}$.
    \STATE $\widetilde{L}_{rm} \gets \widetilde{L}_{rm} + 1$.
\ENDWHILE
\STATE $\widetilde{\textbf{\textit{x}}}_i^{code} \gets \textbf{\textit{x}}_i^{code} \setminus \textit{removedTokens}$.
\RETURN $\widetilde{\textbf{\textit{x}}}_i^{code}$.
\end{algorithmic}
\end{algorithm}

\subsection{Code Compression Dataset Construction}\label{sec:trainingset}
The workflow of Algorithm~\ref{alg:greedy} is as follows. Line 1 initializes a priority queue to store tokens alongside their priorities. Lines 2–5 assign priorities to tokens based on their type, with tokens in the same type ranked by Term Frequency (TF) within the example. Frequently occurring tokens are prioritized for removal, as they are more likely to be redundant. Tokens belonging to multiple types are assigned to the category with the lowest removal priority, while out-of-type tokens are removed last, preserving potentially critical tokens. Line 6 initializes an empty set, removedTokens, to track removed tokens. Line 7 calculates the number of tokens to remove as a fraction of the total, determined by the specified $\tau_{code}$. Lines 9–13 iteratively remove the highest-priority tokens from the queue until the required number is removed. Line 14 constructs the modified training sample by excluding tokens in removedTokens from the original sequence. This iterative, priority-driven approach ensures the compressed code retains essential tokens while meeting the specified compression ratio.

Using Algorithm~\ref{alg:greedy}, we constructed a code compression dataset for training compressors (see dataset statistics in Appendix \ref{sec:stat}).
  

% \begin{table}[]
% \centering
% \footnotesize
% \label{tab:dataset}
% \caption{Statistic of Training Set for compressor.}
% % \sw{is the data for each range of compression rate is evenly distributed?pf: even}}
% \begin{tabular}{c|ccc}
% \hline
% Task       & $\tau_{code}$   & Train & Dev     \\ \hline
% \taskone  & 0.1-0.9 & 282, 932 & 36, 366 \\
% \tasktwo & 0.1-0.9 & 273, 856 & 34, 232 \\ 
% \taskthree  & 0.1-0.9 & 721, 021 & 80, 113 \\\hline
% \end{tabular}
% \end{table}




\subsection{Compressor Architecture}

% Our training set is designed to be extractive, ensuring that all tokens in the compressed snippet are directly derived from the original code. Inspired by \citet{lingua2}, extractive compression preserves faithfulness to the original content while minimizing the risk of hallucination. 
% Although the training data construction method can be viewed as a program analysis (PA)-based compression approach, it relies heavily on PA tools, making it unsuitable for unparsable code. To enhance flexibility and usability,
% Inspired by \citet{lingua2}, extractive compression preserves faithfulness to the original content while minimizing the risk of hallucination. 
% We use encoder-decoder LM (775M), initialized from CodeT5-large checkpoint. However, since the output should only consist of tokens from the input sequence, we apply a mask after the logits calculation.


With the code compression dataset, we fine-tune an encoder-decoder model, $\mathcal{LM_C}$, to effectively compress code examples. We adopt CodeT5 \citep{codet5} as our base model and introduce two key modifications to its architecture. First, we extend the input vocabulary with task-indicative tokens such as \textit{<ASSERTION>}, \textit{<BUGS2FIX>}, and \textit{<SUGGESTION>}, which are added at the beginning of the input sequence to explicitly indicate the task context. This design allows our model to be extended to more coding tasks. Additionally, to enable $\mathcal{LM_C}$ to condition on flexible $\tau_{code}$ settings, we introduce special tokens \textit{<Ratio>}, \textit{</Ratio>}, \textit{<Compress>}, and \textit{</Compress>}. These tokens signal the model to generate compressed code snippets tailored to the specified $\tau_{code}$ and task. Moreover, we incorporate a copy mechanism \citep{copy,copy1} into the architecture, allowing the model to directly copy tokens from the input sequence. This modification aligns with the extractive nature of the code compression task, where the outputs are derived entirely from the inputs.


The detailed architecture is shown in Figure \ref{fig:copyT5}. This mechanism is implemented using a copy module that computes the probability of copying each generated token directly from the input, rather than generating it from entire vocabulary. At first, the tokens of the original code sequence $\textbf{\textit{x}}_i^{code} = \{x_j\}_{j=1}^{|\textbf{\textit{x}}_i^{code}|}$  are fed into the encoder, producing a sequence of encoder hidden states $\textbf{h}=\{h_j\}$. In the decoder, the last cross-attention matrix $\mathbf{A} \in \mathbb{R}^{l_{\text{tgt}} \times l_{\text{src}}}$ represents the attention distribution over the source sequence during decoding. Each row $\mathbf{a}^t \in \mathbb{R}^{l_{\text{src}}}$ corresponds to the attention weights assigned to the source sequence at decoding step $t$. Here, $l_{\text{src}}$ and $l_{\text{tgt}}$ denote the maximum input and output lengths, respectively. The attention distribution not only guides the decoder’s focus for each source token, but also allows tokens to be copied from the source sequence by sampling from the attention distribution. Next, the attention distribution generates a weighted sum of the encoder hidden states, known as context vectors $\textbf{h}^*$:
\begin{align}
\textbf{h}_t^*=\sum_i a_i^t \textbf{h}_i, 
\end{align}

\begin{figure}
    \centering
    \includegraphics[width=1\linewidth]{figures/copyT5.pdf}
    \caption{Illustration of copy mechanism on CodeT5.}
    \label{fig:copyT5}
    \vspace{-0.6cm}
\end{figure}

The context vector $\textbf{h}_t^*$ represents a fixed size summary of what has been read from the source. Then the context vector is concatenated with the decoder state $\textbf{s}_t$, and passed through a copy module to calculate the generation likelihood $p_{gen} \in [0,1]$ at this step:
\begin{align}
    p_{gen} &= \sigma(\textbf{W}_{gen} \cdot [\textbf{h}_t^*, \textbf{s}_t] +\textbf{b}_{gen})
\end{align}
where $\textbf{W}_{gen}$ and $\textbf{b}_{gen}$ are learnable parameters of the linear copy module. Here, $p_{gen}$ corresponds to the probability of generating tokens from the vocabulary, while $(1-p_{gen})$ denotes the probability of copying tokens from the input.

Next, we calculate the copy distribution by summing the attention weights $a_i^t$ for all positions $i$ where the input token $x_i$ match the target token $y$:
\begin{align}
P_{copy}(y)=\sum_{i: x_i=x} a_i^t
\end{align}

% \sw{what is this? it is not english, should we move equation 8 here?}, 
The generation probability is computed through the language model head connected to the decoder’s output, defined as:

\begin{align}
P_{vocab}(y)=Softmax(\textbf{W}_{head} \cdot \textbf{s}_t +\textbf{b}_{head})
\end{align}
where $\textbf{W}_{head}$ and $\textbf{b}_{head}$ denote the weight matrix and bias vector of the head network, respectively.

Finally, the output distribution is computed by interpolating between generation distribution $P_{vocab}$ and copy distribution $P_{\text{copy}}$:

\begin{align}
P(y) = p_{gen}P_{vocab}(y)+(1-p_{gen})P_{copy}(y)
\end{align}

During training, we use the Cross-Entropy Loss to maximize the likelihood of the target sequence. The loss function is defined as:
\begin{equation}
\mathcal{L} = - \sum_{t=1}^{T}y_t \log(\hat{y_t})
\end{equation}

\noindent where $y_t$ is the ground-truth token at step $t$, and $\hat{y_t}$ is the predicted probability of that token. 

We train the model by using the AdamW optimizer with a batch size of 16, a learning rate of 5e-5, and 1,000 warmup steps for 10 epochs.
% Output compressed examples is entailed by the input original code examples.


