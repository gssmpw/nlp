% Given the growing demand for Coding task, although it's still benefit from  prompt containing long-context and suffer from lengthy context and prompt,
% Coding prompts often consist of instructions, queries, and multiple demonstrations. Among these, demonstrations play a particularly important role in providing task-specific knowledge. Concision whole prompt lie in extracting and compressing demonstrations in a way that preserves their essential information. As shown in Fig.1, our proposed framework introduces compressor LM that can compress code examples into short version. The resulting compressed code examples are then provided as context to a high-performance, higher-cost base LMs to produce final outputs.

% Coding prompts often consist of instructions, queries, and multiple demonstrations. Among these, demonstrations play a particularly important role in encoding task-specific knowledge. The challenge lies in extracting and compressing demonstrations in a way that preserves their essential information. Our proposed framework introduces a lightweight, open-source compressor language model that can compress code examples at variable ratios while maintaining the key token types critical for downstream tasks. The resulting compressed examples are then provided as context to a high-performance, higher-cost base language model to produce final outputs.
% The second challenge is the need for extractive compression to reduce hallucination. Discrete prompt compression should allow direct copying of essential tokens from the input to avoid information loss and improve factual accuracy.

% Finite context window: The training and inference cost of the Transformer architecture scale quadratically with sequence length, and the sequence length is limited by the available GPU memory in practice.

% Most existing NLP prompt compression approaches \citep{walk,llmos}, rely on datasets originally designed for NLP tasks. For instance, \citep{walk} repurposed a summarization dataset, relying on manual labeling, while \citep{llmos} utilized a conversation dataset generated by human annotators. These methods are not well-suited to code-specific tasks, where compression affects downstream performance differently. % There are also efforts to use reinforcement learning to compress prompts by optimizing a reward signal based on the difference between the outputs generated by the original and compressed prompts. Similarly, in our approach, we assess token importance by systematically removing them and observing the corresponding degradation in output quality, providing a more direct measure of token relevance.

% This approach allows the model to produce compressed code that is concise yet information-rich, ensuring compatibility with the context limits of modern large language models. By generating concise, high-quality prompts, we significantly lower inference time and API usage costs. Compressed prompts maintain essential information for generating accurate outputs in coding tasks while ensuring scalability across diverse large language model applications and contexts. Our work addresses the critical need for prompt compression in coding tasks, bridging the gap between the limitations of context windows and the need for efficient resource utilization.

% soft prompt: inapplicable when interacting with black-box APIs
% Interpretability: our work is interpretability, the compressed tokens are good as we remove the unimportant tokens.
% the necessity of a learning compressor, the inability to handle unparsable code hinder their real-word applicabilty in such scenarios.
% Oracle approach necessitate that the code is parsable.  

% These extended prompts enhance the factuality, consistency, and alignment of the outputs with user expectations. 

%type information codeT5 cugKM

%However, its dependence on traditional PA approaches for building static program slices limits VulDeePecker to complete code.
%\peter{following the discussion below, here, we should just say using compress code examples in the prompt?}
%\peter{one thing that is not clear is the relation of RAG and our work. Potentially, the approach can work for non-RAG. I feel we need to say why we target RAG, or what is more special about RAG}.
%\peter{one thing that is not clear is the relation of RAG and our work. Potentially, the approach can work for non-RAG. I feel we need to say why we target RAG, or what is more special about RAG}.\sw{actually our approach is more general than RAG, we can say we conduct three tasks using code examples as demonstration. but our approach can be used in any coding tasks that include code in the prompt?}\peter{I agree. We should say for any tasks that take in code as an input to the prompt}\sw{after discussion, let's use the current follow. we can add a sentence in the introduction to highlight our approach can be used in more general scenario. }
%Existing methods either rely on repurposed summarization datasets \citep{summdataset,walk,rl} or annotating compressed prompts using proprietary LLMs like GPT-4\citep{recomp,lingua2}, which incurs additional costs and lacks precise ratio control. Coding tasks lack human-annotated, compressed examples.

%The core principle is that token types whose removal from prompts results in minimal performance degradation while saving the most tokens should be prioritized for deletion. At first, the type information of code \citep{codelm} allow us to using Program Analysis (PA) tool to category different tokens types like identifiers. By ablating them in prompt and observer the performence degradation,  we are able to establish a hierarchy of removal priorities based on their impact. Subsequently, a greedy strategy is employed to iteratively generate compressed code snippets with flexible compression ratios. 

% Coding datasets lack human-annotated, compressed examples. Existing methods often rely on repurposed summarization datasets \citep{summdataset,walk} or question-answering tasks \citep{rl}. Some recent works \citep{recomp,lingua2} annotate compressed prompts using proprietary LMs like GPT-4, which incurs additional costs and lacks precise ratio control. To overcome this, we propose a type-aware, priority-driven method for training dataset construction. As illustrated in Figure \ref{fig:framework}, the core principle is that token types whose removal leads to minimal performance degradation in RAG should be prioritized for deletion. The structured nature of code enables the use of PA-based tools to annotate token types (e.g., identifiers, method signature). By systematically ablating these tokens and observing the impact on performance, we establish a deletion priority hierarchy. Using this hierarchy, we iteratively apply a greedy strategy to acquire compressed code examples at various ratios.
%\sw{Pengfei, this copy mechanism is not very clear. need to make it clearer. my understanding is that this copy mechanism is ensure the generated token must be directly dirived from source sequence by using a copying attention?}

\section{Introduction}\label{sec:intro}


Retrieval-Augmented Generation (RAG) for language models \citep{rag,rag2,recomp} has shown remarkable performance on knowledge-intensive tasks, particularly in coding domains \citep{UBC,allyouneed,he2024}, by incorporating retrieved code examples into input prompts. However, such prompts often span tens of thousands of tokens, which creates challenges due to the limited context window of LMs and the high cost of processing long prompts with proprietary services like GPT-4 (\$2.50 per million tokens).

Prompt compression offers a promising solution for efficient LM utilization  by retaining essential information while reducing prompt length \citep{efficient}. Although existing studies have achieved promising results for natural language (NL) tasks, including language modeling \citep{recomp,soft,gist}, question-answering \citep{rl}, and summarization \citep{llmlingua,selective}, there is no compressor specifically for coding tasks. To address this gap, we introduce \ourtool, a framework to train a code-specific compressor to compress code examples for RAG-based coding tasks. 

We propose using a small LM (i.e., CodeT5 \citep{codet5}, 775M) as the compressor to compress code examples. The LM-based compressor captures the probabilistic relationships between code tokens without being constrained by strict syntax, making our framework applicable to incomplete code. The generated compressed examples aim to be lightweight yet effective, ensuring minimal impact on the base LMâ€™s ability to produce high-quality outputs. To provide flexibility, the compressor accepts original code examples and desired compression ratios as input, generating examples that align with specified constraints. However, training the compressor introduces two key challenges: \circled{1} Constructing suitable datasets tailored for code compression. \circled{2} Designing a compressor architecture that effectively supports code compression while allowing compression ratio control.

To address \circled{1}, we propose a type-aware, priority-driven method to construct code compression datasets. This approach leverages the observation that different token types (e.g., Identifier) in code examples  have varying impacts on generation quality. Using program analysis tools \citep{codelm}, tokens are first categorized by their type. Next, we perform an ablation analysis to measure the impact of each type of tokens and establish a hierarchy of removal priorities based on their impact on performance degradation. Finally, a greedy strategy is employed to iteratively remove higher priority tokens and generate compressed code snippets with varying compression ratios.

To address \circled{2}, we enhance the base CodeT5 architecture with a copy mechanism \citep{copy,copy1}, which enables the model to directly copy tokens from the source. Since the compressed code is fully derived from the original, this mechanism introduces a copy distribution over source tokens to guide the token generation from the source sequence during decoding. Additionally, we extend the vocabulary by incorporating special tokens (e.g., <Ratio>), allowing the model to condition on specified compression ratios and adaptively learn compression at varying levels during training.


We evaluated \ourtool by compressing code examples in three RAG-based coding tasks, i.e., \taskone~\cite{UBC}, \tasktwo~\cite{codexglue}, and \taskthree~\cite{allyouneed}. \ourtool effectively maintains performance while reducing prompt lengths. \ourtool demonstrates improvements over both SOTA entropy-based (e.g., LLMLingua~\citep{llmlingua}) and distillation-based baselines (e.g., RECOMP~\citep{recomp}). \ourtool achieves an improvement of 23.4\%, 28.7\%, and 8.7\% over the best baseline for three coding tasks \taskone, \tasktwo, and \taskthree, respectively.

%Compared with SOTA baselines (e.g., LLMLingua~\citep{llmlingua} and RECOMP~\citep{recomp}), \sw{\ourtool achieves an improvement of ?\%, ?\%, and ?\% over the best baseline for \taskone, \tasktwo, and \tasktwo, respectively, meanwhile allowing for flexible ratio controlling.} 
%By addressing the unique challenges of code-intensive applications, \ourtool provides an more efficient bridging the gap in prompt compression for coding tasks. For instance, in \taskone, \ourtool achieves a 23.4\% improvement in Exact Match while using 7.17\% fewer tokens compared to LongLLMLingua. while allow for flexibel ratio controling and geneability with different BLM

%\sw{show some numbers to demonstrate how effective is our approach? 1) compared with SOTA, how much improvement we achieve. 2) We also observe the trade-off between shots and compression rate ..., 3)our compressor can control the length based on the input compression rate... }

We make the following contributions.
\vspace{-0.1in}
\begin{itemize}
    \setlength\itemsep{-0.07in} % Adjust this value to control spacing
    \item We first observe that different types of tokens have varying impacts on final generation quality. Based on this, we propose a novel prompt compression framework designed for compressing code examples.
    \item We developed a copy-enhanced LM as the compressor to compress code examples effectively and allow compression ratio control.
    \item Our approach achieves significant performance improvements over SOTA baselines and demonstrates generalization across different language models and tasks.
\end{itemize}






