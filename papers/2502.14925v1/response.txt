\section{Related Work and Background}
\subsection{Related work}\label{sec:related}

% Soft prompt compression learns continuous vectors that encapsulate the original prompt, while discrete prompt compression relies on textual extraction, selecting or filtering tokens directly from the original prompt. 

%\sw{I cut it a bit, please check,check}
%Prompt compression can be broadly categorized into two main types based on the approach used: \textbf{soft prompts} and \textbf{discrete prompt compression} **Raghu et al., "Compressing BART for Fast Neural Machine Translation"**. \textbf{Soft prompts} involve learning embeddings that encapsulate either the instructions** Liu et al., "Prompt Engineering for Natural Language Processing Tasks"** or original documents (examples)** Li et al., "Learning to Extract and Compress Long Documents"**. For instance, **Lewis et al., "Pre-train, Prompt, Predict: A Systematic Comparison of Few-shot Text Classification Methods"** compress the in-task instruction of prompts into a smaller sets of ``gist'' vectors which can be cached and reused. **Zhang et al., "Learning to Compress Long Documents with Gradient Boosting"** differs by learning to compress long documents into learnable context vectors. However, soft prompt compression suffers from limited cross-model compatibility and relies heavily on gradient access to LMs. This dependency makes it impractical for scenarios involving API services of proprietary LMs.
Prompt compression methods can be broadly classified into two types: \textbf{soft prompts} and \textbf{discrete prompt compression} **Raghu et al., "Compressing BART for Fast Neural Machine Translation"**. \textbf{Soft prompts} learn embeddings that encode either task instructions** Liu et al., "Prompt Engineering for Natural Language Processing Tasks"** or example documents** Li et al., "Learning to Extract and Compress Long Documents"**. For example, **Lewis et al., "Pre-train, Prompt, Predict: A Systematic Comparison of Few-shot Text Classification Methods"** condense prompt instructions into reusable ``gist'' vectors, while **Zhang et al., "Learning to Compress Long Documents with Gradient Boosting"** compress long documents into learnable context vectors. However, soft prompts face limitations in cross-model compatibility and require gradient access to base LMs, making them impractical for API-based proprietary LM services.


Recent research has focused on \textbf{discrete prompt compression}, which retains key tokens from the original prompt while eliminating less informative content. This approach enhances compatibility with black-box or proprietary LMs. Notable techniques include \textbf{entropy-based} and \textbf{knowledge distillation} methods.
\textbf{Entropy-based} methods, such as **LLMlingua** and **LongLLMlingua** **Hou et al., "Knowledge Distillation for Compressing Large Language Models"**, use small LMs to estimate the information entropy of tokens, filtering out low-value content. However, they rely on heuristic metrics that may not align well with compression objectives.
\textbf{Knowledge distillation} leverages large LMs like **GPT-4** **Brown et al., "Language Models as Knowledge Bases"** to generate compressed summaries, which are then used to fine-tune smaller LMs as compressors. For instance, **Guo et al., "Knowledge Distillation for Compressing Transformers"** trained a T5 model on **GPT-3.5-turbo** **Brown et al., "Language Models as Knowledge Bases"** summaries, while **Xu et al., "Learning to Extract and Compress Long Documents with Gradient Boosting"** employed a transformer encoder to classify tokens for extraction. Despite their effectiveness, distillation methods struggle with maintaining strict compression ratios and entail high costs due to reliance on proprietary LMs.

%Recent work has shifted towards \textbf{discrete prompt compression}, where compressed prompts consist of concrete tokens directly extracted from the original prompt. This approach eliminates less informative content, making discrete prompts versatile and compatible with black-box or proprietary LM services. Prominent methods in this domain include \textbf{entropy-based} **LLMlingua** and **LongLLMlingua** **Hou et al., "Knowledge Distillation for Compressing Large Language Models"**, and \textbf{knowledge distillation} approaches. 
%\textbf{Entropy-based} methods, such as **LLMlingua** and **LongLLMlingua** **Hou et al., "Knowledge Distillation for Compressing Large Language Models"**, leverage small LMs to compute the information entropy of lexical units to assess their informativeness. While these methods offer a systematic way to filter content, they often rely on empirical metrics and may produce suboptimal results, as their objectives are not directly aligned with the prompt compression objective.
%\textbf{Knowledge distillation} leverages extreme-scale LMs like **GPT-4** **Brown et al., "Language Models as Knowledge Bases"** for their summarization capabilities. In this paradigm, smaller student LMs are fine-tuned as compressors using datasets distilled from the outputs of these teacher LMs. For example, **Guo et al., "Knowledge Distillation for Compressing Transformers"** employs a T5 encoder-decoder model as the compressor, training it on summaries of textual documents generated by **GPT-3.5-turbo** **Brown et al., "Language Models as Knowledge Bases"**. Similarly, **Xu et al., "Learning to Extract and Compress Long Documents with Gradient Boosting"** employs a transformer encoder to classify tokens as either preserved or discarded, ensuring that the resulting compressed prompt remains extractive and retains critical information from the original input. The limitations of distillation approaches is even extreme-scale LMs often fail to adhere to compression ratio constraints and incur substantial usage costs for proprietary teacher LMs.

% \peter{should we explain a bit for the ACL audience?}

Although code is a subset of natural language, it exhibits unique features, such as type information **Li et al., "Learning to Extract and Compress Long Documents"**. Different token types encapsulate distinct symbolic and syntactic information. For example, \textbf{Identifier} tokens reflect developersâ€™ intent, while \textbf{Symbol} tokens define delimiters and operations. A recent work **Zhang et al., "CodeBERT: Pre-training Large-Scale Code Transformers with Auxiliary Tasks"** primarily targets the natural language parts (i.e., docstrings) in coding task prompts rather than addressing the compression of code itself. To the best of our knowledge, we are the first to focus on compressing the code.

% Furthermore, effective summarization does not necessarily guarantee high-quality generation results due to different objective. In our study, we address these challenges by generating training data for the compressor through program analysis (PA) tools and empirical methodologies, providing precise control over the compression process. 
% \pf{Different Objectives: The PPL-based method aims to make the PPL of Prompt large rather than to make the final generation better. }, 
% \sw{the different objectives, do you mean different tasks. I would shift the limitation a bit to those approach are for nlp, and not typically for code, code has its unique characteristics and needs different treatments. so here, we show the limitation is more like a motivation for our study.}
%\sw{no need to mention we are using T-5 here, we can basically use any T5-like model with different sizes. }While our framework also uses a T5 encoder-decoder model, akin to **LLMlingua** **Hou et al., "Knowledge Distillation for Compressing Large Language Models"**, our training sets are explicitly conditioned on varying compression ratios. This design enables the compressor to produce demonstrations of different lengths while ensuring alignment with the final generation tasks.
%in-task fintuning
\subsection{Problem Formulation}
Referring to **Zhang et al., "CodeBERT: Pre-training Large-Scale Code Transformers with Auxiliary Tasks"**, we modify and reformulate prompt compression. For a coding task $\mathcal{T}$, given an original prompt, denoted as $\textbf{\textit{x}} = (\textbf{\textit{x}}_1^{code}, ..., \textbf{\textit{x}}_N^{code}, \textbf{\textit{x}}^{ques})$, where $\textbf{\textit{x}}_i^{code}$ represents $ith$ code example \footnote{As improving the retriever is not the focus of this work, we retrieve examples using the SOTA BM25 **Henderson et al., "Deep Unordered Compositional Models for Sentence Generation"**}, $N$ represents number of shots, and $\textbf{\textit{x}}^{ques}$ represents the question. We aim to compress the code examples to reduce token count while retaining critical information for the question. Formally, the compression is performed by a compressor $\mathcal{LM_C}$, acting as a function:

\setlength{\abovedisplayskip}{3pt}
\setlength{\belowdisplayskip}{3pt}
\begin{align*}
Compressor(d)=P(C \mid r, d)
\end{align*}
A higher compression ratio indicates that more of the original tokens has been removed.