

\subsection{RQ1: Comparisons with Baselines}\label{sec:rq1}

\begin{table*}[]
\caption{Results on three coding tasks using GPT-3.5-turbo as the $\mathcal{BLM}$. 
%Prompt lengths are computed with tiktokenâ€™s GPT-3.5-tokenizer. 
To ensure fair comparison with baselines that lack a specified compression rate, we set \ourtool's compression rate to 0.3, keeping it similar to or higher than the baselines. Note that higher metric values indicate better performance, while a higher $\tau$ (\%) reflects a greater proportion of tokens removed from the prompt.}
\centering
\scriptsize
\label{tab:main}
\begin{tabular}{lccccccccc}
\hline
\textbf{}                                             & \multicolumn{3}{c}{\textbf{\taskone}}                                                              & \multicolumn{3}{c}{\textbf{\tasktwo}}                                  & \multicolumn{3}{c}{\textbf{\taskthree}}                  \\ 
\multicolumn{1}{l}{\textbf{Approach}}                  & \textbf{\# tokens} & \textbf{$\tau(\%)$} & {\color[HTML]{242424} \textbf{Exact Match(\%)}} & \textbf{\# tokens} & \textbf{$\tau(\%)$} & \textbf{CodeBleu(\%)} & \textbf{\# tokens} & $\tau(\%)$ & \textbf{CodeBleu(\%)} \\ \hline
\multicolumn{1}{l}{w/o retrieval}                    & 334                & 46.6                         & 23.9                                        & 122                & 66.3                         & 41.7              & 29                 & 82.6                & 14.2               \\ \hline
\multicolumn{7}{l}{\textit{\textbf{Entropy-based}}}                                                                                                                                                                             & \textit{\textbf{}} &                     & \textit{\textbf{}} \\
\multicolumn{1}{l}{LLMLingua}                        & 482                & 22.9                         & 33.8                                        & 286               & 20.9                         & 41.9              & 125                & 25.1                & 21.8               \\
\multicolumn{1}{l}{LongLLMLingua}                    & 474                & 24.2                         & 34.1                                        & 287                & 20.6                         & 42.1              & 126                & 24.1                & 21.2               \\ \hline
\multicolumn{7}{l}{\textit{\textbf{Knowledge Distillation}}}                                                                                                                                                                              & \textit{\textbf{}} &                     & \textit{\textbf{}} \\
\multicolumn{1}{l}{LLMLingua-2}                      & 469                & 25.1                         & 21.2                                        & 282               & 21.9                         & 48.1              & 134                & 19.3                & 21.7               \\
\multicolumn{1}{l}{RECOMP}                           & 465                & 25.6                         & 23.4                                        & 268                & 25.9                         & 45.3              & 132                & 20.9                & 21.0               \\ \hline
\multicolumn{7}{l}{\textit{\textbf{Ours, Setting $\tau_{code}$-0.3, 1-shot}}}                                                                                                                                                     & \textit{\textbf{}} &                     & \textit{\textbf{}} \\
\rowcolor[HTML]{EFEFEF}
\multicolumn{1}{l}{\ourtool w/o Copy} & 447                & 28.5                         & 40.9                                        & 267                & 26.2                         & 56.7              & 131                & 21.7                & 20.5               \\
\rowcolor[HTML]{EFEFEF}
\multicolumn{1}{l}{\ourtool}          & 440                & 29.7                         & 42.1                                        & 262                & 27.4                        & 61.9              & 121               & 27.5                & 23.7               \\ 
\rowcolor[HTML]{F4E5BF}
\multicolumn{1}{l}{Oracle}                           & 454                & 27.4                         & 46.2                                      & 276                & 23.5                         & 66.8              & 120                & 28.1                & 23.8               \\ \hline
\multicolumn{1}{l}{w/o Compression}                  & 626                & 0.0                          & 50.5                                        & 362                & 0.0                       & 81.4              & 167                & 0.0                 & 24.7               \\ \hline
\end{tabular}
\vspace{-0.2in}
\end{table*}

% \begin{table}
% \centering
% \caption{The quality of API-oriented code generated by the three studied LLMs, MagiCoder, DeekSeek Coder, and ChatGPT for both tasks.\label{tab:RQ1}
% \resizebox{\columnwidth}{!}{
% \begin{tabular}{l|l|l|l|l|l}
% \hline
% \multirow{2}{*}{\textbf{Model}} & \textbf{\taskone} & \multicolumn{4}{c}{\textbf{\tasktwo}}                                                          \\ \cline{2-6} 
%    & \textbf{\NoneExistP}  & \textbf{\NoAPIUsedP} & \textbf{\UnCompilableP} & \textbf{\NonRunnableP} & \textbf{\total}\\ \hline
% MagiCoder              & 85.3\% (15,701/18,412)             & 9.1\% (247/2,711)               & 21.0\% (568/2,711)            & 14.3\% (388/2,711)   &  44.4\% 



Table \ref{tab:main} summarizes the results of different approaches for compressing retrieval-augmented prompts across three coding tasks, evaluating metrics such as token count, $\tau$ (overall compression ratio), and task performance, with the best results reported. The baseline approach without retrieved code examples (w/o retrieval) performs significantly worse than approaches with retrieval, highlighting the importance of RAG in enhancing $\mathcal{BLM}$ performance on coding tasks. \ourtool demonstrates improvements over both entropy-based and distillation-based baselines, improving by 23.4\%, 28.7\%, and 8.7\% over the best baseline for \taskone, \tasktwo, and \taskthree, respectively. 

% For \tasktwo, it achieves a 28.7\% improvement in CodeBleu over LLMLingua-2 (61.9\% vs. 48.1\%). In \taskthree, \ourtool achieves an 8.7\% improvement in CodeBleu over LLMLingua (23.7\% vs. 21.8\%).

%For instance, in \taskone, \ourtool achieves a 23.4\% (42.1\% vs. 34.1\%) improvement in Exact Match while using 7.17\% (440 tokens vs. 474 tokens) fewer tokens compared to LongLLMLingua.

%\sw{write a sentence to summarize the improvements \ourtool achieved compared with the best baselines, so that we can use it introduction section, fixed above}

Comparison with Oracle highlights the learning outcomes derived from the code compression dataset. \ourtool closely approaches Oracle-level performance without requiring JavaParser tools or parsable code snippets, falling only 4.1\% short in Exact Match for \taskone and 4.9\% in CodeBleu for \tasktwo, while achieving nearly identical performance in \taskthree.

The ablation study shows the consistent contributions of the copy mechanism to the enhancement of CodeT5, with a 1.2\% Exact Match increase for \taskone, CodeBleu improvements of 5.2\% for \tasktwo, and 3.2\% for \taskthree. While uncompressed prompts achieve the highest quality metrics, they incur a significant token cost.
 % enhancing its practical applicability.
% \sw{show numbers, how much \% improvement \ourtool achieves over baselines. for instance, how much improvement \ourtool can achieve compared with the best baselines? we need such statement to highlight the improvement in introduction as well.}






