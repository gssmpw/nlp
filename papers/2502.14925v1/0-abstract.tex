\begin{abstract}
% Retrieving code snippets and incorporating them in-context during inference enhances the performance of language models (LMs) on a wide range of coding tasks. However, this approach often results in a substantial increase in token count, constrained by finite context windows, and incurs high computational costs due to the processing of lengthy prompts. While recent advancements in prompt compression have demonstrated promising results for natural language (NL), their applicability to coding tasks remains uncertain, as these methods fail to consider the unique characteristics of programming languages, such as syntax and semantic token types. We propose \ourtool, as a step for RAG which compresses retrieved code examples into a short version prior in-context augmentation. compressing retrieved code examples into simplified versions before integrating them in-context. We present a code compressor, a copy-enhanced LM capable of directly copying important tokens from the input while discarding unimportant ones. To construct the training set, we first investigate performance degradation on end coding tasks when specific token types are removed from the original code snippets that prepended to the LMs’ prompt. Token types whose removal results in minimal performance degradation are prioritized for elimination. Using this insight, we apply a greedy algorithm to generate simplified code snippets at various compression ratios. We evaluate our approach on three coding tasks and demonstrate that our approaches achieves higher compression rates with minimal loss in generation quality compared with NL-specific compressors, 

% Retrieval-Augmented Generation (RAG) has shown exceptional performance in coding tasks by integrating retrieved code examples into prompts. However, the extended prompts often exceed tens of thousands of tokens, leading to two critical challenges: 1) the strict constraints of context windows of language models (LMs) and 2) substantial computational and financial costs associated with LMs. While prompt compression has been explored for natural language tasks, no tailored solutions exist for coding tasks. We propose \ourtool, a framework designed to compress code examples before their inclusion in RAG workflows. Our framework first employs a type-aware, priority-driven approach for constructing code-compressing training datasets, utilizing program analysis-based token identification and ablation studies to designate token removal priorities. Then in-task compressors-small conditional LMs are fine-tuned on the training sets, enabling it to condition on specified compression ratios and original code for flexible compression with minimal performance loss. Specially, the compressor’s architecture is enhanced with a copy mechanism, allowing tokens to be directly copied from original code snippet. Our evaluation demonstrates that \ourtool effectively achieves significant prompt compression while maintaining performance across diverse coding tasks.
Retrieval-Augmented Generation (RAG) enhances coding tasks by incorporating retrieved code examples into prompts. However, lengthy prompts—often exceeding tens of thousands of tokens—introduce challenges related to limited context windows of language models (LMs) and high computational costs. Existing prompt compression techniques focus on natural language, lacking tailored solutions for code. To address the gap, we propose \ourtool, a framework that compresses code examples before integrating into RAG workflows. Our framework employs a type-aware, priority-driven strategy to construct training samples for training code compression model. By using program analysis, we identify token types (e.g., Identifier) and perform ablation analysis to rank their removal priorities based on their impact on task performance. We then train a small LM as the compressor on these samples, enabling flexible compression conditioned on specified ratios while minimizing performance degradation. Specially, the compressor’s architecture is augmented with a copy mechanism, allowing tokens to be directly copied from the original code snippets. Evaluation results show that \ourtool surpasses SOTA entropy-based and distillation-based baselines, improving by 23.4\%, 28.7\%, and 8.7\% over the best baseline for \taskone, \tasktwo, and \taskthree, respectively. 

%Experimental evaluations demonstrate the effectiveness of ours. When applied to the Assertion Generation task, \ourtool maintained 83.4\% of the exact match rate while using 33.5\% fewer tokens compared to the original prompts. 

%\sw{how much improvement compared with baselines? show some numbers, fixed}.
\end{abstract}

% Despite advances in NLP, there is currently no dedicated dataset for prompt compression, especially tailored for code-specific tasks. In the NLP domain, summarization datasets are often repurposed for compression tasks. However, this approach can negatively impact the performance of large language models (LLMs) during inference, particularly in downstream applications. The deficiency is 1) different target 2) the structure and requirements in code domain differ significantly from natural language tasks. Thus, specialized datasets and methodology for prompt compression in code tasks are essential to improving prompting efficiency in these contexts. 

% “treating source code as a sequence of tokens empowers such pre-trained models (PTMs) to operate efficiently for both complete and partial code,”

% The general idea for the Pointer-Transformer is that it copies from the input sequence to generate the output sequence. Pointer-Transformer achieves the state-of-the-art performance in solving combinatorial/ordering problems over a finite set of points (e.g., Traveling Salesman Problem) – which aligns with the task of detecting the sequence of program statements belonging to the dynamic slice.
