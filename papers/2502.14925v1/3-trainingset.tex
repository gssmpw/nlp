\section{Type-aware Priority Ranking}\label{sec:ranking}
% \sw{I would name this section as sort of Type-aware Priority Ranking in RAG. We show our observation (different types of tokens have different informative hint for different coding tasks.) as the motivation of our approach.}


Before introducing our framework, we outline its motivation. Different tokens in a prompt contribute unevenly to the final output, with less impactful tokens prioritized for removal~\cite{lessismore,selective,llmlingua,longllmlingua,recomp}. In coding tasks, prompts often include code snippets as RAG demonstrations. A key question arises: \textit{Do different types of tokens in code contribute differently to the final results?} If so, this insight can guide code prompt compression. To explore this, we use program analysis (PA) tools to categorize tokens by type and conduct ablation analysis to identify those with minimal impact, guiding efficient prompt compression.

%Before introducing our framework, we first highlight the motivation of our approach. Different tokens in a prompt contribute unevenly to the final result, with less impactful tokens prioritized for removal during compression. For instance, prior research~\cite{lessismore,selective,llmlingua,longllmlingua} identify tokens with minimal contributions to entropy gains, assigning them a high priority for deletion. Similarly, \citealp{recomp} employs an encoder to evaluate tokens at the embedding level, discarding those deemed semantically less relevant to the query. 

%In coding tasks, prompts often include code snippets as demonstrations for RAG. A key question arises: ``Do different types of tokens in code contribute differently to the final results?'' If so, these differences can be leveraged for code prompt compression. To explore this, we utilize the type information inherent in code by employing program analysis (PA) tools to group tokens into distinct types. Through ablation analysis, we are able to reveal the priority of removing specific token types in the code examples.

%By establishing a prioritized ranking for token removal and forming the basis for an efficient prompt compression framework. 
% Leveraging the derived token priority rankings, we employ a greedy strategy to compress code in various ratios, removing tokens with higher priority before those with lower priority.

\subsection{Type Ablation Analysis}
% code examples play a crucial role in enabling large language models to understand queries effectively and generate accurate and faithful responses. Within these demonstrations, 
% However, given the unique characteristics of programming languages, this study adopts a more tailored approach by examining token units based on their lexical and syntactic properties. 
We categorize tokens into five types, based on the taxonomy proposed by \citealp{natural}: \textbf{Symbol}, \textbf{Signature}, \textbf{Invocation}, \textbf{Identifier}, and \textbf{Structure} (see Appendix \ref{sec:category} for detailed descriptions). 

We constructed Abstract Syntax Trees (ASTs) using JavaParser \citep{javaparser} to identify token types. % via \emph{NameExpr} nodes. 
Tokens of specific types were removed from the retrieved code examples, followed by performing RAG with the type-ablated examples to measure their impact on $\mathcal{BLM}$s performance in downstream tasks. The removal priority of a token type $T$ is defined as follows:

\begin{align}
Priority(T) = \frac{\tau_{code/T}}{d_{T}}
\end{align}

\noindent where $\tau_{code/T}$(\%) represents the code compression ratio achieved by discarding tokens of type $T$, 
 $d_{T}$ (\%) denotes the percentage of performance degradation in the evaluation metric caused by the removal of tokens of type $T$.

Token types that yield a higher ratio of $\tau_{code/t}$ and result in minimal degradation $d_{t}$ are assigned higher priority for removal. This approach ensures the compression process effectively reduces token length while preserving the generation quality.

\subsection{Setup of Downstream Coding Tasks with RAG}\label{sec:setup}
To comprehensively assess the impact of different types of tokens, we evaluated them across three datasets and two frozen-parameter $\mathcal{BLM}$s.

\noindent\textbf{Dataset and metric}:  (i) \textbf{\taskone}: The input is a focal method (the method under test) and its partial unit test, while the outputs are assertion statements verifying its correctness. The evaluation metric is the Exact Match Rate, following prior work \citep{UBC}. (ii)\textbf{ \tasktwo}: The input is a buggy method, and the output is a refined version of the method with the bugs fixed. This task is evaluated using CodeBleu \citep{codebleu}, consistent with the CodexGLUE benchmark \citep{codexglue}. (iii) \textbf{\taskthree}: The input consists of a method header (a summary of a function), and the output is a suggested code snippet for the developer based on the header. The task is also evaluated using CodeBleu defined in the original paper \citep{allyouneed}. Table~\ref{tab:task_statistics} presents the statistics of datasets.

\begin{table}[ht]
    \centering
    \resizebox{0.5\textwidth}{!}{%
        \begin{tabular}{lccc}
            \hline
            \textbf{Task}             & \textbf{Knowledge Base (Parsable)} & \textbf{Test} & \textbf{Val} \\ \hline
            \textbf{\taskone}          & 144,112 (70,433)                  & 18,027            & 18,816                  \\
            \textbf{\tasktwo}          & 52,364 (48,903)                   & 6,545             & 6,546                   \\
            \textbf{\taskthree}        & 128,724 (89,014)                  & 10,727            & 5,149                   \\ \hline
        \end{tabular}
    }
    \caption{Dataset statistics of different coding tasks.}
    \label{tab:task_statistics}
\end{table}

In all tasks, we utilize the code RAG prompt template \citep{allyouneed,he2024,UBC} (see Figure \ref{fig:template}), and craft task-specific instructions in a one-shot setting. As listed in Table \ref{tab:task_statistics}, we follow the original split of the dataset into Train, Validation, and Test partitions. The training partition functions as our knowledge base for example retrieval. Note that some code examples that yield parsing errors in JavaParser due to code incompleteness are classified as \textbf{Unparsable}. 
Due to computational resource constraints, we randomly sample 2,000 instances from both the validation and test sets for our experiments. The sampled validation set is used to study example removal priority, while the sampled test set serves to evaluate performance.


%\peter{parsable training samples? but the definition of parsable is not clear: fixed}
.

\noindent\textbf{Base LMs}: The in-context learning capabilities of large language models enable them to utilize query-related documents to produce outputs that better align with the instructions. To investigate whether the impact of different token types is consistent across models, we tested the constructed prompts on two large-scale  $\mathcal{BLM}s$: GPT-3.5-turbo and Gemini-1.0-Pro. We set temperature to 0 to ensure enhanced stability across experiments.

\subsection{Observation}
% task-specific and model-agnostic.
\begin{figure}
    \centering
    \includegraphics[width=1\linewidth]{figures/rank.pdf}
    \vspace{-0.3in}
    \caption{Removal priority of code token types: e.g., Invocation \textgreater\xspace Symbol in \taskone, and vice versa in \taskthree. Priorities are task-specific yet model-agnostic, applicable to both LMs.}
    \label{fig:rank}
    \vspace{-0.5cm}
\end{figure}

Figure \ref{fig:rank} presents ablation analysis results using a log y-axis to normalize priority scores. The plots reveal type hierarchies of tokens in removal priority, arranged in descending order. This visualization highlights that higher-priority token types should be preferentially removed, providing an intuitive representation of the token-type removal strategy. In addition, the hierarchies are consistent across $\mathcal{BLM}$s but exhibit in-task variations, suggesting the cross-model adaptability of priority-driven code compression.


% Token types that maintain higher performance with minimal deviation from the original results are identified as less critical. These types can be prioritized for removal to reduce the token count of code examples. Notably, the impact of token removal is not strictly proportional to the token count for each type. For instance, while removing all identifiers shortens the prompt by 103 tokens and removing all invocations shortens it by 180 tokens, the removal of identifiers results in a more significant degradation in performance.

%\sw{we can remove this for saving space, it is repeated with plots. delete}





% The key takeaway from our previous experiments is understanding the relative importance of token types for RAG across downstream tasks. We established the order of token importance as follows: Symbols \textgreater\xspace signatures \textgreater\xspace invocations \textgreater\xspace identifiers \textgreater\xspace structures. This hierarchy proved to be task-agnostic across our two method-level learning tasks, indicating its broader applicability.
% \sw{so within a type, how did we select the element, for instance, in the signature, how did we select? Do we select based on their similarity to the query and order them descendingly? } 
% \pf{When we look at priority, we remove all tokens of that type, and when knapsack, We remove the tokens in order.}
% \sw{we can treat this as a primary study as the motivation of our approach. Different types of program elements make different contributions to program understanding. therefore, we can leverage this information to compress prompts, in RAG setting.}\sw{do we have the data for code suggestion task?}
% \sw{how we decide importance of a word, if they are in the same type (e.g., symbol, signature?)}
% \sw{move this to methodology section, in which we can introduce the training data construction.}











