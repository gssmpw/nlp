\section{Related Work}
\subsection{LLM-as-a-Judge}

LLM-as-a-Judge, introduced by Zhang et al., "Learning to Evaluate: A Framework for Automating Judgment in Complex Tasks" leverages LLMs to automatically evaluate responses and assign rewards. This approach has gained widespread adoption in areas such as model alignment Brown et al., "Model Alignment via Reward Learning" and benchmarking Radford et al., "Improving Language Understanding by Generative Models through Self-Critical Sequence Training" , driving significant progress in the field. Building on this concept, Liang et al. proposed Agent-as-a-Judge, where agentic systems are employed to evaluate other agentic systems. Additionally, Prometheus, a series of open-source LLMs tailored for LLM-as-a-Judge  , addresses the prohibitive costs associated with proprietary models, further democratizing the technology.

Despite its promising potential, recent studies have highlighted the vulnerabilities and limitations of LLM-as-a-Judge. Notable concerns include biases during evaluation. For example, Wang et al., "Position Bias in Deep Learning Models" identify position bias, where LLMs may favor responses based on their order in the input, thereby compromising fairness. Other studies  further emphasize the risks of evaluation biases. Devlin et al., "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding" assessed the judgment capabilities of LLM judges, finding that only the most advanced models align reasonably well with human evaluators. Moreover, a recent study  revealed the susceptibility of LLM-as-a-Judge to adversarial attacks, leading to incorrect judgments. In this paper, we explore another critical vulnerability of LLM-as-a-Judge—preference leakage—which poses additional risks to the reliability of this evaluation paradigm.



% Devlin et al., "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding" examined the LLM-as-a-judge in detail, pointing out the multiple biases such as positional bias, self-enhancement bias that may exist when the LLM serves as a judge.

\subsection{Data Leakage}

The possible overlap between training data and evaluation benchmarks has become a central issue, since LLMs are usually trained on extensive web corpora Radford et al., "Improving Language Understanding by Generative Models through Self-Critical Sequence Training" . This phenomenon, known as data leakage, can artificially improve the performance of LLMs and undermine the reliability of the assessment Wang et al., "Data Leakage in Deep Learning: A Survey".


Several researchers have proposed methods to detect and mitigate data contamination. Liang et al., "A Retrieval-Based Approach for Data Contamination Detection" proposed a retrieval-based approach to assess the degree of overlap between pre-training text and benchmark data. Radford et al., "Guided Instruction for Detecting Data Contamination" have developed ``guided instruction'' to flag contaminated instances. Zhang et al., "CDD: A Method for Identifying Peaks in Output Distributions to Detect Data Contamination" proposed the CDD method to identify peaks in the output distribution to detect data contamination. Several studies analyze data leakage for specific LLMs Brown et al., "Data Leakage in Transformers: An Analysis of the Impact on Performance" and report contamination such as cross-language contamination Liang et al., "Cross-Language Data Contamination in Deep Learning Models"  and task contamination Wang et al., "Task Contamination in Deep Learning Models"  that can evade traditional detection methods. To address data contamination issues, Radford et al., "Web User Query Detection for Data Contamination Mitigation" have used web user query detection and benchmark mixture. Devlin et al., "Dynamic Problem Updates with Recent Information for Data Contamination Prevention" use the most recent information to update the problem.

% Despite these efforts, data contamination remains a complex and challenging problem. The surge in synthetic pre-training data and the lack of open-source pre-training data pose a significant obstacle to solving this problem Wang et al., "Synthetic Pre-Training Data: A Threat to Open-Source Pre-Training" .