%%%%%%%% ICML 2025 EXAMPLE LATEX SUBMISSION FILE %%%%%%%%%%%%%%%%%

\documentclass{article}

% Recommended, but optional, packages for figures and better typesetting:
\usepackage{microtype}
\usepackage{graphicx}
\usepackage{subfigure}
\usepackage{booktabs} % for professional tables

% hyperref makes hyperlinks in the resulting PDF.
% If your build breaks (sometimes temporarily if a hyperlink spans a page)
% please comment out the following usepackage line and replace
% \usepackage{icml2025} with \usepackage[nohyperref]{icml2025} above.
\usepackage{hyperref}


% Attempt to make hyperref and algorithmic work together better:

\newcommand{\theHalgorithm}{\arabic{algorithm}}

% Use the following line for the initial blind version submitted for review:
% \usepackage{icml2025}

% If accepted, instead use the following line for the camera-ready submission:
\usepackage[accepted]{icml2025}

% For theorems and such
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{amsthm}
\usepackage{graphicx}
\usepackage{tabularx}

\usepackage{multirow}
\usepackage{booktabs}
\usepackage{float}
\usepackage{times}
\usepackage{latexsym}
\usepackage{url}
\usepackage{forest}
\usepackage{booktabs}
\usepackage{multirow}
\usepackage{tabularx}
\usepackage{graphicx}
\usepackage{adjustbox}
\usepackage{colortbl}
\usepackage{pifont}
\usepackage{enumitem}
\usepackage{tabulary}
\usepackage{colortbl}
\usepackage{xcolor}
\usepackage{amsmath}
\usepackage{pgf} % Include the pgf package for pgfmathsetmacro
\usepackage[skins,breakable]{tcolorbox}

\definecolor{mypurple}{HTML}{9474DE} % DodgerBlue
\definecolor{myblue}{HTML}{88C4E0} % LimeGreen

% if you use cleveref..
\usepackage[capitalize,noabbrev]{cleveref}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% THEOREMS
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\theoremstyle{plain}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{assumption}[theorem]{Assumption}
\theoremstyle{remark}
\newtheorem{remark}[theorem]{Remark}

% Todonotes is useful during development; simply uncomment the next line
%    and comment out the line below the next line to turn off comments
%\usepackage[disable,textsize=tiny]{todonotes}
\usepackage[textsize=tiny]{todonotes}

\newcommand{\yue}[1]{{\small\color{red}{\bf [yue: #1]}}}
\newcommand{\wei}[1]{{\small\color{blue}{\bf [wei: #1]}}}

 % The \icmltitle you define below is probably too long as a header.
% Therefore, a short form for the running title is supplied here:
\icmltitlerunning{Preference Leakage: A Contamination Problem in LLM-as-a-judge}

\begin{document}

\twocolumn[
\icmltitle{\textit{Preference Leakage}: A Contamination Problem in LLM-as-a-judge}

% It is OKAY to include author information, even for blind
% submissions: the style file will automatically remove it for you
% unless you've provided the [accepted] option to the icml2025
% package.

% List of affiliations: The first argument should be a (short)
% identifier you will use later to specify author affiliations
% Academic affiliations should list Department, University, City, Region, Country
% Industry affiliations should list Company, City, Region, Country

% You can specify symbols, otherwise they are numbered in order.
% Ideally, you should not use this facility. Affiliations will be numbered
% in order of appearance and this is the preferred way.
\icmlsetsymbol{equal}{*}

\begin{icmlauthorlist}
\icmlauthor{Dawei Li}{equal,111}
\icmlauthor{Renliang Sun}{equal,222}
\icmlauthor{Yue Huang}{333}
\icmlauthor{Ming Zhong}{444}
\icmlauthor{Bohan Jiang}{111} \\
\icmlauthor{Jiawei Han}{444}
\icmlauthor{Xiangliang Zhang}{333}
\icmlauthor{Wei Wang}{222}
\icmlauthor{Huan Liu}{111}
%\icmlauthor{}{sch}
%\icmlauthor{}{sch}
\end{icmlauthorlist}

\icmlaffiliation{111}{Arizona State University}
\icmlaffiliation{222}{University of California, Los Angeles}
\icmlaffiliation{333}{University of Notre Dame}
\icmlaffiliation{444}{University of Illinois Urbana Champaign}
% \icmlaffiliation{comp}{Company Name, Location, Country}
% \icmlaffiliation{sch}{School of ZZZ, Institute of WWW, Location, Country}

\icmlcorrespondingauthor{Dawei Li}{daweili5@asu.edu}
% \icmlcorrespondingauthor{Firstname2 Lastname2}{first2.last2@www.uk}

% You may provide any keywords that you
% find helpful for describing your paper; these are used to populate
% the "keywords" metadata in the PDF but will not be shown in the document
\icmlkeywords{Machine Learning, ICML}

\vskip 0.3in
]

% this must go after the closing bracket ] following \twocolumn[ ...

% This command actually creates the footnote in the first column
% listing the affiliations and the copyright notice.
% The command takes one argument, which is text to display at the start of the footnote.
% The \icmlEqualContribution command is standard text for equal contribution.
% Remove it (just {}) if you do not need this facility.

% \printAffiliationsAndNotice{}  % leave blank if no need to mention equal contribution
\printAffiliationsAndNotice{\icmlEqualContribution} % otherwise use the standard text.

\begin{abstract}
% Large Language Models (LLMs) as judges have gained popularity as an automated evaluation method. However, concerns about bias and vulnerability in LLM-as-a-judge persist. In this work, we expose preference leakage, a contamination problem in LLM-as-a-judge caused by the relatedness between the synthetic data generators and LLM-based evaluators, the two foundational LLM-based annotators in model development. Through extensive experiments, we empirically confirm the presence of bias caused by preference leakage across multiple LLM baselines and benchmarks. Further analysis suggests that preference leakage is a pervasive problem caused by various forms of relatedness between data generators and evaluators, existing in different tuning techniques and data mixing strategies. Moreover, we find that preference leakage is more subtle and harder to detect compared to previously identified biases in LLM-as-a-judge scenarios. All of these findings imply that preference leakage is a widespread and challenging problem in the area of LLM-as-a-judge.
\input{abstract_v2}
\end{abstract}

\section{Introduction}
\label{Introduction}

% evaluation challenges for LLMs
% Due to their excellent Natural Language Processing (NLP) capability, they are widely used for automatic evaluation, called LLM-as-a-Judge \cite{zheng2023judging}. However, there are still substantial challenges to the LLM-as-a-Judge framework. For instance, recent studies point out and measure various biases in LLMs' judgment \cite{ye2024justice}. 
Recent advancements in Large Language Models (LLMs)~\cite{achiam2023gpt,jaech2024openai,tong2024can,zhang2024shifcon} have empowered various downstream tasks and applications. However, this also poses substantial challenges to the automatic evaluation of these models. Representatively, LLM-based AI agents' focus transfer from traditional natural language processing tasks~\cite{yang2023new,zhang2023assisting} to real-world \cite{liu2023agentbench, huang2023metatool}, open-ended response generation \cite{wu2024unigen}, which greatly limits the applicability of traditional n-gram matching methods (e.g., BLEU \cite{papineni2002bleu} and ROUGE \cite{lin2004rouge})~\cite{liu2016not,reiter2018structured} or model-based evaluators~\cite{zhangbertscore, unieval} for evaluation. 
% On the other hand, data contamination~\cite{golchin2023time,deng2024investigating} has emerged as another foundational problem as it may cause evaluation bias due to the overlapping between training corpora and evaluation set.

% LLM-as-a-judge & its bias
To address these challenges, the paradigm of LLM-as-a-judge~\cite{zheng2023judging,li2024generation,jiang2024assessing,zhong2024law,li2025exploring} has been proposed, designed to leverage LLM as evaluators to assess response quality. By combining powerful LLMs with well-designed prompting strategies, LLM-as-a-judge enables human-like evaluation of long-form and open-ended generation in a more cost-efficient and scalable manner. 
% Additionally, recent works have introduced the LLM-as-an-examiner paradigm~\cite{yu2024kieval,bai2024benchmarking}, which tackles data contamination issues through dynamic question generation.
However, recent studies point out some weaknesses of such assessment. For instance, \citet{ye2024justice} explores various biases and vulnerabilities of LLM-as-a-judge, highlighting the importance of developing a reliable and fair LLM-based evaluation system.


In this work, we aim to introduce another concern in LLM-as-a-Judge--\textit{Preference Leakage}. This issue arises when the LLMs used for data generation and evaluation are closely related, as illustrated in Figure~\ref{fig:overview}. Synthetic data generated by LLMs~\cite{gan2023ziya2,tan2024large,li2024contextualization,li2024dalk} has become a cornerstone of model training~\cite{lee2025distillation}. When combined with LLM-as-a-Judge, they offer significant efficiency gains in model development. However, limited attention has been given to the potential contamination that occurs when the generator and evaluator LLMs share a close relationship. During our preliminary study, we find this issue is particularly pervasive in popular LLM-as-a-judge benchmarks (e.g., AlpacaEval 2.0~\cite{dubois2024length} and Arena-Hard~\cite{li2024crowdsourced}) and LLM-relevant studies (more details can be found in Appendix~\ref{Preliminary Study of Preference Leakage in Real World}), due to the common reliance on the most advanced LLMs, such as GPT-4~\cite{achiam2023gpt}, for both data synthesis and evaluation to ensure the highest quality outputs. In our work, we reveal this relatedness—akin to the overlap between training data and evaluation sets in traditional data contamination—would introduce a systematic bias of judge LLMs towards their related student models (i.e., the model distilled by the data generator which is related to the judge). Compared to other biases in LLM-as-a-Judge, such as length bias or egocentric bias~\cite{ye2024justice,panickssery2024llm}, preference leakage is subtler and more challenging to detect, especially given that most LLMs do not disclose their training data. 
\begin{figure*}[h]
    \centering
    \includegraphics[width=0.9\linewidth]{Figure/overview.pdf}
    \vspace{-10pt}
    \caption{Overview of preference leakage. We make a comparison between data leakage and preference leakage and present three types of relatedness: being the same model, having an inheritance relationship and belonging to the same model family.}
    \label{fig:overview}
\end{figure*}

% our analysis & findings
To investigate and reveal the preference leakage problem, we first define three relatednesses between data generator LLM and judge LLM: being the same model, having an inheritance relationship, and belonging to the same model family. Each of these scenarios is commonly encountered in real-world applications. Then, we pose and answer three core research questions about preference leakage:

% \begin{itemize}[leftmargin=*,itemsep=1pt]
%     \vspace{-1mm}
%     \item \textbf{RQ1:} Does preference leakage result in bias in judge LLMs?
%     \vspace{-1mm}
%     \item \textbf{RQ2:} What is the severity of preference leakage under various scenarios?
%     \vspace{-1mm}
%     \item \textbf{RQ3:} What causes preference leakage to occur?
% \end{itemize}
% We conduct experiments with various LLM baselines in two widely recognized LLM-as-a-judge benchmarks to address the first question. We also introduce the preference leakage score to quantify the bias of judge LLMs toward their student models. The analysis results suggest an obvious bias of judging LLMs toward their related student models. To answer the second question, we conduct experiments under various relatedness settings, tuning techniques, and data mixing strategies, finding that preference leakage consistently affects judge LLMs in various contexts. For the third research question, we analyze LLMs' recognition capabilities on their related student models' generation as well as the distribution of bias across different question types and judgment dimensions. The analysis reveals that preference leakage is a subtle, hard-to-detect issue, particularly affecting subjective questions and judgment dimensions.
% \yue{I have some suggestions in this paragraph, and I do not directly modify them because I'm not sure about them. The first is about the description of the questions, I think you can use more professional expressions like ``To what extent does preference leakage introduce systematic biases in LLM-based evaluation?'' for RQ1. The second is it seems you do not answer the RQ2 clearly (what's the severity).}

\begin{itemize}[leftmargin=*,itemsep=1pt]
    \vspace{-2mm}
    \item \textbf{RQ1: Does preference leakage introduce systematic biases in LLM-based evaluation?} To answer it, we conduct experiments with various LLM baselines in two widely recognized LLM-as-a-judge benchmarks, also introduce the preference leakage score to quantify the bias caused by preference leakage. The analysis results suggest an obvious bias of judging LLMs toward their related student models.
    \vspace{-1mm}
    \item \textbf{RQ2: What is the severity of preference leakage under various scenarios?} We conduct experiments under various relatedness settings, tuning techniques, and data mixing strategies to address it, finding that preference leakage consistently affects judge LLMs. Moreover, the severity of preference leakage correlates with the degree of relatedness between the data generator and LLM judges, as well as the proportion of synthetic data.
    \vspace{-1mm}
    \item \textbf{RQ3: What are the underlying mechanisms causing preference leakage?} For this question, we analyze LLMs' recognition capabilities on their related student models' generation as well as the distribution of bias across different question types and judgment dimensions. The analysis reveals that preference leakage is a subtle, hard-to-detect issue, particularly affecting subjective questions and judgment dimensions.
\end{itemize}


% contribution
To summarize, our contributions in this work are as follows:
\begin{itemize}[leftmargin=*,itemsep=1pt]
    \vspace{-1mm}
    \item We introduce preference leakage, a contamination issue arising from the relatedness between the data generator and judge LLMs.
    \vspace{-1mm}
    \item We conduct extensive experiments across various LLMs and benchmarks to study how and to what extent the potential bias brought by preference leakage influences judgment.
    \vspace{-1mm}
    \item Our further analysis reveals that preference leakage is prevalent in diverse scenarios and difficult for judge LLMs to detect, providing valuable insights for future research on this challenging issue.
\end{itemize}

\section{Related Work}

\subsection{LLM-as-a-Judge}

LLM-as-a-Judge, introduced by \citet{zheng2023judging}, leverages LLMs to automatically evaluate responses and assign rewards. This approach has gained widespread adoption in areas such as model alignment \cite{zhang-etal-2024-self} and benchmarking \cite{liu2023alignbench,zhang2024balancing,gao2023human,zhong2024law}, driving significant progress in the field. Building on this concept, \citet{zhuge2024agent} proposed Agent-as-a-Judge, where agentic systems are employed to evaluate other agentic systems. Additionally, Prometheus, a series of open-source LLMs tailored for LLM-as-a-Judge \cite{kim2023prometheus, kim2024prometheus}, addresses the prohibitive costs associated with proprietary models, further democratizing the technology.

Despite its promising potential, recent studies have highlighted the vulnerabilities and limitations of LLM-as-a-Judge. Notable concerns include biases during evaluation. For example, \citet{zheng2023judging} identify position bias, where LLMs may favor responses based on their order in the input, thereby compromising fairness. Other studies \cite{ye2024justice, koo2023benchmarking, chen2024humans, zheng2023judging, huang2024position} further emphasize the risks of evaluation biases. \citet{thakur2024judging} assessed the judgment capabilities of LLM judges, finding that only the most advanced models align reasonably well with human evaluators. Moreover, a recent study \cite{10.1145/3658644.3690291} revealed the susceptibility of LLM-as-a-Judge to adversarial attacks, leading to incorrect judgments. In this paper, we explore another critical vulnerability of LLM-as-a-Judge—preference leakage—which poses additional risks to the reliability of this evaluation paradigm.



% \citet{zheng2023judging} examined the LLM-as-a-judge in detail, pointing out the multiple biases such as positional bias, self-enhancement bias that may exist when the LLM serves as a judge.

\subsection{Data Leakage}

The possible overlap between training data and evaluation benchmarks has become a central issue, since LLMs are usually trained on extensive web corpora 
\cite{dodge2021documenting}. This phenomenon, known as data leakage, can artificially improve the performance of LLMs and undermine the reliability of the assessment \cite{deng2024unveiling, jiang2024investigating}.


Several researchers have proposed methods to detect and mitigate data contamination. \citet{deng2024investigating} proposed a retrieval-based approach to assess the degree of overlap between pre-training text and benchmark data. \citet{golchin2023time} have developed ``guided instruction'' to flag contaminated instances. \citet{dong2024generalization} proposed the CDD method to identify peaks in the output distribution to detect data contamination. Several studies analyze data leakage for specific LLMs \cite{balloccu2024leak} and report contamination such as cross-language contamination \cite{yao2024data} and task contamination \cite{li2024task} that can evade traditional detection methods. To address data contamination issues, \citet{ni2024mixeval} have used web user query detection and benchmark mixture. \citet{white2024livebench} use the most recent information to update the problem.

% Despite these efforts, data contamination remains a complex and challenging problem. The surge in synthetic pre-training data and the lack of open-source pre-training data pose a significant obstacle to solving this problem \cite{li2024open}.

\section{Preference Leakage}

In this section, we first provide the formal definition of data contamination as the preliminary (Section~\ref{Preliminary: Data Contamination}). Based on the concept, we demonstrate how LLM-based data synthesis and evaluation can lead to the evolving preference leakage problem (Section~\ref{From Data to Preference Leakage}).

\subsection{Preliminary: Data Leakage}
\label{Preliminary: Data Contamination}

Data leakage, also known as data contamination, refers to the inadvertent inclusion of information from the evaluation benchmarks into the training corpus thus creating an overlap between training and testing sets~\cite{kaufman2012leakage}. This overlap would significantly influence the evaluation fairness by inflating the models' performance since the model has prior exposure to and memorized information that it's expected to generalize during testing~\cite{elangovan2021memorization}.

Formally, let $T$ represent the training corpus and $E$ be the evaluation set during test time. Data contamination occurs if:
\begin{equation}
    T \cap E \neq \emptyset,
\end{equation}
where $\cap$ denotes the intersection between the two sets. Such overlap violates the fundamental assumption that training and testing datasets should be disjoint to ensure an unbiased assessment of the model's generalization ability.

\subsection{From Data Leakage to Preference Leakage}
\label{From Data to Preference Leakage}

With the advent of LLMs, synthetic data generated by these models~\cite{tan2024large} has been widely adopted in various stages of model training, including pre-training, reinforcement learning with AI feedback (RLAIF) and supervised fine-tuning. Concurrently, the concept of LLM-as-a-judge has emerged, where LLMs are employed to automate the evaluation process. While these LLM-as-an-oracle approaches reduce human effort in data annotation, significantly enhancing the efficiency and scalability of model training and evaluation, they also blur the lines between models and data, introducing evolving challenges~\cite{shumailov2024ai,dai2024bias}.

In this work, we examine the evolving contamination problem brought by LLM-as-a-oracle and formally propose the concept of preference leakage. This refers to a situation in which the LLMs used for synthetic data generation and evaluation are related. Formally, we define this as:
\begin{equation}
    LLM_{G} \cap LLM_{J} \neq \emptyset,
\end{equation}
% where $LLM_{G}$ and $LLM_{J}$ denote the LLMs used for training data generation and evaluation. $\cap$ represents the relatedness between the two (sets of) LLMs. This relatedness may involve being the same model, having an inheritance relationship, or within the same model family, which we will discuss further in Section~\ref{Results under Different Correlations}. Due to this relatedness, the preference of the judge models (e.g., format, style and wording) can be leaked to the student models through the synthetic data, resulting in non-trivial bias from the judge LLMs.
where $LLM_{G}$ and $LLM_{J}$ denote the LLMs used for training data generation and evaluation. $\cap$ represents the relatedness between the two (sets of) LLMs. This relatedness may involve:
\begin{itemize}[leftmargin=*,itemsep=1pt]
    \item \textbf{Being the same model:} the data generator and evaluator are the same model:
    \begin{equation}
        LLM_{G} = LLM_{J}.
    \end{equation}
    \item \textbf{Inheritance relationship:} one model is trained on synthetic data generated by the other:
    \begin{equation}
        LLM_{G} = \text{Inherit}(LLM_{J}),
    \end{equation}
    \begin{equation}
        LLM_{J} = \text{Inherit}(LLM_{G}).
    \end{equation}
    \item \textbf{Within the same model family:} the data generator and evaluator belong to the same model family (e.g., GPT family~\cite{achiam2023gpt} and Gemini family~\cite{team2024gemini}):
    \begin{equation}
        LLM_{G}, LLM_{J} \in F_{X}.
    \end{equation}
\end{itemize}
Due to this relatedness, the preference of the judge models (e.g., format, style and wording) can be leaked to the student models through the synthetic data, resulting in non-trivial bias from the judge LLMs during the test time.

\section{Main Experiment}

\begin{figure*}[h]
    \centering
    \includegraphics[width=1.0\linewidth]{Figure/ArenaHard_result.pdf}
    \vspace{-15pt}
    \caption{Judgment results with GPT-4o, LLaMA-3.3 and Gemini-1.5 on Arena-Hard.}
    \label{fig:ArenaHard}
\end{figure*}


\subsection{Experiment Setup}

\textbf{Models.} We choose three powerful LLMs as data generator/ judge models. They are GPT-4o-2024-11-20~\cite{achiam2023gpt}, Gemini-1.5-flash~\cite{team2024gemini}, and LLaMA-3.3-70B-Instruct-turbo~\cite{dubey2024llama}. For the student model, we choose Mistral-7B-v0.1~\cite{jiang2023mistral} and Qwen-2.5-14B~\cite{yang2024qwen2}. To avoid potential preference leakage due to distilling data from other LLMs during the instruction-tuning process, we choose to use the \textsc{-pre-trained} version rather than the \textsc{-instruct} version of these student models.

\textbf{Evaluation Datasets.} We choose two representative pairwise evaluation datasets, Arena-Hard \cite{li2024crowdsourced} and AlpacaEval 2.0 \cite{dubois2024length}, to evaluate the trained student models. Arena-Hard includes 500 challenging questions in English. Additionally, the evaluation agreement between Arena-Hard and Chatbot Arena~\cite{zheng2023judging}'s hard prompts achieved a 96.7\% Spearman correlation, demonstrating the consistency of Arena-Hard with human preferences~\cite{li2024crowdsourced}. AlpacaEval 2.0 is an improved evaluation method based on AlpacaEval \cite{li2023alpacaeval} and contains 805 questions. Compared to version 1.0, AlpacaEval 2.0 significantly reduces the effect of text length on the evaluation results.

\textbf{Implementation Details.} 
% same model & sft setting
% model pair
In our main experiment, we examine the preference leakage introduced by using the same data generator and evaluator in supervised fine-tuning (SFT). We will discuss other relatedness and learning methods in Section~\ref{Further Analysis}. To obtain synthetic datasets, We first randomly sample 30,000 prompts from the Ultrafeedback dataset \cite{cui2024ultrafeedback}. The Ultrafeedback dataset includes instructions from several publicly available high-quality datasets such as TruthfulQA \cite{lin2022truthfulqa}, FalseQA \cite{hu2023won}, and Evol-Instruct \cite{xu2023wizardlm}. For each data generator model, we provide these prompts for them to produce synthetic responses, resulting in three synthetic instruction datasets. We then use each dataset to supervised fine-tune the student model, obtaining three different versions for each baseline: Mistral/ Qwen-GPT-4o, Mistral/ Qwen-Gemini-1.5 and Mistral/ Qwen-LLaMA-3.3. After that, we pair each two student models and obtain three model pairs. For each model pair, we perform the pairwise comparison using the three judge models respectively.


\begin{figure*}[h]
    \centering
    \includegraphics[width=1.0\linewidth]{Figure/alpacaeval_result.pdf}
    \vspace{-15pt}
    \caption{Judgment results with GPT-4o, LLaMA-3.3 and Gemini-1.5 on AlpacaEval 2.0. Different from Arena-Hard, there is no tie in AlpacaEval 2.0.}
    \label{fig:alpacaeval}
    \vspace{-15pt}
\end{figure*}


\textbf{Metrics \& Annotation}
% preference leakage score
% human annotation
Based on our hypothesis, preference leakage would lead to bias of judge LLMs towards their related student models. Following this principle, we design the preference leakage score $\text{PLS}(i, j)$ to measure the bias in model pair $(i, j)$ caused by preference leakage:
\begin{equation}
    \text{PLS}(i, j) = \frac{\left(\frac{\text{WR}(i, i) - \text{AVG}(i, j)}{\text{AVG}(i, j)}\right) + \left(\frac{\text{WR}(j, j) - \text{AVG}(j, i)}{\text{AVG}(j, i)}\right)}{2},
\end{equation}
\begin{equation}
    \text{AVG}(i, j) = \frac{\text{WR}(i, i) + \text{WR}(i, j)}{2}.
\end{equation}
Here $\text{WR}(i, j)$ represents the win-rate score from judge model $i$ to student model $j$. Intuitively, a large preference leakage score indicates that the two judge models demonstrate strong bias toward their related student models, suggesting a significant preference leakage phenomenon.

While our proposed preference leakage score quantifies the degree of preference leakage in each model pair, we also perform manual annotation to assess the preference leakage in each individual model. We randomly select 100 questions from AlpacaEval 2.0 and have three well-trained annotators perform pairwise comparisons independently for each response pair. After the annotation, the majority voting is applied to each response pair to get the final annotation results. 
% \wei{was the annotation done independently and then taken majority votes?}

More details about model training, metric explanation, and annotation process can be found in Appendix~\ref{Experiment Detail}.

% For each teacher model, we provided the same instructions for them to generate answers. In this way, we obtained three types of instruction-answer pairs, i.e., instruction data. The instructions were obtained by random sampling from the Ultrafeedback dataset \cite{cui2024ultrafeedback}. The Ultrafeedback dataset includes instructions from several publicly available high-quality datasets such as TruthfulQA \cite{lin2022truthfulqa}, FalseQA \cite{hu2023won}, and Evol-Instruct \cite{xu2023wizardlm}. We then used the instruction data to supervised fine-tune the student model, obtaining different versions: Mistral-sft-by-GPT-4o, Mistral-sft-by-Gemini, Mistral-sft-by-llama.

% \textbf{Training Details.} We used 8 Nvidia A100 GPUs to supervised fine-tune the student model. Due to the space limit, more details are shown in Appendix X.


\begin{table}[h!]
\centering
\scalebox{0.64}{
\begin{tabular}{llccc}
\toprule[1.5pt]
Model                          & Data Generator/ Judge Pair            & Arena-Hard                      & AlpacaEval 2.0                  & Avg.                           \\ \hline
& GPT-4o \& Gemini-1.5     & \cellcolor[HTML]{C1AFEC}28.7\% & \cellcolor[HTML]{D7CBF2}18.4\% & \cellcolor[HTML]{CCBDEF}23.6\% \\
& GPT-4o \& LLaMA-3.3      & \cellcolor[HTML]{EFF7FA}-6.7\% & \cellcolor[HTML]{FCFBFE}1.4\%  & \cellcolor[HTML]{F8FBFD}-2.7\% \\
\multirow{-3}{*}{Mistral-7B}   & LLaMA-3.3 \& Gemini-1.5 & \cellcolor[HTML]{E2DAF6}13.1\% & \cellcolor[HTML]{D4C7F1}19.8\% & \cellcolor[HTML]{DBD1F4}16.4\% \\  \hline
& GPT-4o \& Gemini-1.5     & \cellcolor[HTML]{AF97E6}37.1\%   & \cellcolor[HTML]{D7CBF2}18.6\% & \cellcolor[HTML]{C3B1EC}27.9\%                         \\
& GPT-4o \& LLaMA-3.3      & \cellcolor[HTML]{FCFCFE}1.0\%    & \cellcolor[HTML]{FAF8FD}2.3\%  & \cellcolor[HTML]{FBFAFD}1.7\%                          \\
\multirow{-3}{*}{Qwen-2.5-14B} & LLaMA-3.3 \& Gemini-1.5 & \cellcolor[HTML]{C8B8EE}25.4\%   & \cellcolor[HTML]{D7CBF2}18.4\% & \cellcolor[HTML]{D0C2F0}21.9\%    \\ \toprule[1.5pt]           
\end{tabular}}
\caption{Preference leakage score result on Arena-Hard and AlpacaEval 2.0. The \colorbox{myblue}{blue} background indicates a negative preference leakage score value and the \colorbox{mypurple}{purple} background indicates a positive value. The deeper the color, the larger the absolute value.}
\label{tab:pls}
\vspace{-15pt}
\end{table}


\subsection{Main Results}
\label{Result Analysis}

In our main experiment, we aim to provide insights into RQ1.

\textbf{Preference leakage exists in most model pairs.} The original judgment results from Arena-Hard and AlpacaEval 2.0, along with the calculated preference leakage scores, are shown in Figure~\ref{fig:ArenaHard}, Figure~\ref{fig:alpacaeval}, and Table~\ref{tab:pls}. As the results demonstrate, in most model pairs (except Mistral-GPT-4o vs Mistral-LLaMA-3.3 and Qwen-GPT-4o vs Qwen-LLaMA-3.3), the judge LLMs exhibit a strong preference toward their related student models, leading to large positive values in the preference leakage scores. This finding suggests that preference leakage, along with the resulting bias, is widespread in SFT when the data generator and evaluator are the same.

\begin{figure}[h]
    \centering
    \includegraphics[width=0.75\linewidth]{Figure/llama_mtbench_result.pdf}
    \vspace{-10pt}
    \caption{Comparison between GPT-4 and human's judgment for LLaMA-2 from MTBench.}
    \label{fig:llama}
    \vspace{-10pt}
\end{figure}

\begin{figure*}[h!]
    \centering
    \includegraphics[width=1.0\linewidth]{Figure/human_result.pdf}
    \vspace{-15pt}
    \caption{Manual annotation result on 100 randomly selected samples from AlpacaEval 2.0.}
    \label{fig:human annotation}
    \vspace{-15pt}
\end{figure*}

\textbf{Evaluators' bias towards certain LLMs can be inherited by its student models.} From Figure~\ref{fig:ArenaHard} and Figure~\ref{fig:alpacaeval}, we find an obvious preference of GPT-4o towards Mistral/ Qwen-LLaMA-3.3 and this leads to the low preference leakage score in the Mistral-GPT-4o vs Mistral-LLaMA-3.3 and Qwen-GPT-4o vs Qwen-LLaMA-3.3 pairs. To investigate the source of this preference, we examine whether the GPT-4 evaluator has a bias toward LLaMA series models. Using the MTBench~\cite{zheng2023judging} dataset, which includes pairwise comparison judgments from both humans and GPT-4, we compare GPT-4's and human evaluators' judgments on LLaMA-2 vs other models (including Vicuna, Alpaca, GPT-3.5, and GPT-4, which are preferred by GPT-4 due to preference leakage or egocentric bias) and LLaMA-2 vs Claude. The results, shown in Figure~\ref{fig:llama}, reveal a clear preference for LLaMA-2 by GPT-4. Consequently, we conclude that evaluators' bias can be inherited. In this case, GPT-4’s bias toward LLaMA has been passed on to LLaMA’s student models. This inheritance, combined with the opaque training data of LLMs, makes preference leakage a more complex and challenging problem.


\textbf{Model pairs with similar performance tend to have more obvious preference leakage.} As shown in Table~\ref{tab:pls}, we observe that the preference leakage scores for Mistral-GPT-4o vs Mistral-Gemini-1.5 and Qwen-GPT-4o vs Qwen-Gemini-1.5 (23.6\% and 27.9\% respectively) are consistently higher than that for Mistral-LLaMA-3.3 vs Mistral-Gemini-1.5 and Qwen-LLaMA-3.3 vs Qwen-Gemini-1.5 (16.4\% and 21.9\% respectively). We think that this is largely due to 
%that one possible explanation for this is 
the more comparable performance between the student models of GPT-4o and Gemini-1.5. Intuitively, when the quality of the two responses is similar, the evaluator may rely more heavily on its inherent preferences to make a judgment, thereby exacerbating the preference leakage issue.

\textbf{Larger student models cause more bias from judge LLMs.} Another observation from Table~\ref{tab:pls} is that the overall preference leakage score for Qwen-2.5-14B is higher than that for Mistral-7B. Drawing on insights from previous studies on data leakage, which suggest that larger and more powerful LLMs are more capable of memorizing extensive information and are thus more susceptible to data contamination~\cite{bordt2023elephants,duan2024uncovering}, we attribute this difference in preference leakage to the size and capabilities of the student LLMs. We assume that larger student models, due to their better performance and generalization abilities, are more capable of learning and memorizing the hidden preference pattern from the synthetic data, thus leading to a more serious preference leakage.

\textbf{Different data generator/ judge LLMs result in varying degrees of bias under preference leakage.} While we have concluded that student model pairs with similar performance or more powerful student models tend to exhibit greater preference leakage, we also examine whether different data generator and judge LLMs contribute to varying degrees of preference leakage. Analyzing the manual annotation results presented in Table~\ref{fig:human annotation}, we observe that Gemini-1.5 shows a strong bias toward its students, followed by GPT-4o, with LLaMA-3.3 displaying the least bias. This variation in preference leakage may stem from differences in the level of leaked preference in the synthetic responses generated by the data generator LLMs. For instance, an LLM with a distinctive style or format in its responses offers more opportunities for student models to learn these characteristics, potentially leading to more pronounced preference leakage during evaluation. Future work could 
further quantify the extent of leaked preference for each data generator model.

\section{Further Analysis}
\label{Further Analysis}

In this section, we conduct relatedness analysis, learning method analysis and data mixing analysis (Section~\ref{Results under Different Correlations} - \ref{Data Mixing Analysis}) to answer RQ2. Due to the cost consideration, we conduct these analyses on Mistral-GPT-4o vs Mistral-Gemini-1.5. Moreover, we perform recognition analysis and category analysis to answer RQ3. 

\begin{table}[h]
\centering
\small
\setlength\tabcolsep{2.5pt}
\begin{tabular}{lccc}
\toprule[1.5pt]
& Arena-Hard    & AlpacaEval 2.0     & Avg.                           \\ \hline
Same Model  & \cellcolor[HTML]{C1AFEC}28.7\% & \cellcolor[HTML]{D7CBF2}18.4\% & \cellcolor[HTML]{CCBDEF}23.6\% \\ \hline
\begin{tabular}[c]{@{}l@{}}Inheritance\\   \ \ \ w/ same ins.\end{tabular}        & \cellcolor[HTML]{D8CDF3}17.8\% & \cellcolor[HTML]{D2C5F1}20.7\% & \cellcolor[HTML]{D5C9F2}19.3\% \\
\begin{tabular}[c]{@{}l@{}}Inheritance\\   \ \ \ w/ different ins.\end{tabular}   & \cellcolor[HTML]{D7CCF2}18.3\% & \cellcolor[HTML]{C6B5ED}26.3\% & \cellcolor[HTML]{CFC1F0}22.3\% \\ \hline
\begin{tabular}[c]{@{}l@{}}Same Family\\   \ \ \ w/ same series\end{tabular}      & \cellcolor[HTML]{E9E2F8}10.1\% & \cellcolor[HTML]{EEE9F9}7.6\%  & \cellcolor[HTML]{EBE6F9}8.9\%  \\
\begin{tabular}[c]{@{}l@{}}Same Family\\   \ \ \ w/ different series\end{tabular} & \cellcolor[HTML]{F7F5FC}3.3\%  & \cellcolor[HTML]{FAF8FD}2.2\%  & \cellcolor[HTML]{F9F7FD}2.8\%  \\ \toprule[1.5pt]
\end{tabular}
\caption{Preference leakage score in different relatedness between the data generator and the judging LLM.}
\label{tab:relatedness}
\end{table}


\subsection{Relatedness Analysis}
\label{Results under Different Correlations}

We demonstrate the impact of different relatedness conditions between the data generator and the judge LLM on the preference leakage problem, as shown in Table \ref{tab:relatedness}. 

\textbf{Preference leakage under inheritance settings causes obvious bias of judges towards their related students.} For the inheritance relationship, we consider the situation where the data generator is inherited from the judge model. We conducted the following two experiments: (1). we give the same instructions again as in the SFT stage (Inheritance w/ same ins.), or (2). we sample the same number of different instructions from the Ultrafeedback (Inherence w/ different ins.). Then, we let the fine-tuned Mistral model generate the answers and use these generated data to fine-tune a new Mistral student model. From the results, with the same instructions, the average preference leakage score is 19.3\%. In comparison, the score with different instructions is 22.3\%. Firstly, in an inheritance setting, data generators can inherit judges' preferences, which are then passed on to new student models, thereby compromising the fairness of their evaluation. Second, even when different instructions are used, judges' preferences leaked to data generators can still be transferred to the new student model through synthetic data, leading to a high preference leakage score.

\textbf{Models within the same series tend to cause more significant bias.} For two models within the same family, we consider two settings: (1) Same series, where training data is generated by GPT-4o and Gemini-1.5-flash, and judged by GPT-4-turbo and Gemini-1.5-pro; (2) Different series, where training data is still generated by GPT-4o and Gemini-1.5-flash, but judged by GPT-3.5-turbo and Gemini-1.0-pro.
In the same series setting, the average preference leakage score is 8.9\%, indicating that despite using different models for data generation and judgment, their relatedness in terms of model family leads to some preference leakage. In contrast, the different series setting yields a significantly lower leakage score of 2.8\%, likely due to differences in architecture, training data, and other factors, reducing the influence of model-related biases in evaluation.


\begin{table}[h]
\centering
\small
\begin{tabular}{lccc}
\toprule[1.5pt]
    & Arena-Hard                      & AlpacaEval 2.0                  & Avg.                           \\ \hline
SFT & \cellcolor[HTML]{C1AFEC}28.7\% & \cellcolor[HTML]{D7CBF2}18.4\% & \cellcolor[HTML]{CCBDEF}23.6\% \\
DPO & \cellcolor[HTML]{EEE9F9}7.7\%  & \cellcolor[HTML]{F9F7FD}2.7\%  & \cellcolor[HTML]{F3F0FB}5.2\%  \\
ICL & \cellcolor[HTML]{F5FAFC}-4.2\% & \cellcolor[HTML]{FCFDFE}-1.1\% & \cellcolor[HTML]{F8FBFD}-2.7\% \\ \toprule[1.5pt]
\end{tabular}
\caption{Preference leakage score in different learning methods.}
\label{tab:learning method}
\vspace{-15pt}
\end{table}


\subsection{Learning Method Analysis}
\label{Results under Different Learning Methods}

We also compare three learning methods, supervised fine-tuning (SFT), direct preference optimization (DPO) \cite{rafailov2024direct}, and in-context learning (ICL) \cite{dong2024survey}, to explore the different influences to them under preference leakage. We first build a data pool based on human-written instruction-tuning data from OASST \cite{kopf2024openassistant}, LIMA \cite{zhou2024lima}, and MOSS \cite{Sun2024MOSS} to supervised fine-tune the pre-trained model. For DPO, we sample 2 responses for each instruction from sampled UltraFeedback instruction and prompt each data generator to produce the pairwise feedback. Then we use the DPO loss to further train the fine-tuned policy on each synthetic pairwise dataset.  Appendix~\ref{Learning Method Analysis Details} shows the prompt we use to craft synthetic pairwise feedback. For ICL, we sample 4 instruction-response pairs from each LLMs' synthetic dataset as the demonstration during inference.

\textbf{Tuning approaches would leak judges' preference to the student models.} Various learning methods show significant differences in preference leakage scores across learning methods. SFT exhibits the highest average leakage score at 23.6\%. In contrast, DPO achieves a much lower score of 5.2\%, likely because its focus on preferences helps minimize the unintended transfer of judge model biases. Meanwhile, ICL, which relies on contextual examples without updating model parameters, is least affected by the data generator’s preferences, resulting in the lowest leakage scores.

\begin{figure}[h]
    \centering
    \includegraphics[width=0.8\linewidth]{Figure/ratio.pdf}
    \vspace{-10pt}
    \caption{Experiment results on data mixing. `Manual' represents the original synthetic data mixed with manually-written data. `Synthetic' represents the original data mixed with other synthetic data.}
    \label{fig:ratio}
    \vspace{-15pt}
\end{figure}


\subsection{Data Mixing Analysis}
\label{Data Mixing Analysis}

In real-world applications, synthetic data from a single LLM is often mixed with manually-written data or other multi-source synthetic data to train student models. To mimic these scenarios and explore how much synthetic data could lead to preference leakage, we conduct a data mixing analysis. Specifically, we randomly sample 10\%, 30\%, 50\%, and 70\% from the original synthetic dataset and mix it with manually-written data and multi-source synthetic data, respectively, in order to maintain a consistent total volume of training data (30,000). For the manually-written data, we sample from the data pool collected in Section~\ref{Results under Different Learning Methods}. For the multi-source synthetic data, we use the original synthetic data from Ultrafeedback, which includes responses generated by various LLMs (e.g., WizardLM, Flcon, etc.). After obtaining the mixing training data, we train the student models using SFT and calculate their preference leakage scores based on the judgment results. Figure~\ref{fig:ratio} presents the results with two mixing strategies across two benchmarks.

\textbf{The degree of preference leakage is directly proportional to the amount of synthetic data.} We observe a strong correlation between the proportion of synthetic data in the mixture and the preference leakage score, with no clear threshold separating cases with preference leakage from those without. This suggests that preference leakage can occur even with a small amount of leaked synthetic data, posing significant challenges for its detection.


\begin{table}[h]
\centering
\small
\setlength{\tabcolsep}{8pt}
\begin{tabular}{ccc}
\toprule[1.5pt]
Task                                 & Model      & Accuracy \\ \hline
\multirow{3}{*}{Student Recognition} & GPT-4o      & 60.0\%   \\
                                     & Gemini-1.5 & 25.4\%   \\
                                     & LLaMA-3.3  & 54.2\%   \\ \hline
Response Classification              & BERT       & 82.4\%   \\ \toprule[1.5pt]
\end{tabular}
\caption{Student recognition (binary classification) and response classification results (three-class classification).}
\label{tab:recognition}
\vspace{-15pt}
\end{table}


\begin{figure*}[h]
    \centering
    \subfigure[Question Type]{
        \includegraphics[width=0.45\linewidth]{Figure/category_data.pdf}
    }
    \subfigure[Judgment dimension]{
        \includegraphics[width=0.45\linewidth]{Figure/category_dimension.pdf}
    }
    \vspace{-10pt}
    \caption{Category analysis results on question type and judgment dimension.}
    \label{fig:category}
    \vspace{-15pt}
\end{figure*}


\subsection{Can Judges Recognize Student Models?}

Previous studies demonstrate the LLM judges can recognize and thus prefer their own generation~\cite{panickssery2024llm}. In this work, we pose a similar question: \textit{Does preference leakage also source from the LLM judges' recognition of their related student models' generation?} To study this, we follow~\citet{panickssery2024llm} to prompt the three judge LLMs and test whether they could recognize their related student models' generation. Additionally, we split three student models' generation into training and testing sets, and train a BERT classifier to perform a three-class classification inspired by the previous study on detecting human-AI text \cite{zhang-etal-2024-llm}. Detailed instruction and training settings can be found in Appendix~\ref{Recogniton Analysis Details}.

\textbf{Judge LLMs do not show good performance in recognizing the generation of their student models.} As the result presented in Table~\ref{tab:recognition}, we find that the recognition performance of each judge LLM in the content of related students is poor, with accuracy around the performance of random guess. Moreover, we observe no correlation between recognition performance and the preference leakage degree for judge LLMs. For instance, while Gemini-1.5 leads to the most preference leakage (as shown in Section~\ref{Result Analysis}), it performs the worst in recognition tasks. These suggest that preference leakage is subtler and harder-to-detect for judge LLMs, in contrast to the more obvious egocentric bias.


\textbf{Certain features embedded in student models through synthetic data.} Although judge LLMs do not perform well in related student recognition, we notice the fine-tuned BERT classification demonstrates a high accuracy score in classifier response generated by each student model. This suggests that certain characteristics—such as style and format—are embedded in the student models through the synthetic responses. This finding further supports the existence of preference leakage and lays the groundwork for future research aimed at detecting and preventing it.

\subsection{Impact on Question Type \& Judgment Dimension}

In this section, we explore the impact of preference leakage across various question types and judgment dimensions. For the question type analysis, we first propose several general question types based on the question clusters introduced by Arena-Hard. Then, we prompt GPT-4o to map each question in Arena-Hard and AlpacaEval to one of the question types and calculate the preference leakage score for each question category. For the judgment dimension analysis, we follow the judgment dimensions introduced by~\citet{liu2023alignbench} and also utilize GPT-4o to map the rationale generated by judge LLMs to one or multiple judgment dimensions. More detailed prompt can be found in Appendix~\ref{Category Analysis Details}. The analysis results are presented in Figure~\ref{fig:category}. 
% \wei{judgment dimension or judgment dimension?}

\textbf{Subjective question and judgment dimension tend to lead to more bias.} For question type analysis, we find objective questions with a definitive answer, like mathematical ones, demonstrate the least preference leakage. By contrast, subjective questions that have more than one standard answer, such as programming and writing, usually lead to a more obvious preference leakage. This observation is also applied to judgment dimension analysis, as objective dimensions (like completeness) have an overall lower leakage degree compared with subjective ones (like fairness). This suggests that preference leakage tends to be more significant in objective questions and dimensions, where the contaminated model is more likely to receive biased preference.



\section{Conclusion}
In this work, we formally highlight the preference leakage problem in LLM-as-a-judge systems. The results of our main experiment, measured using the proposed preference leakage score, reveal a clear bias in each judge toward its respective student model. We also observe that this bias is more pronounced in comparable model pairs and larger student models. Furthermore, we conduct additional analysis on various factors, including the relationship between the data generator and judge LLMs, model tuning techniques, and data mixing strategies. Our findings suggest that preference leakage can cause significant bias across diverse scenarios. Finally, through recognition and category analyses, we investigate the underlying mechanisms of preference leakage, demonstrating that it is a challenging and hard-to-detect issue, especially in subjective questions and judgment dimensions. In the future, we aim to explore methods for detecting, preventing, and mitigating this evolving challenge in LLM-as-a-judge systems.

\section*{Impact Statements}

By revealing preference leakage, this work could help build more trustworthy and ethically grounded AI systems. The relatedness between data generators and evaluators can systematically bias evaluations, potentially compromising the fairness and reliability of the automatic evaluation paradigm. These biased evaluations may indirectly affect downstream tasks such as AI alignment and decision-making systems, leading to unintended ethical risks. To mitigate preference leakage, we hope that researchers will propose more reliable evaluation methods, diversify training data sources, and develop contamination-resistant benchmarks in the future.

\bibliography{example_paper}
\bibliographystyle{icml2025}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% APPENDIX
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage
\appendix
\onecolumn
\section{Preliminary Study of Preference Leakage in Real World}
\label{Preliminary Study of Preference Leakage in Real World}

In our preliminary study, we investigate whether preference leakage is a real-world issue in mainstream leaderboards and benchmarks. To this end, we examine two widely used LLM-as-a-judge leaderboards (AlpacaEval 2.0 and Arena-Hard) and a well-known benchmark (MTBench). All three rely on GPT-4 as the judge model and report pairwise judgment results for various LLMs. Our analysis reveals that several candidate models distilled from GPT-4 or other GPT-series models (e.g., Vicuna and Alpaca) appear across all these leaderboards and benchmarks, suggesting that preference leakage is a pervasive issue in these datasets. Besides, we also examine if preference leakage exists in LLM-relevant research studies and also find a bunch of work utilizing the same or related model(s) to do distillation/ data synthesis and evaluation~\cite{yang2023new,liumakes,leerlaif,li2024selective,wang2024bpo,sun2024fostering}. All of these suggest preference leakage to be a widespread problem in both LLM-as-a-judge datasets and LLM-relevant research.


\section{Experiment Details}
\label{Experiment Detail}

\subsection{Training Details}
We use LLaMA-Factory~\cite{zheng2024llamafactory}, an efficient LLM tuning library for our experiment. The maximum sequence length is set to 1024 tokens, and a cutoff length of 1024 tokens is enforced to prevent excessive tokenization. The data preprocessing will be done in parallel with 16 workers to speed up the preparation process. The training use a per-device batch size of 2, with gradient accumulation over 2 steps to simulate a larger batch size for SFT and a per-device batch size of 1, with gradient accumulation over 4 steps to simulate a larger batch size for DPO. The learning rate is set to 1.0e-5 and each model will be trained for 3 epochs. A cosine learning rate scheduler is used with a warmup ratio of 0.1 to gradually increase the learning rate during the initial steps. All of the experiments use BF16 precision to speed up training while maintaining numerical stability. All the experiments are conducted in an 8 Nvidia A100 GPU cluster with CUDA version 11.8.


\begin{table}[h!]
\centering
\begin{tabular}{lcc}
\toprule[1.5pt]
\multirow{2}{*}{Judge Model} & \multicolumn{2}{c}{Mistral-GPT-4o vs Mistral-Gemini-1.5} \\ \cmidrule(l){2-3}
                             & Mistral-GPT-4o Wins       & Mistral-Gemini-1.5 Wins      \\ \hline
GPT-4o                       & 55.1\%                    & 44.9\%                       \\
Gemini-1.5                   & 36.8\%                    & 63.2\%                       \\ \hline
Preference Leakage Score     & \multicolumn{2}{c}{18.4\%}   \\ \toprule[1.5pt]                           
\end{tabular}
\label{tab:pls case}
\caption{A case on AlpacaEval 2.0 with the model pair Mistral-GPT-4o vs Mistral-Gemini-1.5 to demonstrate how the preference leakage score is calculated.}
\end{table}


\subsection{Detailed Explanation for Preference Leakage Score}
We present a case in Table~\ref{tab:pls case} to show how we calculate the preference leakage score for the Mistral-GPT-4o vs Mistral-Gemini-1.5 pair on AlpacaEval 2.0. Based on the definition of preference leakage score, we first calculate: 
\begin{equation}
    \text{AVG}(\text{Mistral-GPT-4o}, \text{Mistral-Gemini-1.5})=\frac{55.1 + 36.8}{2}=45.95\%
\end{equation}
\begin{equation}
    \text{AVG}(\text{Mistral-Gemini-1.5},\text{Mistral-GPT-4o})=\frac{63.2 + 44.9}{2}=54.05\%
\end{equation}
After that, we calculate the preference leakage score:
\begin{equation}
    \text{PLS}(\text{Mistral-GPT-4o},\text{Mistral-Gemini-1.5})=\frac{\left(\frac{55.1 - 45.95}{45.95}\right) + \left(\frac{63.2 - 54.05}{54.05}\right)}{2}=18.4\%
\end{equation}
.


\subsection{Manual Annotation Details}
We randomly sample 100 questions from AlpacaEval 2.0 and ask three well-trained annotators to conduct pairwise comparisons of the responses from each model pair for these questions. For annotation efficiency, we also develop an annotation tool that involves the function of uploading multiple model responses, jumping to specific problems, and downloading annotation results (Figure~\ref{fig:tool}). After annotation, we adopt the majority voting to get the final label for each response pair. We also calculate the average agreement of three annotators and find it to be 78.6, indicating a relatively consistent annotation result.


\section{Learning Method Analysis Details}
\label{Learning Method Analysis Details}

The table below presents the prompt we use to generate synthetic pairwise feedback from each model.
\input{prompt_pairwise_feedback}


\begin{figure}[h!]
    \centering
    \includegraphics[width=1\linewidth]{Figure/annotation_tool.png}
    \caption{The annotation tool we develop for annotation efficiency.}
    \label{fig:tool}
\end{figure}


\section{Recogniton Analysis Details}
The table below presents the prompt we use for the recognition analysis.
\label{Recogniton Analysis Details}
\input{prompt_recognition}
For response classification, we split all the response from three student models into training (80\%) and testing (20\%) subsets. Then, we finetune a BERT-base-uncased model in the training set. The model is trained for 3 epochs with a learning rate of 2e-5, a batch size of 16 for both training and evaluation, and a weight decay of 0.01, with evaluations conducted at the end of each epoch.


\section{Category Analysis Details}
\label{Category Analysis Details}
The tables below present the prompt we use for question type and judgment dimension cateogory analysis.
%\label{Category Analysis Details}
\input{prompt_category_data}
\input{prompt_category_dimension}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\end{document}


% This document was modified from the file originally made available by
% Pat Langley and Andrea Danyluk for ICML-2K. This version was created
% by Iain Murray in 2018, and modified by Alexandre Bouchard in
% 2019 and 2021 and by Csaba Szepesvari, Gang Niu and Sivan Sabato in 2022.
% Modified again in 2023 and 2024 by Sivan Sabato and Jonathan Scarlett.
% Previous contributors include Dan Roy, Lise Getoor and Tobias
% Scheffer, which was slightly modified from the 2010 version by
% Thorsten Joachims & Johannes Fuernkranz, slightly modified from the
% 2009 version by Kiri Wagstaff and Sam Roweis's 2008 version, which is
% slightly modified from Prasad Tadepalli's 2007 version which is a
% lightly changed version of the previous year's version by Andrew
% Moore, which was in turn edited from those of Kristian Kersting and
% Codrina Lauth. Alex Smola contributed to the algorithmic style files.
