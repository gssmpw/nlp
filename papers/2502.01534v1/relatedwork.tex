\section{Related Work}
\subsection{LLM-as-a-Judge}

LLM-as-a-Judge, introduced by \citet{zheng2023judging}, leverages LLMs to automatically evaluate responses and assign rewards. This approach has gained widespread adoption in areas such as model alignment \cite{zhang-etal-2024-self} and benchmarking \cite{liu2023alignbench,zhang2024balancing,gao2023human,zhong2024law}, driving significant progress in the field. Building on this concept, \citet{zhuge2024agent} proposed Agent-as-a-Judge, where agentic systems are employed to evaluate other agentic systems. Additionally, Prometheus, a series of open-source LLMs tailored for LLM-as-a-Judge \cite{kim2023prometheus, kim2024prometheus}, addresses the prohibitive costs associated with proprietary models, further democratizing the technology.

Despite its promising potential, recent studies have highlighted the vulnerabilities and limitations of LLM-as-a-Judge. Notable concerns include biases during evaluation. For example, \citet{zheng2023judging} identify position bias, where LLMs may favor responses based on their order in the input, thereby compromising fairness. Other studies \cite{ye2024justice, koo2023benchmarking, chen2024humans, zheng2023judging, huang2024position} further emphasize the risks of evaluation biases. \citet{thakur2024judging} assessed the judgment capabilities of LLM judges, finding that only the most advanced models align reasonably well with human evaluators. Moreover, a recent study \cite{10.1145/3658644.3690291} revealed the susceptibility of LLM-as-a-Judge to adversarial attacks, leading to incorrect judgments. In this paper, we explore another critical vulnerability of LLM-as-a-Judge—preference leakage—which poses additional risks to the reliability of this evaluation paradigm.



% \citet{zheng2023judging} examined the LLM-as-a-judge in detail, pointing out the multiple biases such as positional bias, self-enhancement bias that may exist when the LLM serves as a judge.

\subsection{Data Leakage}

The possible overlap between training data and evaluation benchmarks has become a central issue, since LLMs are usually trained on extensive web corpora 
\cite{dodge2021documenting}. This phenomenon, known as data leakage, can artificially improve the performance of LLMs and undermine the reliability of the assessment \cite{deng2024unveiling, jiang2024investigating}.


Several researchers have proposed methods to detect and mitigate data contamination. \citet{deng2024investigating} proposed a retrieval-based approach to assess the degree of overlap between pre-training text and benchmark data. \citet{golchin2023time} have developed ``guided instruction'' to flag contaminated instances. \citet{dong2024generalization} proposed the CDD method to identify peaks in the output distribution to detect data contamination. Several studies analyze data leakage for specific LLMs \cite{balloccu2024leak} and report contamination such as cross-language contamination \cite{yao2024data} and task contamination \cite{li2024task} that can evade traditional detection methods. To address data contamination issues, \citet{ni2024mixeval} have used web user query detection and benchmark mixture. \citet{white2024livebench} use the most recent information to update the problem.

% Despite these efforts, data contamination remains a complex and challenging problem. The surge in synthetic pre-training data and the lack of open-source pre-training data pose a significant obstacle to solving this problem \cite{li2024open}.