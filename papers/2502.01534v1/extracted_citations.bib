@inproceedings{10.1145/3658644.3690291,
author = {Shi, Jiawen and Yuan, Zenghui and Liu, Yinuo and Huang, Yue and Zhou, Pan and Sun, Lichao and Gong, Neil Zhenqiang},
title = {Optimization-based Prompt Injection Attack to LLM-as-a-Judge},
year = {2024},
isbn = {9798400706363},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3658644.3690291},
doi = {10.1145/3658644.3690291},
abstract = {LLM-as-a-Judge uses a large language model (LLM) to select the best response from a set of candidates for a given question. LLM-as-a-Judge has many applications such as LLM-powered search, reinforcement learning with AI feedback (RLAIF), and tool selection. In this work, we propose JudgeDeceiver, an optimization-based prompt injection attack to LLM-as-a-Judge. JudgeDeceiver injects a carefully crafted sequence into an attacker-controlled candidate response such that LLM-as-a-Judge selects the candidate response for an attacker-chosen question no matter what other candidate responses are. Specifically, we formulate finding such sequence as an optimization problem and propose a gradient based method to approximately solve it. Our extensive evaluation shows that JudgeDeceive is highly effective, and is much more effective than existing prompt injection attacks that manually craft the injected sequences and jailbreak attacks when extended to our problem. We also show the effectiveness of JudgeDeceiver in three case studies, i.e., LLM-powered search, RLAIF, and tool selection. Moreover, we consider defenses including known-answer detection, perplexity detection, and perplexity windowed detection. Our results show these defenses are insufficient, highlighting the urgent need for developing new defense strategies.},
booktitle = {Proceedings of the 2024 on ACM SIGSAC Conference on Computer and Communications Security},
pages = {660â€“674},
numpages = {15},
keywords = {large language model, llm-as-a-judge, prompt injection attack},
location = {Salt Lake City, UT, USA},
series = {CCS '24}
}

@inproceedings{balloccu2024leak,
  title={Leak, Cheat, Repeat: Data Contamination and Evaluation Malpractices in Closed-Source LLMs},
  author={Balloccu, Simone and Schmidtov{\'a}, Patr{\'\i}cia and Lango, Mateusz and Du{\v{s}}ek, Ond{\v{r}}ej},
  booktitle={Proceedings of the 18th Conference of the European Chapter of the Association for Computational Linguistics (Volume 1: Long Papers)},
  pages={67--93},
  year={2024}
}

@article{chen2024humans,
  title={Humans or llms as the judge? a study on judgement biases},
  author={Chen, Guiming Hardy and Chen, Shunian and Liu, Ziche and Jiang, Feng and Wang, Benyou},
  journal={arXiv preprint arXiv:2402.10669},
  year={2024}
}

@inproceedings{deng2024investigating,
  title={Investigating Data Contamination in Modern Benchmarks for Large Language Models},
  author={Deng, Chunyuan and Zhao, Yilun and Tang, Xiangru and Gerstein, Mark and Cohan, Arman},
  booktitle={Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers)},
  pages={8698--8711},
  year={2024}
}

@article{deng2024unveiling,
  title={Unveiling the spectrum of data contamination in language models: A survey from detection to remediation},
  author={Deng, Chunyuan and Zhao, Yilun and Heng, Yuzhao and Li, Yitong and Cao, Jiannan and Tang, Xiangru and Cohan, Arman},
  journal={arXiv preprint arXiv:2406.14644},
  year={2024}
}

@inproceedings{dodge2021documenting,
  title={Documenting Large Webtext Corpora: A Case Study on the Colossal Clean Crawled Corpus},
  author={Dodge, Jesse and Sap, Maarten and Marasovi{\'c}, Ana and Agnew, William and Ilharco, Gabriel and Groeneveld, Dirk and Mitchell, Margaret and Gardner, Matt},
  booktitle={Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing},
  pages={1286--1305},
  year={2021}
}

@article{dong2024generalization,
  title={Generalization or memorization: Data contamination and trustworthy evaluation for large language models},
  author={Dong, Yihong and Jiang, Xue and Liu, Huanyu and Jin, Zhi and Gu, Bin and Yang, Mengfei and Li, Ge},
  journal={arXiv preprint arXiv:2402.15938},
  year={2024}
}

@article{gao2023human,
  title={Human-like summarization evaluation with chatgpt},
  author={Gao, Mingqi and Ruan, Jie and Sun, Renliang and Yin, Xunjian and Yang, Shiping and Wan, Xiaojun},
  journal={arXiv preprint arXiv:2304.02554},
  year={2023}
}

@article{golchin2023time,
  title={Time travel in llms: Tracing data contamination in large language models},
  author={Golchin, Shahriar and Surdeanu, Mihai},
  journal={arXiv preprint arXiv:2308.08493},
  year={2023}
}

@inproceedings{huang2024position,
  title={Position: TrustLLM: Trustworthiness in large language models},
  author={Huang, Yue and Sun, Lichao and Wang, Haoran and Wu, Siyuan and Zhang, Qihui and Li, Yuan and Gao, Chujie and Huang, Yixin and Lyu, Wenhan and Zhang, Yixuan and others},
  booktitle={International Conference on Machine Learning},
  pages={20166--20270},
  year={2024},
  organization={PMLR}
}

@article{jiang2024investigating,
  title={Investigating data contamination for pre-training language models},
  author={Jiang, Minhao and Liu, Ken Ziyu and Zhong, Ming and Schaeffer, Rylan and Ouyang, Siru and Han, Jiawei and Koyejo, Sanmi},
  journal={arXiv preprint arXiv:2401.06059},
  year={2024}
}

@inproceedings{kim2023prometheus,
  title={Prometheus: Inducing fine-grained evaluation capability in language models},
  author={Kim, Seungone and Shin, Jamin and Cho, Yejin and Jang, Joel and Longpre, Shayne and Lee, Hwaran and Yun, Sangdoo and Shin, Seongjin and Kim, Sungdong and Thorne, James and others},
  booktitle={The Twelfth International Conference on Learning Representations},
  year={2023}
}

@article{kim2024prometheus,
  title={Prometheus 2: An open source language model specialized in evaluating other language models},
  author={Kim, Seungone and Suk, Juyoung and Longpre, Shayne and Lin, Bill Yuchen and Shin, Jamin and Welleck, Sean and Neubig, Graham and Lee, Moontae and Lee, Kyungjae and Seo, Minjoon},
  journal={arXiv preprint arXiv:2405.01535},
  year={2024}
}

@article{koo2023benchmarking,
  title={Benchmarking cognitive biases in large language models as evaluators},
  author={Koo, Ryan and Lee, Minhwa and Raheja, Vipul and Park, Jong Inn and Kim, Zae Myung and Kang, Dongyeop},
  journal={arXiv preprint arXiv:2309.17012},
  year={2023}
}

@inproceedings{li2024open,
  title={An open-source data contamination report for large language models},
  author={Li, Yucheng and Guo, Yunhao and Guerin, Frank and Lin, Chenghua},
  booktitle={Findings of the Association for Computational Linguistics: EMNLP 2024},
  pages={528--541},
  year={2024}
}

@inproceedings{li2024task,
  title={Task contamination: Language models may not be few-shot anymore},
  author={Li, Changmao and Flanigan, Jeffrey},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  volume={38},
  pages={18471--18480},
  year={2024}
}

@article{liu2023alignbench,
  title={Alignbench: Benchmarking chinese alignment of large language models},
  author={Liu, Xiao and Lei, Xuanyu and Wang, Shengyuan and Huang, Yue and Feng, Zhuoer and Wen, Bosi and Cheng, Jiale and Ke, Pei and Xu, Yifan and Tam, Weng Lam and others},
  journal={arXiv preprint arXiv:2311.18743},
  year={2023}
}

@article{ni2024mixeval,
  title={MixEval: Deriving Wisdom of the Crowd from LLM Benchmark Mixtures},
  author={Ni, Jinjie and Xue, Fuzhao and Yue, Xiang and Deng, Yuntian and Shah, Mahir and Jain, Kabir and Neubig, Graham and You, Yang},
  journal={arXiv preprint arXiv:2406.06565},
  year={2024}
}

@article{thakur2024judging,
  title={Judging the Judges: Evaluating Alignment and Vulnerabilities in LLMs-as-Judges},
  author={Thakur, Aman Singh and Choudhary, Kartik and Ramayapally, Venkat Srinik and Vaidyanathan, Sankaran and Hupkes, Dieuwke},
  journal={arXiv preprint arXiv:2406.12624},
  year={2024}
}

@article{white2024livebench,
  title={Livebench: A challenging, contamination-free llm benchmark},
  author={White, Colin and Dooley, Samuel and Roberts, Manley and Pal, Arka and Feuer, Ben and Jain, Siddhartha and Shwartz-Ziv, Ravid and Jain, Neel and Saifullah, Khalid and Naidu, Siddartha and others},
  journal={arXiv preprint arXiv:2406.19314},
  year={2024}
}

@article{yao2024data,
  title={Data Contamination Can Cross Language Barriers},
  author={Yao, Feng and Zhuang, Yufan and Sun, Zihao and Xu, Sunan and Kumar, Animesh and Shang, Jingbo},
  journal={arXiv preprint arXiv:2406.13236},
  year={2024}
}

@article{ye2024justice,
  title={Justice or prejudice? quantifying biases in llm-as-a-judge},
  author={Ye, Jiayi and Wang, Yanbo and Huang, Yue and Chen, Dongping and Zhang, Qihui and Moniz, Nuno and Gao, Tian and Geyer, Werner and Huang, Chao and Chen, Pin-Yu and others},
  journal={arXiv preprint arXiv:2410.02736},
  year={2024}
}

@inproceedings{zhang-etal-2024-self,
    title = "Self-Alignment for Factuality: Mitigating Hallucinations in {LLM}s via Self-Evaluation",
    author = "Zhang, Xiaoying  and
      Peng, Baolin  and
      Tian, Ye  and
      Zhou, Jingyan  and
      Jin, Lifeng  and
      Song, Linfeng  and
      Mi, Haitao  and
      Meng, Helen",
    editor = "Ku, Lun-Wei  and
      Martins, Andre  and
      Srikumar, Vivek",
    booktitle = "Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = aug,
    year = "2024",
    address = "Bangkok, Thailand",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2024.acl-long.107/",
    doi = "10.18653/v1/2024.acl-long.107",
    pages = "1946--1965",
    abstract = "Despite showing impressive abilities, large language models (LLMs) often struggle with factual inaccuracies, i.e., {\textquotedblright}hallucinations{\textquotedblright}, even when they hold relevant knowledge. To mitigate these hallucinations, current approaches typically necessitate high-quality human factuality annotations. In this work, we explore Self-Alignment for Factuality, where we leverage the self-evaluation capability of an LLM to provide training signals that steer the model towards factuality. Specifically, we incorporate Self-Eval, a self-evaluation component, to prompt an LLM to validate the factuality of its own generated responses solely based on its internal knowledge. Additionally, we design Self-Knowledge Tuning (SK-Tuning) to augment the LLM`s self-evaluation ability by improving the model`s confidence estimation and calibration. We then utilize these self-annotated responses to fine-tune the model via Direct Preference Optimization algorithm. We show that the proposed self-alignment approach substantially enhances factual accuracy over Llama family models across three key knowledge-intensive tasks on TruthfulQA and BioGEN."
}

@article{zhang2024balancing,
  title={Balancing Speciality and Versatility: a Coarse to Fine Framework for Supervised Fine-tuning Large Language Model},
  author={Zhang, Hengyuan and Wu, Yanru and Li, Dawei and Yang, Zacc and Zhao, Rui and Jiang, Yong and Tan, Fei},
  journal={arXiv preprint arXiv:2404.10306},
  year={2024}
}

@article{zheng2023judging,
  title={Judging llm-as-a-judge with mt-bench and chatbot arena},
  author={Zheng, Lianmin and Chiang, Wei-Lin and Sheng, Ying and Zhuang, Siyuan and Wu, Zhanghao and Zhuang, Yonghao and Lin, Zi and Li, Zhuohan and Li, Dacheng and Xing, Eric and others},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  pages={46595--46623},
  year={2023}
}

@article{zhong2024law,
  title={Law of the weakest link: Cross capabilities of large language models},
  author={Zhong, Ming and Zhang, Aston and Wang, Xuewei and Hou, Rui and Xiong, Wenhan and Zhu, Chenguang and Chen, Zhengxing and Tan, Liang and Bi, Chloe and Lewis, Mike and others},
  journal={arXiv preprint arXiv:2409.19951},
  year={2024}
}

@article{zhuge2024agent,
  title={Agent-as-a-judge: Evaluate agents with agents},
  author={Zhuge, Mingchen and Zhao, Changsheng and Ashley, Dylan and Wang, Wenyi and Khizbullin, Dmitrii and Xiong, Yunyang and Liu, Zechun and Chang, Ernie and Krishnamoorthi, Raghuraman and Tian, Yuandong and others},
  journal={arXiv preprint arXiv:2410.10934},
  year={2024}
}

