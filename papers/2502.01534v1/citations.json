[
  {
    "index": 0,
    "papers": [
      {
        "key": "zheng2023judging",
        "author": "Zheng, Lianmin and Chiang, Wei-Lin and Sheng, Ying and Zhuang, Siyuan and Wu, Zhanghao and Zhuang, Yonghao and Lin, Zi and Li, Zhuohan and Li, Dacheng and Xing, Eric and others",
        "title": "Judging llm-as-a-judge with mt-bench and chatbot arena"
      }
    ]
  },
  {
    "index": 1,
    "papers": [
      {
        "key": "zhang-etal-2024-self",
        "author": "Zhang, Xiaoying  and\nPeng, Baolin  and\nTian, Ye  and\nZhou, Jingyan  and\nJin, Lifeng  and\nSong, Linfeng  and\nMi, Haitao  and\nMeng, Helen",
        "title": "Self-Alignment for Factuality: Mitigating Hallucinations in {LLM}s via Self-Evaluation"
      }
    ]
  },
  {
    "index": 2,
    "papers": [
      {
        "key": "liu2023alignbench",
        "author": "Liu, Xiao and Lei, Xuanyu and Wang, Shengyuan and Huang, Yue and Feng, Zhuoer and Wen, Bosi and Cheng, Jiale and Ke, Pei and Xu, Yifan and Tam, Weng Lam and others",
        "title": "Alignbench: Benchmarking chinese alignment of large language models"
      },
      {
        "key": "zhang2024balancing",
        "author": "Zhang, Hengyuan and Wu, Yanru and Li, Dawei and Yang, Zacc and Zhao, Rui and Jiang, Yong and Tan, Fei",
        "title": "Balancing Speciality and Versatility: a Coarse to Fine Framework for Supervised Fine-tuning Large Language Model"
      },
      {
        "key": "gao2023human",
        "author": "Gao, Mingqi and Ruan, Jie and Sun, Renliang and Yin, Xunjian and Yang, Shiping and Wan, Xiaojun",
        "title": "Human-like summarization evaluation with chatgpt"
      },
      {
        "key": "zhong2024law",
        "author": "Zhong, Ming and Zhang, Aston and Wang, Xuewei and Hou, Rui and Xiong, Wenhan and Zhu, Chenguang and Chen, Zhengxing and Tan, Liang and Bi, Chloe and Lewis, Mike and others",
        "title": "Law of the weakest link: Cross capabilities of large language models"
      }
    ]
  },
  {
    "index": 3,
    "papers": [
      {
        "key": "zhuge2024agent",
        "author": "Zhuge, Mingchen and Zhao, Changsheng and Ashley, Dylan and Wang, Wenyi and Khizbullin, Dmitrii and Xiong, Yunyang and Liu, Zechun and Chang, Ernie and Krishnamoorthi, Raghuraman and Tian, Yuandong and others",
        "title": "Agent-as-a-judge: Evaluate agents with agents"
      }
    ]
  },
  {
    "index": 4,
    "papers": [
      {
        "key": "kim2023prometheus",
        "author": "Kim, Seungone and Shin, Jamin and Cho, Yejin and Jang, Joel and Longpre, Shayne and Lee, Hwaran and Yun, Sangdoo and Shin, Seongjin and Kim, Sungdong and Thorne, James and others",
        "title": "Prometheus: Inducing fine-grained evaluation capability in language models"
      },
      {
        "key": "kim2024prometheus",
        "author": "Kim, Seungone and Suk, Juyoung and Longpre, Shayne and Lin, Bill Yuchen and Shin, Jamin and Welleck, Sean and Neubig, Graham and Lee, Moontae and Lee, Kyungjae and Seo, Minjoon",
        "title": "Prometheus 2: An open source language model specialized in evaluating other language models"
      }
    ]
  },
  {
    "index": 5,
    "papers": [
      {
        "key": "zheng2023judging",
        "author": "Zheng, Lianmin and Chiang, Wei-Lin and Sheng, Ying and Zhuang, Siyuan and Wu, Zhanghao and Zhuang, Yonghao and Lin, Zi and Li, Zhuohan and Li, Dacheng and Xing, Eric and others",
        "title": "Judging llm-as-a-judge with mt-bench and chatbot arena"
      }
    ]
  },
  {
    "index": 6,
    "papers": [
      {
        "key": "ye2024justice",
        "author": "Ye, Jiayi and Wang, Yanbo and Huang, Yue and Chen, Dongping and Zhang, Qihui and Moniz, Nuno and Gao, Tian and Geyer, Werner and Huang, Chao and Chen, Pin-Yu and others",
        "title": "Justice or prejudice? quantifying biases in llm-as-a-judge"
      },
      {
        "key": "koo2023benchmarking",
        "author": "Koo, Ryan and Lee, Minhwa and Raheja, Vipul and Park, Jong Inn and Kim, Zae Myung and Kang, Dongyeop",
        "title": "Benchmarking cognitive biases in large language models as evaluators"
      },
      {
        "key": "chen2024humans",
        "author": "Chen, Guiming Hardy and Chen, Shunian and Liu, Ziche and Jiang, Feng and Wang, Benyou",
        "title": "Humans or llms as the judge? a study on judgement biases"
      },
      {
        "key": "zheng2023judging",
        "author": "Zheng, Lianmin and Chiang, Wei-Lin and Sheng, Ying and Zhuang, Siyuan and Wu, Zhanghao and Zhuang, Yonghao and Lin, Zi and Li, Zhuohan and Li, Dacheng and Xing, Eric and others",
        "title": "Judging llm-as-a-judge with mt-bench and chatbot arena"
      },
      {
        "key": "huang2024position",
        "author": "Huang, Yue and Sun, Lichao and Wang, Haoran and Wu, Siyuan and Zhang, Qihui and Li, Yuan and Gao, Chujie and Huang, Yixin and Lyu, Wenhan and Zhang, Yixuan and others",
        "title": "Position: TrustLLM: Trustworthiness in large language models"
      }
    ]
  },
  {
    "index": 7,
    "papers": [
      {
        "key": "thakur2024judging",
        "author": "Thakur, Aman Singh and Choudhary, Kartik and Ramayapally, Venkat Srinik and Vaidyanathan, Sankaran and Hupkes, Dieuwke",
        "title": "Judging the Judges: Evaluating Alignment and Vulnerabilities in LLMs-as-Judges"
      }
    ]
  },
  {
    "index": 8,
    "papers": [
      {
        "key": "10.1145/3658644.3690291",
        "author": "Shi, Jiawen and Yuan, Zenghui and Liu, Yinuo and Huang, Yue and Zhou, Pan and Sun, Lichao and Gong, Neil Zhenqiang",
        "title": "Optimization-based Prompt Injection Attack to LLM-as-a-Judge"
      }
    ]
  },
  {
    "index": 9,
    "papers": [
      {
        "key": "zheng2023judging",
        "author": "Zheng, Lianmin and Chiang, Wei-Lin and Sheng, Ying and Zhuang, Siyuan and Wu, Zhanghao and Zhuang, Yonghao and Lin, Zi and Li, Zhuohan and Li, Dacheng and Xing, Eric and others",
        "title": "Judging llm-as-a-judge with mt-bench and chatbot arena"
      }
    ]
  },
  {
    "index": 10,
    "papers": [
      {
        "key": "dodge2021documenting",
        "author": "Dodge, Jesse and Sap, Maarten and Marasovi{\\'c}, Ana and Agnew, William and Ilharco, Gabriel and Groeneveld, Dirk and Mitchell, Margaret and Gardner, Matt",
        "title": "Documenting Large Webtext Corpora: A Case Study on the Colossal Clean Crawled Corpus"
      }
    ]
  },
  {
    "index": 11,
    "papers": [
      {
        "key": "deng2024unveiling",
        "author": "Deng, Chunyuan and Zhao, Yilun and Heng, Yuzhao and Li, Yitong and Cao, Jiannan and Tang, Xiangru and Cohan, Arman",
        "title": "Unveiling the spectrum of data contamination in language models: A survey from detection to remediation"
      },
      {
        "key": "jiang2024investigating",
        "author": "Jiang, Minhao and Liu, Ken Ziyu and Zhong, Ming and Schaeffer, Rylan and Ouyang, Siru and Han, Jiawei and Koyejo, Sanmi",
        "title": "Investigating data contamination for pre-training language models"
      }
    ]
  },
  {
    "index": 12,
    "papers": [
      {
        "key": "deng2024investigating",
        "author": "Deng, Chunyuan and Zhao, Yilun and Tang, Xiangru and Gerstein, Mark and Cohan, Arman",
        "title": "Investigating Data Contamination in Modern Benchmarks for Large Language Models"
      }
    ]
  },
  {
    "index": 13,
    "papers": [
      {
        "key": "golchin2023time",
        "author": "Golchin, Shahriar and Surdeanu, Mihai",
        "title": "Time travel in llms: Tracing data contamination in large language models"
      }
    ]
  },
  {
    "index": 14,
    "papers": [
      {
        "key": "dong2024generalization",
        "author": "Dong, Yihong and Jiang, Xue and Liu, Huanyu and Jin, Zhi and Gu, Bin and Yang, Mengfei and Li, Ge",
        "title": "Generalization or memorization: Data contamination and trustworthy evaluation for large language models"
      }
    ]
  },
  {
    "index": 15,
    "papers": [
      {
        "key": "balloccu2024leak",
        "author": "Balloccu, Simone and Schmidtov{\\'a}, Patr{\\'\\i}cia and Lango, Mateusz and Du{\\v{s}}ek, Ond{\\v{r}}ej",
        "title": "Leak, Cheat, Repeat: Data Contamination and Evaluation Malpractices in Closed-Source LLMs"
      }
    ]
  },
  {
    "index": 16,
    "papers": [
      {
        "key": "yao2024data",
        "author": "Yao, Feng and Zhuang, Yufan and Sun, Zihao and Xu, Sunan and Kumar, Animesh and Shang, Jingbo",
        "title": "Data Contamination Can Cross Language Barriers"
      }
    ]
  },
  {
    "index": 17,
    "papers": [
      {
        "key": "li2024task",
        "author": "Li, Changmao and Flanigan, Jeffrey",
        "title": "Task contamination: Language models may not be few-shot anymore"
      }
    ]
  },
  {
    "index": 18,
    "papers": [
      {
        "key": "ni2024mixeval",
        "author": "Ni, Jinjie and Xue, Fuzhao and Yue, Xiang and Deng, Yuntian and Shah, Mahir and Jain, Kabir and Neubig, Graham and You, Yang",
        "title": "MixEval: Deriving Wisdom of the Crowd from LLM Benchmark Mixtures"
      }
    ]
  },
  {
    "index": 19,
    "papers": [
      {
        "key": "white2024livebench",
        "author": "White, Colin and Dooley, Samuel and Roberts, Manley and Pal, Arka and Feuer, Ben and Jain, Siddhartha and Shwartz-Ziv, Ravid and Jain, Neel and Saifullah, Khalid and Naidu, Siddartha and others",
        "title": "Livebench: A challenging, contamination-free llm benchmark"
      }
    ]
  },
  {
    "index": 20,
    "papers": [
      {
        "key": "li2024open",
        "author": "Li, Yucheng and Guo, Yunhao and Guerin, Frank and Lin, Chenghua",
        "title": "An open-source data contamination report for large language models"
      }
    ]
  }
]