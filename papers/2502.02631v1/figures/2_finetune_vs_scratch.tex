\begin{figure*}[thb]
    \centering
    \includegraphics[width=0.8\linewidth]{figures/2_finetune_vs_scratch_fig.pdf}
    \caption{\small{Analysis of training token requirements for quantization-aware fine-tuning and training from scratch across 1-bit, 1.58-bit, 2-bit, 3-bit, and 4-bit settings. Fine-tuning typically saturates at 10B tokens for 3-bit and 4-bit, and at 30B tokens for 1-bit, 1.58-bit, and 2-bit. Fine-tuning consistently outperforms training from scratch in both accuracy and token efficiency across all bit configurations.}}
    \label{fig:finetune_vs_scratch}
\end{figure*}