\begin{figure}[t!]
    \centering
    \includegraphics[width=0.82\linewidth]{figures/1_QAT_portion_fig.pdf}
    \caption{\small{With a fixed total training budget of 100B tokens ($\mathcal{B}_\text{train}$), where $\mathcal{B}_\text{FP} + \mathcal{B}_\text{QAT} = \mathcal{B}_\text{train}$, we explore optimal allocation between full-precision pretraining ($\mathcal{B}_\text{FP}$) and QAT fine-tuning ($\mathcal{B}_\text{QAT}$). ``0.0'' represents QAT from scratch, while ``1.0'' indicates full-precision pretraining followed by PTQ. Results on MobileLLM-125M show peak accuracy with $\sim$90\% of the budget for full-precision pretraining and $\sim$10\% for QAT fine-tuning.}
}
    \label{fig:qat_proportion}
\end{figure}
