\begin{table}[t]
\renewcommand\arraystretch{0.8}
\centering
\caption{\small Comparison of 1-bit, 1.58-bit and 2-bit quantization methods on the LLaMA-3 8B model. Results for LLM-QAT, GPTQ, AWQ, SpinQuant, OmniQ were obtained using their publicly released codebase. Other results were sourced from respective papers. All methods employ integer quantization, except AQLM, which uses vector quantization with a vector dimension of 16.}
\vspace{-1em}
\Large  
\renewcommand{\arraystretch}{1.2} 
\setlength{\tabcolsep}{1.2pt} 
\label{tab:main_8B_012_bit}
\resizebox{1.02\linewidth}{!}{
\begin{tabular}{c:c:c:cccccc:c}
\hline\hline
 \vspace{-0.4em}
\multirow{2}{*}{Method}  & \multirow{2}{*}{\#Bits}& Group & ARC-e  & ARC-c & PIQA & HellaS & WinoG & Avg. & Wiki2 \\
 &  & Size  & ($\uparrow$) & ($\uparrow$) & ($\uparrow$) & ($\uparrow$) & ($\uparrow$) & ($\uparrow$) & ($\downarrow$) \\ \hline
FP & 16 & -- & 81.0 & 57.7 & 81.0 & 79.5 & 73.9 & 74.6 & 6.15 \\
\hline 
RTN & 2 & channel & 27.2 & 25.1 & 49.7 & 26.1 & 50.5 & 35.7 & 1.2e6 \\
GPTQ & 2 & channel & 27.4 & 24.6 & 51.0 & 25.9 & 50.6 & 35.9 & 1.6e2 \\
OmniQ & 2 & channel & 27.3 & 22.8 & 49.5 & 25.3 & 49.4 & 34.8 & -- \\
SpinQuant & 2 & channel & 32.4 & 21.8 & 53.4 & 31.9 & 50.9 & 38.1 & 31.2 \\
AWQ & 2 & channel & 26.0 & 27.1 & 51.4 & 26.1 & 49.8 & 36.1 & -- \\
QuIP & 2 & channel & 29.0 & 21.3 & 52.9 & 29.2 & 51.7 & 36.8 & 85.1 \\
AQLM & 2.02 & 1x16 & 74.2 & 41.2 & 77.8 & 55.4 & \textbf{71.8} & 64.1 & -- \\
DB-LLM & 2.12 & 128 & 59.1 & 28.2 & 68.9 & 42.1 & 60.4 & 51.7 & 13.6 \\
PB-LLM & 2.12 & 128 & 37.8 & 17.2 & 57.0 & 29.8 & 52.5 & 38.9 & 24.7 \\
LLM-QAT & 2 & channel & 54.8 & 35.9 & 68.0 & 58.0 & 54.7 & 54.3 & 29.5 \\
EfficientQAT & 2.12 & 128 & 69.3 & 46.8 & 76.4 & 69.0 & 66.3 & 65.5 & 9.6 \\
\cellcolor{lightgrey}$\ours{}$ & \cellcolor{lightgrey}2 & \cellcolor{lightgrey}channel & \cellcolor{lightgrey}\textbf{78.5} & \cellcolor{lightgrey}\textbf{54.5} & \cellcolor{lightgrey}\textbf{79.2} & \cellcolor{lightgrey}\textbf{73.8} & \cellcolor{lightgrey}70.0 & \cellcolor{lightgrey}\textbf{71.2} & \cellcolor{lightgrey}\textbf{8.0} \\
 \midrule
PB-LLM & 1.7 & 128 & 31.7 & 17.5 & 52.5 & 27.7 & 50.4 & 36.0 & 41.8 \\
TernaryLLM & 1.58 & channel & 61.2 & 36.4 & 73.7 & 63.9 & 65.0 & 60.0 & 11.2 \\
1-bit era & 1.58 & channel & 72.8 & 45.4 & \textbf{81} & 70.6 & 58 & 65.6 & 11.7 \\
\cellcolor{lightgrey}$\ours{}$ & \cellcolor{lightgrey}1.58 & \cellcolor{lightgrey}channel & \cellcolor{lightgrey}\textbf{76.3} & \cellcolor{lightgrey}\textbf{51.4} & \cellcolor{lightgrey}77.7 & \cellcolor{lightgrey}\textbf{71.9} & \cellcolor{lightgrey}\textbf{67.7} & \cellcolor{lightgrey}\textbf{69.0} & \cellcolor{lightgrey}\textbf{8.6} \\
 \midrule
BiLLM & 1.06 & 128 & 33.2 & 25.6 & \textbf{54.6} & 32.7 & 50.5 & 39.3 & 38.5 \\

ARB-LLM & 1.06 & channel & -- & -- & -- & -- & -- & -- & 27.4 \\
\cellcolor{lightgrey}$\ours{}$ & \cellcolor{lightgrey}1 & \cellcolor{lightgrey}channel & \cellcolor{lightgrey}\textbf{75.5} & \cellcolor{lightgrey}\textbf{51.9} & \cellcolor{lightgrey}47.1 & \cellcolor{lightgrey}\textbf{76.7} & \cellcolor{lightgrey}\textbf{69.4}& \cellcolor{lightgrey}\textbf{64.1} & \cellcolor{lightgrey}\textbf{9.5} \\
\hline\hline
\end{tabular}}
\vspace{-1em}
\end{table}