\section{Experiments}
\label{sec:experiments}
In this section, we compare each point on our Pareto chart with prior methods in the literature. As the first approach to unify training and quantization schemes in the sub-4-bit regime, we evaluate our method against specialized techniques for each bit setting. This includes binary quantization methods: BiLLM~\cite{huang2024billm}, ARB-LLM~\cite{li2024arb}, PB-LLM~\cite{shang2023pb}, and DB-LLM~\cite{chen2024db}; ternary quantization methods: TernaryLLM~\cite{chen2024ternaryllm}, 1-bit Era~\cite{ma2024era}, and Spectra~\cite{kaushal2024spectra}; and lower-bit QAT methods: LLM-QAT~\cite{liu2023llmqat} and EfficientQAT~\cite{chen2024efficientqat} as well as PTQ methods like GPTQ~\cite{frantar2022gptq}, OmniQ~\cite{shao2023omniquant}, SpinQuant~\cite{liu2024spinquant}, QuIP~\cite{chee2024quip} and AWQ~\cite{lin2023awq}. We also compare with a post-training vector quantization method AQLM~\cite{egiazarian2024aqlm}.

We demonstrate that $\ours$, with a unified scheme spanning five distinct bit settings (1, 1.58, 2, 3, and 4 bits), consistently outperforms previous methods specialized for each bit level, including both PTQ and QAT approaches. The performance gains are particularly pronounced in the 1, 1.58, and 2-bit settings, underscoring the robustness and reliability of our conclusions regarding scaling laws.

\input{tables/2_main_table_llama3_8B}


\subsection{Experimental Settings}
We conduct experiments on eight models including MobileLLM~\cite{liu2024mobilellm} 125M/350M/600M/1B/1.5B and LLaMA-3~\citep{llama3modelcard} 1B/3B/8B. Our evaluation was carried out on eight zero-shot commonsense reasoning tasks and Wiki2~\cite{merity2016wiki2} test set. 

During the quantized network training process, we initialized the models with pre-trained weights. Following the common practice~\cite{frantar2022gptq,liu2023llmqat}, all weights except for the embedding and output layers are quantized. We employed the AdamW~\cite{loshchilov2017decoupled} optimizer with zero weight decay for optimization. The training was distributed across 16 GPUs, with each GPU handling a batch size of 8. For binary, ternary, and 2-bit quantization settings, the optimization process spanned 120,000 iterations with initial learning rate of \(2 \times 10^{-5}\). For 3-bit and 4-bit settings, the process involved 40,000 iterations with initial learning rate of \(1 \times 10^{-5}\). The learning rate decayed to zero following cosine learning rate decay.

\input{figures/52_main_result_ternary} 
\input{figures/53_main_result_234bit}

\subsection{Main Results}
\subsubsection{1 / 1.58 / 2-bit Comparison on 8B Model}
\label{sec:exp_8b}
Let's first examine the comparison on 8B parameter models. As depicted in Table~\ref{tab:main_8B_012_bit}, in the 2-bit quantization setting, previous methods, including both PTQ and QAT, experience a significant drop in accuracy. Among PTQ methods, the vector quantization method AQLM~\cite{egiazarian2024aqlm} effectively mitigates some of the quantization loss, achieving 64.1 points, it falls 10.5 points short of full precision. The best quantization-aware training method, EfficientQAT~\cite{chen2024efficientqat}, still suffers a 9.1-point decline in average accuracy.  $\ours$ dramatically narrows the 2-bit quantization gap to full precision to just 3.4 points, outperforming the best QAT method by 5.7 points and the vector quantization method by 7.1 points.

In ternary cases, the accuracy drop is more pronounced, highlighting the effectiveness of different quantization methods. A follow-up work of the 1-bit Era~\cite{llama8B_1.58bit}, which trains 1-bit LLaMA-3 8B models using 100B tokens and complex techniques like binary relax with sigmoid schedulers, still experiences a 9.0-point accuracy drop. In contrast, $\ours$ requiring only 30B tokens and utilizing standard AdamW optimization with cosine learning rate decay, narrows the gap to just 5.6 points. This underscores the robustness of our quantization function design. 

Furthermore, $\ours$ significantly outperforms previous binary quantization techniques, such as BiLLM and ARB-LLM, reducing WikiText perplexity from 27.4 to 9.5.

\subsubsection{1.58-bit Comparison on Sub-8B Models}
\label{sec:exp_ternary}
Figure~\ref{fig:52_main_result_ternary} illustrates that $\ours{}$ also excels in sub-8B models, consistently outperforming previous methods targeting at ternary quantization aware training including Spectra~\cite{kaushal2024spectra} and 1-bit Era~\cite{ma2024era}. Given that a full-precision LLaMA-3 3B model achieves 69.9 accuracy, it's remarkable that $\ours{}$ ternary 3B-parameter model narrows the gap to just 4.1 points, while previous methods experience drops exceeding 11.7 points. Additionally, our 600M-parameter ternary model achieves 58.7 accuracy, even surpassing previous ternary 3B models with only one-fifth of the parameters.


\subsubsection{2-bit / 3-bit / 4-bit Comparisons}
\label{sec:exp_234_bit}
As evidenced by Figure~\ref{fig:53_main_result_234bit}, compared to previous state-of-the-art PTQ and QAT methods on 2, 3 or 4-bit quantization settings, our approach consistently resides on the Pareto front, with a particularly pronounced advantage in lower-bit quantization settings. These results confirm that our bit-accuracy trade-off conclusions are benchmarked against SoTA results across all bit settings, ensuring its reliability.