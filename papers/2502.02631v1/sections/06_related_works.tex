\section{Related Work}
\label{sec:related_work}
The quantization of Large Language Models (LLMs) has emerged as a pivotal research area, driven by the imperative to reduce computational and memory demands while preserving model performance \citep{liu2023llmqat,dettmers2022llmint8,xiao2022smoothquant}. A notable trend is the quantization of LLMs to lower bit-widths \cite{ma2024era,kaushal2024spectra}. 

Initial efforts, such as LLM.int8() \cite{dettmers2022llmint8} and SmoothQuant \cite{xiao2022smoothquant}, concentrated on quantizing LLMs to 8-bit weights and 8-bit activations. Subsequently, numerous studies have demonstrated the feasibility of quantizing LLMs to 4-bit with minimal accuracy degradation, employing both post-training quantization (PTQ) methods \cite{kim2023squeezellm,frantar2022gptq,liu2024spinquant,liu2023llm} and quantization-aware training (QAT) \cite{liu2023llmqat,chen2024efficientqat, bondarenko2021understanding}.

Recently, research has shifted towards sub-4-bit quantization. Some PTQ methods target 3-bit or 2-bit integer quantization \cite{shao2023omniquant, zhao2023atom, chee2024quip, ashkboos2024quarot,lin2023awq,frantar2022gptq}, or employ vector quantization \cite{egiazarian2024aqlm,tseng2024quip,baalen2023gptvq}. Other PTQ approaches even achieve binary weight quantization \cite{huang2024billm,shang2023pb,chen2024db,li2024arb}. Most recently, two QAT studies have claimed that ternary quantized models, trained from scratch, can match the accuracy of full-precision models with equivalent training \cite{ma2024era,kaushal2024spectra}. It generated heated debate within the field, with many practitioners expressing reservations about this conclusion.
To our knowledge, no existing work unifies sub-4-bit quantization schemes to derive a solid conclusion on which bit-width achieves the Pareto optimal in the efficiency-accuracy trade-off. This work presents \ours{} to fill that gap.
