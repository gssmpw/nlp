\section{A Better QAT Scheduling Strategy for Extreme Low-Bit LLMs}
\label{sec:scalinglaw}
\input{figures/1_QAT_portion}
In this work, we systematically investigate trade-offs involving bit precision ($\mathcal{P}$), quantization functions ($\mathcal{F}$), model size ($\mathcal{N}$), training strategies ($\mathcal{S}_{train}$) and training token ($\mathcal{D}$). 
\begin{equation}
\mathcal{L}(\mathcal{P}, \mathcal{F}, \mathcal{N}, \mathcal{S}_{train}, \mathcal{D}) 
\end{equation}
Given the vast search space defined by these variables, we first fix the quantization method ($\mathcal{F}$) and explore the dimensions of bit precision ($\mathcal{P}$), training strategies ($\mathcal{S}_{train}$) and training tokens ($\mathcal{D}$) in this section.


\input{figures/2_finetune_vs_scratch}


\subsection{Training Budget Allocation}

Post-Training Quantization (PTQ) and Quantization-Aware Training (QAT) are two primary quantization approaches. PTQ applies quantization after full-precision training, simplifying deployment but often leads to significant performance loss at bit widths below 4 bits. In contrast, QAT incorporates quantization during training to optimize model performance for low-bit-width representations.

Here we start by answering a key question:

\textbf{Given a fixed training budget (in \#tokens) $\mathcal{B}_{\text{train}} = \mathcal{B}_{\text{FPT}} + \mathcal{B}_{\text{QAT}}$, how should the budget be optimally allocated between full-precision training ($\mathcal{B}_{\text{FPT}}$) and quantization-aware training/fine-tuning ($\mathcal{B}_{\text{QAT}}$) to maximize the accuracy of the quantized model?}

This question is both technically intriguing and practically significant. Our approach begins with analyzing the pretraining phase to determine the optimal switching point from FPT to QAT, aiming to minimize the loss:
\begin{equation}
    \mathcal{B}_\text{FPT}^*, \mathcal{B}_\text{QAT}^* = \mathop{\arg \min}\limits_{\mathcal{B}_{\text{FPT}} + \mathcal{B}_{\text{QAT}}=\mathcal{B}_{\text{train}}} \mathcal{L}(\mathcal{B}_{\text{FPT}}, \mathcal{B}_{\text{QAT}} | \mathcal{N}, \mathcal{P})
\end{equation}
where $\mathcal{B}_\text{FPT}^*$ and $\mathcal{B}_\text{QAT}^*$ describe the optimal allocation of a computational budget $\mathcal{B}_{\text{train}}$. We utilize $\mathcal{B}_{\text{train}}$ to incorporate training tokens utilization ($\mathcal{D}$) into the training strategy ($\mathcal{S}$). 
Specifically, we evaluate various allocation ratios of \(\mathcal{B}_{\text{FPT}}\) and \(\mathcal{B}_{\text{QAT}}\) on MobileLLM-125M across four bit-widths ( 1.58-bit, 2-bit, 3-bit, and 4-bit). The FP models undergo a complete learning rate scheduling cycle for \(\mathcal{B}_{\text{FPT}}\) tokens, followed by another cycle for QAT for \(\mathcal{B}_{\text{QAT}}\) tokens. Detailed experimental settings are provided in the appendix.

Figure~\ref{fig:qat_proportion} reveals a distinct upward trend in the full-precision pre-training proportion versus accuracy curve. Notably, accuracy peaks at $\sim$ 90\% FPT allocation for almost every bit-width choice, then decline sharply when FPT exceeds 90\%, likely because this leaves insufficient tokens and training capacity for QAT. This leads to our first key finding:
\begin{tcolorbox}[
  enhanced,
  colback=blue!4!white,
  boxrule=0.8 pt, 
  boxsep=0pt, 
  left=2pt, 
right=2pt, 
  top=2pt,
  bottom=2pt, 
  drop fuzzy shadow=black!50
]
\textbf{Finding-1} QAT finetuning consistently surpasses both PTQ with $\mathcal{B}_\text{FPT} = \mathcal{B}_\text{train}$ and QAT from scratch with $\mathcal{B}_\text{QAT} = \mathcal{B}_\text{train}$. Optimal performance is nearly achieved by dedicating the majority of the training budget to full precision (FP) training and approximately 10\% to QAT.

\end{tcolorbox}


\subsection{Fine-tuning Characteristics}

Then we investigate the impact of finetuning tokens across various bit choices, spanning 7 architectures and 5 bit levels. Results in Figure~\ref{fig:finetune_vs_scratch} offer several key insights:

1. \textbf{Fine-tuning benefits across all bit-widths}: This observation challenges recent methodologies that trains ternary LLMs from scratch~\cite{kaushal2024spectra, ma2024era}. Instead, we suggest leveraging pre-trained full-precision models for initialization is a more effective approach for training quantized networks, including binary and ternary.

2. \textbf{Optimal fine-tuning budget and bit width}: Lower bit quantization (binary, ternary, 2-bit) requires more fine-tuning than higher bit quantization (3-bit, 4-bit). 3-bit and 4-bit reach near full precision accuracy after 10B tokens, while lower-bit quantization saturates around 30B tokens.

\input{figures/22_err_violin}
\input{figures/31_quantization_function_compare}
3. \textbf{QAT behavior transition between bit-widths}: Networks quantized to 3-bit/4-bit recover near full-precision accuracy after fine-tuning, while binary, ternary, and 2-bit saturate before achieving full accuracy. We hypothesize that QAT acts as ``\textit{compensation}" for bit-widths above 2-bit, adjusting weights within adjacent quantization levels, and as ``\textit{reconstruction}" below 2-bit, where weights adapt beyond nearby grids to form new representations. This is supported by weight change analysis in Figure~\ref{fig:22_err_violin}, showing smaller adjustments in 3-bit/4-bit (10-20\%) and larger shifts in lower-bit quantization ($\sim$40\%), indicating substantial value reconstruction.

\begin{tcolorbox}[
  enhanced,
  colback=blue!4!white,
  boxrule=0.8 pt, 
  boxsep=0pt, 
  left=2pt, 
right=2pt, 
  top=2pt, 
  bottom=2pt, 
  drop fuzzy shadow=black!50
]
\textbf{Finding-2} 
While fine-tuning enhances performance across all bit-widths, even binary and ternary, optimal fine-tuning effort inversely correlates with bit-width. For 3-bit and 4-bit weights, fine-tuning adjusts within a nearby grid to mitigate accuracy loss, and requires less finetuning tokens. In contrast, binary and ternary weights break the grid, creating new semantic representations to maintain performance, requiring longer finetuning.
\end{tcolorbox}
