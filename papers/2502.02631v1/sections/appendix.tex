\appendix
\onecolumn
\section{Appendix / supplemental material}

\subsection{Complete Results of Figure~\ref{fig:52_main_result_ternary}}
Table~\ref{tab:appendix_ternary_table} presents the numerical results of Figure~\ref{fig:52_main_result_ternary}. We evaluate accuracy across eight zero-shot commonsense reasoning tasks: ARC-easy, ARC-challenge~\citep{clark2018arc}, BoolQ~\citep{clark2019boolq}, PIQA~\citep{bisk2020piqa}, SIQA~\citep{sap2019siqa}, HellaSwag~\citep{zellers2019hellaswag}, OBQA~\citep{mihaylov2018obqa}, and WinoGrande~\citep{sakaguchi2021winogrande}, along with perplexity on the WikiText2 test set~\citep{merity2016wiki2}. Our results are compared against prior state-of-the-art ternary quantization-aware training works, including 1-bit era~\cite{ma2024era} and Spectra~\cite{spectra}. We also include the comparison to LLM-QAT~\citep{liu2023llmqat}. Consistent with previous methodologies~\cite{ma2024era,spectra}, we quantize all weights to low-bit, excluding the embedding and output layers. The $\ours{}$ 3B ternary model is quantized from LLaMA3~\cite{llama3modelcard} 3B model, while other models are quantized from MobileLLM~\cite{liu2024mobilellm}. 
As Spectra did not report results on the SIQA and OBQA datasets, the values in Figure~\ref{fig:52_main_result_ternary} represent the average accuracy across the remaining six tasks.

\input{appendix_tables/11_appendix_ternary}

\subsection{Complete Results of Figure~\ref{fig:53_main_result_234bit}}
In Tables~\ref{tab:appendix_w2}, ~\ref{tab:appendix_w3}, and ~\ref{tab:appendix_w4}, we provide detailed results corresponding to Figure~\ref{fig:53_main_result_234bit}. We compare $\ours{}$ against LLM-QAT~\citep{liu2023llmqat}, GPTQ~\citep{frantar2022gptq}, AWQ~\citep{lin2023awq}, OmniQuant~\citep{shao2023omniquant}, and SpinQuant~\citep{liu2024spinquant}. Following the common practice~\cite{frantar2022gptq,liu2023llmqat}, we apply low-bit quantization to all weights, except for the embedding and output layers.
\input{appendix_tables/32_appendix_w2}
\input{appendix_tables/33_appendix_w3}
\input{appendix_tables/34_appendix_w4}

\subsection{CPU Latency Experimental Setup}
We measure the CPU latency of five MobileLLM models on an Apple M1 MacBook Pro (32GB RAM) using 6 threads.  Each evaluation uses 5 prompt tokens and generates 122 tokens.  For the quantized models, embedding and output layers are quantized to 8-bit precision using channel-wise quantization, while weights in fully connected layers are quantized to 2-bit or 4-bit precision. Accuracy and decoding speed (in tokens/s) were measured under identical settings.

\subsection{GPU Latency Experimental Setup and Results}
\input{appendix_figures/gpu}
We measured the latency of LLaMA 3.2 models (1B, 3B, 8B) on an H100 NVL GPU (94GB memory). The W4A16 kernel used the Machete kernel from vLLM~\cite{kwon2023efficient, machete}, while the W2A16 kernel was implemented based on the CUTLASS mixed precision backbone kernel. All tests were performed on a single GPU with a context length of 2048 tokens. For kernel-level latency, we compared the 2-bit kernel to the 4-bit Machete kernel across three weight shapes: (4096 $\times$ 4096), (8192 $\times$ 8192), and (16384 $\times$ 16384).

For smaller models (1B, 3B, 8B), the performance speed-up from reducing weight precision from 4-bit to 2-bit is minimal. This is due to the impact of conversion overhead, which becomes more pronounced when the weight size is small. Since the in-kernel conversion latency ratio is higher for smaller models, the benefits of 2-bit quantization are outweighed by the overhead. Consequently, 4-bit quantization achieves a more favorable speed-accuracy trade-off in these settings, offering better overall performance.
In comparison, for larger weight shapes (16384 $\times$ 16384), the 2-bit kernel provides a substantial speedup, achieving 4.14$\times$ faster performance than FP16 and 1.24$\times$ faster than the Machete 4-bit kernel.


\subsection{QAT Scheduling Experimental Setup}
The total training budget (\(\mathcal{B}_{\text{train}}\)) is set to 100B tokens. We vary the proportion of tokens allocated for full-precision training versus quantization-aware training (QAT) finetuning, sweeping the ratio across \([0, 0.01, 0.05, 0.1, 0.2, 0.4, 0.6, 0.8, 0.9, 0.95, 0.99, 1]\). Here, a ratio of 0 corresponds to QAT from scratch, while a ratio of 1 represents full-precision training followed by post-training quantization (PTQ). 

For full-precision training, we use 8×8 GPUs, a batch size of 16, a weight decay of 0.1, an initial learning rate of \(2.5 \times 10^{-3}\), and a linear learning rate decay to zero. For quantized network training, we also use 8×8 GPUs but with a batch size of 8, no weight decay, an initial learning rate of \(1 \times 10^{-4}\), and a linear learning rate decay to zero.

\subsection{Embedding Bit Precision vs. Accuracy Trade-off}
\input{appendix_figures/pareto_optimal_embedding}
Despite the prevalent practice of not quantizing embedding and output layers, as noted in prior works such as Frantar et al.~\cite{frantar2022gptq} and Ma et al.~\cite{ma2024era}, our study extends the scaling law analysis by examining the impact of quantizing these layers. As illustrated in Figure~\ref{fig:pareto_optimal_embedding}, utilizing 4-bit embeddings or matching the bit precision of embeddings to that of weights positions these configurations on the Pareto front, in contrast to employing 8-bit or 16-bit embeddings.

\subsection{Weight Bit Precision vs. Accuracy Trade-off}
\input{appendix_figures/pareto_optimal_weight}
For the trade-off between weight-bit precision and model accuracy, we consider two configurations: 4-bit embeddings and embeddings with the same bit precision as weights. In both scenarios, lower-bit quantization, such as 1.58-bit, 2-bit, and 3-bit, consistently outperforms 4-bit quantization, as depicted in Figure~\ref{fig:pareto_optimal_weight}.

\subsection{Pareto Curve in More Tasks}
\input{appendix_figures/pareto_curve_othertasks}
Furthermore, we present results from a question-answering task, TriviaQA (TQA)~\cite{joshi2017triviaqa}, and a reading comprehension benchmark, RACE~\cite{lai2017race}, in Figures~\ref{fig:pareto_curve_othertasks} The findings are consistent across these tasks: 1-bit quantization yields the lowest performance, whereas 1.58-bit, 2-bit, and 3-bit quantization are comparable and generally surpass the performance of 4-bit quantization.

Additionally, for context-based word prediction (LAMBADA~\cite{paperno2016lambada}) and multiple-choice science questions (SciQ~\cite{welbl2017crowdsourcing}) in Figrue~\ref{fig:pareto_curve_othertasks2}, the results also shows a clear trend of 2-bit residing on the Pareto optimal frontier, outperforming 4-bit.

