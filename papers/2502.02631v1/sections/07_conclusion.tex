\section{Conclusions}
\label{sec:conclusions}

In this study, we have performed an in-depth analysis of the intricate relationships among model parameters ($N$), training data volume ($D$), quantization training schemes ($\mathcal{B}_\text{train}$), quantization precision ($P$), and the selection of quantization functions ($\mathcal{F}$) in relation to the model's final loss, expressed as $\mathcal{L} = f(N, D, P, \mathcal{B}_\text{train}, \mathcal{F})$. To address these multifaceted challenges, we propose \ours{}, an advanced quantization framework that achieves state-of-the-art performance across all bit-width levels. This framework uniquely enables a direct, consistent comparison across different bit-widths, ensuring an equitable evaluation of performance metrics. Our empirical analysis indicates that quantization at 1.58-bit, 2-bit, and 3-bit offers a superior trade-off between accuracy and effective quantized model size compared to 4-bit, highlighting their potential for optimized model deployment.