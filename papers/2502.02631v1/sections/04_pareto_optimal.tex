
\section{Pareto-Optimality of Extremely Low-Bit LLM}\label{sec:pareto_frontier}
To ensure a consistent apples-to-apples performance comparison across different bit-width configurations, we first determined the optimal training setup ($\mathcal{B}_{train}^*$) in Section~\ref{sec:scalinglaw} and the quantization function ($\mathcal{F}^*$) for each bit in Section~\ref{sec:quantization_choice}. Using this unified framework for all bit widths, we examine the trade-off between model size and quantization bit: $\mathcal{L}(\mathcal{P}, \mathcal{N} | \mathcal{F}^*, \mathcal{B}_{train}^*)$.

\subsection{Accuracy-compression Trade-off}
In on-device deployment scenarios, such as wearables and portables, storage constraints often limit the capacity of large language models (LLMs). To optimize performance within these constraints, quantization is essential. A common dilemma is whether to train a larger model and quantize it to a lower bit-width or to train a smaller model and quantize it to a higher bit-width.

4-bit quantization-aware training (QAT) achieves near-lossless compression in many scenarios, making it widely adopted. However, the landscape below 4-bit remains unclear, with limited comparative analysis. Previous claims about ternary models matching 16-bit performance~\cite{ma2024era} were based on lower FP16 baselines than current standards. Spectra's comparisons between ternary QAT and 4-bit PTQ fall short of a fair evaluation due to inconsistencies in the training schemes used~\cite{spectra}.

With $\ours{}$, we are able to improve the analysis. Figure~\ref{fig:pareto_curve} (a) demonstrates that sub-4-bit quantization, including binary, ternary, 2-bit, and 3-bit, often surpasses 4-bit. Notably, 2-bit and ternary models reside on the Pareto frontier. For instance, a 2-bit MobileLLM-1B model achieves 1.8 points higher accuracy than a 4-bit MobileLLM-600M model, with even smaller model sizes. This trend persists across larger LLaMA models, as shown in Figure~\ref{fig:pareto_curve} (b), demonstrating the potential of lower-bit quantization for achieving both higher accuracy and compression. We calculate the effective quantized model size as $(\#\text{weights} \times \text{weight-bits}+ \#\text{embedding-weights} \times \text{embedding-bits})/8$. More comprehensive analysis is provided in the Appendix.

\subsection{Hardware Implementation Constraints}
\label{sec:on_device}

In practical deployment, both memory limitations and hardware constraints must be considered. While 2-bit and ternary quantization sit on the accuracy-size Pareto frontier, 2-bit quantization is generally more feasible due to practical challenges. 
Ternary quantization, using a 1.58-bit format with values  \(\{-1, 0, 1\}\), appears more storage-efficient but is inefficient in implementation.
Storing ternary values with sparsity exploitation is effective only when sparsity exceeds 90\%, due to high indexing costs. Packing ternary values into an Int32 offers limited compression but complicates GEMM. Some approaches~\cite{yang20241} even store ternary values as 2-bit signed integers, negating the expected storage benefits. In contrast, 2-bit quantization directly maps bit pairs to values, reducing unpacking and conversion overhead, which can be more efficient for custom GEMM kernels. As a result, 2-bit quantization is often a more practical choice for deployment.

\subsection{Accuracy-speed Trade-off}
To evaluate potential speedup benefits beyond memory reduction, we implemented 2-bit quantization kernels on the CPU and compared them with 4-bit quantization. The curves in Figure~\ref{fig:pareto_curve} (c) demonstrate that, within our experimental range, 2-bit quantized models consistently outperform 4-bit models in terms of accuracy-speed performance, positioning 2-bit quantization as a superior choice for on-device applications where both latency and storage are critical. Detailed settings are provided in the appendix.