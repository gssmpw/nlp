\section{A Hitchhikerâ€™s Guide to Quantization Method Choices}\label{sec:quantization_choice}

We have examined the impact of training strategy and budget allocations ($\mathcal{B}_\text{train}$, $\mathcal{B}_\text{QAT}$) on scaling laws. Building on the optimal training practices outlined in Section~\ref{sec:scalinglaw}, we focus on a critical yet often overlooked factor: the choice of quantization functions ($\mathcal{F}$).
\begin{equation}
   \mathcal{F}^* = \mathop{\arg \min}\limits_{\mathcal{F}} \mathcal{L}(\mathcal{F} | \mathcal{P}, \mathcal{B}_\text{QAT}^*)
\end{equation}
The significance of this choice has been largely underestimated in prior scaling law studies~\cite{kumar2024scaling}. Our results show that, especially at sub-4-bit quantization, the choice of function is highly sensitive and can drastically alter scaling law outcomes. An improper selection can distort performance and lead to entirely different conclusions, underscoring the need for a careful design of $\mathcal{F}$.

\subsection{Preliminary}
In general, a uniform quantization function is expressed as
\begin{equation}
\label{eq:minmax}
    \mathbf{W}_\mathbf{Q}^i = \alpha \nint{\frac{\mathbf{W}_\mathbf{R}^i - \beta}{\alpha}} + \beta \ 
\end{equation}
Here $\mathbf{W_Q}$ represents quantized weights, $\mathbf{W_R}$ denotes their real-valued counterparts~\citep{quantization_whitepaper, krishnamoorthi2018quantizing}. Key design choices focus on scale $\alpha$ and bias $\beta$. For symmetric min-max quantization, $\alpha = \frac{\max(|\mathbf{W_R}|)}{2^{N-1} - 1}$ and $\beta = 0$. In asymmetric min-max quantization, $\alpha = \frac{\max(\mathbf{W_R}) - \min(\mathbf{W_R})}{2^N - 1}$ and $\beta = \min(\mathbf{W_R})$. Symmetric min-max quantization is prevalent for weights $\geqslant$ 4 bits, while sub-4-bit quantization requires distinct functions.

For binary quantization, assigning the sign of full-precision weights ($\mathbf{W}_\mathbf{R}$) to binary weights ($\mathbf{W}_\mathbf{B}$) is a commonly used approach~\cite{rastegari2016xnor,liu2018bi}: $\mathbf{W}_\mathbf{B}^i = \alpha \cdot \text{Sign}(\mathbf{W}_\mathbf{R}^i)$, where $\alpha = \frac{||\mathbf{W_R}||_{l1}}{n_{\mathbf{W_R}}}$. 

In ternary quantization, ternary weights are often given by $\mathbf{W}_\mathbf{T}^i = \alpha \cdot \text{Sign}(\mathbf{W}_\mathbf{R}^i) \cdot \mathbf{1}_{|\mathbf{W}_\mathbf{R}^i| > \Delta}$, with $\Delta = \frac{0.7 \cdot ||\mathbf{W_R}||_{l1}}{n_{\mathbf{W_R}}}$ and $\alpha_{_\mathbf{T}} = \frac{\sum_i \mathbf{W}_\mathbf{R}^i \cdot \mathbf{1}_{| \mathbf{W}_\mathbf{R}^i| > \Delta}}{\sum_i \mathbf{1}_{| \mathbf{W}_\mathbf{R}^i| > \Delta}}$ ~\cite{TernaryBERT,liu2023binary}.
Besides binary and ternary quantization, there is less work targeting 2-bit or 3-bit integer quantization function design. Directly using min-max quantization for them will lead to performance collapse.

\subsection{Introducing \ours{}}
In sub-4-bit quantization, design requirements vary significantly across bit levels. Equal attention to each bit choice is crucial for accurate, reliable comparisons.

\subsubsection{Trade-offs}
We identify two key trade-offs in low-bit quantization for LLMs: (1) Outlier precision vs. intermediate value precision and (2) Symmetry vs. inclusion of ``0" at the output level.

\input{figures/32_quantization_method_results}

\textbf{(1) Range clipping}
Outliers challenge LLM quantization~\cite{lin2023awq,liu2024spinquant}, especially when using \textit{min-max} ranges for weight quantization for extremely low-bit quantization. As seen in Figure~\ref{fig:quantization_method} (b)-(e), \textit{min-max} quantization works at 4 bits but loses accuracy at lower bit-widths. On the other hand, range clipping improves lower-bit quantization but harms 4-bit accuracy. We refer to range-setting methods based on weight statistics as ``stats-based" approaches. The effectiveness of these quantization functions varies with different bit choices.

Learnable scales, however, optimize quantization ranges as network parameters, balancing outlier suppression and precision. Solutions like LSQ~\cite{esser2019learned} and its binary~\cite{liu2022bit} and ternary~\cite{liu2023binary} extensions exist. While prior work favored learnable policies for activations but used statistics-based quantization for weights~\cite{liu2023llm}, we find that, with appropriate gradient scaling, learnable scales yield stable, superior performance for weights. As shown in Figure~\ref{fig:quantization_method} (b)-(e), learnable policies consistently outperform stats-based methods across all bit widths.

\textbf{(2) Quantization grids}
Level symmetry in quantization grids is crucial for lower-bit quantization, yet it is rarely discussed. The ``0" in quantization output levels is essential for nullifying irrelevant information, but in even-level quantization (e.g., 2-bit, 3-bit, 4-bit), including ``0" results in imbalanced levels. For example, in 2-bit quantization, options like \((-2, -1, 0, 1)\) and \((-1.5, -0.5, 0.5, 1.5)\) exist. The former limits representation with only one positive level, while the latter offers a balanced distribution. Inspired by this, we propose Stretched Elastic Quant (SEQ), an amendment to LSQ for lower-bit scenarios:
\begin{equation}
   \!\!\!\mathbf{W}_\mathbf{Q}^i \!=\!\alpha\!\left(\!\nint{{\rm Clip}\!\left(\!\frac{\mathbf{W}_\mathbf{R}^i}{\alpha}, -1, 1\!\right)\!\!\times\!\! \frac{k}{2} -\! 0.5}\! +\! 0.5\right)\!\! / k \!\times\! 2 
\end{equation}
Here, \(k\) denotes the number of quantization levels. Figure~\ref{fig:31_quantization_function_compare} visualizes quantization grids, showing that SEQ not only balances output quantized levels but also evenly divides the full-precision weight span to quantization levels, which turns out to be crucial for extremely low-bit quantization. Figure~\ref{fig:quantization_method} (f)-(i) demonstrate SEQ's superiority in ternary and 2-bit quantization, while LSQ with ``0'' in output level slightly outperforms in 3 and 4-bit cases.

\begin{tcolorbox}[
  enhanced,
  colback=blue!4!white,
  boxrule=0.8 pt, 
  boxsep=0pt, 
  left=2pt, 
right=2pt, 
  top=2pt, 
  bottom=2pt, 
  drop fuzzy shadow=black!50
]
\textbf{Finding-3} Extreme low-bit quantization is highly sensitive to quantization function selection, with no single optimal function for all bit widths. Learnable range settings outperform statistics-based methods due to their flexibility in optimizing range parameters with respect to the final loss. Ternary and 2-bit quantization favor symmetric levels and balanced range coverage in quantization grid configuration, while imbalance levels with ``0" in output levels are more effective for 3 and 4-bit quantization.
\end{tcolorbox}

\input{figures/pareto_curve}
\subsubsection{Quantization Function}
Based on our analysis, we integrate the optimal quantization functions identified for each bit-width into one formula, denoted as \ours. This includes Elastic Binarization~\cite{liu2022bit} for 1-bit quantization, LSQ~\cite{esser2019learned} for 3 and 4-bit quantization, and the proposed SEQ for 1.58 and 2-bit quantization: 

\begin{equation}
\begin{split}
\label{eq:custom_quant_forward}
&\mathbf{W}_\mathbf{Q}^i = \alpha \mathbf{\hat{W}_Q}^i  \\
   & = \left\{  
         \begin{array}{lr}
         \!\!\!\alpha \! \cdot \! {\rm Sign}(\mathbf{W}_\mathbf{R}^i),  \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ {\rm if} \ N_{bit}=1 \\ 
         \vspace{0.3em}
         \!\!\!\alpha(\nint{{\rm Clip}(\frac{\mathbf{W}_\mathbf{R}^i}{\alpha}, -1, 1) \times k/2 - 0.5} + 0.5) / k\times2,  \\ 
         \vspace{0.3em}
         \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ {\rm if} \ N_{bit}=1.58, 2 \\ 
         \!\!\!\alpha \nint{{\rm Clip}(\frac{\mathbf{W}_\mathbf{R}^i}{\alpha}, n, p)},  \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ {\rm if} \ N_{bit}=3, 4 \\ 
         \end{array} 
         \right.     
\end{split}
\end{equation}
Here $k$ equals $3$ in the ternary case and $2^{N_{bit}}$ otherwise; $n = -2^{N_{bit} -1}$ and $p =2^{N_{bit} -1} - 1$. In the backward pass, the gradients to the weights and scaling factor can be easily calculated using straight-through estimator:
\begin{equation}
\begin{split}
\label{eq:custom_quant_backward_w}
\frac{\partial\mathbf{W}_\mathbf{Q}^i}{\partial\mathbf{W}_\mathbf{R}^i} \overset{STE}{\approx}
& \left\{ 
        \begin{array}{lr} 
        \vspace{0.3em}
        \mathbf{\large{1}}_{|\frac{\mathbf{W}_\mathbf{R}^i}{\alpha}|< 1}, \ \ \ \ \ \ {\rm if} \ N_{bit}=1, 1.58, 2 \\ 
        \mathbf{\large{1}}_{n < \frac{\mathbf{W}_\mathbf{R}^i}{\alpha} < p}, \ \ \ \ {\rm if} \ N_{bit}=3,4 
        \end{array} 
        \right.     
\end{split}
\end{equation}
\begin{equation}
\begin{split}
\label{eq:custom_quant_backward_w}
\frac{\partial\mathbf{W}_\mathbf{Q}^i}{\alpha} \overset{STE}{\approx}
& \left\{ 
        \begin{array}{lr} 
        \vspace{0.6em}
        \!\!{\rm Sign}(\mathbf{W}_\mathbf{R}^i), \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ {\rm if} \ N_{bit}=1 \\ 
        \vspace{0.5em}
        \!\!\mathbf{\hat{W}_R}^i -\! \frac{\mathbf{W}_\mathbf{R}^i}{\alpha} \cdot \mathbf{\large{1}}_{|\frac{\mathbf{W}_\mathbf{R}^i}{\alpha}|< 1}, \ \ \ \ {\rm if} \ N_{bit}=1.58, 2 \\
        \!\!\mathbf{\hat{W}_R}^i -\! \frac{\mathbf{W}_\mathbf{R}^i}{\alpha} \cdot \mathbf{\large{1}}_{n < \frac{\mathbf{W}_\mathbf{R}^i}{\alpha} < p}, \ \ {\rm if} \ N_{bit}=3,4 
        \end{array} 
        \right.     
\end{split}
\end{equation}

For the initialization of $\alpha$, we use $\alpha = \frac{||\mathbf{W}_\mathbf{R}||_{l1}}{n_{_{\mathbf{W}_\mathbf{R}}}}$ for the binary case, since the scaling factor has the closed-form solution to minimizing quantization error: $\mathcal{E} = || \alpha \hat{\mathbf{W}}_\mathbf{Q} - \mathbf{W}_\mathbf{R} ||_{l2}$. 
For the other cases, we simply initialize $\alpha$ as the maximum absolute value of the weights. For ternary and 2-bit quantization, $\alpha = \max(|\mathbf{W}_\mathbf{R}|)$, associated with SEQ quantizer, and for 3-bit and 4-bit cases, $\alpha =\frac{\max(|\mathbf{W}_\mathbf{R}|)}{p}$,  associated with LSQ quantizer. 

With \ours{}, we present a robust comparison framework across five bit-widths (1-bit, 1.58-bit, 2-bit, 3-bit, 4-bit), each achieving state-of-the-art accuracy. This facilitates direct, apple-to-apple comparisons to identify the most effective bit-width selection.





