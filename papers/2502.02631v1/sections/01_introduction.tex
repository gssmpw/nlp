\section{Introduction}
As deep learning continues to scale toward larger models and datasets, significant attention has been devoted to studying the scaling laws that trade-off between model and dataset size to optimize performance and computational efficiency~\cite{hoffmann2022training,kumar2024scaling,dettmers2023case}. 
In the meantime, the field is shifting toward lower-precision computation, particularly in large language models, driven by the substantial benefits of memory savings and computational efficiency~\cite{liu2023binary,ma2024era}.
This shift necessitates a rethinking of scaling laws to account for the effects of quantization on resulting quantized model performance.

\input{figures/0_pareto_curve_2_4_8_16}

When allowing for lower-bit quantization, we can freely trade off the bit-width and the number of parameters. Keeping the amount of memory used the same, we could have an 8-bit model, or a 4-bit model twice the size. This begs the question: \textit{What is the optimal trade-off between bit-width and model size?}
Recent papers ~\cite{dettmers2023case, kumar2024scaling} on scaling laws for low-precision conclude that 4 or 6-bit quantization often resides on the Pareto frontier to balance accuracy and efficiency. 
Other studies~\cite{ma2024era, spectra} suggest that bit-widths as low as 1.58-bit per parameter hold significant promise for the optimal scaling law trade-off. 
These opposing conclusions highlight the challenges of studying scaling laws in the low-precision domain. 


In this paper, we demonstrate that previous conclusions on the low-bit scaling laws can be significantly sharpened by better quantization scheme design and training improvements.
While previous works define the search space of the QAT scaling laws solely as a function of model parameters ($\mathcal{N}$), token count ($\mathcal{D}$), and quantization precision ($\mathcal{P}$), \textbf{we emphasize the critical role that the training scheme ($\mathcal{S}_{\text{train}}$) and the bit-specific quantization function ($\mathcal{F}$) play in the equation}. We formalize the search space as $\mathcal{L}(\mathcal{N}, \mathcal{D}, \mathcal{P}, \mathcal{S}_{\text{train}}, \mathcal{F})$, comprising five dimensions. 

To disentangle these complexities, we first identify the optimal training strategy for plausible quantization functions in each bit width, $\mathcal{L}(N, \mathcal{D}, \mathcal{S}_{\text{train}} \mid \mathcal{P}, \mathcal{F})$. Subsequently, with the optimal training strategy ($\mathcal{S}_{\text{train}}^*$) and the token count ($\mathcal{D}^*$) required for saturation, we determine the best quantization function for each bit, $\mathcal{L}(N, \mathcal{F} \mid \mathcal{P}, \mathcal{D}^*, \mathcal{S}_{\text{train}}^*)$. Results highlight that \textbf{quantization grids and ranges are pivotal in the sub-4-bit regime, with a sharp learning behavior transition between 1-bit/1.58-bit/2-bit and 3-bit/4-bit.} 

Based on the findings, we derive $\ours{}$, the first framework that unifies the training and quantization scheme in sub 4-bit regime. Rather than fitting hypothetical scaling laws for quantization,\textbf{ $\ours{}$ demonstrate its robustness by yielding state-of-the-art (SOTA) models at all bit widths}, surpassing prior works tailored for individual bit levels. 

These SOTA points in the Pareto chart ensure that our scaling law comparisons are both reliable and consistent, as they derive from homogeneous settings. Leveraging $\ours{}$, we identify the optimal bit-width for minimizing loss within the effective quantized model size, $\mathcal{L}(\mathcal{N}, \mathcal{P} | \mathcal{F}^*, \mathcal{D}^*, \mathcal{S}_{\text{train}}^*)$. Our scaling laws reveal that binary quantization significantly compromises accuracy, while ternary, 2-bit and 3-bit quantization are tied in performance, often surpassing 4-bit. 
The tiebreaker lies in the kernel implementation, which drives real memory savings and speedups. 1.58-bit and 3-bit quantization are in general less hardware-friendly than 2-bit. We implemented an optimized 2-bit CPU kernel and our results indicate that 2-bit quantization achieves higher speed at the same accuracy compared to 4-bit.

The key contributions of this study are as follows: 

$\bullet$ We present a comprehensive study on the intertwined effects of QAT budget allocation and the specific choices of quantization functions across 8 models (125M to 3B) and 5 quantization strategies. Our study highlights the unique characteristics and challenges of binary, ternary, and 2/3/4-bit quantization, offering actionable insights and best practices for achieving optimal accuracy-efficiency trade-offs. 
    
$\bullet$  We introduce \ours{}, the first systematic, apples-to-apples comparison of quantization functions at extreme low-bit settings. Each point in the Pareto chart outperforms prior methods optimized for specific bit widths. Specifically, the 1.58-bit \ours{} LLaMA-3 8B model reduces the performance gap to full precision by relatively 37.8\% compared to the 1-bit Era's LLaMA-3 8B model~\cite{ma2024era}, while using only 30\% of the training tokens. 

$\bullet$ Our research highlights the potential of 2-bit quantization as a prospective alternative to the traditional 4-bit approach, offering improved accuracy-size trade-off, as underlined in Figure~\ref{fig:0_pareto_curve_2_4_8_16}. Preliminary speed benchmarks also demonstrate promising efficiency gains with 2-bit quantization. Nevertheless, widespread adoption will require community-wide efforts, such as INT2 support in NVIDIA tensor cores, to unlock the full benefits of 2-bit quantization. 