@inproceedings{azuma2017making,
author = {Ronald T. Azuma},
booktitle = {Imaging and Applied Optics 2017 (3D, AIO, COSI, IS, MATH, pcAOP)},
pages = {JTu1F.1},
publisher = {Optica Publishing Group},
address = {San Francisco, California, United States},
title = {Making Augmented Reality a Reality},
year = {2017},
url = {https://opg.optica.org/abstract.cfm?URI=AIO-2017-JTu1F.1},
abstract = {Augmented Reality has attracted interest for its potential as a platform for new compelling usages. This paper provides an overview of technical challenges in imaging and optics encountered in near-eye optical see-through AR display systems.},
}

@misc{matsuda2016hyperreality,
  author = {Keiichi Matsuda},
  title = {Hyper-Reality},
  howpublished = {\url{https://www.youtube.com/watch?v=YJg02ivYzSs}},
  year = {2016},
  note = {Accessed: 2024-12-09},
}

@misc{nielsen10usability,
  title={Usability heuristics for user interface design},
  author={Nielsen, Jakob},
  year={10}
}

@inproceedings{dunser2007applying,
  author = {D{\"u}nser, Andreas and Grasset, Rapha{\"e}l and Seichter, Hartmut and Billinghurst, Mark},
  title = {Applying HCI Principles to AR Systems Design},
  booktitle = {Proceedings of the 2nd International Workshop on Mixed Reality User Interfaces: Specification, Authoring, Adaptation (MRUI'07) at IEEE Virtual Reality 2007 Conference},
  year = {2007},
  address = {Charlotte, NC, USA},
  month = {March 11},
  pages = {37--42},
  publisher = {University of Canterbury, Human Interface Technology Laboratory}
}


@misc{uspto20240090818,
  author       = {Apple Inc.},
  title        = {Health Sensing Retention Band},
  number       = {US20240090818},
  year         = {2024},
  month        = {March},
  url          = {https://image-ppubs.uspto.gov/dirsearch-public/print/downloadPdf/20240090818},
  note         = {Accessed: 2024-12-05}
}


@inproceedings{jo2023flowar,
author = {Jo, Hye-Young and Seidel, Laurenz and Pahud, Michel and Sinclair, Mike and Bianchi, Andrea},
title = {FlowAR: How Different Augmented Reality Visualizations of Online Fitness Videos Support Flow for At-Home Yoga Exercises},
year = {2023},
isbn = {9781450394215},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3544548.3580897},
doi = {10.1145/3544548.3580897},
abstract = {Online fitness video tutorials are an increasingly popular way to stay fit at home without a personal trainer. However, to keep the screen playing the video in view, users typically disrupt their balance and break the motion flow — two main pillars for the correct execution of yoga poses. While past research partially addressed this problem, these approaches supported only a limited view of the instructor and simple movements. To enable the fluid execution of complex full-body yoga exercises, we propose FlowAR, an augmented reality system for home workouts that shows training video tutorials as always-present virtual static and dynamic overlays around the user. We tested different overlay layouts in a study with 16 participants, using motion capture equipment for baseline performance. Then, we iterated the prototype and tested it in a furnished lab simulating home settings with 12 users. Our results highlight the advantages of different visualizations and the system’s general applicability.},
booktitle = {Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems},
articleno = {469},
numpages = {17},
keywords = {augmented reality, fitness video, home workouts, yoga},
location = {Hamburg, Germany},
series = {CHI '23}
}
@misc{apple2024visionpro,
  author       = {Apple},
  title        = {Apple Vision Pro arrives in new countries and regions beginning June 28},
  year         = {2024},
  month        = {June},
  url          = {https://www.apple.com/au/newsroom/2024/06/apple-vision-pro-arrives-in-new-countries-and-regions-beginning-june-28/},
  urldate      = {2024-12-05}
}


@misc{shepherd2024youtube,
  title={23 Essential YouTube Statistics You Need to Know in 2024},
  author={Shepherd, Jack},
  year={2024},
  month={November},
  url={https://thesocialshepherd.com/blog/youtube-statistics}
}

@inproceedings{rasheed2024critical,
  author = {Rasheed, Zainab and Ghwanmeh, Sameh and Almahadin, Aziz},
  title = {A Critical Review of AI-Driven AR Glasses for Realtime Multilingual Communication},
  booktitle = {Proceedings of the 2024 Advances in Science and Engineering Technology International Conferences (ASET)},
  year = {2024},
  pages = {1--6},
  publisher = {IEEE},
  address = {Piscataway, NJ, USA},
  doi = {10.1109/ASET60340.2024.10708749},
  keywords = {Accuracy, Navigation, Wireless networks, Instruments, Ecosystems, Glass, Linguistics, Real-time systems, Iterative algorithms, Artificial intelligence, AI-AR glasses, multilingual communication, AI applications, multicultural connectivity, interconnectivity, cross language communication, translation algorithms, communication networks, inclusive communication environments}
}


@article{ruminski2015experimental,
  title={An experimental study of spatial sound usefulness in searching and navigating through AR environments},
  author={Rumi{\'n}ski, Dariusz},
  journal={Virtual Reality},
  volume={19},
  number={3},
  pages={223--233},
  year={2015},
  publisher={Springer}
}
@article{chen2022lisee,
  title={Lisee: A headphone that provides all-day assistance for blind and low-vision users to reach surrounding objects},
  author={Chen, Kaixin and Huang, Yongzhi and Chen, Yicong and Zhong, Haobin and Lin, Lihua and Wang, Lu and Wu, Kaishun},
  journal={Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies},
  volume={6},
  number={3},
  pages={1--30},
  year={2022},
  publisher={ACM New York, NY, USA}
}
@article{gupta2024sonicvista,
  title={SonicVista: Towards Creating Awareness of Distant Scenes through Sonification},
  author={Gupta, Chitralekha and Sridhar, Shreyas and Matthies, Denys JC and Jouffrais, Christophe and Nanayakkara, Suranga},
  journal={Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies},
  volume={8},
  number={2},
  pages={1--32},
  year={2024},
  publisher={ACM New York, NY, USA}
}
@article{li2024beyond,
  title={Beyond Sight: Enhancing Augmented Reality Interactivity with Audio-Based and Non-Visual Interfaces},
  author={Li, Jingya},
  journal={Applied Sciences},
  volume={14},
  number={11},
  pages={4881},
  year={2024},
  publisher={MDPI}
}


@book{bowman20043d,
author = {Bowman, Doug A. and Kruijff, Ernst and LaViola, Joseph J. and Poupyrev, Ivan},
title = {3D User Interfaces: Theory and Practice},
year = {2004},
isbn = {0201758679},
publisher = {Addison Wesley Longman Publishing Co., Inc.},
address = {USA}
}

@book{card2018psychology,
  author = {Card, Stuart K.},
  title = {The Psychology of Human-Computer Interaction},
  publisher = {CRC Press},
  address = {Boca Raton, FL, USA},
  year = {2018},
  edition = {1},
  doi = {10.1201/9780203736166},
  pages = {488},
  isbn = {9780203736166}
}


@article{milgram1994taxonomy,
  title={A taxonomy of mixed reality visual displays},
  author={Milgram, Paul and Kishino, Fumio},
  journal={IEICE TRANSACTIONS on Information and Systems},
  volume={77},
  number={12},
  pages={1321--1329},
  year={1994},
  publisher={The Institute of Electronics, Information and Communication Engineers}
}

@inproceedings{plabst2022push,
author = {Plabst, Lucas and Oberd\"{o}rfer, Sebastian and Ortega, Francisco Raul and Niebling, Florian},
title = {Push the Red Button: Comparing Notification Placement with Augmented and Non-Augmented Tasks in AR},
year = {2022},
isbn = {9781450399487},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3565970.3567701},
doi = {10.1145/3565970.3567701},
abstract = {Visual notifications are omnipresent in applications ranging from smart phones to Virtual Reality (VR) and Augmented Reality (AR) systems. They are especially useful in applications where users performing a primary task have to be interrupted to react to external events. However, these notifications can cause disruptive effects on the performance of users concerning their currently executed primary task. Also, different notification placements have been shown to have an influence on response times, as well as e.g.&nbsp;on user perceived intrusiveness and disruptiveness. We investigated the effects and impacts of four visual notification types in AR environments when the main task was performed (1) in AR and (2) the real world. We used subtitle, heads-up, world space, and user wrist as notification types. In a user study, we interrupted the execution of the main task with one of the AR notification types. When noticing a notification, users responded to it by completing a secondary task. We used a Memory card game as the main task and the pressing of a correctly colored button as the secondary task. Our findings suggest that notifications at a user’s wrist are most suitable when other AR elements are present. Notifications displayed in the World are quick to notice and understand if the view direction of a user is known. Heads-up notifications in the corner of the field-of-view, as they are primarily used in smart glasses, performed significantly worse, especially compared to Subtitle placement. Hence, we recommend to use different notification types depending on the overall structure of an AR system.},
booktitle = {Proceedings of the 2022 ACM Symposium on Spatial User Interaction},
articleno = {17},
numpages = {11},
keywords = {attention, augmented reality, notifications},
location = {Online, CA, USA},
series = {SUI '22}
}

@misc{verge2024doxxing,
  author       = {{The Verge}},
  title        = {College Students Used Meta's Smart Glasses to Dox People in Real Time},
  howpublished = {The Verge},
  year         = {2024},
  month        = {October},
  day          = {2},
  url          = {https://www.theverge.com/2024/10/2/24260262/ray-ban-meta-smart-glasses-doxxing-privacy},
  note         = {Accessed: 2024-11-27}
}


@ARTICLE{reipschlager2021personal,
  author={Reipschlager, Patrick and Flemisch, Tamara and Dachselt, Raimund},
  journal={IEEE Transactions on Visualization and Computer Graphics}, 
  title={Personal Augmented Reality for Information Visualization on Large Interactive Displays}, 
  year={2021},
  volume={27},
  number={2},
  pages={1182-1192},
  keywords={Data visualization;Data analysis;Augmented reality;Three-dimensional displays;Navigation;Visualization;Augmented Reality;Information Visualization;InfoVis;Large Displays;Immersive Analytics;Physical Navigation;Multiple Coordinated Views},
  doi={10.1109/TVCG.2020.3030460}}


@inproceedings{lee2022designspace,
author = {Lee, Benjamin and Cordeil, Maxime and Prouzeau, Arnaud and Jenny, Bernhard and Dwyer, Tim},
title = {A Design Space For Data Visualisation Transformations Between 2D And 3D In Mixed-Reality Environments},
year = {2022},
isbn = {9781450391573},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3491102.3501859},
doi = {10.1145/3491102.3501859},
abstract = {As mixed-reality (MR) technologies become more mainstream, the delineation between data visualisations displayed on screens or other surfaces and those floating in space becomes increasingly blurred. Rather than the choice of using either a 2D surface or the 3D space for visualising data being a dichotomy, we argue that users should have the freedom to transform visualisations seamlessly between the two as needed. However, the design space for such transformations is large, and practically uncharted. To explore this, we first establish an overview of the different states that a data visualisation can take in MR, followed by how transformations between these states can facilitate common visualisation tasks. We then describe a design space of how these transformations function, in terms of the different stages throughout the transformation, and the user interactions and input parameters that affect it. This design space is then demonstrated with multiple exemplary techniques based in MR.},
booktitle = {Proceedings of the 2022 CHI Conference on Human Factors in Computing Systems},
articleno = {25},
numpages = {14},
keywords = {Immersive Analytics, animated transitions, direct manipulation, mixed reality, visualisation},
location = {New Orleans, LA, USA},
series = {CHI '22}
}

@ARTICLE{sereno2022collaborative,
  author={Sereno, Mickael and Wang, Xiyao and Besançon, Lonni and McGuffin, Michael J. and Isenberg, Tobias},
  journal={IEEE Transactions on Visualization and Computer Graphics}, 
  title={Collaborative Work in Augmented Reality: A Survey}, 
  year={2022},
  volume={28},
  number={6},
  pages={2530-2549},
  keywords={Collaboration;Augmented reality;Collaborative work;Visualization;Hardware;Three-dimensional displays;Introductory and survey;computer-supported cooperative work;virtual and augmented reality;immersive analytics},
  doi={10.1109/TVCG.2020.3032761}}


@ARTICLE{medeiros2022shielding,
  author={Medeiros, Daniel and McGill, Mark and Ng, Alexander and McDermid, Robert and Pantidi, Nadia and Williamson, Julie and Brewster, Stephen},
  journal={IEEE Transactions on Visualization and Computer Graphics}, 
  title={From Shielding to Avoidance: Passenger Augmented Reality and the Layout of Virtual Displays for Productivity in Shared Transit}, 
  year={2022},
  volume={28},
  number={11},
  pages={3640-3650},
  keywords={Layout;Space exploration;Public transportation;Automobiles;Productivity;Headphones;Usability;Augmented Reality;Productivity;Virtual Workspaces;Mixed Reality;Extended Reality;Passengers;Transport;Transit;Airplanes;Cars;Trains;Subway;Mobility},
  doi={10.1109/TVCG.2022.3203002}}

@inproceedings{ng2021airplane,
  author = {Ng, Alexander and Medeiros, Daniel and McGill, Mark and Williamson, Julie and Brewster, Stephen},
  title = {The Passenger Experience of Mixed Reality Virtual Display Layouts in Airplane Environments},
  booktitle = {Proceedings of the 2021 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)},
  year = {2021},
  pages = {265--274},
  publisher = {IEEE},
  address = {Piscataway, NJ, USA},
  doi = {10.1109/ISMAR52148.2021.00042},
  keywords = {Productivity, Headphones, Airplanes, Layout, Entertainment industry, Mixed reality, Virtual environments, Mixed Reality, Virtual reality, Augmented reality, Multi-display layouts, Virtual Workspace, Human-centered computing, Human-computer interaction (HCI), Interaction paradigms}
}


@inproceedings{li2019holodoc,
author = {Li, Zhen and Annett, Michelle and Hinckley, Ken and Singh, Karan and Wigdor, Daniel},
title = {HoloDoc: Enabling Mixed Reality Workspaces that Harness Physical and Digital Content},
year = {2019},
isbn = {9781450359702},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3290605.3300917},
doi = {10.1145/3290605.3300917},
abstract = {Prior research identified that physical paper documents have many positive attributes, for example natural tangibility and inherent physical flexibility. When documents are presented on digital devices, however, they can provide unique functionality to users, such as the ability to search, view dynamic multimedia content, and make use of indexing. This work explores the fusion of physical and digital paper documents. It first presents the results of a study that probed how users perform document-intensive analytical tasks when both physical and digital versions of documents were available. The study findings then informed the design of HoloDoc, a mixed reality system that augments physical artifacts with rich interaction and dynamic virtual content. Finally, we present the interaction techniques that HoloDoc affords, and the results of a second study that assessed HoloDoc's utility when working with digital and physical copies of academic articles.},
booktitle = {Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems},
pages = {1–14},
numpages = {14},
keywords = {reading behavior, mixed reality, digital pen input, augmented reality},
location = {Glasgow, Scotland Uk},
series = {CHI '19}
}


@inproceedings{tran2024mapping,
author = {Tran, Tram Thi Minh and Yu, Xinyan and Wang, Yiyuan and Parker, Callum and Tomitsch, Martin},
title = {Mapping Pedestrian-to-Driver Gestures: Implications for Autonomous Vehicle Bidirectional Interaction},
year = {2024},
isbn = {9798400705205},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3641308.3685014},
doi = {10.1145/3641308.3685014},
abstract = {Autonomous vehicles (AVs) necessitate a transformation in how pedestrians and vehicles communicate. Existing research largely focuses on unidirectional communication from AVs to pedestrians. However, traffic interactions are inherently reciprocal and social. To lay the foundation for truly bidirectional interactions with AVs, this study maps the diverse scenarios, messages, and gestures used by pedestrians to initiate communication with human drivers in today’s traffic. Following an online ethnography approach, we conducted an analysis of 20 Reddit discussions comprising 1,094 comments. Our findings reveal a rich communication landscape where pedestrians use gestures to signal urgency, negotiate space, convey warnings, and express needs beyond basic crossing intent. We highlight individual, cultural, and geographical variations in gestures. Based on this expanded understanding, we propose design implications for AV interactions, crucial for promoting safer, more intuitive, and trust-building communication between pedestrians and AVs.},
booktitle = {Adjunct Proceedings of the 16th International Conference on Automotive User Interfaces and Interactive Vehicular Applications},
pages = {1–7},
numpages = {7},
keywords = {autonomous vehicles, bidirectional communication, gestures, pedestrian-driver communication},
location = {Stanford, CA, USA},
series = {AutomotiveUI '24 Adjunct}
}

@misc{wsj2023,
  author       = {Salvador Rodriguez},
  title        = {How Meta's Smart Ray-Ban Glasses Spawned a Silicon Valley Hit},
  year         = {2023},
  url          = {https://www.wsj.com/tech/how-metas-smart-ray-ban-glasses-spawned-a-silicon-valley-hit-3fca3acd},
  note         = {Accessed: 2024-11-18}
}

@incollection{rogers2014diffusion,
  author = {Rogers, Everett M. and Singhal, Arvind and Quinlan, Margaret M.},
  title = {Diffusion of Innovations},
  booktitle = {An Integrated Approach to Communication Theory and Research},
  pages = {432--448},
  year = {2014},
  publisher = {Routledge},
  address = {New York, NY, USA}
}


@article{grubert2018office,
  title={The office of the future: Virtual, portable, and global},
  author={Grubert, Jens and Ofek, Eyal and Pahud, Michel and Kristensson, Per Ola},
  journal={IEEE computer graphics and applications},
  volume={38},
  number={6},
  pages={125--133},
  year={2018},
  publisher={IEEE}
}

@inproceedings{ruvimova2020transport,
author = {Ruvimova, Anastasia and Kim, Junhyeok and Fritz, Thomas and Hancock, Mark and Shepherd, David C.},
title = {"Transport Me Away": Fostering Flow in Open Offices through Virtual Reality},
year = {2020},
isbn = {9781450367080},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3313831.3376724},
doi = {10.1145/3313831.3376724},
abstract = {Open offices are cost-effective and continue to be popular. However, research shows that these environments, brimming with distractions and sensory overload, frequently hamper productivity. Our research investigates the use of virtual reality (VR) to mitigate distractions in an open office setting and improve one's ability to be in flow. In a lab study, 35 participants performed visual programming tasks in four combinations of physical (open or closed office) and virtual environments (beach or virtual office). While participants both preferred and were in flow more in a closed office without VR, in an open office, the VR environments outperformed the no VR condition in all measures of flow, performance, and preference. Especially considering the recent rapid advancements in VR, our findings illustrate the potential VR has to improve flow and satisfaction in open offices.},
booktitle = {Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems},
pages = {1–14},
numpages = {14},
keywords = {flow, open offices, virtual reality, work},
location = {Honolulu, HI, USA},
series = {CHI '20}
}

@misc{zoom2024,
  author       = {{Zoom Video Communications}},
  title        = {Zoom Launches New App for Apple Vision Pro to Make Hybrid Collaboration More Immersive},
  year         = {2024},
  url          = {https://news.zoom.us/zoom-launches-new-app-for-apple-vision-pro-to-make-hybrid-collaboration-more-immersive/},
  note         = {Accessed: 2024-09-12}
}


@inproceedings{kwok2019gaze,
author = {Kwok, Tiffany C.K. and Kiefer, Peter and Schinazi, Victor R. and Adams, Benjamin and Raubal, Martin},
title = {Gaze-Guided Narratives: Adapting Audio Guide Content to Gaze in Virtual and Real Environments},
year = {2019},
isbn = {9781450359702},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3290605.3300721},
doi = {10.1145/3290605.3300721},
abstract = {Exploring a city panorama from a vantage point is a popular tourist activity. Typical audio guides that support this activity are limited by their lack of responsiveness to user behavior and by the difficulty of matching audio descriptions to the panorama. These limitations can inhibit the acquisition of information and negatively affect user experience. This paper proposes Gaze-Guided Narratives as a novel interaction concept that helps tourists find specific features in the panorama (gaze guidance) while adapting the audio content to what has been previously looked at (content adaptation). Results from a controlled study in a virtual environment (n=60) revealed that a system featuring both gaze guidance and content adaptation obtained better user experience, lower cognitive load, and led to better performance in a mapping task compared to a classic audio guide. A second study with tourists situated at a vantage point (n=16) further demonstrated the feasibility of this approach in the real world.},
booktitle = {Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems},
pages = {1–12},
numpages = {12},
keywords = {gaze-guided narratives, outdoor eye tracking, tourist guide},
location = {Glasgow, Scotland Uk},
series = {CHI '19}
}

@misc{facebook_reality_labs_2020,
  author       = {Facebook Reality Labs},
  title        = {Inside Facebook Reality Labs Research: The Future of Audio},
  year         = {2020},
  month        = {September},
  howpublished = {\url{https://tech.facebook.com/reality-labs/2020/09/inside-facebook-reality-labs-research-the-future-of-audio/}},
  note         = {Accessed: 2024-09-13}
}

@inproceedings{niyazov2023user,
author = {Niyazov, Aziz and Ens, Barrett and Satriadi, Kadek Ananta and Mellado, Nicolas and Barthe, Loic and Dwyer, Tim and Serrano, Marcos},
title = {User-Driven Constraints for Layout Optimisation in Augmented Reality},
year = {2023},
isbn = {9781450394215},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3544548.3580873},
doi = {10.1145/3544548.3580873},
abstract = {Automatic layout optimisation allows users to arrange augmented reality content in the real-world environment without the need for tedious manual interactions. This optimisation is often based on modelling the intended content placement as constraints, defined as cost functions. Then, applying a cost minimization algorithm leads to a desirable placement. However, such an approach is limited by the lack of user control over the optimisation results. In this paper we explore the concept of user-driven constraints for augmented reality layout optimisation. With our approach users can define and set up their own constraints directly within the real-world environment. We first present a design space composed of three dimensions: the constraints, the regions of interest and the constraint parameters. Then we explore which input gestures can be employed to define the user-driven constraints of our design space through a user elicitation study. Using the results of the study, we propose a holistic system design and implementation demonstrating our user-driven constraints, which we evaluate in a final user study where participants had to create several constraints at the same time to arrange a set of virtual contents.},
booktitle = {Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems},
articleno = {35},
numpages = {16},
location = {Hamburg, Germany},
series = {CHI '23}
}

@inproceedings{strait2017public,
  title={The public's perception of humanlike robots: Online social commentary reflects an appearance-based uncanny valley, a general fear of a “Technology Takeover”, and the unabashed sexualization of female-gendered robots},
  author={Strait, Megan K and Aguillon, Cynthia and Contreras, Virginia and Garcia, Noemi},
  booktitle={2017 26th IEEE international symposium on robot and human interactive communication (RO-MAN)},
  pages={1418--1423},
  year={2017},
  organization={IEEE}
}


@inproceedings{paay2015connecting,
author = {Paay, Jeni and Kjeldskov, Jesper and Skov, Mikael B.},
title = {Connecting in the Kitchen: An Empirical Study of Physical Interactions while Cooking Together at Home},
year = {2015},
isbn = {9781450329224},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2675133.2675194},
doi = {10.1145/2675133.2675194},
abstract = {Recent research has explored the role technology might play in future kitchens, including virtually dining together, recipe sharing, augmented kitchen furniture, reactive cooking utensils and gestural interaction. When people come together in a kitchen to cook it is about more than just production of sustenance -- it is about being together, helping each other, exchanging stories, and contributing to the gradual emergence of a shared meal. In this paper we present a digital ethnography of how people coordinate and cooperate in their kitchens when cooking together for the purpose of inspiring the design of social natural user interactions for technologies in the kitchen. The study is based on 61 YouTube videos of people cooking together analyzed using the frameworks of proxemics and F-formations. Our findings unfold and illustrate relationships between people's spatial organization, their cooking activities and physical kitchen layouts. Based on these we discuss the kitchen as a design space and particularly the opportunities for social natural user interaction design.},
booktitle = {Proceedings of the 18th ACM Conference on Computer Supported Cooperative Work \& Social Computing},
pages = {276–287},
numpages = {12},
keywords = {cooking together, digital ethnography, digital kitchens, f-formations, natural user interaction, proxemics, the home},
location = {Vancouver, BC, Canada},
series = {CSCW '15}
}

@inproceedings{hover2021uncanny,
  title={Uncanny, sexy, and threatening robots: The online community's attitude to and perceptions of robots varying in humanlikeness and gender},
  author={Hover, Quirien RM and Velner, Ella and Beelen, Thomas and Boon, Mieke and Truong, Khiet P},
  booktitle={Proceedings of the 2021 ACM/IEEE International Conference on Human-Robot Interaction},
  pages={119--128},
  year={2021}
}


@inproceedings{blythe2009critical,
author = {Blythe, Mark and Cairns, Paul},
title = {Critical methods and user generated content: the iPhone on YouTube},
year = {2009},
isbn = {9781605582467},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1518701.1518923},
doi = {10.1145/1518701.1518923},
abstract = {Sites like YouTube offer vast sources of data for studies of human computer interaction. However, they also present a number of methodological challenges. This paper offers an example study of the initial reception of the iPhone 3G through YouTube. It begins with a quantitative account of the overall shape of the most frequently viewed returns for an iPhone 3G" search. A content analysis of the first hundred videos then explores the returns categorized by genre. Comments on the most popular video "Will It Blend"are analysed using grounded theory. It is argued that social science methods are not sufficient for a rich understanding of such material. The paper concludes with an analysis of"Will it Blend"that draws on cultural and critical theory. It is argued that a multi-methodological approach is necessary to exploit such data and also to address the challenges of next generation Human Computer Interaction (HCI).},
booktitle = {Proceedings of the SIGCHI Conference on Human Factors in Computing Systems},
pages = {1467–1476},
numpages = {10},
keywords = {user generated content, user experience, iPhone, green HCI, critical theory, YouTube},
location = {Boston, MA, USA},
series = {CHI '09}
}

@inproceedings{anthony2013analyzing,
  title={Analyzing user-generated youtube videos to understand touchscreen use by people with motor impairments},
  author={Anthony, Lisa and Kim, YooJin and Findlater, Leah},
  booktitle={Proceedings of the SIGCHI conference on human factors in computing systems},
  pages={1223--1232},
  year={2013}
}

@article{nielsen2023using,
author = {Nielsen, Sara and Skov, Mikael B. and Hansen, Karl Damkj\ae{}r and Kaszowska, Aleksandra},
title = {Using User-Generated YouTube Videos to Understand Unguided Interactions with Robots in Public Places},
year = {2023},
issue_date = {March 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {12},
number = {1},
url = {https://doi.org/10.1145/3550280},
doi = {10.1145/3550280},
abstract = {Professional service robots are increasingly being deployed in public places, which thus increases user exposure. However, we lack an empirical understanding of complex encounters taking place in dynamic and often crowded environments as well as how people overcome breakdowns during unguided interaction with a robot in a real-world scenario. In this paper, we conducted a covert, digital ethnographic study analyzing 104 user-generated YouTube videos focusing on people’s unguided interactions with robots in several public places. We identified several types of interaction breakdowns pertaining to someone (person-initiated interaction breakdown, IB) or something (environmental disturbances, ED) having a direct, negative effect on an ongoing unguided interaction. Our findings have implications for the design and development of service robots facing multi-user scenarios entertaining active (primary and secondary) users, inactive (commentators and observers) ‘users’, and Incidentally Co-present Persons (InCoPs). Furthermore, we contribute to and built on the limited prior use of YouTube videos and digital ethnography in HRI research, thereby demonstrating its effectiveness in studying unguided interactions in public places, while supplementing and adding to the existing knowledge base of service robots in public places.},
journal = {J. Hum.-Robot Interact.},
month = {feb},
articleno = {5},
numpages = {40},
keywords = {Service robots, unguided interactions, interaction breakdowns, digital ethnography, robots in public places}
}

@article{masten2003digital,
  title={Digital ethnography: The next wave in understanding the consumer experience},
  author={Masten, Davis L and Plowman, Tim MP},
  journal={Design Management Journal (Former Series)},
  volume={14},
  number={2},
  pages={75--81},
  year={2003},
  publisher={Blackwell Publishing Ltd Oxford, UK}
}

@inproceedings{bowman2021keynote,
  author = {Bowman, Douglas},
  title = {Keynote Speaker: User Experience Considerations for Everyday Augmented Reality},
  booktitle = {2021 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)},
  year = {2021},
  pages = {16-16},
  doi = {10.1109/ISMAR52148.2021.00010},
  publisher = {IEEE},
  address = {Piscataway, NJ, USA},  
  keywords = {Three-dimensional displays, User experience, Smart phones, Human-computer interaction, Computer science, Augmented reality, Visualization}
}



@article{grubert2016towards,
  title={Towards pervasive augmented reality: Context-awareness in augmented reality},
  author={Grubert, Jens and Langlotz, Tobias and Zollmann, Stefanie and Regenbrecht, Holger},
  journal={IEEE transactions on visualization and computer graphics},
  volume={23},
  number={6},
  pages={1706--1724},
  year={2016},
  publisher={IEEE}
}


@inproceedings{xu2023xair,
author = {Xu, Xuhai and Yu, Anna and Jonker, Tanya R. and Todi, Kashyap and Lu, Feiyu and Qian, Xun and Evangelista Belo, Jo\~{a}o Marcelo and Wang, Tianyi and Li, Michelle and Mun, Aran and Wu, Te-Yen and Shen, Junxiao and Zhang, Ting and Kokhlikyan, Narine and Wang, Fulton and Sorenson, Paul and Kim, Sophie and Benko, Hrvoje},
title = {XAIR: A Framework of Explainable AI in Augmented Reality},
year = {2023},
isbn = {9781450394215},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3544548.3581500},
doi = {10.1145/3544548.3581500},
abstract = {Explainable AI (XAI) has established itself as an important component of AI-driven interactive systems. With Augmented Reality (AR) becoming more integrated in daily lives, the role of XAI also becomes essential in AR because end-users will frequently interact with intelligent services. However, it is unclear how to design effective XAI experiences for AR. We propose XAIR, a design framework that addresses when, what, and how to provide explanations of AI output in AR. The framework was based on a multi-disciplinary literature review of XAI and HCI research, a large-scale survey probing 500+ end-users’ preferences for AR-based explanations, and three workshops with 12 experts collecting their insights about XAI design in AR. XAIR’s utility and effectiveness was verified via a study with 10 designers and another study with 12 end-users. XAIR can provide guidelines for designers, inspiring them to identify new design opportunities and achieve effective XAI designs in AR.},
booktitle = {Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems},
articleno = {202},
numpages = {30},
keywords = {Augmented Reality, Design Framework, Explainable AI},
location = {Hamburg, Germany},
series = {CHI '23}
}

@inproceedings{lu2021evaluating,
  author = {Lu, Feiyu and Bowman, Doug A.},
  title = {Evaluating the Potential of Glanceable AR Interfaces for Authentic Everyday Uses},
  booktitle = {Proceedings of the 2021 IEEE Virtual Reality and 3D User Interfaces (VR) Conference},
  year = {2021},
  pages = {768--777},
  publisher = {IEEE},
  address = {Piscataway, NJ, USA},
  doi = {10.1109/VR50410.2021.00104},
  keywords = {Headphones, Three-dimensional displays, Head-mounted displays, Prototypes, Glass, User interfaces, Task analysis, Human-centered computing-Mixed / augmented reality, Human-centered computing-User interface design}
}



@article{o2023privacy,
  title={Privacy-enhancing technology and everyday augmented reality: Understanding bystanders' varying needs for awareness and consent},
  author={O'Hagan, Joseph and Saeghe, Pejman and Gugenheimer, Jan and Medeiros, Daniel and Marky, Karola and Khamis, Mohamed and McGill, Mark},
  journal={Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies},
  volume={6},
  number={4},
  pages={1--35},
  year={2023},
  publisher={ACM New York, NY, USA}
}


@inproceedings{pavanatto2021we,
  author = {Pavanatto, Leonardo and North, Chris and Bowman, Doug A. and Badea, Carmen and Stoakley, Richard},
  title = {Do We Still Need Physical Monitors? An Evaluation of the Usability of AR Virtual Monitors for Productivity Work},
  booktitle = {Proceedings of the 2021 IEEE Virtual Reality and 3D User Interfaces (VR) Conference},
  year = {2021},
  pages = {759--767},
  publisher = {IEEE},
  address = {Piscataway, NJ, USA},
  doi = {10.1109/VR50410.2021.00103},
  keywords = {Productivity, Headphones, Three-dimensional displays, User interfaces, Usability, Monitoring, Augmented reality, Human-centered computing, Human-computer interaction (HCI), Interaction paradigms, Mixed / augmented reality, Empirical studies in interaction design}
}



@article{azuma2019road,
  title={The road to ubiquitous consumer augmented reality systems},
  author={Azuma, Ronald T},
  journal={Human Behavior and Emerging Technologies},
  volume={1},
  number={1},
  pages={26--32},
  year={2019},
  publisher={Wiley Online Library}
}

@inproceedings{lu2020glanceable,
  author = {Lu, Feiyu and Davari, Shakiba and Lisle, Lee and Li, Yuan and Bowman, Doug A.},
  title = {Glanceable AR: Evaluating Information Access Methods for Head-Worn Augmented Reality},
  booktitle = {Proceedings of the 2020 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)},
  year = {2020},
  pages = {930--939},
  publisher = {IEEE},
  address = {Piscataway, NJ, USA},
  doi = {10.1109/VR46266.2020.00113},
  keywords = {Augmented reality, User interfaces, Human computer interaction, Head-mounted displays, Human-centered computing, Mixed / augmented reality, User interface design}
}



@inproceedings{komkaite2019underneath,
author = {Komkaite, Aida and Lavrinovica, Liga and Vraka, Maria and Skov, Mikael B.},
title = {Underneath the Skin: An Analysis of YouTube Videos to Understand Insertable Device Interaction},
year = {2019},
isbn = {9781450359702},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3290605.3300444},
doi = {10.1145/3290605.3300444},
abstract = {During the last decade, people have started to experiment with insertable technology like RFID or NFC chips and use them for e.g. identification. However, little is known about how people in fact interact with and adapt insertables. We conducted a video analysis of 122 YouTube videos to gain insight into the interaction with the insertables. Second, we implemented an online survey to complement our data from the video analysis. Our findings show that there are many opportunities for interaction with insertables both for task-oriented and creative purposes. However, there are also multiple challenges and obstacles as well as side effects and health concerns. Our findings conclude that the current infrastructure is not ready to support the use of insertables yet, and we discuss implications of this.},
booktitle = {Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems},
pages = {1–12},
numpages = {12},
keywords = {hobbyist, in the body, insertables, interaction, nfc, rfid, survey, youtube},
location = {Glasgow, Scotland Uk},
series = {CHI '19}
}

@article{baashar2023towards,
  title={Towards wearable augmented reality in healthcare: a comparative survey and analysis of head-mounted displays},
  author={Baashar, Yahia and Alkawsi, Gamal and Wan Ahmad, Wan Nooraishya and Alomari, Mohammad Ahmed and Alhussian, Hitham and Tiong, Sieh Kiong},
  journal={International journal of environmental research and public health},
  volume={20},
  number={5},
  pages={3940},
  year={2023},
  publisher={MDPI}
}

@ARTICLE{tran2023wearable,
  author={Tran, Tram Thi Minh and Brown, Shane and Weidlich, Oliver and Billinghurst, Mark and Parker, Callum},
  journal={IEEE Transactions on Visualization and Computer Graphics}, 
  title={Wearable Augmented Reality: Research Trends and Future Directions from Three Major Venues}, 
  year={2023},
  volume={29},
  number={11},
  pages={4782-4793},
  keywords={Market research;Headphones;Ethics;Augmented reality;Rendering (computer graphics);Calibration;Biomedical monitoring;Augmented reality;mixed reality;head-mounted displays;survey;trends},
  doi={10.1109/TVCG.2023.3320231}}


@inproceedings{gasques2021artemis,
author = {Gasques, Danilo and Johnson, Janet G. and Sharkey, Tommy and Feng, Yuanyuan and Wang, Ru and Xu, Zhuoqun Robin and Zavala, Enrique and Zhang, Yifei and Xie, Wanze and Zhang, Xinming and Davis, Konrad and Yip, Michael and Weibel, Nadir},
title = {ARTEMIS: A Collaborative Mixed-Reality System for Immersive Surgical Telementoring},
year = {2021},
isbn = {9781450380966},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3411764.3445576},
doi = {10.1145/3411764.3445576},
abstract = {Traumatic injuries require timely intervention, but medical expertise is not always available at the patient’s location. Despite recent advances in telecommunications, surgeons still have limited tools to remotely help inexperienced surgeons. Mixed Reality hints at a future where remote collaborators work side-by-side as if co-located; however, we still do not know how current technology can improve remote surgical collaboration. Through role-playing and iterative-prototyping, we identify collaboration practices used by expert surgeons to aid novice surgeons as well as technical requirements to facilitate these practices. We then introduce ARTEMIS, an AR-VR collaboration system that supports these key practices. Through an observational study with two expert surgeons and five novice surgeons operating on cadavers, we find that ARTEMIS supports remote surgical mentoring of novices through synchronous point, draw, and look affordances and asynchronous video clips. Most participants found that ARTEMIS facilitates collaboration despite existing technology limitations explored in this paper.},
booktitle = {Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems},
articleno = {662},
numpages = {14},
keywords = {Augmented Reality, Collaboration, Mixed Reality, Surgery, Telementoring, Virtual Reality},
location = {Yokohama, Japan},
series = {CHI '21}
}

@inproceedings{eom2022neurolens,
  author = {Eom, Sangjun and Sykes, David and Rahimpour, Shervin and Gorlatova, Maria},
  title = {NeuroLens: Augmented Reality-based Contextual Guidance through Surgical Tool Tracking in Neurosurgery},
  booktitle = {Proceedings of the 2022 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)},
  year = {2022},
  pages = {355--364},
  publisher = {IEEE},
  address = {Piscataway, NJ, USA},
  doi = {10.1109/ISMAR55827.2022.00051},
  keywords = {Training, Visualization, Target tracking, Head, Phantoms, Neurosurgery, Trajectory, Human-centered computing, Human-computer interaction (HCI), Interaction paradigms, Mixed / augmented reality, Interaction devices, Displays and imagers}
}

@inproceedings{burova2020utilizing,
author = {Burova, Alisa and M\"{a}kel\"{a}, John and Hakulinen, Jaakko and Keskinen, Tuuli and Heinonen, Hanna and Siltanen, Sanni and Turunen, Markku},
title = {Utilizing VR and Gaze Tracking to Develop AR Solutions for Industrial Maintenance},
year = {2020},
isbn = {9781450367080},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3313831.3376405},
doi = {10.1145/3313831.3376405},
abstract = {Augmented reality (AR) presents a variety of possibilities for industrial maintenance. However, the development of real-world AR solutions has been limited due to the technological capabilities and uncertainty with respect to safety at deployment. We introduce the approach of using AR simulation in virtual reality (VR) coupled with gaze tracking to enable resource-efficient AR development. We tested in-field AR guidance and safety awareness features in an iterative development-evaluation process with experts from the elevator maintenance industry. We further conducted a survey, utilizing actual gaze data from the evaluation to elicit comments from industry experts on the usefulness of AR simulation and gaze tracking. Our results show the potential of AR within VR approach combined with gaze tracking. With this framework, AR solutions can be iteratively and safely tested without actual implementation, while gaze data provide advanced objective means to evaluate the designed AR content, documentation usage, and safety awareness.},
booktitle = {Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems},
pages = {1–13},
numpages = {13},
keywords = {gaze tracking, industrial maintenance, safety, virtual prototyping, augmented reality, virtual reality},
location = {Honolulu, HI, USA},
series = {CHI '20}
}


@article{gattullo2020and,
  title={What, how, and why are visual assets used in industrial augmented reality? A systematic review and classification in maintenance, assembly, and training (from 1997 to 2019)},
  author={Gattullo, Michele and Evangelista, Alessandro and Uva, Antonio E and Fiorentino, Michele and Gabbard, Joseph L},
  journal={IEEE Trans. Vis. Comput. Graph.},
  volume={28},
  number={2},
  pages={1443--1456},
  year={2020},
  publisher={IEEE}
}

@inproceedings{cao2020exploratory,
author = {Cao, Yuanzhi and Qian, Xun and Wang, Tianyi and Lee, Rachel and Huo, Ke and Ramani, Karthik},
title = {An Exploratory Study of Augmented Reality Presence for Tutoring Machine Tasks},
year = {2020},
isbn = {9781450367080},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3313831.3376688},
doi = {10.1145/3313831.3376688},
abstract = {Machine tasks in workshops or factories are often a compound sequence of local, spatial, and body-coordinated human-machine interactions. Prior works have shown the merits of video-based and augmented reality (AR) tutoring systems for local tasks. However, due to the lack of a bodily representation of the tutor, they are not as effective for spatial and body-coordinated interactions. We propose avatars as an additional tutor representation to the existing AR instructions. In order to understand the design space of tutoring presence for machine tasks, we conduct a comparative study with 32 users. We aim to explore the strengths/limitations of the following four tutor options: video, non-avatar-AR, half-body+AR, and full-body+AR. The results show that users prefer the half-body+AR overall, especially for the spatial interactions. They have a preference for the full-body+AR for the body-coordinated interactions and the non-avatar-AR for the local interactions. We further discuss and summarize design recommendations and insights for future machine task tutoring systems.},
booktitle = {Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems},
pages = {1–13},
numpages = {13},
keywords = {augmented reality, avatar tutor, exploratory study, machine task, tutoring system design},
location = {Honolulu, HI, USA},
series = {CHI '20}
}

@article{henderson2010exploring,
  title={Exploring the benefits of augmented reality documentation for maintenance and repair},
  author={Henderson, Steven and Feiner, Steven},
  journal={IEEE transactions on visualization and computer graphics},
  volume={17},
  number={10},
  pages={1355--1368},
  year={2010},
  publisher={Ieee}
}

@article{starner1997augmented,
  title={Augmented reality through wearable computing},
  author={Starner, Thad and Mann, Steve and Rhodes, Bradley and Levine, Jeffrey and Healey, Jennifer and Kirsch, Dana and Picard, Rosalind W and Pentland, Alex},
  journal={Presence: Teleoperators \& Virtual Environments},
  volume={6},
  number={4},
  pages={386--398},
  year={1997},
  publisher={MIT Press One Rogers Street, Cambridge, MA 02142-1209, USA journals-info~…}
}


@inproceedings{mathis2024everyday,
author = {Mathis, Florian},
title = {Everyday Life Challenges and Augmented Realities: Exploring Use Cases For, and User Perspectives on, an Augmented Everyday Life},
year = {2024},
isbn = {9798400709807},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3652920.3652921},
doi = {10.1145/3652920.3652921},
abstract = {Contextually aware visual, auditory, and haptic interfaces can augment and empower people in their everyday life. However, little is known about the use cases and users’ perspectives on a pervasive augmented reality that augments places and humans. In this paper, we contribute a first step towards an augmented societal future by outlining promising example use cases for assistive mixed reality interfaces (i.e., AssistiveMR). By surveying 60 participants, we found that an augmented reality has the potential to find wide-spread application in a plethora of scenarios, including supporting people in recalling information, disconnecting from reality, and augmenting communication with others using visual augmentations. However, participants expressed concerns regarding the potential high costs associated with errors and raised questions about the social acceptability of augmentations of humans and real-world surroundings. Our exploration of promising use cases for assistive MR augmentations aims at serving as inspiration, motivating researchers to augment, support, and empower individuals in their daily activities.},
booktitle = {Proceedings of the Augmented Humans International Conference 2024},
pages = {52–62},
numpages = {11},
keywords = {Assistive Mixed Reality, Augmenting Humans, Everyday AR},
location = {Melbourne, VIC, Australia},
series = {AHs '24}
}

@inproceedings{wolf2018we,
  author = {Wolf, Katrin and Marky, Karola and Funk, Markus},
  title = {We Should Start Thinking About Privacy Implications of Sonic Input in Everyday Augmented Reality!},
  booktitle = {Proceedings of Mensch und Computer 2018 - Workshopband},
  series = {GI-Edition},
  year = {2018},
  month = {September},
  pages = {353--359},
  publisher = {Gesellschaft für Informatik e.V.},
  address = {Dresden, Germany},
  issn = {1617-5468},
  url = {https://hdl.handle.net/20.500.12738/16327},
  keywords = {Smart Assistant, Sonic, Voice, Privacy}
}


@inproceedings{abraham2024you,
author = {Abraham, Melvin and Mcgill, Mark and Khamis, Mohamed},
title = {What You Experience is What We Collect: User Experience Based Fine-Grained Permissions for Everyday Augmented Reality},
year = {2024},
isbn = {9798400703300},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3613904.3642668},
doi = {10.1145/3613904.3642668},
abstract = {Everyday Augmented Reality (AR) headsets pose significant privacy risks, potentially allowing prolonged sensitive data collection of both users and bystanders (e.g. members of the public). While users control data access through permissions, current AR systems inherit smartphone permission prompts, which may be less appropriate for all-day AR. This constrains informed choices and risks over-privileged access to sensors. We propose (N=20) a novel AR permission control system that allows better-informed privacy decisions and evaluate it using five mock application contexts. Our system’s novelty lies in enabling users to experience the varying impacts of permission levels on not only a) privacy, but also b) application functionality. This empowers users to better understand what data an application depends on and how its functionalities are impacted by limiting said data. Participants found that our method allows for making better informed privacy decisions, and deemed it more transparent and trustworthy than state-of-the-art AR and smartphone permission systems taken from Android and iOS. Our results offer insights into new and necessary AR permission systems, improving user understanding and control over data access.},
booktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},
articleno = {772},
numpages = {24},
keywords = {AR sensing, Augmented Reality, Privacy},
location = {Honolulu, HI, USA},
series = {CHI '24}
}


@inproceedings{bonner2023filters,
author = {Bonner, Jolie and Mathis, Florian and O'Hagan, Joseph and Mcgill, Mark},
title = {When Filters Escape the Smartphone: Exploring Acceptance and Concerns Regarding Augmented Expression of Social Identity for Everyday AR},
year = {2023},
isbn = {9798400703287},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3611659.3615707},
doi = {10.1145/3611659.3615707},
abstract = {Mass adoption of Everyday Augmented Reality (AR) glasses will enable pervasive augmentation of our expression of social identity through AR filters, transforming our perception of self and others. However, despite filters’ prominent and often problematic usage in social media, research has yet to reflect on the potential impact AR filters might have when brought into everyday life. Informed by our survey of 300 existing popular AR filters used on Snapchat, Instagram and Tiktok, we conducted an AR-in-VR user study where participants (N=24) were exposed to 18 filters across six categories. We evaluated the social acceptability of these augmentations around others and attitudes towards an individual’s augmented self.Our findings highlight 1) how users broadly respected another individual’s augmented self; 2) positive use cases, such as supporting the presentation of gender identity; and 3) tensions around applying AR filters to others (e.g. censorship, changing protected characteristics) and their impact on self-perception (e.g. perpetuating unrealistic beauty standards). We raise questions regarding the rights of individuals to augment and be augmented that provoke the need for further consideration of AR augmentations in society.},
booktitle = {Proceedings of the 29th ACM Symposium on Virtual Reality Software and Technology},
articleno = {14},
numpages = {14},
keywords = {AR Filters, Augmented Identity, Augmented Reality, Identity, Mediated Perception, Self-Presentation, Social Identity},
location = {Christchurch, New Zealand},
series = {VRST '23}
}


@inproceedings{mhaidli2021identifying,
author = {Mhaidli, Abraham Hani and Schaub, Florian},
title = {Identifying Manipulative Advertising Techniques in XR Through Scenario Construction},
year = {2021},
isbn = {9781450380966},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3411764.3445253},
doi = {10.1145/3411764.3445253},
abstract = {As Extended Reality (XR) devices and applications become more mainstream, so too will XR advertising&nbsp;—&nbsp;advertising that takes place in XR mediums. Due to the defining features of XR devices, such as the immersivity of the medium and the ability of XR devices to simulate reality, there are fears that these features could be exploited to create manipulative XR ads that trick consumers into buying products they do not need or might harm them. Using scenario construction, we investigate potential future incarnations of manipulative XR advertising and their harms. We identify five key mechanisms of manipulative XR advertising: misleading experience marketing; inducing artificial emotions in consumers; sensing and targeting people when they are vulnerable; emotional manipulation through hyperpersonalization; and distortion of reality. We discuss research challenges and questions in order to address and mitigate manipulative XR advertising risks.},
booktitle = {Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems},
articleno = {296},
numpages = {18},
keywords = {virtual reality, scenario construction, privacy, mixed reality, computer ethics., augmented reality, advertising, Extended reality},
location = {Yokohama, Japan},
series = {CHI '21}
}

@article{krauss2024makes,
author = {Krau\ss{}, Veronika and Saeghe, Pejman and Boden, Alexander and Khamis, Mohamed and McGill, Mark and Gugenheimer, Jan and Nebeling, Michael},
title = {What Makes XR Dark? Examining Emerging Dark Patterns in Augmented and Virtual Reality through Expert Co-Design},
year = {2024},
issue_date = {June 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {31},
number = {3},
issn = {1073-0516},
url = {https://doi.org/10.1145/3660340},
doi = {10.1145/3660340},
abstract = {Dark patterns are deceptive designs that influence a user’s interactions with an interface to benefit someone other than the user. Prior work has identified dark patterns in windows, icons, menus, and pointer (WIMP) interfaces and ubicomp environments, but how dark patterns can manifest in Augmented and Virtual Reality (collectively XR) requires more attention. We therefore conducted 10 co-design workshops with 20 experts in XR and deceptive design. Our participants co-designed 42 scenarios containing dark patterns, based on application archetypes presented in recent HCI/XR literature. In the co-designed scenarios, we identified 10 novel dark patterns in addition to 39 existing ones, as well as 10 examples in which specific characteristics associated with XR potentially amplified the effect dark patterns could have on users. Based on our findings and prior work, we present a classification of XR-specific properties that facilitate dark patterns: perception, spatiality, physical/virtual barriers, and XR device sensing. We also present the experts’ assessments of the likelihood and severity of the co-designed scenarios and highlight key aspects they considered for this evaluation, for example, technological feasibility, ease of upscaling and distributing malicious implementations, and the application’s context of use. Finally, we discuss means to mitigate XR dark patterns and support regulatory bodies to reduce potential harms.},
journal = {ACM Trans. Comput.-Hum. Interact.},
month = aug,
articleno = {32},
numpages = {39},
keywords = {Speculative design workshops, deceptive design, augmented reality, virtual reality, dark patterns}
}

@article{billinghurst2021grand,
  author = {Billinghurst, Mark},
  title = {Grand Challenges for Augmented Reality},
  journal = {Frontiers in Virtual Reality},
  volume = {2},
  year = {2021},
  pages = {1--4},
  publisher = {Frontiers},
  doi = {10.3389/frvir.2021.578080},
  url = {https://www.frontiersin.org/articles/10.3389/frvir.2021.578080},
  issn = {2673-4192}
}


@article{glassner2003everyday,
  title={Everyday computer graphics},
  author={Glassner, Andrew},
  journal={IEEE Computer Graphics and Applications},
  volume={23},
  number={6},
  pages={76--82},
  year={2003},
  publisher={IEEE}
}

@misc{dyson2024,
  author       = {{Dyson}},
  title        = {Dyson CleanTrace: The augmented reality tool that shows where you've vacuumed},
  year         = {2024},
  url          = {https://www.dyson.com.au/newsroom/updates/dyson-cleantrace},
  note         = {Accessed: 2024-09-12}
}


@inproceedings{strait2017online,
  title={Online social commentary reflects an appearance-based uncanny valley a general fear of a “technology takeover” and the unabashed sexualization of female-gendered robots},
  author={Strait, Megan and Aguillon, Cynthia and Contreras, Virginia and Garcia, Noemi},
  booktitle={Proc. RO-MAN},
  year={2017}
}

@article{landis1977measurement,
  title={The measurement of observer agreement for categorical data},
  author={Landis, J Richard and Koch, Gary G},
  journal={biometrics},
  pages={159--174},
  year={1977},
  publisher={JSTOR}
}

@inproceedings{lu2022exploring,
author = {Lu, Feiyu and Xu, Yan},
title = {Exploring Spatial UI Transition Mechanisms with Head-Worn Augmented Reality},
year = {2022},
isbn = {9781450391573},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3491102.3517723},
doi = {10.1145/3491102.3517723},
abstract = {Imagine in the future people comfortably wear augmented reality (AR) displays all day, how do we design interfaces that adapt to the contextual changes as people move around? In current operating systems, the majority of AR content defaults to staying at a fixed location until being manually moved by the users. However, this approach puts the burden of user interface (UI) transition solely on users. In this paper, we first ran a bodystorming design workshop to capture the limitations of existing manual UI transition approaches in spatially diverse tasks. Then we addressed these limitations by designing and evaluating three UI transition mechanisms with different levels of automation and controllability (low-effort manual, semi-automated, fully-automated). Furthermore, we simulated imperfect contextual awareness by introducing prediction errors with different costs to correct them. Our results provide valuable lessons about the trade-offs between UI automation levels, controllability, user agency, and the impact of prediction errors.},
booktitle = {Proceedings of the 2022 CHI Conference on Human Factors in Computing Systems},
articleno = {550},
numpages = {16},
keywords = {controllability, automation, agency, adaptive interfaces},
location = {New Orleans, LA, USA},
series = {CHI '22}
}

@article{yu2024understanding,
author = {Yu, Xinyan and Hoggenm\"{u}ller, Marius and Tran, Tram Thi Minh and Wang, Yiyuan and Tomitsch, Martin},
title = {Understanding the Interaction between Delivery Robots and Other Road and Sidewalk Users: A Study of User-generated Online Videos},
year = {2024},
issue_date = {December 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {13},
number = {4},
url = {https://doi.org/10.1145/3677615},
doi = {10.1145/3677615},
abstract = {The deployment of autonomous delivery robots in urban environments presents unique challenges in navigating complex traffic conditions and interacting with diverse road and sidewalk users. Effective communication between robots and road and sidewalk users is crucial to address these challenges. This study investigates real-world encounter scenarios where delivery robots and road and sidewalk users interact, seeking to understand the essential role of communication in ensuring seamless encounters. Following an online ethnography approach, we collected 117 user-generated videos from TikTok and their associated 2,067 comments. Our systematic analysis revealed several design opportunities to augment communication between delivery robots and road and sidewalk users, which include facilitating multi-party path negotiation, managing unexpected robot behaviour via transparency information, and expressing robot limitations to request human assistance. Moreover, the triangulation of video and comments analysis provides a set of design considerations to realise these opportunities. The findings contribute to understanding the operational context of delivery robots and offer insights for designing interactions with road and sidewalk users, facilitating their integration into urban spaces.},
journal = {J. Hum.-Robot Interact.},
month = oct,
articleno = {59},
numpages = {32},
keywords = {delivery robots, online ethnography, human-robot interaction}
}

@article{hamasaki2019varifocal,
  title={Varifocal occlusion for optical see-through head-mounted displays using a slide occlusion mask},
  author={Hamasaki, Takumi and Itoh, Yuta},
  journal={IEEE transactions on visualization and computer graphics},
  volume={25},
  number={5},
  pages={1961--1969},
  year={2019},
  publisher={IEEE}
}


@inproceedings{pei2022hand,
author = {Pei, Siyou and Chen, Alexander and Lee, Jaewook and Zhang, Yang},
title = {Hand Interfaces: Using Hands to Imitate Objects in AR/VR for Expressive Interactions},
year = {2022},
isbn = {9781450391573},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3491102.3501898},
doi = {10.1145/3491102.3501898},
abstract = {Augmented reality (AR) and virtual reality (VR) technologies create exciting new opportunities for people to interact with computing resources and information. Less exciting is the need for holding hand controllers, which limits applications that demand expressive, readily available interactions. Prior research investigated freehand AR/VR input by transforming the user’s body into an interaction medium. In contrast to previous work that has users’ hands grasp virtual objects, we propose a new interaction technique that lets users’ hands become virtual objects by imitating the objects themselves. For example, a thumbs-up hand pose is used to mimic a joystick. We created a wide array of interaction designs around this idea to demonstrate its applicability in object retrieval and interactive control tasks. Collectively, we call these interaction designs Hand Interfaces. From a series of user studies comparing Hand Interfaces against various baseline techniques, we collected quantitative and qualitative feedback, which indicates that Hand Interfaces are effective, expressive, and fun to use.},
booktitle = {Proceedings of the 2022 CHI Conference on Human Factors in Computing Systems},
articleno = {429},
numpages = {16},
keywords = {AR/VR, Embodiment, Free-hand interactions, Imitation, Interaction design, On-body interactions},
location = {New Orleans, LA, USA},
series = {CHI '22}
}

@inproceedings{hirzle2019design,
author = {Hirzle, Teresa and Gugenheimer, Jan and Geiselhart, Florian and Bulling, Andreas and Rukzio, Enrico},
title = {A Design Space for Gaze Interaction on Head-mounted Displays},
year = {2019},
isbn = {9781450359702},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3290605.3300855},
doi = {10.1145/3290605.3300855},
abstract = {Augmented and virtual reality (AR/VR) has entered the mass market and, with it, will soon eye tracking as a core technology for next generation head-mounted displays (HMDs). In contrast to existing gaze interfaces, the 3D nature of AR and VR requires estimating a user's gaze in 3D. While first applications, such as foveated rendering, hint at the compelling potential of combining HMDs and gaze, a systematic analysis is missing. To fill this gap, we present the first design space for gaze interaction on HMDs. Our design space covers human depth perception and technical requirements in two dimensions aiming to identify challenges and opportunities for interaction design. As such, our design space provides a comprehensive overview and serves as an important guideline for researchers and practitioners working on gaze interaction on HMDs. We further demonstrate how our design space is used in practice by presenting two interactive applications: EyeHealth and XRay-Vision.},
booktitle = {Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems},
pages = {1–12},
numpages = {12},
keywords = {3d gaze, augmented reality, design space, gaze interaction, head-mounted displays, interaction design, virtual reality},
location = {Glasgow, Scotland Uk},
series = {CHI '19}
}


@inproceedings{schmitz2022squeezy,
author = {Schmitz, Martin and G\"{u}nther, Sebastian and Sch\"{o}n, Dominik and M\"{u}ller, Florian},
title = {Squeezy-Feely: Investigating Lateral Thumb-Index Pinching as an Input Modality},
year = {2022},
isbn = {9781450391573},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3491102.3501981},
doi = {10.1145/3491102.3501981},
abstract = {From zooming on smartphones and mid-air gestures to deformable user interfaces, thumb-index pinching grips are used in many interaction techniques. However, there is still a lack of systematic understanding of how the accuracy and efficiency of such grips are affected by various factors such as counterforce, grip span, and grip direction. Therefore, in this paper, we contribute an evaluation (N = 18) of thumb-index pinching performance in a visual targeting task using scales up to 75 items. As part of our findings, we conclude that the pinching interaction between the thumb and index finger is a promising modality also for one-dimensional input on higher scales. Furthermore, we discuss and outline implications for future user interfaces that benefit from pinching as an additional and complementary interaction modality.},
booktitle = {Proceedings of the 2022 CHI Conference on Human Factors in Computing Systems},
articleno = {61},
numpages = {15},
keywords = {Deformation, Input, Mixed Reality, Pinching, Thumb-to-finger, User Studies},
location = {New Orleans, LA, USA},
series = {CHI '22}
}

@inproceedings{lopes2018adding,
author = {Lopes, Pedro and You, Sijing and Ion, Alexandra and Baudisch, Patrick},
title = {Adding Force Feedback to Mixed Reality Experiences and Games using Electrical Muscle Stimulation},
year = {2018},
isbn = {9781450356206},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3173574.3174020},
doi = {10.1145/3173574.3174020},
abstract = {We present a mobile system that enhances mixed reality experiences and games with force feedback by means of electrical muscle stimulation (EMS). The benefit of our approach is that it adds physical forces while keeping the users' hands free to interact unencumbered-not only with virtual objects, but also with physical objects, such as props and appliances. We demonstrate how this supports three classes of applications along the mixed-reality continuum: (1) entirely virtual objects, such as furniture with EMS friction when pushed or an EMS-based catapult game. (2) Virtual objects augmented via passive props with EMS-constraints, such as a light control panel made tangible by means of a physical cup or a balance-the-marble game with an actuated tray. (3) Augmented appliances with virtual behaviors, such as a physical thermostat dial with EMS-detents or an escape-room that repurposes lamps as levers with detents. We present a user-study in which participants rated the EMS-feedback as significantly more realistic than a no-EMS baseline.},
booktitle = {Proceedings of the 2018 CHI Conference on Human Factors in Computing Systems},
pages = {1–13},
numpages = {13},
keywords = {ar, augmented reality, body i/o, electrical muscle stimulation, haptics, hololens, mixed reality, mr, wearable},
location = {Montreal QC, Canada},
series = {CHI '18}
}

@inproceedings{mandl2021neural,
  author = {Mandl, David and Roth, Peter M. and Langlotz, Tobias and Ebner, Christoph and Mori, Shohei and Zollmann, Stefanie and Mohr, Peter and Kalkofen, Denis},
  title = {Neural Cameras: Learning Camera Characteristics for Coherent Mixed Reality Rendering},
  booktitle = {Proceedings of the 2021 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)},
  year = {2021},
  pages = {508--516},
  publisher = {IEEE},
  address = {Piscataway, NJ, USA},
  doi = {10.1109/ISMAR52148.2021.00068}
}


@article{zhang2019hierarchical,
  title={Hierarchical topic model based object association for semantic SLAM},
  author={Zhang, Jianhua and Gui, Mengping and Wang, Qichao and Liu, Ruyu and Xu, Junzhe and Chen, Shengyong},
  journal={IEEE transactions on visualization and computer graphics},
  volume={25},
  number={11},
  pages={3052--3062},
  year={2019},
  publisher={IEEE}
}
@inproceedings{kress2019optical,
  title={Optical waveguide combiners for AR headsets: features and limitations},
  author={Kress, Bernard C},
  booktitle={Digital Optical Technologies 2019},
  volume={11062},
  pages={75--100},
  year={2019},
  organization={SPIE}
}

@inproceedings{runz2018maskfusion,
  author = {Runz, Martin and Buffier, Maud and Agapito, Lourdes},
  title = {MaskFusion: Real-Time Recognition, Tracking and Reconstruction of Multiple Moving Objects},
  booktitle = {Proceedings of the 2018 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)},
  year = {2018},
  pages = {10--20},
  publisher = {IEEE},
  address = {Piscataway, NJ, USA},
  doi = {10.1109/ISMAR.2018.00019}
}


@article{rathinavel2019varifocal,
  title={Varifocal occlusion-capable optical see-through augmented reality display based on focus-tunable optics},
  author={Rathinavel, Kishore and Wetzstein, Gordon and Fuchs, Henry},
  journal={IEEE transactions on visualization and computer graphics},
  volume={25},
  number={11},
  pages={3125--3134},
  year={2019},
  publisher={IEEE}
}

@article{bang2021lenslet,
  title={Lenslet VR: thin, flat and wide-FOV virtual reality display using fresnel lens and lenslet array},
  author={Bang, Kiseung and Jo, Youngjin and Chae, Minseok and Lee, Byoungho},
  journal={IEEE Transactions on Visualization and Computer Graphics},
  volume={27},
  number={5},
  pages={2545--2554},
  year={2021},
  publisher={IEEE}
}

@article{hsieh2005three,
  title={Three approaches to qualitative content analysis},
  author={Hsieh, Hsiu-Fang and Shannon, Sarah E},
  journal={Qualitative health research},
  volume={15},
  number={9},
  pages={1277--1288},
  year={2005},
  publisher={Sage Publications Sage CA: Thousand Oaks, CA}
}

@inproceedings{davari2022validating,
  author = {Davari, Shakiba and Lu, Feiyu and Bowman, Doug A.},
  title = {Validating the Benefits of Glanceable and Context-Aware Augmented Reality for Everyday Information Access Tasks},
  booktitle = {2022 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)},
  year = {2022},
  pages = {436-444},
  publisher = {IEEE},
  address = {Piscataway, NJ, USA},
  doi = {10.1109/VR51125.2022.00063},
  keywords = {Human-computer interaction, Three-dimensional displays, Conferences, User experience, Task analysis, Augmented reality, Smart phones, Human-centered computing, Mixed/augmented reality, Interaction techniques, Empirical Studies in HCI}
}


@misc{GDPR2018,
  author       = {{European Commission}},
  title        = {EU Data Protection Rules},
  howpublished = {\url{https://commission.europa.eu/law/law-topic/data-protection/eu-data-protection-rules_en}},
  note         = {Accessed: 20 August 2024},
  year         = 2018,
  url          = {https://commission.europa.eu/law/law-topic/data-protection/eu-data-protection-rules_en}
}

@misc{FairUse2021,
  author       = {{YouTube}},
  title        = {Fair Use on YouTube},
  howpublished = {\url{https://support.google.com/youtube/answer/9783148?hl=en}},
  note         = {Accessed: 20 August 2024},
  year         = 2021,
  url          = {https://support.google.com/youtube/answer/9783148?hl=en}
}

@ARTICLE{langlotz2014browser,
  author={Langlotz, Tobias and Nguyen, Thanh and Schmalstieg, Dieter and Grasset, Raphael},
  journal={Proceedings of the IEEE}, 
  title={Next-Generation Augmented Reality Browsers: Rich, Seamless, and Adaptive}, 
  year={2014},
  volume={102},
  number={2},
  pages={155-169},
  keywords={Augmented reality;Cameras;Databases;Augmented reality;Mobile handsets;Mobile communication;Browsers;AR browser;augmented reality (AR);mobile devices},
  doi={10.1109/JPROC.2013.2294255}}

@inproceedings{denning2014situ,
author = {Denning, Tamara and Dehlawi, Zakariya and Kohno, Tadayoshi},
title = {In situ with bystanders of augmented reality glasses: perspectives on recording and privacy-mediating technologies},
year = {2014},
isbn = {9781450324731},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2556288.2557352},
doi = {10.1145/2556288.2557352},
abstract = {Augmented reality (AR) devices are poised to enter the market. It is unclear how the properties of these devices will affect individuals' privacy. In this study, we investigate the privacy perspectives of individuals when they are bystanders around AR devices. We conducted 12 field sessions in caf\'{e}s and interviewed 31 bystanders regarding their reactions to a co-located AR device. Participants were predominantly split between having indifferent and negative reactions to the device. Participants who expressed that AR devices change the bystander experience attributed this difference to subtleness, ease of recording, and the technology's lack of prevalence. Additionally, participants surfaced a variety of factors that make recording more or less acceptable, including what they are doing when the recording is being taken. Participants expressed interest in being asked permission before being recorded and in recording-blocking devices. We use the interview results to guide an exploration of design directions for privacy-mediating technologies.},
booktitle = {Proceedings of the SIGCHI Conference on Human Factors in Computing Systems},
pages = {2377–2386},
numpages = {10},
keywords = {wearable camera, surveillance, privacy, augmented reality},
location = {Toronto, Ontario, Canada},
series = {CHI '14}
}


@inproceedings{fender2022causality,
author = {Fender, Andreas Rene and Holz, Christian},
title = {Causality-preserving Asynchronous Reality},
year = {2022},
isbn = {9781450391573},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3491102.3501836},
doi = {10.1145/3491102.3501836},
abstract = {Mixed Reality is gaining interest as a platform for collaboration and focused work to a point where it may supersede current office settings in future workplaces. At the same time, we expect that interaction with physical objects and face-to-face communication will remain crucial for future work environments, which is a particular challenge in fully immersive Virtual Reality. In this work, we reconcile those requirements through a user’s individual Asynchronous Reality, which enables seamless physical interaction across time. When a user is unavailable, e.g., focused on a task or in a call, our approach captures co-located or remote physical events in real-time, constructs a causality graph of co-dependent events, and lets immersed users revisit them at a suitable time in a causally accurate way. Enabled by our system AsyncReality, we present a workplace scenario that includes walk-in interruptions during a person’s focused work, physical deliveries, and transient spoken messages. We then generalize our approach to a use-case agnostic concept and system architecture. We conclude by discussing the implications of Asynchronous Reality for future offices.},
booktitle = {Proceedings of the 2022 CHI Conference on Human Factors in Computing Systems},
articleno = {634},
numpages = {15},
keywords = {Asynchronous communication, Camera networks, Collaboration, Immersive workspaces, Interruption in workplaces, Mixed Reality},
location = {New Orleans, LA, USA},
series = {CHI '22}
}

@article{regenbrecht2024see,
  title={To See and be Seen—Perceived Ethics and Acceptability of Pervasive Augmented Reality},
  author={Regenbrecht, Holger and Knott, Alistair and Ferreira, Jennifer and Pantidi, Nadia},
  journal={IEEE Access},
  volume={12},
  pages={32618--32636},
  year={2024},
  publisher={IEEE}
}

@inproceedings{schroder2023collaborating,
author = {Schr\"{o}der, Jan-Henrik and Schacht, Daniel and Peper, Niklas and Hamurculu, Anita Marie and Jetter, Hans-Christian},
title = {Collaborating Across Realities: Analytical Lenses for Understanding Dyadic Collaboration in Transitional Interfaces},
year = {2023},
isbn = {9781450394215},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3544548.3580879},
doi = {10.1145/3544548.3580879},
abstract = {Transitional Interfaces are a yet underexplored, emerging class of cross-reality user interfaces that enable users to freely move along the reality-virtuality continuum during collaboration. To analyze and understand how such collaboration unfolds, we propose four analytical lenses derived from an exploratory study of transitional collaboration with 15 dyads. While solving a complex spatial optimization task, participants could freely switch between three contexts, each with different displays (desktop screens, tablet-based augmented reality, head-mounted virtual reality), input techniques (mouse, touch, handheld controllers), and visual representations (monoscopic and allocentric 2D/3D maps, stereoscopic egocentric views). Using the rich qualitative and quantitative data from our study, we evaluated participants’ perceptions of transitional collaboration and identified commonalities and differences between dyads. We then derived four lenses including metrics and visualizations to analyze key aspects of transitional collaboration: (1) place and distance, (2) temporal patterns, (3) group use of contexts, (4) individual use of contexts.},
booktitle = {Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems},
articleno = {97},
numpages = {16},
keywords = {analytical lenses, transitional collaboration, transitional interfaces, user study},
location = {Hamburg, Germany},
series = {CHI '23}
}

@misc{eff_augmented_privacy_2020,
  author       = {Katitza Rodriguez and Rory Mir},
  title        = {Augmented Reality Must Have Augmented Privacy},
  year         = {2020},
  month        = {October},
  howpublished = {\url{https://www.eff.org/deeplinks/2020/10/augmented-reality-must-have-augmented-privacy}},
  note         = {Accessed: 2024-09-13}
}

@inproceedings{privaceye2019,
author = {Steil, Julian and Koelle, Marion and Heuten, Wilko and Boll, Susanne and Bulling, Andreas},
title = {PrivacEye: privacy-preserving head-mounted eye tracking using egocentric scene image and eye movement features},
year = {2019},
isbn = {9781450367097},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3314111.3319913},
doi = {10.1145/3314111.3319913},
abstract = {Eyewear devices, such as augmented reality displays, increasingly integrate eye tracking, but the first-person camera required to map a user's gaze to the visual scene can pose a significant threat to user and bystander privacy. We present PrivacEye, a method to detect privacy-sensitive everyday situations and automatically enable and disable the eye tracker's first-person camera using a mechanical shutter. To close the shutter in privacy-sensitive situations, the method uses a deep representation of the first-person video combined with rich features that encode users' eye movements. To open the shutter without visual input, PrivacEye detects changes in users' eye movements alone to gauge changes in the "privacy level" of the current situation. We evaluate our method on a first-person video dataset recorded in daily life situations of 17 participants, annotated by themselves for privacy sensitivity, and show that our method is effective in preserving privacy in this challenging setting.},
booktitle = {Proceedings of the 11th ACM Symposium on Eye Tracking Research \& Applications},
articleno = {26},
numpages = {10},
keywords = {privacy protection, gaze behaviour, egocentric vision},
location = {Denver, Colorado},
series = {ETRA '19}
}

@inproceedings{koelle2018led,
author = {Koelle, Marion and Wolf, Katrin and Boll, Susanne},
title = {Beyond LED Status Lights - Design Requirements of Privacy Notices for Body-worn Cameras},
year = {2018},
isbn = {9781450355681},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3173225.3173234},
doi = {10.1145/3173225.3173234},
abstract = {Privacy notices aim to make users aware of personal data gathered and processed by a system. Body-worn cameras currently lack suitable design strategies for privacy notices that announce themselves and their actions tosecondary andincidental users, such as bystanders, when they are being used in public. Hypothesizing that the commonly used status LED is not optimal for this use case, due to being not sufficiently understandable, noticeable, secure and trustworthy, we explore design requirements of privacy notices for body-worn cameras. Following a two-step approach, we contribute incentives for design alternatives to status LEDs: Starting from 8 design sessions with experts, we discuss 8 physical design artifacts, as well as design strategies and key motives. Finally, we derive design recommendations of the proposed solutions, which we back based on an evaluation with 12 UX \& HCI experts.},
booktitle = {Proceedings of the Twelfth International Conference on Tangible, Embedded, and Embodied Interaction},
pages = {177–187},
numpages = {11},
keywords = {augmented reality, lifelogging, privacy, wearable cameras},
location = {Stockholm, Sweden},
series = {TEI '18}
}


@misc{cnn_negative_reactions_2013,
  author       = {Heather Kelly},
  title        = {Negative Google Glass Reactions: What You're Really Thinking About Glass Wearers},
  year         = {2013},
  month        = {December},
  howpublished = {\url{https://edition.cnn.com/2013/12/10/tech/mobile/negative-google-glass-reactions/index.html}},
  note         = {Accessed: 2024-09-13}
}

@inproceedings{eghtebas2021advantage,
author = {Eghtebas, Chloe and Kiss, Francisco and Koelle, Marion and Wo\'{z}niak, Pawe\l{}},
title = {Advantage and Misuse of Vision Augmentation – Exploring User Perceptions and Attitudes using a Zoom Prototype},
year = {2021},
isbn = {9781450384285},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3458709.3458984},
doi = {10.1145/3458709.3458984},
abstract = {Consequences that deter adoption, such as asymmetrical encounters between wearers and bystanders, need to be explored in order to make Ubiquitous Augmented Reality (UAR) acceptable. In our work we outline how social perception is based on a Head Mounted Displays (HMD) capability, appearance, and the role of the wearer. We fixed the device capability to zooming in AR and explored the privacy implications in 12 interviews through a prototype with the mocked ability to ”super humanly” zoom in on targets. Next, we used the resulting themes to survey 100 participants to deeper explore augmented zoom while we permutate on the device appearance housed in three form-factors: contact lenses, glasses, and helmet and role of wearer based on level of involvement in an abstracted scenario transpiring in a public space. Our results showed that explicit visibility of an AR system provides social translucence as it is rated least likely to cause misuse but also perceived as least likely to have an advantage.},
booktitle = {Proceedings of the Augmented Humans International Conference 2021},
pages = {77–85},
numpages = {9},
keywords = {Zoom Interaction, Privacy Implications, Mixed Method, Augmented Reality},
location = {Rovaniemi, Finland},
series = {AHs '21}
}

@misc{dayahead2024,
  title = {Day Ahead},
  author = {{Day Ahead}},
  year = {2024},
  howpublished = {\url{https://apps.apple.com/us/app/day-ahead/id6476853500}},
  note = {Accessed: 2024-09-15}
}

@misc{tiktok2024,
  title = {TikTok},
  author = {{TikTok}},
  year = {2024},
  howpublished = {\url{https://apps.apple.com/us/app/tiktok/id835599320}},
  note = {Accessed: 2024-09-15}
}

@inproceedings{eghtebas2023co,
author = {Eghtebas, Chloe and Klinker, Gudrun and Boll, Susanne and Koelle, Marion},
title = {Co-Speculating on Dark Scenarios and Unintended Consequences of a Ubiquitous(ly) Augmented Reality},
year = {2023},
isbn = {9781450398930},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3563657.3596073},
doi = {10.1145/3563657.3596073},
abstract = {The vision of a ‘metaverse’ may soon bring a ubiquitous(ly) Augmented Reality (UAR) delivering context-aware, geo-located, and continuous blends of real and virtual elements into reach. This paper draws on speculative design to explore, question, and problematize consequences of AR becoming pervasive. Elaborating on Desjardin et al.’s bespoke booklets, we co-speculate together with 12 globally dispersed participants. Each participant received a custom-made design workbook containing pictures of their immediate surroundings, which they elaborated on in situated brainstorming activities. We present an integration of their speculative ideas and lived experiences in 3 overarching themes from which 7 ‘dark’ scenarios caused by UAR were formed. The Scenarios are indicative of deceptive design patterns that can (and likely will be) devised to misuse UAR, and anti-patterns that could cause unintended consequences. These contributions enable the timely discussion of potential antidotes and to which extent they can mitigate imminent harms of UAR.},
booktitle = {Proceedings of the 2023 ACM Designing Interactive Systems Conference},
pages = {2392–2407},
numpages = {16},
keywords = {Dark patterns, anti-patterns, future technologies, speculative design},
location = {Pittsburgh, PA, USA},
series = {DIS '23}
}

@incollection{azuma2015location,
  author = {Azuma, Ronald},
  title = {Location-Based Mixed and Augmented Reality Storytelling},
  booktitle = {Fundamentals of Wearable Computers and Augmented Reality},
  editor = {Barfield, Woodrow},
  publisher = {CRC Press},
  address = {Boca Raton, FL, USA},  
  year = {2015},
  edition = {2},
  chapter = {11},
  pages = {259--275}, 
  doi = {10.1201/b18703}
}

@article{pavanatto2024multiple,
  author = {Pavanatto, Leonardo and Lu, Feiyu and North, Chris and Bowman, Doug A.},
  title = {Multiple Monitors or Single Canvas? Evaluating Window Management and Layout Strategies on Virtual Displays},
  journal = {IEEE Transactions on Visualization and Computer Graphics},
  year = {2025},
  volume = {31},
  number = {3},
  pages = {1713--1730},
  publisher = {IEEE},
  address = {Piscataway, NJ, USA},
  doi = {10.1109/TVCG.2024.3368930},
  keywords = {Monitoring, Task analysis, Operating systems, Visualization, Layout, User experience, Two-dimensional displays, Augmented reality, Canvas, Layout, Multiple monitors, Virtual displays, Window management}
}


@misc{lv2024aria,
      title={Aria Everyday Activities Dataset}, 
      author={Zhaoyang Lv and Nicholas Charron and Pierre Moulon and Alexander Gamino and Cheng Peng and Chris Sweeney and Edward Miller and Huixuan Tang and Jeff Meissner and Jing Dong and Kiran Somasundaram and Luis Pesqueira and Mark Schwesinger and Omkar Parkhi and Qiao Gu and Renzo De Nardi and Shangyi Cheng and Steve Saarinen and Vijay Baiyya and Yuyang Zou and Richard Newcombe and Jakob Julian Engel and Xiaqing Pan and Carl Ren},
      year={2024},
      eprint={2402.13349},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2402.13349}, 
}

@misc{xraispotlight2024,
  author       = {XRAI Spotlight},
  title        = {Vision Pro Usage Survey: Daily Essential or Future Artifact?},
  year         = {2024},
  url          = {https://xraispotlight.substack.com/p/vision-pro-usage-survey-daily-essential},
  note         = {Published: May 31, 2024. Accessed: 2024-09-07}
}

@misc{krikorian2024,
  author       = {Tom Krikorian},
  title        = {Vision Pro AR/VR Discussion},
  year         = {2024},
  url          = {https://www.linkedin.com/posts/tomkrikorian_visionpro-ar-vr-activity-7237095705612316673-zQHS/},
  note         = {Accessed: 2024-09-07}
}
