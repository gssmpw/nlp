@misc{FairUse2021,
  author       = {{YouTube}},
  title        = {Fair Use on YouTube},
  howpublished = {\url{https://support.google.com/youtube/answer/9783148?hl=en}},
  note         = {Accessed: 20 August 2024},
  year         = 2021,
  url          = {https://support.google.com/youtube/answer/9783148?hl=en}
}

@misc{GDPR2018,
  author       = {{European Commission}},
  title        = {EU Data Protection Rules},
  howpublished = {\url{https://commission.europa.eu/law/law-topic/data-protection/eu-data-protection-rules_en}},
  note         = {Accessed: 20 August 2024},
  year         = 2018,
  url          = {https://commission.europa.eu/law/law-topic/data-protection/eu-data-protection-rules_en}
}

@incollection{azuma2015location,
  author = {Azuma, Ronald},
  title = {Location-Based Mixed and Augmented Reality Storytelling},
  booktitle = {Fundamentals of Wearable Computers and Augmented Reality},
  editor = {Barfield, Woodrow},
  publisher = {CRC Press},
  address = {Boca Raton, FL, USA},  
  year = {2015},
  edition = {2},
  chapter = {11},
  pages = {259--275}, 
  doi = {10.1201/b18703}
}

@article{azuma2019road,
  title={The road to ubiquitous consumer augmented reality systems},
  author={Azuma, Ronald T},
  journal={Human Behavior and Emerging Technologies},
  volume={1},
  number={1},
  pages={26--32},
  year={2019},
  publisher={Wiley Online Library}
}

@article{baashar2023towards,
  title={Towards wearable augmented reality in healthcare: a comparative survey and analysis of head-mounted displays},
  author={Baashar, Yahia and Alkawsi, Gamal and Wan Ahmad, Wan Nooraishya and Alomari, Mohammad Ahmed and Alhussian, Hitham and Tiong, Sieh Kiong},
  journal={International journal of environmental research and public health},
  volume={20},
  number={5},
  pages={3940},
  year={2023},
  publisher={MDPI}
}

@article{bang2021lenslet,
  title={Lenslet VR: thin, flat and wide-FOV virtual reality display using fresnel lens and lenslet array},
  author={Bang, Kiseung and Jo, Youngjin and Chae, Minseok and Lee, Byoungho},
  journal={IEEE Transactions on Visualization and Computer Graphics},
  volume={27},
  number={5},
  pages={2545--2554},
  year={2021},
  publisher={IEEE}
}

@article{billinghurst2021grand,
  author = {Billinghurst, Mark},
  title = {Grand Challenges for Augmented Reality},
  journal = {Frontiers in Virtual Reality},
  volume = {2},
  year = {2021},
  pages = {1--4},
  publisher = {Frontiers},
  doi = {10.3389/frvir.2021.578080},
  url = {https://www.frontiersin.org/articles/10.3389/frvir.2021.578080},
  issn = {2673-4192}
}

@inproceedings{blythe2009critical,
author = {Blythe, Mark and Cairns, Paul},
title = {Critical methods and user generated content: the iPhone on YouTube},
year = {2009},
isbn = {9781605582467},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1518701.1518923},
doi = {10.1145/1518701.1518923},
abstract = {Sites like YouTube offer vast sources of data for studies of human computer interaction. However, they also present a number of methodological challenges. This paper offers an example study of the initial reception of the iPhone 3G through YouTube. It begins with a quantitative account of the overall shape of the most frequently viewed returns for an iPhone 3G" search. A content analysis of the first hundred videos then explores the returns categorized by genre. Comments on the most popular video "Will It Blend"are analysed using grounded theory. It is argued that social science methods are not sufficient for a rich understanding of such material. The paper concludes with an analysis of"Will it Blend"that draws on cultural and critical theory. It is argued that a multi-methodological approach is necessary to exploit such data and also to address the challenges of next generation Human Computer Interaction (HCI).},
booktitle = {Proceedings of the SIGCHI Conference on Human Factors in Computing Systems},
pages = {1467–1476},
numpages = {10},
keywords = {user generated content, user experience, iPhone, green HCI, critical theory, YouTube},
location = {Boston, MA, USA},
series = {CHI '09}
}

@book{bowman20043d,
author = {Bowman, Doug A. and Kruijff, Ernst and LaViola, Joseph J. and Poupyrev, Ivan},
title = {3D User Interfaces: Theory and Practice},
year = {2004},
isbn = {0201758679},
publisher = {Addison Wesley Longman Publishing Co., Inc.},
address = {USA}
}

@inproceedings{bowman2021keynote,
  author = {Bowman, Douglas},
  title = {Keynote Speaker: User Experience Considerations for Everyday Augmented Reality},
  booktitle = {2021 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)},
  year = {2021},
  pages = {16-16},
  doi = {10.1109/ISMAR52148.2021.00010},
  publisher = {IEEE},
  address = {Piscataway, NJ, USA},  
  keywords = {Three-dimensional displays, User experience, Smart phones, Human-computer interaction, Computer science, Augmented reality, Visualization}
}

@inproceedings{burova2020utilizing,
author = {Burova, Alisa and M\"{a}kel\"{a}, John and Hakulinen, Jaakko and Keskinen, Tuuli and Heinonen, Hanna and Siltanen, Sanni and Turunen, Markku},
title = {Utilizing VR and Gaze Tracking to Develop AR Solutions for Industrial Maintenance},
year = {2020},
isbn = {9781450367080},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3313831.3376405},
doi = {10.1145/3313831.3376405},
abstract = {Augmented reality (AR) presents a variety of possibilities for industrial maintenance. However, the development of real-world AR solutions has been limited due to the technological capabilities and uncertainty with respect to safety at deployment. We introduce the approach of using AR simulation in virtual reality (VR) coupled with gaze tracking to enable resource-efficient AR development. We tested in-field AR guidance and safety awareness features in an iterative development-evaluation process with experts from the elevator maintenance industry. We further conducted a survey, utilizing actual gaze data from the evaluation to elicit comments from industry experts on the usefulness of AR simulation and gaze tracking. Our results show the potential of AR within VR approach combined with gaze tracking. With this framework, AR solutions can be iteratively and safely tested without actual implementation, while gaze data provide advanced objective means to evaluate the designed AR content, documentation usage, and safety awareness.},
booktitle = {Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems},
pages = {1–13},
numpages = {13},
keywords = {gaze tracking, industrial maintenance, safety, virtual prototyping, augmented reality, virtual reality},
location = {Honolulu, HI, USA},
series = {CHI '20}
}

@inproceedings{cao2020exploratory,
author = {Cao, Yuanzhi and Qian, Xun and Wang, Tianyi and Lee, Rachel and Huo, Ke and Ramani, Karthik},
title = {An Exploratory Study of Augmented Reality Presence for Tutoring Machine Tasks},
year = {2020},
isbn = {9781450367080},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3313831.3376688},
doi = {10.1145/3313831.3376688},
abstract = {Machine tasks in workshops or factories are often a compound sequence of local, spatial, and body-coordinated human-machine interactions. Prior works have shown the merits of video-based and augmented reality (AR) tutoring systems for local tasks. However, due to the lack of a bodily representation of the tutor, they are not as effective for spatial and body-coordinated interactions. We propose avatars as an additional tutor representation to the existing AR instructions. In order to understand the design space of tutoring presence for machine tasks, we conduct a comparative study with 32 users. We aim to explore the strengths/limitations of the following four tutor options: video, non-avatar-AR, half-body+AR, and full-body+AR. The results show that users prefer the half-body+AR overall, especially for the spatial interactions. They have a preference for the full-body+AR for the body-coordinated interactions and the non-avatar-AR for the local interactions. We further discuss and summarize design recommendations and insights for future machine task tutoring systems.},
booktitle = {Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems},
pages = {1–13},
numpages = {13},
keywords = {augmented reality, avatar tutor, exploratory study, machine task, tutoring system design},
location = {Honolulu, HI, USA},
series = {CHI '20}
}

@book{card2018psychology,
  author = {Card, Stuart K.},
  title = {The Psychology of Human-Computer Interaction},
  publisher = {CRC Press},
  address = {Boca Raton, FL, USA},
  year = {2018},
  edition = {1},
  doi = {10.1201/9780203736166},
  pages = {488},
  isbn = {9780203736166}
}

@inproceedings{denning2014situ,
author = {Denning, Tamara and Dehlawi, Zakariya and Kohno, Tadayoshi},
title = {In situ with bystanders of augmented reality glasses: perspectives on recording and privacy-mediating technologies},
year = {2014},
isbn = {9781450324731},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2556288.2557352},
doi = {10.1145/2556288.2557352},
abstract = {Augmented reality (AR) devices are poised to enter the market. It is unclear how the properties of these devices will affect individuals' privacy. In this study, we investigate the privacy perspectives of individuals when they are bystanders around AR devices. We conducted 12 field sessions in caf\'{e}s and interviewed 31 bystanders regarding their reactions to a co-located AR device. Participants were predominantly split between having indifferent and negative reactions to the device. Participants who expressed that AR devices change the bystander experience attributed this difference to subtleness, ease of recording, and the technology's lack of prevalence. Additionally, participants surfaced a variety of factors that make recording more or less acceptable, including what they are doing when the recording is being taken. Participants expressed interest in being asked permission before being recorded and in recording-blocking devices. We use the interview results to guide an exploration of design directions for privacy-mediating technologies.},
booktitle = {Proceedings of the SIGCHI Conference on Human Factors in Computing Systems},
pages = {2377–2386},
numpages = {10},
keywords = {wearable camera, surveillance, privacy, augmented reality},
location = {Toronto, Ontario, Canada},
series = {CHI '14}
}

@inproceedings{eghtebas2021advantage,
author = {Eghtebas, Chloe and Kiss, Francisco and Koelle, Marion and Wo\'{z}niak, Pawe\l{}},
title = {Advantage and Misuse of Vision Augmentation – Exploring User Perceptions and Attitudes using a Zoom Prototype},
year = {2021},
isbn = {9781450384285},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3458709.3458984},
doi = {10.1145/3458709.3458984},
abstract = {Consequences that deter adoption, such as asymmetrical encounters between wearers and bystanders, need to be explored in order to make Ubiquitous Augmented Reality (UAR) acceptable. In our work we outline how social perception is based on a Head Mounted Displays (HMD) capability, appearance, and the role of the wearer. We fixed the device capability to zooming in AR and explored the privacy implications in 12 interviews through a prototype with the mocked ability to ”super humanly” zoom in on targets. Next, we used the resulting themes to survey 100 participants to deeper explore augmented zoom while we permutate on the device appearance housed in three form-factors: contact lenses, glasses, and helmet and role of wearer based on level of involvement in an abstracted scenario transpiring in a public space. Our results showed that explicit visibility of an AR system provides social translucence as it is rated least likely to cause misuse but also perceived as least likely to have an advantage.},
booktitle = {Proceedings of the Augmented Humans International Conference 2021},
pages = {77–85},
numpages = {9},
keywords = {Zoom Interaction, Privacy Implications, Mixed Method, Augmented Reality},
location = {Rovaniemi, Finland},
series = {AHs '21}
}

@inproceedings{eghtebas2023co,
author = {Eghtebas, Chloe and Klinker, Gudrun and Boll, Susanne and Koelle, Marion},
title = {Co-Speculating on Dark Scenarios and Unintended Consequences of a Ubiquitous(ly) Augmented Reality},
year = {2023},
isbn = {9781450398930},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3563657.3596073},
doi = {10.1145/3563657.3596073},
abstract = {The vision of a ‘metaverse’ may soon bring a ubiquitous(ly) Augmented Reality (UAR) delivering context-aware, geo-located, and continuous blends of real and virtual elements into reach. This paper draws on speculative design to explore, question, and problematize consequences of AR becoming pervasive. Elaborating on Desjardin et al.’s bespoke booklets, we co-speculate together with 12 globally dispersed participants. Each participant received a custom-made design workbook containing pictures of their immediate surroundings, which they elaborated on in situated brainstorming activities. We present an integration of their speculative ideas and lived experiences in 3 overarching themes from which 7 ‘dark’ scenarios caused by UAR were formed. The Scenarios are indicative of deceptive design patterns that can (and likely will be) devised to misuse UAR, and anti-patterns that could cause unintended consequences. These contributions enable the timely discussion of potential antidotes and to which extent they can mitigate imminent harms of UAR.},
booktitle = {Proceedings of the 2023 ACM Designing Interactive Systems Conference},
pages = {2392–2407},
numpages = {16},
keywords = {Dark patterns, anti-patterns, future technologies, speculative design},
location = {Pittsburgh, PA, USA},
series = {DIS '23}
}

@inproceedings{eom2022neurolens,
  author = {Eom, Sangjun and Sykes, David and Rahimpour, Shervin and Gorlatova, Maria},
  title = {NeuroLens: Augmented Reality-based Contextual Guidance through Surgical Tool Tracking in Neurosurgery},
  booktitle = {Proceedings of the 2022 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)},
  year = {2022},
  pages = {355--364},
  publisher = {IEEE},
  address = {Piscataway, NJ, USA},
  doi = {10.1109/ISMAR55827.2022.00051},
  keywords = {Training, Visualization, Target tracking, Head, Phantoms, Neurosurgery, Trajectory, Human-centered computing, Human-computer interaction (HCI), Interaction paradigms, Mixed / augmented reality, Interaction devices, Displays and imagers}
}

@inproceedings{gasques2021artemis,
author = {Gasques, Danilo and Johnson, Janet G. and Sharkey, Tommy and Feng, Yuanyuan and Wang, Ru and Xu, Zhuoqun Robin and Zavala, Enrique and Zhang, Yifei and Xie, Wanze and Zhang, Xinming and Davis, Konrad and Yip, Michael and Weibel, Nadir},
title = {ARTEMIS: A Collaborative Mixed-Reality System for Immersive Surgical Telementoring},
year = {2021},
isbn = {9781450380966},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3411764.3445576},
doi = {10.1145/3411764.3445576},
abstract = {Traumatic injuries require timely intervention, but medical expertise is not always available at the patient’s location. Despite recent advances in telecommunications, surgeons still have limited tools to remotely help inexperienced surgeons. Mixed Reality hints at a future where remote collaborators work side-by-side as if co-located; however, we still do not know how current technology can improve remote surgical collaboration. Through role-playing and iterative-prototyping, we identify collaboration practices used by expert surgeons to aid novice surgeons as well as technical requirements to facilitate these practices. We then introduce ARTEMIS, an AR-VR collaboration system that supports these key practices. Through an observational study with two expert surgeons and five novice surgeons operating on cadavers, we find that ARTEMIS supports remote surgical mentoring of novices through synchronous point, draw, and look affordances and asynchronous video clips. Most participants found that ARTEMIS facilitates collaboration despite existing technology limitations explored in this paper.},
booktitle = {Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems},
articleno = {662},
numpages = {14},
keywords = {Augmented Reality, Collaboration, Mixed Reality, Surgery, Telementoring, Virtual Reality},
location = {Yokohama, Japan},
series = {CHI '21}
}

@article{gattullo2020and,
  title={What, how, and why are visual assets used in industrial augmented reality? A systematic review and classification in maintenance, assembly, and training (from 1997 to 2019)},
  author={Gattullo, Michele and Evangelista, Alessandro and Uva, Antonio E and Fiorentino, Michele and Gabbard, Joseph L},
  journal={IEEE Trans. Vis. Comput. Graph.},
  volume={28},
  number={2},
  pages={1443--1456},
  year={2020},
  publisher={IEEE}
}

@article{glassner2003everyday,
  title={Everyday computer graphics},
  author={Glassner, Andrew},
  journal={IEEE Computer Graphics and Applications},
  volume={23},
  number={6},
  pages={76--82},
  year={2003},
  publisher={IEEE}
}

@article{grubert2016towards,
  title={Towards pervasive augmented reality: Context-awareness in augmented reality},
  author={Grubert, Jens and Langlotz, Tobias and Zollmann, Stefanie and Regenbrecht, Holger},
  journal={IEEE transactions on visualization and computer graphics},
  volume={23},
  number={6},
  pages={1706--1724},
  year={2016},
  publisher={IEEE}
}

@article{hamasaki2019varifocal,
  title={Varifocal occlusion for optical see-through head-mounted displays using a slide occlusion mask},
  author={Hamasaki, Takumi and Itoh, Yuta},
  journal={IEEE transactions on visualization and computer graphics},
  volume={25},
  number={5},
  pages={1961--1969},
  year={2019},
  publisher={IEEE}
}

@article{henderson2010exploring,
  title={Exploring the benefits of augmented reality documentation for maintenance and repair},
  author={Henderson, Steven and Feiner, Steven},
  journal={IEEE transactions on visualization and computer graphics},
  volume={17},
  number={10},
  pages={1355--1368},
  year={2010},
  publisher={Ieee}
}

@inproceedings{hirzle2019design,
author = {Hirzle, Teresa and Gugenheimer, Jan and Geiselhart, Florian and Bulling, Andreas and Rukzio, Enrico},
title = {A Design Space for Gaze Interaction on Head-mounted Displays},
year = {2019},
isbn = {9781450359702},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3290605.3300855},
doi = {10.1145/3290605.3300855},
abstract = {Augmented and virtual reality (AR/VR) has entered the mass market and, with it, will soon eye tracking as a core technology for next generation head-mounted displays (HMDs). In contrast to existing gaze interfaces, the 3D nature of AR and VR requires estimating a user's gaze in 3D. While first applications, such as foveated rendering, hint at the compelling potential of combining HMDs and gaze, a systematic analysis is missing. To fill this gap, we present the first design space for gaze interaction on HMDs. Our design space covers human depth perception and technical requirements in two dimensions aiming to identify challenges and opportunities for interaction design. As such, our design space provides a comprehensive overview and serves as an important guideline for researchers and practitioners working on gaze interaction on HMDs. We further demonstrate how our design space is used in practice by presenting two interactive applications: EyeHealth and XRay-Vision.},
booktitle = {Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems},
pages = {1–12},
numpages = {12},
keywords = {3d gaze, augmented reality, design space, gaze interaction, head-mounted displays, interaction design, virtual reality},
location = {Glasgow, Scotland Uk},
series = {CHI '19}
}

@inproceedings{komkaite2019underneath,
author = {Komkaite, Aida and Lavrinovica, Liga and Vraka, Maria and Skov, Mikael B.},
title = {Underneath the Skin: An Analysis of YouTube Videos to Understand Insertable Device Interaction},
year = {2019},
isbn = {9781450359702},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3290605.3300444},
doi = {10.1145/3290605.3300444},
abstract = {During the last decade, people have started to experiment with insertable technology like RFID or NFC chips and use them for e.g. identification. However, little is known about how people in fact interact with and adapt insertables. We conducted a video analysis of 122 YouTube videos to gain insight into the interaction with the insertables. Second, we implemented an online survey to complement our data from the video analysis. Our findings show that there are many opportunities for interaction with insertables both for task-oriented and creative purposes. However, there are also multiple challenges and obstacles as well as side effects and health concerns. Our findings conclude that the current infrastructure is not ready to support the use of insertables yet, and we discuss implications of this.},
booktitle = {Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems},
pages = {1–12},
numpages = {12},
keywords = {hobbyist, in the body, insertables, interaction, nfc, rfid, survey, youtube},
location = {Glasgow, Scotland Uk},
series = {CHI '19}
}

@inproceedings{lopes2018adding,
author = {Lopes, Pedro and You, Sijing and Ion, Alexandra and Baudisch, Patrick},
title = {Adding Force Feedback to Mixed Reality Experiences and Games using Electrical Muscle Stimulation},
year = {2018},
isbn = {9781450356206},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3173574.3174020},
doi = {10.1145/3173574.3174020},
abstract = {We present a mobile system that enhances mixed reality experiences and games with force feedback by means of electrical muscle stimulation (EMS). The benefit of our approach is that it adds physical forces while keeping the users' hands free to interact unencumbered-not only with virtual objects, but also with physical objects, such as props and appliances. We demonstrate how this supports three classes of applications along the mixed-reality continuum: (1) entirely virtual objects, such as furniture with EMS friction when pushed or an EMS-based catapult game. (2) Virtual objects augmented via passive props with EMS-constraints, such as a light control panel made tangible by means of a physical cup or a balance-the-marble game with an actuated tray. (3) Augmented appliances with virtual behaviors, such as a physical thermostat dial with EMS-detents or an escape-room that repurposes lamps as levers with detents. We present a user-study in which participants rated the EMS-feedback as significantly more realistic than a no-EMS baseline.},
booktitle = {Proceedings of the 2018 CHI Conference on Human Factors in Computing Systems},
pages = {1–13},
numpages = {13},
keywords = {ar, augmented reality, body i/o, electrical muscle stimulation, haptics, hololens, mixed reality, mr, wearable},
location = {Montreal QC, Canada},
series = {CHI '18}
}

@inproceedings{lu2020glanceable,
  author = {Lu, Feiyu and Davari, Shakiba and Lisle, Lee and Li, Yuan and Bowman, Doug A.},
  title = {Glanceable AR: Evaluating Information Access Methods for Head-Worn Augmented Reality},
  booktitle = {Proceedings of the 2020 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)},
  year = {2020},
  pages = {930--939},
  publisher = {IEEE},
  address = {Piscataway, NJ, USA},
  doi = {10.1109/VR46266.2020.00113},
  keywords = {Augmented reality, User interfaces, Human computer interaction, Head-mounted displays, Human-centered computing, Mixed / augmented reality, User interface design}
}

@inproceedings{lu2021evaluating,
  author = {Lu, Feiyu and Bowman, Doug A.},
  title = {Evaluating the Potential of Glanceable AR Interfaces for Authentic Everyday Uses},
  booktitle = {Proceedings of the 2021 IEEE Virtual Reality and 3D User Interfaces (VR) Conference},
  year = {2021},
  pages = {768--777},
  publisher = {IEEE},
  address = {Piscataway, NJ, USA},
  doi = {10.1109/VR50410.2021.00104},
  keywords = {Headphones, Three-dimensional displays, Head-mounted displays, Prototypes, Glass, User interfaces, Task analysis, Human-centered computing-Mixed / augmented reality, Human-centered computing-User interface design}
}

@inproceedings{lu2022exploring,
author = {Lu, Feiyu and Xu, Yan},
title = {Exploring Spatial UI Transition Mechanisms with Head-Worn Augmented Reality},
year = {2022},
isbn = {9781450391573},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3491102.3517723},
doi = {10.1145/3491102.3517723},
abstract = {Imagine in the future people comfortably wear augmented reality (AR) displays all day, how do we design interfaces that adapt to the contextual changes as people move around? In current operating systems, the majority of AR content defaults to staying at a fixed location until being manually moved by the users. However, this approach puts the burden of user interface (UI) transition solely on users. In this paper, we first ran a bodystorming design workshop to capture the limitations of existing manual UI transition approaches in spatially diverse tasks. Then we addressed these limitations by designing and evaluating three UI transition mechanisms with different levels of automation and controllability (low-effort manual, semi-automated, fully-automated). Furthermore, we simulated imperfect contextual awareness by introducing prediction errors with different costs to correct them. Our results provide valuable lessons about the trade-offs between UI automation levels, controllability, user agency, and the impact of prediction errors.},
booktitle = {Proceedings of the 2022 CHI Conference on Human Factors in Computing Systems},
articleno = {550},
numpages = {16},
keywords = {controllability, automation, agency, adaptive interfaces},
location = {New Orleans, LA, USA},
series = {CHI '22}
}

@misc{lv2024aria,
      title={Aria Everyday Activities Dataset}, 
      author={Zhaoyang Lv and Nicholas Charron and Pierre Moulon and Alexander Gamino and Cheng Peng and Chris Sweeney and Edward Miller and Huixuan Tang and Jeff Meissner and Jing Dong and Kiran Somasundaram and Luis Pesqueira and Mark Schwesinger and Omkar Parkhi and Qiao Gu and Renzo De Nardi and Shangyi Cheng and Steve Saarinen and Vijay Baiyya and Yuyang Zou and Richard Newcombe and Jakob Julian Engel and Xiaqing Pan and Carl Ren},
      year={2024},
      eprint={2402.13349},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2402.13349}, 
}

@inproceedings{mandl2021neural,
  author = {Mandl, David and Roth, Peter M. and Langlotz, Tobias and Ebner, Christoph and Mori, Shohei and Zollmann, Stefanie and Mohr, Peter and Kalkofen, Denis},
  title = {Neural Cameras: Learning Camera Characteristics for Coherent Mixed Reality Rendering},
  booktitle = {Proceedings of the 2021 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)},
  year = {2021},
  pages = {508--516},
  publisher = {IEEE},
  address = {Piscataway, NJ, USA},
  doi = {10.1109/ISMAR52148.2021.00068}
}

@article{masten2003digital,
  title={Digital ethnography: The next wave in understanding the consumer experience},
  author={Masten, Davis L and Plowman, Tim MP},
  journal={Design Management Journal (Former Series)},
  volume={14},
  number={2},
  pages={75--81},
  year={2003},
  publisher={Blackwell Publishing Ltd Oxford, UK}
}

@inproceedings{mathis2024everyday,
author = {Mathis, Florian},
title = {Everyday Life Challenges and Augmented Realities: Exploring Use Cases For, and User Perspectives on, an Augmented Everyday Life},
year = {2024},
isbn = {9798400709807},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3652920.3652921},
doi = {10.1145/3652920.3652921},
abstract = {Contextually aware visual, auditory, and haptic interfaces can augment and empower people in their everyday life. However, little is known about the use cases and users’ perspectives on a pervasive augmented reality that augments places and humans. In this paper, we contribute a first step towards an augmented societal future by outlining promising example use cases for assistive mixed reality interfaces (i.e., AssistiveMR). By surveying 60 participants, we found that an augmented reality has the potential to find wide-spread application in a plethora of scenarios, including supporting people in recalling information, disconnecting from reality, and augmenting communication with others using visual augmentations. However, participants expressed concerns regarding the potential high costs associated with errors and raised questions about the social acceptability of augmentations of humans and real-world surroundings. Our exploration of promising use cases for assistive MR augmentations aims at serving as inspiration, motivating researchers to augment, support, and empower individuals in their daily activities.},
booktitle = {Proceedings of the Augmented Humans International Conference 2024},
pages = {52–62},
numpages = {11},
keywords = {Assistive Mixed Reality, Augmenting Humans, Everyday AR},
location = {Melbourne, VIC, Australia},
series = {AHs '24}
}

@article{milgram1994taxonomy,
  title={A taxonomy of mixed reality visual displays},
  author={Milgram, Paul and Kishino, Fumio},
  journal={IEICE TRANSACTIONS on Information and Systems},
  volume={77},
  number={12},
  pages={1321--1329},
  year={1994},
  publisher={The Institute of Electronics, Information and Communication Engineers}
}

@article{nielsen2023using,
author = {Nielsen, Sara and Skov, Mikael B. and Hansen, Karl Damkj\ae{}r and Kaszowska, Aleksandra},
title = {Using User-Generated YouTube Videos to Understand Unguided Interactions with Robots in Public Places},
year = {2023},
issue_date = {March 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {12},
number = {1},
url = {https://doi.org/10.1145/3550280},
doi = {10.1145/3550280},
abstract = {Professional service robots are increasingly being deployed in public places, which thus increases user exposure. However, we lack an empirical understanding of complex encounters taking place in dynamic and often crowded environments as well as how people overcome breakdowns during unguided interaction with a robot in a real-world scenario. In this paper, we conducted a covert, digital ethnographic study analyzing 104 user-generated YouTube videos focusing on people’s unguided interactions with robots in several public places. We identified several types of interaction breakdowns pertaining to someone (person-initiated interaction breakdown, IB) or something (environmental disturbances, ED) having a direct, negative effect on an ongoing unguided interaction. Our findings have implications for the design and development of service robots facing multi-user scenarios entertaining active (primary and secondary) users, inactive (commentators and observers) ‘users’, and Incidentally Co-present Persons (InCoPs). Furthermore, we contribute to and built on the limited prior use of YouTube videos and digital ethnography in HRI research, thereby demonstrating its effectiveness in studying unguided interactions in public places, while supplementing and adding to the existing knowledge base of service robots in public places.},
journal = {J. Hum.-Robot Interact.},
month = {feb},
articleno = {5},
numpages = {40},
keywords = {Service robots, unguided interactions, interaction breakdowns, digital ethnography, robots in public places}
}

@article{o2023privacy,
  title={Privacy-enhancing technology and everyday augmented reality: Understanding bystanders' varying needs for awareness and consent},
  author={O'Hagan, Joseph and Saeghe, Pejman and Gugenheimer, Jan and Medeiros, Daniel and Marky, Karola and Khamis, Mohamed and McGill, Mark},
  journal={Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies},
  volume={6},
  number={4},
  pages={1--35},
  year={2023},
  publisher={ACM New York, NY, USA}
}

@inproceedings{paay2015connecting,
author = {Paay, Jeni and Kjeldskov, Jesper and Skov, Mikael B.},
title = {Connecting in the Kitchen: An Empirical Study of Physical Interactions while Cooking Together at Home},
year = {2015},
isbn = {9781450329224},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2675133.2675194},
doi = {10.1145/2675133.2675194},
abstract = {Recent research has explored the role technology might play in future kitchens, including virtually dining together, recipe sharing, augmented kitchen furniture, reactive cooking utensils and gestural interaction. When people come together in a kitchen to cook it is about more than just production of sustenance -- it is about being together, helping each other, exchanging stories, and contributing to the gradual emergence of a shared meal. In this paper we present a digital ethnography of how people coordinate and cooperate in their kitchens when cooking together for the purpose of inspiring the design of social natural user interactions for technologies in the kitchen. The study is based on 61 YouTube videos of people cooking together analyzed using the frameworks of proxemics and F-formations. Our findings unfold and illustrate relationships between people's spatial organization, their cooking activities and physical kitchen layouts. Based on these we discuss the kitchen as a design space and particularly the opportunities for social natural user interaction design.},
booktitle = {Proceedings of the 18th ACM Conference on Computer Supported Cooperative Work \& Social Computing},
pages = {276–287},
numpages = {12},
keywords = {cooking together, digital ethnography, digital kitchens, f-formations, natural user interaction, proxemics, the home},
location = {Vancouver, BC, Canada},
series = {CSCW '15}
}

@inproceedings{pavanatto2021we,
  author = {Pavanatto, Leonardo and North, Chris and Bowman, Doug A. and Badea, Carmen and Stoakley, Richard},
  title = {Do We Still Need Physical Monitors? An Evaluation of the Usability of AR Virtual Monitors for Productivity Work},
  booktitle = {Proceedings of the 2021 IEEE Virtual Reality and 3D User Interfaces (VR) Conference},
  year = {2021},
  pages = {759--767},
  publisher = {IEEE},
  address = {Piscataway, NJ, USA},
  doi = {10.1109/VR50410.2021.00103},
  keywords = {Productivity, Headphones, Three-dimensional displays, User interfaces, Usability, Monitoring, Augmented reality, Human-centered computing, Human-computer interaction (HCI), Interaction paradigms, Mixed / augmented reality, Empirical studies in interaction design}
}

@article{pavanatto2024multiple,
  author = {Pavanatto, Leonardo and Lu, Feiyu and North, Chris and Bowman, Doug A.},
  title = {Multiple Monitors or Single Canvas? Evaluating Window Management and Layout Strategies on Virtual Displays},
  journal = {IEEE Transactions on Visualization and Computer Graphics},
  year = {2025},
  volume = {31},
  number = {3},
  pages = {1713--1730},
  publisher = {IEEE},
  address = {Piscataway, NJ, USA},
  doi = {10.1109/TVCG.2024.3368930},
  keywords = {Monitoring, Task analysis, Operating systems, Visualization, Layout, User experience, Two-dimensional displays, Augmented reality, Canvas, Layout, Multiple monitors, Virtual displays, Window management}
}

@inproceedings{pei2022hand,
author = {Pei, Siyou and Chen, Alexander and Lee, Jaewook and Zhang, Yang},
title = {Hand Interfaces: Using Hands to Imitate Objects in AR/VR for Expressive Interactions},
year = {2022},
isbn = {9781450391573},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3491102.3501898},
doi = {10.1145/3491102.3501898},
abstract = {Augmented reality (AR) and virtual reality (VR) technologies create exciting new opportunities for people to interact with computing resources and information. Less exciting is the need for holding hand controllers, which limits applications that demand expressive, readily available interactions. Prior research investigated freehand AR/VR input by transforming the user’s body into an interaction medium. In contrast to previous work that has users’ hands grasp virtual objects, we propose a new interaction technique that lets users’ hands become virtual objects by imitating the objects themselves. For example, a thumbs-up hand pose is used to mimic a joystick. We created a wide array of interaction designs around this idea to demonstrate its applicability in object retrieval and interactive control tasks. Collectively, we call these interaction designs Hand Interfaces. From a series of user studies comparing Hand Interfaces against various baseline techniques, we collected quantitative and qualitative feedback, which indicates that Hand Interfaces are effective, expressive, and fun to use.},
booktitle = {Proceedings of the 2022 CHI Conference on Human Factors in Computing Systems},
articleno = {429},
numpages = {16},
keywords = {AR/VR, Embodiment, Free-hand interactions, Imitation, Interaction design, On-body interactions},
location = {New Orleans, LA, USA},
series = {CHI '22}
}

@inproceedings{plabst2022push,
author = {Plabst, Lucas and Oberd\"{o}rfer, Sebastian and Ortega, Francisco Raul and Niebling, Florian},
title = {Push the Red Button: Comparing Notification Placement with Augmented and Non-Augmented Tasks in AR},
year = {2022},
isbn = {9781450399487},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3565970.3567701},
doi = {10.1145/3565970.3567701},
abstract = {Visual notifications are omnipresent in applications ranging from smart phones to Virtual Reality (VR) and Augmented Reality (AR) systems. They are especially useful in applications where users performing a primary task have to be interrupted to react to external events. However, these notifications can cause disruptive effects on the performance of users concerning their currently executed primary task. Also, different notification placements have been shown to have an influence on response times, as well as e.g.&nbsp;on user perceived intrusiveness and disruptiveness. We investigated the effects and impacts of four visual notification types in AR environments when the main task was performed (1) in AR and (2) the real world. We used subtitle, heads-up, world space, and user wrist as notification types. In a user study, we interrupted the execution of the main task with one of the AR notification types. When noticing a notification, users responded to it by completing a secondary task. We used a Memory card game as the main task and the pressing of a correctly colored button as the secondary task. Our findings suggest that notifications at a user’s wrist are most suitable when other AR elements are present. Notifications displayed in the World are quick to notice and understand if the view direction of a user is known. Heads-up notifications in the corner of the field-of-view, as they are primarily used in smart glasses, performed significantly worse, especially compared to Subtitle placement. Hence, we recommend to use different notification types depending on the overall structure of an AR system.},
booktitle = {Proceedings of the 2022 ACM Symposium on Spatial User Interaction},
articleno = {17},
numpages = {11},
keywords = {attention, augmented reality, notifications},
location = {Online, CA, USA},
series = {SUI '22}
}

@article{rathinavel2019varifocal,
  title={Varifocal occlusion-capable optical see-through augmented reality display based on focus-tunable optics},
  author={Rathinavel, Kishore and Wetzstein, Gordon and Fuchs, Henry},
  journal={IEEE transactions on visualization and computer graphics},
  volume={25},
  number={11},
  pages={3125--3134},
  year={2019},
  publisher={IEEE}
}

@article{regenbrecht2024see,
  title={To See and be Seen—Perceived Ethics and Acceptability of Pervasive Augmented Reality},
  author={Regenbrecht, Holger and Knott, Alistair and Ferreira, Jennifer and Pantidi, Nadia},
  journal={IEEE Access},
  volume={12},
  pages={32618--32636},
  year={2024},
  publisher={IEEE}
}

@incollection{rogers2014diffusion,
  author = {Rogers, Everett M. and Singhal, Arvind and Quinlan, Margaret M.},
  title = {Diffusion of Innovations},
  booktitle = {An Integrated Approach to Communication Theory and Research},
  pages = {432--448},
  year = {2014},
  publisher = {Routledge},
  address = {New York, NY, USA}
}

@inproceedings{runz2018maskfusion,
  author = {Runz, Martin and Buffier, Maud and Agapito, Lourdes},
  title = {MaskFusion: Real-Time Recognition, Tracking and Reconstruction of Multiple Moving Objects},
  booktitle = {Proceedings of the 2018 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)},
  year = {2018},
  pages = {10--20},
  publisher = {IEEE},
  address = {Piscataway, NJ, USA},
  doi = {10.1109/ISMAR.2018.00019}
}

@inproceedings{schmitz2022squeezy,
author = {Schmitz, Martin and G\"{u}nther, Sebastian and Sch\"{o}n, Dominik and M\"{u}ller, Florian},
title = {Squeezy-Feely: Investigating Lateral Thumb-Index Pinching as an Input Modality},
year = {2022},
isbn = {9781450391573},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3491102.3501981},
doi = {10.1145/3491102.3501981},
abstract = {From zooming on smartphones and mid-air gestures to deformable user interfaces, thumb-index pinching grips are used in many interaction techniques. However, there is still a lack of systematic understanding of how the accuracy and efficiency of such grips are affected by various factors such as counterforce, grip span, and grip direction. Therefore, in this paper, we contribute an evaluation (N = 18) of thumb-index pinching performance in a visual targeting task using scales up to 75 items. As part of our findings, we conclude that the pinching interaction between the thumb and index finger is a promising modality also for one-dimensional input on higher scales. Furthermore, we discuss and outline implications for future user interfaces that benefit from pinching as an additional and complementary interaction modality.},
booktitle = {Proceedings of the 2022 CHI Conference on Human Factors in Computing Systems},
articleno = {61},
numpages = {15},
keywords = {Deformation, Input, Mixed Reality, Pinching, Thumb-to-finger, User Studies},
location = {New Orleans, LA, USA},
series = {CHI '22}
}

@ARTICLE{tran2023wearable,
  author={Tran, Tram Thi Minh and Brown, Shane and Weidlich, Oliver and Billinghurst, Mark and Parker, Callum},
  journal={IEEE Transactions on Visualization and Computer Graphics}, 
  title={Wearable Augmented Reality: Research Trends and Future Directions from Three Major Venues}, 
  year={2023},
  volume={29},
  number={11},
  pages={4782-4793},
  keywords={Market research;Headphones;Ethics;Augmented reality;Rendering (computer graphics);Calibration;Biomedical monitoring;Augmented reality;mixed reality;head-mounted displays;survey;trends},
  doi={10.1109/TVCG.2023.3320231}}

@inproceedings{wolf2018we,
  author = {Wolf, Katrin and Marky, Karola and Funk, Markus},
  title = {We Should Start Thinking About Privacy Implications of Sonic Input in Everyday Augmented Reality!},
  booktitle = {Proceedings of Mensch und Computer 2018 - Workshopband},
  series = {GI-Edition},
  year = {2018},
  month = {September},
  pages = {353--359},
  publisher = {Gesellschaft für Informatik e.V.},
  address = {Dresden, Germany},
  issn = {1617-5468},
  url = {https://hdl.handle.net/20.500.12738/16327},
  keywords = {Smart Assistant, Sonic, Voice, Privacy}
}

@misc{wsj2023,
  author       = {Salvador Rodriguez},
  title        = {How Meta's Smart Ray-Ban Glasses Spawned a Silicon Valley Hit},
  year         = {2023},
  url          = {https://www.wsj.com/tech/how-metas-smart-ray-ban-glasses-spawned-a-silicon-valley-hit-3fca3acd},
  note         = {Accessed: 2024-11-18}
}

@inproceedings{xu2023xair,
author = {Xu, Xuhai and Yu, Anna and Jonker, Tanya R. and Todi, Kashyap and Lu, Feiyu and Qian, Xun and Evangelista Belo, Jo\~{a}o Marcelo and Wang, Tianyi and Li, Michelle and Mun, Aran and Wu, Te-Yen and Shen, Junxiao and Zhang, Ting and Kokhlikyan, Narine and Wang, Fulton and Sorenson, Paul and Kim, Sophie and Benko, Hrvoje},
title = {XAIR: A Framework of Explainable AI in Augmented Reality},
year = {2023},
isbn = {9781450394215},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3544548.3581500},
doi = {10.1145/3544548.3581500},
abstract = {Explainable AI (XAI) has established itself as an important component of AI-driven interactive systems. With Augmented Reality (AR) becoming more integrated in daily lives, the role of XAI also becomes essential in AR because end-users will frequently interact with intelligent services. However, it is unclear how to design effective XAI experiences for AR. We propose XAIR, a design framework that addresses when, what, and how to provide explanations of AI output in AR. The framework was based on a multi-disciplinary literature review of XAI and HCI research, a large-scale survey probing 500+ end-users’ preferences for AR-based explanations, and three workshops with 12 experts collecting their insights about XAI design in AR. XAIR’s utility and effectiveness was verified via a study with 10 designers and another study with 12 end-users. XAIR can provide guidelines for designers, inspiring them to identify new design opportunities and achieve effective XAI designs in AR.},
booktitle = {Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems},
articleno = {202},
numpages = {30},
keywords = {Augmented Reality, Design Framework, Explainable AI},
location = {Hamburg, Germany},
series = {CHI '23}
}

@article{yu2024understanding,
author = {Yu, Xinyan and Hoggenm\"{u}ller, Marius and Tran, Tram Thi Minh and Wang, Yiyuan and Tomitsch, Martin},
title = {Understanding the Interaction between Delivery Robots and Other Road and Sidewalk Users: A Study of User-generated Online Videos},
year = {2024},
issue_date = {December 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {13},
number = {4},
url = {https://doi.org/10.1145/3677615},
doi = {10.1145/3677615},
abstract = {The deployment of autonomous delivery robots in urban environments presents unique challenges in navigating complex traffic conditions and interacting with diverse road and sidewalk users. Effective communication between robots and road and sidewalk users is crucial to address these challenges. This study investigates real-world encounter scenarios where delivery robots and road and sidewalk users interact, seeking to understand the essential role of communication in ensuring seamless encounters. Following an online ethnography approach, we collected 117 user-generated videos from TikTok and their associated 2,067 comments. Our systematic analysis revealed several design opportunities to augment communication between delivery robots and road and sidewalk users, which include facilitating multi-party path negotiation, managing unexpected robot behaviour via transparency information, and expressing robot limitations to request human assistance. Moreover, the triangulation of video and comments analysis provides a set of design considerations to realise these opportunities. The findings contribute to understanding the operational context of delivery robots and offer insights for designing interactions with road and sidewalk users, facilitating their integration into urban spaces.},
journal = {J. Hum.-Robot Interact.},
month = oct,
articleno = {59},
numpages = {32},
keywords = {delivery robots, online ethnography, human-robot interaction}
}

@article{zhang2019hierarchical,
  title={Hierarchical topic model based object association for semantic SLAM},
  author={Zhang, Jianhua and Gui, Mengping and Wang, Qichao and Liu, Ruyu and Xu, Junzhe and Chen, Shengyong},
  journal={IEEE transactions on visualization and computer graphics},
  volume={25},
  number={11},
  pages={3052--3062},
  year={2019},
  publisher={IEEE}
}

