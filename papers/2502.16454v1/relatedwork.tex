\section{Related Work}
% \subsection{Heterogeneous Graph Neural Networks}

Over the past decade, numerous research on mining information from graphs have shifted from traditional representation learning approaches \cite{perozzi2014deepwalk,grover2016node2vec,dong2017metapath2vec} to methods utilizing deep neural networks, including GNNs \cite{fan2019metapath,yan2021relation,zhang23pagelink,zhu23AutoAC,SHAN24KPI-HGNN,MaYLMC24HetGPT} and GCNs \cite{kipf2016semi,liu2023rhgn}. Inspired by the Transformer \cite{vaswani2017attention}, GAT \cite{velickovic2017graph} integrates the attention to aggregate node-level information in homogeneous networks, while HAN \cite{wang2019heterogeneous} introduces a two-level attention mechanism for node and semantic information in heterogeneous networks. MAGNN \cite{fu2020magnn}, MHGNN \cite{liang2022meta} and R-HGNN \cite{yu23RHGNN} proposed meta-path-based models to learn meta-path-based node embeddings. HetGNN \cite{zhang2019heterogeneous} and MEGNN \cite{chang2022megnn} take a meta-path-free approach to consider both structural and content information for each node jointly. HGT \cite{hu2020heterogeneous} incorporates information from high-order neighbors of different types through messages passing across ``soft'' meta-paths. MHGCN \cite{fu2023multiplex} captures local and global information by modeling the multiplex structures with depth and breadth behavior pattern aggregation. SeHGNN \cite{Yang23Simple} simplifies structural information capture by precomputing neighbor aggregation and incorporating a transformer-based semantic fusion module. HAGNN \cite{zhu2023hagnn} integrates meta-path-based intra-type aggregation and meta-path-free inter-type aggregation to generate the final embeddings. 

% \subsection{Message Passing Neural Networks}

% While MPNNs have achieved notable successes, they face challenges like over-smoothing [10], over-squashing [51], and adapting to large sparse graphs [64]. In response, we explore three adaptive strategies: (1) Skip connections combat the common issue of gradient vanishing in stacked message-passing layers by allowing direct links between different layers of the same node, thus maintaining node distinctiveness [62]. They come in various forms, including residual, initial, dense, and jumping connections [12]. (2) Multi-hop propagation extends beyond first-order neighbors, essential for datasets where long-range dependencies are critical, like macromolecules or image superpixels [16]. This method can involve uniform expansion of receptive fields or personalized fields for individual nodes [2, 26, 28, 66]. (3) Asynchronous communication introduces more flexible message-passing strategies that modify the information flow's pace, exemplified by models like GwAC [17] and Drew [24], which implement queues and oblique connections through delays, respectively. These innovations aim to enhance connectivity and information flow within graph structures, paving the way for more robust graph neural network models.

% \subsection{Selective State Space Model}

% State space models (SSMs) [6, 5] have gained popularity as an efficient alternative to attention-based architectures, utilizing recurrent updates across sequences via hidden states. However, their fixed time-invariant transition mechanisms often limit their effectiveness compared to Transformers. Addressing this limitation, Mamba [4] introduced a data-dependent state transition mechanism that effectively captures long-range context, achieving linear-time efficiency and performance comparable to Transformers. This innovation has prompted the adaptation of Mambaâ€™s architecture in various fields [13, 20, 34]. In the graph domain, researchers [28, 1] have developed strategies for transforming graph structures into ordered sequences, while others [18, 19] have formulated frameworks for spatial-temporal graphs and point cloud tasks, demonstrating the strengths of Mamba-based models in specific benchmarks.