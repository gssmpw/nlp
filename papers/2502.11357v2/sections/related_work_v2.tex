\subsection{LLM-based Web Agents}


Recent advances in multimodal language models have facilitated the development of web agents — autonomous systems designed to interact with real-world websites to perform everyday tasks \cite{mind2web, cogagent, seeclick, zheng2024gpt}.
Key challenges for web agents include long-term planning, visual grounding, and memory management. 
To improve long-context understanding, WebAgent \cite{DBLP:conf/iclr/GurFHSMEF24} utilizes multiple LLMs - one for planning, summarization, and grounded program synthesis.
SeeAct \cite{zheng2024gpt} adopts a two-step procedure of planning followed by grounding at each step using GPT-4 to accomplish web agent tasks.
Another line of work employs a vision-only approach to train a GUI grounding model that directly predicts pixel coordinates for executing GUI agent tasks \cite{seeclick, DBLP:conf/eccv/KapoorBRKKAS24, gou2024uground}.
However, a significant bottleneck remains — the lack of large-scale, high-quality web trajectory data for training robust agents. 
Our work presents a new framework for synthesizing large-scale web trajectory data to train end-to-end web agents.



\subsection{Web Agent Benchmarks and Datasets}
Early benchmarks for web tasks such as MiniWob++ \cite{miniwob} focused on testing low-level actions on simulated websites. 
However, these simulated websites fail to capture the complexity of the real-world web.
Mind2Web \cite{mind2web} introduces a trajectory-level dataset with 2K tasks across 137 real-world websites and 31 domains.
However, it employs a static evaluation method that penalizes alternative valid execution paths.
To overcome this limitation, follow-up work has explored alternative evaluation approaches, including functional correctness-based evaluation in WebArena \cite{DBLP:conf/iclr/ZhouX0ZLSCOBF0N24} and key-node-based evaluation in Mind2Web-Live \cite{pan2024webcanvas}.
Towards the goal of making web agents more capable of performing realistic tasks, GAIA \cite{mialon2024gaia} and AssistantBench \cite{DBLP:conf/emnlp/YoranAMBPB24} introduce benchmarks that include time-consuming information-seeking tasks.
In this work, we develop \model, a multimodal web agent trained on our synthetic dataset, and showcase its strong performance across online and offline benchmarks, including Mind2Web-Live, Multimodal-Mind2Web, and MiniWob++.

\input{tables/task_ex_2}

\subsection{Data Synthesis for Web Agents}
Early efforts to acquire trajectory data for training web agents primarily relied on crowd-sourcing \cite{mind2web, DBLP:conf/icml/LuKR24}.
However, human annotation is cost-prohibitive, prompting the adoption of synthetic data generation approaches to facilitate large-scale data collection.
AutoWebGLM \cite{DBLP:conf/kdd/LaiLIYCSYZZD024} and GUIAct \cite{chen2024guicourse} utilize LLMs to generate task proposals, which human experts subsequently annotate.
OpenWebVoyager \cite{he2024openwebvoyager} employs a web agent to execute auto-generated task descriptions. 
However, since these task descriptions are generated using LLMs without exploring a website, they fail to capture the full diversity of possible tasks on that website.
Another line of work, including Synatra \cite{Ou2024SynatraTI} and AgentTrek \cite{xu2024agenttrek}, leverages web tutorials to guide web trajectory generation.
Meanwhile, concurrent effort \cite{murty2024nnetscape} employs an exploration-based trajectory generation in WebArena’s sandbox, while our work focuses on more realistic web agent evaluation on live websites.
To address diversity limitations in prior trajectory synthesis work, we design a bottom-up web trajectory synthesis pipeline that explores websites dynamically while maintaining a coherent high-level task intent.