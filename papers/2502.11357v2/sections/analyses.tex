\section{Analyses}



\subsection{Ablation Studies}
We conduct ablation studies to assess the impact of various design choices on overall performance (Table~\ref{tab:m2w_live_diff_slm}).
To evaluate the importance of visual modality, we experiment with using just the textual modality for the Phi-3.5V model, replacing it with the text-only Phi-3-mini \cite{abdin2024phi}. In addition to Qwen2-VL-7B and Phi-3.5V, we also evaluate LLaVA-Mistral-7B \cite{DBLP:conf/nips/LiuLWL23a}, a strong MLLM baseline.
Our results show that omitting the visual modality leads to a sharp 4.8\% drop in performance for Phi-3.5V, underscoring its importance for effective GUI grounding.
Furthermore, LLaVA-Mistral-7B significantly underperforms compared to both Qwen2-VL-7B and Phi-3.5V, highlighting the necessity of a stronger MLLM backbone for improved GUI agent performance.

\input{figures/data_scaling}

\subsection{Failure Modes of Trajectory Generation}
We analyze cases where a generated trajectory is ultimately rejected by the task verifier agent. 
Our goal is to synthesize trajectory data that closely resembles human-annotated datasets for training web agents.
However, since our pipeline collects trajectories through an exploration-driven approach, some trajectories result from random, incoherent action sequences that fail to align with a well-defined task intent.
For instance, in shopping tasks, the agent may explore various products without demonstrating an intent to purchase (\eg, by adding items to the cart).
Another failure case arises when the agent encounters errors on the final page due to automated browser detection, CAPTCHA verification, or an unresponsive website.
We note that the verifier agent is instructed to judge a trajectory as successful if the task is completed, except for the final login and payment steps.
The trajectories in failure modes are still valuable for web agents to learn low-level tasks such as form filling, basic interaction with web elements, and visual grounding.


\begin{figure}
    \centering
    \includegraphics[width=\linewidth]{figures/error_plot_2.png}
    \caption{Statistics for different error cases in Mind2Web-Live evaluation. Task deviation is the most prevalent error type.
    }
    \label{fig:error_stats}
\end{figure}

\subsection{Case Studies for Mind2Web-Live}

We randomly sample 20 error cases for \model on Mind2Web-Live to gain insights for future improvement.
These errors fall into the following categories:
\begin{itemize}

\item \textit{Task deviation}: The agent executes actions unrelated to the given task, thus failing to complete it. 

 
\item \textit{Missing key steps}: The agent retrieves results that partially satisfy the required constraints, \eg, the agent finds women's clothes of the correct size but incorrect type or color.


\item \textit{Grounding error}: The agent fails to interact with a valid element on the page.

\item \textit{Website unresponsive}: The agent executes the correct action, but the website does not respond.

\item \textit{Failure to reach the correct website}: This happens when the agent fails to output the correct website URL or use the search engine to arrive at the correct website.

\end{itemize}
Figure~\ref{fig:error_stats} presents the statistics for these error types.