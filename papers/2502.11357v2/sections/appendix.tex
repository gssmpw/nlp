% This is an appendix.
\newpage

\setcounter{table}{0}
\renewcommand\thetable{\Alph{section}.\arabic{table}}
\setcounter{figure}{0}
\renewcommand\thefigure{\Alph{section}.\arabic{figure}}

\section*{Appendices}
This supplementary material provides additional details omitted in the main text.
\begin{itemize}
    \item Appendix~\ref{sec:m2w_eval}: Mind2Web Training and Evaluation Details
    \item Appendix~\ref{sec:cost_breakdown}: Trajectory Synthesis Cost Analysis
    \item Appendix~\ref{sec:reason_gen_agent}: Reasoning Generation Agent
    \item Appendix~\ref{sec:prompt_details}: Prompt Details
    \item Appendix~\ref{sec:traj_ex}: Trajectory Examples
\end{itemize}


\section{Mind2Web Training and Evaluation Details}\label{sec:m2w_eval}

Table~\ref{tab:hyper} shows the hyperparameters and training time for experiments on Mind2Web-Live and Multimodal-Mind2Web.
All experiments use Nvidia H100 GPUs.

\input{tables/m2w_live_results}

\subsection{Mind2Web-Live}
We exclude the following websites - \url{https://www.kbb.com}, \url{https://www.sixflags.com}, \url{https://www.viator.com}, \url{https://www.menards.com}, \url{https://www.amctheatres.com}, \url{https://www.cargurus.com}, \url{https://www.gamestop.com}, \url{https://www.cabelas.com}, \url{https://www.rei.com} due to denial of access faced during our tests.
% faced during trajectory collection or execution of Mind2Web-Live tasks on these websites.
Table~\ref{tab:m2w_live} shows the results on Mind2Web-Live for 83 out of 104 tasks across the remaining 37 websites.
The results on the whole Mind2Web-Live evaluation set are given in Table~\ref{tab:m2w_live_full}.
The results in Table~\ref{tab:m2w_live} are reported as the maximum over three runs, accounting for intermittent website access issues that may affect evaluation consistency.
For Mind2Web-Live, the dataloader first samples training instances at the trajectory level and then randomly samples a step from the trajectory to construct the final training instance.
Thus, the number of epochs is calculated at the trajectory level.
We use a viewport resolution of $1280\times 720$ during inference.
The Mind2Web-Live dataset is released under the MIT license which permits its use for academic research.

\subsection{Multimodal-Mind2Web}
Following \citet{mind2web}, we obtain the top-50 elements from a pre-trained DeBERTa \cite{he2021deberta} candidate generation model, which are then used to construct the accessibility tree and SOM image inputs.
Following \citet{Ou2024SynatraTI}, we always include the ground truth element in the input.
We use a viewport resolution of $1280\times 720$ which includes the GT element during inference.
We follow the setting in \citet{zheng2024gpt} and report element accuracy, operation F1, and step SR as evaluation metrics.
All experiments on Multimodal-Mind2Web use a single training and evaluation run.
The dataloader uniformly samples training instances from the set of action steps across all trajectories.
The Multimodal-Mind2Web dataset is released under the Responsible AI license which permits its use for academic research.

\input{tables/hyperparams}

\section{Trajectory Synthesis Cost Analysis}\label{sec:cost_breakdown}
We use GPT-4o-turbo, which costs \$2.5 per 1M tokens for our trajectory synthesis.
Each proposal or refinement stage uses 3.6K textual tokens on average.
% of resolution $1280\times 720$ 
Each input image costs \$0.0028.
The calculation assumes an average of 7.7 steps per trajectory, including the proposal stage.
Table~\ref{tab:cost_breakdown} shows the breakdown for the different stages of trajectory generation.
\begin{align*}
\textrm{Total cost} &= \$0.0128 * 7.7 + \$0.02581\\ &+ \$0.02381
= \$0.148
\end{align*}

The average cost per raw trajectory is \$0.15.
The success rate is estimated as 53.1\%.
Thus, the average cost per successful trajectory is estimated to be \$0.28.

\input{tables/cost_breakdown}

\section{Reasoning Generation Agent}\label{sec:reason_gen_agent}
Inspired by \citet{xu2024aguvis}, the reasoning generation agent is a pre-trained Qwen2-VL-7B model that takes as input the current action, high-level task description, screenshot, accessibility tree, and action history. It then outputs a post-hoc reasoning trace for performing that action. These reasoning traces are helpful for training GUI agents in a chain-of-thought style.
% We will release the reasoning traces along with the trajectories for \model.

\section{Prompt Details}\label{sec:prompt_details}
The prompts for the task proposer agent, task refiner agent, task summarizer agent, and task verifier agent are given in Table~\ref{tab:proposal_prompt}, Table~\ref{tab:refiner_prompt}, Table~\ref{tab:summarizer_prompt}, and Table~\ref{tab:verifier_prompt}, respectively.
The training prompt for \model is given in Table~\ref{tab:train_prompt}.

\input{tables/prompts/task_proposal}

\nopagebreak
\input{tables/prompts/task_refiner}

\nopagebreak
\input{tables/prompts/summarizer}

\input{tables/prompts/verifier_prompt}

\input{tables/prompts/slm_train_prompt}

\FloatBarrier
% \clearpage

\section{Trajectory Examples}\label{sec:traj_ex}
Figure~\ref{fig:traj_ex} shows a sample trajectory executed on the IKEA website.
Figure~\ref{fig:traj_input} shows the set-of-mark annotations and accessibility tree inputs of the model during trajectory generation, model training, and inference.

% \FloatBarrier
% \vspace{-100mm}

\input{figures/traj_ex}

% \vspace{-100mm}

\input{figures/traj_input}