
We use the synthetic trajectories generated by our pipeline to train small multimodal language models (SLMs) for web agent tasks.
To ensure computational efficiency, we select 40K trajectories from the full set for training.
We further refine this subset by filtering out trajectories that contain more than two scroll actions to mitigate potential model bias toward excessive scrolling behavior.
Finally, we use \textasciitilde30K trajectories obtained after filtering to fine-tune multimodal language models like Phi-3.5V \cite{abdin2024phi} and Qwen2-VL-7B \cite{Qwen2VL}.
For brevity, we denote the models trained on Phi-3.5V and Qwen2-VL-7B as \model-4B and \model-7B, respectively.
To test the effectiveness of our data  for web-based agentic tasks, we evaluate \model-4B and \model-7B
on Mind2Web-Live \cite{pan2024webcanvas}, Multimodal-Mind2Web \cite{mind2web, zheng2024gpt}, and MiniWob++ \cite{miniwob}.


\paragraph{Multimodal-Mind2Web.}
Multimodal-Mind2Web is an offline web agent benchmark comprising 2K open-ended tasks spanning 137 websites across 31 domains. 
Each task comprises a sequence of actions with screenshots, action type, and HTML.
We follow the setting in \citet{zheng2024gpt}
and report element accuracy, operation F1, and step success rate (SR) as evaluation metrics.

\paragraph{Mind2Web-Live.}
Mind2Web-Live is a benchmark modified
from the original Mind2Web dataset to test
web agents on live websites rather than static trajectories.
The benchmark evaluates performance using a key-node-based evaluation approach rather than using a golden action sequence, requiring valid trajectories to reach annotated ``key nodes'' across 104 test tasks in Mind2Web.
Since Mind2Web-Live relies on real-world dynamic websites, it encounters robot detection such as reCAPTCHA, which hinders testing \cite{xu2024aguvis}.
To address this, we select a subset of 83 test set tasks that remain consistently accessible throughout our tests.
Following \citet{pan2024webcanvas}, we report the average step success rate, completion rate, and full task success rate (SR) on the test set.
The average step success rate and completion rate represent the proportion of completed key nodes, using macro and micro averages, respectively.

\paragraph{MiniWob++.} This benchmark consists of low-level tasks on a single webpage.
Typical examples include clicking a sequence of buttons, selecting items from a drop-down list, and filling out a form.
We use the subset of 46 tasks used for evaluation in prior work \cite{DBLP:conf/acl/ZengLLWLD024, Ou2024SynatraTI}.
The final score is obtained by averaging the results of four runs per task.
We use the zero-shot evaluation setting, which does not use any environment-specific trajectories for training. 


\input{tables/m2w_live_results_filter}

\section{Results}


\subsection{In-domain Evaluation}
As an intrinsic evaluation of the trajectory collection pipeline, we generate 100 test tasks using \model, disjoint from the train set.
The SLM agents are tasked with executing the given tasks on live websites while an LLM-as-a-judge verifier (\S~\ref{sec:verifier}) evaluates the correctness of their actions at the trajectory level (Table~\ref{tab:in_domain}).
% Table~\ref{tab:in_domain} shows the results.
We observe that the fine-tuned agents significantly outperform their pre-trained counterparts.
Thus, using in-domain web trajectory data training helps, which is a valuable sanity check.
% It also surpasses GPT-4o, which indicates that the web navigation capabilities of such general closed-source LLMs can be further improved by training on in-domain data.
It also surpasses GPT-4o, further underscoring our synthetic data's quality.


\input{tables/in_domain_results}

\input{tables/miniwob_results}

\input{tables/m2w_orig_results}



\subsection{Mind2Web-Live Results} \label{sec:m2w_live_results}
We evaluate \model-4B and \model-7B trained on the synthetic trajectory dataset (Table~\ref{tab:m2w_live}).
We make the following observations from the results:

\paragraph{Improvement over base pre-trained models.}
We observe that \model-7B yields improvements of 5.1\% and 4.8\% in average step success rate and key node completion rate, respectively, compared to the pre-trained Qwen2-VL-7B model. 
Similarly, \model-4B obtains gains of 15.5\% and 15.9\% in average step success rate and key node completion rate, respectively, over its pre-trained counterpart.
In terms of full task success rate, Phi-3.5V improves significantly from 2.4\% to 18.1\%, while Qwen2-VL-7B improves from 14.5\% to 19.3\%. 
To the best of our knowledge, this represents the state-of-the-art performance on Mind2Web-Live for end-to-end web agents of this size that are trained exclusively on synthetic data.
\\



\paragraph{Improvement over higher capacity pre-trained models.}
Despite having much fewer parameters, we observe that \model-4B outperforms strong baselines such as Mistral-7B-Instruct-0.3 and Qwen2-72B-Instruct in full task SR by margins of 8.5\% and 2.7\%, respectively.
The Phi-3.5V model obtains an 18.1\% full task success rate, which is better than GPT-3.5 (15.4\%), despite using orders of magnitude fewer parameters.
The corresponding results for the entire set of 104 tasks, including unreachable websites, are given in Appendix~\ref{sec:m2w_eval}.


\subsection{Multimodal-Mind2Web Results} \label{sec:mm_m2w_results}
Following \citet{mind2web}, we obtain the top-50 elements from a pre-trained DeBERTa \cite{he2021deberta} candidate generation model which are then used to construct the accessibility tree and SoM image inputs.
The results are shown in Table~\ref{tab:m2w_orig}.

Among baselines, we include API-based models for in-context learning -- GPT-3.5, GPT-4, and SeeAct \cite{zheng2024gpt}. 
SeeAct is a web agent that performs web tasks using a two-step procedure of action generation and grounding using GPT-4V.
Additionally, we include baselines that fine-tune small language models using synthetic data, followed by further fine-tuning on the Mind2Web training set.
SeeClick \cite{seeclick} introduces a visual grounding model (Qwen-VL) trained on synthetically-generated grounding data.
EDGE \cite{chen2024edge} synthesizes QA data on webpages to improve the grounded GUI understanding capabilities of MLLMs.
ScribeAgent-Large \cite{shen2024scribeagent} and MiniCPM-GUI \cite{chen2024guicourse} use human-annotated trajectory data to train web agents.
AgentTrek \cite{xu2024agenttrek} is a GUI agent baseline that also utilizes synthetic trajectory data to fine-tune SLMs for Mind2Web, similar to our setting.
We observe that \model-7B fine-tuned on synthetic data from \model plus Mind2Web outperforms all baselines in average step success rate.
Notably, it surpasses AgentTrek, which uses the same Qwen2-VL-7B MLLM backbone, highlighting the superior quality of our dataset.
The broad domain coverage and task diversity in \model contribute to its superior generalization across environments.



\subsection{MiniWob++ Results}
Table~\ref{tab:miniwob} shows the results on the MiniWob++ benchmark in the zero-shot evaluation setting.
Among baselines, we have API-based models, in-context learning using open-source LMs, and agentic models like AgentLM \cite{DBLP:conf/acl/ZengLLWLD024}, CodeActAgent \cite{DBLP:conf/icml/WangCY0L0J24}, Lemur-Chat \cite{DBLP:conf/iclr/XuSXMLSHZLXCZKW24} and AgentFlan \cite{DBLP:conf/acl/ChenLWZLLCZ24} which include web-based demonstrations in their instruction tuning dataset.
% Synatra-CodeLlama-7B \cite{Ou2024SynatraTI} also synthesizes web-agent trajectories automatically and uses it to fine-tune a CodeLlama-7B model.
Synatra-CodeLlama-7B \cite{Ou2024SynatraTI} and AgentTrek \cite{xu2024agenttrek} also synthesize web-agent trajectories automatically.
We observe that \model outperforms GPT-4 and general-purpose agents like AgentLM, CodeActAgent, Lemur-Chat, and AgentFlan.
% \model-4B also surpasses Synatra-CodeLlama-7B despite using a much smaller model with 4.2B params, highlighting our synthetic data's superior quality and potential for out-of-distribution (OOD) generalization.
\model-4B surpasses Synatra-CodeLlama-7B and AgentTrek-7B despite using a much smaller model with 4.2B params, highlighting our synthetic data's superior quality and potential for out-of-distribution (OOD) generalization.


\input{tables/m2w_live_diff_slm}

\subsection{Data Scaling Experiments} \label{sec:data_scaling}
We conduct experiments with different data scales for \model-4B to analyze the impact of training data size.
Specifically, we subsample the original trajectory dataset to utilize 50\% and 25\% of its original size.
Figure~\ref{fig:data_scaling} presents the resulting performance curves.
Our results show that, even with just 25\% of the training data, the model exhibits rapid performance gains over the base pre-trained model.
Increasing the dataset size further leads to gradual improvements across all reported metrics.
However, the increase in the overall task success rate is more gradual compared to the stepwise metrics, as it is a more coarse-grained metric.



