\begin{table}[htbp]
\centering
\small
% \resizebox{\linewidth}{!}{%
\begin{tabular}{@{}ll@{}}
\toprule
\textbf{Model}       & \textbf{Accuracy (\%)} \\ \midrule
\multicolumn{2}{l}{\textit{API-based Models}} \\ \cmidrule(r){1-2}
GPT-3.5              & \num{39.57}                  \\
GPT-4                & \num{53.04}                  \\ \midrule
\multicolumn{2}{l}{\textit{Open-source Instructed Models}}\\ \cmidrule(r){1-2}
% \cite{llama3modelcard}
Phi-3.5V & \num{35.87} \\
Qwen2-VL-7B & \num{36.96} \\
Llama3-chat-8B       & \num{31.74}                  \\
Llama3-chat-70B      & \num{48.70}                  \\ \midrule
\multicolumn{2}{l}{\textit{Open-source Interactive Data Finetuned Models}}\\ \cmidrule(r){1-2}
% \cite{DBLP:conf/acl/ZengLLWLD024}
AgentLM-7B \cite{DBLP:conf/acl/ZengLLWLD024}            & \num{15.65}                  \\
% \cite{DBLP:conf/icml/WangCY0L0J24}
CodeActAgent-7B \cite{DBLP:conf/icml/WangCY0L0J24}       & \num{9.78}                   \\
% \cite{DBLP:conf/acl/ChenLWZLLCZ24}
AgentFlan-7B \cite{DBLP:conf/acl/ChenLWZLLCZ24}          & \num{20.87}                  \\
% \cite{DBLP:conf/iclr/XuSXMLSHZLXCZKW24}
Lemur-chat-70B \cite{DBLP:conf/iclr/XuSXMLSHZLXCZKW24}        & \num{21.30}                  \\
% \cite{DBLP:conf/acl/ZengLLWLD024}
AgentLM-70B \cite{DBLP:conf/acl/ZengLLWLD024}            & \num{36.52}                  \\
% \cite{Ou2024SynatraTI}
Synatra-CodeLlama-7B \cite{Ou2024SynatraTI}  & \num{38.20}   \\   
AgentTrek-7B \cite{xu2024agenttrek} & \num{45.28}\\
\midrule
\bfseries \model-4B                &  \num{46.74} \\
\bfseries \model-7B                & \bfseries \num{53.26} \\
\bottomrule                
\end{tabular}
% }
\caption{Results on MiniWob++ benchmark \cite{miniwob} in zero-shot evaluation setting. The baseline numbers correspond to \citet{Ou2024SynatraTI}. \model outperforms much larger models by a significant margin.}
\label{tab:miniwob}
\end{table}