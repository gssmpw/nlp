\section{Related Work}
\textbf{Locate-then-Edit Knowledge Editing.} 
The locate-then-edit approach in knowledge editing identifies and modifies specific weights in pre-trained models to achieve desired outputs ____. Various methods have been proposed within this framework. ROME ____ updates the feedforward network to encode new knowledge, while MEMIT ____ extends this for large-scale editing. PMET ____ enhances MEMITâ€™s performance with a residual attribution strategy. Additionally, ROME ____ and MEMIT ____ use input prompts to locate and edit knowledge neurons. However, existing works do not address multi-client scenarios and multi-editing tasks. In this paper, we propose a federated locate-then-edit knowledge editing framework to improve editing efficiency in such settings. \\
\textbf{Federated Learning in LLMs.}
Research on combining large language models (LLMs) and federated learning (FL) primarily focuses on pre-training and prompt engineering ____. Pre-trained models, trained on large datasets, serve as a foundation for FL, significantly reducing training time ____ and helping address data and system heterogeneity ____. Some studies incorporate pre-trained models into FL frameworks for various tasks ____. Prompt-based techniques have shown strong performance in LLMs ____. The pFedPT framework personalizes models efficiently using personalized prompts ____, while DiPrompT ____ applies adaptive prompts to tackle domain generalization challenges in FL. To the best of our knowledge, this is the first work to apply FL for optimizing LEKE in LLMs.