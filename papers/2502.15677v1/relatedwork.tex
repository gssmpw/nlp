\section{Related Work}
\textbf{Locate-then-Edit Knowledge Editing.} 
The locate-then-edit approach in knowledge editing identifies and modifies specific weights in pre-trained models to achieve desired outputs \cite{locate-then-edit-1, locate-then-edit-2}. Various methods have been proposed within this framework. ROME \cite{ROME} updates the feedforward network to encode new knowledge, while MEMIT \cite{MEIMT} extends this for large-scale editing. PMET \cite{PMET} enhances MEMITâ€™s performance with a residual attribution strategy. Additionally, ROME \cite{ROME} and MEMIT \cite{MEIMT} use input prompts to locate and edit knowledge neurons. However, existing works do not address multi-client scenarios and multi-editing tasks. In this paper, we propose a federated locate-then-edit knowledge editing framework to improve editing efficiency in such settings. \\
\textbf{Federated Learning in LLMs.}
Research on combining large language models (LLMs) and federated learning (FL) primarily focuses on pre-training and prompt engineering \cite{fl-llm}. Pre-trained models, trained on large datasets, serve as a foundation for FL, significantly reducing training time \cite{fl-llm-time1, fl-llm-time2} and helping address data and system heterogeneity \cite{fl-llm-non-iid}. Some studies incorporate pre-trained models into FL frameworks for various tasks \cite{pre-fl1, pre-fl2}. Prompt-based techniques have shown strong performance in LLMs \cite{promptfl}. The pFedPT framework personalizes models efficiently using personalized prompts \cite{personlization}, while DiPrompT \cite{DiPrompT} applies adaptive prompts to tackle domain generalization challenges in FL. To the best of our knowledge, this is the first work to apply FL for optimizing LEKE in LLMs.