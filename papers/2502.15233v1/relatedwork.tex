\section{Related Works}
\label{sec:rel}

Privacy protection for large language models (LLMs) can be categorized according to the phase in which it is implemented: during the pre-training and fine-tuning phases, and during the inference phase~\citep{yan2024protecting}.
Privacy protection during the pre-training and fine-tuning phases of LLMs is essential for safeguarding sensitive data while preserving model effectiveness. 
Techniques such as differential privacy~\citep{li2021large,wu2022adaptive,xu2024fwdllm}, data cleaning~\citep{bai2022training,kandpal2022deduplicating}, and federated learning~\citep{yu2023federated,xu2024fwdllm,zhang2024towards} can be utilized to mitigate privacy risks during these phases.
% For example, OpenAI~\citep{brown2020language} uses measures like filtering and fuzzy deduplication to remove personally identifiable information from their training data. This approach improves data quality while enhancing privacy protection.
As previously discussed, these methods primarily aim to protect the privacy of information within LLMs. However, they do not fully address the privacy concerns associated with remote access to LLM services. 
Additionally, privacy protection measures implemented by model providers may not completely alleviate users' concerns regarding the potential misuse of their private data by these providers.

On the other hand, the issue of privacy leakage during the inference phase of LLMs has garnered significant attention.
To address this issue, researchers have developed numerous strategies to ensure privacy security during the inference phase. 
These include encryption-based privacy protection approaches such as Multi-Party Computation~\citep{goldreich1998secure, dong2022fusion}, homomorphic encryption~\citep{acar2018survey, hao2022iron,lu2023bumblebee}, and differential privacy in inference~\citep{dwork2006differential, dwork2008differential, majmudar2022differentially}.
% , as well as detection-based approaches like text pseudonymization~\citep{kan2023protecting,yermilov2023privacy,chen2023hide}.
%
% Homomorphic encryption is an extension of public-key cryptography that enables computations to be performed on encrypted data without the need for decryption \citep{acar2018survey, hao2022iron, lu2023bumblebee}. 
For example, \citet{huang2022cheetah} proposed a specialized encoding method, Cheetah, which encodes vectors and matrices into homomorphic encryption polynomials.
% Similarly, \citet{liu2023llms} proposed a framework designed to enhance the efficiency of private inference on transformer models by substituting computationally intensive operations with privacy-preserving alternatives.
However, these homomorphic encryption methods are challenging to apply to cloud-based black-box LLMs, as they require access to the model's internal structures.
%
% Differential privacy is a technique that preserves individual anonymity in a dataset by adding algorithmic noise, ensuring that the influence of any single data point on the aggregated output is limited~\citep{dwork2006differential, dwork2008differential}.
Additionally, \citet{du2023dp} introduced DP-Forward, which applies differential privacy during inference by perturbing embedding matrices in the forward pass of language models. 
% Similarly, \citet{tong2023privinfer} proposed InferDPT, a method for privacy-preserving inference in black-box LLMs that incorporates differential privacy in text generation.
However, these differential privacy approaches are mainly effective when the LLMâ€™s decision-making does not rely on sensitive information, which differs from the focus of our research.


%%%%%%%%%%
\begin{figure*}[th]
    \centering
    \vspace{-0.2cm}
    \includegraphics[scale=0.37]{figures/framework.pdf}
    \caption{Overview of pseudonymization framework for cloud-based LLMs}
    \vspace{-0.2cm}
    \label{fig:framework}
\end{figure*}
%%%%%%%%%%

In addition to the aforementioned methods, pseudonymization techniques focus on safeguarding the privacy of the prompt by identifying and removing privacy-sensitive information.
For example, \citet{kan2023protecting} and \citet{chen2023hide} proposed anonymizing sensitive terms before inputting them into the LLM and restoring them after the output.
\citet{lin2024emojicrypt} proposed a pseudonymization method to safeguard user privacy by converting user input from natural language into a sequence of emojis.
\citet{zhang2024cogenesis} introduced a mixed-scale model collaboration approach that combines the strengths of a large cloud-based model with a smaller, locally deployed model.
However, there is currently no general definition of a pseudonymization framework for the inference phase of cloud-based LLMs.
% we provide a detailed definition of a pseudonymization framework designed for remote conversational LLM systems
Additionally, these methods have primarily been tested on classification tasks, which differ from the core task of text generation in LLMs. Therefore, their results may not fully capture their effectiveness in text generation.

% %%%%%%%%%%%%%%%%%%%%%%
%