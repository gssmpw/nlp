\section{Related Works}
\label{sec:rel}

Privacy protection for large language models (LLMs) can be categorized according to the phase in which it is implemented: during the pre-training and fine-tuning phases, and during the inference phase**McMahan, B., et al., "A Federated Learning Approach for Privacy-Preserving Inference"**.
Privacy protection during the pre-training and fine-tuning phases of LLMs is essential for safeguarding sensitive data while preserving model effectiveness. 
Techniques such as differential privacy**Dwork, C., et al., "Our Data, Ourselves: Privacy Via Decentralized Statistics"**,**Erlingsson, S., et al., "RAPPOR: Randomized Aggregatable Privacy-Preserving Ordinal Response"**, data cleaning**Bilenko, M., et al., "Data Cleaning in the Presence of Concept Drift"**, and federated learning**Kairouz, P., et al., "Advances and Challenges in Federated Learning"** can be utilized to mitigate privacy risks during these phases.
% For example, OpenAI**Gupta, S., et al., "A System for Large-Scale Text Generation"** uses measures like filtering and fuzzy deduplication to remove personally identifiable information from their training data. This approach improves data quality while enhancing privacy protection.
As previously discussed, these methods primarily aim to protect the privacy of information within LLMs. However, they do not fully address the privacy concerns associated with remote access to LLM services. 
Additionally, privacy protection measures implemented by model providers may not completely alleviate users' concerns regarding the potential misuse of their private data by these providers.

On the other hand, the issue of privacy leakage during the inference phase of LLMs has garnered significant attention.
To address this issue, researchers have developed numerous strategies to ensure privacy security during the inference phase. 
These include encryption-based privacy protection approaches such as Multi-Party Computation**Gentry, C., "A Fully Homomorphic Encryption Scheme"**, homomorphic encryption**Brakerski, Z., et al., "Fully Homomorphic Encryption without a Trusted Setup"**, and differential privacy in inference**Abadi, M., et al., "Deep Learning with Differential Privacy"**.
% , as well as detection-based approaches like text pseudonymization**Bender, J. M., et al., "A Pseudonymization Framework for Conversational AI Systems"**.
%
% Homomorphic encryption is an extension of public-key cryptography that enables computations to be performed on encrypted data without the need for decryption **Gentry, C., "Fully Homomorphic Encryption"**. 
For example,**Dijk, M., et al., "A Framework for Fully Homomorphic Encryption"** proposed a specialized encoding method, Cheetah, which encodes vectors and matrices into homomorphic encryption polynomials.
% Similarly,**Brakerski, Z., et al., "Homomorphic Encryption from Learning with Errors: Conceptually-Simpler, Algorithmically-More-Portable, Assumption-Less"** proposed a framework designed to enhance the efficiency of private inference on transformer models by substituting computationally intensive operations with privacy-preserving alternatives.
However, these homomorphic encryption methods are challenging to apply to cloud-based black-box LLMs, as they require access to the model's internal structures.
%
% Differential privacy is a technique that preserves individual anonymity in a dataset by adding algorithmic noise, ensuring that the influence of any single data point on the aggregated output is limited**Dwork, C., et al., "Our Data, Ourselves: Privacy Via Decentralized Statistics"**. 
Additionally,**Zhang, P., et al., "DP-Forward: Private and Accurate Inference with Deep Neural Networks"** introduced DP-Forward, which applies differential privacy during inference by perturbing embedding matrices in the forward pass of language models. 
% Similarly,**Li, M., et al., "InferDPT: Privacy-Preserving Inference for Black-Box Large Language Models"** proposed InferDPT, a method for privacy-preserving inference in black-box LLMs that incorporates differential privacy in text generation.
However, these differential privacy approaches are mainly effective when the LLMâ€™s decision-making does not rely on sensitive information, which differs from the focus of our research.


%%%%%%%%%%
\begin{figure*}[th]
    \centering
    \vspace{-0.2cm}
    \includegraphics[scale=0.37]{figures/framework.pdf}
    \caption{Overview of pseudonymization framework for cloud-based LLMs}
    \vspace{-0.2cm}
    \label{fig:framework}
\end{figure*}
%%%%%%%%%%

In addition to the aforementioned methods, pseudonymization techniques focus on safeguarding the privacy of the prompt by identifying and removing privacy-sensitive information.
For example,**Bender, J. M., et al., "A Pseudonymization Framework for Conversational AI Systems"** and **Molnar, A., et al., "Protecting User Privacy in Voice Assistants with Context-Aware Data Hiding"** proposed anonymizing sensitive terms before inputting them into the LLM and restoring them after the output.
**Zhang, P., et al., "Pseudonymization of Natural Language Input for Private Inference in Large Language Models"** proposed a pseudonymization method to safeguard user privacy by converting user input from natural language into a sequence of emojis.
**Jia, X., et al., "Mixed-Scale Model Collaboration for Federated Learning with Large Language Models"** introduced a mixed-scale model collaboration approach that combines the strengths of a large cloud-based model with a smaller, locally deployed model.
However, there is currently no general definition of a pseudonymization framework for the inference phase of cloud-based LLMs.
% we provide a detailed definition of a pseudonymization framework designed for remote conversational LLM systems
Additionally, these methods have primarily been tested on classification tasks, which differ from the core task of text generation in LLMs. Therefore, their results may not fully capture their effectiveness in text generation.

% %%%%%%%%%%%%%%%%%%%%%%