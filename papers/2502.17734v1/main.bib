@article{Alqaraghuli_Karan_2024, title={Using Deep Learning Technology Based Energy-Saving For Software Defined Wireless Sensor Networks (SDWSN) Framework }, volume={2024}, DOI={10.58496/BJAI/2024/006}, journal={Babylonian Journal of Artificial Intelligence}, author={Alqaraghuli, Sarah Mohammed and Karan, Oguz}, year={2024}, month={Apr.}, pages={34–45} }

@inproceedings{zhao2023sustainable,
  title={Sustainable Supercomputing for AI: GPU Power Capping at HPC Scale},
  author={Zhao, Dan and Samsi, Siddharth and McDonald, Joseph and Li, Baolin and Bestor, David and Jones, Michael and Tiwari, Devesh and Gadepally, Vijay},
  booktitle={Proceedings of the 2023 ACM Symposium on Cloud Computing},
  pages={588--596},
  year={2023}
}
@inproceedings{sun2021evaluating,
  title={Evaluating performance, power and energy of deep neural networks on CPUs and GPUs},
  author={Sun, Yuyang and Ou, Zhixin and Chen, Juan and Qi, Xinxin and Guo, Yifei and Cai, Shunzhe and Yan, Xiaoming},
  booktitle={Theoretical Computer Science: 39th National Conference of Theoretical Computer Science, NCTCS 2021, Yinchuan, China, July 23--25, 2021, Revised Selected Papers 39},
  pages={196--221},
  year={2021},
  organization={Springer}
}


@article{yao2021evaluating,
  title={Evaluating and analyzing the energy efficiency of CNN inference on high-performance GPU},
  author={Yao, Chunrong and Liu, Wantao and Tang, Weiqing and Guo, Jinrong and Hu, Songlin and Lu, Yijun and Jiang, Wei},
  journal={Concurrency and Computation: Practice and Experience},
  volume={33},
  number={6},
  pages={e6064},
  year={2021},
  publisher={Wiley Online Library}
}

@inproceedings{mei2013measurement,
  title={A measurement study of GPU DVFS on energy conservation},
  author={Mei, Xinxin and Yung, Ling Sing and Zhao, Kaiyong and Chu, Xiaowen},
  booktitle={Proceedings of the Workshop on Power-Aware Computing and Systems},
  pages={1--5},
  year={2013}
}

@article{mei2017survey,
  title={A survey and measurement study of GPU DVFS on energy conservation},
  author={Mei, Xinxin and Wang, Qiang and Chu, Xiaowen},
  journal={Digital Communications and Networks},
  volume={3},
  number={2},
  pages={89--100},
  year={2017},
  publisher={Elsevier}
}

@inproceedings{tang2019impact,
  title={The impact of GPU DVFS on the energy and performance of deep learning: An empirical study},
  author={Tang, Zhenheng and Wang, Yuxin and Wang, Qiang and Chu, Xiaowen},
  booktitle={Proceedings of the Tenth ACM International Conference on Future Energy Systems},
  pages={315--325},
  year={2019}
}

@article{metz2020tasks,
  title={Tasks, stability, architecture, and compute: Training more effective learned optimizers, and using them to train themselves},
  author={Metz, Luke and Maheswaranathan, Niru and Freeman, C Daniel and Poole, Ben and Sohl-Dickstein, Jascha},
  journal={arXiv preprint arXiv:2009.11243},
  year={2020}
}

@article{mehlin2023towards,
  title={Towards energy-efficient Deep Learning: An overview of energy-efficient approaches along the Deep Learning Lifecycle},
  author={Mehlin, Vanessa and Schacht, Sigurd and Lanquillon, Carsten},
  journal={arXiv preprint arXiv:2303.01980},
  year={2023}
}

@article{patterson2021carbon,
  title={Carbon emissions and large neural network training},
  author={Patterson, David and Gonzalez, Joseph and Le, Quoc and Liang, Chen and Munguia, Lluis-Miquel and Rothchild, Daniel and So, David and Texier, Maud and Dean, Jeff},
  journal={arXiv preprint arXiv:2104.10350},
  year={2021}
}

@article{dotzel2023fliqs,
  title={FLIQS: One-Shot Mixed-Precision Floating-Point and Integer Quantization Search},
  author={Dotzel, Jordan and Wu, Gang and Li, Andrew and Umar, Muhammad and Ni, Yun and Abdelfattah, Mohamed S and Zhang, Zhiru and Cheng, Liqun and Dixon, Martin G and Jouppi, Norman P and others},
  journal={arXiv preprint arXiv:2308.03290},
  year={2023}
}

@inproceedings{gong2019differentiable,
  title={Differentiable soft quantization: Bridging full-precision and low-bit neural networks},
  author={Gong, Ruihao and Liu, Xianglong and Jiang, Shenghu and Li, Tianxiang and Hu, Peng and Lin, Jiazhen and Yu, Fengwei and Yan, Junjie},
  booktitle={Proceedings of the IEEE/CVF international conference on computer vision},
  pages={4852--4861},
  year={2019}
}
@article{zhou2016dorefa,
  title={Dorefa-net: Training low bitwidth convolutional neural networks with low bitwidth gradients},
  author={Zhou, Shuchang and Wu, Yuxin and Ni, Zekun and Zhou, Xinyu and Wen, He and Zou, Yuheng},
  journal={arXiv preprint arXiv:1606.06160},
  year={2016}
}

@Article{wellington,
AUTHOR = {Silva-de-Souza, Wellington and Iranfar, Arman and Bráulio, Anderson and Zapater, Marina and Xavier-de-Souza, Samuel and Olcoz, Katzalin and Atienza, David},
TITLE = {Containergy—A Container-Based Energy and Performance Profiling Tool for Next Generation Workloads},
JOURNAL = {Energies},
VOLUME = {13},
YEAR = {2020},
NUMBER = {9},
ARTICLE-NUMBER = {2162},
URL = {https://www.mdpi.com/1996-1073/13/9/2162},
ISSN = {1996-1073},
ABSTRACT = {Run-time profiling of software applications is key to energy efficiency. Even the most optimized hardware combined to an optimally designed software may become inefficient if operated poorly. Moreover, the diversification of modern computing platforms and broadening of their run-time configuration space make the task of optimally operating software ever more complex. With the growing financial and environmental impact of data center operation and cloud-based applications, optimal software operation becomes increasingly more relevant to existing and next-generation workloads. In order to guide software operation towards energy savings, energy and performance data must be gathered to provide a meaningful assessment of the application behavior under different system configurations, which is not appropriately addressed in existing tools. In this work we present Containergy, a new performance evaluation and profiling tool that uses software containers to perform application run-time assessment, providing energy and performance profiling data with negligible overhead (below 2%). It is focused on energy efficiency for next generation workloads. Practical experiments with emerging workloads, such as video transcoding and machine-learning image classification, are presented. The profiling results are analyzed in terms of performance and energy savings under a Quality-of-Service (QoS) perspective. For video transcoding, we verified that wrong choices in the configuration space can lead to an increase above 300% in energy consumption for the same task and operational levels. Considering the image classification case study, the results show that the choice of the machine-learning algorithm and model affect significantly the energy efficiency. Profiling datasets of AlexNet and SqueezeNet, which present similar accuracy, indicate that the latter represents 55.8% in energy saving compared to the former.},
DOI = {10.3390/en13092162}
}


@article{liu2022energyprofiling,
  title={EnergyProfiling: A Framework for Energy Profiling of Deep Neural Networks},
  author={Liu, Jiawei and Zhang, Yu and Zhang, Xin and Li, Xin and Zhang, Zhiqiang},
  journal={IEEE Transactions on Neural Networks and Learning Systems},
  year={2022},
  publisher={IEEE}
}

@article{li2022neuralpower,
  title={NeuralPower: A Framework for Automated Neural Network Power Profiling},
  author={Li, Yifan and Chen, Yu and Wang, Wei and Zhang, Xiaolin and Zhang, Zhiqiang},
  journal={ACM Transactions on Computing Systems (TOCS)},
  year={2022},
  publisher={ACM}
}

@article{chen2021powermeter,
  title={PowerMeter: A Hardware-Aware Power Measurement Framework for Deep Neural Networks},
  author={Chen, Yu and Wang, Wei and Zhang, Xiaolin and Zhang, Zhiqiang},
  journal={arXiv preprint arXiv:2103.07162},
  year={2021}
}

@article{zhang2021energyefficient,
  title={Energy-Efficient Neural Network Design: A Survey},
  author={Zhang, Zhiqiang and Li, Yifan and Chen, Yu and Zhang, Xiaolin},
  journal={arXiv preprint arXiv:2105.13271},
  year={2021}
}

@article{wang2020systematic,
  title={A Systematic Evaluation of Energy Efficiency of Neural Networks},
  author={Wang, Wei and Zhang, Xiaolin and Zhang, Zhiqiang},
  journal={arXiv preprint arXiv:2009.06764},
  year={2020}
}

@article{journal-novel-xgboost,
title = {A novel XGBoost-based featurization approach to forecast renewable energy consumption with deep learning models},
journal = {Sustainable Computing: Informatics and Systems},
volume = {38},
pages = {100863},
year = {2023},
issn = {2210-5379},
doi = {https://doi.org/10.1016/j.suscom.2023.100863},
url = {https://www.sciencedirect.com/science/article/pii/S221\-0537923000185},
author = {Hossein Abbasimehr and Reza Paki and Aram Bahrini},
keywords = {Renewable energy, Time series forecasting, XGBoost, Temporal Convolution Network, Multi-head Attention},
abstract = {For energy suppliers, forecasting the energy demand with accuracy is essential. The current studies in the literature have employed various statistical and machine/deep learning forecasting methods to predict energy consumption. Although deep learning methods have been successfully applied in this context, their performance can be improved by incorporating statistical features representing the characteristics of time series. This study proposes a novel two-stage forecasting framework composed of data preprocessing and model building. The data preprocessing component extracts statistical features from the input data, and then an XGBoost regressor is utilized to obtain the importance of each feature. The model-building component uses the obtained features and the original input data to construct the forecasting model. We implement three forecasting models based on the proposed approach using two state-of-the-art deep learning models, including the temporal convolution neural network and Multi-head Attention. We empirically evaluate the proposed approach on two renewable energy consumption datasets. The results of experiments indicate that incorporating features is beneficial for temporal convolution neural network-based and Multi-head Attention-based deep learning models performance. This study significantly contributes to the existing models in the literature, as the combined methods improve on their regular variants and the benchmark models.}
}

@article{jounal-trends-ai-energy-consumption,
title = {Trends in AI inference energy consumption: Beyond the performance-vs-parameter laws of deep learning},
journal = {Sustainable Computing: Informatics and Systems},
volume = {38},
pages = {100857},
year = {2023},
issn = {2210-5379},
doi = {https://doi.org/10.1016/j.suscom.2023.100857},
url = {https://www.sciencedirect.com/science/article/pii/S221\-0537923000124},
author = {Radosvet Desislavov and Fernando Martínez-Plumed and José Hernández-Orallo},
keywords = {Artificial Intelligence, Deep learning, Inference, Energy consumption, Performance analysis, Performance evaluation, AI progress},
abstract = {The progress of some AI paradigms such as deep learning is said to be linked to an exponential growth in the number of parameters. There are many studies corroborating these trends, but does this translate into an exponential increase in energy consumption? In order to answer this question we focus on inference costs rather than training costs, as the former account for most of the computing effort, solely because of the multiplicative factors. Also, apart from algorithmic innovations, we account for more specific and powerful hardware (leading to higher FLOPS) that is usually accompanied with important energy efficiency optimisations. We also move the focus from the first implementation of a breakthrough paper towards the consolidated version of the techniques one or two year later. Under this distinctive and comprehensive perspective, we analyse relevant models in the areas of computer vision and natural language processing: for a sustained increase in performance we see a much softer growth in energy consumption than previously anticipated. The only caveat is, yet again, the multiplicative factor, as future AI increases penetration and becomes more pervasive.}
}
@article{quantizationpruningsurvey2021,
  title={Pruning and quantization for deep neural network acceleration: A survey},
  author={Liang, Tailin and Glossner, John and Wang, Lei and Shi, Shaobo and Zhang, Xiaotong},
  journal={Neurocomputing},
  volume={461},
  pages={370--403},
  year={2021},
  publisher={Elsevier}
}
@INPROCEEDINGS{trade-off-accuracy-energy2021,
  author={Brownlee, Alexander E.I and Adair, Jason and Haraldsson, Saemundur O. and Jabbo, John},
  booktitle={2021 IEEE/ACM International Workshop on Genetic Improvement (GI)}, 
  title={Exploring the Accuracy – Energy Trade-off in Machine Learning}, 
  year={2021},
  volume={},
  number={},
  pages={11-18},
  doi={10.1109/GI52543.2021.00011}}
  
@misc{cao2020accurate,
      title={Towards Accurate and Reliable Energy Measurement of NLP Models}, 
      author={Qingqing Cao and Aruna Balasubramanian and Niranjan Balasubramanian},
      year={2020},
      eprint={2010.05248},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}
@inproceedings{wenninger2022how,
  title={How Sustainable is Machine Learning in Energy Applications? – The Sustainable Machine Learning Balance Sheet},
  author={Wenninger, Simon and Kaymakci, Can and Wiethe, Christian and Römmelt, Jörg and Baur, Lukas and Häckel, Björn and Sauer, Alexander},
  booktitle={Wirtschaftsinformatik 2022 Proceedings},
  year={2022},
  volume={1}
}

@article{IKHLASSE20228867,
title = {Recent implications towards sustainable and energy efficient AI and big data implementations in cloud-fog systems: A newsworthy inquiry},
journal = {Journal of King Saud University - Computer and Information Sciences},
volume = {34},
number = {10, Part A},
pages = {8867-8887},
year = {2022},
issn = {1319-1578},
doi = {https://doi.org/10.1016/j.jksuci.2021.11.002},
url = {https://www.sciencedirect.com/science/article/pii/S131\-9157821003025},
author = {Hamzaoui Ikhlasse and Duthil Benjamin and Courboulay Vincent and Medromi Hicham},
keywords = {Energy metrics and measurement tools, Energy efficient implementations, Distributed big data processing, Deep neural networks, Multimodal cloud-fog traffics},
abstract = {Cloud-fog based industries are entailing today greedy energy costs, given the wide multiplication of their AI models and distributed BD frameworks implementations. This paper conducts a newsworthy inquiry purposing to study at which extent IT community are conscious about energy evaluation of their hardware and software implementations, and whether they are in line with sustainable implications toward efficient AI and BD deployments. Through an analysis of responses to the first inquiry questions, we were able to address the residing interoperability between AI models, distributed BD frameworks, and cloud-fog systems. Unfortunately, only 10% of respondents were adopting energy metrics when evaluating their implementations. Even worse, multi-level energy consumption measurement techniques were not evident to most respondents. Accordingly, we provided a useful guideline of various multi-level energy and power estimation approaches. Hereafter, we devoted in accordance with both inquiry and literature results some essential parts for analyzing emerging efficient DNNs and distributed BD implementations. These endeavors were mainly manifested in the form of efficient reconfigurable accelerators designs based on Processing-In-Memory and Processing-Near-Memory architectures. To serve eventually IT community with other tangible solutions, we proposed two roadmaps opening up to the possibility of investigating sustainable actions covering hardware, software, and data levels.}
}
@misc{singularity2017container,
  title={A container platform for scientific computing},
  author={Singularity},
  url={https://sylabs.io/singularity/},
  year={2017}
}

@manual{kaggle:2023,
      title  = "Kaggle.com",
      author = "Kaggle.com",
      year   = "2023, (accessed August 01, 2023)"
    }


@manual{rest-protocol:2022,
      title  = "RESTful Web Services Tutorial",
      author = "Tutorials Point",
      url    = "https://www.tutorialspoint.com/restful/index.htm",
      year   = "2022, (accessed December 06, 2022)"
    }


@manual{mogodb-advantage-nosql:2022,
      title  = "Advantages of NoSQL Databases",
      author = "MongoDB",
      url    = "https://www.mongodb.com/nosql-explained/advantages",
      year   = "2022, (accessed December 06, 2022)"
    }


@manual{ibm-containerization:2022,
      title  = "Containerization",
      author = "IBM Cloud Education",
      url    = "https://www.ibm.com/cloud/learn/containerization",
      year   = "2022, (accessed December 03, 2022)"
    }


@inproceedings{State_Neural_Network_Pruning_2020,
 author = {Blalock, Davis and Gonzalez Ortiz, Jose Javier and Frankle, Jonathan and Guttag, John},
 booktitle = {Proceedings of Machine Learning and Systems},
 editor = {I. Dhillon and D. Papailiopoulos and V. Sze},
 pages = {129--146},
 title = {What is the State of Neural Network Pruning?},
 volume = {2},
 year = {2020}
}

@article{Survey_Theories_Quantized_2018,
  author    = {Yunhui Guo},
  title     = {A Survey on Methods and Theories of Quantized Neural Networks},
  journal   = {CoRR},
  volume    = {abs/1808.04752},
  year      = {2018},
  url       = {http://arxiv.org/abs/1808.04752},
  eprinttype = {arXiv},
  eprint    = {1808.04752},
  timestamp = {Sun, 02 Sep 2018 15:01:54 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1808-04752.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@InProceedings{HAWQ_Quantization_2019,
author = {Dong, Zhen and Yao, Zhewei and Gholami, Amir and Mahoney, Michael W. and Keutzer, Kurt},
title = {HAWQ: Hessian AWare Quantization of Neural Networks With Mixed-Precision},
booktitle = {Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)},
month = {October},
year = {2019}
}

@misc{Survey-Quantization-2021,
  doi = {10.48550/ARXIV.2103.13630},
  url = {https://arxiv.org/abs/2103.13630},
  author = {Gholami, Amir and Kim, Sehoon and Dong, Zhen and Yao, Zhewei and Mahoney, Michael W. and Keutzer, Kurt},
  keywords = {Computer Vision and Pattern Recognition (cs.CV), FOS: Computer and information sciences, FOS: Computer and information sciences},
  title = {A Survey of Quantization Methods for Efficient Neural Network Inference},
  publisher = {arXiv},
  year = {2021},
  copyright = {arXiv.org perpetual, non-exclusive license}
}

@misc{Neural-Network-Quantization-2021,
  doi = {10.48550/ARXIV.2106.08295},
  url = {https://arxiv.org/abs/2106.08295},
  author = {Nagel, Markus and Fournarakis, Marios and Amjad, Rana Ali and Bondarenko, Yelysei and van Baalen, Mart and Blankevoort, Tijmen},
  keywords = {Machine Learning (cs.LG), Artificial Intelligence (cs.AI), Computer Vision and Pattern Recognition (cs.CV), FOS: Computer and information sciences, FOS: Computer and information sciences},
  title = {A White Paper on Neural Network Quantization},
  publisher = {arXiv},
  year = {2021},
  copyright = {Creative Commons Attribution 4.0 International}
}


@article{Carbon-Emissions-2021,
  author    = {David A. Patterson and
               Joseph Gonzalez and
               Quoc V. Le and
               Chen Liang and
               Lluis{-}Miquel Munguia and
               Daniel Rothchild and
               David R. So and
               Maud Texier and
               Jeff Dean},
  title     = {Carbon Emissions and Large Neural Network Training},
  journal   = {CoRR},
  volume    = {abs/2104.10350},
  year      = {2021},
  url       = {https://arxiv.org/abs/2104.10350},
  eprinttype = {arXiv},
  eprint    = {2104.10350},
  timestamp = {Mon, 26 Apr 2021 17:25:10 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-2104-10350.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{Estimationofenergy201975,
title = {Estimation of energy consumption in machine learning},
journal = {Journal of Parallel and Distributed Computing},
volume = {134},
pages = {75-88},
year = {2019},
issn = {0743-7315},
doi = {https://doi.org/10.1016/j.jpdc.2019.07.007},
url = {https://www.sciencedirect.com/science/article/pii/S074\-3731518308773},
author = {Eva García-Martín and Crefeda Faviola Rodrigues and Graham Riley and Håkan Grahn},
keywords = {Machine learning, GreenAI, Energy consumption, Deep learning, High performance computing},
abstract = {Energy consumption has been widely studied in the computer architecture field for decades. While the adoption of energy as a metric in machine learning is emerging, the majority of research is still primarily focused on obtaining high levels of accuracy without any computational constraint. We believe that one of the reasons for this lack of interest is due to their lack of familiarity with approaches to evaluate energy consumption. To address this challenge, we present a review of the different approaches to estimate energy consumption in general and machine learning applications in particular. Our goal is to provide useful guidelines to the machine learning community giving them the fundamental knowledge to use and build specific energy estimation methods for machine learning algorithms. We also present the latest software tools that give energy estimation values, together with two use cases that enhance the study of energy consumption in machine learning.}
}

@article{survey-energy2019,
title = {Estimation of energy consumption in machine learning},
journal = {Journal of Parallel and Distributed Computing},
volume = {134},
pages = {75-88},
year = {2019},
issn = {0743-7315},
doi = {https://doi.org/10.1016/j.jpdc.2019.07.007},
url = {https://www.sciencedirect.com/science/article/pii/S07\-43731518308773},
author = {Eva García-Martín and Crefeda Faviola Rodrigues and Graham Riley and Håkan Grahn},
keywords = {Machine learning, GreenAI, Energy consumption, Deep learning, High performance computing},
abstract = {Energy consumption has been widely studied in the computer architecture field for decades. While the adoption of energy as a metric in machine learning is emerging, the majority of research is still primarily focused on obtaining high levels of accuracy without any computational constraint. We believe that one of the reasons for this lack of interest is due to their lack of familiarity with approaches to evaluate energy consumption. To address this challenge, we present a review of the different approaches to estimate energy consumption in general and machine learning applications in particular. Our goal is to provide useful guidelines to the machine learning community giving them the fundamental knowledge to use and build specific energy estimation methods for machine learning algorithms. We also present the latest software tools that give energy estimation values, together with two use cases that enhance the study of energy consumption in machine learning.}
}

@misc{reddi2020mlperf,
      title={MLPerf Inference Benchmark}, 
      author={Vijay Janapa Reddi and Christine Cheng and David Kanter and Peter Mattson and Guenther Schmuelling and Carole-Jean Wu and Brian Anderson and Maximilien Breughe and Mark Charlebois and William Chou and Ramesh Chukka and Cody Coleman and Sam Davis and Pan Deng and Greg Diamos and Jared Duke and Dave Fick and J. Scott Gardner and Itay Hubara and Sachin Idgunji and Thomas B. Jablin and Jeff Jiao and Tom St. John and Pankaj Kanwar and David Lee and Jeffery Liao and Anton Lokhmotov and Francisco Massa and Peng Meng and Paulius Micikevicius and Colin Osborne and Gennady Pekhimenko and Arun Tejusve Raghunath Rajan and Dilip Sequeira and Ashish Sirasao and Fei Sun and Hanlin Tang and Michael Thomson and Frank Wei and Ephrem Wu and Lingjie Xu and Koichi Yamada and Bing Yu and George Yuan and Aaron Zhong and Peizhao Zhang and Yuchen Zhou},
      year={2020},
      eprint={1911.02549},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@misc{tinyml2021,
      title={Benchmarking TinyML Systems: Challenges and Direction}, 
      author={Colby R. Banbury and Vijay Janapa Reddi and Max Lam and William Fu and Amin Fazel and Jeremy Holleman and Xinyuan Huang and Robert Hurtado and David Kanter and Anton Lokhmotov and David Patterson and Danilo Pau and Jae-sun Seo and Jeff Sieracki and Urmish Thakker and Marian Verhelst and Poonam Yadav},
      year={2021},
      eprint={2003.04821},
      archivePrefix={arXiv},
      primaryClass={cs.PF}
}

@misc{resnet2015,
      title={Deep Residual Learning for Image Recognition}, 
      author={Kaiming He and Xiangyu Zhang and Shaoqing Ren and Jian Sun},
      year={2015},
      eprint={1512.03385},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}

@misc{squeezenet2016,
      title={SqueezeNet: AlexNet-level accuracy with 50x fewer parameters and <0.5MB model size}, 
      author={Forrest N. Iandola and Song Han and Matthew W. Moskewicz and Khalid Ashraf and William J. Dally and Kurt Keutzer},
      year={2016},
      eprint={1602.07360},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}

@misc{googlenet2014,
      title={Going Deeper with Convolutions}, 
      author={Christian Szegedy and Wei Liu and Yangqing Jia and Pierre Sermanet and Scott Reed and Dragomir Anguelov and Dumitru Erhan and Vincent Vanhoucke and Andrew Rabinovich},
      year={2014},
      eprint={1409.4842},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}

@misc{VGGA2015,
      title={Very Deep Convolutional Networks for Large-Scale Image Recognition}, 
      author={Karen Simonyan and Andrew Zisserman},
      year={2015},
      eprint={1409.1556},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}

@misc{over-feet2014,
      title={OverFeat: Integrated Recognition, Localization and Detection using Convolutional Networks}, 
      author={Pierre Sermanet and David Eigen and Xiang Zhang and Michael Mathieu and Rob Fergus and Yann LeCun},
      year={2014},
      eprint={1312.6229},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}

@misc{AlexNet2,
      title={One weird trick for parallelizing convolutional neural networks}, 
      author={Alex Krizhevsky},
      year={2014},
      eprint={1404.5997},
      archivePrefix={arXiv},
      primaryClass={cs.NE}
}

@article{federatedml:2019,
  title={Federated machine learning: Concept and applications},
  author={Yang, Qiang and Liu, Yang and Chen, Tianjian and Tong, Yongxin},
  journal={ACM Transactions on Intelligent Systems and Technology (TIST)},
  volume={10},
  number={2},
  pages={1--19},
  year={2019},
  publisher={ACM New York, NY, USA}
}
@misc{accuratereliable:2020,
      title={Towards Accurate and Reliable Energy Measurement of NLP Models}, 
      author={Qingqing Cao and Aruna Balasubramanian and Niranjan Balasubramanian},
      year={2020},
      eprint={2010.05248},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}
@misc{towardssystematic:2020,
      title={Towards the Systematic Reporting of the Energy and Carbon Footprints of Machine Learning}, 
      author={Peter Henderson and Jieru Hu and Joshua Romoff and Emma Brunskill and Dan Jurafsky and Joelle Pineau},
      year={2020},
      eprint={2002.05651},
      archivePrefix={arXiv},
      primaryClass={cs.CY}
}

@article{canfederated:2020,
  author       = {Xinchi Qiu and
                  Titouan Parcollet and
                  Daniel J. Beutel and
                  Taner Topal and
                  Akhil Mathur and
                  Nicholas D. Lane},
  title        = {A first look into the carbon footprint of federated learning},
  journal      = {CoRR},
  volume       = {abs/2010.06537},
  year         = {2020},
  url          = {https://arxiv.org/abs/2010.06537},
  eprinttype    = {arXiv},
  eprint       = {2010.06537},
  timestamp    = {Tue, 20 Oct 2020 15:08:10 +0200},
  biburl       = {https://dblp.org/rec/journals/corr/abs-2010-06537.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@misc{systematicreport:2020,
      title={Towards the Systematic Reporting of the Energy and Carbon Footprints of Machine Learning}, 
      author={Peter Henderson and Jieru Hu and Joshua Romoff and Emma Brunskill and Dan Jurafsky and Joelle Pineau},
      year={2020},
      eprint={2002.05651},
      archivePrefix={arXiv},
      primaryClass={cs.CY}
}

@manual{mitreview:2019,
      title  = "MIT Technology Review",
      author = "Karen Hao",
      year   = "2019"
    }
@manual{paperswithcode:2020,
      title  = "Image Classification",
      author = "Papers With Code",
      url    = "https://paperswithcode.com/task/image-classification",
      year   = "2020, (accessed August 8, 2020)"
    }

@manual{nvidia-smi-properties:2022,
      title  = "nvidia-smi - NVIDIA System Management Interface program",
      author = "Nvidia",
      url    = "https://developer.download.nvidia.com/compute/DCGM/doc\-s/nvidia-smi-367.38.pdf",
      year   = "2022, (accessed October, 2022)"
    }


@manual{paperswithcode-list-classification:2021,
      title  = "Image Classification on ImageNet",
      author = "Papers With Code",
      url    = "https://paperswithcode.com/sota/image-classification-on-imagenet",
      year   = "2021, (accessed May 4, 2020)"
    }


@manual{nvidia-smi:2020,
      title  = "NVIDIA System Management Interface",
      author = "NVIDIA",
      url    = "https://developer.nvidia.com/nvidia-system-management\-interfa",
      year   = "2020, (accessed August 11, 2020)"
    }

@manual{datacenterpower:2017-2020,
      title  = "Power in the Data Center and its Cost Across the U.S.",
      author = "Michael Rareshide",
      url    = "https://info.siteselectiongroup.com/blog/power-in-the-data-center-and-its-costs-across-the-united-states",
      year   = "2017, (accessed August 11, 2020)"
    }

@manual{arangodb:2021,
      title  = "ArangoDB.",
      author = "ArangoDB",
      url    = "https://www.arangodb.com",
      year   = "2021, (accessed March,30, 2021)"
    }
    
@INPROCEEDINGS{energy-efficiency:2016,
  author={D. {Li} and X. {Chen} and M. {Becchi} and Z. {Zong}},
  booktitle={2016 IEEE International Conferences on Big Data and Cloud Computing (BDCloud), Social Computing and Networking (SocialCom), Sustainable Computing and Communications (SustainCom) (BDCloud-SocialCom-SustainCom)}, 
  title={Evaluating the Energy Efficiency of Deep Convolutional Neural Networks on CPUs and GPUs}, 
  year={2016},
  volume={},
  number={},
  pages={477-484},}
  
  @INPROCEEDINGS{towards-power:2019,  author={M. {Hodak} and M. {Gorkovenko} and A. {Dholakia}},  booktitle={2019 IEEE International Conference on Big Data (Big Data)},   title={Towards Power Efficiency in Deep Learning on Data Center Hardware},   year={2019},  volume={},  number={},  pages={1814-1820},}
  
  @INPROCEEDINGS{evaluating-energy-efficiency:2016,
  author={D. {Li} and X. {Chen} and M. {Becchi} and Z. {Zong}},
  booktitle={2016 IEEE International Conferences on Big Data and Cloud Computing (BDCloud), Social Computing and Networking (SocialCom), Sustainable Computing and Communications (SustainCom) (BDCloud-SocialCom-SustainCom)}, 
  title={Evaluating the Energy Efficiency of Deep Convolutional Neural Networks on CPUs and GPUs}, 
  year={2016},
  volume={},
  number={},
  pages={477-484},}
  
  @article{Energy-based-tuning:2018,
  author    = {Francisco M. Castro and
               Nicol{\'{a}}s Guil and
               Manuel J. Mar{\'{\i}}n{-}Jim{\'{e}}nez and
               Jes{\'{u}}s P{\'{e}}rez Serrano and
               Manuel Ujald{\'{o}}n},
  title     = {Energy-based Tuning of Convolutional Neural Networks on Multi-GPUs},
  journal   = {CoRR},
  volume    = {abs/1808.00286},
  year      = {2018},
  url       = {http://arxiv.org/abs/1808.00286},
  archivePrefix = {arXiv},
  eprint    = {1808.00286},
  timestamp = {Sun, 02 Sep 2018 15:01:56 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1808-00286.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{Energy-policy-NLP:2019,
  author    = {Emma Strubell and
               Ananya Ganesh and
               Andrew McCallum},
  title     = {Energy and Policy Considerations for Deep Learning in {NLP}},
  journal   = {CoRR},
  volume    = {abs/1906.02243},
  year      = {2019},
  url       = {http://arxiv.org/abs/1906.02243},
  archivePrefix = {arXiv},
  eprint    = {1906.02243},
  timestamp = {Thu, 13 Jun 2019 13:36:00 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1906-02243.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
  }
  
  @article{Impact-GPU-DVFS:2019,
  author    = {Zhenheng Tang and
               Yuxin Wang and
               Qiang Wang and
               Xiaowen Chu},
  title     = {The Impact of {GPU} {DVFS} on the Energy and Performance of Deep Learning:
               an Empirical Study},
  journal   = {CoRR},
  volume    = {abs/1905.11012},
  year      = {2019},
  url       = {http://arxiv.org/abs/1905.11012},
  archivePrefix = {arXiv},
  eprint    = {1905.11012},
  timestamp = {Mon, 03 Jun 2019 13:42:33 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1905-11012.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}


@inproceedings{tradeoff-energy-accuracy:2021,
  title={Exploring the Accuracy-Energy Trade-off in Machine Learning},
  author={Brownlee, Alexander and Adair, Jason and Haraldsson, Saemundur and Jabbo, John},
  booktitle={Genetic Improvement Workshop at 43rd International Conference on Software Engineering},
  year={2021},
  organization={ACM}
}
@article{Designing-Energy-Efficient:2016,
  author    = {Tien{-}Ju Yang and
               Yu{-}Hsin Chen and
               Vivienne Sze},
  title     = {Designing Energy-Efficient Convolutional Neural Networks using Energy-Aware
               Pruning},
  journal   = {CoRR},
  volume    = {abs/1611.05128},
  year      = {2016},
  url       = {http://arxiv.org/abs/1611.05128},
  archivePrefix = {arXiv},
  eprint    = {1611.05128},
  timestamp = {Mon, 13 Aug 2018 16:46:20 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/YangCS16a.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{speed-accuracy:2016,
  author    = {Jonathan Huang and
               Vivek Rathod and
               Chen Sun and
               Menglong Zhu and
               Anoop Korattikara and
               Alireza Fathi and
               Ian Fischer and
               Zbigniew Wojna and
               Yang Song and
               Sergio Guadarrama and
               Kevin Murphy},
  title     = {Speed/accuracy trade-offs for modern convolutional object detectors},
  journal   = {CoRR},
  volume    = {abs/1611.10012},
  year      = {2016},
  url       = {http://arxiv.org/abs/1611.10012},
  archivePrefix = {arXiv},
  eprint    = {1611.10012},
  timestamp = {Mon, 13 Aug 2018 16:48:56 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/HuangRSZKFFWSG016.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{Low-Power-Computer-Vision:2019,
  author    = {Sergei Alyamkin and
               Matthew Ardi and
               Alexander C. Berg and
               Achille Brighton and
               Bo Chen and
               Yiran Chen and
               Hsin{-}Pai Cheng and
               Zichen Fan and
               Chen Feng and
               Bo Fu and
               Kent Gauen and
               Abhinav Goel and
               Alexander Goncharenko and
               Xuyang Guo and
               Soonhoi Ha and
               Andrew Howard and
               Xiao Hu and
               Yuanjun Huang and
               Donghyun Kang and
               Jaeyoun Kim and
               Jong{-}gook Ko and
               Alexander Kondratyev and
               Junhyeok Lee and
               Seungjae Lee and
               Suwoong Lee and
               Zichao Li and
               Zhiyu Liang and
               Juzheng Liu and
               Xin Liu and
               Yang Lu and
               Yung{-}Hsiang Lu and
               Deeptanshu Malik and
               Hong Hanh Nguyen and
               Eunbyung Park and
               Denis Repin and
               Liang Shen and
               Tao Sheng and
               Fei Sun and
               David Svitov and
               George K. Thiruvathukal and
               Baiwu Zhang and
               Jingchi Zhang and
               Xiaopeng Zhang and
               Shaojie Zhuo},
  title     = {Low-Power Computer Vision: Status, Challenges, Opportunities},
  journal   = {CoRR},
  volume    = {abs/1904.07714},
  year      = {2019},
  url       = {http://arxiv.org/abs/1904.07714},
  archivePrefix = {arXiv},
  eprint    = {1904.07714},
  timestamp = {Thu, 12 Dec 2019 14:21:34 +0100},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1904-07714.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@INPROCEEDINGS{mobilenet:2009,
  author={J. {Deng} and W. {Dong} and R. {Socher} and L. {Li} and  {Kai Li} and  {Li Fei-Fei}},
  booktitle={2009 IEEE Conference on Computer Vision and Pattern Recognition}, 
  title={ImageNet: A large-scale hierarchical image database}, 
  year={2009},
  volume={},
  number={},
  pages={248-255},}
  
  @article{alexnet:2012,
author = {Krizhevsky, Alex and Sutskever, Ilya and Hinton, Geoffrey E.},
title = {ImageNet Classification with Deep Convolutional Neural Networks},
year = {2017},
issue_date = {June 2017},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {60},
number = {6},
issn = {0001-0782},
url = {https://doi.org/10.1145/3065386},
doi = {10.1145/3065386},
journal = {Commun. ACM},
month = may,
pages = {84–90},
numpages = {7}
}
 
 @INPROCEEDINGS{imagnet:2009,
  author={J. {Deng} and W. {Dong} and R. {Socher} and L. {Li} and  {Kai Li} and  {Li Fei-Fei}},
  booktitle={2009 IEEE Conference on Computer Vision and Pattern Recognition}, 
  title={ImageNet: A large-scale hierarchical image database}, 
  year={2009},
  volume={},
  number={},
  pages={248-255},}

@inproceedings{Deep-Learning-COTS-HPC-Systems:13,
author = {Coates, Adam and Huval, Brody and Wang, Tao and Wu, David J. and Ng, Andrew Y. and Catanzaro, Bryan},
title = {Deep Learning with COTS HPC Systems},
year = {2013},
publisher = {JMLR.org},
booktitle = {Proceedings of the 30th International Conference on International Conference on Machine Learning - Volume 28},
pages = {III–1337–III–1345},
numpages = {9},
location = {Atlanta, GA, USA},
series = {ICML’13}
}

@manual{mnist:1998,
      title  = "THE MNIST DATABASE of handwritten digits",
      author = {LeCun, Yann and Cortes, Corinna and J.C. Burges, Christopher},
      url    = "http://yann.lecun.com/exdb/mnist/",
      year   = "1998, (accessed August 18, 2020)"
}

@manual{cifar10,
      title  = "CIFAR10 small images classification dataset",
      author = "Keras",
      url    = "https://keras.io/api/datasets/cifar10/",
      year   = "accessed August 19, 2020"
}

@manual{cifar100,
      title  = "CIFAR100 small images classification dataset",
      author = "Keras",
      url    = "https://keras.io/api/datasets/cifar100/",
      year   = "accessed August 19, 2020"
}

@manual{NVIDIA:Doing-more,
      title  = "Doing more with less of a scarce resource",
      author = "NVIDIA",
      url    = "https://www.nvidia.com.br/object/gcr-energy-efficiency.html",
      year   = "accessed August 22, 2020"
}

@article{Deep-Speech-2:2015,
  author    = {Dario Amodei and
               Rishita Anubhai and
               Eric Battenberg and
               Carl Case and
               Jared Casper and
               Bryan Catanzaro and
               Jingdong Chen and
               Mike Chrzanowski and
               Adam Coates and
               Greg Diamos and
               Erich Elsen and
               Jesse H. Engel and
               Linxi Fan and
               Christopher Fougner and
               Tony Han and
               Awni Y. Hannun and
               Billy Jun and
               Patrick LeGresley and
               Libby Lin and
               Sharan Narang and
               Andrew Y. Ng and
               Sherjil Ozair and
               Ryan Prenger and
               Jonathan Raiman and
               Sanjeev Satheesh and
               David Seetapun and
               Shubho Sengupta and
               Yi Wang and
               Zhiqian Wang and
               Chong Wang and
               Bo Xiao and
               Dani Yogatama and
               Jun Zhan and
               Zhenyao Zhu},
  title     = {Deep Speech 2: End-to-End Speech Recognition in English and Mandarin},
  journal   = {CoRR},
  volume    = {abs/1512.02595},
  year      = {2015},
  url       = {http://arxiv.org/abs/1512.02595},
  archivePrefix = {arXiv},
  eprint    = {1512.02595},
  timestamp = {Mon, 22 Jul 2019 13:51:23 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/AmodeiABCCCCCCD15.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{End-to-End-Learning-for-Self-Driving-Cars:2016,
  author    = {Mariusz Bojarski and
               Davide Del Testa and
               Daniel Dworakowski and
               Bernhard Firner and
               Beat Flepp and
               Prasoon Goyal and
               Lawrence D. Jackel and
               Mathew Monfort and
               Urs Muller and
               Jiakai Zhang and
               Xin Zhang and
               Jake Zhao and
               Karol Zieba},
  title     = {End to End Learning for Self-Driving Cars},
  journal   = {CoRR},
  volume    = {abs/1604.07316},
  year      = {2016},
  url       = {http://arxiv.org/abs/1604.07316},
  archivePrefix = {arXiv},
  eprint    = {1604.07316},
  timestamp = {Mon, 13 Aug 2018 16:47:06 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/BojarskiTDFFGJM16.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{nlp:2018,
  author    = {Emma Strubell and
               Patrick Verga and
               Daniel Andor and
               David Weiss and
               Andrew McCallum},
  title     = {Linguistically-Informed Self-Attention for Semantic Role Labeling},
  journal   = {CoRR},
  volume    = {abs/1804.08199},
  year      = {2018},
  url       = {http://arxiv.org/abs/1804.08199},
  archivePrefix = {arXiv},
  eprint    = {1804.08199},
  timestamp = {Mon, 13 Aug 2018 16:49:13 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1804-08199.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
} 

@misc{inference-mobile-app:2019,
    title={Characterizing the Deep Neural Networks Inference Performance of Mobile Applications},
    author={Samuel S. Ogden and Tian Guo},
    year={2019},
    eprint={1909.04783},
    archivePrefix={arXiv},
    primaryClass={cs.DC}
}
@article{docker:2014,
author = {Merkel, Dirk},
title = {Docker: Lightweight Linux Containers for Consistent Development and Deployment},
year = {2014},
issue_date = {March 2014},
publisher = {Belltown Media},
address = {Houston, TX},
volume = {2014},
number = {239},
issn = {1075-3583},
abstract = {Docker promises the ability to package applications and their dependencies into lightweight containers that move easily between different distros, start up quickly and are isolated from each other.},
journal = {Linux J.},
month = mar,
articleno = {2},
numpages = {1}
}


@manual{trends-google:2020,
      title  = "Image Classification",
      author = "Papers With Code",
      url    = "https://trends.google.com",
      year   = "2020, (accessed August 8, 2020)"
    }

@manual{singularity:2020,
      author = "Sylabs",
      url    = "https://sylabs.io/",
      year   = "2020, (accessed August 8, 2020)",
      title = "Singularity Container Technology \& Services"
    }
@manual{docker:2020,
      author = "Docker",
      url    = "https://docker.com/",
      year   = "2020, (accessed August 8, 2020)",
      title = "Docker: Accelerated Container Application Development"
    }

@article{confusion-matrix-definition,
title = "A systematic analysis of performance measures for classification tasks",
journal = "Information Processing & Management",
volume = "45",
number = "4",
pages = "427 - 437",
year = "2009",
issn = "0306-4573",
doi = "https://doi.org/10.1016/j.ipm.2009.03.002",
url = "http://www.sciencedirect.com/science/article/pii/S0306457309000259",
author = "Marina Sokolova and Guy Lapalme",
keywords = "Performance evaluation, Machine Learning, Text classification",
}

@article{confusion-matrix-definition-2,
title = "Three-way confusion matrix for classification: A measure driven view",
journal = "Information Sciences",
volume = "507",
pages = "772 - 794",
year = "2020",
issn = "0020-0255",
doi = "https://doi.org/10.1016/j.ins.2019.06.064",
url = "http://www.sciencedirect.com/science/article/pii/S0020025519306024",
author = "Jianfeng Xu and Yuanjian Zhang and Duoqian Miao",
keywords = "Three-way decisions, Confusion matrix, Measure, Classification, Rough set theory",
}

@manual{JPEG-compress:2020,
      author = "Docker",
      url    = "https://www.image-engineering.de/library/technotes/745-how-does-the-jpeg-compression-work",
      year   = "2020, (accessed November 3, 2020)"
    }

