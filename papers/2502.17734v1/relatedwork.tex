\section{Related works}
\label{sec:related-works}
In this section, a systematic review of papers and surveys was carried out to identify the main approaches related to the energy consumption of neural networks. The selection of articles was made through a search on Google Scholar using the related search strings below:

 \begin{itemize}
    \item ("Neural Networks" OR "Deep Learning") AND "Energy Consumption"
    \item ("Machine Learning" OR "Artificial Intelligence") AND ("Energy Consumption" OR "Power Consumption")
    \item ("GPU" OR "CPU") AND "Energy Efficiency"
    \item ("Neural Networks" OR "Deep Learning") AND ("Power Consumption" OR "Power Estimation")
    \item ("Artificial Intelligence" OR "Machine Learning") AND ("Energy Estimation" OR "Power Estimation")
\end{itemize}
The selection of the articles was made through the reading of the titles, summary, and, finally, the introduction. The selected papers were grouped into the following categories:
 \begin{itemize}
    \item Energy-measurement methods and tools
    \item Network compression approaches
    \item Training-parameter modification approaches
    \item Hardware-parameter modification methods
    \item Energy consumption studies without intervention
\end{itemize}

This Section places a greater emphasis on works falling under the category of Energy Measurement Methods and Tools, as their approach aligns more closely with Phoeni6. Five such works are subject to analysis and comparison with Phoeni6. While works in other categories maintain relevance due to their adaptability to the Phoeni6 model, only a single exemplary work from each category is spotlighted to showcase the potential of the Phoeni6 approach.

\subsection{Energy Measurement: Methods and Tools}

The works described in the following sub-subsections~\cite{liu2022energyprofiling,li2022neuralpower,chen2021powermeter,zhang2021energyefficient,wang2020systematic} address challenges similar to those addressed by Phoeni6, such as the need for reproducible and fair comparison of energy consumption results. They also provide different approaches to energy consumption evaluation, such as using hardware-aware power measurement techniques or developing frameworks for automated profiling. Table~\ref{tab:comparation-2} compares Phoeni6 and these works.

\begin{table*}[htbp]
\centering
\caption{Feature comparison between Phoeni6 and other frameworks, emphasizing advantages in portability and automation. EP = EnergyProfilin, NP = NeuralPower, PM = PowerMete.}
\begin{tabular}{|c|c|c|c|c|c|c|}
\hline
Feature & Phoeni6 & EP ~\cite{liu2022energyprofiling}& NP\cite{li2022neuralpower} & PM \cite{chen2021powermeter}& Energy-Efficient \cite{zhang2021energyefficient} & Systematic \cite{wang2020systematic}\\
\hline
Portability & Yes & Yes & Yes & Yes & No & No \\\hline
Automation & Yes & No & No & No & No & No \\\hline
Transparency & Yes & No & No & No & No & No \\\hline
Coordination & Yes & No & No & No & No & No \\\hline
Centralized DB & Yes & No & No & No & No & No \\\hline
Multiplatform & Yes & No & Yes & Yes & No & No \\\hline
Data granularity & Yes & No & Yes & Yes & No & No \\\hline
Complex analytical investigation & Yes & No & Yes & Yes & No & No \\\hline
General-purpose & Yes & No & Yes & Yes & Yes & Yes \\
\hline
\end{tabular}
\label{tab:comparation-2}
\end{table*}

\subsubsection{EnergyProfiling \cite{liu2022energyprofiling}} EnergyProfiling is a user-friendly framework for energy profiling of deep neural networks on mobile devices, offering tools to collect and analyze energy consumption data across various neural network models and hardware platforms. While it provides portability and ease of use, it lacks automated profiling, transparency, and coordination, requiring manual data collection and analysis, potentially leading to time-consuming and error-prone processes. Moreover, the absence of a centralized database for sharing energy consumption data hinders comparing findings across different studies.

\subsubsection{NeuralPower \cite{li2022neuralpower}} NeuralPower offers an efficient and accurate framework for automated power profiling of neural networks, employing hardware-aware power measurement techniques to gather data across various models and hardware platforms. Nevertheless, it lacks transparency and coordination, necessitating adaptation by researchers for specific use cases. Furthermore, the absence of a centralized database for sharing collected energy consumption data via NeuralPower poses challenges in comparing results across different studies.

\subsubsection{PowerMeter \cite{chen2021powermeter}} PowerMeter, a hardware-aware power measurement framework for deep neural networks, equips researchers with tools to collect and analyze power consumption data across a range of neural network models and hardware platforms. Its focus on accuracy and comprehensiveness renders it an ideal choice for in-depth power analysis. Nevertheless, PowerMeter lacks transparency and coordination, requiring adaptation for those aiming to assess energy consumption in mobile devices. Furthermore, the absence of a central database for sharing energy consumption data gathered through PowerMeter complicates the comparison of results among various research studies.

\subsubsection{Energy-Efficient Neural Network Design: A Survey \cite{zhang2021energyefficient}} The survey offers a comprehensive overview of energy-efficient neural network design techniques, encompassing model compression, pruning, and hardware acceleration, thus serving as a valuable resource for researchers and developers interested in designing energy-efficient neural networks. However, it falls short in providing tools or frameworks for directly assessing energy efficiency, placing the onus on researchers to develop their tools, a potentially time-consuming and challenging endeavor. Furthermore, the absence of a centralized database for sharing energy consumption data across various studies complicates the comparative analysis of results among different research groups.

\subsubsection{A Systematic Evaluation of Energy Efficiency of Neural Networks \cite{wang2020systematic}} The systematic evaluation assesses the energy efficiency of diverse neural network models and hardware platforms using various benchmarks and metrics, encompassing factors like inference time, throughput, and power consumption, yielding valuable insights into enhancing neural network energy efficiency. Nevertheless, it lacks a prescribed methodology for the systematic collection and analysis of energy consumption data, placing the onus on researchers to develop their own approaches, which can be laborious and prone to errors. Furthermore, the absence of a centralized database for sharing energy consumption data from the evaluation hampers comparing results across different studies.

\subsection{Additional Research Categories}
\label{subsec:other-categories}

Here, we present a representative example of one work from each category to illustrate how Phoeni6 can support a variety of approaches with relative ease.

\subsubsection{Network compression approaches} 
% Amir 
Gholam et al.~\cite{Survey-Quantization-2021} presents a comprehensive overview of quantization methods for efficient neural network inference, categorizing these methods into fixed-point, variable-point, and dynamic quantization, discussing the merits and drawbacks of each category and providing specific examples. Phoeni6 could enhance this work in several ways, such as collecting data on the energy consumption of different quantization methods, including fixed-point, variable-point, and dynamic quantization, across various neural networks and hardware platforms. This data would validate the claims in the article and identify the most energy-efficient quantization methods for diverse applications.
%
A few other works from our systematic survey fall in this category~\cite{zhou2016dorefa,gong2019differentiable,dotzel2023fliqs}.

\subsubsection{Training parameter modification approaches}
% Alexander E.I. 
Brownlee et al.~\cite{trade-off-accuracy-energy2021} provide a comprehensive overview of the factors that affect the accuracy and energy consumption of machine learning models, focusing on neural networks. Phoeni6 supports this work through several approaches: it can collect data on the accuracy and energy consumption of different machine learning models on various hardware platforms, helping to validate claims and identify the most energy-efficient models for specific applications and platforms. 
%
In addition to the works mentioned above, our systematic survey identified the following works as relevant~\cite{patterson2021carbon,mehlin2023towards,metz2020tasks}.
    
\subsubsection{Hardware parameter modification methods}
% Zhenheng 
Tang et al.~\cite{Impact-GPU-DVFS:2019} investigated the effects of GPU dynamic voltage and frequency scaling (DVFS) on the energy and performance of deep learning workloads.
Phoeni6 could establish a general framework for collecting, analyzing, and comparing DVFS impact data, promoting the development of new techniques for enhancing the energy and performance of deep learning workloads. Overall, Phoeni6 has the potential to be a valuable tool for supporting research related to DVFS and its impact on deep learning workloads by enabling data collection, tool development, and data sharing among researchers.
%
Our systematic survey also found the following works relevant~\cite{tang2019impact,mei2017survey,mei2013measurement}.

\subsubsection{Energy consumption studies without any intervention:} 
% Francisco M. 
Castro et al.~\cite{Energy-based-tuning:2018} presented a novel framework for reducing the energy consumption of convolutional neural networks (CNNs) on multi-GPUs.
Phoeni6 can offer substantial support to this research. First, it can collect data on the energy consumption of different CNNs on various multi-GPU platforms, enabling validation of energy consumption models and identification of opportunities for energy reduction. Additionally, Phoeni6 can be used to develop tools that automate the energy-based tuning process for CNNs, considering layer-specific energy consumption, communication overhead, and accuracy thresholds, simplifying the utilization of the proposed framework for energy-efficient CNN tuning.
%
Other relevant works from our systematic survey include~\cite{yao2021evaluating,yao2021evaluating,sun2021evaluating,zhao2023sustainable}.

\subsubsection{Final Analysis:} 

Table~\ref{tab:comparation-2} highlights several key characteristics that underscore the relevance of Phoeni6 within its contextual framework. However, as discussed in Section~\ref{sec:methodology} and briefly examined through the case studies in Sections ~\ref{sec:case-study} and~\ref{sec:case-study-2}, initiating an evaluation requires prior configurations. These configurations demand that the researcher possess a thorough understanding of both the underlying concepts and the preparatory activities, such as:

\begin{itemize}
    \item Containerization of new networks to be used in the evaluations;
    \item Containerization of new collectors when they differ from those already containerized, as is the case with nvidia-smi;
    \item Proper identification of devices to be utilized, such as GPUs;
    \item A working knowledge of the database to facilitate the replication of evaluations.
\end{itemize}