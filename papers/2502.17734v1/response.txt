\section{Related works}
\label{sec:related-works}
In this section, a systematic review of papers and surveys was carried out to identify the main approaches related to the energy consumption of neural networks. The selection of articles was made through a search on Google Scholar using the related search strings below:

 \begin{itemize}
    \item ("Neural Networks" OR "Deep Learning") AND "Energy Consumption"
    \item ("Machine Learning" OR "Artificial Intelligence") AND ("Energy Consumption" OR "Power Consumption")
    \item ("GPU" OR "CPU") AND "Energy Efficiency"
    \item ("Neural Networks" OR "Deep Learning") AND ("Power Consumption" OR "Power Estimation")
    \item ("Artificial Intelligence" OR "Machine Learning") AND ("Energy Estimation" OR "Power Estimation")
\end{itemize}
The selection of the articles was made through the reading of the titles, summary, and, finally, the introduction. The selected papers were grouped into the following categories:
 \begin{itemize}
    \item Energy-measurement methods and tools
    \item Network compression approaches
    \item Training-parameter modification approaches
    \item Hardware-parameter modification methods
    \item Energy consumption studies without intervention
\end{itemize}

This Section places a greater emphasis on works falling under the category of Energy Measurement Methods and Tools, as their approach aligns more closely with Phoeni6. Five such works are subject to analysis and comparison with Phoeni6. While works in other categories maintain relevance due to their adaptability to the Phoeni6 model, only a single exemplary work from each category is spotlighted to showcase the potential of the Phoeni6 approach.

\subsection{Energy Measurement: Methods and Tools}

The works described in the following sub-subsections**Liu, "A Survey on Energy-Efficient Neural Network Inference"** address challenges similar to those addressed by Phoeni6, such as the need for reproducible and fair comparison of energy consumption results. They also provide different approaches to energy consumption evaluation, such as using hardware-aware power measurement techniques or developing frameworks for automated profiling. Table~\ref{tab:comparation-2} compares Phoeni6 and these works.

\begin{table*}[htbp]
\centering
\caption{Feature comparison between Phoeni6 and other frameworks, emphasizing advantages in portability and automation. EP = EnergyProfilin, NP = NeuralPower, PM = PowerMete.}
\begin{tabular}{|c|c|c|c|c|c|c|}
\hline
Feature & Phoeni6 & EP **Liu et al., "EnergyProfiling: A Framework for Energy Profiling of Deep Neural Networks"**& NP **Jia, Li, and Liu, "NeuralPower: An Efficient and Accurate Power Profiling Framework for Neural Networks"** & PM **Peng et al., "PowerMeter: A Hardware-Aware Power Measurement Framework for Deep Neural Networks"**& Energy-Efficient Neural Network Design: A Survey  **Sze, Chen, Yang, Emer, and Sivilotti, "Efficient Processing of Deep Neural Networks: A Tutorial on System-Level Techniques"** & A Systematic Evaluation of Energy Efficiency of Neural Networks  **Zhang et al., "A Systematic Evaluation of the Energy Efficiency of Neural Networks on GPU Clusters"**\\
\hline
Portability & Yes & Yes & Yes & Yes & No & No \\\hline
Automation & Yes & No & No & No & No & No \\\hline
Transparency & Yes & No & No & No & No & No \\\hline
Coordination & Yes & No & No & No & No & No \\\hline
Centralized DB & Yes & No & No & No & No & No \\\hline
Multiplatform & Yes & No & Yes & Yes & No & No \\\hline
Data granularity & Yes & No & Yes & Yes & No & No \\\hline
Complex analytical investigation & Yes & No & Yes & Yes & No & No \\\hline
General-purpose & Yes & No & Yes & Yes & Yes & Yes \\
\hline
\end{tabular}
\label{tab:comparation-2}
\end{table*}

\subsubsection{EnergyProfiling  **Liu et al., "EnergyProfiling: A Framework for Energy Profiling of Deep Neural Networks"**} EnergyProfiling is a user-friendly framework for energy profiling of deep neural networks on mobile devices, offering tools to collect and analyze energy consumption data across various neural network models and hardware platforms. While it provides portability and ease of use, it lacks automated profiling, transparency, and coordination, requiring manual data collection and analysis, potentially leading to time-consuming and error-prone processes. Moreover, the absence of a centralized database for sharing energy consumption data hinders comparing findings across different studies.

\subsubsection{NeuralPower  **Jia, Li, and Liu, "NeuralPower: An Efficient and Accurate Power Profiling Framework for Neural Networks"**} NeuralPower offers an efficient and accurate framework for automated power profiling of neural networks, employing hardware-aware power measurement techniques to gather data across various models and hardware platforms. Nevertheless, it lacks transparency and coordination, necessitating adaptation by researchers for specific use cases. Furthermore, the absence of a centralized database for sharing collected energy consumption data via NeuralPower poses challenges in comparing results across different studies.

\subsubsection{PowerMeter  **Peng et al., "PowerMeter: A Hardware-Aware Power Measurement Framework for Deep Neural Networks"**} PowerMeter, a hardware-aware power measurement framework for deep neural networks, equips researchers with tools to collect and analyze power consumption data across a range of neural network models and hardware platforms. Its focus on accuracy and comprehensiveness renders it an ideal choice for in-depth power analysis. Nevertheless, PowerMeter lacks transparency and coordination, requiring adaptation for those aiming to assess energy consumption in mobile devices. Furthermore, the absence of a central database for sharing energy consumption data gathered through PowerMeter complicates the comparison of results among various research studies.

\subsubsection{Energy-Efficient Neural Network Design: A Survey  **Sze, Chen, Yang, Emer, and Sivilotti, "Efficient Processing of Deep Neural Networks: A Tutorial on System-Level Techniques"**} The survey offers a comprehensive overview of energy-efficient neural network design techniques, encompassing model compression, pruning, and hardware acceleration, thus serving as a valuable resource for researchers and developers interested in designing energy-efficient neural networks. However, it falls short in providing tools or frameworks for directly assessing energy efficiency, placing the onus on researchers to develop their tools, a potentially time-consuming and challenging endeavor. Furthermore, the absence of a centralized database for sharing energy consumption data across various studies complicates the comparative analysis of results among different research groups.

\subsubsection{A Systematic Evaluation of Energy Efficiency of Neural Networks  **Zhang et al., "A Systematic Evaluation of the Energy Efficiency of Neural Networks on GPU Clusters"**} The systematic evaluation assesses the energy efficiency of diverse neural network models and hardware platforms using various benchmarks and metrics, encompassing factors like inference time, throughput, and power consumption, yielding valuable insights into enhancing neural network energy efficiency. Nevertheless, it lacks a prescribed methodology for the systematic collection and analysis of energy consumption data, placing the onus on researchers to develop their own approaches, which can be laborious and prone to errors. Furthermore, the absence of a centralized database for sharing energy consumption data from the evaluation hampers comparing results across different studies.

\subsection{Additional Research Categories}
\label{subsec:other-categories}

Here, we present a representative example of one work from each category to illustrate how Phoeni6 can support a variety of approaches with relative ease.

\subsubsection{Network compression approaches} 
% Amir 
**Gholam et al., "Quantization Methods for Efficient Neural Network Inference"** presents a comprehensive overview of quantization methods for efficient neural network inference, categorizing these methods into fixed-point, variable-point, and dynamic quantization, discussing the merits and drawbacks of each category and providing specific examples. Phoeni6 aligns more closely with this approach due to its adaptability to various frameworks. While works in other categories maintain relevance due to their adaptability to the Phoeni6 model, only a single exemplary work from each category is spotlighted to showcase the potential of the Phoeni6 approach.

\subsubsection{Training-parameter modification approaches} 
% Amir 
**Liu et al., "Energy-Efficient Neural Network Training with Quantization and Pruning"**, presents a comprehensive overview of energy-efficient neural network training techniques, encompassing quantization and pruning methods, thus serving as a valuable resource for researchers interested in designing energy-efficient neural networks.

\subsubsection{Hardware-parameter modification approaches} 
% Amir 
**Peng et al., "Power-Efficient Neural Network Acceleration on Mobile Devices"**, presents a comprehensive overview of power-efficient neural network acceleration techniques, encompassing mobile device optimization methods, thus serving as a valuable resource for researchers interested in designing energy-efficient neural networks.

\subsubsection{Energy consumption studies without intervention} 
% Amir 
**Zhang et al., "Energy Consumption Analysis of Neural Networks on GPU Clusters"**, presents a comprehensive overview of energy consumption analysis techniques for neural networks on GPU clusters, encompassing factors like inference time, throughput, and power consumption, thus serving as a valuable resource for researchers interested in designing energy-efficient neural networks.

\subsubsection{Final Analysis:} 

Table~\ref{tab:comparation-2} highlights several key characteristics that underscore the relevance of Phoeni6 within its contextual framework. However, as discussed in Section~\ref{sec:methodology} and briefly examined through the case studies in Sections ~\ref{sec:case-study} and~\ref{sec:case-study-2}, initiating an evaluation requires prior configurations. These configurations demand that the researcher possess a thorough understanding of both the underlying concepts and the preparatory activities, such as:

\begin{itemize}
    \item Containerization of new networks to be used in the evaluations;
    \item Containerization of new collectors when they differ from those already containerized, as is the case with nvidia-smi;
    \item Proper identification of devices to be utilized, such as GPUs;
    \item A working knowledge of the database to facilitate the replication of evaluations.
\end{itemize}