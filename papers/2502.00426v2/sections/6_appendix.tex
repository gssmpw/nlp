 This document provides more details of our approach, which are organized as follows:
 {
\begin{itemize}
 \item \S A provides the overview of the MSD and TSE modules.
 \item \S B provides the discussion on the Speed and Memory of the proposed method.
 \item \S C provides the detailed configuration used for the experiment.
 \item \S D provides the visualization of Support-set Construction.
\end{itemize}
}



\section{Overview of MSD and TSE Module}
{
\subsection{Support-set constructed in MSD}
\noindent\textbf{Overview.}~{
To further reveal the process of the Multi-prompting Support-set Dilation (MSD), we illustrate the various steps of these modules in Figure~\ref{fig::MSD}. We use a query template to transform each class name into detailed prompts using a large language model (LLM), \eg, ChatGPT~\cite{ChatGPT} and Claude~\cite{Claude3}. Each generated prompt is then fed into the text-to-video model, \eg, ModelScopeT2V~\cite{wang2023modelscopetexttovideotechnicalreport}, and LaVie~\cite{wang2023laviehighqualityvideogeneration} for generating video samples. 
% We can observe that videos generated by the MSD module effectively capture and present diverse scenes and perspectives, thereby enhancing both the quality and diversity of the generated videos. 
}


\begin{figure}[!ht]
  \centering
  \includegraphics[width=1\linewidth]{MSD.pdf}
  % \fbox{\rule{0pt}{1in} \rule{0.5\linewidth}{0pt}}
  \caption{Multi-prompting Support-set Dilation (MSD).}\label{fig::MSD}
\end{figure}


\noindent\textbf{Discussion.}~{
SuS-X~\cite{udandarao2022sus-x}, CaFo~\cite{zhang2023prompt}, and DMN~\cite{zhang2024dual} apply different language commands to describe a class as clearly as possible via LLMs. (e.g., ``What a [CLASS] looks like?", and ``How can you identify a [CLASS]?"). They can be treated as ``\textbf{Explanators}". However, generative models often cannot depict the fine details of human motion but seem good at fabricating some inessential contextual information. Thus, MSD applies only one command and focuses on constructing diverse textual descriptions of motion with different contexts/settings/perspectives, rather than repeatedly explaining the motion, as shown in Figure~\ref{fig::MSD} of Supplementary Materials. Hence, MSD can be treated as a ``\textbf{Constructor}".
}


\begin{figure}[!htbp]
  \centering
  \includegraphics[width=1.0\linewidth]{TSE.pdf}
  \caption{Temporal-aware Support-set Erosion (TSE).}\label{fig::TSE}
\end{figure}

\subsection{Multi-scale Temporal Tuning in TSE}
{
We illustrate the process of multi-scale temporal tuning in Temporal-aware Support-set Erosion (TSE) modules in Figure~\ref{fig::TSE}. For the TSE module, we tune the weights with different temporal scale visual features extracted from pre-trained Vision-Language Models~(VLMs), \eg, BIKE~\cite{wu2023bidirectional} and ViFi-CLIP~\cite{hanoonavificlip}, in multiple steps. Specifically, given the multi-scale features with ``4, 6, 8" frames, we tune the weights with $8$ frames in four steps~(``1,2,3,4"), with $6$ frames in three steps~(``5,6,7"), and with $4$ frames in three steps~(``8,9,10"). 
% Then optimize sequentially over a total of 10 steps. 
% This multi-step optimization process ensures a more effective capture of detailed action information from multi-scale features.
}
}



\input{tabs/supp-model-config}


\section{Discussion on Speed and Memory}{


\subsection{Multi-prompting Support-set Dilation~(MSD)}
The support-set is built offline and the time cost varies on the choice of Large language Models~(LLMs) and Text-to-Video models (T2Vs). LLMs are accessed via API, thus the computational cost is unknown and the corresponding processing time depends on the network speed. GPU memory and processing times of T2Vs for one single video are shown in Table~\ref{tab::infer_time_t2v}.
\input{tabs/supp-T2V-time}

\subsection{Temporal-aware support-set Erosion~(TSE)}
As shown in Table~\ref{tab::infer_time}, despite some parameters and memory introduced by \testv, the sparse frame sampling strategy allows \testv~to understand $3 \sim 5$-sec videos (such as those in HMDB-51 and UCF-101) in less than 1 second. Therefore, we believe that such inference time can still meet the real-time requirements of video understanding.
\input{tabs/supp-time}

}



\section{Experimental Configuration}
% \noindent\textbf{Environment.}
\subsection{Environment}
{All experiments in this study were conducted in an operating environment consisting of an NVIDIA GeForce RTX 3090 GPU with 24 GB of VRAM, an 8-core CPU, and 32 GB of RAM. The software environment included Ubuntu 20.04, Python 3.8, and PyTorch 2.0.1, which enabled efficient training and inference of our models.}

% \noindent\textbf{Datasets.}
\subsection{Datasets}
\begin{itemize}
    \item \textbf{HMDB-51}~\cite{kuehne2011hmdb} collects videos from multiple sources, such as movies and online videos. The dataset consists of a total of $6,766$ video clips from $51$ action categories, where each category contains at least $101$ video clips. We adopt three default \texttt{test} splits provided by~\cite{kuehne2011hmdb} and report the mean accuracy.
    \item \textbf{UCF-101}~\cite{soomro2012ucf101dataset101human} collects a total of $13,320$ action video clips in $101$ categories from YouTube. Each clip has a consistent frame rate of $25$ FPS and a resolution of $320 \times 240$ and is annotated into $5$ different types of categories: Body motion, Human-human interactions, Human-object interactions, Playing musical instruments, and Sports. We adopt three default \texttt{test} splits provided by~\cite{kuehne2011hmdb} and report the mean accuracy.
    \item \textbf{Kinetics-600}~\cite{carreira2018shortnotekinetics600} is a large-scale action recognition dataset that collects a total of $480$K video clips of $600$ action categories from YouTube. Each clip is manually annotated with a singular action class and lasts approximately $10$ seconds. Following~\cite{chen2021elaborative}, we choose the 220 new categories outside of Kinetics-400~\cite{kay2017kinetics} in Kinetics-600 and then split them into three splits for evaluation. The mean accuracy of these splits is reported.
    \item \textbf{ActivityNet}~\cite{caba2015activitynet} contains $200$ different types of activities and a total of $849$ hours of videos collected from YouTube. This dataset comprises $19,994$ untrimmed video clips and is divided into three disjoint subsets~(\ie, \texttt{train/val/test}) by a ratio of $2:1:1$. Following~\cite{yan_2024_DTSTPT}, we evaluate the proposed method on the \texttt{val} set.
\end{itemize}



\subsection{Implementation Details}
{
    We adopt LaVie~\cite{wang2023laviehighqualityvideogeneration} to generate video samples in Multi-prompting Support-set Dilation~(MSD) module. If not specified, the visual and text encoders are both inherited from CLIP with ViT-B. 
    For Temporal-aware Support-set Erosion (TSE), we sample $8$ frames from the test video and utilize AugMix~\cite{hendrycks2020augmix} to augment them into $32$ views. 
    Finally, we initialize all learnable weights~($\bm{r}_\mathrm{vid}$ and $\bm{r}_\mathrm{fr}$) to $1$ and tune them via AdamW optimizer~\cite{loshchilov2019decoupledweightdecayregularization} with a learning rate of $0.001$. We show the detailed configurations of the proposed methods, such as the number of epochs, the size of the textual prompt, and the size of the support set in Table~\ref{tab::model_config}. Notably, \testv, is tuned within $3$ epochs for each video with different temporal scales: (4,6,8).

}


\section{Visualization of Support-set Construction}
{
We select two action categories from each benchmark and feed them in MSD for generating $10$ supporting videos, as shown in Figure~\ref{fig::vis_hmdb}, Figure~\ref{fig::vis_ucf}, Figure~\ref{fig::vis_k600}, and Figure~\ref{fig::vis_anet}. \textit{Multi-prompting generated videos are more diversified in backgrounds, objects, clothes, and viewpoints.} In addition, we highlight the videos and frames given higher weights by TSE in red boxes~(as shown in Figure~\ref{fig::vis_hmdb}~(a) and (c), Figure~\ref{fig::vis_ucf}~(a) and (c), Figure~\ref{fig::vis_k600}~(a) and (c), and Figure~\ref{fig::vis_anet}~(a) and (c)). \textit{We can observe that support cues captured by TSE are motion-related and low redundant~(visually non-repetitive).} Detailed explanations can be found in the caption of each figure.


% Furthermore, compared with videos generated by a single prompt, the videos generated by multiple prompts are more diversified, showcasing actions from different viewpoints and scenes.

% to illustrate the samples of the support set after being reweighted by the TSE module, we highlight the samples in frame-level or patch-level with weights gain in red in Figure~\ref{fig::vis_hmdb},~\ref{fig::vis_ucf},~\ref{fig::vis_k600} and ~\ref{fig::vis_anet}.  

% It can be observed that the features optimized by the TSE module are more capable of capturing action changes in the videos while filtering out poorly performing samples. 


}

% \section{Discussion}
% {
% \subsection{Limitation Analysis}{
% In ${\textsc{TeST-V}}$, one limitation is its dependency on the performance and quality of the Large Language Models (LLMs) and Text-to-Video (T2V) models utilized. Although the proposed method shows compatibility with various models like ChatGPT~\cite{ChatGPT} and LaVie~\cite{wang2023laviehighqualityvideogeneration}, its effectiveness is inherently tied to the advancements and limitations of these underlying technologies. Moreover, the process of constructing a large, semantically diverse Support-set using LLM and T2V models is both time-consuming and resource-intensive. Future work should focus on optimizing these processes to balance performance improvements with computational efficiency.
% }
% \subsection{Social Impact}{
% This work introduces an innovative framework that enhances zero-shot generalization in video classification by constructing a robust visual support set. The effectiveness of the framework is evidenced by its superior performance compared to existing state-of-the-art methods. However, the method improves the accuracy and robustness of video classification systems. However, there are potential negative social impacts to consider. The framework's reliance on Large Language Models (LLMs) and Text-to-Video (T2V) models to build extensive support sets is both time-consuming and labor-intensive. This issue is a common limitation in methods that require substantial computational resources and large-scale data handling. To mitigate these challenges, future work should focus on developing more efficient and scalable approaches to construct support sets, perhaps exploring weakly-supervised or semi-supervised learning techniques that require fewer resources and reduce the labor involved.
% }
% }




\begin{figure*}[!ht]
  \centering
  \includegraphics[width=\textwidth]{vis_hmdb.pdf}  
  % \includegraphics[width=0.9\linewidth]{figs/vis.pdf}  
  \caption{Visualization of frame level samples in the support set eroded by the TSE module on HMDB-51 dataset. Multiple prompts generated videos showcase the actions of ``golf" and ``kick ball" from various backgrounds and viewpoints. For example, in ``golf", the players hit the ball from different angles, and both indoor and outdoor backgrounds are included in ``kick ball”. In addition, the samples chosen by TSE are more action-related and diverse: the players in ``Golf" wearing different colors, and the athletes in ``kick ball” shoot in different poses.}\label{fig::vis_hmdb}
\end{figure*}
\begin{figure*}[!ht]
  \centering
  \includegraphics[width=\textwidth]{vis_ucf.pdf}  
  % \includegraphics[width=0.9\linewidth]{figs/vis.pdf}  
  \caption{Visualization of frame level samples in the support set eroded by the TSE module on UCF-101 dataset. The multiple prompts generated videos demonstrate the cutting action of various objects (potato, onion, carrot, ...) in ``Cutting In Kitchen" and illustrate the different viewpoints of the boat in ``Rowing". Furthermore, the samples captured by TSE showcase different cutting directions in ``Cutting In Kitchen" and select different views in ``Rowing" without duplication.}\label{fig::vis_ucf}
\end{figure*}
\begin{figure*}[!ht]
  \centering
  \includegraphics[width=\textwidth]{vis_k600.pdf}  
  % \includegraphics[width=0.9\linewidth]{figs/vis.pdf}  
  \caption{Visualization of frame level samples in the support set eroded by the TSE module on Kinetics-600 dataset. The samples generated by multiple prompts, which contained different backgrounds compared to single prompt, as evidenced by the different water environments (lake, sea) in ``Catching Fish" and varied venues (indoor, outdoor) in ``Playing Basketball". Furthermore, TSE tends to select frames and samples that are motion-related: coherent catching actions in ``Catching Fish" as well as those with prominent body actions in ``Playing Basketball".}\label{fig::vis_k600}
\end{figure*}
\begin{figure*}[!ht]
  \centering
  \includegraphics[width=\textwidth]{vis_anet.pdf}  
  % \includegraphics[width=0.9\linewidth]{figs/vis.pdf}  
  \caption{Visualization of frame level samples in the support set eroded by the TSE module on ActivityNet dataset. By comparing the samples generated by multiple prompts and the single prompt, ``Applying Sunscreen" includes actions of applying sunscreen from various viewpoints (front, back) and in different backgrounds (beach, outdoor), while ``Painting" demonstrates painting on different objects (canvas, wall). Moreover, TSE is more likely to select samples with clear and coherent motions, such as the consistent application in ``Applying Sunscreen", the hand movement, and the application of paint in ``Painting".}\label{fig::vis_anet}
\end{figure*}

