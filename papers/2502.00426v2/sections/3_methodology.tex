\section{Preliminary and Definition}
\label{sec:method}

\subsection{Problem Statement}
Zero-Shot Activity Recognition~(ZSAR) aims to recognize unseen actions during model testing. Specifically, given a test video $\bm{V}_\mathrm{test}$ and a set of class names $\bm{Y}=\{y_{0},y_{1},\cdots,y_{c}\}$, we employ the pre-trained visual~($\mathcal{F}_\mathrm{vis}$) and text~($\mathcal{F}_\mathrm{txt}$) encoders~\emph{e.g.}, CLIP~\cite{radford2021learning}, to encode them and then calculate the feature similarity for prediction as follows,
\begin{equation}
\begin{gathered}
\bm{f} = \mathcal{F}_\mathrm{vis}(\bm{V}_\mathrm{test}), \bm{W} = \mathcal{F}_\mathrm{txt}(\bm{Y}),\\
\bm{Z}_{\mathrm{ZS}} = \bm{f} \cdot \bm{W}^\mathrm{T}, \bm{f} \in \mathbb{R}^{d}, \bm{W} \in \mathbb{R}^{C \times d}.
\end{gathered}
\end{equation}
Here, $\bm{f}$ and $\bm{W}$ denote visual and text features respectively, and ``$\cdot$" calculates their cosine similarity.

Although the pre-trained representation is robust enough, it cannot avoid the inherent semantic gap between visual and text features~(\emph{a.k.a}, modality gap). To this end, some recent methods (\emph{e.g.}, TIP-Adapter~\cite{zhang2021tipadaptertrainingfreeclipadapterbetter}, SuS-X~\cite{udandarao2022sus-x}) attempt collecting a set of visual samples (support-set) as the replacement of class names and perform zero-shot matching in the visual space for reduce such gap. We first briefly review such support-set-based zero-shot matching methods as below.

\subsection{Support-Set-based Zero-shot Prediction}
\noindent\textbf{Support-Set Construction.}~Given a set of class name $\bm{Y}$, the text-to-image (\texttt{T2I}) generation model (\emph{e.g.}, Stable Diffusion~\cite{rombach2022high}) is employed to generate $K$ images for each category to construct the support set. To obtain more diverse generated samples, SuS-X~\cite{udandarao2022sus-x} explains each class name in detail through LLMs~(\eg, GPT-3~\cite{brown2020languagemodelsfewshotlearners}) as input of \texttt{T2I} model following CuPL~\cite{pratt2023does}. Thus, this support-set consists of $C \times K$ visual samples where $C$ and $K$ denote the number of classes and the number of generated samples for each class.

\noindent\textbf{TIP-Adapter Inference.}
% For the support set of size $C \times K$, 
Based on the above support-set, CLIP's visual encoder $\bm{E}_\mathrm{v}$ is applied to extract its visual features $\bm{F} \in \mathbb{R}^{CK \times d}$, and its labels are converted into the one-hot vector $ \bm{L} \in \mathbb{R}^{CK \times C}$. TIP-Adapter aims to calculate the distance between the test video feature $\bm{f}$ and support videos' feature $\bm{F}$, and then make predictions~($\bm{Z}_{\mathrm{TA}}$) with the help of label information $L$ as follows,
\begin{equation}
\bm{Z}_{\mathrm{TA}} =\exp (-\beta (1 - \bm{f}\bm{F}^\mathrm{T})) \bm{L}. 
\end{equation}
Here, $\beta$ adjusts the sharpness and affinities calculated from $\bm{f}\bm{F}^\mathrm{T}$ is used as attention weights over $\bm{L}$ to achieve logits.

\noindent\textbf{TIP-X Inference.} To avoid the uncalibrated intra-modal feature distances in TIP-Adapter, TIP-X~\cite{udandarao2022sus-x} uses text modality as a bridge between visual modalities. TIP-X calculates the distance between the test video feature $f$ and text features $\bm{W}$, and between support videos' feature $F$ and text feature $W$, respectively. TIP-X applies KL-divergence to construct the affinity matrix for final prediction~($\bm{Z}_{\mathrm{TX}}$) as follows,
\begin{equation}
\bm{Z}_{\mathrm{TX}}=\psi (-\mathtt{KL}(\bm{f} \bm{W}^\mathrm{T} || \bm{F}\bm{W}^\mathrm{T})) \bm{L}.\label{eq::TIP-X}
\end{equation}
Here, $\bm{f} \bm{W}^\mathrm{T}$ and $\bm{F}\bm{W}^\mathtt{T}$ are normalized via softmax, $\mathtt{KL}(\cdot)$ computes the similarity between two probabilities~($\mathtt{KL}(\bm{P} \parallel \bm{Q}) = \sum_i \bm{P}_i \log \frac{\bm{P}_i}{\bm{Q}_i}$) and $\psi$ is the scaling factor.

\begin{figure*}[t]
  \centering
  \includegraphics[width=0.95\linewidth]{overview_2.pdf}
    \caption{Overview of the proposed framework \testv~which \textit{first dilates and then erodes} the support set for zero-shot video classification.
    % Given the test video and the target class set, we build a diversified video support set via the {\color{red} \textbf{Multi-prompting Support-set Construction~(MSC)}} module and then tune a few learnable weights for support set refining via the {\color{red} \textbf{Multi-scale Support-set Temporal Tuning~(MSTT)}} module. 
    i) {\textbf{Multi-prompting Support-set Dilation~(MSD)}}: It builds diversified motion description for each class name via the LLM and then generates video samples with these elaborate descriptions via the text-to-video generation model for constructing a diverse support set. ii) {\textbf{Temporal-aware Support-set Erosion~(TSE)}}: Based on the visual feature of the given test video~{$\bm f$} and support set {$\bm F$}, it applies factorized weights $\bm r_{*}$ to mine critical supporting cues from the support set and tunes the weights with prediction consistency at multiple temporal scales.}\label{fig::overview}
\end{figure*}


\section{Methodology}
{In this section, we present a novel framework, \ie, TEst-time Support-set Tuning for Video classification~(\testv). It constructs a semantically diverse support-set with multiple category descriptions and tunes learnable weights to adjust the support-set with the test samples. The overview is shown in Figure~\ref{fig::overview} and details are presented below.}


\subsection{Multi-prompting Support-set  Dilation~(MSD)}\label{sec::MSD}
{As shown in Figure~\ref{fig::sus}, the performance of the support-set based on SuS-X is extremely saturated when the number of videos per category increases, suggesting that the generated video samples' representations are highly overlapping/similar in each class. This inspires this work to improve the diversity of generated samples to refine the semantic boundary of each class within the support-set. To this end, we propose the Multi-prompting Support-set Dilation~(MSD) module to generate diverse video samples via a video-text generation model with different prompts.}

\noindent\textbf{Diversified Motion Description}.
% As mentioned above, the key to improving the diversity of the support set is generating more diverse prompts. Based on this, 
Given a set of action classes $\bm{Y}=\{\bm{y}_i\}_{i=1}^C$, we employ a large language model (LLM) (\emph{e.g.}, ChatGPT~\cite{ChatGPT}) to generate ${M}$ different prompts for each class as
\begin{equation}\label{eq::LLM}
\bm{d}_i = \texttt{LLM}(\texttt{query\_template}(\bm{y}_i,M)). 
\end{equation} 
Here, $\texttt{query\_template}(\cdot)$ is a crafted text template used to wrap each action class into a text query as the input of LLM, \textit{as shown in Figure $I$ in the Appendix}. After traversing the entire action class set $Y$, we can achieve a diversified motion description set $\bm{D}=\{\bm d_{i}\}_{i=1}^C$ in which each class corresponds to $M$ different explanations and $C$ is the number of action classes.

{
\noindent\textbf{Video Generation from Diversified Prompts}.
For the $i$-th action class, we generate $K=m \times n$ video samples via the text-to-video model $\texttt{T2V}(\cdot, \cdot)$ according to text prompts stored in $\bm d_{i}$ as follows, 
\begin{equation}
\bm{s}_i = \texttt{T2V}(\texttt{Sample}(\bm{d}_i, m), n).\label{eq::MSD}
\end{equation}
Here, we $\texttt{Sample}$ $m$ prompts for each class from $\bm{d}_i$ and repeatedly generate $n$ videos corresponding to each prompt. \textit{$K$ is set to be always factorable for ease of implementation, {two hyper-parameters (\emph{\textit{i.e.}}, $m$ and $n$) are ablated in the experiments (as shown in Figure~\ref{fig::sus})}.} After traversing the entire description set $\bm{D}$, we can construct the video support-set $\bm{S}=\{s_{i}\}_{i=1}^{C}$ in which each class has $K$ support videos.
}

{
Different from previous methods~(SuS-X~\cite{udandarao2022sus-x}, CaFo~\cite{zhang2023prompt}, and DMN~\cite{zhang2024dual}) which explain class names repeatedly, MSD constructs diverse descriptions for each class name with different context/setting/perspectives. More details can be found in Sec-B of the Appendix.
}

\subsection{Temporal-aware Support-set Erosion~(TSE)}
\label{sec::TSE}
{
The support set generated via MSD has a greater diversity in each category~(as shown in Figure~\ref{fig::vis-MSD}). MSD refines the boundary of each class but still 
contains many invalid samples~(\ie, outliers or duplications), which motivates us to further adjust the support-set, \ie, ``\textbf{eroding}" some invalid information from the set. A straightforward solution is to apply learnable weights on video samples and tune them with a self-supervision loss, inspired by TPT~\cite{shu2022tpt}. {However, video-level tuning can miss the potential fine-grained supporting cues hidden in video data. Hence, it is necessary to tune the weights on frame-level or patch-level visual features, but patch-level introduces so many parameters that test time tuning techniques cannot accomplish optimization.}
To this end, we proposed a novel Temporal-aware Support-set Erosion module to reweigh the importance of each frame of supporting videos via factorized video-frame weights, which is tuned via different temporal-scale features in multiple steps during test time. 
\input{tabs/SOTA}

\noindent\textbf{Factorized Re-weighting.}
Given the support set $\bm{S}$ generated from MSD, we utilize the pre-trained visual encoder~($\mathcal{F}_\mathrm{vis}$) from popular VLMs (\emph{e.g.}, CLIP) to extract visual features and concatenate them together as follows,
\begin{equation}
\begin{gathered}
\bm{F}_i =\mathcal{F}_\mathrm{vis}(\bm{s_{i}}), i \in [1, CK], \bm{F}_i\in\mathbb{R}^{T \times d},\\
\bm{F} = \texttt{Concat}([\bm{F}_1,\cdots,\bm{F}_{CK}]), \bm{F} \in \mathbb{R}^{CK \times T \times d}.
\end{gathered}
\end{equation}
Here, $C$, $K$, $T$, and $d$ represent the number of classes, supporting samples of each class, frames, and channels, respectively.
After that, we reweight the support-set feature $\bm{F}$ in video and frame level via factorized learnable weights as follows,
\begin{equation}
\bm{F}' = (\bm{F} \odot \bm{r}_\mathrm{vid})\odot \bm{r}_\mathrm{fr}, \bm{F}' \in \mathbb{R}^{CK \times T \times d}.\label{eq::F_re}
\end{equation}
Here, ``$\odot$" denotes element-wise matrix multiplication~(\ie, Hadamard product). $\bm{r}_\mathrm{vid} \in \mathbb{R}^{CK \times 1 \times 1}$ and $\bm{r}_\mathrm{fr} \in \mathbb{R}^{1 \times T \times 1}$ are frame-level and video-level weights, respectively. 

\noindent\textbf{Multi-scale Temporal Tuning.} 
Inspired by Test-time Prompt Tuning~(TPT~\cite{shu2022tpt}), we tune learnable weights~($\bm r$) via minimizing the prediction consistency between re-weighted support-set features and the augmented test video features as follows, 
\begin{equation}
\begin{gathered}
\min\mathcal{L}(\texttt{Pred}(\bm{F}', \{\bm{f}^\mathrm{S}_{i}\}^{n}_{i=1})).
\label{eq::single_tuning}
\end{gathered}
\end{equation}
Here $\bm{f}^{S}_{*}=\mathcal{F}_\mathrm{vis}(\texttt{Aug}^\mathrm{S}_{n}(\bm{V}_\mathrm{test}))$ denotes the augmented test video features and $\texttt{Aug}^{\mathrm{S}}_{n}(\cdot)$ performs spatial augmentation $n$ times. $\texttt{Pred}(\cdot)$ computes multi-view predictions from the support-set features and augmented test video features according to Eq.~(\ref{eq::TIP-X}). $\mathcal{L}$ calculates the averaged entropy from high-confident predictions~\cite{shu2022tpt}.


However, the temporal evolution rate of different human activities is variable, and building multi-scale representations from the temporal domain has proven beneficial in {video representation~\cite{feichtenhofer2019slowfast,yan_2024_DTSTPT}}. Therefore, Eq.~(\ref{eq::single_tuning}) can be rewritten as follows,
\begin{equation}
\min\mathcal{L}(\texttt{Pred}(\texttt{Aug}^\mathrm{T}_{m}(\bm{F}', \{\bm{f}^\mathrm{S}_{i}\}^{n}_{i=1}))).
\end{equation}Here, $\texttt{Aug}^\mathrm{T}_{m}(\cdot)$ augments two feature sets synchronously via $m$ time sampling at different temporal scales. Different sampling strategies are compared in Table~\ref{tab::MSES} and discussed in Sec.~\ref{sec::ab_temp_sampling}. \textit{Notably, this loss function is optimized in $m$ steps with different temporal-scale features~({as illustrated in Figure~$II$ in the Appendix)}.}
}