\section{Introduction}
\label{intro}
{
In the past few decades, the research on behavior recognition has made rapid progress and has been successfully applied in many fields, including intelligent robotics, security surveillance, and automatic driving. However, the number of behaviors existing methods can identify is still limited, which makes them difficult to meet the growing needs of practical applications. In recent years, the powerful ability of large-scale pre-trained Vision-Language models~(VLMs) in zero-shot generalization has promoted the rapid development of zero-shot/general behavior recognition. VLMs bring hope for breaking through the perceptual boundaries of existing behavior recognition technologies. 

Recently, it has become a trend to project the test video and class names into the joint feature space based on the pre-trained models, and then assign a label to the video according to their feature similarities. Although the amount of pre-training data is huge, the out-of-distribution issue cannot be thoroughly eliminated in the test environment. Hence, it is necessary to further tune the parameters or prompts for the pre-trained model from different modal perspectives.

To address this challenge, existing solutions can be roughly divided into the following two categories. i)~\textbf{Training-based}: Conventional methods fine-tune \textit{part of parameters} belonging/appended to the image-based pre-trained model~(\eg, CLIP~\cite{radford2021learning}) or \textit{extra learnable prompts} appended to (visual/text) inputs, based on a mass of video data. ii)~\textbf{Training-free}: To avoid high training costs, some recent Test-time Prompt Tuning~(TPT)-based methods~\cite{shu2022tpt,yan_2024_DTSTPT} append learnable prompts to visual/text input sequence and tune prompts with a self-supervised manner~(\eg, multi-view prediction consistency) based on the single test video during test time~(Figure~\ref{fig::mov}~(a)). Beyond that, some recent works~\cite{udandarao2022sus-x,zhang2021tipadaptertrainingfreeclipadapterbetter} build a video support-set according to unseen class names via retrieval or generation(\ie, converts class names to videos) and performs intra-modality (video-video) alignment (Figure~\ref{fig::mov}~(b)). TPT freezes the visual space and only tunes the text space, which cannot reduce the modality gap. On the contrary, the support-set-based methods reduce the gap via inter-modality alignment but the supporting samples are fixed.

}

\begin{figure*}[t]
  \centering
\includegraphics[width=0.95\linewidth]{motivation.pdf}
  % \fbox{\rule{0pt}{2in} \rule{1.0\linewidth}{0pt}}
  \caption{Innovation of the zero-shot activity recognition framework. a) Tuning the text input via the given test video in a self-supervised manner. b) Aligning the test video with the label based on the support set from feature similarity or predicted distribution similarity. c) This work combines the above thoughts to construct the support set diversely and tunes this set in a self-supervised manner to mine high-quality support samples.}~\label{fig::mov}
  % \vspace{-3mm}
\end{figure*}


{To this end, we proposed a novel framework namely \textbf{TE}st-time \textbf{S}upport-set \textbf{T}uning for \textbf{V}ideo classification (\testv) in Figure~\ref{fig::mov}~(c). It builds a semantically diverse support-set via off-the-shelf text-video generation models and tunes learnable weights to select effective support samples from the set via two core modules. \textbf{i) Multi-prompting Support-set Dilation~(MSD)}: generate multiple text prompts from the class names via LLMs and feed them into the text-video generation model~(\ie, LaVie~\cite{wang2023laviehighqualityvideogeneration}) for building a semantically diverse support set. \textbf{ii) Temporal-aware Support-set Erosion~(TSE)}: tunes a few learnable weights to dynamically reshape the contribution of each video frame in a self-supervised manner, according to the multi-scale semantic consistency hypothesis in the temporal domain. Extensive experimental results show that \textbf{TEST-V} improves the state-of-art pre-trained VLMs, namely CLIP~\cite{radford2021learning}, BIKE~\cite{wu2023bidirectional} and VIFi-CLIP~\cite{hanoonavificlip} by {$2.98\%$}, $2.15\%$, and $1.83\%$ absolute average accuracy respectively across four benchmarks. 
% Visualizations demonstrate its interpretability in enhancing the diversity of the support set and mining critical cues from it.
}

{Our main contributions can be summarized as follows:
\begin{itemize}
    \item We propose a novel framework, namely \textbf{TE}st-time \textbf{S}upport-set \textbf{T}uning which first dilates and then erodes the visual support-set to enhance the zero-shot generalization on activity recognition during test time.
    \item To ensure the diversity of the support set, we propose a Multi-prompting Support-set Dilation~(MSD) module to generate videos via text-to-video model as supporting samples for each class according to multiple descriptions.
    \item To mine critical supporting cues from the support-set, we proposed a Temporal-aware Support-set Erosion~(TSE) module to tune factorized learnable weights with different temporal-scale features in multiple steps supervised by the prediction consistency.
\end{itemize}
}