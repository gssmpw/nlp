\section{Experiments}
\label{experiment}
%--------------------------------------------------------------
We evaluate the effectiveness of the proposed method on four popular video benchmarks, \ie, HMDB-51~\cite{kuehne2011hmdb}, UCF-101~\cite{soomro2012ucf101dataset101human}, Kinetics-600~\cite{carreira2018shortnotekinetics600}, ActivityNet~\cite{caba2015activitynet}. Detailed configuration of the platform environment, experimental data, models, and optimization can be found in the Appendix.


\subsection{Comparison with State-of-the-Arts}
{

We evaluate the effectiveness of \testv with existing state-of-the-art methods on four popular action recognition benchmarks in Table~\ref{tab:sota}. Existing methods can be categorized into three types: 
i) Uni-modal zero-shot video recognition models: they are trained on video data with elaborated representation engineering;
ii) Adapting pre-trained CLIP: they adapt CLIP to video data via additional temporal learners or VL prompting techniques;
iii) Tuning pre-trained CLIP: they fully fine-tune the CLIP model via video data.

We apply off-the-shelf pre-trained visual/text encoders from VLMs, \eg, BIKE~\cite{wu2023bidirectional} and ViFi-CLIP, to extract features and tune the proposed \testv~during test time without any training data. Our method outperforms the conventional uni-modal zero-shot video recognition models by a large margin~($15\%\sim 20\%$). Moreover, compared with the methods adapted from pre-trained CLIP with video data, our method achieve consistent improvements across all benchmarks without training. Furthermore, our method can significantly improve some recent methods fully-tuned from CLIP, thanks to the reduction of modality gaps through the tuned support set. In general, the proposed \testv~achieves state-of-the-art performance on four popular benchmarks in a training-free manner.


}


\subsection{Ablation Study}
\paragraph{Choice of LLM and T2V for MSD.}
{
As described in Sec.~\ref{sec::MSD}, MSD builds multiple descriptions~(\ie, prompts) for each class via the Large Language Model~(LLM) and then generates video samples via the Text-to-Video~(T2V) model according to multiple prompts. There are many options for LLM and T2V, thus we compare them in Table~\ref{tab::LLM} and~\ref{tab::GEN}, and make the following analysis.
\textbf{i) LLMs}: We compare different Large Language Models (LLMs) for motion description generation defined in Eq.~(\ref{eq::LLM}) in Table~\ref{tab::LLM}. In general, our approach works well on all LLMs, and new versions of the same model perform better. ChatGPT achieves the best performance and is selected as the default LLM.
\textbf{ii) T2Vs}: The proposed method is compatible with various Text-to-Video~(T2V) generation models as shown in Table~\ref{tab::GEN}. {Benefited from the extra training videos data}, LaVie is slightly better than others and is adopted by default.
% ModelScopeT2V~\cite{wang2023modelscopetexttovideotechnicalreport} and 
}

\input{tabs/AB-LLM}
\input{tabs/AB-Generative_models}


\begin{figure}[!t]
  \centering
  \includegraphics[width=1\linewidth]{supportset_4.pdf}
  \caption{Effect of support hyper-parameters $K$ and $n$~($K = m \times n$ defined in the equation~\ref{eq::MSD}) with a single prompt~(SuS-X) and multiple prompts~(our MSD) on HMDB-51 and UCF-101. Top-1 zero-shot recognition accuracy is reported.}
  % \vspace{-3mm}
  \label{fig::sus}
\end{figure}

% \paragraph{$K$ and $n$ Defined in MSD~(Equation~(\ref{eq::MSD})).}
\paragraph{Importance of Diversity of the Support-set}
{
    % \color{blue}
    To figure out the key factor of the quality of the support-set, we ablate the support-set hyper-parameters of the proposed MSD, \ie, the number of supporting videos for each class $K$ and the number of videos repeatedly generated for each prompt $n$ defined in Equation~(\ref{eq::MSD}), as shown in Figure~\ref{fig::sus}.    
    % is sensitive to the number of supporting samples generated for each class~(\ie, support-set size). Therefore, we investigate the effect of $K$ on zero-shot accuracy with a single prompt or multiple prompts on different benchmarks. 
    The performance of all models~(baseline SuS-X and the proposed MSD with different $n$) gradually plateau as the support-set size~($K$) increases across two benchmarks. 
    Besides, the proposed multi-prompting method~(MSD) outperforms the single-prompting baseline~(SuS-X) when $K \neq 0$, demonstrating the importance of sample diversity in the support-set. 
    Meanwhile, we ablate the number of repeatedly generated videos $n$ when the total number of videos $K$ is fixed.~($K= m \times n$). We found that $n=4$ brings stabilized gains compared to the baseline and \textbf{set $n=4$ for all benchmarks}. The number of prompts $m$ used for each benchmark is listed in the Appendix due to limited space.
    % Notably, due to the difficulty in accessing gpt-3 model, we use the gpt-3.5-turbo model and the T2V generation model to extend the method of constructing support sets in SuS-X for a fair comparison.
}
\begin{figure*}[ht]
  \centering
  \includegraphics[width=1\linewidth]{visualization3.pdf}  
  \caption{Feature distribution of supporting samples generated with multiple prompts~(MSD) and single prompt~(SuS-X) on different benchmarks. Multi-prompting and single-prompting samples are shown in color and grey, respectively.}\label{fig::vis-MSD}
  % For each category, we select $20$ multi-prompting and single-prompting samples, respectively, and then extract pooled video features from them with $4$ frames. 
\end{figure*}
\input{tabs/AB-MS_embedding_selection}
\paragraph{Multi-scale Temporal Tuning.}\label{sec::ab_temp_sampling}
{
    To find the suitable temporal scale used in TSE, we try different sampling strategies and report the performance in Table~\ref{tab::MSES}. ``random" sampling frames from the feature sequence randomly, while ``top" selects the top $k$ features with higher frame-level weights~($r_\mathrm{f}$). We observed that multi-scale tuning with the ``random" strategy does not work compared with the single-scale tuning. Besides, multi-scale tuning the ``top" strategy with ``4,6,8" achieves state-of-the-art results. 
    % We compare the effect of different multi-scale temporal tuning strategies with single-scale tuning in Table~\ref{tab::MSES}. 
    % For the test feature and support set feature with $8$ frames, we design two temporal sample strategies to sample multi-scale embedding from the test feature and the reweighted support set feature: the Random method and the Top method. 
    % Based on these methods, we sample features from different temporal scales and tune them in multiple steps. As reported in Table~\ref{tab::MSES}, the Top method shows improvement over the Random method. 
    % Feature in specific temporal scales has better performance improvement.
}

\input{tabs/AB-Comp}
\input{tabs/AB-Weight_module}
\paragraph{Component Analysis.}
{
    % \color{blue}
    We explore the effect of each module designed in \testv~in Table~\ref{tab::Comp}. Compared with the baseline, MSD brings approximately $1\%$ improvements on two benchmarks, indicating that the diversity of supporting samples in each class is beneficial in enhancing the quality of the support set. Meanwhile, TSE demonstrates the ability to select critical visual cues from highly redundant video data, which improves the baseline by approximately $1\%\sim 2\%$. Furthermore, the combination of MSD and TSE yields even more significant performance gains, confirming the complementarity of the two modules.
    % Besides, TSE further adjusted the distribution of the support set, resulting in $2.8\%$ and $1.8\%$ final performance improvement on HMDB-51 and UCF-101 datasets.

    As defined in Eq.~(\ref{eq::F_re}), learnable weights $\bm{r}_\mathrm{vid}$ and $\bm{r}_\mathrm{fr}$ are designed to adjust the contribution of video-level and frame-level features, respectively. As reported in Table~\ref{tab::WM}, each of them brings performance gains and these two weights are complementary.
}

\input{tabs/AB-Pretrain_model}
\paragraph{Generalization to Different VLMs.}
{
    % \color{blue}
    We also extract visual/text features via different pre-trained VLMs, \ie, Vanilla CLIP~\cite{radford2021learning}, and BIKE~\cite{wu2023bidirectional}, 
    % ~({\color{red}\emph{e.g.}, Vanilla CLIP, BIKE, and ViFi-CLIP}) 
    with various network backbones,
    % (\emph{e.g.}, ViT-B/16, ViT-L/14)
    and report top-1 zero-shot accuracy on two benchmarks in Table~\ref{tab::PM} compared with SuS-X~\cite{udandarao2022sus-x}. Our approach shows consistent improvement across two benchmarks with different models and backbones, demonstrating good generalization across different video representations.   
    % We use visual and text encoders from these VLMs to extract features while keeping the model frozen. As shown in Table~\ref{tab::PM}, our \testv method shows improvement across different models and backbones, indicating that our method can be effectively adapted to different VLMs.
}


\paragraph{Visualization.}
{
    % \color{blue}
    We randomly sample $20$ videos from the support sets generated with multiple prompts and the single prompt for four benchmarks, and all videos are embedded via the pre-trained visual encoder of BIKE~\cite{wu2023bidirectional} with $4$ frames. After that, the feature distributions of sampled videos in each benchmark are visualized via t-SNE~\cite{van2008visualizing} in Figure~\ref{fig::vis-MSD}. We observed that single-prompting samples are more likely to cluster together, whereas other multi-prompting samples are more dispersed. This phenomenon shows that the multi-prompting strategy can refine the supporting boundary to enhance the quality of the support-set.
    
    % Single-prompting and multi-prompting samples are shown in gray and color, respectively. 
    % the distribution of MSD has wider boundaries compared to those in SuS-X, demonstrating that the videos generated by MSD are more diverse.

}