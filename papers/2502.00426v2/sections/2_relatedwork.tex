\section{Related Work}
\label{Related Work}
\subsection{Zero-shot Activity Recognition}
Zero-Shot Activity Recognition (ZSAR) aims to recognize actions that are not observed by the model during training, which is a non-trivial task yet useful for practical applications. Early work learn a joint embedding space by aligning video representation with corresponding textual semantic representation, \eg, manually defined attributes~\cite{liu2011recognizing,zellers2017zero}, word embeddings of category names~\cite{qin2017zero,shao2020temporal}, and elaborative action descriptions~\cite{chen2021elaborative,qian2022rethinking}. With the advent of deep neural networks, many works~\cite{chen2021elaborative,lin2019tsm,lin2022cross} make use of various video architectures (\eg, I3D~\cite{carreira2017quo} and TSM~\cite{lin2019tsm}) to obtain high-level visual representation from videos and further project such visual representation into semantic embedding space. Recently, large-scale Vision-Language models (\eg, CLIP~\cite{radford2021learning}) have shown great zero-shot transfer ability. Thus many ZSAR solutions attempt to take the advent of CLIP to align video features and textual features, showing impressive performance in ZSAR. ActionCLIP~\cite{wang2021actionclip}, A5~\cite{ju2022prompting}, and XCLIP~\cite{XCLIP}  adapt CLIP for videos by training additional components, \eg, temporal transformer blocks and text prompts. ViFi-CLIP~\cite{hanoonavificlip}, BIKE~\cite{wu2023bidirectional}, and MAXI~\cite{lin2023match} fully fine-tune the encoder of CLIP to learn video-specific inductive biases, without introducing any additional learnable parameters. Different from current approaches that rely heavily on many labeled video samples to fine-tune network parameters, our work pioneers the idea of building an efficient visual support set for zero-shot classification without training. 

\subsection{Adaptation of Visual-Language Models}
Recently, pre-trained Vision-Language Models (VLMs) (\eg, CLIP~\cite{radford2021learning}, ALIGN~\cite{jia2021scaling}) have shown impressive zero-shot transfer ability in recognizing novel categories. Thus many works attempt to adapt VLMs for downstream tasks through prompt tuning or training-free methods. CoOp~\cite{zhou2022learning} fine-tunes learnable text prompts on downstream training data instead of using hand-crafted templates~\cite{zhou2022learning}, effectively improving CLIP zero-shot performance. CoCoOp~\cite{zhou2022conditional} extends CoOp to learn input-conditioned prompts, leading to better generalizing to unseen classes and different domains. Despite impressive, these works need access to labeled samples from target distribution, and further train prompts over these samples. In addition, recent training-free methods such as TIP-Adapter~\cite{zhang2021tipadaptertrainingfreeclipadapterbetter} leverage a few labeled training samples as a key-value cache to make zero-shot predictions during inference, avoiding the conventional model parameter fine-tuning. Yet these methods still rely on samples from the target distribution. To discard costly labeled data and training, SuS-X~\cite{udandarao2022sus-x} utilizes category name and text-to-image generation model to construct visual support-set, thus enhancing zero-shot transfer abilities. Our test-time support-set tuning shares a similar spirit of pursuing a visual support-set curation strategy to adapt VLMs, but we strive to construct a diverse video support set guided by elaborated descriptions of each class name, and make full use of each sample in the support-set by dynamically adjusting the contribution of each frame.

\subsection{Test-time Tuning}
Test-time Tuning (TTT)~\cite{shocher2018zero,nitzan2022mystyle,xie2023sepico} aims to improve the generalization capabilities of pre-trained models by utilizing test samples to fine-tune the model, especially when faced with data distribution shifts. One of the key challenges in TTT is designing effective optimization objectives. Early works~\cite{sun2020test,liu2021ttt} enhance the training process by integrating a self-supervised multitask loss. Besides, TENT~\cite{wang2020tent} minimizes predictions' entropy on target data to improve generalization capability, yet requiring multiple test samples. To address this issue, MEMO~\cite{zhang2022memo} and TPT~\cite{shu2022tpt} utilize multiple augmented views generated from a single test sample through data augmentation for further test-time tuning. Recent methods extend TTT to the video domain by self-supervised dense tracking~\cite{azimi2022self} or aligning video features with different sampling rates~\cite{zeng2023exploring}. Inspired by this, we tune the support-set via learnable parameters with temporal prediction consistency loss for mining reliable supporting cues.
%--------------------------------------------------------------
