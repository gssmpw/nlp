\section{Related Work}
\subsection{Text-to-SQL}
Text-to-SQL ____, which aims to convert natural language instructions or questions into SQL queries, has drawn significant attention. Since the work of ____, leading text-to-SQL models have adopted attention-based sequence-to-sequence architectures to translate questions and schemas into well-formed SQL queries. These models have increasingly benefited from pre-trained transformer architectures, ranging from BERT ____ to larger language models such as T5 ____ in ____, OpenAI CodeX ____, and GPT variants ____. Along with using pre-trained models, various task-specific enhancements have been introduced, including improved schema encoding via more effective representation learning ____ and fine-tuned attention mechanisms for sequence-to-sequence models ____. On the decoding side, some methods incorporate the syntactic structure of SQL ____.

Recent advances in LLMs have also extended their multi-task capabilities to text-to-SQL. In zero-shot scenarios, a task-specific prompt is added before the schema and the question, guiding the LLM to generate an SQL query. ____ showed that OpenAI CodeX can achieve 67\% execution accuracy using this approach. Building on this, few-shot prompting strategies have been investigated. In particular, ____ proposed GPT-4-based DIN-SQL, which divides the problem into four subtasks (schema linking, classification, generation, and self-correction) and achieves strong performance on the Spider benchmark. However, ____ also noted that DIN-SQL encounters difficulties when dealing with complex queries. In contrast to these approaches, our method reframes text-to-SQL as a reasoning task. By doing so, it leverages the inherent reasoning capabilities of LLMs to boost performance and facilitates the integration of additional reasoning techniques into text-to-SQL systems.

\subsection{Multi-step Reasoning}
Complex reasoning tasks have sparked extensive research in LLMs, which are crucial for handling challenging queries ____. One prominent strategy is the Chain-of-Thought (CoT) prompting technique ____, along with its variants ____, which decompose the reasoning process into sequential steps and systematically approach problem-solving in a human-like manner. To further enhance the accuracy of these intermediate steps, recent studies leverage extensive synthetic datasets, which are either distilled from cutting-edge models ____ or composed of self-generated rationales ____, to fine-tune the LLMs. Such training strategy effectively sharpens the models' ability to produce correct chain-of-thought reasoning.

Additionally, there is growing interest in test-time verification, which involves generating multiple candidate solutions and ranking them with a separate verifier ____ to select the most accurate one. For example, the DIVERSE framework ____ employs a variety of CoT prompts together with a verifier to address reasoning challenges, while CoRe ____ fine-tunes both the generator and verifier in a dual-process system, improving LLM performance on math word problems.

\begin{figure*}[ht]
 \centering
 \includegraphics[width=\linewidth]{method.pdf}
 \caption{An overview of the STaR-SQL framework. It consists of three main steps: step-by-step rationale generation for self-improvement, verifier training, and test-time verification. We transform text-to-SQL into a reasoning task and further explore scaling up test-time computation by incorporating a verifier and employing best-of-N sampling.}
\end{figure*}