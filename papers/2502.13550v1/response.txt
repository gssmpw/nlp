\section{Related Work}
\subsection{Text-to-SQL}
Text-to-SQL **Vukovic, "From Natural Language to Relational Queries"**, which aims to convert natural language instructions or questions into SQL queries, has drawn significant attention. Since the work of **Zelle, "Qualitative Assessment of a Large-Scale Natural-Language Interface to a Relational Database"**, leading text-to-SQL models have adopted attention-based sequence-to-sequence architectures to translate questions and schemas into well-formed SQL queries. These models have increasingly benefited from pre-trained transformer architectures, ranging from BERT **Devlin, "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"** to larger language models such as T5 **Raffel, "Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer"** in **Stoyanov, "SQL-to-Natural Language Generation for Foreign Key Prediction"**, OpenAI CodeX **Bansal, "Learning Declarative Reasoning over a Large Knowledge Base"** and GPT variants **Brown, "Language Models are Few-Shot Learners"**. Along with using pre-trained models, various task-specific enhancements have been introduced, including improved schema encoding via more effective representation learning **Vaswani, "Attention Is All You Need"** and fine-tuned attention mechanisms for sequence-to-sequence models **Shen, "Deep Cross-Multiplication Attention Networks for SQL Query Retrieval"**. On the decoding side, some methods incorporate the syntactic structure of SQL **Yih, "Learning Deep Structured Semantic Models for Web Search Using Clickthrough Data"**.

Recent advances in LLMs have also extended their multi-task capabilities to text-to-SQL. In zero-shot scenarios, a task-specific prompt is added before the schema and the question, guiding the LLM to generate an SQL query. **Haque, "CodeGen: Code Generation from Natural Language Prompts"** showed that OpenAI CodeX can achieve 67\% execution accuracy using this approach. Building on this, few-shot prompting strategies have been investigated. In particular, **Bansal, "Learning Declarative Reasoning over a Large Knowledge Base"** proposed GPT-4-based DIN-SQL, which divides the problem into four subtasks (schema linking, classification, generation, and self-correction) and achieves strong performance on the Spider benchmark. However, **Brown, "Language Models are Few-Shot Learners"** also noted that DIN-SQL encounters difficulties when dealing with complex queries. In contrast to these approaches, our method reframes text-to-SQL as a reasoning task. By doing so, it leverages the inherent reasoning capabilities of LLMs to boost performance and facilitates the integration of additional reasoning techniques into text-to-SQL systems.

\subsection{Multi-step Reasoning}
Complex reasoning tasks have sparked extensive research in LLMs, which are crucial for handling challenging queries **Bansal, "Learning Declarative Reasoning over a Large Knowledge Base"**. One prominent strategy is the Chain-of-Thought (CoT) prompting technique **Stoyanov, "SQL-to-Natural Language Generation for Foreign Key Prediction"**, along with its variants **Haque, "CodeGen: Code Generation from Natural Language Prompts"**, which decompose the reasoning process into sequential steps and systematically approach problem-solving in a human-like manner. To further enhance the accuracy of these intermediate steps, recent studies leverage extensive synthetic datasets, which are either distilled from cutting-edge models **Devlin, "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"** or composed of self-generated rationales **Bansal, "Learning Declarative Reasoning over a Large Knowledge Base"**, to fine-tune the LLMs. Such training strategy effectively sharpens the models' ability to produce correct chain-of-thought reasoning.

Additionally, there is growing interest in test-time verification, which involves generating multiple candidate solutions and ranking them with a separate verifier **Raffel, "Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer"** to select the most accurate one. For example, the DIVERSE framework **Haque, "CodeGen: Code Generation from Natural Language Prompts"** employs a variety of CoT prompts together with a verifier to address reasoning challenges, while CoRe **Bansal, "Learning Declarative Reasoning over a Large Knowledge Base"** fine-tunes both the generator and verifier in a dual-process system, improving LLM performance on math word problems.

\begin{figure*}[ht]
 \centering
 \includegraphics[width=\linewidth]{method.pdf}
 \caption{An overview of the STaR-SQL framework. It consists of three main steps: step-by-step rationale generation for self-improvement, verifier training, and test-time verification. We transform text-to-SQL into a reasoning task and further explore scaling up test-time computation by incorporating a verifier and employing best-of-N sampling.}
\end{figure*}