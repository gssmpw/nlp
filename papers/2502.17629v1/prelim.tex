% !TeX root = main.tex 
%!TEX root = main.tex

\newcommand{\degback}[1]{\ensuremath{\deg^{<}_{\pi}(#1)}}
\newcommand{\Nback}[1]{\ensuremath{N^{<}_{\pi}(#1)}}



\section{Preliminaries}\label{sec:prelim}

\paragraph{Notation.} For any integer $t \geq 1$, we define $[t] := \set{1,2,\ldots,t}$. For a graph $G=(V,E)$, we use $n$ to denote the number of vertices and for each $v \in V$, use $N(v)$ to denote the neighbors of $v$ and $\deg(v)$ as its degree. 

\paragraph{Concentration inequalities.} 

A \textbf{hypergeometric} random variable with parameters $N$, $K$, and $M$ is a discrete random variable in $\IN$ distributed as follows: we have $N$ elements, $K$ of them are marked as `good', 
and we sample $M$ elements uniformly at random and \emph{without} replacement and count the number of good samples. We use a standard result on the concentration of hypergeometric 
random variables. 

\begin{proposition}[cf.~{\cite[Theorem 2.10]{JansonLR11}}]\label{thm:book}
Suppose $X$ is a hypergeometric random variable with parameters $N,K,M$ and thus the expectation $\expect{X} = M \cdot K/N$. Then, for any $t \geq 0$
\[
\Pr(X \leq \expect{X}-t) \leq \exp\paren{-\frac{t^2}{2\expect{X}}}.
\]
\end{proposition}

\subsection{Sublinear Models Considered in this Paper}\label{sec:sublinear}

For completeness, we present a quite brief definition of each of the sublinear algorithms models that we consider. We refer the interested reader to~\cite{AssadiCK19} for more background on these models. 

\paragraph{Graph streaming.} In this model, the input graph $G=(V,E)$ is presented to the algorithm as a stream of its edges ordered in some arbitrary manner. 
The algorithm makes a single pass (or sometimes multiple ones) over this stream while using $\Ot(n)$ memory and at the end of the stream, outputs a solution to the problem on the input graph $G$.  

\paragraph{Sublinear time.} In this model, the input graph $G=(V,E)$ is presented to the algorithm via query access to its adjacency list and matrix or alternatively speaking, using degree-, neighbor-, and pair-queries. 
The algorithm can make its queries adaptively or non-adaptively and then at the end outputs a solution to the problem on the graph $G$. 

\paragraph{MPC.} In this model, the input graph $G=(V,E)$ is originally partitioned across multiple machines, each with $\Ot(n)$ memory. Computation happens in synchronous rounds wherein each machine can send and receive 
$\Ot(n)$-size messages. After the last round, one designated machine outputs a solution to the problem on the graph $G$.  