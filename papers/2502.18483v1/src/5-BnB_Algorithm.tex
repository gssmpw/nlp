\section{Branch-and-Bound Algorithm}
\label{sec:branch-and-bound-algorithm}
In this section, we introduce a branch-and-bound (B\&B) algorithm tailored to our setting, outlined in Algorithm~\ref{bb-algorithm}. The B\&B approach is widely used in sequential decision-making and combinatorial optimization~\cite{learning-to-branch-with-tree-mdps,reinforcement-learning-for-branch-and-bound-optimisation-using-retrospective-trajectories,learning-to-search-in-branch-and-bound-algorithms}. Its effectiveness hinges on the quality of the bounds used to evaluate the search space and eliminate suboptimal branches. For our problem, we derive these bounds based on the recursive structure of the value function $V^\pi$. Specifically, for any policy $\pi$ and positive integer $h \in \mathbb{N}$, the value function $V^\pi$ can be expressed as:
\begin{equation}
  \label{eq:value-function-with-prefix-considerations}
  V^\pi = \sum_{t=1}^{h} \prod_{i=1}^{t} p_{\pi_i}(\bm{b}^{\pi, \bm{q}}_i) + \prod_{i=1}^{h} p_{\pi_i}(\bm{b}^{\pi, \bm{q}}_{i}) \cdot V^{\pi[h+1:]}(\bm{b}^{\pi, \bm{q}}_{h+1}).
\end{equation}


Equation~\eqref{eq:value-function-with-prefix-considerations} decomposes $V^\pi$ into two components: the cumulative rewards for the first $h$ rounds and a recursive term representing the discounted expected value of future rounds. Crucially, substituting $V^{\pi[h+1:]}(\bm{b}^{\pi, \bm{q}}_{h+1})$ with an upper or lower bound yields corresponding bounds for $V^\pi$.

Building on this, we now propose an upper bound. Intuitively, imagine that the RS is entirely certain about the user type. That is, the type would still be sampled according to the belief $\bm b$, but the RS could pick a policy conditioned on the sampled type. In such a case, the RS would pick the type's favorite category indefinitely, leading to an expected social welfare of:
\[
  V^U(\bm{b}) = \sum_{m \in M} \bm{b}(m) \cdot \max_{k \in K} \frac{\bm{P}(k, m)}{1 - \bm{P}(k, m)}.
\]
We stress that the above upper bound is not necessarily attainable. As for the lower bound, we compute the value of the best fixed-action policy w.r.t. the belief, namely,
\[
  V^L(\bm{b}) = \max_{k \in K} \sum_{m \in M} \bm{b}(m) \cdot \frac{\bm{P}(k, m)}{1 - \bm{P}(k, m)}.
\]
Note that the lower bound \emph{is attainable} as it is the value of a valid policy (the best fixed-action policy). 
\begin{lemma} \label{lemma:upper-lower-bound}
  For any belief $\bm b \in \Delta(M)$, it holds that $    V^L(\bm b) \leq V^\star(\bm b) \leq V^U(\bm b)$.
  
  % \[
  %   V^L(\bm b) \leq V^\star(\bm b) \leq V^U(\bm b).
  % \]
\end{lemma}
For ease of notation, for any prefix $\Pi \in \bigcup_{h=1}^{\infty} K^h$ we denote by $\ovv_\Pi, \unv_\Pi$ the integration of $V^U, V^L$ into Equation~\eqref{eq:value-function-with-prefix-considerations}, respectively. Using this notation, $\ovv_\Pi$ acts as an upper bound for the value of any policy that starts with the prefix $\Pi$. Additionally, $\unv_\Pi$ serves as a lower bound of the maximal value of all policies that begin with $\Pi$; namely, $\unv_\Pi \leq \max_{\pi:\text{ begins with }\Pi} V^\pi$. 
\begin{algorithm}[t]
  \caption{B\&B Algorithm for $\prob$}
  \label{bb-algorithm}
  \begin{algorithmic}[1]
    \REQUIRE Instance $\langle M, K, \bm{q}, \bm{P} \rangle$, error term $\varepsilon > 0$
    \ENSURE $\varepsilon$-approximate policy and its value
    \STATE $\tilde \Pi \gets \varnothing$ \COMMENT{The empty prefix} \label{bnbalg:empty_prefix}
    \STATE $\tilde V \gets V^{L}(\bm{q})$ \COMMENT{Lower bound of the empty prefix}
    \STATE $L \gets \text{empty queue}$
    \STATE Append $\tilde \Pi$ to $L$
    \WHILE{$L \neq \emptyset$} 
    \STATE Pop a prefix $\Pi$ from $L$
    \IFTHEN{$\tilde V < \unv_{\Pi}$}{$\tilde V \gets \unv_{\Pi}, \tilde \Pi \gets \Pi$ \alglinelabel{bnbalg:refine}} 
    \FOR{$k \in K$} 
    \IFTHEN{$\ovv_{\Pi \oplus k}-\tilde V > \varepsilon$}{Append $\Pi \oplus k$ to $L$} \alglinelabel{bnbalg:branching}
    \ENDFOR
    \ENDWHILE
    \STATE \textbf{Return} $\tilde \Pi$, $\tilde V$
  \end{algorithmic}
\end{algorithm}

We are ready to present Algorithm~\ref{bb-algorithm}. The algorithm takes as input an instance and an error term $\varepsilon$, outputting an $\varepsilon$-approximately optimal policy and its value. It maintains two key variables: the current best prefix $\tilde \Pi$ and its corresponding value $\tilde V$. Using a queue $L$ to systematically explore policy prefixes, the algorithm implements two critical operations. In Line~\ref{bnbalg:refine}, it performs value refinement by comparing $\tilde V$ against the lower bound $\unv_{\Pi}$ of the examined prefix $\Pi$, updating both $\tilde V$ and $\tilde \Pi$ when an improvement is found. Then, in Line~\ref{bnbalg:branching}, it considers all possible one-step branching of $\Pi$ by appending a category $k$. For each extended prefix $\Pi \oplus k$, it calculates its upper bound $\ovv_{\Pi \oplus k}$. If the potential improvement $\ovv_{\Pi \oplus k} - \tilde{V}$ is more significant than $\varepsilon$, the extended prefix is added to the queue for further exploration. Otherwise, the branch is pruned as it cannot lead to a better solution within the desired accuracy.

The next theorem ensures the $\varepsilon$-optimality of Algorithm~\ref{bb-algorithm}.
\begin{theorem}\label{thm:bb-algorithm-bounded-error}
  For any input $\langle M, K, \bm{q}, \bm{P} \rangle$, $\varepsilon$, Algorithm~\ref{bb-algorithm} terminates after a finite number of steps and returns a value $\tilde V$ such that $V^{\star} - \tilde V \leq \varepsilon$, and a prefix $\tilde \Pi$ such that $\unv_{\tilde \Pi} = \tilde V$.
\end{theorem}
\begin{remark}
  \normalfont
  The output prefix $\tilde{\Pi}$ is a finite sequence of categories, while a policy is defined as an infinite sequence. We can extend $\tilde{\Pi}$ to an approximately optimal policy by using $\tilde{\Pi}$ for the first $h = |\tilde{\Pi}|$ rounds, and then repetitively picking the best-fixed category with respect to $\bm{b}^{\Pi, \bm{q}}_{h+1}$.
\end{remark}

% \begin{remark}
%   \normalfont
%   The output prefix $\tilde \Pi$ is a finite sequence of categories, whereas policies are defined as an infinite sequence; thus, $\tilde \Pi$ is not a valid policy. However, we can extend $\tilde \Pi$ to an approximately optimal policy $\tilde \pi$ as follows: In the first $h = \abs{\tilde \Pi}$ rounds, $\tilde \pi$ acts as $\tilde \Pi$. In round $h+1$ onward, $\tilde \pi$ picks the best fixed category w.r.t $\bm b^{\Pi, \bm q}_{h+1}$. namely, $\forall t > h : \ \tilde \pi_{t} = \argmax_{k \in K} \sum_{m \in M} \bm b^{\Pi, \bm q}_{h+1}(m) \cdot \frac{\bm{P}(k, m)}{1 - \bm{P}(k, m)}.$
% \end{remark}
