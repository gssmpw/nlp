\section{Dynamic Programming-based Algorithms}%Approximating the Optimal Policy}
\label{sec:approximating-the-optimal-policy}

In this section, we present two dynamic programming-based approaches that provide approximations of the optimal expected social welfare. Although inefficient in the general case, such solutions are useful for rectangular instances, namely scenarios in which the number of types $|M|$ or categories $|K|$ is small. We first define the notion of approximation through a finite horizon and then present several methods for achieving it.

While our model lacks an explicit discount factor, Observation~\ref{obs:recursive-formula-of-the-value-function} indicates that an implicit discount emerges through $p_{k}(\bm{b})$. Thus, similar to well-known results in MDPs, one can approximate the value function over an infinite horizon by addressing a finite-horizon problem with a sufficiently large horizon~\citep[Sec.~17.2]{ai-a-modern-approach}. To that end, we define the finite-horizon value function with a horizon $H$ as $V_H^{\pi}(\bm{b})=\sum_{m \in M}\bm q(m)\E{\min \{H, F(m,\pi)\}}$. In other words, $V_H^{\pi}(\bm{b})$ is the value function given that the session terminates after $H$ rounds. Lemma~\ref{lemma:closed-form-representations-of-the-value-function} suggests that $V_H^{\pi}(\bm{b})$ is also given by $V_H^{\pi}(\bm{b}) = \sum_{t=1}^{H} \prod_{j=1}^{t} p_{\pi_{j}}(\bm{b}^{\pi, \bm{b}}_j)$.

Next, we denote $p_{\max}$ as the largest entry in the matrix~$\bm P$, assuming $p_{\max} < 1$ \footnote{If \( p_{\max} = 1 \), adopting a policy \( \pi = (k)_{t=1}^\infty \) where \( \bm{P}_{k,m} = 1 \) for some type \( m \) results in infinite expected social welfare.}, and use it to bound the gap between infinite and finite horizon optimal policies.
% Next, let $p_{\max}$ denote the highest entry in the matrix~$\bm P$,
% where we assume $p_{\max} < 1$.\footnote{If \( p_{\max} = 1 \), adopting a policy \( \pi = (k)_{t=1}^\infty \) where \( \bm{P}_{k,m} = 1 \) for some type \( m \) results in infinite expected social welfare.} We bound the gap between the optimal policies for the infinite and finite horizon.
\begin{lemma} \label{lemma:finite-horizon-approximation}
 For any $\varepsilon >0$, it holds that $V^{\star} \leq \max_{\pi'} V_{H(\varepsilon)}^{\pi'}+ \varepsilon$, where 
 $H(\varepsilon) = \left\lceil \log_{p_{\max}} \frac{\varepsilon (1 - p_{\max})}{p_{\max}} \right\rceil$.
\end{lemma}
Lemma~\ref{lemma:finite-horizon-approximation} reduces the task of finding an approximately optimal policy to finding an (exact or approximate) optimal policy for the finite-horizon case. For the rest of the section, we develop such solutions for a horizon of $H$.
\paragraph{Small number of categories}
%Consider the brute-force approach, which evaluates all $K^H$ possible policies to find the optimal one. This becomes infeasible for large $H$, even with small $K$. However, dynamic programming can exploit the fact that belief updates are order-invariant, significantly reducing the computational effort.
Consider the brute-force approach, which evaluates all $K^H$ possible policies to find the optimal one. This becomes infeasible for large $H$, even with small $K$. However, we can exploit the order-invariance of belief updates: the belief depends only on the multiset of selected categories, not their sequence. Thus, redundant calculations can be avoided by iteratively grouping sequences with the same multiset and retaining only the highest-performing sequence from each group.
\begin{proposition} \label{prop:backward-induction-approximation}
  We can find $\pi^\star_H = \argmax_{\pi'} V^{\pi'}_H$ in a runtime of $O\left( (H + \abs{K}) ^ {\abs{K}} \cdot \abs{K} \cdot \abs{M} \right)$.
\end{proposition}
\paragraph{Small number of user types}
If $M$ is small, we can employ a different approach. Recall that beliefs are points in the simplex $\Delta(M)$. Thus, we can discretize the belief simplex and execute dynamic programming. Specifically, we adopt the approach of \citet{point-based-value-iteration-an-anytime-algorithm-for-pomdps} and obtain:
\begin{proposition} \label{prop:discretization-approximation}
  We can find a policy $\pi$ that satisfies $V^{\pi}_H \geq \max_{\pi'} V^{\pi'}_H - \varepsilon$ in a runtime of \\ $O\left( H \cdot \abs{K} \cdot \abs{M} \cdot \left( \frac{1}{\varepsilon \cdot (1 - p_{\max})^2} \right)^{2\abs{M} - 2} \right)$.
\end{proposition}

