\section{Experiments}\label{sec:experiments}

In this section, we conduct experiments with two goals in mind. First, we complement our convergence result from Theorem~\ref{thm:convergence} by demonstrating that, in practice, the belief walk converges rather quickly. Second, using simulated data, we compare the performance of Algorithm~\ref{bb-algorithm} to a state-of-the-art benchmark. 

\paragraph{Simulation details}
We generate instances using a random sampling procedure. We generate $\bm P$ by independently sampling latent vectors for a given number of categories and user types. Namely, we sample a latent vector for each user type and category from a normal distribution, computing entries of $\bm P$ as negated cosine distances (representing a user's affinities to a category), and normalizing these entries. We generate $\bm q$ by independently sampling logits from a normal distribution. Then, we transform them into a categorical distribution through the softmax function. 
The simulations were conducted on a standard CPU-based PC. Further details appear in \apxref{sec:auxiliary-details-about-the-experiments}. 

\begin{figure}
   \centering
   \includegraphics[width=.5\linewidth]{simulations/uncertainty.png}
   \caption{Convergence of beliefs under optimal policies. The number of categories is set to $10$. The x-axis is the number of rounds, and the y-axis is uncertainty in the user type. The transparent lines illustrate a few individual runs and the solid lines are averages over $500$ runs.}
   \label{fig:uncertainty}
\end{figure}

\paragraph{Convergence of beliefs}
Theorem \ref{thm:convergence} establishes that the optimal policy eventually converges to picking a fixed category. While the theorem guarantees convergence, it does not provide explicit rates. Equivalently, convergence can be analyzed in terms of certainty about a user's type, represented by proximity to the vertex to which the belief walk converges (recall the proof sketch of Theorem~\ref{thm:convergence}). Since beliefs update according to Bayes' rule, they converge at a geometric rate once the policy becomes fixed. In other words, further exploration yields diminishing returns when a belief is sufficiently close to a vertex. Thus, it is tempting to assume that the optimal policy myopically maximizes value for that vertex. On the other hand, a poorly chosen myopic policy can fail drastically, as Proposition~\ref{prop:myopic-policy-suboptimality} illustrates. We resolve these conflicting observations through simulations.

Figure \ref{fig:uncertainty} shows how \emph{uncertainty} in user type, defined as the $l_1$-distance from the vertex to which the belief converges under the optimal policy, evolves throughout the session. We vary the number of user types while fixing the number of categories. For each problem size, we report the averaged uncertainty and several individual runs. Despite the heterogeneity of individual runs, their geometric convergence property roughly transfers to averaged curves: Exponential functions fitted to these curves are almost identical to the originals, with correlation coefficients of at least $R^2=0.98$. This matches our intuition that early rounds are most important in terms of both expected reward and information.

Analyzing individual runs reveals notable patterns. While in some sessions, the optimal policies are fixed from the start, in others, recommendations switch (as characterized by jumps in the slope). This reflects the short-term vs. long-term reward trade-off discussed throughout the paper: The optimal policy may initially prioritize immediate rewards before switching to a riskier recommendation that increases certainty and earns more in the long run. 
Despite this, all the presented curves strictly decrease, suggesting that certainty increases monotonically. However, we found that in rare cases, the optimal policy can move away from a vertex before converging to it.
This resolves the above conflict: Even if the belief approaches some vertex, the optimal policy may eventually lead to a different vertex. We exemplify this behavior in \apxref{sec:belief walks}.

\begin{figure*}
\centering
\begin{subfigure}{0.02\textwidth}
   \centering
   \includegraphics[width=\linewidth]{simulations/time_comp_y_axis_caption.png}
   \vspace{0.7cm} % Add vertical space
\end{subfigure}%
\hspace{0.1cm} % Add horizontal space between subfigures
\begin{subfigure}{0.3\textwidth}
   \centering
   \includegraphics[width=\linewidth]{simulations/types_baseline_time.png}
   \caption{x-axis varies types; 10 categories}
   \label{fig:types_time}
   \vspace{0.5cm} % Add vertical space
\end{subfigure}%
\hspace{0.2cm} % Add horizontal space between subfigures
\begin{subfigure}{0.307\textwidth}
   \centering
   \includegraphics[width=\linewidth]{simulations/types_and_actions_baseline_time.png}
   \caption{x-axis varies types and categories}
   \label{fig:actions_types_time}
   \vspace{0.5cm} % Add vertical space
\end{subfigure}%
\hspace{0.2cm} % Add horizontal space between subfigures
\begin{subfigure}{0.3\textwidth}
   \centering
   \includegraphics[width=\linewidth]{simulations/actions_baseline_time.png}
   \caption{x-axis varies categories; 10 types}
   \label{fig:actions_time}
   \vspace{0.5cm} % Add vertical space
\end{subfigure}%
\caption{Runtime comparison (in milliseconds) between Algorithm~\ref{bb-algorithm} and a POMDP solver SARSOP. Each data point represents an average runtime over $500$ of $\prob$ instances. Shaded intervals represent $95\%$ bootstrap confidence intervals of the empirical average. Both algorithms stop when they reach a precision of $\varepsilon=10^{-6}$.\label{fig:sarsop}}
\end{figure*}


\paragraph{Runtime comparison}
Our model is novel, so there are no specifically tailored baselines. However, since it can be cast as a POMDP, we can compare Algorithm~\ref{bb-algorithm} with more general solvers. As a baseline, we have chosen SARSOP, a well-known offline point-based POMDP solver \cite{kurniawati2009sarsop}. While Algorithm~\ref{bb-algorithm} is straightforward, 
SARSOP is rather complex. It represents the optimal policy through $\alpha$-vectors (a convex piece-wise linear approximation of the value function) and clusters sampled beliefs to estimate the values of new ones. We used an open-source implementation of SARSOP,\footnote{\url{https://github.com/AdaCompNUS/sarsop}} and, for a fair comparison, implemented Algorithm~\ref{bb-algorithm} in the same language. 

Figure~\ref{fig:sarsop} presents the runtime comparison between Algorithm~\ref{bb-algorithm} and SARSOP. The statistical tests that support the comparison are deferred to \apxref{sec:auxiliary-details-about-the-experiments}. While Algorithm~\ref{bb-algorithm} dominates SARSOP on rectangular problems with a few categories (Figure~\ref{fig:types_time}) and overperforms SARSOP on square problems (Figure~\ref{fig:actions_types_time}), it underperforms when the number of categories is much higher than the number of user types (Figure~\ref{fig:actions_time}). 

We hypothesize that this result is due to SARSOP more effectively dealing with similar categories (similar associated rows in the matrix $\bm P$) through the $\alpha$-vector representation and clustering heuristic. Another explanation is that Algorithm~\ref{bb-algorithm} explores the policy space; hence, the branching factor is the number of categories. In contrast, SARSOP explores the belief space, whose dimension is the number of types. Consequently, we could expect SARSOP to struggle in cases with many user types and Algorithm~\ref{bb-algorithm} to encounter challenges in cases with many categories. Overall, each algorithm excels under different conditions.

\section{Experiments with Real-World Data} 
%Our model assumes structural properties that differ significantly from standard recommender systems through its requirements for a denser representation of the system through clusters of users and items. This section aims to delineate the essential transformations between these paradigms, demonstrating how to connect conventional sparse rating data with the parameters of our model. 
Beyond the simulations in Section~\ref{sec:experiments}, we also conducted experiments with the Movielens 1M dataset~\cite{harper2015movielens}. To demonstrate the applicability of our approach, we outline below how we transform sparse rating matrices into the required model parameters.

Real-world RS datasets typically consist of a sparse user-item rating matrix, where observed entries represent user ratings for items, with the majority of entries being unobserved. This representation differs from our model's requirements of a dense probability matrix between user types and content categories, accompanied by a prior distribution over user types; hence, our model cannot be applied directly, and the following two transformations are required.

First, the sparse rating matrix must be aggregated into a concise representation through clustering of users and items. This is a well-studied task and several solutions have been proposed in the literature, such as Spectral co-clustering~\cite{coclustering} or DBSCAN~\cite{dbscan}. Second, the clustered data must be transformed into model parameters. One straightforward way to achieve this is to construct the preference matrix by computing mean ratings within cluster pairs and normalizing to $[0,1]$, and deriving the prior distribution from cluster sizes. We defer this analysis to \apxref{sec:movielens}. Our empirical investigation using this real-world dataset substantiates the qualitative patterns observed in Section~\ref{sec:experiments}. Importantly, we stress that the qualitative results we obtain in this section extend beyond the synthetic setup to real-world datasets.


%We demonstrate this methodology using the MovieLens 1M dataset in \apxref{sec:movielens}. Specifically, we provide comprehensive implementation details and extend our comparison with the baseline algorithms from synthetic preference matrices to parameters derived from empirical rating data.  