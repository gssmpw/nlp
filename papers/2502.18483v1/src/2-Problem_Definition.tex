\section{Problem Definition}
\label{sec:problem-definition}

In this section, we formally introduce our model alongside the optimization problem the recommender system aims to solve. We then provide an illustrative example and explain why this problem necessitates careful planning by showing the sub-optimality of naive and seemingly optimal solutions.

\subsection{Our Model}
In this section, we formally define the \textbf{Rec}ommendation with \textbf{A}ggregated \textbf{P}references under \textbf{C}hurn, or $\prob$ for abbreviation. An instance of the problem consists of several elements, as we formally describe below.

We consider a set $U$ of users who interact with the RS.
The RS does not have access to user information; instead, it relies on aggregated information about the users. Specifically, we assume that there is a finite set $M$ of \emph{user types}, with each type representing a persona, i.e., a cluster of homogeneous users with similar preferences. Each user in $U$ is associated with precisely one user type in $M$ according to its preferences. Accordingly, we denote by $m(u)\in M$ the type of user $u\in U$. The RS has a prior distribution $\bm{q}$ on the elements of $M$, that is $\bm{q} \in \Delta(M)$. This prior distribution reflects the likelihood of a new user being of any given type. %This might involve the proportion of users that fall into each type and the arrival rate of different users to the system. 

In this paper, we assume that contents are abstracted into broader \emph{categories}, each representing a group of similar items. This abstraction allows the RS to recommend content by selecting from a finite set $K$ of categories.

Lastly, there is a user-type preferences matrix $\bm{P}$ with dimensions $[0,1]^{K \times M}$. Each element $\bm{P}(i,j)$ signifies the likelihood that category $i$ satisfies a user of type $j$. We stress that $\bm{P}$ contains information on user types but not on specific users. The RS has complete information on the user-type preference matrix $\bm P$ and prior user-type distribution $\bm q$.

\paragraph{User session}
A user session starts when a user $u \in U$ enters the RS. The RS lacks access to any information about $u$; thus, it only knows that $m(u)$ is distributed according to $\bm q$. The session consists of rounds. In each round $t$, for $t\in \{1,\dots,\infty\}$, the RS recommends a category $k_t \in K$. Afterward, the user gives binary feedback: They either like the item, occurring with a probability of $P(k_t, m(u))$, or they dislike it with the complementary probability. If the user likes the recommended category, the RS receives a reward of $1$, and the session continues for another round. However, when the user ultimately dislikes a recommended category, the RS earns a reward of $0$, and the session concludes as the user leaves the RS.

\paragraph{Recommendation policy}
The RS produces recommendations according to a recommendation \emph{policy}. Recommendation policies can depend solely on the current user session and the history within the session. That is, in round $t$ of the session, the policy can depend on histories of the form $\left(K,\{0,1\} \right)^{t-1}$, where each tuple comprises a recommended category and its corresponding binary feedback. However, since a feedback of zero (dislike) leads to ending the session, we can succinctly represent a policy as a sequence of recommendations. Namely, we represent a policy $\pi$  as the infinite series of categories $\pi = (k_t)_{t=1}^{\infty}$. 
Observe that $k_t$ will only be recommended if the user liked $k_{t-1}$, as otherwise the user would leave. For convenience, we use $\pi[t:]$ to denote the recommendation sequence of $\pi$ starting from the $t$'th round, that is,  $\pi[t:] = \left( k_{i} \right)_{i=t}^{\infty}$.


\paragraph{Social welfare}
We describe the utility of each user type as the count of times it engages with the platform and provides positive feedback, i.e., the number of likes. Let $F(m(u),\pi)$ represent the r.v. that counts the number of likes given by a user of type $m(u)$. This count is influenced by both the user type $m(u)$ and the recommendation policy $\pi$. We define $V^\pi$ as the expected social welfare, which is the mean utility of users under the recommendation policy $\pi$. That is,
\begin{equation*}
    \Scale[0.97
    ]{V^\pi = \E{\frac{1}{\abs{U}}\sum_{u \in U} F(m(u),\pi)} = \sum_{m \in M}\bm q(m)\E{F(m,\pi)}},
\end{equation*}
where the equality follows from the definition of the prior $\bm q$. To be consistent with the literature, we refer to $V^\pi$ as the \emph{value function}; we use both terms interchangeably.
As the goal of the recommender is to maximize expected social welfare, we define \emph{optimal policy} $\pi^\star$ as $\pi^\star \in \argmax_{\pi} V^\pi$ and the corresponding optimal expected social welfare $V^\star$ as $V^\star = V^{\pi^\star}$.

\paragraph{Useful notation}
We emphasize that a policy $\pi$ is a function of $\bm {P}$ and $\bm {q}$, expressed as $\pi=\pi(\bm P, \bm q)$. For abbreviation, it is also convenient to denote this as $\pi(\bm q)$ when $\bm P$ is known from the context. This notation extends to any belief $\bm b \in \Delta(M)$, not just the prior $\bm q$. We make the same abuse of notation for the value function $V^\pi$. That is, we let $V^\pi(\bm b)=\sum_{m \in M}\bm b(m) \E{F(m,\pi)}$ and stress that $V^\pi(\bm q)=V^\pi$. Additionally, we use the star notation $\pi^\star(\bm b), V^\star(\bm b)$ to denote optimal policy and value function w.r.t. the belief $\bm b$. Finally, we let $p_{k}(\bm{b})$ denote the expected \emph{immediate reward}, specifically, the probability that a user will like category $k$ assuming that their type is drawn from $\bm{b}$; that is, $p_{k}(\bm{b}) = \sum_{m \in M} \bm{b}(m) \bm{P}(k, m)$.

\subsection{Bayesian Updates}
At the beginning of each user session, the RS is only informed about the prior $\bm q$. However, it rapidly acquires more information about the user through their feedback. For instance, if a user likes a recommendation of an exotic category favored by only a small subset of user types, we can conclude that the user probably belongs to that subset of types. The RS can then update its \emph{belief} over the current user type, where the belief is a point in the user type simplex, and use it in its future suggestions. 

We employ Bayesian updates to incorporate the new information after user feedback. Starting from a belief $\bm b$ and recommending a category $k$, we let $\tau(\bm{b}, k)$ denote the new belief over user types in case of positive feedback. Namely, $\tau$ is a function $\tau: \Delta(M) \times K \to \Delta(M)$ such that for every belief $\bm{b} \in \Delta(M)$, category $k \in K$ and type $m \in M$,
\begin{equation}\label{eq:bayesian update}
    \tau(\bm{b}, k)(m) =  \frac{\bm{b}(m) \cdot \bm{P}(k,m)}{\sum_{m' \in M} \bm{b}(m') \cdot \bm{P}(k,m')}.
\end{equation}

We stress that Bayesian updates are crucial only after positive feedback, as the RS can utilize this new information for future recommendations. Negative feedback, though still informative, ends the session, preventing the RS from using the new information. We can further use Bayesian updates to obtain a recursive definition of the value function.
\begin{observation}
    \label{obs:recursive-formula-of-the-value-function}
    For every policy $\pi$ and belief $\bm{b} \in \Delta(M)$,
    \[
        V^{\pi}(\bm{b}) = p_{\pi_1}(\bm{b}) \left( 1 + V^{\pi[2:]}(\tau(\bm{b}, \pi_1)) \right).
    \]
\end{observation}
Observation~\ref{obs:recursive-formula-of-the-value-function} provides a Bellman equation-like representation of the value function by isolating the immediate reward of the first round $p_{\pi_1}(\bm{b})$ and the future rewards (implicitly discounted by $p_{\pi_1}(\bm{b})$). It showcases the fundamental exploration-exploitation in our setting: On the one hand, exploitation involves selecting the category $k$ that currently maximizes $p_{k}(\bm{b})$, focusing on immediate reward. On the other hand, exploration aims at steering the updated belief $\tau(\bm{b}, k)$ to a more informative position, thus increasing future rewards. Next, we use the notion of belief walk to assess how policies navigate the belief simplex.

\begin{definition}[Belief Walk]
    The \emph{belief walk} induced by a policy $\pi$ starting at belief $\bm{b}$ is the sequence $( \bm{b}^{\pi, \bm{b}}_t)_{t=1}^{\infty}$, where $\bm{b}^{\pi, \bm{b}}_1 = \bm{b}$ and for all $t > 1$ , $\bm{b}^{\pi, \bm{b}}_t = \tau(\bm{b}^{\pi, \bm{b}}_{t-1}, \pi_{t-1}).$
\end{definition}

To enhance clarity, we provide geometric illustrations of belief walks induced by various recommendation policies in \apxref{sec:belief walks}. Using this notion, we can delineate the value function with closed-form expressions, which will be useful in later analyses. 
\begin{lemma}\label{lemma:closed-form-representations-of-the-value-function}
    For every policy $\pi$ and belief $\bm{b} \in \Delta(M)$,
    {
    \thickmuskip=2mu plus 2mu
    \[
        V^\pi(\bm{b}) = \sum_{t=1}^{\infty} \prod_{j=1}^{t} p_{\pi_j}(\bm{b}^{\pi, \bm{b}}_j) = \sum_{m \in M} \bm{b}(m) \cdot \sum_{t=1}^{\infty} \prod_{j=1}^{t} \bm{P}(\pi_j, m).
    \]
    }%
\end{lemma}
We defer the proof of Lemma~\ref{lemma:closed-form-representations-of-the-value-function} and all other proofs to the appendix.




\subsection{Illustrating Example and Sub-optimality of Myopic Recommendations}
\begin{example}\label{example:body}
    \normalfont
    Consider a $\prob$ instance with two user types $M=\{m_1, m_2\}$ and two categories $K=\{k_1, k_2\}$. The preference matrix and the user type prior are:
    \renewcommand{\kbldelim}{(}% Left delimiter
    \renewcommand{\kbrdelim}{)}% Right delimiter
    \[
        \bm{P} = \kbordermatrix{
            & m_1 & m_2  \\
            k_1 & 0.95 & 0.1  \\
            k_2 & 0.79 & 0.81
        }, \quad
        \bm{q} =
        \kbordermatrix{
            &  \\
            m_1 & 0.5 \\
            m_2 & 0.5
        }.
    \]

    To interpret these values, note that user type $m_2$ likes category $k_1$ with a probability of $0.1$. In addition, recommending category $k_2$ to a random user yields an expected immediate reward of $p_{k_2}(\bm q)=0.5 \cdot 0.79 + 0.5 \cdot 0.81 = 0.8$.

    % For example, user type $m_2$ likes category $k_1$ with a probability of $0.1$. In addition, recommending category $k_2$ to a random user yields an expected immediate reward of $p_{k_2}(\bm q)=0.5 \cdot 0.79 + 0.5 \cdot 0.81 = 0.8$.

    A prudent policy to adopt is the \emph{myopic} policy $\pi^m$, which is defined to be the policy that recommends the highest yielding category in each round, and afterward updates the belief. In this instance, the myopic policy is $\pi^m = \left( k_2 \right)_{t=1}^{\infty}$, as $k_2$ provides the better expected immediate reward compared to $k_1$ for the prior, and the belief update will only increase the preference for $k_2$.
    Under this policy, the expected social welfare is the sum of two infinite geometric series, one for each user type; namely, $V^{\pi^m} = 0.5\cdot \frac{0.79}{1 - 0.79} +0.5 \cdot \frac{0.81}{1 - 0.81} =4.01$.

    While unintuitive at first glance, the optimal policy $\pi^\star = \left( k_1 \right)_{i=1}^{\infty}$ achieves an expected value of $V^\star = 0.5\cdot\frac{0.95}{1-0.95} + 0.5\cdot\frac{0.1}{1-0.1} = 9.55$, outperforming the myopic policy.
\end{example}
Beyond exemplifying our setting and notation, Example~\ref{example:body} demonstrates that myopic policies can be sub-optimal. In fact, the next proposition shows that the sub-optimality gap could be arbitrarily large, highlighting the need for adequate planning.
\begin{proposition} \label{prop:myopic-policy-suboptimality}
    For every $d \in \mathbb R_+$, there exists an instance \( \left\langle M, K, \bm{q}, \bm{P} \right\rangle \) such that
    \( V^{\pi^{m}} \cdot d \leq V^{\star} \),
    where $\pi^{m}$ is the myopic policy for the instance.
\end{proposition}
