\section{Showcasing Belief Walks under Various Policies} \label{sec:belief walks}
In this section, we illustrate the structure of belief walks under varying policies, showcasing the different behaviors and trade-offs that arise in the context of belief walks. Each graph corresponds to a different instance with three categories and three user types ($\abs{M} = \abs{K} = 3$). We provide the full details of the instances below. In each graph, we plot the belief walk induced by three policies. Each belief in the 3-dimensional simplex is characterized by a probability for the first type $\bm b(m_1)$, given in the x-axis, probability in the second type $\bm b(m_2)$, given in the y-axis, and probability for the third type $\bm b(m_3)$, given implicitly by $\bm b(m_3) = 1 -\bm b(m_1) - \bm b(m_2)$. The plotted policies are:
\begin{enumerate}
\item The optimal policy, calculated using Algorithm~\ref{bb-algorithm}.
\item The best fixed-action policy, which always recommends the category that maximizes the value function over all single-action policies w.r.t the prior. Put formally - \[ \pi^f = \left( \tilde k \right)_{t=1}^{\infty}, \quad \tilde k = \argmax_{k \in K} \sum_{m \in M} \bm{q}(m) \cdot \frac{\bm{P}(k, m)}{1 - \bm{P}(k, m)}. \]
For the rest of the section, we abbreviate "best fixed-action policy" to "BFA policy".
\item The myopic policy, which always recommends the category that yields the highest immediate reward in the current belief and then updates its belief afterward. Formally, \[ \pi^m = \left( k_t \right)_{t=1}^{\infty}: k_i = \argmax_{k \in K} \sum_{m \in M} \bm{b_i}(m) \cdot \bm{P}(k, m), \ \ \bm b_1 = \bm q, \ \ \forall i > 1: \bm b_i = \tau(\bm b_{i-1}, k_{i-1}). \]
\end{enumerate}

Each belief in the belief path produced by the optimal policy is assigned a numerical value based on its sequence position. The plots display these values as small black numbers beside some arrows. Each arrow signifies a Bayesian update followed by a recommendation, with the arrow's direction indicating the movement from the previous belief to the updated belief. The black dot depicts the initial prior $\bm q$ over the user type, marking the starting point for any plotted belief walk.

\begin{figure}
\centering
\begin{subfigure}{0.45\textwidth}
    \includegraphics[width=\linewidth]{new_images_with_multiple_policies/P1.png}
    \caption{Figure accompanying Example~\ref{example:1} - the optimal policy avoids myopic behavior.}
    \label{fig:belief-walk1}
\end{subfigure}
\hfill
\begin{subfigure}{0.45\textwidth}
    \includegraphics[width=\linewidth]{new_images_with_multiple_policies/P2.png}
    \caption{Figure accompanying Example~\ref{example:2} - all policies converge to the same vertex.}
    \label{fig:belief-walk2}
\end{subfigure}

\vspace{1em}

\begin{subfigure}{0.45\textwidth}
    \includegraphics[width=\linewidth]{new_images_with_multiple_policies/P3.png}
    \caption{Figure accompanying Example~\ref{example:3} - the optimal policy diverges from the BFA policy prefix and forsakes the myopic policy for faster convergence.}
    \label{fig:belief-walk3}
\end{subfigure}
\hfill
\begin{subfigure}{0.45\textwidth}
    \includegraphics[width=\linewidth]{new_images_with_multiple_policies/P4.png}
    \caption{Figure accompanying Example~\ref{example:4} - there is a decrease followed by an increase in the belief of the type the belief walk converges to.}
    \label{fig:belief-walk4}
\end{subfigure}
\caption{Belief walks under different policies}
\label{fig:belief-walks}
\end{figure}

\begin{example} \label{example:1}
The $\prob$ instance in Figure~\ref{fig:belief-walk1} is $\langle K = \{ k_1, k_2, k_3 \},\ M = \{ m_1, m_2, m_3 \},\ \bm{q}_1,\ \bm{P}_1 \rangle$, where:
\begin{itemize}
\item $\bm{q}_1 = \begin{pmatrix} 0.1713 \\ 0.4465 \\ 0.3822 \end{pmatrix}$
\item $\bm{P}_1 = \begin{pmatrix} 0.8611 & 0.4591 & 0.6862 \\ 0.0969 & 0.5531 & 0.8604 \\ 0.5055 & 0.1430 & 0.8879 \\ \end{pmatrix}$
\end{itemize}
In this instance, the optimal and BFA policies converge to the user type $m_3$, whereas the myopic policy converges to $m_1$. This observation underlines the central idea of Proposition~\ref{prop:myopic-policy-suboptimality}, which asserts that the myopic policy may overlook future rewards, sometimes leading to convergence to a suboptimal user type. Even though the optimal and BFA policies reach the same user type, their pathways differ. This underscores the importance of meticulous initial exploration, as the sequence of recommendations before convergence significantly influences the overall rewards.
\end{example}

\begin{example} \label{example:2}
The $\prob$ instance in Figure~\ref{fig:belief-walk2} is $\langle K = \{ k_1, k_2, k_3 \},\ M = \{ m_1, m_2, m_3 \},\ \bm{q}_2,\ \bm{P}_2 \rangle$, where:
\begin{itemize}
\item $\bm{q}_2 = \begin{pmatrix} 0.3844 \\ 0.1197 \\ 0.4959 \end{pmatrix}$
\item $\bm{P}_2 = \begin{pmatrix} 0.6848 & 0.9100 & 0.5457 \\ 0.7741 & 0.8284 & 0.5833 \\ 0.1931 & 0.9127 & 0.5273 \\ \end{pmatrix}$
\end{itemize}
In this instance, all three policies converge to $m_2$, albeit through different paths. Initially, for the first three recommendations, the optimal policy aligns with the myopic policy. Later, it diverges to secure higher future rewards, considering that it might get lower immediate rewards.
\end{example}

\begin{example} \label{example:3}
The $\prob$ instance in Figure~\ref{fig:belief-walk3} is $\langle K = \{ k_1, k_2, k_3 \},\ M = \{ m_1, m_2, m_3 \},\ \bm{q}_3,\ \bm{P}_3 \rangle$, where:
\begin{itemize}
\item $\bm{q}_3 = \begin{pmatrix} 0.2972 \\ 0.4001 \\ 0.3027 \end{pmatrix}$
\item $\bm{P}_3 = \begin{pmatrix} 0.5492 & 0.0560 & 0.8878 \\ 0.2195 & 0.8576 & 0.2072 \\ 0.7674 & 0.7992 & 0.4051 \\ \end{pmatrix}$
\end{itemize}
In this instance, the BFA policy deviates from optimal and myopic policies early. Initially, these two policies align in their recommendations until the optimal policy transitions from belief (7) to belief (8), as shown in the graph. Although this recommendation provides a lower immediate reward than the myopic recommendation, it leads to a belief state much closer to $m_2$ than the updated belief after the myopic recommendation. This emphasizes the long-term rewards in this scenario, highlighting the common trade-off between short-term and long-term goals in sequential decision-making.
\end{example}

\begin{example} \label{example:4}
The $\prob$ instance in Figure~\ref{fig:belief-walk4} is $\langle K = \{ k_1, k_2, k_3 \},\ M = \{ m_1, m_2, m_3 \},\ \bm{q}_4,\ \bm{P}_4 \rangle$, where:
\begin{itemize}
\item $\bm{q}_4 = \begin{pmatrix} 0.3755 \\ 0.3921 \\ 0.2324 \end{pmatrix}$
\item $\bm{P}_4 = \begin{pmatrix} 0.4011 & 0.8521 & 0.8301 \\ 0.7683 & 0.7837 & 0.8314 \\ 0.7674 & 0.7832 & 0.4051 \\ \end{pmatrix}$
\end{itemize}
In this instance, the BFA and myopic policies coincide and act optimally until the 14th step, where they both differ from the optimal policy. This figure is an example of the phenomenon described in Section~\ref{sec:experiments}â€”in the first 14 steps, the entry in the belief walk that corresponds to user type $m_2$ decreases, and one could expect the belief walk to approach a different vertex. Then, in step 15, as the optimal policy diverges from the other two policies, we see a sudden increase in $\bm{b}(m_2)$, which persists throughout the remainder of the belief walk. In conclusion, it is possible to see a decrease before the increase in the certainty of the user type to which the belief walk finally converges.
\end{example}


% \begin{example}
%     \begin{figure}[H]
%     \centering
%     \includegraphics[width=0.5\linewidth]{new_images_with_multiple_policies/P1.png}
%     \caption{The optimal policy avoids myopic behavior.}
%     \label{fig:belief-walk1}
%     \end{figure}

%     The $\prob$ instance in Figure \ref{fig:belief-walk1} is $\langle K = \{ k_1, k_2, k_3 \}, M = \{ m_1, m_2, m_3 \}, \bm{q}_1, \bm{P}_1 \rangle$, where:
%     \begin{itemize}
%       \item $\bm{q}_1 = \begin{pmatrix}
%                     0.1713 \\
%                     0.4465 \\
%                     0.3822
%                 \end{pmatrix}$
%       \item $\bm{P}_1 = \begin{pmatrix}
%                     0.8611 & 0.4591 & 0.6862 \\
%                     0.0969 & 0.5531 & 0.8604 \\
%                     0.5055 & 0.1430 & 0.8879 \\
%                 \end{pmatrix}$
%     \end{itemize}
    
%     In this instance, the optimal and BFA policies converge to the user type $m_3$, whereas the myopic policy converges to $m_1$. This observation underlines the central idea of Proposition~\ref{prop:myopic-policy-suboptimality}, which asserts that the myopic policy may overlook future rewards, sometimes leading to convergence to a suboptimal user type. Even though the optimal and BFA policies reach the same user type, their pathways differ. This underscores the importance of meticulous initial exploration, as the sequence of recommendations before convergence significantly influences the overall rewards.
% \end{example}


% \begin{example}
%     \begin{figure}[H]
%     \centering
%     \includegraphics[width=0.5\linewidth]{new_images_with_multiple_policies/P2.png}
%         \caption{All policies converge to the same vertex.}
%         \label{fig:belief-walk2}
%     \end{figure}
    
%     The $\prob$ instance in Figure \ref{fig:belief-walk2} is $\langle K = \{ k_1, k_2, k_3 \}, M = \{ m_1, m_2, m_3 \}, \bm{q}_2, \bm{P}_2 \rangle$, where:
%     \begin{itemize}
%       \item $\bm{q}_2 = \begin{pmatrix}
%                     0.3844 \\
%                     0.1197 \\
%                     0.4959
%                 \end{pmatrix}$
%       \item $\bm{P}_2 = \begin{pmatrix}
%                     0.6848 & 0.9100 & 0.5457 \\
%                     0.7741 & 0.8284 & 0.5833 \\
%                     0.1931 & 0.9127 & 0.5273 \\
%                 \end{pmatrix}$
%     \end{itemize}
    
%     In this instance, all three policies converge to $m_2$, albeit through different paths. Initially, for the first three recommendations, the optimal policy aligns with the myopic policy. Later, it diverges to secure higher future rewards, considering that it might get lower immediate rewards.
% \end{example}



% \begin{example}
%     \begin{figure}[H]
%     \centering
%     \includegraphics[width=0.5\linewidth]{new_images_with_multiple_policies/P3.png}
%         \caption{The optimal policy diverges from the best fixed-action policy prefix and forsakes the myopic recommendations for faster convergence.}
%         \label{fig:belief-walk3}
%     \end{figure}
    
%     The $\prob$ instance in Figure \ref{fig:belief-walk3} is $\langle K = \{ k_1, k_2, k_3 \}, M = \{ m_1, m_2, m_3 \}, \bm{q}_3, \bm{P}_3 \rangle$, where:
%     \begin{itemize}
%       \item $\bm{q}_3 = \begin{pmatrix}
%                     0.2972 \\
%                     0.4001 \\
%                     0.3027
%                 \end{pmatrix}$
%       \item $\bm{P}_3 = \begin{pmatrix}
%                     0.5492 & 0.0560 & 0.8878 \\
%                     0.2195 & 0.8576 & 0.2072 \\
%                     0.7674 & 0.7992 & 0.4051 \\
%                 \end{pmatrix}$
%     \end{itemize}
    
%     In this instance, the BFA policy deviates from optimal and myopic policies early. Initially, these two policies align in their recommendations until the optimal policy transitions from belief (7) to belief (8), as shown in the graph. Although this recommendation provides a lower immediate reward than the myopic recommendation, it leads to a belief state much closer to $m_2$ than the updated belief after the myopic recommendation. This emphasizes the long-term rewards in this scenario, highlighting the common trade-off between short-term and long-term goals in sequential decision-making.
% \end{example}

% \begin{example}
%     \begin{figure}[H]
%     \centering    \includegraphics[width=0.5\linewidth]{new_images_with_multiple_policies/P4.png}
%         \caption{Decrease followed by an increase in the belief of the type the belief walk converges to.}
%         \label{fig:belief-walk4}
%     \end{figure}
    
%     The $\prob$ instance in Figure \ref{fig:belief-walk4} is $\langle K = \{ k_1, k_2, k_3 \}, M = \{ m_1, m_2, m_3 \}, \bm{q}_4, \bm{P}_4 \rangle$, where:
%     \begin{itemize}
%       \item $\bm{q}_4 = \begin{pmatrix}
%                     0.37554162 \\
%                     0.392054 \\
%                     0.23240438
%                 \end{pmatrix}$
%       \item $\bm{P}_4 = \begin{pmatrix}
%                     0.40109148 & 0.85207045 & 0.83007302 \\
%                     0.76832171 & 0.78372429 &0.83142133 \\
%                     0.7674 & 0.7832 & 0.4051 \\
%                 \end{pmatrix}$
%     \end{itemize}
    
%     In this instance, the BFA and myopic policies coincide and act optimally until the 14th step, where they both differ from the optimal policy. This figure is an example of the phenomena described in Section~\ref{sec:experiments} - in the first 14 steps, the entry in the belief walk that corresponds to user type $m_2$ decreases, and one could expect the belief walk to approach different vertex. Then, in step 15, as the optimal policy diverges from the other two policies, we see a sudden increase in $\bm b(m_2)$, which persists throughout the remainder of the belief walk. In conclusion, it is possible to see a decrease before the increase in the certainty of the user type to which the belief walk finally converges.
% \end{example}



