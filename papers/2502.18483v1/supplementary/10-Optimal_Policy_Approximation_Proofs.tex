\section{Omitted Proofs from Section \ref{sec:approximating-the-optimal-policy}}
\label{sec:approximating-the-optimal-policy-proofs}

\begin{proof}[\normalfont\bfseries Proof of Lemma~\ref{lemma:finite-horizon-approximation}]
  As can be seen in the proof Lemma~\ref{lemma:closed-form-representations-of-the-value-function}, The value function can be represented as
  \[
    V^{\pi}(\bm{b}) = \sum_{t=1}^{H(\varepsilon)} \prod_{j=1}^{t} p_{\pi_{j}}(\bm{b}^{\pi, \bm{b}}_j) + \prod_{t=1}^{H(\varepsilon)} p_{\pi_{t}}(\bm{b}^{\pi, \bm{b}}_t) \cdot V^{\pi[H(\varepsilon)+1:]}(\bm{b}^{\pi, \bm{b}}_{H(\varepsilon)+1}).
  \]
  Notice that for every $L \in \mathbb{N}$, $\prod_{t=1}^{L} p_{\pi_{t}}(\bm{b}^{\pi, \bm{b}}_t) \leq p_{\max}^L$. Additionally, as stated in Equation~\ref{eq:bounded-value}, for every belief $b$ and policy $\pi$ we have that $V^{\pi}(\bm{b}) \leq \frac{p_{\max}}{1-p_{\max}}$.
  Therefore:
  \[
    V^{\pi^\star}(\bm{b}) \leq \sum_{t=1}^{H(\varepsilon)} \prod_{j=1}^{t} p_{\pi^\star_{j}}(\bm{b}^{\pi^\star, b}_j) + p_{\max}^{H(\varepsilon)} \cdot \frac{p_{\max}}{1-p_{\max}} \leq \sum_{t=1}^{H(\varepsilon)} \prod_{j=1}^{t} p_{\pi^\star_{j}}(\bm{b}^{\pi, \bm{b}}_j) + \varepsilon = V_{H(\varepsilon)}^{\pi^\star}(\bm{b}) + \varepsilon \leq \max_{\pi} V_{H(\varepsilon)}^{\pi}(\bm{b}) + \varepsilon.
  \]
\end{proof}

\begin{lemma}
  \label{lemma:transitivity-of-belief-updates}
  Let $\langle M, K, \bm{q}, \bm{P} \rangle$ be an instance of the problem, a belief $\bm{b} \in \Delta(M)$, and 2 categories $k_1, k_2 \in K$. Then:
  \[
    \tau(\tau(\bm{b}, k_1), k_2) = \tau(\tau(\bm{b}, k_2), k_1)
  \]
\end{lemma}

\begin{proof}[\normalfont\bfseries Proof of Lemma~\ref{lemma:transitivity-of-belief-updates}]
 Using the definition of the Bayesian update, the following holds for every $m \in M$:
 \begin{align*}
   \tau(\tau(\mathbf{b}, k_1), k_2)(m) & = \frac{\mathbf{P}(k_2, m) \cdot \tau(\mathbf{b}, k_1)(m)}{\sum_{m' \in M} \mathbf{P}(k_2, m') \cdot \tau(\mathbf{b}, k_1)(m')} \\
   & = \frac{\mathbf{P}(k_2, m) \cdot \frac{\mathbf{P}(k_1, m) \cdot \mathbf{b}(m)}{\sum_{m' \in M} \mathbf{P}(k_1, m') \cdot \mathbf{b}(m')}}{\sum_{m' \in M} \mathbf{P}(k_2, m') \cdot \frac{\mathbf{P}(k_1, m') \cdot \mathbf{b}(m')}{\sum_{m'' \in M} \mathbf{P}(k_1, m'') \cdot \mathbf{b}(m'')}} \\
   & = \frac{\mathbf{P}(k_2, m) \cdot \mathbf{P}(k_1, m) \cdot \mathbf{b}(m)}{\sum_{m' \in M} \mathbf{P}(k_2, m') \cdot \mathbf{P}(k_1, m') \cdot \mathbf{b}(m')} \\
   & = \frac{\mathbf{P}(k_1, m) \cdot \mathbf{P}(k_2, m) \cdot \mathbf{b}(m)}{\sum_{m' \in M} \mathbf{P}(k_1, m') \cdot \mathbf{P}(k_2, m') \cdot \mathbf{b}(m')} \\
   & = \frac{\mathbf{P}(k_1, m) \cdot \frac{\mathbf{P}(k_2, m) \cdot \mathbf{b}(m)}{\sum_{m' \in M} \mathbf{P}(k_2, m') \cdot \mathbf{b}(m')}}{\sum_{m' \in M} \mathbf{P}(k_1, m') \cdot \frac{\mathbf{P}(k_2, m') \cdot \mathbf{b}(m')}{\sum_{m'' \in M} \mathbf{P}(k_2, m'') \cdot \mathbf{b}(m'')}} \\
   & = \frac{\mathbf{P}(k_1, m) \cdot \tau(\mathbf{b}, k_2)(m)}{\sum_{m' \in M} \mathbf{P}(k_1, m') \cdot \tau(\mathbf{b}, k_2)(m')} \\
   & = \tau(\tau(\mathbf{b}, k_2), k_1)(m).
 \end{align*}
This completes the proof of the lemma.
\end{proof}



\begin{algorithm}
   \caption{Dynamic Programming Approximation \label{alg:backward-induction-approximation}}
   \begin{algorithmic}[1]
     \REQUIRE An instance of the problem $\langle M, K, \bm{q}, \bm{P} \rangle$, a horizon $H \in \mathbb{N}$, and an error term $\varepsilon$.
     \ENSURE $BT(q, 0)$ - an approximation of the optimal policy $\pi$.
     \STATE Initialize dicts $V: \Delta(M) \times \{0, 1, \ldots, H\} \to \mathbb{R}$, $BT: \Delta(M) \times \{0, 1, \ldots, H\} \to \bigcup_{i=1}^{H} K^i$.
     \STATE Initialize $V(b, H+1) = 0$ for every $\bm{b} \in \Delta(M) \text{ s.t } \exists \{ n_i \}_{i=1}^{\abs{K}}: \sum_{i=1}^{\abs{K}} n_i = H+1, \tau(q, k_1^{n_1}, k_2^{n_2}, \ldots, k_{\abs{K}}^{n_{\abs{K}}}) = b$.
     \STATE Initialize $BT(b, H+1) = \emptyset$ for every $\bm{b} \in \Delta(M) \text{ s.t } \exists \{ n_i \}_{i=1}^{\abs{K}}: \sum_{i=1}^{\abs{K}} n_i = H+1, \tau(q, k_1^{n_1}, k_2^{n_2}, \ldots, k_{\abs{K}}^{n_{\abs{K}}}) = b$.
     \FOR{$h = H, ..., 1$}
     \FOR{every $\bm{b} \in \Delta(M) \text{ s.t } \exists \{ n_i \}_{i=1}^{\abs{K}}: \sum_{i=1}^{\abs{K}} n_i = h, \tau(q, k_1^{n_1}, k_2^{n_2}, \ldots, k_{\abs{K}}^{n_{\abs{K}}}) = b$}
     \STATE $V(b, h) = \max_{j \in [K]: n_j > 0} \left\{ p_k \cdot \left( 1 + V(\tau(\bm{b}, k_1^{n_1}, k_2^{n_2}, \ldots, k_j^{n_j-1}, \ldots, k_{\abs{K}}^{n_{\abs{K}}}), h+1) \right) \right\}$.
     \STATE $BT(b, h) = \argmax_{j \in [K]: n_j > 0} \left\{ p_k \cdot \left( 1 + V(\tau(\bm{b}, k_1^{n_1}, k_2^{n_2}, \ldots, k_j^{n_j-1}, \ldots, k_{\abs{K}}^{n_{\abs{K}}}), h+1) \right) \right\} \oplus BT(\tau(\bm{b}, k_1^{n_1}, k_2^{n_2}, \ldots, k_j^{n_j-1}, \ldots, k_{\abs{K}}^{n_{\abs{K}}}), h+1)$.
     \ENDFOR
     \ENDFOR
     \STATE $V(q, 0) = \max_{j \in [K]} \left\{ p_k \cdot \left( 1 + V(\tau(q, k_j), 1) \right) \right\}$.
     \STATE $BT(q, 0) = \argmax_{j \in [K]} \left\{ p_k \cdot \left( 1 + V(\tau(q, k_j), 1) \right) \right\} \oplus BT(\tau(q, k_j), 1)$.
   \end{algorithmic}
\end{algorithm}
  
\begin{proof}[\normalfont\bfseries Proof of Proposition~\ref{prop:backward-induction-approximation}]
  We begin by mentioning two conclusions of Lemma~\ref{lemma:transitivity-of-belief-updates}:
  \begin{enumerate}
    \item The order in which we recommend items only affects the accumulated reward, not the belief the system transitioned into.
    \item The number of beliefs that can be reached after $L$ recommendations is at most ${ L + \abs{K} - 1 \choose \abs{K} - 1}$, as the only thing that effects the belief is the number of times each item was recommended (the multiset of categories rather than the sequence).
  \end{enumerate}

  Following those conclusions, we can define a dynamic programming algorithm for solving the finite horizon recommendation problem and analyze its complexity.
  
  For ease of notation, we will denote the composition of $L$ belief updates with the categories \\
  $k_1^1, \ldots, k_1^{n_1}, k_2^1, \ldots, k_2^{n_2}, \ldots, k_{\abs{K}}^1, \ldots, k_{\abs{K}}^{n_{\abs{K}}}$ (where $n_1 + n_2 + \ldots + n_{\abs{K}} = L$) as $\tau(\bm{b}, k_1^{n_1}, k_2^{n_2}, \ldots, k_{\abs{K}}^{n_{\abs{K}}})$. Remember that the order of the categories does not affect the updated belief, so this is well-defined.

  Algorithm~\ref{alg:backward-induction-approximation} takes a bottom-up approach - it starts by calculating the (low-depth) optimal value and policy for every belief that can be encountered at the most advanced layers and then uses Observation \ref{obs:recursive-formula-of-the-value-function} to calculate the optimal values and policies of every belief in the above layer. The process concludes upon arriving at layer $0$ (the root), where the optimal policy, which the algorithm aims to determine, is found. This methodology represents a significant enhancement over the brute-force approach, as it efficiently eliminates sub-optimal prefixes (those dominated by the identical set of actions arranged differently) in early stages, thereby avoiding unnecessary search operations.

  To establish that this algorithm achieves the optimal $H$-horizon value, we proceed by backward induction. Let $V^*_h(\bm{b})$ denote the optimal value achievable from belief $\bm{b}$ with exactly $h$ steps remaining. For the base case $h=0$, we have $V^*_0(\bm{b}) = 0$ for all beliefs $\bm{b}$, which is correctly initialized in the algorithm. For $h > 0$, the Bellman optimality equation gives us:
  
  \[
    V^*_h(\bm{b}) = \max_{k \in K} \left\{ p_k(\bm{b}) \cdot (1 + V^*_{h-1}(\tau(\bm{b}, k))) \right\}.
  \]

  Our algorithm computes precisely these values through its dynamic programming updates. Furthermore, for any belief $\bm{b}$ that can be reached after $t$ recommendations, the algorithm maintains the invariant that $V(\bm{b}, H-t)$ equals $V^*_{H-t}(\bm{b})$. This holds because:
  \begin{enumerate}
    \item The algorithm considers all possible categories at each step.
    \item It uses the correct Bayesian update rule for belief transitions.
    \item It applies the precise recursive formulation from Observation \ref{obs:recursive-formula-of-the-value-function}.
  \end{enumerate}

  Consequently, when the algorithm terminates, $V(\bm{q}, H)$ equals $V^*_H(\bm{q})$, yielding the optimal $H$-horizon value and its corresponding policy.

  Notice that the complexity of depth $h$ in this algorithm is $O\left( { h + \abs{K} - 1 \choose \abs{K} - 1} \cdot \abs{K} \cdot \abs{M} \right)$, as for every belief $b$ that can be reached after $h$ recommendations, we take a maximum over at most $\abs{K}$ categories, and for each category, we calculate the value of the next belief, which is a linear operation in $\abs{M}$.
  Therefore, the total complexity of this algorithm is $O\left( (H + \abs{K})^{\abs{K}} \cdot \abs{K} \cdot \abs{M} \right)$.
\end{proof}


% \begin{algorithm}
%     \caption{Approximation via belief space discretization\label{alg:discretization-approximation}}
%     \begin{algorithmic}[1]
%       \Require An instance of the problem $\langle M, K, \bm{q}, \bm{P} \rangle$, a horizon $H \in \mathbb{N}$, a discretization factor $\delta$.

%       \State $B \gets \delta \text{-discretized belief space}$  \Comment{A finite set of beliefs such that every belief in the simplex is $\delta$-close to some belief in $B$}
%       \State $\Gamma_0 \gets \vec{0} \in \mathbb{R}^{\abs{M}}$.
%       \For{$i = 1, \ldots, H$}
%       \For{every $b \in B$}
%       \State $k^i_b, \alpha^{\max}_b = \argmax_{k \in K, \alpha \in \Gamma_{i-1}} \sum_{m \in M} \bm{b}(m) \cdot \left( \bm{P}(k, m) \left(1 + \alpha(m) \tau(\bm{b}, k) \right) \right)$.
%       \State $\alpha^i_b = \left( \bm{P}(k^{\max}_b, m) \left( 1 + \alpha^{\max}_{\bm{b}}(m) \right) \right)_{m \in M}$.
%       \EndFor
%       \State $\Gamma_i = \cup_{b \in B} \alpha^i_b$.
%       \EndFor
%       \Return $\left\{ k^i_{\bm{b}_i} \quad | \quad \bm{b}_H = q, \quad \forall i < H : \quad \bm{b}_{i+1} = \tau(\bm{b}_i, k^i_{\bm{b}_i}) \right\}$.
%     \end{algorithmic}
% \end{algorithm}

\begin{proof}[\normalfont\bfseries Proof of Proposition~\ref{prop:discretization-approximation}]
  First, we note that the finite horizon version of our problem can be represented as a POMDP with a finite horizon, where the horizon is the maximal number of recommendations until the session terminates.
  Furthermore, the following build transforms our model, which doesn't have an explicit discount factor, into a POMDP with an explicit discount factor. The main trick here is to use $p_{\max} < 1$ as the discount factor and increase the probabilities in $\bm P$ by a factor of $\frac{1}{p_{\max}}$.
  
  There is a need for distinction between the first round, in which the reward isn't discounted, and the rest of the rounds, in which the rewards are discounted. We do that by introducing $2$ states, $m_s, m_f$, for every user type $m$. This distinction is only to handle the discounting - we still don't allow a change in user type. This will translate to the POMDP transitions model where we allow transitions only from $m_s$ to $m_f$ and to an absorbing state, and then transitions only from $m_f$ to $m_f$ and the absorbing state. For example, one cannot transition between $m_f$ to $m'_f$ for user types $m \neq m'$. 
  
  The observations in this POMDP model will be redundant - as long as the system hasn't transitioned into the absorbing state (the user is still in the system), the received observation will be "like." Otherwise, the received observation will be "dislike".

  Now we will formally describe this transformation. Given a $\prob$ instance $\langle M, K, \bm{q}, \bm{P} \rangle$, construct the following POMDP:
  \begin{itemize}
    \item The states are $\{ m_{s} \}_{m \in M} \cup \{ m_{f} \}_{m \in M} \cup \{ \bot \}$. The distinction between $m_{s}$ and $m_{f}$ is used to differentiate between the first round, in which the reward isn't discounted, and the rest of the rounds, in which the rewards are discounted.
    \item The actions are $K$.
    \item The transition function is defined as follows:
          \begin{itemize}
            \item $\bm{P}(k, m_{s}, m_{f}) = \bm{P}(k, m)$ for every $m \in M, k \in K$.
            \item $\bm{P}(k, m_{s}, \bot) = 1 - \bm{P}(k, m)$ for every $m \in M, k \in K$.
            \item $\bm{P}(k, m_{f}, m_{f}) = \frac{\bm{P}(k, m)}{p_{\max}}$ for every $m \in M, k \in K$.
            \item $\bm{P}(k, m_{f}, \bot) = 1 - \frac{\bm{P}(k, m)}{p_{\max}}$ for every $m \in M, k \in K$.
            \item $\bm{P}(k, \bot, \bot) = 1$ for every $k \in K$.
          \end{itemize}
    \item The reward function is defined as follows:
          \begin{itemize}
            \item $r(k, m_{s}, m_{f}) = 1$ for every $m \in M, k \in K$.
            \item $r(k, m_{s}, \bot) = 0$ for every $m \in M, k \in K$.
            \item $r(k, m_{f}, m_{f}) = 1$ for every $m \in M, k \in K$.
            \item $r(k, m_{f}, \bot) = 0$ for every $m \in M, k \in K$.
            \item $r(k, \bot, \bot) = 0$ for every $k \in K$.
          \end{itemize}
    \item The discount factor is $\gamma = p_{\max}$.
    \item The observations are $"like"$ and $"dislike"$.
    \item The observation model is defined as:
          \begin{itemize}
            \item $\bm{P}("like" | k, m_{s}, m_{f}) = 1$ for every $m \in M, k \in K$.
            \item $\bm{P}("dislike" | k, m_{s}, \bot) = 1$ for every $m \in M, k \in K$.
            \item $\bm{P}("like" | k, m_{f}, m_{f}) = 1$ for every $m \in M, k \in K$.
            \item $\bm{P}("dislike" | k, m_{f}, \bot) = 1$ for every $m \in M, k \in K$.
            \item $\bm{P}("dislike" | k, \bot, \bot) = 1$ for every $k \in K$.
          \end{itemize}
    \item The initial belief is $\forall m \in M: b(m_s) = \bm{q}(s), b(m_f) = 0, b(\bot)=0$.
  \end{itemize}

  As in the $\prob$ model, the "dislike" observation ends the episode and the reward is accumulated only for the "like" observations.

  While the constructed POMDP operates over a belief simplex of dimension $2|M|+1$ (accounting for both initial and non-initial states, plus the absorbing state), the reachable belief space is substantially more constrained. By construction, beliefs cannot simultaneously assign non-zero probability to both initial and non-initial variants of any user type, and the absorbing state, once reached, becomes a singular point in the belief space. Consequently, the effective dimensionality of the reachable belief space remains comparable to that of an $|M|$-dimensional simplex, preserving the theoretical guarantees without incurring additional computational complexity from the expanded state space.

  To show the equivalency between the optimal policy in our model and the one in the above POMDP, we will show that both the expected cumulative reward and the updated belief after performing the same sequence of actions are the same. This will be sufficient as those are the components that uniquely define optimal policy behavior.

  Let $k_1, \ldots, k_n$ be a set of actions. In our model, the expected accumulated reward after performing these actions is
  \[
    V = \sum_{m \in M} b(m) \cdot \sum_{t=1}^{n} \prod_{j=1}^{t} P(k_t, m).
  \]
  In the discussed POMDP model, the value will be as follows:
  \begin{align*}
    V & = \sum_{m \in M} \bm b(m) \cdot ( \bm P(k_1, m) + \bm P(k_1, m) \cdot p_{\max} \cdot \frac{\bm P(k_2, m)}{p_{\max}} + \bm P(k_1, m) \cdot \frac{\bm P(k_2, m)}{p_{\max}} \cdot p_{\max}^2 \cdot \frac{\bm P(k_3, m)}{p_{\max}} + \ldots \\
      & + \bm P(k_1, m) \cdot \frac{\bm P(k_2, m)}{p_{\max}} \cdot \frac{\bm P(k_3, m)}{p_{\max}} \cdot \ldots \cdot \frac{\bm P(k_{n-1}, m)}{p_{\max}} \cdot p_{\max}^{n-1} \cdot \frac{\bm P(k_n, m)}{p_{\max}} )      \\
      & =  \sum_{m \in M}\bm  b(m) \cdot \sum_{t=1}^{n} \prod_{j=1}^{t} \bm P(k_t, m).  \\
  \end{align*}

  Regarding the updated belief, according to the Bayesian update and the likelihood that the user will like all these $n$ categories, the updated belief will be:
  \[
    \forall m \in M: \bm b'(m) = \frac{\bm b(m) \cdot \prod_{t=1}^{n} \bm P(k_t, m)}{\sum_{m' \in M} \bm b(m') \cdot \prod_{t=1}^{n} \bm P(k_t, m')}.
  \]

  In the discussed POMDP model, for similar reasons, the updated belief will be:
  \[
    \forall m \in M: \bm b'(m) = \frac{\bm b(m) \cdot \bm P(k_1, m) \cdot \prod_{t=2}^{n} \frac{\bm P(k_t, m)}{p_{\max}}}{\sum_{m' \in M} \bm b(m') \cdot \bm P(k_1, m') \cdot \prod_{t=2}^{n} \frac{\bm P(k_t, m')}{p_{\max}}} = \frac{\bm b(m) \cdot \prod_{t=1}^{n} \bm P(k_t, m)}{\sum_{m' \in M} \bm b(m') \cdot \prod_{t=1}^{n} \bm P(k_t, m')}.
  \]

  In both models, when receiving "dislike" feedback, the user leaves the system, and there is a full certainty that the system transitioned into the absorbing state. This is not relevant for policy equivalence, as all of the policies guarantee the same reward of $0$ when they are in the absorbing state.

  After being convinced of the equivalency of the two models, we can use the POMDPs literature to our advantage.
  Specifically, we use the PBVI algorithm presented in \citet{point-based-value-iteration-an-anytime-algorithm-for-pomdps} to solve the POMDP we constructed in the desired complexity.
  Note that in our constructed POMDP the discount factor is less than 1, so the guarantees of the PBVI algorithm hold.

    
  % We provide the adaptation of the PBVI algorithm to our case in Algorithm~\ref{alg:discretization-approximation}. The proof of its correctness is omitted and can be found in \cite{point-based-value-iteration-an-anytime-algorithm-for-pomdps}.
    
  \paragraph{Complexity} Let $B \subset \Delta(M)$ be an arbitrarily chosen finite subset of the belief simplex. While this discretization is arbitrary, its density—characterized by $\delta = \max_{b' \in \Delta(M)} \min_{b \in B} \|\bm{b} - \bm{b}'\|_1$—determines the approximation quality of our solution. Intuitively, $\delta$ measures how well $B$ covers the belief simplex by quantifying the maximal distance from any belief to its nearest neighbor in $B$.

  The algorithm exhibits a computational complexity of $O(\abs{B}^2 \cdot \abs{K} \cdot \abs{M})$ per iteration, attributable to the evaluation of belief-to-belief transitions under different actions. Across $H$ iterations, this yields a total complexity of $O(H \cdot \abs{B}^2 \cdot \abs{K} \cdot \abs{M})$.

  When $B$ is sufficiently dense—that is, when $\delta$ is adequately small—we can establish rigorous error bounds on our solution. Specifically, the policy returned by the algorithm is guaranteed to be $\frac{R_{\max} \delta}{(1 - \gamma)^2}$-close to the optimal policy of the finite horizon problem, where $R_{\max}$ represents the maximal immediate reward in the POMDP and $\gamma$ denotes the discount factor. For our particular POMDP formulation, selecting $\delta = \varepsilon \cdot (1 - p_{\max})^2$ ensures the algorithm's output remains within $\varepsilon$ of the optimal value.

  We use a well-known result in measure theory for our advantage - the covering number of the high-dimensional simplex is $O\left( \left( \frac{1}{\delta} \right)^{\abs{M} - 1} \right)$. Therefore, one can construct a $\delta$-discretization of the belief simplex by using $O\left( \left( \frac{1}{\delta} \right)^{\abs{M} - 1} \right)$ beliefs.
  
  All in all, the complexity of the algorithm is $O\left( H \cdot \left( \frac{1}{\varepsilon \cdot (1 - p_{\max})^2} \right)^{2\abs{M} - 2} \cdot \abs{K} \cdot \abs{M} \right)$.
This completes the proof of the proposition.
\end{proof}



