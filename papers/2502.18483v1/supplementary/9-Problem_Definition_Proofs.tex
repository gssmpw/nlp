\section{Omitted Proofs from Section \ref{sec:problem-definition}}
\label{sec:problem-definition-proofs}

\begin{proof}[\normalfont\bfseries Proof of Lemma~\ref{lemma:closed-form-representations-of-the-value-function}]
    The first expression can be derived through Observation~\ref{obs:recursive-formula-of-the-value-function}.
    First we prove via induction that for every $n \in \mathbb{N}$ the value function can be represented as $V^{\pi}(\bm{b}) = \sum_{t=1}^{n} \prod_{j=1}^{t} p_{\pi_{j}}(\bm{b}^{\pi, \bm{b}}_j) + \prod_{t=1}^{n} p_{\pi_{t}}(\bm{b}^{\pi, \bm{b}}_t) \cdot V^{\pi[n+1:]}(\bm{b}^{\pi, \bm{b}}_{n+1})$. Then, as each $p_{\pi_{t}}(\bm{b}^{\pi, \bm{b}}_t)$ is a probability smaller than $1$ and the value function is bounded from above by $\frac{p_{\max}}{1-p_{\max}}$, as $n \to \infty$ the expression $\prod_{t=1}^{n} p_{\pi_{t}}(\bm{b}^{\pi, \bm{b}}_t) \cdot V^{\pi[n+1:]}(\bm{b}^{\pi, \bm{b}}_{n+1})$ will decay to $0$, and the value function will converge to the first expression.

    \textbf{Base case:} For $n=1$ it holds directly from Observation~\ref{obs:recursive-formula-of-the-value-function}.

    \textbf{Inductive step:} Assume that the statement holds for $n$, and prove it for $n+1$.
    \begin{align*}
        V^{\pi}(\bm{b}) & = p_{\pi_{1}}(\bm{b})(1 + V^{\pi[2:]}_n(\bm{b}^{\pi, \bm{b}}_{2}))    \\
        & = p_{\pi_{1}}(\bm{b}) + p_{\pi_{1}}(\bm{b}) \cdot \sum_{t=1}^{n} \prod_{j=1}^{t} p_{\pi_{j+1}}(\bm{b}^{\pi, \bm{b}}_{j+1}) + p_{\pi_{1}}(\bm{b}) \cdot \prod_{t=1}^{n} p_{\pi_{t+1}}(\bm{b}^{\pi, \bm{b}}_{t+1}) \cdot V^{\pi[n+1:]}(\bm{b}^{\pi, \bm{b}}_{n+1}) \\
        & = \sum_{t=1}^{n+1} \prod_{j=1}^{t} p_{\pi_{j}}(\bm{b}^{\pi, \bm{b}}_j) + \prod_{t=1}^{n+1} p_{\pi_{t}}(\bm{b}^{\pi, \bm{b}}_t) V^{\pi[n+1:]}(\bm{b}^{\pi, \bm{b}}_{n+1}).  
    \end{align*}
    Where the first equality is due to Observation~\ref{obs:recursive-formula-of-the-value-function} and the second equality follows from applying the induction hypothesis to $V^{\pi[2:]}_n(\bm{b}^{\pi, \bm{b}}_{2})$.

    For the second expression, we can calculate the expected social welfare via conditioning on the sampled user type.
    \[
    V^\pi(\bm b) = \sum_{m \in M}\bm b(m) \E{F(m,\pi)}.
    \]
    Notice that the expected utility from following policy $\pi$ for a user of type $m \in M$ is:
    % \[
    % \E{F(m,\pi)} = \sum_{t=1}^{\infty} \mathbb{P}(\text{User of type } m \text{ hadn't departed by time } t \text{ when following a policy } \pi) \cdot \mathbb{P}(\text{User of type } m \text{ liked the recommendation in time } t 
    % \]
    \begin{align*}
        \E{F(m,\pi)} &= \sum_{t=1}^{\infty} \mathbb{P}(\text{User of type } m \text{ hadn't departed by time } t \text{ when following a policy } \pi) \\
        &\cdot \mathbb{P}(\text{User of type } m \text{ liked the recommendation in time } t) \\
        &= P(\pi_1, m) + \sum_{t=2}^{\infty} \prod_{j=1}^{t-1} \bm P(\pi_j, m) \cdot P(\pi_t, m) \\
        &= \sum_{t=1}^{\infty} \prod_{j=1}^{t} \bm{P}(\pi_j, m).
    \end{align*}
    Together, the expected social welfare can be calculated as
    \[
        V^\pi(\bm b) = \sum_{m} \bm{b}(m) \sum_{t=1}^{\infty} \prod_{j=1}^{t} \bm{P}(\pi_j, m).
    \]
This completes the proof of the lemma.
\end{proof}

\begin{proof}[\normalfont\bfseries Proof of Proposition~\ref{prop:myopic-policy-suboptimality}]
    Set some arbitrary $d \in \mathbb{R_+}$, and consider the following $\prob$ instance:
    \[
        \left\langle
        K = \{ k_1, k_2 \}, \quad M = \{ m_1, m_2 \},
        \bm q = \begin{pmatrix}
            0.5 \\
            0.5
        \end{pmatrix},
        \bm P = \begin{pmatrix}
            \frac{8d}{1+8d} & 0   \\
            0.8             & 0.8
        \end{pmatrix}
        \right\rangle.
    \]
    In this instance, the myopic policy will always choose $k_2$, as it has a higher immediate reward, and as $\bm{P}(k_2, m_1) = \bm{P}(k_2, m_2)$, the belief will stay the same after the Bayesian update.
    Calculating the expected number of likes for the myopic policy using the second expression in Lemma~\ref{lemma:closed-form-representations-of-the-value-function} yields:
    \[
        V^{\pi^m} = 0.5 \cdot \frac{0.8}{1-0.8} + 0.5 \cdot \frac{0.8}{1-0.8} = 4.
    \]
    Now, consider the policy $\pi^{k_1}$ that chooses $k_1$ indefinitely.
    Calculating the expected number of likes for this policy, we get
    \[
        V^{\pi^{k_1}} = 0.5 \cdot \frac{\frac{8d}{1+8d}}{1-\frac{8d}{1+8d}} + 0.5 \cdot 0 = 0.5 \cdot \frac{8d}{1+8d-8d} + 0.5 \cdot 0 = 4d.
    \]
    Because $\pi^\star$ is optimal we get that $V^\star \geq V^{\pi^{k_1}} = 4d = d \cdot V^{\pi^m}$, and that concludes the proof.
\end{proof}