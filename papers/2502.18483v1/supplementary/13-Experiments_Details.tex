\section{Auxiliary Details about the Experiments}
\label{sec:auxiliary-details-about-the-experiments}

In this appendix, we provide technical details about the simulations in Section~\ref{sec:experiments}.

\subsection{Instance Generation}

Problem instances for the simulations were generated using a random sampling procedure designed to reflect realistic distributions of user preferences and categories. The procedure involves generating two key components: the matrix $\bm{P}$, representing the probabilities that each user type prefers each category, and the distribution $\bm{q}$, representing the prior belief over user types.

The matrix $\bm{P}$ is constructed by first independently sampling latent vectors for each user type and category from the standard normal distribution. These latent vectors are then normalized, and the cosine similarity between the vectors is computed, representing the affinity between each user type and category. This similarity is scaled to fall within the range $[0, 1]$, producing a probability matrix $\bm{P}$. To ensure numerical stability and avoid extreme values, the probabilities are clipped at $[0.01, 0.99]$.

The prior distribution $\bm{q}$ over user types is generated by independently sampling logits for each user type from a normal distribution $\mathcal{N}(0, 0.5)$. These logits are then transformed into probabilities using the softmax function, producing a categorical distribution over the user types. Similar to the probability matrix, the resulting distribution is clipped and normalized to maintain numerical stability and avoid extreme values.

Below are the Python functions used to implement this sampling procedure:

\begin{verbatim}
def get_random_P(n_actions, n_types, threshold=1e-2):
    """
    Generates the probability matrix P representing the likelihood 
    that each user type prefers each category.
    Latent vectors are sampled from a normal distribution, and 
    cosine similarities are computed and scaled to probabilities.
    """
    dim = n_actions
    latent_action_vectors = np.random.normal(0, 1, (n_actions, dim))
    latent_type_vectors = np.random.normal(0, 1, (n_types, dim))
    norm_a = np.linalg.norm(latent_action_vectors, ord=2, axis=1).reshape(-1, 1)
    norm_t = np.linalg.norm(latent_type_vectors, ord=2, axis=1).reshape(1, -1)
    norm = norm_a @ norm_t
    P = (latent_action_vectors @ latent_type_vectors.T) / norm
    P = (P + 1) / 2

    P = np.clip(P, threshold, 1-threshold)
    
    return P

def get_random_q(n_types, std=.5, threshold=1e-2):
    """
    Generates the prior distribution q over user types.
    Logits are sampled from a normal distribution and transformed 
    into probabilities using the softmax function.
    """
    log_q = np.random.normal(0, std, (n_types,))
    q = np.exp(log_q)
    q /= q.sum()
    q = np.clip(q, threshold/n_types, 1-threshold)
    q /= q.sum()
    return q
\end{verbatim}

\subsection{Statistical tests}

We verify the statistical significance of the difference between Algorithm~\ref{bb-algorithm} and the baseline algorithm reported in Figure \ref{fig:sarsop} using the Wilcoxon signed-rank test. For all data points (pairs of number of categories, number of user types), the two-sided test returns very small p-values -- less than $10^{-10}$ -- which is expected for sample sizes as large as 500.

\subsection{Hardware and Performance}

All experiments were conducted on a desktop PC equipped with 16 GB RAM and an 11th Gen Intel(R) Core i5-11600KF @3.90GHz processor. The simulations were run entirely on the CPU, and no GPU was utilized. It takes about two hours to run all experiments.
