% TN for GreenAI\cite{memmel2024position}
\section{Algorithms for CARAML}

In Section~\ref{sec:caraml}, we presented the CARAML framework with a set of recommendations that start from the individual and evolve into global action. As ML practitioners, there are some algorithmic research we can pursue to push the agenda of CARAML forward.


\subsection{Frugal innovation theory for ML} Frugal innovation theory, often associated with emerging markets and resource-constrained environments, emphasizes the development of products, processes, or solutions that are cost-effective, resource-efficient, and simple yet effective~\citep{prabhu2017frugal}. During the COVID pandemic, there was an urgent need for innovation with limited resources to alleviate risks in developing countries, and frugal innovation theory was useful in this setting, as shown in ~\cite{harris2020fast}. Although not as urgent as a global pandemic, there is also a need for frugal innovation within ML. AI model development currently is rife with redundancies that are hogging up resources that could be put to better use. 

Frugal innovation theory, when applied to AI methods, is a concept that centers on optimizing and simplifying DL models and practices to make them more resource-efficient, cost-effective, and adaptable to various environments. This is currently expressed through methods such as model parameter compression~\citep{cheng2018model}, quantization~\citep{hubara2016binarized,nagel2021white}, employing knowledge distillation~\citep{sanh2019distilbert}, promoting sparsity in model weights~\citep{louizos2018learning}, and applying efficient training strategies~\citep{dettmers20218}. These approaches aim to make DL more resource-efficient and cost-effective at {\em individual steps} of its life-cycle while maintaining performance.

These formulations on frugal techniques are based on existing, disconnected solutions that act on individual steps of the DL model life-cycle. An all-encompassing frugal innovation theory for DL needs to emerge to serve as a guiding framework for researchers and practitioners. This can help them make informed decisions about model design, resource allocation, and obtaining meaningful performance trade-offs.

\subsection{Scaling laws and Sustainable AI} 

The current class of upscaled deep learning~\citep{sevilla2022compute,kaplan2020scaling} has garnered immense attention and resources in recent years, often overshadowing theoretical rigor and in-depth understanding. We argue that the rush towards larger and more complex models has led to a neglect of fundamental theoretical underpinnings of these models, and limit propagation of novel ideas from related domains such as theoretical computer science, neuroscience~\citep{friston2010free} or physics~\citep{memmel2024position,brehmer2024does}. The theoretical foundations of DL can become obscured by the enthusiasm for scale, making it difficult to fully explain why certain architectures or techniques work. 

The notion of ``bigger is better" has its limitations; as models scale up, the incremental improvements in performance tend to diminish. The extraordinary computational costs associated with large models may not always justify the modest gains achieved. This diminishing return on investment is a point of concern from a theoretical perspective, challenging the wisdom of extreme scaling. Investigating stronger theoretical foundations of DL can not only benefit in reducing their resource consumption, but can also have implications on improving their interpretability, better generalization, and mitigation of biases~\citep{roberts2022principles}.  

Bridging the gap between learning theory by developing frameworks to include large-scale DL models that encompass current AI methods will be critical in the coming years with implications on improving the overall sustainability of AI. 

% Scale and equivariance~\cite{brehmer2024does}

% The obsession with scaling up is at odds with Sustainable AI. 

% Trickle-down AI does disservice to marginalized communities.


\subsection{Pareto sustainable AI} ``Pareto sustainable AI" extends the concept of sustainability to include a trade-off between three critical dimensions: environmental sustainability, social sustainability, and performance. Pareto sustainable AI is based on multi-objective optimization~\cite{miettinen1999nonlinear} and treats performance, environmental sustainability and social sustainability as objectives to be jointly optimized, illustrated as the three axes in Figure~\ref{fig:pareto}. 


\begin{figure}[t]
    \centering
    \includegraphics[width=0.7\textwidth]{img/pareto.pdf}
    \caption{Pareto Sustainable AI seeks to perform multi-objective optimization between performance measures, climate awareness, and resource awareness.}
    \label{fig:pareto}
\end{figure}

This involves optimizing energy consumption, minimizing carbon emissions, and using eco-friendly hardware and infrastructure. Efforts are made to reduce the environmental footprint, considering the entire life-cycle of AI systems, from data center operations to end-user devices. Pareto sustainable AI ensures that AI technologies contribute positively to society. This includes addressing issues like bias in AI algorithms, ensuring accessibility for all users, and respecting privacy and human rights. Ethical AI development and deployment practices are prioritized to create a socially sustainable AI ecosystem. All the same, the performance dimension remains crucial in Pareto sustainable AI. AI systems must continue to deliver high-quality results and meet the intended objectives. However, this is done in a balanced way that does not excessively compromise environmental and social sustainability. The focus is on optimizing the key performance aspects that matter most, as identified by the Pareto principle.
In this framework, achieving Pareto sustainable AI involves making informed trade-offs among these three dimensions:
\\
% \begin{itemize}
    % \item 
{\bf Performance vs.{} Environmental Sustainability}: The trade-off here implies optimizing AI systems for performance while minimizing their environmental impact. This might involve using energy-efficient hardware, data center cooling solutions, and energy-conscious algorithms to maintain high performance while reducing carbon emissions~\citep{tan2019efficientnet,selvan2021carbon,bakhtiarifard2022energy}.
% \item 
\\
{\bf Performance vs.{} Social Sustainability}: This trade-off emphasizes delivering high-performance AI solutions while ensuring ethical and socially responsible use. Ethical considerations may lead to avoiding certain applications or use cases, even if they promise superior performance, to align with societal values and principles~\citep{bender2021dangers}.
% \item 
\\
{\bf Environmental Sustainability vs.{} Social Sustainability}: Balancing environmental and social sustainability may involve choices that optimize both while not necessarily maximizing either. For instance, ensuring fair and unbiased AI systems may require additional computational resources for fairness testing and bias mitigation, which can impact environmental sustainability to some extent~\citep{bu2023differentially}.
% \end{itemize}


Pareto sustainable AI recognizes that trade-offs are inevitable, but seeks to find the right balance. By posing this as a multi-objective optimization, trade-offs between these axes can be meaningfully explored. 
% It involves prioritizing the optimization of the most impactful aspects for high performance, while keeping environmental and social sustainability at the forefront of AI development and deployment decisions. 
This concept of Pareto sustainable AI reflects a holistic approach to AI that considers not only technical excellence, but also its broader societal and environmental implications.

% \subsection{Resource Pyramid}

% \begin{figure}[h]%{r}{0.5\textwidth}
% % %\vspace{-0.8cm}
%     \centering
%     \includegraphics[width=0.49\textwidth]{img/respyr.pdf}
%     \vspace{-0.55cm}
%     \caption{The inverted resource pyramid view that abstracts the cost of training an AI model. In this case, the training costs of Llama-3.1-405B model are shown~\cite{dubey2024llama}.\footnote{Training costs as reported in the model card for Llama-3.1-405B. Link: \url{https://huggingface.co/meta-llama/Llama-3.1-405B}} Complexity reported in terms of number of trainable parameters or training data are at the bottom of this inverted pyramid. The inverted pyramid is aimed to depict the approaching convergence to the environmental costs, reported on the top as the carbon footprint due to the energy consumption incurred due to the training computations of a complex model. Reporting mandates that follow this hierarchy can be useful to assess the environmental impact of AI models, to some degree.}
%     \label{fig:respyr}.
%     % \vspace{-1.cm}
% \end{figure}
