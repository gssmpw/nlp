\newpage
\appendix


\lstset
{
    language=lean,
    backgroundcolor=\color{black!5!white},
    breaklines=true,
    basicstyle=\tt\small,
    keywordstyle=\color{blue},
    identifierstyle=\color{magenta},
    frame = single
}

\section{Symbolic methods vs. Neural methods} \label{app:symbolic_vs_neural}

\begin{figure}[ht] 
\centering
\subfigure[An example of CAD]{
\includegraphics[width=0.31\linewidth]{figs/cad.pdf}
\label{subfig:cad}
}
\hfill
\subfigure[Inequality proving process]{
\includegraphics[width=0.32\linewidth]{figs/proof.pdf}
\label{subfig:proof}
}
\hfill
\subfigure[An example of proof]{
\includegraphics[width=0.32\linewidth]{figs/proof-exp.pdf}
\label{subfig:proof-exp}
}
\vspace{-0.75em}
\caption{Figure (a) demonstrates how CAD is performed on two intersecting unit circles, deriving multiple sign-invariant cells (i.e., colored region). Figure (b) illustrates the process of inequality proving, which constructs a chain of proof goals by iteratively applying tactics; Figure (c) provides a corresponding instantiation of proving $ab+bc+ca \leq a ^ 2 + b ^ 2 + c ^ 2$. }
\label{fig:sym_vs_neu}
\vspace{-1.0em}
\end{figure}

Figure~\ref{subfig:cad} illustrates how symbolic methods work in proving inequalities. 
Symbolic methods are based on the CAD algorithm, which divides the underlying space $\mathbb{R}^n$ into multiple connected semi-algebraic sets. 
In each cell, the sign of every polynomial remains constant (positive, negative, or zero).
Therefore, we just need to sample one point to determine the satisfiability of each cell, rather than scanning the whole $\mathbb{R}^n$ space. 
One can refer to \citet{caviness2012quantifier, arnon1984cylindrical, jirstrand1995cylindrical} for more details of CAD algorithm.

Figure~\ref{subfig:proof} and~\ref{subfig:proof-exp} illustrate a commonly used paradigm in inequality proving. 
In a nutshell, a theorem prover often starts with the proof goal, and then iteratively transforms it into a simpler form until the final version can be easily confirmed. 
This approach has two main advantages. 
First, it ensures that the resulting proof can be more easily formalized in formal languages such as Lean. 
Second, since the hypotheses involved in inequality proofs are often straightforward, scaling and rewriting the proof goal is typically more efficient.


\section{Pseudo code of \name} \label{app:algorithm}

Algorithm~(\ref{alg:proof}) outlines the overall process of \name proof generation. The detailed steps for tactic generation and pruning are provided in Algorithm~\ref{alg:tactic}, while the specifics for goal filtering and ranking are described in Algorithm~\ref{alg:rank}.

\begin{algorithm}[h] 
\caption{Tactic generation and pruning of \name} 
\label{alg:tactic}
\begin{algorithmic}[1]
\Require A proof goal $g$; A lemma library of scaling tactics $\Phi$ and a prompt set of rewriting tactics $\Psi$; A language model $M$.
\Ensure Tactic set $T$.
\State Initialize the tactic set $T = \{\}$
\For{$t$ in $\Phi$}{\Comment{\emph{Scaling tactic generation}}}
    \State Obtain arguments of the tactic $t$ using {pattern\_match} on the goal $\Phi$.
    \State Check the tactic $t$ (with derived arguments) via the symbolic solvers.
    \If{no counterexamples exist} {\Comment{\emph{Tactic pruning \& Solver update}}}
    \State Add the tactic $t$ into the tactic set $T$.
    \Else 
    \State Update the symbolic solvers by including newly detected counterexamples.
    \EndIf
\EndFor
\For{$t$ in $\Psi$}{\Comment{\emph{Rewriting tactic generation \& pruning}}}
    \State Obtain arguments of the tactic $t$ by prompting the LLM.
    \State Add the tactic $t$ (with derived arguments) into the tactic set $T$.
\EndFor
\end{algorithmic}
\end{algorithm}

\begin{algorithm}[h] 
\caption{Goal filtering and ranking of \name} 
\label{alg:rank}
\begin{algorithmic}[1]
\Require A goal candidate set $\Omega$.
\Ensure A ranked set $\Omega'$.
\State Initialize a new goal set $\Omega' = \{\}$.
\For{$g$ in $\Omega$}{\Comment{\emph{Symbolic goal filtering}}}
    \State Compute the homogeneity $\alpha$ and decoupling $\beta$ for $g$.
    \State Define the average score of the goal $g$ by $(\alpha + \beta)/2$.
\EndFor
\State Prompt the LLM to rank the first ten goals in $\Omega'$ {\Comment{\emph{Neural goal ranking}}}
    \State Obtain arguments of the tactic $t$ by prompting the LLM.
    \State Re-rank the goal set $\Omega'$ according to LLM's responses.
\end{algorithmic}
\end{algorithm}
\begin{algorithm}[h]
\caption{Overall proof generation process of \name} 
\label{alg:proof}
\begin{algorithmic}[1]
\Require A formal inequality problem $g_0$; A lemma library of scaling tactics $\Phi$ and a prompt set of rewriting tactics $\Psi$; A language model $M$.
\Ensure A formal proof or timeout. 
\State Initialize the candidate goal set $\Omega = \{g_0\}$.
\For{$i=1,\dots,$}
    \State Select the first goal $g$ in $S$ for exploration.
    \State Obtain the tactic set $T$ by applying Algorithm~(\ref{alg:tactic}) on the current goal $g$.
    \For{$t$ in $T$} {\Comment{\emph{Tactic application}}}
    \State Apply the tactic to the current $g$ in Lean, deriving a new goal $g'$.
    \State Add the new goal $g'$ into the the candidate goal set $\Omega$.
    \EndFor
    \State Update the goal set $\Omega$ by applying Algorithm~(\ref{alg:rank}).
\EndFor
\end{algorithmic}
\end{algorithm}


\section{Additional details for experiments} \label{app:exp_details}

The experiments were conducted on four Linux servers equipped with 4 Intel(R) Xeon(R) Platinum 8280L CPU @ 2.80GHz. 
Each server ran Ubuntu 22.04 with GNU/Linux kernel 6.5.0-1015-azure. 
Each proving task was performed within a docker sandbox, utilizing 192 assigned CPU cores.

\textbf{Neural provers. } 
For DSP, we directly use the official code, and adapt it to Lean language and GPT-4o (version Azure-05-01). 
MCTS is implemented based on classic upper confidence bounds applied to trees algorithm~\citep{kocsis2006bandit}. 
The value function is defined as $f(\phi) = v_\phi + C \sqrt{\log(N_\phi)/n_\phi}$, where
$n_\phi$ is the number of the proof goal $\phi$ is selected and explored, $N_\phi$ represents the number of $\phi$'s parent represents selected and explored, and $C$ is a hyperparameter set to $C = \sqrt{2}$.
For the average reward $v_\phi$ of the proof goal $\phi$, we use the same heuristic function as~\citet{wei2024proving}, which calculates the maximum depth of the expression trees on both sides.

\textbf{Symbolic provers. } For CAD, we utilize a portfolio including a suite of solvers, i.e., Z3, CVC5, RC-CAD, and Bottema. It will claim the problem is successfully proved if any one of four tools outputs \texttt{unsat}, and vice vica. 
Among four tools, Z3 and CVC5 are two popular SMT solvers;
RC-CAD refers to the CylindricalAlgebraicDecompose function in Maple 2024 RegularChain package;\footnote{\scriptsize\url{https://www.maplesoft.com/support/help/maple/view.aspx?path=RegularChains}}
Bottema is a CAD-based inequality prover developed by~\citet{yang1999recent}.\footnote{\scriptsize\url{https://faculty.uestc.edu.cn/huangfangjian/en/article/167349/content/2378.htm}} 
As to MMA, we integrate two commands, i.e., \texttt{Reduce} and \texttt{FindInstance} in Wolfram-Mathematica (version 13.0.1), and apply the same peripheral logic with CAD.

\textbf{\name. } In our framework, the symbolic solver employed for pruning scaling tactics also consists of solvers Z3, CVC5, RC-CAD, and Bottema, complemented by a numerical optimizer grounded in SciPy~\citep{virtanen2020scipy}. 
The time limit of searching counterexamples is set to 5 seconds. 
Scaling tactics encompasses a comprehensive array of inequality lemma, including AM-GM, AM-HM, Cauchy-Schwarz, Power Mean, Chebyshev, Muirhead, Jensen, Titu, Schur, Holder inequalities, as well as a selection of valuable inequalities contributed by L\'{a}szi\'{o} Kozma.\footnote{\scriptsize\url{https://www.lkozma.net/inequalities_cheat_sheet/ineq.pdf}}
To facilitate their application in Lean 4, we have developed multiple variations of each inequality, accounting for different numbers of variables and directions.
Figure~\ref{fig:scaling_tactic} illustrates our scaling tactics using two-variable AM-GM inequality.

\begin{figure}[ht]
{\small
\begin{lstlisting}
theorem NEQ_AM_GM_left_2vars (u v k l right : ℝ) (hk : k ≥ 0) (h : k * (u ^ 2 + v ^ 2) / 2 + l ≤ right) : k * (u * v) + l ≤ right := by
  suffices (u - v) ^ 2 ≥ 0 by nlinarith
  positivity

theorem NEQ_AM_GM_right_2vars (u v k l left : ℝ) (hk : k ≥ 0) (h : left ≤ 2 * k * (u * v) + l) : left ≤ k * (u ^ 2 + v ^ 2) + l := by
  suffices (u - v) ^ 2 ≥ 0 by nlinarith
  positivity
\end{lstlisting}
}
\vspace{-0.75em}
\caption{Two examples of AM-GM inequality encoded as scaling tactics}
\label{fig:scaling_tactic}
\end{figure}

\begin{figure}[ht]
{\small
\begin{tcolorbox}[colframe=black!75!white, colback=gray!10!white, boxsep=2.5pt, top=5pt, bottom=5pt, left=5pt, right=5pt, title={Prompts of Rewriting Tactics Generation (Simplification)}]  
\#\#\# Task\\
Your task is to use the condition \{condition\}, rearrange and rewrite the expression given by the user into an absolutely different form.\\
\#\#\# Notice\\
1. Please reason step by step\\
2. Only four operators, add, sub, multiply, and division, can be used, and should NOT introduce new variables\\
3. Put the final results within \textbackslash\textbackslash boxed\{\{\}\}, e.g., \textbackslash\textbackslash boxed\{\{x + 1/y - z\}\}\\
\#\#\# Response\\
User: \\
\{problem\}\\
Assistant:
\end{tcolorbox}

\begin{tcolorbox}[colframe=black!75!white, colback=gray!10!white, boxsep=2.5pt, top=5pt, bottom=5pt, left=5pt, right=5pt, title={Prompts of Rewriting Tactics Generation (Others)}]  
\#\#\# Task\\
You should rewrite the inequality given by the user according to the rule \{rule\}\\
\#\#\# Notice\\
1. Please reason step by step\\
2. Follow the given example, and output the result for the given inequality\\
3. Put the final results within \textbackslash\textbackslash boxed\{\{\}\}, e.g., \textbackslash\textbackslash boxed\{\{x + 1/y - z\}\}\\
\#\#\# Example\\
User:\\
\{example\_problem\} \\
Assistant:\\
\{example\_answer\}\\
\#\#\# Response\\
User:\\
\{problem\}\\
Assistant:
\end{tcolorbox}
}
\vspace{-0.75em}
\caption{Prompts of simplification and other operations used for generating rewriting tactics}
\vspace{-1em}
\label{fig:rewriting_tactic}
\end{figure}

For rewriting tactics, we design 16 relevant operations, i.e., simplification w/o assumptions, simplification w/ assumptions, completing the square, variable substitution, expression expansion, expression rearrangement, expression multiplication, cancellation of denominators/numerators, cancellation of powers, extraction/cancellation of common factors, separation/reduction of fractions, sum-of-squares trick, and tangent line trick.
Except for the sum-of-squares trick and tangent line trick implemented based on SymPy, 
we use two prompt templates for these operations, shown in Figure~\ref{fig:rewriting_tactic}. 
For each operation, we repeatedly query the LLM three times to cover more rewriting tactics.
We use a low-temperature setting of GPT-4o in rewriting tactic generation and neural ranking (T=0.1, top\_p=1.0, max\_token=2048).

\section{Ablation Study} \label{app:ablation_study}

\begin{figure}[ht] 
\centering
\subfigure{
\includegraphics[width=0.48\linewidth]{figs/CAD_curve.pdf}
}
\hfill
\subfigure{
\includegraphics[width=0.48\linewidth]{figs/MMA_curve.pdf}
}
\vspace{-0.75em}
\caption{CPU time vs. solution count curves of CAD and MMA. 
The results show that symbolic solvers can verify the inequality in less than five minutes. For other problems that cannot be verified quickly, allocating more time does not improve the results.
In addition, it is important to note that symbolic solvers are unable to construct human-readable proofs for the problems. }
\label{fig:curves}
\vspace{-1.0em}
\end{figure}

{\bf Scaling tactics.} 
Figure~\ref{subfig:scale-1} has shown how the performance on the ChenNEQ dataset (Y-axis) changes as we increase the number of lemmas used by scaling tactics (X-axis). When X = 0 (without scaling tactics), only 3 out of 41 theorems can be proved, whereas LIPS can prove 39 theorems.

{\bf Rewriting tactics.}
We observed that the successfully proved 3 theorems without scaling tactics are achieved by two rewriting tactics, sum-of-squares and tangent line trick. 
Hence, we further investigate the ablation of these two tactics. 
We newly evaluated LIPS w/o sum-of-squares and tangent lines on ChenNEQ. Among the 39 out of 41 proved inequalities, none of them requires sum-of-squares, and 3 requires the tangent line trick.

{\bf Symbolic filtering.} Figure~\ref{subfig:scale-2} has shown how the performance on the ChenNEQ dataset (Y-axis) changes as we increase the size of the filtered set (X-axis). A smaller X means more aggressive symbolic filtering. When X is large, e.g., 32, the performance drops under 40\%, highlighting the importance of symbolic filtering. We did not test values of X beyond 32, as this consistently caused GPT-4o to hit the maximum token limit (set at 4096).

{\bf Neural ranking.} Figure~\ref{subfig:scale-3} presents an ablation analysis where neural ranking is replaced with random selection. The results on ChenNEQ demonstrate a significant drop in LIPS performance, from 95.1\% to 41.2\%, underscoring the necessity of neural ranking. Additionally, we incorporate a new ablation analysis that examines the effect of replacing neural ranking with purely symbolic filtering.
The success rate decreased to 43.9\%, indicating that symbolic filtering cannot determine an accurate proof path.

{\bf Symbolic solvers.} To explore the performance of symbolic solvers, we use the ChenNEQ dataset and plot the CPU time vs. solution count curves on Figure~\ref{fig:curves}. 
It can be observed that (1) symbolic solvers are very fast, they can verify the inequality in less than five minutes; (2) for other problems that cannot be verified quickly, allocating more time cannot improve the results.
It also means that we only need to set a small time limit for symbolic solvers in \name.

\begin{table}[ht]
\centering
\caption{LIPS proving success rate if different time budgets}
\label{tab:lips_curves}
\vspace{-0.75em}
\begin{threeparttable}
\begin{tabular}{cccccc}
\toprule
Method & $<$10min & $<$30min & $<$50min & $<$70min & $<$90min \\
\midrule
LIPS & 5\% & 15\% & 35\% & 60\% & 80\% \\
\bottomrule
\end{tabular}
\end{threeparttable}
\end{table}

As to the results of \name, we list the success rate vs. CPU time in Table~\ref{tab:lips_curves}.
Additionally, we plan to further enhance the efficiency of LIPS in the next version, for example, by optimizing the SymPy \texttt{pattern\_matching} function.


\section{Case Study}
\label{app:exp_results}

\lstset
{
    language=lean,
    breaklines=true,
    basicstyle=\tt\small,
    keywordstyle=\color{blue},
    identifierstyle=\color{magenta},
    frame = None,
}


\input{answers/dsp_answer}

\input{answers/o1_answer}

\input{answers/lips_answer}

