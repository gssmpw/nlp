\section{Introduction}
Automated theorem proving has been a long-standing goal in AI~\citep{newell1956logic}. Recent research explores leveraging large language models (LLMs) to generate formal proofs that can be verified in formal proof systems like Lean~\citep{de2015lean}, opening a new avenue to theorem proving~\citep{li2024survey,yang2024formal}. This promising approach has already led to tools that assist human mathematicians~\citep{song2024towards} and the first AI that achieves silver-medal performance in the International Mathematical Olympiad (IMO)~\citep{alphaproof}.

While LLM-based proof generation shows great promise across various mathematical domains, its performance is constrained by the scarcity of formal proof data. 
Furthermore, it remains an open problem whether LLMs can perform precise and complex symbolic manipulations~\citep{hammond2023large}.
To address these limitations, mechanical symbolic reasoning is still essential. 
Unlike LLMs, symbolic methods leverage domain-specific knowledge to achieve greater efficiency and generalization without relying on extensive training data~\citep{wu2008decision, heule2016solving}. Integrating LLMs with symbolic methods presents a promising strategy for tactic generation and theorem proving. This raises a key question: \emph{Which aspects of mathematical reasoning are best suited to LLMs, and which to symbolic methods?} By exploring this question, we aim to combine the strengths of both approaches effectively. Since symbolic methods are inherently domain-specific, we focus on a concrete domain: inequalities, which offers a balance between feasibility and practicality.

\begin{figure}[t]
\begin{center}
\includegraphics[width=1.0\linewidth]{figs/overall.pdf}
\end{center}
\vspace{-0.75em}
\caption{An overview of our neuro-symbolic inequality prover \name. By integrating both LLMs and symbolic methods in an iterative process of tactic generation and goal selection, it can generate human-readable and formally verifiable proofs in Lean for Olympiad-level inequality problems.
} 
\vspace{-1.2em}
\label{fig:overall}
\end{figure}

\textbf{Mathematical Inequalities.}
Inequalities arise from various branches of mathematics~\citep{hardy1952inequalities}. 
Their proofs often rely on a relatively small set of fundamental techniques, yet these techniques can be applied in remarkably intricate and nuanced ways. In this paper, we further constrain our scope to elementary algebraic inequalities (Figure~\ref{fig:example}), which were prevalent in high-school mathematics competitions~\citep{manfrino2010inequalities}, e.g., Problem 2 in IMO 2000. Additionally, these inequalities are closely related to quantifier-free real arithmetic in satisfiability modulo theories (SMT)~\citep{barrett2018satisfiability}, which have numerous practical applications in formal verification.

Even this restricted class of inequalities poses significant challenges that surpass the capabilities of current symbolic or neural provers. Symbolic methods~\citep{yang1999recent, uray2020proving} partition the variable space (e.g., $\mathbb{R}^3$ for three variables) into a finite number of cells, which are then exhaustively enumerated. These approaches suffer from combinatorial explosion and quickly become computationally infeasible for competition-level problems. Furthermore, due to their enumerative nature, these methods are black boxes that cannot produce human-readable proofs. On the other hand, neural approaches~\citep{wu2021int, wei2024proving} fine-tune language models to generate formal proofs by predicting tactics or evaluating subgoals during proof search. However, due to data scarcity and difficulties in generalization, they frequently underperform on certain types of problems.

\textbf{Our Approach.} We introduce \name (\underline{L}LM-based \underline{i}nequality \underline{p}rover with \underline{s}ymbolic reasoning), a neuro-symbolic framework that synergistically combines LLMs with domain-specific symbolic techniques, as illustrated in Figure~\ref{fig:overall}. Specifically, we analyze common proving strategies used by humans in inequality proofs and categorize them into two types of tactics: \textit{scaling} and \textit{rewriting}. Scaling tactics apply existing lemmas (e.g., the Cauchy-Schwarz inequality) to scale a subterm in the current goal. The set of lemmas is finite, and each lemma can be applied only in a limited number of ways. Therefore, we can enumerate all possible scaling tactics using symbolic tools. However, not all scaling tactics are useful for the current goal; over-scaling may render the goal invalid. We filter out such invalid scaling by using symbolic tools such as SMT solvers to check for counterexamples of the resulting goals. Rewriting tactics, on the other hand, transform a term into an equivalent form (e.g., subtracting $2ab$ from both sides of the current goal). Any term can be rewritten in infinite ways, making exhaustive enumeration impossible. To address this, we use LLMs to generate rewriting tactics by designing a series of prompts for different rewriting formats. By leveraging the mathematical intuition embedded in LLMs, we implicitly prune the infinite tactic space, sampling the most promising equivalent transformations.

Scaling and rewriting the current goal leads to a set of subgoals with new inequalities to prove. Efficient proof search requires prioritizing the most promising subgoals for further exploration. To this end, we employ two strategies: \emph{symbolic filtering} and \emph{neural ranking}. In symbolic filtering, we use heuristics based on inequalities' homogeneity and decoupling properties to filter out unpromising subgoals. In neural ranking, the remaining subgoals are fed into an LLM, which compares and ranks them using chain-of-thought prompting~\citep{wei2022chain}. After filtering and ranking, we end up with a small number of ranked subgoals. We then iteratively generate and apply new tactics, filter and rank the resulting subgoals, until the final goal becomes trivially provable, resulting in a proof that is both human-readable and formally verified by Lean.

We evaluate \name on 161 challenging inequalities collected from three problem sets~\citep{chen2014brief,Tung2012Nice, wei2024proving}. The experimental results show that \name consistently achieves state-of-the-art performance and significantly outperforms existing neural and symbolic methods in both effectiveness and speed.
Remarkably, out of 61 inequality problems sourced from various math Olympiad competitions, \name successfully proves 56 within 90 minutes,
whereas the previous best approach proves only 41 problems and fails to generate human-readable proofs.
