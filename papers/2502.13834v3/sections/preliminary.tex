\section{Inequality Theorem Proving}
\label{sec:preliminary}

In this section, we briefly review existing neural and symbolic methods for inequality proving. 
In a nutshell, 
symbolic methods complete proofs by exploring the problem space exhaustively, whereas
neural methods use LLMs to sample proof steps from a vast tactic space. Each approach comes with distinct advantages and challenges.

\vspace{-0.3em}
\subsection{Symbolic Methods}
\vspace{-0.3em}
Consider a set of polynomial inequalities $\Phi_j \bowtie 0, j=1,\dots,m$, where $\bowtie \, \in \{\leq, <, \geq, >, =, \neq\}$, 
and each polynomial is defined over $n$ variables as $\Phi_j(x_1, \dots, x_n) := \sum_{i=1}^{k} a_i \cdot x_1^{i_1} x_2^{i_2} \cdots x_n^{i_n}$, 
cylindrical algebraic decomposition (CAD)~\citep{arnon1984cylindrical, caviness2012quantifier, kremer2020cylindrical} divides the variable space $\mathbb{R}^n$ into multiple connected semi-algebraic sets, known as \emph{cells}.
Within each cell, the sign of every polynomial remains constant (positive, negative, or zero). 
Figure~\ref{subfig:cad} illustrates this process with an example of CAD-producing cells for two intersecting unit circles.
By exploiting this sign invariance, we can determine the satisfiability of inequalities by enumerating and checking all the cells, rather than searching the entire infinite space $\mathbb{R}^n$.

CAD and its variants have been extensively utilized in modern SMT solvers~\citep{jovanovic2013solving, kremer2022cooperating, uncu2023smt}. For inequality theorem proving, CAD transforms the problem into an enumeration task, exhaustively examining all cells to assess whether the inequality can be satisfied. While it is capable, CAD's performance remains unsatisfactory for several reasons. Firstly, CAD's heuristic strategies are primarily designed to efficiently find counterexamples rather than to optimize the proof search process. Secondly, CAD's proving mechanism fails to generate explicit and interpretable reasoning paths, hindering both automatic verification by existing interactive theorem provers and human interpretation of the proofs. Additionally, CAD suffers from double exponential computational complexity relative to the number of variables $n$~\citep{davenport1988real}, causing its efficiency to decrease significantly as the number of variables increases. Specifically, when dealing with nonlinear inequalities involving fractions or radical expressions, auxiliary variables are always introduced to eliminate these terms (e.g., converting $\sqrt{x_1} + \sqrt{x_2} + x_3^2 = 0$ into $\{x_4 + x_5 + x_3^2 = 0, x_4^2 = x_1, x_5^2 = x_2, x_4 > 0, x_5 > 0\}$). Although this transformation successfully rewrites the inequality into polynomial form, it drastically degrades the performance of CAD.


\vspace{-0.3em}
\subsection{Neural Methods}
\vspace{-0.3em}
In contrast to symbolic methods based on CAD, some approaches leverage neural networks to predict tactics within an interactive theorem prover, generating human-like, step-by-step formal proofs. Specifically, these inequality proofs are typically structured in a top-down sequential manner, where each tactic either transforms the current goal into a new subgoal or directly completes the proof (see Figure~\ref{subfig:proof}). Figure~\ref{subfig:proof-exp} provides an example proof of the inequality $ab+bc+ca \leq a^2+b^2+c^2$.

Among existing work, INT~\citep{wu2021int} designs a theorem generator for elementary-level inequalities by randomly sampling axioms from a fixed set. It trains a Transformer~\citep{vaswani2017attention} model to predict tactics and utilizes another value network with the Monte Carlo tree search to complete proofs. Similarly, AIPS~\citep{wei2024proving} implements a synthetic generator that can produce IMO-level inequalities and trains a language model to score each inequality expression in a curriculum manner, performing a best-first search to solve these problems. However, their inequality generators have some restrictions -- either limited in difficulty or constrained by specific forms like cyclic symmetry. Moreover, these approaches mainly rely on fine-tuning models on large-scale datasets, making them highly dependent on the quantity and diversity of the training data.
