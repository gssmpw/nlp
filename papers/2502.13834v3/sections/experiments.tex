\section{Experiments}
\label{sec:exp}
In this section, we conduct a series of experiments to address the following three research questions:

\textbf{RQ1: Efficacy} -- Can {\name} prove more problems compared to existing methods?

\textbf{RQ2: Efficiency} -- Can \name obtain proofs in less time compared to existing methods?

\textbf{RQ3: Scalability} -- Can \name be improved by using more scaling lemmas or more powerful LLMs?

\vspace{-0.3em}
\subsection{Experimental Setup}
\vspace{-0.3em}
\textbf{Datasets. } 
We evaluate {\name} on three datasets: ChenNEQ, MO-INT-20, and 567NEQ, respectively. 
ChenNEQ consists of 41 Olympiad-level inequalities collected by~\citet{chen2014brief};
MO-INT is a new competition-level inequality benchmark introduced in AIPS~\citep{wei2024proving}, featuring 20 problems sourced from IMO shortlists and various national mathematical Olympiads;
567NEQ consists of 567 hard inequalities created by~\citet{Tung2012Nice} 
and we randomly selected 100 problems from the original problem set as the testbed for our framework.
To formalize the problems in Lean 4, we directly translate the LaTeX source code into Lean using manually defined rules.

\textbf{Baselines. }
We compare {\name} with five baselines: \dsp~\citep{jiang2023draft}, \cad~\citep{kremer2020cylindrical}, \mma~\citep{Mathematica}, \mcts~\citep{wu2021int}, and \aips~\citep{wei2024proving}.
\dsp consists of two steps, natural language reasoning generation and proof autoformalization, and we instantiate the LLM used in each step GPT-4o.  
\mcts (Monte Carlo tree search) has been explored in previous studies~\citep{wu2021int} and serves as an alternative method for proof goal selection.
\aips is an inequality prover system based on SymPy, which has demonstrated the capability to prove competition-level inequalities.
\cad integrates a series of CAD-based inequality solvers including Z3~\citep{de2008z3}, CVC5~\citep{kremer2022cooperating}, RC-CAD~\citep{lemaire2005regularchains}, and Bottema~\citep{lu1998practical}. 
\mma, referring to Mathematica, incorporates the CAD algorithm with other reduction strategies, providing a powerful algebraic system for inequality verification~\citep{wolfram_2024_rps}.
Further implementation details for the baseline methods are provided in Appendix~\ref{app:exp_details}.

\textbf{Implementation. } 
The detailed processes of tactic generation, goal selection, as well as the overall framework are provided in Appendix~\ref{app:algorithm}.
To construct the tactics, 
we design a total of 96 scaling tactics and 16 rewriting tactics, each formalized in Lean 4.
The corresponding premises and LLM prompts are summarized in Appendix~\ref{app:exp_details}.
For the counterexample search in scaling tactic pruning, we integrate four CAD-based solvers (Z3, CVC5, RC-CAD, and Bottema) and implement an optimizer based on SciPy~\citep{virtanen2020scipy}.
For the LLM involved in transformation tactic generation and proof goal ranking, we use GPT-4o (version Azure-0501). In symbolic filtering, we fix the size of the filtered goal set to 10, as it is the largest size that ensures GPT-4o's efficacy. The code, together with the experimental data, is available at \url{https://github.com/Lizn-zn/NeqLIPS}.

\vspace{-0.3em}
\subsection{Experiments}
\vspace{-0.3em}

\input{tabs/efficacy}

\textbf{RQ1 : Efficacy. }  
We evaluate the proof success rates of \name and the five comparative methods across the three datasets. For each proving task, a time limit of 90 minutes is imposed, consistent with that of \aips and the standard problem-solving time constraint in the IMO. The overall results are presented in Table~\ref{tab:efficacy}.
First, we observe that the neural methods (\dsp and \mcts) cannot achieve satisfactory performance. 
An analysis of \dsp's results reveals that GPT-4o is unable to provide accurate natural language solutions or generate precise formal proofs in the Lean 4 language, resulting in a zero success rate.
Alternatively, \mcts struggles to effectively identify the correct reasoning path among numerous proof goals, causing many proving attempts to terminate due to timeouts.
We also evaluate the performance of the recent OpenAI o1-preview model on the MO-INT dataset. 
Through manual inspection of the generated natural language answers, we find that none of the problems are correctly solved.
Examples of these neural methods are provided in Appendix~\ref{app:exp_results}.

Symbolic provers outperform neural provers,
\cad and \mma achieve overall success rates of 59.0\% and 57.1\%, respectively.
However,
{\name} further surpasses symbolic provers by a significant margin of 14.0\% to 24.4\%. 
Notably, 
symbolic provers fail to produce any human-readable reasoning path.
In contrast,
the proofs generated by \name are not only accessible and human-readable, but also have been successfully verified by the Lean theorem prover.
For reference, we include two examples (one showcasing a successful proof and the other demonstrating a failed attempt) in Appendix~\ref{app:exp_results}.

\textbf{RQ2 : Efficiency. }
\begin{figure}[t] 
\centering
\subfigure{
\includegraphics[width=0.48\linewidth]{figs/tacratio.pdf}
}
\hfill
\subfigure{
\includegraphics[width=0.48\linewidth]{figs/numloops.pdf}
}
\vspace{-0.75em}
\caption{Tactic pruning ratio ($\uparrow$, higher is better) and number of iterations ($\downarrow$). The results illustrate that the tactic pruning method of \name is very stable, and outperforms the existing method by 7.92\% on average. Furthermore, the high efficiency of \name is derived from accurate goal selection, allowing a proof to be successfully constructed with a small number of goal selection iterations.}
\label{fig:efficiency}
\end{figure}
Since existing methods are built on different deduction engines, a direct comparison of their proving time could be unfair. 
Instead, we break down the efficiency evaluation into two aspects, i.e., the pruning ratio of scaling tactics and the number of iterations in goal selection.  
Given that \aips uses the equality check as the scaling tactic pruning strategy, we compare this approach with the CAD-based strategy employed in \name. 
Figure~\ref{fig:efficiency} provides the result of each problem in the MO-INT dataset. 
\name outperforms the existing method in 13 out of 20 problems, and achieves an average improvement of 7.92\%.

Furthermore, we count the number of goal selection iterations for each successfully proved problem, and present the results in Figure~\ref{fig:efficiency}. 
Due to the absence of a comparison method, we only include the oracle (i.e., optimal goal selection) as a reference.
We can observe that \name performs no more than 33 search loops to successfully obtain a proof, and for 12 out of 16 problems, it exceeds the oracle by fewer than 10 steps.
In addition, \name requires an average of 15.75 search loops, which is only 2.17 times that of the oracle (7.25),
demonstrating that \name's efficiency also stems from its high accuracy in generating proving paths.

\textbf{RQ3 : Scability. }
We conduct four experiments on the ChenNEQ dataset to explore the scalability of \name's symbolic and neural components. 
For the symbolic part,
we first examine how expanding the scaling tactics affects the performance of \name. 
To this end,
we randomly select 7 sets of scaling tactics with varying sizes and plot the performance curve in Figure~\ref{subfig:scale-1}. 
The results show that the proof success rate consistently increases as more scaling tactics are included, suggesting potential benefits in further enlarging our scaling tactic library.
The second experiment investigates the effect of different sizes of the filtered set. 
The corresponding performance curve is shown in Figure~\ref{subfig:scale-2}.
We observe that the success rate remains robust (over 85\%) with sizes from 8 to 16, but decreases significantly when the set is either too small or too large.

For the neural part, 
we explore the performance of \name with different LLMs serving in rewriting tactic generation and neural ranking. 
We select three alternative LLMs, i.e., Mathstral 7B~\citep{jiang2023mistral}, LLaMA-3 8B~\citep{llama3modelcard}, and DeepSeek-chat V2.5~\citep{deepseekv2}.
We also include a baseline method as an ablative study.
In rewriting tactic generation, the baseline uses SymPy \texttt{simplify} function instead of existing LLM-based rewriting tactics.
In neural ranking, we directly use random selection as the baseline. 
The proving success rates are provided in Figure~\ref{subfig:scale-3}.
The results demonstrate that all three alternative LLMs exhibit strong mathematical intuition, achieving performance comparable to GPT-4o in neural ranking. 
However, there exists a small decline in performance for rewriting tactic generation, which may be due to differences in instruction following and mathematical reasoning capabilities.

\input{tabs/ablation}

\textbf{Ablation study.} 
To showcase the strength of our neuro-symbolic paradigm, we present the results of removing neural or symbolic modules of \name in Appendix~\ref{app:ablation_study}. 
We also analyze the performance of symbolic solvers, offering guidance on optimal time limit settings in \name.

\textbf{Case study. }
Besides the running example~(\ref{fig:example}),
we provide two additional examples in Appendix~\ref{app:case_study} to illustrate that \name can discover new proofs, which are previously unavailable online.
Moreover, we also present two examples in Appendix~\ref{app:case_study} to demonstrate that users can verify human-written proofs by comparing them with the reasoning paths generated by \name.

