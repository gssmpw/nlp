\begin{figure}[ht]
\begin{tcolorbox}[enhanced, colback=black!5!white, colframe=black!75!white]
\textbf{Problem~\citep{chen2014brief}:} 
If $a, b, c$ are positive reals and $a ^ 2 + b ^ 2 + c ^ 2 = 1$, then
\begin{equation*}\label{eqn:example_e1}\tag{1}
\frac{1}{a^2+2}+\frac{1}{b^2+2}+\frac{1}{c^2+2} \leq \frac{1}{6 a b+c^2}+\frac{1}{6 b c+a^2}+\frac{1}{6 c a+b^2}.
\end{equation*}
\end{tcolorbox}
\vspace{-0.75em}
\caption{We prove inequality problems in math Olympiads that involve a finite number of real variables, hypotheses, and one conclusion. Both the hypotheses and the conclusion consist of constants, variables, algebraic operations (e.g., addition, multiplication), and transcendental functions like \textit{exp}.}
\label{fig:example}
\end{figure}

\begin{abstract}
Large language models (LLMs) can prove mathematical theorems formally by generating proof steps (\textit{a.k.a.} tactics) within a proof system. However, the space of possible tactics is vast and complex, while the available training data for formal proofs is limited, posing a significant challenge to LLM-based tactic generation. To address this, we introduce a neuro-symbolic tactic generator that synergizes the mathematical intuition learned by LLMs with domain-specific insights encoded by symbolic methods. The key aspect of this integration is identifying which parts of mathematical reasoning are best suited to LLMs and which to symbolic methods. While the high-level idea of neuro-symbolic integration is broadly applicable to various mathematical problems, in this paper, we focus specifically on Olympiad inequalities (Figure~\ref{fig:example}). We analyze how humans solve these problems and distill the techniques into two types of tactics: (1) scaling, handled by symbolic methods, and (2) rewriting, handled by LLMs. In addition, we combine symbolic tools with LLMs to prune and rank the proof goals for efficient proof search. We evaluate our framework on 161 challenging inequalities from multiple mathematics competitions, achieving state-of-the-art performance and significantly outperforming existing LLM and symbolic approaches without requiring additional training data.
\end{abstract}
