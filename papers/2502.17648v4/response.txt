\section{Related Works}
Extrinsic calibration methods for LiDAR–camera systems can generally be divided into two categories: target-based and targetless approaches. Target-based calibration relies on specially designed calibration targets and is thus commonly associated with manual, offline procedures. Moreover, these methods typically cannot handle real-time decalibrations, which are common in practical scenarios. In contrast, targetless methods extract features directly from natural scenes, making them well-suited for automatic, online calibration.
These methods can be broadly categorized into motion-based, edge alignment-based, mutual information-based, and deep learning–based approaches.

\subsubsection{Motion-based Calibration}
Motion-based calibration methods leverage sensor movements or relative poses derived from visual and LiDAR odometry to compute extrinsic parameters**Petek et al., "Camera-LiDAR Extrinsic Calibration Using Visual and LiDAR Odometry"**. For instance, **Petek et al., "Camera-LiDAR Extrinsic Calibration Using Visual and LiDAR Odometry"**, utilize odometry paths from each sensor, aligning them through non-linear optimization and dense 2D–3D matching. **Park et al., "Deep Learning for Camera-LiDAR Sensor Fusion"**, similarly derive closed-form calibration from relative sensor transformations. Despite effectiveness under certain conditions, these methods heavily rely on accurate odometry or SLAM estimations, which are susceptible to noise and degenerate in scenarios with limited sensor motion (e.g., minimal rotation). **Hand-eye calibration–based methods** further exacerbate this issue, requiring multiple precise sensor poses, thus limiting their practical applicability, particularly for static installations.

\subsubsection{Edge Alignment-based Calibration}
Edge-based approaches attempt to align edges detected from LiDAR point clouds and camera images**Zhang et al., "A Cylindrical Projection-Based 2D-2D Alignment Method for Camera-LiDAR Extrinsic Calibration"**. For example, **Zhang et al., "A Cylindrical Projection-Based 2D-2D Alignment Method for Camera-LiDAR Extrinsic Calibration"**, transform the calibration problem into a cylindrical projection-based 2D–2D alignment task, while **Li et al., "Deep Learning-Based Edge Extraction and Feature Matching for Camera-LiDAR Calibration"**, employ advanced edge extraction techniques such as the Segment Anything Model (SAM) combined with multi-frame filtering. However, reliably matching edges between sensors is inherently challenging due to modality differences—LiDAR captures sparse geometric structures, whereas camera edges reflect dense texture and lighting variations. These differences often lead to inaccurate feature correspondence and suboptimal calibration outcomes.

\subsubsection{Mutual Information-based Calibration}
Mutual information (MI)-based calibration methods utilize statistical relationships between LiDAR reflectance and camera image intensities**Pandey et al., "Calibrating Camera-LiDAR Systems Using Mutual Information"**. **Pandey et al., "Calibrating Camera-LiDAR Systems Using Mutual Information"**, for instance, maximize mutual information between LiDAR reflectance intensities and camera images to find optimal alignment. Despite their conceptual elegance, these methods struggle due to significant variability in LiDAR reflectance caused by surface material differences and camera pixel intensity fluctuations arising from lighting changes, leading to inconsistent and less reliable calibration results.
\begin{figure}[]
	\centering
	\includegraphics[width=0.499\textwidth]{figs/Atten_model.jpg}
	\caption{Schematic Overview of the Attention-based Refinement Process.}
	\label{Atten_model}
\end{figure}
\subsubsection{Deep Learning-based Calibration}
Deep learning-based methods have introduced neural networks to regress extrinsic calibration parameters directly**RegNet**, **LCCNet**, and **Xiao et al., "Transformer-Based Camera-LiDAR Extrinsic Calibration"**. Similarly, **Xiao et al., "Transformer-Based Camera-LiDAR Extrinsic Calibration"**, utilize transformer-based architectures to refine feature correspondences. **Zhu et al., "CalibDepth: Unifying Monocular Depth Estimation and Sequence Modeling for Online Camera-LiDAR Calibration"**, propose CalibDepth, which uses depth maps as a unified representation across modalities, integrating monocular depth estimation and sequence modeling to improve online calibration performance. However, these methods depend heavily on an initial calibration to project LiDAR point clouds into the image plane, forming a "projected LiDAR depth map" that aligns sparse LiDAR data with dense image pixels. While this enables cross-modal feature correlation, it significantly limits generalizability—since the initial calibration is often manually provided or empirically estimated. Moreover, due to the sparsity of LiDAR data, the resulting depth maps are dominated by image features, effectively sidelining useful LiDAR-specific information and heavily biases calibration toward image modality. In addition, direct regression tasks pose significant computational challenges due to their unconstrained nature, further limiting real-time applicability. Alternative semantic segmentation-based methods** also face computational inefficiencies without substantial accuracy benefits over simpler object detection-based methods.



In summary, existing automatic, targetless, and online calibration methods commonly exhibit limitations including reliance on accurate odometry or sensor movement, difficulty in cross-modal edge matching, sensitivity to reflectance and illumination conditions, and computational inefficiencies. Furthermore,  most existing methods fail to fully exploit the advances in object detection and feature extraction developed for LiDAR and camera data processing.
To overcome these limitations, we propose \emph{CalibRefine}, a fully automatic, targetless, and online calibration framework directly processing raw LiDAR point clouds and camera images, eliminating initial calibrations or elaborate preprocessing. Our approach integrates proven object detection algorithms and introduces a novel Common Feature Discriminator for robust cross-sensor correspondence matching. Furthermore, we employ a coarse-to-fine strategy combining iterative optimization and attention-driven refinement, enabling accurate and robust real-time calibration. By directly matching corresponding points across modalities, our approach facilitates a straightforward, one-shot, and end-to-end calibration process between the LiDAR and camera, significantly enhancing adaptability to real-world scenarios.
% suitable for complex and dynamic real-world environments


\begin{figure*}[]
	\centering
	\includegraphics[width=\textwidth]{figs/BlockSampling_1.png}
	\caption{Block-based Sampling Strategy: 1) Project LiDAR points onto the image, identifying LiDAR-camera point pairs (red: camera, green: LiDAR); 2) Divide the image into equal-sized grids, marking centers; 3) Retain pairs whose camera point is nearest to the grid center; 4) Sample pairs at intervals of one block, discarding those in skipped blocks.}
	\label{BlockSampling}
\end{figure*}