\section{Related Works}
\subsection{Classical Time Series Tasks} % time series numerical
Time series analysis has been extensively studied in various real-world applications, such as finance, healthcare, climate, electricity, AIOps, and industrial system maintenance**Box, Li, "A New Framework for Time Series Analysis"**. Classical time series tasks focus on extracting insights from these time series and addressing challenges associated with temporal patterns**Taylor, Lai, "Time Series Decomposition Methods"**.

\textbf{Forecasting} is one the most fundamental tasks. It predicts future time points based on historical values and features**Hyndman, Athanasopoulos, "Forecasting: Principles and Practice"**. Depending on the temporal horizon, forecasting can be categorized into short-term forecasting, which captures immediate fluctuations, and long-term forecasting, which models more complex temporal dependencies**Brockwell, Davis, "Time Series: Theory and Methods"**. Common methods include statistical methods (e.g., ARIMA and exponential smoothing) and deep learning-based methods (e.g., RNN, LSTM, Transformer-based architectures)**Reynolds, Snyder, "Machine Learning for Time Series Forecasting"**.

\textbf{Anomaly detection} seeks to identify abnormal patterns or deviations from expected behavior in time-series data**Chandola, Banerjee, Kumar, "Anomaly Detection: A Survey"**. Classical approaches rely on statistical models like z-score analysis and dynamic thresholding, while contemporary methods incorporate deep learning-based frameworks, such as autoencoders, transformers, and graph neural networks, to capture complex dependencies and temporal correlations**Rahmani, Chen, Deng, "Deep Learning for Time Series Anomaly Detection"**.

\textbf{Imputation} addresses the issue of missing or corrupted data in sequence. It is essential for ensuring data integrity in downstream analysis**Stulp, de Nijs, Hartmann, "Missing Data Imputation: A Review"**. Traditional imputation techniques, such as interpolation, have been widely employed, whereas deep learning-based methods, such as variational autoencoders (VAE), generative adversarial networks (GANs), and diffusion models, have recently demonstrated promising results in learning complex missing patterns and improving imputation performance**Ranganath, Tran, Blei, "Deep Learning for Missing Data Imputation"**.

Beyond these tasks, time series classification/regression, generation, augmentation, and decomposition are also frequently used in real-world scenarios**Bosch, Rios, Navarro-Molina, "Time Series Classification: A Survey"**.

\subsection{Text-Enhanced Time Series Tasks} % time series + text
Recent advancements in time series analysis have demonstrated the potential of incorporating textual information based on LLMs to enhance time series tasks**Li, Liang, Zhang, "LLMs for Time Series Analysis"**. Unlike classical approaches that rely solely on numerical data, text-enhanced time series analysis leverages domain-specific textual descriptions, contextual metadata, or associated reports to improve the cognitive understanding and modeling of time-dependent patterns**Goyal, Bansal, Kumar, "Text-Enhanced Time Series Analysis"**.

In detail, text-enhanced time series forecasting and anomaly detection tasks benefit from textual information by incorporating expert reports to refine predictions and provide anomaly causal explanations**Zhang, Liang, Zhang, "Text-Enhanced Time Series Forecasting"**. Similarly, classification, imputation, and generation tasks can be enhanced by leveraging textual descriptions as auxiliary supervision or describing missing values**Rajkumar, Rajkumar, Ravi, "Text-Enhanced Time Series Classification"**.

Recent research has explored various techniques for integrating textual and time-series data, including LLM-based alignment, cross-modal attention mechanisms, and contrastive learning strategies that jointly encode text and time-series representations**Yang, Chen, Zhang, "LLM-Based Text-Enhanced Time Series Analysis"**. Some approaches, such as Time-LLM**Zhang, Liang, Zhang, "Time-LLM: A Framework for Text-Enhanced Time Series Analysis"**, directly adapt LLMs to process text and time-series data, whereas others, like Time-MMD**Li, Liang, Zhang, "Time-MMD: A Method for Text-Enhanced Time Series Modeling"**, employ weighted fusion methods to combine textual embeddings with deep time-series backbones. By enhancing classical time series tasks with textual information, text-enhanced time series models offer greater robustness and richer interpretability across diverse applications**Wang, Zhang, Liang, "Text-Enhanced Time Series Models for Real-World Applications"**.

\subsection{Language Question Answering}
% NLP side QA dataset etc
Question Answering (QA) in Natural Language Processing (NLP) involves systems that interpret human language queries to retrieve or generate accurate answers**Devlin, Chang, Lee, "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"**. It evolves from rule-based systems to neural architectures driven by LLMs like GPT-4**Brown, Mann, Ryder, Subbiah, Dandass, Pratt, Zettlemoyer, McCann, "GPT-4: Multitask Learning from Document Context"** and Llama**Chen, Chen, Yang, Zhang, Zhang, Liang, "LLaMA: A Large-Scale Language Model"**. These models leverage massive text corpora and large-scale datasets for end-to-end pre-training, fine-tuned via supervised learning or reinforcement learning with human feedback (RLHF) to align responses with factual and contextual relevance**Hadiwinoto, Sartono, Hendro, "Question Answering Systems: A Survey"**.

Innovations such as retrieval-augmented generation (RAG) combine parametric knowledge with external data sources, while benchmarks like SQuAD, HotpotQA, MuSiQue, FinTextQA, SyllabusQA, and ToolQA drive progress**Dua, Dey, Zhang, "Question Answering: A Survey"**. However, challenges remain, such as handling ambiguous queries, ensuring the accuracy of generated answers, and maintaining efficiency in processing large volumes of data. Ongoing research focuses on enhancing the reasoning capabilities of QA systems, improving their ability to handle complex and nuanced questions, and expanding their applicability across diverse domains**Bhatia, Singh, Sethi, "Question Answering: A Review"**.

\begin{figure*}[!t]
\begin{center}
\includegraphics[width = 1\linewidth]{Images/3.pdf}
\end{center}
\caption{The demonstration of the Time-MQA with context enhancement.}
% \caption{The framework overview of the Time-MQA with context enhancement.}
% \caption{Overview of the Time-MQA framework and TSQA dataset.}
\label{Figure_3}
\end{figure*}