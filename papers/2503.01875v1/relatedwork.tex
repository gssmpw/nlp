\section{Related Works}
\subsection{Classical Time Series Tasks} % time series numerical
Time series analysis has been extensively studied in various real-world applications, such as finance, healthcare, climate, electricity, AIOps, and industrial system maintenance~\cite{nie2024survey,guo2024maximizing,ma2024fusionsf,yang2023sgdp,yang2021early}. Classical time series tasks focus on extracting insights from these time series and addressing challenges associated with temporal patterns~\cite{fuller2009introduction,hamilton2020time}. 

\textbf{Forecasting} is one the most fundamental tasks. It predicts future time points based on historical values and features~\cite{lim2021time}. Depending on the temporal horizon, forecasting can be categorized into short-term forecasting, which captures immediate fluctuations, and long-term forecasting, which models more complex temporal dependencies~\cite{wang2024timexer}. Common methods include statistical methods (e.g., ARIMA and exponential smoothing) and deep learning-based methods (e.g., RNN, LSTM, Transformer-based architectures)~\cite{wen2022transformers,miller2024survey}. 

\textbf{Anomaly detection} seeks to identify abnormal patterns or deviations from expected behavior in time-series data~\cite{zamanzadeh2024deep}. Classical approaches rely on statistical models like z-score analysis and dynamic thresholding, while contemporary methods incorporate deep learning-based frameworks, such as autoencoders, transformers, and graph neural networks, to capture complex dependencies and temporal correlations~\cite{han2022adbench,yang2023dcdetector}. 

\textbf{Imputation} addresses the issue of missing or corrupted data in sequence. It is essential for ensuring data integrity in downstream analysis~\cite{du2024tsi}. Traditional imputation techniques, such as interpolation, have been widely employed, whereas deep learning-based methods, such as variational autoencoders (VAE), generative adversarial networks (GANs), and diffusion models, have recently demonstrated promising results in learning complex missing patterns and improving imputation performance~\cite{wang2025deeplearningmultivariatetime}. 

Beyond these tasks, time series classification/regression, generation, augmentation, and decomposition are also frequently used in real-world scenarios~\cite{mohammadi2024deep,wen2020time,zhang2024self}.

\subsection{Text-Enhanced Time Series Tasks} % time series + text
Recent advancements in time series analysis have demonstrated the potential of incorporating textual information based on LLMs to enhance time series tasks~\cite{jin2024position,wang2024chattimeunifiedmultimodaltime}. Unlike classical approaches that rely solely on numerical data, text-enhanced time series analysis leverages domain-specific textual descriptions, contextual metadata, or associated reports to improve the cognitive understanding and modeling of time-dependent patterns~\cite{liu-etal-2024-lstprompt}. This hybrid approach mitigates the limitations of unimodal time series models by integrating additional semantic and contextual cues that are useful for decision-making~\cite{kong2025position,singh2024finqapt}. 

In detail, text-enhanced time series forecasting and anomaly detection tasks benefit from textual information by incorporating expert reports to refine predictions and provide anomaly causal explanations~\cite{hollmann2025accurate,chen2023tele}. Similarly, classification, imputation, and generation tasks can be enhanced by leveraging textual descriptions as auxiliary supervision or describing missing values~\cite{bernardini2023novel,moor2023foundation}. It will help models distinguish subtle variations across different categories and generate more informed reconstructions.

Recent research has explored various techniques for integrating textual and time-series data, including LLM-based alignment, cross-modal attention mechanisms, and contrastive learning strategies that jointly encode text and time-series representations~\cite{jin2023large,zhang2023insight,liu2024picture}. Some approaches, such as Time-LLM~\cite{jin2023time}, directly adapt LLMs to process text and time-series data, whereas others, like Time-MMD~\cite{liutime}, employ weighted fusion methods to combine textual embeddings with deep time-series backbones. By enhancing classical time series tasks with textual information, text-enhanced time series models offer greater robustness and richer interpretability across diverse applications~\cite{jin2024position,kong2025position}. 

\subsection{Language Question Answering}
% NLP side QA dataset etc
Question Answering (QA) in Natural Language Processing (NLP) involves systems that interpret human language queries to retrieve or generate accurate answers~\cite{biancofiore2024interactive, chen2024spiral}. It evolves from rule-based systems to neural architectures driven by LLMs like GPT-4~\cite{achiam2023gpt} and Llama~\cite{touvron2023llama}. These models leverage massive text corpora and large-scale datasets for end-to-end pre-training, fine-tuned via supervised learning or reinforcement learning with human feedback (RLHF) to align responses with factual and contextual relevance~\cite{liu2023summary}. Innovations such as retrieval-augmented generation (RAG) combine parametric knowledge with external data sources, while benchmarks like SQuAD, HotpotQA, MuSiQue, FinTextQA, SyllabusQA, and ToolQA drive progress~\cite{trivedi2022musique, ho2020constructing,yang2018hotpotqa,zhuang2024toolqa,chen2024fintextqa,fernandez2024syllabusqa}. However, challenges remain, such as handling ambiguous queries, ensuring the accuracy of generated answers, and maintaining efficiency in processing large volumes of data. Ongoing research focuses on enhancing the reasoning capabilities of QA systems, improving their ability to handle complex and nuanced questions, and expanding their applicability across diverse domains~\cite{singh2025agentic}.

\begin{figure*}[!t]
\begin{center}
\includegraphics[width = 1\linewidth]{Images/3.pdf}
\end{center}
\caption{The demonstration of the Time-MQA with context enhancement.}
% \caption{The framework overview of the Time-MQA with context enhancement.}
% \caption{Overview of the Time-MQA framework and TSQA dataset.}
\label{Figure_3}
\end{figure*}