\subsection{Variable Frame Rate Tokens and Unit Discovery}
\label{sec:variable-rate}
Information in speech is not uniformly distributed along the time axis~\cite{dieleman2021variable}.
In segments such as silence or long vowels, information density is low, whereas in segments with explosive consonants, speech events occur much more frequently.
This inherent non-uniformity suggests that it might be more natural to allocate more tokenized bits to regions with dense information and higher variance, and fewer bits to regions with less uncertainty.
This kind of discrete speech tokens is referred to as \textit{variable frame rate (VFR) tokens} in this review.
Note that while multi-resolution and variable-bitrate tokens have been introduced previously, the concept of VFR is still distinct.
In multi-resolution tokens~\cite{Siuzdak_SNAC_Multi-Scale_Neural_2024,guo2024speaking}, each quantizer operates at a fixed frame rate.
In variable-bitrate tokens~\cite{chae2024variable}, the frame rate remains fixed, while the variability lies in the  number of quantizers per frame.
Instead, VFR tokens should directly allocate different granularities on the temporal axis.

VFR tokens are closely related to acoustic unit discovery. As speech lacks a natural boundary of phonetic units~\cite{mohamed2022self}, there are much research efforts to find and locate the underlying acoustic units behind speech utterances in an unsupervised manner~\cite{eloff19_interspeech,dunbar20_interspeech,niekerk20b_interspeech,nguyen2020zero}.
This is particularly of interest for low-resource languages.
The discovered units can guide the boundary segmentation of VFR tokens.
To this end, VFR tokens are interesting not only because they might reduce the necessary bitrate, but also because they can introduce a strong inductive bias that linguistic knowledge is encoded~\cite{dieleman2021variable}.

A recent direction of VFR tokens is to discover acoustic units from an SSL model.
Note that deduplicated tokens and acoustic BPE themselves can be regarded as VFR tokens.
% SD-HuBERT~\cite{cho2024sd} finds that using a sentence-level criterion to finetune HuBERT results in syllable-level organizations in its representation similarity matrices.
Sylber~\cite{cho2024sylber} and SyllableLM~\cite{baade2024syllablelm} take similar approaches that first locate acoustic boundaries from existing HuBERT models, and then train another HuBERT student with segment-level pooled targets between boundaries.
The final HuBERT embeddings undergo the same segment-level pooling and kmeans clustering procedure to produce tokens at a very low frame rate ($\approx5$Hz) that align well with syllables.
% Upon it, Sylber~\cite{cho2024sylber} develops a greedy boundary discovery algorithm to locate syllable-level boundaries from SD-HuBERT.
% It then trains another HuBERT student model with segment-wise average pooled representations as targets, and the final HuBERT embeddings undergo kmeans clustering to produce syllable-level tokens at a very low frame rate $F\approx4.27$Hz.
% At the same time, SyllableLM~\cite{baade2024syllablelm} takes a different min-cut algorithm to locate syllable-like boundaries from the original HuBERT, and then trains a similar HuBERT using segment-wise pooled targets.
% The resulting syllable-level tokens exhibit decent reconstruction quality and stronger language modeling capability.

% Variable-rate constraint can also be plugged into an 
Boundary prediction can be involved to achieve frame rate variability in the training process, where a specific model predicts frame-level boundaries and is trained together with other network modules.
The training techniques of such models include reinforcement learning~\cite{cuervo2022variable}, soft pooling~\cite{hwang2024removing}, and slowness constraint~\cite{dieleman2021variable}.
% Cuervo et al.~\cite{cuervo2022variable} applies a hard boundary predictor into a contrastive speech SSL model for variable-rate downsampling and trains it using reinforcement learning.
% Hwang et al.~\cite{hwang2024removing} uses a soft predictor instead and performs downsampling through a soft pooling mechanism.
% \textcolor{red}{Variable rate hierarchical CPC, and variable rate soft-pooling}
% Different from boundary prediction, a slowness constraint is imposed into a VQ-VAE in \cite{dieleman2021variable} that forces the latent features to vary slowly along time, after which run-length encoding (i.e. deduplication with duration saved) is applied on the quantized codes.
% As the resulting quantized tokens will have lots of repeats, deduplication is applied to obtain the variable-rate tokens.
However, these approaches are rarely adopted in the context of discrete speech tokens today, and there have barely been a VFR acoustic token till now.
