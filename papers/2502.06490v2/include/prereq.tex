\section{Pre-requisites: Discrete Representation Learning}
\label{sec:prereq}

Discrete speech tokens are obtained through the quantization of continuous representations, which is usually achieved by offline clustering or online vector quantization algorithms.
% In this section, we briefly introduce the commonly used quantization methods in current discrete speech tokens as a preliminary.
This section provides a concise overview of the existing quantization methods commonly used in discrete speech tokens.

Denote $\bm x\in \mathbb R^d$ as a vector in the $d$-dimensional continuous space. A quantization process $q$ transforms $\bm x$ into a discrete \textit{token} in a finite set, i.e. $q(\bm x): \mathbb R^d\to\{1,2,...,V\}$ where $V$ is the \textit{vocabulary size}.
The output tokens are sometimes referred to as \textit{indexes} in the finite $V$-cardinal set.
The function $q$ is usually associated with a \textit{codebook} $\mathcal C=\{\bm c_1,\bm c_2,...,\bm c_V\}$ where every \textit{code-vector} $\bm c_i\in\mathbb R^d$ corresponds to the $i$-th token. 
The code-vectors are representations of tokens in the original $d$-dimensional space.
As $V$ elements can be encoded using $\lceil \log_2 V\rceil$ raw bits\footnote{We use $\lceil z\rceil$ to denote the ceiling of a scalar $z$, i.e., the smallest integer greater than or equal to $z$. 
Similarly, $\lfloor z\rfloor$ denotes the floor of $z$, i.e., the largest integer less than or equal to $z$.}, quantization often compresses the cost for data storage and transmission to a great extent.

\vspace{-0.15in}
\subsection{Offline Clustering}
\vspace{-0.05in}
Clustering is a simple approach for quantization. 
Given a dataset $X=\{\bm x_1,\bm x_2,...\bm x_N\}$, a clustering algorithm aims to assign each sample $\bm x_i$ to a group such that some cost is minimized.
The most frequently used clustering method for discrete speech tokens is k-means clustering~\cite{IKOTUN2023178}, e.g. in GSLM~\cite{lakhotia2021generative}.
K-means is a clustering algorithm based on Euclidean distances.
Its training process iteratively assigns each data sample to the nearest centroid, and moves cluster centroids till convergence, with a pre-defined number of clusters.
After training, the centroids form the codebook, and new data can be quantized to the index of the nearest centroid in this Voronoi partition.
In practice, centroids are usually initialized with the k-means++ algorithm~\cite{kmeans++} for better convergence.

Hierarchical agglomerative clustering has also been used in discrete speech tokens, which iteratively merges the closest clusters.
It is usually applied after k-means to reduce the number of clusters~\cite{cho2024sd,baade2024syllablelm}.
Other clustering algorithms are less explored in the context of discrete speech tokens.

\vspace{-0.12in}
\subsection{Vector Quantization}
\vspace{-0.04in}

\IEEEpubidadjcol

Clustering is often an isolate process, thus cannot be optimized together with other neural network modules.
Instead, vector quantization (VQ)~\cite{gray1984vector} enables a learnable network module that allows gradients to pass through when producing discrete representations.
Autoencoders with a VQ module is termed VQ-VAE~\cite{VQVAE}.
% There are multiple ways a VQ module can be realized:
There are multiple VQ methods:

\subsubsection{K-means VQ}
Like k-means clustering, k-means VQ method finds the code-vector closest to the input, i.e. 
\begin{equation}
    q(\bm x)=\underset{i\in \{1,2,...,V\}}{\arg\min} \|\bm x-\bm c_i\|^2.
\end{equation}
% \footnote{Note that $q$ outputs the code-vector now instead of the codebook index. Although it is a slight abuse of notation, the essence of $q$ is not changed.}
Then, code-vector $\bm c_k\triangleq\bm c_{q(\mathbf x)}$ is fed to subsequent networks.
As the $\min$ operation is not differentiable, straight-through estimators (STEs)~\cite{bengio2013estimating} are usually applied to graft gradients, i.e. $\operatorname{STE}(\bm c_k,\bm x)=\bm x+\operatorname{sg}(\bm c_k-\bm x)$ where $\operatorname{sg(\cdot)}$ stops tracking gradients.
In this way, the input value to subsequent networks is still $\bm c_k$, but gradients are grafted to $\bm x$ in back propagation.
% In this way, the loss function is still calculated by the value of $q(\bm x)$, but gradients that should be placed on $q(\bm x)$ is now grafted to $\bm x$ itself.

Auxiliary loss functions are often used together with k-means VQ~\cite{VQVAE}: commitment loss $\mathcal L_{\text{cmt}}=\|\operatorname{sg}(\bm c_k)-\bm x\|^2$ and codebook loss $\mathcal L_{\text{code}}=\|\operatorname{sg}(\bm x)-\bm c_k\|^2$.
The commitment loss pushes the continuous input $\bm x$ towards the closest codebook entry, while the codebook loss does the opposite and updates the code-vector $\bm c_k$.
The two loss terms are weighted by different factors to put different optimization strengths on $\bm x$ and $\bm c_k$, as pushing $\bm c_k$ towards $\bm x$ is an easier task.
It is also common to replace $\mathcal L_{\text{code}}$ with exponential moving average (EMA) to update the codebook instead~\cite{razavi2019generating}, which does not rely on explicit loss functions.
% Although EMA does not rely on explicit loss functions, it achieves a similar goal that gradually merges the continuous input $\bm x$ into the code-vector $\bm c_k$.
% that $\bm x$ is quantized to.

VQ in high-dimensional spaces is known to suffer from codebook collapse,  where the codebook usage is highly imbalanced~\cite{lancucki2020robust,dhariwal2020jukebox}.
To improve the utilization of codebook, random replacement (as known as \textit{codebook expiration}) can be applied~\cite{dhariwal2020jukebox} on code-vectors that have remained inactive for a long time. 
Other solutions include additional auxiliary constraints such as entropy penalty~\cite{chang2022maskgit,yu2024language}, factorized codebook lookup in low-dimensional space~\cite{yu2022vectorquantized}, and adding a linear projection to update all code-vectors together~\cite{zhu2024addressing}. 
% SimVQ~\cite{zhu2024addressing} explains that the independent updating of some code-vectors causes them to occupy the entire space, which leads to codebook collapse, and using linear layers to update all code-vectors simultaneously can help mitigate this problem.
% SimVQ~\cite{zhu2024addressing} states that codebook collapse stems from the independent optimization trajectories of code-vectors that lead to  spatial dominance, and proposes to add a simple linear projection layer that simultaneously updates all code-vectors per step to mitigate this problem.

\subsubsection{Gumbel VQ}
Instead of quantizing by Euclidean distance, another choice is by probability. 
Gumbel VQ~\cite{jang2017categorical} uses Gumbel-Softmax as a proxy distribution for traditional Softmax to allow differentiable sampling.
Given input $\bm x$ and a codebook of size $V$, a transform $h(\cdot)$ is applied on $\bm x$ into $V$ logits: $\bm l=h(\bm x)\in \mathbb R^V$.
In inference, quantization is performed by choosing the index with the largest logit, i.e. $q(\bm x)=\arg\max_i \left\{\bm l^{(i)}\right\}$.
In training, samples are drawn from the categorical distribution implied by $\bm l$ for the subsequent neural networks.
To achieve efficient sampling and let gradients pass through, Gumbel trick is used:
\begin{align}
    &\bm u\in \mathbb R^V\sim \operatorname{Uniform}(0, 1),\bm v=-\log(-\log(\bm u)) \label{eq:gumbel-noise} \\
    &\bm s=\operatorname{Softmax}((\bm l+\bm v)/\tau) \label{eq:gumbel-softmax}
\end{align}
where Eq.\eqref{eq:gumbel-noise} samples Gumbel noise $\bm v$ element-wise, and Eq.\eqref{eq:gumbel-softmax} calculates Gumbel-Softmax distribution $\bm s$ with a temperature $\tau$.
The forward pass simply use $j=\arg\max_i \{\bm s^{(i)}\}$ as the sampled index, but the true gradient of Gumbel-Softmax is used in backward pass.
In other words, the gradient on the one-hot distribution corresponding to $j$ is grafted to $\bm s$ as an approximate.
The temperature $\tau$ balances the approximation accuracy and gradient variances.
% : a lower $\tau$ results in sharper $\bm s$ and thus more accurate gradient estimate, but higher gradient variances~\cite{jang2017categorical}. 
% In practice, $\tau$ is usually annealed from high to low~\cite{jang2017categorical,vq-wav2vec}.
The transform $h(\cdot)$ is usually parameterized as neural networks, or negatively proportional to Euclidean distances~\cite{jiang2023latent}.

After quantization, code-vector $\bm c_k$ with $k=q(\bm x)$ is fed to subsequent networks.
Gumbel VQ does not require additional losses, since code-vectors can be directly learned with gradients and do not need to be pushed towards $\bm x$.

\subsubsection{Finite Scalar Quantization}
% \textcolor{gray}{VQ in the high-dimensional space is known to suffer from codebook collapse, a phenomenon where only a portion of codebook is active or the codebook usage is highly imbalanced~\cite{lancucki2020robust,dhariwal2020jukebox}.}
As mentioned before, VQ methods based on code-vector assignment usually suffer from codebook collapse. 
Despite many efforts, this remains a crucial challenge.
Finite scalar quantization (FSQ)~\cite{mentzer2024finite} is an alternative to perform quantization in scalar domain.
FSQ quantizes each dimension of a vector $\bm x$ into $L$ levels.
% \footnote{$L$ should better be odd, if following the original FSQ paper~\cite{mentzer2024finite}.}
For the $i$-th dimension $\bm x^{(i)}$, FSQ transforms the values to into limited range and then rounds to integers, i.e. 
\begin{equation}
    q\left(\bm x^{(i)}\right)=\operatorname{round}\left(\lfloor L/2\rfloor\tanh\left(\bm x^{(i)}\right)\right).
\end{equation}
The quantized values are thus integers ranging from $-\lfloor L/2 \rfloor$ to $\lfloor L/2 \rfloor$\footnote{Following \cite{mentzer2024finite}, this is the symmetric case for $L$ being odd. When $L$ is even, there is an offset before rounding to obtain asymmetric quantized values.}.
For a $d$-dimensional vector $\bm x$, there are $L^d$ possible quantization outcomes.
% Common choices are $L\ge 5$ and $d\le 10$, so FSQ usually has a much smaller hidden dimension than VQ (where usually $d\ge 100$).
Hence, FSQ usually requires a much smaller hidden dimension than VQ.
STE is applied to pass gradients.
As quantization is simply done via rounding to integers, there is no explicit codebooks associated with the FSQ process.
% The range of $q$ here is also not a categorical set where indexes cannot be numerically compared as that of VQ, but an ordered set of integers.
% This indicates a different approach when using FSQ instead of VQ for generative tasks.
FSQ is reported to have  better codebook usage\footnote{Although there is no longer a codebook associated with code-vectors, codebook usage can still be measured among all possible $V=L^d$ outcomes.} especially for a large $V$ compared to VQ methods, without the need for auxiliary losses.

\subsubsection{Other VQ Tricks}
In many cases, a single VQ module suffers from a highly-limited representation space, thus results in poor performance compared to continuous counterparts. 
There are some widely-used VQ tricks that introduce multiple quantizers to refine the quantized space, as shown in Fig.\ref{fig:gvq-rvq}:

\begin{figure}
    \centering
    \includegraphics[width=0.53\linewidth]{figs/GVQ.png}
    \includegraphics[width=0.45\linewidth]{figs/RVQ.png}
    \caption{Diagram of GVQ (left) and RVQ (right).}
    \label{fig:gvq-rvq}
\end{figure}

\begin{enumerate}[leftmargin=5mm]
    \item \textit{Grouped VQ (GVQ)}, also known as \textit{product quantization}~\cite{product_quantization}. It groups the input vector $\bm x$ by dimensions and apply VQ on different parts of $\bm x$ independently. They can have different or shared codebooks. The VQ outputs are then concatenated along dimensions to match that of $\bm x$. For instance, GVQ is used in neural word embeddings~\cite{9164982} and speech self-supervised learning models~\cite{vq-wav2vec,baevski2020wav2vec} to achieve efficient quantization.
    \item \textit{Residual VQ (RVQ)}, also known as \textit{multi-stage quantization}~\cite{multiple-stage-vector-quantization}. It adopts a serial approach that iteratively quantizes the residual of the last quantizer.
    Similar to GVQ, RVQ also has multiple quantizers.
    For the $i$-th quantizer $q_i$ with input $\bm x_i$ and output code-vector $\bm c_{k}$, the residual is defined as $\bm x_{i+1}=\bm x_i-\bm c_{k}$.
    The outputs from all $q_i$ are finally summed as the quantized result of $\bm x$.
    In this way, information in the codebooks is supposed to follow a coarse-to-fine order, and more details in the original $\bm x$ can be preserved than a plain quantizer.
    It is used in various speech codecs~\cite{zeghidour2021soundstream,encodec,kumar2024high}, for instance.
\end{enumerate}
GVQ and RVQ can be flexibly combined to form GRVQ~\cite{yang2023hifi} that applies RVQ on each GVQ branch for better codebook utilization.
% A variant of RVQ is cross-scale VQ (CSVQ)~\cite{jiang22_interspeech} where residuals are not defined as the quantization margins directly, but instead 
A network can also contain multiple VQ modules at different places, like cross-scale VQ (CSVQ)~\cite{jiang22_interspeech} where every decoder layer has a quantizer inside.

Note that RVQ naturally produces an order of importance in residual layers, while all quantizers in GVQ are equally important.
Such order of importance can also be enforced in GVQ by a ``nested dropout'' trick~\cite{rippel2014learning}.
% There is also a ``nested dropout'' trick~\cite{rippel2014learning} that assigns an importance order to GVQ, by manually define an order of quantizers and randomly dropping-out the last few quantizers in training.
