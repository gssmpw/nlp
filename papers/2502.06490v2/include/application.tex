
\section{Discrete Speech Token-based Application Paradigms}
\label{sec:application}
% \subsection{Speech Recognition and Speech Translation}


\subsection{Spoken Language Understanding}
\subsubsection{Motivation}
Spoken language understanding (SLU) tasks, including automatic speech recognition (ASR), speech translation, intent classification and others, aim to extract meaningful domain-specific information from speech.
Most SLU tasks follow a speech-in text-out pipeline, except S2ST which also involves speech generation.
% These tasks present opportunities for the use of discrete tokens, as they enable efficient information distillation by retaining only task-relevant content during token extraction.
The adoption of discrete tokens in SLU offers some benefits. 
Discrete tokens may naturally exhibit some invariance against noise and speaker information, particularly semantic tokens, which can make downstream models to focus more effectively on content-related information in some tasks.
% Also, semantic tokens with insufficient speaker information can make downstream models to focus more effectively on content-related information in some tasks.
On a broader scale, discrete tokens provide a promising approach to unifying speech understanding and generation in spoken language models.

As an alternative input to an SLU model instead of continuous features, discrete speech tokens are typically deduplicated or BPE-encoded before subsequent modules.
Semantic tokens have been better explored than acoustic ones in this context.

\subsubsection{Speech Translation}
Among the various SLU tasks, discrete speech tokens are mostly adopted in speech translation, including speech-to-text translation (S2TT) and speech-to-speech translation (S2ST).
% both of which exemplify the use of discrete tokens in SLU. 
% These tasks typically begin by converting source-language speech into discrete tokens, performing unit-level modeling, and ultimately translating the units into either target-language text or speech. 
Since semantic tokens correlate well with phonetics, they can serve as universal pseudo-labels for untranscribed languages, useful for S2TT in low-resource settings~\cite{zhang-etal-2023-dub}.
% As a result, in low-resource settings, leveraging discrete tokens has demonstrated significant potential for improving S2TT performance~\cite{zhang-etal-2023-dub}.
Direct S2ST using discrete tokens has garnered more attention on the generation side (Section \ref{sec:application-synthesis}).
Early approaches primarily rely on extracting discrete tokens using VQ-VAEs, particularly for unwritten languages~\cite{tjandra2019speech, zhang2021uwspeech}. 
Recent researches in this area include employing semantic tokens~\cite{lee2022direct, lee-etal-2022-textless, wu2023speechgen}, acoustic tokens~\cite{peng2024mslm, wang-etal-2024-speech, gong2024seamlessexpressivelm}, two-pass architectures~\cite{chen-etal-2023-speech, inaguma-etal-2023-unity}, and non-autoregressive frameworks~\cite{huang2023transpeech}. 
% Subsequent studies have focused on enhancing translation quality and efficiency through various methods, such as introducing two-pass architectures~\cite{chen-etal-2023-speech, inaguma-etal-2023-unity}, employing acoustic tokens~\cite{peng2024mslm, wang-etal-2024-speech, gong2024seamlessexpressivelm}, and developing non-autoregressive frameworks~\cite{huang2023transpeech}. 
These efforts collectively contribute to advancing the performance and applicability of discrete token-based speech translation systems.

\subsubsection{Unified Speech Understanding}

Discrete tokens provide opportunity to construct unified and adaptable spoken language models with various SLU functionalities.
Efforts include task identifies~\cite{wang2024viola}, prompt tuning~\cite{chang2022speechprompt, chang2023speechprompt, 10.1109/TASLP.2024.3436618,wu2023speechgen}, shared audio and text  vocabulary~\cite{rubenstein2023audiopalm}, and combining continuous and discrete representations~\cite{chen2023lauragpt}.
% Since SpeechPrompt~\cite{chang2022speechprompt, chang2023speechprompt, 10.1109/TASLP.2024.3436618} first explored the SLU capabilities of the spoken language modeling framework through prompt tuning, discrete tokens have been explored for constructing spoken language models with SLU functionality.
% These models leverage discrete tokens to improve efficiency and adaptability across various tasks.
% AudioPaLM~\cite{rubenstein2023audiopalm} extends the PaLM architecture~\cite{JMLR:v24:22-1144} by incorporating audio embeddings and jointly modeling text and audio tokens, demonstrating abilities in ASR and S2TT by leveraging a shared vocabulary for both modalities. 
% VioLA~\cite{wang2024viola} adapts the VALL-E~\cite{valle} framework by interleaving speech and text tokens and employing autoregressive modeling across both modalities. This design enables VioLA to handle various SLU tasks, including ASR, S2TT, and S2ST.
% LauraGPT~\cite{chen2023lauragpt} also shows notable capabilities in generative tasks besides understanding, such as TTS and speech enhancement.
These efforts highlight the potential of discrete tokens in enhancing the performance and versatility of universal SLU models.
% \textcolor{red}{Seems like a sudden finish.}

\subsubsection{Limitations}

Despite the advantages and growing popularity in S2ST tasks, discrete tokens still underperform in many SLU tasks. 
Lots of SLU studies~\cite{puvvada2024discrete, chang2024exploring, shon2024discreteslu, cui2024exploring_context, cui2024exploring} only verify that discrete tokens can surpass traditional frequency-domain features in certain tasks such as ASR. 
Continuous SSL features continue to have superior performance~\cite{wang2024comparative}. 
The majority of current LLM-based SLU models rely predominantly on continuous inputs, such as Whisper features~\cite{gong2023joint,chu2023qwen,tang2024salmonn,wavllm,ma2024embarrassingly,bai2024seed}.
Moreover, the performance of discrete tokens in speaker-related tasks is generally much inferior to that of continuous features~\cite{puvvada2024discrete,mousavi2024dasb}.
% Therefore, effectively leveraging discrete tokens for SLU remains an open challenge.
% Compared to continuous features, 
A significant limitation of discrete tokens for SLU is the inevitable information loss during the quantization process.
Mitigating such loss with more VQ codebooks may hinder the accessibility of semantic information crucial for SLU as well.
% Although acoustic tokens with sufficient decompression ability will mitigate such loss, they may also hinder the accessibility of semantic information which is crucial for SLU.
Therefore, the full potential of leveraging discrete tokens for SLU remains largely untapped and warrants further exploration.

\subsection{Speech Generation}
\label{sec:application-synthesis}
% \textcolor{red}{More tasks, such as speech editing, SVS, speech enhancement, VC, ...}

\subsubsection{Motivation}

Discrete tokens have catalyzed a paradigm shift in speech generation, with TTS being the most representative application.
In TTS systems, discrete tokens are usually used as intermediate features that bridge the acoustic model (text-to-token) and the vocoder or codec decoder (token-to-wav).
% The utilization of discrete speech tokens has driven the evolution of TTS models towards high-fidelity, zero-shot and controllable synthesis.
There are two major advantages of applying discrete tokens in TTS:
\begin{itemize}
    \item \textbf{Easier training objectives}. Discrete tokens replace the original spectrogram-based regression task with a classification task~\cite{VQTTS}, which can be much easier.
    % As the discrete tokens take values from a finite set, predicting them is easier for acoustic models than continuous features which can be infinitely precise. 
    This also offers a better balance between acoustic models and vocoders, since texts are closer to discrete speech tokens than frequency-domain features.
    % In traditional TTS paradigms~\cite{shen2018natural,ren2021fastspeech}, the vocoder takes a relatively easy task to convert frequency-domain features into waveforms, while the acoustic model transforms text into a largely different domain.
    % Discrete tokens help to better balance the modeling burden between the two processes, thus makes the prediction task of the acoustic model more manageable.
    \item \textbf{Better use of decoder-only language models}. 
    Decoder-only language models have shown remarkable success in natural language generation.
    After discretization, speech can also be autoregressively generated under the same paradigm. 
    This offers huge potential in leveraging the in-context learning and scaling capabilities of language models to achieve zero-shot high-fidelity TTS~\cite{valle}. 
    % Moreover, such TTS models can achieve considerable performance improvements after scaling the data and model size, leading to high-fidelity synthesis~\cite{lajszczak2024base,seedtts}.
    % In such TTS models, the speaker prompt is attended in every layer of Transformer blocks, which allows 
\end{itemize}
Other generative tasks, such as singing voice synthesis and speech editing, can similarly benefit from the advantages of discrete tokens observed in TTS.
% VC with discrete tokens as content representations can be simplified to a token vocoder~\cite{guo2024vec2wav}, if the timbre information is well removed in the tokens.
For voice conversion (VC), using discrete tokens as content representations can simplify the process to a token vocoder~\cite{guo2024vec2wav}, when timbre information is effectively removed from the tokens.
Tasks like speech to speech translation~\cite{lee2022direct,zhang2021uwspeech}, speech enhancement~\cite{wang2024selm,liu2024joint} and target speaker extraction~\cite{tang2024tselm} can also be enhanced through language modeling on discrete tokens.

\subsubsection{Autoregressive TTS}
Autoregressively predicting the next VQ index of discrete speech tokens is first proposed in VQTTS~\cite{VQTTS}, which uses an LSTM conditioned on Transformer representations to generate vq-wav2vec~\cite{vq-wav2vec} semantic tokens.
A discrete token vocoder converts the tokens to waveforms with the assistance of handcrafted prosody features.
% This system uses vq-wav2vec tokens.
VQTTS achieves state-of-the-art TTS quality at that time, and shows promising performance in speaker-adaptive TTS~\cite{TNVQTTS,DSETTS,limmits23} and expressive TTS~\cite{liu2024storytts}.

Subsequently, decoder-only TTS models using neural audio codecs have made tremendous success in zero-shot TTS starting from VALL-E~\cite{valle}.
VALL-E contains an autoregressive (AR) model and non-autoregressive (NAR) model, both of which generate EnCodec~\cite{encodec} RVQ tokens.
The AR model performs next-token prediction on the first RVQ layer conditioned on text.
The NAR model predicts the $n+1$-th RVQ tokens given the text, all EnCodec tokens from the speaker reference, and the previous $n$ RVQ layers.
% receives the text, all EnCodec codes from the speaker reference and the previous $k$ EnCodec RVQ levels to predict the codes at $k+1$-th level together.
VALL-E employs a concise design in which text and speaker references serve as ``prompts'' for a language model.
It achieves remarkable zero-shot TTS performance when trained on 60k hours of speech.
Later, methods have been proposed to improve generation robustness~\cite{song2024ella,xin2024rall,han2024vall,du2024vall,wang2024attention,chen2024vall}, efficiency~\cite{song24b_interspeech}, style control~\cite{kim2023sc,lyth2024natural,ji2024textrolspeech}, and to incorporate LLMs~\cite{hao2023boosting,shen2024get}.

Besides using an NAR model to predict the rest RVQ layers, alternate modeling strategies have been proposed, such as hierarchical modeling~\cite{yang2024uniaudio} and token interleaving patterns~\cite{musicgen,voicecraft}. 
% UniAudio~\cite{yang2024uniaudio} uses hierarachical Transformers to autoregressively predict fine-grained RVQ codes in each frame.
% Token interleaving patterns~\cite{} can further simplify the model architecture to only one AR Transformer with multiple prediction heads.
Semantic tokens are also introduced to cooperate with acoustic codecs~\cite{kharitonov2023speak,vectokspeech,shen2024get,yang2024interleaved}, which might decrease the modeling difficulty since they bridge the gap between texts and acoustics and usually require only a single token stream.
% Such simplification can provide better streaming ability by speech-text interleaving~\cite{}.
Numerous industry-level large-scale TTS systems have been produced in this autoregressive TTS paradigm, such as XTTS~\cite{casanova2024xtts}, BASE-TTS~\cite{lajszczak2024base}, Seed-TTS~\cite{seedtts}, CosyVoice~\cite{du2024cosyvoice,cosyvoice2}, Fish-Speech~\cite{liao2024fish}, etc.

\subsubsection{Non-Autoregressive TTS}
While autoregressive modeling is the current mainstream of TTS with discrete tokens, non-autoregressive models also exist.
These models either treat the code-vectors as continuous features~\cite{shen2024naturalspeech2}, or directly generate discrete tokens by masked prediction~\cite{wang2024maskgct} or discrete diffusion models~\cite{du2024unicats,facodec}.
% A convenient way is to treat the code-vectors of discrete tokens as continuous features and use the well-established regression objective to model these vectors.
% An example is NaturalSpeech 2~\cite{shen2024naturalspeech2} which uses diffusion model to generate the sum of SoundStream code-vectors per frame, with duration-upsampled phone representations as conditions.
% As tokens are inherently discrete, discrete diffusion models~\cite{austin2021structured} are also applied.
% UniCATS~\cite{du2024unicats} and NaturalSpeech 3~\cite{facodec} use discrete diffusion to model semantic and disentangled acoustic tokens respectively.
% Similarly, the masked prediction of tokens proposed by MaskGIT~\cite{chang2022maskgit} is also incorporated into TTS~\cite{wang2024maskgct,gallego2024single}.
These non-autoregressive methods are naturally more robust than autoregressive methods in inference, and also supports speech editing.

\subsubsection{Unified Speech Generation}
The language modeling approach of discrete tokens allows a unified generation framework for multiple tasks.
It suffices to use a task identifier to condition the unified language model.
For example, \cite{vallex,wang2024speechx} extends VALL-E with more tasks like cross-lingual TTS, S2ST, speech editing, etc.
% Make-A-Voice~\cite{huang2023make} uses a hierarchical Transformer architecture, and supports TTS, VC and SVS and singing VC.
% SpeechX~\cite{wang2024speechx} includes more tasks such as speech editing and noise suppression with an AR+NAR codec language model like VALL-E.
UniAudio~\cite{yang2024uniaudio} supports 11 speech and audio generation tasks within a single hierarchical Transformer model.
% \textcolor{red}{MORE?}
% It achieves comparable or even better performance than special-purpose in most tasks, demonstrating the potential of a large-scale foundation model.
Prompt tuning upon a spoken language model has also been explored in \cite{wu2023speechgen} for efficient, transferable and versatile generation.
These efforts demonstrate the potential of a large-scale foundation model for generation.

\subsubsection{Limitations}
In contrast to discrete tokens, another emerging framework for speech generation is diffusion or flow matching-based models, including non-autoregressive models \cite{lee2023hierspeech++,le2024voicebox,liu2024e1,chen2024f5} or autoregressive models \cite{meng2024autoregressive,liu2024autoregressive,zhu2024autoregressive,turetzky2024continuous,jia2025ditar}.
They generate continuous features, and some even eliminate the need for forced alignments in non-autoregressive generation.
Owing to the strong capability of diffusion and flow matching algorithms, they also have remarkable generation fidelity, diversity and controllability.
They can have a higher upper bound for speech quality and intelligibility, as they inherently avoid quantization errors.
In comparison, discrete token-based speech generation models usually fall short in generation robustness.
% Additionally, diffusion-based autoregressive speech generation using continuous features is an emerging area of interest~\cite{meng2024autoregressive,liu2024autoregressive,zhu2024autoregressive,turetzky2024continuous,jia2025ditar}.
Therefore, there is an ongoing debate between discrete and continuous representations for speech generation.

\subsection{Text-Free Spoken Language Models}
\subsubsection{Motivation}
End-to-end spoken language and dialogue modeling is one of the most ultimate goals in speech technology.
% Developing end-to-end speech generation and dialogue models is a critical goal in the field of speech processing.
% An end-to-end speech dialogue model has the ability to directly understand speech input and generate voice responses.
Discrete tokens are a core component of existing spoken language models, as they enable the language modeling technique to be applied directly on speech. The models discussed in this subsection are text-free spoken language models (TF-SLMs). 
We anticipate that a well-trained TF-SLM will be capable of generating semantically coherent speech without the need for text transcription guidance.

\subsubsection{Existing Efforts}
Ever since GSLM~\cite{lakhotia2021generative} and AudioLM~\cite{borsos2023audiolm} proposed the vision of TF-SLMs, building such models remains a significant challenge even till today. 
This difficulty primarily arises from the lower language modeling efficiency of speech token sequences compared to text, due to their lower semantic information density, longer sequence lengths, and the presence of paralinguistic information~\cite{wang2024whyspeech}. 
Current advancements in TF-SLMs mainly focus on two strategies: (1) reducing token frame rates, and (2) aligning speech with text.

The first approach aims to shorten speech sequences and enhance semantic density by lowering frame rates \cite{lakhotia2021generative, hassid2024textually,shen2024acoustic} to even $\approx$5Hz~\cite{baade2024syllablelm, cho2024sylber}. 
While mitigating sequence length issues to different degrees, they still encounter scalability limitations~\cite{cuervo-marxer-2024-scaling} and compromise reconstruction quality. 
The second strategy involves aligning speech with text through methods like initializing pre-training with text LLMs~\cite{hassid2024textually},  reinforcement learning using ASR and text LLM feedback~\cite{lin2024align}, text-speech token interleaving~\cite{nguyen2024spirit}, adopting novel architectures applied in text language modeling~\cite{park2024longform}, etc~\cite{veluri2024beyond, zhang2024intrinsicvoice}. 
Meanwhile, full duplex modeling has been proposed~\cite{ma2024language} to allow users to interrupt and start new dialogues at will.
However, despite many efforts, these models still struggle to generate semantically reliable long speech during inference due to the lack of explicit transcription guidance.

% \subsubsection{Summary} 
\subsubsection{Limitations} 
Although these methods show promise, achieving semantic coherence is still a challenging goal, leaving significant progress to be made toward the goal of truly end-to-end spoken language modeling. 
Improving the semantic density and expressiveness of discrete speech representations, making it easier to align text and speech during TF-SLM training, is a promising direction for future exploration.

\subsection{Text-Guided Spoken Language Models}
\subsubsection{Motivation}
% The demand for end-to-end speech dialogue systems is very strong. 
Since TF-SLM remains an open problem, the prevalent successful speech dialogue systems settle for an alternative choice that uses text as explicit guidance.
Recent researches, especially following work like OpenAI's GPT-4o\footnote{https://openai.com/index/hello-gpt-4o/}, have focused on SLMs that combine three key capabilities: strong understanding of speech semantics, high-quality speech output, and low latency~\cite{zhang2024speechgptgen, fu2024vita, Xie2024MiniOmniLM, kyutai2024moshi, xie2024mini2, fang2024llamaomni, yu2024salmonnomni, zhong2024lyra, chen2024slamomni, chen2024emova, wang2024freezeomni, zeng2024glm, zhang2024internlm, fu2025vita, luo2025openomni, chen2025minmo}. 
We refer to them as text-guided spoken language models (TG-SLMs). 
Unlike TF-SLMs, while TG-SLMs utilize a unified LLM for seamless processing of user's speech input and system's speech output, they internally decompose the end-to-end speech dialogue process into two well-established sub-procedures: SLU powered by LLMs, and real-time TTS. 
The two sub-procedures are connected via text as an intermediary to stabilize the semantic coherency of the final output. The LLM first generates a textual response to the audio input, then synthesizes the speech token sequence in a streaming fashion. 
In a TG-SLM, the SLU sub-procedure usually uses continuous speech features as input since they preserve more acoustic details for understanding, while the TTS sub-procedure typically uses discrete speech tokens as output to better fit LLM autoregressive generation.
% In this way, ideally, the model is capable of semantic-coherent response, high-fidelity speech generation, and quick interaction simultaneously.

\subsubsection{Speech Generation in TG-SLMs}
To reduce modeling complexity and better align with the autoregressive generation paradigm of LLMs, TG-SLMs favor single-layer discrete speech tokens as direct LLM outputs. 
Existing works make use of either the first layer of an RVQ codec~\cite{zhang2024speechgptgen}, single-codebook codec~\cite{fu2025vita}, or single-codebook supervised semantic token~\cite{zeng2024glm,luo2025openomni}.
Specific designs are introduced corresponding to the tokens, such as chain-of-modality~\cite{zhang2024speechgptgen}, token interleaving to lower latency~\cite{zeng2024glm}, two-stage decoding process~\cite{fu2025vita}, etc.
% SpeechGPT-Gen~\cite{zhang2024speechgptgen} applies a chain-of-modality approach, which utilizes the tokens of the first RVQ layer of SpeechTokenizer~\cite{zhang2024speechtokenizer} as the output of LLM (i.e., TTS sub-procedure output), and uses a separate flow matching model to infer the rest RVQ layers after finishing generating the first RVQ layer.
% GLM-4-Voice~\cite{zeng2024glm} uses 12.5Hz supervised semantic tokens as both the speech input and speech output of the LLM. 
% To lower latency and preserve the intelligence of the LLM, it employs an interleaving manner to generate text and corresponding speech token sequences in turn. 
% VITA-1.5~\cite{fu2025vita} leverages TiCodec~\cite{ticodec} as the output tokens and employs a two-stage decoding process: a non-autoregressive decoder for initial semantic processing, followed by an autoregressive decoder for refined speech token generation.
% OpenOmni~\cite{luo2025openomni} deploys the $\mathcal S^3$ Tokenizer~\cite{du2024cosyvoice} as output and adds a streaming speech decoder to simultaneously generate text and speech responses.
To better rebuild the speech information with the help of pretrained LLMs, several TG-SLMs use multi-layer speech tokens as LLM output, such as \cite{kyutai2024moshi,Xie2024MiniOmniLM,xie2024mini2}.
They often employ different techniques to generate the text tokens and multi-layer speech tokens in parallel reduce latency.
% Moshi~\cite{kyutai2024moshi} uses its own Mimi codec with 8 RVQ layers. 
% The text response and Mimi's first RVQ layer (semantic layer) are generated in parallel through separate output heads. 
% After a brief delay, the remaining layers (acoustic layers) begin parallel generation.
% Mini-Onmi~\cite{Xie2024MiniOmniLM} and Mini-Omni2~\cite{xie2024mini2} utilize multi-layer multi-resolution codec SNAC~\cite{Siuzdak_SNAC_Multi-Scale_Neural_2024} as the speech output. 
% Different from Moshi, they decode text tokens and each layer of speech tokens in different output heads in parallel to reduce latency. 
% They further employ a parallel decoding strategy similar to \cite{musicgen} for fast and high-quality multi-layer speech token generation.

Mainstream TG-SLMs with discrete tokens as LLM outputs need an additional decoder to synthesize continuous speech signals, either using the codec decoder or a separately-trained vocoder.
% (whether the codec decoder~\cite{zhang2024speechgptgen, Xie2024MiniOmniLM, kyutai2024moshi, xie2024mini2, fu2025vita} or a separately trained one~\cite{fang2024llamaomni, chen2024emova, zeng2024glm, luo2025openomni, chen2025minmo}). 
There are also efforts to streamingly synthesize speech signals directly based on the LLM hidden embedding~\cite{yu2024salmonnomni,chen2025minmo}, eliminating the need for discrete tokens, additional decoders, or even explicit text tokens, hence further improving the real-time ability.
% Recently, SALMONN-Omni~\cite{yu2024salmonnomni} and MinMo~\cite{chen2025minmo} propose to streamingly synthesize speech signals directly based on the LLM hidden embedding, eliminating the need for discrete tokens, additional decoders, or even explicit text tokens, hence further improving the real-time ability.

% \subsubsection{Summary}
\subsubsection{Limitations}
Overall, TG-SLMs' task decomposition is effective and flexible.
The SLU sub-procedure can handle both continuous and discrete representations, and single-layer discrete tokens simplify the training and inference of the TTS sub-procedure.
% The input of the SLU sub-procedure is flexible, allowing both continuous and discrete representations, similar to advancements in SLU and multimodal LLMs. 
% For the TTS sub-procedure, single-layer discrete tokens are favored as LLM outputs due to their alignment with the autoregressive nature of LLMs and ease of modeling, training, and inference.
However, unlike TF-SLMs, TG-SLMs rely heavily on text as an intermediary in the TTS sub-procedure, which may overlook paralinguistic information such as emotion, prosody, and environmental context from the previous input, resulting in less coherent and natural response. Additionally, the lack of high-quality annotated conversational data and concerns over security pose significant challenges for the future development of TG-SLMs.
