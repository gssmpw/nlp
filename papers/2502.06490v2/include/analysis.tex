
\section{Analysis of Discrete Speech Tokens}
\label{sec:analysis}
\subsection{Metrics and Existing Benchmarks}

Discrete speech tokens can be evaluated from various aspects besides bitrate and codebook utilization:
\begin{itemize}
    \item \textbf{Signal-level reconstruction metrics}: 
    For reconstruction evaluations, signal-level metrics like
    PESQ, STOI, mel distance, GPE, etc. are often used.
    \item \textbf{Perceptual reconstruction metrics}: Apart from  signal-level metrics, there can also be perceptual evaluations of reconstruction performance. This includes intelligibility (often measured by WER, CER, or phone error rates), speaker similarity, subjective or proxy MOS tests, etc.
    \item \textbf{Performance on downstream tasks}: Probing tasks can be used to measure the preservation or prominence of certain information in tokens, like ASR, ASV, emotion recognition, and spoken language modeling~\cite{nguyen2020zero}.
    Note that this is different from perceptual reconstruction metrics since it operates directly on tokens.
    Performance in generative tasks like TTS and VC can also be evaluated.
    % Also, as discrete tokens can be applied to more generative speech processing tasks like TTS and VC, it is feasible to compare the performance of these tasks.
    \item \textbf{Semantic/phonetic relevance}: If the tokens are expected to align with texts (e.g. for semantic tokens and semantic-distilled acoustic tokens), metrics like phone discrminability~\cite{nguyen2020zero}, phone purity, and phone-normalized mutual information~\cite{hsu2021hubert} can be computed.
    \item \textbf{Invariance and robustness}: If the tokens are expected to be invariant to perturbations, unit edit distance~\cite{gat2023augmentation} can be considered as a measurement.
\end{itemize}
There are several existing benchmarks on discrete speech tokens. 
Codec-SUPERB\footnote{\scriptsize\url{https://codecsuperb.com/}}~\cite{wu2024codec} evaluates both signal-level reconstruction metrics and downstream performances of acoustic tokens.
ESPnet-Codec~\cite{shi2024espnet} integrates multiple codecs into a unified training and evaluation framework VERSA~\cite{shi2024versa} and extends evaluation to some generative tasks such as TTS and SVS.
DASB\footnote{\scriptsize\url{https://github.com/speechbrain/benchmarks/tree/main/benchmarks/DASB}}~\cite{mousavi2024dasb} performs more downstream probing tasks, and includes generative tasks as well as semantic tokens.
STAB~\cite{vashishth2024stab} takes a different perspective that measures the invariance, robustness, compressibility, and vocabulary utilization of speech tokens. 
This emphasizes the application in spoken language models instead of reconstruction.
% Expresso~\cite{expresso} proposes an expressive speech dataset and investigates the resynthesis quality of HuBERT and EnCodec tokens. 
% It shows that HuBERT falls short in preserving the expressivity of source speech while EnCodec does better.
% Expresso~\cite{expresso} introduces an expressive speech dataset and evaluates the resynthesis quality of HuBERT and EnCodec tokens, finding that HuBERT struggles to preserve source speech expressivity while EnCodec performs better.


\subsection{Existing Analyses}

There are several theoretical or experimental analyses of the advantages of discrete speech tokens.
Nguyen et al.~\cite{nguyen2022discrete} demonstrates by an encoder-only language model that semantic SSL tokens are favorable for spoken language modeling, due to their removal of linguistically irrelevant information.
Sicherman et al.~\cite{sicherman2023analysing} supports this claim by showing that semantic units have a strong correlation with phonemes, but a weaker correlation with speakers.
Abdullah et al.~\cite{abdullah23_interspeech} refines the correlation between semantic SSL tokens and linguistic content to the ``sub-phonemic'' level instead of high-level phonemes due to contextual variability.
Chang et al.~\cite{chang23b_interspeech} explores the use of WavLM tokens for end-to-end ASR together with deduplication and BPE. Although these tokens underperform continuous SSL features, they still show competitive performance. 
Similar findings are reported on contextual ASR~\cite{cui2024exploring_context}, multilingual ASR~\cite{cui2024exploring}, end-to-end speech translation, 
understanding~\cite{chang2024exploring}, and more LLM-based semantic-related tasks with discrete units as inputs~\cite{wang2024comparative}.
Expresso~\cite{expresso}  evaluates the resynthesis quality of HuBERT and EnCodec tokens on an expressive dataset, finding that HuBERT struggles to preserve source speech expressivity while EnCodec performs better.

The downside of speech tokens is also researched.
Yeh et al.~\cite{yeh2024estimating} suggests that VQ on HuBERT embeddings cannot achieve perfect disentanglement of speaker and phonetic content. 
EMO-Codec~\cite{ren2024emo} shows that codec reconstruction still sometimes degrades the emotion information.
O'Reilly et al.~\cite{o2024code} shows that neural audio codecs often lack stability after repeated encoding and decoding, i.e. not idempotent.

Therefore, the reconstruction quality of acoustic tokens and the performance on discriminative downstream tasks of both acoustic and semantic tokens have been well benchmarked.
% As the reconstruction performance of semantic tokens still lacks thorough comparison, we take the reconstruction approach to compare different kinds of tokens.
However, the reconstruction performance of semantic tokens still requires a more thorough comparison.
Hence, we adopt a reconstruction approach to compare different types of tokens.
% We use a timbre-controllable speech token vocoder to resynthesis semantic tokens back to waveform, and measure the information preservation of content, prosody, speaker and acoustic details respectively. 
% Details and results of these experiments will be elaborated in the rest of this chapter.
Specifically, we use a timbre-controllable speech token vocoder to resynthesize semantic tokens into waveforms and measure the preservation of content, prosody, speaker identity, and acoustic details, respectively. The details and results of these experiments will be elaborated in the rest of this chapter.

\IEEEpubidadjcol


\subsection{Reconstruction Analysis}
\label{sec:reconstruction}


\begin{table*}[]
\centering
\caption{Reconstruction and voice conversion comparisons of tokens in different categories.
For all semantic tokens, we train a specific CTX-vec2wav$^\alpha$ as a vocoder.
All vocoders are trained on LibriTTS~\cite{libritts}, and evaluations are done on LibriTTS testset-B~\cite{du2024unicats}.
``L'' means a certain layer in the SSL Transformer block, and ``km'' means manual k-means clustering.
Settings in parentheses denote model versions. 
% \textcolor{red}{Necessary to include sampling rate?}
% $\Delta$SECS means the difference of SECS values between reconstruction and VC, since they operate on the same source utterances.
% \textcolor{red}{
% NOTE: for tokens that can do VC, the reconstruction is also a VC (prompt from another utterance, instead of itself. This can lead to some concerns.)}
}
\label{tab:recon-vc}
\resizebox{1.99\columnwidth}{!}{
\begin{tabular}{@{}llccccccccc@{}}
\toprule
\multicolumn{1}{c}{\multirow{2}{*}{\textbf{Type}}} & \multicolumn{1}{c}{\multirow{2}{*}{\makecell{\textbf{Token}\\Model (version)}}} & \multicolumn{1}{c}{\multirow{2}{*}{\makecell{\textbf{Bitrate$\downarrow$}\\(kbps)}}} & \multicolumn{5}{c}{\textbf{Reconstruction}} & \multicolumn{3}{c}{\textbf{Voice Conversion}} 
% & \multicolumn{1}{c}{\multirow{2}{*}{\textbf{$\Delta$SECS$\downarrow$}}}
\\ \cmidrule(l){4-8}\cmidrule(l){9-11} 
\multicolumn{1}{c}{} & & \multicolumn{1}{c}{} & \multicolumn{1}{c}{\textbf{WER$\downarrow$}} & \multicolumn{1}{c}{\textbf{GPE$\downarrow$}} & \multicolumn{1}{c}{\textbf{PESQ$\uparrow$}} & \multicolumn{1}{c}{\textbf{STOI$\uparrow$}} & \textbf{SECS$\uparrow$} & \multicolumn{1}{c}{\textbf{WER$\downarrow$}} & \multicolumn{1}{c}{\textbf{SECS$\uparrow$}} & \multicolumn{1}{c}{\textbf{P.Corr$\uparrow$}} \\ 
\midrule
\multirow{2}{*}{\textbf{Continuous Baselines}}  & Ground Truth Recording & - & 1.16 & 0.00 & 4.50 & 1.000 & 1.000 & - & - & - \\
 & Mel + BigVGAN (100 band) & \iffalse 300 \fi - & 1.18 & 0.88 & 4.30 & 0.995 & 0.997 & - & - & -  \\
\midrule
\multirow{6}{*}{\textbf{Acoustic Token} (Reconstruction Only)} & EnCodec ($Q=8$) & 6.00 & 1.53 & 1.33 & 2.83 & 0.946 & 0.979 & - & - & - \\
 & DAC (24kHz, $Q=8$) & 6.00 & 1.34 & 0.93  & 3.52 & 0.958 & 0.982 & - & - &-  \\
 & TiCodec ($Q=2$) & 1.50 & 3.31 & 1.51 & 2.00 & 0.898 & 0.905 & 2.62 & 0.642 & 0.886 
 % & 0.153 
 \\
 & SNAC (24kHz) & 0.98 & 2.25 & 1.48 & 2.23 & 0.914 & 0.952 & - & - & -\\
 & WavTokenizer (Small, $F=75$Hz) & 0.90 & 2.45 & 1.63 & 2.47 & 0.925 & 0.960 &  - & - & -\\
 & Stable-Codec ($Q=12$) & 0.70 & 4.94 & 1.73 & 2.16 & 0.917 & 0.889 & - & - & - \\
 \cmidrule(r){1-2}
\multirow{5}{*}{\textbf{Acoustic Token} (Semantic Distillation)} & SpeechTokenizer & 4.00 & 1.47 & 1.20 & 2.60 & 0.930 & 0.972 & - & - & - \\
& X-Codec (HuBERT LibriSpeech) & 4.00 & 1.27 & 1.49 & 2.82 & 0.905 & 0.971 & - & - & - \\
& Mimi & 1.10 & 2.44 & 1.68 & 2.27 & 0.917 & 0.938 & - & - & - \\
& LLM-Codec & 0.85 & 6.25 & 1.86 & 1.82 & 0.879 & 0.919 & - & - & -\\
& SemantiCodec ($F$=25Hz, $V$=$2^{13}$+$2^{15}$) & 0.70 & 3.44 & 2.28 & 1.75 & 0.866 &  0.930 & - & - & - \\
\cmidrule(r){1-2}
% \multirow{2}{*}{\textbf{Acoustic Token} (Disentanglement)} & FACodec & 4.80kbps & 1.44 & 2.95 & 1.82 & 0.891 &  0.877& 1.57 & 0.772 & 0.583 & 0.105 \\ 
\multirow{2}{*}{\textbf{Acoustic Token} (Disentanglement)} & {FACodec (with \textit{detail} codes)} & 4.80 & 1.37 & 1.02 & 2.91  & 0.954 & 0.971 & 1.57 & 0.773 & 0.583
% & 0.105 
\\ 
 % & LSCodec ($F=50$Hz) & 0.45kbps & 3.48 & 6.39 & 1.49 & 0.673 & 0.885 & 4.04 & 0.852 & 0.671 & 0.033 \\
 & LSCodec ($F=50$Hz) & 0.45 & 3.33 & 2.42 & 1.77 & 0.688 & 0.954 & 4.04 & 0.852 & 0.697 
 % & 0.033 
 \\ % self-prompt experiment
% \midrule
\cmidrule(r){1-2}
% \multirow{5}{*}{\textbf{Semantic Token} (SSL)} & vq-wav2vec & 1.80kbps & 2.88 & 12.32 & 1.23 & 0.763 & 0.843 & 2.86 & 0.798 & 0.613 & 0.045 \\
% \multirow{5}{*}{\textbf{Semantic Token} (SSL)} & vq-wav2vec (kmeans) & 1.80kbps & 2.89 & 17.51 & 1.21 &  0.747 & 0.877 & 3.27 & 0.857 & 0.577 & 0.020 \\
\multirow{5}{*}{\textbf{Semantic Token} (SSL)} & vq-wav2vec (k-means) & 1.80 & 2.81  & 2.73 & 1.49 & 0.795 & 0.940 & 3.27 & 0.857 & 0.718
% & 0.020 
\\
 % & wav2vec 2.0 (inner $Q$) & 0.90kbps & 3.74 & 6.20 & 1.36 & 0.682 & 0.827 & 4.18 & 0.721 &0.734  & 0.106 \\
 % & wav2vec 2.0 (Large, inner quantizer) & 0.90kbps & 3.79 & 6.82 & 1.37 & 0.669 & 0.879 & 4.40 & 0.814 & 0.723 & 0.065 \\
 & wav2vec 2.0 (Large, inner quantizer) & 0.90 & 3.24 & 2.92 & 1.52 & 0.680 & 0.947 & 4.40 & 0.814 & 0.759 
 % & 0.065 
 \\
 % & wav2vec 2.0 (L14+km2048) & 0.55kbps & 2.79 & 23.65 & 1.13 & 0.618 & 0.832 & 2.99 & 0.782 & 0.552  & 0.050 \\
 % & wav2vec 2.0 (Large, L14+km2048) & 0.55kbps & 2.76 & 25.34 & 1.14 & 0.599 & 0.883 & 2.81 & 0.880 & 0.402 & 0.003 \\
 & wav2vec 2.0 (Large, L14+km2048) & 0.55 & 2.51 & 9.57 & 1.20 & 0.630 & 0.933 & 2.81 & 0.880 & 0.492 
 % & 0.003 
 \\
 % & HuBERT (L24+km2048) & 0.55kbps &  2.09 & 26.93 & 1.13 & 0.614 & 0.832 & 1.92 & 0.818 & 0.289 & 0.014 \\
 & HuBERT (Large, L24+km2048) & 0.55 &  1.86 & 15.65 & 1.17 & 0.625 & 0.934 & 1.97 & 0.876 & 0.375
 % & 0.005
 \\
 % & WavLM (L24+km2048) & 0.55kbps &  1.69 &  27.46 & 1.12  & 0.616 & 0.821 &  1.81 & 0.803 & 0.305 & 0.018 \\ 
 % & WavLM (Large, L24+km2048) & 0.55kbps & 1.94 & 38.26 & 1.11 & 0.581 & 0.878 & 1.92 & 0.872 & 0.286 &  0.006 \\ 
 & WavLM (Large, L24+km2048) & 0.55 & 1.67 & 17.94 & 1.16 & 0.621 & 0.934 & 1.92 & 0.872 & 0.374
 % &  0.006 
 \\
 \cmidrule(r){1-2}
% \textbf{Semantic Token} (SSL+Perturb Invariance) & ContentVec (L12+km2048) & 0.55kbps &2.15 & 28.47 & 1.12 & 0.609& 0.818 & 2.87 & 0.807 & 0.312 & 0.011 \\ 
\textbf{Semantic Token} (SSL+Perturb Invariance) & ContentVec (L12+km2048) & 0.55 &2.09  & 18.88 &  1.15 &  0.613 & 0.921 &  2.21 & 0.869 & 0.348
% & 0.007 
\\ 
\cmidrule(r){1-2}
% \textbf{Semantic Token} (Supervised) & $\mathcal S^3$ Tokenizer ($F=50$Hz) & 0.60kbps & 2.35 & 21.80 & 1.20 & 0.637 & 0.890 & 2.52 & 0.868 & 0.540 & 0.022 \\
\textbf{Semantic Token} (Supervised) & $\mathcal S^3$ Tokenizer ($F=50$Hz) & 0.60 & 2.12 & 4.25 & 1.37 & 0.673 & 0.944 & 2.52 & 0.868 & 0.687
% & 0.022 
\\
\bottomrule
\end{tabular}
}
\end{table*}

To enable a fair comparison between acoustic and semantic tokens from a reconstruction perspective, we train a CTX-vec2wav$^\alpha$ vocoder~\cite{guo2024lscodec} for different semantic tokens on LibriTTS~\cite{libritts}.
This vocoder supplements the insufficient speaker timbre information in semantic tokens using continuous WavLM features extracted from the reference prompts.
% This approach enables semantic tokens to conveniently perform VC switching reference prompts.
This approach enables semantic tokens to perform voice conversion (VC) by switching reference prompts conveniently.
The training details follow \cite{du2024unicats}.
We compute several metrics for reconstruction ability:
\begin{itemize}
    \item \textbf{WER} (word error rate, in percentage) measures the content intelligibility of reconstructed speech. It is computed between ground truth texts and ASR-decoded transcriptions. We use NeMo-ASR\footnote{\scriptsize\url{https://huggingface.co/nvidia/stt_en_fastconformer_transducer_large}} here.
    \item \textbf{GPE} (gross pitch error, in percentage) measures the relative error percentage of pitch contours of the reconstructed speech compared to ground truth.
    \item \textbf{PESQ} (perceptual evaluation of speech quality) and \textbf{STOI} (short-time objective intelligibility) measure the speech quality from a signal perspective.
    \item \textbf{SECS} (speaker embedding cosine similarity) computes the cosine similarity of speaker embeddings outputted from a speaker verification model\footnote{\scriptsize\url{https://github.com/resemble-ai/Resemblyzer}}.
\end{itemize}
We use LibriTTS testset-B~ \cite{du2024unicats} as the test set for evaluations.
It contains 500 utterances from unseen speakers that sum up to about 1 hour.
We use the original utterance to provide timbre information when necessary, i.e. for TiCodec, FACodec, LSCodec and all semantic tokens\footnote{Note that there is potential of information leakage in this situation. An alternative strategy is to use a reference prompt from the same speaker to provide timbre information, then compute similarity between the generated utterance and the reference. 
However, this is rather ``same-speaker conversion'' instead of reconstruction, and the SECS values are not comparable to ordinary acoustic tokens that don't require additional speaker inputs.}.
All evaluation metrics are computed on 16kHz waveform for a fair comparison, and reconstructed waveforms with higher sampling rates are downsampled before evaluation.
% When reconstructed waveforms reside at a higher sampling rate, 
% This is because in downstream practices, the speaker reference must come from another registered utterance instead of the utterance to be reconstructed itself.
% The SECS values in this case shall not be compared with those tokens whose timbre information is kept, so we don't calculate SECS for these tokens.

We take representative works in each token category. 
When there are multiple feasible configurations for a model, we choose one typical configuration that balances bitrate and performance.
Note that different variants (especially on frame rate and number of quantizers) within the same model can lead to significant differences in reported metrics.
For SSL models like wav2vec 2.0, HuBERT and WavLM, we take the official ``Large'' model variant.
For wav2vec 2.0, we experiment with both its inner quantizer before the Transformer blocks and k-means clustering results on a specific Transformer layer.

The results are shown in Table \ref{tab:recon-vc}.
It is evident that acoustic tokens designed only for reconstruction can achieve decent speech quality, but still far from the state-of-the-art spectrogram-based vocoders because of higher compression rates.
Retaining good speech intelligibility (i.e. low WER) becomes particularly challenging when the frame rate is low. 
% It is particularly a challenging task to retain good intelligibility when the frame rate is low.
Acoustic tokens with semantic distillation can also achieve strong reconstruction quality.
Explicitly disentangled acoustic tokens may sacrifice some reconstruction performance metrics when the bitrate is extremely low.
% It is hard for semantic tokens to achieve the same level of acoustic reconstruction as acoustic tokens, which is revealed by GPE, PESQ, and STOI scores. 
Semantic tokens generally struggle to achieve the same level of acoustic reconstruction as acoustic tokens, as evidenced by lower GPE, PESQ, and STOI scores.
Notably, most semantic tokens included exhibit significant information loss in prosody as reflected by their GPE scores.
However, their WER scores remain comparable to acoustic tokens, despite having much lower bitrates.
This highlights the property that semantic tokens primarily retain content-related information rather than acoustic details.
% This is another demonstration of the property that semantic tokens retain mainly content-related information rather than acoustic details.
% It is also noteworthy that supervised semantic tokens such as $\mathcal S^3$ Tokenizer does not severely lose prosody information, although they are directly trained with the content-driven ASR criterion.


\subsection{Voice Conversion Analysis}
Despite the loss of acoustic information, a prominent advantage of semantic tokens over most acoustic tokens is their inherent timbre controllability.
Some acoustic tokens also have this ability, such as those with a global encoder like TiCodec and disentangled acoustic tokens, also possess this ability
To compare this ability across different tokens, we conduct voice conversion (VC) experiments using these tokens as the content from the source speech. 
We use the same source utterances in Section \ref{sec:reconstruction}, but assign a different target speaker for each source utterance as the prompt.
Then, we perform VC experiments on the 500 VC pairs.
In addition to WER and SECS, we also measure \textbf{P.Corr} as an objective metric for prosody preservation. 
This calculates the Pearson correlation coefficient between the pitch contours of the converted and source utterances. 
Note that this metric will be meaninglessly high if the VC similarity is low, i.e., when the source timbre is barely changed.
As the source utterances are the same as Section \ref{sec:reconstruction}, the WER numbers are directly comparable to those in the reconstruction experiments.
% Also, we calculate the difference of SECS in reconstruction and VC settings ($\Delta$SECS), which serves as an indicator of speaker information in the tokens.

The results presented in Table \ref{tab:recon-vc} indicate that semantic tokens often achieve much higher VC similarity compared to acoustic tokens. 
However, due to the substantial loss of prosody information, semantic tokens tend to have lower P.Corr scores than acoustic tokens.
Among the acoustic tokens capable of performing VC, explicit disentanglement methods, such as FACodec and LSCodec, outperform the implicit criterion employed in TiCodec.
% From the degree of speaker timbre leakage measured by $\Delta$SECS, semantic tokens also have advantages, particularly for HuBERT, WavLM, and ContentVec tokens.
It is also noteworthy that different tokenization settings in wav2vec 2.0 lead to drastically different outcomes. 
% Taking the output of its inner quantizer produces tokens that preserve prosody well but speaker information too, while clusters on its Transformer hidden embeddings exhibit the contrary characteristics.
Tokens generated from its inner quantizer preserve prosody well but also retain much speaker information, whereas clusters derived from its Transformer hidden embeddings exhibit the opposite characteristics.

Supervised semantic tokens from $\mathcal S^3$ Tokenizer also exhibit good intelligibility and VC ability.
Unlike HuBERT-style SSL models, this supervised tokenizer demonstrates better preservation of prosody both in reconstruction and VC settings.
Given that prosody and intonation are a crucial factors for ASR, it is reasonable to assume that the tokenizer's VQ module encodes some prosody information.
In contrast, while HuBERT-style SSL models do contain rich prosody information in their continuous features (e.g., as evidenced by good emotion recognition results~\cite{superb}), phonetic information is likely the primary component.
Therefore, offline clustering is prone to discard these prosody characteristics.
% \textcolor{red}{Supervised models.}

% \subsection{Analysis of Accents, Emotions and Non-Verbal Vocalizations}