

\subsection{Speech Token Vocoders}
\label{sec:vocoder}
Acoustic tokens are designed naturally with a decoder that outputs waveforms or spectrograms given tokens, but semantic tokens are not.
A necessary component for building a discrete token-based speech generation system with semantic tokens is the speech resynthesis model, or speech token vocoders.
Unlike traditional spectrogram-based vocoders~\cite{kong2020hifigan}, these vocoders receive discrete speech tokens as an input and reconstruct speech signals.
% They are especially important for semantic tokens since these tokens are not born with a reconstructive decoder compared to acoustic tokens.

Polyak et al.~\cite{polyak21} first explores speech resynthesis from discrete speech units by a HifiGAN~\cite{kong2020hifigan} augmented with discretized pitch units and speaker embedding inputs.
% Later, 
\IEEEpubidadjcol
The vec2wav vocoder in VQTTS~\cite{VQTTS} improves this vocoder by a Conformer~\cite{conformer} frontend module before HifiGAN generator.
Later, CTX-vec2wav~\cite{du2024unicats} proposes a position-agnostic cross-attention mechanism that effectively integrates timbre information from surrounding acoustic contexts without the need for pretrained speaker embeddings.
This makes it more timbre-controllable and suitable for zero-shot TTS and VC~\cite{li2024sef}.
Upon it, vec2wav 2.0~\cite{guo2024vec2wav} further advances the timbre controllability by SSL timbre features and adaptive activations, demonstrating a strong VC performance.

It is also feasible to apply diffusion or flow matching algorithms in token vocoders~\cite{tortoise,seedtts,du2024cosyvoice}.
There, the discrete tokens are treated as a condition for diffusion or flow matching to generate mel-spectrograms, and further converted to waveform by a pretrained mel vocoder.
Compared to training a token-to-wav vocoder in an end-to-end fashion, training a token-to-mel model is more convenient and does not need adversarial training. 
To better control timbre, a mask strategy is introduced into the training process where the model only computes loss on the un-masked part of spectrograms~\cite{du2024cosyvoice}.
During inference, spectrogram from speaker prompt conditions the generative process, which can be regarded as a form of ``in-context learning''.
% \textcolor{red}{Drawbacks?}
However, this requires tokens to be extracted from reference prompts before synthesis.
Also, inference efficiency may be compromised for better generation quality with multiple inference steps, and this method is only validated on massive amount of data currently.
