
\subsection{Length Reduction by Deduplication and Acoustic BPE}
\label{sec:dedup-bpe}
In most cases, the frame rate of discrete speech tokens ranges from 25 to 100Hz.
This leads to a huge discrepancy in lengths between speech representations and the underlying text modality.
% Most token sequences are tens of times longer than their corresponding phoneme sequences, not to mention the inner semantics.
This discrepancy has been a critical issue in building decoder-only TTS and other LM-based speech generation tasks, since longer sequences result in harder training and more unstable inference.
Therefore, length reduction techniques have been proposed to address this issue. 
These methods are inspired by language processing techniques and are thus more closely related to semantic tokens. 
Note that although these length reduction methods are universal across token types, they are less frequently applied to acoustic tokens. 
This is because acoustic tokens usually involve multiple VQ streams that complicate token-level operations.
% We will also show that single-codebook acoustic tokens have
% Hence we still 

A common approach to reduce token sequence lengths is deduplication~\cite{chang23b_interspeech,chang2024exploring}, i.e. removing the repeated consecutive tokens in a sequence.
Since the encoded continuous features are often close in consecutive frames where the speech dynamics do not change rapidly, they are likely to be quantized to the same unit.
% (e.g. in short segments inside a vowel)
Therefore, removing these redundant tokens can yield a more phonetic representation.
% Consider an original token stream $[a,a,a,b,b,a,c]$.
% After deduplication, the sequence becomes $[a,b,a,c]$, with corresponding durations $[3,2,1,1]$.
When the deduplicated tokens are used for generative modeling, a unit-to-speech model (similar to TTS) should be employed to upsample the tokens and convert them back to acoustic signals~\cite{lakhotia2021generative}.
% neural network duration predictor~\cite{ren2021fastspeech} is applied to predict the duration per token before converting back to signals.\textcolor{red}{TODO: add some works that use this}

Another popular approach to reducing the length of speech token sequences is acoustic byte-pair encoding (BPE)\footnote{The term ``acoustic'' here is used to distinguish it from traditional BPE applied to text tokens, rather than referring to ``acoustic tokens''.} or so-called subword modeling~\cite{hayashi2020discretalk,ren22_interspeech,chang23b_interspeech,shen2024acoustic,dekel24_interspeech}.
Similar to text BPE~\cite{Gage1994ANA}, acoustic BPE iteratively merges the two most frequent consecutive tokens and adds the merged token to the vocabulary.
After training on a corpus, a deterministic BPE mapping is established between original token combinations and the new vocabulary. 
This mapping enables a lossless compression algorithm, allowing tokens to be perfectly reconstructed after BPE decoding.
This operation can identify certain morphological patterns in token sequences, and offers a powerful way to remove redundant tokens.
% The encoded BPE tokens are used for downstream generation tasks, and original tokens are recovered by the deterministic BPE mapping before converting to signals.
In practice, acoustic BPEs on HuBERT semantic tokens has demonstrated significant speed and performance gains in ASR~\cite{chang23b_interspeech,chang2024exploring}, spoken language modeling~\cite{shen2024acoustic,dekel24_interspeech} and TTS~\cite{li24qa_interspeech,vectokspeech}.
% \textcolor{red}{TODO: other works that use acoustic BPEs, like \cite{vectokspeech}}.

Although deduplication is a simple and training-free method, acoustic BPE offers several unique advantages over it. First, acoustic BPE can identify redundant patterns that are not simply repetitions, whereas deduplication only removes exact duplicates. Additionally, deduplication discards the duration information of every token in the resulting sequence. This could be problematic for downstream tasks, as important rhythmic information may reside in the repetitions of tokens. In contrast, acoustic BPE preserves duration information by encoding repetitions of varying lengths into distinct new tokens. Furthermore, acoustic BPE is flexible in terms of target vocabulary size, which can be adjusted based on the desired length reduction ratio and downstream performance.


% \textcolor{red}{Maybe there can be a figure comparing BPE for acoustic and semantic tokens. Maybe there is more length reduction in semantic tokens than acoustic ones.}
\begin{figure*}
    \centering
    \includegraphics[width=0.24 \linewidth]{figs/single-acoustic-bpe.png}
    \includegraphics[width=0.24 \linewidth]{figs/single-semantic-bpe.png}
    \includegraphics[width=0.24 \linewidth]{figs/hubert-bpe.png}
    \includegraphics[width=0.24 \linewidth]{figs/multi-acoustic-bpe.png}
    \caption{BPE effect comparison of multiple tokens. The starting point of each line represents the original vocabulary size.}
    \label{fig:bpe-effect}
\end{figure*}
We visualize the length reduction effect of BPE on different speech tokens in Fig.\ref{fig:bpe-effect}. 
In addition to semantic tokens from various models and different k-means clusters in HuBERT, we also experiment with acoustic tokens.
For acoustic tokens with multiple codebooks, we apply BPE only to the first quantizer, in accordance with the current speech generation paradigm~\cite{valle}.
From Fig.\ref{fig:bpe-effect}, it is evident that different types of tokens exhibit very distinct patterns.
Semantic tokens generally show significant length reduction when applying BPE, especially for HuBERT models with fewer k-means clusters.
For single-codebook acoustic tokens, speaker-decoupled LSCodec tokens shows more reduction than general-purpose WavTokenizer and BigCodec.
For a single RVQ layer among multiple-codebook acoustic tokens, the reduction effect is also significant.
These findings suggest that the effect of BPE is negatively correlated with the information density in the speech tokens: the less information, the more length reduction achieved by BPE.
% As BPE is hard to apply on all the VQ streams for multiple-codebook tokens, the most reasonable way to apply it is on semantic tokens, which usually match the single-codebook requirement.
