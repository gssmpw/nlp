\vspace{-0.03in}
\section{Speech Tokenization Methods: Acoustic Tokens}
\label{sec:acoustic}

% \begin{table*}[]
% \centering
% \caption{A summary of famous acoustic speech tokens (neural speech codecs). 
% Italic ``\textit{C,T,U}'' denote CNN, Transformer or U-Net-based generator architecture in Fig.\ref{fig:generator}.
% Symbols `/' and `-' denote ``or'' and ``to'' for different model versions, and ``+'' means different configurations in different VQ streams in a single model. 
% $Q,F,V$ mean number of quantizers, frame rate and vocabulary size of each quantizer respectively. 
% For example, ``$Q=2$, $V$=8192+($2^{12}$-$2^{15}$)'' in SemantiCodec means one of the two VQ streams has 8192 possible codes, and the other can vary from $2^{12}$ to $2^{15}$ in different configurations.
% Bitrates are computed by $\frac1{1000}\sum_{i=1}^Q F_i\lceil \log_2 V_i\rceil$ kbps, without entropy coding. }
% \label{tab:acoustic-metadata}
% \resizebox{\textwidth}{!}{
% % if necessary, use "\makecell{A\\B}" to create line break in a cell
% \begin{tabular}{@{}lccccccc@{}}
% \toprule
% \textbf{{Acoustic Speech Tokens}} & \textbf{Model Framework} & \textbf{{Sampling Rate}} & \textbf{Quantization} & $Q$ & \textbf{$F$} & \textbf{$V$} & \textbf{{Bitrate (kbps)}}  \\ \midrule
% \multicolumn{6}{l}{\textbf{\textit{General-purpose acoustic tokens}}} \\
% SoundStream~\cite{zeghidour2021soundstream} & VQ-GAN (\textit{C}) & 24kHz & RVQ & max 24 & 75Hz & 1024 & max 18.00 \\
% EnCodec~\cite{encodec} & VQ-GAN (\textit{C})& 24kHz & RVQ & max 32 & 75Hz & 1024 & max 24.00  \\
% % LMCodec~\cite{LMCodec} & VQ-GAN & 16kHz & RVQ & max 24 & 50Hz & 1024 & max  \\
% TF-Codec~\cite{jiang2023latent} & VQ-GAN (\textit{C}) & 16kHz & GVQ & 3-32 & 25Hz & 512 / 1024 & 0.68-8.00 \\
% Disen-TF-Codec~\cite{jiang2023disentangled} & VQ-GAN (\textit{C}) & 16kHz & GVQ & 2 / 6 & 25Hz & 256 / 1024 & {0.40 / 1.50} \\
% AudioDec~\cite{audiodec} & VQ-VAE (\textit{C})+GAN& 48kHz & RVQ & 8 & 160Hz & 1024 & 12.80 \\
% HiFi-Codec\cite{yang2023hifi} & VQ-GAN (\textit{C})& 16 / 24kHz & GRVQ & 4 & 50-100Hz & 1024 & 2.00-4.00 \\
% DAC~\cite{kumar2024high} & VQ-GAN (\textit{C})& 44.1kHz & RVQ & 9 & 86Hz & 1024 & 7.74 \\
% LaDiffCodec~\cite{yang2024generative} & Latent diffusion & 16kHz & RVQ & 3 / 6 & 50Hz & 1024 & 1.50 / 3.00 \\
% {FreqCodec}~\cite{du2024funcodec} & VQ-GAN (\textit{C})& 16kHz & RVQ & max 32 & 50Hz & 1024 & max 16.00 \\
% TiCodec~\cite{ticodec} & VQ-GAN (\textit{C})& 24kHz & RVQ, GVQ & 1-4 & 75Hz & 1024 & 0.75-3.00  \\
% APCodec~\cite{APCodec} & VQ-GAN (\textit{C})& 48kHz & RVQ & 4 & 150Hz &1024 & 6.00 \\
% % FACodec~\cite{facodec} & 6 & 80Hz & 1024 & 4.80  \\
% % SSVC~\cite{SSVC} & 4 & 50Hz & 512 & 1.80 \\
% % SpeechTokenizer~\cite{zhang2024speechtokenizer} & 8 & 50Hz & 1024 & 4.00 \\
% {SRCodec~\cite{zheng2024srcodec}} & VQ-GAN (\textit{C}) & 16kHz & GRVQ   & 2-8 & 50Hz & 512+1024 & 0.95-3.80\\
% SQ-Codec~\cite{yang24l_interspeech} & VQ-GAN (\textit{C})& 16kHz & FSQ & 32 & 50Hz &19 & 8.00 \\
% Single-Codec~\cite{singlecodec} & VQ-GAN (\textit{T+C})& 24kHz & VQ & 1 & 23Hz & 8192 & 0.30  \\
% ESC~\cite{gu2024esc}  & VQ-GAN (\textit{U})& 16kHz & GVQ & max 18 & 50Hz & 1024 & max 9.00 \\
% CoFi-Codec~\cite{guo2024speaking} & VQ-GAN (\textit{U}) & 16kHz & GVQ & 3 & 8.33+25+50Hz & 16384 & 1.17 \\
% HILCodec~\cite{ahn2024hilcodec} & VQ-GAN (\textit{C})& 24kHz & RVQ & 2-12 & 75Hz & 1024 & 1.50-9.00 \\
% SuperCodec~\cite{zheng2024supercodec} & VQ-GAN (\textit{C})& 16kHz & RVQ & 2-12 & 50Hz & 1024 & 1.00-6.00\\
% SNAC~\cite{Siuzdak_SNAC_Multi-Scale_Neural_2024} & VQ-GAN (\textit{C})& 24kHz & RVQ & 3 &12+23+47Hz & 4096 & 0.98 \\
% dMel~\cite{bai2024dmel} & {Quantizer only} & 16kHz & FSQ & 80 & 80Hz & 16 & 25.60 \\
% WavTokenizer~\cite{ji2024wavtokenizer} & VQ-GAN (\textit{C})& 24kHz & VQ & 1 & 40 / 75Hz & 4096 & 0.48 / 0.90  \\
% BigCodec~\cite{xin2024bigcodec} & VQ-GAN (\textit{C})& 16kHz & VQ & 1 & 80Hz & 8192 & 1.04 \\
% LFSC~\cite{casanova2024low} & VQ-GAN (\textit{C})& 22.05kHz &  FSQ & 8 & 21.5Hz & 2016 & 1.89 \\
% NDVQ~\cite{niu2024ndvq} & VQ-GAN (\textit{C})& 24kHz &  RVQ & max 32 & 75Hz & 1024 & max 24.00 \\
% VRVQ~\cite{chae2024variable} & VQ-GAN (\textit{C})& 44.1kHz & RVQ & 8 & 86Hz & 1024 & {0.26 + max 6.89}\\
% % \textcolor{red}{SimVQ}~\cite{zhu2024addressing} & \\
% TS3-Codec~\cite{wu2024ts3codectransformerbasedsimplestreaming} & VQ-GAN (\textit{T})& 16kHz & VQ & 1 & 40 / 50Hz & $2^{16}$ / $2^{17}$ & 0.64-0.85 \\
% Stable-Codec~\cite{parker2024scalingtransformerslowbitratehighquality} & VQ-GAN (\textit{T})& 16kHz & FSQ  & 6 / 12 & 25Hz & 5 / 6 & 0.40 / 0.70 \\
% FreeCodec~\cite{zheng2024freecodecdisentangledneuralspeech} & VQ-GAN (\textit{C+T})& 16kHz & VQ & 1+1 & 50+7Hz & 256 & 0.45  \\
% \midrule
% \multicolumn{6}{l}{\textbf{\textit{Mixed-objective acoustic tokens: semantic distillation}}} \\
% {Siahkoohi et al.}~\cite{siahkoohi22_interspeech} & VQ-GAN (\textit{C})& 16kHz & RVQ & 2+1 / 2+2 / 6 & 25+50Hz & 64 & 0.60 / 0.90 / 1.80\\
% SpeechTokenizer~\cite{zhang2024speechtokenizer} & VQ-GAN (\textit{C})& 16kHz & RVQ & 8 & 50Hz & 1024 & 4.00 \\
% SemantiCodec~\cite{liu2024semanticodec} & Latent diffusion & 16kHz & VQ & 2 & 12.5-50Hz & {8192+($2^{12}$-$2^{15}$)} & 0.31-1.40 \\
% LLM-Codec~\cite{yang2024uniaudio15} & VQ-GAN (\textit{C})& 16kHz & RVQ & 3 & 8.33+16.67+33.33Hz & 3248+32000+32000& 0.85 \\
% X-Codec~\cite{ye2024codec} & VQ-GAN (\textit{C})& 16kHz & RVQ & max 8 & 50Hz & 1024 & max 4.00 \\
% SoCodec~\cite{guo2024socodec} & VQ-GAN (\textit{C})& 16kHz & GVQ & 1 / 4 / 8 & 25 / 8.3 / 4.2Hz & 16384 & 0.35 / 0.47 \\
% Mimi~\cite{kyutai2024moshi} & VQ-GAN (\textit{C+T})& 24kHz & RVQ & 8 & 12.5Hz & 2048 & 1.10 \\
% \midrule
% \multicolumn{6}{l}{\textbf{\textit{Mixed-objective acoustic tokens: disentanglement}}} \\
% SSVC~\cite{SSVC} & VQ-GAN (\textit{C})& 24kHz & RVQ & 4 & 50Hz & 512 & 1.80 \\
% PromptCodec~\cite{pan2024promptcodec} & VQ-GAN (\textit{C})& 24kHz & GRVQ & 1-4 & 75Hz & 1024 & 0.75-3.00 \\
% FACodec~\cite{facodec} & VQ-GAN (\textit{C})& 16kHz & RVQ & 1+2+3 & 80Hz & 1024 & 4.80 \\
% LSCodec~\cite{guo2024lscodec} & VQ-VAE (\textit{C+T})+GAN& 24kHz &  VQ & 1 & 25 / 50Hz & 1024 / 300& 0.25 / 0.45 \\
% SD-Codec~\cite{bie2024learning} & VQ-GAN (\textit{C})& 16kHz &  RVQ & 12 & 50Hz & 1024 & 6.00 \\
% \bottomrule
% \end{tabular}
% }
% % \vspace{-0.15in}
% \end{table*}


\begin{table*}[]
\centering
\caption{A summary of famous acoustic speech tokens (neural speech codecs). 
Italic ``\textit{C,T,U}'' denote CNN, Transformer or U-Net-based generator architecture in Fig.\ref{fig:generator}.
Symbols `/' and `-' denote ``or'' and ``to'' for different model versions, and ``+'' means different configurations in different VQ streams in a single model. 
$Q,F,V$ mean number of quantizers, frame rate and vocabulary size of each quantizer respectively. 
For example, ``$Q=2$, $V$=8192+($2^{12}$-$2^{15}$)'' in SemantiCodec means one of the two VQ streams has 8192 possible codes, and the other can vary from $2^{12}$ to $2^{15}$ in different configurations.
Bitrates are computed by $\frac1{1000}\sum_{i=1}^Q F_i\lceil \log_2 V_i\rceil$ kbps, without entropy coding. }
\label{tab:acoustic-metadata}
\resizebox{\textwidth}{!}{
% if necessary, use "\makecell{A\\B}" to create line break in a cell
\begin{tabular}{@{}lccccccc@{}}
\toprule
\textbf{{Acoustic Speech Tokens}} & \textbf{Model Framework} & \textbf{\makecell{Sampling\\Rate (kHz)}} & \textbf{\makecell{Quantization\\Method}} & \textbf{$Q$} & \textbf{$F$ (Hz)} & \textbf{$V$} & \textbf{{Bitrate (kbps)}}  \\ \midrule
\multicolumn{6}{l}{\textbf{\textit{General-purpose acoustic tokens}}} \\
SoundStream~\cite{zeghidour2021soundstream} & VQ-GAN (\textit{C}) & 24 & RVQ & max 24 & 75 & 1024 & max 18.00 \\
EnCodec~\cite{encodec} & VQ-GAN (\textit{C})& 24 & RVQ & max 32 & 75 & 1024 & max 24.00  \\
TF-Codec~\cite{jiang2023latent} & VQ-GAN (\textit{C}) & 16 & GVQ & 3-32 & 25 & 512 / 1024 & 0.68-8.00 \\
Disen-TF-Codec~\cite{jiang2023disentangled} & VQ-GAN (\textit{C}) & 16 & GVQ & 2 / 6 & 25 & 256 / 1024 & {0.40 / 1.50} \\
AudioDec~\cite{audiodec} & VQ-VAE (\textit{C})+GAN& 48 & RVQ & 8 & 160 & 1024 & 12.80 \\
HiFi-Codec\cite{yang2023hifi} & VQ-GAN (\textit{C})& 16 / 24 & GRVQ & 4 & 50-100 & 1024 & 2.00-4.00 \\
DAC~\cite{kumar2024high} & VQ-GAN (\textit{C})& 44.1 & RVQ & 9 & 86 & 1024 & 7.74 \\
LaDiffCodec~\cite{yang2024generative} & Latent diffusion & 16 & RVQ & 3 / 6 & 50 & 1024 & 1.50 / 3.00 \\
{FreqCodec}~\cite{du2024funcodec} & VQ-GAN (\textit{C})& 16 & RVQ & max 32 & 50 & 1024 & max 16.00 \\
TiCodec~\cite{ticodec} & VQ-GAN (\textit{C})& 24 & RVQ, GVQ & 1-4 & 75 & 1024 & 0.75-3.00  \\
APCodec~\cite{APCodec} & VQ-GAN (\textit{C})& 48 & RVQ & 4 & 150 &1024 & 6.00 \\
SRCodec~\cite{zheng2024srcodec} & VQ-GAN (\textit{C}) & 16 & GRVQ   & 2-8 & 50 & 512+1024 & 0.95-3.80\\
SQ-Codec~\cite{yang24l_interspeech} & VQ-GAN (\textit{C})& 16 & FSQ & 32 & 50 &19 & 8.00 \\
Single-Codec~\cite{singlecodec} & VQ-GAN (\textit{T+C})& 24 & VQ & 1 & 23 & 8192 & 0.30  \\
ESC~\cite{gu2024esc}  & VQ-GAN (\textit{U})& 16 & GVQ & max 18 & 50 & 1024 & max 9.00 \\
CoFi-Codec~\cite{guo2024speaking} & VQ-GAN (\textit{U}) & 16 & GVQ & 3 & 8.33+25+50 & 16384 & 1.17 \\
HILCodec~\cite{ahn2024hilcodec} & VQ-GAN (\textit{C})& 24 & RVQ & 2-12 & 75 & 1024 & 1.50-9.00 \\
SuperCodec~\cite{zheng2024supercodec} & VQ-GAN (\textit{C})& 16 & RVQ & 2-12 & 50 & 1024 & 1.00-6.00\\
SNAC~\cite{Siuzdak_SNAC_Multi-Scale_Neural_2024} & VQ-GAN (\textit{C})& 24 & RVQ & 3 &12+23+47 & 4096 & 0.98 \\
% dMel~\cite{bai2024dmel} & {Quantizer only} & 16 & FSQ & 80 & 80 & 16 & 25.60 \\
WavTokenizer~\cite{ji2024wavtokenizer} & VQ-GAN (\textit{C})& 24 & VQ & 1 & 40 / 75 & 4096 & 0.48 / 0.90  \\
BigCodec~\cite{xin2024bigcodec} & VQ-GAN (\textit{C})& 16 & VQ & 1 & 80 & 8192 & 1.04 \\
LFSC~\cite{casanova2024low} & VQ-GAN (\textit{C})& 22.05 &  FSQ & 8 & 21.5 & 2016 & 1.89 \\
NDVQ~\cite{niu2024ndvq} & VQ-GAN (\textit{C})& 24 &  RVQ & max 32 & 75 & 1024 & max 24.00 \\
VRVQ~\cite{chae2024variable} & VQ-GAN (\textit{C})& 44.1 & RVQ & 8 & 86 & 1024 & {0.26 + max 6.89}\\
% \textcolor{red}{SimVQ}~\cite{zhu2024addressing} & \\
TS3-Codec~\cite{wu2024ts3codectransformerbasedsimplestreaming} & VQ-GAN (\textit{T})& 16 & VQ & 1 & 40 / 50 & $2^{16}$ / $2^{17}$ & 0.64-0.85 \\
Stable-Codec~\cite{parker2024scalingtransformerslowbitratehighquality} & VQ-GAN (\textit{T})& 16 & FSQ  & 6 / 12 & 25 & 5 / 6 & 0.40 / 0.70 \\
FreeCodec~\cite{zheng2024freecodecdisentangledneuralspeech} & VQ-GAN (\textit{C+T})& 16 & VQ & 1+1 & 50+7 & 256 & 0.45  \\
\midrule
\multicolumn{6}{l}{\textbf{\textit{Mixed-objective acoustic tokens: semantic distillation}}} \\
{Siahkoohi et al.}~\cite{siahkoohi22_interspeech} & VQ-GAN (\textit{C})& 16 & RVQ & 2+1 / 2+2 / 6 & 25+50 & 64 & 0.60 / 0.90 / 1.80\\
SpeechTokenizer~\cite{zhang2024speechtokenizer} & VQ-GAN (\textit{C})& 16 & RVQ & 8 & 50 & 1024 & 4.00 \\
SemantiCodec~\cite{liu2024semanticodec} & Latent diffusion & 16 & VQ & 2 & 12.5-50 & {8192+($2^{12}$-$2^{15}$)} & 0.31-1.40 \\
LLM-Codec~\cite{yang2024uniaudio15} & VQ-GAN (\textit{C})& 16 & RVQ & 3 & 8.33+16.67+33.33 & 3248+32000+32000& 0.85 \\
X-Codec~\cite{ye2024codec} & VQ-GAN (\textit{C})& 16 & RVQ & max 8 & 50 & 1024 & max 4.00 \\
SoCodec~\cite{guo2024socodec} & VQ-GAN (\textit{C})& 16 & GVQ & 1 / 4 / 8 & 25 / 8.3 / 4.2 & 16384 & 0.35 / 0.47 \\
Mimi~\cite{kyutai2024moshi} & VQ-GAN (\textit{C+T})& 24 & RVQ & 8 & 12.5 & 2048 & 1.10 \\
X-Codec 2.0~\cite{ye2025llasa} & VQ-GAN (\textit{C+T}) & 16 & FSQ & 8 & 50 & 4 & 0.80 \\
\midrule
\multicolumn{6}{l}{\textbf{\textit{Mixed-objective acoustic tokens: disentanglement}}} \\
SSVC~\cite{SSVC} & VQ-GAN (\textit{C})& 24 & RVQ & 4 & 50 & 512 & 1.80 \\
PromptCodec~\cite{pan2024promptcodec} & VQ-GAN (\textit{C})& 24 & GRVQ & 1-4 & 75 & 1024 & 0.75-3.00 \\
FACodec~\cite{facodec} & VQ-GAN (\textit{C})& 16 & RVQ & 1+2+3 & 80 & 1024 & 4.80 \\
LSCodec~\cite{guo2024lscodec} & VQ-VAE (\textit{C+T})+GAN& 24 &  VQ & 1 & 25 / 50 & 1024 / 300& 0.25 / 0.45 \\
SD-Codec~\cite{bie2024learning} & VQ-GAN (\textit{C})& 16 &  RVQ & 12 & 50 & 1024 & 6.00 \\
\bottomrule
\end{tabular}
}
\end{table*}

Acoustic tokens, also known as \textit{speech codecs}, refer to the discrete representations optimized mainly for signal compression and reconstruction.
The audio codec technology arises long ago.
Traditional codecs, including 
% AAC~\cite{bosi1997iso}, 
MP3~\cite{rfc5219}, Opus~\cite{Valin2012DefinitionOT} and EVS~\cite{dietz2015overview}, typically take advantage of signal processing algorithms to improve quality and lower the bitrate.

In the deep learning era, numerous codec models based on neural networks have emerged.
% These codec models typically have a encoder-decoder framework that involves a quantization module in the middle.
% These models typically train an encoder that compresses speech signals, and a decoder that recovers speech signals, with a quantizer between the two.
These models typically consist of an encoder that compresses speech signals and a decoder that reconstructs the speech signals, with a quantizer situated between the two.
The quantizer is also parameterized and jointly trained with the whole network in an end-to-end manner. 
% As the purpose of acoustic tokens is signal reconstruction, current acoustic tokens models almost all rely on generative adversarial network (GAN) training criterion, i.e. the encoder-decoder generator tries to fool a set of discriminators by making the reconstructed signals as close to the original as possible.
% Those VQ-VAE model with GAN training criterion is typically referred to as VQ-GAN~\cite{esser2021taming}.
The codebook indices produced by the quantizer are referred to as acoustic tokens.
To improve the representation ability of discrete VQ spaces and thus obtain better codec performance, RVQ, GVQ, GRVQ and FSQ tricks are commonly applied in the quantization module.
% FSQ has also been integrated in acoustic tokens.

We list the VQ method, number of quantizers $Q$, frame rate $F$, vocabulary size $V$ for each quantizer, and the resulting bitrate of existing neural acoustic speech tokens in Table.\ref{tab:acoustic-metadata}.

\subsection{Model Architectures}
\label{sec:acoustic-arch}

\begin{figure}
    \centering
    % \includegraphics[width=0.8\linewidth]{figs/acoustic1.png}
    % \includegraphics[width=0.8\linewidth]{figs/acoustic2.png}
    \includegraphics[width=0.9\linewidth]{figs/acoustic.png}
    \caption{Neural architectures of acoustic tokens.
    % Upper: \textbf{VQ-GAN} type where the quantization module is placed between an encoder and a decoder; Bottom: \textbf{latent diffusion} type where quantized tokens condition the diffusion process towards a latent space learned by an autoencoder.
    Note that inputs and outputs can be waveforms, frequency-domain features or even SSL features depending on purpose and design.}
    \label{fig:acoustic-paradigms}
\end{figure}


Although acoustic codec models differ from one to one regarding their purposes, most of them share a similar encoder-quantizer-decoder framework.
With audio clip $\bm x$ that can either be time-domain sampling points, frequency-domain features or even other machine learning features, an encoder $f_\theta(\cdot)$ transforms it to $f_\theta(\bm x)$ in a continuous latent vector space. 
% For waveform inputs, $f_\theta(\cdot)$ will also downsample
The encoder $f_\theta(\cdot)$ will usually perform downsampling to reduce the temporal length of the input signals, especially for waveform inputs.
A VQ module $q_\phi(\cdot)$ discretizes $f_\theta(\bm x)$ into tokens and corresponding codebook vectors $\bm c$.
A decoder $g_\psi(\cdot)$ then uses $\bm c$ to reconstruct $\hat {\bm x}$, and a certain distance metric of $d(\bm x, \hat{\bm x})$ is usually optimized.
There are two major paradigms for designing the encoder, decoder, and quantizers, which can be summarized as diagrams in Fig.\ref{fig:acoustic-paradigms}.
% \textcolor{red}{Maybe we can merge U-Net into VQ-GAN, since they have nothing different in essence.}

\begin{figure}
    \centering
    % \includegraphics[width=0.8\linewidth]{figs/CNN.png}
    % \includegraphics[width=0.8\linewidth]{figs/transformer.png}
    % \includegraphics[width=0.7\linewidth]{figs/unet.png}
    \includegraphics[width=0.9\linewidth]{figs/acoustic-archs.png}
    \caption{Major generator (VQ-VAE) architectures of VQ-GAN-based acoustic tokens. 
    % Upper: \textbf{CNN-based}; Middle: \textbf{Transformer-based}; Bottom: \textbf{U-Net-based}. 
    ``Q.'' and ``Trans'' are short for quantizer and Transformer, respectively.}
    \label{fig:generator}
    \vspace{-0.2in}
\end{figure}

\subsubsection{VQ-GAN}
VQ-GAN~\cite{esser2021taming} is a very commonly adopted framework of acoustic tokens that trains a VQ-VAE with GAN objectives. 
Besides the original reconstruction and VQ objectives in a VQ-VAE, VQ-GAN uses discriminators $d_\xi(\bm x, \hat{\bm x})$ to distinguish real and reconstructed data that adversarially train the generator network composed of $f_\theta,q_\phi$, and $g_\psi$. In acoustic tokens, there are usually multiple discriminators, e.g. multi-resolution and multi-scale STFT discriminators from the neural vocoder researches~\cite{kumar2019melgan,jang21_interspeech}.
The generator architecture of VQ-GAN-based acoustic tokens has multiple choices, with the three most representative ones visualized in Fig.\ref{fig:generator}: CNN-based, Transformer-based, and U-Net-based.

The CNN-based generator is the most widely used architecture so far in acoustic tokens.
SoundStream~\cite{zeghidour2021soundstream} and EnCodec~\cite{encodec} are two famous early neural acoustic tokens that operate in an end-to-end VQ-GAN manner.
% SoundStream is also the basis for Lyra V2 codec\footnote{\url{https://github.com/google/lyra}}.
They receive time-domain waveforms as inputs and directly reconstruct waveforms.
Their encoder and decoder have a mirrored architecture to perform down and up-samplings.
In SoundStream, the encoder and decoder are purely constructed by convolutional neural networks (CNNs) while EnCodec augments them with an LSTM.
The CNN encoder down-samples the waveform to a high-dimensional embedding sequence, whose frame rate is determined by the sampling rate, CNN kernel sizes and strides at a fixed ratio.
The continuous embeddings are passed to an RVQ quantizer, and the quantized vectors are summed before being transformed to the waveform domain by the CNN decoder.
% Multi-resolution and multi-scale STFT discriminators are applied to distinguish between real and reconstructed speech.
The training criteria include reconstruction loss (in the time and frequency domain), adversarial loss, feature matching loss, and quantization losses for RVQ layers.
To allow for a flexible choice of bitrates, structured dropout is adopted where the number of codebooks in the RVQ module can be randomly chosen~\cite{zeghidour2021soundstream}, such that only a portion of quantizers in front are activated during training.
The acoustic tokens can consequently reside in variable bitrates depending on the chosen number of RVQ quantizers.
The inputs and outputs of the codec model can also be frequency-domain features like magnitude and phase spectra for reducing computation burden~\cite{du2024funcodec}.
There, the convolution kernels are typically 2D instead of 1D in the time-domain codecs.
% \textcolor{red}{Shall training losses be expressed in detail?}

Later, Transformers~\cite{transformer} have been adopted, e.g. in Single-Codec~\cite{singlecodec} and Mimi~\cite{kyutai2024moshi}.
They can be directly applied to frequency-domain inputs and outputs.
When operating on waveform-domain inputs or outputs, a CNN~\cite{kyutai2024moshi} or patchifying~\cite{wu2024ts3codectransformerbasedsimplestreaming,parker2024scalingtransformerslowbitratehighquality} operation is usually added before or after the Transformer blocks.
In Mimi, a shallow Transformer layer is added after the CNN-based encoder, and vice versa in its decoder.
Recently, some propose to use purely Transformer-based backbone and discard the CNN blocks, e.g. TS3-Codec~\cite{wu2024ts3codectransformerbasedsimplestreaming}.
As Transformers demonstrate superior modeling ability and scaling property, these works prove to outperform CNN-based codecs either with less computation~\cite{wu2024ts3codectransformerbasedsimplestreaming} or larger scale~\cite{parker2024scalingtransformerslowbitratehighquality}.
However, to ensure stream-ability, an attention mask should be employed~\cite{kyutai2024moshi}.
The encoder and decoder can also be designed to be different. 
For example, Single-Codec~\cite{singlecodec} uses Conformer~\cite{conformer} encoder and CNN decoder, while LSCodec~\cite{guo2024lscodec} uses the reverse configuration.

% While the most acoustic tokens contain only one quantizer (can be RVQ or GVQ though)
Though RVQ or GVQ is usually applied, most acoustic tokens contain only one quantization module as a whole.
However, there are also U-Net-based codecs where multiple quantizers are employed, e.g. CoFi-Codec~\cite{guo2024speaking} and ESC~\cite{gu2024esc}.
Each sub-encoder or decoder in the U-Net can be a CNN or Transformer.
This offers a more flexible control of the resolution of each VQ stream (Section \ref{sec:multi-resolution}).

It is also noteworthy that training a separate vocoder on top of existing acoustic tokens may result in improved audio quality than the original decoded outputs, since reconstructing waveform alone may be simpler than optimizing VQ representation and reconstruction at the same time.
This is exemplarily verified in AudioDec~\cite{audiodec}, MBD~\cite{san2023discrete} and Vocos~\cite{siuzdak2024vocos}.
Therefore, some acoustic tokens directly simplify the VQ-GAN training objective back to the original VQ-VAE, where the discrete tokens are obtained first by a simple reconstruction loss, and a vocoder is trained as an additional stage, like AudioDec~\cite{audiodec} and LSCodec~\cite{guo2024lscodec}.
These works are denoted as ``VQ-VAE+GAN'' in Table \ref{tab:acoustic-metadata}.
% \textcolor{red}{TODO: maybe add some explanation? Is this because vocoder is larger than decoder?}

\subsubsection{Latent diffusion} 
Different from VQ-GAN which uses GAN to generate waveforms or frequency features, some codecs also use latent diffusion~\cite{ho2020denoising,song2021scorebased,rombach2022high} as an alternative.
These codecs use discretized tokens as a condition to generate some latent acoustic space, e.g. from a pretrained continuous speech autoencoder.
Since diffusion models are strong generative models, acoustic tokens of this type does not need discriminators and adversarial training like VQ-GAN.
For instance, LaDiffCodec~\cite{yang2024generative} uses EnCodec tokens to condition the diffusion process from Gaussian noise to the latent space in a pretrained and frozen waveform autoencoder.
This is to bridge the gap of reconstruction quality between discrete and continuous representations and improve the codec performance compared to the original acoustic tokens.
Inference efficiency is a major concern of these models unless specifically optimized in limited sampling steps.
% By transforming the discrete codes from EnCodec to a properly-shaped continuous latent space, it improves the codec performance compare to original the EnCodec model.
% SemantiCodec~\cite{liu2024semanticodec} also belongs to this type, which will be detailed in Section \ref{sec:acoustic-distillation}.
% \textcolor{red}{Shall training losses be expressed in detail?}

% \paragraph{U-Net}\textcolor{red}{A separate paragraph or an affiliation in VQ-GAN?}

\vspace{-0.1in}
\subsection{General-Purpose Acoustic Tokens}

\label{sec:acoustic-general}

\subsubsection{Motivation}

In this section, we describe the most common type of neural acoustic tokens (speech codecs) that are designed only with the objective of speech signal reconstruction.
Those acoustic tokens are optimized towards better signal or perceptual quality under bitrates as low as possible. 

\subsubsection{Approaches}

% \textcolor{red}{Should we organize this subsection using architectures in Fig 1?}

% \textcolor{red}{HILCodec?}

\paragraph{Advanced VQ methods and model architectures}

Based on SoundStream and EnCodec, more codecs with advanced VQ methods, network structure, or optimization strategies have been researched with depth.
% HiFi-Codec~\cite{yang2023hifi} applies GRVQ on codecs to reduce the number of codebooks.
% SRCodec~\cite{zheng2024srcodec} proposes a dual attention mechanism and a split residual VQ strategy, which can also be regarded as GRVQ with interactions between groups.
% , and also launches the AcademiCodec\footnote{\url{https://github.com/yangdongchao/AcademiCodec}} project to facilitate codec research.
As an example, DAC~\cite{kumar2024high} achieves remarkable reconstruction quality by adding periodic inductive bias, better discriminators, modified loss functions, and a better VQ mechanism from ViT-VQGAN~\cite{yu2022vectorquantized} to improve codebook usage. 
Specifically, it performs L2-normed code lookup in a low-dimensional space (e.g. 8 or 32) instead of a high-dimensional space like 1024.
% Its VQ tricks are reported to improve code usage.
Other architectural improvements include using frequency-domain inputs~\cite{APCodec,ai24b_interspeech,singlecodec}, variance-constrained residual blocks~\cite{ahn2024hilcodec}, multi-filter bank discriminator~\cite{ahn2024hilcodec}, selective down-sampling back-projection~\cite{zheng2024supercodec}, etc.
% APCodec~\cite{APCodec} uses amplitude and phase spectra as inputs and outputs of the VQ-GAN codec model.
% Its encoder and decoder are based on an improved ConvNeXt v2 network~\cite{woo2023convnext}, and it achieves fast and low-latency compression for 48kHz audio.
% Based on APCodec, \cite{ai24b_interspeech} later reduces the necessary bitrate to 1kbps for high sampling rate scenarios by introducing additional bandwidth reduction and recovery modules before and after VQ-GAN.
% HILCodec~\cite{ahn2024hilcodec} proposes spectrogram blocks, variance-constrained residual blocks and a multi-filter bank discriminator to achieve high fidelity and lightweight streaming codec.
% SuperCodec~\cite{zheng2024supercodec} improves the traditional CNN blocks in encoder and decoder into a selective down-sampling back-projection network for better performance.

% ~\cite{kyutai2024moshi} to improve subjective perception: 1) introducing causal Transformers to both its encoder and decoder; 2) not applying VQ with a certain probability; 3) pure adversarial training without reconstruction losses.
% Meanwhile, TF-Codec introduces learnable frequency input compression and bitrate-controllable quantization to optimize with varying bitrates.

Several training tricks are explored, such as not applying VQ with a certain probability and pure adversarial training proposed in Moshi~\cite{kyutai2024moshi}.
Also, the training of neural speech codecs does not need to be end-to-end, i.e. the learning of VQ representations and signal reconstruction can be separated.
\cite{audiodec,du2024apcodec+} adopt a two-stage training process that introduces adversarial losses and an additional vocoder after training only with metric losses, to achieve improved quality.
Additional training criteria around the VQ module are proposed for better VQ utilization, such as delicate code-vector replacement strategy, codebook balancing loss, and similarity loss between consecutive RVQ layers proposed in ERVQ~\cite{zheng2024ervq}.
% ERVQ~\cite{zheng2024ervq} proposes a more delicate code-vector replacement strategy and a codebook balancing loss to enhance the VQ usage.
% It also applies a similarity loss after consecutive RVQ layers to encourage each RVQ layer to focus on different speech features.

% It achieves low latency while improving quality.
% APCodec+~\cite{du2024apcodec+} introduces this two-stage training process into APCodec, and declares that using adversarial loss throughout the entire process yields better performance.

% \textcolor{red}{SRCodec}.

% Although the aforementioned works all use regular GVQ or RVQ quantizers, 
Other VQ methods besides GVQ or RVQ also exist in speech codecs.
NDVQ~\cite{niu2024ndvq} improves the capacity of RVQ space by changing codebook {vectors} to parameterized Gaussian {distributions}.
% Instead of quantizing the continuous input to the closest codebook entry in each RVQ layer, NDVQ performs quantization by choosing the mean and variance with the greatest probability density.
% A sample is then drawn from the chosen Gaussian distribution as the VQ output, with a reparameterization technique.
FSQ has also been introduced to several speech codecs, like SQ-Codec~\cite{yang24l_interspeech} where scalar rounding is applied to each of its 32-dimensional latent space.
Stable-Codec~\cite{parker2024scalingtransformerslowbitratehighquality} adopts FSQ in a Transformer-based architecture, exhibiting strong scalability to large model sizes up to 950M parameter count.
It also explores a flexible post-training quantization level adjustment technique and residual FSQ strategy.
% dMel~\cite{bai2024dmel} directly quantizes mel-filterbanks per dimension with evenly-paced boundaries between the minimum and maximum of filterbank values.
% This is similar to FSQ but is parameter-free.

Note that most acoustic tokens require multiple quantizers, but \textbf{single-codebook} codecs have also been explored.
Single-Codec~\cite{singlecodec} designs an encoder consisting of Conformer and bidirectional LSTM to better compress mel spectrogram inputs.
% It accomplishes codec using only a \textbf{single codebook}.
WavTokenizer~\cite{ji2024wavtokenizer} and BigCodec~\cite{xin2024bigcodec} further explores single-codebook codec modeling with better network designs or larger parameter count.
TS3-Codec~\cite{wu2024ts3codectransformerbasedsimplestreaming} adopts a fully Transformer design that leads to a better single-codebook codec with fewer computation overhead.
LSCodec~\cite{guo2024lscodec} also achieves single-codebook coding with speaker disentanglement (Section \ref{sec:acoustic-disen}).
These single-codebook codecs with remarkably low bitrates offer great benefit to downstream speech generation models on simplicity and efficiency.

\paragraph{Temporal redundancy reduction}

Instead of capturing all the information through VQ layers like the previously mentioned codecs, some researchers have attempted to reduce the redundant bitrate of time-varying VQ codes.
One reasonable method is to encode the global information in speech, e.g. speaker timbre and channel effects, by a global encoder instead of the time-varying codes.
% The global information in speech includes speaker identity, channel effect and so on, and it does not need to be repetitively encoded by time-varying discrete tokens.
% The global information in speech, which includes speaker identity, channel effects, and other attributes, does not need to be repetitively encoded by time-varying discrete tokens. 
Disen-TF-Codec~\cite{jiang2023disentangled} is the first to explore VQ-GAN codec models with an additional global encoder that aids the codec decoder. 
In Disen-TF-Codec, the global features are designed to be sequential to adapt to speaker changes during transmission.
In TiCodec~\cite{ticodec}, the global tokens are time-invariant and vector-quantized instead.
They are extracted from different segments of an utterance in conjunction with time-varying tokens.
% By this design, TiCodec tries to minimize the global information in its RVQ time-varying codes.
Similar global encoders are also seen in \cite{guo2024socodec,guo2024speaking,singlecodec}.
% In \cite{guo2024socodec,guo2024speaking}, an ECAPA-TDNN~\cite{desplanques20_interspeech} reference encoder is employed.
% The introduction of a global encoder also facilitates the development of single-codebook codecs, such as Single-Codec~\cite{singlecodec}.
FreeCodec~\cite{zheng2024freecodecdisentangledneuralspeech} further incorporates a prosody encoder~\cite{ren2022prosospeech} that compresses the low-frequency range of mel spectrograms into a low frame rate VQ sequence to assist in reconstruction.

% This shows potential for compression acoustic information to a \textbf{single codebook} without repetitive RVQ process.

Another typical example of temporal redundancy reduction is predictive coding, as seen in TF-Codec~\cite{jiang2023latent}.
This approach captures temporal-varying information in the latent space by autoregressive prediction, which significantly reduces redundancy and entropy in the residual part for  quantization.
LMCodec~\cite{LMCodec} employs autoregressive prediction from coarse codes (first RVQ levels) to fine codes (last RVQ levels)~\cite{borsos2023audiolm}, enabling the transmission of fewer codes.

\paragraph{Multi-resolution and variable-bitrate coding}
\label{sec:multi-resolution}

% Instead of uni-resolution tokens where all the quantizers share the same temporal frequency of typically 25-86Hz, it is reasonable to design multi-resolution codecs since there are simultaneously fast and slow information streams in speech.
Rather than relying solely on uni-resolution tokens, where all quantizers share the same temporal frequency, it is reasonable to design multi-resolution codecs, because speech contains both fast and slow information streams.
For instance, many vowels exhibit slowly changing characteristics, while events such as explosive consonants and background noises require fine-grained modeling. Therefore, incorporating multiple temporal resolutions in codecs is likely to reduce the necessary bitrate.
% Therefore, incorporating multiple temporal resolutions in codecs is likely to reduce the necessary bitrate.
% Lots of vowels exhibit slowly changing characteristics, while events like explosive consonants and background noises may require finer-grained modeling.
% Hence, designing multiple temporal resolutions in codec is likely to decrease the necessary bitrate.

% Multi-resolution acoustic tokens have been investigated in contrast to the previous uni-resolution tokens.
% In multi-resolution acoustic tokens, different token streams have different frame rates for modeling spoken information from coarse to fine.

% Siahkoohi et al.~\cite{siahkoohi22_interspeech} 
SNAC~\cite{Siuzdak_SNAC_Multi-Scale_Neural_2024} is a notable multi-resolution acoustic token.
It follows the DAC~\cite{kumar2024high} architecture, but in each RVQ layer, residuals are downsampled before codebook look-up and upsampled afterward.
This enables SNAC to have three RVQ streams at a frame rate of 12, 23, 47Hz respectively.
% For example, with a CNN downsampling factor of 512 on 24kHz waveforms, the first quantizer further downsamples the sequence by a factor of 4, and then upsamples the 12Hz quantized vectors by the same factor to compute quantization residuals.
% The downsampling factors for the second and third quantizers are 2 and 1, respectively.
% This design enables SNAC to outperform codecs with uni-resolution RVQ, especially under low bitrates.
Similarly, CoFi-Codec~\cite{guo2024speaking} achieves multi-resolution coding by GVQ quantizers within its U-Net-based architecture.
LLM-Codec~\cite{yang2024uniaudio15} also adopts this idea to achieve very low frame rates with semantic distillation (Section \ref{sec:acoustic-distillation}).
% , CoFi-Codec~\cite{guo2024speaking} uses a U-Net architecture, where each encoder is a CNN with a specific downsampling rate.
% The decoders follow a mirrored procedure, and quantizers are placed between each encoder-decoder pair with at a specific resolution.
% This results in a multi-resolution representation, where, at each scale, GVQ is applied to the residual between the encoder and decoder hidden embeddings.
% In contrast, ESC~\cite{gu2024esc} changes the frequency resolution in each layer rather than the time resolution.

% Apart from multiple temporal resolutions, it is also valuable to notice the different information intensities in different speech frames.
In addition to multiple temporal resolutions, it is also feasible to consider the varying information intensities across different speech frames. 
% Some frames carry critical information, while others may be less informative (e.g., silences). 
This observation motivates the design of codecs to allocate different numbers of quantizers for different speech frames.
% Some frames carry important information while others may be obscure (e.g. silences).
% This inspires codec to allocate different numbers of quantizers for different speech frames.
As an example, VRVQ~\cite{chae2024variable} automatically selects the number of RVQ quantizers per frame by a predictor that is jointly trained with the whole network.
% In VRVQ, a predictor receives the encoder's hidden embeddings and outputs an importance map for each frame ranging from 0 to 1.
% This importance map determines the number of quantizers $Q$ for each frame and masks the quantizers beyond the first $Q$ quantizers.
% Since this masking process is not differentiable, surrogate functions are introduced to train the importance map predictor.

\subsubsection{Challenges}
Despite the emergence of single-codebook and low-bitrate codecs~\cite{singlecodec,ji2024wavtokenizer,xin2024bigcodec,guo2024lscodec}, achieving ideal reconstruction quality with a highly limited VQ space remains a challenging problem. 
% Besides, as acoustic tokens try to encode all necessary information for signal recovery, they may be redundant and too complex for downstream modeling.
Additionally, as acoustic tokens aim to encode all necessary information for signal recovery, they may become redundant and overly complex for downstream modeling.
While scaling up the model size or switching to non-causal networks has been shown to improve performance~\cite{singlecodec,xin2024bigcodec,parker2024scalingtransformerslowbitratehighquality}, these approaches may also compromise streamability or efficiency.
Furthermore, simply introducing global encoders like \cite{jiang2023disentangled,ticodec,guo2024speaking} does not guarantee disentanglement (Section \ref{sec:acoustic-disen}) and may still result in redundancy within the time-varying codes.

\subsection{Acoustic Tokens with Semantic Distillation}
\label{sec:acoustic-distillation}

\begin{figure}
    \centering
    \includegraphics[width=0.8\linewidth]{figs/acoustic-distill1.png}
    \includegraphics[width=0.8\linewidth]{figs/acoustic-distill2.png}
    \includegraphics[width=0.8\linewidth]{figs/acoustic-distill3.png}
    \caption{Different semantic distillation methods in acoustic tokens. Gray color indicates frozen during training.}
    \label{fig:acoustic-distill}
    \vspace{-0.2in}
\end{figure}

\subsubsection{Motivation}
Acoustic tokens are a convenient choice for spoken language models, as they can be directly converted back to waveforms without the need for extra vocoders.
However, if reconstruction is the sole objective of these tokens, their representation space may become overly complex and overly focused on acoustic details, in contrast to natural language tokens that primarily carry semantic information.
A natural improvement is to incorporate speech semantic features either from speech self-supervised learning (SSL) models, supervised models, or even text transcriptions.
Since speech SSL models aim to capture high-level phonetic or semantic information without external supervision~\cite{mohamed2022self}, integrating SSL features does not impose additional data requirements for injecting semantic information into the training process. 
Acoustic tokens with criteria beyond reconstruction are sometimes referred to as having a ``mixed objective''~\cite{cui2024recent}.
Given that the primary purpose of these models remains acoustic reconstruction in these models, we continue to refer to them as acoustic tokens.
The process of introducing semantic information into acoustic tokens is termed \textbf{semantic distillation}, with approaches summarized in Fig. \ref{fig:acoustic-distill}.

\subsubsection{Approaches}

\paragraph{Semantic feature guidance} 
The earliest effort in semantic distillation is to guide some RVQ layers in acoustic tokens towards semantic features, which are typically SSL features. 
Since information in RVQ naturally follows a coarse-to-fine order, guiding early RVQ layers towards semantic-oriented features helps establish and reinforce a semantic-to-acoustic information hierarchy.
For example, SpeechTokenizer~\cite{zhang2024speechtokenizer} uses a HuBERT~\cite{hsu2021hubert} SSL model to guide the first RVQ layer in EnCodec.
This ensures that the first RVQ layer contains more semantic information, thereby pushing acoustic details to the subsequent RVQ layers. 
This distillation is implemented either by regressing the first RVQ output to continuous HuBERT embeddings or by classifying it into discrete HuBERT tokens.
LLM-Codec alternatively uses Whisper~\cite{whisper} and T5~\cite{raffel2020exploring} as semantic teachers.
Mimi~\cite{kyutai2024moshi} uses a WavLM~\cite{chen2022wavlm} teacher and applies distillation to a specialized VQ module rather than the first RVQ layer.
% uses a WavLM~\cite{chen2022wavlm} model as a semantic teacher and designs a specialized VQ module for distillation, rather than using the first RVQ layer. 
% It claims to achieve a better semantic-acoustic trade-off compared to forcing acoustic information into the residual of the semantic quantizer.
Since SSL feature guidance occurs only during the training stage, it does not incur additional inference costs.
It has been reported that TTS language models trained with such acoustic tokens exhibit better robustness than those with unguided tokens~\cite{zhang2024speechtokenizer}.

\paragraph{Fixed semantic codebook} A more direct approach to achieve semantic distillation is to integrate semantic knowledge into the codebook of quantizers. 
% The encoder is then tasked with transforming the original speech into this semantic codebook space, while the decoder must learn to recover acoustics from this semantic space and the residuals.
This forces the quantization space itself to be more semantic-related.
This method is proposed in LLM-Codec~\cite{yang2024uniaudio15} where all three RVQ codebooks are initiated from the token embedding module of LLaMa-2~\cite{touvron2023llama2} and remain frozen during training.
% where pretrained automatic speech recognition (ASR) model Whisper~\cite{whisper}, text language model T5~\cite{raffel2020exploring}, and the LLM LLaMa-2~\cite{touvron2023llama,touvron2023llama2} are employed as semantic teachers.
% LLM-Codec comprises three RVQ layers where all codebooks are initiated from the token embedding module of LLaMa-2 and remain frozen during training.
% Specifically, the first RVQ codebook is constructed by selecting common words and average their corresponding sub-word embeddings from LLaMa-2.
% The rest two codebooks directly utilize the entire vocabulary space of LLaMa-2.
% Input vector sequences are downsampled at different rates before entering the first and second quantizers.
% The outputs of T5 and Whisper encoders are used to guide the first and second RVQ layers, respectively.
This approach not only reduces the bitrate of the codec but also significantly enhances the semantic representation ability of LLM-Codec.

\paragraph{Semantic features as inputs or outputs} 
Semantic features can also be compressed together with the acoustic features. 
This requires the encoder and quantizer to construct a shared acoustic and semantic space that balances the two information sources. 
The first attempt in this direction is made in \cite{siahkoohi22_interspeech} where Conformer representations from a pretrained wav2vec 2.0~\cite{baevski2020wav2vec} are combined with CNN encoder outputs for quantization.
SemantiCodec~\cite{liu2024semanticodec} quantizes AudioMAE~\cite{huang2022masked} SSL features
% \footnote{In fact, a stack of discretized and continuous AudioMAE features.} 
without relying on acoustic inputs. 
The quantized SSL features then serve as a condition for acoustic reconstruction using latent diffusion, which resembles a vocoder that transforms semantic inputs into acoustic outputs.
% SoCodec~\cite{guo2024socodec} also directly quantizes HuBERT features and reconstruct, but incorporates a global acoustic condition to aid reconstruction.
% With a downsampling semantic encoder, it remarkably explores a frame shift up to 240ms.
Providing aligned phoneme sequences instead of SSL features to the quantizer has also shown benefits on reducing bitrates~\cite{du2024funcodec}.
% Additionally, it has also been reported to reduce bitrate when aligned phoneme sequences are added to the encoder output before RVQ~\cite{du2024funcodec}.
% The Mimi tokenizer, proposed in the speech-to-speech LLM Moshi~\cite{kyutai2024moshi}, relies on WavLM~\cite{chen2022wavlm} for semantic distillation. It also regresses the output of a VQ layer to WavLM embeddings, but separates this VQ layer with the rest RVQ layers, differently with SpeechTokenizer.

Moreover, semantic features can also serve as outputs, thereby reinforcing the constraint that semantic information be compressed into the discrete latent space.
% For instance, SoCodec quantizes HuBERT 
For instance, \cite{guo2024socodec,ye2024codec} combine hidden HuBERT embeddings with acoustic features before RVQ and jointly optimizes acoustic and semantic reconstruction objectives.
X-Codec 2.0~\cite{ye2025llasa} improves it by using w2v-BERT 2.0~\cite{barrault2023seamless} and FSQ.
% Then, these tokens can compress semantic features directly.

\subsubsection{Challenges}
Guiding part of the RVQ layers towards semantic features does not guarantee that acoustic information is encoded in the remaining layers, as shown by the degraded VC performance in SpeechTokenizer~\cite{zhang2024speechtokenizer}.
It may impose a greater challenge for the VQ layer to encode both acoustic and semantic information if semantic features serve as inputs as well.
% Fixing a semantic codebook could also negatively impact the acoustic reconstruction ability, since the VQ representation space is too restricted.
Additionally, fixing a semantic codebook could negatively impact acoustic reconstruction ability, as the VQ representation space becomes overly restricted.

\subsection{Acoustic Tokens with Disentanglement}
\label{sec:acoustic-disen}
\subsubsection{Motivation}
Another line of mixed-objective acoustic tokens is {disentanglement}.
A prominent research direction is the disentanglement of speaker timbre information, as this is a global trait among all the speech information aspects.
% It is redundant to encode speaker information into every token timestep, hence information in acoustic tokens will be more compact and the necessary bitrate will be lower if the global speaker timbre is removed.
Encoding speaker information into every token timestep is redundant; thus, removing the global speaker timbre can make the information in acoustic tokens more compact and reduce the necessary bitrate. 
Speaker-decoupled speech tokens can alleviate the modeling burden for downstream tasks. For example, a TTS model using these tokens can achieve independent control over prosody and speaker identity.
% A speaker-decoupled speech token will ease the modeling burden for downstream tasks.
% For example, a TTS model with those tokens can achieve prosody and speaker control independently.
The disentanglement of speaker timbre also enables an acoustic token to perform voice conversion (VC), as timbre from the target speaker can be easily combined with the speaker-agnostic content tokens from the source speech.

Note that in Section \ref{sec:acoustic}, it is mentioned that some codecs introduce a global encoder to reduce the necessary bitrate of time-variant tokens~\cite{jiang2023disentangled,ticodec,singlecodec,zheng2024freecodecdisentangledneuralspeech}.
They have already demonstrated some ability to decouple global speaker timbre and local contents, albeit in an \textbf{implicit} manner through the natural information bottleneck from VQ.
In this section, we elaborate on \textbf{explicit} methods, which involve specialized training techniques and criteria to achieve disentanglement.

\subsubsection{Approaches}
\paragraph{Gradient reversal layer (GRL)} The GRL technique~\cite{drl} is commonly used for disentanglement. Suppose speaker information needs to be disentangled, and a classifier (or speaker verifier, etc.) $s_\mu(\cdot)$ receives some latent feature $\bm h$ from the acoustic token to perform speaker discriminative tasks. 
GRL operates by negating the gradient sign before $s_\mu(\cdot)$, thereby forcing $\bm h$ to fool the speaker classifier while the classifier itself improves, similar to adversarial training.

SSVC~\cite{SSVC} is one of the pioneering efforts in this direction.
% , which is the basis for BASE-TTS~\cite{lajszczak2024base}.
SSVC attempts to decouple content and speaker representations from WavLM features.
The content branch is quantized via RVQ, and the speaker branch is trained using a contrastive loss to produce speaker embeddings.
Disentanglement is enforced by a GRL between the speaker embeddings produced from the speaker branch and the content representations.
% SSVC designs two coupled regressors from WavLM: a speaker regressor and a content regressor.
% These regressors are essentially attention modules on every WavLM layer.
% The speaker regressor is used to train a speaker extractor by contrastive loss to produce discriminative speaker embeddings.
% The output from the content regressor is quantized by RVQ before being combined with speaker embeddings and reconstructed into waveforms.
% SSVC trains a VQ-VAE on WavLM embeddings with RVQ, where the encoder is simply an attention module on every WavLM layer, and the decoder is a BigVGAN vocoder~\cite{lee2023bigvgan}.
% It jointly trains a speaker extractor based on the WavLM embeddings using contrastive loss, similar to GE2E~\cite{wan2018generalized} in speaker verification.
% The speaker extractor outputs discriminative speaker embeddings, which is fed into the BiVGAN vocoder in its VQ-VAE.
% Disentanglement is enforced by a GRL on a cosine distance loss between the speaker extractor outputs from the speaker regressor and content regressor.
Similarly, PromptCodec minimizes an SSIM loss~\cite{wang2004image} between content and speaker representations, with the help of a pretrained speaker verification model.

Such GRL technique is not limited to disentangling speaker timbre alone.
FACodec~\cite{facodec} employs supervised decoupling to factorize speech into speaker timbre, content, prosody, and acoustic detail information.
The timbre extractor in FACodec is optimized via a speaker classification loss.
% For prosody, content, and detail aspects, different RVQ modules are applied respectively before supervised decoupling.
For the other components -- prosody, content, and acoustic detail -- separate RVQ modules are applied prior to the supervised decoupling process.
For each component, some supervision signal with the desired information is applied, and GRL is employed to other non-related information components.
% the normalized F0. In the prosody branch, normalized F0 is predicted, and GRL is applied to the frame-aligned phonemes. Meanwhile, in the acoustic detail branch, GRL is performed using both phonemes and F0.
% The decoder of FACodec receives all four information branches and re-combines them to reconstruct speech.
These three quantized features are then combined before applying GRL with the speaker information. 
Finally, the decoder integrates all four information branches to reconstruct the speech signal.

\paragraph{Perturbation}
For speaker disentanglement, a more straightforward approach is to apply speaker timbre perturbations to speech signals and leverage the strong information bottleneck created by the discrete VQ module.
When the encoder is unable to learn sufficient timbre information, and the decoder is provided with prominent timbre, the bottleneck in the middle will naturally prevent timbre from being encoded~\cite{qian2019autovc}.
% LSCodec~\cite{guo2024lscodec} utilizes speaker disentanglement to achieve ultra-low bitrate.
This idea is adopted in LSCodec~\cite{guo2024lscodec} to achieve speaker decoupling and ultra-low-bitrate coding.
LSCodec leverages continuous WavLM features to represent speaker timbre.
% for its remarkable speaker verification ability~\cite{superb,knnvc}.
These features are fed to a Conformer-based decoder by position-agnostic cross attention~\cite{du2024unicats,li2024sef}.
A stretching-based speaker perturbation algorithm is applied to the input waveform to facilitate speaker disentanglement.
The training process of LSCodec involves multiple stages where a VQ module is injected after constructing a speaker-decoupled continuous space.
% : first, a speech VAE is trained obtain a preliminary speaker-decoupled continuous space.
% Subsequently, this continuous space is discretized into a VQ-VAE.
Through this approach, LSCodec achieves high-quality speech reconstruction and voice conversion using only a single codebook with very low bitrates.

\paragraph{Source separation}
Apart from the disentanglement of speaker timbre, source separation has also been explored in the context of acoustic tokens.
SD-Codec~\cite{bie2024learning} proposes to decouple different audio sources in the neural codec, like speech, music, and sound effects, by employing  multiple parallel RVQ modules.
This approach allows for more efficient and targeted processing of each audio component.

\subsubsection{Challenges}
The GRL technique for disentanglement inherently carries the risk of a more complex optimization trajectory.
Additionally, some disentanglement methods require supervised data~\cite{facodec}, which imposes a significant constraint.
Due to the intricate nature of speech informatics, current efforts are still suboptimal compared to semantic tokens, particularly in terms of VC performance~\cite{guo2024lscodec}.
