
\section{Speech Tokenization Methods: Semantic Tokens}

\label{sec:semantic}
Semantic tokens refer to discrete speech representations from discriminative or self-supervised learning (SSL) models.
While we use the term \textit{semantic tokens} to maintain consistency with prior works, some researchers recently argue that SSL features are more accurately described as \textit{phonetic} than \textit{semantic}~\cite{choi24b_interspeech} in nature.
Hence to clarify, in this review, semantic tokens should be more accurately defined as the complementary set of acoustic tokens, such that they are not primarily aimed at reconstruction purposes.
In practice, the vast majority of these tokens are designed for discriminative tasks and are believed to have a strong correlation with phonetic and semantic information~\cite{wells22_interspeech,mohamed2022self,sicherman2023analysing,yeh2024estimating}.

\subsection{Semantic Tokens from General-Purpose SSL}
\label{sec:semantic-general}
\subsubsection{Motivation}
% A large branch of semantic tokens come from speech SSL features. 
Speech SSL models have consistently outperformed many traditional methods in various speech tasks~\cite{superb,mohamed2022self}.
Their potential has been extensively mined in discriminative tasks such as automatic speech recognition (ASR)~\cite{wav2vec,vq-wav2vec,hsu2021hubert,zhang2020pushing}, automatic speaker verification (ASV)~\cite{chen2022wavlm,jung2024espnet,miara24_interspeech}, speech emotion recognition (SER)~\cite{morais2022speech,chen2022wavlm,MADANIAN2023200266,ma-etal-2024-emotion2vec} and speech translation (ST)~\cite{wu20g_interspeech,nguyen20_interspeech,babu22_interspeech}.
% \textcolor{red}{TODO: add citations on these tasks with SSL inputs.}
Discretized SSL tokens are initially favored for reducing computation costs and improving robustness against irrelevant information for ASR~\cite{chang23b_interspeech}.
As language models have gained increasing attention, these SSL tokens have been further explored in generative tasks such as TTS~\cite{VQTTS,kharitonov2023speak,vectokspeech} and SLM~\cite{lakhotia2021generative,borsos2023audiolm,hassid2024textually}.
This is because they can be considered high-level abstractions of speech semantics that are largely independent of acoustic details.
% \textcolor{red}{TODO: not finished. Perhaps should have a logic plan first.}

\begin{figure}
    \centering
    % \includegraphics[width=0.85\linewidth]{figs/semantic1.png}
    % \includegraphics[width=0.7\linewidth]{figs/semantic2.png}
    \includegraphics[width=0.99\linewidth]{figs/semantic.png}
    \caption{Representatives in different kinds of semantic tokens. 
    % Upper: semantic tokens from \textbf{general-purpose SSL models}; Middle: \textbf{perturbation-invariant SSL models}; Bottom: semantic tokens from \textbf{supervised models}. 
    ``Q.'' denotes quantizer, which can be optional (dotted line).}
    \vspace{-0.1in}
    \label{fig:semantic-types}
\end{figure}
\subsubsection{Approaches}

SSL models initiate the learning process by defining a pretext task which enables the model to learn meaningful representations directly from the data itself. 
Typical speech SSL models employ CNNs and Transformer encoders to extract deep contextual embeddings.
When it comes to semantic tokens, there are mainly two ways to extract those discrete tokens from an SSL model (see upper part of Fig.\ref{fig:semantic-types}):
\begin{itemize}[leftmargin=5mm]
    \item External quantization, like clustering or training a VQ-VAE. This refers to extracting continuous embeddings from a certain layer or multiple layers in a pretrained SSL model, and performing quantization manually.
    For example, a common semantic token is the HuBERT+kmeans units, where k-means clustering is performed on a HuBERT Transformer layer with a portion of training data~\cite{lakhotia2021generative,kharitonov-etal-2022-text}.
    It is also feasible to perform clustering on multiple layers~\cite{shi24h_interspeech,mousavi2024should}, or train a VQ-VAE on the SSL hidden embeddings~\cite{huang2023repcodec,wang2024maskgct}.
    \item When an SSL model contains an inner quantizer that is trained together with other network modules, its outputs can also be regarded as semantic tokens.
    Many SSL models involve quantizers to produce targets for their training objectives~\cite{vq-wav2vec,baevski2020wav2vec,chiu2022self,zhu2025muq}.
    This approach provides an efficient and effective way of extracting semantic tokens.
\end{itemize}
Note that for SSL models with an inner quantizer, it is still practical to perform external quantization on its continuous embeddings, like wav2vec 2.0~\cite{baevski2020wav2vec}.
However, these two methods -- internal and external quantization -- may result in different patterns of information exhibition, which we will investigate in Section \ref{sec:analysis}.

For general-purpose SSL models, there are different designs on the pretext task~\cite{mohamed2022self}.
Table \ref{tab:semantic-metadata} provides a high-level summary of well-known semantic tokens.

\paragraph{Contrastive} This type of speech SSL models aims to learn representations by distinguishing a target sample (positives) from distractors (negatives) given an anchor~\cite{mohamed2022self}.
They minimize the latent space similarity of negative pairs and maximize that of the positive pairs.
For semantic tokens, vq-wav2vec~\cite{vq-wav2vec} and wav2vec 2.0~\cite{baevski2020wav2vec} are two representative contrastive SSL models.
They involve a quantizer to produce localized features that is contrastively compared to contextualized continuous features.
Vq-wav2vec~\cite{vq-wav2vec} uses pure CNN blocks while wav2vec 2.0~\cite{baevski2020wav2vec} adopts a Transformer for stronger capacity.
Both use GVQ quantizers with two groups to expand the VQ space.
Wav2vec 2.0 has also been extended to massively multilingual versions~\cite{conneau21_interspeech,babu22_interspeech,pratap2024scaling}.

\paragraph{Predictive}
This type of speech SSL models incorporates an external target for prediction, either from signal processing features or another teacher network.
A popular line of work is HuBERT~\cite{hsu2021hubert}.
It takes raw waveforms as inputs, applies random masks on the hidden representations before Transformer contextual blocks, and then predicts k-means quantized targets from MFCC or another HuBERT teacher.
% It can take more self-iterations by using a trained HuBERT teacher model and applying k-means clustering as targets.
WavLM~\cite{chen2022wavlm} augments HuBERT by additional speaker and noise perturbations to achieve superior performance in more paralinguistic-related tasks.
There are no inner quantizers in both models, so external quantization like k-means clustering is necessary to obtain semantic tokens.
BEST-RQ~\cite{chiu2022self} changes the prediction target to the output of a random projection quantizer.
% Similar to acoustic tokens, training a VQ-VAE to compress continuous semantic features in a vector quantized space is also explored. \textcolor{red}{RepCodec~\cite{huang2023repcodec}, token in MaskGCT, etc.}
% Data2vec~\cite{baevski2022data2vec,baevski2023efficient} proposes a general teacher-student masked prediction framework the masked and original view of data are fed to the student and teacher respectively, and the student network predicts the teacher outputs. 
The next-token prediction criterion from language models (LMs) have also been adopted into speech SSL~\cite{turetzky2024last,han2024nest}, either with or without a pretrained text LM.
This method emphasizes the autoregressive prediction property of learned tokens that may be better suited for the LM use case.

\subsubsection{Challenges}
% 0. data
Firstly, SSL models typically require large amount of data to train, as indicated in Table \ref{tab:semantic-metadata}.
% 1. clustering problems
For SSL models without a built-in quantizer during pretraining, k-means clustering is a prevalent approach to obtain discrete units.
% However, since most SSL models work in a high-dimensional space (e.g. with 768 or 1024 dimensions), the space and time complexity of such k-means procedures are large.
However, given that most SSL models operate in high-dimensional spaces (e.g., with 768 or 1024 dimensions), the space and time complexity of k-means clustering are substantial. 
% The clustering result is sometimes unreliable because of the curse of dimensionality in the Euclidean space.
The clustering results can sometimes be unreliable due to the curse of dimensionality in Euclidean space.
% 2. Acoustics and reconstruction
Moreover, it is often reported, and will also be shown by experiments in Section \ref{sec:analysis}, that discretized SSL units lose much acoustic details after quantization~\cite{polyak21,sicherman2023analysing,mousavi2024dasb}.
Different clustering settings, such as the chosen layer and vocabulary size, can lead to different outcomes within a single model.
% 3. causality and stream-ability
Finally, since most SSL models utilize Transformer blocks, their causality and streaming ability are compromised.

\subsection{Semantic Tokens from Perturbation-Invariant SSL}
\label{sec:semantic-invariant}
\subsubsection{Motivation}
As SSL tokens feature semantic or phonetic information, a major concern is to improve the resistance against perturbations in the input signal.
This kind of invariance includes noise and speaker aspects that don't affect the contents of speech.
Noise invariance refers to the invariance against signal augmentations such as additive noise, reverberations, etc.
Speaker invariance aims to remove speaker information, similar to speaker-disentangled acoustic tokens.
% SSL semantic tokens with perturbation invariance are often obtained by training with explicit perturbations.
% Perturbations are often explicitly introduced in training of these perturbation-invariant SSL models.
In the training process, perturbations are often explicitly introduced in these perturbation-invariant SSL models.
The original and perturbed view of an utterance are both fed to the same network (or teacher and student networks), and an external loss to reduce the impact of perturbation is applied.
The middle part of Fig.\ref{fig:semantic-types} depicts a typical perturbation-invariant SSL model.

\subsubsection{Approaches}

% \textcolor{red}{Another way to organize this section is to first introduce noise and speaker perturbation methods, and then treat them like the same, and introduce contrastive, distribution-similarity, CTC respectively.}

\paragraph{Perturbations}
The perturbations can either be designed to augment the acoustics or alter the speaker timbre, depending on the objective of invariance.
These perturbations usually preserve temporal alignments, meaning that the perturbed utterance and the original one are strictly synchronized.
For noise-invariant SSL tokens, basic signal variations like time stretching, pitch shifting, additive noise, random replacing, reverberation, and SpecAugment~\cite{park2020specaugment} are commonly applied~\cite{gat2023augmentation,ccc-wav2vec2.0,messica2024nast,huang2022spiral}.
% ~\cite{park2020specaugment} is also used in \cite{huang2022spiral}.
Typical speaker timbre perturbations include formant and pitch scaling as well as random equalization~\cite{qian2022contentvec,chang23_interspeech,chang2024dc}.
In contrast, random time stretching is applied as speaker perturbation in \cite{hwang2024removing}, which alters the tempo in each random segment.

\paragraph{Contrastive-based Methods}
Contrastive loss is a common method to obtain perturbation-invariant representations.
In this context, the contrastive loss is a modified version of that used in wav2vec 2.0~\cite{baevski2020wav2vec}.
Given two embedding sequences derived from the original and perturbed utterances, assuming the perturbation preserves frame-wise alignment, the positive sample of an anchor is taken from the same position in the other utterance.
This is because the content remains unchanged by the perturbation, thus the same position of two representation sequences should encode the same information.
In noise-invariant models~\cite{huang2022spiral,ccc-wav2vec2.0}, negative samples are selected from the other utterance relative to the anchor.
However, in speaker-invariant models~\cite{qian2022contentvec,hwang2024removing}, negative samples are selected from the same utterance as the anchor.
Specifically, in \cite{hwang2024removing}, soft attention pooling is applied to create equal-length representation sequences from two utterances with different durations.
This approach forces SSL models to ignore acoustic differences and focus solely on the unperturbed content.

\paragraph{Distribution-based Methods}
Another method to achieve invariance is to minimize some distance metrics between the representations extracted from the original and perturbed utterances.
In existing perturbation-invariant SSL models, this is typically accomplished using a cross-entropy loss between the underlying distributions in the VQ module of the SSL model.
NAST~\cite{messica2024nast} trains a Gumbel-based VQ-VAE on HuBERT features and enforces similarity between the Gumbel distributions Eq.\eqref{eq:gumbel-softmax} derived from the original and augmented utterances.
Spin~\cite{chang23_interspeech} and DC-Spin~\cite{chang2024dc} explore a speaker-invariant clustering algorithm for HuBERT features.
Similar to NAST~\cite{messica2024nast}, Spin employs a cross-entropy loss to ensure that the distributions over codebook entries are similar between the original and perturbed utterances.
% Spin uses a distribution smoothing technique before pushing the distributions to be similar, thereby preventing collapse into a trivial solution~\cite{chang23_interspeech}.
This distribution-based approach forces the same content to be quantized to the same index regardless of acoustic conditions.
% DC-Spin~\cite{chang2024dc} uses Spin units to train a HuBERT model and extends the Spin algorithm to incorporate two VQ codebooks, both optimized with the same objective
% The auxiliary codebook is designed to be larger than the primary one, allowing for more fine-grained acoustic details
% Additionally, DC-Spin explores fine-tuning with mel reconstruction and supervised ASR, which are anticipated to further enhance speaker invariance.

\paragraph{CTC-based Methods}
Noise invariance can also be achieved like an ASR task with perturbed speech inputs.
As semantic tokens from SSL models are highly content-related, these tokens extracted from the original clean utterance can serve as some pseudo-label for a perturbed view.
% Normally, 
In \cite{gat2023augmentation}, a connectionist temporal classification (CTC)~\cite{ctc} loss is calculated between quantized tokens from the augmented signal and a pretrained HuBERT+kmeans pseudo-labels from the clean signal.
This pushes the quantized tokens to have the same phonetic structure with the pseudo-labels.

\subsubsection{Challenges}
While noise and speaker-invariance have emerged as promising approaches in semantic tokens, they currently rely on content-preserving perturbations that are typically hand-crafted.
Most existing methods have only been evaluated on small-scale data and models.
It also remains unclear how these methods will generally benefit generative tasks such as speech generation and spoken language modeling.

% Contrastive approaches are explored in \cite{huang2022spiral,ccc-wav2vec2.0}.
% There, the original and augmented utterances are fed to the same network (or the teacher and student respectively) to obtain two sequences of representations.
% The contrastive loss from wav2vec 2.0 is borrowed, but the positive and negative samples are taken from the other utterance instead of the same utterance.


% \paragraph{Speaker invariance}
% Common speaker perturbation in this line of work include 
% ContentVec~\cite{qian2022contentvec} and \cite{hwang2024removing} adopt a contrastive objective similar to noise invariance SSL.
% ContentVec chooses to base on the HuBERT architecture and take negative samples from the same utterance than the perturbed utterance.
% ContentVec also introduces a voice conversion module to provide teacher labels from another speaker, for further eliminating the speaker information.
% Hwang et al.~\cite{hwang2024removing}, instead, chooses the CPC framework~\cite{oord2018representation,wav2vec} and introduces variable-length soft-pooling.



\begin{table}[]
\centering
\caption{A high-level summary of famous semantic speech tokens. Notations follow Table.\ref{tab:acoustic-metadata}.
Symbol `/' denotes different versions. 
``Inner Quantizer'' refers to whether the model has a quantizer, or external quantization (e.g. clustering) must be performed.
$F$ denotes frame rate.
In case there are inner quantizers, $Q,V$ denote number of quantizers and vocabulary size for each quantizer, respectively.
% $Q$ denotes number of quantizers (if there are), and $F$ denotes frame rate.
``\textit{NR}.'' means not reported.
% \textcolor{red}{Shall we change this table? Should more info be included?}
}
\label{tab:semantic-metadata}
\resizebox{\columnwidth}{!}{
% {
\begin{tabular}{@{}lcccccc@{}}
\toprule
\textbf{\makecell{Semantic \\Speech Tokens}} & \textbf{\makecell{Criterion \\ / Objective}} & \textbf{\makecell{Training\\Data (h)}} & $F$ \textbf{(Hz)} & \textbf{{Inner Quantizer}} \\ \midrule
\multicolumn{5}{l}{\textbf{\textit{General-purpose self-supervised learning (SSL) models}}} \\
vq-wav2vec~\cite{vq-wav2vec} & Contrastive & 0.96k & 100 & GVQ, $Q=2,V=320$ \\
wav2vec 2.0~\cite{baevski2020wav2vec} & Contrastive & 60k & 50 & GVQ, $Q=2,V=320$ \\
XLSR-53~\cite{conneau21_interspeech} & Contrastive & 50k & 50 & GVQ, $Q=2,V=320$ \\
HuBERT~\cite{hsu2021hubert} & Predictive & 60k & 50 & No \\
WavLM~\cite{chen2022wavlm} & Predictive & 94k & 50 & No \\
BEST-RQ~\cite{chiu2022self} & Predictive & 60k & 25 & {No} \\ 
w2v-BERT~\cite{chung2021w2v} & {Predictive+Contrastive} & 60k & 50 & VQ, $Q=1,V=1024$ \\
w2v-BERT 2.0~\cite{barrault2023seamless} & {Predictive+Contrastive} & 4500k & 50 & GVQ, $Q=2,V=320$ \\
% data2vec 2.0~\cite{baevski2023efficient} & Predictive & 60k& 50Hz  & No \\
DinoSR~\cite{liu2024dinosr} & Predictive & 0.96k & 50 & VQ, $Q=8,V=256$ \\
NEST-RQ~\cite{han2024nest} & {Predictive} & 300k &  25 & {No} \\
LAST~\cite{turetzky2024last} & {Predictive} & \textit{NR.} & 50 & VQ, $Q=1,V=500$ \\
\midrule
\multicolumn{5}{l}{\textbf{\textit{SSL models with perturbation-invariance}}} \\
{Gat et al.~\cite{gat2023augmentation}} & Noise Invariance & 0.10k & 50 & VQ, $G=1,V=50$-$500$  \\
ContentVec~\cite{qian2022contentvec} & Speaker Invariance & 0.96k & 50 & No \\
SPIRAL~\cite{huang2022spiral} & Noise Invariance & 60k & 12.5Hz & No\\
CCC-wav2vec 2.0~\cite{ccc-wav2vec2.0} & Noise Invariance & 0.36k & 50 & GVQ, $G=2,V=320$ \\
Spin~\cite{chang23_interspeech} & Speaker Invariance & 0.10k & 50 & VQ, $Q=1,V=128$-$2048$\\
NAST~\cite{messica2024nast} & Noise Invariance & 0.96k & 50 & VQ, $Q=1,V=50$-$200$\\
DC-Spin~\cite{chang2024dc} & Speaker Invariance & 0.96k & 50 & VQ, $Q=2,V=(50$-$500)$+$4096$ \\
% \textcolor{red}{Hwang et al.~\cite{hwang2024removing}} & Speaker Invariance & 0.96k & \\
\midrule
\multicolumn{5}{l}{\textbf{\textit{Supervised models}}} \\
% Whisper~\cite{whisper} & Supervised ASR & 680k & 50Hz & No \\
$\mathcal S^3$ Tokenizer~\cite{du2024cosyvoice}  & Supervised ASR & 172k & 25 / 50  & VQ, $Q=1,V=4096$ \\
Zeng et al.~\cite{zeng2024scaling} & Supervised ASR & 90k & 12.5 & VQ, $Q=1,Q=16384$ \\
Du et al. \scriptsize{(CosyVoice 2)}~\cite{cosyvoice2} & Supervised ASR & 200k & 12.5 & FSQ, $Q=8,V=3$ \\
\bottomrule
\end{tabular}
}
\vspace{-0.15in}
\end{table}

\IEEEpubidadjcol

\subsection{Semantic Tokens from Supervised  Models}
\label{sec:semantic-supervised}
As representing semantic or phonetic information is the major purpose of semantic tokens, a more direct way to achieve this is through supervised learning.
A famous example shown at the bottom of Fig.\ref{fig:semantic-types} is the $\mathcal S^3$ Tokenizer from CosyVoice~\cite{du2024cosyvoice}.
It places a single-codebook VQ layer between two Transformer encoder modules and optimizes the network through an ASR loss similar to Whisper~\cite{whisper}.
The same method is adopted in \cite{zeng2024scaling,zeng2024glm} where the frame rate is further reduced to 12.5Hz.
CosyVoice 2~\cite{cosyvoice2} improves $\mathcal S^3$ Tokenizer by replacing plain VQ with FSQ for better codebook utilization.
Note that in this kind of supervised semantic tokens, it is the output of the VQ layer that serves as tokens.
This allows for more preservation of paralinguistic information than directly transcribing speech into text.
% Whisper~\cite{whisper}, on the other hand, needs an extra quantization step to produce discrete semantic tokens since it operates on a continuous embedding space.
These supervised tokenizers are trained on massive paired speech-text data, and have demonstrated rich speech content understanding capabilities~\cite{du2024cosyvoice,fang2024llamaomni}.
% citing llama-omni because it uses continuous whisper as speech encoder.

However, training these models is highly costly due to the heavy data demands.
Training with only the ASR task may still result in the loss of some prosody information.
Although \cite{cosyvoice2} has demonstrated that its supervised tokenizer trained on Chinese and English can also work in Japanese and Korean, it remains unclear how well these supervised tokenizers generalize to more unseen languages.
