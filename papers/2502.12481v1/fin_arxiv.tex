
\documentclass{article} %
\usepackage{iclr2025_conference,times}

%
%
\usepackage{color}
\usepackage{epsfig}
\usepackage{graphicx}

%
\usepackage{adjustbox}
\usepackage{array}
\usepackage{booktabs}
\usepackage{colortbl}
\usepackage{wrapfig}
\usepackage{hhline}
\usepackage{multirow}
%
\usepackage{wrapfig}
\usepackage{floatflt}

%
%
%
\usepackage{bm}
\usepackage{nicefrac}
\usepackage{microtype}

%
\usepackage{changepage}
\usepackage{extramarks}
\usepackage{fancyhdr}
\usepackage{lastpage}
\usepackage{setspace}
\usepackage{soul}
\usepackage{xspace}

%
%
%
%
%
%
%
%
%

%
%
\usepackage{enumerate}
%
\usepackage{enumitem}  %

%

%
\usepackage{makecell}

\usepackage{pifont} %
%

\usepackage{algorithm,algpseudocode}
\renewcommand{\algorithmicrequire}{\textbf{Input:}}
\renewcommand{\algorithmicensure}{\textbf{Output:}}
%
%
%
%

\usepackage[symbol]{footmisc}
\renewcommand{\thefootnote}{\fnsymbol{footnote}} %
%

\newcommand{\stderr}[1]{\scriptsize $\pm #1$}

%
\newcolumntype{L}[1]{>{\raggedright\let\newline\\\arraybackslash\hspace{0pt}}m{#1}}
\newcolumntype{C}[1]{>{\centering\let\newline\\\arraybackslash\hspace{0pt}}m{#1}}
\newcolumntype{R}[1]{>{\raggedleft\let\newline\\\arraybackslash\hspace{0pt}}m{#1}}

\newcommand{\xpar}[1]{\noindent\textbf{#1}\ \ }
\newcommand{\vpar}[1]{\vspace{3mm}\noindent\textbf{#1}\ \ }

%
\newcommand{\sect}[1]{Section~\ref{#1}}
\newcommand{\sectapp}[1]{Appendix~\ref{#1}}
%
\newcommand{\eqn}[1]{Equation~\ref{#1}}
\newcommand{\fig}[1]{Fig.~\ref{#1}}
\newcommand{\tbl}[1]{Table~\ref{#1}}
\newcommand{\alg}[1]{Algorithm.~\ref{#1}}

\newcommand{\degree}{\ensuremath{^\circ}\xspace}
\newcommand{\ignore}[1]{}
\newcommand{\norm}[1]{\lVert#1\rVert}
\newcommand{\fcseven}{$\mbox{fc}_7$}
%
%

%

%
%

\def\naive{na\"{\i}ve\xspace}
\def\Naive{Na\"{\i}ve\xspace}

\makeatletter
\DeclareRobustCommand\onedot{\futurelet\@let@token\@onedot}
\def\@onedot{\ifx\@let@token.\else.\null\fi\xspace}

\def\eg{e.g\onedot} \def\Eg{E.g\onedot}
\def\ie{i.e\onedot} \def\Ie{I.e\onedot}
\def\cf{\emph{c.f}\onedot} \def\Cf{\emph{C.f}\onedot}
\def\etc{etc\onedot}
\def\vs{\emph{vs}\onedot}
\def\wrt{w.r.t\onedot}
\def\dof{d.o.f\onedot}
\def\etal{et al\onedot}
\def\aka{a.k.a\onedot}
\makeatother

%
\definecolor{MyDarkBlue}{rgb}{0,0.08,0.8}
\definecolor{MyDarkGreen}{RGB}{45,155,45}
\definecolor{MyDarkRed}{rgb}{0.8,0.02,0.02}
\definecolor{MyOrange}{rgb}{1.0, 0.4, 0.2}
\definecolor{MyPurple}{RGB}{111,0,255}
\definecolor{MyRed}{rgb}{0.8,0.0,0.0}
\definecolor{MyGold}{rgb}{0.75,0.6,0.12}
\definecolor{MyDarkgray}{rgb}{0.66, 0.66, 0.66}

\newcommand{\showcomment}[3]{\colorbox{#1}{\textcolor{white}{#2}}~{\textcolor{#1}{#3}}}
\newcommand{\jw}[1]{\showcomment{MyDarkGreen}{Jiajun}{#1}}
\newcommand{\joy}[1]{\showcomment{MyPurple}{Joy}{#1}}
\newcommand{\todo}[1]{\showcomment{MyDarkBlue}{TODO}{#1}}
\newcommand{\model}{PHIER\xspace}
\newcommand{\modelfull}{Deep Schema Grounding\xspace}
\newcommand{\dataset}{VAD\xspace}
\newcommand{\datasetfull}{Visual Abstractions Dataset\xspace}
\newcommand{\cn}{\textcolor{MyRed}{[Citation!]}}
\newcommand{\xhdr}[1]{{\noindent \textbf{#1}}}

\newcommand{\red}[1]{\textcolor{red}{#1}}


\newcommand{\cmark}{\ding{51}}%
\newcommand{\xmark}{\ding{55}}%

\def\bR{\mathbb{R}}
\def\cL{\mathcal{L}}

\newcommand{\mysubsection}[1]{\vspace{-4pt}\subsection{#1}\vspace{-4pt}}
%
\newcommand{\myparagraph}[1]{\noindent\textbf{#1}}
\newcommand{\myitem}{\vspace{-5pt}\item}
\newcommand{\mybf}[1]{\vspace{4pt}\noindent\textbf{#1}}
\newcommand{\mycell}[1]{\begin{tabular}[t]{@{}l@{}l}#1\end{tabular}}
\newcommand{\mycellc}[1]{\begin{tabular}[t]{@{}c@{}c}#1\end{tabular}}

\usepackage[most]{tcolorbox}
 
%
%
%

\usepackage{amsmath,amsfonts,bm}

%
\newcommand{\figleft}{{\em (Left)}}
\newcommand{\figcenter}{{\em (Center)}}
\newcommand{\figright}{{\em (Right)}}
\newcommand{\figtop}{{\em (Top)}}
\newcommand{\figbottom}{{\em (Bottom)}}
\newcommand{\captiona}{{\em (a)}}
\newcommand{\captionb}{{\em (b)}}
\newcommand{\captionc}{{\em (c)}}
\newcommand{\captiond}{{\em (d)}}

%
\newcommand{\newterm}[1]{{\bf #1}}


%
\def\figref#1{figure~\ref{#1}}
%
\def\Figref#1{Figure~\ref{#1}}
\def\twofigref#1#2{figures \ref{#1} and \ref{#2}}
\def\quadfigref#1#2#3#4{figures \ref{#1}, \ref{#2}, \ref{#3} and \ref{#4}}
%
\def\secref#1{section~\ref{#1}}
%
\def\Secref#1{Section~\ref{#1}}
%
\def\twosecrefs#1#2{sections \ref{#1} and \ref{#2}}
%
\def\secrefs#1#2#3{sections \ref{#1}, \ref{#2} and \ref{#3}}
%
\def\eqref#1{equation~\ref{#1}}
%
\def\Eqref#1{Equation~\ref{#1}}
%
\def\plaineqref#1{\ref{#1}}
%
\def\chapref#1{chapter~\ref{#1}}
%
\def\Chapref#1{Chapter~\ref{#1}}
%
\def\rangechapref#1#2{chapters\ref{#1}--\ref{#2}}
%
\def\algref#1{algorithm~\ref{#1}}
%
\def\Algref#1{Algorithm~\ref{#1}}
\def\twoalgref#1#2{algorithms \ref{#1} and \ref{#2}}
\def\Twoalgref#1#2{Algorithms \ref{#1} and \ref{#2}}
%
\def\partref#1{part~\ref{#1}}
%
\def\Partref#1{Part~\ref{#1}}
\def\twopartref#1#2{parts \ref{#1} and \ref{#2}}

\def\ceil#1{\lceil #1 \rceil}
\def\floor#1{\lfloor #1 \rfloor}
\def\1{\bm{1}}
\newcommand{\train}{\mathcal{D}}
\newcommand{\valid}{\mathcal{D_{\mathrm{valid}}}}
\newcommand{\test}{\mathcal{D_{\mathrm{test}}}}

\def\eps{{\epsilon}}


%
\def\reta{{\textnormal{$\eta$}}}
\def\ra{{\textnormal{a}}}
\def\rb{{\textnormal{b}}}
\def\rc{{\textnormal{c}}}
\def\rd{{\textnormal{d}}}
\def\re{{\textnormal{e}}}
\def\rf{{\textnormal{f}}}
\def\rg{{\textnormal{g}}}
\def\rh{{\textnormal{h}}}
\def\ri{{\textnormal{i}}}
\def\rj{{\textnormal{j}}}
\def\rk{{\textnormal{k}}}
\def\rl{{\textnormal{l}}}
%
\def\rn{{\textnormal{n}}}
\def\ro{{\textnormal{o}}}
\def\rp{{\textnormal{p}}}
\def\rq{{\textnormal{q}}}
\def\rr{{\textnormal{r}}}
\def\rs{{\textnormal{s}}}
\def\rt{{\textnormal{t}}}
\def\ru{{\textnormal{u}}}
\def\rv{{\textnormal{v}}}
\def\rw{{\textnormal{w}}}
\def\rx{{\textnormal{x}}}
\def\ry{{\textnormal{y}}}
\def\rz{{\textnormal{z}}}

%
\def\rvepsilon{{\mathbf{\epsilon}}}
\def\rvtheta{{\mathbf{\theta}}}
\def\rva{{\mathbf{a}}}
\def\rvb{{\mathbf{b}}}
\def\rvc{{\mathbf{c}}}
\def\rvd{{\mathbf{d}}}
\def\rve{{\mathbf{e}}}
\def\rvf{{\mathbf{f}}}
\def\rvg{{\mathbf{g}}}
\def\rvh{{\mathbf{h}}}
\def\rvu{{\mathbf{i}}}
\def\rvj{{\mathbf{j}}}
\def\rvk{{\mathbf{k}}}
\def\rvl{{\mathbf{l}}}
\def\rvm{{\mathbf{m}}}
\def\rvn{{\mathbf{n}}}
\def\rvo{{\mathbf{o}}}
\def\rvp{{\mathbf{p}}}
\def\rvq{{\mathbf{q}}}
\def\rvr{{\mathbf{r}}}
\def\rvs{{\mathbf{s}}}
\def\rvt{{\mathbf{t}}}
\def\rvu{{\mathbf{u}}}
\def\rvv{{\mathbf{v}}}
\def\rvw{{\mathbf{w}}}
\def\rvx{{\mathbf{x}}}
\def\rvy{{\mathbf{y}}}
\def\rvz{{\mathbf{z}}}

%
\def\erva{{\textnormal{a}}}
\def\ervb{{\textnormal{b}}}
\def\ervc{{\textnormal{c}}}
\def\ervd{{\textnormal{d}}}
\def\erve{{\textnormal{e}}}
\def\ervf{{\textnormal{f}}}
\def\ervg{{\textnormal{g}}}
\def\ervh{{\textnormal{h}}}
\def\ervi{{\textnormal{i}}}
\def\ervj{{\textnormal{j}}}
\def\ervk{{\textnormal{k}}}
\def\ervl{{\textnormal{l}}}
\def\ervm{{\textnormal{m}}}
\def\ervn{{\textnormal{n}}}
\def\ervo{{\textnormal{o}}}
\def\ervp{{\textnormal{p}}}
\def\ervq{{\textnormal{q}}}
\def\ervr{{\textnormal{r}}}
\def\ervs{{\textnormal{s}}}
\def\ervt{{\textnormal{t}}}
\def\ervu{{\textnormal{u}}}
\def\ervv{{\textnormal{v}}}
\def\ervw{{\textnormal{w}}}
\def\ervx{{\textnormal{x}}}
\def\ervy{{\textnormal{y}}}
\def\ervz{{\textnormal{z}}}

%
\def\rmA{{\mathbf{A}}}
\def\rmB{{\mathbf{B}}}
\def\rmC{{\mathbf{C}}}
\def\rmD{{\mathbf{D}}}
\def\rmE{{\mathbf{E}}}
\def\rmF{{\mathbf{F}}}
\def\rmG{{\mathbf{G}}}
\def\rmH{{\mathbf{H}}}
\def\rmI{{\mathbf{I}}}
\def\rmJ{{\mathbf{J}}}
\def\rmK{{\mathbf{K}}}
\def\rmL{{\mathbf{L}}}
\def\rmM{{\mathbf{M}}}
\def\rmN{{\mathbf{N}}}
\def\rmO{{\mathbf{O}}}
\def\rmP{{\mathbf{P}}}
\def\rmQ{{\mathbf{Q}}}
\def\rmR{{\mathbf{R}}}
\def\rmS{{\mathbf{S}}}
\def\rmT{{\mathbf{T}}}
\def\rmU{{\mathbf{U}}}
\def\rmV{{\mathbf{V}}}
\def\rmW{{\mathbf{W}}}
\def\rmX{{\mathbf{X}}}
\def\rmY{{\mathbf{Y}}}
\def\rmZ{{\mathbf{Z}}}

%
\def\ermA{{\textnormal{A}}}
\def\ermB{{\textnormal{B}}}
\def\ermC{{\textnormal{C}}}
\def\ermD{{\textnormal{D}}}
\def\ermE{{\textnormal{E}}}
\def\ermF{{\textnormal{F}}}
\def\ermG{{\textnormal{G}}}
\def\ermH{{\textnormal{H}}}
\def\ermI{{\textnormal{I}}}
\def\ermJ{{\textnormal{J}}}
\def\ermK{{\textnormal{K}}}
\def\ermL{{\textnormal{L}}}
\def\ermM{{\textnormal{M}}}
\def\ermN{{\textnormal{N}}}
\def\ermO{{\textnormal{O}}}
\def\ermP{{\textnormal{P}}}
\def\ermQ{{\textnormal{Q}}}
\def\ermR{{\textnormal{R}}}
\def\ermS{{\textnormal{S}}}
\def\ermT{{\textnormal{T}}}
\def\ermU{{\textnormal{U}}}
\def\ermV{{\textnormal{V}}}
\def\ermW{{\textnormal{W}}}
\def\ermX{{\textnormal{X}}}
\def\ermY{{\textnormal{Y}}}
\def\ermZ{{\textnormal{Z}}}

%
\def\vzero{{\bm{0}}}
\def\vone{{\bm{1}}}
\def\vmu{{\bm{\mu}}}
\def\vtheta{{\bm{\theta}}}
\def\va{{\bm{a}}}
\def\vb{{\bm{b}}}
\def\vc{{\bm{c}}}
\def\vd{{\bm{d}}}
\def\ve{{\bm{e}}}
\def\vf{{\bm{f}}}
\def\vg{{\bm{g}}}
\def\vh{{\bm{h}}}
\def\vi{{\bm{i}}}
\def\vj{{\bm{j}}}
\def\vk{{\bm{k}}}
\def\vl{{\bm{l}}}
\def\vm{{\bm{m}}}
\def\vn{{\bm{n}}}
\def\vo{{\bm{o}}}
\def\vp{{\bm{p}}}
\def\vq{{\bm{q}}}
\def\vr{{\bm{r}}}
\def\vs{{\bm{s}}}
\def\vt{{\bm{t}}}
\def\vu{{\bm{u}}}
\def\vv{{\bm{v}}}
\def\vw{{\bm{w}}}
\def\vx{{\bm{x}}}
\def\vy{{\bm{y}}}
\def\vz{{\bm{z}}}

%
\def\evalpha{{\alpha}}
\def\evbeta{{\beta}}
\def\evepsilon{{\epsilon}}
\def\evlambda{{\lambda}}
\def\evomega{{\omega}}
\def\evmu{{\mu}}
\def\evpsi{{\psi}}
\def\evsigma{{\sigma}}
\def\evtheta{{\theta}}
\def\eva{{a}}
\def\evb{{b}}
\def\evc{{c}}
\def\evd{{d}}
\def\eve{{e}}
\def\evf{{f}}
\def\evg{{g}}
\def\evh{{h}}
\def\evi{{i}}
\def\evj{{j}}
\def\evk{{k}}
\def\evl{{l}}
\def\evm{{m}}
\def\evn{{n}}
\def\evo{{o}}
\def\evp{{p}}
\def\evq{{q}}
\def\evr{{r}}
\def\evs{{s}}
\def\evt{{t}}
\def\evu{{u}}
\def\evv{{v}}
\def\evw{{w}}
\def\evx{{x}}
\def\evy{{y}}
\def\evz{{z}}

%
\def\mA{{\bm{A}}}
\def\mB{{\bm{B}}}
\def\mC{{\bm{C}}}
\def\mD{{\bm{D}}}
\def\mE{{\bm{E}}}
\def\mF{{\bm{F}}}
\def\mG{{\bm{G}}}
\def\mH{{\bm{H}}}
\def\mI{{\bm{I}}}
\def\mJ{{\bm{J}}}
\def\mK{{\bm{K}}}
\def\mL{{\bm{L}}}
\def\mM{{\bm{M}}}
\def\mN{{\bm{N}}}
\def\mO{{\bm{O}}}
\def\mP{{\bm{P}}}
\def\mQ{{\bm{Q}}}
\def\mR{{\bm{R}}}
\def\mS{{\bm{S}}}
\def\mT{{\bm{T}}}
\def\mU{{\bm{U}}}
\def\mV{{\bm{V}}}
\def\mW{{\bm{W}}}
\def\mX{{\bm{X}}}
\def\mY{{\bm{Y}}}
\def\mZ{{\bm{Z}}}
\def\mBeta{{\bm{\beta}}}
\def\mPhi{{\bm{\Phi}}}
\def\mLambda{{\bm{\Lambda}}}
\def\mSigma{{\bm{\Sigma}}}

%
\DeclareMathAlphabet{\mathsfit}{\encodingdefault}{\sfdefault}{m}{sl}
\SetMathAlphabet{\mathsfit}{bold}{\encodingdefault}{\sfdefault}{bx}{n}
\newcommand{\tens}[1]{\bm{\mathsfit{#1}}}
\def\tA{{\tens{A}}}
\def\tB{{\tens{B}}}
\def\tC{{\tens{C}}}
\def\tD{{\tens{D}}}
\def\tE{{\tens{E}}}
\def\tF{{\tens{F}}}
\def\tG{{\tens{G}}}
\def\tH{{\tens{H}}}
\def\tI{{\tens{I}}}
\def\tJ{{\tens{J}}}
\def\tK{{\tens{K}}}
\def\tL{{\tens{L}}}
\def\tM{{\tens{M}}}
\def\tN{{\tens{N}}}
\def\tO{{\tens{O}}}
\def\tP{{\tens{P}}}
\def\tQ{{\tens{Q}}}
\def\tR{{\tens{R}}}
\def\tS{{\tens{S}}}
\def\tT{{\tens{T}}}
\def\tU{{\tens{U}}}
\def\tV{{\tens{V}}}
\def\tW{{\tens{W}}}
\def\tX{{\tens{X}}}
\def\tY{{\tens{Y}}}
\def\tZ{{\tens{Z}}}


%
\def\gA{{\mathcal{A}}}
\def\gB{{\mathcal{B}}}
\def\gC{{\mathcal{C}}}
\def\gD{{\mathcal{D}}}
\def\gE{{\mathcal{E}}}
\def\gF{{\mathcal{F}}}
\def\gG{{\mathcal{G}}}
\def\gH{{\mathcal{H}}}
\def\gI{{\mathcal{I}}}
\def\gJ{{\mathcal{J}}}
\def\gK{{\mathcal{K}}}
\def\gL{{\mathcal{L}}}
\def\gM{{\mathcal{M}}}
\def\gN{{\mathcal{N}}}
\def\gO{{\mathcal{O}}}
\def\gP{{\mathcal{P}}}
\def\gQ{{\mathcal{Q}}}
\def\gR{{\mathcal{R}}}
\def\gS{{\mathcal{S}}}
\def\gT{{\mathcal{T}}}
\def\gU{{\mathcal{U}}}
\def\gV{{\mathcal{V}}}
\def\gW{{\mathcal{W}}}
\def\gX{{\mathcal{X}}}
\def\gY{{\mathcal{Y}}}
\def\gZ{{\mathcal{Z}}}

%
\def\sA{{\mathbb{A}}}
\def\sB{{\mathbb{B}}}
\def\sC{{\mathbb{C}}}
\def\sD{{\mathbb{D}}}
%
%
\def\sF{{\mathbb{F}}}
\def\sG{{\mathbb{G}}}
\def\sH{{\mathbb{H}}}
\def\sI{{\mathbb{I}}}
\def\sJ{{\mathbb{J}}}
\def\sK{{\mathbb{K}}}
\def\sL{{\mathbb{L}}}
\def\sM{{\mathbb{M}}}
\def\sN{{\mathbb{N}}}
\def\sO{{\mathbb{O}}}
\def\sP{{\mathbb{P}}}
\def\sQ{{\mathbb{Q}}}
\def\sR{{\mathbb{R}}}
\def\sS{{\mathbb{S}}}
\def\sT{{\mathbb{T}}}
\def\sU{{\mathbb{U}}}
\def\sV{{\mathbb{V}}}
\def\sW{{\mathbb{W}}}
\def\sX{{\mathbb{X}}}
\def\sY{{\mathbb{Y}}}
\def\sZ{{\mathbb{Z}}}

%
\def\emLambda{{\Lambda}}
\def\emA{{A}}
\def\emB{{B}}
\def\emC{{C}}
\def\emD{{D}}
\def\emE{{E}}
\def\emF{{F}}
\def\emG{{G}}
\def\emH{{H}}
\def\emI{{I}}
\def\emJ{{J}}
\def\emK{{K}}
\def\emL{{L}}
\def\emM{{M}}
\def\emN{{N}}
\def\emO{{O}}
\def\emP{{P}}
\def\emQ{{Q}}
\def\emR{{R}}
\def\emS{{S}}
\def\emT{{T}}
\def\emU{{U}}
\def\emV{{V}}
\def\emW{{W}}
\def\emX{{X}}
\def\emY{{Y}}
\def\emZ{{Z}}
\def\emSigma{{\Sigma}}

%
%
\newcommand{\etens}[1]{\mathsfit{#1}}
\def\etLambda{{\etens{\Lambda}}}
\def\etA{{\etens{A}}}
\def\etB{{\etens{B}}}
\def\etC{{\etens{C}}}
\def\etD{{\etens{D}}}
\def\etE{{\etens{E}}}
\def\etF{{\etens{F}}}
\def\etG{{\etens{G}}}
\def\etH{{\etens{H}}}
\def\etI{{\etens{I}}}
\def\etJ{{\etens{J}}}
\def\etK{{\etens{K}}}
\def\etL{{\etens{L}}}
\def\etM{{\etens{M}}}
\def\etN{{\etens{N}}}
\def\etO{{\etens{O}}}
\def\etP{{\etens{P}}}
\def\etQ{{\etens{Q}}}
\def\etR{{\etens{R}}}
\def\etS{{\etens{S}}}
\def\etT{{\etens{T}}}
\def\etU{{\etens{U}}}
\def\etV{{\etens{V}}}
\def\etW{{\etens{W}}}
\def\etX{{\etens{X}}}
\def\etY{{\etens{Y}}}
\def\etZ{{\etens{Z}}}

%
\newcommand{\pdata}{p_{\rm{data}}}
%
\newcommand{\ptrain}{\hat{p}_{\rm{data}}}
\newcommand{\Ptrain}{\hat{P}_{\rm{data}}}
%
\newcommand{\pmodel}{p_{\rm{model}}}
\newcommand{\Pmodel}{P_{\rm{model}}}
\newcommand{\ptildemodel}{\tilde{p}_{\rm{model}}}
%
\newcommand{\pencode}{p_{\rm{encoder}}}
\newcommand{\pdecode}{p_{\rm{decoder}}}
\newcommand{\precons}{p_{\rm{reconstruct}}}

\newcommand{\laplace}{\mathrm{Laplace}} %

\newcommand{\E}{\mathbb{E}}
\newcommand{\Ls}{\mathcal{L}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\emp}{\tilde{p}}
\newcommand{\lr}{\alpha}
\newcommand{\reg}{\lambda}
\newcommand{\rect}{\mathrm{rectifier}}
\newcommand{\softmax}{\mathrm{softmax}}
\newcommand{\sigmoid}{\sigma}
\newcommand{\softplus}{\zeta}
\newcommand{\KL}{D_{\mathrm{KL}}}
\newcommand{\Var}{\mathrm{Var}}
\newcommand{\standarderror}{\mathrm{SE}}
\newcommand{\Cov}{\mathrm{Cov}}
%
%
%
\newcommand{\normlzero}{L^0}
\newcommand{\normlone}{L^1}
\newcommand{\normltwo}{L^2}
\newcommand{\normlp}{L^p}
\newcommand{\normmax}{L^\infty}

\newcommand{\parents}{Pa} %

\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}

\DeclareMathOperator{\sign}{sign}
\DeclareMathOperator{\Tr}{Tr}
\let\ab\allowbreak
 
\usepackage{hyperref}
\usepackage{url}


\title{Predicate Hierarchies Improve\\ Few-Shot State Classification}

%

%

%
%
%

%
%
%
%
%
%

\author{Emily Jin\thanks{Equal contribution} \\ Stanford University 
\And 
Joy Hsu\footnotemark[1] \\ Stanford University 
\And 
Jiajun Wu \\ Stanford University 
}

%
%
%
%
%

%
%
%
%
%
%
%

\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}


\iclrfinalcopy %

%
\newcommand{\emily}[1]{\textcolor{purple}{[Emily: #1]}}
\begin{document}


\maketitle


%
\begin{abstract}
State classification of objects and their relations is core to many long-horizon tasks, particularly in robot planning and manipulation. However, the combinatorial explosion of possible object-predicate combinations, coupled with the need to adapt to novel real-world environments, makes it a desideratum for state classification models to generalize to novel queries with few examples. To this end, we propose \model, which leverages \textit{predicate hierarchies} to generalize effectively in few-shot scenarios. \model uses an object-centric scene encoder, self-supervised losses that infer semantic relations between predicates, and a hyperbolic distance metric that captures hierarchical structure; it learns a structured latent space of image-predicate pairs that guides reasoning over state classification queries. We evaluate \model in the CALVIN and BEHAVIOR robotic environments and show that \model significantly outperforms existing methods in few-shot, out-of-distribution state classification, and demonstrates strong zero- and few-shot generalization from simulated to real-world tasks. Our results demonstrate that leveraging predicate hierarchies improves performance on state classification tasks with limited data.
\end{abstract} 
%
\section{Introduction}

State classification of objects and relations is essential for a plethora of tasks, from scene understanding to robot planning and manipulation \citep{migimatsu2022grounding, chen2024spatialvlm}. Many such long-horizon tasks require accurate and varied state predictions for entities in scenes. For example, planning for ``setting up the table'' requires classifying whether the \texttt{\textit{cup}} is \texttt{NextTo} the \texttt{\textit{plate}}, whether the \texttt{\textit{utensils}} are \texttt{OnTop} of the \texttt{\textit{table}}, and whether the \texttt{\textit{microwave}} is \texttt{Open}. The goal of state classification is to precisely answer such queries about specific entities in an image, and determine whether they satisfy particular conditions across a range of attributes and relations.

%

%

\begin{figure}[b!]
  \centering
  \vspace{-.5cm}
\includegraphics[width=1.0\textwidth]{figures/pull_v9.pdf}
  \vspace{-.5cm}
  \caption{\model improves few-shot state classification, by encoding a \textit{predicate hierarchy} in joint image-predicate latent space. By encouraging such structured representations to emerge, \model enables strong few-shot generalization to novel predicates with few examples.}
  \label{fig:pull_figure}
  \vspace{-0.5cm}
\end{figure}

However, the combinatorial space of objects (e.g., \texttt{\textit{cup}}, \texttt{\textit{plate}}, \texttt{\textit{microwave}}) and predicates (e.g., \texttt{NextTo}, \texttt{OnTop}, \texttt{Open}) gives rise to an explosion of possible object-predicate combinations that is intractable to obtain corresponding training data for. In addition, real-world systems operating in dynamic environments must generalize to queries with novel predicates, often with only a few examples  \citep{bendale2015towards, joseph2021towards, ha2022semantic}. For instance, we may not only want to classify our trained query of whether the \texttt{\textit{cup}} is \texttt{NextTo} the \texttt{\textit{plate}}, but more specifically, whether the \texttt{\textit{cup}} is \texttt{OnRight} of the \texttt{\textit{bottle}}, and whether the \texttt{\textit{cabinet}} is \texttt{Closed} (See Figure~\ref{fig:pull_figure}). Hence, an essential but difficult consideration for state classification models is to quickly learn to adapt to out-of-distribution queries. However, current supervised methods struggle with few-shot state classification, and pretrained large vision language models fail to accurately answer challenging spatial relationship queries in robotics environments.  %

To this end, we propose \model, a state classification model that leverages the hierarchical structure between predicates to few-shot generalize to novel queries. At the core of \model is an image-predicate latent space trained to encode the relationship between pairwise predicates (See Figure~\ref{fig:pull_figure}). Let us consider the predicates, \texttt{OnRight} and \texttt{OnLeft}---while they describe opposite spatial relationships between objects, they are closely related semantically, as assessing them relies on the same underlying features. \model enforces image-predicate representations conditioned on these predicates to lie closer to one another. In addition, there exist predicate pairs with more complex relationships, such as \texttt{OnRight} and \texttt{NextTo}. We see that \texttt{OnRight} is a more specific case of \texttt{NextTo}---verifying \texttt{OnRight} involves recognizing whether the objects are \texttt{NextTo} each other. Features relevant to the higher-level predicate \texttt{NextTo} are therefore useful for reasoning about the lower-level predicate \texttt{OnRight}. \model encodes this \textit{predicate hierarchy} to allow generalizable state classification.

%
Such a predicate hierarchy can be used to quickly adapt to unseen object-predicate pairs. For example, when combining a learned predicate \texttt{NextTo} with the objects \texttt{\textit{apple}} and \textit{\texttt{microwave}}, \model uses a learned embedding of the predicate to control the features extracted from the image depicting an apple and microwave. Notably, for a more challenging state with a novel predicate, such as \texttt{OnRight}, the model can still adapt by drawing on its relationship to \texttt{OnLeft} and \texttt{NextTo} to learn relevant features from limited data. \model leverages pairwise hierarchical relations between predicates to efficiently generalize to out-of-distribution predicates with few examples. %

%

To perform state classification, \model first localizes relevant objects in the input image based on a given query, then leverages an inferred predicate hierarchy to structure its reasoning over the scene. \model processes objects and predicates separately, learning to map object representations conditioned on the predicate into an image-predicate latent space. The hierarchical structure of predicates is learned through self-supervised losses based on the relationships between predicates (e.g., \texttt{OnRight} and \texttt{NextTo}). We use a large language model (LLM) to infer the pairwise predicate relations from language. However, explicit hierarchies inherently assume some discretization over predicates, which does not align with the continuous nature of representations used in deep learning \citep{nickel2017poincare, ganea2018hyperbolic}. To address this, we propose using a hyperbolic distance metric to encode the hierarchical structure of predicates in a continuous manner, enabling \model to effectively incorporate tree-like structure.

We evaluate \model on the state classification task in two robotics environments, CALVIN \citep{mees2022calvin} and BEHAVIOR \citep{li2023behavior}. Beyond the standard test settings, we focus on few-shot, out-of-distribution tasks involving unseen object-predicate combinations and novel predicates. Our results show that \model significantly outperforms existing methods, including both supervised approaches trained on the same amount of data and inference-only vision-language models (VLMs) trained on large corpora of real-world examples. \model improves upon the top-performing prior work in out-of-distribution tasks by $22.5$ percent points on CALVIN and $8.3$ percent points on BEHAVIOR. Notably, trained solely on simulated data, \model also outperforms supervised baselines on zero- and few-shot generalization to real-world state classification tasks by $7$ percent points and $10$ percent points respectively. Overall, we see \model as a promising solution to few-shot state classification, enabling generalization by leveraging representations grounded in predicate hierarchies.

In summary, our contributions are the following:
\vspace{-0.2cm}
\begin{itemize}
  \setlength{\parskip}{0pt} %
  \setlength{\topsep}{0pt}  %
  \setlength{\partopsep}{0pt} %
  \setlength{\parsep}{0pt}  %
  
%
%
%
%
\item We introduce \model, a state classification model that encodes inferred predicate hierarchies in its latent space, enabling generalization to unseen object-predicate combinations and novel predicates with few examples.
\item We propose learning the predicate hierarchy through an object-centric scene encoder, self-supervised losses that encourage pairwise predicate relations, and a hyperbolic distance metric to model the hierarchical nature of predicates in continuous space.
\item We validate \model's performance in few-shot, out-of-distribution generalization, and zero- and few-shot real-world transfer across three state classification datasets. \model significantly outperforms both supervised baselines and large pretrained VLMs. 
%
\end{itemize}
\vspace{-0.2cm}


 
%
\section{Related Work}
\vspace{-0.5cm}
\textbf{State classification.}
The ability to understand object states and relationships is essential for a wide range of tasks in computer vision and robotics, including scene understanding, robotic manipulation, and high-level planning \citep{yao2018visualrelationship, yuan2022sornet}. Earlier works that focus on a similar task of visual relationship detection learn to extract object-centric representations from raw images and make predictions based on them \citep{gkioxari2018detecting, yao2018visualrelationship, ma2022relvit, yuan2022sornet}. A more recent approach by \citet{yuan2022sornet} specifically addresses state classification by extracting object-centric embeddings from RGB images and feeding them into trained networks to classify a set of predefined predicates. However, their approach relies on input images of the objects of interest and is limited to predicates seen during training, as it requires a separate network for each predicate. In contrast, our method can few-shot generalize to unseen predicates given only the input scene and query, without any additional annotations or specific object images.

Recent advancements in robotics simulators have additionally enabled scalable training of state classification in simulation and subsequent transfer to real-world settings. Simulators such as CALVIN \citep{mees2022calvin} and BEHAVIOR \citep{li2023behavior} offer varying levels of realism and are widely used in the robotics community to generate large and diverse datasets, supporting data generation across a broad range of states \citep{ge2024behavior}. We train our model on such simulators and show that, compared to prior works, \model is significantly more effective at zero- and few-shot generalization to real-world state classification tasks.

\vspace{-0.1cm}
\textbf{Question-answering approaches.}
Several key advancements in visual question answering (VQA) have been driven by the development of pretrained large vision-language models (VLMs). These models are trained on extensive image and text data, leading to a unified visual-language representation that can be applied to perform various downstream tasks \citep{shen2021clip, li2023blip, gpt}. Approaches such as Viper-GPT \citep{suris2023vipergpt} further leverage the power of foundation models by composing LLMs to generate programs that pretrained VLMs can execute. However, despite their strong general-purpose capabilities, these models still struggle with questions involving visual and spatial relationships \citep{tong2024eyes}.

On the other hand, a separate class of VQA methods are models trained directly for the VQA task. These approaches follow a general framework of extracting visual and textual features, combining them into a multimodal representation, and learning to predict the answer. Convolutional neural networks (CNNs) and transformers are widely used for feature extraction, with various techniques for fusing the features. FiLM \citep{perez2018film} is an early, modular approach that applies a feature-wise transformation to condition the image features on the text features, while BUTD \citep{anderson2018butd} and Re-Attention \citep{guo2020reattention} are representative attention-based methods that localize important regions based on the question. Furthermore, many approaches, including modular, graph-based, or neurosymbolic approaches, introduce more explicit reasoning to better model the relationships and interactions between objects \citep{andreas2016neural, yi2018neural, ma2022relvit, nguyen2022coarse, wang2023vqa}. Our work lies in this latter class of methods, designed and trained for state classification. In contrast to prior works, we not only encode visual features and textual features of predicates but also learn to capture the inherent predicate hierarchy in a joint image-predicate latent space.

\vspace{-0.1cm}
\textbf{Hyperbolic representations for hierarchy.}
In recent years, several works have explored the benefits of using hyperbolic space to model hierarchical relationships, as it is particularly well-suited for capturing hierarchical structures \citep{ganea2018hyperbolic, nickel2018learning}. In computer vision, prior works have focused on learning hyperbolic representations for image embeddings, demonstrating their effectiveness across various tasks such as semantic segmentation and object recognition \citep{khrulkov2020hyperbolic, liu2020hyperbolic, atigh2022hyperbolic, ermolov2022hyperbolic, Ge_2023_CVPR}. Hyperbolic embeddings allow models to naturally represent hierarchical relationships between various entities, such as objects and scenes or different categories, leading to improved performance and generalization in such tasks \citep{weng2021unsupervised, Ge_2023_CVPR}. Several approaches further incorporate self-supervised learning to learn the underlying hierarchical structure without the need for explicit labels, instead leveraging proxy tasks such as contrastive learning \citep{hsu2021capturingimplicithierarchicalstructure, Ge_2023_CVPR, yue2023hyperbolic}. Recently, \citet{desai2023hyperbolic} proposed a contrastive method to learn joint representations of vision and language in hyperbolic space, yielding a representation space that captures the underlying structure between images and text. Inspired by these works, \model learns a predicate hierarchy in hyperbolic space informed by language and the pairwise relations between predicates, and uses it to conduct few-shot generalization to unseen state classification queries. 
%
%
\section{Method}
\label{sec:method}

\begin{figure}[t!]
  \centering
  \includegraphics[width=1.0\textwidth]{figures/systems_v9.pdf}
  %
  \caption{\model consists of three main components. The first are disentangled image and predicate encoders, which separately extract features based on the objects and predicates in the state classification query. The second is a self-supervised learning process that injects explicit knowledge of pairwise predicate relations into the image-predicate latent space. The third is the use of a hyperbolic distance metric and encoder to encourage encoding of the inferred predicate hierarchy. Together, these components enable few-shot generalization to unseen object-predicate pairs and novel predicates.}
  \label{fig:systems_figure}
  %
\end{figure}

In this section, we describe \model, our model for generalizable state classification. We define \model's task as a binary state classification problem: given a 2D RGB image and a text query describing a state (e.g., \texttt{NextTo(\textit{cup}, \textit{plate})}, the goal is to predict whether the predicate \texttt{NextTo} holds True or False for the objects, \texttt{\textit{cup}} and \texttt{\textit{plate}}, in the input image. \model enables efficient generalization to unseen predicates, by way of inferring a structured latent space of image-predicate pairs to perform reasoning over (See Figure~\ref{fig:systems_figure}). At the core of our method is a predicate hierarchy that captures the semantic relationships between predicates. This hierarchical structure is inferred through three key components: 
\begin{enumerate}
    \item An object-centric scene encoder that localizes regions corresponding to relevant objects;
    \item Self-supervised losses that inject pairwise relations of predicates into the latent space;
    \item A hyperbolic distance metric and an encoder that model hierarchical representations.
\end{enumerate}

In Section~\ref{section: priors}, we describe \model's base architecture of object and predicate conditioning. In Section~\ref{section: ssl}, we introduce self-supervised losses that encourage representations to cluster based on pairwise predicate relationships. In Section~\ref{section: hyperbolic}, we detail how \model learns hierarchical representations by embedding representations in hyperbolic space.

\subsection{Object-Centric Image Encoder}
\label{section: priors}

\model extracts an object-centric scene representation by conditioning the input image $\mathbf{I}$ on the query. Assume the input query is \texttt{NextTo(cup, plate)}. \model first parses it into its objects $\mathcal{O} = \{\texttt{cup, plate}\}$ and predicate $\mathcal{P}=\texttt{NextTo}$. Then, \model separately conditions the latent space on both of these components (See Figure~\ref{fig:systems_figure}). By disentangling the full state classification query, we ensure that \model faithfully identifies the relevant entities; it then learns to focus on their key features for the given predicate's classification. 

\textbf{Object conditioning.} The goal of object conditioning is to localize regions of the image that correspond to the relevant objects in the query. This allows \model to focus on the objects of interest for predicting the downstream predicate condition. To encode an input image $\mathbf{I}$ on the objects $\mathcal{O}$, our encoder $E_{\text{img}}$ generates an object-conditioned image mask $\mathcal{M}(\mathbf{I}, \mathcal{O})$ that highlights image regions based on the relevant entities, following \citet{zhou2022extract}. At a high level, we extract features for the image and object texts, align the image features with each object text's features to generate individual object masks, and then encode the image based on these masks. Concretely, for each object $\mathbf{o}\in \mathcal{O}$, we use a CLIP image encoder $\mathcal{V}$ and text encoder $\mathcal{T}$~\citep{radford2021learning} to obtain image features $\mathcal{V}(\mathbf{I})\in \bR^D_1$ and object text features $\mathcal{T}(\mathbf{o})\in\bR^D_2$, where $D_1$, $D_2$ correspond to the visual and language embedding dimensions, respectively.

To align the image and text features, we first project the image features into the same space as the text features. In order to preserve the spatial structure of the image features, we initialize a $1 \times 1$ convolution layer $\mathcal{C}$ with weights from the last linear layer of the CLIP attention pooling mechanism and apply this to the image features. Then, we apply a convolution $\otimes$ on the image features, using the text features as filters. This directly aligns image regions with the object text, highlighting the parts of the image that are the most relevant to the query. Our resulting object mask $\mathcal{M}(\mathbf{I},\mathbf{o})$ is defined as
\begin{equation*}
    \mathcal{M}(\mathbf{I}, \mathbf{o}) = \mathcal{C}(\mathcal{V}(\mathbf{I})) \otimes \mathcal{T}(\mathbf{o}).
\end{equation*}

To obtain the final image mask, we upsample each object mask to the original image dimensions, normalize the values to the range $[0, 1]$, and take the element-wise $\max$ across the individual object masks. We then apply the final mask to our image using a Hadamard product $\odot$ and subsequently encode the masked image using a pretrained {vision transformer (ViT) encoder~\citep{dosovitskiy2020image} $E_{\text{ViT}}$ as follows:
\begin{equation*}
    E_{\text{img}}(\mathbf{I}, \mathcal{O}) = E_\text{ViT}(\mathbf{I} \odot \max_{o\in\mathcal{O}}(\text{norm}(\text{upsample}(\mathcal{M}(\mathbf{I}, \mathbf{o}))))).
\end{equation*}
This ensures that the representation encodes all relevant regions based on the specified objects. 

\textbf{Predicate conditioning.} Next, \model conditions the latent representation $E_\text{img}(\mathbf{I}, \mathcal{O})$ on the text predicate $\mathcal{P}$, to focus on features essential to the downstream classification task. To do so, we encode the predicate text $P$ using a pre-trained BERT encoder $E_\text{text}$~\citep{devlin2018bert} and concatenate this with the masked image, resulting in a final object-centric scene representation: $$E(\mathbf{I}, \mathcal{O}, \mathcal{P}) = \text{concat}(E_\text{img}(\mathbf{I}, \mathcal{O}), E_\text{text}(\mathcal{P})),$$ which incorporates both object features and context relevant to the predicate. 

\subsection{Self-Supervised Learning}
\label{section: ssl}
\model learns a joint image-predicate space that encodes an inferred \textit{predicate hierarchy} with self-supervised losses. In particular, a specific predicate such as \texttt{OnLeft} is encouraged to be close to a related, more general predicate such as \texttt{NextTo}, as features important to closer predicates tend to be more alike. At the same time, \texttt{OnLeft} should have a larger norm than \texttt{NextTo}, to reflect its lower position in the hierarchy. This ensures that the features that are learned to be relevant to higher-level predicates remain useful when reasoning about their lower-level children. 

We introduce two self-supervised losses to encourage such pairwise relationships: a predicate triplet loss based on the similarity between predicates and a norm regularization loss based on the hierarchy between predicates. For both losses, we sample triplets with unique corresponding predicates and then extract knowledge from an LLM to determine the underlying relationships between the predicates.

\textbf{Predicate triplet loss.} Given a predicate triplet with an anchor $a$, positive $p$, and negative $n$ sample, the triplet loss encourages the distance between the anchor and negative predicate to be at least some margin $\lambda$ greater than the distance between the anchor and positive predicate. In Figure~\ref{fig:systems_figure}, we see that \texttt{NextTo} is the anchor predicate, \texttt{OnLeft} is the positive sample, and \texttt{Inside} is the negative sample. The proper assignment of the samples is critical, as it directly affects how faithfully the model learns the relationship between predicate pairs. To determine the anchor, positive, and negative sample for any given predicate triplet, we query an LLM based on the semantic meanings and scene details described by each query. More concretely, for each triplet, we prompt the LLM to assess the underlying relationships between the predicates. One predicate in the triplet is randomly chosen as the anchor. The LLM is asked to determine which of the other two predicates is more similar to the anchor. The anchor and the more similar predicate form a positive pair, while the anchor and the less similar predicate form a negative pair. We provide the prompt template provided in Appendix~\ref{section: llm_prompts}. By extracting knowledge from an LLM, we leverage the LLM's explicit and extensive understanding of predicate relationships to produce meaningful triplets and guide the model toward a semantically rich image-predicate latent space.

Our formulation of the triplet loss is based on the distance between representations:
$$\mathcal{L}_{\text{triplet}, \lambda}(a, p, n) := \max(0, \textbf{d}(a,p) -\textbf{d}(a,n) + \lambda),$$
where $\textbf{d}$ is the distance metric used in the latent space. We describe our choice of a hyperbolic distance metric in the following section. With this loss and chosen distance metric, the resulting representation space captures the similarity between predicates via their distance in the latent space.

\textbf{Norm regularization loss.} While the triplet loss enforces similarity between related predicates, the norm regularization term introduces hierarchical structure to the latent space. We leverage the LLM's strong semantic understanding of predicates to infer the hierarchy among a triplet, by ranking the predicates based on specificity. Specificity depends on several factors, including the variety and number of objects required by the state, the important features of the objects and relationships between the objects, the level of detail required by the state, and the semantic meaning of each description. The prompt template is provided in Appendix~\ref{section: llm_prompts}.

Given a ranking of three predicates $a,b,c$ from least to most specific, the regularization loss encourages the norm to increase by at least some margin $\gamma$ as the specificity increases: 
\begin{equation*}
    \mathcal{L}_{\text{reg}, \gamma}(a,b,c):=\max (0, ||b||-||a|| + \gamma + \max(0, ||c||-||b|| + \gamma).
\end{equation*}

Intuitively, the regularization loss ensures that the norms of the representations reflect the implicit hierarchy between predicates.

Together, the predicate triplet loss and norm regularization loss encourage a semantically rich and structured representation space. The predicate triplet loss captures the similarity between predicates by enforcing appropriate distances between related predicates, while the norm regularization loss ensures that the hierarchical relationships are reflected in the norm of the representations. By leveraging the explicit knowledge of an LLM to infer both the triplet assignments and hierarchy ranking, our approach ensures that \model's learned representations align with the underlying predicate ontology.

\subsection{Hyperbolic Image-Predicate Latent Space}
\label{section: hyperbolic}

Finally, \model effectively learns this inferred hierarchy of predicates through \textit{hyperbolic} representations. 
%
While \model's self-supervised losses inject semantic knowledge of predicates into its representations, \model further encodes the hierarchical nature of the predicates in hyperbolic space. In hyperbolic space, we can more easily visualize these relationships forming a natural predicate hierarchy. In Figure~\ref{fig:systems_figure}, we visualize a learned predicate hierarchy in \model's latent space. 

Hyperbolic space is a non-Euclidean space characterized by constant negative curvature, which allows hierarchical structure to be easily embedded. Due to hyperbolic space's curvature, the area of a disc increases exponentially with its radius, analogous to the exponential branching of trees. This property makes hyperbolic space well-suited for modeling hierarchies, as it provides a continuous representation of discrete trees. Furthermore, hyperbolic space is differentiable, making it easy to integrate with our model. Hence, we propose using a hyperbolic distance metric for our predicate triplet loss and norm regularization loss to more effectively encode the predicate hierarchy. These hierarchical representations enable \model to generalize effectively to novel predicates, by inferring their representations based on their relationships to learned predicates in the hyperbolic latent space. %

\textbf{Poincaré ball model.} In this work, we use the Poincaré ball model of hyperbolic space. The Poincaré ball is an open $d$-dimensional ball of radius 1, equipped with the metric tensor $g_\mathbf{p}=(\lambda_x)^2g_\mathbf{e}$. Here, $||\cdot||$ is the Euclidean norm, $\lambda_x=\frac{2}{1-||x||^2}$ is the conformal factor, and $g_e$ is the Euclidean metric tensor (i.e., the Euclidean dot product). This induces the distance $\mathbf{d}_\mathbf{p}$ between two points $x, y$ on the Poincaré ball as
\begin{equation*}
    \mathbf{d}_\mathbf{p}(x,y) = \cosh^{-1}\left(1 + 2\frac{||x-y||^2}{(1-||x||^2)(1-||y||^2)}\right).
\end{equation*}

On the Poincaré ball, the distance between two points captures the degree of similarity between them, while the relative norm of two points reflects their hierarchical structure. Thus, the Poincaré ball is a suitable space to represent the underlying predicate hierarchy, and \model's self-supervised losses use such metrics to embed image-predicates onto the Poincaré ball.

\textbf{Hyperbolic encoder.} To obtain the hyperbolic image-predicate representation, we use the exponential map to lift the representation from Euclidean space onto the Poincaré ball and pass it through a small hyperbolic linear network \citep{ganea2018hyperbolic}. For more details on these functions, we refer the reader to Appendix \ref{section: hyperbolic_prelim}.

\subsection{Training Loss}
After the hierarchical representation is learned in hyperbolic space, we apply the logarithmic map to project it back to Euclidean space, where it is passed through a small MLP for state classification. We train \model with a binary cross entropy loss based on the ground truth labels (True or False), along with the predicate triplet loss and the norm regularization loss. Our overall loss is defined as 
$\mathcal{L}_{\text{total}} := \mathcal{L}_{\text{sup}} + \alpha\mathcal{L}_{\text{triplet}, \lambda} + \beta\mathcal{L}_{\text{reg}, \gamma},$
where $\alpha$, $\beta$ are coefficients that control the strength of the triplet and regularization losses, and $\lambda$, $\gamma$ are the margins for the two losses, respectively. 
%
\section{Dataset}
We evaluate \model on established robotics environments, with three state classification datasets designed to test the following key aspects of performance: a faithful understanding of entities and relations between them, few-shot generalization to out-of-distribution queries, and zero- and few-shot transfer to a real-world setting. See Figure~\ref{fig:dataset_examples} for examples from each of the environments.

\begin{figure}[t]
    \centering
    \includegraphics[width=0.99\linewidth]{figures/dataset_v6.pdf}
    \vspace{-0.2cm}
    \caption{Examples of state classification tasks from CALVIN and BEHAVIOR. The datasets span a range of visual realism and complexity.}
    \label{fig:dataset_examples}
    \vspace{-0.2cm}
\end{figure}

%

\textbf{Simulator dataset generation.}
In order to evaluate our method's state classification performance on robotic environments, we generate datasets of varying levels of realism with two widely used robotics simulators, CALVIN \citep{mees2022calvin} and BEHAVIOR \citep{li2023behavior}. Both are known for their ease of use and customizability, allowing us to generate diverse data for various states with different objects under various lighting, camera angle, and object pose conditions. As shown in Figure~\ref{fig:dataset_examples}, CALVIN is visually simplistic while BEHAVIOR is more realistic and complex. By evaluating on data from these two simulators, we assess how well various methods understand the semantics of different predicates.

\textbf{Simulator dataset details.}
To evaluate the effectiveness of our inferred abstraction hierarchy, we define sets of in-distribution and out-of-distribution states, featuring both unary and binary relations. The out-of-distribution states involve unseen predicate-object combinations and novel predicates. See Appendix~\ref{section: states} for the states in each dataset. %
We train on a balanced dataset of $200$ examples ($100$ True, $100$ False) for each in-distribution state. We then evaluate on balanced test sets of $50$ examples for each state under both in-distribution and out-of-distribution settings. 

\textbf{Real-world dataset.}
In addition, we evaluate on BEHAVIOR Vision Suite~\citep{ge2024behavior} (see Figure~\ref{fig:dataset_examples}), a complex real-world benchmark that consists of diverse scenes and distractor objects. Specifically, compared to our train data, this one consists of $10$ unseen combinations and $10$ novel predicates, with $337$ total examples. We use this dataset to test our method's ability to perform zero- and few-shot real-world transfer after training on simulated datasets alone. 
%
\vspace{-0.3cm}
\section{Results}
We evaluate \model on the three datasets and compare against $10$ state-of-the art models, with our evaluation metric as binary state classification accuracy. Our experiments show that \model's learned predicate hierarchy leads to significantly improved performance, especially in the challenging settings of few-shot, out-of-distribution generalization as well as zero- and few-shot, real-world transfer. 

\subsection{Implementation}
\textbf{\model.} For our model, we use the CLIP image encoder, CLIP text encoder, and BERT text encoder as our image, object text, and predicate text encoders, respectively. Our hyperbolic encoder consists of two hyperbolic linear layers with output dimensions of $256$ and $128$, and the final small MLP is a single layer. We use $\alpha=0.05$ as our triplet loss coefficient, $\lambda=10.0$ as our triplet loss margin, $\beta=1.0$ as our regularization loss coefficient, and $\gamma=0.1$ as our regularization margin. We train all models for $50$ epochs using the AdamW optimizer with a learning rate of $1e^{-4}$ using a gradual warmup scheduler and cosine annealing decay. For the few-shot setting, we provide $5$ examples of each novel predicate and train for $20$ epochs with the same optimizer and learning rate.

\textbf{Baselines.}
We use $10$ state-of-the-art methods as baselines, ranging from supervised methods to pretrained large vision language models (VLMs). Of the supervised methods, RelViT \citep{ma2022relvit} and SORNet \citep{yuan2022sornet} are recent methods designed with a focus on state classification, while Re-Attention \citep{guo2020reattention}, Coarse-to-Fine \citep{nguyen2022coarse}, BUTD \citep{anderson2018butd}, finetuned CLIP \citep{shen2021clip}, and FiLM \citep{perez2018film} are top-performing general VQA methods. The supervised models are all trained on the same data as \model, while the pretrained large VLMs, BLIP-2~\citep{li2023blip}, GPT-4V\citep{gpt}, and ViperGPT \citep{suris2023vipergpt}, are run inference-only. All methods are evaluated on both in-distribution and out-of-distribution queries, except for SORNet as its architecture does not allow classification of unseen states. Additional details on our baselines are provided in Appendix~\ref{section: baseline_implementation}.

\subsection{Comparison to Prior Work}
%
\begin{table}[t]
\caption{Comparison of \model to prior works on CALVIN and BEHAVIOR datasets. We compare against trained models (T) and inference-only models pretrained on large-scale data (I). In this table, we report in-distribution (ID) test accuracy, out-of-distibution (OOD) test accuracy, and the difference in performance between the two test sets (ID-OOD). \model significantly outperforms prior work in the OOD setting; it is also the only method that performs similarly in ID and OOD settings.}
\label{tab:main}
\centering
\vspace{5mm}
%
\footnotesize
\begin{tabular}{lccccccc}
\toprule
 & & \multicolumn{3}{c}{CALVIN} & \multicolumn{3}{c}{BEHAVIOR}  \\
 \cmidrule(lr){3-5} \cmidrule(lr){6-8} 
 & & ID $\uparrow$ & OOD $\uparrow$ & ID-OOD $\downarrow$ & ID $\uparrow$ & OOD $\uparrow$ & ID-OOD $\downarrow$ \\
\midrule
\model (Ours) & T & $0.945$ & $\mathbf{0.899}$ & $\mathbf{0.046}$ & $0.859$ & $\mathbf{0.820}$ & $\mathbf{0.039}$\\
\midrule
 Re-Attention~\citep{guo2020reattention} & T & $\mathbf{0.959}$ & $0.674$ & $0.285$ & $0.828$ & $0.652$ & $0.176$  \\
CoarseFine~\citep{nguyen2022coarse} & T & $0.878$ & $0.624$ & $0.254$ & $0.766$ & $0.636$ & $0.130$  \\
 BUTD~\citep{anderson2018butd} & T & $0.898$ & $0.585$ & $0.313$ & $0.808$ & $0.712$ & $0.096$  \\
 RelViT~\citep{ma2022relvit} & T & $0.688$ & $0.563$ & $0.125$ & $\mathbf{0.866}$ & $0.737$ & $0.129$  \\
 CLIP~\citep{shen2021clip} & T & $0.937$ & $0.546$ & $0.391$ & $0.722$ & $0.632$ & $0.090$ \\ 
 FiLM~\citep{perez2018film} & T & $0.798$ & $0.489$ & $0.309$ & $0.753$ & $0.583$ & $0.170$  \\
  SORNet~\citep{yuan2022sornet} & T & $0.943$ & -- & -- & $0.773$ & -- & -- \\
 \midrule
GPT-4V~\citep{gpt} & I & $0.587$ & $0.563$ & $0.024$ & $0.661$ & $0.706$ & $-0.045$ \\
 BLIP-2~\citep{li2023blip} & I & $0.553$ & $0.556$ & $-0.003$ & $0.554$ & $0.535$ & $0.019$ \\
 ViperGPT~\citep{suris2023vipergpt} & I & $0.466$ & $0.475$ & $-0.009$ & $0.552$ & $0.583$ & $-0.031$ \\
 \bottomrule
\end{tabular}
\vspace{-0.2cm}
\end{table} 
\textbf{Few-shot generalization accuracy.}
In Table~\ref{tab:main}, we show comparisons of \model and prior work on CALVIN and BEHAVIOR datasets, in the $5$-shot generalization setting. We split prior works into trained supervised methods (T) and inference-only pretrained VLMs (I). While \model yields comparable performance to top-performing prior works on the in-distribution test set, \model significantly outperforms all methods on the out-of-distribution test set. \model demonstrates a $22.5$ percent point improvement compared to the top-performing prior work in the out-of-distribution CALVIN setting, and a $8.3$ percent point improvement in the out-of-distribution BEHAVIOR setting. Notably, \model sees a low drop in accuracy between in-distribution and out-of-distribution test sets, which we hypothesize is due to \model's structured representation space enabling generalization. We report more detailed results in Table~\ref{tab:ood_breakdown}, specifically the out-of-distribution accuracy of few-shot unseen object-predicate pairs and novel predicates. We see that for both categories of generalization, \model significantly outperforms prior works. On CALVIN, \model improves upon the top-performing prior work by $25.1$ percent points in few-shot generalization to novel predicates. On BEHAVIOR, we see a $13.1$ percent point improvement.

%
%
%
\begin{table}[t]
\caption{Detailed breakdown of CALVIN and BEHAVIOR out-of-distribution results with accuracy on unseen object-predicates combinations and novel predicates.}
\vspace{-0.2cm}
\label{tab:ood_breakdown}
\centering
\vspace{5mm}
%

\resizebox{\textwidth}{!}{%
\begin{tabular}{lcccccc}
\toprule
 & \multicolumn{3}{c}{CALVIN} & \multicolumn{3}{c}{BEHAVIOR} \\
\cmidrule(lr){2-4} \cmidrule(lr){5-7}
 & All & Unseen Comb. & Novel Pred. & All & Unseen Comb. & Novel Pred. \\
\midrule 
\model (Ours) & $\mathbf{0.899}$ & $\mathbf{0.922}$ &  $\mathbf{0.863}$ & $\mathbf{0.820}$ & $\mathbf{0.831}$ & $\mathbf{0.807}$\\
\midrule
 Re-Attention & $0.674$ & $ 0.777$ &  $0.612$ & $0.652$ & $0.718$	 &$0.581$ \\
CoarseFine & $0.624$ & $0.665$ &  $0.557$ & $0.636$ & $0.670$ & $0.600$\\
 BUTD & $0.585$ & $0.622$ &  $0.563$ & $0.712$ & $0.766$ & $0.653$ \\
 RelViT & $0.563$ & $0.591$ &  $0.515$ & $0.737$ & $0.793$ & $0.676$ \\
 CLIP & $0.546$ & $0.596$ &  $0.463$ & $0.632$ & $0.684$ & $0.576$ \\
 FiLM & $0.489$ & $ 0.538$ & $0.406$ & $0.583$ & $0.652$ & $0.508$ \\
 %
 \bottomrule
\end{tabular}
}
\vspace{-0.5cm}
\end{table} 

\textbf{Real world zero- and few-shot transfer accuracy.} 
In Table~\ref{tab:real_world}, we report the zero- and few-shot, real-world transfer results of models trained on the simulated BEHAVIOR dataset, and tested on the BEHAVIOR Vision Suite~\citep{ge2024behavior}, a complex real-world benchmark. We compare \model with previous supervised methods, which have seen the same amount of train data, and find that \model significantly outperforms prior works on this challenging sim-to-real task across both zero- and few-shot settings. We conjecture that this is because \model learns more robust features for images---only features core to the specified state classification task are captured, and hence enables \model to generalize and remain invariant to the visual details in the real world. We additionally include results from pre-trained models, though we note that these models are our upper bound, as they are trained on large-scale real-world datasets with vast amounts of diverse data. Hence, these models inherently do not differentiate between in-distribution and generalization scenarios, as their training data overlaps significantly with both. We see that \model outperforms ViperGPT and BLIP-2 by $6.0\%$ and $1.4\%$ respectively on \model's zero-shot setting, showing the potential for a small model trained on significantly less data, to reach the performance level of large pre-trained models. However, GPT-4v outperforms \model by $10.4\%$. which we hypothesize is due to its model size and dataset scale. In the few-shot setting using only two examples, PHIER’s performance improves significantly and narrows the gap with GPT-4v to just $0.9\%$. This further demonstrates that PHIER’s inferred predicate hierarchy enables it to generalize efficiently to novel queries.

%

%
\begin{table}[t]
\caption{We present zero- and few-shot generalization results on a real-world test set, when trained only on the BEHAVIOR dataset. \model outperforms all prior supervised models.}
\label{tab:real_world}
\centering
\vspace{0.1cm}
%
\resizebox{\textwidth}{!}{%
\begin{tabular}{lcccccc}
\toprule
 & \multicolumn{3}{c}{Zero-shot} & \multicolumn{3}{c}{Few-shot} \\
\cmidrule(lr){2-4} \cmidrule(lr){5-7}
 & All & Unseen Comb. & Novel Pred. & All & Unseen Comb. & Novel Pred. \\
\midrule
\model (Ours) & $\mathbf{0.608}$ & $\mathbf{0.632}$ & $\mathbf{0.585}$ & $\mathbf{0.703}$ & $\mathbf{0.714}$ & $\mathbf{0.691}$ \\
\midrule
Re-Attention & $0.377$ & $0.415$ & $0.341$ & $0.413$ & $0.458$ & $0.368$ \\
CoarseFine & $0.490$ & $0.485$ & $0.494$ & $0.553$ & $0.562$ & $0.543$ \\
BUTD & $0.418$ & $0.427$ & $0.409$ & $0.456$ & $0.464$ & $0.448$ \\
RelViT & $0.556$ & $0.579$ & $0.528$ & $0.603$ & $0.654$ & $0.552$ \\
CLIP & $0.516$ & $0.544$ & $0.489$ & $0.571$ & $0.674$ & $0.468$ \\
FiLM & $0.459$ & $0.480$ & $0.438$ & $0.513$ & $0.542$ & $0.484$ \\
\midrule
GPT-4V & $0.712$ & $0.737$ & $0.688$ & $--$ & $--$ & $--$ \\
BLIP-2 & $0.594$ & $0.591$ & $0.597$ & $--$ & $--$ & $--$ \\
ViperGPT & $0.548$ & $0.538$ & $0.557$ & $--$ & $--$ & $--$ \\
\bottomrule
\end{tabular}
}
\vspace{-0.4cm}
\end{table}
 

\subsection{Ablations}
%
\begin{table}[t]
    \caption{Ablations of each component of \model and its effect on few-shot generalization.}
    \label{tab:ablation}
    \centering
    \label{tab:ablations}
    \centering
    \vspace{0.1cm}
    %
    \begin{tabular}{lcccccc}
    \toprule
     & \multicolumn{1}{c}{CALVIN OOD} & \multicolumn{1}{c}{BEHAVIOR OOD}  \\
    %
    %
    \midrule
    %
    %
    %
    %
    Supervised model & $0.473$ & $0.516$ \\ 
    \ \ + Object-centric encoder & $0.627$ & $0.653$ \\
    \ \ \ \ + Predicate triplet loss & $0.758$ & $0.735$ \\
    \ \ \ \ \ \ + Norm regularization loss & $0.823$ & $0.782$ \\
     \ \ \ \ \ \ \ \ + Hyperbolic metric (\model) & $\mathbf{0.899}$ & $\mathbf{0.830}$ \\

    
    \bottomrule
    \end{tabular}
    \vspace{-0.2cm}
\end{table} 
We ablate the components of \model in Table~\ref{tab:ablation}. Specifically, we begin by reporting the out-of-distribution results of a supervised model for state classification. We then test variants of \model, progressively adding each component: our object-centric encoder, predicate triplet loss, norm regularization, and finally the full model with the hyperbolic distance metric.

We see that each component encourages a structured and semantically relevant latent space for out-of-distribution generalization. The object-centric encoder localizes relevant objects in the scene, improving performance by $15.4$ and $13.7$ percent points on CALVIN and BEHAVIOR, respectively. Adding the predicate triplet loss helps \model encode pairwise relationships between predicates, improving performance by $13.1$ and $8.2$ percent point on CALVIN and BEHAVIOR. The addition of the norm regularization loss introduces hierarchical structure into the latent space, further improving performance by $6.5$ and $4.7$ percent points.  Finally, we highlight the full \model equipped with the hyperbolic metric, which further enforces a tree-like hierarchy to emerge in latent space and yields the strongest generalization performance. 

%
%

\subsection{Discussion}
We propose \model as a framework for incorporating predicate hierarchies into the latent space of state classification models. One important design decision is how explicitly the hierarchy should be enforced---\model softly encourages this structure with self-supervised losses, but does not impose hard constraints on the model's forward pass. We designed \model in this way, in order to allow hierarchical structure to emerge in the hyperbolic latent space based on the data, and capture more nuanced structure than a strict, discrete predicate hierarchy could. Quantitatively, we see that \model retains the ability to perform well on generalization tests, while qualitatively, we can visualize \model's learned predicate hierarchy in the latent space.

We note that \model is potentially limited by the accuracy of the language model in determining pairwise predicate relations. In this paper, we assume that language itself can differentiate the relationship between predicates, but there may be cases where visual cues from data also matter. Empirically, on the datasets we tested, we see that the LLM's predictions match our expectations of what the predicate hierarchy should be. Additionally, as \model's enforcement of this hierarchy is not explicit, it is still possible for \model to learn from data when the language model is incorrect. As a future direction, exploring environments where the dataset for state classification yields a unique predicate hierarchy, which we can encode through explicit enforcement in the model's forward pass, would showcase the effect of an explicitly hierarchical version of \model in generalization. In addition, exploring ways of training a model to infer the pairwise predicate relations with weak supervision, instead of injecting relation priors through a language model, could potentially give rise to a fully emergent and discovered predicate hierarchy.



 
%
\vspace{-0.1cm}
\section{Conclusion}
\vspace{-0.1cm}
\model tackles the challenge of few-shot out-of-distribution state classification by encoding predicate hierarchies into its latent space. Our proposed model, \model, learns language-informed image-predicate representations to generalize to novel predicates with few examples. Our experiments on CALVIN, BEHAVIOR, and a real-world test set demonstrate that \model significantly improves upon existing methods, particularly in highly difficult generalization cases. We show that using predicate hierarchies is a promising approach to enable more robust and adaptable state classification.






 


  
\subsubsection*{Acknowledgments}
We thank Weiyu Liu for providing valuable feedback. This work is in part supported by ONR N00014-23-1-2355, ONR YIP N00014-24-1-2117, ONR MURI N00014-22-1-2740, NSF RI \#2211258, and AFOSR YIP FA9550-23-1-0127. JH is also supported by the Knight Hennessy Scholarship and the NSF Graduate Research Fellowship.




\bibliography{iclr2025_conference}
\bibliographystyle{iclr2025_conference}

\clearpage

%
\section*{\Large Supplementary for: \\ Predicate Hierarchies Improve Few-Shot\\ State Classification} 

\appendix

The appendix is organized as the following. In Appendix~\ref{section: add_results_details}, we include additional \model results, details, and discussion. In Appendix~\ref{section: baseline_implementation}, we describe implementation of baseline methods, including supervised models, pretrained large vision language models, and ablation variants. In Appendix~\ref{section: hyperbolic_prelim}, we present preliminaries on hyperbolic geometry. In Appendix~\ref{section: llm_prompts}, we detail prompts used to extract knowledge of predicates from LLMs. Finally, in Appendix~\ref{section: states}, we list all states in our datasets, and show examples from the BEHAVIOR Vision Suite~\cite{ge2024behavior}.

%
\section{\model Results and Details}
\label{section: add_results_details}

\subsection{Model Details}
\model's image and text encoders are initialized with pretrained CLIP~\citep{radford2021learning} and BERT~\citep{devlin2018bert} weights, respectively. The hyperbolic linear layers are initialized following the approach of \citet{shimizu2020hyperbolic}, with the weights drawn from a normal distribution centered at zero with a standard deviation $(2nm)^{-\frac{1}{2}}$, where $m$ and $n$ are the input and output sizes of the layer, and the biases set to the zero vector. The linear layer in the small MLP is initialized by the standard Kaiming initialization. All of the parameters in \model are trainable and updated during training.

\model disentangles the conditioning of the image on the full state classification query into two distinct ones: one that identifies the relevant objects and another that focuses on key features for the given predicate. While we use MaskCLIP to identify the relevant entities, \model's contribution lies in the decomposition of the query into object and predicate components, enabling it to faithfully identify the relevant entities and extract features based on the predicate.


\subsection{Comparison on Manually Collected Real-world Dataset}

We collect a small real-world dataset with $100$ examples, consisting of $4$ examples for each of the out-of-distribution BEHAVIOR states, to test our method's ability to perform zero-shot real-world transfer after training on simulated datasets alone. See Figure~\ref{fig:dataset_orig} for examples. In Table~\ref{tab:real_world_orig}, we observe similar trends as in the BEHAVIOR Vision Suite evaluation in the main text, even with a simpler dataset. \model significantly outperforms prior supervised baselines. However, as expected, pre-trained models trained on large-scale real-world data outperform \model.


\begin{figure}[h]
    \centering
    \includegraphics[width=0.99\linewidth]{figures/dataset_orig_v1.pdf}
    \caption{Examples from our manually collected real-world dataset.}
    \label{fig:dataset_orig}
\end{figure}

%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%

\begin{table}[h]
\caption{We present zero-shot generalization results of \model and prior works on a real-world test set, when trained only on the BEHAVIOR dataset. \model outperforms all prior supervised models.}
\label{tab:real_world_orig}
\centering
\vspace{5mm}
%
\begin{tabular}{lccc}
\toprule
 & All & Unseen Combination & Novel Predicate \\
\midrule
\model (Ours) & $\mathbf{0.62}$ & $\mathbf{0.64}$ &  $\mathbf{0.60}$ \\
\midrule
 Re-Attention~\citep{guo2020reattention} & $0.41$ & $0.45$ &  $0.37$ \\
CoarseFine~\citep{nguyen2022coarse} & $0.53$ & $0.52$ &  $0.54$ \\
 BUTD~\citep{anderson2018butd} & $0.44$ & $0.42$ & $0.46$ \\
 RelViT~\citep{ma2022relvit} & $0.55$ & $0.59$ &  $0.51$ \\
 CLIP~\citep{shen2021clip} & $0.55$ & $0.65$ &  $0.45$ \\
 FiLM~\citep{perez2018film} & $0.49$ & $0.51$ & $0.47$ \\
  %
\midrule
GPT-4V~\citep{gpt}& 0.72 & 0.74 & 0.70 \\
BLIP-2~\citep{li2023blip} & 0.62 & 0.66 & 0.58 \\
ViperGPT~\citep{suris2023vipergpt} & 0.55 & 0.52 & 0.58 \\
\bottomrule
\end{tabular}
\vspace{-0.2cm}
\end{table}
 
\subsection{Ablation Study on Example Count}
We study the effect of varying the number of examples used in the few-shot setting. We added new ablation experiments with $0$, $1$, $2$, $3$, $4$, $5$, and $10$-shot generalization performance on both CALVIN and BEHAVIOR environments. The results in Figure~\ref{fig:ablation_example_count} show that \model consistently outperforms prior works across all numbers of examples. Notably, in the CALVIN environment, \model's performance plateaus as the number of examples increases, indicating that the method requires only a few examples to adapt effectively to unseen scenarios. 

\begin{figure}[h]
    \centering
    \includegraphics[width=\linewidth]{figures/ablation_example_count_v2.pdf}
    \caption{Ablations varying number of examples given in few-shot setting for CALVIN and BEHAVIOR environments.}
    \label{fig:ablation_example_count}
\end{figure}

%
%

\subsection{Ablation Study with Removed Components}
We add an ablation study that evaluates the impact of removing individual components of \model to evaluate their contributions. We compare \model with four variants, (1) without the object-centric encoder, (2) without the hyperbolic latent space, (3) without the norm regularization loss, and (4) without the predicate triplet loss. We report results in Table~\ref{tab:ablation_new}. We see that without our object-centric design, performance drops significantly in both ID and OOD settings, emphasizing the importance of object-centric encoders for improved representation and reasoning. In addition, we show that removing each of the self-supervised losses leads to much weaker generalization capability. Finally, we observe reduced generalization performance without \model’s hyperbolic latent space and hyperbolic norm regularization loss, demonstrating that the hyperbolic space facilitates better handling of hierarchical relationships. These results validate that each component contributes meaningfully to \model's performance, particularly in improving OOD generalization.


%
\begin{table}[h!]
    \vspace{-3mm}
    \caption{Ablations of each component of \model and its effect on few-shot generalization.}
    \label{tab:ablation_new}
    \centering
    \centering
    \vspace{5mm}
    %
    \begin{tabular}{lcccccc}
    \toprule
     & \multicolumn{3}{c}{CALVIN} & \multicolumn{3}{c}{BEHAVIOR}  \\
     \cmidrule(lr){2-4} \cmidrule(lr){5-7} 
     & ID $\uparrow$ & OOD $\uparrow$ & ID-OOD $\downarrow$ & ID $\uparrow$ & OOD $\uparrow$ & ID-OOD $\downarrow$ \\
    \midrule
    \textcolor{black}{\model (Ours)} & \textcolor{black}{$0.945$} & \textcolor{black}{$0.899$} & \textcolor{black}{$0.046$} & \textcolor{black}{$0.859$} & \textcolor{black}{$0.820$} & \textcolor{black}{$0.039$}\\
    \midrule
    \textcolor{black}{- Object-centric encoder} & \textcolor{black}{$0.786$} & \textcolor{black}{$0.704$} & \textcolor{black}{$0.082$} & \textcolor{black}{$0.703$} & \textcolor{black}{$0.659$} & \textcolor{black}{$0.044$} \\
    \textcolor{black}{- Predicate triplet loss} & \textcolor{black}{$0.867$} & \textcolor{black}{$0.601$} & \textcolor{black}{$0.266$} & \textcolor{black}{$0.774$} & \textcolor{black}{$0.624$} & \textcolor{black}{$0.150$}\\
    \textcolor{black}{- Norm regularization loss} & \textcolor{black}{$0.914$} & \textcolor{black}{$0.823$} & \textcolor{black}{$0.091$} & \textcolor{black}{$0.834$} & \textcolor{black}{$0.782$} & \textcolor{black}{$0.052$}\\
    \textcolor{black}{- Hyperbolic metric} & \textcolor{black}{$0.903$} & \textcolor{black}{$0.784$} & \textcolor{black}{$0.119$} & \textcolor{black}{$0.803$} & \textcolor{black}{$0.761$} & \textcolor{black}{$0.042$}\\
     \bottomrule
    \end{tabular}
    \vspace{-0.2cm}
\end{table}


\iffalse
\begin{table}[t]
    \caption{Ablations of \model and variants of \model.}
    \label{tab:ablation}
    \centering
    \label{tab:ablations}
    \centering
    \vspace{5mm}
    \scriptsize
    \begin{tabular}{lcccccc}
    \toprule
     & \multicolumn{3}{c}{CALVIN} & \multicolumn{3}{c}{BEHAVIOR}  \\
     \cmidrule(lr){2-4} \cmidrule(lr){5-7} 
     & In-dist $\uparrow$ & OOD $\uparrow$ & In no OOD $\downarrow$ & In-dist $\uparrow$ & OOD $\uparrow$ & In no OOD $\downarrow$ \\
    \midrule
    \model (Ours) & $\mathbf{0.945}$ & $\mathbf{0.899}$ & $\mathbf{0.046}$ & $\mathbf{0.859}$ & $\mathbf{0.830}$ & $\mathbf{0.029}$\\
    \midrule
    End to end w/ hyperbolic ssl & $0.840$ & $0.763$ & $0.077$ & $0.751$ & $0.652$ & $0.099$\\
    End to end w/ ssl & $0.943$ & $0.636$ & $0.307$ & $0.786$ & $0.618$ & $0.169$\\
    End to end & $0.826$ & $0.465$ & $0.361$ & $0.792$ & $0.544$ & $0.248$\\
    \bottomrule
    \end{tabular}
\end{table}
\fi
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%

    
%
%
%
%


%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
 
\subsection{Few-shot Generalization to Novel Objects}

We expand our CALVIN and BEHAVIOR experiments to evaluate accuracy on few-shot generalization on novel objects in Table~\ref{tab:novel_objects}. The queries with these novel objects are listed in Table~\ref{tab: tab:novel_object_queries}. As in our experiments on unseen combinations and novel predicates, we observe that \model significantly outperforms prior baselines on unseen objects. Specifically, \model improves upon the top-performing prior work by $21.8$ percent points on CALVIN and $13.5$ percent point on BEHAVIOR. These results demonstrate that \model improves generalization to both novel objects and predicates, further highlighting the benefit of our object-centric encoder and inferred predicate hierarchy.

%
\begin{table}[h!]
\caption{We present novel object generalization results of \model and prior works on CALVIN and BEHAVIOR environments.}
\label{tab:novel_objects}
\centering
\vspace{5mm}
%
\begin{tabular}{lcc}
\toprule
 & CALVIN & BEHAVIOR \\
\midrule
\model (Ours) & $\mathbf{0.851}$ & $\mathbf{0.781}$ \\
\midrule
Re-Attention & $0.633$ & $0.608$ \\
CoarseFine & $0.562$ & $0.632$ \\
BUTD & $0.584$ & $0.646$ \\
RelViT & $0.497$ & $0.642$ \\
CLIP & $0.506$ & $0.595$ \\
FiLM & $0.411$ & $0.521$ \\
\bottomrule
\end{tabular}
\vspace{-0.2cm}
\end{table}
 %
\begin{table}[h!]
\caption{All states with novel objects (bolded) in the CALVIN and BEHAVIOR datasets.}
\label{tab: tab:novel_object_queries} 
\centering
\vspace{5mm}
\footnotesize
\begin{tabular}{cccc}
\toprule
 Dataset & Predicate & Object 1 & Object 2  \\
 \midrule
 \multirow{6}{*}{CALVIN} 
  & OnTop & \textbf{red block} & table \\
  & Stacked & \textbf{red block} & blue block \\  
  & Stacked & \textbf{red block} & pink block \\  
  & TurnedOn & \textbf{led} & -- \\ 
 \midrule
 \multirow{8}{*}{BEHAVIOR} & Inside & \textbf{box} & bottom cabinet \\
  & Inside & \textbf{can} & bottom cabinet \\
  & OnTop & \textbf{bottle} & breakfast table \\   
  & OnTop & \textbf{bottle} & \textbf{chair} \\  
  & OnTop & \textbf{box} & breakfast table \\  
  & OnTop & \textbf{bread} & breakfast table \\  
  & OnTop & \textbf{can} & \textbf{chair} \\   
  & Open & \textbf{refrigerator} & -- \\ 
 \bottomrule
\end{tabular}
\end{table}
 
\subsection{Visualizations on the Inferred Predicate Hierarchy}
In Figure~\ref{fig:poincare_embeddings}, we visualize the joint image-predicate space for BEHAVIOR on the Poincaré disk, highlighting the hierarchical semantic structure captured by \model's embeddings. By grouping the joint image-predicate embeddings by predicate, we uncover the inferred predicate hierarchy. For instance, we see that embeddings for \texttt{NextTo} are positioned closer to the origin compared to those for \texttt{OnLeft}, accurately reflecting their hierarchical relationship—--\texttt{OnLeft} is a more specific case of \texttt{NextTo}. Furthermore, embeddings for \texttt{Touching} are nearest to the origin, consistent with its role as the most general predicate. For example, when one object is \texttt{Inside} or \texttt{OnTop} of another, they are inherently \texttt{Touching}. Similarly, objects that are \texttt{NextTo} or \texttt{OnLeft} are also frequently \texttt{Touching}. This visualization demonstrates that \model captures not only semantic structure but also nuanced hierarchical relationships between predicates.

We further analyze the embeddings for novel predicates after few-shot learning with only five examples. Notably, even with such limited data, \model successfully integrates these novel predicates into the latent space and aligns them with their learned counterparts in semantically consistent regions (e.g., \texttt{OnRight} is near \texttt{OnLeft}). By aligning these predicates in similar regions, \model is able to leverage its existing knowledge of relevant features for learned predicates (e.g., \texttt{OnLeft}) to reason about novel predicates (e.g., \texttt{OnRight}). This alignment highlights that \model effectively encodes the relationships between pairwise predicates in the latent space, enabling generalization to novel predicates with minimal examples.

\begin{figure}[h]
    \centering
    \includegraphics[width=\linewidth]{figures/poincare_embeddings_v1.pdf}
    \caption{Visualizations of the joint image-predicate space for BEHAVIOR on the Poincaré disk, revealing that \model learns a meaningful predicate hierarchy. The novel predicate embeddings are visualized after few-shot learning with 5 examples.}
    \label{fig:poincare_embeddings}
\end{figure}


\subsection{In-Distribution Performance}

Here, we discuss the in-distribution performance of \model in Table~\ref{tab:main} of the main text. We note that in the in-distribution (ID) setting of CALVIN, \model outperforms all prior works except Re-Attention, with only a small margin of $1.4\%$. In the out-of-distribution (OOD) setting, which is our primary focus, \model outperforms Re-Attention by a significant $22.5\%$. Similarly, on ID BEHAVIOR, \model performs comparably to top-performing prior works, surpassing all except RelViT by $0.7\%$; however, in the OOD setting we focus on, \model outperforms RelViT by $8.3\%$. We highlight that \model performs comparably to top-performing prior works in the ID setting, while significantly improving the OOD performance. We focus on the few-shot generalization task and design our method to enforce bottlenecked representations (via a joint image-predicate space), while acknowledging that this might include tradeoffs on ID performance to avoid overfitting to the train distribution.

We also analyze specific cases where \model underperforms on ID examples. For instance, in CALVIN, we hypothesize that \model may struggle with tasks that the baselines may memorize due to their less constrained representations. We show an example in Figure~\ref{fig:specific_example}, and note that for the ID query, \texttt{TurnedOn(lightbulb)}, Re-Attention correctly predicts \texttt{True}, while \model predicts \texttt{False}. However, for the out-of-distribution query, \texttt{TurnedOff(lightbulb)}, which is linguistically similar but semantically opposite, \model generalizes successfully while Re-Attention struggles to adapt. We conjecture that Re-Attention may predict that \texttt{TurnedOn(lightbulb)} is \texttt{True} based solely on the existence of the bulb at the location, instead of learning that the state of the lightbulb depends on its color (yellow is on and white is off). In contrast, we see that although \model's constrained representation may slightly limit learning capacity for ID settings, \model has the potential to conduct better compositional reasoning in OOD scenarios, where \model significantly outperforms baselines.


\begin{figure}[h]
    \centering
    \includegraphics[width=0.6\linewidth]{figures/specific_example_v1.pdf}
    \caption{An example of the ID query, \texttt{TurnedOn(lightbulb)}, and OOD query with a novel predicate, \texttt{TurnedOff(lightbulb)}.}
    \label{fig:specific_example}
\end{figure}

\subsection{Object-centric Encoder Performance}

We see empirically that \model's object-centric encoder performs well even on environments with significant distribution shifts, such as CALVIN. In Figure~\ref{fig:object_mask}, we show an example of how the encoder localizes objects in CALVIN. To adapt to environments with even larger distribution shifts where the performance may decrease, we note that \model's object-centric encoder can be finetuned with more data as well.


\begin{figure}[h]
    \centering
    \includegraphics[width=0.6\linewidth]{figures/object_masks_v1.pdf}
    \caption{\model's object-centric encoder in the CALVIN environment.}
    \label{fig:object_mask}
\end{figure} \clearpage

%
\section{Baseline Details}
\label{section: baseline_implementation}
For all of our baseline methods, we preprocess our input queries by converting the states into questions using the following templates: 
\begin{itemize}
    \item For unary states: ``Is the \{\texttt{\textit{object}}\} \{\texttt{predicate}\}"
    \item For binary states: ``Is the \{\texttt{\textit{object 1}}\} \{\texttt{predicate}\} the \{\texttt{\textit{object }2}\}"
    %
    %
\end{itemize} 

\subsection{Supervised Methods}

We train all of the supervised baselines on the same training data as our method. Below, we describe each baseline and provide implementation details:

\textbf{BUTD \citep{anderson2018butd}.} BUTD uses bottom-up attention to extract image features for important image regions and then top-down attention to focus on image regions based on the input query. We follow the original method, using Faster R-CNN pretrained on Visual Genome to extract bottom-up features for the top $36$ image regions. For the text features, we embed the preprocessed text queries using $300$-dimension word embeddings, initialized with pretrained GloVe vectors, and a GRU. The image and text features are then fed into model, based on the PyTorch implementation of the BUTD model for VQA \citep{yu2020butdgithub}\footnote{https://github.com/MILVLG/bottom-up-attention.pytorch}.  

\textbf{CLIP \citep{shen2021clip}.} We use pretrained CLIP vision and text encoders to extract features for the input image and query, respectively. These features are concatenated and passed through a small $2$-layer network with a hidden layer of dimension $256$ for state classification.

\textbf{CoarseFine \citep{nguyen2022coarse}.} Coarse to Fine learns to reason about scenes with complex semantic information by extracting image and text features at multiple levels of granularity. We follow the official implementation of the Coarse to Fine reasoning framework and use Faster R-CNN to extract image-level features and GRU with $300$-dimensional GloVe embeddings to extract question-level features, which are then fed into the model\footnote{https://github.com/aioz-ai/CFR\_VQA}.
        
\textbf{FiLM \citep{perez2018film}.} FiLM conditions an input image on text by applying learned transformations to the image features. We use a pretrained ViT-16 image encoder and BERT text encoder to extract image and query features. Then, a FiLM layer is applied to condition the image features on the query features, and the conditioned features are passed through a small $2$-layer network with a hidden layer of dimension $256$ for final prediction.

\textbf{Re-Attention \citep{guo2020reattention}.} Re-Attention introduces an attention mechanism to re-attend to objects in the images, based on the answer to the question. We follow the original implementation by using a Faster R-CNN model pretrained on the Visual Genome dataset to extract object-level image features, and $512$-dimensional LSTM initialized with $300$-dimensional GloVe embeddings to extract query features \footnote{https://github.com/gwy-nk/Re-Attention}. 

\textbf{RelViT \citep{ma2022relvit}.} RelViT enhances the reasoning ability of vision transformers by introducing a concept-feature dictionary that enables efficient image feature retrieval during training. This supports a global task to promote relational reasoning and a local task to learn semantic object-centric correspondences. We use the official implementation, with Faster R-CNN to extract image region features, MCAN-Small as our VQA model, and the ImageNet1K-pretrained PVTv2b2 as our vision backbone \footnote{https://github.com/NVlabs/RelViT}.

\textbf{SORNet \citep{yuan2022sornet}.} SORNet extracts object-centric representations from input RGB images, conditioned on a set of object queries represented as images of the objects, to enable generalization to unseen objects on various spatial reasoning tasks. It performs state classification by training readout networks to predict spatial relations based on the object embeddings. For a fair comparison to our method and other baselines, we use MDETR \citep{kamath2021mdetr} to detect regions corresponding to object text, resize then to $32 \times 32$, and then use them as the input object images to train SORNet. We train readout networks for each training state in our dataset \footnote{https://github.com/wentaoyuan/sornet}. Since SORNet requires training a separate network for each predicate, we only evaluate it on in-distribution states.

\subsection{Pretrained Large Vision Language Models (VLM)}
All of the pretrained large VLM baselines are evaluated inference-only.

\textbf{BLIP-2 \citep{li2023blip}.} We use BLIP-2 leveraging the OPT-2.7b language model and treat VQA as an open-ended answer generation problem. The input image is provided along with a query using the following format: ``Question: \{state query as a question\} Answer:"

\textbf{GPT-4V \citep{gpt}.} We provide GPT-4V with the input image and a prompt based on the following template:

\begin{figure}[!ht]
    \centering

\begin{tcolorbox}[
title={\small \textbf{Prompt Template For GPT-4V Inference}},
width=0.99\columnwidth]
\fontsize{8pt}{8pt}\selectfont
\ttfamily
    Given an image of a scene, you will answer a question regarding the states and relationships of objects in the scene. The question is the following: \\
    
    \{state query as a question\} \\

    You need to carefully examine the image, thoughtfully consider the objects in the scene, and analyze their states and relationships before answering the question. \\

    Provide your answer as True or False, and strictly follow this response format:\\
    Answer: [insert your answer as True or False here]\\
    Reasoning: [insert your reasoning here] 
\end{tcolorbox}

\caption{Prompt template for GPT-4V experiments.}
\label{fig: llm_gpt4v_prompt}
\end{figure}

\textbf{ViperGPT \citep{suris2023vipergpt}.} We use the official ViperGPT implementation with Blip-2 Flan-T5 XXL as the pretrained model and GPT-4 for code generation. Our data is formatted according to the ViperGPT specifications, with the input image and query as a question.

\subsection{Ablation Details}

Here, we provide a clear breakdown of our ablation model architectures from Table~\ref{tab:ablation} and explain how we add each component.

\textbf{Supervised model. We start with a supervised baseline model, which uses an image encoder and text incoder initialized with CLIP and BERT weights, respectively.The embeddings from both encoders are concatenated and passed through a small MLP with three linear layers for classification, and the full model is trained with a binary cross-entropy loss based on the ground truth labels (True or False). We then progressively add each component of \model.}

\textbf{+ Object-centric encoder.} First, we incorporate the object-centric encoder by replacing the image encoder, text encoder, and concatenation step with our proposed object-centric encoder, while retaining the MLP and loss.

\textbf{+ Predicate triplet loss.} Next, we introduce the predicate triplet loss by adding this term to the total loss function without changing the architecture.

\textbf{+ Norm regularization loss.} We further add the norm regularization loss to get the total loss function with all components, as described in Section~\ref{sec:method}

\textbf{+ Hyperbolic metric.} Finally, we lift the scene representation to hyperbolic space using an exponential map and replace the first two linear layers in the MLP with two hyperbolic linear layers of the same size. We also use the Poincaré distance metric instead of the Euclidean metric in the self-supervised losses, yielding our final model (\model).
 \clearpage

%
\section{Hyperbolic Geometry Preliminary}
\label{section: hyperbolic_prelim}

We briefly introduce the Poincaré ball model of hyperbolic space and hyperbolic neural networks. For a more detailed explanation, we refer the reader to \citet{cannon1997hyperbolic} and \citet{ganea2018hyperbolic}.

As discussed in the main text, the Poincaré ball is a $d$-dimensional ball of radius $1$, $\mathbb{P}^d = \{x\in\mathbb{R}^n: ||x||<1\}$, where $||\cdot||$ is the Euclidean norm. The ball is equipped with the metric tensor $g_\mathbf{p}=(\lambda_x)^2g_\mathbf{e}$, where $\lambda_x=\frac{2}{1-||x||^2}$ is the conformal factor and $g_e$ is the Euclidean metric tensor (i.e., the Euclidean dot product). This induces the Poincaré distance $\mathbf{d}_\mathbf{p}$ between two points $x, y\in \mathbb{P}^d$ as follows:
\begin{equation*}
    \mathbf{d}_\mathbf{p}(x,y) = \cosh^{-1}\left(1 + 2\frac{||x-y||^2}{(1-||x||^2)(1-||y||^2)}\right)
\end{equation*}

\textbf{Möbius addition.} On the Poincaré ball, Euclidean operations such as addition and multiplication have equivalents to ensure that all operations remain within the hyperbolic space and respect its geometry. Instead of using standard Euclidean addition, Möbius addition is used, which  ensures that the sum of two points on the Poincaré ball still lies within the ball. The Möbius addition for any two points $x,y\in\mathbb{P}^d$ is defined as:
\begin{equation*}
    x \oplus y := \frac{(1+2\langle x,y\rangle + ||y||^2)x + (1-||x||)^2y}{1 + 2\langle x,y \rangle + ||x||^2||y||^2}
\end{equation*}

\textbf{Exponential and logarithmic maps.} To perform operations in hyperbolic space, we use exponential and logarithmic maps to map Euclidean vectors to the hyperbolic space, and vice versa. For any point $z \in \mathbb{P}^d$, the closed form expression of the exponential and logarithmic maps centered around $z$ are defined as:
\begin{equation*}
    \exp_z(y) = z \oplus \left(\tanh\left(\frac{\lambda_z||v||}{2}\right)\frac{v}{||v||}\right)
\end{equation*}
\begin{equation*}
    \log_z(y) =\frac{2}{\lambda_z}\tanh^{-1}(||-z\oplus y||)\frac{-z\oplus y}{||-z\oplus y||}
\end{equation*}
In practice, we use the maps centered at $0$, $\exp_{\mathbf{0}}$ and $\log_{\mathbf{0}}$, to transition between Euclidean space and the Poincaré ball.

\textbf{Hyperbolic neural networks.} \citet{ganea2018hyperbolic} proposes hyperbolic neural networks by defining hyperbolic equivalents of linear maps and bias translations. The hyperbolic linear map $M^\otimes:\mathbb{R}^n\rightarrow\mathbb{R}^m$ of any point $x\in\mathbb{P}^d$ on the Poincaré ball is defined as:
\begin{equation*}
    M^\otimes(x) = (1/\sqrt{c})\tanh \left(\frac{||Mx||}{||x||}\tanh^{-1}\left(\sqrt{c}||x||\right)\right)\frac{Mx}{||Mx||}
\end{equation*}
The translation of a point $x\in\mathbb{P}^d$ by a bias $b\in\mathbb{P}^d$ as:
\begin{equation*}
    x \oplus b = \exp_x\left(\frac{\lambda_\mathbf{0}}{\lambda_x}\log_\mathbf{0}(c)\right)
\end{equation*}
The hyperbolic linear layer is then defined as $M^\otimes(x) \oplus b$. To build a hyperbolic neural network, one simply has to map representations to the Poincaré ball using $\exp_\mathbf{0}$, apply hyperbolic linear layers, and then map back to Euclidean space using $\log_{\mathbf{0}}$. 

\textbf{Disk area of hyperbolic space.}

We provide further details on why the exponential growth of the disc area in hyperbolic space provides a natural and efficient way to represent trees. Note that for a regular tree with a constant branching factor $b$, the number of nodes increases exponentially with the distance from the root, as $(b+1)b^{\mathcal{l}-1}$. We can embed trees in hyperbolic space, as they mirror this exponential growth. For instance, in a two-dimensional hyperbolic space with constant curvature $K = -1$, the circumference of a disc with radiuds $r$ is $2\pi\sinh r$ while the area of a disc is $2\pi(\cosh r - 1)$. Since $\sinh r = \frac{1}{2} (e^r - e^{-r})$ and $cosh r = \frac{1}{2} (e^r + e^{-r})$, both the circumference and area of the disc grow exponentially with the radius.

This exponential growth allows us to efficiently embed tree structures in hyperbolic space: nodes that are $\mathcal{l}$ levels from the root can be placed on the hyperbolic disc with a radius proportional to its level $\mathcal{l}$, while nodes less than $\mathcal{l}$ levels within the sphere. Thus, we see how this property allows hyperbolic space to serve as a continuous representation of discrete trees.


\textbf{Connection between hyperbolic space and hierarchical structure.}
We highlight several prominent prior works who have made theoretical connections between hyperbolic space and trees. Mathematical works such as \citet{gromov1987hyperbolic}, \citet{dyubina2001explicit}, and \citet{hamann2018tree} prove that any finite tree can be embedded into a finite hyperbolic space with approximately preserved distances. A key property of hyperbolic space is its exponentially growing distance, and they show that this underlying property makes hyperbolic space well-suited to model hierarchical structures. Furthermore, works such as \citet{sala2018representation} and \citet{chami2020trees} propose concrete approaches to embed any tree in hyperbolic space with arbitrarily low distortion, establishing upper upper and lower bounds for distortion and further demonstrating the effectiveness of hyperbolic space for hierarchical modeling. 

Notably, \citet{nickel2017poincare} were among the first to explore learning hierarchical representations in hyperbolic space. They found that for data with latent hierarchies, embeddings on the Poincaré ball outperform Euclidean embeddings significantly in terms of representation capacity and generalization ability. Since then, hyperbolic spaces have been increasingly explored for modeling hierarchies across various domains, including NLP~\citep{ganea2018hyperbolic, nickel2018learning, tifrea2018poincar} and computer vision~\citep{khrulkov2020hyperbolic, ermolov2022hyperbolic}, with substantial empirical evidence supporting its efficiency and suitability for modeling hierarchical structures in comparison to Euclidean space. We believe that these prior works provide strong theoretical justification and empirical support for the connection between hyperbolic space and hierarchical structure, which inspires our method.


\textbf{Implementation.} We implement our hyperbolic encoder using the Geoopt package \citep{geoopt2020kochurov}, which provides functions and optimization methods for hyperbolic space \footnote{https://github.com/geoopt/geoopt}.

 \clearpage

%
\section{LLM Prompts for Self-Supervised Losses}
\label {section: llm_prompts}

Here, we present the prompt templates used to extract explicit knowledge of predicates from LLMs. In Figure~\ref{fig: llm_triplet_prompt}, we describe the prompt used to determine the assignment (anchor, positive predicate, and negative negative) for a given triplet of predicates, used in the predicate triplet loss. In Figure~\ref{fig: llm_hierarchy_prompt}, we show the prompt used to determine the hierarchy among a predicate triplet based on specificity, for the norm regularization loss. We query the LLM once before training starts to retrieve the predicate triplet pairs and hierarchy, hence training is not affected by LLM queries.

\begin{figure}[!ht]
    \centering
    
\begin{tcolorbox}[
title={\small \textbf{Prompt Template For Predicate Triplet Assignment}},
width=1\columnwidth]
\fontsize{8pt}{8pt}\selectfont
\ttfamily
    You are given an anchor text query that describes a state of a scene. Given two other text queries describing the state of a scene, you will help determine which of the two queries is more similar to the anchor query.\\
    
    Consider the semantic meaning of the states and the specific aspects of the scene they describe. Additionally, think about how many objects and what kinds of object properties and features you would need to verify if evaluating these states against an image.\\

    The anchor query is the following: \{anchor\}\\

    The other two queries are:\\
    Query 1: \{query1\}\\
    Query 2: \{query2\}\\
    
    You must choose one of the queries as your answer. Respond using the following format:\\
    Answer: [Query 1 or Query 2]
\end{tcolorbox}
\caption{Prompt template for inferring the predicate relations among a triplet with GPT-4.}
\label{fig: llm_triplet_prompt}
\end{figure}

\begin{figure}[ht!]
\centering
\begin{tcolorbox}[
title={\small \textbf{Prompt Template For Triplet Hierarchy Ranking}},
width=1\columnwidth]
\fontsize{8pt}{8pt}\selectfont
\ttfamily
    You are an expert in scene understanding and state hierarchy determination. Given three text descriptions each outlining a potential state of a scene, your task is to establish a hierarchy among these descriptions by identifying which one is the most general, which is the most specific, and which lies in between.\\

    Consider the following when determining the hierarchy:\\
    - The variety and number of objects required by the state.\\
    - The important features of the objects and/or relationships between the objects.\\
    - The level of detail provided about the scene.\\
    - The semantic meaning of each description.\\

    Your goal is to rank these descriptions in order of specificity, from least specific (1) to most specific (3). \\
    
    The three descriptions are:\\
    1. \{anchor\}\\
    2. \{query1\}\\
    3. \{query2\}\\

    You must provide your ranking using the following format:\\
    Least Specific: [content of Description 1, 2, or 3]\\
    Intermediate Specific: [content of Description 1, 2, or 3]\\
    Most Specific: [content of Description 1, 2, or 3]
    
\normalfont
\end{tcolorbox}
\caption{Prompt template for inferring the hierarchy among a triplet with GPT-4.}
\label{fig: llm_hierarchy_prompt}
\end{figure}

\clearpage


\section{Dataset Details}
\label{section: states}

\subsection{Dataset States}
In Tables~\ref{atab: calvin_states} and~\ref{atab:behavior_states}, we provide all of the states included in the CALVIN and BEHAVIOR datasets.
%
\begin{table}[h]
\caption{All states included in the CALVIN dataset.}
\label{atab: calvin_states} 
\centering
\vspace{5mm}
%
\footnotesize
\begin{tabular}{cccc}
\toprule
 State Type & Predicate & Object 1 & Object 2  \\
 \midrule
 ID & Lifted & blue block & --\\  
 ID & OnRight & slider &-- \\
 ID & Open & drawer &-- \\
 ID & TurnedOn & lightbulb &-- \\ 
 ID & Inside & blue block & drawer \\
 ID & Inside & pink block & drawer \\  
 ID & OnTop & blue block & table \\
 %
 ID & Stacked & blue block & pink block \\
 ID & Stacked & blue block & red block \\
 \midrule
 OOD & Closed & drawer & --\\  
 OOD & Lifted & pink block & -- \\  
 OOD & OnLeft & slider & --\\  
 OOD & TurnedOff & lightbulb &-- \\ 
 OOD & Inside & blue block & slider \\ 
 %
 OOD & OnTop & pink block & table \\
 OOD & Stacked & pink block & blue block \\  
 %
 OOD & Under & table & blue block \\
 \bottomrule
\end{tabular}
\end{table} %
\begin{table}[h]
\caption{All states included in the BEHAVIOR dataset.}
\label{atab:behavior_states}
\centering
\vspace{5mm}
%
\footnotesize
\begin{tabular}{cccc}
\toprule
 State Type & Predicate & Object 1 & Object 2  \\
 \midrule
ID & Open & bottom cabinet & --\\
ID & Open & drawer & --\\
ID & Open & microwave & --\\
ID & Open & oven & --\\
ID & Open & top cabinet & --\\
ID & Inside & apple & top cabinet\\
ID & Inside & club sandwich & microwave\\
ID & Inside & pizza & microwave\\
ID & Inside & plate & bottom cabinet\\
ID & Inside & plate & bottom cabinet no top\\
ID & NextTo & apple & coffee cup\\
ID & NextTo & coffee cup & cola bottle\\
ID & NextTo & croissant & cheesecake\\
ID & NextTo & pizza & microwave\\
ID & NextTo & plate & coffee cup\\
ID & OnLeft & apple & coffee cup\\
ID & OnLeft & coffee cup & cola bottle\\
ID & OnLeft & croissant & cheesecake\\
ID & OnLeft & pizza & microwave\\
ID & OnLeft & plate & coffee cup\\
ID & OnTop & apple & plate\\
ID & OnTop & cheesecake & plate\\
ID & OnTop & coffee cup & breakfast table\\
ID & OnTop & cola bottle & countertop\\
ID & OnTop & plate & breakfast table\\
ID & Touching & apple & plate\\
ID & Touching & cheesecake & plate\\
ID & Touching & coffee cup & breakfast table\\
ID & Touching & cola bottle & breakfast table\\
ID & Touching & croissant & plate \\
\midrule
 OOD & Closed & bottom cabinet & -- \\ 
 OOD & Closed & drawer & -- \\ 
 OOD & Closed & microwave & -- \\ 
 OOD & Closed & top cabinet & -- \\ 
 OOD & Contains & bottom cabinet & plate \\
 OOD & Contains & drawer & plate \\ 
 OOD & Contains & top cabinet & drawer \\

 OOD & Inside & apple & microwave \\
 OOD & Inside & coffee cup & top cabinet \\
 OOD & Inside & plate & microwave \\
 OOD & NextTo & apple & plate \\
 OOD & NextTo & plate & microwave \\
 OOD & OnTop & coffee cup & plate \\
 OOD & OnTop & apple & breakfast table \\
 OOD & OnTop & apple & microwave \\     
 OOD & OnLeft & apple & plate \\
 OOD & OnLeft & coffee cup & apple \\
 OOD & OnRight & apple & coffee cup \\ 
 OOD & OnRight & coffee cup & cola bottle \\  
 OOD & OnRight & plate & coffee cup \\ 
 OOD & Touching & apple & breakfast table \\
 OOD & Touching & coffee cup & plate\\
 OOD & Under & breakfast table & coffee cup \\
 OOD & Under & breakfast table & plate \\
 OOD & Under & plate & apple \\
 \bottomrule
\end{tabular}
\end{table} 
\subsection{BEHAVIOR Vision Suite Visualizations}

We include additional examples from BEHAVIOR Vision Suite~\cite{ge2024behavior} in Figure~\ref{fig:bvs}.

\begin{figure}[h]
    \centering
    \includegraphics[width=\linewidth]{figures/real_world_data_v1.pdf}
    \caption{Visualizations of state classification tasks from the real-world BEHAVIOR Vision Suite dataset.}
    \label{fig:bvs}
\end{figure}

  \end{document}
