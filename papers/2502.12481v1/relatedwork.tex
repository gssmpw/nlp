\section{Related Work}
\vspace{-0.5cm}
\textbf{State classification.}
The ability to understand object states and relationships is essential for a wide range of tasks in computer vision and robotics, including scene understanding, robotic manipulation, and high-level planning \citep{yao2018visualrelationship, yuan2022sornet}. Earlier works that focus on a similar task of visual relationship detection learn to extract object-centric representations from raw images and make predictions based on them \citep{gkioxari2018detecting, yao2018visualrelationship, ma2022relvit, yuan2022sornet}. A more recent approach by \citet{yuan2022sornet} specifically addresses state classification by extracting object-centric embeddings from RGB images and feeding them into trained networks to classify a set of predefined predicates. However, their approach relies on input images of the objects of interest and is limited to predicates seen during training, as it requires a separate network for each predicate. In contrast, our method can few-shot generalize to unseen predicates given only the input scene and query, without any additional annotations or specific object images.

Recent advancements in robotics simulators have additionally enabled scalable training of state classification in simulation and subsequent transfer to real-world settings. Simulators such as CALVIN \citep{mees2022calvin} and BEHAVIOR \citep{li2023behavior} offer varying levels of realism and are widely used in the robotics community to generate large and diverse datasets, supporting data generation across a broad range of states \citep{ge2024behavior}. We train our model on such simulators and show that, compared to prior works, \model is significantly more effective at zero- and few-shot generalization to real-world state classification tasks.

\vspace{-0.1cm}
\textbf{Question-answering approaches.}
Several key advancements in visual question answering (VQA) have been driven by the development of pretrained large vision-language models (VLMs). These models are trained on extensive image and text data, leading to a unified visual-language representation that can be applied to perform various downstream tasks \citep{shen2021clip, li2023blip, gpt}. Approaches such as Viper-GPT \citep{suris2023vipergpt} further leverage the power of foundation models by composing LLMs to generate programs that pretrained VLMs can execute. However, despite their strong general-purpose capabilities, these models still struggle with questions involving visual and spatial relationships \citep{tong2024eyes}.

On the other hand, a separate class of VQA methods are models trained directly for the VQA task. These approaches follow a general framework of extracting visual and textual features, combining them into a multimodal representation, and learning to predict the answer. Convolutional neural networks (CNNs) and transformers are widely used for feature extraction, with various techniques for fusing the features. FiLM \citep{perez2018film} is an early, modular approach that applies a feature-wise transformation to condition the image features on the text features, while BUTD \citep{anderson2018butd} and Re-Attention \citep{guo2020reattention} are representative attention-based methods that localize important regions based on the question. Furthermore, many approaches, including modular, graph-based, or neurosymbolic approaches, introduce more explicit reasoning to better model the relationships and interactions between objects \citep{andreas2016neural, yi2018neural, ma2022relvit, nguyen2022coarse, wang2023vqa}. Our work lies in this latter class of methods, designed and trained for state classification. In contrast to prior works, we not only encode visual features and textual features of predicates but also learn to capture the inherent predicate hierarchy in a joint image-predicate latent space.

\vspace{-0.1cm}
\textbf{Hyperbolic representations for hierarchy.}
In recent years, several works have explored the benefits of using hyperbolic space to model hierarchical relationships, as it is particularly well-suited for capturing hierarchical structures \citep{ganea2018hyperbolic, nickel2018learning}. In computer vision, prior works have focused on learning hyperbolic representations for image embeddings, demonstrating their effectiveness across various tasks such as semantic segmentation and object recognition \citep{khrulkov2020hyperbolic, liu2020hyperbolic, atigh2022hyperbolic, ermolov2022hyperbolic, Ge_2023_CVPR}. Hyperbolic embeddings allow models to naturally represent hierarchical relationships between various entities, such as objects and scenes or different categories, leading to improved performance and generalization in such tasks \citep{weng2021unsupervised, Ge_2023_CVPR}. Several approaches further incorporate self-supervised learning to learn the underlying hierarchical structure without the need for explicit labels, instead leveraging proxy tasks such as contrastive learning \citep{hsu2021capturingimplicithierarchicalstructure, Ge_2023_CVPR, yue2023hyperbolic}. Recently, \citet{desai2023hyperbolic} proposed a contrastive method to learn joint representations of vision and language in hyperbolic space, yielding a representation space that captures the underlying structure between images and text. Inspired by these works, \model learns a predicate hierarchy in hyperbolic space informed by language and the pairwise relations between predicates, and uses it to conduct few-shot generalization to unseen state classification queries. 
%
%