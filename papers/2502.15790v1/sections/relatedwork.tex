This section provides the mathematical formulation of pruning and reviews existing work on pruning techniques.

\subsection{Problem Setup}

Consider a pre-trained deep neural network (DNN) $f(\theta;x)$ parameterized by $\theta \in \mathbb{R}^d$ and input $x$. Pruning produces a sparse sub-network $f(\theta \odot m; x)$, where $m \in \{0,1\}^d$ is a binary mask, and $\odot$ denotes element-wise multiplication. Sparsity $\kappa \in [0,1]$ is the proportion of parameters set to zero.
Pruning assigns scores \( z \in \mathbb{R}^d \) to parameters importance, using methods ranging from simple weight magnitude to loss-aware based pruning scores.



\subsection{Related Work}

\textbf{Magnitude-Based Pruning (MP)}  
is a simple and widely used pruning strategy~\cite{hanprune, lth, Using_Relevance, li2017pruning, SynFlow, Renda2020Comparing, gordon2020compressing, Comparing_Biases, liu2021sparse, DNNShifter}. MP ranks weights based on their absolute values:
\begin{equation}
z_i = |\overline{\theta}_i|.
\label{eq:mp_score}
\end{equation}
It prunes parameters with the smallest magnitudes, which is computationally efficient. However, MP does not account for the impact of pruning on the loss function, which can result in suboptimal pruning decisions.

\textbf{Impact-Based Pruning (IP)}  explicitly considers the loss function to guide pruning decisions \cite{brain_damage, OBS, WoodFisher}. The impact of pruning is quantified as a second-order Taylor expansion of the loss function $\mathcal{L}$ centered at the pre-trained weights $\overline{\theta}$:
\begin{equation}
\mathcal{L}(\overline{\theta} + \delta\theta)-\mathcal{L}(\overline{\theta}) 
= \delta\theta^\top \nabla \mathcal{L}(\overline{\theta})
+ \tfrac{1}{2}\delta\theta^\top H \delta\theta + O(\|\delta\theta\|^3),
\label{eq:taylor_expansion}
\end{equation}
where $H = \nabla^2 \mathcal{L}(\overline{\theta})$ is the Hessian.

Assuming $\overline{\theta}$ represents a local minimum of the loss (as is often the case for pre-trained networks), the gradient term $\nabla \mathcal{L}(\overline{\theta}) = 0$. For small perturbations $\delta\theta$, the higher-order terms become negligible, leading to the local quadratic approximation:
\begin{equation}
\mathcal{L}(\overline{\theta} + \delta\theta)-\mathcal{L}(\overline{\theta}) \approx \tfrac{1}{2}\delta\theta^\top H \delta\theta.
\label{eq:quadratic_approximation}
\end{equation}


Below we review key IP methods that build on this quadratic approximation.


\textit{Optimal Brain Damage (OBD)} improves on MP by explicitly estimating the increase in loss due to pruning~\cite{brain_damage}. Assuming the Hessian $H$ is diagonal, the pruning score for a weight $\overline{\theta}_i$ is:
\begin{equation}
z_i = \frac{\overline{\theta}_i^2}{2H_{ii}}.
\label{eq:obd_score}
\end{equation}
OBD ranks weights based on their impact on loss, using a diagonal Hessian approximation, but ignores parameter interactions.

\textit{Optimal Brain Surgeon (OBS)} generalizes OBD by considering the full Hessian to capture cross-parameter interactions~\cite{brain_surgeon}: %The pruning score and the corresponding weight updates are given by:
\begin{equation}
z_i = \frac{\overline{\theta}_i^2}{2[H^{-1}]_{ii}}, \quad 
\delta\theta^{*} = \frac{-\overline{\theta}_i [H^{-1}] e_i}{[H^{-1}]_{ii}}.
\label{eq:obs_score_update}
\end{equation}
Here, $z_i$ represents the pruning score, and $\delta\theta^{*}$ defines the Hessian-based weight updated applied to the unpruned weights. OBS is computationally expensive for modern networks due to the cost of inverting the Hessian $H$; nonetheless, it outperforms MP and OBD.


\textit{Modern Hessian-Based Methods:}  
To reduce the computational cost of OBS, WoodFisher~\cite{WoodFisher} introduces block-diagonal approximations of the Hessian via the empirical Fisher information matrix derived from a subset of training data:
\begin{equation}
H \approx \frac{1}{n}\sum_{i=1}^n \nabla \ell_i(\overline{\theta})\nabla \ell_i(\overline{\theta})^\top,
\label{eq:fisher_approximation}
\end{equation}
where $\ell_i(\overline{\theta})$ is the loss for a single data point. This approximation reduces computational overhead but still focuses on pruning individual weights, without explicitly accounting for interactions between multiple weights.


\textit{Pruning Multiple Weights:} 
Combinatorial Brain Surgeon (CBS)~\cite{CBS} considers the joint effect of pruning multiple weights simultaneously, outperforming WoodFisher. However, its reliance on a dense Hessian $H \in \mathbb{R}^{p \times p}$ makes it computationally intensive, taking hours to prune MobileNet and is not scalable for large networks, such as ResNet-50.
CHITA~\cite{CHITA} uses memory-efficient quadratic approximations for faster pruning than CBS but still relies on Hessian-based updates, modifying unpruned weights rather than identifying existing sparse sub-networks in the original parameter space.


