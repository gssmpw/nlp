In \secref{lang}, we presented a rewriting semantics consisting of just
two rules (corresponding to optimization of a segment and compaction)
%
and
%
proved that any saturating rewrite that applies the two rules to
exhaustion yields a locally optimal circuit.
%
This result immediately suggests an algorithm: simply apply the
rewriting rules until they no longer may be applied, breaking ties
between the two rules arbitrarily.
%
Even though it might seem desirable due to its simplicity, such an
algorithm is not efficient, because searching for a segment to
optimize requires linear time in the size of the circuit (both in
worst and the average case), yielding a quadratic bound for
optimization.
%
For improved efficiency, it is crucial to reduce the search time
needed to find a segment that would benefit from optimization.

Our algorithm, called \algname{}, controls search time by using a
circuit cutting-and-melding technique.
%
The algorithm cuts the circuit hierarchically into smaller
subcircuits, optimizes each subcircuit independently.
%
The hierarchical cutting naturally reduces the search time for the
optimizations by ensuring that most of the optimizations take place in
the context of small circuits.
%
Because the algorithm optimizes each subcircuit independently, it can
miss crucial optimizations.
%
To compensate for this, the algorithm melds the optimized subcircuits
and optimizes further the melded subcircuits starting with
the \defn{seam}, or the boundary between the two subcircuits.
%
The meld operation guarantees local optimality and does so efficiently
by first optimizing the seam and further optimizing into each
subcircuit only if necessary.
%
By melding locally optimal subcircuits, the algorithm can guarantee
that the subcircuits or any of their ``untouched'' portions (what it
means to be ``untouched'' is relatively complex) remain optimal.
%
We make this intuitive explanation precise by proving that the
algorithm yields a locally optimal circuit.
%
We note that circuit cutting techniques have been studied for the
purposes of simulating quantum circuits on classical
hardware~\cite{circuit-cutting-2020,tang-cutqc-2021,bravyi-future-qc-2022}.
%
We are not aware of prior work on circuit melding techniques that can
lazily optimize across circuit cuts.


\begin{comment}
The efficiency properties of the algorithm are far from obvious.
%
One specific complexity is data dependency: the execution of the
algorithm depends not only on the size of the circuits but also the
specific gates in the circuit, and more generally the entire circuit
structure, because optimization themselves depend on the gates, which
in turn impact the run time of the algorithm.
%
We show that it is possible to account for this data dependency by
using a combination of cost-sensitive analysis and amortization
techniques.
%
Specifically, our bound amortizes optimization costs against the cost
improvement, defined as the difference between input and output costs.
%
Note that in cases where optimization concerns total gate count or the
count of certain gates (e.g., T gates or CNOT gates), the cost
improvement is upper bounded by the circuit size.
\end{comment}



% In the rest of the section, we present our algorithm, prove that it always returns a locally optimal
% circuit, and analyze its efficiency.
% %
% \figref{lopt-code} shows the pseudocode the  \algname{}
% algorithm and its key subroutines.
% %
% We describe the \emph{meld} operation in \secref{algorithm::meld}, and prove its
% efficiency.
% %
% Then, in \secref{algorithm::soam}, we prove the efficiency of optimization phases.
% %
% Finally, in \secref{algorithm::lopt}, we prove the efficiency.
% %
% \todo{Update for new subsection structure in this section which changed recently}
%
% In \secref{algorithm::soam}, we describe \coam{}, and in
% \secref{algorithm::correctness} we prove that the \algname{} algorithm
% guarantees that the output is correct (locally optimal and equivalent to
% the input).
% %
% In \secref{algorithm::cost-analysis} we analyze cost and prove that \algname{}
% requires nearly linear time.
% %

% \todo{how is the cost assumption different than the semavtics?  Pull up top.}t

% The algorithm is parameterized by a cost function $\cost$, an oracle optimizer
% $\oracle$, and a segment span $\Omega$.
% %
% The algorithm works for cost functions that are additive under circuit
% concatenation, i.e., the cost of a circuit obtained by concatenating
% two circuits is equal to the sum of costs of the individual circuits
% (Definition~\ref{def:additive-cost}).
% %
% In particular, we note that the algorithm can guarantee local optimality for a variety of
% gate count metrics, such as total number
% of gates, or gates of a particular kind (e.g., T-gates).


% \subsection{The utility of larger windows}

% A key idea used by our algorithm is that the \oracle{} can be applied to segments that
% are a constant fraction longer than $\Omega$ to effectively optimize many $\Omega$-segments
% all at the same time.
% %
% In particular, in our algorithms in this section, we will apply the \oracle{} to segments of
% length at most $2\Omega$; from this, we desire the guarantee that the output is \emph{segment optimal},
% i.e., that every $\Omega$-segment of the output is optimal.
% %
% To prove this, we require the following assumption on the oracle.
% %
% \[
%   \costof{\oracle{}(C_1 ; C_2)} \leq \costof{\oracle{}(C_1)} + \costof{\oracle{}(C_2)}
% \]
% %
% Intuitively, this assumption states that \textbf{with more input, the oracle should
% only be able to do better}, which is a desirable property of any optimizer.
% %
% With this assumption in hand, we can then prove \lemref{suboptimize}.

% \begin{lemma}\label{lem:suboptimize}
% For any $C' = \oracle{}(C)$, if $\costof{C'} = \costof{C}$ then $\windowopt\Omega{C}$.
% \end{lemma}
% \begin{proof}
% Consider any $(P ; X ; S) = C$ where $\mathsf{length}(X) \leq \Omega$.
% %
% It suffices to show $\costof{\oracle{}(X)} = \costof X$.
% %
% \todo{This can be done inductively... it uses additivity, monotonicity (compatibility),
% and the assumption on the oracle}
% \end{proof}



\subsection{The algorithm}
\label{sec:opt-phase}

\input{fig/fig-opt}


\figref{lopt-code} shows the pseudocode for our algorithm.
%
The algorithm (\lstinline{OAC}) organizes the computation into rounds,
where each round corresponds to a recursive invocation
of \lstinline{OAC}.
%
A round consists of a compaction phase (via the function \textsf{compact})
%
and
%
a segment-optimization phase, via the function \textsf{segopt}.
%
The rounds repeat until convergence, i.e., until no more optimization
is possible (at which point the final circuit is guaranteed to
be \emph{locally optimal}).
%
As the terminology suggests, the segment optimization phase always
yields a segment-optimal circuit, where each and every segment is
optimal (as defined by our rewriting semantics).
%
Compaction rounds ensure that the algorithm does not miss optimization
opportunities that arise due to compaction.
%
%% Our experiments show that the number of rounds is quite small, even
%% for large circuits, and the vast majority of all optimizations are
%% performed in the first round.
%% %
%% We specifically observe that the number of rounds does not exceed $3$ in the majority of cases, and does not exceed $11$
%% across all of our benchmarks.
%% %
We also present a relaxed version of our algorithm that stops early
when a user-specified \defn{convergence threshold}
$0 \leq \epsilon \leq 1$, is reached.
%




\myparagraph{Function \textsf{segopt}.}
The function \textsf{segopt} takes a circuit $C$ and produces
a segment optimal output.
%
To achieve this, it uses a divide-and-conquer strategy to cut the
circuit hierarchically into smaller and smaller circuits:
%
it splits the circuit into the
subcircuits $C_1$ and $C_2$, optimizes each recursively, and then
calls \textsf{meld} on the resulting circuits to join them back
together without losing segment optimality.
%
This recursive splitting continues until the circuit has been partitioned into
sufficiently small segments,
specifically where each piece is at most $2\Omega$ in length.
%
For such small segments, the function directly uses the oracle
and obtains optimal segments.

\myparagraph{Function \textsf{meld}.}
\figref{lopt-code} (right) presents the pseudocode of the meld function.
%
The function takes segment-optimal inputs $C_1$ and $C_2$
and returns a segment-optimal circuit that is
functionally equivalent to the concatenation of the input circuits.
%

Given that the inputs $C_1$ and $C_2$ are segment optimal,
all $\Omega$-segments that lie completely within $C_1$ or $C_2$
are already optimal.
%
Therefore, the function only considers and optimizes
``boundary segments''
which have some layers from circuit $C_1$ and other layers from circuit $C_2$.
%

To optimize segments at the boundary,
the function creates a ``super segment'', named $W$,
by concatenating the last $\Omega$ layers of circuit $C_1$
with the first $\Omega$ layers of circuit $C_2$.
%
The function denotes this concatenation as
$C_1[d_1 - \Omega : d_1] + C_2[0 : \Omega]$ (see \lineref{combine}).
% where $C_1[d_1 - \Omega : d_1]$ represents the last $\Omega$ layers of circuit $C_1$
% and $C_2[0 : \Omega]$ represents the first $\Omega$ layers of circuit $C_2$.
The meld function calls the oracle on $W$
and retrieves the $W'$,
which is guaranteed to be segment-optimal because
it is returned by the oracle.
%
The meld function then considers the costs of $W$ and $W'$.

%
If the costs of $W$ and $W'$ are identical, then $W$ is already
segment optimal.  Consequently, all $\Omega$-segments at the boundary
of $C_1$ and $C_2$ are also optimal.
%
The key point is that the ``super segment'' $W$ encompasses
all possible $\Omega$-segments at the boundary of $C_1$ and $C_2$.
%
To see this,
let's choose an $\Omega$-segment at boundary,
which takes the last $i > 0$ layers of circuit $C_1$
and the first $j > 0$ layers from of circuit $C_2$;
we can write this as $C_1[d_1 - i : d_1] + C_2[0 : j]$,
where $d_1$ is the number of layers in $C_1$.
%
Given that this is an $\Omega-$segment and has $i + j$ layers,
we get that $i + j = \Omega$ and $i < \Omega$ and $j < \Omega$.
%
Now observe that our chosen segment $C_1[d_1 - i : d_1] + C_2[0 : j]$
is contained within the super segment $W = C_1[d_1 - \Omega : d_1] + C_2[0 : \Omega]$ (\lineref{combine}),
because $i < \Omega$ and $j < \Omega$.
%
Given that $W$ is segment optimal, our chosen segment is also optimal (relative to the oracle).


Returning to the \textsf{meld} algorithm,
consider the case where
the segment $W'$ improves upon the segment $W$.
In this case, meld incorporates $W'$ into the circuit
and propagates this change to the neighboring layers.
%
To do this, meld works with three segment optimal circuits:
circuit $C_1[0 : d_1 - \Omega]$,
which contains the first $d_1 - \Omega$ layers of circuit $C_1$,
is segment optimal because $C_1$ is segment optimal;
%
the circuit $W'$ is segment optimal because it was returned by the oracle;
%
and the circuit $C_2[\Omega: d_2]$,
which contains the last $d_2 - \Omega$ layers of circuit $C_2$,
is segment optimal because $C_2$ is segment optimal.
%
Thus, we propagate the changes of window $W'$,
by recursively melding these segment optimal circuits.

In \figref{lopt-code},
the function meld first melds the remaining layers of circuit $C_1$ with the segment $W'$,
obtaining circuit $M$ (see \lineref{mrec}),
and then melds the circuit $M$ with the remaining layers of $C_2$.
%


\subsection{Meld Example}
We present an example of how \lstinline{meld} joins two circuits by
optimizing from the ``seam'' out, and does so ``lazily'',  as needed.
%

\begin{figure}
  % \begin{minipage}[b]{0.5\linewidth}
  %     \includegraphics[width=1.3\linewidth, left]{media/transforms.pdf}
  % \end{minipage}%
  % \begin{minipage}[b]{0.5\linewidth}
  %   \includegraphics[width=1.3\linewidth]{media/circuits.pdf}
  % \end{minipage}
  \includegraphics[width=\linewidth]{media/new-ex.pdf}
  \caption{
  The figure shows a three-step meld operation and illustrates
  how it propagates optimizations at the boundary of two optimal circuits.
  %
  At the top, the figure shows specific optimizations ``1'' and ``2'',
  and towards the bottom,
  the figure shows the optimization steps of meld.
  %
  Before the first step,
  two individual circuit segments that are optimal are separated by a dashed line.
  %
  Each meld step considers a segment, represented by a box with solid/dotted/shaded lines,
  and applies an optimization to it, reducing the gate count.
  %
  The first step focuses on the boundary segment within the solid green box,
  overlapping with both circuits,
  and applies ``Optimization 1''.
  %
  This step introduces a flipped $\mathsf{CNOT}$ gate,
  which interacts with a neighboring $\mathsf{CNOT}$ gate
  and triggers ``Optimization 2'' in the purple dotted box.
  %
  The third step merges two neighboring rotation gates in the yellow shaded box.
  %
  }
  \label{fig:propagate}
\end{figure}


%
%
\figref{propagate} shows a three-step meld operation
that identifies optimizations at the boundary of two circuits.
%
All the circuits in the figure are expressed using
the $\mathsf{H}$ gate (Hadamard gate),
the $\mathsf{R_Z}$ gate (rotation around $\mathsf{Z}$),
and the two-qubit $\mathsf{CNOT}$ gate (Controlled Not gate),
which is represented using a dot and an XOR symbol.
%
We first provide background on the optimizations used by meld
and label them ``Optimization 1'' and ``Optimization 2''.
%
Optimization 1 shows that when a $\mathsf{CNOT}$ gate is surrounded by
four $\mathsf{H}$ gates,
all of these gates can replaced by a single
$\mathsf{CNOT}$ gate whose qubits are flipped.
%
Optimization 2 shows that when two $\mathsf{CNOT}$ gates
are separated by a $\mathsf{R_Z}$ gate as shown, they may be removed.
%

The steps in the figure describe a meld operation
on the two circuits separated by a dashed line, which represents their seam.
%
To join the circuits, the meld operation proceeds outwards in both directions
and
and optimizes the boundary segment, represented as a green box with solid lines.
%
The meld applies Optimization 1 to the green segment
and this introduces a flipped $\mathsf{CNOT}$ gate.
%

The meld propagates this change in step 2,
by considering a new segment,
which includes a neighboring layer.
%
We represent this segment by a purple box with dotted lines,
and it contains two $\mathsf{CNOT}$ gates,
one of which was introduced by the first optimization.
%
The meld then applies Optimization 2,
removing the $\mathsf{CNOT}$ gates and
bringing the two rotation gates next to each other.
%
Note that Optimization 2 became possible only because of Optimization 1,
which introduced the flipped the $\mathsf{CNOT}$ gate.
%
In the final step,
the meld considers the segment represented by a yellow box with shaded lines
and performs a third optimization, merging the two rotation gates.
%
Overall,
this sequence of optimizations,
at the boundary of two circuits,
reduces the gate count by seven.
%


\subsection{Correctness and Efficiency}

Because our algorithm cuts the input circuit into subcircuits and
optimizes them independently, it is far from obvious that its output is
segment optimal.
%
We prove that this is indeed the case with \thmref{opt} below.
%
The reason for this is the meld operation that is able to optimize
circuit cuts.
%
We also prove, with \corref{linear-calls}, that even though
meld behaves dynamically and its cost varies from one circuit to
another, it remains efficient, in the sense that the number of calls
to oracle is always linear in the size of the circuit plus the
improvement in the cost.
%

The proofs for these are presented in the provided appendix.

\begin{lem}[Segment optimality of meld]
	\label{lem:meld-is-optimal}
	Given any additive \cost{} function and any segment optimal circuits
	$C_1$ and $C_2$, the result of \lstinline{meld}$(C_1, C_2)$ is a
	segment optimal circuit $C$ and $\costof{C} \leq \costof{C_1} + \costof{C_2}$.
\end{lem}

\begin{theorem}[Segment optimality algorithm] \label{thm:opt}
  For any circuit $C$, the function $\mathsf{segopt}(C)$ outputs
  a segment optimal circuit.
  \end{theorem}


\begin{theorem}[Efficiency of segment optimization] \label{thm:cost}
  % Consider a segment length $\Omega$, an oracle that optimizes for the cost function $\mathbf{cost}$,
  % and a circuit $C$.
  The function $\mathsf{segopt}(C)$ calls the oracle at most
  $\mathsf{length}(C) + 2\Delta$ times on segments of length at most $2\Omega$,
  where $\Delta$ is the improvement in the cost of the output.
  \end{theorem}


\begin{corollary}[Linear calls to the oracle]
  When optimizing for gate count, our $\mathsf{segopt}(C)$ makes a linear,
  $O(\mathsf{length}(C) + \sizeof{C})$, number of calls to the oracle.
\label{cor:linear-calls}
\end{corollary}

We experimentally validate this corollary in \secref{eval-calls},
where we study the number of oracle calls made by our algorithm
for many circuits.




%
\begin{wrapfigure}{r}{0.33\textwidth}
\definecolor{mPurple}{rgb}{0.58,0,0.82}
\lstset{
  basicstyle=\footnotesize\fontfamily{ttfamily}\selectfont, % set the font
  keywordstyle={\color{mPurple}}, % set the keyword style
  morecomment=[l]{//},
  commentstyle=\rmfamily\slshape,
  % commentstyle=\itshape, % set the comment style
  showstringspaces=false, % don't show spaces in strings
  columns=fullflexible, % use proportional spacing
  morekeywords={fun,func,let,val,in,end,case,of,SOME, NONE, and, structure, if, or, else, then, return, def}, % define additional keywords
  mathescape=true, % enable math mode
  escapechar={@},
  keepspaces=true,
  breaklines=true,
  numbers=none,
  numbersep=0pt,
  xleftmargin=0em}
\begin{lstlisting}
def $\mathsf{\algname{}}^*(f, C)$:
  $C'$ = segopt(compact$(C)$)
  if $1 - \frac{\costof{C'}}{\costof{C}} \leq f$:
    return $C'$
  else:
    return $\mathsf{\algname{}}^*(f, C')$
\end{lstlisting}
\caption{$\algname{}^*$ terminates when
the cost improvement ratio falls below the \emph{convergence threshold}, $f$.}
\label{fig:oac-star}
\end{wrapfigure}
%
\subsection{\algname{}*: controlling convergence}
\label{sec:convergence-thresh}
%
We observe that, except for the very last round,
each round of our \algname{} algorithm improves the cost of the circuit.
%
This raises a practical question: how does the
improvement in cost vary across rounds?
%
For the vast majority of our evaluation,
we observed that nearly all optimization (> 99\%) occur in the
first round itself (see \secref{converge});
the subsequent rounds have a small impact on the quality.
% which suggests that local optimality and segment optimality are quite similar
% in practice.
%
Based on this observation,
we propose $\mathsf{\algname{}}^*$ which uses a
convergence threshold $0 \leq \epsilon \leq 1$
to provide control over how quickly the algorithm converges.
%

The $\algname{}^*$ algorithm, in \figref{oac-star},
terminates as soon as the cost is reduced
by a smaller fraction than $f$.
%
For example, if we measure cost as the number of gates and $\epsilon = 0.01$,
then $\algname{}^*$ will terminate as soon as an
optimization round removes fewer than $1\%$ of the remaining gates.
%
Note that $\algname{}^*$ gives two guarantees:
1) the output circuit is segment optimal,
and 2) the fractional cost improvement in the last round is less than $\epsilon$.
%
%
In addition, setting $\epsilon = 0$
results in identical behavior to \algname{} and guarantees local optimality.


