In \secref{lang}, we presented a rewriting semantics consisting of just
two rules (corresponding to optimization of a segment and compaction)
%
and
%
proved that any saturating rewrite that applies the two rules to
exhaustion yields a locally optimal circuit.
%
This result immediately suggests an algorithm: simply apply the
rewriting rules until they no longer may be applied, breaking ties
between the two rules arbitrarily.
%
Even though it might seem desirable due to its simplicity, such an
algorithm is not efficient, because searching for a segment to
optimize requires linear time in the size of the circuit (both in
worst and the average case), yielding a quadratic bound for
optimization.
%
For improved efficiency, it is crucial to reduce the search time
needed to find a segment that would benefit from optimization.

Our algorithm, called \algname{}, controls seach time by using a
circuit cutting technique, which has been studied for the purposes of
simulating quantum circuits on classical
hardware~\cite{circuit-cutting-2020,tang-cutqc-2021,bravyi-future-qc-2022}.
%
More specifically, the algorithm cuts the circuit hierarchically into
smaller subcircuits, optimizes each subcircuit independently.
%
The hierarchical cutting naturally reduces the search time for the
optimizations by ensuring that most of the optimizations take place in
the context of small circuits.
%
Because the algorithm optimizes each subcircuit independently, it can
miss crucial optimizations.
%
To compensate for this, the algorithm melds the optimized subcircuits
and optimizes further the melded subcircuits starting with
the \defn{seam}, or the boundary between the two subcircuits.
%
The meld operation guarantees local optimality and does so efficiently
by first optimizing the seam and further optimizing into each
subcircuit only if necessary.
%
By melding locally optimal subcircuits, the algorithm can guarantee
that the subcircuits or any of their ``untouched'' portions (what it
means to be ``untouched'' is relatively complex) remain optimal.
%
We make this intuitive explanation precise by proving that the
algorithm yields a locally optimal circuit.

The efficiency properties of the algorithm are far from obvious.
%
One specific complexity is data dependency: the execution of the
algorithm depends not only on the size of the circuits but the
specific gates in the circuit, and more generally the entire circuit
structure, because optimization themselves depend on the gates, which
in turn impact the run time of the algorithm.
%
We show that it is possible to account for this data dependency by
using a combination of cost-sensitive analysis and amortization
techniques.
%
Specifically, our bound amortizes optimization costs against the cost
improvement, defined as the difference between input and output costs.
%
Note that in cases where optimization concerns total gate count or the
count of certain gates (e.g., T gates or CNOT gates), the cost
improvement is upper bounded by the circuit size.




% In the rest of the section, we present our algorithm, prove that it always returns a locally optimal
% circuit, and analyze its efficiency.
% %
% \figref{lopt-code} shows the pseudocode the  \algname{}
% algorithm and its key subroutines.
% %
% We describe the \emph{meld} operation in \secref{algorithm::meld}, and prove its
% efficiency.
% %
% Then, in \secref{algorithm::soam}, we prove the efficiency of optimization phases.
% %
% Finally, in \secref{algorithm::lopt}, we prove the efficiency.
% %
% \todo{Update for new subsection structure in this section which changed recently}
%
% In \secref{algorithm::soam}, we describe \coam{}, and in
% \secref{algorithm::correctness} we prove that the \algname{} algorithm
% guarantees that the output is correct (locally optimal and equivalent to
% the input).
% %
% In \secref{algorithm::cost-analysis} we analyze cost and prove that \algname{}
% requires nearly linear time.
% %

% \todo{how is the cost assumption different than the semavtics?  Pull up top.}t

% The algorithm is parameterized by a cost function $\cost$, an oracle optimizer
% $\oracle$, and a segment span $\Omega$.
% %
% The algorithm works for cost functions that are additive under circuit
% concatenation, i.e., the cost of a circuit obtained by concatenating
% two circuits is equal to the sum of costs of the individual circuits
% (Definition~\ref{def:additive-cost}).
% %
% In particular, we note that the algorithm can guarantee local optimality for a variety of
% gate count metrics, such as total number
% of gates, or gates of a particular kind (e.g., T-gates).


% \subsection{The utility of larger windows}

% A key idea used by our algorithm is that the \oracle{} can be applied to segments that
% are a constant fraction longer than $\Omega$ to effectively optimize many $\Omega$-segments
% all at the same time.
% %
% In particular, in our algorithms in this section, we will apply the \oracle{} to segments of
% length at most $2\Omega$; from this, we desire the guarantee that the output is \emph{segment optimal},
% i.e., that every $\Omega$-segment of the output is optimal.
% %
% To prove this, we require the following assumption on the oracle.
% %
% \[
%   \costof{\oracle{}(C_1 ; C_2)} \leq \costof{\oracle{}(C_1)} + \costof{\oracle{}(C_2)}
% \]
% %
% Intuitively, this assumption states that \textbf{with more input, the oracle should
% only be able to do better}, which is a desirable property of any optimizer.
% %
% With this assumption in hand, we can then prove \lemref{suboptimize}.

% \begin{lemma}\label{lem:suboptimize}
% For any $C' = \oracle{}(C)$, if $\costof{C'} = \costof{C}$ then $\windowopt\Omega{C}$.
% \end{lemma}
% \begin{proof}
% Consider any $(P ; X ; S) = C$ where $\mathsf{length}(X) \leq \Omega$.
% %
% It suffices to show $\costof{\oracle{}(X)} = \costof X$.
% %
% \todo{This can be done inductively... it uses additivity, monotonicity (compatibility),
% and the assumption on the oracle}
% \end{proof}



\subsection{The algorithm}
\label{sec:opt-phase}

\input{fig/fig-opt}


\figref{lopt-code} shows the pseudo-code for our algorithm.
%
The algorithm (\lstinline{OAC}) organizes the computation into rounds,
where each round corresponds to a recursive invocation
of \lstinline{OAC}.
%
A round consists of a compaction phase (via function \lstinline{compact})
%
and
%
a segment-optimization phase, via function \lstinline{segopt}.
%
The rounds repeat until convergence, i.e., until no more optimization
is possible (at which point the final circuit is guaranteed to
be \emph{locally optimal}).
%
As the terminology suggests, the segment optimization phase always
yields a segment-optimal circuit, where each and every segment is
optimal (as defined by our rewriting semantics).
%
Compaction rounds ensure that the algorithm does not miss optimization
opportunities that arise due to compaction.
%
%% Our experiments show that the number of rounds is quite small, even
%% for large circuits, and the vast majority of all optimizations are
%% performed in the first round.
%% %
%% We specifically observe that the number of rounds does not exceed $3$ in the majority of cases, and does not exceed $11$
%% across all of our benchmarks.
%% %
We also present a relaxed version of our algorithm that stops early
when a user-specified \defn{convergence threshold}
$0 \leq \epsilon \leq 1$, is reached.
%




\myparagraph{Function \textsf{segopt}.}
The function \textsf{segopt} takes a circuit $C$ and produces
a segment optimal output.
%
To achieve this, it uses a divide-and-conquer strategy to cut the
circuit hierarchically into smaller and smaller circuits:
%
it splits the circuit into the
subcircuits $C_1$ and $C_2$, optimizes each recursively, and then
calls \textsf{meld} on the resulting circuits to join them back
together without losing segment optimality.
%
This recursive splitting continues until the circuit has been partitioned into
sufficiently small segments,
specifically where each piece is at most $2\Omega$ in length.
%
For such small segments, the function directly uses the oracle
and obtains optimal segments.

\myparagraph{Function \textsf{meld}.}
\figref{lopt-code} presents the pseudocode of the meld function.
%
The function takes segment-optimal inputs $C_1$ and $C_2$
and returns a segment-optimal circuit that is
functionally equivalent to the concatenation of the input circuits.
%

Given that the inputs $C_1$ and $C_2$ are segment optimal,
all $\Omega$-segments that lie completely within $C_1$ or $C_2$
are already optimal.
%
Therefore, the function only considers and optimizes
``boundary segments''
which have some layers from circuit $C_1$ and other layers from circuit $C_2$.
%

To optimize segments at the boundary,
the function creates a ``super segment'', named $W$,
by concatenating the last $\Omega$ layers of circuit $C_1$
with the first $\Omega$ layers of circuit $C_2$.
%
The function denotes this concatenation as
$C_1[d_1 - \Omega : d_1] + C_2[0 : \Omega]$ (see \lineref{combine}).
% where $C_1[d_1 - \Omega : d_1]$ represents the last $\Omega$ layers of circuit $C_1$
% and $C_2[0 : \Omega]$ represents the first $\Omega$ layers of circuit $C_2$.
The meld function calls the oracle on $W$
and retrieves the $W'$,
which is guaranteed to be segment-optimal because
its returned by the oracle.
%
The meld function then considers the costs of $W$ and $W'$.

%
If the costs of $W$ and $W'$ are identical, then $W$ is already
segment optimal.  Consequently, all $\Omega$-segments at the boundary
of $C_1$ and $C_2$ are also optimal.
%
The key point is that the ``super segment'' $W$ encompasses
all possible $\Omega$-segments at the boundary of $C_1$ and $C_2$.
%
To see this,
let's choose an $\Omega$-segment at boundary,
which takes the last $i > 0$ layers of circuit $C_1$
and the first $j > 0$ layers from of circuit $C_2$;
we can write this as $C_1[d_1 - i : d_1] + C_2[0 : j]$,
where $d_1$ is the number of layers in $C_1$.
%
Given that this is an $\Omega-$segment and has $i + j$ layers,
we get that $i + j = \Omega$ and $i < \Omega$ and $j < \Omega$.
%
Now observe that our chosen segment $C_1[d_1 - i : d_1] + C_2[0 : j]$
is contained within the super segment $W = C_1[d_1 - \Omega : d_1] + C_2[0 : \Omega]$ (\lineref{combine}),
because $i < \Omega$ and $j < \Omega$.
%
Given that $W$ is segment optimal, our chosen segment is also optimal (relative to the oracle).


Returning to the \textsf{meld} algorithm,
consider the case where
the segment $W'$ improves upon the segment $W$.
In this case, meld incorporates $W'$ into the circuit
and propagates this change to the neighboring layers.
%
To do this, meld works with three segment optimal circuits:
circuit $C_1[0 : d_1 - \Omega]$,
which contains the first $d_1 - \Omega$ layers of circuit $C_1$,
is segment optimal because $C_1$ is segment optimal;
%
the circuit $W'$ is segment optimal because it was returned by the oracle;
%
and the circuit $C_2[\Omega: d_2]$,
which contains the last $d_2 - \Omega$ layers of circuit $C_2$,
is segment optimal because $C_2$ is segment optimal.
%
Thus, we propagate the changes of window $W'$,
by recursively melding these segment optimal circuits.

In \figref{lopt-code},
the function meld first melds the remaining layers of circuit $C_1$ with the segment $W'$,
obtaining circuit $M$ (see \lineref{mrec}),
and then melds the circuit $M$ with the remaining layers of $C_2$.
%
%
Note that both recursive calls satisfy the precondition
that inputs to meld are segment optimal.
%
We now formalize the above intuition by proving,
in \lemref{meld-is-optimal}, that the meld algorithm
produces a segment optimal output if the inputs are segment optimal.


\myparagraph{Segment Optimal Outputs.}
We prove that our \textsf{segopt} and \textsf{meld}
algorithms produce segment optimal circuits.
%
The challenge here is proving that
\textsf{meld} produces segment optimal outputs even though it may decide not to optimize one or both of the subcircuits based on the outcome of an optimization.
% %
%Once we establish the segment optimality of meld,
%the proof that \textsf{segopt} produces segment optimal outputs
%is straightforward.

\begin{lem}[Segment optimality of meld]
  \label{lem:meld-is-optimal}
  Given any additive \cost{} function and any segment optimal circuits
  $C_1$ and $C_2$, the result of \lstinline{meld}$(C_1, C_2)$ is a
  segment optimal circuit $C$ and $\costof{C} \leq \costof{C_1} + \costof{C_2}$.
\end{lem}

\begin{proof}

  We show the lemma by induction on the total cost of the input, i.e.,
  $\costof {C_1} + \costof{C_2}$.
  %
  In the base case, the input cost is zero, and therefore all segments of
  the input circuits have zero cost, due to additivity of the \cost{} function.
  %
  The output of meld in this case is the concatenation $(C_1; C_2)$,
  because the oracle can not improve the boundary segment $W$.
  %
  For the inductive step,
  we consider two cases, depending on whether the oracle optimizes the boundary segment $W$.

  \paragraph{Case I}
  When the oracle finds no improvement in $W$, the output is the concatenation $C = (C_1; C_2)$.
  %
  % In this case, the output cost is $\costof{C_1} + \costof{C_2}$ because
  % our cost function is additive w.r.t. concatenation.
  %
  To prove that the output $C$ is segment optimal,
  we show that every $\Omega$-segment of the output is optimal relative to
  the oracle.
  %
  For any such $\Omega$-segment $X$, there are two cases.
  %
  \begin{itemize}
  \item If $X$ is a segment of either $C_1$ or $C_2$, then $X$ is optimal relative to the oracle
  because the input circuits are segment optimal and $X$ is of length $\Omega$.

  \item If $X$ is not a segment of either $C_1$ or $C_2$, then $X$ is a sub-segment of $W$,
  because the segment $W$ has length $2\Omega$ and contains all possible $\Omega$-segments at the boundary of $C_1$
  and $C_2$. Because the oracle could not improve segment $W$, it can not improve its sub-segment $X$.
  \end{itemize}

  Thus, all $\Omega$-segments of the output are optimal relative to the oracle,
  and the output is segment optimal.

  \paragraph{Case II}
  If the oracle improves the boundary segment $W$,
  then the cost of the optimized segment $W'$ is less than cost of segment $W$, i.e.,
  $\costof{W'} < \costof{W}$.
  %
  In this case, the function first melds the segment $C_1[0 : d_1 - \Omega]$ and the segment $W'$,
  by making a recursive call.
  %
  We apply induction on the costs of the inputs to prove that the output circuit $M$ is segment optimal.
  %
  Formally, we have the cost of inputs
  $\costof{C_1[0 : d_1 - \Omega]} + \costof{W'} < \costof{C_1[0 : d_1 - \Omega]} + \costof{W} \leq \costof{C_1} + \costof{C_2}$,
  because $W'$ has a lower cost than $W$.
  %
  Therefore, by induction, the output $M$ is segment optimal
  and $\costof{M} \leq \costof{C_1[0 : d_1 - \Omega]} + \costof{W'}$.
  % Therefore, by induction, the first recursive call
  % to \lstinline{meld} returns an segment optimal output of size at most
  %

  For the second recursive call $\mathsf{meld}(M, C_2[\Omega : d_2])$,
  we can apply induction because $\costof{M} + \costof{C_2[\Omega : d_2]} < \costof{C_1} + \costof{C_2}$.
  %
  This inequality follows from the cost bound on circuit $M$ above and using that
  $W'$ has a lower cost than $W$.
  %
  Specifically,
  $\costof{M} + \costof{C_2[\Omega : d_2]} \leq \costof{C_1[0 : d_1 - \Omega]} + \costof{W'} + \costof{C_2[\Omega : d_2]}$,
   which is strictly less than
    $\costof{C_1[0 : d_1 - \Omega]} + \costof{W} + \costof{C_2[\Omega : d_2]} = \costof{C_1} + \costof{C_2}$.
  %
  Therefore, by induction on the second recursive call,
  we get that meld returns a segment optimal circuit
  that is bounded in cost by $\costof{C_1} + \costof{C_2}$.
  %
  Note that for both recursive calls, the induction relies on the fact that $W'$ has a lower cost than $W$,
  showing that the algorithm works because the recursion is tied to cost improvement.
  %
\end{proof}

Based on \lemref{meld-is-optimal}, we can show the following theorem.
We leave the proof to
\iffull
\appref{thm-opt}.
\else
the Appendix.
\fi
%We now prove that the hierarchical segment optimization via the
%divide-and-conquer technique always yields a segment optimal circuit.

\begin{theorem}[Segment optimality algorithm] \label{thm:opt}
  For any circuit $C$, the function $\mathsf{segopt}(C)$ outputs
  a segment optimal circuit.
  \end{theorem}


\subsection{Efficiency of segment optimality}
\label{sec:opt-analysis}

% TODO: We can implement the semantics as follows.  We scan the circuit to find a segment that can be optimized, optimize it, and repeat until convergence.  this algorithm can require at least quadratic time, because scanning the circuit takes linear time, and each optimization can reduce as little as a single gate.
% In quadratic time, we can also do compression, by compressing after each optimization.  We want to improve on this, reaching ideally linear time.  To this end, we propose an algorithm that operates in rounds.  In each round, the algorithm only applies optimization rules, and in between rounds it compresses the entire circuit.

% TODO: Highlight the "search" part of the rule: we have to find where to apply

% TODO: add the CircuitCompression algorithm
% idea: take a circuit and construct a new one by incrementally addiing each gate

% In this section,
% we describe our local optimization algorithm \algname{},
% which takes a circuit finds an locally optimal equivalent circuit.
% %
% The local optimality is parameterized by a cost function $\cost$,
% an oracle optimizer $\oracle$, and a segment span $\Omega$.
% %
% Our algorithm assumes these parameters
% and uses the oracle to optimize $(2*\Omega)$-segments of the given
% circuit w.r.t. the cost function.
% %
% The algorithm proceeds in rounds
% and in one round,
% it uses a subroutine called \coam{} to perform local optimizations
% and \emph{compresses} the circuit to fill any gaps that may be created
% due to optimizations.
% %
% The algorithm repeats until no more optimizations can be found
% and at the end,
% it guarantees that the circuit is locally optimal.


% The runtime of the algorithm


% The algorithm works for cost functions that are additive under circuit
% concatenation, i.e., the cost of a circuit obtained by concatenating
% two circuits is equal to the sum of costs of the individual circuits
% (Definition~\ref{def:additive-cost}).
% %
% In particular, the \coam{} algorithm can guarantee (relative) local
% optimality for a variety of gate count metrics, such as total number
% of gates, or gates of a particular kind (e.g., T-gates).
% %



% This section presents an algorithm, called \coam{}, that ensures
% local optimality for a certain class of cost functions.
% %
% The algorithm is parameterized by an \defn{oracle} optimizer that it
% uses to optimize $\Omega$-segments (subcircuits with at most $2\Omega$
% depth).
% %

% One way to guarantee (relative) local optimality is to proceed iteratively.
% %
% At each step, we find a segment of the circuit that is not optimal and
% optimize it by calling the oracle, and repeat until convergence.
% %
% Even though this approach is relatively simple, it would require
% significant time, because each iteration scans the whole
% circuit to find the next segment to optimize.
% %
% Our \coam{} algorithm achieves asymptotically better runtime
% by carefully propagating optimizations.

Having shown that the functions \textsf{segopt} and \textsf{meld}
produce segment optimal outputs,
we now analyze the runtime efficiency of these functions.
%
The runtime efficiency of both functions is data-dependent and varies with the number of optimizations found throughout the circuit.
%
To account for this,
we charge the running time to the cost improvement between the input and the output,
represented by $\Delta$.
%
We prove the following theorem.


\begin{theorem}[Efficiency of segment optimization] \label{thm:cost}
  % Consider a segment length $\Omega$, an oracle that optimizes for the cost function $\mathbf{cost}$,
  % and a circuit $C$.
  The function $\mathsf{segopt}(C)$ calls the oracle at most
  $\mathsf{length}(C) + 2\Delta$ times on segments of length at most $2\Omega$,
  where $\Delta$ is the improvement in the cost of the output.
  \end{theorem}

The theorem shows that our segment optimality algorithm is productive in its use of the oracle.
%
Consider the terms, $\mathsf{length}(C)$ and $2\Delta$, in the bound on the number of oracle calls:
%
The first term, $\mathsf{length}(C)$, is for checking segment
optimality---even if the input circuit is already optimal,
the algorithm needs to call the oracle on each segment and confirm it.
%
The second term $2\Delta$,
shows that all further calls result from optimizations,
with each optimization requiring up to two oracle calls in the worst case.
%
This shows that our algorithm (alongside meld)
carefully tracks the segments on which the oracle needs to be called
and avoids unnecessary calls.


The theorem also highlights one of the key features of our $\mathsf{segopt}$ function:
it only uses the oracle on small circuit segments of length $2\Omega$.
%
Suppose $q$ is the number of qubits in the circuit.
%
Then the $\mathsf{segopt}$ function only calls the oracle on
manageable circuits of size less than or equal to $q * 2\Omega$,
which is significantly smaller than circuit size.
%
This has performance impacts because, for many oracles,
it is faster to invoke the oracle many times on small circuits
rather than invoking the oracle a single time on the full circuit (see \secref{eval}).
%

Since we bound number of oracle calls in terms of the cost improvement $\Delta$,
an interesting question is how large can $\Delta$ be?
%
When optimizing for gate-counting metrics
such as $\mathsf{T}$ count (number of $\mathsf{T}$ gates), $\mathsf{CNOT}$ count (number of $\mathsf{CNOT}$ gates),
gate count (total number of gates),
the cost improvement is trivially bounded by the circuit size,
i.e., $\Delta \leq \sizeof{C}$.
%
This observation shows that our \textsf{segopt}
function only makes a linear number of calls (with depth and gate count)
to the oracle.


\begin{corollary}[Linear calls to the oracle]
  When optimizing for gate count, our $\mathsf{segopt}(C)$ makes a linear,
  $O(\mathsf{length}(C) + \sizeof{C})$, number of calls to the oracle.
\label{cor:linear-calls}
\end{corollary}

We experimentally validate this corollary in \secref{eval-calls},
where we study the number of oracle calls made by our algorithm
for many circuits.
The crux of the proofs of \thmref{cost} and the corollary 
is bounding the number of calls in the \textsf{meld} function,
because it can propagate optimizations through the circuit.
%
We state bound below and leave the proofs to 
\iffull
\appref{thm-meld-num-oracles}.
\else
the Appendix.
\fi
%



% \begin{corollary}
%   Given an optimizer that takes $T(q, \sizeof{C})$ time on a circuit $C$ with $q$ qubits and size \sizeof{C},
%   computing $C' = \mathsf{segopt}(C)$ requires at most $O(T(q, \sizeof{C}) * (\mathsf{length}(C) + \Delta))$ time,
%   where $\Delta = \costof{C} - \costof{C'}$ is the improvement in cost.
% \end{corollary}

% \begin{corollary}
%   Given an optimizer that takes $T(q, \mathsf{length}(C))$ time on a circuit $C$ with $q$ qubits,
%   computing $C' = \mathsf{segopt}(C)$ takes $O((\mathsf{length}(C) + 2\Delta) * T (q, 2\Omega))$ time
% \label{cor:linear-calls}
% \end{corollary}

% The corollary demonstrates that when optimizing for gate count,
% our \textsf{segopt} function scales linearly with both the number of gates and the length/depth
% of the circuit.
% %
% Furthermore, our \textsf{segopt} function and the oracle
% exhibit similar scaling behavior in the number of qubits.


% The corollary shows that when optimizing for gate count,
% our \textsf{segopt} function scales linearly
% with the number of gates and depth of the circuit.
% %
% Both the oracle and our \textsf{segopt} function scale similarly with the number of qubits.

% %


\begin{lem}[Bounded calls to oracle in meld]
  \label{lem:meld-num-oracles}
Computing $C =$ \lstinline{meld}$(C_1, C_2)$
makes at most $1 + 2\Delta$ calls to the oracle,
where $\Delta$ is the improvement in cost,
i.e., $\Delta = \left(\costof {C_1} + \costof{C_2} - \costof{C}\right)$.
\end{lem}




  %

% The meld function and its running time are data dependent,
% as the function decides to make the two recursive calls
% depending on the output of the oracle.
% %
% The recursion tree for a single meld call can be deep,
% as the melding may continue if the oracle continues to find optimizations,
% possibly calling the oracle on segments from  the whole circuit.
% %
% But, because the algorithm recurses only if the oracle finds an optimization,
% we can charge the cost of recursion to the optimization.
% %
% This is formalized in \lemref{meld-num-oracles}.





% ?

% \paragraph{Meld Example.}
% Returning to the example in \figref{propagate},
% we can see the algorithm in action.
% %
% The example we uses $\Omega = 2$ and gate count as the cost function.
% %
% In the first step,
% the function optimizes the $2\Omega-$segment at the boundary of inputs;
% this corresponds to optimizing the $4-$segment in the figure and reducing the gate count.
% %
% Then, the function melds the preceding layers of circuit $C_1$ with the window $W'$,
% corresponding to the second step in \figref{propagate},
% which propagates the optimization
% by considering both the old and new CNot gates.
% %
% Finally, the function melds the remaining layers of $C_2$,
% which corresponds to the third step in \figref{propagate},
% which merges the rotation gates.
%


% \subsection{SOAM Algorithm}

% \figref{lopt-code} presents the pseudocode of the \coam{} algorithm.
% %
% %
% The function $\mathsf{SOAM}$ takes a circuit $C$ and
% returns an segment optimal circuit, where the segment optimality is relative to the oracle.
% %
% To do so,
% the function checks if the circuit is small,
% i.e., its its depth is less than $2\Omega$.
% %
% In this case, the function sends the circuit to the oracle,
% and returns the output of the oracle, which is segment optimal by definition.
% %
% For larger circuits, the function
% $\mathsf{SOAM}$ splits the circuit into two halves
% $C_1$ and $C_2$, and recursively optimizes the two halves,
% retrieving the segment optimal versions $C'_1$ and $C'_2$.
% %
% Then it calls the function $\mathsf{meld}$ which stitches $C'_1$ and $C'_2$
% to produce the segment optimal output circuit.
% %

% We prove that $\mathsf{segopt}(C)$ produces an segment optimal output
% and does so in a bounded number of oracle queries,
% where each query is for small segments, i.e.,
% segments whose length is bounded by $2\Omega$.
% %


% \todo{exponential oracle can be viewed as oversell.  use quadratic. }
% \todo{do not use "huge" in writing to prevent oversell reaction.}
% The corollary shows that the $\mathsf{segopt}$ function can have asymptotic benefits
% for gate count (or T-count) optimization.
% %
% For example, suppose the oracle complexity is exponential in the number of qubits and circuit length,
% i.e., suppose $T(q, d) = O(2 ^ {qd})$.
% %
% Then, using the above corollary,
% the time complexity of the $\mathsf{segopt}$ function is $O(2 ^{2q\Omega}\sizeof{C})$,
% which is bounded by $O(2 ^{2q\Omega}qd)$ (using $\sizeof{C} \leq qd$).
% %
% In this example,
% the base complexity $T(q, d)$ is exponential in circuit length $d$,
% but the $\mathsf{segopt}$ function is effectively linear in the length,
% which is a huge reduction.
% %
% More generally, we can derive a meaningful reduction in complexity if the oracle
% has at least quadratic (in length) time complexity,
% because the $\mathsf{segopt}$ function only sends a linear number of fixed-length circuits to the oracle.
% %
% % This efficiency, however, comes at a potential cost to circuit quality,
% % because the $\mathsf{segopt}$ function guarantees segment optimality whereas
% % the base optimizer can work on the whole circuit and, in principle, find more optimizations.
% % %
% % Our experiments, however, suggest that this potential cost does not show up in practice (\secref{eval}).
% %

% \footnote{We can derive a meaningful, but less pronounced reduction in complexity even when the oracle has quadratic cost}.


% As another corollary of the above theorems,
% we get that if we have an optimizer that returns globally optimal circuits,
% then the $\mathsf{segopt}$ function can produce circuits that are segment optimal
% in an absolute sense,
% i.e., every $\Omega-$segment of the output is globally optimal.

% \begin{corollary}
% Given an oracle that produces optimal circuits,
% the $\mathsf{segopt}$ function produces segment optimal circuits using the oracle
% only on segments of size $2\Omega$.
% \end{corollary}


%\subsection{Putting the pieces together: the \algname{} algorithm}
%\label{sec:full-algorithm}
%

The \algname{} algorithm
uses the segment optimality guarantee, given by \textsf{segopt},
to produce a locally optimal circuit.
%
In \figref{lopt-code}, the function \algname{} takes the input circuit $C$
and computes the circuit $C' = \mathsf{segopt}(\mathsf{compact}(C))$.
%
It then checks if $C'$ differs from the input $C$ and if so,
it recurses on $C'$.
% It then checks if $C'$ differs from the input $C$.
% %
% If $C'$ differs from $C$---which could be due to optimization, or compaction, or
% both---then \algname{} continues recursively on $C'$.
% %
% Otherwise, if $C'$ and $C$ are identical,
% it returns the circuit $C'$ because no optimization or compaction occured.
The function repeats this until convergence, i.e.,
until the circuit does not change.
%
In the process, it
computes a sequence of segment optimal circuits $\{C_i\}_{1 \leq i \leq \kappa}$,
where $\kappa$ is the number of rounds until convergence:
\[C \to C_1 \to C_2 \to \ldots \to C_\kappa \quad\text{where}\quad
\begin{array}{c}
\costof{C_i} > \costof{C_{i+1}} \\
\windowopt\Omega{C_i} \\
\locallyopt{\Omega}{C_\kappa}
\end{array}
\]
%
Each intermediate circuit $C_i$ is segment optimal
and gets compacted before optimization in the next round.
%
Given that \algname{} continues until convergence,
the final circuit, $C_\kappa$, is locally optimal.

% This argument subtly hinges upon a property of our segment optimality algorithm, in particular,
% that $\mathsf{segopt}(C)$ differs from $C$ only if the cost has improved.
% %
% The function $\mathsf{segopt}$ and its key subroutine, meld, are careful to ensure this
% by guarding the oracle calls:
% %
% if applying the oracle does not improve the cost, then the output of the oracle is discarded
% (see lines \ref{line:guard1start}-\ref{line:guard1stop} and \ref{line:guard2start}-\ref{line:guard2stop}
% in \figref{lopt-code}).
% %
% (This aligns with the rewriting semantics of \secref{lang}, which only performs optimization
% rewrites if the cost decreases.)
% %

Using \thmref{cost},
we observe that the number of oracle calls in each round is bounded by
$\mathsf{length}(C_i) + 2\Delta_i$,
where $\Delta_i$ is the improvement in cost (i.e., $\Delta_i = \costof{C_i} - \costof{C_{i+1}}$).
%
Thus, the total number of oracle calls performed by \algname{} can be bounded by
$O(\Delta + \mathsf{length}(C) + \sum_{1 \leq i \leq \kappa} \mathsf{length}(C_i))$
where $\Delta = \costof{C} - \costof{C_\kappa}$ is the overall improvement in cost between the input and output.

\begin{theorem}
\label{thm:local-opt}
Given an additive cost function,
the output of $\algname{}(C)$ is locally optimal
and requires $O(\Delta + \mathsf{length}(C) + \sum_{1 \leq i \leq \kappa} \mathsf{length}(C_i))$ oracle calls,
where $C_i$ denotes the circuit after $i$ rounds,
$\kappa$ is the number of rounds,
and $\Delta$ is the end-to-end cost improvement.
\end{theorem}


The bound on the number of oracle calls in \thmref{local-opt} is very general
because it applies to any oracle (and gate set).
%
For particular cost functions, we can use it to deduce a more precise bound.
%
Specifically, in the case of gate count, where
$\costof C = \sizeof C$, we get the following bound.

\begin{corollary}\label{cor:oac-linear-calls}
When optimizing for gate count,
$\algname{}(C)$ performs at most $O(\sizeof{C} + \kappa \cdot \mathsf{length}(C))$ oracle calls.
\end{corollary}

An interesting question is whether or not it is possible to bound the number of rounds, $\kappa$.
%
In general, this will depend on a number of factors,
such as the quality of the oracle and how quickly the optimizations it performs converge
across compactions.
%
In practice (\secref{eval}), we find that the number of rounds required is small, and that
nearly all optimizations are performed in the first round itself.
%
For gate count optimizations in particular, Corollary~\ref{cor:oac-linear-calls} tells us
that when only a small number of rounds are required, the scalability of \algname{} is
effectively linear in circuit size.

%
\begin{wrapfigure}{r}{0.33\textwidth}
\definecolor{mPurple}{rgb}{0.58,0,0.82}
\lstset{
  basicstyle=\footnotesize\fontfamily{ttfamily}\selectfont, % set the font
  keywordstyle={\color{mPurple}}, % set the keyword style
  morecomment=[l]{//},
  commentstyle=\rmfamily\slshape,
  % commentstyle=\itshape, % set the comment style
  showstringspaces=false, % don't show spaces in strings
  columns=fullflexible, % use proportional spacing
  morekeywords={fun,func,let,val,in,end,case,of,SOME, NONE, and, structure, if, or, else, then, return, def}, % define additional keywords
  mathescape=true, % enable math mode
  escapechar={@},
  keepspaces=true,
  breaklines=true,
  numbers=none,
  numbersep=0pt,
  xleftmargin=0em}
\begin{lstlisting}
def $\mathsf{\algname{}}^*(f, C)$:
  $C'$ = segopt(compact$(C)$)
  if $1 - \frac{\costof{C'}}{\costof{C}} \leq f$:
    return $C'$
  else:
    return $\mathsf{\algname{}}^*(f, C')$
\end{lstlisting}
\caption{$\algname{}^*$ terminates when
the cost improvement ratio falls below the \emph{convergence threshold}, $f$.}
\label{fig:oac-star}
\end{wrapfigure}
%
\subsection{\algname{}*: controlling convergence}
\label{sec:convergence-thresh}
%
We observe that, except for the very last round,
each round of our \algname{} algorithm improves the cost of the circuit.
%
This raises a practical question: how does the
improvement in cost vary across rounds?
%
For the vast majority of our evaluation,
we observed that nearly all optimization (> 99\%) occur in the
first round itself (see \secref{converge});
the subsequent rounds have a small impact on the quality.
% which suggests that local optimality and segment optimality are quite similar
% in practice.
%
Based on this observation,
we propose $\mathsf{\algname{}}^*$ which uses a
convergence threshold $0 \leq \epsilon \leq 1$
to provide control over how quickly the algorithm converges.
%

The $\algname{}^*$ algorithm, in \figref{oac-star},
terminates as soon as the cost is reduced
by a smaller fraction than $f$.
%
For example, if we measure cost as the number of gates and $\epsilon = 0.01$,
then $\algname{}^*$ will terminate as soon as an
optimization round removes fewer than $1\%$ of the remaining gates.
%
Note that $\algname{}^*$ gives two guarantees:
1) the output circuit is segment optimal,
and 2) the fractional cost improvement in the last round is less than $\epsilon$.
%
%
In addition, setting $\epsilon = 0$
results in identical behavior to \algname{} and guarantees local optimality.


\if 0
\input{fig/fig_quadratic_lower_bound}

\subsection{Worst-case number of rounds for \algname{}}
%
Fig.~\ref{fig:quadratic_lower_bound} gives an example showing that a linear number of compactions is necessary in the worst case.
%
The example uses $\Omega=3$.
%
We optimize the solid green segment from (a) to (b) by canceling the two CNOTs.
%
However, at (b), no more optimization can be applied.
%
Although the dashed green segment is not optimal, the two CNOTs are separated enough so that we cannot optimize them.
%
So, we must apply a compaction step to bring the two CNOTs together and then optimize them, as shown in (c).
%
This pattern will continue and can require a number of compactions proportional to the number of qubits.
\fi
% \myparagraph{Linear cost.}
% %
% The algorithm runs in time linear to the size of the circuit,
% including the cost of $\mathsf{meld}$.
% %
% We prove this by counting the number of calls to the oracle.
% %
% All other operations (e.g., \lstinline{split} and \lstinline{concat})
% take $O(1)$ time and there are a constant number of such operations per
% call to the oracle.




% \myparagraph{Oracle complexity.}
% We assume that the oracle takes constant time to optimize its input
% as long as the size of the input is bounded by $2n\Omega$,
% where $n$ is the number of qubits.
% %
% In this model,
% the cost of an algorithm is the number of oracle calls.
% %
% For example,
% the following algorithm verifies segment optimality in linear (in size) number of calls to the oracle:
% the algorithm sends all $\Omega-$segments to the oracle; if the oracle can not improve any of them,
% then the circuit is segment optimal and otherwise, the circuit is not segment optimal.
% %
% This algorithm makes only $(d - \Omega + 1)$ number of calls to the oracle.

% Our goal is to design an algorithm
% that produces segment optimal circuits
% by only making a linear number of calls to the oracle.
% %
% Each call to the oracle may improve a segment by reducing an abstract cost.
% %
% Our algorithm works for the following cost functions.

% We prove that for any circuit $C$, the output
% of $\mathsf{SOAM}(C)$ is segment optimal.
% %
% We start by establishing the key lemma, which shows
% that \lstinline{meld} preserves segment optimality.

% For two circuits $C_1$ and $C_2$,
% the number of times the function $\mathsf{meld}$ invokes
% the oracle is bounded by $1 + \sizeof {C_1} + \sizeof{C_2} - \sizeof {C}$


% \begin{proof}
% We can prove this by induction on the sum $\sizeof{C_1} + \sizeof{C_2}$.
% %
% The function $\mathsf{meld}$ first partitions the circuits $C_1$ and $C_2$
% into three parts: $P$, $W$, and $S$,
% where $P$ is the prefix of circuit $C_1$,
% $W$ is the segment crossing the boundary of $C_1$ and $C_2$,
% and $S$ is the suffix of circuit $C_2$.
% %
% We have $\sizeof{P} + \sizeof{W} + \sizeof{S} = \sizeof{C_1} + \sizeof{C_2}$.
% %
% The function then invokes the oracle on segment $W$.
% %
% If the oracle does not improve the size of the segment,
% the function just returns,
% thereby only making the one call to the oracle and satisfying the lemma.

% When the oracle optimizes the segment and returns $W'$,
% we have that $\sizeof{W'} < \sizeof {W}$.
% %
% From the above equality, we get,
% $\sizeof{P} + \sizeof {W'}  + \sizeof{S} < \sizeof {C_1} + \sizeof {C_2}$.
% %
% In this case,
% the function makes two recursive calls.
% %
% The first call melds the prefix $P$ and the segment $W$.
% %
% Because $\sizeof {P} + \sizeof{W} < \sizeof{C_1} + \sizeof{C_2}$,
% we apply the inductive hypothesis to prove that the first
% call to $\mathsf{meld}$ makes $O(\sizeof {P} + \sizeof{W})$ calls
% to the oracle, which is bounded by $O(\sizeof{C_1} + \sizeof{C_2})$.
% %
% The second call to melds the output of the first call with the suffix $S$.
% %
% Note that the output of meld is never larger than the sum of inputs.
% %
% Thus, the cumulative size of inputs to the second call is bounded by
% $\sizeof{P} + \sizeof {W'}  + \sizeof{S}$.
% %
% Because $\sizeof{P} + \sizeof {W'}  + \sizeof{S} < \sizeof{C_1} + \sizeof{C_2}$,
% we apply the inductive hypothesis to prove that the second call to $\mathsf{meld}$
% makes $O(\sizeof{C_1} + \sizeof{C_2})$ calls to the oracle.
% %
% Because both recursive calls only make $O(\sizeof{C_1} + \sizeof{C_2})$ calls to the oracle,
% the total number of calls is also $O(\sizeof{C_1} + \sizeof{C_2})$.
% \end{proof}
% %

% \begin{lem}[Bounded calls to oracle by SOAM]
% \label{lem:optimize-num-oracles}
%   Computing $C' = \mathsf{SOAM}(C)$ makes at most
%   $\sizeof{C} + 2 * (\costof{C} - \costof{C'}) - 1$ calls to the oracle, assuming
%   $\costof C \geq 2$ and $\Omega \geq 2$.
% \end{lem}
% \begin{proof}
% By induction, in a similar manner to \lemref{meld-num-oracles}. Full proof
% in the Supplementary material.
% \end{proof}


% \begin{theorem}[SOAM takes linear time] \label{thm:time}
%   For any circuit $C$, the algorithm $\mathsf{SOAM}(C)$ runs in time $O(\sizeof{C})$,
%   assuming that the oracle takes constant time on segments of depth $2\Omega$.
% \end{theorem}
% \begin{proof}
% By \lemref{optimize-num-oracles}, the number of calls to the oracle is
% at most $O(\sizeof C)$.
% %
% All other operations (including \lstinline{split}, \lstinline{concat}) take
% $O(1)$ time, and a constant number of such operations are performed per call to
% the oracle.
% %
% Every call to the oracle is on a segment of depth $2\Omega$ or smaller and
% takes constant time, by assumption.
% %
% The overall time is therefore $O(\sizeof C)$.
% \end{proof}




% The function $\mathsf{meld}$ takes two segmenttt optimal circuits $C_1$ and
% $C_2$, and returns an segment optimal circuit that is functionally equivalent to
% their concatenation.
% %
% Since the inputs $C_1$ and $C_2$ are segment optimal,
% all $\Omega$-segments that lie completely in $C_1$,
% or completely in $C_2$ are already optimal by definition.
% %
% The meld only considers those $\Omega$-segments that span the boundary
% between circuits $C_1$ and $C_2$,
% i.e., those segments which have some layers from circuit $C_1$ and other layers from $C_2$.
% %


% As shown in \figref{lopt-code},
% the function $\mathsf{meld}$ creates a $2\Omega$-segment, named $W$,
% by concatenating the last $\Omega$ layers of circuit $C_1$
% with the first $\Omega$ layers of circuit $C_2$,
% leaving behind a prefix $P$ of $C_1$ and a suffix $S$ of $C_2$.
% %
% Note that the $2\Omega$-segment $W$ covers all possible $\Omega$-segments
% at the boundary of $C_1$ and $C_2$.
% %
% This is because an $\Omega$-segment that crosses the boundary between the two circuits
% must contain at least one layer from both sides and has $\Omega$ layers in total.
% %
% Since each segment can contain at most $\Omega$ layers from both circuits,
% the total range of all $\Omega$-segments is $2\Omega$ layers.


% After creating the segment $W$,
% the meld algorithm invokes the oracle on it,
% which returns an segment optimal circuit $W'$.
% %
% If the costs of $W$ and $W'$ are identical,
% then $W$ is segment optimal and there are no optimizations at
% the boundary of $C_1$ and $C_2$.
% %
% In this case, the function $\mathsf{meld}$ returns
% concatenation of layers in $P$, $W$, and $S$.
% %
% Otherwise, the segment $W'$ improves the segment $W$.
% %
% In this case,
% the function meld calls itself recursively, twice:
% first, to meld the prefix $P$ with the (optimized) segment $W'$,
% and then to meld the result of the first recursive call with the suffix $S$.
%

% Note that both recursive calls satisfy the precondition of meld, i.e.,
% its inputs are segment optimal.
% % ?
% In the first recursive call, the circuit $P$ is segment optimal because its a subcircuit
% of circuit $C_1$, and the segment $W'$ is segment optimal
% because it is the returned by the oracle.
% %
% In the second recursive call,
% the circuit $\mathsf{meld}(P, W')$ is segment optimal because $\mathsf{meld}$
% returns an segment optimal circuit and the circuit $S$ is segment optimal
% because its a subcircuit of circuit $C_2$.
% %
% In the next subsection,
% we prove this formally and show that
% the output $\mathsf{meld}(C_1, C_2)$  is segment optimal (\lemref{meld-is-optimal}).
% %
% The overall running time of \coam{} algorithm is $O(\sizeof{C} + \costof{C})$,
% where $\sizeof{C}$ is the number of gates in the circuit.




% each recursive call to $\mathsf{meld}$ is performed
% only after



% Given a quantum circuit, circuit optimizers attempt to find the best
% circuit among functionally equivalent circuits~\cite{quartz-2022, queso-2023}.
% %
% However, the sheer number of these equivalent circuits presents a
% formidable challenge, because it grows exponentially with the size of the
% circuit.
% %
% State-of-the-art optimizers buckle under this exponential growth and
% have to be artifically terminated, typically by using timeouts.
% %
% As a result, the state-of-the-art optimizers are unable to make any
% guarantees on the quality of the optimized circuits and may miss
% available optimizations.


% To tackle this challenge, we propose an algorithm that prunes the
% search space of equivalent circuits by targeting fixed-size
% subcircuits within a given circuit.
% %
% Our algorithm leverages off-the-shelf optimization techniques to
% optimize individual subcircuits,
% and utilizes the obtained results to determine new subcircuits for further
% optimization.
% %
% It proceeds iteratively until there are no subcircuits that can be optimized.


% Our algorithm operates on circuits represented as sequences of layers, and
% considers subcircuits that are contiguous subsequences of layers.
% %
% To control the size of subcircuits,
% we incorporate a parameter $\Omega$, and only choose
% $O(\Omega)$-width subcircuits for optimization, where the \defn{width} of
% a subcircuit is the number of layers.
% %
% When it finishes,
% the algorithm guarantees that the output circuit is
% ${{\boldsymbol{\mathit{\Omega}}}}$\defn{-optimal},
% i.e., all $\Omega$-width subcircuits are optimal w.r.t. the optimizer.


% The algorithm scales linearly with the \defn{size} (number of gates) of the
% circuit, making only $O(\sizeof{C})$ calls to the optimizer and
% using at most $O(\sizeof{C})$ time for identifying and replacing subcircuits.


% \myparagraph{Oracle.}
% %
% Our algorithm uses an off-the-shelf optimizer on small ($O(\Omega)$-width)
% subcircuits.
% %
% We refer to the off-the-shelf optimizer as the \defn{oracle}, to disambiguate
% it from our optimization algorithm.
% %
% For our optimality guarantee, the only assumption we make is that the
% oracle is depth-preserving: i.e., for any circuit $C$, if the oracle optimizes
% this and produces $C'$, then $\textit{depth}(C') \leq \textit{depth}(C)$.


% \myparagraph{Bounded-size subcircuits: $\boldsymbol{k}$-segments.}
% %
% Our algorithm consults the oracle on small subcircuits that we call
% $k$-\emph{segments}, where $k$ is the \defn{width} of the segment.
% %
% A ${{\boldsymbol{\mathit{k}}}}$\defn{-segment} is a contiguous subsequence of
% $k$ layers of the circuit (using all $n$ qubits).
% %
% The total number of a gates in a $k$-segment therefore is at most $kn$.



% We represent a circuit $C = \langle L_0, L_1, \ldots \rangle$ as a sequence of
% layers $L_i$, and each layer is a set of gates, $g$.
% %
% A \defn{subcircuit} is a contiguous subsequence of layers.
% %
% The \defn{width} of a (sub)circuit is the number of layers, and the
% \defn{size} of a (sub)circuit $C$, denoted $|C|$, is the total number of gates,
% i.e., $|C| = \sum_i |L_i|$.


% \myparagraph{Circuit graph.}
% We represent a circuit $C$ with a directed acyclic graph
% consisting of five components
% $(Q_\textsf{src} \with Q_\textsf{snk} \with G \with E \with \Delta)$.
% %
% The graph has three kinds of vertices: sources, sinks, and gates.
% %
% For $n$ qubits, a circuit has $n$ distinguished source vertices
% $Q_\textsf{src}$, and $n$ distinguished sink vertices $Q_\textsf{snk}$,
% representing the input (source) and output (sink) qubit states.
% %
% The gates of the circuit are denoted $G$, and the edges $E$ are the ``wires''
% of the circuit.
% %
% In particular, each edge represents an intermediate qubit
% state which flows between two gates (or between a gate and a source/sink
% vertex).
% %
% The map $\Delta$ labels each edge with its associated qubit.
% %
% % The map $\Delta$ labels each edge of the graph with a qubit in set $Q$.
% %
% Note that for each gate in the set $G$,
% the number of incoming edges is equal to the number of outgoing edges, and
% the sets of incoming and outgoing labels are identical.

% \myparagraph{Subcircuits.}
% A circuit $C'$ is a \defn{subcircuit} of another circuit $C$ if
% it consists of a subset of gates and edges that respects dependencies between
% gates.
% %
% That is, for any two gates in $C'$, all paths between them in $C$ are also in
% $C'$.
% %
% Formally, for $C' = (Q_\textsf{src}' \with Q_\textsf{snk}' \with G' \with E' \with \Delta')$
% and $C = (Q_\textsf{src} \with Q_\textsf{snk} \with G \with E \with \Delta)$, we
% say $C'$ is a subcircuit of $C$ iff:
% \begin{itemize}
%   \item It is a subgraph, i.e., $Q_\textsf{src}' \subseteq Q_\textsf{src}$
%   and $Q_\textsf{snk}' \subseteq Q_\textsf{snk}$
%   and $G' \subseteq G$
%   and $E' \subseteq E$
%   and $\forall e \in E'. \Delta'(e) = \Delta(e)$.
%   \item For every path of gates
%   $p = \{(g_i, g_{i+1})\}_{0 \leq i < k} \subseteq E$,
%   if the endpoints $g_0$ and $g_k$ are in $G'$, then
%   $p \subseteq E'$.
% \end{itemize}
% %
% Intuitively, subcircuits are suitable units for circuit substitution, i.e.,
% any subcircuit can be replaced by an equivalent subcircuit.


% \myparagraph{Vertical circuit splitting.}
% Our algorithm splits circuits vertically, across all qubits.
% %
% To perform a vertical split, edges across the split need to be updated.
% %
% For each edge $(u,v)$ across the split, the edge is
% replaced with $(u, q_\textsf{snk})$ in the left subcircuit, and replaced with
% $(q_\textsf{src}, v)$ in the right subcircuit, where $q_\textsf{src}$ and
% $q_\textsf{snk}$ are the appropriate source and sink vertices for $\Delta(u,v)$.


% \sr{TODO: anything formal needed here for segments?}


% \myparagraph{segment optimality.}
% %
% The segment optimality guarantee, defined below, is a guarantee
% for all subcircuits of width at most $\Omega$.
% %
% \begin{definition}
%   A circuit is \defn{$\boldsymbol{\mathit{\Omega}}$-optimal} w.r.t. an oracle
%   optimizer if the oracle can not improve any subcircuit of width $\Omega$ or
%   smaller.
% \end{definition}


% \begin{figure}
% \includegraphics[width=0.95\columnwidth]{media/split-example.pdf}
% \caption{Example split. Single-lined circles are qubit sources, and double-lined
% circles are qubit sinks. Rectangles are gates. Note that concatenation is
% the inverse operation.}
% \label{fig:split-example}
% \end{figure}



% \myparagraph{$\Omega$-segments.}
% %
% The algorithm proceeds by selecting subcircuits and passing these to the oracle.
% %
% To guarantee segment optimality, the subcircuits selected by the algorithm
% are larger than $\Omega$.
% %
% In particular, the algorithm selects subcircuits that we call
% \emph{$\Omega$-segments}.


% An \defn{$\Omega$-segment} is a subcircuit of size at most $n\Omega$ (where $n$
% is the number of qubits), and is either \emph{initial} or
% \emph{final}.
% %
% An \defn{initial $\Omega$-segment} is the union of all subcircuits of size
% $\Omega$ that begin at the left edge of a circuit; such a segment can be
% constructed by performing a graph traversal starting at the source vertices of
% the circuit, picking at most $\Omega$ edges for each qubit.
% %
% A \defn{final $\Omega$-segment} is defined symmetrically for the right
% edge of a circuit, and can be constructed by performing a backwards graph
% traversal starting at the sink vertices.

% \begin{proof}
% We establish this by counting the number of calls to the oracle
% because other operations such as split and concat take $O(1)$ time.
% %
% The number of times the function $\mathsf{segopt}$ invokes the oracle is bounded
% by $O(\sizeof{C})$.
% %
% We prove this by induction on the size of circuit $\sizeof{C}$.
% %
% The function first partitions the circuit into circuits $C_1$ and $C_2$
% and calls itself recursively on both.
% %
% We can use induction to show that the function calls
% invoke the oracle $O(\sizeof{C_1})$ and $O(\sizeof{C_2})$ times respectively.
% %
% Thus, the total is $O(\sizeof{C_1} + \sizeof{C_2}) = O(\sizeof{C})$.
% %
% The function then calls function $\mathsf{meld}$ on the resulting circuits $C'_1$ and $C'_2$.
% %
% By \lemref{lem:meld-time},
% we have that this call invokes the oracle $O(\sizeof{C'_1} + \sizeof{C'_2})$ times,
% which is bounded by $O(\sizeof{C_1} + \sizeof{C_2})$ because the algorithm
% never increases the circuit sizes.
% %
% Thus, the total number of calls to the oracle is $O(\sizeof{C})$.
% %
% \end{proof}


% \subsection{Discussion}


% \jremark{discuss limitations of omega optimality here}
% \myparagraph{Different cost functions} \\
% \myparagraph{Local minima, and thoughts on global optima}
% meld is neither associative nor commutative.
% \myparagraph{Termination}

% \begin{theorem}

% \end{theorem}
%


\subsection{Circuit Representation and Compaction}
\ur{Maybe move to this to the end.  Say, for the analysis, we assume that split and concat are constant.  We describe in section x, how to achieve this by using a specific circuit representation.

UPDATE: I moved this here.  But I am not sure where it is used.  We
seem to be proving only the number of oracle calls and not end-to-end
run time.

}

Our algorithm represents circuits using a hybrid data structure that
switches between sequences and linked lists for splitting and
concatenating circuits in $O(1)$ time.
%
Initially, it represents the circuit as a sequence of layers,
enabling circuit splits in $O(1)$ time.
%
Later during optimization
when circuit concatenation is required,
the algorithm switches to a linked list of layers,
enabling $O(1)$ concatenation.
%
At the end of each optimization round,
the algorithm uses a function $\mathsf{compact}(C)$
to revert to the sequence of layers representation.
%

The function $\mathsf{compact}(C)$ ensures that the resulting circuit
is equivalent to $C$ while eliminating any unnecessary ``gaps'' in the layering,
meaning that every gate has been shifted left as far as possible.
%
For brevity, we omit the implementation of the $\mathsf{compact}$ function from \figref{lopt-code}.
%
Our implementation in practice is straightforward: we use a single left-to-right
pass over the input circuit and build the output by iteratively adding gates.
%
The time complexity is linear, i.e., $\mathsf{compact}(C)$ requires $O(|C| + \mathsf{length}(C))$ time.
%
%
% Local optimality requires that a layered circuit be as compact as possible,
% and the function $\mathsf{compact}(C)$ satisfies this condition.
%
%


% %
% In particular, if $C' = \mathsf{compact}(C)$ then we have $\compressed{C'}$, using the
% definition of compactness from \secref{lang}; the resulting circuit $C'$ is identical
% to $C$ except that every gate has been shifted left as far as possible.
%
% Our \algname{} algorithm uses this $\mathsf{compact}$ function in two ways.
% \begin{enumerate}
%     \item The output of every \oracle{} call is compacted. The cost of this compaction can be charged
%     against the oracle, because compaction is no more expensive than reading and writing the
%     input and output circuits. In this sense, compacting the output of every oracle call is ``free''
%     from an efficiency perspective.
%     Note that these compactions are not essential for any of our theorems; we include these in the
%     algorithm description because they are useful in practice, helping ensure that every optimization
%     phase is as productive as possible.
%     \item After every optimization phase, the entire circuit is compacted. Note that these compactions
%     are essential for guaranteeing local optimality.
% \end{enumerate}



\subsection{Meld Example}

We present an example of how \lstinline{meld} joins two circuits by
optimizing from the ``seam'' out, and only as needed.
%

\begin{figure}
  % \begin{minipage}[b]{0.5\linewidth}
  %     \includegraphics[width=1.3\linewidth, left]{media/transforms.pdf}
  % \end{minipage}%
  % \begin{minipage}[b]{0.5\linewidth}
  %   \includegraphics[width=1.3\linewidth]{media/circuits.pdf}
  % \end{minipage}
  \includegraphics[width=\linewidth]{media/new-ex.pdf}
  \caption{
  The figure shows a three-step meld operation and illustrates
  how it propagates optimizations at the boundary of two optimal circuits.
  %
  At the top, the figure shows specific optimizations ``1'' and ``2'',
  and towards the bottom,
  the figure shows the optimization steps of meld.
  %
  Before the first step,
  two individual circuit segments that are optimal are separated by a dashed line.
  %
  Each meld step considers a segment, represented by a box with solid/dotted/shaded lines,
  and applies an optimization to it, reducing the gate count.
  %
  The first step focuses on the boundary segment within the solid green box,
  overlapping with both circuits,
  and applies ``Optimization 1''.
  %
  This step introduces a flipped $\mathsf{CNOT}$ gate,
  which interacts with a neighboring $\mathsf{CNOT}$ gate
  and triggers ``Optimization 2'' in the purple dotted box.
  %
  The third step merges two neighboring rotation gates in the yellow shaded box.
  %
  }
  \label{fig:propagate}
\end{figure}


%
%
\figref{propagate} shows a three-step meld operation
that identifies optimizations at the boundary of two circuits.
%
All the circuits in the figure are expressed using
the $\mathsf{H}$ gate (Hadamard gate),
the $\mathsf{R_Z}$ gate (rotation around $\mathsf{Z}$),
and the two-qubit $\mathsf{CNOT}$ gate (Controlled Not gate),
which is represented using a dot and an XOR symbol.
%
We first provide background on the optimizations used by meld
and label them ``Optimization 1'' and ``Optimization 2''.
%
Optimization 1 shows that when a $\mathsf{CNOT}$ gate is surrounded by
four $\mathsf{H}$ gates,
all of these gates can replaced by a single
$\mathsf{CNOT}$ gate whose qubits are flipped.
%
Optimization 2 shows that when two $\mathsf{CNOT}$ gates
are separated by a $\mathsf{R_Z}$ gate as shown, they may be removed.
%

The steps in the figure describe a meld operation
on the two circuits separated by a dashed line, which represents their seam.
%
To join the circuits, the meld operation proceeds outwards in both directions
and
and optimizes the boundary segment, represented as a green box with solid lines.
%
The meld applies Optimization 1 to the green segment
and this introduces a flipped $\mathsf{CNOT}$ gate.
%

The meld propagates this change in step 2,
by considering a new segment,
which includes a neighboring layer.
%
We represent this segment by a purple box with dotted lines,
and it contains two $\mathsf{CNOT}$ gates,
one of which was introduced by the first optimization.
%
The meld then applies Optimization 2,
removing the $\mathsf{CNOT}$ gates and
bringing the two rotation gates next to each other.
%
Note that Optimization 2 became possible only because of Optimization 1,
which introduced the flipped the $\mathsf{CNOT}$ gate.
%
In the final step,
the meld considers the segment represented by a yellow box with shaded lines
and performs a third optimization, merging the two rotation gates.
%
Overall,
this sequence of optimizations,
at the boundary of two circuits,
reduces the gate count by seven.
%

