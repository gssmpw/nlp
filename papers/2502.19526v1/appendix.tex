\if0
% moved back to main paper
\section{Detailed Results for RQ IV: Impact of compaction on the effectiveness of \algname{}}
\label{app:converge}
\figref{num-rounds} shows the number of rounds
and percentage optimizations for each round of our \algname{} algorithm.
%
Similar to \figref{main-gate-count},
we use our \algname{} optimizer, with \voqc{} as the oracle
and convergence ratio $\epsilon = 0.01$,
to optimize the benchmarks in the Nam gate set.
%
We observe that our algorithm converges quickly,
as it terminates after $2$ rounds of optimization.
%
Furthermore,
\textbf{the \algname{} algorithm
	consistently finds over 99\% of the optimizations in the first round itself.}
%
This is because our \algname{} ensures segment optimality directly
after the first round of optimization (see \secref{algorithm}).
%
Segment optimality guarantees that all segments of size $\Omega$ are optimal
with respect to the oracle;
the only difference between segment optimality and local optimality
is that segment optimality does not require compact circuits.
%
But as we observe that,
although compaction can enable some optimizations,
its impact on these benchmarks is minimal.
%
We also analyze these results for $\epsilon = 0$ and observe that
\algname{} produces locally optimal circuits quickly,
requiring $4$ rounds of optimization on average.
%

\input{fig/fig-compression-factor}
\fi


\section{Experiments with Clifford+T Gate Set}
\label{app:clifft}
In this section, we present the results of experiments with the
Clifford+T gate set. These results largely mirror the results
presented in the main body of the paper, showing similar efficiency
improvements and quality guarantees.

\subsection{RQ I: Effectiveness of \algname{} and local optimality}
We validate the effectiveness of local optimality
with the Clifford+T gate set. The Clifford+T gate set contains
the Hadamard ($\mathsf{H}$),
Phase ($\mathsf{S}$),
controlled-NOT ($\mathsf{CNOT}$),
and the $\mathsf{T}$ gate.
Our benchmark suite includes seven circuit families. We generate the Clifford+T circuits by transpiling the preprocessed Nam circuits
using Qiskit and gridsynth~\cite{gridsynth,qiskit-2023}.
%
For \texttt{bwt} and \texttt{hhl}, the two largest circuits of these
families failed to transpile (due to running out of memory) so we exclude
these from the Clifford+T evaluation.

For this gate set,
we run our \algname{} optimizer using \feyntool{}~\cite{amy2019formal} as the oracle optimizer
with segment depth $\Omega = 120$.
%
We describe in the next section, why we chose this value for $\Omega$. 
%
We evaluate the running time and output quality of our optimizer
against the baseline \feyntool{}.
%
The cost function is the $\mathsf{T}$ count, that is the number of $\mathsf{T}$ gates of the circuit.
%


We note that another possibility for optimizing Clifford+T circuits is
the \pyzx{} tool, which uses the ZX-diagrams for optimizations.
%
This tool, however, can require significant time to translate between
ZX-diagrams and the circuit representation.
%
For example, in our experiments, we found that for many circuits, the
\pyzx{} tool spends more than 50\% of its time on average translating
between circuits and diagrams.
%
Because our algorithm invokes the optimizer many times, the
translation times between circuits and diagrams can become a
bottleneck.
%
We therefore use the \feyntool{} in our evaluation, which does not
suffer from this problem, as it operates directly on the circuit.

\begin{figure}
	\centering\small
	\input{fig/prelim.feyn.120.tex}
	\caption{
		The figure shows the optimization results of optimizers
		$\algname{}$ and \feyntool{}, with $\mathsf{T}$ count as the cost function.
		%
		It shows the running time in seconds for both optimizers (lower is better)
		and calculates the speedup of our \algname{} by taking the ratio of the
		two timings.
		%
		The figure also shows the $\mathsf{T}$ count reductions of both tools.
		%
		The results show that our $\algname{}$ delivers excellent time performance
		and runs almost an order of magnitude ($9.9\times$) faster than \feyntool{} on average.
		%
		Our \algname{} optimizer achieves this speedup without any sacrifice in circuit quality,
		producing the same quality of circuits as \feyntool{}.
		%
	}
	\label{fig:feynopt}
\end{figure}


\figref{feynopt} shows the results of this experiment.
%
The figure separates circuit families with horizontal lines
and sorts circuits within families by their size/number of qubits.
%
It presents the initial $\mathsf{T}$ count for each circuit
and the running times of both optimizers,
highlighting the fastest one in bold.
%
It computes the speedup achieved by our \algname{};
a speedup of $10\times$ means our optimizer runs $10\times$ faster.
%
The figure also
shows percentage reductions in $\mathsf{T}$ count achieved by
optimizers \feyntool{} and \algname{}.
%

The results show that our \algname{} generates high-quality circuits
for the Clifford+T gate set reasonably quickly, taking between $4$ seconds and approximately 20 minutes (for a circuit containing over 250,000 $\mathsf{T}$ gates).
%
\if0
and
does so in short running times,
including for circuits containing thousands to hundreds of thousands of $\mathsf{T}$ gates.
%
Our \algname{} optimizer optimizes
all circuits in times ranging from $4$ seconds to $2155$ seconds (< 1 hour).
%
\fi
%
We observe the following:
%
\begin{enumerate}
	\item \textbf{Time Performance:} Our \algname{} is faster for all circuit families,
	except perhaps for some smaller circuits.
	\item \textbf{Scalability: } Within each circuit family,
	the speedup of our \algname{} increases with increasing
	$\mathsf{T}$ counts. It is over $100\times$ faster for some cases and $9.9\times$ faster on average.
	\item \textbf{Circuit Quality:} Our \algname{} matches the $\mathsf{T}$ count reductions of \feyntool{} on
	all benchmarks.
\end{enumerate}

\myparagraph{Summary.}
\figref{feynopt}
shows that
our \algname{} optimizer can be significantly faster, especially for larger circuits, because it scales better, and does so without sacrificing optimization quality when optimizing for $\mathsf{T}$ count.
%
The experiment thus shows that the local optimality approach can work well  for Clifford+T circuits, especially for larger circuits.

\if0
The experiments suggest that local optimality is an effective
optimization criteria for $\mathsf{T}$-count optimization.
%
By focusing on local
optimizations, our \algname{} optimizes for $\mathsf{T}$-count
in a scalable fashion.
%
The approach is almost an order of magnitude faster (9.9x) across circuit
families without any loss in circuit quality.
%
This experiment also demonstrates the versatility of our oracle-based rewriting approach.
%
Our \algname{} shows consistent performance improvements
for both the Nam gate set (\secref{scalability}) and the Clifford+T gate set,
including for different oracles.
\fi

\if 0
For example,
in the ``hwb'' family,
\algname{} runs in $6$ seconds for the $8-$qubit case with $5887$ $\mathsf{T}$ gates
and takes $45.9$ seconds for the $10-$qubit case with $29939$ $\mathsf{T}$ gates.
%
The \feyntool{} optimizer is faster for the smaller $8$-qubit case,
because it runs in $4$ seconds;
Our \algname{} is faster for the other three ``hwb'' cases
with speedups ranging from $5.4\times$ to $55.78\times$.
%
In the ``hhl family'',
our \algname{} optimizer runs $23\times$ faster for the $7$-qubit case
taking $42.6$ seconds compared to \feyntool{}'s $409$ seconds.
%
It runs at least $76\times$ faster for the $9-$qubit case,
as it takes $564$ seconds ($\approx$ 30 minutes) to terminate
and \feyntool{} does not terminate in twelve hours.
%

We observe a similar speedup trend for Clifford+T gate
set as for the Nam gate set (see \secref{nam}).
%
Within any circuit family,
the speedup of our \algname{} optimizer increases with increasing circuit size.
%
For example,
in the ``shor'' family of circuits,
the speedup is less than one for the smallest circuit with $10$ qubits
because our \algname{} runs slower;
for the three larger cases \algname{} runs much faster
and its speedup increases from $1.3\times$ to $14.16\times$ to $63.39\times$
with increasing circuit sizes.
%
For the ``qft'' family, our optimizer is consistently faster
and its speedup trends upwards with increasing circuit sizes.
%
The trend is consistent for all circuit families.

The primary reason our \algname{} delivers scalable performance
is that focuses on local optimizations within the circuit.
%
\algname{} does not spend time on pursuing global optimizations
and instead achieves local optimality in a relatively short amount of time.
%
The key question is: does this approach compromise on circuit quality?
%
Our circuit quality results demonstrate that the answer is no,
and our \algname{} achieves scalability without compromising on quality.


\myparagraph{Optimization Quality.}
The results show that our optimizer \algname{} matches the output quality of \feyntool{}.
%
Both tools achieve significant reductions in $\mathsf{T}$ count ranging from
$20\%-40\%$ across a variety of benchmarks.
%
Note that the baseline \feyntool{} can, in principle, discover more optimizations
because it operates on the full circuit.
%
However, the experiment shows that achieving local optimality,
as guaranteed by our \algname{},
leads to the same quality of circuits.
%
In \secref{var-segment}, we confirm this for a range of $\Omega$ values.
%


% %
% \subsection{Question N/A: Qptimization Quality}

% In answering Question III (Performance), we observed that our \coam{} implementation outperforms existing optimizers, especially as the circuit size increases.
% %
% This naturally raises the question: does this performance improvement come at the cost of optimization quality?
% %
% In this section, we answer this question, and show that there is no noticable reduction in the quality of optimizations and in fact, in some cases, \coam{} improves optimization quality.
% %


\subsubsection{\coam{} with \voqc{} Oracle}

\figref{main-gate-count} shows
the output quality of all optimizers for eight families of circuits.
%
% Each family is separated by horizontal lines,
% with circuits arranged by increasing qubit and gate counts.
% %
The figure shows the original gate count and the percent reduction
in gate count achieved by the respective tools.
%
We observe that our \algname{} and \voqc{} produce
significantly better circuits than
\quartz{} and \queso{} in terms of quality.
%
Our \algname{} and \voqc{} are almost always within 1\% of each other, except for  the ``vqe'' family,
where our optimizer produces smaller (better) circuits than \voqc{}.
%
For example,
in the case with $24$ qubits (``vqe''),
our optimizer improves the gate count
by $60.6\%$ and  \voqc{} improves the gate count by $54.9\%$.
%
It appears that \voqc{} missed some optimizations for the \texttt{vqe} family.
%
To understand this better, we tried running \voqc{} again on its own outpet and observed that it is able to improve quality further in a subsequent run and bridge the quality gap.
\fi


%
\if0
For example,
for the \texttt{hhl} family,
both \algname{} and \voqc{} achieve reductions of around $56\%$,
while \quartz{} and \queso{} are around $26\%$ for $7$ and $9$ qubits,
and less than $1\%$ for $11$ qubits.
%
In the \texttt{statevec} family,
\algname{} and \voqc{} consistently reduce the gate count by $78\%$.
%
However, for the $8-$qubit case, \voqc{} does not finish within our timeout
of twelve hours so we write ``N.A.''.
%
\queso{} also finds comparable reductions for the $5-$qubit benchmark.


Internally,
our \algname{} uses \voqc{} as a subroutine to optimize
small segments of the circuit, rather than optimizing
the entire circuit at once.
%
Specifically, \algname{} considers the circuit in segments
of size $\Omega = 40$ and applies \voqc{} to them.
%
In contrast,
when used as a standalone optimizer,
\voqc{} processes the entire circuit and could theoretically find more optimizations
by considering all possible gates simultaneously.
%
However,
our experiments show that \algname{}'s segment-based approach
achieves the same quality.
%
This suggests that local optimality,
as achieved by our optimizer,
is a good goal for circuit optimization,
because it does not miss any optimizations in practice.
%
As we saw in \figref{main-time},
the approach is significantly faster because it scales better.
%

Indeed across the board, we observe that the output quality
of \algname{} and \voqc{} is almost identical.
%
One interesting exception is the ``vqe'' family,
where our optimizer produces better circuits than \voqc{} in terms of quality.
%
For example,
in the case with $24$ qubits (``vqe''),
our optimizer improves the gate count
by $60.6\%$ and  \voqc{} improves the gate count by $54.9\%$.
%
In these cases, it seems that \voqc{} missed some optimizations and
we tested that running it again on the output circuit
bridges this gap in quality.
%
Our optimizer \algname{} benefits from the guarantee of local optimality.
%
On average \algname{} reduces the gate count by $49.7\%$ across
a range of circuit families and sizes.


Thus, in conclusion,
\textbf{our \algname{} optimizer produces circuits of identical
	quality and is 8x faster on average.}
%
These results demonstrate that the \algname{} is effective for
large circuits and optimizes them efficiently in a scalable fashion.
\fi



\input{fig/fig_oracle_call_vs_input_size}

\subsection{RQ IV: Impact of varying segment size $\Omega$}
\label{app:var-segment}
\input{fig/segment}
\input{fig/tab-omega-clifft}
We evaluate the impact of parameter $\Omega$
on the output quality and running time of our \algname{} algorithm.
%
We use $\algname{}$ on the Clifford+T gate set
and optimize for $\mathsf{T}$ count
with \feyntool{} as the oracle optimizer.
%
\figref{vary-segment} plots the output $\mathsf{T}$ count (number of $\mathsf{T}$ gates)
and the running time against $\Omega$.
%
The figure includes $\Omega$ values $2, 5, 15, 30, 60, 120 \dots 7680$;
we provide the raw data for the plot in \figref{omega-clifft}.
%
For the different values of $\Omega$,
we run our optimizer on the \texttt{hhl} circuit with $7$ qubits,
which initially contains 61246 $\mathsf{T}$ gates.
%

%
The red dotted line in the plots
show the output $\mathsf{T}$ count and running time of the
baseline, where the oracle \feyntool{} runs on the entire circuit.
%
The results show that for a wide range of $\Omega$ values,
our optimizer produces a circuit of similar quality to the baseline $\feyntool$
and typically does so in significantly less time
(with the exception of two values of $\Omega$ at the extremities: $2$ and $7680$).

%

%
\myparagraph{$\mathsf{T}$ count.}
The plot for $\mathsf{T}$ count shows that for small $\Omega$ (around $2$),
increasing it improves the output quality of \algname{}.
%
This is because the local optimality guarantee of $\algname{}$
strengthens with increasing $\Omega$,
as it guarantees larger segments to be optimal.
%
However, the benefits of increasing $\Omega$
become incremental around $\Omega = 60$,
where the $\mathsf{T}$ count stabilizes around 42140 (20 gates from optimal).
%
This demonstrates that local optimality is an effective quality criterion,
generating high-quality circuits
even with relatively small values of $\Omega$ (around $60$).

\myparagraph{Run time.}
One might expect the running time of \algname{}
to increase with segment size $\Omega$ because:
1) \algname{} uses the oracle on segments of size $2\Omega$,
thus each oracle call consumes more time
and 2) \algname{} gives a stronger quality
guarantee---larger the $\Omega$, stronger the guarantee given by local optimality.
%
Indeed, this intuition holds for most values in practice.
%
For $\Omega$ values ranging from $120$ to $7680$,
the running time increases with increasing $\Omega$.
%
In this range,
the increased cost of oracle call dominates the running time,
making it faster to partition circuits into smaller segments
and make many (smaller) calls to the oracle.

However, when $\Omega$ is very small,
we observe the opposite: increasing $\Omega$ reduces the running time.
\begin{wrapfigure}{r}{0.4\textwidth}
	\centering
	\includegraphics[width=0.39\textwidth]{omega_vs_time_zoom.png}
	\caption{Zooming in: Time vs. Omega plot}
	\label{fig:zoom-plot}
\end{wrapfigure}
%
For reference,
we include \figref{zoom-plot},
which zooms in on the running time plot from \figref{vary-segment}
for initial values of $\Omega$, ranging from $2, 5, 15 \dots 120$.
%
For these smaller $\Omega$ values,
although each oracle call is fast,
the number of oracle calls dominates the time cost.
%
The \algname{} algorithm splits the circuit into a large number of small segments
and queries the oracle on each one, resulting in many calls to the optimizer.
%
When a circuit segment is small,
it is more efficient to directly call \feyntool{},
which optimizes it in one pass.
%
For this reason, $\Omega = 120$ is a good value for our optimizer $\algname{}$,
as it does not send large circuits to the oracle,
and also does not split the circuit into a large number of really small segments.
%

Overall,
we observe that for a wide range of $\Omega$ values,
our \algname{} outputs high quality circuits
and does so in a shorter running time than the baseline.
%







%%% ALGO



\section{Correctness of the Segment Optimality Algorithm}

\myparagraph{Segment Optimal Outputs.}
We prove that our \textsf{segopt} and \textsf{meld}
algorithms produce segment optimal circuits.
%
The challenge here is proving that
\textsf{meld} produces segment optimal outputs even though it may decide not to optimize one or both of the subcircuits based on the outcome of an optimization.
% %
%Once we establish the segment optimality of meld,
%the proof that \textsf{segopt} produces segment optimal outputs
%is straightforward.

\begin{lem}[Restatement of \lemref{meld-is-optimal}, Segment optimality of meld]
  %\label{lem:meld-is-optimal}
  Given any additive \cost{} function and any segment optimal circuits
  $C_1$ and $C_2$, the result of \lstinline{meld}$(C_1, C_2)$ is a
  segment optimal circuit $C$ and $\costof{C} \leq \costof{C_1} + \costof{C_2}$.
\end{lem}

\begin{proof}

  We show the lemma by induction on the total cost of the input, i.e.,
  $\costof {C_1} + \costof{C_2}$.
  %
  In the base case, the input cost is zero, and therefore all segments of
  the input circuits have zero cost, due to additivity of the \cost{} function.
  %
  The output of meld in this case is the concatenation $(C_1; C_2)$,
  because the oracle can not improve the boundary segment $W$.
  %
  For the inductive step,
  we consider two cases, depending on whether the oracle optimizes the boundary segment $W$.

  \paragraph{Case I}
  When the oracle finds no improvement in $W$, the output is the concatenation $C = (C_1; C_2)$.
  %
  % In this case, the output cost is $\costof{C_1} + \costof{C_2}$ because
  % our cost function is additive w.r.t. concatenation.
  %
  To prove that the output $C$ is segment optimal,
  we show that every $\Omega$-segment of the output is optimal relative to
  the oracle.
  %
  For any such $\Omega$-segment $X$, there are two cases.
  %
  \begin{itemize}
  \item If $X$ is a segment of either $C_1$ or $C_2$, then $X$ is optimal relative to the oracle
  because the input circuits are segment optimal and $X$ is of length $\Omega$.

  \item If $X$ is not a segment of either $C_1$ or $C_2$, then $X$ is a sub-segment of $W$,
  because the segment $W$ has length $2\Omega$ and contains all possible $\Omega$-segments at the boundary of $C_1$
  and $C_2$. Because the oracle could not improve segment $W$, it can not improve its sub-segment $X$.
  \end{itemize}

  Thus, all $\Omega$-segments of the output are optimal relative to the oracle,
  and the output is segment optimal.

  \paragraph{Case II}
  If the oracle improves the boundary segment $W$,
  then the cost of the optimized segment $W'$ is less than cost of segment $W$, i.e.,
  $\costof{W'} < \costof{W}$.
  %
  In this case, the function first melds the segment $C_1[0 : d_1 - \Omega]$ and the segment $W'$,
  by making a recursive call.
  %
  We apply induction on the costs of the inputs to prove that the output circuit $M$ is segment optimal.
  %
  Formally, we have the cost of inputs
  $\costof{C_1[0 : d_1 - \Omega]} + \costof{W'} < \costof{C_1[0 : d_1 - \Omega]} + \costof{W} \leq \costof{C_1} + \costof{C_2}$,
  because $W'$ has a lower cost than $W$.
  %
  Therefore, by induction, the output $M$ is segment optimal
  and $\costof{M} \leq \costof{C_1[0 : d_1 - \Omega]} + \costof{W'}$.
  % Therefore, by induction, the first recursive call
  % to \lstinline{meld} returns an segment optimal output of size at most
  %

  For the second recursive call $\mathsf{meld}(M, C_2[\Omega : d_2])$,
  we can apply induction because $\costof{M} + \costof{C_2[\Omega : d_2]} < \costof{C_1} + \costof{C_2}$.
  %
  This inequality follows from the cost bound on circuit $M$ above and using that
  $W'$ has a lower cost than $W$.
  %
  Specifically,
  $\costof{M} + \costof{C_2[\Omega : d_2]} \leq \costof{C_1[0 : d_1 - \Omega]} + \costof{W'} + \costof{C_2[\Omega : d_2]}$,
   which is strictly less than
    $\costof{C_1[0 : d_1 - \Omega]} + \costof{W} + \costof{C_2[\Omega : d_2]} = \costof{C_1} + \costof{C_2}$.
  %
  Therefore, by induction on the second recursive call,
  we get that meld returns a segment optimal circuit
  that is bounded in cost by $\costof{C_1} + \costof{C_2}$.
  %
  Note that for both recursive calls, the induction relies on the fact that $W'$ has a lower cost than $W$,
  showing that the algorithm works because the recursion is tied to cost improvement.
  %
\end{proof}

Based on \lemref{meld-is-optimal}, we can prove the following theorem.
%We leave the proof to
%\iffull
%\appref{thm-opt}.
%\else
%the Appendix.
%\fi
%We now prove that the hierarchical segment optimization via the
%divide-and-conquer technique always yields a segment optimal circuit.

\begin{theorem}[Restatement of \thmref{opt}, segment optimality algorithm] %\label{thm:opt}
  For any circuit $C$, the function $\mathsf{segopt}(C)$ outputs
  a segment optimal circuit.
  \end{theorem}

\begin{proof}
	We prove the theorem by induction on $d = \textsf{length}(C)$.
	%
	In the base case, $d \leq 2\Omega$ and the circuit is fed to the oracle,
	thereby guaranteeing segment optimality.
	%
	For the inductive case, the algorithm splits the circuit into
	two smaller circuits $C_1$ and $C_2$, and
	recursively optimizes them
	to obtain segment optimal outputs $C'_1$ and $C'_2$.
	%
	The result $C = \mathsf{meld}(C_1', C_2')$ is then
	segment optimal, by \lemref{meld-is-optimal}.
	%
	% By induction, we have that both are segment optimal.
	% %
	% The algorithm then melds the two circuits, which by
	% Lemma~\ref{lem:meld-is-optimal} results in an segment optimal output.
	%
	% We thus conclude that the resulting output circuit is
	% segment optimal.
\end{proof}


\section{Efficiency of Segment Optimality Algorithm }
\label{sec:opt-analysis}

% TODO: We can implement the semantics as follows.  We scan the circuit to find a segment that can be optimized, optimize it, and repeat until convergence.  this algorithm can require at least quadratic time, because scanning the circuit takes linear time, and each optimization can reduce as little as a single gate.
% In quadratic time, we can also do compression, by compressing after each optimization.  We want to improve on this, reaching ideally linear time.  To this end, we propose an algorithm that operates in rounds.  In each round, the algorithm only applies optimization rules, and in between rounds it compresses the entire circuit.

% TODO: Highlight the "search" part of the rule: we have to find where to apply

% TODO: add the CircuitCompression algorithm
% idea: take a circuit and construct a new one by incrementally addiing each gate

% In this section,
% we describe our local optimization algorithm \algname{},
% which takes a circuit finds an locally optimal equivalent circuit.
% %
% The local optimality is parameterized by a cost function $\cost$,
% an oracle optimizer $\oracle$, and a segment span $\Omega$.
% %
% Our algorithm assumes these parameters
% and uses the oracle to optimize $(2*\Omega)$-segments of the given
% circuit w.r.t. the cost function.
% %
% The algorithm proceeds in rounds
% and in one round,
% it uses a subroutine called \coam{} to perform local optimizations
% and \emph{compresses} the circuit to fill any gaps that may be created
% due to optimizations.
% %
% The algorithm repeats until no more optimizations can be found
% and at the end,
% it guarantees that the circuit is locally optimal.


% The runtime of the algorithm


% The algorithm works for cost functions that are additive under circuit
% concatenation, i.e., the cost of a circuit obtained by concatenating
% two circuits is equal to the sum of costs of the individual circuits
% (Definition~\ref{def:additive-cost}).
% %
% In particular, the \coam{} algorithm can guarantee (relative) local
% optimality for a variety of gate count metrics, such as total number
% of gates, or gates of a particular kind (e.g., T-gates).
% %



% This section presents an algorithm, called \coam{}, that ensures
% local optimality for a certain class of cost functions.
% %
% The algorithm is parameterized by an \defn{oracle} optimizer that it
% uses to optimize $\Omega$-segments (subcircuits with at most $2\Omega$
% depth).
% %

% One way to guarantee (relative) local optimality is to proceed iteratively.
% %
% At each step, we find a segment of the circuit that is not optimal and
% optimize it by calling the oracle, and repeat until convergence.
% %
% Even though this approach is relatively simple, it would require
% significant time, because each iteration scans the whole
% circuit to find the next segment to optimize.
% %
% Our \coam{} algorithm achieves asymptotically better runtime
% by carefully propagating optimizations.

Having shown that the functions \textsf{segopt} and \textsf{meld}
produce segment optimal outputs,
we now analyze the runtime efficiency of these functions.
%
The runtime efficiency of both functions is data-dependent and varies with the number of optimizations found throughout the circuit.
%
To account for this,
we charge the running time to the cost improvement between the input and the output,
represented by $\Delta$.
%
We prove the following theorem.


\begin{theorem}[Restatement of \thmref{cost}, Efficiency of segment optimization]
  % Consider a segment length $\Omega$, an oracle that optimizes for the cost function $\mathbf{cost}$,
  % and a circuit $C$.
  The function $\mathsf{segopt}(C)$ calls the oracle at most
  $\mathsf{length}(C) + 2\Delta$ times on segments of length at most $2\Omega$,
  where $\Delta$ is the improvement in the cost of the output.
  \end{theorem}

The theorem shows that our segment optimality algorithm is productive in its use of the oracle.
%
Consider the terms, $\mathsf{length}(C)$ and $2\Delta$, in the bound on the number of oracle calls:
%
The first term, $\mathsf{length}(C)$, is for checking segment
optimality---even if the input circuit is already optimal,
the algorithm needs to call the oracle on each segment and confirm it.
%
The second term $2\Delta$,
shows that all further calls result from optimizations,
with each optimization requiring up to two oracle calls in the worst case.
%
This shows that our algorithm (alongside meld)
carefully tracks the segments on which the oracle needs to be called
and avoids unnecessary calls.


The theorem also highlights one of the key features of our $\mathsf{segopt}$ function:
it only uses the oracle on small circuit segments of length $2\Omega$.
%
Suppose $q$ is the number of qubits in the circuit.
%
Then the $\mathsf{segopt}$ function only calls the oracle on
manageable circuits of size less than or equal to $q * 2\Omega$,
which is significantly smaller than circuit size.
%
This has performance impacts because, for many oracles,
it is faster to invoke the oracle many times on small circuits
rather than invoking the oracle a single time on the full circuit (see \secref{eval}).
%

Since we bound number of oracle calls in terms of the cost improvement $\Delta$,
an interesting question is how large can $\Delta$ be?
%
When optimizing for gate-counting metrics
such as $\mathsf{T}$ count (number of $\mathsf{T}$ gates), $\mathsf{CNOT}$ count (number of $\mathsf{CNOT}$ gates),
gate count (total number of gates),
the cost improvement is trivially bounded by the circuit size,
i.e., $\Delta \leq \sizeof{C}$.
%
This observation shows that our \textsf{segopt}
function only makes a linear number of calls (with depth and gate count)
to the oracle.


\begin{corollary}[Restatement of \corref{linear-calls}, Linear calls to the oracle]
  When optimizing for gate count, our $\mathsf{segopt}(C)$ makes a linear,
  $O(\mathsf{length}(C) + \sizeof{C})$, number of calls to the oracle.
\end{corollary}

We experimentally validate this corollary in \secref{eval-calls},
where we study the number of oracle calls made by our algorithm
for many circuits.
The crux of the proofs of \thmref{cost} and the corollary 
is bounding the number of calls in the \textsf{meld} function,
because it can propagate optimizations through the circuit.
%
We show the proof below.
%We state bound below and leave the proofs to 
%\iffull
%\appref{thm-meld-num-oracles}.
%\else
%the Appendix.
%\fi
%
\begin{lem}[Restatement of \lemref{meld-num-oracles}]
	%\label{lem:meld-num-oracles}
	Computing $C =$ \lstinline{meld}$(C_1, C_2)$
	makes at most $1 + 2\Delta$ calls to the oracle,
	where $\Delta$ is the improvement in cost,
	i.e., $\Delta = \left(\costof {C_1} + \costof{C_2} - \costof{C}\right)$.
\end{lem}
\begin{proof}
	We prove this by induction on the input cost $\costof {C_1} + \costof{C_2}$.
	%
	The base case is straightforward: when the input cost is zero,
	the meld function only makes the one call and returns.
	%
	For the inductive step,
	we consider two cases depending on whether the oracle improves the boundary
	segment $W$.
	
	\paragraph{Case I} If the oracle finds no improvement in the segment $W$, then
	there is exactly one call to the oracle.
	%
	Thus we have $1 \leq 1 + 2\left(\costof {C_1} + \costof{C_2} - \costof{C}\right)$.
	
	\paragraph{Case II}
	When the oracle finds an improvement in the segment $W$,
	it returns $W'$ such that $\costof{W'} \leq \costof W - 1$.
	%
	(There is a difference of at least 1 because $\cost{}(-) \in \mathbb{N}$.)
	%
	In this case, the meld function recurs twice,
	first to compute $M = \mathsf{meld}(C_1[0 : d_1 - \Omega], W')$,
	and then to compute the output $C = \mathsf{meld}(M, C_2[\Omega:d_2])$.
	The total number of calls to the oracle can be decomposed as follows:
	
	\begin{itemize}
		\item 1 call to the oracle for the segment $W$.
		\item Inductively,
		at most $1 + 2\left(\costof {C_1[0 : d_1 - \Omega]} + \costof{W'} - \costof{M}\right)$ calls to the oracle for the first meld.
		\item Inductively, at most
		$1 + 2\left(\costof M + \costof{C_2[\Omega:d_2]} - \costof{C}\right)$ calls to the oracle in the second recursive meld.
	\end{itemize}
	Adding these up yields at most
	$1 + 2\left(\costof {C_1} + \costof{C_2} - \costof{C}\right)$ calls to the oracle, as desired.
	%
	(We use here the facts
	$\costof{W'} \leq \costof W - 1$ and
	$\costof {C_1[0 : d_1 - \Omega]}  + \costof W + \costof {C_2[\Omega:d_2]} = \costof {C_1} + \costof{C_2}$.)
\end{proof}


% \begin{corollary}
%   Given an optimizer that takes $T(q, \sizeof{C})$ time on a circuit $C$ with $q$ qubits and size \sizeof{C},
%   computing $C' = \mathsf{segopt}(C)$ requires at most $O(T(q, \sizeof{C}) * (\mathsf{length}(C) + \Delta))$ time,
%   where $\Delta = \costof{C} - \costof{C'}$ is the improvement in cost.
% \end{corollary}

% \begin{corollary}
%   Given an optimizer that takes $T(q, \mathsf{length}(C))$ time on a circuit $C$ with $q$ qubits,
%   computing $C' = \mathsf{segopt}(C)$ takes $O((\mathsf{length}(C) + 2\Delta) * T (q, 2\Omega))$ time
% \label{cor:linear-calls}
% \end{corollary}

% The corollary demonstrates that when optimizing for gate count,
% our \textsf{segopt} function scales linearly with both the number of gates and the length/depth
% of the circuit.
% %
% Furthermore, our \textsf{segopt} function and the oracle
% exhibit similar scaling behavior in the number of qubits.


% The corollary shows that when optimizing for gate count,
% our \textsf{segopt} function scales linearly
% with the number of gates and depth of the circuit.
% %
% Both the oracle and our \textsf{segopt} function scale similarly with the number of qubits.

% %


\begin{lem}[Bounded calls to oracle in meld]
  \label{lem:meld-num-oracles}
Computing $C =$ \lstinline{meld}$(C_1, C_2)$
makes at most $1 + 2\Delta$ calls to the oracle,
where $\Delta$ is the improvement in cost,
i.e., $\Delta = \left(\costof {C_1} + \costof{C_2} - \costof{C}\right)$.
\end{lem}




  %

% The meld function and its running time are data dependent,
% as the function decides to make the two recursive calls
% depending on the output of the oracle.
% %
% The recursion tree for a single meld call can be deep,
% as the melding may continue if the oracle continues to find optimizations,
% possibly calling the oracle on segments from  the whole circuit.
% %
% But, because the algorithm recurses only if the oracle finds an optimization,
% we can charge the cost of recursion to the optimization.
% %
% This is formalized in \lemref{meld-num-oracles}.





% ?

% \paragraph{Meld Example.}
% Returning to the example in \figref{propagate},
% we can see the algorithm in action.
% %
% The example we uses $\Omega = 2$ and gate count as the cost function.
% %
% In the first step,
% the function optimizes the $2\Omega-$segment at the boundary of inputs;
% this corresponds to optimizing the $4-$segment in the figure and reducing the gate count.
% %
% Then, the function melds the preceding layers of circuit $C_1$ with the window $W'$,
% corresponding to the second step in \figref{propagate},
% which propagates the optimization
% by considering both the old and new CNot gates.
% %
% Finally, the function melds the remaining layers of $C_2$,
% which corresponds to the third step in \figref{propagate},
% which merges the rotation gates.
%


% \subsection{SOAM Algorithm}

% \figref{lopt-code} presents the pseudocode of the \coam{} algorithm.
% %
% %
% The function $\mathsf{SOAM}$ takes a circuit $C$ and
% returns an segment optimal circuit, where the segment optimality is relative to the oracle.
% %
% To do so,
% the function checks if the circuit is small,
% i.e., its its depth is less than $2\Omega$.
% %
% In this case, the function sends the circuit to the oracle,
% and returns the output of the oracle, which is segment optimal by definition.
% %
% For larger circuits, the function
% $\mathsf{SOAM}$ splits the circuit into two halves
% $C_1$ and $C_2$, and recursively optimizes the two halves,
% retrieving the segment optimal versions $C'_1$ and $C'_2$.
% %
% Then it calls the function $\mathsf{meld}$ which stitches $C'_1$ and $C'_2$
% to produce the segment optimal output circuit.
% %

% We prove that $\mathsf{segopt}(C)$ produces an segment optimal output
% and does so in a bounded number of oracle queries,
% where each query is for small segments, i.e.,
% segments whose length is bounded by $2\Omega$.
% %


% \todo{exponential oracle can be viewed as oversell.  use quadratic. }
% \todo{do not use "huge" in writing to prevent oversell reaction.}
% The corollary shows that the $\mathsf{segopt}$ function can have asymptotic benefits
% for gate count (or T-count) optimization.
% %
% For example, suppose the oracle complexity is exponential in the number of qubits and circuit length,
% i.e., suppose $T(q, d) = O(2 ^ {qd})$.
% %
% Then, using the above corollary,
% the time complexity of the $\mathsf{segopt}$ function is $O(2 ^{2q\Omega}\sizeof{C})$,
% which is bounded by $O(2 ^{2q\Omega}qd)$ (using $\sizeof{C} \leq qd$).
% %
% In this example,
% the base complexity $T(q, d)$ is exponential in circuit length $d$,
% but the $\mathsf{segopt}$ function is effectively linear in the length,
% which is a huge reduction.
% %
% More generally, we can derive a meaningful reduction in complexity if the oracle
% has at least quadratic (in length) time complexity,
% because the $\mathsf{segopt}$ function only sends a linear number of fixed-length circuits to the oracle.
% %
% % This efficiency, however, comes at a potential cost to circuit quality,
% % because the $\mathsf{segopt}$ function guarantees segment optimality whereas
% % the base optimizer can work on the whole circuit and, in principle, find more optimizations.
% % %
% % Our experiments, however, suggest that this potential cost does not show up in practice (\secref{eval}).
% %

% \footnote{We can derive a meaningful, but less pronounced reduction in complexity even when the oracle has quadratic cost}.


% As another corollary of the above theorems,
% we get that if we have an optimizer that returns globally optimal circuits,
% then the $\mathsf{segopt}$ function can produce circuits that are segment optimal
% in an absolute sense,
% i.e., every $\Omega-$segment of the output is globally optimal.

% \begin{corollary}
% Given an oracle that produces optimal circuits,
% the $\mathsf{segopt}$ function produces segment optimal circuits using the oracle
% only on segments of size $2\Omega$.
% \end{corollary}


%\subsection{Putting the pieces together: the \algname{} algorithm}
%\label{sec:full-algorithm}
%

The \algname{} algorithm
uses the segment optimality guarantee, given by \textsf{segopt},
to produce a locally optimal circuit.
%
In \figref{lopt-code}, the function \algname{} takes the input circuit $C$
and computes the circuit $C' = \mathsf{segopt}(\mathsf{compact}(C))$.
%
It then checks if $C'$ differs from the input $C$ and if so,
it recurses on $C'$.
% It then checks if $C'$ differs from the input $C$.
% %
% If $C'$ differs from $C$---which could be due to optimization, or compaction, or
% both---then \algname{} continues recursively on $C'$.
% %
% Otherwise, if $C'$ and $C$ are identical,
% it returns the circuit $C'$ because no optimization or compaction occured.
The function repeats this until convergence, i.e.,
until the circuit does not change.
%
In the process, it
computes a sequence of segment optimal circuits $\{C_i\}_{1 \leq i \leq \kappa}$,
where $\kappa$ is the number of rounds until convergence:
\[C \to C_1 \to C_2 \to \ldots \to C_\kappa \quad\text{where}\quad
\begin{array}{c}
\costof{C_i} > \costof{C_{i+1}} \\
\windowopt\Omega{C_i} \\
\locallyopt{\Omega}{C_\kappa}
\end{array}
\]
%
Each intermediate circuit $C_i$ is segment optimal
and gets compacted before optimization in the next round.
%
Given that \algname{} continues until convergence,
the final circuit, $C_\kappa$, is locally optimal.

% This argument subtly hinges upon a property of our segment optimality algorithm, in particular,
% that $\mathsf{segopt}(C)$ differs from $C$ only if the cost has improved.
% %
% The function $\mathsf{segopt}$ and its key subroutine, meld, are careful to ensure this
% by guarding the oracle calls:
% %
% if applying the oracle does not improve the cost, then the output of the oracle is discarded
% (see lines \ref{line:guard1start}-\ref{line:guard1stop} and \ref{line:guard2start}-\ref{line:guard2stop}
% in \figref{lopt-code}).
% %
% (This aligns with the rewriting semantics of \secref{lang}, which only performs optimization
% rewrites if the cost decreases.)
% %

Using \thmref{cost},
we observe that the number of oracle calls in each round is bounded by
$\mathsf{length}(C_i) + 2\Delta_i$,
where $\Delta_i$ is the improvement in cost (i.e., $\Delta_i = \costof{C_i} - \costof{C_{i+1}}$).
%
Thus, the total number of oracle calls performed by \algname{} can be bounded by
$O(\Delta + \mathsf{length}(C) + \sum_{1 \leq i \leq \kappa} \mathsf{length}(C_i))$
where $\Delta = \costof{C} - \costof{C_\kappa}$ is the overall improvement in cost between the input and output.

\begin{theorem}
\label{thm:local-opt}
Given an additive cost function,
the output of $\algname{}(C)$ is locally optimal
and requires $O(\Delta + \mathsf{length}(C) + \sum_{1 \leq i \leq \kappa} \mathsf{length}(C_i))$ oracle calls,
where $C_i$ denotes the circuit after $i$ rounds,
$\kappa$ is the number of rounds,
and $\Delta$ is the end-to-end cost improvement.
\end{theorem}


The bound on the number of oracle calls in \thmref{local-opt} is very general
because it applies to any oracle (and gate set).
%
For particular cost functions, we can use it to deduce a more precise bound.
%
Specifically, in the case of gate count, where
$\costof C = \sizeof C$, we get the following bound.

\begin{corollary}\label{cor:oac-linear-calls}
When optimizing for gate count,
$\algname{}(C)$ performs at most $O(\sizeof{C} + \kappa \cdot \mathsf{length}(C))$ oracle calls.
\end{corollary}

An interesting question is whether or not it is possible to bound the number of rounds, $\kappa$.
%
In general, this will depend on a number of factors,
such as the quality of the oracle and how quickly the optimizations it performs converge
across compactions.
%
In practice (\secref{eval}), we find that the number of rounds required is small, and that
nearly all optimizations are performed in the first round itself.
%
For gate count optimizations in particular, Corollary~\ref{cor:oac-linear-calls} tells us
that when only a small number of rounds are required, the scalability of \algname{} is
effectively linear in circuit size.

\if0
\subsection{Worst-case number of rounds for \algname{}}
\input{fig/fig_quadratic_lower_bound}
%
\figref{quadratic_lower_bound} gives an example showing that a linear number of compactions is necessary in the worst case.
%
The example uses $\Omega=3$.
%
We optimize the solid green segment from (a) to (b) by canceling the two CNOTs.
%
However, at (b), no more optimization can be applied.
%
Although the dashed green segment is not optimal, the two CNOTs are separated enough so that we cannot optimize them.
%
So, we must apply a compaction step to bring the two CNOTs together and then optimize them, as shown in (c).
%
This pattern will continue and can require a number of compactions proportional to the number of qubits.
\fi

\subsection{Circuit Representation and Compaction}
%\ur{Maybe move to this to the end.  Say, for the analysis, we assume that split and concat are constant.  We describe in section x, how to achieve this by using a specific circuit representation.
%
%UPDATE: I moved this here.  But I am not sure where it is used.  We
%seem to be proving only the number of oracle calls and not end-to-end
%run time.
%
%}

Our algorithm represents circuits using a hybrid data structure that
switches between sequences and linked lists for splitting and
concatenating circuits in $O(1)$ time.
%
Initially, it represents the circuit as a sequence of layers,
enabling circuit splits in $O(1)$ time.
%
Later during optimization
when circuit concatenation is required,
the algorithm switches to a linked list of layers,
enabling $O(1)$ concatenation.
%
At the end of each optimization round,
the algorithm uses a function $\mathsf{compact}(C)$
to revert to the sequence of layers representation.
%

The function $\mathsf{compact}(C)$ ensures that the resulting circuit
is equivalent to $C$ while eliminating any unnecessary ``gaps'' in the layering,
meaning that every gate has been shifted left as far as possible.
%
For brevity, we omit the implementation of the $\mathsf{compact}$ function from \figref{lopt-code}.
%
Our implementation in practice is straightforward: we use a single left-to-right
pass over the input circuit and build the output by iteratively adding gates.
%
The time complexity is linear, i.e., $\mathsf{compact}(C)$ requires $O(|C| + \mathsf{length}(C))$ time.
%
%
% Local optimality requires that a layered circuit be as compact as possible,
% and the function $\mathsf{compact}(C)$ satisfies this condition.
%
%


% %
% In particular, if $C' = \mathsf{compact}(C)$ then we have $\compressed{C'}$, using the
% definition of compactness from \secref{lang}; the resulting circuit $C'$ is identical
% to $C$ except that every gate has been shifted left as far as possible.
%
% Our \algname{} algorithm uses this $\mathsf{compact}$ function in two ways.
% \begin{enumerate}
%     \item The output of every \oracle{} call is compacted. The cost of this compaction can be charged
%     against the oracle, because compaction is no more expensive than reading and writing the
%     input and output circuits. In this sense, compacting the output of every oracle call is ``free''
%     from an efficiency perspective.
%     Note that these compactions are not essential for any of our theorems; we include these in the
%     algorithm description because they are useful in practice, helping ensure that every optimization
%     phase is as productive as possible.
%     \item After every optimization phase, the entire circuit is compacted. Note that these compactions
%     are essential for guaranteeing local optimality.
% \end{enumerate}




%%% ALGO

\section{Proof of Termination}
\label{app:lem-pot-dec}
We prove below (\lemref{pot-dec}) that the potential decreases on every step.
%
\thmref{convergence} then follows from \lemref{pot-dec}, because ordering by $\Phi$
is well-founded and cannot infinitely decrease.
%
% \thmref{convergence} by induction over circuits ordered by $\Phi$.
% and because the range of our potential function is well-founded,
% there can be no infinite rewriting sequences.

\begin{lemma}
	\label{lem:pot-dec}
	For any additive function $\cost{}: \textit{Circuit} \to \mathbb{N}$,
	% and any well-formed circuit $C$,
	if $\localstep\Omega{C}{C'}$ then $\Phi(C') < \Phi(C)$.
\end{lemma}
\begin{proof}
	% If $\locallyopt\Omega{C}$, then $C = C^\text{OPT}$ and we are done; otherwise,
	% by \lemref{can-step} we have $\localstep\Omega{C}{C'}$.
	%
	There are two cases
	for $\localstep\Omega{C}{C'}$:
	either \rulename{Lopt} or
	\rulename{ShiftLeft}.
	%
	In each case we show $\Phi(C') < \Phi(C)$.
	
	In the case of \rulename{Lopt}, we have $\localstep\Omega C {C'}$ where
	$C = (P ; C'' ; S)$ and $C' = (P ; \oracle{}(C'') ; S)$.
	%
	In $C'$, the segment $C''$ has been improved by one call to the \oracle{},
	i.e., $\costof{\oracle{}(C'')} < \costof{C''}$.
	%
	Because the \cost{} function is additive, we have that
	$\costof{C'} = \costof{P ; \oracle{}(C'') ; S} < \costof{P ; C'' ; S} = \costof{C}$.
	%
	This in turn implies $\Phi(C') < \Phi(C)$ due to lexicographic ordering on the
	potential function.
	
	% a segment of $C$ is improved by one
	% call to the \oracle{}.
	% %
	% Because the \cost{} function is additively monotonic, we therefore have
	% that $\costof{C'} < \costof{C}$, which in turn implies that
	% $\Phi(C') < \Phi(C)$ due to lexicographic ordering.
	
	% To show that the rewrite system terminates on any given circuit $C$,
	% we define a well-founded order $\prec$ on the set of circuits
	% and show that each rewrite descends down the order,
	% i.e., for each $\localstep\Omega{C}{C'}$, $C' \prec C$.
	% %
	% Because a well-founded order can not have infinitely descending chains,
	% the number of possible rewrites starting from the circuit $C$ are finite,
	% showing that the circuit $C$ can be made locally optimal in a finite number of steps.
	% %
	
	In the case of \rulename{ShiftLeft}, we have $\localstep\Omega C {C'}$ where
	$C = (P ; \langle L_1, L_2 \rangle ; S)$ and $C' = (P ; \langle L_1', L_2' \rangle ; S)$
	and $L_1' = L_1 \cup \{G\}$ and $L_2' = L_2 \setminus \{G\}$.
	%
	Because the cost function is additive, we have $\costof{\langle L_1, L_2 \rangle} = \costof{\langle L_1', L_2' \rangle}$
	and therefore $\costof C = \costof {C'}$.
	%
	To show $\Phi(C') < \Phi(C)$, due to the lexicographic ordering, it remains to show
	$\mathsf{IndexSum}(C') < \mathsf{IndexSum}(C)$.
	%
	This in turn follows from the definition of $\mathsf{IndexSum}$; in particular, plugging
	in $|L_1'| = |L_1| + 1$ and $|L_2'| = |L_2| - 1$ we get
	$\mathsf{IndexSum}(C') = \mathsf{IndexSum}(C) - 1$.
	%
	Thus we have $\Phi(C') < \Phi(C)$.
\end{proof}
