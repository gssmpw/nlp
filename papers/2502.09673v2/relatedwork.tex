\section{Related Work}
\label{sec::related_work}
\textbf{Reasoning for LLM: } Since a large part of human intelligence is attributed to reasoning capacity~\citep{lohman2011intelligence}, reasoning like humans has become a hot research topic in the studies of LLMs, including the prompt-based and fine-tuning-based methods. Prompt-based methods improve the reasoning capability at the inference stage and one of the most representative ones is the chain-of-thought (CoT) method. As proposed in~\citet{wei2022chain}, they firstly elucidate that better reasoning capacities will emerge once a few chain of thought demonstrations are provided. Following variants reveal that it can be further enhanced with zero-shot prompting~\citep{kojima2022large,zhang2022automatic}, prompt augmentation~\citep{shum2023automatic} or external knowledge~\citep{liu2023retrieval,zhao2023verify}. In contrast, fine-tuning-based methods train the LLM parameters to improve reasoning. Owe to the success of o1 family models~\citep{jaech2024openai}, their power is well recognized by recent works. For example, by fine-tuning on CoT data, we will see the performance of Marco-o1~\citep{zhao2024marco} in math improves in a novel margin (+6.17\%). The significance of it is also demonstrated by other reasoning-enhanced models, \textit{e.g.} OpenR~\citep{wang2024openr}, Qwen2.5-Math~\citep{yang2024qwen25mathtechnicalreportmathematical} and DRT-o1~\citep{wang2024drt}, making it a sharping tool for reasoning enhancement.

\noindent\textbf{Trustworthiness of LLM reasoning: }The trustworthiness of LLM reasoning has been evaluated from various perspectives. \citet{Radhakrishnan2023QuestionDI, li-etal-2024-deceptive, paul-etal-2024-making, Chua2024BiasAugmentedCT} have measured the faithfulness of LLM reaasoning. The focus of the evaluation by \citet{Radhakrishnan2023QuestionDI, han-etal-2024-context, steenhoek2025errmachinevulnerabilitydetection, Self-correct-reasoning} is the robustness of reasoning. There also have been assessments studying the influence of reasoning on the toxicity \citep{not-think}, social bias \citep{not-think, Persona-Reasoning} and machine ethics \citep{Ethics-Reasoning} of LLMs. For reasoning in the Multi-modal LLMs (MLLMs) \citep{lu2022learn, 10.1609/aaai.v38i16.29776}, \citet{multimodel-reasoning} have noticed the improvement in resilience of the models against image adversarial examples brought by \textit{step-by-step} reasoning and designed an adaptive attack accordingly. Except for the various aspects of trustworthiness mentioned above, it is still unclear how safety will change as we increase the reasoning ability of LLMs, which is of paramount concern given the rapid evolvement of methods to enhanced LLM reasoning and the potential risks brought by the consequent models.