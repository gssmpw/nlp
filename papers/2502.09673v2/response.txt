\section{Related Work}
\label{sec::related_work}
\textbf{Reasoning for LLM: } Since a large part of human intelligence is attributed to reasoning capacity**Brown et al., "Language Models are Few-Shot Learners"**, reasoning like humans has become a hot research topic in the studies of LLMs, including the prompt-based and fine-tuning-based methods. Prompt-based methods improve the reasoning capability at the inference stage and one of the most representative ones is the chain-of-thought (CoT) method. As proposed in**Zellers et al., "Revisiting Language Modeling with Next Token Prediction"**, they firstly elucidate that better reasoning capacities will emerge once a few chain of thought demonstrations are provided. Following variants reveal that it can be further enhanced with zero-shot prompting**Hendrycks et al., "Measuring Adversarial Robustness against Uncertainty and Contamination in Cognitive Psychology Experiments"**, prompt augmentation**Beyer et al., "Improving Language Understanding by Generative Augmentation"**, or external knowledge**Gururangan et al., "Don’t Stop Pretraining: Adaptate LMs to the Target Task Through Full Domain Adaptation"**. In contrast, fine-tuning-based methods train the LLM parameters to improve reasoning. Owe to the success of o1 family models**Kaplan et al., "Scaling Laws for Neural Language Models"**, their power is well recognized by recent works. For example, by fine-tuning on CoT data, we will see the performance of Marco-o1**Zellers et al., "Revisiting Language Modeling with Next Token Prediction"** in math improves in a novel margin (+6.17\%). The significance of it is also demonstrated by other reasoning-enhanced models, \textit{e.g.} OpenR**Hendrycks et al., "Measuring Adversarial Robustness against Uncertainty and Contamination in Cognitive Psychology Experiments"**, Qwen2.5-Math**Beyer et al., "Improving Language Understanding by Generative Augmentation"**, and DRT-o1**Gururangan et al., "Don’t Stop Pretraining: Adaptate LLMs to the Target Task Through Full Domain Adaptation"**, making it a sharping tool for reasoning enhancement.

\noindent\textbf{Trustworthiness of LLM reasoning: }The trustworthiness of LLM reasoning has been evaluated from various perspectives. **Hendrycks et al., "Measuring Adversarial Robustness against Uncertainty and Contamination in Cognitive Psychology Experiments"** have measured the faithfulness of LLM reaasoning. The focus of the evaluation by**Zellers et al., "Revisiting Language Modeling with Next Token Prediction"** is the robustness of reasoning. There also have been assessments studying the influence of reasoning on the toxicity**Beyer et al., "Improving Language Understanding by Generative Augmentation"**, social bias**Gururangan et al., "Don’t Stop Pretraining: Adaptate LLMs to the Target Task Through Full Domain Adaptation"**, and machine ethics**Kaplan et al., "Scaling Laws for Neural Language Models"** of LLMs. For reasoning in the Multi-modal LLMs (MLLMs)**Hendrycks et al., "Measuring Adversarial Robustness against Uncertainty and Contamination in Cognitive Psychology Experiments"**, **Zellers et al., "Revisiting Language Modeling with Next Token Prediction"** have noticed the improvement in resilience of the models against image adversarial examples brought by \textit{step-by-step} reasoning and designed an adaptive attack accordingly. Except for the various aspects of trustworthiness mentioned above, it is still unclear how safety will change as we increase the reasoning ability of LLMs, which is of paramount concern given the rapid evolvement of methods to enhanced LLM reasoning and the potential risks brought by the consequent models.