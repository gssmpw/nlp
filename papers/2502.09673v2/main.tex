\documentclass{article} % For LaTeX2e

\usepackage[table]{xcolor}
\usepackage{arxiv}
\usepackage{natbib}
\usepackage{microtype}
\usepackage{graphicx}
\usepackage{booktabs} % for professional tabcolse
\usepackage{tcolorbox}
\tcbuselibrary{breakable} % 启用分页功能
\usepackage{subfig}
\usepackage{threeparttable}
\usepackage{adjustbox} % 在导言区添加此包
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{amsthm}
\usepackage{makecell}
\usepackage{adjustbox}
\usepackage{tabularx}
\usepackage{ragged2e}


\usepackage{xltabular}




\usepackage{multirow}

\tcbset{
    GCG-box/.style={
        colback=gray!5!white,    % 背景色
        colframe=gray!75!black,  % 边框颜色
        boxrule=1pt,             % 边框粗细
        arc=3mm,                 % 圆角
        breakable,               % 允许跨页
        title=The Attack Template for GCG,         % 标题
        fonttitle=\bfseries
    }
}

\tcbset{
    Decom-box/.style={
        colback=gray!5!white,    % 背景色
        colframe=gray!75!black,  % 边框颜色
        boxrule=1pt,             % 边框粗细
        arc=3mm,                 % 圆角
        breakable,               % 允许跨页
        title=The Attack Template for the Decomposition-Based Attack,         % 标题
        fonttitle=\bfseries
    }
}

\tcbset{
    Answer-box/.style={
        colback=gray!5!white,    % 背景色
        colframe=gray!75!black,  % 边框颜色
        boxrule=1pt,             % 边框粗细
        arc=3mm,                 % 圆角
        breakable,               % 允许跨页
        title=The Prompt for Numerical Answer Extraction,         % 标题
        fonttitle=\bfseries
    }
}

\tcbset{
    Privacy-box/.style={
        colback=gray!5!white,    % 背景色
        colframe=gray!75!black,  % 边框颜色
        boxrule=1pt,             % 边框粗细
        arc=3mm,                 % 圆角
        breakable,               % 允许跨页
        title=An Example Query for Evaluating Privacy,         % 标题
        fonttitle=\bfseries
    }
}

\tcbset{
    Privacy-reply/.style={
        colback=gray!5!white,    % 背景色
        colframe=gray!75!black,  % 边框颜色
        boxrule=1pt,             % 边框粗细
        arc=3mm,                 % 圆角
        breakable,               % 允许跨页
        title=An Example Reply from Llama-2-7B,         % 标题
        fonttitle=\bfseries
    }
}


\tcbset{
    Increased-Practicability/.style={
        colback=gray!5!white,    % 背景色
        colframe=gray!75!black,  % 边框颜色
        boxrule=1pt,             % 边框粗细
        arc=3mm,                 % 圆角
        breakable,               % 允许跨页
        title=Increased Practicability and Complexity in Unsafe Responses,         % 标题
        fonttitle=\bfseries
    }
}

\tcbset{
    Toxic-Leakage/.style={
        colback=gray!5!white,    % 背景色
        colframe=gray!75!black,  % 边框颜色
        boxrule=1pt,             % 边框粗细
        arc=3mm,                 % 圆角
        breakable,               % 允许跨页
        title=Toxic Thought Leakage,         % 标题
        fonttitle=\bfseries
    }
}

\tcbset{
    Intention-Alternation/.style={
        colback=gray!5!white,    % 背景色
        colframe=gray!75!black,  % 边框颜色
        boxrule=1pt,             % 边框粗细
        arc=3mm,                 % 圆角
        breakable,               % 允许跨页
        title=User Intention Alternation,         % 标题
        fonttitle=\bfseries
    }
}


\newtcolorbox{cvbox}[1][]{
    after skip=8mm,
    title=#1,
    breakable = true,
    fonttitle=\sffamily\bfseries,
    coltitle=white,
    colbacktitle=gray!100,  
    titlerule= 0pt,        
    overlay={%
        \ifcase\tcbsegmentstate
        \or
        \else
        \fi
    }
    colback = gray,        
    colframe = black!75     
    }


\title{Are Smarter LLMs Safer? \\ Exploring Safety-Reasoning Trade-offs in Prompting and Fine-Tuning}


\author{
Ang Li \\
Peking University\\
\and
Yichuan Mo \\
Peking University\\
\and
Mingjie Li \\
CISPA\\
\AND
Yifei Wang \\ 
MIT CSAIL \\ \and
Yisen Wang \\ 
Peking University 
}

\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}


\begin{document}
\maketitle

\begin{abstract}
Large Language Models (LLMs) have demonstrated remarkable success across various NLP benchmarks. However, excelling in complex tasks that require nuanced reasoning and precise decision-making demands more than raw language proficiency—LLMs must reason, i.e., think logically, draw from past experiences, and synthesize information to reach conclusions and take action. To enhance reasoning abilities, approaches such as prompting and fine-tuning have been widely explored. While these methods have led to clear improvements in reasoning, their impact on LLM safety remains less understood. In this work, we investigate the interplay between reasoning and safety in LLMs. We highlight the latent safety risks that arise as reasoning capabilities improve, shedding light on previously overlooked vulnerabilities. At the same time, we explore how reasoning itself can be leveraged to enhance safety, uncovering potential mitigation strategies. By examining both the risks and opportunities in reasoning-driven LLM safety, our study provides valuable insights for developing models that are not only more capable but also more trustworthy in real-world deployments.

{\color{red}{\textbf{Disclaimer. This paper contains uncensored toxic content that might be offensive or disturbing to the readers.}}}
\end{abstract}


\section{Introduction}

Large Language Models (LLMs) have achieved revolutionary performance in common NLP tasks such as creative writing \citep{yuan2022wordcraft, swanson2021story}, machine translation \citep{zhang2023prompting, swanson2021story}, and knowledge Question-Answering \citep{jiang2024enhancing,zhang2022greaselm}. However, to excel in more complex tasks like mathematics and programming, which demand nuanced thinking and exact actions, LLMs need to reason. In this paper, reasoning refers to the process of \textit{thinking in a logical and sensible way, while using evidence and past experience to reach conclusions and make decisions} \citep{wason1968reasoning, wason1972psychology, galotti1989approaches, fagin2004reasoning, mchugh2018reasoning}. Motivated by the great potential of capable LLM reasoners, there have been many notable works proposing methods to improve LLM reasoning via prompting \citep{kojima2022large, wei2022chain, zhang2022automatic} and fine-tuning \citep{yang2024qwen25mathtechnicalreportmathematical, wang2024openr, wang2024drt}. However, contrary to the clear advancements in benchmarks like Mathematics Olympics \citep{gao2024omnimathuniversalolympiadlevel}, it is less investigated how the safety of LLMs will evolve during the improvement of LLM reasoning. Deeply concerned by the catastrophic outcome of capable reasoning LLMs being manipulated and abused, we ask the question:

\begin{center}
    \textit{How safety will change as we increase the reasoning abilities of LLMs via prompting and fine-tuning?}
\end{center}

Previous to this paper, \citet{not-think} has observed that the Text-Davinci models \citep{GPT-3} exhibit increased toxicity and bias when prompted with \textit{Let's think step by step} \citep{wei2022chain}. On the contrary, \citet{multimodel-reasoning} reports Multi-Modal LLMs (\textit{MLLMs}) exhibit stronger resilience against visual adversarial attacks when provided with the exact same prompt, blurring the relationship between reasoning and safety. In this work, we make efforts to unravel the complicated interplay between LLM reasoning and their safety by carefully monitoring the changes of safety when adopting prompt-based or fine-tuning-based reasoning methods.

\begin{wrapfigure}{r}{0.5\textwidth}
    \centering
    \includegraphics[width=0.48\textwidth]{figures/intro.pdf}
    \caption{\textbf{Safety challenges in improving LLM reasoning}: Starting from the default Llama-2-7B-Chat, we experiment with improving its reasoning abilities via (1) zero-shot Chain-of-Thought (CoT) prompting, (2) few-shot CoT, (3) fine-tuning on a coding and math dataset, and (4) fine-tuning on the Open-o1-SFT dataset. As the accuracy increases from 16\% to 41.4\%, we see the model experiences a catastrophic drop of 66\% in safety score, suggesting the existence of non-negligible safety challenges in improving LLM reasoning.}
    \label{fig::intro_overall}
\end{wrapfigure}


To explore the relationship between safety and reasoning, we first conduct a preliminary study on Llama-2-7B-Chat \citep{touvron2023llama2} with four commonly used reasoning methods:
(1) zero-shot Chain-of-Thought (CoT) prompting \citep{wei2022chain}, (2) few-shot CoT, (3) fine-tuning on a dataset consisting of samples from the GSM8K \citep{cobbe2021training} and Alpaca-Python \citep{alpaca-python}, and (4) fine-tuning on the Open-o1-SFT dataset \citep{open-o1-2025}. Next, we measure their accuracy on the test-set of GSM8K as well as their safety with the HarmBench \citep{mazeika2024harmbench}. The results are presented in Figure \ref{fig::intro_overall}. 

Although the accuracy for the methods gradually increases from 16\% to 41.4\%, we also notice a clear drop in safety: 22.2\% drop for few-shot CoT and 66\% drop for fine-tuning on Open-o1. The results suggest that the interplay between reasoning and safety is not trivial and one may encounter non-negligible safety challenges during the process of improving LLM reasoning. Such results motivate us to do a in-depth analysis on the safety changes caused by different reasoning methods and try to mitigate the possible safety drops when adopting different reasoning methods. Our contributions are listed below:

\begin{itemize}
    \item After exploring 12 prompt-based reasoning methods on 7 different LLMs, we observe a general negative correlation between reasoning ability and safety for prompt-based reasoning methods.
    \item After evaluating the 2 most popular fine-tuning-based reasoning methods, we reveal that reasoning-related fine-tuning compromises safety beyond generic fine-tuning, especially the CoT style responses.
    \item We provide practical insights into mitigating the safety costs while improving reasoning for prompting and fine-tuning.
\end{itemize}


\section{Preliminary: Safety and Reasoning in LLMs}
To investigate the challenges and opportunities in the safety of LLMs during the improvement of their reasoning abilities, we assess how safety metrics evolve when prompting and fine-tuning LLMs to improve reasoning skills. Before presenting our insights gained during the assessment, we first outline the preliminaries of our work below while the detailed related works are provided in Appendix \ref{sec::related_work}.

\subsection{LLM Reasoning}
\label{sec::reasoning_preliminary}
Driven by the irreplaceable role that reasoning plays in human intelligence \citep{lohman2011intelligence}, researchers have proposed various notable methods to improve LLMs in reasoning. In this work, we primarily focus on the prompt-based methods at the inference stage and fine-tuning methods at the post-training stage, which we will introduce below.

\textbf{Prompt Based Methods:} In this study, we explore a diverse set of prompts, which we categorize into Chain-of-Thought (CoT) \citep{wei2022chain} and reflection \citep{shinn2023reflexion}. Specifically, for CoT prompts, we will cover the zero-shot/few-shot CoT in both implicit and explicit forms. For reflection-based prompts, we consider the system prompts of NovaSky-t1 \citep{sky-t1-2025} and Reflextion-70B \citep{reflextion7}, both of which are capable models featuring reflective thinking. We denote them as \textit{Sys-t1} and \textit{Sys-Ref} in later discussion. The complete list of prompts appear in this paper in provided in Table \ref{tab::complete_prompt_list} (Appendix \ref{sec::complete_list_prompts}).

\textbf{Fine-Tuning Based Methods:} Fine-tuning refers to supervised fine-tuning (\textit{SFT}) in our context. We separate our discussion about fine-tuning into two parts based on the datasets adopted: (1) fine-tuning on reasoning-related tasks like math and programming (2) fine-tuning on the long CoT data popularized by the OpenAI-o1. For detailed introduction to the datasets, please refer to Appendix \ref{sec::complete_setup_finetuning}.

\textbf{Evaluation Protocol:} We estimate the reasoning abilities of LLMs with their \textit{accuracy}, defined as the percentage of correctly solved problems in the GSM8K test set \citep{cobbe2021training}, which comprises approximately 1,000 grade-school math problems presented in natural language. We delay the details of reasoning evaluation to Appendix \ref{sec::reason_eval_setup}.


\subsection{LLM Safety}
\label{sec::safety_preliminary}
The safety of large language models has been a paramount concern \citep{wang2023decodingtrust, zhang-etal-2024-safetybench}, which shall be paid more attention to as more and more capable models emerge. Below we outline our evaluation protocol for LLM safety and the complete setup can be found in Appendix \ref{sec::safety_eval_setup}.

In this paper, we assess the safety of LLMs using two types of \textit{Safety Score}: (1) \textit{Direct-Querying Safety Score} that is defined to be the percentage of harmless and safe responses on a dataset consisting of pure harmful instructions. (2) \textit{Jailbreak Safety Score} that is measured on the same dataset, but with adversarially modified instructions incorporating three jailbreak methods: GCG \citep{zou2023universal}, AIR \citep{reform-attack}, and the decomposition-based attack \citep{li-etal-2024-drattack}. Our safety evaluation follows the methodology of HarmBench \citep{mazeika2024harmbench}, using its LLM-based scoring system to quantify safety performance. 



\section{Safety Challenges in Improving LLM Reasoning with Prompts}
\label{sec:prompting_result}

Using prompts to enhance either reasoning or safety during inference is a common practice. However, there is a lack of a systematic understanding of how optimizing for one aspect affects the other, leading to overlooked pitfalls such as those studied in \citet{not-think}. To bridge this gap, we investigate the interplay between safety and reasoning in the context of inference-time prompting. 

In this section, we will first present a general \textbf{negative} correlation between safety and accuracy that we have observed on a broad set of prompts, models, and jailbreak attacks. Second, to obtain deeper insights into the correlation, we will proceed by a detailed case study that reveals the intriguing phenomena behind. Lastly, we will propose several alleviating methods targeted at the trade-off.


\subsection{Revealing the Negative Correlation Between Reasoning and Safety}
\label{sec::prompt_negative_correlation}


\begin{wrapfigure}{r}{0.5\textwidth}
    \begin{minipage}{0.45\textwidth}
        \centering
        \includegraphics[page=1,width=\textwidth]{figures/Avg_Correlation.pdf}
        \caption{The \textbf{negative} correlation between accuracy and safety. We evaluate seven LLMs with eleven prompts under three jailbreak attacks, GCG, AIR, and the decomposition attack (Decom.). Each dot stands for a prompt positioned by its averaged accuracy and safety across the models. We present the correlation coefficient (\textit{C}) between accuracy and safety against each attack, all of which show strong negative correlation between the two indicators.}
        \label{fig::avg-safety-acc-correlation}
    \end{minipage}
\end{wrapfigure}

We would like to begin our observation with a brief experimental setup. 
Our experiments include 12 types of commonly used prompts, including the reasoning-oriented prompts introduced in Section \ref{sec::reasoning_preliminary}, two widely used system prompts, and two safety-oriented prompts.
For LLMs, we consider 7 main-stream LLMs, including Llama-2 series \citep{touvron2023llama2}, Llama-3.1-8B-Instruct \citep{dubey2024llama3}, Mixtral-8x7B-Instruct \citep{jiang2023mistral}, and Qwen-2.5 series \citep{yang2024qwen25mathtechnicalreportmathematical}. We provide the complete introduction to the prompts and models in Appendix \ref{sec::complete_setup_prompting}. 

To reveal the relationship between reasoning ability and safety for inference time prompting, we first need to measure the safety score and accuracy of each prompt, which we define to be the average value across the models introduced above using the same prompt. Then we represent the prompts with dots positioned by their accuracy and safety score in Figure \ref{fig::avg-safety-acc-correlation}. Finally, we report the Pearson Correlation Coefficient (\textit{C}) \citep{freedman2007statistics} between the accuracy and the safety score against each jailbreak attack (GCG, AIR, and Decom.). The complete results of each model using every prompt are provided in Table \ref{tab::complete_prompting_results_acc} and Table \ref{tab::complete_prompting_results_asr} (Appendix \ref{sec::complete_results_prompting}).

Inspecting the plot, the first thing we notice is the strong negative correlation between the safety score and the accuracy, with all of the correlation coefficients being less than -0.75 and the P-values being less than 0.01 (0.004, 0.008, and 0.0002 to be exact). Such a negative correlation suggests that there exists a notable trade-off between safety and reasoning ability when it to comes to improving them with prompts at inference time. Comparing the three jailbreak attacks, the AIR attack and the Decom. attack are written in natural language and they demand attackers to reason to obfuscate the harmful intention of the instructions. However, despite their naturalness, they exhibit a stronger safety-accuracy tradeoff than GCG, appealing more focus on defending reasoning-related attacks like them as growingly capable models are produced. 

Based on the general trend we have discussed above, we have revealed a worrying negative correlation between reasoning and safety in the domain of prompting, shadowing the relationship between the reasoning ability of LLMs and their safety. Next, we will dive into the mysterious correlation with a detailed case study, sharing our insights into the reasons behind the trade-off.








\subsection{A Case Study into the Negative Correlation}


\begin{figure}[t]
    \centering
    \captionsetup[subfigure]{labelformat=simple, labelsep=space, skip=3pt} % 增加标题与图片间距
    \renewcommand{\thesubfigure}{(\alph{subfigure})}
    
    % 子图 (a)
    \subfloat[CoT prompts hurt safety\label{fig::case_study_prompt_1}]{
        \begin{minipage}{0.23\textwidth}
            \centering
            \includegraphics[page=1,width=\textwidth]{figures/acc_asr_Experiment_1.pdf}
        \end{minipage}
    }
    % 子图 (b)
    \subfloat[Reflection prompts hurt safety\label{fig::case_study_prompt_2}]{
        \begin{minipage}{0.23\textwidth}
            \centering
            \includegraphics[page=1,width=\textwidth]{figures/acc_asr_Experiment_2.pdf}
        \end{minipage}
    }\hspace{2mm}
    % 子图 (c)
    \subfloat[Safety prompts impair acc.\label{fig::case_study_prompt_3}]{
        \begin{minipage}{0.23\textwidth}
            \centering
            \includegraphics[page=1,width=\textwidth]{figures/acc_asr_Experiment_3.pdf}
        \end{minipage}
    }
    % 子图 (d)
    \subfloat[A balancing attempt\label{fig::case_study_prompt_4}]{
        \begin{minipage}{0.23\textwidth}
            \centering
            \includegraphics[page=1,width=\textwidth]{figures/acc_asr_Experiment_4.pdf}
        \end{minipage}
    }
    
    \caption{
        \textbf{Effects of different prompting strategies.} Few-shot CoTs are denoted as \textit{-n}, with \textit{-M} and \textit{-S} representing math and safety demonstrations, respectively. \textit{Sys-t1} and \textit{Sys-ref} are reflection-based system prompts. \textit{ICD} and \textit{Ref-S} are safety-oriented prompts.
        (a) Using CoT prompts and adding math-specific CoT examples decreases safety compared to the default system prompt.  
        (b) Reflection-based system prompts lower safety scores relative to the default prompt.
        (c) Four safety-oriented prompts significantly reduce GSM8K accuracy compared to a naive CoT prompt.
        (d) Extending the CoT prompt with safety and math demonstrations (with switched order) alleviates the trade-off between accuracy and safety.
    }
    \label{fig::case_study_prompts}
\end{figure}

We adopt Llama-2-13B-chat \citep{touvron2023llama2} to conduct our case study for its moderate model size, balance between general utility and safety, and wide application. Regarding notation: (1) \textit{Default} refers to the default system prompt of Llama-2 series. (2) For few-shot CoT, 
we use \textit{-n} to denote the number of shots with \textit{-M} and \textit{-S} standing for Math and Safety demos respectively. (3) Safety-oriented prompts: We also experiment with how reasoning ability is affected as we adopt prompts specifically for safety, involving safety-reflection (\textit{Ref-S}) and In-Context Defense \citep{wei2023jailbreak} (\textit{ICD}). 

We report the \textit{Jailbreak Safety Score} achieved by the AIR \citep{reform-attack} attack in this section. Our case study is separated into three parts, discussing (1) how CoT prompts affect safety (2) how reflection-based prompts affect safety (3) how safety-augmented prompts influence reasoning ability.


\textbf{CoT prompting \& Reflection prompting hurts safety} (Figure \ref{fig::case_study_prompt_1} \& Figure \ref{fig::case_study_prompt_2}). We prompt the model with zero-shot CoT (\textit{Let's think step by step}) and few-shot CoT with additional math examples, bringing a maximum increase of 15.3\% in accuracy. However, we not only observe at least 10\% decreases in safety score compared to the default system prompt, but we also notice safety generally decreases as we insert more math examples. The finding complements current research on ICL-based jailbreaking \citep{wei2023jailbreak, anil2024manyshot}, revealing that benign reasoning examples can also degrade safety. Although the two reflection-based system prompts lead to less safety damage than CoT, neither of them notably increases accuracy compared to the default prompt.


\textbf{Safety prompting compromises accuracy} (Figure \ref{fig::case_study_prompt_3}). Augmenting the safety of LLMs with prompts is a commonly adopted strategy to improve the model's safety. Besides its safety benefits, we further explore such action's impact on LLM's reasoning ability in this section. CoT-1-S sacrifices 5\% accuracy on GSM8k for 20\% increase in safety score, adding another safety demo further reducing the accuracy for 7\%. ICD and Safety-Reflection (\textit{Ref-S}) results in accuracies even lower than the default prompt. 

To conclude, the above case study sheds light on the negative impact that reasoning-related prompts have on safety and vice versa, which overall contributes to the strong negative correlation shown in Figure \ref{fig::avg-safety-acc-correlation}. Though the conclusion so far seems daunting, we proceed by showing that it is possible to improve reasoning and safety simultaneously over the naive prompts when careful design choices are made.

\subsection{Attempts to Alleviate the Negative Correlation}
\label{sec::prompt_alleviation}
Last but not least, continuing the setup of the case study, we make preliminary attempts to balance between safety and accuracy via prompting. Specifically, we extend the naive CoT prompt with one safety demo and one math demo, and experiment with switching their order (Figure \ref{fig::case_study_prompt_4}). We first notice that the CoT-S-M does better than zero-shot CoT in both accuracy and safety score, making it a more ideal choice. Second, we argue that the order of the demos matters, as the ASR changes for 10\% and the ACC changes for 6\% due to the switching. The attempt demonstrates the possibility of \textit{Pareto} improvement on the naive prompts in terms of reasoning and safety. We provide a complete discussion about the alleviation methods with additional models and prompts in Appendix \ref{sec::alleviate_discuss_prompt}.
%
\begin{center}
\begin{cvbox}[~~Takeaways]
\begin{itemize}
    \item Reasoning prompts in their pure forms are harmful to safety, so do include safety-related reminders/demos when using them.
    \item Reminding too much about safety in the prompts damages reasoning ability to a non-acceptable extent.
    \item There exists a general trade-off between reasoning and safety when it comes to prompts. Luckily, \textit{Pareto} improvements are possible when careful design choices are made. 
\end{itemize}
\end{cvbox}
\end{center}

\section{Safety Challenges in Improving LLM Reasoning with Fine-Tuning}
Besides improving LLM reasoning in the inference stage, methods applied in the post-training stage like fine-tuning are also widely adopted for the same purpose. Thereby, we continue our investigation into the interplay between reasoning and safety by discussing supervised fine-tuning (\textit{SFT}) LLMs. 
There have been works studying the safety costs of benign fine-tuning \citep{qi2024finetuning, he2024what}. However, as what we have discussed about prompts in the last section, reasoning could share a particular relationship with safety, which is worth careful inspection given its importance in achieving capable and trustworthy LLMs. 
In this section, we will first measure whether fine-tuning on CoT style responses will bring you additional safety costs. Furthermore, we shall take a deep dive into fine-tuning LLMs on the long CoT data popularized by OpenAI-o1 \citep{jaech2024openai}, studying the safety cost induced by the particular type of reasoning data. Lastly, we attempt to propose methods for preserving the safety after fine-tuning with CoT data.

To begin, we introduce three prompts appearing throughout this section: (1) \textit{Default}: The default system prompt from the Llama-2 series. (2) \textit{Helpful (You are a helpful assistant)}: A simple baseline prompt. (3) \textit{CoT (Let's think step by step)}: The prompt proposed by \citet{wei2022chain} to elicit LLM reasoning abilities.



\subsection{Fine-Tuning on CoT Style Responses Induce Additional Safety Tax}
\label{sec::short_cot_finetune}
In this section, we adopt three datasets for fine-tuning, Alpaca \citep{alpaca} for general utility, GSM8K \citep{cobbe2021training} for math, and Alpaca-Python \citep{alpaca-python} for coding. To control the quality of the datasets, we only keep their instructions and re-generate the responses in different styles with GPT-4o-mini \citep{gpt4}. Specifically, we generate the responses with three system prompts respectively: (1) \textit{Default} (2) \textit{Helpful} (3) \textit{CoT}, leading to three different response styles from GPT-4o-mini for each of the datasets.


The first question we ask is: \textbf{given a fixed set of instructions, will fine-tuning on CoT style responses be the worst one in safety among the three response styles?} To answer this, we fine-tune Llama-2-7B-chat on 100 samples from the three datasets re-generated by us, resulting in nine models in total. For each fine-tuned model, we present the average safety score among three jailbreak attacks, GCG, AIR, and Decom., while applying different inference prompts. The average values are provided in Table \ref{tab::finetune_100_result} and the complete results are provided in Table \ref{tab::complete_safety_scores_100_finetune} (Appendix \ref{tab::complete_safety_scores_100_finetune}). 

\begin{table*}[th]
    \centering
    \small
    \caption{Fine-tuning on CoT style responses results in the lowest jailbreak safety scores among three response styles. We fine-tune Llama-2-7B-chat on 100 samples from the Alpaca, Math, and Code datasets generated by us and present the average jailbreak safety scores. We observe that the CoT responses generally lead to the worst safety. We mark the lowest safety score in each dataset as \textbf{bold}.}
    \label{tab::finetune_100_result}
    \begin{adjustbox}{width=1.0\textwidth}
    \begin{tabular}{| c | c | c  c  c | c  c  c | c  c  c | c | }
        \toprule
        \textbf{Jailbreak Safety ($\uparrow$)} & Init & \multicolumn{3}{|c|}{FT on Alpaca} &  \multicolumn{3}{|c|}{FT on Math} &  \multicolumn{3}{|c|}{FT on Code} & \\
        \midrule
        Training Data Style & No &Default & Helpful & CoT &Default & Helpful & CoT &Default & Helpful & CoT & Avg.\\
        \midrule
        Inference w/Default & 0.749 & 0.953 & 0.969 & \textbf{0.934} & 0.950 & \textbf{0.910} & 0.920 & 0.942 & 0.931 & \textbf{0.920} &  0.918 \\
        Inference w/ Helpful & 0.579 & 0.969 & 0.864 & \textbf{0.857} & 0.906 & \textbf{0.768} & 0.774 & 0.931 & 0.802 & \textbf{0.757} & 0.821 \\
        Inference w/ CoT & 0.604 & 0.934 & 0.838 & \textbf{0.803} & 0.923 & 0.784 & \textbf{0.757} & 0.921 & 0.730 & \textbf{0.693} & 0.799 \\
        \midrule
        Avg. & 0.644 & 0.952 & 0.890 &\textbf{ 0.865} & 0.926 & 0.821 & \textbf{0.817} & 0.931 & 0.821 & \textbf{0.790} &  0.846 \\
        \bottomrule
    \end{tabular}
    \end{adjustbox}
\end{table*}



We observe that \textbf{CoT style responses result in the lowest safety scores on all of the three datasets when averaged across inference prompts.} Take the coding dataset as an example, compared to responses from the \textit{Default} prompt, fine-tuning on \textit{CoT} responses leads to an extra 14\% drop in safety score. 
We attribute the phenomenon being not so obvious in the Math dataset to the fact that GPT-4o-mini automatically reason step by step about math problems without explicit prompting. In summary, the results suggest that additional safety costs could be brought by fine-tuning on CoT style responses even when the instructions are all benign. Besides, we note that SFT on math and coding datasets are more destructive to safety than general utility datasets like Alpaca, with \citet{he2024what} also reporting that math questions are particularly harmful to safety when fine-tuning \footnote{\textbf{Remark: }At first glance, one may be puzzled by the results in Table \ref{tab::finetune_100_result} that fine-tuning improves the safety scores when compared to the initial model. There are three factors we believe to contribute to this phenomenon: (1) We follow the fine-tuning strategy proposed by \citet{lyu2024keeping} that removes the system prompt during fine-tuning, which alleviates the safety cost. (2) Our responses is generated with GPT-4o-mini, a highly safe and well-aligned model, making the fine-tuning possibly beneficial. (3) We ensemble Llama-2-7B-Chat when generating the GCG suffix. After fine-tuning, the GCG-suffix becomes less targeted at the models since their weights are different, making this specific attack notably weaker as shown in the detailed results (Table \ref{tab::complete_safety_scores_100_finetune}, Appendix \ref{sec::complete_results_finetuning}).
}.

Secondly, we ask: \textbf{how will safety be affected as we fine-tune on more CoT data.} Following the question, we fine-tune Llama-2-7B-chat on \{0, 4000, 8000, 16000\} samples from (1) the Alpaca dataset and (2) a mixture of the math and the code datasets, all of which are re-generated by us with GPT-4o-mini using the \textit{CoT} prompt. We present the direct-querying safety scores in Table \ref{tab::finetune_scaling_alpaca_gsm_result}. We notice that the two datasets exhibit similar patterns. Take the Math-Code dataset as an example, as the number of fine-tuning data increases, the safety score first increases then sharply decreases, going down from 0.844 to 0.691 when tested with CoT prompts. However, as we show in the complete results (Table \ref{tab::complete_results_scaling_short_cot}, Appendix \ref{sec::complete_results_finetuning}), fine-tuning on the Alpaca dataset generated with the \textit{Default} prompt leads to much less safety damage when using 16000 samples.

\begin{table}[h]
\small
\centering
\caption{Fine-tuning on more CoT responses generally leads to more decrease in safety. We fine-tune Llama-2-7B-chat on \{0, 4000, 8000, 16000\} samples from (1) the Alpaca dataset and (2) a mixture of the Math and the Code datasets, both of which are re-generated by us with the CoT style responses. We notice that as the number of training data increases, safety scores gradually decrease. The lowest safety score in each row is marked as \textbf{bold}.}
\label{tab::finetune_scaling_alpaca_gsm_result}
\begin{tabular}{| c | c | c | c  c  c |}
    \toprule
    \textbf{Direct Safety} ($\uparrow$) & Num. Data & 0 & 4000 & 8000 & 16000 \\
    \midrule
    \multirow{3}{*}{Alpaca} & w/ Default & 0.991 & 0.963          & \textbf{0.774} & 0.781 \\
    & w/ Helpful & 0.837 & 0.966          & 0.768 & \textbf{0.719} \\
    & w/ CoT &  0.844 & 0.972          & 0.747 & \textbf{0.691} \\
    \midrule
    \multirow{3}{*}{ \makecell{Math-Code}} & w/ Default & 0.991 & 0.950           & \textbf{0.827} & 0.834 \\
    & w/ Helpful & 0.837  & 0.940           & 0.800    & \textbf{0.766} \\
    & w/ CoT & 0.844  & 0.903          & 0.719 & \textbf{0.688} \\
    \bottomrule
\end{tabular}
\end{table}

Summarizing the above experiments, we show that fine-tuning LLMs on reasoning related datasets brings extra safety cost as we (1) adopt CoT style responses (2) use more CoT training data (within a reasonable range), but the decreases are rarely substantial. We continue our study on reasoning fine-tuning by studying datasets consisting of the prolonged Chain-of-Thoughts.



\subsection{Fine-tuning on Long CoT Data can Severely Hurt Safety}
\label{sec::long_cot_results}

\begin{figure*}[t]
    \centering
    \captionsetup[subfigure]{labelformat=simple, labelsep=space, skip=3pt} % 增加标题与图片间距
    \renewcommand{\thesubfigure}{(\alph{subfigure})}
    
    % 子图 (a)
    \subfloat[O1-OPEN/OpenO1-SFT Dataset\label{fig:o1_scaling_result_1}]{
        \begin{minipage}{0.475\textwidth}
            \centering
            \includegraphics[page=1,width=\textwidth]{figures/open_o1.pdf}
        \end{minipage}
    }\hspace{2mm}
    % 
    \subfloat[NovaSky-AI/Sky-T1\_data\_17k Dataset\label{fig:o1_scaling_result_2}]{
        \begin{minipage}{0.475\textwidth}
            \centering
            \includegraphics[page=1,width=\textwidth]{figures/novasky_o1.pdf}
        \end{minipage}
    }

    \caption{Long CoT fine-tuning can lead to catastrophic damage to safety. We fine-tune Llama-2-7B-Chat on two open-source long CoT datasets, O1-OPEN/OpenO1-SFT and NovaSky-AI/Sky-T1\_data\_17k. We fine-tune Llama-2-7B-Chat on incrementally more percentages of the datasets and evaluate the direct safety scores. Although the two datasets exhibit different scaling trend as the amount of data increases, we observe both datasets causes decreases of at least 30\% in safety score when considering the worse cases.}
    \label{fig::o1_result}
\end{figure*}


Recently, OpenAI-o1 \citep{jaech2024openai} has demonstrated the new height of LLM reasoning. The novel thinking paradigm, where the models generate long CoTs consisting of dynamic exploration and in-context reflection, is regarded as the key to success. One approachable way to transform the existing LLMs into such reasoning models is supervised fine-tuning on the long CoT datasets. However, unlike conventional fine-tuning which generally maintains consistent response structures, long CoT adaptation requires a larger transformation of the output schema – introducing a long thinking process before outputting the final response. This radical change in generation patterns may induce more harm to existing safety mechanisms in the models, which are not aligned with such model behaviors. Thus, we hereby measure the safety changes during the fine-tuning with long CoT datasets to see whether such risk actually exists and what will happen as we vary the extent of the transformation. 

We fine-tune Llama-2-7B-Chat on two open-source long CoT datasets, O1-OPEN/OpenO1-SFT \citep{open-o1-2025} and NovaSky-AI/Sky-T1\_data\_17k \citep{sky-t1-2025}. To control the degree we transform the model, we sample incrementally large subsets of the datasets and individually fine-tune LLMs on every single subset. After fine-tuning, we evaluate their direct safety scores and present the main results in Figure \ref{fig::o1_result}. Complete results can be found in Table \ref{tab::complete_finetune_long_cot} (Appendix \ref{sec::complete_results_finetuning}). 

The first insight we have into the figure is that both of the datasets cause catastrophic damage to safety in their worst cases. When inference with the CoT prompt, fine-tuning on the OpenO1 and the Sky-T1-data decreases safety score from 0.844 to 0.331 and 0.456 respectively, making the model impossible to deploy without addition safety measures. Second, we observe the two datasets show different scaling trend as more data samples are used for fine-tuning. Open-O1 generally leads to worse safety score while NovaSky exhibits an interesting U-curve. After carefully inspecting the responses from the models, there are three phenomena that we think worth studying in the future, with the detailed examples and explanations of the phenomena present in Appendix \ref{sec::representative_responses}: 
\begin{itemize}
    \item \textbf{Increased Practicability and Complexity in Unsafe Responses: }Once complying to the instructions, the long CoT fine-tuned models generate model nuanced and practical harmful responses compared to the initial model.
    \item \textbf{Toxic Thought Leakage: }Although the model rejects the instruction in the final output, harmful and toxic contents still appear in the thoughts.
    \item \textbf{User Intention Alternation: }When feeling uneasy about the instruction and reluctant to reject directly, the models sometimes alternate the intention of the instruction by \textit{maybe it is a joke} or \textit{maybe the instruction can be understood in a different way}.
\end{itemize}

\par Following OpenAI-o1, the release of DeepSeek-R1 \citep{deepseekai2025deepseekr1incentivizingreasoningcapability} has marked the new height of open-source long CoT LLMs. Besides the 671B R1 model, DeepSeek has also released smaller models that are trained with distilling the reasoning ability of R1. Among the released small models, Llama-3.3-70B-Distill-R1 is the only one that is built upon a general purpose LLM that has gone through the instruction tuning phase, i.e., Llama-3.3-70B-Instruct \citep{dubey2024llama3}. 

\begin{table}[h]
    \centering
    \small
    \caption{Fine-tuning Llama-3.3-70B-Instruct on the long CoT data generated by DeepSeek-R1 notably deteriorates its safety. We compare the safety of Llama-3.3-70B-Distill-R1 released by DeepSeek with the original Llama-3.3-70B-Instruct. In the table, \textit{(Output)} denotes the case where we only consider the safeness of the output part of the responses, excluding the internal thinking part. We also report the safety score measured when considering the complete responses \textit{(complete)}. We observe non-negligible safety degradation during the reasoning enhancement process. The lowest scores in each column is marked as \textbf{bold}.}
\begin{tabular}{| l | c | c c c | c | }
    \toprule
    Model & Direct Query & GCG & Decom. & Reform. & Avg \\
    \midrule
    Llama-3.3-70B-Instruct & 0.788 & 0.360 & 0.710 & 0.800 & 0.664 \\
    \midrule
    Llama-3.3-70B-Distill-R1 (Output) & 0.704 & 0.229 & 0.410 & 0.640 & 0.495 \\
    Llama-3.3-70B-Distill-R1 (Complete) & \textbf{0.541} & \textbf{0.225} & 0.370 & 0.640 & 0.444 \\
    \midrule
    DeepSeek-R1-API (Output) & 0.666 & 0.563 & 0.170 & 0.560 & 0.489 \\
    DeepSeek-R1-API (Complete) & 0.560 & 0.491 & \textbf{0.150} & \textbf{0.550} & \textbf{0.437} \\
    \bottomrule
\end{tabular}
    \label{tab::deepseek_r1_result}
\end{table}

\par Continuing our investigation into the safety cost brought by long CoT fine-tuning as discussed above, we evaluate the safety scores of both Llama-3.3-70B-Instruct and its R1-distilled version with the three jailbreak attacks introduced in Section \ref{sec::safety_preliminary} as well as direct querying. Besides, we also evaluated the safety of DeepSeek-R1 through the official API. For the base model, we set the system prompt to be the default one while we adopt empty system prompt for the R1 models as advised by DeepSeek. We present the results in Table \ref{tab::deepseek_r1_result}. 

\par First, similar to the results shown in Figure \ref{fig::o1_result}, fine-tuning Llama-3.3 on the long CoT data brings non-negligible safety tax. Safety scores notably decrease after long CoT tuning under the four attacks, with a maximum decrease of 30\% (34\% when considering the complete responses). Second, we notice that for the direct querying, there are significant percentages (16.3\% and 10.6\%) of the responses in which the long CoT models generate unsafe internal thoughts despite the final output being safe, which verifies the general existence in long CoT LLMs of the \textit{toxic thought leakage} phenomenon (Example 3 introduced in Section \ref{sec::representative_responses}). Lastly, it further worries us that the distillation process does not strictly interpolate the safety abilities of the base model and the distilled model. Instead, the resulting model could exhibit worse safety than both of them despite the clear advancements in reasoning ability compared to the base model. For example, Llama-3.3-70B-Distill-R1 exhibits the worst safety performance under GCG attack among the three models (54.1\% $<$ 56.0\% $<$ 78.8\%). 

\par To summarize the above observations, we show that the long CoT fine-tuning induces notable safety tax and emergent trustworthy issues during the reasoning improvement process, calling for tailored safety mechanisms for training the long CoT LLMs. To mitigate the safety risk, one would easily think of mixing safety data during fine-tuning. Following the intuition, we will next explore how to incorporate safety data into the long CoT fine-tuning to achieve better balance between safety and reasoning.






\subsection{Attempts on Preserving the Safety After Long Cot Fine-Tuning}
\label{sec::long_cot_alleviation}


Concerned by the safety challenges in fine-tuning LLMs on the long CoT datasets, we next explore how to blend safety data with the reasoning data to balance between reasoning ability and safety. For safety data, we sample 0.1K harmful instructions from the Anthrophic-HH-RLHF dataset \citep{bai2022traininghelpful}. Then we experiment with two kinds of safety responses to the instructions: (1) \textit{Direct Refusal:} simply rejecting the instruction without extra explanation, and (2) \textit{Safety Reflection:} the model first reflects on the safety implications of the instructions before expressing inability to help. Our intuition behind the latter is to leverage the improved reasoning ability for safety via reflection. Lastly, we fine-tune Llama-2-7B-chat and Mistral-7B-Instruct on 4K samples from NovaSky-t1-SFT mixed with the two kinds of safe data. We evaluate their accuracy and safety score using the \textit{Default} system prompt and the results are listed in Table \ref{tab::mix_safe_data}. We provide experimental details in Appendix \ref{sec::complete_assess_setup} and complete experimental records in Appendix \ref{sec::alleviate_discuss_fine_tune}.


\begin{table}[h]
    \small
    \centering
    \caption{Reflective safe data achieves better safety-accuracy balance in long-CoT fine-tuning. We blend 100 safety samples with the long CoT datasets. The safety samples are generated in two ways: (1) rejecting the instruction without explanation (2) reflecting on safety first and then reject. We summarize that adding reflective safety data alleviates safety costs while boosting accuracy, exhibiting better balance between the two than direct rejecting. Below we present the direct-querying safety scores and the accuracies on GSM8K, with the best score in each row marked as \textbf{bold}.}
    \label{tab::mix_safe_data}
    \begin{tabular}{| c | c c | c c |}
        \toprule
        Model & \multicolumn{2}{c|}{Llama-2} & \multicolumn{2}{c|}{Mistral} \\
        \midrule
        Metric & \textbf{Safety} ($\uparrow$) & \textbf{ACC} ($\uparrow$) & \textbf{Safety} ($\uparrow$) & \textbf{ACC} ($\uparrow$) \\
        \midrule
        Init & 0.991 & 0.156 & 0.541 & 0.310 \\
        \midrule
        w/o Safety & 
        0.669\textcolor{red}{(-0.322)} & 
        0.227\textcolor{green!50!black}{(+0.071)} & 
        0.387\textcolor{red}{(-0.154)} & 
        0.425\textcolor{green!50!black}{(+0.115)} \\
        
        +Direct Safety & 
        0.777\textcolor{red}{(-0.214)} & 
        0.219\textcolor{green!50!black}{(+0.063)} & 
        \textbf{0.737}\textcolor{green!50!black}{(\textbf{+0.196})} & 
        0.400\textcolor{green!50!black}{(+0.090)} \\
        
        +Reflective Safety & 
        \textbf{0.834\textcolor{red}{(-0.157)}} & 
        \textbf{0.240}\textcolor{green!50!black}{(\textbf{+0.084})} & 
        0.709\textcolor{green!50!black}{(+0.168)} & 
        \textbf{0.442}\textcolor{green!50!black}{(\textbf{+0.132})} \\
        \bottomrule
    \end{tabular}
\end{table}


The first insight we can gain is that adding a small amount of safe data is notably beneficial for preserving safety after fine-tuning in this setting, reducing safety drop for more than 30\% compared to pure tuning. Second, comparing the \textit{Direct Refusal} with the \textit{Safety Reflection}, we observe that the latter has a clear advantage in accuracy, surpassing the former for 2.1\% and 4.2\%. Unexpectedly, the models fine-tuned on the reasoning data mixed with the \textit{Safety Reflection} achieves higher accuracy than fine-tuning on the pure dataset. We hypothesize that the safety-related reflections serve as data augmentations to the original dataset, stopping the models from over-fitting to certain responses templates without really learning how to reason, thus making the reasoning process more robust and generalizable. On the contrary, \textit{Direct Refusal} simply restricts the model to apologize and reject, which goes against the training objective of the long CoT data. 

So far, we have observed that fine-tuning on reasoning datasets, especially o1-like, can cause notable safety costs while improving the reasoning ability of LLMs. We have also discussed about mixing safety data to mitigate the challenges, and argued that mixing reflective safety data is more suitable for o1-like datasets than direct refusal. Next, we shall extend our research to another aspect of trustworthiness beyond safety, revealing more risks and potential in improving LLM reasoning.

\begin{center}
\begin{cvbox}[~~Takeaways]
\begin{itemize}
    \item Fine-tuning on more CoT style responses hurts safety more, but not substantially.
    \item Fine-tuning on long CoT can lead to catastrophic degradation in safety. Be careful when using them.
    \item Mixing safety data is notably helpful for preserving safety during fine-tuning on long CoT. We recommend using reflective safety data rather than direct refusal for better balance between safety and utility.
\end{itemize}
\end{cvbox}
\end{center}






\section{Extension to Privacy Risks}

\begin{wrapfigure}{r}{0.5\textwidth}
    \begin{minipage}{0.45\textwidth}
        \centering
        \includegraphics[page=1,width=\textwidth]{figures/privacy_new.pdf}
    \caption{Effects of inference-time prompting to privacy. We see that incorporating prompts to improve reasoning will also bring negative effects to privacy.
    }
    \label{fig::prompts_trustworthy}
    \end{minipage}
\end{wrapfigure}

In this section, we generalize our observations on the changes of safety to the privacy risks of LLMs. Following the experimental setting in Section \ref{sec:prompting_result}, for methods that enhance reasoning with prompt engineering, we perform experiments on 7 LLMs and average the results on each prompt. While for fine-tuning on the long CoT datasets, we evaluate the same set of models as Table \ref{tab::mix_safe_data}. The privacy-preserving capability of the models is evaluated on a dataset from the TrustLLM benchmark~\citep{huang2024position}. For more details of the evaluation setup, please refer to Appendix \ref{sec::privacy_example}.







Firstly, for inference-time prompting, we summarize the results in Figure~\ref{fig::prompts_trustworthy}. Similar to the observations on safety, reflection based prompts and zero-shot CoT for enhancing reasoning capability exhibit decreased agreement with human on privacy risks compared to the default prompt. Different from safety, both CoT-S-M and CoT-M-S fail to out-perform the default choice, suggesting the challenges of preserving the overall trustworthiness of LLMs while improving their reasoning abilities. 

Secondly, we shift our attention to fine-tuning on the long CoT reasoning data, which also fails to preserve privacy as present in Table \ref{tab::privacy_ft}. 
Take mistral \citep{jiang2023mistral} as an example, fine-tuning on the 4k long CoT data leads to a catastrophic decrease of 0.48 in the correlation coefficient, meaning the fine-tuned model rarely agrees with human in terms of privacy risks. When it comes to the models fine-tuned on safety-aware datasets, we observe notable increases in privacy scores gained due to the additional safety data. Similar to Table \ref{tab::mix_safe_data}, reflective safety improves over direct safety for 0.027 and 0.062 for the two models in terms of privacy score.


\begin{table}[h]
    \small
    \centering
    \caption{Privacy scores of models fine-tuned on datasets of different components. The models are the same as Table \ref{tab::mix_safe_data}. The higher score for each model is in \textbf{bold}.}
    \label{tab::privacy_ft}
    \begin{tabular}{| c | c c | c c |}
        \toprule
        Model & Llama-2 &Mistral\\
        \midrule
        Init & 0.180 & 0.459  \\
        \midrule
        w/o Safety & 
        0.107\textcolor{red}{(-0.073)}  & 
        -0.021\textcolor{red}{(-0.480)}  \\
        
        +Direct Safety & 
        0.186\textcolor{green!50!black}{(+0.006)}  & 
        0.045\textcolor{red}{(-0.414)}  \\
        
        +Reflective Safety & 
        \textbf{0.213\textcolor{green!50!black}{(+0.033)}}  & 
        \textbf{0.107}\textcolor{red}{(-0.352)} \\
        \bottomrule
    \end{tabular}
\end{table}

To summarize, we generalize our investigation into the relationship between safety and reasoning to the domain of privacy, where we reveal similar degradation in privacy as we improve reasoning with prompts and long CoT fine-tuning. Our attempts to alleviate the safety-reasoning trade-off via prompts fail in this circumstance while fine-tuning works fairly well. The above findings together mark our initial steps toward understand the relationship between reasoning and broader trustworthiness dimensions of LLMs, which we identify as a critical challenge for future work.



\section{Conclusion \& Limitation}% 
In this work, we investigate the safety risks induced by reasoning-enhancing methods in large language models, examining both prompt-based and fine-tuning approaches. Through a comprehensive evaluation of 12 prompt-based methods across 7 popular open-source LLMs, we reveal a consistent negative correlation between gains in reasoning capability and safety scores. Our analysis further demonstrates that while supervised fine-tuning effectively boosts reasoning performance, it concurrently incurs non-negligible safety costs—especially when employing long chain-of-thought data. These findings underscore the complex interplay between reasoning and safety, highlighting the need to preserve safety even as we advance reasoning capabilities. Moreover, we extend our observations to the privacy risks associated with LLMs, arriving at similar conclusions. To mitigate these risks, we propose safety preservation strategies for both prompting and fine-tuning scenarios, thereby offering actionable pathways for developing performant yet responsible LLMs.

While our study focuses on instruction-tuning methods due to practical constraints, this limitation also points to promising future research directions. In particular, reinforcement learning–based reasoning enhancement and multimodal reasoning approaches remain unexplored and represent crucial extensions for a comprehensive safety analysis. Nonetheless, our findings provide valuable insights into the relationship between reasoning and safety, enriching our understanding of LLM development and urging the AI community to address safety considerations with the same rigor as capability improvements.






\bibliography{main}
\bibliographystyle{plainnat}

\newpage
\appendix
\onecolumn

\input{Appendix}

\end{document}