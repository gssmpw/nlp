@article{openattack,
  title={Openattack: An open-source textual adversarial attack toolkit},
  author={Zeng, Guoyang and Qi, Fanchao and Zhou, Qianrui and Zhang, Tingji and Hou, Bairu and Zang, Yuan and Liu, Zhiyuan and Sun, Maosong},
  journal={arXiv preprint arXiv:2009.09191},
  year={2020}
}

@article{perspectiveapi,
  title     = {A New Generation of Perspective API: Efficient Multilingual Character-level Transformers},
  author    = {Alyssa Lees and Vinh Q. Tran and Yi Tay and Jeffrey Scott Sorensen and Jai Gupta and Donald Metzler and Lucy Vasserman},
  journal   = {Knowledge Discovery And Data Mining},
  year      = {2022},
}

@inproceedings{yao2020calm,
    title={Keep CALM and Explore: Language Models for Action Generation in Text-based Games},
    author={Yao, Shunyu and Rao, Rohan and Hausknecht, Matthew and Narasimhan, Karthik},
    booktitle={EMNLP},
    year={2020}
}

@inproceedings{bertscore,
  title={BERTScore: Evaluating Text Generation with BERT},
  author={Zhang, Tianyi and Kishore, Varsha and Wu, Felix and Weinberger, Kilian Q and Artzi, Yoav},
  booktitle={ICLR},
  year={2019}
}

@inproceedings{stereotype1,
    title = {Stereotyping {N}orwegian Salmon: An Inventory of Pitfalls in Fairness Benchmark Datasets},
    author = {Blodgett, Su Lin  and
      Lopez, Gilsinia  and
      Olteanu, Alexandra  and
      Sim, Robert  and
      Wallach, Hanna},
    booktitle = {ACL},
    year = {2021},
}

@article{ganguli2023capacity,
      title={The Capacity for Moral Self-Correction in Large Language Models}, 
      author={Deep Ganguli and Amanda Askell and Nicholas Schiefer and Thomas I. Liao and Kamilė Lukošiūtė and Anna Chen and Anna Goldie and Azalia Mirhoseini and Catherine Olsson and Danny Hernandez and Dawn Drain and Dustin Li and Eli Tran-Johnson and Ethan Perez and Jackson Kernion and Jamie Kerr and Jared Mueller and Joshua Landau and Kamal Ndousse and Karina Nguyen and Liane Lovitt and Michael Sellitto and Nelson Elhage and Noemi Mercado and Nova DasSarma and Oliver Rausch and Robert Lasenby and Robin Larson and Sam Ringer and Sandipan Kundu and Saurav Kadavath and Scott Johnston and Shauna Kravec and Sheer El Showk and Tamera Lanham and Timothy Telleen-Lawton and Tom Henighan and Tristan Hume and Yuntao Bai and Zac Hatfield-Dodds and Ben Mann and Dario Amodei and Nicholas Joseph and Sam McCandlish and Tom Brown and Christopher Olah and Jack Clark and Samuel R. Bowman and Jared Kaplan},
      year={2023},
      journal={arXiv preprint arXiv:2302.07459}
}

@article{stereotype2,
title = {How stereotypes are shared through language: A review and introduction of the Social Categories and Stereotypes Communication (SCSC) Framework},
author = {Beukeboom, {Camiel J.} and Christian Burgers},
year = {2019},
journal = {Review of Communication Research},
}

@article{bolukbasi2016man,
      title={Man is to Computer Programmer as Woman is to Homemaker? Debiasing Word Embeddings}, 
      author={Tolga Bolukbasi and Kai-Wei Chang and James Zou and Venkatesh Saligrama and Adam Kalai},
      year={2016},
      journal={arXiv preprint arXiv:1607.06520}
}

@inproceedings{stereotype3,
    title = {Language (Technology) is Power: A Critical Survey of {``}Bias{''} in {NLP}},
    author = {Blodgett, Su Lin  and
      Barocas, Solon  and
      Daum{\'e} III, Hal  and
      Wallach, Hanna},
    booktitle = {ACL},
    year = {2020}
}

@inproceedings{ood1,
    title = {Distributionally Robust Language Modeling},
    author = {Oren, Yonatan  and
      Sagawa, Shiori  and
      Hashimoto, Tatsunori B.  and
      Liang, Percy},
    booktitle = {EMNLP-IJCNLP},
    year ={2019}
}

@book{nltk,
  title={Natural language processing with Python: analyzing text with the natural language toolkit},
  author={Bird, Steven and Klein, Ewan and Loper, Edward},
  year={2009},
  publisher={" O'Reilly Media, Inc."}
}

@inproceedings{ood2,
  title     = {BREEDS: Benchmarks for Subpopulation Shift},
  author    = {Shibani Santurkar and Dimitris Tsipras and A. Madry},
  booktitle   = {ICLR},
  year      = {2020},
}

@inproceedings{ood3,
  author    = {Pang Wei Koh and Shiori Sagawa and Henrik Marklund and Sang Michael Xie and Marvin Zhang and Akshay Balsubramani and Weihua Hu and Michihiro Yasunaga and Richard Lanas Phillips and Irena Gao and Tony Lee and Etienne David and Ian Stavness and Wei Guo and Berton Earnshaw and Imran S. Haque and Sara M. Beery and Jure Leskovec and Anshul Kundaje and Emma Pierson and Sergey Levine and Chelsea Finn and Percy Liang},
  editor    = {Marina Meila and Tong Zhang},
  title     = {{WILDS:} {A} Benchmark of in-the-Wild Distribution Shifts},
  booktitle = {ICML},
  year      = {2021}
}


@inproceedings{fairness1,
  title     = {Feature Noise Induces Loss Discrepancy Across Groups},
  author    = {Fereshte Khani and Percy Liang},
  booktitle   = {ICML},
  year      = {2019}
}

@article{fairness2,
  title={Big Data's Disparate Impact},
  author={Solon Barocas and Andrew D. Selbst},
  journal={California Law Review},
  year={2016},
  volume={104},
  pages={671}
}

@article{fairness3,
  title     = {Fairness Through Awareness},
  author    = {C. Dwork and Moritz Hardt and T. Pitassi and O. Reingold and R. Zemel},
  journal   = {Information Technology Convergence And Services},
  year      = {2011},
  doi       = {10.1145/2090236.2090255}
}

@inproceedings{fairness4,
  title   = {Counterfactual fairness},
  author  = {Kusner, Matt J and Loftus, Joshua and Russell, Chris and Silva, Ricardo},
  booktitle = {NeurIPS},
  year    = {2017}
}

@inproceedings{fairness5,
  title     = {Roles for Computing in Social Change},
  author    = {Rediet Abebe and Solon Barocas and J. Kleinberg and K. Levy and Manish Raghavan and D. G. Robinson},
  booktitle   = {Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency},
  year      = {2019},
}

@article{hownet,
  title={OpenHowNet: An Open Sememe-based Lexical Knowledge Base},
  author={Fanchao Qi and Chenghao Yang and Zhiyuan Liu and Q. Dong and Maosong Sun and Zhendong Dong},
  year={2019},
  journal={arXiv preprint arXiv:abs/1901.09957}
}

@inproceedings{fever,
  title={FEVER: a Large-scale Dataset for Fact Extraction and VERification},
  author={Thorne, James and Vlachos, Andreas and Christodoulopoulos, Christos and Mittal, Arpit},
  booktitle={NAACL-HLT},
  year={2018}
}

@inproceedings{muppet,
  title={Human vs. Muppet: A Conservative Estimate of Human Performance on the GLUE Benchmark},
  author={Nangia, Nikita and Bowman, Samuel},
  booktitle={ACL},
  year={2019}
}

@article{Yang2018CharacterizingAA,
  title={Characterizing Audio Adversarial Examples Using Temporal Dependency},
  author={Zhuolin Yang and Bo Li and Pin-Yu Chen and Dawn Xiaodong Song},
  year={2018},
  journal={arXiv preprint arXiv:1809.10875}
}

@article{Carlini2018AudioAE,
  title={Audio Adversarial Examples: Targeted Attacks on Speech-to-Text},
  author={Nicholas Carlini and David A. Wagner},
  journal={S\&PW},
  year={2018}
}


@inproceedings{DBLP:conf/nips/MikolovSCCD13,
  author    = {Tomas Mikolov and
               Ilya Sutskever and
               Kai Chen and
               Gregory S. Corrado and
               Jeffrey Dean},
  editor    = {Christopher J. C. Burges and
               L{\'{e}}on Bottou and
               Zoubin Ghahramani and
               Kilian Q. Weinberger},
  title     = {Distributed Representations of Words and Phrases and their Compositionality},
  booktitle = {NeurIPS},
  year      = {2013}
}
@inproceedings{webson-pavlick-2022-prompt,
    title = "Do Prompt-Based Models Really Understand the Meaning of Their Prompts?",
    author = "Webson, Albert  and
      Pavlick, Ellie",
    booktitle = "NAACL",
    month = jul,
    year = "2022",  
}
@inproceedings{min-etal-2022-rethinking,
    title = "Rethinking the Role of Demonstrations: What Makes In-Context Learning Work?",
    author = "Min, Sewon  and
      Lyu, Xinxi  and
      Holtzman, Ari  and
      Artetxe, Mikel  and
      Lewis, Mike  and
      Hajishirzi, Hannaneh  and
      Zettlemoyer, Luke",
    booktitle = "EMNLP",
    year = "2022",
}
@inproceedings{
xie2022an,
title={An Explanation of In-context Learning as Implicit Bayesian Inference},
author={Sang Michael Xie and Aditi Raghunathan and Percy Liang and Tengyu Ma},
booktitle={ICLR},
year={2022}
}
@inproceedings{miller2021accuracy,
  title={Accuracy on the line: on the strong correlation between out-of-distribution and in-distribution generalization},
  author={Miller, John P and Taori, Rohan and Raghunathan, Aditi and Sagawa, Shiori and Koh, Pang Wei and Shankar, Vaishaal and Liang, Percy and Carmon, Yair and Schmidt, Ludwig},
  booktitle={ICML},
  year={2021}
}
@inproceedings{DBLP:conf/emnlp/PenningtonSM14,
  author    = {Jeffrey Pennington and
               Richard Socher and
               Christopher D. Manning},
  title     = {Glove: Global Vectors for Word Representation},
  booktitle = {EMNLP},
  year      = {2014}
}


@article{wall2021left,
  title={Left, Right, and Gender: Exploring Interaction Traces to Mitigate Human Biases},
  author={Wall, Emily and Narechania, Arpit and Coscia, Adam and Paden, Jamal and Endert, Alex},
  journal={arXiv preprint arXiv:2108.03536},
  year={2021}
}

@article{burghardt2020origins,
  title={Origins of Algorithmic Instabilities in Crowdsourced Ranking},
  author={Burghardt, Keith and Hogg, Tad and D'Souza, Raissa and Lerman, Kristina and Posfai, Marton},
  journal={Proceedings of the ACM on Human-Computer Interaction},
  year={2020}
}

@article{dynabenchqa,
  title={Beat the AI: Investigating Adversarial Human Annotation for Reading Comprehension},
  author={Max Bartolo and A. Roberts and Johannes Welbl and Sebastian Riedel and Pontus Stenetorp},
  journal={Transactions of the Association for Computational Linguistics},
  year={2020},
  volume={8},
  pages={662-678}
}

@article{textflint,
  title={Textflint: Unified multilingual robustness evaluation toolkit for natural language processing},
  author={Gui, Tao and Wang, Xiao and Zhang, Qi and Liu, Qin and Zou, Yicheng and Zhou, Xin and Zheng, Rui and Zhang, Chong and Wu, Qinzhuo and Ye, Jiacheng and others},
  journal={arXiv preprint arXiv:2103.11441},
  year={2021}
}

@inproceedings{dynabench,
  title={Dynabench: Rethinking Benchmarking in NLP},
  author={Douwe Kiela and Max Bartolo and Yixin Nie and Divyansh Kaushik and Atticus Geiger and Zhengxuan Wu and Bertie Vidgen and G. Prasad and Amanpreet Singh and Pratik Ringshia and Zhiyi Ma and Tristan Thrush and Sebastian Riedel and Zeerak Waseem and Pontus Stenetorp and Robin Jia and M. Bansal and Christopher Potts and Adina Williams},
  booktitle={NAACL},
  year={2021}
}

@article{stresstest,
  title={Stress test evaluation for natural language inference},
  author={Naik, Aakanksha and Ravichander, Abhilasha and Sadeh, Norman and Rose, Carolyn and Neubig, Graham},
  journal={arXiv preprint arXiv:1806.00692},
  year={2018}
}

@article{robustnessgym,
  title={Robustness gym: Unifying the nlp evaluation landscape},
  author={Goel, Karan and Rajani, Nazneen and Vig, Jesse and Tan, Samson and Wu, Jason and Zheng, Stephan and Xiong, Caiming and Bansal, Mohit and R{\'e}, Christopher},
  journal={arXiv preprint arXiv:2101.04840},
  year={2021}
}

@article{textattack,
  title={Textattack: A framework for adversarial attacks, data augmentation, and adversarial training in nlp},
  author={Morris, John X and Lifland, Eli and Yoo, Jin Yong and Grigsby, Jake and Jin, Di and Qi, Yanjun},
  journal={arXiv preprint arXiv:2005.05909},
  year={2020}
}

@inproceedings{criteria,
  title={What Will it Take to Fix Benchmarking in Natural Language Understanding?},
  author={Samuel R. Bowman and George E. Dahl},
  booktitle={NAACL},
  year={2021}
}

@inproceedings {textshield,
author = {Jinfeng Li and Tianyu Du and Shouling Ji and Rong Zhang and Quan Lu and Min Yang and Ting Wang},
title = {TextShield: Robust Text Classification Based on Multimodal Embedding and Neural Machine Translation},
booktitle = {USNIX Security},
year = {2020}
}


@InProceedings{P16-1123,
  author = 	"Wang, Linlin
		and Cao, Zhu
		and de Melo, Gerard
		and Liu, Zhiyuan",
  title = 	"Relation Classification via Multi-Level Attention CNNs",
  booktitle = 	"ACL",
  year = 	"2016",
}

@InProceedings{P16-1072,
  author = 	"Cai, Rui
		and Zhang, Xiaodong
		and Wang, Houfeng",
  title = 	"Bidirectional Recurrent Convolutional Neural Network for Relation      Classification    ",
  booktitle = 	"ACL",
  year = 	"2016",
}

@InProceedings{P16-2034,
  author = 	"Zhou, Peng
		and Shi, Wei
		and Tian, Jun
		and Qi, Zhenyu
		and Li, Bingchen
		and Hao, Hongwei
		and Xu, Bo",
  title = 	"Attention-Based Bidirectional Long Short-Term Memory Networks for Relation      Classification    ",
  booktitle = 	"ACL",
  year = 	"2016"
}

@InProceedings{K17-1034,
  author = 	"Levy, Omer
		and Seo, Minjoon
		and Choi, Eunsol
		and Zettlemoyer, Luke",
  title = 	"Zero-Shot Relation Extraction via Reading Comprehension",
  booktitle = 	"CoNLL",
  year = 	"2017",
}

@InProceedings{D17-1005,
  author = 	"Liu, Liyuan
		and Ren, Xiang
		and Zhu, Qi
		and Zhi, Shi
		and Gui, Huan
		and Ji, Heng
		and Han, Jiawei",
  title = 	"Heterogeneous Supervision for Relation Extraction: A Representation      Learning Approach",
  booktitle = 	"EMNLP",
  year = 	"2017"
}

@InProceedings{P18-1046,
  author = 	"Qin, Pengda
		and XU, Weiran
		and Wang, William Yang",
  title = 	"DSGAN: Generative Adversarial Training for Distant Supervision Relation Extraction",
  booktitle = 	"ACL",
  year = 	"2018"
}
@inproceedings{
zhou2022metscov,
title={{METS}-CoV: A Dataset of Medical Entity and Targeted Sentiment on {COVID}-19 Related Tweets},
author={Peilin Zhou and Zeqiang Wang and Dading Chong and Zhijiang Guo and Yining Hua and Zichang Su and Zhiyang Teng and Jiageng Wu and Jie Yang},
booktitle={NeurIPS Datasets and Benchmarks Track},
year={2022}
}

@InProceedings{P18-1201,
  author = 	"Huang, Lifu
		and Ji, Heng
		and Cho, Kyunghyun
		and Dagan, Ido
		and Riedel, Sebastian
		and Voss, Clare",
  title = 	"Zero-Shot Transfer Learning for Event Extraction",
  booktitle = 	"ACL",
  year = 	"2018",
}

@InProceedings{D16-1038,
  author = 	"Peng, Haoruo
		and Song, Yangqiu
		and Roth, Dan",
  title = 	"Event Detection and Co-reference with Minimal Supervision",
  booktitle = 	"EMNLP",
  year = 	"2016"
}

@article{kasai2022realtime,
  title={RealTime QA: What's the Answer Right Now?},
  author={Kasai, Jungo and Sakaguchi, Keisuke and Takahashi, Yoichi and Bras, Ronan Le and Asai, Akari and Yu, Xinyan and Radev, Dragomir and Smith, Noah A and Choi, Yejin and Inui, Kentaro},
  journal={arXiv preprint arXiv:2207.13332},
  year={2022}
}

@inproceedings{fisch-etal-2019-mrqa,
    title = "{MRQA} 2019 Shared Task: Evaluating Generalization in Reading Comprehension",
    author = "Fisch, Adam  and
      Talmor, Alon  and
      Jia, Robin  and
      Seo, Minjoon  and
      Choi, Eunsol  and
      Chen, Danqi",
    booktitle = "Proceedings of the 2nd Workshop on Machine Reading for Question Answering",
    year = "2019",
}
@article{schaeffer2023emergent,
  title={Are emergent abilities of Large Language Models a mirage?},
  author={Schaeffer, Rylan and Miranda, Brando and Koyejo, Sanmi},
  journal={arXiv preprint arXiv:2304.15004},
  year={2023}
}

@article{datasheet,
  title={Datasheets for datasets},
  author={Gebru, Timnit and Morgenstern, Jamie and Vecchione, Briana and Vaughan, Jennifer Wortman and Wallach, Hanna and Daum{\'e} III, Hal and Crawford, Kate},
  journal={arXiv preprint arXiv:1803.09010},
  year={2018}
}

@article{wei2022emergent,
  title={Emergent abilities of large language models},
  author={Wei, Jason and Tay, Yi and Bommasani, Rishi and Raffel, Colin and Zoph, Barret and Borgeaud, Sebastian and Yogatama, Dani and Bosma, Maarten and Zhou, Denny and Metzler, Donald and others},
  journal={arXiv preprint arXiv:2206.07682},
  year={2022}
}

@article{maus2023adversarial,
  title={Adversarial prompting for black box foundation models},
  author={Maus, Natalie and Chao, Patrick and Wong, Eric and Gardner, Jacob},
  journal={arXiv preprint arXiv:2302.04237},
  year={2023}
}

@inproceedings{lee2022deduplicating,
  title={Deduplicating Training Data Makes Language Models Better},
  author={Lee, Katherine and Ippolito, Daphne and Nystrom, Andrew and Zhang, Chiyuan and Eck, Douglas and Callison-Burch, Chris and Carlini, Nicholas},
  booktitle={ACL},
  year={2022}
}

@inproceedings{zhao2022provably,
  title={Provably Confidential Language Modelling},
  author={Zhao, Xuandong and Li, Lei and Wang, Yu-Xiang},
  booktitle={Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies},
  pages={943--955},
  year={2022}
}


@article{zhang2021counterfactual,
  title={Counterfactual memorization in neural language models},
  author={Zhang, Chiyuan and Ippolito, Daphne and Lee, Katherine and Jagielski, Matthew and Tram{\`e}r, Florian and Carlini, Nicholas},
  journal={arXiv preprint arXiv:2112.12938},
  year={2021}
}

@inproceedings{shi-etal-2022-just,
    title = "Just Fine-tune Twice: Selective Differential Privacy for Large Language Models",
    author = "Shi, Weiyan  and
      Shea, Ryan  and
      Chen, Si  and
      Zhang, Chiyuan  and
      Jia, Ruoxi  and
      Yu, Zhou",
    booktitle = "EMNLP",
    year = "2022",
    address = "Abu Dhabi, United Arab Emirates",
}




@inproceedings{mattern2022differentially,
    title = "Differentially Private Language Models for Secure Data Sharing",
    author = "Mattern, Justus  and
      Jin, Zhijing  and
      Weggenmann, Benjamin  and
      Schoelkopf, Bernhard  and
      Sachan, Mrinmaya",
    booktitle = "Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing",
    month = dec,
    year = "2022",
    address = "Abu Dhabi, United Arab Emirates",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.emnlp-main.323",
    pages = "4860--4873",
    abstract = "To protect the privacy of individuals whose data is being shared, it is of high importance to develop methods allowing researchers and companies to release textual data while providing formal privacy guarantees to its originators. In the field of NLP, substantial efforts have been directed at building mechanisms following the framework of local differential privacy, thereby anonymizing individual text samples before releasing them. In practice, these approaches are often dissatisfying in terms of the quality of their output language due to the strong noise required for local differential privacy. In this paper, we approach the problem at hand using global differential privacy, particularly by training a generative language model in a differentially private manner and consequently sampling data from it. Using natural language prompts and a new prompt-mismatch loss, we are able to create highly accurate and fluent textual datasets taking on specific desired attributes such as sentiment or topic and resembling statistical properties of the training data. We perform thorough experiments indicating that our synthetic datasets do not leak information from our original data and are of high language quality and highly suitable for training models for further analysis on real-world data. Notably, we also demonstrate that training classifiers on private synthetic data outperforms directly training classifiers with DP-SGD.",
}

@article{panda2023differentially,
  title={Differentially Private In-Context Learning},
  author={Panda, Ashwinee and Wu, Tong and Wang, Jiachen T and Mittal, Prateek},
  journal={arXiv preprint arXiv:2305.01639},
  year={2023}
}


@article{duan2023flocks,
  title={Flocks of Stochastic Parrots: Differentially Private Prompt Learning for Large Language Models},
  author={Duan, Haonan and Dziedzic, Adam and Papernot, Nicolas and Boenisch, Franziska},
  journal={arXiv preprint arXiv:2305.15594},
  year={2023}
}


@article{shao2023quantifying,
  title={Quantifying Association Capabilities of Large Language Models and Its Implications on Privacy Leakage},
  author={Shao, Hanyin and Huang, Jie and Zheng, Shen and Chang, Kevin Chen-Chuan},
  journal={arXiv preprint arXiv:2305.12707},
  year={2023}
}

@article{yue2022synthetic,
  title={Synthetic text generation with differential privacy: A simple and practical recipe},
  author={Yue, Xiang and Inan, Huseyin A and Li, Xuechen and Kumar, Girish and McAnallen, Julia and Sun, Huan and Levitan, David and Sim, Robert},
  journal={ACL},
  year={2023}
}

@article{lukas2023analyzing,
  title={Analyzing leakage of personally identifiable information in language models},
  author={Lukas, Nils and Salem, Ahmed and Sim, Robert and Tople, Shruti and Wutschitz, Lukas and Zanella-B{\'e}guelin, Santiago},
  journal={arXiv preprint arXiv:2302.00539},
  year={2023}
}

@article{li2023multi,
  title={Multi-step jailbreaking privacy attacks on ChatGPT},
  author={Li, Haoran and Guo, Dadi and Fan, Wei and Xu, Mingshi and Song, Yangqiu},
  journal={arXiv preprint arXiv:2304.05197},
  year={2023}
}


@article{dwork2014algorithmic,
  title={The algorithmic foundations of differential privacy},
  author={Dwork, Cynthia and Roth, Aaron and others},
  journal={Foundations and Trends{\textregistered} in Theoretical Computer Science},
  volume={9},
  number={3--4},
  pages={211--407},
  year={2014},
  publisher={Now Publishers, Inc.}
}

@inproceedings{abadi2016deep,
  title={Deep learning with differential privacy},
  author={Abadi, Martin and Chu, Andy and Goodfellow, Ian and McMahan, H Brendan and Mironov, Ilya and Talwar, Kunal and Zhang, Li},
  booktitle={Proceedings of the 2016 ACM SIGSAC conference on computer and communications security},
  pages={308--318},
  year={2016}
}


@article{li2021large,
  title={Large language models can be strong differentially private learners},
  author={Li, Xuechen and Tramer, Florian and Liang, Percy and Hashimoto, Tatsunori},
  journal={arXiv preprint arXiv:2110.05679},
  year={2021}
}



@article{liang2022holistic,
  title={Holistic evaluation of language models},
  author={Liang, Percy and Bommasani, Rishi and Lee, Tony and Tsipras, Dimitris and Soylu, Dilara and Yasunaga, Michihiro and Zhang, Yian and Narayanan, Deepak and Wu, Yuhuai and Kumar, Ananya and others},
  journal={arXiv preprint arXiv:2211.09110},
  year={2022}
}
@inproceedings{
chen2021a,
title={A Dataset for Answering Time-Sensitive Questions},
author={Wenhu Chen and Xinyi Wang and William Yang Wang},
booktitle={Thirty-fifth Conference on Neural Information Processing Systems Datasets and Benchmarks Track (Round 2)},
year={2021},
url={https://openreview.net/forum?id=9-LSfSU74n-}
}
@article{wang2023robustness,
  title={On the Robustness of ChatGPT: An Adversarial and Out-of-distribution Perspective},
  author={Wang, Jindong and Hu, Xixu and Hou, Wenxin and Chen, Hao and Zheng, Runkai and Wang, Yidong and Yang, Linyi and Huang, Haojun and Ye, Wei and Geng, Xiubo and others},
  journal={arXiv preprint arXiv:2302.12095},
  year={2023}
}
@inproceedings{
si2023prompting,
title={Prompting {GPT}-3 To Be Reliable},
author={Chenglei Si and Zhe Gan and Zhengyuan Yang and Shuohang Wang and Jianfeng Wang and Jordan Lee Boyd-Graber and Lijuan Wang},
booktitle={The Eleventh International Conference on Learning Representations },
year={2023},
url={https://openreview.net/forum?id=98p5x51L5af}
}
@inproceedings{krishna-etal-2020-reformulating,
    title = "Reformulating Unsupervised Style Transfer as Paraphrase Generation",
    author = "Krishna, Kalpesh  and
      Wieting, John  and
      Iyyer, Mohit",
    booktitle = "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)",
    month = nov,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.emnlp-main.55",
    doi = "10.18653/v1/2020.emnlp-main.55",
    pages = "737--762",
    abstract = "Modern NLP defines the task of style transfer as modifying the style of a given sentence without appreciably changing its semantics, which implies that the outputs of style transfer systems should be paraphrases of their inputs. However, many existing systems purportedly designed for style transfer inherently warp the input{'}s meaning through attribute transfer, which changes semantic properties such as sentiment. In this paper, we reformulate unsupervised style transfer as a paraphrase generation problem, and present a simple methodology based on fine-tuning pretrained language models on automatically generated paraphrase data. Despite its simplicity, our method significantly outperforms state-of-the-art style transfer systems on both human and automatic evaluations. We also survey 23 style transfer papers and discover that existing automatic metrics can be easily gamed and propose fixed variants. Finally, we pivot to a more real-world style transfer setting by collecting a large dataset of 15M sentences in 11 diverse styles, which we use for an in-depth analysis of our system.",
}
@ARTICLE{2014arXiv1412.6572G,
       author = {{Goodfellow}, Ian J. and {Shlens}, Jonathon and {Szegedy}, Christian},
        title = "{Explaining and Harnessing Adversarial Examples}",
      journal = {arXiv e-prints},
     keywords = {Statistics - Machine Learning, Computer Science - Machine Learning},
         year = 2014,
        month = Dec,
          eid = {arXiv:1412.6572},
        pages = {arXiv:1412.6572},
archivePrefix = {arXiv},
       eprint = {1412.6572},
 primaryClass = {stat.ML},
       adsurl = {https://ui.adsabs.harvard.edu/#abs/2014arXiv1412.6572G},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{2016arXiv160804644C,
       author = {{Carlini}, Nicholas and {Wagner}, David},
        title = "{Towards Evaluating the Robustness of Neural Networks}",
      journal = {arXiv e-prints},
     keywords = {Computer Science - Cryptography and Security, Computer Science -
        Computer Vision and Pattern Recognition},
         year = 2016,
        month = Aug,
          eid = {arXiv:1608.04644},
        pages = {arXiv:1608.04644},
archivePrefix = {arXiv},
       eprint = {1608.04644},
 primaryClass = {cs.CR},
       adsurl = {https://ui.adsabs.harvard.edu/#abs/2016arXiv160804644C},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{2018arXiv180301128C,
       author = {{Cheng}, Minhao and {Yi}, Jinfeng and {Zhang}, Huan and {Chen}, Pin-Yu
        and {Hsieh}, Cho-Jui},
        title = "{Seq2Sick: Evaluating the Robustness of Sequence-to-Sequence Models with
        Adversarial Examples}",
      journal = {arXiv e-prints},
     keywords = {Computer Science - Machine Learning},
         year = 2018,
        month = Mar,
          eid = {arXiv:1803.01128},
        pages = {arXiv:1803.01128},
archivePrefix = {arXiv},
       eprint = {1803.01128},
 primaryClass = {cs.LG},
       adsurl = {https://ui.adsabs.harvard.edu/#abs/2018arXiv180301128C},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{2016arXiv161101603S,
       author = {{Seo}, Minjoon and {Kembhavi}, Aniruddha and {Farhadi}, Ali and
        {Hajishirzi}, Hannaneh},
        title = "{Bidirectional Attention Flow for Machine Comprehension}",
      journal = {arXiv e-prints},
     keywords = {Computer Science - Computation and Language},
         year = 2016,
        month = Nov,
          eid = {arXiv:1611.01603},
        pages = {arXiv:1611.01603},
archivePrefix = {arXiv},
       eprint = {1611.01603},
 primaryClass = {cs.CL},
       adsurl = {https://ui.adsabs.harvard.edu/#abs/2016arXiv161101603S},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{2018arXiv180901478M,
       author = {{Meng}, Yu and {Shen}, Jiaming and {Zhang}, Chao and {Han}, Jiawei},
        title = "{Weakly-Supervised Neural Text Classification}",
      journal = {arXiv e-prints},
     keywords = {Computer Science - Information Retrieval, Computer Science - Computation
        and Language, Computer Science - Machine Learning, Statistics -
        Machine Learning},
         year = 2018,
        month = Sep,
          eid = {arXiv:1809.01478},
        pages = {arXiv:1809.01478},
archivePrefix = {arXiv},
       eprint = {1809.01478},
 primaryClass = {cs.IR},
       adsurl = {https://ui.adsabs.harvard.edu/#abs/2018arXiv180901478M},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@inproceedings{Cohen:2005:FSB:1642293.1642400,
 author = {Cohen, Shay and Ruppin, Eytan and Dror, Gideon},
 title = {Feature Selection Based on the Shapley Value},
 booktitle = {Proceedings of the 19th International Joint Conference on Artificial Intelligence},
 series = {IJCAI'05},
 year = {2005},
 location = {Edinburgh, Scotland},
 pages = {665--670},
 numpages = {6},
 url = {http://dl.acm.org/citation.cfm?id=1642293.1642400},
 acmid = {1642400},
 publisher = {Morgan Kaufmann Publishers Inc.},
 address = {San Francisco, CA, USA},
} 



@article{nfc512,
  title={A Structured Self-Attentive Sentence Embedding},
  author={Zhouhan Lin and Minwei Feng and C{\'i}cero Nogueira dos Santos and Mo Yu and Bing Xiang and Bowen Zhou and Yoshua Bengio},
  journal={arXiv},
  year={2017},
  volume={abs/1703.03130}
}

@inproceedings{t3,
  title={T3: Tree-Autoencoder Constrained Adversarial Text Generation for Targeted Attack},
  author={Boxin Wang and Hengzhi Pei and Boyuan Pan and Qian Chen and Shuohang Wang and Bo Li},
  year={2020},
  booktitle={EMNLP}
}

@inproceedings{advstyle,
  title={AdvStyle: Adversarial Style Transfer for Adversarial Text Generation},
  author={Boxin Wang and Nora Hollenstein and Jiahao Yu and Qian Chen and Yu Cheng and Jingjing Liu and Ce Zhang and Bo Li},
  booktitle={Preprint}
}

@inproceedings{gpate,
  title={Scalable Differentially Private Data Generator via Private Aggregation of Teacher Discriminators},
  author={Yunhui Long* and Boxin Wang* and Zhuolin Yang and Kaizhao Liang and Shuang Yang and Bhavya Kailkhura and Carl Gunter and Bo Li},
  booktitle={Preprint}
}

@inproceedings{datalens,
  title={DataLens: Scalable Privacy Preserving Generative Model via Gradient Compression and Teacher Aggregation},
  author={Boxin Wang and Yunhui Long and Luka Rimanic and Qian Chen and Ce Zhang and Bo Li},
  booktitle={Preprint}
}


@article{advfever,
  author    = {James Thorne and
               Andreas Vlachos},
  title     = {Adversarial attacks against Fact Extraction and VERification},
  journal   = {CoRR},
  volume    = {abs/1903.05543},
  year      = {2019},
  archivePrefix = {arXiv},
  eprint    = {1903.05543}
}




@misc{yelpdataset,
  author = {Yelp Dataset Challenge},
  note = {data retrieved from Yelp Dataset Challenge, 
          \url{https://www.yelp.com/dataset/challenge}},
}

@inproceedings{dialogued,
  author    = {Saizheng Zhang and
               Emily Dinan and
               Jack Urbanek and
               Arthur Szlam and
               Douwe Kiela and
               Jason Weston},
  title     = {Personalizing Dialogue Agents: {I} have a dog, do you have pets too?},
  booktitle = {ACL},
  year      = {2018}
}



@inproceedings{DBLP:conf/nips/VaswaniSPUJGKP17,
  author    = {Ashish Vaswani and
               Noam Shazeer and
               Niki Parmar and
               Jakob Uszkoreit and
               Llion Jones and
               Aidan N. Gomez and
               Lukasz Kaiser and
               Illia Polosukhin},
  title     = {Attention is All you Need},
  booktitle = {NeurIPS},
  pages     = {5998--6008},
  year      = {2017}
}

@article{worstcase,
  author    = {Sicheng Zhu and
               Xiao Zhang and
               David Evans},
  title     = {Learning Adversarially Robust Representations via Worst-Case Mutual
               Information Maximization},
  journal   = {CoRR},
  volume    = {abs/2002.11798},
  year      = {2020}
}

@article{DBLP:journals/computer/Linsker88,
  author    = {Ralph Linsker},
  title     = {Self-Organization in a Perceptual Network},
  journal   = {Computer},
  volume    = {21},
  number    = {3},
  pages     = {105--117},
  year      = {1988}
}

@article{infomin,
  author    = {Yonglong Tian and
               Chen Sun and
               Ben Poole and
               Dilip Krishnan and
               Cordelia Schmid and
               Phillip Isola},
  title     = {What makes for good views for contrastive learning},
  journal   = {CoRR},
  volume    = {abs/2005.10243},
  year      = {2020}
}

@inproceedings{DBLP:conf/icml/SaunshiPAKK19,
  author    = {Nikunj Saunshi and
               Orestis Plevrakis and
               Sanjeev Arora and
               Mikhail Khodak and
               Hrishikesh Khandeparkar},
  title     = {A Theoretical Analysis of Contrastive Unsupervised Representation
               Learning},
  booktitle = {ICML},
  year      = {2019}
}

@inproceedings{DBLP:conf/iclr/TschannenDRGL20,
  author    = {Michael Tschannen and
               Josip Djolonga and
               Paul K. Rubenstein and
               Sylvain Gelly and
               Mario Lucic},
  title     = {On Mutual Information Maximization for Representation Learning},
  booktitle = {ICLR},
  year      = {2020}
}

@inproceedings{DBLP:conf/icml/CohenRK19,
  author    = {Jeremy M. Cohen and
               Elan Rosenfeld and
               J. Zico Kolter},
  title     = {Certified Adversarial Robustness via Randomized Smoothing},
  booktitle = {ICML},
  year      = {2019}
}

@inproceedings{ibp1,
  author    = {Po{-}Sen Huang and
               Robert Stanforth and
               Johannes Welbl and
               Chris Dyer and
               Dani Yogatama and
               Sven Gowal and
               Krishnamurthy Dvijotham and
               Pushmeet Kohli},
  title     = {Achieving Verified Robustness to Symbol Substitutions via Interval
               Bound Propagation},
  booktitle = {EMNLP-IJCNLP},
  year      = {2019}
}

@inproceedings{ibp2,
  author    = {Robin Jia and
               Aditi Raghunathan and
               Kerem G{\"{o}}ksel and
               Percy Liang},
  title     = {Certified Robustness to Adversarial Word Substitutions},
  booktitle = {EMNLP-IJCNLP},
  year      = {2019}
}

@article{ibp,
  author    = {Krishnamurthy Dvijotham and
               Sven Gowal and
               Robert Stanforth and
               Relja Arandjelovic and
               Brendan O'Donoghue and
               Jonathan Uesato and
               Pushmeet Kohli},
  title     = {Training verified learners with learned verifiers},
  journal   = {CoRR},
  volume    = {abs/1805.10265},
  year      = {2018}
}

@inproceedings{safer,
  author    = {Mao Ye and
               Chengyue Gong and
               Qiang Liu},
  title     = {{SAFER:} {A} Structure-free Approach for Certified Robustness to Adversarial
               Word Substitutions},
  booktitle = {ACL},
  year      = {2020}
}

@inproceedings{DBLP:conf/naacl/ZhangBH19,
  author    = {Yuan Zhang and
               Jason Baldridge and
               Luheng He},
  title     = {{PAWS:} Paraphrase Adversaries from Word Scrambling},
  booktitle = {NAACL-HLT},
  year      = {2019}
}

@inproceedings{weight-attack,
  author    = {Shuhuai Ren and
               Yihe Deng and
               Kun He and
               Wanxiang Che},
  title     = {Generating Natural Language Adversarial Examples through Probability
               Weighted Word Saliency},
  booktitle = {ACL},
  year      = {2019}
}

@inproceedings{DBLP:conf/emnlp/AlzantotSEHSC18,
  author    = {Moustafa Alzantot and
               Yash Sharma and
               Ahmed Elgohary and
               Bo{-}Jhang Ho and
               Mani B. Srivastava and
               Kai{-}Wei Chang},
  title     = {Generating Natural Language Adversarial Examples},
  booktitle = {EMNLP},
  year      = {2018}
}

@inproceedings{scpn,
  author    = {Mohit Iyyer and
               John Wieting and
               Kevin Gimpel and
               Luke Zettlemoyer},
  title     = {Adversarial Example Generation with Syntactically Controlled Paraphrase
               Networks},
  booktitle = {NAACL-HLT},
  year      = {2018}
}

@inproceedings{hotflip,
  author    = {Javid Ebrahimi and
               Anyi Rao and
               Daniel Lowd and
               Dejing Dou},
  title     = {HotFlip: White-Box Adversarial Examples for Text Classification},
  booktitle = {ACL},
  year      = {2018}
}

@article{yang2022glue,
  title={GLUE-X: Evaluating Natural Language Understanding Models from an Out-of-distribution Generalization Perspective},
  author={Yang, Linyi and Zhang, Shuibai and Qin, Libo and Li, Yafu and Wang, Yidong and Liu, Hanmeng and Wang, Jindong and Xie, Xing and Zhang, Yue},
  journal={arXiv preprint arXiv:2211.08073},
  year={2022}
}

@article{weber2022certifying,
  title     = {Certifying Out-of-Domain Generalization for Blackbox Functions},
  author    = {Maurice Weber and Linyi Li and Boxin Wang and Zhikuan Zhao and Bo Li and Ce Zhang},
  journal   = {International Conference on Machine Learning},
  year      = {2022},
  bibSource = {Semantic Scholar https://www.semanticscholar.org/paper/663d2e54d8b20fd30282ab40521961fb4438805f}
}

@article{yang2022improving,   title   = {Improving certified robustness via statistical learning with logical reasoning},   author  = {Yang, Zhuolin and Zhao, Zhikuan and Wang, Boxin and Zhang, Jiawei and Li, Linyi and Pei, Hengzhi and Karla{\v{s}}, Bojan and Liu, Ji and Guo, Heng and Zhang, Ce and others},   journal = {Advances in Neural Information Processing Systems},   volume  = {35},   pages   = {34859-34873},   year    = {2022} }

@article{yuan2023revisiting,
  title={Revisiting Out-of-distribution Robustness in NLP: Benchmark, Analysis, and LLMs Evaluations},
  author={Yuan, Lifan and Chen, Yangyi and Cui, Ganqu and Gao, Hongcheng and Zou, Fangyuan and Cheng, Xingyi and Ji, Heng and Liu, Zhiyuan and Sun, Maosong},
  journal={arXiv preprint arXiv:2306.04618},
  year={2023}
}
@inproceedings{hendrycks-etal-2020-pretrained,
    title = "Pretrained Transformers Improve Out-of-Distribution Robustness",
    author = "Hendrycks, Dan  and
      Liu, Xiaoyuan  and
      Wallace, Eric  and
      Dziedzic, Adam  and
      Krishnan, Rishabh  and
      Song, Dawn",
    booktitle = "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics",
    month = jul,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.acl-main.244",
    doi = "10.18653/v1/2020.acl-main.244",
    pages = "2744--2751",
    abstract = "Although pretrained Transformers such as BERT achieve high accuracy on in-distribution examples, do they generalize to new distributions? We systematically measure out-of-distribution (OOD) generalization for seven NLP datasets by constructing a new robustness benchmark with realistic distribution shifts. We measure the generalization of previous models including bag-of-words models, ConvNets, and LSTMs, and we show that pretrained Transformers{'} performance declines are substantially smaller. Pretrained transformers are also more effective at detecting anomalous or OOD examples, while many previous models are frequently worse than chance. We examine which factors affect robustness, finding that larger models are not necessarily more robust, distillation can be harmful, and more diverse pretraining data can enhance robustness. Finally, we show where future work can improve OOD robustness.",
}
@article{agarwal2022temporal,
  title={Temporal effects on pre-trained models for language processing tasks},
  author={Agarwal, Oshin and Nenkova, Ani},
  journal={Transactions of the Association for Computational Linguistics},
  volume={10},
  pages={904--921},
  year={2022},
  publisher={MIT Press}
}
@inproceedings{wang-etal-2021-textflint,
    title = "{T}ext{F}lint: Unified Multilingual Robustness Evaluation Toolkit for Natural Language Processing",
    author = "Wang, Xiao  and
      Liu, Qin  and
      Gui, Tao  and
      Zhang, Qi  and
      Zou, Yicheng  and
      Zhou, Xin  and
      Ye, Jiacheng  and
      Zhang, Yongxin  and
      Zheng, Rui  and
      Pang, Zexiong  and
      Wu, Qinzhuo  and
      Li, Zhengyan  and
      Zhang, Chong  and
      Ma, Ruotian  and
      Fei, Zichu  and
      Cai, Ruijian  and
      Zhao, Jun  and
      Hu, Xingwu  and
      Yan, Zhiheng  and
      Tan, Yiding  and
      Hu, Yuan  and
      Bian, Qiyuan  and
      Liu, Zhihua  and
      Qin, Shan  and
      Zhu, Bolin  and
      Xing, Xiaoyu  and
      Fu, Jinlan  and
      Zhang, Yue  and
      Peng, Minlong  and
      Zheng, Xiaoqing  and
      Zhou, Yaqian  and
      Wei, Zhongyu  and
      Qiu, Xipeng  and
      Huang, Xuanjing",
    booktitle = "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing: System Demonstrations",
    month = aug,
    year = "2021",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.acl-demo.41",
    doi = "10.18653/v1/2021.acl-demo.41",
    pages = "347--355",
    abstract = "TextFlint is a multilingual robustness evaluation toolkit for NLP tasks that incorporates universal text transformation, task-specific transformation, adversarial attack, subpopulation, and their combinations to provide comprehensive robustness analyses. This enables practitioners to automatically evaluate their models from various aspects or to customize their evaluations as desired with just a few lines of code. TextFlint also generates complete analytical reports as well as targeted augmented data to address the shortcomings of the model in terms of its robustness. To guarantee acceptability, all the text transformations are linguistically based and all the transformed data selected (up to 100,000 texts) scored highly under human evaluation. To validate the utility, we performed large-scale empirical evaluations (over 67,000) on state-of-the-art deep learning models, classic supervised methods, and real-world systems. The toolkit is already available at https://github.com/textflint with all the evaluation results demonstrated at textflint.io.",
}
@article{Goodfellow2015ExplainingAH,
  title={Explaining and Harnessing Adversarial Examples},
  author={Ian J. Goodfellow and Jonathon Shlens and Christian Szegedy},
  journal={CoRR},
  year={2015},
  volume={abs/1412.6572}
}

@article{MoosaviDezfooli2016DeepFoolAS,
  title={DeepFool: A Simple and Accurate Method to Fool Deep Neural Networks},
  author={Seyed-Mohsen Moosavi-Dezfooli and Alhussein Fawzi and Pascal Frossard},
  journal={CVPR},
  year={2016},
  pages={2574-2582}
}

@inproceedings{Eykholt2017RobustPA,
  title={Robust Physical-World Attacks on Deep Learning Models.},
  author={Kevin Eykholt and Ivan Evtimov and Earlence Fernandes and Bo Li and Amir Rahmati and Chaowei Xiao and Atul Prakash and Tadayoshi Kohno and Dawn Xiaodong Song},
  year={2017}
}
@inproceedings{arora-etal-2021-types,
    title = "Types of Out-of-Distribution Texts and How to Detect Them",
    author = "Arora, Udit  and
      Huang, William  and
      He, He",
    booktitle = "Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing",
    month = nov,
    year = "2021",
    address = "Online and Punta Cana, Dominican Republic",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.emnlp-main.835",
    doi = "10.18653/v1/2021.emnlp-main.835",
    pages = "10687--10701",
    abstract = "Despite agreement on the importance of detecting out-of-distribution (OOD) examples, there is little consensus on the formal definition of the distribution shifts of OOD examples and how to best detect them. We categorize these examples as exhibiting a background shift or semantic shift, and find that the two major approaches to OOD detection, calibration and density estimation (language modeling for text), have distinct behavior on these types of OOD data. Across 14 pairs of in-distribution and OOD English natural language understanding datasets, we find that density estimation methods consistently beat calibration methods in background shift settings and perform worse in semantic shift settings. In addition, we find that both methods generally fail to detect examples from challenge data, indicating that these examples constitute a different type of OOD data. Overall, while the categorization we apply explains many of the differences between the two methods, our results call for a more explicit definition of OOD to create better benchmarks and build detectors that can target the type of OOD data expected at test time.",
}
@misc{dhole2021nlaugmenter,
      title={NL-Augmenter: A Framework for Task-Sensitive Natural Language Augmentation}, 
      author={Kaustubh D. Dhole and Varun Gangal and Sebastian Gehrmann and Aadesh Gupta and Zhenhao Li and Saad Mahamood and Abinaya Mahendiran and Simon Mille and Ashish Srivastava and Samson Tan and Tongshuang Wu and Jascha Sohl-Dickstein and Jinho D. Choi and Eduard Hovy and Ondrej Dusek and Sebastian Ruder and Sajant Anand and Nagender Aneja and Rabin Banjade and Lisa Barthe and Hanna Behnke and Ian Berlot-Attwell and Connor Boyle and Caroline Brun and Marco Antonio Sobrevilla Cabezudo and Samuel Cahyawijaya and Emile Chapuis and Wanxiang Che and Mukund Choudhary and Christian Clauss and Pierre Colombo and Filip Cornell and Gautier Dagan and Mayukh Das and Tanay Dixit and Thomas Dopierre and Paul-Alexis Dray and Suchitra Dubey and Tatiana Ekeinhor and Marco Di Giovanni and Rishabh Gupta and Rishabh Gupta and Louanes Hamla and Sang Han and Fabrice Harel-Canada and Antoine Honore and Ishan Jindal and Przemyslaw K. Joniak and Denis Kleyko and Venelin Kovatchev and Kalpesh Krishna and Ashutosh Kumar and Stefan Langer and Seungjae Ryan Lee and Corey James Levinson and Hualou Liang and Kaizhao Liang and Zhexiong Liu and Andrey Lukyanenko and Vukosi Marivate and Gerard de Melo and Simon Meoni and Maxime Meyer and Afnan Mir and Nafise Sadat Moosavi and Niklas Muennighoff and Timothy Sum Hon Mun and Kenton Murray and Marcin Namysl and Maria Obedkova and Priti Oli and Nivranshu Pasricha and Jan Pfister and Richard Plant and Vinay Prabhu and Vasile Pais and Libo Qin and Shahab Raji and Pawan Kumar Rajpoot and Vikas Raunak and Roy Rinberg and Nicolas Roberts and Juan Diego Rodriguez and Claude Roux and Vasconcellos P. H. S. and Ananya B. Sai and Robin M. Schmidt and Thomas Scialom and Tshephisho Sefara and Saqib N. Shamsi and Xudong Shen and Haoyue Shi and Yiwen Shi and Anna Shvets and Nick Siegel and Damien Sileo and Jamie Simon and Chandan Singh and Roman Sitelew and Priyank Soni and Taylor Sorensen and William Soto and Aman Srivastava and KV Aditya Srivatsa and Tony Sun and Mukund Varma T and A Tabassum and Fiona Anting Tan and Ryan Teehan and Mo Tiwari and Marie Tolkiehn and Athena Wang and Zijian Wang and Gloria Wang and Zijie J. Wang and Fuxuan Wei and Bryan Wilie and Genta Indra Winata and Xinyi Wu and Witold Wydmański and Tianbao Xie and Usama Yaseen and M. Yee and Jing Zhang and Yue Zhang},
      year={2021},
      eprint={2112.02721},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}
@inproceedings{
hendrycks2021measuring,
title={Measuring Massive Multitask Language Understanding},
author={Dan Hendrycks and Collin Burns and Steven Basart and Andy Zou and Mantas Mazeika and Dawn Song and Jacob Steinhardt},
booktitle={International Conference on Learning Representations},
year={2021},
url={https://openreview.net/forum?id=d7KBjmI3GmQ}
}
@article{Papernot2016DistillationAA,
  title={Distillation as a Defense to Adversarial Perturbations Against Deep Neural Networks},
  author={Nicolas Papernot and Patrick D. McDaniel and Xi Wu and Somesh Jha and Ananthram Swami},
  journal={2016 IEEE Symposium on Security and Privacy (SP)},
  year={2016},
  pages={582-597}
}


@article{simclr,
  author    = {Ting Chen and
               Simon Kornblith and
               Mohammad Norouzi and
               Geoffrey E. Hinton},
  title     = {A Simple Framework for Contrastive Learning of Visual Representations},
  journal   = {CoRR},
  volume    = {abs/2002.05709},
  year      = {2020}
}

@article{roberta,
  author    = {Yinhan Liu and
               Myle Ott and
               Naman Goyal and
               Jingfei Du and
               Mandar Joshi and
               Danqi Chen and
               Omer Levy and
               Mike Lewis and
               Luke Zettlemoyer and
               Veselin Stoyanov},
  title     = {RoBERTa: {A} Robustly Optimized {BERT} Pretraining Approach},
  journal   = {CoRR},
  volume    = {abs/1907.11692},
  year      = {2019}
}

@inproceedings{DBLP:conf/uss/Carlini0EKS19,
  author    = {Nicholas Carlini and
               Chang Liu and
               {\'{U}}lfar Erlingsson and
               Jernej Kos and
               Dawn Song},
  title     = {The Secret Sharer: Evaluating and Testing Unintended Memorization
               in Neural Networks},
  booktitle = {28th {USENIX} Security Symposium, {USENIX} Security 2019},
  year      = {2019}
}

@inproceedings{mireshghallah2022empirical,
  title={An empirical analysis of memorization in fine-tuned autoregressive language models},
  author={Mireshghallah, Fatemehsadat and Uniyal, Archit and Wang, Tianhao and Evans, David K and Berg-Kirkpatrick, Taylor},
  booktitle={Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing},
  pages={1816--1826},
  year={2022}
}

@inproceedings{DBLP:conf/sp/ShokriSSS17,
  author    = {Reza Shokri and
               Marco Stronati and
               Congzheng Song and
               Vitaly Shmatikov},
  title     = {Membership Inference Attacks Against Machine Learning Models},
  booktitle = {2017 {IEEE} Symposium on Security and Privacy, {SP} 2017},
  year      = {2017},
}

@inproceedings{DBLP:conf/ccs/HitajAP17,
  author    = {Briland Hitaj and
               Giuseppe Ateniese and
               Fernando P{\'{e}}rez{-}Cruz},
  title     = {Deep Models Under the {GAN:} Information Leakage from Collaborative
               Deep Learning},
  booktitle = {{ACM} {SIGSAC}},
  year      = {2017},
}

@inproceedings{secretrevealer,
  author    = {Yuheng Zhang and
               Ruoxi Jia and
               Hengzhi Pei and
               Wenxiao Wang and
               Bo Li and
               Dawn Song},
  title     = {The Secret Revealer: Generative Model-Inversion Attacks Against Deep
               Neural Networks},
  booktitle = {{CVPR} },
  year      = {2020},
}


@inproceedings{DBLP:conf/ijcai/LiuDRSH19,
  author    = {Lydia T. Liu and
               Sarah Dean and
               Esther Rolf and
               Max Simchowitz and
               Moritz Hardt},
  title     = {Delayed Impact of Fair Machine Learning},
  booktitle = {{IJCAI}},

  year      = {2019},
}

@inproceedings{DBLP:conf/icml/HashimotoSNL18,
  author    = {Tatsunori B. Hashimoto and
               Megha Srivastava and
               Hongseok Namkoong and
               Percy Liang},
  title     = {Fairness Without Demographics in Repeated Loss Minimization},
  booktitle = {
               {ICML} },
  year      = {2018},
}

@article{DBLP:journals/corr/abs-1710-03184,
  author    = {Pratik Gajane},
  title     = {On formalizing fairness in prediction with machine learning},
  journal   = {CoRR},
  volume    = {abs/1710.03184},
  year      = {2017},
  archivePrefix = {arXiv},
  eprint    = {1710.03184},
}

@inproceedings{DBLP:conf/nips/KilbertusRPHJS17,
  author    = {Niki Kilbertus and
               Mateo Rojas{-}Carulla and
               Giambattista Parascandolo and
               Moritz Hardt and
               Dominik Janzing and
               Bernhard Sch{\"{o}}lkopf},
  editor    = {Isabelle Guyon and
               Ulrike von Luxburg and
               Samy Bengio and
               Hanna M. Wallach and
               Rob Fergus and
               S. V. N. Vishwanathan and
               Roman Garnett},
  title     = {Avoiding Discrimination through Causal Reasoning},
  booktitle = {NeurIPS},
  year      = {2017},
}

@inproceedings{dpdl,
  author    = {Mart{\'{\i}}n Abadi and
               Andy Chu and
               Ian J. Goodfellow and
               H. Brendan McMahan and
               Ilya Mironov and
               Kunal Talwar and
               Li Zhang},
  editor    = {Edgar R. Weippl and
               Stefan Katzenbeisser and
               Christopher Kruegel and
               Andrew C. Myers and
               Shai Halevi},
  title     = {Deep Learning with Differential Privacy},
  booktitle = {{ACM} {SIGSAC} },
  year      = {2016},
}

@inproceedings{pate,
  author    = {Nicolas Papernot and
               Mart{\'{\i}}n Abadi and
               {\'{U}}lfar Erlingsson and
               Ian J. Goodfellow and
               Kunal Talwar},
  title     = {Semi-supervised Knowledge Transfer for Deep Learning from Private
               Training Data},
  booktitle = { {ICLR} },
  year      = {2017},
}

@inproceedings{spate,
  author    = {Nicolas Papernot and
               Shuang Song and
               Ilya Mironov and
               Ananth Raghunathan and
               Kunal Talwar and
               {\'{U}}lfar Erlingsson},
  title     = {Scalable Private Learning with {PATE}},
  booktitle = {{ICLR}},
  year      = {2018},
}

@inproceedings{pategan,
  author    = {James Jordon and
               Jinsung Yoon and
               Mihaela van der Schaar},
  title     = {{PATE-GAN:} Generating Synthetic Data with Differential Privacy Guarantees},
  booktitle = { {ICLR}},
  year      = {2019},
}


@inproceedings{Yang2019XLNetGA,
  title={XLNet: Generalized Autoregressive Pretraining for Language Understanding},
  author={Zhilin Yang and Zihang Dai and Yiming Yang and Jaime G. Carbonell and Ruslan Salakhutdinov and Quoc V. Le},
  booktitle={NeurIPS},
  year={2019}
}

@article{Lan2019ALBERTAL,
  title={ALBERT: A Lite BERT for Self-supervised Learning of Language Representations},
  author={Zhen-Zhong Lan and Mingda Chen and Sebastian Goodman and Kevin Gimpel and Piyush Sharma and Radu Soricut},
  journal={arXiv},
  year={2019},
  volume={abs/1909.11942}
}

@inproceedings{bert,
  author    = {Jacob Devlin and
               Ming{-}Wei Chang and
               Kenton Lee and
               Kristina Toutanova},
  editor    = {Jill Burstein and
               Christy Doran and
               Thamar Solorio},
  title     = {{BERT:} Pre-training of Deep Bidirectional Transformers for Language
               Understanding},
  booktitle = {NAACL-HLT},
  year      = {2019}
}



@inproceedings{klimt2004enron,
  title={The enron corpus: A new dataset for email classification research},
  author={Klimt, Bryan and Yang, Yiming},
  booktitle={Machine Learning: ECML 2004: 15th European Conference on Machine Learning, Pisa, Italy, September 20-24, 2004. Proceedings 15},
  pages={217--226},
  year={2004},
  organization={Springer}
}
@article{alum,
  author    = {Xiaodong Liu and
               Hao Cheng and
               Pengcheng He and
               Weizhu Chen and
               Yu Wang and
               Hoifung Poon and
               Jianfeng Gao},
  title     = {Adversarial Training for Large Neural Language Models},
  journal   = {CoRR},
  volume    = {abs/2004.08994},
  year      = {2020}
}


@inproceedings{smart,
  author    = {Haoming Jiang and
               Pengcheng He and
               Weizhu Chen and
               Xiaodong Liu and
               Jianfeng Gao and
               Tuo Zhao},
  editor    = {Dan Jurafsky and
               Joyce Chai and
               Natalie Schluter and
               Joel R. Tetreault},
  title     = {{SMART:} Robust and Efficient Fine-Tuning for Pre-trained Natural
               Language Models through Principled Regularized Optimization},
  booktitle = {ACL},
  year      = {2020}
}

@inproceedings{mnli,
  author    = {Adina Williams and
               Nikita Nangia and
               Samuel R. Bowman},
  editor    = {Marilyn A. Walker and
               Heng Ji and
               Amanda Stent},
  title     = {A Broad-Coverage Challenge Corpus for Sentence Understanding through
               Inference},
  booktitle = {NAACL-HLT},
  year      = {2018}
}

@inproceedings{snli,
  author    = {Samuel R. Bowman and
               Gabor Angeli and
               Christopher Potts and
               Christopher D. Manning},
  editor    = {Llu{\'{\i}}s M{\`{a}}rquez and
               Chris Callison{-}Burch and
               Jian Su and
               Daniele Pighin and
               Yuval Marton},
  title     = {A large annotated corpus for learning natural language inference},
  booktitle = {EMNLP},
  year      = {2015}
}

@inproceedings{squad,
  author    = {Pranav Rajpurkar and
               Jian Zhang and
               Konstantin Lopyrev and
               Percy Liang},
  editor    = {Jian Su and
               Xavier Carreras and
               Kevin Duh},
  title     = {SQuAD: 100, 000+ Questions for Machine Comprehension of Text},
  booktitle = {EMNLP},
  year      = {2016}
}

@inproceedings{advsquad,
  author    = {Robin Jia and
               Percy Liang},
  editor    = {Martha Palmer and
               Rebecca Hwa and
               Sebastian Riedel},
  title     = {Adversarial Examples for Evaluating Reading Comprehension Systems},
  booktitle = {EMNLP},
  year      = {2017}
}


@inproceedings{textfooler,
  author    = {Di Jin and
               Zhijing Jin and
               Joey Tianyi Zhou and
               Peter Szolovits},
  title     = {Is {BERT} Really Robust? {A} Strong Baseline for Natural Language
               Attack on Text Classification and Entailment},
  booktitle = {AAAI},
  year      = {2020}
}

@article{infonce,
  author    = {A{\"{a}}ron van den Oord and
               Yazhe Li and
               Oriol Vinyals},
  title     = {Representation Learning with Contrastive Predictive Coding},
  journal   = {CoRR},
  volume    = {abs/1807.03748},
  year      = {2018}
}

@inproceedings{freelb,
  author    = {Chen Zhu and
               Yu Cheng and
               Zhe Gan and
               Siqi Sun and
               Tom Goldstein and
               Jingjing Liu},
  title     = {FreeLB: Enhanced Adversarial Training for Natural Language Understanding},
  booktitle = {ICLR},
  year      = {2020}
}


@article{tishby2000information,
  title={The information bottleneck method},
  author={Tishby, Naftali and Pereira, Fernando C and Bialek, William},
  journal={arXiv preprint physics/0004057},
  year={2000}
}

@article{Cheng2020CLUBAC,
  title={CLUB: A Contrastive Log-ratio Upper Bound of Mutual Information},
  author={Pengyu Cheng and Weituo Hao and Shuyang Dai and Jiachang Liu and Zhe Gan and L. Carin},
  journal={arXiv},
  year={2020},
  volume={abs/2006.12013}
}

@INPROCEEDINGS{deepib,
  author={N. {Tishby} and N. {Zaslavsky}},
  booktitle={2015 IEEE Information Theory Workshop (ITW)}, 
  title={Deep learning and the information bottleneck principle}, 
  year={2015},
  volume={},
  number={},
  pages={1-5},}

@inproceedings{Barber2003TheIA,
  title={The IM Algorithm: A Variational Approach to Information Maximization},
  author={David Barber and Felix V. Agakov},
  booktitle={NeurIPS},
  year={2003}
}

@inproceedings{
hjelm2018learning,
title={Learning deep representations by mutual information estimation and maximization},
author={R Devon Hjelm and Alex Fedorov and Samuel Lavoie-Marchildon and Karan Grewal and Phil Bachman and Adam Trischler and Yoshua Bengio},
booktitle={ICLR},
year={2019}
}

@inproceedings{
Kong2020A,
title={A Mutual Information Maximization Perspective of Language Representation Learning},
author={Lingpeng Kong and Cyprien de Masson d'Autume and Lei Yu and Wang Ling and Zihang Dai and Dani Yogatama},
booktitle={ICLR},
year={2020}
}


@inproceedings{Ilyas2019AdversarialEA,
  title={Adversarial Examples Are Not Bugs, They Are Features},
  author={Andrew Ilyas and Shibani Santurkar and Dimitris Tsipras and Logan Engstrom and Brandon Tran and Aleksander Madry},
  booktitle={NeurIPS},
  year={2019}
}



@InProceedings{pmlr-v80-belghazi18a,
  title = 	 {Mutual Information Neural Estimation},
  author = 	 {Belghazi, Mohamed Ishmael and Baratin, Aristide and Rajeshwar, Sai and Ozair, Sherjil and Bengio, Yoshua and Courville, Aaron and Hjelm, Devon},
  booktitle = 	 {ICML},
  year = 	 {2018},
}

@article{gan2020large,
  title={Large-Scale Adversarial Training for Vision-and-Language Representation Learning},
  author={Gan, Zhe and Chen, Yen-Chun and Li, Linjie and Zhu, Chen and Cheng, Yu and Liu, Jingjing},
  journal={arXiv preprint arXiv:2006.06195},
  year={2020}
}

@inproceedings{
vit,
title={An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale},
author={Alexey Dosovitskiy and Lucas Beyer and Alexander Kolesnikov and Dirk Weissenborn and Xiaohua Zhai and Thomas Unterthiner and Mostafa Dehghani and Matthias Minderer and Georg Heigold and Sylvain Gelly and Jakob Uszkoreit and Neil Houlsby},
booktitle={International Conference on Learning Representations},
year={2021},
url={https://openreview.net/forum?id=YicbFdNTTy}
}

@inproceedings{bae,
  title={BAE: BERT-based Adversarial Examples for Text Classification},
  author={Garg, Siddhant and Ramakrishnan, Goutham},
  booktitle={Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)},
  pages={6174--6181},
  year={2020}
}


@inproceedings{chen2020uniter,
  title={Uniter: Universal image-text representation learning},
  author={Chen, Yen-Chun and Li, Linjie and Yu, Licheng and Kholy, Ahmed El and Ahmed, Faisal and Gan, Zhe and Cheng, Yu and Liu, Jingjing},
  booktitle={ECCV},
  year={2020}
}

@inproceedings{villa,
  title={Large-Scale Adversarial Training for Vision-and-Language Representation Learning},
  author={Gan, Zhe and Chen, Yen-Chun and Li, Linjie and Zhu, Chen and Cheng, Yu and Liu, Jingjing},
  booktitle={NeurIPS},
  year={2020}
}

@inproceedings{checklist,
    title = "Beyond Accuracy: Behavioral Testing of {NLP} Models with {C}heck{L}ist",
    author = "Ribeiro, Marco Tulio  and
      Wu, Tongshuang  and
      Guestrin, Carlos  and
      Singh, Sameer",
    booktitle = "ACL",
    month = jul,
    year = "2020",
    pages = "4902--4912",
}

@inproceedings{ng-etal-2020-ssmba,
    title = "{SSMBA}: Self-Supervised Manifold Based Data Augmentation for Improving Out-of-Domain Robustness",
    author = "Ng, Nathan  and
      Cho, Kyunghyun  and
      Ghassemi, Marzyeh",
    booktitle = "EMNLP",
    month = nov,
    year = "2020",
    pages = "1268--1283",
}

@inproceedings{xie2021innout,
  author = {Sang Michael Xie and Ananya Kumar and Robert Jones and Fereshte Khani and Tengyu Ma and Percy Liang},
  booktitle = {International Conference on Learning Representations (ICLR)},
  title = {In-{N}-Out: Pre-Training and Self-Training using Auxiliary Information for Out-of-Distribution Robustness},
  year = {2021},
}

@inproceedings{fairsent,
    title = "Towards Debiasing Sentence Representations",
    author = "Liang, Paul Pu  and
      Li, Irene Mengze  and
      Zheng, Emily  and
      Lim, Yao Chong  and
      Salakhutdinov, Ruslan  and
      Morency, Louis-Philippe",
    booktitle = "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics",
    month = jul,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/2020.acl-main.488",
    doi = "10.18653/v1/2020.acl-main.488",
    pages = "5502--5515",
}

@inproceedings{sun-etal-2019-mitigating,
    title = "Mitigating Gender Bias in Natural Language Processing: Literature Review",
    author = "Sun, Tony  and
      Gaut, Andrew  and
      Tang, Shirlyn  and
      Huang, Yuxin  and
      ElSherief, Mai  and
      Zhao, Jieyu  and
      Mirza, Diba  and
      Belding, Elizabeth  and
      Chang, Kai-Wei  and
      Wang, William Yang",
    booktitle = "ACL",
    month = jul,
    year = "2019",
    address = "Florence, Italy",
    publisher = "Association for Computational Linguistics",
}

@inproceedings{zhao-etal-2017-men,
    title = "Men Also Like Shopping: Reducing Gender Bias Amplification using Corpus-level Constraints",
    author = "Zhao, Jieyu  and
      Wang, Tianlu  and
      Yatskar, Mark  and
      Ordonez, Vicente  and
      Chang, Kai-Wei",
    booktitle = "EMNLP",
    month = sep,
    year = "2017",
    address = "Copenhagen, Denmark",
    publisher = "Association for Computational Linguistics",
}

@inproceedings{Bolukbasi2016ManIT,
  title={Man is to Computer Programmer as Woman is to Homemaker? Debiasing Word Embeddings},
  author={Tolga Bolukbasi and Kai-Wei Chang and James Y. Zou and Venkatesh Saligrama and A. Kalai},
  booktitle={NIPS},
  year={2016}
}

@inproceedings{manzini-etal-2019-black,
    title = "Black is to Criminal as Caucasian is to Police: Detecting and Removing Multiclass Bias in Word Embeddings",
    author = "Manzini, Thomas  and
      Yao Chong, Lim  and
      Black, Alan W  and
      Tsvetkov, Yulia",
    booktitle = "NAACL",
    month = jun,
    year = "2019",
    address = "Minneapolis, Minnesota",
    publisher = "Association for Computational Linguistics",
}

@InProceedings{pmlr-v97-pang19a, title = {Improving Adversarial Robustness via Promoting Ensemble Diversity}, author = {Pang, Tianyu and Xu, Kun and Du, Chao and Chen, Ning and Zhu, Jun}, booktitle = {Proceedings of the 36th International Conference on Machine Learning}, pages = {4970--4979}, year = {2019}, editor = {Kamalika Chaudhuri and Ruslan Salakhutdinov}, volume = {97}, series = {Proceedings of Machine Learning Research}, month = {09--15 Jun}, publisher = {PMLR}}


@inproceedings{
Pang2020Rethinking,
title={Rethinking Softmax Cross-Entropy Loss for Adversarial Robustness},
author={Tianyu Pang and Kun Xu and Yinpeng Dong and Chao Du and Ning Chen and Jun Zhu},
booktitle={International Conference on Learning Representations},
year={2020},
}

@inproceedings{
wang2021infobert,
title={InfoBERT: Improving Robustness of Language Models from An Information Theoretic Perspective},
author={Wang, Boxin and Wang, Shuohang and Cheng, Yu and Gan, Zhe and Jia, Ruoxi and Li, Bo and Liu, Jingjing},
booktitle={ICLR},
year={2021}}

@article{wieting2017paranmt,
  title={ParaNMT-50M: Pushing the limits of paraphrastic sentence embeddings with millions of machine translations},
  author={Wieting, John and Gimpel, Kevin},
  journal={arXiv preprint arXiv:1711.05732},
  year={2017}
}

@article{he2020deberta,
  title={Deberta: Decoding-enhanced bert with disentangled attention},
  author={He, Pengcheng and Liu, Xiaodong and Gao, Jianfeng and Chen, Weizhu},
  journal={arXiv preprint arXiv:2006.03654},
  year={2020}
}

@inproceedings{zhang2019ernie,
  title={ERNIE: Enhanced language representation with informative entities},
  author={Zhang, Zhengyan and Han, Xu and Liu, Zhiyuan and Jiang, Xin and Sun, Maosong and Liu, Qun},
  booktitle={ACL},
  year={2019}
}

@inproceedings{morris-etal-2020-reevaluating,
    title = "Reevaluating Adversarial Examples in Natural Language",
    author = "Morris, John  and
      Lifland, Eli  and
      Lanchantin, Jack  and
      Ji, Yangfeng  and
      Qi, Yanjun",
    booktitle = "Findings of the Association for Computational Linguistics: EMNLP 2020",
    month = nov,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics"
}

@article{clark2020electra,
  title={Electra: Pre-training text encoders as discriminators rather than generators},
  author={Clark, Kevin and Luong, Minh-Thang and Le, Quoc V and Manning, Christopher D},
  journal={arXiv preprint arXiv:2003.10555},
  year={2020}
}

@inproceedings{wang2018glue,
  title={GLUE: A multi-task benchmark and analysis platform for natural language understanding},
  author={Wang, Alex and Singh, Amanpreet and Michael, Julian and Hill, Felix and Levy, Omer and Bowman, Samuel R},
  booktitle={ICLR},
  year={2019}
}

@inproceedings{wang2019superglue,
  title={Superglue: A stickier benchmark for general-purpose language understanding systems},
  author={Wang, Alex and Pruksachatkun, Yada and Nangia, Nikita and Singh, Amanpreet and Michael, Julian and Hill, Felix and Levy, Omer and Bowman, Samuel R},
  booktitle={NeurIPS},
  year={2019}
}

@article{rajpurkar2016squad,
  title={Squad: 100,000+ questions for machine comprehension of text},
  author={Rajpurkar, Pranav and Zhang, Jian and Lopyrev, Konstantin and Liang, Percy},
  journal={arXiv preprint arXiv:1606.05250},
  year={2016}
}

@article{williams2017broad,
  title={A broad-coverage challenge corpus for sentence understanding through inference},
  author={Williams, Adina and Nangia, Nikita and Bowman, Samuel R},
  journal={arXiv preprint arXiv:1704.05426},
  year={2017}
}

@inproceedings{kaushik2019learning,
  title={Learning The Difference That Makes A Difference With Counterfactually-Augmented Data},
  author={Kaushik, Divyansh and Hovy, Eduard and Lipton, Zachary},
  booktitle={International Conference on Learning Representations},
  year={2019}
}

@inproceedings{qi-etal-2021-mind,
    title = "Mind the Style of Text! Adversarial and Backdoor Attacks Based on Text Style Transfer",
    author = "Qi, Fanchao  and
      Chen, Yangyi  and
      Zhang, Xurui  and
      Li, Mukai  and
      Liu, Zhiyuan  and
      Sun, Maosong",
    booktitle = "EMNLP",
    year = "2021"
}

@inproceedings{qi-etal-2021-hidden,
    title = "Hidden Killer: Invisible Textual Backdoor Attacks with Syntactic Trigger",
    author = "Qi, Fanchao  and
      Li, Mukai  and
      Chen, Yangyi  and
      Zhang, Zhengyan  and
      Liu, Zhiyuan  and
      Wang, Yasheng  and
      Sun, Maosong",
    booktitle = "ACL-IJCNLP",
    year = "2021"
}

@article{dai2019backdoor,
  title={A backdoor attack against lstm-based text classification systems},
  author={Dai, Jiazhu and Chen, Chuanshuai and Li, Yufeng},
  journal={IEEE Access},
  volume={7},
  pages={138872--138878},
  year={2019},
  publisher={IEEE}
}

@inproceedings{borgeaud2022improving,
  title={Improving language models by retrieving from trillions of tokens},
  author={Borgeaud, Sebastian and Mensch, Arthur and Hoffmann, Jordan and Cai, Trevor and Rutherford, Eliza and Millican, Katie and Van Den Driessche, George Bm and Lespiau, Jean-Baptiste and Damoc, Bogdan and Clark, Aidan and others},
  booktitle={ICML},
  year={2022}
}

@article{hnsw,
  title     = {Efficient and robust approximate nearest neighbor search using hierarchical navigable small world graphs},
  author    = {Malkov, Yu A and Yashunin, Dmitry A},
  journal   = {IEEE transactions on pattern analysis and machine intelligence},
  volume    = {42},
  number    = {4},
  pages     = {824-836},
  year      = {2018},
  publisher = {IEEE}
}

@ARTICLE{pq,
  author={Gray, R.M. and Neuhoff, D.L.},
  journal={IEEE Transactions on Information Theory}, 
  title={Quantization}, 
  year={1998},
  volume={44},
  number={6},
  pages={2325-2383},
  doi={10.1109/18.720541}}

@ARTICLE{opq,
  author={Ge, Tiezheng and He, Kaiming and Ke, Qifa and Sun, Jian},
  journal={IEEE Transactions on Pattern Analysis and Machine Intelligence}, 
  title={Optimized Product Quantization}, 
  year={2014},
  volume={36},
  number={4},
  pages={744-755},
  doi={10.1109/TPAMI.2013.240}}


@article{lee2022factuality,
  title   = {Factuality Enhanced Language Models for Open-Ended Text Generation},
  author  = {Nayeon Lee and Wei Ping and Peng Xu and Mostofa Patwary and Pascale Fung and Mohammad Shoeybi and Bryan Catanzaro},
  year    = {2022},
  journal = {NeurIPS}
}

@article{nakano2021webgpt,
  title={Webgpt: Browser-assisted question-answering with human feedback},
  author={Nakano, Reiichiro and Hilton, Jacob and Balaji, Suchir and Wu, Jeff and Ouyang, Long and Kim, Christina and Hesse, Christopher and Jain, Shantanu and Kosaraju, Vineet and Saunders, William and others},
  journal={arXiv preprint arXiv:2112.09332},
  year={2021}
}

@software{lmharness,
  author       = {Gao, Leo and
                  Tow, Jonathan and
                  Biderman, Stella and
                  Black, Sid and
                  DiPofi, Anthony and
                  Foster, Charles and
                  Golding, Laurence and
                  Hsu, Jeffrey and
                  McDonell, Kyle and
                  Muennighoff, Niklas and
                  Phang, Jason and
                  Reynolds, Laria and
                  Tang, Eric and
                  Thite, Anish and
                  Wang, Ben and
                  Wang, Kevin and
                  Zou, Andy},
  title        = {A framework for few-shot language model evaluation},
  month        = sep,
  year         = 2021,
  publisher    = {Zenodo},
  version      = {v0.0.1},
  doi          = {10.5281/zenodo.5371628},
  url          = {https://doi.org/10.5281/zenodo.5371628}
}

@article{lin2021truthfulqa,
  title     = {Truthful{QA}: Measuring How Models Mimic Human Falsehoods},
  author    = {Stephanie C. Lin and Jacob Hilton and Owain Evans},
  journal   = {ACL},
  year      = {2021},
}


@inproceedings{khandelwal2019generalization,
  title={Generalization through memorization: Nearest neighbor language models},
  author={Khandelwal, Urvashi and Levy, Omer and Jurafsky, Dan and Zettlemoyer, Luke and Lewis, Mike},
  journal={ICLR},
  year={2020}
}

@inproceedings{karpukhin2020dense,
  title={Dense passage retrieval for open-domain question answering},
  author={Karpukhin, Vladimir and O{\u{g}}uz, Barlas and Min, Sewon and Lewis, Patrick and Wu, Ledell and Edunov, Sergey and Chen, Danqi and Yih, Wen-tau},
  booktitle={EMNLP},
  year={2020}
}

@inproceedings{guu2020retrieval,
  title={{REALM}: Retrieval augmented language model pre-training},
  author={Guu, Kelvin and Lee, Kenton and Tung, Zora and Pasupat, Panupong and Chang, Mingwei},
  booktitle={ICML},
  year={2020}
}

@inproceedings{lewis2020retrieval,
  title={Retrieval-augmented generation for knowledge-intensive {NLP} tasks},
  author={Lewis, Patrick and Perez, Ethan and Piktus, Aleksandra and Petroni, Fabio and Karpukhin, Vladimir and Goyal, Naman and K{\"u}ttler, Heinrich and Lewis, Mike and Yih, Wen-tau and Rockt{\"a}schel, Tim and others},
  booktitle={NeurIPS},
  year={2020}
}

@article{faiss,
  title={Billion-scale similarity search with {GPUs}},
  author={Johnson, Jeff and Douze, Matthijs and J{\'e}gou, Herv{\'e}},
  journal={IEEE Transactions on Big Data},
  volume={7},
  number={3},
  pages={535--547},
  year={2019},
  publisher={IEEE}
}

@article{nucleus,
  title     = {The Curious Case of Neural Text Degeneration},
  author    = {Ari Holtzman and Jan Buys and Maxwell Forbes and Yejin Choi},
  journal   = {International Conference On Learning Representations},
  year      = {2019},
  bibSource = {Semantic Scholar https://www.semanticscholar.org/paper/cf4aa38ae31b43fd07abe13b4ffdb265babb7be1}
}

@article{izacard2022few,
  title={Few-shot learning with retrieval augmented language models},
  author={Izacard, Gautier and Lewis, Patrick and Lomeli, Maria and Hosseini, Lucas and Petroni, Fabio and Schick, Timo and Dwivedi-Yu, Jane and Joulin, Armand and Riedel, Sebastian and Grave, Edouard},
  journal={arXiv preprint arXiv:2208.03299},
  year={2022}
}

@unpublished{spacy2,
    AUTHOR = {Honnibal, Matthew and Montani, Ines},
    TITLE  = {{spaCy 2}: Natural language understanding with {B}loom embeddings, convolutional neural networks and incremental parsing},
    YEAR   = {2017},
    Note   = {To appear}
}


@article{piantadosi14zipfs,
  publisher = {Springer Science and Business Media LLC},
  author    = {Steven T. Piantadosi},
  title     = {Zipf’s word frequency law in natural language: A critical review and future directions},
  year      = {2014},
  doi       = {10.3758/s13423-014-0585-6},
  pages     = {1112-1130},
  volume    = {21},
  journal   = {Psychonomic Bulletin \& Review},
  pdf       = {https://link.springer.com/content/pdf/10.3758/s13423-014-0585-6.pdf},
  url       = {https://link.springer.com/article/10.3758/s13423-014-0585-6/fulltext.html}
}

@inproceedings{selfbleu,
author = {Zhu, Yaoming and Lu, Sidi and Zheng, Lei and Guo, Jiaxian and Zhang, Weinan and Wang, Jun and Yu, Yong},
title = {Texygen: A Benchmarking Platform for Text Generation Models},
year = {2018},
isbn = {9781450356572},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3209978.3210080},
doi = {10.1145/3209978.3210080},
abstract = {We introduce Texygen, a benchmarking platform to support research on open-domain text generation models. Texygen has not only implemented a majority of text generation models, but also covered a set of metrics that evaluate the diversity, the quality and the consistency of the generated texts. The Texygen platform could help standardize the research on text generation and improve the reproductivity and reliability of future research work in text generation.},
booktitle = {The 41st International ACM SIGIR Conference on Research \& Development in Information Retrieval},
pages = {1097–1100},
numpages = {4},
keywords = {evaluation metrics, benchmarking, text generation},
location = {Ann Arbor, MI, USA},
series = {SIGIR '18}
}

@inproceedings{hans,
    title = "Investigating the Effect of Natural Language Explanations on Out-of-Distribution Generalization in Few-shot {NLI}",
    author = "Zhou, Yangqiaoyu  and
      Tan, Chenhao",
    booktitle = "Proceedings of the Second Workshop on Insights from Negative Results in NLP",
    month = nov,
    year = "2021",
    address = "Online and Punta Cana, Dominican Republic",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.insights-1.17",
    doi = "10.18653/v1/2021.insights-1.17",
    pages = "117--124",
    abstract = "Although neural models have shown strong performance in datasets such as SNLI, they lack the ability to generalize out-of-distribution (OOD). In this work, we formulate a few-shot learning setup and examine the effects of natural language explanations on OOD generalization. We leverage the templates in the HANS dataset and construct templated natural language explanations for each template. Although generated explanations show competitive BLEU scores against ground truth explanations, they fail to improve prediction performance. We further show that generated explanations often hallucinate information and miss key elements that indicate the label.",
}

@article{izacardunsupervised,
  title={Unsupervised Dense Information Retrieval with Contrastive Learning},
  author={Izacard, Gautier and Caron, Mathilde and Hosseini, Lucas and Riedel, Sebastian and Bojanowski, Piotr and Joulin, Armand and Grave, Edouard},
  journal={Transactions on Machine Learning Research}
}

@article{radford2019language,
  title={Language models are unsupervised multitask learners},
  author={Radford, Alec and Wu, Jeffrey and Child, Rewon and Luan, David and Amodei, Dario and Sutskever, Ilya and others},
  journal={OpenAI blog},
  volume={1},
  number={8},
  pages={9},
  year={2019}
}

@misc{chatgpt,
  title={Chat{GPT}},
  author={OpenAI},
  year={2022},
  howpublished={\url{https://chat.openai.com}},
}

@misc{prompthack,
  title={Introduction to Prompt Hacking},
  author={{Learn Prompting}},
  year={2023},
  howpublished={\url{https://learnprompting.org/docs/prompt-hacking/intro}},
}

@misc{shakespearean,
  title={Shakespearean},
  howpublished={\url{https://lingojam.com/shakespearean}},
}

@article{shoeybi2019megatron,
  title={Megatron-lm: Training multi-billion parameter language models using model parallelism},
  author={Shoeybi, Mohammad and Patwary, Mostofa and Puri, Raul and LeGresley, Patrick and Casper, Jared and Catanzaro, Bryan},
  journal={arXiv preprint arXiv:1909.08053},
  year={2019}
}

@article{devlin2018bert,
  title={{BERT}: Pre-training of deep bidirectional transformers for language understanding},
  author={Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
  journal={arXiv preprint arXiv:1810.04805},
  year={2018}
}

@article{raffel2020exploring,
  title={Exploring the limits of transfer learning with a unified text-to-text transformer.},
  author={Raffel, Colin and Shazeer, Noam and Roberts, Adam and Lee, Katherine and Narang, Sharan and Matena, Michael and Zhou, Yanqi and Li, Wei and Liu, Peter J and others},
  journal={Journal of Machine Learning Research},
  year={2020}
}

% Use this file for citations not found in the ACL Anthology (contained in "anthology.bib").

@inproceedings{adapter,
  title={Parameter-Efficient Transfer Learning for NLP},
  author={Neil Houlsby and Andrei Giurgiu and Stanislaw Jastrzebski and Bruna Morrone and Quentin de Laroussilhe and Andrea Gesmundo and Mona Attariyan and Sylvain Gelly},
  booktitle={ICML},
  year={2019}
}

@inproceedings{
wang2022semattack,
author = {Wang, Boxin and Xu, Chejian and Liu, Xiangyu and Cheng, Yu and Li, Bo},
title = {{S}em{A}ttack: Natural Textual Attacks via Different Semantic Spaces},
year = {2022},
booktitle={Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies}
}

@inproceedings{wang2020t3,
  title={T3: Tree-Autoencoder Regularized Adversarial Text Generation for Targeted Attack},
  author={Wang, Boxin and Pei, Hengzhi and Pan, Boyuan and Chen, Qian and Wang, Shuohang and Li, Bo},
  booktitle={Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)},
  pages={6134--6150},
  year={2020}
}


@inproceedings{anli,
  title={Adversarial NLI: A New Benchmark for Natural Language Understanding},
  author={Nie, Yixin and Williams, Adina and Dinan, Emily and Bansal, Mohit and Weston, Jason and Kiela, Douwe},
  booktitle={ACL},
  year={2020}
}

@inproceedings{booq,
  title={BoolQ: Exploring the Surprising Difficulty of Natural Yes/No Questions},
  author={Clark, Christopher and Lee, Kenton and Chang, Ming-Wei and Kwiatkowski, Tom and Collins, Michael and Toutanova, Kristina},
  booktitle={NAACL},
  year={2019}
}

@inproceedings{izacard2021leveraging,
  title={Leveraging Passage Retrieval with Generative Models for Open Domain Question Answering},
  author={Izacard, Gautier and Grave, {\'E}douard},
  booktitle={Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume},
  pages={874--880},
  year={2021}
}


@inproceedings{li-etal-2016-diversity,
    title = "A Diversity-Promoting Objective Function for Neural Conversation Models",
    author = "Li, Jiwei  and
      Galley, Michel  and
      Brockett, Chris  and
      Gao, Jianfeng  and
      Dolan, Bill",
    booktitle = "Proceedings of the 2016 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies",
    month = jun,
    year = "2016",
    address = "San Diego, California",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/N16-1014",
    doi = "10.18653/v1/N16-1014",
    pages = "110--119",
}

@inproceedings{instuning,
  author    = {Jason Wei and Maarten Bosma and Vincent Y. Zhao and Kelvin Guu and Adams Wei Yu and Brian Lester and Nan Du and Andrew M. Dai and Quoc V. Le},
  title     = {Finetuned Language Models are Zero-Shot Learners},
  booktitle = {The Tenth International Conference on Learning Representations, {ICLR} 2022, Virtual Event, April 25-29, 2022},
  publisher = {OpenReview.net},
  year      = {2022},
  url       = {https://openreview.net/forum?id=gEZrGCozdqR},
  timestamp = {Sat, 20 Aug 2022 01:15:42 +0200},
  biburl    = {https://dblp.org/rec/conf/iclr/WeiBZGYLDDL22.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{shinn2023reflexion,
  title   = {Reflexion: an autonomous agent with dynamic memory and self-reflection},
  author  = {Noah Shinn and Beck Labash and Ashwin Gopinath},
  year    = {2023},
  journal = {arXiv preprint arXiv: Arxiv-2303.11366}
}



@article{liu2023pre,
  title     = {Pre-train, prompt, and predict: A systematic survey of prompting methods in natural language processing},
  author    = {Liu, Pengfei and Yuan, Weizhe and Fu, Jinlan and Jiang, Zhengbao and Hayashi, Hiroaki and Neubig, Graham},
  journal   = {ACM Computing Surveys},
  volume    = {55},
  number    = {9},
  pages     = {1-35},
  year      = {2023},
  publisher = {ACM New York, NY}
}

@article{solaiman2021process,
  title   = {Process for adapting language models to society (palms) with values-targeted datasets},
  author  = {Solaiman, Irene and Dennison, Christy},
  journal = {Advances in Neural Information Processing Systems},
  volume  = {34},
  pages   = {5861-5873},
  year    = {2021}
}

@article{instuning2,
  title     = {Scaling Instruction-Finetuned Language Models},
  author    = {Hyung Won Chung and Le Hou and S. Longpre and Barret Zoph and Yi Tay and W. Fedus and Eric Li and Xuezhi Wang and M. Dehghani and Siddhartha Brahma and Albert Webson and S. Gu and Zhuyun Dai and Mirac Suzgun and Xinyun Chen and Aakanksha Chowdhery and Dasha Valter and Sharan Narang and Gaurav Mishra and A. Yu and Vincent Zhao and Yanping Huang and Andrew M. Dai and Hongkun Yu and Slav Petrov and E. Chi and J. Dean and Jacob Devlin and Adam Roberts and Denny Zhou and Quoc V. Le and Jason Wei},
  journal   = {ARXIV.ORG},
  year      = {2022},
  doi       = {10.48550/arXiv.2210.11416},
  bibSource = {Semantic Scholar https://www.semanticscholar.org/paper/5484d228bfc50efbac6e86677bc2ec2ee4ede1a6}
}



@inproceedings{baheti-etal-2021-just,
    title = "Just Say No: Analyzing the Stance of Neural Dialogue Generation in Offensive Contexts",
    author = "Baheti, Ashutosh  and
      Sap, Maarten  and
      Ritter, Alan  and
      Riedl, Mark",
    booktitle = "Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing",
    month = nov,
    year = "2021",
    address = "Online and Punta Cana, Dominican Republic",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.emnlp-main.397",
    doi = "10.18653/v1/2021.emnlp-main.397",
    pages = "4846--4862",
    abstract = "Dialogue models trained on human conversations inadvertently learn to generate toxic responses. In addition to producing explicitly offensive utterances, these models can also implicitly insult a group or individual by aligning themselves with an offensive statement. To better understand the dynamics of contextually offensive language, we investigate the stance of dialogue model responses in offensive Reddit conversations. Specifically, we create ToxiChat, a crowd-annotated dataset of 2,000 Reddit threads and model responses labeled with offensive language and stance. Our analysis reveals that 42{\%} of human responses agree with toxic comments, whereas only 13{\%} agree with safe comments. This undesirable behavior is learned by neural dialogue models, such as DialoGPT, which we show are two times more likely to agree with offensive comments. To enable automatic detection of offensive language, we fine-tuned transformer-based classifiers on ToxiChat that achieve 0.71 F1 for offensive labels and 0.53 Macro-F1 for stance labels. Finally, we quantify the effectiveness of controllable text generation (CTG) methods to mitigate the tendency of neural dialogue models to agree with offensive comments. Compared to the baseline, our best CTG model achieves a 19{\%} reduction in agreement with offensive comments and produces 29{\%} fewer offensive replies. Our work highlights the need for further efforts to characterize and analyze inappropriate behavior in dialogue models, in order to help make them safer.",
}

@inproceedings{hellaswag,
  title={HellaSwag: Can a Machine Really Finish Your Sentence?},
  author={Zellers, Rowan and Holtzman, Ari and Bisk, Yonatan and Farhadi, Ali and Choi, Yejin},
  booktitle={ACL},
  year={2019}
}

@inproceedings{winogrande,
  title={Winogrande: An adversarial winograd schema challenge at scale},
  author={Sakaguchi, Keisuke and Le Bras, Ronan and Bhagavatula, Chandra and Choi, Yejin},
  booktitle={AAAI},
  year={2020}
}

@inproceedings{wic,
  title={WiC: the Word-in-Context Dataset for Evaluating Context-Sensitive Meaning Representations},
  author={Pilehvar, Mohammad Taher and Camacho-Collados, Jose},
  booktitle={NAACL},
  year={2019}
}

@inproceedings{lambada,
  title={The LAMBADA dataset: Word prediction requiring a broad discourse context},
  author={Paperno, Denis and Kruszewski, Germ{\'a}n and Lazaridou, Angeliki and Pham, Ngoc-Quan and Bernardi, Raffaella and Pezzelle, Sandro and Baroni, Marco and Boleda, Gemma and Fern{\'a}ndez, Raquel},
  booktitle={NAACL},
  year={2016}
}

@inproceedings{dhamala2021bold,
  title={Bold: Dataset and metrics for measuring biases in open-ended language generation},
  author={Dhamala, Jwala and Sun, Tony and Kumar, Varun and Krishna, Satyapriya and Pruksachatkun, Yada and Chang, Kai-Wei and Gupta, Rahul},
  booktitle={Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency},
  pages={862--872},
  year={2021}
}

@inproceedings{race,
  title={RACE: Large-scale ReAding Comprehension Dataset From Examinations},
  author={Lai, Guokun and Xie, Qizhe and Liu, Hanxiao and Yang, Yiming and Hovy, Eduard},
  booktitle={EMNLP},
  year={2017}
}

@inproceedings{piqa,
  title={Piqa: Reasoning about physical commonsense in natural language},
  author={Bisk, Yonatan and Zellers, Rowan and Gao, Jianfeng and Choi, Yejin and others},
  booktitle={AAAI},
  year={2020}
}

@article{ramesh2021zero,
  title={Zero-shot text-to-image generation},
  author={Ramesh, Aditya and Pavlov, Mikhail and Goh, Gabriel and Gray, Scott and Voss, Chelsea and Radford, Alec and Chen, Mark and Sutskever, Ilya},
  journal={arXiv preprint arXiv:2102.12092},
  year={2021}
}

@article{welbl2021challenges,
  title={Challenges in detoxifying language models},
  author={Welbl, Johannes and Glaese, Amelia and Uesato, Jonathan and Dathathri, Sumanth and Mellor, John and Hendricks, Lisa Anne and Anderson, Kirsty and Kohli, Pushmeet and Coppin, Ben and Huang, Po-Sen},
  journal={Findings of EMNLP},
  year={2021}
}


@inproceedings{dathathri2019plug,
  title={Plug and Play Language Models: A Simple Approach to Controlled Text Generation},
  author={Dathathri, Sumanth and Madotto, Andrea and Lan, Janice and Hung, Jane and Frank, Eric and Molino, Piero and Yosinski, Jason and Liu, Rosanne},
  booktitle={ICLR},
  year={2019}
}

@article{selfdebiasing,
  title={Self-diagnosis and self-debiasing: A proposal for reducing corpus-based bias in nlp},
  author={Schick, Timo and Udupa, Sahana and Sch{\"u}tze, Hinrich},
  journal={TACL},
  year={2021}
}

@article{megatron,
  title={Megatron-LM: Training Multi-Billion Parameter Language Models Using Model Parallelism},
  author={Mohammad Shoeybi and Mostofa Ali Patwary and Raul Puri and Patrick LeGresley and Jared Casper and Bryan Catanzaro},
  journal={arXiv},
  year={2019}
}

@article{mcguffie2020radicalization,
  title={The radicalization risks of {GPT}-3 and advanced neural language models},
  author={McGuffie, Kris and Newhouse, Alex},
  journal={arXiv},
  year={2020}
}

@article{gpt2,
  title={Language Models are Unsupervised Multitask Learners},
  author={Radford, Alec and Wu, Jeff and Child, Rewon and Luan, David and Amodei, Dario and Sutskever, Ilya},
  year={2019}
}

@article{MegatronTuring,
  title={Using DeepSpeed and Megatron to Train Megatron-Turing NLG 530B, the World’s Largest and Most Powerful Generative Language Model},
  author={Kharya, Paresh and Alvi, Ali},
  year={2021},
}

@article{krause2020gedi,
  title={Ge{D}i: Generative discriminator guided sequence generation},
  author={Krause, Ben and Gotmare, Akhilesh Deepak and McCann, Bryan and Keskar, Nitish Shirish and Joty, Shafiq and Socher, Richard and Rajani, Nazneen Fatema},
  journal={arXiv},
  year={2020}
}

@inproceedings{holtzman2019curious,
  title={The Curious Case of Neural Text Degeneration},
  author={Holtzman, Ari and Buys, Jan and Du, Li and Forbes, Maxwell and Choi, Yejin},
  booktitle={ICLR},
  year={2019}
}

@inproceedings{welleck2019neural,
  title={Neural text generation with unlikelihood training},
  author={Welleck, Sean and Kulikov, Ilia and Roller, Stephen and Dinan, Emily and Cho, Kyunghyun and Weston, Jason},
  booktitle={International Conference on Learning Representations (ICLR)},
  year={2020}
}

@misc{Gokaslan2019OpenWeb,  
	title={OpenWebText Corpus},
	author={Aaron Gokaslan and Vanya Cohen},
	howpublished={\url{http://Skylion007.github.io/OpenWebTextCorpus}}, 
	year={2019}
}

@inproceedings{transformers,
  title={Attention is all you need},
  author={Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, {\L}ukasz and Polosukhin, Illia},
  booktitle={NIPS},
  year={2017}
}

@article{gao2020pile,
  title={The Pile: An 800GB Dataset of Diverse Text for Language Modeling},
  author={Gao, Leo and Biderman, Stella and Black, Sid and Golding, Laurence and Hoppe, Travis and Foster, Charles and Phang, Jason and He, Horace and Thite, Anish and Nabeshima, Noa and others},
  journal={arXiv preprint arXiv:2101.00027},
  year={2020}
}

@misc{adam,
    title={Adam: A Method for Stochastic Optimization},
    author={Diederik P. Kingma and Jimmy Ba},
    year={2014},
    booktitle={ICLR},
}

@article{survey,
    title={Pre-train, Prompt, and Predict: A Systematic Survey of Prompting Methods in Natural Language Processing},
    author={Pengfei Liu and Weizhe Yuan and Jinlan Fu and Zhengbao Jiang and Hiroaki Hayashi and Graham Neubig},
    year={2021},
    journal={arXiv},
}
@article{bubeck2023sparks,
  title={Sparks of artificial general intelligence: Early experiments with gpt-4},
  author={Bubeck, S{\'e}bastien and Chandrasekaran, Varun and Eldan, Ronen and Gehrke, Johannes and Horvitz, Eric and Kamar, Ece and Lee, Peter and Lee, Yin Tat and Li, Yuanzhi and Lundberg, Scott and others},
  journal={arXiv preprint arXiv:2303.12712},
  year={2023}
}

@article{t5,
  author  = {Colin Raffel and Noam Shazeer and Adam Roberts and Katherine Lee and Sharan Narang and Michael Matena and Yanqi Zhou and Wei Li and Peter J. Liu},
  title   = {Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer},
  journal = {Journal of Machine Learning Research},
  year    = {2020},
  volume  = {21},
  number  = {140},
  pages   = {1-67},
  url     = {http://jmlr.org/papers/v21/20-074.html}
}

@inproceedings{wang2022exploring,
  title     = {Exploring the Limits of Domain-Adaptive Training for Detoxifying Large-Scale Language Models},
  author    = {Boxin Wang and Wei Ping and Chaowei Xiao and Peng Xu and Mostofa Patwary and Mohammad Shoeybi and Bo Li and Anima Anandkumar and Bryan Catanzaro},
  booktitle = {Advances in Neural Information Processing Systems},
  editor    = {Alice H. Oh and Alekh Agarwal and Danielle Belgrave and Kyunghyun Cho},
  year      = {2022},
  url       = {https://openreview.net/forum?id=v-0F4IZJZw}
}

@article{prompt1,
  title={Autoprompt: Eliciting knowledge from language models with automatically generated prompts},
  author={Shin, Taylor and Razeghi, Yasaman and Logan IV, Robert L and Wallace, Eric and Singh, Sameer},
  journal={arXiv},
  year={2020}
}

@inproceedings{li2021prefix,
  title={Prefix-{T}uning: Optimizing continuous prompts for generation},
  author={Li, Xiang Lisa and Liang, Percy},
  booktitle={ACL},
  year={2021}
}


@article{prompt3,
  title={The power of scale for parameter-efficient prompt tuning},
  author={Lester, Brian and Al-Rfou, Rami and Constant, Noah},
  journal={arXiv},
  year={2021}
}

@article{prompt4,
  title={Calibrate before use: Improving few-shot performance of language models},
  author={Zhao, Tony Z and Wallace, Eric and Feng, Shi and Klein, Dan and Singh, Sameer},
  journal={arXiv},
  year={2021}
}

@article{prompt5,
  title={Exploiting Cloze Questions for Few-Shot Text Classification and Natural Language Inference},
  author={Timo Schick and Hinrich Schütze},
  journal={arXiv},
  year={2020}
}

@article{prompt6,
  title={It's Not Just Size That Matters: Small Language Models Are Also Few-Shot Learners},
  author={Timo Schick and Hinrich Schütze},
  journal={arXiv},
  year={2020}
}

@inproceedings{liu2021dexperts,
  title={D{E}xperts: Decoding-time controlled text generation with experts and anti-experts},
  author={Liu, Alisa and Sap, Maarten and Lu, Ximing and Swayamdipta, Swabha and Bhagavatula, Chandra and Smith, Noah A and Choi, Yejin},
  booktitle={ACL},
  year={2021}
}

@article{basta2019evaluating,
  title={Evaluating the underlying gender bias in contextualized word embeddings},
  author={Basta, Christine and Costa-Juss{\`a}, Marta R and Casas, Noe},
  journal={arXiv preprint arXiv:1904.08783},
  year={2019}
}

@inproceedings{gehman2020realtoxicityprompts,
  title={Real{T}oxicity{P}rompts: Evaluating neural toxic degeneration in language models},
  author={Gehman, Samuel and Gururangan, Suchin and Sap, Maarten and Choi, Yejin and Smith, Noah A},
  booktitle={Findings in EMNLP},
  year={2020}
}

@inproceedings{gururangan2020don,
  title={Don't stop pretraining: adapt language models to domains and tasks},
  author={Gururangan, Suchin and Marasovi{\'c}, Ana and Swayamdipta, Swabha and Lo, Kyle and Beltagy, Iz and Downey, Doug and Smith, Noah A},
  booktitle={ACL},
  year={2020}
}

@inproceedings{may2019measuring,
  title={On measuring social biases in sentence encoders},
  author={May, Chandler and Wang, Alex and Bordia, Shikha and Bowman, Samuel R and Rudinger, Rachel},
  booktitle={NAACL},
  year={2019}
}

@inproceedings{wallace2019universal,
  title={Universal adversarial triggers for attacking and analyzing NLP},
  author={Wallace, Eric and Feng, Shi and Kandpal, Nikhil and Gardner, Matt and Singh, Sameer},
  booktitle={EMNLP},
  year={2019}
}

@inproceedings{xu2021detoxifying,
  title={Detoxifying language models risks marginalizing minority voices},
  author={Xu, Albert and Pathak, Eshaan and Wallace, Eric and Gururangan, Suchin and Sap, Maarten and Klein, Dan},
  booktitle={NAACL},
  year={2021}
}

@inproceedings{zhao2019gender,
  title={Gender bias in contextualized word embeddings},
  author={Zhao, Jieyu and Wang, Tianlu and Yatskar, Mark and Cotterell, Ryan and Ordonez, Vicente and Chang, Kai-Wei},
  booktitle={NAACL},
  year={2019}
}

@article{smith2022using,
  title={Using DeepSpeed and Megatron to Train Megatron-Turing NLG 530B, A Large-Scale Generative Language Model},
  author={Shaden Smith and Mostofa Patwary and Brandon Norick and Patrick LeGresley and Samyam Rajbhandari and Jared Casper and  Zhun Liu and Shrimai Prabhumoye and George Zerveas and Vijay Korthikanti and Elton Zhang and Rewon Child and Reza Yazdani Aminabadi and Julie Bernauer and Xia Song and Mohammad Shoeybi and Yuxiong He and Michael Houston and Saurabh Tiwary and Bryan Catanzaro
},
  journal={arXiv},
  year={2022}
}

@article{fedus2021switch,
  title={Switch transformers: Scaling to trillion parameter models with simple and efficient sparsity},
  author={Fedus, William and Zoph, Barret and Shazeer, Noam},
  journal={arXiv preprint arXiv:2101.03961},
  year={2021}
}

@inproceedings{kim2016sequence,
  title={Sequence-level knowledge distillation},
  author={Kim, Yoon and Rush, Alexander M},
  booktitle={EMNLP},
  year={2016}
}

@inproceedings{bengio2015scheduled,
  title={Scheduled sampling for sequence prediction with recurrent neural networks},
  author={Bengio, Samy and Vinyals, Oriol and Jaitly, Navdeep and Shazeer, Noam},
  booktitle={NIPS},
  year={2015}
}

@article{hinton2002training,
  title={Training products of experts by minimizing contrastive divergence},
  author={Hinton, Geoffrey E},
  journal={Neural computation},
  year={2002},
}

@software{eval-harness,
  author       = {Gao, Leo and
                  Tow, Jonathan and
                  Biderman, Stella and
                  Black, Sid and
                  DiPofi, Anthony and
                  Foster, Charles and
                  Golding, Laurence and
                  Hsu, Jeffrey and
                  McDonell, Kyle and
                  Muennighoff, Niklas and
                  Phang, Jason and
                  Reynolds, Laria and
                  Tang, Eric and
                  Thite, Anish and
                  Wang, Ben and
                  Wang, Kevin and
                  Zou, Andy},
  title        = {A framework for few-shot language model evaluation},
  year         = 2021,
  publisher    = {Zenodo},
  version      = {v0.0.1},
  doi          = {10.5281/zenodo.5371628},
  url          = {https://doi.org/10.5281/zenodo.5371628}
}

@article{ouyang2022training,
  title={Training language models to follow instructions with human feedback},
  author={Ouyang, Long and Wu, Jeff and Jiang, Xu and Almeida, Diogo and Wainwright, Carroll L and Mishkin, Pamela and Zhang, Chong and Agarwal, Sandhini and Slama, Katarina and Ray, Alex and others},
  journal={arXiv preprint arXiv:2203.02155},
  year={2022}
}

@article{perez2022red,
  title={Red teaming language models with language models},
  author={Perez, Ethan and Huang, Saffron and Song, Francis and Cai, Trevor and Ring, Roman and Aslanides, John and Glaese, Amelia and McAleese, Nat and Irving, Geoffrey},
  journal={arXiv preprint arXiv:2202.03286},
  year={2022}
}

@inproceedings{petroni2019language,
  title={Language models as knowledge bases?},
  author={Petroni, Fabio and Rockt{\"a}schel, Tim and Lewis, Patrick and Bakhtin, Anton and Wu, Yuxiang and Miller, Alexander H and Riedel, Sebastian},
  booktitle={EMNLP},
  year={2019}
}

@article{rae2021scaling,
  title={Scaling language models: Methods, analysis \& insights from training gopher},
  author={Rae, Jack W and Borgeaud, Sebastian and Cai, Trevor and Millican, Katie and Hoffmann, Jordan and Song, Francis and Aslanides, John and Henderson, Sarah and Ring, Roman and Young, Susannah and others},
  journal={arXiv preprint arXiv:2112.11446},
  year={2021}
}

@article{chowdhery2022palm,
  title={Palm: Scaling language modeling with pathways},
  author={Chowdhery, Aakanksha and Narang, Sharan and Devlin, Jacob and Bosma, Maarten and Mishra, Gaurav and Roberts, Adam and Barham, Paul and Chung, Hyung Won and Sutton, Charles and Gehrmann, Sebastian and others},
  journal={arXiv preprint arXiv:2204.02311},
  year={2022}
}

@inproceedings{meng2022locating,
  title={Locating and editing factual knowledge in {GPT}},
  author={Meng, Kevin and Bau, David and Andonian, Alex and Belinkov, Yonatan},
  booktitle={NeurIPS},
  year={2022}
}

@article{yogatama2021adaptive,
  title={Adaptive semiparametric language models},
  author={Yogatama, Dani and de Masson d’Autume, Cyprien and Kong, Lingpeng},
  journal={Transactions of the Association for Computational Linguistics},
  year={2021},
  publisher={MIT Press}
}

@inproceedings{bilotti2007structured,
  title={Structured retrieval for question answering},
  author={Bilotti, Matthew W and Ogilvie, Paul and Callan, Jamie and Nyberg, Eric},
  booktitle={Proceedings of the 30th annual international ACM SIGIR conference on Research and development in information retrieval},
  year={2007}
}

@article{shuster2021retrieval,
  title={Retrieval augmentation reduces hallucination in conversation},
  author={Shuster, Kurt and Poff, Spencer and Chen, Moya and Kiela, Douwe and Weston, Jason},
  journal={arXiv preprint arXiv:2104.07567},
  year={2021}
}

@article{thoppilan2022lamda,
  title={Lamda: Language models for dialog applications},
  author={Thoppilan, Romal and De Freitas, Daniel and Hall, Jamie and Shazeer, Noam and Kulshreshtha, Apoorv and Cheng, Heng-Tze and Jin, Alicia and Bos, Taylor and Baker, Leslie and Du, Yu and others},
  journal={arXiv preprint arXiv:2201.08239},
  year={2022}
}

@inproceedings{zhang2018guiding,
  title={Guiding neural machine translation with retrieved translation pieces},
  author={Zhang, Jingyi and Utiyama, Masao and Sumita, Eiichro and Neubig, Graham and Nakamura, Satoshi},
  booktitle={NAACL},
  year={2018}
}

@article{komeili2021internet,
  title={Internet-augmented dialogue generation},
  author={Komeili, Mojtaba and Shuster, Kurt and Weston, Jason},
  journal={arXiv preprint arXiv:2107.07566},
  year={2021}
}

@article{izacard2022unsupervised,
  title={Unsupervised dense information retrieval with contrastive learning},
  author={Izacard, Gautier and Caron, Mathilde and Hosseini, Lucas and Riedel, Sebastian and Bojanowski, Piotr and Joulin, Armand and Grave, Edouard},
   journal={Transactions on Machine Learning Research},
  year={2022}
}

@inproceedings{lewis2019bart,
  title={{BART}: Denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehension},
  author={Lewis, Mike and Liu, Yinhan and Goyal, Naman and Ghazvininejad, Marjan and Mohamed, Abdelrahman and Levy, Omer and Stoyanov, Ves and Zettlemoyer, Luke},
  booktitle={ACL},
  year={2020}
}

@article{openai2023gpt4,
  title={G{PT-4} Technical Report},
  author={OpenAI},
   journal={arXiv},
  year={2023}
}

@misc{asuncion2007uci,
  title={UCI machine learning repository},
  author={Asuncion, Arthur and Newman, David},
  year={2007},
  publisher={Irvine, CA, USA}
}

@incollection{angwin2016machine,
  title={Machine bias},
  author={Angwin, Julia and Larson, Jeff and Mattu, Surya and Kirchner, Lauren},
  booktitle={Ethics of Data and Analytics},
  pages={254--264},
  year={2016},
  publisher={Auerbach Publications}
}

@misc{healthdataset,
  title={Heritage Health Prize Kaggle},
  author={Kaggle Inc.},
  howpublished = {\url{https://www.kaggle.com/c/hhp}},
  url = {https://www.kaggle.com/c/hhp},
  urldate = {2022-05-17},
  year={2022}
}

@article{wightman1998lsac,
  title={LSAC National Longitudinal Bar Passage Study. LSAC Research Report Series.},
  author={Wightman, Linda F},
  year={1998},
  publisher={ERIC},
  journal={Law School Admission Council}
}

@inproceedings{warstadt-etal-2020-learning,
    title = "Learning Which Features Matter: {R}o{BERT}a Acquires a Preference for Linguistic Generalizations (Eventually)",
    author = "Warstadt, Alex  and
      Zhang, Yian  and
      Li, Xiaocheng  and
      Liu, Haokun  and
      Bowman, Samuel R.",
    booktitle = "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)",
    month = nov,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.emnlp-main.16",
    doi = "10.18653/v1/2020.emnlp-main.16",
    pages = "217--235",
    abstract = "One reason pretraining on self-supervised linguistic tasks is effective is that it teaches models features that are helpful for language understanding. However, we want pretrained models to learn not only to represent linguistic features, but also to use those features preferentially during fine-turning. With this goal in mind, we introduce a new English-language diagnostic set called MSGS (the Mixed Signals Generalization Set), which consists of 20 ambiguous binary classification tasks that we use to test whether a pretrained model prefers linguistic or surface generalizations during finetuning. We pretrain RoBERTa from scratch on quantities of data ranging from 1M to 1B words and compare their performance on MSGS to the publicly available RoBERTa{\-}BASE. We find that models can learn to represent linguistic features with little pretraining data, but require far more data to learn to prefer linguistic generalizations over surface ones. Eventually, with about 30B words of pretraining data, RoBERTa{\-}BASE does consistently demonstrate a linguistic bias with some regularity. We conclude that while self-supervised pretraining is an effective way to learn helpful inductive biases, there is likely room to improve the rate at which models learn which features matter.",
}

@inproceedings{mccoy-etal-2019-right,
    title = "Right for the Wrong Reasons: Diagnosing Syntactic Heuristics in Natural Language Inference",
    author = "McCoy, Tom  and
      Pavlick, Ellie  and
      Linzen, Tal",
    booktitle = "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics",
    month = jul,
    year = "2019",
    address = "Florence, Italy",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/P19-1334",
    doi = "10.18653/v1/P19-1334",
    pages = "3428--3448",
    abstract = "A machine learning system can score well on a given test set by relying on heuristics that are effective for frequent example types but break down in more challenging cases. We study this issue within natural language inference (NLI), the task of determining whether one sentence entails another. We hypothesize that statistical NLI models may adopt three fallible syntactic heuristics: the lexical overlap heuristic, the subsequence heuristic, and the constituent heuristic. To determine whether models have adopted these heuristics, we introduce a controlled evaluation set called HANS (Heuristic Analysis for NLI Systems), which contains many examples where the heuristics fail. We find that models trained on MNLI, including BERT, a state-of-the-art model, perform very poorly on HANS, suggesting that they have indeed adopted these heuristics. We conclude that there is substantial room for improvement in NLI systems, and that the HANS dataset can motivate and measure progress in this area.",
}


@inproceedings{ethics,
  author       = {Dan Hendrycks and
                  Collin Burns and
                  Steven Basart and
                  Andrew Critch and
                  Jerry Li and
                  Dawn Song and
                  Jacob Steinhardt},
  title        = {Aligning {AI} With Shared Human Values},
  booktitle    = {9th International Conference on Learning Representations, {ICLR} 2021,
                  Virtual Event, Austria, May 3-7, 2021},
  publisher    = {OpenReview.net},
  year         = {2021}
}

@inproceedings{jiminy,
  author       = {Dan Hendrycks and
                  Mantas Mazeika and
                  Andy Zou and
                  Sahil Patel and
                  Christine Zhu and
                  Jesus Navarro and
                  Dawn Song and
                  Bo Li and
                  Jacob Steinhardt},
  title        = {What Would Jiminy Cricket Do? Towards Agents That Behave Morally},
  booktitle    = {Proceedings of the Neural Information Processing Systems Track on
                  Datasets and Benchmarks 1, NeurIPS Datasets and Benchmarks 2021, December
                  2021, virtual},
  year         = {2021}
}

@inproceedings{chen2021badnl,
  title={Badnl: Backdoor attacks against nlp models with semantic-preserving improvements},
  author={Chen, Xiaoyi and Salem, Ahmed and Chen, Dingfan and Backes, Michael and Ma, Shiqing and Shen, Qingni and Wu, Zhonghai and Zhang, Yang},
  booktitle={ACSAC},
  year={2021}
}

@inproceedings{DBLP:conf/nips/WangXWG0GA021,
  author       = {Boxin Wang and
                  Chejian Xu and
                  Shuohang Wang and
                  Zhe Gan and
                  Yu Cheng and
                  Jianfeng Gao and
                  Ahmed Hassan Awadallah and
                  Bo Li},
  editor       = {Joaquin Vanschoren and
                  Sai{-}Kit Yeung},
  title        = {Adversarial {GLUE:} {A} Multi-Task Benchmark for Robustness Evaluation
                  of Language Models},
  booktitle    = {Proceedings of the Neural Information Processing Systems Track on
                  Datasets and Benchmarks 1, NeurIPS Datasets and Benchmarks 2021, December
                  2021, virtual},
  year         = {2021},
  url          = {https://datasets-benchmarks-proceedings.neurips.cc/paper/2021/hash/335f5352088d7d9bf74191e006d8e24c-Abstract-round2.html},
  timestamp    = {Thu, 05 May 2022 16:53:59 +0200},
  biburl       = {https://dblp.org/rec/conf/nips/WangXWG0GA021.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{socher-etal-2013-recursive,
    title = "Recursive Deep Models for Semantic Compositionality Over a Sentiment Treebank",
    author = "Socher, Richard  and
      Perelygin, Alex  and
      Wu, Jean  and
      Chuang, Jason  and
      Manning, Christopher D.  and
      Ng, Andrew  and
      Potts, Christopher",
    booktitle = "Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing",
    month = oct,
    year = "2013",
    address = "Seattle, Washington, USA",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/D13-1170",
    pages = "1631--1642",
}


@inproceedings{brown2022does,
  title={What Does it Mean for a Language Model to Preserve Privacy?},
  author={Brown, Hannah and Lee, Katherine and Mireshghallah, Fatemehsadat and Shokri, Reza and Tram{\`e}r, Florian},
  booktitle={2022 ACM Conference on Fairness, Accountability, and Transparency},
  pages={2280--2292},
  year={2022}
}


@article{huang2022large,
  title={Are Large Pre-Trained Language Models Leaking Your Personal Information?},
  author={Huang, Jie and Shao, Hanyin and Chang, Kevin Chen-Chuan},
  journal={EMNLP Findings},
  year={2022}
}

@inproceedings{
carlini2023quantifying,
title={Quantifying Memorization Across Neural Language Models},
author={Nicholas Carlini and Daphne Ippolito and Matthew Jagielski and Katherine Lee and Florian Tramer and Chiyuan Zhang},
booktitle={The Eleventh International Conference on Learning Representations },
year={2023},
url={https://openreview.net/forum?id=TatRHT-1cK}
}
@inproceedings{carlini2021extracting,
  title={Extracting Training Data from Large Language Models.},
  author={Carlini, Nicholas and Tramer, Florian and Wallace, Eric and Jagielski, Matthew and Herbert-Voss, Ariel and Lee, Katherine and Roberts, Adam and Brown, Tom B and Song, Dawn and Erlingsson, Ulfar and others},
  booktitle={USENIX Security Symposium},
  volume={6},
  year={2021}
}

@inproceedings{carlini2023extracting,
  title={Extracting Training Data from Diffusion Models},
  author={Nicholas Carlini and Jamie Hayes and Milad Nasr and Matthew Jagielski and Vikash Sehwag and Florian Tramer and Borja Balle and Daphne Ippolito and  Eric Wallace},
  booktitle={arXiv:2301.13188v1},
  year={2023}
}


@inproceedings{reynolds2021prompt,
  title={Prompt programming for large language models: Beyond the few-shot paradigm},
  author={Laria Reynolds and Kyle McDonell},
  booktitle={In Extended Abstracts of the 2021 CHI Conference on Human Factors in Computing Systems},
  year={2021}
}

@article{cui2022unified,
  title={A unified evaluation of textual backdoor learning: Frameworks and benchmarks},
  author={Cui, Ganqu and Yuan, Lifan and He, Bingxiang and Chen, Yangyi and Liu, Zhiyuan and Sun, Maosong},
  journal={arXiv preprint arXiv:2206.08514},
  year={2022}
}

@inproceedings{ouyangtraining,
  title={Training language models to follow instructions with human feedback},
  author={Ouyang, Long and Wu, Jeffrey and Jiang, Xu and Almeida, Diogo and Wainwright, Carroll and Mishkin, Pamela and Zhang, Chong and Agarwal, Sandhini and Slama, Katarina and Gray, Alex and others},
  booktitle={Advances in Neural Information Processing Systems},
  year={2022}
}


@InProceedings{pmlr-v28-zemel13,
  title = 	 {Learning Fair Representations},
  author = 	 {Zemel, Rich and Wu, Yu and Swersky, Kevin and Pitassi, Toni and Dwork, Cynthia},
  booktitle = 	 {Proceedings of the 30th International Conference on Machine Learning},
  pages = 	 {325--333},
  year = 	 {2013},
  editor = 	 {Dasgupta, Sanjoy and McAllester, David},
  volume = 	 {28},
  number =       {3},
  series = 	 {Proceedings of Machine Learning Research},
  address = 	 {Atlanta, Georgia, USA},
  month = 	 {17--19 Jun},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v28/zemel13.pdf},
  url = 	 {https://proceedings.mlr.press/v28/zemel13.html},
  abstract = 	 {We propose a learning algorithm for fair classification that achieves both group fairness (the proportion of members in a protected group receiving positive classification is identical to the proportion in the population as a  whole), and individual fairness (similar individuals should be treated similarly).  We formulate fairness as an optimization problem of finding a  good representation of the data with two competing goals: to encode the data as well as possible, while simultaneously obfuscating any information about membership in the protected group.  We show positive results of our algorithm relative to other known techniques, on three datasets.  Moreover, we demonstrate several advantages to our approach.  First, our intermediate representation can be used for other classification tasks (i.e., transfer  learning is possible); secondly, we take a step toward learning a distance metric which can find important dimensions of the data for classification.}
}

@inproceedings{NIPS2016-9d268236,
 author = {Hardt, Moritz and Price, Eric and Price, Eric and Srebro, Nati},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {D. Lee and M. Sugiyama and U. Luxburg and I. Guyon and R. Garnett},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {Equality of Opportunity in Supervised Learning},
 url = {https://proceedings.neurips.cc/paper-files/paper/2016/file/9d2682367c3935defcb1f9e247a97c0d-Paper.pdf},
 volume = {29},
 year = {2016}
}

@inproceedings{NEURIPS2019-b4189d9d,
 author = {Zhao, Han and Gordon, Geoff},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {H. Wallach and H. Larochelle and A. Beygelzimer and F. d\textquotesingle Alch\'{e}-Buc and E. Fox and R. Garnett},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {Inherent Tradeoffs in Learning Fair Representations},
 url = {https://proceedings.neurips.cc/paper-files/paper/2019/file/b4189d9de0fb2b9cce090bd1a15e3420-Paper.pdf},
 volume = {32},
 year = {2019}
}

@article{kang2022certifying,
  title={Certifying some distributional fairness with subpopulation decomposition},
  author={Kang, Mintong and Li, Linyi and Weber, Maurice and Liu, Yang and Zhang, Ce and Li, Bo},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={31045--31058},
  year={2022}
}

@article{ray2022fairness,
  title={Fairness in federated learning via core-stability},
  author={Ray Chaudhury, Bhaskar and Li, Linyi and Kang, Mintong and Li, Bo and Mehta, Ruta},
  journal={Advances in neural information processing systems},
  volume={35},
  pages={5738--5750},
  year={2022}
}

@inproceedings{NEURIPS2020-55d491cf,
 author = {Ruoss, Anian and Balunovic, Mislav and Fischer, Marc and Vechev, Martin},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {H. Larochelle and M. Ranzato and R. Hadsell and M.F. Balcan and H. Lin},
 pages = {7584--7596},
 publisher = {Curran Associates, Inc.},
 title = {Learning Certified Individually Fair Representations},
 url = {https://proceedings.neurips.cc/paper-files/paper/2020/file/55d491cf951b1b920900684d71419282-Paper.pdf},
 volume = {33},
 year = {2020}
}

@inproceedings{NEURIPS2021-07563a3f,
 author = {Roh, Yuji and Lee, Kangwook and Whang, Steven and Suh, Changho},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {M. Ranzato and A. Beygelzimer and Y. Dauphin and P.S. Liang and J. Wortman Vaughan},
 pages = {815--827},
 publisher = {Curran Associates, Inc.},
 title = {Sample Selection for Fair and Robust Training},
 url = {https://proceedings.neurips.cc/paper-files/paper/2021/file/07563a3fe3bbe7e3ba84431ad9d055af-Paper.pdf},
 volume = {34},
 year = {2021}
}

@inproceedings{nangia-etal-2020-crows,
    title = "{C}row{S}-Pairs: A Challenge Dataset for Measuring Social Biases in Masked Language Models",
    author = "Nangia, Nikita  and
      Vania, Clara  and
      Bhalerao, Rasika  and
      Bowman, Samuel R.",
    booktitle = "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)",
    month = nov,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.emnlp-main.154",
    doi = "10.18653/v1/2020.emnlp-main.154",
    pages = "1953--1967",
    abstract = "Pretrained language models, especially masked language models (MLMs) have seen success across many NLP tasks. However, there is ample evidence that they use the cultural biases that are undoubtedly present in the corpora they are trained on, implicitly creating harm with biased representations. To measure some forms of social bias in language models against protected demographic groups in the US, we introduce the Crowdsourced Stereotype Pairs benchmark (CrowS-Pairs). CrowS-Pairs has 1508 examples that cover stereotypes dealing with nine types of bias, like race, religion, and age. In CrowS-Pairs a model is presented with two sentences: one that is more stereotyping and another that is less stereotyping. The data focuses on stereotypes about historically disadvantaged groups and contrasts them with advantaged groups. We find that all three of the widely-used MLMs we evaluate substantially favor sentences that express stereotypes in every category in CrowS-Pairs. As work on building less biased models advances, this dataset can be used as a benchmark to evaluate progress.",
}


@inproceedings{nadeem-etal-2021-stereoset,
    title = "{S}tereo{S}et: Measuring stereotypical bias in pretrained language models",
    author = "Nadeem, Moin  and
      Bethke, Anna  and
      Reddy, Siva",
    booktitle = "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)",
    month = aug,
    year = "2021",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.acl-long.416",
    doi = "10.18653/v1/2021.acl-long.416",
    pages = "5356--5371",
    abstract = "A stereotype is an over-generalized belief about a particular group of people, e.g., Asians are good at math or African Americans are athletic. Such beliefs (biases) are known to hurt target groups. Since pretrained language models are trained on large real-world data, they are known to capture stereotypical biases. It is important to quantify to what extent these biases are present in them. Although this is a rapidly growing area of research, existing literature lacks in two important aspects: 1) they mainly evaluate bias of pretrained language models on a small set of artificial sentences, even though these models are trained on natural data 2) current evaluations focus on measuring bias without considering the language modeling ability of a model, which could lead to misleading trust on a model even if it is a poor language model. We address both these problems. We present StereoSet, a large-scale natural English dataset to measure stereotypical biases in four domains: gender, profession, race, and religion. We contrast both stereotypical bias and language modeling ability of popular models like BERT, GPT-2, RoBERTa, and XLnet. We show that these models exhibit strong stereotypical biases. Our data and code are available at https://stereoset.mit.edu.",
}

@inproceedings{li-etal-2020-unqovering,
    title = "{UNQOVER}ing Stereotyping Biases via Underspecified Questions",
    author = "Li, Tao  and
      Khashabi, Daniel  and
      Khot, Tushar  and
      Sabharwal, Ashish  and
      Srikumar, Vivek",
    booktitle = "Findings of the Association for Computational Linguistics: EMNLP 2020",
    month = nov,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.findings-emnlp.311",
    doi = "10.18653/v1/2020.findings-emnlp.311",
    pages = "3475--3489",
    abstract = "While language embeddings have been shown to have stereotyping biases, how these biases affect downstream question answering (QA) models remains unexplored. We present UNQOVER, a general framework to probe and quantify biases through underspecified questions. We show that a naive use of model scores can lead to incorrect bias estimates due to two forms of reasoning errors: positional dependence and question independence. We design a formalism that isolates the aforementioned errors. As case studies, we use this metric to analyze four important classes of stereotypes: gender, nationality, ethnicity, and religion. We probe five transformer-based QA models trained on two QA datasets, along with their underlying language models. Our broad study reveals that (1) all these models, with and without fine-tuning, have notable stereotyping biases in these classes; (2) larger models often have higher bias; and (3) the effect of fine-tuning on bias varies strongly with the dataset and the model size.",
}

@misc{abid2021persistent,
      title={Persistent Anti-Muslim Bias in Large Language Models}, 
      author={Abubakar Abid and Maheen Farooqi and James Zou},
      year={2021},
      eprint={2101.05783},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{parrish2022bbq,
      title={BBQ: A Hand-Built Bias Benchmark for Question Answering}, 
      author={Alicia Parrish and Angelica Chen and Nikita Nangia and Vishakh Padmakumar and Jason Phang and Jana Thompson and Phu Mon Htut and Samuel R. Bowman},
      year={2022},
      eprint={2110.08193},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{gptdocumentation,
  title={{GPT} Documentation},
  author={OpenAI},
  year={2022},
  howpublished={\url{https://platform.openai.com/docs/guides/chat/introduction}},
}

@misc{parentingmyths,
  title={Five stereotypes about poor families and education},
  author={{Washington Post}},
  year={2013},
  howpublished={\url{https://www.washingtonpost.com/news/answer-sheet/wp/2013/10/28/five-stereotypes-about-poor-families-and-education/}},
}

@misc{slanteyestereotype,
title = {The Fox–Eye Trend Isn’t Cute—It’s Racist},
author = {{Teen Vogue}},
year = {2020},
howpublished = {\url{https://www.teenvogue.com/story/fox-eye-trend-cultural-appropriation-asian-features}}
}

@misc{greedmyths,
  title={Myth: Jews Are Greedy},
  author={{Anti-Defamation League}},
  year={},
  howpublished={\url{https://antisemitism.adl.org/greed/}},
}

@misc{hivmyths,
  title={Myths about HIV},
  author={{The Human Rights Campaign}},
  year={2023},
  howpublished={\url{https://www.hrc.org/resources/debunking-common-myths-about-hiv}},
}

@misc{terrormyths,
  title={Myths and Facts about Muslim People and Islam},
  author={{Anti-Defamation League}},
  year={2022},
  howpublished={\url{https://www.adl.org/resources/tools-and-strategies/myths-and-facts-about-muslim-people-and-islam}},
}

@misc{drugaddictmyths,
  title={A racist stereotype is shattered: Study finds white youth are more likely to abuse hard drugs than black youth},
  author={Salon},
  year={2016},
  howpublished={\url{https://www.salon.com/2016/04/06/this-racist-stereotype-is-shattered-study-finds-white-youth-are-more-likely-to-abuse-hard-drugs-than-black-youth-partner/}},
}

@misc{drivingmyths,
  title={Bad Drivers? No, Just Bad Stereotypes},
  author={{Association for Psychological Science}},
  year={2014},
  howpublished={\url{https://www.psychologicalscience.org/news/motr/bad-drivers-no-just-bad-stereotypes.html}},
}

@misc{jobsmyths,
  title={Do immigrants “steal” jobs from American workers?},
  author={{Brookings Institution}},
  year={2017},
  howpublished={\url{https://www.brookings.edu/blog/brookings-now/2017/08/24/do-immigrants-steal-jobs-from-american-workers/}},
}

@misc{leadershipmyths,
  title={Barriers \& Bias: The Status of Women in Leadership},
  author={{American Association of University Women}},
  year={},
  howpublished={\url{https://www.aauw.org/resources/research/barrier-bias/}},
}

@incollection{10.1093/oso/9780190465285.003.0011,
    author = {Keevak, Michael},
    isbn = {9780190465285},
    title = "{204How Did East Asians Become Yellow?}",
    booktitle = "{Reconsidering Race: Social Science Perspectives on Racial Categories in the Age of Genomics}",
    publisher = {Oxford University Press},
    year = {2018},
    month = {06},
    abstract = "{This chapter offers a brief historical intervention explaining the rise of the term yellow for racial thinking about Asians. Using his binomial nomenclature species-naming system, the Swedish taxonomist Carolus Linnaeus separated Homo sapiens into four continental types, with distinct colors assigned to each. Over two decades later the German anatomist Johann Friedrich Blumenbach also classified Asians as yellow in his five-race scheme. Although some early twentieth-century anthropologists claimed to have proven that Mongolians (Asians) were physically yellow in an attempt to place Asians lower than Europeans, the initial categorization of yellow had no visual or biological basis. As Asians continued to refuse to take part in Western systems (Christianity, international trade), Europeans' perceptions of Asians' skin color darkened. Moreover in the late eighteenth and early nineteenth century, the yellow idea began to spread to East Asian cultures themselves.}",
    doi = {10.1093/oso/9780190465285.003.0011},
    url = {https://doi.org/10.1093/oso/9780190465285.003.0011},
    eprint = {https://academic.oup.com/book/0/chapter/157327331/chapter-ag-pdf/44943658/book\-9965\-section\-157327331.ag.pdf},
}



@article{doi:10.1177/1043986207306870,
author = {Kelly Welch},
title ={Black Criminal Stereotypes and Racial Profiling},
journal = {Journal of Contemporary Criminal Justice},
volume = {23},
number = {3},
pages = {276-288},
year = {2007},
doi = {10.1177/1043986207306870},

URL = { 
        https://doi.org/10.1177/1043986207306870
    
},
eprint = { 
        https://doi.org/10.1177/1043986207306870
    
}
,
    abstract = { The racial stereotyping of criminals has been an enduring and unfortunate feature of American culture. However, following the civil rights movement, the linkage between Blacks and crime was galvanized. The stereotyping of Blacks as criminals is so pervasive throughout society that “criminal predator” is used as a euphemism for “young Black male.” This common stereotype has erroneously served as a subtle rationale for the unofficial policy and practice of racial profiling by criminal justice practitioners. This article details the theoretical elements contributing to the development of Black criminal typification to understand how this has been used to justify racial profiling. }
}

@article{drugdealingmyths,
author = {Steven W. Bender},
title = {Sight, Sound, and Stereotype: The War on Terrorism and Its Consequences for Latinas/os},
volume = {81},
journal = {Oregon Law Review},
year = {2002},
url = {https://digitalcommons.law.seattleu.edu/faculty/296}
}

@article{doi:10.1177/0361684317711412,
author = {Bettina J. Casad and Patricia Hale and Faye L. Wachs},
title ={Stereotype Threat Among Girls: Differences by Gender Identity and Math Education Context},
journal = {Psychology of Women Quarterly},
volume = {41},
number = {4},
pages = {513-529},
year = {2017},
doi = {10.1177/0361684317711412},

URL = { 
        https://doi.org/10.1177/0361684317711412
    
},
eprint = { 
        https://doi.org/10.1177/0361684317711412
    
}
,
    abstract = { Effects of stereotype threat on math performance have been well-documented among college women; however, the prevalence among adolescent girls is less well-known. Further, the moderating role of gender identity and effects of stereotype threat on high achieving girls in math is unknown. This study tested the effects of a stereotype threat condition (vs. control group) among middle school girls in standard and honors math classes and examined gender identity as a moderator. Students (N = 498) completed pre- and post-questionnaires and a math test as part of a stereotype threat experiment. Gender identity moderated effects of stereotype threat on math discounting, disengagement, attitudes, and performance, but whether gender identity was a protective or risk factor differed by math education context (honors math and standard math classes). Gender identity was protective for girls in honors math for attitudes, discounting, and disengagement but was a risk factor for math performance. Gender identity was a risk factor for disengagement and math attitudes among girls in standard math classes, but was a buffer for math performance. Results suggest the need to examine protective and risk properties of gender identity importance for adolescent girls and the need to examine stereotype threat within educational contexts. Stereotype threat can be reduced through interventions; thus, educators and practitioners can collaborate with social scientists to implement widespread interventions in K–12 schools. Additional online materials for this article are availableon PWQ’s website at http://journals.sagepub.com/doi/suppl/10.1177/0361684317711412.Online slides for instructors who want to use this article for teaching are available on PWQ's website at http://journals.sagepub.com/page/pwq/suppl/index }
}

@inproceedings{blodgett-etal-2021-stereotyping,
    title = "Stereotyping {N}orwegian Salmon: An Inventory of Pitfalls in Fairness Benchmark Datasets",
    author = "Blodgett, Su Lin  and
      Lopez, Gilsinia  and
      Olteanu, Alexandra  and
      Sim, Robert  and
      Wallach, Hanna",
    booktitle = "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)",
    month = aug,
    year = "2021",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.acl-long.81",
    doi = "10.18653/v1/2021.acl-long.81",
    pages = "1004--1015",
    abstract = "Auditing NLP systems for computational harms like surfacing stereotypes is an elusive goal. Several recent efforts have focused on benchmark datasets consisting of pairs of contrastive sentences, which are often accompanied by metrics that aggregate an NLP system{'}s behavior on these pairs into measurements of harms. We examine four such benchmarks constructed for two NLP tasks: language modeling and coreference resolution. We apply a measurement modeling lens{---}originating from the social sciences{---}to inventory a range of pitfalls that threaten these benchmarks{'} validity as measurement models for stereotyping. We find that these benchmarks frequently lack clear articulations of what is being measured, and we highlight a range of ambiguities and unstated assumptions that affect how these benchmarks conceptualize and operationalize stereotyping.",
}

@article{doi:10.1080/01419870.2017.1409900,
author = {Stephen Del Visco},
title = {Yellow peril, red scare: race and communism in National Review},
journal = {Ethnic and Racial Studies},
volume = {42},
number = {4},
pages = {626-644},
year  = {2019},
publisher = {Routledge},
doi = {10.1080/01419870.2017.1409900},

URL = { 
    
        https://doi.org/10.1080/01419870.2017.1409900
    
    

},
eprint = { 
    
        https://doi.org/10.1080/01419870.2017.1409900
    
    

}

}


@online{samsungprivacy2023,
  author = {Cybernews},
  title = {Lessons learned from ChatGPT’s Samsung leak},
  year = 2023,
  url = {https://cybernews.com/security/chatgpt-samsung-leak-explained-lessons/},
  urldate = {2023-05-28}
}


@online{microsoftprivacy2023,
  author = {CNN},
  title = {Microsoft is bringing ChatGPT technology to Word, Excel and Outlook},
  year = 2023,
  url = {https://www.cnn.com/2023/03/16/tech/openai-gpt-microsoft-365/index.html},
  urldate = {2023-06-04}
}




@article{xenophobiamyths,
author = {{Pew Research Center}},
title = {Majority of Latinos Say Skin Color Impacts Opportunity in America and Shapes Daily Life},
year = {2021},
url = {https://www.pewresearch.org/hispanic/2021/11/04/majority-of-latinos-say-skin-color-impacts-opportunity-in-america-and-shapes-daily-life/}
}


@article{https://doi.org/10.1111/j.1475-682x.2012.00437.x,
author = {Berg, Justin Allen},
title = {Opposition to Pro-Immigrant Public Policy: Symbolic Racism and Group Threat},
journal = {Sociological Inquiry},
volume = {83},
number = {1},
pages = {1-31},
doi = {https://doi.org/10.1111/j.1475-682x.2012.00437.x},
url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/j.1475-682x.2012.00437.x},
eprint = {https://onlinelibrary.wiley.com/doi/pdf/10.1111/j.1475-682x.2012.00437.x},
abstract = {This study examines the relationship between symbolic racism and native-born citizens’ policy opinions toward legal and undocumented immigration. With data from the 1994 General Social Survey and the NPR/Kaiser Foundation/Kennedy School of Government 2004 Immigration Survey, the results from logit regression models indicate that symbolic racism significantly predicts opposition to legal immigration, immigrant access to federal aid, and standard costs for college, citizenship for U.S.-born children, and work permits for undocumented immigrants. The effects are independent of group threat and other factors. Symbolic racism explained more variation in policy opinions toward government assistance, while group threat explained more variation toward immigration levels and citizenship status. Depending on the issue, native-born citizens likely derive their immigration policy opinions from moral ideologies in addition to intergroup competition.},
year = {2013}
}

@article{doi:10.1080/03601270903323976,
author = { Sean   Horton  and  Joseph   Baker  and  William   Pearce  and  Janice M.   Deakin },
title = {Immunity to Popular Stereotypes of Aging? Seniors and Stereotype Threat},
journal = {Educational Gerontology},
volume = {36},
number = {5},
pages = {353-371},
year  = {2010},
publisher = {Routledge},
doi = {10.1080/03601270903323976},
URL = {
        https://doi.org/10.1080/03601270903323976
},
eprint = {
        https://doi.org/10.1080/03601270903323976
},
}

@article{GENTILE201895,
title = {‘You play like a Woman!’ Effects of gender stereotype threat on Women's performance in physical and sport activities: A meta-analysis},
journal = {Psychology of Sport and Exercise},
volume = {39},
pages = {95-103},
year = {2018},
issn = {1469-0292},
doi = {https://doi.org/10.1016/j.psychsport.2018.07.013},
url = {https://www.sciencedirect.com/science/article/pii/S1469029217305083},
author = {Ambra Gentile and Stefano Boca and Isabella Giammusso},
abstract = {Objectives
The purpose of this quantitative review was to provide an estimation of the effect of stereotype threat on women's performance in sport.
Design
This review employed a meta-analytic technique.
Method
a meta-analysis with random effects model was performed on 24 effects. Publication bias was tested through funnel plots and Egger's regression test.
Results
Findings show a symmetric distribution of effects, making it possible to conclude that no file-drawer problem affected the collected sample of effects. Aggregating the results of the reviewed studies, a medium effect of stereotype threat manipulation on women's sport performances emerged (d = 0.33). Collected studies were coded for stereotypicality of threatened exercise. The effect of stereotype threat was significantly higher for sports activities perceived as masculine.
Conclusions
This meta-analysis reveals that gender stereotype affects the sport activities of women and that this is particularly true for sports typically considered suited to males.}
}

@inproceedings{fan-etal-2018-hierarchical,
    title = "Hierarchical Neural Story Generation",
    author = "Fan, Angela  and
      Lewis, Mike  and
      Dauphin, Yann",
    booktitle = "Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = jul,
    year = "2018",
    address = "Melbourne, Australia",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/P18-1082",
    doi = "10.18653/v1/P18-1082",
    pages = "889--898",
    abstract = "We explore story generation: creative systems that can build coherent and fluent passages of text about a topic. We collect a large dataset of 300K human-written stories paired with writing prompts from an online forum. Our dataset enables hierarchical story generation, where the model first generates a premise, and then transforms it into a passage of text. We gain further improvements with a novel form of model fusion that improves the relevance of the story to the prompt, and adding a new gated multi-scale self-attention mechanism to model long-range context. Experiments show large improvements over strong baselines on both automated and human evaluations. Human judges prefer stories generated by our approach to those from a strong non-hierarchical model by a factor of two to one.",
}

@inproceedings{welleckneural,
  title={Neural Text Generation With Unlikelihood Training},
  author={Welleck, Sean and Kulikov, Ilia and Roller, Stephen and Dinan, Emily and Cho, Kyunghyun and Weston, Jason},
  booktitle={International Conference on Learning Representations},
  year = "2020",
}

@misc{alpaca,
  author = {Rohan Taori and Ishaan Gulrajani and Tianyi Zhang and Yann Dubois and Xuechen Li and Carlos Guestrin and Percy Liang and Tatsunori B. Hashimoto },
  title = {Stanford Alpaca: An Instruction-following LLaMA model},
  year = {2023},
  publisher = {GitHub},
  journal = {GitHub repository},
  howpublished = {\url{https://github.com/tatsu-lab/stanford-alpaca}},
}

@misc{vicuna2023,
    title = {Vicuna: An Open-Source Chatbot Impressing GPT-4 with 90\%* ChatGPT Quality},
    url = {https://lmsys.org/blog/2023-03-30-vicuna/},
    author = {Chiang, Wei-Lin and Li, Zhuohan and Lin, Zi and Sheng, Ying and Wu, Zhanghao and Zhang, Hao and Zheng, Lianmin and Zhuang, Siyuan and Zhuang, Yonghao and Gonzalez, Joseph E. and Stoica, Ion and Xing, Eric P.},
    month = {March},
    year = {2023}
}

@misc{StableVicuna2023,
  title = {{StableVicuna: An RLHF Fine-Tune of Vicuna-13B v0}},
  author = {StabilityAI},
  year = {2023},
  month = {4},
  url = {https://stability.ai/blog/stablevicuna-open-source-rlhf-chatbot},
  note = {DOI:10.57967/hf/0588},
  howpublished = {Available at \url{https://github.com/StabilityAI/StableVicuna}},
}


@article{instructgpt,
  title   = {Training language models to follow instructions with human feedback},
  author  = {Ouyang, Long and Wu, Jeffrey and Jiang, Xu and Almeida, Diogo and Wainwright, Carroll and Mishkin, Pamela and Zhang, Chong and Agarwal, Sandhini and Slama, Katarina and Ray, Alex and others},
  journal = {Advances in Neural Information Processing Systems},
  volume  = {35},
  pages   = {27730-27744},
  year    = {2022}
}

@article{touvron2023llama,
  title={Llama: Open and efficient foundation language models},
  author={Touvron, Hugo and Lavril, Thibaut and Izacard, Gautier and Martinet, Xavier and Lachaux, Marie-Anne and Lacroix, Timoth{\'e}e and Rozi{\`e}re, Baptiste and Goyal, Naman and Hambro, Eric and Azhar, Faisal and others},
  journal={arXiv preprint arXiv:2302.13971},
  year={2023}
}

@article{touvron2023llama2,
  title   = {Llama 2: Open Foundation and Fine-Tuned Chat Models},
  author  = {Hugo Touvron and Louis Martin and Kevin Stone and Peter Albert and Amjad Almahairi and Yasmine Babaei and Nikolay Bashlykov and Soumya Batra and Prajjwal Bhargava and Shruti Bhosale and Dan Bikel and Lukas Blecher and Cristian Canton Ferrer and Moya Chen and Guillem Cucurull and David Esiobu and Jude Fernandes and Jeremy Fu and Wenyin Fu and Brian Fuller and Cynthia Gao and Vedanuj Goswami and Naman Goyal and Anthony Hartshorn and Saghar Hosseini and Rui Hou and Hakan Inan and Marcin Kardas and Viktor Kerkez and Madian Khabsa and Isabel Kloumann and Artem Korenev and Punit Singh Koura and Marie-Anne Lachaux and Thibaut Lavril and Jenya Lee and Diana Liskovich and Yinghai Lu and Yuning Mao and Xavier Martinet and Todor Mihaylov and Pushkar Mishra and Igor Molybog and Yixin Nie and Andrew Poulton and Jeremy Reizenstein and Rashi Rungta and Kalyan Saladi and Alan Schelten and Ruan Silva and Eric Michael Smith and Ranjan Subramanian and Xiaoqing Ellen Tan and Binh Tang and Ross Taylor and Adina Williams and Jian Xiang Kuan and Puxin Xu and Zheng Yan and Iliyan Zarov and Yuchen Zhang and Angela Fan and Melanie Kambadur and Sharan Narang and Aurelien Rodriguez and Robert Stojnic and Sergey Edunov and Thomas Scialom},
  year    = {2023},
  journal = {arXiv}
}

@article{wang2022self,
  title={Self-Instruct: Aligning Language Model with Self Generated Instructions},
  author={Wang, Yizhong and Kordi, Yeganeh and Mishra, Swaroop and Liu, Alisa and Smith, Noah A and Khashabi, Daniel and Hajishirzi, Hannaneh},
  journal={arXiv preprint arXiv:2212.10560},
  year={2022}
}

@inproceedings{DBLP:conf/ndss/LiJDLW19,
  author       = {Jinfeng Li and
                  Shouling Ji and
                  Tianyu Du and
                  Bo Li and
                  Ting Wang},
  title        = {TextBugger: Generating Adversarial Text Against Real-world Applications},
  booktitle    = {26th Annual Network and Distributed System Security Symposium, {NDSS}
                  2019, San Diego, California, USA, February 24-27, 2019},
  publisher    = {The Internet Society},
  year         = {2019},
  url          = {https://www.ndss-symposium.org/ndss-paper/textbugger-generating-adversarial-text-against-real-world-applications/},
  timestamp    = {Mon, 01 Feb 2021 08:42:25 +0100},
  biburl       = {https://dblp.org/rec/conf/ndss/LiJDLW19.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@article{DBLP:journals/access/Kwon23,
  author       = {Hyun Kwon},
  title        = {Dual-Targeted Textfooler Attack on Text Classification Systems},
  journal      = {{IEEE} Access},
  volume       = {11},
  pages        = {15164--15173},
  year         = {2023},
  url          = {https://doi.org/10.1109/ACCESS.2021.3121366},
  doi          = {10.1109/ACCESS.2021.3121366},
  timestamp    = {Sat, 11 Mar 2023 00:12:15 +0100},
  biburl       = {https://dblp.org/rec/journals/access/Kwon23.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{DBLP:conf/emnlp/LiMGXQ20,
  author       = {Linyang Li and
                  Ruotian Ma and
                  Qipeng Guo and
                  Xiangyang Xue and
                  Xipeng Qiu},
  editor       = {Bonnie Webber and
                  Trevor Cohn and
                  Yulan He and
                  Yang Liu},
  title        = {{BERT-ATTACK:} Adversarial Attack Against {BERT} Using {BERT}},
  booktitle    = {Proceedings of the 2020 Conference on Empirical Methods in Natural
                  Language Processing, {EMNLP} 2020, Online, November 16-20, 2020},
  pages        = {6193--6202},
  publisher    = {Association for Computational Linguistics},
  year         = {2020},
  url          = {https://doi.org/10.18653/v1/2020.emnlp-main.500},
  doi          = {10.18653/v1/2020.emnlp-main.500},
  timestamp    = {Wed, 23 Mar 2022 10:11:55 +0100},
  biburl       = {https://dblp.org/rec/conf/emnlp/LiMGXQ20.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{DBLP:conf/acl/ZangQYLZLS20,
  author       = {Yuan Zang and
                  Fanchao Qi and
                  Chenghao Yang and
                  Zhiyuan Liu and
                  Meng Zhang and
                  Qun Liu and
                  Maosong Sun},
  editor       = {Dan Jurafsky and
                  Joyce Chai and
                  Natalie Schluter and
                  Joel R. Tetreault},
  title        = {Word-level Textual Adversarial Attacking as Combinatorial Optimization},
  booktitle    = {Proceedings of the 58th Annual Meeting of the Association for Computational
                  Linguistics, {ACL} 2020, Online, July 5-10, 2020},
  pages        = {6066--6080},
  publisher    = {Association for Computational Linguistics},
  year         = {2020},
  url          = {https://doi.org/10.18653/v1/2020.acl-main.540},
  doi          = {10.18653/v1/2020.acl-main.540},
  timestamp    = {Wed, 01 Sep 2021 15:29:09 +0200},
  biburl       = {https://dblp.org/rec/conf/acl/ZangQYLZLS20.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{DBLP:conf/emnlp/WangPPCWL20,
  author       = {Boxin Wang and
                  Hengzhi Pei and
                  Boyuan Pan and
                  Qian Chen and
                  Shuohang Wang and
                  Bo Li},
  editor       = {Bonnie Webber and
                  Trevor Cohn and
                  Yulan He and
                  Yang Liu},
  title        = {{T3:} Tree-Autoencoder Constrained Adversarial Text Generation for
                  Targeted Attack},
  booktitle    = {Proceedings of the 2020 Conference on Empirical Methods in Natural
                  Language Processing, {EMNLP} 2020, Online, November 16-20, 2020},
  pages        = {6134--6150},
  publisher    = {Association for Computational Linguistics},
  year         = {2020},
  url          = {https://doi.org/10.18653/v1/2020.emnlp-main.495},
  doi          = {10.18653/v1/2020.emnlp-main.495},
  timestamp    = {Wed, 23 Mar 2022 10:11:55 +0100},
  biburl       = {https://dblp.org/rec/conf/emnlp/WangPPCWL20.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@article{wang2023instructretro,
    title   = {InstructRetro: Instruction Tuning post Retrieval-Augmented Pretraining},
    author  = {Boxin Wang and Wei Ping and Lawrence McAfee and Peng Xu and Bo Li and Mohammad Shoeybi and Bryan Catanzaro},
    year    = {2023},
    journal = {arXiv preprint arXiv: 2310.07713}
}

@inproceedings{
wang2023shall,
title={Shall We Pretrain Autoregressive Language Models with Retrieval? A Comprehensive Study},
author  = {Boxin Wang and Wei Ping and Peng Xu and Lawrence McAfee and Zihan Liu and Mohammad Shoeybi and Yi Dong and Oleksii Kuchaiev and Bo Li and Chaowei Xiao and Anima Anandkumar and Bryan Catanzaro},
booktitle={The 2023 Conference on Empirical Methods in Natural Language Processing},
year={2023},
}

@article{DBLP:journals/corr/abs-1903-05543,
  author       = {James Thorne and
                  Andreas Vlachos},
  title        = {Adversarial attacks against Fact Extraction and VERification},
  journal      = {CoRR},
  volume       = {abs/1903.05543},
  year         = {2019},
  url          = {http://arxiv.org/abs/1903.05543},
  eprinttype    = {arXiv},
  eprint       = {1903.05543},
  timestamp    = {Tue, 03 Nov 2020 12:45:06 +0100},
  biburl       = {https://dblp.org/rec/journals/corr/abs-1903-05543.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{DBLP:conf/naacl/IyyerWGZ18,
  author       = {Mohit Iyyer and
                  John Wieting and
                  Kevin Gimpel and
                  Luke Zettlemoyer},
  editor       = {Marilyn A. Walker and
                  Heng Ji and
                  Amanda Stent},
  title        = {Adversarial Example Generation with Syntactically Controlled Paraphrase
                  Networks},
  booktitle    = {Proceedings of the 2018 Conference of the North American Chapter of
                  the Association for Computational Linguistics: Human Language Technologies,
                  {NAACL-HLT} 2018, New Orleans, Louisiana, USA, June 1-6, 2018, Volume
                  1 (Long Papers)},
  pages        = {1875--1885},
  publisher    = {Association for Computational Linguistics},
  year         = {2018},
  url          = {https://doi.org/10.18653/v1/n18-1170},
  doi          = {10.18653/v1/n18-1170},
  timestamp    = {Fri, 06 Aug 2021 00:41:31 +0200},
  biburl       = {https://dblp.org/rec/conf/naacl/IyyerWGZ18.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}


@inproceedings{DBLP:conf/ijcai/RibeiroWG021,
  author       = {Marco T{\'{u}}lio Ribeiro and
                  Tongshuang Wu and
                  Carlos Guestrin and
                  Sameer Singh},
  editor       = {Zhi{-}Hua Zhou},
  title        = {Beyond Accuracy: Behavioral Testing of {NLP} Models with Checklist
                  (Extended Abstract)},
  booktitle    = {Proceedings of the Thirtieth International Joint Conference on Artificial
                  Intelligence, {IJCAI} 2021, Virtual Event / Montreal, Canada, 19-27
                  August 2021},
  pages        = {4824--4828},
  publisher    = {ijcai.org},
  year         = {2021},
  url          = {https://doi.org/10.24963/ijcai.2021/659},
  doi          = {10.24963/ijcai.2021/659},
  timestamp    = {Wed, 25 Aug 2021 17:11:16 +0200},
  biburl       = {https://dblp.org/rec/conf/ijcai/RibeiroWG021.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{DBLP:conf/coling/NaikRSRN18,
  author       = {Aakanksha Naik and
                  Abhilasha Ravichander and
                  Norman M. Sadeh and
                  Carolyn P. Ros{\'{e}} and
                  Graham Neubig},
  editor       = {Emily M. Bender and
                  Leon Derczynski and
                  Pierre Isabelle},
  title        = {Stress Test Evaluation for Natural Language Inference},
  booktitle    = {Proceedings of the 27th International Conference on Computational
                  Linguistics, {COLING} 2018, Santa Fe, New Mexico, USA, August 20-26,
                  2018},
  pages        = {2340--2353},
  publisher    = {Association for Computational Linguistics},
  year         = {2018},
  url          = {https://aclanthology.org/C18-1198/},
  timestamp    = {Wed, 21 Sep 2022 12:40:04 +0200},
  biburl       = {https://dblp.org/rec/conf/coling/NaikRSRN18.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{DBLP:conf/acl/NieWDBWK20,
  author       = {Yixin Nie and
                  Adina Williams and
                  Emily Dinan and
                  Mohit Bansal and
                  Jason Weston and
                  Douwe Kiela},
  editor       = {Dan Jurafsky and
                  Joyce Chai and
                  Natalie Schluter and
                  Joel R. Tetreault},
  title        = {Adversarial {NLI:} {A} New Benchmark for Natural Language Understanding},
  booktitle    = {Proceedings of the 58th Annual Meeting of the Association for Computational
                  Linguistics, {ACL} 2020, Online, July 5-10, 2020},
  pages        = {4885--4901},
  publisher    = {Association for Computational Linguistics},
  year         = {2020},
  url          = {https://doi.org/10.18653/v1/2020.acl-main.441},
  doi          = {10.18653/v1/2020.acl-main.441},
  timestamp    = {Thu, 14 Oct 2021 09:46:09 +0200},
  biburl       = {https://dblp.org/rec/conf/acl/NieWDBWK20.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{DBLP:conf/emnlp/JiaL17,
  author       = {Robin Jia and
                  Percy Liang},
  editor       = {Martha Palmer and
                  Rebecca Hwa and
                  Sebastian Riedel},
  title        = {Adversarial Examples for Evaluating Reading Comprehension Systems},
  booktitle    = {Proceedings of the 2017 Conference on Empirical Methods in Natural
                  Language Processing, {EMNLP} 2017, Copenhagen, Denmark, September
                  9-11, 2017},
  pages        = {2021--2031},
  publisher    = {Association for Computational Linguistics},
  year         = {2017},
  url          = {https://doi.org/10.18653/v1/d17-1215},
  doi          = {10.18653/v1/d17-1215},
  timestamp    = {Fri, 06 Aug 2021 00:40:40 +0200},
  biburl       = {https://dblp.org/rec/conf/emnlp/JiaL17.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@article{DBLP:journals/corr/abs-2203-05314,
  author       = {Junjie Shen and
                  Ningfei Wang and
                  Ziwen Wan and
                  Yunpeng Luo and
                  Takami Sato and
                  Zhisheng Hu and
                  Xinyang Zhang and
                  Shengjian Guo and
                  Zhenyu Zhong and
                  Kang Li and
                  Ziming Zhao and
                  Chunming Qiao and
                  Qi Alfred Chen},
  title        = {SoK: On the Semantic {AI} Security in Autonomous Driving},
  journal      = {CoRR},
  volume       = {abs/2203.05314},
  year         = {2022},
  url          = {https://doi.org/10.48550/arXiv.2203.05314},
  doi          = {10.48550/arXiv.2203.05314},
  eprinttype    = {arXiv},
  eprint       = {2203.05314},
  timestamp    = {Wed, 04 Jan 2023 08:38:50 +0100},
  biburl       = {https://dblp.org/rec/journals/corr/abs-2203-05314.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{yoo-etal-2022-ground,
    title = "Ground-Truth Labels Matter: A Deeper Look into Input-Label Demonstrations",
    author = "Yoo, Kang Min  and
      Kim, Junyeob  and
      Kim, Hyuhng Joon  and
      Cho, Hyunsoo  and
      Jo, Hwiyeol  and
      Lee, Sang-Woo  and
      Lee, Sang-goo  and
      Kim, Taeuk",
    booktitle = "Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing",
    month = dec,
    year = "2022",
    address = "Abu Dhabi, United Arab Emirates",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.emnlp-main.155",
    pages = "2422--2437",
}

@article{wei2023larger,
  title={Larger language models do in-context learning differently},
  author={Wei, Jerry and Wei, Jason and Tay, Yi and Tran, Dustin and Webson, Albert and Lu, Yifeng and Chen, Xinyun and Liu, Hanxiao and Huang, Da and Zhou, Denny and others},
  journal={arXiv preprint arXiv:2303.03846},
  year={2023}
}

@inproceedings{lu-etal-2022-fantastically,
    title = "Fantastically Ordered Prompts and Where to Find Them: Overcoming Few-Shot Prompt Order Sensitivity",
    author = "Lu, Yao  and
      Bartolo, Max  and
      Moore, Alastair  and
      Riedel, Sebastian  and
      Stenetorp, Pontus",
    booktitle = "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = may,
    year = "2022",
    address = "Dublin, Ireland",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.acl-long.556",
    doi = "10.18653/v1/2022.acl-long.556",
    pages = "8086--8098",
}

@inproceedings{mishra-etal-2022-cross,
    title = "Cross-Task Generalization via Natural Language Crowdsourcing Instructions",
    author = "Mishra, Swaroop  and
      Khashabi, Daniel  and
      Baral, Chitta  and
      Hajishirzi, Hannaneh",
    booktitle = "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = may,
    year = "2022",
    address = "Dublin, Ireland",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.acl-long.244",
    doi = "10.18653/v1/2022.acl-long.244",
    pages = "3470--3487",
    abstract = "Humans (e.g., crowdworkers) have a remarkable ability in solving different tasks, by simply reading textual instructions that define them and looking at a few examples. Despite the success of the conventional supervised learning on individual datasets, such models often struggle with generalization across tasks (e.g., a question-answering system cannot solve classification tasks). A long-standing challenge in AI is to build a model that learns a new task by understanding the human-readable instructions that define it. To study this, we introduce NATURAL INSTRUCTIONS, a dataset of 61 distinct tasks, their human-authored instructions, and 193k task instances (input-output pairs). The instructions are obtained from crowdsourcing instructions used to create existing NLP datasets and mapped to a unified schema. Using this meta-dataset, we measure cross-task generalization by training models on seen tasks and measuring generalization to the remaining unseen ones. We adopt generative pre-trained language models to encode task-specific instructions along with input and generate task output. Our results indicate that models benefit from instructions when evaluated in terms of generalization to unseen tasks (19{\%} better for models utilizing instructions). These models, however, are far behind an estimated performance upperbound indicating significant room for more progress in this direction.",
}

@inproceedings{lu1codexglue,
  title={CodeXGLUE: A Machine Learning Benchmark Dataset for Code Understanding and Generation},
  author={Lu, Shuai and Guo, Daya and Ren, Shuo and Huang, Junjie and Svyatkovskiy, Alexey and Blanco, Ambrosio and Clement, Colin and Drain, Dawn and Jiang, Daxin and Tang, Duyu and others},
  booktitle={Thirty-fifth Conference on Neural Information Processing Systems Datasets and Benchmarks Track (Round 1)}
}

@book{ross2002right,
  title={The right and the good},
  author={Ross, William David},
  year={2002},
  publisher={Oxford University Press}
}

@article{srivastava2022beyond,
  title={Beyond the imitation game: Quantifying and extrapolating the capabilities of language models},
  author={Srivastava, Aarohi and Rastogi, Abhinav and Rao, Abhishek and Shoeb, Abu Awal Md and Abid, Abubakar and Fisch, Adam and Brown, Adam R and Santoro, Adam and Gupta, Aditya and Garriga-Alonso, Adri{\`a} and others},
  journal={arXiv preprint arXiv:2206.04615},
  year={2022}
}

@inproceedings{wang-etal-2022-super,
    title = "Super-{N}atural{I}nstructions: Generalization via Declarative Instructions on 1600+ {NLP} Tasks",
    author = "Wang, Yizhong  and
      Mishra, Swaroop  and
      Alipoormolabashi, Pegah  and
      Kordi, Yeganeh  and
      Mirzaei, Amirreza  and
      Naik, Atharva  and
      Ashok, Arjun  and
      Dhanasekaran, Arut Selvan  and
      Arunkumar, Anjana  and
      Stap, David  and
      Pathak, Eshaan  and
      Karamanolakis, Giannis  and
      Lai, Haizhi  and
      Purohit, Ishan  and
      Mondal, Ishani  and
      Anderson, Jacob  and
      Kuznia, Kirby  and
      Doshi, Krima  and
      Pal, Kuntal Kumar  and
      Patel, Maitreya  and
      Moradshahi, Mehrad  and
      Parmar, Mihir  and
      Purohit, Mirali  and
      Varshney, Neeraj  and
      Kaza, Phani Rohitha  and
      Verma, Pulkit  and
      Puri, Ravsehaj Singh  and
      Karia, Rushang  and
      Doshi, Savan  and
      Sampat, Shailaja Keyur  and
      Mishra, Siddhartha  and
      Reddy A, Sujan  and
      Patro, Sumanta  and
      Dixit, Tanay  and
      Shen, Xudong",
    booktitle = "Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing",
    month = dec,
    year = "2022",
    address = "Abu Dhabi, United Arab Emirates",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.emnlp-main.340",
    pages = "5085--5109",
    abstract = "How well can NLP models generalize to a variety of unseen tasks when provided with task instructions? To address this question, we first introduce Super-NaturalInstructions, a benchmark of 1,616 diverse NLP tasks and their expert-written instructions. Our collection covers 76 distinct task types, including but not limited to classification, extraction, infilling, sequence tagging, text rewriting, and text composition. This large and diverse collection of tasks enables rigorous benchmarking of cross-task generalization under instructions{---}training models to follow instructions on a subset of tasks and evaluating them on the remaining unseen ones.Furthermore, we build Tk-Instruct, a transformer model trained to follow a variety of in-context instructions (plain language task definitions or k-shot examples). Our experiments show that Tk-Instruct outperforms existing instruction-following models such as InstructGPT by over 9{\%} on our benchmark despite being an order of magnitude smaller. We further analyze generalization as a function of various scaling parameters, such as the number of observed tasks, the number of instances per task, and model sizes. We hope our dataset and model facilitate future progress towards more general-purpose NLP models.",
}

@inproceedings{kandpal2022deduplicating,
  title={Deduplicating training data mitigates privacy risks in language models},
  author={Kandpal, Nikhil and Wallace, Eric and Raffel, Colin},
  booktitle={International Conference on Machine Learning},
  pages={10697--10707},
  year={2022},
  organization={PMLR}
}

@article{wang2023chatcad,
  title={Chatcad: Interactive computer-aided diagnosis on medical image using large language models},
  author={Wang, Sheng and Zhao, Zihao and Ouyang, Xi and Wang, Qian and Shen, Dinggang},
  journal={arXiv preprint arXiv:2302.07257},
  year={2023}
}

@article{driess2023palm,
  title={Palm-e: An embodied multimodal language model},
  author={Driess, Danny and Xia, Fei and Sajjadi, Mehdi SM and Lynch, Corey and Chowdhery, Aakanksha and Ichter, Brian and Wahid, Ayzaan and Tompson, Jonathan and Vuong, Quan and Yu, Tianhe and others},
  journal={arXiv preprint arXiv:2303.03378},
  year={2023}
}

@article{zhou2023navigating,
  title={Navigating the Grey Area: Expressions of Overconfidence and Uncertainty in Language Models},
  author={Zhou, Kaitlyn and Jurafsky, Dan and  Hashimoto, Tatsunori},
  journal={arXiv:2302.13439v1},
  year={2023}
}

@article{morris2022unsupervised,
  title={Unsupervised Text Deidentification},
  author={Morris, John X. and Chiu, Justin T. and Zabih, Ramin and Rush, Alexander M.},
  journal={arXiv:2210.11528v1},
  year={2022}
}

@article{tramer2022considerations,
  title={Considerations for Differentially Private Learning
with Large-Scale Public Pretraining},
  author={Tram`er, Florian and Gautam, Kamath and Carlini, Nicholas Carlini},
  journal={arXiv:2212.06470 },
  year={2022}
}


@inproceedings{yudifferentially,
  title={Differentially Private Fine-tuning of Language Models},
  author={Yu, Da and Naik, Saurabh and Backurs, Arturs and Gopi, Sivakanth and Inan, Huseyin A and Kamath, Gautam and Kulkarni, Janardhan and Lee, Yin Tat and Manoel, Andre and Wutschitz, Lukas and others},
  booktitle={International Conference on Learning Representations},
  year={2022}
}


@article{zhu2023promptbench,
  title={PromptBench: Towards Evaluating the Robustness of Large Language Models on Adversarial Prompts},
  author={Zhu, Kaijie and Wang, Jindong and Zhou, Jiaheng and Wang, Zichen and Chen, Hao and Wang, Yidong and Yang, Linyi and Ye, Wei and Gong, Neil Zhenqiang and Zhang, Yue and others},
  journal={arXiv preprint arXiv:2306.04528},
  year={2023}
}

@article{suzgun2022challenging,
  title={Challenging BIG-Bench tasks and whether chain-of-thought can solve them},
  author={Suzgun, Mirac and Scales, Nathan and Sch{\"a}rli, Nathanael and Gehrmann, Sebastian and Tay, Yi and Chung, Hyung Won and Chowdhery, Aakanksha and Le, Quoc V and Chi, Ed H and Zhou, Denny and others},
  journal={arXiv preprint arXiv:2210.09261},
  year={2022}
}

@article{shi2022language,
  title={Language models are multilingual chain-of-thought reasoners},
  author={Shi, Freda and Suzgun, Mirac and Freitag, Markus and Wang, Xuezhi and Srivats, Suraj and Vosoughi, Soroush and Chung, Hyung Won and Tay, Yi and Ruder, Sebastian and Zhou, Denny and others},
  journal={arXiv preprint arXiv:2210.03057},
  year={2022}
}

@article{kim2022ground,
  title={Ground-Truth Labels Matter: A Deeper Look into Input-Label Demonstrations},
  author={Kim, Junyeob and Kim, Hyuhng Joon and Cho, Hyunsoo and Jo, Hwiyeol and Lee, Sang-Woo and Lee, Sang-goo and Yoo, Kang Min and Kim, Taeuk},
  journal={arXiv preprint arXiv:2205.12685},
  year={2022}
}

@article{zhong2023can,
  title={Can chatgpt understand too? a comparative study on chatgpt and fine-tuned bert},
  author={Zhong, Qihuang and Ding, Liang and Liu, Juhua and Du, Bo and Tao, Dacheng},
  journal={arXiv preprint arXiv:2302.10198},
  year={2023}
}

@article{liu2021makes,
  title={What Makes Good In-Context Examples for GPT-$3 $?},
  author={Liu, Jiachang and Shen, Dinghan and Zhang, Yizhe and Dolan, Bill and Carin, Lawrence and Chen, Weizhu},
  journal={arXiv preprint arXiv:2101.06804},
  year={2021}
}

@article{wang2023adversarial,
  title={Adversarial Demonstration Attacks on Large Language Models},
  author={Wang, Jiongxiao and Liu, Zichen and Park, Keun Hee and Chen, Muhao and Xiao, Chaowei},
  journal={arXiv preprint arXiv:2305.14950},
  year={2023}
}

@inproceedings{text-agent1,
  author       = {Marc{-}Alexandre C{\^{o}}t{\'{e}} and
                  {\'{A}}kos K{\'{a}}d{\'{a}}r and
                  Xingdi Yuan and
                  Ben Kybartas and
                  Tavian Barnes and
                  Emery Fine and
                  James Moore and
                  Matthew J. Hausknecht and
                  Layla El Asri and
                  Mahmoud Adada and
                  Wendy Tay and
                  Adam Trischler},
  title        = {TextWorld: {A} Learning Environment for Text-Based Games},
  booktitle    = {Computer Games - 7th Workshop, {CGW}, Held in Conjunction with
                  the 27th International Conference on Artificial Intelligence, {IJCAI}},
  series       = {Communications in Computer and Information Science},
  volume       = {1017},
  pages        = {41--75},
  publisher    = {Springer},
  year         = {2018}
}

@inproceedings{text-agent2,
  author       = {Mohit Shridhar and
                  Xingdi Yuan and
                  Marc{-}Alexandre C{\^{o}}t{\'{e}} and
                  Yonatan Bisk and
                  Adam Trischler and
                  Matthew J. Hausknecht},
  title        = {ALFWorld: Aligning Text and Embodied Environments for Interactive
                  Learning},
  booktitle    = {9th International Conference on Learning Representations, {ICLR}},
  year         = {2021}
}

@inproceedings{text-agent3,
  author       = {Matthew J. Hausknecht and
                  Prithviraj Ammanabrolu and
                  Marc{-}Alexandre C{\^{o}}t{\'{e}} and
                  Xingdi Yuan},
  title        = {Interactive Fiction Games: {A} Colossal Adventure},
  booktitle    = {The Thirty-Fourth {AAAI} Conference on Artificial Intelligence, {AAAI}},
  pages        = {7903--7910},
  publisher    = {{AAAI} Press},
  year         = {2020}
}

@inproceedings{moral-story,
  author       = {Denis Emelin and
                  Ronan Le Bras and
                  Jena D. Hwang and
                  Maxwell Forbes and
                  Yejin Choi},
  title        = {Moral Stories: Situated Reasoning about Norms, Intents, Actions, and
                  their Consequences},
  booktitle    = {Proceedings of the 2021 Conference on Empirical Methods in Natural
                  Language Processing, {EMNLP}},
  pages        = {698--718},
  publisher    = {Association for Computational Linguistics},
  year         = {2021}
}

@inproceedings{moral-exception-qa,
  author       = {Zhijing Jin and
                  Sydney Levine and
                  Fernando Gonzalez Adauto and
                  Ojasv Kamal and
                  Maarten Sap and
                  Mrinmaya Sachan and
                  Rada Mihalcea and
                  Josh Tenenbaum and
                  Bernhard Sch{\"{o}}lkopf},
  title        = {When to Make Exceptions: Exploring Language Models as Accounts of
                  Human Moral Judgment},
  booktitle    = {NeurIPS},
  year         = {2022}
}

@article{ritual,
  author       = {Anurag Acharya and
                  Kartik Talamadupula and
                  Mark A. Finlayson},
  title        = {An Atlas of Cultural Commonsense for Machine Reasoning},
  journal      = {CoRR},
  volume       = {abs/2009.05664},
  year         = {2020}
}

@inproceedings{social-chemistry,
  author       = {Maxwell Forbes and
                  Jena D. Hwang and
                  Vered Shwartz and
                  Maarten Sap and
                  Yejin Choi},
  title        = {Social Chemistry 101: Learning to Reason about Social and Moral Norms},
  booktitle    = {Proceedings of the 2020 Conference on Empirical Methods in Natural
                  Language Processing, {EMNLP}},
  pages        = {653--670},
  publisher    = {Association for Computational Linguistics},
  year         = {2020}
}

@article{MACHIAVELLI,
  author       = {Alexander Pan and
                  Jun Shern Chan and
                  Andy Zou and
                  Nathaniel Li and
                  Steven Basart and
                  Thomas Woodside and
                  Jonathan Ng and
                  Hanlin Zhang and
                  Scott Emmons and
                  Dan Hendrycks},
  title        = {Do the Rewards Justify the Means? Measuring Trade-Offs Between Rewards
                  and Ethical Behavior in the {MACHIAVELLI} Benchmark},
  journal      = {CoRR},
  volume       = {abs/2304.03279},
  year         = {2023}
}

@article{caton2020fairness,
  title={Fairness in machine learning: A survey},
  author={Caton, Simon and Haas, Christian},
  journal={arXiv preprint arXiv:2010.04053},
  year={2020}
}

@inproceedings{dwork2012fairness,
  title={Fairness through awareness},
  author={Dwork, Cynthia and Hardt, Moritz and Pitassi, Toniann and Reingold, Omer and Zemel, Richard},
  booktitle={Proceedings of the 3rd innovations in theoretical computer science conference},
  pages={214--226},
  year={2012}
}

@article{mehrabi2021survey,
  title={A survey on bias and fairness in machine learning},
  author={Mehrabi, Ninareh and Morstatter, Fred and Saxena, Nripsuta and Lerman, Kristina and Galstyan, Aram},
  journal={ACM Computing Surveys (CSUR)},
  volume={54},
  number={6},
  pages={1--35},
  year={2021},
  publisher={ACM New York, NY, USA}
}

@article{zhou2023ethical,
  title={Ethical ChatGPT: Concerns, Challenges, and Commandments},
  author={Zhou, Jianlong and M{\"u}ller, Heimo and Holzinger, Andreas and Chen, Fang},
  journal={arXiv preprint arXiv:2305.10646},
  year={2023}
}

@article{zhuo2023exploring,
  title={Exploring ai ethics of chatgpt: A diagnostic analysis},
  author={Zhuo, Terry Yue and Huang, Yujin and Chen, Chunyang and Xing, Zhenchang},
  journal={arXiv preprint arXiv:2301.12867},
  year={2023}
}

@article{liu2023summary,
  title={Summary of chatgpt/gpt-4 research and perspective towards the future of large language models},
  author={Liu, Yiheng and Han, Tianle and Ma, Siyuan and Zhang, Jiayue and Yang, Yuanyuan and Tian, Jiaming and He, Hao and Li, Antong and He, Mengshen and Liu, Zhengliang and others},
  journal={arXiv preprint arXiv:2304.01852},
  year={2023}
}

@article{hariri2023unlocking,
  title={Unlocking the Potential of ChatGPT: A Comprehensive Exploration of its Applications, Advantages, Limitations, and Future Directions in Natural Language Processing},
  author={Hariri, Walid},
  journal={arXiv preprint arXiv:2304.02017},
  year={2023}
}

@article{nori2023capabilities,
  title={Capabilities of gpt-4 on medical challenge problems},
  author={Nori, Harsha and King, Nicholas and McKinney, Scott Mayer and Carignan, Dean and Horvitz, Eric},
  journal={arXiv preprint arXiv:2303.13375},
  year={2023}
}


@article{li2023fairness,
  title={Fairness of ChatGPT},
  author={Li, Yunqi and Zhang, Yongfeng},
  journal={arXiv preprint arXiv:2305.18569},
  year={2023}
}

@article{promptinject,
  author       = {F{\'{a}}bio Perez and
                  Ian Ribeiro},
  title        = {Ignore Previous Prompt: Attack Techniques For Language Models},
  journal      = {CoRR},
  volume       = {abs/2211.09527},
  year         = {2022}
}

@article{morethan,
  author       = {Kai Greshake and
                  Sahar Abdelnabi and
                  Shailesh Mishra and
                  Christoph Endres and
                  Thorsten Holz and
                  Mario Fritz},
  title        = {More than you've asked for: {A} Comprehensive Analysis of Novel Prompt
                  Injection Threats to Application-Integrated Large Language Models},
  journal      = {CoRR},
  volume       = {abs/2302.12173},
  year         = {2023}
}

@misc{promptinject-app1,
   author = {Riley Goodside},
   title = {Exploiting GPT-3 prompts with malicious inputs that order the model to ignore its previous directions},
   howpublished = {\url{https://web.archive.org/web/20220919192024/https://twitter.com/goodside/status/1569128808308957185}}
}

@misc{promptinject-app2,
   author = {Simon Willison},
   title = {Prompt injection attacks against GPT-3},
   howpublished = {\url{http://web.archive.org/web/20220928004736/https://simonwillison.net/2022/Sep/12/prompt-injection/}}
}

@misc{promptinject-app3,
   author = {Simon Willison},
   title = {I missed this one: Someone did get a prompt leak attack to work against the bot},
   howpublished = {\url{https://web.archive.org/web/20220924105826/https://twitter.com/simonw/status/1570933190289924096}}
}

@misc{dan,
   author = {Lavina Daryanani},
   title = {How to jailbreak chatgpt},
   howpublished = {\url{https://watcher.guru/news/how-to-jailbreak-chatgpt}}
}

@article{program-attack,
  author       = {Daniel Kang and
                  Xuechen Li and
                  Ion Stoica and
                  Carlos Guestrin and
                  Matei Zaharia and
                  Tatsunori Hashimoto},
  title        = {Exploiting Programmatic Behavior of LLMs: Dual-Use Through Standard
                  Security Attacks},
  journal      = {CoRR},
  volume       = {abs/2302.05733},
  year         = {2023}
}


@article{clever-han,
  author       = {Natalie Shapira and
                  Mosh Levy and
                  Seyed Hossein Alavi and
                  Xuhui Zhou and
                  Yejin Choi and
                  Yoav Goldberg and
                  Maarten Sap and
                  Vered Shwartz},
  title        = {Clever Hans or Neural Theory of Mind? Stress Testing Social Reasoning
                  in Large Language Models},
  journal      = {CoRR},
  volume       = {abs/2305.14763},
  year         = {2023}
}

@misc{european2021aia,
  title={Laying down harmonised rules on artificial intelligence (Artificial Intelligence Act) and amending certain union legislative acts},
  author={European Commission},
  year={2021},
  publisher={Office for Official Publications of the European Communities Luxembourg},
  howpublished={\url{https://eur-lex.europa.eu/resource.html?uri=cellar:e0649735-a372-11eb-9585-01aa75ed71a1.0001.02/DOC-1\&format=PDF}}
}

@misc{ep2021aia,
  title={Amendments adopted by the European Parliament on 14 June 2023 on the proposal for a regulation of the European Parliament and of the Council on laying down harmonised rules on artificial intelligence (Artificial Intelligence Act) and amending certain Union legislative acts },
  author={European Parliament},
  year={2023},
  howpublished={\url{https://www.europarl.europa.eu/doceo/document/TA-9-2023-0236-EN.pdf}}
}


@article{wh2022blueprint,
  title={Blueprint for an AI Bill of Rights},
  author={{White House Office of Science and Technology Policy}},
  year={2022}
}

@article{floridi2022capai,
  title={CapAI-A Procedure for Conducting Conformity Assessment of AI Systems in Line with the EU Artificial Intelligence Act},
  author={Floridi, Luciano and Holweg, Matthias and Taddeo, Mariarosaria and Amaya Silva, Javier and M{\"o}kander, Jakob and Wen, Yuni},
  journal={Available at SSRN 4064091},
  year={2022}
}

@misc{bommasani2023eu-ai-act, 
    author = {Rishi Bommasani and Kevin Klyman and Daniel Zhang and Percy Liang}, 
    title  = {Do Foundation Model Providers Comply with the EU AI Act?}, 
    url    = {https://crfm.stanford.edu/2023/06/15/eu-ai-act.html}, 
    year   = {2023}
}

@inproceedings{akyurek-etal-2022-measuring,
    title = "On Measuring Social Biases in Prompt-Based Multi-Task Learning",
    author = {Aky{\"u}rek, Afra Feyza  and
      Paik, Sejin  and
      Kocyigit, Muhammed  and
      Akbiyik, Seda  and
      Runyun, Serife Leman  and
      Wijaya, Derry},
    booktitle = "Findings of the Association for Computational Linguistics: NAACL 2022",
    month = jul,
    year = "2022",
    address = "Seattle, United States",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.findings-naacl.42",
    doi = "10.18653/v1/2022.findings-naacl.42",
    pages = "551--564",
    abstract = "Large language models trained on a mixture of NLP tasks that are converted into a text-to-text format using prompts, can generalize into novel forms of language and handle novel tasks. A large body of work within prompt engineering attempts to understand the effects of input forms and prompts in achieving superior performance. We consider an alternative measure and inquire whether the way in which an input is encoded affects social biases promoted in outputs. In this paper, we study T0, a large-scale multi-task text-to-text language model trained using prompt-based learning. We consider two different forms of semantically equivalent inputs: question-answer format and premise-hypothesis format. We use an existing bias benchmark for the former BBQ and create the first bias benchmark in natural language inference BBNLI with hand-written hypotheses while also converting each benchmark into the other form. The results on two benchmarks suggest that given two different formulations of essentially the same input, T0 conspicuously acts more biased in question answering form, which is seen during training, compared to premise-hypothesis form which is unlike its training examples. Code and data are released under https://github.com/feyzaakyurek/bbnli.",
}

@inproceedings{bowman-etal-2015-large,
    title = "A large annotated corpus for learning natural language inference",
    author = "Bowman, Samuel R.  and
      Angeli, Gabor  and
      Potts, Christopher  and
      Manning, Christopher D.",
    booktitle = "Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing",
    month = sep,
    year = "2015",
    address = "Lisbon, Portugal",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/D15-1075",
    doi = "10.18653/v1/D15-1075",
    pages = "632--642",
}

@article{DBLP:journals/corr/abs-2307-09288,
  author       = {Hugo Touvron and
                  Louis Martin and
                  Kevin Stone and
                  Peter Albert and
                  Amjad Almahairi and
                  Yasmine Babaei and
                  Nikolay Bashlykov and
                  Soumya Batra and
                  Prajjwal Bhargava and
                  Shruti Bhosale and
                  Dan Bikel and
                  Lukas Blecher and
                  Cristian Canton{-}Ferrer and
                  Moya Chen and
                  Guillem Cucurull and
                  David Esiobu and
                  Jude Fernandes and
                  Jeremy Fu and
                  Wenyin Fu and
                  Brian Fuller and
                  Cynthia Gao and
                  Vedanuj Goswami and
                  Naman Goyal and
                  Anthony Hartshorn and
                  Saghar Hosseini and
                  Rui Hou and
                  Hakan Inan and
                  Marcin Kardas and
                  Viktor Kerkez and
                  Madian Khabsa and
                  Isabel Kloumann and
                  Artem Korenev and
                  Punit Singh Koura and
                  Marie{-}Anne Lachaux and
                  Thibaut Lavril and
                  Jenya Lee and
                  Diana Liskovich and
                  Yinghai Lu and
                  Yuning Mao and
                  Xavier Martinet and
                  Todor Mihaylov and
                  Pushkar Mishra and
                  Igor Molybog and
                  Yixin Nie and
                  Andrew Poulton and
                  Jeremy Reizenstein and
                  Rashi Rungta and
                  Kalyan Saladi and
                  Alan Schelten and
                  Ruan Silva and
                  Eric Michael Smith and
                  Ranjan Subramanian and
                  Xiaoqing Ellen Tan and
                  Binh Tang and
                  Ross Taylor and
                  Adina Williams and
                  Jian Xiang Kuan and
                  Puxin Xu and
                  Zheng Yan and
                  Iliyan Zarov and
                  Yuchen Zhang and
                  Angela Fan and
                  Melanie Kambadur and
                  Sharan Narang and
                  Aur{\'{e}}lien Rodriguez and
                  Robert Stojnic and
                  Sergey Edunov and
                  Thomas Scialom},
  title        = {Llama 2: Open Foundation and Fine-Tuned Chat Models},
  journal      = {CoRR},
  volume       = {abs/2307.09288},
  year         = {2023},
  url          = {https://doi.org/10.48550/arXiv.2307.09288},
  doi          = {10.48550/arXiv.2307.09288},
  eprinttype    = {arXiv},
  eprint       = {2307.09288},
  timestamp    = {Tue, 25 Jul 2023 16:04:08 +0200},
  biburl       = {https://dblp.org/rec/journals/corr/abs-2307-09288.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@online{MosaicML2023Introducing,
    author    = {MosaicML NLP Team},
    title     = {Introducing MPT-7B: A New Standard for Open-Source,
    ly Usable LLMs},
    year      = {2023},
    url       = {www.mosaicml.com/blog/mpt-7b},
    note      = {Accessed: 2023-08-19},
    urldate   = {2023-08-19}
}

@article{falcon40b,
  title={{Falcon-40B}: an open large language model with state-of-the-art performance},
  author={Almazrouei, Ebtesam and Alobeidli, Hamza and Alshamsi, Abdulaziz and Cappelli, Alessandro and Cojocaru, Ruxandra and Debbah, Merouane and Goffinet, Etienne and Heslow, Daniel and Launay, Julien and Malartic, Quentin and Noune, Badreddine and Pannier, Baptiste and Penedo, Guilherme},
  year={2023}
}

@software{together2023redpajama,
  author = {Together Computer},
  title = {RedPajama: An Open Source Recipe to Reproduce LLaMA training dataset},
  month = April,
  year = 2023,
  url = {https://github.com/togethercomputer/RedPajama-Data}
}


@article{Qiu2023LatentJA,
  title={Latent Jailbreak: A Benchmark for Evaluating Text Safety and Output Robustness of Large Language Models},
  author={Huachuan Qiu and Shuai Zhang and Anqi Li and Hongliang He and Zhenzhong Lan},
  journal={arXiv},
  year={2023},
  volume={abs/2307.08487},
  url={https://api.semanticscholar.org/CorpusID:259937347}
}

@inproceedings{Liu2023TrustworthyLA,
  title={Trustworthy LLMs: a Survey and Guideline for Evaluating Large Language Models' Alignment},
  author={Yang Liu and Yuanshun Yao and Jean-Francois Ton and Xiaoying Zhang and Ruocheng Guo Hao Cheng and Yegor Klochkov and Muhammad Faaiz Taufiq and Hanguang Li},
  year={2023},
  url={https://api.semanticscholar.org/CorpusID:260775522}
}


@misc{jailbreakingprompts,
  title={Jailbreak Chat},
  howpublished={\url{https://www.jailbreakchat.com/}},
}

@article{dubey2024llama3,
  title={The llama 3 herd of models},
  author={Dubey, Abhimanyu and Jauhri, Abhinav and Pandey, Abhinav and Kadian, Abhishek and Al-Dahle, Ahmad and Letman, Aiesha and Mathur, Akhil and Schelten, Alan and Yang, Amy and Fan, Angela and others},
  journal={arXiv},
  year={2024}
}

@article{yang2024qwen2,
  title={Qwen2 technical report},
  author={Yang, An and Yang, Baosong and Hui, Binyuan and Zheng, Bo and Yu, Bowen and Zhou, Chang and Li, Chengpeng and Li, Chengyuan and Liu, Dayiheng and Huang, Fei and others},
  journal={arXiv},
  year={2024}
}

@article{jiang2023mistral,
  title={Mistral 7B},
  author={Jiang, Albert Q and Sablayrolles, Alexandre and Mensch, Arthur and Bamford, Chris and Chaplot, Devendra Singh and Casas, Diego de las and Bressand, Florian and Lengyel, Gianna and Lample, Guillaume and Saulnier, Lucile and others},
  journal={arXiv},
  year={2023}
}

@article{achiam2023gpt4,
  title={Gpt-4 technical report},
  author={Achiam, Josh and Adler, Steven and Agarwal, Sandhini and Ahmad, Lama and Akkaya, Ilge and Aleman, Florencia Leoni and Almeida, Diogo and Altenschmidt, Janko and Altman, Sam and Anadkat, Shyamal and others},
  journal={arXiv},
  year={2023}
}


@inproceedings{GPT-3,
  title={Language models are few-shot learners},
  author={Brown, Tom and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared D and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and others},
  booktitle={NeurIPS},
  year={2020}
}

@article{wei2023jailbreak,
  title={Jailbreak and guard aligned language models with only few in-context demonstrations},
  author={Wei, Zeming and Wang, Yifei and Li, Ang and Mo, Yichuan and Wang, Yisen},
  journal={arXiv},
  year={2023}
}


@inproceedings{wei2022chain,
  title={Chain-of-thought prompting elicits reasoning in large language models},
  author={Wei, Jason and Wang, Xuezhi and Schuurmans, Dale and Bosma, Maarten and Xia, Fei and Chi, Ed and Le, Quoc V and Zhou, Denny and others},
  booktitle={NeurIPS},
  year={2022}
}


@article{reason-jailbreak-1,
  title={Jailbreaking proprietary large language models using word substitution cipher},
  author={Handa, Divij and Chirmule, Advait and Gajera, Bimal and Baral, Chitta},
  journal={arXiv},
  year={2024}
}


@inproceedings{reason-jailbreak-2,
    title = "Cognitive Overload: Jailbreaking Large Language Models with Overloaded Logical Thinking",
    author = "Xu, Nan  and
      Wang, Fei  and
      Zhou, Ben  and
      Li, Bangzheng  and
      Xiao, Chaowei  and
      Chen, Muhao",
    booktitle = "NAACL",
    year = "2024",
}

@inproceedings{reason-jailbreak-3,
    title = "{C}ode{A}ttack: Revealing Safety Generalization Challenges of Large Language Models via Code Completion",
    author = "Ren, Qibing  and
      Gao, Chang  and
      Shao, Jing  and
      Yan, Junchi  and
      Tan, Xin  and
      Lam, Wai  and
      Ma, Lizhuang",
    booktitle = "ACL",
    year = "2024"
}

@article{reason-jailbreak-4,
  title={Jailbreaking Large Language Models with Symbolic Mathematics},
  author={Bethany, Emet and Bethany, Mazal and Flores, Juan Arturo Nolazco and Jha, Sumit Kumar and Najafirad, Peyman},
  journal={arXiv},
  year={2024}
}

@article{reform-attack,
  title={You Know What I'm Saying: Jailbreak Attack via Implicit Reference},
  author={Wu, Tianyu and Mei, Lingrui and Yuan, Ruibin and Li, Lujun and Xue, Wei and Guo, Yike},
  journal={arXiv},
  year={2024}
}

@inproceedings{bender2020climbing,
 author = {Bender, Emily M.  and
Koller, Alexander},
 booktitle = {ACL},
 title = {Climbing towards {NLU}: {On} Meaning, Form, and Understanding in the Age of Data},
 year = {2020}
}

@inproceedings{lin2020commongen,
 author = {Lin, Bill Yuchen  and
Zhou, Wangchunshu  and
Shen, Ming  and
Zhou, Pei  and
Bhagavatula, Chandra  and
Choi, Yejin  and
Ren, Xiang},
 booktitle = {EMNLP},
 title = {{C}ommon{G}en: A Constrained Text Generation Challenge for Generative Commonsense Reasoning},
 year = {2020}
}

@inproceedings{huang2022open,
 author = {Huang, Jie  and
Chang, Kevin  and
Xiong, Jinjun  and
Hwu, Wen-mei},
 booktitle = {ACL},
 title = {Open Relation Modeling: Learning to Define Relations between Entities},
 year = {2022}
}

@inproceedings{huang2022deer,
 author = {Huang, Jie  and
Zhu, Kerui  and
Chang, Kevin Chen-Chuan  and
Xiong, Jinjun  and
Hwu, Wen-mei},
 booktitle = {EMNLP},
 title = {{DEER}: Descriptive Knowledge Graph for Explaining Entity Relationships},
 year = {2022}
}

@inproceedings{wiegreffe2021reframing,
 author = {Wiegreffe, Sarah  and
Hessel, Jack  and
Swayamdipta, Swabha  and
Riedl, Mark  and
Choi, Yejin},
 booktitle = {NAACL},
 year = {2022}
}

@article{marcus2020next,
 author = {Marcus, Gary},
 journal = {arXiv},
 title = {The next decade in ai: four steps towards robust artificial intelligence},
 year = {2020}
}

@inproceedings{kojima2022large,
 author = {Takeshi Kojima and Shixiang Shane Gu and Machel Reid and Yutaka Matsuo and Yusuke Iwasawa},
 booktitle = {NeurIPS},
 title = {Large Language Models are Zero-Shot Reasoners},
 year = {2022}
}


@inproceedings{huang2022large,
 address = {Abu Dhabi, United Arab Emirates},
 author = {Huang, Jie  and
Shao, Hanyin  and
Chang, Kevin Chen-Chuan},
 booktitle = {Findings of the Association for Computational Linguistics: EMNLP 2022},
 pages = {2038--2047},
 publisher = {Association for Computational Linguistics},
 title = {Are Large Pre-Trained Language Models Leaking Your Personal Information?},
 url = {https://aclanthology.org/2022.findings-emnlp.148},
 year = {2022}
}

@article{huang2022ver,
 author = {Huang, Jie and Chang, Kevin Chen-Chuan},
 journal = {arXiv preprint},
 title = {VER: Learning Natural Language Representations for Verbalizing Entities and Relations},
 url = {https://arxiv.org/abs/2211.11093},
 volume = {abs/2211.11093},
 year = {2022}
}

@article{dasgupta2022language,
 author = {Dasgupta, Ishita and Lampinen, Andrew K and Chan, Stephanie CY and Creswell, Antonia and Kumaran, Dharshan and McClelland, James L and Hill, Felix},
 journal = {arXiv preprint},
 title = {Language models show human-like content effects on reasoning},
 url = {https://arxiv.org/abs/2207.07051},
 volume = {abs/2207.07051},
 year = {2022}
}

@article{saparov2022language,
 author = {Saparov, Abulhair and He, He},
 journal = {arXiv preprint},
 title = {Language models are greedy reasoners: A systematic formal analysis of chain-of-thought},
 url = {https://arxiv.org/abs/2210.01240},
 volume = {abs/2210.01240},
 year = {2022}
}

@article{wei2022emergent,
 author = {Wei, Jason and Tay, Yi and Bommasani, Rishi and Raffel, Colin and Zoph, Barret and Borgeaud, Sebastian and Yogatama, Dani and Bosma, Maarten and Zhou, Denny and Metzler, Donald and others},
 journal = {Transactions on Machine Learning Research},
 title = {Emergent abilities of large language models},
 year = {2022}
}

@article{mitchell2021abstraction,
 author = {Mitchell, Melanie},
 journal = {Annals of the New York Academy of Sciences},
 number = {1},
 pages = {79--101},
 publisher = {Wiley Online Library},
 title = {Abstraction and analogy-making in artificial intelligence},
 volume = {1505},
 year = {2021}
}

@article{russin2020deep,
 author = {Russin, Jacob and O’Reilly, Randall C and Bengio, Yoshua},
 journal = {Work Bridging AI Cogn Sci},
 pages = {603--616},
 title = {Deep learning needs a prefrontal cortex},
 volume = {107},
 year = {2020}
}

@article{piantasodi2022meaning,
 author = {Piantasodi, Steven T and Hill, Felix},
 journal = {arXiv preprint},
 title = {Meaning without reference in large language models},
 url = {https://arxiv.org/abs/2208.02957},
 volume = {abs/2208.02957},
 year = {2022}
}

@inproceedings{li2021implicit,
 address = {Online},
 author = {Li, Belinda Z.  and
Nye, Maxwell  and
Andreas, Jacob},
 booktitle = {Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)},
 doi = {10.18653/v1/2021.acl-long.143},
 pages = {1813--1827},
 publisher = {Association for Computational Linguistics},
 title = {Implicit Representations of Meaning in Neural Language Models},
 url = {https://aclanthology.org/2021.acl-long.143},
 year = {2021}
}

@article{manning2022human,
 author = {Manning, Christopher D},
 journal = {Daedalus},
 number = {2},
 pages = {127--138},
 publisher = {MIT Press One Rogers Street, Cambridge, MA 02142-1209, USA journals-info~…},
 title = {Human language understanding \& reasoning},
 volume = {151},
 year = {2022}
}

@article{galotti1989approaches,
 author = {Galotti, Kathleen M},
 journal = {Psychological bulletin},
 number = {3},
 pages = {331},
 publisher = {American Psychological Association},
 title = {Approaches to studying formal and everyday reasoning.},
 volume = {105},
 year = {1989}
}

@article{bronkhorst2020logical,
 author = {Bronkhorst, Hugo and Roorda, Gerrit and Suhre, Cor and Goedhart, Martin},
 journal = {International Journal of Science and Mathematics Education},
 number = {8},
 pages = {1673--1694},
 publisher = {Springer},
 title = {Logical reasoning in formal and everyday reasoning tasks},
 volume = {18},
 year = {2020}
}

@article{bhargava2022commonsense,
 author = {Bhargava, Prajjwal and Ng, Vincent},
 journal = {Proceedings of the AAAI Conference on Artificial Intelligence},
 title = {Commonsense Knowledge Reasoning and Generation with Pre-trained Language Models: A Survey},
 year = {2022}
}

@article{bommasani2021opportunities,
 author = {Bommasani, Rishi and Hudson, Drew A and Adeli, Ehsan and Altman, Russ and Arora, Simran and von Arx, Sydney and Bernstein, Michael S and Bohg, Jeannette and Bosselut, Antoine and Brunskill, Emma and others},
 journal = {arXiv preprint},
 title = {On the opportunities and risks of foundation models},
 url = {https://arxiv.org/abs/2108.07258},
 volume = {abs/2108.07258},
 year = {2021}
}

@article{rae2021scaling,
 author = {Rae, Jack W and Borgeaud, Sebastian and Cai, Trevor and Millican, Katie and Hoffmann, Jordan and Song, Francis and Aslanides, John and Henderson, Sarah and Ring, Roman and Young, Susannah and others},
 journal = {arXiv preprint},
 title = {Scaling language models: Methods, analysis \& insights from training gopher},
 url = {https://arxiv.org/abs/2112.11446},
 volume = {abs/2112.11446},
 year = {2021}
}

@article{cobbe2021training,
 author = {Cobbe, Karl and Kosaraju, Vineet and Bavarian, Mohammad and Hilton, Jacob and Nakano, Reiichiro and Hesse, Christopher and Schulman, John},
 journal = {arXiv},
 title = {Training verifiers to solve math word problems},
 year = {2021}
}

@inproceedings{nye2021show,
 author = {Maxwell Nye and Anders Johan Andreassen and Guy Gur-Ari and Henryk Michalewski and Jacob Austin and David Bieber and David Dohan and Aitor Lewkowycz and Maarten Bosma and David Luan and Charles Sutton and Augustus Odena},
 booktitle = {Deep Learning for Code Workshop},
 title = {Show Your Work: Scratchpads for Intermediate Computation with Language Models},
 url = {https://openreview.net/forum?id=HBlx2idbkbq},
 year = {2022}
}

@inproceedings{brown2020language,
 author = {Tom B. Brown and
Benjamin Mann and
Nick Ryder and
Melanie Subbiah and
Jared Kaplan and
Prafulla Dhariwal and
Arvind Neelakantan and
Pranav Shyam and
Girish Sastry and
Amanda Askell and
Sandhini Agarwal and
Ariel Herbert{-}Voss and
Gretchen Krueger and
Tom Henighan and
Rewon Child and
Aditya Ramesh and
Daniel M. Ziegler and
Jeffrey Wu and
Clemens Winter and
Christopher Hesse and
Mark Chen and
Eric Sigler and
Mateusz Litwin and
Scott Gray and
Benjamin Chess and
Jack Clark and
Christopher Berner and
Sam McCandlish and
Alec Radford and
Ilya Sutskever and
Dario Amodei},
 bibsource = {dblp computer science bibliography, https://dblp.org},
 biburl = {https://dblp.org/rec/conf/nips/BrownMRSKDNSSAA20.bib},
 booktitle = {Advances in Neural Information Processing Systems 33: Annual Conference
on Neural Information Processing Systems 2020, NeurIPS 2020, December
6-12, 2020, virtual},
 editor = {Hugo Larochelle and
Marc'Aurelio Ranzato and
Raia Hadsell and
Maria{-}Florina Balcan and
Hsuan{-}Tien Lin},
 timestamp = {Tue, 19 Jan 2021 00:00:00 +0100},
 title = {Language Models are Few-Shot Learners},
 url = {https://proceedings.neurips.cc/paper/2020/hash/1457c0d6bfcb4967418bfb8ac142f64a-Abstract.html},
 year = {2020}
}

@article{chowdhery2022palm,
 author = {Chowdhery, Aakanksha and Narang, Sharan and Devlin, Jacob and Bosma, Maarten and Mishra, Gaurav and Roberts, Adam and Barham, Paul and Chung, Hyung Won and Sutton, Charles and Gehrmann, Sebastian and others},
 journal = {arXiv preprint},
 title = {Palm: Scaling language modeling with pathways},
 url = {https://arxiv.org/abs/2204.02311},
 volume = {abs/2204.02311},
 year = {2022}
}

@inproceedings{chen2019neural,
 author = {Xinyun Chen and
Chen Liang and
Adams Wei Yu and
Denny Zhou and
Dawn Song and
Quoc V. Le},
 bibsource = {dblp computer science bibliography, https://dblp.org},
 biburl = {https://dblp.org/rec/conf/iclr/ChenLYZSL20.bib},
 booktitle = {8th International Conference on Learning Representations, {ICLR} 2020,
Addis Ababa, Ethiopia, April 26-30, 2020},
 publisher = {OpenReview.net},
 timestamp = {Thu, 07 May 2020 01:00:00 +0200},
 title = {Neural Symbolic Reader: Scalable Integration of Distributed and Symbolic
Representations for Reading Comprehension},
 url = {https://openreview.net/forum?id=ryxjnREFwH},
 year = {2020}
}

@inproceedings{dong2018neural,
 author = {Honghua Dong and
Jiayuan Mao and
Tian Lin and
Chong Wang and
Lihong Li and
Denny Zhou},
 bibsource = {dblp computer science bibliography, https://dblp.org},
 biburl = {https://dblp.org/rec/conf/iclr/DongMLWLZ19.bib},
 booktitle = {7th International Conference on Learning Representations, {ICLR} 2019,
New Orleans, LA, USA, May 6-9, 2019},
 publisher = {OpenReview.net},
 timestamp = {Thu, 25 Jul 2019 01:00:00 +0200},
 title = {Neural Logic Machines},
 url = {https://openreview.net/forum?id=B1xY-hRctX},
 year = {2019}
}

@article{shi2022language,
 author = {Shi, Freda and Suzgun, Mirac and Freitag, Markus and Wang, Xuezhi and Srivats, Suraj and Vosoughi, Soroush and Chung, Hyung Won and Tay, Yi and Ruder, Sebastian and Zhou, Denny and others},
 journal = {arXiv preprint},
 title = {Language models are multilingual chain-of-thought reasoners},
 url = {https://arxiv.org/abs/2210.03057},
 volume = {abs/2210.03057},
 year = {2022}
}

@article{chen2022large,
 author = {Chen, Wenhu},
 journal = {arXiv preprint},
 title = {Large language models are few (1)-shot table reasoners},
 url = {https://arxiv.org/abs/2210.06710},
 volume = {abs/2210.06710},
 year = {2022}
}

@article{prystawski2022psychologically,
 author = {Prystawski, Ben and Thibodeau, Paul and Goodman, Noah},
 journal = {arXiv preprint},
 title = {Psychologically-informed chain-of-thought prompts for metaphor understanding in large language models},
 url = {https://arxiv.org/abs/2209.08141},
 volume = {abs/2209.08141},
 year = {2022}
}

@article{zhou2022least,
 author = {Zhou, Denny and Sch{\"a}rli, Nathanael and Hou, Le and Wei, Jason and Scales, Nathan and Wang, Xuezhi and Schuurmans, Dale and Bousquet, Olivier and Le, Quoc and Chi, Ed},
 journal = {arXiv preprint},
 title = {Least-to-Most Prompting Enables Complex Reasoning in Large Language Models},
 url = {https://arxiv.org/abs/2205.10625},
 volume = {abs/2205.10625},
 year = {2022}
}

@inproceedings{lake2018generalization,
 author = {Brenden M. Lake and
Marco Baroni},
 bibsource = {dblp computer science bibliography, https://dblp.org},
 biburl = {https://dblp.org/rec/conf/icml/LakeB18.bib},
 booktitle = {Proceedings of the 35th International Conference on Machine Learning,
{ICML} 2018, Stockholmsm{\"{a}}ssan, Stockholm, Sweden, July 10-15,
2018},
 editor = {Jennifer G. Dy and
Andreas Krause},
 pages = {2879--2888},
 publisher = {{PMLR}},
 series = {Proceedings of Machine Learning Research},
 timestamp = {Wed, 03 Apr 2019 01:00:00 +0200},
 title = {Generalization without Systematicity: On the Compositional Skills
of Sequence-to-Sequence Recurrent Networks},
 url = {http://proceedings.mlr.press/v80/lake18a.html},
 volume = {80},
 year = {2018}
}

@inproceedings{keysers2019measuring,
 author = {Daniel Keysers and
Nathanael Sch{\"{a}}rli and
Nathan Scales and
Hylke Buisman and
Daniel Furrer and
Sergii Kashubin and
Nikola Momchev and
Danila Sinopalnikov and
Lukasz Stafiniak and
Tibor Tihon and
Dmitry Tsarkov and
Xiao Wang and
Marc van Zee and
Olivier Bousquet},
 bibsource = {dblp computer science bibliography, https://dblp.org},
 biburl = {https://dblp.org/rec/conf/iclr/KeysersSSBFKMSS20.bib},
 booktitle = {8th International Conference on Learning Representations, {ICLR} 2020,
Addis Ababa, Ethiopia, April 26-30, 2020},
 publisher = {OpenReview.net},
 timestamp = {Thu, 07 May 2020 01:00:00 +0200},
 title = {Measuring Compositional Generalization: {A} Comprehensive Method on
Realistic Data},
 url = {https://openreview.net/forum?id=SygcCnNKwr},
 year = {2020}
}

@article{khot2022decomposed,
 author = {Khot, Tushar and Trivedi, Harsh and Finlayson, Matthew and Fu, Yao and Richardson, Kyle and Clark, Peter and Sabharwal, Ashish},
 journal = {arXiv preprint},
 title = {Decomposed prompting: A modular approach for solving complex tasks},
 url = {https://arxiv.org/abs/2210.02406},
 volume = {abs/2210.02406},
 year = {2022}
}

@article{zhou2022reflection,
 author = {Zhou, Fan and Dong, Haoyu and Liu, Qian and Cheng, Zhoujun and Han, Shi and Zhang, Dongmei},
 journal = {arXiv preprint},
 title = {Reflection of Thought: Inversely Eliciting Numerical Reasoning in Language Models via Solving Linear Systems},
 url = {https://arxiv.org/abs/2210.05075},
 volume = {abs/2210.05075},
 year = {2022}
}

@article{razeghi2022impact,
 author = {Razeghi, Yasaman and Logan IV, Robert L and Gardner, Matt and Singh, Sameer},
 journal = {arXiv preprint},
 title = {Impact of pretraining term frequencies on few-shot reasoning},
 url = {https://arxiv.org/abs/2202.07206},
 volume = {abs/2202.07206},
 year = {2022}
}

@article{fu2022complexity,
 author = {Fu, Yao and Peng, Hao and Sabharwal, Ashish and Clark, Peter and Khot, Tushar},
 journal = {arXiv preprint},
 title = {Complexity-Based Prompting for Multi-Step Reasoning},
 url = {https://arxiv.org/abs/2210.00720},
 volume = {abs/2210.00720},
 year = {2022}
}

@article{zhang2022automatic,
 author = {Zhang, Zhuosheng and Zhang, Aston and Li, Mu and Smola, Alex},
 journal = {arXiv},
 title = {Automatic Chain of Thought Prompting in Large Language Models},
 year = {2022}
}

@inproceedings{liu2022makes,
 address = {Dublin, Ireland and Online},
 author = {Liu, Jiachang  and
Shen, Dinghan  and
Zhang, Yizhe  and
Dolan, Bill  and
Carin, Lawrence  and
Chen, Weizhu},
 booktitle = {Proceedings of Deep Learning Inside Out (DeeLIO 2022): The 3rd Workshop on Knowledge Extraction and Integration for Deep Learning Architectures},
 doi = {10.18653/v1/2022.deelio-1.10},
 pages = {100--114},
 publisher = {Association for Computational Linguistics},
 title = {What Makes Good In-Context Examples for {GPT}-3?},
 url = {https://aclanthology.org/2022.deelio-1.10},
 year = {2022}
}

@article{wang2022self,
 author = {Wang, Xuezhi and Wei, Jason and Schuurmans, Dale and Le, Quoc and Chi, Ed and Zhou, Denny},
 journal = {arXiv preprint},
 title = {Self-consistency improves chain of thought reasoning in language models},
 url = {https://arxiv.org/abs/2203.11171},
 volume = {abs/2203.11171},
 year = {2022}
}

@article{li2022advance,
 author = {Li, Yifei and Lin, Zeqi and Zhang, Shizhuo and Fu, Qiang and Chen, Bei and Lou, Jian-Guang and Chen, Weizhu},
 journal = {arXiv preprint},
 title = {On the Advance of Making Language Models Better Reasoners},
 url = {https://arxiv.org/abs/2206.02336},
 volume = {abs/2206.02336},
 year = {2022}
}

@article{zhou2022teaching,
 author = {Zhou, Hattie and Nova, Azade and Larochelle, Hugo and Courville, Aaron and Neyshabur, Behnam and Sedghi, Hanie},
 journal = {arXiv preprint},
 title = {Teaching Algorithmic Reasoning via In-context Learning},
 url = {https://arxiv.org/abs/2211.09066},
 volume = {abs/2211.09066},
 year = {2022}
}

@article{chen2021evaluating,
 author = {Chen, Mark and Tworek, Jerry and Jun, Heewoo and Yuan, Qiming and Pinto, Henrique Ponde de Oliveira and Kaplan, Jared and Edwards, Harri and Burda, Yuri and Joseph, Nicholas and Brockman, Greg and others},
 journal = {arXiv preprint},
 title = {Evaluating large language models trained on code},
 url = {https://arxiv.org/abs/2107.03374},
 volume = {abs/2107.03374},
 year = {2021}
}

@article{lewkowycz2022solving,
 author = {Lewkowycz, Aitor and Andreassen, Anders and Dohan, David and Dyer, Ethan and Michalewski, Henryk and Ramasesh, Vinay and Slone, Ambrose and Anil, Cem and Schlag, Imanol and Gutman-Solo, Theo and others},
 journal = {arXiv preprint},
 title = {Solving quantitative reasoning problems with language models},
 url = {https://arxiv.org/abs/2206.14858},
 volume = {abs/2206.14858},
 year = {2022}
}

@article{chung2022scaling,
 author = {Chung, Hyung Won and Hou, Le and Longpre, Shayne and Zoph, Barret and Tay, Yi and Fedus, William and Li, Eric and Wang, Xuezhi and Dehghani, Mostafa and Brahma, Siddhartha and others},
 journal = {arXiv preprint},
 title = {Scaling instruction-finetuned language models},
 url = {https://arxiv.org/abs/2210.11416},
 volume = {abs/2210.11416},
 year = {2022}
}

@article{raffel2020exploring,
 author = {Raffel, Colin and Shazeer, Noam and Roberts, Adam and Lee, Katherine and Narang, Sharan and Matena, Michael and Zhou, Yanqi and Li, Wei and Liu, Peter J and others},
 journal = {J. Mach. Learn. Res.},
 number = {140},
 pages = {1--67},
 title = {Exploring the limits of transfer learning with a unified text-to-text transformer.},
 volume = {21},
 year = {2020}
}

@article{anil2022exploring,
 author = {Anil, Cem and Wu, Yuhuai and Andreassen, Anders and Lewkowycz, Aitor and Misra, Vedant and Ramasesh, Vinay and Slone, Ambrose and Gur-Ari, Guy and Dyer, Ethan and Neyshabur, Behnam},
 journal = {arXiv preprint},
 title = {Exploring length generalization in large language models},
 url = {https://arxiv.org/abs/2207.04901},
 volume = {abs/2207.04901},
 year = {2022}
}

@inproceedings{zelikman2022star,
 author = {Eric Zelikman and Yuhuai Wu and Jesse Mu and Noah Goodman},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {Alice H. Oh and Alekh Agarwal and Danielle Belgrave and Kyunghyun Cho},
 title = {{ST}aR: Bootstrapping Reasoning With Reasoning},
 url = {https://openreview.net/forum?id=-3ELRdg2sgI},
 year = {2022}
}

@article{huang2022large2,
 author = {Huang, Jiaxin and Gu, Shixiang Shane and Hou, Le and Wu, Yuexin and Wang, Xuezhi and Yu, Hongkun and Han, Jiawei},
 journal = {arXiv preprint},
 title = {Large language models can self-improve},
 url = {https://arxiv.org/abs/2210.11610},
 volume = {abs/2210.11610},
 year = {2022}
}

@inproceedings{roy2015solving,
 address = {Lisbon, Portugal},
 author = {Roy, Subhro  and
Roth, Dan},
 booktitle = {Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing},
 doi = {10.18653/v1/D15-1202},
 pages = {1743--1752},
 publisher = {Association for Computational Linguistics},
 title = {Solving General Arithmetic Word Problems},
 url = {https://aclanthology.org/D15-1202},
 year = {2015}
}

@inproceedings{amini2019mathqa,
 address = {Minneapolis, Minnesota},
 author = {Amini, Aida  and
Gabriel, Saadia  and
Lin, Shanchuan  and
Koncel-Kedziorski, Rik  and
Choi, Yejin  and
Hajishirzi, Hannaneh},
 booktitle = {Proceedings of the 2019 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)},
 doi = {10.18653/v1/N19-1245},
 pages = {2357--2367},
 publisher = {Association for Computational Linguistics},
 title = {{M}ath{QA}: Towards Interpretable Math Word Problem Solving with Operation-Based Formalisms},
 url = {https://aclanthology.org/N19-1245},
 year = {2019}
}

@inproceedings{talmor2019commonsenseqa,
 address = {Minneapolis, Minnesota},
 author = {Talmor, Alon  and
Herzig, Jonathan  and
Lourie, Nicholas  and
Berant, Jonathan},
 booktitle = {Proceedings of the 2019 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)},
 doi = {10.18653/v1/N19-1421},
 pages = {4149--4158},
 publisher = {Association for Computational Linguistics},
 title = {{C}ommonsense{QA}: A Question Answering Challenge Targeting Commonsense Knowledge},
 url = {https://aclanthology.org/N19-1421},
 year = {2019}
}

@article{geva2021did,
 address = {Cambridge, MA},
 author = {Geva, Mor  and
Khashabi, Daniel  and
Segal, Elad  and
Khot, Tushar  and
Roth, Dan  and
Berant, Jonathan},
 doi = {10.1162/tacl-a-00370},
 journal = {Transactions of the Association for Computational Linguistics},
 pages = {346--361},
 publisher = {MIT Press},
 title = {Did Aristotle Use a Laptop? A Question Answering Benchmark with Implicit Reasoning Strategies},
 url = {https://aclanthology.org/2021.tacl-1.21},
 volume = {9},
 year = {2021}
}

@article{ahn2022can,
 author = {Ahn, Michael and Brohan, Anthony and Brown, Noah and Chebotar, Yevgen and Cortes, Omar and David, Byron and Finn, Chelsea and Gopalakrishnan, Keerthana and Hausman, Karol and Herzog, Alex and others},
 journal = {arXiv preprint},
 title = {Do as i can, not as i say: Grounding language in robotic affordances},
 url = {https://arxiv.org/abs/2204.01691},
 volume = {abs/2204.01691},
 year = {2022}
}

@article{liu2021pre,
 author = {Liu, Pengfei and Yuan, Weizhe and Fu, Jinlan and Jiang, Zhengbao and Hayashi, Hiroaki and Neubig, Graham},
 journal = {arXiv preprint},
 title = {Pre-train, prompt, and predict: A systematic survey of prompting methods in natural language processing},
 url = {https://arxiv.org/abs/2107.13586},
 volume = {abs/2107.13586},
 year = {2021}
}

@article{srivastava2022beyond,
 author = {Srivastava, Aarohi and Rastogi, Abhinav and Rao, Abhishek and Shoeb, Abu Awal Md and Abid, Abubakar and Fisch, Adam and Brown, Adam R and Santoro, Adam and Gupta, Aditya and Garriga-Alonso, Adri{\`a} and others},
 journal = {arXiv preprint},
 title = {Beyond the Imitation Game: Quantifying and extrapolating the capabilities of language models},
 url = {https://arxiv.org/abs/2206.04615},
 volume = {abs/2206.04615},
 year = {2022}
}

@article{suzgun2022challenging,
 author = {Suzgun, Mirac and Scales, Nathan and Sch{\"a}rli, Nathanael and Gehrmann, Sebastian and Tay, Yi and Chung, Hyung Won and Chowdhery, Aakanksha and Le, Quoc V and Chi, Ed H and Zhou, Denny and others},
 journal = {arXiv preprint},
 title = {Challenging BIG-Bench tasks and whether chain-of-thought can solve them},
 url = {https://arxiv.org/abs/2210.09261},
 volume = {abs/2210.09261},
 year = {2022}
}

@article{drozdov2022compositional,
 author = {Drozdov, Andrew and Sch{\"a}rli, Nathanael and Aky{\"u}rek, Ekin and Scales, Nathan and Song, Xinying and Chen, Xinyun and Bousquet, Olivier and Zhou, Denny},
 journal = {arXiv preprint},
 title = {Compositional semantic parsing with large language models},
 url = {https://arxiv.org/abs/2209.15003},
 volume = {abs/2209.15003},
 year = {2022}
}

@inproceedings{yang-etal-2022-seqzero,
 address = {Seattle, United States},
 author = {Yang, Jingfeng  and
Jiang, Haoming  and
Yin, Qingyu  and
Zhang, Danqing  and
Yin, Bing  and
Yang, Diyi},
 booktitle = {Findings of the Association for Computational Linguistics: NAACL 2022},
 doi = {10.18653/v1/2022.findings-naacl.5},
 pages = {49--60},
 publisher = {Association for Computational Linguistics},
 title = {{SEQZERO}: Few-shot Compositional Semantic Parsing with Sequential Prompts and Zero-shot Models},
 url = {https://aclanthology.org/2022.findings-naacl.5},
 year = {2022}
}

@inproceedings{lewis2020bart,
 address = {Online},
 author = {Lewis, Mike  and
Liu, Yinhan  and
Goyal, Naman  and
Ghazvininejad, Marjan  and
Mohamed, Abdelrahman  and
Levy, Omer  and
Stoyanov, Veselin  and
Zettlemoyer, Luke},
 booktitle = {Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics},
 doi = {10.18653/v1/2020.acl-main.703},
 pages = {7871--7880},
 publisher = {Association for Computational Linguistics},
 title = {{BART}: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension},
 url = {https://aclanthology.org/2020.acl-main.703},
 year = {2020}
}

@inproceedings{perez2020unsupervised,
 address = {Online},
 author = {Perez, Ethan  and
Lewis, Patrick  and
Yih, Wen-tau  and
Cho, Kyunghyun  and
Kiela, Douwe},
 booktitle = {Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)},
 doi = {10.18653/v1/2020.emnlp-main.713},
 pages = {8864--8880},
 publisher = {Association for Computational Linguistics},
 title = {Unsupervised Question Decomposition for Question Answering},
 url = {https://aclanthology.org/2020.emnlp-main.713},
 year = {2020}
}

@inproceedings{min2019multi,
 address = {Florence, Italy},
 author = {Min, Sewon  and
Zhong, Victor  and
Zettlemoyer, Luke  and
Hajishirzi, Hannaneh},
 booktitle = {Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics},
 doi = {10.18653/v1/P19-1613},
 pages = {6097--6109},
 publisher = {Association for Computational Linguistics},
 title = {Multi-hop Reading Comprehension through Question Decomposition and Rescoring},
 url = {https://aclanthology.org/P19-1613},
 year = {2019}
}

@inproceedings{talmor2018web,
 address = {New Orleans, Louisiana},
 author = {Talmor, Alon  and
Berant, Jonathan},
 booktitle = {Proceedings of the 2018 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers)},
 doi = {10.18653/v1/N18-1059},
 pages = {641--651},
 publisher = {Association for Computational Linguistics},
 title = {The Web as a Knowledge-Base for Answering Complex Questions},
 url = {https://aclanthology.org/N18-1059},
 year = {2018}
}

@article{dua2022successive,
 author = {Dua, Dheeru and Gupta, Shivanshu and Singh, Sameer and Gardner, Matt},
 journal = {arXiv preprint},
 title = {Successive Prompting for Decomposing Complex Questions},
 url = {https://arxiv.org/abs/2212.04092},
 volume = {abs/2212.04092},
 year = {2022}
}

@article{jung2022maieutic,
 author = {Jung, Jaehun and Qin, Lianhui and Welleck, Sean and Brahman, Faeze and Bhagavatula, Chandra and Bras, Ronan Le and Choi, Yejin},
 journal = {The 2022 Conference on Empirical Methods for Natural Language Processing},
 title = {Maieutic Prompting: Logically Consistent Reasoning with Recursive Explanations},
 year = {2022}
}

@article{creswell2022selection,
 author = {Creswell, Antonia and Shanahan, Murray and Higgins, Irina},
 journal = {arXiv preprint},
 title = {Selection-Inference: Exploiting Large Language Models for Interpretable Logical Reasoning},
 url = {https://arxiv.org/abs/2205.09712},
 volume = {abs/2205.09712},
 year = {2022}
}

@article{creswell2022faithful,
 author = {Creswell, Antonia and Shanahan, Murray},
 journal = {arXiv preprint},
 title = {Faithful reasoning using large language models},
 url = {https://arxiv.org/abs/2208.14271},
 volume = {abs/2208.14271},
 year = {2022}
}

@inproceedings{Wang2022iteratively,
 author = {Wang, Boshi and Deng, Xiang and Sun, Huan},
 booktitle = {The 2022 Conference on Empirical Methods for Natural Language Processing},
 title = {Iteratively Prompt Pre-trained Language Models for Chain of Thought},
 year = {2022}
}

@inproceedings{lu2022learn,
 author = {Pan Lu and Swaroop Mishra and Tony Xia and Liang Qiu and Kai-Wei Chang and Song-Chun Zhu and Oyvind Tafjord and Peter Clark and Ashwin Kalyan},
 booktitle = {NeurIPS},
 title = {Learn to Explain: Multimodal Reasoning via Thought Chains for Science Question Answering},
 year = {2022}
}

@inproceedings{pmlr-v162-huang22a,
 author = {Huang, Wenlong and Abbeel, Pieter and Pathak, Deepak and Mordatch, Igor},
 booktitle = {Proceedings of the 39th International Conference on Machine Learning},
 editor = {Chaudhuri, Kamalika and Jegelka, Stefanie and Song, Le and Szepesvari, Csaba and Niu, Gang and Sabato, Sivan},
 pages = {9118--9147},
 publisher = {PMLR},
 series = {Proceedings of Machine Learning Research},
 title = {Language Models as Zero-Shot Planners: Extracting Actionable Knowledge for Embodied Agents},
 url = {https://proceedings.mlr.press/v162/huang22a.html},
 volume = {162},
 year = {2022}
}

@inproceedings{huanginner,
 author = {Huang, Wenlong and Xia, Fei and Xiao, Ted and Chan, Harris and Liang, Jacky and Florence, Pete and Zeng, Andy and Tompson, Jonathan and Mordatch, Igor and Chebotar, Yevgen and others},
 booktitle = {2022 Conference on Robot Learning},
 title = {Inner Monologue: Embodied Reasoning through Planning with Language Models},
 year = {2022}
}

@article{song2022llm,
 author = {Song, Chan Hee and Wu, Jiaman and Washington, Clayton and Sadler, Brian M and Chao, Wei-Lun and Su, Yu},
 journal = {arXiv preprint},
 title = {LLM-Planner: Few-Shot Grounded Planning for Embodied Agents with Large Language Models},
 url = {https://arxiv.org/abs/2212.04088},
 volume = {abs/2212.04088},
 year = {2022}
}

@inproceedings{rajani2019explain,
 address = {Florence, Italy},
 author = {Rajani, Nazneen Fatema  and
McCann, Bryan  and
Xiong, Caiming  and
Socher, Richard},
 booktitle = {Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics},
 doi = {10.18653/v1/P19-1487},
 pages = {4932--4942},
 publisher = {Association for Computational Linguistics},
 title = {Explain Yourself! Leveraging Language Models for Commonsense Reasoning},
 url = {https://aclanthology.org/P19-1487},
 year = {2019}
}

@article{radford2018improving,
 author = {Radford, Alec and Narasimhan, Karthik and Salimans, Tim and Sutskever, Ilya and others},
 publisher = {OpenAI},
 title = {Improving language understanding by generative pre-training},
 year = {2018}
}

@inproceedings{talmor2020leap,
 author = {Alon Talmor and
Oyvind Tafjord and
Peter Clark and
Yoav Goldberg and
Jonathan Berant},
 bibsource = {dblp computer science bibliography, https://dblp.org},
 biburl = {https://dblp.org/rec/conf/nips/TalmorTCGB20.bib},
 booktitle = {Advances in Neural Information Processing Systems 33: Annual Conference
on Neural Information Processing Systems 2020, NeurIPS 2020, December
6-12, 2020, virtual},
 editor = {Hugo Larochelle and
Marc'Aurelio Ranzato and
Raia Hadsell and
Maria{-}Florina Balcan and
Hsuan{-}Tien Lin},
 timestamp = {Tue, 19 Jan 2021 00:00:00 +0100},
 title = {Leap-Of-Thought: Teaching Pre-Trained Models to Systematically Reason
Over Implicit Knowledge},
 url = {https://proceedings.neurips.cc/paper/2020/hash/e992111e4ab9985366e806733383bd8c-Abstract.html},
 year = {2020}
}

@article{liu2019roberta,
 author = {Liu, Yinhan and Ott, Myle and Goyal, Naman and Du, Jingfei and Joshi, Mandar and Chen, Danqi and Levy, Omer and Lewis, Mike and Zettlemoyer, Luke and Stoyanov, Veselin},
 journal = {arXiv preprint},
 title = {Roberta: A robustly optimized bert pretraining approach},
 url = {https://arxiv.org/abs/1907.11692},
 volume = {abs/1907.11692},
 year = {2019}
}

@inproceedings{dan2021@measuring,
 author = {Hendrycks, Dan and Burns, Collin and Kadavath, Saurav and Arora, Akul and Basart, Steven and Tang, Eric and Song, Dawn and Steinhardt, Jacob},
 booktitle = {Proceedings of the Neural Information Processing Systems Track on Datasets and Benchmarks},
 editor = {J. Vanschoren and S. Yeung},
 pages = {},
 title = {Measuring Mathematical Problem Solving With the MATH Dataset},
 url = {https://datasets-benchmarks-proceedings.neurips.cc/paper/2021/file/be83ab3ecd0db773eb2dc1b0a17836a1-Paper-round2.pdf},
 volume = {1},
 year = {2021}
}

@inproceedings{valmeekam2022large,
 author = {Valmeekam, Karthik and Olmo, Alberto and Sreedharan, Sarath and Kambhampati, Subbarao},
 booktitle = {NeurIPS 2022 Foundation Models for Decision Making Workshop},
 title = {Large Language Models Still Can't Plan (A Benchmark for LLMs on Planning and Reasoning about Change)},
 year = {2022}
}

@article{ruis2022large,
 author = {Ruis, Laura and Khan, Akbir and Biderman, Stella and Hooker, Sara and Rockt{\"a}schel, Tim and Grefenstette, Edward},
 journal = {arXiv preprint},
 title = {Large language models are not zero-shot communicators},
 url = {https://arxiv.org/abs/2210.14986},
 volume = {abs/2210.14986},
 year = {2022}
}

@article{han2022folio,
 author = {Han, Simeng and Schoelkopf, Hailey and Zhao, Yilun and Qi, Zhenting and Riddell, Martin and Benson, Luke and Sun, Lucy and Zubova, Ekaterina and Qiao, Yujie and Burtell, Matthew and others},
 journal = {arXiv preprint},
 title = {Folio: Natural language reasoning with first-order logic},
 url = {https://arxiv.org/abs/2209.00840},
 volume = {abs/2209.00840},
 year = {2022}
}

@article{scao2022bloom,
 author = {Scao, Teven Le and Fan, Angela and Akiki, Christopher and Pavlick, Ellie and Ili{\'c}, Suzana and Hesslow, Daniel and Castagn{\'e}, Roman and Luccioni, Alexandra Sasha and Yvon, Fran{\c{c}}ois and Gall{\'e}, Matthias and others},
 journal = {arXiv preprint},
 title = {BLOOM: A 176B-Parameter Open-Access Multilingual Language Model},
 url = {https://arxiv.org/abs/2211.05100},
 volume = {abs/2211.05100},
 year = {2022}
}

@book{levi2013introduction,
 author = {Levi, Edward H},
 publisher = {University of Chicago Press},
 title = {An introduction to legal reasoning},
 year = {2013}
}

@article{edwards1954theory,
 author = {Edwards, Ward},
 journal = {Psychological bulletin},
 number = {4},
 pages = {380},
 publisher = {American Psychological Association},
 title = {The theory of decision making.},
 volume = {51},
 year = {1954}
}

@article{zimmerman2000development,
 author = {Zimmerman, Corinne},
 journal = {Developmental review},
 number = {1},
 pages = {99--149},
 publisher = {Elsevier},
 title = {The development of scientific reasoning skills},
 volume = {20},
 year = {2000}
}

@article{li2022explanations,
 author = {Li, Shiyang and Chen, Jianshu and Shen, Yelong and Chen, Zhiyu and Zhang, Xinlu and Li, Zekun and Wang, Hong and Qian, Jing and Peng, Baolin and Mao, Yi and others},
 journal = {arXiv preprint},
 title = {Explanations from Large Language Models Make Small Reasoners Better},
 url = {https://arxiv.org/abs/2210.06726},
 volume = {abs/2210.06726},
 year = {2022}
}

@article{shridhar2022distilling,
 author = {Shridhar, Kumar and Stolfo, Alessandro and Sachan, Mrinmaya},
 journal = {arXiv preprint},
 title = {Distilling Multi-Step Reasoning Capabilities of Large Language Models into Smaller Models via Semantic Decompositions},
 url = {https://arxiv.org/abs/2212.00193},
 volume = {abs/2212.00193},
 year = {2022}
}

@inproceedings{patel2021nlp,
 address = {Online},
 author = {Patel, Arkil  and
Bhattamishra, Satwik  and
Goyal, Navin},
 booktitle = {Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies},
 doi = {10.18653/v1/2021.naacl-main.168},
 pages = {2080--2094},
 publisher = {Association for Computational Linguistics},
 title = {Are {NLP} Models really able to Solve Simple Math Word Problems?},
 url = {https://aclanthology.org/2021.naacl-main.168},
 year = {2021}
}

@article{dohan2022language,
 author = {Dohan, David and Xu, Winnie and Lewkowycz, Aitor and Austin, Jacob and Bieber, David and Lopes, Raphael Gontijo and Wu, Yuhuai and Michalewski, Henryk and Saurous, Rif A and Sohl-Dickstein, Jascha and others},
 journal = {arXiv preprint},
 title = {Language model cascades},
 url = {https://arxiv.org/abs/2207.10342},
 volume = {abs/2207.10342},
 year = {2022}
}

@book{wason1972psychology,
 author = {Wason, Peter Cathcart and Johnson-Laird, Philip Nicholas},
 publisher = {Harvard University Press},
 title = {Psychology of reasoning: Structure and content},
 volume = {86},
 year = {1972}
}

@book{fagin2004reasoning,
 author = {Fagin, Ronald and Halpern, Joseph Y and Moses, Yoram and Vardi, Moshe},
 publisher = {MIT press},
 title = {Reasoning about knowledge},
 year = {2004}
}

@article{mchugh2018reasoning,
 author = {McHugh, Conor and Way, Jonathan},
 journal = {Mind},
 number = {505},
 pages = {167--196},
 publisher = {Oxford University Press},
 title = {What is reasoning?},
 volume = {127},
 year = {2018}
}

@article{wason1968reasoning,
 author = {Wason, Peter C},
 journal = {Quarterly journal of experimental psychology},
 number = {3},
 pages = {273--281},
 publisher = {SAGE Publications Sage UK: London, England},
 title = {Reasoning about a rule},
 volume = {20},
 year = {1968}
}

@inproceedings{lampinen2022can,
 author = {Lampinen, Andrew K and Dasgupta, Ishita and Chan, Stephanie CY and Matthewson, Kory and Tessler, Michael Henry and Creswell, Antonia and McClelland, James L and Wang, Jane X and Hill, Felix},
 booktitle = {Findings of the Association for Computational Linguistics: EMNLP 2022},
 title = {Can language models learn from explanations in context?},
 year = {2022}
}

@article{shoeybi2019megatron,
 author = {Shoeybi, Mohammad and Patwary, Mostofa and Puri, Raul and LeGresley, Patrick and Casper, Jared and Catanzaro, Bryan},
 journal = {arXiv preprint},
 title = {Megatron-lm: Training multi-billion parameter language models using model parallelism},
 url = {https://arxiv.org/abs/1909.08053},
 volume = {abs/1909.08053},
 year = {2019}
}

@article{passmore1961philosophical,
 author = {Passmore, John Arthur},
 title = {Philosophical reasoning},
 year = {1961}
}

@book{huth2004logic,
 author = {Huth, Michael and Ryan, Mark},
 publisher = {Cambridge university press},
 title = {Logic in Computer Science: Modelling and reasoning about systems},
 year = {2004}
}

@article{qiu2020pre,
 author = {Qiu, Xipeng and Sun, Tianxiang and Xu, Yige and Shao, Yunfan and Dai, Ning and Huang, Xuanjing},
 journal = {Science China Technological Sciences},
 number = {10},
 pages = {1872--1897},
 publisher = {Springer},
 title = {Pre-trained models for natural language processing: A survey},
 volume = {63},
 year = {2020}
}

@inproceedings{miao2020diverse,
 address = {Online},
 author = {Miao, Shen-yun  and
Liang, Chao-Chun  and
Su, Keh-Yih},
 booktitle = {Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics},
 doi = {10.18653/v1/2020.acl-main.92},
 pages = {975--984},
 publisher = {Association for Computational Linguistics},
 title = {A Diverse Corpus for Evaluating and Developing {E}nglish Math Word Problem Solvers},
 url = {https://aclanthology.org/2020.acl-main.92},
 year = {2020}
}

@inproceedings{ling2017program,
 address = {Vancouver, Canada},
 author = {Ling, Wang  and
Yogatama, Dani  and
Dyer, Chris  and
Blunsom, Phil},
 booktitle = {Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
 doi = {10.18653/v1/P17-1015},
 pages = {158--167},
 publisher = {Association for Computational Linguistics},
 title = {Program Induction by Rationale Generation: Learning to Solve and Explain Algebraic Word Problems},
 url = {https://aclanthology.org/P17-1015},
 year = {2017}
}

@article{clark2018think,
 author = {Clark, Peter and Cowhey, Isaac and Etzioni, Oren and Khot, Tushar and Sabharwal, Ashish and Schoenick, Carissa and Tafjord, Oyvind},
 journal = {arXiv preprint},
 title = {Think you have solved question answering? try arc, the ai2 reasoning challenge},
 url = {https://arxiv.org/abs/1803.05457},
 volume = {abs/1803.05457},
 year = {2018}
}

@inproceedings{pasupat2015compositional,
 address = {Beijing, China},
 author = {Pasupat, Panupong  and
Liang, Percy},
 booktitle = {Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)},
 doi = {10.3115/v1/P15-1142},
 pages = {1470--1480},
 publisher = {Association for Computational Linguistics},
 title = {Compositional Semantic Parsing on Semi-Structured Tables},
 url = {https://aclanthology.org/P15-1142},
 year = {2015}
}

@article{nan2022fetaqa,
 address = {Cambridge, MA},
 author = {Nan, Linyong  and
Hsieh, Chiachun  and
Mao, Ziming  and
Lin, Xi Victoria  and
Verma, Neha  and
Zhang, Rui  and
Kry{\'s}ci{\'n}ski, Wojciech  and
Schoelkopf, Hailey  and
Kong, Riley  and
Tang, Xiangru  and
Mutuma, Mutethia  and
Rosand, Ben  and
Trindade, Isabel  and
Bandaru, Renusree  and
Cunningham, Jacob  and
Xiong, Caiming  and
Radev, Dragomir  and
Radev, Dragomir},
 doi = {10.1162/tacl-a-00446},
 journal = {Transactions of the Association for Computational Linguistics},
 pages = {35--49},
 publisher = {MIT Press},
 title = {{F}e{T}a{QA}: Free-form Table Question Answering},
 url = {https://aclanthology.org/2022.tacl-1.3},
 volume = {10},
 year = {2022}
}

@article{magister2022@teaching,
 author = {Magister, Lucie Charlotte and Mallinson, Jonathan and Adamek, Jakub and Malmi, Eric and Severyn, Aliaksei},
 journal = {arXiv preprint},
 title = {Teaching Small Language Models to Reason},
 url = {https://arxiv.org/abs/2212.08410},
 volume = {abs/2212.08410},
 year = {2022}
}

@article{golovneva2022roscoe,
 author = {Golovneva, Olga and Chen, Moya and Poff, Spencer and Corredor, Martin and Zettlemoyer, Luke and Fazel-Zarandi, Maryam and Celikyilmaz, Asli},
 journal = {arXiv preprint},
 title = {ROSCOE: A Suite of Metrics for Scoring Step-by-Step Reasoning},
 url = {https://arxiv.org/abs/2212.07919},
 volume = {abs/2212.07919},
 year = {2022}
}

@inproceedings{pi2022reasoning,
 author = {Pi, Xinyu and Liu, Qian and Chen, Bei and Ziyadi, Morteza and Lin, Zeqi and Gao, Yan and Fu, Qiang and Lou, Jian-Guang and Chen, Weizhu},
 booktitle = {Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing (EMNLP)},
 title = {Reasoning like program executors},
 year = {2022}
}

@inproceedings{madaan2022language,
 author = {Madaan, Aman and Zhou, Shuyan and Alon, Uri and Yang, Yiming and Neubig, Graham},
 booktitle = {Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing (EMNLP)},
 title = {Language models of code are few-shot commonsense learners},
 year = {2022}
}

@article{gao2022pal,
 author = {Gao, Luyu and Madaan, Aman and Zhou, Shuyan and Alon, Uri and Liu, Pengfei and Yang, Yiming and Callan, Jamie and Neubig, Graham},
 journal = {arXiv preprint},
 title = {PAL: Program-aided Language Models},
 url = {https://arxiv.org/abs/2211.10435},
 volume = {abs/2211.10435},
 year = {2022}
}

@article{press2022measuring,
 author = {Press, Ofir and Zhang, Muru and Min, Sewon and Schmidt, Ludwig and Smith, Noah A and Lewis, Mike},
 journal = {arXiv preprint},
 title = {Measuring and Narrowing the Compositionality Gap in Language Models},
 url = {https://arxiv.org/abs/2210.03350},
 volume = {abs/2210.03350},
 year = {2022}
}

@article{ye2022unreliability,
 author = {Ye, Xi and Durrett, Greg},
 journal = {Advances in neural information processing systems},
 title = {The unreliability of explanations in few-shot prompting for textual reasoning},
 year = {2022}
}

@article{weng2022large,
 author = {Weng, Yixuan and Zhu, Minjun and He, Shizhu and Liu, Kang and Zhao, Jun},
 journal = {arXiv preprint},
 title = {Large Language Models are reasoners with Self-Verification},
 url = {https://arxiv.org/abs/2212.09561},
 volume = {abs/2212.09561},
 year = {2022}
}

@article{chen2022program,
 author = {Chen, Wenhu and Ma, Xueguang and Wang, Xinyi and Cohen, William W},
 journal = {arXiv preprint},
 title = {Program of thoughts prompting: Disentangling computation from reasoning for numerical reasoning tasks},
 url = {https://arxiv.org/abs/2211.12588},
 volume = {abs/2211.12588},
 year = {2022}
}

@misc{fu-khot-peng-2022b, title={How does GPT Obtain its Ability? Tracing Emergent Abilities of Language Models to their Sources}, url={https://yaofu.notion.site/How-does-GPT-Obtain-its-Ability-Tracing-Emergent-Abilities-of-Language-Models-to-their-Sources-b9a57ac0fcf74f30a1ab9e3e36fa1dc1}, author={Fu, Yao and Peng, Hao and Khot, Tushar}, year={2022}, month={Dec}} 


@article{he2022rethinking,
 author = {He, Hangfeng and Zhang, Hongming and Roth, Dan},
 journal = {arXiv preprint},
 title = {Rethinking with Retrieval: Faithful Large Language Model Inference},
 url = {https://arxiv.org/abs/2301.00303},
 volume = {abs/2301.00303},
 year = {2023}
}

@inproceedings{helwe2021reasoning,
 author = {Helwe, Chadi and Clavel, Chlo{\'e} and Suchanek, Fabian M},
 booktitle = {3rd Conference on Automated Knowledge Base Construction},
 title = {Reasoning with transformer-based models: Deep learning, but shallow reasoning},
 year = {2021}
}

@article{kazemi2022lambada,
 author = {Kazemi, Seyed Mehran and Kim, Najoung and Bhatia, Deepti and Xu, Xin and Ramachandran, Deepak},
 journal = {arXiv preprint},
 title = {LAMBADA: Backward Chaining for Automated Reasoning in Natural Language},
 url = {https://arxiv.org/abs/2212.13894},
 volume = {abs/2212.13894},
 year = {2022}
}

@article{wang2022towards,
 author = {Wang, Boshi and Min, Sewon and Deng, Xiang and Shen, Jiaming and Wu, You and Zettlemoyer, Luke and Sun, Huan},
 journal = {arXiv preprint},
 title = {Towards Understanding Chain-of-Thought Prompting: An Empirical Study of What Matters},
 url = {https://arxiv.org/abs/2212.10001},
 volume = {abs/2212.10001},
 year = {2022}
}

@article{taylor2022galactica,
 author = {Taylor, Ross and Kardas, Marcin and Cucurull, Guillem and Scialom, Thomas and Hartshorn, Anthony and Saravia, Elvis and Poulton, Andrew and Kerkez, Viktor and Stojnic, Robert},
 journal = {arXiv preprint},
 title = {Galactica: A large language model for science},
 url = {https://arxiv.org/abs/2211.09085},
 volume = {abs/2211.09085},
 year = {2022}
}

@article{yang2022language,
 author = {Yang, Zonglin and Dong, Li and Du, Xinya and Cheng, Hao and Cambria, Erik and Liu, Xiaodong and Gao, Jianfeng and Wei, Furu},
 journal = {arXiv preprint},
 title = {Language Models as Inductive Reasoners},
 url = {https://arxiv.org/abs/2212.10923},
 volume = {abs/2212.10923},
 year = {2022}
}

@article{misra2022property,
 author = {Misra, Kanishka and Rayz, Julia Taylor and Ettinger, Allyson},
 journal = {arXiv preprint},
 title = {A Property Induction Framework for Neural Language Models},
 url = {https://arxiv.org/abs/2205.06910},
 volume = {abs/2205.06910},
 year = {2022}
}

@article{han2022human,
 author = {Han, Simon Jerome and Ransom, Keith and Perfors, Andrew and Kemp, Charles},
 publisher = {PsyarXiv},
 title = {Human-like property induction is a challenge for large language models},
 year = {2022}
}

@article{yu2022alert,
 author = {Yu, Ping and Wang, Tianlu and Golovneva, Olga and Alkhamissy, Badr and Ghosh, Gargi and Diab, Mona and Celikyilmaz, Asli},
 journal = {arXiv preprint},
 title = {ALERT: Adapting Language Models to Reasoning Tasks},
 url = {https://arxiv.org/abs/2212.08286},
 volume = {abs/2212.08286},
 year = {2022}
}

@article{zhang2022opt,
 author = {Zhang, Susan and Roller, Stephen and Goyal, Naman and Artetxe, Mikel and Chen, Moya and Chen, Shuohui and Dewan, Christopher and Diab, Mona and Li, Xian and Lin, Xi Victoria and others},
 journal = {arXiv preprint},
 title = {Opt: Open pre-trained transformer language models},
 url = {https://arxiv.org/abs/2205.01068},
 volume = {abs/2205.01068},
 year = {2022}
}

@article{radford2019language,
 author = {Radford, Alec and Wu, Jeffrey and Child, Rewon and Luan, David and Amodei, Dario and Sutskever, Ilya and others},
 journal = {OpenAI blog},
 number = {8},
 pages = {9},
 title = {Language models are unsupervised multitask learners},
 volume = {1},
 year = {2019}
}

@inproceedings{kenton2019bert,
 address = {Minneapolis, Minnesota},
 author = {Devlin, Jacob  and
Chang, Ming-Wei  and
Lee, Kenton  and
Toutanova, Kristina},
 booktitle = {Proceedings of the 2019 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)},
 doi = {10.18653/v1/N19-1423},
 pages = {4171--4186},
 publisher = {Association for Computational Linguistics},
 title = {{BERT}: Pre-training of Deep Bidirectional Transformers for Language Understanding},
 url = {https://aclanthology.org/N19-1423},
 year = {2019}
}

@article{openai2022chatgpt,
 author = {OpenAI},
 journal = {OpenAI},
 title = {Chatgpt: Optimizing language models for dialogue},
 year = {2022}
}

@article{qiao2022reasoning,
 author = {Qiao, Shuofei and Ou, Yixin and Zhang, Ningyu and Chen, Xiang and Yao, Yunzhi and Deng, Shumin and Tan, Chuanqi and Huang, Fei and Chen, Huajun},
 journal = {arXiv preprint},
 title = {Reasoning with Language Model Prompting: A Survey},
 url = {https://arxiv.org/abs/2212.09597},
 volume = {abs/2212.09597},
 year = {2022}
}

@article{ye2023large,
 author = {Ye, Yunhu and Hui, Binyuan and Yang, Min and Li, Binhua and Huang, Fei and Li, Yongbin},
 journal = {arXiv preprint},
 title = {Large language models are versatile decomposers: Decompose evidence and questions for table-based reasoning},
 url = {https://arxiv.org/abs/2301.13808},
 volume = {abs/2301.13808},
 year = {2023}
}

@article{liu2022dimongen,
 author = {Liu, Chenzhengyi and Huang, Jie and Zhu, Kerui and Chang, Kevin Chen-Chuan},
 journal = {arXiv preprint},
 title = {DimonGen: Diversified Generative Commonsense Reasoning for Explaining Concept Relationships},
 url = {https://arxiv.org/abs/2212.10545},
 volume = {abs/2212.10545},
 year = {2022}
}

@article{zheng2023does,
 author = {Zheng, Shen and Huang, Jie and Chang, Kevin Chen-Chuan},
 journal = {arXiv preprint},
 title = {Why Does ChatGPT Fall Short in Providing Truthful Answers?},
 url = {https://arxiv.org/abs/2304.10513},
 volume = {abs/2304.10513},
 year = {2023}
}

@article{bang2023multitask,
 author = {Bang, Yejin and Cahyawijaya, Samuel and Lee, Nayeon and Dai, Wenliang and Su, Dan and Wilie, Bryan and Lovenia, Holy and Ji, Ziwei and Yu, Tiezheng and Chung, Willy and others},
 journal = {arXiv preprint},
 title = {A multitask, multilingual, multimodal evaluation of chatgpt on reasoning, hallucination, and interactivity},
 url = {https://arxiv.org/abs/2302.04023},
 volume = {abs/2302.04023},
 year = {2023}
}

@inproceedings{wang2023decodingtrust,
  title={DecodingTrust: A Comprehensive Assessment of Trustworthiness in GPT Models.},
  author={Wang, Boxin and Chen, Weixin and Pei, Hengzhi and Xie, Chulin and Kang, Mintong and Zhang, Chenhui and Xu, Chejian and Xiong, Zidi and Dutta, Ritik and Schaeffer, Rylan and others},
  booktitle={NeurIPS},
  year={2023}
}



@inproceedings{not-think,
    title = "On Second Thought, Let`s Not Think Step by Step! Bias and Toxicity in Zero-Shot Reasoning",
    author = "Shaikh, Omar  and
      Zhang, Hongxin  and
      Held, William  and
      Bernstein, Michael  and
      Yang, Diyi",
    booktitle = {ACL},
    year = {2023}
}


@inproceedings{han-etal-2024-context,
    title = "In-Context Learning May Not Elicit Trustworthy Reasoning: A-Not-{B} Errors in Pretrained Language Models",
    author = "Han, Pengrui  and
      Song, Peiyang  and
      Yu, Haofei  and
      You, Jiaxuan",
    booktitle = "EMNLP",
    year = "2024"
}

@inproceedings{paul-etal-2024-making,
    title = "Making Reasoning Matter: Measuring and Improving Faithfulness of Chain-of-Thought Reasoning",
    author = "Paul, Debjit  and
      West, Robert  and
      Bosselut, Antoine  and
      Faltings, Boi",
    booktitle = "EMNLP",
    year = "2024"
}

@article{Radhakrishnan2023QuestionDI,
  title={Question Decomposition Improves the Faithfulness of Model-Generated Reasoning},
  author={Ansh Radhakrishnan and Karina Nguyen and Anna Chen and Carol Chen and Carson E. Denison and Danny Hernandez and Esin Durmus and Evan Hubinger and John Kernion and Kamil.e Lukovsiut.e and Newton Cheng and Nicholas Joseph and Nicholas Schiefer and Oliver Rausch and Sam McCandlish and Sheer El Showk and Tamera Lanham and Tim Maxwell and Venkat Chandrasekaran and Zac Hatfield-Dodds and Jared Kaplan and Janina Brauner and Sam Bowman and Ethan Perez},
  journal={arXiv},
  year={2023}
}

@article{Chua2024BiasAugmentedCT,
  title={Bias-Augmented Consistency Training Reduces Biased Reasoning in Chain-of-Thought},
  author={James Chua and Edward Rees and Hunar Batra and Samuel R. Bowman and Julian Michael and Ethan Perez and Miles Turpin},
  journal={arXiv},
  year={2024}
}

@article{Meinke2024scheming,
  title={Frontier Models are Capable of In-context Scheming},
  author={Alexander Meinke and Bronson Schoen and J'er'emy Scheurer and Mikita Balesni and Rusheb Shah and Marius Hobbhahn},
  journal={arXiv},
  year={2024}
}

@inproceedings{li-etal-2024-deceptive,
    title = "Deceptive Semantic Shortcuts on Reasoning Chains: How Far Can Models Go without Hallucination?",
    author = "Li, Bangzheng  and
      Zhou, Ben  and
      Wang, Fei  and
      Fu, Xingyu  and
      Roth, Dan  and
      Chen, Muhao",
    booktitle = "NAACL",
    year = "2024",
}

@misc{reflextion7,
  author       = {Matt Shumer},
  title        = {mattshumer/Reflection-Llama-3.1-70B},
  year         = {2024},
  howpublished = {https://huggingface.co/mattshumer/Reflection-Llama-3.1-70B}
}





@inproceedings{multimodel-reasoning,
title={Stop Reasoning! When Multimodal {LLM} with Chain-of-Thought Reasoning Meets Adversarial Image},
author={Zefeng Wang and Zhen Han and Shuo Chen and Fan Xue and Zifeng Ding and Xun Xiao and Volker Tresp and Philip Torr and Jindong Gu},
booktitle={COLM},
year={2024}
}




@inproceedings{Ethics-Reasoning,
    title = "Rethinking Machine Ethics {--} Can {LLM}s Perform Moral Reasoning through the Lens of Moral Theories?",
    author = "Zhou, Jingyan  and
      Hu, Minda  and
      Li, Junan  and
      Zhang, Xiaoying  and
      Wu, Xixin  and
      King, Irwin  and
      Meng, Helen",
    booktitle = "ACL",
    year = "2024",
}

@inproceedings{Persona-Reasoning,
title={Bias Runs Deep: Implicit Reasoning Biases in Persona-Assigned {LLM}s},
author={Shashank Gupta and Vaishnavi Shrivastava and Ameet Deshpande and Ashwin Kalyan and Peter Clark and Ashish Sabharwal and Tushar Khot},
booktitle={ICLR},
year={2024},
}

@article{Individual-Reasoning,
  title={Can Language Models Reason about Individualistic Human Values and Preferences?},
  author={Liwei Jiang and Taylor Sorensen and Sydney Levine and Yejin Choi},
  journal={arXiv},
  year={2024},
}


@article{Hardness-Reasoning,
  title={On the Hardness of Faithful Chain-of-Thought Reasoning in Large Language Models},
  author={Sree Harsha Tanneru and Dan Ley and Chirag Agarwal and Himabindu Lakkaraju},
  journal={arXiv},
  year={2024},
}

@inproceedings{Social-Reasoning,
 author = {Gandhi, Kanishk and Fraenken, Jan-Philipp and Gerstenberg, Tobias and Goodman, Noah},
 booktitle = {NeurIPS},
 title = {Understanding Social Reasoning in Language Models with Language Models},
 year = {2023}
}


@article{Self-correct-reasoning,
  title={Large Language Models Cannot Self-Correct Reasoning Yet},
  author={Jie Huang and Xinyun Chen and Swaroop Mishra and Huaixiu Steven Zheng and Adams Wei Yu and Xinying Song and Denny Zhou},
  journal={arXiv},
  year={2023},
}



@inproceedings{vicuna,
  title={Judging llm-as-a-judge with mt-bench and chatbot arena},
  author={Zheng, Lianmin and Chiang, Wei-Lin and Sheng, Ying and Zhuang, Siyuan and Wu, Zhanghao and Zhuang, Yonghao and Lin, Zi and Li, Zhuohan and Li, Dacheng and Xing, Eric and others},
  booktitle={NeurIPS},
  year={2024}
}


@inproceedings{llama,
      title={Llama 2: Open Foundation and Fine-Tuned Chat Models}, 
      author={Hugo Touvron and Louis Martin and Kevin Stone and Peter Albert and Amjad Almahairi and Yasmine Babaei and Nikolay Bashlykov and Soumya Batra and Prajjwal Bhargava and Shruti Bhosale and Dan Bikel and Lukas Blecher and Cristian Canton Ferrer and Moya Chen and Guillem Cucurull and David Esiobu and Jude Fernandes and Jeremy Fu and Wenyin Fu and Brian Fuller and Cynthia Gao and Vedanuj Goswami and Naman Goyal and Anthony Hartshorn and Saghar Hosseini and Rui Hou and Hakan Inan and Marcin Kardas and Viktor Kerkez and Madian Khabsa and Isabel Kloumann and Artem Korenev and Punit Singh Koura and Marie-Anne Lachaux and Thibaut Lavril and Jenya Lee and Diana Liskovich and Yinghai Lu and Yuning Mao and Xavier Martinet and Todor Mihaylov and Pushkar Mishra and Igor Molybog and Yixin Nie and Andrew Poulton and Jeremy Reizenstein and Rashi Rungta and Kalyan Saladi and Alan Schelten and Ruan Silva and Eric Michael Smith and Ranjan Subramanian and Xiaoqing Ellen Tan and Binh Tang and Ross Taylor and Adina Williams and Jian Xiang Kuan and Puxin Xu and Zheng Yan and Iliyan Zarov and Yuchen Zhang and Angela Fan and Melanie Kambadur and Sharan Narang and Aurelien Rodriguez and Robert Stojnic and Sergey Edunov and Thomas Scialom},
      year={2023},
      booktitle={arXiv},
}


@article{gpt4,
  title={Gpt-4 technical report},
  author={Achiam, Josh and Adler, Steven and Agarwal, Sandhini and Ahmad, Lama and Akkaya, Ilge and Aleman, Florencia Leoni and Almeida, Diogo and Altenschmidt, Janko and Altman, Sam and Anadkat, Shyamal and others},
  journal={arXiv},
  year={2023}
}


@inproceedings{yuan2022wordcraft,
  title={Wordcraft: story writing with large language models},
  author={Yuan, Ann and Coenen, Andy and Reif, Emily and Ippolito, Daphne},
  booktitle={IUI},
  year={2022}
}

@inproceedings{swanson2021story,
  title={Story centaur: Large language model few shot learning as a creative writing tool},
  author={Swanson, Ben and Mathewson, Kory and Pietrzak, Ben and Chen, Sherol and Dinalescu, Monica},
  booktitle={EACL},
  year={2021}
}


@inproceedings{zhang2023prompting,
  title={Prompting large language model for machine translation: A case study},
  author={Zhang, Biao and Haddow, Barry and Birch, Alexandra},
  booktitle={ICML},
  year={2023},
}


@inproceedings{brants2007large,
  title={Large language models in machine translation},
  author={Brants, Thorsten and Popat, Ashok and Xu, Peng and Och, Franz Josef and Dean, Jeffrey},
  booktitle={EMNLP},
  year={2007}
}


@inproceedings{jiang2024enhancing,
  title={Enhancing question answering for enterprise knowledge bases using large language models},
  author={Jiang, Feihu and Qin, Chuan and Yao, Kaichun and Fang, Chuyu and Zhuang, Fuzhen and Zhu, Hengshu and Xiong, Hui},
  booktitle={DASFAA},
  year={2024},
}


@inproceedings{
zhang2022greaselm,
title={Grease{LM}: Graph {REAS}oning Enhanced Language Models},
author={Xikun Zhang and Antoine Bosselut and Michihiro Yasunaga and Hongyu Ren and Percy Liang and Christopher D Manning and Jure Leskovec},
booktitle={ICLR},
year={2022},
}



@inproceedings{yu2023metamath,
  title={Metamath: Bootstrap your own mathematical questions for large language models},
  author={Yu, Longhui and Jiang, Weisen and Shi, Han and Yu, Jincheng and Liu, Zhengying and Zhang, Yu and Kwok, James T and Li, Zhenguo and Weller, Adrian and Liu, Weiyang},
  booktitle={arXiv},
  year={2023}
}


@article{yuan2023scaling,
  title={Scaling relationship on learning mathematical reasoning with large language models},
  author={Yuan, Zheng and Yuan, Hongyi and Li, Chengpeng and Dong, Guanting and Lu, Keming and Tan, Chuanqi and Zhou, Chang and Zhou, Jingren},
  journal={arXiv},
  year={2023}
}


@article{xie2023olagpt,
  title={Olagpt: Empowering llms with human-like problem-solving abilities},
  author={Xie, Yuanzhen and Xie, Tao and Lin, Mingxiong and Wei, WenTao and Li, Chenglin and Kong, Beibei and Chen, Lei and Zhuo, Chengxiang and Hu, Bo and Li, Zang},
  journal={arXiv},
  year={2023}
}


@inproceedings{li2022pre,
  title={Pre-trained language models for interactive decision-making},
  author={Li, Shuang and Puig, Xavier and Paxton, Chris and Du, Yilun and Wang, Clinton and Fan, Linxi and Chen, Tao and Huang, De-An and Aky{\"u}rek, Ekin and Anandkumar, Anima and others},
  booktitle={NeurIPS},
  year={2022}
}


@inproceedings{plaat2024reasoning,
  title={Reasoning with large language models, a survey},
  author={Plaat, Aske and Wong, Annie and Verberne, Suzan and Broekens, Joost and van Stein, Niki and Back, Thomas},
  booktitle={arXiv},
  year={2024}
}


@article{anil2023palm,
  title={Palm 2 technical report},
  author={Anil, Rohan and Dai, Andrew M and Firat, Orhan and Johnson, Melvin and Lepikhin, Dmitry and Passos, Alexandre and Shakeri, Siamak and Taropa, Emanuel and Bailey, Paige and Chen, Zhifeng and others},
  journal={arXiv},
  year={2023}
}


@article{snell2024scaling,
  title={Scaling llm test-time compute optimally can be more effective than scaling model parameters},
  author={Snell, Charlie and Lee, Jaehoon and Xu, Kelvin and Kumar, Aviral},
  journal={arXiv},
  year={2024}
}


@article{tong2024optimizing,
  title={Optimizing Language Model's Reasoning Abilities with Weak Supervision},
  author={Tong, Yongqi and Wang, Sizhe and Li, Dawei and Wang, Yifan and Han, Simeng and Lin, Zi and Huang, Chengsong and Huang, Jiaxin and Shang, Jingbo},
  journal={arXiv preprint arXiv:2405.04086},
  year={2024}
}

@article{li2024openai,
  title={OpenAI-o1 AB Testing: Does the o1 model really do good reasoning in math problem solving?},
  author={Li, Leo and Luo, Ye and Pan, Tingyou},
  journal={arXiv},
  year={2024}
}

@article{quan2025codeelo,
  title={CodeElo: Benchmarking Competition-level Code Generation of LLMs with Human-comparable Elo Ratings},
  author={Quan, Shanghaoran and Yang, Jiaxi and Yu, Bowen and Zheng, Bo and Liu, Dayiheng and Yang, An and Ren, Xuancheng and Gao, Bofei and Miao, Yibo and Feng, Yunlong and others},
  journal={arXiv},
  year={2025}
}

@article{cai2024driving,
  title={Driving with Regulation: Interpretable Decision-Making for Autonomous Vehicles with Retrieval-Augmented Reasoning via LLM},
  author={Cai, Tianhui and Liu, Yifan and Zhou, Zewei and Ma, Haoxuan and Zhao, Seth Z and Wu, Zhiwen and Ma, Jiaqi},
  journal={arXiv},
  year={2024}
}

@inproceedings{goyal2024healai,
  title={Healai: A healthcare llm for effective medical documentation},
  author={Goyal, Sagar and Rastogi, Eti and Rajagopal, Sree Prasanna and Yuan, Dong and Zhao, Fen and Chintagunta, Jai and Naik, Gautam and Ward, Jeff},
  booktitle={WSDM},
  year={2024}
}




@misc{sky-t1-2025,
  author       = {Team NovaSky},
  title        = {Sky-T1: Train your own O1 preview model within \$450},
  howpublished = {https://novasky-ai.github.io/posts/sky-t1},
  note         = {Accessed: 2025-01-09},
  year         = {2025}
}

@misc{open-o1-2025,
  author       = {OpenO1},
  title        = {O1-OPEN/OpenO1-SFT},
  howpublished = {https://huggingface.co/datasets/O1-OPEN/OpenO1-SFT},
  note         = {Accessed: 2025-01-09},
  year         = {2025}
}

@misc{alpaca-python,
    author = {iamtarun},
title = {Alpaca-Python-18K},
  howpublished = {https://huggingface.co/datasets/\\iamtarun/python\_code\_instructions\_18k\_alpaca},
  year         = {2023}
}


@misc{steenhoek2025errmachinevulnerabilitydetection,
      title={To Err is Machine: Vulnerability Detection Challenges LLM Reasoning}, 
      author={Benjamin Steenhoek and Md Mahbubur Rahman and Monoshi Kumar Roy and Mirza Sanjida Alam and Hengbo Tong and Swarna Das and Earl T. Barr and Wei Le},
      year={2025},
      archivePrefix={arXiv}
}




@article{
zhang2024multimodal,
title={Multimodal Chain-of-Thought Reasoning in Language Models},
author={Zhuosheng Zhang and Aston Zhang and Mu Li and hai zhao and George Karypis and Alex Smola},
journal={Transactions on Machine Learning Research},
year={2024}
}

@inproceedings{10.1609/aaai.v38i16.29776,
author = {He, Liqi and Li, Zuchao and Cai, Xiantao and Wang, Ping},
title = {Multi-modal latent space learning for chain-of-thought reasoning in language models},
year = {2025},
booktitle = {AAAI},
}

@inproceedings{
carlini2023are,
title={Are aligned neural networks adversarially aligned?},
author={Nicholas Carlini and Milad Nasr and Christopher A. Choquette-Choo and Matthew Jagielski and Irena Gao and Pang Wei Koh and Daphne Ippolito and Florian Tram{\`e}r and Ludwig Schmidt},
booktitle={NeurIPS},
year={2023}
}


@article{lohman2011intelligence,
  title={Intelligence and reasoning},
  author={Lohman, David F and Lakin, Joni M},
  journal={The Cambridge handbook of intelligence},
  year={2011},
  publisher={Cambridge University Press New York, NY}
}




@article{shum2023automatic,
  title={Automatic prompt augmentation and selection with chain-of-thought from labeled data},
  author={Shum, KaShun and Diao, Shizhe and Zhang, Tong},
  journal={arXiv},
  year={2023}
}


@article{zhao2023verify,
  title={Verify-and-edit: A knowledge-enhanced chain-of-thought framework},
  author={Zhao, Ruochen and Li, Xingxuan and Joty, Shafiq and Qin, Chengwei and Bing, Lidong},
  journal={arXiv},
  year={2023}
}


@article{liu2023retrieval,
  title={Retrieval-augmented multi-modal chain-of-thoughts reasoning for large language models},
  author={Liu, Bingshuai and Lyu, Chenyang and Min, Zijun and Wang, Zhanyu and Su, Jinsong and Wang, Longyue},
  journal={arXiv},
  year={2023}
}


@article{jaech2024openai,
  title={Openai o1 system card},
  author={Jaech, Aaron and Kalai, Adam and Lerer, Adam and Richardson, Adam and El-Kishky, Ahmed and Low, Aiden and Helyar, Alec and Madry, Aleksander and Beutel, Alex and Carney, Alex and others},
  journal={arXiv},
  year={2024}
}


@article{zhao2024marco,
  title={Marco-o1: Towards open reasoning models for open-ended solutions},
  author={Zhao, Yu and Yin, Huifeng and Zeng, Bo and Wang, Hao and Shi, Tianqi and Lyu, Chenyang and Wang, Longyue and Luo, Weihua and Zhang, Kaifu},
  journal={arXiv},
  year={2024}
}


@article{wang2024openr,
  title={Openr: An open source framework for advanced reasoning with large language models},
  author={Wang, Jun and Fang, Meng and Wan, Ziyu and Wen, Muning and Zhu, Jiachen and Liu, Anjie and Gong, Ziqin and Song, Yan and Chen, Lei and Ni, Lionel M and others},
  journal={arXiv},
  year={2024}
}


@article{yang2024qwen25mathtechnicalreportmathematical,
  title={Qwen2.5-Math Technical Report: Toward Mathematical Expert Model via Self-Improvement}, 
  author={An Yang and Beichen Zhang and Binyuan Hui and Bofei Gao and Bowen Yu and Chengpeng Li and Dayiheng Liu and Jianhong Tu and Jingren Zhou and Junyang Lin and Keming Lu and Mingfeng Xue and Runji Lin and Tianyu Liu and Xingzhang Ren and Zhenru Zhang},
  journal={arXiv},
  year={2024}
}


@article{wang2024drt,
  title={DRT-o1: Optimized Deep Reasoning Translation via Long Chain-of-Thought},
  author={Wang, Jiaan and Meng, Fandong and Liang, Yunlong and Zhou, Jie},
  journal={arXiv},
  year={2024}
}

@article{ICD,
  title={Jailbreak and guard aligned language models with only few in-context demonstrations},
  author={Wei, Zeming and Wang, Yifei and Li, Ang and Mo, Yichuan and Wang, Yisen},
  journal={arXiv},
  year={2023}
}

@article{CotHub,
  title={Chain-of-Thought Hub: A Continuous Effort to Measure Large Language Models' Reasoning Performance},
  author={Fu, Yao and Ou, Litu and Chen, Mingyu and Wan, Yuhao and Peng, Hao and Khot, Tushar},
  journal={arXiv},
  year={2023}
}

@article{shinn2024reflexion,
  title={Reflexion: Language agents with verbal reinforcement learning},
  author={Shinn, Noah and Cassano, Federico and Gopinath, Ashwin and Narasimhan, Karthik and Yao, Shunyu},
  journal={NeurIPS},
  year={2024}
}


@article{math1,
  title={Mathprompter: Mathematical reasoning using large language models},
  author={Imani, Shima and Du, Liang and Shrivastava, Harsh},
  journal={arXiv},
  year={2023}
}


@article{math2,
  title={Llemma: An open language model for mathematics},
  author={Azerbayev, Zhangir and Schoelkopf, Hailey and Paster, Keiran and Santos, Marco Dos and McAleer, Stephen and Jiang, Albert Q and Deng, Jia and Biderman, Stella and Welleck, Sean},
  journal={arXiv},
  year={2023}
}

@article{mazeika2024harmbench,
title={HarmBench: A Standardized Evaluation Framework for Automated Red Teaming and Robust Refusal},
author={Mantas Mazeika and Long Phan and Xuwang Yin and Andy Zou and Zifan Wang and Norman Mu and Elham Sakhaee and Nathaniel Li and Steven Basart and Bo Li and David Forsyth and Dan Hendrycks},
year={2024},
journal={arXiv}
}

@inproceedings{li-etal-2024-drattack,
    title = "{D}r{A}ttack: Prompt Decomposition and Reconstruction Makes Powerful {LLM}s Jailbreakers",
    author = "Li, Xirui  and
      Wang, Ruochen  and
      Cheng, Minhao  and
      Zhou, Tianyi  and
      Hsieh, Cho-Jui",
    booktitle = "EMNLP",
    year = "2024",
}

@article{zou2023universal,
      title={Universal and Transferable Adversarial Attacks on Aligned Language Models}, 
      author={Andy Zou and Zifan Wang and J. Zico Kolter and Matt Fredrikson},
      year={2023},
      journal={arXiv}
}

@article{freedman2007statistics,
  title={Statistics (international student edition)},
  author={Freedman, David and Pisani, Robert and Purves, Roger},
  journal={Pisani, R. Purves, 4th edn. WW Norton \& Company, New York},
  year={2007}
}


@inproceedings{
anil2024manyshot,
title={Many-shot Jailbreaking},
author={Cem Anil and Esin DURMUS and Nina Rimsky and Mrinank Sharma and Joe Benton and Sandipan Kundu and Joshua Batson and Meg Tong and Jesse Mu and Daniel J Ford and Francesco Mosconi and Rajashree Agrawal and Rylan Schaeffer and Naomi Bashkansky and Samuel Svenningsen and Mike Lambert and Ansh Radhakrishnan and Carson Denison and Evan J Hubinger and Yuntao Bai and Trenton Bricken and Timothy Maxwell and Nicholas Schiefer and James Sully and Alex Tamkin and Tamera Lanham and Karina Nguyen and Tomasz Korbak and Jared Kaplan and Deep Ganguli and Samuel R. Bowman and Ethan Perez and Roger Baker Grosse and David Duvenaud},
booktitle={NeurIPS},
year={2024},
}

@inproceedings{
qi2024finetuning,
title={Fine-tuning Aligned Language Models Compromises Safety, Even When Users Do Not Intend To!},
author={Xiangyu Qi and Yi Zeng and Tinghao Xie and Pin-Yu Chen and Ruoxi Jia and Prateek Mittal and Peter Henderson},
booktitle={ICLR},
year={2024}
}

@inproceedings{
he2024what,
title={What is in Your Safe Data? Identifying Benign Data that Breaks Safety},
author={Luxi He and Mengzhou Xia and Peter Henderson},
booktitle={COLM},
year={2024}
}

@article{tunstall2023zephyrdirectdistillationlm,
      title={Zephyr: Direct Distillation of LM Alignment}, 
      author={Lewis Tunstall and Edward Beeching and Nathan Lambert and Nazneen Rajani and Kashif Rasul and Younes Belkada and Shengyi Huang and Leandro von Werra and Clémentine Fourrier and Nathan Habib and Nathan Sarrazin and Omar Sanseviero and Alexander M. Rush and Thomas Wolf},
      year={2023},
    journal={arXiv}
}

@inproceedings{
lyu2024keeping,
    title={Keeping {LLM}s Aligned After Fine-tuning: The Crucial Role of Prompt Templates},
    author={Kaifeng Lyu and Haoyu Zhao and Xinran Gu and Dingli Yu and Anirudh Goyal and Sanjeev Arora},
    booktitle={ICLR Workshop on Reliable and Responsible Foundation Models},
    year={2024},
}

@article{wu2024separatewheatchaffposthoc,
      title={Separate the Wheat from the Chaff: A Post-Hoc Approach to Safety Re-Alignment for Fine-Tuned Language Models}, 
      author={Di Wu and Xin Lu and Yanyan Zhao and Bing Qin},
      year={2024},
    journal={arXiv}
}

@articlec{bai2022traininghelpful,
      title={Training a Helpful and Harmless Assistant with Reinforcement Learning from Human Feedback}, 
      author={Yuntao Bai and Andy Jones and Kamal Ndousse and Amanda Askell and Anna Chen and Nova DasSarma and Dawn Drain and Stanislav Fort and Deep Ganguli and Tom Henighan and Nicholas Joseph and Saurav Kadavath and Jackson Kernion and Tom Conerly and Sheer El-Showk and Nelson Elhage and Zac Hatfield-Dodds and Danny Hernandez and Tristan Hume and Scott Johnston and Shauna Kravec and Liane Lovitt and Neel Nanda and Catherine Olsson and Dario Amodei and Tom Brown and Jack Clark and Sam McCandlish and Chris Olah and Ben Mann and Jared Kaplan},
      year={2022}, 
      journal={arXiv}
}


@inproceedings{huang2024position,
  title={Trustllm: Trustworthiness in large language models},
  author={Huang, Yue and Sun, Lichao and Wang, Haoran and Wu, Siyuan and Zhang, Qihui and Li, Yuan and Gao, Chujie and Huang, Yixin and Lyu, Wenhan and Zhang, Yixuan and others},
  booktitle={ICML},
  year={2024},
}


@inproceedings{vllm,
author = {Kwon, Woosuk and Li, Zhuohan and Zhuang, Siyuan and Sheng, Ying and Zheng, Lianmin and Yu, Cody Hao and Gonzalez, Joseph and Zhang, Hao and Stoica, Ion},
title = {Efficient Memory Management for Large Language Model Serving with PagedAttention},
year = {2023},
booktitle = {SOSP}
}

@software{Tunstall_The_Alignment_Handbook,
  author = {Tunstall, Lewis and Beeching, Edward and Lambert, Nathan and Rajani, Nazneen and Huang, Shengyi and Rasul, Kashif and Bartolome, Alvaro and M. Rush, Alexander and Wolf, Thomas},
  title = {{The Alignment Handbook}},
  url = {https://github.com/huggingface/alignment-handbook},
  version = {0.3.0.dev0}
}

@article{gao2024omnimathuniversalolympiadlevel,
    title={Omni-MATH: A Universal Olympiad Level Mathematic Benchmark For Large Language Models}, 
    author={Bofei Gao and Feifan Song and Zhe Yang and Zefan Cai and Yibo Miao and Qingxiu Dong and Lei Li and Chenghao Ma and Liang Chen and Runxin Xu and Zhengyang Tang and Benyou Wang and Daoguang Zan and Shanghaoran Quan and Ge Zhang and Lei Sha and Yichang Zhang and Xuancheng Ren and Tianyu Liu and Baobao Chang},
    year={2024},
    journal={arXiv}
}

@inproceedings{zhang-etal-2024-safetybench,
    title = "{S}afety{B}ench: Evaluating the Safety of Large Language Models",
    author = "Zhang, Zhexin  and
      Lei, Leqi  and
      Wu, Lindong  and
      Sun, Rui  and
      Huang, Yongkang  and
      Long, Chong  and
      Liu, Xiao  and
      Lei, Xuanyu  and
      Tang, Jie  and
      Huang, Minlie",
    booktitle = "ACL",
    year = "2024",

}


@inproceedings{jan-etal-2025-multitask,
    title = "Multitask-Bench: Unveiling and Mitigating Safety Gaps in {LLM}s Fine-tuning",
    author = "Jan, Essa  and
      Aldahoul, Nouar  and
      Ali, Moiz  and
      Ahmad, Faizan  and
      Zaffar, Fareed  and
      Zaki, Yasir",
    booktitle = "COLING",
    year = "2025"
}

@article{deepseekai2025deepseekr1incentivizingreasoningcapability,
      title={DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning}, 
      author={DeepSeek-AI and Daya Guo and Dejian Yang and Haowei Zhang and Junxiao Song and Ruoyu Zhang and Runxin Xu and Qihao Zhu and Shirong Ma and Peiyi Wang and Xiao Bi and Xiaokang Zhang and Xingkai Yu and Yu Wu and Z. F. Wu and Zhibin Gou and Zhihong Shao and Zhuoshu Li and Ziyi Gao and Aixin Liu and Bing Xue and Bingxuan Wang and Bochao Wu and Bei Feng and Chengda Lu and Chenggang Zhao and Chengqi Deng and Chenyu Zhang and Chong Ruan and Damai Dai and Deli Chen and Dongjie Ji and Erhang Li and Fangyun Lin and Fucong Dai and Fuli Luo and Guangbo Hao and Guanting Chen and Guowei Li and H. Zhang and Han Bao and Hanwei Xu and Haocheng Wang and Honghui Ding and Huajian Xin and Huazuo Gao and Hui Qu and Hui Li and Jianzhong Guo and Jiashi Li and Jiawei Wang and Jingchang Chen and Jingyang Yuan and Junjie Qiu and Junlong Li and J. L. Cai and Jiaqi Ni and Jian Liang and Jin Chen and Kai Dong and Kai Hu and Kaige Gao and Kang Guan and Kexin Huang and Kuai Yu and Lean Wang and Lecong Zhang and Liang Zhao and Litong Wang and Liyue Zhang and Lei Xu and Leyi Xia and Mingchuan Zhang and Minghua Zhang and Minghui Tang and Meng Li and Miaojun Wang and Mingming Li and Ning Tian and Panpan Huang and Peng Zhang and Qiancheng Wang and Qinyu Chen and Qiushi Du and Ruiqi Ge and Ruisong Zhang and Ruizhe Pan and Runji Wang and R. J. Chen and R. L. Jin and Ruyi Chen and Shanghao Lu and Shangyan Zhou and Shanhuang Chen and Shengfeng Ye and Shiyu Wang and Shuiping Yu and Shunfeng Zhou and Shuting Pan and S. S. Li and Shuang Zhou and Shaoqing Wu and Shengfeng Ye and Tao Yun and Tian Pei and Tianyu Sun and T. Wang and Wangding Zeng and Wanjia Zhao and Wen Liu and Wenfeng Liang and Wenjun Gao and Wenqin Yu and Wentao Zhang and W. L. Xiao and Wei An and Xiaodong Liu and Xiaohan Wang and Xiaokang Chen and Xiaotao Nie and Xin Cheng and Xin Liu and Xin Xie and Xingchao Liu and Xinyu Yang and Xinyuan Li and Xuecheng Su and Xuheng Lin and X. Q. Li and Xiangyue Jin and Xiaojin Shen and Xiaosha Chen and Xiaowen Sun and Xiaoxiang Wang and Xinnan Song and Xinyi Zhou and Xianzu Wang and Xinxia Shan and Y. K. Li and Y. Q. Wang and Y. X. Wei and Yang Zhang and Yanhong Xu and Yao Li and Yao Zhao and Yaofeng Sun and Yaohui Wang and Yi Yu and Yichao Zhang and Yifan Shi and Yiliang Xiong and Ying He and Yishi Piao and Yisong Wang and Yixuan Tan and Yiyang Ma and Yiyuan Liu and Yongqiang Guo and Yuan Ou and Yuduan Wang and Yue Gong and Yuheng Zou and Yujia He and Yunfan Xiong and Yuxiang Luo and Yuxiang You and Yuxuan Liu and Yuyang Zhou and Y. X. Zhu and Yanhong Xu and Yanping Huang and Yaohui Li and Yi Zheng and Yuchen Zhu and Yunxian Ma and Ying Tang and Yukun Zha and Yuting Yan and Z. Z. Ren and Zehui Ren and Zhangli Sha and Zhe Fu and Zhean Xu and Zhenda Xie and Zhengyan Zhang and Zhewen Hao and Zhicheng Ma and Zhigang Yan and Zhiyu Wu and Zihui Gu and Zijia Zhu and Zijun Liu and Zilin Li and Ziwei Xie and Ziyang Song and Zizheng Pan and Zhen Huang and Zhipeng Xu and Zhongyu Zhang and Zhen Zhang},
      year={2025},
      journal={arXiv}
}