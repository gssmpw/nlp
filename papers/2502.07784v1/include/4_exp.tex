

\input{figures_latex/tables}
\input{figures_latex/fig_irradiance}
\input{figures_latex/fig_dropout}

\section{Experiments}

We now compare our method against state-of-the-art inpainting and material transfer methods.

\textbf{Baselines.} For inpainting models, we consider version 2.1 of Stable Diffusion~\cite{rombach2021highresolution}, the official inpainting fine-tuning of SD-XL~\cite{podell2023sdxl}, the inpainting FLUX.1 model from Black Forest Labs~\cite{flux.1}, and Blended Latent Diffusion~\cite{avrahami2023blended}. We also compare against ZeST~\cite{cheng2024zest}, the current state-of-the-art on material transfer. We employ the original models provided by the authors in all cases. 

\textbf{Data.} We conduct our quantitative analysis on 300 pairs of synthetic renders and 50 real images. The synthetic test set includes artist-made 3D scenes~\cite{evermotion} rendered with Blender's physically-based Cycles renderer~\shortcite{blender}. We render the images and the ground truth normals and irradiance maps. Unless stated otherwise, the real images are sourced from the Materialistic evaluation set \cite{sharma2023materialistic}. All our evaluation data will be publicly released. 

\textbf{Metrics.} We evaluate synthetic data using PSNR and LPIPS. Given that real data lacks ground truth maps, we evaluate its appearance using CLIP-I~\cite{radford2021learning} by computing the cosine similarity score between the exemplar image and a crop of the generated region. Additionally, we compare its estimated irradiance to that of the original target image to assess the adherence to lighting cues.



Quantitative results are reported in \cref{tab:generation}. For this experiment, all conditionings are supplied to the methods. While inpainting methods provide competitive performance, they do not perform as well as specialized methods for material transfer. On synthetic data, FLUX.1 shows competitive performance, beating even ZeST, which specializes in material transfer. On this data, our method achieves a $+3.5\%$ and $+2.4\%$ improvement in PSNR and LPIPS, respectively. On real data, we offer an improvement of $+2.6\%$ on the CLIP-I measure over ZeST, the second best-performing method. Our method establishes a new state-of-the-art on all metrics evaluated.

We further evaluate the shading error produced by each method in \cref{tab:irradiance}. This error is determined by comparing the estimated irradiance maps of the output image $\phi_\irra(\hat{\image})$ and the reference image $\phi_\irra(\image)$. We observe that newer inpainting methods based on FLUX.1 and Blended LD preserve the illumination from the original image well. Our method, guided by the irradiance map \irra{}, understandably outperforms all compared methods in illumination preservation. 


We present qualitative results in \cref{fig:baselines}. We note that earlier inpainting methods such as Stable Diffusion based methods (SD v2.1, SD-XL inpaint) have trouble with perspective projection, often offering an orthographic view of the material pasted directly into the region (second, third, and sixth rows), greatly hindering the realism of the edits. Newer methods such as FLUX.1 and Blended LD better adhere to the scene's geometry, but either lack perspective for FLUX.1 (sixth row) or differ from the exemplar material \exemplar{} (third to seventh rows). ZeST generally provides good geometry coherence, but exhibits artifacts (second, fourth, and seventh rows). In addition to good perspective projection (sixth row) and good respect for the exemplar material \exemplar{} (fourth row), our method \method{} provides more complex lighting interactions as reflections and highlights from lights (first and third rows). In general, \method{} produces material transfers that blend well with their surroundings while preserving illumination on the applied material. 

Additional analysis on color control can be found in~\cref{fig:variations}. For these results, we convert the exemplar material \exemplar{} from RGB to HSV and change its hue. Our method respects the user-defined color well, seamlessly integrating it into the scene. 

We evaluate the importance of the irradiance map~\irra{} in~\cref{fig:irradiance}. Our approach generates better matching shading than previous work, even without the irradiance map. However, high-frequency lighting effects from the original image such as highlights (first to third rows) require the irradiance map to be preserved. 

Finally, we demonstrate our ability to control the scale of the inpainted material by adjusting the scale of the exemplar~\exemplar{}. We evaluate this effect in~\cref{fig:scale} with three zoom levels. As our method processes larger features, it scales them up in the scene accordingly. 


\subsection{Ablation study}

We quantitatively evaluate the impact of each component of our method in \cref{tab:ablation}. All ablations are trained on the entirety of our \datasetname{} dataset, and evaluated on synthetic scenes. Unfreezing the UNet $(A_2)$ gives freedom for the image and mask to be used as conditionings $(A_{4+})$, which significantly boosts performance. Further introducing the irradiance map~\irra{} $(A_6)$ helps preserve the image shading, thus improving the results. Adding normals~\normal{} $(A_5)$ improves the results slightly; we hypothesize its role is to disambiguate possible confusion between the geometry and the lighting. Overall, training the IP-Adapter encoders slightly improves the result compared to solely fine-tuning the denoising U-Net, keeping the IP-Adapter layers frozen with pretrained weights.

We explore the role of lighting conditioning in \cref{fig:dropout}, showing that our model considers cues from both the target \target{} and the irradiance map \irra{}. As expected, removing all lighting cues significantly deteriorates shading quality. We do so by masking the target region \mask{} in the target image \target{} and removing the irradiance map \irra{}, that is using $\xx = \{\target\cdot\mask, \normal, \mask\}$. The best results are obtained when both the full target image \target{} (providing local geometry cues) and the irradiance (providing lighting information) are supplied. 










    
        



    



\begin{figure}
    \centering
    \setlength{\tabcolsep}{0pt}
    \renewcommand{\arraystretch}{0}
    \newcommand{\imgsize}{.18\linewidth}
    \newcommand{\imgsizehalf}{.09\linewidth}
    \newcommand{\shift}{5.6ex}

    \newcommand{\row}[1]{%
        & \includegraphics[width=\imgsizehalf,height=\imgsizehalf]{images/scale/irradiance_#1.png}%
        & \includegraphics[width=\imgsizehalf,height=\imgsizehalf]{images/scale/image_#1.png}%
        & \multirow{2}{*}[\shift]{\includegraphics[width=\imgsize,height=\imgsize]{images/scale/texture_z0_#1.png}}%
        & \multirow{2}{*}[\shift]{\includegraphics[width=\imgsize,height=\imgsize]{images/scale/train_mocka_v7adamW_mocka_v3_a100_512px_26000_#1_z0.png}}%
        & \multirow{2}{*}[\shift]{\includegraphics[width=\imgsize,height=\imgsize]{images/scale/train_mocka_v7adamW_mocka_v3_a100_512px_26000_#1_z1.png}}%
        & \multirow{2}{*}[\shift]{\includegraphics[width=\imgsize,height=\imgsize]{images/scale/train_mocka_v7adamW_mocka_v3_a100_512px_26000_#1_z3.png}} \\%
        
        &\includegraphics[width=\imgsizehalf,height=\imgsizehalf]{images/scale/normals_#1.png} %
        & \includegraphics[width=\imgsizehalf,height=\imgsizehalf]{images/scale/overlay_#1.png} \\[0.4ex]%
    }
    
	\resizebox{.99\linewidth}{!}{%
    \begin{tabular}{c cc S{.2ex} c S{.2ex} ccc}
        &\multicolumn{2}{c}{Conditions} & Texture & $\times 1$ & $\times 2$ & $\times 8$ \\[.7ex]

        \row{22f65695-image_023_D_tc_bricks_005}
        \row{36515637-image_010_A_tc_bricks_022}
        \row{36c7a88c-image_042_A_acg_paving_stones_009}
        \row{36c7a88c-image_042_A_ms_paving_stones_018__grass_003}
    \end{tabular}}
    \vspace{-2mm}
    \caption{Impact of exemplar \exemplar{} scale. We can implicitly control the 2D material appearance in the 3D scene by providing various the exemplar material \exemplar{} at different scales. We show results using the entire material ($\times{}\!1$), half the original material size ($\times{}\!2$), and one-eighth ($\times{}\!8$) and see that the transferred material scale follows that of the input.}
    \label{fig:scale}
\end{figure}

\begin{figure}
    \centering
    \setlength{\tabcolsep}{0pt}
    \renewcommand{\arraystretch}{0}
    \newcommand{\imgsize}{.2\linewidth}
	
    \resizebox{.99\linewidth}{!}{%
    \begin{tabular}{ccccc}
        \includegraphics[width=\imgsize]{images/archviz/scene_1.png}&%
        \includegraphics[width=\imgsize]{images/archviz/scene_2.png}&%
        \includegraphics[width=\imgsize]{images/archviz/scene_5.png}&%
        \includegraphics[width=\imgsize]{images/archviz/scene_6.png}&%
        \includegraphics[width=\imgsize]{images/archviz/scene_7.png}\\ 
        
        \includegraphics[width=\imgsize]{images/archviz/scene_3.png}&%
        \includegraphics[width=\imgsize]{images/archviz/scene_4.png}&%
        \includegraphics[width=\imgsize]{images/archviz/scene_8.png}&%
        \includegraphics[width=\imgsize]{images/archviz/scene_9.png}&%
        \includegraphics[width=\imgsize]{images/archviz/scene_10.png}\\ 
    \end{tabular}}
    \vspace{-2mm}
    \caption{Samples of our synthetic evaluation dataset scenes showing their diversity in appearance and illumination.}
    \label{fig:archviz}
\end{figure}




\begin{figure}
    \centering
    \setlength{\tabcolsep}{0pt}
    \renewcommand{\arraystretch}{0}
    \newcommand{\imgsize}{.18\linewidth}
    \newcommand{\imgsizehalf}{.09\linewidth}
    \newcommand{\shift}{5.7ex}
    
    \newcommand{\row}[2]{%
        &\includegraphics[width=\imgsizehalf,height=\imgsizehalf]{images/failure/irradiance_#1.png}%
        & \includegraphics[width=\imgsizehalf,height=\imgsizehalf]{images/failure/image_#1.png}%
        & \multirow{2}{*}[\shift]{\includegraphics[width=\imgsize,height=\imgsize]{images/failure/texture_#1.png}}%
        & \multirow{2}{*}[\shift]{\includegraphics[width=\imgsize,height=\imgsize]{images/failure/ours_#1.png}}%
        & \includegraphics[width=\imgsizehalf,height=\imgsizehalf]{images/failure/irradiance_#2.png}%
        & \includegraphics[width=\imgsizehalf,height=\imgsizehalf]{images/failure/image_#2.png}%
        & \multirow{2}{*}[\shift]{\includegraphics[width=\imgsize,height=\imgsize]{images/failure/texture_#2.png}}%
        & \multirow{2}{*}[\shift]{\includegraphics[width=\imgsize,height=\imgsize]{images/failure/ours_#2.png}} \\
        
        &\includegraphics[width=\imgsizehalf,height=\imgsizehalf]{images/failure/normals_#1.png}%
        & \includegraphics[width=\imgsizehalf,height=\imgsizehalf]{images/failure/mask_#1.png}%
        &&& \includegraphics[width=\imgsizehalf,height=\imgsizehalf]{images/failure/normals_#2.png}%
        & \includegraphics[width=\imgsizehalf,height=\imgsizehalf]{images/failure/mask_#2.png} \\[.7ex]
    }
    
	\resizebox{.99\linewidth}{!}{%
    \begin{tabular}{c cccc S{.5ex} cccc}
        &\multicolumn{2}{c}{Conditions} & Texture & Output & \multicolumn{2}{c}{Conditions} & Texture & Output \\[.7ex]
        
        \row{9259cdee-image_018_A_acg_bricks_023}{2414415f-image_039_B_st_pavement_022}
        \row{a9adf536-image_014_B_th_cobblestone_square}{79597e23-image_071_A_ms_paving_stones_094__grass_003}
        
    \end{tabular}}
    \vspace{-3mm}
    \caption{Limitations. We illustrate our method's limitations with objects with detailed normals being lost during material transfer (left column), and downward facing normals (right column). We believe these limitations could be mitigated by explicitly creating these situations in the training dataset.}
    \label{fig:limitations}
\end{figure}

