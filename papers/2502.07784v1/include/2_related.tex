
\section{Related works}

\subsection{Image generation and editing}
Neural image generation received significant attention over the past decade, starting with GANs~\cite{goodfellow2014gans,Karras2019stylegan2} and more recently diffusion models~\cite{sohl2015deep,rombach2021highresolution,podell2023sdxl,flux.1} or normalizing flows~\cite{zhang2021diffusionflow,esser2024sd3}. Lately, diffusion models have become the \textit{de facto} image generation framework~\cite{podell2023sdxl,peebles2023dit,ramesh2022dalle2,betker2023dalle3}, producing high-quality results and benefiting from internet-scale datasets~\cite{schuhmann2022laionb}. 

Controlling these image generation models became an active research field, first using text~\cite{ho2021classifier,meng2022sdedit}, then using various image modalities~\cite{zhang2023controlnet} or physically-based rendering (PBR) properties~\cite{zeng2024rgbx}. An effective approach to conditioning diffusion models is to train or fine-tune them with the desired control as input. For instance, using an image~\cite{ke2023marigold,he2024lotus,liu2024hyperhuman}, or other scene properties~\cite{zeng2024rgbx}. 
An alternative is to inject the conditioning maps into the pre-trained frozen diffusion model, either using a parallel network~\cite{zhang2023controlnet}, an adapter \cite{ye2023ipadapter}, or via low-rank adaptation of the text encoder \cite{lopes2024material}. 
Another control is to directly manipulate the input image embedding to modify its semantic properties~\cite{guerrero2024texsliders}. Finally, operations on attention maps~\cite{hertz2022prompt,epstein2023diffusion,parmar2023zero} are also commonly used to manipulate parts of images during the generation process. 

These conditionings can be coupled with the task of inpainting, when part of an image is generated to be seamlessly integrated into the input image. Generative diffusion models can be used for inpainting by compositing, at every denoising step, the estimated latent of the inpainted region with the latent of the input image \cite{lugmayr2022repaint}. However, this leads to visible artifacts along the mask boundaries \cite{cheng2024zest}. 
Text-driven local image editing was proposed by \citet{avrahami2023blended}, blending latents in the masked and unmasked areas during the denoising process. 

In this work, we fine-tune an existing diffusion model~\cite{rombach2021highresolution} to perform material replacement. We incorporate additional inputs through zero-weight addition~\cite{zhang2023controlnet}. 
To avoid the complexities of UV mappings linked to pixel-aligned conditionings, we leverage the priors of the diffusion model regarding object appearance, paired with global conditioning as suggested by IP-adapter~\cite{ye2023ipadapter}, to define the desired material appearance. 



\subsection{Environment-aware image editing}
Changing the appearance of a surface within an image is trivial in a 3D editor, but proves very challenging in photographs due to the complex interactions conflating appearance, light, and geometry.

\textbf{Lighting} plays a crucial role in photorealism and is a clear sign of forgery when not correctly handled \cite{kee2013exposing}. When editing images, one way to encode lighting is through radiance, either using a parametric \cite{griffiths2022outcast,gardner2019deep,poirier2024diffusion} or non-parametric light model \cite{pandey2021total,gardner2017learning}. However, radiance is challenging for deep learning models due to its high dynamic range and spherical nature, making it difficult to map to the image plane. Inspired by intrinsic image decomposition, recent image editing methods chose irradiance as encoded by shading maps to represent illumination and perform object insertion \cite{zhang2024zerocomp,fortier2024spotlight} or relighting \cite{kocsis2024lightit,ponglertnapakorn2023difareli,yu2020self}. Our method uses this same irradiance representation, estimating shading maps using \rgbtooxx~\shortcite{zeng2024rgbx}. 

\textbf{Material} replacement is a long-standing problem in Computer Graphics~\cite{an2008appprop, khan2006image} with early methods proposing to adjust materials reflectance of color~\cite{an2008appprop} through user scribble and edit propagation, or changing materials to metallic or glossy~\cite{khan2006image}. Leveraging deep learning, methods were proposed to edit materials in photographs, often targeting textures~\cite{guerrero2024texsliders} or objects~\cite{delanoy2022generative, cheng2024zest,sharma2024alchemist}. On a scene scale, \rgbtooxx~\shortcite{zeng2024rgbx} proposed using PBR maps as control, enabling the per-pixel change of material properties, in particular the albedo. However, this requires manual editing, which is impractical for textures or perspective-distorted objects. 
Closest to our work is ZeST~\cite{cheng2024zest}, which proposes a training-free method based on IP-Adapter to perform material transfer. It utilizes a depth-based ControlNet \cite{zhang2023controlnet}, yet lacks explicit modeling of the scene illumination, leading to inaccurate shading and approximate material appearance. In contrast, our method uses off-the-shelf estimators to guide the diffusion with geometric and lighting cues, resulting in better transfers and fewer artifacts at the mask edges.

