\section{Introduction}

Photographs capture the visual appearance of a scene by measuring the radiant energy resulting from the complex interaction of light, geometry, and materials. Among others, textures and materials are key components that contribute to the aesthetics and emotions conveyed by images~\cite{joshi2011aesthetics}. %
Unfortunately, their appearances are largely entangled with the scene's lighting and geometry, making it difficult to edit a posteriori. 

Recently, the editing or generation of images has been significantly simplified by advancements in diffusion models that can benefit from internet-scale datasets~\cite{schuhmann2022laionb}. Such models can be used for prompt-guided diffusion inpainting, where only part of an image is modified to follow user guidance~\cite{meng2022sdedit,lugmayr2022repaint}. However, textually describing a material is not trivial, especially when it {exhibits} complex patterns or appearance. An alternative is to use pixel-aligned maps to drive the generation. 
Using ControlNet~\cite{zhang2023controlnet} such conditions can come in the form of semantics (\eg, segmentation maps), visual maps (\eg, edges), or geometry information (\eg, depth, normal). 
IP-Adapter~\cite{ye2023ipadapter} proposes a similar approach for global conditioning, where CLIP~\cite{radford2021learning} visual embeddings are used as an effective guidance signal for image generation. 
ZeST~\cite{cheng2024zest} builds on the latter, showing that material inpainting can be conditioned with a CLIP encoder to extract a material appearance from an image. However, this offers little control to the artist over the transferred material appearance (\eg, scale, rotation). 
To achieve greater control, \rgbtooxx~\cite{zeng2024rgbx} proposes to directly modify the estimated PBR maps, at the cost of manual per-pixel editing. However, such an approach is impractical for spatially varying materials, as manual editing requires careful geometry and perspective texture projection handling.

In this work, we introduce \method, an exemplar-based method that improves material transfer in images while simplifying its usage and offering more controllability to the user. 
Given an image and a texture sample rendered or photographed on a mostly flat surface, our method inpaints a region of the image using the texture sample (\cf~\cref{fig:teaser}), ensuring a close alignment with the original geometric and illumination cues obtained from off-the-shelf single image estimators~\cite{zeng2024rgbx,he2024lotus}. 

More precisely, we rely on a light- and geometry-aware diffusion model fine-tuned for generating realistic inpainted images. We fine-tune a pre-trained diffusion model~\cite{rombach2021highresolution}, which already incorporates priors about object appearances in images, using a new synthetic dataset. Our goal is therefore to specialize our model to the material transfer task while retaining its original priors for better generalization to real photographs. 
To train our model, we generate a procedural 3D dataset named \datasetname{}, which consists of primitive shapes randomly arranged and lit with captured environment maps, and render pairs of images with varying materials applied on a randomly selected object surface.
Finally, we compare \method{} to state-of-the-art inpainting~\cite{rombach2021highresolution,podell2023sdxl,flux.1,avrahami2023blended} and the latest material transfer method~\cite{cheng2024zest} showing that it outperforms them qualitatively and quantitatively. 

Our approach enables material replacement in images through the following contributions:
\begin{itemize}
    \item A lighting- and geometry-aware diffusion model that performs material transfer in a single image;
    \item A two-stage training pipeline based on synthetic data that generalizes to real images;
    \item \datasetname{}, a new synthetic procedural dataset that provides \nbtotal~paired renderings suitable for training material replacement methods.
\end{itemize}


\begin{figure*}[t]
  \includegraphics[width=.98\textwidth]{figures/method_v4.pdf}
  \caption{Overview of \method{}.
  We learn to transfer the material \exemplar on a given region \mask of an input image \target by training a lighting- and geometry-aware diffusion model, leveraging irradiance \irra and normal \normal maps. Once encoded ($\mathcal{E}$) or downsampled ($\mathcal{S}_\downarrow$), the image, mask, and maps are concatenated into a scene descriptor $z_X$ which, together with the noise latent $z_t$, serve as input of the denoising UNet, $\epsilon_{\theta}$. To integrate the exemplar conditioning, we inject the visual CLIP features of the texture via adapter layers from IP-Adapter~\cite{ye2023ipadapter}. 
  During inference, we leverage off-the-shelf estimators ($\phi_N$, $\phi_E$) to obtain normal and irradiance from real-world images. 
  }
  \label{fig:method}
\end{figure*}
