
\section{Material Transfer}


Material transfer involves applying a material to a designated surface in an image, ensuring it integrates seamlessly into the scene.
It can be seen as a form of 3D-aware inpainting, where a given material is plausibly blended within a target image while preserving its shading and geometry cues. 

Specifically, given an exemplar texture image~\exemplar, a target image~\target, and a target mask~\mask, our material transfer consists of replacing the region defined by \mask in \target with a material that resembles the exemplar texture image~\exemplar. 
The mask is arbitrarily defined by the user and can cover either an object or a surface, or be obtained by an automatic segmentation method \cite{kirillov2023segment,sharma2023materialistic}. None of the inputs require time-consuming annotations or expertise in 3D modeling, such as UV maps or texture wrapping. 

To accomplish this, we build upon a latent diffusion model that encodes an image \image into a latent space represented by an encoder: $z_0 = \mathcal{E}(\image)$. From this point, we carry out iterative denoising as outlined in \cite{ho2020denoising}. 
We start with a Gaussian-sampled latent vector $z_T$ and aim to produce its denoised counterpart, the latent vector $z_0$. This is a multi-step process, where a UNet predicts the residual $\epsilon$ to denoise the latent variable at every step. 



\subsection{\method}
\label{sec:meth_meth}
We consider material transfer as a conditional generative task leveraging diffusion models. 
While the literature typically relies solely on geometrical cues~\cite{zhang2023controlnet,cheng2024zest} we observe that this can lead to inconsistent shading. Therefore, we guide our diffusion process with both scene illumination and geometry. 

Assuming an image \target of a scene, we describe it with a conditioning \xx which includes the target image and mask along with pixel-wise intrinsic maps representing normals \normal %
and diffuse irradiance \irra%
. Importantly, we ensure that these maps do not contain material information, avoiding to include properties such as albedo, roughness, or metalness. This ensures the diffusion is provided with enough information about the scene structure and illumination while remaining material-independent. Subsequently, as \irra we choose to represent only the diffuse illumination of the scene without specular effects since these are too correlated with materials, making it impractical to condition the model. 
We train our method on synthetic data (\cref{sec:meth_dataset}), using the conditioning buffers readily provided by rendering engines while demonstrating that our model successfully generalizes to real images. 
For the latter, we leverage recent advances in single-image intrinsic channel estimation to obtain reasonably accurate maps from off-the-shelf methods such as \rgbtooxx~\cite{zeng2024rgbx} or LoTUS~\cite{he2024lotus}. Thus, for real images we define $\irra=\phi_\irra(\target)$ and $ \normal=\phi_\normal(\target)$, with $\phi_\normal$ and $\phi_\irra$ the normal and an irradiance estimators, respectively.

Next, we explain how to integrate the exemplar texture conditioning into the framework. 
Recently, IP-Adapter~\cite{ye2023ipadapter} demonstrated that image-prompt guidance could be accomplished by training adapters between the CLIP \cite{radford2021learning} visual encoder and the denoising UNet. Further works~\cite{vecchio2024controlmat,cheng2024zest,Yan:2023:PSDR-Room,guerrero2024texsliders} have shown that CLIP can be used to extract rich material features from images. 
Similarly, we condition our pipeline on the CLIP image embedding of the material we want to transfer in the target image. We replace the standard text cross-attention mechanism, injecting the visual CLIP features via adapter layers instead.




Our proposed method is illustrated in \cref{fig:method}. We encode the ground-truth image \image{} as $z_0 = \mathcal{E}(\image)$ and its scene descriptor stack $\xx=\{\target, \normal, \irra, \mask\}$ describing our target image \target, defined as:
\begin{equation}
    z_\xx = \left( \mathcal{E}(\target),\, \mathcal{E}(\normal),\, \mathcal{S}_\downarrow(\irra),\, \mathcal{S}_\downarrow(\mask) \right),
    \label{eq:encoding}
\end{equation} 
where $\mathcal{E}$ is a pre-trained latent encoder and $\mathcal{S}_\downarrow$ is a down-sampling operator. As seen in~\cref{eq:encoding} both the target image~$\target$ and normal maps~$\normal$ are encoded while the irradiance map~$\irra$ and the inpainting mask~$\mask$ are downsampled following previous works~\cite{zeng2024rgbx,rombach2021highresolution}.
To provide the conditioning signal, $z_\xx$ is concatenated to the noisy input latent $z_t$ at every timestep $t$. The diffusion loss is defined as:
\begin{equation}
    \mathcal{L}_{\theta} = \left\| \epsilon_t - \epsilon_{\theta} \left( z_t, z_\xx, t, \tau(\exemplar) \right) \right\|^2_2.
\end{equation}
\noindent We write $\epsilon_{\theta}$, the denoising UNet, with its parameters $\theta$. It receives two kinds of inputs: the noisy image $z_t$ concatenated with the scene descriptor $z_\xx$; and a global conditioning via the cross-attention layers of $\epsilon_{\theta}$ containing the timestep $t$ and a CLIP embedding $\tau(\exemplar)$, with \exemplar being the exemplar texture image. To condition the diffusion process with visual features, we initialize the adapter weights with those from IP-Adapter \cite{ye2023ipadapter} and freeze the image encoder.
In our pipeline, we train the full UNet and drop the text prompt to rely exclusively on the image prompt embedding $\tau(\exemplar)$, for which we train adapter layers, as seen \cref{fig:method} \textit{top}.

Our end-to-end training method uses modality dropout on the input latents of $z_\xx$ \cite{zeng2024rgbx} by randomly setting these to null vectors. This ensures the model can inpaint the target region with partial conditioning or even completely unconditionally. 




\subsection{Dataset}
\label{sec:meth_dataset}

To train our method, we need paired images showing the same scene with identical lighting but with a known material change.
We design a simple procedural 3D dataset in Blender, named \datasetname, consisting of \nbtotal~scene pairs which we render along with irradiance, normals, UV and material segmentation maps. Scenes are created by randomly placing 3D primitives and lights within the boundaries of a cubic scene, and randomly varying the wall heights to allow direct lighting and occlusions. The images are rendered with image-based lighting \cite{debevec2008rendering} using a randomly rotated environment map sampled from a set of 100 HDRIs~\cite{polyhaven}. We use approximately 4,000 unique materials from MatSynth~\cite{vecchio2023matsynth}, randomly assigned to all objects. For each scene, one of the objects is randomly selected, and its material is replaced with another. This generates two buffers per scene, each containing $\left( \target, \normal, \irra, \mask, \exemplar, \image \right)$ with only the material on the selected object changed. We show samples from our dataset in~\cref{fig:dataset}. 

To enforce consistency between the scale of the conditioning image and the rendered texture, we measure the texture coverage from the scene UV buffer and scale the exemplar image \exemplar accordingly, similarly to recent work~\cite{ma2024materialpicker}. This ensures that the rendered texture appears at the same scale as in the conditioning image (\eg, a brick texture will have roughly the same number of tiles in \exemplar and \image). 
Despite the simplicity of~\datasetname, we found it sufficient for our model to learn strong priors for material transfer. 

\begin{figure}[h!]
    \centering
    \setlength{\tabcolsep}{.5pt}
    \renewcommand{\arraystretch}{0.1}
    \newcommand{\imgsize}{.19\linewidth}

    \newcommand{\row}[2]{%
        \verti{10ex}{envmap #1} & \includegraphics[width=\imgsize,height=\imgsize]{images/dataset/#2_irradiance.png} & \includegraphics[width=\imgsize,height=\imgsize]{images/dataset/#2_normal.png} & \includegraphics[width=\imgsize,height=\imgsize]{images/dataset/#2_target.png} & \includegraphics[width=\imgsize,height=\imgsize]{images/dataset/#2_mask.png} & \includegraphics[width=\imgsize,height=\imgsize]{images/dataset/#2_texture.png} & \includegraphics[width=\imgsize,height=\imgsize]{images/dataset/#2_image.png}
    }
    
	\resizebox{.99\linewidth}{!}{%
    \begin{tabular}{c cccccc}
        & irradiance \irra & normal \normal & target \target & mask \mask & texture \exemplar & image \image \\[.5ex]
        \row{1}{000155-01a} \\
        \verti{1.5ex}{...} \\
        \row{N}{000155-02a} \\[.5ex]
        \row{1}{000156-00a} \\
        \verti{1.5ex}{...} \\
        \row{N}{000156-03a} \\
    \end{tabular}}
    
    \caption{Procedural dataset. We show examples of our \datasetname{} dataset, which we use for training. It consists of primitive objects (spheres, cubes, cylinders, and tori) with random placements, orientations, and materials enclosed within four walls of varying heights. A total of \nbscenes{} 3D scenes were created in Blender, each rendered under $\nbvariants{}$ light variations, with image-based lighting to achieve realistic occlusions and cast shadows. 
    For every scene, we render a second scene identical to the first, except for one object for which we swap the material. Under ``texture \exemplar'', we show the full texture as well as a crop (outlined in white) that has a matching scale with the rendered surface.}
    \label{fig:dataset}
\end{figure}



\subsection{Implementation Details}

Our approach is based on Stable Diffusion~\cite{rombach2021highresolution}, a large publicly available text-to-image model. Specifically, we fine-tune from the IP-Adapter~\cite{ye2023ipadapter} checkpoint. Drawing inspiration from ControlNet \cite{zhang2023controlnet}, we initialize additional convolutions with zero-convs.
We train in two phases: first at a resolution of $256\!\times{}\!256\;\mathrm{px}$ for 100k iterations, followed by 50k iterations at $512\!\times{}\!512\;\mathrm{px}$ resolution. The training employs a batch size of 64 and spans roughly five days on an Nvidia A100 GPU accelerator. The AdamW optimizer \cite{kingma2014adam,loshchilov2017fixing} is used with a learning rate of $2\!\times{}\!10^{-5}$.

To enhance robustness, we apply horizontal flipping as data augmentation, taking care to adjust the normals accordingly. Additionally, we implement a $10\%$ probability of dropping all the conditioning inputs except the mask, along with another $10\%$ chance of dropping any of the inputs: \irra, \normal, \target, \exemplar. To drop \exemplar{}, we set the CLIP embedding to the null vector. The mask \mask is always retained as input. 

Our input \exemplar corresponds to a material rendered on a flat surface covering the whole image in a fronto-parallel setting, illuminated with a random HDRI from PolyHaven~\cite{polyhaven}. During training, we utilize the materials from MatSynth~\cite{vecchio2023matsynth}, from which we extract 16 random crops. To account for the scene scale in the reference image~\target{}, we extract the UV coordinates of each material within the image, and resize all material samples~\exemplar{} by a factor of $\left( \max{\left( \mathrm{UV} \right)} - \min{\left( \mathrm{UV} \right)}\right)^{-1}$ in both horizontal and vertical dimensions. 
These exemplars are then resized to $224\!\times{}\!224\;\mathrm{px}$ for input into CLIP. 

