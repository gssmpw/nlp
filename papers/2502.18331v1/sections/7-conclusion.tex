We introduced \method{}, an unsupervised method inspired by the information bottleneck principle that addresses  humor explanation tasks by eliciting relevant knowledge from VLMs and iteratively refining the explanation. Our experiments show that \method{} outperforms a range of baselines on three datasets, underscoring the importance of incorporating relevant world knowledge in humor understanding. Our analysis offers insights into the impact of individual components in our method, and justifies the use of the IB principle. We further propose an LLM-based evaluation framework and an adaptation of an interpretability technique. While we tested our contributions in the context of humor interpretation, future work can adapt them to any task that can benefit from eliciting and reasoning on world knowledge. 
