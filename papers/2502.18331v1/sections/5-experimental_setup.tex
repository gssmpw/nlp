\subsection{Datasets}
\label{sec:exp_setup:datasets}

We evaluate \method{} on three multimodal humor datasets (see examples in Appendix \ref{app:dataset-eg}):

\paragraph{MemeCap \cite{hwang-shwartz-2023-memecap}.} Each instance includes a meme paired with a title (social media post to which the meme was attached). The task is to generate a brief explanation, compared against multiple reference explanations. The task requires interpreting visual metaphors in relation to the text, where models can benefit from reasoning about background knowledge. 
% , and reference captions in the test images can contain up to four captions. 
% includes 5.8k training and validation samples, along with 559 test samples. 

\paragraph{New Yorker Cartoon \cite{hessel-etal-2023-androids}.} We focus on the explanation generation task: given a New Yorker cartoon and its caption, generate an explanation for why the caption is funny given the cartoon, requiring an understanding of the scene, caption, and commonsense and world knowledge.  
%indirect and playful meanings tied to human experience and culture.
% It includes 2.3k training samples, 130 validation samples, and 130 test samples, and offers five different splits. 

\paragraph{YesBut \cite{nandy-etal-2024-yesbut}.} 
%dataset consists of 1k satirical and 1k non-satirical test samples, with our focus on the satirical test set. 
Each instance contains an image with two parts captioned ``yes'' and ``but''. The task is to explain why the image is funny or satirical.
% requiring an understanding of commonsense knowledge, social norms, and cultural references related to everyday objects and situations.

Since our method is unsupervised, we use the test set portions of these datasets. Due to resource and cost constraints, we don't evaluate our method on the full test sets. Instead, from each dataset, we randomly sample 100 test instances. We repeat the process three times using different random seeds to obtain three test splits and report average performance and standard deviation.

\subsection{Models}
\label{sec:exp_setup_models}

We test our method with two closed-source and two open-source VLMs.
\paragraph{GPT-4o \cite{hurst2024gpt}} is an advanced, closed-source multimodal model processing text, audio, images, and video and generating text, audio, and images. It matches GPT-4's performance in English text tasks with improved vision understanding.
\paragraph{Gemini \cite{team2023gemini}} is a closed-source multimodal model from Google, available in multiple variants optimized for different tasks. 
We use \texttt{Gemini 1.5 Flash} for evaluation and \texttt{Gemini 1.5 Flash-8B} for experiments, a smaller, faster variant with comparable performance.
\paragraph{Qwen2 \cite{yang2024qwen2technicalreport}} is an open-source multimodal model built on a vision transformer with strong visual reasoning. We use the \texttt{Qwen2-VL-7B-Instruct} model, competitive with GPT-4o on several benchmarks.
\paragraph{Phi \cite{phi}} is a lightweight, open-source 4.2B-parameter multimodal model, trained on synthetic and web data. We use \texttt{Phi-3.5-Vision-Instruct}, optimized for precise instruction adherence.

\subsection{Baselines}
\label{sec:exp_setup:baselines}

We compare our method to four prompting-based baselines:\footnote{Temperature set to 0.8 for all baselines.} zero-shot (\base{}), Chain-of-Thought (\chain{}) prompting, and self-refinement with (\critic{}) and without (\nocritic{}) a critic.

\base{} generates a final explanation directly from the image and caption using VLM. \chain{} follows a similar setup but instructs the model to produce intermediate reasoning chains \cite{cot}. 
Additionally, we implement \critic{}, a multimodal variant of self-refinement \cite{madaan2023selfrefine}, where a \textit{generator} produces a response, and a \textit{critic} evaluates it based on predefined criteria. The critic's feedback helps refine the output iteratively\footnote{Refinement steps set to 2 for fair comparison.}. Evaluation criteria include correctness, soundness, completeness, faithfulness, and clarity (details in Appendix \ref{app:base-prompts}).
\nocritic{} functions identically to \critic{} but without a \textit{critic model}, refining candidate explanations without feedback. This also serves as an ablation of the implications from \method{}. Prompts for baselines are in Appendix \ref{app:base-prompts}.


\subsection{Evaluation Metrics}
\label{sec:exp_setup:eval}
While human evaluation is often the most reliable option for open-ended tasks like ours \cite{hwang-shwartz-2023-memecap}, it is costly at scale. LLM-based evaluations (e.g., with \texttt{Gemini 1.5 Flash}) offer a more affordable alternative but are not always reliable \cite{biases_paper}. Prior research in fact verification has found that modern closed-source LLMs excel at fact checking when the complex facts are decomposed into simpler, atomic facts and verified individually \cite{gunjal-durrett-2024-molecular, samir-etal-2024-locating}. Inspired by this approach, we propose LLM-based precision and recall scores.

For recall, we decompose the reference $ref$ into atomic facts: $\{y_1, y_2, ..., y_n\}$ and check whether each appears in the predicted response $pred$.
% The percent of facts present in the predicted response forms the recall score:
\[
\text{Recall} = \frac{1}{n} \sum_{i=1}^{n} \mathbbm{1} \big( LLM(y_i, pred) = \text{Yes} \big)
\]
where $n$ is the number of atomic facts in $ref$.

Precision follows the same process in reverse, decomposing $pred$ into a list of atomic facts: $\{x_1, x_2, ..., x_m\}$ and verifying their presence in $ref$:
\[
\text{Precision} = \frac{1}{m} \sum_{i=1}^{m} \mathbbm{1} \big( LLM(x_i, ref) = \text{Yes} \big)
\]
where $m$ is the number of atomic facts in $pred$. Both decomposition and verification use Gemini-Flash-1.5 with a temperature of 0.2.

In preliminary experiments, we observed that human references tend to omit obvious visual details, whereas model-generated answers are often more complete, referencing visual information. To prevent penalizing the models for these facts, we incorporate literal image descriptions (Sec~\ref{sec:method}) into the reference by decomposing them and adding them to the atomic facts for fairer evaluation. Based on the precision and recall scores, we report the macro-$F_1$ score.

To assess the reliability of our metrics, we conducted a human evaluation on 130 random samples across all models and datasets via CloudResearch (details in Appendix \ref{app:cloudresearch}). Human annotators determined whether each atomic sentence appeared in the corresponding text (e.g., reference). The average agreement between the LLM-based evaluator and two human annotators was 77.1\% (\(\kappa = 54.1\)), similar to the agreement between the two annotators: 75.4\% (\(\kappa = 50.8\)), indicating considerable alignment with human judgment. Prompts are in Appendix \ref{app:eval-prompts}.
