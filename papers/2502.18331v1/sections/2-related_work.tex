\paragraph{Multimodal Humor Understanding.} Earlier works on humor understanding primarily focus on detection in images and videos \cite{Chandrasekaran_2016_CVPR, castro-etal-2019-towards, patro-humor}. 
Recent work shifted to generative tasks, typically explaining humor in an image \cite{hwang-shwartz-2023-memecap, hessel-etal-2023-androids, nandy-etal-2024-yesbut} or video \cite{smile, hasan-etal-2019-ur}. Understanding and explanation generation remain underexplored due to the complexity of the task and free-text evaluation. The V-Flute dataset \cite{vflute} addresses this by re-casting this as predicting whether an image containing humorous elements or visual metaphors \emph{entails} a given description, while providing justification. We focus on the generative version of this task, proposing a method to enhance humor explanation and a framework for automatic evaluation.

% \citet{li-etal-2024-enhancing-advanced} introduce multimodal in-context learning but focus on discriminative tasks such as Winoground \cite{winoground} and Visual Commonsense Reasoning \cite{vcr}.

\paragraph{Iterative LLM-based Reasoning.} Many methods elicit knowledge from the LLM for intermediate reasoning steps.  
\citet{shwartz-etal-2020-unsupervised} elicited clarification questions and answers, then incorporated these in the input. Modern Few-shot prompting removed the need for supervision for these explanations \cite{marasovic-etal-2022-shot,wiegreffe-etal-2022-reframing}. One popular approach is Chain-of-Thought \cite[CoT;][]{cot}. CoT steers LLMs to generate intermediate reasoning steps towards the final answer, improving multi-step arithmetic, commonsense, and symbolic reasoning tasks. Relevant successor approaches include self-refine \cite{madaan2023selfrefine} which prompts LLMs to iteratively improve their answers with self-generated feedback. Eliciting knowledge from LLMs to improve predictions has been used for opinion understanding \cite{hwang-etal-2024-graph, hoyle-etal-2023-natural}, factuality \cite{akyurek-etal-2024-deductive}, and consistency \cite{liang-et-al-consistency}.

CoT has been adapted to the vision and language setting \cite{zhang2024multimodal} by adding external knowledge \cite{kam-cot}, extracting a scene graph \cite{MitraCCoT}, or using visual sketches as intermediate reasoning steps \cite{hu2024visual}. 
Most existing works focus on benchmarks such as ScienceQA \cite{scienceqa} and visual commonsense reasoning \cite{okvqa}, with (a) definitive/objective answers; and (b) simple evaluation metrics (e.g., ScienceQA is multiple-choice). We focus on multimodal explanation generation tasks in which the answers are open-ended and nuanced. As in CoT, we elicit intermediate reasoning steps from the models, but propose a novel method using the information bottleneck principle to guide generation and selection of useful knowledge for a correct explanation. 

\paragraph{Information Bottleneck Principle.} The Information Bottleneck principle \cite[IB;][]{ib}, based on information theory, extracts relevant information from an input while minimizing redundancy (Sec.~\ref{sec:ib}). It has been applied to a wide range of tasks \cite{ib-review}, including representation learning \cite{graph-ib, lee2021compressive}, deep learning \cite{michael2018on, icml2023kzxinfodl}, summarization \cite{west-etal-2019-bottlesum, ju-etal-2021-leveraging-information, li-etal-2021-ease}, speech recognition \cite{hecht09c_interspeech}, and multimodal learning \cite{Mai_2023, FangWZHZXW024}. Most prior works apply the IB principle during training to learn useful feature representations, with the exception of \citet{west-etal-2019-bottlesum, ju-etal-2021-leveraging-information}, who use IB for unsupervised summarization. In this work, we extend the IB principle to multimodal humor understanding to identify relevant LLM world knowledge.
