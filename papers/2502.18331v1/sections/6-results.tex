

\input{figures/experimental_result_overall}

\input{figures/ablation_results}

\begin{figure}[t]
  \includegraphics[width=\linewidth]{figures/good-example-small.pdf} \hfill
  \vspace{-10pt}
  \caption{An example analysis of the explanations of \base{} and \method{} for a New Yorker Cartoon, using SentenceSHAP. Implications are sorted according to their SentenceSHAP score from most to least important.}
  \label{fig:shap-example}
\end{figure}

We present the comparison of \method{} to the baselines (\S\ref{sec:results:rq1}), look into the contribution of each individual component in our method (\S\ref{sec:results:rq2}), justify the IB framework (\S\ref{sec:results:rq3}), and present an error analysis of our method's predictions  (\S\ref{sec:results:rq4}).  

% \begin{itemize}
%   \item RQ1. How does the performance of our method compare to that of VLM Zero-shot or CoT variants? Table \ref{tab:overall}
% % zs: memecap: 3.4, 4.0, 1.4, 8.2: 4.25
% % zs: newyorker: 4.3, 0.1, 0.6 : 1.6
% % zs: yesbut: 2.4, 2.0, 2.8, 1.4: 2.15
% % cot: memecap: 4.0, 6.2, 2.6, 5.7: 4.62
% % cot: newyorker: 19.4, 14.4, 9.1, 8.8 : 12.9
% % cot: yesbut: 21.7, 16.0, 10.8, 10.4: 14.7
% best memecap: 3.4,4.0, 1.4, 8.2
% best newyorker: 4.3, 0.1, 0.6, 1.6
% best yesbut: 2.4, 2.0, 2.8, 1.4
%   \item RQ2. How does our method compare to VLMs that undergo iterative refinement? Table \ref{tab:overall}
%   \item RQ3. Why do we need IB? TBD.. Feel free to suggest something
%   \item RQ4. What types of input provide the most useful information for our method? Table \ref{tab:ablation}
%   \item RQ5. What are the key success and failure cases in our method's application?
% \end{itemize} 

% sr: memecap: 6.7, 2.0, 0.1, 1.5: 2.57
% sr: newyorker: 2.2, 1.8, 3.0: 2.3
% sr: yesbut: 4.6, 1.8, 2.2, 3.9: 3.12
% sr-no: memecap: 4.0, 1.6, 1.0, 5.6: 3.05
% sr-no: newyorker: 1.3, 3.8, 0.3: 1.8
% sr-no: yesbut: 3.9, 4.6, 1.7, 3.6: 3.45

\subsection{Comparison to the Baselines}
\label{sec:results:rq1}
% While Phi performs the worst among the larger models, our method still boosts its performance by an average of 3.5 F$_1$ points across datasets.
Table~\ref{tab:overall} presents the overall experimental results. Compared to the best of \base{} and \chain{}, \method{} improves an average of 4.2, 1.6, and 2.1 $F_1$ points on the MemeCap, NewYorker, and YesBut datasets, respectively, across models. Among all models, GPT-4o performs best, averaging 3.4 F$_1$ point improvement across datasets. 
\method{} significantly boosts recall while maintaining comparable precision. This suggests that our method effectively integrates external knowledge to generate more comprehensive final explanations, with a slight precision drop due to potential noise.

\base{} performs reasonably well, likely due to these strong VLMs trained on similar tasks. However, \chain{} causes a substantial performance drop. We observe that \chain{}'s reasoning often leads the model to produce more generic explanations and lose focus on explaining the humor.

The self-refine baselines perform similarly to \base{}, with \critic{} slightly outperforming \nocritic{}. This suggests that merely refining the output without adding new information might not be beneficial for these tasks. Furthermore, incorrect feedback from \critic{} could even negatively impact the performance.  
In contrast, \method{} outperforms both self-refinement baselines, improving an average of 2.8, 2.0, and 3.3 $F_1$ points on the MemeCap, NewYorker, and YesBut datasets, respectively; supporting our hypothesis that humor understanding requires additional world knowledge, which \method{} can successfully integrate into the reasoning process.
% , highlighting the importance of incorporating world knowledge in humor understanding

\subsection{Contribution of Individual Components} 
\label{sec:results:rq2}

Since our method introduces several modifications to the standard prompting approach, we assess the contribution of each individual component to the final performance. We conduct ablation tests and employ an explainability technique to point to the features that the model relies on most. 

\paragraph{Ablation study.} Table~\ref{tab:ablation} presents an ablation study where only a single input is provided after refining implications and candidate explanations. GPT-4o and Phi perform better with both inputs, suggesting they effectively integrate relevant information from both to generate improved explanations. In contrast, Flash-1.5 and Qwen2 models rely more on the candidate responses, which contain more readily-useful information than the implications, indicating these models are less proficient at ignoring noisy or irrelevant implications. 
% This indicates that when implications include distracting or irrelevant information, these models are more likely to incorporate it into the final explanation, decreasing the quality of the final explanation.

\paragraph{Feature importance.} To further pinpoint the contribution of individual implications to the final explanations, we turn to interpretability methods.  
We adapt TokenSHAP \cite{horovicz-goldshmidt-2024-tokenshap}, which estimates the importance of individual tokens to the model's prediction using Monte Carlo Shapley value estimation, to a sentence-level variation that we refer to as SentenceSHAP (see Appendix~\ref{app:sentence-shap} for details). This approach visualizes each sentence's contribution to the final explanation, as shown in Figure~\ref{fig:shap-example}. The explanation from \base{} misses the humor in the long CVS receipt that the officer is holding as a badge of honor, while \method{} is directly informed by the top implication. 

\subsection{Assessment of the IB Framework} 
\label{sec:results:rq3}
\paragraph{IB component analysis.}
We focus on GPT4o, the best performing model across all datasets, and analyze the contribution of each IB component in our method through ablation tests. We evaluate four implication selection approaches (iterative refinement; Sec.~\ref{sec:method:refine-imps}): (1) \textit{Random}, where implications are selected randomly; (2) \textit{Cosine}, which selects implications with the lowest cosine similarity to the previous inputs; (3) \textit{CE}, which selects implications that yield the lowest cross-entropy value when we condition on them to generate the candidate explanations; and (4) \textit{Cosine+CE}, our method presented in Sec.~\ref{sec:method:refine-imps} that combines cosine similarity and cross-entropy based on the IB principle. We conduct the analysis on 100 random instances from each dataset. Figure~\ref{fig:ib-performance} shows that \textit{Cosine+CE} method outperforms the \textit{Cosine} and \textit{CE} baselines, improving $F_1$ score by 4.8 and 2.3 points, respectively, confirming the importance of balancing reducing redundancy with increasing the signal. 

\paragraph{Quality of intermediate explanations.} 
To analyze whether the candidate explanations improve across iterations, we randomly sample 50 examples from each dataset and their outputs generated by GPT-4o and Flash1.5. Since each iteration generates three candidate explanations, we report the highest $F_1$ score among them, and the corresponding precision and recall values in Table~\ref{tab:hop-analysis}. For GPT-4o, $F_1$ scores consistently improve across iterations, primarily driven by recall, which increases by an average of 11.4 points at $h_2$ compared to the initial hop. Precision also improves significantly at $h_1$, averaging an 8.0 point gain across datasets, then stabilizes.
A similar trend is observed in Flash1.5-8B, a considerably smaller model, except for the MemeCap, where $F_1$ scores peak at $h_1$ but decrease by 2.5 points at $h_2$. While precision remains similar at the final hop compared to $h_1$, recall drops by 2.4 points, suggesting smaller models are more susceptible to noisy information as iterations progress.

\begin{figure}[t]
\small
\centering
  \includegraphics[width=0.9\linewidth]{figures/ib-analysis-gpt4o.pdf}
  \caption {Performance of GPT4o on different IB components.}
  \label{fig:ib-performance}
\end{figure}


\begin{table}[h]
\small
\centering
\begin{tabular}{lc|ccc|ccc} % Added one more column (l)
\toprule
& & \multicolumn{3}{c|}{\textbf{GPT4o}} & \multicolumn{3}{c}{\textbf{Flash1.5}} \\
 & \textbf{} & $\mathbf{h_{0}}$ & $\mathbf{h_{1}}$ & $\mathbf{h_{2}}$ & $\mathbf{h_{0}}$ & $\mathbf{h_{1}}$ & $\mathbf{h_{2}}$ \\
\midrule
\textbf{MC} & P & 88.5 & \textbf{92.7} & \textbf{92.7} & 81.0 & 92.2 & \textbf{92.3} \\
  & R & 35.6 & 47.0 & \textbf{48.5} & 21.0 & \textbf{35.0} & 32.6\\
 & $F_1$ & 50.8 & 62.4 & \textbf{63.6} & 33.3 & \textbf{50.7} & 48.2\\
\midrule
\textbf{NY} & P & 79.6 & \textbf{86.5} & 84.7 & 72.2 & \textbf{83.5} & \textbf{83.5}\\
 & R & 50.6 & 57.9 & \textbf{62.8} & 22.9 & 33.4 & \textbf{34.7} \\
 & $F_1$ & 61.9 & 69.4 & \textbf{72.1} & 34.8 & 47.7 & \textbf{49.0} \\
\midrule
\textbf{YB} & P & 67.2 & \textbf{82.3} & 82.0  & 81.0 & 92.2 & \textbf{92.3} \\
 & R & 48.2 & 56.2 & \textbf{57.6} & 26.2 & 36.8 & \textbf{38.1} \\
 & $F_1$ & 56.2 & 66.8 & \textbf{67.6} & 39.6 & 52.6 & \textbf{54.0}\\
\bottomrule
\end{tabular}
\caption{Precision, Recall, and $F_1$ scores on intermediate explanations across hops. h stands for hop.}
\label{tab:hop-analysis}
\vspace{-10pt}
\end{table}



\paragraph{Error analysis.} 
\label{sec:results:rq4}
We manually analyzed 40 randomly sampled explanations across different models where implications negatively impacted performance. The two most common errors are: dilution of focus (81.2\%) and introducing irrelevant information (18.7\%).
Dilution of focus occurs when implications repeat the same concept multiple times or include overly generalized statements that override more specific details. Irrelevant information, such as common phrases unrelated to the humor can also distort the explanation. See Appendix \ref{app:error-analysis-shap} for examples analyzed using SentenceSHAP.
