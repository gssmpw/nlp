\begin{figure*}[t]
  \includegraphics[width=\linewidth]{figures/overall_flow.pdf} \hfill
  \vspace{-8pt}
  \caption {Overview of \method{}. We begin by generating descriptions, implications, and a candidate explanation (steps 1 and 2). Then, we refine the implications and candidate explanations over $h$ iterations using the IB principle (steps 3 to 5), ultimately generating a final explanation from the refined implications and candidate explanations (step 6).}
  \label{fig:overview}
\end{figure*}

Given a humoristic image along with an accompanying text (\textit{caption}), our goal is to generate a descriptive explanation of the humor. For example, in Figure~\ref{fig:overview}, a fairy woman with a plunger looking at a boy can be humorously explained as ``The humor comes from a fairy with a plunger, taking a side job because of a tough economy'' \cite[from the NewYorker dataset;][]{hessel-etal-2023-androids}.

We propose \method{} (Figure~\ref{fig:overview}), a multihop reasoning method inspired by the IB principle (Sec.~\ref{sec:ib}).
We integrate the visual and textual components to generate implications (Sec.~\ref{sec:method:gen_imps}). We then select the most useful implications by employing the IB principle (Sec.~\ref{sec:method:refine-imps}), and add them to the input to generate candidate explanations  (Sec.~\ref{sec:method:gen_response}). This iterative process alternates between refining implications and explanations.

\subsection{The Information Bottleneck Principle}
\label{sec:ib}

We use the Information Bottleneck principle \cite[IB;][]{ib} to select useful implications in \method{}.
IB aims to extract the most relevant information from a given input variable while minimizing redundancy. Specifically, IB seeks to compress the input source $S$ into a representation $\hat{S}$ while retaining the information most relevant to predicting the target $Y$. This objective is formulated as minimizing the following equation:
% \vspace{-1mm}
\begin{align}
    \notag
    I(S, \hat{S}) - \alpha I(\hat{S}, Y)
\end{align}
where $I$ denotes mutual information, and $\alpha$ is a parameter to balance compression term $I(S, \hat{S})$ with relevance term $I(\hat{S}, Y)$.

% \paragraph{Notation}
% Let $M$ represent the total number of images. For each image $I_m$, where $m$ ranges from 1 to $M$, the following components are associated:

% - The \textbf{caption} of the image is denoted as $C_m$.

% - The \textbf{description} of the image is represented as $D_m$, which consists of sentences $\{ d_m^1, d_m^2, \dots, d_m^{i} \}$. Here, $i$ is the total number of sentences in the description.

% - The \textbf{implications} at hop $h$ are denoted as $P_m^{h,j} = \{ p_m^{h,1}, p_m^{h,2}, \dots, p_m^{h,j} \}$, where $p_m^{h,j}$ represents the $j$-th implication at hop $h$.

% - The \textbf{candidate explanation} set at hop $h$ is represented as $R_m^{h,k} = \{ r_m^{h,1}, r_m^{h,2}, \dots, r_m^{h,k} \}$. Here, $r_m^{h,k}$ is the $k$-th candidate explanation at hop $h$.

\subsection{Eliciting Multi-Hop Implications} 
\label{sec:method:gen_imps}

First, we generate a set of natural language implications of the input.
The goal of this step is to discover connections across different objects, concepts, and situations described in the input.

\paragraph{Image Descriptions.} 
As a first step, we provide the image $I$ to a VLM to generate a detailed \textit{image description} $D$, focusing on the scene and objects while ignoring the humoristic meaning behind the image. We limit the description to a maximum of five sentences. 

\paragraph{Implications.} 
Using these descriptions, the VLM elicits \textit{implications}: commonsense knowledge, social norms, and possible connections for the objects in the description $D$ and the caption $C$. Implications generated at hop $h$ are denoted as $P^{h} = \{ p_1^{h}, p_2^{h}, \dots, p_j^{h} \}$.

In the first hop, the implications are derived from the image $I$, its caption $C$, and a subset of two image descriptions $D$, selected via a sliding window to balance efficiency (i.e., input length and cost) and coverage. 
From the second hop onward, we provide the VLM with \textit{candidate explanations} (see below) and one of the previously selected top-$k$ implications (Sec.~\ref{sec:method:refine-imps}). 

When the number of generated implications exceeds 15, we cluster them using sentence embeddings and select the implications closest to each cluster's centroid. This step reduces redundancy while preserving diversity.

%commonsense knowledge, or social norms associated with different objects, concepts, or situations, referred to as \textit{implications}. 

\paragraph{Candidate Explanations.} To guide implication selection for generating the correct output, we provide the image $I$ and caption $C$ to the VLM to generate a set of \textit{candidate explanations} at each hop: $R^{h} = \{ r_1^{h}, r_2^{h}, \dots, r_k^{h} \}$. One candidate explanation acts as an initial hypothesis, refined iteratively when additional information (implications) becomes available.
In the first hop, we generate candidate explanations by providing the VLM with the image $I$, caption $C$, and descriptions $D$. From the second hop onward, we condition---in addition to the previous inputs---on each of the $k$ implications selected in the previous hop (\S\ref{sec:method:refine-imps}) to generate $k$ candidate explanations. 
The prompts used for generating image descriptions, implications, and candidate explanations are in Appendix \ref{app:gen-prompts}.

\subsection{Selecting and Refining Useful Implications}
\label{sec:method:refine-imps}

We aim to select the top $k$ most useful implications at each hop, which should add meaningful information beyond the image and caption while providing relevant context for generating a target response. These requirements lend themselves to the two core IB components: compression and relevance.

\paragraph{Compression.} 

The compression term is used to ensure that new implications provide additional information beyond what is already known. We measure the redundancy of each implication generated in the current hop $h$, $\{P_j^{h}\}_{j=1}^J$ with the inputs $X^h = \{C, D, P^{h-1}\}$, which include the image, caption, and implications generated at previous hops (when applicable). We can think of this as testing whether the new set $X^h + \{P_j^{h}\}_{j=1}^J$ can be easily compressed back to $X^h$ (redundant). To that end, we embed each of the inputs using sentence embeddings and compute the maximum cosine similarity between the target implication and each input in $X^h$, representing the maximum redundancy with existing information:
% \[
% I(X, Z) = \min(\max_{i\in I, j \in J}(\text{CosSim}(X_{i}, P_{j}^{h})))
% \]
\[
\hat{I}(X, P_{j}^{h}) = \max_{i\in I}(\operatorname{cos}(X_{i}, P_{j}^{h}))
\]

\paragraph{Relevance.} The relevance term is used to ensure that implications provide useful information for generating a target explanation. Since our method is unsupervised, we use the VLM to generate candidate explanations at hop $h-1$: $Y = \{R^{h-1}\}_{i=1}^I$, which we use as a proxy for the gold standard answer in the next hop $h$. We measure the relevance of the target implication $P_j^{h}$ as the maximum probability (minimum cross entropy loss) for predicting the candidate explanation from the current (textual) inputs 
$\hat{Z}_j^{h} = \{C, D, P^{h-1}, P_j^{h}\}$, which include the caption, image description,  implications from previous hops, and the target 
implication: 
% \[
% I(Z, Y) = \min(\text{CE}(R_m^{h-1} \mid \sigma(\hat{Z}))
% \]
\[
\hat{I}(P_{j}^{h}, Y) = \min_{i \in I} (\operatorname{CE}(R_i^{h-1} \mid \hat{Z}_j^{h}))
\]
Cross-entropy values tend to be lower for short candidate explanations, leading to abnormally low scores for low-quality responses. To address this, we introduce a length penalty to adjust for deviations from the average response length. Responses significantly shorter or longer than the average receive a larger penalty.
We incorporate a scaling factor $\beta$, defined as the ratio of the average cross-entropy to the average length. The length penalty is then formulated as:
\[
LP_i = \beta \cdot |L_i - \bar{L}|, \quad \beta = \frac{\bar{CE}}{\bar{L}}
\]
\noindent where $L_i$ is a length for $i$-th candidate explanation, $\bar{L}$ is the mean token length across all candidate explanations, and $\bar{CE}$ is the mean cross-entropy loss across all candidate explanations.
The final relevance term for each implication becomes:
\[
\hat{I}(P_{j}^{h}, Y) = \min_{i \in I} (\operatorname{CE}(R_i^{h-1} \mid \hat{Z}_j^{h}) + LP)
\]
% \[
% I(Z, Y) = \min(\text{CE}(R_m^{h-1} \mid \sigma(\hat{Z})) + LP)
% \]
We use the open/efficient Qwen2-1.5B \cite{yang2024qwen2technicalreport} LLM to compute cross-entropy values.

\paragraph{Selecting Implications.} With these compression and relevance terms, we formulate the final IB-based objective function. Since the goal is to minimize redundancy (maximize compression) and maximize relevance, we select $k$ implications based on the following equation:
\begin{align}
\label{eq:select_impl}
& {\min\limits_{\underset{}{k}}}_{j \in J} \hat{I}(X, P_{j}^{h}) - \hat{I}(P_{j}^{h}, Y) = \\  
& {\min\limits_{\underset{}{k}}}_{j \in J} \left\{
\begin{array}{l}
    \max_{i\in I}(\operatorname{cos}(X_{i}, P_{j}^{h}))~+ \\
    \alpha \min_{i \in I} (\operatorname{CE}(R_i^{h-1} \mid \hat{Z}_j^{h}) + LP)
\end{array}
\right\} \notag
% \label{eq:select_impl}
\end{align}
\noindent where $\alpha$ is a hyperparameter that controls the trade-off between the compression and relevance terms. In our experiments, we set $\alpha = 0.7$.

We use the implications in each hop to refine the candidate explanations in the next hop and vice versa. To avoid excessive calculation during the implication refinement step, we keep the number of candidate explanations to a maximum of three based on the cross entropy scores computed using all existing inputs. These inputs, denoted as $\hat{Z}_j^{h} = \{C, D, P_j^{h}, R_i^{h-1}\}$, include caption, image descriptions, current hop implications, and previous hop candidate explanations. We then select top-$k$ candidate explanations ($k=3$) in current hop candidate explanations $R_i^{h}$ that minimize the cross-entropy: 
\begin{equation}
R_{\text{top-}k}^{h} = \arg min_{i \in I, |I| = k} \operatorname{CE}(R_i^{h} \mid \hat{Z}_j^{h})
\label{eq:select_response}
\end{equation}

\noindent In our experiments, we set the number of hops $H$ to 2 and the number of reasoning chains $k$ to 3. %, meaning that we find top-3 paths of 2-hop implications.  we set the number of reasoning hops $H$ to 2 and the number of reasoning chains $k$ to 3

\subsection{Generating Final Answer}
\label{sec:method:gen_response}

After $H$ iterations of refinement, we generate the final answer. As for candidate explanation generation in earlier hops, we provide the VLM with the image $I$, its caption $C$, the $k$ implications selected in the previous hop (Eq.~\ref{eq:select_impl}), and the $k$ candidate answers selected in the previous hop (Eq.~\ref{eq:select_response}), instructing it to generate a response.

We used Sentence Transformer\footnote{BAAI/bge-large-en-v1.5} for all sentence embeddings. The prompts for generating multi-hop implications and explanations are in Appendix \ref{app:gen-prompts}.