\paragraph{Data.}
All datasets used in our work, MemeCap, NewYorker, and YesBut, are publicly available. The datasets include images, accompanying texts, and humor interpretations collected from humans and may contain offensive content to some people.

\paragraph{Models.} 
The LLMs and VLMs we used for the experiments are trained on a large-scale web corpora and some of them utilize human feedback. Given their training sources, they could potentially generate content (i.e., descriptions, implications, and explanations) that exhibit societal biases.

\paragraph{Data Collection.} 
We use CloudResearch to collect judgments about model-generated explanations in order to validate our proposed  automatic evaluation method.  To ensure the quality of evaluation, we required that workers were located in English-speaking countries (e.g. US, UK, Canada, Australia, and New Zealand), and had an acceptance rate of at least 93\% on 1,000 prior annotations. We paid \$0.20 for the evaluation task, which means that annotators were compensated with an average hourly wage of \$13, which is comparable to the US minimum wage. We did not use any personal information from annotators. We obtained ethics approval from our institution's research ethics board prior to running the study. 