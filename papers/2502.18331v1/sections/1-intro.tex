% \textbf{Problem}:
% \paragraph{Novelty:}
% \paragraph{Related Work:}
% \paragraph{Contribution:}

Humor is an effective communication tool \cite{stauffer-humor, wanzer-humor, vartabedian-humor, kasulis-humor} that can manifest in various forms, including puns, exaggerated facial expressions, absurd behaviors, and incongruities \cite{philosophy-humor}. It is shaped by multiple factors such as culture, social interactions, societal phenomena, and personal imagination \cite{humorous-things, Warren-humor}. %, and can be expressed in images and text both explicitly and implicitly. Because of these factors, its interpretation varies based on age, cultural background, and context. While individuals may perceive humor differently, it remains an effective communication tool when used appropriately \cite{stauffer-humor, wanzer-humor, vartabedian-humor, kasulis-humor}.

In particular, humor is prevalent in online communications \cite{mcculloch2020because}, often spanning multiple modalities \cite[e.g., cartoons and memes;][]{shifman2013memes}. Interpreting humor across modalities requires ``reading between the lines'', connecting textual and visual elements to grasp the meaning \cite{Warren-humor}. 
For example, in Fig.~\ref{fig:fig1}, connecting the tooth fairy depicted in the image carrying a plunger to the caption, ``In this economy, it's good to have an extra trade'', creates the humorous interpretation that in this state of the economy, even the imaginary fairy needs a side job as a plumber.
% For example, in Fig.~\ref{fig:fig1}, the humor arises from recognizing the tooth fairy carrying a plunger in the image and linking it to the caption, ``In this economy, it's good to have an extra trade'', meaning even the imaginary fairy needs a side job as a plumber due to economic hardship.

\begin{figure}[t]
  \includegraphics[width=\linewidth]{figures/figure1.pdf} \hfill
  \vspace{-20pt}
  \caption{Humor understanding requires understanding world knowledge. \method{} aims to reduce redundancy in existing inputs (e.g. image descriptions) while increasing relevance to candidate explanations.}
  \label{fig:fig1}
\end{figure}

Several datasets for multimodal humor understanding tasks were proposed, where models are tasked with generating free-text humor explanations for an image and a caption \cite{hwang-shwartz-2023-memecap, hessel-etal-2023-androids, nandy-etal-2024-yesbut}. However, they are often overlooked in vision-and-language models (VLMs) evaluations, possibly due to the subjective nature of humor and the challenges in evaluating free-text explanations. With that said, VLMs have demonstrated remarkable visual reasoning capabilities on datasets requiring scientific knowledge \cite{scienceqa}, commonsense knowledge \cite{okvqa}, and spatial reasoning \cite{Liu2022VisualSR} and there is a prominent line of work on enhancing multimodal reasoning \cite{zhang2024multimodal, MitraCCoT, kam-cot, hu2024visual}.

In this paper, we introduce \method{}, a method inspired by the information bottleneck (IB) principle. \method{} leverages VLMs to generate and iteratively refine implications and explanations from an image and text, selecting those most relevant for explaining the humor in the image and maximizing information gain.
% These implications iteratively refine the generated explanations and vice versa.   
As an off-the-shelf method, it is applicable to any VLM. 

We evaluate \method{} on three multimodal humor explanation datasets: MemeCap \cite{hwang-shwartz-2023-memecap}, NewYorker \cite{hessel-etal-2023-androids}, and YesBut \cite{nandy-etal-2024-yesbut}. Prior work relied on reference-based automatic metrics that overlook lexical variability and the open-endedness of explanations and costly human evaluation. Leveraging the strong text understanding capabilities of LLMs, we propose new automatic evaluation metrics that resemble precision and recall, and better correlate with human judgments. \method{} improves $F_1$ by up to 8.2, 4.3, and 2.8 points on MemeCap, NewYorker, and YesBut, respectively, compared to zero-shot baselines and outperforms existing self-refine methods that merely iterate on and refine the explanation without generating intermediate implications. Our results highlight the importance of incorporating implications, paving the way for future research on incorporating diverse world knowledge in complex reasoning tasks.\footnote{Our code and data are available at:\\ \url{https://github.com/eujhwang/bottle-humor}}
% \footnote{We will make our code and data available upon publication.}
% Uncomment for the camera-ready version

% Our method lays the foundation for future research exploring the discovery of diverse world knowledge in complex problems requiring multifaceted knowledge integration.


% (3.4+4+1.4+8.2)/4 = 4.2
% (4.3+0.1+0.6)/3 = 1.7
% (2.4+2.0+2.8+1.4)/4 = 2.15
% reference : https://aclanthology.org/2024.findings-naacl.73.pdf
% 