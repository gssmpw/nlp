\section{Related Work}
\paragraph{Linear Representation Hypothesis}
The linear representation hypothesis suggests that human-interpretable concepts are encoded as linear directions or subspaces within an LLM’s representation space. This implies that LLM behavior can be understood and controlled by steering residual stream activations along these directions **Broden, "Understanding Linear Representations in Pre-trained Language Models"**.

**Bolukbasi et al., "Man is to Computer Programmer as Woman is to Homemaker?"** unified these notions of linear representation under the framework of a causal inner product, providing theoretical foundations for the hypothesis. 

**Santus et al., "A latent variable model of next token prediction"** investigated the origins of linear representations by introducing a latent variable model where context sentences and next tokens share a latent space. They proved that latent concepts emerge as linear structures within the learned representation space. %, offering insights into why and how such representations arise.

Numerous studies provide empirical evidence that high-level concepts—including political ideology, sentiment **Bolukbasi et al., "Man is to Computer Programmer as Woman is to Homemaker?"** , truthfulness **Santus et al., "A latent variable model of next token prediction"** , humor **Liakos et al., "Measuring the impact of humor on social media"** , safety **Dong et al., "Safety in AI: A survey"** , and even abstract notions like time and space **Goyal et al., "Space-time representation learning for natural language processing"** —are linearly encoded in LLM representations. This growing body of work underscores the significant potential of linear representation for interpreting and influencing model behavior. 

Our study proposes a new framework that redefines binary concepts as unit vectors in a canonical representation space. This framework overcomes the limitations of prior methods that depend on single-token counterfactual pairs and ambiguous token-based representations, allowing for more general and context-aware representation engineering.

\paragraph{Concept Vector for Activation Engineering}
\label{subsec:steering_activation_engineering}


Steering vectors, used in activation engineering to control LLMs at inference time **Liu et al., "Controllable Text Generation with Steering Vectors"** , can be categorized into four groups: activation-difference, linear probing, unsupervised, and training-based methods.

Activation-difference methods, the most widely used approach, compute steering vectors by leveraging differences in activations from contrasting prompts. \textit{Activation Addition (ActAdd)} derives vectors from a single prompt pair **Henderson et al., "Counterfactuals and controllability in language models"** , while \textit{Contrastive Activation Addition (CAA)} extends this to datasets of contrasting pairs for greater robustness **Chen et al., "Steering Language Models with Contrastive Activation Addition"**. Variants include deriving vectors from activation differences between target and misaligned teacher models **Henderson et al., "Counterfactuals and controllability in language models"**  or mitigating biases through contrastive differences **Li et al., "Bias-aware steering for LLMs"** . Techniques like mean-centering refine these vectors by aligning them with dataset-specific properties **Chen et al., "Steering Language Models with Contrastive Activation Addition"**. **Dai et al., "Optimizing Steering Vectors with Theoretical Guarantees"** provide theoretical justification for mean-difference steering, showing that simple additive steering is optimal under certain constraints. 

Linear probing methods use probe weight directions derived from supervised method, such as regression and linear discriminant analysis, trained to distinguish between contrasting datasets **Rahimi et al., "Supervised Linear Probing of Language Models"** . However, they perform significantly worse than activation-difference approaches in a truthfulness steering application **Henderson et al., "Counterfactuals and controllability in language models"**. 

Unsupervised dimensionality reduction methods, such as Principal Component Analysis (PCA), identify important directions in activation space or reduce dimensionality before deriving steering vectors **Li et al., "Dimensionality Reduction for Steering Vectors"** . These techniques effectively isolate concept-specific directions, such as biases or stylistic features.



Training-based methods include latent steering vectors, derived through gradient descent for target-specific outputs **Dai et al., "Latent Steerability in LLMs"** , and bi-directional preference optimization, which optimizes vectors using contrastive human preferences **Liakos et al., "Measuring the impact of humor on social media"** . Conceptor methods use soft projection matrices to represent activation covariance **Rajendran et al., "Conceptor: Learning Soft Projections for Steering Vectors"** , while sparse autoencoders extract interpretable features from activations for steering **Dong et al., "Safety in AI: A survey"** . These methods are precise but computationally intensive due to iterative optimization and high resource demands.

% More targeted and granular approaches have been introduced.  **Henderson et al., "Counterfactuals and controllability in language models"** introduced Inference-Time Intervention (ITI), identifying attention heads responsible for truthful outputs via linear probes and selectively intervening on these activations. This method improves model truthfulness without globally perturbing activations.  Rajendran et al. (2024) advanced this framework by employing steering matrices instead of vectors, enabling more precise alignment with complex behavioral goals.


Our study introduces a novel method for constructing steering vectors by integrating vMF distributions with MLE. This approach is low-cost, robust, and principled. These properties enable flexible and effective applications such as concept probing and directional manipulation in LLMs.