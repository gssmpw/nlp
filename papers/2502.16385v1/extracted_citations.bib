@inproceedings{adiladiscovering,
  title={Discovering Bias in Latent Space: An Unsupervised Debiasing Approach},
  author={Adila, Dyah and Zhang, Shuai and Han, Boran and Wang, Bernie},
  booktitle={Forty-first International Conference on Machine Learning}, 
  year={2024}
}

@article{arditi2024refusal,
  title={Refusal in language models is mediated by a single direction},
  author={Arditi, Andy and Obeso, Oscar and Syed, Aaquib and Paleka, Daniel and Panickssery, Nina and Gurnee, Wes and Nanda, Neel},
  journal={arXiv preprint arXiv:2406.11717},
  year={2024}
}

@inproceedings{burnsdiscovering,
  title={Discovering Latent Knowledge in Language Models Without Supervision},
  author={Burns, Collin and Ye, Haotian and Klein, Dan and Steinhardt, Jacob},
  booktitle={The Eleventh International Conference on Learning Representations},
  year={2023}
}

@article{cao2024personalized,
  title={Personalized Steering of Large Language Models: Versatile Steering Vectors Through Bi-directional Preference Optimization},
  author={Cao, Yuanpu and Zhang, Tianrong and Cao, Bochuan and Yin, Ziyi and Lin, Lu and Ma, Fenglong and Chen, Jinghui},
  journal={arXiv preprint arXiv:2406.00045},
  year={2024}
}

@inproceedings{chu2024causal,
  title={A causal explainable guardrails for large language models},
  author={Chu, Zhixuan and Wang, Yan and Li, Longfei and Wang, Zhibo and Qin, Zhan and Ren, Kui},
  booktitle={Proceedings of the 2024 on ACM SIGSAC Conference on Computer and Communications Security},
  pages={1136--1150},
  year={2024}
}

@article{gurnee2023language,
  title={Language models represent space and time},
  author={Gurnee, Wes and Tegmark, Max},
  journal={arXiv preprint arXiv:2310.02207},
  year={2023}
}

@inproceedings{hollinsworth2024language,
  title={Language Models Linearly Represent Sentiment},
  author={Hollinsworth, Oskar and Tigges, Curt and Geiger, Atticus and Nanda, Neel},
  booktitle={Proceedings of the 7th BlackboxNLP Workshop: Analyzing and Interpreting Neural Networks for NLP},
  pages={58--87},
  year={2024}
}

@inproceedings{jiangorigins,
  title={On the Origins of Linear Representations in Large Language Models},
  author={Jiang, Yibo and Rajendran, Goutham and Ravikumar, Pradeep Kumar and Aragam, Bryon and Veitch, Victor},
  booktitle={Forty-first International Conference on Machine Learning}, 
year = {2024}
}

@article{jorgensen2023improving,
  title={Improving activation steering in language models with mean-centring},
  author={Jorgensen, Ole and Cope, Dylan and Schoots, Nandi and Shanahan, Murray},
  journal={arXiv preprint arXiv:2312.03813},
  year={2023}
}

@article{li2024inference,
  title={Inference-time intervention: Eliciting truthful answers from a language model},
  author={Li, Kenneth and Patel, Oam and Vi{\'e}gas, Fernanda and Pfister, Hanspeter and Wattenberg, Martin},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2024}
}

@article{liu2023context,
  title={In-context vectors: Making in context learning more effective and controllable through latent space steering},
  author={Liu, Sheng and Ye, Haotian and Xing, Lei and Zou, James},
  journal={arXiv preprint arXiv:2311.06668},
  year={2024}
}

@article{mallen2023eliciting,
  title={Eliciting latent knowledge from quirky language models},
  author={Mallen, Alex and Brumley, Madeline and Kharchenko, Julia and Belrose, Nora},
  journal={arXiv preprint arXiv:2312.01037},
  year={2023}
}

@article{marks2023geometry,
  title={The geometry of truth: Emergent linear structure in large language model representations of true/false datasets},
  author={Marks, Samuel and Tegmark, Max},
  journal={arXiv preprint arXiv:2310.06824},
  year={2023}
}

@article{o2024steering,
  title={Steering language model refusal with sparse autoencoders},
  author={O'Brien, Kyle and Majercak, David and Fernandes, Xavier and Edgar, Richard and Chen, Jingya and Nori, Harsha and Carignan, Dean and Horvitz, Eric and Poursabzi-Sangde, Forough},
  journal={arXiv preprint arXiv:2411.11296},
  year={2024}
}

@article{panickssery2023steering,
  title={Steering llama 2 via contrastive activation addition},
  author={Panickssery, Nina and Gabrieli, Nick and Schulz, Julian and Tong, Meg and Hubinger, Evan and Turner, Alexander Matt},
  journal={arXiv preprint arXiv:2312.06681},
  year={2023}
}

@article{postmus2024steering,
  title={Steering Large Language Models using Conceptors: Improving Addition-Based Activation Engineering},
  author={Postmus, Joris and Abreu, Steven},
  journal={arXiv preprint arXiv:2410.16314},
  year={2024}
}

@inproceedings{rimsky-etal-2024-steering,
    title = "Steering Llama 2 via Contrastive Activation Addition",
    author = "Rimsky, Nina  and
      Gabrieli, Nick  and
      Schulz, Julian  and
      Tong, Meg  and
      Hubinger, Evan  and
      Turner, Alexander",
    editor = "Ku, Lun-Wei  and
      Martins, Andre  and
      Srikumar, Vivek",
    booktitle = "Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = aug,
    year = "2024",
    address = "Bangkok, Thailand",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2024.acl-long.828/",
    doi = "10.18653/v1/2024.acl-long.828",
    pages = "15504--15522",
    abstract = "We introduce Contrastive Activation Addition (CAA), a method for steering language models by modifying their activations during forward passes. CAA computes {\textquotedblleft}steering vectors{\textquotedblright} by averaging the difference in residual stream activations between pairs of positive and negative examples of a particular behavior, such as factual versus hallucinatory responses. During inference, these steering vectors are added at all token positions after the user`s prompt with either a positive or negative coefficient, allowing precise control over the degree of the targeted behavior. We evaluate CAA`s effectiveness on Llama 2 Chat using multiple-choice behavioral question datasets and open-ended generation tasks. We demonstrate that CAA significantly alters model behavior, is effective over and on top of traditional methods like finetuning and system prompt design, and minimally reduces capabilities. Moreover, we gain deeper insights into CAA`s mechanisms by employing various activation space interpretation methods. CAA accurately steers model outputs and sheds light on how high-level concepts are represented in Large Language Models (LLMs)."
}

@inproceedings{singhrepresentation,
  title={Representation Surgery: Theory and Practice of Affine Steering},
  author={Singh, Shashwat and Ravfogel, Shauli and Herzig, Jonathan and Aharoni, Roee and Cotterell, Ryan and Kumaraguru, Ponnurangam},
  booktitle={Forty-first International Conference on Machine Learning}, 
   year={2024}
}

@inproceedings{subramani2022extracting,
  title={Extracting Latent Steering Vectors from Pretrained Language Models},
  author={Subramani, Nishant and Suresh, Nivedita and Peters, Matthew E},
  booktitle={Findings of the Association for Computational Linguistics: ACL 2022},
  pages={566--581},
  year={2022}
}

@article{tigges2023linear,
  title={Linear representations of sentiment in large language models},
  author={Tigges, Curt and Hollinsworth, Oskar John and Geiger, Atticus and Nanda, Neel},
  journal={arXiv preprint arXiv:2310.15154},
  year={2023}
}

@misc{turner2024steeringlanguagemodelsactivation,
      title={Steering Language Models With Activation Engineering}, 
      author={Alexander Matt Turner and Lisa Thiergart and Gavin Leech and David Udell and Juan J. Vazquez and Ulisse Mini and Monte MacDiarmid},
      year={2024},
      eprint={2308.10248},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2308.10248}, 
}

@inproceedings{vonlanguage,
  title={A Language Modelâ€™s Guide Through Latent Space},
  author={von R{\"u}tte, Dimitri and Anagnostidis, Sotiris and Bachmann, Gregor and Hofmann, Thomas},
  booktitle={Forty-first International Conference on Machine Learning},
year = {2024}
}

@inproceedings{wang2024trojan,
  title={Trojan activation attack: Red-teaming large language models using steering vectors for safety-alignment},
  author={Wang, Haoran and Shu, Kai},
  booktitle={Proceedings of the 33rd ACM International Conference on Information and Knowledge Management},
  pages={2347--2357},
  year={2024}
}

@article{wu2024reft,
  title={Reft: Representation finetuning for language models},
  author={Wu, Zhengxuan and Arora, Aryaman and Wang, Zheng and Geiger, Atticus and Jurafsky, Dan and Manning, Christopher D and Potts, Christopher},
  journal={Advancess in Neural Information Processing Systems},
  year={2024}
}

@article{zhao2024steering,
  title={Steering knowledge selection behaviours in LLMs via sae-based representation engineering},
  author={Zhao, Yu and Devoto, Alessio and Hong, Giwon and Du, Xiaotang and Gema, Aryo Pradipta and Wang, Hongru and Wong, Kam-Fai and Minervini, Pasquale},
  journal={arXiv preprint arXiv:2410.15999},
  year={2024}
}

@misc{zou2023transparency,
      title={Representation Engineering: A Top-Down Approach to AI Transparency}, 
      author="Andy Zou and Long Phan and Sarah Chen and James Campbell and Phillip Guo and Richard Ren and Alexander Pan and Xuwang Yin and Mantas Mazeika and Ann-Kathrin Dombrowski and Shashwat Goel and Nathaniel Li and Michael J. Byun and Zifan Wang and Alex Mallen and Steven Basart and Sanmi Koyejo and Dawn Song and Matt Fredrikson and Zico Kolter and Dan Hendrycks",
      year={2023},
      eprint={2310.01405},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

