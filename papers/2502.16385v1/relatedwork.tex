\section{Related Work}
\paragraph{Linear Representation Hypothesis}
The linear representation hypothesis suggests that human-interpretable concepts are encoded as linear directions or subspaces within an LLM’s representation space. This implies that LLM behavior can be understood and controlled by steering residual stream activations along these directions \citep{singhrepresentation, zou2023transparency}.


\citet{park2024the} unified these notions of linear representation under the framework of a causal inner product, providing theoretical foundations for the hypothesis. 

\citet{jiangorigins} investigated the origins of linear representations by introducing a latent variable model where context sentences and next tokens share a latent space. They proved that latent concepts emerge as linear structures within the learned representation space. %, offering insights into why and how such representations arise.

Numerous studies provide empirical evidence that high-level concepts—including political ideology, sentiment \citep{tigges2023linear, hollinsworth2024language}, truthfulness \citep{zou2023transparency, li2024inference, marks2023geometry}, humor \citep{vonlanguage}, safety \citep{arditi2024refusal}, and even abstract notions like time and space \citep{gurnee2023language}—are linearly encoded in LLM representations. This growing body of work underscores the significant potential of linear representation for interpreting and influencing model behavior. 

Our study proposes a new framework that redefines binary concepts as unit vectors in a canonical representation space. This framework overcomes the limitations of prior methods that depend on single-token counterfactual pairs and ambiguous token-based representations, allowing for more general and context-aware representation engineering.

\paragraph{Concept Vector for Activation Engineering}
\label{subsec:steering_activation_engineering}


Steering vectors, used in activation engineering to control LLMs at inference time \citep{li2024inference, zhao2024steering}, can be categorized into four groups: activation-difference, linear probing, unsupervised, and training-based methods.

Activation-difference methods, the most widely used approach, compute steering vectors by leveraging differences in activations from contrasting prompts. \textit{Activation Addition (ActAdd)} derives vectors from a single prompt pair \citep{turner2024steeringlanguagemodelsactivation}, while \textit{Contrastive Activation Addition (CAA)} extends this to datasets of contrasting pairs for greater robustness \citep{rimsky-etal-2024-steering}. Variants include deriving vectors from activation differences between target and misaligned teacher models \citep{wang2024trojan} or mitigating biases through contrastive differences \citep{chu2024causal, arditi2024refusal}. Techniques like mean-centering refine these vectors by aligning them with dataset-specific properties \citep{jorgensen2023improving, postmus2024steering, panickssery2023steering}. \citet{singhrepresentation} provide theoretical justification for mean-difference steering, showing that simple additive steering is optimal under certain constraints. 

Linear probing methods use probe weight directions derived from supervised method, such as regression and linear discriminant analysis, trained to distinguish between contrasting datasets~\citep{zhao2024steering, mallen2023eliciting, park2024the}. However, they perform significantly worse than activation-difference approaches in a truthfulness steering application~\citep{li2024inference}. 

Unsupervised dimensionality reduction methods, such as Principal Component Analysis (PCA), identify important directions in activation space or reduce dimensionality before deriving steering vectors \citep{zou2023transparency, liu2023context, adiladiscovering, wu2024reft, park2024the, burnsdiscovering}. These techniques effectively isolate concept-specific directions, such as biases or stylistic features.



Training-based methods include latent steering vectors, derived through gradient descent for target-specific outputs \citep{subramani2022extracting}, and bi-directional preference optimization, which optimizes vectors using contrastive human preferences \citep{cao2024personalized}. Conceptor methods use soft projection matrices to represent activation covariance \citep{postmus2024steering}, while sparse autoencoders extract interpretable features from activations for steering \citep{o2024steering, zhao2024steering}. These methods are precise but computationally intensive due to iterative optimization and high resource demands.

% More targeted and granular approaches have been introduced.  \citet{li2024inference} introduced Inference-Time Intervention (ITI), identifying attention heads responsible for truthful outputs via linear probes and selectively intervening on these activations. This method improves model truthfulness without globally perturbing activations.  Rajendran et al. (2024) advanced this framework by employing steering matrices instead of vectors, enabling more precise alignment with complex behavioral goals.


Our study introduces a novel method for constructing steering vectors by integrating vMF distributions with MLE. This approach is low-cost, robust, and principled. These properties enable flexible and effective applications such as concept probing and directional manipulation in LLMs.