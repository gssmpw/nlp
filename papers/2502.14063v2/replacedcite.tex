\section{Related Works}
\paragraph{Multimodal Fusion Challenges}
Multimodal information, such as RGB and infrared spectra, has been extensively studied for improving pedestrian detection performance. Despite its potential, many existing methods struggle to effectively harness the complementary advantages of multimodal features. The YOLO series, first introduced by Joseph et al. ____, has demonstrated excellent performance in single-modal detection tasks. However, its original architecture lacks specific mechanisms for multimodal fusion. Subsequent versions, such as YOLOv3 ____ and its successors ____, focus primarily on structural optimization and speed improvements without addressing the challenges of multimodal integration.

To address these limitations, researchers have proposed various solutions. For example, Hwang et al. ____ introduced the KAIST dataset, a large-scale multispectral pedestrian benchmark with well-aligned visible and thermal images, along with dense pedestrian annotations. They also proposed a method that extracts Aggregated Channel Features (ACF+T+THOG) and employs Boosted Decision Trees (BDT) for classification. However, this approach relies on shallow feature fusion, limiting its adaptability to environmental changes and resulting in instability under complex scenarios.

Wagner et al. ____ pioneered the application of deep neural networks (DNNs) in multispectral pedestrian detection and compared early and late fusion strategies. Liu et al. ____ extended this research by applying Faster R-CNN  ____ to multispectral pedestrian detection and designing four ConvNet-based fusion architectures. Among these, the Halfway Fusion model, which merges middle-layer convolutional features from dual-branch ConvNets, achieved the best results. König et al. ____ introduced the Fusion RPN+BDT model, integrating dual-stream DNNs at middle convolutional layers for enhanced fusion.


Recently, Park et al.  ____ proposed a three-branch DNN architecture to handle multimodal inputs, incorporating a Channel Weighted Fusion (CWF) layer to enhance detection performance by adaptively weighting the contributions of each modality. Loveday et al.  ____ developed an orthogonal dual-camera imaging system to capture parallax-free, well-aligned multispectral images. Their findings demonstrated that combining visible and infrared data significantly improves foreground object detection compared to using single-modal data.
\paragraph{Illumination Robustness Challenges}
With the advancement of object detection ____, pedestrian detection methods have divided into two main methods: one-stage detectors [14], and two-stage detectors [15-16],. However, these detectors predominantly rely on RGB images, leading to performance degradation under low-light or extreme illumination conditions.


The YOLO series ____, while excelling under standard lighting conditions, exhibits significant limitations in handling complex illumination scenarios. Models such as YOLOv5 and YOLOv6 improved detection speed and accuracy through structural optimizations but struggled to maintain robustness in extreme lighting environments. These algorithms often fail to effectively distinguish pedestrian features from background noise under conditions like uneven lighting or occlusion. Although RGB images provide rich texture and detail information, achieving reliable detection results in challenging conditions—such as extreme lighting, occlusions, and complex backgrounds—requires incorporating multimodal data.

Existing approaches have explored various fusion strategies for integrating multimodal information, including early fusion, late fusion, and intermediate fusion. Early fusion (pixel-level fusion) directly concatenates data from different modalities and processes it using conventional object detectors ____. Late fusion involves feeding each modality into separate single-modal detectors and subsequently merging the predicted bounding boxes using statistical methods ____. While simple, both early and late fusion methods often overlook the interdependencies between modalities, limiting their ability to fully exploit complementary features.

To address these limitations, recent studies have introduced \textit{illumination-aware feature fusion} and \textit{attention-based feature fusion} strategies. Illumination-aware fusion methods ____ typically incorporate a classification branch to determine the significance of RGB features based on lighting conditions. However, classification-based approaches cannot accurately reflect the importance of individual regions within an image. In contrast, attention-based methods leverage spatial attention, channel attention, or cross-attention mechanisms derived from transformers to facilitate feature fusion ____. Spatial and channel attention generate element-wise and channel-wise weighting factors for multispectral features, respectively, while cross-attention models global contextual correlations to resolve feature misalignment between modalities. Although cross-attention provides superior fusion capabilities, it incurs high computational costs, posing challenges for real-time applications.