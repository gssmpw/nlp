



















\newcommand{\supplementarytitle}{
    \twocolumn[
        \begin{center}
            \LARGE \textbf{PedDet: Adaptive Spectral Optimization for Multimodal Pedestrian Detection} \\
            \vspace{1em}
            \Large \textbf{- Supplementary Materials -}
            \vspace{1em}
        \end{center}
    ]
}


\supplementarytitle

\renewcommand{\thesection}{\arabic{section}}
\setcounter{section}{0}

\renewcommand{\thefigure}{\arabic{figure}}
\setcounter{figure}{0}

\renewcommand{\thetable}{\arabic{table}}
\setcounter{table}{0}

\section{Related Works}

\paragraph{Multimodal Fusion Challenges}
Multimodal information, such as RGB and infrared spectra, has been extensively studied for improving pedestrian detection performance. Despite its potential, many existing methods struggle to effectively harness the complementary advantages of multimodal features. The YOLO series, first introduced by Joseph et al. \cite{5}, has demonstrated excellent performance in single-modal detection tasks. However, its original architecture lacks specific mechanisms for multimodal fusion. Subsequent versions, such as YOLOv3 \cite{5} and its successors \cite{6}, focus primarily on structural optimization and speed improvements without addressing the challenges of multimodal integration.

To address these limitations, researchers have proposed various solutions. For example, Hwang et al. \cite{7} introduced the KAIST dataset, a large-scale multispectral pedestrian benchmark with well-aligned visible and thermal images, along with dense pedestrian annotations. They also proposed a method that extracts Aggregated Channel Features (ACF+T+THOG) and employs Boosted Decision Trees (BDT) for classification. However, this approach relies on shallow feature fusion, limiting its adaptability to environmental changes and resulting in instability under complex scenarios.

Wagner et al. \cite{8} pioneered the application of deep neural networks (DNNs) in multispectral pedestrian detection and compared early and late fusion strategies. Liu et al. \cite{9} extended this research by applying Faster R-CNN  \cite{10} to multispectral pedestrian detection and designing four ConvNet-based fusion architectures. Among these, the Halfway Fusion model, which merges middle-layer convolutional features from dual-branch ConvNets, achieved the best results. König et al. \cite{11} introduced the Fusion RPN+BDT model, integrating dual-stream DNNs at middle convolutional layers for enhanced fusion.


Recently, Park et al.  \cite{12} proposed a three-branch DNN architecture to handle multimodal inputs, incorporating a Channel Weighted Fusion (CWF) layer to enhance detection performance by adaptively weighting the contributions of each modality. Loveday et al.  \cite{13} developed an orthogonal dual-camera imaging system to capture parallax-free, well-aligned multispectral images. Their findings demonstrated that combining visible and infrared data significantly improves foreground object detection compared to using single-modal data.
\paragraph{Illumination Robustness Challenges}
With the advancement of object detection \cite{zhang2024meddet,cai2024medical,cai2024msdet}, pedestrian detection methods have divided into two main methods: one-stage detectors [14], and two-stage detectors [15-16],. However, these detectors predominantly rely on RGB images, leading to performance degradation under low-light or extreme illumination conditions.


The YOLO series \cite{6}, while excelling under standard lighting conditions, exhibits significant limitations in handling complex illumination scenarios. Models such as YOLOv5 and YOLOv6 improved detection speed and accuracy through structural optimizations but struggled to maintain robustness in extreme lighting environments. These algorithms often fail to effectively distinguish pedestrian features from background noise under conditions like uneven lighting or occlusion. Although RGB images provide rich texture and detail information, achieving reliable detection results in challenging conditions—such as extreme lighting, occlusions, and complex backgrounds—requires incorporating multimodal data.

Existing approaches have explored various fusion strategies for integrating multimodal information, including early fusion, late fusion, and intermediate fusion. Early fusion (pixel-level fusion) directly concatenates data from different modalities and processes it using conventional object detectors \cite{5,15,17}. Late fusion involves feeding each modality into separate single-modal detectors and subsequently merging the predicted bounding boxes using statistical methods \cite{18,19,20}. While simple, both early and late fusion methods often overlook the interdependencies between modalities, limiting their ability to fully exploit complementary features.

To address these limitations, recent studies have introduced \textit{illumination-aware feature fusion} and \textit{attention-based feature fusion} strategies. Illumination-aware fusion methods \cite{17,21,22,23} typically incorporate a classification branch to determine the significance of RGB features based on lighting conditions. However, classification-based approaches cannot accurately reflect the importance of individual regions within an image. In contrast, attention-based methods leverage spatial attention, channel attention, or cross-attention mechanisms derived from transformers to facilitate feature fusion \cite{9,24,25,26,27,28}. Spatial and channel attention generate element-wise and channel-wise weighting factors for multispectral features, respectively, while cross-attention models global contextual correlations to resolve feature misalignment between modalities. Although cross-attention provides superior fusion capabilities, it incurs high computational costs, posing challenges for real-time applications.




\section{Implement Details}

\paragraph{Epochs and Batch Size.} The training process was conducted for 50 epochs, with a batch size of 8 images per iteration.

\paragraph{Optimizer and Learning Rate.} We employed the Adam optimizer with an initial learning rate of 0.001. To address the issue of high learning rates during later training stages, we applied the Cosine Annealing learning rate scheduling strategy. This approach gradually decreases the learning rate throughout the training process, approaching zero at the end of each cycle. This dynamic decay effectively mitigates overfitting.

\paragraph{Weight Decay.} To further enhance model stability and generalization, a weight decay mechanism with a coefficient of 0.0005 was introduced. This regularization technique suppresses overly complex parameter updates in the network, improving the model’s ability to generalize to unseen data.
	
\paragraph{Warm-up Strategy.} To ensure a smooth transition into optimal training conditions, a warm-up strategy was applied during the first three epochs. This gradually increased the learning rate, preventing instability caused by large parameter adjustments in the early stages of training.
	
\paragraph{Multi-scale Training.} To improve the model's robustness and adaptability to varying object sizes, we incorporated a multi-scale training strategy. This involved randomly scaling images in different training batches, enhancing the network's ability to detect objects of various scales. This strategy is particularly advantageous for multimodal pedestrian detection, improving the detection of small objects while maintaining performance on larger targets.


The detailed configuration and hyperparameters are in Table \ref{tab:config}.


\begin{table}[h]
  \centering
\resizebox{\linewidth}{!}{
  \begin{tabular}{l c c}
    \toprule
    & \multicolumn{2}{c}{\textbf{Object Detectors}} \\
    \cmidrule(r){2-3}
    \textbf{Hyperparameter} & \textbf{Traditional YOLO} & \textbf{PedDet (Ours)} \\
    \midrule
    Learning Rate & 0.001 & 0.001 \\
    Epochs & 400 & 50 \\
    Batch Size & 16 & 8 \\
    Optimizer & SGD & AdamW \\
    Weight Decay & 0.0005 & 0.0005 \\
    Warmup Epochs & 3 & 5 \\
    Scheduler & Cosine & Cosine \\
    \bottomrule
  \end{tabular}
}
\caption{\textbf{Pedestrian detection training hyperparameters.} Comparison of hyperparameters used in traditional YOLO variants and our approach.}
\label{tab:config}
\end{table}


\paragraph{Training Analysis.}
As shown in Figure \ref{fig:fig_llvip} and \ref{fig:fig_msrs}, in the evaluation of performance metrics, both precision and recall show a relatively fluctuating upward trend during the training process. As training progresses, both precision and recall gradually improve, indicating that the model becomes increasingly proficient at distinguishing targets from non-targets. The initial fluctuations observed in these metrics during certain training phases suggest the model's iterative adjustments to optimize its detection performance.

\begin{figure}[h]
    \centering
    \includegraphics[width=\linewidth]{fig7}
    \caption{Training curves of LLVIP dataset.}
    \label{fig:fig_llvip}
    \includegraphics[width=\linewidth]{fig8}
    \caption{Training curves of MSRS dataset.}
    \label{fig:fig_msrs}
\end{figure}




\section{Datasets}


\paragraph{Multi-Spectral Road Scenarios (MSRS).}
The MSRS dataset is a refined version of the MFNet dataset, specifically designed for infrared-visible image fusion in multimodal applications. The original MFNet dataset comprises 1,569 image pairs (820 captured during the day and 749 at night) with a resolution of 480$\times$640. However, it suffers from several limitations, including misaligned image pairs and low signal-to-noise ratio (SNR) and contrast in most infrared images.

To address these issues, the MSRS dataset enhances data quality through several steps:
\begin{itemize}
	\item \textbf{Removal of Misaligned Image Pairs:} 125 misaligned pairs were excluded, resulting in 715 daytime and 729 nighttime image pairs.
	\item \textbf{Infrared Image Enhancement:} A dark channel prior-based image enhancement algorithm was applied to improve the contrast and SNR of the infrared images.
\end{itemize}

As a result, the MSRS dataset comprises 1,444 pairs of high-quality, aligned infrared and visible images, making it a reliable benchmark for multimodal pedestrian detection under diverse lighting conditions.

\paragraph{LLVIP.}
The LLVIP dataset is a more challenging multimodal dataset, tailored for pedestrian detection under low-light conditions. It includes a large number of pedestrians captured in various lighting environments, making it particularly valuable for testing low-light detection models.

The dataset was collected with a dual-mode camera system that integrates visible and infrared cameras, ensuring temporal and spatial consistency between image pairs. Each pair was carefully registered and cropped to ensure identical fields of view and dimensions. This strict alignment makes the dataset especially suitable for image fusion and image-to-image translation tasks.

The LLVIP dataset comprises 12,025 pairs of aligned visible-infrared images, with a training set of 3,463 pairs. Each image has a resolution of 1,024$\times$1,280, providing high-quality data for training and evaluation.

\section{Visualization of Results}
Figure \ref{fig:vis2} showcases typical examples of pedestrian detection in various environments, with each image highlighting the detected objects (pedestrians or vehicles) through bounding boxes and displaying the corresponding confidence scores. The detection results reflect the model's performance in handling complex visual scenes, especially its accuracy under different lighting conditions. In low-light or high-contrast environments, the model can still effectively identify pedestrians and exhibits strong robustness against background interference. This capability is made possible by the two core modules introduced in the paper: the Multi-scale Spectral Feature Perception Module (MSFPM) and the Illumination Robustness Feature Decoupling Module (IRFDM).

\begin{figure}[h]
    \centering
    \includegraphics[width=\linewidth]{EVR3.jpg}
    \caption{Visualization of PedDet results.}
    \label{fig:vis2}
\end{figure}




