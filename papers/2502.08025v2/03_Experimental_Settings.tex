\subsection{Data Preprocessing}
We followed the preprocessing method described in the literature \cite{calhas2022eeg}. 
Specifically, given an EEG-fMRI dataset with an EEG sampling rate of $f_s$ (Hz) and an fMRI Time Response (TR) of $f_{TR}$ (second), the EEG signal at each electrode is divided into non-overlapping windows of length $f_s \times f_{TR}$. 
Then, the Fast Fourier Transform (FFT) is applied to extract the frequency components of size $F$ from a 1-dimensional EEG window signal. 
An upper-band filter is applied to the FFT with a cutoff at 250 Hz, and the Direct Current (DC) component is removed. 
As a result, the frequency dimension is $F = 249$. 
For $C$ electrodes in EEG data, the frequency components for a single window have a size of $C \times F$. 
The temporal dimension $T$ of EEG data is set to 20 (i.e., 20 consecutive windows) so that an EEG sample $x \in \mathbb{R}^{20 \times C \times 249}$ corresponds to an fMRI volume $y \in \mathbb{R}^{D \times W \times H}$. 
We normalized fMRI data to range [0, 1] per each volume and the EEG data to range [0, 1] across the entire dataset. For more details on the data pre-processing, please refer to the literature \cite{calhas2022eeg} and our GitHub repository. 

\subsection{Datasets}
In this study, we used three publicly available simultaneous EEG and fMRI datasets namely NODDI \cite{deligianni2014relating}, Oddball \cite{walz2015prestimulus}, and CN-EPFL \cite{pereira2020disentangling}. 
The specific of each dataset is described as follows: 
\subsubsection{NODDI}
A dataset comprising simultaneous EEG-fMRI recordings from 17 healthy adults during resting-state sessions with eyes open. 
EEG was acquired using a 64-channel cap (10-20 system) at a 250Hz sampling rate, capturing millisecond-level neural signals. 
In parallel, fMRI data were collected using a Siemens Avanto 1.5 T clinical scanner with a time response (TR) of 2.16s. 
The resolution of a volume is $30 \times 64 \times 64$. 
Following \cite{lanzino2024nt}, we only processed 15 out of the original 17 participants, resulting in 4,110 paired EEG-fMRI samples. 

\subsubsection{Oddball}
The dataset includes EEG-fMRI recordings from 17 healthy adults performing an oddball paradigm involving auditory and visual tasks. 
EEG was recorded at a 1000 Hz sampling rate across 43 channels, capturing rapid neural oscillations linked to attentional engagement. 
Concurrently, fMRI scans were acquired in a $32 \times 64 \times 64$ volume at a 2-second TR using a Philips Achieva 3T clinical scanner with a single-channel head coil, capturing hemodynamic responses to these rare events. 
This dataset consists of 14,688 paired EEG-fMRI samples. 

\subsubsection{CN-EPFL}
This dataset was collected by the Center for Neuroprosthetics, École Polytechnique Fédérale de Lausanne (EPFL) in Switzerland. 
It comprises concurrent EEG–fMRI recordings from 20 participants, each engaged in a speeded visual discrimination task during data acquisition. 
EEG was collected via a 64-channel cap (10-20 system) at a high sampling rate of 5,000Hz. 
Meanwhile, fMRI scans were obtained at a repetition time (TR) of 1.28 seconds in a $54 \times 108 \times 108$ volume. 
Following \cite{calhas2022eeg}, the discrete cosine transform (DCT II \& III) \cite{ahmed2006discrete} was applied to the original volumes, and subsequently down-sampled to $30 \times 64 \times 64$ for consistency with the other datasets. 
This dataset has a total of 6,880 paired EEG-fMRI samples.

\subsection{Comparison Models}
We compared our E2fNet model with previous methods: CNN-TC \cite{liu2019convolutional}, CNN-TAG \cite{calhas2022eeg}, and NT-ViT \cite{lanzino2024nt}. 
Additionally, we evaluated our model against a GAN-based approach. 
For this, we treated the E2fNet as the generator and employed a binary discriminator to classify an fMRI volume as real or fake. 
We refer to this model as E2fGAN. 
The E2fGAN model was trained using the loss function in Eq. \ref{eq:1} and the GAN loss \cite{goodfellow2014generative}, with the same training settings as in E2fNet. 
Details of the E2fGAN architecture are available in our GitHub repository. 

\subsection{Training and Evaluation}
We trained our E2fNet on the three datasets described above. 
The loss in Eq. \ref{eq:1} with $\lambda_{1} = \lambda_{2} = 0.5$ was employed to train our models. 
The AdamW optimizer \cite{loshchilov2017adamw} was used with the learning rate set to $10^{-3}$. 
The learning rate warm-up was applied for the first 50 training steps. 
All models were trained for 50 epochs on an NVIDIA GeForce RTX 3090 GPU with batch size set to 64. 

In this work, we used the SSIM and peak signal-to-noise ratio (PSNR) metrics to evaluate the quality of the generated results. 
To compare with \cite{lanzino2024nt}, we adopted the leave-one-subject-out cross-validation scheme for the NODDI and Oddball datasets. 
For a dataset with $K$ subjects, we perform $K$ experiments, each time using one subject for evaluation while the remaining subjects are used for training. 
The result is averaged across $K$ experiments. 
For the CN-EPFL dataset, we used the first 16 individuals for training, and the last four for evaluation as in \cite{calhas2022eeg}. 
% Fig. 2
\input{figures/tex_files/Fig_2}