Table \ref{tab:table_1} presents the performance comparison of different models on the three datasets. 
Our proposed E2fNet achieved state-of-the-art results in terms of SSIM across all benchmarks, with SSIM scores of 0.605 on NODDI, 0.631 on Oddball, and 0.674 on CN-EPFL datasets. 
The second-best performing model was NT-ViT, followed by our E2fGAN, with the CNN-TAG and CNN-TC models ranking lower.

In comparison to the previous CNN-TC and CNN-TAG models, our models significantly improved the fMRI reconstruction performance by substantial margins. 
Most notably on the Oddball dataset with a 0.442 SSIM increment over the CNN-TC \cite{liu2019convolutional} model (from 0.189 to 0.631), marking a considerable leap in performance. 
The CNN-TAG and CNN-TC models, in contrast, were less effective in capturing meaningful features from EEG data for generating fMRI targets. 

The strong performance of E2fNet can be attributed to two primary innovations: the design of the EEG encoder $\mathcal{M}_{\mathrm{EEG}}$ and the integration of the U-Net module $\mathcal{M}_{\mathrm{UNet}}$. 
The encoder effectively captures meaningful features across EEG electrodes, while the U-Net module enables the extraction of multi-scale features. 
We observed that the model without the U-Net module failed to accurately reconstruct the fMRI targets, with relatively low SSIM scores (e.g., below 0.45 on the NODDI dataset). 

Compared to the NT-ViT model, E2fNet achieved higher SSIM scores but lower PSNR on the NODDI and Oddball datasets (see Table \ref{tab:table_1}). 
The NT-ViTâ€™s strong PSNR results may be attributed to the use of the Vision Transformer architecture \cite{dosovitskiy21vit}. 
However, we argue that SSIM is a more relevant metric for assessing perceptual quality from a human visual perspective, which is critical in medical imaging applications. 
Furthermore, E2fNet offers a much simpler design compared to NT-ViT. 
Specifically, our E2fNet follows a simple encoder-decoder architecture design whereas the NT-ViT employed two transformer-based encoder-decoder networks. 

To provide a more comprehensive evaluation, we compared E2fNet with the GAN-based E2fGAN model, which uses E2fNet as the generator. 
Fig. \ref{fig:fig_2} presents a visual comparison of E2fNet and E2fGAN across the three datasets. 
During the experiments, we observed that training the GAN model was often unstable, leading to unexpected outputs (Fig. \ref{fig:fig_2}, first row of the NODDI and Oddball datasets, first example in the second row of the CN-EPFL dataset). 
This instability contributed to E2fGAN's lower SSIM and PSNR scores compared to E2fNet. 
Nevertheless, in some cases, E2fGAN produced visually sharper fMRI volumes. 
Future work could explore improvements by adopting more robust GAN training strategies.

It is important to note that SSIM and PSNR metrics may be sometimes less reliable when evaluating small or blurry images. 
For instance, while E2fGAN frequently generated oversmoothed and less detailed results on the CN-EPFL dataset, it still achieved seemingly reasonable SSIM and PSNR scores. 
We believe developing better similarity metrics could help address this limitation. There is still room for improvement, and we plan to investigate these aspects in future work.