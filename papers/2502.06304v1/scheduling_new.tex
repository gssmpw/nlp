\subsection{\zhen{Scheduling Algorithm}}

\textsc{DyPe} employs a dynamic programming-based scheduling algorithm, detailed in Algorithm~\ref{alg:scheduling}, to strategically allocate computational kernels across available FPGAs and GPUs. While the algorithm is applicable to various device types, we focus on FPGAs and GPUs to reflect our prototype setup.

% The algorithm accepts as inputs the sequence of operations in the workload ($wl$) and the numbers of available FPGAs ($n_{\text{FPGA}}$) and GPUs ($n_{\text{GPU}}$). The performance estimation function $f_{\text{perf}}(K, \textit{dev}, n)$ predicts the execution time for a set of kernels $K$ when executed on $n$ devices of type \textit{dev}. The communication cost function $f_{\text{comm}}$ estimates data transfer time between pipeline stages and evaluates costs for both destination and source devices. The function $f_{\text{eng}}$ estimates the energy consumption of a given schedule.

The algorithm accepts the list of operations in the workload $wl$ and the counts of available FPGAs $n_{\text{F}}$ and GPUs $n_{\text{G}}$. It relies on three functions: performance estimation $f_{\text{perf}}$ predicts execution time for a group of kernels when executed on a specific number of devices of a particular device type. Communication cost estimation $f_{\text{comm}}$ estimates data transfer time between pipeline stages, assessing costs for destination and source devices. Energy estimation $f_{\text{eng}}$ computes energy consumption for a given schedule.


The algorithm builds a dynamic programming table $dp[i][f][g]$ representing the best scheduling option for the kernels $wl[0:i]$ with $f$ FPGAs and $g$ GPUs. Two tables, $dp_{\text{perf}}$ and $dp_{\text{eng}}$, track optimal schedules for performance and energy, respectively. These tables are updated independently, with performance optimization strategies highlighted in blue and energy optimization-related operations in orange, as shown in Algorithm~\ref{alg:scheduling}. Each entry in the $dp$ table stores the execution time of each stage and device assignments.


% We construct a dynamic programming table $dp[i][f][g]$, which records the best schedule after the first $i$ kernels have been scheduled using $f$ FPGAs and $g$ GPUs. The tables $dp_{\text{perf}}$ and $dp_{\text{eng}}$ record the best schedules optimized for performance and energy separately, respectively. These tables are updated separately, and operations related to energy optimization are marked in orange in Algorithm~\ref{alg:scheduling}. Each schedule in the $dp$ table contains information about the execution time of each stage and the device allocation. 

The table is filled iteratively through nested loops over $i$, $f$, and $g$ in lines 6–8 of Algorithm~\ref{alg:scheduling}. For each kernel $i$, the algorithm explores scheduling options by considering two strategies: \textbf{1) multiple devices can be allocated to the same kernel}, and \textbf{2) multiple consecutive kernels can be grouped together to form one pipeline stage, with these kernels executed sequentially by the same devices.} To implement these strategies, the algorithm references previous states in the dynamic programming table. Specifically, it looks back to $dp[i $-$ j][f$-$n_f][g]$ to consider grouping kernels from $i $-$ j$ to $i$ into a new pipeline stage using $n_f$ FPGAs, or to $dp[i $-$ j][f][g $-$ n_g]$ when using $n_g$ GPUs. 
% The complexity of the algorithm can be deduced by following the nested loops: $\mathcal{O}(|wl|^2 \times n_{FPGA}^2 \times n_{GPU}^2)$.

Without loss of generality, we detail the case where the stage accommodating the $i^\text{th}$ kernel is allocated to FPGA(s) ($n_f > 0$, $n_g $=$ 0$). In this scenario, we first retrieve information about the last stage of the previous schedule (lines 10–15) to accurately account for data transfer time to the destination devices, i.e., FPGAs (line 17). In the case of energy optimization, the $prev\_stage$ is obtained from the $dp_{\text{eng}}$ table. We then calculate the execution time of the new stage containing kernel $i$ by summing the kernel execution time and the incoming data transfer time (line 19). Additionally, we include the data transfer cost to the last stage of the previous schedule when adding the new stage (line 21). The longest stage duration in the updated pipeline is determined by taking the maximum among: 1) the previous stage's duration with updated communication cost, 2) the longest stage duration recorded thus far, and 3) the duration of the new stage. The obtained new pipeline time is compared and used to update the best schedule in the $dp$ table (lines 25–27).

% \zhen{For energy optimization, a new pipeline stage may become the longest stage of the pipeline, potentially introducing more idleness to the previously scheduled stages. Therefore, the energy evaluation must account for the entire pipeline. We create the new schedule from the previous one in line 31 and calculate its energy consumption based on the idleness in each stage, the data transfers, and the kernel execution. The power consumption of the accelerators in different states—data transfer as sender, data transfer as receiver, kernel execution, and idleness—is described separately and specified by the system specification files.}
For energy optimization, the pipeline’s total energy is assessed by accounting for stage idleness, data transfers, and kernel execution. Accelerator power consumption in states such as data transfer, execution, and idleness is specified in system configuration files.

Once both $dp_{\text{eng}}$ and $dp_{\text{perf}}$ tables are fully populated, we can analyze energy-performance trade-offs by reviewing $dp[|wl|][f][g]$ across various combinations of $f$ and $g$, representing the states after all kernels have been scheduled. Our system, \textsc{DyPe}, automatically explores these configurations based on specified throughput and energy efficiency requirements. For instance, our predefined \textit{balanced} mode allows up to a 30\% reduction in throughput compared to the maximum achievable throughput, aiming to minimize energy consumption.


\setlength{\textfloatsep}{1pt}% Remove \textfloatsep
\begin{algorithm}
\caption{\textsc{DyPe}'s Scheduling Algorithm}\label{alg:scheduling}
\begin{algorithmic}[1]

\footnotesize
\Require $wl,  n_{F}, n_{G}, f_{perf}$ , $f_{comm}$, $f_{eng}$
\State $dp_{perf}[|wl|][n_{F}][n_{G}] \leftarrow {\top}$ 
\State $dp_{perf}[0][0][0] \leftarrow$ empty pipeline with execution time 0
\State $dp_{eng}[|wl|][n_{F}][n_{G}] \leftarrow \top$ 
\State $dp_{eng}[0][0][0] \leftarrow$ empty pipeline with energy 0\\
\textbf{for} {$i \in [1, |wl|]$} \textbf{do} \\
\textbf{for} {$f \in [0, n_{F}]$} \textbf{do} \\
\textbf{for} {$g \in [0, n_{G}]$} \textbf{do} \\
           \textbf{for} {$j \in [1,i]$} \textbf{do} \\
                 \hspace{1.3em} \textbf{for} {$n_f\in [1, f]$} \textbf{do} \\
                    \hspace{0.7em}  \commentgreen{// information about last stage of the previous schedule} \\ 
                     \textcolor{blue}{ \hspace{2.6em} $prev\_stage \leftarrow$ last stage of $dp_{perf}[i$-$j][f$-$n_f][g]$} \\
                    \textcolor{orange}{ \hspace{2.6em} $prev\_stage \leftarrow$ last stage of $dp_{eng}[i$-$j][f$-$n_f][g]$} \\
                    \hspace{2.6em} $src\_dev \leftarrow$ device used in $prev\_stage$ \\
                    \hspace{2.6em} $n_{src\_dev} \leftarrow$ number of devices used in $prev\_stage$ \\
                    \hspace{2.6em} $t_{prev\_stage} \leftarrow$ the execution time of $prev\_stage$ \\
                    \commentgreen{\hspace{2.6em}// data transfer time to the FPGAs} \\
                    \hspace{2.6em} $t^{dst}_{comm_{i\text{-}j}}\leftarrow$ data transfer time to the \textbf{destination} devices, between the kernel $wl[i$-$j]$ and $wl[i$-$j$+$1]$, using $n_{src\_dev}$ units of $src\_dev$ devices and $n_f$ untis of FPGAs, evaluated with $f^t_{comm}$. \\
                    \hspace{2.6em}\commentgreen{// the execution time of the new stage} \\
                    \hspace{2.6em} $t_{new\_stage} \leftarrow f_{perf} (wl[$$i$-$j$:$i$$], $FPGA$, n_f) + t^{dst}_{comm_{i-j}} $ \\
                    \hspace{2.6em}\commentgreen{// data transfer time of the previous stage} \\
                     \hspace{2.6em} $t^{src}_{comm_{i-j}}\leftarrow$ data transfer time to the \textbf{source} devices, between the kernel $wl[i$-$j]$ and $wl[i$-$j$+$1]$, using $n_{src\_dev}$ units of $src\_dev$ devices and $n_f$ untis of FPGAs, evaluated with $f^t_{comm}$. \\
                    \hspace{2.6em}\commentgreen{// update the longest stage} \\
                    \hspace{2.6em} $t_{new\_pipeline} \leftarrow $new longest stage \\
                    \hspace{2.6em}\commentgreen{// update $dp_{perf}$ table} \\
                    \hspace{2.6em} \textcolor{blue}{\textbf{if} {$t_{old\_pipeline} > t_{new\_pipeline}$} \textbf{then} \\
                        \hspace{3.9em} $dp_{perf}[i][f][g] \leftarrow$ new pipeline  \\
                    \hspace{2.6em} \textbf{end if} }\\
                \hspace{2.6em} \commentgreen{// update $dp_{eng}$ table} \\
                \hspace{2.6em} \textcolor{orange}{$new\_pipeline \leftarrow$ add the new stage to $dp_{eng}[i$-$j][f$-$n_f][g]$ \\
                \hspace{2.6em} $e_{new\_pipeline}$ = $f_{eng}(new\_pipeline, t_{new\_pipeline})$ \\
                \hspace{2.6em} \textbf{if} {$dp_{eng}[i][f][g] > e_{new\_pipeline}$} \textbf{then} \\
                    \hspace{3.9em} $dp_{eng}[i][f][g] \leftarrow  new\_pipeline $\\
                \hspace{2.6em} \textbf{end if}}\\
                
                \hspace{1.3em} \textbf{end for}\\
                \hspace{1.3em} \textbf{for} {$n_g\in [0, g]$} \textbf{do}\\
                    \hspace{2.6em} same procedure for GPUs by looking into $dp_{perf}[i$-$j][f][g$-$n_g]$ and $dp_{eng}[i$-$j][f][g$-$n_g]$\\
                \hspace{1.3em} \textbf{end for} \\
\textbf{end for} \\
\textbf{end for} \\
\textbf{end for} \\
\textbf{end for}
\end{algorithmic}
\end{algorithm}
\setlength{\textfloatsep}{1pt}% Remove \textfloatsep

\subsection{\zhen{Data Transfer Costs Estimation}}
% \textbf{Data Transfer Costs} 
To evaluate data transfer costs effectively, our algorithm uses the $f_{comm}$ function. It is crucial to manage communications between GPU-FPGA and CPU-FPGA pairs to prevent conflicts. Implementing communication processes naively at the onset or conclusion of each computational stage often leads to interference. We maintain a scheduling technique that introduces a delay equivalent to one CPU-FPGA communication cycle at the end of the initial phase, ensuring temporal separation from the subsequent FPGA-GPU data transfer. Handling conflicts is crucial in environments where compute and communication kernels compete for limited resources like HBM and PCIe\textsuperscript{\textregistered} bandwidth, potentially slowing down computations. By avoiding such overlaps, our model accurately predicts latencies for both compute and communication kernels. However, overlaps between CPU-FPGA and GPU-CPU communications are permissible due to their connection to distinct CPUs (see Figure~\ref{fig:schedules}). Figure~\ref{fig:schedules}b illustrates how this scheduling approach aids in deducing a viable schedule.

Moreover, intra-stage data transfers are occasionally necessary when managing input data across multiple devices. For instance, in Graph Neural Network (GNN) workloads, this involves distributing graphs and network data across devices. Our approach employs a data partition strategy, pre-loading static components like graphs and weights onto devices while distributing dynamic elements such as feature matrices and intermediate results during runtime. This reduces the frequency of data transfers and amortizes costs across multiple processing iterations. We incorporate the costs of gather-scatter operations into $f_{perf}$, which assesses stage execution times by considering both the data volume transferred and the available bandwidth for cross-device communication.


\begin{figure}[h] 
\centering
\includegraphics[width=\linewidth]{figures/schedules.pdf}
\caption{Example 2-stages pipeline with and without conflict.}
\label{fig:schedules}
\vspace{-10pt}
\end{figure}


