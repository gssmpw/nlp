\begin{abstract}
Progress in AI has relied on human-generated data, from annotator marketplaces to the wider Internet. However, the widespread use of large language models now threatens the quality and integrity of human-generated data on these very platforms. We argue that this issue goes beyond the immediate challenge of filtering AI-generated content\textemdash{}it reveals deeper flaws in how data collection systems are designed. Existing systems often prioritize speed, scale, and efficiency at the cost of intrinsic human motivation, leading to declining engagement and data quality.
% {\bf \uline{
We propose that rethinking data collection systems to align with contributors' intrinsic motivations\textemdash{}rather than relying solely on external incentives\textemdash{}can help sustain high-quality data sourcing at scale while maintaining contributor trust and long-term participation.
% }}
\end{abstract}



% \begin{abstract}
% Progress in AI has relied on human-generated data, sourced from a wide range of platforms, from annotator marketplaces to Internet communities. However, the widespread use of large language models now threatens the quality and integrity of human-generated data on these very platforms. We argue that this issue goes beyond the immediate challenge of filtering AI-generated content\textemdash{}it requires a deeper analysis of how data collection systems are designed and operate. While existing systems prioritize speed, scale, and efficiency, they tend to ignore the role of intrinsic human motivations in data generation, leading to declining engagement and data quality.
% % {\bf \uline{
% We propose that rethinking data collection systems to align with, and boost, contributors' intrinsic motivations\textemdash{}rather than relying solely on external incentives\textemdash{}can help sustain high-quality data sourcing at scale while maintaining contributor trust and long-term participation.
% % }}
% \end{abstract}


% Progress in AI has relied on human-generated data, from annotator marketplaces to the wider Internet. However, the widespread use of large language models now threatens the quality and integrity of human-generated data on these very platforms. We argue that this issue goes beyond the immediate challenge of filtering AI-generated content -- it reveals deeper flaws in how data collection systems are designed. Existing systems often prioritize speed, scale, and efficiency at the cost of intrinsic human motivation, leading to declining engagement and data quality. We propose that rethinking data collection systems to align with contributors' intrinsic motivations--rather than relying solely on external incentives--can help sustain high-quality data sourcing at scale while maintaining contributor trust and long-term participation.