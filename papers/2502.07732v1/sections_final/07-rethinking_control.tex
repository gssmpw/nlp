\section{Elevating the Quality-Quantity Trade-off: \\Rethinking Control}
% \section{Uplifting the Trade-off: Rethinking Control}
Data collection systems have long operated under the assumption that control at the task level\textemdash{}through explicit instructions, incentive structures, and quality enforcement mechanisms\textemdash{}is necessary to ensure both high-quality and high-quantity data. However, as we have discussed, these very mechanisms often introduce unintended side effects. External incentives, while effective in driving participation, tend to crowd out intrinsic motivations over time, leading to disengagement and lower-quality contributions. Similarly, excessive task fragmentation, though useful for efficiency and scalability, can erode a sense of purpose and hence leads contributors to disengage from the task itself\textemdash{}resulting in an over-reliance on shortcuts, manifesting in careless task completion and non-judicious use of LLMs.

In contrast, data that we obtain from systems not intentionally designed for data collection\textemdash{}such as Wikipedia, Reddit, and open-source projects\textemdash{}demonstrate an alternative paradigm. These platforms do not enforce control at the task level but instead create environments where intrinsically motivated contributors engage meaningfully. Crucially, this lack of task-level control removes two major pitfalls seen in structured data collection systems: it prevents (a) the crowding-out effect, where external incentives replace intrinsic motivation over time, and (b) excessive fragmentation, ensuring that contributors remain connected to their purpose of participation. Taking inspiration, ceding control at task level could be the key to pushing the pareto-frontier forward.

\textbf{Ceding Control at the Task Level.} Rather than controlling individual (micro) tasks, data collection systems can benefit from structuring conditions that naturally guide contributor engagement. Moving from direct task management to a broader, environment-driven approach discourages individuals from attributing their participation to external rewards, helping preserve intrinsic motivation~\cite{bem1972self}. 

However, this shift presents a new challenge: relinquishing fine-grained control over tasks means that collectors must instead focus on shaping engagement at a more systemic level. Coarse-grained control\textemdash{}where engagement is influenced through platform design, incentives, and structural conditions\textemdash{}takes longer to align with desired outcomes and demands greater up-front effort. But once in place, it can lead to more sustainable data collection, enabling both higher-quality and higher-quantity contributions, as seen in rare but influential examples.

\begin{figure}[h]
    \centering
    \includegraphics[width=0.8\linewidth]{illustrations/environment-control.png}
    \label{fig:environment-control}
\end{figure}

\textbf{Deployed Robots} are a prime example of achieving this delicate balance. For example, robotic vacuum cleaners (e.g., Roomba) provide utility to users, by cleaning their homes, while simultaneously collecting spatial and navigation data that improves future performance~\cite{astor2017your}. Users engage with the system for its primary function, yet their interactions naturally generate high-quality data that feeds back into the AIâ€™s development~\cite{brynjolfsson2014second}. This data collection approach also scales effectively, as data is gathered continuously and passively as a result of users' routine behaviors, without requiring any additional effort towards data contribution. This model extends to more complex, high-stakes deployments, such as electric vehicles equipped with driver-assist and self-driving features. As an example, consider how Tesla uses real-world driving data from its fleet to refine its self-driving AI algorithms, which ultimately benefit car owners by improving the technology~\cite{karpathy2021ai,tesla_autopilot} and can drive future innovation for the company (e.g., driverless Robotaxi). Similarly, Waymo operates self-driving taxis in real-world environments and has gathered large-scale data that has proven valuable for advancing computer vision research~\cite{sun2020scalability}.

However, replicating such large-scale, product-driven data collection systems is exceptionally difficult. They demand massive hardware infrastructure, well-articulated and trusted benefits for both users and companies, and real-world applications with extensive safeguards and privacy protections. For entities whose primary goal is simply to collect human-generated data, establishing such ecosystems solely for this purpose is neither feasible nor sustainable.

More importantly, these symbiotic relationships rely on a delicate balance of implicit or explicit social contracts, mutual trust, and fair distribution of costs and benefits, as explained by theories of social interaction and exchange\textemdash{}such as Social Exchange Theory~\cite{homans1958social} and Social Contract Theory~\cite{rousseau1762social}. When the benefits clearly extend beyond the primary parties\textemdash{}for example, when collected data is repurposed to serve third parties\textemdash{}this balance can be disrupted. If users feel that their contributions are being exploited without fair reciprocity, trust erodes, and they may come to see the system as exploitative or unjust.

These feelings of mistrust and exploitation are already prevalent in AI, particularly in creative and knowledge-sharing communities. Artists have protested against their work being scraped to train AI models without consent or compensation~\cite{jiang2023ai}\footnote{\url{https://www.aitrainingstatement.org/}}, with many calling for stronger protections against AI-generated art~\cite{guardian2025aiart}. Similarly, Stack Overflow users, frustrated by their contributions being used for external profits, have intentionally altered or deleted their posts to hinder AI training\textemdash{}leading to bans, boycott, and subsequent decline in engagement on the platform~\cite{ars2024stackoverflow, tomshardware2024stackoverflow}. These examples highlight the fragility of trust in data collection and the potential consequences when contributors feel that their data is being repurposed beyond its original intent.