% \section{Quantity Factors: Efficiency through Fragmentation}
\section{Human Factors in Quantity\textemdash{}\\Efficiency through Fragmentation}
Data collection systems differ not only in how they compensate contributors but also in how they structure tasks for scalability. At one end, rapid crowdwork platforms fragment tasks into micro-tasks (e.g., HITs on MTurk) that take seconds to complete, optimizing for speed and mass throughput~\cite{malsburg2024mturk}. Platforms like Prolific handle slightly larger but still modular tasks, spanning minutes to hours~\cite{prolific2024completion}. On the other end, freelance job platforms (e.g., UpWork) structure work as full projects, lasting days or weeks and offering greater autonomy and depth of engagement~\cite{upwork2024times,fiverr2024comparison,workathomesmart2024lionbridge}.

Fragmentation into repeatable units that can be completed in a consistent and orderly manner allows tasks to be parallelized across multiple workers, replacing the traditionally serial process of creation. Many innovations and processes begin as creative, effortful tasks\textemdash{}akin to System 2 processes, requiring deliberate, conscious effort~\cite{kahneman2013prospect}. However, to scale, they are often refined into a System 1 process, where execution becomes fast, automated, and intuitive. This transformation\textemdash{}breaking down complex, uncertain tasks into simpler, repeatable steps—underpins mass production systems. 

Crucially, this shift from System 2 to System 1 is not just a natural evolution but is actively accelerated by task fragmentation. When work is divided into modular, repetitive tasks, the process of routinization happens more quickly.  A useful analogy here is \textbf{Fordism}~\cite{hounshell1984american}, which introduced the assembly line, a mechanized, repetitive setup where products are built step by step. This structured fragmentation enables processes to be repeated at scale, maximizing efficiency and throughput.

\begin{figure}[h]
    \centering
    \includegraphics[width=0.8\linewidth]{illustrations/fragmentation.png}
    \label{fig:fragmentation}
\end{figure}

However, as tasks become increasingly repetitive and fragmented, they sacrifice the creativity and problem-solving scope that contribute to meaningful engagement. Over time, workers become disconnected from the broader purpose of their efforts~\citetext{Theory of Alienation; \textcolor{myorange}{\citealt{marx2016economic}}}, %as Marx's Alienation Theory~\cite{marx2016economic} suggests, 
shifting their motivation from seeking fulfillment to merely achieving survival goals -- a regression in the hierarchy of needs~\cite{maslow1943theory}. For employers/collectors, this shift manifests as a decline in quality, as contributors disengage from the task itself. Unlike physical labor, which has built-in quality checks (e.g., material standards, product inspections), knowledge-based tasks lack robust safeguards. In data annotation, for example, there is often no immediate way to verify whether a task was completed thoughtfully or rushed~\cite{klie2024analyzing,klie2024efficient}. As a result, quality can quietly degrade, with errors compounding over time\textemdash{}often going unnoticed until the system has deteriorated beyond repair.

\textbf{So, how does task fragmentation impact real-world data sourcing?} Micro-tasking on platforms like MTurk was once hailed as a transformative shift in computer science, enabling large-scale user studies~\cite{bohannon2011social,kittur2013future,bernstein2011crowds} and efficient data collection for machine learning~\cite{deng2009imagenet}. However, over time, research has raised concerns about the reliance on ``piece rate'' or pay-per-task systems, favoring ``quota'' systems instead~\cite{ikeda2016pay,mason2009financial}, which alludes to a reduction in task quality when micro-tasking is pushed too far.

Micro-tasking doesn’t just impact data quality\textemdash{}it also affects the workers behind it. Beyond quality concerns, it has drawn criticism for its effect on worker well-being. Works such as Ghost Work~\cite{gray2019ghost} and Anatomy of AI~\cite{crawford2018anatomy} have illustrated the often invisible and exploitative nature of these atomized tasks. %performed by an  underpaid global workforce. 
The non-physical nature of knowledge labor further exacerbates this issue, making its value difficult to quantify~\cite{martin2016turking}. This issue is taken to the extreme when microtasks are outsourced to developing countries with favorable exchange rates~\cite{dicken2007global} to cut costs\textemdash{}and thus, incentives\textemdash{}even further~\cite{perrigo2023exclusive,microsoft_google_questioned}, often trapping workers in exploitative, sweatshop-like conditions~\cite{williams2022exploited,hao2022ai}.


% This issue is compounded by global labor arbitrage~\cite{dicken2007global}, where favorable exchange rates in developing countries allow platforms to offer low wages, trapping workers in exploitative, sweatshop-like conditions~\cite{perrigo2023exclusive,microsoft_google_questioned}.

\textbf{Then, what happens when tasks become so repetitive and unfulfilling that workers disengage from them entirely?} As discussed earlier, over time, many human-driven processes shift from System 2 (deliberate \& effortful) to System 1 (intuitive \& fast). As tasks become more structured and predictable, they become prime targets for automation. In physical labor, this transition has been gradual\textemdash{}machines take over repetitive, routine tasks, while humans focus on creative and uncertain work~\cite{brynjolfsson2014second}.

A similar shift is occurring in knowledge-based work, where high-quality LLMs give workers the opportunity to offload mundane tasks\textemdash{}such as grammar corrections, spell-checking, and phrasing refinements\textemdash{}to AI. When used judiciously, this assistance promotes meaningful engagement and enhances productivity without compromising data quality. However, the problem arises when workers become over-reliant on LLMs, using them indiscriminately to complete entire tasks without oversight~\cite{veselovsky2023artificial,veselovsky2023prevalence}. Since knowledge-based tasks often lack clear-cut quality standards, it becomes harder to detect when quality has deteriorated, making it easier for such opportunistic behavior to go unchecked.

As a result, the transition to automation in data sourcing has been rather chaotic. While repetitive physical labor was gradually offloaded to machines in structured ways, knowledge work faces conflicting views\textemdash{}some advocate for fully replacing human contributors~\citetext{e.g., \textcolor{myorange}{\citealt{dubois2024alpacafarm}}}, while others for eliminating LLM usage entirely~\citetext{e.g., \textcolor{myorange}{\citealt{thorp2023chatgpt}}}. However, fully relying on synthetic data risks model feedback loops and collapse~\cite{taori2023data,shumailov2024ai}, while a complete ban slows human productivity and sacrifices efficiency~\cite{liao2024llms,kreitmeir2023unintended}. The most effective approach lies somewhere in between\textemdash{}where AI serves as a tool that productively and progressively supports human effort rather than a crutch for task completion~\citetext{e.g., \textcolor{myorange}{\citealt{ashok2024little,qian2024evolution}}}.

In this landscape, intrinsic motivation becomes even more crucial. Workers must make thoughtful decisions about how to incorporate LLMs in ways that enhance rather than replace meaningful engagement. Designing a sustainable data collection system, therefore, is not just about limiting LLM use for workers or maximizing automation with synthetic data\textemdash{}it's about creating an environment where contributors remain actively engaged with the task, rather than optimizing for speed at the cost of quality.