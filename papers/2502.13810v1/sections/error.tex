\section{Error}

Error minimisation attempts to achieve a particular output in one space $d \in D$ of a given mapping $Inf : M \rightarrow D$ by selecting an appropriate input $m \in M$. To compare different choices of $m$ there is a bivariate function into the non negative real numbers $Err : D \times D \rightarrow \mathbb{R}_+$ which allows some measurement of difference between the actual output $Inf(m)$ and the desired output $d$. Such a problem may be codified with sets and functions.
\begin{definition}[Set Theoretic Error Minimisation Problem]
\label{definition:set_error}
\begin{align*}
Given \quad & M,\ D \in Set\\
& Inf \in Set(M,\ D)\\
& Err \in  Set(D \times D,\ \mathbb{R}_+)\quad\\
& d = d' \implies Err(d,d') = 0\\
&d \in D\\
\textrm{minimise} \quad & Err(d,\ Inf(m))
\end{align*}
\end{definition}

To minimise $Err(d,\ Inf(m))$ means to select a global error minimiser with respect to $d$.

\begin{definition}[Global Error Minimiser]
Given an error minimisation problem (Def \ref{definition:set_error}), $x \in M$ is a global error minimiser with respect to $d \in D$ if for any $m \in M$ then $Err(d, Inf(x)) \leq Err(d, Inf(m))$.
\end{definition}

The choice to call the mapping between input and output $Inf$ is in direct reference to the notion of model inference, where a machine learning model is used to predict an output given an input. Model inference is often referred to in the context of individual inputs vs outputs. The $Inf$ function maps a particular parametrisation of a machine learning model onto the dataset it produces when allowed to produce inference over the entire set of training inputs. From this perspective, $M$ represents the set of all choices of parameters for the machine learning model and $D$ is the set of all datasets that one may try to train against.

In order to apply the category theoretic constructions of adjunctions and Kan extensions to this definition, it needs to be lifted into a description using categories and functors. This is easily done with respect to $Inf$ by the statement that $M$ and $D$ should be categories and $Inf : M \rightarrow D$ a functor. The next question is how to categorify the notion of error. Morphisms are commonly thought of as structure preserving transformations. In the context of $D$ this would suggest that the morphisms are transformations between datasets. Data transformations, functions, or programs can only lose information. They cannot add information that wasn't previously there. The better one dataset represents the information of another, the less information loss a mapping between them may experience. This would indicate that error may be associated to a category by assigning each morphism some quantity of error that represents its information loss. The values which represent error require some order structure so they can be compared and should reflect how error composes as morphisms compose. This suggests that the values of error may be represented by a monoidal preorder (Def \ref{def:monoidal_preorder}).

\begin{definition}[$S$ Flavoured Error]
\label{def:s_flavoured_error}
Given a monoidal preorder $S$, where $Id_*$ is the bottom element, then $S$ flavoured error on $D$ is a lax 2-functor $Err : D \rightarrow S$.
\end{definition}

To make use of the structure of $D$, the choice of error should respect the information that $D$ contains. Namely, the composition of morphisms. A mapping of morphisms to morphisms which respects their composition would usually be indicative of a functor. However, though it would work, an error functor would be an excessively restrictive constraint. In practice, the information loss of the composite of two processes cannot usually be represented by the composition of the information loss of each of the processes individually. Two processes may lose the same portion of information, so their composite loss is not much worse than their individual losses. In contrast, the lossy-ness of a different pair of processes may affect entirely different portions of the information content, so their composite information loss would be much larger than their individual losses. It is much easier to represent the information loss of the composite of two morphisms via an inequality, which can be done using a lax 2-functor, encoding the relationship that the error of a composition of morphisms must be greater than or equal to some composition of their errors.
\begin{equation}
Err(g)Err(f) \leq Err(gf)
\end{equation}
One example of a suitable monoidal preorder would be the single object category $\mathbb{R}_\wedge$ whose morphisms are the non-negative real numbers composed by taking the maximum and ordered by the standard ordering on the reals. Imagine the case of the objects of $D$ being data streams with morphisms being functions which map one data stream to another. In this case, the system is well described by an $\mathbb{R}_\wedge$ flavoured error on $D$. The functions between data streams have some associated information loss. If one is considering the error to be measured purely by the lost information and not just some invertible scrambling, then it wouldn't be possible to undo the error. So the error of those two functions composed together must always be greater than the maximum of the two errors.
\begin{equation}
Max(Err(g), Err(f)) \leq Err(gf)
\end{equation}

From a choice of $S$ flavoured error, it is possible to recover an ordering of error associated with pairs of objects of $D$ by looking at the best case scenario, the least errorful morphism.

\begin{definition}[Error Comparison]
\label{def:error_comparison}
Given $S$ flavoured error on $D$. For objects $x,y,z,w \in D$ then $Err(x,y) \leq_S Err(z,w)$ if and only if, for any $f : z \rightarrow w$ there exists a $g : x \rightarrow y$ and $\sigma : Err(g) \Rightarrow Err(f)$. 
\end{definition}

\begin{remark}
The value $Err(x,y)$ is a notational convenience and is not an object in $S$. Instead, the important aspect of error in the traditional case is that it induces a preorder on pairs of objects. Def \ref{def:error_comparison} is a way of inducing a preorder on pairs of objects of $D$ using $S$ flavoured error.
\end{remark}

\begin{proposition}
The error comparison of an $S$ flavoured error on $D$ defines a preorder.
\end{proposition}
%
\begin{proof}
For any morphism $f : x \rightarrow y$ there is an identity 2-morphism $Id_{Err(f)}: Err(f) \Rightarrow Err(f)$ which implies that $Err(x,y) \leq_S Err(x,y)$, demonstrating that the relation is reflexive.

If $Err(x,y) \leq Err(w,z)$ and $Err(w,z) \leq Err(a,b)$ then for any morphism $f:a \rightarrow b$ there is a morphism $g: w \rightarrow z$ and 2-morphism $\sigma : Err(g) \Rightarrow Err(f)$. Given the existence of $g$, there must be a morphism $h:x \rightarrow y$ with associated 2-morphism $\varphi : Err(h) \Rightarrow Err(g)$, which by composition induces $\sigma\varphi : Err(h) \Rightarrow Err(f)$ for any $f$ implying that $Err(x,y) \leq_S Err(a,b)$, demonstrating that the relation is transitive. As it is both reflexive and transitive the error comparison relation is a preorder.
\end{proof}
%
By combining the requirement that $Inf : M \rightarrow D$ is a functor, with a choice of $S$ flavoured error on $D$, one may produce a category-theoretic definition of an error minimisation problem.
%
\begin{definition}[Category Theoretic Error Minimisation Problem]
\label{definition:category_error}
\begin{align*}
Given \quad & M,\ D \in Cat\\
& Inf \in Cat(M,\ D)\\
& \textit{$S$ flavoured error on $D$}\quad\\
&d \in D\\
\textrm{return} \quad & \textit{A global error minimiser with respect to $d$}
\end{align*}
\end{definition}

\begin{remark}
By fixing $d$, Def \ref{definition:category_error} may also serve as a definition for category theoretic loss minimisation.
\end{remark}

The value of translating the set-theoretic definition into the category theoretic definition is that the existence of morphisms between models and datasets provides more information about the structure of the problem. The first observation to make about the additional information is that it constrains the choices of error to those which respect the structure of the category. From a practical perspective, this provides a novel approach to the selection of an error function given a particular error minimisation problem, if one knows the morphisms between datasets. The second observation is that because the choice of error is now dependent on the morphisms of $D$, the properties of category theoretic constructions which reference only morphisms may be translated into their consequences with respect to error. The first such consequence is that adjunctions are error minimisers.

%why should error be an s flavoured preorder
\begin{theorem}[Adjunctions are Error Minimisers]
\label{thrm:adjoints_are_minimisers}
Given a category-theoretic error minimisation problem (Def \ref{definition:category_error}) where $ Inf : M \rightarrow D$ has a left adjoint $ Alg : D \rightarrow M$, then for all $d \in D$, $Alg(d)$ is a global error minimiser with respect to $d$.
\end{theorem}

\begin{proof}
For any $d\in D$ show that $Alg(d)$ is a global error minimiser with respect to $d$ by demonstrating that for any $m\in M$, $Err(d, InfAlg(d)) \leq_S Err(d, Inf(m))$.

If there does not exist a morphism $f: d \rightarrow Inf(m)$ then the error comparison requirement (Def \ref{def:error_comparison}) is trivially satisfied.

If there does exists a morphism $f: d \rightarrow Inf(m)$ then by the definition of an adjunction (Def \ref{def:AdjointFunctorTriangle}) for any morphism $f : d \rightarrow Inf(m)$ there is a unique morphism $\tilde{f} : Alg(d) \rightarrow m$ such that $Inf(\tilde f)\eta_d = f$, where $\eta : Id_D \Rightarrow InfAlg$ is the adjunction unit.

By the definition of a lax 2-functor (Def \ref{definition:Lax2Functor}) there exists a 2-morphism. \[\sigma: Err(Inf(\tilde f))Err(\eta_d) \Rightarrow Err(Inf(\tilde f)\eta_d) = Err(f)\]

Because the identity of $S$ is the bottom element there is a 2-morphism $\varphi : Id_* \Rightarrow Err(Inf(\tilde f))$ which by right whiskering produces the 2-morphism \[\varphi \cdot Err(\eta_d) : Err(\eta_d)\Rightarrow Err(Inf(\tilde f))Err(\eta_d)\]
Compose this with $\sigma$.
\[\sigma(\varphi \cdot Err(\eta_d)) : Err(\eta_d) \Rightarrow Err(f) \] 
As this is true for any choice of $f$ this proves the error comparison. \[Err(d, InfAlg(d)) \leq_S Err(d, Inf(m))\]
As the error comparison is true for any choice of $m$ them $Alg(d)$ is a global error minimiser with respect to $d$.
\end{proof}

\begin{corollary}
\label{corollary:error_independent}
The left adjoint of $Inf$ is an error minimiser for any choice of error on $D$. If one can compute the left adjoint of $Inf$ than they may identify the global error minimiser, with respect to any $d$, without ever making a particular choice of error.
\end{corollary}

\begin{remark}
If a true error minimising algorithm returns a global error minimiser for any element $d\in D$, then one could define algorithms as left adjoint to inference.
\end{remark}

If an inverse to inference exists then it would make sense it would represent an error minimisation algorithm. Ideally, any algorithm would return a model which produced exactly the intended dataset. In the case that one is not capable of reproducing exactly the desired dataset, then a pseudo inverse to inference would be an intuitive choice. In the context of category theory, the natural choice of pseudo inverse is an adjunction, but this does not have to be an adjunction of functors. The definition of an adjunction can be generalised to any 2-category. If one believes that algorithms are left adjoint to inference, then whatever object one uses to represent the collection of models and datasets, if these objects exist in a suitable 2-category, then one can define what a global error minimising algorithm should be.

% https://q.uiver.app/#q=WzAsMixbMCwwLCJNIl0sWzIsMCwiRCJdLFswLDEsIkluZiIsMCx7ImN1cnZlIjotMX1dLFsxLDAsIkFsZyIsMCx7ImN1cnZlIjotMX1dLFsyLDMsIlxcdG9wIiwxLHsic2hvcnRlbiI6eyJzb3VyY2UiOjIwLCJ0YXJnZXQiOjIwfSwic3R5bGUiOnsiYm9keSI6eyJuYW1lIjoibm9uZSJ9fX1dXQ==
\[\begin{tikzcd}
	M && D
	\arrow[""{name=0, anchor=center, inner sep=0}, "Inf", curve={height=-6pt}, from=1-1, to=1-3]
	\arrow[""{name=1, anchor=center, inner sep=0}, "Alg", curve={height=-6pt}, from=1-3, to=1-1]
	\arrow["\top"{description}, shorten >=2pt, Rightarrow, no body, from=0, to=1]
\end{tikzcd}\]

\begin{corollary}
\label{corollary:sufficient_conditions}
If an error minimising problem is presented in the categorical form, then Theorem \ref{thrm:adjoints_are_minimisers} shows that one may prove that a global minimiser exists by proving that a left adjoint to $Inf$ exists. This statement may be repackaged with the various adjoint functor theorems \cite{porst_history_2024} to produce sufficient conditions for the existence of optimal solutions to error minimisation problems.
\end{corollary}

While the production of sufficient conditions for the existence of global error minimisers is an incredibly powerful result, it is excessively strict to always require the existence of an adjunction to discuss global error minimisers in a category-theoretic context. Furthermore, there are many practical problems where a global error minimiser may exist for some datasets but not for others. It would be more helpful to provide a construction which exists as long as a particular dataset has a global error minimiser. Such a construction would be the left Kan extension. The application of a left Kan extension to a category theoretic error minimisation problem requires the problem to be represented as an extension triangle. This can be done with a suitable choice of 2-category.

\begin{proposition}[Extension Error Minimisation]
\label{prop:extension_error_minimisation}
An extension triangle (see below) in a 2-category $\mathbb{T}$ with an $S$ flavoured error on $\mathbb{T}(\delta, \tau)$ defines a category theoretic error minimisation problem (Def \ref{definition:category_error})
% https://q.uiver.app/#q=WzAsMyxbMCwxLCJcXGRlbHRhIl0sWzIsMSwiXFx0YXUiXSxbMSwwLCJcXG11Il0sWzAsMiwiXFxpb3RhIl0sWzAsMSwiZCIsMl0sWzIsMSwiIiwwLHsic3R5bGUiOnsiYm9keSI6eyJuYW1lIjoiZGFzaGVkIn19fV1d
\[\begin{tikzcd}
	& \mu \\
	\delta && \tau
	\arrow[dashed, from=1-2, to=2-3]
	\arrow["\iota", from=2-1, to=1-2]
	\arrow["d"', from=2-1, to=2-3]
\end{tikzcd}\]
\end{proposition}

\begin{proof}
The 1-morphism $\iota : \delta \rightarrow \mu$ and the object $\tau$ induce the functor $\mathbb{T}(\iota, \tau) : \mathbb{T}(\mu, \tau) \rightarrow \mathbb{T}(\delta, \tau)$. The functor $\mathbb{T}(\iota, \tau)$ is defined via precomposition, sending any object $m \in \mathbb{T}(\mu, \tau)$ to $m\iota \in \mathbb{T}(\delta, \tau)$. By renaming the functor and categories as $Inf : M \rightarrow D$ it is clear that they define a category theoretic error minimisation problem. 
\end{proof}

\begin{remark}
It is also the case that any category-theoretic error minimisation problem may be presented as an extension in a 2-category $\mathbb{T}$ by directly defining the hom categories and composition functor of $\mathbb{T}$ to be the categories and inference functor of the error minimisation problem. Proving this is also true for the set-theoretic error minimisation problem is slightly trickier (Thm \ref{theorem:universal_ml_representation}).
\end{remark}

\begin{theorem}[Kan Extensions are Error Minimisers]
Given a category theoretic error minimisation problem (Def \ref{definition:category_error}) in the form of an extension (Prop \ref{prop:extension_error_minimisation}), the left Kan extension $Lan_\iota d$ is, if it exists, a global error minimiser with respect to $d$.
\end{theorem}

\begin{proof}
For any $d\in D$ show that $Lan_\iota d$ is a global error minimiser by demonstrating that for any $m\in M$, $Err(d, Inf(Lan_\iota d)) \leq_S Err(d, Inf(m))$.

If there does not exist a morphism $f: d \rightarrow Inf(m)$ then the error comparison requirement (Def \ref{def:error_comparison}) is trivially satisfied.

If there does exist a morphism $f: d \rightarrow Inf(m)$ then this is also a 2-morphism, $f: d \Rightarrow m\iota$ (recalling that $Inf(m) = m\iota$ as described in Prop \ref{prop:extension_error_minimisation}) in $\mathbb{T}$. By the definition of a left Kan extension (Def \ref{def:left_kan_extension:local}), for any 2-morphism $f: d \Rightarrow m\iota$ there exists a 2-morphism $\alpha : Lan_\iota d \Rightarrow m$ such that $f = (\alpha\cdot \iota)\eta$. These 2-morphisms in $\mathbb{T}$ correspond directly with 1-morphisms of $D$ i.e. $\eta : d \rightarrow Inf(Lan_\iota d)$ and $\alpha\cdot \iota : Inf(Lan_\iota d) \rightarrow Inf(m)$ where $Inf(Lan_\iota d) = (Lan_\iota d)\iota$.

By the definition of a lax 2-functor (Def \ref{definition:Lax2Functor}) there exists a 2-morphism. \[\sigma: Err(\alpha\cdot \iota)Err(\eta) \Rightarrow Err((\alpha\cdot \iota)\eta) = Err(f)\]

Because the identity of $S$ is the bottom element there is a 2-morphism $\varphi : Id_* \Rightarrow Err(\alpha\cdot \iota)$ which by right whiskering produces the 2-morphism \[\varphi \cdot Err(\eta) : Err(\eta_d)\Rightarrow Err(\alpha\cdot \iota)Err(\eta_d)\]
Compose this with $\sigma$.
\[\sigma(\varphi \cdot Err(\eta)) : Err(\eta_d) \Rightarrow Err(f) \] 
As this is true for any choice of $f$ this proves the error comparison. \[Err(d, Inf(Lan_\iota d)) \leq_S Err(d, Inf(m))\]
As the error comparison is true for any choice of $m$ them $Lan_\iota d$ is a global error minimiser.
\end{proof}