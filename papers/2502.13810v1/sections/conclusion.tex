\section{Conclusion}

This paper has introduced an important bridge in the field of category theory for machine learning, providing a connection that allows the application of powerful category theoretic tools to old problems.

In addition to the theorems relating to the category-theoretic constructions, this methodology has also introduced insights that may impact how one views machine learning problems.

Firstly, the introduction of $S$ flavoured error supplements rigorous notions of how one might select an error function. Suggesting that error should be associated with the transformations between datasets gives an indication of how one may appropriately choose an error for a given problem. From this perspective, traditional notions such as distance, accuracy, and information loss reveal themselves as measurements of the minimal transformation necessary to convert one dataset into another.

Secondly, the independence of the left adjoint to choices of error may indicate that there are more fundamental ways of selecting a globally optimal model with respect to a dataset without referring to error. Re-framing model inference as a way of producing a dataset from a model, a mapping from a space of models to a space of datasets, allows one to think of algorithms as pseudo inverses to inference. In category theory, the natural choice of pseudo inverse is the adjunction, which is not constrained to an adjunction of functors. The definition of an adjunction can be generalised to any 2-category. Suppose it is more appropriate to represent the spaces of models and datasets as some other object, such as manifolds or measure spaces. In that case, finding an appropriate choice of 2-morphisms will indicate how to construct an algorithm as a left adjoint to inference.
% https://q.uiver.app/#q=WzAsMixbMCwwLCJNIl0sWzIsMCwiRCJdLFswLDEsIkluZiIsMCx7ImN1cnZlIjotMX1dLFsxLDAsIkFsZyIsMCx7ImN1cnZlIjotMX1dLFsyLDMsIlxcdG9wIiwxLHsic2hvcnRlbiI6eyJzb3VyY2UiOjIwLCJ0YXJnZXQiOjIwfSwic3R5bGUiOnsiYm9keSI6eyJuYW1lIjoibm9uZSJ9fX1dXQ==
\[\begin{tikzcd}
	M && D
	\arrow[""{name=0, anchor=center, inner sep=0}, "Inf", curve={height=-6pt}, from=1-1, to=1-3]
	\arrow[""{name=1, anchor=center, inner sep=0}, "Alg", curve={height=-6pt}, from=1-3, to=1-1]
	\arrow["\top"{description}, shorten >=2pt, Rightarrow, no body, from=0, to=1]
\end{tikzcd}\]

Finally, the demonstration that left Kan extensions may represent any error minimisation problem provides an interesting connection between the purpose of machine learning and the presentation. It is often intuitive to think that machine learning models find patterns within a dataset, allowing it to extend the already present data. However, this is only possible if one introduces additional assumptions about the data. Assumptions such as linearity, distance, smoothness, and maximum likelihood are all examples of assumptions machine learning models utilise to extend datasets. The nature of taking some aspect of data and extending it to a different context is precisely the structure encoded by a Kan extension, connecting intuitions about machine learning to a rigorous algebraic representation.
