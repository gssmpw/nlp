\section{Introduction}
%recent work shows kan extensions might relate to machine learning (cite me and sheibler)

Recent work has indicated that Kan extensions have a structural similarity to many machine learning algorithms \cite{shieblerKanExtensionsData2022, pugh_using_2023}. There is a tremendous amount of theory around the study of Kan extensions \cite{perrone_kan_2022, Kelly2005} and even algorithms for computing left Kan extensions efficiently \cite{meyers_fast_2022}. If the connection between Kan extensions and machine learning algorithms can be made more concrete, then it would be possible to leverage this body of work in the study of machine learning algorithms. 

This paper seeks to provide a concrete connection by proving that all error minimisation problems may be presented as a left Kan extension (Thm \ref{theorem:universal_ml_representation}).

A definition of error minimisation using sets and functions is lifted into the category-theoretic domain by representing it with categories and functors (Def \ref{definition:category_error}). It is shown that error may be represented by a lax 2-functor, which associates a form of information loss to transformations between datasets (morphisms in a category) (Def \ref{def:s_flavoured_error}).

The category-theoretic presentation of an error minimisation problem is used to show that left adjoint functors produce a global error minimiser for any input dataset (Thm \ref{thrm:adjoints_are_minimisers}). Furthermore, the error minimiser is independent of the error, indicating that an appropriate choice of the category of datasets is sufficient to determine the global error minimisation solutions (Cor \ref{corollary:error_independent}). A consequence of this result is the connection between adjoint functor theorems \cite{porst_history_2024} and error minimisation problems, providing sufficient conditions to define when an optimal solution to an error minimisation problem must exist (Cor \ref{corollary:sufficient_conditions}).

It is then shown that left Kan extensions are also error minimisers and that for any traditional or set-theoretic error minimisation problem, there is a 2-category whose left Kan extensions are precisely the global minimisers of the error minimisation problem (Thm \ref{theorem:universal_ml_representation}).

%this paper shows:
%%starts at a set theoretic definition of error minimisation
%%lifts the definition of error minimisation into category theory
%%defines monoid flavoured preorder which show how error relates to information loss
%% proves that left adjoints are error minimisors
%% shows that left adjoints are indepedent of error so are more general algorithms
%% sufficient conditions that come from adjoint functor theorem prove 
%% shows that any extension problems can be presented as an extension diagrams in a T category
%% proves that left kan extensions are error minimisors
%$ proves that any set theoretic error minimisation problem can be converted to an extension problem in a 2-category, where global minima are kan extensions and visa versa.
