\documentclass[lettersize, journal]{IEEEtran}
\usepackage{amsmath,amsfonts}
\usepackage{algorithmic}
\usepackage{algorithm}
\usepackage{array}
\usepackage{booktabs}
\usepackage[caption=false,font=footnotesize,labelfont=rm,textfont=rm]{subfig}
\usepackage{textcomp}
\usepackage{stfloats}
\usepackage{url}
\usepackage{verbatim}
\usepackage{graphicx}
\usepackage{subfloat}
\usepackage{hyperref}
\usepackage{bm}
\usepackage{multirow}
\usepackage{cite}
\usepackage{dsfont}
\usepackage{color}
\usepackage{soul}
\hypersetup{
	colorlinks=true,
	linkcolor=blue,
	filecolor=black,      
	urlcolor=black,
	citecolor=blue,
}
\usepackage{balance}
\usepackage{xcolor}


\begin{document}

	\title{Target Defense with Multiple Defenders and an Agile Attacker via Residual Policy Learning}
	
	\author{Jiyue Tao, Tongsheng Shen, Dexin Zhao\textsuperscript{*}, and Feitian Zhang\textsuperscript{*}
		% <-this % stops a space
		\thanks{J. Tao and F. Zhang are with the Robotics and Control Laboratory, Department of Advanced Manufacturing and Robotics, College of Engineering, and the State Key Laboratory of Turbulence and Complex Systems, Peking University, Beijing, 100871, China (\href{mailto: jiyuetao@pku.edu.cn}{email: jiyuetao@pku.edu.cn}; \href{mailto: feitian@pku.edu.cn}{email: feitian@pku.edu.cn}).
		}
		\thanks{T. Shen and D. Zhao are with the National Innovation Institute of Defense Technology, Beijing 100071, China (\href{mailto: shents_bj@126.com}{email: shents\_bj@126.com}; \href{mailto: zhaodx2008@163.com}{email: zhaodx2008@163.com}.)}
		\thanks{\textsuperscript{*}Send all correspondence to D.~Zhao and F.~Zhang.}
	}
        
        % The paper headers
        \markboth{}
        {Shell \MakeLowercase{\textit{et al.}}: A Sample Article Using IEEEtran.cls for IEEE Journals}
        
        %\IEEEpubid{0000--0000/00\$00.00~\copyright~2021 IEEE}
        % Remember, if you use this you must call \IEEEpubidadjcol in the second
        % column for its text to clear the IEEEpubid mark.
        
        \maketitle
        \pagestyle{empty}
        \thispagestyle{empty}
	
	\begin{abstract}
		The target defense problem involves intercepting an attacker before it reaches a designated target region using one or more defenders. This letter focuses on a particularly challenging scenario in which the attacker is more agile than the defenders, significantly increasing the difficulty of effective interception. To address this challenge, we propose a novel residual policy framework that integrates deep reinforcement learning (DRL) with the force-based Boids model. In this framework, the Boids model serves as a baseline policy, while DRL learns a residual policy to refine and optimize the defenders' actions. Simulation experiments demonstrate that the proposed method consistently outperforms traditional interception policies, whether learned via vanilla DRL or fine-tuned from force-based methods. Moreover, the learned policy exhibits strong scalability and adaptability, effectively handling scenarios with varying numbers of defenders and attackers with different agility levels.
	\end{abstract}
	
	\begin{IEEEkeywords}
		Target defense problem, reinforcement learning, multi-agent system, residual policy
	\end{IEEEkeywords}
	
	\section{Introduction}
	\IEEEPARstart{T}{he} pursuit-evasion game (PEG) has garnered significant attention due to its wide range of applications in navigation, security, and surveillance\cite{weintraub2020introduction}. With the advancements in robotics and mechatronics, the use of multiple robotic agents in PEGs is becoming increasingly realistic and popular. This letter considers a specific class of PEGs, called target defense problem (TDP), which is initially studied by Isaacs\cite{isaacs1999differential} and important for defense and security applications\cite{shi2024strategy, lee2024solutions, guo2024pursuit, feng2025decentralized}. 
	\par
	In TDP, a team of defenders (or pursuers) is responsible for intercepting a team of attackers (or evaders) before they can reach a designated target region. A direct approach for multi-robotic cooperative intercept is based on a centralized framework in which actions of all defenders are generated by a central rule. However, the complexity of TDP increases significantly with the number of agents involved. As a result, classical approaches usually address simplified versions of TDP, where each team consists of no more than two agents \cite{lcss1, lcss2, lcss3, shishika2021partial, garcia2019cooperative}. These approaches often leverage a well-established geometric construct known as the Apollonius circle, which is defined based on the relative positions and the speed ratio of the defender and the attacker \cite{isaacs1999differential}.
	\par 
	Force-based methods have been widely adopted by researchers to develop decentralized pursuit strategies, where attractive and repulsive forces are designed to guide the defenders' movements \cite{force1, force2, force3}. While these methods are typically computationally efficient and suitable for multi-agent scenarios, they often suffer from challenges such as parameter tuning and a lack of robustness when facing more intelligent attackers. Recently, deep reinforcement learning (DRL) has emerged as a promising approach for various PEGs and TDPs\cite{rl1, rl2, rl3}. Unlike traditional force-based methods, DRL agents are able to learn diverse cooperative pursuit strategies through a well-designed reward function, leading to a significant improvement in defense success rates (SRs). However, DRL is notoriously limited by poor sample efficiency and generalization capabilities \cite{sutton2018reinforcement, johannink2019residual}. As a result, the performance of DRL-based defensive strategies often deteriorates in scenarios involving varying numbers of defenders or attackers with different speeds. 
	\par
	This letter aims to address the challenge of defending a target region against a more agile attacker. In this scenario, the attacker possesses superior speed and angular velocity, requiring a higher level of cooperation among defenders to achieve successful interception. To tackle this problem, we propose a novel residual policy approach that integrates DRL with a force-based, bio-inspired method known as the Boids model \cite{bajec2007computational}. In this framework, the Boids model serves as the baseline policy, while DRL learns a residual policy to refine and enhance the actions generated by the baseline. To handle the problem of an uncertain number of defenders, we draw inspiration from prior work \cite{huttenrauch2019deep} and incorporate the mean embedding of observations. Given the homogeneity of the agents, a shared experience approach is adopted to efficiently train a single policy, which is then executed independently by each agent in deployment. Additionally, curriculum learning is employed during training, starting with a slightly more agile attacker and progressively increasing the attacker's agility as task complexity grows. We evaluate the proposed method in simulation environments using the capture SR as the primary metric. Results demonstrate that our method significantly outperforms both vanilla DRL and force-based methods, showcasing superior generalization to scenarios with varying attacker agility levels and different numbers of defenders.
	
	\section{Problem Formulation} 
	\begin{figure}[t]
		\centering
		\includegraphics[width=0.9\linewidth]{figs/tgp.pdf}
		\caption{Schematic diagram of the target defense problem with multi-defenders and more agile attacker considered in this letter.}
		\label{fig:tgp}
	\end{figure}
	The problem addressed in this letter involves multiple homogeneous defenders tasked with protecting a target region from a significantly more agile attacker, as illustrated in Fig.~\ref{fig:tgp}. Since individual defenders are unable to effectively counter the attacker, their objective is to work as a team and cooperatively capture the attacker. A trial is considered successful if, at any point during the trial, the distance between the attacker (evader) and at least one defender (pursuer) is less than a predefined capture radius ($d_{i,A} < d_\text{cap}$). Additionally, the trial is deemed successful if the attacker fails to reach the target region within a fixed time period, $T_\text{total}$. Conversely, the trial is considered a failure if the attacker breaches the boundary of the circular target region, defined as $\|\boldsymbol{x}\| \leq r_0$.
	\par
	In this letter, the kinematics of each robot $i$ are modeled using a unicycle model as follows
	\begin{equation}
		\begin{aligned}
			\dot{x}_i &= v \cos \psi_i \\
			\dot{y}_i &= v \sin \psi_i \\
			\dot{\psi}_i &= \omega.
		\end{aligned}
	\end{equation} 
	Here, $\boldsymbol{x}_i = (x_i, y_i)$ is the current position, $\psi_i$ is the heading angle, and $\omega$ is the angular velocity. We assume that the attacker and defenders move at a constant speed $v_A$ and $v_D$, respectively. At the initial stage, defenders are positioned near the target, while the attacker approaches the target's sensing region from a random direction. The angular velocities are constrained within specific ranges $\omega_A \in \left[-\omega^\text{max}_A, \omega^\text{max}_A\right]$ for the attacker and $\omega_D \in \left[-\omega^\text{max}_D, \omega^\text{max}_D\right]$ for the defenders, where $\omega^\text{max}_A > \omega^\text{max}_D$. To simplify the problem, we set the ratio of the attacker's and defenders' maximum linear speeds equal to the ratio of their maximum angular velocities, i.e., $v_A / v_D = \omega_A^\text{max} / \omega_D^\text{max}$. Based on this, the attacker's agility level is defined as
	\begin{equation}
		\mathcal{L}_\text{agi} := v_A / v_D = \omega_A^\text{max} / \omega_D^\text{max}.
	\end{equation}
	
	\section{Proposed Method}
	\par
	We first formulate the problem as a Markov Decision Process (MDP) which is a mathematical framework described by a 5-tuple $(\mathcal{S}, \mathcal{A}, \mathcal{R}, \mathcal{P}, \gamma)$ where $\mathcal{S}$ is the state space, $\mathcal{A}$ is the action space, $\mathcal{R}$ is the reward function, $\mathcal{P}$ is the state-transition probability, and $\gamma$ is a discount factor. The objective of DRL is to maximize the cumulative reward $R=\sum_{t=0}^T \gamma^t r_t$, where $r_t$ is the reward obtained at time $t$.
	
	\subsection{Soft Actor Critic}
	As for the DRL algorithm, we employ soft actor critic (SAC)\cite{SAC}, which is a state-of-the-art DRL algorithm known for its strong robustness and sample efficiency. Unlike traditional DRL algorithms, SAC adopts an entropy-maximization approach, with its learning objective formulated as
	\begin{equation}
		J(\pi)=\sum_{t=0}^T\mathbb{E}_{(s_t,a_t)\sim\rho_\pi}\left[\mathcal{R}(s_t,a_t)+\alpha\mathcal{H}(\pi(\cdot|s_t))\right],
	\end{equation}
	where $\mathcal{H}(\pi(\cdot | s_t))$ denotes the entropy of the policy $\pi$, and $\alpha$ is a temperature parameter that controls the trade-off between exploration and exploitation. This entropy-regularized objective enhances the robustness of the DRL agent and accelerates the training process. Besides, our approach follows the \textit{centralized training decentralized execution} paradigm, combined with parameter sharing. Specifically, a single shared policy is trained using experiences collected by all homogeneous defenders, enabling faster data collection and improved convergence of the algorithm. During execution, each agent independently takes actions based on its local observations, resulting in a fully decentralized system. The action taken by agent $i$ is computed as the average of the action output by the DRL policy $a_{i, \text{drl}}$ and the action generated by the Boids model $a_{i, \text{boids}}$, and is expressed as
	\begin{equation}
		\label{eq:residual}
		a_i = a_{i, \text{drl}} + a_{i, \text{boids}}.
	\end{equation}
	
	\subsection{Boids Model}
	The Boids model, first introduced by C.~W.~Reynolds\cite{reynolds1987flocks}, is a bio-inspired computational framework for simulating the collective behavior of swarm-like agents, such as flocks of birds or schools of fish. This decentralized model governs the behavior of each agent using three fundamental rules including separation, alignment, and cohesion. To enable defenders to pursue the attacker in our context, we introduce an additional attraction rule. The mathematical formulation of these rules used in this letter is defined as follows
	\begin{itemize}
		\item	\textbf{Separation} ($\boldsymbol{F}_\text{sep}$): Ensures agents avoid collisions by maintaining a safe distance from their neighbors.
		\begin{equation}
			\boldsymbol{F}_{i, \text{sep}} = -k_\text{sep} \sum_{j\in\mathcal{N}_i} \frac{\boldsymbol{x}_j - \boldsymbol{x}_i}{\| \boldsymbol{x}_j - \boldsymbol{x}_i \|},
		\end{equation}
		where $k_\text{sep}$ is the weight of the separation force, $\boldsymbol{x}_j$ is the position of neighbor $j$, and $\mathcal{N}_i$ is the set of neighbors of agent $i$.
		
		\item	\textbf{Alignment} ($\boldsymbol{F}_\text{ali}$): Aligns an agent's velocity with the average velocity of its neighbors.
		\begin{equation}
			\boldsymbol{F}_{i, \text{ali}} = \frac{k_\text{ali}}{|\mathcal{N}_i|}\sum_{j\in\mathcal{N}_i} \boldsymbol{v}_j - \boldsymbol{v}_i,
		\end{equation}
		where $k_\text{ali}$ is the weight of the alignment force, and $\boldsymbol{v}_j$ is the velocity of neighbor $j$.
		
		\item	\textbf{Cohesion} ($\boldsymbol{F}_\text{coh}$): Drives agents to move toward the center of mass of their local group.
		\begin{equation}
			\boldsymbol{F}_{i, \text{coh}} = \frac{k_\text{coh}}{|\mathcal{N}_i|} \sum_{j\in\mathcal{N}_i} \boldsymbol{x}_j - \boldsymbol{x}_i,
		\end{equation}
		where $k_\text{coh}$ is the weight of the cohesion force.
		
		\item	\textbf{Attraction} ($\boldsymbol{F}_\text{att}$): Attracts agents toward the attacker's current position
		\begin{equation}
			\boldsymbol{F}_{i, \text{att}} = k_\text{att}(\boldsymbol{x}_A - \boldsymbol{x}_i),
		\end{equation}
		where $k_\text{att}$ is the weight of the attraction force, and $\boldsymbol{x}_A$ is the position of the attacker.
	\end{itemize}
	\par
	Therefore, the total virtual force acting on agent $i$ is the sum of these forces given by
	\begin{equation}
		\label{eq:forces}
		\boldsymbol{F}_i = \boldsymbol{F}_{i, \text{sep}} + \boldsymbol{F}_{i, \text{ali}} + \boldsymbol{F}_{i, \text{coh}} + \boldsymbol{F}_{i, \text{att}}.
	\end{equation}
	
	\subsection{State Representation}
	\label{subsec:state}
	\begin{figure*}[t]
		\centering
		\includegraphics[width=0.9\linewidth]{figs/obs-emb.pdf}
		\caption{(a) The observation space for each defender includes information about the attacker $A$, the target region $T$, and other defenders $D$, which is all represented in local coordinates. (b) The neural network's state embedding structure is shown. Each feature is processed through fully connected (Fc) layers with a specified number of hidden units, using ReLU as the activation function. The three embedded features are concatenated and passed to an output network consisting of two additional fully connected layers.}
		\label{fig:obs-emb}
	\end{figure*}
	The state space of each defender $i$, assuming a total of $n$ defenders, consists of three components and is defined as $s_i = \left[ s_{i,D}, s_{i,AT}, s_{i, \text{boids}} \right]$. As illustrated in Fig.~\hyperref[fig:obs-emb]{\ref*{fig:obs-emb}(a)}, $s_{i,D} = \left[ \phi_{i,1}, d_{i,1}, \dots, \phi_{i,n}, d_{i,n} \right]$ represents the observations of other defenders, while $s_{i,AT} = \left[ \phi_{i,A}, d_{i,A}, \phi_{i,T}, d_{i,T} \right]$ includes the observations of the attacker and the target. Since the DRL algorithm is designed to learn a residual policy based on the Boids model, $s_{i, \text{boids}} = \left[ \boldsymbol{F}_{i, \text{sep}}, \boldsymbol{F}_{i, \text{ali}}, \boldsymbol{F}_{i, \text{coh}}, a_{i, \text{boids}} \right]$ contains the virtual forces from \eqref{eq:forces} and the Boids model's output action. This feature implicitly includes the information of teammate positions and overall formation, assisting the DRL agent in learning an improved policy. 
	\par
	As shown in Fig.~\hyperref[fig:obs-emb]{\ref*{fig:obs-emb}(b)}, $s_{i,D}$ is processed using mean observation embedding, following the approach in\cite{huttenrauch2019deep}, while $s_{i, AT}$ and $s_{i, \text{boids}}$ are processed independently through single fully connected layers. The three resulting features are concatenated and passed through a network with two additional fully connected layers. The network's output is the DRL action $a_{i,\text{drl}}$ as defined in \eqref{eq:residual}. The critic network employs a similar architecture but includes an additional action embedding layer and outputs the corresponding Q-value.
	
	\subsection{Reward Function}
	In our proposed framework, each agent obtains its own reward at each time step, which is designed as  
	\begin{equation}
		r_i = r_\text{main} + r_\text{att} + r_\text{form},
	\end{equation}
	where $r_\text{main}$ gives the agent a reward or penalty when it succeeds or fails, and is defined as
	\begin{equation}
		r_\text{main}=
		\begin{cases}
			-100, 	&\text{if}\ \|\boldsymbol{x}_A\| \leq r_0, \\
			100, 	&\text{if}\ d_{i,A} \leq d_\text{cap}, \\
			50, 	&\text{if}\ d_{j,A} \leq d_\text{cap}\ \text{s.t.} \ d_{i,A} \leq 4 \cdot d_\text{cap}, \\
			0, 		&\text{otherwise}.
		\end{cases}
	\end{equation}
	Here, if a teammate successfully captures the attacker and the agent is sufficiently close to the capture location ($d_{i,A} \leq 4 \cdot d_\text{cap}$), the agent is considered to have assisted in the capture and is awarded a helper reward of 50. 
	\par
	The attraction reward $r_\text{att}$ is designed to guide the defenders towards the attacker using a potential-based structure defined as $r_\text{att} = \Phi(s_{t+1}) - \Phi(s_t)$. Here, the potential function $\Phi(s)$ is given by
	\begin{equation}
		r_\text{att}=
		\begin{cases}
			15, 	&\text{if}\ d_{i,A} \leq 5\,\text{m}, \\
			10, 	&\text{if}\ 5\,\text{m} < d_{i,A} \leq 15\,\text{m}, \\
			5, 	&\text{if}\ 15\,\text{m} < d_{i,A} \leq 25\,\text{m} \\
			0, 		&\text{otherwise}.
		\end{cases}
	\end{equation}
	The reward is only provided when the defender is relatively close to the attacker, ensuring that flanking behaviors are not penalized. Instead of penalizing the distance to the attacker, this design encourages agents to develop cooperative strategies for approaching and surrounding the attacker.
	
	\begin{figure}[t]
		\centering
		\includegraphics[width=0.72\linewidth]{figs/r-form.pdf}
		\caption{Schematic diagram of the components included in the formation reward $r_{form}$.}
		\label{fig:r-form}
	\end{figure}
	
	The formation reward $r_\text{form}$ is designed to encourage defenders to surround the attacker and maintain a cohesive defensive formation. As illustrated in Fig.~\ref{fig:r-form}, $\vec{d}_{iA}$ represents the unit vector pointing from defender $i$ toward the attacker, while $\vec{d}_{DA}$ denotes the sum of all such unit vectors, i.e., $\vec{d}_{DA} = \sum_{i=1}^{n} \vec{d}_{iA}$). The formation reward $r_\text{form}$ is then calculated as
	\begin{equation}
		r_\text{form} = 0.5 \cdot \vec{d}_{TA} \cdot \frac{\vec{d}_{DA}}{\| \vec{d}_{DA} \|} - \frac{\| \vec{d}_{DA} \|}{n},
	\end{equation}
	where the first term promotes a symmetric distribution of defenders on both sides of $\vec{d}_{TA}$, and the second term encourages the defenders to spread out, thereby increasing the defensive coverage.
	
	\subsection{Curriculum Learning}
	We employ a curriculum learning approach to progressively increase task difficulty, thereby promoting effective exploration during the early stages of training. In this study, the primary factor influencing task difficulty is the attacker's agility level, $\mathcal{L}_\text{agi}$. At each training episode, $\mathcal{L}_\text{agi}$ is randomly sampled from a uniform distribution, i.e.,
	\begin{equation}
		\mathcal{L}_\text{agi} \sim \text{U}(\bar{\mathcal{L}}_\text{agi} - 1.0, \bar{\mathcal{L}}_\text{agi} + 1.0),
	\end{equation}
	where $\bar{\mathcal{L}}_\text{agi}$ represents the average agility level. In our designed curriculum learning, the value of $\bar{\mathcal{L}}_\text{agi}$ starts at a relatively low level ($\bar{\mathcal{L}}_\text{agi} = 1.75$) and is gradually increased over the course of training. Specifically, $\bar{\mathcal{L}}_\text{agi}$ is increased by $0.25$ every $5 \times 10^5$ training steps, corresponding to one-quarter of the total training duration. This curriculum learning approach encourages the agents to develop robust and generalizable defensive strategies, rather than relying on simple tactics effective only at specific agility levels. 
	
	\section{Simulation Experiment}
	\begin{figure*}[t]
		\centering
		\includegraphics[width=0.9\linewidth]{figs/sr-baseline.pdf}
		\vspace{-0.1in}
		\caption{Comparison of our method with baseline methods. (a) Success rate with respect to different attacker's agility level, for $n=3$ defenders. (b) Success rate with respect to different numbers of defenders, for an agility level of $\mathcal{L}_\text{agi} = 2.5$.}
		\label{fig:sr-baseline}
	\end{figure*}
	
	\begin{figure}[t]
		\centering
		\includegraphics[width=0.85\linewidth]{figs/train-res.pdf}
		\vspace{-0.1in}
		\caption{Comparison of success rates between the proposed method and vanilla DRL with respect to the number of training steps. The performance is evaluated under the scenario where $n = 3$ and $\mathcal{L}_\text{agi} = 2.25$.}
		\label{fig:train-res}
	\end{figure}
	In this section, simulation experiments are performed to demonstrate the effectiveness of the proposed algorithm, which is compared with two baseline methods. The test scenarios include attackers of different agility levels and different numbers of defenders.
	
	\subsection{Experiment Setup}
	\subsubsection{Simulation parameters}
	In our experiments, the simulation parameters were set as follows: the defender's speed $v_D$ and maximum angular velocity $\omega_D^\text{max}$ were set to $1.0$ m/s, $0.3$ rad/s, respectively. The capture distance $d_\text{cap}$ was defined as $2$ m. The target radius $r_0$, the target's sensing range $\rho_T$, and the attacker's sensing range $\rho_A$ were set to $5$ m, $55$ m, and $15$ m, respectively. The total simulation time $T_\text{total}$ and action interval $T_\text{action}$ were set to $60$ s and $0.2$ s, respectively. The Boids model weights $k_\text{sep}$, $k_\text{ali}$, $k_\text{coh}$, and $k_\text{att}$ were configured as $10$, $0.1$, $0.5$, $1$ respectively. During training, the number of defenders was fixed at $3$. The SAC algorithm was used with a learning rate of $1\times 10^{-4}$, a batch size of $4096$, a total of $2\times 10^6$ training steps, and a discount factor of $0.99$.
	\subsubsection{Attacker policy}
	The attacker employs the artificial potential field (APF) method, which combines attractive and repulsive forces. Each defender within the attacker's sensing range applies a repulsive force directed from the defender to the attacker. Meanwhile, the target region generates an attractive force, guiding the attacker toward the target. The total virtual force acting on the attacker is computed as
	\begin{equation}
		\boldsymbol{F}_A = (\boldsymbol{x}_T - \boldsymbol{x}_A) + \sum_i \frac{\boldsymbol{x}_A - \boldsymbol{x}_i}{d^{2}_{i,A}} \left(\frac{1}{d_{i,A}} - \frac{1}{\rho_A} \right),
	\end{equation}
	where $\boldsymbol{x}_T$, $\boldsymbol{x}_A$, and $\boldsymbol{x}_i$ represent the positions of target, the attacker, and defender $i$, respectively; and $d_{i,A}$ denotes the distance between defender $i$ and the attacker.
	
	\subsubsection{Baseline methods}
	We compared our method against two baseline approaches: the Boids model and a vanilla DRL policy learned through SAC algorithm. For the Boids model, all weights were set identically to those used in our method to ensure a fair comparison. For the vanilla DRL policy, the Boids-related state component $s_{i, \text{boids}}$ was removed from the state space (see Section~\ref{subsec:state}), while all other settings and parameters were kept identical.
	
	\subsection{Comparison Results with Baseline Methods}
	\subsubsection{Agility levels}
	In this subsection, we evaluate the SR of our method and two baselines across different attacker agility levels, with $n = 3$ defenders. As shown in Fig.~\hyperref[fig:sr-baseline]{\ref*{fig:sr-baseline}(a)}, the SR of all three methods declines as the agility level increases. However, our method maintains a success rate of over 90\% when the agility level $\mathcal{L}_\text{agi} \leq 2$ and consistently outperforms the baseline methods across all agility levels. Specifically, the proposed method achieves an average SR improvement of 10.4\% compared to the vanilla DRL algorithm and 23.6\% compared to the Boids model. These results highlight the superior abilitety of our method to effectively generalize to varying agility levels the attacker.
	
	\subsubsection{Number of defenders}
	The success rates of the three methods with respect to the number of defenders are shown in Fig.~\hyperref[fig:sr-baseline]{\ref*{fig:sr-baseline}(b)} for an agility level of $\mathcal{L}_\text{agi} = 2.5$. It is important to note that the learning-based methods were trained exclusively in the 3-defender setting. As expected, the success rate of the Boids model increases with the number of defenders, owing to its strong generalizability to varying numbers of agents. In contrast, learning-based methods tend to overfit to policies tailored to the specific number of defenders used during training. Consequently, the SR of the vanilla DRL method decreases as the number of defenders increases. By leveraging the Boids model and residual policy, our proposed method achieves a relatively high SR across different numbers of defenders. 
	
	\begin{figure*}[t]
		\centering
		\includegraphics[width=0.9\linewidth]{figs/sr-ablation.pdf}
		\vspace{-0.1in}
		\caption{Comparison results of the ablation study. (a) Success rate with respect to different attacker's agility level, for $n=3$ defenders. (b) Success rate with respect to different numbers of defenders, for an agility level of $\mathcal{L}_\text{agi} = 2.5$.}
		\label{fig:sr-ablation}
	\end{figure*}
	
	\subsubsection{Sample efficiency}
	A key advantage of the proposed method over vanilla DRL is its high sample efficiency. Fig.~\ref{fig:train-res} illustrates the impact of the proposed residual policy on sample efficiency. The network was evaluated on the benchmark with $n=3$ defenders and an agility level of $\mathcal{L}_\text{agi} = 2.25$ every $5000$ training steps.  with each model trained across four independent runs. The results demonstrate that the proposed residual policy significantly enhances sample efficiency, achieving a SR of over $80\%$ after just $0.75$ million steps. In contrast, the vanilla DRL required $2$ million steps to reach a comparable level of performance. Additionally, the proposed method exhibited greater consistency, as indicated by the low variance observed across the four training runs.
	
	\subsection{Ablation Study}
	To assess the contributions of individual components in the proposed method, we conducted a series of ablation experiments. Three variants of the method were tested: (1) \textit{w/o curriculum learning:} This variant omits the curriculum learning approach, fixing the attacker agility level $\mathcal{L}_\text{agi}$ at 2.25 during training. (2) \textit{w/o Boids state:} This variant excludes the Boids state observation $s{i, \text{boids}}$ from the inputs to the actor and critic networks.  (3) \textit{w/o formation reward:} This variant removes the formation reward $r_\text{form}$ from the reward function.
	\par
	As illustrated in Fig.~\ref{fig:sr-ablation}, we evaluated the SR of these variants under varying attacker agility levels and different numbers of defenders. In Fig.~\hyperref[fig:sr-ablation]{\ref*{fig:sr-ablation}(a)}, the SR of \textit{w/o curriculum learning} declines significantly when $\mathcal{L}_\text{agi}$ deviates from 2.25, highlighting the effectiveness of the curriculum learning method. The SR of \textit{w/o formation reward} exhibits a slight decrease, while the exclusion of the Boids state has minimal impact in this scenario. In Fig.~\hyperref[fig:sr-ablation]{\ref*{fig:sr-ablation}(b)}, all three variants demonstrate reduced SR, confirming that each component enhances the policy's generalization. Notably, the SR of \textit{w/o Boids state} decreased most significantly, as the model lacks critical defensive formation information provided by the Boids state and the observation embedding tends to overfit to scenarios with three defenders.
	
	\subsection{Extension to Multi-Attacker Scenarios}
		\begin{figure}[t]
			\centering
			\includegraphics[width=\linewidth]{figs/multi-exp.pdf}
			\vspace{-0.15in}
			\caption{Numerical simulation results for multi-attacker scenarios. The proposed approach effectively defends against attackers exhibiting varying agility levels and approaching from different directions.}
			\label{fig:multi-exp}
		\end{figure}
		
	In the multi-attacker scenario, a new defender grouping sub-problem arises. A successful defense is generally more likely when defenders are positioned closer to the attackers \cite{chen2024distributed}. To address this, the objective of the sub-problem is to minimize the total distance between defenders and attackers, which can be formulated as follows
	\begin{equation}
		\begin{aligned}
			\min_{y_{ij}} &\sum_{i\in\mathcal{N}_D} \sum_{j\in\mathcal{N}_A} y_{ij} \| \boldsymbol{x}_i - \boldsymbol{x}_j \| \\
			\text{s.t.}\ & \sum_{j\in\mathcal{N}_A} y_{ij} \leq 1,\ \forall i\in\mathcal{N}_D \\
			& \sum_{i\in\mathcal{N}_D} y_{ij} \geq 3,\ \forall j\in\mathcal{N}_A
		\end{aligned}
	\end{equation}
	where $\mathcal{N}_D$ and $\mathcal{N}_A$ denote the sets of defenders and attackers, respectively. The binary variable $y_{ij} = 1$ indicates that defender $i$ is assigned to group $\mathcal{G}_j$ while $y_{ij} = 0$ indicates otherwise. The first constraint ensures that each defender is assigned to at most one group, while the second constraint requires that each attacker is targeted by at least three defenders to ensure a relatively high SR. 
	\par
	Fig.~\ref{fig:multi-exp} presents the simulation results for extending the problem to multi-attacker scenarios. In this setup, three attackers with varying agility levels approach the target from random directions. The results demonstrate that the proposed approach has been successfully adapted to handle multi-attacker scenarios, highlighting its adaptability and potential for broader applications.
	
	\section{Conclusion}
	This letter investigated the target defense problem involving a more agile attacker and multiple defenders. To tackle this challenging problem, we propose a novel residual policy framework that integrates DRL with the force-based Boids model. The DRL agent was trained to refine the actions generated by the Boids model, thereby improving the defenders' success rate. Furthermore, a carefully designed reward function and a curriculum learning strategy were incorporated to improve sample efficiency and generalizability. Simulation results demonstrated that our method outperformed both the baseline Boids model and the policy learned by a standard DRL algorithm. Additionally, our approach exhibited strong generalizability across varying attacker agility levels and different numbers of defenders, highlighting its potential for application in diverse scenarios. Future research will focus on exploring collision avoidance and real-world applications.
	
	\bibliographystyle{IEEEtran}
	%	\footnotesize
	%	 \balance
	
	\bibliography{ref}
	
\end{document}



