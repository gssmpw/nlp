\section{Introduction}\label{sec:introduction}
% 引入mllm
\input{latex/figure/teaser_curry}
The advent of large language models (LLMs)~\citep{radford2019language, brown2020language, achiam2023gpt, touvron2023llama, peng2023instruction, bi2024deepseek} and pre-trained vision models~\citep{radford2021learning, liu2025grounding} has propelled multimodal large language  models (MLLMs)~\citep{team2023gemini, Qwen-VL, wang2024qwen2, chen2023internvl, li2024mini, li2023blip} by advancing connections between visual and linguistic modalities through linear projection~\citep{li2024llava} or q-former~\citep{instructblip}. 
% With modal alignment and visual instruction fine-tuning \citep{du2021glm, liu2023improvedllava, zhu2023minigpt}, recent MLLMs successfully adapt LLMs to the vision domain and inherit their perception and reasoning abilities.
These MLLMs exhibit remarkable capabilities across a diverse range of multimodal tasks, including image captioning, visual question answering (VQA), video understanding~\citep{wang2024internvideo2}, and multimodal reasoning~\citep{wang2024exploring}. % todo: add reference
\input{latex/figure/random_vs_others.tex}

% 指出现有mllm中image tokens占比过高且信息稀疏
% 尽管各种mllm展现出了promising performance，但与文本模态相比过长的vision token给mllm带来了巨大存储和计算量过大的挑战，尤其是当处理超高分辨率图像，视频理解以及多图场景。
Although various MLLMs have demonstrated promising performance, the significantly longer vision tokens compared to the linguistic modality pose substantial challenges in terms of massive storage and computational overhead~\citep{ju2023turbo}, especially when handling high-resolution images~\citep{li2024mini} and multi-frame video~\citep{tang2023video}.
For instance, processing a single video with Video-LLaVA~\citep{lin2023video} involves extracting $8$ frames, generating $2048$ vision tokens—significantly surpassing the number of text tokens.

This limits the application scenarios of MLLMs, especially in cases that require low latency, such as autonomous driving~\citep{jia2023adriver} and real-time interactions~\citep{fu2024vita}.

% 介绍现有的解决mllm中image token过长的方法
% 一些方法需要更改组件然后训练

% 1. fastv->无法兼容fa
% 2. sparsevlm->无法兼容fa
% 3. mustdrop->无法兼容fa

Despite occupying a significant portion of the input sequence, vision tokens lack the information density like text modality~\citep{marr2010vision}.
To address this issue, several studies have sought to extract more compact image representations by enhancing the image encoder or projector~\citep{alayrac2022flamingo, li2023llama, instructblip, cha2024honeybee}.
However, these methods typically necessitate substantial modifications to the MLLM architecture and demand extensive training.
% 与此同时，一些最近的工作尝试用training-free的方法减少vision tokens。
While some recent works have explored training-free approaches to reduce vision tokens by token pruning, which firstly defines the importance score of each token, and then prunes the most unimportant tokens during the inference phrase~\citep{chen2024image, zhang2024sparsevlm, liu2024multi}. 
These importance-based token pruning methods implicitly assumes that there exists a static and oracle score that can characterize the influence of this token on model prediction. However, in this paper, we argue that this ``importance'' perspective exist several serious problems as follows.

\textbf{``Importance'' scores ignore the interaction between different tokens during pruning. For two similar tokens, }

% 独立性假设
% 效果不好
% 不支持flashattention
% 位置bias



%the influence of pruning a token can be considered independently with other tokens. Obviously, this assumption ignores the interaction between multiple tokens. In other words, the choice of pruning a token should not only judged by itself, but also influenced by the pruning choice of other tokens, especially influenced by the tokens which are similar to this token.



Due to their reliance on attention scores for assessing the importance of vision tokens, these methods struggle to integrate smoothly with Flash Attention~\citep{dao2022flashattention, dao2023flashattention2}, significantly limiting their acceleration benefits.
% Additionally, these approaches place excessive emphasis on the importance of tokens, overlooking the crucial role that token duplication plays in token reduction.

% our method
% 在这篇工作中，我们试图回答三个问题：
% 1. 能否不利用attention score挑选tokens,做到兼容flash attention？
% 2. 通过衡量token重要性进行token reduction是最优的？
% 3. 除了token重要性，在token reduction中还需要关注什么？
% 加一句:为了解决这一问题，我们尝试设计能够与flash attention兼容的token reduction方法。
To address this issue, we aim to develop token reduction methods that are compatible with flash attention.
Before that, we delve into two intriguing research questions:
\textbf{Q1:} \textit{Is it optimal for the attention score to measure token importance to guide the token reduction paradigm?}
\textbf{Q2:} \textit{Beyond token importance, what factors should be considered in token reduction?}
 \\
\textbf{Is it optimal for the attention score to measure token importance to guide the token reduction paradigm?}
 % TODO: refine this sentence.
We conducted a comparative analysis of token reduction performance across random token selection, FastV~\citep{chen2024image}, and SparseVLM~\citep{zhang2024sparsevlm}. As illustrated in Figure~\ref{fig:random_vs_others}, random token selection even outperforms both FastV and SparseVLM on certain benchmarks. This suggests that relying on attention scores to measure token importance for reduction may not be optimal. Instead, it is crucial to explore factors beyond token importance to address the inherent limitations of attention scores in flash attention.
% 加个柱状图来说明random token selection有时候好于依据attent score精心设计的fastv & sparsevlm
% 根据图讲解下，得到结论：在token reduction中考虑token importance不是最优方案。
\\
\textbf{Beyond token importance, what factors should be considered in token reduction?}
% fastv是典型的利用attention score衡量token重要性进行token reduction的方法->画图
 %1. 首先在同一张图像上将fastv保留image token对应的patch可视化出来 vs 我们利用k-norm + token重复性考虑的patch 可视化
 %2. 然后在特征空间进行tsne可视化的对比，展示出我们保留的特征没有太聚集，比较分散
% 顺势引出考虑token重复性在token reduction中的必要性。
Previous research has extensively investigated the over-smoothing phenomenon in transformers~\citep{ nguyen2023mitigating, wang2022anti}, with studies demonstrating that self-attention mechanism inherently drives token representations toward increasingly identical latent states~\citep{gong2021vision, wang2023label}.
This insight inspires us to incorporate token duplication into the token reduction. When multiple tokens exhibit identical or highly similar representations, it is natural to retain only one of them for prediction, thereby maintaining efficiency without compromising performance.
% \input{latex/figure/radar_compare.tex}
\input{latex/figure/latency_vs_performance}

In this work, building upon the insights from the aforementioned discussions, we propose a plug-and-play token reduction method grounded in token duplication without training, which is designed to be fully compatible with flash attention. 
% Specifically, during the forward inference process of the MLLMs, at the shallow layers, we first select a small subset of pivot tokens (no more than 2\% of the total tokens) based on their high $\mathrm{K}_{\mathrm{norm}}$ values. We then compute the cosine similarity between these selected tokens and the remaining image tokens. Based on the predefined reduction ratio, we retain the image tokens with the lowest cosine similarity, as we hypothesize that image tokens exhibiting high feature redundancy with the selected tokens contribute minimally to subsequent predictions.
% Specifically, during the inference process of MLLMs, we begin by selecting a small subset of pivot tokens—comprising no more than $2\%$ of the total tokens—based on their elevated K-norm values in the shallow layers. 
Specifically, during the inference process of MLLMs, we begin by selecting a small subset of pivot tokens, comprising no more than $2\%$ of the total tokens in the shallow layers. 
We then calculate the cosine similarity between pivot tokens and the remaining image tokens. Guided by a predefined reduction ratio, we retain only those image tokens with the lowest cosine similarity. The entire process is highly efficient, completing in no more than $0.08$ seconds. This approach is motivated by the prior insights~\citep{gong2021vision} that image tokens exhibiting high feature duplication with the pivot tokens contribute negligibly to predictions. Moreover, since our entire process operates independently of attention scores, it is fully compatible with flash attention, ensuring seamless integration and enhanced computational efficiency.
In summary, the contributions of this work are three-fold:
\begin{itemize}
    \item \textbf{Rethink Token Important} Through empirical analysis, we demonstrate the suboptimality of relying on attention scores to measure token importance to guide the token reduction paradigm.
    \item \textbf{Token Duplication as a Key Factor} Building on the concept of token duplication, we introduce a training-free, plug-and-play token reduction method that seamlessly integrates with flash attention. 
    \item ...
\end{itemize}

