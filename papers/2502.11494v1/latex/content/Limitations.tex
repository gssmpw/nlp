\section{Limitations}
% pivot token 数量动态选择或许更好？
One of the limitations of our work is that it cannot be applied to black-box models like the GPT (\emph{e.g.} GPT 3.5 and more advanced versions) and Claude series, as we are unable to access their encoded tokens during the inference process. Additionally, our proposed DART exhibits slightly degraded performance when dealing with models that encode tokens with high information density. For instance, Qwen2-VL employs token merging during training, and MiniCPM-V-2.6 utilizes learnable queries in its Resampler module to map variable-length patch features into shorter feature representations. In such cases, removing a single visual token results in a greater loss of information, thereby impacting the effectiveness of our approach. Despite this, DART is still able to maintain 97.0\% and 92.9\% of its performance while reducing visual tokens by 66.7\%, as demonstrated in Tables~\ref{tab:qwen2vl}, \ref{tab:minicpm}.