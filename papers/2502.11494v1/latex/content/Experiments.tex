\input{latex/content/table/image_understanding}
\section{Experiments}
\vspace{-0.1cm}
%In this section, we evaluate the effectiveness of \algname across a diverse range of comprehensive multi-modal benchmarks, encompassing both image understanding and video understanding tasks.
% 为节省空间删掉了
\noindent\textbf{Experiment Setting.} We conduct experiments on over four MLLMs across ten image-based and four video-based benchmarks. For details on implementation, please refer to Appendix~\ref{app:detailed_experiment_settings}.
%\textbf{Datasets.}  
%For the image understanding task, we performed experiments on nine widely used benchmarks, including GQA \citep{hudson2019gqa}, MMBench (MMB) and MMB-CN \citep{liu2025mmbench}, MME \citep{fu2023mme}, POPE~\citep{li2023evaluating}, VizWiz \citep{bigham2010vizwiz}, SQA \citep{lu2022learn}, VQA$^{\text{V2}}$ (VQA V2) \citep{goyal2017making}, and VQA$^{\text{Text}}$ (TextVQA) \citep{singh2019towards}.
%For the video understanding task, we evaluated our method on three video-based benchmarks: TGIF-QA \citep{jang2017tgif}, MSVD-QA \citep{xu2017video}, and MSRVTT-QA \citep{xu2017video}. More details are provided in the Appendix~\ref{app:dataset}.

%\noindent\textbf{Implementation Details}


%We evaluate \algname using various open-source MLLMs. For image understanding tasks, experiments are conducted on LLaVA family, including LLaVA-1.5-7B~\citep{liu2024visual} and LLaVA-Next-7B~\citep{liu2024llavanext}, with the latter used to validate performance on high-resolution images.
% Additionally, we evaluated Qwen2-VL-7B~\citep{wang2024qwen2}, which incorporates dynamic resolution and token merging techniques.
%Furthermore, we validate our method on more advanced models, including Qwen2-VL-7B~\citep{wang2024qwen2} and MiniCPM-V-2.6~\citep{yao2024minicpm}.
%For video understanding tasks, we use Video-LLaVA~\citep{lin2023video} as the baseline model.
%following the settings reported in their paper to ensure a fair comparison. 
%Additional implementation details are provided in the Appendix~\ref{app:Implementation_details}.
\input{latex/content/table/efficiency}
\subsection{Main Results}
\textbf{Image understanding task.}
\noindent The results presented in Table~\ref{tab:main} highlight the exceptional performance of \textbf{\algname}, across a diverse range of image understanding tasks under varying vision token configurations. 
% Compared to existing methods such as SparseVLM~\citep{zhang2024sparsevlm}, and MustDrop~\citep{liu2024multi}, \algname consistently achieves superior results. 
We can observe that \textbf{(i)} with only $192$ tokens retained (\(\downarrow 66.7\%\)), \algname achieves an impressive average performance of $98.8\%$, substantially outperforming the second-best method, MustDrop by $\mathbf{1.6\%}$. 
% This trend continues under more aggressive reductions, with DART maintaining its dominance at $128$ tokens (\(\downarrow 77.8\%\)) and $64$ tokens (\(\downarrow 88.9\%\)), achieving average scores of $97.7\%$ and $93.9\%$, respectively. 
\textbf{(ii)} This trend becomes even more pronounced under more aggressive reduction ratios, with \algname outperforming by $\mathbf{2.2\%}$ while retaining only $64$ tokens.
\textbf{(iii)} Moreover, \algname scales seamlessly to more advanced models, as evidenced by its performance on the LLaVA-Next-7B backbone, where it achieves an average score of $\mathbf{93.9\%}$ with only $11.1\%$ of visual tokens retained, outperforming all competing methods by a significant margin. 
These results highlight \algname's exceptional efficiency in leveraging limited visual tokens while preserving critical information, showcasing its robust performance across diverse tasks and seamless adaptability to various model architectures.
\\
% \textbf{Video understanding task.}
% \noindent To evaluate the effectiveness of \algname in video understanding tasks, we integrate it with Video-LLaVA~\citep{lin2023video} and compare its performance against state-of-the-art methods, including FastV~\citep{chen2024image}. Video-LLaVA processes video data by sampling $8$ frames and extracting $2048$ vision tokens. 
% We adhere to the setting established in prior research, retaining $50\%$ of the visual tokens.
% As shown in Table~\ref{tab:main_table_video}, \algname consistently outperforms FastV across all benchmarks. On MSVD, it achieves a higher score of $4.0$, while also delivering strong results on TGIF ($46.3\%$ accuracy) and MSRVT ($56.7\%$ accuracy). Overall, \algname achieves better average accuracy ($58.0\%$) and evaluation score ($3.7$), showcasing its ability to retain critical visual information with fewer tokens and effectively reason over complex multimodal inputs.
\textbf{Video Understanding Task.}  
To assess \algname's capabilities in video understanding, we integrate it with Video-LLaVA~\citep{lin2023video} and benchmark it against state-of-the-art methods, including FastV~\citep{chen2024image}. Following established protocols, Video-LLaVA processes videos by sampling $8$ frames and extracting $2048$ vision tokens, with $50\%$ retained for evaluation. As demonstrated in Table~\ref{tab:main_table_video}, \algname surpasses FastV across all benchmarks, achieving a notable $4.0$ score on MSVD, $46.3\%$ accuracy on TGIF, and $56.7\%$ accuracy on MSRVT. With an average accuracy of $58.0\%$ and an evaluation score of $3.7$, \algname 
%not only excels in preserving essential visual information under token constraints but also 
demonstrates superior reasoning over complex multimodal data.%, solidifying its position as a robust and efficient solution for video understanding tasks.

% \input{latex/content/table/image_understanding_qwen2vl}

% \input{latex/content/table/image_understanding_minicpm}

\input{latex/content/table/qwen_minicpn_same_column}

\input{latex/content/table/video_understanding}






