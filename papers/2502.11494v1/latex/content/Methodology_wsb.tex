\vspace{-1mm}
\section{Methodology}
\vspace{-1mm}
\subsection{Preliminary}
\label{sec:preliminary}
\vspace{-2mm}
\mypara{Architecture of MLLM.} The architecture of Multimodal Large Language Models (MLLMs) typically comprises three core components: a visual encoder, a modality projector, and a language model (LLM). Given an image $I$, the visual encoder and a subsequent learnable MLP are used to encode $I$ into a set of visual tokens $e_v$. These visual tokens $e_v$ are then concatenated with text tokens $e_t$ encoded from text prompt $p_t$, forming the input for the LLM. The LLM decodes the output tokens $y$ sequentially, which can be formulated as:
%\begin{equation}
%\label{eq1}
    $y_i = f(I, p_t, y_0, y_1, \cdots, y_{i-1}).$
%\end{equation}

%\mypara{Computational Complexity.}  
%To evaluate the computational complexity of MLLMs, it is essential to analyze their core components, including the self-attention mechanism and the feed-forward network (FFN). The total floating-point operations (FLOPs) required can be expressed as:  
%\begin{equation}
%\text{Total FLOPs} = T \times (4nd^2 + 2n^2d + 2ndm),
%\end{equation}  
%where $T$ denotes the number of transformer layers, $n$ is the sequence length, $d$ represents the hidden dimension size, and $m$ is the intermediate size of the FFN.  
%This equation highlights the significant impact of sequence length $n$ on computational complexity. In typical MLLM tasks, the sequence length is defined as: 
%\begin{equation}
%    n = n_S + n_I + n_Q, 
%\end{equation}
%where $n_I$, the tokenized image representation, often dominates, sometimes exceeding other components by an order of magnitude or more.  
%As a result, minimizing $n_I$ becomes a critical strategy for enhancing the efficiency of MLLMs.

\vspace{-1mm}
\subsection{Beyond Token Importance: Questioning the Status Quo}
Given the computational burden associated with the length of visual tokens in MLLMs, numerous studies have embraced a paradigm that utilizes attention scores to evaluate the significance of visual tokens, thereby facilitating token reduction.
Specifically, in transformer-based MLLMs, each layer performs attention computation as illustrated below:
\begin{equation}
   \text{Attention}(\mathbf{Q}, \mathbf{K}, \mathbf{V}) = \text{softmax}\left(\frac{\mathbf{Q} \cdot \mathbf{K}^\top}{\sqrt{d_k}}\right)\cdot \mathbf{V},
\end{equation}
where $d_k$ is the dimension of $\mathbf{K}$. The result of $\text{Softmax}(\mathbf{Q}\cdot \mathbf{K}^\top/\sqrt{d_k})$ is a square matrix known as the attention map.
Existing methods extract the corresponding attention maps from one or multiple layers and compute the average attention score for each visual token based on these attention maps:
\begin{equation}
    \phi_{\text{attn}}(x_i) = \frac{1}{N} \sum_{j=1}^{N} \text{Attention}(x_i, x_j),
\end{equation}
where $\text{Attention}(x_i, x_j)$ denotes the attention score between token $x_i$ and token $x_j$, $\phi_{\text{attn}}(x_i)$ is regarded as the importance score of the token $x_i$, $N$ represents the number of visual tokens.
Finally, based on the importance score of each token and the predefined reduction ratio, the most important visual tokens are selectively retained:
\begin{equation}
    \mathcal{R} = \{ x_i \mid (\phi_{\text{attn}}(x_i) \geq \tau) \},
\end{equation}
where $\mathcal{R}$ represents the set of retained visual tokens, and $\tau$ is a threshold determined by the predefined reduction ratio.

\noindent{\textbf{Problems:}} Although this paradigm has demonstrated initial success in enhancing the efficiency of MLLMs, it is accompanied by several inherent limitations that are challenging to overcome.

One key limitation is disregarding the dynamic nature of token importance during pruning.
% \textcolor{blue}{
For a token sequence \(\{x_1, \ldots, x_n\}\), importance-based methods compute static token importance via a scoring function \(s_i = \mathcal{F}(x_i | X)\), where \(X\) is the full token set. The strategy retains Top-\(k\) tokens:
\begin{equation}
    X_{\text{pruned}} = \arg \max_{X' \subseteq X, |X'| = k} \sum_{x_j \in X'} s_j
\end{equation}
% which implies an \textbf{independence assumption}: the scoring function \( s_j \) remains constant for any subset \( X' \subset X \). This clearly contradicts the dynamic nature of token interactions. Specifically, when two semantically similar tokens \( x_p, x_q \) satisfy \( s_p \approx s_q \), if \( x_q \) is removed, the importance of the remaining token should be recalibrated as:
This implies an \textbf{independence assumption}: the score \( s_j \) remains unchanged for any subset \( X' \subset X \), ignoring dynamic token interactions. For example, if two similar tokens \( x_p, x_q \) have \( s_p \approx s_q \), removing \( x_q \) should recalibrate \( s_p \) as:
\begin{equation}
    s_p' = \mathcal{F}(x_p | X' \setminus \{x_q\}) > s_p,
\end{equation}
which leads to a bias in importance estimation \(\Delta = s_p' - s_p\). This contradiction between static scoring and dynamic interaction can be quantified as:
\begin{equation}
    \mathbb{E}_{X' \subset X} \left[ \sum_{x_i \in X'} \left( \mathcal{F}(x_i | X') - \mathcal{F}(x_i | X) \right) \right]
\end{equation}
%}
% First, when it comes to leveraging attention scores to derive token importance, it usually lacks full compatibility with Flash Attention, resulting in limited hardware acceleration affinity and diminished acceleration benefits.

% Second, \emph{does the paradigm of using attention scores to evaluate token importance truly ensure the effective retention of crucial visual tokens?} Our empirical investigations reveal that it is not the optimal approach.

% Performance evaluations on certain benchmarks, as illustrated in Figure~\ref{fig:random_vs_others}, demonstrate that methods meticulously designed based on this paradigm sometimes underperform compared to randomly retaining the same number of visual tokens.
\vspace{-2mm}
Additionally, Figure~\ref{fig:teaser_curry} visualizes the results of token reduction, revealing that selecting visual tokens based on attention scores introduces a noticeable bias toward tokens in the lower-right region of the image, those appearing later in the visual token sequence. However, this region is not always the most significant in every image. Further, we present the outputs of various methods. Notably, FastV generates more hallucinations than the vanilla model, while \algname effectively reduces them. 
We attribute this to the inherent bias of attention-based methods, which tend to retain tokens concentrated in specific regions, often neglecting the broader context of the image. In contrast, \algname removes highly duplication tokens and preserves a more balanced distribution across the image, enabling more accurate and consistent outputs.

% \textcolor{blue}{
Furthermore, methods relying on attention scores for token importance are incompatible with Flash Attention, compromising speed, and sometimes even underperforming random token reduction in effectiveness (See Fig.~\ref{fig:random_vs_others}).
%}

\input{latex/figure/latency_vs_performance}
\subsection{Token Duplication: Rethinking Reduction}
Given the numerous drawbacks associated with the paradigm of using attention scores to evaluate token importance for token reduction, \textit{what additional factors should we consider beyond token importance in the process of token reduction?}
Inspired by the intuitive ideas mentioned in \secref{sec:introduction} and the phenomenon of tokens in transformers tending toward uniformity (\emph{i.e.}, over-smoothing)~\citep{nguyen2023mitigating, gong2021vision}, we propose that token duplication should be a critical focus.

Due to the prohibitively high computational cost of directly measuring duplication among all tokens, we adopt a paradigm that involves selecting a minimal number of pivot tokens.
% \textcolor{red}{
\begin{definition}[Pivot Tokens]
Let $ \mathcal{P} = \{p_1, p_2, \dots, p_k\} \subseteq \mathcal{X} $ denote the  pivot tokens, where $ k \ll n $ and $ n $ is the total length of the tokens $ \mathcal{X} = \{x_1, x_2, \dots, x_n\}$. The pivot tokens $\mathcal{P}$ are a subset of $ \mathcal{X} $, selected for their representativeness of the entire set.
\end{definition}
% }
% where $p_i$ denotes pivot token, $\mathcal{P}$ represents the set of pivot tokens and $n$ means the length of tokens.

% Subsequently, we compute the cosine similarity between these pivot tokens and the remaining visual tokens:
% \textcolor{red}{
Given the pivot tokens, we can define the duplication score based on it.
\begin{definition}[$\epsilon$-duplicate Score]\label{def:dup}
The token duplication score between a pivot token $ p_i $ and a visual token $ x_j $ is defined as: %the cosine similarity between the two tokens:
\begin{equation}
    \text{dup}(p_i, x_j) = \frac{p_i^\top x_j}{\| p_i \|  \| x_j \|},
\end{equation}
where $ \| \cdot \| $ denotes the Euclidean norm. Two tokens $ p_i, x_j $ are \textbf{$ \epsilon $-duplicates} if
\begin{equation}
    \text{dup}(p_i, x_j)  > \epsilon.
\end{equation}
% 1 - \epsilon -> \epsilon
\end{definition}
% With the $\epsilon$-duplicate score, the set of retained tokens $\mathcal{R}$  is obtained. $\mathcal{R}$ consists of tokens $ x_j $ that exhibit low duplication with any pivot token $ p_i $. Formally:
% \begin{equation}
% \mathcal{R} = \mathcal{P} \cup \left\{ x_j \mid \min_{p_i \in \mathcal{P}} \text{dup}(p_i, x_j) \leq \epsilon \right\},
% \end{equation}
% ---------- todo: more accurate
With the $\epsilon$-duplicate score, for each pivot \( p_i \), the associated retained token set is defined as:
\begin{equation}
    \mathcal{R}_i = \{ x_j \mid dup(p_i, x_j) \leq \epsilon \}
\end{equation}
The final retained set is:
\begin{equation}
    \mathcal{R} = \mathcal{P} \cup \left( \bigcup_{p_i \in \mathcal{P}} \mathcal{R}_i \right)
\end{equation}
where \( \epsilon \) is the threshold dynamically determined for each pivot \( p_i \) based on reduction ratio.
% where $ \epsilon $ is a threshold determined by the reduction ratio. 
This ensures that only tokens that are sufficiently different from the pivot tokens are kept.
% --------- todo end

% }
% \begin{equation}
%     \mathcal{R} = \{ x_j \mid \min_{p_i \in \mathcal{P}} dup (p_i, x_j) \leq \epsilon \}.
% \end{equation}
% Here, $\mathcal{R}$ denotes the set of retained tokens, and $\epsilon$ is a threshold determined by the reduction ratio.

% \vspace{-4mm}
Our method is orthogonal to the paradigm of using attention scores to measure token importance, meaning it is compatible with existing approaches. Specifically, we can leverage attention scores to select pivot tokens, and subsequently incorporate token duplication into the process.

However, this still does not fully achieve compatibility with Flash Attention. Therefore, we explored alternative strategies for selecting pivot tokens, such as using K-norm, V-norm\footnote{Here, the K-norm and V-norm refer to the L1-norm of K matrix and V matrix in attention computing, respectively.}, or even random selection. Surprisingly, all these strategies achieve competitive performance across multiple benchmarks. This indicates that our token reduction paradigm based on token duplication is not highly sensitive to the choice of pivot tokens. Moreover, it suggests that removing duplicate tokens may be more critical than identifying ``important tokens'', highlighting token duplication as a more significant factor in token reduction.
Detailed discussion on pivot token selection is provided in \secref{pivot_token_selection}.

\subsection{Theoretical Analysis}
To further justify trustworthiness of our proposed method, we provide a theoretical analysis of it.
% \textcolor{red}{
\begin{assumption}[Transformer Property]\label{assump:transformer} For transformer property, we assume the following: \\
\noindent \textbf{(A1)}. (Lipschitz continuity under Hausdorff distance). The model $f$ is Lipschitz continuous with respect to the Hausdorff distance between token sets. Formally, there exists $K > 0$ such that for any two token sets $\mathcal{X}_1, \mathcal{X}_2 \subseteq \mathbb{R}^d$:
    $$
    \|f(\mathcal{X}_1) - f(\mathcal{X}_2)\| \leq K \cdot d_H(\mathcal{X}_1, \mathcal{X}_2),
    $$
    where $d_H(\mathcal{X}_1, \mathcal{X}_2) \triangleq \max$\\ $\left\{\sup\limits_{x_1 \in \mathcal{X}_1} \inf\limits_{x_2 \in \mathcal{X}_2} \|x_1 - x_2\|, \sup\limits_{x_2 \in \mathcal{X}_2} \inf\limits_{x_1 \in \mathcal{X}_1} \|x_1 - x_2\|\right\}$.
\noindent \textbf{(A2)}. (Bounded embedding). All tokens have bounded Euclidean norms:
    $$
    \|x\| \leq B, \quad \forall x \in \mathcal{X},
    $$
    where $B > 0$ is a constant.
\end{assumption}
% }
% \vspace{-7mm}
% \textcolor{red}{
\begin{lemma}[Bounded Distance]\label{lemma:cosine} $\min_{p_i\in \mathcal{P}} |p_i-x_j|\leq (2(1-\epsilon))^{1/2}B,\quad \forall x_j\in \mathcal{X}\setminus \mathcal{R}$.
\end{lemma}
\begin{proof} Using A2 and Definition~\ref{def:dup}, we obtain:
\begin{equation*}
    \begin{aligned}
        & \min_{p_i\in \mathcal{P}} |p_i-x_j|^2  = \min_{p_i\in \mathcal{P}} (|p_i|^2 + |x_j|^2 - 2 p_i^\top x_j) \\
        & \leq \min_{p_i\in \mathcal{P}} (B^2 + B^2 - 2 \epsilon \cdot B \cdot B) \leq 2(1-\epsilon) B^2
    \end{aligned}
\end{equation*}
Therefore, the duplication distance bound is given by:
$\min_{p_i\in \mathcal{P}} |p_i-x_j|^2\leq (2(1-\epsilon))^{1/2}B$
\end{proof}
\vspace{-2mm}
\begin{lemma}[Bounded Approximation Error]\label{lemma:bound}
Under Assumption~\ref{assump:transformer}, the Hausdorff distance between original and retained tokens satisfies:
$$
d_H(\mathcal{X}, \mathcal{R}) \leq \sqrt{2(1-\epsilon)}B.
$$
\end{lemma}
\begin{proof}
For any $x \in \mathcal{X}$:
\begin{itemize}[leftmargin=10pt, topsep=0pt, itemsep=1pt, partopsep=1pt, parsep=1pt]
    \item If $x \in \mathcal{R}$, then $\inf_{r \in \mathcal{R}} \|x - r\| = 0$
    \item If $x \notin \mathcal{R}$, by definition and Lemma~\ref{lemma:cosine} there exists $p_i \in \mathcal{P} \subseteq \mathcal{R}$ with $\|x - p_i\| \leq \sqrt{2(1-\epsilon)}B$
\end{itemize}
Thus:
$$
\sup_{x \in \mathcal{X}} \inf_{r \in \mathcal{R}} \|x - r\| \leq \sqrt{2(1-\epsilon)}B.
$$
Since $\mathcal{R} \subseteq \mathcal{X}$, Hausdorff distance simplifies to:
$d_H(\mathcal{X}, \mathcal{R}) = \sup_{x \in \mathcal{X}} \inf_{r \in \mathcal{R}} \|x - r\| \leq \sqrt{2(1-\epsilon)}B.$
\end{proof}
% }
% \vspace{-6mm}
% \textcolor{red}{
\begin{theorem}[Performance Guarantee]\label{thm:main}
Under Assumptions~\ref{assump:transformer}, the output difference between original and pruned token sets is bounded by:
$$
\|f(\mathcal{X}) - f(\mathcal{R})\| \leq K\sqrt{2(1-\epsilon)}B.
$$
\end{theorem}
\begin{proof}
Direct application of Lipschitz continuity (A1) with Lemma~\ref{lemma:bound}: $\|f(\mathcal{X}) - f(\mathcal{R})\| \leq K \cdot d_H(\mathcal{X}, \mathcal{R}) \leq K\sqrt{2(1-\epsilon)}B.$
\end{proof}
% }





