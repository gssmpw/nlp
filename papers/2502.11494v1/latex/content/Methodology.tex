\section{Methodology}
\subsection{Preliminary}
\label{sec:preliminary}
\mypara{Architecture of MLLM.}
% The MLLM architectures generally consist of three components: a visual encoder, a modality projector, and a LLM. The visual encoder, typically a pre-trained image encoder like CLIP's vision model, converts input images into visual tokens. The projector module aligns these visual tokens with the LLM's word embedding space, enabling the LLM to process visual data effectively. The LLM then integrates the aligned visual and textual information to generate responses.
The architecture of Multimodal Large Language Models (MLLMs) typically comprises three core components: a visual encoder, a modality projector, and a language model (LLM). Given an image $I$, the visual encoder and a subsequent learnable MLP are used to encode $I$ into a set of visual tokens $e_v$. These visual tokens $e_v$ are then concatenated with text tokens $e_t$ encoded from text prompt $p_t$, forming the input for the LLM. The LLM decodes the output tokens $y$ sequentially, which can be formulated as:
\begin{equation}
\label{eq1}
    y_i = f(I, p_t, y_0, y_1, \cdots, y_{i-1}).
\end{equation}

\mypara{Computational Complexity.}  
To evaluate the computational complexity of MLLMs, it is essential to analyze their core components, including the self-attention mechanism and the feed-forward network (FFN). The total floating-point operations (FLOPs) required can be expressed as:  
\begin{equation}
\text{Total FLOPs} = T \times (4nd^2 + 2n^2d + 2ndm),
\end{equation}  
where $T$ denotes the number of transformer layers, $n$ is the sequence length, $d$ represents the hidden dimension size, and $m$ is the intermediate size of the FFN.  
This equation highlights the significant impact of sequence length $n$ on computational complexity. In typical MLLM tasks, the sequence length is defined as: 
\begin{equation}
    n = n_S + n_I + n_Q, 
\end{equation}
where $n_I$, the tokenized image representation, often dominates, sometimes exceeding other components by an order of magnitude or more.  
As a result, minimizing $n_I$ becomes a critical strategy for enhancing the efficiency of MLLMs.

\subsection{Beyond Token Importance: Questioning the Status Quo}
Given the computational burden associated with the length of visual tokens in MLLMs, numerous studies have embraced a paradigm that utilizes attention scores to evaluate the significance of visual tokens, thereby facilitating token reduction.
Specifically, in transformer-based MLLMs, each layer performs attention computation as illustrated below:
\begin{equation}
   \text{Attention}(\mathbf{Q}, \mathbf{K}, \mathbf{V}) = \text{softmax}\left(\frac{\mathbf{Q} \cdot \mathbf{K}^\mathbf{T}}{\sqrt{d_k}}\right)\cdot \mathbf{V},
\end{equation}
where $d_k$ is the dimension of $\mathbf{K}$. The result of $\text{Softmax}(\mathbf{Q}\cdot \mathbf{K}^\mathbf{T}/\sqrt{d_k})$ is a square matrix known as the attention map.
Existing methods extract the corresponding attention maps from one or multiple layers and compute the average attention score for each visual token based on these attention maps:
\begin{equation}
    \phi_{\text{attn}}(x_i) = \frac{1}{N} \sum_{j=1}^{N} \text{Attention}(x_i, x_j),
\end{equation}
where $\text{Attention}(x_i, x_j)$ denotes the attention score between token $x_i$ and token $x_j$, $\phi_{\text{attn}}(x_i)$ is regarded as the importance score of the token $x_i$, $N$ represents the number of visual tokens.
Finally, based on the importance score of each token and the predefined reduction ratio, the most significant tokens are selectively retained:
\begin{equation}
    \mathcal{R} = \{ x_i \mid (\phi_{\text{attn}}(x_i) \geq \tau) \},
\end{equation}
where $\mathcal{R}$ represents the set of retained tokens, and $\tau$ is a threshold determined by the predefined reduction ratio.

\noindent{\textbf{Problems:}} Although this paradigm has demonstrated initial success in enhancing the efficiency of MLLMs, it is accompanied by several inherent limitations that are challenging to overcome.

First, when it comes to leveraging attention scores to derive token importance, it inherently lacks full compatibility with Flash Attention, resulting in limited hardware acceleration affinity and diminished acceleration benefits.

Second, does the paradigm of using attention scores to evaluate token importance truly ensure the effective retention of crucial visual tokens? Our empirical investigations reveal that it is not the optimal approach.

% As illustrated in Figure~\ref{fig:random_vs_others}, performance evaluations on certain benchmarks show that methods meticulously designed based on this paradigm sometimes underperform compared to randomly retaining the same number of visual tokens.
Performance evaluations on certain benchmarks, as illustrated in Figure~\ref{fig:random_vs_others}, demonstrate that methods meticulously designed based on this paradigm sometimes underperform compared to randomly retaining the same number of visual tokens.

% As depicted in Figure~\ref{fig:teaser_curry}, which visualizes the results of token reduction, the selection of visual tokens based on attention scores exhibits a noticeable bias, favoring tokens located in the lower-right region of the image—those positioned later in the visual token sequence. However, it is evident that the lower-right region is not always the most significant in every image.
% Furthermore, in Figure~\ref{fig:teaser_curry}, we present the outputs of the original LLaVA-1.5-7B, FastV, and our proposed \algname. Notably, FastV introduces more hallucinations compared to the vanilla model, while \algname demonstrates a noticeable trend of reducing hallucinations.
% We suppose that this phenomenon arises because the important-based method, which relies on attention scores, tends to retain visual tokens that are concentrated in specific regions of the image due to the inherent bias in attention scores. As a result, relying on only a portion of the image often leads to outputs that are inconsistent with the overall image content. In contrast, \algname primarily removes highly duplication tokens and retains tokens that are more evenly distributed across the entire image, enabling it to make more accurate and consistent judgments.
%--------------- shorter version ---------------------
Figure~\ref{fig:teaser_curry} visualizes the results of token reduction, revealing that selecting visual tokens based on attention scores introduces a noticeable bias toward tokens in the lower-right region of the image—those appearing later in the visual token sequence. However, this region is not always the most significant in every image. Additionally, we present the outputs of the original LLaVA-1.5-7B, FastV, and our proposed \algname. Notably, FastV generates more hallucinations compared to the vanilla model, while \algname effectively reduces them. 
We attribute this to the inherent bias of attention-based methods, which tend to retain tokens concentrated in specific regions, often neglecting the broader context of the image. In contrast, \algname removes highly duplication tokens and preserves a more balanced distribution across the image, enabling more accurate and consistent outputs.

\subsection{Token Duplication: Rethinking Reduction}
Given the numerous drawbacks associated with the paradigm of using attention scores to evaluate token importance for token reduction, \textit{what additional factors should we consider beyond token importance in the process of token reduction?}
Inspired by the intuitive ideas mentioned in \secref{sec:introduction} and the phenomenon of tokens in transformers tending toward uniformity (i.e., over-smoothing)~\citep{nguyen2023mitigating, gong2021vision}, we propose that token duplication should be a critical focus.

Due to the prohibitively high computational cost of directly measuring duplication among all tokens, we adopt a paradigm that involves selecting a minimal number of pivot tokens. 
\begin{equation}
    \mathcal{P} = \{p_1, p_2, \dots, p_k\}, \quad k \ll n,
\end{equation}
where $p_i$ denotes pivot token, $\mathcal{P}$ represents the set of pivot tokens and $n$ means the length of tokens.

Subsequently, we compute the cosine similarity between these pivot tokens and the remaining visual tokens:
\begin{equation}
    dup (p_i, x_j) = \frac{p_i \cdot x_j}{\|p_i\| \cdot \|x_j\|}, \quad p_i \in \mathcal{P}, \, x_j \in \mathcal{X},
\end{equation}
where $dup (p_i, x_j)$ represents the token duplication score between $i$-th pivot token $p_i$ and $j$-th visual token $x_j$,
ultimately retaining those tokens that exhibit the lowest duplication with the pivot tokens.
\begin{equation}
    \mathcal{R} = \{ x_j \mid \min_{p_i \in \mathcal{P}} dup (p_i, x_j) \leq \epsilon \}.
\end{equation}
Here, $\mathcal{R}$ denotes the set of retained tokens, and $\epsilon$ is a threshold determined by the reduction ratio.

Our method is orthogonal to the paradigm of using attention scores to measure token importance, meaning it is compatible with existing approaches. Specifically, we can leverage attention scores to select pivot tokens, and subsequently incorporate token duplication into the process.

However, this approach still does not fully achieve compatibility with Flash Attention. To this end, we explored alternative strategies for selecting pivot tokens, such as using K-norm, V-norm\footnote{Here, the K-norm and V-norm refer to the L1-norm of K matrix and V matrix in attention computing, respectively.}, or even random selection. Surprisingly, we found that all these methods achieve competitive performance across multiple benchmarks. This indicates that our token reduction paradigm based on token duplication is not highly sensitive to the choice of pivot tokens. Furthermore, it suggests that removing duplicate tokens may be more critical than identifying ``important tokens'', highlighting token duplication as a potentially more significant factor to consider in token reduction.
The selection of pivot tokens is discussed in greater detail in \secref{pivot_token_selection}.
% 加个总结
