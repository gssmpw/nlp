\vspace{-2mm}
\section{Analysis and Discussion}\label{sec:discussion}
% 1. pruned layer的影响
% 2. v_norm, k_norm, x_norm的影响
% 3. pivot token 数量影响
% 4. 是否需要language参与
% 5. 在vit上做还是在language backbone上做
% \input{latex/content/table/layer}
\vspace{-2mm}
\subsection{Efficiency Analysis}
% - 继续论证重要性的问题，猜想：存在两个交集很小的子集，evaluation的效果都很好
% - vit or llm
% - language ?
% 提供了一个支持flashattention，不需要直接计算text - vision cross attention score，就可以获取语言信息指导的方法
\vspace{-1mm}
As shown in Table~\ref{tab:efficiency}, we compare the total inference time, prefill time, FLOPs, and KV cache memory of multiple methods. 
\textbf{(i)}  DART achieves a \textbf{2.99$\times$ speedup} in the prefill phase and a \textbf{1.99$\times$ speedup} in the entire inference, while its performance on POPE degrades by less than \textbf{3\%} compared to the vanilla model.  
\textbf{(ii)} Further analysis reveals that \emph{{although the FLOPs reduction of different methods is similar, their actual inference speeds vary significantly.}} For instance, SparseVLM increases FLOPs by only \textbf{2.8\%} compared to DART, but its speedup drops by \textbf{21.6\%}, demonstrating that relying solely on FLOPs to measure acceleration effectiveness lacks practical significance.  
\textbf{(iii)} We further evaluate the performance-latency trade-off of each method based on actual latency. As illustrated in Figure~\ref{fig:latency_vs_performance}, \emph{{when considering the balance between latency and performance, some existing methods even underperform random visual token retention}}. We argue that SparseVLM and MustDrop suffer from significant speed degradation due to their sequential multi-stage token processing.
For FastV, its reliance on biased attention scores for ``important'' token selection results in slightly inferior performance. In contrast, \algname seamlessly integrates Flash Attention and incurs less than \textbf{0.08s} overhead for token reduction, achieving a better trade-off between performance and speed.


\vspace{-2mm}
\subsection{Influence from Selection of Pivot Tokens}\label{pivot_token_selection}
\vspace{-2mm}
% \input{latex/content/table/pivot_token_selection}
% In this section, we aim to delve into whether the selection of pivot tokens in DART significantly impacts its overall effectiveness.
% In Table~\ref{tab:pivot_token_selection}, we investigate the selection of a fixed set of $8$ pivot tokens (comprising $4$ visual tokens and $4$ text tokens) based on various criteria, including the maximum$^\spadesuit$ and minimum$^\heartsuit$ values of attention scores, K-norm, V-norm, and Random.
% As demonstrated, various strategies for selecting pivot tokens achieve an average performance exceeding $94.9\%$ of the vanilla model across multiple benchmarks. Even random selection of pivot tokens results in only a $1.2\%$ decrease in average performance compared to the most effective pivot token selection strategy.
In this section, we investigate whether pivot token selection in \algname significantly affects its performance. 
% Table~\ref{tab:pivot_token_selection} in Appendix~\ref{app:pivot_selection} evaluates a fixed set of $8$ pivot tokens ($4$ visual and $4$ text) based on criteria such as maximum ($\spadesuit$), minimum ($\heartsuit$) attention scores, K-norm, V-norm, and random selection.
Table~\ref{tab:pivot_token_selection} in Appendix~\ref{app:pivot_selection} evaluates pivot tokens based on criteria such as maximum ($\spadesuit$), minimum ($\heartsuit$) attention scores, K-norm, V-norm, and random selection.
Results show that various strategies achieve over $94.9\%$ of the vanilla model's performance across benchmarks. \ul{\emph{Even DART with randomly selected pivot tokens incurs only a $1.2\%$ performance drop compared to the best strategy and outperforms the previous importance-based methods by $2.1\%$.}}
This observation shows the robustness in the selection of pivot tokens in DART, and 
highlights the crucial role of duplication in token reduction, as \ul{\emph{selecting ``important'' pivot tokens based on attention scores is only 0.2\% better than selecting ``unimportant'' ones as pivot tokens}}.

% Furthermore, we explore the sets of visual tokens retained by selecting pivot tokens based on maximum and minimum K-norm on MME benchmark. Remarkably, statistical analysis reveals that the overlap between visual tokens preserved by these two strategies is less than \textbf{50\%} on average. Despite this low overlap, both strategies yield highly effective results, {\emph{indicating the existence of multiple distinct groups of tokens which should not be pruned}}.
% This finding challenges the conventional wisdom of identifying a unique set of critical tokens based on importance scores, as it demonstrates that multiple token sets, even with little overlap, can achieve comparable good performance. 

Furthermore, on the MME benchmark, we analyze the visual tokens retained by selecting pivot tokens based on K-norm$^\spadesuit$ and K-norm$^\heartsuit$. Interestingly, statistical analysis shows that the overlap between tokens preserved by these two strategies is, on average, less than \textbf{50\%}. Despite this low overlap, both strategies achieve highly effective results, {\emph{indicating the existence of multiple distinct groups of tokens which should not be pruned}}. This finding challenges the conventional notion of a single critical token set defined by importance scores, demonstrating that diverse token subsets with minimal overlap can yield comparable performance.

\input{latex/figure/layer_scaling_law}
\vspace{-2mm}
\subsection{Influence from Choice of the Pruned Layer and the Number of Pivot Tokens }\label{sec:Layer_and_num}
\vspace{-1mm}
% In this section, we primarily investigate the impact of the pruned layer and the pivot token number on performance.
% The experiments in Table~\ref{tab:main} were conducted under the configuration where the pruned layer is set to $2$, with $4$ visual tokens and $4$ text tokens selected as pivot tokens. 
% Here, we extend the range of two key parameters on MME and TextVQA: for the pruned layer, we select values from $\{1, 2, 3, 5, 10, 20\}$, and for the pivot token number, we explore settings of $\{1, 2, 4, 10, 15, 20\}$.
% As illustrated in Figure~\ref{fig:sensitivity}, our method achieves solid results across other settings except for cases with extremely shallow layers or an insufficient number of pivot tokens. Moreover, increasing the number of pivot tokens does not demonstrate significant performance gains.
We explore the impact of layer on model performance. As expected, pruning deeper layers yields performance closer to the vanilla model but increases latency, as shown in Figure~\ref{fig:layer_scaling_law}. However, we observe two intriguing findings: \textbf{(i)} Pruning at layers 10, 15, and 20 surprisingly outperforms the vanilla model (Fig.~\ref{fig:pope_layer_scaling_law}), consistent with Fig.~\ref{fig:teaser_curry}, suggesting that removing duplicate tokens may reduce hallucinations in MLLMs on the POPE. \textbf{(ii)} At deeper layers (\emph{e.g.}, 15, 20), the latency-minimizing points correspond to pruning all vision tokens, yet performance drops only by $\mathbf{0.1\%} \mathbf{\sim} \mathbf{1.6\%}$. This highlights a modality imbalance in MLLMs, indicating underutilization of the visual modality.

\input{latex/figure/heatmap}
Furthermore, we delved into the impact of the number of pivot tokens on performance. 
As depicted in Figure~\ref{fig:sensitivity}, choosing either an insufficient or an excessive number of pivot tokens leads to suboptimal outcomes.
When a limited number of pivot tokens (\emph{e.g.}, one or two), the lack of diversity among these tokens may impede their ability to comprehensively represent the entire feature space. In contrast, when an overly large number of pivot tokens, for example, 20 or more, are chosen, the majority of retained visual tokens tend to be pivot tokens. In extreme cases, our approach starts to resemble the importance-based method, where pivot tokens essentially transform into important tokens, overlooking the impact of duplication factors.


\subsection{Influence from Modalities of Pivot Tokens}
% In addition to examining the pruned layer and the number of pivot tokens, 
We further analyze the impact of the source of pivot tokens on the overall performance of \algname, with a particular focus on understanding whether guidance from the language modality is essential for effective token reduction. 
% For example, when the number of pivot tokens is set to $4$, our default configuration selects $4$ tokens distributed evenly across both the visual and text modalities. 
We evaluate the performance implications of selecting pivot tokens exclusively from either the visual or text modality, aiming to quantify the influence of each modality.
As illustrated in Figure~\ref{fig:pivot_tokens}, the absence of pivot tokens from either modality leads to a noticeable decline in performance. 
This demonstrates that information from both modalities contributes to the token reduction process to varying degrees. Moreover, it highlights that we provide an effective method for incorporating textual guidance without the need to explicitly compute cross-modal attention scores while remaining compatible with Flash Attention.
\input{latex/figure/pivot_tokens}

% However, the degradation is significantly more pronounced when pivot tokens from the visual modality are excluded. This observation underscores the pivotal role of visual tokens in guiding the token reduction process, highlighting their critical contribution to the success of \algname.

