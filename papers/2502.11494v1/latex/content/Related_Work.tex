\vspace{-1mm}
\section{Related Work}
\vspace{-1mm}
\subsection{Multimodal Large Language Models}
% Building on the success of large language models (LLMs) \citep{yao2024tree, glm2024chatglm, achiam2023gpt, touvron2023llama, brown2020language}, multimodal large language models (MLLMs) \citep{liu2024improved, li2023blip, zhu2023minigpt, wang2023cogvlm, liu2024visual} extend these capabilities by integrating vision and text processing, achieving remarkable performance in tasks involving images, videos, and multimodal reasoning. However, handling visual data poses computational challenges due to the redundancy and low information density of high-resolution tokens \citep{liang2022evit} and the quadratic scaling of attention mechanisms \citep{vaswani2017attention}.
% For instance, models like LLaVA \citep{liu2023improvedllava} and mini-Gemini-HD \citep{li2024mini} encode high-resolution images into thousands of tokens, while video-based models such as VideoLLaVA \citep{lin2023video} and VideoPoet \citep{kondratyuk2023videopoet} allocate even more tokens to process multiple frames. These challenges highlight the need for more efficient token representations and longer context lengths to enable scalability. Recent advancements, such as Gemini \citep{geminiteam2023gemini} and LWM \citep{liu2024world}, have focused on addressing these issues by optimizing token efficiency and extending the context length, paving the way for more scalable and effective MLLMs.
% ----------------- v2 --------------------------
% Building on the success of large language models (LLMs) \citep{yao2024tree, achiam2023gpt, touvron2023llama}, 
% Multimodal large language models (MLLMs) \citep{liu2024improved, li2023blip, zhu2023minigpt, liu2024visual} extend capabilities by integrating vision and text processing, achieving remarkable performance in tasks involving images, videos, and multimodal reasoning. 
Multimodal large language models (MLLMs) \citep{liu2024improved, li2023blip, zhu2023minigpt, liu2024visual} achieve remarkable performance in tasks involving images, videos, and multimodal reasoning by integrating vision and text processing. 
However, processing visual data poses computational challenges due to the redundancy and low information density of high-resolution tokens \citep{liang2022evit} and the quadratic scaling of attention mechanisms \citep{vaswani2017attention}. 
For instance, models like LLaVA \citep{liu2023improvedllava} and mini-Gemini-HD \citep{li2024mini} encode high-resolution images into thousands of tokens, while video-based models such as VideoLLaVA \citep{lin2023video} and VideoPoet \citep{kondratyuk2023videopoet} require even more tokens to process multiple frames. These challenges highlight the need for efficient token representations and longer context lengths to enable scalability. Recent advancements, such as Gemini \citep{geminiteam2023gemini} and LWM \citep{liu2024world}, address these issues by optimizing token efficiency and extending context length, paving the way for more scalable and effective MLLMs.

\vspace{-1mm}
\subsection{Visual Token Compression}
Visual tokens often exceed text tokens by tens to hundreds of times, with visual signals being more spatially redundant compared to information dense text \citep{marr2010vision}. 
Various methods have been proposed to address this issue. For instance, LLaMA-VID \citep{li2023llama} uses a Q-Former with context tokens, and DeCo \citep{yao2024deco} applies adaptive pooling to downsample visual tokens at the patch level.
However, these approaches require modifying model components and additional training, increasing computational and training costs.
ToMe~\citep{bolya2022tome} reduces tokens without training by adding a token merge module to ViTs~\citep{alexey2020image}, but this disrupts early cross-modal interactions in language models~\citep{xing2024PyramidDrop}. FastV~\citep{chen2024image} selects important visual tokens using attention scores, while SparseVLM~\citep{zhang2024sparsevlm} incorporates text guidance via cross-modal attention. 
However, these methods forgo Flash-Attention~\citep{dao2022flashattention, dao2023flashattention2} and primarily focus on token importance, overlooking the impact of token duplication.
% In our work, we preserve hardware acceleration compatibility, including flash attention, while considering the other key factor duplication for token reduction.
In our work, we maintain hardware acceleration compatibility, including Flash Attention, while focusing on another critical factor, token duplication, for efficient token reduction.
