\section{Introduction}\label{sec:introduction}
\input{latex/figure/teaser_curry}

Multimodal large language models (MLLMs) exhibit remarkable capabilities across a diverse range of multimodal tasks, including image captioning, visual question answering (VQA), video understanding~\citep{wang2024internvideo2}, and multimodal reasoning~\citep{wang2024exploring}. % todo: add reference
\input{latex/figure/random_vs_others.tex} However, such impressive performance is always accompanied by huge computation costs, which are mainly caused by massive vision tokens in the input data, especially for high-resolution images~\citep{li2024mini} and multi-frame video~\citep{tang2023video}, leading to challenges in their applications. 


To solve this problem, abundant recent methods introduce \emph{token pruning} to remove the vision tokens in a training-free manner, which usually first defines the importance score of each token, and then prunes the most unimportant tokens during the inference phrase~\citep{chen2024image, zhang2024sparsevlm, liu2024multi}.  The key to a token pruning method is the definition of the importance of vision tokens, where most existing methods are based on the attention scores between vision-only tokens and vision-language tokens. However, this paper argues that these importance-based methods have several serious problems.

\noindent \textbf{(I) Ignoring interactions between tokens during pruning:} Although the interaction between different tokens is considered in attention scores, however, importance-based methods directly remove the most unimportant tokens, ignoring the truth that the importance of each token should be adjusted when other tokens are pruned or preserved. For instance, for two similar tokens, if one of both is determined to be pruned, then the importance of the other token should be improved and vice versa. Unfortunately, previous importance-based token pruning methods fail to model such interaction.


\noindent \textbf{(II) Incompatibility to efficient attention:} Efficient attention operators such as FlashAttention~\cite{dao2022flashattention} have become the default configure in neural networks, which accelerates attention computation by around 2$\times$ and reduce the memory costs from $O(N^2)$ to $O(N)$. % 确认一下陈述对不对
However, these efficient attention operators make attention scores not accessible during computation, indicating conflicts with most previous importance-based token pruning methods. Disabling FlashAttention for accessing attention scores significantly improves the overall latency and memory footprint.


\noindent \textbf{(III) Bias in token positions:} As claimed by abundant recent works~\citep{endo2024feather, zhang2024cls} and shown in Figure~\ref{fig:teaser_curry}, attention scores have position bias, where the tokens are positionally close to the last token tend to have a higher attention score, making attention score does not truly reveal the value of this token.


\noindent \textbf{(IV) Significant accuracy drop:} Although the aforementioned three problems have reminded us the ineffectiveness of importance-based token pruning, however, it is still extremely surprising to find that \textbf{\emph{{some influential importance-based token pruning methods show inferior accuracy than random token pruning}}}, (\emph{i.e.}, randomly selecting the tokens for pruning), as shown in Figure~\ref{fig:random_vs_others}. 





%These importance-based token pruning methods implicitly assumes that there exists a static and oracle score that can characterize the influence of this token on model prediction. However, in this paper, we argue that this ``importance'' perspective exist several serious problems as follows.

%Despite occupying a significant portion of the input sequence, vision tokens lack the information density like text modality~\citep{marr2010vision}.
%To address this issue, several studies have sought to extract more compact image representations by enhancing the image encoder or projector~\citep{alayrac2022flamingo, li2023llama, instructblip, cha2024honeybee}.
%However, these methods typically necessitate substantial modifications to the MLLM architecture and demand extensive training.
% 与此同时，一些最近的工作尝试用training-free的方法减少vision tokens。
%While some recent works 
%have explored training-free approaches to reduce vision tokens by token pruning, which firstly defines the importance score of each token, and then prunes the most unimportant tokens during the inference phrase~\citep{chen2024image, zhang2024sparsevlm, liu2024multi}. 
%These importance-based token pruning methods implicitly assumes that there exists a static and oracle score that can characterize the influence of this token on model prediction. 
%However, in this paper, we argue that this ``importance'' perspective exist several serious problems as follows.

%\textbf{``Importance'' scores ignore the interaction between different tokens during pruning. For two similar tokens, }

% 独立性假设
% 效果不好
% 不支持flashattention
% 位置bias



%the influence of pruning a token can be considered independently with other tokens. Obviously, this assumption ignores the interaction between multiple tokens. In other words, the choice of pruning a token should not only judged by itself, but also influenced by the pruning choice of other tokens, especially influenced by the tokens which are similar to this token.



%Due to their reliance on attention scores for assessing the importance of vision tokens, these methods struggle to integrate smoothly with Flash Attention~\citep{dao2022flashattention, dao2023flashattention2}, significantly limiting their acceleration benefits.
% Additionally, these approaches place excessive emphasis on the importance of tokens, overlooking the crucial role that token duplication plays in token reduction.

% our method
% 在这篇工作中，我们试图回答三个问题：
% 1. 能否不利用attention score挑选tokens,做到兼容flash attention？
% 2. 通过衡量token重要性进行token reduction是最优的？
% 3. 除了token重要性，在token reduction中还需要关注什么？
% 加一句:为了解决这一问题，我们尝试设计能够与flash attention兼容的token reduction方法。
%To address this issue, we aim to develop token reduction methods that are compatible with flash attention.
%Before that, we delve into two intriguing research questions:
%\textbf{Q1:} \textit{Is it optimal for the attention score to measure token importance to guide the token reduction paradigm?}
%\textbf{Q2:} \textit{Beyond token importance, what factors should be considered in token reduction?}
% \\
%\textbf{Is it optimal for the attention score to measure token importance to guide the token reduction paradigm?}
 % TODO: refine this sentence.
%We conducted a comparative analysis of token reduction performance across random token selection, FastV~\citep{chen2024image}, and SparseVLM~\citep{zhang2024sparsevlm}. As illustrated in Figure~\ref{fig:random_vs_others}, random token selection even outperforms both FastV and SparseVLM on certain benchmarks. This suggests that relying on attention scores to measure token importance for reduction may not be optimal. Instead, it is crucial to explore factors beyond token importance to address the inherent limitations of attention scores in flash attention.
% 加个柱状图来说明random token selection有时候好于依据attent score精心设计的fastv & sparsevlm
% 根据图讲解下，得到结论：在token reduction中考虑token importance不是最优方案。
%\\
%\textbf{Beyond token importance, what factors should be considered in token reduction?}
% fastv是典型的利用attention score衡量token重要性进行token reduction的方法->画图
 %1. 首先在同一张图像上将fastv保留image token对应的patch可视化出来 vs 我们利用k-norm + token重复性考虑的patch 可视化
 %2. 然后在特征空间进行tsne可视化的对比，展示出我们保留的特征没有太聚集，比较分散
% 顺势引出考虑token重复性在token reduction中的必要性。

The above observations demonstrates the disadvantages of importance-based token pruning methods, while also introducing the expectation for the ideal alternative: The expected method should consider both the individual value of a token and its interaction to other tokens. It should be cheap in computation and friendly to hardware, and shows no bias in the positions of tokens.


%Previous research has extensively investigated the over-smoothing phenomenon in transformers~\citep{ nguyen2023mitigating, wang2022anti}, with studies demonstrating that self-attention mechanism inherently drives token representations toward increasingly identical latent states~\citep{gong2021vision, wang2023label}.

These insights inspire us to incorporate token duplication into the token reduction. Intuitively, when multiple tokens exhibit identical or highly similar representations, it is natural to retain only one of them for the following computation, thereby maintaining efficiency without harming accuracy. Building upon this idea, we introduce a simple but effective token pruning pipeline referred to as \textbf{DART} (\textbf{D}uplication-\textbf{A}ware
\textbf{R}eduction of \textbf{T}okens) with the following two steps.
% \input{latex/figure/radar_compare.tex}
\input{latex/figure/overview}

%In this work, building upon the insights from the aforementioned discussions, we propose a plug-and-play token reduction method grounded in token duplication without training, which is designed to be fully compatible with flash attention. 
% Specifically, during the forward inference process of the MLLMs, at the shallow layers, we first select a small subset of pivot tokens (no more than 2\% of the total tokens) based on their high $\mathrm{K}_{\mathrm{norm}}$ values. We then compute the cosine similarity between these selected tokens and the remaining image tokens. Based on the predefined reduction ratio, we retain the image tokens with the lowest cosine similarity, as we hypothesize that image tokens exhibiting high feature redundancy with the selected tokens contribute minimally to subsequent predictions.
% Specifically, during the inference process of MLLMs, we begin by selecting a small subset of pivot tokens—comprising no more than $2\%$ of the total tokens—based on their elevated K-norm values in the shallow layers. 
Firstly, we begin by selecting a small subset of tokens as pivot tokens, which comprise no more than $2\%$ of the total tokens. Such pivot tokens can be selected based on the norm of tokens or even randomly selected, which does not introduce notable computations.
Secondly, we then calculate the cosine similarity between pivot tokens and the remaining image tokens. Since the pivot tokens are fewer than $2\%$, such computation is efficient in both computing and memory.
With a desired token reduction ratio, we retain only those vision tokens with the lowest cosine similarity to pivot tokens and remove the similar ones.
The entire process is simple and highly efficient, completing in no more than \textbf{0.08} seconds, friendly to efficient attention operators, and leading to significantly higher accuracy than previous methods.
%This approach is motivated by the prior insights~\citep{gong2021vision} that image tokens exhibiting high feature duplication with the pivot tokens contribute negligibly to predictions. Moreover, since our entire process operates independently of attention scores, it is fully compatible with flash attention, ensuring seamless integration and enhanced computational efficiency.

In summary, our contributions  are three-fold:
\begin{itemize}[leftmargin=10pt, topsep=0pt, itemsep=1pt, partopsep=1pt, parsep=1pt]
    \item \textbf{Rethink Token Importance.} Through empirical analysis, we demonstrate the suboptimality of relying on attention scores to measure token importance to guide the token reduction paradigm.
    \item \textbf{Token Duplication as a Key Factor.} Building on token duplication, we introduce a training-free, plug-and-play token reduction method that seamlessly integrates with Flash Attention.
    \item \textbf{Superior Performance with Extreme Compression.} Extensive experiments across four diverse MLLMs and over 10 benchmarks demonstrate the clear superiority of \algname. For instance, our method outperforms the second-best method by 2.2\% ($93.7\% \text{ vs. } 91.5\%$) on LLaVA-1.5-7B with an 88.9\% reduction ratio.
    % For instance, our method outperforms 3.8\% (93.9\%) compared with the second-best Mustdrop on LLaVA-1.5-7B with 88.9\% reduction ratio.
\end{itemize}





