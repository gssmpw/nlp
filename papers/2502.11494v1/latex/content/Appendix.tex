\appendix
% \label{sec:appendix}
\section{Additional Experiments}
\input{latex/content/table/pivot_token_selection}
\subsection{Supplementary Results on Pivot Token Selection}\label{app:pivot_selection}
This section presents comprehensive experimental results conducted on the LLaVA-1.5-7B model, supporting the analysis of pivot token selection strategies within \algname. Table~\ref{tab:pivot_token_selection} details performance metrics across multiple benchmarks, including GQA, MMB, MME, POPE, SQA, and VQA, with all experiments retaining 128 vision tokens. These findings further validate the robustness of \algname under various pivot token selection criteria, ranging from random selection to methods based on attention scores and norm-based approaches. The table also includes comparisons with baseline methods (\emph{e.g.}, SparseVLM and FastV), highlighting the consistent superiority of \algname across different configurations. For additional insights, refer to the main discussion in \secref{pivot_token_selection}.
% \input{latex/content/table/OCRBench_details}

% \input{latex/content/table/appendix_qwen2vl}

\section{Detailed Experiment Settings}\label{app:detailed_experiment_settings}
\subsection{Datasets}
\label{app:dataset}
Our experiments are conducted on a suite of widely recognized benchmarks, each designed to evaluate distinct aspects of multimodal intelligence.
For image understanding task, we performed experiments on ten widely used benchmarks, including GQA \citep{hudson2019gqa}, MMBench (MMB) and MMB-CN \citep{liu2025mmbench}, MME \citep{fu2023mme}, POPE~\citep{li2023evaluating}, VizWiz \citep{bigham2010vizwiz}, SQA \citep{lu2022learn}, VQA$^{\text{V2}}$ (VQA V2) \citep{goyal2017making}, VQA$^{\text{Text}}$ (TextVQA) \citep{singh2019towards}, and OCRBench ~\citep{liu2024ocrbench}.
For video understanding task, we evaluated our method on three video-based benchmarks: TGIF-QA \citep{jang2017tgif}, MSVD-QA \citep{xu2017video}, and MSRVTT-QA \citep{xu2017video}.

\textbf{GQA.} GQA is structured around three core components: scene graphs, questions, and images. It includes not only the images themselves but also detailed spatial features and object-level attributes. The questions are crafted to assess a model's ability to comprehend visual scenes and perform reasoning tasks based on the image content.

\textbf{MMBench.} MMBench offers a hierarchical evaluation framework, categorizing model capabilities into three levels. The first level (L-1) focuses on perception and reasoning. The second level (L-2) expands this to six sub-abilities, while the third level (L-3) further refines these into 20 specific dimensions. This structured approach allows for a nuanced and comprehensive assessment of a model's multifaceted abilities. MMBench-CN is the Chinese version of the dataset.

\textbf{MME.} The MME benchmark is designed to rigorously evaluate a model's perceptual and cognitive abilities through 14 subtasks. It employs carefully constructed instruction-answer pairs and concise instructions to minimize data leakage and ensure fair evaluation. This setup provides a robust measure of a model's performance across various tasks.

\textbf{POPE.} POPE is tailored to assess object hallucination in models. It presents a series of binary questions about the presence of objects in images, using accuracy, recall, precision, and F1 score as metrics. This approach offers a precise evaluation of hallucination levels under different sampling strategies.

\textbf{ScienceQA.} ScienceQA spans a wide array of domains, including natural, language, and social sciences. Questions are hierarchically categorized into 26 topics, 127 categories, and 379 skills, providing a diverse and comprehensive testbed for evaluating multimodal understanding, multi-step reasoning, and interpretability.

\textbf{VQA V2.} VQA V2 challenges models with open-ended questions based on 265,016 images depicting a variety of real-world scenes. Each question is accompanied by 10 human-annotated answers, enabling a thorough assessment of a model's ability to accurately interpret and respond to visual queries.

\textbf{TextVQA.} TextVQA emphasizes the integration of textual information within images. It evaluates a model's proficiency in reading and reasoning about text embedded in visual content, requiring both visual and textual comprehension to answer questions accurately.

\textbf{OCRBench.} OCRBench is a comprehensive benchmark for evaluating the OCR capabilities of multi-modal language models across five key tasks: text recognition, scene text-centric and document-oriented VQA, key information extraction, and handwritten mathematical expression recognition.

\textbf{TGIF-QA.} TGIF-QA extends the image question-answering task to videos, featuring 165,000 question-answer pairs. It introduces tasks that require spatio-temporal reasoning, such as repetition count and state transition, as well as frame-based questions, promoting advancements in video question answering.

\textbf{MSVD-QA.} Based on the MSVD dataset, MSVD-QA includes 1970 video clips and approximately 50.5K QA pairs. The questions cover a broad spectrum of topics and are open-ended, categorized into what, who, how, when, and where types, making it a versatile tool for video understanding tasks.

\textbf{MSRVTT-QA.} MSRVTT-QA comprises 10K video clips and 243K QA pairs. It addresses the challenge of integrating visual and temporal information in videos, requiring models to effectively process both to answer questions accurately. Similar to MSVD-QA, it includes five types of questions, further enriching the evaluation landscape.

\subsection{Models}\label{app:models}
We evaluate \algname using various open-source MLLMs. For image understanding tasks, experiments are conducted on the LLaVA family, including LLaVA-1.5-7B\footnote{\url{https://huggingface.co/liuhaotian/llava-v1.5-7b}}~\citep{liu2024visual} and LLaVA-Next-7B\footnote{\url{https://huggingface.co/liuhaotian/llava-v1.6-vicuna-7b}}~\citep{liu2024llavanext}, with the latter used to validate performance on high-resolution images.
% Additionally, we evaluated Qwen2-VL-7B\footnote{\url{https://huggingface.co/Qwen/Qwen2-VL-7B-Instruct}}~\citep{wang2024qwen2}, which incorporates dynamic resolution and token merging techniques.
Furthermore, we validate our method on more advanced models, including Qwen2-VL-7B\footnote{\url{https://huggingface.co/Qwen/Qwen2-VL-7B-Instruct}}~\citep{wang2024qwen2} and MiniCPM-V-2.6\footnote{\url{https://huggingface.co/openbmb/MiniCPM-V-2_6}}~\citep{yao2024minicpm}.
For video understanding tasks, we use Video-LLaVA~\citep{lin2023video} as the baseline model.
following the settings reported in their paper to ensure a fair comparison.

% \subsection{Baselines}  
% We analyze multiple representative methods for accelerating multi-modal language models through token reduction. \textbf{ToMe}~\citep{bolya2022tome} merges similar tokens in visual transformer layers via lightweight matching without training. \\
% \textbf{FastV}~\citep{chen2024image} leverages attention map in early layers to prune visual tokens. \\
% \textbf{SparseVLM}~\citep{zhang2024sparsevlm} leverages text-visual attention to rank token importance and employs adaptive sparsity ratios with token recycling. \\
% \textbf{HiRED} allocates token budgets for image partitions by \texttt{CLS} token attention and selects informative tokens within each partition.  \\
% \textbf{LLaVA-PruMerge} dynamically prunes tokens based on sparse \texttt{CLS}-visual attention and merges retained tokens via key similarity clustering. \\
% \textbf{PDrop}~\citep{xing2024PyramidDrop} progressively drops tokens across model stages, forming pyramid-like token structures. \\
% \textbf{MustDrop}~\citep{liu2024multi} integrates spatial merging, text-guided pruning, and output-aware cache policies across multiple stages. \\
% \textbf{FasterVLM}~\citep{zhang2024cls} evaluates token importance via \texttt{CLS}-visual attention in the encoder and prunes tokens before LLM interaction. \\
% \textbf{GlobalCom$^2$}~\citep{liu2025compression} coordinates thumbnail tokens to allocate retention ratios for high-resolution crops while preserving local details.

% These approaches span merging, pruning, hierarchical compression, and multi-stage optimization paradigms.

\subsection{Baselines}
We analyze multiple representative methods for accelerating multi-modal language models (MLLMs) through token reduction. These methods share the goal of improving efficiency by reducing redundant tokens, yet differ in their strategies, such as token merging, pruning, or adaptive allocation.

\textbf{ToMe}~\citep{bolya2022tome} merges similar tokens in visual transformer layers through lightweight matching techniques, achieving acceleration without requiring additional training.

\textbf{FastV}~\citep{chen2024image} focuses on early-stage token pruning by leveraging attention maps, effectively reducing computational overhead in the initial layers.

\textbf{SparseVLM}~\citep{zhang2024sparsevlm} ranks token importance using cross-modal attention and introduces adaptive sparsity ratios, complemented by a novel token recycling mechanism.

\textbf{HiRED}~\citep{arif2024hired} allocates token budgets across image partitions based on \texttt{CLS} token attention, followed by the selection of the most informative tokens within each partition, ensuring spatially aware token reduction.

\textbf{LLaVA-PruMerge}~\citep{shang2024llava} combines pruning and merging strategies by dynamically removing less important tokens using sparse \texttt{CLS}-visual attention and clustering retained tokens based on key similarity.

\textbf{PDrop}~\citep{xing2024PyramidDrop} adopts a progressive token-dropping strategy across model stages, forming a pyramid-like token structure that balances efficiency and performance.

\textbf{MustDrop}~\citep{liu2024multi} integrates multiple strategies, including spatial merging, text-guided pruning, and output-aware cache policies, to reduce tokens across various stages.

\textbf{FasterVLM}~\citep{zhang2024cls} evaluates token importance via \texttt{CLS} attention in the encoder and performs pruning before interaction with the language model, streamlining the overall process.

\textbf{GlobalCom$^2$}~\citep{liu2025compression} introduces a hierarchical approach by coordinating thumbnail tokens to allocate retention ratios for high-resolution crops while preserving local details.

These methods collectively highlight diverse approaches to token reduction, ranging from attention-based pruning to adaptive merging, offering complementary solutions for accelerating MLLMs.



\subsection{Implementation Details}\label{app:Implementation_details}
All of our experiments are conducted on Nvidia A100-80G GPU. The implementation was carried out in Python 3.10, utilizing PyTorch 2.1.2, and CUDA 11.8. All baseline settings follow the original paper. \\
% \textbf{Baseline.}


\section{Computational Complexity.}
To evaluate the computational complexity of MLLMs, it is essential to analyze their core components, including the self-attention mechanism and the feed-forward network (FFN). The total floating-point operations (FLOPs) required can be expressed as:  
\begin{equation}
\text{Total FLOPs} = T \times (4nd^2 + 2n^2d + 2ndm),
\end{equation}  
where $T$ denotes the number of transformer layers, $n$ is the sequence length, $d$ represents the hidden dimension size, and $m$ is the intermediate size of the FFN.  
This equation highlights the significant impact of sequence length $n$ on computational complexity. 
Notable, we follow FastV~\cite{chen2024image} to roughly estimate various token reduction baseline FLOPs.
The FLOPs after token pruning can be represented as:
\begin{equation}
\begin{split}
    \text{Post-Pruning FLOPs} \\
    &\hspace{-7em}= L\times(4nd^2+2n^2d+2ndm) +{} \\
    &\hspace{-5em} (T-L) \times (4\hat{n}d^2+2\hat{n}^2d+2\hat{n}dm),
\end{split}
\end{equation}
where $L$ denotes the pruned layer, $\hat{n}$ represents token sequence length after pruning.
The theoretical FLOPs reduction ratio related to visual tokens is computed as:
\begin{equation}
    1-\frac{\text{Post-Pruning FLOPs}}{\text{Total FLOPs}}.
\end{equation}
% In typical MLLM tasks, the sequence length is defined as: 
% \begin{equation}
%    n = n_S + n_I + n_Q, 
% \end{equation}
% where $n_I$, the tokenized image representation, often dominates, sometimes exceeding other components by an order of magnitude or more.  
% As a result, minimizing $n_I$ becomes a critical strategy for enhancing the efficiency of MLLMs.


\section{Future Works}
As can be observed from Figure~\ref{fig:teaser_curry} and Figure~\ref{fig:pope_layer_scaling_law}, in certain cases, token pruning contributes to the reduction of hallucinations. Our method achieved better results than the vanilla model on the POPE benchmark, which is specifically designed for evaluating the hallucination issues of multimodal large language models. Therefore, we believe that it is worth exploring in the future why token pruning is beneficial for reducing hallucinations and how we can better utilize efficient techniques (\emph{e.g.}, token pruning, and token merge) to reduce hallucinations while achieving acceleration benefits.

% In the methodology section, the limitations can be reformulated mathematically as follows:

% \textbf{Formalizing Static Importance Evaluation:}

% Consider an input sequence containing \( n \) tokens \(\{t_1, \ldots, t_n\}\). Traditional importance-based pruning methods compute the static importance of each token using an independent scoring function \( s_i = \mathcal{F}(t_i | T) \), where \( T \) is the complete set of tokens. The pruning strategy retains the Top-\( k \) tokens:
% \begin{equation}
%     T_{\text{pruned}} = \arg \max_{T' \subseteq T, |T'| = k} \sum_{t_j \in T'} s_j
% \end{equation}
% However, this method implies an \textbf{independence assumption}: the scoring function \( s_j \) remains constant for any subset \( T' \subset T \). This clearly contradicts the dynamic nature of token interactions. Specifically, when two semantically similar tokens \( t_p, t_q \) satisfy \( s_p \approx s_q \) if the pruning decision removes \( t_q \), the importance of the remaining token should be recalibrated as:
% \begin{equation}
%     s_p' = \mathcal{F}(t_p | T' \setminus \{t_q\}) > s_p
% \end{equation}
% Traditional methods, however, use the initial score \( s_p \), leading to a bias in importance estimation \(\Delta = s_p' - s_p\). This contradiction between static scoring and dynamic interaction can be quantified as:
% \begin{equation}
%     \mathbb{E}_{T' \subset T} \left[ \sum_{t_i \in T'} \left( \mathcal{F}(t_i | T') - \mathcal{F}(t_i | T) \right) \right]
% \end{equation}
% where $\mathbb{E}$ accommodates the evaluation error threshold. Experiments demonstrate that traditional methods significantly underperform in long text tasks (see experimental section). This mathematical description reveals the fundamental contradiction between the independent scoring assumption and true dynamic interactions.

% Consider an input sequence containing \( n \) tokens \(\{x_1, \ldots, x_n\}\). Traditional importance-based pruning methods compute the static importance of each token using an independent scoring function \( s_i = \mathcal{F}(x_i | X) \), where \( X \) is the complete set of tokens. The pruning strategy retains the Top-\( k \) tokens:
% \begin{equation}
%     X_{\text{pruned}} = \arg \max_{X' \subseteq X, |X'| = k} \sum_{x_j \in X'} s_j
% \end{equation}
% However, this method implies an \textbf{independence assumption}: the scoring function \( s_j \) remains constant for any subset \( X' \subset X \). This clearly contradicts the dynamic nature of token interactions. Specifically, when two semantically similar tokens \( x_p, x_q \) satisfy \( s_p \approx s_q \), if the pruning decision removes \( x_q \), the importance of the remaining token should be recalibrated as:
% \begin{equation}
%     s_p' = \mathcal{F}(x_p | X' \setminus \{x_q\}) > s_p
% \end{equation}
% Traditional methods, however, use the initial score \( s_p \), leading to a bias in importance estimation \(\Delta = s_p' - s_p\). This contradiction between static scoring and dynamic interaction can be quantified as:
% \begin{equation}
%     \mathbb{E}_{X' \subset X} \left[ \sum_{x_i \in X'} \left( \mathcal{F}(x_i | X') - \mathcal{F}(x_i | X) \right) \right]
% \end{equation}

\section{Sparsification Visualization on Different Pivot Token Selection Strategy}
\input{latex/figure/dart_visual_cases}
Figure~\ref{fig:dart_visual_cases} showcases a diverse array of sparsification visualization examples on different pivot token selection strategy, including K-norm$\spadesuit$, K-norm$\heartsuit$, V-norm$\spadesuit$, V-norm$\heartsuit$, Attention Score$\spadesuit$, Attention Score$\heartsuit$, and Random.
Here, we can observe two interesting points: (i) The commonality is that \algname employs different pivot token selection strategies for token reduction, and the retained tokens are distributed in a relatively scattered manner without obvious bias, \emph{i.e.}, spatial uniformity, which contributes to a more accurate understanding of the entire image and consistent responses. (ii) The difference lies in the fact that although each strategy achieves comparable performance, it is noticeable that the final set of retained tokens varies significantly across strategies, indicating the existence of multiple token sets that can deliver satisfactory results. This further corroborates the limitation of selecting a unique set of tokens based solely on importance scores.





