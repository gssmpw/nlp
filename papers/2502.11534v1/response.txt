\section{Related Work}
\subsection{Surgical Tool Pose Estimation and Tracking}
SurgPose is designed for developing and evaluating surgical tool pose estimation and tracking. Template matching is a commonly used method for articulated surgical tool pose estimation and tracking. Low-level visual features are extracted to learn templates by machine learning algorithms**Kurmann, "Simultaneous Tool Recognition and Pose Estimation"**. Reiter et al., **"Virtual Template-Based Articulated Surgical Tool Pose Estimation and Tracking"** proposed a method using the CAD model and kinematics of the tool to generate the virtual template. Ye et al., **"Real-Time Virtual Tool Rendering for Surgical Instrument Tracking"** achieved real-time tracking by introducing an online virtual tool renderer. 
Allan et al., **"Silhouette-Based 3D Pose Estimation for Robotic Surgical Instruments"** used silhouette and optical flow-based features to estimate the 3D pose of the robotic surgical instrument.
However, these low-level feature representations suffer from a lack of semantic information, which causes performance degradation when facing corrupted images and different surgical scenes.
Deep learning-based methods have demonstrated better robustness and generalizability in surgical scene understanding tasks**Kurmann, "Simultaneous Tool Recognition and Pose Estimation"**. Du et al., **"Articulation-Agnostic Multi-Instrument Pose Estimation Using Fully Convolutional Networks"** used a UNet-based architecture for simultaneous tool recognition and pose estimation. Kayhan et al., **"Deep Attention-Based Semi-Supervised Surgical Instrument Pose Estimation"** proposed a deep attention-based semi-supervised method for 2D surgical instrument pose estimation. Li et al., **"Semi-Supervised Multi-Task Learning for Joint Pose Estimation and Segmentation of Surgical Instruments"** proposed a semi-supervised multi-task learning framework for joint pose estimation and segmentation. Lu et al., **"Sim-to-Real Transfer for Robot Keypoint Detection via Domain Randomization"** explored the sim-to-real transfer for robot keypoint detection via domain randomization.

\subsection{Datasets for Surgical Tool Pose Estimation}
There are many datasets and benchmarks for the human hand**Lassner, "Learning Realistic Human Hand Motion"**, whole body**Pavlakos, "Volumetric and Multi-View CNNs for Object Instance Segmentation on Objects and Persons"**, and animal**Rogez, "Automated Animal Pose Estimation in the Wild"** pose estimation. However, the public datasets for articulated surgical tool pose estimation are limited. The Retinal Microsurgery Instrument Tracking (RMIT)**Ye, "Retinal Microsurgery Instrument Tracking Dataset"** dataset consists of three video clips of 480p resolution during \textit{in vivo} retinal microsurgery. A single instrument with 4 annotated joint positions is presented in this dataset. Table~\ref{table_1} provides a comparison between our SurgPose dataset versus other existing datasets that exclusively consider robotic instruments.

The Instrument Segmentation and Tracking challenge at EndoVis15**EndoVis, "Instrument Segmentation and Tracking Challenge"** provides a dataset for articulated surgical tool tracking with only shaft point annotation.  To estimate the articulation of the instrument, Du et al., **"Relabeled EndoVis15 Dataset for Articulated Surgical Tool Pose Estimation"** relabeled the EndoVis15 dataset by manually annotating 1850 frames with 5 keypoints. The SurgRIPE**SurgRIPE, "6D Pose Estimation of Da Vinci Wrist"** challenge at EndoVis23 released a dataset for 6D pose estimation of the wrist of da Vinci surgical instruments. SurgRIPE provides high-quality images captured from the da Vinci Si endoscope with 6D pose annotations obtained from a special keydot pattern. This pattern is removed by inpainting. In EndoVis24, the PhaKIR sub-challenge**PhaKIR, "Multi-Instrument Multi-Joint Keypoints Dataset"** release a new dataset of 13 cholecystectomy videos collected in three hospitals. PhaKIR includes 30K frames with multi-instrument multi-joint keypoints annotations at an interval of one frame per second, but the instruments are conventional instead of robotic. Furthermore, the monocular laparoscopes used in this dataset are uncalibrated. 

\subsection{Fluorescent Markers}
Manually labeling keypoints is time-consuming, cumbersome, and prone to error. To address this, Thananjeyan et al., **"LUV: Labeling Using a UV-Fluorescent Paint for Deformable Objects"** proposed LUV, a labeling methodology using a transparent and UV-fluorescent paint. They show LUV's feasibility for collecting labels of fabric keypoints, surgical thread, and suturing needles. In endoscopy, Schmidt et al. developed STIR**Schmidt, "STIR: A Dataset of Videos of Deformable Surgical Tissue"**, a dataset of videos of deformable surgical tissue. By tattooing the tissue with Infrared (IR) fluorescent dye, indocyanine green (ICG), STIR generates markers that are invisible to visible spectrum algorithms. STIR markers can be detected under the IR spectrum using da Vinci Xi's Firefly mode.