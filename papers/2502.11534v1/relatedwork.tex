\section{Related Work}
\subsection{Surgical Tool Pose Estimation and Tracking}
SurgPose is designed for developing and evaluating surgical tool pose estimation and tracking. Template matching is a commonly used method for articulated surgical tool pose estimation and tracking. Low-level visual features are extracted to learn templates by machine learning algorithms~\cite{reiter2014appearance}. Reiter et al.~\cite{reiter2012articulated} proposed a method using the CAD model and kinematics of the tool to generate the virtual template. Ye et al.~\cite{ye2016real} achieved real-time tracking by introducing an online virtual tool renderer. 
Allan et al.~\cite{allan20183} used silhouette and optical flow-based features to estimate the 3D pose of the robotic surgical instrument.
However, these low-level feature representations suffer from a lack of semantic information, which causes performance degradation when facing corrupted images and different surgical scenes.
Deep learning-based methods have demonstrated better robustness and generalizability in surgical scene understanding tasks~\cite{zhao2019real,wu2024real,schmidt2023sendd}. Kurmann~\cite{kurmann2017simultaneous} used a UNet-based architecture for simultaneous tool recognition and pose estimation. Du et al.~\cite{du2018articulated} used Fully Convolutional Networks (FCN) for articulation-agnostic multi-instrument pose estimation and evaluated it on different datasets. Kayhan et al.~\cite{kayhan2021deep} proposed a deep attention-based semi-supervised method for 2D surgical instrument pose estimation. Li et al.~\cite{li2022multi} proposed a semi-supervised multi-task learning framework for joint pose estimation and segmentation. Lu et al.~\cite{9714837} explored the sim-to-real transfer for robot keypoint detection via domain randomization.

\subsection{Datasets for Surgical Tool Pose Estimation}
There are many datasets and benchmarks for the human hand~\cite{hampali2022keypoint}, whole body~\cite{andriluka2018posetrack}, and animal~\cite{yang2022apt} pose estimation. However, the public datasets for articulated surgical tool pose estimation are limited. The Retinal Microsurgery Instrument Tracking (RMIT)~\cite{sznitman2012data} dataset consists of three video clips of 480p resolution during \textit{in vivo} retinal microsurgery. A single instrument with 4 annotated joint positions is presented in this dataset. Table~\ref{table_1} provides a comparison between our SurgPose dataset versus other existing datasets that exclusively consider robotic instruments.

The Instrument Segmentation and Tracking challenge at EndoVis15~\cite{bodenstedt2018comparative} provides a dataset for articulated surgical tool tracking with only shaft point annotation.  To estimate the articulation of the instrument, Du et al.~\cite{du2018articulated} relabeled the EndoVis15 dataset by manually annotating 1850 frames with 5 keypoints. The SurgRIPE~\cite{surgripe2024xu} challenge at EndoVis23 released a dataset for 6D pose estimation of the wrist of da Vinci surgical instruments. SurgRIPE provides high-quality images captured from the da Vinci Si endoscope with 6D pose annotations obtained from a special keydot pattern. This pattern is removed by inpainting. In EndoVis24, the PhaKIR sub-challenge~\cite{phakir2024rueckert} release a new dataset of 13 cholecystectomy videos collected in three hospitals. PhaKIR includes 30K frames with multi-instrument multi-joint keypoints annotations at an interval of one frame per second, but the instruments are conventional instead of robotic. Furthermore, the monocular laparoscopes used in this dataset are uncalibrated. 

\subsection{Fluorescent Markers}
Manually labeling keypoints is time-consuming, cumbersome, and prone to error. To address this, Thananjeyan et al.~\cite{thananjeyan2022all} proposed LUV, a labeling methodology using a transparent and UV-fluorescent paint. They show LUV's feasibility for collecting labels of fabric keypoints, surgical thread, and suturing needles. In endoscopy, Schmidt et al. developed STIR~\cite{schmidt2024surgical}, a dataset of videos of deformable surgical tissue. By tattooing the tissue with Infrared (IR) fluorescent dye, indocyanine green (ICG), STIR generates markers that are invisible to visible spectrum algorithms. STIR markers can be detected under the IR spectrum using da Vinci Xi's Firefly mode.