


This section is based on \cite{belhadji_determinantal_2020}.
It is well known that the best (e.g. in Frobenius norm) approximation of a given matrix $X \in \mathbb{R}^{N \times  d}$ by a matrix of smaller rank $k \ll d$ is obtained by principal component analysis (PCA). The main drawback from this is that one loses the explainability of the features (columns), which can be particularly troublesome in some areas. Column subset selection aims at resolving this particular issue: for the matrix $X$ containing $d$ features about $N$ individuals, feature selection consists in selecting $k \ll d$ columns that are the most relevant. Finding the k-columns which contributes the most to the matrix is a complex task, and randomized methods have proven to be particularly efficient in this regard.

To fix the notations, for a matrix $A = (A_{ij})$, and subsets $I$, $J$, let $A_{IJ} = (A_{ij})_{i \in I, j \in J}$. 

Assume $\operatorname{rk}(X) = r$, fix also the singular value decomposition (SVD) of $X$ as $U \Sigma V^T$, where the singular values of $X$, $\sigma_i$, are ordered in decreasing order, $V \in \mathbb{R}^{d  \times r}$, $U \in \mathbb{R}^{N  \times r}$ are orthogonal and $\Sigma \in \mathbb{R}^{r  \times r}$ is diagonal. Let $V_k = V_{:[k]} = (V_{ij})_{i \in [d], j \in [k]}$ for $k\leq d$.

The quality of the approximation is seen through the ability of the span of the submatrix $X_{:S} = (X_{ij})_{i \in [n], j \in S}$ to approximate the columns of $X$, hence the criterion:

\[ \mathcal{L}_{k} : S  \subset  \{1, \dots, d\} \rightarrow  \underset{B \in \mathbb{R}^{\lvert S \rvert \times d} \text{ , } rk(B) \leq k}{\operatorname{min}} \lVert X - X_{:S} B \rVert_{\operatorname{Fr}}^2 \]

Call $\Pi_{S,k}X = X_{:S} X_{:S}^+ X $ (which is of rank at most $k$), where $X_{:S}^+$ is the Moore-Penrose pseudo-inverse of $X_{:S}$, the above criterion then reads, $\mathcal{L}_{k}(S) = \lVert X - \Pi_{S,k}X  \rVert_{\operatorname{Fr}}^2 $. 

Denote by $\Pi_k X$ the best rank $k$ approximation of $X$ (in the Frobenius sense), which can be obtained through classical PCA. Finally define the $k$-leverage scores as the norms of the columns of $V_k^T$, for $1\leq j \leq d$, $l_j^k = \sum \limits_{i = 1}^k V_{ij}^2$.

Solving the problem of finding a good candidate $S$ is a challenge. Deterministic algorithms exist such as rank revealing QR (RRQR) decomposition, see \cite{boutsidis_improved_nodate}, which have their own caveats. Here we choose to focus on non-deterministic methods.

\subsection{Non-repulsive algorithms}

\subsubsection{Length squared importance sampling}

The first idea to randomly select columns is to give them importance scores as their norm. This algorithm was introduced by Drineas et al. in \cite{drineas_clustering_2004}. More precisely, the idea is to sample $s$ indices, from a multinomial distribution on $[d]$, where the probability to sample $i$ is  $p_i \propto \lVert X_{:,i} \rVert^2$.

Theorem 3 in \cite{drineas_clustering_2004} states that:

 \[ \mathbb{P}( \lVert X - \Pi_{S,k}X  \rVert_{\operatorname{Fr}}^2 \leq \lVert X - \Pi_{k}X  \rVert_{\operatorname{Fr}}^2 + 2(1 + \sqrt{8 \log(\dfrac{2}{\delta} )}) \sqrt{\dfrac{k}{s}} \lVert X \rVert_{\operatorname{Fr}}^2) \geq 1 + \delta   \]

Notice first that the bound is additive here. Besides, this method leads to sample $s \gg k$ columns to achieve a good rank $k$ approximation.


\subsubsection{Leverage score importance sampling}

Another importance score that can be considered is the leverage score. This method was introduced by Drineas et al. in \cite{drineas_relative-error_2008}. The $k$-leverage scores distribution is the multinomial distribution on $[d]$ with parameters $p_i = l_i^k/k$. 
It is easy to see that the leverage scores sum to $k$ so that this definition gives a proper probability distribution.
Besides, for $\mathcal{P}_k = \operatorname{Span}(V_k)$, when $(e_i)_{i \in [d]}$ is the canonical basis of $\mathbb{R}^d$, and $\theta_i$ is the angle between $\mathcal{P}_k$ and $e_i$, some linear algebra manipulations show that $l_i^k = \cos^2(\theta_i) $.
So selecting the columns according to the leverage score sampling is in fact choosing the coordinates $e_i$ that are the most aligned with $\mathcal{P}_k$. PCA selects exactly $\mathcal{P}_k$,  hence, selecting a subspace which is almost aligned with $\mathcal{P}_k$ should yield a good approximation. 

Theorem 3 in \cite{drineas_relative-error_2008} states that, for $\epsilon>0$, $\delta > 0$, if the number of sampled columns $s$ satisfies $s \geq 4000(k^2/\epsilon^2)\log(1/\delta)$, then under the $k$-leverage score sampling it holds:

    \[ \mathbb{P}( \lVert X - \Pi_{S,k}X \rVert_{\operatorname{Fr}} \leq (1+ \epsilon) \lVert X - \Pi_kX \rVert_{\operatorname{Fr}}) \geq 1 - \delta \]

Finally, the lower bound on $s$ is sharp.
Likewise, the main drawback from this method is that to achieve a good rank $k$ approximation of $X$, one has to sample $s \gg k$ columns. 



\subsection{Repulsive algorithms} 

\subsubsection{Volume sampling}

One way to introduce correlation between the columns of $X$ is through volume sampling. The idea is here to use the kernel $X^TX$ and sample a $k$-DPP from it. This method will be called volume sampling (VS) in the following. Notice that the diagonal terms of the kernel are exactly the squared lenghts of the columns of $X$.
Volume sampling was first introduced by Deshpande et al. in \cite{deshpande_matrix_2006} and they established the following result:

\begin{proposition} \label{prop:VS} [\cite{deshpande_matrix_2006} Theorem 1.3]
    Assume $\mathcal{S}$ is sampled according to the volume sampling method. Then:
 
\begin{equation}
 \mathbb{E}_{\operatorname{VS}} [\lVert X - \Pi_{S,k}X \rVert_{\operatorname{Fr}}^2] \leq (k+1) \lVert X - \Pi_k X \rVert_{\operatorname{Fr}}^2 
\end{equation}

Besides, this bound is sharp in the worst case scenario.
\end{proposition}

The proof scheme for this result revolves around algebraic manipulations of the determinant. More precisely, expending the expectation gives: 

\[ \mathbb{E}_{\operatorname{VS}} [\lVert X - \Pi_{S,k}X \rVert_{\operatorname{Fr}}^2] = \dfrac{1}{\sum \limits_{S, \lvert S \rvert = k } \operatorname{det}(X_{:S}^TX_{:S})} \sum \limits_{S, \lvert S \rvert = k } \operatorname{det}(X_{:S}^TX_{:S} ) \lVert X - \Pi_{S,k}X \rVert_{\operatorname{Fr}}^2 \]

Now, for $T$ such that $\lvert T \rvert = k+1$, $T = \{i_1, \dots, i_{k+1}\}$, if $S = \{i_1, \dots, i_k\}$, the "base times height" formula states that: 

\[ \operatorname{det}(X_{:T}^TX_{:T}) = \dfrac{\operatorname{det}(X_{:T}^TX_{:T}) \operatorname{d}(X_{:,i_{k+1}}, \operatorname{span}\{X_{:,i_j} \text{ , } j \leq k\})^2}{(k+1)^2}  \]

Where $\operatorname{d}$ is the distance. Taking the sum of this when $T$ ranges over the parts of size $k+1$, and noticing that $\sum \limits_{i = 1}^d \operatorname{d}(X_{:,i}, \operatorname{span}\{X_{:,j} \text{ , } j \in S^2\})^2 =  \lVert X - \Pi_{S,k}X \rVert_{\operatorname{Fr}}^2$ gives:

\[ \sum \limits_{T, \lvert T \rvert = k +1 } \operatorname{det}(X_{:T}^TX_{:T}) = \dfrac{1}{(k+1)^3} \sum \limits_{S, \lvert S \rvert = k } \operatorname{det}(X_{:S}^TX_{:S} ) \lVert X - \Pi_{S,k}X \rVert_{\operatorname{Fr}}^2  \]

To conclude, the authors use a technical lemma on symmetric polynomials:

\[\sum \limits_{S, \lvert S \rvert = k } \operatorname{det}(X_{:S}^TX_{:S} ) = \dfrac{1}{(k!)^2} \sum \limits_{1 \leq t_1 < \dots < t_k \leq r } \sigma_{t_1}^2 \dots \sigma_{t_k}^2\]

This technical lemma allows them to write:

\[\sum \limits_{1 \leq t_1 < \dots < t_{k+1} \leq r } \sigma_{t_1}^2 \dots \sigma_{t_{k+1}}^2 \leq \sum \limits_{1 \leq t_1 < \dots < t_k \leq r } \sigma_{t_1}^2 \dots \sigma_{t_k}^2 \sum\limits_{j=k+1}^r \sigma_j^2 = (k!)^2 \sum \limits_{S, \lvert S \rvert = k } \operatorname{det}(X_{:S}^TX_{:S} ) \lVert X - \Pi_{k}X \rVert_{\operatorname{Fr}}^2  \]

Finally, the theorem follows from applying the technical lemma and then the aforementioned inequality on:

\[\mathbb{E}_{\operatorname{VS}} [\lVert X - \Pi_{S,k}X \rVert_{\operatorname{Fr}}^2] = \dfrac{(k+1)^3}{\sum \limits_{S, \lvert S \rvert = k } \operatorname{det}(X_{:S}^TX_{:S})} \sum \limits_{T, \lvert T \rvert = k +1 } \operatorname{det}X_{:T}^TX_{:T}  \]


Notice that this method samples exactly $k$ columns, unlike the previous i.i.d. schemes discussed and still enjoys strong theoretical properties.

Recently, \cite{derezinski_improved_2021} have worked on these bounds and they proved the following result. Notice that the bound is sharp in the worst case scenario, so they play on the decay of the singular values to achieve a better bound in some cases.

\begin{proposition} [\cite{derezinski_improved_2021} Theorem 1]
    For $s< r$, the stable rank of $X$, $sr_s(X)$ is $\sigma_{s+1}^{-2} \sum \limits_{j = s+1}^N \sigma_j ^2$. 

    Let $0< \epsilon \leq 1/2$, $s<r$ and $t_s = s + sr_s(X)$, then for $ s + \frac{7}{\epsilon^2} \ln(\frac{1}{\epsilon^2}) \leq k \leq t_s -1 $, the following holds:

    \[\mathbb{E}_{\operatorname{VS}} [\lVert X - \Pi_{S,k}X \rVert_{\operatorname{Fr}}^2] \leq (1 +2\epsilon)^2 \Phi_s(k) \lVert X - \Pi_{k}X \rVert_{\operatorname{Fr}}^2 \]

    where: $\Phi_s(k) = (1+ \frac{s}{k-s}) รท \sqrt{1 + \frac{2(k-s)}{t_s - k }}$

\end{proposition}

This upper bound is rather complicated to interpret in full generality. In their paper \cite{derezinski_improved_2021}, the authors show that it actually improves \ref{prop:VS} in cases where the singular values decay at a polynomial or an exponential rate.

\subsubsection{DPP sampling} 

The last algorithm we wanted to mention is taken from \cite{belhadji_determinantal_2020}. Their idea is to force the alignment between the subspace generated by the $k$ selected coordinates and the subspace generated by simultaneously force each principle angles to be the smallest possible.
In this regard, consider $S \subset [d]$, $\lvert S \rvert = k$ and denote $\mathcal{P}_S = \operatorname{Span} \{e_j | j \in S \}$. Let $\mathbb{\theta} = (\theta_i)_{1 \leq i \leq k}$ be the principal angle between $ \mathcal{P}_k$ and $\mathcal{P}_S$. Then it can be shown that: $\prod \limits_{i =1}^k \cos(\theta_i)^2 = \det(V_{S,[k]} V_{S,[k]}^T)$.
Hence sampling from an elementary DPP (of rank $k$) with kernel $K = V_k V_k^T$ emerges naturally from the geometry of the problem. This is what will be called DPP sampling in the following. 

Notice that this method actually generalizes leverage score importance sampling since the inclusion probabilities are given by: $\mathbb{P}_{\operatorname{DPP}}(i \in \mathcal{S}) = l_i^k $.
Belhadji et al. also show that there is a clear connection to volume sampling. Indeed, using the mixture property of DPP, one can notice that the highest mixture in the decomposition of $X^T X$ corresponds to the elementary DPP of kernel $V_k V_k^T$.

Without any further assumptions, they show the following: 

\begin{proposition} [\cite{belhadji_determinantal_2020} Proposition 16]
    Let $\mathcal{S}\sim \operatorname{DPP}(K)$, then:
\begin{equation}
     \mathbb{E}_{\operatorname{DPP}} [ \lVert X - \Pi_{S,k}X \rVert_{\operatorname{Fr}}^2] \leq k(d+1-k) \lVert X - \Pi_kX \rVert_{\operatorname{Fr}}^2
\end{equation}
\end{proposition}

Note that, in full generality, this bound is worse than the bound in the Volume sampling case. The main advantage of this method is that it has more flexibility.

Indeed, let the sparsity be the number $p$ of non zero $k$ leverage scores, and the flatness $\beta$ quantify the decay of the singular values of $X$ as: $\beta = \sigma_{k+1}^2 (\dfrac{1}{d-k}\sum \limits_{i = k+1} ^d \sigma_i^2)^{-1}$

Then the following holds:

\begin{proposition} [\cite{belhadji_determinantal_2020} Proposition 17]
    Let $\mathcal{S} \sim \operatorname{DPP}(K)$, then :
\begin{equation}
\mathbb{E}_{\operatorname{DPP}} [ \lVert X - \Pi_{S,k}X \rVert_{\operatorname{Fr}}^2] \leq (1+\beta \dfrac{p-k}{d-k}k) \lVert X - \Pi_kX \rVert_{\operatorname{Fr}}^2 
\end{equation}

\end{proposition}

The proof scheme for this result is quite similar to the one of \ref{prop:VS} with added technicalities. More precisely, notice that the inequality which was used there is rather loose, and a better understanding of the decay of the singular values of $X$ would give a better bound. Besides, this projection DPP has a strong connection to the principal angles as mentioned previously, and in a certain way, sampling from this DPP makes us find the columns that are best aligned with the principal eigenspaces. Using these facts, and correctly manipulating the symmetric polynomials lead to this result. We refer the reader to \cite{belhadji_determinantal_2020} for more details.

Notice that $ \beta \in [1,d-k]$, with the extreme cases being all singular values being equal after $k+1$ (which leads to $\beta =1$) and $\sigma_{k+1}$ being the last non zero singular value (which leads to $\beta = d-k$). 
In the regime of small $\beta$ and $p$, this bound is actually better than \ref{prop:VS} since $p\leq d$. Likewise, here to achieve a good rank $k$ approximation, only $k$ columns are sampled.

It is worth mentioning that it is possible to reduce the sparsity by looking at some "effective sparsity", and conditioning on sampling indices for which the leverage score contributes significantly to the whole. All of these results can also be extended when working with the spectral norm instead of the Frobenius one, see \cite{belhadji_determinantal_2020}.
