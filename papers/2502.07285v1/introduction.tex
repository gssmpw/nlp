\subsection{Background}
The traditional framework of statistical independence has significantly advanced the fields of machine learning and statistics. However, its applicability and effectiveness may be reaching their limits in certain contexts, both as a modeling approach and as a foundation for algorithm design. This presents a strong motivation to explore alternative learning paradigms. Notable among these are models inspired by statistical physics that exhibit strong correlations, an approach that has attracted significant attention in recent years. The goal is to take advantage of the global dependence structures and long-range correlations that characterize these processes to enhance efficiency and reduce complexity in achieving improved learning objectives.

Negative dependence, where a stochastic system encourages diversity among its variables, is emerging as a key driver for advancing learning capabilities beyond the constraints of classical independence. In a range of core learning challenges — including but not limited to optimization, sampling, dimensionality reduction, and sparse signal recovery — recent research has shown that negatively dependent systems outperform state-of-the-art methods based on statistical independence, positioning them as a powerful new approach for future developments in this field.

\subsection{Motivation and heuristics}
A typical instantiation of this approach may be seen in the setting of quadratures, a topic that will be covered in a more technical and in-depth discussion later in this article. In this introductory segment, we will undertake a heuristic discussion for illustrative purposes. 

\begin{figure}[h]
    \centering
    \includegraphics[width=5cm, height=5cm]{Figures/Poisson.jpg}
    \includegraphics[width=5cm, height=5cm]{Figures/Ginibre.jpg}
     \includegraphics[width=5cm, height=5cm]{Figures/GAF.jpg}
     \caption{(a) Left: Poisson  \qquad (b) Center: DPP \qquad (c) Right: GAF}
    \label{fig:poisson-dpp-gaf}
\end{figure}


The panel of Fig. \ref{fig:poisson-dpp-gaf} demonstrates three competing point processes on a 2D square (with the same average density of points per unit area): (a) uniformly chosen random points, (b) a determinantal point process (abbrv DPP), and (c) zeros of a Gaussian random series. While the first one is a representation of the basic i.i.d. notion of randomness that is classically popular in probability, statistics, and applied mathematics, the latter two are strongly correlated models of randomness, exhibiting \textit{negative association} to varying degrees. The negative association entails that the random point set model incentivizes points to \textit{repel} each other (esp. at short ranges) -- in more technical terms, the stochastic model puts more weight on point configurations with fewer pairs of points that are close to each other. For the DPP models, this feature holds true at all distance scales, whereas for the Gaussian zero processes this is known to hold true at short distances (the most important regime for repulsion in practical scenarios) (c.f. Eq. \ref{eq:DPP_repel}, Eq. \ref{eq:Poisson_repel} in Section \ref{sec: DPP} and the discussion about GAFs in the end of subsection 7.1, Section \ref{sec: spectrogram} for a more analysis of this phenomenon). 


Based on these considerations, we present below a heuristic argument to demonstrate how negatively associated random point sets can contribute to improved learning outcomes and approximation guarantees. To this end, let us consider a simple scenario where we use the sampled point set $\Xi$ for \textit{quadrature} -- namely, for a reasonably regular real-valued function $f$ defined on the square $D$ (c.f. Fig. \ref{fig:poisson-dpp-gaf}), we desire to use a (weighted) average of $f$ over $\Xi$ in order to approximate the integral $\int_D f(x) \d x$. A traditional baseline in randomized quadrature would be to use uniformly sampled independent points (c.f. Fig. \ref{fig:poisson-dpp-gaf} panel (a)), which we compare against negatively dependent samples (such as in Fig. \ref{fig:poisson-dpp-gaf} panel (b) and (c)). 

Observe that the independent point set in panel Fig. \ref{fig:poisson-dpp-gaf} (a) exhibits clusters of points that are relatively closer to each other, interspersed with vacant spaces. In comparison, the negatively dependent point sets in Fig. \ref{fig:poisson-dpp-gaf} (b) and (c) exhibit a much more homogeneous spatial outlay of points. In fact, they almost resemble a randomly perturbed or stochastic grid, which would incidentally be another viable model of negatively dependent random point set for applications (see Sec. \ref{sec: networks}). Such behaviour can, in fact, be seen from negative dependence at short length scales via a simple heuristic argument. To wit, viewed as a physical particle system, the mutual repulsion tends to push the particles away from each other. But the confining potential (in physical terms), equivalently the background measure (in stochastic terms), prevents the particles from escaping to infinity. The tension between these two opposing effects compels the particles to equilibriate around a spatially homogeneous point configuration, as observable in Fig \ref{fig:poisson-dpp-gaf} panels (b) and (c).

Coming back to the question of randomized quadrature, notice that any estimate of $\int_D f(x) \d x$ based on independent points will be overly dependent on the functional behaviour in the region of point clusters, and relatively uninformative about the vacant regions with relatively few or no points. This introduces inherent inefficiencies in the quadrature procedure. On the other hand, owing to their homogeneous spatial distribution, a negatively dependent point set captures the functional behaviour across spatial regions in a more comprehensive manner, thereby leading to more robust approximation properties. 

On a related note, the fact that the expected density of points per unit area is constant necessitates that in different samples of the independent point sets, the location of the point clusters and vacant spaces (i.e., regions with relatively higher and lower density of points ) keep changing, which implies that quadrature estimates based on such point sets would exhibit large fluctuations from one sample to another. On the other hand, quadrature estimates based on negatively dependent point sets tend to be much more stable from one quadrature sample to another, thereby leading to much tighter approximation guarantees.

The above considerations are further reinforced by the fact that it is possible to develop a theory of negatively dependent random point sets on very general background spaces, widely extending the ambit of the heuristics presented above. This includes, in particular, settings where the notion of a natural grid is not available (e.g., manifolds with non-trivial curvature such as spheres, spaces without geometry such as combinatorial objects like graphs and discrete spaces which are a staple in many machine learning problems).

\subsection{Related literature}
Random point sets with negative dependence have been in vogue as a scientific discipline for several decades. The principal motivation for this came from statistical and quantum physics. An early connection arose in the context of Fermionic particle systems, wherein determinantal structures were observed in ground state densities of Fermionic systems via Slater determinants \cite{Slater}. Subsequently, it was developed as a mathematical discipline by Macchi and others \cite{Mac72, Mac75, SoshDPP}, and new connections arose with a wide array of areas in mathematics and statistical physics, including random matrices and Coulomb gases  \cite{johanssondpp, borodin}, integrable systems  \cite{Deift_1,Deift_2}, combinatorics \cite{borodin2015_integrable,borodin2016_integrable}, among others. 

Negative dependence as a tool for machine learning and statistics is of a more recent historical provenance. Earlier work in this direction is well explored  in the excellent survey by Kulesza and Taskar  \cite{kulesza_determinantal_2012}, which appeared only around a decade ago, where we refer the interested reader for an account of formative developments. This was also roughly contemporaneous with the emergence of works in the statistical literature that looked at DPPs and allied negatively dependent processes as a statistical modelling paradigm for strongly correlated data \cite{LaMoRu14, lavancier-1,lavancier-2}. These developments motivated investigations into statistical estimation and inference on DPPs, e.g., learning the parameters of DPPs \cite{Fox-Affandi-1, Rigollet-2},  efficient sampling schemes for continuous DPPs \cite{Fox-Affandi-2},  rates of estimation for DPPs \cite{Rigollet-1}, and DPPs for minibatch sampling in stochastic gradient descent \cite{OPE-NIPS}.  

This also ties in with a burgeoning interest, principally in the theoretical computer science and adjacent communities, in carrying out investigations into general models of negatively associated random sets (in the discrete setting), known as \emph{strongly Rayleigh measures}. 
Within the limited boundaries of the present article, we will not have the occasion to delve into this beautiful theory, instead referring the reader to the seminal works of Borcea, Branden and Liggett \cite{Borcea-Branden, Borcea-Branden-2, Borcea-Branden-3, Borcea-Branden-Liggett, BRANDEN, Branden-2}, for the development of the general theory as well as its connections to other parts of discrete mathematics, such as matroid theory, log concavity of sequences and related topics. In the setting of applications to learning theory, notable contributions include counting bases of matroids \cite{anari_logconcave2}, sampling algorithms \cite{anari_mcmc_rayleigh_dpp, Jegelka}, traveling salesman problem \cite{travelling-salesman}, learning with DPPs \cite{gartrell2020scalable,Sra,mariet_diversity_2017}. 
On a related note, we must mention the resolution of the Kadison-Singer conjecture and construction of a very general class of Ramanujan graphs by Marcus, Spielman and Srivastava over the last decade that strongly leverages zeros of characteristic polynomials and their interlacing structures \cite{interlacing-1, Marcus-Spielman-Srivastava, interlacing-3, interlacing-4}.

In this article, we will review recent developments focusing on the use of negative dependence as a tool for improving the state of the art in fundamental learning problems. We will make the subtle distinction of this approach from the use of DPPs as a statistical modelling paradigm (cf. references above), a topic of its own interest that we will have the occasion to address only in passing within the limited span of this article. In the main, we will largely focus in detail on problems in this direction where the authors have had the opportunity to make a contribution, with elaborate discussions on connections to other related topics. These include DPPs for Monte Carlo methods, DPPs and directionality in data, DPPs for coresets in machine learning, negative dependence for stochastic networks, zeroes of Gaussian analytic functions for spectrogram analysis, DPPs for feature selection, and a quantum sampler for DPPs. We will also present new theoretical results on applications of DPPs in the context of neural network pruning, a problem that has attracted considerable interest in recent years. 



