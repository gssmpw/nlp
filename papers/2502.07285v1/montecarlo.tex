\subsection{Motivation}
Let $\mu$ be a finite positive Borel measure on $\mathbb R^d$.
A fundamental question in numerical analysis is to approximate integrals $\int_{\mathbb R^d} f(x) \mu(\d x)$, where $f$ ranges over some class of functions $\mathcal F$ on $\mathbb R^d$. 
Given $N \in \mathbb N$, a \textit{quadrature} is an algorithm that outputs $N$ points $x_1,\ldots,x_N\in \mathbb R^d$ (called \textit{nodes}) and weights $w_1,\ldots,w_N \in \mathbb R$. Based on these nodes and weights, we construct an estimator for $\int f(x) \mu(\d x)$ as
\[ \sum_{i=1}^N w_i f(x_i), \quad \text{ for each } f\in \mathcal F.\]
In general, the nodes and weights can depend on $N$ and $\mu$, but should not depend on any specific $f$. Given a quadrature, the approximation error is defined as
\[\mathcal E_N(f) := \sum_{i=1}^N w_i f(x_i) - \int f(x) \mu(\d x).\]
The asymptotic behavior of $\mathcal E_N(f)$ as $N\rightarrow\infty$  reflects the performance of the corresponding quadrature.


\subsection{Existing algorithms}
Given the classical nature of the problem, many quadrature methods have been developed, including Gaussian quadrature \cite{DavisRabi,Gautschi,BrassPetras, GautschiVarga}, Monte Carlo methods \cite{MonteCarlobook, bardenet:MCMC}, Quasi-Monte Carlo methods \cite{Dick_Pillichshammer_2010, Dick_2013, Oates16}, Monte Carlo with DPPs \cite{bardenet:montecarlogaussian, OPE-AOAP, bardenet:montecarlodpp, lemoine2024montecarlomethodscompact} and Bayesian quadrature \cite{Huszar12, Briol2015}, to name a few. In this survey, we briefly review Gaussian quadrature, Monte Carlo methods and Monte Carlo with DPPs.

\subsubsection{Gaussian quadrature}
We consider the case when $\mu$ is a measure supported on the interval $I:=[-1,1]$. Let $(\varphi_k)_{k\in\mathbb N}$ be a sequence of polynomials such that for each $k\in \mathbb N$, $\varphi_k$ has degree $k$, positive leading coefficient, and such that
\[ \int_I \varphi_k(x)\varphi_l(x)\mu(\d x) =\delta_{kl}, \quad\forall k,l \in \mathbb N.\]
Such a sequence is called \emph{orthonormal polynomials} associated with $\mu$.
We define the corresponding $N$-th Christoffel-Darboux kernel by 
\[ K_N(x,y) := \sum_{k=0}^{N-1} \varphi_k(x)\varphi_k(y).\]

Gaussian quadrature \cite{DavisRabi, Gautschi, BrassPetras} outputs $N$ nodes $x_1,\ldots,x_N$ to be the zeros of $\varphi_N(x)$ (which are real and simple) and weights $w_i = K_N(x_i,x_i)^{-1}$. The error is then given by
\[\mathcal E_N(f) = \sum_{i=1}^N \frac{f(x_i)}{K_N(x_i,x_i)} - \int f(x) \mu(\d x).\]

Gaussian quadrature is particularly effective when the test function $f$ is close to a polynomial. Specifically, this method gives $\mathcal E_N(f) =0$ for every polynomial $f$ of degree less than $2N-1$, and $\mathcal E_N(f)$ decays exponentially fast when $f$ is analytic (e.g., see \cite{GautschiVarga}). However, optimal rates of decay for $\mathcal E_N(f)$ are still unknown when $f$ is less regular. For instance, when $f$ is $C^1$, it is only known that $\mathcal E_N(f) = O(N^{-1})$, based on the classical Jackson's approximation theorem.

Another drawback of Gaussian quadrature is its limitation in higher-dimensional spaces. When the dimension $d\ge 2$, the zeros of multivariate polynomials form hypersurfaces, making it meaningless to use these zeros as nodes. Some modifications address this issue, such as considering $\mu = \otimes_{j=1}^d \mu_j$ as a product measure, where each $\mu_j$ is supported on $I$ and constructing a grid of nodes using $d$
one-dimensional Gaussian quadratures. However, this method suffers from the curse of dimensionality.

\subsubsection{Monte Carlo methods}
Monte Carlo methods \cite{MonteCarlobook} refers to picking up the nodes as the realization of random points in $\mathbb R^d$. A standard algorithm in this vein is the so-called \textit{importance sampling}. We consider the case when $\mu(\d x) = \omega(x) \d x$ supported in the hypercube $I^d$. Importance sampling outputs $N$ nodes $X_1,\ldots,X_N$ as i.i.d. samples from a \emph{proposal density} $q(x)\d x$ and 
\[ w_i = \frac{1}{N} \frac{\omega(X_i)}{q(X_i)} \cdot\]
The approximation error is then
\[\mathcal E_N(f) = \frac{1}{N}\sum_{i=1}^N \frac{\omega(X_i)}{q(X_i)} f(X_i) - \int_{I^d} f(x) \omega(x)\d x.\]

A simple calculation shows that $\mathbb E[\mathcal E_N(f)] = 0$.
If
\[ \sigma^2_f := \var_{X\sim q} \Big [\frac{\omega(X)f(X)}{q(X)}  \Big ] < \infty \]
then the classical CLT for independent random variables gives
\[  \sqrt{N} \mathcal E_N(f) \overset{law}{\longrightarrow} \mathcal N(0,\sigma_f^2).\]
In particular, $\var[\mathcal E_N(f)] = O(N^{-1})$.
Thus the typical order of magnitude of $\mathcal E_N(f)$ for the Monte Carlo method is $N^{-1/2}$,
which represents a slow decay rate for classical independent samplers. 
%This rate arises from the independence of the $X_i$'s; as long as the nodes are selected independently, it is impossible to improve upon the rate of $N^{-1/2}$. 
%\vp{The rate can be improved using iid samples using some well chosen weights, see Portier Segers MONTE CARLO INTEGRATION WITH A GROWING NUMBER OF
%CONTROL VARIATES}

\subsection{Monte Carlo with DPPs}
%To improve the decay rate, one needs to use a different sampling scheme rather than independent sampling. Repulsive particle systems, e.g., DPPs, are promising candidates since sampling with these point processes usually gives smaller variance. However, one needs to be more rigorous at this point, i.e., to construct explicitly the sampling (e.g., specify the kernels for DPPs) and to quantify the decay rate.

In \cite{OPE-AOAP}, the authors show that using DPPs for Monte Carlo methods can give better decay rates for $\var[\mathcal E_N(f)]$. Their approach is based on the theory of (multivariate) orthogonal polynomials.

To elaborate, let $\mu$ be a measure supported in $I^d$.
We define an inner product associated with $\mu$
\[ \langle f,g \rangle_\mu := \int f(x)\overline{g(x)} \mu(\d x).\]
Consider the sequence of monomials $x_1^{k_1}\ldots x_d^{k_d}, k_i \in \mathbb N$, taken in the graded lexical order. Applying the Gram-Schmidt procedure to this sequence (w.r.t. $\langle \cdot,\cdot \rangle_\mu$), we obtain the orthonormal sequence $(\varphi_k)_{k\in \mathbb N}$ associated with $\mu$.
Let $K_N(x,y)$ be the corresponding $N$-th Christoffel-Darboux kernel for the (multivariate) orthogonal polynomial ensemble
\[K_N(x,y)= \sum_{k=0}^{N-1} \varphi_k(x)\varphi_k(y).\]

Let the nodes $\{X_1, \ldots, X_N \}$ be the DPP on $I^d$ with kernel $K_N$ and reference measure $\mu$, and let the weights be $w_i = K_N(X_i,X_i)^{-1}$. Then 
\begin{equation} \label{eq:MonteCarloDPP}
    \mathcal E_N(f) = \sum_{i=1}^N \frac{f(X_i)}{K_N(X_i,X_i)} - \int_{I^d} f(x) \mu(\d x)
\end{equation}
has mean zero by construction.
Intuitively, the negative dependence in DPPs results in more cancellation in linear statistics, often leading to smaller variance. Motivated by this intuition, the authors in \cite{OPE-AOAP} demonstrated that this is indeed the case for the multivariate OPE through a careful analysis of the estimator \eqref{eq:MonteCarloDPP}. To convey their idea while maintaining simplicity, we will present a particular case from their results.

\begin{theorem}[Theorem 1 in \cite{OPE-AOAP}]
    Let $\mu(\d x) = \d x$ be the uniform measure on the hypercube $I^d$. Then for any $f\in C^1$, compactly supported in $(-1,1)^d$, the estimator \eqref{eq:MonteCarloDPP} satisfies
    \[\var[\mathcal E_N(f)] = O \Big (\frac{1}{N^{1+1/d}}\Big ) \cdot \]
\end{theorem}
In fact, the authors further demonstrated that the rate mentioned above eventually applies to a larger class of measures $\mu$, and presented central limit theorems suited for Monte Carlo integration. For a more general result, we refer readers to the paper \cite{OPE-AOAP}. 

%\begin{theorem}
    %Let $\mu(\d x) =\omega(x)\d x= \omega_1(x_1)\ldots \omega_d(x_d)\d x$ be a product reference measure supported in $I^d$. Assume $\omega(x)$ is $C^1$ and positive on $(-1,1)^d$ and satisfies 
    %\[ \frac{1}{N} \sup_{x\in [-1+\varepsilon, 1-\varepsilon]^d} |\nabla K_N(x,x)| < \infty, \quad \forall \varepsilon>0.\]
    %Then 
    %\[ \mathbb E [|\mathcal E_N(f)|^2] = O(N^{-(1+1/d)}),\]
    %or any $f\in C^1$ supported in $[-1+\varepsilon, 1-\varepsilon]^d$ for some $\varepsilon>0$.
%\end{theorem}




