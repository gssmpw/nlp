\subsection{A parametric model}
 Directionality in data pertains to isotropy-breaking -- namely, the degree of dependency between points (for instance, repulsion) is much stronger in a few special directions compared to others. Thus, directionality corresponds to a type of (low dimensional) structure on data (compare, e.g., to sparsity), and is naturally applicable to learning scenarios such as clustering where such distinguished directions are of canonical importance. In \cite{GDP} the authors considered Gaussian Determinantal Processes (abbrv. GDP) as a well-structured parametric model of DPPs, with particular focus on exploring isotropy versus directionality in data.

For a $d \times d$ non-negative definite matrix $\Sigma$ (referred to as the scattering matrix), we define the  Gaussian Determinantal Process (GDP) as a translation-invariant determinantal process on $\R^d$ with kernel given by
$$
K(x-y)=\frac{1}{(2\pi)^{\frac{d}{2}} \sqrt{\det{\Sigma}}}\exp\big(-\frac{1}{2}(x-y)^\top \Sigma^{-1}(x-y)\big)\,, \quad x,y \in \R^d\,.
$$
and the standard Lebsegue measure on $\R^d$ as the background measure. It can be shown via Fourier analytic arguments that the Macchi-Soshnikov Theorem guarantees the existence of a GDP for any such scattering matrix $\Sigma$.

\subsection{A spiked model for DPPs and directionality in data }
The isotropic case of the GDP, where the dependency structure of the points is the same in all directions, corresponds to the choice $\Sigma = \sigma^2 I_d$, where $\sigma$ is a scalar and $I_d$ is the $d\times d$ identity matrix. For purposes of clarity, let us look at GDPs with average one particle per unit volume; this corresponds to \[K(0)=\frac{1}{(2\pi)^{\frac{d}{2}} \sqrt{\det{\Sigma}}}=1.\] Under this normalization, the truncated pair correlation function $\bar{\rho}_2(x,y)=-|K(x,y)|^2$ is given by \[\bar \rho_2(x,y)= - \exp\big(- (x-y)^T \Sigma^{-1} (x-y)  \big)\,.\] Therefore, if $x - y$ is well-aligned with an eigenvector of $\Sigma$ corresponding to a relatively large eigenvalue, then the magnitude of $\bar{\rho}_2(x, y)$ remains significant even when $\|x - y\|$ is large. As such, in these directions, the spatial correlation extends over a much longer range (compared to others).

This observation forms the basis of a spiked DPP model, analogous to the widely-used spiked covariance model \cite{BBP}, where the covariance matrix $\Sigma$ is modeled as a rank-one perturbation of the identity: $\Sigma = \text{Id} + \lambda uu^\top$, with $\|u\|=1$ and $\lambda \geq 0$. In this GDP setting, the directional structure is captured by the rank-one perturbation. In the context of GDPs, it is natural to consider scenarios where the presence of the spike does not alter the mean density of points compared to the isotropic case, where the scattering matrix is the identity. If this condition does not hold, the spike's presence can be detected by estimating the density. Maintaining the same mean density implies that $\det(\Sigma)$ is the same for both the null and alternative cases; this corresponds to shear-type transformations of the ambient Euclidean space.

Putting together the above observations, \cite{GDP} proposes the following spiked model for DPPs:
\begin{equation}
\label{eq:spike}
(2\pi) \cdot \Sigma=(1+\lambda)^{-\frac1{d-1}}(I_d-uu^\top) + (1+\lambda) uu^\top\,, \quad \|u\|=1\,,
\end{equation}
where $\lambda\ge 0$ is the strength parameter and $uu^\top$ is the spike. Notice that when $\lambda =0$, we are reduced to the isotropic setting $\Sigma=I_d$ (the factor $2 \pi$ is just a convenient normalization). 

\subsection{Inference on spiked models for GDP}
Consider a given collection of data points $\{X_1,\ldots,X_n\} \subset B_d(0;R)$ (where  $B_d(0;R)$ is the ball with centre 0 and radius $R$),  our goal is to infer distinguished directions along which there are long range dependencies in the data. We aim to do this in the framework of the GDP model. 

Inferential procedures on the GDP, as laid out in \cite{GDP}, are based on statistic $\hat \Sigma$ of  given by the $d \times d$ matrix
$$
\hat \Sigma := |B(0;1)|\frac{r_{n,d}^{d+2}}{d+2} I_d - \frac{1}{|B_d(0;R-r_{n,d})|}\sum_{i \in \mathcal{N}_0} \sum_{j \in \mathcal{N}_i} (X_i-X_j)(X_i-X_j)^\top ,
$$
where, for each $1\le i \le N, \, \mathcal{N}_i=\{j\,:\,j\ne i,  \,\|X_i-X_j\| < r_{n,d} \}$ is the  set of other observations that are at distance at most $r$ from it, and $\mathcal{N}_0=\{j\,:\, \|X_j\| < R-r_{n,d} \}$ is the set of \textit{interior} data points in $B(0;R)$. Here $|A|$ denotes the volume of a measurable set $A \subset \R^d$, and $r_{n,d}$ is a parameter chosen to be $r_{n,d}= c \cdot \sqrt{d \log n}$ for some constant $c$.

Setting \[\mathfrak{r}_{n,d}:=\frac{d^2(c \sqrt{\log n})^{d+1}}{\sqrt{n}}\] and denoting by $\chi(B)$ the indicator of the event $B$, it is demonstrated in \cite{GDP} that the test  
\[\psi_\delta=\chi\big\{ (2\pi)\|\hat \Sigma\|_{\mathrm{op}}>1+ \frac{\mathfrak{r}_{n,d}}{\delta}\big\}\]  
detects the presence of a spike with power $\delta$ as soon as the spike strength $\lambda$ is above the threshold $\lambda_{\mathrm{thresh}}=2\mathfrak{r}_{n,d}$. In the event that a spike is detected, the spike direction can be effectively estimated by the maximal eigenvector of $\hat \Sigma$. It has been demonstrated in \cite{GDP} that these statistical procedures achieve parametric rates in fixed dimensions. This raises natural and interesting questions vis-a-vis the study of GDP type models and related statistical procedures in a high dimensional setting, wherein new kinds of random matrix ensembles are expected to emerge.

\subsection{Clustering and random matrices}
A key application of GDP based methodology, as outlined above, is to clustering problems, and more generally, the problem of dimensionality reduction. A natural comparison would be to classical dimension reduction methods based on Principal Component Analysis (PCA), wherein low dimensional approximation to data is achieved via projections onto the principal components (i.e., principal eigendirections) of the sample covariance matrix. 

It is well known that the principal directions in PCA correspond to the directions of maximal variance in the data. In \cite{GDP}, dimension reduction methodology based on finding directions of maximal repulsion was proposed, based on the GDP as an ansatz. In this vein, we obtain dimension reduction by projecting onto the principal eigendirections of the matrix $\hat \Sigma$ as above. The performance of this approach has been demonstrated on the celebrated Fisher's Iris dataset, which is a prototypical benchmark for clustered data \cite{GDP}. 

\begin{figure}[h]
    \centering
    \includegraphics[width=0.49\linewidth]{Figures/Iris-DPP.pdf}
    \includegraphics[width=0.49\linewidth]{Figures/W-DPP.pdf}
    \caption{Left: Iris-DPP \qquad \qquad Right: W-DPP \qquad (c.f. \cite{GDP})}
    \label{fig:iris}
\end{figure}


It may be noted that the essential part of the estimator $\hat \Sigma$ which determines its eigendirections (thereby playing the central role in clustering and dimension reduction methods based on it) is the component 
\[ \tilde \Sigma = \sum_{i \in \mathcal{N}_0} \sum_{j \in \mathcal{N}_i} (X_i-X_j)(X_i-X_j)^\top .\]

Since the data points $\{X_1,\ldots,X_n\}$ are random, $\tilde \Sigma$ is a random matrix. The GDP model, and its applications to clustering and dimension reduction, therefore naturally leads to intriguing questions in random matrix theory. In particular, the spiked model \eqref{eq:spike} leads to a new kind of spiked random matrix model featuring a kernelized truncation (based on mutual separation).

It maybe noted that, even if the data points $\{X_1,\ldots,X_n\}$ are independent, random matrices of the form $\tilde \Sigma$ have dependent entries (due to the truncation in the definition). However, as a random matrix model, it still offers a significant degree of structure so that hope for analytical progress is still reasonable. Indeed, in \cite{ghosh-mukherjee-talukdar}, the authors investigate the most basic setting for such kernelized random matrix ensembles, where the points $\{X_1,\ldots,X_n\}$ are i.i.d., and establish the emergence of novel families limiting spectral distributions that generalize the classical Marcenko-Pastur laws, and beyond. The methods involve crucial ingredients from free probability theory. We expect that, motivated by the GDP ansatz and its methodological application, the study of such kernelized ensembles will lead to novel and interesting problems in random matrix theory with dependent entries.

%\SG{[Insert Fig : Fisher's Iris experiment from GDP]}

