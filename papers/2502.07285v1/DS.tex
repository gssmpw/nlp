%\subsection{Sampling from DPPs} \label{sec:SamplingDPP}

DPPs are particularly useful tools for machine learning since they can be sampled in polynomial time. The classic algorithm to sample a DPP comes from \cite{HKPV} which leverages a neat geometric understanding of a DPP as a probability measure. Recent algorithmic advances significantly improve this classical algorithm to achieve computational efficiency. 

\subsection{The classic Spectral Sampler for DPPs} \label{sec:HKPV}

Let $\mathcal S$ be a projection DPP of rank $m$, i.e., its kernel $K$ defines a projection operator $\mathcal K_H$ from $L^2(\mathcal X,\mu)$ to an $m$-dimensional subspace $H \subset L^2(\mathcal X,\mu)$. We denote by $\|\cdot\|$ the norm in the Hilbert space $L^2(\mathcal X,\mu)$. Then for each $f\in L^2(\mathcal X,\mu)$, $\mathcal K_H f$ is the unique element in $H$ closest to $f$.

In the discrete case, the delta mass at $x$ for $x\in \mathcal X$ is an element in $L^2(\mathcal X,\mu)$, namely
\[  \delta_x (y) = \begin{cases} 
    1/\mu(\{x\}) & \text{ if } y = x \\
    0 & \text{ otherwise.}
\end{cases}\]
Thus, $\mathcal K_H$ can act on $\delta_x$ and by direct calculations, one has $\mathcal K_H \delta_x (\cdot ) = K(\cdot ,x)$. In the general case,
we define $\mathcal K_H \delta_x (\cdot) := K(\cdot, x)$. Intuitively, $\mathcal K_H \delta_x$ represents the element in $H$ which is closest to $\delta_x$.

Since $K$ is a projection kernel, one can easily check that
\[ \| \mathcal K_H\delta_x\|^2 = \int |K(y,x)|^2 \d \mu(y) = \int K(x,y) K(y,x) \d \mu(y) = K(x,x).\]
The intensity measure of $\mathcal S$ is then given by
\[ \d \mu_H(x) = K(x,x) \d \mu(x) = \|\mathcal K_H\delta_x\|^2 \d \mu(x).\]
We note that $\mu_H(\mathcal X) = m$, so $\frac{1}{m}\mu_H$ is a probability measure on $\mathcal X$. Starting with $H_m := H$, the algorithm to sample the DPP $\mathcal S$ is as follows.

\begin{algorithm} [H] \label{alg:HKPV}
\caption{Sampling a projection DPP}
\begin{algorithmic}[1]
    \State If $m=0$, stop.
    \State Pick a random point $X_m$ in $\mathcal X$ from the probability measure $\frac{1}{m}\mu_{H_m}$.
    \State Let $H_{m-1} \subset H_m$ be the orthogonal complement of the function $\mathcal K_{H_m}\delta_{X_m}$ in $H_m$. In the discrete case, $H_{m-1} = \{f\in H_m : f(X_m) =0 \}.$ Note that $\dim H_{m-1} = m-1$ almost surely. 
    \State Decrease $m$ by $1$ and iterate.
\end{algorithmic}
\end{algorithm}

\begin{proposition}
 Let $(X_1,\ldots,X_m)$ be the points constructed by Algorithm 1. Then $ \mathcal S \overset{d}{=}  \{X_1,\ldots,X_m\} $.
\end{proposition}

\begin{proof}
    We compute the density of $(X_1,\ldots,X_m)$ at $(x_1,\ldots,x_m) \in \mathcal X^m$ with respect to $\mu^{\otimes m}$ (where $x_i \in \mathcal X$ are  distinct). Let $\Psi_j:= \mathcal K_H \delta_{x_j}$, then one can easily check that $\mathcal K_{H_j}\delta_{x_j} = \mathcal K_{H_j}\Psi_j$. Thus, the density of $(X_1,\ldots,X_m)$ at $(x_1,\ldots,x_m)$ equals
    \[ p(x_1,\ldots,x_m) = \prod_{j=1}^m \frac{\|\mathcal K_{H_j}\Psi_j \|^2}{j} \cdot\]
    We further note that $H_j = H \cap \langle \Psi_{j+1},\ldots, \Psi_m \rangle^\perp$. Therefore, the quantity $\prod_j \|\mathcal K_{H_j} \Psi_j\|^2$ equals the determinant of the Gram matrix whose $(i,j)$ entry is given by the inner product of $\Psi_i$ and $\Psi_j$. By direct computation,
    $ \int \Psi_i \bar\Psi_j \d \mu = K(x_i,x_j)$.
    Thus
    \[ p(x_1,\ldots,x_m) = \frac{1}{m!} \det [K(x_i,x_j)]_{m\times m}.\]
    Hence, the point process $\{X_1,\ldots,X_m\}$ is a DPP on $\mathcal X$ defined by the kernel $K$ and background measure $\mu$, as desired.
\end{proof}

In the discrete setting, when the DPP $\mathcal S$ has the structure of an $L$-ensemble with kernel $\bL = \bK(\bI-\bK)^{-1}$, we can sample $\mathcal S$ via a spectral argument as follows. 
Let $\bL=\sum_{i =1}^N \lambda_i \mathbf{v}_i \mathbf{v}_i^\top$ be the spectral decomposition of $\bL$. The spectral decomposition of $\bK$ is then given by: $\bK=\sum_{i =1}^N (\lambda_i/(1+\lambda_i)) \mathbf{v}_i \mathbf{v}_i^\top$.

\begin{algorithm} [H]
\caption{Spectral sampling}
\begin{algorithmic}[1]
    \State \textbf{Input:} $(\mathbf v_i, \lambda_i)_{i \in [N]}$ an eigendecomposition of $\bL$
    \State $V \gets \emptyset$, $\mathcal S \gets \emptyset$
    \For {$1 \leq j \leq N$}
        \State Add $\mathbf v_j$ to $V$ with probability $\lambda_j/(1+\lambda_j)$
    \EndFor
    \While {$\lvert V \rvert > 0$}
        \State Sample $i \in [N]$ with probability: $\frac{1}{\lvert V \rvert} \sum \limits_{\mathbf v \in V} (\mathbf v^\top \mathbf e_i)^2 $ and add it to $\mathcal S$
        \State $V \gets V_\perp$ an orthonormal basis of the subspace of $V$ orthogonal to $\mathbf e_i$
    \EndWhile
    \State \textbf{Return} $\mathcal S$
\end{algorithmic}
\end{algorithm}
This algorithm is an avatar of the complete algorithm of \cite{HKPV}, and the proof of validity in the discrete form we use here can be found in \cite{kulesza_determinantal_2012}.
%The longest part of the procedure presented above is actually the Gram-Schmidt part which runs in the ``for" loop. However, w
When $N$ becomes large, the true bottleneck here is accessing the eigendecomposition of $\mathbf{L}$, which required $\mathcal{O}(N^3)$ operations. For kernels with low rank $r \ll N$, the computational cost of the eigendecomposition reduces to $\mathcal{O}(r^2N)$ by using dual kernel. 
%\textcolor{red}

\subsection{Tree-based algorithms for computational efficiency}
Recent algorithmic advances have introduced tree-based methods to perform expedited sampling of DPPs, focusing on the setting where repeated DPP samples are required via the same kernel. This situation is very common in machine learning applications, such as Stochastic Gradient Descent (abbrv. SGD), recommender systems, coresets, etc. 

When repeated DPP samples are required, economy can be achieved by reusing the eigendecomposition of the kernel, to be computed in the preprocessing step. For the second phase, given the eigendecomposition, the complexity of getting each additional sample is linear in $N$ in Algorithm 2. This dependence can be significantly reduced to $\log N$ by using a tree-based algorithm for fast repeated sampling from DPPs, developed recently in \cite{kulesza-gillenwater}. 

%\textcolor{red}
{
 To elaborate, let $\mathcal S$ be an $L$-ensemble with kernel $\mathbf{L}\in\mathbb{R}^{N\times N}$ of rank $r$. We write $\mathbf{L}=\mathbf{B}^\top \mathbf{B}$, where $\mathbf{B}\in\mathbb{R}^{r\times N}$ and define $\mathbf{C}:=\mathbf{B}\mathbf{B}^\top \in \mathbb{R}^{r\times r}$, called the \textit{dual kernel}. In practice, when the user provides $\mathbf{B}$, the dual kernel $\mathbf{C}$ is preferred in sampling low-rank DPPs, as it reduces the eigendecomposition cost to $\mathcal{O}(r^2N)$. We consider the eigendecomposition of $\mathbf{C}=\sum_{i=1}^r \lambda_i \mathbf{w}_i\mathbf{w}_i^\top$. The first phase of the tree-based algorithm in \cite{kulesza-gillenwater} is the same as the first phase in Algorithm 2, where we use the eigendecomposition to sample a subset of indices $E\subset\{1,2,\ldots,r\}$ by independently picking $i\in E$ with probability $\lambda_i/(\lambda_i+1)$. The advancement comes in the second phase, where instead of directly sampling from a multinomial distribution defined by a projection kernel (obtained in the first phase) and repeatedly updating the kernel as in Algorithm 2 (which is costly), we will repeatedly sample from a binary tree of depth $\log N$. The tree construction, appropriate summary statistics stored in the tree will be done in the pre-processing phase, which is a one-time cost.  }

%\textcolor{red}
{
The construction of the tree is as follows. The root of the tree is the full set of items $[N]$, and each child node is half of its parent's items. We first precompute two $r\times N$ matrices $\mathbf{G}$ and $\mathbf{H}$ 
\[ \mathbf{G}_{ij} := \mathbf{b}_j^\top \mathbf{w}_i \quad,\quad \mathbf{H}_{ij} := \gamma_i \mathbf{G}_{ij}\]
where $\mathbf{b}_j$'s are the columns of $\mathbf{B}\in\mathbb{R}^{r\times N}$ and $\gamma_i:=1/\lambda_i$ for $1\le i \le r$.
The $j^{th}$ columns of $\mathbf{G}$ and $\mathbf{H}$ will be denoted by $\mathbf{g}_j$ and $\mathbf{h}_j$ respectively.
At each tree node $S \subset [N]$, we store a vector $\mathbf{z}^{(S)}\in\mathbb R^r$ and a matrix $A^{(S)}\in \mathbb{R}^{r\times r}$, given by
\[\mathbf{z}^{(S)}_i := \gamma_i \sum_{j\in S} \mathbf{G}_{ij}^2 \quad,\quad \mathbf{A}^{(S)}_{i_1 i_2} := \sum_{j\in S} \mathbf{H}_{i_1 j} \mathbf{H}_{i_2 j}.\]
We remark that a recursive argument can be applied here to efficiently compute these quantities. Indeed, if $S=S_l \cup S_r$, where $S_l$ and $S_r$ are child nodes of $S$, then 
\[\mathbf{z}^{(S)} = \mathbf{z}^{(S_l)} + \mathbf{z}^{(S_r)}\quad,\quad \mathbf{A}^{(S)} = \mathbf{A}^{(S_l)} + \mathbf{A}^{(S_r)}.\]
The algorithm for tree construction is described below. Here note that $\mathrm{Split}$ is a function that partitions a set into two subsets of approximately equal size, and $\circ$ denotes the entrywise product.}
\begin{algorithm} [H] \label{alg:tree-const}
\caption{Tree construction}
\begin{algorithmic}[1]
    \State \textbf{procedure} $\mathrm{ConstructTree}(S,\gamma,\mathbf{G},\mathbf{H})$
    \State \quad \textbf{if} $S=\{j\}$ \textbf{then}
    \State \quad \quad $\mathbf{z} \leftarrow \gamma \circ \mathbf{g}_j^2$
    \State \quad \quad $\mathbf{A} \leftarrow \mathbf{h}_j \mathbf{h}_j^\top$
    \State \quad \quad \textbf{return} $\mathrm{Tree}(\mathbf{z},\mathbf{A},j)$
    \State \quad $S_l, S_r \leftarrow \mathrm{Split}(S)$ 
    \State \quad $\mathsf{T}_l \leftarrow \mathrm{ConstructTree}(S_l,\gamma,\mathbf{G},\mathbf{H})$
    \State \quad $\mathsf{T}_r \leftarrow \mathrm{ConstructTree}(S_r,\gamma,\mathbf{G},\mathbf{H})$
    \State \quad $\mathbf{z}\leftarrow \mathsf{T}_l.\mathbf{z} + \mathsf{T}_r.\mathbf{z} $
    \State \quad $\mathbf{A}\leftarrow \mathsf{T}_l.\mathbf{A} + \mathsf{T}_r.\mathbf{A} $
    \State \textbf{return} $\mathrm{Tree}(\mathbf{z}, \mathbf{A}, \mathsf{T}_l,\mathsf{T}_r)$
\end{algorithmic}
\end{algorithm}

%\textcolor{red}
{
Given the binary tree constructed via Algorithm 3, we sample from the DPP as follows. Let 
\begin{equation}\label{eq:dpp-def-by-E}
    \mathbf{K}:= \sum_{i\in E} \lambda_i^{-1} (\mathbf{B}^\top \mathbf{w}_i)(\mathbf{B}^\top \mathbf{w}_i)^\top
\end{equation}
be the projection kernel obtained in the first phase. Given a subset $Y\subset [N]$ of selected items, 
we remark that the next item can be directly sampled from the ground set $[N]$ using the conditional kernel
\[\mathbf{K}^Y := \mathbf{K}_{\bar Y} - \mathbf{K}_{\bar Y Y} (\mathbf{K}_Y)^{-1} \mathbf{K}_{Y\bar Y}, \]
where $\bar Y:= [N]\setminus Y$, and $\mathbf{K}_{AB}$ is the submatrix of $\mathbf{K}$ whose rows are indexed by $A\subset [N]$ and columns are indexed by $B\subset [N]$, and $\mathbf{K}_A := \mathbf{K}_{AA}$. However, $\mathbf{K}^Y$ could be costly to compute. The key idea is that, instead of computing the conditional kernel, we will find the next item by traversing the tree from the root to one of its leaves. 
Assuming that we have reached the node $S$ in the tree, we will go to its left child node $S_l$ with probability 
$\mathrm{Pr}(S_l | Y):= (\sum_{j \in S_l} \mathbf{K}^Y_{jj})/(\sum_{j \in S} \mathbf{K}^Y_{jj})$,
and go to its right child node $S_r$ with probability $\mathrm{Pr}(S_r|Y) := 1 - \mathrm{Pr}(S_l|Y)$.
We repeatedly doing this until we reach a leaf of the tree, which gives the next item. 
The following proposition shows that we do not have to fully compute the conditional kernel $\mathbf{K}^Y$ to determine $\mathrm{Pr}(S_l|Y)$ and $\mathrm{Pr}(S_r|Y)$. In fact, they can be effectively computed from the information which is already available in the pre-processing phase.
\begin{proposition}[Proposition 1 in \cite{kulesza-gillenwater}] \label{prop:tree-based}
    Let $\mathbf{K}$ be the projection kernel defined by a subset of indices $E\subset \{1,\ldots,r\}$ as in Equation \eqref{eq:dpp-def-by-E}.
    Let $Y$ be a (potentially empty) subset of elements that have already been selected. Then
    \[ \sum_{j\in S} \mathbf{K}^Y_{jj} = \mathbf{1}^\top \mathbf{z}^{(S)}_E - \mathbf{1}^\top \Big [(\mathbf{K}_Y)^{-1} \circ \Big (\mathbf{G}_{EY}^\top \mathbf{A}^{(S)}_E \mathbf{G}_{EY}\Big) \Big ] \mathbf{1},\]
    where $\circ$ denotes entrywise product, and $\mathbf{1}$ is the all-ones vector.
\end{proposition}
  Since the depth of the tree is $\log N$, this tree-based sampling algorithm is in $\mathcal{O}(\log N)$ time, which is a significant improvement over the linea dependence of Algorithm 2. We summarize the argument above in the algorithm below.
}

\begin{algorithm} [H] \label{alg:tree-samp}
\caption{Tree-based sampling}
\begin{algorithmic}[1]
    \State \textbf{procedure} $\mathrm{Sample}(\mathsf{T},\lambda,\mathbf{G},\mathbf{H})$
    \State \quad $E\leftarrow \emptyset, Y \leftarrow \emptyset$
    \State \quad \textbf{for } $i=1,\ldots,r$ \textbf{ do}
    \State \quad \quad $E\leftarrow E\cup\{i\} \text{ w.p. } \lambda_i/(\lambda_i+1)$
    \State \quad $\mathbf{Q}\leftarrow 0\times 0 \text{ matrix}$
    \State \quad \textbf{for } $j=1,\ldots, |E|$ \textbf{ do}
    \State \quad \quad $y \leftarrow \mathrm{SampleItem}(\mathsf{T}, E, Y , \mathbf{G}, \mathbf{Q})$
    \State \quad \quad $Y \leftarrow Y \cup y$
    \State \quad \quad $\mathbf{Q}\leftarrow \mathrm{ExtendInverse}(\mathbf{Q},\mathbf{G}^\top_{Ey}\mathbf{H}_{EY})$
    \State \quad \textbf{return} $Y$
    \State \textbf{procedure} $\mathrm{SampleItem}(\mathsf{T},E,Y,\mathbf{G},\mathbf{Q})$
    \State \quad \textbf{if } $\mathsf{T}$ is a leaf \textbf{then return} item at this leaf
    \State \quad $p_l \leftarrow \mathrm{ComputeMarginal}(\mathsf{T}_l, E , Y, \mathbf{G},\mathbf{Q})$
    \State \quad $p_r \leftarrow \mathrm{ComputeMarginal}(\mathsf{T}_r, E , Y, \mathbf{G},\mathbf{Q})$
    \State \quad \textbf{if } $\mathcal U(0,1) \le \frac{p_l}{p_l + p_r}$ \textbf{ then}
    \State \quad \quad \textbf{return } $\mathrm{SampleItem}(\mathsf{T}_l, E , Y, \mathbf{G},\mathbf{Q})$
    \State \quad \textbf{return } $\mathrm{SampleItem}(\mathsf{T}_r, E , Y, \mathbf{G},\mathbf{Q})$
    \State \textbf{procedure} $\mathrm{ComputeMarginal}(\mathsf{T}, E , Y, \mathbf{G},\mathbf{Q})$
    \State \quad $\Phi \leftarrow \mathbf{G}_{EY}^\top \mathsf{T}.\mathbf{A}_E \mathbf{G}_{EY} $
    \State \quad \textbf{return } $\mathbf{1}^\top \mathsf{T}.\mathbf{z}_E - \mathbf{1}^\top (\mathbf{Q} \circ \Phi) \mathbf{1}$
    \State \textbf{procedure} $\mathrm{ExtendInverse}(\mathbf{Q},\mathbf{u})$
    \State \quad \textbf{return} Inverse of $\mathbf{u}$ appended to $\mathbf{Q}^{-1}$
\end{algorithmic}
\end{algorithm}

More recently, many sophisticated sampling algorithms for DPPs have been explored and developed, extending the tree-based approach or otherwise. 
However, within the limited scope of this article we only address the most fundamental techniques, referring the interested reader to \cite{gartrell_scalablesampling, han2022scalable,kulesza-gillenwater, Tremblay-samplingdpp,anari-samplingdpp} and the references therein for recent developments. 
