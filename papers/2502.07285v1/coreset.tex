In recent years, DPPs have emerged as a powerful sampling technique for various machine learning applications. This includes, but is not limited to, applications of foundational importance such as Stochastic Gradient Descent (abbrv. SGD), the general problem of sampling coresets, sampling for recommender systems, to name a few. 

Roughly speaking, the diversity accorded by a determinantal sample leads to stable statistical outcomes, leading to downstream improvements in performance guarantees and convergence rates in a variety of applications. In addition to applications, this approach gives rise to several natural and interesting mathematical problems related to stochastic properties of DPPs (such as concentration phenomena), some of which have seen activity and progress in the recent past.

In this section, we undertake a discussion of DPPs as a sampling device, and the mathematics and the applications surrounding this paradigm.


\subsection{Coresets in machine learning}

%\textit{Coresets} were first introduced and developed in the context of computational geometry. The intuition behind coresets is to extract a small amount but most relevant information from a large data set. We refer readers to \cite{} for standard references.

In machine learning, learning tasks are often formulated as an optimization problem of minimizing a suitable loss function \cite{Shalev-Shwartz_Ben-David_2014}. Let $\mathcal X$ be a set of size $N$ (called the data set) and $\mathcal F$ be a family of real-valued functions on $\mathcal X$. In this survey, we will focus on a loss function of the following form
\begin{equation} \label{eq:loss}
    L(f) = \sum_{x\in \mathcal X} f(x), \quad f \in \mathcal F.
\end{equation}
This setting includes many classical learning tasks. Here are some examples.

\begin{example}[$k$-means] \label{eg:k-mean}
    Let $k\in \mathbb N$ and $\mathcal F$ contain functions of the following form
    \[ f_{\mathcal C}(x) = \min_{q\in \mathcal C} \|x-q\|_2^2,\]
    where $\mathcal C$ is a set of $k$ points (called \textit{cluster centers}) in $\mathbb R^d$. The $k$-means problem refers to finding a cluster $\mathcal C^*$ that minimizes \eqref{eq:loss}.
\end{example}

\begin{example}[linear regression] \label{ex:linear}
    Let $\mathcal X = \{x_i = (y_i,z_i) \in \mathbb R^d\times \mathbb R: 1\le i \le N \} $. Linear regression corresponds to minimizing \eqref{eq:loss} over 
    \[ \mathcal F = \{(y,z)\mapsto |\langle a, y \rangle + b - z|^2 , a \in \mathbb R^d, b \in \mathbb R\}.\]
\end{example}

When the data size $N$ is large, the computational cost can become prohibitive, and the optimization problem often becomes intractable. A natural approach is to reduce the amount of data by using only a manageable number of representative samples. This idea is formalised by the notion of \textit{coreset} (see \cite{bachem2017coresetML}).
\begin{definition}[Coresets]
    Let $\mathcal S$ be a subset of $\mathcal X$ and $\{w(x),x\in \mathcal S\}$ be some weights associated with $\mathcal S$. We define
    \[L_{\cS}(f):= \sum_{x\in \cS} w(x) f(x).\]
    Given $\varepsilon>0$,  $(\cS,w)$ is called an $\varepsilon$-coreset for $\mathcal F$ if
    \[ \Big |\frac{L_{\mathcal S}(f)}{L(f)} - 1 \Big | \le \varepsilon, \quad \text{ uniformly in } f\in \mathcal F.\]
\end{definition}
The existence of coresets is trivial since $\mathcal X$ with weights $1$ is always a coreset for any $\mathcal F$. In fact, only coresets of small size are interesting. We will focus on constructing coresets of a given size $m \ll N$, for which we will refer to as \textit{coreset problem}.

Constructing deterministic coresets is a challenge, and practical coresets are often specially designed for each particular learning tasks. In fact, practitioners prefer \emph{randomized} coresets, namely, a random subset $\mathcal S$ of $\mathcal X$ being a coreset with high probability.  When $\mathcal S$ is random, a natural choice for the weights is $w(x) = \mathbb P(x\in \mathcal S)^{-1}$. This particular choice makes $L_{\mathcal S}(f)$ an unbiased estimator of $L(f)$ 
\begin{eqnarray*}
\mathbb E[L_{\mathcal S}(f)] = \mathbb E \Big [ \sum_{x\in \mathcal S} \frac{f(x)}{\mathbb P(x\in \mathcal S)} \Big ] 
=
\mathbb E \Big [ \sum_{x\in \mathcal X} \frac{f(x)}{\mathbb P(x\in \mathcal S)} \mathbf{1}_{x\in \mathcal S}\Big ] 
= \sum_{x\in \mathcal X} \frac{f(x)}{\mathbb P(x\in \mathcal S)}\mathbb E[\mathbf{1}_{x\in \mathcal S}] = L(f).    
\end{eqnarray*}

Let $\mathcal S$ be a random subset of $\mathcal X$ and define
\[ L_\cS(f) := \sum_{x\in \cS} \frac{f(x)}{\mathbb P(x\in \cS)}, \quad f\in \mathcal F.\]
The event that $\mathcal S$ is an $\varepsilon$-coreset for $\mathcal F$ can be rewritten as
\begin{equation} \label{eq:coreset}
    \Big \{ \Big |\frac{L_{\mathcal S}(f) - \mathbb E[L_{\mathcal S}(f)]}{\mathbb E[L_\cS(f)]} \Big | \le \varepsilon , \: \forall f\in \mathcal F \Big \}.
\end{equation}
Hence, for $\cS$ to be an $\varepsilon$-coreset of $\mathcal F$ (with high probability), one needs each $L_\cS(f)$ to highly concentrate around its mean, and then performs a chaining argument to obtain an uniform bound over $\mathcal F$.  
Thus, constructing coresets will require \textit{concentration inequalities} for each $L_{\mathcal S}(f)$ and techniques from \textit{chaining}.

\subsection{DPPs for minibatch sampling in SGD}
Before presenting results on DPP-based coresets, we briefly discuss minibatch sampling in stochastic gradient descent (SGD), another important topic in machine learning which turns out to be closely related to our coreset problem. 

Consider the problem of minimizing an empirical loss
\[ \min_{\theta \in \Theta} \frac{1}{N} \sum_{i=1}^N \ell_{\theta}(x_i),\]
where $\mathcal X:= \{x_1,\ldots,x_N\}$ is the data set, and the loss $\ell_{\theta}(\cdot)$ is differentiable in the parameter $\theta$. When $N\gg 1$, the computational cost for the gradient of the objective function becomes expensive. SGD method \cite{Robbins_SGD} refers to building an estimator for the gradient at each iteration of the gradient descent, using a small samples of data points (called a \textit{minibatch}). Given $m\ll N$, a key problem in SGD is to construct minibatches of size $m$ that keep the variance of the gradient estimators as small as possible \cite{NIPS2011_SGD, zhang2017_SGD}. More precisely, we want to construct a (random) subset $\mathcal S \subset \mathcal X$ of size $m$, such that
\[ L_\cS(\theta) := \sum_{x\in \mathcal S} w(x) \nabla_{\theta} \ell_{\theta}(x) \approx \frac{1}{N} \sum_{i=1}^N \nabla_{\theta} \ell_{\theta}(x_i) =: L(\theta),\]
and $\mathbb E|L_\cS(\theta) - L(\theta)|^2$ is small, uniformly in $\theta\in \Theta$. From this point of view, the problem of minibatch sampling in SGD has a similar flavor to the coreset problem we introduced above.

In \cite{OPE-NIPS}, the authors used DPPs to construct minibatch sampling for SGD which yields significant variance reduction, compared to independent sampling. To elaborate, we assume that the data $x_i$'s are drawn i.i.d. from a distribution $\gamma(x)\d x$ on the hypercube $[-1,1]^d$. Using a suitable multivariate OPE and spectral approximation, Bardenet et al. constructed an $N\times N$ orthogonal projection matrix $\mathbf{K}$ of rank $m$. Let $\mathcal S$ be the DPP on $\mathcal X$ defined by the kernel matrix $\mathbf K$ w.r.t. the reference measure $N^{-1} \sum_{i=1}^N \delta_{x_i}$, the estimator is then given by
\[ L_{\mathcal S_\dpp}(\theta) := \sum_{x_i \in \mathcal S} \frac{1}{\mathbf K_{ii}} \nabla_\theta \ell_{\theta}(x_i). \]
Under mild assumptions, they proved that
\begin{theorem}
    With high probability in the data set, we have
    \[ \mathbb E [L_{\mathcal S_\dpp}(\theta) | \mathcal X] = L(\theta) \quad, \quad \var[L_{\mathcal S_\dpp}(\theta)|\mathcal X] = O_P(m^{-(1+1/d)}), \quad \forall \theta \in \Theta. \]
\end{theorem}
For more detailed discussions, we refer the readers to their paper \cite{OPE-NIPS} and the references therein. We remark that, for independent sampling minibatches of size $m$, the variance of $L_\cS(\theta)$ is of order $m^{-1}$. Thus, sampling with DPPs improves the decay rate of the variance in the exponent, which is significant.

\begin{comment}
    \subsection{Coreset construction}
 namely, to construct a random subset $\mathcal S$ of $\mathcal X$ and weights $w(x)$, such that $\mathcal S$ is an $\varepsilon$-coreset for the class $\mathcal F$ with high probability.
\end{comment}

\subsection{Importance sampling for coresets}
We come back to the problem of randomized coresets construction. 
A well-studied method to construct coresets is the so-called \emph{importance sampling} (\cite{bachem2017coresetML}, Chapter 2). Given a proposal probability distribution $q$ on $\mathcal X$ and $m\in \mathbb N$, importance sampling outputs $\mathcal S_\iid = \{X_1,\ldots,X_m\}$, where $X_i \sim_\iid q$, and weights $w(x)=(mq(x))^{-1}$. Thus, 
\begin{equation} \label{eq:coreset_iid}
    L_{\mathcal S_\iid}(f) = \frac{1}{m} \sum_{i=1}^m \frac{f(X_i)}{q(X_i)}\cdot
\end{equation}

It is easy to see that 
\[\mathbb E[L_{\mathcal S_\iid}(f)] = \mathbb E \Big [ \frac{1}{m} \sum_{i=1}^m \frac{f(X_i)}{q(X_i)}\Big ] = \mathbb E \Big [ \frac{f(X_1)}{q(X_1)} \Big ] = \sum_{x\in \mathcal X} \frac{f(x)}{q(x)}q(x) = L(f),\]
which implies $L_{\mathcal S_\iid}(f)$ is an unbiased estimator of $L(f)$, for every $f\in \mathcal F$. Moreover,
\begin{equation} \label{eq:var_iid}
    \var[L_{\mathcal S_\iid}(f)] = \frac{1}{m} \var \Big [ \frac{f(X_1)}{q(X_1)}\Big ] = \frac{1}{m} \sum_{x\in \mathcal X} \Big | \frac{f(x)}{q(x)} - L(f) \Big |^2 q(x). 
\end{equation}

A natural task is then to find a distribution $q$ that makes $\var[L_{\mathcal S_\iid}(f)]$ small uniformly in $f\in \mathcal F$. A candidate for this task is the so-called \textit{sensitivity based importance sampling}, introduced by Langberg and Schulman \cite{sensitivity}. For simplicity, let us consider the case when $\mathcal F$ consists of non-negative functions.

\begin{definition}
    The sensitivity of a point $x \in \mathcal X$ with respect to $\mathcal F$ is defined as
    \[ \sigma(x) := \sup_{f\in \mathcal F} \frac{f(x)}{L(f)} \cdot\]
    The total sensitivity is defined as $\mathfrak S := \sum_{x\in \mathcal X} \sigma (x)$.
\end{definition}

The intuition behind the notion of sensitivity is that those points which contribute more to the loss should be more likely to be sampled.
In practice, computing sensitivity could be a challenge; in fact, we often only have access to upper bounds of sensitivity, namely, a function $s:\mathcal X \rightarrow \mathbb R_+$ such that $s(x) \ge \sigma(x)$ for every $x\in \mathcal X$. A \emph{sensitivity based importance sampling scheme} is to sample $X_i$ i.i.d. from 
$q(x) := s(x)/S$, where $S:= \sum_{x\in \mathcal X} s(x).$ From \eqref{eq:coreset_iid}, we have
\begin{equation}\label{eq:importantsamplingcoreset}
    \frac{L_{\cS_\iid}(f)}{L(f)} = \frac{1}{m}\sum_{i=1}^m \frac{f(X_i)}{L(f) q(X_i)}\cdot
\end{equation}
From the definition of sensitivity and the non-negativity of $f$, we have
    \[0 \le \frac{f(x)}{L(f)q(x)} = S\frac{f(x)}{L(f) s(x)} \le S, \quad \forall x\in \mathcal  X.\]
Applying the classical Hoeffding's inequality for the sum of independent bounded random variables in \eqref{eq:importantsamplingcoreset} gives

\begin{proposition} \label{p:concentration-iid}
    For any $\varepsilon>0$ and for any $f\in \mathcal F$, we have
    \[ \mathbb P \Big ( \Big |\frac{L_{\mathcal S_\iid}(f)}{L(f)} -1 \Big | > \varepsilon \Big ) \le 2 \exp \Big ( -\frac{2m \varepsilon^2}{S} \Big ) \cdot \]
\end{proposition}

The sharper the bound $s(x) \ge \sigma(x)$, the better performance of importance sampling. Obtaining a sharp upper bound on sensitivity could be a challenge and will depend on the specific models (see \cite{bachem2017coresetML}). 

After establishing the concentration result for each  $L_\cS(f)$,  the next step is to obtain a uniform bound over $\mathcal F$.  For this task, it is necessary to quantify the complexity of $\mathcal F$, and the uniform bound will depend heavily on that quantity. In \cite{bachem2017coresetML}, the authors use \emph{Vapnik-Chervonenkis dimension} \cite{VCdim} and \emph{pseudo-dimension} to quantify the complexity of $\mathcal F$, and then apply a chaining argument to obtain the desired upper bound. However, it is important to note that this chaining argument relies on the i.i.d. nature of the sampling scheme. For more details, we refer the readers to \cite{bachem2017coresetML}.

\subsection{DPPs as coresets}

 Intuitively, DPPs avoid situations where a data point is sampled multiple times, leading to redundancy. Furthermore, as we have already observed, DPPs with suitable kernels often result in smaller variance for linear statistics (\cite{OPE-AOAP, OPE-NIPS}). 
Thus, there is hope to improve performance by using DPPs instead of independent sampling.

In \cite{tremblay2019determinantal}, Tremblay et al. provide extensive theoretical and empirical justification for the use of DPPs to construct randomized coresets.  However, the crucial question of whether DPP-based coresets can provide a strict improvement remained open. As mentioned earlier, constructing coresets necessitates concentration inequalities and chaining, and these theories have been well-developed for independent processes. Unfortunately, corresponding results for correlated processes are less established, which could pose an obstacle for sampling with correlated processes.

In a recent result \cite{NIPS2024}, the authors confirm that, with a careful choice of kernels, DPP-based coresets can outperform independently drawn coresets. The key ingredient is a Bernstein-type concentration inequality for linear statistics of DPPs. This result was first investigated in a seminal paper \cite{BREUER2014441} by Breuer and Duits for projection DPPs. In \cite{NIPS2024}, the authors extend this result to the case of DPPs with general Hermitian kernels, and even non-symmetric kernels, which seems to go beyond the state of the art. It is worth noting that DPPs with non-symmetric kernels have recently been shown to be of significant interest in machine learning, but come with a limited theoretical toolbox \cite{gartrell2019learning, gartrell2020scalable, han2022scalable}.

\begin{comment}
    In the context of coresets, a more precise motivation is that sampling with negative dependent processes results in smaller variance of linear statistics compared to independent sampling. To elaborate, we will recall a few standard concepts from point process theory.


\begin{remark}
It is easy to see that $L_{\mathcal S}(f)$ could be realized as a linear statistic
\begin{equation} \label{eq:li-stat}
    L_{\mathcal S}(f) = \sum_{x\in \mathcal S} \frac{f(x)}{\mathbb P(x\in \mathcal S)} = \Lambda_{\mathcal S}\Big ( \frac{f(\cdot)}{\mathbb P(\cdot \in \mathcal S)}\Big ) \cdot 
\end{equation}
\end{remark}

A point process $\mathcal S$ on $\mathcal X$ is said to be \textit{pairwise negatively correlated} (pNC) if 
\[ \mathbb P(x \in \mathcal S, y \in\mathcal S) \le \mathbb P(x\in \mathcal S) \mathbb P(y \in \mathcal S), \quad \forall x,y \in \mathcal X.\]
Given a point process $\mathcal S$ on $\mathcal X$, let us denote by $\mathcal S_{\poi}$ the Poisson point process on $\mathcal X$ with the same first intensity as $\mathcal S$, i.e.,
\[ \mathbb P(x \in \mathcal S_\poi) = \mathbb P(x\in\mathcal S), \quad \forall x\in \mathcal X.\]

\begin{proposition} \label{p:var-reduction}
    Let $\mathcal S$ be a pNC point process on $\mathcal X$ and $\mathcal S_\poi$ be defined as above. Let
    $f: \mathcal X \rightarrow \mathbb R$ be any test function. Then
    \[\var[\Lambda_{\mathcal S}(f)] \le \var[\Lambda_{\mathcal S_\poi}(f)].\]
\end{proposition}

\begin{proof}
    By direct computation, one has
\[ \mathbb E[\Lambda_{\mathcal S}(f)] = \sum_{x\in \mathcal X} f(x) \mathbb P(x\in \mathbb S) = \mathbb E[\Lambda_{\mathcal S_\poi}(f)] \]
and
\begin{eqnarray*}
    \mathbb E [\Lambda_{\mathcal S}(f)^2] &=& \mathbb E \Big [ \sum_{x\in\mathcal S} f(x)^2 + \sum_{x \neq y \in \mathcal S} f(x)f(y)\Big ] \\
    &=& \mathbb E \Big [ \sum_{x\in\mathcal S} f(x)^2 \Big ] + \mathbb E \Big [ \sum_{x \neq y \in \mathcal S} f(x)f(y)\Big ] \\
    &=& \sum_{x\in \mathcal X} f(x)^2 \mathbb P(x\in\mathcal S) + \sum_{x\neq y \in \mathcal X} f(x)f(y) \mathbb P(x\in \mathcal S, y \in \mathcal S) \\
    &\le& \sum_{x\in \mathcal X} f(x)^2 \mathbb P(x\in\mathcal S) + \sum_{x\neq y \in \mathcal X} f(x)f(y) \mathbb P(x\in \mathcal S)\mathbb P(y \in \mathcal S) \\
    &=& \mathbb E[\Lambda_{\mathcal S_\poi}(f)^2],
\end{eqnarray*}
which implies 
$\var[\Lambda_{\mathcal S}(f)] \le \var[\Lambda_{\mathcal S_\poi}(f)]$ as desired.
\end{proof}

Proposition \ref{p:var-reduction} provides a hint about the potential of sampling with negative dependent processes. However, applying this to the context of coresets requires more than just that.
\end{comment}




\subsubsection{Concentration inequalities for linear statistics of DPPs}

In \cite{PEMANTLE_PERES_2014}, Pemantle and Peres introduced concentration inequalities for Lipschitz functionals of probability measures on $2^{\mathcal X}$ (equivalently, random subsets of $\mathcal X$) satisfying the \textit{stochastic covering property} (SCP). 
We refer the readers to \cite{PEMANTLE_PERES_2014} to the precise definition of SCP. However, we remark that the class of probability measures on $2^{\mathcal X}$ satisfying the SCP particularly includes DPPs.

For simplicity, we only state here the result for $m$-homogeneous point processes (i.e., point processes with a.s. $m$ points). For the general case, we refer the readers to \cite{PEMANTLE_PERES_2014}.
\begin{theorem}[Theorem 3.1 in \cite{PEMANTLE_PERES_2014}] \label{t:concentration-lips}
    Let $\mathcal S$ be an $m$-homogeneous point process on $\mathcal X$ satisfying the SCP. Let $F$ be a $1$-Lipschitz function (with respect to the total variation distance) on counting measures with total mass $m$ on $\mathcal X$. Then
    \[\mathbb P(F(\mathcal S) - \mathbb E[F(\mathcal S)] \ge a ) \le \exp \Big (-\frac{a^2}{8m}\Big), \quad \forall a \ge 0.\]
\end{theorem}

\begin{comment}
    The SCP property is crucial in this approach. In particular, the authors decomposed $F(\mathcal S) - \mathbb E [F(\mathcal S)]$ as a martingale, and the bounded differences property was verified using the SCP and the Lipschitz of $F$. The concentration inequality follows from the classical Azuma-Hoeffding inequality.
\end{comment}
This concentration inequality has a similar flavor to the classical Hoeffding inequality, in the sense that it provides a Gaussian-tail bound for $F(\mathcal S) - \mathbb E[F(\mathcal S)]$ but does not take the variance into account. 

Since we are particularly interested in linear statistics of DPPs, we will apply Theorem \ref{t:concentration-lips} to the particular case where $\mathcal S$ is a projection DPP of rank $m$, and $F(\mathcal S) = \sum_{x\in \mathcal S} f(x) = \Lambda_{\mathcal S}(f)$. It is easy to see that if $\|f\|_\infty := \sup_{x\in \mathcal X}|f(x)|\le 1$ then $\Lambda_{\mathcal S}(f)$ is $1$-Lipschitz.

\begin{corollary} \label{c:concentration-lips}
    Let $\mathcal S$ be a projection DPP on $\mathcal X$ of rank $m$ and let $f:\mathcal X\rightarrow \mathbb R$ be such that $\|f\|_\infty \le 1$. Then
    \[\mathbb P(\Lambda_{\mathcal S}(f) - \mathbb E[\Lambda_{\mathcal S}(f)] \ge a ) \le \exp \Big (-\frac{a^2}{8m \|f\|_\infty^2}\Big), \quad \forall a \ge 0.\]
\end{corollary}

Let us apply Corollary \ref{c:concentration-lips} to the setting of coresets. We remark that $L_{\mathcal S}(f)$ could be realized as a linear statistic
\begin{equation} \label{eq:li-stat}
    L_{\mathcal S}(f) = \sum_{x\in \mathcal S} \frac{f(x)}{\mathbb P(x\in \mathcal S)} 
    = \sum_{x\in \mathcal S} \frac{f(x)}{K(x,x)} 
    = \Lambda_{\mathcal S}\Big ( \frac{f(\cdot)}{K(\cdot,\cdot)}\Big ) ,
\end{equation}
where we used the fact that if $\cS$ is a DPP with kernel $K$ and the background measure is the counting measure on $\mathcal X = \{x_1,\ldots,x_N\}$, then $\mathbb P(x\in \cS) = K(x,x), \: \forall x\in \mathcal X$.
Using \eqref{eq:li-stat}, one has
\begin{equation} \label{eq:coreset-linearstat}
    \mathbb P \Big (\Big |\frac{L_{\mathcal S}(f)}{L(f)}-1\Big | \ge \varepsilon \Big ) 
= \mathbb P \Big (\Big | \Lambda_{\mathcal S}\Big (\frac{f(\cdot)}{L(f) K(\cdot,\cdot)} \Big ) - \mathbb E \Lambda_{\mathcal S}\Big (\frac{f(\cdot)}{L(f)K(\cdot,\cdot)} \Big ) \Big | \ge \varepsilon \Big ) \cdot
\end{equation}
If $|\mathcal S| = m$ a.s., then $K(x,x)=\mathbb P(x \in \mathcal S)$ is typically $m/N$. For a bounded non-negative function $f$, $L(f)$ is of order $N$. Thus,
\[ \Big \| \frac{f(\cdot)}{L(f) K(\cdot,\cdot)}  \Big \|_\infty \sim \frac{1}{m} \cdot \]
Applying Corollary \ref{c:concentration-lips} will give a concentration inequality of the form
\begin{equation} \label{eq:concentration-coreset-iid}
    \mathbb P \Big (\Big |\frac{L_{\mathcal S}(f)}{L(f)}-1\Big | \ge \varepsilon \Big ) \le 2 \exp (-Cm\varepsilon^2 ),
\end{equation}
for some constant $C>0$ (possibly depending on $f$), which is not better than independent sampling (c.f. Proposition \ref{p:concentration-iid}). One possible reason is that the concentration inequalities in Theorem \ref{t:concentration-lips} and Proposition \ref{c:concentration-lips} do not take the variance into account; meanwhile the variance reduction for linear statistics of DPPs is one of the main motivation for sampling with DPPs.
Thus, a concentration inequality for linear statistics of DPPs which takes variance into account is a desire. As we introduced earlier, such results have been investigated in \cite{BREUER2014441, NIPS2024}. 
\begin{theorem}[Hermitian kernels]\label{t:concentration-dpp}
    Let $\mathcal S$ be a DPP on $\mathcal X$ with a Hermitian kernel $K$. Let $f:\mathcal X \rightarrow \mathbb R$ be a bounded test function, then
    \[ \mathbb P(|\Lambda_{\mathcal S}(f) - \mathbb E \Lambda_{\mathcal S}(f) | \ge a ) \le 2 \exp \Big ( - \frac{a^2}{4A\var[\Lambda_{\mathcal S}(f)]} \Big ), \quad \forall 0 \le a \le \frac{2A \var[\Lambda_{\mathcal S}(f)]}{3\|f\|_\infty}, \]
    where $A>0$ is an universal constant.
\end{theorem}

\begin{theorem}[Non-symmetric kernels] \label{t:concentration-dpp-nonsym}
    Let $\mathcal S$ be a DPP on $\mathcal X$ with a non-symmetric kernel $K$. Let $f: \mathcal X \rightarrow \mathbb R$ be a bounded test function, then
    \[\mathbb P(|\Lambda_{\mathcal S}(f) - \mathbb E[\Lambda_{\mathcal S}(f)]| \ge a ) 
    \le 2 \exp \Big ( -{a^2 \over 4\var[\Lambda_{\mathcal S}(f)]} \Big ), 
    \: \forall 0 \le a \le {\var[\Lambda_{\mathcal S}(f)]^2 \over  40\|f\|_\infty^3 \cdot \max(1,\|K\|_\op^2) \cdot  \|K\|_* } ,\]
    where $\|\cdot\|_\op$ denotes the spectral norm and $\|\cdot \|_*$ denotes the nuclear norm of a matrix.
\end{theorem}

%We recall the celebrated Campbell formula for the Laplace transform of linear statistics of DPPs
%\[ \mathbb E[e^{t \Lambda_{\mathcal S}(f)} ] = \det [I- M_{1-e^{tf}}K].\]
%Using Taylor expansion, one could deduce the formula for cumulants of $\Lambda_{\mathcal S}(f)$ as follows
%\[ \log \mathbb E[e^{t\Lambda_{\mathcal S}(f)}] = \sum_{k=1}^\infty C_k(f) \frac{t^k}{k!},\]
%where
%\begin{equation} \label{eq:cumulant}
%C_k(f) = \sum_{q=1}^k \frac{(-1)^{q+1}}{q} \sum_{\underset{k_1,\ldots,k_q \ge 1}{k_1 + \ldots + k_q =k}} {k! \over k_1! \ldots k_q!} \Tr[ f^{k_1} K \ldots f^{k_q} K].
%\end{equation}
%In particular, $C_1(f) = \mathbb E[\Lambda_{\mathcal S}(f)], C_2(f) = \var[\Lambda_{\mathcal S}(f)]$. The explicit formula \eqref{eq:cumulant} for cumulants is the main ingredient in the approach of \cite{} and \cite{} to prove their concentration inequalities.
\begin{remark}
    For simplicity, we will restrict our attention to the case of Hermitian kernels. However, we remark that we always can use Theorem \ref{t:concentration-dpp-nonsym} to obtain analogous results in the non-symmetric case.
\end{remark}


Applying Theorem \ref{t:concentration-dpp} to Equation \eqref{eq:coreset-linearstat} gives 
\begin{corollary} \label{c:coreset-dpp}
      Let $\mathcal S$ be a DPP on $\mathcal X$ with a Hermitian kernel $K$ and $f:\mathcal X \rightarrow \mathbb R$ be a bounded measurable function. Then
      \[ \mathbb P \Big ( \Big |\frac{L_{\mathcal S}(f)}{L(f)} -1 \Big | \ge \varepsilon \Big ) \le 2 \exp \Big ( -\frac{\varepsilon^2}{4A} \var \Big [ \frac{L_{\mathcal S}(f)}{L(f)}\Big ]^{-1} \Big ),\]
      for $0 \le \varepsilon\le \frac{2A}{3} \cdot \var \Big [ \frac{L_{\mathcal S}(f)}{L(f)}\Big ] \cdot 
      \Big \| \frac{f(\cdot)}{L(f) K(\cdot,\cdot)} \Big \|_\infty^{-1}$.
\end{corollary}

To compare, assume that $\mathbb E[|\mathcal S|]=m$, then typically $\var[ L_{\mathcal S}(f)/L(f)] = O(m^{-(1+\delta)})$,  for some parameter $\delta>0$, depending on the specific model (see \cite{OPE-NIPS}, \cite{OPE-AOAP}). Hence, Corollary \ref{c:coreset-dpp} yields a concentration inequality of the form
\begin{equation} \label{eq:concentration-coreset-dpp}
    \mathbb P \Big ( \Big |\frac{L_{\mathcal S}(f)}{L(f)} -1 \Big | \ge \varepsilon \Big ) \le 2 \exp ( -C m^{1+\delta}\varepsilon^2  ),
\end{equation}
for some constant $C>0$, which is clearly tighter than \eqref{eq:concentration-coreset-iid}.

A limitation of Corollary \ref{c:coreset-dpp} is the range of $\varepsilon$ for which the concentration inequality is valid. Extending this result to all non-negative reals $\varepsilon$ is a desire. However, this task is really challenging and remains open.

\subsubsection{PAC bound}
As we remarked earlier, the uniform bound over $\mathcal F$ will particularly depend on the complexity of the class $\mathcal F$; thus, an universal chaining argument for all cases could be a challenge. We will restrict ourselves to some common functional classes in machine learning.

Before going to details, let us assume here an assumption, which is not harmful due to homogeneity (namely, $L_\cS(f)/L(f) = L_\cS(\lambda f)/L(\lambda f)$ for any $\lambda \neq 0$). However, this assumption will be useful later.
\begin{equation} \tag{A.0} \label{eq:lowerbound}
   \frac{1}{N}|L(f)| \ge c , \quad \text{ for some } c>0 \text{ uniformly over } \mathcal F.
\end{equation}

The first scenario is when $\mathcal F$ is a subset of a finite dimensional vector space of functions. In particular, we assume that
\begin{equation} \tag{A.1} \label{eq:finite-dim}
    \dim \span_{\mathbb R}(\mathcal F) = D < \infty.
\end{equation}
This assumption covers common situations like linear regression in Example \ref{ex:linear}, where we observe that each $f\in \mathcal F$ is a quadratic function in $(d+1)$ variables. 
Thus the dimension of the linear span of $\mathcal F$ is at most $(d+1)^2 + (d+1) + 1$. 
Another popular class is the class of \emph{band-limited functions}, originating in signal processing problems.
A function $f : \mathbb{T}^d \mapsto \mathbb{R}$ (where  $\mathbb T^d$ denotes the $d$-dimensional torus) is said to be \textit{band-limited} if there exists $B \in \mathbb N$ such that its Fourier coefficients $\hat f (k_1,\ldots,k_d) =0$ whenever there is a $k_j$ such that $|k_j| > B$. 
It is easy to see that the space of $B$-bandlimited functions is of the dimension at most $(2B+1)^d$.

Another common scenario is when $\mathcal F$ is parametrized by a finite-dimensional parameter space
\begin{equation} \tag{A.2} \label{eq:parametrize}
    \mathcal F = \{f_\theta: \theta \in \Theta\}, \text{ where $\Theta$ is a bounded subset of $\mathbb R^D$ for some $D$}.
\end{equation}
We further assume a Lipschitz condition on $\mathcal F$
\begin{equation} \tag{A.3} \label{eq:lipschitz}
   \|f_{\theta} - f_{\theta'}\|_\infty \le \ell \|\theta - \theta'\| \text{ for some $\ell>0$, uniformly on $\Theta$.}
\end{equation}
These conditions particularly cover the $k$-means problem of Example~\ref{eg:k-mean}, as well as linear/non-linear regression settings. 
For instance, for $k$-mean, each query is parametrized by its cluster centers $\mathcal C= \{q_1,\ldots,q_k\}$, which can be viewed as a parameter $(q_1,\ldots,q_k)$ in $ \mathbb R^{kd}$.  

In these two common situations, we can perform a chaining argument to obtain the following result (see \cite{NIPS2024}).

\begin{theorem}\label{t:core-union}
    Let $\mathcal S$ be a DPP with a Hermitian kernel $\bK$ on a finite set $\mathcal X = \{x_1, \dots, x_N\}$ and $m = \mathbb E[|\mathcal S|]$. Assume that for all $i\in\{1, \dots, N\}$, $\bK_{ii} \ge \rho \cdot m/N$ for some $\rho> 0$ not depending on $m,N$. 
    Let $V\ge \sup_{f\in \mathcal F} \Var[N^{-1} L_{\mathcal S}(f)]$. 
    \medskip
    
    Assuming \eqref{eq:lowerbound} and \eqref{eq:finite-dim}, we have
   \[\mathbb P
   \Big ( \exists f \in \mathcal F: 
  \Big |\frac{ L_{\mathcal S}(f)}{L(f)}-1 \Big | \ge \varepsilon \Big ) \le 2 \exp \Big (6D - {c^2 \varepsilon^2 \over 16 AV} \Big ), \quad 0 \le  \varepsilon \le  {4A\rho m V \over 3c \sup_{f\in \mathcal F}\|f\|_\infty} \cdot \]

  Assuming \eqref{eq:lowerbound}, \eqref{eq:parametrize}, \eqref{eq:lipschitz} and $|\mathcal S| \le B \cdot m$ a.s. for some $B>0$, we have
   \[\mathbb P
   \Big ( \exists f \in \mathcal F: 
  \Big |\frac{L_{\mathcal S}(f)}{L(f)}-1 \Big | \ge \varepsilon \Big ) \le  2 \exp \Big (CD- D\log \varepsilon - {c^2 \varepsilon^2 \over 16 AV}\Big ), \: 0 \le  \varepsilon \le  {4A\rho mV \over 3c \sup_{f\in \mathcal F}\|f\|_\infty} \cdot \]
  Here $A>0$ is a universal constant and $C=C(\Theta, B,\rho,\ell, c)>0$ is some constant.
\end{theorem}


   For independent sampling, $\Var[N^{-1}  L_{\mathcal S}(f)]$ is of order $O(m^{-1})$. In contrast, sampling with DPPs often results in a variance of order $ O(m^{-(1+\delta)})$ for some $\delta > 0$ (e.g., see \cite{OPE-AOAP, OPE-NIPS}). 
   Therefore, the range of $\varepsilon$ for which we can apply our concentration result is $ O(m^{-\delta})$. 
    Substituting $\varepsilon = m^{-\alpha} (\alpha \ge \delta)$ into Theorem \ref{t:core-union}  yields the upper bounds $2\exp(6D - C' m^{1+\delta - 2\alpha})$ and $2\exp(CD + \alpha D \log m - C' m^{1+\delta - 2\alpha}) $, where $C$ and $C'$ are positive constants independent of $m$ and $N$. Both of them converge to $0$ as $m\rightarrow \infty$, provided $\alpha < (1+\delta)/2$. Hence, the accuracy rate $\varepsilon$ can be as small as $m^{-1/2 -\delta'/2}$, for any $0<\delta' <\delta$, which clearly outperforms the best accuracy rate of $m^{-1/2}$ for i.i.d. sampling.


The assumptions in Theorem \ref{t:core-union} are fairly moderate and reasonable. For i.i.d. sample $\mathcal S$ with expected size $m$, the probability $\mathbb P(x \in \mathcal S) = m/N$  applies to each $x\in \mathcal X$. For a DPP sample $\mathcal S$ with kernel $\bK$, we have $\mathbb P(x_i\in \mathcal S) = \bK_{ii}$. 
Therefore, assuming $\bK_{ii} \ge \rho \cdot m/N$ for each $i$ and some $\rho>0$ ensures that every point in the dataset $\mathcal X$ has a fair chance of being selected. 
This also helps prevent the estimated loss $L_{\mathcal S}(f) = \sum_{x_i\in \mathcal S} f(x_i)/\bK_{ii}$ from blowing up to infinity. In the second scenario, the assumption $|\mathcal S|\le B \cdot m$ a.s. for some $B>0$ is not strictly required but is included to simplify the presentation of the results. Indeed, for a DPP $\mathcal S$ with Hermitian kernel, it is known that $|\mathcal S|$ is equal in distribution to the sum of independent Bernoulli variables, which consequently implies that $|\mathcal S|$ highly concentrates around its mean $m$. This property allows us to relax the assumption on the size of $|\mathcal S|$ by applying a conditioning argument.  However, the  assumption $|\mathcal S| \le B \cdot m$ a.s. holds for most kernels of interest; DPPs with projection kernels being typical and significant examples. In machine learning terms, this means that the coresets are not much bigger than their expected size $m$; whereas in practice, sampling methods often produce coresets of a fixed size, as observed with projection DPPs. We refer the readers to \cite{NIPS2024} for more detailed discussion.