\subsection{Background}

 Pruning neural networks consists in reducing the number of neurons in the network. Neural network pruning is a central question when it comes to optimizing neural network performances. Mariet and Sra developed a method, DIVNET in \cite{mariet_diversity_2017}, for  pruning neural network using DPP which has proven to be empirically efficient and is independent of the choice of other parameters such as the activation function or the number of hidden layers. We will describe the method for feed forward neural networks.

Their method can be defined in the following way. For a neural network with $M$ hidden layers trained on a database $\mathcal{T}$. For $1 \leq l \leq M$ assume the $l$-th layer has $n_l$ neurons. Let $1\leq i \leq n_l$ and $1 \leq j \leq n_{l+1}  $, the trainable weight from neuron $i$ in layer $l$ to neuron $j$ in layer $l+1$ is $W_{ij}^l$.  Define the activation vectors recursively :

\[
\begin{cases}
    v_1^0 = \mathcal{T} \\
    \forall 0 \leq l \leq M-1 \text{ , } \forall 1 \leq j \leq n_{l+1} \text{ , } v_j^{l+1} = \sigma ( \sum \limits_{i=1}^{n_l} W_{ij}^l v_i^l)
\end{cases}
\]


Assume  we want to prune the neural network on the $l$-th layer. Define then the kernel with respect to which the DPP is going to be sampled : 

\[ L'_l = ( \exp(-\beta \lVert v_i^l-v_j^l \rVert ^2))_{1\leq  i,j \leq n_l} \]

Here $\beta$ is a hyperparameter, which empirically is chosen to be of the magnitude $10/\lvert \mathcal{T} \rvert$. For better numerical stability, add a regularization term $\epsilon I$ so that the real kernel used is :

$$L_l = L_l' + \epsilon I $$ 

From here fix $k_n$ the number of neurons to keep in the layer and sample $\mathcal{S}$ a $k_n$-DPP with kernel $L_l$. For each $1 \leq j \leq n_{l+1}$, to improve the performances and minimize the information lost in the process, they introduce a reweighting phase which takes the following shape : 

\begin{equation} \label{eqn:reweighting}
\Tilde{\mathbf{W}_j^l} = \underset{\Tilde{\mathbf{W}_j^l} \in \mathbb{R}^{k}}{\operatorname{argmin}} \lVert \sum \limits_{i =1}^{n_l} W_{ij}^l v_i^l - \sum \limits_{i \in \mathcal{S}} \Tilde{W}_{ij}^l v_i^l \rVert 
\end{equation}

This method has proven to be particularly effective empirically while not requiring a retraining phase. Correctly understanding its theoretical implications is still an open question.

Looking at it through the lens of statistical mechanics offers some insight. This framework was already introduced in \cite{acharyya_statistical_2021}.
We aim at offering a comprehensive overview of the methods coming from statistical mechanics applied to the study of neural network pruning. We also extend and formalize the results from \cite{acharyya_statistical_2021} to more general settings. 

The analysis presented in this section was originally developed in the masters thesis \cite{petrovic2023pruning} of co-author V. Petrovic .


\subsection{The student/teacher framework}


The student/teacher framework is a common setting in statistical mechanical analysis of neural networks. The full theory behind this model is fully explained and detailed in \cite{engel_statistical_2001}. The model that we will use in this section comes from \cite{goldt_dynamics_2020} in the noiseless regime.

The framework is the following, assume we are given a data set $(X_\mu)_{1 \leq \mu \leq p}$ of i.i.d. Gaussian variables which take values in $\mathbb{R}^d$, of mean $0$ and variance matrix $I_d$. Let us get two neural networks respectively called the teacher and the student. The teacher network acts as a black box, to which we have access to the activation function and the number of layers and neurons per layer. It gets as inputs the data set $(X_\mu)_{1 \leq \mu \leq p}$ and assigns them outputs $(Y_\mu)_{1 \leq \mu \leq p}$, where if $1 \leq \mu \leq p$, $Y_\mu = \phi^*(X_\mu)$, where $\phi^*$ is a deterministic function. The student network is then another neural network, on which we have full control of the architecture, that we train to reproduce the results of the teacher network. In other words, the student network performs supervised learning with inputs $(X_\mu, Y_\mu)_{1 \leq \mu \leq p}$. Finally, let $g$ be the activation function, which is chosen to be the sigmoid function $\operatorname{erf}$. 
For the rest of the section, we will use $i,j,k$ to describe nodes from the student network and $n,m$ to describe nodes from the teacher network.

Here we will give special attention to the two layers student/teacher framework. In this case, both the student and the teacher framework have one hidden layer. Assume then, that the teacher network has $M$ hidden units and the student one has $K$ hidden units with $M \leq K$. The output of the teacher network is then:

\begin{equation*}
Y_\mu = \sum \limits_{m=1}^M v_m^* g(\dfrac{(w_m^{*})^T X_\mu}{\sqrt{N}}) = \phi^{*}(X_\mu) 
\end{equation*}  
The output of the student network is: 

\begin{equation*}
 \phi(X_\mu) = \sum \limits_{i=1}^K v_k g(\dfrac{w_k^T X_\mu}{\sqrt{N}})
\end{equation*}

The student network is trained to learn the teacher network on the empirical quadratic loss, defined as follows:

$$ L(\phi) = \dfrac{1}{2}\sum \limits_{\mu = 1}^p (\sum \limits_{i=1}^K v_k g(\dfrac{w_k^T X_\mu}{\sqrt{N}})-Y_\mu)^2$$

Define also the generalization error as: 

$$\epsilon_g (\phi) = \dfrac{1}{2}\braket{(\phi(X)- \phi^*(X))^2}$$

where $\braket{\cdot}$ is the average over the input distribution.

Finally introduce the following macro-parameters:

\[ Q_{ik} = \dfrac{w_i^Tw_k}{N} \text{ , }  T_{mn} = \dfrac{(w_m^*)^Tw_n^*}{N} \text{ , }  R_{in} =\dfrac{(w_n^*)^Tw_i}{N}  \]

The whole analysis will revolve around the dynamics established by Goldt et al. in \cite{goldt_dynamics_2020} .

\subsection{Dynamics of the macroparameters in online learning} \label{sec:dynamics}

Goldt et al establish very general results on the evolution of the paramaters in online learning. In the following, we will focus on a specific case mentioned by the authors of \cite{goldt_dynamics_2019} and further assume that:

\begin{itemize} \label{list:hypothesis}
    \item (H1) The dataset is large enough so that we visit each sample at most once during training.
    \item (H2) We are in the regime $N \rightarrow \infty$
\end{itemize}

In this context, $v$ satisfies:

\begin{equation} \label{eqn:evov}
\forall 1 \leq i \leq K \text{, } \dfrac{dv_i}{dt} = \eta_v[\sum \limits_{m=1}^M v_m^* I_2(i,m) - \sum \limits_{j =1}^K v_j I_2(i,j)] 
\end{equation}

where:

$$\forall 1 \leq i,k \leq K \text{, } I_2(i,k) = \braket{g(\lambda_i) g(\lambda_k)} =  \dfrac{1}{\pi} \arcsin{\dfrac{Q_{ik}}{\sqrt{1+Q_{ii}} \sqrt{1+Q_{kk}}}}$$

$$\forall 1 \leq i \leq K \text{, } \forall 1 \leq n \leq M \text{, } I_2(i,n) = \braket{g(\lambda_i)g(\rho_n)} = \dfrac{1}{\pi} \arcsin{\dfrac{R_{in}}{\sqrt{1+Q_{ii}} \sqrt{1+T_{nn}}}}$$

Let us also introduce the following quantity that plays a role in our coming analysis:

$$\forall 1 \leq m,n \leq M \text{, } I_2(m,n) = \braket{g(\rho_m)g(\rho_n)} = \dfrac{1}{\pi} \arcsin{\dfrac{T_{mn}}{\sqrt{1+T_{nn}} \sqrt{1+T_{mm}}}}$$

The authors of \cite{goldt_dynamics_2020} establish the following theorem:

\begin{theorem}
    With all the previous assumptions, the generalisation error $\epsilon_{g}$ satisfies: 

    \begin{equation} \label{eqn:generror}
 \epsilon_g(\phi) = \sum \limits_{i,k \in [K]} v_i v_k I_2(i,k) + \sum \limits_{n,m \in [M]} v_n^* v_m^* I_2(n,m) - 2 \sum \limits_{i \in [K], n \in [M]} v_i v_m^* I_2(i,n)
    \end{equation}
\end{theorem}

This formula is the key of our coming analysis of the effect of the pruning and reweighting phase using DPPs. 



\subsection{Application to neural network pruning}
We assume that $T_{nm} = \delta_{nm}$ (as \cite{acharyya_statistical_2021}), i.e., that we work in the ideal case where the weights in the teacher network are a subfamily of orthonormal vectors. Assume further that both hypothesis (H1) and (H2) from section \ref{sec:dynamics} hold. 

 In this section, $\mathcal{S}$ describes the subset selected after pruning (i.e. the complementary of the pruned neurons) and $k_n = \lvert \mathcal{S} \rvert$. We also fix $\epsilon_{\operatorname{DPP}}$ (resp. $\hat{\epsilon}_{\operatorname{DPP}}$) the generalization error before reweighting (resp. after reweighting) of the pruned network.

The main goal of this section is to prove the following theorem:

\begin{theorem} \label{thm:nndpp}
    For any other sampling process $S$, with same marginal inclusion probabilities (ie $\mathbb{P}( i \in S) = \mathbb{P}_{\operatorname{DPP}}(i \in S)$ for all $1 \leq i \leq K$ ) the expectation (over the random sample) of the generalization error when the neural network is pruned and reweighted $\hat{\epsilon}_S$ satisfies:

    \[ \mathbb{E} [\hat{\epsilon}_S] \geq \mathbb{E}_{\operatorname{DPP}} [\hat{\epsilon}_{\operatorname{DPP}}] \]

\end{theorem}

This proposition asserts that in expectation, DPP sampling is the one that performs the best among any other random sampler in this specific framework.

\begin{remark}
    This is a generalization of \cite{acharyya_statistical_2021} in the following sense:
    \begin{itemize}
        \item The weights in the teacher network are not assumed to be all equal, likewise for the student network.
        \item The size of the subcells in the student network are not all equal. The number of neurons in the student network is not assumed to be a multiple of the number of neurons in the teacher network.
    \end{itemize}
\end{remark}

\subsection{Proof of the theorem}

The proof of this theorem can be decomposed in several parts.

\subsubsection{The student network achieves perfect reconstruction}

We first notice that in a specific context, the student network achieves perfect reconstruction. More precisely:


\begin{proposition} \label{prop:perfectrec}
    Let us write $\{1,\dots,K\} = \bigsqcup \limits_{m = 1}^M G_m $ where each $G_m$ is non empty and the $G_m$ are pairwise disjoint. This partition gives an equivalence relation on $[K]$, that we will write as $\sim$. Then, if:

    \begin{equation} \label{eqn:hyponw}
    \forall i \in \{1,\dots,K\} \text{, } \lVert w_i \rVert = \sqrt{N}
    \end{equation}

    \begin{equation} \label{eqn:hyponR}
    R_{in} = \begin{cases}
    1 & \text{ if } i \in G_n\\
    0 & \text{ otherwise } 
    \end{cases}
    \end{equation}

    we have $\phi \equiv \phi^*$ after training.
\end{proposition}

\begin{proof}

    First, notice that  $T_{nn} = 1$  means that $ \lVert w_n^* \rVert = \sqrt{N}$. Hence, if $i\in G_n$:

    $$ w_i^T w_n^* = \lVert w_i \rVert \lVert w_n^* \rVert = N R_{in}$$

    So we are in the equality case of the Cauchy-Schwartz inequality, $w_i$ and $w_n^*$ are colinear, of same norm and their scalar product is positive. This means that $w_i = w_n^*$. Observe that this implies straightforwardly, since $T_{nm} = \delta_{nm}$:

    \begin{equation} \label{eqn:hyponQ}
    Q_{ik} = \begin{cases}
    1 & \text{ if } i \sim k\\
    0 & \text{ otherwise } 
    \end{cases}
    \end{equation}

    Now, notice that, if $\mathbbm{1}$ is the indicator function, under equation \ref{eqn:hyponQ} and equation \ref{eqn:hyponR}: $I_2(i,k) = \frac{1}{6} \mathbbm{1}_{i \sim k}$ and  $I_2(i,n) = \frac{1}{6} \mathbbm{1}_{i \in G_n} $, where $I_2$ was introduced in the section \ref{sec:dynamics}.

    After training, the algorithm is assumed to reach a stable minimizer, which means $\frac{dv_i}{dt} = 0$ for all $i \in [K]$. Using equation \ref{eqn:evov}, we get that, after training, if $i \in G_n$:

    \begin{equation*} 
        \dfrac{dv_i}{dt} = 0 = \dfrac{\eta_v}{6}[v_n^* - \sum \limits_{i \in G_n} v_i]
    \end{equation*}

    Hence: 

    \begin{equation}\label{eqn:stablev}
        v_n^* = \sum \limits_{i \in G_n} v_i    
    \end{equation}


    Now, if $x\in \mathbb{R}^d$:

   % \begin{equation*}
    \begin{eqnarray*}
     \phi(x) &= &\sum \limits_{i = 1}^K v_i g\Big (\frac{w_i^Tx}{\sqrt{N}} \Big ) =\sum \limits_{i = 1}^K v_i g \Big (\frac{w_i^Tx}{\sqrt{N}} \Big ) = \sum \limits_{m = 1}^M \sum \limits_{i \in G_m} v_i g \Big (\frac{w_i^Tx}{\sqrt{N}} \Big ) \\ 
     &= &\sum \limits_{m = 1}^M g \Big (\frac{(w_m^*)^Tx}{\sqrt{N}} \Big )\sum \limits_{i \in G_m} v_i  = \sum \limits_{m = 1}^M v_m^* g \Big (\frac{(w_m^*)^Tx}{\sqrt{N}} \Big ) = \phi^*(x)
    \end{eqnarray*}    
   % \end{equation*}    
    \end{proof}

This establishes that the learning process achieves perfect reconstruction, when it reaches the fixed point corresponding to equation \ref{eqn:hyponQ} and equation \ref{eqn:hyponR}. Notice here that no assumptions were made on the size of the elements of the partition.


\subsubsection{Computing the generalization error}

The aim of this section is to find the generalization error, when we work in the framework of Proposition \ref{prop:perfectrec}. Here, we will not put much emphasis on the sampling process. Assume that some sampling process was used to produce $\mathcal{S} \subset [K]$, and that we apply some reweighting process to the second layer. Once again, assume the framework from section \ref{sec:dynamics} holds. Define then $q = \lvert \{ m | G_m \cap \mathcal{S} \neq \emptyset \} \rvert$, and, assume that $ \operatorname{EXP} = \{ m | G_m \cap \mathcal{S} \neq \emptyset \}$  are the explained neurons ($\operatorname{EXP}$ stands for explained).

\begin{theorem}\label{thm:geneerrorreweight}
    Let $(\tilde{v_i})_{i\in \mathcal{S}}$ be the reweighted weights for some reweighting procedure we apply to the second layer after pruning. Then:

    $$\hat{\epsilon}_g = \dfrac{1}{6} \sum \limits_{m \in \operatorname{EXP}} (v_m^* - \sum \limits_{i \in G_m \cap S} \Tilde{v}_i)^2 + \dfrac{1}{6} \sum \limits_{m \notin \operatorname{EXP}} (v_m^*)^2 $$
\end{theorem}

\begin{proof}
    It is a consequence of equation \ref{eqn:generror} applied to the pruned and reweighted network. Indeed, in this context, we have that $I_2$ is not changed (i.e. $I_2(i,k) = \frac{1}{6} \mathbbm{1}_{i \sim k}$, $I_2(i,n) = \frac{1}{6} \mathbbm{1}_{i \in G_n}$ and $I_2(n,m) = \frac{1}{6} \mathbbm{1}_{n=m}$) and:
    \begin{equation*}
     \hat{\epsilon}_g = \sum \limits_{i , k \in \mathcal{S}} \Tilde{v_i}\Tilde{v_k} I_2(i,k) + \sum \limits_{m , n \in [M]} v_n^*v_m^* I_2(n,m) +\sum \limits_{i \in \mathcal{S} \text{, } n \in [M]} \Tilde{v_i} v_n^* I_2(i,n)
     \end{equation*}

    Hence, thanks to our previous remark:

    \begin{eqnarray*}
   % \begin{align*}
     \hat{\epsilon}_g 
     &=  &\dfrac{1}{6}\Big [\sum \limits_{m \in \operatorname{ EXP }} (\sum \limits_{i \in G_m \cap \mathcal{S}}\Tilde{v}_i)^2 + \sum \limits_{m \notin \operatorname{ EXP 
 }} (v_m^*)^2 - 2 \sum \limits_{m \in \operatorname{ EXP }} v_m^* \sum \limits_{i \in G_m \cap \mathcal{S}} \Tilde{v}_i \Big ] \\
     &=  &\dfrac{1}{6} \Big [\sum \limits_{m \in \operatorname{ EXP }}(v_m^* - \sum \limits_{i \in G_m \cap \mathcal{S}}\Tilde{v}_i)^2 + \sum \limits_{m \notin \operatorname{ EXP }} (v_m^*)^2 \Big ]
   % \end{align*}
     \end{eqnarray*}
\end{proof}


\subsubsection{DPP sampling}

In this section, the goal is to present and motivate the choice to sample  according to a DPP in this setting. From the framework we described, it is clear that, when pruned, the remaining network performs better when the remaining neurons are sampled from distinct groups $G_m$. This is why DPP sampling is here particularly interesting, since it enables us to sample $\mathcal{S}$ by maximizing the diversity within it.

To maximize the clarity of our statement, let us introduce the following quantities: for a sample $\mathcal{S} \subset [K]$, define $l_m = \lvert G_m \cap \mathcal{S} \rvert$, for $1 \leq m \leq M$. Notice that $\sum \limits _{m \in \operatorname{EXP}} l_m = k_n$, so that, since $l_m \geq 1$, $k_n \geq q$, the following property holds:

\begin{lemma} \label{lem:DPPsampling}
    Let $L = (L_{ij})_{i,j \in [K]}$ be the sampling kernel of a DPP $\mathcal{S}$. Assume $L_{ij} = f(Q_{ij})$ for some function well chosen $f$ ($L$ has to be semi-definite positive). Then $l_m \in \{0,1\}$ a.s. for every $m \in [M]$. 
\end{lemma}

\begin{proof}
    Notice that if $i,j \in G_m$ for some $m \in [M]$, then the vectors $Q_{i:} = (Q_{ik})_{k \in [K]}$ and and $Q_{j:} = (Q_{jk})_{k \in [K]}$ are equal. Hence, if $A \subset [K]$ is such that $i,j \in A$, the matrix $L_A$ has two identical columns so $\det(L_A) = 0$.

    Now:

    $$ \mathbb{P}( \mathcal{S} = A)  \propto \det (L_A) = 0$$

    This proves that the sampling process only outputs samples with elements from distinct groups $G_m$, hence $l_m \in \{0,1\}$ a.s.
\end{proof}

\begin{remark}
    Equivalently, if $i,j \in G_m$, $i \neq j$, then $\mathbb{P}(\{i \in \mathcal{S} \} \cap \{j \in \mathcal{S}\}) = 0$.
\end{remark}
We will give two examples of such kernels:

\begin{itemize}
    \item the first example is by taking $L_{ij} = Q_{ij}$. Notice that, since $Q_{ij} = \frac{1}{N} w_i^Tw_j$, $Q$ is a Gram-matrix so it is semi definite positive, and is an admissible choice for $L$.
    \item the second example is the following: $L_{ij} = \exp(-\dfrac{\beta}{N} \lVert w_i -w_j \rVert^2) $ for some $\beta>0$. Notice that $L_{ij} = \exp(-2\beta (1+Q_{ij})) $ so the previous lemma applies.
\end{itemize}

\subsubsection{Analysis in the pruned and reweighted case}

It is time to come to analyse the generalization error, when we prune and reweight the network. Notice that theorem \ref{thm:geneerrorreweight} gives a natural way to reweight our neural network with respect to the generalization error. Indeed, to minimize the true generalization error, it is sufficient to choose the $\Tilde{v}_i $ in the following way:

\begin{equation}\label{eqn:choicereweight}
\forall m \in \operatorname{EXP} \text{, } \sum \limits_{i \in G_m \cap \mathcal{S}} \Tilde{v}_i = v_m^{*} = \sum \limits_{i \in G_m}v_i
\end{equation}

The second part of this equation was added to remind that this reweighting process can be performed even though we do not have prior access to $v_m^{*}$.

Finally the proof of theorem \ref{thm:nndpp} is as follows:

\begin{proof}
    This a consequence of theorem \ref{thm:geneerrorreweight} by plugging equation \ref{eqn:choicereweight} into the formula. Indeed, with this choice of weights, we get that:
    $$ \hat{\epsilon}_S = \sum \limits_{m \notin \operatorname{EXP}} (v_m^*)^2$$

    Hence :

    $$ \mathbb{E}[\hat{\epsilon}_S] = \sum \limits_{m=1}^M (v_m^*)^2 \mathbb{P}(m \notin \operatorname{EXP})$$

    Look at $\mathbb{P}(m \notin \operatorname{EXP})$ for $m \in [M]$:

    $$\mathbb{P}(m \notin \operatorname{EXP}) = 1 - \mathbb{P}(m \in \operatorname{EXP}) = 1 - \mathbb{P}(\exists i \in G_m \cap S) = 1 - \mathbb{P}(\bigcup \limits_{i \in G_m} \{ i \in S \}) $$

    Using the union bound:

    $$\mathbb{P}(m \notin \operatorname{EXP}) \geq 1 - \sum \limits_{i \in G_m} \mathbb{P}(i \in \mathcal{S})$$
    
    Now, thanks to the previous section, we know that, if $i$ and $j$ lie in the same $G_m$, for DPP sampling, $\mathbb{P}_{\operatorname{DPP}}(\{i \in S\} \cap \{j \in S \}) = 0 $. Hence:

    $$ \mathbb{P}_{\operatorname{DPP}} (\bigcup \limits_{i \in G_m} \{ i \in S \})) = \sum \limits_{i \in G_m} \mathbb{P}_{\operatorname{DPP}}(i \in \mathcal{S})$$

    This proves that for all $m \in [M]$:

    $$ \mathbb{P}(m \notin \operatorname{EXP}) \geq \mathbb{P}_{\operatorname{DPP}}(m \notin \operatorname{EXP})$$

\end{proof}


