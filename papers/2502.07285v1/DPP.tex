\subsection{Basic notions}

\emph{Determinantal point processes} (DPPs) are random configurations of locally finite point sets over some background space, whose correlation functions are given by determinants of certain kernels. They have a long history in both mathematics and physics. DPPs were first introduced by Macchi \cite{Mac72}, motivated by fermions in quantum mechanics. Macchi discovered that DPPs describe the distribution of a fermionic system at thermal equilibrium; their repulsive behavior captures the Pauli exclusion principle, which states that two fermions cannot occupy the same quantum state. Surprisingly, DPPs also arise naturally in many mathematical settings, such as the eigenvalues of random matrices, random spanning trees, and zeros of random analytic functions, among others. As such, DPPs have intertwined with various fields in mathematics, including random matrix theory \cite{borodin,johanssondpp}, integrable systems \cite{Deift_1,Deift_2}, combinatorics \cite{borodin2015_integrable,borodin2016_integrable}, and complex geometry \cite{Berman0,Berman1,Berman2,Berman3}, to name a few.

Formally, let $\mathcal X$ be a Polish space equipped with a positive Radon measure $\mu$ (e.g., $\mathbb R^d$ with the Lebesgue measure or a discrete set with the counting measure).
A \emph{point process} $\mathcal S$ on $\mathcal{X}$ is a random $\mathbb N$-valued Radon measure on $\mathcal X$. If $\mathcal S$ assigns mass at most $1$ for each point almost surely, we say that $\mathcal S$ is a \emph{simple point process}.

\begin{definition}[DPPs] \label{def:GeneralDPPs}
    A point process $\mathcal S$ is said to be determinantal if there exists a measurable function $K : \mathcal{X} \times \mathcal{X} \rightarrow \mathbb{C}$ such that, for all $n \geq 1$, if $f : \mathcal{X}^n \rightarrow \mathbb{R}$ is a bounded measurable function:
    \[ \mathbb{E}\Big [\sum_{\neq} f(x_{i_1}, \cdots, x_{i_n}) \Big ] = \int _{\mathcal{X}^n}f(x_1, \cdots x_n) \det((K(x_i,x_j))_{1 \leq i,j \leq n}) \d\mu^{\otimes n}(x_1, \cdots, x_n), \]
    where the sum in the LHS ranges over all pairwise distinct $n$-tuplets of $\mathcal{S}$.
    We then call $\mathcal S$ a DPP on $\mathcal X$ with kernel $K$ and reference measure $\mu$.
\end{definition}

\begin{remark}
    It can be shown that DPPs are simple, thus one can visualize DPPs as random configurations of locally finite point sets on $\mathcal X$. 
\end{remark}

In the theory of point processes, an important object to study is the so-called \emph{linear statistics}.
\begin{definition}[Linear statistics]
    For a point process $\mathcal S$ on $\mathcal X$ and a measurable function $f: \mathcal X \rightarrow \mathbb C$, the linear statistic $\Lambda_{\mathcal S}(f)$ is defined as
\[\Lambda_{\mathcal S}(f) := \sum_{x\in \mathcal S} f(x).\]
\end{definition}
Under mild conditions, the joint distribution of $\Lambda_\cS(f)$ for a sufficiently rich class of test functions $f$ (e.g., the class of all continuous, compactly supported functions) can determine the distribution of the process $\cS$; thus, studying linear statistics is of fundamental interest. For DPPs, due to the determinantal structure, there is an explicit formula for the Laplace transform of linear statistics. As a consequence, one can deduce explicit formulas for the cumulants (hence, moments) of linear statistics of DPPs (e.g., see \cite{lambertdpps}).
\begin{proposition}
    Let $\cS$ be a DPP on $\mathcal X$ with kernel $K$ and reference measure $\mu$. Let $f:\mathcal X \rightarrow \mathbb C$ be a bounded, compactly supported measurable function. Then
    \begin{eqnarray*}
        \mathbb E[e^{t\Lambda_\cS(f)}] &=& \det[I + M_f \mathcal K],
    \end{eqnarray*}
    where $\det$ denotes the Fredholm determinant, $M_f$ is the multiplication operator by the function $e^{tf(x)}-1$, acting on $L^2(\mathcal X,\mu)$, and $\mathcal K$ denotes the integral operator acting on $L^2(\mathcal X,\mu)$ induced by the kernel $K$.
\end{proposition}

We warn the readers that not every kernel $K$ defines a DPP. In fact, we have the following theorem by Macchi \cite{Mac75} and Soshnikov \cite{SoshDPP}.

\begin{theorem}[Macchi-Soshnikov]
    For a kernel $K: \mathcal X \times \mathcal X \rightarrow\mathbb C$, we denote by $\mathcal K: L^2(\mathcal X,\mu) \rightarrow L^2(\mathcal X, \mu)$ the integral operator induced by $K$. Let $K$ be a kernel such that $\mathcal K$ is self-adjoint and locally trace class. Then $K$ defines a DPP (w.r.t $\mu$) if and only if all the eigenvalues of $\mathcal K$ are in $[0,1]$.
\end{theorem}

An important subclass of DPPs, called \emph{projection DPPs}, consists of those for which the associated integral operator $\mathcal K: L^2(\mathcal X,\mu) \rightarrow L^2(\mathcal X,\mu)$ is self-adjoint, bounded, and has all non-zero eigenvalues equal to 1 (equivalently, $\mathcal K$ is a projection operator onto a subspace of $L^2(\mathcal X,\mu)$). It turns out that all DPPs with Hermitian kernels are mixtures of projection DPPs (see \cite{HKPV}).

Similar to Gaussian processes, all statistical properties of a DPP are encoded in the
kernel $K$ and background measure $\mu$ (or equivalently, the integral operator $\mathcal K$). For example, for any compact subset $D\subset \mathcal X$, the number of points of the DPP defined by $\mathcal K$ lying inside $D$ is equal in distribution to a sum of independent Bernoulli random variables with parameters the eigenvalues of $\mathcal K_D$, the integral operator on $L^2(\mathcal X,\mu)$ defined by the kernel $\mathbf{1}_D(x)K(x,y)\mathbf{1}_D(y)$. In particular, for projection DPPs, the total number of points is almost surely a constant, which is equal to $\rank(\mathcal K)$.

DPPs are known to exhibit \emph{repulsiveness}, i.e., particles tend to repel each other. This entails, for example, that the probability of having multiple points inside the same small neighbourhood would be much smaller for a DPP compared to that for a Poisson process with the same first intensity. To see this, in what follows we present a simple computation.

Let $\mathcal S$ be a DPP on $\mathbb R^d$ with Hermitian, translation-invariant kernel $K$ (i.e., $K(x,y) = \overline{K(y,x)}$ and $K(x,y)=\Phi(x-y)$ for some $\Phi$), and background measure the Lebesgue measure. Let $B_\varepsilon$ be a ball of small radius $\varepsilon>0$ (one can assume the center is $0$ due to the translation invariance) and denote by $\mathcal S(B_\varepsilon)$ the number of points in $\mathcal S$ inside $B_\varepsilon$. Then we have
\begin{eqnarray*}
    \mathbb P(\mathcal S(B_\varepsilon) \ge 2) &\le& \mathbb E[\mathcal S(B_\varepsilon)(S(B_\varepsilon)-1)] \\
    &=&\iint_{B_\varepsilon^2} \Big( K(x,x)K(y,y) - K(x,y)K(y,x) \Big ) \d x \d y\\
    &=&\iint_{B_\varepsilon^2} \Big( |\Phi(0)|^2 - |\Phi(x-y)|^2 \Big ) \d x \d y \\
    &=& \varepsilon^{2d} \iint_{B(0,1)^2} \Big( |\Phi(0)|^2 - |\Phi(\varepsilon(u-v))|^2 \Big ) \d u \d v.
\end{eqnarray*}
Thus, for sufficiently regular $\Phi$, 
%(say, $|\Phi|$ is $C^1$ 
%\rb{Shouldn't $\vert\Phi\vert$ be $C^1$ for the argument to hold?}), 
we would have
\begin{equation} \label{eq:DPP_repel}
    \mathbb P(\mathcal S(B_\varepsilon) \ge 2) = \mathcal{O}(\varepsilon^{2d+1}) \quad \text{as } \varepsilon\rightarrow 0.
\end{equation}

In contrast, for Poisson point processes, the order of the probability above is $\varepsilon^{2d}$. To elaborate, let $\mathcal S_\poi$ be a Poisson point process on $\mathbb R^d$ with the same first intensity $K(x,x)\d x = \Phi(0) \d x$ as $\mathcal S$. Then, by definition, $\mathcal S_\poi(B_\varepsilon)$ is a random variable following the Poisson distribution with parameter $\lambda :=\Phi(0) \cdot\mathrm{Vol}(B_\varepsilon)$. Thus,
\begin{equation} \label{eq:Poisson_repel}
    \mathbb P(\mathcal S_\poi(B_\varepsilon) \ge 2) = e^{-\lambda}(e^\lambda - 1 - \lambda) \asymp \varepsilon^{2d} \quad \text{as } \varepsilon\rightarrow 0,
\end{equation}
where we used the fact that $\lambda = \Phi(0) \cdot \mathrm{Vol}(B(0,1)) \cdot \varepsilon^d$.

\medskip  

For more detailed discussion and further properties of DPPs, we refer the readers to \cite{SoshDPP,HKPV,LyonsIHES,lyons2014survey} for excellent overviews. 
\begin{comment}
    This definition implies that, for each bounded measurable real-valued function $\phi$, $\mathbb{E}[\sum \limits_{x \in \mathcal{S}} \phi(x) ] = \int_{\mathcal{X}} \phi(x) K(x,x) \mu(dx)$.
The variance can be computed in a similar way.
It is also worth mentioning that each kernel $K$ do not carry a DPP and the precise conditions for this to be the case are given by the Macchi-Soshnikov theorem. This definition is rather general, but, in machine learning, often discrete DPPs are considered instead. The general framework for discrete DPPs in machine learning was developed in \cite{KuleszaTaskar}.
\end{comment}

\subsection{DPPs in machine learning}
In recent years, DPPs have attracted the attention of the machine learning community. They are generic statistical models for repulsion in
spatial statistics \cite{LaMoRu14, biscio2017contrast} and machine learning \cite{kulesza_determinantal_2012, belhadji_determinantal_2020, gartrell2019learning, gartrell2020scalable, determinantal-averaging, tremblay2019determinantal, GDP}. Sampling and inference with DPPs are tractable, and there are efficient algorithms for such tasks \cite{kulesza_determinantal_2012, GDP, gartrell_scalablesampling, han2022scalable}.

In the setting of machine learning, the background set (sometimes called the \emph{data set}) is usually finite, say, $\mathcal X = \{1,..,N\}$. It is then common to use the counting measure as the background measure on $\mathcal X$. In this setting, a more intuitive way to define DPPs is as follows. 

\begin{definition}[Discrete DPPs] \label{def:DPPdef}
    A random subset $\mathcal S$ of $\mathcal X$ is called a DPP if there exists an $N\times N$ matrix $\bK$ such that
    \[ \mathbb P(A\subset \mathcal S) = \det [\bK_{A}], \quad \forall A\subset \mathcal X,\]
    where $\bK_A$ denotes the submatrix of $\bK$ with rows and columns indexed by the subset $A$.
\end{definition} 


    The Macchi-Soshnikov theorem is then translated to that if $\bK$ is a Hermitian matrix, then $\bK$ defines a DPP if and only if $\mathbf{0} \preceq \bK \preceq \mathbf I $. 
    In this setting, projection DPPs are precisely those whose kernels are projection matrices.
    
    If all eigenvalues of $\bK$ are strictly less than $1$, the DPP defined by $\bK$ is also an $L$-ensemble, namely
    \[ \mathbb P(\mathcal S= A) \propto \det [\bL_A], \quad \forall A\subset \mathcal X\]
    where $\bL:= \bK(\bI - \bK)^{-1}$ and $\bL_A$ denotes the submatrix of $\bL$ whose rows and columns indexed by the subset $A$ (see \cite{kulesza_determinantal_2012}).

\begin{comment}
    DPPs are often used to sample diverse subsets. A first insight on this statement is to notice that DPPs introduce negative correlation between the elements of the ground set. Indeed, for $i \neq j$, one can notice that:  $\mathbb{P}_{\operatorname{DPP}}(\{ i,j \} \subset \mathcal{S}) = \mathbb{P}_{\operatorname{DPP}}(i \in \mathcal{S})\mathbb{P}_{\operatorname{DPP}}(j \in \mathcal{S}) - K_{ij}^2$
\end{comment}

\begin{comment}
    Looking at the atomic distribution, one can also define L-ensembles.

\begin{definition}
    Let $L$ be a semi-definite matrix (the likelihood kernel). An L-ensemble over $[N]$ is a probability distribution over $2^{[N]}$ such that, if $\mathcal{S}$ is sampled according to this distribution:

    \[ \forall S \subset [N] \text{ , } \mathbb{P}(\mathcal{S} = S) = \det(L+I)^{-1}\operatorname{Det}(L_{Y}) \]

    where $\operatorname{Det}(L+I)^{-1}$ acts as the normalization constant.
\end{definition}

It can be shown that each L-ensemble is a DPP with marginal kernel $K = L(I+L)^{-1}$. A DPP with kernel $K$ is also an $L$-ensemble, provided that the spectrum of the marginal kernel $K$  is in $[0,1)$.

This definition gives also a second insight on the repulsiveness. Indeed, on a geometric point of view, if to each element $i$, one associates a feature $q_i$ in some abstract Hilbert space, then $L_{ij}$ can be interpreted as $q_i^T q_j $. Then $\mathbb{P}(S = \mathcal{S}) \propto \operatorname{Vol}(q_i)_{i \in S}^2$, which show, using the "base times height" formula, that sampling from a DPP leads to favour elements which have diverse features.
\end{comment}


In general, the cardinality $|\cS|$ of a DPP $\cS$ is an $\mathbb N$-valued random variable. In applications, sometimes it is interesting to sample a point process similarly to a DPP but with fixed size $m$ (called an $m$-DPP in  \cite{kulesza_determinantal_2012}). This can be done by a conditioning argument as follows. Given a DPP $\mathcal S$ on $\mathcal X$, one can obtain an $m$-DPP by considering the conditioned process $(\cS| \{|\cS| = m\} )$, provided that $\mathbb P (|\cS|= m ) >0$. However, $m$-DPPs do not have the determinantal structure in general (i.e., they are not DPPs); the only exception is when the original DPP $\mathcal S$ is a projection DPP.

\begin{comment}
    
\subsection{Some properties}

In the following fix some  $\mathcal{S}$  a random variable whose distribution is an $L$-ensemble with kernel $L$ whose eigendecomposition is: $L=\sum_{i =1}^N \lambda_i \mathbf{v}_i \mathbf{v}_i^T$ where $\mathbf{v}_i$s are orthonormal vectors. Fix also its marginal kernel $K$, note that its eigendecomposition is then given by: $L=\sum_{i =1}^N (\lambda_i/(1+\lambda_i)) \mathbf{v}_i \mathbf{v}_i^T$.

It can be shown that $\mathbb{E} [\lvert \mathcal{S} \rvert] =\operatorname{Tr}(K) $ and $ \operatorname{Var}( \lvert  \mathcal{S} \rvert) = \sum \limits_{i=1}^N \lambda_n/(1+\lambda_n)^2 $. 

These observations allow us to notice that an elementary DPP are in fact also $k$-DPPs with $k = \operatorname{rk}(K) $.

The following properties can be useful to sample from discrete DPP.

\begin{proposition} \label{prop:mixture}
    An L-ensemble is a mixture of elementary DPP. More precisely, define $\mathbb{P}_L$ a DPP with likelihood kernel $L$ and for $J \subset [N]$, let $V_J = \{ v_i | i \in J \}$. Let $\mathbb{P}^{K_J}$ be an elementary DPP with marginal kernel $K^{V_J} = \sum_{j \in J} v_j v_j^T$ :

    \[\mathbb{P}_L = \dfrac{1}{\det(L+I)}\sum \limits_{J\subset [n]} \mathbb{P}^{K_J}\prod \limits_{j \in J} \lambda_j\]
\end{proposition}

A similar property holds for $k$-DPPs and all these definitions and properties generalize to the continuous setting \ref{def:GeneralDPPs}.
\end{comment}

