\section{Literature Review}
\label{sec:liter}
	
	In this section, we briefly review %provides an overview of 
	existing methods for solving minimax problems % 
	and for WDRO. Also, we review the use of the log-sum-exp function. %in the literature.
	
	\subsubsection{Existing Methods for Solving Minimax Problems}
	Regardless of the dimension curse problem caused by a large $n$ in~\eqref{eq:minmax22}, existing algorithms for solving minimax problems can be applied there.
	Since the seminal work in ____, convex-concave minimax problems have been extensively studied based on the concept of saddle points (see e.g. ____ and the references therein). 
	%For the case where~\eqref{eq:minmax22} has 
	Convex-nonconcave/nonconvex-concave minimax problems have also been studied; see____ and references therein. For nonconvex-nonconcave minimax structured problems, see____. 
	
	In cases where the minimax problem has a nonconvex-nonconcave structure, the existence of a saddle point is not guaranteed____. 
	Hence, researchers have turned to developing algorithms for finding the so-called \emph{game stationary point} or \textit{Nash equilibrium point}; see____ for examples.  
	However, for a game stationary  point $(\bar{\vy}, [\bar{\vz}_i]^n_{i=1})$, $\bar{\vy}$ and $\bar{\vz}_i$ may be far away from any local minimizer of problems $\min_{\vy}g(\vy)$ and $\min_{\vz\in\mathcal{Z}}-\Psi(\bar{ \vy},\vz;\vx_i)$, respectively____. Such a flaw occurs because a game stationary point fails to capture the order between the min-problem and the max-problem____.  
	
	To address the fundamental limitations of game stationary points, several works focus on minimizing the primal function $g$ to obtain %specific 
	stronger types of stationary points, such as the Clarke stationary point; see Definition \ref{def:sta} below. %____. 
	The Clarke stationarity  
	has been %extensively 
	studied for convex-concave and nonconvex-concave minimax problems ____. However, for nonconvex-nonconcave min-(sum-)max problems, computing (or even approximating) $\max_{\vz}\Phi_i(\vy,\vz;\vx)$ becomes intractable, making it difficult to verify the stationarity condition. In contrast, through the smoothing technique, our method can produce an iterate sequence converging to a Clarke stationary solution. Further, by establishing a Clarke regularity condition, we are able to show the convergence to a directional stationary point, which is claimed to be the sharpest first-order stationary point for nonconvex optimization problems ____.
	
	\subsubsection{Existing Methods for Solving WDRO}
	As introduced in ____, a broad approach in the DRO literature is to solve problem~\eqref{eq:model2}. Notably, problem~\eqref{eq:model2} exhibits a min-sum-max structure, allowing methods for solving minimax problems to be applied to WDRO.
	As we have mentioned, the nonconcavity of the loss function presents significant challenges in obtaining a stationary point. Moreover, in WDRO, the function $\ell(\vtheta,\cdot) - \lb d(\vx,\cdot)$ is often nonsmooth, further complicating the problem. To overcome these obstacles, researchers have proposed algorithms based on various reformulations of~\eqref{eq:model2} (see e.g. in____),  and the convergence of these methods requires somewhat stringent conditions on the loss function~$\ell$ and the transport cost.
	When the %\remove{uncertainty} 
	{support} set~$\cal{Z}$
	is compact, a common approach to solve WDRO is to approximate $\cal{Z}$ using a finite, discrete grid. This method, though widely used ____, becomes prohibitively expensive as the number of grid points grows. For WDRO with $l_1$ transport cost, a convex reformulation is available when the loss function can be expressed as a pointwise maximum of finitely many concave functions ____, or when the log-loss function is applied, such as in robust logistic regression ____. Moreover, efficient first-order algorithms have been developed for specific WDRO problems where the function $\ell(\vtheta,\cdot)-\lb d(\cdot,\vx)$ is strongly concave____.  To guarantee the strong concavity, the transport cost must satisfy stringent conditions relative to the loss function. %However, beyond 
	Without these conditions on the loss function or the transport cost, solving WDRO problems poses significant computational challenges. This is especially true when the structure of the problem does not lend itself to the simplifications used in previous work. 
	
	Recognizing the difficulty in solving the WDRO problem, ____ innovatively proposes addressing a dual problem of Sinkhorn DRO (SDRO),  
	which can be written as 
	\begin{equation}
		\label{eq:smoothg}
		\min_{\vtheta\in \Theta, \lb\ge 0}g_{\mathrm{s}}(\vtheta, \lb) :=  \lambda \delta^p+  \lb\eta \mathbb{E}_{\vx \sim \widehat{\mathbb{P}}_n}\left[\log \mathbb{E}_{\vz \sim \zeta}\left[e^{(\ell(\vtheta,\vz) -\lb d(\vx,\vz)) / \lb\eta}\right]\right]
	\end{equation}
	for some $\eta>0$.  
	The objective function in \eqref{eq:smoothg} can be viewed as a smoothing function of \eqref{eq:model2} by Definition~\ref{def:smooth} below with $\eta $ as the smoothing parameter. %; see Lemma \ref{lem:smoothphi}(a).
	Assuming $\ell(\cdot,\vz)$ is convex for all feasible~$\vz$, the authors of ____ develop a convergent triple-loop algorithm by skillfully combining a bisection method, a stochastic mirror descent method, and multilevel Monte-Carlo simulation.
	However, they fix $\eta>0$. In addition, in their complexity result \cite[Theorem 3]{wang2021sinkhorn}, they assume that any optimal solution $(\vtheta^*,\lb^*)$ of problem \eqref{eq:smoothg} satisfies $\lb^*\ge \overline{\lb}$ for some positive scalar $\overline{\lb}$.
	This assumption circumvents significant computational challenges by preventing both the Lipschitz constant and the gradient Lipschitz constant from exploding if %should \red{GMB: Not sure if \textit{should} is right here, but can we guarantee that $\lambda\to0$ would occur for SDRO?}
	$\lb$ approaches~0 as the algorithm progresses, 
	%\revise{This is the theoretical flaw. I do not think we should guarantee that $\lambda\to0$ occurs for SDRO.}
	%, which would otherwise lead to an exponential increase in the sampling complexity of their multilevel Monte-Carlo simulations with respect to $\frac{1}{\lb}$ (see the proof in \cite[Proposition EC.4]{wang2021sinkhorn}). 
	%Should $\lb$ approach~0, 
	in which case, an exponential increase in the sampling complexity of their multilevel Monte-Carlo simulations would occur with respect to $\frac{1}{\lb}$ (see the proof in \cite[Proposition EC.4]{wang2021sinkhorn}). 
	Hence, their algorithm does not solve the WDRO problem but rather addresses a simpler approximation.    
	
	%{
		In contrast, when applied to WDRO, our proposed method for \eqref{eq:minmax} smoothes the function in \eqref{eq:model2} by the log-sum-exp technique and solves 
		\begin{equation}
			\label{eq:smoothg2}
			\min_{\vtheta \in \Theta, \lb \geq 0} \widetilde{g}(\vtheta, \lb, \mu) := \lambda \delta^p + \mu \mathbb{E}_{\vx \sim \widehat{\mathbb{P}}_n}\left[\log \mathbb{E}_{\vz \sim \zeta}\left[e^{(\ell(\vtheta, \vz) -\lb d(\vx, \vz)) / \mu}\right]\right].
		\end{equation}
		Different from ____, we do not fix $\mu$ at a big number but instead push it to zero and we further treat $\lambda$ as a variable that can reach zero. The formulation in \eqref{eq:smoothg2} 
		replaces \(\lb \eta\) in \eqref{eq:smoothg} with \(\mu\).  However, this subtle difference in the formulation leads to a fundamentally different method  
		from the algorithm in ____ in terms of algorithmic development, methodological foundation, and theoretical analysis.
		First, ____  
		solves a dual problem of SDRO with a  fixed  \(\eta\). In contrast, we  
		solve WDRO directly, requiring \(\mu \downarrow 0\) in our algorithmic framework. 
		This divergence alters the optimization objectives and methodological considerations.
		Second, the requirement of \(\mu \downarrow 0\)
		introduces unique computational and theoretical challenges. It demands rigorous convergence analysis  %guarantees \red{GMB: To prove? (instead of that)} 
		to prove that solving a sequence of smoothed problems approximates a WDRO solution, while also handling the ill-conditioning and increased computational complexity arising from the vanishing~$\mu$. This necessitates sophisticated  
		analysis and careful design to ensure stability and efficiency.
		Third, \(\lb\) is a primal variable in problem \eqref{eq:smoothg} and appears in the denominator of the exponential term. 
		This results in both the Lipschitz constant and the gradient Lipschitz constant of 
		$g_s$  with respect to~$\lb$ becoming unbounded as 
		$\lambda$ approaches zero, which potentially yields slow convergence and high complexity. %complicate the complexity analysis. %To avoid this issue, Wang et al. ____ propose a convergent triple-loop algorithm under some stringent assumptions, which significantly increases complexity of tuning parameters of their algorithm.  
		In constrast, the Lipschitz constant of $\widetilde{g}$  with respect to \(\lb\) is bounded, and the gradient Lipschitz constant of $\widetilde{g}$   with respect to \(\lb\) can be bounded  by $\frac{C}{\mu}$ for some positive scalar $C$; %\red{GMB: If $\mu\to0$, isn't this bound sort of meaningless?} 
		this enables us to apply our established complexity results of our proposed method to the WDRO problem to obtain a scaled near-stationary solution; see Definition~\ref{def:scaled-stat} below.
		Additionally, 
		our proposed method applied to WDRO is  
		a single-loop algorithm compared to the triple-loop algorithm developed in____.
		
		
		
		\subsubsection{The Log-sum-exp Function}
		When $\mathcal{Z}$ is a finite discrete set, $\mu \log \frac{1}{|\cZ|}\sum_{\vz_i\in\mathcal{Z}} e^{\Phi(\vy,\vz_i)/\mu}$ is the log-sum-exp function of  $\max_{\vz_i\in\mathcal{Z}} \Phi(\vy,\vz_i)$. In the literature, this function is sometimes referred to as the \emph{softmax maximum}____ or the \emph{Neural Networks smoothing  function}____. This smoothing function has been used for solving finite minimax problems ____. It is useful in scenarios where a differentiable approximation for the maximum is required____.
		Its optimization properties, such as convexity and gradient structure, have been studied extensively, highlighting its theoretical importance and practical utility in various fields ranging from statistics to deep learning. Notably, the gradient of the Neural Networks smoothing  function can also be viewed as the softmax function ____, which is widely used in neural networks.
		The authors in ____ utilize the Neural Networks smoothing function to solve nonsmooth convex minimization problems.  
		However, 	the log-sum-exp function that we will introduce in \eqref{eq:smoothphi} has not yet been applied as an approximation of the max operator over a continuous compact set within any known smoothing algorithm. %, and its properties have not been fully understood.