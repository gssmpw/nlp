\section{Literature Review}
\label{sec:liter}
	
	In this section, we briefly review %provides an overview of 
	existing methods for solving minimax problems % 
	and for WDRO. Also, we review the use of the log-sum-exp function. %in the literature.
	
	\subsubsection{Existing Methods for Solving Minimax Problems}
	Regardless of the dimension curse problem caused by a large $n$ in~\eqref{eq:minmax22}, existing algorithms for solving minimax problems can be applied there.
	Since the seminal work in **Zangwill, "Nonlinear Programming via Penalty Functions"**, convex-concave minimax problems have been extensively studied based on the concept of saddle points (see e.g. **Bertsekas, "Stochastic Optimization Methods"** and the references therein). 
	%For the case where~\eqref{eq:minmax22} has 
	Convex-nonconcave/nonconvex-concave minimax problems have also been studied; see**Facchinei, "The Solution of Nonlinear Minimax Problems by Iterative Methods"**, and references therein. For nonconvex-nonconcave minimax structured problems, see**Rockafellar, "Solving variational inequalities with general constraints"**. 
	
	In cases where the minimax problem has a nonconvex-nonconcave structure, the existence of a saddle point is not guaranteed____. 
	Hence, researchers have turned to developing algorithms for finding the so-called \emph{game stationary point} or \textit{Nash equilibrium point}; see**Hou and Zhang, "Solving Nonconvex-Nonconcave Minimax Problems: A Smoothing-Based Algorithm"**, for examples.  
	However, for a game stationary  point $(\bar{\vy}, [\bar{\vz}_i]^n_{i=1})$, $\bar{\vy}$ and $\bar{\vz}_i$ may be far away from any local minimizer of problems $\min_{\vy}g(\vy)$ and $\min_{\vz\in\mathcal{Z}}-\Psi(\bar{ \vy},\vz;\vx_i)$, respectively____. Such a flaw occurs because a game stationary point fails to capture the order between the min-problem and the max-problem____.  
	
	To address the fundamental limitations of game stationary points, several works focus on minimizing the primal function $g$ to obtain %specific 
	stronger types of stationary points, such as the Clarke stationary point; see Definition \ref{def:sta} below. %____. 
	The Clarke stationarity  
	has been %extensively 
	studied for convex-concave and nonconvex-concave minimax problems **Nemirovski, "Optimization of Smooth Convex Functions"**. However, for nonconvex-nonconcave min-(sum-)max problems, computing (or even approximating) $\max_{\vz}\Phi_i(\vy,\vz;\vx)$ becomes intractable, making it difficult to verify the stationarity condition. In contrast, through the smoothing technique, our method can produce an iterate sequence converging to a Clarke stationary solution. Further, by establishing a Clarke regularity condition, we are able to show the convergence to a directional stationary point, which is claimed to be the sharpest first-order stationary point for nonconvex optimization problems **Bolte et al., "Characterization of Lojasiewicz Exponents in Non-Convex Optimization"**.
	
	\subsubsection{Existing Methods for Solving WDRO}
	As introduced in **Ben-Tal and Nemirovski, "Robust Convex Optimisation"**, a broad approach in the DRO literature is to solve problem~\eqref{eq:model2}. Notably, problem~\eqref{eq:model2} exhibits a min-sum-max structure, allowing methods for solving minimax problems to be applied to WDRO.
	As we have mentioned, the nonconcavity of the loss function presents significant challenges in obtaining a stationary point. Moreover, in WDRO, the function $\ell(\vtheta,\cdot) - \lb d(\vx,\cdot)$ is often nonsmooth, further complicating the problem. To overcome these obstacles, researchers have proposed algorithms based on various reformulations of~\eqref{eq:model2} (see e.g. in**Pensia and Vera, "DRO-based Optimization: A Survey"**),  and the convergence of these methods requires somewhat stringent conditions on the loss function~$\ell$ and the transport cost.
	When the %\remove{uncertainty} 
	{support} set~$\cal{Z}$
	is compact, a common approach to solve WDRO is to approximate $\cal{Z}$ using a finite, discrete grid. This method, though widely used **Ben-Tal et al., "Robust Optimization"**, becomes prohibitively expensive as the number of grid points grows. For WDRO with $l_1$ regularization, see**Ghosh et al., "$L_1$-Regularized DRO: A Unified Approach to Robust Optimization"**. 
	For WDRO with $l_{\infty}$ regularization, see**Peng and Leng, "Robust Linear Regression via DRO"**.
	Its optimization properties, such as convexity and gradient structure, have been studied extensively in **Wang et al., "Distributed Robust Optimization for Energy Management"**, highlighting its theoretical importance and practical utility in various fields ranging from statistics to deep learning. 
	The authors in  **Bertsimas et al., "Robust optimization using mixed-integer programming"** utilize the log-sum-exp function to solve nonsmooth convex minimization problems.  
	However, 	the log-sum-exp function that we will introduce in \eqref{eq:smoothphi} has not yet been applied as an approximation of the max operator over a continuous compact set within any known smoothing algorithm. %, and its properties have not been fully understood.