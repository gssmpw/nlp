
% \subsection*{Initial Drafter Success Metrics of Different Models}

% The Drafter Success Metrics in Table~\ref{tab:initial_success} reveal that all models perform significantly better on the initial tasks of \textbf{Setup} and \textbf{Download}, with success rates ranging from approximately 0.23 to 0.28. These tasks are straightforward, involving standardized processes like \texttt{pip} or \texttt{conda} installations. In contrast, the success rates drop sharply for the more complex tasks of \textbf{Training}, \textbf{Evaluation}, and \textbf{Inference}, often nearing zero. This disparity indicates that while the drafter models handle basic setup effectively, they struggle with tasks requiring adjustments such as updating file paths and setting environment variables. The low performance in these areas suggests that the initial extraction from human-oriented README instructions does not yield syntactically correct scripts suitable for automated execution.

% \subsection*{Analyzer Success Metrics of Different Models}

% The Analyzer Success Metrics in Table~\ref{tab:analyzer_success} show a noticeable improvement across all tasks and models compared to the drafter stage. Success rates for \textbf{Setup} and \textbf{Download} increase to around 0.34 to 0.40. Importantly, the more complex tasks of \textbf{Training}, \textbf{Evaluation}, and \textbf{Inference} see significant gains, with success rates rising to between 0.10 and 0.18. This enhancement is attributed to the analyzer models leveraging dynamic feedback from the deployment environment, allowing them to refine scripts and correct errors that the drafter models could not resolve. The analyzers' ability to interpret execution feedback leads to more accurate and effective scripts for complex operations.

% \subsection*{Issue Retriever Success Metrics of Different Models}

% In the Issue Retriever stage, success metrics continue to improve, particularly for the complex tasks. Success rates for \textbf{Training}, \textbf{Evaluation}, and \textbf{Inference} increase further, reaching up to 0.25 in some cases (e.g., Claude-Sonnet for Training). The incorporation of a knowledge base enables these models to retrieve relevant information and solutions to issues encountered during earlier stages. This additional context helps the models address specific problems like dependency conflicts or configuration errors, enhancing their overall performance in executing complex tasks that require deeper problem-solving capabilities.

% \subsection*{Web Searcher Success Metrics of Different Models}

% The Searcher Success Metrics in Table~\ref{tab:searcher_success} exhibit the highest performance across all tasks and models. Success rates for \textbf{Setup} and \textbf{Download} tasks reach upwards of 0.46, while the complex tasks of \textbf{Training}, \textbf{Evaluation}, and \textbf{Inference} see success rates between 0.15 and 0.29. The searcher models benefit from open information access through web search, allowing them to find up-to-date solutions and troubleshoot issues that previous agents could not resolve. This access to external resources results in the most substantial improvements, particularly in tasks that are highly dependent on current best practices and environment-specific configurations. The sequential workflow culminating with the searcher stage demonstrates that incorporating dynamic feedback and external information sources is crucial for achieving higher success rates in complex automated tasks.



% \begin{table}[htbp]
%   \centering
%   \caption{Drafter Success Metrics of Different Models.}
%   \resizebox{\linewidth}{!}{%
%   \begin{tabular}{ll ccccc}
%   \toprule
%   \textbf{Model Type} & \textbf{Model Name} & \textbf{Setup} & \textbf{Download} & \textbf{Training} & \textbf{Evaluation} & \textbf{Inference} \\
%   \midrule
%   \multirow[=]{3}{*}{\textbf{Claude}} 
%   & Claude-3-Instant & 0.232 & 0.189 & 0.007 & 0.000 & 0.000 \\
%   & Claude-3-Haiku & 0.253 & 0.239 & 0.046 & 0.005 & 0.052 \\
%   & Claude-3-Sonnet & 0.284 & 0.283 & 0.045 & 0.024 & 0.031 \\
%   \midrule
%   \multirow[=]{3}{*}{\textbf{GPT-4}} 
%   & GPT-4o-Mini & 0.242 & 0.229 & 0.008 & 0.016 & 0.029 \\
%   & GPT-4o & 0.261 & 0.238 & 0.039 & 0.022 & 0.031 \\
%   & GPT-4-Turbo & 0.271 & 0.252 & 0.028 & 0.039 & 0.032 \\
%   \midrule
%   \multirow[=]{4}{*}{\textbf{Llama-3}} 
%   % & Llama-3-8B & 0.261 & 0.344 & 0.049 & 0.019 & 0.025 \\
%   & Llama-3-70B & 0.239 & 0.306 & 0.019 & 0.040 & 0.032 \\
%   & Llama-3.1-8B & 0.243 & 0.200 & 0.051 & 0.037 & 0.007 \\
%   & Llama-3.1-70B & 0.260 & 0.280 & 0.032 & 0.022 & 0.019 \\
%   \midrule
%   \multirow[=]{2}{*}{\textbf{Mistral}} 
%   & Mistral-Large & 0.243 & 0.266 & 0.047 & 0.031 & 0.026 \\
%   & Mistral-Large-2 & 0.251 & 0.279 & 0.039 & 0.025 & 0.024 \\
%   \bottomrule
%   \end{tabular}%
%   }
%   \label{tab:initial_success}
% \end{table}



% \begin{table}[htbp]
%   \centering
%   \caption{Analyzer Success Metrics of Different Models.}
%   \resizebox{\linewidth}{!}{%
%   \begin{tabular}{ll ccccc}
%   \toprule
%   \textbf{Model Type} & \textbf{Model Name} & \textbf{Setup} & \textbf{Download} & \textbf{Training} & \textbf{Evaluation} & \textbf{Inference} \\
%   \midrule
%   \multirow{3}{*}{\textbf{Claude}} 
%   & Claude-3-Instant & 0.342 & 0.353 & 0.104 & 0.109 & 0.151 \\
%   & Claude-3-Haiku & 0.350 & 0.301 & 0.132 & 0.037 & 0.130 \\
%   & Claude-Sonnet & 0.388 & 0.400 & 0.168 & 0.116 & 0.129 \\
%   \midrule
%   \multirow{3}{*}{\textbf{GPT-4}} 
%   & GPT-4o-Mini & 0.347 & 0.317 & 0.118 & 0.078 & 0.131 \\
%   & GPT-4o & 0.362 & 0.353 & 0.148 & 0.115 & 0.145 \\
%   & GPT-4-Turbo & 0.353 & 0.322 & 0.161 & 0.094 & 0.148 \\
%   \midrule
%   \multirow{4}{*}{\textbf{Llama-3}} 
%   % & Llama-3-8B & 0.329 & 0.425 & 0.130 & 0.052 & 0.104 \\
%   & Llama-3-70B & 0.361 & 0.382 & 0.111 & 0.185 & 0.176 \\
%   & Llama-3.1-8B & 0.304 & 0.386 & 0.183 & 0.114 & 0.123 \\
%   & Llama-3.1-70B & 0.313 & 0.335 & 0.141 & 0.182 & 0.151 \\
%   \midrule
%   \multirow{2}{*}{\textbf{Mistral}} 
%   & Mistral-Large & 0.324 & 0.349 & 0.121 & 0.143 & 0.144 \\
%   & Mistral-Large-2 & 0.340 & 0.357 & 0.152 & 0.199 & 0.163 \\
%   \bottomrule
%   \end{tabular}%
%   }
%   \label{tab:analyzer_success}
% \end{table}


% \begin{table}[htbp]
%   \centering
%   \caption{Issue Retriever Success Metrics of Different Models.}
%   \resizebox{\linewidth}{!}{%
%   \begin{tabular}{ll ccccc}
%   \toprule
%   \textbf{Model Type} & \textbf{Model Name} & \textbf{Setup} & \textbf{Download} & \textbf{Training} & \textbf{Evaluation} & \textbf{Inference} \\
%   \midrule
%   \multirow{3}{*}{\textbf{Claude}} 
%   & Claude-3-Instant & 0.365 & 0.369 & 0.129 & 0.130 & 0.169 \\
%   & Claude-3-Haiku & 0.374 & 0.329 & 0.139 & 0.061 & 0.143 \\
%   & Claude-Sonnet & 0.442 & 0.436 & 0.254 & 0.183 & 0.163 \\
%   \midrule
%   \multirow{3}{*}{\textbf{GPT-4}} 
%   & GPT-4o-Mini & 0.375 & 0.367 & 0.171 & 0.122 & 0.159 \\
%   & GPT-4o & 0.379 & 0.375 & 0.169 & 0.128 & 0.160 \\
%   & GPT-4-Turbo & 0.377 & 0.381 & 0.178 & 0.126 & 0.164 \\
%   \midrule
%   \multirow{4}{*}{\textbf{Llama-3}} 
%   % & Llama-3-8B & 0.355 & 0.444 & 0.146 & 0.072 & 0.118 \\
%   & Llama-3-70B & 0.364 & 0.399 & 0.113 & 0.159 & 0.154 \\
%   & Llama-3.1-8B & 0.305 & 0.389 & 0.182 & 0.115 & 0.122 \\
%   & Llama-3.1-70B & 0.312 & 0.334 & 0.143 & 0.179 & 0.153 \\
%   \midrule
%   \multirow{2}{*}{\textbf{Mistral}} 
%   & Mistral-Large & 0.357 & 0.358 & 0.174 & 0.155 & 0.153 \\
%   & Mistral-Large-2 & 0.359 & 0.380 & 0.181 & 0.155 & 0.152 \\
%   \bottomrule
%   \end{tabular}%
%   }
%   \label{tab:ragger_success}
% \end{table}



% \begin{table}[htbp]
%   \centering
%   \caption{Searcher Success Metrics of Different Models.}
%   \resizebox{\linewidth}{!}{%
%   \begin{tabular}{ll ccccc}
%   \toprule
%   \textbf{Model Type} & \textbf{Model Name} & \textbf{Setup} & \textbf{Download} & \textbf{Training} & \textbf{Evaluation} & \textbf{Inference} \\
%   \midrule
%   \multirow{3}{*}{\textbf{Claude}} 
%   & Claude-Instant & 0.388 & 0.406 & 0.151 & 0.131 & 0.190 \\
%   & Claude-3-Haiku & 0.385 & 0.338 & 0.155 & 0.070 & 0.163 \\
%   & Claude-3-Sonnet & 0.467 & 0.467 & 0.291 & 0.194 & 0.189 \\
%   \midrule
%   \multirow{3}{*}{\textbf{GPT-4}} 
%   & GPT-4o-Mini & 0.412 & 0.405 & 0.201 & 0.131 & 0.179 \\
%   & GPT-4o & 0.415 & 0.407 & 0.198 & 0.130 & 0.183 \\
%   & GPT-4-Turbo & 0.416 & 0.406 & 0.200 & 0.133 & 0.182 \\
%   \midrule
%   \multirow{4}{*}{\textbf{Llama-3}} 
%   % & Llama-3-8B & 0.392 & 0.443 & 0.176 & 0.102 & 0.165 \\
%   & Llama-3-70B & 0.380 & 0.442 & 0.200 & 0.173 & 0.170 \\
%   & Llama-3.1-8B & 0.318 & 0.447 & 0.201 & 0.157 & 0.165 \\
%   & Llama-3.1-70B & 0.344 & 0.463 & 0.183 & 0.196 & 0.184 \\
%   \midrule
%   \multirow{2}{*}{\textbf{Mistral}} 
%   & Mistral-Large & 0.375 & 0.450 & 0.199 & 0.174 & 0.168 \\
%   & Mistral-Large-2 & 0.373 & 0.452 & 0.201 & 0.172 & 0.169 \\
%   \bottomrule
%   \end{tabular}%
%   }
%   \label{tab:searcher_success}
% \end{table}


\subsection*{Initial Drafter Success Metrics of Different Models}

As shown in Table~\ref{tab:initial_success} and Figure~\ref{fig:initial}, all models perform well on \textbf{Setup} and \textbf{Download} tasks (success rates around 0.23 to 0.28) but struggle with \textbf{Training}, \textbf{Evaluation}, and \textbf{Inference}, where success rates are near zero. This indicates that drafter models handle basic installation effectively but have difficulty with complex tasks requiring updates to file paths and environment variables.

\subsection*{Analyzer Success Metrics of Different Models}

Table~\ref{tab:analyzer_success} and Figure~\ref{fig:analyzer} show noticeable improvements in all tasks compared to the drafter stage. Success rates for \textbf{Setup} and \textbf{Download} increase to around 0.34 to 0.40, while complex tasks see gains up to 0.18. Analyzers leverage dynamic feedback from the environment to refine scripts and correct errors unresolved by the drafters.

\subsection*{Issue Retriever Success Metrics of Different Models}

In the Issue Retriever stage (Table~\ref{tab:ragger_success} and Figure~\ref{fig:retriever}), success rates continue to improve, especially for complex tasks like \textbf{Training}, \textbf{Evaluation}, and \textbf{Inference}, reaching up to 0.25. Access to a knowledge base allows models to retrieve solutions to issues from earlier stages, enhancing performance in complex operations.

\subsection*{Web Searcher Success Metrics of Different Models}

The Searcher Success Metrics in Table~\ref{tab:searcher_success} and Figure~\ref{fig:searcher} exhibit the highest performance. Success rates for \textbf{Setup} and \textbf{Download} reach up to 0.46, and complex tasks improve to between 0.15 and 0.29. Web search enables models to find up-to-date solutions, resolving issues that previous agents could not, leading to substantial improvements.

\begin{table}[htbp]
  \centering
  \caption{Drafter Success Metrics of Different Models.}
  \resizebox{\linewidth}{!}{%
  \begin{tabular}{ll ccccc}
  \toprule
  \textbf{Model Type} & \textbf{Model Name} & \textbf{Setup} & \textbf{Download} & \textbf{Training} & \textbf{Evaluation} & \textbf{Inference} \\
  \midrule
  \multirow[=]{3}{*}{\textbf{Claude}} 
  & Claude-3-Instant & 0.232 & 0.189 & 0.007 & 0.000 & 0.000 \\
  & Claude-3-Haiku & 0.253 & 0.239 & 0.046 & 0.005 & 0.052 \\
  & Claude-3-Sonnet & 0.284 & 0.283 & 0.045 & 0.024 & 0.031 \\
  \midrule
  \multirow[=]{3}{*}{\textbf{GPT-4}} 
  & GPT-4o-Mini & 0.242 & 0.229 & 0.008 & 0.016 & 0.029 \\
  & GPT-4o & 0.261 & 0.238 & 0.039 & 0.022 & 0.031 \\
  & GPT-4-Turbo & 0.271 & 0.252 & 0.028 & 0.039 & 0.032 \\
  \midrule
  \multirow[=]{4}{*}{\textbf{Llama-3}} 
  % & Llama-3-8B & 0.261 & 0.344 & 0.049 & 0.019 & 0.025 \\
  & Llama-3-70B & 0.239 & 0.306 & 0.019 & 0.040 & 0.032 \\
  & Llama-3.1-8B & 0.243 & 0.200 & 0.051 & 0.037 & 0.007 \\
  & Llama-3.1-70B & 0.260 & 0.280 & 0.032 & 0.022 & 0.019 \\
  \midrule
  \multirow[=]{2}{*}{\textbf{Mistral}} 
  & Mistral-Large & 0.243 & 0.266 & 0.047 & 0.031 & 0.026 \\
  & Mistral-Large-2 & 0.251 & 0.279 & 0.039 & 0.025 & 0.024 \\
  \bottomrule
  \end{tabular}%
  }
  \label{tab:initial_success}
\end{table}

\begin{table}[htbp]
  \centering
  \caption{Analyzer Success Metrics of Different Models.}
  \resizebox{\linewidth}{!}{%
  \begin{tabular}{ll ccccc}
  \toprule
  \textbf{Model Type} & \textbf{Model Name} & \textbf{Setup} & \textbf{Download} & \textbf{Training} & \textbf{Evaluation} & \textbf{Inference} \\
  \midrule
  \multirow{3}{*}{\textbf{Claude}} 
  & Claude-3-Instant & 0.342 & 0.353 & 0.104 & 0.109 & 0.151 \\
  & Claude-3-Haiku & 0.350 & 0.301 & 0.132 & 0.037 & 0.130 \\
  & Claude-Sonnet & 0.388 & 0.400 & 0.168 & 0.116 & 0.129 \\
  \midrule
  \multirow{3}{*}{\textbf{GPT-4}} 
  & GPT-4o-Mini & 0.347 & 0.317 & 0.118 & 0.078 & 0.131 \\
  & GPT-4o & 0.362 & 0.353 & 0.148 & 0.115 & 0.145 \\
  & GPT-4-Turbo & 0.353 & 0.322 & 0.161 & 0.094 & 0.148 \\
  \midrule
  \multirow{4}{*}{\textbf{Llama-3}} 
  % & Llama-3-8B & 0.329 & 0.425 & 0.130 & 0.052 & 0.104 \\
  & Llama-3-70B & 0.361 & 0.382 & 0.111 & 0.185 & 0.176 \\
  & Llama-3.1-8B & 0.304 & 0.386 & 0.183 & 0.114 & 0.123 \\
  & Llama-3.1-70B & 0.313 & 0.335 & 0.141 & 0.182 & 0.151 \\
  \midrule
  \multirow{2}{*}{\textbf{Mistral}} 
  & Mistral-Large & 0.324 & 0.349 & 0.121 & 0.143 & 0.144 \\
  & Mistral-Large-2 & 0.340 & 0.357 & 0.152 & 0.199 & 0.163 \\
  \bottomrule
  \end{tabular}%
  }
  \label{tab:analyzer_success}
\end{table}

\begin{table}[htbp]
  \centering
  \caption{Issue Retriever Success Metrics of Different Models.}
  \resizebox{\linewidth}{!}{%
  \begin{tabular}{ll ccccc}
  \toprule
  \textbf{Model Type} & \textbf{Model Name} & \textbf{Setup} & \textbf{Download} & \textbf{Training} & \textbf{Evaluation} & \textbf{Inference} \\
  \midrule
  \multirow{3}{*}{\textbf{Claude}} 
  & Claude-3-Instant & 0.365 & 0.369 & 0.129 & 0.130 & 0.169 \\
  & Claude-3-Haiku & 0.374 & 0.329 & 0.139 & 0.061 & 0.143 \\
  & Claude-Sonnet & 0.442 & 0.436 & 0.254 & 0.183 & 0.163 \\
  \midrule
  \multirow{3}{*}{\textbf{GPT-4}} 
  & GPT-4o-Mini & 0.375 & 0.367 & 0.171 & 0.122 & 0.159 \\
  & GPT-4o & 0.379 & 0.375 & 0.169 & 0.128 & 0.160 \\
  & GPT-4-Turbo & 0.377 & 0.381 & 0.178 & 0.126 & 0.164 \\
  \midrule
  \multirow{4}{*}{\textbf{Llama-3}} 
  % & Llama-3-8B & 0.355 & 0.444 & 0.146 & 0.072 & 0.118 \\
  & Llama-3-70B & 0.364 & 0.399 & 0.113 & 0.159 & 0.154 \\
  & Llama-3.1-8B & 0.305 & 0.389 & 0.182 & 0.115 & 0.122 \\
  & Llama-3.1-70B & 0.312 & 0.334 & 0.143 & 0.179 & 0.153 \\
  \midrule
  \multirow{2}{*}{\textbf{Mistral}} 
  & Mistral-Large & 0.357 & 0.358 & 0.174 & 0.155 & 0.153 \\
  & Mistral-Large-2 & 0.359 & 0.380 & 0.181 & 0.155 & 0.152 \\
  \bottomrule
  \end{tabular}%
  }
  \label{tab:ragger_success}
\end{table}

\begin{table}[htbp]
  \centering
  \caption{Searcher Success Metrics of Different Models.}
  \resizebox{\linewidth}{!}{%
  \begin{tabular}{ll ccccc}
  \toprule
  \textbf{Model Type} & \textbf{Model Name} & \textbf{Setup} & \textbf{Download} & \textbf{Training} & \textbf{Evaluation} & \textbf{Inference} \\
  \midrule
  \multirow{3}{*}{\textbf{Claude}} 
  & Claude-Instant & 0.388 & 0.406 & 0.151 & 0.131 & 0.190 \\
  & Claude-3-Haiku & 0.385 & 0.338 & 0.155 & 0.070 & 0.163 \\
  & Claude-3-Sonnet & 0.467 & 0.467 & 0.291 & 0.194 & 0.189 \\
  \midrule
  \multirow{3}{*}{\textbf{GPT-4}} 
  & GPT-4o-Mini & 0.412 & 0.405 & 0.201 & 0.131 & 0.179 \\
  & GPT-4o & 0.415 & 0.407 & 0.198 & 0.130 & 0.183 \\
  & GPT-4-Turbo & 0.416 & 0.406 & 0.200 & 0.133 & 0.182 \\
  \midrule
  \multirow{4}{*}{\textbf{Llama-3}} 
  % & Llama-3-8B & 0.392 & 0.443 & 0.176 & 0.102 & 0.165 \\
  & Llama-3-70B & 0.380 & 0.442 & 0.200 & 0.173 & 0.170 \\
  & Llama-3.1-8B & 0.318 & 0.447 & 0.201 & 0.157 & 0.165 \\
  & Llama-3.1-70B & 0.344 & 0.463 & 0.183 & 0.196 & 0.184 \\
  \midrule
  \multirow{2}{*}{\textbf{Mistral}} 
  & Mistral-Large & 0.375 & 0.450 & 0.199 & 0.174 & 0.168 \\
  & Mistral-Large-2 & 0.373 & 0.452 & 0.201 & 0.172 & 0.169 \\
  \bottomrule
  \end{tabular}%
  }
  \label{tab:searcher_success}
\end{table}
