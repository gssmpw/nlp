\section{Related Work} \label{related}




% \subsection{Benchmarks in Coding Scenarios}
% \begin{enumerate}
%     \item Code Generation
%     \item Bug Fixing
% \end{enumerate}

% \subsection{Large Language Model Agents}

% At the heart of the LLM Agent is an Agent Core, which coordinates the core \textit{logic} and \textit{behavioral} characteristics of the agent. In addition, the Agent includes the following key components:

% \begin{itemize}
%     \item Memory Module: It consists of both short-term and long-term memory components that record the agent's internal logs and interactions with the user.
%     \item Tools: These are the tools that the agent can use to perform tasks, usually specific third-party APIs.
%     \item Planning Module: This is used for solving complex problems, such as decomposing tasks and problems, reflexivity or critique.
% \end{itemize}

% \subsection{Multi Agent Collaboration Framework}

% MetaGPT \url{https://arxiv.org/abs/2308.00352}


\parabf{Coding \llm{s}.}
Large Language Models (\llm{s}) have become the go-to solution for a wide array of coding tasks due to their exceptional performance in both code generation and comprehension~\cite{codex}. These models have been successfully applied to various software engineering activities, including program synthesis~\cite{patton2024programming, codex, li2022competition, iyer2018mapping}, code translation~\cite{pan2024lost, roziere2020unsupervised, roziere2021leveraging}, program repair~\cite{xia2023repairstudy, chatrepair, monperrus2018living, bouzenia2024repairagent}, and test generation~\cite{titanfuzz, fuzz4all, deng2023fuzzgpt, lemieux2023codamosa, kang2023testing}. Beyond general-purpose \llm{s}, specialized models have been developed by further training on extensive datasets of open-source code snippets. Notable examples of these code-specific \llm{s} include \codex~\cite{codex}, \codellama~\cite{codellama}, StarCoder~\cite{starcoder,starcodertwo}, and \deepseek~\cite{deepseek}. Additionally, instruction-following code models have emerged, refined through instruction-tuning techniques. These include models such as \codellamainstruct~\cite{codellama}, \deepseekinstruct~\cite{deepseek}, \wizardcoder~\cite{wizardcoder}, \magicoder~\cite{magicoder}, and OpenCodeInterpreter~\cite{zheng2024opencodeinterpreter}.

\parabf{Benchmarking \llm-based coding tasks.}
To assess the capabilities of \llm{s} in coding, a variety of benchmarks have been proposed. Among the most widely utilized are \humaneval~\cite{codex} and \mbpp~\cite{austin2021program}, which are handcrafted benchmarks for code generation that include test cases to validate the correctness of \llm outputs. Other benchmarks have been developed to offer more rigorous tests~\cite{evalplus}, cover additional programming languages~\cite{zheng2023codegeex,cassano2023multipl}, and address different programming domains~\cite{livecodebench, hendrycksapps2021, codecontest, ds1000, arcade}.

More recently, research has shifted towards evaluating \llm{s} on real-world software engineering challenges by operating on entire code repositories rather than isolated coding problems~\cite{swebench, zhang2023repocoder, liu2023repobench}. A notable benchmark in this area is \swebench~\cite{swebench}, which includes tasks requiring repository modifications to resolve actual GitHub issues. The authors of \swebench have also released a more focused subset, \swebenchlite~\cite{swebenchlite}, which contains 300 problems centered on bug fixing that only involves single-file modifications in the ground truth patches. ML-Bench \cite{liu2023mlbench} is a benchmark for evaluating large language models and agents for Machine Learning tasks on reporitory-level code. It involves 18 repositories and focuses on code generation and interactions with Jupyter Notebooks.

\parabf{Repository-level coding.}
The rise of agent-based frameworks~\cite{xi2023rise} has spurred the development of agent-based approaches to software engineering tasks. Devin~\cite{devinwebpage} (and its open-source counterpart OpenDevin~\cite{opendevin}) is among the first comprehensive \llm agent-based frameworks. Devin employs agents to first perform task planning based on user requirements, then allows them to use tools like file editors, terminals, and web search engines to iteratively execute the tasks. \sweagent~\cite{sweagent} introduces a custom agent-computer interface (ACI), enabling the \llm agent to interact with the repository environment through actions like reading and editing files or running bash commands. Another agent-based approach, \autocoderover~\cite{autocoderover}, equips the \llm agent with specific APIs (e.g., searching for methods within certain classes) to effectively identify the necessary modifications for issue resolution. Beside these examples, a variety of other agent-based approaches have been developed in both open-source~\cite{aidar} and commercial products~\cite{bouzenia2024repairagent, coder, repounderstander, lingma, factorydroid, ibmagent, opencsgstarship, marscode, amazonqdeveloper}.

% Unlike these agent-based methods, \tech offers a straightforward and cost-efficient solution for addressing real-world software engineering challenges. Our work is the first to demonstrate that an \emph{agentless} approach can achieve comparable performance without the need for complex tools or modeling intricate environment behavior and feedback.

Unlike existing benchmarks and agent-based frameworks, which focus on the code generation/completion tasks, our proposed \model and \agent focus on the code deployment task, which is under-studied in the field.