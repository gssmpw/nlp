% % \vspace{-0.5em}
\section{CSR-Agents}

In this section, we propose \agent, a multi-agent framework that leverages LLMs for different tasks and achieves effective cooperation among agents for code deployment tasks. We introduce our standardized environment for code deployment, functionalities of different agents, and their cooperation workflow.


% % \vspace{-0.5em}
\subsection{Code Deployment with LLM}
% % \vspace{-0.5em}
For each repository, we use the README file under the root folder as the major source of information and prompt LLMs to generate executable bash commands for different steps of deployment including environment setup, data preparation, training, inference and evaluation. 

%In terms of instruction extraction, we use README as the language input and extract the five parts related to environment setup, data preparation, training, inference and testing. For each part, extract the relevant scripts in it, such as downloading and Python execution. 

To achieve reproducbility and safety, we use the Docker container to isolate the code deployment environment. We build a standard Docker image that is equipped with essential tools like bash, Conda, GCC, Make, Python, etc. for evaluations across all repositories and various LLMs. In the container, we use a counter to count the total number of scripts that needed to be executed and the number of scripts that were executed successfully. In this way, we safeguard the entire computing system, especially from bash command executions that involve system-level permissions and could potentially break the whole system.


We note that the use of Docker is not part of the original repositories. The introduction of Docker into our evaluation system can ensure that each time we conduct the evaluation, we start from the same environment for fair comparisons across different LLMs, and the evaluation of one repository does not affect any other repositories. Another benefit of using Docker is that we can speed up the evaluation by running different evaluations in separate docker instances at the same time.

The LLM generated bash commands may not work well for successful deployment due to various reasons. First, in some README files, some basic directives (such as \verb|conda| and \verb|pip|) were missing, so the LLMs could not generate these commands at the first attempt. Secondly, the installation of packages may not be completed in a single attempt and usually needs several iterations of trial-and-error. Thirdly, some steps of the deplopyment require checking additional information like GitHub issues of the corresponding repositories and from the internet. To handle these issues, we design a group of LLM agents that cooperate effectively.
%Through experimentation, we found that sometimes some basic directives (such as \verb|conda| and \verb|pip|) were missing. Error logs of such a missing command are not difficult for LLMs to recognize. Therefore, we designed a repository that had missing packages or packages that needed to be installed or configuration files that needed to be modified. At the same time, we use the repository's flexible file structure as part of the input, which can be used as additional information to help the model understand the previous operations and documentation logic.

% \subsection{{\includegraphics[height=28pt]{assets/GSR-Agent.png}}GSR-Agent: LLM Agent Design}
% % % \vspace{-1em}
\subsection{\raisebox{-12pt}{\includegraphics[height=30pt]{assets/GSR-Agent.png}} \hspace{-0.5em} CSR-Agent: LLM Agent Design}

%\vspace{-1em}
\begin{figure*}[h]
    \centering
    \includegraphics[width=\linewidth]{assets/schema.pdf}
    \caption{Workflow of \model}
    \label{fig:workflow}
\end{figure*}

In \agent, we adopt an iterative trial-and-error process for successful code deployment. Specifically, LLM takes README, directory structure, and error logs from failed command execution as input and generates bash commands to complete the deployment tasks.
The system comprises five agents: Command Drafter, Script Executor, Log Analyzer, Issue Retriver, and Web Searcher. These agents collectively facilitate the deployment of a code repository. The complete workflow is shown in Figure \ref{fig:workflow}.

\textbf{Command Drafter}: This agent reads the README files and directory structure and generates a draft script containing bash commands for deployment. Then, it divides the entire script into five sections, each corresponding to a step in the code repository deployment. This sectional division also serves as an evaluation standard later on.

\textbf{Script Executor}: This agent receives the draft commands from the Command Drafter and execute the commands in our standardized Docker environment. After execution, it collects logs from \verb|bash|, including standard output and errors. Note that during bash script execution, no explicit return code is provided by \verb|bash|. We experimented with setting predefined special prompt to \verb|bash| and parse the return code from the returned output and error message. However, we find that a good quantity of commands do not have return code, making the feedback from \verb|bash| not informative to other agents.  To address this challenge, we leverage the LLM in the executor, instructing it to provide feedback based on the standard output and error messages. We then parse this feedback to generate a return code: if the return code is zero, then the command is executed successfully; otherwise, the command, output, and error messages are logged and sent to the Log Analyzer.


% The Script Executor takes the draft Command from the Drafter and deploys it in a Docker environment equipped with essential tools such as a batch terminal, Conda, GCC, and Make. After execution, it collects the return messages, including standard output and standard error. Note that during bash script execution, no explicit return code is provided by \verb|bash|. We experimented with setting predefined special prompt to \verb|bash| and parse the return code from the returned output and error message. However, we find that a good quantity of commands do not have return code, making the feedback from \verb|bash| not informative to other agents. To resolve this, we integrated an LLM into the script executor, instructing the LLM to provide feedback on the execution provided the standard output and error message (if any). Then parse the feedback from the LLM to generate a return code. If the return code is zero, then the command is executed successfully; otherwise, the command executed, standard outputm and error message are logged and transferred to the Log Analyzer.

\textbf{Log Analyzer}: This agent reads the logs and the associated bash command, and checks for updates, missing prerequisites, or script paths that need updating. It also identifies any other missing components and returns a curated command for a successful execution. We note that this agent only reply on the internal knowledge of the LLM and may not be able to correctly solve the errors.


\textbf{Issue Retriever}: This agent takes in the command, standard output and error message and search them against the issue database we collected from the repository. It leverages RAG and its pipeline requires a search algorithm to query the input against the database. In our design, the queries are the combination of the commands executed, standard output and error messages. The database contains GitHub issues and their communication records for the repository. We experimented with BM25 and Contriever as the retrieval algorithm and decided to use BM25 for (1) BM25's higher search speed and (2) the fact that error logs and issues generally share keywords, so sentimental search do not possess much advantage over lexical search.

\textbf{Web Searcher}: This agent utilizes the Perplexity API to obtain solutions for failed execution. If the pipeline reaches this stage, it indicates that the Log Analyzer and Issue Retriever failed to solve a failed command. The standard output, standard error, and the failed command are fed to Perplexity to search the web for solutions. The Web Searcher then analyzes the solutions from Perplexity and generates new bash commands to resolve the issue.

% \textbf{Deployment Summarizer}: The Deployment Summarizer reviews the entire bash script, the execution, and the deployment and correction process involving the Executor and Log Analyzer. It summarizes the whole deployment process, highlighting successfully deployed parts and any information or action the user requires. For instance, some repositories may require an API key or user agreement to a license before downloading. The Summarizer identifies these human intervention steps. Once these steps are completed, the user can rerun the process to deploy the entire science repository.

% % \vspace{-1em}
\subsection{LLM Coorporation Framwfork}
% % \vspace{-0.5em}
The workflow operates as follows: Deployment commands are drafted from repository documentation, executed in a bash environment, and adjusted based on log analysis if errors arise. Additional information is retrieved from an issue database and web search if needed. The process concludes with a summary that outlines successes and identifies steps needing further attention.

\parabf{Drafting the Initial Commands} The instructions from each repository's README are fed into the Command Drafter LLM Agent to draft the necessary commands for deploying the repository. These commands are organized into five stages: prerequisite installation, data and model checkpoints downloading, training, inference, and evaluation. It is important to note that not every repository contains all these sections, and some sections may be empty.

\parabf{Execution of Commands} After the draft stage, the script drafted will be sent to the Script Executor. To provide LLM Agents with the Python interface, we implement a BashExecutor that encapsulates the bash binary executable file in it. If the commands execute successfully, the process is deemed successful and the deployment pipeline will return \verb|True|. Though looks simple, the bash simulation was not straightforward during our experiments, since we need to consider the environment variables, stdout, stderr, etc. Initially, we used a subprocess to handle it, but the success rate was extremely low. Upon analyzing the logs, we discovered that no commands related to environment variable changes were reflected in subsequent instructions. For instance, if the Command changes the environment to a Python virtual environment or another \verb|conda| environment, this change only applies to that specific command. Afterward, the Python interpreter and package manager revert to their default settings. Besides, not all command executions have a valid return code, therefore, we utilize an LLM to parse the standard output and standard error (if any) to obtain the return state of the execution.


\parabf{Analyze the Execution Log} If the execution is not successful and error occurs, the standard output, standard error, command, and return code are logged and analyzed. The Log Analyzer examines the error messages and attempts to refine the execution commands or adjust prerequisites to ensure the environment is prepared. We have a \verb|max_attempt| argument that limits the number of retries for the log analyzer. If the issue persists after certain attempts, the workflow utilizes the web search tool to request external information.

\parabf{Retrieve Augmented Generation from Issue Database}
After analyzing the execution log and making initial adjustments, the next step is to retrieve more insights using a Retrieve Augmented Generation (RAG) approach. The Issue Retriever agent uses the logged command, output, and error messages to query the issue database for similar past problems or discussions. Leveraging the BM25 retrieval algorithm, it matches keywords from the logs to relevant entries. If a match is found, the agent extracts solutions or troubleshooting steps, feeding them back into the workflow to refine commands. If no relevant match is found, the process escalates to the Web Searcher for external information.

% Once the execution log has been analyzed and any initial adjustments have been made, the next step involves retrieving additional insights from the issue database using a Retrieve Augmented Generation (RAG) approach. The Issue Retriever agent takes the logged command, along with its standard output and error messages, and queries the issue database to find similar past issues or relevant discussions. By leveraging the BM25 retrieval algorithm, the agent performs a lexical search, matching keywords from the logs to entries in the database. If a match is found, the agent extracts the corresponding solution or troubleshooting steps and feeds them back into the workflow. This process helps to automatically generate refined commands or adjustments based on previous resolutions, significantly increasing the chances of successfully overcoming the encountered issues. If the database does not provide a relevant match, the workflow escalates to the Web Searcher for external information.


\parabf{Search the Internet for External Information} Using tools such as Perplexity, the WebSearcher agent integrates the command error logs and standard output with external information. This process refines the command and retries execution in the bash with a limited number of attempts. The agent records the information and transfers it to the deployment summarizer if the issue remains unresolved.
% % % \vspace{-1em}
% \parabf{Summarize the Deployment of Repository} Although large language model agents have demonstrated impressive understanding abilities, their reasoning and action-taking capacities (e.g. acquiring an API by clicking on the LICENSE agreement) remain limited. The DeploymentSummarizer generates a report outlining the steps taken and the expected outcomes. If everything is successful, the report details the deployment process. If not, the summarizer identifies key steps that need human intervention and presents them in the report for further action.

\section{Evaluations}

In this section, we evaluate \agent in \model with a wide range of popular foundation LLM families, including Claude~\footnote{https://www.anthropic.com/news/claude-3-family}, GPT-4~\cite{achiam2023gpt}, Llama-3~\cite{dubey2024llama}, and Mistral~\footnote{https://mistral.ai/technology/\#models}. For each LLM family, we experimented with different model sizes for thorough comparisons.

We show the completion rate in Table~\ref{tab:initial_success}, Table~\ref{tab:analyzer_success}, Table~\ref{tab:ragger_success}, and Table~\ref{tab:searcher_success}. In these tables, we use \textbf{S} to stand for the Setup stage, \textbf{D} to stand for the ownload stage, \textbf{T} to stand for the Training stage, \textbf{E} to stand for the Evaluation stage, and \textbf{I} for the inference stage. 

% \input{assets/tables/success_rate}




% % \begin{figure*}[h]
% %     \centering
% %     \begin{minipage}{0.49\linewidth}
% %         \centering
% %         \includegraphics[width=\linewidth]{assets/agg/Claude_3_Haiku.pdf}
% %         \caption{Claude 3 Haiku}
% %         \label{fig:Claude_3_Haiku}
% %     \end{minipage}
% %     \hfill
% %     \begin{minipage}{0.49\linewidth}
% %         \centering
% %         \includegraphics[width=\linewidth]{assets/agg/Llama_3_8b_Instruct.pdf}
% %         \caption{Llama 3 8B Instruct}
% %         \label{fig:Llama_3_8b_Instruct}
% %     \end{minipage}
% % \end{figure*}



\subsection{Initial Drafter} 
As shown in Table~\ref{tab:initial_success}, all models perform well on \textit{Setup} and \textit{Download} tasks (success rates around 0.23 to 0.28) but struggle with \textit{Training}, \textit{Evaluation}, and \textit{Inference}, where success rates are close to zero. This indicates that the drafter agent handles basic installation effectively but has difficulty with complex tasks requiring updates to file paths and environment variables.

We note that this is also similar to the deployment process of a real researcher, where their first execution is more likely to fail and they need to analyze the errors and leverage tools like GitHub issues or search engines to solve the problem.


\begin{table}[htbp]
  \centering
  \caption{Drafter Success Metrics of Different Models.}
  %\vspace{-0.5em}
  \resizebox{\linewidth}{!}{%
  \begin{tabular}{ll ccccc}
  \toprule
  \textbf{Model Type} & \textbf{Name} & \textbf{S} & \textbf{D} & \textbf{T} & \textbf{E} & \textbf{I} \\
  \midrule
  \multirow[=]{3}{*}{\textbf{Claude}} 
  & Instant & 0.232 & 0.189 & 0.007 & 0.000 & 0.000 \\
  & 3-Haiku & 0.253 & 0.239 & 0.046 & 0.005 & 0.052 \\
  & 3-Sonnet & 0.284 & 0.283 & 0.045 & 0.024 & 0.031 \\
  \midrule
  \multirow[=]{3}{*}{\textbf{GPT}} 
  & 4o-Mini & 0.242 & 0.229 & 0.008 & 0.016 & 0.029 \\
  & 4o & 0.261 & 0.238 & 0.039 & 0.022 & 0.031 \\
  & 4-Turbo & 0.271 & 0.252 & 0.028 & 0.039 & 0.032 \\
  \midrule
  \multirow[=]{4}{*}{\textbf{Llama}} 
  % & Llama-3-8B & 0.261 & 0.344 & 0.049 & 0.019 & 0.025 \\
  & 3-70B & 0.239 & 0.306 & 0.019 & 0.040 & 0.032 \\
  & 3.1-8B & 0.243 & 0.200 & 0.051 & 0.037 & 0.007 \\
  & 3.1-70B & 0.260 & 0.280 & 0.032 & 0.022 & 0.019 \\
  \midrule
  \multirow[=]{2}{*}{\textbf{Mistral}} 
  & Large & 0.243 & 0.266 & 0.047 & 0.031 & 0.026 \\
  & Large-2 & 0.251 & 0.279 & 0.039 & 0.025 & 0.024 \\
  \bottomrule
  \end{tabular}%
  }
  \label{tab:initial_success}
  %\vspace{-1.5em}
\end{table}





\subsection{Log Analyzer} 
Table~\ref{tab:analyzer_success} shows noticeable improvements in all tasks compared to the drafter stage. Success rates for \textit{Setup} and \textit{Download} increase to around 0.34 to 0.40, while complex tasks see gains up to 0.18.

\begin{table}[htbp]
  \centering
  \caption{Analyzer Success Metrics of Different Models.}

  \resizebox{\linewidth}{!}{%
  \begin{tabular}{ll ccccc}
  \toprule
  \textbf{Model Type} & \textbf{Name} & \textbf{S} & \textbf{D} & \textbf{T} & \textbf{E} & \textbf{I} \\
  \midrule
  \multirow{3}{*}{\textbf{Claude}} 
  & Instant & 0.342 & 0.353 & 0.104 & 0.109 & 0.151 \\
  & 3-Haiku & 0.350 & 0.301 & 0.132 & 0.037 & 0.130 \\
  & 3-Sonnet & 0.388 & 0.400 & 0.168 & 0.116 & 0.129 \\
  \midrule
  \multirow{3}{*}{\textbf{GPT}} 
  & 4o-Mini & 0.347 & 0.317 & 0.118 & 0.078 & 0.131 \\
  & 4o & 0.362 & 0.353 & 0.148 & 0.115 & 0.145 \\
  & 4-Turbo & 0.353 & 0.322 & 0.161 & 0.094 & 0.148 \\
  \midrule
  \multirow{4}{*}{\textbf{Llama}} 
  % & Llama-3-8B & 0.329 & 0.425 & 0.130 & 0.052 & 0.104 \\
  & 3-70B & 0.361 & 0.382 & 0.111 & 0.185 & 0.176 \\
  & 3.1-8B & 0.304 & 0.386 & 0.183 & 0.114 & 0.123 \\
  & 3.1-70B & 0.313 & 0.335 & 0.141 & 0.182 & 0.151 \\
  \midrule
  \multirow{2}{*}{\textbf{Mistral}} 
  & Large & 0.324 & 0.349 & 0.121 & 0.143 & 0.144 \\
  & Large-2 & 0.340 & 0.357 & 0.152 & 0.199 & 0.163 \\
  \bottomrule
  \end{tabular}%
  }

  \label{tab:analyzer_success}
\end{table}

Analyzers leverage dynamic feedback from the execution of commands to refine scripts and try to correct errors from executed commands.



\subsection{Issue Retriever} 
In the Issue Retriever stage (Table~\ref{tab:ragger_success}), success rates continue to improve, especially for complex tasks like \textbf{Training}, \textbf{Evaluation}, and \textbf{Inference}, reaching up to 0.25.

\begin{table}[tbp]
  \centering
  \caption{Issue Retriever Success Metrics of Different Models.}
  %\vspace{-0.5em}
  \resizebox{\linewidth}{!}{%
  \begin{tabular}{ll ccccc}
  \toprule
  \textbf{Model Type} & \textbf{Name} & \textbf{S} & \textbf{D} & \textbf{T} & \textbf{E} & \textbf{I} \\
  \midrule
  \multirow{3}{*}{\textbf{Claude}} 
  & Instant & 0.365 & 0.369 & 0.129 & 0.130 & 0.169 \\
  & 3-Haiku & 0.374 & 0.329 & 0.139 & 0.061 & 0.143 \\
  & 3-Sonnet & 0.442 & 0.436 & 0.254 & 0.183 & 0.163 \\
  \midrule
  \multirow{3}{*}{\textbf{GPT}} 
  & 4o-Mini & 0.375 & 0.367 & 0.171 & 0.122 & 0.159 \\
  & 4o & 0.379 & 0.375 & 0.169 & 0.128 & 0.160 \\
  & 4-Turbo & 0.377 & 0.381 & 0.178 & 0.126 & 0.164 \\
  \midrule
  \multirow{4}{*}{\textbf{Llama}} 
  % & Llama-3-8B & 0.355 & 0.444 & 0.146 & 0.072 & 0.118 \\
  & 3-70B & 0.364 & 0.399 & 0.113 & 0.159 & 0.154 \\
  & 3.1-8B & 0.305 & 0.389 & 0.182 & 0.115 & 0.122 \\
  & 3.1-70B & 0.312 & 0.334 & 0.143 & 0.179 & 0.153 \\
  \midrule
  \multirow{2}{*}{\textbf{Mistral}} 
  & Large & 0.357 & 0.358 & 0.174 & 0.155 & 0.153 \\
  & Large-2 & 0.359 & 0.380 & 0.181 & 0.155 & 0.152 \\
  \bottomrule
  \end{tabular}%
  }
  \label{tab:ragger_success}
  %\vspace{-0.5em}
\end{table}

The results show that access to a knowledge base with informative discussions on issues of the code repository allows LLMs to retrieve solutions to execution errors from earlier stages, enhancing performance in complex operations.


\subsection{Web Searcher}
The Searcher Success Metrics in Table~\ref{tab:searcher_success} exhibits the highest performance. Success rates for \textit{Setup} and \textit{Download} reach up to 0.46, and complex tasks improve to between 0.15 and 0.29.

\begin{table}[htbp]
  \centering
  \caption{Searcher Success Metrics of Different Models.}
  %\vspace{-0.5em}
  \resizebox{\linewidth}{!}{%
  \begin{tabular}{ll ccccc}
  \toprule
  \textbf{Model Type} & \textbf{Name} & \textbf{S} & \textbf{D} & \textbf{T} & \textbf{E} & \textbf{I} \\
  \midrule
  \multirow{3}{*}{\textbf{Claude}} 
  & Instant & 0.388 & 0.406 & 0.151 & 0.131 & 0.190 \\
  & 3-Haiku & 0.385 & 0.338 & 0.155 & 0.070 & 0.163 \\
  & 3-Sonnet & 0.467 & 0.467 & 0.291 & 0.194 & 0.189 \\
  \midrule
  \multirow{3}{*}{\textbf{GPT}} 
  & 4o-Mini & 0.412 & 0.405 & 0.201 & 0.131 & 0.179 \\
  & 4o & 0.415 & 0.407 & 0.198 & 0.130 & 0.183 \\
  & 4-Turbo & 0.416 & 0.406 & 0.200 & 0.133 & 0.182 \\
  \midrule
  \multirow{4}{*}{\textbf{Llama}} 
  % & Llama-3-8B & 0.392 & 0.443 & 0.176 & 0.102 & 0.165 \\
  & 3-70B & 0.380 & 0.442 & 0.200 & 0.173 & 0.170 \\
  & 3.1-8B & 0.318 & 0.447 & 0.201 & 0.157 & 0.165 \\
  & 3.1-70B & 0.344 & 0.463 & 0.183 & 0.196 & 0.184 \\
  \midrule
  \multirow{2}{*}{\textbf{Mistral}} 
  & Mistral-Large & 0.375 & 0.450 & 0.199 & 0.174 & 0.168 \\
  & Mistral-Large-2 & 0.373 & 0.452 & 0.201 & 0.172 & 0.169 \\
  \bottomrule
  \end{tabular}%
  }
  \label{tab:searcher_success}
  \vspace{-0.5em}
\end{table}

Web search enables models to find up-to-date solutions, resolving issues that previous agents could not, leading to substantial improvements.



% \begin{figure*}[htpb]
%     \centering
%     \begin{minipage}{0.49\linewidth}
%         \centering
%         \includegraphics[width=\linewidth]{assets/agg/Initial_success_rates.pdf}
%         \caption{Initial Drafter Success Ratio}
%         \label{fig:initial}
%     \end{minipage}
%     \hfill
%     \begin{minipage}{0.49\linewidth}
%         \centering
%         \includegraphics[width=\linewidth]{assets/agg/Analyzer_success_rates.pdf}
%         \caption{Log Analyzer Success Ratio}
%         \label{fig:analyzer}
%     \end{minipage}
% \end{figure*}

% \begin{figure*}[htpb]
%     \centering
%     \begin{minipage}{0.49\linewidth}
%         \centering
%         \includegraphics[width=\linewidth]{assets/agg/Issue_Retriever_success_rates.pdf}
%         \caption{Issue Retriever Success Ratio}
%         \label{fig:retriever}
%     \end{minipage}
%     \hfill
%     \begin{minipage}{0.49\linewidth}
%         \centering
%         \includegraphics[width=\linewidth]{assets/agg/Searcher_success_rates.pdf}
%         \caption{Web Searcher Success Ratio}
%         \label{fig:searcher}
%     \end{minipage}
% \end{figure*}


\subsection{Aggregated Results}

\begin{figure*}[htbp]
    \centering
    \begin{minipage}[b]{0.48\linewidth}
        \centering
        \includegraphics[width=\linewidth]{assets/agg/Claude_3_Sonnet.pdf}
        \caption{Performance of Claude 3 Sonnet}
        \label{fig:claude}
    \end{minipage}
    \hfill
    \begin{minipage}[b]{0.48\linewidth}
        \centering
        \includegraphics[width=\linewidth]{assets/agg/GPT_4o.pdf}
        \caption{Performance of GPT 4o}
        \label{fig:gpt}
    \end{minipage}

\end{figure*}

\begin{figure*}[htbp]
    \centering
    \begin{minipage}[b]{0.48\linewidth}
        \centering
        \includegraphics[width=\linewidth]{assets/agg/Llama_3.1_70b_Instruct.pdf}
        \caption{Performance of Llama 3.1 70B Instruct}
        \label{fig:llama}
    \end{minipage}
    \hfill
    \begin{minipage}[b]{0.48\linewidth}
        \centering
        \includegraphics[width=\linewidth]{assets/agg/Mistral_Large_2.pdf}
        \caption{Performance of Mistral Large 2}
        \label{fig:mistral}
    \end{minipage}

\end{figure*}

We show the aggregated results of a single LLM on different tasks with different level of engagements of multi-agents in Figure~\ref{fig:claude}, Figure~\ref{fig:gpt}, Figure~\ref{fig:llama}, and Figure~\ref{fig:mistral}. In short, with more agents contributing to solving the tasks, the success rate increases across all tasks for all LLMs, which demonstrates the effectiveness of our proposed \agent.