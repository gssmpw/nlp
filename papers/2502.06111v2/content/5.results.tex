\subsection{Results Interpretation} \label{results}
%  \vspace{-1em}
The evaluation of the \model involved assessing the performance of various large language models (LLMs) across key tasks: Setup, Download, Training, Inference, and Evaluation. These tasks are essential for deploying repositories within the \model.





% \begin{table*}[h!]
% \centering
% \caption{Performance of Different Models Across Stages (Drafter Only)}
% \label{tab:performance_models_stages_drafter_only}
% \begin{tabular}{lcccccc}
% \toprule
% \textbf{Model} & \textbf{Environment Preparation} & \textbf{Data/Checkpoint} & \textbf{Training} & \textbf{Evaluation} & \textbf{Inference/Demo} \\
% \midrule
% LLama2         & X1 & Y1 & Z1 & W1 & V1 \\
% LLama3         & X2 & Y2 & Z2 & W2 & V2 \\
% Claude 3       & X3 & Y3 & Z3 & W3 & V3 \\
% \bottomrule
% \end{tabular}
% \end{table*}


% \begin{figure}[htbp]
%     \centerline{\includesvg[width=0.75\linewidth]{assets/GSRBench_Sunburst.svg}}
%     \caption{\model Topic Distribution}
%     \label{fig:sunburst}
% \end{figure}

% \begin{figure}[htbp]
%     \centerline{\includesvg[width=0.75\linewidth]{assets/GSRBench_Conf.svg}}
%     \caption{\model Topic Distribution}
%     \label{fig:sunburst}
% \end{figure}



% \begin{figure*}[htbp]
%     \centering
%     \includegraphics[width=0.7\linewidth]{assets/success_rate_histograms.pdf}
%     \caption{Success Ratio Histogram}
%     \label{fig:success_ratio}
% \end{figure*}




% \begin{figure*}[htbp]
%     \centering
%     \includegraphics[width=0.7\linewidth]{assets/agg/Claude_3_Sonnet.pdf}
%     \caption{Claude 3 Sonnet}
%     \label{fig:Claude_3_Sonnet}
% \end{figure*}

% \begin{figure*}[htbp]
%     \centering
%     \includegraphics[width=0.7\linewidth]{assets/agg/Llama_3_8b_Instruct.pdf}
%     \caption{Llama 3 8B Instruct}
%     \label{fig:Llama_3_8b_Instruct}
% \end{figure*}

% \begin{figure*}[htbp]
%     \centering
%     \includegraphics[width=0.7\linewidth]{assets/agg/Mistral_Large.pdf}
%     \caption{Mistral Large}
%     \label{fig:Mistral_Large}
% \end{figure*}




\subsubsection{Task-Specific Outcomes}
% % \vspace{-0.5em}
\parabf{Setup and Download:} Most models consistently performed well, reflecting their capability to initiate and manage basic deployment processes.



\parabf{Inference and Evaluation:} Performance was less consistent, with some models demonstrating moderate success, but generally struggling with the complexity of these tasks.

% % \vspace{-0.5em}
\parabf{Training:} Training tasks are particularly challenging, with lower success rates across the board, indicating that current LLMs require further refinement to handle training processes effectively.


% % \vspace{-1em}
\subsubsection{Overall Performance}





The success metrics across different tasks and models indicate a wide variability in performance. Generally, models showed higher success rates in Setup and Download tasks, with performance tapering off in more complex tasks such as Inference, Evaluation, and Training. This pattern highlights the challenges LLMs face in handling the full deployment process autonomously.

The results demonstrate that while LLMs have made significant strides in automating repository deployment, their ability to manage complex tasks remains limited. Improvements are needed, particularly in the areas of Inference and Training, to achieve fully autonomous and reliable deployment of science repositories.

However, there is still a large gap between LLMs and real scientists even if the advnanced tools are provided to the LLMs. To explain, it is not trivial to handle the nuances in the experiement environment setup for the science repositories. For example, the hardware and software compatibility issues are very common in code deployment and often causes confusions even for domain experts.
% % % \vspace{-1em}
% \begin{table*}[htbp]
%   \centering
%   \caption{Success Metrics Across Different Tasks}
%   \resizebox{\linewidth}{!}{%
%   \begin{tabular}{ll ccccc}
%   \toprule
%   \textbf{Model Type} & \textbf{Setup} & \textbf{Download} & \textbf{Inference} & \textbf{Evaluation} & \textbf{Training} \\
%   \midrule
%   Claude & 0.232--0.467 & 0.189--0.467 & 0.000--0.190 & 0.000--0.194 & 0.007--0.291 \\
%   Llama-2 & 0.152--0.840 & 0.182--1.000 & 0.000--0.467 & 0.000--0.700 & 0.000--1.000 \\
%   Llama-3 & 0.251--0.444 & 0.200--0.629 & 0.007--0.268 & 0.019--0.238 & 0.019--0.259 \\
%   Mistral & 0.170--0.427 & 0.178--0.495 & 0.029--0.291 & 0.024--0.317 & 0.040--0.444 \\
%   \bottomrule
%   \end{tabular}%
%   }
%   \label{tab:success_summary}
% \end{table*}


% % % \vspace{-1em}
% \begin{figure*}[htbp]
%     \centering
%     \begin{minipage}{0.49\linewidth}
%         \centering
%         \includegraphics[width=\linewidth]{assets/agg/Mistral_Large.pdf}
%         \caption{Mistral Large}
%         \label{fig:Mistral_Large}
%     \end{minipage}
%     \hfill
%     \begin{minipage}{0.49\linewidth}
%         \centering
%         \includegraphics[width=\linewidth]{assets/agg/Llama_3.1_8b_Instruct.pdf}
%         \caption{Llama 3.1 8B Instruct}
%         \label{fig:Llama_3.1_8b_Instruct}
%     \end{minipage}
%     % % \vspace{-1em}
% \end{figure*}
