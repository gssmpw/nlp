\section{Introduction} \label{intro}

With the rapid evolution of Large Language Models (LLMs), it has been demonstrated that LLMs have increased reasoning ability over the last few years, making intelligent agents based on LLM possible. Current agent-related applications in computer science include code writing (Codex~\cite{codex}, Deepseek Code~\cite{deepseek}, CodeLLaMA~\cite{codellama}, etc.), code base generation (MetaGPT~\cite{hong2023metagpt}, Agentless~\cite{xia2024agentless} and CodeStar\cite{starcoder}), code correction (SWEBench~\cite{swebench}), and more.

In computer science research projects, the associated codebases grow very rapidly, and a self-consistent codebase typically has several parts, including the instruction file (e.g., the README file), associated code packages, and related data. The instruction file usually contains an overview of the project, including environment setup, data preparation (e.g., data download and pre-processing, model weights download and preparation), model training process, performance evaluation, and the setup of a demo project (e.g., a chatbot project that interacts with users). In computer science, prestigious conferences, including NAACL, ACL, ICLR,  NeurIPS, CVPR, KDD, etc., encourage researchers to release source code for reproducibility of their accepted papers, and GitHub is the top choice for maintaining codebases for most researchers. A major step of computer science research is reproducing existing work, which is essential to gain insights and propose novel methodologies. However, even for well-documented and self-consistent projects, the setup process requires manual efforts and cannot be fully automated; many steps of setting up a code repository are rather mechanical, such as installing/updating dependency packages to configure the environment, downloading data, updating the relevant script/data directories, etc., which is tedious and often time-consuming. 

To tackle such challenges, we propose to use LLM agents for automating the deployment of code repositories of research projects, and build a benchmark, \textbf{C}omputer \textbf{S}cience \textbf{R}esearch \textbf{Bench}mark (\model) for evaluating LLM Agents on code repository deployment tasks. We also propose a multi-agent collaborative framework, \agent, to automate the deployment of code repositories by coordinating multiple LLM agents with different expertise and iterative improvement with provided tools. From more than ten major NLP/CV/AI/ML/DM conferences, we collected the 100 highly rated repositories\footnote{With appropriately permissive licenses.}, which are carefully selected from an initial candidate set that has more than 1500 top-star repositories. %Of the initial collection, repositories with more than 100 stars are kept. 
Our selection criteria include topic diversity and self-containment\footnote{The information within the repository is mostly sufficient for a successful deployment.} so that \model can provide a comprehensive evaluation of LLM agents on code deployment tasks including instruction generation, command execution, and self-improvement with tools.

\iffalse % 3 difficuly levels
The repositories in \model are at different levels of difficulty.
\begin{itemize}
    \item Easy: Repositories with standard environment configurations (e.g. \verb|requirements.txt| or \verb|conda_env.yaml|). The commands in the instructions and code are relative paths instead of absolution paths (which do not depend on the environment in which they are executed). And no additional files (dataset, model checkpoints, etc) downloading is required.
    \item Medium: Repositories that use descriptive instructions (e.g. ``Install Python 3.6'', ``Run the main.py script'') rather than rigorous commands (e.g. \verb|apt install package|, \verb|python script.py|). LLM agents need to be able to understand the needs (deployment of the repositories) and generate rigorous commands (Linux commands).
    \item Difficult: Hierarchical repositories with an entry README in the root directory. While there are detailed instructions in the subdirectories (e.g. \verb|./datasets|, \verb|./train|) and the entry README has links to the related documents, it remains challenging for LLM agents. Since LLM agents are required to understand the contents of the entry README, the directory structure, as well as being able to go to the corresponding directory to read its corresponding documents. With such information, the LLM agents will be able to translate instructions into commands for deployment.
\end{itemize}
\fi 

% To the best of our knowledge, \model is the first benchmark for scientific project exploration, which provides a high-quality benchmark as a reference for the testing of LLM agents. And we propose a multi-intelligence collaborative framework for repository configuration on this basis.

% Our contributions are categorized into the following three points
% \begin{enumerate}
%     \item We propose \model to study LLM's ability to understand instruction manuals and the structure of projects organized in multiple files.
%     \item We propose the GSR Agent framework, which uses multiagent for collaboration, with different agents having different capabilities (instruction comprehension, execution of instructions, log analysis and error correction, etc.).
%     \item We propose a system of testing that provides a relatively standardized process to draw upon for the reproducibility of experiment-based scientific projects. For example, the proposal of CI/CD in software development provides developers with a relatively standardized process, which allows for quality assurance of the continuous deployment and updating of projects. In the field of scientific research, in order to protect scientific research projects, especially those in the field of artificial intelligence and machine learning, we propose a project structure that further ensures the ease of use, reusability, and reproducibility of the project; reduces the cost of communication between the publisher and the user, and improves the efficiency of scientific research communication and collaboration.
% \end{enumerate}


To the best of our knowledge, \model is the first benchmark for the deployment of computer science research projects, providing a reference for evaluating LLM agents. We note that the success of code deployment depends not only on code generation, but also on many non-coding factors including tasks like experimental environment setup, data/model preparation, correcting bash commands, searching for solutions, and etc. Our proposed multi-agent framework, \agent, aims to achieve automation of code deployment, which can accelerate the progress for computer science research projects.

Our contributions are as follows:
\begin{itemize}
    \item We introduce \model to assess LLM's ability to understand instruction manuals and complex project structures, generate executable commands for code deployment, and solve errors during deployment.
    \item We propose the \agent framework, which leverages multi-agent cooperation with specialized capabilities including instruction comprehension, command execution, error log analysis, and error correction with searching and retrieval tools.
    \item We design a standardized testing system for reproducibility in \model, which can make \agent a CI/CD\footnote{Continuous Integration and Continuous Delivery.} standard system of computer science code repository deployment, ensuring ease of use, reusability, and improving communication and collaboration efficiency in computer science research projects.
    \item We evaluated a wide range of foundation models for \agent on \model. Results indicate that LLM agents can potentially accelerate the process of repository deployment, thereby boosting researcher productivity. However, it is still challenging to achieve full automation.
\end{itemize}


% \begin{enumerate}
%     \item StarCoder
%     \item \url{https://github.com/coinse/GHRB}

% \end{enumerate}




% \vspace{-1em}
% \begin{table*}[htbp]
%   \centering
%   \caption{Initial Success Metrics Across Different Models}
%   \resizebox{\linewidth}{!}{%
%   \begin{tabular}{ll ccccc}
%   \toprule
%   \textbf{Model Type} & \textbf{Model Name} & \textbf{Setup} & \textbf{Download} & \textbf{Inference} & \textbf{Evaluation} & \textbf{Training} \\
%   \midrule
%   \multirow[=]{3}{*}{\textbf{Claude}} 
%   & Claude-3-Sonnet & 0.232 & 0.189 & 0.000 & 0.000 & 0.007 \\
%   & Claude-3-Haiku & 0.253 & 0.239 & 0.052 & 0.005 & 0.046 \\
%   & Claude-Instant & 0.284 & 0.283 & 0.031 & 0.024 & 0.045 \\
%   \midrule
%   \multirow[=]{4}{*}{\textbf{Llama-3}} 
%   & Llama-3-8B-Instruct & 0.261 & 0.344 & 0.025 & 0.019 & 0.049 \\
%   & Llama-3-70B-Instruct & 0.251 & 0.506 & 0.032 & 0.040 & 0.019 \\
%   & Llama-3.1-8B-Instruct & 0.251 & 0.200 & 0.007 & 0.037 & 0.051 \\
%   & Llama-3.1-70B-Instruct & 0.260 & 0.280 & 0.019 & 0.022 & 0.032 \\
%   \midrule
%   \multirow[=]{1}{*}{\textbf{Mistral}} 
%   & Mistral-Large & 0.230 & 0.264 & 0.054 & 0.047 & 0.048 \\
%   \midrule
%   \multirow[=]{2}{*}{\textbf{GPT-4}} 
%   & GPT-4o & 0.116 & 0.125 & 0.012 & 0.015 & 0.007 \\
%   & GPT-4-Turbo & 0.021 & 0.074 & 0.031 & 0.040 & 0.027 \\
%   \bottomrule
%   \end{tabular}%
%   }
%   \label{tab:initial_success}
% \end{table*}


% \begin{figure*}[h]
%     \centering
%     \begin{minipage}{0.49\linewidth}
%         \centering
%         \includegraphics[width=\linewidth]{assets/agg/Claude_3_Haiku.pdf}
%         \caption{Claude 3 Haiku}
%         \label{fig:Claude_3_Haiku}
%     \end{minipage}
%     \hfill
%     \begin{minipage}{0.49\linewidth}
%         \centering
%         \includegraphics[width=\linewidth]{assets/agg/Llama_3_8b_Instruct.pdf}
%         \caption{Llama 3 8B Instruct}
%         \label{fig:Llama_3_8b_Instruct}
%     \end{minipage}
% \end{figure*}


% \vspace{-1em}
% \begin{table*}[htbp]
%   \centering
%   \caption{Analyzer Success Metrics Across Different Models}
%   \resizebox{\linewidth}{!}{%
%   \begin{tabular}{ll ccccc}
%   \toprule
%   \textbf{Model Type} & \textbf{Model Name} & \textbf{Setup} & \textbf{Download} & \textbf{Inference} & \textbf{Evaluation} & \textbf{Training} \\
%   \midrule
%   \multirow[=]{3}{*}{\textbf{Claude}} 
%   & Claude-3-Sonnet & 0.340 & 0.350 & 0.155 & 0.107 & 0.107 \\
%   & Claude-3-Haiku & 0.353 & 0.299 & 0.125 & 0.038 & 0.128 \\
%   & Claude-Instant & 0.387 & 0.402 & 0.132 & 0.113 & 0.173 \\
%   \midrule
%   \multirow[=]{4}{*}{\textbf{Llama-3}} 
%   & Llama-3-8B-Instruct & 0.326 & 0.427 & 0.100 & 0.053 & 0.128 \\
%   & Llama-3-70B-Instruct & 0.363 & 0.581 & 0.179 & 0.183 & 0.108 \\
%   & Llama-3.1-8B-Instruct & 0.271 & 0.385 & 0.069 & 0.112 & 0.187 \\
%   & Llama-3.1-70B-Instruct & 0.311 & 0.338 & 0.150 & 0.185 & 0.139 \\
%   \midrule
%   \multirow[=]{1}{*}{\textbf{Mistral}} 
%   & Mistral-Large & 0.322 & 0.443 & 0.236 & 0.140 & 0.120 \\
%   \midrule
%   \multirow[=]{2}{*}{\textbf{GPT-4}} 
%   & GPT-4o & 0.116 & 0.125 & 0.073 & 0.030 & 0.022 \\
%   & GPT-4-Turbo & 0.196 & 0.200 & 0.145 & 0.149 & 0.262 \\
%   \bottomrule
%   \end{tabular}%
%   }
%   \label{tab:analyzer_success}
% \end{table*}




% \begin{table*}[htbp]
%   \centering
%   \caption{Issue Retriever Success Metrics Across Different Models}
%   \resizebox{\linewidth}{!}{%
%   \begin{tabular}{ll ccccc}
%   \toprule
%   \textbf{Model Type} & \textbf{Model Name} & \textbf{Setup} & \textbf{Download} & \textbf{Inference} & \textbf{Evaluation} & \textbf{Training} \\
%   \midrule
%   \multirow[=]{3}{*}{\textbf{Claude}} 
%   & Claude-3-Sonnet & 0.364 & 0.371 & 0.167 & 0.131 & 0.128 \\
%   & Claude-3-Haiku & 0.375 & 0.327 & 0.145 & 0.059 & 0.140 \\
%   & Claude-Instant & 0.440 & 0.435 & 0.164 & 0.185 & 0.251 \\
%   \midrule
%   \multirow[=]{4}{*}{\textbf{Llama-3}} 
%   & Llama-3-8B-Instruct & 0.357 & 0.443 & 0.117 & 0.073 & 0.148 \\
%   & Llama-3-70B-Instruct & 0.435 & 0.608 & 0.253 & 0.230 & 0.181 \\
%   & Llama-3.1-8B-Instruct & 0.297 & 0.405 & 0.069 & 0.127 & 0.228 \\
%   & Llama-3.1-70B-Instruct & 0.335 & 0.343 & 0.174 & 0.185 & 0.163 \\
%   \midrule
%   \multirow[=]{1}{*}{\textbf{Mistral}} 
%   & Mistral-Large & 0.352 & 0.471 & 0.277 & 0.171 & 0.139 \\
%   \midrule
%   \multirow[=]{2}{*}{\textbf{GPT-4}} 
%   & GPT-4o & 0.116 & 0.125 & 0.110 & 0.105 & 0.051 \\
%   & GPT-4-Turbo & 0.378 & 0.503 & 0.304 & 0.149 & 0.215 \\
%   \bottomrule
%   \end{tabular}%
%   }
%   \label{tab:ragger_success}
% \end{table*}

% \vspace{-1em}
% \begin{figure*}[htbp]
%     \centering
%     \begin{minipage}{0.49\linewidth}
%         \centering
%         \includegraphics[width=\linewidth]{assets/agg/GPT_4o.pdf}
%         \caption{GPT-4o}
%         \label{fig:GPT_4o}
%     \end{minipage}
%     \hfill
%     \begin{minipage}{0.49\linewidth}
%         \centering
%         \includegraphics[width=\linewidth]{assets/agg/GPT_4_Turbo.pdf}
%         \caption{GPT-4 Turbo}
%         \label{fig:GPT_4_Turbo}
%     \end{minipage}
% \end{figure*}


% \begin{table*}[htbp]
%   \centering
%   \caption{Searcher Success Metrics Across Different Models}
%   \resizebox{\linewidth}{!}{%
%   \begin{tabular}{ll ccccc}
%   \toprule
%   \textbf{Model Type} & \textbf{Model Name} & \textbf{Setup} & \textbf{Download} & \textbf{Inference} & \textbf{Evaluation} & \textbf{Training} \\
%   \midrule
%   \multirow[=]{3}{*}{\textbf{Claude}} 
%   & Claude-3-Sonnet & 0.388 & 0.406 & 0.190 & 0.131 & 0.151 \\
%   & Claude-3-Haiku & 0.385 & 0.338 & 0.163 & 0.070 & 0.155 \\
%   & Claude-Instant & 0.467 & 0.467 & 0.189 & 0.194 & 0.291 \\
%   \midrule
%   \multirow[=]{4}{*}{\textbf{Llama-3}} 
%   & Llama-3-8B-Instruct & 0.392 & 0.443 & 0.125 & 0.102 & 0.176 \\
%   & Llama-3-70B-Instruct & 0.444 & 0.629 & 0.268 & 0.238 & 0.185 \\
%   & Llama-3.1-8B-Instruct & 0.318 & 0.447 & 0.092 & 0.157 & 0.259 \\
%   & Llama-3.1-70B-Instruct & 0.344 & 0.367 & 0.184 & 0.196 & 0.183 \\
%   \midrule
%   \multirow[=]{1}{*}{\textbf{Mistral}} 
%   & Mistral-Large & 0.360 & 0.486 & 0.291 & 0.178 & 0.149 \\
%   \midrule
%   \multirow[=]{2}{*}{\textbf{GPT-4}} 
%   & GPT-4o & 0.116 & 0.125 & 0.134 & 0.120 & 0.102 \\
%   & GPT-4-Turbo & 0.378 & 0.503 & 0.304 & 0.188 & 0.262 \\
%   \bottomrule
%   \end{tabular}%
%   }
%   \label{tab:searcher_success}
% \end{table*}
