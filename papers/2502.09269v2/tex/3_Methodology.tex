\section{Methodology}
\subsubsection{Ensemble Strategy} We define superadditive among classifiers as follows:
    \begin{equation}
        f(\{\hat{y}_{i}(\textbf{x})\}_{N}) \succeq \frac{1}{N} \sum_{i=1}^{N} \hat{y}_{i}(\textbf{x}), \quad \hat{y}_{i}(\textbf{x}) \in \mathbb{R}^{+}_{D \times 4 \times H \times W},
    \end{equation}
where $\textbf{x}$ is an arbitrary 3D frame from the 4D cardiac cine sequence sample space $\mathcal{X}$. The ensemble method is denoted as \( f(\cdot) \), with \( N \) classifiers. \(\hat{y}_{i}(\textbf{x})\) is the probabilities of the \textit{i}-th classifierâ€™s segmentation prediction. The four channels represent BG, RV, MYO, and LV, while \(D, H, W\) denote the depth, height, and width of the 3D frame. $\succeq$ indicates that classifiers collaborate better on the Average DSC metric in Eq. \ref{eq:DSC} than working individually.

To synthesize the outputs from multiple classifiers for superadditive improvement, an intuition is to construct a convex combination, and then use the Softmax function to implement Majority Vote Soft pooling in ensemble learning:
\begin{equation}
\hat{y}(\textbf{x}) = \text{Softmax}\left(\sum_{\textit{i}=1}^{\textit{N}} \omega_{\textit{i}} \cdot \hat{y}_{\textit{i}}(\textbf{x})\right) \quad , \quad \text{s.t.}\sum_{\textit{i}=1}^{\textit{N}} \omega_{\textit{i}} = 1, \quad \omega_{\textit{i}} \in \mathbb{R^{+}},
\end{equation}
where $\omega_{i}$ is the prior fixed weight (say 0.5) of the $i$-th classifier. We adopted this approach as a baseline for ablation in our experiments (denoted as \textit{Fixed}). 

A more refined method is dynamically allocating weights at the pixelwise level based on the performance of different classifiers. Therefore, we propose:
\begin{equation}
\hat{y}(\textbf{x}) = \text{Softmax}\left(\sum_{\textit{i}=1}^{\textit{N}} \bar{\boldsymbol{\omega}}_{i} \odot \hat{y}_{\textit{i}}(\textbf{x})\right),
\end{equation}
\begin{equation*}
\text{s.t.}\left\{
\begin{aligned}
& \boldsymbol{\mu}_{i} = Mean_{D}(\hat{y}_{\textit{i}}(\textbf{x})), \quad \boldsymbol{\mu}_{i} \in \mathbb{R}^{+}_{4\times H \times W}\\
& \boldsymbol{\sigma}_{i} = Var_{channel}(\boldsymbol{\mu}_{i}), \quad \boldsymbol{\sigma}_{i} \in  \mathbb{R}^{+}_{H \times W}\\
& \bar{\boldsymbol{\omega}}_{i} = \frac{\exp{(\boldsymbol{\sigma}_{i}})}{\sum_{\textit{i}=1}^{\textit{N}}\exp{(\boldsymbol{\sigma}_{i}})}, \quad \sum_{\textit{i}=1}^{\textit{N}}\bar{\boldsymbol{\omega}}_{i} = J_{H \times W}\\
\end{aligned}
\right.
\end{equation*}
where the mean \(\boldsymbol{\mu}_{i}\) is computed along the depth dimension, representing the likelihood of each channel at each pixel. The uncertainty of the \(i\)-th classifier is the variance \(\boldsymbol{\sigma}_{i}\) of \(\boldsymbol{\mu}_{i}\) along the \(\textit{channel}\) dimension, where higher variance indicates higher consistency to a channel across multiple slices, thus lower uncertainty (i.e., higher variance between channels indicates the \(i\)-th classifier's high confidence in a specific class, allowed to dominate the ensemble). \( J \) is an all-ones \( H \times W \) matrix, indicating a finer-grained convex combination. $\bar{\boldsymbol{\omega}}_{i}$, positively correlated with $\boldsymbol{\sigma}_{i}$, is obtained through exponentiation and regularization, then broadcasted to $D \times 4 \times H \times W$ to perform the Hadamard product $\odot$ with $\hat{y}_{i}(\textbf{x})$.

Thus, the forward process uses uncertainty by having the \(i\)-th classifier independently predict each 3D frame, storing \(\boldsymbol{\sigma}_{i}\) as memory. During pooling, it dynamically assigns pixelwise weights $\bar{\boldsymbol{\omega}}_{i}$ based on \(\boldsymbol{\sigma}_{i}\), which leverages spatial continuity to make slices could refer to each other and stabilize performance on end slices. To encourage diverse feature learning, we apply Dropout (\( p = 0.5 \)) at the bottleneck between the encoder and decoder of each classifier.

\subsubsection{Loss Function}
Since RV, MYO, and LV in CMR images exhibit a highly imbalanced distribution, with BG occupying most of the image, our loss function is designed as follows to better handle hard-to-classify samples: 
\begin{equation}
    \left\{
\begin{aligned}
& \mathcal{L}_{\text{Dice}}(\hat{y}, y) = \sum_{j=1}^{4}\lambda_{j}(1-\text{DSC}(\hat{y_{j}},y_{j})) \\
& \mathcal{L}_{\text{Focal}}(\hat{y}, y) = -\sum_{j=1}^{4}\alpha_{j}(1-\hat{y_{j}})^{\gamma}y_{j}\log{\hat{y_{j}}} \\
& \mathcal{L}_{\text{Total}} = \mathcal{L}_{\text{Dice} } + 10\mathcal{L}_{\text{Focal}}
\end{aligned}
\right.
\label{eq:loss function}
\end{equation}
where \(\hat{y}_{j}\) and \(y_{j}\) are the predicted probabilities and ground truth 0-1 mask for each class \(j\). \(\mathcal{L}_{\text{Dice}}\) is the weighted Dice loss, with DSC from Eq.~\ref{eq:DSC} and weights \(\lambda_{j} = 2\) for RV, \(1\) for BG, MYO, and LV. \(\mathcal{L}_{\text{Focal}}\) is the Focal Loss \cite{lin2017focal}, with \(\alpha_{j} = 0.1\) and \(\gamma = 2\). The total loss \(\mathcal{L}_{\text{Total}}\) is a weighted sum of Dice and Focal losses with weights \(1\) and \(10\), respectively.


\subsubsection{End-to-end Training}
\label{sec:end-to-end training}

% To optimize all parameters \(\Theta\) simultaneously in an end-to-end manner, the derivative of \(\mathcal{L}_{\text{Total}}\) is expressed as:
% \begin{equation}
% \frac{\partial{\mathcal{L}_{\text{Total}}}}{\partial{\Theta}} = \sum_{i=1}^{N}  \frac{\partial{\mathcal{L}_{\text{Total}}}}{\partial{\hat{y}}} \cdot \frac{\partial(\text{\small{Softmax}}(\sum_{i=1}^{N}\bar{\boldsymbol{\omega}}_{i} \odot \hat{y}_{i}))}{\partial{\Theta_{i}}}.
% \end{equation}
% where $\bar{\boldsymbol{\omega}}_{i}$ participates in gradient backpropagation, with each component being fully differentiable for efficient gradient propagation. 

% For the parameters \(\Theta_{i}\) of the \(i\)-th classifier, the gradient for training is:
% \begin{equation}
% \frac{\partial{\mathcal{L}_{\text{Total}}}}{\partial{\Theta_{i}}} = \frac{\partial{\mathcal{L}_{\text{Total}}}}{\partial{\hat{y}}} \cdot 
% \frac{\partial(\text{\small{Softmax}}(\sum_{i=1}^{N}\bar{\boldsymbol{\omega}}_{i} \odot \hat{y}_{i}))}{\partial{\hat{y}_{i}}} \cdot \frac{\partial \hat{y}_{i}}{\partial \Theta_{i}},
% \end{equation}

For the \(i\)-th classifier, the gradient for training is:
\begin{equation}
\frac{\partial{\mathcal{L}_{\text{Total}}}}{\partial{\Theta_{i}}} = \frac{\partial{\mathcal{L}_{\text{Total}}}}{\partial{\hat{y}}} \cdot 
\frac{\partial(\text{\small{Softmax}}(\sum_{i=1}^{N}\bar{\boldsymbol{\omega}}_{i} \odot \hat{y}_{i}))}{\partial{\hat{y}_{i}}} \cdot \frac{\partial \hat{y}_{i}}{\partial \Theta_{i}},
\end{equation}
where \(\Theta_{i}\) is the parameters of the \(i\)-th classifier, $\bar{\boldsymbol{\omega}}_{i}$ participates in gradient backpropagation, with each component being fully differentiable to enable efficient gradient flow, allowing \(\Theta_{i}\) to be optimized optimized in an end-to-end manner. 



