\section{Related Works}
\subsubsection{Ensemble Learning}
Müller et al. \cite{muller2022analysis} reviewed three ensemble techniques in medical imaging: \textit{Bagging}, \textit{Stacking}, and \textit{Augmenting}, which are contrasted with our framework Streaming in Fig.~\ref{fig:end-to-end}. \textit{Stacking} trains different networks (i.e. heterogeneous) on the same dataset and pools their outputs, while \textit{Bagging} trains the same type of network (i.e. homogeneous) on different trainsets and aggregates predictions on a shared testset. \textit{Augmenting}, in contrast, uses a single network to process augmented test data, combining outputs via pooling. These methods do not fully exploit deep learning’s end-to-end nature. Our Streaming unifies the dataset and loss function across all components, streamlining training (see Section.\ref{sec:end-to-end training}). Unlike traditional pooling methods that aggregate features such as Voting, our approach uses Weighted Average.


\begin{figure}
    \centering
    \includegraphics[width=1.0\linewidth]{fig/end-to-end.pdf}
    \caption{Visual differences between traditional ensemble learning and ours.}
    \label{fig:end-to-end}
\end{figure}


\subsubsection{Uncertainty for Segmentation}
Kendall and Gal \cite{kendall2017uncertainties,li2017dropout, kendall2015bayesian} proposed the use of Bayesian networks to simultaneously manage aleatoric uncertainties caused by data and epistemic uncertainties caused by the network per se. However, these studies focus primarily on integrating uncertainty into the loss function of an existing single segmentation network to optimize its training process. They do not consider leveraging uncertainty to allocate weights among multiple diverse networks to capitalize on their respective strengths within ensemble learning.