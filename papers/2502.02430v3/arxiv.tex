%%
%% This is file `sample-sigconf.tex',
%% generated with the docstrip utility.
%%
%% The original source files were:
%%
%% samples.dtx  (with options: `all,proceedings,bibtex,sigconf')
%% 
%% IMPORTANT NOTICE:
%% 
%% For the copyright see the source file.
%% 
%% Any modified versions of this file must be renamed
%% with new filenames distinct from sample-sigconf.tex.
%% 
%% For distribution of the original source see the terms
%% for copying and modification in the file samples.dtx.
%% 
%% This generated file may be distributed as long as the
%% original source files, as listed above, are part of the
%% same distribution. (The sources need not necessarily be
%% in the same archive or directory.)
%%
%%
%% Commands for TeXCount
%TC:macro \cite [option:text,text]
%TC:macro \citep [option:text,text]
%TC:macro \citet [option:text,text]
%TC:envir table 0 1
%TC:envir table* 0 1
%TC:envir tabular [ignore] word
%TC:envir displaymath 0 word
%TC:envir math 0 word
%TC:envir comment 0 0
%%
%%
%% The first command in your LaTeX source must be the \documentclass
%% command.
%%
%% For submission and review of your manuscript please change the
%% command to \documentclass[manuscript, screen, review]{acmart}.
%%
%% When submitting camera ready or to TAPS, please change the command
%% to \documentclass[sigconf]{acmart} or whichever template is required
%% for your publication.
%%
%%
% \documentclass[sigconf]{acmart}
\documentclass[authorversion,nonacm]{acmart}
%%
%% \BibTeX command to typeset BibTeX logo in the docs
\AtBeginDocument{%
  \providecommand\BibTeX{{%
    Bib\TeX}}}

%% Rights management information.  This information is sent to you
%% when you complete the rights form.  These commands have SAMPLE
%% values in them; it is your responsibility as an author to replace
%% the commands and values with those provided to you when you
%% complete the rights form.
\setcopyright{acmlicensed}
\copyrightyear{2025}
\acmYear{2025}
\acmDOI{XXXXXXX.XXXXXXX}

%% These commands are for a PROCEEDINGS abstract or paper.
\acmConference[WWW '25] {Proceedings of the ACM Web Conference 2025}{April 28--May 2, 2025}{Sydney, NSW, Australia.}
%%
%%  Uncomment \acmBooktitle if the title of the proceedings is different
%%  from ``Proceedings of ...''!
%%
%%\acmBooktitle{Woodstock '18: ACM Symposium on Neural Gaze Detection,
%%  June 03--05, 2018, Woodstock, NY}
\acmISBN{978-1-4503-XXXX-X/18/06}


%%
%% Submission ID.
%% Use this when submitting an article to a sponsored event. You'll
%% receive a unique submission ID from the organizers
%% of the event, and this ID should be used as the parameter to this command.
%%\acmSubmissionID{123-A56-BU3}

%%
%% For managing citations, it is recommended to use bibliography
%% files in BibTeX format.
%%
%% You can then either use BibTeX with the ACM-Reference-Format style,
%% or BibLaTeX with the acmnumeric or acmauthoryear sytles, that include
%% support for advanced citation of software artefact from the
%% biblatex-software package, also separately available on CTAN.
%%
%% Look at the sample-*-biblatex.tex files for templates showcasing
%% the biblatex styles.
%%

%%
%% The majority of ACM publications use numbered citations and
%% references.  The command \citestyle{authoryear} switches to the
%% "author year" style.
%%
%% If you are preparing content for an event
%% sponsored by ACM SIGGRAPH, you must use the "author year" style of
%% citations and references.
%% Uncommenting
%% the next command will enable that style.
%%\citestyle{acmauthoryear}


%%
%% end of the preamble, start of the body of the document source.
\include{commands}


\begin{document}
%%
%% The "title" command has an optional parameter,
%% allowing the author to define a "short title" to be used in page headers.
\title{A Scalable Crawling Algorithm Utilizing Noisy Change-Indicating Signals}

%%
%% The "author" command and its associated commands are used to define
%% the authors and their affiliations.
%% Of note is the shared affiliation of the first two authors, and the
%% "authornote" and "authornotemark" commands
%% used to denote shared contribution to the research.
\author{R\'obert Busa-Fekete}
\email{busarobi@google.com}
\affiliation{%
  \institution{Google Research}
  \city{New York}
  \country{USA}
}

\author{Julian Zimmert}
\email{zimmert@google.com}
\affiliation{%
  \institution{Google Research}
  \city{Berlin}
  \country{Germany}
}

\author{Andr\'as Gy\"orgy}
\email{agyorgy@google.com}
\affiliation{%
  \institution{Google DeepMind}
  \city{London}
  \country{UK}
}

\author{Linhai Qiu}
\email{linhai@google.com}
\affiliation{%
  \institution{Google}
  \city{Sunnyvale}
  \country{USA}
}

\author{Tzu-Wei Sung}
\email{twsung@google.com}
\affiliation{%
  \institution{Google}
  \city{Sunnyvale}
  \country{USA}
}

\author{Hao Shen}
\email{shenhao@google.com}
\affiliation{%
  \institution{Google}
  \city{Sunnyvale}
  \country{USA}
}

\author{Hyomin Choi}
\email{hyominchoi@google.com}
\affiliation{%
  \institution{Google}
  \city{Sunnyvale}
  \country{USA}
}

\author{Sharmila Subramaniam}
\email{sharmila@google.com}
\affiliation{%
  \institution{Google}
  \city{Sunnyvale}
  \country{USA}
}

\author{Li Xiao}
\email{lixiao@google.com}
\affiliation{%
  \institution{Google}
  \city{Sunnyvale}
  \country{USA}
}

%%
%% By default, the full list of authors will be used in the page
%% headers. Often, this list is too long, and will overlap
%% other information printed in the page headers. This command allows
%% the author to define a more concise list
%% of authors' names for this purpose.
\renewcommand{\shortauthors}{Busa-Fekete et al.}


%%
%% The abstract is a short summary of the work to be presented in the
%% article.
\begin{abstract}
  Web refresh crawling is the problem of keeping a cache of web pages fresh, that is, having the most recent copy available when a page is requested, given a limited bandwidth available to the crawler. Under the assumption that the change and request events, resp., to each web page follow independent Poisson processes, the optimal scheduling policy was derived by \citet{Azar8099}. In this paper, we study an extension of this problem where side information indicating content changes, such as various types of web pings, for example, signals from sitemaps, content delivery networks, etc., is available. Incorporating such side information into the crawling policy is challenging, because (i) the signals can be noisy with false positive events and with missing change events; and (ii) the crawler should achieve a fair performance over web pages regardless of the quality of the side information, which might differ from web page to web page. We propose a scalable crawling algorithm which (i) uses the noisy side information in an optimal way under mild assumptions; (ii) can be deployed without heavy centralized computation; (iii) is able to crawl web pages at a constant total rate without spikes in the total bandwidth usage over any time interval, and automatically adapt to the new optimal solution when the total bandwidth changes without centralized computation. Experiments clearly demonstrate the versatility of our approach.
\end{abstract}

%%
%% The code below is generated by the tool at http://dl.acm.org/ccs.cfm.
%% Please copy and paste the code instead of the example below.
%%
\begin{CCSXML}
<ccs2012>
 <concept>
  <concept_id>00000000.0000000.0000000</concept_id>
  <concept_desc>Do Not Use This Code, Generate the Correct Terms for Your Paper</concept_desc>
  <concept_significance>500</concept_significance>
 </concept>
 <concept>
  <concept_id>00000000.00000000.00000000</concept_id>
  <concept_desc>Do Not Use This Code, Generate the Correct Terms for Your Paper</concept_desc>
  <concept_significance>300</concept_significance>
 </concept>
 <concept>
  <concept_id>00000000.00000000.00000000</concept_id>
  <concept_desc>Do Not Use This Code, Generate the Correct Terms for Your Paper</concept_desc>
  <concept_significance>100</concept_significance>
 </concept>
 <concept>
  <concept_id>00000000.00000000.00000000</concept_id>
  <concept_desc>Do Not Use This Code, Generate the Correct Terms for Your Paper</concept_desc>
  <concept_significance>100</concept_significance>
 </concept>
</ccs2012>
\end{CCSXML}

\ccsdesc[500]{World Wide Web}
\ccsdesc[300]{Web searching and information discovery}
\ccsdesc{Web search engines}
\ccsdesc[100]{Web crawling}

%%
%% Keywords. The author(s) should pick words that accurately describe
%% the work being presented. Separate the keywords with commas.
\keywords{Search \& Information retrieval, Web page indexing, Crawling policy}



%%
%% This command processes the author and affiliation and title
%% information and builds the first part of the formatted document.
\maketitle

\section{Introduction}

Efficient web refresh crawling is one of the most fundamental data management problems in web search. Web pages are the simplest and most ubiquitous source of information on the Internet, therefore, extracting and organizing their information content is a crucial task. The first step in this process is acquiring the information, which means that web pages are regularly crawled to ensure that the information at the search engine is up-to-date. However, the scale of the problem is enormous since, as of today, there are trillions of web pages available online, and hence efficient, resource-aware crawling algorithms are of great practical interest.

In this paper, we consider the problem of designing scheduling policies for refreshing web pages in a local cache with the objective of maximizing the probability that incoming requests to the cache are processed and served with the latest version of the page. An efficient web page caching policy, thus, aims to schedule refresh crawl events (i.e., crawling a web page) between subsequent content change and content request events in order to have a fresh copy upon request. The issue of course is that the scheduler needs to check if a web page has changed, hence web pages need to be polled regularly, which uses bandwidth, and polling them only provides partial information about their change process, such as a single bit indicating whether the web page has changed since it was last refreshed \cite{cho2003estimating}. 

It has been estimated that up to 53\% of the crawling traffic of major search engines is redundant \citep{cloudfare_blogpost} in the sense that the crawler visits an unchanged version of a web page.
There is a growing awareness of the environmental and monetary cost of this redundancy, which motivated initiatives to provide additional signals for crawlers, allowing them in theory to improve their crawling policies.
One form of this effort is for web servers to actively send so-called \emph{web pings}, which notify interested parties about content changes, so that some active crawling traffic for detecting changes can be saved. There are different types of web pings, such as signals from sitemaps, content delivery networks, web servers, etc., which can all provide content change notifications to downstream services, such as web crawlers or proxy servers.
\citet{kolobov2019staying} recognized the importance of side information and proposed an algorithm under the assumption of complete and accurate change information for URLs (i.e., web pages;  throughout the paper we use the expressions URLs and web pages interchangeably) with side information.
We provide empirical evidence and conceptual arguments that this assumption is too strong and
extend the model to cover noisy signals as well.

In the classical crawling setup, the change and request sequences are assumed to obey certain stochastic processes~\cite{ChGa03}, which allows to derive policies with some optimality guarantee, even if the change and request sequences are not fully observable. The most standard assumption is that the sequence of changes and requests, resp., to each web page are independent Poisson processes \cite{wolf2002optimal,ChGa03,cho2003estimating}. For this setting, \citet{ChGa03} derived a convex optimization problem to find the optimal refresh rates for the web pages if the arrival rates of the Poisson processes are known.
\citet{Azar8099} presented an explicit efficient algorithm to find the solution, which, in fact, crawls every web page at a fixed, page-specific rate.  However, this results in a non-uniform total crawl rate, which is problematic in practice (as the resources available to the crawler have to match the peak rate). 
Therefore, they also proposed a method to transfer a variable-rate continuous-time (\emph{continuous} for short) crawling policy with page-specific fixed crawl rates to a policy with constant total crawl rate; such a policy schedules crawl events at regular intervals, that is, at discrete time steps (hence, such policies will be referred to as \emph{discrete} policies).
This reduction relies crucially on crawling all pages at a fixed rate. In the presence of side information, the optimal continuous policy does not satisfy fixed rates and prior work has not provided reductions to discrete policies for this setup \citep{kolobov2019staying}.

An alternative method to derive discrete policies from continuous ones based on Lagrange multipliers has been proposed in a Google blog post \citep{google_blogpost}.
A benefit of their reduction is that one can modify problem parameters such as the change rate of a URL or its importance  on-the-fly without requiring a global recomputation of the solution. This makes this method especially appealing for real-world scenarios where such parameters are continuously estimated \citep{cho2003estimating,UpadhyayBKPS20}.\footnote{In this work we do not consider the estimation of the standard parameters of the Poisson model (i.e., the change rate), but discuss the estimation of the parameters describing the behavior of the CI signals in Appendix~\ref{sec:estimation}.}

Another line of research leverages ideas from online learning and reinforcement learning to learn a crawling policy beyond the Poisson model \citep{kolobov2019staying,KolobovBZ20}; however, reinforcement learning methods are computationally infeasible in large-scale environments.


In this paper we build on the Poisson model and the continuous to discrete reduction via Lagrange-multipliers and make the following contributions:
\begin{enumerate}
    \item Motivated by the empirical evaluation of information available to real-world crawlers, we extend the crawling problem with imperfect (i.e., \emph{noisy}) \emph{change-indicating signals} (\emph{CI signals} or \emph{CISs}, for short).
    We derive the optimal (continuous) crawling strategy for this model under a global bandwidth constraint.
    \item Based on the above policy, we also derive a discrete crawling strategy that is able to use CISs. This strategy is practically appealing for several reasons: (i) up to a final $\argmax$ operation, it is fully decentralized and parallelizable over web pages both in computation and memory; (ii) the total crawl rate is constant over time without spikes in bandwidth usage over any time interval; (iii) the method automatically adapts to changes of the bandwidth constraint without centralized computation; and (iv) regular changes to the estimations of model parameters do not induce any additional computational overhead.
    \item We provide an extensive empirical study showing a clear benefit of our method on semi-synthetic data.
\end{enumerate}

The paper is organized as follows: In Section~\ref{sec:cis empirical} we provide empirical evaluation of CIS justifying the need of our extended model. In Section~\ref{sec:setup}, we introduce the crawling setup, its objective function and the formal definition of CI signals. In Section~\ref{sec:continuous}, we recall the optimization view of the crawling problem which allows to compute optimal continuous policies, and introduce a principled way to incorporate change-indicating signals into this setting. We introduce a general approach to derive easy-to-implement and scalable greedy polices based on continuous ones in Section~\ref{sec:policy_discrete}. Experimental studies are presented in Section \ref{sec:experiments}, followed by our conclusions and future plans in Section~\ref{sec:conclusion}. 
All the proofs of our theoretical results are relegated
to Appendix~\ref{sec:proofs}, and additional experiments are presented in Appendices~\ref{app:emp_rates_comp}, \ref{sec:burnin} and~\ref{sec:delayed_policy}.

\section{Change-Indicating Signals}\label{sec:cis empirical}
In this section, we present some basic statistics about change detection signals in general and in the dataset we use in our experiments.

We take the dataset from \cite{kolobov2019staying}, published in 2019, as a starting point. It contains 18,532,314 URLs which were crawled intensively over a period of 2 weeks to compute their empirical change rates. Additionally, they recorded for which sites they obtained side-information and the importance of the pages according to the Bing production crawler based on PageRank and popularity.

While only 4\% of the URLs has side information, they represent 26.4\% of importance-adjusted weight, which is why the question of including side information is relevant even when the adoption is not wide-spread yet. In their work, \citet{kolobov2019staying} assume that these CISs are noiseless, that is, for every change there is a CIS generated, and every CIS corresponds to a valid change. 

Concerning the quality of CISs, it is worth noting that defining a change itself is a complex question, since what counts as a change depends on the actual use case: For example, should correcting a typo on a web page count as a change, or changing the header, or some on-the-fly generated content, such as an ad? Because of this, there is no unified definition of what counts as change; \citet{kolobov2019staying} use the non-public change definition of their (Bing) production crawler, we also use our own definition, and CIS providers use their own, as well. As a result, CISs are necessarily noisy from the viewpoint of their users (even if they are perfect according to the change definition of their providers), and hence the model of complete and error free signals, used in \cite{kolobov2019staying}, is too simplistic to capture the real world.

In fact, we measured the quality of the sitemap signals for the web pages in their dataset with declared (perfect) sitemaps and found that the \emph{precision} of these signals (i.e., the proportion of time there is a change following a signal) is below 0.2, and their \emph{recall} (i.e., the proportion of time when a change is accompanied with a CIS) is below 0.5, hence they are far from perfect.
To give more insights into the quality of CISs, we measured the distribution of precision and recall of all the web pages we have sitemap signals for, and the resulting importance-weighted histograms (using our own confidential definition of importance, which is also based on PageRank and should be similar to that of Bing) are shown in  Figure~\ref{fig:prec_rec}.  These results clearly demonstrate that sitemap signals are noisy, but they are in general better than what we found for the URLs in the dataset of \cite{kolobov2019staying}. Importantly, there are very few pages which have perfect signal with precision and recall that are higher than 0.8.
\iffalse
\begin{tabular}{l|c|c|c}
     & \cite{Azar8099}&\cite{kolobov2019staying}&Our  \\\hline\hline
     Balanced& Yes & No& Yes\\\hline
     Cost-function & Binary & \makecell{Binary / \\ Harmonic} &Binary \\\hline
     CIS & None & Perfect & Noisy \\\hline
      Local Model update&   No&  No& Yes
\end{tabular}
\fi
\begin{figure}
  \centering
  \includegraphics[width=0.9\linewidth]{figs/prec_rec_w.png}
  \caption{ \label{fig:prec_rec}
  Histogram of precision and recall for URLs with sitemap signals. The histogram is computed by weighting the pages with their importance.}
\end{figure}

\section{Setup}
\label{sec:setup}
\paragraph{Classical crawling model.}
We are given $m$ web pages to be cached that are indexed by unique URLs. For the sake of presentation, we index them by $[m]=\{ 1, \dots, m \}$ and we consider the set of URLs to be fixed. The classical caching problem for a web page $i$ is modelled by three processes: 1) the request process; 2) the change process; and 3) the refresh (or crawling) process. The request and change processes are assumed to be independent Poisson processes with parameters $\mu_i$ and $\Delta_i$, respectively. We also assume that these Poisson processes are independent across all the web pages. In a practical system, the request processes can be fully observed, while the change process of a web page can only be partially observed by comparing its last cached version to the current one when a crawl event is triggered. The goal of caching is to have a fresh copy of the web page when a request event happens. So a good crawling policy tries to refresh each web page between every subsequent change and request events. Without further constraints, the naive solution is to continuously refresh each web page all the time, which is obviously not an implementable policy in practice (due to limitations in computations and communications). To obtain a more realistic setting, we assume a \emph{bandwidth constraint}, which either limits the overall frequency of crawl events in expectation, or sets a hard constraint on the minimal time interval between two subsequent crawl events.

\paragraph{Policies.}
A crawling policy decides at any time $t\in[0,\infty)$ (potentially based on the history up to time $t$) whether it crawls web pages, thereby over time generating an infinite sequence of crawl events: crawl events are specified by time-web page pairs where $(t, i)$ defines a crawl event for web page $i$ at time $t$, and the sequence of crawl events is denoted as
$\mathcal{T}=((\crawl_1,i_1),(\crawl_2,i_2),\dots)$,  where $\crawl_s\leq \crawl_{s+1}$ for any $s\in\mathbb{N}$.
We denote the crawl-event sequence of an individual web page $i$ by $\mathcal{T}_i =(\crawl_{i,1},\crawl_{i,2},\dots),$ where $t\in\mathcal{T}_i$ if and only if $(t,i)\in\mathcal{T}$.
As mentioned before, we consider two policy classes under bandwidth-budget constraints:

\begin{itemize}
    \item {\bf Continuous:} the average number of crawl events over time is asymptotically upper bounded by a budget $R$ almost surely (a.s.):
    $\limsup_{T\rightarrow \infty}\frac{1}{T}\max\{i\in\mathbb{N}\,|\,t_i\leq T\}
     \le R$ almost surely.
    \item {\bf Discrete:} the crawl events are uniformly triggered at fixed times: $\crawl_j = \frac{j}{R}$.
\end{itemize}

The policies in the continuous class specify the time of each crawl event, while the policies in the discrete class choose a web page to crawl at each time step $\crawl_j$ by treating the time when a crawl event can happen as a constraint. Furthermore, an optimal solution of the continuous class only needs to satisfy the total bandwidth constraint asymptotically, while an optimal solution of the discrete class satisfies the total bandwidth constraint over any time interval. The infrastructure and bandwidth constraints as well as the volatile web environment make the continuous class of policies less practical. We have assumed a uniform, fixed rate for the discrete class of policies. However, our method can be easily applied to other discrete classes with non-uniform rates (e.g. when the total bandwidth changes). Optimizing over the discrete class is a hard combinatorial problem and it is significantly easier to find the optimal solution in the continuous class.
Prior work followed the recipe of first computing the optimal solution in the continuous case and then approximating the continuous crawl rate in the discrete policy class \citep{Azar8099}; in this paper we also take this approach.


\paragraph{CI signals}
We extend the classical model described above by incorporating CI signals as additional observations. We assume that for each web page $i$, for every change a CI signal\footnote{It is straightforward to extend the model to multiple independent sources of CI signals. We consider a single signal for the sake of presentation.} is available with probability $\lambda_i$ for some $\lambda_i\in[0,1]$, (independently for each change). Given this assumption, the (Poisson) change process for web page $i$ can be split into two independent Poisson processes, a signalled change
process with rate $\lambda_i\Delta_i$ and an unsignalled -- and hence also directly unobserved -- change process with rate $(1-\lambda_i)\Delta_i$.
Additionally, we also allow the presence of false CI signals, and assume that these signals are generated by independent Poisson processes with rate $\nu_i$ for each web page $i$.
The decision maker cannot distinguish whether a CI signal was produced by the signalled change process or the false signal process.
In summary, for each web page $i$, one observes an additional sequence of CI signals $\mathcal{C}_i=(\ping_{i,1},\ping_{i,2},\dots)$ generated by a Poisson process with rate $\gamma_i=\lambda_i\Delta_i+\nu_i$, which contains both true and false CI signals.
We denote the rate of the unobserved change process by $\alpha_i = (1-\lambda_i)\Delta_i$.\footnote{ 
\citet{kolobov2019staying} study the regime where $\nu_i=0$ (noiseless) and $\lambda_i\in\{0,1\}$ (complete or no information at all).}
The quality of CISs for page $i$ is often measured by their precision and recall: \emph{precision}, the probability that a CI signal corresponds to a real change, is given by $\lambda_i \Delta_i/\gamma_i$, while \emph{recall}, the the probability that a change is indicated by a signal, is equal to $\lambda_i$ by definition.

The sequence of change and request events for a page are illustrated in Figure~\ref{fig:changesequences}.
\begin{figure}[h]
  \centering
  \includegraphics[width=0.6\linewidth]{figs/ping.png}
  \caption{Model of the change, request and CI signal processes.}
  \label{fig:changesequences}
\end{figure}

\paragraph{Objective.}
The objective of the crawl scheduler is to maximize the expected number of requests that are served with a fresh copy of the corresponding web page, defined, for a policy $\pi$, as\footnote{While we define limiting quantities in terms of $\liminf$ and $\limsup$ for generality, they can be replaced with $\lim$ for the optimal policies we derive, as the corresponding limits exist.}
\[
O(\pi) := \liminf_{N\rightarrow \infty}\frac{1}{N}\sum_{n=1}^N\E_\pi\left[\mathbb{I}\left\{\text{\tiny{the $n$-th request hits a fresh copy}}\right\}\right]\,.
\]
In the notation we explicitly included the dependence on $\pi$ to the expectation; note, however, that the expectation is also taken over the randomness induced by the environment, in particular by the different Poisson processes. This dependence is also omitted later.

To simplify this objective, we recall two important properties of Poisson processes:
\begin{itemize}
    \item {\bf Uniformity}:  Events that obey a Poisson process will be uniformly distributed over any time interval.
    \item {\bf Merging}: A collection of independent Poisson processes with rates $(\mu_1, \dots, \mu_m)$ is equivalent to a single Poisson process with rate $\mu= \sum_i \mu_i$ where each time an event is generated, it is assigned a random index from $[m]$ drawn from a discrete distribution with parameters $(\mu_1/\mu, \dots, \mu_m/\mu)$.
\end{itemize}
We assume the request events obey Poisson processes, therefore due to uniformity and merging, we can rewrite the objective as
\[
O(\pi) =\sum_{i=1}^m\liminf_{T\rightarrow\infty}\frac{\tilde{\mu}_i}{T}\int_{0}^T\mathbb{P}_\pi\left[E_i(t)\,|\,\mathcal{H}_t\right]\,dt\,,
\]
where $E_i(t)$ denotes the event that page $i$ is fresh at time $t$, $\tilde\mu_i=\mu_i/(\sum_{j=1}^m\mu_j)$ is the normalized importance and $\mathcal{H}_t=(\mathcal{T}_i\cap[0,t),\mathcal{C}_i\cap[0,t) )_{1,\dots,m}$ is the conditioning on all observable events before time $t$. 
Let 
\[\tau_i^{\textsc{elap}}(t) = t-\max\{\crawl_{i,k}\,|\,k\in\mathbb{N},\,\crawl_{i,k}\leq t\}\]
be the elapsed time since the last crawl for web page $i$ at time $t$ and
\[n^{\textsc{cis}}_i(t)=\left|\mathcal{C}_i\cap(t-\tau_i^{\textsc{elap}},t]\right|\]
be the number of CI signals received since the last crawl event of web page $i$.
Then the conditional probability of page $i$ being fresh is
\begin{equation}
\label{eq:freshnessprob-cond}
\mathbb{P}\left[E_i(t) \,|\,\mathcal{H}_t\right]=\exp(-\alpha_i\tau^{\textsc{elap}}_i(t))\cdot\left(\frac{\nu_i}{\gamma_i}\right)^{n^{\textsc{cis}}_{i}(t)}\,.
\end{equation}
The first factor is the probability that a Poisson process with rate $\alpha_i$ did not produce any event in an interval of length $\tau_{i}^{\textsc{elap}}(t)$. The second factor follows from the fact that all CI signals are i.i.d. events and with every event, the probability that the CI signal was a false positive is $\frac{\nu_i}{\gamma_i}$.
We can reduce the state (probability of freshness) to a single number by observing that the freshness reduction of each CI signal is equivalent to that of not observing any signal for a certain time. Motivated by this fact, let 
$$
\beta_i = -\frac{\log(\nu_i/\gamma_i)}{\alpha_i}\quad\text{and}\quad\tau^{\textsc{eff}}_{i}(t)=\tau^{\textsc{elap}}_{i}(t)+\beta_in^{\textsc{cis}}_{i}(t)
$$
be the time-equivalent of a single CI signal and the effective elapsed time, respectively. Then \eqref{eq:freshnessprob-cond} equals to $\exp(-\alpha_i\tau_{i}^{\textsc{eff}}(t))$, and
the objective becomes
\begin{align}
O(\pi)=\sum_{i=1}^m\liminf_{T\rightarrow\infty}\tilde\mu_i\E_{\pi, t\sim\text{unif}([0,T])}\left[\exp(-\alpha_i\tau_{i}^{\textsc{eff}}(t))\,|\,\mathcal{H}_t\right]\,. \label{eq:opt_objective}
\end{align}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Continuous policies}
\label{sec:continuous}

In this section, we focus on how to find optimal continuous policies which optimize \eqref{eq:opt_objective} under some bandwidth constraint. Our derivation can be viewed as a generalization of the approach of \citet{Azar8099}, but here we make use of noisy CISs. All proofs are deferred to Appendix~\ref{sec:proofs}. First, we narrow down the policy class of potentially optimal policies.
\begin{lemma}
\label{lem: threshold policy}
Any policy optimizing the objective $O(\pi)$ defined in (\ref{eq:opt_objective}) has a decision rule such that it triggers a crawl event $(\tau,i)$ when
%a.s. if and only if 
$\tau^{\textsc{eff}}_{i,\tau} \geq \iota_i$
for some fixed threshold vector $\biota=(\iota_1,\ldots,\iota_m)\in(0,\infty]^m$.
\end{lemma}
From now on, let $\pi(\biota)$ denote the thresholded policy which crawls web page $i$ when $\tau^{\textsc{eff}}_{i,\tau} \geq \iota_i$. The class of such thresholded policies contains at least one optimal policy by Lemma \ref{lem: threshold policy}, so we restrict our attention to this class. Also, because of Lemma \ref{lem: threshold policy}, any thresholded policy is \emph{separable} over web pages, which means that the threshold $\iota_i$ determines the performance of the policy $\pi(\biota) $ on web page $i$ regardless what thresholds are picked for the rest of the web pages. Nonetheless, solving the optimization problem \eqref{eq:opt_objective} under bandwidth constraint is still not straightforward, since the length of the crawl intervals are random quantities and so are the crawling frequencies. Motivated by this observation, we now focus only on a single page: the crawl frequency of policy $\pi(\biota)$ for web page $i$ and parameters $\mathcal{E}_i=(\alpha_i,\beta_i,\gamma_i,\tilde{\mu}_i)$ can be computed as
$$
f(\iota_i, \mathcal{E}_i) := \limsup_{T\rightarrow \infty}\frac{1}{T}\E_{\pi(\biota)}\bigl[\left|\mathcal{T}_i \cap[0,T]\right|\bigr]\,.
$$
Note that the crawl frequency for web page $i$ depends only on $\iota_i, \mathcal{E}_i$, and no other parameters.
Also note that the number of crawl events for an interval is a random quantity even for a thresholded policy $\pi(\biota)$, since the sequence $\tau^{\textsc{eff}}_{i,\tau}$ is random due to the presence of CISs. 


The following theorem is our main result and characterizes the solution to the optimization problem \eqref{eq:opt_objective}.
\begin{theorem}
\label{thm:main}
There exists $\Lambda\in\mathbb{R}$ such that the optimal threshold $\iota^\star$ for a policy maximizing \eqref{eq:opt_objective} satisfies
\begin{align*}
    &\forall i\in[m]:\, V(\iota^\star_i;\mathcal{E}_i) = \Lambda
    \text{ or } (V(\iota^\star_i;\calE_i) < \Lambda\text{ and }\iota^\star_i=\infty)\\
    &
    \text{and  }\sum_{i=1}^mf(\iota^\star_i,\mathcal{E}_i)=R\,,\\
     &\text{where  }V(\iota;\mathcal{E})=\tilde{\mu}\Bigg(\sum_{i=0}^{\lfloor\frac{\iota}{\beta}\rfloor}\frac{\nu^i}{(\Delta+\nu)^{i+1}}\calR^i_{\exp}((\alpha+\gamma) (\iota-i\beta))-\sum_{i=0}^{\lfloor\frac{t}{\beta}\rfloor}\frac{\exp(-\alpha \iota)}{\gamma}\calR^i_{\exp}(\gamma( \iota-i\beta))\Bigg)\\
    &\qquad\qquad\text{and  }f(\iota,\mathcal{E})=\Bigg(\sum_{i=0}^{\lfloor\frac{t}{\beta}\rfloor}\frac{1}{\gamma}\calR^i_{\exp}(\gamma( \iota-i\beta))\Bigg)^{-1}
\end{align*}
and where $\calR^i_{\exp}$ denotes the normalized residual of the $i$-th Taylor approximation of the exponential function
$\calR^i_{\exp}(x) = \frac{\exp(x)-\sum_{j=0}^i\frac{x^j}{j!}}{\exp(x)}\,.$
\end{theorem}

While this does not allow for a simple analytical solution, we can approximate the solution via the following Lemma.
\begin{lemma}
\label{lem: monotonuous}
The functions $V$ and $f$ are monotonous in their first argument for any $\calE$.
\end{lemma} 
Lemma \ref{lem: monotonuous} suggest that, given oracle access to $V$ and $f$, for any $\Lambda'$ we can find $\iota'_i$ such that $V(\iota'_i;\calE_i)\approx \Lambda'$ via line-search and compute its frequency $f(\iota'_i;\calE_i)$. We run an outer line search over $\Lambda'$ to find a value of $\Lambda'$ such that $\sum_{i=1}^m f(\iota'_i;\calE_i)\approx R$.

\section{A scalable discrete policy}
\label{sec:policy_discrete}
Optimal continuous policies are in general undesirable in practice, since the actual bandwidth constraint forbids spikes of crawl events over any time interval, not only asymptotically. This cannot be controlled in the optimal solution of the continuous policy class.
This is why, from an infrastructure point of view, it is desirable to execute a policy from the discrete class.

\citet{Azar8099} proposed a transition from their continuous policy to a discrete one via low-discrepancy sampling, a procedure that crucially relies on the fact that the crawl events for each web page under the optimal continuous policy are evenly spaced. Unfortunately, this approach is not practical in a real world crawler for the following two reasons. On the one hand, solving the joint optimization problem, for example the one defined in \eqref{eq:cd_optimization} in Appendix~\ref{sec:proofs}, is computationally demanding when trillions of pages are in the system. In addition to this, change and request rates are constantly being updated and new pages are added to the pool which requires to again solve the optimization problem to get an update for the crawling policy. On the other hand,
in the presence of CI signals, the crawl intervals of the optimal continuous policy are random and depend on the number of CI signals received in any interval. It is unclear how to generalize low-discrepancy sampling to this scenario.


Next we present a more general discretization strategy based on a method presented in \emph{The Unofficial Google Data Science Blog} \citep{google_blogpost} for the case without the change-indicating signals. Below we extend it to include CISs, as well.


\paragraph{Discrete policy via bandwidth control.}
We propose the following general procedure to derive a discrete policy. The continuous solution induces for any bandwidth $R$ a policy $\pi^R$ that asymptotically satisfies the bandwidth constraint. Instead of running the algorithm with a fixed $R$, we are controlling the bandwidth over time in such a way that the crawl satisfies a discrete policy. At time $t=\frac{j-1}{R}$, pick $R_j$ such that the first crawl of policy $\pi^{R_j}(\mathcal{H}_t)$ happens exactly at time $\frac{j}{R}$.
While this sounds like a computationally demanding reduction, this is in fact easier to solve than the continuous problem.
We obtain this policy by crawling any $i_t\in \argmax_{i\in\{1,\dots,m\}} V(\tau^{\textsc{eff}}_{i}(t);\calE_i)$ at any time step
(we call $V(\tau^{\textsc{eff}}_{i}(t);\calE_i)$ the crawl value for each page). This immediately follows from Theorem~\ref{thm:main} by setting the Lagrange multiplier $\Lambda$ to the maximum crawl value.

The resulting crawling strategy is given in Algorithm~\ref{alg: crawl}: 

\begin{algorithm}
    \caption{Effective crawling with CI signals}\label{alg: crawl}
    \begin{algorithmic}
\REQUIRE  $\forall i:\calE_i=(\alpha_i,\beta_i,\gamma_i,\tilde\mu_i)$, $R$ 
\FOR{ $t = \frac{1}{R},\frac{2}{R},\dots$}
    \STATE Pick Web page $i_t$ to crawl where 
    \STATE $\qquad i_t \gets \argmax_{i\in\{1,\dots,m\}}V(\tau^{\textsc{eff}}_{i}(t);\calE_i)$
    \STATE (Instances of $V$ are defined in App.~\ref{app: value functions}.)
    \STATE Crawl page $i_t$ at time $t$.
\ENDFOR
\end{algorithmic}
\end{algorithm}

Our empirical evaluations in Section~\ref{sec:experiments} show that the algorithm yields a small, albeit significant, improvement in performance compared to the algorithm of \cite{Azar8099}.




\subsection{Special cases of the crawl value function}
\label{sec: value functions}




We provide explicit formulas of the crawl value function for different settings.\newline
{\bf No change-indicating signals:} If there are no change-indicating signals available, then this reduces to the vanilla crawl problem studied by \citet{ChGa03}. This implies $\eff{t}=\tau^{\textsc{elap}}(t)$, $\alpha=\Delta$ and the value function reduces to
$
V_{\textsc{Greedy}}(\iota;\calE) = \frac{\tilde\mu}{\Delta}\calR^1_{\exp}(\Delta \iota)
$.

\noindent{\bf Change-indicating signals, no false positives:} Without false positive signals, whenever a signal is provided for a page it implies that the content of a page as outdated. 
    This corresponds to $\beta=\infty$ and $\eff{t}=\tau^{\textsc{elap}}(t)$ if no CI signal has been received and $\eff{t}=\infty$ otherwise.
The value function becomes in the limit of these parameters
\[
V_{\textsc{Greedy\_CIS}}(\iota;\calE) = \begin{cases}
\frac{\tilde\mu}{\Delta}\text{ if }\iota=\infty \text{ or otherwise}\\
\tilde\mu\left(\frac{\calR^0_{\exp}((\alpha+\gamma)\iota)}{\alpha+\gamma}-\frac{\calR^0_{\exp}(\gamma \iota)}{\gamma \exp(\alpha \iota)}\right)\,.\\
\end{cases}
\]
Notice that $\gamma\rightarrow 0$ recovers the value function of without change-indicating signals. \newline
{\bf Noisy change-indicating signals:} In the general case, we obtain the general value function discussed in section~\ref{sec:continuous}.
    \begin{align*}
        V_{\textsc{Greedy\_NCIS}}(\iota,\calE) = \tilde\mu\sum_{i=0}^{\lfloor\frac{\iota}{\beta}\rfloor}&\Big( \frac{\nu^i}{(\Delta+\nu)^{i+1}}\calR^i_{\exp}((\alpha+\gamma) (\iota-i\beta))\\
        &\qquad-\frac{\exp(-\alpha \iota)}{\gamma}\calR^i_{\exp}(\gamma( \iota-i\beta))\Big)\,.
    \end{align*}

\noindent{\bf Approximations of \Algo{Greedy\_NCIS}:} Since the value function in the general case is computationally demanding when $\lfloor\frac{t}{\beta}\rfloor$ is large, we also consider an approximation where we set any higher order residuals to $0$ in the computation of the crawl value. We denote the $j$-level approximation of the general value function, summing the first $j$ terms only, by $V_{\textsc{G\_NCIS-Approx-j}}$ (the exact formula is presented in Appendix~\ref{app: value functions}).


\subsection{Scalability} 

We note that Algorithm~\ref{alg: crawl} is fully decentralized except for the $\argmax$ operation. This means that as long as the crawl value can be efficiently computed (as discussed in the previous subsection), the method can be scaled very efficiently.

First, it is trivial to incorporate change of parameters and addition or removal of URLs in a fully decentralized manner.
Furthermore, 
to find the page with the largest value, that is, 
$$\argmax_{i \in \{1, \dots, m\}} V(\tau^{\textsc{eff}}_{i}(t);\calE_i)$$
to be crawled at each time step, only the comparison between the pages with the top crawl values matters, and hence the crawl values of most pages do not need to be computed at every time step. 
We can estimate the crawl value threshold where a page is likely to be selected to be crawled by keeping track of the crawl values of the selected pages over time, and estimate the next time when the crawl value of a page needs to be recomputed.




Other distributed computation technique can also be applied to scale up the computation in practice. For example, we can shard the web pages into $N$ shards and assign $1/N$ bandwidth to each shard, and schedule the web pages in each shard independently in parallel.



\section{Experiments}
\label{sec:experiments}

The goal of our experiments is to support the following claims: (1) Our discrete policy indeed constantly achieves a performance that is close to the optimal continuous one. (2) The CI signals can be utilized by the proposed methods even if the CI signals are partially observable, come with false positives and are delayed. All results which we report in this study  are averaged over 100 repetitions which also allows us to compute standard error for all reported quantities.

\subsection{Problem instances}
\label{sec:problem_inst}
A crawling problem when there is no CI signals is fully determined by the change and request rates; we generate these from a uniform distributions (with parameters specified later).
The CI signals in our model has three parameters for each page:\\
\newline
    {\bf Partially observability parameter ($\lambda_i$):} some part of the change process is not observable in which case the policy does not get CI signals about the change event. The fraction of the change events of page $i$ which is observable by the policy is denoted by $\lambda_i$ and this parameter 
    is generated from a Beta distribution whose parameters are denoted by $\lambda_a$ and $\lambda_b$.\newline
    {\bf False positive rate ($v_i$):} the CI signals might contain false positive events in which case not all CIS do correspond to some change event. In our model we assume that the false positive events are also generated according to a Poisson process. The rate with which the false positive events are generated is denoted by $v_i$ for page $i$ and this parameter is generated from a uniform distribution over $[v_{\min}, v_{\max}]$.

\subsection{Policies}

For each problem instance and bandwidth $R$, the optimal policy can be computed by solving \eqref{eq:cd_optimization} which corresponds to the optimal continuous policy, and Algorithm~\ref{alg: crawl} provides our proposed approximation in the space of discrete policies. We consider the performance of the optimal continuous policy as the baseline, and shall refer to this method as \Algo{Baseline}. The performance of a policy is its accuracy: fraction of events when there is a fresh copy upon request. Note that the accuracy of a continuous policy with rates $(\xi_1, \dots, \xi_m)$ can be computed as $\tfrac{1}{\sum_{i=1}^m \mu_i }\sum_{i=1}^m G(\xi_i; \mu_i, \Delta_i ).$

Our comparison study includes a few policies based on Algorithm~\ref{alg: crawl} using the special cases of value functions presented in Section~\ref{sec: value functions}.
\Algo{Greedy} uses the value function without knowledge of change-indicating signals.  \Algo{Greedy-CIS} operates under the (possibly false) assumption that change-indicating signals are noiseless. \Algo{Greedy-NCIS} using the exact value function in the general case and  \Algo{G-NCIS-APPROX-1} and \Algo{G-NCIS-APPROX-2} are the one or two step approximations of that function. 

\subsection{Accuracy of a policy}

Each algorithm was run with a bandwidth $R=100$ and for $t\le 1000$, thus each policy schedule $100,000$ crawl events in a single run. We are varying the number of pages $m$ in our experiments to control the hardness of the problem instance at hand. Note that the change parameters $\Delta_i$ as well as the request parameters are drawn from uniform distribution over $[0,1]$ thus $\sum_{i=1}^m \Delta_i$ and $\sum_{i=1}^m \mu_i$ are close $m/2$ in expectation. Therefore the expected change and request events given that $t\le 1000$ are around $(m\cdot 1000)/2$ in a single run with $m$ web pages. We compute the accuracy of a crawling policy over these $(m\cdot 1000)/2$ events, i.e. fraction of request events when fresh copy is available. Note that the larger the number of pages, the lower accuracy is that is achievable by any policy. We will always report the accuracy of the \Algo{Baseline} method which corresponds to the optimal continuous policy and can be computed by solving \eqref{eq:cd_optimization}.

\subsection{Continuous vs. discrete policies}
\label{sec:exp_disc}

Discrete policies have several advantages comparing to continuous ones in a production system. \citep{ChGa03} already came up with several simple approaches for converting a continuous policy into a discrete one. In a recent paper, Azar et. al.~\cite{Azar8099} made use of the technique of low discrepancy sequences~\cite{LiLa73} which is a scheduling technique for discrete systems so as the empirical rates of each scheduled event has to be close to a predefined rate. Their approach consists of solving the continuous problem, that is given in \eqref{eq:cd_optimization}, to obtain optimal rates for the continuous case, and then applying a low discrepancy sequence algorithm to turn this continuous policy into a discrete one. We implement Algorithm 3 of~\cite{Azar8099} in experiments for comparison, and refer to this approach as \Algo{LDS}.

The \Algo{GREEDY} approach conceptually also converts a continuous policy into a discrete one like \Algo{LSD} algorithm; however, there is no need to solve the continuous problem, but it computes the value of function $V_{\textsc{Greedy}}$ for each page which only depends on the elapsed times since the last crawl, and on the change and request rates. Therefore, \Algo{GREEDY} does not require to solve a large constrained optimization before its deployment. 

In the first set of experiments, we will compare the performance of \Algo{GREEDY} and \Algo{LDS}. The results are shown in Figure~\ref{fig:exp_1}.
Based on the results, we found that both policies have very similar performance regardless of the number of pages and, in addition to this, the performance of both algorithms is on par with the performance of the baseline which is the optimal policy in the continuous class. We also compared the empirical crawling frequency of \Algo{GREEDY} and \Algo{LDS} to the frequency of the optimal policy, and we found that they are close to each other. This analysis is deferred to Appendix~\ref{app:emp_rates_comp}.

\begin{figure}
  \centering
  \includegraphics[width=1.0\linewidth]{figs/exp1.png}
  \caption{Accuracy of discrete policies without using change-indicating signal. The \Algo{LDS} corresponds to the Algorithm 3 in \cite{Azar8099} where the input rates are coming from the solution of \eqref{eq:cd_optimization} with the true change and request rates. \label{fig:exp_1}}
\end{figure}

\subsection{Partially observable change sequences}

In the second set of experiments, we assess the utility of CI signals when they are partially observable. We generate problem instance as in Section~\ref{sec:exp_disc}, but this time we compare the performance of \Algo{GREEDY-CIS} to the performance of \Algo{GREEDY}, since \Algo{GREEDY-CIS} is supposed to be amenable to utilize CI signal in an efficient way when there is no false positive CI signal. 

The parameter $\lambda_i$, which controls the fraction of observable change events according to our model, is generated from $\text{Beta} ( 0.25, 0.25)$ which has a bi-modal density function. Thus, the change events are not revealed too much via CI signals for some pages, and they are revealed almost every time for some other pages. The results are presented in Figure \ref{fig:exp_2_beta}. The results clearly show that CI signals can significantly improve the performance of the crawling policy regardless to the relation of bandwidth $(R=100)$ and sum of change parameters $\sum_{i=1}^{m} \Delta_i$.

\begin{figure}
  \centering
  \includegraphics[width=1.0\linewidth]{figs/exp_2_beta.png}
  \caption{Accuracy of \Algo{GREEDY} and \Algo{GREEDY-CIS} with change-indicating signal. \label{fig:exp_2_beta}}
\end{figure}

\label{sec:emp_rates}
\begin{figure}[h]
\centering
\includegraphics[width=0.39\linewidth]{figs/exp_2_rates.png}
\qquad\qquad
\includegraphics[width=0.39\linewidth]{figs/exp2_rates_change.png}
% \begin{minipage}{.46\textwidth}
%   \centering
%   \includegraphics[width=0.7\linewidth]{figs/exp_2_rates.png}
  \caption{Empirical crawl rates for various web pages achieved by discrete policies \Algo{GREEDY} and \Algo{GREEDY-CIS}. Each dot corresponds to a web page for which we computed the rate of the \Algo{Baseline} method and plotted it versus the empirical rate achieved by the corresponding policy. The color of the dots on the indicates (i) the observability of the change sequence that is controlled by $\lambda$ on the left panel, while (ii) it indicates the change rates of the web pages on the right panel.  \label{fig:exp_2_beta_rates}}
\end{figure}


Next, we investigate where this performance boost of \Algo{GREEDY-CIS} comes from. To do so, we plotted the empirical crawl rates achieved by \Algo{GREEDY} and \Algo{GREED-CIS} which are shown in Figure~\ref{fig:exp_2_beta_rates} (left). Each dot corresponds to a web page for which we compute the crawl rate of the \Algo{Baseline} method versus the empirical crawl rates of various methods. The color of the dots indicates the value of parameters $\lambda_i$
which represents the fraction of change events for which CI signals are available. The larger the parameter $\lambda$, the more CI signals are provided for a web page. Figure~\ref{fig:exp_2_beta_rates} (right) shows the very same crawl rates but the color of the dots indicate the change parameter $\Delta_i$ of the web page. Based on the results with 100 web pages that are shown in the top subplots in both panels of Figure~\ref{fig:exp_2_beta_rates}, one can see that \Algo{GREEDY-CIS} decreases the crawl rate for web pages with many CI signals, and allocates more bandwidth for web pages with no or few CI signal. Interestingly, the results for 300 web pages reveal a different behaviour (see the bottom subplots of Figure~\ref{fig:exp_2_beta_rates}). In this case, some of the web pages with many CISs get boosted and some of them get decreased. Typically, those web pages that have high change rates get a higher crawl rate compared to the rate of \Algo{Baseline} (see bottom right subplot of the right panel in Figure~\ref{fig:exp_2_beta_rates}). In general, the crawl rates of those web pages which have no or few CISs remained close to the optimal rate of the \Algo{Baseline} method which is the optimal behaviour when no CI signals are provided (see the blue dots around the diagonal of right subplots of the left panel of Figure~\ref{fig:exp_2_beta_rates}). 

\vspace{-0.1cm}
\subsection{Presence of false positive CIS}
\vspace{-0.1cm}
In this set of experiments, we compare the performance of various policies when false positive are also present among the change indicating signals, and the policy is aware of the rate of the false positive events. 
In the first experiment, we run all policies, including \Algo{GREEDY}, \Algo{GREEDY-CIS}, \Algo{GREEDY-NCIS},  \Algo{G-NCIS-APPROX-1} and \Algo{G-NCIS-APPROX-2}, with $m\in \{ 100, 200, 500, 750, 1000, 10000\}$ and with a constant bandwidth $R=100$. Thus, the larger the number of web pages, the less bandwidth can be allocated per web page. The observability parameters ($\lambda_i$) were generated from $\text{Beta} (0.25, 0.25)$, and the parameters that determines the rate of false positive CISs is generated from $\text{Unif} (0.1, 0.6)$. The results are presented in Figure~\ref{fig:exp_all}.
\begin{figure}[h!]
  \centering
  \includegraphics[width=0.9\linewidth]{figs/exp_all.png}
  \caption{Accuracy for policies with CIS that is partially observable in the presence of false positives signals.}
  \label{fig:exp_all}
\end{figure}

\begin{figure}[t]
  \centering
  \includegraphics[width=0.6\linewidth]{figs/exp_all_rates.png}
  \caption{Empirical crawl rates in the presence of false positive CISs for various web pages achieved by discrete policies \Algo{GREEDY}, \Algo{GREEDY-CIS} and \Algo{GREEDY-NCIS}. Each dot corresponds to a web page for which we computed the crawl rate of the \Algo{Baseline} method and plotted it against the empirical crawl rate achieved by the corresponding policy.}
  \label{fig:exp_all_rates}
\end{figure}
The results reveal several trends which we summarize as follows. The performance of \Algo{GREEDY-NCIS}, \Algo{G-NCIS-APPROX-1} and \Algo{G-NCIS-APPROX-2} is superior to \Algo{GREEDY} and \Algo{GREEDY-CIS} in almost every case, so they can utilize CIS with false positives. When the band width is not tight, i.e.  $m \in \{ 100, 200, 500\}$ the performance of the algorithms with approximated value function, i.e. \Algo{G-NCIS-APPROX-1} and \Algo{G-NCIS-APPROX-2}) is very close to the performance of the one with exact crawl value, i.e. \Algo{GREEDY-NCIS}. However, for larger number of web pages, the exact computation shows superiority and policies with approximated value function cannot utilize the CI signal. The performance of \Algo{GREEDY-CIS}, which assumes no false positive CI signals, is deteriorated with the number of pages. This policy fully relies on CI signals meaning that it assumes that the content of a page gets outdated upon incoming CI signal. Therefore it allocates more crawl bandwidth to those web pages which have CI signals with many false positive CI signals. 
This is justified by the empirical rates that can be seen in Figure~\ref{fig:exp_all_rates}, presented in Appendix~\ref{sec:emp_rates}. The results show that \Algo{GREEDY-CIS} achieves significantly higher empirical rates for some pages with high observability rate (indicated by red arrows), whereas \Algo{GREEDY-NCIS} is able to handle this more noisy CI signal efficiently.
This means that in the presence of false positive signals, \Algo{Greedy-CIS}, which assumes no false positive CIS, overly relies on the side information, that is, the false positives unnecessarily boost its empirical crawl rate. 



\subsection{Real world change-rate and precision}
Finally, we conduct an experiment with empirical parameters (Figure~\ref{fig:real_data}). We take the dataset provided by \citet{kolobov2019staying} mentioned in Section~\ref{sec:cis empirical}.
We follow their protocol to create a semi-synthetic simulation environment: We subsample 100k URLs uniformly from the dataset, and use the provided importance and change-rate information for running simulations with the Poisson model. However, to reflect that CISs are not perfect, we generate CISs differently.

In their paper, \citet{kolobov2019staying} labelled a set of URLs havin CISs with perfect precision and recall (ca. 5\% of sampled URLs).
Since our measurements, presented in Section~\ref{sec:cis empirical}, did not validate the presence of such good signals, we apply the following procedure to produce CI signals for our experiments, which both respects the URL selection of \citep{kolobov2019staying} and the precision/recall distributions presented in Section~\ref{sec:cis empirical}: 
We take the fraction of URLs which the dataset labels as perfect precision and recall (ca. 5\% of sampled URLs).
We split the empirical precision and recall distributions of Section~\ref{sec:cis empirical} into a lower part consisting of the lowest 95\% values and the highest 5\% values. We sample precision and recall numbers for the labelled top URLs according to the upper tail of the distribution and for all other URLs from the lower end.
We set the crawl budget to 5000 per time step in accordance with prior work, evaluate the policies on 200 time steps and repeat the experiment 10 times. 

Since the policy of \citet{kolobov2019staying} is not trivially extendable to uniform crawling, we use \Algo{GREEDY-CIS+} for a fair comparison. This algorithm assigns the crawl value of \Algo{GREEDY} for all non-high-quality URLs (defined by Precision > 0.7 and Recall > 0.6 to match all top URLs without corruption) and the crawl value of \Algo{GREEDY-CIS} for high quality pages.

To simulate the impact of imperfect estimations, we corrupt all precision and recall numbers by mixing in uniformly sampled noise $\xi_i \sim \operatorname{unif}(0,1)$ with $precision = (1-p)precision+p\xi_i, recall=(1-p)recall + \xi_i$ for $p\in\{0,0.1,0.2\}$.

The accuracy of the crawling policies under the different settings are presented in Figure~\ref{fig:real_data}. We observe that splitting the pages into high and low quality and using simplified crawl values is close to optimal when the estimations are correct, but is less robust to corrupted precision and recall estimations.
\begin{figure}
% \vspace{-0.28cm}
  \centering
  \includegraphics[width=0.9\linewidth]{figs/real_data.png}
%   \vspace{-0.7cm}
  \caption{Accuracy of \Algo{GREEDY-NCIS},\Algo{GREEDY} and \Algo{GREEDY-CIS+} with corrupted change-indicating signals on 100k URLs. \label{fig:real_data}}
\end{figure}

\section{Conclusions and future work}
\label{sec:conclusion}

In this paper, we derived several discrete policies which can be implemented easily in a decentralized and scalable way which is demanded by the huge number of URLs that a web caching system has to cope with. In addition, these scalable policies are extended so as they are amenable to handle noisy CI signals with great efficiency. We empirically justified that the performance of the discrete policies is on par with the continuous one that requires lots of centralized computation, and, in addition, they are able to improve their performance with respect to the continuous optimal policy when CI signals are available. 

We tested our approach in a crawling system including couple billion URLs. The relative performance improvement and outcome of the experiments is reported in Appendix \ref{sec:realworld}. In addition to this, we  also tested \Algo{Greedy-NCIS} when the CI signals are delayed and the bandwidth constraint is changing. In this case, we apply a simple heuristic of discarding CI signals if they arrive shortly after a crawl event (see Appendix~\ref{sec:delayed_policy} for details). We found that this simple approach can handle delayed CISs if the delays follows an exponential distribution. In addition, we had found that our policies can adjust well to changing bandwidth which result is presented in Appendix \ref{sec:burnin}. We plan to devise a more principled way of handling delayed CISs in the future. 


\bigskip

%\bibliography{biblio}

\begin{thebibliography}{11}


\ifx \showCODEN    \undefined \def \showCODEN     #1{\unskip}     \fi
\ifx \showDOI      \undefined \def \showDOI       #1{#1}\fi
\ifx \showISBNx    \undefined \def \showISBNx     #1{\unskip}     \fi
\ifx \showISBNxiii \undefined \def \showISBNxiii  #1{\unskip}     \fi
\ifx \showISSN     \undefined \def \showISSN      #1{\unskip}     \fi
\ifx \showLCCN     \undefined \def \showLCCN      #1{\unskip}     \fi
\ifx \shownote     \undefined \def \shownote      #1{#1}          \fi
\ifx \showarticletitle \undefined \def \showarticletitle #1{#1}   \fi
\ifx \showURL      \undefined \def \showURL       {\relax}        \fi
% The following commands are used for tagged output and should be
% invisible to TeX
\providecommand\bibfield[2]{#2}
\providecommand\bibinfo[2]{#2}
\providecommand\natexlab[1]{#1}
\providecommand\showeprint[2][]{arXiv:#2}

\bibitem[Azar et~al\mbox{.}(2018)]%
        {Azar8099}
\bibfield{author}{\bibinfo{person}{Yossi Azar}, \bibinfo{person}{Eric Horvitz},
  \bibinfo{person}{Eyal Lubetzky}, \bibinfo{person}{Yuval Peres}, {and}
  \bibinfo{person}{Dafna Shahaf}.} \bibinfo{year}{2018}\natexlab{}.
\newblock \showarticletitle{Tractable near-optimal policies for crawling}.
\newblock \bibinfo{journal}{\emph{Proceedings of the National Academy of
  Sciences}} \bibinfo{volume}{115}, \bibinfo{number}{32}
  (\bibinfo{year}{2018}), \bibinfo{pages}{8099--8103}.
\newblock


\bibitem[Boyd and Vandenberghe(2004)]%
        {Boyd2004}
\bibfield{author}{\bibinfo{person}{Stephen Boyd} {and} \bibinfo{person}{Lieven
  Vandenberghe}.} \bibinfo{year}{2004}\natexlab{}.
\newblock \bibinfo{booktitle}{\emph{Convex Optimization}}.
\newblock \bibinfo{publisher}{Cambridge University Press},
  \bibinfo{address}{New York, NY, USA}.
\newblock


\bibitem[Cho and Garcia-Molina(2003a)]%
        {ChGa03}
\bibfield{author}{\bibinfo{person}{Junghoo Cho} {and} \bibinfo{person}{Hector
  Garcia-Molina}.} \bibinfo{year}{2003}\natexlab{a}.
\newblock \showarticletitle{Effective Page Refresh Policies for Web Crawlers}.
\newblock \bibinfo{journal}{\emph{ACM Trans. Database Syst.}}
  \bibinfo{volume}{28}, \bibinfo{number}{4} (\bibinfo{date}{Dec.}
  \bibinfo{year}{2003}), \bibinfo{pages}{390--426}.
\newblock


\bibitem[Cho and Garcia-Molina(2003b)]%
        {cho2003estimating}
\bibfield{author}{\bibinfo{person}{Junghoo Cho} {and} \bibinfo{person}{Hector
  Garcia-Molina}.} \bibinfo{year}{2003}\natexlab{b}.
\newblock \showarticletitle{Estimating frequency of change}.
\newblock \bibinfo{journal}{\emph{ACM Transactions on Internet Technology
  (TOIT)}} \bibinfo{volume}{3}, \bibinfo{number}{3} (\bibinfo{year}{2003}),
  \bibinfo{pages}{256--290}.
\newblock


\bibitem[Cloudfare(2021)]%
        {cloudfare_blogpost}
\bibfield{author}{\bibinfo{person}{Cloudfare}.}
  \bibinfo{year}{2021}\natexlab{}.
\newblock \bibinfo{title}{Crawler Hints: How Cloudflare Is Reducing The
  Environmental Impact Of Web Searches}.
\newblock
  \bibinfo{howpublished}{\url{https://blog.cloudflare.com/crawler-hints-how-cloudflare-is-reducing-the-environmental-impact-of-web-searches/}}.
\newblock


\bibitem[Kolobov et~al\mbox{.}(2020)]%
        {KolobovBZ20}
\bibfield{author}{\bibinfo{person}{Andrey Kolobov},
  \bibinfo{person}{S{\'{e}}bastien Bubeck}, {and} \bibinfo{person}{Julian
  Zimmert}.} \bibinfo{year}{2020}\natexlab{}.
\newblock \showarticletitle{Online Learning for Active Cache Synchronization}.
  In \bibinfo{booktitle}{\emph{Proceedings of the 37th International Conference
  on Machine Learning, {ICML} 2020, 13-18 July 2020, Virtual Event}}
  \emph{(\bibinfo{series}{Proceedings of Machine Learning Research},
  Vol.~\bibinfo{volume}{119})}. \bibinfo{publisher}{{PMLR}},
  \bibinfo{pages}{5371--5380}.
\newblock


\bibitem[Kolobov et~al\mbox{.}(2019)]%
        {kolobov2019staying}
\bibfield{author}{\bibinfo{person}{Andrey Kolobov}, \bibinfo{person}{Yuval
  Peres}, \bibinfo{person}{Cheng Lu}, {and} \bibinfo{person}{Eric~J Horvitz}.}
  \bibinfo{year}{2019}\natexlab{}.
\newblock \showarticletitle{Staying up to date with online content changes
  using reinforcement learning for scheduling}.
\newblock \bibinfo{journal}{\emph{Advances in Neural Information Processing
  Systems}}  \bibinfo{volume}{32} (\bibinfo{year}{2019}).
\newblock


\bibitem[Liu and Layland(1973)]%
        {LiLa73}
\bibfield{author}{\bibinfo{person}{C.~L. Liu} {and} \bibinfo{person}{James~W.
  Layland}.} \bibinfo{year}{1973}\natexlab{}.
\newblock \showarticletitle{Scheduling Algorithms for Multiprogramming in a
  Hard-Real-Time Environment}.
\newblock \bibinfo{journal}{\emph{J. ACM}} \bibinfo{volume}{20},
  \bibinfo{number}{1} (\bibinfo{date}{jan} \bibinfo{year}{1973}),
  \bibinfo{pages}{46–61}.
\newblock


\bibitem[Richoux(2018)]%
        {google_blogpost}
\bibfield{author}{\bibinfo{person}{Bill Richoux}.}
  \bibinfo{year}{2018}\natexlab{}.
\newblock \bibinfo{title}{The Unofficial Google Data Science Blog}.
\newblock
  \bibinfo{howpublished}{\url{https://www.unofficialgoogledatascience.com/2018/07/by-bill-richoux-critical-decisions-are.html}}.
\newblock


\bibitem[Upadhyay et~al\mbox{.}(2020)]%
        {UpadhyayBKPS20}
\bibfield{author}{\bibinfo{person}{Utkarsh Upadhyay},
  \bibinfo{person}{R{\'{o}}bert Busa{-}Fekete}, \bibinfo{person}{Wojciech
  Kotlowski}, \bibinfo{person}{D{\'{a}}vid P{\'{a}}l}, {and}
  \bibinfo{person}{Bal{\'{a}}zs Sz{\"{o}}r{\'{e}}nyi}.}
  \bibinfo{year}{2020}\natexlab{}.
\newblock \showarticletitle{Learning to Crawl}. In
  \bibinfo{booktitle}{\emph{The Thirty-Fourth {AAAI} Conference on Artificial
  Intelligence, {AAAI} 2020, The Thirty-Second Innovative Applications of
  Artificial Intelligence Conference, {IAAI} 2020, The Tenth {AAAI} Symposium
  on Educational Advances in Artificial Intelligence, {EAAI} 2020, New York,
  NY, USA, February 7-12, 2020}}. \bibinfo{publisher}{{AAAI} Press},
  \bibinfo{pages}{6046--6053}.
\newblock


\bibitem[Wolf et~al\mbox{.}(2002)]%
        {wolf2002optimal}
\bibfield{author}{\bibinfo{person}{Joel~L Wolf}, \bibinfo{person}{Mark~S
  Squillante}, \bibinfo{person}{PS Yu}, \bibinfo{person}{Jay Sethuraman}, {and}
  \bibinfo{person}{Leyla Ozsen}.} \bibinfo{year}{2002}\natexlab{}.
\newblock \showarticletitle{Optimal crawling strategies for web search
  engines}. In \bibinfo{booktitle}{\emph{Proceedings of the 11th international
  conference on World Wide Web}}. \bibinfo{pages}{136--147}.
\newblock


\end{thebibliography}

\bibliographystyle{ACM-Reference-Format}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% APPENDIX
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage 
\appendix
\onecolumn
\newpage 

\section{Proofs}
\label{sec:proofs}
We provide the proofs for our theoretical results in Section~\ref{sec:continuous}.

The following property of the residual function is useful in this section.
For ease of notation, from now on we drop the subscript $\exp$ for the function $\calR_{\exp}$. 
For any $i>0$, we have
\begin{align}
    \frac{\partial}{\partial x}\calR^i(x)= \frac{\partial}{\partial x}\left[1-\sum_{j=0}^i\frac{x^j}{j!\exp(x)}\right] =\calR^{i-1}(x)-\calR^{i}(x)\,.
\end{align}
\begin{proof}[Proof of Lemma~\ref{lem: threshold policy}]
Assume the opposite is true and with probability $p>0$, a crawl event is not triggered by a threshold rule.
This implies than we can find consecutive crawl intervals such that $\eff{\crawl_i}>\eff{\crawl_{i+1}}$, such that there is no CI signal directly at $\crawl_i$ and that these crawl intervals have positive volume on the real line. We show that one can increase the objective by moving the crawl event, which means that the current policy was not optimal.
Pick a small time $\epsilon$, such that no CI signal falls between $[\crawl_i-\epsilon,\crawl_{i}]$ and assume that the crawl event would have happened at time $\crawl_i-\epsilon$ instead. The original unnormalized objective over the two crawl intervals is
\[
\int_{\crawl_{i-1}}^{\crawl_i}\exp(-\alpha \eff{s})\,ds+
\int_{\crawl_{i}}^{\crawl_{i+1}}\exp(-\alpha \eff{s})\,ds\,.
\]
After changing the policy by shifting the value, we have an objective of
\begin{align*}
\int_{\crawl_{i-1}}^{\crawl_i-\epsilon}\exp(-\alpha \eff{s})\,ds+\int_{0}^{\epsilon}\exp(-\alpha s)\,ds
+
\int_{\crawl_{i}}^{\crawl_{i+1}}\exp(-\alpha (\eff{s}+\epsilon))\,ds\,.
\end{align*}
The difference in objective is
\begin{align*}
    (1-p_i\cdot \exp(\alpha \epsilon))&\int_{0}^{\epsilon}\exp(-\alpha s))\,ds
    -(1-\exp(-\alpha\epsilon))\, \times \\ &\int_{\crawl_{i}}^{\crawl_{i+1}}\exp(-\alpha \eff{s})\,ds
    \geq \frac{1-\exp(-\alpha\epsilon)}{\alpha}(p_{i+1}-p_i\cdot \exp(\alpha \epsilon))\,,
\end{align*}
where the last inequality follows from
\begin{align*}
    \int_{\crawl_{i}}^{\crawl_{i+1}}\exp(-\alpha \eff{s}) &\leq \int_{0}^{\eff{\crawl_{i+1}}}\exp(-\alpha s)\,ds 
    = \frac{1-p_{i+1}}{\alpha}\,.
\end{align*}
Since We assumed $p_{i} < p_{i+1}$, we can find an $\epsilon >0$ such that the objective increases with our policy change. Hence the original policy was not optimal.
\end{proof}

\begin{proof}[Proof of Lemma~\ref{lem: derivative ratio}]
We can consider $\crawl_{1}$ as a function of the threshold $\iota$ and consider any realization of the CI signal process at which changing the threshold changes the first crawl event. By the Leibniz integral rule for differentiation, we have
\begin{align*}
    &\frac{\partial}{\partial t}\int_{0}^{\crawl_{1}(\iota)}\exp(-\alpha\eff{s})\,ds 
    = \exp(-\alpha\eff{\crawl_{1}(\iota)})\frac{\partial}{\partial t}\crawl_{1}(\iota)\,.
\end{align*}
We assume changing the threshold changes the time of crawl, hence $\eff{\crawl_{1}(\iota)}=\iota.$
Taking the expectation over all these trajectories finishes the proof.
\end{proof}

\begin{proof}[Proof of Lemma~\ref{lem: monotonuous}]
Using Lemma~\ref{lem: derivative ratio} and
\[V(\iota) = \tilde \mu(w(\iota)-\exp(-\alpha \iota)\psi(\iota))\]
yields
\[V'(\iota) = \tilde\mu \alpha \exp(-\alpha \iota)\psi(\iota)>0 \,.\]
Hence the function $V$ is monotonically increasing.
The derivative of the function $\calR^i(x)$ is $$\calR^{i-1}(x)-\calR^{i}(x)=\frac{x^i}{i!}\exp(-x)> 0.$$
Hence the derivative of the function $\psi(\iota)$ is strictly positive and the function $f$ is monotonically decreasing.
\end{proof}

\begin{proof}[Proof of Theorem~\ref{thm:main}]
The contribution of page $i$ to the overall objective $O(\pi(\biota))$ (i.e., its weighted expected freshness) can be expressed as 
\[
o(\iota_i;\mathcal{E}_i)= \tilde \mu_i \, \E_{\pi(\biota),t\sim\text{unif}(\mathbb{R}_+)}\left[\exp(-\alpha_i\tau_i^{\textsc{eff}}(t))\right]\,,
\]
which again only depends on the parameters $\iota_i$ and $\mathcal{E}_i$.
Then 
\[
O(\pi) = \sum_{i=1}^m o(\iota_i;\mathcal{E}_i),
\]
and finding the optimal policy with bandwidth constraint $R$ that maximizes \eqref{eq:opt_objective} reduces to the optimization problem 
\begin{align*}
\text{maximize} \quad \sum_{i=1}^m o(\iota_i;\mathcal{E}_i)
\quad \text{subject to} \quad \sum_{i=1}^m f(\iota_i;\mathcal{E}_i) \leq R\,,
\end{align*}
or by substituting $\xi_i=f(\iota_i;\mathcal{E}_i)$, we have equivalently
\begin{align}\label{eq:general_optimization}
%F(\xi)=
\text{maximize} \quad \sum_{i=1}^m o(f^{-1}(\xi_i;\mathcal{E}_i);\mathcal{E}_i)
\quad \text{subject to} \quad  \sum_{i=1}^m \xi_i \leq R\,,
\end{align}
where the inverse of $f$ is with respect to its first argument. Since the crawl intervals $\crawl_{i,n+1}-\crawl_{i,n}$ are i.i.d. random variables for any $n\in\mathbb{N}$ due to the Poisson processes, we can rewrite the frequency as the inverse of the average length between two subsequent crawls.

The objective in \eqref{eq:general_optimization} is the sum of the expected freshness of the different web pages at a random point in time, weighted by the request intensity $\tilde \mu_i$, similarly to the seminal work of \cite{Azar8099}. In fact, it is easy to show that \eqref{eq:general_optimization} matches the objective in \cite{Azar8099} in the absence of CI signals, in which case $\alpha_i=\Delta_i$ for all $i \in [m]$, and the crawling interval is deterministic with length $\iota_i$. Consequently, picking a random point in time is equivalent to picking a random point in any interval between two crawls, since all intervals are of the same length. So in this case, for any threshold $\iota \ge 0$ and Poisson parameters $\mathcal{E}$, the objective function boils down to
\begin{align*}
    o(\iota; \mathcal{E}) &= \tilde{\mu}\cdot\frac{1}{\iota}\int_{0}^{\iota}\exp(-\Delta s)\,ds
    =\tilde{\mu}\cdot\frac{1}{\Delta \iota}(1-\exp(-\Delta \iota))\,,
\end{align*}
and
\begin{align*}
    o(f^{-1}(\xi;\mathcal{E});\mathcal{E}) = G(\xi;\tilde\mu,\Delta) := \frac{\tilde \mu}{\Delta} \xi \left(1- \exp \left( -\frac{\Delta}{\xi}\right)  \right)\,.
\end{align*}
% with 
% \[
% G( x; \mu, \Delta ) = \frac{\mu}{\Delta} x \left(1- \exp \left( -\frac{\Delta}{x}\right)  \right) \enspace .
% \]
In the absence of CISs, the optimal policy is given by the optimization problem
\begin{align}\label{eq:cd_optimization}
\text{maximize} \quad %F_{\text{CD}}(\xi) & =
\sum_{i=1}^m G(\xi_i; \mu_i, \Delta_i )
\quad \text{subject to} \quad \sum_{i=1}^m \xi_i \leq R \,.
\end{align}
% where
% \[
% G( x; \mu, \Delta ) = \frac{\mu}{\Delta} x \left(1- \exp \left( -\frac{\Delta}{x}\right)  \right) \enspace .
% \]
which is a special case of \eqref{eq:general_optimization}.
The solution $\xi^*=(xi^*_1,\ldots,\xi^*_m)$ of \eqref{eq:cd_optimization} results in a policy which crawls web page $i$ in optimal intervals of length exactly $1/\xi^*_i$.
(In fact, this optimization objective is already introduced in equation~6 of~\cite{Azar8099}, where it is referenced from \cite{ChGa03}.)




Before we derive the functions $o$ and $f$ for the web-ping setting, we first discuss how to actually solve the optimization problem \eqref{eq:general_optimization} in the general form.
By the Karush–Kuhn–Tucker conditions \cite{Boyd2004}, any local optimum $\xi^*$ satisfies for all $i\in[m]$
\begin{align*}
    &\left.\frac{\partial}{\partial x} o(f^{-1}(x;\mathcal{E}_i);\mathcal{E}_i)\right|_{x=\xi^\star_i} = \Lambda\\
    \text{ or } &\left.\frac{\partial}{\partial x} o(f^{-1}(x;\mathcal{E}_i);\mathcal{E}_i)\right|_{x=\xi^\star_i} < \Lambda\text{ and }\xi^*_i=0\,
\end{align*}
for some Lagrange multiplier $\Lambda \ge 0$ (obtained by solving \eqref{eq:general_optimization} with Lagrange's method). The case $\xi^*_i=0$ corresponds to never crawling a web page. This can be the optimal for maximizing the objective if there are web pages with low importance $\tilde\mu$ and high change rate $\Delta$.
In practice, completely abandoning web pages might be unacceptable and can be alleviated by enforcing a hard threshold $\xi_i > \varepsilon$ on the crawl frequency.

Under sufficient regularities, e.g., $o\circ f^{-1}$ being concave, this is also a sufficient condition for optimality.
Define the function
\[
V(\iota;\mathcal{E}) = \left.\frac{\partial}{\partial x} o(f^{-1}(x;\mathcal{E});\mathcal{E})\right|_{x=f(\iota;\mathcal{E})}\,,
\]
then any optimal threshold $\iota^\star$ satisfies
\begin{align*}
    &V(\iota^\star_i;\mathcal{E}_i) = \Lambda\qquad 
    \text{ or } &V(\iota^\star_i;\calE_i) < \Lambda\text{ and }\iota^\star_i=\infty 
    \,
\end{align*}
subject to the bandwidth constraint 
$\sum_{i=1}^mf(\iota^\star_i,\calE_i)=R$.

\paragraph{Computing $V$ and $f$ with CI signals.}
We define the following auxiliary functions for a policy parameterized by $\iota$ so as
we drop the web page subscript of all quantities, since we are considering a single web page:
\begin{align}
    &w(\iota;\calE) = \E_{\pi(\biota)}\left[\int_{0}^{\crawl}\exp(-\alpha\tau^{\textsc{eff}}(s))\,ds\right]\\
    &\psi(\iota;\calE) = \E_{\pi(\biota)}\left[\crawl\right]\,.
\end{align}
These quantities are the expected cumulative freshness between two consecutive crawl events and the expected length of that interval, respectively.
These two functions are closely related as the following lemma shows.
\begin{lemma}
\label{lem: derivative ratio}
The derivatives of $w,\psi$ with respect to the first argument satisfy for any environment $\calE$
%\begin{align*}
    $w'(x;\calE) = \exp(-\alpha x)\psi'(x;\calE)\,.$
%\end{align*}
\end{lemma}
These two functions allow us to express $V$ and $f$ nicely. As mentioned before, the frequency is simply the inverse expected length, so $f(\iota;\calE)=1/\psi(\iota;\calE)$.
The objective $o$ is given by
\begin{align*}
    o(\iota;\calE) = \tilde\mu\cdot w(\iota;\calE)\cdot f(\iota;\calE)\,,
\end{align*}
hence by the inverse function rule, we have
\begin{align*}
    V(\iota;\calE)
    &=
    \tilde\mu\left.\frac{\partial}{\partial x}\left[w(f^{-1}(x;\calE);\calE)\cdot x\right]\right|_{x=f(\iota;\calE)}\\
    &=\tilde{\mu}\left(w(\iota;\calE)+\frac{w'(\iota;\calE)}{f'(\iota;\calE)}\right)\\
    &=\tilde{\mu}\left(w(\iota;\calE)-\exp(-\alpha \iota)\psi(\iota;\calE)\right)\,,
\end{align*}
where the last equation uses Lemma~\ref{lem: derivative ratio} and $\frac{1}{f'(x)}=-\frac{\psi(x)}{\psi'(x)}$.
Finally, we present the analytical solutions for the functions $\psi$ and $w$.
\begin{lemma}
\label{lem: function values}
The expected length of an interval between two consecutive crawl events and its cumulative freshness under the threshold-policy $\pi(\biota)$ in environment $\calE$ are given by
\begin{align*}
    &\psi(\iota;\calE)=\sum_{i=0}^{\lfloor\frac{t}{\beta}\rfloor}\frac{1}{\gamma}\calR^i_{\exp}(\gamma( \iota-i\beta))\\
    &w(\iota;\calE)=\sum_{i=0}^{\lfloor\frac{\iota}{\beta}\rfloor}\frac{\nu^i}{(\Delta+\nu)^{i+1}}\calR^i_{\exp}((\alpha+\gamma) (\iota-i\beta))\,,
\end{align*}
where $\calR^i_{\exp}$ denotes the normalized residual of the $i$-th Taylor approximation of the exponential function
$\calR^i_{\exp}(x) = \frac{\exp(x)-\sum_{j=0}^i\frac{x^j}{j!}}{\exp(x)}\,.$
\end{lemma}
Note that the computational complexity of evaluating the functions $\psi,w$ grows in $t$. To avoid unbounded computation, one can approximate the function values very well by terminating the summation after a small finite number of terms (e.g. 3, see Figure~\ref{fig:V function}), since the residual of the $i$-th Taylor approximation converges quickly to 0. 

\begin{figure}[t]
  \centering
  \includegraphics[width=0.4\linewidth]{figs/V_with_approximations}
  \caption{Example of the crawl-value function $V$. Dashed lines are approximations when terminating the sum after 1, 2 or 3 terms, respectively.
  Red line is the asymptotic value of all functions for $\iota\rightarrow\infty$.}
  \label{fig:V function}
\end{figure}
\end{proof}

\begin{proof}[Proof of Lemma~\ref{lem: function values}]
We begin with the function $\psi$. Assume $\iota\in[0,\beta]$. Then the length of the first interval is given by $\min\{\ping_1,\iota\}$, since the first received CI signal puts the effective time above the threshold.
The expected length is given by
\begin{align*}
    \psi(\iota;\calE)&=\int_{0}^\iota\gamma \exp(-\gamma s)s\,ds+\exp(-\gamma \iota)\iota\\
    &=\left[-\exp(-\gamma s)s\right]^\iota_0-\int_{0}^\iota-\exp(-\gamma s)\,ds+\exp(-\gamma \iota)\iota\\
    &=\frac{1-\exp(-\gamma \iota)}{\gamma}\\
    &=\frac{\calR^0(\gamma \iota)}{\gamma}\,,
\end{align*}
which is the same function value as claimed.
Now by induction, the show that the function is correct for any $\iota$. Assume that the formula is correct for any $\iota'\in[0,i\beta]$, then we show that it is also correct for $\iota\in(i\beta, (i+1)\beta]$.
The expected length between crawls can be given recursively by the time that passes until $\ping_{1}\in[0,\iota-i\beta]$ plus the length for the remaining threshold, or $\iota-i\beta$ plus the length for a threshold $i\beta$ if no CI signal was received in $[0,\iota-i\beta]$ (omitting $\calE$ for brevity) 
\begin{align*}
    \psi(\iota) &= \E[\mathbb{I}\{\ping_{1}\in[0,\iota-i\beta]\}(\ping_{1}+\psi(\iota-\beta-\ping_{1}) +\mathbb{I}\{\ping_{1}>\beta\}(\iota-i\beta+\psi(i\beta)]\\
    &=\int_{0}^{\iota-i\beta}\gamma\exp(-\gamma s)(s+\psi(\iota-\beta-s))\,ds +\exp(-\gamma(\iota-i\beta))(\iota-i\beta+\psi(i\beta))\\
    &=\psi(\iota-i\beta)+\int_{0}^{\iota-i\beta}\sum_{j=0}^{i-1}\frac{\calR^j(\gamma(\iota-(j+1)\beta-s))}{\exp(\gamma s)}\,ds +\exp(-\gamma(\iota-i\beta))\psi(i\beta)\,.
\end{align*}
The middle term is
\begin{align*}
    \int_{0}^{\iota-i\beta}\sum_{j=0}^{i-1}& \frac{\calR^j(\gamma(\iota-(j+1)\beta-s))}{\exp(\gamma s)}\,ds
    \\
    &=\sum_{j=0}^{i-1}\left[-\frac{\calR^{j+1}(\gamma(\iota-(j+1)\beta-s))}{\gamma\exp(\gamma s)}\right]^{\iota-i\beta}_0\\
    &=\sum_{j=0}^{\lfloor\frac{\iota}{\beta}\rfloor}\frac{\calR^j((\iota-\beta j)\gamma)}{\gamma}-\frac{\calR^0(\gamma\tau)}{\gamma}+\frac{\calR^0(\gamma i\beta)}{\gamma\exp(\gamma(\iota-i\beta))} -\exp(-\gamma(\iota-i\beta))\psi(i\beta)\\
    &=\sum_{j=0}^{\lfloor\frac{\iota}{\beta}\rfloor}\frac{\calR^j((\iota-\beta j)\gamma)}{\gamma}-\psi(\iota-i\beta)-\exp(-\gamma(\iota-i\beta))\psi(i\beta)\,.
\end{align*}
Combining this with the previous equation finishes the derivation for $\psi(\iota)$.

Next, we show that the claimed function for $w$ is correct. Note that $w(0)=0$ which is the correct value.
By Lemma~\ref{lem: derivative ratio}, it is sufficient to show that the ratio of derivatives between $\psi(x)$ and $w(x)$ matches.
Recall
\begin{align*}
    \frac{\partial}{\partial x}\calR^i(x) = \calR^{i-1}(x)-\calR^{i}(x)=\frac{x^i}{i!}\exp(-x)\,.
\end{align*}
Hence
\begin{align*}
\left.\frac{\partial}{\partial x}\psi(x)\right|_{x=\iota} = \sum_{i=0}^{\lfloor\frac{t}{\beta}\rfloor} \frac{(\gamma(\iota-i\beta))^i}{i!\exp(\gamma(\iota-i\beta))}\,.
\end{align*}
The derivative of our claimed form of $w$ is
\begin{align*}
    \left.\frac{\partial}{\partial x}w(x)\right|_{x=\iota} &= \sum_{i=0}^{\lfloor\frac{t}{\beta}\rfloor} \frac{\gamma^{i}}{(\alpha+\gamma)^{i}}\frac{((\alpha+\gamma)(\iota-i\beta))^i}{i!\exp(\alpha \iota+\gamma(\iota-i\beta))}\\
    &= \sum_{i=0}^{\lfloor\frac{t}{\beta}\rfloor} \frac{(\gamma(\iota-i\beta))^i}{i!\exp(\gamma(\iota-i\beta))}\exp(-\alpha \iota)\,.
\end{align*}
Hence the ratio of the derivatives satisfy Lemma~\ref{lem: derivative ratio}, which implies that we have found the correct analytical form of $w(x)$.
\end{proof}

\subsection{Crawl value based on approximation}
\label{app: value functions}

{\bf Approximations of \Algo{Greedy\_NCIS}:} Since the value function in the general case is computationally demanding when $\lfloor\frac{t}{\beta}\rfloor$ is large, we also consider an approximation where we set any higher order residuals to $0$ in the computation of the crawl value. We denote the $j$-level approximation of the general value function by
    \begin{align*}
        &V_{\textsc{G\_NCIS-Approx-j}}(\iota,\calE)=
         \tilde\mu\sum_{i=0}^{\min\{j-1,\lfloor\frac{\iota}{\beta}\rfloor\}}\Big( \frac{\nu^i}{(\Delta+\nu)^{i+1}}\calR^i_{\exp}((\alpha+\gamma) (\iota-i\beta))-\frac{\exp(-\alpha \iota)}{\gamma}\calR^i_{\exp}(\gamma( \iota-i\beta))\Big)\,.
    \end{align*}

\smallskip

\section{Comparison of empirical rates of \Algo{GREEDY} and \Algo{LDS}}
\label{app:emp_rates_comp}

Even if the performances of the \Algo{GREEDY} and \Algo{LDS} algorithms are very similar, they implement quite different scheduling strategies. When we compare the empirical rates of \Algo{GREEDY} and \Algo{LDS} to the rates of the \Algo{Baseline} method that can be obtained by solving \eqref{eq:cd_optimization}, we can see that even if the performances of these two algorithms are on-par, the rates for the same pages are different from each other. Figure~\ref{fig:exp_1_rates} shows the empirical rates achieved by \Algo{GREEDY} and \Algo{LDS} with 100 and 500 web pages takes from 10 problem instances. The empirical rates of \Algo{LDS} are very close to the optimal continuous policy rates since dots are on the diagonal red line. This result can be explained by the fact that the \Algo{LDS} is based on a low discrepancy scheduling algorithm whose objective is to directly minimize the deviation of these two rates over time. However, the \Algo{GREEDY} takes into account the staleness of web pages when it picks the next web page to be crawled.


\begin{figure*}[ht!]
  \centering
  \includegraphics[width=0.5\linewidth]{figs/exp1_rates.png}
  \caption{Empirical rates for various web pages achieved by discrete policies without using change-indicating signals. Each dot corresponds to a web page for which the optimal rate versus the empirical rate of the corresponding method is plotted. The web pages are taken from 10 synthetic problem instances with 100 and 500 web pages, respectively. The optimal rates are computed based on \eqref{eq:cd_optimization}, and they correspond to the \Algo{Baseline} method. \Algo{LDS} corresponds to Algorithm~3 of \cite{Azar8099} with the rates of \Algo{Baseline} method as input.). \label{fig:exp_1_rates}}

\end{figure*}



\section{Delayed CI signals}
\label{sec:delayed_policy}
So far, we have assumed that CI signals arrive instantaneous with the corresponding change event.
In practice, there is some latency either due to the network or the entity generating the CI signals itself.
However, the bulk of web pages changes on a scale of several days, and for such pages any CI signal delay is minuscule compared to the average interval between change events.
We propose to simply discard CI signals that arrive within an interval $[\crawl,\crawl+T_{\textsc{delay}}]$ after a crawl for some tuneable parameter $T_{\textsc{delay}}$, to ensure that we do not make decisions based on out-dated CI signals.


Next experiment, we assess the impact of the delay of CI signals on the performance of \Algo{GREEDY-NCIS} policy and to what extent the proposed thresholded approach, defined in Section \ref{sec:delayed_policy} can remedy the performance drop that is caused by the delay of CI signals. The thresholding consists of discarding the CI signal for a page $i$ when it is close to a recent crawl event that fetches the content of page $i$. We set this time window to $T_{\text{DELAY}} = 5/R$.

The problem instances we use in this experiment are generated in a similar way as in the previous section: the partial observability parameter is generated as $\lambda_i \sim \text{Beta} (0.25, 0.25)$, and the rate of false positive CI signals is $v_i \sim \text{Unif}(0.1, 0.6)$. In addition to this, the CI signals are delayed with a random quantity that is generated from the Poisson distribution with $v=6$ as it is described in Section \ref{sec:problem_inst}.

Figure~\ref{fig:exp_delay} shows the result with the delayed CI signals. We indicate the baseline by the red line as before which is the performance of the optimal continuous policy without using CI signals. In addition to this we also indicate the performance of \Algo{GREEDY-NCIS} policy without delayed CI signals by the blue line. The performance that is indicated by the blue line coincides with the one reported in Figure \ref{fig:exp_all}. Based on the results, one can see that the delay of the CI signals indeed deteriorates the performance of \Algo{GREEDY-NCIS} when the bandwidth is not so tight, and has marginal impact when the crawling problem has tighter bandwidth allocation. This can be explained by the fact that when the bandwidth is tighter the improvement achieved by \Algo{GREEDY-NCIS} over \Algo{GREEDY} policy, which does not use CI signals, is smaller, which implies that the CI signals do not help so much in this case. Thus, their delay has not so significant impact in that case, either. Nevertheless, when we compare the performance of \Algo{GREEDY-NCIS-D} to the performance of \Algo{GREEDY-NCIS} with no delay (blue line), we see that when $m=100$, their performances are on-par. In this case, this simple thresholding policy can almost recover the performance drop caused by delay.

\begin{figure*}[h]
  \centering
  \includegraphics[width=0.5\linewidth]{figs/delay.png}
  \caption{Accuracy achieved by discrete policies including \Algo{GREEDY-NCIS} and \Algo{GREEDY-NCIS-D}. The partial observability parameter is generated as $\lambda_i \sim \text{Beta} (0.25, 0.25)$, and the rate of false positive CI signals as $v_i \sim \text{Unif}(0.1, 0.6)$. In addition, the CI signals are delayed with a random quantity that is generated from the Poisson distribution with $v=6$. The parameter selection is described in Section~\ref{sec:problem_inst}. }
  \label{fig:exp_delay}
\end{figure*}

\section{Burn-in time}
\label{sec:burnin}

In the next experiment, we demonstrate that the discrete policy \Algo{GREEDY} is able to automatically adjust the prioritization of web pages to new optimal solutions when the total bandwidth changes, without centralized computation. In this example, the total bandwidth starts from $R = 100$, and suddenly changes to $R = 150$, before changing back to $R = 100$, with $m=1000$ web pages to crawl, running for $t \leq 400$. The GREEDY policy is used to select web pages, and there is no extra computation when the total bandwidth changes. Figure \ref{fig:changing_freshness} shows that the accuracy automatically rises to the new optimal level when the bandwidth increases, and automatically falls back to the original optimal level when the bandwidth decreases back. The accuracy which is plotted in Figure Figure \ref{fig:changing_freshness} is computed on the last $1000$ crawl events for every time step.

\begin{figure*}[t]
  \centering
  \includegraphics[width=0.25\linewidth]{figs/freshness_with_changing_bandwidth.png}
  \caption{Accuracy of GREEDY over time. The red line shows how the accuracy of GREEDY changes over time, when the total bandwidth starts from 100, and increases to 150 at time 133, before decreasing back to 100 at time 266. The green line shows the accuracy of GREEDY, when the total bandwidth is always 100. The blue line shows the accuracy of GREEDY, when the total bandwidth is always 150.  \label{fig:changing_freshness}}
\end{figure*}

\iffalse
\section{Ignoring noise}
A naive solution is to restrict the use of CIS to pages above a certain threshold and then pretend they have perfect precision and recall.
We compute the objective value for such a policy and show for which parameter ranges this can be reasonable.

We consider the following class of policies: 
\begin{enumerate}
    \item Always crawl when a signal arrives, or when a time $\tau$ has passed without crawl.
    \item Crawl when a signal arrives with probability $p<1$.
\end{enumerate}
The objective values are computed via
\begin{align*}
    o(\mathcal{E}) = \frac{w(\mathcal{E})}{\psi(\mathcal{E})}\,.
\end{align*}
\textbf{Crawl every ping.}
The expected length between crawls is
\begin{align*}
    \psi(\tau;\mathcal{E}) &= \int_{0}^\tau s\gamma \exp(-\gamma s)\,ds +\exp(-\gamma \tau)\tau\\
    & = \frac{1}{\gamma}(1-\exp(-\gamma\tau))\,,
\end{align*}
while the expected cumulative freshness is given by
\begin{align*}
    w(\tau;\mathcal{E}) &= \int_{0}^\tau \frac{1}{\alpha}(1-\exp(-\alpha s))\gamma \exp(-\gamma s)\,ds +\exp(-\gamma \tau)\frac{1}{\alpha}(1-\exp(-\alpha \tau))\\
    & = \frac{\exp(-\gamma\tau)(\gamma(\exp(-\alpha\tau)-1)-\alpha)+\alpha}{\alpha(\alpha+\gamma)}+\exp(-\gamma \tau)\frac{1}{\alpha}(1-\exp(-\alpha \tau))\\
    &= \frac{1-\exp(-(\alpha+\gamma)\tau)}{\alpha+\gamma}\,,
\end{align*}
hence the objective is
\begin{align*}
    o(\tau;\mathcal{E}) = \mu\frac{\gamma(1-\exp(-(\alpha+\gamma)\tau))}{(\alpha+\gamma)(1-\exp(-\gamma\tau)}\,.
\end{align*}
\textbf{Crawl every ping at rate $p$.}
The expected length between crawls is
\begin{align*}
    \psi(p;\mathcal{E}) =\frac{1}{\gamma}(1+(1-p)+(1-p)^2+\dots)=\frac{1}{\gamma p}
\end{align*}
For the expected cumulative freshness, assume an interval between two pings where the expected freshness is $q$ at the beginning of the interval, then we have
\begin{align*}
    q\int_{0}^\infty \frac{1}{\alpha}(1-\exp(-\alpha s))\gamma\exp(-\gamma s)\,ds = \frac{ q}{\alpha+\gamma}\,.
\end{align*}
The expected freshness of the page at the end of the interval is
\begin{align*}
    q\cdot \underbrace{\frac{\alpha+\gamma-\delta}{\gamma}}_{\text{probability of change during ping}}\int_{0}^\infty \gamma\exp(-\gamma s)\exp(-\alpha s)\,ds=q\frac{\alpha+\gamma-\Delta}{\alpha+\gamma}\,.
\end{align*}
Since we assume a Poisson process and the events in intervals are independent, the expected cumulative freshness is given by
\begin{align*}
    w(p,\mathcal{E}) = \frac{1}{\alpha+\gamma}\left[1+(1-p)\frac{\alpha+\gamma-\Delta}{\alpha+\gamma}+\left((1-p)\frac{\alpha+\gamma-\Delta}{\alpha+\gamma}\right)^2+\dots\right]=\frac{1}{(\alpha+\gamma)(p+\frac{\Delta}{\alpha+\gamma}(1-p))}\,.
\end{align*}
The objective function is
\begin{align*}
    o(p;\mathcal{E}) =\mu \frac{\gamma p}{(\alpha+\gamma)(p+\frac{\Delta}{\alpha+\gamma}(1-p))}\,.
\end{align*}
\fi
\section{Estimating model parameters}
\label{sec:estimation}
We directly observe all request events and CI signals, hence we can estimate the importance parameters $\mu$ and the rate of the CIS $\gamma$ with good accuracy based on the overall frequency of logged events.
The unobserved change rate $\alpha$ and ``goodness'' of CI signals $\beta$ are harder to estimate, since only partial knowledge available of the content change events.

A naive approach is to use empirical crawl events which are observed to create an approximation of the change sequence and directly calculate the empirical precision and recall of the CI signals based on this approximation.
To evaluate this approach, 
we conduct the following experiment: We sample precision and recall values uniformly between $[0.2,0.95]$. The expected lenth of the change process is sampled uniformly between $[2,20]$ and the relative rate of crawls is set between four times to one quarter of the change rate.
We use the models to create synthetic datasets of time horizon $100000$ and reconstruct precision and recall based on either a statistical approach that computes
\begin{align*}
    &\text{precision}=\frac{\text{number of intervals with CI signal and change}}{\text{number of intervals with change}}\\
    &\text{recall}=\frac{\text{number of intervals with CI signal and change}}{\text{number of intervals with change}}\,,
\end{align*}
or fitting the linear model for $\alpha$ and $\beta$ to the data and compute precision and recall based on these.

Figure~\ref{fig:prec_rec_estimate} shows that the statistical estimator is clearly biased. 


\begin{figure}[h]
  \centering
  \includegraphics[width=0.5\linewidth]{figs/plug_in_prec_rec.png}
%  \vspace{-0.5cm}
  \caption{Bias of the naive estimator of precision and recall of CIS. \label{fig:prec_rec_estimate}}
\end{figure}

We achieve more faithful results by fitting a model to the empirical data.
We collect the data $\left((\binom{\tau^{\textsc{elap}}}{n^{\textsc{cis}}}_1, z_1),\dots\right)$, where $z_i$ is a binary variable indicating whether there has been a change between crawl $i-1$ and $i$.
The model predicts that $z_i\sim \text{Ber}(\exp(-\langle \binom{\alpha}{\alpha\beta},\binom{\tau^{\textsc{elap}}}{n^{\textsc{cis}}}_i\rangle)) .$
Estimating the unknown parameter vector $\binom{\alpha}{\alpha\beta}$ can be done via MLE. In our experiments, we assume that these parameters are known to the policies, but in a production system this estimation can be carried out based on logged data. The absolute error of this estimator was on the order of $10^{-4}$ as shown in Figure~\ref{fig:recall_estimate_mle}. 

\begin{figure}[h]
  \centering
  \includegraphics[width=0.35\linewidth]{figs/est_rec_mle.png}
%  \vspace{-0.5cm}
  \caption{Bias of the MLE estimator of precision and recall of CIS. \label{fig:recall_estimate_mle}}
\end{figure}





\section{Real-world experiment}
\label{sec:realworld}
We have also performed a real-world experiment to assess the improvements brought by CISs. In this experiment we compared the performance of our method to that of its baseline variant that does not use CISs signals; this baseline is obtained by applying our continuous-to-discrete policy reduction (see Section~\ref{sec:policy_discrete}) to the optimal continuous policy given by \cite{Azar8099}; this baseline is identical to setting precision to 0 in our algorithm and not giving any change-indicating signals. The algorithms were evaluated on two randomly selected sets of web pages, each set consisting of approximately 1 billion URLs coming from about 10,000 web hosts, with a crawl rate limit of 10K pages/sec in total. In this setting, our algorithm achieved roughly 10-20\% of refresh-crawling bandwidth saving on the affected hosts (for which CISs were available) depending on the noise level of the sitemap signals, while keeping at least the same or improved freshness, and the resulting freed-up extra crawling bandwidth improved freshness on other pages.

To save on computing the crawl value for each URL at every time step, in this experiments URLs were (dynamically) classified into lower and higher tiers, based on their crawl values, and the crawl values were recomputed more often for URLs in higher tiers.


\end{document}
\endinput
