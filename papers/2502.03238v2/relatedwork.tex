\section{Related Work}
% 自然场景long tail
\subsection{Long-tailed Classification}
Deep neural networks have demonstrated promising performance on various computer vision benchmarks, encompassing image classification  \citep{luo2024surgplan,chen2023star,chen2023surgical,yang2023hierarchical} and image segmentation \citep{yang2022d,zhu2023feddm,chen2024sam}.
However, real-world datasets usually follow a long-tailed class distribution, where most labels are associated with only a few samples but others are associated with only a few samples \citep{li2022long}. 
Such imbalanced distribution makes the data-sensitive deep learning models trained by naive likelihood maximization strategy biased towards the majority classes, leading to poor model performance on the minority classes \citep{lu2023label}. This impaired performance on the tail classes has hindered the implementation of deep learning models in real-world scenarios, becoming an increasing concern \citep{jin2023long}.

To tackle the challenge of class imbalance, a straightforward way is to resample the original dataset to retain a class-balanced subset, including over-sampling the tail classes \citep{more2016survey}, under-sampling the head classes \citep{buda2018systematic}, or sampling each class with the uniform probability \citep{kang2019decoupling}. Some studies \citep{lin2017focal, wang2021seesaw} propose to reweight the contribution of different classes to the loss function gradient to reach a balanced solution. \cite{lin2017focal} assigned a higher weight to misclassified examples that are hard to classify, while down-weighting easy examples that are correctly classified, to improve the performance on tail classes. \cite{cao2019learning} adjusted the margin between the decision boundary and training samples based on the label distribution, moving the boundary towards rare classes. \cite{wang2021seesaw} adaptively rebalanced positive and negative gradients for each category to mitigate the punishments to tail classes as well as compensate for the risk of misclassification caused by diminished penalties.
Yet, these re-weighting techniques improve the performance of tail classes at the cost of degradation on the head classes.


\subsection{Long-tails in Medical Imaging}
\pl{With rapid advancements, deep learning methods have demonstrated a strong capability in medical image classification tasks \citep{almalik2022self, liu2021medical, chen2020joint,chen2022instance}, highlighting the ability of computer-aided diagnosis and helping to alleviate the workload of clinicians \citep{zhao2022reasoning,chen2021diagnose,pan2024focus}}.
Meanwhile, the medical datasets are naturally imbalanced due to the scarcity of disease samples, causing the same long-tailed problems \citep{yang2020rethinking}. In the medical field, where constructing datasets is costly and diagnostic accuracy is crucial, addressing the challenges posed by long-tailed data is of utmost importance \citep{islam2021review}.

% existing methods except the decoupling
To mitigate the long-tailed problem in medical imaging, \cite{khushi2021comparative} explored a set of resampling-based methods, including under-sampling majority categories and over-sampling minority categories, to construct balanced subsets from the original dataset. 
\pl{\cite{chen2023pcct} proposed a novel class-balanced triplet sampler to alleviate the class imbalance in representation learning.}
\cite{rezaei2020addressing} proposed weighted cross-entropy loss, which manually adjusts the weight of the components of cross-entropy loss to address the long-tailed problem in medical image classification.
\cite{galdran2021balanced} performed instance-based and class-based re-sampling of the training data and mixed up the two sets of samples to construct a more balanced dataset.
\cite{ju2022flexible} incorporated a curriculum learning module with resampling methods to query new samples with per-class difficulty-aware sampling probability. However, these resampling approaches tend to undersample the head classes and lack the mechanism to synthesize new data for the tail classes, thereby limiting the model performance on the majority classes while providing marginal improvement for the minority groups.

\subsection{Decoupling Learning for Long-tails}
Despite the long-tailed problem causing performance degradation, \cite{tang2020long} pointed out that representation learning of encoders can still benefit from imbalanced data.
\cite{yang2020rethinking} proposed that even the imbalanced labeled data can be leveraged to boost the model's representation ability, but also emphasized that this may reduce classification performance due to classifier bias.  
To retain the visual representation ability of the encoder and alleviate the bias in the classifier, \cite{kang2019decoupling} disentangled the training process of the encoder and the classifier, which first trains the encoder on the whole dataset and then fine-tunes the classifier on frozen features under class-balanced sampling.

With the success of decoupling methods in the computer vision field \citep{zhou2023class, nam2023decoupled,chen2023medical}, recent long-tailed medical image classification tasks have adopted this two-stage training strategy.
In particular, \cite{chen2021self} conducted unsupervised learning in the first stage to eliminate the impact of label space and fine-tune the model on the class-balanced dataset to address the long-tailed problem.
\cite{marrakchi2021fighting} employed supervised contrastive learning in the first stage, which separates the feature space into different clusters by minimizing the distance between samples from the same class and maximizing the distance between samples from different classes, to boost the representation learning of the encoder.
\cite{li2022flat} proposed a flat-aware optimization strategy to approach a flatter optimum in the first stage, which better coordinates the training of the two stages. Nevertheless, these decoupling methods still suffer from imbalanced representation learning in the first stage and insufficient classifier calibration in the second stage, which can lead to suboptimal results. Different from existing decoupling methods, our LMD framework enhances representation learning with the multi-view relation-aware consistency strategy and iteratively calibrates the classifier with abundant virtual features.

\begin{figure*}[t]
\centering
\makebox[\textwidth][c]{\includegraphics[width=.99\textwidth]{framework.pdf}}
\caption{The illustration of our LMD framework. (a) In the Relation-aware Representation Learning, we enhance encoder's the representation learning ability with the MRC module on imbalanced datasets. (b) In the Iterative Classifier Calibration, we calibrate the classifier with abundant virtual features generated by VFC during the Maximization step and fine-tune the encoder with FDC during the Expectation step.} \label{fig:framework}
\end{figure*}