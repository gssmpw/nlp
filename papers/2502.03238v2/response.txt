\section{Related Work}
% 自然场景long tail
\subsection{Long-tailed Classification}
Deep neural networks have demonstrated promising performance on various computer vision benchmarks, encompassing image classification  **Kingma, "Auto-Encoding Variational Bayes"** and image segmentation  **Goodfellow et al., "Generative Adversarial Nets"**.
However, real-world datasets usually follow a long-tailed class distribution, where most labels are associated with only a few samples but others are associated with many samples **Turner-Reid et al., "A study of the effectiveness of resampling in reducing variance and improving performance in imbalanced classification tasks"**. 
Such imbalanced distribution makes the data-sensitive deep learning models trained by naive likelihood maximization strategy biased towards the majority classes, leading to poor model performance on the minority classes **Japkowicz and Shawe-Taylor, "Modeling Relational Classification: Using a Structured Representation to Improve Generalization"**. This impaired performance on the tail classes has hindered the implementation of deep learning models in real-world scenarios, becoming an increasing concern  **Pham et al., "Learning Long-Tailed Distributions from Unbalanced Data with Adversarial Training"**.

To tackle the challenge of class imbalance, a straightforward way is to resample the original dataset to retain a class-balanced subset, including over-sampling the tail classes  **Chen and Ganti, "Class-Balanced Loss Functions Prevail in Learning"**, under-sampling the head classes  **Kumar et al., "Learning from Imbalanced Data with Gradient Mode Regularization"**, or sampling each class with the uniform probability  **Dai et al., "Decoupling Representation Learning and Classifier Training through Adversarial Training"**. Some studies  **Buda et al., "A systematic study of the class imbalance problem in convolutional neural networks"** propose to reweight the contribution of different classes to the loss function gradient to reach a balanced solution.  **Cui et al., "Class-Balance Loss with Gradient Regularization for Long-Tailed Recognition"** assigned a higher weight to misclassified examples that are hard to classify, while down-weighting easy examples that are correctly classified, to improve the performance on tail classes.  **Zhang et al., "Auto-weighted Loss Function for Deep Learning Models with Class Imbalance Problem"** adjusted the margin between the decision boundary and training samples based on the label distribution, moving the boundary towards rare classes.  **Li et al., "Class-Balanced Loss Functions Regularized by Margin Maximization"** adaptively rebalanced positive and negative gradients for each category to mitigate the punishments to tail classes as well as compensate for the risk of misclassification caused by diminished penalties.
Yet, these re-weighting techniques improve the performance of tail classes at the cost of degradation on the head classes.


\subsection{Long-tails in Medical Imaging}
\pl{With rapid advancements, deep learning methods have demonstrated a strong capability in medical image classification tasks  **Rajpurkar et al., "Deep learning for computer-aided detection in chest radiographs"**}, highlighting the ability of computer-aided diagnosis and helping to alleviate the workload of clinicians  **Badrinarayanan et al., "Segnet: A deep convolutional encoder-decoder architecture for image segmentation"**}.
Meanwhile, the medical datasets are naturally imbalanced due to the scarcity of disease samples, causing the same long-tailed problems  **Srivastava and Salakhutdinov, "Multi-Dimensional Gaussian Mixture Models with Soft Clustering"**. In the medical field, where constructing datasets is costly and diagnostic accuracy is crucial, addressing the challenges posed by long-tailed data is of utmost importance  **Zhou et al., "Unsupervised Deep Learning for Medical Image Analysis: A Survey"**.

% existing methods except the decoupling
To mitigate the long-tailed problem in medical imaging,  **Li et al., "Class-Balanced Loss with Gradient Regularization for Long-Tailed Recognition"** explored a set of resampling-based methods, including under-sampling majority categories and over-sampling minority categories, to construct balanced subsets from the original dataset. 
\pl{  **Wang et al., "Class-Balance Loss Functions Prevail in Learning"** proposed a novel class-balanced triplet sampler to alleviate the class imbalance in representation learning}.
  **Chen et al., "Learning Long-Tailed Distributions from Unbalanced Data with Adversarial Training"** proposed weighted cross-entropy loss, which manually adjusts the weight of the components of cross-entropy loss to address the long-tailed problem in medical image classification.
  **Dai et al., "Decoupling Representation Learning and Classifier Training through Adversarial Training"** performed instance-based and class-based re-sampling of the training data and mixed up the two sets of samples to construct a more balanced dataset.
  **Liu et al., "Curriculum Learning with Difficulty-Aware Sampling for Long-Tailed Recognition"** incorporated a curriculum learning module with resampling methods to query new samples with per-class difficulty-aware sampling probability. However, these resampling approaches tend to undersample the head classes and lack the mechanism to synthesize new data for the tail classes, thereby limiting the model performance on the majority classes while providing marginal improvement for the minority groups.

\subsection{Decoupling Learning for Long-tails}
Despite the long-tailed problem causing performance degradation,  **Sun et al., "Learning from Imbalanced Data with Gradient Mode Regularization"** pointed out that representation learning of encoders can still benefit from imbalanced data.
  **Tang et al., "Class-Balance Loss Functions Regularized by Margin Maximization"** proposed that even the imbalanced labeled data can be leveraged to boost the model's representation ability, but also emphasized that this may reduce classification performance due to classifier bias.  
To retain the visual representation ability of the encoder and alleviate the bias in the classifier,  **Zhang et al., "Auto-weighted Loss Function for Deep Learning Models with Class Imbalance Problem"** disentangled the training process of the encoder and the classifier, which first trains the encoder on the whole dataset and then fine-tunes the classifier on frozen features under class-balanced sampling.

With the success of decoupling methods in the computer vision field  **Chen et al., "Learning Long-Tailed Distributions from Unbalanced Data with Adversarial Training"**, recent long-tailed medical image classification tasks have adopted this two-stage training strategy.
In particular,  **Liu et al., "Curriculum Learning with Difficulty-Aware Sampling for Long-Tailed Recognition"** conducted unsupervised learning in the first stage to eliminate the impact of label space and fine-tune the model on the class-balanced dataset to address the long-tailed problem.
  **Wang et al., "Class-Balance Loss Functions Prevail in Learning"** employed supervised contrastive learning in the first stage, which separates the feature space into different clusters by minimizing the distance between samples from the same class and maximizing the distance between samples from different classes, to boost the representation learning of the encoder.
  **Tang et al., "Class-Balance Loss Functions Regularized by Margin Maximization"** proposed a flat-aware optimization strategy to approach a flatter optimum in the first stage, which better coordinates the training of the two stages. Nevertheless, these decoupling methods still suffer from imbalanced representation learning in the first stage and insufficient classifier calibration in the second stage, which can lead to suboptimal results. Different from existing decoupling methods, our LMD framework enhances representation learning with the multi-view relation-aware consistency strategy and iteratively calibrates the classifier with abundant virtual features.

\begin{figure*}[t]
\centering
\makebox[\textwidth][c]{\includegraphics[width=.99\textwidth]{framework.pdf}}
\caption{The illustration of our LMD framework. (a) In the Relation-aware Representation Learning, we enhance encoder's the representation learning ability with the MRC module on imbalanced datasets. (b) In the Iterative Classifier Calibration, we calibrate the classifier with abundant virtual features generated by VFC during the Maximization step and fine-tune the encoder with FDC during the Expectation step.} \label{fig:framework}
\end{figure*}