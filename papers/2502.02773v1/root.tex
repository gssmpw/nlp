\newcommand{\CLASSINPUTtoptextmargin}{50mm}
\newcommand{\CLASSINPUTbottomtextmargin}{50mm}
\newcommand{\CLASSINPUTinnersidemargin}{50mm}
\newcommand{\CLASSINPUToutersidemargin}{50mm}
% , margin=72pt
\documentclass[letterpaper, 10 pt, conference]{ieeeconf} 
\IEEEoverridecommandlockouts
% The preceding line is only needed to identify funding in the first footnote. If that is unneeded, please comment it out.
\usepackage{cite}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{algorithmic}
\usepackage{graphicx}
\usepackage{textcomp}
\usepackage{xcolor}
\usepackage{subcaption}
\usepackage{multirow}
% \def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em
%     T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}
\usepackage{url}
% \usepackage[left=1.91cm,right=1.91cm,top=1.91cm, bottom=1.97cm]{geometry}

\begin{document}
\include{commands}

\title{\LARGE \bf 
\our: Enhancing Standard Definition Maps \\ by Incorporating Road Knowledge using LLMs}

\author{Hitvarth Diwanji$^{*1}$,  Jing-Yan Liao$^{*1}$, Akshar Tumu$^{*1}$, \\ Henrik I. Christensen$^{2}$, Marcell Vazquez-Chanlatte$^{3}$, and Chikao Tsuchiya$^{3}$
\thanks{This research is supported by Nissan.}
\thanks{*These authors contributed equally to this work.}%
\thanks{$^{1}$Contextual Robotics Institute,
University of California San Diego, La Jolla, CA 92093, USA 
{\tt\small \{hdwanji, j3liao, atumu\}@ucsd.edu}}
\thanks{$^{2}$Dept. of Comp. Sci. and Eng., UC San Diego, La Jolla, CA 92093, USA {\tt\small hichristensen@ucsd.edu}}
\thanks{$^{3}$Nissan North America, 3400 Central Expy, Santa Clara, CA 95051}%
}

\maketitle

\begin{abstract}
High-definition maps (HD maps) are detailed and informative maps capturing lane
centerlines and road elements. Although very useful for autonomous driving, HD
maps are costly to build and maintain. Furthermore, access to these high-quality
maps is usually limited to the firms that build them. On the other hand,
standard definition (SD) maps provide road centerlines
with an accuracy of a few meters. In this paper, we explore the possibility of
enhancing SD maps by incorporating information from road manuals using LLMs. We
develop \our, an end-to-end pipeline to enhance SD maps with location-dependent
road information obtained from a road manual. We suggest and compare several
ways of using LLMs for such a task. Furthermore, we show the generalization
ability of \our\ by showing results from both California and Japan.
\end{abstract}


% \begin{IEEEkeywords}
% Mapping, LLMs, RAG, Autonomous Vehicles
% \end{IEEEkeywords}



\section{Introduction}

High-Definition (HD) maps play a critical role in autonomous driving systems,
supporting key components such as localization, prediction, and planning. Their
popularity stems from their precision in capturing road geometry and providing
lane-level details, offering valuable context for a wide range of driving tasks.
HD maps leverage advances in sensor fusion technology, incorporating multiple
modalities that complement each other. For instance, 3D point cloud data from
LiDAR is often enriched with semantic information derived from camera inputs,
enabling a comprehensive understanding of lanes, junctions, and road segments.

Despite their importance, the creation and maintenance of HD maps remain
resource-intensive and challenging. Producing these maps requires extensive data
collection using vehicles equipped with high-end sensors like LiDAR and RGB
cameras. This raw data then undergoes sophisticated processing to extract
detailed semantic features like lane boundaries. However, the process often
involves significant manual annotation, which is both time-consuming and
labor-intensive. Furthermore, HD maps require frequent updates to accommodate
changes in road conditions, such as construction or layout modifications, making
them costly and difficult to scale. Prior studies~\cite{osmvshdmap} highlight
these operational challenges, emphasizing the need for more efficient and
scalable solutions. To address these limitations, researchers have explored
alternative approaches, such as generating HD maps directly from onboard sensor
data~\cite{li2021hdmapnet, MapTR, li2023toponet, ranganatha2024semvecnetgeneralizablevectormap}. While promising,
these methods often face challenges with generalization, limited range, and still require
significant data collection.

\begin{figure}
    \centering
    \includegraphics[width=0.95\linewidth]{figs/thumbnail.png}
    \caption{Overview of our proposed pipeline, which explores the potential of leveraging open-source map data, large language models, and official documents (e.g., highway design manuals) for mapping.}
    \label{fig:teaser}
\end{figure}

Standard-Definition (SD) maps offer a cost-effective
and scalable alternative. Open-source options like OpenStreetMap
(OSM)~\cite{OpenStreetMap} represent road networks using basic geographical
coordinates, making them easy to scale. However, SD maps lack the fine-grained
detail and accuracy~\cite{osm_quality} required for many autonomous driving applications, limiting
their direct usability. Recent research~\cite{smerf, ory_iros2024} has attempted to generate HD maps by integrating them with real-time sensor data, improving their usability for lane topology reasoning.  Nevertheless, these methods still face challenges related to generalization and data dependency. Given that most existing approaches extract lane-level details primarily from visual cues, we explore whether SD maps can be enhanced using other publicly available resources, potentially narrowing the gap between SD and HD maps.

In this study, we propose a novel approach to bridge the gap between HD and SD
maps without relying on sensor data or manual annotation. Specifically, we
explore the use of large language models (LLMs) to enhance SD maps using
publicly available resources, such as highway design manuals~\cite{hdm} that
provide detailed road design specifications. By leveraging these resources alongside OSM data, our method aims to generate enriched map representations that approximate the detail and utility of HD maps. Our key contributions are as follows:

\begin{itemize}
\item \textbf{Sensor-Free SD Map Enhancement: }We present a novel method for
  enhancing SD maps without using any sensor data, leveraging open-source
  information and LLMs to automate the process.
\item \textbf{Generalization Across Regions: }We demonstrate the generalization
  ability of our approach by incorporating road design guidelines from multiple
  regions, including various U.S. states and Japan.
\end{itemize}

This work aims to provide a scalable and cost-effective alternative to
traditional HD map creation while retaining its critical functionality for
autonomous driving systems.

\section{Related Work} \label{related_work}
\subsection{High Definition (HD) Maps Generation}
HD map generation has been extensively studied in the context of autonomous
driving, with research broadly categorized into online and offline methods. Both
approaches aim to create detailed, accurate maps but differ significantly in
their operational processes and use cases. Online HD map
generations~\cite{li2021hdmapnet,li2023toponet,MapTR,ranganatha2024semvecnetgeneralizablevectormap,
  ory_iros2024} estimate locations of map elements based on sensor inputs to
avoid continuous map maintenance. Offline HD map generation methods allow the
aggregation of more information. For example, Zhou et
al.\cite{Zhou2021IROS_HDmap} automate the HD Map building pipeline with instance
segmentation, mapping, and particle filter-based lane aggregation.

\subsection{LLM for Autonomous Driving}
Recently, the autonomous driving domain saw many diverse applications of LLMs.
DriveGPT4~\cite{drivegpt4} and LMDrive~\cite{lmdrive} attempt to approach
autonomous driving in an end-to-end fashion. Elhafsi et al.
\cite{anomalydetection} detect visual anomalies using LLMs to prevent certain
failure modes in autonomous driving. Furthermore, we see numerous works focusing
on either of perception~\cite{hilmd}, prediction~\cite{llm_mp, mtd_gpt, lcllm},
or planning and control~\cite{planagent, dme_driver, agent_driver} aspects of
autonomous driving. However, to the best of our knowledge, we have not found any
work that leverages LLMs for mapping. The closest work we see are around map
annotations for more awareness of the surroundings. Talk2BEV~\cite{talk2bev}
annotates an instantaneous Bird's Eye View (BEV) map with natural language
descriptions of identified map elements (like cars, bikes, etc) using LLMs.
However, their language-enhanced map is frame-dependent, which implies its
single usage. \our\ on the other hand, is a static map and can be reused for
downstream tasks. Moreover, our method focuses on enhancing existing SD maps
without using any sensors.

% \red{hitvarth}
% \begin{itemize}
%     \item end to end AV
%     \item anomaly avoidance
%     \item not much work on mapping
%     \item map annotations: Talk2BEV
%     \item how our work is different
% \end{itemize}
% \begin{figure}
%     \centering
%     \includegraphics[width=0.9\linewidth]{figs/osm.png}
%     \caption{An extract from an example OSM}
%     \label{fig:osm}
% \end{figure}

\section{Prerequisites}
\subsection{OSM Data Format}
An OSM consists of \textit{nodes}, \textit{ways} and \textit{relations}
supplemented with metadata for each data structure. Every node $n=(id, lat,
lon)$ in an OSM has a unique node identifier and represents a geographical
point denoted by latitude and longitude. All the roads are defined using ways. A
way $w = (n_0, n_1, ...)$ is represented as a sequence of nodes that make up the
way. Structures like buildings can also be represented as ways. Finally,
relations are used to indicate logical associations between nodes and ways. In
our case, relations are not used as our focus is to enhance lane-level details
of road elements. 
% An OSM can be downloaded as an XML file, as shown in Fig.\ref{fig:osm}.

\subsection{Road Manuals}
Highway Design Manual (HDM)~\cite{hdm} is a comprehensive document that provides
standardized guidelines and specifications for the design, construction, and
maintenance of roadways and related infrastructure. It provides detailed
parameters such as lane widths, shoulder dimensions, road alignments, and
traffic control elements like signage and markings. These manuals are created
and maintained by government transportation departments, such as the U.S.
Department of Transportation (DOT) or state-level agencies to ensure safety,
efficiency, and consistency in highway design across different regions.

The HDM is particularly valuable for SD map enhancement, as it defines precise
geometric and structural details that can serve as priors for enhancing map
accuracy. For example, the manual specifies road lane configurations, minimum
and maximum widths, and intersection designs, all critical inputs when
augmenting SD maps with lane-level information. With this official document in
hand, \our\ makes our map representation closer to an HD map at a low cost.

An example illustration from HDM is shown in Fig. \ref{fig:hdm}

\begin{figure}
    \centering
    \includegraphics[width=0.95\linewidth]{figs/hdm.png}
    \caption{An example illustration from the Highway Design Manual (HDM)}
    \label{fig:hdm}
\end{figure}

\begin{figure*}
\centering \includegraphics[width=0.9\textwidth]{figs/pipeline.png}
\caption{\our\ takes OSM as input, filters it by removing non-road elements,
  processes it using A/B street software and prompts the RAG pipeline with the
  basic road information from A/B street. The RAG pipeline maintains a vector
  database of information from road manuals. When prompted with a query, it uses
  an LLM to obtain detailed road information of the queried roads using the
  context from road manuals. This detailed road information then goes to an
  LLM-generated code that gives us the enhanced HD map.}
\label{fig:pipeline}
\end{figure*}

\subsection{Retrieval-Augmented Generation (RAG)}
RAG is an approach in natural language processing that combines the retrieval of
relevant information from an external knowledge base with the generation of text
using a language model. Large documents are first broken down into smaller
chunks. These chunks of text are converted to embeddings using an embedding
model and stored in a vector database. Then, the embedding of a user query is
compared with all the stored embeddings using similarity measures like the
cosine similarity, and the document chunks corresponding to $k$, where most similar
embeddings are retrieved. The retrieved documents are then appended to the user
query and passed to the LLM to generate the final answer. This method enables
the language model to access domain-specific information, improving its accuracy
and relevance for specific tasks. RAG is beneficial for SD map enhancement due
to the following reasons.


\begin{itemize}

\item \textbf{Incorporating Domain Knowledge: }RAG allows the integration of
  external resources like highway design manuals or traffic regulations. By
  retrieving specific details, such as lane dimensions or intersection
  configurations, and providing them as context to the language model, RAG
  ensures the outputs adhere to real-world standards.
\item \textbf{Enhancing Consistency: }By retrieving consistent and structured
  information from SD maps or related documents, RAG can help create enhanced HD
  maps that follow established rules and guidelines, reducing errors or
  inconsistencies.
\item \textbf{Dynamic Updates: }As road conditions or standards change, RAG can
  retrieve the most up-to-date information, enabling HD map generation processes
  to adapt without retraining the language model.
\item \textbf{Reducing Model Limitations: }LLMs alone may struggle with
  domain-specific or granular tasks like accurate map enhancement. RAG bridges
  this gap by grounding the model's outputs in retrieved, authoritative
  information.
\end{itemize}


\section{Methodology}
In this section, we introduce two pipelines for generating enhanced Standard
Definition (SD) maps: Direct Generation and Knowledge-Based Algorithmic
Generation. The comparison of these methods and their results are presented in
Section \ref{experiments}. Inspired by the recent advancements in Large Language
Models (LLMs) for text generation \cite{gpt3, llama}, we explore their potential
to enhance SD maps, given that SD maps, such as OpenStreetMap (OSM) data, are
represented in text form. The Direct Generation method uses a straightforward
approach by combining OSM input, LLM capabilities, and highway design manual
(HDM) knowledge. In contrast, Knowledge-Based Algorithmic Generation
incorporates Retrieval-Augmented Generation (RAG) techniques and additional
post-processing steps, leveraging LLMs for specific tasks while addressing the
limitations of direct text-based map generation.


\subsection{Direct Generation}

The Direct Generation method tests the capability of LLMs to enhance SD maps
through structured prompting and contextual input.
\subsubsection{Process}
\begin{itemize}
  \item \textbf{Preprocessing OSM Data: }OSM data is first filtered to retain
    only road-related information. This preprocessing step ensures that
    irrelevant details do not overwhelm the LLM.
  \item \textbf{Providing Context with Pre-Prompt: }A carefully designed
    pre-prompt describes the OSM data structure, its common tags, and how these
    relate to HD maps. This step helps the LLM understand the domain-specific
    context and prepares it for processing the input.
  \item \textbf{Incorporating Highway Design Manual (HDM): }The HDM is uploaded
    with instructions for the LLM to extract key road-related details such as
    lane width, shoulder widths, and other essential parameters for SD map
    enhancement.
  \item \textbf{Constraints for HD Map Generation: }The LLM is tasked with
    generating the enhanced map while adhering to specific constraints:
  \begin{enumerate}
      \item All road nodes in the OSM data must be included.
      \item No modifications are allowed to the existing road nodes in OSM.
      \item Any assumptions made during the process must be explicitly stated.
      \item Clearly state any assumptions made.
  \end{enumerate}
  \item \textbf{Output Format Specification: }The final output is formatted to
    allow further analysis and validation.
\end{itemize}

\subsubsection{Challenges Identified}

While this method leverages LLMs for generating enhanced maps, its performance
is limited by inconsistencies in text-based map generation, as demonstrated in
Section \ref{experiments}. These limitations led us to design an improved
method: Knowledge-Based Algorithmic Generation.


\subsection{Knowledge-based Algorithmic Generation}
Recognizing that text-based map generation is not a strength of LLMs; we shifted
their role to tasks they excel at, such as extracting structured knowledge from
the HDM. This refined pipeline, shown in Fig.~\ref{fig:pipeline}, integrates
additional preprocessing and post-processing steps to address the shortcomings
of the Direct Generation approach.
\begin{itemize}
  \item \textbf{Enhanced Preprocessing Using A/B Street: }To address
    inconsistencies in OSM data due to its open-source nature, we use the A/B
    Street tool \cite{abstreet} to further refine and standardize road segment
    attributes. This step ensures consistent labels across all road segments.
    Furthermore, it converts the XML format of OSM to JSON format, which is
    easier to parse for an LLM.
  \item \textbf{Segment-Level Information Extraction: }Essential details for
    each road segment—such as lane width, bike lane width, and total road
    width—are extracted from the HDM using LLMs. The LLM focuses solely on
    understanding and retrieving relevant information, leveraging its strengths
    in contextual text analysis.
  \item \textbf{Map Generation: }With the extracted road information, the
    enhanced map is generated through algorithmic methods. These methods ensure
    that the final output adheres to the constraints of HD maps and aligns with
    the HDM specifications.
\end{itemize}

We present three variants of Algorithmic Generation:

\subsubsection{One-Shot Generation (\oneshot)}
Here the complete road information is generated in one run of the pipeline
(shown in Fig. \ref{fig:pipeline}).

\subsubsection{Incremental Generation (\increm)} In this variant, we request the LLM to generate the road information one lane at a time. This is to put more attention to every lane.

\subsubsection{Autoregressive Generation (\autoreg)} Just like \increm\ variant, \autoreg\ generates lanes one-by-one. But here, the pipeline is run in an autoregressive manner. That is, once we generate information for one lane, it is appended to the context for generating the next lane. Hence, the generated lanes benefit from the extracted information of other lanes. 

In terms of cost, \autoreg\ $>$ \increm\ $\gg$ \oneshot. We compare these variants in section \ref{experiments} to see if the added expense is worth the performance gain.


% Here, 
% In this variant, the pipeline is run multiple times to extract the road info for
% every road segment. Generated information for each road segment is appended to
% the context of LLM for the next run of the pipeline. 


% \begin{figure}
% % https://lucid.app/lucidchart/3b35d148-1e03-4c02-9595-796ca0db6997/edit?viewport_loc=189%2C-535%2C2912%2C1615%2CBAx-jatOpryT&invitationId=inv_91ba16a1-0939-4a9b-b60e-0ebd6a8f8eac
% % \begin{adjustwidth}{-\extralength}{0cm}
% \centering
% \includegraphics[width=0.35\textwidth]{figs/direct_gen.png}
% % \end{adjustwidth}
% \caption{The diagrams (a), (b), and (c) display the significant configuration changes across platforms in sensor poses and number of sensors, for both LiDAR and cameras in NuScenes~\cite{caesar_nuscenes_2020}, Argoverse 2~\cite{Argoverse2} and AVL.}
% \label{fig:direct_gen}%MDPI: Please note that figures and tables should be put after their first citations, and the size and position of figures and tables will be adjusted appropriately to ensuring the clarity and readability of the image during the final steps. %Author: The tables and figures are ordered properly now.
% \end{figure}
% \subsection{Meta-Prompting}
% improve the prompt by using LLM
\section{Experiments} \label{experiments}
In this section, we first introduce our implementation details in
\ref{implementation}, and show our qualitative and quantitative results for both
of our approaches in \ref{ourmethod}. Finally, we compare on a subset of
Argoverse 2 dataset~\cite{Argoverse2}, and Tokyo
Japan to demonstrate our generalization ability.
\subsection{Implementational Details}\label{implementation}
For preprocessing, we use osmium~\cite{osmium} and osmfilter~\cite{osmfilter} to
extract and filter OSM data.

In the RAG pipeline, we utilize LangChain~\cite{langchain}. We use chapter 300 of the Highway Design Manual (HDM)~\cite{hdm} as our road manual. The document is
divided into smaller chunks of text using PyPDFLoader~\cite{PyPDFLoader}, and we generate text
embeddings for these chunks using the OpenAI embedding model.

For the generation component of the RAG pipeline, we primarily use GPT-4o as the
large language model (LLM). We also experiment with Llama~\cite{llama}, and provide a comparison of their
performance in the following subsections.

To parse the extracted road information, we use Python code generated by an LLM. This code converts the data into a JSON format, representing sequences of points for roads, lanes, and bike lanes.

\subsection{Direct generation vs Algorithmic Generation}\label{ourmethod}

In this experiment, we analyze and compare the outputs of direct generation with
algorithmic generation for a small OSM map spanning about 100 meters. Fig. \ref{fig:direct_vs_algo} 
shows example output for both methods. The lane widths are consistent for algorithmic 
generation but vary noticeably across the directly generated map. The direct generation 
method also fails to comprehend the scale of lane width compared to the map's scale. On 
conducting several runs, we observe that the results of algorithmic
generation are consistent, whereas the direct generation fails to produce clean
outputs. The example shown in Fig. \ref{fig:direct_vs_algo} was the best
of 5 runs. Furthermore, there were cases in which direct generation produced
broken outputs or failed to provide an output altogether.

\begin{figure}[]
\centering
\begin{subfigure}{.5\linewidth}
  \centering
  \includegraphics[width=.95\linewidth]{figs/gilman_direct.png}
  \caption{Direct Generation}
\end{subfigure}%
\begin{subfigure}{.5\linewidth}
  \centering
  \includegraphics[width=.95\linewidth]{figs/gilman_python.png}
  \caption{Algorithmic Generation}
\end{subfigure}
\caption{A qualitative example for direct generation vs algorithmic generation}
\label{fig:direct_vs_algo}
\end{figure}



\begin{table*}[h]
    \centering
    \begin{tabular}{|c|c|c|c|c|c|}
    \hline
         \multirow{2}{*}{Model} & \multirow{2}{*}{Variant} & \multirow{2}{*}{\% of Valid Maps}  & \multirow{2}{*}{Chamfer$_{avg}$ (m)} & \multirow{2}{*}{Chamfer$_{min}$ (m)} & \multirow{2}{*}{Recall} \\ 
          &  &  &  &  &  \\ \hline
         % \our\ (Llama, OSG) & & & \\ 
         % \our\ (GPT-4o, OSG) & & & \\ 
         % \our\ (Llama, IG) & & & \\ 
         % \our\ (GPT-4o, IG) & & & \\ \hline
         GPT-4o ($P1$) & \oneshot & 83 & $32.04 \pm 36.12$ & $11.68 \pm 22.17$ & $0.28 \pm 0.27$ \\
         GPT-4o ($P2$)& \oneshot & 100 & $2.85 \pm 0.4$ & $0.23 \pm 0.11$ & $0.77 \pm 0.11$ \\
         GPT-4o & \increm & 100 & $2.87 \pm 0.37$ & $0.26 \pm 0.16$ & $0.78 \pm 0.11$ \\
         GPT-4o & \autoreg & 100 & $3.01 \pm 0.51$ & $0.44 \pm 0.29$ & $0.76 \pm 0.11$ \\ \hline
         Llama ($P1$) & \oneshot & 0 & N/A & N/A & $0.00 \pm 0.00$ \\
         Llama ($P2$) & \oneshot & 50 & $3.99 \pm 1.47$ & $2.99 \pm 2.28$ & $0.10 \pm 0.08$ \\
         Llama & \increm  & 100 & $3.12 \pm 0.31$ & $0.41 \pm 0.51$ & $0.78 \pm 0.11$ \\
         Llama & \autoreg & 100 & $3.32 \pm 1.18$ & $0.58 \pm 0.74$ & $0.72 \pm 0.21$ \\ \hline
    \end{tabular}
    \caption{Quantitative results on Argoverse 2 comparing various variants of \our. N/A indicates no matching lane for chamfer distance calculation. }
    \label{tab:av2}
\end{table*}

\subsection{Results on Argoverse 2}
\subsubsection{Evaluation method} We compare the output of \our\ with Argoverse ground truth using two metrics: Averaged Chamfer distance~\cite{chamfer}, and Recall. To calculate the Chamfer distance-based metric, the corresponding road in \our\ map for every road in Argoverse is established by matching their OSM way ID. The correspondence between predicted and ground-truth lanes is determined by minimizing the Chamfer distance. Table \ref{tab:av2} shows the average of all these minimum distances. A \our\ lane prediction is considered correct if its Chamfer Distance to a lane in Argoverse 2 is less than 5 meters, as this threshold roughly corresponds to the typical road width. Following that definition of a correct lane, the recall is calculated as 
\begin{align*}
    recall &= \frac{\text{correct lanes}}{\text{total ground truth lanes}}
\end{align*}

\begin{figure}[]
\centering
\begin{subfigure}{.8\linewidth}
  \centering
  \includegraphics[width=.95\linewidth]{figs/p1.png}
  \caption{Prompt $P1$}
\end{subfigure}%
\vspace{1.5mm}
\begin{subfigure}{.8\linewidth}
  \centering
  \includegraphics[width=.95\linewidth]{figs/p2.png}
  \caption{Prompt $P2$}
\end{subfigure}
\caption{Prompts $P1$ (a) and $P2$ (b) used in the RAG pipeline to obtain the detailed road information from the LLM}
\label{fig:prompt}
\end{figure}
% \begin{table*}[]
%     \centering
%     \begin{tabular}{|c|c|c|c|c|c|}
%     \hline
%          Model & Variant & \# City & Chamfer & Chamfer & Recall \\ 
%           &  &  & (Avg$\pm \sigma$) & (Min$\pm \sigma$) &  \\ \hline
%          % \our\ (Llama, OSG) & & & \\ 
%          % \our\ (GPT-4o, OSG) & & & \\ 
%          % \our\ (Llama, IG) & & & \\ 
%          % \our\ (GPT-4o, IG) & & & \\ \hline
%          GPT-4o (old) & \oneshot & 5 & $32.04 \pm 36.12$ & $11.68 \pm 22.17$ & $0.28 \pm 0.27$ \\
%          GPT-4o & \oneshot & 6 & $2.85 \pm 0.4$ & $0.23 \pm 0.11$ & $0.77 \pm 0.11$ \\
%          GPT-4o & \increm & 6 & $2.87 \pm 0.37$ & $0.26 \pm 0.16$ & $0.78 \pm 0.11$ \\
%          GPT-4o & \autoreg & 6 & $3.01 \pm 0.51$ & $0.44 \pm 0.29$ & $0.76 \pm 0.11$ \\ \
%          Llama (old) & \oneshot & 0 & N/A & N/A & $0.00 \pm 0.00$ \\
%          Llama & \oneshot & 3 & $3.99 \pm 1.47$ & $2.99 \pm 2.28$ & $0.10 \pm 0.08$ \\
%          Llama & \increm  & 6 & $3.12 \pm 0.31$ & $0.41 \pm 0.51$ & $0.78 \pm 0.11$ \\
%          Llama & \autoreg & 6 & $3.32 \pm 1.18$ & $0.58 \pm 0.74$ & $0.72 \pm 0.21$ \\ \hline
%     \end{tabular}
%     \caption{Quantitative results on Argoverse 2 comparing various variants of \our. N/A indicates no matching lane for chamfer distance calculation.}
%     \label{tab:av2}
% \end{table*}




\subsubsection{Quantitative results}\label{quantitative} The quantitative evaluation of our approach on Argoverse is provided 2 in Table \ref{tab:av2}. Our experiments reveal several key insights regarding the performance of different language models and the impact of prompt design.

First, we compare the performance of two large language models (LLMs), GPT-4o and Llama 3.1. Our results show that GPT-4o consistently outperforms Llama across various methods. Notably, GPT-4o achieves strong results simply by using an improved prompt, whereas Llama benefits more from an incremental generation approach. This difference in performance may be attributed to the model size, as we use the 7-billion-parameter version of Llama, which is significantly smaller than GPT-4o.

Second, we analyze the role of prompt design in improving map generation. Previous studies have shown that even small changes in prompt phrasing can lead to significant variations in LLM outputs~\cite{promptprogramming, calibrateuse}. We observe a similar effect in our \our\ predictions.  As shown in Fig. \ref{fig:prompt}, we compare two prompts, $P1$ and $P2$, and find that the phrasing of the prompt has a notable impact on the quality of generated maps. These findings underscore the importance of both model selection and prompt engineering in improving SD map generation using LLMs. Furthermore, our results demonstrate consistency, as supported by the Chamfer Distance and variance metrics. Since road width is typically around 5 meters, our generated maps could serve as a strong prior, incorporating road connectivity and lane-level information.

\begin{figure}[]
\centering
\begin{subfigure}{.5\linewidth}
  \centering
  \includegraphics[width=.95\linewidth]{figs/paloalto/compressed-images/gpt_osg_old.jpg}
  \caption{GPT-4o OSG $P1$}
\end{subfigure}%
\begin{subfigure}{.5\linewidth}
  \centering
  \includegraphics[width=.95\linewidth]{figs/paloalto/compressed-images/gpt_osg_new.jpg}
  \caption{GPT-4o OSG $P2$}
\end{subfigure}
% \begin{subfigure}{.195\linewidth}
%   \centering
%   \includegraphics[width=.95\linewidth]{figs/paloalto/llama_osg_new.jpg}
%   \caption{Llama OSG $P2$}
% \end{subfigure}
\begin{subfigure}{.49\linewidth}
  \centering
  \includegraphics[width=.95\linewidth]{figs/paloalto/compressed-images/llama_ig.jpg}
  \caption{Llama IG}
\end{subfigure}
\begin{subfigure}{.45\linewidth}
  \centering
  \includegraphics[width=.95\linewidth]{figs/paloalto/google_earth_compressed_1.jpg}
  \caption{Satellite Image}
\end{subfigure}

\caption{Qualitative comparison for an Argoverse 2 sample from Palo Alto}
\label{fig:av2_qual_cali}
\end{figure}

\begin{figure}[]
\centering
\begin{subfigure}{.5\linewidth}
  \centering
  \includegraphics[width=.95\linewidth]{figs/miami/compressed-images/gpt_osg_old.jpg}
  \caption{GPT-4o OSG $P1$}
\end{subfigure}%
\begin{subfigure}{.5\linewidth}
  \centering
  \includegraphics[width=.95\linewidth]{figs/miami/compressed-images/gpt_osg_new.jpg}
  \caption{GPT-4o OSG $P2$}
\end{subfigure}
% \begin{subfigure}{.195\linewidth}
%   \centering
%   \includegraphics[width=.95\linewidth]{figs/miami/llama_osg_new.jpg}
%   \caption{Llama OSG $P2$}
% \end{subfigure}
\begin{subfigure}{.5\linewidth}
  \centering
  \includegraphics[width=.95\linewidth]{figs/miami/compressed-images/llama_ig.jpg}
  \caption{Llama IG}
\end{subfigure}
\begin{subfigure}{.45\linewidth}
  \centering
  \includegraphics[width=.95\linewidth]{figs/miami/google_earth_compressed_3.jpg}
  \caption{Satellite Image}
\end{subfigure}

\caption{Qualitative comparison for an argoverse 2 sample from Miami}
\label{fig:av2_qual_miami}
\end{figure}

\subsubsection{Qualitative result} To further illustrate the differences between models and prompt variations, we present qualitative results as well as satellite images~\cite{googleearth} in Fig. \ref{fig:av2_qual_cali} and Fig. \ref{fig:av2_qual_miami}. These examples demonstrate how changes in prompt phrasing influence the outputs of both Llama and GPT-4o. As observed in the quantitative analysis, small modifications in prompt wording can lead to significant differences in map generation. For instance, in both Miami and Palo Alto, using prompt $P1$, the model fails to extract all road segments, whereas $P2$ results in a more complete output. 

Additionally, increasing the amount of contextual information in the prompt sometimes leads to a loss in output quality. We hypothesize that the additional input distracts the LLM, making it less effective in generating accurate lane predictions. Furthermore, Llama tends to produce wider lane estimates compared to GPT-4o. We believe this is due to Llama’s smaller model size, which may affect its ability to precisely interpret road design specifications.

Overall, our results highlight the crucial role of both prompt engineering and model selection in improving SD map enhancement. By refining prompt strategies and optimizing incremental processing, we can achieve more reliable and detailed lane structures.

% 3. Generalization ability based on AV2
% 4. ablation studies across LLM Models

% \begin{figure}[]
% \centering
% \begin{subfigure}{.5\linewidth}
%   \centering
%   \includegraphics[width=.95\linewidth]{figs/japan/tokyo_openai_gpt-4o_wo_borders.png}
%   \caption{GPT-4o}
% \end{subfigure}%
% \begin{subfigure}{.5\linewidth}
%   \centering
%   \includegraphics[width=.95\linewidth]{figs/japan/google_earth.png}
%   \caption{Satellite Image}
% \end{subfigure}
% \caption{A qualitative example for direct generation vs algorithmic generation}
% \label{fig:japan_results}
% \end{figure}



\subsection{Results from Japan} In this section, we demonstrate the generalization capability of \our\ by presenting results from Japan (Fig. \ref{fig:japan_results}). We use an English-translated version of the Japan Road Law~\cite{JapanManual1} as our reference road manual to adapt our approach to a different country. By leveraging this official document, \our\ successfully generates road maps that align with local road design standards, showcasing its ability to adapt across regions.

This result highlights the potential of \our\ to be applied beyond its initial designed region without requiring sensor data or manual annotations. The ability to incorporate diverse road regulations from different countries suggests that our approach can scale efficiently and provide reliable enhanced prior knowledge of roads in various geographic locations.

\begin{figure}[]
\centering
\begin{subfigure}{.95\linewidth}
  \centering
  \includegraphics[width=.95\linewidth]{figs/japan/tokyo_openai_gpt-4o_wo_borders_compressed.png}
  \caption{GPT-4o \oneshot $P2$}
\end{subfigure}%
\vspace{1.5mm}
\begin{subfigure}{.95\linewidth}
  \centering
  \includegraphics[width=.95\linewidth]{figs/japan/google_earth_compressed_2.jpg}
  \caption{Satellite Image}
\end{subfigure}
\caption{A qualitative example in Japan to demonstrate generalization capability}
\label{fig:japan_results}
\end{figure}

\section{Conclusion}
In this work, \our\ presents a novel map representation without needing
sensor data. By leveraging LLMs and RAG, we utilize official documents like
Highway Design Manuals to enhance existing SD maps with lane-level details,
providing valuable context for downstream tasks. \our\ demonstrates
generalizability across OSM data from different states and other countries.
However, it has limitations; SD maps' accuracy is not improved since
no additional sensor data is used. Also, the performance of the \our\ depends
notably on the road manuals used. The outputs will suffer if the road manuals do not contain adequate
information. Future work will focus on improving
intersection handling and possibly exploring live official web pages, such as
Caltrans project data~\cite{caltrain}, to achieve more accurate results and keep
road users updated with the latest information. Overall, we see significant
potential for LLMs in advancing mapping technologies, especially when used as a
for generating strong prior for autonomous driving applications.

% Future work
% \begin{itemize}
%     \item incorporate other elements like cul-de sacs
%     \item better handling of intersections
%     \item incorporate updated information (e.g. roads under construction) using Caltrans current projects (https://consmap.dot.ca.gov)
% \end{itemize}

\bibliographystyle{unsrt}
\bibliography{ref}

\end{document}
