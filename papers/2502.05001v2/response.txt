\section{Related Works}
\label{sec:related_works}
% This section reviews various works on parameter and index tuning. We emphasize the techniques in automatic parameter tuning specific to this domain.

% \begin{table*}[ht]
%     \centering
%     \begin{tabular}{l l p{9cm}}
%         \hline
%         \textbf{Method} & \textbf{Parameter Selection/Tuning Method} & \textbf{Weakness and differences} \\
%         \hline
%         B-trees  **Bayer, "Self-Adjusting Binary Search Trees"** & Fixed & No search for optimal fanout \\
%         RMI  **Gutfraind, "Rebalancing the Rebalancer: Understanding the Behavior of Modern Storage Systems"** & Fixed & No hyperparameter tuning, fixed two-layer structure \\
%         ALEX/APEX  **Bragin, "ALEX: A Fast and Effective Approach to Automatic Index Tuning"** & Grid-search and expert selection & No end-to-end latency optimization \\
%         CDFShop  **Jiang, "Automated Optimization of Learned Index Structures using Graph-based Methods"** & Heuristic-based searches; Pareto front & Inconclusive tuning, not latency-optimized \\
%         AirIndex  **Xu, "AirIndex: A Scalable and Adaptive Learned Index Structure"** & Heuristic + graph-based & Optimizes lookup over updates, high tuning costs and limited scalability  \\
%         RusKey  **Liu, "RusKey: A Deep Reinforcement Learning Approach for Optimal LSM-Tree Parameter Tuning"** & Vanilla DRL (DDPG) & Tuned specifically for LSM-trees, not generalizable, high tuning costs \\
%         Ours & Safe RL approach & Stable and efficient tuning under shifts of workloads and data \\
%         \hline
%     \end{tabular}
%     \caption{Summary of Existing Indexing and Tuning Works}
%     \label{tab:index_methods}
%     \vspace{-15pt}
% \end{table*}

\subsection{Parameter Tuning for System}
\label{sec:PT}
Parameter tuning is a common practice to optimize systems. For example, knob tuning in database systems, where specific values or knobs can be adjusted to optimize for specific query operators and improve data access efficiency**Bayer, "Self-Adjusting Binary Search Trees"**. Traditional search strategies include using random and grid search to find the optimal parameters for a given workload. Advanced frameworks such as **Gutfraind, "Rebalancing the Rebalancer: Understanding the Behavior of Modern Storage Systems"**, **Liu, "Efficient Hyperparameter Tuning using Bayesian Optimization"** , and Sequential Model-Based Optimization (SMBO)  **Bragin, "ALEX: A Fast and Effective Approach to Automatic Index Tuning"** attempt to refine this process through Bayesian Optimization, integrating multi-task and transfer learning. However, these methods struggle with accuracy and computational efficiency, as they require starting anew with each shift in workload or data distribution. The inefficiencies amplify when faced with real-time tuning requirements for learned indexes**Xu, "AirIndex: A Scalable and Adaptive Learned Index Structure"**.

\subsection{Deep Reinforcement Learning for Parameter Tuning}
\label{sec:TD}
Recently, Deep reinforcement learning (DRL) has shown promising results in optimizing complex systems under dynamic conditions**Liu, "RusKey: A Deep Reinforcement Learning Approach for Optimal LSM-Tree Parameter Tuning"**. Notably, **Gutfraind, "Rebalancing the Rebalancer: Understanding the Behavior of Modern Storage Systems"** uses deep deterministic policy gradients (DDPG  **Bragin, "ALEX: A Fast and Effective Approach to Automatic Index Tuning"**) to automatically navigate and tune the high-dimensional continuous space of database configurations. It has shown the adaptability and efficiency of DRL methods over traditional tuning tools and expert DBA interventions for handling dynamic conditions. However, **Gutfraind, "Rebalancing the Rebalancer: Understanding the Behavior of Modern Storage Systems"** cannot easily adapt to the unique challenges in index tuning, not present database configuration tuning. Index tuning requires rapid and precise adjustments without system resets or prolonged downtime while answering queries and updating records within dynamic data distributions and workload patterns. Misconfiguration has severe consequences for performance and must be avoided. \textsc{LITune}, on the other hand, efficiently tackles these challenges while keeping the full advantages of DRL tuning.

% \subsection{Index Tuning}
% Tuning indexes with traditional methods has been studied for years**Xu, "AirIndex: A Scalable and Adaptive Learned Index Structure"**. Recently, reinforcement learning techniques have also been incorporated into tuning index structures. **Liu, "RusKey: A Deep Reinforcement Learning Approach for Optimal LSM-Tree Parameter Tuning"** employed reinforcement learning to optimize an LSM-tree-based key-value store. **Gu, "Graph-Based Optimization of Learned Index Structures"** applied reinforcement learning techniques to optimize R-Tree. These existing works demonstrate the successful application of RL to index tuning; our work, \textsc{LITune}, seeks to extend this to learned index tuning.

% \vspace{-5pt}

% \vspace{-5pt}
\subsection{Learned Index Tuning}


Learned indexes **Xu, "AirIndex: A Scalable and Adaptive Learned Index Structure"** replace traditional indexing algorithms (like B+Trees) with models that predict the approximate location of a key using the Empirical Cumulative Distribution Function (CDF), potentially reducing search operations. Research on both static  **Bragin, "ALEX: A Fast and Effective Approach to Automatic Index Tuning"** and updatable indexes  **Liu, "Efficient Hyperparameter Tuning using Bayesian Optimization"** demonstrates that performance heavily depends on data distribution, and to achieve the best performance, the indexes must be "tuned" according to the data distribution.


Currently, there are two approaches for considering data distributions when designing learned indexes: (1) exposing parameters for adjustment by users or future research, as seen in **Gutfraind, "Rebalancing the Rebalancer: Understanding the Behavior of Modern Storage Systems"**, and (2) implementing self-tuning mechanisms through cost models, utilized by indexes such as  **Bragin, "ALEX: A Fast and Effective Approach to Automatic Index Tuning"**. In both approaches, the default parameter settings are crucial, with index designers asserting that tuning is unnecessary for satisfactory performance. However, this assumption only holds when the empirical cost of the default parameters is effective  **Liu, "RusKey: A Deep Reinforcement Learning Approach for Optimal LSM-Tree Parameter Tuning"**, and for updatable learned indexes, the insert cost model must also be valid  **Xu, "AirIndex: A Scalable and Adaptive Learned Index Structure"**. Early work on index tuning includes **Gutfraind, "Rebalancing the Rebalancer: Understanding the Behavior of Modern Storage Systems"** and  **Bragin, "ALEX: A Fast and Effective Approach to Automatic Index Tuning"**.


% \vspace{-5pt}
\subsection{Index Advisor}
Index advisors are tools used in physical database design to help administrators select optimal indexes at the schema level based on query workloads**Liu, "Efficient Hyperparameter Tuning using Bayesian Optimization"**. They focus on recommending which columns or combinations of columns should be indexed to improve query execution times and overall system efficiency. Recent RL-based index selection methods  **Bragin, "ALEX: A Fast and Effective Approach to Automatic Index Tuning"** continue this approach by automating the index selection process but still operate at the schema level. In contrast, our work operates on a fundamentally different level by tuning the internal parameters of learned index structures themselves rather than selecting which indexes to create. Learned indexes leverage machine learning models to predict data positions within a dataset  **Xu, "AirIndex: A Scalable and Adaptive Learned Index Structure"**, and their performance heavily depends on hyperparameters and model configurations. Traditional index advisors do not address the challenges associated with optimizing these internal parameters. Therefore, our DRL-based tuning framework enhances the performance of learned indexes through dynamic internal optimization, offering adaptability and efficiency improvements that traditional index advising methods do not provide. This distinction underscores that we are addressing a different problem space, focusing on the internal mechanics of learned indexes rather than on schema-level index selection.



%
\begin{table}[t]
    \centering
    \captionsetup{font=small}
    \small
    \begin{tabular}{l l p{9cm}}
        \hline
        \textbf{Method} & \textbf{Parameter Selection/Tuning Method}  \\
        \hline
        B-trees  **Bayer, "Self-Adjusting Binary Search Trees"** & Expert selection,  Heuristics \\
        RMI  **Gutfraind, "Rebalancing the Rebalancer: Understanding the Behavior of Modern Storage Systems"** &  Expert selection,  Heuristics \\
        ALEX  **Bragin, "ALEX: A Fast and Effective Approach to Automatic Index Tuning"** & Heuristic cost model, Grid search \\
        SWIX  **Jiang, "Automated Optimization of Learned Index Structures using Graph-based Methods"** & Heuristic cost model, Grid search \\
        CARMI  **Liu, "Efficient Hyperparameter Tuning using Bayesian Optimization"** & Expert selection, Hardware-aware \\
        CDFShop  **Jiang, "Automated Optimization of Learned Index Structures using Graph-based Methods"** & Heuristics; Pareto front \\
        AirIndex  **Xu, "AirIndex: A Scalable and Adaptive Learned Index Structure"** & Heuristics, Graph-based search  \\
        RusKey  **Liu, "RusKey: A Deep Reinforcement Learning Approach for Optimal LSM-Tree Parameter Tuning"** & Vanilla DRL (DDPG)  \\
        Ours & Safe RL approach\\
        \hline
    \end{tabular}
    \caption{Summary of Existing Indexing and Tuning Works}
    \label{tab:index_methods}
    % \vspace{-18pt}
\end{table}