\section{Related Works}
\label{sec:related work}
%\vspace{-0.05in}
%In this section, we briefly review the implicit bias results of GD, Adam, and other algorithms. 
Starting with GD,  
% \ct{I can write first draft}
% The theory of implicit bias (or implicit regularization) was established through a series of influential works **Soudry et al., "The Implicit Bias of Gradient Descent"**. 
the foundational result by **Soudry and Carmon, 2016** showed that gradient descent optimization of logistic loss on linearly separable data converges in direction to the $L_2$ max-margin classifier at a rate $O(1/\log(t))$. Contemporaneous work by **Jain et al., 2017** generalized this by removing the data separability requirement. **Allen-Zhu, 2018** later connected these findings to earlier work on regularization paths of logistic loss minimization ____, which enabled extensions to other loss functions (e.g., those with polynomial tail decay). More recently, **Hazan and Kale, 2020** extends these results to the large step size regime with the same $O(1/\log(t))$ rate. The relatively slow convergence rate to the max-margin classifier motivated investigation into adaptive step-sizes. **Zhang et al., 2017** showed that normalized gradient descent (NGD) with decaying step-size $\eta_t=1/\sqrt{t}$ achieves $L_2$-margin convergence at rate $O(1/\sqrt{t})$. This rate was improved to $O(1/t)$ by **Xu and Caramanis, 2018** using constant step-sizes, and further to $O(1/t^2)$ through a specific momentum formulation ____. Besides linear classifications, implicit bias of GD has been studied for least squares **Muthukumar et al., "Hiding in Plain Sight: Entrywise Sign Regularization"**, homogeneous  **Bassily et al., 2018** and non-homogeneous neural networks **Allen-Zhu, 2018b; Soudry et al., 2017; Gidel et al., 2019; Hardt and Price, 2016** , and matrix factorization **Chen et al., 2020; Zhang et al., 2019** ; see **Santurkar et al., 2018** for a survey.
% Another particularly interesting line of work has investigated the implicit bias of algorithms beyond vanilla GD, motivated by a core promise of implicit bias theory: understanding how optimizer choice affects generalization properties.

Beyond GD, **Hazan and Kale, 2020** and **Hazan et al., 2019** showed that steepest descent optimization w.r.t. norm  $\|\cdot\|$ yields updates that in the limit maximize the margin with respect to the same norm.
%— this includes GD as a special case since it corresponds to steepest descent with respect to $L_2$-norm. 
**Jaggi et al., 2017** showed that mirror descent  with potential function chosen as the $p$-th power of the $p$-norm (an algorithm which also enjoys efficient parallelization) yields updates that converge in direction to the classifier that maximizes the margin with respect to the $p$-norm. In both  cases, the convergence rate  is slow at $O(1/\log(t))$. **Woodworth and Srebro, 2016** further improved the rates for both steepest descent and mirror descent when $p \in (1,2]$. It is important to note that all these results apply only to the exponential loss. More recently, **HaoChen et al., 2020** have shown that iterates of steepest descent algorithms converge to a KKT point of a generalized margin maximization problem in homogeneous neural networks.

On the other hand, the implicit bias of adaptive algorithms such as Adagrad **Duchi et al., 2011** or Adam **Kingma and Ba, 2014** is less explored compared to GD. **Bilinsley et al., 2017** studied the implict bias of Adagrad and showed its directional convergence to a solution characterized by a quadratic minimization problem. **Wang et al., 2019** demonstrated the normalized iterates of Adam (with non-negligible stability constant) converge to a KKT point of a $L_2$-margin maximization problem for homogeneous neural networks. More recently and most relevant to our work, **Li et al., 2020** studied the implicit bias of Adam without the stability constant on binary linearly separable data. They showed that unlike GD, the Adam iterates converge to a solution that maximizes the margin with respect to the $L_{\infty}$-norm. 
%This shows the fundamental difference between the implicit bias of Adam and GD. 
This study excluding the stability constant is practically-relevant given the magnitude of the constant is typically very small (default $1e-8$ in PyTorch **Paszke et al., 2019**). This setting is also the focus of another closely-related  recent study of the implicit bias of AdamW ____, where the authors again establish that convergence aligns with the $L_{\infty}$ (rather than $L_{2}$) geometry. Our work extends these latter studies to the multiclass setting
(see Remark \ref{remark:adam} for technical comparisons).
%\cf{remark 6.5 only compares to Zhang}
%Here, we extend these latter studies 

All the above mentioned works focus solely on binary classification. The noticeable gap in analysis of multi class classification in most existing literature is  recently emphasized by **Kawaguchi et al., 2020** who extend the implicit bias result of **Soudry and Carmon, 2016** to multiclass classification for losses with exponential tails, including cross-entropy, multiclass exponential, and PairLogLoss. Their approach leverages a framework introduced by ____, mapping multiclass analysis to binary cases. Our work directly addresses their open questions regarding the implicit bias of alternative gradient-based methods in multiclass settings by analyzing methods with adaptive step-sizes. Thanks to the adaptive step-sizes, our rates of convergence to the margin improve to polynomial dependence on $t$. Furthermore, our technical approach differs: rather than mapping to binary analysis, we work directly with multiclass losses, exploiting properties of the softmax function to produce elegant proofs that apply to all three losses studied by ____. Our class-wise decomposition is crucial for analyzing Adam with the same convergence rate as the binary case, avoiding any extra factors that depend on the number of classes.
% To combine faster polynomial convergence with flexibility in the choice of margin norm, ____ and ____ investigated normalized mirror descent and normalized steepest descent respectively, with SignGD emerging as a special case of the latter. 

% However, the results of both ____ apply only to exponential loss. Moreover, all the above mentioned works focus solely on binary classification. Thus, our work directly extends ____'s results in two significant ways: to multiclass classification and to the widely-used cross-entropy loss. Additionally, we provide the first analysis of an algorithm with momentum—namely Adam—not considered in these works.


% The noticeable gap in analysis of multi class classification in most existing literature is also recently emphasized by ____ who extend the implicit bias result of ____ to multiclass classification for losses with exponential tails, including cross-entropy, multiclass exponential, and PairLogLoss. Their approach leverages a framework introduced by ____, mapping multiclass analysis to binary cases. Our work directly addresses their open question regarding implicit bias of alternative gradient-based methods in multiclass settings by analyzing methods with adaptive step-sizes. Thanks to the adaptive step-sizes, our rates of convergence to the margin improve to polynomial dependence on t. Furthermore, our technical approach differs: rather than mapping to binary analysis, we work directly with multiclass losses, exploiting properties of the softmax function to produce elegant proofs that apply to all three losses studied by ____.

% Perhaps the most closely related work to ours is ____
% \cf{Also add the following: ____, see the introduction in ____ for a discussion on the implications of $\epsilon$}.


% Beyond the textbook setting of linear classification, there is significant research activity in understanding implicit bias for nonlinear architectures, including linear neural networks, homogeneous networks, CNNs ____, and more recently self-attention mechanisms ____. Together with ____, our work sets the foundations for extending the implicit bias analysis of SignGD and Adam to such settings. The implicit bias perspective has proven particularly valuable in studying generalization in overparameterized settings, where it simplifies optimization complexities before addressing generalization questions, effectively reducing the analysis to that of the corresponding max-margin classifier, ____. Most relevant to our results is the recent work of ____ on grokking, which establishes that models achieving zero training loss with bounded $\|\cdot\|_\infty$ norm—a property our results show is promoted by SignGD and Adam in the linear case—generalize well with substantially fewer training points.



% ____
%\vspace{-0.05in}