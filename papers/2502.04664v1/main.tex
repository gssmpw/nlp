%%%%%%%% ICML 2025 EXAMPLE LATEX SUBMISSION FILE %%%%%%%%%%%%%%%%%

\documentclass{article}

% Recommended, but optional, packages for figures and better typesetting:
\usepackage{microtype}
\usepackage{graphicx}
\usepackage{subfigure}
\usepackage{booktabs} % for professional tables

% hyperref makes hyperlinks in the resulting PDF.
% If your build breaks (sometimes temporarily if a hyperlink spans a page)
% please comment out the following usepackage line and replace
% \usepackage{icml2025} with \usepackage[nohyperref]{icml2025} above.
\usepackage{hyperref}



% Attempt to make hyperref and algorithmic work together better:
\newcommand{\theHalgorithm}{\arabic{algorithm}}

% Use the following line for the initial blind version submitted for review:
%\usepackage{icml2025}

% If accepted, instead use the following line for the camera-ready submission:
\usepackage[accepted]{icml2025}

% For theorems and such
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{amsthm}
%\usepackage{caption}
%\usepackage{subcaption}
% if you use cleveref..
%\usepackage{subfigure}

\usepackage[capitalize,noabbrev]{cleveref}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% THEOREMS
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\theoremstyle{plain}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{assumption}[theorem]{Assumption}
\theoremstyle{remark}
\newtheorem{remark}[theorem]{Remark}

% Todonotes is useful during development; simply uncomment the next line
%    and comment out the line below the next line to turn off comments
%\usepackage[disable,textsize=tiny]{todonotes}
\usepackage[textsize=tiny]{todonotes}


% The \icmltitle you define below is probably too long as a header.
% Therefore, a short form for the running title is supplied here:
\icmltitlerunning{Implicit Bias of SignGD and Adam on Multiclass Separable Data}

\input{commands}

\begin{document}

\twocolumn[
\icmltitle{Implicit Bias of SignGD and Adam on Multiclass Separable Data}

% It is OKAY to include author information, even for blind
% submissions: the style file will automatically remove it for you
% unless you've provided the [accepted] option to the icml2025
% package.

% List of affiliations: The first argument should be a (short)
% identifier you will use later to specify author affiliations
% Academic affiliations should list Department, University, City, Region, Country
% Industry affiliations should list Company, City, Region, Country

% You can specify symbols, otherwise they are numbered in order.
% Ideally, you should not use this facility. Affiliations will be numbered
% in order of appearance and this is the preferred way.
\icmlsetsymbol{equal}{*}

\begin{icmlauthorlist}
\icmlauthor{Chen Fan}{ubc}
\icmlauthor{Mark Schmidt}{ubc,cifar}
\icmlauthor{Christos Thrampoulidis}{ubc}
% \icmlauthor{Firstname3 Lastname3}{comp}
% \icmlauthor{Firstname4 Lastname4}{sch}
% \icmlauthor{Firstname5 Lastname5}{yyy}
% \icmlauthor{Firstname6 Lastname6}{sch,yyy,comp}
% \icmlauthor{Firstname7 Lastname7}{comp}
% %\icmlauthor{}{sch}
% \icmlauthor{Firstname8 Lastname8}{sch}
% \icmlauthor{Firstname8 Lastname8}{yyy,comp}
%\icmlauthor{}{sch}
%\icmlauthor{}{sch}
\end{icmlauthorlist}

\icmlaffiliation{ubc}{University of British Columbia}
\icmlaffiliation{cifar}{Canada CIFAR AI Chair (Amii)}

\icmlcorrespondingauthor{Chen Fan}{fanchen3@outlook.com}
\icmlcorrespondingauthor{Christos Thrampoulidis}{cthrampo@ece.ubc.ca}

% You may provide any keywords that you
% find helpful for describing your paper; these are used to populate
% the "keywords" metadata in the PDF but will not be shown in the document
\icmlkeywords{Machine Learning, ICML}

\vskip 0.3in
]

% this must go after the closing bracket ] following \twocolumn[ ...

% This command actually creates the footnote in the first column
% listing the affiliations and the copyright notice.
% The command takes one argument, which is text to display at the start of the footnote.
% The \icmlEqualContribution command is standard text for equal contribution.
% Remove it (just {}) if you do not need this facility.

%\printAffiliationsAndNotice{}  % leave blank if no need to mention equal contribution
% \printAffiliationsAndNotice{\icmlEqualContribution} % otherwise use the standard text.
\printAffiliationsAndNotice{}

\begin{abstract}
In the optimization of overparameterized models, different gradient-based methods can achieve zero training error yet converge to distinctly different solutions inducing different generalization properties. While a decade of research on implicit optimization bias has illuminated this phenomenon in various settings, even the foundational case of linear classification with separable data still has important open questions. We resolve a fundamental gap by characterizing the implicit bias of both Adam and Sign Gradient Descent in multi-class cross-entropy minimization: we prove that their iterates converge to solutions that maximize the margin with respect to the classifier matrix's max-norm and characterize the rate of convergence. We extend our results to general p-norm normalized steepest descent algorithms and to other multi-class losses.
\end{abstract}

%\vspace{-0.12in}
\section{Introduction}
%\vspace{-0.05in}
Machine learning models are trained to minimize a surrogate loss function with the goal of finding model-weight configurations that generalize well. The loss function used for training is typically a proxy for the evaluation metric. The prototypical example is one-hot classification where the predominant choice, the cross-entropy (CE) loss, serves as a convex surrogate of the zero-one metric that measures correct class membership. Intuitively, a good surrogate loss is one where weight configurations minimizing the training loss also achieve strong generalization performance. However, modern machine learning models are overparameterized, leading to multiple weight configurations that achieve identical training loss but exhibit markedly different generalization properties \citep{zhang2017understanding, belkin2019reconciling}.

This observation has motivated extensive research into the \emph{implicit bias} (or implicit regularization) of gradient-based optimization. This theory investigates how optimizers select specific solutions from the infinite set of possible minimizers in overparameterized settings. The key insight is that gradient-based methods inherently prefer ``simple'' solutions according to optimizer-specific notions of simplicity. Understanding this preference requires analyzing not just loss convergence, but the geometric trajectory of parameter updates throughout training.

The prototypical example—which has rightfully earned its place as a ``textbook'' setting in the field—is the study of implicit optimization bias of gradient descent in linear classification. In this setting, embeddings of the training data are assumed fixed and only the classifier is learned, with overparameterization modeled through linear separability of the training data. \cite{soudry2018implicit} demonstrated that while the GD parameters diverge in norm, they converge in direction to the maximum-margin classifier—the minimum $L_2$-norm solution that separates the data. This non-trivial result, despite (or perhaps because of) the simplicity of its setting, spurred numerous research directions extending to nonlinear models, alternative optimizers, different loss functions, and theoretical explanations of phenomena like benign overfitting (see Sec. \ref{sec:related work}).

Our paper contributes a fundamental result to this literature by establishing the implicit bias of Adam and its simpler variant, signed GD (SignGD), in the multiclass version of this textbook setting: minimization of CE loss on multiclass linearly separable data. Our work addresses several  gaps between theory assumption and practice: the predominant focus on binary classification despite multiclass problems dominating real-world applications; the much theoretical emphasis on exponential loss despite practitioners' overwhelming preference for cross-entropy loss; and the limited theoretical understanding of Adam's implicit bias, which has only recently been studied in the binary case 
\citep{zhang2024implicit} despite its widespread adoption in deep learning practice. Here, we provide a direct analysis of the multiclass setting by exploiting the nice properties of the softmax function, avoiding the traditional approach of reducing multiclass problems to binary ones \citep{wang2024unified}.

\noindent\textbf{Contributions.} Our contributions are  as follows:
% \begin{enumerate}[leftmargin=*]
    % \item For multiclass separable data trained with cross-entropy loss, we prove that the iterates of Adam converge to a solution that maximizes the margin defined with respect to matrix one-norm. The convergence rate is $\mathcal{O}(\frac{}{})$, matching that of the   binary setting \cite{zhang2024implicit}. 
    % \item We extend the implicit bias result of Adam in the binary setting \cite{zhang2024implicit} to the multiclass separable data trained with cross-entropy loss. Specifically, 
     For multiclass separable data trained with CE loss, we show that the iterates of SignGD converge to a solution that maximizes the margin defined with respect to (w.r.t.) the matrix max-norm, with a rate $\mathcal{O}(\frac{1}{t^{1/2}})$.
    We generalize this result to normalized steepest descent algorithms w.r.t. any entry-wise matrix $p$-norm.
    This directly extends \citet{nacson2019convergence,sun2023unified}'s results to multiclass classification and to the widely-used CE loss. To achieve this, we construct a proxy for the loss and show that it closely traces its value and gradient. We also show the same machinery applies to other multiclass losses such as the exponential loss \citep{mukherjee2010theory} and the PairLogLoss \citep{wang2021rank4class}. 
    
     Under the same setting, we prove that the iterates of Adam also maximize the margin w.r.t. the matrix max-norm, with a rate $\mathcal{O}(\frac{1}{t^{1/3}})$. This matches the convergence rate previously established for binary classification \citep{zhang2024implicit}, and remarkably, is independent of the number of classes $k$. This class-independence is non-trivial since naive reduction approaches that map a $k$-class problem in $d$ dimensions to a binary problem in $kd$ dimensions inevitably introduce $k$-dependent factors. Our key insight is to decompose the CE proxy function class-wise, enabling us to bound Adam's first and second gradient moments separately for each class.This decomposition reveals how each component of the proxy function governs the dynamics of the weight vectors associated with its corresponding class.
    %This rate matches that of the binary setting \cite{zhang2024implicit}. In particular, it does \emph{not} depend on the number of classes $k$ (a dependence that is unavoidable if one works directly by mapping the multiclass problem of $k$ classes in $d$-dimensions to a binary one over $kd$ dimensions). To achieve this, we carefully decompose the CE proxy function into a sum over classes, and apply the decomposed proxy functions to bound the first and second gradient moments of Adam class-wise. In essence, each decomposed proxy function governs the dynamics of the weights whose coordinates are in the class associated with the proxy function. 
    % Given the popularity of Adam in transformer training \cite{zhang2020adaptive}, and the recent empirical observations that class imbalances may cause the performance gap between Adam and GD on language modeling where the number of classes can be large \cite{kunstner2024heavy}, it is very important to understand the implicit bias of Adam beyond $2$ classes. Our work takes a firm step in this direction, by establishing the first max-norm convergence result of Adam for multiclass separable data. 
     Finally, we experimentally verify our theory predictions for the algorithms considered. Specific to Adam, we numerically demonstrate that the implicit bias results are consistent with or without the (small) stability constant in the multiclass setting, and the solutions found by SignGD and Adam  favor the max-norm margin over the 2-norm margin.  
 
%\vspace{-0.05in}
\section{Related Works}\label{sec:related work}
%\vspace{-0.05in}
%In this section, we briefly review the implicit bias results of GD, Adam, and other algorithms. 
Starting with GD,  
% \ct{I can write first draft}
% The theory of implicit bias (or implicit regularization) was established through a series of influential works \cite{gunasekar2017implicit,gunasekar2018characterizing,soudry2018implicit}. 
the foundational result by \cite{soudry2018implicit} showed that gradient descent optimization of logistic loss on linearly separable data converges in direction to the $L_2$ max-margin classifier at a rate $O(1/\log(t))$. Contemporaneous work by \cite{ji2019implicit} generalized this by removing the data separability requirement. \cite{ji2020gradient} later connected these findings to earlier work on regularization paths of logistic loss minimization \citep{Rosset2003MarginML}, which enabled extensions to other loss functions (e.g., those with polynomial tail decay). More recently, \cite{wu2024implicit} extends these results to the large step size regime with the same $O(1/\log(t))$ rate. The relatively slow convergence rate to the max-margin classifier motivated investigation into adaptive step-sizes. \cite{nacson2019convergence} showed that normalized gradient descent (NGD) with decaying step-size $\eta_t=1/\sqrt{t}$ achieves $L_2$-margin convergence at rate $O(1/\sqrt{t})$. This rate was improved to $O(1/t)$ by \cite{ji2021characterizing} using constant step-sizes, and further to $O(1/t^2)$ through a specific momentum formulation \citep{ji2021fast}. Besides linear classifications, implicit bias of GD has been studied for least squares \citep{gunasekar2017implicit,gunasekar2018characterizing}, homogeneous  \citep{lyu2019gradient,ji2020directional} and non-homogeneous neural networks \citep{wu2024large}, and matrix factorization \citep{gunasekar2017implicit}; see \cite{vardi2023implicit} for a survey.
% Another particularly interesting line of work has investigated the implicit bias of algorithms beyond vanilla GD, motivated by a core promise of implicit bias theory: understanding how optimizer choice affects generalization properties.

Beyond GD, \cite{gunasekar2018characterizing} and \cite{nacson2019convergence} showed that steepest descent optimization w.r.t. norm  $\|\cdot\|$ yields updates that in the limit maximize the margin with respect to the same norm.
%— this includes GD as a special case since it corresponds to steepest descent with respect to $L_2$-norm. 
\cite{sun2022mirror} showed that mirror descent  with potential function chosen as the $p$-th power of the $p$-norm (an algorithm which also enjoys efficient parallelization) yields updates that converge in direction to the classifier that maximizes the margin with respect to the $p$-norm. In both  cases, the convergence rate  is slow at $O(1/\log(t))$. \citet{wang2023faster} further improved the rates for both steepest descent and mirror descent when $p \in (1,2]$. It is important to note that all these results apply only to the exponential loss. More recently, \citet{tsilivis2024flavors} have shown that iterates of steepest descent algorithms converge to a KKT point of a generalized margin maximization problem in homogeneous neural networks.

On the other hand, the implicit bias of adaptive algorithms such as Adagrad \citep{duchi2011adaptive} or Adam \citep{kingma2014adam} is less explored compared to GD. \cite{qian2019implicit} studied the implict bias of Adagrad and showed its directional convergence to a solution characterized by a quadratic minimization problem. \cite{wang2021implicit,wang2022does} demonstrated the normalized iterates of Adam (with non-negligible stability constant) converge to a KKT point of a $L_2$-margin maximization problem for homogeneous neural networks. More recently and most relevant to our work, \cite{zhang2024implicit} studied the implicit bias of Adam without the stability constant on binary linearly separable data. They showed that unlike GD, the Adam iterates converge to a solution that maximizes the margin with respect to the $L_{\infty}$-norm. 
%This shows the fundamental difference between the implicit bias of Adam and GD. 
This study excluding the stability constant is practically-relevant given the magnitude of the constant is typically very small (default $1e-8$ in PyTorch \citep{pytorch}). This setting is also the focus of another closely-related  recent study of the implicit bias of AdamW \citep{xie2024implicit}, where the authors again establish that convergence aligns with the $L_{\infty}$ (rather than $L_{2}$) geometry. Our work extends these latter studies to the multiclass setting
(see Remark \ref{remark:adam} for technical comparisons).
%\cf{remark 6.5 only compares to Zhang}
%Here, we extend these latter studies 

All the above mentioned works focus solely on binary classification. The noticeable gap in analysis of multi class classification in most existing literature is  recently emphasized by \cite{ravi2024implicit} who extend the implicit bias result of \cite{soudry2018implicit} to multiclass classification for losses with exponential tails, including cross-entropy, multiclass exponential, and PairLogLoss. Their approach leverages a framework introduced by \cite{wang2024unified}, mapping multiclass analysis to binary cases. Our work directly addresses their open questions regarding the implicit bias of alternative gradient-based methods in multiclass settings by analyzing methods with adaptive step-sizes. Thanks to the adaptive step-sizes, our rates of convergence to the margin improve to polynomial dependence on $t$. Furthermore, our technical approach differs: rather than mapping to binary analysis, we work directly with multiclass losses, exploiting properties of the softmax function to produce elegant proofs that apply to all three losses studied by \cite{ravi2024implicit}. Our class-wise decomposition is crucial for analyzing Adam with the same convergence rate as the binary case, avoiding any extra factors that depend on the number of classes.
% To combine faster polynomial convergence with flexibility in the choice of margin norm, \cite{sun2023unified} and \cite{nacson2019convergence} investigated normalized mirror descent and normalized steepest descent respectively, with SignGD emerging as a special case of the latter. 

% However, the results of both \cite{nacson2019convergence,sun2023unified} apply only to exponential loss. Moreover, all the above mentioned works focus solely on binary classification. Thus, our work directly extends \citet{nacson2019convergence,sun2023unified}'s results in two significant ways: to multiclass classification and to the widely-used cross-entropy loss. Additionally, we provide the first analysis of an algorithm with momentum—namely Adam—not considered in these works.


% The noticeable gap in analysis of multi class classification in most existing literature is also recently emphasized by \cite{ravi2024implicit} who extend the implicit bias result of \cite{soudry2018implicit} to multiclass classification for losses with exponential tails, including cross-entropy, multiclass exponential, and PairLogLoss. Their approach leverages a framework introduced by \cite{wang2024unified}, mapping multiclass analysis to binary cases. Our work directly addresses their open question regarding implicit bias of alternative gradient-based methods in multiclass settings by analyzing methods with adaptive step-sizes. Thanks to the adaptive step-sizes, our rates of convergence to the margin improve to polynomial dependence on t. Furthermore, our technical approach differs: rather than mapping to binary analysis, we work directly with multiclass losses, exploiting properties of the softmax function to produce elegant proofs that apply to all three losses studied by \cite{ravi2024implicit}.

% Perhaps the most closely related work to ours is \cite{zhang2024implicit}
% \cf{Also add the following: \cite{qian2019implicit, wang2021implicit,wang2022does}, see the introduction in \cite{zhang2024implicit} for a discussion on the implications of $\epsilon$.}



% Beyond the textbook setting of linear classification, there is significant research activity in understanding implicit bias for nonlinear architectures, including linear neural networks, homogeneous networks, CNNs \cite{lyu2020Gradient,ji2020directional,gunasekar2018characterizing,gunasekar2018implicit}, and more recently self-attention mechanisms \cite{tarzanagh2023maxmargin,tarzanagh2023transformers,deora2024implicit}. Together with \cite{other Adam implicit-bias papers}, our work sets the foundations for extending the implicit bias analysis of SignGD and Adam to such settings. The implicit bias perspective has proven particularly valuable in studying generalization in overparameterized settings, where it simplifies optimization complexities before addressing generalization questions, effectively reducing the analysis to that of the corresponding max-margin classifier, \cite{muthukumar2020classification,hastie2019surprises,bartlett2020benign,koehler2021uniform,donhauser2022fast}. Most relevant to our results is the recent work of \cite{mohama} on grokking, which establishes that models achieving zero training loss with bounded $\|\cdot\|_\infty$ norm—a property our results show is promoted by SignGD and Adam in the linear case—generalize well with substantially fewer training points.



% \cite{https://openreview.net/pdf?id=nm40lbbwoR}
%\vspace{-0.05in}
\section{Preliminaries}
%\vspace{-0.05in}
\paragraph{Notations} For any integer $k$, $[k]$ denotes $\{1,\ldots,k\}$. Matrices, vectors, and scalars are denoted by $\A$, $\ab$, and $a$ respectively. For matrix $\A$, we denote its $(i,j)$-th entry as $\A[i,j]$, and for vector $\ab$, its $i$-th entry as $\ab[i]$. We consider entry-wise matrix $p$-norms defined as  $\|\A\|_p=(\sum_{i,j}|\Ab[i,j]|^p)^{1/p}$. Central to our results are: the infinity norm, denoted as $\ninf{\A}=\max_{i,j} |\A[i,j]|$ and called the \textbf{max-norm}, and the \textbf{entry-wise 1-norm}, denoted as $\none{\Ab}=\sum_{i,j}|\A[i,j]|$. For any other entry-wise $p$-norm with $p> 1$, we write $\|\Ab\|$ (dropping subscripts) for simplicity.  We denote by $\|\A\|_*$ the dual-norm with respect to the standard matrix inner product $\inp{\A}{\Bb}=\tr(\A^\top\Bb)$. The entry-wise 1-norm is dual to the max-norm. For vectors, the max-norm is equivalent to the infinity norm, denoted as $\|\ab\|_\infty$, while we denote the $\ell_1$ norm as $\|\ab\|_1$. Let indicator $\delta_{ij}$ such that $\delta_{ij} = 1$ if and only if $i=j$. Denote $\mathbb{S}: \R^{k} \rightarrow \triangle^{k-1}$ the softmax map of $k$-dimensional vectors to the  probability simplex $\triangle^{k-1}$ such that for $\ab\in\R^k$: \[\sft{\ab} = \big[\frac{\exp({\ab[c]})}{\sum_{c \in [k]} \exp(\ab[c])}\big]_{c=1}^k \in \triangle^{k-1}.
\]Let $\sfti{c}{\vb}$ denote the $c$-th entry of $\mathbb{S}(\vb)$. Let $\mathbb{S}'(\ab)=\diag{\mathbb{S}(\ab)}-\mathbb{S}(\ab)\mathbb{S}(\ab)^\top$ denote the softmax gradient, with $\diag{\cdot}$  a diagonal matrix. Finally, let $\{\eb_c\}_{c=1}^k$ be the standard basis vectors of $\R^k$.  

\paragraph{Setup} Consider a multiclass classification problem with training data ${\hb_1, \ldots, \hb_n }$ and labels ${y_1,\ldots,y_n}$. Each datapoint $\hb_i\in\R^d$ is a vector in a $d$-dimensional embedding space (denote data matrix $\Hb = [\hb_1, \ldots, \hb_n]^\top \in \R^{n \times d}$), and each label $y_i\in[k]$ represents one of $k$ classes. We assume each class contains at least one datapoint. The classifier $f_{\W}: \R^d \rightarrow \R$ is a linear model with weight matrix $\W \in \R^{k \times d}$. The model outputs logits $\ellb_i = f_{\W}(\hb_i) = \W \hb_i$ for $i \in [n]$, which are passed through the softmax map to produce class probabilities $\hat{p}(c|\hb_i) = \mathbb{S}_c(\ellb_i)$. We train using empirical risk minimization (ERM):
$
\Lc_\text{ERM}(\W):=-\frac{1}{n}\sum_{i\in[n]}\ell\left({\W\hb_i};y_i\right)\,,  
$
where the loss function $\ell$  takes as input the logits of a datapoint and its label. The predominant choice in classification is the CE loss 
\begin{align}
     &\Lc(\W):=-\frac{1}{n}\sum\nolimits_{i\in[n]}\log\big(\sfti{\yi}{\W\hb_i}\big)\label{eq:loss} 
     \\
     &= \frac{1}{n}\sum\nolimits_{i\in[n]}\log\big(1+\sum\nolimits_{c\neq y_i}\exp(-(\eb_\yi-\eb_c)^\top\W\hb_i)\big)\,,\nn
\end{align} 
% where recall that $\eb_c$ denotes the $c$-th basis vector in $\R^k$.
 We focus our discussions on the CE loss due to its ubiquity in practice. However, our results hold for other multiclass losses such as the 
exponential \citep{mukherjee2010theory} and the PairLogLoss \citep{wang2021rank4class} (see App. \ref{sec:app_other_losses}).
% While, as we will show, our results extend to other multiclass functions such as the multiclass exponential loss \cite{mukherjee2010theory}  and the PairLogLoss \cite{wang2021rank4class}, we focus on CE loss due to its ubiquity in practice. \red{In our analysis, we work with the form in Eq. \eqref{eq:loss}, which allows us to leverage nice  properties of the softmax map.}

Define maximum margin of the dataset  w.r.t. norm $\|\cdot\|$ as 
\begin{align}\label{eq:p-margin}
    \gamma_{\|\cdot\|}:= \max_{\| \W \| \leq 1}\,\min_{\substack{i\in[n]\\ c\neq \yi}}\,\left(\eb_\yi-\eb_c\right)^\top\W\hb_i\,,
\end{align}
where recall  $\|\cdot\|$ denotes entrywise $p$-norm. Of special interest to us is the maximum margin with respect to the max-norm. For simplicity, we refer to this as $\gamma:=\max_{\ninf{\W} \leq 1}\,\min_{{i\in[n], c\neq \yi}}\,\left(\eb_\yi-\eb_c\right)^\top\W\hb_i\,.
$

\paragraph{Optimization Methods} To minimize the empirical loss $\Lc_\text{ERM}$, we use Adam without the stability constant, which performs the following coordinate-wise updates for iteration $t \geq 0$ and initialization $\W_0$ \citep{kingma2014adam}:
\begin{subequations}
\begin{align}
    \mathbf{M}_t &= \beta_{1} \mathbf{M}_{t-1} + (1 - \beta_1) \nabla \Lc(\W_t) \label{eq: adam1}\\
    \mathbf{V}_t &= \beta_{2} \mathbf{V}_{t-1} + (1 - \beta_2) \nabla \Lc(\W_t)^2 \label{eq: adam2}\\
    \W_{t+1}&= \W_t - \eta_t \frac{\mathbf{M}_t}{\sqrt{\mathbf{V}_t}}, \label{eq: adam3}
\end{align}
\end{subequations}
where $\mathbf{M}_t$, $\mathbf{V}_t$ are first and second moment estimates of the gradient, with decay rates $\beta_1$ and $\beta_2$. The squaring $(\cdot)^2$ and dividing $\frac{\cdot}{\cdot}$ operations are  applied \emph{entry-wise}. 

% Note that these equations represent Adam with $\epsilon=0$, which approximates the standard implementation where $\epsilon$ is set to a very small value (default $1e-8$ in PyTorch \cite{pytorch}).

In the special case $\beta_1 = \beta_2 = 0$, the updates simplify to:
\begin{align}
    \W_{t+1} &= \W_t - \eta_t \sign {\nabla \Lc(\W_t)}), \label{eq: signGD}
\end{align}
which we recognize as the signed gradient descent (SignGD) algorithm. We will also consider a generalization of SignGD called normalized steepest descent (NSD) \cite{boyd2004convex}  with respect to entry-wise matrix $p$-norm ($p\geq 1$) $\lVert \cdot \rVert$ given by the updates:  
\begin{align}
\W_{t+1} = \W_t - \eta_t \Deltab_t,\qquad\text{where} \nn \\ \Deltab_t:=\arg\max\nolimits_{\|\Deltab\|\leq 1}\inp{\nabla\Lc(\W_t)}{\Deltab}\,. \label{eq:nsd_main}
\end{align}
% Note this reduces to SignGD when the max-norm ($\ninf{\cdot}$) is used (i.e. $p=\infty$) and to NGD when the Frobenious Euclidean-norm  is used (i.e. $p=2$).
{
Note that this reduces to SignGD, Coordinate Descent (e.g.,  \citep{nutini2015coordinate}), or normalized gradient-descent (NGD) when the max-norm (i.e. $p=\infty$), the entry-wise $1$-norm, or the Frobenious Euclidean-norm (i.e. $p=2$) is used, respectively.}

\paragraph{Assumptions} Establishing the implicit bias of the above mentioned gradient-based optimization algorithms, requires the  following assumptions. First, we assume data are linearly separable, ensuring the margin $\gamma_{\|\cdot\|}$ is strictly positive, an assumption routinely used in previous works \citep{soudry2018implicit, ravi2024implicit,soudry2018implicit,gunasekar2018characterizing,nacson2019convergence, wu2024implicit}. 

\begin{assumption} \label{ass:sep}
    There exists $\W \in \R^{k \times d}$ such that $\min_{c \neq y_i} (\e_{y_i} - \e_c)^T \W \hb_i > 0$ for all $i \in [n]$.
\end{assumption} 

The second assumption ensures that all entries of the second moment buffer $\Vb_t$ of Adam are bounded away from $0$ for all $t \geq 0$. Previously used by \citet{zhang2024implicit} in binary classification, this assumption is  satisfied when the data distribution is continuous and non-degenerate. A similar assumption appears in  \cite{xie2024implicit}.% for  AdamW. 

\begin{assumption} \label{ass:adam_init}The Adam initialization satisfies $\nabla \Lc(\W_0)[c,j]^2 \geq \omega$ for all $c \in [k]$ and $j \in [d]$. 
\end{assumption}

In this work, we consider a decay learning rate schedule of the form $
\eta_t = \Theta(\frac{1}{t^a})$, where $a \in (0,1]$. Such schedule has been studied in the convergence and implicit bias of various optimization algorithms (e.g.,  \citep{bottou2018optimization, nacson2019convergence,sun2023unified}) including Adam \citep{huang2021super,zhang2024implicit,xie2024implicit}.  

\begin{assumption} \label{ass:learning_rate_1}The learning rate schedule $\{ \eta_t \}$ is decreasing with respect to $t$ and satisfies the following conditions: $\lim_{t \rightarrow \infty} \eta_t = 0$ and $\sum_{t=0}^{\infty}\eta_t = \infty$.
\end{assumption}

Assumption \ref{ass:learning_rate_2} can be satisfied by the above learning rate for a sufficiently large $t$ shown in \citet[Lemma C.1]{zhang2024implicit}. It is used in our analysis of Adam.
\begin{assumption}\label{ass:learning_rate_2} The learning rate schedule satisfies the following: let $\beta \in (0,1)$ and $c_1 > 0$ be two constants, there exist time $t_0 \in \mathbb{N}_{+}$ and constant $c_2 = c_2(c_1, \beta) >0$ such that $\sum_{s=0}^t \beta^s(e^{c_1 \sum_{\tau = 1}^s \eta_{s - \tau}}-1) \leq c_2 \eta_t$ for all $t \geq t_0$.    
\end{assumption}

Finally, we assume the $1$-norm of the data is bounded. Similar assumptions were used in \cite{ji2019implicit}, \cite{nacson2019convergence}, \cite{wu2024implicit}, and \cite{zhang2024implicit}.
\begin{assumption}\label{ass:data_bound}
    There exists constant $B > 0$ such that $\lVert \hb_i \rVert_{1} \leq B$ for all $i \in [n]$. 
\end{assumption}

\section{$\Gc(\W)$ - A proxy to $\Lc(\W)$}

Analyzing margin convergence begins with studying loss convergence through  second-order Taylor expansion of the CE loss:
\begin{align}
&\Lc(\W+\Deltab) = \Lc(\W) + \inp{\nabla\Lc(\W)}{\Deltab} \nn\\
&\quad+ \frac{1}{2n}\sum\nolimits_{i\in[n]}\hb_i^\top\Deltab^\top\sftd{\W\hb_i}\Deltab\hb_i + o(\|\Deltab\|_F^3), \label{eq:loss taylor main}
\end{align}
where recall that $\sftd{\vb}=\diag{\vb}-\vb\vb^\top$.
% $\W \in \mathbb{R}^{k \times d}$ is the weight matrix, and $\hb_i \in \mathbb{R}^d$ represents the $i$-th input feature vector.
To bound the loss at $\W_{t+1}=\W_t-\eta_t\Deltab_t$, we must bound both terms in \eqref{eq:loss taylor main}. For the NSD updates defined in Eq. \eqref{eq:nsd_main}, the first term evaluates to $-\eta_t \| \nabla\Lc(\W) \|_*$ ($\|\cdot \|_*$ is the dual norm). This leads to two key tasks: (1) Lower-bounding the dual gradient norm. (2) Upper-bounding the second-order term.

For the proof to proceed, these bounds should satisfy two desiderata: (1) They are expressible as the same function of $\W_t$, call it $\Gc(\W)$, up to constants.
(2) The function $\Gc(\W)$ is a good proxy for the loss for small values of the latter. The former helps with combining the terms, while the latter helps with demonstrating descent. Next, we obtain these key bounds for the CE loss by determining the appropriate proxy  $\Gc(\W)$. We focus on the matrix max-norm $\ninf{\cdot}$, which arises naturally in the analysis of SignGD and Adam. 
%For SignGD, this is immediate since $\ninf{\sign{\nabla_W\Lc(\W)}}=1$.
Later, we extend these results to arbitrary $p$-norms $\|\cdot\|$ for $p\geq 1$, establishing the implicit bias of $p$-norm NSD.

% Given the separability assumption \ref{ass:sep}, we define the p-norm margin as
% \begin{align}
%     \gamma:= \max_{\| \W \| \leq 1}\,\min_{\substack{i\in[n]\\ c\neq \yi}}\,\left(\eb_\yi-\eb_c\right)^\top\W\hb_i\,.
% \end{align}

\paragraph{Construction of $\Gc(\W)$} 
% As mentioned, bounding the loss update in \eqref{eq:loss taylor main} boils down to identifying the existence and properties of an appropriate proxy  $\Gc(\W)$ for the loss. 
Before showing our construction for the CE loss, it is insightful to discuss how previous works do this in the binary case with labels $y_{b,i}\in{\pm1}$, classifier vector $\w\in\R^d$ and  binary margin $\gamma_{b} \coloneqq \max_{\lVert \w\rVert \leq 1} \min_{i \in [n]} y_{b,i} \w^\top \hb_i$. For exponential loss, \citet{gunasekar2018characterizing} showed that $\lVert \nabla \Lc(\w) \rVert_{} \geq \gamma_b \Lc(\w)$. For logistic loss $\ell(t)=\log(1+\exp(-t))$, \citet{zhang2024implicit} proved $\lVert \nabla \Lc(\w) \rVert_{1} \geq \gamma_b \Gc(\w)$, where $\Gc(\w) = \frac{1}{n} \sum_{i=1}^n |\ell'(y_{b,i} \w^\top \hb_i)|$ and $\ell'$ is the first-derivative. In both cases, one can take the common form $\Gc_b(\w)=\frac{1}{n} \sum_{i=1}^n |\ell'(y_{b,i} \w^\top \hb_i)|$. The proof relies on showing $\gamma \leq \min_{\rb \in \triangle^{n-1}} \lVert \Hb^T \rb \rVert_{}$ via Fenchel Duality \citep{telgarsky2013margins,gunasekar2018characterizing} and appropriately choosing  $\rb$.

 % Turning now back to the multiclass problem, the key challenge we are immediately faced with is that the loss function is vector-valued and it is not clear how to immediately extend the binary definition of $\Gc(\W)$ and its proof.
 In the multiclass setting, where the loss function is vector-valued, it is unclear how to extend the binary proof or definition of $\Gc(\W)$. To this end, we realize that the key is in the proper manipulation of the gradient inner product $\inp{\Ab}{-\nabla\Lc(\W)}$ (for arbitrary matrix $\Ab\in\R^{k\times d}$). The CE gradient evaluates to $\nabla\Lc(\W)=\frac{1}{n} \sum_{i=1}^n(\eb_{y_i}-\sft{\W\hb_i})\hb_i^\top$ and using the fact that $\sft{\W\hb_i}\in\triangle^{k-1}$, it turns out that we can express (details in Lemma \ref{lem:CE logit loss properties})  \[\inp{\Ab}{-\nabla\Lc(\W)} = \frac{1}{n}\sum_{i\in[n]}\sum_{c\neq y_i}\sfti{c}{\W\hb_i}(\eb_{y_i}-\eb_c)^\top\Ab\hb_i\,.\]
 % \begin{align}
 %  \frac{1}{n}\sum\nolimits_{i\in[n]}\sum\nolimits_{c\neq y_i}\sfti{c}{\W\hb_i}\cdot(\eb_{y_i}-\eb_c)^\top\Ab\hb_i\,,\nn%.\label{eq:key inp}
 % \end{align}
 This  motivates defining $\Gc(\W)$ as:
 \begin{align}\label{eq:G defn}
    \Gc(\W) \coloneqq \frac{1}{n}\sum_{i\in[n]}(1-\sfti{\yi}{\W\hb_i})
    %=\frac{1}{n}\sum_{i\in[n]}(1-\sfti{\yi}{\W\hb_i}),
    \,.
\end{align}
 The following key lemma
, which directly follows from the inner-product calculation above and our definition of $\Gc(\W)$, 
confirms that this is the right choice. For convenience, denote $s_{i c} \coloneqq \sfti{c}{\W \hb_i}$, for $i\in[n],c\in[k]$.
% Previous works have attempted to construct such proxy function $\Gc(\w)$ in the binary setting (where $\w \in \R^d$ and $\gamma_{b} \coloneqq \max_{\lVert \w\rVert \leq 1} \min_{i \in [n]} \e_i^T \X \w$) \cite{gunasekar2018characterizing, nacson2019convergence, zhang2024implicit}. It was shown that $\lVert \nabla \Lc(\w) \rVert_{*} \geq \gamma_b \Lc(\w)$ for exponential loss in \cite{gunasekar2018characterizing}. To also include logistic loss, this result was derived to be $\lVert \nabla \Lc(\w) \rVert_{1} \geq \gamma_b \Gc(\w)$ in \cite{zhang2024implicit} where $\Gc(\w) = \frac{1}{n} \sum_{i=1}^n |l'(\langle \w, \hb_i y_i\rangle)|$. The essence of these results lie in the inequality $\gamma \leq \min_{\rb \in \triangle^{k-1}} \lVert \X^T \rb \rVert_{*}$ (obtained using Fenchel Duality \cite{gunasekar2018characterizing}), and choosing a proper vector from the simplex $\triangle^{k-1}$. However,   in the multiclass setting, there is no simple form of $l'(\cdot)$, and the choice of the vector from the simplex remains non-trivial. 
% To this end, we work with the definition of $\| \nabla \Lc(\W) \|_{*}$ and $\gamma$. We define 
% \begin{align}\label{eq:G defn}
%     \Gc(\W) \coloneqq \frac{1}{n}\sum_{i\in[n]}(1-\sfti{\yi}{\W\hb_i}),
% \end{align}
% and show Lemma \ref{lem:lemma_main_G} holds for any $\W \in \R^{k \times d}$.
\begin{lemma}[Lower bounding the gradient dual-norm] \label{lem:lemma_main_G} 
For any $\W\in\R^{k\times d}$, it holds that $\none{\nabla\Lc(\W)}  \geq \gamma\cdot \Gc(\W) $.
\end{lemma}
\begin{proof} By duality, the above calculated formula for the CE-gradient inner product and the fact that $\sum_{c\in[k]}s_{ic}=1$:
    \begin{align*}
    &\none{\nabla \Lc(\W)}  = \max\nolimits_{\ninf{\A} \leq 1}\, \langle \A, -\nabla \Lc(\W)\rangle \\
    &= \max\nolimits_{\ninf{\A} \leq 1} \,\frac{1}{n}\sum\nolimits_{i\in[n]} \sum\nolimits_{c\neq y_i} s_{ic}\, (\eb_{\yi}-\eb_c)^\top\A\hb_i\\
    &\geq \max_{\ninf{\A} \leq 1} \frac{1}{n} \sum_{i \in [n]} (1 - s_{iy_i})\, \min_{c \neq y_i} (\e_{y_i} - \e_c)^T \A \hb_i \\
    &= \frac{1}{n} \sum_{i \in [n]} (1 - s_{iy_i}) \cdot  \max_{\ninf{\A} \leq 1}\min_{i \in [n],c \neq y_i} (\e_{y_i} - \e_c)^T \A \hb_i\,.
\end{align*}
To finish recall the definitions of $\Gc(\W)$ and of the max-norm margin $\gamma$.
%Then, we use the fact that $\none{\nabla \Lc(\W)} \geq \| \nabla \Lc(\W) \|_{*}$.  
\end{proof}
The lemma completes the first of our two tasks: lower bounding the gradient's dual norm. Importantly, the factor appearing in front of $\Gc(\W)$ is the max-norm margin $\gamma$, which will prove crucial in the forthcoming margin analysis.

% In contrast to the above, the function $\Gc(\W)$ defined in \eqref{eq:G defn} cannot be obtained by taking simple derivatives. In summary, Lemma \ref{lem:lemma_main_G} demonstrates that the constructed function $\Gc(\W)$ solves task (1) above. 

\paragraph{$\Gc(\W)$ and second-order term} We now show how to bound the second-order term in \eqref{eq:loss taylor main}. For this, we establish the following essential lemma.

%using again In this section, we focus on task (2) above and show that the second-order term can be written in terms of the function $\Gc(\W)$ defined in \eqref{eq:G defn} via the help of the following lemma.

\begin{lemma} \label{lem:hessian_bound_main}
     For any $\s\in\Delta^{k-1}$ in the $k$-dimensional simplex, any index $c\in[k]$, and any $\vb\in\R^k$ it holds:
     \[
     \vb^\top\left(\diag{\s}-\s\s^\top\right)\vb \leq 4\,(1-s_c)\,\|\vb\|_\infty^2\, .
     \]
\end{lemma}
\begin{proof}
    By Cauchy-Schwartz, 
    \begin{align*}
        \vb^\top&\left(\diag{\s} -\s \s^\top\right) \vb = \inp{\diag{\s} -\s \s^\top}{\vb\vb^\top} 
        % \\
        % &\leq \none{\diag{\s}-\s\s^\top} \ninf{\vb\vb^\top} 
        \\
        &\leq \,\none{\diag{\s}-\s\s^\top}\,\|\vb\|_\infty^2\,.
    \end{align*}
    Direct calculation yields $\none{\diag{\s}-\s\s^\top} = 2\sum\nolimits_{c\in[k]} s_c(1-s_c)$. The advertised bound then follows by noting the following $\sum_{c\in[k]} s_c(1-s_c)\leq 2(1-s_{c'})$ for any $c'\in[k]$ (verified in Lemma \ref{lem:trivial softmax}).
    % Direct calculation and use of  $\sum_{c\in[k]}s_c=1$ yields:
    % \begin{align*}
    % \none{\diag{\s}-\s\s^\top} &= \sum_{c\in[k]} s_c(1-s_c) + \sum_{c'\neq c \in[k]} s_c s_{c'}
    % \\
    % &=2\sum\nolimits_{c\in[k]} s_c(1-s_c).
    % \end{align*}
% The advertised bound then follows by noting that for any $c'\in[k]$ it holds 
% $\sum_{c\in[k]} s_c(1-s_c)\leq 2(1-s_{c'})$, which can be verified algebraically (see Lemma \ref{lem:trivial softmax} for details).
\end{proof}
To bound the second-order term in Eq. \eqref{eq:loss taylor main}, we can apply the above lemma with $\vb\leftarrow\Deltab\hb_i$ and $c\leftarrow y_i$ and further use the inequality $\|{\Deltab\hb_i}\|_\infty\leq \ninf{\Deltab}\|\hb_i\|_1 \leq B \ninf{\Deltab}$ (the last step invoked Ass. \ref{ass:data_bound}). This yields an upper bound 
\[
2B^2\|{\Deltab}\|_{\max}^2\cdot\frac{1}{n}\sum_{i\in[n]}(1-\sfti{y_i}{\W\hb_i})
\]
for the second-order term in the CE loss expansion in terms of the proxy function $\Gc(\W)$.
% This lemma combined with the following inequality $\|\Deltab\hb\| \leq \|\Deltab\|\|\hb\|_\star$ upper bounds the second-order term in terms of $\Gc(\W)$ provided both $\| \Deltab \|$ and $\| \hb \|_{*}$ terms are bounded by some constants. 
% (as in the case of SignGD and Adam under Assumption \ref{ass:data_bound}). 
Thus, we have fullfilled the first desiderate by finding $\Gc(\W)$ that simultaneously bounds the first and second-order terms  in Eq.  \eqref{eq:loss taylor main}.  

\paragraph{Properties of $\Gc(\W)$} 
% The definition of $\Gc(\W)$ in \eqref{eq:G defn} provides a way to lower bound $1$-norm of the gradient using margin. This is one important factor in the analysis. 
We now show that the function $\Gc(\W)$ in Eq. \eqref{eq:G defn} meets the second desiderata: being a good proxy for the loss $\Lc(\W)$. This is rooted in elementary relationships between $\Gc(\W)$ and $\Lc(\W)$, which play crucial roles in the various parts of the proof. Below, we summarize this key relationships. 

\begin{lemma}[Properties of $\Gc(\W)$ and $\Lc(\W)$]
\label{lem:properties_of_G_L_main}
Let $\W \in \R^{k \times d}$. The following hold: (i)
Under Ass. \ref{ass:data_bound}, $2B \cdot \Gc(\W) \geq \none{\nabla\Lc(\W)} $; (ii) $1\geq \frac{\Gc(\W)}{\Lc(\W)} \geq 1-\frac{n\Lc(\W)}{2} $; (iii) If $\W$ satisfies $\Lc (\W) \leq \frac{\log 2}{n}$ or $\Gc(\W) \leq \frac{1}{2n}$, then $\Lc(\W) \leq 2 \Gc(\W)$.
% \begin{enumerate}[label=(\roman*)]
% \item Under Assumption \ref{ass:data_bound}, $2B \cdot \Gc(\W) \geq \none{\nabla\Lc(\W)} $ 
% \item $1\geq \frac{\Gc(\W)}{\Lc(\W)} \geq 1-\frac{n\Lc(\W)}{2} $
% \item  Suppose that $\W$ satisfies $\Lc (\W) \leq \frac{\log 2}{n}$ or $\Gc(\W) \leq \frac{1}{2n}$, then $\Lc(\W) \leq 2 \Gc(\W)$.
% \end{enumerate}
\end{lemma}
% Lemma \ref{lem:properties_of_G_L_main} (i) extends  Lemma \ref{lem:lemma_main_G} further by establishing a sandwich relationship between $\Gc(\W)$ and the dual-normof the gradient. Besides the gradient, statements (ii) and (iii) demonstrate that $\Gc(\W)$ can be used as an alternative to the loss, given that it always lower bounds $\Lc(\W)$, and can serve as an upper bound for a sufficiently small $\Lc(\W)$ or $\Gc(\W)$. Moreover, we observe that the ratio $\frac{\Gc(\W)}{\Lc(\W)}$ converges to $1$ if the loss (monotonically) decreases, and the rate of convergence depends on the loss decreasing rate. \red{The key property (ii) above might appear algebraically involved at first, but it turns out that the two sides of the sandwich relationship both really boil down to the elementary fact that for all $x>0$: $1-x \geq e^{-x}\leq 1-x + x^2/2$.}
Lemma \ref{lem:properties_of_G_L_main} (i) extends Lemma \ref{lem:lemma_main_G} by establishing a sandwich relationship between $\Gc(\W)$ and the gradient's dual norm. The lemma's statements (ii) and (iii) show that $\Gc(\W)$ can substitute for the loss - it lower bounds $\Lc(\W)$ and serves as an upper bound when either $\Lc(\W)$ or $\Gc(\W)$ is sufficiently small. Specifically, the ratio ${\Gc(\W)}\big/{\Lc(\W)}$ converges to 1 as the loss decreases, with the convergence rate depending on the rate of decrease. 
{The key property (ii) may seem algebraically complex, but it turns out (Lemma \ref{lem:G and L}) that both sides of the sandwich relationship follow from the elementary fact that $\forall x>0: 1-x \leq e^{-x}\leq 1-x + x^2/2$.} 
%Therefore, we conclude that the second desiderate is fulfilled as $\Gc(\W)$ traces both the loss (conditioned on being decreasing) and its gradient closely.  
% Alternatively, we have $\Gc(\W) = \frac{1}{n} \sum_{i\in[n]} \bigl( \sum_{c \in [k], c \neq y_i} \sfti{c}{\W \hb_i} \bigr) \geq \frac{1}{n} \sum_{i\in[n]} \sfti{c(i)}{\W \hb_i}$, where $c(i)$ is in $[k]$ such that $c(i) \neq y_i, \forall i \in [n]$.
% \nocite{langley00}

\section{Implicit Bias of SignGD}
We now leverage our construction of $\Gc(\W)$ to show that the margin of SignGD's iterates converges to max-norm margin. We only highlight the key steps in the proof and defer details to Appendix \ref{sec:app_sign}. 

\paragraph{SignGD Descent} We start by showing a descent property. By applying Lemmas \ref{lem:lemma_main_G} and \ref{lem:hessian_bound_main} to lower and upper bound the first and second order terms in Eq.  \eqref{eq:loss taylor main} yields:
\begin{align*}
    \Lc(\W_{t+1}) &\leq \Lc(\W_t) - \gamma \eta_t \Gc(\W_t) +  \\
    &\quad \quad 2 \eta_t^2 B^2 \Gc(\W_{t}) \sup_{\zeta \in [0,1]} \frac{\Gc(\W_t + \zeta \Deltab_t)}{\Gc(\W_{t})}.
\end{align*}
Algebraic manipulations of the definition of $\Gc(\W)$ allows us to bound the ratio in the right hand side.
% The next lemma comes in handy when dealing with the ratio of $\Gc(\W)$ at two different points. 
\begin{lemma} [Ratio of $\Gc(\W)$] \label{lem:G_ratio_main}
    For any $\psi \in [0,1]$, we have the following: $\frac{\Gc(\W + \psi \triangle \W)}{\Gc(\W)} \leq e^{2 B \psi \ninf{\triangle \W}}$.
\end{lemma}
From this and 
the fact $\ninf{\Deltab_t} \leq \eta_t$ for SignGD, 
% SignGD update \eqref{eq: signGD} (i.e. $\ninf{\Deltab_t}  = \eta_t \ninf{\sign{\nabla \Lc(\W_t)}} \leq \eta_t$), 
we obtain
\begin{align}
    \Lc(\W_{t+1}) &\leq \Lc(\W_{t}) - \gamma \eta_t (1 - \alpha_{s_1}  \eta_t )\Gc(\W_t), \label{eq:sign_descent_main} 
\end{align}
where $\alpha_{s_1} = 2 B^2 e^{2B\eta_0} / \gamma$. Given a decay learning rate of the form $\eta_t = \Theta(\frac{1}{t^a})$, we can conclude that the loss starts to monotonically decrease after some time. 

\paragraph{SignGD Unnormalized Margin} We now use  the descent property in  \eqref{eq:sign_descent_main} to  lower bound the unnormalized margin. An intermediate result towards this is recognizing that sufficiently small loss $\Lc (\W) \leq \frac{\log 2}{n}$ guarantees $\W$ separates the data (Lemma \ref{lem:sep}). The descent property ensures that SignGD iterates will eventually achieve this loss threshold, thereby guaranteeing separability.  The main result of this section, shows that eventually the iterates achieve separability with a substantial margin. 
%Before we present the main result, one important intermediate step is to show that a classifier of low loss separates the data, summarized in  Lemma \ref{lem:sep_main}.
% \begin{lemma} [Separability] \label{lem:sep_main}
%     Suppose that there exists $\W \in \R^{k\times d}$ such that $\Lc (\W) \leq \frac{\log 2}{n}$, then we have
%     $(\eb_{y_i} - \e_c)^T \W \hb_i \geq 0$ for all $i \in [n]$ and for all $c \in [k]$ such that $c \neq y_i$.
% \end{lemma}
%We know the assumption in Lemma \ref{lem:sep_main} can be satisfied given a decreasing loss in \eqref{eq:sign_descent_main}. 
%Next, we state the main result of this section.  
\begin{lemma}[SignGD Unnormalized Margin] \label{lem:sign_unnormalized_margin_main}
    Assume $\tilde{t}$ such that $\Lc(\W_t) \leq \frac{\log 2}{n}, \forall t > \tilde{t}$. Then, the minimum unnormalized margin $\min_{i \in [n], c \neq y_i} (\eb_{y_i} - \eb_c)^T \W_t \hb_i$ of iterates $\W_t, t\geq \tilde{t}$ is lower bounded by ($\alpha_{s_2} = 2B e^{2B\eta_0}$)
    \begin{align}
        %\min_{i \in [n], c \neq y_i} (\eb_{y_i} - \eb_c)^T \W_t \hb_i \geq 
        \gamma \sum_{s=\tilde{t}}^{t-1} \eta_s \frac{\Gc(\W_s)}{\Lc(\W_s)} - \alpha_{s_2} \sum_{s=\tilde{t}}^{t-1} \eta_s^2.\label{eq:unnormalized_signgd_main}
    \end{align} 
\end{lemma}
\begin{proof}
    By exponentiating the unnormalized margin:
    \begin{align*}
        &e^{-\min_{i \in [n], c \neq y_i} (\eb_{y_i} - \eb_c)^T \W_t \hb_i} = \max_{i \in [n]} e^{-\min_{c \neq y_i} (\eb_{y_i} - \eb_c)^T \W_t \hb_i} \\
        &\stackrel{(a)}{\leq} \max_{i \in [n]} \frac{1}{\log 2}  \log \bigl( 1 + e^{-\min_{c \neq y_i} (\eb_{y_i} - \eb_c)^T \W_t \hb_i} \bigr) \\
        &\leq \max_{i \in [n]} \frac{1}{\log 2}  \log(1 + \sum_{c \neq y_i} e^{-(\eb_{y_i} - \eb_c)^T \W_t \hb_i}) \leq \frac{n \Lc(\W_t)}{\log 2} \\
        &\stackrel{(b)}{\leq} \exp \bigl( - \gamma \sum_{s=\tilde{t}}^{t-1} \eta_s \frac{\Gc(\W_s)}{\Lc(\W_s)} + \alpha_{s_2} \sum_{s=\tilde{t}}^{t-1} \eta_s^2 \bigr).
    \end{align*}
    % (a) is by the following: the assumption $\Lc(\W_t) \leq \frac{\log 2}{n}$ implies that $\min_{c \neq y_i} (\eb_{y_i} - \eb_c)^T \W_t \hb_i \geq 0$ for all $i \in [n]$ by Lemma \ref{lem:sep_main}. We also know the inequality $\frac{\log(1 + e^{-z})}{e^{-z}} \geq \log 2$ holds for any $z \geq 0$. Then, for any $i \in [n]$, we can set $z = \min_{c \neq y_i} (\eb_{y_i} - \eb_c)^T \W_t \hb_i$ to obtain the desired inequality; and (b) is by some manipulations of \eqref{eq:sign_descent_main} (see Appendix \ref{sec:app_sign}. for a full proof). Next, we study the ratio $\frac{\Gc(\W)}{\Lc(\W)}$ more carefully. 
    (a) is because $\frac{\log(1 + e^{-z})}{e^{-z}} \geq \log 2, z\geq 0$  with $z$ chosen to be $\min_{c \neq y_i} (\eb_{y_i} - \eb_c)^T \W_t \hb_i$ for any $i \in [n]$; (b) is by some manipulations of \eqref{eq:sign_descent_main} (details in App. \ref{lem:sign_unnormalized_margin}). 
\end{proof}
\paragraph{SignGD Margin Convergence} Proceeding from Eq. \eqref{eq:unnormalized_signgd_main} requires showing convergence of the ratio $\frac{\Gc(\W)}{\Lc(\W)}$. The two key ingredients are given in Lemma \ref{lem:properties_of_G_L_main} (ii) and (iii). Lemma \ref{lem:properties_of_G_L_main} (ii) suggests that it is sufficient to study the convergence of $\Lc(\W)$, which is captured in \eqref{eq:sign_descent_main}. However, to obtain an explicit rate via \eqref{eq:sign_descent_main}, we need to rewrite $\Gc(\W_t)$ in terms of $\Lc(\W_t)$. This is where Lemma \ref{lem:properties_of_G_L_main} (iii)  helps. Putting them together, we arrive at the following theorem. 
\begin{theorem} \label{thm:signgd_main} Suppose that Ass. \ref{ass:sep}, \ref{ass:learning_rate_1}, and \ref{ass:data_bound} hold, then there exists $t_{s_2} = t_{s_2}(n, \gamma, B, \W_0)$ such that the margin gap $\gamma - {\min_{i \in [n], c \neq y_i} (\e_{y_i} - \e_c)^T \W_t \hb_i}/\ninf{\W_t}$ of SignGD's iterates for all $t > t_{s_2}$ is upper bounded by
\begin{align*}
     \mathcal{O}\Big(\frac{\sum_{s=t_{s_2}}^{t-1} \eta_s e^{-\frac{\gamma}{4} \sum_{\tau = t_{s_2}}^{s-1}\eta_{\tau}} + \sum_{s=0}^{t_{s_2}-1}\eta_s + \sum_{s = t_{s_2}}^{t-1}\eta_s^2}{\sum_{s=0}^{t-1} \eta_s}\Big).
\end{align*}
\end{theorem}
\begin{proof}
    SignGD's update rule \eqref{eq: signGD} gives $\ninf{\W_t} \leq \ninf{\W_0} + \sum_{s=0}^{t-1} \eta_s$. We first find iteration $t_{s_1}$ via \eqref{eq:sign_descent_main} such that $    \Lc(\W_{t+1}) \leq \Lc(\W_t) - \frac{\eta_t \gamma}{2} \Gc(\W_t)$ for all $t \geq t_{s_1}$. Then, we find iteration $t_{s_2}>t_{s_1}$ such that $\Lc(\W_t) \leq \frac{\log 2}{n}$ for all $t \geq t_{s_2}$. By Lemma \ref{lem:properties_of_G_L_main} (iii), this guarantees $\Lc(\W_t) \leq 2 \Gc(\W_t)$. Substituting it to the above recursion on $\Lc(\W_t)$, we obtain $\Lc(\W_{t}) \leq \frac{\log 2}{n} e^{-\frac{\gamma}{4} \sum_{s = t_{s_2}}^{t-1} \eta_s}$. By Lemma \ref{lem:properties_of_G_L_main} (ii), we further obtain $\frac{\Gc(\W_t)}{\Lc(\W_t)} \geq  1 - e^{-\frac{\gamma}{4} \sum_{s = t_{s_2}}^{t-1} \eta_s}$. Combining this with \eqref{eq:unnormalized_signgd_main} and the upper bound on $\ninf{\W_t}$, the final result is proved (details in App. \ref{thm:signgd}).
\end{proof}

In the proof, $t_{s_1}$ can be set to $(4 B^2 e^{2B\eta_0} / {\gamma})^{1/a}$, and $t_{s_2}$ is chosen such that $t_{s_2} \leq \Theta(n^{\frac{1}{1-a}} + n^{\frac{1}{1-a}} \Lc(\W_0)^{\frac{1}{1-a}})$. Note that both  are independent of the problem's dimensionality, yielding the following convergence rates. 
\begin{corollary} \label{cor:sign_gd_main}
    Set learning rate $\eta_t = \Theta(\frac{1}{t^a}), a \in (0,1]$. Under the setting of Theorem \ref{thm:signgd_main}, the margin gap of SignGD iterates with $a=1/2$ reduces at rate $\mathcal{O}(\frac{\log t + n}{t^{1/2}})$.\footnote{The rates for other values of $a$ can be found in Corollary \ref{cor:sign_gd}.}
    % we have for SignGD with $a=1/2$ that
    % \begin{align*}
    % |\frac{\min_{i \in [n], c \neq y_i} (\e_{y_i} - \e_c)^T \W_t \hb_i}{\ninf{\W_t}} - \gamma|
    %  \leq \mathcal{O}(\frac{\log t + n}{t^{1/2}}).
   % \left\{
   %  \begin{array}{ll}
   %         \mathcal{O} (\frac{t^{1-2a}+n}{t^{1-a}}) &  \text{if} \quad a < \frac{1}{2}\\
   %          \mathcal{O} (\frac{\log t + n}{t^{1/2}}) &  \text{if}\quad a=\frac{1}{2} \\
   %          \mathcal{O} (\frac{n}{t^{1-a}}) &  \text{if}\quad \frac{1}{2} < a<1 \\
   %          \mathcal{O} (\frac{n}{\log t}) &  \text{if} \quad a=1
   %  \end{array} 
   %  \right.     
    % \end{align*}
    
\end{corollary}
\begin{remark}
These results generalize to NSD defined in \eqref{eq:nsd_main}. Concretely, Lemma \ref{lem:lemma_main_G}, \ref{lem:hessian_bound_main}, and \ref{lem:properties_of_G_L_main} (i) generalize to any $p$-norm (where margin in \eqref{eq:p-margin} is defined w.r.t. the same norm) together with the dual norm on the loss gradient. These results are summarized in Lemmas \ref{lem:G and gradient SD} and \ref{lem:hessian_bound_SD} in App. \ref{sec:app_nsd}. 
%Therefore, the proof of NSD is the same as SignGD with the max-norm and $1$-norm replaced by $p$-norm and its dual norm respectively. 
In terms of margin convergence of NSD, 
\cite{nacson2019convergence} showed a rate of $\mathcal{O}(\frac{\log t}{t^{1/2}})$ in the binary setting, limited to the exponential loss. Compared to this, our results hold for the more practical setting of multilcass data and CE loss. The rate in Cor. \ref{cor:sign_gd_main} surpasses the max-norm margin convergence rate of Adam in the binary setting \citep{zhang2024implicit}; thus even in the binary setting it improves the latter and extends \cite{nacson2019convergence}'s result to logistic loss. We  elaborate  on this gap in the next section.
% We note the rates in Corollary \ref{cor:sign_gd_main} either match (for $a \in [\frac{2}{3},1]$) or surpass (for $a \in (0,\frac{2}{3})$) the max-norm margin convergence results of Adam in the binary setting \cite{zhang2024implicit}. We will elaborate more on this gap in the next section.
% We generalize these results to PairLogLoss loss \cite{wang2021rank4class} in section \ref{sec:other_losses}. 
\end{remark}

% \begin{remark} Besides cross-entropy loss, we can use the same proxy function $\Gc(\W)$ for the exponential loss defined as $\Lcexp(\W):=\frac{1}{n}\sum_{i\in[n]}\sum_{c\neq y_i}\exp\left(-(\eb_{y_i}-\eb_c)^\top\W\hb_i\right)$ \cite{mukherjee2010theory}. For the PairLogLoss $\Lcpll(\W):=\frac{1}{n}\sum_{i\in[n]}\sum_{c\neq y_i}\log \left(1+\exp(-(\eb_{y_i}-\eb_c)^\top\W\hb_i)\right)$ \cite{ravi2024implicit}, we recognize that $\inp{\Ab}{-\nabla\Lcpll(\W)}$ has the following simple form $\frac{1}{n}\sum_{i\in[n]}\sum_{c\neq y_i}|f'\left((\eb_{y_i}-\eb_c)^\top\W\hb_i\right)|\cdot\left(\eb_{y_i}-\eb_c\right)^\top\Ab\hb_i$, from which we directly deduce that $    \Gcpll(\W) \coloneqq \frac{1}{n}\sum_{i\in[n]}\sum_{c\neq y_i}\left|f'\left(-(\eb_{y_i}-\eb_c)^\top\W\hb_i\right)\right|$ can serve as the proxy for the loss. In Appendix \ref{sec:app_other_losses}, we prove analogous results of Lemma \ref{lem:lemma_main_G}, \ref{lem:hessian_bound_main}, and \ref{lem:properties_of_G_L_main} for $\Lc_{pll}(\W)$. Thus, the results of SignGD and NSD can all be extended to these settings following the same approach above. \ct{shall we remove this and instead add a reference to Appendix F when we first talk about these other losses below Eq. (1)?} 
% \begin{align*}
% \frac{1}{n}\sum_{i\in[n]}\sum_{c\neq y_i}|f'\left((\eb_{y_i}-\eb_c)^\top\W\hb_i\right)|\cdot\left(\eb_{y_i}-\eb_c\right)^\top\Ab\hb_i    
% \end{align*} 
% where $f(t):=\log(1+e^{-t})$, from which we can directly deduce that $    \Gcpll(\W) \coloneqq \frac{1}{n}\sum_{i\in[n]}\sum_{c\neq y_i}\left|f'\left(-(\eb_{y_i}-\eb_c)^\top\W\hb_i\right)\right|$. In Appendix \ref{sec:app_other_losses}, we prove analogous results of Lemma \ref{lem:lemma_main_G}, \ref{lem:hessian_bound_main}, and \ref{lem:properties_of_G_L_main} for $\Lc_{pll}(\W)$. Thus, the results of SignGD, NSD, and Adam can all be extended to PairLogLoss       

\section{Implicit Bias of Adam} 
%Our analysis for Adam for  multiclass separable data draws inspirations from the binary setting of \cite{zhang2024implicit}, and takes similar proof steps as SignGD above. We want to emphasize that all the important properties of the function $\Gc(\W)$ discussed above are used in the analysis of Adam. However, using $\Gc(\W)$ along is not sufficient to obtain a tight bound without an extra factor $k$. The heart of the proof lies in the following \textbf{per-class} decomposition:
Our analysis of Adam for multiclass data, while inspired by the binary setting of \cite{zhang2024implicit}, requires significant new technical insights to achieve the same convergence rate without class-dependent factors. While the proof follows similar high-level steps as SignGD above and leverages all the key properties of $\Gc(\W)$ identified in Sec. \ref{sec: G and L}, using $\Gc(\W)$ alone would lead to suboptimal bounds with an extra factor of $k$. Our crucial insight lies in a \textbf{per-class} decomposition of $\Gc(\W)$ as follows:
\begin{align*}
    \Gc(\W) &= \sum_{c \in [k]} \frac{1}{n} \sum_{i \in [n], y_i = c} (1 - s_{iyi}) \\&= \sum_{c \in [k]} \frac{1}{n} \sum_{i \in [n], y_i \neq c} s_{ic},
\end{align*}
 where recall $s_{i c} \coloneqq \sfti{c}{\W \hb_i}$. This decomposition motivates further defining the ``per-class proxies'':
 \begin{align*}
 \Gc_c (\W) &\coloneqq \frac{1}{n} \sum_{i \in [n], y_i = c} (1 - s_{iy_i})
 \\
 \Qc_c(\W) &\coloneqq \frac{1}{n} \sum_{i \in [n], y_i \neq c}  s_{ic}.
 \end{align*}
 Both terms play crucial roles in our per-class analysis, with this decomposition being key to avoiding the factor $k$ in our bounds. The remainder of this section highlights the technical challenges beyond those encountered in analyzing SignGD (full proof details are provided in Appendix \ref{sec:sec_adam}).
 %Importantly, both of them are used when doing the analysis at per-class level, and the above decomposition is central to avoid the factor $k$ showing up in the bound. In this section, we highlight the differences and technical challenges in addition to those faced from analyzing SignGD (proof details are in Appendix \ref{sec:sec_adam}). 

\paragraph{Adam Descent} To show descent of Adam via Taylor expansion \eqref{eq:loss taylor main}, the first-order term involves an additional factor when introducing the dual norm of the gradient, i.e., 
\begin{align}
 &\inp{\nabla\Lc(\W_t)}{\Deltab_t} \leq 
    -\eta_t  \none{\nabla \Lc(\W_t)} \nn \\
    &\qquad+\eta_t \underbrace{\bigm| \langle \nabla \Lc(\W_t), \frac{\M_t}{\sqrt{\Vb_t}} - \frac{\nabla \Lc(\W_t)}{|\nabla \Lc(\W_t)|}\rangle \bigm|}_{\clubsuit}. \label{eq:adam_intermediate_main}
\end{align}
Given that we bound the remaining terms in \eqref{eq:loss taylor main} using $\Gc(\W)$, it seems natural to bound the $\clubsuit$ term similarly. While one could attempt to relate the entries of both matrices $\M_t$ and $\Vb_t$ to $\Gc(\W_t)$, our entry-wise analysis of the $\clubsuit$ term (following \cite{zhang2024implicit}'s approach in the binary setting) would inevitably introduce an extra factor of $k$. To avoid this, we instead relate the row vectors $\M_t[c,:]$ and $\Vb_t[c,:]$ to the class-specific functions $\Gc_c(\W)$ and $\Qc_c(\W)$ for each $c \in [k]$. This class-wise approach requires careful accounting of interactions between the $k>2$ classes throughout our analysis.
%Given the rest terms in \eqref{eq:loss taylor main} are bounded using the function  $\Gc(\W)$, it is natural to consider doing it for the $\clubsuit$ term as well. One feasible approach is to connect the entries of both $\M_t$ and $\Vb_t$ (matrices) to $\Gc(\W_t)$. However, we realize that this would introduce an extra factor of $k$ given our entry-wise analysis of the $\clubsuit$ term, an approach also taken in the binary setting by \cite{zhang2024implicit}. To address this, instead, we connect the entries of the vectors $\M_t[c,:]$ and $\Vb_t[c,:]$ to the functions $\Gc_c(\W)$ and $\Qc_c(\W)$ for each $c \in [k]$. Compared to the binary case, this requires carefully tracking the presence of $k>2$ classes throughout our calculations.
%This part is highly non-trivial considering the gradient of CE loss is more involved than that of the logistic loss in the binary . We have one more dimension (class) to track, and this becomes more challenging when handling $\Vb_t$. We first show the result on $\M_t$. 
We first show how to bound $\M_t$.
\begin{lemma} \label{lem:first_G_main} Let $c \in [k]$. Under the setting of Theorem \ref{thm:adam_main}, there exists time $t_0$ such that  for all $t \geq t_0$: 
\begin{align*}
    |\mathbf{M}_t[c,j] -(1-\beta_{1}^{t+1}) & \nabla \Lc(\W_t)[c,j]|  \leq\\
    & \alpha_M \eta_t \bigl( \Gc_c(\W_t)+ \Qc_c(\W_t) \bigr),
\end{align*}
where $j \in [d]$, and $\alpha_M := B(1-\beta_1)c_2$. 
\end{lemma}
Here, we provide a quick proof sketch. By the Adam update rule \ref{eq: adam1}, we have for all $c \in [k]$ and $j \in [d]$: the following $|\mathbf{M}_t[c,j] - (1-\beta_{1}^{t+1})\nabla \Lc(\W_t)[c,j]|$ can be bounded by 
\begin{align*}
    % &|\mathbf{M}_t[c,j] - (1-\beta_{1}^{t+1})\nabla \Lc(\W_t)[c,j]| \\ 
    &\sum_{\tau=0}^{t} (1-\beta_1) \beta_1^{\tau} \underbrace{|\nabla \Lc (\W_{t -\tau})[c,j] - \nabla \Lc (\W_{t}) [c,j]|}_{\spadesuit}.
\end{align*}
By explicitly writing out the gradient and grouping terms we can show: 
\begin{align*}
    \spadesuit  &\leq B \frac{1}{n} \sum\nolimits_{i \in [n]} |\sfti{c}{\W_{t - \tau}\hb_i} - \sfti{c}{\W_{t}\hb_i}| \\
        &= B \underbrace{\frac{1}{n} \sum\nolimits_{i \in [n] : y_i \neq c} |\sfti{c}{\W_{t - \tau}\hb_i} - \sfti{c}{\W_{t}\hb_i}|}_{\spadesuit_1}  \\ 
        &\quad \quad  +B \underbrace{\frac{1}{n} \sum\nolimits_{i \in [n]: y_i = c} |\sfti{y_i}{\W_{t  - \tau}\hb_i} - \sfti{y_i}{\W_{t}\hb_i}|}_{\spadesuit_2}.
\end{align*}
In the above equality, we split the sum into two cases: samples where $y_i=c$ and the rest. Using the definitions of $\Gc_c(\W)$ and $\Qc_c(\W)$, we manipulate and express the first ($\spadesuit_1$) and second ($\spadesuit_2$) terms  via $\Qc_c(\W)$ and $\Gc_c(\W)$ respectively. This leaves us to bound expressions of the form  $|\sfti{c}{\vb'}/\sfti{c}{\vb}-1|$ and $|(1-\sfti{c}{\vb'})/(1-\sfti{c}{\vb})-1|$ respectively for the two terms. We address this technical challenge in Lemma \ref{lem:unified_helper}, showing how to bound such terms using $\|\vb-\vb'\|_\infty$, which we can control for Adam.
%Finally, we rewrite both $\spadesuit_1$ and $\spadesuit_2$ terms using $\Gc(\W)$ with the help of Lemma \ref{lem:unified_helper} in Appendix \ref{sec:sec_adam}\ct{refering to some lemma in App. seems a bit unsatisfactory here, cause then what's the point of showing the steps until here. Can instead say something like that (attempt above): }.
We now show how to bound $\Vb_t$. 
\begin{lemma} \label{lem:second_G_main} Let $c \in [k]$. Under the setting of Theorem \ref{thm:adam_main}, there exists time $t_0$ such that  for all $t \geq t_0$:
\begin{align*}
    \bigm| \sqrt{\Vb_t[c,j]} - \sqrt{(1 - \beta_2^{t+1})} | & \nabla \Lc(\W_t) [c,j]| \bigm| 
    \leq \\
    & \alpha_V \sqrt{\eta_t} \bigl( \Gc_c(\W_t) + \Qc_c(\W_t) \bigr),
\end{align*}
where $j \in [d]$, and $\alpha_V = B \sqrt{(1-\beta_2) c_2}$. 
\end{lemma}
By Adam's update rule \eqref{eq: adam3}, we have $\forall c \in [k], j \in [d]$ that $|\Vb_t[c,j] - (1 - \beta_2^{t+1}) \nabla \Lc(\W_t) [c,j]^2|$ can be bounded by 
\begin{align*}
        % & |\Vb_t[c,j] - (1 - \beta_2^{t+1}) \nabla \Lc(\W_t) [c,j]^2| \\
        & \sum_{\tau = 0}^t (1-\beta_2) \beta_2^{\tau} \underbrace{|\nabla \Lc (\W_{t-\tau}) [c,j]^2 - \nabla \Lc (\W_t)[c,j]^2|}_{\diamond}. \label{eq: second_G_eq1}
\end{align*}
        For the $\diamond$ term, we compute $\nabla \Lc (\W) [c,j]^2$ and define the function $f_{c,i,p}(\W) := (\delta_{cy_i} - \sfti{c}{\W \hb_i}) (\delta_{cy_p} - \sfti{c}{\W \hb_p})$ to obtain (after some algebra):
\begin{align*}
    &\diamond \leq \frac{B^2}{n^2} \sum\nolimits_{i \in [n]} \sum\nolimits_{p \in [n]}  |f_{c,i,p}(\W_{t-\tau}) - f_{c,i,p}(\W_{t})|. 
    % &= B^2 \underbrace{\frac{1}{n^2} \sum_{i \in [n], y_i \neq c} \sum_{p \in [n], y_p \neq c} |f_{c,i,p}(\W_{t-\tau}) - f_{c,i,p}(\W_{t})|}_{\diamond_1} \\
    % & \quad +   B^2 \underbrace{ \frac{1}{n^2} \sum_{i \in [n], y_i \neq c} \sum_{p \in [n], y_p = c} |f_{c,i,p}(\W_{t-\tau}) - f_{c,i,p}(\W_{t})|}_{\diamond_2} \\ 
    % & \quad +  B^2 \underbrace{ \frac{1}{n^2} \sum_{i \in [n], y_i = c} \sum_{p \in [n], y_p \neq c} |f_{c,i,p}(\W_{t-\tau}) - f_{c,i,p}(\W_{t})|}_{\diamond_3} \\ 
    % & \quad +  B^2 \underbrace{ \frac{1}{n^2} \sum_{i \in [n], y_i = c} \sum_{p \in [n], y_p = c} |f_{c,i,p}(\W_{t-\tau}) - f_{c,i,p}(\W_{t})|}_{\diamond_4}
\end{align*}
Unlike the first moment $\M_t$, this summation involves pairs of sample indices $i$ and $p$ with label $c$ taking values equal to one of four cases: $y_i, \text{non-}y_i, y_p, \text{non-}y_p$. Following our first-moment analysis strategy, we decompose the sum into these four components. The resulting bounds involve squared terms $\Gc_c(\W)^2$, $\Qc_c(\W)^2$, and $\Gc_c(\W) \Qc_c(\W)$ due to the quadratic softmax expressions in each component (see Lemma \ref{lem:unified_helper} for bounding ratios of softmax quadratics).

With Lemmas \ref{lem:first_G_main} and \ref{lem:second_G_main} at hand, we can now bound the Adam-specific term $\clubsuit$ in Eq. \eqref{eq:adam_intermediate_main} in terms of $\Gc(\W)$. The essence is to do the double sum (over $c \in [k]$ and $j \in [d]$) in $\clubsuit$ in two stages: first sum over $d$ for each $c \in [k]$ with the help of Lemma \ref{lem:first_G_main} and \ref{lem:second_G_main}; then sum over $c$ with the recognition of $\Gc(\W) = \sum_{c \in [k]} \Gc_c(\W) = \sum_{c \in [k]} \Qc_c(\W)$. The results are summarized in Lemma \ref{lem:adam_intermediate_bound}. From this point on, following similar steps of SignGD, we can show the CE loss (eventually) monotonically decreases (see Lemma \ref{lem:adam_descent}).   

\begin{figure*}[t!]
\centering     %%% not \center
\subfigure[Loss]{\label{fig:main_loss}\includegraphics[width=33mm]{figs/Loss.pdf}}
\subfigure[$\gamma_{\lVert \cdot \rVert_2} = 2.18$]{\label{fig:max_margin}\includegraphics[width=33mm]{figs/rel_fro_margin.pdf}}
\subfigure[$\gamma_{\lVert \cdot \rVert_{\max}} = 18.69$]{\label{fig:l2_margin}\includegraphics[width=33mm]{figs/rel_max_margin.pdf}}
\subfigure[$\frac{\langle \W_t, \Vb_2\rangle} {\lVert \W_t \rVert_2 \lVert \Vb_2 \rVert_2}$]
{\label{fig:cor_v2}\includegraphics[width=33mm]{figs/cor_v2.pdf}}
\subfigure[$\frac{\langle \W_t, \Vb_{\infty}\rangle} {\lVert \W_t \rVert_2 \lVert \Vb_{\infty} \rVert_2}$]
{\label{fig:cor_inf}\includegraphics[width=33mm]{figs/cor_vinf.pdf}}
\captionsetup{width=\textwidth}
%\vspace{-0.1in}
\caption{Implicit bias on multiclass separable data. \textbf{(a)} Loss vs. iterations: SignGD converges fast. \textbf{(b)}  We normalize the iterates w.r.t. 2-norm (aka Frobenius), compute the margin, then plot its difference to the dataset's max-margin w.r.t. 2-norm (given in captions). Only NGD converges to the max 2-norm margin. \textbf{(c)} Same  as (b) with 2-norm replaced by max-norm. %In this case, we observe that the max-norm convergence of SignGD or Adam is faster than NGD. 
Margins of SignGD/Adam converge to max-margin w.r.t max-norm norm.
\textbf{(d, e)} Correlations between  $\W_t$ and max-margin classifiers $\Vb_2$, $\Vb_{\infty}$ against iterations. SignGD/Adam iterates correlate well with $\Vb_{\infty}$. NGD aligns with $\Vb_2$.}
%\vspace{-0.1in}
\label{fig:main_fig1}
\end{figure*}

\paragraph{Adam Implicit Bias} To establish Adam's implicit bias, we follow the same strategy as SignGD: bound the margin and control weight growth. For the margin bound, we leverage our descent property from above (details in Lemma \ref{lem:adam_unnormalized_margin}). Unlike SignGD's simple updates, Adam's moment averaging makes bounding $\ninf{\W_t}$ more challenging. Our solution again relies on the proxy $\Gc(\W)$, which we have shown approximates both loss and gradients well. Specifically, using this, we prove that the second moment $\Vb_t$ remains controlled when the loss is small, i.e. $\forall c \in [k],j \in [d]$:
\begin{align*}
    \Vb_t[c,j] &\leq \nabla \Lc(\W_t)[c,j]^2 + \alpha_V \eta_t \Gc(\W_t)^2 \\
    &\leq 4 B^2 \Gc(\W_t)^2 + \alpha_V \eta_0 \Gc(\W_t)^2 \\
    &\leq (4 B^2 + \alpha_V \eta_0) \Lc(\W_t)^2,
\end{align*}
where the penultimate and last inequalities are by Lemma \ref{lem:properties_of_G_L_main} (i) and (ii), respectively.
This suggests that with sufficiently small loss, all entries of $\Vb_t$ remain bounded by 1. 
Building on our bound for $\Vb_t$, we apply \citet[Lemma A.4]{zhang2024implicit} (see also \citet[Lemma~4.2]{xie2024implicit}), which connects $\W_t$ and $\log(\Vb_t)$ entry-wise. This allows us to bound $\ninf{\W_t}$ using learning rate sums under Ass. \ref{ass:adam_init} (details in Lemma \ref{lem:adam_weight}). With the convergence $\frac{\Gc(\W)}{\Lc(\W)} \rightarrow 1$ following as in SignGD, we arrive at our main theorem. 
% Given the descent property established, we can follow the same strategy as proving SignGD (i.e., Lemma \ref{lem:sign_unnormalized_margin_main}) to lower bound the unnormalized margin of Adam (Lemma \ref{lem:adam_unnormalized_margin} in Appendix \ref{sec:sec_adam}). Besides this, another piece required for the proof is a bound on $\ninf{\W_{t}}$ (see Theorem \ref{thm:signgd_main} above). In the case of SignGD, the bound can be easily found given its simple form of update. However, Adam involves exponential averaging of both first and second moments of the gradient, which renders the task of bounding $\ninf{\W_t}$ more complex. To solve this, we again rely on the function $\Gc(\W)$ being good approximations of the loss and its gradient. Concretely, by Lemma \ref{lem:second_G_main} and some manipulations, we obtain for all $c \in [k]$ and $j \in [d]$
% \begin{align*}
%     \Vb_t[c,j] &\leq \nabla \Lc(\W_t)[c,j]^2 + \alpha_V \eta_t \Gc(\W_t)^2 \\
%     &\leq 4 B^2 \Gc(\W_t)^2 + \alpha_V \eta_0 \Gc(\W_t)^2 \\
%     &\leq (4 B^2 + \alpha_V \eta_0) \Lc(\W_t)^2,
% \end{align*}
% where the penultimate and last inequalities are by Lemma \ref{lem:properties_of_G_L_main} (i) and (ii), respectively. This suggests that with sufficiently small loss, all entries of $\Vb_t$ remain bounded by 1. 
% Incorporating \cite[Lemma A.4]{zhang2024implicit}\ct{this I believe is the lemma that actually is due to the Linf constrained adam paper by Ziyuan Li, so we should add this ref too (to also avoid keep referencing zhang only)}, which links $\W_t$ and $\log(\Vb_t)$ (entry-wise), we are able to bound $\ninf{\W_t}$ of Adam in terms of summation of learning rates under Assumption \ref{ass:adam_init} (see Lemma \ref{lem:adam_weight} in Appendix \ref{sec:sec_adam}). Lastly, the arguments on $\frac{\Gc(\W)}{\Lc(\W)} \rightarrow 1$ follow the same as SignGD. Combining these pieces together, we arrive at the following main theorem.
\begin{theorem} \label{thm:adam_main} 
% Suppose that Assumption \ref{ass:sep}, 
% \ref{ass:adam_init}, 
% \ref{ass:learning_rate_1},
% \ref{ass:learning_rate_2}, and \ref{ass:data_bound} hold, 
Suppose that Ass. \ref{ass:sep}-\ref{ass:data_bound} hold, 
and $\beta_1 \leq \beta_2$. There exists $t_{a_2} = t_{a_2}(n,d, \gamma,B,\W_0,\beta_1,\beta_2,\omega)$ such that the margin gap $\gamma - {\min_{i \in [n], c \neq y_i} (\e_{y_i} - \e_c)^T \W_t \hb_i}/\ninf{\W_t}$ of Adam's iterates for for all $t > t_{a_2}$ is upper bounded by
\begin{align*}
     % &\left|\frac{\min_{i \in [n], c \neq y_i} (\e_{y_i} - \e_c)^T \W_t \hb_i}{\ninf{\W_t}} - \gamma\right| \leq \\
     \mathcal{O}(\frac{\sum_{s=t_{a_2}}^{t-1} \eta_s e^{-\frac{\gamma}{4} \sum_{\tau = t_{a_2}}^{s-1}\eta_{\tau}} + \sum_{s=0}^{t_{a_2}-1}\eta_s + d\sum_{s = t_{a_2}}^{t-1}\eta_s^{3/2}}{\sum_{s=0}^{t-1} \eta_s}). 
\end{align*}
% %\vspace{-0.1in}
\end{theorem}
The referenced iterations  are $t_{a_1} = \Theta(d^{2/a})$ and $t_{a_2} \leq \Theta(n^{\frac{1}{1-a}}d^{2/a}+n^{\frac{1}{1-a}}\Lc(\W_0)^{\frac{1}{1-a}} + \log(1/\omega))$. Note that compared to the respective iterations for SignGD,  $t_{a_1}$ depends on $d$, unlike $t_{s_1}$. These values the explicit rates below.

\begin{corollary} \label{cor:adam_main}
    Set learning rate $\eta_t = \Theta(\frac{1}{t^a}), a \in (0,1]$. Under the setting of Theorem \ref{thm:adam_main}, the margin gap of Adam iterates with $a=2/3$ reduces at rate $\mathcal{O} (\frac{d \log(t) + nd + [\log(1/\omega)]^{1/3}}{t^{1/3}})$.\footnote{The rates for other values of $a$ can be found in Corollary \ref{cor:adam}.}
    % Consider learning rate schedule of the form $\eta_t = \Theta(\frac{1}{t^a})$ where $a \in (0,1]$, under the same setting as Theorem \ref{thm:adam_main}, then we have for Adam when $a = 2/3$
    % \begin{align*}
    % |& \frac{\min_{i \in [n], c \neq y_i}  (\e_{y_i} - \e_c)^T \W_t \hb_i}{\ninf{\W_t}} - \gamma|
    % \leq  \\ 
    % &\quad \quad \mathcal{O} (\frac{d \log(t) + nd + [\log(1/\omega)]^{1/3}}{t^{1/3}}).
    % % &\left\{
    % % \begin{array}{ll}
    % %        \mathcal{O} (\frac{d t^{1-\frac{3a}{2}}+ n d^{\frac{2(1-a)}{a}} + [\log(1/\omega)]^{1-a}}{t^{1-a}}) &  \text{if} \quad a < \frac{2}{3}\\
    % %        \mathcal{O} (\frac{d \log(t) + nkd + [\log(1/\omega)]^{1/3}}{t^{1/3}}) & \text{if} \quad a = \frac{2}{3} \\
    % %         \mathcal{O} (\frac{d + n d^{\frac{2(1-a)}{a}} + [\log(1/\omega)]^{1-a}}{t^{1-a}}) &  \text{if}\quad \frac{2}{3} < a<1 \\
    % %         \mathcal{O} (\frac{d + n \log(d) + \log \log(1/\omega)}{\log t}) &  \text{if} \quad a=1
    % % \end{array} 
    % % \right. 
    % \end{align*}
    % The rates for other values of $a$ can be found in Lemma \ref{cor:adam}. 
\end{corollary}

\begin{remark} \label{remark:adam}
These rates exactly match those in the binary case of \cite{zhang2024implicit} with logarithmic dependence on the initialization parameter $\omega$ (Ass. \ref{ass:adam_init}). This is only made possible through the fine-grained \textbf{per-class} bounding of the first and second moments using both $\Gc_c(\W)$ and $\Qc_c(\W)$. Note that Lemma \ref{lem:adam_intermediate_bound} takes the same form as \citet[Lemma A.4]{zhang2024implicit}. However, without the tight \textbf{per-class} bound and the equivalent decomposition of $\Gc(\W)$ using either $\Qc_c(\W)$ or $\Gc_c(\W)$, an extra factor of $k$ would appear. Interestingly, our rates for SignGD in Corollary \ref{cor:sign_gd_main} reveal a theoretical gap: Adam's optimal choice $a=\frac{2}{3}$ yields $\mathcal{O}(\frac{d \log(t) + nd}{t^{1/3}})$ while SignGD achieves $\mathcal{O}(\frac{\log(t)+ n}{t^{1/2}})$ with $a = \frac{1}{2}$. Despite achieving tightness w.r.t. class-dimension ($k$), this gap emerges from our entry-wise analysis of the $\clubsuit$ term in \eqref{eq:adam_intermediate_main} across the feature dimension $(d)$ using scalar functions $\Gc_c(\W),\Qc_c(\W)$. Closing this theoretical gap--revealed through our SignGD analysis--that also appears in the binary case \citep{zhang2024implicit}, forms an important direction for future work.
\end{remark}

% \begin{remark}\ct{@Chen: remove this if you are ok with the above version} The corollary above yields the first implicit bias rates for Adam in the multiclass case. Importantly, these rates match those in the binary case \cite{zhang2024implicit} up to a factor of $k$. We also note that the dependence on the initialization parameter $\omega$ (Assumption \ref{ass:adam_init}) remains logarithmic as in \cite{zhang2024implicit}. At the same time, our rates for SignGD in Corollary \ref{cor:sign_gd_main} reveal a theoretical gap: Adam's optimal choice $\alpha=\frac{2}{3}$ yields rate $\mathcal{O}(\frac{kd \log(t) + nkd}{t^{1/3}})$ while SignGD achieves $\mathcal{O}(\frac{\log(t)+ n}{t^{1/2}})$ with $a = \frac{1}{2}$. We attribute this theoretical gap to the  entry-wise analysis of the $\clubsuit$ term in \eqref{eq:adam_intermediate_main} using a scalar $\Gc(\W)$ and note that the same gap appears in the binary case following \cite{zhang2024implicit}. 
% This does not imply that SignGD outperforms Adam in practice. It is caused by when dealing with the $\clubsuit$  term in \eqref{eq:adam_intermediate_main}, the current analysis relies on bounding it entry-wisely using the same scalar-valued function $\Gc(\W)$, as in the approach of the binary setting \cite{zhang2024implicit}. We also note that the dependence on the constant $\omega$ (Assumption \ref{ass:adam_init}) in the multiclass setting is only logarithmic matching that of \cite{zhang2024implicit}. The questions of whether the margin convergence rate of Adam can match that of SignGD, and removing the dependence on $d$, remain open for both binary and multiclass separable data problems. \cf{Nonetheless, to the best of our knowledge, this is the first result on the margin convergence of Adam and SignGD on multiclass separable data with cross-entropy loss}.
% \end{remark}
% \section{Extensions and Experiments} 
% \paragraph{NSD Margin Convergence} Previously, we have shown that the iterates of SignGD converge to the solution that maximizes the margin defined with respect to the matrix max-norm. In this section, we generalize this result to NSD in \eqref{eq:nsd_main} with respect to entry-wise matrix $p$-norm ($p > 1$). We show that the margin of NSD iterates converges to $\gamma_{\lVert \cdot \rVert}$ defined in \eqref{eq:p-margin}. The essence is to show the proxy function in \eqref{eq:G defn} traces the dual norm of the gradient while can be used to bound the second-order term in \eqref{eq:loss taylor main}. We summarize the key results in the follow lemma that generalizes Lemma \ref{lem:lemma_main_G}, \ref{lem:hessian_bound_main}, and \ref{lem:properties_of_G_L_main} (i).  

% \begin{lemma} \label{lem:nsd_main}
% Consider NSD updates in \eqref{eq:nsd_main},  we have 
% \begin{enumerate}[label=(\roman*)]
% \item For any $\W \in \R^{k \times d}$, it holds: $2 B \cdot \Gc(\W) \geq \|{\nabla\Lc(\W)}\|_\star \geq \gamma_{\lVert \cdot \rVert} \cdot \Gc(\W)$
% \item For any $\s\in\Delta^{k-1}$, $c\in[k]$, and $\vb\in\R^k$, it holds:
%      $\vb^\top\left(\diag{\s}-\s\s^\top\right)\vb \leq 4\,\|\vb\|^2\, (1-s_c)$
% \end{enumerate}    
% \end{lemma}

% Moreover, Lemma \ref{lem:G_ratio_main} also holds when the max-norm $\ninf{\cdot}$ is replaced by $p$-norm $\lVert \cdot \rVert$. These results enable us to carry the same analysis for NSD but with parameters in any $p$-norm ($p \geq 1$) and the loss gradient in the dual norm. Therefore, the rates in Corollary \ref{cor:sign_gd_main} for SignGD also hold for NSD with $\gamma$ replaced by $\gamma_{\lVert \cdot \rVert}$ in \eqref{eq:p-margin}. \cf{may need to discuss the rates of NSD of other work here? We may need to reorganize the paragraph under corollary 5.5}

% \paragraph{Other Losses} Besides cross-entropy loss, we also consider the multiclass exponential loss \cite{mukherjee2010theory} and the 
% PairLogLoss \cite{ravi2024implicit}.
% They are defined as follows:
% \begin{align*}
%     &\Lcexp(\W):=\frac{1}{n}\sum_{i\in[n]}\sum_{c\neq y_i}\exp\left(-(\eb_{y_i}-\eb_c)^\top\W\hb_i\right)\\
%     &\Lcpll(\W):=\frac{1}{n}\sum_{i\in[n]}\sum_{c\neq y_i}\log\left(1+\exp\left(-(\eb_{y_i}-\eb_c)^\top\W\hb_i\right)\right)
% \end{align*}
% For the exponential loss, we can use the same proxy function $\Gc(\W)$, and it is straightforward to extend the previous results to this setting. For the PairLogLoss, we recognize that $\inp{\Ab}{-\nabla\Lcpll(\W)}$ has the following simple form 
% \begin{align*}
% \frac{1}{n}\sum_{i\in[n]}\sum_{c\neq y_i}|f'\left((\eb_{y_i}-\eb_c)^\top\W\hb_i\right)|\cdot\left(\eb_{y_i}-\eb_c\right)^\top\Ab\hb_i    
% \end{align*} 
% where $f(t):=\log(1+e^{-t})$, from which we can directly deduce that $    \Gcpll(\W) \coloneqq \frac{1}{n}\sum_{i\in[n]}\sum_{c\neq y_i}\left|f'\left(-(\eb_{y_i}-\eb_c)^\top\W\hb_i\right)\right|$. In Appendix \ref{sec:app_other_losses}, we prove analogous results of Lemma \ref{lem:lemma_main_G}, \ref{lem:hessian_bound_main}, and \ref{lem:properties_of_G_L_main} for $\Lc_{pll}(\W)$. Thus, the results of SignGD, NSD, and Adam can all be extended to PairLogLoss following the same steps above. In summary, we have provided the implicit bias results of these algorithms for different multiclass objectives.   

% \begin{figure*}[t]
%     \centering
%     \begin{subfigure}[b]{0.19\textwidth}
%         \centering
%         \includegraphics[width=\textwidth]{figs/Loss.pdf}
%         \caption{}
%         \label{fig:first}
%     \end{subfigure}
%     %\hfill
%     \begin{subfigure}[b]{0.19\textwidth}
%         \includegraphics[width=\textwidth]{figs/rel_fro_margin.pdf}
%         \caption{}
%         \label{fig:second}
%     \end{subfigure}
%     %\hfill
%     \begin{subfigure}[b]{0.19\textwidth}
%         \includegraphics[width=\textwidth]{figs/rel_max_margin.pdf}
%         \caption{}
%         \label{fig:third}
%     \end{subfigure}
%     \caption{}
%     \label{fig:figure1}
% \end{figure*}

%\vspace{-0.1in}
\paragraph{Experiments} We generate snythetic multiclass separable data as follows: $k=5$ class centers are sampled from a standard normal distribution; within each class, data is sampled from  normal distribution $\mathcal{N}(0, \sigma^2 I),\sigma=0.1$. We set $d = 25$, sample $5$ data points for each class, and ensure that margin is positive (thus data is separable). 
% The margins for the dataset in Figure \ref{fig:main_fig1} are $\gamma = 18.69$, $\gamma_{\lVert \cdot \rVert} = 2.18$. 
We run different algorithms to minimize CE loss using   $\eta_t = \frac{\eta_0}{t^{a}}$ ($\eta_0 = 0.01$), where (based on our theorems) $a$ is set to $\nicefrac{1}{2}$, $\nicefrac{1}{2}$, and $\nicefrac{2}{3}$ for NGD, SignGD, and Adam, respectively. The stability constant $\eps$ for Adam is set from $\{0, 10^{-6}, 10^{-7}, 10^{-8}\}$. We denote max-margin classifiers defined w.r.t. the 2-norm and the max-norm as $\Vb_2$ and $\Vb_{\infty}$ respectively.
%, and define the correlation as  $\langle \A,\Bb\rangle / (\lVert \A \rVert_2 \lVert \Bb \rVert_2)$. 
Fig.  \ref{fig:main_fig1} shows the following: (1) SignGD/Adam iterates favor the the max-norm margin over the 2-norm margin. The opposite  is true for NGD (Figs. \ref{fig:l2_margin},\ref{fig:max_margin}); (2) SignGD/Adam iterates correlate well with $\Vb_{\infty}$ whereas NGD correlates better with $\Vb_2$ (Figs. \ref{fig:cor_inf},\ref{fig:cor_v2}); (3) Results are consistent for different (small) values of stability constant (curves nearly overlap). 
% \begin{figure*}[t!]
% \centering     %%% not \center
% \subfigure[Loss]{\label{fig:main_loss}\includegraphics[width=30mm]{figs/Loss.pdf}}
% \subfigure[$\lVert \cdot \rVert_2$ Margin]{\label{fig:max_margin}\includegraphics[width=30mm]{figs/rel_fro_margin.pdf}}
% \subfigure[$\lVert \cdot \rVert_{\max}$ Margin]{\label{fig:l2_margin}\includegraphics[width=30mm]{figs/rel_max_margin.pdf}}
% \subfigure[$\frac{\langle \W_t, \Vb_2\rangle} {\lVert \W_t \rVert_2 \lVert \Vb_2 \rVert_2}$]
% {\label{fig:cor_v2}\includegraphics[width=30mm]{figs/cor_v2.pdf}}
% \subfigure[$\frac{\langle \W_t, \Vb_{inf}\rangle} {\lVert \W_t \rVert_2 \lVert \Vb_{inf} \rVert_2}$]
% {\label{fig:cor_inf}\includegraphics[width=30mm]{figs/cor_vinf.pdf}}
% \captionsetup{width=\textwidth}
% \caption{Implicit bias of Adam on multiclass separable data (a) Loss vs. iterations. We observe that SignGD converges fast for the given learning rate schedule. (b) We normalize the weights w.r.t. 2-norm, compute its margin, then plot the difference between this and the margin of $\Vb_2$. We observe that the 2-norm margin convergence of NGD is faster than others. (c) Same plot as (b) with 2-norm replaced by max-norm. In this case, we observe that the max-norm convergence of SignGD or Adam converge is faster than NGD. (d) and (e) Correlations between $\W_t$ and $\Vb_2$ or $\Vb_{inf}$ against iterations respectively. SignGD or Adam iterates correlate well with $\Vb_{inf}$, while for NGD is $\Vb_2$.}
% \label{fig:main_fig1}
% \end{figure*}

%\vspace{-0.05in}
\section{Conclusion}
% %\vspace{-0.05in}
We have characterized the implicit bias of SignGD, NSD, and Adam for multiclass separable data with CE loss, providing explicit rates for their margin maximization behavior.
%Our analysis provides explicit max-norm margin convergence rates for SignGD/Adam, and generalizes them  to NSD w.r.t. arbitrary $p$-norms. 
While these results establish fundamental theoretical guarantees, they are limited to linear models and separable data. Yet, they open several promising directions for future research: \textbf{(1)} Improving SignGD's rates through appropriate momentum leveraging ideas from \citet{ji2021fast,wang2023faster} for NGD.
%(2) Deriving dimension-independent rates for Adam, addressing a limitation that, as discussed in Rem. \ref{remark:adam}, persists even in the binary setting.
\textbf{(2)} Extending the results to non-separable data \citep{ji2019implicit}
and soft-label classification \citep{ntp} settings.
\textbf{(3)} Extending the analysis to non-linear architectures like self-attention \citep{tarzanagh2023maxmargin, tarzanagh2023transformers,deora2024implicit,julistiono2024optimizing}. Our treatment of CE loss through softmax properties in Sec. \ref{sec: G and L} is particularly relevant, as the implicit bias in self-attention is fundamentally driven by the softmax map. Moreover, self-attention naturally aligns with our multiclass setting since each token attends to multiple other tokens through softmax.
\textbf{(4)} Investigating whether different  implicit biases of SignGD/Adam to GD could theoretically explain empirical observations about their superior performance on heavy-tailed, imbalanced multiclass data \citep{kunstner2024heavy}.
\textbf{(5)} From a statistical perspective, identifying specific scenarios where max-norm margin maximization leads to better generalization (building on initial investigations from \cite{salehi2019impact,varma2024benefits,akhtiamov2024regularized} on the impact of different regularization forms in linear classification, and recent findings from \cite{mohamadi2024you} specifically related to infinity-norm regularization in neural networks).
% In summary, we have studied the implicit bias of SignGD, NSD, and Adam for multiclass separable data using cross-entropy loss and others. We characterize the explicit max-norm margin convergence rates of SignGD and Adam while generalizing them to NSD w.r.t. any $p$-norm. Our study also leads to several future directions: 
% 1. Extend the current analysis to other (non-linear) models such as self-attention \citep{vaswani2017attention, tarzanagh2023transformers}; 2. Improve the rates of SignGD with use of momentum \citep{ji2021characterizing}; 3. Obtain dimension-independent rates for Adam. 

% 1. What is the rate of margin convergence of SignGD for constant step-size? This is known for normalized gradient descent (NGD) \cite{ji2021characterizing}. 
% 1. Can we extend the current analysis to other models such as attention mechanism \citep{vaswani2017attention, tarzanagh2023transformers}? 
% 2. Can the rates be even further accelerated with appropriate use of momentum, as previously done for NGD \citep{ji2021fast}? 3. Can we get dimension-independent rates for Adam? 
%\label{sec:other_losses}

\section*{Acknowledgement}
% This work was partially funded by the NSERC Discovery Grant No. 2021-03677, the Alliance GrantALLRP 581098-22, and a CIFAR AI Catalyst grant.
This work is funded partially by  NSERC Discovery Grants RGPIN-2021-03677 and RGPIN-2022-03669, Alliance GrantALLRP 581098-22, a CIFAR AI Catalyst grant, and the Canada CIFAR AI Chair Program.
\bibliography{refs,refs_CT}
\bibliographystyle{icml2025}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% APPENDIX
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% \newpage
\appendix
\onecolumn
% \paragraph{Notations.} $\ninf{\Ab}=\max_{i,j} A_{ij}$ denotes the max-norm of a matrix $\Ab$. $\lVert \A \rVert_{1,1}$ denotes the vector 1-norm of a matrix $\A$, i.e., $\lVert \A \rVert_{1,1} = \lVert \vect{\A} \rVert_{1}$. Recall this is an entry-wise norm and does not satisfy the sub-multiplicative property of norms.
% Let classifier $\W\in\R^{k\times d}$, embeddings matrix $\Hb = [\hb_1, \ldots, \hb_n] \in\R^{d\times n}$, and logits matrix $\Lb = [\ellb_1, \ldots, \ellb_n] \in\R^{k\times n}$ (where $\ellb_i = \W \hb_i$ for all $i \in [n]$). Let $\yi\in[k]$ be the label of $i$-th datapoint. Let $\y_i=\eb_\yi$ be its one-hot representation and denote $\Y=[\y_1,\ldots,\y_n]\in\R^{k\times n}$ the one-hot label matrix. Denote the softmax function $\mathbb{S}: \R^{k} \rightarrow \R$ s.t. $\mathbb{S}(\vb) = [\frac{e^{v_i}}{\sum_{i \in [k]} e^{v_i}}]_{i=1}^k \in \R^{k}$, $\s_i:=\sft{\W\hb_i} \in \R^k$ for all $i \in [n]$, and $s_{i c} \coloneqq \sfti{c}{\W \hb_i} = \s_i[c]$ be the $c$th entry of the vector $\s_i$. Denote $t_0$ as the time that satisfies Assumption \ref{ass:learning_rate_2}. Times associated with SignGD and Adam are denoted as $t_{s_{(\cdot)}}$ and $t_{a_{(\cdot)}}$, respectively.

\section{Facts about CE loss and Softmax} \label{sec: grad and hess}
Lemma \ref{lem:CE logit loss properties} is on the gradient of the cross-entropy loss. It will be used for showing the form of $\Gc(\W)$ in \eqref{eq:G defn} lower bounds $\ninf{\nabla \Lc(\W)}$ in Lemma \ref{lem:G and gradient}.  
\begin{lemma}[Gradient]\label{lem:CE logit loss properties}
   Let CE loss
   \[\Lc(\W):=-\frac{1}{n}\sum_{i\in[n]}\log\big(\sfti{\yi}{\W\hb_i}\big).\]
   For any $\W$, it holds 
    \begin{itemize}
    \item $\nabla\Lc(\W) = -\frac{1}{n}\sum_{i\in[n]}  \left(\eb_\yi-\s_i\right)\hb_i^\top = -\frac{1}{n}(\Y-\Sb)\Hb^\top$
        \item $\ones_k^\top\nabla\Lc(\W)=0$
        \item For any matrix $\Ab\in\R^{k\times d}$,  
        \begin{align}
            \inp{\A}{-\nabla\Lc(\W)} &= \frac{1}{n}\sum_{i} \left(1-s_{i\yi}\right)\left(\eb_\yi^\top\A\hb_i-\frac{\sum_{c\neq \yi} s_{ic}\, \eb_c^\top\A\hb_i}{\left(1-s_{i\yi}\right)}\right)\nn\\
            &=
            \frac{1}{n}\sum_{i\in[n]} {\sum_{c\neq \yi} s_{ic}\, (\eb_{\yi}-\eb_c)^\top\A\hb_i}\label{eq:CE gradient inp}
            \end{align}
    % \item Suppose that $\lVert \hb_i \rVert_{1} \leq B$ for all $i \in [n]$, then $\lVert \nabla \Lc(\W) \rVert_{1,1} \leq 2B$.
    \end{itemize}
    where we simplify $\Sb:=\sft{\W\Hb} = [\s_1, \ldots, \s_n]\in \R^{k \times n}$.
    The last statement yields
    \begin{align}\label{eq:nabla_ineq_basic}
    \inp{\A}{-\nabla\Lc(\W)} \geq \frac{1}{n}\sum_{i\in[n]} \left(1-s_{i\yi}\right)\,\cdot\,\min_{c\neq \yi}\left(\eb_\yi-\eb_c\right)^\top\A\hb_i.
    \end{align}
\end{lemma}
\begin{proof}
    First bullet is by direct calculation. Second bullet uses the fact that $\ones^\top(\y_i-\s_i)=1-1=0$ since $\ones^\top\s_i=1$. The third bullet follows by direct calculation and writing $\s_i^\top\A\hb_i=(\sum_{c}s_{ic}\eb_c)^\top\A\hb_i=\sum_{c}s_{ic}\,\eb_c^\top\A\hb_i$. 
    % The last bullet follows from the following
    % \begin{align*}
    %     \lVert \nabla \Lc(\W) \rVert_{1,1} \leq \frac{1}{n} \sum_{i=1}^n \lVert (\e_{y_i} - \s_i) \hb_i^T\rVert_{1,1} = \frac{1}{n} \sum_{i=1}^n \lVert \e_{y_i} - \s_i \rVert_1 \lVert \hb_i \rVert_1 \leq 2B.
    % \end{align*}
\end{proof}
Lemma \ref{lem:hessian} is on the Taylor expansion of the loss. It will be used in showing the descent properties of SignGD (Lemma \ref{lem:sign_descent}) and Adam (Lemma \ref{lem:adam_descent}). 
\begin{lemma}[Hessian] \label{lem:hessian}
    Let perturbation $\Deltab\in\R^{k\times d}$ and denote $\W'=\W+\Deltab$. Then, 
    \begin{align}
        \Lc(\W') &= \Lc(\W) - \frac{1}{n}\sum_{i\in[n]}\inp{(\eb_\yi-\sft{\W\hb_i})\hb_i^\top}{\Deltab} \nn
        \\
        &\quad+ \frac{1}{2n}\sum_{i\in[n]}\hb_i^\top\Deltab^\top\left(\diag{\sft{\W\hb_i}}-\sft{\W\hb_i}\sft{\W\hb_i}^\top\right)\Deltab\,\hb_i+o(\|\Deltab\|^3)\,.
    \end{align}
\end{lemma}
\begin{proof}
    Define function $\ell_y:\R^{k}\rightarrow\R$ parameterized by $y\in[k]$ as follows:
    \[
     \ell_y(\ellbb):=-\log(\sfti{y}{\ellbb})\,.
    \]
    From Lemma \ref{lem:CE logit loss properties},
    \[
    \nabla\ell_y(\ellbb)=-(\eb_y-\sft{\ellbb})\,.
    \]
    Thus,
    \[
\nabla^2\ell_y(\ellbb)=\nabla\sft{\ellbb}=\diag{\sft{\ellbb}}-\sft{\ellbb}\sft{\ellbb}^\top
    \]
    Combining these the second-order taylor expansion of $\ell_y$ writes as follows for any $\ellbb,\deltab\in\R^k$:
    \begin{align*}
        \ell_y(\ellbb+\deltab) = \ell_y(\ellbb) - (\eb_y-\sft{\ellbb})^\top\deltab + \frac{1}{2}\deltab^\top\left(\diag{\sft{\ellbb}}-\sft{\ellbb}\sft{\ellbb}^\top\right)\deltab+o(\|\deltab\|^3)\,.
    \end{align*}
    To evaluate this with respect to a change on the classifier parameters, set $\ellbb=\W\hb$ and $\deltab=\Deltab\hb$ for $\Deltab\in\R^{k\times d}$. Denoting $\W'=\W+\Deltab$, we then have 
    \begin{align*}
        \ell_y(\W') = \ell_y(\W) - \inp{(\eb_y-\sft{\ellbb})\hb^\top}{\Deltab} + \frac{1}{2}\hb^\top\Deltab^\top\left(\diag{\sft{\ellbb}}-\sft{\ellbb}\sft{\ellbb}^\top\right)\Deltab\hb+o(\|\Deltab\|^3)\,.
    \end{align*}
    This shows the desired since $n\Lc(\W):=\sum_{i\in[n]}\ell_\yi(\W\hb_i)$\, and we can further obtain
    \begin{align}
        \ell_y(\W') = \ell_y(\W) - \inp{(\eb_y-\sft{\ellbb})\hb^\top}{\Deltab} + \frac{1}{2}\hb^\top\Deltab^\top\left(\diag{\sft{\ellbb'}}-\sft{\ellbb'}\sft{\ellbb'}^\top\right)\Deltab\hb \label{eq:taylor_eq1}, 
    \end{align}
    where $\ellbb' = \ellbb + \zeta \deltab$ for some $\zeta \in [0,1]$. 
\end{proof}

Lemma \ref{lem:hessian_bound} is used in bounding the second order term in the Taylor expansion of $\Lc(\W)$.

\begin{lemma} \label{lem:hessian_bound}
     For any $\s\in\Delta^{k-1}$ in the $k$-dimensional simplex, any index $c\in[k]$, and any $\vb\in\R^k$ it holds:
     \[
     \vb^\top\left(\diag{\s}-\s\s^\top\right)\vb \leq 4\,\|\vb\|_\infty^2\, (1-s_c)
     \]
\end{lemma}
\begin{proof}
    By Cauchy-Schwartz, 
    \begin{align*}
        \vb^\top\left(\diag{\s}-\s\s^\top\right)\vb &= \vect{\diag{\s}-\s\s^\top}^\top \vect{\vb\vb^\top} \\
        &\leq \|\vect{\diag{\s}-\s\s^\top}\|_1 \| \vect{\vb\vb^\top}\|_\infty 
        \\&\leq \|\vb\|_\infty^2\,\|\vect{\diag{\s}-\s\s^\top}\|_1\,.
    \end{align*}
    But, 
    \begin{align*}
        \|\vect{\diag{\s}-\s\s^\top}\|_1 &= \sum_{c\in[k]} s_c(1-s_c) + \sum_{c'\neq c \in[k]} s_c s_{c'}
        \\
        &=2\sum_{c\in[k]} s_c(1-s_c)
    \end{align*}
    where the  last line uses $\sum_{c\neq c}s_{c'}=1-s_c$. 
    
    The proof completes by applying Lemma \ref{lem:trivial softmax} to the above.
    
\end{proof}
Lemma \ref{lem:trivial softmax} is used in the proof of Lemma \ref{lem:hessian_bound}. 
\begin{lemma}\label{lem:trivial softmax}
    For any $\s\in\Delta^{k-1}$ in the $k$-dimensional simplex and any index $c\in[k]$ it holds that
    \[
    \sum_{c'}s_{c'}(1-s_{c'}) \leq 2(1-s_c)\,.
    \]
\end{lemma}
\begin{proof}
With a bit of algebra and using $\sum_{c'\neq c}s_{c'}=1-s_c$ the claim becomes equivalent to 
\[
    \sum_{c'\neq c}s_{c'}^2 + s_c^2-2s_c + 1 \geq 0.
    \]
    Since this holds true, the lemma holds.
\end{proof}

\section{Lemmas on $\Lc(\W)$ and $\Gc(\W)$} \label{sec: G and L}

Lemma \ref{lem:G and gradient} shows that $\Gc(\W)$ lower bounds the loss gradient and that the bound becomes tight as the loss approaches zero.

\begin{lemma}[$\Gc(\W)$ as proxy to the loss-gradient norm]
\label{lem:G and gradient}
Recall the margin $\gamma$ and the assumption $\|\hb_i\|_1\leq B$. 
Then, for any $\W$ it holds that
    \[
    2B \cdot \Gc(\W) \geq \none{\nabla\Lc(\W)} \geq \gamma\cdot \Gc(\W)\,.
    \]
\end{lemma}

\begin{proof} First, we prove the lower bound. By duality and direct application of \eqref{eq:nabla_ineq_basic}
\begin{align*}
    \none{\nabla \Lc(\W)} & = \max_{\ninf{\A} \leq 1} \langle \A, -\nabla \Lc(\W)\rangle \\
    &\geq \max_{\ninf{\A} \leq 1} \frac{1}{n} \sum_{i \in [n]} (1 - s_{iy_i}) \min_{c \neq y_i} (\e_{y_i} - \e_c)^T \A \hb_i \\
    &\geq \frac{1}{n} \sum_{i \in [n]} (1 - s_{iy_i}) \cdot  \max_{\ninf{\A} \leq 1}\min_{i \in [n],c \neq y_i} (\e_{y_i} - \e_c)^T \A \hb_i.
\end{align*}
Second, for the upper bound, start with noting by triangle inequality that 
\[
\none{\nabla \Lc(\W)} \leq \frac{1}{n}\sum_{i\in[n]}\none{\nabla \ell_i(\W)}\,,
\]
where $\ell_i(\W)=-\log(\sfti{y_i}{\W\hb_i})$. Recall that
\[
\nabla\ell_i(\W) = -(\eb_y-\sfti{y_i}{\W\hb_i})\hb_i^\top,
\]
and, for two vectors $\vb,\ub$: $\none{\ub\vb^\top}=\|\ub\|_1\|\vb\|_1$. Combining these and noting that \[\|\eb_{y_i}-\sfti{y_i}{\W\hb_i}\|_1=2(1-s_{y_i})\] together with using the assumption $\|\hb_i\|\leq B$ yields the advertised upper bound.
\end{proof}

Built upon \ref{lem:G and gradient}, we obtain a simple bound on the difference between losses at two different points using the max-norm on the iterates.
\begin{lemma} \label{lem:l_fast_bound}
    For any $\W, \W_0 \in \R^{k \times d}$, suppose that $\Lc(\W)$ is convex, we have
    \begin{align*}
        |\Lc(\W) - \Lc(\W_0)| \leq 2B \ninf{\W - \W_0}. 
    \end{align*}
\end{lemma}
\begin{proof} By convexity of $\Lc$, we have
    \begin{align*}
        \Lc(\W_0) - \Lc(\W) \leq \langle \nabla \Lc(\W_0), \W_0 - \W \rangle \leq \none{\nabla \Lc(\W_0)}  \ninf{\W_0 - \W} \leq 2B \ninf{\W_0 - \W} \,,
    \end{align*}
    where the last inequality is by Lemma \ref{lem:G and gradient}. Similarly, we can also show that $\Lc(\W) - \Lc(\W_0) \leq  2B \ninf{\W_0 - \W}$.
\end{proof}

Lemma \ref{lem:G and L} shows the close relationship between $\Gc(\W)$ and $\Lc(\W)$. $\Gc(\W)$ not only lower bounds $\Lc(\W)$, but also upper bounds $\Lc(\W)$ up to a constant provided that the loss is sufficiently small. Moreover, the rate of convergence $\frac{\Gc(\W)}{\Lc(\W)}$ depends on the rate of decrease in the loss.

\begin{lemma}[$\Gc(\W)$ as proxy to the loss]\label{lem:G and L}
    Let $\W \in \R^{k \times d}$, we have
    \begin{enumerate}[label=(\roman*)]
    \item $1\geq \frac{\Gc(\W)}{\Lc(\W)} \geq 1-\frac{n\Lc(\W)}{2} $
    \item  Suppose that $\W$ satisfies $\Lc (\W) \leq \frac{\log 2}{n}$ or $\Gc(\W) \leq \frac{1}{2n}$, then $\Lc(\W) \leq 2 \Gc(\W).$
    \end{enumerate}
\end{lemma}
\begin{proof}
    (i) Denote for simplicity $s_i:=s_{i\yi} =\sfti{\yi}{\W\hb_i}$, thus
    $\Lc(\W)=\frac{1}{n}\sum_{i\in[n]}\log(1/s_i)$ and $\Gc(W)=\frac{1}{n}\sum_{i\in[n]}(1-s_i)$.
    For the upper bound, simply use the fact that $e^{x-1}\geq x,$ forall $x\in[0,1],$
    thus $\log(1/s_i)\geq 1-s_i$ for all $i\in[n]$. 
    
    The lower bound can be proved using the exact same arguments in the proof of \citet[Lemma C.7]{zhang2024implicit} for 
    the binary case. For completeness, we provide an alternative elementary proof. It suffices to prove for $n=1$ that for $s\in(0,1)$:
\begin{align}\label{eq:GL lb proof n=1}
1-s \geq \log(1/s) - \frac{1}{2}\log^2(1/s).
\end{align}
The general case follows by summing over $s=s_i$ and using $\sum_{i\in[n]}\log^2(1/s_i)\leq \left(\sum_{i\in[n]}\log(1/s_i)\right)^2$ since $\log(1/s_i)>0$. For \eqref{eq:GL lb proof n=1}, let $x=\log(1/s)>0$. The inequality becomes $e^{-x}\leq 1-x+x^2/2$, which holds for $x>0$ by the second-order Taylor expansion of $e^{-x}$ around $0$.
    
    (ii) The sufficiency of $\Lc(\W) \leq \frac{\log 2}{n}$ (to guarantee that $\Lc(\W) \leq 2\Gc(\W)$) follows from (i) and $\Lc(\W) \leq \frac{\log 2}{n} \leq \frac{1}{n}$. The inequality $\log(\frac{1}{x}) \leq 2(1-x)$ holds when $x \in [0.2032,1]$. This translates to the following sufficient condition on $s_{i y_i}$
    \begin{align*}
        s_{i} = \frac{e^{\ellb_i[y_i]}}{\sum_{c \in [k]} e^{\ellb_i[c]}} = \frac{1}{1+ \sum_{c \in [k], c \neq y_i} e^{\ellb_i[c] - \ellb_i[y_i]}} \geq 0.2032. 
    \end{align*}
    Under the assumption $\Gc(\W) \leq \frac{1}{2n}$, we have $1 - s_{i} \leq \sum_{i \in [n]} (1 - s_{i}) = n \Gc(\W) \leq \frac{1}{2}$, from which we obtain $s_{i} \geq \frac{1}{2} \geq 0.2032$ for all $i \in [n]$. 
\end{proof}

Lemma  \ref{lem:sep} shows that weights $\W$ of low loss separate the data. It is used in deriving the lower bound on the unnormalized margin. 
\begin{lemma} [Low $\Lc(\W)$ implies separability] \label{lem:sep}
    Suppose that there exists $\W \in \R^{k\times d}$ such that $\Lc (\W) \leq \frac{\log 2}{n}$, then we have
    \begin{align}
        (\eb_{y_i} - \e_c)^T \W \hb_i \geq 0, \quad \text{for all $i \in [n]$ and for all $c \in [k]$ such that $c \neq y_i$}. \label{eq:sep}
    \end{align}
\end{lemma}
\begin{proof} We rewrite the loss into the form:
\begin{align*}
    \Lc(\W) = - \frac{1}{n} \sum_{i \in [n]} \log(\frac{e^{\ellb_{i}[y_i]}}{\sum_{c \in [k]} e^{\ellb_i[c]}}) = \frac{1}{n} \sum_{i \in [n]} \log(1 + \sum_{c\neq y_i} e^{-(\ellb_i[y_i] - \ellb_i[c])}).
\end{align*}
Fix any $i\in [n]$, by the assumption that $\Lc (\W) \leq \frac{\log 2}{n}$, we have the following:
\begin{align*}
    \log(1 + \sum_{c\neq y_i} e^{-(\ellb_i[y_i] - \ellb_i[c])} )\leq n \Lc(\W) \leq \log(2).
\end{align*}
This implies:
\begin{align*}
    e^{- \min_{c\neq y_i} (\ellb_i[y_i] - \ellb_i[c])} = \max_{c \neq y_i} e^{-(\ellb_i[y_i] - \ellb_i[c]) \leq }  \leq \sum_{c\neq y_i} e^{-(\ellb_i[y_i] - \ellb_i[c])} \leq 1.
\end{align*}
After taking $\log$ on both sides, we obtain the following: $\ellb_i[y_i] - \ellb_i[c] = (\eb_{y_i} - \e_c)^T \W \hb_i \geq 0$ for any $c \in [k]$ such that $c \neq y_i$. 
% Now, suppose that $\Gc(\W) \leq \frac{1}{2n}$, we have for any $i \in [n]$
% \begin{align*}
%     1 - \frac{e^{\ellb_i[y_i]}}{\sum_{c \in [k]} e^{\ellb_i[c]}} \leq \sum_{i \in [n]} 1 - \frac{e^{\ellb_i[y_i]}}{\sum_{c \in [k]} e^{\ellb_i[c]}} = n \Gc(\W) \leq \frac{1}{2}.
% \end{align*}
% This implies that 
% \begin{align*}
%     e^{-(\ellb_i[y_i] - \ellb_i[c])} \leq \sum_{c \in [k], c \neq y_i} e^{-(\ellb_i[y_i] - \ellb_i[c])} \leq 1,
% \end{align*}
% for any $c \in [k]$ such that $c \neq y_i$. The result follows after taking $\log$ on both sides. 
\end{proof}

Lemma \ref{lem:G_ratio} shows that the ratio of $\Gc(\W)$ at two points can be bounded by the exponential of the max-norm of their differences. It is used in handling the second order term in the Taylor expansion of the loss. 
\begin{lemma} [Ratio of $\Gc(\W)$] \label{lem:G_ratio}
    For any $\psi \in [0,1]$, we have the following:
    \begin{align*}
        \frac{\Gc(\W + \psi \triangle \W)}{\Gc(\W)} \leq e^{2 B \psi \ninf{\triangle \W}}
    \end{align*}
\end{lemma}
\begin{proof}
    By the definition of $\Gc(\W)$, we have:
    \begin{align*}
        \frac{\Gc(\W + \psi \triangle \W)}{\Gc(\W)} = \frac{\sum_{i \in [n]} \bigl( 1 - \sfti{y_i}{(\W + \psi \triangle \W)\hb_i} \bigr) }{\sum_{i \in [n]} \bigl( 1 - \sfti{y_i}{\W\hb_i} \bigr)}. 
    \end{align*}
    For any $c \in [k]$ and $\vb, \vb' \in \R^{k}$, we have:
    \begin{align*}
        \frac{1 - \sfti{c}{\vb'}}{1 - \sfti{c}{\vb}} &= \frac{1 - \frac{e^{v'_c}}{\sum_{i \in [k]} e^{v'_i}}}{ 1 - \frac{e^{v_c}}{\sum_{i \in [k]} e^{v_i}}} \\
        &= \frac{\frac{\sum_{j \in [k], j \neq c} e^{v'_j}}{\sum_{i \in [k]} e^{v'_i}}}{ \frac{\sum_{j \in [k], j \neq c} e^{v_j}}{\sum_{i \in [k]} e^{v_i}}} \\
        &= \frac{\sum_{j \in [k], j \neq c} \sum_{i \in [k]} e^{v'_j + v_i}}{\sum_{j \in [k], j \neq c} \sum_{i \in [k]} e^{v_j + v'_i}} \\
        &\leq e^{2 \lVert \vb - \vb' \rVert_{\infty}}.
    \end{align*}

    
    
    
    The last inequality is because $\frac{e^{v'_j + v_i}}{e^{v_j + v'_i}} \leq e^{|v'_j - v_j| + |v_i - v'_i|} \leq e^{2 \lVert \vb - \vb' \rVert_{\infty}}$, which implies that $\sum_{j \in [k], j \neq c} \sum_{i \in [k]} e^{v'_j + v_i} \leq e^{2 \lVert \vb - \vb' \rVert_{\infty}} \sum_{j \in [k], j \neq c} \sum_{i \in [k]} e^{v_j + v'_i}$. Next, we specialize this result to $\vb' = (\W + \psi \triangle \W) \hb_i$, $\vb = \W \hb_i$, and $c = y_i$ for any $i \in [n]$ to obtain:
    \begin{align*}
        \frac{1 - \sfti{y_i}{(\W + \psi \triangle \W)\hb_i} \bigr)}{1 - \sfti{y_i}{\W\hb_i}} \leq e^{2 \psi \lVert \triangle \W \hb_i \rVert_{\infty}} \leq e^{2 B \psi  \ninf{\triangle \W}}.
    \end{align*}
    Then, we rearrange and sum over $i \in [n]$ to obtain: $ \sum_{i \in [n]} \bigl( 1 - \sfti{y_i}{(\W + \psi \triangle \W)\hb_i} \bigr) \leq e^{2 B \psi \ninf{\triangle \W}} \sum_{i \in [n]} \bigl( 1 - \sfti{y_i}{\W\hb_i} \bigr)$, from which the desired inequality follows. 
\end{proof}

\paragraph{Proof Overview} 
We consider a decay learning rate schedule of the form $\eta_t = \Theta(\frac{1}{t^a})$ where $a \in (0,1]$. The first step is to show that the \textbf{loss monotonically decreases} after certain time and the rate depends on $\Gc(\W)$. To obtain this, we apply Lemma \ref{lem:G and gradient} and Lemma \ref{lem:hessian_bound} to upper bound the first-order and second-order terms in the Taylor expansion of the loss \ref{eq:taylor}, respectively. The difference between SignGD and Adam is that Adam involves an additional term $\bigm| \langle \nabla \Lc(\W_t), \frac{\M_t}{\sqrt{\Vb_t}} - \frac{\nabla \Lc(\W_t)}{|\nabla \Lc(\W_t)|}\rangle \bigm|$. To handle it, we apply Lemma \ref{lem:first_G} and Lemma \ref{lem:second_G} to bound the first ($\M_t$) and second moment ($\Vb_t$) buffer of Adam using $\Gc(\W)$. Next, we use loss monotonically to derive \textbf{a lower bound on the unnormalized margin} which involves the ratio $\frac{\Gc(\W)}{\Lc(\W)}$. A crucial step involved is to find a time $\bar{t}_2$ such that separability \eqref{eq:sep} holds for all $t \geq \bar{t}_2$, and the existence of $\bar{t}_2$ is guaranteed by loss monotonicity given $\Lc(\W_t) \leq \frac{\log 2}{n}$ implying separability \eqref{eq:sep} proved in Lemma \ref{lem:sep}. \\

Then, we argue that the \textbf{ratio $\frac{\Gc(\W_t)}{\Lc(\W_t)}$ converges to $1$ exponentially fast} (recalling that $1 \geq \frac{\Gc(\W_t)}{\Lc(\W_t)} \geq 1 - \frac{n \Lc(\W_t)}{2}$) by proving the same rate for the decrease of the loss $\Lc(\W_t)$. We first choose a time $t_{1}$ after $t_0$ (recall that $t_0$ is the time that satisfies Assumption \ref{ass:learning_rate_2}) such that  $\Lc(\W_{t+1}) \leq \Lc(\W_t) - \frac{\eta_t \gamma}{2} \Gc(\W_T)$ for all $t \geq t_{1}$. Next, we lower bound $G(\W_t)$ using $\Lc(\W_t)$. By Lemma \ref{lem:G and L}, there are two sufficient conditions (namely, $\Lc(\W_t) \leq \frac{\log 2}{n} \eqqcolon \tilde{\Lc}$ or $ \G(\W_t) \leq \frac{1}{2n}$) that guarantee $\Lc(\W_t) \leq 2 \Gc(\W_t)$. We choose a time $t_{2}$ (after $t_{1}$) that is sufficiently large such that there exists $t^* \in [t_{1}, t_{2}]$ for which we have $\Gc(\W_{t^*}) \leq \frac{\tilde{\Lc}}{2} \leq \frac{1}{2n}$. This not only guarantees that $\Lc(\W_{t^*}) \leq 2 \Gc(\W_{t^*})$ at time $t^*$, but also (crucially due to monotonicity) implies that $\Lc(\W_t) \leq \Lc(\W_{t^*}) \leq 2 \Gc(\W_{t^*}) \leq \frac{\log 2}{n}$ for all $t \geq t_{2}$. Thus, we observe that the other sufficient condition $\Lc(\W_t) \leq \frac{\log 2}{n}$ is satisfied, from which we conclude that $\Lc(\W_t) \leq 2 \Gc(\W_t)$ for all $t \geq t_{2}$. We remark that the choice of $t_{2}$ depends on $\Lc(\W_{t_{1}})$ (whose magnitude is bounded using Lemma \ref{lem:l_fast_bound}), and $t_2$ can be used as $\bar{t}_2$ above. \textbf{To recap, $t_{1}$ is the time (after $t_0$) after which the successive loss decrease is lower bounded by the product $\eta_t \gamma \Gc(\W_t)$; $t_{2}$ (after $t_{1}$) is the time after which $\Lc(\W_t) \leq \frac{\log 2}{n}$ (thus, both $\Lc(\W_t) \leq 2 \Gc(\W_t)$ and separability condition \eqref{eq:sep} hold for all $t \geq t_{2}$).}\\

The next step is to bound the max-norm of the iterates in terms of learning rates. The SignGD case is straightforward given $\ninf{\sign{\nabla \Lc(\W)}} = 1$. In the case of Adam, the important step is to show that $\Vb_t[c,j] \leq \mathcal{O}(\Lc(\W_t)^2)$ for all $c \in [k]$ and $j \in [d]$. Thus, $\Vb_t[c,j] \leq 1$ is feasible again due to loss monotonicity. To prove this, we use Lemma \ref{lem:G and gradient} that lower bounds $\ninf{\nabla \Lc(\W)}$ using $\Gc(\W)$ up to some constant. Here, we provide a summary of the lemmas (in sections \ref{sec: grad and hess} and \ref{sec: G and L}) on the properties of $\Lc(\W)$ and $\Gc(\W)$ that are used in proving the various key steps in the Proof Overview:
\begin{itemize}
    \item \textbf{Loss Monotonicity:} Lemma \ref{lem:hessian} is on the Taylor expansion of the loss; Lemma \ref{lem:hessian_bound} and \ref{lem:G_ratio} are for handling the second-order term in the expansion; Lemma \ref{lem:G and gradient} proves that $\none{\nabla \Lc(\W)} \geq \gamma \Gc(\W)$.
    \item \textbf{Unnormalized Margin:} Lemma \ref{lem:sep} is on the separability condition \eqref{eq:sep} implied by a low loss ($\Lc(\W) \leq \frac{\log 2}{n}$); Lemma \ref{lem:G and L} proves that $\frac{\Gc(\W)}{\Lc(\W)} \leq 1$.
    \item \textbf{Convergence of $\frac{\Gc(\W_t)}{\Lc(\W_t)}$:} Lemma \ref{lem:G and L} proves that $\frac{\Gc(\W_t)}{\Lc(\W_t)} \geq 1 - \frac{n \Lc(\W_t)}{2}$, and it provides the sufficient conditions that gaurantee $\Lc(\W) \leq 2 \Gc(\W)$; Lemma \ref{lem:l_fast_bound} provides a simple bound on the loss using the max-norm of the iterates.
    \item \textbf{Iterate Bound:} Lemma \ref{lem:G and gradient} proves that $2 B \cdot \Gc(\W) \leq \ninf{\nabla \Lc(\W)}$. 
\end{itemize}

\section{Proof of SignGD} \label{sec:app_sign}
In this section, we break the proof of implicit bias of SignGD into several parts following the arguments in the Proof Overview. Lemma \ref{lem:sign_descent} shows the descent properties of SignGD. It is used in Lemma \ref{lem:sign_unnormalized_margin} to lower bound the unnormalized margin, and in the proof of Theorem \ref{thm:signgd} to show the convergence of $\frac{\Gc(\W_t)}{\Lc(\W_t)}$.

\begin{lemma} [SignGD Descent] \label{lem:sign_descent}
Under the same setting as Theorem \ref{thm:signgd}, it holds for all $t\geq 0$,
\begin{align*}
    \Lc(\W_{t+1}) &\leq \Lc(\W_{t}) - \gamma \eta_t (1 - \alpha_{s_1}  \eta_t )\Gc(\W_t), 
\end{align*}
where $\alpha_{s_1}$ is some constant that depends on $B$ and $\gamma$.
\end{lemma}
\begin{proof}
By Lemma \ref{lem:hessian}, we let $\W' = \W_{t+1}$, $\W = \W_t$, $\Deltab_t = \W_{t+1} - \W_t$, and define $\W_{t,t+1,\zeta} := \W_t + \zeta (\W_{t+1} - \W_t)$. We choose $\zeta^*$ such that $\W_{t,t+1,\zeta^*}$ satisfies \eqref{eq:taylor_eq1}, we have:   
    \begin{align}
        \Lc(\W_{t+1}) &= \Lc(\W_t) + \underbrace{\inp{\nabla \Lc(\W_{t})}{\Deltab_t}}_{\spadesuit_t} \nn
        \nn \\
        &\quad+ \frac{1}{2n}\sum_{i\in[n]} \underbrace{\hb_i^\top\Deltab_t^\top\left(\diag{\sft{\W_{t,t+1,\gamma}\hb_i}}-\sft{\W_{t,t+1,\zeta^*}\hb_i}\sft{\W_{t,t+1,\zeta^*}\hb_i}^\top\right)\Deltab_t\,\hb_i}_{\clubsuit_t}\,. \label{eq:taylor}
    \end{align}
    For the $\spadesuit_t$ term, we have by Lemma \ref{lem:G and gradient}: 
    \begin{align*}
        \spadesuit_t = \inp{\nabla \Lc(\W_t)}{-\eta_t \frac{\nabla \Lc (\W_t)}{\sqrt{\nabla \Lc^2(\W_t)}}} = -\eta_t \none{\nabla \Lc(\W_t)} \leq -\eta_t \gamma G(\W_t)    
    \end{align*}
    For the $\clubsuit_t$ term, we let $\vb = \Deltab_t \hb_i$ and $\s = \sft{\W_{t,t+1,\zeta^*} \hb_i}$, and apply Lemma \ref{lem:hessian_bound} to obtain 
    \begin{align*}
    \clubsuit_t \leq 4 \| \Deltab_t \hb_i \|_{\infty}^2 (1 - \sfti{y_i}{\W_{t,t+1,\zeta^*} \hb_i}) \leq 4 \eta_t^2 B^2 (1 - \sfti{y_i}{\W_{t,t+1,\zeta^*} \hb_i}),
    \end{align*}
    where in the second inequality we have used $\| \Deltab_t \hb_i \|_{\infty} \leq \ninf{\Deltab_t} \| \hb_i \|_{1}$, $\| \hb_i \|_{1} \leq B$, and $\ninf{\Deltab_t}  = \eta_t \ninf{\sign{\nabla \Lc(\W_t)}} \leq \eta_t$. Putting these two pieces together, we obtain        
    \begin{align}
        \Lc(\W_{t+1}) &\leq \Lc(\W_t)  - \gamma \eta_t \Gc(\W_t) + 2 \eta_t^2 B^2 \frac{1}{n} \sum_{i \in [n]} (1 - \sfti{y_i}{\W_{t,t+1,\zeta^*} \hb_i}) \nn \\
        &= \Lc(\W_t) - \gamma \eta_t \Gc(\W_t) +  2 \eta_t^2  B^2 \Gc(\W_{t,t+1,\zeta^*}) \nn \\
        &\leq \Lc(\W_t) - \gamma \eta_t \Gc(\W_t) +  2 \eta_t^2  B^2 \sup_{\zeta \in [0,1]} \Gc(\W_{t,t+1,\zeta}) \nn \\
        &= \Lc(\W_t) - \gamma \eta_t \Gc(\W_t) +  2 \eta_t^2 B^2 \Gc(\W_{t}) \sup_{\zeta \in [0,1]} \frac{\Gc(\W_t + \zeta \Deltab_t)}{\Gc(\W_{t})} \nn \\
        &\stackrel{(a)}{\leq} \Lc(\W_t) - \gamma \eta_t \Gc(\W_t) +  2 \eta_t^2 B^2 \Gc(\W_{t}) \sup_{\zeta \in [0,1]} e^{2B \zeta \ninf{\Deltab_t}} \nn \\
        &\stackrel{(b)}{\leq} \Lc(\W_t) - \gamma \eta_t \Gc(\W_t) +  2 \eta_t^2 B^2 e^{2B\eta_0} \Gc(\W_{t}), \label{eq: sign_descent_eq1}
        % &\leq \Lc(\W_t) - \gamma \eta_t G(\W_t) +  2 \eta_t^2 \alpha^2 \beta^2 \Lc(\W_{t,t+1,\zeta}) \\
        % &\leq \Lc(\W_t) - \gamma \eta_t G(\W_t) +  2 \eta_t^2 \alpha^2 \beta^2 \max\{ \Lc(\W_{t}), \Lc(\W_{t+1}) \} 
        % &= \Lc(\W_t) - \gamma \eta_t G(\W_t) +  2 \eta_t^2 k^2 \alpha^2 \beta^2 \Lc(\W_t) \max\{ 1, \frac{\Lc(\W_{t+1})}{\Lc(\W_t) }\}
    \end{align}
    where (a) is by Lemma \ref{lem:G_ratio} and (b) is by $\ninf{\Deltab} \leq \eta_t$. Letting $\alpha_{s_1} = \frac{2 B^2 e^{2B\eta_0}}{\gamma}$, Eq. \eqref{eq: sign_descent_eq1} simplifies to: 
    \begin{align*}
        \Lc(\W_{t+1}) &\leq \Lc(\W_{t}) - \gamma \eta_t (1 - \alpha_{s_1}  \eta_t )\Gc(\W_t), 
    \end{align*}
    from which we observe that the loss starts to monotonically decrease after $\eta_t$ satisfies $\eta_t \leq \frac{1}{\alpha_{s_1}}$ for a decreasing learning rate schedule.
\end{proof}

For a decaying learning rate schedule, Lemma \ref{lem:sign_descent} implies that the loss monotonically decreases after a certain time. Thus, we know that the assumption of Lemma \ref{lem:sign_unnormalized_margin} can be satisfied. In the proof of Theorem  \ref{thm:signgd}, we will specify a concrete form of $\tilde{t}$ in Lemma \ref{lem:sign_unnormalized_margin}. 
\begin{lemma}[SignGD Unnormalized Margin] \label{lem:sign_unnormalized_margin}
    Suppose that there exist $\tilde{t}$ such that $\Lc(\W_t) \leq \frac{\log 2}{n}$ for all $t > \tilde{t}$, then we have 
    \begin{align*}
        \min_{i \in [n], c \neq y_i} (\eb_{y_i} - \eb_c)^T \W_t \hb_i \geq \gamma \sum_{s=\tilde{t}}^{t-1} \eta_s \frac{\Gc(\W_s)}{\Lc(\W_s)} - \alpha_{s_2} \sum_{s=\tilde{t}}^{t-1} \eta_s^2,
    \end{align*}
    where $\alpha_{s_2}$ is some constant that depends on $B$.
\end{lemma} 
\begin{proof}
    We let $\alpha_{s_2} = 2B e^{2B\eta_0}$, then from \eqref{eq: sign_descent_eq1}, we have for $t > \tilde{t}$: 
    \begin{align}
        \Lc(\W_{t+1}) &\leq \Lc(\W_t) - \gamma \eta_t \Gc(\W_t) + \alpha_{s_2} \eta_t^2 \Gc(\W_t) \nn \\
        &= \Lc(\W_t) \bigl( 1 - \gamma \eta_t \frac{\Gc(\W_t)}{\Lc(\W_t)} + \alpha_{s_2} \eta_t^2 \frac{\Gc(\W_t)}{\Lc(\W_t)} \bigr) \nn \\
        &\leq \Lc(\W_t) \exp \bigl( - \gamma \eta_t \frac{\Gc(\W_t)}{\Lc(\W_t)} + \alpha_{s_2} \eta_t^2 \frac{\Gc(\W_t)}{\Lc(\W_t)} \bigr) \nn \\
        &\leq \Lc(\W_{\tilde{t}}) \exp \bigl( - \gamma \sum_{s=\tilde{t}}^{t} \eta_s \frac{\Gc(\W_s)}{\Lc(\W_s)} + \alpha_{s_2} \sum_{s=\tilde{t}}^{t} \eta_s^2 \bigr). \nn \\ 
        &\leq \frac{\log 2}{n} \exp \bigl( - \gamma \sum_{s=\tilde{t}}^{t} \eta_s \frac{\Gc(\W_s)}{\Lc(\W_s)} + \alpha_{s_2} \sum_{s=\tilde{t}}^{t} \eta_s^2 \bigr),
        \label{eq:sign_unnormalized_margin_eq1}
    \end{align}
    where the penultimate inequality uses Lemma \ref{lem:G and L}, and the last inequality uses the assumption that $\Lc(\W_t) \leq \frac{\log 2}{n}$ for all $t \geq \tilde{t}$. Then, we have for all $t > \tilde{t}$: 
    \begin{align*}
        e^{-\min_{i \in [n], c \neq y_i} (\eb_{y_i} - \eb_c)^T \W_t \hb_i} &= \max_{i \in [n]} e^{-\min_{c \neq y_i} (\eb_{y_i} - \eb_c)^T \W_t \hb_i} \\
        &\stackrel{(a)}{\leq} \max_{i \in [n]} \frac{1}{\log 2}  \log \bigl( 1 + e^{-\min_{c \neq y_i} (\eb_{y_i} - \eb_c)^T \W_t \hb_i} \bigr) \\
        &\leq \max_{i \in [n]} \frac{1}{\log 2}  \log(1 + \sum_{c \neq y_i} e^{-(\eb_{y_i} - \eb_c)^T \W_t \hb_i}) \leq \frac{n \Lc(\W_t)}{\log 2} \\
        &\stackrel{(b)}{\leq} \exp \bigl( - \gamma \sum_{s=\tilde{t}}^{t-1} \eta_s \frac{\Gc(\W_s)}{\Lc(\W_s)} + \alpha_{s_2} \sum_{s=\tilde{t}}^{t-1} \eta_s^2 \bigr).
    \end{align*}
    (a) is by the following: the assumption $\Lc(\W_t) \leq \frac{\log 2}{n}$ implies that $\min_{c \neq y_i} (\eb_{y_i} - \eb_c)^T \W_t \hb_i \geq 0$ for all $i \in [n]$ by Lemma \ref{lem:sep}. We also know the inequality $\frac{\log(1 + e^{-z})}{e^{-z}} \geq \log 2$ holds for any $z \geq 0$. Then, for any $i \in [n]$, we can set $z = \min_{c \neq y_i} (\eb_{y_i} - \eb_c)^T \W_t \hb_i$ to obtain the desired inequality; and (b) is by \eqref{eq:sign_unnormalized_margin_eq1}. Finally, taking $\log$ on both sides leads to the result. 
\end{proof}

Next Lemma upper bounds the max-norm of SignGD iterates using learning rates. It is used in the proof of Theorem \ref{thm:signgd}. 
\begin{lemma} [SignGD $\ninf{\W_t}$] \label{lem:sign_Wt} For SignGD, we have for any $t > 0$ that
\begin{align*}
    \ninf{\W_t} \leq \ninf{\W_0} + \sum_{s=0}^{t-1} \eta_s.
\end{align*}    
\end{lemma}
\begin{proof}
    By the SignGD update rule \eqref{eq: signGD}, we have
    \begin{align*}
        \W_{t+1} = \W_0 - \sum_{s=0}^t \eta_s \sign{\nabla \Lc(\W_t)}.  
    \end{align*}
    This leads to $\ninf{\W_t} \leq \ninf{\W_0} + \sum_{s=0}^{t-1} \eta_s$\,.
\end{proof}

The main step in the proof of Theorem \ref{thm:signgd} is to determine the time that satisfies the assumption in Lemma \ref{lem:sign_unnormalized_margin} and show the convergence of $\frac{\Gc(\W_t)}{\Lc(\W_t)}$. After this, Lemma \ref{lem:sign_unnormalized_margin} and Lemma \ref{lem:sign_Wt} will be combined to obtain the final result.

\begin{theorem} \label{thm:signgd} Suppose that Assumption \ref{ass:sep}, \ref{ass:learning_rate_1}, and \ref{ass:data_bound} hold, then there exists $t_{s_2} = t_{s_2}(n, \gamma, B, \W_0)$ such that SignGD achieves the following for all $t > t_{s_2}$
\begin{align*}
     \left|\frac{\min_{i \in [n], c \neq y_i} (\e_{y_i} - \e_c)^T \W_t \hb_i}{\ninf{\W_t}} - \gamma\right| &\leq \mathcal{O}\Bigg(\frac{\sum_{s=t_{s_2}}^{t-1} \eta_s e^{-\frac{\gamma}{4} \sum_{\tau = t_{s_2}}^{s-1}\eta_{\tau}} + \sum_{s=0}^{t_{s_2}-1}\eta_s + \sum_{s = t_{s_2}}^{t-1}\eta_s^2}{\sum_{s=0}^{t-1} \eta_s}\Bigg).
\end{align*}
\end{theorem}

\begin{proof}
\textbf{Determination of $t_{s_1}$.} In Lemma \ref{lem:sign_descent} we choose $t_{s_1}$ such that $\eta_{t} \leq \frac{1}{2 \alpha_{s_1}}$ for all $t \geq t_{s_1}$.  Considering $\eta_t = \Theta(\frac{1}{t^a})$ (where $a \in (0,1]$), we set $t_{s_1} = (2 \alpha_{s_1})^{\frac{1}{a}} = (\frac{4 B^2 e^{2 B \eta_0}}{\gamma})^{\frac{1}{a}}$. Then, we have for all $t \geq t_{s_1}$
\begin{align}
    \Lc(\W_{t+1}) \leq \Lc(\W_t) - \frac{\eta_t \gamma}{2} \Gc(\W_t). \label{eq:signgd_main_eq1} 
\end{align}
Rearranging this equation and using non-negativity of the loss we obtain $\gamma \sum_{s = t_{s_1}}^t \eta_s \Gc(\W_s) \leq 2\Lc(\W_{t_{s_1}})$. \\
\textbf{Determination of $t_{s_2}$.} By Lemma \ref{lem:l_fast_bound}, we can bound $\Lc(\W_{t_{s_1}})$ as follows
\begin{align*}
    |\Lc(\W_{t_{s_1}}) - \Lc(\W_0)| \leq 2 B \ninf{\W_{t_{s_1}} - \W_0} \leq 2 B \sum_{s = 0}^{t_{s_1}-1} \eta_s \ninf{\sign{\nabla \Lc(\W_s)}}
    \leq 2 B \sum_{s = 0}^{t_{s_1}-1} \eta_s,
\end{align*}
where the last inequality is by Lemma \ref{lem:iter_bound}. Combining this with the result above and letting $\tilde{\Lc} \coloneqq \frac{\log 2}{n}$, we obtain
\begin{align*}
    \Gc(\W_{t^*}) = \min_{s \in [t_{s_1}, t_{s_2}]} \Gc(\W_s) \leq \frac{ 2 \Lc(\W_0) + 4B \sum_{s = 0}^{t_{s_1}-1} \eta_s}{\gamma \sum_{s=t_{s_1}}^{t_{s_2}} \eta_s} \leq \frac{\tilde{\Lc}}{2} \leq \frac{1}{2n}, 
\end{align*}
from which we derive the sufficient condition on $t_{s_2}$ to be $\sum_{s=t_{s_1}}^{t_{s_2}} \eta_s \geq \frac{4\Lc(\W_0) + 8 B \sum_{s = 0}^{t_{s_1}-1} \eta_s}{\gamma \tilde{\Lc}}$.\\ 
\textbf{Convergence of $\frac{\Gc(\W_t)}{\Lc(\W_t)}$} Given $\Gc(\W_{t^*}) \leq \frac{\tilde{\Lc}}{2} \leq \frac{1}{2n}$, we obtain that $\Lc(\W_t) \leq \Lc(\W_{t^*}) \leq 2 \Gc(\W_{t^*}) \leq \tilde{L}$ for all $t \geq t_{s_2}$, where the first and second inequalities are due to monotonicity in the risk and Lemma \ref{lem:G and L}, respectively. Thus, the other sufficient condition $\Lc(\W_t) \leq \frac{\log 2}{n}$ in Lemma \ref{lem:G and L} is satisfied, from which we conclude that $\Lc(\W_t) \leq 2 \Gc(\W_t)$ for all $t \geq t_{s_2}$.   
% we follow the arguments (specifically on why $\Lc(\W_t) \leq \tilde{L}$ for all $t \geq t_{s_2}$) in the Proof of the Convergence of $\frac{\Gc(\W_t)}{\Lc(\W_t)}$ to conclude that $\Lc(\W_t) \leq 2 \Gc(\W_t)$ for all $t \geq t_{s_2}$. 
Substituting this into \eqref{eq:signgd_main_eq1}, we obtain for all $t > t_{s_2}$ 
\begin{align*}
    \Lc(\W_{t}) \leq (1 - \frac{\gamma \eta_{t-1}}{4}) \Lc(\W_{t-1}) \leq \Lc(\W_{t_{s_2}}) e^{-\frac{\gamma}{4} \sum_{s = t_{s_2}}^{t-1} \eta_s} \leq \tilde{\Lc} e^{-\frac{\gamma}{4} \sum_{s = t_{s_2}}^{t-1} \eta_s}
\end{align*}
Then, by Lemma \ref{lem:G and L}, we obtain
\begin{align}
    \frac{\Gc(\W_t)}{\Lc(\W_t)} \geq 1 - \frac{n \Lc(\W_t)}{2} \geq 1 - \frac{n\tilde{\Lc} e^{-\frac{\gamma}{4} \sum_{s = t_{s_2}}^{t-1}\eta_s}}{2} \geq 1 - e^{-\frac{\gamma}{4} \sum_{s = t_{s_2}}^{t-1} \eta_s}. \label{eq:signgd_main_eq2} 
\end{align}\\
\textbf{Margin Convergence} Finally, we combine Lemma \ref{lem:sign_unnormalized_margin}, Lemma \ref{lem:sign_Wt}, and \eqref{eq:signgd_main_eq2} to obtain 
\begin{align*}
    |\frac{\min_{i \in [n], c \neq y_i} (\e_{y_i} - \e_c)^T \W_t \hb_i}{\ninf{\W_t}} - \gamma| &\leq \frac{\gamma \bigl(\ninf{\W_0} + \sum_{s=t_{s_2}}^{t-1} \eta_s e^{-\frac{\gamma}{4} \sum_{\tau = t_{s_2}}^{s-1}\eta_{\tau}} + \sum_{s=0}^{t_{s_2}-1}\eta_s \bigr) + \alpha_{s_2} \sum_{s = t_{s_2}}^{t-1}\eta_s^2}{\ninf{\W_0} + \sum_{s=0}^{t-1} \eta_s} \\
    &\leq \mathcal{O}(\frac{\sum_{s=t_{s_2}}^{t-1} \eta_s e^{-\frac{\gamma}{4} \sum_{\tau = t_{s_2}}^{s-1}\eta_{\tau}} + \sum_{s=0}^{t_{s_2}-1}\eta_s + \sum_{s = t_{s_2}}^{t-1}\eta_s^2}{\sum_{s=0}^{t-1} \eta_s})
\end{align*}
\end{proof}

Next, we explicitly upper bound $t_{s_2}$ in Theorem \ref{thm:signgd} and derive the margin convergence rates of SignGD.

\begin{corollary} \label{cor:sign_gd}
    Consider learning rate schedule of the form $\eta_t = \Theta(\frac{1}{t^a})$ where $a \in (0,1]$, under the same setting as Theorem \ref{thm:signgd}, then we have for SignGD 
    \[ 
   |\frac{\min_{i \in [n], c \neq y_i} (\e_{y_i} - \e_c)^T \W_t \hb_i}{\ninf{\W_t}} - \gamma| = \left\{
    \begin{array}{ll}
           \mathcal{O} (\frac{t^{1-2a}+n}{t^{1-a}}) &  \text{if} \quad a < \frac{1}{2}\\
            \mathcal{O} (\frac{\log t + n}{t^{1/2}}) &  \text{if}\quad a=\frac{1}{2} \\
            \mathcal{O} (\frac{n}{t^{1-a}}) &  \text{if}\quad \frac{1}{2} < a<1 \\
            \mathcal{O} (\frac{n}{\log t}) &  \text{if} \quad a=1
    \end{array} 
    \right. 
    \]
\end{corollary}

\begin{proof}
    Recall that $t_{s_1} = (\frac{4 B^2 e^{2 B \eta_0}}{\gamma})^{\frac{1}{a}} =: C_{s_1}$, and the condition on $t_{s_2}$ is 
    $\sum_{s=t_{s_1}}^{t_{s_2}} \eta_s \geq \frac{4\Lc(\W_0) + 8 B \sum_{s = 0}^{t_{s_1}-1} \eta_s}{\gamma \tilde{\Lc}}$, where $\tilde{L} = \frac{\log 2}{n}$. We can apply integral approximations to the terms that involve sums of learning rates to obtain 
    \begin{align*}
        t_{s_2} \leq C_{s_2} n^{\frac{1}{1-a}} t_{s_1} + C_{s_3} n^{\frac{1}{1-a}} \Lc(\W_0)^{\frac{1}{1-a}}.
    \end{align*}
    Given $t_{s_1}$ is some constant, this further implies that
    \begin{align*}
        \sum_{s=0}^{t_{s_2}-1} \eta_s = \mathcal{O}(t_{s_2}^{1-a} ) = \mathcal{O}(n + n \Lc(\W_0)).
    \end{align*}
    Next, we focus on the term $\sum_{s=t_{s_2}}^{t-1} \eta_s^2$. For $a > \frac{1}{2}$, this term can be bounded by some constant. For $a < \frac{1}{2}$, we have $\sum_{s=t_{s_2}}^{t-1} \eta_s^2 = \mathcal{O}(t^{1 - 2a})$, and it evaluates to $\mathcal{O}(\log t)$ for $a = \frac{1}{2}$. Finally, we have that $\sum_{s=0}^{t-1} \eta_s = \mathcal{O}(t^{1-a})$ for $a < 1$ and $\sum_{s=0}^{t-1} \eta_s = \mathcal{O}(\log t)$ for $a = 1$. The rest arguments can be found in \citet[Corollary 4.7 and Lemma C.1]{zhang2024implicit},  including showing the learning rate schedule $\eta_t = \frac{1}{(t+2)^a}$ satisfying Assumption \ref{ass:learning_rate_2}, and the term $\sum_{s=t_{s_2}}^{t-1} \eta_s e^{-\frac{\gamma}{4} \sum_{\tau = t_{s_2}}^{s-1}\eta_{\tau}}$ is bounded by some constant 
    for all $a \in (0,1]$. 
\end{proof}

\section{Proof of Adam} \label{sec:sec_adam}
The proof of Adam follows the similar approach as SignGD. The key challenge is to connect $\M_t$ and $\Vb_t$ to a per-class decomposition of $\Gc(\W_t)$. The following Lemma in \cite[Lemma 6.5]{zhang2024implicit} is useful. It provides an entry-wise bound on the ratio between the first moment and square root of the second moment. 
\begin{lemma} \label{lem:iter_bound}
    Considering the Adam updates given in \eqref{eq: adam1}, \eqref{eq: adam2}, and \eqref{eq: adam3}, suppose that $\beta_1 \leq \beta_2$ and set $\alpha = \sqrt{\frac{\beta_2(1-\beta_1)^2}{(1 - \beta_2) (\beta_2 - \beta_1^2)^2}}$, then we obtain $\M_{t} [c,j] \leq \alpha \cdot \sqrt{\Vb_{t}[c,j]}$ for all $c \in [k]$ and $j \in [d]$. 
\end{lemma}

The following Lemma bounds the first moment buffer ($\M_t$) of Adam in terms of the product of $\eta_t$ with $\Gc_c(\W_t)$ and $\Qc_c(\W_t)$. It is used in the proof of Lemma \ref{lem:adam_intermediate_bound}. 

\begin{lemma} \label{lem:first_G} Let $c \in [k]$. Under the same setting as Theorem \ref{thm:adam}, there exists a time $t_0$ such that the following holds for all $t \geq t_0$ 
\begin{align*}
    |\mathbf{M}_t[c,j] - (1-\beta_{1}^{t+1})\nabla \Lc(\W_t)[c,j]| &\leq \alpha_M \eta_t (\Gc_c(\W_t) + \Qc_c(\W_t)),
\end{align*}
where $j \in [d]$ and $\alpha_M$ is some constant that depends on $B$ and $\beta_1$. 
\end{lemma}
% \begin{proof}
% For any fixed $c \in [k]$ and $j \in [d]$,
%     \begin{align}
%         |\mathbf{M}_t[c,j] - (1-\beta_{1}^{t+1})\nabla \Lc(\W_t)[c,j]| &= |\sum_{\tau=0}^{t} (1-\beta_1) \beta_1^{\tau} \bigl(\nabla \Lc (\W_{t-\tau})[c,j] - \nabla \Lc (\W_{t}) [c,j]\bigr)| \nn \\
%         &\leq \sum_{\tau=0}^{t} (1-\beta_1) \beta_1^{\tau} \underbrace{|\nabla \Lc (\W_{t -\tau})[c,j] - \nabla \Lc (\W_{t}) [c,j]|}_{\clubsuit}. \label{eq: first_G_eq1}
%     \end{align}
% We first notice that for any  $\W\in\R^{k\times d}$, we have $\nabla \Lc (\W)[c,j] = \e_c^T \nabla \Lc (\W) \e_j = -\frac{1}{n} \sum_{i \in [n]} \e_c^T \bigl( \e_{y_i} - \sft{\W \hb_i} \bigr) \hb_i^T \e_j = -\frac{1}{n} \sum_{i \in [n]} \e_c^T \bigl( \e_{y_i} - \sft{\W \hb_i} \bigr) h_{ij}$. 
% Then, the gradient difference term becomes 
%     \begin{align*}
%         \clubsuit &= |-\frac{1}{n} \sum_{i \in [n]} \e_c^T \bigl( \e_{y_i} - \sft{\W_{t - \tau} \hb_i} \bigr) h_{ij} + \frac{1}{n} \sum_{i \in [n]} \e_c^T \bigl( \e_{y_i} - \sft{\W_t \hb_i} \bigr) h_{ij}| \\ 
%         &= |\frac{1}{n}\sum_{i \in [n]} \e_c^T\bigl(\sft{\W_{t-\tau}\hb_i} - \sft{\W_{t}\hb_i}\bigr)h_{ij}| \\
%         &= |\frac{1}{n} \sum_{i \in [n]} \bigl( \sfti{c}{\W_{t - \tau}\hb_i} - \sfti{c}{\W_{t}\hb_i} \bigr) h_{ij}| \\
%         &\leq B \frac{1}{n} \sum_{i \in [n]} |\sfti{c}{\W_{t - \tau}\hb_i} - \sfti{c}{\W_{t}\hb_i}| \\
%         &= B \underbrace{\frac{1}{n} \sum_{i \in [n], y_i \neq c} |\sfti{c}{\W_{t - \tau}\hb_i} - \sfti{c}{\W_{t}\hb_i}|}_{\clubsuit_1} + B \underbrace{\frac{1}{n} \sum_{i \in [n], y_i = c} |\sfti{c}{\W_{t  - \tau}\hb_i} - \sfti{c}{\W_{t}\hb_i}|}_{\clubsuit_2}
%     \end{align*}
% Next, we link the $\clubsuit_1$ and $\clubsuit_2$ terms with $\Gc(\W)$. Starting with the first term, we obtain: 
% \begin{align*}
%     \clubsuit_1 &= \frac{1}{n} \sum_{i \in [n], y_i \neq c} \sfti{c}{\W_{t}\hb_i} |\frac{\sfti{c}{\W_{t - \tau}\hb_i}}{\sfti{c}{\W_{t}\hb_i}} - 1| \\
%     & \stackrel{(a)}{\leq} \frac{1}{n} \sum_{i \in [n], y_i \neq c} \sfti{c}{\W_{t}\hb_i} (e^{2 \lVert (\W_{t - \tau} - \W_t) \hb_i \rVert_{\infty}} - 1)\\
%     & \stackrel{(b)}{\leq} \frac{1}{n} \sum_{i \in [n], y_i \neq c} \sfti{c}{\W_{t}\hb_i} (e^{2 B  \ninf{\W_{t - \tau} - \W_t}} - 1) \\
%     & \stackrel{(c)}{\leq} \bigl( e^{2 B \sum_{s=1}^{\tau} \eta_{t-s} \ninf{\frac{\M_{t-s}}{\sqrt{\Vb_{t-s}}}}} - 1 \bigr) \bigl( \frac{1}{n} \sum_{i \in [n], y_i \neq c} \sfti{c}{\W_{t}\hb_i}  \bigr) \\ 
%     & \stackrel{(d)}{\leq} \bigl( e^{2 \alpha B \sum_{s=1}^{\tau} \eta_{t-s} } - 1 \bigr) \Gc(\W_t),
% \end{align*}
% where (a) is by Lemma \ref{lem:unified_helper}, (b) is by $\lVert \hb_i \rVert_{1} \leq B$ for all $i \in [n]$, (c) is by \eqref{eq: adam3} and triangle inequality, and $(d)$ is by Lemma \ref{lem:iter_bound} and the definition of $\Gc(\W_t)$. 
% % \Chen{Here, we still need to determine the form of $f(\eta_t)$. In the last inequality, recall that we have $\Gc(\W) \geq \frac{1}{n} \sum_{i\in[n]} \sfti{c(i)}{\W \hb_i}$, where $c(i) \neq y_i, \forall i \in [n]$. Here, we pick $c(i) = c$ for all $i$'s such that $y_i \neq c$.} 
% For the second term, we obtain: 
% \begin{align*}
%     \clubsuit_2 &= \frac{1}{n} \sum_{i \in [n], y_i = c} |\sfti{c}{\W_{t - \tau}\hb_i} - \sfti{c}{\W_{t}\hb_i}| \\
%     % &\leq \frac{1}{n} \sum_{i \in [n], y_i = c} |\sfti{c}{\W_{t - \tau}\hb_i} - \sfti{c}{\W_{t}\hb_i}| + \frac{1}{n} \sum_{i \in [n], y_i \neq c} |\sfti{y_i}{\W_{t - \tau}\hb_i} - \sfti{y_i}{\W_{t}\hb_i}|\\
%     % &=  \frac{1}{n} \sum_{i \in [n]} |\sfti{y_i}{\W_{t - \tau}\hb_i} - \sfti{y_i}{\W_{t}\hb_i}| \\
%     &=  \frac{1}{n} \sum_{i \in [n], y_i = c} |\sfti{y_i}{\W_{t - \tau}\hb_i} - 1 + 1 - \sfti{y_i}{\W_{t}\hb_i}| \\
%     &=  \frac{1}{n} \sum_{i \in [n], y_i = c} \bigl(1 - \sfti{y_i}{\W_{t}\hb_i}\bigr) | \frac{ \sfti{y_i}{\W_{t - \tau}\hb_i} - 1}{1 - \sfti{y_i}{\W_{t}\hb_i}} + 1| \\
%     % &=  \frac{1}{n} \sum_{i \in [n], y_i = c} \bigl(1 - \sfti{y_i}{\W_{t}\hb_i}\bigr) | 1 - \frac{ 1 - \sfti{y_i}{\W_{t - \tau}\hb_i}}{1 - \sfti{y_i}{\W_{t}\hb_i}}| \\
%     % &= \frac{1}{n} \sum_{i \in [n], y_i = c} \bigl(1 - \sfti{y_i}{\W_{t}\hb_i}\bigr) |\frac{ \sfti{y_i}{\W_{t - \tau}\hb_i} - \sfti{y_i}{\W_{t}\hb_i}}{1 - \sfti{y_i}{\W_{t}\hb_i}}| \\
%     % &\lesssim \frac{1}{n} \sum_{i \in [n], y_i = c} \bigl(1 - \sfti{y_i}{\W_{t}\hb_i}\bigr) |\frac{ \sfti{y_i}{\W_{t - \tau}\hb_i} - \sfti{y_i}{\W_{t}\hb_i}}{1 - \sfti{y_i}{\W_{t}\hb_i}}| \\
%     &=  \frac{1}{n} \sum_{i \in [n], y_i = c} \bigl(1 - \sfti{y_i}{\W_{t}\hb_i}\bigr) | \frac{ 1 - \sfti{y_i}{\W_{t - \tau}\hb_i}}{1 - \sfti{y_i}{\W_{t}\hb_i}} - 1| \\
%     & \stackrel{(e)}{\leq} \frac{1}{n} \sum_{i \in [n], y_i \neq c} \sfti{c}{\W_{t}\hb_i} (e^{2 \lVert (\W_{t - \tau} - \W_t) \hb_i \rVert_{\infty}} - 1) \\
%     & \stackrel{(f)}{\leq} \bigl( e^{2 \alpha B \sum_{s=1}^{\tau} \eta_{t-s} } - 1 \bigr) \Gc(\W_t),
% \end{align*}
% where (e) is by Lemma \ref{lem:unified_helper}, and (f) is by the same approach taken for $\clubsuit_1$. Based on the upper bounds for $\clubsuit_1$ and $\clubsuit_2$, we obtain the following: $\clubsuit \leq 2B \bigl( e^{2 \alpha B \sum_{s=1}^{\tau} \eta_{t-s} } - 1 \bigr) \Gc(\W_t)$. Then, we substitute this into \eqref{eq: first_G_eq1} to obtain: 
% \begin{align*}
%      |\mathbf{M}_t[c,j] - (1-\beta_{1}^{t+1})\nabla \Lc(\W_t)[c,j]| &\leq 2B(1-\beta_1) \Gc(\W_t) \sum_{\tau = 0}^t \beta_1^{\tau} \bigl( e^{2 \alpha B \sum_{s=1}^{\tau} \eta_{t-s} } - 1 \bigr) \\
%      & \stackrel{(g)}{\leq} 2B(1-\beta_1)c_2 \Gc(\W_t) \eta_t,
% \end{align*}
% where (g) is by the Assumption \ref{ass:learning_rate_2}. 
% \end{proof}

\begin{proof}
For any fixed $c \in [k]$ and $j \in [d]$,
    \begin{align}
        |\mathbf{M}_t[c,j] - (1-\beta_{1}^{t+1})\nabla \Lc(\W_t)[c,j]| &= |\sum_{\tau=0}^{t} (1-\beta_1) \beta_1^{\tau} \bigl(\nabla \Lc (\W_{t-\tau})[c,j] - \nabla \Lc (\W_{t}) [c,j]\bigr)| \nn \\
        &\leq \sum_{\tau=0}^{t} (1-\beta_1) \beta_1^{\tau} \underbrace{|\nabla \Lc (\W_{t -\tau})[c,j] - \nabla \Lc (\W_{t}) [c,j]|}_{\clubsuit}. \label{eq: first_G_eq1}
    \end{align}
We first notice that for any  $\W\in\R^{k\times d}$, we have $\nabla \Lc (\W)[c,j] = \e_c^T \nabla \Lc (\W) \e_j = -\frac{1}{n} \sum_{i \in [n]} \e_c^T \bigl( \e_{y_i} - \sft{\W \hb_i} \bigr) \hb_i^T \e_j = -\frac{1}{n} \sum_{i \in [n]} \e_c^T \bigl( \e_{y_i} - \sft{\W \hb_i} \bigr) h_{ij}$. 
Then, the gradient difference term becomes 
    \begin{align*}
        \clubsuit &= |-\frac{1}{n} \sum_{i \in [n]} \e_c^T \bigl( \e_{y_i} - \sft{\W_{t - \tau} \hb_i} \bigr) h_{ij} + \frac{1}{n} \sum_{i \in [n]} \e_c^T \bigl( \e_{y_i} - \sft{\W_t \hb_i} \bigr) h_{ij}| \\ 
        &= |\frac{1}{n}\sum_{i \in [n]} \e_c^T\bigl(\sft{\W_{t-\tau}\hb_i} - \sft{\W_{t}\hb_i}\bigr)h_{ij}| \\
        &= |\frac{1}{n} \sum_{i \in [n]} \bigl( \sfti{c}{\W_{t - \tau}\hb_i} - \sfti{c}{\W_{t}\hb_i} \bigr) h_{ij}| \\
        &\leq B \frac{1}{n} \sum_{i \in [n]} |\sfti{c}{\W_{t - \tau}\hb_i} - \sfti{c}{\W_{t}\hb_i}| \\
        &= B \underbrace{\frac{1}{n} \sum_{i \in [n], y_i \neq c} |\sfti{c}{\W_{t - \tau}\hb_i} - \sfti{c}{\W_{t}\hb_i}|}_{\clubsuit_1} + B \underbrace{\frac{1}{n} \sum_{i \in [n], y_i = c} |\sfti{c}{\W_{t  - \tau}\hb_i} - \sfti{c}{\W_{t}\hb_i}|}_{\clubsuit_2}
    \end{align*}
Next, we link the $\clubsuit_1$ and $\clubsuit_2$ terms with $\Gc(\W)$. Starting with the first term, we obtain: 
\begin{align*}
    \clubsuit_1 &= \frac{1}{n} \sum_{i \in [n], y_i \neq c} \sfti{c}{\W_{t}\hb_i} |\frac{\sfti{c}{\W_{t - \tau}\hb_i}}{\sfti{c}{\W_{t}\hb_i}} - 1| \\
    & \stackrel{(a)}{\leq} \frac{1}{n} \sum_{i \in [n], y_i \neq c} \sfti{c}{\W_{t}\hb_i} (e^{2 \lVert (\W_{t - \tau} - \W_t) \hb_i \rVert_{\infty}} - 1)\\
    & \stackrel{(b)}{\leq} \frac{1}{n} \sum_{i \in [n], y_i \neq c} \sfti{c}{\W_{t}\hb_i} (e^{2 B  \ninf{\W_{t - \tau} - \W_t}} - 1) \\
    & \stackrel{(c)}{\leq} \bigl( e^{2 B \sum_{s=1}^{\tau} \eta_{t-s} \ninf{\frac{\M_{t-s}}{\sqrt{\Vb_{t-s}}}}} - 1 \bigr) \bigl( \frac{1}{n} \sum_{i \in [n], y_i \neq c} \sfti{c}{\W_{t}\hb_i}  \bigr) \\ 
    & \stackrel{(d)}{\leq} \bigl( e^{2 \alpha B \sum_{s=1}^{\tau} \eta_{t-s} } - 1 \bigr) \Qc_c(\W_t),
\end{align*}
where (a) is by Lemma \ref{lem:unified_helper}, (b) is by $\lVert \hb_i \rVert_{1} \leq B$ for all $i \in [n]$, (c) is by \eqref{eq: adam3} and triangle inequality, and $(d)$ is by Lemma \ref{lem:iter_bound} and the definition of $\Gc(\W_t)$. 
% \Chen{Here, we still need to determine the form of $f(\eta_t)$. In the last inequality, recall that we have $\Gc(\W) \geq \frac{1}{n} \sum_{i\in[n]} \sfti{c(i)}{\W \hb_i}$, where $c(i) \neq y_i, \forall i \in [n]$. Here, we pick $c(i) = c$ for all $i$'s such that $y_i \neq c$.} 
For the second term, we obtain: 
\begin{align*}
    \clubsuit_2 &= \frac{1}{n} \sum_{i \in [n], y_i = c} |\sfti{c}{\W_{t - \tau}\hb_i} - \sfti{c}{\W_{t}\hb_i}| \\
    % &\leq \frac{1}{n} \sum_{i \in [n], y_i = c} |\sfti{c}{\W_{t - \tau}\hb_i} - \sfti{c}{\W_{t}\hb_i}| + \frac{1}{n} \sum_{i \in [n], y_i \neq c} |\sfti{y_i}{\W_{t - \tau}\hb_i} - \sfti{y_i}{\W_{t}\hb_i}|\\
    % &=  \frac{1}{n} \sum_{i \in [n]} |\sfti{y_i}{\W_{t - \tau}\hb_i} - \sfti{y_i}{\W_{t}\hb_i}| \\
    &=  \frac{1}{n} \sum_{i \in [n], y_i = c} |\sfti{y_i}{\W_{t - \tau}\hb_i} - 1 + 1 - \sfti{y_i}{\W_{t}\hb_i}| \\
    &=  \frac{1}{n} \sum_{i \in [n], y_i = c} \bigl(1 - \sfti{y_i}{\W_{t}\hb_i}\bigr) | \frac{ \sfti{y_i}{\W_{t - \tau}\hb_i} - 1}{1 - \sfti{y_i}{\W_{t}\hb_i}} + 1| \\
    % &=  \frac{1}{n} \sum_{i \in [n], y_i = c} \bigl(1 - \sfti{y_i}{\W_{t}\hb_i}\bigr) | 1 - \frac{ 1 - \sfti{y_i}{\W_{t - \tau}\hb_i}}{1 - \sfti{y_i}{\W_{t}\hb_i}}| \\
    % &= \frac{1}{n} \sum_{i \in [n], y_i = c} \bigl(1 - \sfti{y_i}{\W_{t}\hb_i}\bigr) |\frac{ \sfti{y_i}{\W_{t - \tau}\hb_i} - \sfti{y_i}{\W_{t}\hb_i}}{1 - \sfti{y_i}{\W_{t}\hb_i}}| \\
    % &\lesssim \frac{1}{n} \sum_{i \in [n], y_i = c} \bigl(1 - \sfti{y_i}{\W_{t}\hb_i}\bigr) |\frac{ \sfti{y_i}{\W_{t - \tau}\hb_i} - \sfti{y_i}{\W_{t}\hb_i}}{1 - \sfti{y_i}{\W_{t}\hb_i}}| \\
    &=  \frac{1}{n} \sum_{i \in [n], y_i = c} \bigl(1 - \sfti{y_i}{\W_{t}\hb_i}\bigr) | \frac{ 1 - \sfti{y_i}{\W_{t - \tau}\hb_i}}{1 - \sfti{y_i}{\W_{t}\hb_i}} - 1| \\
    & \stackrel{(e)}{\leq} \frac{1}{n} \sum_{i \in [n], y_i = c} \bigl(1 - \sfti{y_i}{\W_{t}\hb_i}\bigr) (e^{2 \lVert (\W_{t - \tau} - \W_t) \hb_i \rVert_{\infty}} - 1) \\
    & \stackrel{(f)}{\leq} \bigl( e^{2 \alpha B \sum_{s=1}^{\tau} \eta_{t-s} } - 1 \bigr) \Gc_c(\W_t),
\end{align*}
where (e) is by Lemma \ref{lem:unified_helper}, and (f) is by the same approach taken for $\clubsuit_1$. Based on the upper bounds for $\clubsuit_1$ and $\clubsuit_2$, we obtain the following: $\clubsuit \leq 2B \bigl( e^{2 \alpha B \sum_{s=1}^{\tau} \eta_{t-s} } - 1 \bigr) (\Gc_c(\W_t) + \Qc_c(\W_t))$. Then, we substitute this into \eqref{eq: first_G_eq1} to obtain: 
\begin{align*}
     |\mathbf{M}_t[c,j] - (1-\beta_{1}^{t+1})\nabla \Lc(\W_t)[c,j]| &\leq B (1-\beta_1) (\Gc_c(\W_t) + \Qc_c(\W_t)) \sum_{\tau = 0}^t \beta_1^{\tau} \bigl( e^{2 \alpha B \sum_{s=1}^{\tau} \eta_{t-s} } - 1 \bigr) \\
     & \stackrel{(g)}{\leq} B(1-\beta_1)c_2 \eta_t (\Gc_c(\W_t) + \Qc_c(\W_t)),
\end{align*}
where (g) is by the Assumption \ref{ass:learning_rate_2}. 
\end{proof}

The following Lemma bounds the first moment buffer ($\Vb_t$) of Adam in terms of the product of $\eta_t$ and with $\Gc_c(\W_t)$ and $\Qc_c(\W_t)$. It is used in the proof of Lemma \ref{lem:adam_intermediate_bound}. 

\begin{lemma} \label{lem:second_G} Let $c \in [k]$. Under the same setting as Theorem \ref{thm:adam}, there exists a time $t_0$ such that the following holds for all $t \geq t_0$ 
\begin{align*}
    \bigm| \sqrt{\Vb_t[c,j]} - \sqrt{(1 - \beta_2^{t+1})} |\nabla \Lc(\W_t) [c,j]| \bigm| 
    &\leq \alpha_V \sqrt{\eta_t} (\Qc_c(\W_t) + \Gc_c(\W_t)),
\end{align*}
where $j \in [d]$, and $\alpha_V$ is some constant that depends on $B$ and $\beta_2$. 
\end{lemma}
    % \begin{proof}
    % Consider any fixed $c \in [k]$ and $j \in [d]$,
    % \begin{align}
    %     |\Vb_t[c,j] - (1 - \beta_2^{t+1}) \nabla \Lc(\W_t) [c,j]^2| &= |\sum_{\tau=0}^t (1-\beta_2) \beta_2^{\tau} \bigl( \nabla \Lc(\W_{t-\tau})[c,j]^2 - \nabla \Lc (\W_t) [c,j]^2 \bigr)| \nn \\
    %     & \leq \sum_{\tau = 0}^t (1-\beta_2) \beta_2^{\tau} \underbrace{|\nabla \Lc (\W_{t-\tau}) [c,j]^2 - \nabla \Lc (\W_t)[c,j]^2|}_{\spadesuit}. \label{eq: second_G_eq1}
    % \end{align}
    % For any  $\W\in\R^{k\times d}$, recall that $-\frac{1}{n} \sum_{i \in [n]} \e_c^T \bigl( \e_{y_i} - \sft{\W \hb_i} \bigr) h_{ij}$. Then, we can obtain $\nabla \Lc (\W)[c,j]^2 = \frac{1}{n^2} \sum_{i \in [n]} \sum_{p \in [n]} h_{ij} h_{pj} (\delta_{cy_i} - \sfti{c}{ \W \hb_i}) (\delta_{cy_p} - \sfti{c}{\W \hb_p})$ where $\delta_{cy} = 1$ if and only if $c=y$. Next, we define the function $f_{c,i,p}$ to be $f_{c,i,p}(\W) := (\delta_{cy_i} - \sfti{c}{\W \hb_i}) (\delta_{cy_p} - \sfti{c}{\W \hb_p})$. Then, we have
    % \begin{align*}
    %     |f_{c,i,p}(\W_{t-\tau}) - f_{c,i,p}(\W_{t})| &= \delta_{c y_i} \bigl( \sfti{c}{\W_t \hb_p} - \sfti{c}{\W_{t - \tau} \hb_p}\bigr) + \delta_{c y_p} \bigl( \sfti{c}{\W_{t} \hb_i} - \sfti{c}{\W_{t-\tau} \hb_i}\bigr) \\
    %     &\quad \quad + \bigl( \sfti{c}{\W_{t - \tau} \hb_i}\sfti{c}{\W_{t - \tau} \hb_p} - \sfti{c}{\W_{t} \hb_i} \sfti{c}{\W_{t} \hb_p} \bigr)    
    % \end{align*}
    % We can substitute this result into $\spadesuit$ to obtain
    % \begin{align*}
    %     \spadesuit &= |\frac{1}{n^2} \sum_{i \in [n]} \sum_{p \in [n]} h_{ij} h_{pj} (f_{c,i,p}(\W_{t-\tau}) - f_{c,i,p}(\W_t))| \\
    %     &\leq \frac{B^2}{n^2} \sum_{i \in [n]} \sum_{p \in [n]}  |f_{c,i,p}(\W_{t-\tau}) - f_{c,i,p}(\W_{t})| \\
    %     &= B^2 \underbrace{\frac{1}{n^2} \sum_{i \in [n], y_i \neq c} \sum_{p \in [n], y_p \neq c} |f_{c,i,p}(\W_{t-\tau}) - f_{c,i,p}(\W_{t})|}_{\spadesuit_1} \\
    %     &\quad \quad +   B^2 \underbrace{ \frac{1}{n^2} \sum_{i \in [n], y_i \neq c} \sum_{p \in [n], y_p = c} |f_{c,i,p}(\W_{t-\tau}) - f_{c,i,p}(\W_{t})|}_{\spadesuit_2} \\ 
    %     &\quad \quad +  B^2 \underbrace{ \frac{1}{n^2} \sum_{i \in [n], y_i = c} \sum_{p \in [n], y_p \neq c} |f_{c,i,p}(\W_{t-\tau}) - f_{c,i,p}(\W_{t})|}_{\spadesuit_3} \\ 
    %     &\quad \quad +  B^2 \underbrace{ \frac{1}{n^2} \sum_{i \in [n], y_i = c} \sum_{p \in [n], y_p = c} |f_{c,i,p}(\W_{t-\tau}) - f_{c,i,p}(\W_{t})|}_{\spadesuit_4}
    % \end{align*}
    % We deal with the $4$ terms $\spadesuit_1, \spadesuit_2, \spadesuit_3$, and $\spadesuit_4$ separately. Starting with the first term, we have
    % \begin{align*}
    %     \spadesuit_1 &= \frac{1}{n^2} \sum_{i \in [n], y_i \neq c} \sum_{p \in [n], y_p \neq c} |\sfti{c}{\W_{t - \tau} \hb_i}\sfti{c}{\W_{t - \tau} \hb_p} - \sfti{c}{\W_{t} \hb_i} \sfti{c}{\W_{t} \hb_p} | \\
    %     &= \frac{1}{n^2} \sum_{i \in [n], y_i \neq c} \sum_{p \in [n], y_p \neq c} \sfti{c}{\W_{t} \hb_i} \sfti{c}{\W_{t} \hb_p} |\frac{\sfti{c}{\W_{t - \tau} \hb_i}\sfti{c}{\W_{t - \tau} \hb_p}}{\sfti{c}{\W_{t} \hb_i} \sfti{c}{\W_{t} \hb_p}}  - 1| \\
    %     &\stackrel{(a)}{\leq} \frac{1}{n^2} \sum_{i \in [n], y_i \neq c} \sum_{p \in [n], y_p \neq c} \sfti{c}{\W_{t} \hb_i} \sfti{c}{\W_{t} \hb_p} \bigl( e^{2 \bigl( \lVert (\W_{t - \tau} - \W_{t}) \hb_i \rVert_{\infty} + \lVert (\W_{t - \tau} - \W_{t}) \hb_p \rVert_{\infty} \bigr) } - 1\bigr) \\
    %     &\stackrel{(b)}{\leq} \frac{1}{n^2} \sum_{i \in [n], y_i \neq c} \sum_{p \in [n], y_p \neq c} \sfti{c}{\W_{t} \hb_i} \sfti{c}{\W_{t} \hb_p} \bigl( e^{4 B \ninf{\W_{t - \tau} - \W_t}}  -1 \bigr) \\
    %     &\stackrel{(c)}{\leq} \bigl( e^{4 B \sum_{s=1}^{\tau} \eta_{t-s} \ninf{\frac{\M_{t-s}}{\sqrt{\Vb_{t-s}}}}} - 1 \bigr) \frac{1}{n^2} \sum_{i \in [n], y_i \neq c} \sum_{p \in [n], y_p \neq c} \sfti{c}{\W_{t} \hb_i} \sfti{c}{\W_{t} \hb_p} \\
    %     &\stackrel{(d)}{\leq} \bigl( e^{4 B \alpha \sum_{s=1}^{\tau} \eta_{t-s}} - 1 \bigr) \Gc(\W_t)^2,
    % \end{align*}
    % where (a) is by Lemma \ref{lem:unified_helper}, (b) is by $\lVert \hb_i \rVert_{1} \leq B$ for all $i \in [n]$, (c) is by \eqref{eq: adam3} and the triangle inequality, and (d) is by Lemma \ref{lem:iter_bound} and the definition of $\Gc(\W_t)$. 
    % For the second term, we have
    % \begin{align*}
    %     \spadesuit_2 &= \frac{1}{n^2} \sum_{i \in [n], y_i \neq c} \sum_{p \in [n], y_p = c} |\bigl( \sfti{c}{\W_{t} \hb_i} - \sfti{c}{\W_{t-\tau} \hb_i}\bigr) \\
    %     &\quad \quad \quad \quad + \bigl( \sfti{c}{\W_{t - \tau} \hb_i}\sfti{c}{\W_{t - \tau} \hb_p} - \sfti{c}{\W_{t} \hb_i} \sfti{c}{\W_{t} \hb_p} \bigr)| \\
    %     &= \frac{1}{n^2} \sum_{i \in [n], y_i \neq c} \sum_{p \in [n], y_p = c} |\sfti{c}{\W_t \hb_i} \bigl( 1 - \sfti{c}{\W_t \hb_p} \bigr) - \bigl( 1 - \sfti{c}{\W_{t - \tau} \hb_p} \bigr) \sfti{c}{\W_{t - \tau} \hb_i}| \\
    %     &= \frac{1}{n^2} \sum_{i \in [n], y_i \neq c} \sum_{p \in [n], y_p = c} \sfti{c}{\W_t \hb_i} \bigl( 1 - \sfti{c}{\W_t \hb_p} \bigr)| 1 - \frac{\bigl( 1 - \sfti{c}{\W_{t - \tau} \hb_p} \bigr) \sfti{c}{\W_{t - \tau} \hb_i}}{\bigl( 1 - \sfti{c}{\W_t \hb_p} \bigr) \sfti{c}{\W_t \hb_i}}| \\
    %     &\leq \bigl( e^{4 B \alpha \sum_{s=1}^{\tau} \eta_{t-s}} - 1 \bigr) \Gc(\W_t)^2,
    % \end{align*}
    % where the last inequality is by Lemma \ref{lem:unified_helper} and the same steps taken for $\spadesuit_1$. 
    % The third term can be derived similarly as the second term and we can obtain the same bound as follows: 
    % \begin{align*}
    %     \spadesuit_3 &= \frac{1}{n^2} \sum_{i \in [n], y_i = c} \sum_{p \in [n], y_p \neq c} \sfti{c}{\W_t \hb_p} \bigl( 1 - \sfti{c}{\W_t \hb_i} \bigr)| 1 - \frac{\bigl( 1 - \sfti{c}{\W_{t - \tau} \hb_p} \bigr) \sfti{c}{\W_{t - \tau} \hb_i}}{\bigl( 1 - \sfti{c}{\W_t \hb_i} \bigr) \sfti{c}{\W_t \hb_p}}| \\
    %     &\leq \bigl( e^{4 B \alpha \sum_{s=1}^{\tau} \eta_{t-s}} - 1 \bigr) \Gc(\W_t)^2.
    % \end{align*}
    % For the fourth term, we obtain: 
    % \begin{align*}
    %      \spadesuit_4 &= \frac{1}{n^2} \sum_{i \in [n], y_i = c} \sum_{p \in [n], y_p = c} |\bigl( 1 - \sfti{c}{\W_{t - \tau} \hb_i} \bigr) \bigl( 1 - \sfti{c}{\W_{t - \tau} \hb_p} \bigr) - \bigl( 1 - \sfti{c}{\W_{t} \hb_i} \bigr) \bigl( 1 - \sfti{c}{\W_{t} \hb_p} \bigr)| \\
    %      &= \frac{1}{n^2} \sum_{i \in [n], y_i = c} \sum_{p \in [n], y_p = c} \bigl( 1 - \sfti{c}{\W_{t} \hb_i} \bigr) \bigl( 1 - \sfti{c}{\W_{t} \hb_p} \bigr) |\frac{\bigl( 1 - \sfti{c}{\W_{t - \tau} \hb_i} \bigr) \bigl( 1 - \sfti{c}{\W_{t - \tau} \hb_p} \bigr)} {\bigl( 1 - \sfti{c}{\W_{t} \hb_i} \bigr) \bigl( 1 - \sfti{c}{\W_{t} \hb_p} \bigr)} - 1| \\
    %      &\leq \bigl( e^{4 B \alpha \sum_{s=1}^{\tau} \eta_{t-s}} - 1 \bigr) \Gc(\W_t)^2,
    % \end{align*}
    % where the last inequality is by Lemma \ref{lem:unified_helper} and the same steps taken for $\spadesuit_1$. We combine the bounds for $\spadesuit_1$, $\spadesuit_2$, $\spadesuit_3$, and $\spadesuit_4$ to obtain: $\spadesuit \leq 4B^2  \bigl( e^{4 B \alpha \sum_{s=1}^{\tau} \eta_{t-s}} - 1 \bigr) \Gc(\W_t)^2$. Then, we substitute this into \eqref{eq: second_G_eq1} to obtain:
    % \begin{align*}
    %     |\Vb_t[c,j] - (1 - \beta_2^{t+1}) \nabla \Lc(\W_t) [c,j]^2| &\leq 4B^2(1-\beta_2) \Gc(\W_t)^2 \sum_{\tau = 0}^t \beta_2^{\tau} \bigl( e^{4 \alpha B \sum_{s=1}^{\tau} \eta_{t-s} } - 1 \bigr) \\
    %     &\leq 4B^2(1-\beta_2) c_2 \eta_t \Gc(\W_t)^2,
    % \end{align*}
    % where the last inequality is by the Assumption \ref{ass:learning_rate_2}. The final result follows from the fact that $|p-q|^2 \leq |p^2 - q^2|$ when both $p$ and $q$ are positive.
    % \end{proof}

    \begin{proof}
    Consider any fixed $c \in [k]$ and $j \in [d]$,
    \begin{align}
        |\Vb_t[c,j] - (1 - \beta_2^{t+1}) \nabla \Lc(\W_t) [c,j]^2| &= |\sum_{\tau=0}^t (1-\beta_2) \beta_2^{\tau} \bigl( \nabla \Lc(\W_{t-\tau})[c,j]^2 - \nabla \Lc (\W_t) [c,j]^2 \bigr)| \nn \\
        & \leq \sum_{\tau = 0}^t (1-\beta_2) \beta_2^{\tau} \underbrace{|\nabla \Lc (\W_{t-\tau}) [c,j]^2 - \nabla \Lc (\W_t)[c,j]^2|}_{\spadesuit}. \label{eq: second_G_eq1}
    \end{align}
    For any  $\W\in\R^{k\times d}$, recall that $-\frac{1}{n} \sum_{i \in [n]} \e_c^T \bigl( \e_{y_i} - \sft{\W \hb_i} \bigr) h_{ij}$. Then, we can obtain $\nabla \Lc (\W)[c,j]^2 = \frac{1}{n^2} \sum_{i \in [n]} \sum_{p \in [n]} h_{ij} h_{pj} (\delta_{cy_i} - \sfti{c}{ \W \hb_i}) (\delta_{cy_p} - \sfti{c}{\W \hb_p})$ where $\delta_{cy} = 1$ if and only if $c=y$. Next, we define the function $f_{c,i,p}$ to be $f_{c,i,p}(\W) := (\delta_{cy_i} - \sfti{c}{\W \hb_i}) (\delta_{cy_p} - \sfti{c}{\W \hb_p})$. Then, we have
    \begin{align*}
        |f_{c,i,p}(\W_{t-\tau}) - f_{c,i,p}(\W_{t})| &= \delta_{c y_i} \bigl( \sfti{c}{\W_t \hb_p} - \sfti{c}{\W_{t - \tau} \hb_p}\bigr) + \delta_{c y_p} \bigl( \sfti{c}{\W_{t} \hb_i} - \sfti{c}{\W_{t-\tau} \hb_i}\bigr) \\
        &\quad \quad + \bigl( \sfti{c}{\W_{t - \tau} \hb_i}\sfti{c}{\W_{t - \tau} \hb_p} - \sfti{c}{\W_{t} \hb_i} \sfti{c}{\W_{t} \hb_p} \bigr)    
    \end{align*}
    We can substitute this result into $\spadesuit$ to obtain
    \begin{align*}
        \spadesuit &= |\frac{1}{n^2} \sum_{i \in [n]} \sum_{p \in [n]} h_{ij} h_{pj} (f_{c,i,p}(\W_{t-\tau}) - f_{c,i,p}(\W_t))| \\
        &\leq \frac{B^2}{n^2} \sum_{i \in [n]} \sum_{p \in [n]}  |f_{c,i,p}(\W_{t-\tau}) - f_{c,i,p}(\W_{t})| \\
        &= B^2 \underbrace{\frac{1}{n^2} \sum_{i \in [n], y_i \neq c} \sum_{p \in [n], y_p \neq c} |f_{c,i,p}(\W_{t-\tau}) - f_{c,i,p}(\W_{t})|}_{\spadesuit_1} \\
        &\quad \quad +   B^2 \underbrace{ \frac{1}{n^2} \sum_{i \in [n], y_i \neq c} \sum_{p \in [n], y_p = c} |f_{c,i,p}(\W_{t-\tau}) - f_{c,i,p}(\W_{t})|}_{\spadesuit_2} \\ 
        &\quad \quad +  B^2 \underbrace{ \frac{1}{n^2} \sum_{i \in [n], y_i = c} \sum_{p \in [n], y_p \neq c} |f_{c,i,p}(\W_{t-\tau}) - f_{c,i,p}(\W_{t})|}_{\spadesuit_3} \\ 
        &\quad \quad +  B^2 \underbrace{ \frac{1}{n^2} \sum_{i \in [n], y_i = c} \sum_{p \in [n], y_p = c} |f_{c,i,p}(\W_{t-\tau}) - f_{c,i,p}(\W_{t})|}_{\spadesuit_4}
    \end{align*}
    We deal with the $4$ terms $\spadesuit_1, \spadesuit_2, \spadesuit_3$, and $\spadesuit_4$ separately. Starting with the first term, we have
    \begin{align*}
        \spadesuit_1 &= \frac{1}{n^2} \sum_{i \in [n], y_i \neq c} \sum_{p \in [n], y_p \neq c} |\sfti{c}{\W_{t - \tau} \hb_i}\sfti{c}{\W_{t - \tau} \hb_p} - \sfti{c}{\W_{t} \hb_i} \sfti{c}{\W_{t} \hb_p} | \\
        &= \frac{1}{n^2} \sum_{i \in [n], y_i \neq c} \sum_{p \in [n], y_p \neq c} \sfti{c}{\W_{t} \hb_i} \sfti{c}{\W_{t} \hb_p} |\frac{\sfti{c}{\W_{t - \tau} \hb_i}\sfti{c}{\W_{t - \tau} \hb_p}}{\sfti{c}{\W_{t} \hb_i} \sfti{c}{\W_{t} \hb_p}}  - 1| \\
        &\stackrel{(a)}{\leq} \frac{1}{n^2} \sum_{i \in [n], y_i \neq c} \sum_{p \in [n], y_p \neq c} \sfti{c}{\W_{t} \hb_i} \sfti{c}{\W_{t} \hb_p} \bigl( e^{2 \bigl( \lVert (\W_{t - \tau} - \W_{t}) \hb_i \rVert_{\infty} + \lVert (\W_{t - \tau} - \W_{t}) \hb_p \rVert_{\infty} \bigr) } - 1\bigr) \\
        &\stackrel{(b)}{\leq} \frac{1}{n^2} \sum_{i \in [n], y_i \neq c} \sum_{p \in [n], y_p \neq c} \sfti{c}{\W_{t} \hb_i} \sfti{c}{\W_{t} \hb_p} \bigl( e^{4 B \ninf{\W_{t - \tau} - \W_t}}  -1 \bigr) \\
        &\stackrel{(c)}{\leq} \bigl( e^{4 B \sum_{s=1}^{\tau} \eta_{t-s} \ninf{\frac{\M_{t-s}}{\sqrt{\Vb_{t-s}}}}} - 1 \bigr) \frac{1}{n^2} \sum_{i \in [n], y_i \neq c} \sum_{p \in [n], y_p \neq c} \sfti{c}{\W_{t} \hb_i} \sfti{c}{\W_{t} \hb_p} \\
        &\stackrel{(d)}{\leq} \bigl( e^{4 B \alpha \sum_{s=1}^{\tau} \eta_{t-s}} - 1 \bigr) \Qc_c(\W_t)^2,
    \end{align*}
    where (a) is by Lemma \ref{lem:unified_helper}, (b) is by $\lVert \hb_i \rVert_{1} \leq B$ for all $i \in [n]$, (c) is by \eqref{eq: adam3} and the triangle inequality, and (d) is by Lemma \ref{lem:iter_bound} and the definition of $\Gc(\W_t)$. 
    For the second term, we have
    \begin{align*}
        \spadesuit_2 &= \frac{1}{n^2} \sum_{i \in [n], y_i \neq c} \sum_{p \in [n], y_p = c} |\bigl( \sfti{c}{\W_{t} \hb_i} - \sfti{c}{\W_{t-\tau} \hb_i}\bigr) \\
        &\quad \quad \quad \quad + \bigl( \sfti{c}{\W_{t - \tau} \hb_i}\sfti{c}{\W_{t - \tau} \hb_p} - \sfti{c}{\W_{t} \hb_i} \sfti{c}{\W_{t} \hb_p} \bigr)| \\
        &= \frac{1}{n^2} \sum_{i \in [n], y_i \neq c} \sum_{p \in [n], y_p = c} |\sfti{c}{\W_t \hb_i} \bigl( 1 - \sfti{c}{\W_t \hb_p} \bigr) - \bigl( 1 - \sfti{c}{\W_{t - \tau} \hb_p} \bigr) \sfti{c}{\W_{t - \tau} \hb_i}| \\
        &= \frac{1}{n^2} \sum_{i \in [n], y_i \neq c} \sum_{p \in [n], y_p = c} \sfti{c}{\W_t \hb_i} \bigl( 1 - \sfti{c}{\W_t \hb_p} \bigr)| 1 - \frac{\bigl( 1 - \sfti{c}{\W_{t - \tau} \hb_p} \bigr) \sfti{c}{\W_{t - \tau} \hb_i}}{\bigl( 1 - \sfti{c}{\W_t \hb_p} \bigr) \sfti{c}{\W_t \hb_i}}| \\
        &\leq \bigl( e^{4 B \alpha \sum_{s=1}^{\tau} \eta_{t-s}} - 1 \bigr) \Qc_c(\W_t) \Gc_c(\W_t) ,
    \end{align*}
    where the last inequality is by Lemma \ref{lem:unified_helper} and the same steps taken for $\spadesuit_1$. 
    The third term can be derived similarly as the second term and we can obtain the same bound as follows: 
    \begin{align*}
        \spadesuit_3 &= \frac{1}{n^2} \sum_{i \in [n], y_i = c} \sum_{p \in [n], y_p \neq c} \sfti{c}{\W_t \hb_p} \bigl( 1 - \sfti{c}{\W_t \hb_i} \bigr)| 1 - \frac{\bigl( 1 - \sfti{c}{\W_{t - \tau} \hb_p} \bigr) \sfti{c}{\W_{t - \tau} \hb_i}}{\bigl( 1 - \sfti{c}{\W_t \hb_i} \bigr) \sfti{c}{\W_t \hb_p}}| \\
        &\leq \bigl( e^{4 B \alpha \sum_{s=1}^{\tau} \eta_{t-s}} - 1 \bigr) \Qc_c(\W_t) \Gc_c(\W_t).
    \end{align*}
    For the fourth term, we obtain: 
    \begin{align*}
         \spadesuit_4 &= \frac{1}{n^2} \sum_{i \in [n], y_i = c} \sum_{p \in [n], y_p = c} |\bigl( 1 - \sfti{c}{\W_{t - \tau} \hb_i} \bigr) \bigl( 1 - \sfti{c}{\W_{t - \tau} \hb_p} \bigr) - \bigl( 1 - \sfti{c}{\W_{t} \hb_i} \bigr) \bigl( 1 - \sfti{c}{\W_{t} \hb_p} \bigr)| \\
         &= \frac{1}{n^2} \sum_{i \in [n], y_i = c} \sum_{p \in [n], y_p = c} \bigl( 1 - \sfti{c}{\W_{t} \hb_i} \bigr) \bigl( 1 - \sfti{c}{\W_{t} \hb_p} \bigr) |\frac{\bigl( 1 - \sfti{c}{\W_{t - \tau} \hb_i} \bigr) \bigl( 1 - \sfti{c}{\W_{t - \tau} \hb_p} \bigr)} {\bigl( 1 - \sfti{c}{\W_{t} \hb_i} \bigr) \bigl( 1 - \sfti{c}{\W_{t} \hb_p} \bigr)} - 1| \\
         &\leq \bigl( e^{4 B \alpha \sum_{s=1}^{\tau} \eta_{t-s}} - 1 \bigr) \Gc_c(\W_t)^2,
    \end{align*}
    where the last inequality is by Lemma \ref{lem:unified_helper} and the same steps taken for $\spadesuit_1$. We combine the bounds for $\spadesuit_1$, $\spadesuit_2$, $\spadesuit_3$, and $\spadesuit_4$ to obtain: $\spadesuit \leq 4B^2  \bigl( e^{4 B \alpha \sum_{s=1}^{\tau} \eta_{t-s}} - 1 \bigr)  (\Gc_c(\W_t) + \Qc_c(\W_t))^2$. Then, we substitute this into \eqref{eq: second_G_eq1} to obtain:
    \begin{align*}
        |\Vb_t[c,j] - (1 - \beta_2^{t+1}) \nabla \Lc(\W_t) [c,j]^2| &\leq B^2(1-\beta_2) 
        (\Qc_c(\W_t) + \Gc_c(\W_t))^2 \sum_{\tau = 0}^t \beta_2^{\tau} \bigl( e^{4 \alpha B \sum_{s=1}^{\tau} \eta_{t-s} } - 1 \bigr) \\
        &\leq B^2(1-\beta_2) c_2 \eta_t (\Qc_c(\W_t) + \Gc_c(\W_t))^2,
    \end{align*}
    where the last inequality is by the Assumption \ref{ass:learning_rate_2}. The final result follows from the fact that $|p-q|^2 \leq |p^2 - q^2|$ when both $p$ and $q$ are positive.
    \end{proof}

The following Lemma bounds the term $\bigm| \langle \nabla \Lc(\W_t), \frac{\M_t}{\sqrt{\Vb_t}} - \frac{\nabla \Lc(\W_t)}{|\nabla \Lc(\W_t)|}\rangle \bigm|$ using $\Gc(\W_t)$.  It is used in Lemma \ref{lem:adam_descent} to show the decrease in the risk. The proof is similar to that of \citet[Lemma A.3]{zhang2024implicit}, but here we need to carefully track the index $c\in[k]$ using both $\Gc_c(\W)$ and $\Qc_c(\W)$ to avoid $k$ dependence. The final result crucially relies on the decomposition $\Gc(\W_t) = \sum_{c \in [k]} \Tc_c(\W_t) = \sum_{c \in [k]} \Qc_c(\W_t)$. 
% Here, we only highlight the steps that involve Lemma \ref{lem:first_G} and \ref{lem:second_G}. 

\begin{lemma} \label{lem:adam_intermediate_bound} 
Under the same setting as Theorem \ref{thm:signgd}, we have 
% \begin{align*}
%     \bigm| \langle \nabla \Lc(\W_t), \frac{\M_t}{\sqrt{\Vb_t}} - \frac{\nabla \Lc(\W_t)}{|\nabla \Lc(\W_t)|}\rangle \bigm| \leq 4 \sqrt{\frac{\beta_1^{t+1}}{1 - \beta_2^{t+1}}} \none{\nabla \Lc(\W_t)} + \frac{kd}{\sqrt{1-\beta_2}}\bigl( \frac{6 \alpha_V}{\sqrt{1 - \beta_2^{t+1}}}\sqrt{\eta_t} + 3 \alpha_M \eta_t\bigr) \Gc(\W_t). 
% \end{align*}    

\begin{align*}
    \bigm| \langle \nabla \Lc(\W_t), \frac{\M_t}{\sqrt{\Vb_t}} - \frac{\nabla \Lc(\W_t)}{|\nabla \Lc(\W_t)|}\rangle \bigm| &\leq 4 \sqrt{\frac{\beta_1^{t+1}}{1 - \beta_2^{t+1}}} \none{\nabla \Lc(\W_t)} + \\ 
    &\quad \quad \frac{2d}{\sqrt{1-\beta_2}}\bigl( \frac{6 \alpha_V}{\sqrt{1 - \beta_2^{t+1}}}\sqrt{\eta_t} + 3 \alpha_M \eta_t\bigr) \Gc(\W_t). 
\end{align*}    
\end{lemma}

\begin{proof} For simplicity, we drop the subscripts $t$.
    Denote $\Tc_c(\W) \coloneqq \Gc_c(\W) + \Qc_c(\W)$. Then, by Lemmas \ref{lem:first_G} and \ref{lem:second_G}, we have for any $c \in [k]$ and $j \in [d]$: 
    \begin{align}
        \M[c,j] &= (1 - \beta_1^{t+1}) \nabla \Lc(\W)[c,j] + \alpha_M \eta_t \Tc_c(\W) \epsilon_{m,c,j}, \label{eq:M_t}\\
        \sqrt{\Vb[c,j]} &= \sqrt{1-\beta_2^{t+1}} |\nabla \Lc(\W)[c,j]| + \alpha_V \sqrt{\eta_t} \Tc_c(\W) \epsilon_{v,c,j},
        \label{eq:V_t}
    \end{align}
    where $|\epsilon_{m,c,j}| \leq 1$ and $|\epsilon_{v,c,j}| \leq 1$ are some residual terms. We denote $\psi_{c,j} \coloneqq \nabla \Lc(\W)[c,j](\frac{\M[c,j]}{\sqrt{\Vb[c,j]}} - \frac{\nabla \Lc(\W)[c,j]}{|\nabla \Lc(\W)[c,j]|})$, the set of index $E_{c,j} \coloneqq \{ j \in [d] \Bigm|  \sqrt{1 - \beta_2^{t+1}}|\nabla \Lc(\W)[c,j]| \geq 2 \alpha_V \sqrt \eta_t \Tc_c(\W) |\epsilon_{v,c,j}|\}$, and its complement $E_{c,j}^c = [d] \backslash E_{c,j}$. The goal is to bound $|\psi_{c,j}|$ when $j \in E_{c,j}^c$ or $j \in E_{c,j}$ using $\Tc_c(\W)$. We start with the indices in $E_{c,j}^c$:
    \begin{align*}
        \sum_{j \in E_{c,j}^c} |\psi_{c,j}| &\leq \sum_{j \in E_{c,j}^c} |\nabla \Lc(\W)[c,j]| \bigl( \frac{|\M[c,j]|}{\sqrt{\Vb[c,j]}} + 1 \bigr) \\
        &\stackrel{(a)}{\leq} \sum_{j \in E_{c,j}^c} 
        |\nabla \Lc(\W)[c,j]| \bigl( \frac{(1 - \beta_1^{t+1})|\nabla \Lc(\W)[c,j]| + \alpha_M \eta_t \Tc_c(\W)}{\sqrt{1 - \beta_2} |\nabla \Lc(\W)[c,j]|} + 1\bigr)\\
        &\stackrel{(b)}{\leq} \sum_{j \in E_{c,j}^c} 
        (\frac{1 - \beta_1^{t+1}}{\sqrt{1 - \beta_2}} + 1)\frac{2\alpha_V \sqrt{\eta_t} \Tc_c(\W)}{\sqrt{1 - \beta_2^{t+1}}} + \frac{\alpha_M \eta_t \Tc_c(\W)}{\sqrt{1 - \beta_2}}\\
        &\leq \frac{d}{\sqrt{1-\beta_2}}\bigl( \frac{4 \alpha_V}{\sqrt{1 - \beta_2^{t+1}}}\sqrt{\eta_t} + \alpha_M \eta_t\bigr) \Tc_c(\W),
    \end{align*}
    where (a) is by \eqref{eq:M_t}, $|\epsilon_{m,c,j}| \leq 1$, and $\Vb[c,j] \geq (1-\beta_2) \nabla \Lc(\W)[c,j]^2$; and (b) is by $j \in E^c_{c,j}$ s.t. $|\nabla \Lc(\W)[c,j]| \leq \frac{2\alpha_V \sqrt{\eta_t} \Tc_c(\W)}{\sqrt{1-\beta_2^{t+1}}}$. Next, we focus on the indices $j \in E_{c,j}$. In this case, we have
    \begin{align*}
        \psi_{c,j} &= \nabla \Lc(\W)[c,j] \bigl( \frac{\M[c,j]}{\sqrt{1-\beta_2^{t+1}}|\nabla \Lc(\W)[c,j]| + \alpha_V \sqrt{\eta_t} \Tc_c(\W) \epsilon_{v,c,j}} - \frac{\Lc(\W)[c,j]}{|\nabla \Lc(\W)[c,j]|}\bigr) \\
        &=  \nabla \Lc(\W)[c,j]  \underbrace{\frac{\M[c,j]|\nabla \Lc(\W)[c,j]| - \bigl(\sqrt{1-\beta_2^{t+1}}|\nabla \Lc(\W)[c,j]| + \alpha_V \sqrt{\eta_t} \Tc_c(\W) \epsilon_{v,c,j} \bigr) \nabla \Lc(\W)[c,j]}{\bigl(\sqrt{1-\beta_2^{t+1}}|\nabla \Lc(\W)[c,j]| + \alpha_V \sqrt{\eta_t} \Tc_c(\W) \epsilon_{v,c,j}\bigr)|\nabla \Lc(\W)[c,j]|}}_{\frac{\spadesuit_1}{\spadesuit_2}},
    \end{align*}
    where 
    \begin{align*}
        \Bigm| \spadesuit_1 \Bigm| &= \Bigm| \bigl( 1 - \beta_1^{t+1} - \sqrt{1-\beta_2^{t+1}}\bigr) \nabla \Lc(\W)[c,j] |\nabla \Lc(\W)[c,j]| + \\
        &\quad \quad \alpha_M \eta_t \Tc_c(\W) \epsilon_{m,c,j} | \nabla \Lc(\W)[c,j]| - \alpha_V \sqrt{\eta_t} \Tc_c(\W) \epsilon_{v,c,j} \nabla \Lc(\W) [c,j]\Bigm| \\
        &\stackrel{(c)}{\leq} \bigm| 1 - \beta_1^{t+1} - \sqrt{1-\beta_2^{t+1}} | | \nabla \Lc(\W) [c,j] |^3 + (\alpha_M \eta_t + \alpha_V \sqrt{\eta_t}) \Tc_c(\W) |\nabla \Lc(\W)[c,j]|^2, 
    \end{align*}
    and 
    \begin{align*}
        \Bigm| \spadesuit_2 \Bigm| = \spadesuit_2 \stackrel{(d)}{\geq} \frac{1}{2} \sqrt{1 - \beta_2^{t+1}} |\nabla \Lc(\W)[c,j]|^2.
    \end{align*}
    Inequality (c) is by $|\epsilon_{m,c,j}| \leq 1$ and $|\epsilon_{v,c,j}| \leq 1$, and (d) is by $\alpha_V \sqrt{\eta_t} \Tc_c(\W) \epsilon_{v,c,j} \geq -\frac{1}{2} \sqrt{1 - \beta_2^{t+1}} |\nabla \Lc(\W)[c,j]|$ for any $j \in E_{c,j}$. Putting these two pieces together, we obtain
    \begin{align*}
         \sum_{j \in E_{c,j}} |\psi_{c,j}| &\leq \sum_{j \in E_{c,j}} \frac{|1 - \beta_1^{t+1} - \sqrt{1-\beta_2^{t+1}}||\nabla \Lc(\W) [c,j]|^3 + (\alpha_M \eta_t + \alpha_V \sqrt{\eta_t}) \Tc_c(\W) |\nabla \Lc(\W)[c,j]|^2}{\frac{1}{2} \sqrt{1 - \beta_2^{t+1}} |\nabla \Lc(\W)[c,j]|^2}\\
         &\stackrel{(e)}{\leq}  \Bigl( \sum_{j \in E_{c,j}} 4\sqrt{\frac{\beta_1^{t+1}}{1 - \beta_2^{t+1}}}|\nabla \Lc(\W)[c,j]| \Bigr) + d \bigl( \frac{2 \alpha_V}{\sqrt{1 - \beta_2^{t+1}}} \sqrt{\eta_t} + \frac{2 \alpha_M}{\sqrt{1 - \beta_2^{t+1}}} \eta_t \bigr) \Tc_c(\W) \\
         &\leq 4 \sqrt{\frac{\beta_1^{t+1}}{1 - \beta_2^{t+1}}} \none{\nabla \Lc(\W)[c,:]} + \frac{d}{\sqrt{1-\beta_2}}\bigl( \frac{2 \alpha_V}{\sqrt{1 - \beta_2^{t+1}}}\sqrt{\eta_t} + 2\alpha_M \eta_t\bigr) \Tc_c(\W),
    \end{align*}
    where (e) is by $\sqrt{a} \leq \sqrt{a-b} + \sqrt{b}$ implying $1 - \sqrt{1 - \beta_2^{t+1}} \leq \beta_1^{\frac{t+1}{2}}$, and $\nabla \Lc(\W)[c,:]$ denotes the $c$th row of $\nabla \Lc(\W)$. Finally, we note that 
    $|\langle \nabla \Lc(\W), \frac{\M}{\sqrt{\Vb}} - \frac{\nabla \Lc(\W)}{|\nabla \Lc(\W)|}\rangle |  = | \sum_{(c,j)} \psi_{c,j}| \leq \sum_{(c,j)} |\psi_{c,j}|$. Then, we obtain
    \begin{align*}
        \sum_{c,j} |\psi_{c,j}| &= \sum_{c \in [k]} \bigl( \sum_{j \in E^c_{c,j}} |\psi_{c,j}| + \sum_{j \in E_{c,j}} |\psi_{c,j}| \bigr) \\
        &= \sum_{c \in [k]} 4\sqrt{\frac{\beta_1^{t+1}}{1 - \beta_2^{t+1}}} \none{\nabla \Lc(\W)[c,:]} + \sum_{c \in [k]} \frac{d}{\sqrt{1-\beta_2}}\bigl( \frac{2 \alpha_V}{\sqrt{1 - \beta_2^{t+1}}}\sqrt{\eta_t} + 2\alpha_M \eta_t\bigr) \Tc_c(\W) \\
        &\stackrel{(f)}{=}4\sqrt{\frac{\beta_1^{t+1}}{1 - \beta_2^{t+1}}} \none{\nabla \Lc(\W)} + \frac{2d}{\sqrt{1-\beta_2}}\bigl( \frac{2 \alpha_V}{\sqrt{1 - \beta_2^{t+1}}}\sqrt{\eta_t} + 2\alpha_M \eta_t\bigr) \Gc(\W),
    \end{align*}
    where (f) is by $\sum_{c \in [k]} \Tc_c(\W) = \sum_{c \in [k]} \Qc_c(\W) + \Gc_c(\W) = 2 \Gc(\W)$. 
\end{proof}

% \begin{proof}
%     By Lemmas \ref{lem:first_G} and \ref{lem:second_G}, we have for any $c \in [k]$ and $j \in [d]$: 
%     \begin{align*}
%         \M_t[c,j] &= (1 - \beta_1^{t+1}) \nabla \Lc(\W_t)[c,j] + \alpha_M \eta_t \Gc(\W_t) \epsilon_{t,m,c,j}, \\
%         \sqrt{\Vb_t[c,j]} &= \sqrt{1-\beta_2^{t+1}} |\nabla \Lc(\W_t)[c,j]| + \alpha_V \sqrt{\eta_t} \Gc(\W_t) \epsilon_{t,v,c,j},
%     \end{align*}
%     where $|\epsilon_{t,m,c,j}| \leq 1$ and $|\epsilon_{t,v,c,j}| \leq 1$ are some residual terms. These equalities replace the ratio $\frac{\M_t[c,j]}{\sqrt{\Vb_t[c,j]}}$ with terms that only involve $\nabla \Lc(\W_t)$ and $\Gc(\W_t)$. The rest of the proof follows the same steps in \cite[Lemma A.3] {zhang2024implicit}. 
% \end{proof}

\begin{lemma}[Adam Descent] \label{lem:adam_descent}
    Under the same setting as Theorem \ref{thm:adam}, set $t_A \coloneqq \frac{2 \log(\frac{\sqrt{1-\beta_2}}{4})}{\log(\beta_1)}$, then we have for all $t \geq t_A$
\begin{align*}
\Lc(\W_{t+1}) &\leq  \Lc(\W_t) - \eta_t \gamma \bigl( 1 - \alpha_{a_1} \beta_1^{t/2} - \alpha_{a _2} d \eta_{t}^{\frac{1}{2}} - \alpha_{a_3} d \eta_t\bigr) \Gc(\W_t), 
\end{align*}
where $\alpha_{a_1}$, $\alpha_{a_2}$, and $\alpha_{a_3}$ are some constants that depend on $B$, $\gamma$,$\beta_1$, and $\beta_2$.
\end{lemma}
\begin{proof}
    We follow the same notations and strategy of Lemma \ref{lem:sign_descent}, and recall the definitions $\spadesuit_t = \inp{\nabla \Lc(\W_{t})}{\Deltab_t}$ and $\clubsuit_t = \hb_i^\top\Deltab_t^\top\left(\diag{\sft{\W_{t,t+1,\gamma}\hb_i}}-\sft{\W_{t,t+1,\zeta^*}\hb_i}\sft{\W_{t,t+1,\zeta^*}\hb_i}^\top\right)\Deltab_t\,\hb_i$. In the case of Adam, we have $\Deltab_t = \frac{\M_t}{\sqrt{\Vb_t}}$. We bound $\spadesuit_t$ and $\clubsuit_t$ separately. Starting with $\spadesuit_t$, we have for all $t \geq t_{A}$
    \begin{align*}
        \spadesuit_t &= -\eta_t \langle \nabla \Lc(\W_t), \frac{\M_t}{\sqrt{\Vb_t}}\rangle \\
        &= -\eta_t \bigl( \langle \nabla \Lc(\W_t), \frac{\M_t}{\sqrt{\Vb_t}} - \frac{\nabla \Lc(\W_t)}{|\nabla \Lc(\W_t)|}\rangle + \langle \nabla \Lc(\W_t), \frac{\nabla \Lc(\W_t)}{|\nabla \Lc(\W_t)|}\rangle \bigr) \\
        &\leq -\eta_t \none{\nabla \Lc(\W_t)} + \eta_t \bigm| \langle \nabla \Lc(\W_t), \frac{\M_t}{\sqrt{\Vb_t}} - \frac{\nabla \Lc(\W_t)}{|\nabla \Lc(\W_t)|}\rangle \bigm| \\
        &\stackrel{(a)}{\leq} -\eta_t \bigl( 1- 4\sqrt{\frac{\beta_1^{t+1}}{1 - \beta_2^{t+1}}} \bigr) \none{\nabla \Lc(\W_t)}  + \frac{2d}{\sqrt{1-\beta_2}}\bigl( \frac{6 \alpha_V}{\sqrt{1 - \beta_2^{t+1}}} \eta_t^{\frac{3}{2}} + 3 \alpha_M \eta_t^2\bigr) \Gc(\W_t) \\
        &\leq -\eta_t \bigl( 1- 4\frac{\beta_1^{\frac{t}{2}}}{\sqrt{1 - \beta_2}} \bigr) \none{\nabla \Lc(\W_t)}  + \frac{12 \alpha_V}{1 - \beta_2} d \eta_t^{3/2}\Gc(\W_t) + \frac{6\alpha_M}{\sqrt{1-\beta_2}} d \eta_t^2 \Gc(\W_t) \\
        &\stackrel{(b)}{\leq} - \eta_t \gamma \bigl( 1- 4\frac{\beta_1^{\frac{t}{2}}}{\sqrt{1 - \beta_2}} \bigr) \Gc(\W_t) + \frac{12 \alpha_V}{1 - \beta_2} d \eta_t^{3/2}\Gc(\W_t) + \frac{6\alpha_M}{\sqrt{1-\beta_2}} d \eta_t^2 \Gc(\W_t),
    \end{align*}
    where (a) is by Lemma \ref{lem:adam_intermediate_bound}, and (b) is by Lemma \ref{lem:G and gradient}. For $\clubsuit_t$, we apply Lemma \ref{lem:hessian_bound} to obtain
    \begin{align*}
        \clubsuit_t \leq 4 \| \Deltab_t \hb_i \|_{\infty}^2 (1 - \sfti{y_i}{\W_{t,t+1,\zeta^*} \hb_i}) \leq 4 \eta_t^2 \alpha^2 B^2 (1 - \sfti{y_i}{\W_{t,t+1,\zeta^*} \hb_i}),
    \end{align*}
    where in the second inequality we have used $\| \Deltab_t \hb_i \|_{\infty} \leq \ninf{\Deltab_t} \| \hb_i \|_{1}$, $\| \hb_i \|_{1} \leq B$, and $\ninf{\Deltab_t}  = \eta_t \ninf{\frac{\M_t}{\sqrt{\Vb_t}}} \leq \eta_t \alpha$ by Lemma \ref{lem:iter_bound} given $t \geq  t_A$ implying that $1 \geq 4 \frac{\beta_1^{\frac{t}{2}}}{\sqrt{1 - \beta_2}}$. Combing this with Lemma \ref{lem:hessian_bound}, we obtain
    \begin{align*}
        \frac{1}{2n}\sum_{i\in[n]}  \hb_i^\top\Deltab_t^\top & \left(\diag{\sft{\W_{t,t+1,\gamma}\hb_i}}- \sft{\W_{t,t+1,\zeta^*}\hb_i}\sft{\W_{t,t+1,\zeta^*}\hb_i}^\top\right)\Deltab_t\,\hb_i \\
        &\leq \frac{1}{2n} \sum_{i \in [n]} 4 \eta_t^2 \alpha^2 B^2 (1 - \sfti{y_i}{\W_{t,t+1,\zeta^*} \hb_i}) \leq 2 \alpha^2 \eta_t^2 B^2 e^{2B\eta_0} \Gc(\W_{t}),
    \end{align*}
    where the derivation of the second inequality can be found in the derivation of \ref{eq: sign_descent_eq1}. Putting everything together, we obtain
    \begin{align*}
        \Lc(\W_{t+1}) &\leq \Lc(\W_t) - \eta_t \gamma \Gc(\W_t) + 4 \frac{\beta_1^{\frac{t}{2}}}{\sqrt{1-\beta_2}} \gamma \eta_t \Gc(\W_t) +  \frac{12 \alpha_V}{1 - \beta_2} d \eta_t^{3/2}\Gc(\W_t) + \\
        &\quad \quad \quad \bigl( \frac{6\alpha_M}{\sqrt{1-\beta_2}} + 2 \alpha^2 B^2 e^{2 B\eta_0}\bigr) d\eta_t^{2}\Gc(\W_t) \\
        &= \Lc(\W_t) - \eta_t \gamma \bigl( 1 - \alpha_{a_1} \beta_1^{t/2} - \alpha_{a _2} d \eta_{t}^{\frac{1}{2}} - \alpha_{a_3} d \eta_t\bigr) \Gc(\W_t),
    \end{align*}
    where we have defined $\alpha_{a_1} \coloneqq \frac{4}{\sqrt{1 - \beta_2}}$, $\alpha_{a_2} \coloneqq \frac{12 \alpha_V}{\gamma(1-\beta_2)}$, and $\alpha_{a_3} \coloneqq \frac{6 \alpha_M}{\gamma \sqrt{1-\beta_2}} + \frac{2 \alpha^2 B^2 e^{2 B \eta_0}}{\gamma}$.
\end{proof}

Built upon Lemma \ref{lem:adam_descent}, we can further lower bound the unnormalized margin of Adam iterates for a sufficiently large $t$. The proof is similar to that of SignGD (i.e., Lemma \ref{lem:sign_unnormalized_margin}), which crucially depends on the separability condition obtained after achieving a low loss (Lemma \ref{lem:sep}). The time $\tilde{t}_A$ will be specified in the proof of Theorem \ref{thm:adam}.

\begin{lemma} [Adam Unnormalized Margin]\label{lem:adam_unnormalized_margin} 
Under the same setting as Theorem \ref{thm:adam}, suppose that there exist $\tilde{t}$ such that $\Lc(\W_t) \leq \frac{\log 2}{n}$ for all $t > \tilde{t}$, then we have for all $t \geq \tilde{t}_A \coloneqq \max \{t_A, \tilde{t}\}$
    \begin{align*}
       \min_{i \in [n], c \neq y_i} (\eb_{y_i} - \eb_c)^T \W_t \hb_i \geq \gamma\sum_{s=\tilde{t}_A}^{t-1} \eta_s \frac{\Gc(\W_s)}{\Lc(\W_s)} - \alpha_{a_5} d \sum_{s=\tilde{t}_A}^{t-1} \eta_s^{\frac{3}{2}} - \alpha_{a_6} d \sum_{s=\tilde{t}_A}^{t-1} \eta_s^2 - \alpha_{a_7},
    \end{align*}
where $t_A = \frac{2 \log(\frac{\sqrt{1-\beta_2}}{4})}{\log(\beta_1)}$, and $\alpha_{a_5}$, $\alpha_{a_6}$, and $\alpha_{a_7}$ are some constants that depend on $B$, $\beta_1$, and $\beta_2$.
\end{lemma}
\begin{proof}
     We denote $\alpha_{a_4} \coloneqq \frac{4}{\sqrt{1 - \beta_2}}$, $\alpha_{a_5} \coloneqq \frac{12 \alpha_V}{1 - \beta_2}$, and $\alpha_{a_6} \coloneqq \frac{6 \alpha_M}{\sqrt{1-\beta_2}} + 2 \alpha^2 B^2 e^{2B\eta_0}$. Under the assumption that $\Lc(\W_t) \leq \frac{\log2}{n}$ for all $t\geq \tilde{t}$, we have for all $t \geq \tilde{t}_A \coloneqq \max \{t_A, \tilde{t}\}$ (recall that $t_A = \frac{2 \log(\frac{\sqrt{1-\beta_2}}{4})}{\log(\beta_1)}$) 
    \begin{align*}
        \Lc(\W_{t+1}) &\stackrel{(a)}{\leq} \Lc(\W_t) - \eta_t \gamma \Gc(\W_t) + \alpha_{a_4} \beta_1^{\frac{t}{2}} \gamma \eta_t \Gc(\W_t) + \alpha_{a_5} k d \eta_t^{\frac{3}{2}} \Gc(\W_t) + \alpha_{a_6} k d \eta_t^2 \Gc(\W_t) \\
        &\stackrel{(b)}{\leq} \Lc(\W_t) \bigl( 1 - \eta_t \gamma \frac{\Gc(\W_t)}{\Lc(\W_t)} + \alpha_{a_4} \beta_1^{\frac{t}{2}} \gamma \eta_t + \alpha_{a_5} d \eta_t^{\frac{3}{2}} + \alpha_{a_6} d \eta_t^2\bigr) \\
        &\leq \Lc(\W_{\tilde{t}_A}) \exp \bigl(-\gamma\sum_{s=\tilde{t}_A}^t \eta_s \frac{\Gc(\W_s)}{\Lc(\W_s)} + \alpha_{a_4} \gamma \sum_{s=\tilde{t}_A}^t \beta_1^{\frac{s}{2}} \eta_s + \alpha_{a_5} d \sum_{s=\tilde{t}_A}^t \eta_s^{\frac{3}{2}} + \alpha_{a_6} d \sum_{s=\tilde{t}_A}^t \eta_s^2 \bigr) \\
        &\stackrel{(c)}{\leq} \frac{\log 2}{n} \exp \bigl(-\gamma\sum_{s=\tilde{t}_A}^t \eta_s \frac{\Gc(\W_s)}{\Lc(\W_s)} + \alpha_{a_5} d \sum_{s=\tilde{t}_A}^t \eta_s^{\frac{3}{2}} + \alpha_{a_6} d \sum_{s=\tilde{t}_A}^t \eta_s^2 + \alpha_{a_7}\bigr), 
    \end{align*}
    where (a) is by Lemma \ref{lem:adam_descent}, (b) is by $\frac{\Gc(\W_t)}{\Lc(\W_t)} \leq 1$ (shown in Lemma \ref{lem:G and L}), and (c) is by $\Lc(\W_{\tilde{t}_A}) \leq \frac{\log 2}{n}$ and $\alpha_{a_4} \gamma \sum_{s = \tilde{t}_A}^t \beta_1^{\frac{s}{2}} \eta_s \leq \frac{\alpha_{a_4} \gamma \eta_0}{1 - \beta_1^{\frac{1}{2}}} \eqqcolon \alpha_{a_7}$ . The rest of the proof follows the same arguments in Lemma \ref{lem:sign_unnormalized_margin}. Namely, the assumption $\Lc(\W_t) \leq \frac{\log 2}{n}$ implies that $\min_{c \neq y_i} (\eb_{y_i} - \eb_c)^T \W_t \hb_i \geq 0$ for all $i \in [n]$. This separability condition can be used further to show that for all $t \geq \tilde{t}_A$
    \begin{align*}
       e^{-\min_{i \in [n], c \neq y_i} (\eb_{y_i} - \eb_c)^T \W_t \hb_i} \leq \exp \bigl(-\gamma\sum_{s=\tilde{t}_A}^{t-1} \eta_s \frac{\Gc(\W_s)}{\Lc(\W_s)} + \alpha_{a_5} d \sum_{s=\tilde{t}_A}^{t-1} \eta_s^{\frac{3}{2}} + \alpha_{a_6} d \sum_{s=\tilde{t}_A}^{t-1} \eta_s^2 + \alpha_{a_7}\bigr).
    \end{align*}
    Taking the $\log$ on both sides leads to the final result.
\end{proof}

Next lemma upper bounds the max-norm of Adam iterates. It involves showing that the risk upper bounds entry-wise second moment, which will become small after the risk starts to monotonically decrease. Its proof can be found in \citet[Lemma 6.4]{zhang2024implicit}. Here, we only show the steps that are specific in our settings.
\begin{lemma}[Adam $\ninf{\W_t}$]\label{lem:adam_weight}
    Under the same setting as Theorem \ref{thm:adam}, suppose that there exists $\tilde{t}_B > \log(\frac{1}{\omega})$ such that $\Lc(\W_t) \leq \frac{1}{\sqrt{4 B^2 + \alpha_V \eta_0}}$ for all $t \geq \tilde{t}_B$, then we have
    \begin{align*}
        \ninf{\W_t} \leq \alpha_{a_8} \sum_{s=0}^{\tilde{t}_B - 1} \eta_s + \sum_{s=\tilde{t}_B}^{t-1} \eta_s + \ninf{\W_0},
    \end{align*}
    where $\alpha_{a_8}$ is some constant that depends on $B$, $\beta_1$, and $\beta_2$. 
\end{lemma}
\begin{proof}
    For any $c \in [k]$ and $j \in [d]$, we have for all $t \geq \tilde{t}_B$
    \begin{align*}
        \Vb_t[c,j] &\stackrel{(a)}{\leq} (1-\beta_2^{t+1}) \nabla \Lc(\W_t) [c,j]^2 +\alpha_V \eta_t \Gc(\W_t)^2 \\
        &\leq \nabla \Lc(\W_t)[c,j]^2 + \alpha_V \eta_t \Gc(\W_t)^2 \\
        &\stackrel{(b)}{\leq} 4 B^2 \Gc(\W_t)^2 + \alpha_V \eta_0 \Gc(\W_t)^2 \\
        &\stackrel{(c)}{\leq} (4 B^2 + \alpha_V \eta_0) \Lc(\W_t)^2 \stackrel{(d)}{\leq} 1, 
    \end{align*}
    where (a) is by Lemma \ref{lem:first_G}, (b) is by Lemma \ref{lem:G and gradient}, (c) is by Lemma \ref{lem:G and L}, and (d) is by the assumption.
    This implies that for all $t \geq \tilde{t}_B$
    \begin{align*}
        0 \geq \log(\Vb_t[c,j]) \geq \log(\beta_2^t (1-\beta_2) \Lc(\W_t)[c,j]^2) \stackrel{(e)}{\geq} t \log(\beta_2) + \log(1 - \beta_2) + \log(\omega),
    \end{align*}
    where (e) is by the Assumption \ref{ass:adam_init}. The rest proof follows the same arguments in \citet[Lemma 6.4]{zhang2024implicit}. 
\end{proof}

\begin{theorem} \label{thm:adam} Suppose that Assumption \ref{ass:sep}, 
\ref{ass:adam_init}, 
\ref{ass:learning_rate_1},
\ref{ass:learning_rate_2}, and \ref{ass:data_bound} hold, and $\beta_1 \leq \beta_2$,  then there exists $t_{a_2} = t_{a_2}(n,d, \gamma,B,\W_0,\beta_1,\beta_2,\omega)$ such that Adam achieves the following for all $t > t_{a_2}$
\begin{align*}
     \left|\frac{\min_{i \in [n], c \neq y_i} (\e_{y_i} - \e_c)^T \W_t \hb_i}{\ninf{\W_t}} - \gamma\right| &\leq  \mathcal{O}(\frac{\sum_{s=t_{a_2}}^{t-1} \eta_s e^{-\frac{\gamma}{4} \sum_{\tau = t_{a_2}}^{s-1}\eta_{\tau}} + \sum_{s=0}^{t_{a_2}-1}\eta_s + d\sum_{s = t_{a_2}}^{t-1}\eta_s^{3/2}}{\sum_{s=0}^{t-1} \eta_s}). 
\end{align*}
\end{theorem}


\begin{proof}
\textbf{Determination of $t_{a_1}$.} Here, we consider learning rate schedule of the form $\eta_t = \Theta(\frac{1}{t^a})$ where $a \in (0,1]$. We choose $t_{a_1}$ after ($\max\{t_0,t_A, \log(\frac{1}{\omega})\}$ where $t_0$ satisfies Assumption \ref{ass:learning_rate_2} and $t_A = \frac{2 \log(\frac{\sqrt{1 - \beta_2}}{4})}{\log \beta_1}$) such that the following conditions are met: $\alpha_{a_1} \beta_1^{t/2} \leq \frac{1}{6}$, $\alpha_{a_2} d\eta_t^{1/2} \leq \frac{1}{6}$, and $\alpha_{a_3} d \eta_t \leq \frac{1}{6}$. Concretely, we can set $t_{a_1} = \max \{\frac{- 2\log(6 \alpha_{a_1})}{\log \beta_1}, (36 \alpha_{a_2}^2 d^2)^{1/a}, (6 \alpha_{a_3} d)^{1/a} \} = \Theta(d^{2/a})$. Then, we have for all $t \geq t_{a_1}$
\begin{align}
    \Lc(\W_{t+1}) \leq \Lc(\W_t) - \frac{\eta_t \gamma}{2} \Gc(\W_t). \label{eq:adam_main_eq1} 
\end{align}
Rearranging this equation and using non-negativity of the loss we obtain $\gamma \sum_{s = t_{a_1}}^t \eta_s \Gc(\W_s) \leq 2\Lc(\W_{t_{a_1}})$. \\
\textbf{Determination of $t_{a_2}$.} By Lemma \ref{lem:l_fast_bound}, we can bound $\Lc(\W_{t_{s_1}})$ as follows
\begin{align*}
    |\Lc(\W_{t_{a_1}}) - \Lc(\W_0)| \leq 2 B \ninf{\W_{t_{a_1}} - \W_0} \leq 2 B \sum_{s = 0}^{t_{a_1}-1} \eta_s \ninf{\frac{\M_s}{\sqrt{\Vb_s}}}
    \leq 2 B \alpha \sum_{s = 0}^{t_{a_1}-1} \eta_s,
\end{align*}
where the last inequality is by Lemma \ref{lem:iter_bound}. 
 Combining this with the result above and letting $\tilde{\Lc} = \min \{\frac{\log 2}{n},  \frac{1}{\sqrt{4 B^2 + \alpha_V \eta_0}} \}$), we obtain
\begin{align*}
    \Gc(\W_{t^*}) = \min_{s \in [t_{a_1}, t_{a_2}]} \Gc(\W_s) \leq \frac{ 2 \Lc(\W_0) + 4B\alpha \sum_{s = 1}^{t_{a_1}-1} \eta_s}{\gamma \sum_{s=t_{a_1}}^{t_{a_2}} \eta_s} \leq \frac{\tilde{\Lc}}{2} \leq \frac{1}{2n}, 
\end{align*}
from which we derive the sufficient condition on $t_{a_2}$ to be $\sum_{s=t_{a_1}}^{t_{a_2}} \eta_s \geq \frac{4\Lc(\W_0) + 8 B \alpha \sum_{s = 1}^{t_{a_1}-1} \eta_s}{\gamma \tilde{\Lc}}$.\\ 
\textbf{Convergence of $\frac{\Gc(\W_t)}{\Lc(\W_t)}$} We follow the same arguments in the proof of SignGD (Theorem \ref{thm:signgd}) to conclude that 
\begin{align}
    \frac{\Gc(\W_t)}{\Lc(\W_t)} \geq 1 - e^{-\frac{\gamma}{4} \sum_{s = t_{a_2}}^{t-1} \eta_s}. \label{eq:adam_main_eq2} 
\end{align}
We note that $t_{a_2}$ satisfies the assumptions in Lemma \ref{lem:adam_unnormalized_margin} and Lemma \ref{lem:adam_weight}. \\
\textbf{Margin Convergence} Finally, we combine Lemma \ref{lem:adam_unnormalized_margin}, Lemma \ref{lem:adam_weight}, and \eqref{eq:adam_main_eq2} to obtain 
\begin{align*}
    |\frac{\min_{i \in [n], c \neq y_i} (\e_{y_i} - \e_c)^T \W_t \hb_i}{\ninf{\W_t}} - \gamma| 
    % &\leq \frac{\gamma \bigl(\ninf{\W_0} + \sum_{s=t_{a_2}}^{t-1} \eta_s e^{-\frac{\gamma}{4} \sum_{\tau = t_{a_2}}^{s-1}\eta_{\tau}} + \alpha_{a_8} \sum_{s=0}^{t_{a_2}-1}\eta_s \bigr) + \alpha_{a_5} kd\sum_{s = t_{a_2}}^{t-1}\eta_s^{3/2} + \alpha_{a_6} kd\sum_{s = t_{a_2}}^{t-1}\eta_s^{3/2}}{\ninf{\W_0} + \sum_{s=t_{a_2}}^{t-1} \eta_s + \alpha_{a_8} \sum_{s=0}^{t_{a_2}-1}\eta_s} \\
    &\leq \mathcal{O}(\frac{\sum_{s=t_{a_2}}^{t-1} \eta_s e^{-\frac{\gamma}{4} \sum_{\tau = t_{a_2}}^{s-1}\eta_{\tau}} + \sum_{s=0}^{t_{a_2}-1}\eta_s + d\sum_{s = t_{a_2}}^{t-1}\eta_s^{3/2} + d\sum_{s = t_{a_2}}^{t-1}\eta_s^{2}}{\sum_{s=0}^{t-1} \eta_s}) \\
    &\leq \mathcal{O}(\frac{\sum_{s=t_{a_2}}^{t-1} \eta_s e^{-\frac{\gamma}{4} \sum_{\tau = t_{a_2}}^{s-1}\eta_{\tau}} + \sum_{s=0}^{t_{a_2}-1}\eta_s + d\sum_{s = t_{a_2}}^{t-1}\eta_s^{3/2}}{\sum_{s=0}^{t-1} \eta_s})
\end{align*}
\end{proof}

Similar to the case of SignGD, we can derive the margin convergence rates for Adam.

\begin{corollary} \label{cor:adam}
    Consider learning rate schedule of the form $\eta_t = \Theta(\frac{1}{t^a})$ where $a \in (0,1]$, under the same setting as Theorem \ref{thm:adam}, then we have for Adam 
    \[ 
   |\frac{\min_{i \in [n], c \neq y_i} (\e_{y_i} - \e_c)^T \W_t \hb_i}{\ninf{\W_t}} - \gamma| = \left\{
    \begin{array}{ll}
           \mathcal{O} (\frac{d t^{1-\frac{3a}{2}}+ n d^{\frac{2(1-a)}{a}} + n\Lc(\W_0) + [\log(1/\omega)]^{1-a}}{t^{1-a}}) &  \text{if} \quad a < \frac{2}{3}\\
           \mathcal{O} (\frac{d \log(t) + nd + n \Lc(\W_0) + [\log(1/\omega)]^{1/3}}{t^{1/3}}) & \text{if} \quad a = \frac{2}{3} \\
            \mathcal{O} (\frac{d + n d^{\frac{2(1-a)}{a}}+ n \Lc(\W_0) + [\log(1/\omega)]^{1-a}}{t^{1-a}}) &  \text{if}\quad \frac{2}{3} < a<1 \\
            \mathcal{O} (\frac{d + n \log(d) + n \Lc(\W_0) + \log \log(1/\omega)}{\log t}) &  \text{if} \quad a=1
    \end{array} 
    \right. 
    \]
\end{corollary}

\begin{proof}
    Recall that $t_{a_1} = \Theta(d^{2/a}) = C_{a_1} d^{2/a}$, and the condition on $t_{a_2}$ is $\frac{ 2 \Lc(\W_0) + 4B\alpha \sum_{s = 1}^{t_{a_1}-1} \eta_s}{\gamma \sum_{s=t_{a_1}}^{t_{a_2}} \eta_s} \leq \frac{\tilde{\Lc}}{2}$, where $\tilde{\Lc} = \min \{\frac{\log 2}{n},  \frac{1}{\sqrt{4 B^2 + \alpha_V \eta_0}} \}$. Then,  we apply integral approximations and the rest of the proof can be found in \cite[Corollary 4.7 and Lemma C.1]{zhang2024implicit}.   
    % terms that involve sums in this inequality to obtain 
    % \begin{align*}
    %     t_{a_2} \leq C_{a_2} n^{\frac{1}{1-a}} t_{a_1} + C_{a_3} n^{\frac{1}{1-a}} \Lc(\W_0)^{\frac{1}{1-a}} + C_{a_4} \log(\frac{1}{\omega}).
    %\end{align*}
    % This further implies that
    % \begin{align*}
    %     \sum_{s=0}^{t_{a_2}-1} \eta_s = \mathcal{O}(t_{a_2}^{1-a} ) = \mathcal{O} \bigl( n (kd)^{\frac{2(1-a)}{a}} + n \Lc(\W_0) + [\log(\frac{1}{\omega})]^{1-a} \bigr).
    % \end{align*}
    % Next, we focus on the term $\sum_{s=t_{s_2}}^{t-1} \eta_s^2$. For $a >= \frac{2}{3}$, this term can be bounded by some constant. For $a < \frac{2}{3}$, we have $\sum_{s=t_{s_2}}^{t-1} \eta_s^2 = \mathcal{O}(t^{1 - 2a})$. Finally, we have that $\sum_{s=0}^{t-1} \eta_s = \mathcal{O}(t^{1-a})$ for $a < 1$ and $\sum_{s=0}^{t-1} \eta_s = \mathcal{O}(\log t)$ for $a = 1$. The rest arguments can be found in \cite[Corollary 4.7 and Lemma C.1]{zhang2024implicit},  including showing the learning rate schedule $\eta_t = \frac{1}{(t+2)^a}$ satisfying Assumption \ref{ass:learning_rate_2}, and the term $\sum_{s=t_{s_2}}^{t-1} \eta_s e^{-\frac{\gamma}{4} \sum_{\tau = t_{s_2}}^{s-1}\eta_{\tau}}$ is bounded by some constant 
    % for all $a \in (0,1]$. 
\end{proof}

\begin{lemma} \label{lem:unified_helper} 
    For any $\vb, \vb', \qb, \qb' \in \R^k$ and $c \in [k]$, the following inequalities hold:
    \begin{enumerate}[label=(\roman*)]
        \item $|\frac{\sfti{c}{\vb'}}{\sfti{c}{\vb}} - 1| \leq e^{2\lVert \vb -\vb'\rVert_{\infty}} - 1$
        \item $|\frac{1 - \sfti{c}{\vb'}}{1 - \sfti{c}{\vb}} - 1| \leq e^{2\lVert \vb - \vb' \rVert_{\infty}} - 1$
        \item $|\frac{\sfti{c}{\vb'} \sfti{c}{\qb'}}{\sfti{c}{\vb} \sfti{c}{\qb}} - 1| \leq e^{2(\lVert \vb' - \vb \rVert_{\infty} + \lVert \qb' - \qb \rVert_{\infty})} - 1$
        \item $|\frac{\sfti{c}{\vb'} (1 - \sfti{c}{\qb'})}{\sfti{c}{\vb} (1 - \sfti{c}{\qb})} - 1| \leq e^{2(\lVert \vb' - \vb \rVert_{\infty} + \lVert \qb' - \qb \rVert_{\infty})} - 1$
        \item $|\frac{(1 - \sfti{c}{\vb'}) (1 - \sfti{c}{\qb'})}{(1 -\sfti{c}{\vb}) (1 - \sfti{c}{\qb})} - 1| \leq e^{2(\lVert \vb' - \vb \rVert_{\infty} + \lVert \qb' - \qb \rVert_{\infty})} - 1$
    \end{enumerate}
\end{lemma}

\begin{proof}
    We prove each inequality:

    (i) First, observe that
    \begin{align*}
        |\frac{\sfti{c}{\vb'}}{\sfti{c}{\vb}} - 1| &= |\frac{e^{v'_c}}{e^{v_c}} \frac{\sum_{i \in [k]} e^{v_i}}{\sum_{i \in [k]} e^{v'_i}} - 1| \\
        &= |\frac{\sum_{i \in [k]} e^{v'_c + v_i} - \sum_{i \in [k]} e^{v_c + v'_i}}{\sum_{i \in [k]} e^{v_c + v'_i}}| \\
        &\leq \frac{\sum_{i \in [k]} |e^{v'_c + v_i} - e^{v_c + v'_i}|}{\sum_{i \in [k]} e^{v_c + v'_i}}
    \end{align*}
    For any $i \in [k]$, we have $\frac{|e^{v'_c + v_i} - e^{v_c + v'_i}|}{e^{v_c + v'_i}} = | e^{v'_c - v_c + v_i - v'_i} - 1| \leq e^{|v'_c - v_c + v_i - v'_i|} - 1 \leq e^{2\lVert \vb -\vb'\rVert_{\infty}} - 1$. This implies $\sum_{i \in [k]}|e^{v'_c + v_i} - e^{v_c + v'_i}| \leq \bigl( e^{2\lVert \vb -\vb'\rVert_{\infty}} - 1\bigr) \sum_{i \in [k]} e^{v_c + v'_i}$, from which we obtain the desired inequality.

    (ii) For the second inequality:
    \begin{align*}
        |\frac{1 - \sfti{c}{\vb'}}{1 - \sfti{c}{\vb}} - 1| &= |\frac{1 - \frac{e^{v'_c}}{\sum_{i \in [k]} e^{v'_i}}}{ 1 - \frac{e^{v_c}}{\sum_{i \in [k]} e^{v_i}}} - 1| \\
        &= |\frac{(\sum_{j \in [k], j \neq c} e^{v'_j}) (\sum_{i \in [k]} e^{v_i})}{(\sum_{j \in [k], j \neq c} e^{v_j}) (\sum_{i \in [k]} e^{v'_i})} - 1| \\
        &= |\frac{\sum_{j \in [k], j \neq c} \sum_{i \in [k]} \bigl[ e^{v'_j + v_i} - e^{v_j + v'_i}\bigl]}{\sum_{j \in [k], j \neq c} \sum_{i \in [k]} e^{v_j + v'_i}}| \\
        &\leq \frac{ \sum_{j \in [k], j \neq c} \sum_{i \in [k]} |e^{v'_j + v_i} - e^{v_j + v'_i}|}{ \sum_{j \in [k], j \neq c} \sum_{i \in [k]} e^{v_j + v'_i}}
    \end{align*}
    For any $j \in [k]$, $j \neq c$, and $i \in [k]$, we have $\frac{|e^{v_j' + v_i} - e^{v_j + v_i'}|}{e^{v_j + v_i'}} \leq e^{2 \lVert \vb - \vb' \rVert_{\infty}} - 1$. This implies that $\sum_{j \in [k], j \neq c} \sum_{i \in [k]} |e^{v_j' + v_i} - e^{v_j + v_i'}| \leq (e^{2\lVert \vb - \vb' \rVert_{\infty}} - 1) \sum_{j \in [k], j \neq c} \sum_{i \in [k]} e^{v_j + v_i'}$, from which the result follows.

    (iii) For the third inequality:
    \begin{align*}
         |\frac{\sfti{c}{\vb'} \sfti{c}{\qb'}}{\sfti{c}{\vb} \sfti{c}{\qb}} - 1| &= | \frac{ \frac{e^{v'_c}}{\sum_{i \in [k]} e^{v'_i}} \frac{e^{q'_c}}{\sum_{i \in [k]}e^{q'_i}} }{ \frac{e^{v_c}}{\sum_{i \in [k]} e^{v_i}} \frac{e^{q_c}}{\sum_{i \in [k]}e^{q_i}} } - 1| \\
        &= |\frac{ \frac{e^{v'_c}}{\sum_{i \in [k]} e^{v'_i}} \frac{e^{q'_c}}{\sum_{i \in [k]}e^{q'_i}} }{ \frac{e^{v_c}}{\sum_{i \in [k]} e^{v_i}} \frac{e^{q_c}}{\sum_{i \in [k]}e^{q'_i}} } - \frac{ \frac{e^{v_c}}{\sum_{i \in [k]} e^{v'_i}} \frac{e^{q_c}}{\sum_{i \in [k]}e^{q'_i}} }{ \frac{e^{v_c}}{\sum_{i \in [k]} e^{v'_i}} \frac{e^{q_c}}{\sum_{i \in [k]}e^{q'_i}} }| \\
        &= |\frac{e^{v'_c}e^{q'_c} \sum_{i \in [k]} e^{v_i} \sum_{j \in [k]} e^{q_j}} {e^{v_c}e^{q_c} \sum_{i \in [k]} e^{v'_i} \sum_{j \in [k]} e^{q'_j}} - \frac{e^{v_c}e^{q_c} \sum_{i \in [k]} e^{v'_i} \sum_{j \in [k]} e^{q'_j}}{e^{v_c}e^{q_c} \sum_{i \in [k]} e^{v'_i} \sum_{j \in [k]} e^{q'_j}}| \\
        &= | \frac{\sum_{i \in [k]} \sum_{j \in [k]} \bigl[ e^{v'_c + v_i + q'_c + q_j} - e^{v_c + v'_i + q_c + q'_j} \bigr] }{ \sum_{i \in [k]} \sum_{j \in [k]} e^{v_c + v'_i + q_c + q'_j}}| \\
        &\leq \frac{\sum_{i \in [k]} \sum_{j \in [k]} |e^{v'_c + v_i + q'_c + q_j} - e^{v_c + v'_i + q_c + q'_j}|} {\sum_{i \in [k]} \sum_{j \in [k]} e^{v_c + v'_i + q_c + q'_j}}
    \end{align*}
    For any $i \in [k]$ and $j \in [k]$, $\frac{|e^{v'_c + v_i + q'_c + q_j} - e^{v_c + v'_i + q_c + q'_j}|}{e^{v_c + v'_i + q_c + q'_j}} = |e^{v'_c - v_c + v_i - v'_i + q'_c - q_c + q_j - q'_j} - 1| \leq e^{|v'_c - v_c| + |v_i - v'_i| + |q'_c - q_c| + |q_j - q'_j|} -  1 \leq  e^{2(\lVert \vb' - \vb \rVert_{\infty} + \lVert \qb' - \qb \rVert_{\infty})} - 1$. Then, rearranging and summing over $i$ and $j$ leads to the result.

    (iv) For the fourth inequality:
    \begin{align*}
         |\frac{\sfti{c}{\vb'} (1 - \sfti{c}{\qb'})}{\sfti{c}{\vb} (1 - \sfti{c}{\qb})} - 1| &= |\frac{ \frac{e^{v'_c}}{\sum_{s \in [k]}e^{v'_s}} (1- \frac{e^{q'_c}}{\sum_{t \in [k]}e^{q'_t}})}{\frac{e^{v_c}}{\sum_{s \in [k]}e^{v_s}} (1- \frac{e^{q_c}}{\sum_{t \in [k]}e^{q_t}})} - 1| \\
         &= |\frac{ \frac{e^{v'_c}}{\sum_{s \in [k]}e^{v'_s}}\frac{ \sum_{i \in [k], i \neq c }e^{q'_i}}{\sum_{t \in [k]}e^{q'_t}}}{\frac{e^{v_c}}{\sum_{s \in [k]}e^{v_s}} \frac{\sum_{i \in [k], i \neq c }e^{q_t}}{\sum_{t \in [k]}e^{q_t}}} - 1| \\
         &= | \frac{\sum_{i \in [k], i \neq c} \sum_{t \in [k]} \sum_{s \in [k]} e^{v'_c + q'_i + v_s + q_t}}{ \sum_{i \in [k], i \neq c} \sum_{t \in [k]} \sum_{s \in [k]} e^{v_c + q_i + v'_s + q'_t}} - 1| \\
         &\leq \frac{\sum_{i \in [k], i \neq c} \sum_{t \in [k]} \sum_{s \in [k]} |e^{v'_c + q'_i + v_s + q_t} - e^{v_c + q_i + v'_s + q'_t}|}{\sum_{i \in [k], i \neq c} \sum_{t \in [k]} \sum_{s \in [k]} e^{v_c + q_i + v'_s + q'_t}}
    \end{align*}
    For each $i \in [k], i \neq c$, $s \in [k]$, and $t \in [k]$, we obtain $\frac{|e^{v'_c + q'_i + v_s + q_t} - e^{v_c + q_i + v'_s + q'_t}|}{e^{v_c + q_i + v'_s + q'_t}} \leq e^{2(\lVert \vb' - \vb \rVert_{\infty} + \lVert \qb' - \qb \rVert_{\infty})} - 1$. Then, rearranging and summing over $i$, $s$, and $t$ leads to the result.

    (v)   Finally, for the fifth inequality:
    \begin{align*}
         |\frac{(1 - \sfti{c}{\vb'}) (1 - \sfti{c}{\qb'})}{(1 -\sfti{c}{\vb}) (1 - \sfti{c}{\qb})} - 1| &= |\frac{ (1-\frac{e^{v'_c}}{\sum_{s \in [k]}e^{v'_s}}) (1- \frac{e^{q'_c}}{\sum_{t \in [k]}e^{q'_t}})}{ (1 - \frac{e^{v_c}}{\sum_{s \in [k]}e^{v_s}}) (1- \frac{e^{q_c}}{\sum_{t \in [k]}e^{q_t}})} - 1| \\
         &= |\frac{ \frac{ \sum_{j \in [k], j \neq c} e^{v'_j}}{\sum_{s \in [k]}e^{v'_s}}\frac{ \sum_{i \in [k], i \neq c }e^{q'_i}}{\sum_{t \in [k]}e^{q'_t}}}{\frac{\sum_{j \in [k], j \neq c}  e^{v_j}}{\sum_{s \in [k]}e^{v_s}} \frac{\sum_{i \in [k], i \neq c }e^{q_i}}{\sum_{t \in [k]}e^{q_t}}} - 1| \\
         &= | \frac{\sum_{j \in [k], j \neq c} \sum_{i \in [k], i \neq c} \sum_{t \in [k]} \sum_{s \in [k]} e^{v'_j + q'_i + v_s + q_t}}{ \sum_{j \in [k], j \neq c} \sum_{i \in [k], i \neq c} \sum_{t \in [k]} \sum_{s \in [k]} e^{v_j + q_i + v'_s + q'_t}} - 1| \\
         &\leq \frac{\sum_{j \in [k], j \neq c} \sum_{i \in [k], i \neq c} \sum_{t \in [k]} \sum_{s \in [k]} |e^{v'_j + q'_i + v_s + q_t} - e^{v_j + q_i + v'_s + q'_t}|}{\sum_{j \in [k], j \neq c} \sum_{i \in [k], i \neq c} \sum_{t \in [k]} \sum_{s \in [k]} e^{v_j + q_i + v'_s + q'_t}}.
    \end{align*}
    For each $j \in [k]$ ($j \neq c$), $i \in [k]$ ($i \neq c$), $s \in [k]$, and $t \in [k]$, we have 
    \begin{align*}
        \frac{|e^{v'_j + q'_i + v_s + q_t} - e^{v_j + q_i + v'_s + q'_t}|}{e^{v_j + q_i + v'_s + q'_t}} &= |e^{v'_j - v_j + q'_i - q_i + v_s - v'_s + q_t - q'_t} - 1| \\
        &\leq e^{|v'_j - v_j| + |q'_i - q_i| + |v_s - v'_s| + |q_t - q'_t|} - 1 \\
        &\leq e^{2(\lVert \vb' - \vb \rVert_{\infty} + \lVert \qb' - \qb \rVert_{\infty})} - 1
    \end{align*}
    Then, rearranging and summing over $j$, $i$, $s$, and $t$ leads to the result.
\end{proof}


\section{Normalized $p$-Nrom Steepest Descent} \label{sec:app_nsd}

Let $\|\cdot\|$ denote (entrywise) $p$-norm with $p\geq 1$ and $\|\cdot\|_\star$ denote its dual.

Consider normalized steepest-descent with respect to the $p$-norm:
\begin{align}
\W_{t+1} = \W_t - \eta_t \Deltab_t,\qquad\text{where}~~\Deltab_t:=\arg\max_{\|\Deltab\|\leq 1}\inp{\nabla\Lc(\W_t)}{\Deltab}\,.
\end{align}

Lemma \ref{lem:G and gradient SD} generalizes Lemma \ref{lem:G and gradient}. 
\begin{lemma}[$\Gc(\W)$ as proxy to the loss-gradient norm]
\label{lem:G and gradient SD}
Define the margin $\gamma$ with respect to $p$-norm, for $p\geq 1$ and denote $\|\cdot\|_\star$ its dual norm. 
Then, for any $\W$ it holds that
    \[
    2B \cdot \Gc(\W) \geq \|{\nabla\Lc(\W)}\|_\star \geq \gamma\cdot \Gc(\W)\,.
    \]
\end{lemma}

\begin{proof} First, we prove the lower bound. By duality and direct application of \eqref{eq:nabla_ineq_basic}
\begin{align*}
    \|{\nabla \Lc(\W)}\|_\star & = \max_{\|{\A}\| \leq 1} \langle \A, -\nabla \Lc(\W)\rangle \\
    &\geq \max_{\|\A\| \leq 1} \frac{1}{n} \sum_{i \in [n]} (1 - s_{iy_i}) \min_{c \neq y_i} (\e_{y_i} - \e_c)^T \A \hb_i \\
    &\geq \frac{1}{n} \sum_{i \in [n]} (1 - s_{iy_i}) \cdot  \max_{\|\A\| \leq 1}\min_{i \in [n],c \neq y_i} (\e_{y_i} - \e_c)^T \A \hb_i.
\end{align*}
The upper bound follows because for $p$-norms with $p\geq 1$
\[
\|\nabla \Lc(\W)\|_\star \leq \none{\nabla \Lc(\W)}
\]
and we can use the bound for $\none{\nabla\Lc(\W)}$.
\end{proof}

A direct consequence of Lemma \ref{lem:G and gradient SD} is the following. 

\begin{lemma} \label{lem:l_fast_bound_SD}
    For any $\W, \W_0 \in \R^{k \times d}$, suppose that $\Lc(\W)$ is convex, we have
    \begin{align*}
        |\Lc(\W) - \Lc(\W_0)| \leq 2B \lVert \W - \W_0 \rVert. 
    \end{align*}
\end{lemma}
\begin{proof} We replace the term $\none{\nabla \Lc(\W_0)} \ninf{\W_0 - \W}$ in Lemma \ref{lem:l_fast_bound} with $\lVert \nabla \Lc(\W_0) \rVert_{*} \lVert \W_0 - \W \rVert$. The rest proof follows the same steps as Lemma \ref{lem:l_fast_bound}.
\end{proof}

Lemma \ref{lem:hessian_bound_SD} generalizes Lemma \ref{lem:hessian_bound}. 
\begin{lemma} \label{lem:hessian_bound_SD}
     For any $\s\in\Delta^{k-1}$ in the $k$-dimensional simplex, any index $c\in[k]$, and any $\vb\in\R^k$ it holds:
     \[
     \vb^\top\left(\diag{\s}-\s\s^\top\right)\vb \leq 4\,\|\vb\|^2\, (1-s_c)
     \]
\end{lemma}
\begin{proof}
    By Cauchy-Schwartz, 
    \begin{align*}
        \vb^\top\left(\diag{\s}-\s\s^\top\right)\vb &= \vect{\diag{\s}-\s\s^\top}^\top \vect{\vb\vb^\top} \\
        &\leq \|\vect{\diag{\s}-\s\s^\top}\|_\star \| \vect{\vb\vb^\top}\| 
        \\&\leq \|\vb\|^2\,\|\vect{\diag{\s}-\s\s^\top}\|_*\,.
    \end{align*}
    But, 
    \begin{align*}
        \|\vect{\diag{\s}-\s\s^\top}\|_\star\leq \|\vect{\diag{\s}-\s\s^\top}\|_1
    \end{align*}
and we can use Lemma \ref{lem:hessian_bound}.
\end{proof}


When applying the above lemma to bound the Hessian term we also need to use the following:
\begin{align}
    \|\Deltab\hb\| \leq \|\Deltab\|\|\hb\|_\star,
\end{align}
which is true because for $q=p/(1-p)$:
\begin{align}
    \|\Deltab\hb\|^p = \|\Deltab\hb\|_p^p = \sum_{j} |\eb_j^\top\Deltab\hb|_p^p \leq \sum_{j} \|\eb_j^\top\Deltab\|_p^p\|\hb\|_q^p = \|\hb\|_q^p \sum_{ij} |\Deltab[i,j]|^p = \|\hb\|_q^p \|\Deltab\|_p^p
\end{align}


\section{Other multiclass loss functions} \label{sec:app_other_losses}

\subsection{Exponential Loss}

The multiclass exponential loss is given as
\[
\Lcexp(\W):=\frac{1}{n}\sum_{i\in[n]}\sum_{c\neq y_i}\exp\left(-(\eb_{y_i}-\eb_c)^\top\W\hb_i\right)\,.
\]
The gradient of $\Lc_{\exp}(\W)$ is 
\begin{align*}
    \nabla \Lc_{\exp}(\W) = \frac{1}{n} \sum_{i \in [n]} \sum_{c \neq y_i} -\exp(-(\e_{y_i} - \e_c)^T \W \hb_i) (\e_{y_i} - \e_c) \hb_i^T. 
\end{align*}
Thus, for any matrix $\A \in \R^{k \times d}$, we have
\begin{align*}
\inp{\Ab}{-\nabla\Lc_{\exp}(\W)} = \frac{1}{n}\sum_{i\in[n]}\sum_{c\neq y_i} \exp\left(-(\eb_{y_i}-\eb_c)^\top\W\hb_i\right) \cdot\left(\eb_{y_i}-\eb_c\right)^\top\Ab\hb_i\,.
\end{align*}
This motivates us to define $\Gc(\W)$ as 
\begin{align*}
    \Gc_{\exp}(\W) = \frac{1}{n} \sum_{i\in[n]}\sum_{c\neq y_i} \exp\left(-(\eb_{y_i}-\eb_c)^\top\W\hb_i\right), 
\end{align*}
from which we recognize that $\Gc_{\exp}(\W) = \Lc_{\exp} (\W)$ and the rest follows. 
% The extension of previous results to the exponential loss is straightforward given we can use the same proxy function $\Gc(\W)$ defined in \eqref{eq:G defn}. 
\subsection{PairLogLoss}

The PairLogLoss loss \cite{wang2021rank4class} is given as
\[
\Lcpll(\W):=\frac{1}{n}\sum_{i\in[n]}\sum_{c\neq y_i}\log\left(1+\exp\left(-(\eb_{y_i}-\eb_c)^\top\W\hb_i\right)\right)\,.
\]
Note that $\Lc=\frac{1}{n}\sum_{i\in[n]}\sum_{c\neq y_i}f\left((\eb_{y_i}-\eb_c)^\top\W\hb_i\right)$ where $f(t):=\log(1+e^{-t})$ denotes the logistic loss. Therefore, the Taylor expansion of PLL writes:
\begin{align}
\Lcpll(\W+\Deltab)&=\Lc(\W) + \frac{1}{n}\sum_{i\in[n]}\sum_{c\neq y_i}f'\left((\eb_{y_i}-\eb_c)^\top\W\hb_i\right)\cdot(\eb_{y_i}-\eb_c)^\top\Deltab\hb_i\nn  \\
&\qquad +\frac{1}{n}\sum_{i\in[n]}\sum_{c\neq y_i}f''\left((\eb_{y_i}-\eb_c)^\top\W\hb_i\right)\cdot \hb_i^\top\Deltab^\top(\eb_{y_i}-\eb_c)(\eb_{y_i}-\eb_c)^\top\Deltab\hb_i + o\left(\|\Deltab\|^3\right)\,.\label{eq:pll taylor}
\end{align}

From the above, the gradient of the PLL loss is:
\begin{align}
\nabla\Lcpll(\W) &= \frac{1}{n}\sum_{i\in[n]}\sum_{c\neq y_i}f'\left((\eb_{y_i}-\eb_c)^\top\W\hb_i\right)\cdot\left(\eb_{y_i}-\eb_c\right)\hb_i^\top\nn\\
&= \frac{1}{n}\sum_{i\in[n]}\sum_{c\neq y_i}\frac{-\exp\left(-(\eb_{y_i}-\eb_c)^\top\W\hb_i\right)}{1+\exp\left(-(\eb_{y_i}-\eb_c)^\top\W\hb_i\right)}\left(\eb_{y_i}-\eb_c\right)\hb_i^\top
% \\
% &= \frac{-1}{n}\sum_{i\in[n]}\sum_{c\neq y_i}\sfti{c}{\W\hb_i}\left(\eb_{y_i}-\eb_c\right)\hb_i^\top\nn
% \\
% &= \frac{-1}{n}\sum_{i\in[n]}\left(1-\sfti{y_i}{\W\hb_i}\right)\left(\eb_{y_i}-\eb_c\right)\hb_i^\top\,.
\label{eq:pll gradient}
\end{align}

Thus, for any matrix $\Ab\in\R^{k\times d}$,
\begin{align}\label{eq:PLL inp}
\inp{\Ab}{-\nabla\Lcpll(\W)} = \frac{1}{n}\sum_{i\in[n]}\sum_{c\neq y_i}|f'\left((\eb_{y_i}-\eb_c)^\top\W\hb_i\right)|\cdot\left(\eb_{y_i}-\eb_c\right)^\top\Ab\hb_i\,.
\end{align}
% Note from \eqref{eq:CE gradient inp} that the exact same formula holds for the CE loss. Thus, it is not surprising that the same function $\Gc(\W)$ used for our analysis of CE loss also applies to PLL.

This motivates us to define
\begin{align}
    \Gcpll(\W) = \frac{1}{n}\sum_{i\in[n]}\sum_{c\neq y_i}\left|f'\left(-(\eb_{y_i}-\eb_c)^\top\W\hb_i\right)\right| = \frac{1}{n}\sum_{i\in[n]}\sum_{c\neq y_i}\frac{\exp\left(-(\eb_{y_i}-\eb_c)^\top\W\hb_i\right)}{1+\exp\left(-(\eb_{y_i}-\eb_c)^\top\W\hb_i\right)}
\end{align}

\begin{lemma}[Analogue of Lemma \ref{lem:G and gradient} for PLL]
\label{lem:PLL G and gradient}
For any $\W$, the PairLogLoss (PLL) satisfies:
    \[
    2B \cdot \Gcpll(\W) \geq \none{\nabla\Lcpll(\W)} \geq \gamma\cdot \Gcpll(\W)\,.
    \]
\end{lemma}
\begin{proof}
The lower bound follows immediately from \eqref{eq:PLL inp} and expressing $\none{\nabla\Lcpll(\W)}=\max_{\ninf{\Ab}\leq 1}\inp{\Ab}{-\nabla\Lcpll(\W)}$. The lower bound follows from triangle inequality applied to \eqref{eq:pll gradient}:
\[
\none{\nabla\Lcpll(\W)}\leq \frac{1}{n}\sum_{i\in[n]}\sum_{c \neq y_i}\left|f'\left(-(\eb_{y_i}-\eb_c)^\top\W\hb_i\right)\right|\|\eb_{y_i}-\eb_c\|_1\|\hb_i\|_1 \leq 2B\cdot \Gc(\W)\,.
\]
\end{proof}

For bounding with $\Gc(\W)$ the second-order term in the Taylor expansion of PLL, note the following. First, for all $i\in[n],c\neq y_i$:
\begin{align*}
    \hb_i^\top\Deltab^\top(\eb_{y_i}-\eb_c)(\eb_{y_i}-\eb_c)^\top\Deltab\hb_i &= \inp{(\eb_{y_i}-\eb_c)(\eb_{y_i}-\eb_c)^\top}{\Deltab\hb_i\hb_i^\top\Deltab^T}\leq \none{(\eb_{y_i}-\eb_c)(\eb_{y_i}-\eb_c)^\top} \ninf{\Deltab\hb_i\hb_i^\top\Deltab^T}
    \\
    &\leq \|\eb_{y_i}-\eb_c)\|_1^2 \cdot \|\Deltab\hb_i\|_\infty^2
    \\
    &\leq 4 \cdot \left(\ninf{\Deltab}\right)^2 \cdot \|\hb_i\|_1^2 \leq 4B^2 \left(\ninf{\Deltab}\right)^2\,.
\end{align*}
Second,  the (easy to check) property of logistic loss that $f''(t)\leq |f'(t)|$. 
Putting these together:
\[
\frac{1}{n}\sum_{i\in[n]}\sum_{c\neq y_i}f''\left((\eb_{y_i}-\eb_c)^\top\W\hb_i\right)\cdot \hb_i^\top\Deltab^\top(\eb_{y_i}-\eb_c)(\eb_{y_i}-\eb_c)^\top\Deltab\hb_i \leq 4B^2 \cdot\Gc(\W)\cdot \left(\ninf{\Deltab}\right)^2\,.
\]

Finally, we verify PLL satisfies Lemma \ref{lem:G and L}.

\begin{lemma}[Analogue of Lemma \ref{lem:G and L} for PLL]\label{lem:pll G and L}
    Let $\W \in \R^{k \times d}$, we have
    \begin{enumerate}[label=(\roman*)]
    \item $1\geq \frac{\Gcpll(\W)}{\Lcpll(\W)} \geq 1-\frac{n\Lcpll(\W)}{2} $
    \item  Suppose that $\W$ satisfies $\Lcpll (\W) \leq \frac{\log 2}{n}$ or $\Gcpll(\W) \leq \frac{1}{2n}$, then $\Lcpll(\W) \leq 2 \Gcpll(\W).$
    \end{enumerate}
\end{lemma}
\begin{proof}
    (i) The upper bound follows by the well-known self-boundedness property of the logistic loss, namely $|f'(t)|\leq f(t)$ 
    

To prove the upper bound, it suffices to prove for for $x>0$:
\begin{align}\label{eq:pll GL lb proof n=1}
\frac{x}{1+x} \geq \log(1+x) - \frac{1}{2}\log^2(1+x).
\end{align}
The general case follows by summing over $x_{ic}=\exp\left(-(\eb_{y_i}-\eb_c)^\top\W\hb_i\right), i\in[n], c\neq y_i$ since then we have
\begin{align}
\Gc(\W)=\sum_{i\in[n]}\sum_{c\neq y_i}\frac{x_{ic}}{1+x_{ic}} 
&\geq \sum_{i\in[n]}\sum_{c\neq y_i}\log(1+x_{ic}) - \frac{1}{2}\sum_{i\in[n]}\sum_{c\neq y_i}\log^2(1+x_{ic}) \nn
\\
&\geq \sum_{i\in[n]}\sum_{c\neq y_i}\log(1+x_{ic}) - \frac{1}{2}\left(\sum_{i\in[n]}\sum_{c\neq y_i}\log(1+x_{ic})\right)^2\,,\nn
\end{align}
where the last line used $\log(1+x_{ic})\geq 0$.
 For \eqref{eq:GL lb proof n=1}, let $a=\log(1+x)>0$. The inequality becomes $e^{-a}\leq 1-a+a^2/2$, which holds for $a>0$ by the second-order Taylor expansion of $e^{-a}$ around $0$.

 (ii) Denote $\Lc \coloneqq \Lc_{pll}$ and $\Gc \coloneqq \Gc_{pll}$. Given $\Lc \leq \frac{\log(2)}{n} \leq \frac{1}{n}$, we have $1-\frac{n \Lc}{2} \geq \frac{1}{2}$, then the first part follows from (i). For the second part, denote $l_{ic} := (\eb_{y_i}-\eb_c)^\top\W\hb_i, i\in[n], c\neq y_i$.  For $\Lc \leq 2\Gc$ to hold, it is sufficient to show that $\log (1 + e^{-l_{ic}}) \leq 2 \frac{e^{-l_{ic}}}{1+ e^{-l_{ic}}}$ for all $i\in[n], c\neq y_i$. This holds true when $l_{ic} \geq -1.366$, which is clearly satisfied given the assumption $\Gc \leq \frac{1}{2n}$ implying $l_{ic} \geq 0$. 
\end{proof}

\begin{lemma} [Analogue of Lemma \ref{lem:G_ratio} for PLL] \label{lem:G_ratio_pll}
    For any $\psi \in [0,1]$, we have the following:
    \begin{align*}
        \frac{\Gc_{pll}(\W + \psi \triangle \W)}{\Gc_{pll}(\W)} \leq e^{2 B \psi \ninf{\triangle \W}} + 2
    \end{align*}
\end{lemma}

\begin{proof}
    For logistic loss $f(z) = \log(1 + e^{-z})$, for any $z_1, z_2 \in \R$, we have the following
    \begin{align*}
        \bigm| \frac{f'(z_1)}{f'(z_2)}\bigm| = \bigm| \frac{1+e^{z_2}}{1+e^{z_1}} \bigm| &= \bigm| \frac{1 + e^{z_2} - e^{z_1} + e^{z_1}}{1 + e^{z_1}} \bigm| \\
        &= \bigm| \frac{e^{z_2} - e^{z_1}}{1+e^{z_1}} +1\bigm| \leq \bigm| \frac{e^{z_2} - e^{z_1}}{1+e^{z_1}} \bigm| + 1\\
        &\leq \bigm| e^{z_2 - z_1} - 1 \bigm| + 1\\
        &\leq e^{|z_2-z_1|} + 2.
    \end{align*}
    Denote $x_{ic}^{\W} := (\e_{y_i} - \e_c)^T \W \hb_i$ and $x_{ic}^{\W'} := (\e_{y_i} - \e_c)^T (\W + \psi \Deltab \W) \hb_i$, then we have for $i \in [n]$, $c \neq y_i$
    \begin{align*}
        \frac{f'(x_{ic}^{\W'})}{f'(x_{ic}^{\W})} = |\frac{f'(x_{ic}^{\W'})}{f'(x_{ic}^{\W})}| \leq e^{|x_{ic}^{\W} - x_{ic}^{\W'}|} + 2 &= e^{\psi|(\e_c - \e_{y_i})^T \Deltab \W \hb_i|} + 2 = e^{\psi |\langle \Deltab \W, (\e_c - \e_{y_i}) \hb_i^T\rangle|} +2\\
        &\leq e^{\psi \lVert \Deltab \W \rVert_{\max} \none{(\e_c - \e_{y_i}) \hb_i^T}} + 2 \\ 
        &= e^{\psi \lVert \Deltab \W \rVert_{\max} \none{\e_c - \e_{y_i}} \none{\hb_i}} + 2 \\
        &\leq e^{2B\psi\ninf{\Deltab \W}} + 2.
    \end{align*}
    This leads to $\sum_{i \in [n]} \sum_{c \neq y_i} f'(x_{ic}^{\W'}) \leq (e^{2B\psi\ninf{\Deltab \W}} + 2) \sum_{i \in [n]} \sum_{c \neq y_i} f'(x_{ic}^{\W})$. Rearrange and using the definition of $\Gc_{pll}(\W)$, we obtain the desired.  
\end{proof}

\begin{lemma} [Analogue of Lemma \ref{lem:sep} for PLL]
\label{lem:sep_pll}
    Suppose that there exists $\W \in \R^{k\times d}$ such that $\Lc_{pll} (\W) \leq \frac{\log 2}{n}$, then we have
    \begin{align}
        (\eb_{y_i} - \e_c)^T \W \hb_i \geq 0, \quad \text{for all $i \in [n]$ and for all $c \in [k]$ such that $c \neq y_i$}. \label{eq:sep}
    \end{align}
\end{lemma}
\begin{proof}
    Denote $x_{ic} = (\e_{y_i} - \e_c)^T \W \hb_i$. Then, by the assumption, we have for any $i \in [n], c \neq y_i$
    \begin{align*}
        \log(1 + e^{-x_{ic}}) \leq \sum_{i \in [n]} \sum_{c \neq y_i} \log(1+ e^{-x_{ic}}) \leq \log(2).
    \end{align*}
    This implies that $x_{ic} \geq 0$ for all $i \in [n], c \neq y_i$. 
\end{proof}

\begin{lemma} [Analogue of Lemma \ref{lem:l_fast_bound} for PLL] 
\label{lem:l_pll_fast_bound}
    For any $\W, \W_0 \in \R^{k \times d}$, suppose that $\Lc(\W)$ is convex, we have
    \begin{align*}
        |\Lc_{pll}(\W) - \Lc_{pll}(\W_0)| \leq 2B \ninf{\W - \W_0}. 
    \end{align*}
\end{lemma}
\begin{proof}
    This lemma is a direct consequence of Lemma \ref{lem:PLL G and gradient} and be proved in the same way as Lemma \ref{lem:l_fast_bound}. 
\end{proof}

Thus, we have proved all the Lemmas for $\Gc_{pll}(\W)$ and its relationships to $\Lc_{pll} (\W)$ in analogous to those  in section \ref{sec: G and L}. The proof of SignGD (\eqref{eq: signGD}) or NSD (\eqref{eq:nsd_main}) with PairLogLoss  follow the same steps as with cross-entropy loss given in section \ref{sec:app_sign}.  

% \section{Some Tighter Bounds} \label{sec:tighter_bounds}
% In this section, we derive some tighter results when linking the first ($\M_t$) and second moment ($\Vb_t$) of Adam to the proxy function $\Gc(\W)$. We start by rewriting $\Gc(\W)$ in a per-class form:
% \begin{align*}
%     \Gc(\W) = \frac{1}{n} \sum_{i=1}^{n} (1-s_{iyi}) = \frac{1}{n} \sum_{c \in [k]} \sum_{y_i = c} (1 - s_{i y_i}) = \sum_{c \in [k]} \frac{1}{n} \sum_{y_i = c} (1 - s_{i y_i}) = \sum_{c \in [k]} \Gc_c(\W),
% \end{align*}
% where $\Gc_c(\W) \coloneqq \frac{1}{n} \sum_{y_i = c} (1-s_{i y_i})$. We further denote $\Qc_c(\W) \coloneqq \frac{1}{n} \sum_{y_i \neq c} s_{ic}$ for any $c \in [k]$ (Note that the summation here is over all $i \in [n]$ such that $y_i \neq c$). 
% Then, the result of Lemma \ref{lem:first_G} can be extended to: let $c \in [k]$, for all $j \in [d]$, the following holds 
% \begin{align}
%     |\mathbf{M}_t[c,j] - (1-\beta_{1}^{t+1})\nabla \Lc(\W_t)[c,j]| &\leq \tilde{\alpha}_M \eta_t (\Gc(\W_t) + \Qc(\W_t)). \label{eq:tighter_M_bound}
% \end{align}
% This is done by recognizing that the $\clubsuit_1$ term in the proof of Lemma \ref{lem:first_G} can be bounded by $(e^{2 \alpha B \sum_{s=1}^\tau \eta_{t-s}} - 1) \Qc_c(\W_t)$, and the $\clubsuit_2$ term can be bounded by $(e^{2 \alpha B \sum_{s=1}^\tau \eta_{t-s}} - 1) \Gc_c(\W_t)$. Note that the bound in \eqref{eq:tighter_M_bound} is tighter than the bound in Lemma \ref{lem:first_G}. 
\end{document} 
\end{document}


% This document was modified from the file originally made available by
% Pat Langley and Andrea Danyluk for ICML-2K. This version was created
% by Iain Murray in 2018, and modified by Alexandre Bouchard in
% 2019 and 2021 and by Csaba Szepesvari, Gang Niu and Sivan Sabato in 2022.
% Modified again in 2023 and 2024 by Sivan Sabato and Jonathan Scarlett.
% Previous contributors include Dan Roy, Lise Getoor and Tobias
% Scheffer, which was slightly modified from the 2010 version by
% Thorsten Joachims & Johannes Fuernkranz, slightly modified from the
% 2009 version by Kiri Wagstaff and Sam Roweis's 2008 version, which is
% slightly modified from Prasad Tadepalli's 2007 version which is a
% lightly changed version of the previous year's version by Andrew
% Moore, which was in turn edited from those of Kristian Kersting and
% Codrina Lauth. Alex Smola contributed to the algorithmic style files.
