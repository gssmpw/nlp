
@article{pytorch,
  title={PyTorch: An Imperative Style, High-Performance Deep Learning Library},
  author={Paszke, Adam and Gross, Sam and Massa, Francisco and Lerer, Adam and Bradbury, James and Chanan, Gregory and Killeen, Trevor and Lin, Zeming and Gimelshein, Natalia and Antiga, Luca and others},
  journal={Advances in Neural Information Processing Systems},
  volume={32},
  year={2019}
}

@article{wang2024unified,
  title={Unified Binary and Multiclass Margin-Based Classification},
  author={Wang, Yutong and Scott, Clayton},
  journal={Journal of Machine Learning Research},
  volume={25},
  number={143},
  pages={1--51},
  year={2024}
}

@inproceedings{ji2021characterizing,
  title={Characterizing the implicit bias via a primal-dual analysis},
  author={Ji, Ziwei and Telgarsky, Matus},
  booktitle={Algorithmic Learning Theory},
  pages={772--804},
  year={2021},
  organization={PMLR}
}

@inproceedings{gunasekar2018characterizing,
  title={Characterizing implicit bias in terms of optimization geometry},
  author={Gunasekar, Suriya and Lee, Jason and Soudry, Daniel and Srebro, Nathan},
  booktitle={International Conference on Machine Learning},
  pages={1832--1841},
  year={2018},
  organization={PMLR}
}

@article{soudry2018implicit,
  title={The implicit bias of gradient descent on separable data},
  author={Soudry, Daniel and Hoffer, Elad and Nacson, Mor Shpigel and Gunasekar, Suriya and Srebro, Nathan},
  journal={Journal of Machine Learning Research},
  volume={19},
  number={70},
  pages={1--57},
  year={2018}
}

@inproceedings{nacson2019convergence,
  title={Convergence of gradient descent on separable data},
  author={Nacson, Mor Shpigel and Lee, Jason and Gunasekar, Suriya and Savarese, Pedro Henrique Pamplona and Srebro, Nathan and Soudry, Daniel},
  booktitle={The 22nd International Conference on Artificial Intelligence and Statistics},
  pages={3420--3428},
  year={2019},
  organization={PMLR}
}

@inproceedings{ji2019implicit,
  title={The implicit bias of gradient descent on nonseparable data},
  author={Ji, Ziwei and Telgarsky, Matus},
  booktitle={Conference on learning theory},
  pages={1772--1798},
  year={2019},
  organization={PMLR}
}

@inproceedings{wang2021implicit,
  title={The implicit bias for adaptive optimization algorithms on homogeneous neural networks},
  author={Wang, Bohan and Meng, Qi and Chen, Wei and Liu, Tie-Yan},
  booktitle={International Conference on Machine Learning},
  pages={10849--10858},
  year={2021},
  organization={PMLR}
}

@article{ravi2024implicit,
  title={The Implicit Bias of Gradient Descent on Separable Multiclass Data},
  author={Ravi, Hrithik and Scott, Clayton and Soudry, Daniel and Wang, Yutong},
  journal={arXiv preprint arXiv:2411.01350},
  year={2024}
}

@article{xie2024implicit,
  title={Implicit Bias of AdamW: L-infinity Norm Constrained Optimization},
  author={Xie, Shuo and Li, Zhiyuan},
  journal={arXiv preprint arXiv:2404.04454},
  year={2024}
}

@article{wu2024implicit,
  title={Implicit bias of gradient descent for logistic regression at the edge of stability},
  author={Wu, Jingfeng and Braverman, Vladimir and Lee, Jason D},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2024}
}

@article{zhang2024implicit,
  title={The Implicit Bias of Adam on Separable Data},
  author={Zhang, Chenyang and Zou, Difan and Cao, Yuan},
  journal={arXiv preprint arXiv:2406.10650},
  year={2024}
}

@article{bottou2018optimization,
  title={Optimization methods for large-scale machine learning},
  author={Bottou, L{\'e}on and Curtis, Frank E and Nocedal, Jorge},
  journal={SIAM review},
  volume={60},
  number={2},
  pages={223--311},
  year={2018},
  publisher={SIAM}
}

@article{huang2021super,
  title={Super-adam: faster and universal framework of adaptive gradients},
  author={Huang, Feihu and Li, Junyi and Huang, Heng},
  journal={Advances in Neural Information Processing Systems},
  volume={34},
  pages={9074--9085},
  year={2021}
}

@article{wang2021rank4class,
  title={Rank4Class: a ranking formulation for multiclass classification},
  author={Wang, Nan and Qin, Zhen and Yan, Le and Zhuang, Honglei and Wang, Xuanhui and Bendersky, Michael and Najork, Marc},
  journal={arXiv preprint arXiv:2112.09727},
  year={2021}
}

@article{mukherjee2010theory,
  title={A theory of multiclass boosting},
  author={Mukherjee, Indraneel and Schapire, Robert E},
  journal={Advances in Neural Information Processing Systems},
  volume={23},
  year={2010}
}

@article{qian2019implicit,
  title={The implicit bias of adagrad on separable data},
  author={Qian, Qian and Qian, Xiaoyuan},
  journal={Advances in Neural Information Processing Systems},
  volume={32},
  year={2019}
}

@article{wang2022does,
  title={Does momentum change the implicit regularization on separable data?},
  author={Wang, Bohan and Meng, Qi and Zhang, Huishuai and Sun, Ruoyu and Chen, Wei and Ma, Zhi-Ming and Liu, Tie-Yan},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={26764--26776},
  year={2022}
}


@article{lyu2019gradient,
  title={Gradient descent maximizes the margin of homogeneous neural networks},
  author={Lyu, Kaifeng and Li, Jian},
  journal={arXiv preprint arXiv:1906.05890},
  year={2019}
}

@article{duchi2011adaptive,
  title={Adaptive subgradient methods for online learning and stochastic optimization.},
  author={Duchi, John and Hazan, Elad and Singer, Yoram},
  journal={Journal of machine learning research},
  volume={12},
  number={7},
  year={2011}
}


@article{ji2020directional,
  title={Directional convergence and alignment in deep learning},
  author={Ji, Ziwei and Telgarsky, Matus},
  journal={Advances in Neural Information Processing Systems},
  volume={33},
  pages={17176--17186},
  year={2020}
}

@article{wu2024large,
  title={Large Stepsize Gradient Descent for Logistic Loss: Non-Monotonicity of the Loss Improves Optimization Efficiency},
  author={Wu, Jingfeng and Bartlett, Peter L and Telgarsky, Matus and Yu, Bin},
  journal={arXiv preprint arXiv:2402.15926},
  year={2024}
}

@article{zhang2020adaptive,
  title={Why are adaptive methods good for attention models?},
  author={Zhang, Jingzhao and Karimireddy, Sai Praneeth and Veit, Andreas and Kim, Seungyeon and Reddi, Sashank and Kumar, Sanjiv and Sra, Suvrit},
  journal={Advances in Neural Information Processing Systems},
  volume={33},
  pages={15383--15393},
  year={2020}
}

@article{kunstner2024heavy,
  title={Heavy-tailed class imbalance and why adam outperforms gradient descent on language models},
  author={Kunstner, Frederik and Yadav, Robin and Milligan, Alan and Schmidt, Mark and Bietti, Alberto},
  journal={arXiv preprint arXiv:2402.19449},
  year={2024}
}

@article{vaswani2017attention,
  title={Attention is all you need},
  author={Vaswani, A},
  journal={Advances in Neural Information Processing Systems},
  year={2017}
}

@article{vardi2023implicit,
  title={On the implicit bias in deep-learning algorithms},
  author={Vardi, Gal},
  journal={Communications of the ACM},
  volume={66},
  number={6},
  pages={86--93},
  year={2023},
  publisher={ACM New York, NY, USA}
}

@article{tsilivis2024flavors,
  title={Flavors of Margin: Implicit Bias of Steepest Descent in Homogeneous Neural Networks},
  author={Tsilivis, Nikolaos and Vardi, Gal and Kempe, Julia},
  journal={arXiv preprint arXiv:2410.22069},
  year={2024}
}

@article{wang2023faster,
  title={Faster margin maximization rates for generic optimization methods},
  author={Wang, Guanghui and Hu, Zihao and Muthukumar, Vidya and Abernethy, Jacob D},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  pages={62488--62518},
  year={2023}
}

@inproceedings{nutini2015coordinate,
  title={Coordinate descent converges faster with the gauss-southwell rule than random selection},
  author={Nutini, Julie and Schmidt, Mark and Laradji, Issam and Friedlander, Michael and Koepke, Hoyt},
  booktitle={International Conference on Machine Learning},
  pages={1632--1641},
  year={2015},
  organization={PMLR}
}