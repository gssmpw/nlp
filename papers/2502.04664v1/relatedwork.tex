\section{Related Works}
\label{sec:related work}
%\vspace{-0.05in}
%In this section, we briefly review the implicit bias results of GD, Adam, and other algorithms. 
Starting with GD,  
% \ct{I can write first draft}
% The theory of implicit bias (or implicit regularization) was established through a series of influential works \cite{gunasekar2017implicit,gunasekar2018characterizing,soudry2018implicit}. 
the foundational result by \cite{soudry2018implicit} showed that gradient descent optimization of logistic loss on linearly separable data converges in direction to the $L_2$ max-margin classifier at a rate $O(1/\log(t))$. Contemporaneous work by \cite{ji2019implicit} generalized this by removing the data separability requirement. \cite{ji2020gradient} later connected these findings to earlier work on regularization paths of logistic loss minimization \citep{Rosset2003MarginML}, which enabled extensions to other loss functions (e.g., those with polynomial tail decay). More recently, \cite{wu2024implicit} extends these results to the large step size regime with the same $O(1/\log(t))$ rate. The relatively slow convergence rate to the max-margin classifier motivated investigation into adaptive step-sizes. \cite{nacson2019convergence} showed that normalized gradient descent (NGD) with decaying step-size $\eta_t=1/\sqrt{t}$ achieves $L_2$-margin convergence at rate $O(1/\sqrt{t})$. This rate was improved to $O(1/t)$ by \cite{ji2021characterizing} using constant step-sizes, and further to $O(1/t^2)$ through a specific momentum formulation \citep{ji2021fast}. Besides linear classifications, implicit bias of GD has been studied for least squares \citep{gunasekar2017implicit,gunasekar2018characterizing}, homogeneous  \citep{lyu2019gradient,ji2020directional} and non-homogeneous neural networks \citep{wu2024large}, and matrix factorization \citep{gunasekar2017implicit}; see \cite{vardi2023implicit} for a survey.
% Another particularly interesting line of work has investigated the implicit bias of algorithms beyond vanilla GD, motivated by a core promise of implicit bias theory: understanding how optimizer choice affects generalization properties.

Beyond GD, \cite{gunasekar2018characterizing} and \cite{nacson2019convergence} showed that steepest descent optimization w.r.t. norm  $\|\cdot\|$ yields updates that in the limit maximize the margin with respect to the same norm.
%— this includes GD as a special case since it corresponds to steepest descent with respect to $L_2$-norm. 
\cite{sun2022mirror} showed that mirror descent  with potential function chosen as the $p$-th power of the $p$-norm (an algorithm which also enjoys efficient parallelization) yields updates that converge in direction to the classifier that maximizes the margin with respect to the $p$-norm. In both  cases, the convergence rate  is slow at $O(1/\log(t))$. \citet{wang2023faster} further improved the rates for both steepest descent and mirror descent when $p \in (1,2]$. It is important to note that all these results apply only to the exponential loss. More recently, \citet{tsilivis2024flavors} have shown that iterates of steepest descent algorithms converge to a KKT point of a generalized margin maximization problem in homogeneous neural networks.

On the other hand, the implicit bias of adaptive algorithms such as Adagrad \citep{duchi2011adaptive} or Adam \citep{kingma2014adam} is less explored compared to GD. \cite{qian2019implicit} studied the implict bias of Adagrad and showed its directional convergence to a solution characterized by a quadratic minimization problem. \cite{wang2021implicit,wang2022does} demonstrated the normalized iterates of Adam (with non-negligible stability constant) converge to a KKT point of a $L_2$-margin maximization problem for homogeneous neural networks. More recently and most relevant to our work, \cite{zhang2024implicit} studied the implicit bias of Adam without the stability constant on binary linearly separable data. They showed that unlike GD, the Adam iterates converge to a solution that maximizes the margin with respect to the $L_{\infty}$-norm. 
%This shows the fundamental difference between the implicit bias of Adam and GD. 
This study excluding the stability constant is practically-relevant given the magnitude of the constant is typically very small (default $1e-8$ in PyTorch \citep{pytorch}). This setting is also the focus of another closely-related  recent study of the implicit bias of AdamW \citep{xie2024implicit}, where the authors again establish that convergence aligns with the $L_{\infty}$ (rather than $L_{2}$) geometry. Our work extends these latter studies to the multiclass setting
(see Remark \ref{remark:adam} for technical comparisons).
%\cf{remark 6.5 only compares to Zhang}
%Here, we extend these latter studies 

All the above mentioned works focus solely on binary classification. The noticeable gap in analysis of multi class classification in most existing literature is  recently emphasized by \cite{ravi2024implicit} who extend the implicit bias result of \cite{soudry2018implicit} to multiclass classification for losses with exponential tails, including cross-entropy, multiclass exponential, and PairLogLoss. Their approach leverages a framework introduced by \cite{wang2024unified}, mapping multiclass analysis to binary cases. Our work directly addresses their open questions regarding the implicit bias of alternative gradient-based methods in multiclass settings by analyzing methods with adaptive step-sizes. Thanks to the adaptive step-sizes, our rates of convergence to the margin improve to polynomial dependence on $t$. Furthermore, our technical approach differs: rather than mapping to binary analysis, we work directly with multiclass losses, exploiting properties of the softmax function to produce elegant proofs that apply to all three losses studied by \cite{ravi2024implicit}. Our class-wise decomposition is crucial for analyzing Adam with the same convergence rate as the binary case, avoiding any extra factors that depend on the number of classes.
% To combine faster polynomial convergence with flexibility in the choice of margin norm, \cite{sun2023unified} and \cite{nacson2019convergence} investigated normalized mirror descent and normalized steepest descent respectively, with SignGD emerging as a special case of the latter. 

% However, the results of both \cite{nacson2019convergence,sun2023unified} apply only to exponential loss. Moreover, all the above mentioned works focus solely on binary classification. Thus, our work directly extends \citet{nacson2019convergence,sun2023unified}'s results in two significant ways: to multiclass classification and to the widely-used cross-entropy loss. Additionally, we provide the first analysis of an algorithm with momentum—namely Adam—not considered in these works.


% The noticeable gap in analysis of multi class classification in most existing literature is also recently emphasized by \cite{ravi2024implicit} who extend the implicit bias result of \cite{soudry2018implicit} to multiclass classification for losses with exponential tails, including cross-entropy, multiclass exponential, and PairLogLoss. Their approach leverages a framework introduced by \cite{wang2024unified}, mapping multiclass analysis to binary cases. Our work directly addresses their open question regarding implicit bias of alternative gradient-based methods in multiclass settings by analyzing methods with adaptive step-sizes. Thanks to the adaptive step-sizes, our rates of convergence to the margin improve to polynomial dependence on t. Furthermore, our technical approach differs: rather than mapping to binary analysis, we work directly with multiclass losses, exploiting properties of the softmax function to produce elegant proofs that apply to all three losses studied by \cite{ravi2024implicit}.

% Perhaps the most closely related work to ours is \cite{zhang2024implicit}
% \cf{Also add the following: \cite{qian2019implicit, wang2021implicit,wang2022does}, see the introduction in \cite{zhang2024implicit} for a discussion on the implications of $\epsilon$.}



% Beyond the textbook setting of linear classification, there is significant research activity in understanding implicit bias for nonlinear architectures, including linear neural networks, homogeneous networks, CNNs \cite{lyu2020Gradient,ji2020directional,gunasekar2018characterizing,gunasekar2018implicit}, and more recently self-attention mechanisms \cite{tarzanagh2023maxmargin,tarzanagh2023transformers,deora2024implicit}. Together with \cite{other Adam implicit-bias papers}, our work sets the foundations for extending the implicit bias analysis of SignGD and Adam to such settings. The implicit bias perspective has proven particularly valuable in studying generalization in overparameterized settings, where it simplifies optimization complexities before addressing generalization questions, effectively reducing the analysis to that of the corresponding max-margin classifier, \cite{muthukumar2020classification,hastie2019surprises,bartlett2020benign,koehler2021uniform,donhauser2022fast}. Most relevant to our results is the recent work of \cite{mohama} on grokking, which establishes that models achieving zero training loss with bounded $\|\cdot\|_\infty$ norm—a property our results show is promoted by SignGD and Adam in the linear case—generalize well with substantially fewer training points.



% \cite{https://openreview.net/pdf?id=nm40lbbwoR}
%\vspace{-0.05in}