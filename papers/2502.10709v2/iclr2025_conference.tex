\documentclass[table]{article}


\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{bm}
\usepackage{url}
\usepackage{booktabs}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{multirow}
\usepackage{graphicx}
\usepackage{wrapfig}
\usepackage{lipsum}
\usepackage{tabularx}
\usepackage{xcolor}
\usepackage{marvosym} % 如果需要特殊符号
\usepackage{soul} % 如果需要高亮功能
\usepackage{hyperref}
\usepackage{iclr2025_conference,times}



% Optional math commands from https://github.com/goodfeli/dlbook_notation.
% \input{math_commands.tex}


% \title{Evaluating Large Language Models with Uncertainty}
\title{An Empirical Analysis of Uncertainty in Large Language Model Evaluations}

% Authors must not appear in the submitted version. They should be hidden
% as long as the \iclrfinalcopy macro remains commented out below.
% Non-anonymous submissions will be rejected without review.

\makeatletter
\renewcommand*{\@fnsymbol}[1]{\ensuremath{\ifcase#1\or \or \dag\or \ddagger\or \mathsection\or \mathparagraph\or \|\or **\or \dag\dag \or \ddagger\ddagger \else\@ctrerr\fi}}
\makeatother

\author{Qiujie Xie$^{1,2}$   
        Qingqiu Li$^{3}$ 
        Zhuohao Yu$^{4}$  
        Yuejie Zhang$^{3}$ 
        Yue Zhang$^{2,5}$ 
        Linyi Yang$^{6,7}$\dag 
        \thanks{\dag  Correspondence to: Linyi Yang (\texttt{yanglinyiucd@gmail.com}).}
        \\
        {\footnotesize  $^{1}$Zhejiang University $^{2}$School of Engineering, Westlake University $^{3}$School of Computer Science,} \\ {\footnotesize Shanghai Key Lab of Intelligent Information Processing, Shanghai Collaborative Innovation Center } \\ 
        {\footnotesize of Intelligent Visual Computing, Fudan University
        $^{4}$Peking University $^{5}$Westlake Institute for} \\
        {\footnotesize Advanced Study
        $^{6}$University College London 
        $^{7}$Huawei Noah’s Ark Lab}
}  

% \author{Qiujie Xie \\
% Westlake University
% \and
% Qingqiu Li \\
% Fudan University \\
% \and
% Zhuohao Yu \\
% Peking University \\
% \and
% Yuejie Zhang \\
% Fudan University \\
% \and
% Linyi Yang \\
% Fudan University \\
% \and
% Yue Zhang \\
% Westlake University 
% }

% \author{Antiquus S.~Hippocampus, Natalia Cerebro \& Amelie P. Amygdale \thanks{ Use footnote for providing further information
% about author (webpage, alternative address)---\emph{not} for acknowledging
% funding agencies.  Funding acknowledgements go at the end of the paper.} \\
% Department of Computer Science\\
% Cranberry-Lemon University\\
% Pittsburgh, PA 15213, USA \\
% \texttt{\{hippo,brain,jen\}@cs.cranberry-lemon.edu} \\
% \And
% Ji Q. Ren \& Yevgeny LeNet \\
% Department of Computational Neuroscience \\
% University of the Witwatersrand \\
% Joburg, South Africa \\
% \texttt{\{robot,net\}@wits.ac.za} \\
% \AND
% Coauthor \\
% Affiliation \\
% Address \\
% \texttt{email}
% }

% The \author macro works with any number of authors. There are two commands
% used to separate the names and addresses of multiple authors: \And and \AND.
%
% Using \And between authors leaves it to \LaTeX{} to determine where to break
% the lines. Using \AND forces a linebreak at that point. So, if \LaTeX{}
% puts 3 of 4 authors names on the first line, and the last on the second
% line, try using \AND instead of \And before the third author name.

\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}

\iclrfinalcopy % Uncomment for camera-ready version, but NOT for submission.
\begin{document}


\maketitle



\begin{abstract}
As LLM-as-a-Judge emerges as a new paradigm for assessing large language models (LLMs), concerns have been raised regarding the alignment, bias, and stability of LLM evaluators. While substantial work has focused on alignment and bias, little research has concentrated on the stability of LLM evaluators. In this paper, we conduct extensive experiments involving 9 widely used LLM evaluators across 2 different evaluation settings to investigate the uncertainty in model-based LLM evaluations. We pinpoint that LLM evaluators exhibit varying uncertainty based on model families and sizes. With careful comparative analyses, we find that employing special prompting strategies, whether during inference or post-training, can alleviate evaluation uncertainty to some extent. By utilizing uncertainty to enhance LLM's reliability and detection capability in Out-Of-Distribution (OOD) data, we further fine-tune an uncertainty-aware LLM evaluator named ConfiLM using a human-annotated fine-tuning set and assess ConfiLM's OOD evaluation ability on a manually designed test set sourced from the 2024 Olympics. Experimental results demonstrate that incorporating uncertainty as additional information during the fine-tuning phase can largely improve the model's evaluation performance in OOD scenarios. The code and data are released at: \url{https://github.com/hasakiXie123/LLM-Evaluator-Uncertainty}.


% We hope this work can draw broader research attention to the stability of LLM evaluators.
% 随着model-based evaluation成为一种新的大模型评估范式，concerns have been raised about the alignment, bias, and stability of LLM evaluators. While extensive work has explored alignment, bias, little to no work 聚焦于 the stability of LLM evaluators。In this paper, we conduct extensive experiments and analysis involving 9 widely-used LLM evaluators with 2 different evaluation settings to investigate the existence and mitigation of uncertainty in model-based LLM evaluation. We pinpoint that LLM evaluators exhibit varying uncertainty based on model families and employ prompting strategies, whether during inferencing or post-training, can help alleviate evaluation uncertainty in LLM evaluators. To 进一步探索 如何utilize the uncertainty to improve the evaluation capability of LLM evaluators for Out-Of-Distribution (OOD) data，we first a diverse human-annotated fine-tuning set, based on which we fine-tune an uncertainty-aware LLM evaluator named ConfiLM, and assess its evaluation ability on a manually designed OOD test set sourced from Olympic 2024. 实验结果证明incorporating uncertainty as additional information into the fine-tuning phase can largely boost the model's evaluation performance in the OOD scenarios. We hope this work can draw broader research attention to the stability of LLM evaluators.
\end{abstract}



\section{Introduction}

\begin{figure*}[htbp]
    \vspace{-1em}
    \centering
    \setlength{\abovecaptionskip}{5pt}
    \includegraphics[width=0.75\textwidth]{figs/overview.pdf}
    \caption{An example of uncertainty (i.e., model confidence) in model-based LLM evaluation. The evaluation process is influenced by the uncertainty of both the evaluator and the candidate model.}
    \label{fig:overview}
    \vspace{-1em}
\end{figure*}

Large language models (LLMs) have garnered increasing attention due to their unprecedented performance in various real-world applications \citep{zhao2023survey, wang2024survey}. In this context, how to accurately assess the performance of a LLM becomes particularly important. This area of research includes benchmark-based evaluation, model-based evaluation, and human evaluation~\citep{chang2024survey}. While various benchmarks~\citep{zellers2019hellaswag, hendrycks2measuring,yang2023glue,xie2024human} have been proposed to measure the core abilities of LLMs in comprehension and generation, human evaluation remains the gold standard for testing overall performance due to its complexity and open-endless. However, this approach is limited by subjectivity issue~\citep{krishna2023longeval} and resource costs~\citep{karpinska2021perils}. Consequently, LLM evaluators have emerged as a cost-effective alternative to human evaluators, providing reproducible judgments for responses from different candidate models~\citep{zheng2023judging, wangpandalm, yu2024kieval}.


% 第一段：
% llm很火，怎么去evaluate很重要，最近llm-as-judge很火，展现出替代人类的趋势，
% 大语言模型（LLMs）因其unprecedented performance in various real-world applications而has garnered increasing attention. In this context, 如何准确地评估一个大语言模型的性能变得尤为重要。This line of work includes benchmark-based evaluation, model-based evaluation, and human evaluation. While various benchmarks have been proposed to measure the underlying ability of LLMs in 理解和生成. Human evaluation remains the gold standard for testing the general performance due to the 复杂性和开放性。However, this is limited by subjectivity (Krishna et al., 2023a) and reproducible (Karpinska et al., 2021). Hence, LLM evaluators have emerged as a cost-effective alternative for human evaluators, serving 可复现的 judgments for responses from different candidate models.

% However, as LLM-as-a-Judge gains more attention, criticism has also emerged. Researchers have raised concerns about the alignment, robustness, and stability of model-based LLM evaluation. There has been a surging interest in exploring whether LLM evaluators can truly understand complex contexts and make judgments aligned with human values~\citep{liu2024calibrating,yu2024kieval,hada2024large}, as well as whether they can maintain robust performance when faced with noise or unexpected inputs.

As LLM-as-a-Judge gains more attention, criticism has also emerged~\citep{thakur2024judging}. Researchers have raised concerns about the alignment~\citep{liu2024calibrating}, bias~\citep{wang2023large}, and stability of model-based LLM evaluation. There has been a surging interest in exploring whether LLM evaluators can truly understand complex contexts and make judgments aligned with human values~\citep{ yu2024kieval, hada2024large, dubois2024alpacafarm}, as well as whether they exhibit preference biases when faced with different inputs~\citep{koo2023benchmarking, liu2024aligning, thakur2024judging}. Despite significant research on LLM evaluators' alignment and bias, there has been relatively little work on the investigation of evaluation stability. In particular, the relationship between uncertainty and LLM-as-Judge is a question that remains to be underexplored. Can LLMs give consistent evaluation quality across different inputs and domains? 
 

% Despite the 充分的研究on LLM evaluators' alignment and robustness, there has been relatively little to no work on the evaluation stability. 大语言模型的不确定性与LLM-as-judge之间的关系是一个有待探索的问题。  Following previous studies that 将generation logit视作a proxy for模型置信度, we use token probabilities to represent the LLM's internal confidence.
% 第二段：
% 但是也有批评的声音，包括是否对齐，是否稳定，是否鲁棒。有一些工作解决对齐（kieval），解决鲁棒（adv）
% 然而，随着LLM-as-a-Judge获得更多关注，批评的声音也随之而来。一些研究者对LLM evaluators的对齐（alignment）、鲁棒性（robustness）和稳定性（stability）提出了质疑。previous work 聚焦于研究LLM evaluators是否能够真正理解复杂的语境并做出贴合人类价值观的判断以及在面对噪声或非预期输入时是否能保持稳健的表现
%模型是否在不同环境或任务下表现一致
% zhu2023promptbench
% 第三段：
% 尽管已经取得很大的进步，但是稳定性一直没得到充分地研究，大模型置信度和LLM-as-judge之间的关系是一个有待探索的问题（文献：logit与模型口是心非），我们把logit用在代表模型evaluation的confidence
% 第四段：
% 我们做了这样的pipline，我们构建几个分析实验+方法实验 在哪些数据集，做了什么事，得到了哪些结论， 发现了一些insight，
Following previous studies that treat generation logits as a proxy for model confidence~\citep{varshney2023stitch, kumar2024confidence, duan2024shifting, guptalanguage}, we use token probabilities to represent the LLM's internal confidence. 
Through extensive experiments (Figure~\ref{fig:experiment-intro}) involving 9 widely-used LLM evaluators under 2 different evaluation settings (single-answer grading and pairwise comparison) on the MT-Bench~\citep{zheng2023judging} and PandaLM~\citep{wangpandalm} test sets, we demonstrate that uncertainty is prevalent across LLMs and varies with model families and sizes (\S\ref{subsec:investigation-measurement}). We find that the evaluation confidence of LLM evaluators exhibits sensitivity to changes in data distribution (\S\ref{subsec:investigation-uncertainty-cause}). With careful comparative analyses, we pinpoint that employing special prompting strategies (e.g., chain-of-thoughts; \citet{wei2022chain}), whether during inference or post-training, can alleviate evaluation uncertainty to some extent (\S\ref{subsec:investigation-prompt} and \S\ref{subsec:investigation-trained}).

Prior work has shown that incorporating the model confidence during the LLM’s inference stage can improve reliability in OOD scenarios~\citep{yangsupervised} and enhance detection capability in hallucinations~\citep{farquhar2024detecting}. To leverage this fact, we further fine-tune an uncertainty-aware LLM evaluator named ConfiLM using instruction instances collected from the Alpaca 52K dataset~\citep{taori2023stanford}. For evaluation in OOD scenarios (\S\ref{sec:utilization}), we manually craft a test dataset called Olympic 2024 based on data from \href{https://olympics.com/en/paris-2024/}{the Olympics site}. Olympic 2024 contains 220 high-quality instances, each labeled by three PhD-level human evaluators.
% each labeled independently by three PhD-level human evaluators. 
Samples unanimously deemed low quality
% or difficult to evaluate 
by the annotators are removed, resulting in an annotator agreement rate of 97.27\%. Experimental results demonstrate that incorporating uncertainty as auxiliary information during the fine-tuning process can largely improve the LLM evaluators' performance in OOD scenarios.

% 总结段：
% To our knowledge, this is the first empirical study to investigate uncertainty as a fundamental aspect of LLM-as-Judge. 
In this paper, we conduct a comprehensive uncertainty analysis, propose a high-quality OOD test set, and offer an uncertainty-aware LLM evaluator named ConfiLM. Our empirical findings reveal the impact of uncertainty on LLM-as-Judge, especially in eliminating and utilizing evaluation uncertainty, shedding light on future research into the stability of model-based LLM evaluations. 

% To our knowledge, 我们是第一个把uncertainty作为辅助信息引入LLM-as-Judge的实验探究型工作。We conduct comprehensive uncertainty analysis, propose a 高质量的pairwise ood数据集并训练了一个uncertainty-aware的LLM Evaluator named ConfiLM。Our empirical findings reveal 不确定性对LLM-as-a-Judge的影响, shedding light on future research of model-based LLM evaluation.

\begin{figure*}[tbp]
    % \vspace{-1em}
    \centering
    \setlength{\abovecaptionskip}{5pt}
    \includegraphics[width=1.0\textwidth]{figs/experiment_introv7.pdf}
    \caption{We conduct extensive experiments and analysis to investigate the existence, mitigation and utilization of uncertainty in model-based LLM evaluation. Uncertainty plays a key role in the evaluation process and can be leveraged to enhance the evaluator's performance in OOD scenarios.
    % out-of-distribution (OOD) scenarios. 
    }
    % 简略版本
    % \caption{We conduct extensive experiments and corresponding analysis to investigate the existence, mitigation and utilization of uncertainty in model-based LLM evaluation.  
    % }
    \label{fig:experiment-intro}
    \vspace{-1.5em}
\end{figure*}


\vspace{-1.0em}
\section{Related Work}
\vspace{-0.5em}

With rapid development of LLMs, the accurate evaluation of their capabilities has become one of the key challenges in this field. Several LLM evaluation paradigms have been proposed in recent years \citep{chang2024survey}, which have coalesced around a few well-established methods, including benchmark-based evaluation, model-based evaluation, and human evaluation.

\textbf{Benchmark-based evaluations} involve using a set of standardized tests to quantitatively measure a model’s performance across different tasks. Examples include HellaSwag~\citep{zellers2019hellaswag}, HELM~\citep{liang2022holistic} and MMLU~\citep{hendrycks2020measuring} for general knowledge and reasoning, or MATH~\citep{hendrycks2measuring} and ToolBench~\citep{xu2023tool} for specific capabilities. The performance of LLMs is measured by their ability to correctly perform these tasks. However, these metrics often reflect models' performance in narrowly defined areas and risk inflated scores due to data contamination~\citep{orenproving}.

\textbf{Human evaluations} involve human raters who assess LLM performance based on criteria such as fluency, coherence, and relevance. This approach can take the form of A/B testing~\citep{tang2010overlapping}, preference ranking~\citep{bai2022training}, or scoring individual model outputs against predefined rubrics~\citep{novikova2017we}. While human evaluations are often considered the gold standard for tasks where quantitative metrics fall short, they are resource-intensive in terms of time and cost. Moreover, they are constrained by subjectivity~\citep{krishna2023longeval} and reproducible issues~\citep{karpinska2021perils}, limiting their scalability for large-scale assessments.

\textbf{Model-based evaluations} involve employing a powerful LLM as an auto-evaluator to assess the performance of the candidate model. This promising method serves as a cost-effective alternative to human evaluators~\citep{zheng2023judging, wangpandalm, yu2024kieval, yu2024freeeval}. However, concerns have been raised regarding the alignment, bias, and stability of model-based LLM evaluation. While researchers have made progress in exploring the alignment and bias of LLM evaluators~\citep{liu2024calibrating,wang2023large}, understanding the stability of these evaluators remains an open question. A concurrent work~\citep{doddapaneni2024finding} proposes a novel framework to evaluate the proficiency of LLM evaluators through targeted perturbations. Different from this work, we focus on the role of uncertainty in LLM-based evaluators, which has yet to be systematically explored.
% Our work addresses this gap by focusing on the role of model confidence, which has yet to be systematically explored.
%In this paper, we conduct an in-depth exploration of the uncertainty in model-based LLM evaluation.

\textbf{Confidence Estimation for LLMs.} 
% \subsection{Confidence Elicitation for LLMs}
Model confidence refers to the degree of certainty a model holds regarding its generated responses~\citep{gal2016uncertainty}. Reliable confidence estimation for LLM is crucial for effective human-machine collaboration, as it provides valuable insights into the reliability of the model’s output, facilitates risk assessment~\citep{geng2024survey}, and reduces hallucinations~\citep{varshney2023stitch}. Research in this field includes (1) verbalization-based methods~\citep{linteaching, yona2024can}, which prompt LLMs to directly output calibrated confidence along with their responses; (2) consistency-based methods~\citep{tian2023just, xiongcan}, which require LLMs to generate multiple responses for the same question and measure their consistency as a proxy for confidence; and (3) logit-based methods~\citep{duan2024shifting, malininuncertainty, kumar2024confidence}, which estimate confidence based on the model's internal states during response generation. Inspired by this line of work, we use token probabilities to represent the LLM's internal confidence. Previous work has considered the utilization of model confidence in natural language understanding~\citep{yangsupervised}, fact checking~\citep{geng2024multimodal} and hallucination detection~\citep{varshney2023stitch, farquhar2024detecting}. Differently, our work focuses on utilizing confidence within the evaluation process.

% A related concept is that of model calibration, which focuses on align- ing predictive probabilities (estimated confidence) to actual accuracy (Guo et al., 2017).
%researchers propose to evaluate the sentence uncertainty by the output languages directly
% researchers have explored prompting LLMs to output calibrated confidence alongside text (Tian et al., 2023) for determining LLMs’ re- liability.% linteaching yona2024can
%, consistency-based methods prompting LLMs to generate multiple responses to the same question and evaluating the self-consistency of those responses % Current calibra- tion methods for text generation rely on heuristics that consistency among multiple responses (Xiong et al., 2023) or the top-k responses facilitate cal- ibration (Tian et al., 2023).
% logit-based % huang2023look duan2024shifting malininuncertainty
%  In this paper, inspired by prior studies \citep{kumar2024confidence, zhou2023navigating, guptalanguage}, we use token probabilities to represent the LLM's internal confidence.

% \subsection{Interpretability for Large Language Models (optional)}

\section{Uncertainty in LLM-as-a-Judge}
\label{sec:task_definition}

% Confidence, or uncertainty, refers to the degree of certainty or assurance that accompanies a pre- diction or decision made by a model
\textbf{Task definitions.} To ensure the validity of our experimental conclusions, we conduct experiments under two distinct and commonly used evaluation settings, including single-answer grading and pairwise comparison. See Appendix \ref{appendix:prompts} for the relevant prompts.

(1) Single-answer grading~\citep{yu2024kieval, li2023alpacaeval, liu2023g}: given a user instruction $q$ and a response $r$ from the candidate model, the evaluator is tasked with assigning an overall score $s \in \mathbb{N}$ based on specific criteria set $c$, while minimizing potential bias. This is expressed as:

\vspace{-1.0em}
\begin{equation}
\vspace{-0.5em}
s = f(q, r; c, \boldsymbol{\theta}),
\end{equation} 

where $c = \{c_1, c_2, \ldots, c_m\}$, each $c_i$ represents a specific evaluation dimension (e.g., content accuracy, logical coherence); $\boldsymbol{\theta}$ represents the parameters of the LLM evaluator.

(2) Pairwise comparison~\citep{wangpandalm, zengevaluating, raina2024llm}: given an instruction $q$ and two responses $r_1$, $r_2$ from different candidate models, the evaluator is asked to compare the two responses and indicate a preference $p \in \{1, 2, \text{Tie}\}$ according to $c$, determining whether one response is better than the other or if they are 
% equally good according to specific evaluation criteria $c$.
equally good. This is expressed as:

\vspace{-1em}
\begin{equation}
% \vspace{-1em}
p = f(q, r_1, r_2; c, \boldsymbol{\theta})
\vspace{-0.5em}
\end{equation}

\textbf{Quantification of uncertainty.} As shown in Figure~\ref{fig:overview}, the LLM-based evaluation process is influenced by the uncertainty of both the evaluator (evaluation uncertainty) and the candidate model (response uncertainty). Following previous studies \citep{varshney2023stitch, zhou2023navigating, guptalanguage}, we use token probabilities to represent the LLM's internal confidence. Specifically, we take the probability of the token representing the evaluation result (e.g., ``Tie'') as the evaluation confidence. For response confidence, we calculate the average probabilities of all generated tokens. See Table~\ref{tab:appendix-confidence-calculation-example} for an example of tokens involved in the confidence calculation. 
To investigate whether different quantification methods impact the empirical findings, we conduct experiments under a pairwise comparison setting on the MT-Bench. The result is presented in Appendix \ref{appendix-sub:uncertainty}.




\section{The Impact of Confidence in LLM Evaluation} % experiment and experiment findings 
\label{sec:Investigation}
We present the empirical study involving 9 widely-used LLM evaluators (3 proprietary models~\citep{achiam2023gpt} and 6 open-source models~\citep{touvron2023llama, yang2024qwen2}) with 2 different evaluation settings (single-answer grading and pairwise comparison) on the MT-Bench~\citep{zheng2023judging} and PandaLM~\citep{wangpandalm} test datasets.
% In this section, we present the empirical study involving 6 widely-used LLM evaluators (3 proprietary models and 3 open-source models) with 2 different evaluation settings (single grading and pairwise comparison) on the MT-Bench dataset~\citep{zheng2023judging} and PandaLM test set~\citep{wangpandalm}.
%\ref{fig:experiment-intro}

% \begin{table*}[tbp]
% \setlength{\belowcaptionskip}{5pt}
%     % \vspace{-1em}
% \centering
% \captionsetup{skip=0pt}
% % comment
% % \caption{Uncertainty analysis on MT-Bench and PandaLM test set. For single-answer grading, the scoring range is 0-9. The evaluation subject is Llama2-7B-Instruct, with average response confidence of 0.945 (MT-Bench) and 0.936 (PandaLM test set). For pairwise comparison, the evaluation subjects are Llama2-7B-Instruct and Llama2-13B-Instruct, with average response confidences of 0.945, 0.949 (MT-Bench), and 0.935, 0.936 (PandaLM test set), respectively. For each evaluation, we query the evaluator twice with the order swapped. ``Win / Lose / Tie'' represents the average number of times Llama-2-7b-chat's response is better than, worse than, or equal to Llama-2-13b-chat's response.}
% \caption{Uncertainty analysis on MT-Bench and PandaLM test set. Uncertainty in model-based LLM evaluation varies significantly, with single-answer grading showing lower average confidence (65.4\%) compared to pairwise comparisons (79.7\%).}
% \label{tab:uncertainty-normal}
%     \begin{minipage}[b]{0.42\linewidth}
%         \subcaption{Single-answer grading.}
%         % \caption{Uncertainty analysis in single-answer grading. The scoring range is 0-9. The subject of the evaluation is Llama-2-7b-chat, which has an average response confidence of 0.945 (MT-Bench) and 0.936 (PandaLM test set).} % rating的范围，candidate model是什么  average response confidence for candidate model 是多少 
%         \label{tab:uncertainty-normal-single}
%         \large
%         \centering
%         \resizebox{1.0\textwidth}{!}{
%         \begin{tabular}{@{}c|cc|cc@{}}
%         \toprule[1pt]
%         \multirow{3}{*}{Evaluator} & \multicolumn{2}{c|}{MT-Bench}  & \multicolumn{2}{c}{PandaLM Test set} \\ \cmidrule(l){2-5} 
%                                    & Rating &  \begin{tabular}[c]{@{}c@{}}Evaluation\\ Confidence\end{tabular} & Rating    &  \begin{tabular}[c]{@{}c@{}}Evaluation\\ Confidence\end{tabular}   \\ \midrule[1pt]
%         GPT-4o                     & 5.413  & 0.417                 & 6.541     & 0.473                   \\
%         GPT-4o-mini                & 6.038  & 0.605                 & 6.641     & 0.645                   \\
%         GPT-3.5-Turbo              & 6.288  & 0.629                 & 6.665     & 0.594                   \\
%         Llama3-70B-Instruct        & 7.250  & 0.644                 & 7.424     & 0.548                   \\
%         Llama2-70B-Instruct        & 7.875  & 0.953                 & 7.924     & 0.960                   \\
%         Qwen2-72B-Instruct         & 5.875  & 0.675                 & 7.153     & 0.692                   \\ \midrule[1pt]
%         \textbf{Average}                    & \textbf{6.456}  & \textbf{0.654}                 & \textbf{7.058}     & \textbf{0.652}                   \\ \bottomrule[1pt]
%         \end{tabular}
%         }
%     \end{minipage}
%     \hfill
%     \begin{minipage}[b]{0.56\textwidth}
%         \subcaption{Pairwise comparison.}
%         % \caption{Uncertainty analysis in pairwise comparison. For each evaluation, we query the evaluator twice with the order swapped. The subjects of the evaluation are Llama-2-7b-chat and Llama-2-13b-chat, with average response confidences of 0.945 and 0.949 (MT-Bench), and 0.935 and 0.936 (PandaLM Test set), respectively. 'Win / Lose / Tie' represents the average number of times Llama-2-7b-chat's response is better than, worse than, or equal to Llama-2-13b-chat's response.} % wins 的含义，candidate model a和b是什么，结果是swap位置的平均值 average response confidence for candidate model a和b是多少 
%         \label{tab:uncertainty-normal-pair}
%         \centering
%         \large
%         \resizebox{1.0\textwidth}{!}{
%         \begin{tabular}{@{}c|cc|cc@{}}
%         \toprule[1pt]
%         \multirow{3}{*}{Evaluator} & \multicolumn{2}{c|}{MT-Bench} & \multicolumn{2}{c}{PandaLM Test set} \\ \cmidrule(l){2-5} 
%                                    & Win / Lose / Tie    & \begin{tabular}[c]{@{}c@{}}Evaluation\\ Confidence\end{tabular}  & Win / Lose / Tie        & \begin{tabular}[c]{@{}c@{}}Evaluation\\ Confidence\end{tabular}    \\ \midrule[1pt]
%         GPT-4o                     & 10.0 / 16.5 / 53.5  & 0.699       & 30.0 / 38.0 / 102.0     & 0.809         \\
%         GPT-4o-mini                & 27.0 / 43.5 / 9.5   & 0.776       & 53.0 / 61.0 / 56.0      & 0.820         \\
%         GPT-3.5-Turbo              & 38.5 / 35.0 / 6.5   & 0.848       & 76.5 / 81.0 / 12.5      & 0.884         \\
%         Llama3-70B-Instruct        & 39.5 / 37.5 / 3.0   & 0.791       & 78.0 / 86.5 / 5.5       & 0.849         \\
%         Llama2-70B-Instruct        & 33.0 / 34.0 / 13.0  & 0.908       & 72.5 / 73.0 / 24.5      & 0.931         \\
%         Qwen2-72B-Instruct         & 22.0 / 29.0 / 29.0  & 0.762       & 54.0 / 70.0 / 46.0      & 0.806         \\ \midrule[1pt]
%         \textbf{Average}                    & \textbf{28.3} / \textbf{32.6} / \textbf{19.1}  & \textbf{0.797}      & \textbf{60.7} / \textbf{68.2} / \textbf{41.1}      & \textbf{0.850}         \\ \bottomrule[1pt]
%         \end{tabular}
%         }
%     \end{minipage}
%     % \vspace{-1.5em}
% \end{table*}

\subsection{Experimental Settings}
\label{subsec:settings}
\textbf{Prompting strategies.} To explore whether special output formats can reduce the evaluation uncertainty of LLM evaluators, we conduct evaluations using prevalent prompting strategies, including:

(1) \textbf{Default}~\citep{wangpandalm, dubois2024alpacafarm}. We instruct the LLM to act as an impartial judge and consider factors such as helpfulness and relevance. The LLM is asked to first output its rating $s \in \{0, 1, \ldots, 9\}$ or preference $p \in \{1, 2, \text{Tie}\}$, followed by a brief explanation $e$.

(2) Chain-of-thoughts (\textbf{CoT};~\citep{wei2022chain, kojima2022large}). Instead of generating judgments first, we instruct the LLM to first generate a concise reasoning $e$ before providing its rating $s$ or preference $p$ for the responses.

(3) Self-generated reference (\textbf{Reference};~\citep{zheng2023judging, zengevaluating}). We prompt the LLM evaluator to generate a short reference answer $a$ for the given instruction $q$. The generated answer is then provided to the LLM evaluator as a reference when making its judgments.

\textbf{LLM Evaluators.} We employ 6 general yet powerful LLMs across various LLM families as evaluators, including GPT-4o~\citep{achiam2023gpt}, GPT-4o-mini, GPT-3.5-Turbo, Llama3-70B-Instruct~\citep{dubey2024llama}, Llama2-70B-Instruct and Qwen2-72B-Instruct~\citep{yang2024qwen2}. To explore the relationship between evaluation capability and evaluation stability~(\S\ref{subsec:investigation-trained}), we further assess the stability of 3 specialized LLM evaluators, including (1) \textbf{Prometheus2-7b} and \textbf{Prometheus2-bgb-8x7b} models~\citep{kim2024prometheus2,kim2024biggen}, both of which are trained to output in a \textbf{CoT} format, providing a concise rationale before indicating a preference or providing its rating; and (2) \textbf{PandaLM}~\citep{wangpandalm}, which is trained to output in a default format.
% Are those specially trained LLMs more stable evaluators? We answer this question by experimenting with 3 open-source evaluators including (1) \textbf{Prometheus2-7b} and \textbf{Prometheus2-bgb-8x7b} models~\citep{kim2024prometheus2,kim2024biggen}, both of which are trained to output in a Chain-of-Thoughts format, providing a concise rationale before indicating a preference between the two responses or providing its rating for single response; and (2) \textbf{PandaLM} model~\citep{wangpandalm}, which is trained to output in a normal format, offering a preference between the two responses, followed by a concise rationale.

To enhance reproducibility and alleviate the impact of temperature sampling on uncertainty analysis, we set the temperature to 0 for proprietary models, and utilize greedy decoding for open-source models. For single-answer grading, the scoring range is 0-9. The evaluation subject is Llama2-7B-Instruct. For pairwise comparison, the evaluation subjects are Llama2-7B-Instruct and Llama2-13B-Instruct. We query the evaluator twice with the order swapped to eliminate position bias~\citep{wang2023large, jung2019earlier}. 
% temperature To enhance reproducibility, we set the temperature to 0 for proprietary models, and utilize greedy decoding for open-source models.

\textbf{Benchmarks.} We conduct experiments on MT-Bench~\citep{zheng2023judging} and PandaLM~\citep{wangpandalm} test dataset. The MT-Bench contains 80 manually constructed questions designed to challenge chatbots based on their core capabilities on common tasks (e.g., reasoning and math). In contrast, the PandaLM test set contains 170 instructions sampled from the human evaluation dataset of self-instruct~\citep{wang2023self}, where expert-written instructions for novel tasks serve as a testbed for evaluating how instruction-based models handle diverse and unfamiliar instructions.
% MT-Bench consists of instructions designed to evaluate common tasks, focusing on core capabilities like reasoning and math. In contrast, the PandaLM test set includes diverse and unfamiliar instructions, representing more varied and less conventional tasks.
% the Alpaca 52K dataset~\citep{taori2023stanford}, which 
% . The Alpaca 52K dataset 
% is built on the self-instruct generation pipeline from \citet{wang2023self}.

% The instructions and inputs in the input tuple are sampled from the Alpaca 52K dataset automatically
% construct a large-scale dataset of 52K instructions in a self-instruct manner
% MT-bench is also manually constructed to differentiate chatbots based on their core capabilities, such as reasoning and math. 难度更高，更贴近真实分布

% \textbf{Quantification of uncertainty.} The LLM-based evaluation process is influenced by the uncertainty of both the evaluator (evaluation uncertainty) and the candidate model (response uncertainty). Following previous studies \citep{varshney2023stitch, kumar2024confidence, zhou2023navigating, guptalanguage}, we use token probabilities to represent the LLM's internal confidence. Specifically, we take the probability of the token representing the evaluation result (e.g., ``Tie'') as the evaluation confidence. For response confidence, we calculate the average probabilities of all generated tokens. See Table~\ref{tab:appendix-confidence-calculation-example} for an example of tokens involved in the confidence calculation.

\begin{table*}[tbp]
\setlength{\belowcaptionskip}{5pt}
    \vspace{-0.5em}
\centering
\captionsetup{skip=10pt}
% \renewcommand{\arraystretch}{1.1}
\caption{Uncertainty analysis of 6 LLM-based evaluators on MT-Bench and PandaLM test set. The evaluation subject is Llama2-7B-Instruct and Llama2-13B-Instruct. For single-answer grading, the scoring range is 0-9. 
% The evaluation subject is Llama2-7B-Instruct. For pairwise comparison, the evaluation subjects are Llama2-7B-Instruct and Llama2-13B-Instruct. 
``Win / Lose / Tie'' represents the average number of times Llama-2-7b-chat's response is better than, worse than, or equal to Llama-2-13b-chat's response.
% Uncertainty in model-based LLM evaluation varies significantly, with single-answer grading showing lower average confidence (65.4\%) than pairwise comparisons (79.7\%).
}
% Uncertainty analysis on MT-Bench and PandaLM test set. For single-answer grading, the scoring range is 0-9. The evaluation subject is Llama2-7B-Instruct, with average response confidence of 0.945 (MT-Bench) and 0.936 (PandaLM test set). For pairwise comparison, the evaluation subjects are Llama2-7B-Instruct and Llama2-13B-Instruct, with average response confidences of 0.945, 0.949 (MT-Bench), and 0.935, 0.936 (PandaLM test set), respectively. For each evaluation, we query the evaluator twice with the order swapped. ``Win / Lose / Tie'' represents the average number of times Llama-2-7b-chat's response is better than, worse than, or equal to Llama-2-13b-chat's response.
\label{tab:uncertainty-normal}
    \large
    \resizebox{1.0\textwidth}{!}{
    \begin{tabular}{@{}c|cccc|cccc@{}}
    \toprule
    \multirow{5}{*}{Evaluator} & \multicolumn{4}{c|}{Single-answer grading}                                                                                                                                               & \multicolumn{4}{c}{Pairwise comparison}                                                                                                                                                                            \\ \cmidrule(l){2-9} 
                               & \multicolumn{2}{c|}{MT-Bench}                                                                         & \multicolumn{2}{c|}{PandaLM Test set}                                            & \multicolumn{2}{c|}{MT-Bench}                                                                                      & \multicolumn{2}{c}{PandaLM Test set}                                                          \\ \cmidrule(l){2-9} 
                               & Rating         & \multicolumn{1}{c|}{\begin{tabular}[c]{@{}c@{}}Evaluation\\ Confidence\end{tabular}} & Rating         & \begin{tabular}[c]{@{}c@{}}Evaluation\\ Confidence\end{tabular} & Win / Lose / Tie            & \multicolumn{1}{c|}{\begin{tabular}[c]{@{}c@{}}Evaluation\\ Confidence\end{tabular}} & Win / Lose / Tie            & \begin{tabular}[c]{@{}c@{}}Evaluation\\ Confidence\end{tabular} \\ \midrule
    GPT-4o                     & 5.413          & \multicolumn{1}{c|}{0.417}                                                           & 6.541          & 0.473                                                           & 10.0 / 16.5 / 53.5          & \multicolumn{1}{c|}{0.699}                                                           & 30.0 / 38.0 / 102.0         & 0.809                                                           \\
    GPT-4o-mini                & 6.038          & \multicolumn{1}{c|}{0.605}                                                           & 6.641          & 0.645                                                           & 27.0 / 43.5 / 9.5           & \multicolumn{1}{c|}{0.776}                                                           & 53.0 / 61.0 / 56.0          & 0.820                                                           \\
    GPT-3.5-Turbo              & 6.288          & \multicolumn{1}{c|}{0.629}                                                           & 6.665          & 0.594                                                           & 38.5 / 35.0 / 6.5           & \multicolumn{1}{c|}{0.848}                                                           & 76.5 / 81.0 / 12.5          & 0.884                                                           \\
    Llama3-70B-Instruct        & 7.250          & \multicolumn{1}{c|}{0.644}                                                           & 7.424          & 0.548                                                           & 39.5 / 37.5 / 3.0           & \multicolumn{1}{c|}{0.791}                                                           & 78.0 / 86.5 / 5.5           & 0.849                                                           \\
    Llama2-70B-Instruct        & 7.875          & \multicolumn{1}{c|}{0.953}                                                           & 7.924          & 0.960                                                           & 33.0 / 34.0 / 13.0          & \multicolumn{1}{c|}{0.908}                                                           & 72.5 / 73.0 / 24.5          & 0.931                                                           \\
    Qwen2-72B-Instruct         & 5.875          & \multicolumn{1}{c|}{0.675}                                                           & 7.153          & 0.692                                                           & 22.0 / 29.0 / 29.0          & \multicolumn{1}{c|}{0.762}                                                           & 54.0 / 70.0 / 46.0          & 0.806                                                           \\ \midrule
    \textbf{Average}           & \textbf{6.456} & \multicolumn{1}{c|}{\textbf{0.654}}                                                  & \textbf{7.058} & \textbf{0.652}                                                  & \textbf{28.3 / 32.6 / 19.1} & \multicolumn{1}{c|}{\textbf{0.797}}                                                  & \textbf{60.7 / 68.2 / 41.1} & \textbf{0.850}                                                  \\ \bottomrule
    \end{tabular}
    }
    \vspace{-1.5em}
\end{table*}

\subsection{Results and Analysis}
\label{subsec:investigation-measurement}
We first conduct an extensive investigation of LLM evaluators with 2 different evaluation settings to gain a preliminary understanding of uncertainty in model-based LLM evaluation, showing partial results in Table~\ref{tab:uncertainty-normal} for a brief presentation and putting the full results in Appendix~\ref{appendix:full_result}. The following main observations can be drawn:

% \textbf{Key Observation.}
\textbf{LLM evaluators exhibit varying uncertainty based on model families and sizes.}
The evaluation uncertainty is more pronounced in the single-answer grading, where the average evaluation confidence is 65.4\%, compared to 79.7\% for pairwise comparison on MT-Bench. This lower confidence suggests that evaluators exhibit higher uncertainty when scoring individual models, 
which could stem from evaluators being uncertain about how to score a model’s response without the context of a comparison. In contrast, pairwise comparison benefits from direct comparison, leading to more decisive assessments.

\textbf{Evaluations within the same model family show significantly higher evaluation confidence.}
As shown in Table~\ref{tab:uncertainty-normal}, when Llama2-70B-Instruct is employed to evaluate Llama2-7B-Instruct, both the score (7.875 v.s. 6.456) and evaluation confidence (0.953 v.s. 0.654) are significantly higher than the averages for other evaluators. We speculate that this uncommon high confidence arises from the shared training corpus and similar linguistic patterns between the models, leading to a self-preference bias~\citep{koo2023benchmarking, zheng2023judging}, where the evaluating model is more familiar with the response style and content generated by a closely related model. This phenomenon highlights the potential threats for self-preference when evaluators from the same model family are used, which could lead to biased evaluations.

% Evaluation within the same model family 会展现出远超平均值的evaluation confidence。如表所示，当使用Llama2-70B-Instruct对Llama2-7B-Instruct的回答进行打分时，其给出的分数和evaluation confidence都大幅度超过了其余五个evaluator给出的均值。We speculate that 这种异常的高信心 是因为他们共享training corpus, 学习相同的语言模式，从而导致了self preference.

% \textbf{Improvement in general capabilities does not guarantee more stable evaluation capabilities.}
\textbf{Improved general performance does not guarantee more stable evaluation capabilities.}
For example, while GPT-4o demonstrates superior performance in general tasks (such as reasoning and math) compared to GPT-3.5-Turbo~\citep{chiang2024chatbot}, its evaluation confidence remains low. In the single-answer grading, GPT-4o has an evaluation confidence of only 0.417, which indicates that despite its enhanced abilities in general tasks, it struggles with stability in evaluating other models' responses. This suggests that there is no certain correlation between a model's competence in performing general tasks and its ability to reliably evaluate the responses of other models, which may be because LLMs are not heavily fine-tuned for the evaluation task~\citep{wangpandalm}.
% 在通用任务上能力的提升并没有带来更加稳定的评估能力。例如，根据Chatbot Arena提供的人类评价可知，从GPT-3.5-Turbo到GPT-4o，模型在通用任务（e.g.,Instruction Following and math）上的能力不断增强，但其不稳定性并没有得到改善，GPT-4o在Single-answer grading中的evaluation confidence仅有0.419.





% \subsection{What caused uncertainty?}
\subsection{The influences of data distribution}
\label{subsec:investigation-uncertainty-cause}

LLMs are typically trained using next token prediction, where the model generates the most likely next word based on the preceding context~\citep{zhao2023survey}. Different contexts can lead to multiple token choices, and the model makes predictions based on the training distribution, which inherently introduces uncertainty. As displayed in Table~\ref{tab:uncertainty-sensitivity}, we study the impact of data distribution on uncertainty in model-based LLM evaluation. The results demonstrate that evaluation confidence, as measured across both single-answer grading and pairwise comparison settings, exhibits sensitivity to changes in data distribution. When the evaluation scenario shifts from common, high-difficulty tasks (MT-Bench) to novel, unfamiliar tasks (PandaLM test set), the evaluation confidence fluctuates significantly (e.g., from 0.417 to 0.473 on GPT-4o). In contrast, the response confidence (Table~\ref{tab:uncertainty-sensitivity-pair}) remains more consistent, showing a much smaller variance (\textbf{0.014}) between the two datasets. This analysis highlights that in model-based LLM evaluation, evaluation uncertainty is more pronounced compared to response uncertainty, as evidenced by the lower confidence value and larger confidence differences when comparing performance across different datasets.

% specially trained LLM evaluators appear to be more sensitive to changes in data distribution. When the evaluation scenario shifts from manually constructed, high-difficulty tasks (MT-Bench) to machine-generated tasks (PandaLM test set), the scores of the Prometheus2-7b and Prometheus2-bgb-8x7b models fluctuate more significantly (from 4.725 to 6.101) compared to the general LLMs (from 6.456 to 7.058). We speculate that this fluctuation is attributed to the teacher forcing-based post-training~\citep{bengio2015scheduled, he2021exposure}, which, while enhancing LLMs’ evaluation capabilities, may also compromise their generalization performance.
% the sensitivity of model confidence to different data distributions.
% LLM一般通过next token prediction的方式进行训练。在这个过程中，模型基于上文生成下一个最有可能的词，因此天然地存在不确定性。
% As displayed in Table 4, we study the


% 在这一节讲evaluation uncertainty和response uncertainty的一些性质，比如对于数据分布的敏感程度
% The instructions and inputs in the input tuple are sampled from the Alpaca 52K dataset automatically
% construct a large-scale dataset of 52K instructions in a self-instruct manner
% MT-bench is also manually constructed to differentiate chatbots based on their core capabilities, such as reasoning and math. 难度更高，更贴近真实分布
% 只是简单的探索，leave the 深入的，对成因的研究for future work

\begin{table*}[tbp]
\setlength{\belowcaptionskip}{5pt}
    \vspace{-0.5em}
\centering
\captionsetup{skip=0pt}
\caption{Sensitivity of model confidence to different data distributions. $\triangle$: the absolute confidence difference between MT-Bench and PandaLM.}
\label{tab:uncertainty-sensitivity}
    \begin{minipage}[b]{0.55\linewidth}
        \subcaption{Evaluation confidence.}
        \label{tab:uncertainty-sensitivity-single}
        \huge
        \centering
        \renewcommand{\arraystretch}{1.05}
        \resizebox{1.0\textwidth}{!}{
        \begin{tabular}{@{}c|ccc|ccc@{}}
        \toprule[1pt]
        \multirow{4}{*}{Model} & \multicolumn{3}{c|}{Single-answer grading}                                          & \multicolumn{3}{c}{Pairwise comparison}                                                 \\ \cmidrule(l){2-7} 
                               & MT-Bench & \begin{tabular}[c]{@{}c@{}}PandaLM\\ Test set\end{tabular} & $\bm{\triangle}$  & MT-Bench & \begin{tabular}[c]{@{}c@{}}PandaLM\\ Test set\end{tabular} & $\bm{\triangle}$ \\ \midrule[1pt]
            GPT-4o                 & 0.417    & 0.473                                                     & 0.056 & 0.699    & 0.809                                                     & 0.110 \\
            GPT-4o-mini            & 0.605    & 0.645                                                     & 0.040 & 0.776    & 0.820                                                     & 0.044 \\
            GPT-3.5-Turbo          & 0.629    & 0.594                                                     & 0.035 & 0.848    & 0.884                                                     & 0.036 \\
            Llama-3-70B-Instruct   & 0.644    & 0.548                                                     & 0.096 & 0.791    & 0.849                                                     & 0.058 \\
            Llama-2-70B-Instruct   & 0.953    & 0.960                                                     & 0.007 & 0.908    & 0.931                                                     & 0.023 \\
            Qwen2-72B-Instruct     & 0.675    & 0.692                                                     & 0.017 & 0.762    & 0.806                                                     & 0.044 \\ \midrule[1pt]
            \textbf{Average}                & \textbf{0.654}    & \textbf{0.652}                                                     & \textbf{0.042} & \textbf{0.797}    & \textbf{0.850}                                                     & \textbf{0.053} \\ \bottomrule[1pt]
        \end{tabular}
        }
    \end{minipage}
    \hfill
    \begin{minipage}[b]{0.43\textwidth}
        \subcaption{Response confidence.}
        \label{tab:uncertainty-sensitivity-pair}
        \centering
        \scriptsize
        % \renewcommand{\arraystretch}{1.01}
        \resizebox{1.0\textwidth}{!}{
        \begin{tabular}{@{}c|cc|c}
        \toprule[0.5pt]
        Model                    & MT-Bench & \begin{tabular}[c]{@{}c@{}}PandaLM\\ Test set\end{tabular} & $\bm{\triangle}$     \\ \midrule[0.5pt]
            Llama2-7B-Instruct       & 0.944    & 0.936                                                     & 0.008 \\
            Llama2-13B-Instruct      & 0.948    & 0.940                                                     & 0.008 \\
            Gemma-1.1-7B-it          & 0.867    & 0.858                                                     & 0.009 \\
            Qwen2-7B-Instruct        & 0.856    & 0.836                                                     & 0.020 \\
            Internlm2.5-7B-chat      & 0.782    & 0.810                                                     & 0.028 \\
            Mistral-7B-Instruct-v0.3 & 0.855    & 0.843                                                     & 0.012 \\ \midrule
            \textbf{Average}                  & \textbf{0.875}    & \textbf{0.871}                                                     & \textbf{0.014} \\ \bottomrule[0.5pt]
        \end{tabular}
        }
    \end{minipage}
    \vspace{-0.5em}
\end{table*}

\subsection{Can we employ prompting strategies to mitigate uncertainty?}
\label{subsec:investigation-prompt}

% 为了探究特殊的输出格式是否能够减少LLM evaluators的evaluation uncertainty， we conduct LLM-based evaluation with special prompting strategies including:
% (1) Default prompt: We instruct the LLM to be  an impartial judge consider factors
% such as the helpfulness, relevance等等. The LLM is asked to output its rating / preference first, followed by a short explanation.

% (2) Chain-of-thoughts: Instead of generating judgments first, we instruct the LLM
% to first generate a concise reasoning, before generating its rating / preference for the responses.
% Considering that a better comprehension of a problem can lead to a more accurate understanding of one’s certainty, we adopt a reasoning-augmented prompting strategy. In this paper, we use zero-shot Chain-of-Thought, CoT (Kojima et al., 2022) for its proven efficacy in inducing reasoning processes and improving model accuracy across diverse datasets. 

% (3) Self-generated reference: We prompt the LLM evaluator to
% generate a short reference answer for the given instruction. The generated answer is then passed to the LLM evaluator as a reference when making the judgments.
% See Appendix \ref{appendix:prompts} for the relevant prompts. 实验结果如Figure~\ref{fig:uncertainty-special-prompt-single} and Figure~\ref{fig:uncertainty-special-prompt-pair}所示. 

\begin{figure*}[tbp]
    % \vspace{-1em}
    \centering
    \setlength{\abovecaptionskip}{5pt}
    \includegraphics[width=0.8\textwidth]{figs/special_prompt_single_v3.pdf}
    % 把所有confidence列出来的版本
    % \caption{Uncertainty analysis of single-answer grading under special prompting strategies on MT-Bench (first row) and PandaLM Test set (second row). The scoring range is 0-9. We evaluate Llama2-7B-Instruct with default prompt, Chain-of-Thoughts (CoT) and Self-Generated Reference (Reference). The average response confidences on MT-Bench and PandaLM Test set are 0.945 (default), 0.937 (CoT), 0.933 (Reference), and 0.936 (default), 0.927 (CoT), 0.930 (Reference), respectively.} % Chain-of-Thoughts (CoT) Self-Generated Reference  (Reference) Llama-2-7B-chat MT-Bench (first row) PandaLM Test set (second row) average response confidence for candidate model 是多少 
    % 色彩版本
    \caption{Uncertainty analysis of single-answer grading under special prompting strategies on MT-Bench (first row) and PandaLM Test set (second row).  
    % The scoring range is 0-9. 
    We evaluate Llama2-7B-Instruct with default prompt, chain-of-thoughts and self-generated reference strategies. 
    % The average response confidences on MT-Bench and PandaLM Test set are 0.938 and 0.931, respectively. 
    See Appendix ~\ref{appendix:full_result} for full results.} % Chain-of-Thoughts (CoT) Self-Generated Reference  (Reference) Llama-2-7B-chat MT-Bench (first row) PandaLM Test set (second row) average response confidence for candidate model 是多少 
    \label{fig:uncertainty-special-prompt-single}
    \vspace{-1.5em}
\end{figure*}

Prompting is the major approach to solving specialized tasks using LLMs. Prior studies demonstrate that special prompting strategies can enhance LLM's performance on downstream tasks by roleplaying~\citep{salewski2024context}, incorporating contextual information~\citep{pan2023context, yangsupervised} and standardizing output formats~\citep{wei2022chain}. To explore whether a well-designed prompt can reduce the evaluation uncertainty of LLM evaluators, we conduct experiments using several commonly used prompting strategies, including \textbf{Default}, \textbf{Chain-of-thoughts} and \textbf{Self-generated reference}. 
% See Appendix \ref{appendix:prompts} for the relevant prompts. 
%We refer the readers to Appendix \ref{appendix:prompts} for the relevant prompts. 
The experimental results are shown in Figures~\ref{fig:uncertainty-special-prompt-single} and~\ref{fig:uncertainty-special-prompt-pair}. Based on the data presented in Figures~\ref{fig:uncertainty-special-prompt-single} and~\ref{fig:uncertainty-special-prompt-pair}, we have the following observations: 

(1) Employing special prompting strategies can significantly enhance the evaluation confidence. 
% of LLM evaluators. 
From the ``Evaluation Confidence'' subgraphs, we observe that special prompting strategies consistently lead to higher evaluation confidence across different LLM evaluators. In all experiments utilizing the \textbf{CoT} strategy, evaluation confidence improved notably. We speculate that this improvement arises from the structured output formats. By explicitly guiding the LLM through step-by-step reasoning before making a judgment, it reduces ambiguity and uncertainty in the evaluation process. While the \textbf{Reference} strategy also yields positive results, its effectiveness is less consistent across evaluators, suggesting that the \textbf{CoT} strategy is more universally applicable and robust. 

(2) The \textbf{CoT} strategy seems to alleviate self-preference bias to some extent. For instance, as shown in Figure~\ref{fig:uncertainty-special-prompt-single}, when Llama2-70B-Instruct evaluates Llama2-7B-Instruct using the \textbf{CoT} strategy, the scores are generally lower compared to the \textbf{Default} strategy. This decrease indicates that the evaluator, when prompted to generate reasoning first, may become more objective and critical, reducing inherent bias towards the response style and content generated by a closely related model. 
% In contrast, the \textbf{Default} strategy may not prompt enough introspection or critical thinking, which could explain the inflated scores observed without chain-of-thought reasoning. 

(3) Using the \textbf{CoT} strategy can enhance the LLM evaluators' abilities to perform fine-grained assessments. As shown in Figure~\ref{fig:uncertainty-special-prompt-pair}, the tie rate decreases in all experiments based on the \textbf{CoT} strategy, indicating that the evaluator is able to perform fine-grained judgments with the generated rationale, allowing it to distinguish between high-quality responses in complex comparisons. In contrast, although the \textbf{Reference} strategy achieves similar effects with GPT-4o and GPT-4o-mini, its benefits are less consistent and not observed across other evaluators.

\begin{figure*}[tbp]
    \vspace{-0.5em}
    \centering
    \setlength{\abovecaptionskip}{5pt}
    \includegraphics[width=1.0\textwidth]{figs/special_prompt_pair_v3.pdf}
    % \caption{Specially designed prompt pairwise comparison} % Chain-of-Thoughts (CoT) Self-Generated Reference (Reference) swap Llama-2-7B-chat vs Llama-2-13B-chat Win Rate's meaning MT-Bench (first row) PandaLM Test set (second row) average response confidence for candidate model a和b是多少 
    % 色彩版本
    \caption{Uncertainty analysis of pairwise comparison under special prompting strategies on MT-Bench (first row) and PandaLM Test set (second row). 
    % For each evaluation, we query the evaluator twice with the order swapped. 
    % The evaluation subjects are Llama-2-7b-chat and Llama-2-13b-chat, with average response confidences of 
    % The average response confidences on MT-Bench and PandaLM Test set are 
    % 0.948 and 0.949 (MT-Bench), and 0.938 and 0.940 (PandaLM Test set), respectively. 
    ``Win Rate'' represents the proportion of non-tie cases where Llama2-7B-Instruct's response is better than Llama2-13B-Instruct's response. ``Tie Rate'' represents the proportion of tie cases.}
    \label{fig:uncertainty-special-prompt-pair}
    \vspace{-1.5em}
\end{figure*}

\vspace{-0.5em}
\subsection{Is a specially trained LLM a more stable evaluator?}
\label{subsec:investigation-trained}
% The result is shown in Table \ref{tab:uncertainty-special-trained-single} and~\ref{tab:uncertainty-special-trained-pair}.

As discussed in Section~\ref{subsec:investigation-measurement}, there is still 
a capability gap between an LLM's general performance and its evaluation ability. Improved general capabilities normally do not guarantee better evaluation capabilities. To address this issue, prior
work~\citep{kim2024prometheus, kim2024prometheus2, wangpandalm, vu2024foundational} focuses on developing powerful LLM evaluators trained on a large and diverse collection of high-quality human assessments. Are those specially trained LLMs more stable evaluators? We answer this question by experimenting with 3 open-source evaluators including Prometheus2-7b, Prometheus2-bgb-8x7b and PandaLM~\citep{kim2024prometheus2,kim2024biggen, wangpandalm}. The experimental results, as depicted in Table~\ref{tab:uncertainty-special-trained}, lead to the following conclusions:  
% Are those specially trained LLMs more stable evaluators? We answer this question by experimenting with 3 open-source evaluators including (1) \textbf{Prometheus2-7b} and \textbf{Prometheus2-bgb-8x7b} models~\citep{kim2024prometheus2,kim2024biggen}, both of which are trained to output in a Chain-of-Thoughts format, providing a concise rationale before indicating a preference between the two responses or providing its rating for single response; and (2) \textbf{PandaLM} model~\citep{wangpandalm}, which is trained to output in a normal format, offering a preference between the two responses, followed by a concise rationale.

\begin{table*}[tbp]
\setlength{\belowcaptionskip}{5pt}
    % \vspace{-1em}
\centering
\captionsetup{skip=0pt}
\caption{Uncertainty analysis with specially trained LLM evaluators on MT-Bench and PandaLM test set. ``General LLMs'' refers to the average performance of evaluators from Table~\ref{tab:uncertainty-normal}. ``Win / Lose / Tie'' represents the average number of times Llama2-7B-Instruct's response is better than, worse than, or equal to Llama2-13B-Instruct's response.}
\label{tab:uncertainty-special-trained}
    \begin{minipage}{0.465\linewidth}
        \subcaption{Single-answer grading.
        % The average response confidence of Llama2-7B-Instruct is 0.941.
        }
        \label{tab:uncertainty-special-trained-single}
        \centering
        \renewcommand{\arraystretch}{1.1}
        \resizebox{1.0\textwidth}{!}{
            \begin{tabular}{@{}c|cc|cc}
            \toprule[1pt]
        \multirow{3}{*}{Evaluator} & \multicolumn{2}{c|}{MT-Bench}  & \multicolumn{2}{c}{PandaLM Test set} \\ \cmidrule(l){2-5} 
                                   & Rating &  \begin{tabular}[c]{@{}c@{}}Evaluation\\ Confidence\end{tabular} & Rating    &  \begin{tabular}[c]{@{}c@{}}Evaluation\\ Confidence\end{tabular}   \\ \midrule
            Prometheus2-7B         & 5.963       & 0.993           & 7.187          & 0.991              \\
            Prometheus2-bgb-8x7B   & 4.725       & 0.870           & 6.101          & 0.887              \\ \midrule
            \textbf{General LLMs}                & \textbf{6.456}       & \textbf{0.654}           & \textbf{7.058}          & \textbf{0.652} \\ \bottomrule[1pt]
            \end{tabular}
        }
    \end{minipage}
    \hfill
    \begin{minipage}{0.525\linewidth}
        \subcaption{Pairwise comparison.
        % The average response confidence of Llama2-7B-Instruct and Llama2-13B-Instruct is 0.944 and 0.944.
        }
        \label{tab:uncertainty-special-trained-pair}
        \centering
        \renewcommand{\arraystretch}{1.1}
        \huge
        \resizebox{1.0\textwidth}{!}{
            \begin{tabular}{@{}c|cc|cc}
            \toprule[2pt]
        \multirow{3}{*}{Evaluator} & \multicolumn{2}{c|}{MT-Bench} & \multicolumn{2}{c}{PandaLM Test set} \\ \cmidrule(l){2-5} 
                                   & Win / Lose / Tie    & \begin{tabular}[c]{@{}c@{}}Evaluation\\ Confidence\end{tabular}  & Win / Lose / Tie        & \begin{tabular}[c]{@{}c@{}}Evaluation\\ Confidence\end{tabular}    \\ \midrule[1.5pt]
            
            PandaLM-7B              & 42.0 / 28.5 / 9.5  & 0.596      & 58.0 / 72.0 / 40.0   & 0.704        \\
            Prometheus2-7B             & 37.5 / 42.0 / 0.5  & 0.990      & 77.5 / 92.5 / 0.0    & 0.993        \\
            Prometheus2-bgb-8x7B       & 31.5 / 32.5 / 16.0 & 0.967      & 77.0 / 80.5 / 12.5   & 0.974        \\ \midrule[1.5pt]
            \textbf{General LLMs}                & \textbf{28.3} / \textbf{32.6} / \textbf{19.1} & \textbf{0.797}      & \textbf{60.7} / \textbf{68.2} / \textbf{41.1}   & \textbf{0.850}        \\ \bottomrule[2pt]
            \end{tabular}
        }
    \end{minipage}
    \vspace{-1.5em}
\end{table*}

(1) The Prometheus2-7b and Prometheus2-bgb-8x7b models, which are trained in a \textbf{CoT} format, consistently achieve higher evaluation confidence across all experiments compared to both the General LLMs and the PandaLM.  
% which output evaluations in a normal format. 
We attribute this phenomenon to the step-by-step rationale provided by the \textbf{CoT} strategy, which reduces ambiguity in the evaluation process. This phenomenon aligns with the findings from Section~\ref{subsec:investigation-prompt}, confirming that using \textbf{CoT} as an output format, whether during inferencing or post-training, can help alleviate evaluation uncertainty in LLM evaluators. 

(2) The fine-grained evaluation ability of specially trained LLM evaluators surpasses that of general LLMs, as evidenced by the reduced number of tie cases in pairwise comparison (Table~\ref{tab:uncertainty-special-trained-pair}). This improvement is likely due to the incorporation of human assessments as training data, which enhances the evaluators' analytical skills. Moreover, in the Prometheus2 models, this benefit is further amplified by the \textbf{CoT} format. 

(3) As shown in Table~\ref{tab:uncertainty-special-trained-single}, specially trained LLM evaluators appear to be more sensitive to changes in data distribution. When moving from MT-Bench to the PandaLM test set, the scores of the Prometheus2-7b and Prometheus2-bgb-8x7b models fluctuate more significantly (from 4.725 to 6.101) compared to the general LLMs (from 6.456 to 7.058). 
% These large fluctuations for the same model are unexpected as evaluation scores are constrained within a 0-9 scale. 
Given that Prometheus2-7b and Prometheus2-bgb-8x7b are fine-tuned on specialized data, we speculate that this fluctuation is attributed to the use of teacher forcing in the evaluator's post-training process~\citep{bengio2015scheduled, he2021exposure}, which, while enhancing LLMs’ evaluation capabilities, may also increase their sensitivity to changes in data distribution.
% (1) 基于CoT的Prometheus2-7b和Prometheus2-bgb-8x7b model 在所有实验中均取得了高于使用normal format输出评估结果的General LLM和PandaLM model的evaluation confidence。我们将这种现象归功于step-by-step rationale能够reduces ambiguity in the evaluation process。该现象与Section~\ref{subsec:investigation-prompt}的实验结论相吻合,证明了无论是在推理时还是训练时使用CoT作为输出格式，都能够缓解LLM evaluators的evaluation uncertainty。(2) specially trained LLM evaluators的细粒度判断能力强于general LLM，具体表现在pairwise comparison的tie cases数据变少了。这可能是使用human assessments作为训练数据带来的能力增益。而在Prometheus2-7b和Prometheus2-bgb-8x7b model上，有了CoT的加持，这种增益更加明显。(3) 由Table~\ref{tab:uncertainty-special-trained-single}可知，specially trained LLM evaluators 可能对数据分布的变化更加敏感。当评估场景从manually constructed的高难度问题(MT-Bench)转向机器生成的问题(PandaLM test set)时，相较于general LLM (6.456 to 7.058), Prometheus2-7b和Prometheus2-bgb-8x7b model的分数波动更明显，更大(4.725 to 6.101) . 我们推测原因是基于teacher forcing的post-training技术在增强LLM评估能力的同时，也损害了其泛化性能。
% The MT-Bench contains 80 manually constructed questions designed to differentiate chatbots based on their core capabilities, such as reasoning and math. The PandaLM test set contains 170 instructions sampled from the Alpaca 52K dataset~\citep{taori2023stanford}. The Alpaca 52K dataset is built on the self-instruct generation pipeline from \citet{wang2023self}.
% 该现象与基于xxx的实验结论相吻合

% \label{subsec:investigation-conclusion}
Based on the systematic empirical analyses mentioned above, we can conclude that 
the stability of LLM evaluators is a significant issue, with uncertainty permeating various aspects of model-based LLM evaluation (\S\ref{subsec:investigation-measurement}). 
% the stability of LLM evaluators poses a formidable challenge, revealing critical vulnerabilities in model-based assessment.
Compared to single-answer grading, pairwise comparison reduces the influence of subjective bias by directly comparing the relative merits of model outputs, thereby mitigating the uncertainty in evaluation to some extent. Furthermore, due to the auto-regressive nature of language models, employing special output formats (such as CoT) can effectively reduce evaluation uncertainty (\S\ref{subsec:investigation-prompt} and \S\ref{subsec:investigation-trained}). Our findings corroborate the conclusions of \citet{raina2024llm} from different perspectives, providing a nuanced analysis of the uncertainty issue.
% offering a more nuanced perspective on its implications.
% 基于以上一系列系统的实证实验和相应的分析，我们可以得出结论：LLM evaluator的稳定性是一个存在且显著的问题，不确定性贯穿于model-based LLM evaluation的各个环节。相较于single-answer grading，pairwise LLM evaluation由于直接比较模型输出的相对优劣，降低了主观偏见的影响，从而抑制了部分评估不确定性。此外，由于语言模型自回归建模的特性，使用特殊的输出格式（如CoT）能够有效减少evaluation uncertainty。我们的研究结果从不同角度验证了XXX等人的结论，并在不确定性问题上提供了更深入的分析。
% 基于以上a set of systematic empirical experiments和对应的分析，我们可以draw the conclusion that 不确定性贯穿model-based LLM evaluation。相较于 single-answer grading，pairwise LLM evaluation 因为xxx，从而抑制了部分的评估不确定性。由于语言模型的自回归建模的特性，使用特殊的输出格式（如CoT）能够减少evaluation uncertainty。our findings align with等人的研究结论，并在uncertainty上有更深入的分析。
% 
% To this end, we conduct a set of systematic empirical evaluations.
% our findings 进一步验证了等人的研究结论
% Pair 强于 single, 对于 pair -> 先推理更好


\vspace{-0.5em}
\section{Making use of uncertainty for better evaluation} % for fine-tune experiments
\label{sec:utilization}

As new and tailored tasks constantly emerge in real applications, they pose OOD challenges~\citep{yang2023glue, yangsupervised, liu2024good} to the capability and stability of LLM evaluators. 
% Inspired by previous work~\citep{yangsupervised} that incorporates the prediction confidence of specialized classifiers during the LLM’s inference stage to enhance reliability in OOD scenarios, 
We consider the problem of whether we can utilize the response confidence of candidate models to improve the evaluation capability of LLM evaluators for OOD data. To validate this hypothesis, we first collect ID instances from the Alpaca 52K dataset~\citep{taori2023stanford} as the fine-tuning set, based on which we fine-tune an uncertainty-aware LLM evaluator named ConfiLM, and assess its evaluation ability on a manually designed OOD test set.
% As new and tailored tasks constantly emerge in real applications, they can pose OOD challenges to the stability of LLM evaluator. Inspired by previous work that incorporates specialized classifier's prediction confidence during the LLM’s inference stage to bolster its reliability in OOD scenarios, 我们提出了一个猜想，即能否利用candidate model的response confidence增强LLM evaluator对于ood数据的评价能力。为验证该猜想，我们首先收集了ID数据作为Fine-tuning set，在此基础上微调了一个uncertainty-aware的LLM Evaluator named ConfiLM，并在手工设计的OOD test set上验证其评估能力。
% inspired by previous work that incorporate confidence as an additional information into xxx to boost yyy
% supercontext - Aligning with Human Judgement: The Role of Pairwise Preference in Large Language Model Evaluators: 把常用的直接scoring，改为 pair evaluation -> text rank，在两两成对的pair evaluation 转变为多个candidate text的rank的过程中，会涉及到两个有序列表的合并(有点像归并排序)，在这个时候它使用了uncertainty加速这种合并。PS. employ entropy as the uncertainty measurement
%在真实生产环境中，很多ood数据，according to table x, 在ood场景下，evaluator的Evaluation uncertainty和candidate的Response uncertainty都更加明显。如何利用uncertainty提升在ood场景中LLM的评估能力？
% 
\label{sec:Utilize}

\begin{figure*}[tbp]
\vspace{-0.5em}
    \centering
    \begin{minipage}[c]{0.50\textwidth}
        \centering
        \captionsetup{skip=5pt}
        \captionof{table}{Data Statistics. The fine-tuning set is sampled from the Alpaca 52K dataset~\citep{taori2023stanford}. Test set (Olympic 2024) is manually created based on data from \href{https://olympics.com/en/paris-2024/}{the Olympics site}. Each instance is annotated by three human evaluators.}
        \label{tab:data-statistics}
        
        % \renewcommand{\arraystretch}{1.0}
        \resizebox{1.0\textwidth}{!}{
            \begin{tabular}{@{\hspace{1mm}}ccc@{\hspace{1mm}}}
            % \begin{tabular}{@{}ccc@{}}

            \toprule
            Data            & \#Instances & Annotator Agreement \\ \midrule
            Fine-tuning set & 694         & 94.96\%             \\
            Test set        & 220         & 97.27\%             \\ \bottomrule
            \end{tabular}
        }
    \end{minipage}
    \hspace{0.01\textwidth} % 间距
    \begin{minipage}[c]{0.4\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figs/dataset_category.pdf}
        % \caption{Categories of instances in the Olympic 2024.}
        \caption{Categories of test instances.}
        \label{fig:test_set_examples}
    \end{minipage}
    \vspace{-1.5em}
\end{figure*}

\subsection{Dataset Construction}
\label{subsec:Utilize-datatset}

\textbf{Data collection.}  
Each instance of the fine-tuning set and OOD test set consists of an input tuple (user instruction $q$, response 1 $r_1$, response confidence of response 1 $u_1$, response 2 $r_2$, response confidence of response 2 $u_2$) and an output tuple (evaluation explanation $e$, evaluation result $p$). Following \citet{wangpandalm}, we sample 150 instructions from the Alpaca 52K dataset as the instruction source for the fine-tuning set. For the OOD test set, we manually craft 50 instructions based on data from \href{https://olympics.com/en/paris-2024/}{the Olympics site}. We identify 5 common categories of user questions to guide the construction, including writing, math, extraction, reasoning and roleplay. For each category, we then manually design 10 instructions. Each instruction is accompanied by an optional reference answer. We showcase several sample instances and instructions in Tables~\ref{tab:appendix-instance-example}, \ref{tab:appendix-instance-example-2}, \ref{tab:appendix-instruction-examples} and \ref{tab:appendix-instance-example-test}. 

The response pairs $r_1, r_2$ are produced by various instruction-tuned models including Gemma-1.1-7B-it~\citep{team2024gemma}, Internlm2.5-7B-chat~\citep{cai2024internlm2}, Qwen2-7B-Instruct~\citep{yang2024qwen2}, and Mistral-7B-Instruct-v0.3~\citep{jiang2023mistral}. For each source instruction, we pair the responses from two instruction-tuned models, resulting in a total of 900 unprocessed question-response pairs for the fine-tuning set and 300 for the test set. We then employ the calculation method introduced in \S~\ref{sec:task_definition} to quantify the response confidence $u_1, u_2$. Notably, to ensure the quality and diversity of the generated responses, we set the sampling temperature to 0.7 for all 4 instruction-tuned models. Experimental results (Figure~\ref{fig:appendix-confidence-temperature}) indicate that a sampling temperature of 0.7 achieves comparable response confidence to that of greedy sampling while maintaining generation diversity.

\textbf{Human annotations.}
The output tuple of each instance includes a brief explanation $e$ for the evaluation and an evaluation result $p$. The evaluation result would be either ‘1’ or ‘2’, indicating that response 1 or response 2 is better. To ensure the quality of human annotations, we involve three experts to concurrently annotate the same data point during the annotation process. These experts are hired by an annotation company, and all annotators receive redundant labor fees. To guarantee clarity and consistency, we provide comprehensive guidelines for every annotator, which emphasizes the need to consider the correctness, logical coherence, vividness and confidence of each response.



%  reference answer

% The corresponding output tuple includes an evaluation result, a brief explanation for the evaluation, and a reference response. The evaluation result would be either ‘1’ or ‘2’, indicating that response 1 or response 2 is better, and ‘Tie’ indicates that two responses are similar in quality. 

% each label independently provided by three different human evaluators. Samples with significant divergences are excluded to ensure the Inter Annotator Agreement (IAA) of each annotator remains larger than 0.85.

% % To maintain high-quality crowdsourcing work, we involve three experts to annotate the same data point concurrently during the annotation process. There is no prior relationship between the experts and the authors. The experts are hired from an annotation company. These experts receive specialized training that goes beyond evaluating response correctness, enabling them to emphasize other crucial aspects like relative conciseness, clarity, comprehensiveness, formality, and adherence to instructions. Furthermore, we guide these annotators in identifying and addressing issues such as logical fallacies, unnecessary repetitions, grammatical inaccuracies, and a lack of contextual relevance. All human ratings are collected consistently within the same session. To ensure clarity and consistency, we provide comprehensive instructions for every annotator. After the trial phase of data annotation, we eliminate some low-quality labeled data. The final IAA amongst the three annotators, as measured by Cohen’s Kappa (Cohen, 1960), yields average scores of 0.85, 0.86, and 0.88 respectively, indicating a relatively high level of reliability for our test dataset. To refine the model’s performance assess- ment compared to human evaluators, we can use the inter-annotator agreement (IAA) of 0.85 as a benchmark. If our model exceeds this, it indicates strong performance. However, setting a realistic target slightly above this human IAA, say around 0.90, offers a challenging yet achievable goal. The distribution of the test data comprises 105 instances of ties, 422 instances where Response 1 wins, and 472 instances where Response 2 takes the lead. Note that the human-generated dataset has no personally identifiable information or offensive content, and all annotators receive redundant labor fees.

\begin{table*}[tbp]
\setlength{\belowcaptionskip}{5pt}
    \vspace{-1.0em}
\centering
\captionsetup{skip=5pt}
\renewcommand{\arraystretch}{1.1}
\caption{Evaluation performance of 12 evaluators on Olympic 2024. The highest F1 and evaluation confidence of each group is marked by \textbf{bold}.}
% 最优 次优 pandalm是default 格式
\label{tab:ood-performance}
    \resizebox{1.0\textwidth}{!}{
    \begin{tabular}{@{}c|cccccc|c@{}}
    \toprule
    \multirow{2}{*}{Evaluator}  & \multicolumn{6}{c|}{F1}                                                            & \multirow{2}{*}{Evaluation Confidence} \\ \cmidrule(lr){2-7}
                                & \multicolumn{1}{c|}{Overall} & Writing & Roleplay & Math  & Reasoning & Extraction &                                        \\ \midrule
    GPT-4o                      & \multicolumn{1}{c|}{\textbf{0.678}}   & 0.391   & \textbf{0.761}    & 0.857 & 0.720     & \textbf{0.641}      & 0.968                                  \\
    GPT-4o-mini                 & \multicolumn{1}{c|}{0.677}   & 0.423   & 0.727    & 0.820 & \textbf{0.800}     & 0.627      & \textbf{0.986}                                  \\
    GPT-3.5-Turbo               & \multicolumn{1}{c|}{0.637}   & \textbf{0.505}   & 0.715    & 0.727 & 0.646     & 0.564      & 0.977                                  \\
    Llama3-70B-Instruct         & \multicolumn{1}{c|}{0.542}   & 0.316   & 0.627    & 0.647 & 0.684     & 0.377      & 0.981                                  \\
    Llama2-70B-Instruct         & \multicolumn{1}{c|}{0.534}   & 0.241   & 0.701    & 0.546 & 0.567     & 0.613      & 0.973                                  \\
    Qwen2-72B-Instruct          & \multicolumn{1}{c|}{0.631}   & 0.404   & 0.689    & \textbf{0.867} & 0.696     & 0.472      & 0.978                                  \\ \midrule
    Prometheus2-7B              & \multicolumn{1}{c|}{0.515}   & 0.280   & \textbf{0.703}    & 0.537 & 0.611     & 0.307      & \textbf{0.971}                                  \\
    Prometheus2-bgb-8x7B        & \multicolumn{1}{c|}{0.556}   & \textbf{0.394}   & 0.658    & \textbf{0.641} & \textbf{0.696}     & 0.267      & 0.965                                  \\
    PandaLM-7B                  & \multicolumn{1}{c|}{\textbf{0.560}}   & 0.388   & 0.677    & 0.585 & 0.608     & \textbf{0.455}      & 0.712                                  \\ \midrule
    Llama3-8B-Instruct          & \multicolumn{1}{c|}{0.536}   & 0.267   & \textbf{0.650}    & \textbf{0.693} & 0.635     & 0.388      & 0.973                                  \\
    Llama3-8B-Instruct-Finetune & \multicolumn{1}{c|}{0.582}   & 0.603   & 0.573    & 0.333 & 0.628     & 0.458      & 0.979                                  \\
    \rowcolor[HTML]{E1EAFF}
    ConfiLM                     & \multicolumn{1}{c|}{\textbf{0.621}}   & \textbf{0.723}   & 0.566    & 0.510 & \textbf{0.670}     & \textbf{0.594}      & \textbf{0.982}                                  \\ \bottomrule
    \end{tabular}
    }
    \vspace{-1.5em}
\end{table*}

\textbf{Data preprocessing.}
To ensure the quality of the instances and the consistency of human annotations, we implement several data cleaning measures, including (1) removing instances that are unanimously deemed low quality or difficult to evaluate by the annotators; (2) excluding special tokens in the responses (e.g., \texttt{<|im\_end|>}, \texttt{<eos>} ) that may introduce bias to the evaluators; (3) adjusting the ratio of label 1 to label 2 to prevent class imbalance. Additionally, given the ongoing concerns about LLMs' numerical understanding~\citep{liu2023goat}, we verbalize each instance's $u_1$ and $u_2$ into natural language statements to avoid introducing additional errors. The ablation result of verbalization is presented in Table~\ref{tab:appendix-Verbalized}. The mapping relationship between confidence values and declarative statements is displayed in Table~\ref{tab:appendix-confidence-mapping}. Ultimately, we obtain a fine-tuning set containing 694 high-quality instances and an OOD test set with 220 diverse instances. The annotator agreement rates (Table~\ref{tab:data-statistics}) are 94.96\% and 97.27\%, respectively. We report the distributions of response confidence $u_1$ and $u_2$ from the finetuning set in Figure~\ref{fig:appendix-distribution}.

% 为了保证instance的质量和人类标注的一致性，我们采取了一些数据清理措施，包括（1）去除了那些被Annotater一致认为质量低下或难以评估优劣的instance；（2）我们去掉了response中可能包含的特殊token(e.g., <|im_end|>, <eos>)，这些token可能会引发evaluator的bias；（3）我们调整了label 1和label 2 的比例，避免类别失衡。此外，由于LLM的数值理解能力一直受到诟病，我们verbalizing每一个instance包含的$u_1,u_2$into natural language statements，从而避免引入额外的误差. 最终，我们得到了一个包含694条高质量instance的fine-tuning set和一个包含220条instance的OOD test set。Annotator Agreement分别为94.96%和97.27%。
% expanded six times, resulting 900 unfiltered 300 unfiltered
% 我们去除了那些被Annotater一致认为质量低下或难以评估优劣的instance
%  去掉了tie情况,statistics in \ref{tab:data-statistics}
% Samples with significant divergences are excluded to ensure the Inter Annotator Agreement (IAA) of each annotator remains larger than 0.85.
%  数据清理的措施: 去掉special token, 调整label 1和label 2 的比例, verbalize
% \label{tab:appendix-confidence-mapping}
% statics
% annotation agreement
% we showcase a fine-tuning example for ConfiLM in Table~\ref{tab:appendix-instance-example}  We believe it can serve as a testbed for evaluating how instruction-based models handle diverse and unfa- miliar instructions.

\vspace{-0.5em}
\subsection{Training Details}
\label{subsec:Utilize-training}
Based on the collected fine-tuning set, we fine-tune the Llama3-8B-Instruct by incorporating response confidence as additional information in the prompt (Figure~\ref{fig:appendix-prompt-confilm}), obtaining an uncertainty-aware LLM evaluator named ConfiLM. During the fine-tuning phase of ConfiLM, we use the AdamW ~\citep{loshchilov2017decoupled} optimizer with a learning rate of 5e-5 and a cosine learning rate scheduler. The model is fine-tuned for 6 epochs on 2 NVIDIA A100-SXM4-80GB GPUs. 
Notably, to differentiate the effects of fine-tuning and the incorporation of response confidence on the model's evaluation performance in the OOD test set, we remove the response confidence $u_1$ and $u_2$ from all fine-tuning instances and fine-tune the Llama3-8B-Instruct again using the same configuration, with the learning rate set to 3e-5. We refer to this variant model as Llama-3-8B-Instruct-Finetune. The performance comparison results between different fine-tuning hyperparameters are presented in Figure~\ref{fig:appendix-hyperparameters}.
% 基座模型：llama-3-8B-Instruct}
% 在微调数据集上1）用输入：[Instruction][Response1][Response2]输出：[理由][判断]的格式微调得到的evaluator A。2）用输入：[Instruction][Response1][Confidence1][Response2][Confidence2]输出：[理由][判断]的格式微调得到evaluator B
% 以5e-5学习率在694个instance上训练了5个epoch, 2A100
%  The training prompt of PandaLM is shown at Appendix A.

\vspace{-1em}
\subsection{Experimental Settings}
To enhance reproducibility, we set the temperature to 0 for proprietary models and utilize greedy decoding for open-source models. For each evaluation, we query the evaluator twice with the order swapped. 
All general LLM-based evaluators (e.g., GPT-4o) are required to output in a CoT format. To obtain the best evaluation results, specially trained or fine-tuned evaluators (e.g., PandaLM-7B) are assessed using their original prompt and output format.
% To enhance reproducibility, we set the temperature to 0 for proprietary models, and utilize greedy decoding for open-source models. For each evaluation, we query the evaluator twice with the order swapped. 为了验证response confidence对评估能力的影响，we conduct experiments under two distinct settings, including 1) default:给evaluator提供完整的instance ($q$, $r_1$, $u_1$, $r_2$, $u_2$) and w/o confidence: remove the response confidence $u_1$ and $u_2$ from all test instances. All general LLM-based evaluators are required to output in a CoT format. 为了获得最好的评测效果，sepecially trained/fine-tuned evaluators (e.g., PandaLM-7B) are assessed by their original prompt and output format.

% except for PandaLM-7B, as it is trained to output in a default format. 
% temperature 0 prompt (confidence and w/o confidence)
% For each evaluation, we query the evaluator twice with the order swapped. 
% All evaluators are required to output in a CoT format, except for PandaLM-7B, as it is trained to output in a default format. we two distinct settings
\subsection{Evaluation performance on Out-of-distribution data}
\label{subsec:Utilize-performance}


% We show partial results in Table~\ref{tab:ood-performance} for a brief presentation and put the full results in Appendix~\ref{appendix:full_result}. 
Table~\ref{tab:ood-performance} presents the evaluation performance of 12 evaluators on Olympic 2024. 
Our observations include: (1) all LLM evaluators struggle with the Olympic 2024 (with the best F1 score only reaching 0.678), demonstrating that OOD data poses significant challenges to LLM evaluators' capabilities. 
(2) ConfiLM outperforms Llama3-8B-Instruct-Finetune and Llama3-8B-Instruct on F1 by 3.9\% and 8.5\%, respectively. This improvement demonstrates that fine-tuning high-quality human assessments enhances LLMs' evaluation capabilities, and incorporating uncertainty as auxiliary information significantly boosts evaluator performance in OOD scenarios.
% Fine-tuning high-quality human assessment data significantly enhances the evaluators' assessment capabilities, even in OOD scenarios.
% 该增长一方面证明了针对性微调能够提升LLM的评估能力，另一方面也说明了incorporating uncertainty as auxiliary information can largely improve the evaluators' performance in OOD scenarios. 
% We attribute this improvement to the incorporation of uncertainty as auxiliary information during the fine-tuning phase. 
(3) Compared to reasoning and math tasks, most evaluators show weaker performance on writing tasks. We speculate that this unusual trend arises because LLMs can evaluate response from reasoning tasks based on in-distribution knowledge, but fail to make judgement in creative tasks like writing. 

% \textbf{Case study.} 
% \begin{table*}[h]
% \setlength{\abovecaptionskip}{5pt}
% \caption{examples.}
% \label{tab:appendix-instance-example}
% \definecolor{redE}{HTML}{C1272D}
% \centering
%     % \vspace{5pt}
%         \resizebox{0.6\textwidth}{!}{
%         \begin{tabularx}{0.6\textwidth}{@{\hskip 2pt}X@{\hskip 2pt}}
%         \toprule
% Generate a news report: Australia defeated Ireland 40:7 in the Women's Rugby Sevens Quarterfinal on 29/07/2024, securing a Semifinal spot.  \\ \midrule
% Response 1: Women's Rugby Sevens: Australia Cruises Past Ireland in ...  (Confidence: 0.865)                                                                                                                             \\
% Response 2: ... match between Australia and Ireland took place on 29th July 2024 ... (Confidence: 0.715)                                                                                                             \\ \midrule
% GPT-4o: Response 2 offers a engaging report.                                                                                                                                        \\
% Llama3-8B-Instruct-Finetune: Response 2 is detailed.                                                                                                                                             \\
% ConfiLM: Response 2 contains incorrect dates.  \\ \bottomrule
%         \end{tabularx}
        
%         }
            
%     % \vspace{5pt}
% \end{table*}


\newlength{\oldintextsep}
\setlength{\oldintextsep}{\intextsep}
\setlength{\intextsep}{0pt}
\begin{wraptable}{r}{0.53\linewidth}
    \setlength{\columnseprule}{10pt}
    \centering
    \caption{Hallucination case. Full version in Table~\ref{tab:appendix-case-examples}. } 
    \label{tab:case-examples}
    \setlength{\belowcaptionskip}{-10pt} % 设置标题下方的间距
    % \vspace{5pt}
    \small
        \resizebox{0.53\textwidth}{!}{
        \begin{tabularx}{0.53\textwidth}{@{\hskip 2pt}X@{\hskip 2pt}}
        \toprule
        Generate a news report: Australia defeated Ireland 40:7 in the Women's Rugby Sevens Quarterfinal on 29/07/2024, securing a Semifinal spot.  \\ \midrule
        \textbf{Response 1:} Women's Rugby Sevens: Australia Cruises Past Ireland in ...  (Confidence: 0.865)                                                                                                                             \\
        \textbf{Response 2:} ... match between Australia and Ireland took place on \textcolor{red}{30th July 2024} ... (Confidence: 0.715)                                                                                                             \\ \midrule
        \textbf{GPT-4o:} Response 2 offers an engaging report.                                                                                                                                        \\
        \textbf{Llama3-8B-Instruct-Finetune:} Response 2 is detailed.                                                                                                                                             \\
        \textbf{ConfiLM:} Response 2 contains incorrect dates.  \\ \bottomrule
                \end{tabularx}
        }
            
    
\end{wraptable}
\textbf{Case study.}
% 由于长文本包含难以察觉的幻觉，且其评估标准较主观，general LLM-based evaluators (GPT-4o) show relatively weak performance than ConfiLM on writing tasks. we showcase an test sample that 解释了response confidence在幻觉检测上的作用，作为uncertainty-aware的evaluator，较低的response confidence会降低ConfiLM认为response的可靠程度，从而做出正确的判断。
Due to the presence of subtle hallucinations in long texts and the inherently subjective nature of their evaluation, general LLM-based evaluators (such as GPT-4) tend to underperform in writing tasks. We present a test sample (Table~\ref{tab:case-examples}) that illustrates the role of response confidence in detecting hallucinations of model response~\citep{farquhar2024detecting, varshney2023stitch}. As an uncertainty-aware evaluator, ConfiLM reduces the reliability of a response when it detects low confidence, leading to more accurate judgments.

% \vspace{-0.5em}
%evaluation process中的作用。我们简化了细节仅保留核心部分，完整版在Table 7中。response confidence使得ConfiLM能够
% 由于长文本包含模型难以察觉的幻觉

% \setlength{\intextsep}{\oldintextsep}

% Using the \textbf{CoT} strategy can enhance the LLM evaluators' abilities to perform fine-grained assessments. As shown in Figure~\ref{fig:uncertainty-special-prompt-pair}, the tie rate decreases in all experiments based on the \textbf{CoT} strategy, indicating that the evaluator is able to perform fine-grained judgments with the generated rationale, allowing it to distinguish between high-quality responses in complex comparisons. In contrast, although the \textbf{Reference} strategy achieves similar effects with GPT-4o and GPT-4o-mini, its benefits are less consistent and not observed across other evaluators.
% Additionally, compared to reasoning and math tasks, all non-fine-tuned evaluators show weaker performance on writing tasks. We speculate that this unusual trend arises because LLMs can partially understand reasoning tasks (like math) in OOD scenarios based on in-distribution knowledge, but fail to make analogies in creative tasks like writing. (2) Fine-tuning high-quality human assessment data significantly enhances the evaluators' assessment capabilities, even in OOD scenarios. (3) ConfiLM outperforms Llama3-8B-Instruct-Finetune and Llama3-8B-Instruct on F1 by 3.9\% and 8.5\%, respectively. We attribute this improvement to the incorporation of uncertainty as auxiliary information during the fine-tuning phase.

%(1) All LLM evaluators struggle on the Olympic 2024, 这证明OOD 数据pose 很大 challenges to the capability of LLM evaluators. 此外，相较于reasoning, math等tasks，所有未经过微调的evaluators都在writing task上表现出较弱的评估能力。We speculate that this uncommon trend 是因为LLM能够基于分布内知识部分理解OOD场景中的推理任务（如math task），但在创作型task，即writing task上无法做到类比。（2）通过在fine-tuning set上进行微调，Llama3-8B-Instruct-Finetune和ConfiLM都有了大幅度的评估能力增长。（3） ConfiLM surpasses Llama3-8B-Instruct-Finetune and Llama3-8B-Instruct on F1 by 3.9/% and 8.5%. We attribute this improvement to the incorporation uncertainty as auxiliary information during the fine-tuning phase
%  incorporating uncertainty as additional information during the
% fine-tuning phase can largely improve the model’s evaluation performance in OOD
% scenarios. 
%  incorporating uncertainty as auxiliary information during the fine-tuning process
% can largely improve the evaluator’s evaluation performance in OOD scenarios.
% We attribute this phenomenon to the
% pose OOD challenges to the capability and
\section{Discussion} % 可以放这也可以放appendix，正文没有要求必须有Discussion
%Limitations.Societal impacts.Future directions.
LLM-based evaluation requires a comprehensive consideration of prompt optimization~\citep{zhou2023survival, zhou2024fairer}, bias calibration~\citep{zhoubatch}, and uncertainty mitigation strategies.
% The stability of LLM-based evaluation is a topic that merits in-depth research. 
The performance of LLMs as evaluation tools is influenced by various factors, such as the diversity of training data~\citep{shidetecting}, inherent model biases~\citep{zheng2023judging}, and the complexity of the tasks. These uncertainties can cause fluctuations in the consistency of evaluation results. Improving the stability of LLM evaluators can 
% significantly enhance the reliability of automated evaluation processes, 
% reduce human intervention and subjective biases, and 
decrease the randomness that may arise during the evaluation process, thus providing more accurate and reproducible results~\citep{chiang2023can}.

While our work provides extensive analysis on the stability of LLM evaluators, there are other critical aspects of evaluation uncertainty that warrant attention. For example, the relationship between evaluation uncertainty and evaluation bias, as well as the uncertainty in the evaluation of multimodal large language models~\citep{Li_2024_CVPR}. Our work only focuses on single-round evaluations. For evaluations conducted on multi-turn benchmarks (i.e., MT-Bench), we use the first-round question as input. It would be interesting to investigate how the uncertainty of LLM evaluators affects judgments on multi-round conversations. Additionally, this research does not cover language models that do not provide token probabilities (e.g., Claude~\citep{anthropic2024claude}). Exploring how to conduct uncertainty analysis for LLM evaluators based on these proprietary models is a valuable topic. 
% \textcolor{blue}{We also note that LLM-based evaluation requires a comprehensive consideration of prompt optimization~\citep{zhou2023survival, zhou2024fairer}, calibration~\citep{liu2024calibrating, zhoubatch}, and uncertainty mitigation strategies. We leave the exploration in those aspects to future work.}
It is also important to note that commonly used LLM evaluators require strong calibration to ensure that their output probabilities accurately reflect the precision of their assessments~\citep{chen2023close}. We provide an analysis of the relation between evaluation confidence and accuracy in Appendix ~\ref{appendix-sub:relation} and leave further exploration in those aspects to future work.

% Model-based LLM evaluation 的稳定性是一个值得深入研究的问题。当前，LLM 作为评估工具的表现往往受到多种因素的影响，如模型训练数据的多样性、模型的偏见、以及评估任务的复杂性等。这些不确定性导致其评估结果的稳定性和一致性可能出现波动。提升 LLM 作为 evaluators 的稳定性，能够显著提高自动化评估流程的可靠性，减少人为干预和主观偏见；同时，它还能降低模型在评估过程中可能产生的随机性，从而提供更为精准、可重复的结果。尽管我们的工作提供了对于LLM evaluators稳定性的大量分析，there are other important aspects of evaluation uncertainty that we should care about, for example, evaluation uncertainty 和 evaluation bias之间的关联，多模态大语言模型评估的不确定性等等。 Our work only focuses on single-round evaluations. For evaluations conducted on multi-turn benchmarks, 我们用第一轮问题作为输入。 It would be interesting to explore LLM evaluators的不确定性是如何影响对 multi-round conversations的judgements. 除此之外，本文的研究并未涉及到那些不提供token probabilities的语言模型 (e.g., Claude~\citep{anthropic2024claude}),如何对基于这些proprietary models的llm evaluators进行不确定性分析也是一个值得研究的问题。 We leave these explorations to future work.


\section{Conclusion}
In this paper, we empirically investigated the existence, mitigation and utilization of uncertainty in model-based LLM evaluation. Extensive empirical analyses demonstrate that uncertainty is prevalent across various LLMs and can be alleviated with special prompting strategies such as chain-of-thought and self-generated reference. 
Experimental results on an OOD test set with 220 diverse instances show that incorporating uncertainty as auxiliary information during the fine-tuning process can largely improve the LLM evaluators' evaluation performance. We hope the empirical analyses in this work and the proposed uncertainty-aware LLM evaluator can inspire future research on the stability of model-based LLM evaluation.

\section*{Acknowledgment}
We would like to thank the anonymous reviewers for their insightful comments and suggestions to help improve the paper. This publication has been supported by the National Natural Science Foundation of China (NSFC) Key Project under Grant Number 62336006.

\section*{Reproducibility Statement}
To ensure the reproducibility of our results, we have made detailed efforts throughout the paper. All experimental settings, including model configurations, prompting strategies, and benchmarks, are described in Section \S\ref{subsec:settings}. Additionally, we provide comprehensive information about the dataset construction and training details of ConfiLM in Section \S\ref{sec:utilization}. Our code, data, and other resources necessary to replicate are released at: \url{https://github.com/hasakiXie123/LLM-Evaluator-Uncertainty}.
% Inspired by prior work~\citep{yangsupervised} that incorporates the prediction confidence of specialized classifiers during the LLM’s inference stage to enhance reliability in OOD scenarios, we fine-tune an uncertainty-aware LLM evaluator named ConfiLM using instruction instances collected from the Alpaca 52K dataset~\citep{taori2023stanford}. To validate ConfiLM's evaluation capability in OOD scenarios (\S\ref{sec:utilization}), we manually craft a test dataset called Olympic 2024 based on data from \href{https://olympics.com/en/paris-2024/}{the Olympics site}. Olympic 2024 contains 220 high-quality instances, each labeled independently by three different human evaluators. Samples unanimously deemed low quality or difficult to evaluate by the annotators are removed, resulting in an annotator agreement rate of 97.27\%. Experimental results demonstrate that incorporating uncertainty as auxiliary information during the fine-tuning process can largely improve the evaluator's evaluation performance in OOD scenarios.

% 总结段：
% To the best of our knowledge, we are the first empirical study to investigate uncertainty as a fundamental aspect for LLM-as-Judge. We conduct a comprehensive uncertainty analysis, propose a high-quality OOD test set, and offer an uncertainty-aware LLM evaluator named ConfiLM. Our empirical findings reveal the impact of uncertainty on LLM-as-Judge, especially in eliminating and utilizing evaluation uncertainty, shedding light on future research in model-based LLM evaluation.

% To our knowledge, 我们是第一个把uncertainty作为辅助信息引入LLM-as-Judge的实验探究型工作。We conduct comprehensive uncertainty analysis, propose a 高质量pairwise ood数据集并训练了一个uncertainty-aware的LLM Evaluator named ConfiLM。Our findings reveal 不确定性对LLM-as-a-Judge的影响, shedding light on future research of model-based LLM evaluation.

\bibliography{iclr2025_conference}
\bibliographystyle{iclr2025_conference}

\appendix

\section{Prompts Demonstration}
\label{appendix:prompts}
All the relevant prompts used in this study are provided in Figures~\ref{fig:appendix-prompt-single}, \ref{fig:appendix-prompt-pair}, \ref{fig:appendix-prompt-trained} and \ref{fig:appendix-prompt-confilm}. Prompts for PandaLM and Prometheus2 model are obtained from their GitHub repository \footnote{GitHub repository for PandaLM: \url{https://github.com/WeOpenML/PandaLM}; GitHub repository for Prometheus2 model: \url{https://github.com/prometheus-eval}.}.

\setlength{\intextsep}{\oldintextsep}

\section{Analysis}
\label{appendix:analysis}

\subsection{Different ways of measuring uncertainty}
\label{appendix-sub:uncertainty}
In this paper, we used token probabilities to represent the LLM's internal confidence, a method inspired by previous works \citep{varshney2023stitch, zhou2023navigating, guptalanguage, kumar2024confidence}. To investigate whether different definitions of uncertainty impact the empirical findings, we conducted additional experiments under a pairwise comparison setting on the MT-Bench dataset. These experiments involved two commonly used confidence quantification methods: (1) Verbalization-based confidence, where we prompted LLMs to directly output calibrated confidence scores along with their responses~\citep{linteaching, yona2024can}; (2) Consistency-based confidence, which involved generating 5 / 10 / 20 responses to the same question and measuring their consistency as a proxy for confidence~\citep{tian2023just, xiongcan}. For these experiments, we set the sampling temperature to 0.7.

In the experiments, the evaluation subjects were Llama2-7B-Instruct and Llama2-13B-Instruct. The confidence quantification results are presented in Table~\ref{tab:appendix-ways-of-quantification}. Based on the analysis of these results, we observed that the evaluation confidence obtained using different confidence quantification methods follows the same patterns. This further supports the conclusions drawn in Section \S\ref{sec:Investigation}: (1) LLM evaluators exhibit varying levels of uncertainty; (2) Evaluations within the same model family demonstrate higher evaluation confidence.


\subsection{The relation between evaluation confidence and accuracy}
\label{appendix-sub:relation}
To investigate the relation between evaluation confidence and accuracy, we analyzed the average accuracy of judgments made by six LLM-based evaluators on Olympic 2024 across different confidence intervals. The experimental results, as presented in Table~\ref{tab:appendix-relation}, reveal a positive correlation between evaluation confidence and accuracy. Specifically, when evaluation confidence is low, the accuracy of judgments across evaluators is generally lower across evaluators. As evaluation confidence increases, judgment accuracy improves steadily, reaching peak performance in high-confidence intervals (e.g., [0.8, 1.0)). This indicates that models are more reliable in performing evaluation tasks when evaluating with higher confidence.

\subsection{The in-domain evaluation performance of ConfiLM}
\label{appendix-sub:in-domain}
In Section \S \ref{sec:utilization}, we fine-tuned an uncertainty-aware LLM evaluator named ConfiLM, which leverages the response confidence of candidate models to enhance ConfiLM's evaluation capability for OOD data. To investigate the evaluation performance of ConfiLM on in-domain (ID) data, we re-split its fine-tuning dataset, selecting 94 human-annotated instances as an in-domain test set, named Alpaca-94. Based on the remaining 600 fine-tuning instances, we re-trained the models using the same experimental setup as in Section \S \ref{subsec:Utilize-training}, obtaining ConfiLM-600 and Llama-3-8B-Instruct-Finetune-600 models. Their evaluation performance on Alpaca-94 (ID data) and Olympic 2024 (OOD data) is reported in Table~\ref{tab:appendix-alpaca-94}. Experimental results from Table~\ref{tab:appendix-alpaca-94} demonstrate that incorporating uncertainty as auxiliary information significantly enhances the performance of LLM evaluators in OOD scenarios. While ConfiLM-600's advantage is reduced in ID scenarios, it still achieves evaluation performance comparable to Llama-3-8B-instruct-finetune-600.

\section{Dataset Construction}
\label{appendix:dataset}
Each instance of the fine-tuning set and OOD test set consists of an input tuple (user instruction $q$, response 1 $r_1$, response confidence of response 1 $u_1$, response 2 $r_2$, response confidence of response 2 $u_2$) and an output tuple (evaluation explanation $e$, evaluation result $p$). The human-annotated evaluation result would be either ‘1’ or ‘2’, indicating that response 1 or response 2 is better. To ensure the quality and consistency of the human annotations, we first selected 100 samples from the dataset for preliminary annotation by two of the authors. This process facilitated the development of a well-defined annotation guideline. Then, we hired three PhD-level human annotators from an annotation company to annotate all samples (both the fine-tuning set and the test set) in two rounds: (1) In the first round, two annotators were asked to label each sample based on the established annotation guidelines; (2) In the second round, a third annotator reviewed samples where disagreements arose and provided an extra label. The final label for each sample is determined through majority voting. During the annotation process, samples unanimously deemed low quality or difficult to evaluate by the annotators were excluded. 

Given the ongoing concerns about LLMs' numerical understanding~\citep{liu2023goat}, we verbalized each instance's $u_1$ and $u_2$ into natural language statements to avoid introducing additional errors. The mapping relationship between confidence values and declarative statements is displayed in Table~\ref{tab:appendix-confidence-mapping}. Ultimately, we obtained a fine-tuning set containing 694 high-quality instances and an OOD test set with 220 diverse instances. The annotator agreement rates are 94.96\% and 97.27\%, respectively. We showcase several sample instances and instructions in Tables~\ref{tab:appendix-instance-example}, \ref{tab:appendix-instance-example-2}, \ref{tab:appendix-instruction-examples} and \ref{tab:appendix-instance-example-test}.

\section{Full Experimental Results}
\label{appendix:full_result}
The full results of experiments introduced in Sections \ref{sec:Investigation} and \ref{sec:utilization} are displayed in Tables~\ref{tab:appendix-full-single}, \ref{tab:appendix-full-pair-mtbench} and \ref{tab:appendix-full-pair-pandalmdata}. Additionally, to investigate the impact of response confidence on LLM evaluators' evaluation capabilities, we further conducted experiments under two distinct settings: (1) \textbf{default}: providing the evaluator with the complete instance ($q$, $r_1$, $u_1$, $r_2$, $u_2$) and (2) \textbf{without confidence}: removing the response confidence $u_1$ and $u_2$ from all test instances. All general LLM-based evaluators (e.g., GPT-4o) were required to output in a CoT format. To obtain the best evaluation results, specially trained or fine-tuned evaluators (e.g., PandaLM-7B) were assessed using their original prompt and output format. Table~\ref{tab:appendix-ood-performance} presents the evaluation performance of 12 evaluators on Olympic 2024. 

Based on the analysis of these results, we found that ConfiLM outperforms Llama3-8B-Instruct-Finetune and Llama3-8B-Instruct on F1 by 3.9\% and 8.5\%, respectively. We attributed this improvement to the incorporation of uncertainty as auxiliary information during the fine-tuning phase. Furthermore, adding uncertainty to the prompts also brings certain performance improvements to general LLM-based evaluators (e.g., 0.690 v.s. 0.641 on GPT-4o-Extraction), but these gains are unstable due to the LLMs' analytical capabilities. 

% \subsection{The Fine-tuning set}

% \subsection{The Olympic 2024 Dataset}
% test set数据的种类，介绍

\begin{figure*}[h]
    % \vspace{-1em}
    \centering
    % \setlength{\abovecaptionskip}{0.5em}
    \includegraphics[width=0.87\textwidth]{figs/single_prompt_v1.pdf}
    \caption{Prompts for single-answer grading. The output format is highlighted in red.}
    \label{fig:appendix-prompt-single}
    % \vspace{-1.5em}
\end{figure*}

\begin{figure*}[h]
    % \vspace{-1em}
    \centering
    % \setlength{\abovecaptionskip}{0.5em}
    \includegraphics[width=0.725\textwidth]{figs/pair_prompt_v1.pdf}
    \caption{Prompts for pairwise comparison. The output format is highlighted in red.}
    \label{fig:appendix-prompt-pair}
    % \vspace{-1.5em}
\end{figure*}

\begin{figure*}[h]
    % \vspace{-1em}
    \centering
    % \setlength{\abovecaptionskip}{0.5em}
    \includegraphics[width=0.755\textwidth]{figs/special_trained_prompt_v3.pdf}
    \caption{Prompts for PandaLM~\citep{wangpandalm} and Prometheus2 model~\citep{kim2024prometheus2}. The output format is highlighted in red.}
    \label{fig:appendix-prompt-trained}
    % \vspace{-1.5em}
\end{figure*}

%\footnotetext{GitHub repository for PandaLM: \url{https://github.com/WeOpenML/PandaLM}}
%\footnotetext{GitHub repository for Prometheus2 model: \url{https://github.com/prometheus-eval}}

\begin{figure*}[h]
    % \vspace{-1em}
    \centering
    % \setlength{\abovecaptionskip}{0.5em}
    \includegraphics[width=0.85\textwidth]{figs/finetuning_promptv1.pdf}
    \caption{Prompts for fine-tuning ConfiLM.}
    \label{fig:appendix-prompt-confilm}
    % \vspace{-1.5em}
\end{figure*}

\clearpage

\begin{table*}[h]
\setlength{\belowcaptionskip}{5pt}
    % \vspace{-1em}
\centering
\captionsetup{skip=10pt}
\renewcommand{\arraystretch}{1.2}
\caption{The mapping between confidence values and declarative statements.}
\label{tab:appendix-confidence-mapping}
    
    \begin{tabular}{@{\hspace{5mm}}c@{\hspace{5mm}}c@{\hspace{5mm}}}
    \toprule
    Confidence Value & Declarative Statement  \\ \midrule
    {[}0, 0.1)       & Complete doubt         \\
    {[}0.1, 0.2)     & Highly uncertain       \\
    {[}0.2, 0.3)     & Clearly doubtful       \\
    {[}0.3, 0.4)     & Significantly doubtful \\
    {[}0.4, 0.5)     & Slightly doubtful      \\
    {[}0.5, 0.6)     & Neutral                \\
    {[}0.6, 0.7)     & Slightly confident     \\
    {[}0.7, 0.8)     & Clearly confident      \\
    {[}0.8, 0.9)     & Highly confident       \\
    {[}0.9, 1.0{]}   & Absolute confidence    \\ \bottomrule
    \end{tabular}
\end{table*}

\begin{figure*}[h]
    % \vspace{-1em}
    \centering
    % \setlength{\abovecaptionskip}{0.5em}
    \includegraphics[width=0.75\textwidth]{figs/distribution.pdf}
    \caption{The distribution of response confidence from the fine-tuning set for ConfiLM. The interval [0.0, 0.1) denotes the response confidence is greater than or equal to 0.0 but less than 0.1. }
    \label{fig:appendix-distribution}
    % \vspace{-1.5em}
\end{figure*}

\begin{figure*}[h]
    % \vspace{-1em}
    \centering
    % \setlength{\abovecaptionskip}{0.5em}
    \includegraphics[width=0.75\textwidth]{figs/hyperparameters.pdf}
    \caption{The evaluation performance of ConfiLM and Llama3-8B-Instruct-Finetune under different combinations of learning rate and epoch. $5e^{-5}+3ep$ represents the combinations of learning rate  $5e^{-5}$ and 3 fine-tuning epoch. We selected the hyperparameter combination that yielded the best F1 evaluation performance in Olympic 2024.}
    \label{fig:appendix-hyperparameters}
    % \vspace{-1.5em}
\end{figure*}

\begin{table*}[h]
\setlength{\abovecaptionskip}{5pt}
\caption{An example of tokens involved in the confidence calculation. We take the probability of the token representing the evaluation result (e.g., ``Tie'') as the evaluation confidence. For response confidence, we calculate the average probabilities of all generated tokens. We highlight these tokens with a \sethlcolor{red!30}\hl{red background.}}
\label{tab:appendix-confidence-calculation-example}
\definecolor{redE}{HTML}{C1272D}
\centering
\begin{tabularx}{\textwidth}{@{}X|X@{}}
\toprule
\multicolumn{2}{@{}l@{}}{Instruction: Describe a unique trait of the raccoon.} \\ \midrule
Response 1 & Response 2 \\
\sethlcolor{red!30}\hl{A unique trait of raccoons is their masked face, which features a black ring around each eye and a broad black stripe extending from the nose.} & \sethlcolor{red!30}\hl{A unique trait of a Raccoon is its ability to open and close its eyes while they are closed. This adaptation helps them stay alert for potential threats even when they're sleeping.} \\ \midrule
\multicolumn{2}{@{}p{\textwidth}@{}}{Pairwise comparison for both responses: The evaluation result is \sethlcolor{red!30}\hl{1}. The trait provided by Response 2 is incorrect. Response 1 provides the correct answer. Response 1 is better than Response 2. } \\ \midrule
\multicolumn{2}{@{}p{\textwidth}@{}}{Single-answer grading for response 1: The rating for response 1 is \sethlcolor{red!30}\hl{7}.} \\ \midrule
\multicolumn{2}{@{}p{\textwidth}@{}}{Single-answer grading for response 2: The rating for response 2 is \sethlcolor{red!30}\hl{6}.} \\ \bottomrule
\end{tabularx}
\end{table*}

\begin{table*}[h]
\setlength{\abovecaptionskip}{5pt}
\caption{A fine-tuning instance for ConfiLM. The fine-tuning set and the test set use the same instance format, which consists of an input tuple (user instruction $q$, response 1 $r_1$, response confidence of response 1 $u_1$, response 2 $r_2$, response confidence of response 2 $u_2$) and an output tuple (evaluation explanation $e$, evaluation result $p$).}
\label{tab:appendix-instance-example}
\definecolor{redE}{HTML}{C1272D}
\centering
\begin{tabularx}{\textwidth}{@{}X|X@{}}
\toprule
\multicolumn{2}{@{}l@{}}{Instruction $q$: Describe a unique trait of the raccoon.} \\ \midrule
\multicolumn{2}{@{}l@{}}{Response 1 $r_1$:}\\
\multicolumn{2}{@{}p{\textwidth}@{}}{A unique trait of raccoons is their masked face, which features a black ring around each eye and a broad black stripe extending from the nose.} \\ \midrule
\multicolumn{2}{@{}l@{}}{Response 1's response confidence $u_1$: Highly confident}\\ \midrule
\multicolumn{2}{@{}l@{}}{Response 2 $r_2$:}\\
\multicolumn{2}{@{}p{\textwidth}@{}}{A unique trait of a Raccoon is its ability to open and close its eyes while they are closed. This adaptation helps them stay alert for potential threats even when they're sleeping.} \\ \midrule
\multicolumn{2}{@{}l@{}}{Response 2's response confidence $u_2$: Clearly confident}\\ \midrule
\multicolumn{2}{@{}l@{}}{Evaluation explanation $e$:}\\
\multicolumn{2}{@{}p{\textwidth}@{}}{The trait provided by Response 2 is incorrect. Response 1 provides the correct answer.} \\ \midrule
\multicolumn{2}{@{}l@{}}{Preference $p$: 1}\\ \bottomrule
\end{tabularx}
\end{table*}

\begin{table*}[h]
\setlength{\abovecaptionskip}{5pt}
\caption{Another fine-tuning instance for ConfiLM.}
\label{tab:appendix-instance-example-2}
% \definecolor{redE}{HTML}{C1272D}
\centering
\begin{tabularx}{\textwidth}{@{}X|X@{}}
\toprule
\multicolumn{2}{@{}l@{}}{Instruction $q$: What is a good way to handle a difficult problem?} \\ \midrule
\multicolumn{2}{@{}l@{}}{Response 1 $r_1$:}\\
\multicolumn{2}{@{}p{\textwidth}@{}}{How can I provide helpful assistance in resolving it? Please provide me with the specific problem you are facing, and I will do my best to offer suggestions and solutions.} \\ \midrule
\multicolumn{2}{@{}l@{}}{Response 1's response confidence $u_1$: Clearly confident}\\ \midrule
\multicolumn{2}{@{}l@{}}{Response 2 $r_2$:}\\
\multicolumn{2}{@{}p{\textwidth}@{}}{Handling a difficult problem can be a challenging task, but here are some steps that might help:1. **Understand the problem**: Clearly define the problem and identify its root cause. Break it down into smaller, manageable parts if necessary. 2. **Research**: Gather ...} \\ \midrule
\multicolumn{2}{@{}l@{}}{Response 2's response confidence $u_2$: Highly confident}\\ \midrule
\multicolumn{2}{@{}l@{}}{Evaluation explanation $e$:}\\
\multicolumn{2}{@{}p{\textwidth}@{}}{Response 2 is closer to the user's true intent. Response 1 mistakenly interprets the user's intent as asking for help.} \\ \midrule
\multicolumn{2}{@{}l@{}}{Preference $p$: 2}\\ \bottomrule
\end{tabularx}
\end{table*}



% \subsubsection{Instance Examples}
% test set数据例子
\begin{table*}[h]
\setlength{\abovecaptionskip}{5pt}
\caption{Examples of user instruction from the Olympic 2024 dataset. Due to space limitations, we truncate the content of the Extraction instance.}
\label{tab:appendix-instruction-examples}
\centering
\begin{tabularx}{\textwidth}{@{}X|X@{}}
\toprule
\multicolumn{2}{@{}p{\textwidth}@{}}{\textbf{Writing:} Generate a news report based on the following sentences: The Men's Water Polo Gold Medal Match took place on 11/08/2024 at Paris La Defense Arena, Paris. Serbia claimed the Gold with a 13:11 victory against Croatia.} \\ \midrule
\multicolumn{2}{@{}p{\textwidth}@{}}{\textbf{Math:} In the Women's Synchronised 3m Springboard Final at the Paris Olympics, teams from different countries will compete in five rounds. The scores for each round for the athletes from the United States of America are 49.80, 51.00, 71.10, 72.54, and 70.20. The scores for each round for the athletes from Great Britain are 50.40, 46.20, 63.90, 71.10, and 70.68. What is the total score for the athletes from the United States of America over the five rounds, and on average, how many more points did they score per round compared to the athletes from Great Britain?} \\ 
\multicolumn{2}{@{}p{\textwidth}@{}}{Reference answer: 314.64, 2.472.} \\
\midrule
\multicolumn{2}{@{}p{\textwidth}@{}}{\textbf{Extraction:} I will provide you with a report on a specific Olympic event. Extract the following information from the presented report: the country Sky Brown represents in the Olympics, the sport Sky Brown competes in at the Olympic Games and the age of Sky Brown during the Paris 2024 Olympics. The Report is: For most athletes, the Olympic Games are a battle for medals. For 16-year-old Sky Brown, they are also a battle against injuries.  ``I don't know what's up with that!'' the British skateboarder told Olympics.com about her injury-plagued months leading up to the Olympic Games Paris 2024. ``The injury timing is not the best timing. But I do feel like I'm just going to get stronger from this.'' Brown had to overcome ...} \\
\multicolumn{2}{@{}p{\textwidth}@{}}{Reference answer: Britain, skateboard, 16.} \\
\midrule
\multicolumn{2}{@{}p{\textwidth}@{}}{\textbf{Reasoning:} In the men's Group A volleyball competition at the Paris Olympics, each team played against every other team once, and the final results are as follows: Serbia defeated Canada but lost to Slovenia, France lost to Slovenia but won against Canada. Canada lost to Slovenia, and France defeated Serbia. Based on this information, please rank these four teams in their final standings.} \\
\multicolumn{2}{@{}p{\textwidth}@{}}{Reference answer: Ranking: Slovenia, France, Serbia, Canada.} \\
\midrule
\multicolumn{2}{@{}p{\textwidth}@{}}{\textbf{Roleplay:} Imagine you are a spectator at an Olympic event, and the athletes have just finished their competition. The team you support has won the competition. A reporter approaches you to ask about your thoughts on the event. I will provide you with a brief report on the event. Based on this information, please respond to the reporter with your impressions of the event and the athlete. The report is: The Men's Water Polo Gold Medal Match took place on 11/08/2024 at Paris La Defense Arena, Paris. Serbia claimed the Gold with a 13:11 victory against Croatia.} \\  \bottomrule
\end{tabularx}
\end{table*}

\begin{table*}[h]
\setlength{\abovecaptionskip}{5pt}
\caption{A test instance for ConfiLM.}
\label{tab:appendix-instance-example-test}
\definecolor{redE}{HTML}{C1272D}
\centering
\begin{tabularx}{\textwidth}{@{}X|X@{}}
\toprule
\multicolumn{2}{@{}p{\textwidth}@{}}{Instruction $q$: In the Women's +81kg weightlifting event at the Paris Olympics, the Chinese athlete successfully lifted 130kg in her first attempt and 136kg in her second attempt in the Snatch category. In the Clean and Jerk category, she successfully lifted 167kg in her first attempt and 173kg in her second attempt, but failed to lift 174kg in her third attempt. The Korean athlete successfully lifted 123kg in her first attempt, 127kg in her second attempt, and 131kg in her third attempt in the Snatch category. In the Clean and Jerk category, she successfully lifted 163kg in her first attempt and 168kg in her second attempt, but failed to lift 173kg in her third attempt. How many more kilograms did the Chinese athlete lift compared to the Korean athlete in their final totals?} \\ 
\multicolumn{2}{@{}l@{}}{Reference answer: 10 kilograms.}\\
\midrule
\multicolumn{2}{@{}l@{}}{Response 1 $r_1$:}\\
\multicolumn{2}{@{}p{\textwidth}@{}}{**Step 1: Calculate the total weight lifted by each athlete.** - Chinese athlete: 136kg (Snatch) + 173kg (Clean and Jerk) = 309kg - Korean athlete: 131kg (Snatch) + 168kg (Clean and Jerk) = 299kg. **Step 2: Calculate the difference in weight lifted between the athletes.** - Difference = 309kg (Chinese) - 299kg (Korean) = **10kg**. Therefore, the Chinese athlete lifted 10kg more than the Korean athlete in their final totals.} \\ \midrule
\multicolumn{2}{@{}l@{}}{Response 1's response confidence $u_1$: Absolute confidence}\\ \midrule
\multicolumn{2}{@{}l@{}}{Response 2 $r_2$:}\\
\multicolumn{2}{@{}p{\textwidth}@{}}{To find out how many more kilograms the Chinese athlete lifted compared to the Korean athlete in their final totals, we first need to calculate the total weight lifted by each athlete. **Chinese Athlete:** - Snatch: (130kg + 136kg = 266kg) - Clean and Jerk: (167kg + 173kg = 340kg) - Total: (266kg + 340kg = 606kg) **Korean Athlete:** - Snatch: (123kg + 127kg + 131kg = 381kg) - Clean and Jerk: (163kg + 168kg + 173kg = 504kg) - Total: (381kg + 504kg = 885kg) **Comparison:** The Chinese athlete lifted a total of 606kg, while the Korean athlete lifted a total of 885kg. Therefore, the Korean athlete lifted (885kg - 606kg = 279kg) more than the Chinese athlete.} \\ \midrule
\multicolumn{2}{@{}l@{}}{Response 2's response confidence $u_2$: Clearly confident}\\ \midrule
\multicolumn{2}{@{}l@{}}{Evaluation explanation $e$:}\\
\multicolumn{2}{@{}p{\textwidth}@{}}{The answer provided by Response 2 is incorrect. Response 1 provides the correct answer.} \\ \midrule
\multicolumn{2}{@{}l@{}}{Preference $p$: 1}\\ \bottomrule
\end{tabularx}
\end{table*}



\begin{table*}[h]
\setlength{\abovecaptionskip}{5pt}
\caption{Hallucination case. Response 2 contains a match date that contradicts the instruction.} 
\label{tab:appendix-case-examples}
\centering
    \begin{tabularx}{0.8\textwidth}{@{\hskip 2pt}X@{\hskip 2pt}}
        \toprule
        User instruction: Generate a news report based on the following sentences: The Women's Rugby Sevens Quarterfinal between Australia and Ireland was held on 29/07/2024. Australia booked a place in the Semifinals with a 40:7 win.   \\ \midrule
        \textbf{Response 1:} Women's Rugby Sevens: Australia Cruises Past Ireland in Quarterfinal\textbackslash{}n\textbackslash{}nThe Australian Women's Rugby Sevens team secured their place in the Semifinals with a dominant 40-7 victory over Ireland in the Quarterfinal on Saturday afternoon. \textbackslash{}n\textbackslash{}nThe match started with Australia showcasing their attacking prowess, scoring four unanswered tries in the first half. Their clinical execution and strong running game proved too much for Ireland, who struggled to contain the Australian onslaught. \textbackslash{}n\textbackslash{}nIrish resistance finally came in the second half, with a try of their own. However, it was too little, too late. Australia responded immediately with two more tries to secure their comfortable victory.\textbackslash{}n\textbackslash{}n\textbackslash{}"We are thrilled to advance to the next stage,\textbackslash{}" said Australian captain, Bianca McLean. \textbackslash{}"The girls played with tremendous spirit and executed their game plan perfectly. We are confident that we can build on this performance and achieve even greater things in the Semifinals.\textbackslash{}"\textbackslash{}n\textbackslash{}nAustralia will face either New Zealand or Fiji in the next round. The winner of that match will go on to contest the final on Sunday.\textbackslash{}n\textbackslash{}n**Additional Notes:**\textbackslash{}n\textbackslash{}n* The match took place on 29/07/2024.\textbackslash{}n* The score was 40-7 in favor of Australia.\textbackslash{}n* Australia booked a place in the Semifinals.  
        \\ \textbf{Response confidence 1:} 0.865                                                                     \\
        \textbf{Response 2:} **Title: Australia Triumphs in Women's Rugby Sevens Quarterfinals, Advances to Semifinals**\textbackslash{}n\textbackslash{}n**Date: 30/07/2024**\textbackslash{}n\textbackslash{}n**Location: {[}Host City/Venue{]}**\textbackslash{}n\textbackslash{}n**{[}Host City, July 30, 2024{]} –** In a thrilling display of rugby prowess and determination, the Women's Rugby Sevens Quarterfinal match between Australia and Ireland took place on 30th July 2024, culminating in a decisive victory for the Australian team.\textbackslash{}n\textbackslash{}n**Match Overview:**\textbackslash{}n\textbackslash{}nThe highly anticipated showdown between Australia and Ireland began with a palpable sense of intensity from both sides. The Australian team, renowned for its strength and strategic gameplay, took control early on, setting a robust defensive stance against the Irish team's aggressive tactics.\textbackslash{}n\textbackslash{}n**First Half Highlight:**\textbackslash{}n\textbackslash{}nThroughout the first half, Australia's attacking dominance became apparent. {[}Player Name{]}, a crucial member of the Australian squad, showcased exceptional skills, scoring several crucial tries that paved the way for Australia's substantial lead. The Australian pack also executed precise forward play, creating openings for the backs to exploit.\textbackslash{}n\textbackslash{}n**Second Half Drama:**\textbackslash{}n\textbackslash{}nEntering the second half, Ireland displayed remarkable resilience, attempting to claw back the lead through a series of well-executed plays. Their scrum-half {[}Player Name{]} managed to breach the Australian defense, scoring a much-needed try for Ireland. However, the Australian team's robust defensive formation and tactical adjustments quickly stifled any further Irish progress.\textbackslash{}n\textbackslash{}n**Turning Point:**\textbackslash{}n\textbackslash{}nA critical moment in the game came when {[}Player Name{]}, an Australian fullback, intercepted an Irish pass, sprinting down the field to score a try. This play, combined with the Australian team's unwavering strategic discipline, sealed the match in favor of Australia.\textbackslash{}n\textbackslash{}n**Final Score:**\textbackslash{}n\textbackslash{}nThe match concluded with Australia emerging victorious with a commanding score of 40:7. The resounding \\
        \textbf{Response confidence 2:} 0.715                                                                       \\ \midrule
        \textbf{GPT-4o:} The explanation is: Response 2 offers a more thorough and engaging report, despite the missing ending. The result is: {[}2{]}                             \\ \textbf{Llama3-8B-Instruct-Finetune:} Explanation: Response 2 is more detailed and considers more aspects. Evaluation result: 2.                               \\ \textbf{ConfiLM:} Explanation: Response 1 is more informative and accurate, and it does not contain factual errors such as incorrect dates. Evaluation result:1.  \\ \bottomrule
    \end{tabularx}
\end{table*}



% \clearpage

\begin{table*}[h]
\setlength{\belowcaptionskip}{5pt}
    % \vspace{-1em}
\centering
\captionsetup{skip=10pt}
\renewcommand{\arraystretch}{1.1}
\caption{Evaluation performance on Olympic 2024. The highest F1 and evaluation confidence is marked by \textbf{bold}. \colorbox[HTML]{d9f5d6}{Green} and \colorbox[HTML]{ffccc9}{red} are used to indicate whether the evaluator's performance is improved or diminished by the incorporation of model confidence, respectively.}
% 最优 次优 pandalm是default 格式
\label{tab:appendix-ood-performance}
    \resizebox{0.9\textwidth}{!}{
    
    
    \begin{tabular}{@{}c|cccccc|c@{}}
    \toprule
                                & \multicolumn{6}{c|}{F1}                                                                                                                                                                                                                                &                                                                                   \\ \cmidrule(lr){2-7}
    \multirow{-2}{*}{Evaluator} & \multicolumn{1}{c|}{Overall}                                & Writing                                & Roleplay                      & Math                                   & Reasoning                              & Extraction                    & \multirow{-2}{*}{\begin{tabular}[c]{@{}c@{}}Evaluation\\ Confidence\end{tabular}} \\ \midrule
    GPT-4o                      & \multicolumn{1}{c|}{0.653}                                  & 0.332                                  & 0.704                         & 0.809                                  & 0.732                                  & \textbf{0.690}                & 0.960                                                                             \\
    w/o confidence              & \multicolumn{1}{c|}{\cellcolor[HTML]{FFCCC9}0.678}          & \cellcolor[HTML]{FFCCC9}0.391          & \cellcolor[HTML]{FFCCC9}0.761 & \cellcolor[HTML]{FFCCC9}0.857          & \cellcolor[HTML]{D9F5D6}0.720          & \cellcolor[HTML]{D9F5D6}0.641 & 0.968                                                                             \\
    GPT-4o-mini                 & \multicolumn{1}{c|}{\textbf{0.687}}                         & 0.478                                  & \textbf{0.774}                & 0.761                                  & \textbf{0.812}                         & 0.570                         & 0.979                                                                             \\
    w/o confidence              & \multicolumn{1}{c|}{\cellcolor[HTML]{D9F5D6}0.677}          & \cellcolor[HTML]{D9F5D6}0.423          & \cellcolor[HTML]{D9F5D6}0.727 & \cellcolor[HTML]{FFCCC9}0.820          & \cellcolor[HTML]{D9F5D6}0.800          & \cellcolor[HTML]{FFCCC9}0.627 & \textbf{0.986}                                                                    \\
    GPT-3.5-Turbo               & \multicolumn{1}{c|}{0.595}                                  & 0.425                                  & 0.672                         & 0.606                                  & 0.654                                  & 0.626                         & 0.976                                                                             \\
    w/o confidence              & \multicolumn{1}{c|}{\cellcolor[HTML]{FFCCC9}0.637}          & \cellcolor[HTML]{FFCCC9}\textbf{0.505} & \cellcolor[HTML]{FFCCC9}0.715 & \cellcolor[HTML]{FFCCC9}0.727          & \cellcolor[HTML]{D9F5D6}0.646          & \cellcolor[HTML]{D9F5D6}0.564 & 0.977                                                                             \\
    Llama3-70B-Instruct         & \multicolumn{1}{c|}{0.511}                                  & 0.387                                  & 0.570                         & 0.608                                  & 0.523                                  & 0.440                         & 0.965                                                                             \\
    w/o confidence              & \multicolumn{1}{c|}{\cellcolor[HTML]{FFCCC9}0.542}          & \cellcolor[HTML]{D9F5D6}0.316          & \cellcolor[HTML]{FFCCC9}0.627 & \cellcolor[HTML]{FFCCC9}0.647          & \cellcolor[HTML]{FFCCC9}0.684          & \cellcolor[HTML]{D9F5D6}0.377 & 0.981                                                                             \\
    Llama2-70B-Instruct         & \multicolumn{1}{c|}{0.562}                                  & 0.452                                  & 0.685                         & 0.482                                  & 0.583                                  & 0.522                         & 0.974                                                                             \\
    w/o confidence              & \multicolumn{1}{c|}{\cellcolor[HTML]{D9F5D6}0.534}          & \cellcolor[HTML]{FFCCC9}0.241          & \cellcolor[HTML]{FFCCC9}0.701 & \cellcolor[HTML]{FFCCC9}0.546          & \cellcolor[HTML]{D9F5D6}0.567          & \cellcolor[HTML]{FFCCC9}0.613 & 0.973                                                                             \\
    Qwen2-72B-Instruct          & \multicolumn{1}{c|}{0.597}                                  & 0.413                                  & 0.730                         & 0.620                                  & 0.648                                  & 0.472                         & 0.976                                                                             \\
    w/o confidence              & \multicolumn{1}{c|}{\cellcolor[HTML]{FFCCC9}0.631}          & \cellcolor[HTML]{D9F5D6}0.404          & \cellcolor[HTML]{D9F5D6}0.689 & \cellcolor[HTML]{FFCCC9}\textbf{0.867} & \cellcolor[HTML]{FFCCC9}0.696          & \cellcolor[HTML]{D9F5D6}0.462 & 0.978                                                                             \\ \midrule
    Prometheus2-7B              & \multicolumn{1}{c|}{0.515}                                  & 0.280                                  & \textbf{0.703}                & 0.537                                  & 0.611                                  & 0.307                         & \textbf{0.971}                                                                    \\
    Prometheus2-bgb-8x7B        & \multicolumn{1}{c|}{\textbf{0.556}}                         & 0.394                                  & 0.658                         & \textbf{0.641}                         & \textbf{0.696}                         & 0.267                         & 0.965                                                                             \\
    PandaLM-7B                  & \multicolumn{1}{c|}{0.476}                                  & \textbf{0.483}                         & 0.428                         & 0.436                                  & 0.576                                  & \textbf{0.325}                & 0.712                                                                             \\ \midrule
    Llama3-8B-Instruct          & \multicolumn{1}{c|}{0.545}                                  & 0.284                                  & \textbf{0.674}                & 0.677                                  & 0.610                                  & \textbf{0.610}                & 0.980                                                                             \\
    w/o confidence              & \multicolumn{1}{c|}{\cellcolor[HTML]{D9F5D6}0.536}          & \cellcolor[HTML]{D9F5D6}0.267          & \cellcolor[HTML]{D9F5D6}0.650 & \cellcolor[HTML]{FFCCC9}\textbf{0.693} & \cellcolor[HTML]{FFCCC9}0.635          & \cellcolor[HTML]{D9F5D6}0.388 & 0.973                                                                             \\
    Llama3-8B-Instruct-finetune & \multicolumn{1}{c|}{0.582}                                  & 0.603                                  & 0.573                         & 0.333                                  & 0.628                                  & 0.458                         & 0.979                                                                             \\
    ConfiLM                     & \multicolumn{1}{c|}{\cellcolor[HTML]{D9F5D6}\textbf{0.621}} & \cellcolor[HTML]{D9F5D6}\textbf{0.723} & \cellcolor[HTML]{FFCCC9}0.566 & \cellcolor[HTML]{D9F5D6}0.510          & \cellcolor[HTML]{D9F5D6}\textbf{0.670} & \cellcolor[HTML]{D9F5D6}0.594 & \textbf{0.982}                                                                    \\ \bottomrule
    \end{tabular}
    }
\end{table*}

% \begin{table*}[htbp]
% \setlength{\abovecaptionskip}{5pt}
% \caption{Data Statistics.}
% \label{tab:data-statistics}
% \centering
% % \renewcommand{\arraystretch}{1.0}
% \resizebox{1.0\textwidth}{!}{
%     \begin{tabular}{@{}ll@{}}
%     \toprule
%     \multicolumn{2}{l}{Instruction: Describe a unique trait of the raccoon.}                                                                                                                                                                                                                                                                                                                                                                                    \\ \midrule
%     \multicolumn{1}{l|}{Candidate A's response}                                                                                                                                                                       & Candidate B's response                                                                                                                                                                                                                  \\
%     \multicolumn{1}{l|}{\begin{tabular}[c]{@{}l@{}}A unique trait of raccoons is their masked face, which features a black ring around each eye and a broad black stripe extending from the nose.\end{tabular}} & \begin{tabular}[c]{@{}l@{}}A unique trait of a Raccoon is its ability to open and close its eyes while they are closed. This adaptation helps them stay alert for potential threats even when they're sleeping.\end{tabular} \\ \midrule
%     \multicolumn{2}{l}{Pairwise comparison: The evaluation result is A. Candidate A's reponse is better than candidate B's reponse.}                                                                                                                                                                                                                                                                                                                            \\
%     \multicolumn{2}{l}{Single-answer grading for candidate A's response:The rating for candidate A's response is 7.}                                                                                                                                                                                                                                                                                                                                        \\
%     \multicolumn{2}{l}{Single-answer grading for candidate B's response:The rating for candidate A's response is 6.}                                                                                                                                                                                                                                                                                                                                        \\ \bottomrule
%     \end{tabular}
% }
% % \vspace{-10pt}
% \end{table*}

\begin{table*}[h]
\setlength{\belowcaptionskip}{5pt}
    % \vspace{-1em}
\centering
\captionsetup{skip=0pt}
\caption{Full result of uncertainty analysis of single-answer grading on MT-Bench and PandaLM test set. The scoring range is 0-9. The evaluation subject is Llama2-7B-Instruct. The Prometheus2 series models are trained to output in a Chain-of-Thoughts format (providing a concise rationale before indicating a preference between the two outputs)~\citep{kim2024prometheus2}.}
\label{tab:appendix-full-single}
    \begin{minipage}{\linewidth}
        \subcaption{MT-Bench}
        \label{tab:appendix-full-single-mtbench}
        \centering
        \resizebox{1.0\textwidth}{!}{
        \begin{tabular}{@{}c|ccc|ccc|ccc@{}}
        \toprule
                                    & \multicolumn{3}{c|}{Default}                                                                                                                                                               & \multicolumn{3}{c|}{Chain-of-Thoughts}                                                                                                                                                     & \multicolumn{3}{c}{Self-generated Reference}                                                                                                                                               \\ \cmidrule(l){2-10} 
        \multirow{-2}{*}{Evaluator} & \begin{tabular}[c]{@{}c@{}}Average\\ Rating\end{tabular} & \begin{tabular}[c]{@{}c@{}}Evaluation\\ Confidence\end{tabular} & \begin{tabular}[c]{@{}c@{}}Response\\ Confidence\end{tabular} & \begin{tabular}[c]{@{}c@{}}Average\\ Rating\end{tabular} & \begin{tabular}[c]{@{}c@{}}Evaluation\\ Confidence\end{tabular} & \begin{tabular}[c]{@{}c@{}}Response\\ Confidence\end{tabular} & \begin{tabular}[c]{@{}c@{}}Average\\ Rating\end{tabular} & \begin{tabular}[c]{@{}c@{}}Evaluation\\ Confidence\end{tabular} & \begin{tabular}[c]{@{}c@{}}Response\\ Confidence\end{tabular} \\ \midrule
        GPT-4o-2024-05-13           & 5.413                                                    & 0.417                                                           & 0.945                                                         & 4.988                                                    & 0.681                                                           & 0.945                                                         & 4.738                                                    & 0.637                                                           & 0.915                                                         \\
        GPT-4o-mini-2024-07-18      & 6.038                                                    & 0.605                                                           & 0.944                                                         & 5.113                                                    & 0.833                                                           & 0.944                                                         & 4.500                                                    & 0.800                                                           & 0.944                                                         \\
        GPT-3.5-Turbo               & 6.288                                                    & 0.629                                                           & 0.944                                                         & 5.925                                                    & 0.703                                                           & 0.924                                                         & 5.650                                                    & 0.632                                                           & 0.944                                                         \\
        Llama-3-70B-Instruct        & 7.250                                                    & 0.644                                                           & 0.944                                                         & 6.275                                                    & 0.829                                                           & 0.945                                                         & 7.125                                                    & 0.941                                                           & 0.905                                                         \\
        Llama-2-70B-Instruct        & 7.875                                                    & 0.953                                                           & 0.945                                                         & 6.775                                                    & 0.979                                                           & 0.915                                                         & 7.563                                                    & 0.931                                                           & 0.925                                                         \\
        Qwen2-72B-Instruct          & 5.875                                                    & 0.675                                                           & 0.946                                                         & 5.550                                                    & 0.752                                                           & 0.946                                                         & 5.175                                                    & 0.777                                                           & 0.946                                                         \\ \midrule
        Prometheus2-7B              & \textbackslash{}                                         & \textbackslash{}                                                & \textbackslash{}                                              & 5.963                                                    & 0.993                                                           & 0.946                                                         & \textbackslash{}                                         & \textbackslash{}                                                & \textbackslash{}                                              \\
        Prometheus2-bgb-8x7B        & \textbackslash{}                                         & \textbackslash{}                                                & \textbackslash{}                                              & 4.725                                                    & 0.870                                                           & 0.945                                                         & \textbackslash{}                                         & \textbackslash{}                                                & \textbackslash{}                                              \\ \midrule
        \rowcolor[HTML]{E1EAFF} 
        Average                     & 6.456                                                    & 0.654                                                           & 0.945                                                         & 5.486                                                    & 0.886                                                           & 0.942                                                         & 5.792                                                    & 0.786                                                           & 0.930                                                         \\ \bottomrule
        \end{tabular}
        }
    \end{minipage}
    \hfill
    \begin{minipage}{\textwidth}
        \subcaption{PandaLM Test set}
        \label{tab:appendix-full-single-pandalmdata}
        \centering
        \resizebox{1.0\textwidth}{!}{
        \begin{tabular}{@{}c|ccc|ccc|ccc@{}}
        \toprule
                                    & \multicolumn{3}{c|}{Default}                                                                                                                                                               & \multicolumn{3}{c|}{Chain-of-Thoughts}                                                                                                                                                     & \multicolumn{3}{c}{Self-generated Reference}                                                                                                                                               \\ \cmidrule(l){2-10} 
        \multirow{-2}{*}{Evaluator} & \begin{tabular}[c]{@{}c@{}}Average\\ Rating\end{tabular} & \begin{tabular}[c]{@{}c@{}}Evaluation\\ Confidence\end{tabular} & \begin{tabular}[c]{@{}c@{}}Response\\ Confidence\end{tabular} & \begin{tabular}[c]{@{}c@{}}Average\\ Rating\end{tabular} & \begin{tabular}[c]{@{}c@{}}Evaluation\\ Confidence\end{tabular} & \begin{tabular}[c]{@{}c@{}}Response\\ Confidence\end{tabular} & \begin{tabular}[c]{@{}c@{}}Average\\ Rating\end{tabular} & \begin{tabular}[c]{@{}c@{}}Evaluation\\ Confidence\end{tabular} & \begin{tabular}[c]{@{}c@{}}Response\\ Confidence\end{tabular} \\ \midrule
        GPT-4o-2024-05-13           & 6.541                                                    & 0.473                                                           & 0.935                                                         & 6.400                                                    & 0.753                                                           & 0.935                                                         & 6.082                                                    & 0.637                                                           & 0.934                                                         \\
        GPT-4o-mini-2024-07-18      & 6.641                                                    & 0.645                                                           & 0.936                                                         & 6.229                                                    & 0.861                                                           & 0.915                                                         & 6.406                                                    & 0.792                                                           & 0.916                                                         \\
        GPT-3.5-Turbo               & 6.665                                                    & 0.594                                                           & 0.936                                                         & 6.588                                                    & 0.763                                                           & 0.935                                                         & 6.971                                                    & 0.627                                                           & 0.934                                                         \\
        Llama-3-70B-Instruct        & 7.424                                                    & 0.548                                                           & 0.937                                                         & 7.453                                                    & 0.975                                                           & 0.907                                                         & 6.965                                                    & 0.964                                                           & 0.927                                                         \\
        Llama-2-70B-Instruct        & 7.924                                                    & 0.960                                                           & 0.934                                                         & 6.965                                                    & 0.966                                                           & 0.934                                                         & 7.741                                                    & 0.930                                                           & {\color[HTML]{1F2329} 0.934}                                  \\
        Qwen2-72B-Instruct          & 7.153                                                    & 0.692                                                           & 0.935                                                         & 7.306                                                    & 0.841                                                           & 0.935                                                         & 6.950                                                    & 0.861                                                           & {\color[HTML]{1F2329} 0.935}                                  \\ \midrule
        Prometheus2-7B              & \textbackslash{}                                         & \textbackslash{}                                                & \textbackslash{}                                              & 7.187                                                    & 0.991                                                           & 0.937                                                         & \textbackslash{}                                         & \textbackslash{}                                                & \textbackslash{}                                              \\
        Prometheus2-bgb-8x7B        & \textbackslash{}                                         & \textbackslash{}                                                & \textbackslash{}                                              & 6.101                                                    & 0.887                                                           & 0.936                                                         & \textbackslash{}                                         & \textbackslash{}                                                & \textbackslash{}                                              \\ \midrule
        \rowcolor[HTML]{E1EAFF} 
        Average                     & 7.058                                                    & 0.652                                                           & 0.936                                                         & 6.704                                                    & 0.912                                                           & 0.933                                                         & 6.853                                                    & 0.802                                                           & 0.930                                                         \\ \bottomrule
        \end{tabular}
        }
    \end{minipage}
    % \vspace{-1.5em}
\end{table*}



% \begin{table*}[t]
% % \setlength{\abovecaptionskip}{5pt}
% % \setlength{\belowcaptionskip}{20pt}
% \caption{Full result of uncertainty analysis of single-answer grading on MT-Bench. The scoring range is 0-9. The evaluation subject is Llama2-7B-Instruct. The Prometheus2 series models are trained to output in a Chain-of-Thoughts format (providing a concise rationale before indicating a preference between the two outputs)~\citep{kim2024prometheus2}.}
% \label{tab:appendix-full-single-mtbench}
% \centering
% % \renewcommand{\arraystretch}{1.1}
% \resizebox{\linewidth}{!}{
% \begin{tabular}{@{}c|ccc|ccc|ccc@{}}
% \toprule
%                             & \multicolumn{3}{c|}{Default}                                                                                                                                                               & \multicolumn{3}{c|}{Chain-of-Thoughts}                                                                                                                                                     & \multicolumn{3}{c}{Self-generated Reference}                                                                                                                                               \\ \cmidrule(l){2-10} 
% \multirow{-2}{*}{Evaluator} & \begin{tabular}[c]{@{}c@{}}Average\\ Rating\end{tabular} & \begin{tabular}[c]{@{}c@{}}Evaluation\\ Confidence\end{tabular} & \begin{tabular}[c]{@{}c@{}}Response\\ Confidence\end{tabular} & \begin{tabular}[c]{@{}c@{}}Average\\ Rating\end{tabular} & \begin{tabular}[c]{@{}c@{}}Evaluation\\ Confidence\end{tabular} & \begin{tabular}[c]{@{}c@{}}Response\\ Confidence\end{tabular} & \begin{tabular}[c]{@{}c@{}}Average\\ Rating\end{tabular} & \begin{tabular}[c]{@{}c@{}}Evaluation\\ Confidence\end{tabular} & \begin{tabular}[c]{@{}c@{}}Response\\ Confidence\end{tabular} \\ \midrule
% GPT-4o-2024-05-13           & 5.413                                                    & 0.417                                                           & 0.945                                                         & 4.988                                                    & 0.681                                                           & 0.945                                                         & 4.738                                                    & 0.637                                                           & 0.915                                                         \\
% GPT-4o-mini-2024-07-18      & 6.038                                                    & 0.605                                                           & 0.944                                                         & 5.113                                                    & 0.833                                                           & 0.944                                                         & 4.500                                                    & 0.800                                                           & 0.944                                                         \\
% GPT-3.5-Turbo               & 6.288                                                    & 0.629                                                           & 0.944                                                         & 5.925                                                    & 0.703                                                           & 0.924                                                         & 5.650                                                    & 0.632                                                           & 0.944                                                         \\
% Llama-3-70B-Instruct        & 7.250                                                    & 0.644                                                           & 0.944                                                         & 6.275                                                    & 0.829                                                           & 0.945                                                         & 7.125                                                    & 0.941                                                           & 0.905                                                         \\
% Llama-2-70B-Instruct        & 7.875                                                    & 0.953                                                           & 0.945                                                         & 6.775                                                    & 0.979                                                           & 0.915                                                         & 7.563                                                    & 0.931                                                           & 0.925                                                         \\
% Qwen2-72B-Instruct          & 5.875                                                    & 0.675                                                           & 0.946                                                         & 5.550                                                    & 0.752                                                           & 0.946                                                         & 5.175                                                    & 0.777                                                           & 0.946                                                         \\ \midrule
% Prometheus2-7B              & \textbackslash{}                                         & \textbackslash{}                                                & \textbackslash{}                                              & 5.963                                                    & 0.993                                                           & 0.946                                                         & \textbackslash{}                                         & \textbackslash{}                                                & \textbackslash{}                                              \\
% Prometheus2-bgb-8x7B        & \textbackslash{}                                         & \textbackslash{}                                                & \textbackslash{}                                              & 4.725                                                    & 0.870                                                           & 0.945                                                         & \textbackslash{}                                         & \textbackslash{}                                                & \textbackslash{}                                              \\ \midrule
% \rowcolor[HTML]{E1EAFF} 
% Average                     & 6.456                                                    & 0.654                                                           & 0.945                                                         & 5.486                                                    & 0.886                                                           & 0.942                                                         & 5.792                                                    & 0.786                                                           & 0.930                                                         \\ \bottomrule
% \end{tabular}
% }
% % \vspace{-0.5em}
% \end{table*}

% \begin{table*}[t]
% % \setlength{\abovecaptionskip}{5pt}
% % \setlength{\belowcaptionskip}{20pt}
% \caption{Full result of uncertainty analysis of single-answer grading on PandaLM test set. The scoring range is 0-9. The evaluation subject is Llama2-7B-Instruct. The Prometheus2 series models are trained to output in a Chain-of-Thoughts format (providing a concise rationale before indicating a preference between the two outputs)~\citep{kim2024prometheus2}.}
% \label{tab:appendix-full-single-pandalmdata}
% \centering
% % \renewcommand{\arraystretch}{1.1}
% \resizebox{\linewidth}{!}{
% \begin{tabular}{@{}c|ccc|ccc|ccc@{}}
% \toprule
%                             & \multicolumn{3}{c|}{Default}                                                                                                                                                               & \multicolumn{3}{c|}{Chain-of-Thoughts}                                                                                                                                                     & \multicolumn{3}{c}{Self-generated Reference}                                                                                                                                               \\ \cmidrule(l){2-10} 
% \multirow{-2}{*}{Evaluator} & \begin{tabular}[c]{@{}c@{}}Average\\ Rating\end{tabular} & \begin{tabular}[c]{@{}c@{}}Evaluation\\ Confidence\end{tabular} & \begin{tabular}[c]{@{}c@{}}Response\\ Confidence\end{tabular} & \begin{tabular}[c]{@{}c@{}}Average\\ Rating\end{tabular} & \begin{tabular}[c]{@{}c@{}}Evaluation\\ Confidence\end{tabular} & \begin{tabular}[c]{@{}c@{}}Response\\ Confidence\end{tabular} & \begin{tabular}[c]{@{}c@{}}Average\\ Rating\end{tabular} & \begin{tabular}[c]{@{}c@{}}Evaluation\\ Confidence\end{tabular} & \begin{tabular}[c]{@{}c@{}}Response\\ Confidence\end{tabular} \\ \midrule
% GPT-4o-2024-05-13           & 6.541                                                    & 0.473                                                           & 0.935                                                         & 6.400                                                    & 0.753                                                           & 0.935                                                         & 6.082                                                    & 0.637                                                           & 0.934                                                         \\
% GPT-4o-mini-2024-07-18      & 6.641                                                    & 0.645                                                           & 0.936                                                         & 6.229                                                    & 0.861                                                           & 0.915                                                         & 6.406                                                    & 0.792                                                           & 0.916                                                         \\
% GPT-3.5-Turbo               & 6.665                                                    & 0.594                                                           & 0.936                                                         & 6.588                                                    & 0.763                                                           & 0.935                                                         & 6.971                                                    & 0.627                                                           & 0.934                                                         \\
% Llama-3-70B-Instruct        & 7.424                                                    & 0.548                                                           & 0.937                                                         & 7.453                                                    & 0.975                                                           & 0.907                                                         & 6.965                                                    & 0.964                                                           & 0.927                                                         \\
% Llama-2-70B-Instruct        & 7.924                                                    & 0.960                                                           & 0.934                                                         & 6.965                                                    & 0.966                                                           & 0.934                                                         & 7.741                                                    & 0.930                                                           & {\color[HTML]{1F2329} 0.934}                                  \\
% Qwen2-72B-Instruct          & 7.153                                                    & 0.692                                                           & 0.935                                                         & 7.306                                                    & 0.841                                                           & 0.935                                                         & 6.950                                                    & 0.861                                                           & {\color[HTML]{1F2329} 0.935}                                  \\ \midrule
% Prometheus2-7B              & \textbackslash{}                                         & \textbackslash{}                                                & \textbackslash{}                                              & 7.187                                                    & 0.991                                                           & 0.937                                                         & \textbackslash{}                                         & \textbackslash{}                                                & \textbackslash{}                                              \\
% Prometheus2-bgb-8x7B        & \textbackslash{}                                         & \textbackslash{}                                                & \textbackslash{}                                              & 6.101                                                    & 0.887                                                           & 0.936                                                         & \textbackslash{}                                         & \textbackslash{}                                                & \textbackslash{}                                              \\ \midrule
% \rowcolor[HTML]{E1EAFF} 
% Average                     & 7.058                                                    & 0.652                                                           & 0.936                                                         & 6.704                                                    & 0.912                                                           & 0.933                                                         & 6.853                                                    & 0.802                                                           & 0.930                                                         \\ \bottomrule
% \end{tabular}
% }
% % \vspace{-0.5em}
% \end{table*}
% \vspace{10pt}

\begin{table*}[h]
\setlength{\belowcaptionskip}{5pt}
    % \vspace{-1em}
\centering
\captionsetup{skip=0pt}
\caption{Full result of uncertainty analysis of pairwise comparison on MT-Bench. The evaluation subjects are Llama2-7B-Instruct and Llama2-13B-Instruct. For each evaluation, we query the evaluator twice with the order swapped. 'Win / Lose / Tie' represents the average number of times Llama-2-7b-chat's response is better than, worse than, or equal to Llama-2-13b-chat's response. The PandaLM model~\citep{wangpandalm} is trained to output in a normal format (providing a preference between the two outputs, followed by a concise rationale). The Prometheus2 series models~\citep{kim2024prometheus2} are trained to output in a Chain-of-Thoughts format (providing a concise rationale before indicating a preference between the two responses).}
\label{tab:appendix-full-pair-mtbench}
    \begin{minipage}{\linewidth}
    \subcaption{Default prompt}
    \label{tab:appendix-full-pair-mtbench-default}
    \centering
    \resizebox{0.7\textwidth}{!}{
        \begin{tabular}{@{}c|cccccc@{}}
        \toprule
                                    & \multicolumn{6}{c}{Default} \\ \cmidrule(l){2-7} 
                                    &                       &                        & \multicolumn{1}{c|}{}                             &                                                                                   & \multicolumn{2}{c}{Response Confidence} \\
        \multirow{-3}{*}{Evaluator} & \multirow{-2}{*}{Win} & \multirow{-2}{*}{Lose} & \multicolumn{1}{c|}{\multirow{-2}{*}{Tie}}        & \multirow{-2}{*}{\begin{tabular}[c]{@{}c@{}}Evaluation\\ Confidence\end{tabular}} & A                   & B                  \\ \midrule
        GPT-4o-2024-05-13           & 10.0                  & 16.5                   & \multicolumn{1}{c|}{53.5}                         & 0.699                                                                             & 0.950               & 0.949              \\
        GPT-4o-mini-2024-07-18      & 27.0                  & 43.5                   & \multicolumn{1}{c|}{9.5}                          & 0.776                                                                             & 0.950               & 0.948              \\
        GPT-3.5-Turbo               & 38.5                  & 35.0                   & \multicolumn{1}{c|}{6.5}                          & 0.848                                                                             & 0.945               & 0.949              \\
        Llama-3-70B-Instruct        & 39.5                  & 37.5                   & \multicolumn{1}{c|}{3.0}                          & 0.791                                                                             & 0.937               & 0.950              \\
        Llama-2-70B-Instruct        & 33.0                  & 34.0                   & \multicolumn{1}{c|}{13.0}                         & 0.908                                                                             & 0.950               & 0.950              \\
        Qwen2-72B-Instruct          & 22.0                  & 29.0                   & \multicolumn{1}{c|}{29.0}                         & 0.762                                                                             & 0.939               & 0.949              \\ \midrule
        PandaLM-7B                  & 42.0                  & 28.5                   & \multicolumn{1}{c|}{9.5}                          & 0.617                                                                             & 0.949               & 0.939              \\ \midrule
        \rowcolor[HTML]{E1EAFF} 
        Average                     & 35.2                  & 30.5                   & \multicolumn{1}{c|}{\cellcolor[HTML]{E1EAFF}14.3} & 0.707                                                                             & 0.947               & 0.944              \\ \bottomrule
        \end{tabular}
        }
    \end{minipage}
    \hfill
    \begin{minipage}{\linewidth}
    \subcaption{Chain-of-Thoughts}
    \label{tab:appendix-full-pair-mtbench-cot}
    \centering
    \resizebox{0.7\textwidth}{!}{
    \begin{tabular}{@{}c|cccccc@{}}
        \toprule
                                    & \multicolumn{6}{c}{Chain-of-Thoughts} \\ \cmidrule(l){2-7} 
                                    &                       &                        & \multicolumn{1}{c|}{}                             &                                                                                   & \multicolumn{2}{c}{Response Confidence} \\
        \multirow{-3}{*}{Evaluator} & \multirow{-2}{*}{Win} & \multirow{-2}{*}{Lose} & \multicolumn{1}{c|}{\multirow{-2}{*}{Tie}}        & \multirow{-2}{*}{\begin{tabular}[c]{@{}c@{}}Evaluation\\ Confidence\end{tabular}} & A                   & B                  \\ \midrule 
        GPT-4o-2024-05-13           & 25.0                  & 42.0                   & \multicolumn{1}{c|}{13.0}                  & 0.940                                                                             & 0.949               & 0.949              \\
        GPT-4o-mini-2024-07-18      & 39.5                  & 37.0                   & \multicolumn{1}{c|}{3.5}                   & 0.990                                                                             & 0.950               & 0.948              \\
        GPT-3.5-Turbo               & 37.0                  & 38.5                   & \multicolumn{1}{c|}{4.5}                   & 0.924                                                                             & 0.950               & 0.949              \\
        Llama-3-70B-Instruct        & 38.0                  & 37.5                   & \multicolumn{1}{c|}{4.5}                   & 0.999                                                                             & 0.947               & 0.950              \\
        Llama-2-70B-Instruct        & 36.0                  & 37.0                   & \multicolumn{1}{c|}{7.0}                   & 0.991                                                                             & 0.950               & 0.950              \\
        Qwen2-72B-Instruct          & 30.5                  & 32.5                   & \multicolumn{1}{c|}{17.0}                  & 0.988                                                                             & 0.949               & 0.949              \\ \midrule
        Prometheus2-7B              & 37.5                  & 42.0                   & \multicolumn{1}{c|}{0.5}                   & 0.990                                                                             & 0.949               & 0.951              \\
        Prometheus2-bgb-8x7B        & 31.5                  & 32.5                   & \multicolumn{1}{c|}{16.0}                  & 0.967                                                                             & 0.948               & 0.950              \\ \midrule
        \rowcolor[HTML]{E1EAFF} 
        Average                     & 34.4                  & 37.3                   & \multicolumn{1}{c|}{\cellcolor[HTML]{E1EAFF}8.3} & 0.977                                                                             & 0.949               & 0.950              \\ \bottomrule
    \end{tabular}
    }
    \end{minipage}
    \hfill
    \begin{minipage}{\linewidth}
    \subcaption{Self-generated reference}
    \label{tab:appendix-full-pair-mtbench-self}
    \centering
    \resizebox{0.7\textwidth}{!}{
    \begin{tabular}{@{}c|cccccc@{}}
        \toprule
                                    & \multicolumn{6}{c}{Self-generated Reference} \\ \cmidrule(l){2-7} 
                                    &                       &                        & \multicolumn{1}{c|}{}                             &                                                                                   & \multicolumn{2}{c}{Response Confidence} \\
        \multirow{-3}{*}{Evaluator} & \multirow{-2}{*}{Win} & \multirow{-2}{*}{Lose} & \multicolumn{1}{c|}{\multirow{-2}{*}{Tie}}        & \multirow{-2}{*}{\begin{tabular}[c]{@{}c@{}}Evaluation\\ Confidence\end{tabular}} & A                   & B                  \\ \midrule
        GPT-4o-2024-05-13           & 21.5                  & 31.0                   & \multicolumn{1}{c|}{27.5}                  & 0.799                                                                             & 0.949                        & 0.949            \\
        GPT-4o-mini-2024-07-18      & 32.5                  & 37.0                   & \multicolumn{1}{c|}{10.5}                  & 0.793                                                                             & 0.950                        & 0.948            \\
        GPT-3.5-Turbo               & 31.5                  & 33.5                   & \multicolumn{1}{c|}{15.0}                  & 0.802                                                                             & 0.950                        & 0.949            \\
        Llama-3-70B-Instruct        & 34.5                  & 41.0                   & \multicolumn{1}{c|}{4.5}                   & 0.991                                                                             & 0.947                        & 0.950            \\
        Llama-2-70B-Instruct        & 27.5                  & 25.0                   & \multicolumn{1}{c|}{27.5}                  & 0.956                                                                             & 0.950                        & 0.950            \\
        Qwen2-72B-Instruct          & 28.0                  & 27.5                   & \multicolumn{1}{c|}{24.5}                  & 0.944                                                                             & 0.949                        & 0.949            \\ \midrule
        \rowcolor[HTML]{E1EAFF} 
        Average                     & 29.3                  & 32.5                   & \multicolumn{1}{c|}{18.2}                  & 0.881                                                                             & 0.949                        & 0.949            \\ \bottomrule
    \end{tabular}
    }
    \end{minipage}
    % \vspace{-1.5em}
\end{table*}

\begin{table*}[h]
\setlength{\belowcaptionskip}{5pt}
 % \vspace{-1em}
\centering
\captionsetup{skip=0pt}
\caption{Full result of uncertainty analysis of pairwise comparison and PandaLM test set. The evaluation subjects are Llama2-7B-Instruct and Llama2-13B-Instruct. For each evaluation, we query the evaluator twice with the order swapped. 'Win / Lose / Tie' represents the average number of times Llama-2-7b-chat's response is better than, worse than, or equal to Llama-2-13b-chat's response. The PandaLM model~\citep{wangpandalm} is trained to output in a normal format (providing a preference between the two responses, followed by a concise rationale). The Prometheus2 series models~\citep{kim2024prometheus2} are trained to output in a Chain-of-Thoughts format (providing a concise rationale before indicating a preference between the two responses).}
\label{tab:appendix-full-pair-pandalmdata}
    \begin{minipage}{\textwidth}
        \subcaption{Default prompt}
        \label{tab:appendix-full-pair-pandalmdata-default}
        \centering
        \resizebox{0.7\textwidth}{!}{
        \begin{tabular}{@{}c|cccccc@{}}
        \toprule
                                    & \multicolumn{6}{c}{Default} \\ \cmidrule(l){2-7} 
                                    &                       &                        & \multicolumn{1}{c|}{}                             &                                                                                   & \multicolumn{2}{c}{Response Confidence} \\
        \multirow{-3}{*}{Evaluator} & \multirow{-2}{*}{Win} & \multirow{-2}{*}{Lose} & \multicolumn{1}{c|}{\multirow{-2}{*}{Tie}}        & \multirow{-2}{*}{\begin{tabular}[c]{@{}c@{}}Evaluation\\ Confidence\end{tabular}} & A                   & B                  \\ \midrule
        GPT-4o-2024-05-13           & 30.0                  & 38.0                   & \multicolumn{1}{c|}{102.0}                        & 0.809                                                                             & 0.932               & 0.941              \\
        GPT-4o-mini-2024-07-18      & 53.0                  & 61.0                   & \multicolumn{1}{c|}{56.0}                         & 0.820                                                                             & 0.938               & 0.940              \\
        GPT-3.5-Turbo               & 76.5                  & 81.0                   & \multicolumn{1}{c|}{12.5}                         & 0.884                                                                             & 0.930               & 0.940              \\
        Llama-3-70B-Instruct        & 78.0                  & 86.5                   & \multicolumn{1}{c|}{5.5}                          & 0.849                                                                             & 0.941               & 0.941              \\
        Llama-2-70B-Instruct        & 72.5                  & 73.0                   & \multicolumn{1}{c|}{24.5}                         & 0.931                                                                             & 0.929               & 0.940              \\
        Qwen2-72B-Instruct          & 54.0                  & 70.0                   & \multicolumn{1}{c|}{46.0}                         & 0.806                                                                             & 0.938               & 0.940              \\ \midrule
        PandaLM-7B                  & 58.0                  & 72.0                   & \multicolumn{1}{c|}{40.0}                         & 0.704                                                                             & 0.942               & 0.939              \\ \midrule
        \rowcolor[HTML]{E1EAFF} 
        Average                     & 59.3                  & 70.1                   & \multicolumn{1}{c|}{\cellcolor[HTML]{E1EAFF}40.5} & 0.777                                                                             & 0.938               & 0.940              \\ \bottomrule
        \end{tabular}
        }
    \end{minipage}
    \hfill
    \begin{minipage}{\textwidth}
        \subcaption{Chain-of-Thoughts}
        \label{tab:appendix-full-pair-pandalmdata-cot}
        \centering
        % \huge
        \resizebox{0.7\textwidth}{!}{
        \begin{tabular}{@{}c|cccccc@{}}
        \toprule
                                    & \multicolumn{6}{c}{Chain-of-Thoughts} \\ \cmidrule(l){2-7} 
                                    &                       &                        & \multicolumn{1}{c|}{}                             &                                                                                   & \multicolumn{2}{c}{Response Confidence} \\
        \multirow{-3}{*}{Evaluator} & \multirow{-2}{*}{Win} & \multirow{-2}{*}{Lose} & \multicolumn{1}{c|}{\multirow{-2}{*}{Tie}}        & \multirow{-2}{*}{\begin{tabular}[c]{@{}c@{}}Evaluation\\ Confidence\end{tabular}} & A                   & B                  \\ \midrule
        GPT-4o-2024-05-13           & 63.0                  & 92.5                   & \multicolumn{1}{c|}{14.5}                        & 0.966                                                                             & 0.940               & 0.940              \\
        GPT-4o-mini-2024-07-18      & 82.5                  & 86.5                   & \multicolumn{1}{c|}{1.0}                         & 0.996                                                                             & 0.939               & 0.941              \\
        GPT-3.5-Turbo               & 82.5                  & 79.5                   & \multicolumn{1}{c|}{8.0}                         & 0.962                                                                             & 0.941               & 0.940              \\
        Llama-3-70B-Instruct        & 71.0                  & 94.5                   & \multicolumn{1}{c|}{4.5}                         & 0.999                                                                             & 0.941               & 0.941              \\
        Llama-2-70B-Instruct        & 79.0                  & 72.5                   & \multicolumn{1}{c|}{18.5}                        & 0.986                                                                             & 0.939               & 0.940              \\
        Qwen2-72B-Instruct          & 63.5                  & 83.0                   & \multicolumn{1}{c|}{23.5}                        & 0.992                                                                             & 0.938               & 0.940              \\ \midrule
        Prometheus2-7B                  & 77.5                  & 92.5                   & \multicolumn{1}{c|}{0.0}                         & 0.993                                                                             & 0.940               & 0.940              \\
        Prometheus2-bgb-8x7B                  & 77.0                  & 80.5                   & \multicolumn{1}{c|}{12.5}                        & 0.974                                                                             & 0.941               & 0.939              \\ \midrule
        \rowcolor[HTML]{E1EAFF} 
        Average                     & 76.0                  & 85.9                   & \multicolumn{1}{c|}{8.1} & 0.984                                                                             & 0.940               & 0.940              \\ \bottomrule
        \end{tabular}
        }
    
    \end{minipage}
    \hfill
    \begin{minipage}{\textwidth}
        \subcaption{Self-generated reference}
        \label{tab:appendix-full-pair-pandalmdata-self}
        \centering
        % \huge
        \resizebox{0.7\textwidth}{!}{
        \begin{tabular}{@{}c|cccccc@{}}
        \toprule
                                    & \multicolumn{6}{c}{Self-generated Reference} \\ \cmidrule(l){2-7} 
                                    &                       &                        & \multicolumn{1}{c|}{}                             &                                                                                   & \multicolumn{2}{c}{Response Confidence} \\
        \multirow{-3}{*}{Evaluator} & \multirow{-2}{*}{Win} & \multirow{-2}{*}{Lose} & \multicolumn{1}{c|}{\multirow{-2}{*}{Tie}}        & \multirow{-2}{*}{\begin{tabular}[c]{@{}c@{}}Evaluation\\ Confidence\end{tabular}} & A                   & B                  \\ \midrule
        GPT-4o-2024-05-13           & 47.5                  & 59.5                   & \multicolumn{1}{c|}{63.0}                         & 0.774                                                                             & 0.940                        & 0.940              \\
        GPT-4o-mini-2024-07-18      & 72.5                  & 80.5                   & \multicolumn{1}{c|}{17.0}                         & 0.800                                                                             & 0.939                        & 0.941              \\
        GPT-3.5-Turbo               & 65.5                  & 63.5                   & \multicolumn{1}{c|}{41.0}                         & 0.809                                                                             & 0.941                        & 0.936              \\
        Llama-3-70B-Instruct        & 68.0                  & 94.5                   & \multicolumn{1}{c|}{7.5}                          & 0.997                                                                             & 0.941                        & 0.941              \\
        Llama-2-70B-Instruct        & 47.0                  & 55.5                   & \multicolumn{1}{c|}{67.5}                         & 0.964                                                                             &0.939 
        & 0.939              \\
        Qwen2-72B-Instruct          & 56.0                  & 70.5                   & \multicolumn{1}{c|}{43.5}                         & 0.947                                                                             &  0.938
        & 0.940              \\  \midrule
        \rowcolor[HTML]{E1EAFF} 
        Average                     & 60.1                  & 70.7                   & \multicolumn{1}{c|}{39.9} & 0.882                                                                             & 0.940                        & 0.939              \\ \bottomrule
        \end{tabular}
        }
    
    \end{minipage}
    
    % \vspace{-1.5em}
\end{table*}

% \begin{table*}[h]
% \setlength{\abovecaptionskip}{5pt}
% \caption{Evaluation performance on Olympic 2024. For each evaluation, we query the evaluator twice with the order swapped. The highest value is marked in \textbf{bold}.}
% \label{tab:appendix-0lympic-2024-w/o}
% \centering
% \resizebox{0.8\textwidth}{!}{
%     \begin{tabular}{@{}c|ccccc@{}}
%     \toprule
%     \multirow{2}{*}{Evaluator}   & \multicolumn{5}{c}{Overall}                                                               \\ \cmidrule(l){2-6} 
%                                  & Accuracy       & Precision      & Recall         & F1             & Evaluation Confidence \\ \midrule
%     GPT-4o                       & \textbf{0.683} & \textbf{0.698} & \textbf{0.684} & \textbf{0.678} & 0.968                 \\
%     GPT-4o-mini                  & 0.680          & 0.687          & 0.681          & 0.677          & \textbf{0.986}        \\
%     GPT-3.5-Turbo                & 0.643          & 0.658          & 0.644          & 0.637          & 0.977                 \\
%     Llama-3-70B-Instruct         & 0.543          & 0.544          & 0.544          & 0.542          & 0.981                 \\
%     Llama-2-70B-Instruct         & 0.548          & 0.553          & 0.549          & 0.534          & 0.973                 \\
%     Qwen2-72B-Instruct           & 0.633          & 0.635          & 0.633          & 0.631          & 0.978                 \\ \midrule
%     Prometheus2-7B               & 0.518          & 0.520          & 0.519          & 0.515          & \textbf{0.971}        \\
%     Prometheus2-bgb-8x7B         & 0.557          & 0.557          & 0.557          & 0.556          & 0.965                 \\
%     PandaLM-7B                   & \textbf{0.570} & \textbf{0.580} & \textbf{0.572} & \textbf{0.560} & 0.712                 \\ \midrule
%     Llama-3-8B-instruct          & 0.545          & 0.551          & 0.547          & 0.536          & 0.973                 \\
%     Llama-3-8B-instruct-finetune & 0.536          & 0.538          & 0.538          & 0.582          & 0.979                 \\
%     \rowcolor[HTML]{E1EAFF} 
%     ConfiLM                      & \textbf{0.623} & \textbf{0.625} & \textbf{0.622} & \textbf{0.621} & \textbf{0.982}        \\ \bottomrule
%     \end{tabular}
%     }
% \end{table*}

% \begin{table*}[htbp]
% % \setlength{\abovecaptionskip}{5pt}
% % \setlength{\belowcaptionskip}{20pt}
% \caption{Full result of uncertainty analysis of pairwise comparison on MT-Bench and PandaLM test set. The evaluation subjects are Llama2-7B-Instruct and Llama2-13B-Instruct. For each evaluation, we query the evaluator twice with the order swapped. 'Win / Lose / Tie' represents the average number of times Llama-2-7b-chat's response is better than, worse than, or equal to Llama-2-13b-chat's response. The PandaLM model is trained to output in a normal format (providing a preference between the two responses, followed by a concise rationale)~\citep{wangpandalm}. The Prometheus2 series models are trained to output in a Chain-of-Thoughts format (providing a concise rationale before indicating a preference between the two responses)~\citep{kim2024prometheus}.}
% \label{tab:appendix-full-pair-mtbench}
% \centering
% % \renewcommand{\arraystretch}{1.1}
% \resizebox{\linewidth}{!}{
% \begin{tabular}{@{}c|cccccc|cccccc|cccccc@{}}
% \toprule
%                             & \multicolumn{6}{c|}{Default}                                                                                                                                                                                                      & \multicolumn{6}{c|}{Chain-of-Thoughts}                                                                                                                                                                                           & \multicolumn{6}{c}{Self-generated Reference}                                                                                                                                                                                             \\ \cmidrule(l){2-19} 
%                             &                       &                        & \multicolumn{1}{c|}{}                             &                                                                                   & \multicolumn{2}{c|}{Response Confidence} &                       &                        & \multicolumn{1}{c|}{}                            &                                                                                   & \multicolumn{2}{c|}{Response Confidence} &                       &                        & \multicolumn{1}{c|}{}                             &                                                                                   & \multicolumn{2}{c}{Response Confidence}         \\
% \multirow{-3}{*}{Evaluator} & \multirow{-2}{*}{Win} & \multirow{-2}{*}{Lose} & \multicolumn{1}{c|}{\multirow{-2}{*}{Tie}}        & \multirow{-2}{*}{\begin{tabular}[c]{@{}c@{}}Evaluation\\ Confidence\end{tabular}} & A                   & B                  & \multirow{-2}{*}{Win} & \multirow{-2}{*}{Lose} & \multicolumn{1}{c|}{\multirow{-2}{*}{Tie}}       & \multirow{-2}{*}{\begin{tabular}[c]{@{}c@{}}Evaluation\\ Confidence\end{tabular}} & A                   & B                  & \multirow{-2}{*}{Win} & \multirow{-2}{*}{Lose} & \multicolumn{1}{c|}{\multirow{-2}{*}{Tie}}        & \multirow{-2}{*}{\begin{tabular}[c]{@{}c@{}}Evaluation\\ Confidence\end{tabular}} & A                            & B                \\ \midrule
% GPT-4o-2024-05-13           & 10.0                  & 16.5                   & \multicolumn{1}{c|}{53.5}                         & 0.699                                                                             & 0.950               & 0.949              & 25.0                  & 42.0                   & \multicolumn{1}{c|}{13.0}                        & 0.940                                                                             & 0.949               & 0.949              & 21.5                  & 31.0                   & \multicolumn{1}{c|}{27.5}                         & 0.799                                                                             & 0.949                        & 0.949            \\
% GPT-4o-mini-2024-07-18      & 27.0                  & 43.5                   & \multicolumn{1}{c|}{9.5}                          & 0.776                                                                             & 0.950               & 0.948              & 39.5                  & 37.0                   & \multicolumn{1}{c|}{3.5}                         & 0.990                                                                             & 0.950               & 0.948              & 32.5                  & 37.0                   & \multicolumn{1}{c|}{10.5}                         & 0.793                                                                             & 0.950                        & 0.948            \\
% GPT-3.5-Turbo               & 38.5                  & 35.0                   & \multicolumn{1}{c|}{6.5}                          & 0.848                                                                             & 0.945               & 0.949              & 37.0                  & 38.5                   & \multicolumn{1}{c|}{4.5}                         & 0.924                                                                             & 0.950               & 0.949              & 31.5                  & 33.5                   & \multicolumn{1}{c|}{15.0}                         & 0.8019                                                                            & 0.950                        & 0.949            \\
% Llama-3-70B-Instruct        & 39.5                  & 37.5                   & \multicolumn{1}{c|}{3.0}                          & 0.791                                                                             & 0.937               & 0.950              & 38.0                  & 37.5                   & \multicolumn{1}{c|}{4.5}                         & 0.999                                                                             & 0.947               & 0.950              & 34.5                  & 41.0                   & \multicolumn{1}{c|}{4.5}                          & 0.991                                                                             & 0.947                        & 0.950            \\
% Llama-2-70B-Instruct        & 33.0                  & 34.0                   & \multicolumn{1}{c|}{13.0}                         & 0.908                                                                             & 0.950               & 0.950              & 36.0                  & 37.0                   & \multicolumn{1}{c|}{7.0}                         & 0.991                                                                             & 0.950               & 0.950              & 27.5                  & 25.0                   & \multicolumn{1}{c|}{27.5}                         & 0.956                                                                             & {\color[HTML]{1F2329} 0.950} & 0.950            \\
% Qwen2-72B-Instruct          & 22.0                  & 29.0                   & \multicolumn{1}{c|}{29.0}                         & 0.762                                                                             & 0.939               & 0.949              & 30.5                  & 32.5                   & \multicolumn{1}{c|}{17}                          & 0.988                                                                             & 0.949               & 0.949              & 28.0                  & 27.5                   & \multicolumn{1}{c|}{24.5}                         & 0.944                                                                             & {\color[HTML]{1F2329} 0.949} & 0.949            \\ \midrule
% PandaLM-7B                  & 42.0                  & 28.5                   & \multicolumn{1}{c|}{9.5}                          & 0.617                                                                             & 0.949               & 0.939              & \textbackslash{}      & \textbackslash{}       & \multicolumn{1}{c|}{\textbackslash{}}            & \textbackslash{}                                                                  & \textbackslash{}    & \textbackslash{}   & \textbackslash{}      & \textbackslash{}       & \multicolumn{1}{c|}{\textbackslash{}}             & \textbackslash{}                                                                  & \textbackslash{}             & \textbackslash{} \\
% Prometheus2-7B              & \textbackslash{}      & \textbackslash{}       & \multicolumn{1}{c|}{\textbackslash{}}             & \textbackslash{}                                                                  & \textbackslash{}    & \textbackslash{}   & 37.5                  & 42.0                   & \multicolumn{1}{c|}{0.5}                         & 0.990                                                                             & 0.949               & 0.951              & \textbackslash{}      & \textbackslash{}       & \multicolumn{1}{c|}{\textbackslash{}}             & \textbackslash{}                                                                  & \textbackslash{}             & \textbackslash{} \\
% Prometheus2-bgb-8x7B        & \textbackslash{}      & \textbackslash{}       & \multicolumn{1}{c|}{\textbackslash{}}             & \textbackslash{}                                                                  & \textbackslash{}    & \textbackslash{}   & 31.5                  & 32.5                   & \multicolumn{1}{c|}{16.0}                        & 0.967                                                                             & 0.948               & 0.950              & \textbackslash{}      & \textbackslash{}       & \multicolumn{1}{c|}{\textbackslash{}}             & \textbackslash{}                                                                  & \textbackslash{}             & \textbackslash{} \\ \midrule
% \rowcolor[HTML]{E1EAFF} 
% Average                     & 35.2                  & 30.5                   & \multicolumn{1}{c|}{\cellcolor[HTML]{E1EAFF}14.3} & 0.707                                                                             & 0.947               & 0.944              & 34.4                  & 37.3                   & \multicolumn{1}{c|}{\cellcolor[HTML]{E1EAFF}8.3} & 0.977                                                                             & 0.949               & 0.950              & 29.3                  & 32.5                   & \multicolumn{1}{c|}{\cellcolor[HTML]{E1EAFF}18.2} & 0.881                                                                             & 0.949                        & 0.949            \\ \cmidrule(r){1-18}
% \end{tabular}
% }
% % \vspace{-0.5em}
% \end{table*}


% \section{Supplementary Experimental Results}
% \label{appendix:supp-result}

\begin{figure*}[h]
    % \vspace{-1em}
    \centering
    % \setlength{\abovecaptionskip}{0.5em}
    \includegraphics[width=0.65\textwidth]{figs/confidence_with_temperature.pdf}
    \caption{Generation confidence under varying sampling temperatures. We take the average probabilities of all generated tokens as generation confidence and investigate the performance of Gemma-1.1-7B-it~\citep{team2024gemma}, Internlm2.5-7B-chat~\citep{cai2024internlm2}, Qwen2-7B-Instruct~\citep{yang2024qwen2}, and Mistral-7B-Instruct-v0.3~\citep{jiang2023mistral} on the Olympic2024 test set. To ensure the validity, 
    % of the results
    we run three experiments with the same settings at each sampling temperature.}
    \label{fig:appendix-confidence-temperature}
    % \vspace{-1.5em}
\end{figure*}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{table}[]
    \centering
    \renewcommand{\arraystretch}{1.2}
    \caption{The evaluation confidence results with different quantification methods.}
    \label{tab:appendix-ways-of-quantification}
    \resizebox{\textwidth}{!}{
        \begin{tabular}{c|ccccc}
        \toprule
        Evaluator            & Logit-based & Verbalization-based & Consistency-5 & Consistency-10 & Consistency-20 \\ \midrule
        GPT-4o               & 0.699       & 0.764               & 0.751         & 0.725          & 0.696          \\
        GPT-4o-mini          & 0.776       & 0.725               & 0.771         & 0.736          & 0.735          \\
        GPT-3.5-Turbo        & 0.848       & 0.755               & 0.828         & 0.790          & 0.804          \\
        Llama-3-70B-Instruct & 0.791       & 0.808               & 0.846         & 0.833          & 0.778          \\
        Llama-2-70B-Instruct & 0.908       & 0.856               & 0.911         & 0.915          & 0.891          \\
        Qwen2-72B-Instruct   & 0.762       & 0.730               & 0.765         & 0.717          & 0.657          \\ \midrule
        \rowcolor[HTML]{E1EAFF} 
        Average              & 0.797       & 0.773               & 0.812         & 0.786          & 0.760          \\ \bottomrule
        \end{tabular}
    }
\end{table}


% \begin{table}[]
%     \centering
%     \caption{The evaluation confidence of 6 LLM-based evaluators.}
%     \resizebox{\textwidth}{!}{
%         \begin{tabular}{ccc}
%         \toprule
%         Evaluator            & Result from the original submission & Results with the 'Tie' option discarded \\ \hline
%         GPT-4o               & 0.699                               & 0.824                                   \\
%         GPT-4o-mini          & 0.776                               & 0.806                                   \\
%         GPT-3.5-Turbo        & 0.848                               & 0.816                                   \\
%         Llama-3-70B-Instruct & 0.791                               & 0.823                                   \\
%         Llama-2-70B-Instruct & 0.908                               & 0.920                                   \\
%         Qwen2-72B-Instruct   & 0.762                               & 0.850                                   \\ \bottomrule
%         \end{tabular}
%     }
% \end{table}

\begin{table}[]
    \centering
    \tiny
    \caption{Evaluation performance on Alpaca-94 and Olympic 2024.}
    \label{tab:appendix-alpaca-94}
    \resizebox{\textwidth}{!}{
        \begin{tabular}{c|ccc}
        \toprule
        Dataset         & ConfiLM-600 & Llama-3-8B-Instruct-finetune-600 & Llama-3-8B-Instruct \\ \midrule
        Alpaca-94        & 0.581       & \textbf{0.585}                            & 0.518               \\
        Olympic 2024     & \textbf{0.577}       & 0.535                            & 0.519               \\ \bottomrule
        \end{tabular}
    }
\end{table}


% \begin{table}[]
%     \centering
%     \tiny
%     \caption{Pearson correlation between response confidence and evaluation confidence.}
%     \resizebox{\textwidth}{!}{
%         \begin{tabular}{ccc}
%         \hline
%         Evaluator            & Default prompt & Adding response confidence to the prompt \\ \hline
%         GPT-4o               & -0.039         & 0.346                                   \\
%         GPT-4o-mini          & -0.167         & 0.140                                   \\
%         GPT-3.5-Turbo        & -0.027         & 0.241                                   \\
%         Llama-3-70B-Instruct & 0.135          & 0.345                                   \\
%         Llama-2-70B-Instruct & 0.244          & 0.412                                   \\
%         Qwen2-72B-Instruct   & 0.034          & 0.173                                   \\ \hline
%         \end{tabular}
%     }
% \end{table}

\begin{table}[]
    \centering
    \tiny
    \caption{The relation between evaluation confidence and evaluation accuracy on Olympic 2024.}
     \label{tab:appendix-relation}
    \resizebox{\textwidth}{!}{
        \begin{tabular}{c|ccccc}
        \toprule
        Evaluator            & {[}0.0, 0.2) & {[}0.2, 0.4) & {[}0.4, 0.6) & {[}0.6, 0.8) & {[}0.8, 1.0) \\ \midrule
        GPT-4o               & 0.000        & 0.250        & 0.333        & 0.625        & 0.684        \\
        GPT-4o-mini          & 0.000        & 0.333        & 0.222        & 0.625        & 0.721        \\
        GPT-3.5-Turbo        & 0.125        & 0.333        & 0.400        & 0.556        & 0.634        \\
        Llama-3-70B-Instruct & 0.000        & 0.000        & 0.200        & 0.364        & 0.680        \\
        Llama-2-70B-Instruct & 0.000        & 0.000        & 0.000        & 0.267        & 0.579        \\
        Qwen2-72B-Instruct   & 0.000        & 0.500        & 0.571        & 0.600        & 0.668        \\ \bottomrule
        \end{tabular}
    }
\end{table}



% \begin{table}[]
%     \centering
%     \caption{The evaluation performance under different combinations of learning rate and epoch.}
%     \resizebox{\textwidth}{!}{
%         \begin{tabular}{ccccccc}
%         \hline
%         Model               & 5e-5 + 3 epoch & 5e-5 + 5 epoch & 5e-5 + 6 epoch & 3e-5 + 3 epoch & 3e-5 + 5 epoch & 3e-5 + 6 epoch \\ \hline
%         ConfiLM             & 0.603          & 0.615          & 0.621          & 0.607          & 0.599          & 0.596          \\
%         Llama-3-8B-instruct-finetune & 0.537   & 0.560          & 0.562          & 0.556          & 0.573          & \textbf{0.582}          \\ \bottomrule
%         \end{tabular}
%     }
% \end{table}

% \begin{table}[]
%     \centering
%     \caption{The distribution of response confidence from the fine-tuning set}
%     \resizebox{\textwidth}{!}{
%         \begin{tabular}{ccccccccccc}
%         \hline
%                        & {[}0.0-0.1) & {[}0.1-0.2) & {[}0.2-0.3) & {[}0.3-0.4) & {[}0.4-0.5) & {[}0.5-0.6) & {[}0.6-0.7) & {[}0.7-0.8) & {[}0.8-0.9) & {[}0.9-1.0) \\ \hline
%         Percentage (\%) & 0.00        & 0.29        & 0.00        & 0.00        & 0.58        & 0.86        & 5.04        & 33.57       & 43.80       & 15.85       \\ \hline
%         \end{tabular}
%     }
% \end{table}

\begin{table}[]
    \centering
    \caption{The evaluation performance of ConfiLM with different fine-tuning formats.}
    \label{tab:appendix-Verbalized}
    % \resizebox{0.7\textwidth}{!}{
        % \tiny
        \begin{tabular}{@{}cc@{}}
        \toprule
            Settings              & F1 Score on Olympic 2024 \\ \midrule
            Numerized Confidence  & 0.505                    \\
            Verbalized Confidence & \textbf{0.621}                    \\ \bottomrule
        \end{tabular}
    % }
\end{table}

\end{document}
