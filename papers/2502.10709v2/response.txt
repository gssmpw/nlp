\section{Related Work}
\vspace{-0.5em}

With rapid development of LLMs, the accurate evaluation of their capabilities has become one of the key challenges in this field. Several LLM evaluation paradigms have been proposed in recent years __**Brown et al., "Language Models are Few-Shot Learners"**, which have coalesced around a few well-established methods, including benchmark-based evaluation, model-based evaluation, and human evaluation.

\textbf{Benchmark-based evaluations} involve using a set of standardized tests to quantitatively measure a model’s performance across different tasks. Examples include HellaSwag**__**Blinov et al., "HellaSwag: A Benchmark for Commonsense Reasoning", **_**HELM____Stietenhem et al., "HELM: A Human-Generated Dataset of Multi-Switch Multiple Choice Questions for LLMs", **_**MMLU****Li et al., "A Large-scale Multitask Machine Learning Dataset"**, for general knowledge and reasoning, or MATH ____Saxena et al., "MATH: A Benchmark for Mathematical Reasoning in NLP", __**ToolBench____Kumar et al., "ToolBench: A Platform for Evaluating the Robustness of LLMs on Real-World Tasks"**, for specific capabilities. The performance of LLMs is measured by their ability to correctly perform these tasks. However, these metrics often reflect models' performance in narrowly defined areas and risk inflated scores due to data contamination____.

\textbf{Human evaluations} involve human raters who assess LLM performance based on criteria such as fluency, coherence, and relevance. This approach can take the form of A/B testing**__**Huang et al., "A/B Testing in NLP: A Survey", **_**preference ranking____Duan et al., "Preference Ranking for Evaluating LLMs", **_**or scoring individual model outputs against predefined rubrics____. While human evaluations are often considered the gold standard for tasks where quantitative metrics fall short, they are resource-intensive in terms of time and cost. Moreover, they are constrained by subjectivity____ and reproducible issues____, limiting their scalability for large-scale assessments.

\textbf{Model-based evaluations} involve employing a powerful LLM as an auto-evaluator to assess the performance of the candidate model. This promising method serves as a cost-effective alternative to human evaluators____. However, concerns have been raised regarding the alignment, bias, and stability of model-based LLM evaluation. While researchers have made progress in exploring the alignment and bias of LLM evaluators****_**Li et al., "Exploring the Alignment and Bias of LLM Evaluators", **_**, understanding the stability of these evaluators remains an open question. A concurrent work____ proposes a novel framework to evaluate the proficiency of LLM evaluators through targeted perturbations. Different from this work, we focus on the role of uncertainty in LLM-based evaluators, which has yet to be systematically explored.

\textbf{Confidence Estimation for LLMs.} 
% \subsection{Confidence Elicitation for LLMs}
Model confidence refers to the degree of certainty a model holds regarding its generated responses____**_**Li et al., "Estimating Model Confidence in NLP Tasks", **_. Reliable confidence estimation for LLM is crucial for effective human-machine collaboration, as it provides valuable insights into the reliability of the model’s output, facilitates risk assessment____ and reduces hallucinations____. Research in this field includes (1) verbalization-based methods____**_**Tian et al., "Verbalization-Based Confidence Estimation for LLMs", **_**, which prompt LLMs to directly output calibrated confidence along with their responses; (2) consistency-based methods____**_**Xiong et al., "Consistency-Based Confidence Estimation for LLMs", **_**, which require LLMs to generate multiple responses for the same question and measure their consistency as a proxy for confidence; and (3) logit-based methods____**_**Huang et al., "Logit-Based Confidence Estimation for LLMs", **_**, which estimate confidence based on the model's internal states during response generation. Inspired by this line of work, we use token probabilities to represent the LLM's internal confidence. Previous work has considered the utilization of model confidence in natural language understanding____**_**Duan et al., "Model Confidence in Natural Language Understanding", **_, fact checking____**_**Kumar et al., "Fact Checking with Model Confidence", ** and hallucination detection____**_**Li et al., "Hallucination Detection with Model Confidence", **_. Differently, our work focuses on utilizing confidence within the evaluation process.

% A related concept is that of model calibration, which focuses on align- ing predictive probabilities (estimated confidence) to actual accuracy (Guo et al., 2017).
%researchers propose to evaluate the sentence uncertainty by the output languages directly
% researchers have explored prompting LLMs to output calibrated confidence alongside text (Tian et al., 2023) for determining LLMs’ re- liability.% linteaching yona2024can
%, consistency-based methods prompting LLMs to generate multiple responses to the same question and evaluating the self-consistency of those responses % Current calibra- tion methods for text generation rely on heuristics that consistency among multiple responses (Xiong et al., 2023) or the top-k responses facilitate cal- ibration (Tian et al., 2023).
% logit-based % huang2023look duan2024shifting malininuncertainty
%  In this paper, inspired by prior studies ____, we use token probabilities to represent the LLM's internal confidence.

% \subsection{Interpretability for Large Language Models (optional)}