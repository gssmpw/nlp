@inproceedings{kim2024prometheus,
  title={Prometheus: Inducing fine-grained evaluation capability in language models},
  author={Kim, Seungone and Shin, Jamin and Cho, Yejin and Jang, Joel and Longpre, Shayne and Lee, Hwaran and Yun, Sangdoo and Shin, Seongjin and Kim, Sungdong and Thorne, James and others},
  booktitle={The Twelfth International Conference on Learning Representations},
  year=2024
}

@article{kim2024prometheus2,
  title={Prometheus 2: An open source language model specialized in evaluating other language models},
  author={Kim, Seungone and Suk, Juyoung and Longpre, Shayne and Lin, Bill Yuchen and Shin, Jamin and Welleck, Sean and Neubig, Graham and Lee, Moontae and Lee, Kyungjae and Seo, Minjoon},
  journal={arXiv preprint arXiv:2405.01535},
  year={2024}
}

@article{kim2024biggen,
  title={The BiGGen Bench: A Principled Benchmark for Fine-grained Evaluation of Language Models with Language Models},
  author={Kim, Seungone and Suk, Juyoung and Cho, Ji Yong and Longpre, Shayne and Kim, Chaeeun and Yoon, Dongkeun and Son, Guijin and Cho, Yejin and Shafayat, Sheikh and Baek, Jinheon and others},
  journal={arXiv preprint arXiv:2406.05761},
  year={2024}
}

@inproceedings{wangpandalm,
  title={PandaLM: An Automatic Evaluation Benchmark for LLM Instruction Tuning Optimization},
  author={Wang, Yidong and Yu, Zhuohao and Yao, Wenjin and Zeng, Zhengran and Yang, Linyi and Wang, Cunxiang and Chen, Hao and Jiang, Chaoya and Xie, Rui and Wang, Jindong and others},
  booktitle={The Twelfth International Conference on Learning Representations},
  year={2024}
}

@article{achiam2023gpt,
  title={Gpt-4 technical report},
  author={Achiam, Josh and Adler, Steven and Agarwal, Sandhini and Ahmad, Lama and Akkaya, Ilge and Aleman, Florencia Leoni and Almeida, Diogo and Altenschmidt, Janko and Altman, Sam and Anadkat, Shyamal and others},
  journal={arXiv preprint arXiv:2303.08774},
  year={2023}
}

@article{touvron2023llama,
  title={Llama 2: Open foundation and fine-tuned chat models},
  author={Touvron, Hugo and Martin, Louis and Stone, Kevin and Albert, Peter and Almahairi, Amjad and Babaei, Yasmine and Bashlykov, Nikolay and Batra, Soumya and Bhargava, Prajjwal and Bhosale, Shruti and others},
  journal={arXiv preprint arXiv:2307.09288},
  year={2023}
}

@article{yang2024qwen2,
  title={Qwen2 technical report},
  author={Yang, An and Yang, Baosong and Hui, Binyuan and Zheng, Bo and Yu, Bowen and Zhou, Chang and Li, Chengpeng and Li, Chengyuan and Liu, Dayiheng and Huang, Fei and others},
  journal={arXiv preprint arXiv:2407.10671},
  year={2024}
}

@article{zheng2023judging,
  title={Judging llm-as-a-judge with mt-bench and chatbot arena},
  author={Zheng, Lianmin and Chiang, Wei-Lin and Sheng, Ying and Zhuang, Siyuan and Wu, Zhanghao and Zhuang, Yonghao and Lin, Zi and Li, Zhuohan and Li, Dacheng and Xing, Eric and others},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  pages={46595--46623},
  year={2023}
}

@article{dubey2024llama,
  title={The llama 3 herd of models},
  author={Dubey, Abhimanyu and Jauhri, Abhinav and Pandey, Abhinav and Kadian, Abhishek and Al-Dahle, Ahmad and Letman, Aiesha and Mathur, Akhil and Schelten, Alan and Yang, Amy and Fan, Angela and others},
  journal={arXiv preprint arXiv:2407.21783},
  year={2024}
}

@article{kumar2024confidence,
  title={Confidence Under the Hood: An Investigation into the Confidence-Probability Alignment in Large Language Models},
  author={Kumar, Abhishek and Morabito, Robert and Umbet, Sanzhar and Kabbara, Jad and Emami, Ali},
  journal={arXiv preprint arXiv:2405.16282},
  year={2024}
}

@inproceedings{zhou2023navigating,
  title={Navigating the Grey Area: How Expressions of Uncertainty and Overconfidence Affect Language Models},
  author={Zhou, Kaitlyn and Jurafsky, Dan and Hashimoto, Tatsunori B},
  booktitle={Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing},
  pages={5506--5524},
  year={2023}
}

@inproceedings{guptalanguage,
  title={Language Model Cascades: Token-Level Uncertainty And Beyond},
  author={Gupta, Neha and Narasimhan, Harikrishna and Jitkrittum, Wittawat and Rawat, Ankit Singh and Menon, Aditya Krishna and Kumar, Sanjiv},
  booktitle={The Twelfth International Conference on Learning Representations},
  year={2024}
}

@article{team2024gemma,
  title={Gemma 2: Improving open language models at a practical size},
  author={Team, Gemma and Riviere, Morgane and Pathak, Shreya and Sessa, Pier Giuseppe and Hardin, Cassidy and Bhupatiraju, Surya and Hussenot, L{\'e}onard and Mesnard, Thomas and Shahriari, Bobak and Ram{\'e}, Alexandre and others},
  journal={arXiv preprint arXiv:2408.00118},
  year={2024}
}

@misc{cai2024internlm2,
      title={InternLM2 Technical Report},
      author={Zheng Cai and Maosong Cao and Haojiong Chen and Kai Chen and Keyu Chen and Xin Chen and Xun Chen and Zehui Chen and Zhi Chen and Pei Chu and Xiaoyi Dong and Haodong Duan and Qi Fan and Zhaoye Fei and Yang Gao and Jiaye Ge and Chenya Gu and Yuzhe Gu and Tao Gui and Aijia Guo and Qipeng Guo and Conghui He and Yingfan Hu and Ting Huang and Tao Jiang and Penglong Jiao and Zhenjiang Jin and Zhikai Lei and Jiaxing Li and Jingwen Li and Linyang Li and Shuaibin Li and Wei Li and Yining Li and Hongwei Liu and Jiangning Liu and Jiawei Hong and Kaiwen Liu and Kuikun Liu and Xiaoran Liu and Chengqi Lv and Haijun Lv and Kai Lv and Li Ma and Runyuan Ma and Zerun Ma and Wenchang Ning and Linke Ouyang and Jiantao Qiu and Yuan Qu and Fukai Shang and Yunfan Shao and Demin Song and Zifan Song and Zhihao Sui and Peng Sun and Yu Sun and Huanze Tang and Bin Wang and Guoteng Wang and Jiaqi Wang and Jiayu Wang and Rui Wang and Yudong Wang and Ziyi Wang and Xingjian Wei and Qizhen Weng and Fan Wu and Yingtong Xiong and Chao Xu and Ruiliang Xu and Hang Yan and Yirong Yan and Xiaogui Yang and Haochen Ye and Huaiyuan Ying and Jia Yu and Jing Yu and Yuhang Zang and Chuyu Zhang and Li Zhang and Pan Zhang and Peng Zhang and Ruijie Zhang and Shuo Zhang and Songyang Zhang and Wenjian Zhang and Wenwei Zhang and Xingcheng Zhang and Xinyue Zhang and Hui Zhao and Qian Zhao and Xiaomeng Zhao and Fengzhe Zhou and Zaida Zhou and Jingming Zhuo and Yicheng Zou and Xipeng Qiu and Yu Qiao and Dahua Lin},
      year={2024},
      eprint={2403.17297},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@article{jiang2023mistral,
  title={Mistral 7B},
  author={Jiang, Albert Q and Sablayrolles, Alexandre and Mensch, Arthur and Bamford, Chris and Chaplot, Devendra Singh and Casas, Diego de las and Bressand, Florian and Lengyel, Gianna and Lample, Guillaume and Saulnier, Lucile and others},
  journal={arXiv preprint arXiv:2310.06825},
  year={2023}
}

@article{taori2023stanford,
  title={Stanford alpaca: an instruction-following llama model (2023)},
  author={Taori, Rohan and Gulrajani, Ishaan and Zhang, Tianyi and Dubois, Yann and Li, Xuechen and Guestrin, Carlos and Liang, Percy and Hashimoto, Tatsunori B},
  journal={URL https://github. com/tatsu-lab/stanford\_alpaca},
  volume={1},
  number={9},
  year={2023}
}

@inproceedings{wang2023self,
  title={Self-Instruct: Aligning Language Models with Self-Generated Instructions},
  author={Wang, Yizhong and Kordi, Yeganeh and Mishra, Swaroop and Liu, Alisa and Smith, Noah A and Khashabi, Daniel and Hajishirzi, Hannaneh},
  booktitle={The 61st Annual Meeting Of The Association For Computational Linguistics},
  year={2023}
}

@article{kojima2022large,
  title={Large language models are zero-shot reasoners},
  author={Kojima, Takeshi and Gu, Shixiang Shane and Reid, Machel and Matsuo, Yutaka and Iwasawa, Yusuke},
  journal={Advances in neural information processing systems},
  volume={35},
  pages={22199--22213},
  year={2022}
}

@article{koo2023benchmarking,
  title={Benchmarking cognitive biases in large language models as evaluators},
  author={Koo, Ryan and Lee, Minhwa and Raheja, Vipul and Park, Jong Inn and Kim, Zae Myung and Kang, Dongyeop},
  journal={arXiv preprint arXiv:2309.17012},
  year={2023}
}

@inproceedings{zengevaluating,
  title={Evaluating Large Language Models at Evaluating Instruction Following},
  author={Zeng, Zhiyuan and Yu, Jiatong and Gao, Tianyu and Meng, Yu and Goyal, Tanya and Chen, Danqi},
  booktitle={The Twelfth International Conference on Learning Representations},
  year=2024
}

@article{yu2024kieval,
  title={Kieval: A knowledge-grounded interactive evaluation framework for large language models},
  author={Yu, Zhuohao and Gao, Chang and Yao, Wenjin and Wang, Yidong and Ye, Wei and Wang, Jindong and Xie, Xing and Zhang, Yue and Zhang, Shikun},
  journal={arXiv preprint arXiv:2402.15043},
  year={2024}
}

@article{wang2023large,
  title={Large language models are not fair evaluators},
  author={Wang, Peiyi and Li, Lei and Chen, Liang and Cai, Zefan and Zhu, Dawei and Lin, Binghuai and Cao, Yunbo and Liu, Qi and Liu, Tianyu and Sui, Zhifang},
  journal={arXiv preprint arXiv:2305.17926},
  year={2023}
}

@article{jung2019earlier,
  title={Earlier Isn't Always Better: Sub-aspect Analysis on Corpus and System Biases in Summarization},
  author={Jung, Taehee and Kang, Dongyeop and Mentch, Lucas and Hovy, Eduard},
  journal={arXiv preprint arXiv:1908.11723},
  year={2019}
}

@article{wei2022chain,
  title={Chain-of-thought prompting elicits reasoning in large language models},
  author={Wei, Jason and Wang, Xuezhi and Schuurmans, Dale and Bosma, Maarten and Xia, Fei and Chi, Ed and Le, Quoc V and Zhou, Denny and others},
  journal={Advances in neural information processing systems},
  volume={35},
  pages={24824--24837},
  year={2022}
}

@article{dubois2024alpacafarm,
  title={Alpacafarm: A simulation framework for methods that learn from human feedback},
  author={Dubois, Yann and Li, Chen Xuechen and Taori, Rohan and Zhang, Tianyi and Gulrajani, Ishaan and Ba, Jimmy and Guestrin, Carlos and Liang, Percy S and Hashimoto, Tatsunori B},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2024}
}

@article{vu2024foundational,
  title={Foundational Autoraters: Taming Large Language Models for Better Automatic Evaluation},
  author={Vu, Tu and Krishna, Kalpesh and Alzubi, Salaheddin and Tar, Chris and Faruqui, Manaal and Sung, Yun-Hsuan},
  journal={arXiv preprint arXiv:2407.10817},
  year={2024}
}

@article{bengio2015scheduled,
  title={Scheduled sampling for sequence prediction with recurrent neural networks},
  author={Bengio, Samy and Vinyals, Oriol and Jaitly, Navdeep and Shazeer, Noam},
  journal={Advances in neural information processing systems},
  volume={28},
  year={2015}
}

@inproceedings{he2021exposure,
  title={Exposure Bias versus Self-Recovery: Are Distortions Really Incremental for Autoregressive Text Generation?},
  author={He, Tianxing and Zhang, Jingzhao and Zhou, Zhiming and Glass, James},
  booktitle={Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing},
  pages={5087--5102},
  year={2021}
}

@article{chang2024survey,
  title={A survey on evaluation of large language models},
  author={Chang, Yupeng and Wang, Xu and Wang, Jindong and Wu, Yuan and Yang, Linyi and Zhu, Kaijie and Chen, Hao and Yi, Xiaoyuan and Wang, Cunxiang and Wang, Yidong and others},
  journal={ACM Transactions on Intelligent Systems and Technology},
  volume={15},
  number={3},
  pages={1--45},
  year={2024},
  publisher={ACM New York, NY}
}

@article{zhu2023promptbench,
  title={Promptbench: Towards evaluating the robustness of large language models on adversarial prompts},
  author={Zhu, Kaijie and Wang, Jindong and Zhou, Jiaheng and Wang, Zichen and Chen, Hao and Wang, Yidong and Yang, Linyi and Ye, Wei and Zhang, Yue and Gong, Neil Zhenqiang and others},
  journal={arXiv preprint arXiv:2306.04528},
  year={2023}
}

@inproceedings{yang2023glue,
  title={GLUE-X: Evaluating Natural Language Understanding Models from an Out-of-Distribution Generalization Perspective},
  author={Yang, Linyi and Zhang, Shuibai and Qin, Libo and Li, Yafu and Wang, Yidong and Liu, Hanmeng and Wang, Jindong and Xie, Xing and Zhang, Yue},
  booktitle={Findings of the Association for Computational Linguistics: ACL 2023},
  pages={12731--12750},
  year={2023}
}

@article{xie2024human,
  title={Human Simulacra: Benchmarking the Personification of Large Language Models}, 
  author={Xie, Qiuejie and Feng, Qiming and Zhang, Tianqi and Li, Qingqiu and Yang, Linyi and Zhang, Yuejie and Feng, Rui and He, Liang and Gao, Shang and Zhang, Yue},
  journal={arXiv preprint arXiv:2402.18180},
  year={2024}
}

@inproceedings{hendrycks2measuring,
  title={Measuring Mathematical Problem Solving With the MATH Dataset},
  author={Hendrycks, Dan and Burns, Collin and Kadavath, Saurav and Arora, Akul and Basart, Steven and Tang, Eric and Song, Dawn and Steinhardt, Jacob},
  booktitle={Thirty-fifth Conference on Neural Information Processing Systems Datasets and Benchmarks Track (Round 2)},
  year=2021
}

@inproceedings{zellers2019hellaswag,
  title={HellaSwag: Can a Machine Really Finish Your Sentence?},
  author={Zellers, Rowan and Holtzman, Ari and Bisk, Yonatan and Farhadi, Ali and Choi, Yejin},
  booktitle={Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics},
  pages={4791--4800},
  year={2019}
}

@inproceedings{karpinska2021perils,
  title={The Perils of Using Mechanical Turk to Evaluate Open-Ended Text Generation},
  author={Karpinska, Marzena and Akoury, Nader and Iyyer, Mohit},
  booktitle={Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing},
  pages={1265--1285},
  year={2021}
}

@inproceedings{krishna2023longeval,
  title={LongEval: Guidelines for Human Evaluation of Faithfulness in Long-form Summarization},
  author={Krishna, Kalpesh and Bransom, Erin and Kuehl, Bailey and Iyyer, Mohit and Dasigi, Pradeep and Cohan, Arman and Lo, Kyle},
  booktitle={Proceedings of the 17th Conference of the European Chapter of the Association for Computational Linguistics},
  pages={1650--1669},
  year={2023}
}

@article{zhao2023survey,
  title={A survey of large language models},
  author={Zhao, Wayne Xin and Zhou, Kun and Li, Junyi and Tang, Tianyi and Wang, Xiaolei and Hou, Yupeng and Min, Yingqian and Zhang, Beichen and Zhang, Junjie and Dong, Zican and others},
  journal={arXiv preprint arXiv:2303.18223},
  year={2023}
}

@inproceedings{liu2024calibrating,
  title={Calibrating LLM-Based Evaluator},
  author={Liu, Yuxuan and Yang, Tianchi and Huang, Shaohan and Zhang, Zihan and Huang, Haizhen and Wei, Furu and Deng, Weiwei and Sun, Feng and Zhang, Qi},
  booktitle={Proceedings of the 2024 Joint International Conference on Computational Linguistics, Language Resources and Evaluation (LREC-COLING 2024)},
  pages={2638--2656},
  year={2024}
}

@inproceedings{hada2024large,
  title={Are Large Language Model-based Evaluators the Solution to Scaling Up Multilingual Evaluation?},
  author={Hada, Rishav and Gumma, Varun and Wynter, Adrian and Diddee, Harshita and Ahmed, Mohamed and Choudhury, Monojit and Bali, Kalika and Sitaram, Sunayana},
  booktitle={Findings of the Association for Computational Linguistics: EACL 2024},
  pages={1051--1070},
  year={2024}
}

@article{liu2024aligning,
  title={Aligning with human judgement: The role of pairwise preference in large language model evaluators},
  author={Liu, Yinhong and Zhou, Han and Guo, Zhijiang and Shareghi, Ehsan and Vulic, Ivan and Korhonen, Anna and Collier, Nigel},
  journal={arXiv preprint arXiv:2403.16950},
  year={2024}
}

@article{thakur2024judging,
  title={Judging the Judges: Evaluating Alignment and Vulnerabilities in LLMs-as-Judges},
  author={Thakur, Aman Singh and Choudhary, Kartik and Ramayapally, Venkat Srinik and Vaidyanathan, Sankaran and Hupkes, Dieuwke},
  journal={arXiv preprint arXiv:2406.12624},
  year={2024}
}

@article{anthropic2024claude,
  title={The claude 3 model family: Opus, sonnet, haiku},
  author={Anthropic, AI},
  journal={Claude-3 Model Card},
  year={2024}
}

@article{zhou2023don,
  title={Don't make your llm an evaluation benchmark cheater},
  author={Zhou, Kun and Zhu, Yutao and Chen, Zhipeng and Chen, Wentong and Zhao, Wayne Xin and Chen, Xu and Lin, Yankai and Wen, Ji-Rong and Han, Jiawei},
  journal={arXiv preprint arXiv:2311.01964},
  year={2023}
}

@inproceedings{shidetecting,
  title={Detecting Pretraining Data from Large Language Models},
  author={Shi, Weijia and Ajith, Anirudh and Xia, Mengzhou and Huang, Yangsibo and Liu, Daogao and Blevins, Terra and Chen, Danqi and Zettlemoyer, Luke},
  booktitle={The Twelfth International Conference on Learning Representations},
  year=2024
}

@article{tam2024literature,
  title={A Literature Review and Framework for Human Evaluation of Generative Large Language Models in Healthcare},
  author={Tam, Thomas Yu Chow and Sivarajkumar, Sonish and Kapoor, Sumit and Stolyar, Alisa V and Polanska, Katelyn and McCarthy, Karleigh R and Osterhoudt, Hunter and Wu, Xizhi and Visweswaran, Shyam and Fu, Sunyang and others},
  journal={arXiv preprint arXiv:2405.02559},
  year={2024}
}

@inproceedings{chiang2023can,
  title={Can Large Language Models Be an Alternative to Human Evaluations?},
  author={Chiang, Cheng-Han and Lee, Hung-Yi},
  booktitle={Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
  pages={15607--15631},
  year={2023}
}

@InProceedings{Li_2024_CVPR,
    author    = {Li, Bohao and Ge, Yuying and Ge, Yixiao and Wang, Guangzhi and Wang, Rui and Zhang, Ruimao and Shan, Ying},
    title     = {SEED-Bench: Benchmarking Multimodal Large Language Models},
    booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
    month     = {June},
    year      = {2024},
    pages     = {13299-13308}
}

@article{liang2022holistic,
  title={Holistic evaluation of language models},
  author={Liang, Percy and Bommasani, Rishi and Lee, Tony and Tsipras, Dimitris and Soylu, Dilara and Yasunaga, Michihiro and Zhang, Yian and Narayanan, Deepak and Wu, Yuhuai and Kumar, Ananya and others},
  journal={arXiv preprint arXiv:2211.09110},
  year={2022}
}

@article{hendrycks2020measuring,
  title={Measuring massive multitask language understanding},
  author={Hendrycks, Dan and Burns, Collin and Basart, Steven and Zou, Andy and Mazeika, Mantas and Song, Dawn and Steinhardt, Jacob},
  journal={arXiv preprint arXiv:2009.03300},
  year={2020}
}

@misc{xu2023tool,
      title={On the Tool Manipulation Capability of Open-source Large Language Models}, 
      author={Qiantong Xu and Fenglu Hong and Bo Li and Changran Hu and Zhengyu Chen and Jian Zhang},
      year={2023},
      eprint={2305.16504},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@inproceedings{orenproving,
  title={Proving Test Set Contamination in Black-Box Language Models},
  author={Oren, Yonatan and Meister, Nicole and Chatterji, Niladri S and Ladhak, Faisal and Hashimoto, Tatsunori},
  booktitle={The Twelfth International Conference on Learning Representations},
  year=2024
}

@inproceedings{novikova2017we,
  title={Why We Need New Evaluation Metrics for NLG},
  author={Novikova, Jekaterina and Du{\v{s}}ek, Ond{\v{r}}ej and Curry, Amanda Cercas and Rieser, Verena},
  booktitle={Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing},
  pages={2241--2252},
  year={2017}
}

@article{bai2022training,
  title={Training a helpful and harmless assistant with reinforcement learning from human feedback},
  author={Bai, Yuntao and Jones, Andy and Ndousse, Kamal and Askell, Amanda and Chen, Anna and DasSarma, Nova and Drain, Dawn and Fort, Stanislav and Ganguli, Deep and Henighan, Tom and others},
  journal={arXiv preprint arXiv:2204.05862},
  year={2022}
}

@inproceedings{tang2010overlapping,
  title={Overlapping experiment infrastructure: More, better, faster experimentation},
  author={Tang, Diane and Agarwal, Ashish and O'Brien, Deirdre and Meyer, Mike},
  booktitle={Proceedings of the 16th ACM SIGKDD international conference on Knowledge discovery and data mining},
  pages={17--26},
  year={2010}
}

@article{varshney2023stitch,
  title={A stitch in time saves nine: Detecting and mitigating hallucinations of llms by validating low-confidence generation},
  author={Varshney, Neeraj and Yao, Wenlin and Zhang, Hongming and Chen, Jianshu and Yu, Dong},
  journal={arXiv preprint arXiv:2307.03987},
  year={2023}
}

@article{linteaching,
  title={Teaching Models to Express Their Uncertainty in Words},
  author={Lin, Stephanie and Hilton, Jacob and Evans, Owain},
  journal={Transactions on Machine Learning Research},
  year=2022
}

@article{huang2023look,
  title={Look before you leap: An exploratory study of uncertainty measurement for large language models},
  author={Huang, Yuheng and Song, Jiayang and Wang, Zhijie and Zhao, Shengming and Chen, Huaming and Juefei-Xu, Felix and Ma, Lei},
  journal={arXiv preprint arXiv:2307.10236},
  year={2023}
}

@inproceedings{duan2024shifting,
  title={Shifting attention to relevance: Towards the predictive uncertainty quantification of free-form large language models},
  author={Duan, Jinhao and Cheng, Hao and Wang, Shiqi and Zavalny, Alex and Wang, Chenan and Xu, Renjing and Kailkhura, Bhavya and Xu, Kaidi},
  booktitle={Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
  pages={5050--5063},
  year={2024}
}

@inproceedings{malininuncertainty,
  title={Uncertainty Estimation in Autoregressive Structured Prediction},
  author={Malinin, Andrey and Gales, Mark},
  booktitle={International Conference on Learning Representations},
  year=2021
}

@inproceedings{tian2023just,
  title={Just Ask for Calibration: Strategies for Eliciting Calibrated Confidence Scores from Language Models Fine-Tuned with Human Feedback},
  author={Tian, Katherine and Mitchell, Eric and Zhou, Allan and Sharma, Archit and Rafailov, Rafael and Yao, Huaxiu and Finn, Chelsea and Manning, Christopher D},
  booktitle={The 2023 Conference on Empirical Methods in Natural Language Processing},
  year=2023 
}

@inproceedings{xiongcan,
  title={Can LLMs Express Their Uncertainty? An Empirical Evaluation of Confidence Elicitation in LLMs},
  author={Xiong, Miao and Hu, Zhiyuan and Lu, Xinyang and LI, YIFEI and Fu, Jie and He, Junxian and Hooi, Bryan},
  booktitle={The Twelfth International Conference on Learning Representations},
  year=2023 
}

@article{yona2024can,
  title={Can Large Language Models Faithfully Express Their Intrinsic Uncertainty in Words?},
  author={Yona, Gal and Aharoni, Roee and Geva, Mor},
  journal={arXiv preprint arXiv:2405.16908},
  year={2024}
}

@inproceedings{geng2024survey,
  title={A Survey of Confidence Estimation and Calibration in Large Language Models},
  author={Geng, Jiahui and Cai, Fengyu and Wang, Yuxia and Koeppl, Heinz and Nakov, Preslav and Gurevych, Iryna},
  booktitle={Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers)},
  pages={6577--6595},
  year={2024}
}

@inproceedings{NEURIPS2020_1457c0d6,
 author = {Brown, Tom and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared D and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and Agarwal, Sandhini and Herbert-Voss, Ariel and Krueger, Gretchen and Henighan, Tom and Child, Rewon and Ramesh, Aditya and Ziegler, Daniel and Wu, Jeffrey and Winter, Clemens and Hesse, Chris and Chen, Mark and Sigler, Eric and Litwin, Mateusz and Gray, Scott and Chess, Benjamin and Clark, Jack and Berner, Christopher and McCandlish, Sam and Radford, Alec and Sutskever, Ilya and Amodei, Dario},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {H. Larochelle and M. Ranzato and R. Hadsell and M.F. Balcan and H. Lin},
 pages = {1877--1901},
 publisher = {Curran Associates, Inc.},
 title = {Language Models are Few-Shot Learners},
 url = {https://proceedings.neurips.cc/paper_files/paper/2020/file/1457c0d6bfcb4967418bfb8ac142f64a-Paper.pdf},
 volume = {33},
 year = {2020}
}

@inproceedings{pan2023context,
  title={What In-Context Learning" Learns" In-Context: Disentangling Task Recognition and Task Learning},
  author={Pan, Jane and Gao, Tianyu and Chen, Howard and Chen, Danqi},
  booktitle={The 61st Annual Meeting Of The Association For Computational Linguistics},
  year={2023}
}

@inproceedings{yangsupervised,
  title={Supervised Knowledge Makes Large Language Models Better In-context Learners},
  author={Yang, Linyi and Zhang, Shuibai and Yu, Zhuohao and Bao, Guangsheng and Wang, Yidong and Wang, Jindong and Xu, Ruochen and Ye, Wei and Xie, Xing and Chen, Weizhu and others},
  booktitle={The Twelfth International Conference on Learning Representations},
  year=2024
}

@article{salewski2024context,
  title={In-Context Impersonation Reveals Large Language Models' Strengths and Biases},
  author={Salewski, Leonard and Alaniz, Stephan and Rio-Torto, Isabel and Schulz, Eric and Akata, Zeynep},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2024}
}

@article{raina2024llm,
  title={Is LLM-as-a-Judge Robust? Investigating Universal Adversarial Attacks on Zero-shot LLM Assessment},
  author={Raina, Vyas and Liusie, Adian and Gales, Mark},
  journal={arXiv preprint arXiv:2402.14016},
  year={2024}
}

@article{liu2023goat,
  title={Goat: Fine-tuned llama outperforms gpt-4 on arithmetic tasks},
  author={Liu, Tiedong and Low, Bryan Kian Hsiang},
  journal={arXiv preprint arXiv:2305.14201},
  year={2023}
}

@article{loshchilov2017decoupled,
  title={Decoupled weight decay regularization},
  author={Loshchilov, I},
  journal={arXiv preprint arXiv:1711.05101},
  year={2017}
}

@misc{li2023alpacaeval,
  title={Alpacaeval: An automatic evaluator of instruction-following models},
  author={Li, Xuechen and Zhang, Tianyi and Dubois, Yann and Taori, Rohan and Gulrajani, Ishaan and Guestrin, Carlos and Liang, Percy and Hashimoto, Tatsunori B},
  year={2023}
}

@inproceedings{liu2023g,
  title={G-Eval: NLG Evaluation using Gpt-4 with Better Human Alignment},
  author={Liu, Yang and Iter, Dan and Xu, Yichong and Wang, Shuohang and Xu, Ruochen and Zhu, Chenguang},
  booktitle={Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing},
  pages={2511--2522},
  year={2023}
}

@article{geng2024multimodal,
  title={Multimodal Large Language Models to Support Real-World Fact-Checking},
  author={Geng, Jiahui and Kementchedjhieva, Yova and Nakov, Preslav and Gurevych, Iryna},
  journal={arXiv preprint arXiv:2403.03627},
  year={2024}
}

@inproceedings{liu2024good,
  title={How Good Are LLMs at Out-of-Distribution Detection?},
  author={Liu, Bo and Zhan, Li-Ming and Lu, Zexin and Feng, Yujie and Xue, Lei and Wu, Xiao-Ming},
  booktitle={Proceedings of the 2024 Joint International Conference on Computational Linguistics, Language Resources and Evaluation (LREC-COLING 2024)},
  pages={8211--8222},
  year={2024}
}

@article{wang2024survey,
  title={A survey on large language model based autonomous agents},
  author={Wang, Lei and Ma, Chen and Feng, Xueyang and Zhang, Zeyu and Yang, Hao and Zhang, Jingsen and Chen, Zhiyuan and Tang, Jiakai and Chen, Xu and Lin, Yankai and others},
  journal={Frontiers of Computer Science},
  volume={18},
  number={6},
  pages={186345},
  year={2024},
  publisher={Springer}
}

@article{gal2016uncertainty,
  title={Uncertainty in deep learning},
  author={Gal, Yarin and others},
  year={2016},
  publisher={phd thesis, University of Cambridge}
}

@article{farquhar2024detecting,
  title={Detecting hallucinations in large language models using semantic entropy},
  author={Farquhar, Sebastian and Kossen, Jannik and Kuhn, Lorenz and Gal, Yarin},
  journal={Nature},
  volume={630},
  number={8017},
  pages={625--630},
  year={2024},
  publisher={Nature Publishing Group UK London}
}

@article{yu2024freeeval,
  title={FreeEval: A Modular Framework for Trustworthy and Efficient Evaluation of Large Language Models},
  author={Yu, Zhuohao and Gao, Chang and Yao, Wenjin and Wang, Yidong and Zeng, Zhengran and Ye, Wei and Wang, Jindong and Zhang, Yue and Zhang, Shikun},
  journal={arXiv preprint arXiv:2404.06003},
  year={2024}
}

@misc{chiang2024chatbot,
    title={Chatbot Arena: An Open Platform for Evaluating LLMs by Human Preference},
    author={Wei-Lin Chiang and Lianmin Zheng and Ying Sheng and Anastasios Nikolas Angelopoulos and Tianle Li and Dacheng Li and Hao Zhang and Banghua Zhu and Michael Jordan and Joseph E. Gonzalez and Ion Stoica},
    year={2024},
    eprint={2403.04132},
    archivePrefix={arXiv},
    primaryClass={cs.AI}
}

@article{doddapaneni2024finding,
  title={Finding Blind Spots in Evaluator LLMs with Interpretable Checklists},
  author={Doddapaneni, Sumanth and Khan, Mohammed Safi Ur Rahman and Verma, Sshubam and Khapra, Mitesh M},
  journal={arXiv preprint arXiv:2406.13439},
  year={2024}
}

@inproceedings{zhoubatch,
  title={Batch Calibration: Rethinking Calibration for In-Context Learning and Prompt Engineering},
  author={Zhou, Han and Wan, Xingchen and Proleev, Lev and Mincu, Diana and Chen, Jilin and Heller, Katherine A and Roy, Subhrajit},
  booktitle={The Twelfth International Conference on Learning Representations},
    year={2024}
}

@inproceedings{zhou2023survival,
  title={Survival of the Most Influential Prompts: Efficient Black-Box Prompt Search via Clustering and Pruning},
  author={Zhou, Han and Wan, Xingchen and Vuli{\'c}, Ivan and Korhonen, Anna},
  booktitle={The 2023 Conference on Empirical Methods in Natural Language Processing},
  year={2023}
}

@article{zhou2024fairer,
  title={Fairer Preferences Elicit Improved Human-Aligned Large Language Model Judgments},
  author={Zhou, Han and Wan, Xingchen and Liu, Yinhong and Collier, Nigel and Vuli{\'c}, Ivan and Korhonen, Anna},
  journal={arXiv preprint arXiv:2406.11370},
  year={2024}
}

@inproceedings{chen2023close,
  title={A Close Look into the Calibration of Pre-trained Language Models},
  author={Chen, Yangyi and Yuan, Lifan and Cui, Ganqu and Liu, Zhiyuan and Ji, Heng},
  booktitle={Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
  pages={1343--1367},
  year={2023}
}
