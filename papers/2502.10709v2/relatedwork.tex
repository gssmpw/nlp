\section{Related Work}
\vspace{-0.5em}

With rapid development of LLMs, the accurate evaluation of their capabilities has become one of the key challenges in this field. Several LLM evaluation paradigms have been proposed in recent years \citep{chang2024survey}, which have coalesced around a few well-established methods, including benchmark-based evaluation, model-based evaluation, and human evaluation.

\textbf{Benchmark-based evaluations} involve using a set of standardized tests to quantitatively measure a model’s performance across different tasks. Examples include HellaSwag~\citep{zellers2019hellaswag}, HELM~\citep{liang2022holistic} and MMLU~\citep{hendrycks2020measuring} for general knowledge and reasoning, or MATH~\citep{hendrycks2measuring} and ToolBench~\citep{xu2023tool} for specific capabilities. The performance of LLMs is measured by their ability to correctly perform these tasks. However, these metrics often reflect models' performance in narrowly defined areas and risk inflated scores due to data contamination~\citep{orenproving}.

\textbf{Human evaluations} involve human raters who assess LLM performance based on criteria such as fluency, coherence, and relevance. This approach can take the form of A/B testing~\citep{tang2010overlapping}, preference ranking~\citep{bai2022training}, or scoring individual model outputs against predefined rubrics~\citep{novikova2017we}. While human evaluations are often considered the gold standard for tasks where quantitative metrics fall short, they are resource-intensive in terms of time and cost. Moreover, they are constrained by subjectivity~\citep{krishna2023longeval} and reproducible issues~\citep{karpinska2021perils}, limiting their scalability for large-scale assessments.

\textbf{Model-based evaluations} involve employing a powerful LLM as an auto-evaluator to assess the performance of the candidate model. This promising method serves as a cost-effective alternative to human evaluators~\citep{zheng2023judging, wangpandalm, yu2024kieval, yu2024freeeval}. However, concerns have been raised regarding the alignment, bias, and stability of model-based LLM evaluation. While researchers have made progress in exploring the alignment and bias of LLM evaluators~\citep{liu2024calibrating,wang2023large}, understanding the stability of these evaluators remains an open question. A concurrent work~\citep{doddapaneni2024finding} proposes a novel framework to evaluate the proficiency of LLM evaluators through targeted perturbations. Different from this work, we focus on the role of uncertainty in LLM-based evaluators, which has yet to be systematically explored.
% Our work addresses this gap by focusing on the role of model confidence, which has yet to be systematically explored.
%In this paper, we conduct an in-depth exploration of the uncertainty in model-based LLM evaluation.

\textbf{Confidence Estimation for LLMs.} 
% \subsection{Confidence Elicitation for LLMs}
Model confidence refers to the degree of certainty a model holds regarding its generated responses~\citep{gal2016uncertainty}. Reliable confidence estimation for LLM is crucial for effective human-machine collaboration, as it provides valuable insights into the reliability of the model’s output, facilitates risk assessment~\citep{geng2024survey}, and reduces hallucinations~\citep{varshney2023stitch}. Research in this field includes (1) verbalization-based methods~\citep{linteaching, yona2024can}, which prompt LLMs to directly output calibrated confidence along with their responses; (2) consistency-based methods~\citep{tian2023just, xiongcan}, which require LLMs to generate multiple responses for the same question and measure their consistency as a proxy for confidence; and (3) logit-based methods~\citep{duan2024shifting, malininuncertainty, kumar2024confidence}, which estimate confidence based on the model's internal states during response generation. Inspired by this line of work, we use token probabilities to represent the LLM's internal confidence. Previous work has considered the utilization of model confidence in natural language understanding~\citep{yangsupervised}, fact checking~\citep{geng2024multimodal} and hallucination detection~\citep{varshney2023stitch, farquhar2024detecting}. Differently, our work focuses on utilizing confidence within the evaluation process.

% A related concept is that of model calibration, which focuses on align- ing predictive probabilities (estimated confidence) to actual accuracy (Guo et al., 2017).
%researchers propose to evaluate the sentence uncertainty by the output languages directly
% researchers have explored prompting LLMs to output calibrated confidence alongside text (Tian et al., 2023) for determining LLMs’ re- liability.% linteaching yona2024can
%, consistency-based methods prompting LLMs to generate multiple responses to the same question and evaluating the self-consistency of those responses % Current calibra- tion methods for text generation rely on heuristics that consistency among multiple responses (Xiong et al., 2023) or the top-k responses facilitate cal- ibration (Tian et al., 2023).
% logit-based % huang2023look duan2024shifting malininuncertainty
%  In this paper, inspired by prior studies \citep{kumar2024confidence, zhou2023navigating, guptalanguage}, we use token probabilities to represent the LLM's internal confidence.

% \subsection{Interpretability for Large Language Models (optional)}