We identify that for a given reward model, there exists a specific threshold beyond which the proxy reward becomes both meaningless and inaccurate. Based on this observation, we establish three fundamental principles for designing reward shaping methods.

In alignment with these principles, we propose an effective shaping method, Preference As Reward (PAR). Through extensive experimentation with various mitigation approaches, our results demonstrate that PAR not only outperforms other baseline methods by the end of one training epoch but also maintains a high winrate after two epochs of training. Notably, PAR is also data-efficient, requiring only a single reference reward to achieve strong performance.