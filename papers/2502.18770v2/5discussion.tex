Reward shaping is not applicable to DPO~\cite{DPO}, as it does not require a reward model during training. We also explore online DPO, which employs the policy model to generate two responses, and the reward model selects the response with the higher reward as the chosen response and the lower reward as the rejected response. However, since most reward shaping techniques are monotonic, they do not alter the binary preference and therefore, they do not influence the training procedure of online DPO.

For GRPO~\cite{shao2024deepseekmathpushinglimitsmathematical}, we argue that its advantage calculation inherently normalizes the proxy reward, making linear transformations (e.g., Minmax and mean\_std) ineffective. However, our non-linear PAR demonstrates slightly better performance than Vanilla GRPO in later stages (see Appendix~\ref{section:rsnotavail}).
An important observation is that GRPO does not exhibit the reward hacking problem during training, primarily because its advantage calculation effectively normalizes the rewards. Although the win rate decreases in the later stages, the proxy rewards also decrease proportionally, maintaining alignment between the optimization objective and the desired outcomes.