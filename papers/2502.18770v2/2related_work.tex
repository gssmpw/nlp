% Reward hacking is a well-known issue in reinforcement learning, affecting both traditional RL and RLHF in LLMs~\cite{weng2024rewardhack}.
\subsection{Reward Hacking in Traditional RL}  
Reward hacking arises when an RL agent exploits flaws or ambiguities in the reward function to achieve high rewards without performing the intended task~\cite{weng2024rewardhack}. This aligns with Goodhart’s Law: \emph{When a measure becomes a target, it ceases to be a good measure.} For example: 
A bicycle agent rewarded for not falling and moving toward a goal (but not penalized for moving away) learns to circle the goal indefinitely~\cite{Randlv1998LearningTD}.  
A walking agent in the DMControl suite, rewarded for matching a target speed, learns to walk unnaturally using only one leg~\cite{lee2021pebblefeedbackefficientinteractivereinforcement}.  
An RL agent allowed to modify its body grows excessively long legs to fall forward and reach the goal~\cite{Ha2018designrl}.  
In the Elevator Action ALE game, the agent repeatedly kills the first enemy on the first floor to accumulate small rewards~\cite{toromanoff2019deepreinforcementlearningreally}.  
% A robot trained to stay on track learns to reverse along straight paths by alternating left and right turns instead of following curves~\cite{Vamplew2004}.

\citet{amodei2016concrete} propose several potential mitigation strategies to address reward hacking, including
\emph{(1) Adversarial Reward Functions}: Treating the reward function as an adaptive agent capable of responding to new strategies where the model achieves high rewards but receives low human ratings.
\emph{(2) Model Lookahead}: Assigning rewards based on anticipated future states; for example, penalizing the agent with negative rewards if it attempts to modify the reward function~\cite{everitt2016selfmodificationpolicyutilityfunction}.
\emph{(3) Adversarial Blinding}: Restricting the model’s access to specific variables to prevent it from learning information that could facilitate reward hacking~\cite{ajakan2015domainadversarialneuralnetworks}.
\emph{(4) Careful Engineering}: Designing systems to avoid certain types of reward hacking by isolating the agent’s actions from its reward signals, such as through sandboxing techniques~\cite{The_AGI_Containment_Problem}.
\emph{(5) Trip Wires}: Deliberately introducing vulnerabilities into the system and setting up monitoring mechanisms to detect and alert when reward hacking occurs.

\subsection{Reward Hacking in RLHF of LLMs}  
Reward hacking in RLHF for large language models has been extensively studied. \citet{gao2023scaling} systematically investigate the scaling laws of reward hacking in small models, while \citet{wen2024languagemodelslearnmislead} demonstrate that language models can learn to mislead humans through RLHF. Beyond exploiting the training process, reward hacking can also target evaluators. Although using LLMs as judges is a natural choice given their increasing capabilities, this approach is imperfect and can introduce biases. For instance, LLMs may favor their own responses when evaluating outputs from different model families~\cite{liu2024llmsnarcissisticevaluatorsego} or exhibit positional bias when assessing responses in sequence~\cite{wang2023largelanguagemodelsfair}.  

To mitigate reward hacking, several methods have been proposed. Reward ensemble techniques have shown promise in addressing this issue~\cite{Eisenstein2023HelpingOH, Rame2024WARMOT, ahmed2024scalableensemblingmitigatingreward, coste2023reward, zhang2024improvingreinforcementlearninghuman}, and shaping methods have also proven straightforward and effective~\cite{yang2024regularizinghiddenstatesenables, jinnai2024regularizedbestofnsamplingmitigate}. \citet{miao2024informmitigatingrewardhacking} introduce an information bottleneck to filter irrelevant noise, while \citet{moskovitz2023confrontingrewardmodeloveroptimization} employ constrained RLHF to prevent reward over-optimization. \citet{Chen2024ODINDR} propose the ODIN method, which uses a linear layer to separately output quality and length rewards, reducing their correlation through an orthogonal loss function. Similarly,
~\citet{sun2023salmon} train instructable reward models to give a more comprehensive reward signal from multiple objectives. \citet{Dai2023SafeRS} constrain reward magnitudes using regularization terms. ~\citet{liu2024rrmrobustrewardmodel} curate diverse pairwise training data. Additionally, post-processing techniques have been explored, such as the log-sigmoid centering transformation introduced by \citet{Wang2024TransformingAC}.  

