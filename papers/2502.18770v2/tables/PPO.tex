\begin{figure*}[htbp]
    \centering
\begin{algorithm}[H]
\caption{PPO}
\label{alg:ppo}
\begin{algorithmic}[1]
\Require sft model $\pi_{\text{sft}}$, reward model $r_{\phi}$, prompt set $\mathcal{D}$.
\Ensure 
    Aligned model $\pi_{\theta^*}$\\
Initialize policy model $\pi_\theta\leftarrow\pi_{\text{sft}}$\\
Initialize reference model $\pi_{\text{ref}}\leftarrow\pi_{\text{sft}}$\\
Initialize critic model $V_\alpha\leftarrow r_{\phi}$
\For{$x\in\mathcal{D}$}
    \State ppo\_batch = build\_ppo\_batch($x,\pi_\theta,\pi_{\text{ref}},V_{\alpha},r_{\phi}$)
    \State ppo\_batch = buffer.substitute(ppo\_batch) \Comment{sample a ppo\_batch from replay buffer and save current ppo\_batch into the buffer}
    \State $\mathcal{L}_\text{ppo}(\theta),\mathcal{L}_\text{critic}(\alpha)$ = calculate\_loss(ppo\_batch, $\pi_\theta$, $V_\alpha$) 
    \State $\theta\leftarrow\theta-\mathsf{plr}*\nabla_{\theta}\mathcal{L}_\text{ppo}(\theta)$ \Comment{update policy model via gradient descent, $\mathsf{plr}$ is policy learning rate}
    \State $\alpha\leftarrow\alpha-\mathsf{clr}*\nabla_{\alpha}\mathcal{L}_\text{critic}(\alpha)$ \Comment{$\mathsf{clr}$ is critic learning rate}
\EndFor\\
\Return $\pi_{\theta^*}$ 
\end{algorithmic}
\end{algorithm}
%%%%%%%%%%%%%%%%%
\begin{algorithm}[H]
\caption{build\_ppo\_batch}
\label{alg:build_ppo_batch}
\begin{algorithmic}[1]
\Require prompt $x$, four models $\pi_\theta,\pi_{\text{ref}},V_{\alpha},r_{\phi}$.
\Ensure ppo\_batch: A dictionary

\State Initialize ppo\_batch = {}
\State sample $y\sim\pi_\theta(.|x)$
\State sample $y_{\text{ref}}^{1,\ldots,M}\sim \pi_{\text{ref}}(.|x)$  \Comment{optional}
\State $r=r_\phi(x,y)$
\State $r_{\text{ref}}^{1,\ldots,M}=r_{\phi}(x, y_{\text{ref}}^{1,\ldots,M})$ \Comment{optional}
\State $r_{\text{RL}}=\text{reward\_reshape}(r, r_{\text{ref}}^{1,\ldots,M}, \text{len}(y),\text{mode}=\mathsf{PAR})$
\State Now we split (x,y) into $(s_t,a_t)_{t=0}^T$
\State $\text{KL\_penalty} = \log\pi_{\theta}(a_t|s_t)-\log\pi_{\text{ref}}(a_t|s_t)$
\State construct per-token rewards $r_{1,\ldots,T}$ from $r_{\text{RL}}$ and KL\_penalty
\State $V_t = V_\alpha(s_t)$
\State Compute GAE $\hat{A}_t$ and Return $G_t$ from $V_t$ and $r_t$.
\State ppo\_batch = ($\log\pi_{\theta}(a_t|s_t)$, $G_t$, $\hat{A}_t$, $V_t$, $s_t$, $a_t$)
\\
\Return $\text{ppo\_batch}$ 
\end{algorithmic}
\end{algorithm}

%%%%%%%%%%%%%%%%%%
\begin{algorithm}[H]
\caption{Buffer.substitute}
\label{alg:buffer}
\begin{algorithmic}[1]
\Require ppo\_batch.
\Ensure ppo\_batch: A dictionary

\State Global List pool = []
\State Global buffer\_size = 4 \\
IF len(pool)<buffer\_size:
\State \ \ \ \ pool.append(ppo\_batch) \\
\ \ \ \ \Return None \\
ELSE:
\State\ \ \ \ selected\_batch = random.choice(pool) 
\State\ \ \ \ pool.pop(selected\_batch) 
\State\ \ \ \ pool.append(ppo\_batch) \\
\ \ \ \ \Return selected\_batch 
\end{algorithmic}
\end{algorithm}
\end{figure*}
%%%%%%%%%%%%%%%%%%%

\begin{figure*}[htbp]
    \centering
\begin{algorithm}[H]
\caption{reward\_reshape}
\label{alg:reward_reshape}
\begin{algorithmic}[1]
\Require policy reward $r$, reference reward $r_{\text{ref}}^{1,\ldots,M}$, response length $l$, reshape mode $\mathsf{mode}$.
\Ensure RL reward 
\State IF $l>300$:
\State \ \ \ $r=r-0.01*(l-300)$ \Comment{penalize long response}
\State IF mode==$\mathsf{meanstd}$: 
\State\ \ \ \ $r_{\text{RL}} = \frac{r-\mu}{s}$ \Comment{$\mu,s$ are running mean and running standard variance respectively.}
\State IF mode==$\mathsf{reward\_clip}$:
\State\ \ \ \  ...
\State IF mode==$\mathsf{PAR}$:
\State \ \ \ \ $r_\text{RL}$ = $\frac{1}{M}\sum_{m=1}^M\sigma(r-r_{\text{ref}}^{m})$ 

\end{algorithmic}
\end{algorithm}
%%%%%%%%%%%%%%%%%%%
\begin{algorithm}[H]
\caption{calculate\_loss}
\label{alg:policy_loss}
\begin{algorithmic}[1]
\Require ppo\_batch, policy model $\pi_\theta$, critic model $V_\alpha$.
\Ensure policy loss $\mathcal{L}_\text{ppo}(\theta)$, critic loss $\mathcal{L}_\text{critic}(\alpha)$
\State ($\log\pi_{\theta_\text{old}}(a_t|s_t)$, $G_t$, $\hat{A}_t$, $V_t$, $s_t$, $a_t$) = ppo\_batch \Comment{Extract elements from ppo\_batch}
\State $\mathcal{L}_\text{ppo}(\theta) = \hat{\mathbb{E}}_t \left[ \min \left( \frac{\pi_\theta(a_t|s_t)}{\pi_{\theta_{\text{old}}}(a_t|s_t)} \hat{A}_t, \text{clip} \left( \frac{\pi_\theta(a_t|s_t)}{\pi_{\theta_{\text{old}}}(a_t|s_t)}, 1 - \epsilon, 1 + \epsilon \right) \hat{A}_t \right) \right]$
\State $\mathcal{L}_\text{critic}(\alpha)=\hat{\mathbb{E}}_t\left[\max\left(||V_{\alpha}(s_t)-G_t||_2^2, ||\text{clip}(V_\alpha(s_t),V_t-\delta,V_t+\delta)-G_t||_2^2\right)\right]$ \Comment{Critic clip trick}\\

\Return $\mathcal{L}_\text{ppo}(\theta), \mathcal{L}_\text{critic}(\phi)$
\end{algorithmic}
\end{algorithm}


\begin{algorithm}[H]
\caption{Online DPO}
\label{alg:dpo}
\begin{algorithmic}[1]
\Require sft model $\pi_{\text{sft}}$, reward model $r_{\phi}$, prompt set $\mathcal{D}$.
\Ensure 
    Aligned model $\pi_{\theta^*}$\\
Initialize policy model $\pi_\theta\leftarrow\pi_{\text{sft}}$\\
Initialize reference model $\pi_{\text{ref}}\leftarrow\pi_{\text{sft}}$

\For{$x\in\mathcal{D}$}
    \State Sample $y_1,y_2 \sim \pi_\theta(.|x)$ 
    \State Calculate rewards $r_1=r_\phi(x,y_1), r_2=r_\phi(x,y_2)$\\
    \ \ \ \ \ \ IF $r_1>r_2$:
    \State \ \ \ $y_w=y_1,y_l=y_2$\\
    \ \ \ \ \ \ ELSE:
    \State \ \ \ $y_w=y_2,y_l=y_1$
    \State $\mathcal{L}_{\text{DPO}}(\theta) = -\left[ \log \sigma\left( \beta \left( \log \frac{\pi_\theta(y_w | x)}{\pi_{\text{ref}}(y_w | x)} - \log \frac{\pi_\theta(y_l | x)}{\pi_{\text{ref}}(y_l | x)} \right) \right) \right]$
    \State $\theta\leftarrow\theta-lr*\nabla_{\theta}\mathcal{L}_{\text{DPO}}(\theta)$ 
\EndFor\\
\Return $\pi_{\theta^*}$ 
\end{algorithmic}
\end{algorithm}
\end{figure*}