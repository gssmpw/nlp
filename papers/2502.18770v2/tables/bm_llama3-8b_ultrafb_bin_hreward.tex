\begin{table*}[t]
\centering
\small
\begin{tabular}{cccccccc}
\toprule
& & \multicolumn{3}{c}{\textbf{AlpacaEval2.0}}  & \multicolumn{3}{c}{\textbf{MT-Bench}} \\
\cmidrule(lr){3-5} \cmidrule(lr){6-8} 
& & \multicolumn{1}{c}{\textbf{LC Winrate}(\%) $\uparrow$}  & \multicolumn{1}{c}{\textbf{Winrate}(\%) $\uparrow$} & \multicolumn{1}{c}{\textbf{Length} $\downarrow$}& \multicolumn{1}{c}{\textbf{T1} $\uparrow$} &  \multicolumn{1}{c}{\textbf{T2} $\uparrow$} &  \multicolumn{1}{c}{\textbf{Overall} $\uparrow$ } \\

\midrule
\multirow{10}{*}
& SFT & 50.000 & 50.000 & \textbf{941} & 5.550 & 3.900 & 4.725 \\
\cdashlinelr{2-8}
& PPO vanilla& 0.040 & 0.120 & 2857 & 1.000 & 1.013 & 1.006\\
\cdashlinelr{2-8}
& PPO+WARM& \textbf{72.180} & 72.420 & 988 & 6.313 & 4.750 & 5.531 \\
& PPO+ODIN& 0.480 & 1.370 & 1839 & 2.625 & 2.450 & 2.538\\
& PPO+reg& 12.310 & 14.410 & 1051 & 4.788 & 3.825 & 4.306\\
\cdashlinelr{2-8}
& PPO+meanstd& 45.170 & 49.070 & 1341 & 5.613 & 4.138 & 4.875 \\
& PPO+clip& 42.140 & 45.340 & 1237 & 5.788 & 4.688 & 5.238\\
& PPO+minmax& 70.140 & 69.190 & 954 & 6.338 & 4.788 & 5.563\\
& PPO+lsc& 63.490 & 65.900 & 1058 & \textbf{6.475} & 4.750 & \textbf{5.613} \\
& PPO+PAR& 70.740 & \textbf{72.800} & 1092 & 6.000 & \textbf{4.975} & 5.488 \\
\bottomrule
\end{tabular}
\caption{Results of Llama3-8B on Ultrafeedback-Binarized: We selected the checkpoint with the highest reward within one epoch for comparison. For the SFT model, we chose the checkpoint obtained after training for two epochs.}
\label{tab:bm_llama3-8b_ultrafb_hreward}
\end{table*}