\begin{table*}[t]
\centering
\small % This command makes the font size smaller
\setlength{\tabcolsep}{3.5pt}
\begin{tabular}{ll}
\toprule
\textbf{Symbol} & \textbf{Meaning} \\
\midrule
$\mathcal{D}$ & Dataset \\
$x,y_w,y_l\sim\mathcal{D}$ & Prompt, chosen response, rejected response in Dataset\\
$\pi_\theta$ & Policy model\\
$\pi_\text{ref}$ & Reference model, also the SFT model \\
$r_\phi$ & Reward model \\
$V_{\alpha}$ & Critic model \\
$y\sim\pi_\theta(.|x)$ & The response generated by policy model for prompt $x$\\
$y_{\text{ref}}\sim\pi_{\text{ref}}(.|x)$ & Reference response, the response generated by reference model \\
$r=r_\phi(x,y)$ & Proxy reward, the reward given directly by reward model\\
$r_{\text{ref}}=r_\phi(x,y_{\text{ref}})$ & Reference reward, the proxy reward for reference response\\
$\mathcal{P}_\phi(y\succ y_{\text{ref}}|x)=\text{sigmoid}(r-r_\text{ref})$ & The hidden preference of reward model $r_\phi$ \\
$r_{\text{centered}}=r-r_{\text{ref}}$ & Centered reward, the proxy reward subtracted by reference reward.\\
$r_{\text{RL}}=f(r_{\text{centered}})$ & RL reward, the reward for RL training \\
$s_t=[x,y_{1,...,t}]$ & The state at position $t$ \\
$a_t=y_{t+1}$ & The Action taken at position $t$ \\
$\hat{A}_t = \sum_{l=t}^{T} (\gamma\lambda)^{l-t} \delta_l$ & The generalized advantage estimation (GAE) \\
$\delta_t = r_t + \gamma V_{\alpha_{\text{old}}}(s_{t+1}) - V_{\alpha_{\text{old}}}(s_t)$ & The temporal difference (TD) error \\
$ G_t = \sum_{l=t}^T \gamma^{l-t} r_l$ & The return \\
$r_t=\begin{cases} 
       r_{\text{RL}}-\eta \log\frac{\pi_\theta(y_{t}|x,y_{<t})}{\pi_{\text{ref}}(y_{t}|x,y_{<t})} & \text{if } t = T\\
      -\eta \log\frac{\pi_\theta(y_{t}|x,y_{<t})}{\pi_{\text{ref}}(y_{t}|x,y_{<t})} & \text{if } t<T 
   \end{cases}$ & The per token reward \\
\bottomrule
\end{tabular}
\caption{Summary of notations.}
\label{table:notation}
\end{table*}