Our analysis is structured to first validate the three key design principles, followed by a comparison of PAR with other reward mitigation methods, and finally, an evaluation of the data efficiency and robustness of PAR.

\subsection{Experimental Setting}
\paragraph{Datasets and Models}
We utilize two dialogue datasets:HH-RLHF~\citep{bai2022traininghelpfulharmlessassistant} and Ultrafeedback-Binarized~\citep{cui2023ultrafeedback}, alongside two base models, Gemma-2B~\citep{gemma_2024} and Llama3-8B~\citep{llama3modelcard}, for our experiments. We present the results of Gemma2-2B on the Ultrafeedback-Binarized in this section. For additional results and comprehensive training details, please refer to Appendix~\ref{section:training_details}. 
\paragraph{Mitigation Baselines}
We evaluate seven baseline methods to mitigate reward hacking, which are described as follows:
\begin{itemize}[leftmargin=*, itemsep=0pt]
\item \textbf{WARM}~\cite{Rame2024WARMOT}: This approach combines the weights of multiple reward models and employs the aggregated model to provide rewards for reinforcement learning training.
\item \textbf{ODIN}~\cite{Chen2024ODINDR}: This method introduces an additional head (length head) during reward training to capture the response length. Only the quality head is utilized for reinforcement learning training.
\item \textbf{Reg}~\cite{Dai2023SafeRS}: A regularization term is integrated into the reward training loss, defined as:
$l_{\text{reward}}=\mathbb{E}_{(x,y_w,y_l)\sim D}[-\log \sigma(r_{\phi}(x,y_w)-r_{\phi}(x,y_l))+\beta ||r_{\phi}(x,y_w)||_2^2+\beta ||r_{\phi}(x,y_l)||_2^2]$.
\item \textbf{Meanstd}: The reward is normalized using the running mean and running standard deviation:
$r_{\text{RL}} = \frac{r - \mu}{s}$,
where \( \mu \) and \( s \) represent the running mean and standard deviation, respectively.
\item \textbf{Clip}: The reward is clipped based on the running mean and standard deviation:
$r_{\text{RL}} = \text{clip}(r, \mu - s, \mu + s)$.
\item \textbf{Minmax}: The reward is normalized using the running minimum and maximum rewards:
$r_{\text{RL}} = \frac{r - r_{\min}}{r_{\max} - r_{\min}}$,
where \( r_{\max} \) and \( r_{\min} \) denote the running maximum and minimum rewards, respectively.
\item \textbf{LSC}~\cite{Wang2024TransformingAC}: The reward is normalized using the log-sigmoid-centered shaping method, defined as: $r_{\text{RL}} = \log\sigma(r - r_{\text{ref}}^{.85})$
where \( r_{\text{ref}}^{.85} \) represents the 85th percentile of the normal distribution, calculated from the mean and variance of the reference rewards.
\end{itemize}

\paragraph{Evaluation Metrics}
Two primary metrics are employed to monitor training progress, both computed on the test set: Proxy Reward (shown as a solid line) and Winrate (shown as a dashed line). The winrate measures the policy model's winning rate against the SFT model, as evaluated by DeepSeek-V3~\cite{deepseekai2024deepseekv3technicalreport}. For the benchmarks AlpacaEval2.0~\cite{alpaca_eval} and MT-Bench~\cite{zheng2023judgingllmasajudgemtbenchchatbot}, six metrics are utilized, with all metrics except the length metric being assessed by DeepSeek-V3.
\paragraph{Training Details}
We briefly outline the training details here; for a comprehensive discussion, please refer to Appendix~\ref{section:training_details}. The dataset is preprocessed to remove noise, and hyperparameters are carefully tuned to ensure continuous growth in the proxy reward. The SFT model is trained for two epochs on chosen responses with a learning rate of 5e-6, while the reward model, consisting of a linear head appended to the base model, is trained for one epoch with a learning rate of 5e-6. The policy model, initialized as the SFT model, is trained for one epoch with a learning rate of 3e-7, and the critic model, initialized as the reward model, is trained for one epoch with a learning rate of 5e-6. A linear learning rate scheduler is employed for all training procedures, gradually increasing the learning rate from 0 to the maximum value during the first 0.1 epoch. To generate the reward and winrate curves, the policy model is evaluated on the test set at intervals of 0.1 epochs, yielding 10 checkpoints for each mitigation method.

\subsection{Principle One}
\label{subsection:P1}
\begin{figure}[t]
\centering
\includegraphics[width=0.99\linewidth]{figures/P1.pdf}
\caption{PPO training curves over two epochs.  `ceil5.0' indicates that $r_{\text{RL}}=\min(r, 5.0)$, and `kl0.1' refers to the KL penalty with $\beta=0.1$. This figure indicates two important results: (1) Excessive rewards can cause reward hacking, hence the RL reward is ideally bounded. (2) PAR is more robust than Minmax and WARM.}
\label{fig:P1}
\end{figure}

To validate the first principle that \emph{RL reward is ideally bounded}, we conducted experiments by employing a larger KL penalty coefficient and constraining the maximum reward during reinforcement learning training (see Figure~\ref{fig:P1}). The results demonstrate that limiting excessive rewards significantly mitigates reward hacking. For instance, increasing the KL penalty coefficient from 0.01 to 0.1 leads to a rise in the winrate curve and a corresponding decline in the reward curve. A similar effect is observed when reducing the reward ceiling (i.e., the maximum reward threshold). Furthermore, Figure~\ref{fig:P1} reveals that while PAR and kl0.1 exhibit comparable proxy rewards, PAR consistently outperforms kl0.1 in terms of winrate, highlighting the superiority of our proposed PAR method.

\begin{figure}[t]
\centering
\includegraphics[width=0.99\linewidth]{figures/calibration_img.pdf}
\caption{The calibration between hidden preference score given by reward model and winrate given by DeepSeek-V3 for different mitigation methods. For all reward shaping methods, the preference score aligns well with winrate initially followed by sudden winrate decrease as preference score exceeds 0.8, while PAR can effectively resist such decrease by constrain the preference score. As for the methods that modify the reward model itself, there is no calibration at all.}
\label{fig:calibration}
\end{figure}
\label{subsection:P2}

\begin{figure}[t]
\centering
\includegraphics[width=0.99\linewidth]{figures/P2.pdf}
\caption{Average winrate for different sigmoid-like functions. Here, `tanh(centered)' denotes $r_{\text{RL}}=\frac{1}{M}\sum_{m=1}^M\text{tanh}(r-r_{\text{ref}}^m)$ and `tanh(uncentered)' denotes $r_{\text{RL}}=\text{tanh}(r)$. Note that `sigmoid(centered)' corresponds to our PAR method. The number of reference rewards for the centered method is $M=10$. RL reward that formulated as a function of centered reward achieves higher winrate than its uncentered counterpart.}
\label{fig:P2}
\end{figure}
We also investigate the calibration between the hidden preference score of the reward model and the winrate provided by DeepSeek-V3 (see Figure~\ref{fig:calibration}). For all reward shaping methods, the preference score initially calibrates well with the winrate but deteriorates when the preference score exceeds 0.8. Notably, PAR effectively resists this deterioration by limiting the preference score. In contrast, methods that modify the reward model itself exhibit poor calibration, rendering their results less meaningful.

\subsection{Principle Two and Three}
To validate the second and third principles—which state that \emph{RL reward is best formulated as a function of centered reward and exhibit rapid initial growth followed by gradual convergence}—we conducted experiments using several sigmoid-like functions, including their centered and uncentered variants, such as tanh, fitted polynomial, sigmoidk2, and sigmoidk3. The results are presented in Figure~\ref{fig:P2}.

\begin{figure}[htbp]
\centering
\includegraphics[width=0.99\linewidth]{figures/sigmoid_like_function_img.pdf}
\caption{The mathematical formulation for the sigmoid-like function is given by $\sigma_k (x) = \frac{1}{1+e^{-kx}}$, where k = 2, 3 corresponds to sigmoidk2 and sigmoidk3, respectively. The poly\_fitted is a fifth-order polynomial function fitted to approximate the sigmoid function.}
\label{fig:sigmoid-like_function}
\end{figure}

\begin{figure*}[t]
\centering
\includegraphics[width=0.75\linewidth]{figures/gemma2-2b_ultrafb_bin_curve_lab1.pdf}
\caption{PPO training curve for different mitigation methods on Gemma2-2B and Ultrafeedback-Binarized.  Solid lines denote the Proxy Reward, and dashed lines denote the Winrate.  Vanilla PPO demonstrates significant reward hacking. ODIN, Reg, Meanstd, Clip, and LSC fail to mitigate this issue, indicated by increasing proxy rewards but decreasing winrates.  PAR achieves highest winrate at the end of training.}
\label{fig:gemma2-2b_ultrafb_bin_lab1}
\end{figure*}

We observe that functions applied to centered rewards achieve higher winrates compared to their uncentered counterparts, providing strong support for the third principle. Furthermore, all sigmoid-like functions operating on centered rewards exhibit similar performance, and their shapes (see Figure~\ref{fig:sigmoid-like_function}) demonstrate rapid growth near the initial point (zero) and slow convergence toward their upper bound (one).
Notably, the centered reward is initially zero because the policy model is initialized as the reference model. By combining the properties of sigmoid-like functions with centered rewards, we ensure that the RL reward exhibits the intended behavior of rapid initial growth and gradual convergence.
We also evaluate a function that increases slowly at the beginning but accelerates towards the end, referred to as SgFc. This function is a bounded function of centered reward, and its curve is depicted in Figure~\ref{fig:sigmoid-like_function}. As shown in Figure~\ref{fig:gemma2-2b_ultrafb_bin_lab1}, SgFc exhibits a lower winrate compared to PAR in the initial stages and also demonstrates reward hacking issues in later stages. This behavior aligns with the second principle, further validating its implications.

\subsection{PAR Effectively Mitigates Reward Hacking}

\paragraph{Reward and Winrate Curve}
As illustrated in Figure~\ref{fig:gemma2-2b_ultrafb_bin_lab1}, the Vanilla PPO suffers from the reward hacking problem severely.
To address this issue, we conduct a comprehensive study of several mitigation methods. While some approaches, such as ODIN, Reg, Meanstd, Clip, and LSC, fail to mitigate the problem, others, including WARM, Minmax, and PAR, demonstrate varying degrees of effectiveness over a single training epoch. Notably, the PAR method achieves the highest winrate by the end of the training process.

Another intriguing observation is that Vanilla, Meanstd, Clip, and LSC exhibit hacking behavior when the proxy reward reaches a specific threshold, such as 6.0, as shown in Figure~\ref{fig:gemma2-2b_ultrafb_bin_lab1}. In contrast, Minmax and PAR show no signs of hacking, and their proxy rewards do not exceed this threshold.

\begin{table*}[t]
\centering
\small
\begin{tabular}{clcccccc}
\toprule
& \multirow{2}{*}{\textbf{Method}} & \multicolumn{3}{c}{\textbf{AlpacaEval2.0}}  & \multicolumn{3}{c}{\textbf{MT-Bench}} \\
\cmidrule(lr){3-5} \cmidrule(lr){6-8} 
& & \multicolumn{1}{c}{\textbf{LC Winrate}(\%) $\uparrow$}  & \multicolumn{1}{c}{\textbf{Winrate}(\%) $\uparrow$} & \multicolumn{1}{c}{\textbf{Length} $\downarrow$}& \multicolumn{1}{c}{\textbf{T1} $\uparrow$} &  \multicolumn{1}{c}{\textbf{T2} $\uparrow$} &  \multicolumn{1}{c}{\textbf{Overall} $\uparrow$ } \\

\midrule
\multirow{10}{*}
& SFT & 50.000 & 50.000 & \textbf{899} & 5.150 & 3.975 & 4.563 \\
\cdashlinelr{1-8}
\multirow{9}{*}{\rotatebox{90}{\textbf{PPO training}}} 
& Vanilla & 0.100 & 0.370 & 2008 & 2.150 & 1.700 & 1.925\\
\cdashlinelr{2-8}
& WARM& 60.670 & 63.170 & 1073 & 5.525 & 3.938 & 4.731 \\
& ODIN& 0.000 & 0.000 & 3672 & 1.375 & 1.338 & 1.356\\
& Reg& 0.000 & 0.000 & 1868 & 1.513 & 1.388 & 1.450\\
\cdashlinelr{2-8}
& Meanstd& 0.030 & 0.120 & 3183 & 1.713 & 1.300 & 1.506 \\
& Clip& 0.000 & 0.000 & 3096 & 1.288 & 1.225 & 1.256\\
& Minmax& 66.980 & 70.930 & 1159 & 5.750 & 4.013 & 4.881\\
& LSC& 47.560 & 53.790 & 1556 & 5.538 & 4.100 & 4.819 \\
\rowcolor{gray!20}
\cellcolor{white}& PAR& \textbf{70.810} & \textbf{75.370} & 1207 & \textbf{5.813} & \textbf{4.313} & \textbf{5.063} \\
\bottomrule
\end{tabular}
\caption{In our evaluation, the checkpoint after one epoch of PPO training is selected for comparison, while the SFT model checkpoint is chosen after two epochs of training. The results indicate that PAR consistently achieves superior performance across all benchmark metrics.}
\label{tab:bm_gemma2-2b_ultrafb_hreward}
\end{table*}

\begin{table}[ht]
\centering
\small
\begin{tabular}{clcc}
\toprule
& \multirow{2}{*}{\textbf{Method}}  & \multicolumn{1}{c}{\textbf{AlpacaEval2.0}}  & \multicolumn{1}{c}{\textbf{MT-Bench}} \\
\cmidrule(lr){3-3} \cmidrule(lr){4-4} 
& & \multicolumn{1}{c}{\textbf{LC Winrate}(\%) $\uparrow$} &  \multicolumn{1}{c}{\textbf{Overall} $\uparrow$ } \\
\midrule
\multirow{10}{*}
& SFT & 50.00 & 4.56 \\
\cdashlinelr{1-4}
\multirow{9}{*}{\rotatebox{90}{\textbf{PPO training}}} 
& Vanilla & 70.48 & 4.94\\
\cdashlinelr{2-4}
& WARM& 70.03 & 4.83\\
& ODIN& 68.96 & 5.06\\
& Reg& 69.44 & 4.74\\
\cdashlinelr{2-4}
& Meanstd& 69.88 & 4.90\\
& Clip& 70.55 & 4.92\\
& Minmax& 68.95 & 4.81\\
& LSC& 72.24 & 4.89\\
\rowcolor{gray!20}
\cellcolor{white}& PAR& 69.43 & 4.93\\
\bottomrule
\end{tabular}
\caption{For comparison, we select the checkpoint with the highest win rate on the test set within one epoch of PPO training. For the SFT model, we utilize the checkpoint obtained after two epochs of training. All methods exhibit comparable peak performance during the training process.}
\label{tab:bm_gemma2-2b_ultrafb_hwin}
\end{table}

\paragraph{Benchmark Performance}
We further investigate the generalization ability of the policy model on out-of-distribution (OOD) data. For each mitigation method, we select the checkpoint after one epoch of training and evaluate these checkpoints on two benchmarks: AlpacaEval2.0 and MT-bench. The results, presented in Table~\ref{tab:bm_gemma2-2b_ultrafb_hreward}, align with the training curve depicted in Figure~\ref{fig:gemma2-2b_ultrafb_bin_lab1}. The Vanilla PPO method exhibits complete deterioration, while the top-performing methods are PAR, Minmax, and WARM. 

To assess whether PAR enhances peak performance—defined as the performance of the best checkpoint on the benchmark—we select the checkpoint with the highest winrate on the test set during a single PPO training epoch for each mitigation method and compare their performance on the benchmarks. The results, shown in Table~\ref{tab:bm_gemma2-2b_ultrafb_hwin}, indicate that all mitigation methods achieve comparable peak performance.

\subsection{PAR is Data Efficient}
The default number of reference rewards for each prompt in our PAR method is set to 10. However, we hypothesize that this number may be higher than necessary for PAR to function effectively. To explore this, we conduct an experiment to determine the minimum number of reference rewards required for PAR to perform efficiently. As shown in Figure~\ref{fig:lab2}, the results reveal that PARref1 to PARref10 exhibit similar trends in both proxy reward and winrate during training. This suggests that a single reference reward is sufficient for PAR to operate effectively. In contrast, the sigmoid method, which can be viewed as a variant of PAR without any reference rewards, performs significantly worse than PARref1. This indicates that completely eliminating reference rewards is not feasible for maintaining performance.

\subsection{PAR is Robust}
To assess the robustness of the mitigation methods discussed earlier, we select the top three performing methods on benchmarks: PAR, Minmax, and WARM. For a more comprehensive evaluation, we extend the training process to two epochs instead of one. The rationale is that if a mitigation method can effectively address the reward hacking problem even under prolonged training, it can be considered robust. The training curves for proxy reward and winrate are presented in Figure~\ref{fig:P1}.

Among the three methods, it is evident that Minmax and WARM lack robustness when the training process is extended to two epochs. In contrast, PAR demonstrates consistent robustness throughout the extended training period. Notably, PAR consistently achieves the highest winrate among all methods, further highlighting its effectiveness and reliability in mitigating reward hacking over extended training durations.

\begin{figure}[t]
\centering
\includegraphics[width=0.99\linewidth]{figures/lab2_img.pdf}
\caption{PPO training curves, evaluated across varying numbers of reference rewards for the PAR method. E.g., the PAR5 means $r_{\text{RL}}=\frac{1}{5}\sum_{m=1}^5\sigma(r-r_{\text{ref}}^m)$. Single reference reward is suffice for PAR to achieve comparable winrate.}
\label{fig:lab2}
\end{figure}