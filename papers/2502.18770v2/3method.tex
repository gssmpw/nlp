\subsection{Design Principles} 

\begin{figure}[t]
    \centering
\includegraphics[width=1.0\linewidth]{figures/loss_figure.pdf}
    \caption{Loss curves from PPO training show that PAR exhibits greater stability, particularly in critic loss, compared to Vanilla training. This stability is attributed to PAR's bounded RL reward.}
    \label{fig:ppo_loss_figure}
\end{figure}

As detailed in Section~\ref{section:intro}, we restate our three design principles here: (1) RL reward is ideally bounded, (2) RL benefits from rapid initial growth followed by gradual convergence, and (3) RL reward is best formulated as a function of centered reward. To elucidate the rationale behind these principles, we examine the Proximal Policy Optimization (PPO) policy and critic loss functions (notation detailed in Table~\ref{table:notation}):
\begin{align*}
    \mathcal{L}_\text{policy}(\theta) & = \hat{\mathbb{E}}_t \bigg[ \min \bigg(  \frac{\pi_\theta(y_{t}|x,y_{<t})}{\pi_{\theta_{\text{old}}}(y_{t}|x,y_{<t})} \cdot \hat{A}_t, \\
    & \text{clip} \left( \frac{\pi_\theta(y_{t}|x,y_{<t})}{\pi_{\theta_{\text{old}}}(y_{t}|x,y_{<t})}, 1 - \epsilon, 1 + \epsilon \right) \cdot \hat{A}_t \bigg) \bigg], \\
\mathcal{L}_\text{critic}(\alpha) &= \hat{\mathbb{E}}_t \big[ ||V_{\alpha}(x,y_{<t}) - G_t||_2^2 \big].
\end{align*}
For the policy loss, \( \hat{A}_t = \sum_{l=t}^{T} (\gamma\lambda)^{l-t} \delta_l \) represents the generalized advantage estimation (GAE) at token $t$, where \( \delta_t = r_t + \gamma V_{\alpha_{\text{old}}}(s_{t+1}) - V_{\alpha_{\text{old}}}(s_t) \) is the temporal difference (TD) error. \( \pi_\theta \) denotes the current policy model, and \( \pi_{\theta_{\text{old}}} \) refers to the policy model from the previous iteration.
$V_{\alpha_{\text{old}}}$ is the criticâ€™s value function from the previous iteration. For the critic loss, \( G_t = \sum_{l=t}^T \gamma^{l-t} r_l \) represents the return, defined as the discounted sum of per-token rewards.

The per-token reward at position $t$, denoted as $r_t$, is defined as:
$$r_t=\begin{cases} 
       r_{\text{RL}}-\eta \log\frac{\pi_\theta(y_{t}|x,y_{<t})}{\pi_{\text{ref}}(y_{t}|x,y_{<t})} & \text{if } t = T\\
      -\eta \log\frac{\pi_\theta(y_{t}|x,y_{<t})}{\pi_{\text{ref}}(y_{t}|x,y_{<t})} & \text{if } t<T 
\end{cases}$$ This formulation ensures that the final token receives the RL reward $r_{\text{RL}}$ while earlier tokens are shaped by the KL divergence regularization term.

The first principle, advocating for bounded RL rewards, is crucial for stabilizing critic training. Excessively large rewards can hinder the critic model's ability to accurately learn the value function, as illustrated in Figure~\ref{fig:ppo_loss_figure}. We hypothesize that this issue arises from the nature of the regression loss used in the critic model. Specifically, large reinforcement learning rewards $r_\text{RL}$ lead to large returns \( G_t \), making the critic loss \( \mathcal{L}_\text{critic}(\alpha) \) more challenging to optimize. Furthermore, this effect propagates to the excessive advantage estimate \( \hat{A}_t \), rendering it unstable and leading to overly aggressive policy updates.

The second principle focuses on regulating the rate of change in the advantage function. A rapid change early in training encourages the policy model to learn quickly, while a slower change toward the end of training helps prevent the policy model from collapsing. We posit that this behavior is due to the advantage function's role in controlling both the direction and magnitude of the policy model's optimization steps.

Finally, the third principle addresses the per-prompt problem, where certain prompts are inherently assigned higher rewards by the reward model, regardless of the actual quality of the generated responses. By centering the reward around a reference response, we focus on relative improvement, mitigating the impact of these per-prompt biases.

To operationalize these principles, we explore several candidate functions that satisfy these criteria, including: the sigmoid function $\sigma(x) = \frac{1}{1+e^{-x}}$ and its variants with  \( \sigma_k (x) = \frac{1}{1+e^{-kx}}, k = 2, 3 \), the hyperbolic tangent (\(\tanh\)) function, and a fitted fifth-order polynomial designed to approximate the sigmoid function. The corresponding curves for these functions are illustrated in Figure~\ref{fig:sigmoid-like_function}.

\subsection{Preference as Reward}
After careful consideration and empirical evaluation, we recommend using the sigmoid function applied to centered rewards as the preferred reward shaping method. The sigmoid function is bounded, has the steepest slope at the initial point (zero), and converges gradually to its upper bound of one. This property makes it particularly suitable for stabilizing the RL training process.
Furthermore, our analysis reveals that this shaping approach is intrinsically linked to the hidden preferences encoded within the reward model. The reward model is designed to simulate human preferences, and the RL training process aims to maximize the reward using an RL algorithm. Given a reward model \( r_{\phi} \), the hidden preference between two responses \( y \) and \( y' \) to a prompt \( x \) can be expressed as:
\[
\mathcal{P}_{\phi}(y \succ y' | x) = \sigma(r_{\phi}(x, y) - r_{\phi}(x, y'))
\]
This formulation shows that applying the sigmoid function to centered rewards corresponds precisely to the preference score of the policy response over the reference response. Consequently, we term this method \textbf{Preference As Reward (PAR)}, which is defined as follows. To enhance stability, we use multiple $M$ reference rewards:
\[
r_{\text{RL}} = \frac{1}{M}\sum_{m=1}^M \sigma(r - r_{\text{ref}}^m) = \frac{1}{M}\sum_{m=1}^M \mathcal{P}_{\phi}(y \succ y_{\text{ref}}^m)
\]

Our proposed PAR method serves exclusively as a reward shaping technique, which is fundamentally orthogonal to other strategies for mitigating reward hacking, such as robust reward model training~\citep{Dai2023SafeRS} or the construction of diverse datasets~\cite{liu2024rrmrobustrewardmodel}.

The pseudo-code for the reward shaping procedure under PAR is detailed in Algorithm~\ref{alg:reward_reshape}, while the complete implementation of the Proximal Policy Optimization (PPO) algorithm is provided in Algorithm~\ref{alg:ppo}. Additionally, the pipeline for reward shaping is illustrated in Figure~\ref{fig:reward_shaping}.