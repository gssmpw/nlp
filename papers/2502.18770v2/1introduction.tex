\input{figures/Reward_shaping}

Reinforcement learning from human feedback is essential for the capabilities of powerful large language models \cite{Training-language-models-to-follow-instructions-with-human-feedback,openai2024gpt4technicalreport}. It not only aligns LLMs with human intentions \cite{bai2022traininghelpfulharmlessassistant} but also enhances their general capabilities \cite{guo2024deepseekcoderlargelanguagemodel}. However, an inherent issue in RLHF, known as reward hacking \citep{amodei2016concrete, gao2023scaling}, has raised concerns about the reliability of this paradigm. Reward hacking occurs when a policy model optimizes for higher reward model scores while failing to improve its actual performance. For example, the policy model might produce repetitive or formulaic content to maximize rewards, rather than genuinely accomplishing the intended tasks \citep{Singhal_Goyal_Xu_Durrett_2023}.

Proximal Policy Optimization (PPO) \citep{Schulman2017ProximalPO} is the de facto RL algorithm for RLHF and is widely used for training LLMs \citep{Training-language-models-to-follow-instructions-with-human-feedback}. However, PPO also suffers from reward hacking, limiting its overall performance. To address this problem, we focus on reward shaping. Reward shaping techniques, such as clipping and rescaling, are relatively straightforward to implement and have demonstrated effectiveness in mitigating reward hacking by modifying the proxy reward \citep{Dai2023SafeRS, Wang2024TransformingAC}. However, a comprehensive comparison of these techniques and a set of well-defined design principles for their application remain largely unexplored.

To address the research gap and establish a foundation for future advancements in reward shaping techniques, we conduct a systematic investigation of widely used methods. Our findings demonstrate that during PPO training with a specific reward model, a distinct reward threshold emerges~\cite{moskovitz2023confrontingrewardmodeloveroptimization}. Exceeding this threshold frequently indicates the onset of reward hacking, marked by a decrease in the model's win rate. We hypothesize that excessively high rewards from the reward model often become misaligned with true performance and lose meaningful correlation with the task's goals. These inflated rewards can also hinder the critic model's ability to learn an accurate value function. This insight leads to our first design principle: \emph{(1) RL reward is ideally bounded}. Moreover, we observe that the low-reward region is considerably safer for optimization compared to the high-reward region, prompting our second design principle: \emph{(2) RL benefits from rapid initial growth followed by gradual convergence}. Finally, we highlight that the absolute values of proxy rewards are uninformative, as different reward models and prompts yield varying scores independent of response quality. Instead, optimizing the relative difference between rewards is more reliable \cite{Wang2024TransformingAC}. This motivates our third principle: \emph{(3) RL reward is best formulated as a function of centered reward}.

Motivated by these principles, we introduce Preference As Reward (PAR), a novel reward shaping technique (see Figure \ref{fig:reward_shaping}). PAR applies a sigmoid function to the centered reward (the difference between the proxy reward $r$ and a reference reward $r_{\text{ref}}$). This design is intuitive: since the policy model is typically initialized from a reference model, the centered reward is initially near zero. The sigmoid function's steep slope at zero promotes rapid initial learning. Crucially, the sigmoid's gradual convergence towards its upper bound ensures training stability. We further observe that PAR's functional form closely resembles the Bradley-Terry model~\cite{Bradley_Terry}, interpreting the exponential of the proxy reward as an Elo score \cite{Elo_1978}. In this context, the RL reward \(r_{\text{RL}}=\text{sigmoid}(r-r_{\text{ref}})\) can be interpreted as the relative preference of the policy response over the reference response, as determined by the reward model.

We conduct experiments on two base models, Gemma2-2B \citep{gemma_2024} and Llama3-8B \citep{llama3modelcard}, using two widely used RLHF datasets, Ultrafeedback-Binarized \citep{cui2023ultrafeedback} and HH-RLHF \citep{bai2022traininghelpfulharmlessassistant}. The result shows that PAR achieves high winrates on the test set after training of one epoch. We also evaluate its performance on two benchmarks AlpacaEval2.0 \citep{alpaca_eval} and MT-Bench \citep{zheng2023judgingllmasajudgemtbenchchatbot}, the PAR consistently tops the benchmark and achieves a winrate that is at least 5 percentage points higher than that of its competitors. Additionally, PAR is data-efficient, requiring only a single reference reward to perform well. It also remains robust against reward hacking, even after two epochs of training.

In conclusion, our contributions are threefold.
\begin{itemize}[leftmargin=*, itemsep=0pt]
\item We propose three key principles for designing effective reward shaping strategies.
\item We introduce PAR, a novel reward shaping technique, and analyze its connection to the underlying preferences of the reward model. 
\item We conduct extensive experiments demonstrating PAR's superiority in mitigating reward hacking compared to other baselines.
\end{itemize}