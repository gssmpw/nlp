Although our PAR method effectively mitigates reward hacking, it does not improve peak performance, as measured by the winrate of the best checkpoint. Furthermore, its design principles lack precision. While PAR sets the upper bound of the RL reward to 1.0, alternative bounds and their selection criteria remain unexplored. Additionally, the dynamics of reward adjustment—such as the initial rate of increase and the pace of convergence—are not fully elucidated. 
% \newpage