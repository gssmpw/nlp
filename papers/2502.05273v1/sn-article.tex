%Version 3 December 2023
% See section 11 of the User Manual for version history
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%                                                                 %%
%% Please do not use \input{...} to include other tex files.       %%
%% Submit your LaTeX manuscript as one .tex document.              %%
%%                                                                 %%
%% All additional figures and files should be attached             %%
%% separately and not embedded in the \TeX\ document itself.       %%
%%                                                                 %%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%\documentclass[referee,sn-basic]{sn-jnl}% referee option is meant for double line spacing

%%=======================================================%%
%% to print line numbers in the margin use lineno option %%
%%=======================================================%%

%%\documentclass[lineno,sn-basic]{sn-jnl}% Basic Springer Nature Reference Style/Chemistry Reference Style

%%======================================================%%
%% to compile with pdflatex/xelatex use pdflatex option %%
%%======================================================%%

%%\documentclass[pdflatex,sn-basic]{sn-jnl}% Basic Springer Nature Reference Style/Chemistry Reference Style


%%Note: the following reference styles support Namedate and Numbered referencing. By default the style follows the most common style. To switch between the options you can add or remove Numbered in the optional parenthesis. 
%%The option is available for: sn-basic.bst, sn-vancouver.bst, sn-chicago.bst%  
 
%%\documentclass[pdflatex,sn-nature]{sn-jnl}% Style for submissions to Nature Portfolio journals
%%\documentclass[pdflatex,sn-basic]{sn-jnl}% Basic Springer Nature Reference Style/Chemistry Reference Style
\documentclass[pdflatex,sn-mathphys-num]{sn-jnl}% Math and Physical Sciences Numbered Reference Style 
%%\documentclass[pdflatex,sn-mathphys-ay]{sn-jnl}% Math and Physical Sciences Author Year Reference Style
%%\documentclass[pdflatex,sn-aps]{sn-jnl}% American Physical Society (APS) Reference Style
%%\documentclass[pdflatex,sn-vancouver,Numbered]{sn-jnl}% Vancouver Reference Style
%%\documentclass[pdflatex,sn-apa]{sn-jnl}% APA Reference Style 
%%\documentclass[pdflatex,sn-chicago]{sn-jnl}% Chicago-based Humanities Reference Style

%%%% Standard Packages
%%<additional latex packages if required can be included here>

\usepackage{graphicx}%
\usepackage{multirow}%
\usepackage{amsmath,amssymb,amsfonts}%
\usepackage{amsthm}%
\usepackage{mathrsfs}%
\usepackage[title]{appendix}%
\usepackage{xcolor}%
\usepackage{textcomp}%
\usepackage{manyfoot}%
\usepackage{booktabs}%
\usepackage{algorithm}%
\usepackage{algorithmicx}%
\usepackage{algpseudocode}%
\usepackage{listings}%
\usepackage{graphicx}%
\usepackage{longtable}%
\usepackage{array}%
\usepackage{geometry}%
%%%%

%%%%%=============================================================================%%%%
%%%%  Remarks: This template is provided to aid authors with the preparation
%%%%  of original research articles intended for submission to journals published 
%%%%  by Springer Nature. The guidance has been prepared in partnership with 
%%%%  production teams to conform to Springer Nature technical requirements. 
%%%%  Editorial and presentation requirements differ among journal portfolios and 
%%%%  research disciplines. You may find sections in this template are irrelevant 
%%%%  to your work and are empowered to omit any such section if allowed by the 
%%%%  journal you intend to submit to. The submission guidelines and policies 
%%%%  of the journal take precedence. A detailed User Manual is available in the 
%%%%  template package for technical guidance.
%%%%%=============================================================================%%%%

%% as per the requirement new theorem styles can be included as shown below
% \theoremstyle{thmstyleone}%
\newtheorem{theorem}{Theorem}%  meant for continuous numbers
%%\newtheorem{theorem}{Theorem}[section]% meant for sectionwise numbers
%% optional argument [theorem] produces theorem numbering sequence instead of independent numbers for Proposition
\newtheorem{proposition}[theorem]{Proposition}% 
%%\newtheorem{proposition}{Proposition}% to get separate numbers for theorem and proposition etc.

% \theoremstyle{thmstyletwo}%
\newtheorem{example}{Example}%
\newtheorem{remark}{Remark}%

% \theoremstyle{thmstylethree}%
\newtheorem{definition}{Definition}%

\raggedbottom
%%\unnumbered% uncomment this for unnumbered level heads

\begin{document}

\title[Article Title]{Principles and Components of Federated Learning Architectures}

%%=============================================================%%
%% GivenName	-> \fnm{Joergen W.}
%% Particle	-> \spfx{van der} -> surname prefix
%% FamilyName	-> \sur{Ploeg}
%% Suffix	-> \sfx{IV}
%% \author*[1,2]{\fnm{Joergen W.} \spfx{van der} \sur{Ploeg} 
%%  \sfx{IV}}\email{iauthor@gmail.com}
%%=============================================================%%


\author*[1]{\fnm{Sarwar} \sur{Saif}}\email{saifmu6@gmail.com}
% \equalcont{These authors contributed equally to this work.}

\author*[2]{\fnm{MD Abdullah Al} \sur{Nasim}}\email{nasim.abdullah@ieee.org}
% \equalcont{These authors contributed equally to this work.}

\author[3]{\fnm{Parag} \sur{Biswas}}\email{text2parag@gmail.com}
% \equalcont{These authors contributed equally to this work.}

\author[4]{\fnm{Abdur} 
\sur{Rashid}}\email{rabdurrashid091@gmail.com}
% \equalcont{These authors contributed equally to this work.}

\author[5]{\fnm{MD Mahim Anjum} 
\sur{Haque}}\email{mahim@vt.edu}


% \author[6]{\fnm{Kishor} \sur{Datta Gupta}}\email{kgupta@cau.edu}
% % \equalcont{These authors contributed equally to this work.}

\author[6]{\fnm{Md. Zihad Bin} \sur{Jahangir}}\email{zihad.bscincse@gmail.com}
% \equalcont{These authors contributed equally to this work.}


\affil[1]{\orgdiv{Department of Business Sciences, Humanities and Social Sciences}, \orgname{University of Tsukuba}, {\city{Tsukuba}, \country{Japan}}}

\affil[2, 5, 6]{\orgdiv{Research and Development Department}, \orgname{Pioneer Alpha},  \orgaddress{{\city{Dhaka}, \country{Bangladesh}}}}

\affil[3, 4]{\orgdiv{MSEM Department}, \orgname{Westcliff university},  \orgaddress{{\city{California}, \country{United States}}}}


% \affil[6]{\orgdiv{Department of Computer and Information Science}, \orgname{Clark Atlanta University}, {\city{Georgia}, \country{USA}}}


%%==================================%%
%% Sample for unstructured abstract %%
%%==================================%%

\abstract{Federated learning, also known as FL, is a machine learning framework in which a significant amount of clients (such as mobile devices or whole enterprises) collaborate to collaboratively train a model while keeping decentralized training data, all overseen by a central server (such as a service provider). There are advantages in terms of privacy, security, regulations, and economy with this decentralized approach to model training. FL is not impervious to the flaws that plague conventional machine learning models, despite its seeming promise. This study offers a thorough analysis of the fundamental ideas and elements of federated learning architectures, emphasizing five important areas: communication architectures, machine learning models, data partitioning, privacy methods, and system heterogeneity. We additionally address the difficulties and potential paths for future study in the area. Furthermore, based on a comprehensive review of the literature, we present a collection of architectural patterns for federated learning systems. This analysis will help to understand the basic of Federated learning, the primary components of FL, and also about several architectural details. }

\keywords{Federated Learning, Federated Learning Architectures, Machine Learning, Vertical federated learning, Horizontal federated learning, FEDF framework}



\maketitle

\section{Introduction}\label{sec1}

Artificial intelligence (AI) and machine learning (ML) have gained popularity in recent years after AI's victory over humans in the board game Alpha-Go \cite{costa2022machine}. The availability of big data and powerful processing units has accelerated the use of Machine Learning technologies in various sectors, including banking, healthcare \cite{biswas2021brain}, \cite{biswas2023hybrid}, \cite{biswas2023active}, \cite{biswas2023generative}, transportation \cite{idris2024cognitive}, customer services \cite{prabadevi2023customer}, e-commerce, and smart home applications \cite{yang2023robust}. Because machine learning techniques are so extensively employed, it is critical to ensure their security and privacy. The bulk of machine learning systems train the model by combining data from several devices or organizations on a central server or cloud platform. This is a significant disadvantage, especially when there are security threats in the training data set because to the sensitive information it includes. Several hospitals can pool their data to construct a collaborative machine-learning model for breast cancer diagnosis \cite{khalid2023breast}, \cite{zhao2023clinical} from MRI images. On the other hand, disclosing private patient data to a central server may expose confidential information to the public, which may have many negative effects. Federated Learning may be a superior choice in certain situations. Federated Learning is a cooperative learning method whereby devices or organizations exchange and aggregate the model parameters from local models rather than exchanging local data \cite{savazzi2021opportunities}.


Federated Learning (FL) has profoundly impacted machine learning, particularly in terms of data security and privacy management \cite{mothukuri2021survey}. In 2016, Google announced FL for collaborative machine learning model training across several clients, supervised by a central server \cite{liu2022distributed}. Clients can range from mobile devices to entire businesses. By ensuring that the training data is decentralized, this strategy helps to reduce the hazards that come with sharing data, which is a feature of typical centralized machine learning techniques. FL has especially great promise in industries like finance and healthcare, where data protection and sensitivity are vital. The overview of federated learning is shown in Fig. \ref{1}. A federated learning system has three stakeholders: (1) the system owner, or learning coordinator; (2) the contributor client, which includes local model trainers and data contributors; and (3) the user client, which is the model user \cite{lo2022architectural}. Keep in mind that a user client can also be a contributor client. System nodes, or hardware components, come in two varieties: (1) central servers; and (2) client devices.


\begin{figure}
\centering
\includegraphics[height=7 cm]{image/fl1.JPG}
\caption{General Outlook of Federated Learning
 \cite{lo2022architectural}}
\label{1}
\end{figure}

Google originally presented the concept of federated learning in 2016 when they implemented it in the Google Keyboard, enabling several Android phones to learn together. Because FL may be implemented on any edge device, it has the potential to completely transform a number of important industries, including finance, healthcare, transportation, and smart homes \cite{omoniwa2018fog}. The most well-known instance was when scientists and doctors from various countries worked together to create an AI pandemic engine for COVID-19 diagnosis \cite{imran2020ai4covid} using chest scans. Transportation networks present yet another intriguing use case: teaching cars to drive themselves and create city routes. In a similar vein, federated learning frameworks enable edge devices in various homes to cooperatively learn on context-aware policies for smart-home applications \cite{rentero2022using}, \cite{costa2022machine}.


FL functions based on two fundamental concepts: model transmission and local computing \cite{agrawal2022federated}, \cite{bouzinis2021wireless}. Clients use their data to do local training; they only provide the trained model parameters to the central server, which combines them to update the global model \cite{agrawal2022federated}, \cite{nguyen2021federated}. Until a workable model is produced, this iterative process is continued. Although FL dramatically lowers some operating expenses and systemic privacy issues, it also presents some special difficulties, such as the requirement for complex coordination and communication methods and vulnerability to a variety of attacks \cite{bouacida2021vulnerabilities}.

The authors of research paper \cite{rieke2020future} illustrate federated learning (FL) operations and highlight the distinctions between learning on a centralized data lake and on a workstation, which is shown in Fig. \ref{2}. The study \cite{rieke2020future} focuses on how federated learning (FL) may be used to incorporate data-driven machine learning (ML), particularly deep learning (DL), into medical practice. The paper examines the difficulty of exploiting massive amounts of medical data due to data silos and privacy concerns, and FL is proposed as a potential solution. Federated learning addresses privacy concerns while also enabling the creation of trustworthy and accurate machine learning (ML) models for the healthcare business without the need for centralized data collection for collaborative model training. The primary contribution of this study is to investigate federated learning (FL) as a viable solution to overcome data silos and privacy concerns in the application of ML for digital health.


\begin{figure}
\centering
\includegraphics[height= 6.5 cm]{image/fl2.JPG}
\caption{The standard FL workflow involves a federation of training nodes that receive a global model, periodically send partially trained models to a central server for aggregation, and continue training on a consensus model provided by the server. We call this process the FL aggregation server (a). (b) FL Peer-to-Peer: An alternative FL formulation where each training node performs its own aggregation and shares its partially learned model with some or all of its peers. A basic non-FL training approach, known as "centralized training" (c), involves data collection sites providing data to a central data lake, from which they retrieve data for independent local training \cite{rieke2020future}}. 
\label{2}
\end{figure}

Figure \ref{3} illustrates the various topologies and computation strategies that may be used to achieve a FL process. Peer-to-peer is the most preferred technique for healthcare applications, followed by an aggregate server. Because FL participants only get model parameters that are averaged among a group of participants and never have direct access to data from other institutions, FL always implicitly ensures a degree of anonymity.

\begin{figure}
\centering
\includegraphics[height=4.8cm]{image/fl3.JPG}
\caption{Overview of different FL theme options. FL topology: communication architecture of federation. (a) Centralized:  models are collected, aggregated, and distributed among training nodes (hub and spokes) by an aggregation server that also manages the training iterations. (b) Distributed: aggregation happens simultaneously at each training node connected to one or more peers. (c) Hierarchical: peer-to-peer federations and aggregation server federations can be combined to create various sub-federations forming a federated network (d). FL computation plan: Passing the model through multiple partners. Cycles of transfer learning and sequential training. (f) Peer-to-peer, (g) aggregation server \cite{rieke2020future}}. 
\label{3}
\end{figure}


The focus of the research paper\cite{niknam2020federated}  is on Federated Learning (FL) as a distributed and privacy-preserving approach to solving wireless communication problems, especially in the context of fifth-generation (5G) networks. The study highlights the shortcomings of conventional, centralized machine learning (ML) techniques in wireless applications because of serious communication costs and worries about data privacy. Federated learning is offered as a possible solution to improve various wireless communication applications \cite{gao2023federated}, \cite{gebremariam2023blockchain} and avoid these problems by enabling local model training without centralizing data. The study illustrates the appropriateness of federated learning for a range of use cases by discussing numerous possible implementations of the technology within 5G networks \cite{rahman2023icn}, \cite{kazmi2024security}. Federated learning, which uses locally trained models instead of gaining direct access to user data, appears to be a perfect fit for proactive caching in wireless networks, specifically for content popularity prediction, as shown in Fig. \ref{4}


\begin{figure}
\centering
\includegraphics[height= 5.2 cm]{image/fl4.JPG}
\caption{An example of federated learning running on caches and edge computing, where the aggregator can be an edge computing platform on an edge network (such as a wireless base station or an unmanned aerial vehicle) and the local learner can be an edge user (an autonomous vehicle in an autonomous vehicle network, or an augmented/virtual platform for a user reality headset) \cite{niknam2020federated}}. 
\label{4}
\end{figure}

The aim of this study is to provide a comprehensive overview of the basic ideas and elements of federated learning architecture. We explore the key components that characterize FL systems, such as data partitioning tactics, privacy-preserving methods, the kinds of machine learning models used, communication protocols, and the management of heterogeneity within the system. Moreover, we recognize and talk about architectural patterns that provide reusable fixes for typical design issues that arise during FL system development. This review aims to improve knowledge and application of federated learning systems by summarizing ongoing research and suggesting future research avenues. In order to advance the discipline and meet the inherent issues of designing safe, reliable, and efficient federated learning systems, we hope to be of great assistance to researchers and practitioners.


\section{Basic Principles of Federated Learning}

Federated Learning (FL) is a machine learning approach that is based on a fundamental premise that differs from typical centralized machine learning. Among those rules are:

\subsection{Localization of Data and Decentralization}

\subsubsection{Local Data Utilization:} FL makes sure that information is never moved to a central server, always staying on local devices or clients. Each client performs local computations and model training using its own data \cite{chahoud2023feasibility}.

\subsubsection{Decentralized Data Storage:} FL improves privacy and reduces the chance of data leaks by maintaining decentralized data \cite{qu2020decentralized}. This strategy is particularly useful in industries like healthcare and finance where data protection regulations are very strict.

\subsection{Training Collaborative Models}

\subsubsection{Federated Training Process:} Users train together on a common global model. Each client computes model parameter updates based on its local data and sends them to a central server \cite{han2023practical}. Apart from this, the server accommodates all the updates using raw data. Moreover, it is improving to bolster the transmission of data with minimal amount of loss. As the model iteratively moves, it adapts better to eradicate the insecure data of the various local datasets. 


\subsubsection{Model Aggregation:} To improve the global model, a central server compiles the updates from each client. This is an iterative process that involves multiple rounds of central aggregation and local training until the model converges. From each step, insightful insights from a wider array of data were found. This procedure allows us to get proper generalization capabilities. It is employed to train ML-based algorithms that provide precise global performance. 


\subsection{Maintaining Privacy}

\subsubsection{No Raw Data Sharing:} Federated learning significantly mitigates privacy concerns compared to centralized machine learning (ML) by enabling clients to exchange only model updates, such as gradients or parameter changes, rather than sharing raw data. This mechanism effectively reduces the risk of sensitive information being exposed during the training process. Moreover, FL empowers clients to retain control over their data, thus addressing critical issues related to data ownership and compliance with privacy regulations. By concentrating on the aggregation of model updates, federated learning can also help alleviate biases that may emerge from the consolidation of sensitive datasets. The decentralized architecture of FL not only enhances privacy but also fosters trust among participants, ultimately encouraging broader participation in collaborative training endeavors. This makes federated learning a promising approach for developing robust and privacy-preserving machine learning systems.

\subsection{Strategies for Improving Privacy:} To further protect the confidentiality of the shared updates, strategies such as homomorphic encryption, secure multi-party computation, and differential privacy can be used.


\subsection{Effective Communication}

\subsubsection{Effective contact Protocols:} To reduce latency and bandwidth utilization, FL requires frequent contact between clients and the central server \cite{mills2022client}. As a result, effective communication protocols are crucial.

\subsubsection{Model Compression and Update Sparsification:} To reduce communication costs, methods such as model compression, quantization, and update sparsification might be used.

\subsection{System and Data Diversities}

\subsubsection{Managing Diverse Data Distributions:} The local data distributions of clients in a Federated Learning system may differ greatly if their data is non-IID (independently and identically distributed). FL algorithms need to withstand this kind of variation.
\subsubsection{Client and System Variability:} The computing capacity, energy availability, and network connectivity of clients can vary. FL systems need to be built with these variances in mind.

\subsection{Safety and Robustness}

\subsubsection{Attack Resistance:} Front-end systems (FL) need to be able to withstand a variety of attacks, including poisoning attempts, in which malevolent clients transmit destructive updates in an attempt to tamper with the global model.

\subsubsection{Fault Tolerance:} Without jeopardizing the training process as a whole, the system should be able to manage client dropouts or erratic communication.


\subsection{Scalable Architecture}
FL systems ought to be able to effectively expand to accommodate a substantial client base, maybe reaching the millions. Scalable server-side infrastructure and effective update aggregation algorithms are needed for this.


\section{Evaluation Of The Performance Of Federated Learning Algorithms}

The main focus of the study \cite{nilsson2018performance} is the evaluation and comparison of several federated learning algorithms, focusing on their performance on independent identically distributed (i.i.d.) and non-i.i.d datasets. Federated learning is an important distributed machine learning technique that combines locally trained models from data-generating clients, such as connected cars and smartphones, to train a global model. Federated Averaging (FedAvg), Federated Stochastic Variance Reduced Gradient (FSVRG), and CO-OP are the three federated learning techniques compared in this article. The MNIST dataset is used to thoroughly compare the performance of these techniques. Among the tested federated learning algorithms, FedAvg proves to be the most successful, especially when dealing with i.i.d data. In this section, we will discuss these three algorithms taken from the study \cite{nilsson2018performance}.


FedAvg employs a central server to assist training by hosting the shared global model \(w_t \), where \(t \) specifies the communication round. Nonetheless, true optimization is performed locally on clients using technologies such as Stochastic Gradient Descent (SGD). FedAvg's five hyperparameters are the proportion of customers to train (C), the local mini-batch size (B), the number of local epochs (E), a learning rate \(eta\), and optionally a learning rate decay \(lambda\). SGD training typically uses the parameters \(B \), \(E \), \(\eta \), and \(\lambda \). However, in this scenario, \(E \) indicates the total number of iterations over the local data before an update to the global model. 
 The number of local training instances determines the weighting system, as stated in Algorithm \ref{5} on line 7.

\begin{figure}
\centering
\includegraphics[height= 5 cm]{image/fl5.JPG}
%\caption{}. 
\label{5}
\end{figure}

FSVRG works by executing multiple distributed stochastic modifications on each client following an expensive central full gradient calculation. To obtain a stochastic update, one update is performed iteratively for each data point, using a random permutation of the local data. A basic FSVRG only has one hyperparameter, the stepsize \(h\). Algorithm \ref{6} thoroughly explains FSVRG, which involves a single iteration as follows: To calculate a total gradient, all clients acquire the most recent version of the model and calculate loss gradients in connection to their local data. Clients then submit their gradients, which the server aggregates to generate the entire gradient \(\nabla f (w_t) \).


\begin{figure}
\centering
\includegraphics[height= 7 cm]{image/fl6.JPG}
%\caption{}. 
\label{6}
\end{figure}


Unlike FedAvg and FSVRG, which rely on coordinated model updates, CO-OP \cite{wang2017co} proposes an asynchronous approach. This technique instantaneously combines any incoming client model with the global model. The global model has age \(a \), and each client \(k \) has an age \(a_k \) associated with it. When merging models, the equation for the age difference  \( a - a_k \)) is used to compute a weight. The justification for this is that in an asynchronous structure, some clients will train on out-of-date models while others will train on newer models.


Additionally, CO-OP gets all of its hyperparameters from the optimization technique that underpins it, such as SGD.
The following is the training protocol: Using its own training set of data, each client runs an optimization process over E rounds before asking the server for the global model age as of right now. At this point, the client determines whether the age gap satisfies the requirements. In the event that the local model is out of date, the client makes amends with the global model and restarts. In the event that the client exhibits excessive activity, training just continues. If not, the local model is uploaded to the server in order to be combined. We see the CO-OP pseudocode in Algorithm \ref{7}.

\begin{figure}
\centering
\includegraphics[height= 9 cm]{image/fl7.JPG}
%\caption{}. 
\label{7}
\end{figure}


\section{Federated Learning Architectures}

Federated Learning (FL) is an approach that allows machine learning models to be trained in a distributed manner using remotely hosted datasets without polluting the data through aggregation.
 \cite{antunes2022federated}. FL is a viable way to improve ML-based systems, increase alignment with regulatory standards, and improve data sovereignty and trust. Many questions remain unanswered before FL is widely used \cite{antunes2022federated}.
 Both federated learning and neural architecture search face many unsolved challenges. However, the search for optimal neural designs in the context of federated learning is particularly challenging \cite{zhu2021federated}. This work provides background on Federated Learning and Neural Architecture Search (NAS)  \cite{zhu2021federated}, with a particular focus on the recently developed area of Federated Neural Architecture Search (FNAS). Systems are categorized into offline and online approaches, and single- and multi-objective NAS methods are discussed. The study classifies federated learning systems, draws attention to the difficulties and limitations of online FNAS, looks at ways to balance various goals including precision and communication expenses, and concludes by summarizing the primary issues still facing FNAS.

Since its inception, federated learning has seen tremendous evolution. In 2016, Google researchers formally presented the idea, concentrating at first on enhancing user privacy for mobile keyboard prediction applications \cite{shaheen2022applications}. FL's application base has grown over time to include a wide range of fields, including banking, healthcare, and other industries. The creation of novel algorithms, privacy-preserving methods \cite{yin2021comprehensive}, and designs to manage the inherent heterogeneity in federated environments are important developments.

The study \cite{aledhari2020federated} provides a comprehensive examination of Federated Learning (FL), emphasizing privacy-preserving solutions and focusing on enabling technologies, protocols, practical implementations, and use cases across various sectors. The paper \cite{aledhari2020federated} intends to help data scientists create more effective privacy-preserving solutions by offering a complete review of relevant FL protocols, platforms, and applications. It also discusses the primary advantages and disadvantages of FL, as well as detailed use cases that demonstrate how successfully it can be utilized in various industries.


\begin{figure}
\centering
\includegraphics[height= 5 cm]{image/fl8.JPG}
\caption{Universal architecture for federated learning \cite{aledhari2020federated}.}
\label{8}
\end{figure}

The revised models are returned to the principal server for aggregation. The devices receive a single, aggregated model based on distributed computing principles \cite{ben2021deep}. This enables us to monitor and disperse each model among several devices. FL's technique is particularly advantageous for using affordable machine learning models on devices such as sensors and mobile phones \cite{doku2019towards}. Figure \ref{8} exhibits FL's general architecture. 

\begin{figure}
\centering
\includegraphics[height= 5 cm]{image/fl9.JPG}
\caption{Utilizing Federated Learning Architecture in a Healthcare Environment \cite{aledhari2020federated}.}
\label{9}
\end{figure}

There is a wealth of research on the usage of FL. One of its unique use cases is the healthcare industry \cite{stoian2008current}, \cite{brisimi2018federated}. Figure \ref{9} illustrates the use of a FL design in a hospital context. Unfortunately, there are still considerable impediments to FL's full integration in other situations, notably with regard to data. 

The study \cite{singh2022federated} examines the field of federated learning, highlighting its potential in a number of industries and its use in mobile devices. It explores the different forms, structures, possibilities, and difficulties associated with federated learning with the goal of creating common practices for broad deployment in dispersed settings, protecting data, and facilitating diverse networks. It seeks to offer a road map for federated learning adoption and use across a range of industries, such as mobile networks, healthcare, and transportation.

\begin{figure}
\centering
\includegraphics[height= 5 cm]{image/fl10.png}
\caption{Some major types of Federated Learning Architecture are shown by the authors \cite{singh2022federated}.}
\label{10}
\end{figure}

There are numerous platforms and architectures included with FL.
Numerous organizations are currently working to create FL designs in the medical industry \cite{bonawitz2019towards}, \cite{cheng2021secureboost}. Intel and the University of Pennsylvania are two of the top universities. Furthermore, a variety of platforms have been developed for FL, a few of which will be discussed in this part. Table \ref{tab:fl_architectures} summarizes several designs and their focus points. This section goes into further information regarding these architectures.


% \geometry{margin=2.5cm}

\begin{longtable}{|>{\centering\arraybackslash}m{2.5 cm}|>{\centering\arraybackslash}m{4 cm}|>{\centering\arraybackslash}m{2.5 cm}|>{\centering\arraybackslash}m{3.5cm}|}
\caption{An overview of architectures, a brief synopsis, and their main focus} \label{tab:fl_architectures} \\
\hline
\textbf{Name of FL Architecture} & \textbf{Short Description} & \textbf{Characteristics} & \textbf{Benefits and Main Focus of Application} \\
\hline
Horizontal Federated Learning (HFL) & Federated learning use an identical feature space but distinct sample spaces. & Each client has data with the same features. & Enhances collaboration among institutions with similar data structures. Focus: Healthcare collaboration between hospitals. \\
\hline
Vertical Federated Learning (VFL) & Federated learning use the same sample region but distinct feature spaces. & Each client has different features for the same samples. & Combines complementary data from different domains. Focus: Cross-sector collaboration, e.g., between banks and insurance companies. \\
\hline
Federated Transfer Learning (FTL) & Combines transfer and federated learning for many instances and feature spaces. & Uses pre-trained models to adapt to new tasks or domains. & Facilitates knowledge transfer across domains. Focus: Cross-domain collaborations to improve models. \\
\hline
Centralized Federated Learning & Coordinates the learning process through a central server. & Simplifies aggregation, potential central bottleneck. & Easy to manage but may suffer from central bottlenecks. Focus: General applications with centralized data control. \\
\hline
Decentralized Federated Learning & No central server; clients communicate and share updates directly with each other. & No central bottleneck, no single point of failure. & Increases robustness and fault tolerance. Focus: Applications needing high robustness. \\
\hline
Hierarchical Federated Learning & Introduces intermediate aggregators between clients and the central server. & Reduces central server load, and enhances scalability. & Leverages edge computing for scalability. Focus: Scalable applications using edge devices. \\
\hline
Asynchronous Federated Learning & Clients send updates to the server asynchronously. & Reduces idle time, and handles stragglers effectively. & Optimizes for environments with latency issues. Focus: Real-time applications with intermittent connectivity. \\
\hline
PERFIT & Federated learning for personalized fitness recommendations. & Customizable to individual fitness data and goals. & Provides personalized health insights. Focus: Fitness and health tracking applications. \\
\hline
MMVLF (Multi-Model Vertical Federated Learning) & Enables the training of multiple models vertically. & Combines various feature sets for comprehensive insights. & Enhances model accuracy and robustness. Focus: Multi-domain data analysis and insights. \\
\hline
FADL (Federated Anomaly Detection Learning) & Federated learning tailored for anomaly detection. & Detects anomalies across distributed datasets. & Improves security and fault detection. Focus: Cybersecurity and fraud detection. \\
\hline
Blockchain-FL & Integrates blockchain with federated learning for secure model updates. & Decentralized ledger for verifiable updates. & Enhances security and transparency. Focus: Secure and transparent data collaboration. \\
\hline
FEDF (Federated Edge-Device Framework) & Framework for federated learning on edge devices. & Optimized for resource-constrained environments. & Empowers edge devices with federated learning capabilities. Focus: IoT and mobile device applications. \\
\hline
\end{longtable}

\begin{figure}
\centering
\includegraphics[height= 6 cm]{image/fl11.JPG}
\caption{The architecture of Vertical Federated Learning \cite{aledhari2020federated}.}
\label{11}
\end{figure}

FL (also known as sample-based FL) refers to comparable features that differ in terms of data. It's worth noting that ideas for a horizontal FL framework have been made. One example is when Google proposed utilizing a Horizontal FL method to manage Android phone upgrades. Horizontal FL assumes that consumers are trustworthy and that the server is secure. Customer data can only be updated by the central server \cite{bonawitz2019towards}. Horizontal FL's architecture allows x number of analogous structural pieces to learn a model with the support of servers or parameters, as seen in Fig. \ref{12}.

Vertical FL is also known as feature-based FL. Figure \ref{11} depicts the Vertical FL procedure. In this case, data sets may differ in features but have similar sample IDs. What we are doing with Vertical FL is gathering and organizing these different elements. Next, in order to create a model that collectively incorporates data from both entities, we must compute the training loss. Every entity in Vertical FL shares the same identification and status. The Vertical FL system also presumes that its customers are trustworthy when it comes to security. Nonetheless, Vertical FL raises two security-related issues. The Vertical FL architecture consists of two primary components: encrypted model training and encrypted entity alignment \cite{bonawitz2019towards}, \cite{cheng2021secureboost}, \cite{yang2019federated}.

\begin{figure}
\centering
\includegraphics[height= 6 cm]{image/fl12.JPG}
\caption{The architecture of Horizontal Federated Learning \cite{aledhari2020federated}.}
\label{12}
\end{figure}

This architecture's independence from other machine-learning techniques is one of its advantages. It's interesting to note that horizontal FL has been applied to medical situations like drug detection. Federated Transfer Learning (FTL) is an additional FL architecture in addition to the Horizontal FL and Vertical Architectures. In \cite{liu2020secure}, FTL was proposed.

\begin{figure}
\centering
\includegraphics[height= 6 cm]{image/fl13.JPG}
\caption{The architecture of Federated Transfer Learning \cite{aledhari2020federated}.}
\label{13}
\end{figure}

Figure \ref{13} presents an overview of the FTL procedure. To complete the method, the Guest and Host must first compute and encrypt their findings locally. Gradients and losses are calculated using the data. They then provide Arbiter access to the encrypted values. The Arbiter then provides the Guest and Host with the gradients and loss computations, which they may use to make model adjustments. Until the loss function converges, the FTL framework iterates \cite{liu2020secure}. Additionally, FTL offers support for both homogenous and heterogeneous training methodologies. Using a variety of sample types, entities assist in training the model or models in the homogeneous method. When entities are heterogeneous, they have identical samples but differing feature spaces. 

The second work by Siwei Feng and Han Yu proposes a new architecture based on the vertical FL system. The architecture proposed by the authors is specifically known as the Multi-Participant Multi-Class Vertical Federated Learning Framework (MMVFL). This particular architecture (Figure \ref{14}) is designed to manage multiple participants. The authors note that MMVFL enables the sharing of labels in a way that preserves the privacy of the owner and other participants. The assumption that records from different entities have the same feature space but may not be associated with the same sample ID space is problematic when introducing a horizontal fuzzy logic architecture. Unfortunately, this is not always the case, and the proposed structure aims to mitigate this drawback. The goal of the MMVFL framework is to learn a large number of frameworks to achieve different objectives. The goal is to increase the level of personalization in the learning process. The authors used two computer vision datasets to evaluate the performance of their framework: Additionally, the authors compare their framework with alternative approaches: the more features the framework includes, the better the results. They also observe that the MMVFL framework performs better the more features it uses \cite{feng2020multi}.


\begin{figure}
\centering
\includegraphics[height= 6 cm]{image/fl14.JPG}
\caption{The architecture of MMVFL \cite{aledhari2020federated}.}
\label{14}
\end{figure}

Tien-Dung et al. [42] present a further FL framework. FL's method is meant to allow for concurrent training while still protecting anonymity. A model may be trained on numerous geographically scattered training data sets—which may belong to different owners—using their framework, known as FEDF. As seen in Fig. \ref{15}, the authors' proposed design consists of a master and X workers. Additionally, the writers were able to test their framework on a variety of systems. The major datasets utilized to assess the FEDF architecture were the CIFAR-10 membrane data set (MEMBRANE) and the health care imaging data set (HEART-VESSEL). The assessment criteria were performance, training speed, and data volume transmitted.

\begin{figure}
\centering
\includegraphics[height= 5 cm]{image/fl15.JPG}
\caption{The architecture of FEDF \cite{aledhari2020federated}.}
\label{15}
\end{figure}

Qiong Wu et al.'s framework for FL is another intriguing one [43]. They have focused their FL architecture on IoT adaptability. Although this study is not focused on IoT, it is worth noting that FL has been proposed for IoT. [44, 45]. Figure \ref{16} shows the authors' PerFit framework. PerFit was designed to help with a few FL and IoT-related issues. Upon closer inspection, PerFit's cloud-based architecture should provide IoT devices with easily accessible processing capability, according to the authors. The architecture is set up in a way that allows any Internet of Things device to release its computational burden, thereby meeting the demands for low latency and efficiency.

\begin{figure}
\centering
\includegraphics[height= 5 cm]{image/fl16.JPG}
\caption{The architecture of  PerFit \cite{aledhari2020federated}.}
\label{16}
\end{figure}

The creation of a privacy-preserving federated learning (FL) application for anticipating customers' financial distress is the major focus of the study \cite{imteaj2022leveraging}. This approach addresses the resource and data privacy restrictions of traditional centralized machine learning models. The main contribution is a new FL method that allows partial task contributions, which reduces the effect of straggler agents and enhances model convergence and performance in resource-constrained contexts. The suggested approach outperforms current FL models in accuracy and maintains data privacy while achieving accuracy comparable to centralized models. The authors provided the general process flowchart in Figure \ref{17} and provided a step-by-step breakdown of their suggested methodology as follows:

\begin{figure}
\centering
\includegraphics[height= 12 cm]{image/fl20.jpg}
\caption{Diagram illustrating the suggested FL approach for forecasting a customer's financial demise \cite{imteaj2022leveraging}.}
\label{17}
\end{figure}

\section{Federated Learning's Limitations and Difficulties}\label{sec2}
Federated Learning (FL) has several advantages, but its complete adoption across industries is hampered by a number of obstacles, particularly those related to privacy, security, and technical constraints. The fact that FL training data is inherently imperfect—it might be biased, uneven, or incomplete—is one of the main obstacles \cite{khan2021federated}. Poor model performance results from an uneven distribution of training samples among entities, a phenomenon known as data imbalance \cite{zhang2022federated}. The training process is made more difficult by missing classes, features, and values since distinct entities may have datasets that are missing crucial information, which leads to erroneous models. Moreover, the complexity is increased by the heterogeneity of data resulting from its dispersion across several places, rendering crude applications of FL models ineffectual.

Effective communication presents a big additional difficulty. FL uses a lot of devices, particularly in environments like healthcare \cite{singh2022framework}, \cite{ali2022federated} where privacy concerns make local data maintenance essential. In order to solve the slower communication speeds inherent in FL, it is imperative that the number of communication rounds and message sizes exchanged throughout the training process be reduced. Effective communication techniques are required to guarantee model updates in a timely manner without jeopardizing data privacy.

System heterogeneity adds still another level of complexity \cite{zhou2024distributed}. Stragglers—devices that are unable to keep up with the training process—can result from the varied processing capacities and network circumstances of participating devices, which delays the convergence of the model. Concerns about privacy are also very important since, whereas FL tries to keep sensitive data local, there is always a chance that information could leak during model upgrades \cite{makkar2023securefed}. To effectively apply FL across several industries, it is imperative to find creative solutions that improve data handling, communication efficiency, and privacy preservation.


\section{Conclusion}\label{sec13}
Federated Learning (FL), which decentralizes the training process across multiple clients while preserving data security and privacy, offers a revolutionary approach to machine learning. This article explored the fundamental ideas of FL in great detail, assessed the efficacy of several federated learning algorithms, and looked closely at the numerous architectures that make up the FL ecosystem. The substantial advantages of FL, including improved privacy, regulatory compliance, and cost savings, are highlighted by our analysis. But FL also has to deal with a number of issues that call for creative solutions, like communication overhead, data heterogeneity, and privacy problems. The objective of this work is to make a contribution to the progress of FL research and its application in many fields by tackling these restrictions and investigating possible future paths. We also put forward a set of architectural principles that can direct the creation and execution of reliable and effective federated learning systems through a thorough analysis of the literature. This work opens the door for more investigation and advancement in this exciting topic by offering a useful resource for comprehending the fundamental elements and architectures of FL.

\backmatter


\bmhead{Acknowledgements}

The authors would like to thank their friends and family for their continued support.









%%=============================================%%
%% For submissions to Nature Portfolio Journals %%
%% please use the heading ``Extended Data''.   %%
%%=============================================%%

%%=============================================================%%
%% Sample for another appendix section			       %%
%%=============================================================%%

%% \section{Example of another appendix section}\label{secA2}%
%% Appendices may be used for helpful, supporting or essential material that would otherwise 
%% clutter, break up or be distracting to the text. Appendices can consist of sections, figures, 
%% tables and equations etc.



%%===========================================================================================%%
%% If you are submitting to one of the Nature Portfolio journals, using the eJP submission   %%
%% system, please include the references within the manuscript file itself. You may do this  %%
%% by copying the reference list from your .bbl file, paste it into the main manuscript .tex %%
%% file, and delete the associated \verb+\bibliography+ commands.                            %%
%%===========================================================================================%%

\bibliography{sn-bibliography}% common bib file
%% if required, the content of .bbl file can be included here once bbl is generated
%%\input sn-article.bbl


\end{document}
