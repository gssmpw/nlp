
%% bare_jrnl_compsoc.tex
%% V1.4b
%% 2015/08/26
%% by Michael Shell
%% See:
%% http://www.michaelshell.org/
%% for current contact information.
%%
%% This is a skeleton file demonstrating the use of IEEEtran.cls
%% (requires IEEEtran.cls version 1.8b or later) with an IEEE
%% Computer Society journal paper.
%%
%% Support sites:
%% http://www.michaelshell.org/tex/ieeetran/
%% http://www.ctan.org/pkg/ieeetran
%% and
%% http://www.ieee.org/

%%*************************************************************************
%% Legal Notice:
%% This code is offered as-is without any warranty either expressed or
%% implied; without even the implied warranty of MERCHANTABILITY or
%% FITNESS FOR A PARTICULAR PURPOSE!
%% User assumes all risk.
%% In no event shall the IEEE or any contributor to this code be liable for
%% any damages or losses, including, but not limited to, incidental,
%% consequential, or any other damages, resulting from the use or misuse
%% of any information contained here.
%%
%% All comments are the opinions of their respective authors and are not
%% necessarily endorsed by the IEEE.
%%
%% This work is distributed under the LaTeX Project Public License (LPPL)
%% ( http://www.latex-project.org/ ) version 1.3, and may be freely used,
%% distributed and modified. A copy of the LPPL, version 1.3, is included
%% in the base LaTeX documentation of all distributions of LaTeX released
%% 2003/12/01 or later.
%% Retain all contribution notices and credits.
%% ** Modified files should be clearly indicated as such, including  **
%% ** renaming them and changing author support contact information. **
%%*************************************************************************


% *** Authors should verify (and, if needed, correct) their LaTeX system  ***
% *** with the testflow diagnostic prior to trusting their LaTeX platform ***
% *** with production work. The IEEE's font choices and paper sizes can   ***
% *** trigger bugs that do not appear when using other class files.       ***                          ***
% The testflow support page is at:
% http://www.michaelshell.org/tex/testflow/


%\documentclass[12pt,journal,compsoc,onecolumn]{IEEEtran}
%\setlength{\columnsep}{0.10in}
%\documentclass[12pt,journal,draftclsnofoot,onecolumn,letterpaper]{IEEEtran}
\documentclass[10pt,journal,compsoc]{IEEEtran}
%
% If IEEEtran.cls has not been installed into the LaTeX system files,
% manually specify the path to it like:
% \documentclass[10pt,journal,compsoc]{../sty/IEEEtran}
%\usepackage{cite}
\usepackage{booktabs}
\usepackage{multirow}
\usepackage{makecell}
\usepackage{amsthm}
\usepackage{diagbox}
\usepackage{xcolor}
\usepackage{subfigure}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{algorithm}
\usepackage{algorithmicx}
%\usepackage{hyperref}
\usepackage{algpseudocode}
\usepackage{ragged2e}
\usepackage{cuted}
\usepackage{graphicx}
\usepackage{epstopdf}

\usepackage{booktabs}
\usepackage{array}
\usepackage{lscape} % 用于横向表格


\usepackage[colorlinks, linkcolor=red,anchorcolor=blue,citecolor=green]{hyperref}


\newtheorem{myDef}{Definition}
\newtheorem{myTheo}{Theorem}
\newtheorem{myCoro}{Corollary}
\newtheorem{myRem}{Remark}
\newtheorem{myPro}{Proposition}
%\newtheorem{proof}{Proof}[section]

\renewcommand{\algorithmicrequire}{ \textbf{Input:}}     %Use Input in the format of Algorithm
\renewcommand{\algorithmicensure}{ \textbf{Output:}}    %UseOutput in the format of Algorithm
%\renewcommand{\baselinestretch}{0.97}

\ifCLASSOPTIONcompsoc
  % IEEE Computer Society needs nocompress option
  % requires cite.sty v4.0 or later (November 2003)
\usepackage[nocompress]{cite}
\else
  % normal IEEE
  \usepackage{cite}
\fi

%
\ifCLASSINFOpdf

\else

   \usepackage[dvips]{graphicx}

\fi

\hyphenation{op-tical net-works semi-conduc-tor}


\begin{document}
%
% paper title
% Titles are generally capitalized except for words such as a, an, and, as,
% at, but, by, for, in, nor, of, on, or, the, to and up, which are usually
% not capitalized unless they are the first or last word of the title.
% Linebreaks \\ can be used within to get better formatting as desired.
% Do not put math or special symbols in the title.
\title{Privacy-Aware Joint DNN Model Deployment and Partition Optimization for Delay-Efficient Collaborative Edge Inference}
%Intra-Serial-Inter-Parallel:
%
% author names and IEEE memberships
% note positions of commas and nonbreaking spaces ( ~ ) LaTeX will not break
% a structure at a ~ so this keeps an author's name from being broken across
% two lines.
% use \thanks{} to gain access to the first footnote area
% a separate \thanks must be used for each paragraph as LaTeX2e's \thanks
% was not built to handle multiple paragraphs
%
%
%\IEEEcompsocitemizethanks is a special \thanks that produces the bulleted
% lists the Computer Society journals use for "first footnote" author
% affiliations. Use \IEEEcompsocthanksitem which works much like \item
% for each affiliation group. When not in compsoc mode,
% \IEEEcompsocitemizethanks becomes like \thanks and
% \IEEEcompsocthanksitem becomes a line break with idention. This
% facilitates dual compilation, although admittedly the differences in the
% desired content of \author between the different types of papers makes a
% one-size-fits-all approach a daunting prospect. For instance, compsoc
% journal papers have the author affiliations above the "Manuscript
% received ..."  text while in non-compsoc journals this is . Sigh.

\author{Zhipeng Cheng, Xiaoyu Xia,~\IEEEmembership{Senior Member,~IEEE,} Hong Wang, Minghui Liwang, Ning Chen,  Xuwei Fan, and Xianbin Wang,~\IEEEmembership{Fellow,~IEEE}

\thanks{Zhipeng Cheng (chengzp\_x@163.com) and Hong Wang (wh\_5233@163.com) are with School of Future Science and Engineering, Soochow University, Suzhou 215006, China. Xiaoyu Xia (xiaoyu.xia@rmit.edu.au) is with the School of Computing Technologies, RMIT University, Melbourne, Victoria, Australia. Minghui Liwang (minghuiliwang@tongji.edu.cn, Corresponding author) is with the Department of Control Science and Engineering,
 The National Key Laboratory of Autonomous Intelligent Unmanned Systems, Tongji University, Shanghai 201804, China, and also
with the Frontiers Science Center for Intelligent Autonomous Systems, Ministry of Education, Tongji University, Shanghai 201804, China. Ning Chen (chenning@upc.edu.cn) is with Department of automation, China University of Petroleum (East China), Qingdao, China. Xuwei Fan (xwfan@stu.xmu.edu.cn) is with College of Computer and Information Sciences, Fujian Agriculture and Forestry University, Fuzhou 350002, China. Xianbin Wang (xianbin.wang@uwo.ca) is with Department of Electrical and Computer Engineering, Western University, Ontario, Canada.}

}

% note the % following the last \IEEEmembership and also \thanks -
% these prevent an unwanted space from occurring between the last author name
% and the end of the author line. i.e., if you had this:
%
% \author{....lastname \thanks{...} \thanks{...} }
%                     ^------------^------------^----Do not want these spaces!
%
% a space would be appended to the last name and could cause every name on that
% line to be shifted left slightly. This is one of those "LaTeX things". For
% instance, "\textbf{A} \textbf{B}" will typeset as "A B" not "AB". To get
% "AB" then you have to do: "\textbf{A}\textbf{B}"
% \thanks is no different in this regard, so shield the last } of each \thanks
% that ends a line with a % and do not let a space in before the next \thanks.
% Spaces after \IEEEmembership other than the last one are OK (and needed) as
% you are supposed to have spaces between the names. For what it is worth,
% this is a minor point as most people would not even notice if the said evil
% space somehow managed to creep in.



% The paper headers
\markboth{}%
{Shell \MakeLowercase{\textit{et al.}}:}
% The only time the second header will appear is for the odd numbered pages
% after the title page when using the twoside option.
%
% *** Note that you probably will NOT want to include the author's ***
% *** name in the headers of peer review papers.                   ***
% You can use \ifCLASSOPTIONpeerreview for conditional compilation here if
% you desire.



% The publisher's ID mark at the bottom of the page is less important with
% Computer Society journal papers as those publications place the marks
% outside of the main text columns and, therefore, unlike regular IEEE
% journals, the available text space is not reduced by their presence.
% If you want to put a publisher's ID mark on the page you can do it like
% this:
%\IEEEpubid{0000--0000/00\$00.00~\copyright~2015 IEEE}
% or like this to get the Computer Society new two part style.
%\IEEEpubid{\makebox[\columnwidth]{\hfill 0000--0000/00/\$00.00~\copyright~2015 IEEE}%
%\hspace{\columnsep}\makebox[\columnwidth]{Published by the IEEE Computer Society\hfill}}
% Remember, if you use this you must call \IEEEpubidadjcol in the second
% column for its text to clear the IEEEpubid mark (Computer Society jorunal
% papers don't need this extra clearance.)



% use for special paper notices
%\IEEEspecialpapernotice{(Invited Paper)}



% for Computer Society papers, we must declare the abstract and index terms
% PRIOR to the title within the \IEEEtitleabstractindextext IEEEtran
% command as these need to go into the title area created by \maketitle.
% As a general rule, do not put math, special symbols or citations
% in the abstract or keywords.
\IEEEtitleabstractindextext{%
\begin{abstract}
\justifying
Edge inference (EI) is a key solution to address the growing challenges of delayed response times, limited scalability, and privacy concerns in cloud-based Deep Neural Network (DNN) inference. However, deploying DNN models on resource-constrained edge devices faces more severe challenges, such as model storage limitations, dynamic service requests, and privacy risks. This paper proposes a novel framework for privacy-aware joint DNN model deployment and partition optimization to minimize long-term average inference delay under resource and privacy constraints. Specifically, the problem is formulated as a complex optimization problem considering model deployment, user-server association, and model partition strategies. To handle the NP-hardness and future uncertainties, a Lyapunov-based approach is introduced to transform the long-term optimization into a single-time-slot problem, ensuring system performance. Additionally, a coalition formation game model is proposed for edge server association, and a greedy-based algorithm is developed for model deployment within each coalition to efficiently solve the problem. Extensive simulations show that the proposed algorithms effectively reduce inference delay while satisfying privacy constraints, outperforming baseline approaches in various scenarios.
\end{abstract}

\begin{IEEEkeywords}
Collaborative edge inference, Model deployment, Model partition,  Lyapunov optimization, Coalition formation game.
\end{IEEEkeywords}}


% make the title area
\maketitle


% To allow for easy dual compilation without having to reenter the
% abstract/keywords data, the \IEEEtitleabstractindextext text will
% not be used in maketitle, but will appear (i.e., to be "transported")
% here as \IEEEdisplaynontitleabstractindextext when the compsoc
% or transmag modes are not selected <OR> if conference mode is selected
% - because all conference papers position the abstract like regular
% papers do.
\IEEEdisplaynontitleabstractindextext
% \IEEEdisplaynontitleabstractindextext has no effect when using
% compsoc or transmag under a non-conference mode.



% For peer review papers, you can put extra information on the cover
% page as needed:
% \ifCLASSOPTIONpeerreview
% \begin{center} \bfseries EDICS Category: 3-BBND \end{center}
% \fi
%
% For peerreview papers, this IEEEtran command inserts a page break and
% creates the second title. It will be ignored for other modes.
\IEEEpeerreviewmaketitle

\IEEEraisesectionheading{\section{Introduction}\label{sec:introduction}}

\IEEEPARstart{N}{owadays}, many Artificial Intelligence (AI) applications rely on remote cloud servers to perform complex inference tasks \cite{c1}. However, this approach introduces several challenges, including delayed response due to long-distance network transmission, scalability issues arising from high bandwidth consumption, and privacy concerns related to the forwarding and storage of user data over the network\cite{c2,c3}. Furthermore, the growing disparity between the limited resource on Mobile Devices (MDs) and the computational demands of AI services presents another significant hurdles\cite{c4,c5}. As AI models increase in size and complexity, both storage and computational requirements have escalated, making local processing increasingly difficult\cite{c6}. For example, the VGG19 model, with 143 million parameters and a storage requirement of about 600 MB, poses substantial obstacles for execution on resource-constrained MDs\cite{c7}.

To address these challenges, Edge Inference (EI) has emerged as a promising solution, enabling low-delay decision-making by deploying AI inference services on edge servers\cite{c8,c9,c10,c11}. This approach involves offloading user requests to edge servers, which collaborate with mobile devices to complete inference tasks. By partitioning AI models and distributing parts of the computation across both edge servers and mobile devices, collaborative inference helps minimize data transmission, alleviate the computational burden on mobile devices, and capitalize on the computational power of edge servers\cite{c12,c13}.

Despite its promising potential, collaborative EI faces two major challenges:
\begin{itemize}
  \item \textbf{Resource Constraints}: One of the primary obstacles is the limited storage and computational resources available on mobile devices and edge servers, especially in the context of a dynamic and diverse range of user requests\cite{c14}. Optimizing model deployment among MDs and edge servers along with user-server association strategies is crucial to overcoming these resource limitations. Model deployment involves determining how AI models are stored and accessed on edge servers, with the goal of reducing inference delays and improving service quality\cite{c15}. An effective deployment strategy optimizes model caching, reducing download and transmission times while dynamically allocating resources based on request probability and computational requirements, thus enhancing overall system responsiveness. Similarly, user association strategies focus on assigning user requests to the most suitable edge servers, balancing system load and maximizing resource utilization while minimizing transmission overhead. However, the limited resources of edge servers and the diverse nature of user requests make this optimization complex, requiring careful coordination of both model deployment and user association to fully leverage available resources and provide efficient, delay-minimized inference services.
  \item \textbf{Privacy-Communication-Computation Trilemma}: Collaborative inference introduces a trilemma in balancing privacy, communication, and computation. Specifically, model partition directly impacts the computational burden on MDs and edge servers and the communication overhead between them\cite{c16,c17}. While partitioning models helps reduce the need for MDs to transmit raw data, by sending only processed feature maps from early layers, it still exposes the system to privacy risks. These feature maps are susceptible to model inversion attacks, which could lead to privacy breaches and the compromise of sensitive data\cite{c18,c19}. To highlight this, our experiments in Fig. \ref{fig1} and \ref{fig2} measure the privacy leakage during the model inference process using data similarity metrics such as Structural Similarity Index Measure (SSIM), Peak Signal-to-Noise Ratio (PSNR), and Learned Perceptual Image Patch Similarity (LPIPS), which can illustrate the extent of privacy leakage under potential inversion attacks\cite{c18,c20}. This creates a fundamental trade-off, deeper local computations enhance privacy (lower SSIM), while shallower partitions reduce local computation at the cost of higher communication overhead and privacy risks. As a result, an effective model partition strategy must not only optimize computational and communication efficiency but also carefully manage privacy concerns.
\end{itemize}

\begin{figure}[t]
    \centering
    \subfigure[] {\includegraphics[width=1.1in,angle=0]{fig1_a.png}}
    \subfigure[] {\includegraphics[width=1.1in,angle=0]{fig1_b.png}}
    \subfigure[] {\includegraphics[width=1.1in,angle=0]{fig1_c.png}}
   \caption{Evaluation of the SSIM of images reproduced from the intermediate feature maps leaked from different layers of different models and data sets: (a) LeNet12 on CIFAR-10; (b) ResNet18 on CIFAR-100; (c) VGG13 on Caltech-101.}
    \label{fig1}
\end{figure}
\begin{figure}[t]
	\centering
	\includegraphics[width=3.3in]{fig2.png}
	\caption{Illustration of reproduced image and different metrics after different layers of VGG13.}
	\label{fig2}
\end{figure}

Building on these insights, this paper aims to strike a balance between minimizing inference delay and safeguarding user privacy, thereby enhancing the overall performance of EI. Specifically, given a edge network composed of multiple inference services based on various Deep Neural Network (DNN) models, edge servers and MDs, we propose a joint optimization approach that integrates model deployment, user association, and model partition, targeting the reduction of long-term average inference delay while adhering to resource limitations and privacy constraints. Notably, while previous research has focused on model deployment, model partition, or user association individually, to the best of our knowledge, no work has jointly optimized all three elements to minimize inference delay while respecting privacy and resource constraints.
The key contributions of this paper are as follows:
\begin{itemize}
  \item We define a novel joint optimization problem in a multi-edge environment. This problem aims to minimize long-term average inference delay while respecting the resource limitations of edge servers and privacy constraints. We also demonstrate that the optimization problem is NP-hard, highlighting the complexity of the problem and the necessity for suboptimal solutions.
  \item We propose a Lyapunov-based approach that transforms the long-term optimization problem into a single-time-slot problem, enabling effective handling of dynamic inference requests. Additionally, we introduce a coalition formation game model for  user-server association, decoupling the complex joint optimization into manageable sub-problems, which facilitates efficient suboptimal solutions through iterative optimization.
  \item We present a greedy-based algorithm for model deployment within each coalition, leveraging submodularity to provide efficient suboptimal solutions. We develop an exhaustive search method to identify the optimal partition strategy for each mobile device.
  \item  We conduct extensive simulations to evaluate the performance of proposed algorithms. Simulation results demonstrate that compared to baseline algorithms, the proposed algorithms can effectively minimize the average inference delay while meeting long-term privacy constraints.
\end{itemize}
\section{Related Work}
Extensive research efforts have been dedicated to partition-based collaborative edge inference paradigms, wherein DNN models are strategically dissected between edge servers and end-user devices to optimize computational load distribution and minimize communication overhead. For example, foundational researches such as \cite{c10,c21,c22} have established methodologies for model partition and inference optimization. For readers interested in a comprehensive overview of existing works, we recommend recent systematic surveys such as \cite{c4,c23,c24,c25}. Aligned with the scope of this work, we focus on two critical dimensions of edge-end collaborative inference: 1) model deployment/placement, or caching strategies for efficient DNN inference, and 2) privacy-aware mechanisms to mitigate data leakage risks during collaborative inference. Below, we discuss representative works in these areas.

\subsection{Model Deployment for Efficient DNN Inference}
Recent studies have made significant progress in optimizing AI deployment across edge-cloud environments. \cite{c15} develops a Lyapunov-based joint resource management scheme for IIoT systems, co-optimizing model deployment, task offloading, and resource allocation to minimize delay and error penalty while ensuring system stability. \cite{c26} introduces JointDNN, a collaborative inference engine that employs shortest path optimization and integer linear programming to enhance performance and energy efficiency between mobile devices and cloud servers. For large-scale deployments, \cite{c27} proposes MPDM, a multi-paradigm deployment model that uses heuristic algorithms to optimize accuracy, scale, and cost for deep learning inference services. Resource-constrained scenarios are addressed by \cite{c28}, which focuses on neural network optimization for edge devices, leveraging logic circuits, in-memory computing, and learning automata algorithms to improve efficiency and accuracy.

In the context of edge server deployments, \cite{c29} establishes a joint caching and inference framework for foundation models, utilizing a least context algorithm to enhance inference accuracy and reduce system costs. Automation in cloud deployment is advanced by \cite{c30}'s AutoDeep framework, which integrates Bayesian Optimization and Deep Reinforcement Learning to optimize cloud configurations and device placement, achieving significant cost reductions through techniques like probing-informed block multiplexing. For heterogeneous edge environments, \cite{c31} designs a joint optimization framework for device placement and model partition, employing evolutionary algorithms and dynamic programming to maximize throughput and minimize inference time.

Dynamic co-inference scenarios are further explored by \cite{c32}, which formulates model placement and online splitting in wireless networks as an optimal stopping problem, proposing algorithms to minimize energy and time costs. \cite{c33} addresses multi-task inference in vehicular edge computing through a share-aware joint deployment and task offloading framework, utilizing a time period-aware algorithm to reduce total response time. Finally, \cite{c34} investigates energy-optimal DNN placement in UAV-enabled edge networks, developing a Lyapunov-based online algorithm to minimize transmission delay and energy costs while stabilizing data queues.

In contrast to these studies that predominantly focus on isolated aspects of performance optimization, our work holistically investigates privacy-aware model deployment, adaptive model partition, and user-edge server association under resource and privacy constraints, a tripartite challenge unaddressed in existing literature, where privacy considerations are either oversimplified or entirely decoupled from system-level coordination.

\subsection{Privacy-Aware Collaborative DNN Inference}
Recent advancements in privacy-preserving deep learning inference for edge and IoT systems have addressed critical challenges in distributed intelligence. \cite{c35} introduces DistPrivacy, a distributed feature map allocation methodology that enhances data privacy for IoT surveillance systems through strategic data partition. Building on this foundation, \cite{c36} develops an adaptive DNN partition framework that dynamically balances privacy and performance under varying network conditions. The coalition-based approach by \cite{c37} optimizes IoT camera-edge node associations, simultaneously addressing privacy preservation, energy efficiency, and multi-view detection performance through game-theoretic optimization.

Federated learning-based architectures have gained significant attention, with \cite{c38} proposing a quality-aware framework that integrates adaptive model splitting and device association to handle resource and data heterogeneity in edge environments. For device-edge co-inference scenarios, \cite{c39} designs a deep reinforcement learning-based model splitting mechanism that achieves optimal privacy-computation tradeoffs. Extending distributed inference capabilities, \cite{c40} presents RL-DistPrivacy, which employs reinforcement learning to optimize privacy-aware deep inference in latency-sensitive IoT systems.

Recent innovations focus on enhancing both privacy and reliability: \cite{c41} develops a queue-optimized CNN distributed inference method that leverages reinforcement learning to minimize latency while ensuring customer privacy protection. Finally, \cite{c42} introduces Salted DNNs, a novel semantic rearrangement technique that uses cryptographic salt to enhance output privacy in split inference without compromising computational efficiency. Collectively, these works significantly advance the state-of-the-art in privacy-aware edge intelligence, yet they predominantly focus on isolated aspects of the privacy-performance tradeoff, leaving opportunities for more holistic frameworks that integrate adaptive model deployment optimization, user-server association and comprehensive privacy preservation mechanisms.

\section{System model and problem formulation}

\subsection{System Overview}
As shown in Fig. \ref{fig3}, we consider a hierarchical edge network composed of a cloud server, multiple edge servers and mobile devices. Within the cloud server, there resides a DNN model library that encompasses a variety of well-trained models designed for inference services. Assuming there are a total of $L$ distinct inference models, denoted as $\mathcal{L}=\{1,\dots,l,\dots,L\}$, and the data size of model $l$ is $D_{l}$. Furthermore, $M$ edge servers, collectively denoted as $\mathcal{M}=\{1,\dots,m,\dots,M\}$, can offer limited storage resources and computing capabilities for deploying and executing DNN models, thereby providing inference services for a set $\mathcal{N}= \{1,\dots,n,\dots, N\}$ of $N$ mobile devices (MDs). In this paper, a sufficiently long system time period is divided into $T$ equal-length time slots, denoted as $\mathcal{T}= \{1,\dots,t,\dots, T\}$.

The major workflow and rationale of the considered edge-end collaborative EI is as follows. At the beginning of each time slot $t$, each MD generates and sends an inference request to its associated edge server according to the user association strategy, which involves inputting its local data into a specific DNN model to obtain the inference result. Meanwhile, the cloud server allow the edge servers to download and deploy models locally in advance to reduce the inference delay. Due to the limited resources of edge servers and the dynamic inference requests, the edge server needs to optimize the model deployment strategy to deploy the popular and significant models for its MDs. Based on the service requests and resource availability of servers and MDs, then the edge server partitions the model into two partitions for each MD according to the model partition strategy: the part that includes the input layer is allocated to the MD, while the other partition remains on the server side. Notably, two special cases are allowed in this paper. If the MD has the ability to accomplish the required services or has strict privacy constraints, it may choose to download the model from the edge server for full local inference; or it can upload the whole local data to the edge server for remote inference, when facing resource scarcity and lower privacy concerns.
\begin{figure}[t]
	\centering
	\includegraphics[width=3.0in]{System1224.jpg}
	%via \DeclareGraphicsExtensions.
	\caption{An illustration of edge-end collaborative EI in an edge network.}
	\label{fig3}
\end{figure}

\subsection{Model Deployment and Association Model}
At each time slot $t$, each MD $n \in \mathcal{N}$ generates an inference service request $r_{n}(t)$, which can be described by a tuple $r_{n}(t)=\{p_{l,n}(t), d_{l,n}(t)\}$, where $p_{l,n}(t)$ represents the probability of MD $n$ requesting model $l$ at time slot $t$, which is assumed to follow a Zipf-like distribution and can be estimated as:
\begin{equation}\label{}
p_{l, n}(t)=\frac{l^{-\eta_n^{(t)}}}{\sum_{l=1}^L l^{-\eta_n^{(t)}}}, \forall n \in \mathcal{N}, l \in \mathcal{L}
\end{equation}
where $\eta_n^{(t)}$ represents the parameter of the Zipf distribution, reflecting the shape of the content popularity distribution. The larger the value of $\eta_n^{(t)}$, the more concentrated the content popularity. $d_{l,n}(t)$ is the input data size for inference service $l$, and we assume that $d_{l,n}(t)$ follows a general random distribution within the range of $[d_{l,n}^{min}, d_{l,n}^{max}]$.

At each time slot $t$, each edge server can download part of the models and deploy them locally. Suppose a binary variable $x_{l, m}(t)$ is introduced to indicate whether model $l$ is deployed on edge server $m \in \mathcal{M}$ at time slot $t$. Then, we have
\begin{equation}\label{}
x_{l, m}(t)=\begin{cases}1, & \text { if model $l$} \text { is deployed on server $m$}  \\ 0, & \text { otherwise. }\end{cases}
\end{equation}
As the storage resource of edge server is limited, the number of models that can be deployed at the server $m$ should also be constrained. Besides, since the service request may vary in different time slots, the model deployment strategy needs to be adjusted to update the model deployment status.

Besides, let the binary variable $y_{m,n}(l)$ represent the user association status between MD $n$ and server $m$ at time slot $t$. Then, we have
\begin{equation}\label{}
y_{m,n}(t)=\begin{cases}1, & \text { if MD $n$} \text { is associated with server $m$}  \\ 0, & \text { otherwise. }\end{cases}
\end{equation}
In this paper, we assume that each MD can only be served with only one server at each time slot.

\subsection{Model Partition and Privacy Evaluation Model}

If MD $n$ associates with server $m$ for model $l$, the edge server may need to partition the model into two parts, and deploy one part on the server, while the other on the MD, with a partition strategy $z_{l,m,n}(t)$. Assuming model $l$ is a chained-DNN with $A_l$ layers, a model partition strategy $z_{l,m,n}(t)$ can be represented as $z_{l,m,n}(t) \in \mathcal{A}_l=\{0,1,2,...,A_l\}$\footnote{For models of different granularities, the level of partition detail here can be adjusted, such as a block containing multiple layers, or even a sub-network branch.}. Specifically, if $z_{l,m,n}(t)=0$ means that no model partition is deployed on the MD, and inference is conducted entirely on the edge server; If $z_{l,m,n}(t)=A_l$, it signifies that the entire model is deployed on the MD for full local processing.

In this paper, we use the evaluated SSIM value at the edge server to quantify the probability of data privacy leakage at different model layers. Specifically, when the data is directly sent to the edge server and SSIM value equals to 1, it is considered that the data privacy is exposed with probability 1. Conversely, when inference is conducted solely on the local device, it indicates data privacy is compromised with probability 0. We denote the SSIM value as $s\left(z_{l,m,n}(t)\right)$, simplified as $s_{l,m,n}(t)$, which signifies that it is determined by $z_{l,m,n}(t)$.

Then, under any given model partition strategy $z_{l,m,n}(t)$, the total amount of data that may lead to privacy leakage during the inference process of MD $n$ can be represented as
\begin{equation}\label{}
\Upsilon_{n}(t)=d_{l,n}(t)s_{l,m,n}(t)
\end{equation}

In addition, it is necessary to conduct extensive preliminary experiments on the dataset using the corresponding models, to obtain the SSIM value. In particular, is there a correlation between the SSIM value and the number of model layers? If an approximate fitting function can be derived, it could be used to evaluate the SSIM value, thereby reducing the workload of preliminary experiments. To this end, we attempt to use a series of LeNet models and VGG models, plotting scatter plots and fitting curves of SSIM values against the model layer index. The results are presented in Fig. 4.

\begin{figure}[t]
    \centering
    \subfigure[] {\includegraphics[width=2.3in,angle=0]{fig4_1.eps}}
    \subfigure[] {\includegraphics[width=2.3in,angle=0]{fig4_2.eps}}
   \caption{The relationship between SSIM values and the model layer index: (a) LeNet models on CIFAR-10; (b) VGG models on Caltech-101.}
    \label{fig4}
\end{figure}

As shown in the figure, the SSIM values of both the LeNet and VGG models exhibit similar trends with respect to the model layer index. Through curve fitting, these trends can be approximated by a function of the following form:
\begin{equation}\label{}
s=\omega_1 /\left(1+\exp \left(-\omega_2(index-\omega_3)\right)\right)+\omega_4
\end{equation}
where $\omega_1$, $\omega_2$,$\omega_3$ and $\omega_4$ are constants. For example, $\omega_1=0.9031$, $\omega_2=-1.6683$, $\omega_3=4.9119$, $\omega_4=0.0983$ for LeNet models, while $\omega_1=0.6957$, $\omega_2=-0.6047$, $\omega_3=6.7718$, $\omega_4=0.3371$ for VGG models. It is worth noting that these values may vary across different datasets, but the general trend remains consistent. Furthermore, we were unable to effectively fit this trend for the ResNet models. This is because, in ResNet, the model is partitioned based on model blocks, causing the SSIM values to drop sharply and then stabilize, as shown in Fig. 1. Therefore, for other models, if a similar trend can be observed in a pre-experiment with one model, the fitted function can be used to approximate the SSIM values of other variants in the series. This approach helps reduce the workload of pre-experimentation to some extent.


\subsection{Inference Delay Model}
Depending on the joint model deployment, user associations, and model partition strategies, the delay throughout the entire inference process will vary under different scenarios. This section presents how to calculate the inference delay.

We assume at the initial time slot $t=0$, none of the models have been deployed (i.e., $x_{l,m}=0$). At the beginning of time slot $t$, the requested models need to be downloaded from the cloud server to the edge server. Denote the model downloading delay as $\tau_{l,m}^{c2e}$, which can be calculated as:
\begin{equation}\label{}
\tau_{l,m}^{c2e}(t)= \begin{cases}D_{l} / R_{m}, & x_{l, m}(t)=1 \& x_{l,m}(t-1)=0 \\ 0, & \text {otherwise.}\end{cases}
\end{equation}
where $R_{m}$ is the transmission rate from the cloud server to the edge server $m$ over the wired link.

Assuming that MD $n$ can obtain service based on model $l$ from the associated edge server (i.e., $x_{l,m}(t)=1$ and $y_{m,n}(t)=1$), with the model partition strategy $z_{l,m,n}(t)$, then the model partition that needs to be downloaded from the edge server first, and the dowload delay can be calculated as:
\begin{equation}\label{}
\tau^{d}_{l,m,n}(t)=\frac{D^{md}(z_{l,m,n}(t))}{R_{n,m}(t)}
\end{equation}
where $D^{md}\left(z_{l,m,n}(t)\right)$ is the data size of the model partition deployed at the MD. $R_{n,m}(t)$ is the transmission rate between MD $n$ and its associated edge server $m$, which can be calculate as:
\begin{equation}\label{}
R_{m,n}(t)=B_{n,m}(t) \log _2\left(1+\frac{{p}_{n,m}d_{n,m}(t)^{-\vartheta}}{n_0 {B}_{n,m}(t)}\right)
\end{equation}
where $B_{n,m}(t)=B_{m}(t)/N_{m}(t)$ denotes the bandwidth allocated to the associated MDs by server $m$, $B_{m}(t)$ is total available bandwidth, and $N_{m}(t)=\sum_{n=1}^{N}y_{m,n}(t)$ is the total number of associated MDs. ${p}_{n,m}$ is the transmit power allocated to MD $n$, $d_{n,m}(t)$ is the distance between server $m$ and MD $n$. $n_{0}$ is the power spectral density of the additional Gaussian white noise.

Then, the inference process is initiated by MD $n$, the local computation delay on the MD can be represented as:
\begin{equation}\label{}
\tau^{p,0}_{l,m,n}(t)= \frac{d_{l,n}(t)F^{md}\left(z_{l,m,n}(t)\right)}{f_{n}}
\end{equation}
where $F^{md}\left(z_{l,m,n}(t)\right)$ represents the unit computational load for inference that the MD undertakes under the partitioning strategy $z_{l,m,n}(t)$, and $f_{n}$ is the computation capability of MD $n$ at time slot $t$.

Once the inference is completed on the MD side, the intermediate results (feature maps) output by the partitioned layer are sent to the server. The required transmission delay can be calculated as:
\begin{equation}\label{}
\tau^{u}_{l,m,n}(t)= \frac{I\left(z_{l,m,n}(t)\right)}{R_{n,m}(t)}
\end{equation}
where $I\left(z_{l,m,n}(t)\right)$ represents the data size of the intermediate results output by the partitioned layer under partition strategy $z_{l,m,n}(t)$, and $R_{m,n}(t)$ is the uplink transmission rate.

After the server receiving the intermediate output results, it will continue to perform inference on the remaining model partition, and the processing delay is:
\begin{equation}\label{}
\tau^{p,1}_{l,m,n}(t)= \frac{d_{l,n}(t)F^{s}\left(z_{l,m,n}(t)\right)}{f_{n,m}(t)}
\end{equation}
where $F^{s}\left(z_{l,m,n}(t)\right)$ represents the unit computational load that the server side undertakes under the partition strategy $z_{l,m,n}(t)$, and $f_{n,m}(t)$ is the computational resource allocated to the MD $n$ for inference service $l$. Similarly, we assume that the edge server equally allocate its computational resources among the associated MDs:
\begin{equation}\label{alloComp}
f_{n,m}(t)=\frac{f_{m}(t)}{N_{m}}
\end{equation}
where $f_{m}(t)$ is the total computational resources available of edge server $m$ at time slot $t$, and $N_{m}$ denote the total number of MDs requesting resource allocation.

Once edge servers have completed the inference and obtained the results, they can return these outcomes to the MDs. However, due to the relatively small size of the data associated with the inference results, the transmission latency for this part can be considered negligible, the total inference delay experienced in the aforementioned process can be calculated as:
\begin{equation}\label{}
\begin{aligned}
& \tau_{l,m,n}(t)=x_{l,m}(t)y_{m,n}(t)\left(\tau_{l,m}^{c2e}(t)+\tau^{d}_{l,m,n}(t)+ \right. \\
& \left. \tau^{p,0}_{l,m,n}(t)+\tau^{u}_{l,m,n}(t)+\tau^{p,1}_{l,m,n}(t)\right)
\end{aligned}
\end{equation}

However, edge server $m$ selected by MD $n$ may decide not to deploy model $l$ (i.e., $x_{l,m}(t)=0$ and $y_{m,n}(t)=1$). In this case, we assume that the edge server will forward the request to the cloud server, and the MD will experience a considerable delay $\tau_{n}^{extra}(t)$ (much larger than the edge and local inference delay) due to the long distance transmission. For example, the extra delay can include the transmission delays occurring when MDs upload inference data to cloud server after model deployment is completed, as well as the delay in returning inference results. Besides, we no longer consider data privacy in this scenario, as cloud services may offer more resource-intensive privacy protection protocols, which are beyond the scope of this paper.

Therefore, the final inference delay when MD $n$ associated with edge server $m$ under arbitrary model partition strategy $z_{l,m,n}$ can be expressed as:
\begin{equation}\label{}
\tau_{n}(t) = \tau_{l,m,n}(t)+y_{m,n}(t) \left(1-x_{l,m}(t)\right)\tau_{n}^{extra}(t)
\end{equation}

Finally, the total inference delay of all MDs can be denoted as:
\begin{equation}\label{}
\tau(t) = \sum_{n=1}^{N}\tau_{n}(t)
\end{equation}


\subsection{Problem Formulation and Hardness}
This paper aims to enhance the inference delay performance at the systemic level while simultaneously reducing the risk of inference data privacy leakage, thereby achieving a secure and efficient inference service framework. Furthermore, due to the stochastic and dynamic of inference service requests, optimizing any individual time slot alone cannot guarantee the long-term stability of the system. Therefore, this paper decides to minimize the long-term average inference delay under sustained privacy leakage constraints by jointly optimizing the model deployment strategy $\mathcal{X}(t)=\left\{x_{l,m}(t) \mid l \in \mathcal{L}, m \in \mathcal{M}\right\}$, association strategy $\mathcal{Y}(t)=\left\{y_{m,n}(t) \mid m \in \mathcal{M}, n \in \mathcal{N} \right\}$, and model partition strategy $\mathcal{Z}(t)=\left\{z_{l,m,n}(t) \mid l \in \mathcal{L}, m \in \mathcal{M}, n \in \mathcal{N} \right\}$. The formulated optimization problem can be expressed as follows:

\begin{equation}\label{p1}
\begin{aligned}
\mathcal{P}_1: & \min _{\mathcal{X}(t), \mathcal{Y}(t), \mathcal{Z}(t)} \quad \lim _{T \rightarrow \infty} \frac{1}{T} \sum_{t=0}^{T-1} \mathbb{E}[\tau(t)] \\
\text { s.t. } & C_1: x_{l,m}, y_{m,n}(t) \in\{0, 1\}, \forall l \in \mathcal{L}, \forall m \in \mathcal{M}, \forall n \in \mathcal{N} \\
& C_2: z_{l,m,n}(t) \in \{0,1,2,...,A_l\} , \forall l \in \mathcal{L}, \forall m \in \mathcal{M}, \forall n \in \mathcal{N} \\
& C_3: \sum_{l=1}^{L}x_{l, m}(t)D_l \leq C_{m}(t)\\
& C_4: D^{md}\left(z_{l,m,n}(t)\right) \leq D_{l} \\
& C_5: \sum_{m=1}^{M} y_{m,n}(t) =1 \\
& C_6: \sum_{n=1}^{N}f_{n,m}(t) = f_{m}(t) \\
& C_7: \sum_{n=1}^{N}B_{n,m}(t) = B_{m}(t) \\
& C_8: \lim _{T \rightarrow \infty} \frac{1}{T} \sum_{t=0}^{T-1} \mathbb{E}[\Upsilon_{n}(t)] \leq \bar{\Upsilon}_{n}
\end{aligned}
\end{equation}
where constraint C3 ensures that the total data size of model deployed on edge server $m$ does not exceed its storage capacity $C_{m}(t)$. Constraint C4 ensures that the model partition deployed on the MD side is not larger than the entire model. Constraint C5 restricts that the MD can and must be associated with exactly one edge server within a single time slot. Constraint C6 and C7 ensure that the total computational resources and bandwidth allocated by the edge server do not exceed its maximum available capacity. Constraint C8 ensures that the each MD's long-term average data privacy leakage does not exceed the threshold $\bar{\Upsilon}_{n}$.

It is clear that the problem constituted above is a long-term constrained optimization problem and it is straightforward to obtain that it is an NP-hard problem. This is because the impact of model partition on the problem is difficult to express in terms of any deterministic continuous function. Next, we also prove that problem $P_1$ is an NP-hard problem even in a single time slot. i.e., without constraint C7, and we give the following Theorem 1.

\begin{myTheo}\label{theo1}
The proposed problem (\ref{p1}) is NP-hard in a single time slot.
\end{myTheo}

\begin{proof}
To do this proof, we first introduce a well-known NP-hard problem, the Set Covering Problem (SCP). In SCP, we are given: A finite set $U=\{u_1,u_2,\dots,u_n\}$ of elements that needs to be covered, and a collection of subsets $S=\{S_1,S_2,\dots, S_m\}$, where each subset $S_j \subseteq U$. The goal of SCP is to select the minimum number of subsets from $S$ such that their union covers the entire set $U$, i.e., every element in $U$ is contained in at least one chosen subset from $S$. Next, we show how to the formulated problem to the SCP problem. First, we disregard the model partition strategy between the MD and the edge server, i.e., considering only the joint model deployment and association strategies, and reduce the problem into a model deployment problem. In this problem, each MD has specific model requirements. We map each element $u_i$ in the SCP instance to a model requirement $M_k$ for MD $u_k$. Therefore, the set $U$ in SCP represents all the model requirements that need to be satisfied for each MD in the model deployment process. Besides, each subset $S_j$ in the SCP instance represents a collection of models that could potentially be deployed on a particular edge server $s_j$. Thus, the collection $S$ corresponds to the set of possible model deployments across the available servers. For any server $s_j$, the subsets $S_j \subseteq U$ includes all models that can be deployed on $s_j$ given its capacity constraints. In SCP, the objective is to minimize the number of subsets chosen to cover $U$. In the model deployment process, we aim to minimize the average delay by deploying models on servers such that MDs’ requirements are covered with minimal delay costs. Therefore, this objective can be interpreted as a form of set covering where we attempt to find a minimal subset of servers that can fully satisfy all user model requirements. Choosing fewer servers (while covering all inference requests) is beneficial, as this minimizes the server load and inference delay. Thus, it indicates that the reduced model deployment problem is a instance of SCP problem and is NP-hard, which also proves the proposed problem (\ref{p1}) is NP-hard in a single time slot.
\end{proof}

Since Theorem \ref{theo1} can be regarded as a special case when $T=1$, we can also prove that problem (\ref{p1}) is NP-hard.

\section{LYAPUNOV-Based Problem Transformation, Decomposition, and Solution Design}
In this section, we initially transform the long-term optimization problem into a single-time-slot optimization problem based on Lyapunov optimization theory, thereby mitigating the impact of long-term constraint conditions. Subsequently, considering the distinct impacts of three different optimization strategies on the optimization objective, we decouple the problem for optimization. We first formulate the association problem between the MD and edge server as a coalition game problem. Then, we address the model deployment and partition optimization problem under a given coalition structure. Finally, through iterative optimization, we obtain an efficient suboptimal solution for the original problem, and analyze the properties of the proposed algorithms.

\subsection{Lyapunov-Based Problem Transformation}
In problem (\ref{p1}), constraint C7 is used to stabilize the time-averaged maximum data privacy leakage. However, this approach requires complete information of the system for every time slot, which is not feasible to acquire in advance in real-world systems. To address this issue, we introduce accumulated data privacy leakage in Definition 1.
\begin{myDef}\label{def1}
Let $\Xi(t+1)$ denote the accumulated data privacy leakage that exceeds the data privacy threshold over $t$ time slots, which can be calculated as
\begin{equation}\label{}
\Xi(t+1) =\left[\Xi(t)+ \sum_{n=1}^{N}\left(\Upsilon_n(t)-\bar{\Upsilon}_n \right)\right]^{+}
\end{equation}
\end{myDef}
where $\Xi(0)=0$ denote the initial value of the accumulated data privacy leakage. A higher value of $\Xi(t)$ indicates that the data privacy leakage caused by inappropriate strategy-making exceeds the time-averaged privacy constraint. Based on Definition \ref{def1}, we can transform the long-term constraint C7 in problem (\ref{p1}) as follows
\begin{equation}\label{}
\lim _{T \rightarrow \infty} \frac{1}{T} \sum_{t=0}^{T-1} \mathbb{E}\left[\Xi(t)\right] \leq 0
\end{equation}

Then, we introduce the most widely applied Lyapunov function ${L}(\Xi(t)) \triangleq \frac{1}{2} \Xi^2(t)$, to measure the satisfaction status of the long-term privacy constraint. Thus, a smaller value of ${L}(\Xi(t))$ means a better data privacy satisfaction. To ensure the value of ${L}(\Xi(t))$ is small enough to meet long-term privacy constraints, we define $\Delta(\Xi(t))$ below by introducing Lyapunov drift function:
\begin{equation}\label{ldrift}
\Delta(\Xi(t)) \triangleq \mathbb{E}[L(\Xi(t+1))-L(\Xi(t)) \mid \Xi(t)]
\end{equation}
By expanding the above expression further, we can obtain that:
\begin{equation}\label{drift}
\begin{aligned} & \Delta(\Xi(t))=\frac{1}{2} \mathbb{E}\left[\Xi^2(t+1)-\Xi^2(t) \mid \Xi(t)\right] \\ & =\mathbb{E}\left[\Xi(t) \sum_{n=1}^{N}\left(\Upsilon_n(t)-\bar{\Upsilon}_n \right) \mid \Xi(t)\right]+ \\& \frac{1}{2} \mathbb{E}\left[ \sum_{n=1}^{N}\left(\Upsilon_n(t)-\bar{\Upsilon}_n \right)^2 \mid \Xi(t)\right]\end{aligned}
\end{equation}

Next, within the Lyapunov drift-penalty framework, we incorporate equation (\ref{drift}) into the optimization objective to strike a balance between minimizing inference delay and ensuring long-term privacy constraints, as shown below:
\begin{equation}\label{newobj}
\mathbb{E}\left[\tau(t)\mid \Xi(t) \right]+\Delta\left(\Xi(t)\right)
\end{equation}

According to (\ref{ldrift}), the calculation of $\Delta(\Xi(t))$ requires the information of $L(\Xi(t+1))$, which will not be available in each time slot $t$. Thus, we tend to obtain the upper bound of (\ref{newobj}) that can be calculated only based on the available information in time slot $t$. Firstly, it can be easily deduced that $\Upsilon_{n}(t)$ will be no lower than 0, and $\sum_{n=1}^{N}\left(\Upsilon_n(t)-\bar{\Upsilon}_n \right)$ will be no higher than $\sum_{n=1}^{N}\bar{\Upsilon}_{n}^2$. Then, define a constant $\Theta =\frac{1}{2}\sum_{n=1}^{N}\bar{\Upsilon}_{n}^2$, the upper bound of $\Delta\left(\Xi(t)\right)$ can be represented as:
\begin{equation}\label{upper}
\Delta\left(\Xi(t)\right) \leq \Xi(t) \cdot \mathbb{E}\left[\sum_{n=1}^{N}\left(\Upsilon_n(t)-\bar{\Upsilon}_n\right) \mid \Xi(t)\right]+\Theta
\end{equation}

Based on (\ref{upper}), the upper bound of (\ref{newobj}) can be deduced as:
\begin{equation}\label{upperbound}
\begin{aligned}
& \mathbb{E}\left[\tau(t)\mid \Xi(t) \right]+\Delta\left(\Xi(t)\right) \leq \\
&  \mathbb{E}\left[\tau(t)\mid \Xi(t) \right]-\Xi(t) \cdot \mathbb{E}\left[\sum_{n=1}^{N}\left(\bar{\Upsilon}_n-{\Upsilon}_n(t)\right) \mid \Xi(t)\right]+\Theta
\end{aligned}
\end{equation}

Based on (\ref{upperbound}), Problem $\mathcal{P}_1$ can be approximated only rely on the information in each current time slot. Thus, it can be solved by finding the solution to the following problem $\mathcal{P}_2$ that minimize the upper bound in each time slot over $T$:
\begin{equation}\label{p2}
\begin{aligned}
& \mathcal{P}_2: \min _{\mathcal{X}(t), \mathcal{Y}(t), \mathcal{Z}(t)} \alpha \cdot \tau(t)-\Xi(t) \cdot \sum_{n=1}^{N}\left(\bar{\Upsilon}_n-{\Upsilon}_n(t)\right)+\Theta\\
& \text { s.t. } \quad \text { Constraints : }\left(C_1\right)-\left(C_7\right)
\end{aligned}
\end{equation}
where $\alpha$ is a positive constant that is used to achieve the tradeoff between delay minimization and satisfaction status of the long-term privacy constraints.

\subsection{Coalition Formation Game Model for Edge Server Association}
Using the Lyapunov optimization theory, we previously reformulated the problem into a single-slot optimization problem. However, as this problem is NP-hard, finding an optimal solution at a larger scale is exceedingly time-consuming. Therefore, we aim to find an efficient suboptimal solution for the above problem. Furthermore, by analyzing the interdependencies among the three optimization strategies, it can be discerned that the model partition strategy depends solely on the relationship between a specific edge server and its associated MDs, independent of other edge servers and MDs. Similarly, the model deployment strategy for an edge server is contingent upon the association strategy between the MDs and the server. Consequently, we first conceptualize MDs and edge servers as a coalition formation game in a cooperative game framework, then optimize model deployment and partition strategies within each coalition\cite{xiaTPDS}.

\subsubsection{Coalition Formation Game Model}
Under any given model deployment and model partition strategies, associating $N$ MDs with $M$ edge servers can be regarded as a coalition game involving $M$ coalitions. Initially, we provide the fundamental definitions concerning coalition games.
\begin{myDef}\label{def2}
The MD-server association coalition formation game is denoted by $\left(\mathcal{N}, U, \mathcal{F}\right)$, where $\mathcal{N}$ denote the $N$ MDs as the players, and $U$ is a mapping that determines the utility of the coalitions. $\mathcal{F}=\{\mathcal{F}_1,\mathcal{F}_2,\dots, \mathcal{F}_M\}$ denote the coalition set of $M$ mutually disjoint coalitions, where $\cap_{m=1}^M \mathcal{F}_m=\emptyset$ and $\cup_{m=1}^M \mathcal{F}_m=\mathcal{N}$. The strategy of each player is to joint or leave the coalition to improve the coalition utility, or exchange to improve the total utility.
\end{myDef}

\begin{myDef}\label{def3}
For coalition $F_m$ in coalition structure $\mathcal{F}$, we define the following function to characterize the coalition utility of $\mathcal{F}_m$:
\begin{equation}\label{}
U(\mathcal{F}_m) = \frac{-\alpha \sum \limits_{n \in F_m} \tau_n+\Xi(t) \cdot \sum \limits_{n \in \mathcal{F}_m}\left(\bar{\Upsilon}_n-{\Upsilon}_n(t)\right)-\Theta}{|\mathcal{F}_m|}
\end{equation}
where $|\mathcal{F}_m|$ denote the number of MDs in $\mathcal{F}_m$.
\end{myDef}

Based on Definition \ref{def3}, each coalition aims to minimize the average inference delay and privacy leakage by allocating the MDs into different coalitions, which also aligns with the original optimization objective. For any partition of $\mathcal{F}=\{\mathcal{F}_1,\mathcal{F}_2,\dots, \mathcal{F}_M\}$, the social welfare of the game is
\begin{equation}\label{}
U(\mathcal{F}) = \sum_{m=1}^{M} U(\mathcal{F}_m)
\end{equation}

To ensure that players can join a coalition that yields higher utility, it is necessary to define the players' preferences for different coalitions. Thus, we define the preference order for player $n$.
\begin{myDef}
For any player $n \in \mathcal{N}$, the preference order $\succ_{n}$ is defined as a complete, transitive, and reflexive binary relation over the set of all partitions that player $n$ can possibly form. Give two partitions $\mathcal{F}$ and $\mathcal{F}^{\prime}$, we say player $n$ prefers $\mathcal{F}$ over $\mathcal{F}^{\prime}$ if and only if:
\begin{equation}\label{}
\mathcal{F} \succ_{n} \mathcal{F}^{\prime} \Leftrightarrow U(\mathcal{F}) > U(\mathcal{F}^{\prime})
\end{equation}
\end{myDef}

Based on the above definition, players establish preferences across all possible coalitions, and each player selects the coalition that enhances the total utility of the newly formed coalitions according to the preference order. Thus, we further define the switch operation for forming the final coalitions as follows.
\begin{myDef}
Switch operation: Given a partition $\mathcal{F}=\{\mathcal{F}_1,\mathcal{F}_2,\dots, \mathcal{F}_M\}$ of $\mathcal{N}$, for player $n \in \mathcal{F}_m$, when and only when $\mathcal{F}_{m^{\prime}} \succ_{n} \mathcal{F}_m$ ($m^{\prime}\neq m$) is achieved, a switch operation moves player $n$ from $\mathcal{F}_m$ to $\mathcal{F}_{m^{\prime}}$, then $\mathcal{F}_m$ will be replaced by $\mathcal{F}_{m^{\prime}}$, where $\mathcal{F}_{m^{\prime}}=(\mathcal{F}\setminus\{\mathcal{F}_m, \mathcal{F}_{m^{\prime}}\})\cup\{ \mathcal{F}_m\setminus\{n\},\mathcal{F}_{m^{\prime}}\cup \{n\} \}$.
\end{myDef}

In coalition formation game, the coalition structure may trap in local optimum if only single-player switch operation is conducted. Therefore, in addition to the operation where a single player leaves to join another coalition, players from two distinct coalitions can also directly engage in exchange operations to achieve coalition updates, thereby attaining greater total utility. Thus, we define the coalition exchange operation as follows.
\begin{myDef}
Exchange operation: Given a partition $\mathcal{F}=\{\mathcal{F}_1,\mathcal{F}_2,\dots, \mathcal{F}_M\}$ of $\mathcal{N}$, two different players $n \in \mathcal{F}_{m}$, and $ n^{\prime} \in \mathcal{F}_{m^{\prime}}$, they perform exchange operations, i.e., $n$ moves from $\mathcal{F}_{m}$ to $\mathcal{F}_{m^{\prime}}$, $ n^{\prime}$ moves from $\mathcal{F}_{m^{\prime}}$ to $\mathcal{F}_{m}$, when and only when $\mathcal{F}_{m^{\prime}} \succ_{n,n^{\prime}} \mathcal{F}_m$, and the partition $\mathcal{F}$ is replaced by $\mathcal{F}^{\prime}$, where $\mathcal{F}^{\prime}=\mathcal{F} \backslash\left\{F_m, F_{m^{\prime}}\right\} \cup\left\{F_m \backslash\{n\} \cup\left\{n^{\prime}\right\}, F_{m^{\prime}} \backslash\left\{n^{\prime}\right\} \cup\{n\}\right\}$

\end{myDef}

\subsubsection{Algorithm for Coalition Formation Game}
Following the definitions of coalition switching and coalition exchange previously outlined, MDs are able to iteratively modify their associated edge servers through a sequence of steps. Within each step, only one MD can switch its associated server or two MDs in different coalitions can exchange their servers. After coalition change, the joint model deployment and partition strategies are optimized based on the further proposed Algorithm 2 and 3, respectively. Then, calculate the coalition utility for the coalitions. However, due to the complexity introduced by the switch operation, the proposed algorithm performs an exchange operation only after $K$ switch operations. The value of $K$ is proportional to the problem scale. When $M$ and $N$ are large, a larger $K$ can effectively reduce the number of exchange operations, thereby lowering the algorithm's time complexity. In some cases, exchange operations can even be entirely omitted, allowing the algorithm to efficiently obtain a suboptimal solution. By repeating the above steps until the final partition structure reaches Nash-stable, we summarize the proposed distributed algorithm in Algorithm 1. The convergence, stability and complexity of the algorithm is analyzed in the next subsection.

\begin{algorithm}[!htbp]\label{Alg1}
	\caption{Coalition Formation Game-Based Algorithm}
	\begin{algorithmic}[1]
    \State Initializes a random partition $\mathcal{F}_{init}=\{\mathcal{F}_1,\mathcal{F}_2,\dots, \mathcal{F}_M\}$: Each MD associate the edge server based on the given partition;
    \State Set the current partition $\mathcal{F}_{cur}$ as $\mathcal{F}_{cur} = \mathcal{F}_{init}$;
    \State Set number of iteration $t=1$;
    \Repeat
    \State Randomly select a MD $n$ and its coalition $\mathcal{F}_{m} \in \mathcal{F}_{cur}$, and randomly select another coalition $\mathcal{F}_{m^{\prime}} \in \mathcal{F}_{cur}$, $m\neq m^{\prime}$;
    \State Set a temp partition $\mathcal{F}_{temp}=\{\mathcal{F}_{m}\setminus\{n\}, \mathcal{F}_{m^{\prime}}\cup\{n\}\}$;
    \State Obtain the new partition $\mathcal{F}_{new}=(\mathcal{F}_{cur}\setminus\{\mathcal{F}_{m}, \mathcal{F}_{m^{\prime}}\})\cup \mathcal{F}_{temp}$;
     \State Optimize the model deployment strategy via \textbf{Algorithm 2}, and model partition strategy in \textbf{Algorithm 3} for $\mathcal{F}_{cur}$ and $\mathcal{F}_{new}$;
     \State Calculate the coalition utility of $\mathcal{F}_{cur}$ and $\mathcal{F}_{new}$;
    \If{$\mathcal{F}_{new} \succ_{n}  \mathcal{F}_{cur}$}
        \State $\mathcal{F}_{cur} =\mathcal{F}_{new}$ ;
    \EndIf
    \If{$t\% K=0$}
        \State Randomly select two MDs $n$, $n^{\prime}$ from two different coalitions $\mathcal{F}_{m}$ and $\mathcal{F}_{m^{\prime}}$;
        \State Set a temp partition $F_{temp}=\{F_m\setminus \{n\} \cup \{n^{\prime}\}, F_{n^{\prime}} \setminus \left\{n^{\prime}\right\} \cup\{n\}\}$;
        \State Obtain the new partition $\mathcal{F}_{new}=(\mathcal{F}_{cur}\setminus\{\mathcal{F}_{m}, \mathcal{F}_{m^{\prime}}\})\cup \mathcal{F}_{temp}$;
         \State Optimize the model deployment strategy via \textbf{Algorithm 2}, and model partition strategy \textbf{Algorithm 3} for $\mathcal{F}_{cur}$ and $\mathcal{F}_{new}$;
          \State Calculate the coalition utility of $\mathcal{F}_{cur}$ and $\mathcal{F}_{new}$;
         \If{$\mathcal{F}_{new} \succ_{n,n^{\prime}}  \mathcal{F}_{cur}$}
        \State $\mathcal{F}_{cur} =\mathcal{F}_{new}$ ;
    \EndIf
    \EndIf
    \State $t=t+1$;
     \Until{The final partition reaches Nash-stable.}
	\end{algorithmic}
\end{algorithm}

\subsection{Problem Decomposition within Each Coalition}
After obtaining a given coalition structure, that is, the association pattern between MDs and edge servers, it is necessary to optimize the model deployment strategy and model segmentation strategy, so as to obtain higher coalition utility. Revisiting the optimization objective of Problem $\mathcal{P}_2$, it becomes evident that after given server association decisions and model partition decisions, the model deployment problem for each edge server and its associated users can be transformed into a knapsack problem, and can be solved with a greedy-based algorithm. Then, the optimal model partition strategy can be made for each MD via an exhaustive search method for DNN with finite number of layers.

\subsubsection{Greedy-Based Algorithm for Model Deployment Problem}
Under the given edge association and model partition strategies,  the optimization goal of the model deployment problem is to minimize the average delay and privacy leakage of all MDS under the constraint of limited model storage space by deploying required models, which has been proved to be NP-hard according to Theorem \ref{theo1}. To derive a efficient suboptimal solution, we first give the following Theorem. prove
\begin{myTheo}\label{theo3}
The model deployment subproblem is a submodular problem within each coalition under given model partition strategy.
\end{myTheo}
\begin{proof}
A function $f: 2^{\mathcal{L}} \rightarrow \mathbb{R}$ is submodular if, for any sets $\mathcal{A} \subseteq B \subseteq \mathcal{L}$ and any element $x \subseteq \mathcal{L}\setminus B$, the marginal gain of adding $x$ to $A$ is at least as great as the marginal gain of adding $x$ to $B$. Formally, this means:
\small
\begin{equation}\label{}
\Delta f(A, x)=f(A \cup\{x\})-f(A) \geq f(B \cup\{x\})-f(B)=\Delta f(B, x)
\end{equation}

Let $ V \subseteq \mathcal{L}$ be the set of models deployed on an edge server, where $L$ is the set of all available models. Define the utility function $f(V)$ to represent the total utility of the edge server to its associated MDs, which can be represented as:
\begin{equation}\label{}
f(V)=\sum_{i \in \mathcal{Q}} U_i(V)
\end{equation}
where $\mathcal{Q}$ is the set of MDs associated with the edge server, and $U_{i}(V)$ is the utility of MD $i$ when the server has deployed model set $V$. For each MD $i$, the utility $U_{i}(V)$ depends inversely on the inference delay as formulated in $\mathcal{P}_2$. The rationale behind this is that based on the optimization objective of problem $\mathcal{P}_2$, the model deployment strategy only influences the inference delay experienced by MDs, when the model partition strategy is fixed within the coalition. Thus, as inference delay decreases, the utility increases.

For a set $V \subseteq \mathcal{L}$, define the marginal utility gain from adding model $x$ to $S$ as:
\begin{equation}\label{}
\Delta f(V, x)=f(V \cup\{x\})-f(V)
\end{equation}

As the communication, computation and storage resources are limited, deploying more models on the server reduce the resources available for each MD according to (\ref{alloComp}), resulting in diminishing marginal utility gains. Therefore, the marginal utility gain $\Delta f(V, x)$ decreases as the set $V$ grows. Since $U_{i}(V)$ is associated with inference delay, it is a non-decreasing function with respect to $V$. For any sets $V \subseteq T \subseteq \mathcal{L}$ and model $x \in \mathcal{L} \setminus T$, $U_{i}(V)$ satisfies the diminishing marginal returns property:
\begin{equation}\label{}
U_i(V \cup\{x\})-U_i(V) \geq U_i(T \cup\{x\})-U_i(T)
\end{equation}

Based on the above, the marginal gain in $f$ from adding $x$ to $V$ can be expressed as:
\begin{equation}\label{}
\Delta f(V, x)=\sum_{i \in \mathcal{Q}}\left(U_i( V \cup\{x\})-U_i(V)\right)
\end{equation}
and similarly,
\begin{equation}\label{}
\Delta f(T, x)=\sum_{i \in \mathcal{Q}}\left(U_i(T \cup\{x\})-U_i(T)\right)
\end{equation}

Summing over all MDs $i \in \mathcal{Q}$, we have:
\begin{equation}\label{}
\sum_{i \in \mathcal{Q}}\left(U_i(V \cup\{x\})-U_i(V)\right) \geq \sum_{i \in \mathcal{Q}}\left(U_i(T \cup\{x\})-U_i(T)\right)
\end{equation}
which implies that:
\begin{equation}\label{}
\Delta f(V, x) \geq \Delta f(T, x)
\end{equation}

The inequality above holds for any sets $V \subseteq T \subseteq \mathcal{L}$, and any elements $x \in \mathcal{L}\setminus T $, thereby satisfying the definition of submodularity. Hence, we have shown that $f(V)$ is a submodular function, and model deployment subproblem is a submodular problem within each coalition under given model partition strategy.
\end{proof}

\begin{algorithm}[t]\label{Alg-2}
	\caption{Greedy-Based Algorithm for Model Deployment}
	\begin{algorithmic}[1]
    \Require $\mathcal{L}$, $\mathcal{F}_m$, $C_m$, $\{D_l\}$, $\{p_{l,n}\}$
    \Ensure $\{x_{l,m}\}$
    \State Initialize $V = \emptyset$, $f(V)=0$, remaining storage $C^{\prime}=C_m$;
    \While{$C^{\prime} >0 $ and $\mathcal{L} \setminus V \neq \emptyset$}
        \For{each $j \in \mathcal{L} \setminus V$}
           \State Calculated the total request probability for $j$: $p_{j}=\sum_{n \in \mathcal{F}_m}p_{j,n}$;
            \State Calculate the marginal gain $\Delta f(V, j)$ and weighted utility-to-storage ratio $\varpi_j$ as follows:
            \Statex    \quad\quad\quad$ \Delta f(V, j)= f(V \cup\{j\})-f(V)$;
            \Statex     \quad\quad\quad $ \varpi_j=p_{j} \Delta f(V, j) /D_j$;
        \EndFor
        \State Select $j^*=\arg \max _{j \in \mathcal{L}\setminus V, D_j \leq C^{\prime}}: \varpi_j$;
            \If{$j$ exists}
            \State $V = V \cup \{j^*\}$;
            \Statex \quad\quad\quad $f(V) = f\left(V \cup \{j^*\}\right)$;
            \Statex \quad\quad\quad $C^{\prime}=C_m-D_{j^*}$;
             \Else
             \State Break;
            \EndIf
     \EndWhile
     \State Set $x_{l,m}$ according to $V$.
	\end{algorithmic}
\end{algorithm}

As we have proved that the model deployment subproblem is submodular, we can propose a greedy-based algorithm for the model deployment problem, which guarantees an approximate solution with an approximation ratio of $1-\frac{1}{e}$, where $e$ is the Euler's constant\cite{xiaTPDS}. We propose a greedy-based algorithm for model deployment in Algorithm 2. The algorithm follows a series of steps, and we first calculate the marginal gain and weighted utility-to-storage ratio $\varpi_j$. This ratio is introduced to quantify the \textit{cost-effectiveness} of each model, that is, the utility gained from deploying each model relative to its storage resource consumption under limited storage resources. Based on this, the algorithm prioritizes deploying models that have a high request probability and can significantly enhance the system utility, continuing this process until the server's storage capacity is exhausted. In the algorithm, it has a general time complexity of $\mathcal{O}(L)$ to calculate the weighted utility-to-storage ratio, and $\mathcal{O}(LlogL)$ for sorting, and $\mathcal{O}(L)$ for greedy selection. Therefore, the general time complexity of Algorithm 2 is $\mathcal{O}(LlogL)$.

\subsubsection{Optimal Algorithm for Model Partition Problem}
After obtaining the model deployment strategy within the given coalition, the goal of the model partition subproblem is to select a partition layer for each model such that the optimization objective of $\mathcal{P}_2$ is minimized. Since the number of partition layers in a model is finite and decreases as the granularity of partition is increased, we can find the optimal value by a simple exhaustive search. Algorithm 3 summarizes the optimal model partition algorithm.
When independently searching for the optimal model partition strategy for each MD, the general time complexity of Algorithm 3 is $\mathcal{O}(A)$, where $A$ represents the number of candidate partition layers in the model.

\begin{algorithm}[t]\label{Alg-3}
	\caption{Optimal Algorithm for Model Partition}
	\begin{algorithmic}[1]
    \Require $L$, $\mathcal{F}_m$, $\{x_{l,m}\}$, $\{\mathcal{A}_l\}$, $\{D_l\}$
    \Ensure $\{y_{l,m,n}\}$
    \State Optimal partition selection $y_{l,m,n}^*=-1$, optimal value $Obj=+\infty$;
        \For{each $x_{l,m}=1$}
                \For{each $y_{l,m,n}=0:A_l$}
                    \State Calculated the optimization objective $Obj(y_{l,m,n})$ of problem $\mathcal{P}_2$ for $y_{l,m,n}$;
                    \If{$Obj(y_{l,m,n}) < Obj$}
                        \State $Obj=Obj(y_{l,m,n})$, $y_{l,m,n}^*=y_{l,m,n}$;
                    \EndIf
               \EndFor
               \State Save $y_{l,m,n}$ into $\{y_{l,m,n}\}$;
        \EndFor
	\end{algorithmic}
\end{algorithm}

\subsection{Algorithm Property Analysis}
In this part, we analyze the convergence, stability and complexity of the proposed
We first analyze the convergence of the proposed algorithm.
\begin{myTheo}\label{theo3}
The proposed coalition formation game-based algorithm in Algorithm 1 always converges to a final partition $\mathcal{F}_{final}$ with regardless the initial partition structure.
\end{myTheo}
\begin{proof}
In the defined coalition game, the number of MDs and edge servers are finite, and the maximum number of coalitions is $M$, is also finite. We assume $\mathcal{F}_{t}$ is the partition obtained in Algorithm 1 at iteration $t$, and  $\mathcal{F}_{t+j}$ is the partition obtained at iteration $t+j$. If $\mathcal{F}_{t} \neq \mathcal{F}_{t+j}$, it indicates that with additional $j$ steps of iteration it is possible to achieve a new partition structure that enhances the overall system utility. Given that the number of MDs and edge servers are finite, and the total number of coalitions is $M$, starting from any initial partition structure, a final partition structure $\mathcal{F}_{final}$ will inevitably be reached; If $\mathcal{F}_{t} = \mathcal{F}_{t+j}, \exists t, t+j$, at this point, two cases arise. First, both $\mathcal{F}_{t}$ and $\mathcal{F}_{t+j}$ represent the final converged partition structure $\mathcal{F}_{final}$, it ensures that convergence will occur within a finite number of iterations. Second, if $\mathcal{F}_{t+j}$ is not the final partition structure, it implies that further iterations can still lead to gradual convergence by improving the utility. Since the number of coalitions is bounded and the coalition utility is guaranteed to be computable, we can ensure that, starting from any initial structure, the game will eventually converge to a final partition structure $\mathcal{F}_{final}$.
\end{proof}

D-Stability (Deviation Stability) is a robust form of stability in coalition formation games, ensuring that no group of players has an incentive to deviate from the current coalition structure to form a new coalition or reorganize the existing structure in a way that improves the total utility\cite{c37}. Before analyzing the stability, we first give the definition of $\mathbb{D}$-stable.
\begin{myDef}
A partition $\mathcal{F}=\{\mathcal{F}_1,\mathcal{F}_2,\dots, \mathcal{F}_M\}$ is $\mathbb{D}$-stable if, for any subset of players, the total utility under a new partition structure $\mathcal{F}^{\prime}$ after deviation is not greater than the total utility of $\mathcal{F}$, that is:
\begin{equation}\label{}
\sum_{\mathcal{F}_i \in \mathcal{F}^{\prime}} U\left(\mathcal{F}_i \right) \leq  \sum_{\mathcal{F}_i \in \mathcal{F}} U\left(\mathcal{F}_i\right)
\end{equation}
\end{myDef}


\begin{myTheo}\label{theo3}
The final converged partition $\mathcal{F}_{final}$ is $\mathbb{D}$-stable.
\end{myTheo}

\begin{proof}
We complete the proof by contradiction. Consider the case that a player $i$ deviates from its current coalition under $\mathcal{F}_{final}$, to join another coalition with switch or exchange operation, and obtain a new partition structure $\mathcal{F}_{final}^{\prime}$ . If the deviation improves utility, it must satisfy:
\begin{equation}\label{}
\sum_{\mathcal{F}_i \in \mathcal{F}_{final}^{\prime}} U\left(\mathcal{F}_i \right) > \sum_{\mathcal{F}_i \in \mathcal{F}_{final}} U\left(\mathcal{F}_i\right)
\end{equation}
which indicates that the total utility of $\mathcal{F}_{final}$ can be improved with switch or exchange operation, and it contradicts our assumption that $\mathcal{F}_{final}$ is the final converged partition. Therefore, we can guarantee that the final converged partition $\mathcal{F}_{final}$ from Algorithm 1 is $\mathbb{D}$-stable.
\end{proof}

In Algorithm 1, we assume the time complexity of coalition utility calculation proportional to $M$, i.e., $\mathcal{O}(M)$. Then, the time complexity of switch and exchange operation is $\mathcal{O}(NM)$ and $\mathcal{O}(\frac{M^2}{K})$, respectively. Therefore, if  Algorithm 1 converges after $T$ iterations, the general time complexity of Algorithm 3 can be denoted as $\mathcal{O}\left(T(NM+LlogL+A+\frac{M^2+LlogL+A}{K})\right)$.

\begin{table*}[htbp]
    \centering
    \small
    \setlength{\tabcolsep}{1pt} 
    \renewcommand{\arraystretch}{1.1} 
   \caption{Layer-wise Data Size, Computation (Comp.) Load, Communication (Comm.) Load, and Privacy Leakage Possibility of VGG19}
    \label{tab:layer_params}
    \begin{tabular}{|l |c |c |c |c | l |c| c |c| c|}
        \toprule
       \textbf{Layer} & \textbf{Data Size} & \textbf{Comp. Load} & \textbf{Comm. Load } & \textbf{Possibility} &
        \textbf{Layer} & \textbf{Data Size} & \textbf{Comp. Load} & \textbf{Comm. Load} & \textbf{Possibility} \\ \hline
        & (KB) & (MFLOPs) & (KB) & & & (KB) & (MFLOPs) & (KB) &  \\ \hline
        \midrule
        Conv1\_1  & 7       & 86.7    & 12544  & 1.000  & Conv4\_2  & 9218    & 1849.7 & 1568  & 0.1636  \\ \hline
        Conv1\_2  & 144.3   & 1849.7  & 12544  & 0.9973 & Conv4\_3  & 9218    & 1849.7 & 1568  & 0.1586  \\\hline
        Conv2\_1  & 288.5   & 924.8   & 6272   & 0.9439 & Conv4\_4  & 9218    & 1849.7 & 1568  & 0.0793  \\\hline
        Conv2\_2  & 576.5   & 1849.7  & 6272   & 0.9137 & Conv5\_1  & 9218    & 462.4  & 392   & 0.0785  \\\hline
        Conv3\_1  & 1153    & 924.8   & 3136   & 0.8308 & Conv5\_2  & 9218    & 462.4  & 392   & 0.0627  \\\hline
        Conv3\_2  & 2305    & 1849.7  & 3136   & 0.6128 & Conv5\_3  & 9218    & 462.4  & 392   & 0.0551  \\\hline
        Conv3\_3  & 2305    & 1849.7  & 3136   & 0.4335 & Conv5\_4  & 9218    & 462.4  & 392   & 0.0381  \\\hline
        Conv3\_4  & 2305    & 1849.7  & 3136   & 0.4314 & FC1       & 401424  & 102.8  & 16    & 0.0300  \\\hline
        Conv4\_1  & 4610    & 924.8   & 1568   & 0.3311 & FC2       & 65552   & 16.8   & 16    & 0.0191  \\\hline
        -         & -       & -       & -      & -      & FC3       & 1616.4  & 0.41   & 0.4     & 0.0000  \\\hline
        \bottomrule
    \end{tabular}
\end{table*}


\section{Simulation Evaluation}

\subsection{Simulation Setup}
In the simulation, the system consists of 100 MDs and 10 edge servers by default. Each MD has a randomly assigned computational capability $f_{n}$, with values uniformly distributed between $[10, 100]$ GFLOPS. We assume the computation capacity of each edge server $f_{m}$ follows uniform distribution of $[500, 2000]$ GFLOPS, and the storage capacity $C_{m}$ vary randomly within the range of $[2, 5]$ GB\cite{c33,c37}. The Zipf distribution parameter $\eta_{n}$ varies in $[0.6, 1]$. We set $\alpha=1$. For the wireless communication setup, the transmission power of the server is fixed at 43 dBm, while each MD transmits at 23 dBm. The total available communication bandwidth for each server is set to 400 MHz for both uplink and downlink\cite{c43}. The channel noise is modeled with a Gaussian distribution having a standard deviation of 8 dB. The distance between the users and the base station is randomly distributed between 100 and 200 meters. The path loss exponent $\vartheta$ is set to 3.5, which is characteristic of a non-line-of-sight (NLOS) propagation environment. We assume the transmission data rate between the cloud and edge server is within the range of $[500, 700]$ Mbps\cite{c33,c37}.

Regarding the inference models, the simulation considers nine deep learning architectures commonly used in edge inference tasks, namely: VGG19, VGG16, VGG13, ResNet50, ResNet34, ResNet18, LeNet12, LeNet9, and LeNet7. These models represent various complexities and computational demands, ranging from relatively simple architectures like LeNet7 to more complex ones like ResNet50. For each of these models, we assume that multiple inference tasks are derived based on specific downstream requirements such as task precision or real-time processing constraints\cite{c43}. Specifically, each model is assumed to give rise to 10 distinct inference services, resulting in a total of 90 inference services. Each MD's inference data size, which represents the amount of data to be processed for each service, is randomly distributed between 8 and 64 data units (e.g., number of images), simulating the variability of computational workloads typical of applications such as image classification, object recognition, and machine learning tasks in edge environments. Additionally, each MD has a data privacy leakage constraints, which is randomly distributed between 20\% and 80\% of the total inference raw data. This represents the portion of the MD's data that is susceptible to privacy leakage during processing, with lower values indicating more sensitive data that needs to be carefully protected during collaborative inference. The simulation results presented subsequently are averaged over 2000 time slots.

Due to page limitations, we present the network architecture and parameters of VGG19 as an example, as shown in Table 1. This table provides detailed information on each layer’s data size (KB), computation load (MFLOPs), communication Load (KB), and privacy leakage possibility, allowing readers to better understand the computational and communication characteristics of DNN models.

In our simulation, four benchmark algorithms are implemented to evaluate the performance of the proposed methods and algorithms in terms of average inference delay:
\begin{itemize}
  \item \textbf{Full Local}: In this scenario, all models are fully deployed and executed locally on each MD based on our proposed algorithms. This approach ensures complete data privacy, as no data is offloaded to the edge servers. The primary purpose of this algorithm is to provide a baseline for the inference delay when all computation remain entirely local, thus reflecting the worst-case latency scenario in terms of computation resource limitations on MDs.
  \item \textbf{Full Edge}: The Full Edge algorithm assumes that all inference services are offloaded to the edge servers for computation. In this configuration, the edge servers handle the entire computational load, leading to full exposure of the data privacy. This setup serves as a benchmark to evaluate the inference delay when privacy is completely compromised, providing insights into the performance of edge-based computation without any privacy constraints. This setup serves as a benchmark to evaluate the inference delay when privacy is completely compromised, providing insights into the performance of edge-based inference without any privacy constraints.
  \item \textbf{Matching}: The Matching algorithm employs matching theory to solve the problem of allocating MDs to edge servers and selecting appropriate models for deployment\cite{c33}. This algorithm models the problem as a bipartite graph matching problem, where one set of nodes represents MDs and the other represents edge servers and models. The goal of the Matching algorithm is to minimize the average inference delay while adhering to the privacy constraints.
  \item \textbf{LyDQN}: Based on the approach outlined in \cite{LyDQN}, the LyDQN algorithm utilizes Lyapunov optimization to transform the problem into a single-slot optimization problem. Then, the system seeks to minimize the long-term average inference delay while adhering to the privacy constraints by optimizing the joint server association, model deployment and model partition strategies in real time with Deep Q-Network (DQN) techniques.
\end{itemize}



\subsection{Simulation Results}

\subsubsection{Impact of the Number of MDs and Servers}
Fig. \ref{fig5} illustrates the average delay of the five algorithms as the number of MDs increases from 20 to 140. As the number of MDs grows, the average delay for all algorithms increases, with significant variations in performance. The Full Local algorithm exhibits the highest delay growth, with an increase of 51.2\% (from 15.42 s to 23.32 s), reflecting its reliance on resource-constrained local devices. In contrast, the Full Edge algorithm shows the lowest delay growth, with an increase of 88.8\% (from 7.17 s to 13.54 s), as it leverages the computational power of edge servers. However, this comes at the cost of complete privacy loss. The Proposed algorithm demonstrates a moderate delay increase of 45.2\% (from 11.45 s to 16.62 s), balancing privacy preservation and computational efficiency. The LyDQN algorithm outperforms the Proposed algorithm, with a delay increase of only 30.9\% (from 11.35 s to 14.86 s), due to its dynamic optimization capabilities. However, its higher computational complexity may limit its practicality. The Matching algorithm shows the second-highest delay growth, with an increase of 78.2\% (from 11.59 s to 20.66 s), as its resource allocation strategy struggles to handle larger MD populations. These results highlight the trade-offs between inference delay, data privacy, and computational efficiency, with the Proposed algorithm offering a practical balance for real-world deployment.

Fig. \ref{fig6} compares the average delay of the five algorithms under different number of servers. When the number of servers increases from 4 to 18, the average delay for all algorithms decreases, reflecting the benefits of enhanced computational resources. The Full Edge algorithm shows the most significant improvement, with delays dropping from 16.15 s to 8.46 s (a 47.6\% reduction), as additional servers directly alleviate its computational burden. The Full Local algorithm, however, benefits the least, with delays decreasing from 29.85 s to 16.21 s (a 45.7\% reduction), as it does not utilize edge servers, and its delay reduction is primarily due to reduced network contention. The Proposed algorithm demonstrates a notable reduction in delay, with delays falling from 17.92 s to 12.20 s (a 31.9\% reduction), leveraging additional servers to optimize task offloading while maintaining privacy. The LyDQN algorithm achieves slightly better performance than the Proposed algorithm, particularly at lower server counts, with delays decreasing from 17.70 s to 12.18 s (a 31.2\% reduction), due to its adaptive optimization capabilities. However, its performance gap narrows as server resources increase, highlighting the scalability of the Proposed algorithm. The Matching algorithm also benefits from additional servers, with delays dropping from 24.78 s to 13.46 s (a 45.7\% reduction), but remains less efficient than the optimization-based approaches, especially in resource-constrained environments. These findings emphasize the critical role of server scalability in improving system performance and the need for efficient resource allocation strategies in efficient inference.

\subsubsection{Impact of the Number of Inference Services and Input Data Size}
Fig. \ref{fig7a} curves the average delay of the five algorithms under different number of services with fixed number of MDs and servers. The increase in the number of services leads to a reduced probability of users requesting the same service. This diminishes the potential for service reuse, placing greater pressure on the storage and computational resources of the servers. The Full Local algorithm, which relies entirely on local devices, is less affected by the reduced service reuse, as it only depends on shared storage and communication resources of servers. However, its delay growth remains the highest due to the limited computational capacity of local devices, which struggle to handle the increasing diversity of service requests. The Full Edge algorithm, which offloads all computations to edge servers, experiences a moderate delay growth. However, the reduced service reuse increases the storage and computational burden on edge servers, as each unique service request requires additional resources. This highlights a trade-off between low delay growth and increased resource pressure on edge infrastructure. The Proposed algorithm, which balances local and edge resource utilization, shows a delay growth of 13.9\%. Its ability to dynamically allocate models helps mitigate the impact of reduced service reuse. However, the increasing diversity of services still places some pressure on edge servers, albeit less than in the Full Edge algorithm. The LyDQN algorithm achieves the lowest delay growth of 12.3\% among optimization-based algorithms. However, the reduced service reuse increases the complexity of its optimization process, as it must account for a wider variety of service requests. This may lead to higher computational overhead in large-scale deployments. The Matching algorithm, which uses a static resource allocation strategy, experiences a significant delay growth of 15.0\%. The reduced service reuse exacerbates its inefficiency, as it cannot adapt to the increasing diversity of service requests, which further results in suboptimal resource allocation and higher delays. These findings underscore the importance of designing scalable, resource-efficient algorithms that can adapt to the growing diversity of service requests in EI systems.
\begin{figure}[t]
	\centering
	\includegraphics[width=3.0in]{Delay_vs_Users.png}
	\caption{Performance comparisons of the five algorithms for different number of MDs.}
	\label{fig5}
\end{figure}
\begin{figure}[t]
	\centering
	\includegraphics[width=2.9in]{Delay_vs_Servers.png}
	\caption{Performance comparisons of the five algorithms for different number of servers.}
	\label{fig6}
\end{figure}
Fig. \ref{fig7b} further measures the impact of the Zipf distribution parameter $\eta_n$ on average delay when the number of services is 90. The simulation results reveal an unexpected trend: as the Zipf parameter increases, the inference delay also increases, contrary to the conventional understanding that larger parameter (indicating more concentrated requests) should reduce delay due to higher model hit rates. This counterintuitive outcome is attributed to the specific design of the simulation, where service IDs are mapped to computational and communication loads in a decreasing manner, i.e., smaller IDs correspond to services with higher computational and communication demands (from VGG19 to LeNet7). Therefore, in this scenario, larger parameter lead to higher delay due to the concentration of requests on high-computation and complex services, while the smaller value result in lower delay due to the distribution of requests across low-computation services. These findings highlight the importance of considering service load characteristics, and underscore the need for dynamic resource allocation and service scheduling strategies to optimize performance under varying request distributions.

Fig. \ref{fig8} demonstrates the impact of increasing input data size on the average delay of different algorithms. As the input data size increases, the average delay for all algorithms rises due to the higher computational and communication requirements. However, the rate of delay growth and the absolute delay values differ significantly across algorithms, highlighting their varying efficiency and scalability. The Full Local algorithm shows the highest delays and the fastest growth rate. Since it relies entirely on local devices for computation, its performance degrades significantly as the data size increases, highlighting its limitations in scalable scenarios. The Full Edge algorithm exhibits the lowest delay and the slowest growth rate. This superior performance is attributed to the efficient utilization of edge server resources, which can handle large-scale data processing effectively. The Proposed algorithm demonstrates moderate performance, strikes a balance between computational efficiency and resource utilization. The LyDQN algorithm shows slightly higher delays than the FE algorithm but outperforms the other algorithms. These results indicates that edge-end collaborative inference mechanism can achieve superior performance in overcoming the rapid growth of delay while achieving a balance between delay and privacy.
\begin{figure}[t]
	\centering
	\includegraphics[width=3.0in]{Delay_vs_Services.png}
	\caption{Performance comparisons of the five algorithms for different number of services.}
	\label{fig7a}
\end{figure}

\begin{figure}[t]
	\centering
	\includegraphics[width=3.0in]{Delay_vs_zipf.png}
	\caption{Performance comparisons of the five algorithms for Zipf distribution parameters.}
	\label{fig7b}
\end{figure}
\begin{figure}[t]
	\centering
	\includegraphics[width=3.0in]{Delay_vs_data_size.png}
	\caption{Performance comparisons of the five algorithms for different input data size.}
	\label{fig8}
\end{figure}

\subsubsection{Impact of Edge Server Storage and Computation Capacity}
\begin{figure}[t]
	\centering
	\includegraphics[width=3.0in]{Delay_vs_storage.png}
	\caption{Performance comparisons of the five algorithms for different storage capacity.}
	\label{fig9}
\end{figure}
\begin{figure}[t]
	\centering
	\includegraphics[width=3.0in]{Delay_vs_comp.png}
	\caption{Performance comparisons of the five algorithms for different computation capacity.}
	\label{fig10}
\end{figure}
\begin{figure}[t]
	\centering
	\includegraphics[width=3.0in]{Delay_vs_privacy.png}
	\caption{Performance comparisons of the five algorithms under different privacy constraints.}
	\label{fig11}
\end{figure}
Fig. \ref{fig9} illustrates the impact of increasing edge server storage capacity on the average delay of different algorithms. As the edge server storage capacity increases, the delay for all algorithms decreases, reflecting the benefits of mode deployment. Larger storage capacity allows more models to be deployed locally, reducing dependency on cloud downloads and minimizing communication overhead. However, the magnitude of improvement varies across algorithms due to differences in resource utilization strategies. The Full Edge algorithm benefits most from increased storage capacity, as its full dependency on edge servers allows it to maximize the advantages of model deployment. The Full Local algorithm also improves significantly due to reduced model download requirements, despite its reliance on local computation. Again, LyDQN and the Proposed algorithm outperform static Matching algorithm by adapting to storage availability and optimizing resource allocation.  Besides, beyond 4-5 GB of storage, delay improvements diminish for most algorithms (e.g., the Proposed and LyDQN stabilizes near 13.5 s), suggesting a threshold where additional storage provides marginal returns, and edge servers should allocate sufficient storage (4–5 GB in this case) to cache frequently requested models, balancing cost and performance. These results underscore the importance of model deployment and adaptive resource management for delay-efficient EI systems.

Fig. \ref{fig10} shows the impact of increasing edge server computation capacity on the average delay of different algorithms. As the edge server computational capacity increases, the average delay for most algorithms decreases, reflecting the benefits of enhanced processing power. The Full Edge algorithm achieves the lowest delay across all computational capacities, and its delay decreases from 15.28 s to 10.13 s with a reduction of 33.7\%. The Full Local algorithm shows no delay improvement (fixed at 19.58 ms), as it relies entirely on local devices and is unaffected by edge server computation capacity. The LyDQN algorithm exhibits rapid delay reduction, from 16.17 s to 12.95 s with a 19.9\% improvement, as it can leverage increased computational resources efficiently. The Proposed algorithm shows moderate improvement, with delay decreasing from 16.44 s to 13.57 s, a 17.5\% reduction. The Matching algorithm demonstrates the slowest improvement, with delay decreasing from 18.88 s to 16.46 s, a reduction of 12.8\%.  Similarly, beyond 1700–2100 GFLOPS, delay improvements for most algorithms diminish, suggesting a threshold where additional computational power provides marginal returns due to the limited storage capacity.

\subsubsection{Impact of Data Privacy Constraints}


Fig. \ref{fig11} indicates the impact of varying privacy constraints on the inference delay of different algorithms with 50 MDs and 10 servers. Here, the privacy constraint percentage represents the probability of data privacy leakage during edge-end collaborative inference. A lower percentage (e.g., 20\%) indicates stricter privacy requirements, meaning more computations must be performed locally to minimize the risk of data exposure. Conversely, a higher percentage (e.g., 90\%) allows more computations to be offloaded to edge servers, reducing delay but increasing privacy risks. As privacy constraints relax, the delay for the Proposed, LyDQN, and Matching algorithms decreases significantly, reflecting the benefits of offloading more computations to edge servers. In contrast, the Full Local and Full Edge algorithms exhibit constant delay, as their designs are unaffected by privacy constraints. These results underscore the critical trade-off between privacy and delay for EI. Specifically, the Proposed and LyDQN algorithms strike a balance, with delay reductions of 18.0\% and 19.4\%, respectively, as privacy constraints relax. These findings emphasize the importance of designing adaptive, privacy-aware algorithms that dynamically optimize model partition and computational resources for real-world applications.

\section{Conclusion}
In this paper, we addressed the critical challenges of delay-efficient and privacy-preserving collaborative EI by proposing a novel framework for joint DNN model deployment and partition optimization. Our approach leverages a Lyapunov-based optimization technique to handle dynamic and stochastic inference requests, ensuring long-term system stability and privacy constraints. By formulating the user-server association problem as a coalition formation game, we decoupled the complex joint optimization into manageable subproblems, enabling efficient suboptimal solutions through iterative optimization. The proposed greedy-based algorithm for model deployment and exhaustive search method for model partitioning further enhanced the system's performance. Extensive simulations demonstrated that our framework significantly reduces inference delay while maintaining stringent privacy constraints, outperforming existing baseline algorithms in various scenarios. The results highlight the importance of adaptive resource allocation, dynamic model partition, and privacy-aware optimization in edge computing environments. Future work could explore the integration of advanced machine learning techniques for real-time decision-making and further optimization of the trade-offs between computational efficiency, resource utilization, and data privacy in large-scale edge networks. Our contributions provide a robust foundation for the development of efficient and secure EI systems, paving the way for the next generation of AI-driven edge applications.
\begin{thebibliography}{1}
\addtolength{\itemsep}{-0.1em}

\bibitem{c1}
F. Xu, J. Xu, J. Chen, et al., "iGniter: Interference-Aware GPU Resource Provisioning for Predictable DNN Inference in the Cloud," \textit{IEEE Trans. Parallel Distrib. Syst.}, vol. 34, no. 3, pp. 812-827, Mar. 1, 2023.

\bibitem{c2}
A. Yousefpour, S. Devic, B. Q. Nguyen, et al., "Guardians of the Deep Fog: Failure-Resilient DNN Inference from Edge to Cloud," in \textit{Proc. First Int. Workshop Challenges Artif. Intell. Mach. Learn. Internet Things (AIChallengeIoT'19)}, New York, NY, USA, 2019, pp. 25-31.

\bibitem{c3}
Z. Xu, L. Zhao, W. Liang, et al., "Energy-Aware Inference Offloading for DNN-Driven Applications in Mobile Edge Clouds," \textit{IEEE Trans. Parallel Distrib. Syst.}, vol. 32, no. 4, pp. 799-814, Apr. 1, 2021.

\bibitem{c4}
M. M. H. Shuvo, S. K. Islam, J. Cheng, et al., "Efficient Acceleration of Deep Learning Inference on Resource-Constrained Edge Devices: A Review," \textit{Proc. IEEE}, vol. 111, no. 1, pp. 42-91, Jan. 2023.

\bibitem{c5}
D. Kafetzis, I. Koutsopoulos, "DNN Partitioning and Inference Task Offloading in 6G Resource-Constrained Networks," in \textit{Proc. Joint Eur. Conf. Networks Commun. \& 6G Summit (EuCNC/6G Summit)}, Antwerp, Belgium, Jul. 2024, pp. 753-758.

\bibitem{c6}
N. Chen, J. Yang, Z. Cheng, et al., "GainNet: Coordinates the Odd Couple of Generative AI and 6G Networks," \textit{IEEE Network}, vol. 38, no. 5, pp. 56-65, Sept. 2024.

\bibitem{c7}
W. Qi, X. Guo, H. Du, "LMIE-BERT: A Learnable Method for Inter-Layer Ensembles to Accelerate Inference of BERT-Style Pre-Trained Models," in \textit{Proc. 9th Int. Conf. Big Data Comput. Commun. (BigCom)}, Hainan, China, 2023, pp. 271-277.

\bibitem{c8}
X. Xu, Y. Ding, S. X. Hu, et al., "Scaling for Edge Inference of Deep Neural Networks," \textit{Nat. Electron.}, vol. 1, pp. 216-222, 2018.

\bibitem{c9}
C. Jean, D. Brooks, K. Chen, et al., "Machine Learning at Facebook: Understanding Inference at the Edge," in \textit{Proc. IEEE Int. Symp. High Perform. Comput. Architecture (HPCA)}, Washington, DC, USA, Feb. 2019, pp. 331-344.

\bibitem{c10}
E. Li, L. Zeng, Z. Zhou, et al., "Edge AI: On-Demand Accelerating Deep Neural Network Inference via Edge Computing," \textit{IEEE Trans. Wireless Commun.}, vol. 19, no. 1, pp. 447-457, Jan. 2020.

\bibitem{c11}
J. Shao, J. Zhang, "Communication-Computation Trade-Off in Resource-Constrained Edge Inference," \textit{IEEE Commun. Mag.}, vol. 58, no. 12, pp. 20-26, Dec. 2020.

\bibitem{c12}
N. Shlezinger, I. V. Bajić, "Collaborative Inference for AI-Empowered IoT Devices," \textit{IEEE Internet Things Mag.}, vol. 5, no. 4, pp. 92-98, Dec. 2022.

\bibitem{c13}
X. Fan, Z. Cheng, M. Liwang, et al., "Multi-Fine-Grained DNNs Partition and Offloading over Fog Computing Networks," in \textit{Proc. IEEE Int. Conf. Commun. Workshops (ICC Workshops)}, Rome, Italy, Jun. 2023, pp. 599-604.

\bibitem{c14}
N. Ng, A. Souza, S. Diggavi, "Collaborative Inference in Resource-Constrained Edge Networks: Challenges and Opportunities," in \textit{Proc. MILCOM 2024 - IEEE Military Commun. Conf. (MILCOM)}, Washington, DC, USA, 2024, pp. 1-6.

\bibitem{c15}
W. Fan, Z. Chen, Z. Hao, et al., "DNN Deployment, Task Offloading, and Resource Allocation for Joint Task Inference in IIoT," \textit{IEEE Trans. Ind. Informat.}, vol. 19, no. 2, pp. 1634-1646, Feb. 2023.

\bibitem{c16}
Z. Liao, W. Hu, J. Huang, et al., "Joint Multi-User DNN Partitioning and Task Offloading in Mobile Edge Computing," \textit{Ad Hoc Networks}, vol. 144, May 2023, Art. no. 103156.

\bibitem{c17}
T. Mohammed, C. Joe-Wong, R. Babbar, "Distributed Inference Acceleration with Adaptive DNN Partitioning and Offloading," in \textit{Proc. IEEE INFOCOM 2020 - IEEE Conf. Comput. Commun.}, Toronto, ON, Canada, 2020, pp. 854-863.

\bibitem{c18}
Z. He, T. Zhang, R. B. Lee, "Model Inversion Attacks Against Collaborative Inference," in \textit{Proc. 35th Annu. Comput. Security Appl. Conf. (ACSAC '19)}, New York, NY, USA, 2019, pp. 148-162.

\bibitem{c19}
Z. He, T. Zhang, R. B. Lee, "Attacking and Protecting Data Privacy in Edge–Cloud Collaborative Inference Systems," \textit{IEEE Internet Things J.}, vol. 8, no. 12, pp. 9706-9716, Jun. 15, 2021.

\bibitem{c20}
R. Zhang, P. Isola, A. A. Efros, "The Unreasonable Effectiveness of Deep Features as a Perceptual Metric," in \textit{Proc. IEEE Conf. Comput. Vis. Pattern Recognit.}, 2018, pp. 586-595.

\bibitem{c21}
C. Hu, W. Bao, D. Wang, et al., "Dynamic Adaptive DNN Surgery for Inference Acceleration on the Edge," in \textit{Proc. IEEE INFOCOM 2019 - IEEE Conf. Comput. Commun.}, Paris, France, May 2019, pp. 1423-1431.

\bibitem{c22}
W. He, S. Guo, S. Guo, et al., "Joint DNN Partition Deployment and Resource Allocation for Delay-Sensitive Deep Learning Inference in IoT," \textit{IEEE Internet Things J.}, vol. 7, no. 10, pp. 9241-9254, Oct. 2020.

\bibitem{c23}
W. Q. Ren, Y. B. Qu, C. Dong, et al., "A Survey on Collaborative DNN Inference for Edge Intelligence," \textit{Mach. Intell. Res.}, vol. 20, pp. 370-395, Jun. 2023.

\bibitem{c24}
H. Hussain, P. S. Tamizharasan, C. S. Rahul, "Design Possibilities and Challenges of DNN Models: A Review on the Perspective of End Devices," \textit{Artif. Intell. Rev.}, vol. 55, pp. 5109-5167, Jun. 2022.

\bibitem{c25}
L. Cheng, Y. Gu, Q. Liu, et al., "Advancements in Accelerating Deep Neural Network Inference on AIoT Devices: A Survey," \textit{IEEE Trans. Sustain. Comput.}, vol. 9, no. 6, pp. 830-847, Nov.-Dec. 2024.

\bibitem{c26}
A. Eshratifar, M. S. Abrishami, M. Pedram, "JointDNN: An Efficient Training and Inference Engine for Intelligent Mobile Cloud Computing Services," \textit{IEEE Trans. Mobile Comput.}, vol. 20, no. 2, pp. 565-576, Feb. 2021.

\bibitem{c27}
L. Wang, X. Ren, C. Zhao, et al., "MPDM: A Multi-Paradigm Deployment Model for Large-Scale Edge-Cloud Intelligence," \textit{IEEE Internet Things J.}, vol. 10, no. 10, pp. 8773-8785, May 15, 2023.

\bibitem{c28}
M. Zyliński, A. Nassibi, I. Rakhmatulin, et al., "Deployment of Artificial Intelligence Models on Edge Devices: A Tutorial Brief," \textit{IEEE Trans. Circuits Syst. II: Express Briefs}, vol. 71, no. 3, pp. 1738-1748, Mar. 2024.

\bibitem{c29}
M. Xu, D. Niyato, H. Zhang, et al., "Joint Foundation Model Caching and Inference of Generative AI Services for Edge Intelligence," in \textit{Proc. IEEE Global Commun. Conf.}, 2023, pp. 3548-3553.

\bibitem{c30}
Y. Li, Z. Li, Z. Han, et al., "Automating Cloud Deployment for Real-Time Online Foundation Model Inference," \textit{IEEE/ACM Trans. Netw.}, vol. 32, no. 2, pp. 1509-1522, Apr. 2024.

\bibitem{c31}
P. Dai, B. Han, K. Li, et al., "Joint Optimization of Device Placement and Model Partitioning for Cooperative DNN Inference in Heterogeneous Edge Computing," \textit{IEEE Trans. Mobile Comput.}, vol. 24, no. 1, pp. 210-226, Jan. 2025.

\bibitem{c32}
J. Yan, S. Bi, Y.-J. A. Zhang, "Optimal Model Placement and Online Model Splitting for Device-Edge Co-Inference," \textit{IEEE Trans. Wireless Commun.}, vol. 21, no. 10, pp. 8354-8367, Oct. 2022.

\bibitem{c33}
Y. Wu, J. Wu, L. Chen, et al., "Share-Aware Joint Model Deployment and Task Offloading for Multi-Task Inference," \textit{IEEE Trans. Intell. Transp. Syst.}, vol. 25, no. 6, pp. 5674-5686, Jun. 2024.

\bibitem{c34}
J. Tang, G. Wu, M. M. Jalalzai, et al., "Energy-Optimal DNN Model Placement in UAV-Enabled Edge Computing Networks," \textit{Digit. Commun. Netw.}, 2023.

\bibitem{c35}
E. Baccour, A. Erbad, A. Mohamed, et al., "DistPrivacy: Privacy-Aware Distributed Deep Neural Networks in IoT Surveillance Systems," in \textit{Proc. IEEE Global Commun. Conf.}, 2020, pp. 1-6.

\bibitem{c36}
C. Shi, L. Chen, C. Shen, et al., "Privacy-Aware Edge Computing Based on Adaptive DNN Partitioning," in \textit{Proc. IEEE Int. Conf. Comput. Commun. (INFOCOM)}, 2019.

\bibitem{c37}
Y.-T. Yang, H.-Y. Wei, "A Coalition Formation Approach for Privacy and Energy-Aware Split Deep Learning Inference in Edge Camera Network," \textit{IEEE Trans. Netw. Service Manag.}, vol. 20, no. 3, pp. 3673-3685, Sep. 2023.

\bibitem{c38}
S. Fu, F. Dong, D. Shen, et al., "Privacy-Preserving Model Splitting and Quality-Aware Device Association for Federated Edge Learning," \textit{Software: Pract. Exp.}, vol. 54, no. 10, pp. 2063-2085, Oct. 2024.

\bibitem{c39}
G. Jiang, S. Han, X. Xu, et al., "Privacy-Aware Adaptive Model Splitting for Device-Edge Co-Inference," in \textit{Proc. IEEE Globecom Workshops (GC Wkshps)}, 2023, pp. 1-6.

\bibitem{c40}
E. Baccour, A. Erbad, A. Mohamed, et al., "RL-DistPrivacy: Privacy-Aware Distributed Deep Inference for Low Latency IoT Systems," \textit{IEEE Trans. Network Sci. Eng.}, vol. 9, no. 4, pp. 2066-2083, Jul.-Aug. 2022.

\bibitem{c41}
Y. Hu, X. Xu, L. Qi, et al., "Latency and Privacy Aware Convolutional Neural Network Distributed Inference for Reliable Artificial Intelligence Systems," \textit{IEEE Trans. Artif. Intell.}, early access, 2024.

\bibitem{c42}
M. Malekzadeh, F. Kawsar, "Salted Inference: Enhancing Privacy while Maintaining Efficiency of Split Inference in Mobile Computing," in \textit{Proc. 25th Int. Workshop Mobile Comput. Syst. Appl. (HOTMOBILE ’24)}, 2024, pp. 1-7.

\bibitem{xiaTPDS}
X. Xia, F. Chen, Q. He, et al., "Formulating Cost-Effective Data Distribution Strategies Online for Edge Cache Systems," \textit{IEEE Trans. Parallel Distrib. Syst.}, vol. 33, no. 12, pp. 4270-4281, 2022.

\bibitem{c43}
G. Qu, Z. Lin, F. Liu, et al., "TrimCaching: Parameter-Sharing AI Model Caching in Wireless Edge Networks," in \textit{Proc. 44th Int. Conf. Distrib. Comput. Syst. (ICDCS)}, 2024.

\bibitem{LyDQN}
Y. Liang, H. Tang, H. Wu, et al., "Lyapunov-Guided Offloading Optimization Based on Soft Actor-Critic for ISAC-Aided Internet of Vehicles," \textit{IEEE Trans. Mobile Comput.}, vol. 23, no. 12, pp. 14708-14721, Dec. 2024.

\end{thebibliography}

\end{document}


