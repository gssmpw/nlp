\section{Related Work}
Extensive research efforts have been dedicated to partition-based collaborative edge inference paradigms, wherein DNN models are strategically dissected between edge servers and end-user devices to optimize computational load distribution and minimize communication overhead. For example, foundational researches such as \cite{c10,c21,c22} have established methodologies for model partition and inference optimization. For readers interested in a comprehensive overview of existing works, we recommend recent systematic surveys such as \cite{c4,c23,c24,c25}. Aligned with the scope of this work, we focus on two critical dimensions of edge-end collaborative inference: 1) model deployment/placement, or caching strategies for efficient DNN inference, and 2) privacy-aware mechanisms to mitigate data leakage risks during collaborative inference. Below, we discuss representative works in these areas.

\subsection{Model Deployment for Efficient DNN Inference}
Recent studies have made significant progress in optimizing AI deployment across edge-cloud environments. \cite{c15} develops a Lyapunov-based joint resource management scheme for IIoT systems, co-optimizing model deployment, task offloading, and resource allocation to minimize delay and error penalty while ensuring system stability. \cite{c26} introduces JointDNN, a collaborative inference engine that employs shortest path optimization and integer linear programming to enhance performance and energy efficiency between mobile devices and cloud servers. For large-scale deployments, \cite{c27} proposes MPDM, a multi-paradigm deployment model that uses heuristic algorithms to optimize accuracy, scale, and cost for deep learning inference services. Resource-constrained scenarios are addressed by \cite{c28}, which focuses on neural network optimization for edge devices, leveraging logic circuits, in-memory computing, and learning automata algorithms to improve efficiency and accuracy.

In the context of edge server deployments, \cite{c29} establishes a joint caching and inference framework for foundation models, utilizing a least context algorithm to enhance inference accuracy and reduce system costs. Automation in cloud deployment is advanced by \cite{c30}'s AutoDeep framework, which integrates Bayesian Optimization and Deep Reinforcement Learning to optimize cloud configurations and device placement, achieving significant cost reductions through techniques like probing-informed block multiplexing. For heterogeneous edge environments, \cite{c31} designs a joint optimization framework for device placement and model partition, employing evolutionary algorithms and dynamic programming to maximize throughput and minimize inference time.

Dynamic co-inference scenarios are further explored by \cite{c32}, which formulates model placement and online splitting in wireless networks as an optimal stopping problem, proposing algorithms to minimize energy and time costs. \cite{c33} addresses multi-task inference in vehicular edge computing through a share-aware joint deployment and task offloading framework, utilizing a time period-aware algorithm to reduce total response time. Finally, \cite{c34} investigates energy-optimal DNN placement in UAV-enabled edge networks, developing a Lyapunov-based online algorithm to minimize transmission delay and energy costs while stabilizing data queues.

In contrast to these studies that predominantly focus on isolated aspects of performance optimization, our work holistically investigates privacy-aware model deployment, adaptive model partition, and user-edge server association under resource and privacy constraints, a tripartite challenge unaddressed in existing literature, where privacy considerations are either oversimplified or entirely decoupled from system-level coordination.

\subsection{Privacy-Aware Collaborative DNN Inference}
Recent advancements in privacy-preserving deep learning inference for edge and IoT systems have addressed critical challenges in distributed intelligence. \cite{c35} introduces DistPrivacy, a distributed feature map allocation methodology that enhances data privacy for IoT surveillance systems through strategic data partition. Building on this foundation, \cite{c36} develops an adaptive DNN partition framework that dynamically balances privacy and performance under varying network conditions. The coalition-based approach by \cite{c37} optimizes IoT camera-edge node associations, simultaneously addressing privacy preservation, energy efficiency, and multi-view detection performance through game-theoretic optimization.

Federated learning-based architectures have gained significant attention, with \cite{c38} proposing a quality-aware framework that integrates adaptive model splitting and device association to handle resource and data heterogeneity in edge environments. For device-edge co-inference scenarios, \cite{c39} designs a deep reinforcement learning-based model splitting mechanism that achieves optimal privacy-computation tradeoffs. Extending distributed inference capabilities, \cite{c40} presents RL-DistPrivacy, which employs reinforcement learning to optimize privacy-aware deep inference in latency-sensitive IoT systems.

Recent innovations focus on enhancing both privacy and reliability: \cite{c41} develops a queue-optimized CNN distributed inference method that leverages reinforcement learning to minimize latency while ensuring customer privacy protection. Finally, \cite{c42} introduces Salted DNNs, a novel semantic rearrangement technique that uses cryptographic salt to enhance output privacy in split inference without compromising computational efficiency. Collectively, these works significantly advance the state-of-the-art in privacy-aware edge intelligence, yet they predominantly focus on isolated aspects of the privacy-performance tradeoff, leaving opportunities for more holistic frameworks that integrate adaptive model deployment optimization, user-server association and comprehensive privacy preservation mechanisms.