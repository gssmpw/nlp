\section{Related Work}
\label{sec:relatedWork}
Previously, most studies solely focused on evaluating correctness of the code generation\cite{10403378, 10.1145/3627217.3627233, Chen2021EvaluatingLL}. However, the focus has shifted to also addressing the issue of efficiency in code generated by LLMs as AI-assisted tools become increasingly prevalent in software engineering and development processes~\cite{han-etal-2023-sample, Nitin_Sherje_2024, copilotAcc}.
\\
A closely related study by Vartziotis et al.~\cite{vartziotis2024learn} examines the energy efficiency of Python code generated by three widely used tools, namely GitHub Copilot, ChatGPT 3 and Amazon CodeWhisperer. The results show that AI models can generate code optimized for sustainability when explicitly requested to do so. However, they also reported that  human-written code is consistantly more energy-efficient.  A related study by Coignion et al.~\cite{10.1145/3661167.3661221} evaluates LLM-generated code from a performance perspective. Their analysis compares 18 LLMs using LeetCode data, examining factors such as model temperature and success rate and their influence on code performance. The findings of this study align with those of Varziotis et al.~\cite{vartziotis2024learn}, emphasizing that LLM-generated code, on average, demonstrates greater efficiency compared to human-written code. Both studies, however, share limitations, as they focus exclusively on Python data, with Varziotis et al.~\cite{vartziotis2024learn} deriving their conclusions from a limited dataset comprising only six coding problems.
\par In contrast, this study investigates three programming languages: Python, Java, and C++. Furthermore,it utilizes a comprehensive benchmark of 53 coding tasks, thereby increasing the robustness and reliability of the findings. We argue that Python is generally not regarded as an inherently efficient programming language~\cite{PEREIRA2021102609}, making it less likely to be chosen by developers when performance is a critical requirement. Consequently, studies evaluating the performance and energy efficiency of LLM-generated code for Python may have limited practical relevance, as such analyses may not align with real-world scenarios where more performance-oriented languages are typically preferred.
\par Several studies~\cite{du2024mercury, huang2024effibench, 10.1145/3597503.3623316, Hendrycks2021MeasuringCC, wang-etal-2023-recode, Huang2024EffiCodeUC} have introduced benchmarks specifically designed to evaluate LLM-generated code, focusing on runtime performance and memory consumption. In contrast, our study expands this scope by including the energy consumption as an additional metric, providing a more comprehensive overview of the energy efficiency of LLM-generated code. Furthermore, while the benchmarks developed by two of the prior studies~\cite{du2024mercury, huang2024effibench} were extensive, each encompassing over 1000 coding problems, their analyses were limited to Python, whereas our study also includes Java and C++. 
\par Rather than developing a benchmark from scratch, the study by Liu et al.~\cite{liu2024evaluatinglanguagemodelsefficient} grouped efficiency-demanding Python programming tasks from \textit{HumanEval+} and \textit{MBPP+} to form \textit{EvalPerf} addressing the limitation of previous works, which primarily focused on light computational requirements and possibly misrepresenting the capabilities of LLMs. To improve the quality of their evaluation, they augmented their experiments with computationally intensive inputs, aiming to provide a more accurate assessment of the efficiency and performance of the generated code. Although the referenced study focused exclusively on performance-intensive tasks from \textit{HumanEval+} and \textit{MBPP+}, several other works have shown that LLMs perform notably well on these datasets. In contrast, our research does not depend on these benchmarks, as they are generally labeled as "Easy" and are less challenging for evaluating model capabilities.  In addition, we aim to compare the efficiency of the generated code with human-written solutions, a consideration that was overlooked in the cited study.
\par The study by Du et al.~\cite{duCodeGen} sought to examine a more complex code generation scenario. To this end, they developed their own benchmark, \textit{ClassEval}, which consists of 100 class-level Python code generation tasks. Based on the new benchmark, this is the first study that evaluated LLMs in the context of class-level code generation. Their experiments included 11 state-of-the-art models, each varying in size, architecture, data sources, and application domains. The objective of the cited study differs from the primary goal of our research. While the cited work introduced a novel benchmark to assess the correctness of the code generated by LLMs, our study focuses primarily on evaluating the energy efficiency of the generated code. Nonetheless, we also account for the correctness of the code, as evaluating the efficiency is only meaningful when the code is correct.
\par LeetCode, although primarily a platform for coding competitions,  is also extensively used as a dataset for evaluating the programming capabilities of LLMs. DÃ¶derlein et al.~\cite{Dderlein2022PilotingCA} evaluated the performance of Copilot and Codex on LeetCode, analyzing the impact of varying prompt structures on the models' effectiveness. Nguyen and Nadi~\cite{9796235} investigated GitHub Copilot's code recommendations for LeetCode problems, focusing on the complexity and intricacies of the generated solutions. Vasconcelos et al.~\cite{10.1145/3702320} examined the impact of emphasizing uncertainty in AI-driven code completions, utilizing LeetCode problems and the Codex model as part of their analysis.

%-------------------------------------------------------------