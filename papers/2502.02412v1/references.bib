@inbook{basil,
author = {van Solingen (Revision), Rini and Basili (Original article, 1994 ed.), Vic and Caldiera (Original article, 1994 ed.), Gianluigi and Rombach (Original article, 1994 ed.), H. Dieter},
publisher = {John Wiley \& Sons, Ltd},
isbn = {9780471028956},
title = {Goal Question Metric (GQM) Approach},
booktitle = {Encyclopedia of Software Engineering},
chapter = {},
pages = {},
doi = {https://doi.org/10.1002/0471028959.sof142},
url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/0471028959.sof142},
eprint = {https://onlinelibrary.wiley.com/doi/pdf/10.1002/0471028959.sof142},
year = {2002},
keywords = {industrial application, phases, measurable software, quality improvement},
abstract = {Abstract As with any engineering discipline, software development requires a measurement mechanism for feedback and evaluation. Measurement supports creating a corporate memory and is an aid in answering a variety of questions associated with the enactment of any software process. Measurement also helps, during the course of a project, to assess its progress, to take corrective action based on this assessment, and to evaluate the impact of such action. According to many studies made on the application of metrics and models in industrial environments, measurement in order to be effective must be. Focused on specific goals Applied to all life-cycle products, processes, and resources Interpreted on the basis of characterization and understanding of the organizational context, environment, and goals This means that measurement must be defined in a top-down fashion. It must be focused, based on goals and models. A metric-driven, bottom-up approach, will not work because there are many observable characteristics in software (e.g., time, number of defects, complexity, lines of code, severity of failures, effort, productivity, defect density). A context specific selection of metrics and guidelines on how to use and interpret them should be made, based on the appropriate models and goals of that environment. The most common and popular mechanism for goal-oriented software measurement is the Goal Question Metric approach which is presented in this article in combination with examples from GQM application in industry}
}

@misc{EnergyEfficiencyLLMCode2024,
  title        = {GitHub Repository: EnergyEfficiencyLLMCode},
  year         = {2024},
  publisher    = {GitHub},
  url          = {https://github.com/energyefficienctcode/EnergyEfficiencyLLMCode},
  note         = {Accessed: Dec. 6, 2024}
}

@inproceedings{promptEfficiency,
author = {Niu, Changan and Zhang, Ting and Li, Chuanyi and Luo, Bin and Ng, Vincent},
title = {On Evaluating the Efficiency of Source Code Generated by LLMs},
year = {2024},
isbn = {9798400706097},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3650105.3652295},
doi = {10.1145/3650105.3652295},
abstract = {Recent years have seen the remarkable capabilities of large language models (LLMs) for code generation. Different from existing work that evaluate the correctness of the code generated by LLMs, we propose to further evaluate its efficiency. More efficient code can lead to higher performance and execution efficiency of programs and software completed by LLM-assisted programming. First, we evaluate the efficiency of the code generated by LLMs on two benchmarks, HumanEval and MBPP. Then, we choose a set of programming problems from the online judge platform LeetCode to conduct a more difficult evaluation. Finally, we explore several prompts that would enable LLMs to generate more efficient code.},
booktitle = {Proceedings of the 2024 IEEE/ACM First International Conference on AI Foundation Models and Software Engineering},
pages = {103–107},
numpages = {5},
location = {Lisbon, Portugal},
series = {FORGE '24}
}

@misc{statistaMostUsed,
  author       = {{Statista}},
  title        = {Most used languages among software developers globally 2024},
  howpublished = {\url{https://www.statista.com/statistics/793628/worldwide-developer-survey-most-used-languages/}},
  year         = {2024},
  note         = {[Accessed: 12-Sep-2024]}
}

@inproceedings{niu2024evaluating,
  author    = {Changan Niu and Ting Zhang and Chuanyi Li and Bin Luo and Vincent Ng},
  title     = {On Evaluating the Efficiency of Source Code Generated by LLMs},
  booktitle = {Proceedings of the AI Foundation Models and Software Engineering (FORGE '24)},
  year      = {2024},
  pages     = {5},
  publisher = {ACM},
  address   = {Lisbon, Portugal},
  doi       = {10.1145/3650105.3652295},
  url       = {https://doi.org/10.1145/3650105.3652295}
}

@article{vartziotis2024learn,
  author    = {Tina Vartziotis and Ippolyti Dellatolas and George Dasoulas and Maximilian Schmidt and Florian Schneider and Tim Hoffmann and Sotirios Kotsopoulos and Michael Keckeisen},
  title     = {Learn to Code Sustainably: An Empirical Study on LLM-based Green Code Generation},
  journal   = {arXiv preprint},
  volume    = {arXiv:2403.03344v1},
  year      = {2024},
  url       = {https://arxiv.org/abs/2403.03344v1}
}

@article{du2024mercury,
  title     = {Mercury: A Code Efficiency Benchmark for Code Large Language Models},
  author    = {Mingzhe Du and Anh Tuan Luu and Bin Ji and Qian Liu and See-Kiong Ng},
  journal   = {arXiv preprint},
  volume    = {arXiv:2402.07844v4},
  year      = {2024},
  url       = {https://arxiv.org/abs/2402.07844v4},
}

@article{huang2024effibench,
  title     = {EFFIBENCH: Benchmarking the Efficiency of Automatically Generated Code},
  author    = {Dong Huang and Yuhao Qing and Weiyi Shang and Heming Cui and Jie M. Zhang},
  journal   = {arXiv preprint},
  volume    = {arXiv:2402.02037v4},
  year      = {2024},
  url       = {https://arxiv.org/abs/2402.02037v4},
}


@article{johnson2023large,
  author    = {Martin Johnson and Kim Lee},
  title     = {Large Language Models: A Performance Comparison for Code Generation Tasks},
  journal   = {arXiv preprint},
  volume    = {arXiv:2302.07867v5},
  year      = {2023},
  url       = {https://arxiv.org/abs/2302.07867v5}
}

@article{radford2023evaluating,
  author    = {Alec Radford and Kai Chen},
  title     = {Evaluating the Performance of GPT-4 on Code Generation Benchmarks},
  journal   = {Technical Report},
  year      = {2023},
  url       = {https://www.some-link-to-the-paper.com}
}

@misc{liu2024evaluatinglanguagemodelsefficient,
      title={Evaluating Language Models for Efficient Code Generation}, 
      author={Jiawei Liu and Songrun Xie and Junhao Wang and Yuxiang Wei and Yifeng Ding and Lingming Zhang},
      year={2024},
      eprint={2408.06450},
      archivePrefix={arXiv},
      primaryClass={cs.SE},
      url={https://arxiv.org/abs/2408.06450}, 
}

@inproceedings{duCodeGen, author = {Du, Xueying and Liu, Mingwei and Wang, Kaixin and Wang, Hanlin and Liu, Junwei and Chen, Yixuan and Feng, Jiayi and Sha, Chaofeng and Peng, Xin and Lou, Yiling}, title = {Evaluating Large Language Models in Class-Level Code Generation}, year = {2024}, isbn = {9798400702174}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3597503.3639219}, doi = {10.1145/3597503.3639219}, abstract = {Recently, many large language models (LLMs) have been proposed, showing advanced proficiency in code generation. Meanwhile, many efforts have been dedicated to evaluating LLMs on code generation benchmarks such as HumanEval. Although being very helpful for comparing different LLMs, existing evaluation focuses on a simple code generation scenario (i.e., function-level or statement-level code generation), which mainly asks LLMs to generate one single code unit (e.g., a function or a statement) for the given natural language description. Such evaluation focuses on generating independent and often small-scale code units, thus leaving it unclear how LLMs perform in real-world software development scenarios.To fill this knowledge gap, we make the first attempt to evaluate LLMs in a more challenging code generation scenario, i.e., class-level code generation. Compared with existing code generation benchmarks, it better reflects real-world software development scenarios due to it comprising broader contextual dependencies and multiple, interdependent units of code. We first manually construct the first class-level code generation benchmark ClassEval of 100 class-level Python code generation tasks with approximately 500 person-hours. Based on the new benchmark ClassEval, we then perform the first study of 11 state-of-the-art LLMs on class-level code generation. Based on our results, we find that all LLMs perform much worse on class-level code generation compared to the method-level. While GPT models still dominate other LLMs on class-level code generation, the performance rankings of other models on method-level code generation no longer holds for class-level code generation. Besides, most models (except GPT models) perform better when generating the class method by method; and they have the limited ability of generating dependent code. Based on our findings, we call for software engineering (SE) researchers' expertise to build more LLM benchmarks based on practical and complicated software development scenarios.}, booktitle = {Proceedings of the IEEE/ACM 46th International Conference on Software Engineering}, articleno = {81}, numpages = {13}, keywords = {class-level code generation, large language model, benchmark}, location = {Lisbon, Portugal}, series = {ICSE '24} }

@misc{shypula2024learningperformanceimprovingcodeedits,
      title={Learning Performance-Improving Code Edits}, 
      author={Alexander Shypula and Aman Madaan and Yimeng Zeng and Uri Alon and Jacob Gardner and Milad Hashemi and Graham Neubig and Parthasarathy Ranganathan and Osbert Bastani and Amir Yazdanbakhsh},
      year={2024},
      eprint={2302.07867},
      archivePrefix={arXiv},
      primaryClass={cs.SE},
      url={https://arxiv.org/abs/2302.07867}, 
}

@ARTICLE{9585139,
  author={Verdecchia, Roberto and Lago, Patricia and Ebert, Christof and de Vries, Carol},
  journal={IEEE Software}, 
  title={Green IT and Green Software}, 
  year={2021},
  volume={38},
  number={6},
  pages={7-15},
  keywords={Energy consumption;Data centers;Climate change;Program processors;Codes;Air pollution;Software design;Usability},
  doi={10.1109/MS.2021.3102254}}

@ARTICLE{10214579,
  author={Paul, Showmick Guha and Saha, Arpa and Arefin, Mohammad Shamsul and Bhuiyan, Touhid and Biswas, Al Amin and Reza, Ahmed Wasif and Alotaibi, Naif M. and Alyami, Salem A. and Moni, Mohammad Ali},
  journal={IEEE Access}, 
  title={A Comprehensive Review of Green Computing: Past, Present, and Future Research}, 
  year={2023},
  volume={11},
  number={},
  pages={87445-87494},
  keywords={Green computing;Green products;Sustainable development;Energy efficiency;Cloud computing;Software;Hardware;Carbon emissions;Green computing;Sustainable development;Carbon emissions;eco-friendly computing;energy efficiency;green computing;green computing area;sustainable development},
  doi={10.1109/ACCESS.2023.3304332}}

@inproceedings{strubell-etal-2019-energy,
    title = "Energy and Policy Considerations for Deep Learning in {NLP}",
    author = "Strubell, Emma  and
      Ganesh, Ananya  and
      McCallum, Andrew",
    editor = "Korhonen, Anna  and
      Traum, David  and
      M{\`a}rquez, Llu{\'\i}s",
    booktitle = "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics",
    month = jul,
    year = "2019",
    address = "Florence, Italy",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/P19-1355",
    doi = "10.18653/v1/P19-1355",
    pages = "3645--3650",
    abstract = "Recent progress in hardware and methodology for training neural networks has ushered in a new generation of large networks trained on abundant data. These models have obtained notable gains in accuracy across many NLP tasks. However, these accuracy improvements depend on the availability of exceptionally large computational resources that necessitate similarly substantial energy consumption. As a result these models are costly to train and develop, both financially, due to the cost of hardware and electricity or cloud compute time, and environmentally, due to the carbon footprint required to fuel modern tensor processing hardware. In this paper we bring this issue to the attention of NLP researchers by quantifying the approximate financial and environmental costs of training a variety of recently successful neural network models for NLP. Based on these findings, we propose actionable recommendations to reduce costs and improve equity in NLP research and practice.",
}
@article{Khan:2018:RAE,
  author       = {Kashif Nizam Khan and
                  Mikael Hirki and
                  Tapio Niemi and
                  Jukka K. Nurminen and
                  Zhonghong Ou},
  title        = {{RAPL} in Action: Experiences in Using {RAPL} for Power Measurements},
  journal      = {{ACM} Trans. Model. Perform. Evaluation Comput. Syst.},
  volume       = {3},
  number       = {2},
  pages        = {9:1--9:26},
  year         = {2018},
}

@Article{info11040193,
AUTHOR = {Raschka, Sebastian and Patterson, Joshua and Nolet, Corey},
TITLE = {Machine Learning in Python: Main Developments and Technology Trends in Data Science, Machine Learning, and Artificial Intelligence},
JOURNAL = {Information},
VOLUME = {11},
YEAR = {2020},
NUMBER = {4},
ARTICLE-NUMBER = {193},
URL = {https://www.mdpi.com/2078-2489/11/4/193},
ISSN = {2078-2489},
ABSTRACT = {Smarter applications are making better use of the insights gleaned from data, having an impact on every industry and research discipline. At the core of this revolution lies the tools and the methods that are driving it, from processing the massive piles of data generated each day to learning from and taking useful action. Deep neural networks, along with advancements in classical machine learning and scalable general-purpose graphics processing unit (GPU) computing, have become critical components of artificial intelligence, enabling many of these astounding breakthroughs and lowering the barrier to adoption. Python continues to be the most preferred language for scientific computing, data science, and machine learning, boosting both performance and productivity by enabling the use of low-level libraries and clean high-level APIs. This survey offers insight into the field of machine learning with Python, taking a tour through important topics to identify some of the core hardware and software paradigms that have enabled it. We cover widely-used libraries and concepts, collected together for holistic comparison, with the goal of educating the reader and driving the field of Python machine learning forward.},
DOI = {10.3390/info11040193}
}

@ARTICLE{10507163,
  author={Liu, Zhijie and Tang, Yutian and Luo, Xiapu and Zhou, Yuming and Zhang, Liang Feng},
  journal={IEEE Transactions on Software Engineering}, 
  title={No Need to Lift a Finger Anymore? Assessing the Quality of Code Generation by ChatGPT}, 
  year={2024},
  volume={50},
  number={6},
  pages={1548-1584},
  keywords={Codes;Chatbots;Task analysis;Complexity theory;Security;Transformers;Electronic mail;Large language model;ChatGPT;code generation},
  doi={10.1109/TSE.2024.3392499}}

@inproceedings{10.1145/3580305.3599790, author = {Zheng, Qinkai and Xia, Xiao and Zou, Xu and Dong, Yuxiao and Wang, Shan and Xue, Yufei and Shen, Lei and Wang, Zihan and Wang, Andi and Li, Yang and Su, Teng and Yang, Zhilin and Tang, Jie}, title = {CodeGeeX: A Pre-Trained Model for Code Generation with Multilingual Benchmarking on HumanEval-X}, year = {2023}, isbn = {9798400701030}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3580305.3599790}, doi = {10.1145/3580305.3599790}, abstract = {Large pre-trained code generation models, such as OpenAI Codex, can generate syntax-and function-correct code, making the coding of programmers more productive. In this paper, we introduce CodeGeeX, a multilingual model with 13 billion parameters for code generation. CodeGeeX is pre-trained on 850 billion tokens of 23 programming languages as of June 2022. Our extensive experiments suggest that CodeGeeX outperforms multilingual code models of similar scale for both the tasks of code generation and translation on HumanEval-X. Building upon HumanEval (Python only), we develop the HumanEval-X benchmark for evaluating multilingual models by hand-writing the solutions in C++, Java, JavaScript, and Go. In addition, we build CodeGeeX-based extensions on Visual Studio Code, JetBrains, and Cloud Studio, generating 8 billion tokens for tens of thousands of active users per week. Our user study demonstrates that CodeGeeX can help to increase coding efficiency for 83.4\% of its users. Finally, CodeGeeX is publicly accessible since Sep. 2022, we open-sourced its code, model weights, API, extensions, and HumanEval-X at https://github.com/THUDM/CodeGeeX.}, booktitle = {Proceedings of the 29th ACM SIGKDD Conference on Knowledge Discovery and Data Mining}, pages = {5673–5684}, numpages = {12}, keywords = {pre-trained model, large language model, code generation}, location = {Long Beach, CA, USA}, series = {KDD '23} }
@article{10.1126/science.abq1158,
author = {Yujia Li  and David Choi  and Junyoung Chung  and Nate Kushman  and Julian Schrittwieser  and Rémi Leblond  and Tom Eccles  and James Keeling  and Felix Gimeno  and Agustin Dal Lago  and Thomas Hubert  and Peter Choy  and Cyprien de Masson d’Autume  and Igor Babuschkin  and Xinyun Chen  and Po-Sen Huang  and Johannes Welbl  and Sven Gowal  and Alexey Cherepanov  and James Molloy  and Daniel J. Mankowitz  and Esme Sutherland Robson  and Pushmeet Kohli  and Nando de Freitas  and Koray Kavukcuoglu  and Oriol Vinyals },
title = {Competition-level code generation with AlphaCode},
journal = {Science},
volume = {378},
number = {6624},
pages = {1092-1097},
year = {2022},
doi = {10.1126/science.abq1158},
URL = {https://www.science.org/doi/abs/10.1126/science.abq1158},
eprint = {https://www.science.org/doi/pdf/10.1126/science.abq1158},
abstract = {Programming is a powerful and ubiquitous problem-solving tool. Systems that can assist programmers or even generate programs themselves could make programming more productive and accessible. Recent transformer-based neural network models show impressive code generation abilities yet still perform poorly on more complex tasks requiring problem-solving skills, such as competitive programming problems. Here, we introduce AlphaCode, a system for code generation that achieved an average ranking in the top 54.3\% in simulated evaluations on recent programming competitions on the Codeforces platform. AlphaCode solves problems by generating millions of diverse programs using specially trained transformer-based networks and then filtering and clustering those programs to a maximum of just 10 submissions. This result marks the first time an artificial intelligence system has performed competitively in programming competitions. Computer programming competitions are popular tests among programmers that require critical thinking informed by experience and creating solutions to unforeseen problems, both of which are key aspects of human intelligence but challenging to mimic by machine learning models. Using self-supervised learning and an encoder-decoder transformer architecture, Li et al. developed AlphaCode, a deep-learning model that can achieve approximately human-level performance on the Codeforces platform, which regularly hosts these competitions and attracts numerous participants worldwide (see the Perspective by Kolter). The development of such coding platforms could have a huge impact on programmers’ productivity. It may even change the culture of programming by shifting human work to formulating problems, with machine learning being the main one responsible for generating and executing codes. —YS Modern machine learning systems can achieve average human-level performance in popular competitive programming contests.}}

@inproceedings{10.1145/3520312.3534862, author = {Xu, Frank F. and Alon, Uri and Neubig, Graham and Hellendoorn, Vincent Josua}, title = {A systematic evaluation of large language models of code}, year = {2022}, isbn = {9781450392730}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3520312.3534862}, doi = {10.1145/3520312.3534862}, abstract = {Large language models (LMs) of code have recently shown tremendous promise in completing code and synthesizing code from natural language descriptions. However, the current state-of-the-art code LMs (e.g., Codex) are not publicly available, leaving many questions about their model and data design decisions. We aim to fill in some of these blanks through a systematic evaluation of the largest existing models: Codex, GPT-J, GPT-Neo, GPT-NeoX-20B, and CodeParrot, across various programming languages. Although Codex itself is not open-source, we find that existing opensource models do achieve close results in some programming languages, although targeted mainly for natural language  modeling. We further identify an important missing piece in the form of a large open-source model trained exclusively on a multi-lingual corpus of code. We release a new model, PolyCoder, with 2.7B parameters based on the GPT-2 architecture, that was trained on 249GB of code across 12 programming  languages on a single machine. In the C programming language, PolyCoder outperforms all models including Codex. Our trained models are open-source and publicly available at https://github.com/VHellendoorn/Code-LMs, which enables future research and application in this area.  We have an online appendix at https://arxiv.org/abs/2202.13169.}, booktitle = {Proceedings of the 6th ACM SIGPLAN International Symposium on Machine Programming}, pages = {1–10}, numpages = {10}, keywords = {pretraining, open-source, evaluation, code language model, code generation}, location = {San Diego, CA, USA}, series = {MAPS 2022} }

@misc{o1mini,
  title = {OpenAI o1-mini},
    author={OpenAI},
  howpublished = {\url{https://openai.com/index/openai-o1-mini-advancing-cost-efficient-reasoning/}},
  note = {Accessed: 2024-11-01}
}

@article{reasons,
author = {Palminteri, Stefano and Yax, Nicolas and Anllo, Hernan},
year = {2023},
month = {06},
pages = {},
title = {Studying and improving reasoning in humans and machines},
journal ={Nature},
doi = {10.21203/rs.3.rs-3124634/v1}
}

@INPROCEEDINGS{10444862,
  author={Cui, Jinku and Zhao, Qidong and Hao, Yueming and Liu, Xu},
  booktitle={2024 IEEE/ACM International Symposium on Code Generation and Optimization (CGO)}, 
  title={DrPy: Pinpointing Inefficient Memory Usage in Multi-Layer Python Applications}, 
  year={2024},
  volume={},
  number={},
  pages={245-257},
  keywords={Codes;Runtime;Source coding;Flow graphs;Optimization;Monitoring;Python},
  doi={10.1109/CGO57630.2024.10444862}}
@article{Reya2023GreenPyEA,
  title={GreenPy: Evaluating Application-Level Energy Efficiency in Python for Green Computing},
  author={Nurzihan Fatema Reya and Abtahi Ahmed and Tashfia Rifa Zaman and Md. Motaharul Islam},
  journal={Annals of Emerging Technologies in Computing},
  year={2023},
  url={https://api.semanticscholar.org/CorpusID:259509821}
}

@InProceedings{10.1007/978-3-540-79409-7_23,
author="Langtangen, Hans Petter
and Cai, Xing",
editor="Bock, Hans Georg
and Kostina, Ekaterina
and Phu, Hoang Xuan
and Rannacher, Rolf",
title="On the Efficiency of Python for High-Performance Computing: A Case Study Involving Stencil Updates for Partial Differential Equations",
booktitle="Modeling, Simulation and Optimization of Complex Processes",
year="2008",
publisher="Springer Berlin Heidelberg",
address="Berlin, Heidelberg",
pages="337--357",
abstract="The purpose of this paper is to assess the loss of computational efficiency that may occur when scientific codes are written in the Python programming language instead of Fortran or C. Our test problems concern the application of a seven-point finite stencil for a three-dimensional, variable coefficient, Laplace operator. This type of computation appears in lots of codes solving partial differential equations, and the variable coefficient is a key ingredient to capture the arithmetic complexity of stencils arising in advanced multi-physics problems in heterogeneous media.",
isbn="978-3-540-79409-7"
}

@inproceedings{compiler_opt, author = {Schmitt, Norbert and Bucek, James and Lange, Klaus-Dieter and Kounev, Samuel}, title = {Energy Efficiency Analysis of Compiler Optimizations on the SPEC CPU 2017 Benchmark Suite}, year = {2020}, isbn = {9781450371094}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3375555.3383759}, doi = {10.1145/3375555.3383759}, abstract = {The growth of cloud services leads to more and more data centers that are increasingly larger and consume considerable amounts of power. To increase energy efficiency, both the actual server equipment and the software themselves must become more energy-efficient. It is the software that controls the hardware to a considerable degree. In this work-in-progress paper, we present a first analysis of how compiler optimizations can influence energy efficiency. We base our analysis on workloads of the SPEC CPU 2017 benchmark. With 43 benchmarks from different domains, including integer and floating-point heavy computations executed on a state-of-the-art server system for cloud applications, SPEC CPU 2017 offers a representative selection of workloads.}, booktitle = {Companion of the ACM/SPEC International Conference on Performance Engineering}, pages = {38–41}, numpages = {4}, keywords = {performance, energy efficiency, compiler optimizations, benchmark, SPEC CPU 2017}, location = {Edmonton AB, Canada}, series = {ICPE '20} }

@inproceedings{han-etal-2023-sample,
    title = "On Sample-Efficient Code Generation",
    author = "Han, Hojae  and
      Kim, Yu Jin  and
      Kim, Byoungjip  and
      Lee, Youngwon  and
      Lee, Kyungjae  and
      Lee, Kyungmin  and
      Lee, Moontae  and
      Bae, Kyunghoon  and
      Hwang, Seung-won",
    editor = "Wang, Mingxuan  and
      Zitouni, Imed",
    booktitle = "Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing: Industry Track",
    month = dec,
    year = "2023",
    address = "Singapore",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2023.emnlp-industry.73",
    doi = "10.18653/v1/2023.emnlp-industry.73",
    pages = "783--791",
    abstract = "Large language models often struggle to predict runtime behavior in code generation tasks, leading to a reliance on rejection sampling (best-of-n) to generate multiple code snippets then select the best. Our distinction is reducing sampling costs, without compromising generation quality. We introduce EFFICODE, a novel framework that prioritizes sampling on test problems that models can solve. We show how EFFICODE estimates solvability to optimize computational costs during multiple sampling. Based on empirical evidence, EFFICODE consistently demonstrates reduced sampling budgets while maintaining comparable code generation performance, especially when problems are challenging. In addition, utilizing EFFICODE to rank sampled code snippets also shows its effectiveness in answer code selection for reducing temporal costs, by not requiring any execution or test case generation.",
}

@article{Nitin_Sherje_2024, title={Enhancing Software Development Efficiency through AI-Powered Code Generation}, volume={5}, url={https://technicaljournals.org/RJCSE/index.php/journal/article/view/90}, abstractNote={&amp;lt;p&amp;gt;Software development is a critical process in today&amp;#039;s digital age, demanding high levels of efficiency and accuracy. However, traditional methods of coding often prove time-consuming and error-prone. To address these challenges, recent advancements in artificial intelligence (AI) have introduced a novel approach – AI-powered code generation. This paper delves into the potential of AI-powered code generation techniques to significantly enhance software development efficiency. Beginning with an exploration of the current landscape of AI in software development, we scrutinize various AI-powered code generation methodologies, including rule-based systems, machine learning algorithms, neural networks, generative adversarial networks (GANs), and transformer models. We assess the benefits of AI-powered code generation, such as accelerated development speed, heightened code quality, reduced human error, and increased developer productivity. Moreover, we scrutinize the challenges and limitations associated with these techniques, encompassing data quality, interpretability, domain-specific knowledge, and ethical considerations. Through case studies and real-world examples, we illustrate the practical applications and implications of AI-generated code.&amp;lt;/p&amp;gt;}, number={1}, journal={Research Journal of Computer Systems and Engineering}, author={Nitin Sherje}, year={2024}, month={Jul.}, pages={01–12} }

@inproceedings{copilotAcc, author = {Yetistiren, Burak and Ozsoy, Isik and Tuzun, Eray}, title = {Assessing the quality of GitHub copilot’s code generation}, year = {2022}, isbn = {9781450398602}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3558489.3559072}, doi = {10.1145/3558489.3559072}, abstract = {The introduction of GitHub’s new code generation tool, GitHub Copilot, seems to be the first well-established instance of an AI pair-programmer. GitHub Copilot has access to a large number of open-source projects, enabling it to utilize more extensive code in various programming languages than other code generation tools. Although the initial and informal assessments are promising, a systematic evaluation is needed to explore the limits and benefits of GitHub Copilot. The main objective of this study is to assess the quality of generated code provided by GitHub Copilot. We also aim to evaluate the impact of the quality and variety of input parameters fed to GitHub Copilot. To achieve this aim, we created an experimental setup for evaluating the generated code in terms of validity, correctness, and efficiency. Our results suggest that GitHub Copilot was able to generate valid code with a 91.5\% success rate. In terms of code correctness, out of 164 problems, 47 (28.7\%) were correctly, while 84 (51.2\%) were partially correctly, and 33 (20.1\%) were incorrectly generated. Our empirical analysis shows that GitHub Copilot is a promising tool based on the results we obtained, however further and more comprehensive assessment is needed in the future.}, booktitle = {Proceedings of the 18th International Conference on Predictive Models and Data Analytics in Software Engineering}, pages = {62–71}, numpages = {10}, keywords = {empirical study, code generation, code completion, GitHub Copilot, AI pair programmer}, location = {Singapore, Singapore}, series = {PROMISE 2022} }

@article{KERN201553,
title = {Impacts of software and its engineering on the carbon footprint of ICT},
journal = {Environmental Impact Assessment Review},
volume = {52},
pages = {53-61},
year = {2015},
note = {Information technology and renewable energy - Modelling, simulation, decision support and environmental assessment},
issn = {0195-9255},
doi = {https://doi.org/10.1016/j.eiar.2014.07.003},
url = {https://www.sciencedirect.com/science/article/pii/S0195925514000687},
author = {Eva Kern and Markus Dick and Stefan Naumann and Tim Hiller},
keywords = {Green Software Development, Carbon footprint, Energy efficiency measurements},
abstract = {The energy consumption of information and communication technology (ICT) is still increasing. Even though several solutions regarding the hardware side of Green IT exist, the software contribution to Green IT is not well investigated. The carbon footprint is one way to rate the environmental impacts of ICT. In order to get an impression of the induced CO2 emissions of software, we will present a calculation method for the carbon footprint of a software product over its life cycle. We also offer an approach on how to integrate some aspects of carbon footprint calculation into software development processes and discuss impacts and tools regarding this calculation method. We thus show the relevance of energy measurements and the attention to impacts on the carbon footprint by software within Green Software Engineering.}
}

@article{CAPRA201260,
title = {Is software “green”? Application development environments and energy efficiency in open source applications},
journal = {Information and Software Technology},
volume = {54},
number = {1},
pages = {60-71},
year = {2012},
issn = {0950-5849},
doi = {https://doi.org/10.1016/j.infsof.2011.07.005},
url = {https://www.sciencedirect.com/science/article/pii/S0950584911001777},
author = {Eugenio Capra and Chiara Francalanci and Sandra A. Slaughter},
keywords = {Green IT, Software energy efficiency, Software development application environment},
abstract = {Context
The energy efficiency of IT systems, also referred to as Green IT, is attracting more and more attention. While several researchers have focused on the energy efficiency of hardware and embedded systems, the role of application software in IT energy consumption still needs investigation.
Objective
This paper aims to define a methodology for measuring software energy efficiency and to understand the consequences of abstraction layers and application development environments for the energy efficiency of software applications.
Method
We first develop a measure of energy efficiency that is appropriate for software applications. We then examine how the use of application development environments relates to this measure of energy efficiency for a sample of 63 open source software applications.
Results
Our findings indicate that a greater use of application development environments – specifically, frameworks and external libraries – is more detrimental in terms of energy efficiency for larger applications than for smaller applications. We also find that different functional application types have distinctly different levels of energy efficiency, with text and image editing and gaming applications being the most energy inefficient due to their intense use of the processor.
Conclusion
We conclude that different designs can have a significant impact on the energy efficiency of software applications. We have related the use of software application development environments to software energy efficiency suggesting that there may be a trade-off between development efficiency and energy efficiency. We propose new research to further investigate this topic.}
}
@inproceedings{EmpericalGreen, author = {Manotas, Irene and Bird, Christian and Zhang, Rui and Shepherd, David and Jaspan, Ciera and Sadowski, Caitlin and Pollock, Lori and Clause, James}, title = {An empirical study of practitioners' perspectives on green software engineering}, year = {2016}, isbn = {9781450339001}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/2884781.2884810}, doi = {10.1145/2884781.2884810}, abstract = {The energy consumption of software is an increasing concern as the use of mobile applications, embedded systems, and data center-based services expands. While research in green software engineering is correspondingly increasing, little is known about the current practices and perspectives of software engineers in the field. This paper describes the first empirical study of how practitioners think about energy when they write requirements, design, construct, test, and maintain their software. We report findings from a quantitative, targeted survey of 464 practitioners from ABB, Google, IBM, and Microsoft, which was motivated by and supported with qualitative data from 18 in-depth interviews with Microsoft employees. The major findings and implications from the collected data contextualize existing green software engineering research and suggest directions for researchers aiming to develop strategies and tools to help practitioners improve the energy usage of their applications.}, booktitle = {Proceedings of the 38th International Conference on Software Engineering}, pages = {237–248}, numpages = {12}, keywords = {survey, green software engineering, empirical study}, location = {Austin, Texas}, series = {ICSE '16} }

@article{ALZOUBI2024143090,
title = {Green artificial intelligence initiatives: Potentials and challenges},
journal = {Journal of Cleaner Production},
volume = {468},
pages = {143090},
year = {2024},
issn = {0959-6526},
doi = {https://doi.org/10.1016/j.jclepro.2024.143090},
url = {https://www.sciencedirect.com/science/article/pii/S0959652624025393},
author = {Yehia Ibrahim Alzoubi and Alok Mishra},
keywords = {Artificial intelligence, Carbon footprint, Cloud, Green AI, Green AI tools, Sustainability},
abstract = {Recently, the widespread adoption of artificial intelligence, particularly generative AI technology, has surged across various industries. However, a notable drawback of this technology is its significant energy consumption during model training and operation, which poses challenges to sustainability goals and the environment. Consequently, various initiatives have emerged to promote what is termed "green artificial intelligence," aiming to mitigate these environmental impacts. Nevertheless, research discussing these initiatives remains scarce. Hence, this study aims to identify green artificial intelligence initiatives that contribute to environmental friendliness. This paper has comprehensively reviewed the existing literature, professional websites, and expert blogs to identify and analyze available green AI initiatives. This paper has identified 55 such initiatives, broadly categorized into six themes: cloud optimization, model efficiency, carbon footprinting, sustainability-focused AI development, open-source initiatives, and green AI research and community. This study discusses the strengths and limitations of each initiative to offer a comprehensive overview. The findings provide valuable insights, particularly for industries interested in green artificial intelligence and green technology in general. While some tools have been recognized and studied, comprehensive research and analysis are still required to empirically evaluate the majority of other tools due to their early stages of development in this field.}
}

@ARTICLE{8584429,
  author={Yang, Jun and Xiao, Wenjing and Jiang, Chun and Hossain, M. Shamim and Muhammad, Ghulam and Amin, Syed Umar},
  journal={IEEE Access}, 
  title={AI-Powered Green Cloud and Data Center}, 
  year={2019},
  volume={7},
  number={},
  pages={4195-4203},
  keywords={Cloud computing;Data centers;Engines;Energy consumption;Processor scheduling;Green products;Optimization;Green cloud and data center;energy optimizing;deep learning;particle swarm optimization},
  doi={10.1109/ACCESS.2018.2888976}}

@article{DESISLAVOV2023100857,
title = {Trends in AI inference energy consumption: Beyond the performance-vs-parameter laws of deep learning},
journal = {Sustainable Computing: Informatics and Systems},
volume = {38},
pages = {100857},
year = {2023},
issn = {2210-5379},
doi = {https://doi.org/10.1016/j.suscom.2023.100857},
url = {https://www.sciencedirect.com/science/article/pii/S2210537923000124},
author = {Radosvet Desislavov and Fernando Martínez-Plumed and José Hernández-Orallo},
keywords = {Artificial Intelligence, Deep learning, Inference, Energy consumption, Performance analysis, Performance evaluation, AI progress},
abstract = {The progress of some AI paradigms such as deep learning is said to be linked to an exponential growth in the number of parameters. There are many studies corroborating these trends, but does this translate into an exponential increase in energy consumption? In order to answer this question we focus on inference costs rather than training costs, as the former account for most of the computing effort, solely because of the multiplicative factors. Also, apart from algorithmic innovations, we account for more specific and powerful hardware (leading to higher FLOPS) that is usually accompanied with important energy efficiency optimisations. We also move the focus from the first implementation of a breakthrough paper towards the consolidated version of the techniques one or two year later. Under this distinctive and comprehensive perspective, we analyse relevant models in the areas of computer vision and natural language processing: for a sustained increase in performance we see a much softer growth in energy consumption than previously anticipated. The only caveat is, yet again, the multiplicative factor, as future AI increases penetration and becomes more pervasive.}
}
@inbook{inbook,
author = {Alloghani, Mohamed},
year = {2024},
month = {01},
pages = {65-86},
title = {Architecting Green Artificial Intelligence Products: Recommendations for Sustainable AI Software Development and Evaluation},
isbn = {978-3-031-45213-0},
doi = {10.1007/978-3-031-45214-7_4}
}

@article{BOLONCANEDO2024128096,
title = {A review of green artificial intelligence: Towards a more sustainable future},
journal = {Neurocomputing},
volume = {599},
pages = {128096},
year = {2024},
issn = {0925-2312},
doi = {https://doi.org/10.1016/j.neucom.2024.128096},
url = {https://www.sciencedirect.com/science/article/pii/S0925231224008671},
author = {Verónica Bolón-Canedo and Laura Morán-Fernández and Brais Cancela and Amparo Alonso-Betanzos},
keywords = {Green machine learning, Sustainability, Green-by AI, Green-in AI},
abstract = {Green artificial intelligence (AI) is more environmentally friendly and inclusive than conventional AI, as it not only produces accurate results without increasing the computational cost but also ensures that any researcher with a laptop can perform high-quality research without the need for costly cloud servers. This paper discusses green AI as a pivotal approach to enhancing the environmental sustainability of AI systems. Described are AI solutions for eco-friendly practices in other fields (green-by AI), strategies for designing energy-efficient machine learning (ML) algorithms and models (green-in AI), and tools for accurately measuring and optimizing energy consumption. Also examined are the role of regulations in promoting green AI and future directions for sustainable ML. Underscored is the importance of aligning AI practices with environmental considerations, fostering a more eco-conscious and energy-efficient future for AI systems.}
}

@inproceedings{10.1145/3661167.3661221,
author = {Coignion, Tristan and Quinton, Cl\'{e}ment and Rouvoy, Romain},
title = {A Performance Study of LLM-Generated Code on Leetcode},
year = {2024},
isbn = {9798400717017},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3661167.3661221},
doi = {10.1145/3661167.3661221},
abstract = {This study evaluates the efficiency of code generation by Large Language Models (LLMs) and measures their performance against human-crafted solutions using a dataset from Leetcode. We compare 18 LLMs, considering factors such as model temperature and success rate, and their impact on code performance. This research introduces a novel method for measuring and comparing the speed of LLM-generated code, revealing that LLMs produce code with comparable performance, irrespective of the adopted LLM. We also find that LLMs are capable of generating code that is, on average, more efficient than the code written by humans. The paper further discusses the use of Leetcode as a benchmarking dataset, the limitations imposed by potential data contamination, and the platform’s measurement reliability. We believe that our findings contribute to a better understanding of LLM capabilities in code generation and set the stage for future optimizations in the field.},
booktitle = {Proceedings of the 28th International Conference on Evaluation and Assessment in Software Engineering},
pages = {79–89},
numpages = {11},
location = {Salerno, Italy},
series = {EASE '24}
}

@inproceedings{10.1145/3597503.3623316, author = {Yu, Hao and Shen, Bo and Ran, Dezhi and Zhang, Jiaxin and Zhang, Qi and Ma, Yuchi and Liang, Guangtai and Li, Ying and Wang, Qianxiang and Xie, Tao}, title = {CoderEval: A Benchmark of Pragmatic Code Generation with Generative Pre-trained Models}, year = {2024}, isbn = {9798400702174}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3597503.3623316}, doi = {10.1145/3597503.3623316}, abstract = {Code generation models based on the pre-training and fine-tuning paradigm have been increasingly attempted by both academia and industry, resulting in well-known industrial models such as Codex, CodeGen, and PanGu-Coder. To evaluate the effectiveness of these models, multiple existing benchmarks (e.g., HumanEval and AiXBench) are proposed, including only cases of generating a standalone function, i.e., a function that may invoke or access only built-in functions and standard libraries. However, non-standalone functions, which typically are not included in the existing benchmarks, constitute more than 70\% of the functions in popular open-source projects, and evaluating models' effectiveness on standalone functions cannot reflect these models' effectiveness on pragmatic code generation scenarios (i.e., code generation for real settings of open source or proprietary code).To help bridge the preceding gap, in this paper, we propose a benchmark named CoderEval, consisting of 230 Python and 230 Java code generation tasks carefully curated from popular real-world open-source projects and a self-contained execution platform to automatically assess the functional correctness of generated code. CoderEval supports code generation tasks from six levels of context dependency, where context refers to code elements such as types, APIs, variables, and consts defined outside the function under generation but within the dependent third-party libraries, current class, file, or project. CoderEval can be used to evaluate the effectiveness of models in generating code beyond only standalone functions. By evaluating three state-of-the-art code generation models (CodeGen, PanGu-Coder, and ChatGPT) on CoderEval and HumanEval, we find that the effectiveness of these models in generating standalone functions is substantially higher than that in generating non-standalone functions. Our analysis highlights the current progress and pinpoints future directions to further improve a model's effectiveness by leveraging contextual information for pragmatic code generation.}, booktitle = {Proceedings of the IEEE/ACM 46th International Conference on Software Engineering}, articleno = {37}, numpages = {12}, keywords = {code generation, large language models, benchmark}, location = {Lisbon, Portugal}, series = {ICSE '24} }

@article{Hendrycks2021MeasuringCC,
  title={Measuring Coding Challenge Competence With APPS},
  author={Dan Hendrycks and Steven Basart and Saurav Kadavath and Mantas Mazeika and Akul Arora and Ethan Guo and Collin Burns and Samir Puranik and Horace He and Dawn Xiaodong Song and Jacob Steinhardt},
  journal={ArXiv},
  year={2021},
  volume={abs/2105.09938},
  url={https://api.semanticscholar.org/CorpusID:234790100}
}

@inproceedings{wang-etal-2023-recode,
    title = "{R}e{C}ode: Robustness Evaluation of Code Generation Models",
    author = "Wang, Shiqi  and
      Li, Zheng  and
      Qian, Haifeng  and
      Yang, Chenghao  and
      Wang, Zijian  and
      Shang, Mingyue  and
      Kumar, Varun  and
      Tan, Samson  and
      Ray, Baishakhi  and
      Bhatia, Parminder  and
      Nallapati, Ramesh  and
      Ramanathan, Murali Krishna  and
      Roth, Dan  and
      Xiang, Bing",
    editor = "Rogers, Anna  and
      Boyd-Graber, Jordan  and
      Okazaki, Naoaki",
    booktitle = "Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = jul,
    year = "2023",
    address = "Toronto, Canada",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2023.acl-long.773",
    doi = "10.18653/v1/2023.acl-long.773",
    pages = "13818--13843",
    abstract = "Code generation models have achieved impressive performance. However, they tend to be brittle as slight edits to a prompt could lead to very different generations; these robustness properties, critical for user experience when deployed in real-life applications, are not well understood. Most existing works on robustness in text or code tasks have focused on classification, while robustness in generation tasks is an uncharted area and to date there is no comprehensive benchmark for robustness in code generation. In this paper, we propose ReCode, a comprehensive robustness evaluation benchmark for code generation models. We customize over 30 transformations specifically for code on docstrings, function and variable names, code syntax, and code format. They are carefully designed to be natural in real-life coding practice, preserve the original semantic meaning, and thus provide multifaceted assessments of a model{'}s robustness performance. With human annotators, we verified that over 90{\%} of the perturbed prompts do not alter the semantic meaning of the original prompt. In addition, we define robustness metrics for code generation models considering the worst-case behavior under each type of perturbation, taking advantage of the fact that executing the generated code can serve as objective evaluation. We demonstrate ReCode on SOTA models using HumanEval, MBPP, as well as function completion tasks derived from them. Interesting observations include: better robustness for CodeGen over InCoder and GPT-J; models are most sensitive to syntax perturbations; more challenging robustness evaluation on MBPP over HumanEval.",
}

@article{Dderlein2022PilotingCA,
  title={Piloting Copilot and Codex: Hot Temperature, Cold Prompts, or Black Magic?},
  author={Jean-Baptiste D{\"o}derlein and Mathieu Acher and Djamel Eddine Khelladi and Beno{\^i}t Combemale},
  journal={ArXiv},
  year={2022},
  volume={abs/2210.14699},
  url={https://api.semanticscholar.org/CorpusID:253117147}
}

@INPROCEEDINGS{9796235,
  author={Nguyen, Nhan and Nadi, Sarah},
  booktitle={2022 IEEE/ACM 19th International Conference on Mining Software Repositories (MSR)}, 
  title={An Empirical Evaluation of GitHub Copilot's Code Suggestions}, 
  year={2022},
  volume={},
  number={},
  pages={1-5},
  keywords={Measurement;Java;Codes;Static analysis;Software;Natural language processing;Complexity theory;Program Synthesis;Codex;GitHub Copilot;Empirical Evaluation},
  doi={10.1145/3524842.3528470}}

@article{10.1145/3702320,
author = {Vasconcelos, Helena and Bansal, Gagan and Fourney, Adam and Liao, Q. Vera and Vaughan, Jennifer Wortman},
title = {Generation Probabilities Are Not Enough: Uncertainty Highlighting in AI Code Completions},
year = {2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
issn = {1073-0516},
url = {https://doi.org/10.1145/3702320},
doi = {10.1145/3702320},
abstract = {Large-scale generative models have enabled the development of AI-powered code completion tools to assist programmers in writing code. Like all AI-powered tools, these code completion tools are not always accurate and can introduce bugs or even security vulnerabilities into code if not properly detected and corrected by a human programmer. One technique that has been proposed and implemented to help programmers locate potential errors is to highlight uncertain tokens. However, little is known about the effectiveness of this technique. Through a mixed-methods study with 30 programmers, we compare three conditions: providing the AI system's code completion alone, highlighting tokens with the lowest likelihood of being generated by the underlying generative model, and highlighting tokens with the highest predicted likelihood of being edited by a programmer. We find that highlighting tokens with the highest predicted likelihood of being edited leads to faster task completion and more targeted edits, and is subjectively preferred by study participants. In contrast, highlighting tokens according to their probability of being generated does not provide any benefit over the baseline with no highlighting. We further explore the design space of how to convey uncertainty in AI-powered code completion tools and find that programmers prefer highlights that are granular, informative, interpretable, and not overwhelming. This work contributes to building an understanding of what uncertainty means for generative models and how to convey it effectively.},
note = {Just Accepted},
journal = {ACM Trans. Comput.-Hum. Interact.},
month = oct,
keywords = {human-AI programming, generative AI, uncertainty}
}

@article{Huang2024EffiCodeUC,
  title={Effi-Code: Unleashing Code Efficiency in Language Models},
  author={Dong Huang and Guangtao Zeng and Jianbo Dai and Meng Luo and Han Weng and Yuhao Qing and Heming Cui and Zhijiang Guo and Jie M. Zhang},
  journal={ArXiv},
  year={2024},
  volume={abs/2410.10209},
  url={https://api.semanticscholar.org/CorpusID:273345361}
}

@INPROCEEDINGS{10403378,
  author={Wang, Jianxun and Chen, Yixiang},
  booktitle={2023 IEEE International Conference on Medical Artificial Intelligence (MedAI)}, 
  title={A Review on Code Generation with LLMs: Application and Evaluation}, 
  year={2023},
  volume={},
  number={},
  pages={284-289},
  keywords={Productivity;Computer science;Codes;Writing;Encoding;Task analysis;Software engineering;large language models (LLMs);code generation;code completion;automatic program repair;code quality evaluation},
  doi={10.1109/MedAI59581.2023.00044}}

@inproceedings{10.1145/3627217.3627233,
author = {Balse, Rishabh and Kumar, Viraj and Prasad, Prajish and Warriem, Jayakrishnan Madathil},
title = {Evaluating the Quality of LLM-Generated Explanations for Logical Errors in CS1 Student Programs},
year = {2023},
isbn = {9798400708404},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3627217.3627233},
doi = {10.1145/3627217.3627233},
abstract = {When students in CS1 (Introductory Programming) write erroneous code, course staff can use automated tools to provide various types of helpful feedback. In this paper, we focus on syntactically correct student code containing logical errors. Tools that explain logical errors typically require course staff to invest greater effort than tools that detect such errors. To reduce this effort, prior work has investigated the use of Large Language Models (LLMs) such as GPT-3 to generate explanations. Unfortunately, these explanations can be incomplete or incorrect, and therefore unhelpful if presented to students directly. Nevertheless, LLM-generated explanations may be of adequate quality for Teaching Assistants (TAs) to efficiently craft helpful explanations on their basis. We evaluate the quality of explanations generated by an LLM (GPT-3.5-turbo) in two ways, for 30&nbsp;buggy student solutions across 6&nbsp;code-writing problems. First, in a study with 5&nbsp;undergraduate TAs, we compare TA perception of LLM-generated and peer-generated explanation quality. TAs were unaware which explanations were LLM-generated, but they found them to be comparable in quality to peer-generated explanations. Second, we performed a detailed manual analysis of LLM-generated explanations for all 30&nbsp;buggy solutions. We found at least one incorrect statement in 15/30 explanations (50\%). However, in 28/30 cases (93\%), the LLM-generated explanation correctly identified at least one logical error. Our results suggest that for large CS1 courses, TAs with adequate training to detect erroneous statements may be able to extract value from such explanations.},
booktitle = {Proceedings of the 16th Annual ACM India Compute Conference},
pages = {49–54},
numpages = {6},
keywords = {Explanation, GPT-3.5-Turbo, Large language models (LLMs), Logical Errors, Python Programming},
location = {Hyderabad, India},
series = {COMPUTE '23}
}
@article{Chen2021EvaluatingLL,
  title={Evaluating Large Language Models Trained on Code},
  author={Mark Chen and Jerry Tworek and Heewoo Jun and Qiming Yuan and Henrique Pond{\'e} and Jared Kaplan and Harrison Edwards and Yura Burda and Nicholas Joseph and Greg Brockman and Alex Ray and Raul Puri and Gretchen Krueger and Michael Petrov and Heidy Khlaaf and Girish Sastry and Pamela Mishkin and Brooke Chan and Scott Gray and Nick Ryder and Mikhail Pavlov and Alethea Power and Lukasz Kaiser and Mohammad Bavarian and Clemens Winter and Philippe Tillet and Felipe Petroski Such and David W. Cummings and Matthias Plappert and Fotios Chantzis and Elizabeth Barnes and Ariel Herbert-Voss and William H. Guss and Alex Nichol and Igor Babuschkin and Suchir Balaji and Shantanu Jain and Andrew Carr and Jan Leike and Joshua Achiam and Vedant Misra and Evan Morikawa and Alec Radford and Matthew M. Knight and Miles Brundage and Mira Murati and Katie Mayer and Peter Welinder and Bob McGrew and Dario Amodei and Sam McCandlish and Ilya Sutskever and Wojciech Zaremba},
  journal={ArXiv},
  year={2021},
  volume={abs/2107.03374},
  url={https://api.semanticscholar.org/CorpusID:235755472}
}

@article{ZHU2023104322,
title = {Future data center energy-conservation and emission-reduction technologies in the context of smart and low-carbon city construction},
journal = {Sustainable Cities and Society},
volume = {89},
pages = {104322},
year = {2023},
issn = {2210-6707},
doi = {https://doi.org/10.1016/j.scs.2022.104322},
url = {https://www.sciencedirect.com/science/article/pii/S2210670722006266},
author = {Hongyu Zhu and Dongdong Zhang and Hui Hwang Goh and Shuyao Wang and Tanveer Ahmad and Daijiafan Mao and Tianhao Liu and Haisen Zhao and Thomas Wu},
keywords = {Data center, Smart and low-carbon city, Carbon neutralization, Energy-conservation and emission-reduction technology, Computing power, Integration of storage and calculation},
abstract = {The energy consumption of data centers accounts for approximately 1% of that of the world, the average power usage effectiveness is in the range of 1.4–1.6, and the associated carbon emissions account for approximately 2–4% of the global carbon emissions. To reduce the energy consumption of data centers and promote smart, sustainable, and low-carbon city development, this study analyzes the energy conservation and emission-reduction technologies and potential decarbonization paths for data centers, compares the energy-saving situation of 20 typical data center cases, and highlights the impact of green data centers on the global carbon neutrality goal. The analysis reveals that data center energy consumption can be reduced by about 20–40% and 15–27% through IT equipment optimization and cooling technology improvements, respectively. Data center energy-saving strategies must consider differences in geographical location, natural resources, and economic bases. Therefore, this study examines the necessary steps for building zero-carbon data centers from the perspectives of public policy, technological innovation, and resource management. Specifically, the following aspects are explored: 1) accelerating the intelligent and unified management of data center resources; 2) building storage-computing integrated data centers that are compatible with heterogeneous resources and streamlined business models; 3) realizing large-scale and diversified use of clean energy in data centers.}
}

@article{MALMODIN2024102701,
title = {ICT sector electricity consumption and greenhouse gas emissions – 2020 outcome},
journal = {Telecommunications Policy},
volume = {48},
number = {3},
pages = {102701},
year = {2024},
issn = {0308-5961},
doi = {https://doi.org/10.1016/j.telpol.2023.102701},
url = {https://www.sciencedirect.com/science/article/pii/S0308596123002124},
author = {Jens Malmodin and Nina Lövehagen and Pernilla Bergmark and Dag Lundén},
keywords = {ICT, Information and communication technologies, Carbon footprint, Electricity consumption, Greenhouse gas emissions, Embodied emissions},
abstract = {The Information and Communication Technology (ICT) sector has gained much attention in the discussions on climate change, as it could impact global emissions both positively and negatively. The objective of the present study is to provide estimates for the 2020 use stage electricity consumption and ICT sector's total lifecycle greenhouse gas (GHG) emissions divided in three main parts: user devices including internet-of-things, networks and data centers. The study builds on a high number of data sources including measured and reported data from 150 companies that is estimated to cover about 80% of network subscriptions, about 55% of data center electricity, and about 35% of upstream GHG emissions. To understand the development, the results are put into the perspective of earlier studies. In conclusion, the ICT sector used about 4% of the global electricity in the use stage and represented about 1.4% of the global GHG emissions in 2020. The use stage electricity consumption and the total GHG emissions have increased since 2015, but the impact per subscription has decreased. The user devices accounted for over half of all GHG emissions, with equal parts relating to use stage and other lifecycle stages. For networks and data centers, the use stage GHG emissions are dominating. The electricity consumption and GHG emissions are also estimated for the closely related areas Entertainment and Media (including e.g., TVs), paper media, and cryptocurrencies.}
}

@article{JMLR:v24:23-0069,
  author  = {Alexandra Sasha Luccioni and Sylvain Viguier and Anne-Laure Ligozat},
  title   = {Estimating the Carbon Footprint of BLOOM, a 176B Parameter Language Model},
  journal = {Journal of Machine Learning Research},
  year    = {2023},
  volume  = {24},
  number  = {253},
  pages   = {1--15},
  url     = {http://jmlr.org/papers/v24/23-0069.html}
}

@InProceedings{10.1007/978-3-031-73110-5_22,
author="Iftikhar, Sunbal
and Davy, Steven",
editor="Arai, Kohei",
title="Reducing Carbon Footprint in AI: A Framework for Sustainable Training of Large Language Models",
booktitle="Proceedings of the Future Technologies Conference (FTC) 2024, Volume 1",
year="2024",
publisher="Springer Nature Switzerland",
address="Cham",
pages="325--336",
abstract="In the world of artificial intelligence (AI), large language models (LLMs) are leading the way, transforming how people understand and use language. These models have significantly impacted various domains, from natural language processing (NLP) to content generation, sparking a wave of innovation and exploration. However, this rapid progress brings to light the environmental implications of LLMs, particularly the significant energy consumption and carbon emissions during their training and operational phases. This requires a shift towards more energy-efficient practices in training and deploying LLMs, balancing AI innovation with environmental responsibility. This paper emphasizes the need for improving the energy efficiency of LLMs to align their benefits with environmental sustainability. The discussion covers the significant power consumption associated with training LLMs. We present a generic energy-efficient training framework of LLMs that employs federated learning (FL) and integrates renewable energy (RE), aiming to mitigate environmental impact of LLMs. Our objective is to encourage the implementation of sustainable AI practices that preserve the capabilities of LLMs while reducing their environmental impact, thus guiding the AI community towards the responsible advancement of technology.",
isbn="978-3-031-73110-5"
}

@article{PEREIRA2021102609,
title = {Ranking programming languages by energy efficiency},
journal = {Science of Computer Programming},
volume = {205},
pages = {102609},
year = {2021},
issn = {0167-6423},
doi = {https://doi.org/10.1016/j.scico.2021.102609},
url = {https://www.sciencedirect.com/science/article/pii/S0167642321000022},
author = {Rui Pereira and Marco Couto and Francisco Ribeiro and Rui Rua and Jácome Cunha and João Paulo Fernandes and João Saraiva},
keywords = {Energy efficiency, Programming languages, Language benchmarking, Green software},
abstract = {This paper compares a large set of programming languages regarding their efficiency, including from an energetic point-of-view. Indeed, we seek to establish and analyze different rankings for programming languages based on their energy efficiency. The goal of being able to rank programming languages based on their energy efficiency is both recent, and certainly deserves further studies. We have taken rigorous and strict solutions to 10 well defined programming problems, expressed in (up to) 27 programming languages, from the well known Computer Language Benchmark Game repository. This repository aims to compare programming languages based on a strict set of implementation rules and configurations for each benchmarking problem. We have also built a framework to automatically, and systematically, run, measure and compare the energy, time, and memory efficiency of such solutions. Ultimately, it is based on such comparisons that we propose a series of efficiency rankings, based on single and multiple criteria. Our results show interesting findings, such as how slower/faster languages can consume less/more energy, and how memory usage influences energy consumption. We also present a simple way to use our results to provide software engineers and practitioners support in deciding which language to use when energy efficiency is a concern. In addition, we further validate our results and rankings against implementations from a chrestomathy program repository, Rosetta Code., by reproducing our methodology and benchmarking system. This allows us to understand how the results and conclusions from our rigorously and well defined benchmarked programs compare to those based on more representative and real-world implementations. Indeed our results show that the rankings do not change apart from one programming language.}
}

@misc{rapl,
title={Intel® Performance Counter Monitor - A better way to measure CPU.}, 
url={https://www.intel.com/content/www/us/en/developer/articles/tool/performance-counter-monitor.html}, 
journal={Intel}, 
author={Thomas Willhalm, Roman Dementiev}} 

@misc{OpenAI, 
url={https://openai.com/index/openai-o1-mini-advancing-cost-efficient-reasoning},  
author={OpenAI}} 

@misc{GPT4o, url={https://openai.com/index/hello-gpt-4o}, journal={Hello gpt-4o}, author={OpenAI}} 

@misc{GitHubDocs, 
url={https://docs.github.com/en/copilot/about-github-copilot/what-is-github-copilot}, 
journal={GitHub Docs}, 
author={GitHub}} 

@article{10.1145/2714064.2660235,
author = {Pinto, Gustavo and Castor, Fernando and Liu, Yu David},
title = {Understanding energy behaviors of thread management constructs},
year = {2014},
issue_date = {October 2014},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {49},
number = {10},
issn = {0362-1340},
url = {https://doi.org/10.1145/2714064.2660235},
doi = {10.1145/2714064.2660235},
abstract = {Java programmers are faced with numerous choices in managing concurrent execution on multicore platforms. These choices often have different trade-offs (e.g., performance, scalability, and correctness guarantees). This paper analyzes an additional dimension, energy consumption. It presents an empirical study aiming to illuminate the relationship between the choices and settings of thread management constructs and energy consumption. We consider three important thread management constructs in concurrent programming: explicit thread creation, fixed-size thread pooling, and work stealing. We further shed light on the energy/performance trade-off of three ``tuning knobs'' of these constructs: the number of threads, the task division strategy, and the characteristics of processed data. Through an extensive experimental space exploration over real-world Java programs, we produce a list of findings about the energy behaviors of concurrent programs, which are not always obvious. The study serves as a first step toward improving energy efficiency of concurrent programs on parallel architectures.},
journal = {SIGPLAN Not.},
month = oct,
pages = {345–360},
numpages = {16},
keywords = {energy consumption, java, multi-threaded programming, performance, thread management}
}

@inproceedings{oopsla,
author = {Pinto, Gustavo and Castor, Fernando and Liu, Yu David},
title = {Understanding energy behaviors of thread management constructs},
year = {2014},
isbn = {9781450325851},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2660193.2660235},
doi = {10.1145/2660193.2660235},
abstract = {Java programmers are faced with numerous choices in managing concurrent execution on multicore platforms. These choices often have different trade-offs (e.g., performance, scalability, and correctness guarantees). This paper analyzes an additional dimension, energy consumption. It presents an empirical study aiming to illuminate the relationship between the choices and settings of thread management constructs and energy consumption. We consider three important thread management constructs in concurrent programming: explicit thread creation, fixed-size thread pooling, and work stealing. We further shed light on the energy/performance trade-off of three ``tuning knobs'' of these constructs: the number of threads, the task division strategy, and the characteristics of processed data. Through an extensive experimental space exploration over real-world Java programs, we produce a list of findings about the energy behaviors of concurrent programs, which are not always obvious. The study serves as a first step toward improving energy efficiency of concurrent programs on parallel architectures.},
booktitle = {Proceedings of the 2014 ACM International Conference on Object Oriented Programming Systems Languages \&amp; Applications},
pages = {345–360},
numpages = {16},
keywords = {energy consumption, java, multi-threaded programming, performance, thread management},
location = {Portland, Oregon, USA},
series = {OOPSLA '14}
}

@inproceedings{niu,
author = {Niu, Changan and Zhang, Ting and Li, Chuanyi and Luo, Bin and Ng, Vincent},
title = {On Evaluating the Efficiency of Source Code Generated by LLMs},
year = {2024},
isbn = {9798400706097},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3650105.3652295},
doi = {10.1145/3650105.3652295},
abstract = {Recent years have seen the remarkable capabilities of large language models (LLMs) for code generation. Different from existing work that evaluate the correctness of the code generated by LLMs, we propose to further evaluate its efficiency. More efficient code can lead to higher performance and execution efficiency of programs and software completed by LLM-assisted programming. First, we evaluate the efficiency of the code generated by LLMs on two benchmarks, HumanEval and MBPP. Then, we choose a set of programming problems from the online judge platform LeetCode to conduct a more difficult evaluation. Finally, we explore several prompts that would enable LLMs to generate more efficient code.},
booktitle = {Proceedings of the 2024 IEEE/ACM First International Conference on AI Foundation Models and Software Engineering},
pages = {103–107},
numpages = {5},
location = {Lisbon, Portugal},
series = {FORGE '24}
}