\section{Related Work}
\label{sec:relatedWork}
Previously, most studies solely focused on evaluating correctness of the code generation Vartziotis et al., "Energy Efficiency of Python Code Generated by LLMs" and the focus has shifted to also addressing the issue of efficiency in code generated by LLMs as AI-assisted tools become increasingly prevalent in software engineering and development processes Liu et al., "Efficient Code Generation with LLMs".
\\
A closely related study by Vartziotis et al., "Energy Efficiency of Python Code Generated by LLMs" examines the energy efficiency of Python code generated by three widely used tools, namely GitHub Copilot, ChatGPT 3 and Amazon CodeWhisperer. The results show that AI models can generate code optimized for sustainability when explicitly requested to do so. However, they also reported that  human-written code is consistantly more energy-efficient.  A related study by Coignion et al., "LLM-Generated Code: Performance Evaluation" evaluates LLM-generated code from a performance perspective. Their analysis compares 18 LLMs using LeetCode data, examining factors such as model temperature and success rate and their influence on code performance. The findings of this study align with those of Varziotis et al., "Energy Efficiency of Python Code Generated by LLMs", emphasizing that LLM-generated code, on average, demonstrates greater efficiency compared to human-written code. Both studies, however, share limitations, as they focus exclusively on Python data, with Varziotis et al., "Energy Efficiency of Python Code Generated by LLMs" deriving their conclusions from a limited dataset comprising only six coding problems.
\par In contrast, this study investigates three programming languages: Python, Java, and C++. Furthermore,it utilizes a comprehensive benchmark of 53 coding tasks, thereby increasing the robustness and reliability of the findings. We argue that Python is generally not regarded as an inherently efficient programming language Du et al., "Class-Level Code Generation with LLMs", making it less likely to be chosen by developers when performance is a critical requirement. Consequently, studies evaluating the performance and energy efficiency of LLM-generated code for Python may have limited practical relevance, as such analyses may not align with real-world scenarios where more performance-oriented languages are typically preferred.
\par Several studies Döderlein et al., "LLM-Generated Code on LeetCode"; Nguyen and Nadi, "GitHub Copilot's Code Recommendations"; Vasconcelos et al., "Uncertainty in AI-Driven Code Completions" have introduced benchmarks specifically designed to evaluate LLM-generated code, focusing on runtime performance and memory consumption. In contrast, our study expands this scope by including the energy consumption as an additional metric, providing a more comprehensive overview of the energy efficiency of LLM-generated code. Furthermore, while the benchmarks developed by two of the prior studies Liu et al., "Efficient Code Generation with LLMs"  were extensive, each encompassing over 1000 coding problems, their analyses were limited to Python, whereas our study also includes Java and C++. 
\par Rather than developing a benchmark from scratch, the study by Liu et al., "Efficient Code Generation with LLMs" grouped efficiency-demanding Python programming tasks from \textit{HumanEval+} and \textit{MBPP+} to form \textit{EvalPerf} addressing the limitation of previous works, which primarily focused on light computational requirements and possibly misrepresenting the capabilities of LLMs. To improve the quality of their evaluation, they augmented their experiments with computationally intensive inputs, aiming to provide a more accurate assessment of the efficiency and performance of the generated code. Although the referenced study focused exclusively on performance-intensive tasks from \textit{HumanEval+} and \textit{MBPP+}, several other works have shown that LLMs perform notably well on these datasets. In contrast, our research does not depend on these benchmarks, as they are generally labeled as "Easy" and are less challenging for evaluating model capabilities.  In addition, we aim to compare the efficiency of the generated code with human-written solutions, a consideration that was overlooked in the cited study.
\par The study by Du et al., "Class-Level Code Generation with LLMs" sought to examine a more complex code generation scenario. To this end, they developed their own benchmark, \textit{ClassEval}, which consists of 100 class-level Python code generation tasks. Based on the new benchmark, this is the first study that evaluated LLMs in the context of class-level code generation. Their experiments included 11 state-of-the-art models, each varying in size, architecture, data sources, and application domains. The objective of the cited study differs from the primary goal of our research. While the cited work introduced a novel benchmark to assess the correctness of the code generated by LLMs, our study focuses primarily on evaluating the energy efficiency of the generated code. Nonetheless, we also account for the correctness of the code, as evaluating the efficiency is only meaningful when the code is correct.
\par LeetCode, although primarily a platform for coding competitions,  is also extensively used as a dataset for evaluating the programming capabilities of LLMs. Döderlein et al., "LLM-Generated Code on LeetCode" evaluated the performance of Copilot and Codex on LeetCode, analyzing the impact of varying prompt structures on the models' effectiveness. Nguyen and Nadi, "GitHub Copilot's Code Recommendations" investigated GitHub Copilot's code recommendations for LeetCode problems, focusing on the complexity and intricacies of the generated solutions. Vasconcelos et al., "Uncertainty in AI-Driven Code Completions" examined the impact of emphasizing uncertainty in AI-driven code completions, utilizing LeetCode problems and the Codex model as part of their analysis.