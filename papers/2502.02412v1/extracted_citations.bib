@inproceedings{10.1145/3597503.3623316, author = {Yu, Hao and Shen, Bo and Ran, Dezhi and Zhang, Jiaxin and Zhang, Qi and Ma, Yuchi and Liang, Guangtai and Li, Ying and Wang, Qianxiang and Xie, Tao}, title = {CoderEval: A Benchmark of Pragmatic Code Generation with Generative Pre-trained Models}, year = {2024}, isbn = {9798400702174}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3597503.3623316}, doi = {10.1145/3597503.3623316}, abstract = {Code generation models based on the pre-training and fine-tuning paradigm have been increasingly attempted by both academia and industry, resulting in well-known industrial models such as Codex, CodeGen, and PanGu-Coder. To evaluate the effectiveness of these models, multiple existing benchmarks (e.g., HumanEval and AiXBench) are proposed, including only cases of generating a standalone function, i.e., a function that may invoke or access only built-in functions and standard libraries. However, non-standalone functions, which typically are not included in the existing benchmarks, constitute more than 70\% of the functions in popular open-source projects, and evaluating models' effectiveness on standalone functions cannot reflect these models' effectiveness on pragmatic code generation scenarios (i.e., code generation for real settings of open source or proprietary code).To help bridge the preceding gap, in this paper, we propose a benchmark named CoderEval, consisting of 230 Python and 230 Java code generation tasks carefully curated from popular real-world open-source projects and a self-contained execution platform to automatically assess the functional correctness of generated code. CoderEval supports code generation tasks from six levels of context dependency, where context refers to code elements such as types, APIs, variables, and consts defined outside the function under generation but within the dependent third-party libraries, current class, file, or project. CoderEval can be used to evaluate the effectiveness of models in generating code beyond only standalone functions. By evaluating three state-of-the-art code generation models (CodeGen, PanGu-Coder, and ChatGPT) on CoderEval and HumanEval, we find that the effectiveness of these models in generating standalone functions is substantially higher than that in generating non-standalone functions. Our analysis highlights the current progress and pinpoints future directions to further improve a model's effectiveness by leveraging contextual information for pragmatic code generation.}, booktitle = {Proceedings of the IEEE/ACM 46th International Conference on Software Engineering}, articleno = {37}, numpages = {12}, keywords = {code generation, large language models, benchmark}, location = {Lisbon, Portugal}, series = {ICSE '24} }

@inproceedings{10.1145/3627217.3627233,
author = {Balse, Rishabh and Kumar, Viraj and Prasad, Prajish and Warriem, Jayakrishnan Madathil},
title = {Evaluating the Quality of LLM-Generated Explanations for Logical Errors in CS1 Student Programs},
year = {2023},
isbn = {9798400708404},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3627217.3627233},
doi = {10.1145/3627217.3627233},
abstract = {When students in CS1 (Introductory Programming) write erroneous code, course staff can use automated tools to provide various types of helpful feedback. In this paper, we focus on syntactically correct student code containing logical errors. Tools that explain logical errors typically require course staff to invest greater effort than tools that detect such errors. To reduce this effort, prior work has investigated the use of Large Language Models (LLMs) such as GPT-3 to generate explanations. Unfortunately, these explanations can be incomplete or incorrect, and therefore unhelpful if presented to students directly. Nevertheless, LLM-generated explanations may be of adequate quality for Teaching Assistants (TAs) to efficiently craft helpful explanations on their basis. We evaluate the quality of explanations generated by an LLM (GPT-3.5-turbo) in two ways, for 30&nbsp;buggy student solutions across 6&nbsp;code-writing problems. First, in a study with 5&nbsp;undergraduate TAs, we compare TA perception of LLM-generated and peer-generated explanation quality. TAs were unaware which explanations were LLM-generated, but they found them to be comparable in quality to peer-generated explanations. Second, we performed a detailed manual analysis of LLM-generated explanations for all 30&nbsp;buggy solutions. We found at least one incorrect statement in 15/30 explanations (50\%). However, in 28/30 cases (93\%), the LLM-generated explanation correctly identified at least one logical error. Our results suggest that for large CS1 courses, TAs with adequate training to detect erroneous statements may be able to extract value from such explanations.},
booktitle = {Proceedings of the 16th Annual ACM India Compute Conference},
pages = {49–54},
numpages = {6},
keywords = {Explanation, GPT-3.5-Turbo, Large language models (LLMs), Logical Errors, Python Programming},
location = {Hyderabad, India},
series = {COMPUTE '23}
}

@inproceedings{10.1145/3661167.3661221,
author = {Coignion, Tristan and Quinton, Cl\'{e}ment and Rouvoy, Romain},
title = {A Performance Study of LLM-Generated Code on Leetcode},
year = {2024},
isbn = {9798400717017},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3661167.3661221},
doi = {10.1145/3661167.3661221},
abstract = {This study evaluates the efficiency of code generation by Large Language Models (LLMs) and measures their performance against human-crafted solutions using a dataset from Leetcode. We compare 18 LLMs, considering factors such as model temperature and success rate, and their impact on code performance. This research introduces a novel method for measuring and comparing the speed of LLM-generated code, revealing that LLMs produce code with comparable performance, irrespective of the adopted LLM. We also find that LLMs are capable of generating code that is, on average, more efficient than the code written by humans. The paper further discusses the use of Leetcode as a benchmarking dataset, the limitations imposed by potential data contamination, and the platform’s measurement reliability. We believe that our findings contribute to a better understanding of LLM capabilities in code generation and set the stage for future optimizations in the field.},
booktitle = {Proceedings of the 28th International Conference on Evaluation and Assessment in Software Engineering},
pages = {79–89},
numpages = {11},
location = {Salerno, Italy},
series = {EASE '24}
}

@article{10.1145/3702320,
author = {Vasconcelos, Helena and Bansal, Gagan and Fourney, Adam and Liao, Q. Vera and Vaughan, Jennifer Wortman},
title = {Generation Probabilities Are Not Enough: Uncertainty Highlighting in AI Code Completions},
year = {2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
issn = {1073-0516},
url = {https://doi.org/10.1145/3702320},
doi = {10.1145/3702320},
abstract = {Large-scale generative models have enabled the development of AI-powered code completion tools to assist programmers in writing code. Like all AI-powered tools, these code completion tools are not always accurate and can introduce bugs or even security vulnerabilities into code if not properly detected and corrected by a human programmer. One technique that has been proposed and implemented to help programmers locate potential errors is to highlight uncertain tokens. However, little is known about the effectiveness of this technique. Through a mixed-methods study with 30 programmers, we compare three conditions: providing the AI system's code completion alone, highlighting tokens with the lowest likelihood of being generated by the underlying generative model, and highlighting tokens with the highest predicted likelihood of being edited by a programmer. We find that highlighting tokens with the highest predicted likelihood of being edited leads to faster task completion and more targeted edits, and is subjectively preferred by study participants. In contrast, highlighting tokens according to their probability of being generated does not provide any benefit over the baseline with no highlighting. We further explore the design space of how to convey uncertainty in AI-powered code completion tools and find that programmers prefer highlights that are granular, informative, interpretable, and not overwhelming. This work contributes to building an understanding of what uncertainty means for generative models and how to convey it effectively.},
note = {Just Accepted},
journal = {ACM Trans. Comput.-Hum. Interact.},
month = oct,
keywords = {human-AI programming, generative AI, uncertainty}
}

@INPROCEEDINGS{10403378,
  author={Wang, Jianxun and Chen, Yixiang},
  booktitle={2023 IEEE International Conference on Medical Artificial Intelligence (MedAI)}, 
  title={A Review on Code Generation with LLMs: Application and Evaluation}, 
  year={2023},
  volume={},
  number={},
  pages={284-289},
  keywords={Productivity;Computer science;Codes;Writing;Encoding;Task analysis;Software engineering;large language models (LLMs);code generation;code completion;automatic program repair;code quality evaluation},
  doi={10.1109/MedAI59581.2023.00044}}

@INPROCEEDINGS{9796235,
  author={Nguyen, Nhan and Nadi, Sarah},
  booktitle={2022 IEEE/ACM 19th International Conference on Mining Software Repositories (MSR)}, 
  title={An Empirical Evaluation of GitHub Copilot's Code Suggestions}, 
  year={2022},
  volume={},
  number={},
  pages={1-5},
  keywords={Measurement;Java;Codes;Static analysis;Software;Natural language processing;Complexity theory;Program Synthesis;Codex;GitHub Copilot;Empirical Evaluation},
  doi={10.1145/3524842.3528470}}

@article{Chen2021EvaluatingLL,
  title={Evaluating Large Language Models Trained on Code},
  author={Mark Chen and Jerry Tworek and Heewoo Jun and Qiming Yuan and Henrique Pond{\'e} and Jared Kaplan and Harrison Edwards and Yura Burda and Nicholas Joseph and Greg Brockman and Alex Ray and Raul Puri and Gretchen Krueger and Michael Petrov and Heidy Khlaaf and Girish Sastry and Pamela Mishkin and Brooke Chan and Scott Gray and Nick Ryder and Mikhail Pavlov and Alethea Power and Lukasz Kaiser and Mohammad Bavarian and Clemens Winter and Philippe Tillet and Felipe Petroski Such and David W. Cummings and Matthias Plappert and Fotios Chantzis and Elizabeth Barnes and Ariel Herbert-Voss and William H. Guss and Alex Nichol and Igor Babuschkin and Suchir Balaji and Shantanu Jain and Andrew Carr and Jan Leike and Joshua Achiam and Vedant Misra and Evan Morikawa and Alec Radford and Matthew M. Knight and Miles Brundage and Mira Murati and Katie Mayer and Peter Welinder and Bob McGrew and Dario Amodei and Sam McCandlish and Ilya Sutskever and Wojciech Zaremba},
  journal={ArXiv},
  year={2021},
  volume={abs/2107.03374},
  url={https://api.semanticscholar.org/CorpusID:235755472}
}

@article{Dderlein2022PilotingCA,
  title={Piloting Copilot and Codex: Hot Temperature, Cold Prompts, or Black Magic?},
  author={Jean-Baptiste D{\"o}derlein and Mathieu Acher and Djamel Eddine Khelladi and Beno{\^i}t Combemale},
  journal={ArXiv},
  year={2022},
  volume={abs/2210.14699},
  url={https://api.semanticscholar.org/CorpusID:253117147}
}

@article{Hendrycks2021MeasuringCC,
  title={Measuring Coding Challenge Competence With APPS},
  author={Dan Hendrycks and Steven Basart and Saurav Kadavath and Mantas Mazeika and Akul Arora and Ethan Guo and Collin Burns and Samir Puranik and Horace He and Dawn Xiaodong Song and Jacob Steinhardt},
  journal={ArXiv},
  year={2021},
  volume={abs/2105.09938},
  url={https://api.semanticscholar.org/CorpusID:234790100}
}

@article{Huang2024EffiCodeUC,
  title={Effi-Code: Unleashing Code Efficiency in Language Models},
  author={Dong Huang and Guangtao Zeng and Jianbo Dai and Meng Luo and Han Weng and Yuhao Qing and Heming Cui and Zhijiang Guo and Jie M. Zhang},
  journal={ArXiv},
  year={2024},
  volume={abs/2410.10209},
  url={https://api.semanticscholar.org/CorpusID:273345361}
}

@article{Nitin_Sherje_2024, title={Enhancing Software Development Efficiency through AI-Powered Code Generation}, volume={5}, url={https://technicaljournals.org/RJCSE/index.php/journal/article/view/90}, abstractNote={&amp;lt;p&amp;gt;Software development is a critical process in today&amp;#039;s digital age, demanding high levels of efficiency and accuracy. However, traditional methods of coding often prove time-consuming and error-prone. To address these challenges, recent advancements in artificial intelligence (AI) have introduced a novel approach – AI-powered code generation. This paper delves into the potential of AI-powered code generation techniques to significantly enhance software development efficiency. Beginning with an exploration of the current landscape of AI in software development, we scrutinize various AI-powered code generation methodologies, including rule-based systems, machine learning algorithms, neural networks, generative adversarial networks (GANs), and transformer models. We assess the benefits of AI-powered code generation, such as accelerated development speed, heightened code quality, reduced human error, and increased developer productivity. Moreover, we scrutinize the challenges and limitations associated with these techniques, encompassing data quality, interpretability, domain-specific knowledge, and ethical considerations. Through case studies and real-world examples, we illustrate the practical applications and implications of AI-generated code.&amp;lt;/p&amp;gt;}, number={1}, journal={Research Journal of Computer Systems and Engineering}, author={Nitin Sherje}, year={2024}, month={Jul.}, pages={01–12} }

@article{PEREIRA2021102609,
title = {Ranking programming languages by energy efficiency},
journal = {Science of Computer Programming},
volume = {205},
pages = {102609},
year = {2021},
issn = {0167-6423},
doi = {https://doi.org/10.1016/j.scico.2021.102609},
url = {https://www.sciencedirect.com/science/article/pii/S0167642321000022},
author = {Rui Pereira and Marco Couto and Francisco Ribeiro and Rui Rua and Jácome Cunha and João Paulo Fernandes and João Saraiva},
keywords = {Energy efficiency, Programming languages, Language benchmarking, Green software},
abstract = {This paper compares a large set of programming languages regarding their efficiency, including from an energetic point-of-view. Indeed, we seek to establish and analyze different rankings for programming languages based on their energy efficiency. The goal of being able to rank programming languages based on their energy efficiency is both recent, and certainly deserves further studies. We have taken rigorous and strict solutions to 10 well defined programming problems, expressed in (up to) 27 programming languages, from the well known Computer Language Benchmark Game repository. This repository aims to compare programming languages based on a strict set of implementation rules and configurations for each benchmarking problem. We have also built a framework to automatically, and systematically, run, measure and compare the energy, time, and memory efficiency of such solutions. Ultimately, it is based on such comparisons that we propose a series of efficiency rankings, based on single and multiple criteria. Our results show interesting findings, such as how slower/faster languages can consume less/more energy, and how memory usage influences energy consumption. We also present a simple way to use our results to provide software engineers and practitioners support in deciding which language to use when energy efficiency is a concern. In addition, we further validate our results and rankings against implementations from a chrestomathy program repository, Rosetta Code., by reproducing our methodology and benchmarking system. This allows us to understand how the results and conclusions from our rigorously and well defined benchmarked programs compare to those based on more representative and real-world implementations. Indeed our results show that the rankings do not change apart from one programming language.}
}

@inproceedings{copilotAcc, author = {Yetistiren, Burak and Ozsoy, Isik and Tuzun, Eray}, title = {Assessing the quality of GitHub copilot’s code generation}, year = {2022}, isbn = {9781450398602}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3558489.3559072}, doi = {10.1145/3558489.3559072}, abstract = {The introduction of GitHub’s new code generation tool, GitHub Copilot, seems to be the first well-established instance of an AI pair-programmer. GitHub Copilot has access to a large number of open-source projects, enabling it to utilize more extensive code in various programming languages than other code generation tools. Although the initial and informal assessments are promising, a systematic evaluation is needed to explore the limits and benefits of GitHub Copilot. The main objective of this study is to assess the quality of generated code provided by GitHub Copilot. We also aim to evaluate the impact of the quality and variety of input parameters fed to GitHub Copilot. To achieve this aim, we created an experimental setup for evaluating the generated code in terms of validity, correctness, and efficiency. Our results suggest that GitHub Copilot was able to generate valid code with a 91.5\% success rate. In terms of code correctness, out of 164 problems, 47 (28.7\%) were correctly, while 84 (51.2\%) were partially correctly, and 33 (20.1\%) were incorrectly generated. Our empirical analysis shows that GitHub Copilot is a promising tool based on the results we obtained, however further and more comprehensive assessment is needed in the future.}, booktitle = {Proceedings of the 18th International Conference on Predictive Models and Data Analytics in Software Engineering}, pages = {62–71}, numpages = {10}, keywords = {empirical study, code generation, code completion, GitHub Copilot, AI pair programmer}, location = {Singapore, Singapore}, series = {PROMISE 2022} }

@article{du2024mercury,
  title     = {Mercury: A Code Efficiency Benchmark for Code Large Language Models},
  author    = {Mingzhe Du and Anh Tuan Luu and Bin Ji and Qian Liu and See-Kiong Ng},
  journal   = {arXiv preprint},
  volume    = {arXiv:2402.07844v4},
  year      = {2024},
  url       = {https://arxiv.org/abs/2402.07844v4},
}

@inproceedings{duCodeGen, author = {Du, Xueying and Liu, Mingwei and Wang, Kaixin and Wang, Hanlin and Liu, Junwei and Chen, Yixuan and Feng, Jiayi and Sha, Chaofeng and Peng, Xin and Lou, Yiling}, title = {Evaluating Large Language Models in Class-Level Code Generation}, year = {2024}, isbn = {9798400702174}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3597503.3639219}, doi = {10.1145/3597503.3639219}, abstract = {Recently, many large language models (LLMs) have been proposed, showing advanced proficiency in code generation. Meanwhile, many efforts have been dedicated to evaluating LLMs on code generation benchmarks such as HumanEval. Although being very helpful for comparing different LLMs, existing evaluation focuses on a simple code generation scenario (i.e., function-level or statement-level code generation), which mainly asks LLMs to generate one single code unit (e.g., a function or a statement) for the given natural language description. Such evaluation focuses on generating independent and often small-scale code units, thus leaving it unclear how LLMs perform in real-world software development scenarios.To fill this knowledge gap, we make the first attempt to evaluate LLMs in a more challenging code generation scenario, i.e., class-level code generation. Compared with existing code generation benchmarks, it better reflects real-world software development scenarios due to it comprising broader contextual dependencies and multiple, interdependent units of code. We first manually construct the first class-level code generation benchmark ClassEval of 100 class-level Python code generation tasks with approximately 500 person-hours. Based on the new benchmark ClassEval, we then perform the first study of 11 state-of-the-art LLMs on class-level code generation. Based on our results, we find that all LLMs perform much worse on class-level code generation compared to the method-level. While GPT models still dominate other LLMs on class-level code generation, the performance rankings of other models on method-level code generation no longer holds for class-level code generation. Besides, most models (except GPT models) perform better when generating the class method by method; and they have the limited ability of generating dependent code. Based on our findings, we call for software engineering (SE) researchers' expertise to build more LLM benchmarks based on practical and complicated software development scenarios.}, booktitle = {Proceedings of the IEEE/ACM 46th International Conference on Software Engineering}, articleno = {81}, numpages = {13}, keywords = {class-level code generation, large language model, benchmark}, location = {Lisbon, Portugal}, series = {ICSE '24} }

@inproceedings{han-etal-2023-sample,
    title = "On Sample-Efficient Code Generation",
    author = "Han, Hojae  and
      Kim, Yu Jin  and
      Kim, Byoungjip  and
      Lee, Youngwon  and
      Lee, Kyungjae  and
      Lee, Kyungmin  and
      Lee, Moontae  and
      Bae, Kyunghoon  and
      Hwang, Seung-won",
    editor = "Wang, Mingxuan  and
      Zitouni, Imed",
    booktitle = "Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing: Industry Track",
    month = dec,
    year = "2023",
    address = "Singapore",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2023.emnlp-industry.73",
    doi = "10.18653/v1/2023.emnlp-industry.73",
    pages = "783--791",
    abstract = "Large language models often struggle to predict runtime behavior in code generation tasks, leading to a reliance on rejection sampling (best-of-n) to generate multiple code snippets then select the best. Our distinction is reducing sampling costs, without compromising generation quality. We introduce EFFICODE, a novel framework that prioritizes sampling on test problems that models can solve. We show how EFFICODE estimates solvability to optimize computational costs during multiple sampling. Based on empirical evidence, EFFICODE consistently demonstrates reduced sampling budgets while maintaining comparable code generation performance, especially when problems are challenging. In addition, utilizing EFFICODE to rank sampled code snippets also shows its effectiveness in answer code selection for reducing temporal costs, by not requiring any execution or test case generation.",
}

@article{huang2024effibench,
  title     = {EFFIBENCH: Benchmarking the Efficiency of Automatically Generated Code},
  author    = {Dong Huang and Yuhao Qing and Weiyi Shang and Heming Cui and Jie M. Zhang},
  journal   = {arXiv preprint},
  volume    = {arXiv:2402.02037v4},
  year      = {2024},
  url       = {https://arxiv.org/abs/2402.02037v4},
}

@misc{liu2024evaluatinglanguagemodelsefficient,
      title={Evaluating Language Models for Efficient Code Generation}, 
      author={Jiawei Liu and Songrun Xie and Junhao Wang and Yuxiang Wei and Yifeng Ding and Lingming Zhang},
      year={2024},
      eprint={2408.06450},
      archivePrefix={arXiv},
      primaryClass={cs.SE},
      url={https://arxiv.org/abs/2408.06450}, 
}

@article{vartziotis2024learn,
  author    = {Tina Vartziotis and Ippolyti Dellatolas and George Dasoulas and Maximilian Schmidt and Florian Schneider and Tim Hoffmann and Sotirios Kotsopoulos and Michael Keckeisen},
  title     = {Learn to Code Sustainably: An Empirical Study on LLM-based Green Code Generation},
  journal   = {arXiv preprint},
  volume    = {arXiv:2403.03344v1},
  year      = {2024},
  url       = {https://arxiv.org/abs/2403.03344v1}
}

@inproceedings{wang-etal-2023-recode,
    title = "{R}e{C}ode: Robustness Evaluation of Code Generation Models",
    author = "Wang, Shiqi  and
      Li, Zheng  and
      Qian, Haifeng  and
      Yang, Chenghao  and
      Wang, Zijian  and
      Shang, Mingyue  and
      Kumar, Varun  and
      Tan, Samson  and
      Ray, Baishakhi  and
      Bhatia, Parminder  and
      Nallapati, Ramesh  and
      Ramanathan, Murali Krishna  and
      Roth, Dan  and
      Xiang, Bing",
    editor = "Rogers, Anna  and
      Boyd-Graber, Jordan  and
      Okazaki, Naoaki",
    booktitle = "Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = jul,
    year = "2023",
    address = "Toronto, Canada",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2023.acl-long.773",
    doi = "10.18653/v1/2023.acl-long.773",
    pages = "13818--13843",
    abstract = "Code generation models have achieved impressive performance. However, they tend to be brittle as slight edits to a prompt could lead to very different generations; these robustness properties, critical for user experience when deployed in real-life applications, are not well understood. Most existing works on robustness in text or code tasks have focused on classification, while robustness in generation tasks is an uncharted area and to date there is no comprehensive benchmark for robustness in code generation. In this paper, we propose ReCode, a comprehensive robustness evaluation benchmark for code generation models. We customize over 30 transformations specifically for code on docstrings, function and variable names, code syntax, and code format. They are carefully designed to be natural in real-life coding practice, preserve the original semantic meaning, and thus provide multifaceted assessments of a model{'}s robustness performance. With human annotators, we verified that over 90{\%} of the perturbed prompts do not alter the semantic meaning of the original prompt. In addition, we define robustness metrics for code generation models considering the worst-case behavior under each type of perturbation, taking advantage of the fact that executing the generated code can serve as objective evaluation. We demonstrate ReCode on SOTA models using HumanEval, MBPP, as well as function completion tasks derived from them. Interesting observations include: better robustness for CodeGen over InCoder and GPT-J; models are most sensitive to syntax perturbations; more challenging robustness evaluation on MBPP over HumanEval.",
}

