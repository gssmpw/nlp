\begin{table*}[hbtp]
    \centering
    \resizebox{\textwidth}{!}{
        \begin{tabular}{l c c c c c c c c}
            \toprule
            & \multicolumn{5}{c}{\textbf{VL-RewardBench}} & \multicolumn{1}{c}{\textbf{TextVQA}} & \multicolumn{2}{c}{\textbf{MMMU-Pro}} \\
            \cmidrule(lr){2-6} \cmidrule(lr){7-7} \cmidrule(lr){8-9}
            \textbf{Method} & General & Hallucination & Reasoning & Overall & Macro Avg. & Overall & Standard & Vision \\
            \midrule
            \texttt{Llama-3.2-Vision}          & 33.3* & 38.4* & 56.6* & 42.9* & 42.8* & 46.4 & 28.8 & 19.8 \\
            \texttt{Tulu-3-RM}                 & 45.4 & 36.6 & 56.6 & 43.0 & 46.2 & 27.4 & 29.4 & 20.4 \\
            \midrule
            \texttt{Random}                    & 50.0 & 50.0 & 50.0 & 50.0 & 50.0 & \textbf{48.2} & 29.2 & 18.4 \\
            \texttt{Cascade}                   & 54.1 & 40.5 & 57.2 & 46.7 & 50.6 & 38.3 & 31.3 & \textbf{23.7} \\
            \midrule
            \texttt{Linear}                    & 47.5 & 51.0 & 55.0 & 51.5 & 51.2 & 45.8 & 29.1 & 19.0 \\
            \texttt{Task Vec.}                 & 63.4 & 66.4 & 57.5 & 63.7 & 62.4 & 36.0 & \textbf{31.6} & 20.9 \\
            \texttt{TIES}                      & 59.0 & \textbf{74.1} & 50.9 & \textbf{66.0} & 61.4 & 28.3 & 30.7 & 20.6 \\
            \texttt{DARE} + \texttt{Task Vec.} & 63.4 & 68.9 & \textbf{58.5} & 65.4 & \textbf{63.6} & 36.1 & 30.2 & 20.9 \\
            \texttt{DARE} + \texttt{TIES}      & \textbf{63.9} & 65.6 & 57.2 & 63.2 & 62.2 & 56.9 & 31.4 & 21.8 \\
            \bottomrule
        \end{tabular}
    }
    \caption{Comparison of merging methods across the VL-RewardBench, TextVQA, and MMMU-Pro datasets using \texttt{TULU-3-RM} for merging. *Indicates results from~\citet{li2024vlrewardbench}.}
    \label{tab:main_tulu3}
\end{table*}