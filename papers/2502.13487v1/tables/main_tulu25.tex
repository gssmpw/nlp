\begin{table*}[hbtp]
    \centering
    \resizebox{0.85\textwidth}{!}{
        \begin{tabular}{l c c c c c c c c}
            \toprule
            & \multicolumn{5}{c}{\textbf{VL-RewardBench}} & \multicolumn{1}{c}{\textbf{TextVQA}} & \multicolumn{2}{c}{\textbf{MMMU-Pro}} \\
            \cmidrule(lr){2-6} \cmidrule(lr){7-7} \cmidrule(lr){8-9}
            \textbf{Method} & General & Hallucination & Reasoning & Overall & Macro Avg. & Overall & Standard & Vision \\
            \midrule
            \texttt{Llama-3.2-Vision}          & 33.3* & 38.4* & 56.6* & 42.9* & 42.8* & 46.4 & 28.8 & 19.8 \\
            \texttt{Tulu-2.5-RM}               & 43.2 & 31.4 & 54.1 & 38.9 & 42.9 & 42.6 & 29.8 & 21.4 \\
            \midrule
            \texttt{Random}                    & \textbf{50.0} & 50.0 & 50.0 & 50.0 & 50.0 & 48.2 & 29.2 & 18.4 \\
            \texttt{Cascade}                   & 44.8 & 37.8 & 57.2 & 43.8 & 46.6 & 43.2 & 30.9 & \textbf{23.4} \\
            \midrule
            \texttt{Linear}                    & 39.3 & 52.3 & 54.4 & 51.0 & 48.7 & 54.7 & 27.8 & 22.1 \\
            \texttt{Task Vec.}                 & 48.6 & 59.4 & 59.7 & 57.9 & 55.9 & 59.0 & 31.0 & 22.7 \\
            \texttt{TIES}                      & 43.7 & 58.2 & 58.5 & 56.2 & 53.5 & \textbf{64.2} & 29.1 & 22.6 \\
            \texttt{DARE} + \texttt{Task Vec.} & 49.2 & \textbf{61.7} & \textbf{61.0} & \textbf{59.7} & \textbf{57.3} & 58.8 & 30.3 & 22.4 \\
            \texttt{DARE} + \texttt{TIES}      & 49.2 & 59.1 & 58.2 & 57.4 & 55.5 & 57.3 & \textbf{31.6} & 22.0 \\
            \bottomrule
        \end{tabular}
    }
    \vspace{-5pt}
    \caption{Comparison of merging methods across the VL-RewardBench, TextVQA, and MMMU-Pro datasets using \texttt{TULU-2.5-RM} for merging. *Indicates results from~\citet{li2024vlrewardbench}.}
    \vspace{-15pt}
    \label{tab:main_tulu25}
\end{table*}