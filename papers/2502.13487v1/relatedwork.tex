\section{Related Work}
\label{sec:related_work}
% Preference dataset -> NLP a lot, but no Vision language preference data
\paragraph{Preference Dataset} A common approach to train a reward model is to use the Bradleyâ€“Terry model~\citep{bradley1952rank}, which relies on paired data for learning. In NLP, many high-quality preference datasets are already available~\citep{learningtosummarize,bai2022traininghelpfulharmlessassistant,pmlr-v162-ethayarajh22a,oasst,cui2024ultrafeedback,zhu2024starlingb,wang2024helpsteer}. Similarly, in the vision-language domain, several preference datasets have been introduced~\citep{Yu_2024_CVPR,yu2024rlaifv,chen2024sharegpt4v,wijaya2024multimodalpreferencedatasynthetic,li2024vlfeedback,zhou2024aligning,xiao2024detecting}. In this work, we explore the potential of transferring textual preferences to LVLMs in a training-free manner, specifically through model merging.

% VLM reward (VL RewardBench), VLM as a judge
\vspace{-3pt}
\paragraph{LVLM-as-a-Judge \& Evaluation} LVLM-as-a-Judge refers to utilizing strong large vision-language models for evaluation and judgment. These LVLMs can be either closed-source~\citep{openai2023gpt4v,hurst2024gpt,team2024gemini,anthropic2024claude35} or open-source~\citep{lee-etal-2024-prometheus,dubey2024llama,deitke2024molmo,Qwen2.5-VL}. To assess LVLMs as generative reward models, \citet{chen2024mllmasajudge} established benchmarks and found that LVLMs exhibit high agreement with humans in pairwise comparison judgments, but perform poorly in scoring evaluation and batch ranking tasks. Recently, VL-RewardBench~\citep{li2024vlrewardbench} introduced challenging cases and complex multimodal reasoning tasks, revealing that most off-the-shelf LVLMs struggle with such evaluations.

% Merging
\vspace{-3pt}
\paragraph{Model Merging} Model merging is a common, training-free method for combining skills from multiple models within the parameter space. A basic approach involves simple weighted averaging~\citep{wortsman2022model}, while more advanced techniques have been developed~\citep{yadav2024ties,yu2024language,yang2024modelmergingllmsmllms}. These techniques have already proven effective in reward modeling~\citep{rame2024warm,lin-etal-2024-dogerm} and LLM-as-a-judge~\citep{kim-etal-2024-prometheus} in NLP. Recently, REMEDY~\citep{zhu2025remedy} introduced strategies for merging LVLMs. In contrast, our work focuses on merging textual reward models into the language modeling components of LVLMs.