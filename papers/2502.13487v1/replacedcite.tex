\section{Related Work}
\label{sec:related_work}
% Preference dataset -> NLP a lot, but no Vision language preference data
\paragraph{Preference Dataset} A common approach to train a reward model is to use the Bradleyâ€“Terry model____, which relies on paired data for learning. In NLP, many high-quality preference datasets are already available____. Similarly, in the vision-language domain, several preference datasets have been introduced____. In this work, we explore the potential of transferring textual preferences to LVLMs in a training-free manner, specifically through model merging.

% VLM reward (VL RewardBench), VLM as a judge
\vspace{-3pt}
\paragraph{LVLM-as-a-Judge \& Evaluation} LVLM-as-a-Judge refers to utilizing strong large vision-language models for evaluation and judgment. These LVLMs can be either closed-source____ or open-source____. To assess LVLMs as generative reward models, ____ established benchmarks and found that LVLMs exhibit high agreement with humans in pairwise comparison judgments, but perform poorly in scoring evaluation and batch ranking tasks. Recently, VL-RewardBench____ introduced challenging cases and complex multimodal reasoning tasks, revealing that most off-the-shelf LVLMs struggle with such evaluations.

% Merging
\vspace{-3pt}
\paragraph{Model Merging} Model merging is a common, training-free method for combining skills from multiple models within the parameter space. A basic approach involves simple weighted averaging____, while more advanced techniques have been developed____. These techniques have already proven effective in reward modeling____ and LLM-as-a-judge____ in NLP. Recently, REMEDY____ introduced strategies for merging LVLMs. In contrast, our work focuses on merging textual reward models into the language modeling components of LVLMs.