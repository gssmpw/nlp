% This must be in the first 5 lines to tell arXiv to use pdfLaTeX, which is strongly recommended.
%\pdfoutput=1
% In particular, the hyperref package requires pdfLaTeX in order to break URLs across lines.

\documentclass[11pt]{article}

% Change "review" to "final" to generate the final (sometimes called camera-ready) version.
% Change to "preprint" to generate a non-anonymous version with page numbers.
\usepackage[final]{acl}

% Standard package includes
\usepackage{times}
\usepackage{latexsym}

% For proper rendering and hyphenation of words containing Latin characters (including in bib files)
\usepackage[T1]{fontenc}
% For Vietnamese characters
% \usepackage[T5]{fontenc}
% See https://www.latex-project.org/help/documentation/encguide.pdf for other character sets

% This assumes your files are encoded as UTF8
\usepackage[utf8]{inputenc}

% This is not strictly necessary, and may be commented out,
% but it will improve the layout of the manuscript,
% and will typically save some space.
\usepackage{microtype}

% This is also not strictly necessary, and may be commented out.
% However, it will improve the aesthetics of text in
% the typewriter font.
\usepackage{inconsolata}
\usepackage{caption}
\usepackage{subcaption}
%Including images in your LaTeX document requires adding
%additional package(s)
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{booktabs}

\usepackage{longtable}
%\usepackage[htt]{hyphenat}
% If the title and author information does not fit in the area allocated, uncomment the following
%
%\setlength\titlebox{<dim>}
%
% and set <dim> to something 5cm or larger.

\title{Transferring Textual Preferences to Vision-Language Understanding through Model Merging}

% Author information can be set in various styles:
% For several authors from the same institution:
% \author{Author 1 \and ... \and Author n \\
%         Address line \\ ... \\ Address line}
% if the names do not fit well on one line use
%         Author 1 \\ {\bf Author 2} \\ ... \\ {\bf Author n} \\
% For authors from different institutions:
% \author{Author 1 \\ Address line \\  ... \\ Address line
%         \And  ... \And
%         Author n \\ Address line \\ ... \\ Address line}
% To start a separate ``row'' of authors use \AND, as in
% \author{Author 1 \\ Address line \\  ... \\ Address line
%         \AND
%         Author 2 \\ Address line \\ ... \\ Address line \And
%         Author 3 \\ Address line \\ ... \\ Address line}

\author{Chen-An Li \quad Tzu-Han Lin \quad Yun-Nung Chen \quad  Hung-yi Lee \\
National Taiwan University, Taipei, Taiwan \\
\texttt{\{r13942069,r12944034\}@ntu.edu.tw} \texttt{y.v.chen@ieee.org} \quad \texttt{hungyilee@ntu.edu.tw}
}

%\author{
%  \textbf{First Author\textsuperscript{1}},
%  \textbf{Second Author\textsuperscript{1,2}},
%  \textbf{Third T. Author\textsuperscript{1}},
%  \textbf{Fourth Author\textsuperscript{1}},
%\\
%  \textbf{Fifth Author\textsuperscript{1,2}},
%  \textbf{Sixth Author\textsuperscript{1}},
%  \textbf{Seventh Author\textsuperscript{1}},
%  \textbf{Eighth Author \textsuperscript{1,2,3,4}},
%\\
%  \textbf{Ninth Author\textsuperscript{1}},
%  \textbf{Tenth Author\textsuperscript{1}},
%  \textbf{Eleventh E. Author\textsuperscript{1,2,3,4,5}},
%  \textbf{Twelfth Author\textsuperscript{1}},
%\\
%  \textbf{Thirteenth Author\textsuperscript{3}},
%  \textbf{Fourteenth F. Author\textsuperscript{2,4}},
%  \textbf{Fifteenth Author\textsuperscript{1}},
%  \textbf{Sixteenth Author\textsuperscript{1}},
%\\
%  \textbf{Seventeenth S. Author\textsuperscript{4,5}},
%  \textbf{Eighteenth Author\textsuperscript{3,4}},
%  \textbf{Nineteenth N. Author\textsuperscript{2,5}},
%  \textbf{Twentieth Author\textsuperscript{1}}
%\\
%\\
%  \textsuperscript{1}Affiliation 1,
%  \textsuperscript{2}Affiliation 2,
%  \textsuperscript{3}Affiliation 3,
%  \textsuperscript{4}Affiliation 4,
%  \textsuperscript{5}Affiliation 5
%\\
%  \small{
%    \textbf{Correspondence:} \href{mailto:email@domain}{email@domain}
%  }
%}

\begin{document}
\maketitle

\begin{abstract}
Large vision-language models (LVLMs) perform outstandingly across various multimodal tasks. However, their ability to evaluate generated content remains limited, and training vision-language reward models (VLRMs) with preference data is computationally expensive. This paper explores a training-free alternative by merging text-based reward models (RMs) with LVLMs to create VLRMs. Our approach shows that integrating these models leads to improved performance over LVLMs' scoring and text-based RMs, offering an efficient method for incorporating textual preferences into LVLMs.
\end{abstract}

\section{Introduction}
\label{sec:introduction}
Large vision-language models (LVLMs) have shown exceptional performance across a wide range of multimodal tasks~\citep{hurst2024gpt, team2024gemini, anthropic2024claude35}, primarily due to the implementation of reinforcement learning from human feedback (RLHF)~\citep{ouyang2022training}, which utilizes preference data~\citep{sun-etal-2024-aligning, li-etal-2024-vlfeedback}. This process often requires the use of reward models (RMs). However, LVLMs still struggle to assess generated content effectively~\citep{chen2024mllmasajudge, li2024vlrewardbench}, and training an RM with preference data is resource-intensive.

In this work, we investigate an alternative approach: \textit{Can knowledge derived from text-only preference data be transferred to LVLMs without additional training?} Several state-of-the-art LVLMs are built upon pre-trained language models with vision encoders and adapters~\citep{dubey2024llama, Qwen2.5-VL, lu2024deepseek}. This architectural design suggests that textual preferences learned by text-based RMs may potentially integrate into LVLMs through parameter merging.

\begin{figure}[t!]
    \centering    
    \includegraphics[width=0.92\linewidth]{figures/overview.pdf}
    \caption{Framework for merging a text-based RM with an LVLM. LVLMs excel at visual tasks, while text-based RMs struggle to provide accurate rewards without visual cues. We transfer textual preferences to the vision-language understanding, resulting in a VLRM.}
    \vspace{-15pt}
    \label{fig:overview}
\end{figure}

Building on this idea, we propose merging LVLMs with text-based RMs to create vision-language reward models (VLRMs), as illustrated in Figure~\ref{fig:overview}. Our approach leverages existing RMs and LVLMs, eliminating the need for costly multimodal preference data collection and training. We explore various merging strategies, ranging from simple weighted averaging~\citep{wortsman2022model} to advanced techniques such as task arithmetic~\citep{ilharco2023editing}, TIES~\citep{yadav2024ties}, and DARE~\citep{yu2024language}.

We assess performance using VL-RewardBench \citep{li2024vlrewardbench} and Best-of-N sampling with TextVQA~\citep{singh2019towards} and MMMU-Pro~\citep{yue2024mmmu}. The results show that our combined VLRMs outperform scoring through LVLMs and reward generation with text-based RMs. Our approach offers a training-free method for transferring textual preferences to LVLMs via model merging, and we provide a detailed analysis of merging strategies, demonstrating its effectiveness across multiple benchmarks.

\section{Related Work}
\label{sec:related_work}
% Preference dataset -> NLP a lot, but no Vision language preference data
\paragraph{Preference Dataset} A common approach to train a reward model is to use the Bradleyâ€“Terry model~\citep{bradley1952rank}, which relies on paired data for learning. In NLP, many high-quality preference datasets are already available~\citep{learningtosummarize,bai2022traininghelpfulharmlessassistant,pmlr-v162-ethayarajh22a,oasst,cui2024ultrafeedback,zhu2024starlingb,wang2024helpsteer}. Similarly, in the vision-language domain, several preference datasets have been introduced~\citep{Yu_2024_CVPR,yu2024rlaifv,chen2024sharegpt4v,wijaya2024multimodalpreferencedatasynthetic,li2024vlfeedback,zhou2024aligning,xiao2024detecting}. In this work, we explore the potential of transferring textual preferences to LVLMs in a training-free manner, specifically through model merging.

% VLM reward (VL RewardBench), VLM as a judge
\vspace{-3pt}
\paragraph{LVLM-as-a-Judge \& Evaluation} LVLM-as-a-Judge refers to utilizing strong large vision-language models for evaluation and judgment. These LVLMs can be either closed-source~\citep{openai2023gpt4v,hurst2024gpt,team2024gemini,anthropic2024claude35} or open-source~\citep{lee-etal-2024-prometheus,dubey2024llama,deitke2024molmo,Qwen2.5-VL}. To assess LVLMs as generative reward models, \citet{chen2024mllmasajudge} established benchmarks and found that LVLMs exhibit high agreement with humans in pairwise comparison judgments, but perform poorly in scoring evaluation and batch ranking tasks. Recently, VL-RewardBench~\citep{li2024vlrewardbench} introduced challenging cases and complex multimodal reasoning tasks, revealing that most off-the-shelf LVLMs struggle with such evaluations.

% Merging
\vspace{-3pt}
\paragraph{Model Merging} Model merging is a common, training-free method for combining skills from multiple models within the parameter space. A basic approach involves simple weighted averaging~\citep{wortsman2022model}, while more advanced techniques have been developed~\citep{yadav2024ties,yu2024language,yang2024modelmergingllmsmllms}. These techniques have already proven effective in reward modeling~\citep{rame2024warm,lin-etal-2024-dogerm} and LLM-as-a-judge~\citep{kim-etal-2024-prometheus} in NLP. Recently, REMEDY~\citep{zhu2025remedy} introduced strategies for merging LVLMs. In contrast, our work focuses on merging textual reward models into the language modeling components of LVLMs.

\section{Methodology}
\label{sec:methodology}
We propose a training-free method to transfer textual preferences from a text-based RM $\theta^{\text{RM}}$ to a LVLM $\theta^{\text{LVLM}}$ through model merging.

Since both models originate from the same pre-trained language model $\theta^{\text{PRE}}$, we merge modules that appear in both models and preserve the LVLMâ€™s vision capabilities and text-based RM reward function, resulting in a VLRM that can assess textual and visual content without additional training. Below, we outline the components and merging strategies involved.

\subsection{Model Components}
\input{tables/main_tulu25}

The pre-trained language model consists of:
\vspace{-3pt}
\begin{equation*}
\theta^{\text{PRE}} = \{\theta^{\text{PRE}}_{\text{emb}}, \theta^{\text{PRE}}_{\text{trans}}, \theta^{\text{PRE}}_{\text{lm}}\},
\vspace{-3pt}
\end{equation*}
where $\theta^{\text{PRE}}_{\text{emb}}$ is the embedding layer, $\theta^{\text{PRE}}_{\text{trans}}$ is the transformer, and $\theta^{\text{PRE}}_{\text{lm}}$ is the language modeling head, which maps the final hidden state of the transformer to the vocabulary.

The LVLM expands upon this with:
\vspace{-3pt}
\begin{equation*}
\theta^{\text{LVLM}} = \{\theta^{\text{LVLM}}_{\text{venc}}, \theta^{\text{LVLM}}_{\text{adapt}}, \theta^{\text{LVLM}}_{\text{emb}}, \theta^{\text{LVLM}}_{\text{trans}}, \theta^{\text{LVLM}}_{\text{lm}}\},
\vspace{-3pt}
\end{equation*}
where $\theta^{\text{LVLM}}_{\text{venc}}$ is the vision encoder, and $\theta^{\text{LVLM}}_{\text{adapt}}$ is the adapter that integrates the vision encoder outputs into the language model.

Similarly, the text-based RM is defined as:
\vspace{-3pt}
\begin{equation*}
\theta^{\text{RM}} = \{\theta^{\text{RM}}_{\text{emb}}, \theta^{\text{RM}}_{\text{trans}}, \theta^{\text{RM}}_{\text{rm}}\},
\vspace{-3pt}
\end{equation*}
where $\theta^{\text{RM}}_{\text{rm}}$ is the reward modeling head, which projects the transformer's final hidden state to a scalar value as the reward for a given input.

\subsection{Merging Strategies}

We explore four merging strategies.

\vspace{-3pt}
\paragraph{Weighted Averaging}
The weighted averaging strategy is defined as:
\vspace{-3pt}
\begin{equation*}
    \theta^{\text{MERGE}}_{\text{trans}} = \lambda \cdot \theta^{\text{LVLM}}_{\text{trans}} + (1 - \lambda) \cdot \theta^{\text{RM}}_{\text{trans}},
    \vspace{-3pt}
\end{equation*}
where $\lambda$ is a hyperparameter that controls the weight distribution between the two terms.

\vspace{-3pt}
\paragraph{Task Arithmetic}
Task arithmetic strategy is defined as:
\vspace{-3pt}
\begin{equation*}
    \begin{aligned}
        &\tau^{\text{LVLM}} = \theta^{\text{LVLM}}_{\text{trans}} - \theta^{\text{PRE}}_{\text{trans}}, \\
        &\tau^{\text{RM}} = \theta^{\text{RM}}_{\text{trans}} - \theta^{\text{PRE}}_{\text{trans}}, \\
        &\theta^{\text{MERGE}}_{\text{trans}} = \lambda \cdot \tau_{\text{LVLM}} + \lambda \cdot \tau_{\text{RM}},
    \end{aligned}
    \vspace{-3pt}
\end{equation*}
where $\tau^{\text{LVLM}}$ represents the task vector derived from instruction tuning, and $\tau^{\text{RM}}$ is the task vector obtained from reward modeling. The hyperparameter $\lambda$ controls the contribution of the task vectors.

\vspace{-3pt}
\paragraph{TIES \& DARE}
For the TIES and DARE strategies, we simplify the expression to:
\vspace{-3pt}
\begin{equation*} 
    \theta^{\text{MERGE}}_{\text{trans}} = \lambda \cdot f(\tau^{\text{LVLM}}, d) + \lambda \cdot f(\tau^{\text{RM}}, d),
    \vspace{-3pt}
\end{equation*}
where $f(\cdot)$ denotes the function for trimming, selecting, and rescaling the task vector, and $d$ is the density determining how many parameters are retained. The two strategies apply different methods for trimming, selecting, and rescaling. See Appendix~\ref{sec:appendix:merging_details} for more details on TIES and DARE.

\subsection{Merged VLRM}

The merged embedding parameters, $\theta^{\text{MERGE}}_{\text{emb}}$ are obtained following standard embedding merging techniques outlined in MergeKit~\citep{goddard-etal-2024-arcees}, as detailed in Appendix~\ref{sec:appendix:merging_details}. 

Finally, the merged VLRM $\theta^{\text{MERGE}}$ is obtained by combining several components:
\vspace{-3pt}
\begin{equation*}
    \theta^{\text{MERGE}} = \{\theta^{\text{LVLM}}_{\text{venc}}, \theta^{\text{LVLM}}_{\text{adapt}}, \theta^{\text{MERGE}}_{\text{emb}}, \theta^{\text{MERGE}}_{\text{trans}}, \theta^{\text{RM}}_{\text{rm}}\},
    \vspace{-3pt}
\end{equation*}
As a result, the merged VLRM can be used to provide rewards for both text and image content.

\section{Experiments}
\label{sec:experiments}

\subsection{Experimental Setup}
\label{subsec:experimental_setup}

\subsubsection{Models}
\label{subsubsec:models}
In this paper, we employ \texttt{Llama-3.2-11B-Vision\\-Instruct}~\citep{dubey2024llama} as our LVLM, referred to as \texttt{Llama-3.2-Vision}. For text-based RMs, we use \texttt{Llama-3.1-Tulu-2-8B-uf-mean-\\rm}~\citep{ivison2024unpacking} and \texttt{Llama-3.1-Tulu-3-\\8B-RM}~\citep{lambert2024t}, which we denote as \texttt{Tulu-2.5-RM} and \texttt{Tulu-3-RM}, respectively. All models derive from the same pre-trained language model \texttt{Llama-3.1-8B}. Our main results focus on \texttt{Tulu-2.5-RM} since it outperforms \texttt{Tulu-3-RM} on several VQA tasks with text-based input. Please refer to Appendix~\ref{sec:appendix:open_soruce_details} for the model details.

\subsubsection{Model Merging}
\label{subsubsec:model_merging}
We use MergeKit for model merging and apply several techniques: weighted averaging, task arithmetic, TIES, and DAREâ€”labeled as \texttt{Linear}, \texttt{Task Vec.}, \texttt{TIES}, and \texttt{DARE}, respectively. Additionally, we explore combining DARE with task arithmetic and TIES for a more thorough analysis. To determine the optimal merging hyperparameters, we conduct a hyperparameter search and sample 400 instances from the RLAIF-V~\citep{yu2024rlaifv} training set as our validation set. More details are provided in Appendix~\ref{sec:appendix:merging_details}.

\subsection{Reward Model Evaluation}
\label{subsec:evaluation}

\subsubsection{VL-RewardBench}
\label{subsubsec:vl_rewardbench}
We assess the merged VLRMs using VL-RewardBench~\citep{li2024vlrewardbench}, a benchmark that includes three domains: general multimodal instructions, hallucination-related tasks, and multimodal reasoning tasks. Each instance includes a multimodal query that consists of an image and a user prompt, along with a chosen response and a rejected response.

\subsubsection{Best-of-N Sampling}
\label{subsubsec:best_of_n}
We assess our reward model's effectiveness in enhancing performance through reranking using Best-of-N sampling, where N = 8 in our work. This method scores and ranks responses to check if the highest-scoring one matches the correct answer. Specifically, we use \texttt{Llama-3.2-11B-Vision-Instruct} to generate eight candidates for the TextVQA~\citep{singh2019towards} and MMMU-Pro~\citep{yue2024mmmu} datasets. See Appendix~\ref{sec:appendix:dataset_details} for dataset details.

\subsection{Main Results}
\label{subsec:results}
Table~\ref{tab:main_tulu25} demonstrates the effectiveness of merging methods for combining an LVLM with a text-based RM. The baseline approaches include \texttt{Llama-3.2-Vision}, which utilizes the LVLM for direct scoringâ€”pairwise scoring in VL-RewardBench and verbalized scoring in Best-of-N sampling tasks. Another baseline method, \texttt{Tulu-2.5-RM}, utilizes the text-based RM that focuses solely on evaluating the textual elements of questions and responses. We also incorporate a \texttt{Random} baseline that randomly selects responses. Furthermore, we implement a \texttt{Cascade} approach that employs a two-stage process: it first uses the LVLM to generate text descriptions of images based on the given question, then passes these descriptions with the original text inputs through the text-based RM to produce final scores.

As shown in Table~\ref{tab:main_tulu25}, merged VLRMs consistently outperform \texttt{Llama-3.2-Vision} and \texttt{Tulu-2.5-RM} across nearly all merging methods and benchmarks. This result demonstrates that combining a text-based RM with an LVLM effectively transfers textual preferences without training. Different merging strategies achieve the highest scores in different benchmarks, but overall, more advanced methods outperform simpler ones, highlighting the advantages of structured merging techniques. 
Additionally, in several benchmarks, merged VLRMs surpass or match the strong \texttt{Cascade} baseline, suggesting that model merging captures more information than merely cascading two models. Notably, in VL-RewardBench, our merged VLRMs even exceed the performance of the 90B LVLM and achieve results comparable to commercial models.
A similar trend emerges when using \texttt{Tulu-3-RM} as the text-based RM; further details are provided in Appendix~\ref{subsec:appendix:main_results}.

\subsection{Analysis}
\label{subsec:analysis}
\vspace{-3pt}
\paragraph{Without Image Input}
\label{paragraph:without_image_input}
\input{tables/cmp_img_small_tulu25}
To further investigate whether the merged VLRMs effectively use the vision encoder, we conduct an ablation study by evaluating the models without image input. As shown in Table~\ref{tab:cmp_image_small_tulu25}, most models with image input outperform those without it across various merging techniques. This result suggests that the vision encoder plays an active role after merging, with performance gains not solely attributed to the text-based RM. These findings highlight how merging methods effectively combine textual and visual information. However, image input does not improve performance in the MMMU-Pro Standard set, likely because this set emphasizes reasoning, where reward assessments depend more on textual coherence than visual understanding, limiting the vision encoder's contribution. A similar trend occurs when using \texttt{Tulu-3-RM} as the text-based RM; see Appendix~\ref{subsec:appendix:without_image_input} for details.

\begin{figure}[t]
    \centering
    \begin{subfigure}[t!]{0.49\linewidth}
        \centering
        \includegraphics[width=\textwidth]{figures/full_results/tulu2.5/dare_tv/vlrb.png}
        \caption{VL-RewardBench}
        \label{fig:daretv_vlrb}
    \end{subfigure}
    \hfill
    \begin{subfigure}[t!]{0.49\linewidth}
        \centering
        \includegraphics[width=\textwidth]{figures/full_results/tulu2.5/dare_tv/mmmu_std.png}
        \caption{MMMU-Pro (Standard)}
        \label{fig:daretv_mps}
    \end{subfigure}
    \vspace{-3pt}
    \caption{Effect of \texttt{Dare + Task Vec.} merging hyperparameters with \texttt{Tulu-2.5-RM} as the text-based RM.}
    \vspace{-15pt}
    \label{fig:hyperparameters_effect}
\end{figure}

\vspace{-3pt}
\paragraph{Effect of Merging Hyperparameters}
\label{paragraph:hyperparameters_effect}
We also investigate how merging hyperparameters impacts performance. Figure~\ref{fig:hyperparameters_effect} presents the results of searching for $d$ within the range [0.2, 0.4, 0.6, 0.8] and $\lambda$ within [0.5, 0.7, 1.0] for \texttt{DARE} + \texttt{Task Vec.}. 
Our findings indicate that optimal hyperparameter values vary across benchmarks. For example, in VL-RewardBench, $\lambda$ values do not have a significant effect, but in the MMMU-Pro standard set, we observe that $\lambda=1.0$ performs best.
This variation indicates that the choice of hyperparameters affects the performance of the final merged VLRM differently across tasks. Consequently, it highlights the importance of a well-curated validation set when selecting the optimal hyperparameters, which could be further explored in future research. 

Furthermore, our results for $d$ align with previous studies on TIES and DARE: even when task vectors are trimmed to lower rates (e.g., 0.4, 0.2), the merged VLRMs maintain strong performance, consistent with the findings on LLM merging. For further hyperparameter search results across other methods and benchmarks, refer to Appendix~\ref{subsec:appendix:hyperparameters_effect}.

\section{Conclusion}
\label{sec:conclusion}
This work presents a training-free approach for integrating text-based RMs into LVLMs through model merging. Our method enables the efficient transfer of textual preferences without the expensive multimodal preference data collection or additional training. Experimental results show that our approach outperforms LVLM scoring and text-based RMs in multimodal reward assessment tasks. 

\section*{Limitations}
\label{sec:limitations}
Our study has several limitations. First, we focused on a specific 11B vision-language model paired with an 8B text-based reward model, primarily due to limitations in computational resources. Additionally, we focused solely on the LLaMA architecture and did not explore alternatives like Qwen~\citep{bai2023qwen,bai2023qwenvl} due to the absence of a suitable Qwen-based reward model for our experiments. Furthermore, we did not perform extensive ablation studies on the validation set. Our experimental results highlight the importance of a well-curated validation set in selecting optimal hyperparameters, which could be explored further in future research. Finally, due to the sensitivity of RLHF to hyperparameter tuning and our computational constraints, we did not implement algorithms like PPO~\citep{schulman2017proximal}. Future work could explore integrating RLHF with merged VLRMs to assess its potential impact.

\section*{Ethics Statement}
\label{sec:ethics_statement}
Our approach leverages pre-trained language and reward models, which may inherit biases from the training data. While merging models can enhance efficiency, it does not inherently mitigate existing biases. We encourage further research to evaluate and address potential biases in merged models to ensure fairness across diverse user groups. 

\bibliography{custom}

\appendix

\section{Merging Details}
\label{sec:appendix:merging_details}

\paragraph{Weighted Averaging}
\label{sec:appendix:paragraph:weighted_averaging}
\citet{wortsman2022model} showed that combining the weights of multiple models fine-tuned with varying hyperparameter settings often leads to improved accuracy and robustness. In this work, we employ a weighted averaging strategy as a straightforward method to merge a large vision-language model with a text-based reward model. The weighted averaging strategy is formally defined as:
\begin{equation*}
    \theta^{\text{MERGE}}_{\text{trans}} = \lambda \cdot \theta^{\text{LVLM}}_{\text{trans}} + (1 - \lambda) \cdot \theta^{\text{RM}}_{\text{trans}},
\end{equation*}
where $\lambda$ is a hyperparameter that determines the weight distribution between the two models. We explore $\lambda$ values in the range: [0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0].

\paragraph{Task Arithmetic}
\label{sec:appendix:paragraph:task_arithmetic}
\citet{ilharco2023editing} demonstrated that the task vector, obtained by subtracting the weights of a pre-trained model from those of the same model after fine-tuning for a specific task, defines the task direction. Utilizing this task vector can improve task performance. We also apply the task arithmetic approach to develop a vision-language reward model. The task arithmetic strategy is formally defined as:
\begin{equation*}
    \begin{aligned}
        &\tau^{\text{LVLM}} = \theta^{\text{LVLM}}_{\text{trans}} - \theta^{\text{PRE}}_{\text{trans}}, \\
        &\tau^{\text{RM}} = \theta^{\text{RM}}_{\text{trans}} - \theta^{\text{PRE}}_{\text{trans}}, \\
        &\theta^{\text{MERGE}}_{\text{trans}} = \lambda \cdot \tau_{\text{LVLM}} + \lambda \cdot \tau_{\text{RM}},
    \end{aligned}
\end{equation*}
where $\tau^{\text{LVLM}}$ denotes the task vector derived from instruction tuning, and $\tau^{\text{RM}}$ refers to the task vector obtained from reward modeling. The hyperparameter $\lambda$ controls the relative contribution of task vectors. We explore $\lambda$ values in the range: [0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0].

\paragraph{TIES}
\label{sec:appendix:paragraph:ties}
\citet{yadav2024ties} consider the interference between parameters from different models during the model merging process. Their approach consists of three main steps. First, they prune task vector values based on magnitude, retaining only a proportion $d$ of the task vector. Second, they resolve sign conflicts by calculating the total magnitude of parameter values in positive and negative directions and selecting the direction with the larger total magnitude. Only values that match the chosen sign are retained. Finally, they compute the mean of the retained values to determine the final parameter value. The TIES method can be simply expressed as:
\begin{equation*} 
    \theta^{\text{MERGE}}_{\text{trans}} = \lambda \cdot f(\tau^{\text{LVLM}}, d) + \lambda \cdot f(\tau^{\text{RM}}, d),
\end{equation*}
where $f(\cdot)$ denotes the function for trimming, selecting, and rescaling the task vector, and $d$ is the density determining how many parameters are retained. We search for optimal values of $\lambda$ within the range [0.5, 0.7, 1.0] and $d$ within the range [0.2, 0.4, 0.6, 0.8].

\paragraph{DARE}
\label{sec:appendix:paragraph:dare}
\citet{yu2024language} also addresses the interference between parameters from different models during the model merging process. They randomly drop delta parameters with a probability of $p$ and rescale the remaining ones by $1/(1-p)$. The DARE method can be combined with both the Task Arithmetic and TIES approaches. When combined with Task Arithmetic, a proportion $p$ of task vectors is randomly dropped, and the remaining ones are rescaled by $1/(1-p)$. When DARE is combined with TIES, a proportion $p$ of task vectors is randomly dropped, and the sign of each parameter is determined by comparing the total magnitude in the positive and negative directions. The sign corresponding to the larger total magnitude is selected, and only values matching this sign are retained. Their mean is then computed as the final parameter value, and the result is rescaled by $1/(1-p)$. The DARE method can also be expressed as:
\begin{equation*} 
    \theta^{\text{MERGE}}_{\text{trans}} = \lambda \cdot f(\tau^{\text{LVLM}}, d) + \lambda \cdot f(\tau^{\text{RM}}, d),
\end{equation*}
where $d$ represents the density, determining the proportion of retained parameters, with $d=1-p$. We search for optimal values of $\lambda$ within the range [0.5, 0.7, 1.0] and $d$ within the range [0.2, 0.4, 0.6, 0.8].

\paragraph{Merging Embeddings}
\label{sec:appendix:paragraph:merger_embeds}
We follow the embedding merging procedure from MergeKit~\citep{goddard-etal-2024-arcees}. The process is as follows:

\begin{enumerate}
\item If a token exists in the pre-trained model, we use its embedding from that model.
\item If a token appears in only one model (either the LVLM or the text-based RM), we use its embedding from that model.
\item If a token appears in multiple models, we compute the average of its embeddings.
\end{enumerate}

Notably, the pre-trained model is not required for the weighted averaging method. Therefore, we omit the first step when applying this merging approach.

\paragraph{Merging Hyperparameter Selection}
\label{sec:appendix:paragraph:hyper_select}
We select the merging hyperparameter by using a sampled set of 400 instances from the RLAIF-V~\citep{yu2024rlaifv} training set as our validation set. In case of a tie in scores, an additional 100 sampled instances will be used for evaluation. Results are discussed in Appendix~\ref{subsec:appendix:hyperparameters_effect}.

\section{Dataset Details}
\label{sec:appendix:dataset_details}

\paragraph{VL-RewardBench}
\label{sec:appendix:paragraph:vl_rewardbench}
VL-RewardBench~\citep{li2024vlrewardbench} is a benchmark comprising 1,250 high-quality examples spanning three domains: general multimodal instructions, hallucination-related tasks, and multimodal reasoning tasks. Each example includes a multimodal queryâ€”consisting of an image and a user promptâ€”along with a selected response and a rejected response.

\paragraph{TextVQA}
\label{sec:appendix:paragraph:textvqa}
TextVQA~\citep{singh2019towards} is a dataset designed to evaluate the ability of visual question-answering (VQA) models to read and reason about text within images. We use its validation set, which contains 5,000 instances, to assess our merged VLRMs.

\paragraph{MMMU-Pro}
\label{sec:appendix:paragraph:mmmu_pro}
MMMU-Pro~\citep{yue2024mmmu} is an advanced benchmark designed to assess the understanding and reasoning abilities of multimodal models. It is derived from the original MMMU~\citep{yue2024mmmu1} dataset and consists of two subsets: a standard set, which includes image and text queries with 10 answer options, and a vision set, which features a vision-only input scenario. In the vision set, the questions are embedded within screenshots or photos, with no explicit text provided.

\paragraph{RLAIF-V}
\label{sec:appendix:paragraph:rlaif_v}
RLAIF-V~\citep{yu2024rlaifv} preference dataset is created by generating multiple candidate responses for a given prompt and image using various random seeds. Each response is divided into individual claims, which are then assessed using an open-source large vision-language model. This model assigns confidence scores to each claim, which are combined to form an overall response score. Preference pairs are generated by comparing the response scores for the same prompt, selecting the preferred response and the less favorable one based on the score differences. Pairs with significant length disparities are excluded to avoid bias. We select 400 instances from this preference dataset to serve as our validation set for selecting the hyperparameters of merging methods.

\section{Best-of-N Sampling Details}
\label{sec:appendix:best_of_n}
We use lmms-eval~\citep{zhang2024lmms} for response generation with the Best-of-N sampling technique. For the TextVQA dataset, we set both the temperature and top-p to 1.0, sampling 8 responses. To encourage concise answers, we append ``Answer the question using a single word or phrase.'' after the generation prompt. For the MMMU-Pro dataset, we also set the temperature and top p to 1.0, with a maximum token limit of 4096, to sample 8 responses. Additionally, we apply chain-of-thought (CoT) for generating both answers and their reasoning.

\section{Prompt Template}
\label{sec:appendix:prompt_template}
For Best-of-N sampling using \texttt{LLaMA-3.2-Vision} as the generative reward model, the prompt template is provided in Table~\ref{tab:prompt_templates}. For image captioning with \texttt{LLaMA-3.2-Vision} and reward modeling using \texttt{Tulu-3-RM} and \texttt{Tulu-2.5-RM}, the detailed prompt template can also be found in Table~\ref{tab:prompt_templates}.

\section{Open-Source Model Details}
\label{sec:appendix:open_soruce_details}

\paragraph{\texttt{Llama-3.2-11B-Vision-Instruct}}
\texttt{Llama-3.2\\-11B-Vision-Instruct}~\citep{dubey2024llama} is an 11B-parameter LVLM consisting of three main components: a vision encoder, an adapter, and a pre-trained language model. The language model is based on \texttt{Llama-3.1-8B-Instruct}. The adapter incorporates cross-attention layers to integrate image representations into the language model. During adapter training, the language model remains frozen, enabling seamless drop-in replacement for Llama-3.1 series models without requiring re-training.

\paragraph{\texttt{Tulu-2.5-RM}}
\texttt{Tulu-2.5-RM}~\citep{ivison2024unpacking} is a reward model initialized from \texttt{Llama-3.1-8B} and fine-tuned using the Tulu 2 recipe~\citep{ivison2023camels}. It is adapted for reward modeling by replacing the language modeling head with a linear layer and fine-tuning it on preference data from diverse sources, including Ultrafeedback~\citep{cui2024ultrafeedback}, Nectar~\citep{zhu2024starlingb}, HH-RLHF~\citep{bai2022traininghelpfulharmlessassistant}, and AlpacaFarm~\citep{dubois2023alpacafarm}, among others.

\paragraph{\texttt{Tulu-3-RM}}
\texttt{Tulu-3-RM}~\citep{lambert2024t} is another reward model initialized from \texttt{Llama-3.1-8B} and fine-tuned following the Tulu 3 recipe~\citep{lambert2024t}. Like \texttt{Tulu-2.5-RM}, it is adapted for reward modeling by replacing the language modeling head with a linear layer. However, \texttt{Tulu-3-RM} is trained on a mixture of on-policy and off-policy preference data collected through an enhanced version of the Ultrafeedback~\citep{cui2024ultrafeedback} pipeline. This dataset includes prompts from various sources, such as the SFT dataset in the Tulu 3 recipe, WildChat~\citep{zhao2024wildchat}, Ultrafeedback~\citep{cui2024ultrafeedback}, and synthetic persona-augmented instructions.

\section{Qualitative Results}
\label{sec:appendix:qualitative_results}
We investigate reward model behavior before and after merging, and we evaluate qualitatively on VL-RewardBench. Tables~\ref{tab:qualitative_results} and~\ref{tab:qualitative_results_2} present results for \texttt{Tulu-2.5-RM}, while Tables~\ref{tab:qualitative_results_3} and~\ref{tab:qualitative_results_4} show \texttt{Tulu-3-RM}. \textcolor{Red}{Red text} indicates misalignment with the image. Before merging, the text-based reward model made incorrect predictions. After merging, the vision-language reward models correctly identified the better response. In most cases, more advanced merging methodsâ€”such as task arithmetic, TIES, and DAREâ€”produce larger reward differences between chosen and rejected responses than simple weighted averaging.

\section{Full Results}
\label{sec:appendix:full_results}
\subsection{Main Results}
\label{subsec:appendix:main_results}
The main results of merging with \texttt{Tulu-2.5-RM} are discussed in Section~\ref{subsec:results} of the main text. As shown in Table~\ref{tab:main_tulu25}, merged VLRMs consistently outperform \texttt{Llama-3.2-Vision} and \texttt{Tulu-2.5-RM} across nearly all merging methods and benchmarks. Notably, in VL-RewardBench, they show the greatest improvement in the Hallucination domain. In Best-of-N evaluation, they perform well in both TextVQA and MMMU-Pro. Additionally, merged VLRMs match or surpass the strong \texttt{Cascade} baseline, suggesting that merging captures more information than simply cascading two models.

A similar trend is observed when merging with \texttt{Tulu-3-RM}. As shown in Table~\ref{tab:main_tulu3}, merged VLRMs outperform \texttt{Llama-3.2-Vision} and \texttt{Tulu-3-RM} across most methods and benchmarks. In VL-RewardBench, they improve mainly in the General and Hallucination domains. For Best-of-N evaluation, they perform well in MMMU-Pro, but only a few achieve results comparable to \texttt{Llama-3.2-Vision} in TextVQA, likely due to \texttt{Tulu-3-RM}â€™s weaker performance in this task. While merging with \texttt{Llama-3.2-Vision} enhances performance over \texttt{Tulu-3-RM}, it does not surpass \texttt{Llama-3.2-Vision}'s score. Additionally, merged VLRMs exceed the strong \texttt{Cascade} baseline in other benchmarks and remain competitive with it in TextVQA.

In Table~\ref{tab:vlrb_full}, we compare our merged VLRMs with large open-source LVLMs and commercial systems on VL-RewardBench. Surprisingly, our merged VLRMs outperform 90B LVLMs and achieve performance comparable to commercial models, demonstrating the effectiveness of transferring textual preferences from text-based RMs to LVLMs.

\subsection{Without Image Input}
\label{subsec:appendix:without_image_input}
We conduct an ablation study by evaluating models without image input. Full results with \texttt{Tulu-2.5-RM} are shown in Table~\ref{tab:cmp_image_full_tulu25}. Models with image input consistently outperform those without it across various merging techniques, suggesting that the vision encoder actively contributes after merging rather than performance gains being solely due to the text-based RM. This indicates that merged VLRMs effectively utilize the vision encoder in most cases. Notably, in VL-RewardBench, merged VLRMs match or surpass those without image input, especially in the hallucination domain, where image input significantly improves performance. In Best-of-N evaluation, models with image input perform better in the TextVQA and MMMU-Pro Vision sets. However, in the MMMU-Pro Standard set, image input does not provide an advantage, likely because this set emphasizes text reasoning, where reward assessments depend more on textual coherence than visual information.

Full results with \texttt{Tulu-3-RM} are shown in Table~\ref{tab:cmp_image_full_tulu3}, following a similar trend. In VL-RewardBench, merged VLRMs outperform those without image input in the hallucination domain and are comparable to or surpass them in general and reasoning domains. Image input also enhances Best-of-N evaluation, particularly in TextVQA and MMMU-Pro Vision. However, in the MMMU-Pro Standard, image input does not provide a clear advantage, reaffirming that this set prioritizes text reasoning over visual input.

\subsection{Effect of Merging Hyperparameters}
\label{subsec:appendix:hyperparameters_effect}
In this study, we optimize hyperparameter merging using sampled instances from RLAIF-V. The results, based on 400 sampled RLAIF-V instances used as a validation set, are presented in Tables~\ref{tab:hyper_select_linear_tulu25} to \ref{tab:hyper_select_dare_ties_tulu3}. Bold text highlights the best performance, while \textbf{text with *} indicates cases where scores are tied. In these cases, an additional 100 samples are used, and * marks the top-performing result among them.

Figures~\ref{fig:full_tulu2.5_linear} to \ref{fig:full_tulu3_dare_ties} show the effect of hyperparameters across various benchmarks, merging methods, and text-based RMs. The results reveal that optimal hyperparameters differ across these factors, emphasizing the importance of a well-constructed validation set. Future research could further explore this. For example, Figure~\ref{fig:full_tulu2.5_linear} shows the results of searching for $\lambda$ values between 0 and 1 for the \texttt{Linear} method using \texttt{Tulu-2.5-RM}. In the VL-RewardBench, a mid-range $\lambda$ produces the best performance, while in the MMMU-Pro vision set, a smaller $\lambda$ yields better results. This variation suggests that hyperparameter choices influence the performance of the final merged VLRMs differently depending on the task.

Moreover, we observe a trend consistent with prior studies~\citep{yadav2024ties, yu2024language}: even when task vectors are reduced to lower rates (e.g., 0.4, 0.2), merged VLRMs continue to perform well, aligning with findings on LLM merging.

\input{tables/prompt_templates}

\input{tables/qualitatives/qualitative_1}
\input{tables/qualitatives/qualitative_2}
\input{tables/qualitatives/qualitative_3}
\input{tables/qualitatives/qualitative_4}

\input{tables/main_tulu3}
\input{tables/cmp_image_full_tulu25}
\input{tables/cmp_image_full_tulu3}
\input{tables/VLRB_full}
\input{figures/full_results}
\input{tables/hyper_select}


\end{document}
