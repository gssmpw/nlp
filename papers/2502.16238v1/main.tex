%%%%%%%% ICML 2025 EXAMPLE LATEX SUBMISSION FILE %%%%%%%%%%%%%%%%%

\documentclass{article}
\usepackage[T1]{fontenc}

% Recommended, but optional, packages for figures and better typesetting:
\usepackage{amsmath,amssymb}
\usepackage{microtype}
\usepackage{graphicx}
%\usepackage{subfigure}
\usepackage{subcaption}
\usepackage{booktabs} % for professional tables
\usepackage{xcolor}
\usepackage{multirow}               % For multi-row cells if needed
\usepackage{array}                  % For extended table features
% Optional: Reduce the space between columns
\setlength{\tabcolsep}{4pt}          % Default is 6pt

% Optional: Adjust row spacing
\renewcommand{\arraystretch}{1.0}    % Default is 1.0

% mathcommands:
\input{math_imports}
% for code
\usepackage{listings}
\usepackage{xcolor}

\lstset{
    backgroundcolor=\color{lightgray!20}, % Light gray background
    basicstyle=\ttfamily,  % Typewriter font
    breaklines=true,  % Allows line breaks
    captionpos=b, % Position of captions
    showstringspaces=false, % No visible spaces
}

% hyperref makes hyperlinks in the resulting PDF.
% If your build breaks (sometimes temporarily if a hyperlink spans a page)
% please comment out the following usepackage line and replace
% \usepackage{icml2025} with \usepackage[nohyperref]{icml2025} above.
\usepackage{hyperref}


% Attempt to make hyperref and algorithmic work together better:
\newcommand{\theHalgorithm}{\arabic{algorithm}}

% Use the following line for the initial blind version submitted for review:
%\usepackage{icml2025}

% If accepted, instead use the following line for the camera-ready submission:
\usepackage[accepted]{icml2025}
\makeatletter
\renewcommand{\icmlEqualContribution}{\textsuperscript{*}Equal contribution, authors listed alphabetically.}
\makeatother

% Delete this after accepted
\makeatletter
\renewcommand{\ICML@appearing}{Preprint.}

% Remove the two lines:
\def\@copyrightspace{%
  \@float{copyrightbox}[b]
  \begin{center}
  \setlength{\unitlength}{1pc}
  \begin{picture}(20,1.5)
    % \put(0,2.5){\line(1,0){4.818}}  % comment out this line
    \put(0,0){\parbox[b]{19.75pc}{\small \Notice@String}}
  \end{picture}
  \end{center}
  \end@float
}
\renewcommand{\footnoterule}{}
\makeatother
% end to delete section

% For theorems and such
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{amsthm}

% if you use cleveref..
\usepackage[capitalize,noabbrev]{cleveref}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% THEOREMS
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\theoremstyle{plain}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{assumption}[theorem]{Assumption}
\theoremstyle{remark}
\newtheorem{remark}[theorem]{Remark}

% Todonotes is useful during development; simply uncomment the next line
%    and comment out the line below the next line to turn off comments
%\usepackage[disable,textsize=tiny]{todonotes}
\usepackage[textsize=tiny]{todonotes}


% The \icmltitle you define below is probably too long as a header.
% Therefore, a short form for the running title is supplied here:
\icmltitlerunning{NeuroAI Turing Test}

\begin{document}

\twocolumn[
\icmltitle{Brain-Model Evaluations Need the NeuroAI Turing Test}

\icmlsetsymbol{equal}{*}

\begin{icmlauthorlist}
  \icmlauthor{Jenelle Feather}{equal,yyy}
  \icmlauthor{Meenakshi Khosla}{equal,comp}
  \icmlauthor{N. Apurva Ratan Murty}{equal,sch}
  \icmlauthor{Aran Nayebi}{equal,cmu}
\end{icmlauthorlist}

\icmlaffiliation{yyy}{Flatiron Institute, New York, USA}
\icmlaffiliation{comp}{University of California San Diego, La Jolla, USA}
\icmlaffiliation{sch}{Georgia Institute of Technology, Atlanta, USA}
\icmlaffiliation{cmu}{Carnegie Mellon University, Pittsburgh, USA}

\icmlcorrespondingauthor{Jenelle Feather}{jfeather@flatironinstitute.org}
\icmlcorrespondingauthor{Meenakshi Khosla}{mkhosla@ucsd.edu}
\icmlcorrespondingauthor{N. Apurva Ratan Murty}{ratan@gatech.edu}
\icmlcorrespondingauthor{Aran Nayebi}{anayebi@cs.cmu.edu}


\icmlkeywords{NeuroAI, Turing Test, Machine Learning}

\vskip 0.3in
]

\begin{abstract}
What makes an artificial system a good model of intelligence?
The classical test proposed by Alan Turing focuses on behavior, requiring that an artificial agent's behavior be indistinguishable from that of a human.
While behavioral similarity provides a strong starting point, two systems with very different internal representations can produce the same outputs.
Thus, in modeling biological intelligence, the field of NeuroAI often aims to go beyond behavioral similarity and achieve representational convergence between a model's activations and the measured activity of a biological system.
This position paper argues that the standard definition of the Turing Test is incomplete for NeuroAI, and proposes a stronger framework called the ``NeuroAI Turing Test,'' a benchmark that extends beyond behavior alone and \emph{additionally} requires models to produce internal neural representations that are empirically indistinguishable from those of a brain up to measured individual variability, i.e. the differences between a computational model and the brain is no more than the difference between one brain and another brain.
While the brain is not necessarily the ceiling of intelligence, it remains the only universally agreed-upon example, making it a natural reference point for evaluating computational models.
By proposing this framework, we aim to shift the discourse from loosely defined notions of brain inspiration to a systematic and testable standard centered on both behavior and internal representations, providing a clear benchmark for neuroscientific modeling and AI development.
\vspace{10pt}
\end{abstract}
%]
---------------------------------------------
% 3) Print the footnotes for authors & notice:
% ---------------------------------------------
\printAffiliationsAndNotice{\icmlEqualContribution}

\section{Introduction}
\label{sec:intro}

\begin{figure}[t]
    \centering
    \includegraphics[width=\linewidth]{figures/Figure1Sketch.pdf}
    \vspace{-20pt}
    \caption{The complete NeuroAI Turing Test reflects the similarity of both the behavior of an artificial system and the similarity of the internal representations.}
    \label{fig:neuroaiTuringTest}
    \vspace{-15pt}
\end{figure}

Humanity is in the midst of an intense pursuit to understand and replicate intelligence in artificial systems.
But the AI community (computer scientists scaling up AI algorithms) and the NeuroAI community (computational cognitive neuroscientists leveraging AI systems to build models of the brain) seem to be approaching the challenge of intelligence from fundamentally divergent perspectives.
AI researchers have primarily focused on developing systems that exhibit intelligent behavior, a tradition rooted in Alan Turing’s seminal idea of the Turing Test~\citep{turing1950computing}. 
The focus on achieving behavioral similarity to humans has undoubtedly propelled AI forward, but the goal is rarely to study and understand how the internal representations of AI systems generate behavior.
In contrast, NeuroAI aims to model the computational principles underlying intelligence in biological systems, using the brain—the only example of intelligence we universally recognize and agree upon—as a reference point.
Evaluations of NeuroAI models often involve collecting brain data (e.g., neural spike trains or fMRI data) and comparing the similarity of the evoked representations with those measured from an artificial system. 
While the brain serves as a natural ground-truth for intelligence, providing a benchmark for any system aspiring to match or exceed it, we have yet to settle on the core algorithmic principles that give rise to many of its intelligent behaviors. 
Meeting this threshold is a necessary step before we can surpass it, yet despite significant advancements in our ability to collect and analyze neural data, NeuroAI has not established a clear set of criteria for when a model would successfully be considered ``brain-like.''

Here we propose the NeuroAI Turing Test to address these challenges. \textbf{Our central position is that brain-model evaluations in computational neuroscience urgently need a NeuroAI Turing Test that assesses both behavior and internal representations to the limit of carefully recorded biological measurements (the ceiling).} 

The NeuroAI Turing Test extends Turing’s original evaluation framework by integrating representational convergence of the internal features into the evaluation criteria (the integrative benchmarking condition \cite{schrimpf2020integrative}). 
It also mandates that an artificial model’s internal neural activations must be empirically indistinguishable from those observed in biological brains, but within the bounds of natural inter-individual variability (a measure of success that goes beyond integrative benchmarking). 

We propose that the NeuroAI Turing Test become the standard in NeuroAI, towards the development of truly brain-like artificial intelligence. Our proposition is timely and necessary. As AI systems become increasingly sophisticated, the demand for models that genuinely reflect human intelligence will intensify. The NeuroAI Turing Test offers a testable standard that brings together the behavioral focus of AI with the mechanistic insights of neuroscience.

\section{Why do we need a distinct test for NeuroAI?}
\label{sec: need}

\textbf{We think that the NeuroAI Turing Test is imperative for overcoming fundamental limitations inherent in both the classical Turing Test and the prevailing practices within NeuroAI.} 

The traditional Turing Test and recent proposed extensions 
\citep{zador2023catalyzing}, focus extensively on behavioral indistinguishability between biological organisms and machines i.e. the outputs.
However, testing the output behavior alone is insufficient because many possible internal processes can produce identical behaviors through entirely different computational mechanisms (see Fig.~\ref{fig:neuroaiTuringTest} top).

NeuroAI on the other hand, places importance on faithfully modeling both the human brain and behavior (cognition). But even within the NeuroAI community,  multiple definitions exist for what constitutes a ``good'' model.
In addition, the methodologies for comparing models with brains are fragmented, and rely on evaluation measures (metrics) that offer only relative comparisons between models without establishing any specific consideration for a definitive standard of success (more on this to follow).

The NeuroAI Turing Test directly addresses these shortcomings by establishing rigorous criteria for replicating behavior and internal mechanisms (biological brains) benchmarked against the variability observed between individual humans (or animals, in the case of many neural recordings).
To assess internal similarity, we focus on measures of \textit{representational convergence} which capture similarity at Marr's ``algorithmic'' level, in contrast to the original Turing Test which evaluates similarity at the ``computational'' level \cite{marr2010vision}. Note however that the NeuroAI Turing Test framework is also sufficiently general to also be applied to models that attempt to capture ``implementation'' level constraints. 

Together, the NeuroAI Turing Test sets a higher bar for scientific rigor and empirical validation for brain models. Our goal is to empower the AI and Neuroscience communities with the tools and methods to identify superficial imitations and to strive for neuroscientific accuracy with brains. We think this shift is crucial for steering the co-evolution of AI and neuroscience toward new insights and the development of truly robust, brain-inspired systems. Without a rigorous standard, AI risks perpetuating models that are impressive in performance yet fundamentally disconnected from the biological intelligence they seek to emulate. The NeuroAI Turing Test is a necessary evolution.

\textbf{Motivation for standardization.}
Often, measures of brain-brain similarity are relegated to the supplement of papers. These numbers are typically below the noise ceiling obtained from data splits within the same participants, and the amount of variance yet to explain relative to the noise-ceiling is emphasized as progress to be made with future models.
There is also a lack of agreement about what value on a particular dataset and metric would suggest that the similarity measure has been saturated, reflected by a diverse set of ceiling measurements in common benchmarks \cite{Schrimpf2018, ratan2021computational, conwell2022can}. 
Our NeuroAI Turing test provides practical guidance for future reporting of benchmarks: in addition to noise correcting for variability due to internal stochasticity and measurement noise, we must also report brain-brain similarity values (Fig.~\ref{fig:possible_outcomes}).
And when we do, we find on ``classic'' standard benchmarks (such as for primate object recognition), that current models are closer to saturating them than we thought before (Fig. ~\ref{fig:hvm_analysis}), suggesting a greater need for \emph{new} benchmarks, rather than pushing further on the existing ones to yield incremental improvement.

\begin{figure}
    \centering
    \includegraphics[width=\columnwidth]{figures/SchematicAndHVM_1.pdf}
    \vspace{-10pt}
    \caption{Possible outcomes of a NeuroAI Turing Test (brain-brain similarity). Each bar is for a different model. The model similarity measure and the brain-brain similarity (NeuroAI Turing Test) are both corrected by the square root of the product of the internal and mapping consistencies, constituting the ``Statistical Noise Ceiling'' (see Appendix~\ref{sec:methods-interanimal} for details). Different interpretations arise from the relationship between the model similarity and the distribution of the brain-brain similarity. Researchers should report both values to ensure that a benchmark is not saturated according to brain-brain similarity. Although this figure focuses on the alignment of internal representations, similar comparisons should be reported for behavioral tests.
    }
    \label{fig:possible_outcomes}
    \vspace{-15pt}
\end{figure}

\begin{figure}[htb]
    \centering
    \includegraphics[width=\columnwidth]{figures/SchematicAndHVM_4.pdf}
    \vspace{-10pt}
    \caption{The NeuroAI Turing Test on the classic HvM dataset~\citep{Majaj2015} with different metrics. Neural predictivity is shown for a ResNet-18 inspired feedforward network from~\citet{nayebi2022}. The linear mapping was performed on multiple timepoints with different numbers of PLS components, and each dataset and measure has a separate value for the NeuroAI Turing Test. These data suggest that at least for HvM, we have reasonably saturated this benchmark and should choose other ones for primate vision.}
    \label{fig:hvm_analysis}
    \vspace{-10pt}
\end{figure}

\section{Defining the NeuroAI Turing Test}
\label{sec:def-neuroai-turing}
In the standard Turing Test, Alan Turing reframed the question ``Can machines think?'' into ``Can machines produce behavior indistinguishable from a human?'' 
Similarly, in the NeuroAI Turing Test, we extend this idea by requiring machines not only to produce behavior that aligns with human capabilities but also to generate \emph{internal} representations that are indistinguishable from those recorded from human (or animal) brains.

\textbf{Setup.} Let $\D \in \mathbb{R}^{C \times T \times N}$, where $C=\text{(\# conditions)},\;T=\text{(\# timepoints)},\;N=\text{(\# outputs)}$, be a dataset of neural (e.g., neuron spike counts or fMRI voxel responses) or behavioral responses from a set of organisms (animals or human subjects) $\mathcal{O}(\mathcal{D})$.
Let $\{X_i\}_{i \in \mathcal{O}(\mathcal{D})}$ be the measurements from those organisms.
In this framework, the time dimension is optional, as one can examine time averages, but at minimum conditions and outputs must be obtained from the brain or behavior.
Let $X_m$ be the corresponding measurements from a model (e.g. unit activations or behavioral output from a neural network) under the same conditions.
Note that the model can either be embodied~\citep{zador2023catalyzing,pak2023newborn} or not, that is a flexible choice.
Let $\M : \mathbb{R}^{C \times T \times N} \to \R$ be a metric on the space of these representations.

\textbf{Distances.}
Define the \emph{inter-organism} distance set:
\begin{equation*}
    \Delta_{\text{organism}} \;=\; 
    \bigl\{\, \mathcal{M}(X_i, \, X_j) 
    \;:\; i,j \in \mathcal{O}(\mathcal{D}),\, i \neq j \bigr\}.
\end{equation*}
Next, define the \emph{model-organism} distance set:
\begin{equation*}
    \Delta_{\text{model}} \;=\;
    \bigl\{\, \mathcal{M}(X_m, \, X_i) 
    \;:\; i \in \mathcal{O}(\mathcal{D}) \bigr\}.
\end{equation*}

\textbf{Hypothesis Testing.}
Next, choose a significance level $\alpha \in (0,1)$ of convergence.
Select a distribution-free two-sample test $T$ (e.g., rank-sum, permutation, or KS) to compare $\Delta_{\text{organism}}$ and $\Delta_{\text{model}}$.
Let
\begin{equation*}
\begin{split}
& H_0:\ \Delta_{\text{model}} \text{ and } \Delta_{\text{organism}} 
    \text{ come from the same distribution,}\\
& H_1:\ \Delta_{\text{model}} \text{ and } \Delta_{\text{organism}} 
    \text{ differ systematically } \\
& \quad \text{(e.g., model-organism distances are larger).}
\end{split}
\end{equation*}

\begin{definition}[Convergence in Distribution.]\label{def:rep-conv}
We say the model's representation \emph{converges in distribution} to that of the organisms in $\mathcal{O}(\mathcal{D})$ (at level $\alpha$) if:
\begin{enumerate}
    \item Under the chosen test $T$, we \emph{fail} to reject $H_0$ at the $\alpha$ level (i.e., $p \ge \alpha$),
    \item A chosen statistic (e.g., difference in means or medians) indicates that model-organism distances are not systematically larger (or otherwise different) than the inter-organism distances.
\end{enumerate}
\end{definition}

Note that our definition of the NeuroAI Turing test is applicable to any measure of similarity used to compare a computational model with a biological system.
The choice of this measure $\mathcal{M}$ is determined by the user, but we highlight some considerations here(for more discussion see Appendix \S\ref{ss:notions-metrics}, Table~\ref{tab:metrics}).
Common choices of mapping function and metric include linear regression (ridge, PLS), and RSA; however, it is recommended to restrict oneself to the ``sharpest'' transform class depending on the scientific question at hand~\citep{thobani2024inter}, to be in a position to most stringently separate models.
Note that for common choices of metric $\mathcal{M}$, such as Pearson correlation, RSA, and especially any metric that satisfies transitive closure~\citep{williams2021generalized}, one will need to correct by the square root of the product of the mapping consistency and internal consistency of the units, in order to properly approximate the true value of $\mathcal{M}$ in the limit of infinite trials.
We provide a heuristic derivation of why this correction needs to be encoded in $\mathcal{M}$ in Appendix \S\ref{sec:methods-interanimal}.
Finally, in the case of regressions, which return a per-unit consistency value, if we want $\mathcal{M}$ to be a real number per subject, we recommend taking the median across the population to avoid sensitivity to outliers, following \citet{yamins2014performance} and many other subsequent works.
One can also plot these consistencies per unit if they wish, rather than collapsing to a single number.

While this paper centers on our proposed definition of the NeuroAI Turing Test, models that explicitly posit a mechanism of variability could be held to a higher standard: the variability among model instances should mirror that of real animals (see Appendix \S\ref{ss:notions-variability}). This type of variability may be especially important when studying higher-order cognitive functions, where subjects often take different \emph{strategies}. For this work, however, we focus on testing models based on the convergence of distribution between $\Delta_{\text{model}}$ and $\Delta_{\text{organism}}$. Even in situations where there is a large amount of inter-subject variability, one can learn about commonalities in the representations by focusing on metrics that identify common features \emph{across} different individuals (see~\citet{nayebi2021explaining} for an example of this approach).

\section{Current State of the NeuroAI Turing Test}
\label{sec:current-state}
With the formalized  NeuroAI Turing test, we detail how current work on behavioral and representational convergence fits into this framing. 

\textbf{Behavioral alignment alone is not sufficient for representational convergence between models and brains.}
Let's consider a model that passes the behavioral Turing Test. Would this model also pass a representational level test for brains? 
Some researchers have argued that several high-capacity neural networks are converging to universal ``platonic representations''  \citep{huhposition}. This idea is closely related to notions of efficient coding \cite{barlow1961possible, simoncelli2001natural} in neuroscience. But note converging to the \textit{optimal} code still critically depends on strong implementational constraints. And the constraints in biology are quite different than those in artificial systems. 
Some work has suggested that including biological constraints in the architecture or training environment of models leads to better representational alignment \cite{dapello2020simulating, dapello2021neural,anonymous2025toponets}. It remains an open question whether (and which) of these biological constraints critically shape the representation, such that a high-capacity system without such constraints would yield a different optimal solution. Thus, we argue that testing behavior alone is not \textit{sufficient}, and we need models that also achieve representational convergence with the brain. 

\begin{figure*}[htb]
    \centering
    \includegraphics[width=\linewidth]{figures/NeuroAITuringTestStateOfTheField_Results_v3.pdf}
    \vspace{-20pt}
    \caption{Examples of previous studies using behavioral and representational similarity tests. Artificial models have reached biological behavior and representational similarity in some datasets but not others.}
    \label{fig:previous_examples}
    \vspace{-15pt}
\end{figure*}

\textbf{Behavioral alignment is necessary for a complete model of the brain to pass the NeuroAI Turing test.}
Is passing a behavioral test \textit{necessary} for passing our NeuroAI Turing test? 
If the goal is to model the entire organism, passing a behavioral test is necessary. 
Take a simple deterministic example where $f(x)$ is an internal representation and $g(x)$ outputs behavior: if
$g(f(x)) \ne g'(f'(x))$ then we know that $g'\ne g$ and/or $f' \ne f$. 
Models in recent years have made immense progress at solving complex behavioral tasks, including recent work showing increasing convergence in domains that previously had large gaps~\citep{geirhos2021partial} (Fig.~\ref{fig:previous_examples}A shows one metric from this work). 
However, even in well-studied domains such as vision, these models are ``not yet adequate computational models of human core object recognition behavior'' \citep{wichmann2023deep}.
There are many instances of models failing to reach human-like performance on behavioral measures of brain-model alignment~\citep{feather2019metamers,feather2023model, hermann2020origins,baker2022deep,bowers2023deep}.
The gap is especially pronounced for complex dynamic stimuli, such as future prediction \citep[Fig 2A]{nayebi2023neural} (key result reproduced in Fig.~\ref{fig:previous_examples}B).

We emphasize that these proposed behavioral measures (and non-correlational controlled experiments) are compatible with our NeuroAI Turing Test, as they amount to a choice of metric $\mathcal{M}$.
However, our test goes beyond merely specifying a metric---it establishes a well-defined inter-subject behavioral consistency ceiling against which models can be evaluated. 
Without such a measure, one risks drawing incorrect conclusions, such as the claim that model-IT predictivity is disproportionately driven by background-processing~\citep{malhotra2024predicting}.
This misunderstanding arises from conflating ``core object recognition'' with category-only processing, despite well-established evidence that IT itself encodes category-orthogonal features~\citep{Hong2016}. 
It is therefore critical to consider what is already accounted for in the neural data relative to models; failing to do so in this case overlooks background-related processing in IT, leading to misleading model comparisons.

Further, models achieving ``super-human'' performance on tasks are generally not considered brain-like, as their behavior could easily be distinguished from humans by looking at these abilities (for instance, in \citet{zhang2022human} the authors state that ``advanced scientific topics'' can serve as a way to discern human and AI agents, as the AI does not find the specialized jargon difficult to discuss). With the lack of clear behavioral alignment between brains and machines, we can conclude that we have yet to achieve a foundation model of the whole brain that would pass our NeuroAI Turing Test. 

\textbf{NeuroAI Turing test for individual brain modules.} Currently in neuroscience, most researchers focus on building models for a subset of neural responses (for instance, one particular visual area), rather than the entirety of the brain. In these cases, even if a model's behavior is not aligned with humans, it might encode a good representation for an intermediate brain module. While our long-term goal should be for models that capture all neural components, targeting modules of the brain is one stepping stone for building a larger-scale system. In some domains and datasets, models have begun to come close to or achieve representational-level convergence with biological measurements. For instance, in common vision benchmarks from macaque inferior temporal (IT) cortex \citep{Majaj2015}, and human fMRI data on naturalistic stimuli,  neural predictivity seems to be at 90\% of the inter-animal variability ~\citep{nayebi2022, conwell2022can, ratan2021computational}. 
In rodent (mouse and rat) medial entorhinal cortex, models closely approach the inter-animal consistency measure~\citep[Fig. 2A]{nayebi2021explaining} (replotted in Fig.~\ref{fig:previous_examples}C), and in mouse visual cortex, models also achieve nearly 90\% of the inter-animal variability~\citep[Figs. 2A, S2, and S3]{nayebi2021unsupervised}. In cases where predictions on the dataset have been nearly saturated with respect to the inter-animal baseline, it is critical to examine the dataset and metrics that have been used to gain insight for future experiments. Other benchmarks show a clear gap between the best models and inter-animal consistency.  
In primate dorsomedial frontal cortex, the neural predictions from the best models are far below that obtained from inter-animal consistency \citep[Fig 3B]{nayebi2023neural} (replotted in Fig.~\ref{fig:previous_examples}D), and measures of RSA with auditory fMRI data show a clear different from a between-participant ceiling \citep[Fig 2E]{tuckute2023many}. 

\section{Alternate Views}
\label{sec:against}
In this section, we discuss alternate views to our proposed NeuroAI Turing Test.
We group the main arguments against it by subsection:
\subsection{Alternate Notions of ``Brain-Likeness''}
\label{ss:against-likeness}
The past century of neuroscience has had many definitions of what a brain ``does'', thereby providing an implicit definition for what might constitute a ``brain-like'' model (cf. Table~\ref{tab:brain_likeness} and Appendix \S\ref{sec:notions}). 
For example, the most notable of these include classic ideas of predictive coding~\citep{rao1999predictive}, sparse coding~\citep{olshausen1996emergence}, energy efficiency~\citep{laughlin2001energy}, and redundancy reduction~\citep{barlow1961possible}.

While these notions are valuable, they do not inherently define a level of convergence that researchers should aim to achieve for a model of the brain. 
Our requirement of distributional convergence, in contrast, remains agnostic to any single (and potentially subjective) principle of how brain-likeness \emph{ought} to be defined \emph{a priori}. Instead, the metric $\mathcal{M}$ encodes the user's preference for which features of brain-likeness they wish to align with, depending on their specific scientific question. Importantly, one is not restricted to a single choice of metric; each metric induces a distinct NeuroAI Turing Test, capturing a different dimension of ``brain-likeness'' to investigate. This flexibility allows our framework to encompass and generalize past notions of brain-likeness by requiring that models are distributionally similar to brains under the chosen metric.

Other work has focused explicitly on the biological plausibility of models as the gold standard for `brain-likeness'. But biological plausibility alone has not yet led to systems that capture complex behavior.
Spiking neural networks, for example, while more biologically plausible than a typical neural network architecture, often underperform in predicting neural responses or behavior. On the other hand, the constraints imposed by biologically inspired components have led to models that are closer to human behavior~\citep{saddler2021deep} or better at predicting neural responses~\citep{dapello2020simulating}.
These examples highlight that behavioral similarity, representational convergence, and biological plausibility are complementary yet distinct goals. Here, we focus on both behavior and representational convergence, as the goal of NeuroAI is typically not to \emph{emulate} the brain at every level of biological detail, but rather to understand and abstract the biological details necessary for intelligent behaviors, by communicating the evolutionary constraints of brain function through predictive model tasks and architecture classes.

\subsection{Beyond ``Brain-likeness'': Predictivity, Interpretability, and Practical relevance\label{sec:metric_interpretability}}
\textbf{Predictivity vs. Interpretability.}
There is a perceived tension between approaches that optimize for neural predictivity and approaches that seek interpretable explanations for neural phenomena.  Predictive approaches prioritize obtaining quantitatively accurate replications of neural activity, while interpretability approaches seek to uncover human-explainable insights into the causes of neural phenomena.
Handcrafting mechanistic explanations in neuroscience has always been challenging, a situation analogous to the ``bitter lesson''~\citep{sutton2019bitter} in AI. 

Interpretability, however, is an inherently subjective measure, often in the eye of the beholder. Previous approaches that prioritized \emph{particular} mathematically interpretable features consistently fell short in accurately predicting large-scale neural activity. This is very likely because neural populations are heterogeneous and inherent difficult-to-interpret and hand-engineer. Notable examples of these failures across species and brain areas include Gabors and macaque V1~\citep{cadena2019deep}, HMAX~\citep{riesenhuber1999hierarchical} and macaque V4/IT shape parameterization methods ~\citep{yamins2014performance, pasupathy2001shape, pasupathy2002population, yamane2008neural}, and grid-cell-only models and rodent MEC~\citep{nayebi2021explaining}.

Critically, the predictive modeling strategy is \textit{not} antithetical to the NeuroAI Turing Test or those aiming to uncover an interpretable understanding of neural responses.  It is  much easier to seek interpretable explanations for scientific phenomena in a task-optimized \emph{in silico} system, within which we have perfect access to every computation, than biological brains which are significantly more noisy, and limited-access (see, e.g.~\citep{mcintosh2016deep,tanaka2019deep,maheswaranathan2023interpreting, tanaka2019deep, nayebi2022goal, ratan2021computational, khosla2022highly}, for how this can be achieved in artificial neural networks (ANNs) relevant to neurobiology). In fact NeuroAI Turing Test, strongly supports attempts to extract interpretable explanations and mechanistic understanding of neurobiological phenomena. It makes this enterprise more rigorous by enforcing a critical constraint: all benchmarks, including mechanistic explanations, must be evaluated against a carefully measured inter-animal ceiling.

\textbf{Beyond Neuroscience: AI Safety.} Even beyond neuroscience, the NeuroAI Turing Test is in alignment with AI mechanistic interpretability and safety efforts that have been trying to make sense of the model internals ~\citep{olah2020zoom,nanda2023comprehensive,rager2025auditing}.
We are suggesting a step further by enforcing that what is found mechanistically should be \emph{replicable} across individuals; otherwise, it risks being a spurious, idiosyncratic observation.
This convergence not only bridges neuroscience and AI safety but also paves the way for interdisciplinary methodologies that enhance our understanding of complex systems.

\textbf{Practical applications: Brain-Computer Interfaces (BCIs).}
Accurate neural prediction is also intrinsically valuable for BCIs.
``Digital twins'' of neural responses can be manipulated to test interventions~\citep{lee2020topographic,schrimpf2021topographic,pak2023newborn,anonymous2025toponets} or produce stimuli that optimally drive biological neurons~\citep{bashivan2019neural,walker2019inception, tuckute2024driving}. These results show that once a model reliably predicts a system, it can also \emph{control} the system---a principle seen in many scientific advances. For example, advances in quantum mechanics laid the groundwork for nuclear physics, which ultimately enabled the development of nuclear reactors to control subatomic processes. For BCIs, matching the \emph{individual} may be ultimately more important than matching the population distribution, but achieving reliable alignment at a population level is a first step before tailoring person-specific solutions. If the broader distribution remains elusive, personalized alignment becomes even more difficult.

\subsection{Against Integrative Benchmarking More Generally: ``Goodharting''}
\label{ss:against-goodharting}
A common criticism of measurement and benchmarking strategies is their susceptibility to Goodhart’s law~\citep{goodhart1975problems}: ``When a measure becomes a target, it ceases to be a good measure.''
When a single benchmark becomes the primary objective, researchers may optimize for it at the expense of broader goals it was meant to capture. 
This issue is not unique to our proposed NeuroAI Turing Test but applies to integrative benchmarking more generally.

We address the more general criticisms against integrative benchmarking here, but note that the NeuroAI Turing Test already avoids this pitfall by not being tied to any \emph{one} metric. 
Instead, it is \emph{parametrized} by a choice of metric $\mathcal{M}$, with a flexible threshold $\alpha$ that allows for distributional rather than exact equivalence. 
This adaptability ensures that the NeuroAI Turing Test is not a static benchmark that can be trivially over-optimized but a framework that evolves with different contexts.

It has also been argued that some metrics might encode biases to ``pick out'' outliers in a high-dimensional space (a consequence of the ``Ugly Duckling Theorem''~\citep{watanabe1969knowing}, the basis for the ``No Free Lunch Theorem''~\citep{mitchell1980need,wolpert1997no}).
While this is theoretically possible, one should note that even using metrics alone \emph{independently} of the NeuroAI Turing Test, has not resulted in ``Goodharting''. Experimental results across domains and datasets paint a consistent picture: the model-to-brain matching metrics have not unilaterally gone up, but have instead either plateaued or even gone down~\citep{schrimpf2020integrative, conwell2022can, tuckute2023many, soni2024conclusions, linsley2023performanceoptimizeddeepneuralnetworks}.
Simple model manipulations do not lead to a perfect match~\citep{riedel2022bag}.
Rather, the largest advances in neural and behavioral predictivity have come from significant AI advances, such as ImageNet-categorization-optimized CNNs over everything before it (e.g. handcrafted SIFT features, HMAX, as in \citet{yamins2014performance}), deeper CNNs like ResNets over AlexNet~\citep{schrimpf2020integrative,Kar2021}, or the advent of performant contrastive, self-supervised learning objectives~\citep{Zhuang2021,nayebi2021unsupervised,konkle2022self, yerxacontrastive}.
In recent analyses, pretraining dataset scaling has shown to be related to improved behavioral alignment \cite{geirhos2021partial}, but this does not appear sufficient for neural alignment~\citep{gokce2024scaling}.
This indicates that current models may not be sufficient and innovations in inductive biases are likely needed for improved neural alignment. This is particularly true in domains such as 3D vision and embodied, physical intelligence~\citep{hermann2023humanlike}, where large-scale data collection is feasible but yields significantly smaller performance gains compared to language, as seen with existing Vision Transformers~\citep{bear2021physion,nayebi2023neural,bear2023unifying,vc2023}.

Furthermore, even \emph{despite} differences between regressed and non-fitted metrics, there is generally agreement across large-scale model loss function and architecture trends across species and brain areas.
More specifically, studies commonly examine \emph{multiple} metrics rather than one, which can otherwise be susceptible to ``Goodharting''. 
These include both fitted metrics such as linear regression, along with non-fitted metrics (e.g. RSA, score function distributions, simpler-than-linear mappings, etc.), which found similar conclusions that matched linear regression results, across rodent medial entorhinal cortex~\citep[Fig. 3]{nayebi2021explaining}, macaque and human visual cortex (cf. \citet{khaligh2014deep} as the first of many such demonstrations), and human auditory cortex~\citep[Fig. S10]{tuckute2023many}.

However, one could make the argument that this general agreement among metrics is meaningless, since all the metrics we currently use could pick out ``functionally orthogonal'' features of neural data. 
One commonly referenced candidate for a functionally orthogonal feature is ``effective dimensionality''.
We note that it is debated whether this should even be a candidate, since it may be necessary even in the brain to maintain high effective dimensionality to support many different intelligent behaviors.
However, let us suppose for this discussion that it was undesirable if effective dimensionality unilaterally predicted brain-model alignment on a metric---is this true for common metrics?
This has largely been shown to \emph{not} be the case for linear regression or RSA, across brain areas and species, e.g. rodent medial entorhinal cortex where networks like the ``SimpleRNN''~\citep{Elman1990,nayebi2021explaining} have high neural predictivity but low participation ratio~\citep[Fig. 6]{schaeffer2022no}; human visual cortex and RSA~\citep[Fig. 5]{conwell2022can}; with current vision models and regression~\citep[Figs. SI5.10-SI5.12]{canatar2024spectral}; and human auditory cortex~\citep[Fig. S9]{tuckute2023many}.
These empirical findings are also supported by theoretical results in regression, such as in the recent spectral theory of linear regression of \citet{canatar2024spectral}, where there is an explicit model-brain alignment term ($Y^Tv_i$) of the model's principal components ($v_i$) being aligned with the neural data ($Y$).
Note the ``Universal Approximation Theorem''~\citep{cybenko1989approximation,hornik1989multilayer,hornik1991approximation} is inapplicable here for trivial reasons. 
Factors like out-of-distribution generalization -- since pre-training images often differ from images used for neural evaluation~\citep{yamins2014performance,schrimpf2020integrative}---along with finite layer widths and sample sizes, misalign the theorem with practice.

Instead, what we see emerging from these models often mirrors known features of neural processing in visual~\citep{yamins2014performance,khaligh2014deep,cadena2019deep,nayebi2018task,Zhuang2021,nayebi2021unsupervised,nayebi2022}, auditory~\citep{kell2018task,tuckute2023many}, motor~\citep{sussillo2015neural, michaels2020goal}, entorhinal~\citep{sorscher2019unified,nayebi2021explaining}, and other brain areas such as language~\citep{schrimpf2021neural}---such as Gabors, hierarchical structure, tuning properties, sparsity, and functional responses.
That is not to say the models are perfect (this is even evident from our metrics~\citep{schrimpf2020integrative}), but this is a reflection that the standard choices for the metric $\mathcal{M}$ are not pointing us \emph{away} from the brain. 
After all, the ``bread and butter'' of science is empirical, quantitative comparison, and this approach is standardizing this consideration to the brain sciences more formally.

Finally, from an oracle point of view, independent of \emph{any} model, we expect neural responses in the brain area of one animal to overlap in their encoding with the responses in the same brain area of another animal. 
This is a critical component of our NeuroAI Turing Test, where we strive for models that, at minimum, reach this criterion. 
In fact, models evaluated on many current metrics have not yet reached this consistency level, suggesting that we have not reached a point where models ``overfit'' to all available datasets (e.g. macaque dorsomedial frontal cortex and human physics prediction~\citep[Figs. 2B and 3B]{nayebi2023neural}, human auditory cortex~\citep[Fig. 2E]{tuckute2023many}, and human language areas~\citep[Fig. SI1]{schrimpf2021neural}).

\subsection{Finite Numbers of Models and Organisms}
\label{ss:against-finite}
Another criticism of the NeuroAI Turing Test is that the sets $\Delta_{\text{model}}$ and $\Delta_{\text{organism}}$ are inherently finite.
In other words, we rely on a finite sample of models and organisms (animals or human subjects), rather than the ``true'' continuous distribution.
Therefore, this leaves room for models which are not considered in any given test.

But this issue is part of \emph{every} empirical science.
Science has always been about comparing a finite number of models against data.
As a result, in science we can never claim a model (``theory'') \emph{is} the natural phenomenon, we can only rule models out and identify the model most consistent with measured phenomena~\citep{popper1934logik,lakatos1978methodology,doerig2023neuroconnectionist}.
 We can then test the remaining models of the brain with new experimental data and diverse similarity measures.
This process is very much in line with traditional scientific progress, in that we have hypotheses that we continually evaluate against data to see what is most consistent, up to the data’s ability to differentiate said hypotheses. 
If the model matches all data it has been tested against, then it cannot be rejected as a perfect model of the system. 
This is why it is important to test as stringently as possible---the point is to have more metrics, not less (or even just one)! 

Furthermore, the finite model hypotheses we pick are highly non-random, as they are the ones that exhibit intelligent behavior, rather than strawmen models or trivial direct fits to the data, making them non-trivially normative.
The more of these non-trivial hypotheses we have, the better positioned we are to identify theoretical invariances (if there are any).
In fact, we are even seeing now that functional models converge on similar representations across modalities~\citep{conwell2022can,huhposition,hosseini2024universality}, and that models that share invariances with other models are also more likely to share invariances with humans ~\citep[Fig. 8E,F]{feather2023model}.
Thus, identifying trends for invariances to later theoretically verify is strongly consistent with our measures of distributional equivalence for the NeuroAI Turing Test, since having more samples will naturally boost its accuracy.

It is also important to note that predictive measures go well beyond classic interventional controls in traditional psychological studies that only test a small number of situation-specific conditions. 
This shift to prediction as a higher-order readout of model-brain alignment is necessary because the hypothesis space for cognitive function in real-world environments is vast, with an enormous number of independent variables to consider that are not always guaranteed to be human-understandable.

Therefore, if such a deeper theory exists that governs the ``true'' distribution of invariant model representations that can perform intelligent tasks, then we can view what we are doing here as the necessary ``empirics gathering'' stage to strongly \emph{separate} hypotheses about what such a theory could even look like, through the study of successfully brain-predictive ANNs.
Almost every science begins this way, by collecting many empirical examples of a given phenomenon before identifying unifying principles (if any) responsible for the common trends that have been observed (e.g. the steam engine preceded thermodynamics).
This is especially the case since it is currently faster to build an ANN that is task-performant and predicts a brain area, than to theorize \emph{ab initio} about it and hope it explains the brain, as prior views of brain-likeness did (cf. \S\ref{ss:against-likeness}).

\subsection{Achievability of the NeuroAI Turing Test}
 \label{ss:against-achievable}
\textbf{One concern for model-brain comparisons is that our proposed NeuroAI Turing Test may be too lenient.}
We emphasize that for a finite sample of data, the NeuroAI Turing Test is a \emph{lower bound} on the ``true'' inter-organism consistency.
For instance, the inter-organism distance may be underestimated if available data are insufficient to capture the aspects of true distributional equivalence across organisms. In this case, many models may reach or surpass the inter-organism distance. Limited samples, topographical discrepancies between individuals, and noisy measurements can all degrade the dynamic range needed to discriminate genuinely brain-like models~\citep{dapello2022aligning}.

However, these are limitations of the datasets, not the NeuroAI Turing Test. By quantifying how inter-animal consistency changes as a function of the number of neurons recorded, the number of stimuli presented, or the number of repetitions for each stimulus, researchers can derive \emph{actionable} prescriptions for future data collection. For instance, ~\citet[Fig. S6B]{nayebi2021unsupervised} reported that expanding the stimulus set can increase inter-animal consistency and lead to a stronger dataset for brain-model comparisons.
Extrapolation analyses with existing data can further guide data collection efforts. For example, in mouse visual cortex, \citet[Fig. S5]{nayebi2021unsupervised} showed that under several choices of metric $\mathcal{M}$, a log-linear extrapolation analysis revealed that as the number of units increases, the inter-animal consistency approaches 1.0 more rapidly for Neuropixels data~\citep{de2020large}, than for calcium imaging data~\citep{Siegle2021}, indicating that Neuropixels as a data collection modality results in higher inter-animal consistencies. 
If the inter-animal alignment continues to improve with more extensive sampling of brain areas or expanded recording durations, it underscores the need to gather larger datasets. Conversely, if the inter-animal consistency begins to saturate, then the observed variability between organisms reflects individual differences in the population.

Importantly, if a model passes the NeuroAI Turing Test for a brain area under idealized data conditions, this may suggest that current neural datasets in these domains are not sufficiently complex to falsify the existing best models.
Just as the standard Turing Test involves multiple rounds of questioning to probe the depth of a machine's intelligence, the NeuroAI Turing Test should involve iterative refinement of the model and its evaluation on multiple new datasets with a diverse set of stimulus conditions and metrics. 
The ``Contravariance Principle'' of neural modeling~\citep{cao2024explanatory2} is helpful in this scenario: working in more complex experimental environments may make it easier to identify correct models by reducing susceptibility to overly simplistic data.

In other cases, models may surpass the brain-brain similarity, not because the data is impoverished but because the model representations reflect an implicit ``average'' brain rather than any single observed neural pattern. This phenomenon, akin to the ``wisdom of the crowd'' effect~\citep{stroop1932judgment}, has been observed in large language models (LLMs), where model-generated annotations often align more closely with humans than other individual human judgments~\citep{dillion2023can, trott2024large}. Similarly, in NeuroAI, models trained on diverse datasets may develop representations that reflect population-level patterns rather than idiosyncratic inter-organism variability. 
This could be particularly relevant for higher-level cognitive tasks, where individual brains may exhibit substantial variability (see Appendix \S\ref{ss:notions-variability} for discussion on an alternative test of models that includes such variability), but a model trained across many examples may develop representations that align with the central tendencies of human cognition and surpass the brain-brain similarity value.  

\textbf{Another concern for the NeuroAI Turing test is that it is impossibly stringent.} Under-achievability arises when all models fall short of inter-animal consistency---a situation that can occur in domains where the underlying neural or behavioral data are inherently complex. 
However, just as AI is making progress on the classic behavioral Turing Test, we believe our representational benchmark for model-brain similarity is also achievable. Further, representational convergence between computational models and brains has been achieved to a large extent in datasets and domains like the first 200 ms of macaque visual cortex~\citep{Majaj2015}, mouse visual cortex~\citep{nayebi2021unsupervised}, and rodent medial entorhinal cortex~\citep{nayebi2021explaining}. While it remains unattained in domains like audition~\citep{tuckute2023many}, language~\citep[Fig. SI12]{kauf2024lexical}, and physical understanding~\citep{nayebi2023neural,bear2021physion}, the rapid progress of modeling efforts suggests that models may reach representational convergence with the brain for these domains and datasets in the near future. 
Additionally, identifying challenging domains is critical.
Studying higher-level cognition can be difficult due to varying subject strategies, but rather than abandoning inter-subject comparisons, we advocate for metrics that extract shared features across trajectories, as in \citet{nayebi2021explaining}, where spatial averaging formed generalizable ``rate maps''.
Systematically tracking the gap between top models and inter-animal consistency reveals ``frontier'' datasets where models lag behind biological benchmarks.
These datasets offer fertile ground for targeted modeling and experimental improvements (see Appendix \S\ref{ss:notions-gap}), yielding more generalizable brain-like models.

\section{Conclusion}
In summary, the NeuroAI Turing Test provides a much-needed standard to bridge AI and neuroscience. 
Without it, AI systems may achieve high performance yet remain detached from any principled scientific notion of intelligence. 
Rather than treating the brain as an optimal blueprint, this framework positions it as a natural reference point---our only universally agreed-upon example of intelligence---while allowing for artificial systems to surpass it.
We call on the scientific community to adopt a rigorous common standard for evaluating models of intelligence, moving beyond loosely defined notions of biological similarity.

\section*{Acknowledgments}
A.N. thanks Leila Wehbe for helpful discussions, and the Burroughs Wellcome Fund CASI award for funding. N.A.R.M. is supported by the NIH Pathway to Independence Award (R00EY032603) from the National Eye Institute. J.F. is supported by the Flatiron Institute, a division of the Simons Foundation.

\section*{Impact Statement}
This work seeks to define a rigorous ``Turing Test'' not just for NeuroAI, but for the broader science and engineering of intelligence---establishing a systematic framework for evaluating models of intelligence in artificial and biological systems alike.
Rather than solely focusing on whether artificial models capture biological brain function, this framework provides a principled benchmark for assessing representational and behavioral alignment, helping to ground intelligence research across neuroscience, cognitive science, and AI.

By formalizing a standard for evaluating models, this work has the potential to improve the scientific rigor of computational neuroscience, advance cognitive science by clarifying functional and behavioral constraints on intelligence, and provide AI with a clearer benchmark for what constitutes a generalizable model of intelligence.
More broadly, it contributes to the growing effort to unify these disciplines into a predictive and functional science of intelligence---one that seeks \emph{algorithmic} principles abstracted from biological implementations and runnable on machines.

The societal impact of this research includes potential applications in medicine, brain-computer interfaces (BCIs), and neurotechnology, which we briefly discuss in this article. 
A clearer framework for modeling intelligence may aid in understanding neural computations relevant to neurological disorders or cognitive interventions. 
However, as with all work at the intersection of neuroscience and AI, ethical considerations arise regarding data privacy, informed consent in neural recording, and the potential misuse of predictive models of brain activity.

Ultimately, this work is conceptual and methodological, with no immediate ethical risks beyond those already inherent to advancing AI and neuroscience. 
Nonetheless, we emphasize the importance of continued discussion on the responsible development and application of intelligence research, especially the responsible use of AI in understanding brain function.

\textbf{We encourage a dialogue: Please feel free to contact us if you have any questions or suggestions!}

\bibliography{references}
\bibliographystyle{icml2025}

% APPENDIX
\newpage
\appendix
\onecolumn
\input{appendix}

\end{document}