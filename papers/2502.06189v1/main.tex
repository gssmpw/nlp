% CVPR 2025 Paper Template; see https://github.com/cvpr-org/author-kit

\documentclass[10pt,twocolumn,letterpaper]{article}

%%%%%%%%% PAPER TYPE  - PLEASE UPDATE FOR FINAL VERSION
% \usepackage{cvpr}              % To produce the CAMERA-READY version
% \usepackage[review]{cvpr}      % To produce the REVIEW version
\usepackage[11]{cvpr} % To force page numbers, e.g. for an arXiv version
% \usepackage{indentfirst}
\setlength{\parindent}{1em}
\usepackage{bm}

% Import additional packages in the preamble file, before hyperref
\usepackage [utf8]{inputenc}
\input{preamble}
% It is strongly recommended to use hyperref, especially for the review version.
% hyperref with option pagebackref eases the reviewers' job.
% Please disable hyperref *only* if you encounter grave issues, 
% e.g. with the file validation for the camera-ready version.
%
% If you comment hyperref and then uncomment it, you should delete *.aux before re-running LaTeX.
% (Or just hit 'q' on the first LaTeX run, let it finish, and you should be clear).
\definecolor{cvprblue}{rgb}{0.21,0.49,0.74}
\usepackage[pagebackref,breaklinks,colorlinks,allcolors=cvprblue]{hyperref}

%%%%%%%%% PAPER ID  - PLEASE UPDATE
\def\paperID{2977} % *** Enter the Paper ID here
\def\confName{CVPR}
\def\confYear{2025}
% \newcommand{\yep}{\textcolor{red}} 
% \newcommand{\lwh}{\textcolor{magenta}}
% \newcommand{\yyx}{\textcolor{blue}}

%%%%%%%%% TITLE - PLEASE UPDATE
\title{Multi-Level Decoupled Relational Distillation for Heterogeneous Architectures}

%%%%%%%%% AUTHORS - PLEASE UPDATE
\author{Yaoxin Yang\\
% Fudan University\\
% {\tt\small yxyang24@m.fudan.edu.cn}
% For a paper whose authors are all at the same institution,
% omit the following lines up until the closing ``}''.
% Additional authors and addresses can be added with ``\and'',
% just like the second author.
% To save space, use either the email address or home page, not both
\and
Peng Ye\\
\and 
Weihao Lin\\
\and
Kangcong Li\\
\and
Yan Wen\\
\and
Jia Hao\\
\and
Tao Chen\\
% Fudan University\\
% {\tt\small secondauthor@i2.org}
}
\author{Yaoxin Yang$^1$ \quad
Peng Ye$^1$ \quad
Weihao Lin$^1$ \quad
Kangcong Li$^1$ \quad
Yan Wen$^1$ \quad
\\
Jia Hao$^1$ \quad
Tao Chen$^1$\footnotemark[2]\\
$^1$School of Information Science and Technology, Fudan University\\
{\tt\small yxyang24@m.fudan.edu.cn, eetchen@fudan.edu.cn}
}
\begin{document}
\maketitle
\renewcommand{\thefootnote}{\fnsymbol{footnote}}
\footnotetext[2]{Corresponding authors.}
\vspace{-5mm}
\input{sec/0_abstract}    
\input{sec/1_intro}
\input{sec/2_related_work}
\input{sec/3_methodology}
\input{sec/4_experiments}
\input{sec/5_conclution}
% \input{sec/6_acknowledgements}


% {
%     \small
%     \bibliographystyle{unsrt}
%     \bibliography{main}
% }

% WARNING: do not forget to delete the supplementary pages from your submission 
% \input{sec/X_suppl}
% \input{sec/7_suppl}



% {  
%     \small
%     \bibliographystyle{unsrt}
%     \bibliography{main}
% }

\begin{thebibliography}{10}

    \bibitem{hinton2015distilling}
    Geoffrey Hinton.
    \newblock Distilling the knowledge in a neural network.
    \newblock {\em arXiv preprint arXiv:1503.02531}, 2015.
    
    \bibitem{tang2022patchslimmingefficientvision}
    Yehui Tang, Kai Han, Yunhe Wang, Chang Xu, Jianyuan Guo, Chao Xu, and Dacheng Tao.
    \newblock Patch slimming for efficient vision transformers, 2022.
    
    \bibitem{zhang2019your}
    Linfeng Zhang, Jiebo Song, Anni Gao, Jingwei Chen, Chenglong Bao, and Kaisheng Ma.
    \newblock Be your own teacher: Improve the performance of convolutional neural networks via self distillation.
    \newblock In {\em Proceedings of the IEEE/CVF international conference on computer vision}, pages 3713--3722, 2019.
    
    \bibitem{phuong2019distillation}
    Mary Phuong and Christoph~H Lampert.
    \newblock Distillation-based training for multi-exit architectures.
    \newblock In {\em Proceedings of the IEEE/CVF international conference on computer vision}, pages 1355--1364, 2019.
    
    \bibitem{luan2019msd}
    Yunteng Luan, Hanyu Zhao, Zhi Yang, and Yafei Dai.
    \newblock Msd: Multi-self-distillation learning via multi-classifiers within deep neural networks.
    \newblock {\em arXiv preprint arXiv:1911.09418}, 2019.
    
    \bibitem{zhu2018knowledge}
    Xiatian Zhu, Shaogang Gong, et~al.
    \newblock Knowledge distillation by on-the-fly native ensemble.
    \newblock {\em Advances in neural information processing systems}, 31, 2018.
    
    \bibitem{gu2023mamba}
    Albert Gu and Tri Dao.
    \newblock Mamba: Linear-time sequence modeling with selective state spaces.
    \newblock {\em arXiv preprint arXiv:2312.00752}, 2023.
    
    \bibitem{he2022masked}
    Kaiming He, Xinlei Chen, Saining Xie, Yanghao Li, Piotr Doll{\'a}r, and Ross Girshick.
    \newblock Masked autoencoders are scalable vision learners.
    \newblock In {\em Proceedings of the IEEE/CVF conference on computer vision and pattern recognition}, pages 16000--16009, 2022.
    
    \bibitem{xie2022simmim}
    Zhenda Xie, Zheng Zhang, Yue Cao, Yutong Lin, Jianmin Bao, Zhuliang Yao, Qi~Dai, and Han Hu.
    \newblock Simmim: A simple framework for masked image modeling.
    \newblock In {\em Proceedings of the IEEE/CVF conference on computer vision and pattern recognition}, pages 9653--9663, 2022.
    
    \bibitem{beit}
    Hangbo Bao, Li~Dong, Songhao Piao, and Furu Wei.
    \newblock {BEiT}: {BERT} pre-training of image transformers.
    \newblock In {\em International Conference on Learning Representations}, 2022.
    
    \bibitem{vim}
    Lianghui Zhu, Bencheng Liao, Qian Zhang, Xinlong Wang, Wenyu Liu, and Xinggang Wang.
    \newblock Vision mamba: Efficient visual representation learning with bidirectional state space model.
    \newblock In {\em Forty-first International Conference on Machine Learning}, 2024.
    
    \bibitem{he2016deep}
    Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.
    \newblock Deep residual learning for image recognition.
    \newblock In {\em Proceedings of the IEEE conference on computer vision and pattern recognition}, pages 770--778, 2016.
    
    \bibitem{yu2024unleashing}
    Rui Yu, Runkai Zhao, Jiagen Li, Qingsong Zhao, Songhao Zhu, HuaiCheng Yan, and Meng Wang.
    \newblock Unleashing the potential of mamba: Boosting a lidar 3d sparse detector by using cross-model knowledge distillation.
    \newblock {\em arXiv preprint arXiv:2409.11018}, 2024.
    
    \bibitem{Liu2024DDKDD}
    Jiaheng Liu, Chenchen Zhang, Jinyang Guo, Yuanxing Zhang, Haoran Que, Ken Deng, Zhiqi Bai, Jie Liu, Ge~Zhang, Jiakai Wang, Yanan Wu, Congnan Liu, Wenbo Su, Jiamang Wang, Lin Qu, and Bo~Zheng.
    \newblock Ddk: Distilling domain knowledge for efficient large language models.
    \newblock {\em ArXiv}, abs/2407.16154, 2024.
    
    \bibitem{wang2024the}
    Junxiong Wang, Daniele Paliotta, Avner May, Alexander~M Rush, and Tri Dao.
    \newblock The mamba in the llama: Distilling and accelerating hybrid models.
    \newblock In {\em The Thirty-eighth Annual Conference on Neural Information Processing Systems}, 2024.
    
    \bibitem{touvron2022deit}
    Hugo Touvron, Matthieu Cord, and Herv{\'e} J{\'e}gou.
    \newblock Deit iii: Revenge of the vit.
    \newblock In {\em European conference on computer vision}, pages 516--533. Springer, 2022.
    
    \bibitem{wang2024repvit}
    Ao~Wang, Hui Chen, Zijia Lin, Jungong Han, and Guiguang Ding.
    \newblock Repvit: Revisiting mobile cnn from vit perspective.
    \newblock In {\em Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition}, pages 15909--15920, 2024.
    
    \bibitem{hao2024one}
    Zhiwei Hao, Jianyuan Guo, Kai Han, Yehui Tang, Han Hu, Yunhe Wang, and Chang Xu.
    \newblock One-for-all: Bridge the gap between heterogeneous architectures in knowledge distillation.
    \newblock {\em Advances in Neural Information Processing Systems}, 36, 2024.
    
    \bibitem{peng2019correlation}
    Baoyun Peng, Xiao Jin, Jiaheng Liu, Dongsheng Li, Yichao Wu, Yu~Liu, Shunfeng Zhou, and Zhaoning Zhang.
    \newblock Correlation congruence for knowledge distillation.
    \newblock In {\em Proceedings of the IEEE/CVF International Conference on Computer Vision}, pages 5007--5016, 2019.
    
    \bibitem{Heo_2019_ICCV}
    Byeongho Heo, Jeesoo Kim, Sangdoo Yun, Hyojin Park, Nojun Kwak, and Jin~Young Choi.
    \newblock A comprehensive overhaul of feature distillation.
    \newblock In {\em Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)}, October 2019.
    
    \bibitem{zagoruyko2022paying}
    Sergey Zagoruyko and Nikos Komodakis.
    \newblock Paying more attention to attention: Improving the performance of convolutional neural networks via attention transfer.
    \newblock In {\em International Conference on Learning Representations}, 2022.
    
    \bibitem{romero2014fitnets}
    Adriana Romero, Nicolas Ballas, Samira~Ebrahimi Kahou, Antoine Chassang, Carlo Gatta, and Yoshua Bengio.
    \newblock Fitnets: Hints for thin deep nets.
    \newblock {\em arXiv preprint arXiv:1412.6550}, 2014.
    
    \bibitem{Heo2018KnowledgeTV}
    Byeongho Heo, Minsik Lee, Sangdoo Yun, and Jin~Young Choi.
    \newblock Knowledge transfer via distillation of activation boundaries formed by hidden neurons.
    \newblock In {\em AAAI Conference on Artificial Intelligence}, 2018.
    
    \bibitem{Yim2017AGF}
    Junho Yim, Donggyu Joo, Ji‚ÄêHoon Bae, and Junmo Kim.
    \newblock A gift from knowledge distillation: Fast optimization, network minimization and transfer learning.
    \newblock {\em 2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)}, pages 7130--7138, 2017.
    
    \bibitem{Ahn2019VariationalID}
    Sungsoo Ahn, Shell~Xu Hu, Andreas~C. Damianou, Neil~D. Lawrence, and Zhenwen Dai.
    \newblock Variational information distillation for knowledge transfer.
    \newblock {\em 2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)}, pages 9155--9163, 2019.
    
    \bibitem{Chen2022KnowledgeDW}
    Defang Chen, Jianhan Mei, Hailin Zhang, C.~Wang, Yan Feng, and Chun Chen.
    \newblock Knowledge distillation with the reused teacher classifier.
    \newblock {\em 2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)}, pages 11923--11932, 2022.
    
    \bibitem{Chen2021DistillingKV}
    Pengguang Chen, Shu Liu, Hengshuang Zhao, and Jiaya Jia.
    \newblock Distilling knowledge via knowledge review.
    \newblock {\em 2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)}, pages 5006--5015, 2021.
    
    \bibitem{guo2023class}
    Ziyao Guo, Haonan Yan, Hui Li, and Xiaodong Lin.
    \newblock Class attention transfer based knowledge distillation.
    \newblock In {\em Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition}, pages 11868--11877, 2023.
    
    \bibitem{Li2021OnlineKD}
    Zheng Li, Jingwen Ye, Mingli Song, Ying Huang, and Zhigeng Pan.
    \newblock Online knowledge distillation for efficient pose estimation.
    \newblock {\em 2021 IEEE/CVF International Conference on Computer Vision (ICCV)}, pages 11720--11730, 2021.
    
    \bibitem{Lin2022KnowledgeDV}
    Sihao Lin, Hongwei Xie, Bing Wang, Kaicheng Yu, Xiaojun Chang, Xiaodan Liang, and G.~Wang.
    \newblock Knowledge distillation via the target-aware transformer.
    \newblock {\em 2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)}, pages 10905--10914, 2022.
    
    \bibitem{park2019relational}
    Wonpyo Park, Dongju Kim, Yan Lu, and Minsu Cho.
    \newblock Relational knowledge distillation.
    \newblock In {\em Proceedings of the IEEE/CVF conference on computer vision and pattern recognition}, pages 3967--3976, 2019.
    
    \bibitem{tian2019contrastive}
    Yonglong Tian, Dilip Krishnan, and Phillip Isola.
    \newblock Contrastive representation distillation.
    \newblock {\em arXiv preprint arXiv:1910.10699}, 2019.
    
    \bibitem{yang2022masked}
    Zhendong Yang, Zhe Li, Mingqi Shao, Dachuan Shi, Zehuan Yuan, and Chun Yuan.
    \newblock Masked generative distillation.
    \newblock In {\em European Conference on Computer Vision}, pages 53--69. Springer, 2022.
    
    \bibitem{yang2022focal}
    Zhendong Yang, Zhe Li, Xiaohu Jiang, Yuan Gong, Zehuan Yuan, Danpei Zhao, and Chun Yuan.
    \newblock Focal and global knowledge distillation for detectors.
    \newblock In {\em Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition}, pages 4643--4652, 2022.
    
    \bibitem{zhao2022decoupled}
    Borui Zhao, Quan Cui, Renjie Song, Yiyu Qiu, and Jiajun Liang.
    \newblock Decoupled knowledge distillation.
    \newblock In {\em Proceedings of the IEEE/CVF Conference on computer vision and pattern recognition}, pages 11953--11962, 2022.
    
    \bibitem{sun2024logit}
    Shangquan Sun, Wenqi Ren, Jingzhi Li, Rui Wang, and Xiaochun Cao.
    \newblock Logit standardization in knowledge distillation.
    \newblock In {\em Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition}, pages 15731--15740, 2024.
    
    \bibitem{huang2022knowledge}
    Tao Huang, Shan You, Fei Wang, Chen Qian, and Chang Xu.
    \newblock Knowledge distillation from a stronger teacher.
    \newblock {\em Advances in Neural Information Processing Systems}, 35:33716--33727, 2022.
    
    \bibitem{yang2022crossimagerelationalknowledgedistillation}
    Chuanguang Yang, Helong Zhou, Zhulin An, Xue Jiang, Yongjun Xu, and Qian Zhang.
    \newblock Cross-image relational knowledge distillation for semantic segmentation, 2022.
    
    \bibitem{guo2023linklesslinkpredictionrelational}
    Zhichun Guo, William Shiao, Shichang Zhang, Yozen Liu, Nitesh~V. Chawla, Neil Shah, and Tong Zhao.
    \newblock Linkless link prediction via relational distillation, 2023.
    
    \bibitem{liu2022cross}
    Yufan Liu, Jiajiong Cao, Bing Li, Weiming Hu, Jingting Ding, and Liang Li.
    \newblock Cross-architecture knowledge distillation.
    \newblock In {\em Proceedings of the Asian conference on computer vision}, pages 3396--3411, 2022.
    
    \bibitem{zhao2023cross}
    Weisong Zhao, Xiangyu Zhu, Zhixiang He, Xiao-Yu Zhang, and Zhen Lei.
    \newblock Cross-architecture distillation for face recognition.
    \newblock In {\em Proceedings of the 31st ACM International Conference on Multimedia}, pages 8076--8085, 2023.
    
    \bibitem{krizhevsky2009learning}
    Alex Krizhevsky, Geoffrey Hinton, et~al.
    \newblock Learning multiple layers of features from tiny images.
    \newblock 2009.
    
    \bibitem{le2015tiny}
    Ya~Le and Xuan Yang.
    \newblock Tiny imagenet visual recognition challenge.
    \newblock {\em CS 231N}, 7(7):3, 2015.
    
    \bibitem{howard2018inverted}
    Andrew Howard, Andrey Zhmoginov, Liang-Chieh Chen, Mark Sandler, and Menglong Zhu.
    \newblock Inverted residuals and linear bottlenecks: Mobile networks for classification, detection and segmentation.
    \newblock In {\em Proceedings of the IEEE conference on computer vision and pattern recognition}, pages 4510--4520, 2018.
    
    \bibitem{liu2021swin}
    Ze~Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin, and Baining Guo.
    \newblock Swin transformer: Hierarchical vision transformer using shifted windows.
    \newblock In {\em Proceedings of the IEEE/CVF international conference on computer vision}, pages 10012--10022, 2021.
    
    \bibitem{touvron2021training}
    Hugo Touvron, Matthieu Cord, Matthijs Douze, Francisco Massa, Alexandre Sablayrolles, and Herv{\'e} J{\'e}gou.
    \newblock Training data-efficient image transformers \& distillation through attention.
    \newblock In {\em International conference on machine learning}, pages 10347--10357. PMLR, 2021.
    
    \bibitem{yuan2021tokens}
    Li~Yuan, Yunpeng Chen, Tao Wang, Weihao Yu, Yujun Shi, Zi-Hang Jiang, Francis~EH Tay, Jiashi Feng, and Shuicheng Yan.
    \newblock Tokens-to-token vit: Training vision transformers from scratch on imagenet.
    \newblock In {\em Proceedings of the IEEE/CVF international conference on computer vision}, pages 558--567, 2021.
    
    \bibitem{touvron2022resmlp}
    Hugo Touvron, Piotr Bojanowski, Mathilde Caron, Matthieu Cord, Alaaeldin El-Nouby, Edouard Grave, Gautier Izacard, Armand Joulin, Gabriel Synnaeve, Jakob Verbeek, et~al.
    \newblock Resmlp: Feedforward networks for image classification with data-efficient training.
    \newblock {\em IEEE transactions on pattern analysis and machine intelligence}, 45(4):5314--5321, 2022.
    
    \bibitem{dosovitskiy2020image}
    Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et~al.
    \newblock An image is worth 16x16 words: Transformers for image recognition at scale.
    \newblock In {\em International Conference on Learning Representations}, 2020.
    
    \bibitem{tolstikhin2021mlp}
    Ilya~O Tolstikhin, Neil Houlsby, Alexander Kolesnikov, Lucas Beyer, Xiaohua Zhai, Thomas Unterthiner, Jessica Yung, Andreas Steiner, Daniel Keysers, Jakob Uszkoreit, et~al.
    \newblock Mlp-mixer: An all-mlp architecture for vision.
    \newblock {\em Advances in neural information processing systems}, 34:24261--24272, 2021.
    
    \bibitem{liu2022convnet}
    Zhuang Liu, Hanzi Mao, Chao-Yuan Wu, Christoph Feichtenhofer, Trevor Darrell, and Saining Xie.
    \newblock A convnet for the 2020s.
    \newblock In {\em Proceedings of the IEEE/CVF conference on computer vision and pattern recognition}, pages 11976--11986, 2022.
    
    \bibitem{loshchilovdecoupled}
    Ilya Loshchilov and Frank Hutter.
    \newblock Decoupled weight decay regularization.
    \newblock In {\em International Conference on Learning Representations}, 2017.
    
    \end{thebibliography}
    

\end{document}

