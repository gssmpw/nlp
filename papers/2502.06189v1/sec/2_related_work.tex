\section{Related work}
\label{sec:related_work}

\textbf{Homogeneous Distillation}
Hinton \textit{et al.}~\cite{hinton2015distilling} firstly introduces knowledge distillation to transfer a teacher’s knowledge to a student by minimizing their Kullback-Leibler divergence. Following works can be mainly categorized into two pipelines: Feature-based KD and Logits-based KD. To enhance representational capacity, Feature-based KD methods~\cite{romero2014fitnets,park2019relational} distill knowledge from both intermediate layers and logit outputs. Subsequent works explore various perspectives: CRD~\cite{tian2019contrastive} emphasizes the structural knowledge of the teacher, while CC~\cite{peng2019correlation} identifies instance-level congruent constraints, transferring both instance-level information and inter-instance correlation. Further advancements~\cite{guo2023class,yang2022masked,yang2022focal} refine this process with class activation mapping, feature masking, and focal techniques for object detection. Logits-based KD enhances student models by transferring softened targets from teacher models~\cite{hinton2015distilling,zhao2022decoupled}.~\cite{sun2024logit} introduces a Z-score logit standardization method to better capture inter-logit relations to conquer the shared-temperature constraint. 

However, in logits-based KD simply using KL divergence is insufficient for exact matching. To tackle the issue,~\cite{huang2022knowledge} proposes a relation-based loss to preserve inter-class relationships.~\cite{yang2022crossimagerelationalknowledgedistillation} proposes a novel Cross-Image Relational KD (CIRKD), which focuses on transferring structured pixel-to-pixel and pixel-to-region relations among the whole images.~\cite{guo2023linklesslinkpredictionrelational} proposes a relational KD framework, Linkless Link Prediction (LLP), to distill knowledge for link prediction with MLPs. These methods seem to solve the problem that dark knowledge is not well transferred in heterogeneous distillation.
Nonetheless, these relational distillation methods 
% of relational distillation 
smooth the logit too much, leading to a reduction in the confidence in the correct category. Moreover, we show that directly transferring conventional relational KD to the heterogeneous distillation setting proves ineffective. Thus, it is a significant necessity to investigate the effective application of relational distillation methods in heterogeneous distillation. 

\textbf{Heterogeneous Distillation}
Heterogeneous distillation allows efficient models to inherit rich representations from powerful teacher models of different architectures, enhancing student model performance and generalization across architectural boundaries. Liu \textit{et al.}~\cite{liu2022cross} pioneers heterogeneous knowledge distillation by aligning the output, attention, and feature spaces of heterogeneous models, assuming identical pixel-level spatial information. To overcome the limitations of this assumption,~\cite{zhao2023cross} addresses the architecture gap in cross-architecture distillation by synchronizing the pixel-wise receptive fields of teacher and student networks. However, these methods overlook spatial differences and global context, which FASD~\cite{yu2024unleashing} addresses by aligning heterogeneous features and logit mappings between Transformer and Mamba models. However, these approaches do not directly scale to all heterogeneous architectures.
Furthermore, OFA-KD~\cite{hao2024one} explores the feasibility of distilling between multiple architectures. They identified two key limitations in existing methods: lack of latent space alignment, causing inconsistencies in heterogeneous distillation, and absence of adaptive target enhancement, weakening focused knowledge transfer. OFA-KD introduces latent space alignment to eliminate architecture-specific information and adaptive target enhancement to sharpen knowledge transfer, achieving notable gains across diverse models.
In this paper, we find that dark knowledge is severely corrupted as OFA-KD changes the distribution of the output logit of the teacher model, which limits the performance of heterogeneous distillation. 
Therefore, we design a novel heterogeneous relational KD framework called MLDR-KD, which can retain redundant dark knowledge while enhancing confidence in the correct target.
% attempt to retain dark knowledge by incorporating relational KD, while enhancing confidence in the correct target.
% By designing a novel heterogeneous KD framework MLDR-KD, this problem is solved. 


\begin{figure*}[t]
  \centering
  \includegraphics[width=0.96\linewidth]{sec/figures/framework.png}
  \vspace{-3mm}
  \caption{Overview of the proposed MLDR-KD framework. It comprises two main components:  Decoupled Finegrained Relation Alignment (DFRA), and Multi-Scale Dynamic Fusion (MSDF). In DFRA, after obtaining the logits of teacher and student, we decouple them into class-wise relation and sample-wise relation, and then align these relationships via Kullback-Leibler divergence. DFRA is applied to both logit and feature levels. MSDF further improves the effect of feature-level DFRA by dynamically fusing feature maps of student.
  % By dynamically fusing feature maps of student, MSDF further improves the effect of DFRA.
  }
  \vspace{-5mm}
  \label{fig:framework}
\end{figure*}


%Nonetheless, all existing methods tend to overlook the architecture-specific differences in features and logits between models, which significantly limits the ability to transfer fine-grained knowledge. This limitation arises from the fact that these methods fail to account for the unique characteristics of different model architectures, such as varying feature representations, logits, and intermediate processing stages. In contrast, by employing Decoupled Fine-Grained Relation Alignment (DFRA), we are able to decouple logits into multiple fine-grained relationships at different levels of abstraction. This decoupling is applied across various categories and images, ensuring that the alignment process is sensitive to the differences between the predictions of the teacher and student models. This ability to identify and amplify discrepancies helps the model focus on areas where the student model needs improvement, thereby improving its learning efficiency. Furthermore, our approach incorporates the Multi-Scale Dynamic Fusion (MSDF) module, which enhances the feature-level distillation process by dynamically fusing multi-scale feature maps from different stages of the student model. This fusion enables more effective information exchange between the student and teacher models, as it captures different levels of abstraction across the model’s layers. The gated network used for dynamic fusion ensures that the most relevant features are selected and combined, allowing the student model to learn from both fine-grained and high-level information. Extensive experiments and evaluations demonstrate that MLDR-KD consistently achieves state-of-the-art results, outperforming other distillation techniques. By incorporating architecture-specific features into the distillation process, MLDR-KD successfully bridges the gap between teacher and student models of different architectures, achieving a more comprehensive and effective knowledge transfer.

