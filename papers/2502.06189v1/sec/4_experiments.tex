% Please add the following required packages to your document preamble:
% \usepackage{booktabs}
% \usepackage{graphicx}
% \usepackage[table,xcdraw]{xcolor}
% \usepackage{pifont}
% Beamer presentation requires \usepackage{colortbl} instead of \usepackage[table,xcdraw]{xcolor}


% Please add the following required packages to your document preamble:
% \usepackage{booktabs}
% \usepackage{graphicx}
% \usepackage[table,xcdraw]{xcolor}
% Beamer presentation requires \usepackage{colortbl} instead of \usepackage[table,xcdraw]{xcolor}
\begin{table*}[t]
\centering
\caption{Results on Tiny-ImageNet dataset. The best results are indicated in bold. We conducted all of the additional experiments in baseline. CNN-based experiments are through 100 epochs training. Other experiments are through 300 epochs training. }
\vspace{-2mm}
\label{tab:tinyimg}
\resizebox{1.4\columnwidth}{!}{%
\fontsize{7pt}{7}\selectfont
\begin{tabular}{@{}cccccccc@{}}
\toprule
Student Model        & From Scratch & Teacher Model                & From Scratch                 & KD                      & OFAKD                        & \textbf{MLDRKD}                       & {\color[HTML]{FF0000} $\Delta$}     \\ \midrule
\textit{CNNs-based}   &              &                              &                              &                         &                              & \textbf{}                             & {\color[HTML]{FF0000} }      \\ \midrule
                     &              & ViT-S                        & 80.03                        & 65.34                   & 65.82                        & \textbf{67.13}                        & {\color[HTML]{FF0000} +1.31} \\
ResNet18             & 63.39        & Swin-T                       & 76.13                        & 66.20                   & 66.94                        & \textbf{68.51}                        & {\color[HTML]{FF0000} +1.57} \\
                     &              & Mixer-B/16                   & 69.74                        & 64.42                   & 65.03                        & \textbf{66.02}                        & {\color[HTML]{FF0000} +0.99} \\
                     &              & ViM-T                        & 76.13                        & 66.69                   & 66.62                        & {\color[HTML]{000000} \textbf{67.61}} & {\color[HTML]{FF0000} +0.92} \\ \midrule
                     &              & ViT-S                        & 80.03                        & 66.00                   & 65.56                        & \textbf{66.96}                        & {\color[HTML]{FF0000} +0.96} \\
MobileNetV2          & 63.93        & Swin-T                       & 76.13                        & 66.51                   & 66.60                        & \textbf{68.06}                        & {\color[HTML]{FF0000} +1.46} \\
                     &              & Mixer-B/16                   & 69.74                        & 64.89                   & 65.28                        & \textbf{65.54}                        & {\color[HTML]{FF0000} +0.26} \\
                     &              & ViM-T                        & 76.13                        & 66.26                   & 66.14                        & {\color[HTML]{000000} \textbf{67.24}} & {\color[HTML]{FF0000} +0.98} \\ \midrule
\textit{Transformers-based}   &              &                              &                              &                         &                              & \textbf{}                             & {\color[HTML]{FF0000} }      \\ \midrule
                     &              & Mixer-B/16                   & 69.74                        & 68.67                   & 68.24                        & \textbf{69.10}                        & {\color[HTML]{FF0000} +0.43} \\
Swin-P               & 65.09        & ConvNeXt-T                   & 72.82                        & 66.90                   & 67.74                        & \textbf{68.03}                        & {\color[HTML]{FF0000} +0.29} \\
                     &              & ResNet50                     & 74.61                        & 70.84                   & 71.90                        & \textbf{72.36}                        & {\color[HTML]{FF0000} +0.46} \\
                     &              & {\color[HTML]{000000} ViM-T} & 76.13                        & 70.63                   & 70.22                        & {\color[HTML]{000000} \textbf{70.83}} & {\color[HTML]{FF0000} +0.20} \\ \midrule
                     &              & Mixer-B/16                   & 69.74                        & 64.13                   & 68.74                        & \textbf{69.26}                        & {\color[HTML]{FF0000} +0.52} \\
                     &              & ConvNeXt-T                   & 72.82                        & 59.33                   & 62.83                        & \textbf{64.86}                        & {\color[HTML]{FF0000} +2.03} \\
Deit-T               & 58.27        & ResNet50                     & 74.61                        & 66.72                   & 71.89                        & \textbf{72.29}                        & {\color[HTML]{FF0000} +0.4}  \\
                     &              & ViM-S                        & 83.86                        & 66.19                   & 66.96                        & \textbf{68.44}                        & {\color[HTML]{FF0000} +1.48} \\
                     &              & ViM-T                        & 76.13                        & 67.56                   & 68.69                        & \textbf{71.47}                        & {\color[HTML]{FF0000} +2.78} \\ \midrule
                     &              & Mixer-B/16                   & 69.74                        & 67.34                   & 68.85                        & \textbf{69.36}                        & {\color[HTML]{FF0000} +0.51} \\
T2t ViT-7            & 64.37        & ConvNeXt-T                   & 72.82                        & 65.16                   & 66.65                        & \textbf{69.31}                        & {\color[HTML]{FF0000} +2.66} \\
                     &              & ResNet50                     & 74.61                        & 70.08                   & 70.41                        & \textbf{72.66}                        & {\color[HTML]{FF0000} +2.25} \\
                     &              & ViM-T                        & 76.13                        & 69.89                   & 70.79                        & {\color[HTML]{000000} \textbf{72.22}} & {\color[HTML]{FF0000} +1.43} \\ \midrule
\textit{MLPs-based}   &              &                              &                              &                         &                              & \textbf{}                             & {\color[HTML]{FF0000} }      \\ \midrule
                     &              & ConvNeXt-T                   & 72.82                        & 66.37                   & 66.74                        & \textbf{67.23}                        & {\color[HTML]{FF0000} +0.49} \\
                     &              & ResNet50                     & 74.61                        & 72.06                   & 70.63                        & \textbf{73.44}                        & {\color[HTML]{FF0000} +1.38} \\
ResMLP-S12           & 65.46        & {\color[HTML]{000000} ViM-T} & {\color[HTML]{000000} 76.13} & 71.58                   & {\color[HTML]{000000} 70.31} & {\color[HTML]{000000} \textbf{71.72}} & {\color[HTML]{FF0000} +0.14} \\
                     &              & Swin-T                       & 76.13                        & 71.70                   & 73.09                        & \textbf{73.21}                        & {\color[HTML]{FF0000} +0.12} \\
                     &              & ViT-S                        & 80.03                        & 70.32                   & 69.64                        & \textbf{71.93}                        & {\color[HTML]{FF0000} +1.61} \\ \midrule
\textit{Mambas-based} &              &                              &                              &                         &                              & \textbf{}                             & {\color[HTML]{FF0000} }      \\ \midrule
                     &              & ViT-S                        & 80.03                        & 67.66                   & 72.84                        & \textbf{74.97}                        & {\color[HTML]{FF0000} +2.13} \\
ViM-T                & 61.85        & Swin-T                       & 76.13                        & 70.53                   & 72.08                        & \textbf{73.31}                        & {\color[HTML]{FF0000} +1.23} \\
                     &              & Mixer-B/16                   & 69.74                        & 65.59                   & 69.63                        & \textbf{70.55}                        & {\color[HTML]{FF0000} +0.92} \\ \bottomrule
\end{tabular}%
}
\vspace{-4mm}
\end{table*}

\section{Experiments}
\label{sec:experiments}
\subsection{Dataset and Settings}

\par In this section, we will introduce the dataset used in the experiment
%the teacher model and student model selected for knowledge distillation, the baseline compared with our method, 
and the implementation details.

\textbf{Datasets} \ We validate the proposed method on CIFAR-100~\cite{krizhevsky2009learning} and Tiny-ImageNet~\cite{le2015tiny}. The CIFAR-100 dataset comprises 60,000 images divided into 100 categories, with 600 images per category. The size of each image is 32×32. 50,000 images are used as the training set, and 10,000 are used as the test set. The Tiny-ImageNet dataset is a smaller version of the ImageNet dataset. It contains 100,000 images, which are divided into 200 categories. Each category has 500 training images, 50 validation images, and 50 test images. Each image is resized to 64×64.
%\textbf{Model} \ In knowledge distillation, the student model is typically smaller than the teacher model. In particular, in heterogeneous distillation, the student and teacher models belong to different architectures. 
% \textbf{Baselines} \ Our experiments focus on the logits-based space, taking OFAKD~\cite{}, KD~\cite{}, and DKD~\cite{} as baselines.

\textbf{Implementation Details} \
%During the experimental implementation, the model was optimized using different optimizers. 
To validate the generality of our method, we conduct experiments with different student and teacher models. For student models, CNN-based ResNet18~\cite{he2016deep}, MobileNet-v2~\cite{howard2018inverted}, Transformers-based Swin-p~\cite{liu2021swin}, Deit-t~\cite{touvron2021training}, T2t Vit-7~\cite{yuan2021tokens}, MLP-based ResMLP-S12~\cite{touvron2022resmlp}, and Mamba-based Vim-t~\cite{vim}, are selected. For teacher models, Resnet50~\cite{he2016deep}, Vit-S~\cite{dosovitskiy2020image}, Swin-T~\cite{liu2021swin}, Mixer-B/16~\cite{tolstikhin2021mlp}, and ConvNeXt-T~\cite{liu2022convnet}, are considered.
For CNNs, the SGD is adopted as the optimizer, with a base learning rate of 0.05. For Transformers, MLPs, and Mambas, the Adamw~\cite{loshchilovdecoupled} is adopted as the optimizer, with a base learning rate of 5e-4. The cosine learning rate decay strategy is used. For all datasets, we set the batch size as 128.
The training epoch number of CIFAR-100 is 300 for all models. For Tiny-ImageNet, CNNs are trained with 100 epochs, whereas ViTs, MLPs, and Mambas are trained with 300 epochs. All experiments are conducted using Nvidia RTX 3090 GPU.

% \begin{table}[htbp]
% \centering
% \begin{minipage}{0.49\textwidth}
%     \centering
    
%     \captionof{table}{Impact of number of stages in student.}
%     \label{tab:ablation3}
%     \footnotesize
%     \begin{tabular}{@{}cc@{}}
%     \toprule
%     Number of stage & ACC@1  \\ \midrule
%     0               &65.33  \\
%     1              &66.03                   \\
%     2              &66.94                   \\
%     3              &67.08                   \\
%     4              &\textbf{67.13}                   \\ \bottomrule
%     \end{tabular}
    
%     % \vspace{-2mm}
    
%     \centering
%     \captionof{table}{Validity of CWRD and SWRD in DFRA, where CWRM is Class-Wise Relation Decoupling and SWRD is Sample-Wise Relation Decoupling. }
%     \label{tab:ablation1}
%     % \resizebox{\columnwidth}{!}{%
%     % \fontsize{3pt}{3}\selectfont
%     \footnotesize
%     \resizebox{0.6\linewidth}{!}{
%     \begin{tabular}{@{}cccc@{}}
%     \toprule
%     MSDF Module & CWRD
%     % Class-Level Decoupled Relation Alignment 
%     & SWRD
%     % Sample-Level Decoupled Relation Alignment 
%     & Acc@l \\ \midrule
%     \checkmark           & $\times$                                       & $\times$                                        & 78.24 \\
%     \checkmark           & $\times$                                       & \checkmark                                         & 78.66 \\
%     \checkmark           & \checkmark                                        & $\times$                                        & 78.39 \\
%     \checkmark           & \checkmark                                        & \checkmark                                         & \textbf{78.76} \\ \bottomrule
%     \end{tabular}%
%     }
%     \vspace{-2mm}
% \end{minipage}%
% \vspace{-4mm}
% \end{table}

\begin{table*}[htbp]
\centering
\begin{minipage}[t]{0.22\linewidth}
\centering
\captionof{table}{Impact of the number of stages in MSDF (Multi-Scale Dynamic Fusion ).}
\vspace{-3mm}
\label{tab:ablation3}
\small
\begin{tabular}{@{}cc@{}}
\toprule
Number of stage & ACC@1  \\ \midrule
0               &65.33  \\
1              &66.03                   \\
2              &66.94                   \\
3              &67.08                   \\
4              &\textbf{67.13}                   \\ \bottomrule
\end{tabular}
\end{minipage}\hfill
\begin{minipage}[t]{0.32\linewidth}
\centering
\captionof{table}{Effect of CWRD (Class-Wise Relation Decoupling) and SWRD (Sample-Wise Relation Decoupling) in DFRA (Decoupled Finegrained Relation Alignment).}
\vspace{-3.4pt}
\label{tab:ablation1}
\footnotesize
\begin{tabular}{@{}cccc@{}}
\toprule
MSDF Module & CWRD
% Class-Level Decoupled Relation Alignment 
& SWRD
% Sample-Level Decoupled Relation Alignment 
& Acc@l \\ \midrule
\checkmark           & $\times$                                       & $\times$                                        & 66.76 \\
\checkmark           & $\times$                                       & \checkmark                                         & 66.98 \\
\checkmark           & \checkmark                                        & $\times$                                        & 66.93 \\
\checkmark           & \checkmark                                        & \checkmark                                         & \textbf{67.13} \\ \bottomrule
\end{tabular}
\end{minipage}\hfill
\begin{minipage}[t]{0.42\linewidth}
\centering
\captionof{table}{More ablation studies in feature and logit levels. 
% MSDF means Multi-Scale Dynamic Fusion, DFRA denotes Decoupled Finegrained Relation Alignment.
}
\vspace{-6.4pt}
\label{tab:ablation2}
\footnotesize
\setlength{\tabcolsep}{7.4pt}
\renewcommand{\arraystretch}{0.73}
\begin{tabular}{@{}ccccc@{}}
\toprule
Feature level & Logit level & MSDF & DFRA & Acc@1 \\ \midrule
\checkmark              &$\times$             &$\times$      &$\times$      &65.98       \\
\checkmark              &$\times$             &\checkmark      &$\times$      &66.43       \\
\checkmark              &$\times$             &$\times$      &\checkmark      &66.60       \\
\checkmark              &$\times$             &\checkmark      &\checkmark      &66.96       \\
$\times$              &\checkmark             &$\times$      &$\times$      &65.23       \\
$\times$              &\checkmark             &$\times$      &\checkmark      &65.33       \\
\checkmark              &\checkmark             &$\times$      &$\times$      &66.68       \\
\checkmark              &\checkmark             &\checkmark      &$\times$      &66.76       \\
\checkmark              &\checkmark             &$\times$      &\checkmark      &66.91       \\
\checkmark              &\checkmark             &\checkmark      &\checkmark      &\textbf{67.13}      \\   \bottomrule
\end{tabular}
\end{minipage}
\vspace{-3mm}
\end{table*}

\subsection{Results and Analysis}
\textbf{Results on CIFAR-100} \ We first conduct experiments on the CIFAR-100 dataset. Comparisons with the baselines are presented in Table~\ref{tab:cifar100}. It can be observed that our method can improve the performance of commonly used CNNs-based student models by 0.33\% to 1.43\%. Moreover, our method achieves remarkable results on Transformers-based student models, especially on the Mixer-B/16 and Deit-t pair, where the accuracy is raised by 4.86\%. Compared with KD, DKD, RKD and OFAKD, our method's improvement ranges from 0.09\% to 4.86\%. For the less prevalent MLPs-based student models, our method also achieves an improvement in accuracy by 0.20\% to 0.93\% compared with OFAKD. Moreover, the scarcely explored Mambas-based student models are significantly improved by our method, further verifying its effectiveness and generality. Overall, our proposed method achieves state-of-the-art performance on all different student architectures.

% In heterogeneous distillation, it is challenging to distill knowledge from complicated models. Our method performs well on CNN-based models and even impressively on more complicated ViT-based models. Furthermore, our method is also promising for MLP-based and Mamba-based models, which indicates the generality of our method.

\begin{figure}[t]
  \centering
  \includegraphics[width=\linewidth]{sec/figures/featuremap2.png}
  \vspace{-6mm}
  \caption{Comparisons of feature visualizations between OFA-KD and our MLDR-KD. The teacher is Vision Mamba Tiny, the student is ResNet-18. Clearly, our approach makes the student model more focused on the target across various samples.}
  \label{fig:featuremap2}
  \vspace{-4mm}
\end{figure}

\textbf{Results on Tiny-ImageNet} \ To assess the capability of our approach in coping with larger datasets, we expand experiments to the Tiny-ImageNet dataset. To align with the CIFAR-100 dataset, we select corresponding teacher and student models from CNNs-based, Transformers-based, MLPs-based, and Mambas-based models. As no previous baseline results are available, we compare our method with the baselines KD and OFAKD by reproducing them on Tiny-Imagenet. The results are showed in Table~\ref{tab:tinyimg}.


Our method exhibits more stable accuracy improvements than the baselines from the results on the Tiny-ImageNet dataset. Specifically, the accuracy improvement ranges from 0.26\% to 1.57\% on the CNNs-based student models and 0.20\% to 2.78\% on the Transformers-based student models, especially achieving an improvement of 2.78\% in the architecture pair ViM-T-to-Deit-T. Additionally, there is a significant improvement in our newly added student models T2t ViT-7 compared to the baseline methods. On the MLPs-based student models, our method could achieve an accuracy improvement of 0.12\% to 1.61\% compared to the baseline methods, particularly attaining the highest improvement of 1.61\% on the ResNet50 teacher model. For the latest Mamba-based student models, our method still presents considerable accuracy improvements.

Compared with the results on the CIFAR-100 dataset, the accuracy on the Tiny-ImageNet dataset exhibits advanced stability, indicating the advantages of our fine-grained design over traditional methods when dealing with larger datasets. Moreover, it can be observed that our method has more obvious improvements when applied to larger and more complicated models (such as ConvNeXt-T and ResNet50), validating that our method is potentially practical in further boosting off-the-shelf high-performance models, which are generally large and complicated. Similar to the case on the CIFAR-100 dataset, our method is applicable to the Mamba-based student models, further illustrating the generalization ability of the proposed heterogeneous distillation method.



\textbf{Visualization} \ A visual comparison between our method and baselines on the Tiny-Imagenet dataset is illustrated in Fig.~\ref{fig:featuremap2}. The students trained via our MLDR-KD can better learn from heterogeneous teachers. For example, even though the target occupies a small portion of the sample image, our student model is always able to focus on the information related to the target. The student model's attention does not diverge to distracting information. This is a strong indication that the student model, after our MLDR-KD, is well able to assimilate knowledge from heterogeneous teachers.


% \begin{figure*}[!p]
%   \centering
%   \includegraphics[width=0.95\linewidth]{sec/figures/logit.png}
%   \vspace{-1mm}l
%   \caption{Visualization of four methods feature map in Resnet18. The teacher is Vision Mamba Tiny.}
%   \label{fig:logit}
%   \vspace{-2mm}
% \end{figure*}

% \begin{table}[t]
%     \centering
%     \label{tab:ablation2}
%     \caption{Effect of our MLDR-KD in feature or logit level.}
%     \resizebox{0.8\columnwidth}{!}{%
%     \fontsize{5t}{5}\selectfont
%     \begin{tabular}{@{}ccccc@{}}
% \toprule
% Feature level & Logit level & MSDF & DRFA & Acc@1 \\ \midrule
% \checkmark              &$\times$             &$\times$      &$\times$      &65.98       \\
% \checkmark              &$\times$             &\checkmark      &$\times$      &66.43       \\
% \checkmark              &$\times$             &$\times$      &\checkmark      &66.60       \\
% \checkmark              &$\times$             &\checkmark      &\checkmark      &66.96       \\
% $\times$              &\checkmark             &$\times$      &$\times$      &65.23       \\
% $\times$              &\checkmark             &$\times$      &\checkmark      &65.33       \\
% \checkmark              &\checkmark             &$\times$      &$\times$      &66.68       \\
% \checkmark              &\checkmark             &\checkmark      &$\times$      &66.93       \\
% \checkmark              &\checkmark             &$\times$      &\checkmark      &66.91       \\
% \checkmark              &\checkmark             &\checkmark      &\checkmark      &\textbf{67.13}      \\   \bottomrule
% \end{tabular}
% }
% \end{table}

\subsection{Ablation study}

Ablative experiments are designed to verify the effectiveness of the proposed MLDR-KD, shown in Table.~\ref{tab:ablation3}, Table.~\ref{tab:ablation1} and Table.~\ref{tab:ablation2}. In this part, all
experiments are conducted on Tiny-Imagenet, with ViT-S as the
teacher model, Resnet18 as the student model.

\textbf{Number of stages of student} \ In Table.~\ref{tab:ablation3}, we conduct experiments to explore the impact of stages in student model. In order to accommodate different architectures, we divide the student model into a maximum of 4 stages. We chose stages 0 to 4 to compare the difference. We find that as the number of stages increases, the improvement effect of our methodology enhances. Four stages are the most potent.

\textbf{Validity of CWRD and SWRD in DFRA.} \ In order to verify the validity of proposed DFRA, we ablate our method in the presence of both feature and logit levels, shown in Table.~\ref{tab:ablation1}. From the results, CWRD and SWRD have improved by 0.22\% and 0.17\% relative to the original, respectively. Both of them can enhance 0.37\% in performance. Evidently, CWRD and SWRD in DFRA have an indispensable role to play. 

\textbf{Effect of our MLDR-KD in feature or logit level} \  Our MLDR-KD improves heterogeneous distillation in both feture and logit
levels. We study this improvement in this part. In Table.~\ref{tab:ablation2}, We ablate different modules at different levels. In a side-by-side comparison (\textit{e.g.} line 1-4), both of our methods MSDF and DFRA are effective when applied to only one level or to both levels. Vertical comparisons (\textit{e.g.} line 4 and 10) show that applying our methodology to multiple levels is the most effective.

% \begin{table*}
% \caption{table1}
% \label{tab:1}
% \centering
% \subtable[subtab1]{
% \begin{tabular}{@{}cc@{}}
% \bottomrule
% Number of stage & ACC@1  \\ \bottomrule
% 0               &65.33  \\
% 1              &66.03                   \\
% 2              &66.94                   \\
% 3              &67.08                   \\
% 4              &\textbf{67.13}                   \\ \bottomrule
% \end{tabular}
% \label{tab:ablation3}
% }
% \subtable[subtab2]{
% \begin{tabular}{@{}cccc@{}}
% \toprule
% MSDF Module & CDRA
% % Class-Level Decoupled Relation Alignment 
% & SDRA
% % Sample-Level Decoupled Relation Alignment 
% & Acc@l \\ \midrule
% \checkmark           & $\times$                                       & $\times$                                        & 78.24 \\
% \checkmark           & $\times$                                       & \checkmark                                         & 78.66 \\
% \checkmark           & \checkmark                                        & $\times$                                        & 78.39 \\
% \checkmark           & \checkmark                                        & \checkmark                                         & \textbf{78.76} \\ \bottomrule
% \end{tabular}%
% \label{tab:ablation1}
% }
% \subtable[subtab3]{
% \begin{tabular}{@{}ccccc@{}
% \bottomrule
% Feature level & Logit level & MSDF & DRFA & Acc@1 \\ \bottomrule
% \checkmark              &$\times$             &$\times$      &$\times$      &65.98       \\
% \checkmark              &$\times$             &\checkmark      &$\times$      &66.43       \\
% \checkmark              &$\times$             &$\times$      &\checkmark      &66.60       \\
% \checkmark              &$\times$             &\checkmark      &\checkmark      &66.96       \\
% $\times$              &\checkmark             &$\times$      &$\times$      &65.23       \\
% $\times$              &\checkmark             &$\times$      &\checkmark      &65.33       \\
% \checkmark              &\checkmark             &$\times$      &$\times$      &66.68       \\
% \checkmark              &\checkmark             &\checkmark      &$\times$      &66.93       \\
% \checkmark              &\checkmark             &$\times$      &\checkmark      &66.91       \\
% \checkmark              &\checkmark             &\checkmark      &\checkmark      &\textbf{67.13}      \\ \bottomrule

% \end{tabular}
% \label{tab:ablation2}
% }
% \end{table*}