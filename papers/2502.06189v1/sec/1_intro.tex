\section{Introduction}
\label{sec:intro}

\begin{figure}[t]
  \centering
  \includegraphics[width=0.9\linewidth]{sec/figures/header_v3_to_pdf_v2.pdf}
  \vspace{-2mm}
  \caption{Conceptual comparisons of different knowledge distillation methods.
  % Multi-level Decoupled Relational Knowledge Distillation (MLDR-KD). 
  % In our method, 
  Our Decoupled Relational KD first decouples the logits of teacher and student into multiple finegrained relationships between different classes under each sample and different samples under each class, and then aligns the relationships. In our method, Decoupled Relational KD is applied to both logit and multiscale feature levels (namely MLDR-KD).
  % in higher level. 
  % Then these relationships will be dynamically aligned in logit and feature levels.
  }
  \label{fig:header}
  \vspace{-4mm}
\end{figure}

%Recently, the rapid development of deep neural networks (DNNs) has revolutionized the field of computer vision (CV). However, as their performance and capabilities improve, DNN models are also becoming increasingly larger and consuming more computational and storage resources, making them difficult to use in some practical scenarios such as mobile devices and industrial facilities. To solve this problem, % 这一段不要没什么影响，快速进入KD的主题
Recently, knowledge distillation (KD)~\cite{hinton2015distilling}, which aims to train a superior lightweight student model by mimicking the teacher model, has been demonstrated to be one of the most effective approaches for model compression~\cite{tang2022patchslimmingefficientvision,zhang2019your}. 
% Feature-based distillation and logit-based distillation are generally employed to transfer knowledge from the teacher model to the student model. 
The majority of existing knowledge distillation methods~\cite{tang2022patchslimmingefficientvision,zhang2019your,phuong2019distillation,luan2019msd,zhu2018knowledge} concentrate on the 
% process of 
distillation between teacher and student models with homogeneous architectures. 
However, this narrow focus limits the widespread use of knowledge distillation. On one hand, there continually emerge new network architectures such as mamba~\cite{gu2023mamba}. On the other hand, there exist various pretrained models that have superior performance but different architectures~\cite{he2022masked,xie2022simmim,beit}.
% With the emergence of various new network architectures such as mamba~\cite{gu2023mamba}, heterogeneous distillation can take advantage of existing teacher models that have worked well to enhance their performance of new architectural models, 
% facilitating the practical deployment and application of new architectures. 
Consequently, it is essential to explore the potential of knowledge distillation between heterogeneous architectures.

\begin{figure}[t]
  \centering
  \includegraphics[width=0.95\linewidth]{sec/figures/featuremap.png}
  \vspace{-3mm}
  \caption{Comparisons of feature visualizations when using kinds of knowledge distillation methods. The teacher is Vision Mamba Tiny~\cite{vim}, the student is ResNet-18~\cite{he2016deep}. The direct use of conventional relational KD underperforms on heterogeneous distillation, while our MLDR-KD could greatly improve this problem.}
  \label{fig:featuremap}
  \vspace{-5mm}
\end{figure}

A few recent studies attempt to investigate the feasibility of using heterogeneous teachers for knowledge transfer~\cite{yu2024unleashing,Liu2024DDKDD,wang2024the}. Touvron \textit{et al.}~\cite{touvron2022deit} achieves successful training of a ViT student model using a CNN teacher model. Ao Wang \textit{et al.}~\cite{wang2024repvit} revisits the efficient design of lightweight CNNs from the ViT perspective and emphasizes their promising prospect for mobile devices. Although achieving good results, these approaches cannot be extended to various architectures.
% the full range of architecture. 
As a pioneer, Zhiwei Hao \textit{et al.}~\cite{hao2024one} finds there is a huge gap among feature maps of heterogeneous architecture, resulting in the failure of feature-based knowledge distillation~\cite{peng2019correlation,Heo_2019_ICCV,zagoruyko2022paying,romero2014fitnets,Heo2018KnowledgeTV,Yim2017AGF,Ahn2019VariationalID,Chen2022KnowledgeDW,Chen2021DistillingKV,guo2023class,Li2021OnlineKD,Lin2022KnowledgeDV}. Thus, they propose logit-based generic heterogeneous distillation. 
% with promising outcomes. 
Specifically, by increasing the confidence in the correct category of the teacher model, the impact of architectural differences is reduced, and the results are improved. However, this approach somehow weakens the transfer of dark knowledge, which is regarded as very important in knowledge distillation (e.g., whether a sample that is actually a dog or more like a cat), which limits the performance of heterogeneous distillation.
% . This limits the performance of heterogeneous distillation across different architectures. 

In this paper, we further explore how to effectively transfer the dark knowledge during heterogeneous distillation for the first time. In traditional homogeneous distillation, relational knowledge distillation (RKD)~\cite{park2019relational} is generally considered as an effective method for transferring dark knowledge, as shown in Fig.~\ref{fig:header}. RKD aligns correlations or dependencies among multiple instances between the student and teacher networks. 
However, we find that the direct use of RKD in heterogeneous distillation causes a new problem: the over-amplification of the role of dark knowledge, which may reduce the confidence in the correct category of the teacher model. Since the latter is equally important in heterogeneous distillation due to the variability between architectures, this can directly contribute to the failure of the RKD method, as shown in Fig.~\ref{fig:featuremap}. Facing such a dilemma, a question naturally arises: \textit{can we effectively transfer the abundant dark knowledge while keeping the confidence of the correct category during heterogeneous distillation?} 

% Facing such a dilemma, we present a novel framework called Multi-Scale Decoupled Relational Knowledge Distillation (MSDR-KD) to further explore the potential of the logit-based methods in cross-architectures distillation. 
% Considering the trade-off between dark knowledge and the confidence in the correct category of the teacher model, 
To answer this question, we present an innovative framework called Multi-Level Decoupled Relational Knowledge Distillation (MLDR-KD) for heterogeneous distillation. Specifically, we first propose Decoupled Finegrained Relation Alignment (DFRA),
% to investigate the application of the decoupled logit relation inside the heterogeneous distillation. In MLDRA, 
in which model logits are first decoupled into multiple finegrained relationships between different categories under each image and different images under each category. Due to the multiple steps finegrained decoupling, the subsequent alignment is sensitive to whether the model classifies correctly, and it can magnify the gap when the classification results of student model and teacher model are not aligned.
% The gap between the student model classification results is further magnified when they are not aligned with those of the teacher model. 
% Because of the sensitivity of decoupled logit relation,
% As a result, our method is able to retain dark knowledge in the logit of the teacher model while enhancing the confidence of the classification results of the teacher model. 
As a result, our method can well transfer dark knowledge while enhancing the confidence of the classification results during heterogeneous distillation. Further, we apply the DFRA to both logit and feature levels, and 
% To further improve the performance of MLDRA, we 
present the Multi-Scale Dynamic Fusion (MSDF) module in the feature level.
% Inspired by OFA-KD, 
In the MSDF module, the multiscale feature maps of different stages in student model are projected into multiple logits, 
% to do the next processing, 
% In the MSDF module, we 
and a gated network is used to dynamically fuse these logits.
% balance the impact of each stage of feature on distillation learning. 
% By aligning the fused multi-scale features with the teacher model, the student model is dynamically improved by MLDRA at each stage. 
% By aligning the fused multi-level logits with the teacher model, the student model is dynamically improved by MLDRA. 
As shown in Fig.~\ref{fig:featuremap}, our method can release the potential of logit-based cross-architectures distillation, where the student model will focus more on information related to the goal. 



% MSDR-KD guides the student network to learn dark knowledge from the heterogeneous teacher, further improving the classification performance. 
To illustrate the robustness and generality of our approach, we conduct 12 kinds of experiments between 4 architectures including CNNs, Transformers, MLPs and Mambas. We distill them two by two, with image classification as the evaluation task and acc@1 as the evaluation metric. Compared with the best available method, our MLDR-KD framework improves student model performance with gains of up to 1.43\%, 4.86\%, 0.93\%, 0.83\% on CIFAR-100 dataset and 1.57\%, 2.78\%, 1.61\%, 2.13\% on Tiny-ImageNet dataset for CNNs, Transformers, MLPs and Mambas architectures under the same conditions, respectively. The ablation study has also demonstrated the effectiveness of our methods.
In summary, our main contributions can be summarized as follows:
\begin{enumerate}
    \item[$\bullet$] We first propose to utilize dark knowledge for heterogeneous distillation. We find that: 1) previous work~\cite{hao2024one} destroys the dark knowledge present in the teacher model logit, which limits the performance of heterogeneous distillation; 2) The direct use of relational knowledge distillation in traditional homogenous distillation 
    to transfer dark knowledge 
    reduces the confidence in the correct category, bringing about catastrophic performance in heterogeneous distillation. 

    \item[$\bullet$] To address these, we present a novel framework called Multi-Level Decoupled Relational Knowledge Distillation (MLDR-KD).
    % MLDR-KD framework consists of two parts, 
    It consists of Decoupled Finegrained Relation Alignment (DFRA) and Multi-Scale Dynamic Fusion (MSDF) module. Specifically, DFRA enables the student model to learn more finegrained relationships in both logit and feature levels. MSDF module further improves the feature level DFRA by dynamically fusing the predictions of multiscale features of students.
    % In logit level, 
    % DFRA decouples logits into relationships between different categories under each image and different images under each category. 
    % % At the same time, 
    % MSDF module is used to improve the student model dynamically at each stage by DFRA in feature level.
    % By aligning student and teacher in logit and feature levels, our MLDR-KD can balance the trade-off between dark knowledge and the confidence in the correct category of the teacher model. It enables the student model to learn more finegrained information, pushing the performance of our method to a higher level.

    \item[$\bullet$] Extensive experiments across diverse datasets and models consistently verify that MLDR-KD can achieve new state-of-the-art performance. In particular, we extend the MLDR-KD method to the new architecture Mamba, and find our method also performs best, which well illustrates the robustness and generality of our method.
\end{enumerate}

