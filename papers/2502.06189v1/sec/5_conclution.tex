\section{Conclusion}
\label{sec:conclusion}

In this paper, we propose Multi-Level Decoupled Relational Knowledge Distillation (MLDR-KD), a novel approach to balance the trade-off between dark knowledge and the confidence in the correct category of the teacher model for heterogeneous architectures. Specifically, DFRA is designed to align finegrained relationship for heterogeneous architectures in feature and logit level. The MSDF module is further introduced to improve DFRA performance by fusing feature maps of student in feature level. Extensive experiments show the robustness and generality of our MLDR-KD. Our future work involves how to efficiently take full advantage of feature information   
to further enhance the proposed MLDR-KD.






