\section{Methodology}
\label{sec:methodology}
\subsection{Preliminaries}
\label{sec:KD}
We start from the original Logit-based Knowledge Distillation (KD) method. Generally, We denote the logit out as $z \in \mathbb{R}^{B\times N}$, where $B$ is the batch size in training and $N$ means the number of categories in dataset. The softmax function is then used to obtain a probability distribution:



\begin{equation}
    \begin{aligned}
        p_{i} = \frac{\exp(z_{i})}{\sum_{j = 1}^{N}\exp(z_{j})}, i = 1,2,\cdots,N
    \end{aligned}
\end{equation}
where $p_{i}$ is the probability distribution of a sample.

In Logit-based KD, the cross-entropy loss $\mathcal{L}_{CE}$ is used to minimize gap between the student model and the ground truth: 
\begin{equation}
    \begin{aligned}
        \mathcal{L}_{CE}=-\sum_{i = 1}^{B}\sum_{j = 1}^{N}y_{ij}\log(p_{s,ij})
    \end{aligned}
\end{equation}
where $y_{ij}$ is the one-hot encoded true label, and $p_{s,ij}$ is the probability distribution of the student model after softmax.

Student model mimics the teacher model by means of distillation loss $\mathcal{L}_{KL}$. We use the Kullback-Leibler divergence to measure the difference between the student model and the teacher model: 
\begin{equation}
    \begin{aligned}
        \mathcal{D}_{KL}(p_{s,i}||p_{t,i})=\sum_{j = 1}^{N}p_{s,ij}\log\frac{p_{s,ij}}{p_{t,ij}}
    \end{aligned}
\end{equation}
\begin{equation}
    \begin{aligned}
        \mathcal{L}_{KL}=\frac{1}{B}\sum_{i = 1}^{B}\mathcal{D}_{KL}(p_{s,i}||p_{t,i})
    \end{aligned}
\end{equation}
where $\mathcal{D}_{KL}(p_{s,i}||p_{t,i})$ is the KL divergence. 

The overall knowledge distillation loss function: 
\begin{equation}
    \begin{aligned}
        \mathcal{L} = \mathcal{L}_{CE}+\lambda \mathcal{L}_{KL}
    \end{aligned}
\end{equation}
where $\lambda$ is a weighting parameter that balances the cross-entropy loss and distillation loss.

\subsection{MLDR-KD}
The overview of MLDR-KD is depicted in Fig.~\ref{fig:framework}. Our method framework is primarily divided into two modules: the Decoupled Finegrained Relation Alignment (DFRA) and the Multi-Scale Dynamic Fusion (MSDF) Module. Initially, the student model are segmented into multiple stages. After forward inference, the logit outputs of the teacher and the student are obtained. Meanwhile, feature maps of student at each stage are fused by MSDF Module to get a fusion logit. Finally, the fusion logit and the logit output of student will be aligned with logit output of teacher by DFRA in logit and feature levels. We present our two modules of MLDR-KD framework in Sec.~\ref{sec:DFRA} and Sec.~\ref{sec:MSDFM}.

\subsubsection{Decoupled Finegrained Relation Alignment}

\begin{figure*}[t]
  \centering
  \includegraphics[width=0.98\linewidth]{sec/figures/logit_v3.pdf}
  \vspace{-6mm}
  \caption{Comparisons of the averaged prediction distribution of all samples of single category among OFA-KD ((a),(d),(g)), RKD ((b),(e),(h)), and our MLDR-KD ((c),(f),(i)). Three black boxes represent three randomly selected categories.
  % , and the averaged prediction distribution of all samples.
  % three categories that are randomly selected.
  In each figure (left), we show the logit of category in addition to the correct category. In each figure (right), the logit of the correct category is displayed. From  the figure we can see that our method has 
high confidence for the correct category while transferring abundant dark knowledge in the teacher model logit.
  }
  \label{fig:logit}  
  \vspace{-4mm}
\end{figure*}

\label{sec:DFRA}
In heterogeneous distillation, it is crucial to balance the correct samples' confidence and the dark knowledge from teacher model. To deal with this problem, we propose DFRA to enhance knowledge transfer between heterogeneous architectures. As shown in Fig.~\ref{fig:framework}, we decouple logit prediction into Class-Wise Relation and Sample-Wise Relation in contrast to exact match. These relations will be aligned in multi level. 

\textbf{Class-Wise Relation Decoupling} \ Class-Wise relation represents the degree of similarity among different categories. In this section, we refine this relationship to each sample in the batch to transfer more information (\emph{e.g.} under a particular sample labeled dog, the similarity between cat and elephant). Firstly, we expand logit prediction to three dimensions, which is defined as:
\begin{equation}
    \begin{aligned}
        \hat{z}_{c} = \operatorname{Expand}(z/T), \hat{z}_{c} \in \mathbb{R}^{B\times N\times 1}
    \end{aligned}
\end{equation}
where $T$ is the soft factor in knowledge distillation. 
Then we could calculate its self-relation, which is implemented as the scaled product relation:
\begin{equation}
    \begin{aligned}
        \mathcal{R}_{class} =\operatorname{Softmax} \left(\frac{\hat{z}_{c} \hat{z}_{c}^{T}}{\sqrt{N}}\right), \mathcal{R}_{class} \in \mathbb{R}^{B\times N\times N}
    \end{aligned}
\end{equation}
where $\mathcal{R}_{class}$ indicates class-wise relation decoupled from initial logit out $z$. $N$ denotes a scaling factor that equals to the number of categories in dataset.

\textbf{Sample-Wise Relation Decoupling} \ The other information then can be decoupled from initial logit out $z$ is sample-wise relation. It's regarded as the degree of similarity between samples under one category (\emph{e.g.} in a batch which of the many samples is more like a dog). Sample-wise relation can be modeled by predictions of a batch of data as follows:
\begin{equation}
    \begin{aligned}
        \hat{z}_{b} = \operatorname{Expand}(z^{T}/T), \hat{z}_{b} \in \mathbb{R}^{N\times B\times 1}
    \end{aligned}
\end{equation}
\begin{equation}
    \begin{aligned}
        \mathcal{R}_{sample} = \operatorname{Softmax}\left(\frac{\hat{z}_{b} \hat{z}_{b}^{T}}{\sqrt{N}}\right), \mathcal{R}_{batch} \in \mathbb{R}^{N\times B\times B}
    \end{aligned}
\end{equation}
where $\mathcal{R}_{sample}$ indicates sample-wise relation decoupled from initial logit out $z$.



\textbf{Multiple Relation Alignment} \ Decoupled finegrained relation between heterogeneous student model and teacher model can be aligned by Kullback-Leibler divergence:
\begin{equation}
    \begin{aligned}
        \mathcal{L}_{class} = \mathcal{L}_{KL}(\mathcal{R}_{class}^{s},\mathcal{R}_{class}^{t})
    \end{aligned}
\end{equation}
\begin{equation}
    \begin{aligned}
        \mathcal{L}_{sample} = \mathcal{L}_{KL}(\mathcal{R}_{sample}^{s},\mathcal{R}_{sample}^{t})
    \end{aligned}
\end{equation}
\begin{equation}
    \begin{aligned}
        \mathcal{L}_{total} = \mathcal{L}_{class}+\mathcal{L}_{sample}+\lambda\mathcal{L}_{KL}(p^{s},p^{t})
    \end{aligned}
\end{equation}
where $\lambda$ denotes the balance coefficient. $p^{s}$ and $p^{t}$ are the probability distributions of $z^{s}$ and $z^{t}$ after softmax function.

\begin{table*}[t]
\centering
\caption{Results on CIFAR100 dataset. The best results are indicated in bold. For the baseline, most of the experimental results are inherited from OFAKD, while the additional experiments we conducted are marked with *.}
\vspace{-2mm}
\label{tab:cifar100}
\resizebox{1.7\columnwidth}{!}{%
\fontsize{7pt}{7}\selectfont
\begin{tabular}{@{}cccccccccc@{}}
\toprule
Student Model        & From Scratch & Teacher Model & From Scratch & KD~\cite{hinton2015distilling}    & RKD~\cite{park2019relational}   & DKD~\cite{zhao2022decoupled}   & OFAKD~\cite{hao2024one}  & \textbf{MLDRKD} & {\color[HTML]{FF0000} $\Delta$}     \\ \midrule
\textit{CNNs-based}   &              &               &              &       &       &       &        &                 & {\color[HTML]{FF0000} }      \\ \midrule
                     &              & ViT-S         & 92.04        & 77.26 & 73.72 & 78.10 & 80.15  & \textbf{80.51}  & {\color[HTML]{FF0000} +0.36} \\
ResNet18             & 74.01        & Swin-T        & 89.26        & 78.74 & 74.11 & 80.26 & 80.54  & \textbf{81.56}  & {\color[HTML]{FF0000} +1.02} \\
                     &              & Mixer-B/16    & 87.29        & 77.79 & 73.75 & 78.67 & 79.39  & \textbf{80.79}  & {\color[HTML]{FF0000} +1.40}  \\
                     &              & ViM-S         &  87.89*       & 78.22*     & 77.41*     & 79.20*     & 79.90*  & \textbf{80.23}  & {\color[HTML]{FF0000} +0.33} \\ \midrule
MobileNetV2          & 73.68        & ViT-S         & 92.04        & 72.77 & 68.46 & 69.80 & 78.45  & \textbf{79.31}  & {\color[HTML]{FF0000} +0.86} \\
                     &              & Mixer-B/16    & 89.26        & 73.33 & 68.95 & 70.20 & 78.78  & \textbf{80.21}  & {\color[HTML]{FF0000} +1.43} \\ \midrule
\textit{Transformers-based}   &              &               &              &       &       &       &        &                 & {\color[HTML]{FF0000} }      \\ \midrule
                     &              & Mixer-B/16    & 87.29        & 75.93 & 69.89 & 76.39 & 78.93  & \textbf{80.09}  & {\color[HTML]{FF0000} +1.16} \\
Swin-P               & 72.63        & ConvNeXt-T    & 88.41        & 76.44 & 69.79 & 76.80 & 78.32  & \textbf{81.21}  & {\color[HTML]{FF0000} +2.89} \\
                     &              & ViM-S         & 87.89*       & 78.42*     &  72.69*      & 79.29*    & 79.48* & \textbf{79.91}  & {\color[HTML]{FF0000} +0.43} \\ \midrule
                     &              & Mixer-B/16    & 87.29        & 71.36 & 70.82 & 73.44 & 73.90  & \textbf{78.76}  & {\color[HTML]{FF0000} +4.86} \\
Deit-T               & 68.00        & ConvNeXt-T    & 88.41        & 72.99 & 71.73 & 74.60 & 75.76  & \textbf{79.18}  & {\color[HTML]{FF0000} +3.42} \\
                     &              & ViM-S         & 87.89*       & 73.28*     & 70.22*     & 74.68*     & 76.69* & \textbf{77.27}  & {\color[HTML]{FF0000} +0.58} \\ \midrule
                     &              & Mixer-B/16    & 87.29*       & 77.43*     & 75.76*     & 79.53*     & 81.54* & \textbf{81.61}  & {\color[HTML]{FF0000} +0.07} \\
T2t ViT-7            & 74.74        & ConvNeXt-T    & 88.41*       & 79.26*     & 75.31*     &  79.83*     & 82.52* & \textbf{82.67}  & {\color[HTML]{FF0000} +0.15} \\
                     &              & ViM-S         & 87.89*       & 77.39*     & 72.53*     & 78.48*     & 81.38* & \textbf{81.47}  & {\color[HTML]{FF0000} +0.09} \\ \midrule
\textit{MLPs-based}   &              &               &              &       &       &       &        &                 & {\color[HTML]{FF0000} }      \\ \midrule
                     &              & ConvNeXt-T    & 88.41        & 72.25 & 65.82 & 73.22 & 81.22  & \textbf{81.96}  & {\color[HTML]{FF0000} +0.74} \\
ResMLP-S12           & 66.56        & Swin-T        & 89.26        & 71.89 & 64.66 & 72.82 & 80.63  & \textbf{81.56}  & {\color[HTML]{FF0000} +0.93} \\
                     &              & ViM-S         & 87.89*       & 80.23*     & 78.19*     & 80.72*     & 80.37* & \textbf{80.92}  & {\color[HTML]{FF0000} +0.20} \\ \midrule
\textit{Mambas-based} &              &               &              &       &       &       &        &                 & {\color[HTML]{FF0000} }      \\ \midrule
                     &              & ViT-S         & 92.04*       & 77.55*     & 68.85*     & 79.58*     & 81.24* & \textbf{81.87}  & {\color[HTML]{FF0000} +0.63} \\
ViM-T                & 70.99        & Swin-T        & 89.26*       & 78.53*     & 66.91*     & 80.50*      & 82.22* & \textbf{83.01}  & {\color[HTML]{FF0000} +0.79} \\
                     &              & Mixer-B/16    & 87.29*       & 79.34*     & 73.08*     & 80.57*     & 82.19* & \textbf{83.02}  & {\color[HTML]{FF0000} +0.83} \\
                     &              & ConvNeXt-T    & 88.41*       &  80.59*     &  66.41*     & 82.51*     & 82.89* & \textbf{82.95}  & {\color[HTML]{FF0000} +0.06} \\ \bottomrule
\end{tabular}%
}
\vspace{-4mm}
\end{table*}

Due to the multi-step decoupling of logit, our proposed Decoupled Finegrained Relation Alignment method is robust to enhance the performance of student model. DFRA not only improves the confidence level of the classification results, but also retains a lot of details of the information. Following experiments will further demonstrate the effectiveness of our method.
\subsubsection{Multi-Scale Dynamic Fusion Module}
\label{sec:MSDFM}
In heterogeneous distillation, it can transfer more knowledge in addition to the logit level. Specifically, there is a huge gap in the feature maps between heterogeneous models. So it seems feasible to transfer feature-level knowledge in a latent logit space. In other words, feature maps at each stage of student are projected to logit space to be aligned. It can be viewed as training each stage as a separate student. However, the learning abilities of the student models at different stages are disparate because they have different numbers of parameters. It is manifestly inappropriate to assign them the same weighting for learning. So we propose a method that introduce class token in each stage of the student model, denoted as $x_{i}$ where $i$ is ordinal number of each stage, to dynamically balance weighting of each stage.

The student model is divided into four stages, each of which requires feature matching with the teacher model in logit space, as shown in Figure 2. For each forward inference, the student model outputs features of four stages $\{f_{i}\}_{i=1}^{4}$. We split $\{f_{i}\}_{i=1}^{4}$ into the class token $\{x_{i}\}_{i=1}^{4}$ for each stage and the architecture-independent feature information $\{\hat{f}_{i}\}_{i=1}^{4}$. After that, $\{\hat{f}_{i}\}_{i=1}^{4}$ is mapped to the logit space through the projector, denoted as $\{\hat{p}_{i}\}_{i=1}^{4}$. We use the global semantic information contained in the class token $\{x_{i}\}_{i=1}^{4}$ at each stage to dynamically balance the feature matching under logit space. We apply an MLP layer to generate the balancing weights. MLP layer can be represented as follows:
\begin{equation}
    \begin{aligned}
         X_{token} &= \operatorname{Stack}(\{x_{i}\}_{i=1}^{4}) \\
         X_{hidden} &= \operatorname{GELU}(\operatorname{Linear}(X_{token}))   \\
         W_{balance} &= \operatorname{Softmax}(\operatorname{Linear}(X_{hidden}))
    \end{aligned}
\end{equation}
where $\operatorname{Stack}(\cdot)$ denotes a stacking function for class token aggregation. $\operatorname{GELU}(\cdot)$ indicates an activation function. $\operatorname{Linear}(\cdot)$ is a fully connected layer. 
In MLP layer, the token sequence $\{x_{i}\}_{i=1}^{4}$ is compressed into the vector $X_{token}$. Then with a linear layer and softmax function, we can calculate the balancing weights $W_{balance}$. Further, we use dot product to balance the $\{\hat{p}_{i}\}_{i=1}^{4}$, as
\begin{equation}
    \begin{aligned}
        P_{stage} &= \operatorname{Stack}(\{\hat{p}_{i}\}_{i=1}^{4})\\
        Logit &= W_{balance}\cdot P_{stage}
    \end{aligned}
\end{equation}
where $Logit$ is logit output balanced by class token. Finally, we employ DFRA in Sec.~\ref{sec:DFRA} to minimize the gap between $Logit$ and teacher logit $p^{t}$.
\begin{equation}
    \begin{aligned}
        \mathcal{L}_{balance} = \operatorname{DFRA}(Logit, p^{t})
    \end{aligned}
\end{equation}

\subsubsection{Effectiveness Analysis}
In Fig.~\ref{fig:logit}, we compare the averaged prediction distribution of all samples of a single category among OFA-KD ~\cite{hao2024one}, RKD
~\cite{park2019relational}, and our MLDR-KD. Three categories are randomly selected.
By comparing each figure left in Fig.~\ref{fig:logit}, we can find that conventional RKD and our MLDR-KD both retain more dark knowledge than the previous heterogeneous distillation method OFA-KD.
However, as each figure right shows, conventional RKD reduces the confidence of the student model in the correct category, which leads to its poor performance in heterogeneous distillation. 
In contrast, the student model trained by our MLDR-KD has high confidence for the correct category while transferring abundant dark knowledge in the teacher model logit, which is consistent with our key observations.