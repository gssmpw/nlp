
\documentclass{article} % For LaTeX2e
%\usepackage{iclr2025_conference,times}
\usepackage[preprint]{tmlr}
% Optional math commands from https://github.com/goodfeli/dlbook_notation.
\input{math_commands.tex}

\usepackage{hyperref}
\usepackage{url}
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{booktabs}
\usepackage{enumitem}
\usepackage{multirow}
\usepackage{graphicx}
\usepackage{relsize}
\usepackage{subcaption}
\usepackage{mathtools, nccmath}
\usepackage[section]{placeins}
\usepackage{wrapfig}
\usepackage{xcolor}
\usepackage{diagbox}
\usepackage{ulem}


\newcommand{\gt}[1]{\textbf{\color{brown}[GT: #1]}}
\newcommand{\sg}[1]{\textbf{[SG: #1]}}

\title{How vulnerable is my policy? Adversarial attacks on modern behavior cloning policies}

% Authors must not appear in the submitted version. They should be hidden
% as long as the \iclrfinalcopy macro remains commented out below.
% Non-anonymous submissions will be rejected without review.

\author{ \thanks{Corresponding Author: \texttt{bpatil@umich.edu}} \\
Department of Robotics\\
University of Michigan, Ann Arbor\\
\AND
, ,  \\
 \\
University of Utah, Salt Lake City \\
% \texttt{\{dbrown\}@cs.utah.edu} \\
%\AND
%Coauthor \\
%Affiliation \\
%Address \\
%\texttt{email}
}
\author{\name Basavasagar Patil \email bpatil@umich.edu \\
      \addr Department of Robotics\\
      University of Michigan, Ann Arbor
      \AND
      \name Akansha Kalra \email akanshakalracmu@gmail.com \\
      \addr Kahlert School of Computing\\
      University of Utah
      \AND
      \name Guanhong Tao \email guanhong.tao@utah.edu\\
      \addr Kahlert School of Computing\\
      University of Utah
      \AND
      \name Daniel S. Brown \email daniel.s.brown@utah.edu\\
      \addr Kahlert School of Computing\\
      University of Utah}



% The \author macro works with any number of authors. There are two commands
% used to separate the names and addresses of multiple authors: \And and \AND.
%
% Using \And between authors leaves it to \LaTeX{} to determine where to break
% the lines. Using \AND forces a linebreak at that point. So, if \LaTeX{}
% puts 3 of 4 authors names on the first line, and the last on the second
% line, try using \AND instead of \And before the third author name.

\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}

% \iclrfinalcopy % Uncomment for camera-ready version, but NOT for submission.
\begin{document}


\maketitle

\begin{abstract}
Learning from Demonstration (LfD) algorithms have shown promising results in robotic manipulation tasks, but their vulnerability to adversarial attacks remains underexplored. This paper presents a comprehensive study of adversarial attacks on both classic and recently proposed algorithms, including Behavior Cloning (BC), LSTM-GMM, Implicit Behavior Cloning (IBC), Diffusion Policy (DP), and VQ-Behavior Transformer (VQ-BET).
We study the vulnerability of these methods to untargeted, targeted and universal adversarial perturbations.
While explicit policies, such as BC, LSTM-GMM and VQ-BET can be attacked in the same manner as standard computer vision models, we find that attacks for implicit and denoising policy models are nuanced and require developing novel attack methods.
Our experiments on several simulated robotic manipulation tasks reveal that most of the current methods are highly vulnerable to adversarial perturbations.
{
We also show that these attacks are transferable across algorithms, architectures, and tasks, raising concerning security vulnerabilities with potentially a white-box threat model.
% We find that the success rate of the transfer attacks is highly dependent on the task, raising necessity for more fine-grained metrics that capture both the task difficulties and baseline performance of the algorithms. 
In addition, we test the efficacy of a randomized smoothing, a widely used adversarial defense technique, and highlight its limitation in defending against attacks on complex and multi-modal action distribution common in complex control tasks.
In summary, our findings highlight the vulnerabilities of modern BC algorithms, paving way for future work in addressing such limitations.
}
\end{abstract}
\section{Introduction}
Learning from Demonstration (LfD) has emerged as a powerful paradigm in AI and robotics, enabling agents to acquire complex behaviors from expert demonstrations.
{These techniques are increasingly deployed in real-world scenarios, such as to enable robots in industrial automation and household robotics.}
% These techniques are increasingly being explored as effective methods to enable robots to be applied in real-world scenarios, such as industrial automation and household robotics.
However, these policies pose potential security risks since they can be easily maliciously manipulated by adversaries, causing undesired behaviors or even catastrophic incidents. Motivated by these risks, to the best of our knowledge, we are the first to  present a systematic study of the vulnerabilities of LfD algorithms to adversarial attacks with a focus on modern behavior cloning algorithms.
% For example, if an industrial robot were manipulated to place the wrong chemicals in containers, it could lead to a disastrous explosion.
% , understanding their vulnerabilities becomes crucial for ensuring robust and reliable deployment.
% \gt{Maybe add one or two examples showing its real-world applications and how it would impact the outcome if not executed correctly. This then can be connected to the following discussion why we want to study adv attacks in this domain.}\sg{addressed}
% uncovered severe security issues with the use of neural networks for machine learning. 
% \citet{Goodfellow2014ExplainingAH} developed a fast method for developing these adversarial perturbations using the gradient information, naming the method Fast Gradient Sign Method (FGSM). 
% Subsequently\citet{Madry2017TowardsDL} proposed a stronger version of the attack, by iteratively using the gradients to optimize the perturbations. 


Adversarial attacks  are a widely studied area that aims to develop imperceptible perturbations~\citep{Szegedy2013IntriguingPO} to change the output of machine learning models.
% Adversarial attacks have gained significant attention since the discovery of their vulnerability in deep neural networks, particularly in the domain of image classification. 
Early work by \cite{Szegedy2013IntriguingPO} and \cite{Goodfellow2014ExplainingAH} revealed that adding small, imperceptible perturbations to images could drastically alter the prediction of neural network classifiers.
Since then, a significant number of works have explored various attack methods and defense techniques against adversarial attacks~\citep{akhtar2018threat,zhang2020adversarial,chakraborty2021survey}.
% This led to the development of a variety of attack methods, including the Fast Gradient Sign Method (FGSM), Projected Gradient Descent (PGD), and others, which exploit these vulnerabilities to manipulate model outputs with minimal effort. 
% \gt{This paragraph is fine. But you may want to discuss what is the difference between existing domains and the one we are studying here. What kind of challenges we may encounter in our setting. This will establish the uniqueness and contribution of our work.}\sg{addressed}

However, the robustness of LfD models to adversarial attacks has been largely overlooked in prior research, particularly in the context of robotic manipulation tasks.
While previous works have examined adversarial robustness in reinforcement learning~\citep{Mo2023AttackingDR, Sun2020StealthyAE, Pattanaik2017RobustDR, Lin2017TacticsOA, Gleave2019AdversarialPA}, {including whitebox attacks~\citep{huang2017adversarial,casper2022white} and backdoor detection~\citep{chen2023bird}}, the impact of such perturbations on LfD remains underexplored.
% With few works on robot navigation \citep{Khedher2021AnalyzingAA}, robot manipulation \sg{citations needed}.
Attacks in the LfD domain present  {unique} challenges, %including temporal dependencies in sequential decision-making and the multimodal nature of actions in complex environments.
LfD policies must process temporal sequences of observations, handle multimodal action distributions, and maintain stable control even under perturbations.
% , which distinguishes LfD from other machine learning domains like image classification.
This paper aims to investigate the vulnerabilities specific to LfD algorithms under adversarial perturbations, shedding light on their susceptibility and resilience in robotic settings.
% In addition to pointing out to the safety vulnerabilities of these policies, 
% \sout{Additionally, the attacks developed in this study can be utilized for faster reliability estimation}. \textcolor{red}{I am going to expand and move this to future works, if we want to include this.}


Previous works such as \citet{9826387} show that adversarial patches can mislead a robotic arm's object detector, causing it to behave in an undesirable manner.
Unlike our work, {which examines} the vulnerabilities of modern LfD algorithms, they target traditional object detection models in industrial robots.
To the best of our knowledge, the only prior work closely related to this paper is by \citet{Chen2024DiffusionPA}, who explore adversarial attacks on diffusion policies.
Their method involves attacking the entire denoising process in the diffusion policy, which is computationally expensive.
By contrast, we demonstrate effective attacks by manipulating only a few steps of the denoising process, significantly reducing the attack cost (interms of time and compute).
In addition, while Chen et al. only focus on developing attacks for a single type of policy learning algorithm, our research examines the vulnerabilities of several different LfD algorithms and also explores how adversarial perturbations transfer across different {algorithms, tasks,} and and visual backbones, offering a broader and more comprehensive perspective on the vulnerability of modern behavior cloning and the generalizability of such attacks.


% However their approach does not 

% A closely related recent work is DP-Attacker \citep{Chen2024DiffusionPA}, released around the same time as our research, . While their approach is similar to our approach for diffusion models, presenting adversarial attacks across offline and online scenarios, but focuses exclusively on diffusion policies. 
% In contrast,

% \gt{May not need to explain simple algorithms like BC and LSTM-GMM. For IBC and diffusion policy these methods probably can be combined with the previous paragraph. For example, after the first sentence of the previous paragraph, you can then briefly explain IBC or diffusion policy and point out the challenges in generating adversarial examples for these algorithms.}

{Our work focuses specifically on post-deployment white-box attacks, where an adversary has access to the trained model parameters but cannot modify the training process. 
% This threat model is particularly relevant for open-source robotics systems where large pre-trained model weights are publicly available, a common practice in modern robotics research and deployment. 
Unlike training-time attacks that aim to corrupt the learning process, our attacks target the inference phase, attempting to cause task failures through carefully crafted perturbations to visual observations. While this may seem like a strong adversarial capability, the increasing trend toward open-source release of robotic policy weights makes this a practical concern that needs to be addressed.
}

In this paper, we evaluate the adversarial robustness of several leading LfD frameworks, including Vanilla Behavior Cloning (BC), LSTM-GMM~\citep{Mandlekar2021WhatMI}, Implicit Behavior Cloning (IBC)~\citep{florence2021implicit}, Diffusion Policy~\citep{chi2023diffusionpolicy}, and VectorQuantizied-Behavior Transformer (VQ-BET)~\citep{Lee2024BehaviorGW}.
% Vanilla Behavior Cloning (BC) is a straightforward approach where agents learn by directly imitating expert demonstrations, but it struggles with tasks requiring long-term dependencies. LSTM-GMM \citep{Mandlekar2021WhatMI} improves on this by integrating a Long Short-Term Memory (LSTM) network to capture temporal dynamics, paired with a Gaussian Mixture Model (GMM) to handle multimodal action outputs, making it more effective in sequential decision-making scenarios.
Among these, IBC and Diffusion Policy have unique design pipelines due to the fact that they are implicit rather than explicit policies and require more nuanced attacks. 
% that cause naive adversarial attacks to largely fail.
IBC employs energy-based models to learn implicit policies, offering greater flexibility in learning complex, multimodal behaviors compared to traditional methods. In IBC, the correct action is selected iteratively based on energy distribution rather than explicitly during inference.
% naive attacks struggle to target the correct action.} \textcolor{red}{I have some results for this for Lift but not for other environments} 
To address this, we introduce a sampling-based attack method that approximates the local energy surface, increasing the likelihood of selecting the desired target action.
Diffusion Policy, on the other hand, uses a generative model approach with denoising diffusion techniques to iteratively refine actions, allowing it to capture diverse and continuous action distributions. While existing attacks~\citep{Chen2024DiffusionPA} can degrade performance, they require manipulating the entire denoising process, leading to high compute costs of attack. One of our insights is that attacks are most effective at later stages of the denoising process. By applying attacks specifically only on later stages we can significantly improve attack efficiency.
Overall, we hope our results serve as an impetus for enhancing {awareness of the security and reliability concerns regarding policies learned via behavior cloning and will inspire researchers to develop more robust LfD algorithms.}

% Lastly, VQ-Behavior Transformer (VQ-BET) \citep{Lee2024BehaviorGW} combines vector quantization and transformer architectures, enabling it to model both high-level behavior and precise, fine-grained actions, making it well-suited for complex robotic tasks.  The fundamental challenge we aim to tackle was developing the adversarial objective for IBC and Diffusion Policy as these frameworks employ iterative resampling and iterative denoising respectively, for action selection during inference.


% Each of these algorithms possess unique strengths and face different challenges when subjected to adversarial perturbations. Understanding how these state-of-the-art methods respond to adversarial attacks provides key insights into their robustness and opens new avenues for improving LfD systems in real-world deployments.

The primary contributions of {our} work are as follows:
\begin{itemize}
    \item We conduct the first comprehensive study of white-box adversarial attacks on Learning from Demonstration (LfD) algorithms, encompassing both online (Projected Gradient Descent (PGD) \citep{Madry2017TowardsDL}) and offline (Universal Adversarial Perturbation \citep{MoosaviDezfooli2016UniversalAP}) attacks. We evaluate these attacks in both targeted and untargeted settings {and highlight the vulnerability of all the LfD algorithms studied in this paper}.
    \item We propose novel attack formulations for implicit models such as IBC and Diffusion Policy. Our work addresses the unique challenges posed by the iterative action selection process, representing one of the first successful attacks on these implicit policy models.
    \item {We provide insights into the transferability of attacks across algorithms, tasks, and vision backbones. We find surprising transferability of the attacks across all the variations, highlighting the importance of considering robustness in the training pipeline of modern imitation learning.}
    \item {We also show the limited efficacy of the commonly used adversarial defense,  Randomized Smoothing \citep{Cohen2019CertifiedAR}, and discuss the implications of using such defenses in real-time and multi-modal distributional settings such as robotics.}
    \item We find that, out of all the policies we test, Diffusion Policies are the most robust. {While recent work~\citep{carlini2023certified} has shown that combining a pretrained denoising diffusion probabilistic model and a standard high-accuracy classifier can yield robustness for image classifiers, we are the first to study and showcase the relative robustness of diffusion policies.} We provide evidence that this robustness might stem from its multi-step prediction process rather than inherent resilience. Our results show that reducing the prediction horizon significantly decreases the adversarial robustness of diffusion policies.
\end{itemize}

% \item We conduct a comprehensive evaluation to reveal the varying degrees of susceptibility of LfD algorithms to different types of attacks. Our results serve as an impetus for enhancing the security and reliability of robotic learning algorithms in real-world applications.

\vspace{-0.6cm}
\section{Behavior Cloning Algorithms}\label{BC_ALGORITHMS}
In this section, we provide a brief background on the Behavior Cloning (BC) algorithms we study in this paper. To provide clarity throughout the discussion, we first define some key notations used across these algorithms.
Let $\xi \in \Xi$ represent a set of expert trajectory demonstrations, where $\xi$ is a trajectory consisting of a sequence of state-action pairs $(s, a)$, sampled from an expert policy $\pi^*(s)$. Our objective in behavior cloning is to learn a policy $\pi_{\theta}(s)$, parameterized by $\theta$, that imitates the expert's behavior by minimizing a loss function $L$ that measures the difference between the expert actions and the  actions predicted by the learned policy.

Formally, for a given policy $\pi_{\theta}$, we aim to minimize: $\pi_{\theta}^* = \arg\min_{\pi_{\theta}} \sum_{\xi \in \Xi} \sum_{s \in \xi} L(\pi_{\theta}(s), \pi^*(s))$, where $L$ is typically the cross-entropy loss for discrete actions or mean squared error for continuous actions.
% \begin{equation}
% \pi_{\theta}^* = \arg\min_{\pi_{\theta}} \sum_{\xi \in \Xi} \sum_{s \in \xi} L(\pi_{\theta}(s), \pi^*(s)) \;,
% \end{equation}
% However, some methods leverage specialized losses: LSTM-GMM uses a log-likelihood objective to capture multimodal action distributions, while VQ-BET incorporates additional quantization losses. In Implicit Behavior Cloning (IBC), the loss is framed as a contrastive energy-based model, and Diffusion Policy relies on a loss function based on noise estimation in a denoising process. Despite these differences, the overarching goal remains the same: learning a policy that best imitates the expert’s demonstrations. We provide details about the algorithms used for testing adversarial robustness in Appendix \ref{BCAlgorithms}.


We study five different behavior cloning approaches in this work:

\textbf{Vanilla Behavior Cloning (BC)} learns a direct mapping from states to actions through supervised learning. Given state-action pairs $(s, a)$, it trains a neural network to minimize the mean squared error (continuous actions). While effective for simple tasks, BC struggles with multimodal behaviors and suffers from compounding errors during execution.

\textbf{LSTM-GMM} enhances the performance of BC by incorporating temporal dependencies through an LSTM network and modeling multimodal action distributions using Gaussian Mixture Models (GMMs). At each timestep $t$, the LSTM processes the state sequence and outputs GMM parameters, enabling the policy to capture both temporal dynamics and complex action distributions. The policy $\pi_\theta(a_t|s_t, h_{t-1})$ is trained by maximizing the likelihood of expert actions under the predicted GMM distributions.

\textbf{Implicit Behavior Cloning (IBC)} reformulates policy learning using energy-based models. Instead of directly predicting actions, IBC learns an energy function $E_\theta(s, a)$ that assigns low energy to expert actions and high energy to other actions. The policy is implicitly defined as $\pi_\theta(s) = \arg\min_a E_\theta(s, a)$ and is trained using contrastive learning to minimize the energy of expert actions relative to sampled negative actions.

\textbf{Diffusion Policy (DP)} takes a generative approach by learning to iteratively denoise actions. Starting from Gaussian noise $a_T$, the policy refines actions through $T$ denoising steps conditioned on the state: $a_{t-1} = \alpha(a_t - \gamma\varepsilon_\theta(s, a_t, t)) + \sigma\mathcal{N}(0, I)$, where $\varepsilon_\theta$ is a learned noise prediction network. This approach enables modeling of complex, continuous action distributions.

\textbf{VQ-Behavior Transformer (VQ-BET)} combines transformer architectures with vector quantization to handle multimodal continuous actions. The policy discretizes the action space using a hierarchical codebook and predicts actions by minimizing L1 loss. This approach balances expressiveness and tractability while capturing both coarse and fine-grained action details.

\section{Adversarial Attacks on Imitation Learning}
%We study two widely used adversarial attacks in our paper, namely, 
% Fast Gradient Sign Method (FGSM) \citep{Goodfellow2014ExplainingAH}, 
%Projected Gradient Descent (PGD) \citep{Madry2017TowardsDL} and Universal Adversarial Perturbations (UAP) \citep{MoosaviDezfooli2016UniversalAP}.
% FGSM applies a one-step gradient, computed from the attack-desired output prediction, to the input to craft adversarial perturbations. 
%PGD iteratively applies a projected Fast Gradient Sign Method (FGSM) attack~\citep{Goodfellow2014ExplainingAH}. UAP generates adversarial perturbations by considering multiple samples.
% Detailed explanations about these attacks are given in Appendix \ref{AdversarialAttacks}.
% \gt{I remember you adapt these attacks to LfD algorithms, right? I think the basic attack explanation can be put in a background section. And here you discuss what you have adapted to make them work on LfD models.}\sg{Addressed}

\subsection{Adversarial Attack Methods}

In our study, we focus on two primary adversarial attack methods that have shown significant effectiveness in computer vision tasks:

\textbf{Projected Gradient Descent (PGD)} \citep{Madry2017TowardsDL} is an iterative attack method that generates adversarial perturbations by taking multiple steps in the direction that maximizes the loss function. Starting from an initial input $x_0$, PGD iteratively updates the input using the following rule: $x_{k+1} = \Pi_{B_\epsilon(x_0)}(x_k + \alpha \cdot \sign(\nabla_x J(\theta, x_k, y)))$, where $J(\theta, x_k, y)$ is the model's loss function, $\alpha$ is the step size, and $\Pi_{B_\epsilon(x_0)}$ denotes projection onto the $\ell_p$-norm ball of radius $\epsilon$ centered at $x_0$. The projection step ensures that the perturbed input remains within the allowable perturbation bound. PGD is considered a strong attack as it explores the local loss landscape thoroughly through multiple iterations.

\textbf{Universal Adversarial Perturbations (UAP)} \citep{MoosaviDezfooli2016UniversalAP} aims to find a single perturbation vector that can cause misclassification when applied to multiple different inputs. Unlike PGD which computes perturbations for each input independently, UAP generates a single perturbation $v$ that satisfies $\|v\|_p \leq \epsilon$ and successfully affects a high fraction of the input distribution. The perturbation is computed iteratively over a set of training inputs by accumulating the minimal perturbation needed to cause misclassification for each input while maintaining the constraint on the perturbation magnitude. This approach is particularly relevant for real-world scenarios where computing per-input perturbations may not be feasible, and the same attack vector needs to work across multiple observations.

\textbf{Targeted vs Untargeted Attacks:} Targeted and untargeted attacks represent two distinct adversarial objectives. Untargeted attacks aim to degrade the policy's performance by maximizing the loss function, causing the model to output any incorrect action that leads to task failure. They focus on disrupting normal policy execution without specifying a particular alternative behavior. In contrast, targeted attacks attempt to force the policy to output specific undesired actions by minimizing the distance between the policy's output and a chosen target action. For example, in a robotic manipulation task, an untargeted attack might cause the robot to move erratically or freeze in place, while a targeted attack would intentionally guide the robot's motion toward a specific undesired location or trajectory. This distinction is important because targeted attacks often require more sophisticated optimization strategies to achieve precise control over the policy's behavior.
\vspace{-0.5cm}
\subsection{{Threat Model}}
{
Before describing our adapted attacks, we first clearly specify our threat model:
}

{
%\textbf{Adversary's Goal}: The attacker aims to cause task failure by perturbing visual observations during deployment, either through untargeted perturbations that disrupt normal policy execution or targeted perturbations that force specific undesired actions.


{
%\textbf{Adversary's Knowledge and Capabilities}: The attacker has white-box access to the trained policy parameters but cannot modify them. Perturbations are limited to the visual observation space (no direct action manipulation). Perturbations must remain within an $L_p$ norm ball of radius $\epsilon$ to maintain imperceptibility. The attacker can compute gradients through the entire policy network.
}

% {
% \textbf{Adversary's Limitations}: Cannot modify the policy training data nor the policies weights---the policy is assumed to be fixed and pre-trained.
%  Cannot directly manipulate robot actions or state.
%  Must maintain perturbations within specified bounds.
%  No access to internal robot states beyond visual observations.
% }

{
This threat model is particularly relevant for deployed robotic systems using large pre-trained policies, where model weights are publicly available but the training process is complete. We study both online attacks (PGD) that can adapt perturbations in real-time and offline attacks (UAP) that must generate a single fixed perturbation designed to work across all states.
}

{
\textbf{Adversary's Goal}: The attacker aims to cause task failure by perturbing visual observations during deployment, either through untargeted perturbations that disrupt normal policy execution or targeted perturbations that force specific undesired actions.
}

{
\textbf{Adversary's Knowledge and Capabilities}: The attacker has white-box access to the trained policy parameters but cannot modify them. Perturbations are limited to the visual observation space (no direct action manipulation). Perturbations must remain within an $L_p$ norm ball of radius $\epsilon$ to maintain imperceptibility. The attacker can compute gradients through the entire policy network.
}

{
% \textbf{Incase of Transferability}: 
{We would like to emphasize that, even though we consider whitebox attacks, we also study how adversarial perturbations can transfer across different models, architectures and tasks without requiring access to the target model's parameters or training data. In these kinds of transfer settings, an attacker can generate effective perturbations using only a surrogate model with similar architecture, making these attacks feasible even in black-box scenarios where the target model's internals are inaccessible. This property significantly broadens the threat surface, as successful attacks can be mounted with minimal knowledge of the target system.
}}

{
% This threat model is particularly relevant for deployed robotic systems using large pre-trained policies, where model weights are publicly available but the training process is complete. We study both online attacks (PGD) that can adapt perturbations in real-time and offline attacks (UAP) that must generate a single fixed perturbation designed to work across all states.
\subsection{Attacks on Explicit BC Algorithms}

The objectives for running PGD, and UAP on explicit behavior cloning methods such as Vanilla BC, LSTM-GMM, and VQ-BET are based directly on their respective loss functions. 
For Vanilla BC, the adversarial attacks aim to maximize the mean squared error (MSE) loss between the predicted and expert actions by introducing small perturbations to the input states. In LSTM-GMM, the attacks target the temporal dependencies modeled by the LSTM and the multimodal action distributions captured by the Gaussian Mixture Model (GMM), aiming to disrupt the likelihood maximization over the GMM outputs. For VQ-BET, the attacks exploit the latent action space by targeting the prediction loss of discrete latent codes, ultimately leading to suboptimal action predictions.
Each attack (PGD, UAP) thus aims to create adversarial perturbations that exploit the specific vulnerabilities of these loss functions to degrade performance. 
% Because these attacks are more traditional, we provide the details of the attacks on explicit BC models (Vanilla BC, LSTM-GMM, and VQ-BET) in Appendix \ref{Adv_vlv}. \sg{shall we provide details in appendix?}

\subsection{Attacks on Implicit BC Algorithms}
% In contrast to the explicit BC algorithms mentioned above, the adversarial objectives for implicit BC models are more involved. 

Implicit BC algorithms differ from explicit ones in modeling the learning process and action selection, causing naive adversarial attacks largely fail to generate feasible perturbations.
In this section, we propose new attack formulations for implicit BC models considering their unique designs.
% Attacks on Implicit Behavior Cloning and Diffusion Policy need to be formulated differently to account for the fact that these methods use iterative resampling and iterative denoising for action selection, respectively.
% In this section, we focus on the specific modifications in the formulation of the adversarial examples for IBC and Diffusion Policy.

\subsubsection{Implicit Behavior Cloning}\label{ibc_pgd_attack}
Implicit Behavior Cloning (IBC) leverages implicit modeling techniques and contrastive learning to learn a policy directly from expert demonstrations. 
In IBC, the policy \(\pi_\theta(\mathbf{a} \mid \mathbf{s})\) is learned by optimizing an energy-based model (EBM) that assigns low energy values to actions demonstrated by the expert and higher energy values to other actions. The energy function \(E_\theta(\mathbf{s}, \mathbf{a})\) parameterized by \(\theta\) is trained using the InfoNCE loss, for a batch of $N$ actions:

\begin{equation}
\mathcal{L}_{\text{InfoNCE}} = \sum_{i=1}^N -\log \left( \frac{e^{-E_\theta\left(\mathbf{s}_i, \mathbf{a}_i\right)}}{e^{-E_\theta\left(\mathbf{s}_i, \mathbf{a}_i\right)} + \sum_{j=1}^{N_{\text{neg.}}} e^{-E_\theta\left(\mathbf{s}_i, \tilde{\mathbf{a}}_i^j\right)}} \right)
% \vspace{-6mm}
\end{equation} 

where
$\tilde{\mathbf{a}}_i^j$ for $j=1,\ldots N_{\text{neg.}}$ are the negative samples {and $i = 1 \ldots N$ are the training samples in the batch.}
The parameters \(\theta\) are optimized by minimizing \(\mathcal{L}_{\text{InfoNCE}}\), encouraging the model to assign lower energy to expert actions compared to negative samples. This approach allows IBC to capture complex and multimodal action distributions, leading to more robust imitation of expert behaviors~\citep{florence2021implicit}.
Since IBC uses an implicit model with iterative sampling procedure for selecting actions, we need to develop specific formulations for untargeted and targeted attacks for these kinds of implicit models, specifically for the Derivative-Free Optimizer version of the inference. We summarize our attack in Algorithm \ref{alg:implicit_bc_inference_pgd}.

{For the targeted attack, a key challenge arises from the contrastive nature of IBC's loss function. Since the model can only minimize energy for actions present in the sample set, and these samples are drawn randomly, there is no straightforward way to directly optimize for a specific target action. The probability of the desired target action appearing in the negative samples is very low, making it difficult to construct an end-to-end loss function that reliably guides the model toward selecting this target action.}
Hence, we formulate the problem as finding the perturbation $\delta$ for a target action $\mathbf{a'}$ such that, $\tilde{p}_{\theta}(\mathbf{a'}|\mathbf{s + \delta}) > \tilde{p}_{\theta}(\mathbf{a}|\mathbf{s + \delta})$.
To achieve this, \textit{we introduce a sampling-based attack method to approximate the local energy surface}, making it easier to select the target action.
Specifically, we randomly sample a small number of negative actions, along with the target action, to estimate the energy surface around the target region. We also consider the original action during the attack.
% This is achieved by injecting the target action along with the original action and other negative samples, and 
we then iteratively perform gradient ascent to {decrease} the energy of the target action compared to both the original and negative actions.
However, in-order to further {increase} the probability of the target action being chosen during inference, we repeat this procedure $N_{iter}$ times to {decrease} the energy of the target action with respect to more actions that could possibly be selected due to their vicinity to the original actions. The details are presented in Algorithm~\ref{alg:implicit_bc_inference_pgd}.

For the untargeted attack, the objective is to perturb the state $s$  by finding a perturbation  $\delta$  that increases the energy (reduces the probability) of the actions that would normally be taken by the trained policy on clean state observations. In this case, we aim to push the model towards selecting less optimal actions by maximizing the energy associated with the learned actions in the perturbed state. To achieve this, we perform gradient ascent on the input pixels to maximize the energy of the correct action ($a_{clean}$ in Algorithm~\ref{alg:implicit_bc_inference_pgd}).
% \textbf{ As illustrated in Fig. \ref{fig:energy_hist}, the energy of actions selected through sampling in the perturbed state is significantly higher than the energy of actions selected in the original, non-perturbed state}. This indicates the effectiveness of the attack in pushing the model toward suboptimal decisions.
% \sout{Thus, the selected actions are essentially random actions with respect to the original state-action distribution.}
{Thus, by maximizing the energy of the correct action, the policy is forced to select alternative actions that are perturbed away from the intended action along different dimensions of the action space. While these selected actions maintain some structure from the original state-action distribution, the perturbations cause sufficient deviation to disrupt successful task execution.}

% Empirically we found this strategy to work best. However, due to inherent multi-modality of the distribution, this attack does not guarantee that the targeted action will be executed during the inference.  
%\begin{figure}
%    \centering
%    \includegraphics[scale=0.15]{figures/energy_histogram_10.pdf}
%    \caption{The energy values of the samples before and after perturbation of the state.}
%    \label{fig:energy_hist}
%\end{figure}


\begin{algorithm}[!hbt]
% \small
\footnotesize
\caption{Implicit BC PGD Attack}
\label{alg:implicit_bc_inference_pgd}
\begin{algorithmic}[1]
\Require Trained energy model $E_\theta(s, a)$, state $s$, observation $o$, number of samples $N_{samples}$, number of iterations $N_{iters}$, decay rate $K$, perturbation bound $\epsilon$, step size $\alpha$
% \State Initialize sample set from a Uniform Distribution, $\mathcal{S} = \{\tilde{a}^i\}_{i=1}^{N_{samples}} \sim \mathcal{U}(a_{min}, a_{max})$, $\sigma = \sigma_{init}$, $s' = s$
\State Obtain clean action $a_{clean}$ by running IBC on $s$
\For{$epoch = 1, 2, \ldots, N_{epochs}$} \Comment{Optimize over multiple samples}
    \State Initialize sample set $\mathcal{S} = \{\tilde{a}^i\}_{i=1}^{N_{samples}} \sim \mathcal{U}(a_{min}, a_{max})$
    \If {Targeted}
        \State Introduce $a_{clean}$, $a_{target}$ into $\mathcal{S}$
    \EndIf
    \For{$iter = 1, 2, \ldots, N_{iters}$} \Comment{Inner PGD attack iterations}
        \State $\{E_i\}_{i=1}^{|\mathcal{S}|} \gets \{E_\theta(s', \tilde{a}^i)\}_{i=1}^{|\mathcal{S}|}$ \Comment{Compute energies with perturbation}
        \State $\{\tilde{p}_i\}_{i=1}^{|\mathcal{S}|} \gets \left\{\frac{e^{-E_i}}{\sum_{j=1}^{|\mathcal{S}|} e^{-E_j}}\right\}_{i=1}^{|\mathcal{S}|}$ \Comment{Compute softmax probabilities}
        
        \If {Targeted}
            \State Compute cross-entropy loss with $a_{target}$ as the true label:
            \State $\text{Loss} = -\log(\tilde{p}_{\text{target}})$
            \Comment{$\tilde{p}_{\text{target}}$ is the probability of $a_{target}$}
        \Else
            \State Compute untargeted loss:
            \State $\text{Loss} = -E_{\theta}(s', a_{clean})$
            \Comment{Maximize energy of the correct action for untargeted attacks}
        \EndIf
        
        \State Update $s'$ using PGD step: 
        \State $s' = s' + \alpha \cdot (\nabla_{o_t} \text{Loss})_{\mathcal{B}_\epsilon}$
        \Comment{Projected on the $l_p$ norm ball}
    \EndFor
\EndFor
\State \textbf{return} $s'$
\end{algorithmic}
\end{algorithm}
% \vspace{-20mm}

\begin{algorithm}[!hbt]
% \small
\footnotesize
\caption{Diffusion Policy PGD Attack}
\label{alg:diffusion_policyattack}
\begin{algorithmic}[1]
\Require Observation horizon $T_0$, Action Horizon $T_a$, Prediction Horizon $T_p$, State sequence $\mathbf{S}_t = \{\mathbf{s}_{t-T_o+1}, \dots, \mathbf{s}_t\}$, number of denoising iterations $K$
\Ensure Action sequence $\mathbf{A}_t = \{\mathbf{a}_t, \dots, \mathbf{a}_{t+T_p-1}\}$
\State  $\mathbf{A}_t^{clean}$ = Diffusion Policy Inference($\mathbf{S}_t$)
\State $\mathbf{A}^{target}_t = \mathbf{A}_t^{clean} + \text{Desired Perturbations}$ \Comment{Only for Targeted Attack}
\State Initialize $T_{\text{attack}}$, $\epsilon$, $\alpha$, $\gamma$, $\sigma, N_{iters}$
\State Initialize $\mathbf{A}_t^{(K)} \sim \mathcal{N}(\mathbf{0}, \mathbf{I})$
\For{$k = K, K-1, \dots, 1$}
    \State $\mathbf{A}_t^{(k-1)} = \alpha(\mathbf{A}_t^{(k)} - \gamma \epsilon_{\theta}(\mathbf{S}_t, \mathbf{A}_t^{(k)}, k)) + \sigma \mathcal{N}(\mathbf{0}, \mathbf{I})$
    \If {$k < T_{\text{attack}}$}   \Comment{Attack during the last $K - T_{attack}$ timesteps}
            \For{$N_{iters}$}       \Comment{Inner PGD iterations}
                \If {Targeted}
                    \State $Loss = -{\rm MSELoss}(\mathbf{A}_t^{k-1}, \mathbf{A}^{{target}{(k-1)}}_t)$
                \Else
                    \State $Loss = {\rm MSELoss}(\mathbf{A}_t^{k-1}, \mathbf{A}^{{clean}{(k-1)}}_t)$
                \EndIf
                \State $\mathbf{S}_t = \mathbf{S}_t + \alpha \cdot \nabla_{\mathbf{O}_t}(Loss)_{\mathcal{B}_\epsilon}$ \Comment{Grad. ascent w.r.t current observations and project on $\epsilon$ ball.}
            \EndFor
    \EndIf
\EndFor
\State \textbf{return} $\mathbf{S}_t$
\end{algorithmic}
\end{algorithm}

\subsubsection{Diffusion Policy}
Diffusion Policy (DP)~\citep{chi2023diffusionpolicy} aims to overcome the necessity of approximating the normalizing constant (the negative samples required in the above IBC method) in an energy based model, by learning the score function of the action-distribution.

In particular, the score function is defined as the gradient of the log-conditional probability distribution of actions, which is usually learnt as a noise-prediction network ($\varepsilon_\theta$) parameterized by $\theta$.
\begin{equation}
\nabla_{\mathbf{a}} \log p(\mathbf{a} \mid \mathbf{s}) = -\nabla_{\mathbf{a}} E_\theta(\mathbf{s}, \mathbf{a}) 
%- \underbrace{\nabla_{\mathbf{a}} \log Z(\mathbf{s}, \theta)}_{\underset{\mathlarger{0}}{}} 
\approx -\varepsilon_\theta(\mathbf{s}, \mathbf{a})
\end{equation}
Starting from $\mathbf{a}^k$ sampled from Gaussian noise, DP iteratively denoises the sample $k$ times to get a desired noise-free sample $\mathbf{a}^0$.
\begin{equation}
\mathbf{a}^{k-1}=\alpha\left(\mathbf{a}^k-\gamma \varepsilon_\theta\left(\mathbf{s}, \mathbf{a}^k, k\right)+\mathcal{N}\left(0, \sigma^2 I\right)\right)
\end{equation}
where $\alpha, \gamma, \sigma$ are the hyper-parameters that collectively define the noise schedule.
The complete inference is defined in the Appendix. \ref{bc_policies}.

 Algorithm \ref{alg:diffusion_policyattack} shows our online attack method for Diffusion Policy. 
 Both targeted and untargeted attacks use Mean Squared Error (MSE) loss for propogation of gradient. 
For the targeted attack, we try to minimize the distance between our predicted action and the target action (line 10) by doing gradient descent. Whereas, in the untargeted attack, we try to maximize the distance between the predicted action and the clean action (line 12) by doing gradient ascent. Running this attack end-to-end during the whole denoising process can be costly, as we need to backpropogate through the entire network for each iteration for a single inference step.
However, we can reduce this computation by taking inspiration from prior work on image editing attacks~\citep{Salman2023RaisingTC} and only apply the perturbations during last timesteps (line 7 of Algorithm~\ref{alg:diffusion_policyattack}) of the denoising process. This enables us to avoid wasting attacks when the actions are very random (during the initial steps of denoising) and only apply the attack when the data has started to converge towards the mode. This significantly reduces the attack effort when compard with prior work~\citep{Chen2024DiffusionPA} ({in our case by 80\%, since we use $T_{attack}=80$ and $K=100$})  while not affecting the quality of adversarial attack.

\section{Experiments \& Results}
% \sg{mention that FGSM attacks are in the appendix and the other algorithms are also in the appendix somewhere.}

We design our experiments to the answer to following questions:
% \begin{itemize}[topsep=0pt]
    (1) How vulnerable are modern behavior cloning algorithms to adversarial attacks?
    (2) How transferable are the attacks across different algorithms {and different tasks}? 
    (3) What is the impact of different feature extraction backbones on attack performance, as in how transferable are the attacks between different vision architectures?
    (4) How does the action prediction horizon of the diffusion policy affect its vulnerability?
    {(5) How do adversarial defense techniques perform in our setting and what are the takeaways?}
% \end{itemize}



% \begin{figure}[!t]
%     \centering
%     \begin{subfigure}[b]{0.12\textwidth}
%         \centering
%         \includegraphics[width=\textwidth]{figures/lift2.png}
%         \caption{Lift Task}
%         \label{fig:image1}
%     \end{subfigure}%
%     \hfill
%     \begin{subfigure}[b]{0.12\textwidth}
%         \centering
%         \includegraphics[width=\textwidth]{figures/can2.png}
%         \caption{Can Task}
%         \label{fig:image2}
%     \end{subfigure}%
%     \hfill
%     \begin{subfigure}[b]{0.12\textwidth}
%         \centering
%         \includegraphics[width=\textwidth]{figures/square2.png}
%         \caption{Square Task}
%         \label{fig:image3}
%     \end{subfigure}
%     \hfill
%     % \vspace{1em}
%     % \begin{minipage}[b]{\textwidth}
%         \centering
%         \begin{subfigure}[b]{0.1\textwidth}
%             \centering
%             \includegraphics[width=\textwidth]{figures/pusht.png}
%             \caption{Push-T Task}
%             \label{fig:image4}
%         \end{subfigure}
%         \begin{subfigure}[b]{0.12\textwidth}
%             \centering
%             \includegraphics[width=\textwidth]{figures/TH.png}
%             \caption{Tool Hang Task}
%             \label{fig:image5}
%         \end{subfigure}
        
%     % \end{minipage}
%     \caption{Environments used to study adversarial robustness of modern behavior cloning algorithms. (a)-(c) and (e)  are from RoboMimic~\citep{Mandlekar2021WhatMI} and (d) is from \citet{florence2021implicit} }
%     \label{fig:robomimic_envs}
% \end{figure}




% \section{Experiments \& Results}
% % \sg{mention that FGSM attacks are in the appendix and the other algorithms are also in the appendix somewhere.}

% We design our experiments to the answer to following questions:
% % \begin{itemize}[topsep=0pt]
%     (1) How vulnerable are modern behavior cloning algorithms to adversarial attacks?
%     (2) How transferable are the attacks across different algorithms {and different tasks}? 
%     (3) What is the impact of different feature extraction backbones on attack performance, as in how transferable are the attacks between different vision architectures?
%     (4) How does the action prediction horizon of the diffusion policy affect its vulnerability?
%     {(5) How do adversarial defense techniques perform in our setting and what are the takeaways?}
% % \end{itemize}

\subsection{Environments}

\begin{figure}[!hbt]
    \centering
    \begin{subfigure}[b]{0.18\textwidth}
        \centering
        \includegraphics[width=\linewidth]{figures/lift2.png}
        \caption{Lift Task}
        \label{fig:image1}
    \end{subfigure}%
    \hspace{0.01\textwidth}
    \begin{subfigure}[b]{0.18\textwidth}
        \centering
        \includegraphics[width=\linewidth]{figures/can2.png}
        \caption{Can Task}
        \label{fig:image2}
    \end{subfigure}%
    \hspace{0.01\textwidth}
    \begin{subfigure}[b]{0.18\textwidth}
        \centering
        \includegraphics[width=\linewidth]{figures/square2.png}
        \caption{Square Task}
        \label{fig:image3}
    \end{subfigure}%
    \hspace{0.01\textwidth}
    \begin{subfigure}[b]{0.15\textwidth}
        \centering
        \includegraphics[width=\linewidth]{figures/pusht.png}
        \caption{Push-T Task}
        \label{fig:image4}
    \end{subfigure}%
    \hspace{0.01\textwidth}
    %\begin{subfigure}[b]{0.18\textwidth}
    %    \centering
    %    \includegraphics[width=\linewidth]{figures/TH.png}
    %    \caption{Tool Hang Task}
    %    \label{fig:image5}
    %\end{subfigure}
    \caption{Environments used to study adversarial robustness of modern behavior cloning algorithms. (a)-(c) are from RoboMimic~\citep{Mandlekar2021WhatMI} and (d) is from \citet{florence2021implicit} }
    \label{fig:robomimic_envs}
    
\end{figure}




% In order to answer these questions we evaluate our 3 attacks across 4 different frameworks on total 4 tasks from two different benchmarks. 


To  demonstrate the adversarial robustness of modern behavior cloning algorithms, we consider a diverse set of common benchmarks shown in Figure~\ref{fig:robomimic_envs}. The tasks of Lift, Can and Square are taken from Robomimic \citep{Mandlekar2021WhatMI}, where the state-of-the art frameworks such as Diffusion Policy and LSTM-GMM have been shown to have a nearly 100\% success rate in non-adversarial settings.
To further assess the ability of adversarial attacks to breach these frameworks on {a diverse task}, we consider the Push-T environment, first introduced by \citet{florence2021implicit} and then subsequently used by Diffusion Policy ~\citep{chi2023diffusionpolicy} and VQ-BET ~\citep{Lee2024BehaviorGW}. 
% {Descriptions and details of these tasks are included in Appendix~\ref{app:task-description}.} %\textcolor{red}{while we have some results for tool hang, I think Akansha has not finished it for all the algorithms. And I need to check the results again. If they ask for a different environment I can add franka kitchen as well later instead of tool-hang.}
\textbf{Lift} is a foundational manipulation task where a robot arm must lift a small cube from a table surface. \textbf{{Can}} is a manipulation task requiring the robot to transfer a soda can from a large source bin into a smaller target bin. 
\textbf{{Square}} is a high-precision manipulation task where the robot must pick up a square nut and insert it onto a vertical rod. \textbf{{Push-T}} is a contact-rich manipulation task where the robot must guide a T-shaped block to a fixed target location using a circular end-effector. 
Further descriptions and details of these tasks are included in Appendix~\ref{app:task-description}.

\subsection{Pretrained Policies}
To provide consistent and reproducible results, we attack the pre-trained checkpoints for LSTM-GMM, IBC and Diffusion Policy released by the authors of Diffusion Policy~\citep{chi2023diffusionpolicy} on these suite of tasks, and train our policies for Vanilla-BC and VQ-BET, due to absence of publicly available checkpoints. We evaluate all the environments on 50 randomly initialized environments across 3 different seeds for reporting the mean and standard deviation of the success rate. 
%All pre-trained policies and source code for generating and evaluating attacks and reproducing our results will be open-sourced at [\textit{url masked for anonymous submission}].

% RoboMimic provides over 46000 demonstrations including from proficient humans over several different tasks 

\subsection{\textbf{How vulnerable are modern behavior cloning algorithms to adversarial attacks?}} % est attack, FGSM to PGD to strongest, Universal Adversarial Pertubation ?}

%performance of PGD and univer pert across all/some tasks and frameworks in \textit{add references}. In Appendix \textit{add reference} we see a similar trend across the fgsm and pgd where we observe that since fgsm is a weaker attack, the mean score at test time of these frameworks is but the frameworks start to perform poorly as the strength of attack increases to pgd and universal adversarial perturbation.


To assess the vulnerability of modern behavior cloning algorithms to adversarial attacks, we conducted a comprehensive evaluation using both online (PGD) and offline (UAP) attack methods. Answering question (1) our findings, as illustrated in Figures \ref{fig:lift_attacks_comparison} and \ref{fig:pusht_attacks_comp}, reveal significant vulnerabilities in the adversarial robustness of current algorithms when faced with perturbations in the observation space.
Among the algorithms tested, VQ-BET demonstrated the highest susceptibility to adversarial perturbations. We hypothesize that this vulnerability stems from the discrete nature of its action space, which may lead to discontinuous decision boundaries. In contrast, algorithms employing iterative methods for action selection, such as IBC and Diffusion Policy, exhibited relatively higher robustness. This enhanced resilience can be attributed to the inherent stochasticity in their action selection processes during inference.
It is important to note that the effectiveness of these attacks varies depending on the complexity of the task environment. For instance, the Lift environment allows for a larger margin of error, making it more forgiving to substantial perturbations in actions. However, as task complexity increases, we observe a dramatic reduction in the {robot task} success rates {(increase in attack success rates)} across all algorithms. {For example, \citet{Mandlekar2021WhatMI} categorize the difficulty of the tasks with Lift being the easiest, Can being harder than Lift, and Square being harder than Can. As we increase the complexity of the task, we notice an increase in the efficacy of the adversarial attacks as detailed in Appendix~\ref{additional_environments}. 

\begin{figure}[!hbt]
    \centering
    \begin{subfigure}[b]{0.495\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figures/pgd_lift.pdf}
        \caption{PGD}
        \label{fig:pgd_lift_a}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.495\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figures/univ_pert_lift.pdf}
        \caption{UAP}
        \label{fig:pgd_lift_b}
    \end{subfigure}
    \caption{Comparison of PGD and UAP attacks for the Lift task. The y axis denotes the normal performance of the evaluated policies, which is the lower the better for attacks.}
    \label{fig:lift_attacks_comparison}
\end{figure}

\begin{figure}[!hbt]
    \centering
    \begin{subfigure}[b]{0.495\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figures/pgd_pusht.pdf}
        \caption{PGD}
        \label{fig:pgd_lift_a}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.495\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figures/univ_pert_pusht.pdf}
        \caption{UAP}
        \label{fig:pgd_lift_b}
    \end{subfigure}
    \caption{Comparison of PGD and UAP attacks for the Push-T task. The y axis denotes the normal performance of the evaluated policies, which is the lower the better for attacks.}
    \label{fig:pusht_attacks_comp}
\end{figure}


{Particularly concerning is our finding that these vulnerabilities persist even with minimal perturbations - even when epsilon is restricted to values as small as 4/256 (approximately 1.5\% of the input range), we observe substantial degradation in performance across most algorithms. This heightened sensitivity to small perturbations suggests a fundamental brittleness in current behavior cloning approaches, as even well-constrained adversarial attacks can significantly compromise policy performance (Fig. \ref{fig:eps_sensitivity} in Appendix \ref{eps_sensitivity}).}




\begin{table}[t]
\caption{Inter-Algorithm Transferability of Untargeted UAP on the Lift task, {where the rows correspond to the attacker policy over which perturbations were developed (random refers to a Gaussian noise with the mean of zero and std of epsilon) and the columns correspond to target policy over which attacks were tested.}}
\label{tab:confusion_table_lift}
\centering
\small
\begin{tabular}{c|ccccc}
\toprule
% & \multicolumn{5}{c}{Lift} \\
\diagbox{{Attacker}}{{Target Policy}} & Vanilla BC & LSTM-GMM & IBC & DiffusionPolicy-C & VQ-BET \\
\midrule
Random & 0.96 & 0.84 & 0.80 & 1.00  & 0.98\\
Vanilla BC & 0.00 & 0.00 & 0.80 & 1.00 & 0.94\\
LSTM-GMM & 0.94 & 0.00 & 0.72 & 1.00 & 0.96 \\
IBC & 1.00 & 0.10 & 0.64 & 1.00 & 0.98\\
DiffusionPolicy-C & 0.82 & 0.22 & 0.78 & 0.00 & 0.94\\
VQ-BET & 0.94 & 0.50 & 0.84 & 1.00 & 0.00\\
\bottomrule
\end{tabular}
\end{table}

\begin{table}[t!]
\caption{Inter-Algorithm Transferability of Untargeted UAP on the Push-T task.}
\label{tab:confusion_table_pusht}
\centering
\small
\begin{tabular}{c|ccccc}
\toprule
\diagbox{{Attacker}}{{Target Policy}} & Vanilla BC & LSTM-GMM & IBC & DiffusionPolicy-C & VQ-BET \\
\midrule
Random & 0.60 & 0.61 & 0.64 & 0.82 & 0.55\\
Vanilla BC & 0.10 & 0.08 & 0.41 & 0.80 & 0.22\\
LSTM-GMM & 0.15 & 0.09 & 0.33 & 0.78 & 0.31 \\
IBC & 0.14 & 0.08 & 0.14 & 0.71 & 0.17\\
DiffusionPolicy-C & 0.27 & 0.10 & 0.24 & 0.14 & 0.22\\
VQ-BET & 0.26 & 0.14 & 0.47 & 0.61 & 0.08\\
\bottomrule
\end{tabular}
\end{table}

\begin{table}[!hbt]
\caption{{Multi-Task Transferability of the Universal Pertubations}. In the table, we report the percentage decrease in the robot task completion rate compared to the non-attacked version.}
\label{tab:mt_transferability}
\centering
\begin{tabular}{l|c|c|c|c|c}
\toprule
\textbf{Algorithm $\backslash$  Task } & \textbf{LIFT} & \textbf{CAN} & \textbf{LIFT-TO-CAN} &\textbf{SQUARE}  &  \textbf{LIFT-TO-SQUARE}\\
\midrule
Vanilla BC & 100\% & 100\% & 50\% & 100\% & -40\% \\
LSTM-GMM & 100\% & 100\% & 40\% &100\% & 38.9\%\\
IBC\textsuperscript{*}  & 7.89\% & 100\% &  100\% & 100\%&  100\% \\
DiffusionPolicy-C & 100\% & 100\% & 45\% & 100\% & 6.6\% \\
VQ-BET & 100\% & 100\% & 0\% & 100\%& 11.4\% \\
\bottomrule
\end{tabular}
\\\small\textsuperscript{*}IBC has very low (almost zero) performance on the Can and Square task, so the above metric may not capture the full picture for (only) IBC.
\end{table}




\subsection{\textbf{Can adversarial examples transfer across different algorithms and tasks?}}

%\textbf{Our hypothesis here was that since these frameworks have very distinct inference strategies and training loss , the  universal pertubation attack developed for one framework should not be able to transfer and generalize to the other frameworks}. Tables ~\textit{add references} across tasks of Lift and Square provide evidence that our hypothesis is valid. For more tasks, please refer to appendix \textit{addreference.}

%All of the algorithms tested although share a similar image encoder, Resnet-18 \citep{He2015DeepRL}, are however trained with substantially different loss functions as mentioned in section \ref{BC_ALGORITHMS}. 
%This difference in loss function can induce different features and since these models are trained in an end-to-end manner with no interpretable intermediate representations the separation between vision module and the action module is not clear. 
%We initially hypothesized that adversarial perturbations (in this case Untargeted UAPs) should not be transferable among the algorithms due to inductive biases induced by the algorithms for finding useful features. However, based on the results we observe that while this hold true for most of the algorithms in an easy environment like Lift, however, as we move to difficult environments we observe transfer of adversarial perturbations between algorithms.
%Based on the observations and empirical evidence, we hypothesize that although we develop UAPs on entire training dataset certain state-action pairs are more adversarially vulnerable and these are shared among the algorithms and is a property of the environment itself.
%Thus offline adversarial perturbations although applied throughout the trajectory during inference overfit to these vulnerable state-action pairs.

The transferability of adversarial examples across different behavior cloning algorithms presents an intriguing phenomenon, given the substantial differences in their loss functions and training methodologies (as detailed in Section \ref{BC_ALGORITHMS}). While these algorithms share a common image encoder (ResNet-18), their end-to-end training approach results in distinct feature representations that are not easily interpretable.
% The transferability of adversarial examples across different behavior cloning algorithms presents an intriguing phenomenon, given the substantial differences in their loss functions and training methodologies (as detailed in Section \ref{BC_ALGORITHMS}). While these algorithms share a common image encoder (ResNet-18), their end-to-end training approach results in distinct feature representations that are not easily interpretable.

{In simpler environments like the Lift task (see Table~\ref{tab:confusion_table_lift}), where baseline success rates are high ($>$90\% for most algorithms), we observed limited transferability with relatively small proportional drops in performance, aligning with our initial expectations. Intriguingly, in the PushT environment (Table~\ref{tab:confusion_table_pusht}) we noticed high-transferability of attacks between algorithms except for Diffusion Policy and also as we progressed to more complex environments (e.g., Square in Table~\ref{tab:confusion_table_square} in the Appendix), where baseline success rates are lower and tasks are naturally less robust to action perturbations, we noticed that transferred attacks often caused larger proportional drops in performance relative to the baseline.}
% This analysis highlights the importance of considering relative performance metrics when evaluating transferability across tasks of different complexity. Future work could benefit from developing normalized metrics that better account for task difficulty and baseline performance.

% \section{Inter-Task Transerability}\label{multi_task_transferability}
% We investigate the transferability of untargeted Adversarial Perturbation attacks developed in one environment to unseen new environments. We use the attacks developed for the Lift task across all algorithms and measure their ability to impact performance of the respective algorithms in both the Can and Square tasks. 
% For every task, we report the percentage decrease in the robot task completion rate compared to the non-attacked version. 


To answer the question regarding whether the attacks developed for one environment can transfer to other environments keeping the algorithm fixed, we evaluated the transferability of the attacks across environments. 
Our results in Table~\ref{tab:mt_transferability}, where we report the percentage decrease in the robot task completion rate compared to the non-attacked version show that the attacks developed in Lift can transfer to both the other environments (Can and Square).
However in rare case of BC for Square, we see an unexpected increase in performance when attacked using the attack developed for Lift environment but this could be due to random initialization of environments and the time constraint for testing only 3 seeds for each algorithm. It could also be due to the fact that adding a small amount of action noise to policies can sometimes increase performance by helping the policy get unstuck.


\subsection{\textbf{What is the impact of different feature extraction backbones to attack performance?}}
% Can we leverage the attacks designed for one vision architecture to attack another architecture ?}
We further wanted to gain insights about the transferability of attack with different vision-backbones used for policy learning.
In-order to test this, we developed perturbations using ResNet-18 as the backbone and then deployed these attacks on policies that were trained using ResNet-50, without regenerating the attacks. This cross-architecture transfer scenario yielded surprising results. In the Lift task (see Table~\ref{tab:algorithm-comparison-lift}), we observed high transferability for some algorithms (e.g., LSTM-GMM and IBC), while others showed more resilience (e.g., Diffusion Policy-C and VQ-BET). The more complex Push-T task (see Table~\ref{tab:algorithm-comparison-pusht}) demonstrated a more consistent pattern of partial transferability across all algorithms.

\begin{table}[!hbt]
\caption{Inter-Architecture Transferability. Transferring adversarial perturbations generated on ResNet-18 to ResNet-50 as backbone on the Lift task. NA: No Attack}
\label{tab:algorithm-comparison-lift}
\centering
\small
\begin{tabular}{l|c|c|c|c}
\toprule
\textbf{Algorithm} & \textbf{NA Resnet-18} & \textbf{NA Resnet-50} & \textbf{Resnet-18} & \textbf{Resnet-50}\\
\midrule
Vanilla BC & 1.00 & 1.00 & 0.21 &  0.75\\
LSTM-GMM & 1.00 & 1.00 & 0.00 & 0.25\\
IBC & 0.95 & 0.50 & 0.85 & 0.38\\
DiffusionPolicy-C & 1.00 & 1.00 & 0.00 & 1.00\\
VQ-BET & 1.00 & 1.00 & 0.00 & 0.98 \\
\bottomrule
\end{tabular}
% \vspace{1ex}
% \begin{tabular}{l}
% \multicolumn{1}{l}{\footnotesize NA: No Attack}
% \end{tabular}
\end{table}


\begin{table}[!hbt]
\caption{Inter-Architecture Transferability. Transferring adversarial perturbations generated on ResNet-18 to ResNet-50 as backbone on the Push-T task.}
\label{tab:algorithm-comparison-pusht}
\centering
\small
\begin{tabular}{l|c|c|c|c}
\toprule
\textbf{Algorithm} & \textbf{NA Resnet-18} & \textbf{NA Resnet-50} & \textbf{Resnet-18} & \textbf{Resnet-50}\\
\midrule
Vanilla BC & 0.72 & 0.62 & 0.09 & 0.20\\
LSTM-GMM & 0.72 & 0.56 & 0.08 & 0.21\\
IBC & 0.74 & 0.57 & 0.63  & 0.27 \\
DiffusionPolicy-C & 0.88 & 0.78 & 0.14 & 0.54\\
VQ-BET & 0.62 & 0.65 & 0.08 & 0.29\\
\bottomrule
\end{tabular}
\end{table}

Notably, in many cases, the ResNet-50 models showed vulnerability to attacks developed for ResNet-18, suggesting that simply increasing model capacity does not guarantee improved robustness against cross-architecture attacks. It also highlights the existence of shared vulnerabilities across different network architectures, which adversarial perturbations can exploit even when transferred to a different backbone.
These results underscore the importance of considering cross-architecture vulnerabilities in the design of robust behavior cloning systems.

\subsection{\textbf{How does the action prediction horizon of the diffusion policy affect its vulnerability?}}

\begin{wrapfigure}{r}{0.42\textwidth}
%\begin{figure}[!hbt]
    \centering
    % \vspace{-25pt}
    \includegraphics[width=0.8\linewidth]{figures/test_scores_vs_epochs.pdf}
    \caption{Test mean score vs epochs for action prediction horizon of 16 for lift, and various action horizons during Untargeted UAP.}
    \label{fig:diffusion_action_horizon}
    % \vspace{-40pt}
%    \end{figure}
\end{wrapfigure}

In addition to the above experiments, we find an interesting trade-off between the action horizon of the Diffusion Policy and robustness. In Fig. \ref{fig:diffusion_action_horizon}, we observe that as the action horizon increases (the number of actions taken at a time), while keeping the prediction horizon the same, the policy shows increasing robustness to universal attack. We hypothesize that as the action horizon increases the number of times the perturbed observation gets observed decreases thus allowing for smaller compounding errors during the inference.

However, if the action horizon is too long then the latency and recovering from sub-optimal trajectories might lead to worse overall performance.





\subsection{\textbf{How do adversarial defense techniques perform?}}
{To evaluate the applicability of adversarial defenses from computer vision to robotic control, we investigate Randomized Smoothing \citep{Cohen2019CertifiedAR} (we provide further details about the defense technique and hyperparameters in Appendix \ref{randomized_smoothing}), a widely adopted defense technique, against our strongest attack method (PGD). Our experimental results, presented in Tables \ref{tab:rs_lift} and \ref{tab:rs_pusht}, reveal that the effectiveness of randomized smoothing varies significantly across different algorithms and task domains.}

{
In the Lift task, we observe a spectrum of improvements in robustness. Most notably, Diffusion Policy and VQ-BET demonstrate remarkable enhancement in performance, with the task success rate with randomized smoothing increasing from 25\% to 98\% and 8\% to 66\% respectively. Vanilla BC exhibits moderate improvements, with success rates increasing from 48\% to 52\%, respectively. IBC also shows substantial gains, improving from 21\% to 50\% success rate, while LSTM-GMM demonstrates minimal benefits.}
{
The Push-T task, however, presents a markedly different scenario. The benefits of randomized smoothing are considerably attenuated across most algorithms, with IBC being the slight exception, showing improvement from 38\% to 50\% success rate. The limited effectiveness in this task may be attributed to the inherent multi-modal nature of the action distribution in the Push-T environment \citep{Lee2024BehaviorGW}. In such cases, averaging predictions can lead to actions that lie between valid modes of the distribution, resulting in suboptimal behavior.}

{
This discrepancy in effectiveness across tasks suggests that the utility of randomized smoothing is highly task-dependent. The defense appears more effective in tasks with simpler, unimodal action distributions, while its benefits diminish in scenarios involving complex, multi-modal action spaces. These findings highlight the need for defense mechanisms specifically designed for robotic control tasks that can maintain effectiveness across varying degrees of task complexity and action space characteristics.
}
\begin{table}[!t]
\caption{Comparison of the randomized smoothing on the algorithmic performance for the lift task.}
\label{tab:rs_lift}
\centering
\begin{tabular}{l|c|c|c|c}
\toprule
\textbf{Algorithm} & \multirow{2}{*}{\textbf{NA}} & \textbf{NA Randomized} & \textbf{PGD} & \textbf{Randomized}\\
& & \textbf{Smoothing} & \textbf{Attack} & \textbf{Smoothing with PGD}\\
\midrule
Vanilla BC & 1.00 & 1.00 & 0.48 & 0.52\\
LSTM-GMM & 1.00 & 0.93 & 0.00 & 0.00 \\
IBC & 0.95 & 0.80 & 0.21 & 0.50\\
DiffusionPolicy-C & 1.00 & 1.00 & 0.25 & 0.98\\
VQ-BET & 1.00 & 1.00 & 0.08 & 0.66\\
\bottomrule
\end{tabular}
\end{table}


\begin{table}[!t]
\caption{Comparison of the randomized smoothing on the algorithmic performance for the Pusht task.}
\label{tab:rs_pusht}
\centering
\begin{tabular}{l|c|c|c|c}
\toprule
\textbf{Algorithm} & \multirow{2}{*}{\textbf{NA}} & \textbf{NA Randomized} & \textbf{PGD} & \textbf{Randomized}\\
& & \textbf{Smoothing} & \textbf{Attack} & \textbf{Smoothing with PGD}\\
\midrule
Vanilla BC & 0.74 & 0.74 & 0.08 & 0.08\\
LSTM-GMM & 0.66 & 0.54 & 0.00 & 0.00\\
IBC & 0.68 & 0.67 & 0.38 & 0.50\\ 
DiffusionPolicy-C & 0.88 & 0.84 & 0.23 & 0.24\\
VQ-BET & 0.72 & 0.71 & 0.10 & 0.10\\
\bottomrule
\end{tabular}
\end{table}
}


%\section{Discussion}
%\subsection{Multi-step prediction explains Diffusion Policy apparent robustness}
%Diffusion policy arguably performs much better than most of the algorithms in terms of robustness to adversarial perturbation. However, there is nothing fundamentally unique about its objective that would make it more robust to perturbations, as the diffusion process is applied to action space not the input image space. This led us to think that the multi-step prediction that diffusion policy uses might be to blame. For the lift task, diffusion policy predicts upto 16 timesteps of action and executes the first 8 timesteps. So we trained a new model which predicts 4 timesteps in the future and executes first 2 timesteps and made sure that the policy achieves a score of 1.00 and is robust to random perturbation in both the action and image space similar to original trained policy. Applying Universal perturbation to this policy reduces the success rate of the policy to 0.00 in both targeted and untargeted attacks. 



%\subsection{Explanation for BETs bad performance and its adversarial robustness to targeted attacks}
%Targeted attack on BET although is able to reduce the success rate is subjected to noisy loss function for targeted universal adversarial attack during optimization process. This is due to focal loss function which makes the loss function discontinuous. In the ideal case the optimization process should affect the representations in such a way that it is similar to a moving the block in the direction of the targeted action perturbations but the state representations 

%\subsection{Non-transferability of attacks}
%The non-transferability of attacks among different algorithms can be understood by the fact that different loss functions induce different loss landscape. This is different from computer vision domain where the main difference is the architecture and not the loss function. The latter difference is more difficult for transferability of the attack as seen by the gradient alignment between different these attacks.



\section{Conclusion \& Future Work}
Our results show that modern behavior cloning  algorithms are vulnerable to adversarial attacks. Interestingly, implicit policies such as Implicit Behavior Cloning and Diffusion Policy seem to be more robust than the explicit policies. However, our results also demonstrate that the attack success rate is dependent on the task. As tasks gets harder, it becomes easier to attack these algorithms.
This also holds true based on the results from transferability of attacks between different algorithms. 
%as we can see in the Push-T, Square and Can environments.
Our results provide evidence that the different algorithms as well as the same algorithm trained with a different architecture are learning some similar features that are not completely orthogonal. Thus posing a security challenge since even if we are using different vision encoders, {task}, or policy these perturbations are transferable.
% We provide videos of the attack and the anonymized code for reproducing our work temporarily, under this link for anonymity: https://tinyurl.com/5xwd6ejz. 

We believe that our work lays foundation for future work in the direction of adversarial robustness of LfD policies. 
Future work includes designing better metrics to capture the nuanced effects of adversarial attacks on trajectories, rather than relying solely on success rates. Such metrics could provide deeper insights into the uncertainty in the state-action distributions learned by the policies.
Furthermore, while a much progress on adversarial robustness has been made in computer vision, the sequential nature of robotic LfD policies and the complex relationship between  vision representations and resulting actions (especially for implicit policies such as IBC and Diffusion Policy) provides an underexplored and exciting are for future research into both vulnerabilities and defenses. In-addition, adversarial attacks with respect to the physical parameters can provide reliability estimates and better coverage of state-action distribution.

\section{Broader Impact}
This work reveals significant vulnerabilities in behavior cloning algorithms that could affect the safe deployment of robotic systems in real-world applications. While exposing these security weaknesses could potentially enable malicious exploitation, we believe their disclosure is crucial for developing robust safety measures before widespread deployment in sensitive environments like industrial automation or healthcare robotics. Our systematic evaluation provides concrete guidance for developing more secure systems.
Our goal is to ensure reliable and safe operation of robotic systems by identifying and addressing potential vulnerabilities before deployment, while encouraging the research community to prioritize adversarial robustness in future algorithm development.

% {While adversarial defenses such as Randomized Smoothing can help in increasing the robustness, it comes at the cost of large increase in the reaction time and may struggle when the action distribution exhibits multi-modality. %Additional defenses such as Adversarial Training \citep{Goodfellow2014ExplainingAH} are left for future work.}

%\subsubsection*{Author Contributions}
%If you'd like to, you may include  a section for author contributions as is done
%in many journals. This is optional and at the discretion of the authors.

%\subsubsection*{Acknowledgments}
%Use unnumbered third level headings for the acknowledgments. All
%acknowledgments, including those to funding agencies, go at the end of the paper.


\bibliography{iclr2025_conference}
\bibliographystyle{iclr2025_conference}

\appendix

% \section{Appendix}
\newpage
%\section{Behavior Cloning Algorithms}\label{BCAlgorithms}
%\subsection{Vanilla Behavior Cloning}
%Vanilla Behavior Cloning (Vanilla BC) learns a policy via supervised learning~\citep{bain1995framework,torabi2018behavioral}. Given a dataset of state-action pairs $(s, a)$, it directly maps states to actions using a neural network trained to minimize the cross-entropy loss for discrete actions or mean squared error (MSE) for continuous actions. While effective for simple tasks, Vanilla BC struggles with tasks requiring long-term dependencies owing to the problem of compounding error and the tasks with multimodalilty in expert behavior, as it assumes a unimodal distribution over actions~\citep{ross2011reduction,florence2021implicit}.

%\subsection{LSTM-GMM}
% LSTM-GMM (Long Short-Term Memory with Gaussian Mixture Model)~\citep{Mandlekar2021WhatMI} enhances Vanilla BC by incorporating temporal dependencies through an LSTM network \citep{Hochreiter1997LongSM}. The LSTM processes a sequence of states $\{s_1, s_2, \ldots, s_T\}$ and predicts a multimodal distribution over actions using a GMM. Basically, the two components function together to model the temporal structure and to capture the  multimodal action distributions at each time step.
% The policy is trained by maximizing the likelihood of the observed actions given the state sequence:
% \begin{equation}
% \pi_{\theta} = \arg\max_{\theta} \sum_{\xi \in \Xi} \sum_{(s_t, a_t) \in \xi} \log p(a_t | s_t, \theta)
% \end{equation}
% where $p(a_t | s_t, \theta)$ is the probability of action $a_t$ under the GMM.

% \sg{below is the newer version}

%Long Short-Term Memory with Gaussian Mixture Model (LSTM-GMM)~\citep{Mandlekar2021WhatMI} enhances Vanilla BC by incorporating temporal dependencies through an LSTM network \citep{Hochreiter1997LongSM}. The LSTM processes a sequence of states ${s_1, s_2, \ldots, s_T}$ recursively, maintaining an internal hidden state $h_t$ at each time step. The policy $\pi_{\theta}(a_t|s_t, h_{t-1})$ is parameterized by the LSTM to model the temporal structure, while a GMM captures multimodal action distributions at each time step.
%At each time step $t$, the LSTM updates its hidden state and predicts a multimodal distribution over actions {: $h_t = \text{LSTM}(s_t, h_{t-1})$ and $p(a_t|s_t, h_{t-1}, \theta) = \text{GMM}(h_t)$. The policy is trained by maximizing the likelihood of the observed actions given the state sequence: $\pi_{\theta} = \arg\max_{\theta} \sum_{\xi \in \Xi} \sum_{t=1}^T \log p(a_t | s_t, h_{t-1}, \theta)$,}
%where $p(a_t | s_t, h_{t-1}, \theta)$ is the probability of action $a_t$ under the GMM, conditioned on the current state $s_t$ and the previous hidden state $h_{t-1}$.
% \begin{align}
% h_t &= \text{LSTM}(s_t, h_{t-1}) \\
% p(a_t|s_t, h_{t-1}, \theta) &= \text{GMM}(h_t)
% \end{align}
% \begin{equation}
% \pi_{\theta} = \arg\max_{\theta} \sum_{\xi \in \Xi} \sum_{t=1}^T \log p(a_t | s_t, h_{t-1}, \theta)
% \end{equation}

%\subsection{Implicit Behavior Cloning}
%Implicit Behavior Cloning (IBC)~\citep{florence2021implicit} reformulates the problem of policy learning as an energy-based model (EBM). Instead of explicitly predicting actions, IBC defines a compatibility score between states and actions using an energy function $E_{\theta}(s, a)$. The policy is implicitly represented by selecting actions that minimize the energy:{
%$\pi_{\theta}(s) = \arg\min_a E_{\theta}(s, a)$}
% \begin{equation}
% \pi_{\theta}(s) = \arg\min_a E_{\theta}(s, a)
% \end{equation}
%The model is trained using contrastive learning, where the energy of expert actions is minimized relative to negative (non-expert) samples. The training loss typically follows the InfoNCE objective, as discussed in more detail in section \ref{ibc_pgd_attack}.

%\subsection{Diffusion Policy}
%Diffusion Policy (DP)~\citep{chi2023diffusionpolicy} uses a novel generative approach to model action distributions by leveraging Denoising Diffusion Probabilistic Models~\citep{Ho2020DenoisingDP}. The policy is represented as a reverse diffusion process, which iteratively refines actions from Gaussian noise towards the true distribution. Given a noisy action $a_T$ sampled from a Gaussian distribution, the model iteratively denoises it using a learned denoising function conditioned on the state.
%The policy,{with $a_0$ being the final action obtained after denoising}, is  defined as: {$\pi_{\theta}(a_0 | s) = p_{\theta}(a_T | s) \prod_{t=1}^T p_{\theta}(a_{t-1} | a_t, s)$}
% \begin{equation}
% \pi_{\theta}(a_0 | s) = p_{\theta}(a_T | s) \prod_{t=1}^T p_{\theta}(a_{t-1} | a_t, s)
% \end{equation}


%\subsection{VQ-BET}
%The Vector Quantized Behavior Transformer (VQ-BET)~\citep{Lee2024BehaviorGW} combines a transformer-based architecture with vector quantization to handle multi-modal continuous action spaces. The policy discretizes actions into latent codes using a hierarchical quantization process, which allows the model to capture both coarse- and fine-grained action details. The model’s policy is formulated as a sequence prediction problem, where the transformer predicts discrete latent codes and continuous offsets for actions.

% \section{Adversarial Attacks}\label{AdversarialAttacks}\label{AdversarialAttacks}
% \subsection{Fast Gradient Sign Method (FGSM)}\label{FGSM}
% Fast Gradient Sign Method was proposed by \citet{Goodfellow2014ExplainingAH}.
% The basic idea behind FGSM is to use the linearity of neural networks to craft adversarial examples. 
% It is designed to be fast (on the $L_{\infty}$ space) instead of a more close or robust adversarial example.
% Given an image $x$ the method sets the adversarial example as,
% $$
% x' = x + \epsilon \cdot sign(\nabla loss_F(x))
% $$
% Intuitively, it tries to move each pixel by a small amount ($\epsilon$) with the direction determined by the sign of the gradient of the loss function wrt input.

% \subsection{Projected Gradient Descent (PGD)}\label{PGD}
% Projected Gradient Descent (PGD) is an iterative adversarial attack algorithm that generalizes the Fast Gradient Sign Method (FGSM) by applying multiple steps of gradient ascent to maximize the loss function with respect to the input, subject to a constraint on the perturbation magnitude. Mathematically, starting from an initial input \( \mathbf{x}_0 \), the PGD algorithm iteratively updates the input \( \mathbf{x}_{k+1} \) using the following rule:

% \[
% \mathbf{x}_{k+1} = \Pi_{\mathcal{B}_\epsilon(\mathbf{x}_0)} \left( \mathbf{x}_k + \alpha \cdot \text{sign}(\nabla_{\mathbf{x}} J(\theta, \mathbf{x}_k, y)) \right),
% \]

% where \( J(\theta, \mathbf{x}_k, y) \) is the loss function of the model with parameters \( \theta \), input \( \mathbf{x}_k \), and true label \( y \); \( \alpha \) is the step size; and \( \Pi_{\mathcal{B}_\epsilon(\mathbf{x}_0)} \) denotes the projection operator onto the \( l_p \)-norm ball \( \mathcal{B}_\epsilon(\mathbf{x}_0) \) of radius \( \epsilon \) centered at \( \mathbf{x}_0 \). The projection step ensures that the perturbed input remains within the allowable perturbation bound. When the number of iterations is set to one and the step size \( \alpha \) equals \( \epsilon \), PGD reduces to FGSM, which can be seen as a special case of PGD. The iterative nature of PGD allows it to find more effective adversarial perturbations compared to FGSM, making it a stronger attack method used in adversarial training to enhance model robustness.

% \subsection{Universal Adversarial Perturbations (UAP)}\label{UAP}
% Similar to \cite{MoosaviDezfooli2016UniversalAP}, we aim to find perturbations that are state-agnostic, such that a single perturbation can be applied to all the images to cause failure of the agent.
% To this end, we collect few samples of state-action pairs by rolling out our policy and optimizing the perturbations as a parameter to minimize the loss similar to our PGD attacks.

\section{Task Description}\label{app:task-description}
{We evaluate the vulnerability of behavior cloning methods on several manipulation tasks of varying complexity. }
\begin{itemize}
    \item \textbf{Lift}: {A foundational manipulation task where a robot arm must lift a small cube (4cm x 4cm x 4cm) from a table surface. The task tests basic pick-and-place capabilities and serves as an entry-level benchmark. Success is determined by elevating the cube above a threshold height. Initial cube poses are randomized with z-axis rotation within a small square region at the table center.}
    \item \textbf{Can}: {A manipulation task requiring the robot to transfer a soda can from a large source bin into a smaller target bin. This task presents increased difficulty over Lift due to the more complex grasping requirements of the cylindrical can and the constrained placement target. The can's initial pose is randomized with z-axis rotation anywhere within the source bin.}
    \item \textbf{Square}: {A high-precision manipulation task where the robot must pick up a square nut and insert it onto a vertical rod. This task significantly increases complexity by requiring precise alignment and complex insertion dynamics. The nut's initial pose is randomized with z-axis rotation within a square region on the table surface.}
    \item \textbf{Push-T:} {A contact-rich manipulation task adapted from \citep{florence2021implicit} where the robot must guide a T-shaped block to a fixed target location using a circular end-effector. The task requires precise control of contact dynamics, as the robot must strategically apply point contacts to maneuver the block along the desired trajectory. Unlike pick-and-place tasks, success depends on understanding and exploiting the complex dynamics of planar pushing. We evaluate using RGB image observations augmented with end-effector proprioception. Initial positions of both the T-shaped block and the end-effector are randomized to ensure learned policies must generalize across different pushing strategies.}
\end{itemize}
%\begin{algorithm}[H]
%\caption{Computation of Universal Perturbations for Behavior Cloning}
%\label{alg:universal_perturbations_bc}
%\begin{algorithmic}[1]
%\Require Data points $\mathcal{D} = \{(s_i, a_i^{target})\}_{i=1}^N$, behavior cloning model $\pi_\theta$, desired $\ell_p$ norm of the perturbation $\xi$
%\Ensure Universal perturbation vector $v$
%\State Initialize $v \gets 0$
%    \For{each datapoint $(s_i, a_i^{target}) \in \mathcal{D}$}
    
%        $a_i = \text{Algorithm}(s_i + v)$
        
%        $v = v - \alpha \cdot \nabla_{v} Loss(a_i, a_i^{target})$
%    \EndFor
%\State \textbf{return} $v$
%\end{algorithmic}
%\end{algorithm}
% \subsection{{Tool Hang}}\label{Square}

%{It's the most difficult task in robomimic suite, as it requires a robotic arm to assemble the frame consisting of a base piece and hook
%piece by inserting the hook into the base, and hang a wrench on the hook. This task at multiple stages necessitates precise, and dexterous, rotation-heavy movements. Initial position of the insertion hook as well as that of  ratcheting
%wrench and z-rotation  are randomized in a small square at the beginning of the
%episode.}
\section{Behavior Cloning Policies}\label{bc_policies}
\subsection{Implicit Behavior Cloning}
\begin{algorithm}[H]
\caption{Implicit BC Inference}
\label{alg:implicit_bc_inference}
\begin{algorithmic}[1]
\Require Trained energy model $E_\theta(s, a)$, observation $s$, number of samples $N_{samples}$, number of iterations $N_{iters}$, initial sampling std. dev. $\sigma_{init}$, decay rate $K$
\State Initialize $\{\tilde{a}^i\}_{i=1}^{N_{samples}} \sim \mathcal{U}(a_{min}, a_{max})$, $\sigma = \sigma_{init}$
\For{$iter = 1, 2, \ldots, N_{iters}$}
    \State $\{E_i\}_{i=1}^{N_{samples}} \gets \{E_\theta(s, \tilde{a}^i)\}_{i=1}^{N_{samples}}$ \Comment{Compute energies}
    \State $\{\tilde{p}_i\}_{i=1}^{N_{samples}} \gets \left\{\frac{e^{-E_i}}{\sum_{j=1}^{N_{samples}} e^{-E_j}}\right\}_{i=1}^{N_{samples}}$ \Comment{Compute softmax probabilities}
    \If{$iter < N_{iters}$}
        \State $\{\tilde{a}^i\}_{i=1}^{N_{samples}} \gets \text{Multinomial}(N_{samples}, \{\tilde{p}_i\}_{i=1}^{N_{samples}}, \{\tilde{a}^i\}_{i=1}^{N_{samples}})$ \Comment{Resample with replacement}
        \State $\{\tilde{a}^i\}_{i=1}^{N_{samples}} \gets \{\tilde{a}^i + \mathcal{N}(0, \sigma)\}_{i=1}^{N_{samples}}$ \Comment{Add noise}
        \State $\{\tilde{a}^i\}_{i=1}^{N_{samples}} \gets \text{clip}(\{\tilde{a}^i\}_{i=1}^{N_{samples}}, a_{min}, a_{max})$ \Comment{Clip to bounds}
        \State $\sigma \gets K\sigma$ \Comment{Shrink sampling scale}
    \EndIf
\EndFor
\State $i = \argmax\limits_i(\{\tilde{p}_i\}_{i=1}^{N_{samples}})$
\State \textbf{return} $\tilde{a}^i$
\end{algorithmic}
\end{algorithm}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

 \subsection{Diffusion Policy}\label{Diffusion Policy}
For Diffusion policy (Algorithm~\ref{alg:diffusion_policy_inference}) we use absolute positional actions as the original work shows that CNN-based diffusion policy performs poorly with robomimic's offical dataset, that uses veloocity control, as in the actions are represented as delta with respect to the current.
 
 \begin{algorithm}[H]
\caption{Diffusion Policy Inference}
\label{alg:diffusion_policy_inference}
\begin{algorithmic}[1]
\Require Observation horizon $T_0$, Action Horizon $T_a$, Prediction Horizon $T_p$, State sequence $\mathbf{S}_t = \{\mathbf{s}_{t-T_o+1}, \dots, \mathbf{s}_t\}$, number of denoising iterations $K$
\Ensure Action sequence $\mathbf{A}_t = \{\mathbf{a}_t, \dots, \mathbf{a}_{t+T_p-1}\}$
\State Initialize  $\alpha$, $\gamma$, $\sigma$
\State Initialize $\mathbf{A}_t^{(K)} \sim \mathcal{N}(\mathbf{0}, \mathbf{I})$
\For{$k = K, K-1, \dots, 1$}
    \State $\mathbf{A}_t^{(k-1)} = \alpha(\mathbf{A}_t^{(k)} - \gamma \epsilon_{\theta}(\mathbf{S}_t, \mathbf{A}_t^{(k)}, k)) + \sigma \mathcal{N}(\mathbf{0}, \mathbf{I})$
\EndFor
\State \textbf{return} $\mathbf{S}_t$
\end{algorithmic}
\end{algorithm}


% \begin{algorithm}[H]
% \caption{Diffusion Policy Inference}
% \label{alg:diffusion_policy_inference}
% \begin{algorithmic}[1]
% \Require State sequence $\mathbf{S}_t = \{\mathbf{s}_{t-T_o+1}, \dots, \mathbf{s}_t\}$,, number of denoising iterations $K$
% \Ensure Action sequence $\mathbf{A}_t = \{\mathbf{a}_{t}, \dots, \mathbf{a}_{t+T_p-1}\}$
% \State Initialize $\mathbf{A}_t^{(K)} \sim \mathcal{N}(\mathbf{0}, \mathbf{I})$ 
% \For{$k = K, K-1, \dots, 1$}
%     \State $\mathbf{A}_t^{(k-1)} = \alpha(\mathbf{A}_t^{(k)} - \gamma \epsilon_\theta(\mathbf{S}_t, \mathbf{A}_t^{(k)}, k)) + \sigma\mathcal{N}(\mathbf{0}, \mathbf{I})$
% \EndFor
% \State \textbf{return} $\mathbf{A}_t = \{\mathbf{a}_{t}, \dots, \mathbf{a}_{t+T_p-1}\}$
% \end{algorithmic}
% \end{algorithm}

% \section{Adversarial Attacks on Behavior Cloning Algorithms}\label{Adv_vlv}
% \subsection{Vanilla BC}
% \subsection{LSTM-GMM}
% \subsection{VQ-BET}

{
\section{Hyperparameters}\label{hyperparameters}

We adopt the following temporal horizons from Diffusion Policy:
\begin{itemize}
    \item Action prediction horizon ($T_p$): 16 steps
    \item Action execution horizon ($T_a$): 8 steps
    \item Observation context window ($T_o$): 2 steps
\end{itemize}

For the adversarial attacks, we use the following settings:
\begin{enumerate}
    \item Overall attack budget:
    \begin{itemize}
        \item $\varepsilon = 0.0625$ (16/256) $L_{\infty}$ norm (normalized to input range $[0,1]$)
        \item Perturbations are clipped to $[0,1]$ range
    \end{itemize}

    \item Framework-specific perturbation bounds:
    \begin{itemize}
        \item For standard BC frameworks on Robomimic: $[0.15, 0.15, 0]$ in $(x,y,z)$ directions for relative end-effector positions
        \item For Diffusion Policy on Robomimic: $[0.45, 0.45, 0]$ in $(x,y,z)$ directions for absolute end-effector positions. The larger perturbation magnitude accounts for the absolute position representation, compared to relative positions used in other frameworks
        \item For all the frameworks on Push-T: $[100, 100]$ for the two action dimensions.
    \end{itemize}

    \item PGD attack parameters:
    \begin{itemize}
        \item Number of iterations: 40
        \item Per-iteration step size ($\varepsilon_{\text{iteration}}$): 0.005
    \end{itemize}
\end{enumerate}
}
For IBC inference, we use derivative-free optimization with $N_{\text{samples}} = 1024$.

\subsection{{TARGET ACTION SELECTION}}\label{target_action_selection}
{For targeted attacks across all algorithms, target actions are generated by perturbing the expected clean actions:}
{
\begin{equation}
    a_{target} = a_{clean} + \delta_{action}
\end{equation}
}
{
where $a_{clean}$ is the action predicted by the unperturbed policy and $\delta_{action}$ is the desired action perturbation. For our experiments, we set $\delta_{action} = [0.15, 0.15]$ for perturbations in x and y directions for all frameworks except Diffusion Policy, where we use $\delta_{action} = [0.45, 0.45]$. These values were chosen to ensure the target actions remain within physically feasible bounds while being sufficiently different from the clean actions to potentially cause task failures. For PGD attacks, this target action computation is performed at each inference step using the current clean action prediction, while for UAP the target actions are computed once using the perturbed offline action trajectories.
}

\section{Results on Additional Environments}\label{additional_environments}

% \subsection{Lift Environments}
% \FloatBarrier
%\begin{table}[!htb]
%\caption{Adversarial attacks performance on Lift task}
%\label{tab:modified-lift}
%\vspace{1em}
%\centering
%\begin{tabular}{r|c|cc|cc|cc}
%\hline
%& \multirow{2}{*}{NA} & \multicolumn{2}{c|}{FGSM} & \multicolumn{2}{c|}{PGD} & \multicolumn{2}{c}{Universal Perturbations} \\
%& & Targeted & Untargeted & Targeted & Untargeted & Targeted & Untargeted \\
%\hline
%Vanilla BC & 0.94(0.02) & 0.76(0.13) & 0.88(0.10) & 0.23(0.09) & 0.32 (0.06) & 0.48(0.10)& 0.00(0.00) \\
%LSTM-GMM & 0.98(0.00) &  0.42 (0.16) & 0.51 (0.18) & 0.00(0.00) & 0.00(0.00) & 0.00(0.00) & 0.00 (0.00) \\
%IBC & 0.77 (0.11) & 0.69 (0.08) & 0.73(0.08) & 0.03 (0.02) & 0.12(0.06) & 0.68 (0.06) & 0.71(0.08) \\
%DiffusionPolicy-C & 1.00 (0.00) & - & - & 0.25 (0.06) & \textbf{0.86 (0.03) }& 0.92(0.02) & 0.00(0.00)\\
%VQ-BET & 1.00 (0.00) & 0.00 (0.00)& 0.70 & 0.00 (0.00)& 0.00 (0.00)& 0.00 (0.00) & 0.00(0.00)\\
%\hline
%\end{tabular}
%\end{table}

% \begin{table}[!htb]
% \caption{FGSM attack performance on Lift task}
% \label{tab:modified-lift}
% \vspace{1em}
% \centering
% \begin{tabular}{r|c|cc}
% \hline
% & \multirow{2}{*}{NA} & \multicolumn{2}{c}{FGSM} \\
% & & Targeted & Untargeted \\
% \hline
% Vanilla BC & 0.94(0.02) & 0.76(0.13) & 0.88(0.10) \\
% LSTM-GMM & 0.98(0.00) & 0.42(0.16) & 0.51(0.18) \\
% IBC & 0.77(0.11) & 0.69(0.08) & 0.73(0.08) \\
% DiffusionPolicy-C & 1.00(0.00) & - & - \\
% VQ-BET & 1.00(0.00) & 0.00(0.00) & 0.70 \\
% \hline
% \end{tabular}
% \end{table}



% \FloatBarrier


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Square Environment}\label{square_results}
\FloatBarrier

\begin{figure}[!htb]
    \centering
    \begin{subfigure}[b]{0.495\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figures/pgd_square.pdf}
        \caption{PGD attacks for square.}
        \label{fig:pgd_lift_a}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.495\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figures/univ_pert_square.pdf}
        \caption{Universal perturbation attacks for square.}
        \label{fig:pgd_lift_b}
    \end{subfigure}
    \caption{Comparison of PGD and universal perturbation attacks for square task.}
    \label{fig:square_attacks_comp}
\end{figure}

\begin{table}[h]
\caption{Inter-Algorithm Transferability of Universal Untargeted Perturbations for Square}
\label{tab:confusion_table_square}
\centering
\begin{tabular}{c|ccccc}
\hline
& \multicolumn{5}{c}{Square} \\
\diagbox{{Attacker}}{{Target Policy}} & Vanilla BC & LSTM-GMM & IBC & DiffusionPolicy-C & VQ-BET \\
\hline
Random & 0.42 & 0.36 & 0.00 & 0.98  & 0.64\\
Vanilla BC & 0.00 & 0.08 & 0.00 & 0.94 & 0.38\\
LSTM-GMM & 0.18  & 0.00 & 0.00 & 0.96 & 0.62 \\
IBC & 0.42 & 0.1 & 0.00 & 0.98 & 0.62\\
DiffusionPolicy-C & 0.00 & 0.00 & 0.00 & 0.00 & 0.32\\
VQ-BET & 0.26 & 0.00 & 0.00 & 0.98 & 0.00 \\
\hline
\end{tabular}
\end{table}



% \begin{table}[!htb]
% \caption{Inter-Architecture Transferability. Transferability of attacks trained with resnet-18 to resnet-50 as backbone for Square.}
% \label{tab:algorithm-comparison}
% \centering
% \begin{tabular}{l|c|c|c|c}
% \textbf{Algorithm} & \textbf{NA Resnet-18} & \textbf{NA Resnet-50} & \textbf{Resnet-18} & \textbf{Resnet-50}\\
% \hline
% Vanilla BC & 0.75 & 0.70 & 0.00 & 0.34\\
% LSTM-GMM & 1.00 & 0.25 & 0.00 & 0.00\\
% IBC & 0.09 & 0.00 & 0.00 & 0.00\\
% DiffusionPolicy-C & 1.00 & 0.875 & 0.00 & 0.30\\
% VQ-BET & 1.00 & 0.70 & 0.04 & 0.70\\
% \end{tabular}
% \end{table}
\FloatBarrier

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\subsection{Can Environment}\label{can_environment}
\FloatBarrier

\begin{figure}[!htb]
    \centering
    \begin{subfigure}[b]{0.495\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figures/pgd_can.pdf}
        \caption{PGD attacks for Can.}
        \label{fig:pgd_lift_a}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.495\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figures/univ_pert_can.pdf}
        \caption{Universal perturbation attacks for Can.}
        \label{fig:pgd_lift_b}
    \end{subfigure}
    \caption{Comparison of PGD and universal perturbation attacks for Can task.}
    \label{fig:can_attacks_comp}
\end{figure}

%\begin{table}[!htb]
%\caption{Adversarial attacks performance on \textbf{Can task}}
%\label{tab:modified-lift}
%\vspace{1em}
%\centering
%\begin{tabular}{r|c|cc|cc|cc}
%\hline
%& \multirow{2}{*}{NA} & \multicolumn{2}{c|}{FGSM} & \multicolumn{2}{c|}{PGD} & \multicolumn{2}{c}{Universal Perturbations} \\
%& & Targeted & Untargeted & Targeted & Untargeted & Targeted & Untargeted \\
%\hline
%Vanilla BC & 0.80(0.10) & 0.02(0.02) & 0.24 (0.10) & 0.00(0.00) & 0.00(0.00) & 0.00(0.00) & 0.00(0.00) \\
%LSTM-GMM & 1.00 (0.00) & 0.52(0.15) & 0.60 (0.13) & 0.00(0.00) & 0.00(0.00)  & 0.00(0.00) & 0.00(0.00) \\
%IBC & 0.07(0.02) & 0.00 (0.00) & 0.00(0.00) & 0.00(0.00) &  0.00(0.00) & 0.00(0.00) &  0.00(0.00) \\
%DiffusionPolicy-C & 1.00 (0.00)  &  &  & 0.00(0.00) & 0.00(0.00) & 0.00(0.00) & 0.00(0.00)\\
%VQ-BET & 0.96 (0.03) & 0.18(0.07) & 0.70 (0.03) & 0.05 (0.01)& 0.00 (0.00)& 0.00(0.00) %& 0.00(0.00) \\
%\hline
%\end{tabular}
%\end{table}

% \begin{table}[!htb]
% \caption{FGSM attack performance on Can task}
% \centering
% \label{tab:modified-can}
% \vspace{1em}
% \begin{tabular}{r|c|cc}
% \hline
% & \multirow{2}{*}{NA} & \multicolumn{2}{c}{FGSM} \\
% & & Targeted & Untargeted \\
% \hline
% Vanilla BC & 0.80(0.10) & 0.02(0.02) & 0.24(0.10) \\
% LSTM-GMM & 1.00(0.00) & 0.52(0.15) & 0.60(0.13) \\
% IBC & 0.07(0.02) & 0.00(0.00) & 0.00(0.00) \\
% DiffusionPolicy-C & 1.00(0.00) & & \\
% VQ-BET & 0.96(0.03) & 0.18(0.07) & 0.70(0.03) \\
% \hline
% \end{tabular}
%\end{table}
\begin{table}[!htb]
\caption{Inter-Algorithm Transferability of Universal Untargeted Perturbations for Can}
\label{tab:confusion_table_can}
\centering
\begin{tabular}{c|ccccc}
\hline
& \multicolumn{5}{c}{Can} \\
\diagbox{{Attacker}}{{Target Policy}} & Vanilla BC & LSTM-GMM & IBC & DiffusionPolicy-C & VQ-BET \\
\hline
Random & 0.62 & 0.94 & 0.00 & 1.00  & 0.96\\
Vanilla BC & 0.00 & 0.66 & 0.00 & 0.42 & 0.88\\
LSTM-GMM & 0.18 & 0.00 & 0.00 & 0.72 & 0.68\\
IBC & 0.72 & 0.98 & 0.00 & 1.00 & 0.92\\
DiffusionPolicy-C & 0.02 & 0.24 & 0.00 & 0.00 & 0.70\\
VQ-BET & 0.34 & 0.64 & 0.00 & 0.42 & 0.04\\
\hline
\end{tabular}
\end{table}

\begin{table}[!htb]
\caption{Inter-Architecture Transferability. Transferability of attacks trained with resnet-18 to resnet-50 as backbone for Can.}
\label{tab:algorithm-comparison}
\centering
\begin{tabular}{l|c|c|c|c}
\textbf{Algorithm} & \textbf{NA Resnet-18} & \textbf{NA Resnet-50} & \textbf{Resnet-18} & \textbf{Resnet-50}\\
\hline
Vanilla BC & 0.75 & 0.70 & 0.00 & 0.34\\
LSTM-GMM & 1.00 & 0.25 & 0.00 & 0.00\\
IBC & 0.09 & 0.00 & 0.00 & 0.00\\
DiffusionPolicy-C & 1.00 & 0.875 & 0.00 & 0.30\\
VQ-BET & 1.00 & 0.70 & 0.04 & 0.70\\
\end{tabular}
\end{table}
\FloatBarrier


{
\section{Randomized Smoothing}\label{randomized_smoothing}

Randomized smoothing is a technique used to enhance the robustness of deep neural networks against adversarial perturbations. The core idea is to smooth the model's predictions by averaging over multiple randomly perturbed versions of the input. For a given input state $s$, the smoothed policy $\tilde{\pi}(s)$ is defined as:

\begin{equation}
    \tilde{\pi}(s) = \mathbb{E}_{\varepsilon}[\pi(s + \varepsilon)], \quad \text{where} \quad \varepsilon \sim \mathcal{N}(0, \sigma^2I)
\end{equation}

During inference, we approximate this expectation by averaging predictions over $N$ randomly sampled perturbations:

\begin{equation}
    \tilde{\pi}(s) \approx \frac{1}{N} \sum_{i=1}^N \pi(s + \varepsilon_i), \quad \text{where} \quad \varepsilon_i \sim \mathcal{N}(0, \sigma^2I)
\end{equation}

\subsection{Implementation Details}\label{implementation_details}

For our experiments, we used:
\begin{itemize}
    \item Number of random samples ($N$): 100
    \item Noise standard deviation ($\sigma$):
    \begin{itemize}
        \item Lift task: $\sigma = 0.1$
        \item Push-T task: $\sigma = 0.05$
    \end{itemize}
\end{itemize}

The $\sigma$ values were carefully chosen through validation to maintain performance on clean (non-attacked) inputs while providing meaningful defense against adversarial perturbations.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% \subsection{PushT Environment}
% \FloatBarrier
%\begin{table}[!htb]
%\centering
%\caption{Adversarial attacks performance on PushT Task}
%\vspace{1em}
%\label{tab:modified-lift}
%\begin{tabular}{r|c|cc|cc|cc}
%\hline
%& \multirow{2}{*}{NA} & \multicolumn{2}{c|}{FGSM} & \multicolumn{2}{c|}{PGD} & %\multicolumn{2}{c}{Universal Perturbations} \\
%& & Targeted & Untargeted & Targeted & Untargeted & Targeted & Untargeted \\
%\hline
%Vanilla BC & 0.65(0.05) & 0.52 (0.02) & 0.63 (0.03)& 0.08(0.00) & 0.08 (0.00)&  0.09 (0.01)&  0.10(0.00)\\
%LSTM-GMM & 0.69 (0.04) & 0.65(0.04) & 0.57 (0.04)& 0.09 (0.00) & 0.09 (0.00)& 0.08 (0.01) & 0.09 (0.02) \\
%IBC & 0.72(0.01) &  &  & 0.11(0.01) & 0.31 (0.05) & 0.61 (0.08) & 0.57 (0.05)\\
%DiffusionPolicy-C & 0.89 (0.00) & - & - & 0.22(0.06) & 0.39 (0.10) & 0.14(0.04) & 0.25(0.05)\\
%VQ-BET & 0.62 (0.03) & 0.30 (0.02) & 0.62(0.03) & 0.08 (0.00)& 0.08 (0.00)& 0.08 (0.00) & 0.08 (0.00)\\
%\hline
%\end{tabular}
%\end{table}

% \begin{table}[!htb]
% \centering
% \caption{Adversarial attacks performance on PushT Task}
% \vspace{1em}
% \label{tab:modified-pusht}
% \begin{tabular}{r|c|cc}
% \hline
% & \multirow{2}{*}{NA} & \multicolumn{2}{c}{FGSM} \\
% & & Targeted & Untargeted \\
% \hline
% Vanilla BC & 0.65(0.05) & 0.52(0.02) & 0.63(0.03) \\
% LSTM-GMM & 0.69(0.04) & 0.65(0.04) & 0.57(0.04) \\
% IBC & 0.72(0.01) & & \\
% DiffusionPolicy-C & 0.89(0.00) & - & - \\
% VQ-BET & 0.62(0.03) & 0.30(0.02) & 0.62(0.03) \\
% \hline
% \end{tabular}
% \end{table}
% \FloatBarrier




\section{ILLUSTRATIONS}\label{illustration}
In this section, we show examples of the adversarial perturbations. Figure~\ref{fig:lift_untargeted_ex} shows an example of \textit{untargeted attacks} on the visual input for the Lift task. Figure~\ref{fig:lift_targeted_ex} shows an example of \textit{targeted attacks} on the visual input for the Lift task. 
Figure~\ref{fig:pusht_untargeted_ex} shows an example of \textit{untargeted attacks} on the visual input for the Push-T task. Figure~\ref{fig:pusht_targeted_ex} shows an example of \textit{targeted attacks} on the visual input for the Push-T task. We note that these perturbations are minor and in some cases almost imperceptible. 

\begin{figure}[!htb]
        \centering
        \includegraphics[width=\textwidth]{figures/lift_untargeted.pdf}
        \caption{Untargeted Attacks on Lift task.}
        \label{fig:lift_untargeted_ex}
\end{figure}

\begin{figure}[!htb]
        \centering
        \includegraphics[width=\textwidth]{figures/lift_targeted.pdf}
        \caption{Targeted Attacks on Lift task, where the target direction is towards top-left corner of the object.}
        \label{fig:lift_targeted_ex}
\end{figure}

\begin{figure}[!htb]
        \centering
        \includegraphics[width=\textwidth]{figures/pusht_untargeted.pdf}
        \caption{Untargeted Attacks on PushT task.}
        \label{fig:pusht_untargeted_ex}
\end{figure}

\begin{figure}[!htb]
        \centering
        \includegraphics[width=\textwidth]{figures/pusht_targeted.pdf}
        \caption{Targeted Attacks on PushT task, where the target is bottom right corner of the environment.}
        \label{fig:pusht_targeted_ex}
\end{figure}


% \section{Tool Hang}
% We investigate the vulnerability of modern behavior cloning algorithms on Tool Hang, specifically we look at the targeted universal perturbation attack for Diffusion Policy, LSTM-GMM, IBC and Vanilla BC. 
% As before, we use pre-trained checkpoints for Diffusion Policy , LSTM-GMM and IBC. We train our own policies for Vanilla-BC. Training VQ-BET on this task is extremely slow and due to time constraints during the rebuttal phase we couldn't finish training VQ-BET policies all 3 seeds but we promise to have these and their attacked versions by camera-ready deadline. 
% We report mean and standard deviation of success rate across 3 different seeds where we evaluate each seed policy for 50 randomly initialized environments. 
% Our results  in  Fig  \ref{fig:bc_tool_hang_uap} show a similar trend as in other tasks and environments, of decrease in performance of the attacked policy for all algorithms except BC and IBC which fail to even learn a good behavior cloned policy owing to difficulty of the task. As observed before, Diffusion Policy seems to be more robust than LSTM-GMM to the universal pertubation attack.
% 
% \begin{figure}[!htb]
%         \centering
%         \includegraphics[width=\textwidth]{figures/tool_hang_UAP.png}
%         \caption{ UAP for Tool Hang task. The y axis denotes the normal
% performance of the evaluated policies, which is the lower the better for attacks.0.65}
%         \label{fig:tool_hang_uap}
% \end{figure}


%\begin{figure}[!htb]
%       \centering
%       \includegraphics[width=\textwidth]{figures/bcibc_tool_hang_UAP.png}
%       \caption{ UAP for Tool Hang task. The y axis denotes the normal
%performance of the evaluated policies, which is the lower the better for attacks.0.65}
%       \label{fig:bc_tool_hang_uap}
%\end{figure}






{
\section{Sensitivity to Epsilon Values}\label{eps_sensitivity}

Our analysis reveals vulnerabilities in behavior cloning algorithms even with minimal perturbations (for Universal Untargeted Attacks). As shown in Figure \ref{fig:eps_sensitivity}, while decreasing epsilon values generally reduces attack efficacy, algorithms like VQ-BET, LSTM-GMM, and Diffusion Policy still exhibit substantial performance degradation even at very small epsilon values ($\epsilon$ \text{of 4/256}). This heightened sensitivity to small perturbations highlights a concerning vulnerability in current behavior cloning approaches, suggesting that even well-constrained adversarial attacks can significantly compromise policy performance.
\begin{figure}[htbp]
    \centering
    \begin{subfigure}[b]{0.45\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figures/eps_sensitivity.png}
        \caption{Lift}
        \label{fig:subfig1}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.45\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figures/eps_sensitivity_pusht.png}
        \caption{PushT}
        \label{fig:subfig2}
    \end{subfigure}
    \caption{Performance of the algorithms to smaller epsilon values highlight the vulnerability and lack of robustness of the Behavior Cloning Algorithms.}
    \label{fig:eps_sensitivity}
\end{figure}


}

\end{document}
