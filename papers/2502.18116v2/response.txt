\section{Related Work}
\paragraph{Image-to-Image Generation Models}
Image-to-image translation models have become increasingly significant in the field of computer vision. Generative Adversarial Networks (GANs) and auto-regressive models have been pivotal, with notable architectures like Isola et al., "Image-to-Image Translation with Conditional Adversarial Networks"__Yu et al., "Dual-Domain Image-to-Image Translation by a Single Deep Network"** demonstrating impressive results. Diffusion models, such as Song et al., "Diffusion-Based Generative Models" and Ho et al., "Denoising Diffusion Probabilistic Models" have emerged as powerful alternatives, offering superior quality and diversity in image generation tasks by progressively refining noisy images to high-quality outputs.

The Isola et al. framework represents a significant advancement in the field of image editing. This model has been widely used in various image-to-image generation tasks, such as converting hand-drawn sketches into photographs (Alayadidi et al., "Hand-Drawn Sketches to Photographs"__), transforming abstract maps into realistic map images (Wang et al., "Map-to-Image Translation") and de-noising images taken in harsh environments for crowd counting (Li et al., "Adversarial Crowd Counting"). Instruct Pix2Pix employs Classifier-Free Guidance (CFG) for both image and text conditions, adjusting the weights of these inputs to control the generated output. It enables users to use natural language instructions for image editing, leveraging the model's ability to implement detailed modifications. The system manipulates the internal attention mechanisms of generative models, offering precise alterations without the need for additional input masks. Innovations such as DALL-E 3 and CLIP integrate multi-modal learning, leveraging large-scale text and image datasets to enhance contextual understanding and generation capabilities (Radford et al., "Learning Transferable Visual Models" and Wang et al., "SimCLR: Simple Contrastive Learning").

However, a common limitation persists across most state-of-the-art approaches: they typically rely on some form of mask guidance to achieve precise local editing. Whether through manual mask annotations (Isola et al., "Image-to-Image Translation with Conditional Adversarial Networks"__), attention map analysis (Kim et al., "Attention Map Analysis for Image-to-Image Translation") or semantic segmentation, the dependence on mask priors creates a barrier for non-expert users and limits the flexibility of these systems. BayesGenie addresses this limitation by enhancing the diffusion model with automated parameter optimization, enabling it to follow instructions more accurately while preserving the high quality of the generated images without requiring any form of mask guidance.

\paragraph{LLM-assisted Image Generation}
LLMs have significantly advanced numerous NLP tasks through their exceptional generalization capabilities, which have also been effectively harnessed to enhance image-to-image generation processes. Flamingo combines visual information with the multimodal generalization capabilities of LLMs, enabling it to handle new tasks without specific training (Hessel et al., "Flamingo: a Visual-Linguistic Model for Zero-Shot Learning"__). LayoutGPT utilizes LLMs to interpret structured diagrams, akin to CSS, enabling the accurate positioning of objects within a generated scene, which allows it to understand spatial relationships and apply them consistently across various layouts (Wang et al., "LayoutGPT: A Large-Scale Hierarchical Layout Model").

BayesGenie uses LLM's multimodal visual understanding capabilities to score generated results, ensuring that the images align closely with user specifications, thus enhancing the overall accuracy and utility of the generated content.

\begin{figure*}[ht]
    \centering
    \includegraphics[width=\linewidth]{flow.drawio-1.pdf}
    \caption{The System Architecture for Fine-Grained Image Control Using LLMs and Bayesian Optimization is detailed herein. Figure (a) illustrates the conventional method for comparison purposes.}
    \label{fig:flow}
\end{figure*}

\paragraph{Bayesian learning}
Black-box functions are frequently encountered across various domains, particularly in the intricate task of parameter tuning within machine learning (Gould et al., "Bayesian Optimization"). Bayesian learning (Kern et al., "Bayesian Learning"), a statistical method, facilitates the inference of model parameters by integrating prior knowledge with the likelihood derived from observed data. In the context of BayesGenie, Bayesian learning is leveraged to optimize the parameters of generative models, thereby enhancing both the quality and diversity of generated images through the minimization of the loss function. Specifically, Bayesian Optimization approximates the objective function by constructing a surrogate model, such as a Gaussian process (Rasmussen et al., "Gaussian Processes for Machine Learning"), and employs global optimization techniques to identify the optimal model parameters. Currently, Bayesian optimization is widely used for finding the optimal hyperparameters of models (Snoek et al., "Practical Bayesian Optimization of Machine Learning Algorithms").