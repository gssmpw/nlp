% This must be in the first 5 lines to tell arXiv to use pdfLaTeX, which is strongly recommended.
\pdfoutput=1
% In particular, the hyperref package requires pdfLaTeX in order to break URLs across lines.

\documentclass[11pt]{article}

% Change "review" to "final" to generate the final (sometimes called camera-ready) version.
% Change to "preprint" to generate a non-anonymous version with page numbers.
\usepackage[preprint]{acl}

% Standard package includes
\usepackage{times}
\usepackage{latexsym}

% For proper rendering and hyphenation of words containing Latin characters (including in bib files)
\usepackage[T1]{fontenc}
% For Vietnamese characters
% \usepackage[T5]{fontenc}
% See https://www.latex-project.org/help/documentation/encguide.pdf for other character sets

% This assumes your files are encoded as UTF8
\usepackage[utf8]{inputenc}
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.

\usepackage{xcolor}         % colors
\usepackage{graphicx}
\usepackage{caption}
\usepackage{subcaption}

\newcommand{\xu}[1]{\textcolor{blue}{{\bf Xu:} #1}}
\newcommand{\lenny}[1]{\textcolor{red}{{\bf Lenny:} #1}}
\newcommand{\cai}[1]{\todo[color=blue!100]{cai: #1}}
\usepackage{amssymb,amsmath,amsthm}
\usepackage{lipsum} % Just for sample text

% \newtheorem{theorem}{Theorem}
\newtheorem{assumption}{Assumption}
\newtheorem{theorem}{Theorem}
\newtheorem{lemma}{Lemma}

% \usepackage{duckuments}
\usepackage{graphicx} 
\usepackage{microtype}
\usepackage{amsmath} % for mathematical content
\usepackage{algorithm} % for algorithm environment
\usepackage{natbib}
\usepackage{newfloat}
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{listings}

% This is not strictly necessary, and may be commented out,
% but it will improve the layout of the manuscript,
% and will typically save some space.
\usepackage{microtype}

% This is also not strictly necessary, and may be commented out.
% However, it will improve the aesthetics of text in
% the typewriter font.
\usepackage{inconsolata}

%Including images in your LaTeX document requires adding
%additional package(s)
\usepackage{graphicx}



% If the title and author information does not fit in the area allocated, uncomment the following
%
%\setlength\titlebox{<dim>}
%
% and set <dim> to something 5cm or larger.

\title{Bayesian Optimization for Controlled Image Editing via LLMs}

% Author information can be set in various styles:
% For several authors from the same institution:
% \author{Author 1 \and ... \and Author n \\
%         Address line \\ ... \\ Address line}
% if the names do not fit well on one line use
%         Author 1 \\ {\bf Author 2} \\ ... \\ {\bf Author n} \\
% For authors from different institutions:
% \author{Author 1 \\ Address line \\  ... \\ Address line
%         \And  ... \And
%         Author n \\ Address line \\ ... \\ Address line}
% To start a separate ``row'' of authors use \AND, as in
% \author{Author 1 \\ Address line \\  ... \\ Address line
%         \AND
%         Author 2 \\ Address line \\ ... \\ Address line \And
%         Author 3 \\ Address line \\ ... \\ Address line}

\author{\normalfont
Chengkun Cai$^{1*}$, \quad
Haoliang Liu$^{2*}$, \quad
Xu Zhao$^{1*}$, \quad
Zhongyu Jiang$^{3}$, \quad
Tianfang Zhang$^{4}$, \\
Zongkai Wu$^{5}$, \quad
Jenq-Neng Hwang$^{3}$, \quad
Serge Belongie$^{6}$, \quad
Lei Li$^{3,6\dagger}$ 
\thanks{* Equal contribution.}
\thanks{$\dagger$ Corresponding author: lilei@di.ku.dk} \\
\\
$^1$University of Edinburgh \quad
$^2$University of Manchester \quad
$^4$Tsinghua University \\
$^5$FancyTech \quad
$^3$University of Washington \quad
$^6$University of Copenhagen
}
%\author{
%  \textbf{First Author\textsuperscript{1}},
%  \textbf{Second Author\textsuperscript{1,2}},
%  \textbf{Third T. Author\textsuperscript{1}},
%  \textbf{Fourth Author\textsuperscript{1}},
%\\
%  \textbf{Fifth Author\textsuperscript{1,2}},
%  \textbf{Sixth Author\textsuperscript{1}},
%  \textbf{Seventh Author\textsuperscript{1}},
%  \textbf{Eighth Author \textsuperscript{1,2,3,4}},
%\\
%  \textbf{Ninth Author\textsuperscript{1}},
%  \textbf{Tenth Author\textsuperscript{1}},
%  \textbf{Eleventh E. Author\textsuperscript{1,2,3,4,5}},
%  \textbf{Twelfth Author\textsuperscript{1}},
%\\
%  \textbf{Thirteenth Author\textsuperscript{3}},
%  \textbf{Fourteenth F. Author\textsuperscript{2,4}},
%  \textbf{Fifteenth Author\textsuperscript{1}},
%  \textbf{Sixteenth Author\textsuperscript{1}},
%\\
%  \textbf{Seventeenth S. Author\textsuperscript{4,5}},
%  \textbf{Eighteenth Author\textsuperscript{3,4}},
%  \textbf{Nineteenth N. Author\textsuperscript{2,5}},
%  \textbf{Twentieth Author\textsuperscript{1}}
%\\
%\\
%  \textsuperscript{1}Affiliation 1,
%  \textsuperscript{2}Affiliation 2,
%  \textsuperscript{3}Affiliation 3,
%  \textsuperscript{4}Affiliation 4,
%  \textsuperscript{5}Affiliation 5
%\\
%  \small{
%    \textbf{Correspondence:} \href{mailto:email@domain}{email@domain}
%  }
%}

\begin{document}
\maketitle
\begin{abstract}
In the rapidly evolving field of image generation, achieving precise control over generated content and maintaining semantic consistency remain significant limitations, particularly concerning grounding techniques and the necessity for model fine-tuning. To address these challenges, we propose BayesGenie, an off-the-shelf approach that integrates Large Language Models (LLMs) with Bayesian Optimization to facilitate precise and user-friendly image editing. Our method enables users to modify images through natural language descriptions without manual area marking, while preserving the original image's semantic integrity. Unlike existing techniques that require extensive pre-training or fine-tuning, our approach demonstrates remarkable adaptability across various LLMs through its model-agnostic design. BayesGenie employs an adapted Bayesian optimization strategy to automatically refine the inference process parameters, achieving high-precision image editing with minimal user intervention. Through extensive experiments across diverse scenarios, we demonstrate that our framework significantly outperforms existing methods in both editing accuracy and semantic preservation, as validated using different LLMs including Claude3 and GPT-4.

\end{abstract}
\section{Introduction}

In the rapidly evolving field of visual content manipulation, image editing has gained significant attention due to its practical applications across various domains. Unlike traditional image generation models such as Stable Diffusion~\cite{rombach2022high} and DALL-E 3~\cite{ramesh2022hierarchical}, our work is specifically focused on improving control in the image editing process. Recent advancements in controllable synthesis, such as those by Hertz et al. \cite{hertz2022prompt} and Brooks et al. \cite{brooks2023instructpix2pix}, have introduced methods to fine-tune and guide transformations in existing images rather than generating new ones from scratch.

\begin{figure}[t]
    \centering
    \begin{subfigure}[b]{0.23\textwidth}
        \includegraphics[width=\textwidth,height=2.5cm]{original.jpg}
        \caption{Original image.}
        \label{fig:first}
    \end{subfigure}
    \begin{subfigure}[b]{0.23\textwidth}
        \includegraphics[width=\textwidth,height=2.5cm]{edited.jpeg}
        \caption{Edited image.}
        \label{fig:second}
    \end{subfigure}
    \caption{Practical application results in design scenarios: "replace the tree next to the bench with balloons"}
    \label{fig:1}
\end{figure}

Controllable synthesis in generation technology~\cite{guan2025learning,yao2024car,hertz2022prompt,brooks2023instructpix2pix,patashnik2021styleclip,jiang2024back} has recently attracted significant attention due to its expanded range of applications. Models such as Pix2Pix and CycleGAN have demonstrated the ability to transform images from one domain to another, effectively applying controllable synthesis to tasks like style transfer and image enhancement \cite{isola2017image,zhu2017unpaired}. Recent advances like ZONE have enabled instruction-driven modifications without pre-defined training samples \cite{li2024zone}, while new frameworks have emerged for intuitive and localized image editing by manipulating internal attention mechanisms \cite{brooks2023instructpix2pix}. This represents a shift towards more granular control over AI-generated content, enabling precise, region-specific alterations without additional input masks.

However, despite these advancements, existing methods face several critical challenges. First, most state-of-the-art local editing methods heavily rely on mask priors to constrain the editing regions—either through manual input or derived from attention map analysis and semantic segmentation—making them less accessible for non-expert users. Additionally, methods like cross-attention control and diffusion models often encounter challenges in fine-tuning model parameters to align with user requirements, resulting in a disparity between desired and actual outputs. These issues are particularly pronounced in applications that demand detailed modifications based on user instructions. With the LLMs-Driven adaptation, recent various applications~\cite{cai2024role,cai2024t,li2024human,shi2025explaining,liu2024graph,shi2024chops} have advanced developed.

To address these challenges, we propose BayesGenie: a novel framework that achieves precise localized editing without any form of mask guidance. Our approach uniquely combines the semantic understanding capabilities of LLMs with the parameter optimization power of Bayesian methods~\cite{openai2023,snoek2012practical}. BayesGenie leverages LLMs to generate detailed prompts from user requirements, which then guide a Stable Diffusion model to modify images accurately. The framework employs Bayesian Optimization to systematically explore the parameter space, particularly the image and text Classifier Free Guidance (CFG) weights, to maximize output quality.

Our method provides an end-to-end solution, where users are only required to provide a textual description, eliminating the need for manual selection or marking of specific image regions. This approach streamlines user interaction, enhancing intuitiveness and accessibility. Moreover, our method operates without pre-training or fine-tuning on specific datasets, instead leveraging the capabilities of multiple multimodal LLMs to produce high-quality outcomes. 

Our experimental results demonstrate that the integration of LLMs and Bayesian Optimization enables more intuitive and accurate image editing. As shown in Figure \ref{fig:1}, our method can effectively implement specific modifications while maintaining the scene's overall coherence and aesthetic integrity.

In summary, our contributions are:

% \section{Introduction}

% In the rapidly evolving field of visual content manipulation, image editing has gained significant attention due to its practical applications across various domains. Unlike traditional image generation models such as Stable Diffusion~\cite{rombach2022high} and DALL-E 3~\cite{ramesh2022hierarchical}, our work is specifically focused on improving control in the image editing process. Recent advancements in controllable synthesis, such as those by Hertz et al. \cite{hertz2022prompt} and Brooks et al. \cite{brooks2023instructpix2pix}, have introduced methods to fine-tune and guide transformations in existing images rather than generating new ones from scratch.

% While these methods have shown promising results, they often struggle with maintaining semantic consistency between the original and edited images, particularly when handling complex modifications that require understanding spatial and contextual relationships.

% \begin{figure}[t]
%     \centering
%     \begin{subfigure}[b]{0.23\textwidth}
%         \includegraphics[width=\textwidth,height=2.5cm]{latex/images/17.jpg}
%         \caption{Original image.}
%         \label{fig:first}
%     \end{subfigure}
%     % \vspace{1cm}
%     \begin{subfigure}[b]{0.23\textwidth}
%         \includegraphics[width=\textwidth,height=2.5cm]{latex/images/11best_G.jpeg}
%         \caption{Edited image.}
%         \label{fig:second}
%     \end{subfigure}
%     \caption{Practical application results in design scenarios: "replace the tree next to the bench with balloons"}
%     \label{fig:1}
% \end{figure}

% Controllable synthesis in generation technology~\cite{hertz2022prompt,brooks2023instructpix2pix,patashnik2021styleclip} has recently attracted significant attention due to its expanded range of applications. Models such as Pix2Pix and CycleGAN have demonstrated the ability to transform images from one domain to another, effectively applying controllable synthesis to tasks like style transfer, domain adaptation, and image enhancement \cite{isola2017image,zhu2017unpaired}. These models allow for precise control over the output by utilizing the structure and content of the input image, thus ensuring that the transformations align with specific visual characteristics dictated by the user. A pioneering method, introduced by Hertz et al. \cite{hertz2022prompt}, leverages cross-attention control to guide image editing processes, further extending the principles of controllable synthesis. Moreover, recent advancements such as ZONE have further pushed the boundaries of localized image editing by allowing instruction-driven modifications without the need for pre-defined training samples \cite{li2024zone}.  Furthermore, a novel framework has been proposed for intuitive and localized image editing by manipulating the internal attention mechanisms of generative models \cite{brooks2023instructpix2pix}. This approach exemplifies the ongoing shift towards more granular control over AI-generated content, transitioning from global modifications to precise, region-specific alterations without the need for additional input masks. 

% However, despite these advancements, existing methods face several critical challenges. First, most state-of-the-art local editing methods heavily rely on mask priors to constrain the editing regions—either through manual input or derived from attention map analysis and semantic segmentation—making them less accessible for non-expert users. Additionally, methods like cross-attention control and diffusion models often encounter challenges in fine-tuning model parameters to align with user requirements, resulting in a disparity between desired and actual outputs. These issues are particularly pronounced in applications that demand detailed modifications based on user instructions, where the accuracy of visual outputs is critical. Furthermore, these methods pose significant challenges for non-specialists due to their complexity and steep learning curve, rendering them less accessible and user-friendly.



% To address these challenges, we propose BayesGenie: a novel framework that achieves precise localized editing without any form of mask guidance. Our approach uniquely combines the semantic understanding capabilities of LLMs with the parameter optimization power of Bayesian methods~\cite{openai2023,snoek2012practical} to enhance the fine-grained control of image generation processes. Unlike existing methods that require mask priors, BayesGenie leverages LLMs' semantic understanding to automatically identify and modify specific image regions based purely on natural language descriptions, marking a significant advancement in making image editing more accessible and intuitive.

% Our method utilizes LLMs to generate detailed prompts based on user requirements first. After which these prompts guide a Stable Diffusion model to modify images accurately. However, generating high-quality images that precisely match the user’s intent requires tuning of parameters, such as image Classifier Free Guidance (CFG) weight and text CFG weight. This is where Bayesian Optimization comes into play. By systematically exploring the parameter space, Bayesian Optimization identifies the optimal settings that maximize the quality of the generated images.

% Our method provides an end-to-end solution, where users are only required to provide a textual description, eliminating the need for manual selection or marking of specific image regions. This approach streamlines user interaction, enhancing intuitiveness and accessibility. Moreover, our method operates without the necessity for pre-training or fine-tuning on specific datasets, instead leveraging the capabilities of multiple multimodal LLMs to produce high-quality outcomes. This flexibility allows the method to be applied across a range of LLMs, demonstrating improved versatility and adaptability.



% Our experimental results demonstrate the effectiveness of this approach. The integration of LLMs and Bayesian Optimization not only automates the parameter tuning process but also enables more intuitive and accurate image editing, making it easier for users to modify images as they wish. Selected example can be found in Figure \ref{fig:1}. This example demonstrates the practical utility in design contexts, specifically illustrating the demand: "Replace the tree next to the bench with balloon." The left image represents the original scene, while the right image shows the modified version with added chairs that seamlessly integrate with the existing decor, enhancing both functionality and aesthetic coherence.

% In summary, our contributions are:


\begin{itemize}
    \item We propose BayesGenie, a novel image editing framework that enables precise localized editing without manual region annotations or mask creation. Our model-agnostic approach leverages LLMs for region understanding and semantic interpretation, making it possible to perform accurate local edits based purely on natural language descriptions, while ensuring high-precision image editing.
    
    \item We introduce an automated parameter optimization system based on Bayesian Optimization that eliminates the need for manual parameter tuning or pre-training. This system automatically discovers optimal editing parameters through iterative refinement, independent of the model training, making our framework immediately deployable across different scenarios without requiring specialized adjustments.
    
    \item Through extensive experiments, we demonstrate that our off-the-shelf framework achieves superior performance and broad adaptability across various editing scenarios. The framework's effectiveness has been validated with different multimodal LLMs, showcasing its versatility and robustness while maintaining both local precision and global consistency.
\end{itemize}

\subsection{Related Work}
\paragraph{Image-to-Image Generation Models}
Image-to-image translation models have become increasingly significant in the field of computer vision. Generative Adversarial Networks (GANs) and auto-regressive models have been pivotal, with notable architectures like Instruct Pix2Pix, CycleGAN, and PixelCNN demonstrating impressive results~\cite{isola2017image,zhu2017unpaired,van2016pixel}. Diffusion models, such as SR3 and ADM, have emerged as powerful alternatives, offering superior quality and diversity in image generation tasks by progressively refining noisy images to high-quality outputs~\cite{saharia2022image,dhariwal2021diffusion}. 

The Instruct Pix2Pix framework represents a significant advancement in the field of image editing~\cite{brooks2023instructpix2pix}. This model has been widely used in various image-to-image generation tasks, such as converting hand-drawn sketches into photographs~\cite{m2022transfer}, transforming abstract maps into realistic map images~\cite{li2024mapping} and de-noising images taken in harsh environments for crowd counting~\cite{khan2023crowd}. Instruct Pix2Pix employs Classifier-Free Guidance (CFG) for both image and text conditions, adjusting the weights of these inputs to control the generated output. It enables users to use natural language instructions for image editing, leveraging the model's ability to implement detailed modifications. The system manipulates the internal attention mechanisms of generative models, offering precise alterations without the need for additional input masks. Innovations such as DALLE-3 and CLIP integrate multi-modal learning, leveraging large-scale text and image datasets to enhance contextual understanding and generation capabilities~\cite{betker2023improving,radford2021learning}. 

However, a common limitation persists across most state-of-the-art approaches: they typically rely on some form of mask guidance to achieve precise local editing. Whether through manual mask annotations~\cite{hertz2022prompt}, attention map analysis~\cite{li2024zone}, or semantic segmentation, the dependence on mask priors creates a barrier for non-expert users and limits the flexibility of these systems. BayesGenie addresses this limitation by enhancing the diffusion model with automated parameter optimization, enabling it to follow instructions more accurately while preserving the high quality of the generated images without requiring any form of mask guidance.


\paragraph{LLM-assisted Image Generation}
LLMs have significantly advanced numerous NLP tasks through their exceptional generalization capabilities, which have also been effectively harnessed to enhance image-to-image generation processes. Flamingo combines visual information with the multimodal generalization capabilities of LLMs, enabling it to handle new tasks without specific training \cite{alayrac2022flamingo}. LayoutGPT utilizes LLMs to interpret structured diagrams, akin to CSS, enabling the accurate positioning of objects within a generated scene, which allows it to understand spatial relationships and apply them consistently across various layouts \cite{NEURIPS2023_3a7f9e48}. 
% Ranni, on the other hand, employs LLMs to assist in the semantic segmentation of images, meticulously adjusting object placements based on contextual cues provided by the language model \cite{Feng_2024_CVPR}. 
% BayesGenie uses LLM's multimodal visual understanding capabilities to score generated results, ensuring that the images align closely with user specifications, thus enhancing the overall accuracy and utility of the generated content.


\begin{figure*}[ht]
    \centering
    \includegraphics[width=\linewidth]{flow.drawio-1.pdf}
    \caption{The System Architecture for Fine-Grained Image Control Using LLMs and Bayesian Optimization is detailed herein. Figure (a) illustrates the conventional method for comparison purposes.}
    \label{fig:flow}
\end{figure*}
 

\paragraph{Bayesian learning}
Black-box functions are frequently encountered across various domains, particularly in the intricate task of parameter tuning within machine learning \cite{JMLR:v25:23-0269}. Bayesian learning \cite{10.1115/1.3653121}, a statistical method, facilitates the inference of model parameters by integrating prior knowledge with the likelihood derived from observed data. In the context of BayesGenie, Bayesian learning is leveraged to optimize the parameters of generative models, thereby enhancing both the quality and diversity of generated images through the minimization of the loss function. Specifically, Bayesian Optimization approximates the objective function by constructing a surrogate model, such as a Gaussian process \cite{Jones1998}, and employs global optimization techniques to identify the optimal model parameters. Currently, Bayesian optimization is widely used for finding the optimal hyperparameters of models\cite{boyar2024latent,aristodemou2025maximizing}.

\section{Methodology}
% The overall system architecture integrates LLMs and Bayesian optimization for image editing, as depicted in Figure \ref{fig:flow}. The original image and modification requirements are processed by an LLM, which generates a textual prompt that accurately captures the desired changes. This prompt, along with the original image, is fed into a diffusion model to generate a preliminary modified image.

% BayesGenie enhances conventional image editing by dynamically refining the prompt and optimizing key parameters. After each modification, the LLM evaluates the image and assigns a score based on how well it meets the user-defined requirements. This feedback is used to refine the prompt, addressing areas needing improvement such as adding details or adjusting object positioning. Bayesian optimization then iteratively adjusts key parameters, specifically `text\_cfg\_scale` and `image\_cfg\_scale`, which control the balance between text and image components in the diffusion model. By optimizing an objective function that minimizes the negative score from the LLM, BayesGenie maximizes the alignment between generated images and desired outcomes, providing a robust tool for fine-tuning generative models in high-precision applications.


Our system architecture integrates LLMs and Bayesian optimization for image editing (Figure \ref{fig:flow}). An LLM processes the original image and modification requirements to generate a textual prompt capturing the desired changes. This prompt and the original image are then fed into a diffusion model to generate a modified image.
BayesGenie enhances this process through dynamic prompt refinement and parameter optimization. The LLM evaluates each generated image, scoring it based on requirement satisfaction and providing feedback for improvements. Bayesian optimization then iteratively adjusts key parameters, specifically `text\_cfg\_scale` and `image\_cfg\_scale`, which balance text and image components in the diffusion model. This optimization minimizes the negative LLM score, maximizing alignment between generated images and desired outcomes.

\subsection{Dynamic Prompt Optimization with LLMs}
In our approach, the prompt is dynamically optimized through an iterative optimization process. Initially, the LLM generates a prompt based on the user's modification requirements, which guides the diffusion model to generate a preliminary image. Once the image is generated, the LLM evaluates it by assigning a score based on how well it aligns with the user's specifications and provides feedback on areas needing improvement, such as adding more details or adjusting object positioning. This feedback is then used to refine the prompt for the next iteration. The process repeats, with the refined prompt guiding the generation of a new image, until the desired result is achieved or the maximum number of iterations is reached.

\subsection{Preliminaries}



\begin{assumption}

Consider the existence of a set of optimized guidance scales, denoted as \( s_I^* \) and \( s_T^* \) such that, the generated image \( I_{\text{gen}}(s_I^*, s_T^*) \) produced by the score network \( \tilde{e}_{\theta}(z_t, c_I, c_T) \) satisfies a specific requirement or objective function \( \mathcal{L}(I_{\text{gen}}(s_I^*, s_T^*)) \). Mathematically, this can be expressed as:

\begin{equation}
(s_I^*, s_T^*) = \arg\min_{s_I, s_T} \mathcal{L}\left(I_{\text{gen}}(s_I, s_T)\right)
\end{equation}
where the score network \( \tilde{e}_{\theta}(z_t, c_I, c_T) \) is defined as:
\begin{equation}
\begin{aligned}
    \tilde{e}_{\theta}(z_t, c_I, c_T) &= e_{\theta}(z_t, \emptyset, \emptyset) 
    \\
    &+ s_I \cdot \left( e_{\theta}(z_t, c_I, \emptyset) - e_{\theta}(z_t, \emptyset, \emptyset) \right) 
    \\
    &+ s_T \cdot \left( e_{\theta}(z_t, c_I, c_T) - e_{\theta}(z_t, c_I, \emptyset) \right)
\end{aligned}
\end{equation}

where $z_t$ represents the noisy latent variable at timestep $t$. $\emptyset$ denotes the unconditional input. $e_{\theta}$ represents the score network which estimates the gradient of the noisy latent variable \( z_t \) relative to the clean data given the conditioning inputs \( c_I \) and \( c_T \).
\end{assumption}

This problem formulation stems from the need to address the inherent challenges in fine-grained image modification tasks. While the diffusion models, particularly the Instruct Pix2Pix variant, offer significant control and precision through their diffusion process, the manual adjustment of parameters such as imageCFG and textCFG ($s_I, s_T$).

Given the hypothesis that for any initial image and a sufficiently detailed and comprehensive image generation prompt, there exists a pair of imageCFG and textCFG parameters that ensure the final generated image meets the specified requirements, this problem can be framed as an optimization task. 

In this context, the task becomes one of finding the optimal set of parameters (imageCFG and textCFG) that maximize the quality of the generated image according to the specified criteria. This involves searching through the parameter space to identify the values that produce the best possible image modifications, as defined by the evaluation metrics.


\subsection{Bayesian Optimization}
The optimization process involves the repeated evaluation of the objective function using Bayesian optimization, where each evaluation includes generating a modified image with the current CFG parameters and scoring it through the LLM. The system iteratively samples the parameter space and updates its model of the objective function landscape based on the results of previous evaluations, aiming to find the optimal guidance scales that yield the highest quality edited images.

Specifically, the objective function of Bayesian optimization is 
\begin{equation}
f(\mathbf{s}) = f([s_I, s_T]),
\end{equation}
where $\mathbf{s} = [s_I, s_T]$ represents our two guidance parameters (ImageCFG and TextCFG), and $f(\mathbf{s})$ represents the quality score evaluated by the LLM based on both semantic alignment and image quality.

Bayesian optimization uses a Gaussian Process (GP) to approximate the objective function $f(\mathbf{s})$. The Gaussian Process assumes that all possible function values have a joint Gaussian distribution:
\begin{equation}
f(\mathbf{s}) \sim \mathcal{GP}(\mu(\mathbf{s}), k(\mathbf{s}, \mathbf{s'}))
\end{equation}
where $\mu(\mathbf{s})$ is the mean function and $k(\mathbf{s}, \mathbf{s'})$ is the kernel function that defines the similarity between parameter settings. We employ the Matérn kernel function for its robustness in optimization tasks.

To select the next evaluation point, we use Expected Improvement (EI) as the acquisition function:
\begin{equation}
\text{EI}(\mathbf{s}) = \mathbb{E}[\max(0, f(\mathbf{s}) - f(\mathbf{s}^+))]
\end{equation}
where $\mathbf{s}^+$ represents the current best parameter setting.

The optimization process follows these steps:
\begin{enumerate}
    \item \textbf{Initialization}: Select initial guidance scales $\mathbf{s}_1 = [s_I, s_T]$ and compute their corresponding quality scores $y_i = f(\mathbf{s}_i)$.
    
    \item \textbf{Model Update}: Update the Gaussian Process model using the accumulated observations.
    
    \item \textbf{Select Next Evaluation Point}: Maximize the acquisition function to determine the next parameter setting:
    \begin{equation}
    \mathbf{s}_{n+1} = \arg\max_{\mathbf{s}} \text{EI}(\mathbf{s})
    \end{equation}
    
    \item \textbf{Evaluate Objective Function}: Generate and evaluate an image using $\mathbf{s}_{n+1}$, computing $f(\mathbf{s}_{n+1})$ and adding it to the observation dataset.
    
    \item \textbf{Iterate}: Repeat steps 2-4 until reaching the maximum number of iterations or achieving convergence.
\end{enumerate}

The effectiveness of this approach relies on the smooth relationship between CFG parameters and image quality, where small parameter adjustments typically lead to predictable changes in the output. By systematically exploring the parameter space, our method efficiently discovers optimal guidance scales that balance maintaining original image features with implementing the desired edits.


\subsection{Scoring Evaluation}
% In the context of our optimization task, accurate evaluation of the generated images is crucial. Traditional metrics like CLIP scores are commonly used in image generation tasks but fall short when it comes to fine-grained image modifications. To address this limitation, we utilize LLMs for a more nuanced evaluation. LLMs offer robust multimodal understanding, allowing them to interpret and correlate textual and visual data effectively. The evaluation is guided by a predefined 0-shot prompt designed to ensure consistency and objectivity in the assessment (see Supplement for the full prompt). 
In the context of our optimization task, accurate evaluation of the generated images is crucial. While traditional metrics like CLIP scores are commonly used, they fall short in evaluating fine-grained image modifications. To address this limitation, we utilize LLMs for a more nuanced evaluation, leveraging their robust multimodal understanding capabilities.

\begin{figure}[ht]
    \centering
    \includegraphics[width=0.4\textwidth]{ba.pdf}
    \caption{An illustration of the prompt-based LLM evaluation process}
    \label{fig:ptompt}
\end{figure}
Our evaluation is guided by a predefined 0-shot prompt designed to ensure consistency and objectivity (see Supplement for the full prompt). This prompt directs the LLM to integrate three distinct types of constraints—excessive modification, insufficient modification, and compliance with requirements—into a comprehensive, high-dimensional representation of the image's quality. Figure \ref{fig:ptompt} visually outlines how the LLM utilizes this prompt to systematically calculate a score and provide a detailed explanation.

% The prompt-driven evaluation ensures that the LLM not only assesses how well the generated image adheres to the specified criteria but also normalizes the score to follow a normal distribution, reflecting a balanced judgment across the dataset. Furthermore, the LLM generates a concise yet insightful explanation for the final score, enhancing the transparency and interpretability of the evaluation process.

The evaluation process ensures that scores follow a normal distribution, reflecting a balanced judgment across the dataset. Furthermore, the LLM generates concise explanations for each score, enhancing the transparency and interpretability of the evaluation process.







\begin{figure*}[h]
    \centering
    \includegraphics[width=\linewidth]{flow_result.pdf}
    \caption{The effect of Bayesian optimization of different numbers of iterations}
    \label{fig:gp}
\end{figure*}

\section{Experiments}

\subsection{Evaluation Protocol}
In evaluating our approach, we carefully considered various metrics commonly used in image processing tasks. While traditional metrics like SSIM and PSNR are widely used, they present significant limitations for instruction-guided image editing evaluation:

\begin{itemize}
    \item \textbf{Pixel-level Comparison:} SSIM and PSNR operate on pixel-level comparisons, which would unfairly penalize intentional edits even when they successfully follow the instructions.
    
    \item \textbf{Semantic Understanding:} These metrics cannot evaluate whether edits align with semantic instructions or distinguish between meaningful changes and random noise.
    
    \item \textbf{Local Edit Evaluation:} For local editing tasks, these metrics would give unreasonably low scores due to pixel changes, even when the edits are successful and appropriate.
\end{itemize}

To address these limitations, we adopt a comprehensive evaluation strategy combining both objective and subjective assessments:

\paragraph{Objective Metrics} We utilize two complementary approaches:
\begin{itemize}
    \item \textbf{CLIP-based Similarity:} To evaluate the semantic alignment between edited images and intended modifications.
    \item \textbf{LLM Scoring:} To assess the nuanced visual content and adherence to editing instructions.
\end{itemize}

\paragraph{Subjective Evaluation} We conducted an anonymous user study approved by our institution's ethics committee. Participants evaluated the edited images based on: (1) faithfulness to the editing requirements, (2) overall visual quality, and (3) preservation of original image context.


\subsection{Experimental Setup}
\paragraph{Baselines Selection}
Most state-of-the-art local editing methods, including ZONE~\cite{li2024zone}, heavily rely on mask priors to constrain the editing regions—either through manual input or derived from operations such as attention map analysis and semantic segmentation. In contrast, our method, to the best of our knowledge, is the first to aim for localized editing without relying on any form of mask guidance. As such, comparing our approach with mask-guided methods would neither be fair nor meaningful.

Instead, we focus our evaluations on methods that, like ours, do not utilize masks or region segmentation in their pipeline:
\begin{itemize}
    \item InstructPix2Pix~\cite{brooks2023instructpix2pix}, which performs editing purely based on text instructions and ranks second to ZONE in their reported experiments.
    \item DALLE-3, which is widely recognized as the current strongest image generation model due to its massive parameter scale.
\end{itemize}
\paragraph{Dataset and Tasks} We constructed a balanced evaluation dataset comprising over 500 images, with the following editing operations:
\begin{itemize}
    \item Adding objects to images
    \item Removing objects from images
    \item Modifying existing objects
\end{itemize}
This diverse set of tasks was selected to comprehensively evaluate our method's versatility and effectiveness across different editing scenarios.


\subsection{Results}

In our experiments, BayesGenie effectively handled the three key image editing tasks, demonstrating robust performance across all scenarios. The results indicate that BayesGenie produces visually consistent images that align closely with user specifications, thanks to the LLM’s multimodal understanding capabilities.

\begin{figure}[ht]
\centering
\includegraphics[width=0.5\textwidth]{com_image_pre.pdf}
\caption{Practical application results in different scenarios}
\label{fig:optimise result}
\end{figure}

\paragraph{Qualitative Analysis} 
Figure \ref{fig:gp} shows how Bayesian optimization improves accuracy over iterations, initially with incorrect placement and mismatched background features, but gradually aligning the replacement to match the original image. Figure \ref{fig:optimise result} illustrates our method’s ability to add, remove, and modify elements in a scene while preserving the core features, demonstrating successful addition, removal, and substitution tasks in different visual contexts, showcasing our model's performance in fine-grained image editing.

% 1. Figure \ref{fig:gp} illustrates the impact of Bayesian optimization across different numbers of iterations. Initially, the horse replacing the lion is incorrectly placed, and the background features differ from the original image. As the number of iterations increases, the replacement becomes more accurate, closely matching the original image's background and characters.

% 2. The top two images in Figure \ref{fig:optimise result} show our method’s ability to add elements, such as a lamp, to a park scene while retaining most of the original image’s features. 

% 3. The middle two images illustrate the successful removal of grass from a 3D-rendered park scene, with the bench and streetlamp remaining intact.

\begin{figure*}[ht]
\centering
\includegraphics[width=\linewidth]{image_all.pdf}
\caption{Comparison of Image Editing Techniques: DALLE-3, Instruct Pix2pix, and Our method}
\label{fig:compare}
\end{figure*}

% 4. The bottom two images demonstrate our method’s ability to modify elements. In this example, bread in a kitchen scene is replaced with oatmeal, preserving the original light and shadow contrasts in the image.

\paragraph{Model Comparisons} 
As shown in Figure \ref{fig:compare}, the images generated by DALLE-3 and Instruct Pix2Pix tend to make significant changes to the original images. Although these modifications usually meet the given prompts, they often alter the overall style and content of the image. In contrast, our method preserves the information and characteristics of the original image to a much greater extent, making precise modifications that meet the specified requirements without unnecessary changes.

\paragraph{Similarity Detection} 
For similarity detection, we primarily relied on the CLIP model's capability to extract features from both images and text, providing a similarity score \cite{hessel2021clipscore}. However, since CLIP evaluates similarities either between images or between images and text, it cannot directly assess the alignment between the modified image and the combined original image plus modification prompt. To address this limitation, we also used ChatGPT to describe the original image, incorporating the modification requirements into the text. We then used this textual description to calculate a GPT score, shown in Figure \ref{fig:score}(b). This approach allowed us to assess the alignment between the combined text and the final output image more effectively.

As shown in the left plot of Figure \ref{fig:score}, our method, including versions using either ChatGPT-4o or Claude 3.5 (referred to as Bay-GPT4o and Bay-Claude), consistently achieved higher CLIP scores than the baseline methods, including Instruct Pix2Pix. This suggests that our method provides greater stability and precision in meeting user expectations. While DALLE-3 performed well, its tendency to make extensive modifications to satisfy the prompts resulted in slightly lower scores.



\begin{figure*}[!htb]
\centering
\includegraphics[width=1.0\linewidth]{500_final_results.pdf}
\caption{Comparison of CLIP scores, GPT scores, and Human evaluation results across different models}
\label{fig:score}
\end{figure*}

\paragraph{Human Evaluation:} 
We also conducted a human voting phase to gather subjective evaluations. Participants were asked to select the images they believed best met the editing requirements (see Figure \ref{fig:score}(c)). Instruct Pix2Pix was frequently found to be unsatisfactory due to visible instability and excessive alterations. While DALLE-3 retained some of the original image features, it was selected by fewer participants. In contrast, our method was favored by the majority of participants, demonstrating its higher stability and precision in executing the specified modifications. This result clearly demonstrates the superiority of our approach over the other two methods.

% \section{Experiment}


% To evaluate the performance of our method, we compare the resulting images with those generated by two baseline models, DALLE-3 and Instruct Pix2Pix. We use the same input images and prompts for all three methods to ensure a fair comparison. 

% We explore three primary operations for fine-tuning images: adding objects to images, removing objects from images, and modifying existing objects within images. For each operation, we conduct multiple experiments to generate output images using all three methods. These operations are specifically chosen to test the versatility and effectiveness of our technique in handling diverse image editing tasks.


% \textbf{Evaluation Metrics:} We employ both subjective and objective evaluation metrics to comprehensively assess the effectiveness of our method. 

% The evaluation of our approach employs both objective and subjective metrics. Objectively, we utilize a combination of a CLIP-based image similarity detection system and LLM scoring to assess the alignment between output images and the intended modifications, with the CLIP model providing a similarity score and LLMs evaluating the nuanced visual content. Subjectively, we conduct a user study involving multiple volunteers who select images that best meet the editing requirements, allowing us to capture human judgments on the effectiveness of the modifications and complement the objective assessments.



% This experimental setup allows for a thorough assessment of our method's performance and a fair comparison with existing methods like DALLE-3 and Instruct Pix2Pix, thus providing a comprehensive analysis of the capability of our method in image fine-tuning tasks.


% \subsection{Result}

% In our experiments, we tested BayesGenie's performance in three key image editing tasks: addition, modification, and deletion of elements. The results demonstrate that BayesGenie handles these tasks effectively, producing visually consistent images that closely align with user specifications, thanks to the LLM's multimodal understanding capabilities.

% \begin{figure}[ht]
% \centering
% \includegraphics[width=0.5\textwidth]{com_image_pre.pdf}
% \caption{Practical application results in different scenarios}
% \label{fig:optimise result}
% \end{figure}

% 1. Figure \ref{fig:gp} shows the impact of Bayesian optimization iterations. Initially, the horse replacing the lion is misplaced, but with more iterations, the substitution becomes accurate, closely matching the original image's background and characters.

% 2. The top two images in Figure \ref{fig:optimise result} display our method’s ability to add elements, demonstrating the seamless addition of a lamp to a park scene while retaining the original image’s integrity.

% 3. The middle two images show the successful removal of grass from a park scene while preserving key elements like the bench and streetlamp.

% \begin{figure*}[ht]
% \centering
% \includegraphics[width=0.9\linewidth,height=280]{image_all.pdf}
% \caption{Comparison of Image Editing Techniques: DALLE-3, Instruct Pix2pix, and Our method}
% \label{fig:compare}
% \end{figure*}

% 4. The bottom two images illustrate element modification, where the bread in a kitchen scene is replaced with oatmeal, maintaining the original image’s composition.

% % \subsection{Evaluation}



% \paragraph{Model Comparisons}
% As Figure \ref{fig:compare} shown, the images on the left are initial picture, images in the second column are generated by DALLE-3 based on the requirements. As to the images in the third column, they are generate by the baseline Instruct Pix2Pix method, while images in the last column are the picture generated by our method. Overall, the images generated by DALLE-3 and Instruct Pix2pix tend to make significant alterations to the original images. Although the final outputs generally align with the modification prompts, they often change the style and content of the images. In contrast, our method preserves the information and characteristics of the original image to the greatest extent, making precise modifications to meet the specified requirements.


% \paragraph{Similarity detection}
% Similarity detection mainly relies on CLIP model's capability to extract features from images and text, and then score their similarity \cite{hessel2021clipscore}. However, the CLIP score is limited to evaluating similarities either between images or between images and text. Therefore, it cannot directly assess the similarity between the modified image and the combination of the original image and the modification prompts. To address this limitation, we also use ChatGPT to describe original image, converting the original image into a textual description, and incorporated the modification requirements into this text. And then we have the GPT Score, as is shown in plot(b) from Figure \ref{fig:score}. Thus, we assess the similarity between the combined text and the final output image, which indirectly assesses the alignment between the modified images and the expected output.

% From the plot on the left of Figure \ref{fig:score}, it is evident that our method, including that using either Chagt-GPT4o or Claude 3.5, which are called Bay-GPT4o and Bay-Claude respectively, achieves higher CLIP score compared to the baseline (Instruct 
% pix2pix) method. This is mostly attributed to the randomness in image modifications produced by the baseline model, indicating our method has greater stability and precision. Additionally, our scores are slightly higher compared to DALLE-3. This is because DALLE-3 tends to make extensive modifications to the image in order to meet the modification requirements. The CLIP score, as a metric that evaluates the resemblance between images and text, emphasizes the overall similarity of the image. This score potentially neglects the importance of fine-grained modifications.

% \begin{figure*}[ht]
%     \centering
%     \includegraphics[width=\linewidth]{combined_evaluation_results.pdf}
%     \caption{Comparison of CLIP scores, GPT scores and Human evaluation result across different models}
%     \label{fig:score}
% \end{figure*}

% \paragraph{Experiment Setup}

% In the image on the right side of Figure \ref{fig:score}, we employ the same LLM evaluation method as described in the methodology section to assess the images. As illustrated from the figure, our method achieved scores approximately 3 times higher than those of other methods, indicating a significant improvement in performance. It is worth noting that, unlike the variance caused by the inaccuracy of description generated by ChatGPT when using similarity detection, ChatGPT's evaluation of images generated by DALLE-3 exhibited some discrepancies due to closed-domain hallucinations. An example of such hallucination is illustrated in Figure. 
%  \ref{fig:optimise result}: in this case, the LLM initially assigned a score of 85 to an image generated by DALLE-3. However, after being prompted with the reminder that "the image still contains the lamp," the LLM revised the score down to 0.
% \paragraph{Human votes} Since fine-tuning images ultimately needs to meet the user's expectations and enhance creative efficiency, we incorporate a human voting phase to assess the methods based on subjective evaluations. As is shown in Figure. 
%  \ref{fig:score}(c), the Instruct pix2pix model is often found unsatisfactory due to its visible instability, frequently altering the images excessively to meet the requirements. DALLE-3, while retaining some original image information, is chosen by a minority of participants. In contrast, our method is favored by the majority due to its high stability and precise ability to modify images according to the specifications. This clearly demonstrates the superiority of our approach compared to the other two methods.
\section{Discussions}
\paragraph{Cost and efficiency}
The algorithm requires relatively low computational resources and costs. The costs for GPT-4o are shown in the table below, with the main expense being the Prompt tokens for Bayesian loop evaluation. The total cost for running the algorithm once to generate a 512x512 image is 0.176 dollar. Additionally, this experiment was conducted on a machine with a single RTX 4080, and each algorithm run takes approximately 2.5 minutes.

In terms of iterations, more iterations represent a finer exploration of the solution space, allowing the algorithm to more precisely converge on an optimal result. Our Bayesian optimization strikes a balance between accuracy and cost by using 20 iterations, as increasing beyond this number has shown diminishing returns in terms of accuracy improvements, while still incurring higher computational costs. This choice ensures that the algorithm remains both computationally efficient and capable of generating high-quality outputs without unnecessary resource consumption.
\begin{table}[htbp]
  \centering
  \caption{Bay-GPT4o Cost}
  \resizebox{\columnwidth}{!}{%
  \begin{tabular}{lcc}
    \toprule
    \textbf{Image size} & \textbf{Generate/Prompt tokens} & \textbf{Cost per case} \\
    \midrule
    512$\times$320 & 5.5k / 24.5k & 0.174 \\
    512$\times$512 & 5.5k / 24.5k & 0.176 \\
    \bottomrule
  \end{tabular}%
  }
  \label{tab:bay-gpt4o-cost} % Use underscores for labels
\end{table}


% \paragraph{Evaluation metrics}
% In the domain of image processing, the CLIP score is extensively utilized for image assessment. However, the CLIP score may not be optimal for specific image editing tasks due to its focus on computing overall similarity. There remains an absence of a robust and reliable evaluation methodology for assessing image editing tasks. Conversely, LLMs have been extensively employed as evaluators across a range of natural language processing (NLP) tasks, demonstrating their efficacy in text evaluation \cite{zheng2024judging,chiang2023can}. Similarly, in this work, we utilize the visual understanding capabilities of multimodal large models to evaluate images \cite{yang2023dawn}. However, akin to text evaluation, multimodal large models also exhibit some hallucination issues when assessing the fineness of image modifications, especially with complex images or very subtle changes. Future directions include using other techniques, such as Retrieval-Augmented Generation (RAG) or Pairwise Preference, to enhance the accuracy and stability of LLM evaluations \cite{lewis2020retrieval,liu2024aligning}.

\paragraph{Generalizability and Robustness}

In the experiments described above, we demonstrated the robustness of this approach across different LLMs by using GPT-4o and Claude 3.5. Additionally, due to the versatility of the Classifier-Free Guidance (CFG) technique, this method serves as a training-free optimization solution that can be applied to various diffusion models. Future directions may include implementing the text-CFG and image-CFG modules in other diffusion models and testing this method's generalizability.



\section{Conclusion}

Our work introduces BayesGenie, a novel and model-agnostic framework that combines Bayesian Optimization with Large Language Models (LLMs) to enhance the fine-grained image editing process. By leveraging the power of LLMs to generate natural language prompts, BayesGenie simplifies the image creation process, making it more intuitive and accessible. The framework's robust performance across various scenarios highlights its versatility and adaptability, demonstrating its potential for broad applicability in both academic and practical contexts. Ultimately, BayesGenie sets the stage for future advancements in AI-driven content creation, offering a powerful tool for precise and user-friendly image editing.



% \section{Limitations}
% While our proposed method, BayesGenie, demonstrates substantial improvements in controllable image editing, it is not without limitations. First, the reliance on large language models (LLMs) like GPT-4 and Claude 3.5 introduces potential biases and limitations inherent to these models. Specifically, LLMs can exhibit hallucination effects, particularly when evaluating subtle image modifications, leading to inconsistencies in scoring or inappropriate recommendations for image adjustments, which could impact the overall precision of the edited image. Future directions could include fine-tuning open-source LLMs, such as Llava, to develop a more stable and consistent model for both image modification evaluation and textual prompt generation. By improving the alignment between the generated images and user intentions, a fine-tuned LLM could provide a more reliable baseline for both assessing and generating image outputs, thus enhancing the overall effectiveness of the system.


\bibliography{custom}


\appendix
\section{Examples of Prompt Generation}
In this section, we present examples of input images alongside the corresponding requirements and GPT-4 generated prompts. These examples illustrate how the Large Language Model (LLM) interprets user instructions and generates detailed prompts that guide the image modification process.
\begin{figure}[h]
    \centering
    \includegraphics[width=0.5\textwidth,height=0.25\textheight]{table_applendix.pdf}
    \caption{Examples of Input Images, Requirements, and GPT-4 Generated Prompts}
    \label{fig:ptompt}
\end{figure}

\section{Scoring Evaluation Prompt}
This appendix provides the specific prompt used for evaluating the modified images generated by the model. The prompt is designed to ensure that the generated images adhere to the specified requirements, while also maintaining the overall integrity and coherence of the original image. The evaluation prompt outlines the criteria for scoring the generated image, which includes assessing the degree of alteration and its alignment with the original content. 
\begin{quote}
        {
        The following is a requirement for modifying an image:
        
        Below are two images: the original image and the generated image after modification.

        The first one is the original Image.
        
        The second one is the generated Image.

        Please evaluate whether the generated image meets the requirement. Provide a score from 0 to 100 based on the following criteria:
        \newline
        1. If the generated image is altered too much compared to the original image, give a low score.
        \newline
        2. If the generated image is altered too little, give a low score.
        \newline
        3. If the generated image meets the requirement well, give a high score.
        \newline
        The return should begin with: The score is:
        \newline
        Ensure the scores follow a normal distribution, with the majority of scores being around the middle range, and only exceptional cases scoring very low or very high. Also, provide a brief explanation for the score.
        }
        \end{quote}


\section{Ethics and Participant Consent}
This study was conducted following the ethical guidelines established by our institution’s Research Ethics Committee, all participants in the study provided informed consent before participating. Participants were fully informed about the purpose of the study, which aimed to evaluate AI-generated image modifications. Participation was voluntary, with the right to withdraw at any point before submitting the survey responses. No personally identifiable information was collected, ensuring complete anonymity. Anonymised data was used for academic publications and presentations, with participants given the option to consent to future use in ethically approved research. Participants filled out the survey via social media platforms and were offered a chance to win a £10 Amazon gift card as an incentive for their participation.


\end{document}
