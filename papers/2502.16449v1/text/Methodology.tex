\section{Methodology}\label{sec_methodology}
In this section, we elaborate the methodology of EMVLight. We begin with implementing a decentralized shortest path onto EMV navigation in traffic networks, and then incorporate it into the proposed multi-class RL agent design. Subsequently, we introduce the multi-agent advantage actor-critic framework as well as the RL training workflow in details.
\subsection{Decentralized Routing for EMVLight}
Dijkstra's algorithm is an algorithm that finds the shortest path between a given node and every other nodes in a graph. The time-based Dijkstra's algorithm finds the fastest path and has been widely used for EMV routing. In order to find such a path, the EMV travel time along each link need to be estimated first and we refer to it as the \emph{intra-link travel time.}
Dijkstra's algorithm takes as input the traffic graph, the intra-link travel time and a destination, and can return the time-based shortest path as well as estimated travel time from each intersection to the destination. The latter is usually referred to as the \emph{estimated time of arrival} (ETA) of each intersection. 


% Pre-determined route-based signal pre-emption techniques often yield sub-optimal paths due to stochastic traffic dynamics, and overall network-level traffic conditions are significantly exacerbated.
% Navigating emergency vehicles in congested traffic networks relies on dynamically finding the time-based shortest path, \dz{meaning routing decisions are updated every time they pass an intersection. Todo: inaccurate, decision distance.}

% \subsection{Estimation of Intra-link EMV Travel Time}
% After reviewing very limited literature on the investigation of traffic impacts on intra-link segment, we propose a meso-scopic approach based on the fundamental diagram to estimate the intra-link travel time for EMVs. See Figure. 
% \subsection{Dynamic Dijkstra's algorithm for EMV routing}

In a traffic network, the intra-link travel time usually depends on the link's emergency capacity and number of vehicles in that link. In our model, this dependency is captured by EMV speed Eqn.~\eqref{eqn:EMV_speed}. The intra-link travel time is then calculated as the link length divided by the EMV speed. 
However, traffic conditions are constantly changing and so does EMV travel time along each link. Moreover, EMV pre-emption techniques alter traffic signal phases, which will significantly change the traffic condition as the EMV travels. The pre-determined shortest path might become congested due to stochasticity and pre-emption. Thus, updating the optimal route dynamically can facilitate EMV passage. One option is to run Dijkstra's algorithm repeatedly as the EMV travels through the network in order to take into account the updated EMV intra-link travel time.
\hs{However, this requires global traffic information of the entire traffic network throughout EMVs' trips. Even when a centralized controller is established to navigate the EMV, the synchronization and communication cost grows exponentially when the network size increases and the nonscability disallows the centralized scheme to be real-time for the navigation. 

Decentralized routing approaches \cite{ADACHER2014routeguidance, Chen2006Riskaverse,He2015Kshortest} were introduced to find the shortest path with partial observability of the system. However, these models require network decomposition or partitioning in advance, and solve optimal paths with polynomial-bounded time complexity at best \cite{JOHNSON2016partitioning}. Some other approach heavily relies on V2I communication \cite{Mostafizi2021decentralized}. Considering the massive amount of iterations of trial-and-error in reinforcement learning, none of these decentralized routing methods provide a suitable design for a learning-based framework.
}

To achieve efficient decentralized dynamic routing, we extend Dijkstra's algorithm to update the optimal route based on the updated intra-link travel times. As shown in Algorithm~\ref{alg:ETA_prepopulation}, first a pre-populating process is carried out where a standard Dijkstra's algorithm is run to get the $\mathsf{ETA}^0$ from each intersection to the destination. For each intersection, the next intersection $\mathsf{Next}^0$ along the shortest path is also calculated. For an intersection $i$, the result $\mathsf{ETA}_i^0$ and $\mathsf{Next}_i^0$ are stored locally in the intersection. We assume this process can be done before the EMV leaves the dispatching hub. This is reasonable since a sequence of processes, including call-taker processing, are performed before the EMVs are dispatched. Once the pre-populating process is finished, we can update $\mathsf{ETA}$ and $\mathsf{Next}$ for each intersection efficiently in parallel in a decentralized way, since the update only depends on information of neighboring intersections. 
\begin{figure}[h]
\includegraphics[width=\linewidth]{images/revised_figure4.png}
\centering
\caption{EMVLight routing (top) vs conventional Dijkstra's routing (bottom).}
\label{fig_EMVLight_routing}
\end{figure}
\hs{
Fig. \ref{fig_EMVLight_routing} provides an example comparing between Algorithm.\ref{alg:ETA_prepopulation} and conventional Dijkstra's algorithm on an 3-by-3 traffic network. The numerical value represents the $\mathsf{ETA}$ of each intersection, and it gets updated as described above. EMVLight, rather than solving the full shortest path like the conventional Dijkstra's algorithm, only determines the next link to travel each iteration. A comparison of the worst-case time complexity between the EMVLight routing heuristic and a Dijkstra-based dynamic shortest path method is provided in Table\ref{tab_routing_comparison}. Notice that the Dijkstra' algorithm can be implemented with a Fibonacci heap min-priority queue and solve the shortest path with a time complexity of $\mathcal{O}(|\mathcal{V}|\log{}|\mathcal{V}| + |\mathcal{E}|)$ \cite{Fredman1984Fibonacci}.
\begin{table}[h]
\centering
\fontsize{10.0pt}{10.0pt} \selectfont
\begin{tabular}{@{}ccc@{}}
\toprule
                     & EMVLight heuristic & Dynamic shortest-path based on Dijkstra's \\ \midrule
Initialization & $\mathcal{O}(|\mathcal{V}|\log{}|\mathcal{V}| + |\mathcal{E}|)$    &   -                          \\
updating &   $\mathcal{O}(|\mathcal{V}|)$        &   $\mathcal{O}(|\mathcal{V}|\log{}|\mathcal{V}| + |\mathcal{E}|)$      \\
updating frequency &   $|\mathcal{V}|$  & $M$  \\ \bottomrule
\end{tabular}
\caption{Time complexities of the proposed routing heuristic and the dynamic shortest-path approach. $M$ can be arbitrarily set to determine the updating frequency. The larger $M$ is, the shorter selected route can be.}
\label{tab_routing_comparison}
\end{table}

By adopting the proposed heuristic routing algorithm, we facilitate the RL agent design, which is introduced in Sec. \ref{sec:agent_design}.
}


% \hs{ 
% As compared to repeatedly running standard Dijkstra's algorithm, our proposed algorithm cannot guarantee that at each time step, the updated path is the time-based shortest path at that time step. However, as our proposed algorithm only requires local information, it is more practical to be implemented in the real world than repeatedly running standard Dijkstra's algorithm, which requires global information all the time. Moreover, it can be naturally integrated into our multi-agent reinforcement learning formulation presented in the next section. Although the time-based shortest path is not generated at all time steps, in practice, our algorithm still performs well in terms of routing. This is because in our update scheme, the updated traffic information in general propagates from intersections closer to the destination to intersections closer to the EMV. Let's say the EMV enters into a link at time $t_0$ and travels for $k$ time steps. Then the updated traffic information in the EMV's $k$-hop neighborhood at time $t_0$ will be able to propagated to the EMV when it is about to enter the next link. If the traffic information outside of this $k$-hop neighborhood is not changing rapidly, then the updated route would be close to the one calculated by running a full Dijkstra's algorithm. 
% This mechanism enables the EMV to select the immediate next intersection quickly.
% Once the EMV is close to the destination, the selected route would match the one calculated by the standard Dijkstra's algorithm. }

% \hs{Chow: this seems rather simplistic given the list of references I shared with you above on stochastic path finding in a dynamic setting. You might need to justify this -- maybe acknowledge the advances and downplay this saying you are focusing on capturing the coupling with preemption. (and how does it capture the effect of preemption? that should be clear in the algorithm, which it currently isn't.)}
% When an emergency incident occurs, the nearest intersection $i$ broadcasts the incident information to its adjacent intersections. As one of $i$'s neighbor intersection $j$, it calculates an \emph{Estimated Time of Arrival} (ETA) for EMV passage based on traffic state, i.e. number of vehicles, of link $ji$. 
% The ETA of an intersection represents the shortest amount of time needed for an EMV to travel from this intersection to the destination. The pre-populating of ETA continues when $j$'s neighbor retrieve its ETA based on the traffic state on their connecting link.
% Since each intersection may have more than one neighbor, it may compute ETA more than once. However, only the smallest ETA is embedded in the state, indicating the direction for the shortest path. \hs{The pre-population completes when all intersections in the network have ETAs.} Notice that the pre-population of the ETA does not follow the Markov decision process so the time to pre-populate ETA through the entire traffic network is negligible. 

% In a traffic network with an incident reported at intersection $d$, we propose Algo. \ref{alg:ETA_prepopulation} to pre-populate ETA for emergency vehicle passage.
%
\begin{algorithm}[t]
    \caption{Decentralized routing for EMVs}
    \label{alg:ETA_prepopulation}
    \SetEndCharOfAlgoLine{}
    \SetKwInOut{Input}{Input}
    \SetKwInOut{Output}{Output}
    \SetKwData{ETA}{ETA}
    \SetKwData{Next}{Next}
    \SetKwFor{ParrallelForEach}{foreach}{do (in parallel)}{endfor}
    \Input{\\\hspace{-3.7em}
        \begin{tabular}[t]{l @{\hspace{3.3em}} l}
        $G=(\mathcal{V}, \mathcal{E})$ & traffic map as a graph \\
        $T^t = [T_{ij}^t]$ & intra-link travel time at time $t$ \\
        $i_d$  & index of the destination
        \end{tabular}
    }
    \Output{\\\hspace{-3.7em}
        \begin{tabular}[t]{l @{\hspace{1.5em}} l}
        $\mathsf{ETA}^t = [\mathsf{ETA}^t_i]$ & ETA of each intersection \\
        $\mathsf{Next}^t = [\mathsf{Next}^t_i]$ & next intersection to go \\
        & from each intersection
        \end{tabular}
    }
    \tcc{pre-populating}
    % \ForEach{$i \in V\setminus \{d\}$}{
    %     \ETA$_i^0 \gets \infty$\;
    %     \Next$_i^0 \gets \text{Undefined}$\;
    % }
    $\mathsf{ETA}^0, \mathsf{Next}^0$ $=$ \texttt{Dijkstra}$(G, T^0, i_d)$\;
    \tcc{dynamic routing}
    \For{$t = 0 \to T$}{
        \ParrallelForEach{$i \in \mathcal{V}$}{
            $\mathsf{ETA}_i^{t+1} \gets \min_{(i, j)\in \mathcal{E}} (\mathsf{ETA}_j^t + T_{ji}^t)$\;
            $\mathsf{Next}_i^{t+1} \gets \arg\min_{\{j|(i, j)\in \mathcal{E}\}}(\mathsf{ETA}_j^{t} + T_{ji}^t$)\;}}
\end{algorithm}
\begin{remark}
    In static Dijkstra's algorithm, the shortest path is obtained by repeatedly querying the $\mathsf{Next}$ attribute of each node from the origin until we reach the destination. In our dynamic Dijkstra's algorithm, since the shortest path changes, at a intersection $i$, we only care about the immediate next intersection to go to, which is exactly $\mathsf{Next}_i$.
\end{remark}

% in this system design, the algorithm is feasible
% pros: 
% parallel
% global information
% cons:
% not guarantee the 
% when EMV is closer to the destination, the ETA are more accurate

% calculate a point to destination
% number of vehicles, 

%
%
% \begin{algorithm}
% \caption{Dynamic Dijkstra's for EMV routing}\label{alg:ETA_prepopulation}
% \begin{algorithmic}
% \State Initialize ETA, Next

% \State $\text{ETA}[d] \gets 0$
% \State Initialize an intersection priority queue Q
% \For{each intersection $i \in V$}
% \If{$i \neq d$}
%     \State $\text{ETA}[i] \gets \infty$
%     \State $\text{Next}[i] \gets \text{Undefined}$  \Comment{The next intersection of $i$ in the direction of travelling under backward Dijkstra's}
%     \State $Q.\text{insertPriority}(i, \text{ETA}[i])$
    
% \EndIf
% \EndFor

% \While{Q} \Comment{Pre-populating of ETA starts}
%     \State $m \gets Q.\text{extractMin}()$
%     \For{each of $m$'s neighbor intersection $n$}
%         \State $\text{ETA'}[n] \gets \text{ETA'}[m] + T_{ij}^{intra}$
%         \If{$\text{ETA'}[n] \leq \text{ETA}[n]$}
%             \State $\text{ETA}[n] \gets \text{ETA'}[n]$
%             \State $\text{Next}[i] \gets m$
%             \State $Q.\text{insertPriority}(n, \text{ETA'}[n])$
%         \EndIf
%     \EndFor
% \EndWhile
% \State Return ETA, Next

% \State $\text{ETA}[i]$, $\text{Next}[i]$
% \For{each MDP step $t \in [0, T]$} \Comment{During dispatching starts}
%     \For{each intersection $i \in V$ simultaneously}
%         \State ETA$[i]_{t+1} \gets \min_{j\in J}ETA[j]_{t} + T_{ji}^{Intra}$
%         \State Next$[i]_{t+1} \gets \arg \min_{j\in J} ETA[j]_{t} + T_{ji}^{Intra}$
%     \EndFor
% \EndFor
% % \State Initialize intersection priority queue Q
% % \State $\text{ETA}[d] \gets 0$
% % \For{each intersection $i \in G$}
% % \State $y \gets 1$
% % \State $X \gets x$
% % \State $N \gets n$
% % \While{$N \neq 0$}
% % \If{$N$ is even}
% %     \State $X \gets X \times X$
% %     \State $N \gets \frac{N}{2}$  \Comment{This is a comment}
% % \ElsIf{$N$ is odd}
% %     \State $y \gets y \times X$
% %     \State $N \gets N - 1$
% % \EndIf
% % \EndWhile
% \end{algorithmic}
% \end{algorithm}

\subsection{Reinforcement Learning Agent Design}\label{sec:agent_design}
While dynamic routing directs the EMV to the destination, it does not take into account the possible waiting times for red lights at the intersections. Thus, traffic signal pre-emption is also required for the EMV to arrive at the destination in the least amount of time. However, since traditional pre-emption only focuses on reducing the EMV travel time, the average travel time of non-EMVs can increase significantly. Thus, we set up traffic signal control for efficient EMV passage as a decentralized RL problem. \modi{In our problem, each intersection is an RL agent. Each agent makes decisions on the traffic signal phases of this intersection based on local information. Aside from communicating with the approaching EMV, agents are also communicating with each other.} Multiple agents coordinate the control signal phases of intersections cooperatively to \textbf{(1)} reduce EMV travel time and \textbf{(2)} reduce the average travel time of non-EMVs. First we design 3 agent types. Then we present agent design and multi-agent interactions.

\subsubsection{Types of agents for EMV passage}
% pre-emption phases can help it pass through an intersection more quickly than normal phases. This motivates us to
When an EMV is on duty, we distinguish 3 types of traffic control agents based on EMV location and routing (Fig.~\ref{fig_secondary}). An agent is a \emph{primary pre-emption agent} $i_p$ if an EMV is on one of its incoming links. The agent of the next intersection $i_s = \mathsf{Next}_{i_p}$ is refered to as a \emph{secondary pre-emption agent}.
The rest of the agents are \emph{normal agents}. We design these types since different agents have different local goals, which is reflected in their reward designs. 
% We assume normal agents can only execute normal phases, while primary and secondary pre-emption agents can execute both normal phases and pre-emption phases. 

\subsubsection{Agent design}
%\item \textbf{State}: The local state of an agent $i$ includes the number of vehicles on each outgoing lanes and incoming lanes, the distance of the EMV to the intersection, the estimated time of arrival and which link the EMV will be routed to:
\textbf{State}: The state of an agent $i$ at time $t$ is denoted as $s^t_i$ and it includes the number of vehicles on each outgoing lanes and incoming lanes, the distance of the EMV to the intersection, the estimated time of arrival ($\mathsf{ETA}$), and which link the EMV will be routed to ($\mathsf{Next}$), i.e.,
\begin{equation}
    s^t_i = \{x^t(l), x^t(m),  d^t_{\text{EMV}}[L_{ji}], \mathsf{ETA}^{t'}_i, \mathsf{Next}^{t'}_i \},%_{ji, im\in E, l \in L_{ji}},
\end{equation}
where $L_{ji}$ represents the links incoming to intersection  $i$ from its adjacent intersections $j\in\mathcal{N}_{i}$. With a slight abuse of notation, $l$ and $m$ denote the set of incoming and outgoing lanes, respectively. The vector $d^t_{\text{EMV}}$ contains the information about the distance of an EMV to the intersection is an EMV is present. For the intersection shown in Fig.~\ref{fig_movements}, $d^t_{\text{EMV}}$ is a vector of four elements. In particular, for primary pre-emption agents, one of the elements represents the distance of EMV to the intersection in the corresponding link and the rest of the elements are set to -1. For all other agents, $d^t_{\text{EMV}}$ are padded with -1. 
%where $L_{ji}$ represents the links incoming to intersection  $i$, $l$ includes all incoming lanes, i.e., $ji\in \mathcal{E}, l\in L_{ji}$, and $m$ includes all outgoing lanes, i.e., $ik\in \mathcal{E}, m \in L_{ik}$. For grid networks, $d^t$ contains four elements. For primary pre-emption agents, one of the elements represents the distance of EMV to the intersection in the corresponding link. The rest of the elements are set to -1. For all other agents, $d^t$ are padded with -1. 

\textbf{Action}: Prior work has focused on using phase switch, phase duration and phase itself as actions. In this work, we define the action of an agent as one of the 8 phases in Fig.~\ref{fig_phases}; this enables more flexible signal patterns as compared to the traditional cyclical patterns. 
Due to safety concerns, once a phase has been initiated, it should remain unchanged for a minimum amount of time, e.g. 5 seconds. Because of this, we set our MDP time step length to be 5 seconds to avoid rapid switch of phases. 
% We also assume that once a phase is executed, it must remain unchanged for at least a fixed number of time steps to avoid rapid phase changing.

\textbf{Reward}: PressLight has shown that minimizing the pressure is an effective way to encourage efficient vehicle passage. For normal agents, we adopt a similar idea, as shown in Eqn.~\ref{eqn:reward1}. For secondary pre-emption agents, we additionally encourage less vehicles on the link where the EMV is about to enter in order to encourage efficient EMV passage. Thus, the reward is a weighted sum of the pressure and this additional term, with a weight $\beta$, as shown in Eqn.~\ref{eqn:reward2}. For a default setting of balancing EMV navigation and traffic congestion alleviation, we choose $\beta=0.5$. For primary pre-emption agents, we simply assign a unit penalty at each time step to encourage fast EMV passage, as shown in Eqn.~\ref{eqn:reward3}. Thus, depending on the agent type, the local reward for agent $i$ at time $t$ is as follows
% Each intersection is controlled by an agent and each agent has partial observation of the entire traffic network. By proactively determining the traffic signal phases of each agent, we are trying to \textbf{(1)} dynamically navigate the EMV so it can arrive at the incident scene as soon as possible, and to\textbf{(2)} disrupt the original traffic flow as little as possible.
\begin{subnumcases}{r_{i}^{t} = \label{eqn:reward}}
  -P_{i}^{t} & $i \notin \{i_p, i_s\}$, \label{eqn:reward1}\\
  - \beta P_{i_s}^{t} - \frac{1-\beta}{|L_{i_pi_s}|}\sum\limits_{l\in L_{i_pi_s}} \frac{x(l)}{x_{max}(l)}  & $i=i_s,$ \label{eqn:reward2}\\
  -1 & $i=i_p$. \label{eqn:reward3} 
\end{subnumcases} 
%
\begin{figure}[h]
    \centering
    \includegraphics[width=\linewidth]{images/fig_agent_types.jpeg}
  \caption{Types of agents through the EMV's passage to the destination.}
  \label{fig_secondary}
\end{figure}
%
\textbf{Justification of agent design.} The quantities in local agent state can be obtained at each intersection using various technologies. Numbers of vehicles on each lane $(x^t(l), x^t(m))$ can be obtained by vehicle detection technologies, such as inductive loop \cite{gajda2001vehicle} based on the hardware installed underground. The distance of the EMV to the intersection $d^t_{EMV}[L_{ji}]$ can be obtained by \emph{vehicle-to-infrastructure} technologies such as VANET\cite{buchenscheit2009vanet}, which broadcasts the real-time position of a vehicle to an intersection. Prior work by \cite{wang2013design} and  \cite{noori2016connected} have explored these technologies for traffic signal pre-emption. 

The dynamic routing algorithm (Algorithm~\ref{alg:ETA_prepopulation}) can provide $(\mathsf{ETA}, \mathsf{Next})$ for each agent at every time step. However, due to the stochastic nature of traffic flows, updating the route too frequently might confuse the EMV driver, since the driver might be instructed a new route, say, every 5 seconds. 
% Moreover, different route might lead to frequent change of agent types. For example, an agent might be a secondary pre-emption agent after one update and be a normal agent a few seconds later. This is undesirable since the local reward for different agents are different. 
There are many ways to ensure reasonable frequency. One option is to inform the driver only once while the EMV travels on a single link. We implement it by updating the state of a RL agent $(\mathsf{ETA}^{t'}_i, \mathsf{Next}^{t'}_i)$ at the time step when the EMV travels through half of a link. For example, if the EMV travels through a link to agent $i$ from time step 11 to 20 in constant speed, then dynamic routing information in $s_i^{16}$ to $s_i^{20}$ are the same, which is $(\mathsf{ETA}_i^{15}, \mathsf{Next}_i^{15})$, i.e., $t'=15$.
% We use the notation $t'$ to indicate that $s_i^t$ might contain these information from different time steps.

As for the reward design, one might wonder how an agent can know its type. As we assume an agent can observe the state of its neighbors, agent type can be inferred from the observation. This will become clearer in Section~\ref{sec:MA2C}.


% \subsection{State Space $\mathcal{S}$ and Observation Space $\mathcal{O}$}

% Assuming there are $\mathcal{N}$ agents in the traffic network, and each agent has a partial observation of the whole network $s \in \mathcal{S}$ as its observation space $o \in \mathcal{O}$. We denote $o_{i}^{t}$ to be the observation space of the $i$th agent at MDP step $i$. $s_{i}^{t}$ consists of the following components:
% \begin{itemize}

% \item The distance of the EMV to the intersection. Broadcasting real-time position to the intersection can be achieved by \emph{vehicle-to-infrastructure} technologies such as VANET\cite{buchenscheit2009vanet}. Monitoring the distance of the EMV to the intersection helps determining the maintenance of the coordination phases.

% % The EMV en route broadcasts real-time position and destination location via VANET\cite{buchenscheit2009vanet}. Based on the residual distance of the EMV on approaching link $d_{l}$ and the number of non-EMVs, the agent estimates the link residual time. The link residual distance is one-hot encoded for all connecting links and invalidated, i.e. labeled as $-1$, on links without EMVs.

% \item Numbers of non-EMVs on all incoming and outgoing links. Vehicle detection technologies, such as inductive loop \cite{gajda2001vehicle}, can accurately decide the number of vehicles on each lane based on the hardware installed underground.

% \item The \emph{estimated time of arrival} (ETA) indicates the amount of time needed to travel from this intersection to the destination. The ETA associates with the corresponding adjacent intersection, advising the direction for proceeding from this intersection. With that being said, the EMV dispatched does not proactively determine the time-based shortest path. Instead, it receives a sequence of direction instructions to pursue the shortest path.

% \end{itemize}
% To summarize, an agent's state is represented as
% \begin{align*}
%     s_{i} = [\underset{\textbf{link residual distance}}{d_{N}, d_{W}, d_{S}, d_{E}}, \underset{\textbf{Volumes}}{V},
%     \underset{\textbf{ETA}}{\text{T}_{i}, j},
%     ],
% \end{align*}

% \dz{TODO: describe each of the variables and justify}

% The urban traffic network is densely connected and neighboring intersections to communicate and cooperate control strategies. 
% In this study, we define the cooperation scope of an intersection and all of its adjacent intersections. Thus, the observation space for an intersection $o_{i}^{t}$ includes the states for all agents in the defined cooperation scope.

% \subsection{Action $\mathcal{A}$}
 
% At time $t$, an agent $i$ will choose one of the candidate traffic signal phases $a_{i}^{t}$ from the predefined phase set $\mathcal{A}_{i}$. For the next $\Delta t$ period if time, the agent will exhibit the selected phase. Choosing the next phase as action results in a more flexible signal timing, and, as a result, the agent can learn phasing without assuming that the signal change cyclically. \hs{A lower limit on phase length is applied for the safe passage of any vehicle across the intersection.}

% The agent also latently selects the direction for EMV passage after this intersection whenever EMVs are approaching. \dz{A decision distance is prearranged so that the direction decision is determined based on its ETA information when the EMV passes, see Fig. \ref{fig_action_example}.}
% \begin{figure}[h]
%     \centering
%     \includegraphics[width=\linewidth]{images/fig_decision.jpeg}
%   \caption{Choosing action of northbound for the next link.}
%   \label{fig_action_example}
% \end{figure}


% \subsection{Transition Probability $\mathcal{P}$} 
% Given the system state $s^{t}$ and joint action selected by all agents $\mathbf{a}^{t}$, the system transits into the next state $s^{t+1}$ based on the transition probability $\mathcal{P}(s^{t+1}|s^{t}, \mathbf{a}^{t}): \mathcal{S} \times \mathcal{A}_{1} \times \mathcal{A}_{2} \times \dots \times \mathcal{A}_{N}$.


% \subsection{Reward $r$} 
% Each agent receives an immediate step reward $r_{i}^{t}$ at step $t$ and the joint action leads to the next system state $\mathcal{S}^{t} \times \mathcal{A}_{1} \times \mathcal{A}_{2} \times \dots \times \mathcal{A}_{N} \xrightarrow{} \mathcal{S}^{t+1}$. To facilitate the efficient passage of dispatched EMV as well as reduce the traffic impacts incurred, our reward function contains the following components:
% \begin{itemize}
%     \item A unit penalty for every step $\Delta t$ elapsed to motivate the efficient passage of the traveling EMV.
%     \item The pressure of the intersection inspired by \cite{wei2019presslight, varaiya2013max}. 
%     Since the traffic movement for going straight and turning right shares the same lane, it is conventional to divide the volume on the right lane for each movement separately.
%     Denote $x(m), x(n)$ as the numbers of vehicles on the source lane $m$ and target lane $n$ respectively, and we can compute the pressure at each intersection as
%     \begin{align*}
%         P = \Sigma_{(m,n)}|\frac{x(m)}{x_{max}(m)} - \frac{x(n)}{x_{max}(n)}|,
%     \end{align*}
%     where $x_{max}$ represents the maximum permissible number of vehicles on that lane.
%     \item A downstream traffic volume penalty. When an intersection is in pre-emption phase, its priority is to provide an efficient and safe passage for the EMV rather than reducing congestion. 
%     \hs{Thus, we encourage the vehicles to leave the target link by setting up a penalty on the number of vehicles on the target link.} This signal is communicated to the target intersection to reduce the number of vehicles on the connecting link as well.
    
% \end{itemize}
% The reward function for an agent $i$ at step $t$ is described as:
% \begin{align*}
%     r_{i}^{t} = \begin{cases}
%       -P_{i}^{t} & \text{\hs{Normal phases/No EMV present }}\\
%       -\frac{1}{2}x_{ij} - \frac{1}{2}P_{i}^{t} & \text{\hs{when you are the next intersection of some emergency phase intersection}}\\
%       -1 & \text{\hs{Emergency phases}}
%     \end{cases} 
% \end{align*}

% \subsection{Policy $\pi$ and Discounted Factor $\gamma$} 


\subsection{Multi-agent Advantage Actor-critic}
\label{sec:MA2C}
We adopt a multi-agent advantage actor-critic (MA2C) framework similar to \cite{chu2019multi} to address the coupling of EMV navigating and traffic signal control simultaneously in a decentralized manner. The difference is that our local state includes dynamic routing information and our local reward encourages efficient passage of EMV. Here we briefly introduce the MA2C framework.

In a multi-agent network $G(\mathcal{V}, \mathcal{E})$, the neighborhood of agent $i$ is denoted as $\mathcal{N}_i = \{ j | ji\in \mathcal{E} \textrm{ or } ij\in \mathcal{E}\}$. The local region of agent $i$ is $\mathcal{V}_i = \mathcal{N}_i \cup i$. We define the distance between two agents $d(i, j)$ as the minimum number of edges that connect them. For example, $d(i, i) = 0$ and $d(i, j)=1, \forall j \in \mathcal{N}_i$. In MA2C, each agent learns a policy $\pi_{\theta_i}$ (actor) and the corresponding value function $V_{\phi_i}$ (critic), where ${\theta_i}$ and ${\phi_i}$ are learnable neural network parameters of agent $i$.


\textbf{Local Observation.} In an ideal setting, agents can observe the states of every other agent and leverage this global information to make a decision. However, this is not practical in our problem due to communication latency and will cause scalability issues. We assume an agent can observe its own state and the states of its neighbors, i.e., $s^t_{\mathcal{V}_i} = \{s^t_j|j\in \mathcal{V}_i\}$. The agents feed this observation to its policy network $\pi_{\theta_i}$ and value network $V_{\phi_i}$.

% $\pi_{\theta_i}$ to decide signal phases. As for the value network, we leverage the spatial discount factor $\alpha$ to let the value function focus less on neighboring agents. Thus, the adjusted observation that are fed into the value network $V_{\phi_i}$ is $s^t_{\mathcal{V}_i} = s^t_i \cup \{ s^t_j|j\in \mathcal{N}_i \}$.

\textbf{Fingerprint.} In multi-agent training, each agent treats other agents as part of the environment, but the policy of other agents are changing over time. \cite{foerster2017stabilising} introduce \emph{fingerprints} to inform agents about the changing policies of neighboring agents in multi-agent Q-learning. \cite{chu2019multi} bring fingerprints into MA2C. Here we use the probability simplex of neighboring policies $\pi^{t-1}_{\mathcal{N}_i} = \{\pi^{t-1}_j|j\in \mathcal{N}_i\}$ as fingerprints, and include it into the input of policy network and value network. Thus, our policy network can be written as $\pi_{\theta_i}(a_i^t|s^t_{\mathcal{V}_i}, \pi^{t-1}_{\mathcal{N}_i})$ and value network as $V_{\phi_i}(s^t_{\mathcal{V}_i}, \pi^{t-1}_{\mathcal{N}_i})$, where $s^t_{\mathcal{V}_i}$ is the local observation with spatial discount factor introduced below.

\textbf{Spatial Discount Factor and Adjusted Reward.} MA2C agents cooperatively optimize a global cumulative reward. We assume the global reward is decomposable as $r_t = \sum_{i\in \mathcal{V}} r^t_i$, where $r^t_i$ is defined in Eqn.~\eqref{eqn:reward}. Instead of optimizing the same global reward for every agent, here we employ the spatial discount factor $\alpha$, introduced by \cite{chu2019multi}, to let each agent pay less attention to rewards of agents farther away. The adjusted reward for agent $i$ is 
\begin{equation}
    \Tilde{r}_i^t = \sum_{d=0}^{D_i}\Big( \sum_{j\in\mathcal{V}|d(i, j)=d} (\alpha)^d r^t_j\Big),
\end{equation}
where $D_i$ is the maximum distance of agents in the graph from agent $i$. When $\alpha > 0$, the adjusted reward include global information, it seems this is in contradiction to the local communication assumption. However, since reward is only used for offline training, global reward information is allowed. Once trained, the RL agents can control a traffic signal without relying on global information. 

\textbf{Temporal Discount Factor and Return.} 
The local return $\Tilde{R}^t_i$ is defined as the cumulative adjusted reward $\Tilde{R}^t_i := \sum_{\tau=t}^T \gamma^{\tau-t} \Tilde{r}^\tau_i$, where $\gamma$ is the temporal discount factor and $T$ is the length of an episode. We can estimate the local return using value function,
\begin{equation}
    \Tilde{R}^t_i = \Tilde{r}^t_i + \gamma V_{\phi_i^-}(s^{t+1}_{\mathcal{V}_i}, \pi^{t}_{\mathcal{N}_i}|\pi_{\theta_{-i}^-}),
\end{equation}
where $\phi_i^-$ means parameters $\phi_i$ are frozen and $\theta_{-i}^-$ means the parameters of policy networks of all other agents are frozen. 

\textbf{Network architecture and training.} 
As traffic flow data are spatial temporal, we leverage a long-short term memory (LSTM) layer along with fully connected (FC) layers for policy network (actor) and value network (critic). Fig. \ref{fig_model} provides an overview for the MA2C frameworks for EMVLight.
\begin{figure}[h]
    \centering
    \includegraphics[width=0.9\linewidth]{images/pipeline_revised.jpg}  
    \caption{Overview of MA2C framework for EMVLight's navigation and traffic signal control.}
    \label{fig_model}
\end{figure}
% We provide neural architecture details, policy loss expression,  value loss expression as well as a training pseudocode in the Appendix.

% \section{Network Training}
% \begin{table}[]
% \centering
% \begin{tabular}{@{}cc@{}}
% \toprule
% Hyper-parameters                & Value    \\ \midrule
% Optimizer                       & Adam     \\
% temporal discount factor $\gamma$ & 0.99     \\
% spatial discounted factor  $\gamma_{spatial}$    & 0.90     \\
% batch size                      & 1000     \\
% Initial learning rate           & 1e-3     \\
% learning rate decay             & constant \\
% MDP step length  $\Delta t$              & 5s       \\
% $\beta$ for secondary pre-emption & 0.5      \\
% entropy coefficient              & 0.01     \\ \bottomrule
% \end{tabular}
% \caption{Hyper-parameters used for training.}
% \label{tab_hyperparameters}
% \end{table}
\textbf{Value loss function}
With a batch of data $B = \{(s_i^t, \pi_i^t, a_i^t, s_i^{t+1}, r_i^t)_{i\in \mathcal{V}}^{t\in \mathcal{T}}\}$, each agent's value network is trained by minimizing the difference between bootstrapped estimated value and neural network approximated value
\begin{equation}
    \label{eqn:L_v}
    \mathcal{L}_v(\phi_i) = \frac{1}{2|B|} \sum_{B}\Big( \Tilde{R}^t_i - V_{\phi_i}(s^t_{\mathcal{V}_i}, \pi^{t-1}_{\mathcal{N}_i}) \Big)^2.
\end{equation}

\textbf{Policy loss function}
Each agent's policy network is trained by minimizing its policy loss
\begin{align}
    \label{eqn:L_p}
    \mathcal{L}_p(\theta_i) = -& \frac{1}{|B|}\sum_{B} \bigg(\ln \pi_{\theta_i}(a_i^t|s^t_{\mathcal{V}_i}, \pi^{t-1}_{\mathcal{N}_i}) \Tilde{A}^t_i \\
    &- \lambda \sum_{a_i \in \mathcal{A}_i} \pi_{\theta_i} \ln \pi_{\theta_i} (a_i | s^t_{\mathcal{V}_i}, \pi^{t-1}_{\mathcal{N}_i}) \bigg),
\end{align}
where $\Tilde{A}^t_i = \Tilde{R}^t_i - V_{\phi_i^-}(s^t_{\mathcal{V}_i}, \pi^{t-1}_{\mathcal{N}_i})$ is the estimated advantage which measures how much better the action $a^t_i$ is as compared to the average performance of the policy $\pi_{\theta_i}$ in the state $s_i^t$. The second term is a regularization term that encourage initial exploration, where $\mathcal{A}_i$ is the action set of agent $i$. For an intersection as shown in Fig. 1, $\mathcal{A}_i$ contains 8 traffic signal phases. 


\textbf{Training algorithm}
Algorithm \ref{alg:training} shows the multi-agent A2C training process. %
\setcounter{algocf}{0}
\renewcommand{\thealgocf}{S\arabic{algocf}}
\begin{algorithm}[ht]
    \caption{Multi-agent A2C Training}
    \label{alg:training}
    \SetEndCharOfAlgoLine{}
    \SetKwInOut{Input}{Input}
    \SetKwInOut{Output}{Output}
    \SetKwData{ETA}{ETA}
    \SetKwData{Next}{Next}
    \SetKwFor{ParrallelForEach}{foreach}{do (in parallel)}{endfor}
    \Input{\\\hspace{-3.7em}
        \begin{tabular}[t]{l @{\hspace{3.3em}} l}
        $T$ & maximum time step of an episode \\
        $N_{\mathrm{bs}}$ & batch size \\
        $\eta_\theta$  & learning rate for policy networks \\
        $\eta_\phi$    & learning rate for value networks \\
        $\alpha$       & spatial discount factor \\
        $\gamma$       & (temporal) discount factor\\ $\lambda$      & regularizer coefficient
        \end{tabular}
    }
    \Output{\\\hspace{-3.7em}
        \begin{tabular}[t]{l @{\hspace{1.4em}} l}
        $\{\phi_i\}_{i\in\mathcal{V}}$ & learned parameters in value networks \\
        $\{\theta_i\}_{i\in\mathcal{V}}$ & learned parameters in policy networks \\
        \end{tabular}
    }
    \textbf{initialize} $\{\phi_i\}_{i\in\mathcal{V}}$, $\{\theta_i\}_{i\in\mathcal{V}}$, $k \gets 0$, $B \gets \varnothing$;
    \textbf{initialize} SUMO, $t \gets 0$, \textbf{get} $\{s^0_i\}_{i\in\mathcal{V}}$\;
    \Repeat{Convergence}{
        \tcc{generate trajectories}
        \ParrallelForEach{$i \in \mathcal{V}$}{
            \textbf{sample} $a^t_i$ from $\pi^t_i$\;
            \textbf{receive} $\Tilde{r}^t_i$ and $s^{t+1}_i$\;
        }
        $B \gets B \cup \{(s_i^t, \pi_i^t, a_i^t, s_i^{t+1}, r_i^t)_{i\in \mathcal{V}}\}$\;
        $t \gets t+1$, $k \gets k+1$\;
        \If{$t == T$}{
            \textbf{initialize} SUMO, $t \gets 0$, \textbf{get} $\{s^0_i\}_{i\in\mathcal{V}}$\;
        }
        \tcc{update actors and critics}
        \If{$k == N_{\mathrm{bs}}$}{
            \ParrallelForEach{$i \in \mathcal{V}$}{
                \textbf{calculate} $\Tilde{r}^t_i$ (Eqn. (4)), $\Tilde{R}^t_i$ (Eqn. (5))\;
                $\phi_i \gets \phi_i - \eta_\phi \nabla \mathcal{L}_v(\phi_i)$\;
                $\theta_i \gets \theta_i - \eta_\theta \nabla \mathcal{L}_p(\theta_i)$\;
            }
            $k \gets 0, B \gets \varnothing$\;
        }
    }
\end{algorithm}
\FloatBarrier