\section{Literature Review}\label{sec_related_work}

\textbf{Conventional routing optimization and traffic signal pre-emption for EMVs.}
Although routing and pre-emption are coupled in reality, existing methods usually solve them separately. Many of the existing approaches leverage Dijkstra's shortest path algorithm to determine the optimal route~\cite{wang2013development, Mu2018Route, kwon2003route, JOTSHI20091}. Nordin et al.~\cite{nordin2012finding} proposed an A* algorithm for ambulance routing, assuming that routes and traffic conditions are fixed and static, which fails to address the dynamic nature of real-world traffic flows. Another line of work considers the temporal dynamics of traffic flows. For instance, Ziliaskopoulos et al.~\cite{ziliaskopoulos1993time} developed a shortest-path algorithm for time-dependent traffic networks, assuming that travel times associated with each edge are known in advance. Similarly, Musolino et al.~\cite{musolino2013travel} proposed routing strategies tailored to specific times of the day (e.g., peak/non-peak hours) based on historical traffic data. However, in the problem under consideration, routing and pre-emption strategies can significantly influence the travel time of each edge during the EMV passage, and the existing methods fail to handle such real-time changes effectively. Haghani et al.~\cite{haghani2003optimization} formulated the dynamic shortest path problem as a mixed-integer programming model, and Koh et al.~\cite{koh2020real} utilized reinforcement learning (RL) for real-time vehicle navigation and routing. Related studies~\cite{miller2000least, gao2006optimal, kim2005optimal, fan2005shortest, yang2014constraint, huang2012optimal, gao2012real, samaranayake2012tractable, nie2012optimal, thomas2007dynamic} explored adaptive routing problems in various stochastic and time-dependent traffic scenarios. However, none of these works considered the coupling of traffic signal pre-emption with EMV routing or the specific context of EMV passage when solving shortest path problems.

Once an optimal route for the EMV has been determined, traffic signal pre-emption is deployed to further reduce the EMV travel time. A common pre-emption strategy is to extend the green phases of traffic signals to allow the EMV to pass through intersections along a fixed optimal route~\cite{wang2013development, bieker2019modelling}. Pre-emption strategies for handling multiple EMV requests were introduced by Asaduzzaman et al.~\cite{Asaduzzaman2017APriority}. Wu et al.~\cite{wu2020emergency} approached the lane-clearing problem for emergency vehicles from a microscopic motion planning perspective, while Hosseinzadeh et al.~\cite{hosseinzadeh2022mpc} developed an EMV-centered traffic control scheme for multiple intersections to alleviate congestion. While these studies offer valuable insights into pre-emption strategies, they do not consider the dynamic routing of EMVs to determine the optimal path.

For a thorough survey of conventional routing optimization and traffic signal pre-emption methods, readers are referred to Lu et al.~\cite{Lu2019Literature} and Humagain et al.~\cite{humagain2020systematic}. It is worth noting that conventional methods prioritize EMV passage, often causing significant disturbances to traffic flow and increasing the average travel time for non-EMVs.

\textbf{RL-based traffic signal control.}
Traffic signal pre-emption only adjusts traffic phases at intersections where an EMV travels. However, reducing congestion often requires coordinated phase adjustments at nearby intersections. The problem of coordinating traffic signals to mitigate congestion has been addressed using deep reinforcement learning (RL) in an increasing body of research. Abdulhai et al.~\cite{abdulhai2003reinforcement} were among the first to apply Q-learning, a model-free RL method, to adaptive traffic signal control. Their work demonstrated the concept for both isolated traffic signal controllers and networks of controllers. However, as the state representation grows exponentially with the number of traffic signals, the learning process becomes computationally expensive, and their study was limited to isolated intersections. Prashanth et al.~\cite{prashanth2010reinforcement} introduced feature-based state representations to address the curse of dimensionality, reducing computational complexity while improving performance. El-Tantawy et al.~\cite{el2013multiagent} incorporated game-theoretic approaches into Q-learning, allowing agents to converge to best-response policies relative to their neighbors.

The advent of deep neural networks enabled more advanced applications of RL. Van der Pol et al.~\cite{van2016coordinated} integrated deep Q-networks (DQN) into traffic coordination tasks, which inspired subsequent research leveraging the Q-learning framework for traffic signal control. For instance, CoLight~\cite{wei2019colight} utilized graph attentional networks to enhance communication and cooperation between traffic signals, while FRAP~\cite{zheng2019frap} introduced a phase competition model to improve RL-based methods. PressLight~\cite{wei2019presslight} incorporated the max pressure strategy~\cite{varaiya2013max, LI2019Backpressure, LEVIN2020maxpressure, WANG2022Learning, Lazar2021Routing} into reward design, achieving better results than traditional designs. ThousandLights~\cite{ThousandLights} combined FRAP and PressLight to achieve city-level traffic signal control, and Zang et al.~\cite{Zang_Yao_Zheng_Xu_Xu_Li_2020} employed meta-learning algorithms to accelerate Q-learning.

Actor-critic methods represent another category of RL approaches that address the scalability challenge in multi-agent settings. Aslani et al.~\cite{aslani2017adaptive} demonstrated the effectiveness of actor-critic controllers for adaptive signal control under different disruption scenarios, while Xu et al.~\cite{xu2021hierarchically} introduced a hierarchical actor-critic method to foster cooperation between intersections. Chu et al.~\cite{chu2019multi} used a multi-agent independent advantage actor-critic algorithm, and Ma et al.~\cite{Ma2020Feudal} proposed a manager-worker hierarchy to manage traffic locally within actor-critic frameworks. While existing RL-based traffic control methods effectively reduce network congestion, they are not designed specifically for EMV pre-emption. In contrast, the proposed RL framework builds on state-of-the-art methods, such as max pressure, to minimize both EMV travel time and overall congestion. Moreover, the centralized training with decentralized execution paradigm enables compatibility with stochastic traffic settings while incurring minimal communication costs. Mo et al.~\cite{Mo2020CVLight} investigated traffic signal control strategies based on connected vehicle communications, illustrating the potential of RL in this domain.

For a detailed review of RL-based traffic signal control, see Noaeen et al.~\cite{RLSurvey2022Mohammad} and Wei et al.~\cite{wei2019survey}. While RL methods have demonstrated effectiveness in traffic signal control, no existing studies have explicitly addressed the unique challenges posed by EMV passages or their impact on non-EMV traffic flow.

\textbf{Intra-link EMV traversal strategies.}
Advances in vehicle-to-vehicle communication technology~\cite{LeBrun2005Knowledge} have sparked interest in optimizing EMV traversal strategies on congested road links. Agarwal et al.~\cite{Agarwal2016V2V} proposed Fixed Lane and Best Lane strategies for EMV traversal, which were further analyzed by Insaf et al.~\cite{Insaf2019Emergency}, showing that Fixed Lane outperforms Best Lane under conditions of low speed variance. Hannoun et al.~\cite{Hannoun2019Facilitating, hannoun2021sequential} developed a semi-automated warning system that instructs downstream traffic to yield using mixed-integer linear programming. This system adapts to varying connected vehicle penetration rates and is validated as computationally efficient for real-time deployment. Su et al.~\cite{su2021dynamic} proposed a dynamic queue-jump lane (DQJL) strategy leveraging multi-agent reinforcement learning.