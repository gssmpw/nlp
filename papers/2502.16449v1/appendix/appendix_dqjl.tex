\chapter{Appendix for Chapter 2}
In this appendix, we provide a comprehensive description of the network architectures for CAVs in Section~\ref{sec:network_design_for_CAVs}, along with the SUMO configurations and MAPPO-DQJL parameters detailed in Section~\ref{sec:sumo_configurations}.

\section{Network Details for CAVs}\label{sec:network_design_for_CAVs}
\subsection{Policy Network}\label{subsec:appendix_policy_network}
\[
\pi_{\theta}(a_i \mid o_i) = \text{PolicyHead}\bigl(\text{TransformerEncoder}(\text{PositionalEncoding}(\text{Embedding}(o_i)))\bigr).
\]
\noindent\textbf{Architecture Overview (CAVs):}
\begin{itemize}
    \item \textbf{Input:} Observation \(o_i\)
    \item \textbf{Embedding Layer:} Dimension \( d_{\text{model}} = 256 \)
    \item \textbf{Positional Encoding:} Spatial and temporal context
    \item \textbf{Transformer Encoder:} 
    \begin{itemize}
        \item Number of Layers: \( N = 6 \)
        \item Attention Heads: \( h = 8 \)
        \item Hidden Dimension: \( d_{\text{hidden}} = 528 \)
        \item Feed-Forward Dimension: \( d_{\text{ff}} = 1024 \)
        \item Activation: ReLU, Dropout Rate: 0.1
    \end{itemize}
    \item \textbf{Policy Head:} Fully connected layer producing action probabilities
\end{itemize}
  
\subsection{Centralized Value Network}\label{subsec:appendix_value_network}
\[
V_{\phi}(s) = \text{ValueHead}(\text{TransformerEncoder}(\text{Embedding}(s))).
\]
\noindent\textbf{Architecture Overview (CAVs):}
\begin{itemize}
    \item \textbf{Input:} Global state $s$
    \item \textbf{Embedding Layer:} Dimension $d_{\text{model}} = 256$
    \item \textbf{Transformer Encoder:}
    \begin{itemize}
        \item Number of Layers: $M = 6$
        \item Attention Heads: $h = 8$
        \item Hidden Dimension: $d_{\text{hidden}} = 512$
        \item Feed-Forward Dimension: $d_{\text{ff}} = 2048$
        \item Activation: ReLU, Dropout Rate: $0.1$
    \end{itemize}
    \item \textbf{Value Head:} Fully connected layer producing a scalar value
\end{itemize}

\subsection{Transformer Encoder Layer}\label{subsec:appendix_tf_encoder_layer}
Each transformer encoder layer operates over representations of dimension \( d_{\text{model}} = 256 \) (as adopted in the policy network design for CAVs) and comprises:
\begin{itemize}
    \item \textbf{Multi-Head Attention:} 
    \begin{itemize}
        \item Number of Heads: \( h = 8 \)
        \item Key/Value Dimension per Head: \( d_k = d_v = \frac{d_{\text{model}}}{h} = 32 \)
    \end{itemize}
    The output of the multi-head attention is projected back to \( d_{\text{model}} = 256 \).
    \item \textbf{Feed-Forward Network:} 
    Two fully connected layers with \( d_{\text{ff}} = 4096 \) and ReLU activation.
    \item \textbf{Residual Connections:} 
    Applied after both the multi-head attention and feed-forward sub-layers, improving gradient flow and stability.
    \item \textbf{Layer Normalization:}
    Applied to each sub-layer’s output (after residual addition) to stabilize training dynamics.
\end{itemize}

\subsection{Number of Parameters}\label{subsec:number_of_parameters}
Most parameters arise from the Transformer Encoder layers, each MHA and FFN sub-layers. For both the policy and value networks, MHA (with $8$ heads and $d_{\text{model}}=256$) requires four linear transformations (queries, keys, values, and output), each $256\times256$, yielding about $4 \times 256 \times 256 \approx 262{,}000$ parameters per layer. In the policy network, the FFN has $d_{\text{ff}}=1024$, contributing $2 \times 256 \times 1024 \approx 524{,}000$ parameters per layer. Thus, each encoder layer adds roughly $786{,}000$ parameters; for $6$ layers, this is about $4.7$ million parameters, plus a smaller amount for embeddings and the policy head.

In the value network, with $d_{\text{ff}}=2048$, each layer’s FFN has about $2 \times 256 \times 2048 \approx 1{,}048{,}000$ parameters. Combined with MHA, each layer adds approximately $1.31$ million parameters; over $6$ layers, around $7.9$ million parameters in total, again plus modest contributions from embeddings and the value head.

\section{SUMO Configurations}\label{sec:sumo_configurations}
\subsection{Car-Following Model}\label{subsec:appendix_car_following}
In SUMO, the car-following behavior of vehicles is modeled using the Intelligent Driver Model (IDM), which governs how a vehicle adjusts its speed based on the proximity and dynamics of the vehicle ahead. The acceleration \(a(t)\) of a vehicle is defined as:
\[
a(t) = a_{\text{max}} \left( 1 - \left( \frac{v(t)}{v_0} \right)^4 - \left( \frac{s^*(v(t), \Delta v)}{s(t)} \right)^2 \right),
\]
where \(a_{\text{max}}\) is the maximum achievable acceleration, \(v(t)\) is the current speed, and \(v_0\) denotes the desired (free-flow) speed. The variable \(s(t)\) represents the current gap between the vehicle and the one immediately in front, while \(\Delta v\) is the speed difference between these two vehicles.

The desired gap \(s^*(v(t), \Delta v)\) is given by:
\[
s^*(v(t), \Delta v) = s_0 + v(t)T + \frac{v(t) \Delta v}{2\sqrt{a_{\text{max}} b}},
\]
where \(s_0\) is the minimum safety gap, \(T\) the desired time headway, and \(b\) the comfortable deceleration. By maintaining a safe following distance and adjusting speed in response to the leading vehicle, IDM ensures realistic and safe traffic flow dynamics.

In the context of DQJL formation, IDM serves as a baseline safety constraint. Both HDVs and CAVs adhere to IDM-based accelerations and distance requirements, guaranteeing that their interactions remain safe and well-coordinated even under dynamic and congested traffic conditions.
\subsection{Lane-Changing Model}\label{subsec:appendix_lane_changing}
The lane-changing behavior in SUMO employs a rule-based framework that assesses both safety and incentive conditions prior to executing a maneuver. Safety is paramount, ensuring that a vehicle can merge into the target lane without inducing hazardous situations. The safety condition, accounting for deceleration capabilities and reaction times, is expressed as:
\[
v(t) + bT \leq v_{\text{leader}}(t) \quad \text{and} \quad v_{\text{follower}}(t) + bT \leq v(t),
\]
where \( v_{\text{leader}}(t) \) and \( v_{\text{follower}}(t) \) are the speeds of the leading and following vehicles in the target lane, respectively. The parameter \( b \) denotes the comfortable deceleration, while \( T \) represents the driver’s reaction time or headway. These inequalities guarantee that both the merging vehicle and those in the target lane can safely adjust their speeds if necessary.

In addition to satisfying the safety condition, the lane change must offer a tangible incentive. A vehicle will opt to change lanes only if the anticipated speed in the target lane is substantially higher than in its current lane. This incentive criterion ensures that maneuvers are not only safe, but also yield an improvement in travel efficiency, thereby supporting the formation of DQJLs without compromising overall traffic stability.

\subsection{Parameters, Coefficients and Training Hyper-parameters for MAPPO-DQJL}
\begin{table}[htbp]
\centering
\scriptsize
\begin{tabular}{lccc}
\toprule
\textbf{Parameter} & \textbf{Description} & \textbf{Value} & \textbf{Units}\\
\midrule
\multicolumn{4}{l}{\textit{Car-Following (IDM) Parameters}} \\[6pt]
$s_0$        & Minimum safety gap                         & 2.5        & m \\
$T$          & Desired time headway                       & 1.2        & s \\
$v_0$        & Desired (free-flow) speed                  & 20.0       & m/s \\
$a_{\text{max}}$ & Maximum acceleration                     & 1.5        & m/s$^2$ \\
$b$          & Comfortable deceleration                   & 2.5        & m/s$^2$ \\[6pt]

\multicolumn{4}{l}{\textit{Lane-Changing Model Parameters (Same $b$ and $T$ as IDM)}} \\[6pt]
\midrule
\multicolumn{4}{l}{\textit{Global Reward Coefficients}} \\[6pt]
$\alpha$     & Global-vs-Local reward balance             & 0.75        & -- \\
$\beta_1$    & EMV time penalty weight                    & 0.02       & 1/s \\
$\beta_2$    & EMV lane-change penalty weight             & 1.0        & 1/(\#lane changes) \\
$\beta_3$    & DQJL completeness reward weight            & 1.0        & 1/m \\[6pt]
\midrule
\multicolumn{4}{l}{\textit{Local Reward Coefficients (CAVs)}} \\[6pt]
$\eta_1$     & CAV lane-change penalty weight             & 1.0        & 1/(\#lane changes) \\
$\eta_2$     & CAV acceleration penalty weight            & 0.2        & 1/(m/s$^2$) \\[6pt]
\midrule
\multicolumn{4}{l}{\textit{Local Reward Coefficients (HDVs)}} \\[6pt]
$\eta_3$     & HDV yield reward weight                    & 1.0        & 1/(yield action) \\[6pt]
\midrule
\multicolumn{4}{l}{\textit{MDP and Discount Factor}} \\[6pt]
$\gamma$     & Future reward weighting                    & 0.99       & -- \\
\textbf{MDP Timestep} & Simulation step per action        & 0.5        & s \\[6pt]
\midrule
\multicolumn{4}{l}{\textit{Training Hyper-parameters}} \\[6pt]
Learning Rate & Policy and Value Function Optimizer       & $1 \times 10^{-4}$ & -- \\
Batch Size    & Number of samples per update              & 64         & -- \\
Horizon       & Steps per rollout before update           & 1024       & steps \\
PPO Epochs    & Number of PPO update epochs per batch     & 10         & -- \\
Clip Range    & PPO clip range for policy updates         & 0.2        & -- \\
GAE $\lambda$ & Advantage estimation parameter            & 0.95       & -- \\
\bottomrule
\end{tabular}
\caption{Parameter summary for MAPPO-DQJL simulations}
\label{tab:parameters_summary}
\end{table}
