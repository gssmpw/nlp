\chapter{Dynamic Queue-jump Lane for Emergency Vehicles: a Multi-agent Proximal Policy Optimization Approach}\label{chap:mappo-dqjl}
\section{Introduction}
\label{sec:introduction}
The increasing population and urbanization have made it exceedingly difficult to operate urban emergency services efficiently. Over the past decade, response times for emergency vehicles (EMVs), such as ambulances, fire trucks, and police vehicles, have worsened due to rising traffic congestion. Historical data from New York City, USA~\cite{NY2019}, shows that the number of annual emergency incidents increased from 1,114,693 in 2004 to 1,352,766 in 2014, with average response times increasing from 7 minutes 53 seconds to 9 minutes 23 seconds~\cite{Emergency2014}. This represents a 20\% increase in response times over a decade. Furthermore, in fiscal year 2023, the city's emergency response times worsened by an additional 82 seconds—16.1\% higher than in 2019~\cite{Calder_2023}. These delays are especially critical in life-threatening situations, such as cardiac arrests, where every minute without defibrillation reduces survival chances by 7\% to 10\%, with survival rates dropping sharply after 8 minutes~\cite{Heart2013}.

Given the urgent need to reduce EMV response times, technological solutions must be explored to mitigate the impact of traffic congestion. One promising direction is the use of connected vehicle technologies, specifically vehicle-to-everything (V2X) communication, which allows for real-time information sharing between EMVs, non-EMVs, and traffic management systems. These systems enable EMVs to receive optimal routing information based on current traffic conditions, and they facilitate coordination between non-EMVs to clear paths for approaching EMVs.

This study focuses on a critical yet specific challenge within the focused context of intra-link movement of EMVs: the establishment of dynamic queue jump lanes (DQJL) to expedite EMV passage through congested road segments.

\begin{definition}[Dynamic Queue Jump Lane]\label{def:dqjl}
A DQJL is a dynamically formed lane created while an EMV is approaching or en route, clearing downstream space to facilitate its passage. 
\end{definition}

A queue jump lane (QJL) is typically used in bus operations to allow buses to bypass long queues before intersections~\cite{Zhou2005Performance,Cesme2015Queue}. 
In the context of EMV movement, we introduced DQJL as in Definition~\ref{def:dqjl}. The formation of these lanes is facilitated by V2X technology, which enables connected and autonomous vehicles (CAVs) to coordinate their maneuvers based on real-time instructions. However, the challenge is not only to ensure rapid EMV passage but also to minimize the disruption to surrounding traffic, particularly the non-connected human-driven vehicles (HDVs), which are not directly controllable.

The task of DQJL formation is highly intricate, as it requires coordinating CAVs to influence the behavior of HDVs indirectly, a task fraught with uncertainty due to the unpredictability of human drivers. The key objective is to minimize the time it takes for the EMV to traverse the congested segment while ensuring reduced maneuvers from non-EMVs, particularly HDVs, motivating all maneuvers to be efficient. Striking a balance between these objectives—optimizing EMV passage time while reducing the complexity of maneuvers for non-EMVs—requires sophisticated coordination strategies.

To address these challenges, we propose modeling the DQJL formation as a partially observable Markov decision process (POMDP), which allows for the incorporation of both controllable CAVs and uncontrollable HDVs. The POMDP framework enables agents (CAVs) to make real-time decisions based on partial observations of the traffic environment. We adopt a multi-agent actor-critic deep reinforcement learning approach to derive optimal coordination strategies, enabling CAVs to dynamically clear lanes for EMVs while minimizing the impact on other vehicles. This approach accounts for partial connectivity, ensuring flexibility with varying levels of CAV penetration rates in mixed traffic environments.

The contributions of this work are threefold. First, we introduce the concept of DQJL as a novel traffic management strategy, providing a scalable framework for its dynamic formation in mixed traffic environments. This framework addresses the inherent complexities of heterogeneous traffic systems by integrating both CAVs and HDVs. Second, we design a multi-agent reinforcement learning algorithm tailored to optimize DQJL formation. This algorithm leverages centralized training with decentralized execution to effectively coordinate CAVs while accounting for the stochastic and reactive behavior of HDVs, ensuring robust performance under varying traffic conditions. Finally, through extensive SUMO-based simulations, we validate the proposed system’s effectiveness. Our results demonstrate substantial improvements over baseline scenarios, achieving significant reductions in EMV passage time (up to 39.8\%) and overall lane-changing maneuvers (up to 55.7\%), particularly under scenarios with increasing CAV penetration rates.

The remainder of this chapter is organized as follows. In Section~\ref{sec:literature_review}, we review relevant studies on queue-jump lanes and multi-agent reinforcement learning (MARL) frameworks in connected vehicle applications. Section~\ref{sec:modeling} formulates the DQJL problem as a partially observable Markov decision process (POMDP), differentiating between CAVs and HDVs in mixed traffic scenarios. In Section~\ref{sec:learning}, we present our MARL-based approach for solving the DQJL problem, detailing the network architecture and learning algorithms. Section~\ref{sec:experimental_setup} outlines the experimental setup. In Section~\ref{sec:discussion}, we present experimental results and key insights and a conclusion is drawn in Section~\ref{sec:conclusion}.

\section{Literature Review}
\label{sec:literature_review}
In this section, we discuss literature available on queue jump lane in Subsec.~\ref{sec:dqjl_literature}, and existing multi-agent deep reinforcement learning techniques on similar connected vehicle applications in Subsec. \ref{sec:marl_literature}.

\subsection{Queue Jump Lane for EMVs}
\label{sec:dqjl_literature}
Deep learning has recently demonstrated significant promise across various applications \cite{you2019ct,you2018structurally,lyu2018super,lyu2019super,you2019low,you2020unsupervised,guha2020deep}.
Queue jump lanes (QJLs) have never been used for EMV deployment and represent a novel operational strategy that leverages CAV technologies for EMV deployment. Although QJLs are relatively new, the literature already demonstrates their positive effects in reducing travel time variability, particularly when combined with transit signal priority (TSP). However, these studies are primarily based on moving-bottleneck models for buses \cite{Zhou2005Performance,Cesme2015Queue,cheng2017body,cheng2016hybrid}. We adapt this bus operation strategy for EMV deployment in our setting, given that EMVs typically travel faster than non-EMVs and can “preempt” non-EMV traffic due to their priority status.

At the same time, traditional siren technologies often fail to provide adequate warning time for other vehicles, and there is frequently a lack of clarity regarding which route should be avoided \cite{chen2021adaptive,you2021mrd,liu2021aligning}. This confusion, particularly under highly congested conditions, leads to increased delays, as mentioned earlier, and contributes to accident rates that are 4 to 17 times higher \cite{Buchenscheit2009AVE,cheng2016identification}, as well as increased collision severity \cite{Yasmin2012Effects,cheng2016random,li2019novel}, both of which can further delay EMV response times. A study by Savolainen et al.~\cite{Savolainen2010Effects}, utilizing data from Detroit, Michigan, confirmed the sensitivity of driver behaviors to various ITS communication methods for conveying EMV route information. 
Kohneh et al.~\cite{Kohneh2024twoways} adopts a hybrid non-dominated sorting genetic algorithm II-particle swarm optimization (NSGAII-PSO) approach to solve the bi-objective optimization problem which minimizes EMV travel time while maximizing safety.

Wu et al.~\cite{WU2020preclearing} proposed a lane pre-clearing strategy for normal road segments, employing cooperative driving with CAVs to maintain EMV speed while minimizing traffic disruption. Their approach formulates the problem as a bi-level optimization and employs an EMV sorting algorithm, effectively clearing lanes and identifying a linear relationship between optimal solutions and road density to improve EMV routing decisions. Hannoun et al.~\cite{Hannoun2019Facilitating} utilized mixed-integer linear programming (MILP) to develop a static path clearance control strategy for approaching EMVs. A follow-up work~\cite{hannoun2021sequential}, relying on V2X, extended the problem to larger distances by dividing roads into segments, allowing for a divide-and-conquer approach to optimization. These approaches simplify the road network into cells, approximating vehicle positions and velocities, limiting its precision and applicability to complex real-world traffic scenarios. Moreover, the computational complexity of MILP-based approaches makes them less suited for real-time applications in dynamic traffic settings. 

QJLs have not yet been studied as a dynamic control strategy, underscoring the need to account for uncertainties in realistic, non-deterministic traffic conditions, such as yielding to an approaching EMV \cite{liu2021auto,you2022megan,liu2022retrieve}. To address this gap, we introduce the concept of dynamic queue jump lanes (DQJLs), as illustrated in Fig.~\ref{fig:dqjl}. During the DQJL clearing process for an approaching EMV, non-EMVs are continuously monitored and instructed, enabling the rapid and safe establishment of DQJLs. Additionally, we explore the application of DQJLs in partially connected scenarios, where only a subset of vehicles are equipped with communication devices, allowing for partial communication between vehicles and infrastructure.

\begin{figure}[htbp]
    \centering
    \begin{subfigure}{0.7\textwidth}
        \centering
        \includegraphics[width=\linewidth]{Figures/dqjl_before.png}
        \caption{Actions planning.}
    \end{subfigure}

    \vspace{0.2cm} % Adds some vertical space between the figures

    \begin{subfigure}{0.7\textwidth}
        \centering
        \includegraphics[width=\linewidth]{Figures/dqjl_after.png}
        \caption{DQJL established.}
    \end{subfigure}
    \caption{Illustration of a DQJL formation: Green vehicles are CAVs and yellow ones are HDVs. HDVs yields and CAVs perform coordinated lane changes for a DQJL.}
    \label{fig:dqjl}
\end{figure}

A related task, referred to as corridor clearance, is investigated by Suo et al.~\cite{suo2024model,liu2020rethinking}, who propose a proximal policy optimization (PPO) method to enhance clearance between intersections. In their approach, the corridor is formed by designating a single CAV as a "splitting point", which blocks following traffic to allow the EMV to switch into a less congested lane. However, their study adopts a single-agent perspective, focusing solely on the interaction between a CAV and an EMV, thereby limiting the collaborative potential of multiple CAVs in the system. Additionally, the simulation was conducted on a simple two-lane road segment, failing to capture the more complex interactions between CAVs and background traffic in multi-lane environments.
This "splitting point" design, however, suggests that DQJLs do not always have to be a straight line. Instead, they can consist of different lanes that require minimal lane-change maneuvers by the EMV. While this might slightly increase the EMV's passage time, it minimizes disruption to the surrounding traffic, see Fig.~\ref{fig:lane_change_dqjl}.

\begin{figure}[h]
    \centering
    \begin{subfigure}{0.7\textwidth}
        \centering
        \includegraphics[width=\linewidth]{Figures/alt_dqjl_before.png}
        \caption{Actions planning.}
    \end{subfigure}

    \vspace{0.2cm} % Adds some vertical space between the figures

    \begin{subfigure}{0.7\textwidth}
        \centering
        \includegraphics[width=\linewidth]{Figures/alt_dqjl_after.png}
        \caption{DQJL established.}
    \end{subfigure}
    \caption{An alternative DQJL formation, where the EMV will change lane once.}
    \label{fig:lane_change_dqjl}
\end{figure}

\subsection{Multi-agent Reinforcement Learning for CAVs}\label{sec:marl_literature}
The proliferation of V2X communication technologies has significantly increased the presence of CAVs, driving extensive research into multi-agent reinforcement learning (MARL) frameworks for vehicular applications \cite{zhou2023survey,henao2023multiclass,liu2023medical,cai2023unsupervised}. In these systems, individual vehicles are modeled as autonomous agents collaborating toward shared objectives. This paradigm offers several advantages, including decentralized decision-making, scalability, adaptability to dynamic traffic conditions, and the capacity for collaborative learning among vehicles.

MARL in CAV scenarios presents a promising approach to addressing complex transportation challenges, particularly where traditional methods have shown limitations. Recent studies have applied MARL to various aspects of traffic management and vehicle coordination, highlighting its versatility and effectiveness. For instance, in the realm of congestion mitigation, Ha et al.~\cite{ha2020leveraging} developed a MARL-based control model using graph convolutional networks (GCN) and deterministic policy gradient (DDPG) techniques for highway scenarios, outperforming traditional speed harmonization methods even with low CAV penetration rates. Similarly, Nakka et al.~\cite{Nakka2022Merging} applied a decentralized multi-agent deterministic policy gradient (MADDPG) approach to highway merging, successfully eliminating stop-and-go traffic.

Intersection management has also been a key focus for MARL applications \cite{you2023rethinking,liu2023benchmarking,yan2024after,han2024hybrid,ma2024segment}. Guo et al.~\cite{guo2022coordination} employed an enhanced monotonic value function factorisation (QMIX) algorithm to coordinate CAVs at unsignalized intersections in mixed-autonomy traffic, while Antonio et al.~\cite{antonio2022multi} introduced an advanced autonomous intersection management (AIM) system using MARL and Curriculum through Self-Play, significantly surpassing traditional traffic light systems in performance.

In vehicle control, Zhou et al.~\cite{zhou2022multi} proposed an MA2C method for lane-changing decisions in mixed traffic environments, utilizing a multi-objective reward function. Shi et al.~\cite{SHI2021Connected} presented a cooperative longitudinal control strategy for CAVs in mixed traffic, improving both string stability and efficiency. Additionally, Chen et al.~\cite{Chen2023OnRamp} addressed mixed-traffic highway on-ramp merging with a MARL framework, enabling autonomous vehicles to collaborate and adapt to human-driven vehicles (HDVs).

Beyond traffic flow management, MARL has demonstrated potential in other CAV applications \cite{liang2024rescuing,cao2024multi,you2024calibrating}. Zhang et al.~\cite{ZHANG2022parking} developed a MARL framework for online parking assignment in mixed CV and non-connected vehicle (NCV) environments, achieving substantial improvements over conventional methods. In the domain of robotic driving simulation, Palanisamy et al.~\cite{Palanisamy2020Driving} introduced MACAD-Gym, a multi-agent simulation platform for CAVs, which supports research into deep reinforcement learning (DRL)-based algorithms in complex, multi-agent environments beyond geo-fenced operational design domains.

Parada et al.~\cite{parada2023safe} have proposed a MAPPO learning framework for autonomous vehicle maneuvering in the presence of EMVs. However, this study assumes a fully autonomous traffic setting and focuses primarily on collision risk avoidance. While effective for collision minimization, this design de-prioritizes the reduction of EMV passage time, which is a critical factor in emergency response. Such an approach may also not be feasible for near-term deployment in mixed autonomy environments.

These diverse applications of MARL in CAV contexts underscore its potential to revolutionize traffic management and vehicle coordination \cite{you2024mine,sun2024medical,huang2024cross}. For those interested in a more comprehensive review of MARL frameworks for CAV applications in mixed traffic environments, Yadav et al.~\cite{yadav2023comprehensive} and Dinneweth et al.~\cite{Dinneweth2022Survey} provide valuable insights into the current state of the art and emerging trends in this rapidly advancing field.

Despite the extensive research on MARL in CAV applications, there is currently no work that employs MARL and focuses on pre-clearing lanes for EMVs in heterogeneous traffic settings. This gap presents an exciting opportunity for this research to explore how MARL can be leveraged to optimize the formation and management of DQJLs, potentially resulting in more efficient and adaptive traffic management strategies in urban environments.

\section{Dynamic Queue Jump Lanes in Mixed Traffic}
In this section, we describe the modeling of DQJL formation as a discrete-time partially observed Markov Decision Process (POMDP) in Subsec.~\ref{subsec:system_modeling}. Additionally, in Subsec.~\ref{subsec:agent_design}, we introduce two distinct agent designs for CAVs and HDVs, enabling the study of system performance under varying levels of connectivity penetration. This approach allows for a comprehensive evaluation of DQJL formation dynamics in mixed traffic environments.
\label{sec:modeling}

\subsection{System Modeling}
\label{subsec:system_modeling}
We formulate the establishment of DQJLs in heterogeneous traffic as a collaborative multi-agent system. Each non-EMV is an autonomous agent, with the agent set $\mathcal{N}$ comprising two subsets:

\begin{equation}
\mathcal{N} = \mathcal{N}_{\text{CAV}} \cup \mathcal{N}_{\text{HDV}}
\end{equation}

DQJLs can be formed either before or when EMVs enter a road segment. To investigate interactions between EMVs and non-EMVs, we assume an EMV begins traversing a congested road segment at time $t_0$, where the EMV itself is not considered an active agent but passively awaits the clearing of a path. In this scenario, CAVs leverage V2X communication capabilities, while HDVs rely solely on local sensory information. All agents collaborate to create a clear path for the EMV.

This mixed autonomy environment is modeled as a Partially Observable Markov Decision Process (POMDP), represented by the tuple $(S, A, T, R, \Omega, O, \gamma)$. In this formulation, $S$ denotes the global state space that represents the overall traffic environment, including the positions, speeds, and states of CAVs, HDVs, and the EMV. Each agent takes actions from the action space $A$, with choices such as lane changes or speed adjustments aimed at optimizing DQJL formation. The transition function $T: S \times A \rightarrow S'$ defines how the state of the environment evolves probabilistically in response to agents' actions.

The reward function $R_i$, specific to each agent $i \in \mathcal{N}$, balances global system-wide objectives with individual goals. Observations are partial, where $\Omega$ represents the observation space for each agent, and the observation function $O$ maps the global state to the local observation available to each agent. Notably, the observation space and observation function differ between CAVs and HDVs, reflecting their distinct capabilities, which will be further discussed in Subsec~\ref{subsec:agent_design}. The process is further governed by a discount factor $\gamma$, which moderates the trade-off between immediate and future rewards.

\subsection{Agent Design}
\label{subsec:agent_design} 
This study proposes two realistic and straightforward designs for CAVs and HDVs, tailored to their respective communication, perception capabilities, and maneuverability. 

\subsubsection{Connected and Automated Vehicles (CAVs)}
The POMDP for the \(i\)-th CAV agent is defined as:

\begin{equation}
\text{POMDP}_{\text{CAV}}^i = \left( S^i, A^i, T^i, R^i, \Omega^i, O^i, \gamma \right)
\end{equation}

\paragraph{State Space}
The state space for the \(i\)-th CAV is given by:

\begin{equation}
S^i = \{x, y, v, d_\text{EMV}, l_\text{EMV}, d_\text{clear}\}
\end{equation}

In this formulation, \(x\) denotes the longitudinal position of the CAV, while \(y\) refers to the lateral position, represented as an integer indicating the lane index. The variable \(v\) describes the velocity of the CAV. The term \(d_\text{EMV}\) represents the longitudinal distance to the approaching EMV, and \(l_\text{EMV}\) denotes the lane difference with respect to the EMV. This lane difference is expressed as an integer, where positive or negative values indicate the relative lane position of the CAV compared to the EMV, allowing for multiple lane differences (e.g., \(-2, -1, 0, 1\)). Lastly, \(d_\text{clear}\) stands for the clear distance ahead, providing essential feedback for the agent to learn lane-changing strategies and evaluate the completeness of DQJL formation. Based on this design, the responsibility for effective DQJL formation predominantly rests with CAVs.

\paragraph{Action Space}
The action space for the \(i\)-th CAV is decomposed into longitudinal and lateral actions:

\begin{equation}
A^i = A_{\text{long}} \times A_{\text{lat}}
\end{equation}

The longitudinal action space \( A_{\text{long}} \) consists of discrete control actions for speed adjustment, allowing a CAV to accelerate, decelerate, or maintain its current speed. Specifically, we discretize the action space as follows:

\[
A_{\text{long}} = \{-2.5 \text{ m/s}^2,\, 0 \text{ m/s}^2,\, +1.5 \text{ m/s}^2\}.
\]

Here, a negative value corresponds to deceleration, zero indicates maintaining the current speed, and a positive value corresponds to acceleration. These choices reflect representative urban driving behaviors, enabling the CAV to respond flexibly to varying traffic conditions, including the distance to the EMV (\(d_{\text{EMV}}\)) and the clear distance ahead (\(d_{\text{clear}}\)).


The lateral action space, \(A_{\text{lat}}\), determines whether the CAV performs a lane change. It includes actions to indicate the direction of lane changes, where \(A_{\text{lat}} = 1\) denotes a lane change to the right, \(A_{\text{lat}} = -1\) indicates a lane change to the left, and \(A_{\text{lat}} = 0\) means no lane change. These lateral actions are based on the CAV’s current lane position \(y\), the lane difference relative to the EMV (\(l_{\text{EMV}}\)), and the state of neighboring vehicles \(\mathbf{N}_{\text{local}}\), allowing the agent to perform lane changes that contribute to efficient DQJL formation.

\paragraph{Transition Function}
The transition function \(T^i\) represents the probabilistic model of how the environment changes in response to the CAV’s actions. Given the current state \(S^i\) and action \(A^i\), \(T^i\) models the transition to the next state. This includes the changes in the CAV's position, speed, and the dynamic states of neighboring vehicles based on both local actions and external factors like traffic flow and EMV behavior. Mathematically, this can be expressed as:
\[
T^i_{\text{CAV}}(S^i_t, A^i_t) \sim P(S^i_{t+1} \mid S^i_t, A^i_t),
\]

\paragraph{Observation Space and Function}
The observation space \(\Omega^i\) consists of information that the \(i\)-th CAV can perceive from the environment. The observation function \(O^i\) maps the global state \(S^i\) to the partial observation \(\Omega^i\) available to the CAV. Mathematically, this can be expressed as:

\[
\Omega^i = O^i(S^i) \quad \text{where} \quad O^i: S^i \rightarrow \Omega^i
\]

The observation space \(\Omega^i\) is a concatenation of the state of the ego vehicle and the states of all vehicles within a certain radius. If a neighboring vehicle is a CAV, full state information is communicated, including position, velocity, and lane index. For HDVs, the CAV can only perceive partial information, limited to the position \((x, y)\) and velocity \(v\). Therefore, \(\Omega^i\) is defined as:

\[
\Omega^i = \{ x, y, v, \mathbf{N}_{\text{CAV}}, \mathbf{N}_{\text{HDV}} \},
\]

where \(\mathbf{N}_{\text{CAV}}\) denotes the full state information of neighboring CAVs, including their position \((x, y)\), velocity \(v\), and \(\mathbf{N}_{\text{HDV}}\) represents the partial state information of neighboring HDVs, limited to their position \((x, y)\) and velocity \(v\).

This design aligns with the advanced communication and perception capabilities of CAVs, positioning them as the proactive agents in traffic control. By leveraging their ability to exchange comprehensive state information with other CAVs and perceive partial data from HDVs, CAVs can make informed, real-time decisions that optimize traffic flow and ensure smoother interactions with CAVs and HDVs. 
\subsubsection{Human-Driven Vehicles (HDVs)}
The POMDP for the \(i\)-th HDV agent is defined as:

\begin{equation}
\text{POMDP}^i = \left( S^i, A^i, T^i, R^i, \Omega^i, O^i, \gamma \right)
\end{equation}

\paragraph{State Space}
The state space for the \(i\)-th HDV is given by:

\begin{equation}
S^i = \{x, y, v, b_{\text{alert}}\}
\end{equation}

In this formulation, \(x\) denotes the longitudinal position of the HDV, while \(y\) refers to the lateral position, represented as an integer indicating the lane index. The variable \(v\) represents the velocity of the HDV. The boolean variable \(b_{\text{alert}}\) indicates whether the HDV has been alerted to the EMV’s presence, either through visual or auditory cues such as sirens or traffic signals.

\paragraph{Action Space}
The action space for the \(i\)-th HDV is simplified to account for basic human driving maneuvers:

\begin{equation}
A^{i} = A_{\text{yield}} \times A_{\text{lat}}
\end{equation}

The longitudinal action space, \(A_{\text{yield}}\), is binary, indicating whether the HDV decides to yield (1) or not (0) in response to an EMV alert. The lateral action space, \(A_{\text{lat}}\), governs lane-changing behavior. Similar to CAVs, \(A_{\text{lat}} = 1\) represents a lane change to the right, \(A_{\text{lat}} = -1\) signifies a lane change to the left, and \(A_{\text{lat}} = 0\) indicates no lane change. Under normal driving conditions, HDVs do not yield. However, when hearing sirens or observing visual cues from an approaching EMV, the HDVs' actions are locked into \(A_{\text{yield}} = 1\), prompting them to find the next available free space to change lanes, pull over, or stop. These simplified actions reflect the limited complexity of human drivers' decision-making when responding to EMVs. It is anticipated that CAVs, being more proactive agents, can learn the yielding patterns of HDVs and adjust accordingly.

\paragraph{Transition Function}
The transition function \(T^i\) for the \(i\)-th HDV models how the environment evolves based on the HDV’s actions. Given the current state \(S^i\) and action \(A^i\), \(T^i\) describes the transition to the next state, accounting for changes in the HDV’s position, speed, and interactions with other vehicles. Since HDVs rely on limited sensory information, external factors such as CAVs' maneuver and EMV behavior have significant influence on these transitions. Mathematically, this can be expressed as:
\[
T^i_{\text{HDV}}(S^i_t, A^i_t) \sim P(S^i_{t+1} \mid S^i_t, A^i_t),
\]

\paragraph{Observation Space and Function}
HDVs operate with a simplified observation space, limited to the ego vehicle's state and nearby vehicle positions and velocities. Lacking communication capabilities, HDVs rely solely on local perceptions from onboard sensors, resulting in restricted and naive decision-making. This limitation underscores the critical role of CAVs in compensating for HDVs' reduced situational awareness and reactivity.

Despite their mostly passive behavior, HDVs are still modeled as agents for several reasons. First, their actions, such as deciding to yield or not, directly influence the efficiency of DQJL formation and the passage time of the EMV. Modeling HDVs as agents allows the framework to incorporate the stochastic and reactive nature of human drivers, which is critical for robust policy learning by CAVs. Second, this design enables the simulation of realistic interactions between CAVs and HDVs, allowing CAVs to anticipate HDV behavior and adapt accordingly. Finally, HDVs contribute to the learning process by generating reward signals (see Subsubsec.~\ref{subsubsec:reward_design}), ensuring that the policies learned by CAVs account for both cooperative and uncooperative HDV behaviors. By treating HDVs as agents, the framework effectively captures the dynamics of mixed traffic environments and ensures scalability under varying levels of CAV penetration.


\subsubsection{Reward Design}\label{subsubsec:reward_design}

The reward structure is designed to encourage both global cooperation and individual efficiency. For each agent \(i\), the reward function combines a shared global reward and an agent-specific local reward:

\begin{equation}\label{eqn:global_reward}
R_{\text{global}} = -\Biggl(
\underbrace{\beta_1 \cdot t_{\text{EMV}}}_{\textcolor{BrickRed}{\text{Time Penalty}}}
+ \underbrace{\beta_2 \cdot n_{\text{EMV}}^{\text{lane change}}}_{\textcolor{RoyalBlue}{\text{Lane Change Penalty}}} 
+ \underbrace{\beta_3 \cdot \left(1 - \frac{d_{\text{clear}}}{L}\right)}_{\textcolor{ForestGreen}{\text{DQJL Completeness Penalty}}}
\Biggr)
\end{equation}

\begin{equation}\label{eqn:return}
R^i = \alpha R_{\text{global}} - (1 - \alpha) \cdot R_{\text{local}}^i
\end{equation}

The global reward \(R_{\text{global}}\) is shared among all agents and reflects system-wide objectives: minimizing EMV passage time, reducing EMV lane changes, and ensuring complete DQJL formation. This shared reward promotes coordination between agents toward common goals.

The local reward \(R_{\text{local}}^i\) varies by agent type:

1. **For CAVs**:
   \[
R_{\text{local}}^{\text{CAV}} = 
\underbrace{-\eta_1 \cdot n_{\text{CAV}}^{\text{lane change}}}_{\textcolor{RoyalBlue}{\text{Local: CAV Lane Change Penalty}}}
\;+\;
\underbrace{-\eta_2 \cdot |a_{\text{long}}|}_{\textcolor{Sepia}{\text{Local: CAV Acceleration Penalty}}}
\]
   The local reward for CAVs penalizes excessive lane changes and sudden acceleration or deceleration, encouraging smoother driving behavior.

2. **For HDVs**:
\[
R_{\text{local}}^{\text{HDV}} = 
\underbrace{- \eta_3 \cdot n_{\text{HDV}}^{\text{lane change}}}_{\textcolor{RoyalBlue}{\text{Local: HDV Lane Change Penalty}}}
\]
   For HDVs (passive agents), the local reward simply tracks lane-changing penalties.

The parameter \(\alpha \in [0,1]\) balances the trade-off between global coordination and local efficiency. A higher \(\alpha\) emphasizes system-wide performance, while a lower \(\alpha\) prioritizes individual agent behavior.


\section{Multi-Agent Learning for DQJLs}
\label{sec:learning}
To address the challenge of coordinating multiple agents in forming DQJLs, we employ the Multi-Agent Proximal Policy Optimization (MAPPO) algorithm. This section details MAPPO's adaptation for the DQJL task, starting with preliminaries in Subsec.~\ref{subsec:preliminaries}, an explanation of MAPPO's alignment with DQJL in Subsec.~\ref{subsec:MAPPO_DQJL}, the MAPPO-DQJL algorithm in Algorithm~\ref{algo:mappo_dqjl}, and network architectures in Subsec.~\ref{subsec:network_architecture}.

\subsection{Preliminaries}
\label{subsec:preliminaries}
MAPPO is an on-policy reinforcement learning algorithm designed to optimize individual agent policies while accounting for the collective behavior of all agents. Schulman et al.~\cite{schulman2017proximal} introduced Proximal Policy Optimization (PPO), which has since become a widely adopted algorithm for model-free reinforcement learning due to its balance of stability and performance. Yu et al.~\cite{yu2022surprising} highlight the exceptional performance of multi-agent reinforcement learning in collaborative settings, further emphasizing its potential for diverse applications.

In the context of DQJL formation, this approach enables agents to quickly form DQJLs by learning cooperative strategies. MAPPO operates under centralized training and decentralized execution, making it ideal for this scenario. During centralized training, agents share information and are trained with a global perspective of the environment. This allows the system to leverage full-state information, ensuring that the policies learned reflect the global objective of minimizing EMV passage time while maintaining traffic flow. 

In contrast, decentralized execution allows agents to make decisions based on local observations. This ensures scalability and real-world applicability, as agents act independently, responding to their immediate environment in real-time. This autonomy is crucial in mixed autonomy settings where communication might be limited, allowing agents to function effectively despite partial observability. The decentralized execution phase ensures robust behavior in dynamic traffic conditions, while the centralized training phase guarantees that these local actions contribute to the broader goal of DQJL formation.

A key feature of MAPPO is its centralized value function estimation, where a centralized critic evaluates each agent’s actions based on the global state of the system. This helps agents understand how their actions contribute to the collective objective. Policy updates using the PPO objective ensure a balance between exploration and exploitation, leading to stable and efficient learning. The reward structure encourages both individual and collective coordination, optimizing DQJL formation and minimizing disruptions to surrounding traffic, thus ensuring the safe and swift passage of EMVs.

\subsection{MAPPO for DQJL Formation}
\label{subsec:MAPPO_DQJL}
In our approach to DQJL formation, we adapt MAPPO with parameter sharing for CAVs, designing policy and value networks specifically tailored to the dynamic, interactive nature of traffic systems. All CAV agents utilize a common policy network \(\pi_{\theta_{\text{CAV}}}(a_i \mid o_i)\), where \(\theta_{\text{CAV}}\) denotes the shared policy parameters, ensuring that these agents learn a unified, generalizable strategy based on their local observations. HDVs are modeled as passive agents with fixed policy networks \(\pi_{\theta_i}(a_i \mid o_i)\), enabling a unified agent-based representation of the traffic system while maintaining realistic human driving patterns. This design choice not only simplifies state representation but also provides the flexibility to incorporate different HDV behavioral models within the same framework. Our experimental results demonstrate that treating HDVs as agents rather than environmental elements leads to superior performance in mixed-autonomy scenarios.

To foster coordination across all vehicles, we employ a centralized value network \(V_{\phi}(s)\) that evaluates the global state \(s\), which includes information from both active (CAVs) and passive (HDVs) agents. This unified value function, parameterized by \(\phi\), enables consistent evaluation of vehicle interactions regardless of autonomy level, providing a comprehensive assessment of the traffic situation. The centralized evaluation mechanism guides CAVs toward cooperative behaviors while realistically accounting for HDV responses, ultimately supporting efficient DQJL formation.

The reward function \(R_i\) for each CAV reconciles individual-level goals—such as safety and maneuver efficiency—with the broader aim of expediting EMV passage. By balancing local incentives with global objectives, this hierarchical reward structure motivates agents to coordinate their actions in a manner that promotes improved overall traffic flow and reduced disruptions.

To ensure stable learning and robust policy improvement, we employ the clipped PPO objective. This training paradigm maintains a careful equilibrium between exploration and exploitation, controlling volatility in performance as the CAVs refine their shared policy. By combining parameter-sharing among CAVs, a unified agent-based representation for both CAVs and HDVs, and a carefully crafted reward structure, our MAPPO-based framework facilitates the rapid, secure, and cooperative implementation of DQJLs.

\subsubsection{MAPPO-DQJL Algorithm}
We present the pseudocode for our MAPPO-DQJL algorithm in Algorithm~\ref{algo:mappo_dqjl}, followed by a detailed walk-through of its key components.
\begin{algorithm}[htbp]
\caption{MAPPO-DQJL Training with Parameter Sharing}
\label{algo:mappo_dqjl}
\SetKwInOut{Input}{Input}
\SetKwInOut{Output}{Output}

\Input{
$T$: maximum time step of an episode \\
$N_{\text{bs}}$: batch size \\
$\eta_{\theta}$: learning rate for policy networks \\
$\eta_{\phi}$: learning rate for value networks
}

\Output{
$\phi$: learned parameters for the shared value network \\
$\theta_{\text{CAV}}$: learned parameters for the shared CAV policy network
}

\BlankLine
Initialize $\phi, \theta_{\text{CAV}}, k \gets 0, B \gets \varnothing$ \;
Initialize simulation environment (SUMO), $t \gets 0$, get $\{s_t^i\}_{i \in \mathcal{N}}$ \;

\Repeat{\textnormal{Convergence}}{
    \tcc{Generate trajectories}
    \ForEach{CAV $i \in \mathcal{N}_{\text{CAV}}$ \textnormal{(in parallel)}}{
        Sample $a_t^i$ from $\pi_{\theta_{\text{CAV}}}$ \;
        Execute fixed policy for HDVs $j \in \mathcal{N}_{\text{HDV}}$ \;
        Receive $r_t^i$ and $s_{t+1}^i$ \;
        $B \gets B \cup \{(s_t^i, a_t^i, s_{t+1}^i, r_t^i)\}$ \;
    }
    
    $t \gets t + 1, k \gets k + 1$ \;

    \If{$t == T$}{
        Reset simulation, $t \gets 0$, get $\{s_0^i\}_{i \in \mathcal{N}}$ \;
    }

    \If{$k == N_{\text{bs}}$}{
        \tcc{Update using shared parameters for CAVs}
        Compute advantages $\{\hat{A}_t^i\}_{i \in \mathcal{N}_{\text{CAV}}}$ using GAE \;
        Compute returns $\{\hat{R}_t^i\}_{i \in \mathcal{N}_{\text{CAV}}}$ \;
        
        Aggregate gradients over CAVs in $\mathcal{N}_{\text{CAV}}$ \;
        $\phi \gets \phi - \eta_\phi \nabla_{\phi} L_V(\phi)$ \;
        $\theta_{\text{CAV}} \gets \theta_{\text{CAV}} + \eta_\theta \nabla_{\theta_{\text{CAV}}} L^{\text{CLIP}}(\theta_{\text{CAV}})$ \;
        
        $k \gets 0$, $B \gets \varnothing$ \;
    }
}
\end{algorithm}

\paragraph{Centralized Training with Decentralized Execution}
The algorithm adopts the centralized training and decentralized execution (CTDE) paradigm, which is well-suited for the DQJL task. During decentralized execution, each CAV $i$ observes its local state $o_{i,t}$, including neighboring vehicles, and independently selects actions $a_{i,t}$ based on its shared policy $\pi_{\theta_{\text{CAV}}}$. This decentralized approach enables real-time autonomy, scalability, and robust responses to dynamic traffic conditions without requiring communication between agents during deployment.

During centralized training, given our reward design that incorporates both shared global objectives and agent-specific local rewards, a critic evaluates the global state $s_t$ through a value function:

\begin{equation}
V_\phi(s_t) = \mathbb{E}_{\pi} \left[ \sum_{k=0}^\infty \gamma^k \left(R_{\text{global}}(s_{t+k}, \mathbf{a}_{t+k}) + \sum_{i \in \mathcal{N}_{\text{non-EMV}}} R^i_{\text{local}}(s^i_{t+k}, a^i_{t+k})\right) \mid s_t \right]
\end{equation}

where $R_{\text{global}}$ represents the shared system-wide reward focusing on EMV-related objectives, and $R^i_{\text{local}}$ captures the local efficiency of each CAV $i$.

This centralized value function provides a comprehensive assessment that:
1. Tracks global DQJL formation progress through $R_{\text{global}}$
2. Aggregates individual CAV contributions through $\sum_{i \in \mathcal{N}_{\text{CAV}}} R^i_{\text{local}}$
3. Accounts for HDV interactions implicitly through state transitions

The value network $V_{\phi}$ is updated by minimizing the squared error between this estimated value and the observed returns:
\begin{equation}
    L_V(\phi) = \mathbb{E}_{\mathcal{B}} \left[ \left( V_{\phi}(s_t) - \hat{R}_t \right)^2 \right]
\end{equation}

where $\hat{R}_t$ represents the actual discounted sum of rewards observed during rollouts. This CTDE framework enables agents to learn coordinated behaviors that optimize both system-wide efficiency and individual performance while maintaining scalability in real-world deployments.

\paragraph{Generalized Advantage Estimation (GAE)}
In the MAPPO framework, the update of policy networks relies on accurate estimation of the advantage function $\hat{A}_t$, which measures how much better or worse an action $a_{i,t}$ is relative to the current policy. In our context, this helps the CAVs learn how their actions (e.g., lane changes, acceleration) contribute to efficient DQJL formation. To compute the advantage, we use Generalized Advantage Estimation (GAE), which reduces variance in the policy gradient estimates while maintaining manageable bias.

The advantage function at time $t$ is given by:
\begin{equation}\label{eqn:gae}
\hat{A}_t = \sum_{k=0}^{\infty} (\gamma \lambda)^k \delta_{t+k}
\end{equation}


where $\delta_t = r_t + \gamma V_{\phi}(s_{t+1}) - V_{\phi}(s_t)$ represents the temporal difference (TD) error, and $r_t$ is the reward received at time $t$. The parameters $\gamma$ and $\lambda$ control the trade-off between emphasizing immediate and future rewards, which in this context ensures that agents focus not only on short-term lane clearing but also long-term efficiency in EMV passage.


\paragraph{Policy Network Update and Parameter Sharing for CAVs}
Our framework employs PPO with a clipped objective to refine the policy parameters shared among all CAV agents, while HDVs maintain fixed policies that reflect realistic driving behaviors. By consolidating the learning process into a single, unified policy $\pi_{\theta_{\text{CAV}}}$ for all CAVs, we enable these vehicles to collectively learn and improve their decision-making behaviors from one another's experiences. This parameter sharing is particularly advantageous in DQJL formation, where CAVs must jointly coordinate their maneuvers while accounting for HDV responses to ensure effective lane-clearing and prompt EMV passage.

The PPO objective with the CLIP function is given by:
\begin{equation}\label{eqn:ppo_objectives}
    L^{\text{CLIP}}(\theta) = \mathbb{E}_{\mathcal{B}} \Bigl[ \min \bigl( r_t(\theta) \hat{A}_t,\; \text{clip}\bigl( r_t(\theta), 1 - \epsilon, 1 + \epsilon \bigr) \hat{A}_t \bigr) \Bigr],
\end{equation}
where
\[
r_t(\theta) = \frac{\pi_{\theta}(a_t \mid o_t)}{\pi_{\theta_{\text{old}}}(a_t \mid o_t)}
\]
represents the probability ratio between the updated and previous shared policies. By constraining \(r_t(\theta)\) to \([1-\epsilon, 1+\epsilon]\), the policy avoids large, destabilizing jumps. This controlled adaptation is critical in a DQJL context, where sudden, drastic policy shifts might lead to erratic maneuvers and reduced cooperation among CAVs.

The policy update is performed via gradient ascent:
\[
\theta \gets \theta + \alpha_{\pi} \nabla_{\theta} L^{\text{CLIP}}(\theta),
\]
with \(\alpha_{\pi}\) as the learning rate. Over successive updates, all CAVs collectively converge toward more reliable and efficient DQJL formation strategies, as knowledge gained from one scenario disseminates through the shared parameters to benefit the entire CAV fleet.

% \paragraph{Value Network Update}
% The value network $V_{\phi}$ is updated by minimizing the squared error between the estimated value $V_{\phi}(s_t)$ and the observed return $\hat{R}_t$. This centralized value function evaluates the global state incorporating both active CAVs and passive HDVs, where HDVs execute predetermined fixed policies. This unified approach enables comprehensive assessment of the traffic state, accounting for both learnable CAV actions and predictable HDV behaviors in the context of DQJL formation.

% The value network objective is defined as:
% \begin{equation}\label{eqn:clip}
%     L_V(\phi) = \mathbb{E}_{\mathcal{B}} \left[ \left( V_{\phi}(s_t) - \hat{R}_t \right)^2 \right],
% \end{equation}
% where $\hat{R}_t = \sum_{k=0}^{T} \gamma^k r_{t+k}$ denotes the discounted cumulative reward. The shared value network leverages experiences from CAVs while accounting for fixed HDV behaviors, guiding the optimization of CAV policies toward efficient DQJL formation.
\subsection{Transformer-based Network Architecture}
\label{subsec:network_architecture}
This study implements a transformer-based architecture for both policy and value networks in our MAPPO-DQJL algorithm. The transformer architecture, introduced by Vaswani et al.~\cite{vaswani2017attention}, is selected for its proven capability in modeling complex dependencies through self-attention mechanisms. In the context of DQJL formation, where vehicles must coordinate across varying distances and time horizons, the transformer's ability to capture long-range interactions and process information in parallel makes it particularly suitable for real-time decision-making in dense traffic conditions~\cite{parisotto2020stabilizing,chen2021decision}.

\subsubsection{Policy Network}
Under our parameter-sharing framework, all CAV agents utilize a single shared policy network \(\pi_{\theta}\). The network processes each CAV's local observation \(o_i\) through the following sequence:
\begin{enumerate}
    \item Embedding layer transforms observations into a higher-dimensional feature space
    \item Positional encodings incorporate spatial and temporal information
    \item \(N\) transformer encoder layers process the embedded input using self-attention
    \item Policy head (fully connected layer) computes action distribution parameters
\end{enumerate}

Formally, for any CAV agent \(i\):
\[
\pi_{\theta}(a_i \mid o_i) = \text{PolicyHead}\Bigl( \text{TransformerEncoder}\bigl( \text{PositionalEncoding}( \text{Embedding}(o_i) ) \bigr) \Bigr),
\]
where \(\pi_{\theta}(a_i \mid o_i)\) represents the probability distribution over actions \(a_i\), conditioned on \(o_i\). The parameter sharing enables efficient knowledge transfer across the CAV fleet. See Subsections~\ref{subsec:appendix_policy_network} for implementation details.

\subsubsection{Value Network}
The centralized value network \(V_{\phi}\) evaluates the global state \(s\) through a similar transformer-based architecture:
\[
V_{\phi}(s) = \text{ValueHead}\bigl( \text{TransformerEncoder}( \text{Embedding}(s) ) \bigr).
\]
This shared value network provides a unified assessment of how joint actions impact the global traffic state, guiding CAVs toward optimal DQJL formation strategies. See Subsections~\ref{subsec:appendix_value_network} for implementation details.

\subsubsection{Transformer Encoder Layer}
Each transformer encoder layer maintains the standard architecture: multi-head attention followed by a position-wise feed-forward network, with layer normalization and residual connections after each sub-layer. Detailed implementation specifications and parameter estimations are provided in Subsections~\ref{subsec:appendix_tf_encoder_layer} and~\ref{subsec:number_of_parameters}, respectively.

\section{Experimental Setup}
\label{sec:experimental_setup}
In this section, we delineate the methodology used to evaluate the performance of MAPPO-DQJL. Subsection~\ref{subsec:objectives} specifies the central research objectives that guide our experimental investigations. Subsection~\ref{subsec:metrics} details the evaluation metrics employed to gauge the framework’s effectiveness. 
Subsection~\ref{subsec:simulation} describes the simulation environment utilized in these experiments. Finally, Subsection~\ref{subsec:benchmarks} introduces the benchmarks that serve as comparative baselines for assessing MAPPO-DQJL. 

\subsection{Experimental Objectives}
\label{subsec:objectives}

The experiments are designed to address the following key research questions:

\begin{itemize}
    \item \textbf{Effectiveness of MAPPO-DQJL:} To what extent does our MARL-based approach for DQJL formation outperform established benchmarks with respect to EMV passage time and holistic traffic flow optimization?
    \item \textbf{Impact of Penetration Rates:} How does varying CAV penetration rates influence the performance of the proposed framework, particularly regarding DQJL coordination and operational efficiency?
    \item \textbf{Impact of Traffic Density:} Similarly, how does varying traffic density influence the performance of the proposed framework?
\end{itemize}

These focal points collectively shape the principal objectives of our study. By rigorously examining these questions, we aim to elucidate the complex interplay between DQJL formation strategies, CAV adoption levels, and performance trade-offs, ultimately informing near-term policy decisions and strategic deployment of intelligent traffic management solutions.

\subsection{Evaluation Metrics}
\label{subsec:metrics}

Although a DQJL is typically formed prior to EMV ingress into a given road segment, the underlying agent behaviors remain consistent throughout the EMV’s traversal. Consequently, the temporal duration required to establish the DQJL is effectively captured by the EMV’s passage time. We employ the following evaluation metrics:

\begin{itemize}
    \item \textbf{EMV Passage Time ($t_{\text{EMV}}$):} This metric represents the duration required for the EMV to traverse the designated road segment once the DQJL is in place, emphasizing the primary objective of expediting emergency response.

    \item \textbf{Number of Lane-changing for EMVs ($N_{\text{EMV}}^{\text{LC}}$) and Non-EMVs ($N_{\text{non-EMV}}^{\text{LC}}$):} We also measure the lane-changing efforts of both EMVs and non-EMVs, including CAVs and HDVs. Slight increments in EMV passage time may substantially diminish the maneuvering burden imposed on non-EMVs. Our analysis focuses on carefully balancing these trade-offs to minimize overall disruption while maintaining efficient EMV transit.
\end{itemize}
\subsection{Simulation Environment}
\label{subsec:simulation}
We employ Simulation of Urban Mobility (SUMO) as our simulation environment. SUMO's ability to realistically model fine-grained interactions in mixed traffic scenarios, coupled with its native support for V2X communication and precise vehicle dynamics, provides a high-fidelity platform for examining DQJL formation and its implications on traffic flow. The simulation is conducted on a representative roadway environment, reflecting a typical urban street layout.

To ensure simulation reliability, we conducted a comprehensive calibration and validation process using empirical traffic flow data from the NYC Mobility Report~\cite{nycdot2019mobility}. The calibration focused on key parameters including car-following behavior, lane-changing dynamics, and intersection delays, leveraging Manhattan's peak hour metrics such as average vehicle speeds and traffic volumes. Our model validation against these real-world observations achieved a goodness-of-fit of over 85\% for key metrics including average travel times, queue lengths, and throughput. These calibrated parameters were then applied consistently across all experimental scenarios to maintain reliability in our comparative analyses. For vehicle behavior modeling, we utilized SUMO's built-in car-following and lane-changing models (detailed in Subsec.~\ref{subsec:appendix_car_following} and Subsec.~\ref{subsec:appendix_lane_changing}), adapting them to implement fixed policies for HDVs. Rather than treating HDVs as purely rule-based environmental elements, we represented them as passive agents, ensuring consistent representation of all vehicle types within our MAPPO framework while preserving SUMO's realistic vehicle dynamics.

To ensure a focused investigation of the interplay between CAVs and HDVs, we assume homogeneous configurations for each class of vehicle. Specifically, all CAVs share a uniform set of learning parameters, while all HDVs share a common fixed policy derived from realistic driving behaviors.

\paragraph{Road Configurations} We adopt three distinct road configurations in SUMO: Two-Lane, Three-Lane, and Four-Lane roadways, each spanning a length of 1500 meters. Traffic flow is varied across different simulation settings, ensuring a realistic depiction of dynamic traffic conditions. The EMV on duty enters the roadway already populated with vehicles on any lane randomly, simulating real-world congestion scenarios. Vehicles continuously enter and leave the roadway as the EMV traverses, and the observation window concludes once the EMV has exited the roadway.

\subsection{Benchmarks}
\label{subsec:benchmarks}
Although DQJL is a novel concept specifically designed to facilitate EMV passage through congested urban roads, there are other frameworks aimed at improving EMV intra-link movements in similar traffic conditions. Two control frameworks that address tasks closely related to DQJL in heterogeneous traffic environments are as follows:

\begin{enumerate}
    \item \textbf{No Control}: This scenario represents a baseline condition where no specialized control strategy is employed. Non-EMVs adhere strictly to their default car-following and lane-changing behaviors in SUMO, reacting only to immediate traffic conditions without any global coordination. Although vehicles may occasionally yield when the EMV is in close proximity, these responses are purely local and uncoordinated, often resulting in suboptimal clearance times for the EMV.

    \item \textbf{PPO-based Corridor Clearance (PPO-CC)}: Suo et al.~\cite{suo2024model} introduced a single-agent PPO-based control framework where a CAV acts as a "splitting point" agent to assist EMVs in clearing corridors. The CAV dynamically adjusts its speed and positioning based on local traffic conditions, creating a corridor for the EMV to pass through with minimal disruption to other vehicles. 

    \item \textbf{MAPPO-DQJL-CAV}: A reduced version of our proposed framework that considers only CAVs as agents, treating HDVs as part of the environment. The agent design for CAVs remains consistent with MAPPO-DQJL. This benchmark evaluates whether the inclusion of HDVs as agents significantly impacts performance.
\end{enumerate} 


\section{Results and Discussion}
\label{sec:discussion}
In this section, we present the results from the simulated experiments. The effects of penetration rate, traffic density, and the adoption of contra-flow lanes on MAPPO-DQJL performance are detailed in Subsec.~\ref{subsec:sensitivity}. Additionally, the contributions of different components within MAPPO-DQJL are analyzed in Subsec.~\ref{subsec:ablations}. Environment configurations, MDP parameters, and training hyper-parameters selected to produce the following results are listed in Table~\ref{tab:parameters_summary}.


\subsection{Sensitivity Studies}\label{subsec:sensitivity}
Many factors influence the effectiveness of candidate lane-clearing schemes. Among these, two primary determinants stand out: the penetration rate of CAVs and the prevailing traffic density. Variations in CAV penetration rates influence the degree of cooperative behaviors and the system’s capacity to execute sophisticated maneuvers, while shifts in traffic density alter the underlying operational conditions, potentially affecting both safety and efficiency outcomes. 

To ensure a fair comparison of performance among the candidate schemes, all other parameters remain consistent across different experimental conditions. Traffic flow is fixed as \textbf{15} veh/lane/min for penetration rate study and penetration rate is set as \textbf{50\%} for traffic density study. All statistics reported in the tables are based on a total of 20 runs per scenario.

\subsubsection{Penetration Rate}
\paragraph{Two-Lane Roadway} When the traffic density is set to 15 veh/lane/min, the EMV passage time, the number of EMV lane-change maneuvers, and the number of non-EMVs’ lane-change maneuvers are presented Table~\ref{tab:penetration_study_two_lane}.

\begin{table}[htbp]
\centering
{\fontsize{9}{11}\selectfont
\begin{tabular}{lccccc}
\toprule
\textbf{Method} & \textbf{0\%} & \textbf{25\%} & \textbf{50\%} & \textbf{75\%} & \textbf{100\%} \\
\midrule
\textbf{\textit{$t_{\text{EMV}} (s)$}} & & & & & \\ 
\multirow{1}{*}{No Control} & \multicolumn{5}{c}{191.2 $\pm$ 9.7} \\[6pt]
PPO-CC~\cite{suo2024model} 
& 190.3 $\pm$ 9.7 
& 183.5 $\pm$ 10.3
& 171.5 $\pm$ 10.5 
& 152.2 $\pm$ 8.9 
& 145.6 $\pm$ 7.3 \\[6pt]
MAPPO-DQJL-CAV 
& 189.8 $\pm$ 9.9 
& 175.0 $\pm$ 10.0
& 162.3 $\pm$ 10.1 
& 145.7 $\pm$ 7.9 
& 143.0 $\pm$ 6.5 \\[6pt]
MAPPO-DQJL 
& \textbf{189.6} $\pm$ 9.5 
& \textbf{170.8} $\pm$ 10.8
& \textbf{150.9} $\pm$ 11.5 
& \textbf{138.4} $\pm$ 8.8 
& \textbf{133.8} $\pm$ 7.6 \\
\midrule
\textbf{\textit{\boldmath$N_{\text{EMV}}^{\text{LC}}$}} & & & & & \\ 
\multirow{1}{*}{No Control} & \multicolumn{5}{c}{4.0 $\pm$ 0.3} \\[6pt]
PPO-CC~\cite{suo2024model} 
& \textbf{1.0} $\pm$ 0.0 & \textbf{1.0} $\pm$ 0.0 & 1.0 $\pm$ 0.0 & 1.0 $\pm$ 0.0 & 1.0 $\pm$ 0.0 \\[6pt]
MAPPO-DQJL-CAV 
& 3.9 $\pm$ 0.4 & 2.8 $\pm$ 0.4 & 1.6 $\pm$ 0.3 & 1.3 $\pm$ 0.2 & 1.1 $\pm$ 0.2 \\[6pt]
MAPPO-DQJL 
& 4.0 $\pm$ 0.3 & 1.8 $\pm$ 0.3 & \textbf{0.9} $\pm$ 0.2 & \textbf{0.7} $\pm$ 0.1 & \textbf{0.6} $\pm$ 0.1 \\
\midrule
\textbf{\textit{\boldmath$N_{\text{non-EMV}}^{\text{LC}}$}} & & & & & \\ 
\multirow{1}{*}{No Control} & \multicolumn{5}{c}{20.3 $\pm$ 2.7} \\[6pt]
PPO-CC~\cite{suo2024model} 
& \textbf{19.3} $\pm$ 2.0 & 17.0 $\pm$ 1.8 & 14.7 $\pm$ 1.5 & 12.1 $\pm$ 1.2 & 9.5 $\pm$ 1.0 \\[6pt]
MAPPO-DQJL-CAV 
& 19.5 $\pm$ 2.0 & 18.0 $\pm$ 1.8 & 16.0 $\pm$ 1.6 & 13.0 $\pm$ 1.3 & 11.0 $\pm$ 1.1 \\[6pt]
MAPPO-DQJL 
& 20.5 $\pm$ 1.8 & \textbf{16.9} $\pm$ 1.7 & \textbf{14.0} $\pm$ 1.4 & \textbf{10.7} $\pm$ 1.2 & \textbf{8.3} $\pm$ 0.9 \\
\bottomrule
\end{tabular}
}
\caption{Performance for all methods on evaluation metrics in the Two-Lane Roadway across different penetration rates. Bold values indicate the best performance.}
\label{tab:penetration_study_two_lane}
\end{table}
The results presented in Table~\ref{tab:penetration_study_two_lane} demonstrate that the proposed MAPPO-DQJL framework outperforms all other methods across all non-zero penetration rates with respective to all metrics. Although MAPPO-DQJL-CAV also leverages multi-agent coordination, its reliance on treating HDVs as part of the environment rather than as learning agents leads to noticeably longer EMV passage times compared to MAPPO-DQJL. This suggests that fully integrating HDVs into the learning process is crucial for achieving more efficient cooperation and faster EMV clearance.

As the CAV penetration rate increases, the passage time for EMVs under MAPPO-DQJL ultimately reduces by approximately 30\% relative to the No Control scenario, underscoring the substantial benefits gained from widespread CAV adoption. When comparing MAPPO-DQJL to PPO-CC, the advantage is less pronounced—on the order of 8\% at intermediate penetration levels—yet still meaningful. These findings highlight the importance of fully integrated, multi-agent cooperation among CAVs and HDVs to attain near-optimal traffic flow conditions and expedite EMV passage.

A notable observation is that the PPO-CC approach consistently reports a mean of $1.0 \pm 0.0$ lane changes. This behavior arises from its design: PPO-CC designates a CAV as a stationary “splitting point,” ensuring that the EMV must execute exactly one lane-changing maneuver to travel through the cleared corridor. In contrast, as the penetration rate reaches the 50\% mark, MAPPO-DQJL achieves an average lane-change count below 1.0, indicating that for than half times, the EMV can traverse through the corridor without changing lane. This improvement becomes even more pronounced at higher penetration rates, where MAPPO-DQJL consistently outperforms all other methods, demonstrating its efficiency in coordinating maneuvers and reducing unnecessary maneuvers.

On the other hand, the MAPPO-DQJL-CAV variant, which treats HDVs as part of the environment rather than as active learning agents, cannot achieve this level of efficiency. Consequently, the EMV under MAPPO-DQJL-CAV often performs more than one lane-change on average, reflecting a non-ideal scenario where less integrated coordination with HDVs leads to suboptimal maneuvering behaviors.

The results also indicate that the proposed MAPPO-DQJL method outperforms the other approaches across all non-zero CAV penetration rates. In particular, MAPPO-DQJL-CAV, which does not treat HDVs as learning agents, suffers from a lack of coordination between CAVs and HDVs. This oversight allows PPO-CC, despite its simpler structure, to achieve fewer lane-change maneuvers in certain intermediate scenarios than MAPPO-DQJL-CAV. Nonetheless, at 100\% CAV penetration, MAPPO-DQJL not only reduces non-EMV lane-change maneuvers by roughly 60\% compared to the No Control baseline, but also maintains about a 10\% margin of improvement over PPO-CC. These findings underscore the value of fully integrated cooperation among CAVs and HDVs, enabling more substantial efficiency gains at higher penetration levels.

Fig.~\ref{fig:training} illustrates the training patterns of MAPPO-DQJL, MAPPO-DQJL-CAV, and PPO-CC across episodes for the Two-Lane Roadway with 15 veh/lane/min under 50\% penetration rate. As shown in Fig.~\ref{fig:emv_travel_time}, PPO-CC converges around 3000 episodes, but $t_{\text{EMV}}$ fluctuates around 170s at convergence. MAPPO-DQJL converges near 7500 episodes, achieving the lowest $t_{\text{EMV}}$ overall. MAPPO-DQJL-CAV demonstrates a similar learning pattern but stabilizes at a higher $t_{\text{EMV}}$ of around 162s. Fig.~\ref{fig:reward_per_episode} reflects this trend, also highlighting greater fluctuations in MAPPO-DQJL-CAV compared to MAPPO-DQJL.
\begin{figure}[hbtp]
    \centering
    \begin{subfigure}[b]{0.48\linewidth}
        \centering
        \includegraphics[width=\linewidth]{Figures/emv_travel_time_Two_Lane.png}
        \caption{EMV travel time.}
        \label{fig:emv_travel_time}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.48\linewidth}
        \centering
        \includegraphics[width=\linewidth]{Figures/reward_Two_Lane.png}
        \caption{Reward.}
        \label{fig:reward_per_episode}
    \end{subfigure}
    \caption{Comparison of EMV travel time (a) and reward (b) per episode achieved by learning-based methods.}
    \label{fig:training}
\end{figure}

\paragraph{Three-Lane Roadway} 
Table~\ref{tab:penetration_rate_three_lane} shows the performance achieved by all methods on the Three-Lane Roadway. The outcomes mirror the trends observed in the two-lane scenario, with MAPPO-DQJL consistently yielding superior performance as CAV penetration increases.
\begin{table}[htbp]
\centering
{\fontsize{9}{11}\selectfont
\begin{tabular}{lccccc}
\toprule
\textbf{Method} & \textbf{0\%} & \textbf{25\%} & \textbf{50\%} & \textbf{75\%} & \textbf{100\%} \\
\midrule
\textbf{\textit{$t_{\text{EMV}} (s)$}} & & & & & \\ 
\multirow{1}{*}{No Control} & \multicolumn{5}{c}{180.0 $\pm$ 9.2} \\[6pt]
PPO-CC~\cite{suo2024model} 
& 179.5 $\pm$ 9.5 & 165.3 $\pm$ 10.1 & 150.4 $\pm$ 10.3 & 138.7 $\pm$ 9.1 & 130.1 $\pm$ 8.5 \\[6pt]
MAPPO-DQJL-CAV 
& 178.8 $\pm$ 9.7 & 159.6 $\pm$ 10.5 & 141.8 $\pm$ 10.6 & 134.2 $\pm$ 9.0 & 127.5 $\pm$ 8.0 \\[6pt]
MAPPO-DQJL 
& \textbf{178.4} $\pm$ 9.3 & \textbf{156.4} $\pm$ 10.8 & \textbf{135.9} $\pm$ 10.9 & \textbf{127.8} $\pm$ 9.2 & \textbf{125.7} $\pm$ 8.7 \\
\midrule
\textbf{\textit{\boldmath$N_{\text{EMV}}^{\text{LC}}$}} & & & & & \\ 
\multirow{1}{*}{No Control} & \multicolumn{5}{c}{3.2 $\pm$ 0.4} \\[6pt]
PPO-CC~\cite{suo2024model} 
& \textbf{1.0} $\pm$ 0.0 & \textbf{1.0} $\pm$ 0.0 & 1.0 $\pm$ 0.0 & 1.0 $\pm$ 0.0 & 1.0 $\pm$ 0.0 \\[6pt]
MAPPO-DQJL-CAV 
& 2.9 $\pm$ 0.4 & 2.1 $\pm$ 0.3 & 1.5 $\pm$ 0.2 & 1.2 $\pm$ 0.2 & 1.1 $\pm$ 0.2 \\[6pt]
MAPPO-DQJL 
& 3.1 $\pm$ 0.3 & 1.7 $\pm$ 0.3 & \textbf{0.9} $\pm$ 0.2 & \textbf{0.5} $\pm$ 0.1 & \textbf{0.4} $\pm$ 0.1 \\
\midrule
\textbf{\textit{\boldmath$N_{\text{non-EMV}}^{\text{LC}}$}} & & & & & \\ 
\multirow{1}{*}{No Control} & \multicolumn{5}{c}{27.3 $\pm$ 2.1} \\[6pt]
PPO-CC~\cite{suo2024model} 
& \textbf{26.8} $\pm$ 2.2 & 21.6 $\pm$ 1.8 & 17.2 $\pm$ 1.4 & 14.6 $\pm$ 1.2 & 12.3 $\pm$ 1.0 \\[6pt]
MAPPO-DQJL-CAV 
& 27.4 $\pm$ 2.2 & 24.5 $\pm$ 1.9 & 19.3 $\pm$ 1.5 & 14.0 $\pm$ 1.2 & 11.5 $\pm$ 1.0 \\[6pt]
MAPPO-DQJL 
& 26.9 $\pm$ 1.9 & \textbf{22.0} $\pm$ 1.6 & \textbf{16.5} $\pm$ 1.4 & \textbf{12.3} $\pm$ 1.1 & \textbf{10.2} $\pm$ 0.9 \\
\bottomrule
\end{tabular}
}
\caption{Performance metrics on the Three-Lane Roadway across different penetration rates. Bold values indicate the best performance.}
\label{tab:penetration_rate_three_lane}
\end{table}
As shown in Table~\ref{tab:penetration_rate_three_lane}, MAPPO-DQJL yields lower EMV passage times than all other methods at every non-zero penetration level. Similar to the two-lane scenario, MAPPO-DQJL-CAV remains at a disadvantage due to its limited integration of HDVs. However, the introduction of a third lane provides additional flexibility for non-EMVs to yield, thereby creating more free space and further reducing EMV passage times relative to the two-lane case.

Similarly, the presence of an additional lane provides the EMV with greater lateral flexibility, reducing the number of lane changes required during its traversal. This increased spatial availability allows the EMV to complete its passage with fewer maneuvers, highlighting the benefits of enhanced clearance. However, this improvement comes at the cost of increased lane changes by non-EMVs, as they actively yield space for the EMV. This trend is particularly evident in the evaluation of $N_{\text{non-EMV}}^{\text{LC}}$, where the total number of non-EMV maneuvers increases to accommodate the EMV’s passage. 

Nevertheless, it is important to note that the rise in $N_{\text{non-EMV}}^{\text{LC}}$ is partially attributable to the increased number of non-EMVs present on the roadway. When normalized by the number of vehicles, $N_{\text{non-EMV}}^{\text{LC}}$ per vehicle is lower compared to other methods. This observation underscores the effectiveness of MAPPO-DQJL in minimizing additional disturbances.

\paragraph{Four-Lane Roadway} With the same traffic flow configurations as above, we present the performance metrics in Table~\ref{tab:penetration_study_four_lane}.
\begin{table}[htbp]
\centering
{\fontsize{9}{11}\selectfont
\begin{tabular}{lccccc}
\toprule
\textbf{Method} & \textbf{0\%} & \textbf{25\%} & \textbf{50\%} & \textbf{75\%} & \textbf{100\%} \\
\midrule
\textbf{\textit{$t_{\text{EMV}} (s)$}} & & & & & \\ 
\multirow{1}{*}{No Control} & \multicolumn{5}{c}{184.1 $\pm$ 9.9} \\[6pt]
PPO-CC~\cite{suo2024model}
& 179.0 $\pm$ 9.2 & 165.0 $\pm$ 9.8 & 150.5 $\pm$ 10.0 & 128.5 $\pm$ 8.8 & 120.0 $\pm$ 8.2 \\[6pt]
MAPPO-DQJL-CAV
& 178.8 $\pm$ 9.5 & 150.2 $\pm$ 10.2 & 130.8 $\pm$ 10.3 & 122.0 $\pm$ 9.0 & 115.0 $\pm$ 8.5 \\[6pt]
MAPPO-DQJL
& \textbf{178.5} $\pm$ 9.3 & \textbf{147.5} $\pm$ 10.5 & \textbf{125.0} $\pm$ 10.7 & \textbf{118.5} $\pm$ 8.8 & \textbf{113.0} $\pm$ 8.0 \\
\midrule
\textbf{\textit{\boldmath$N_{\text{EMV}}^{\text{LC}}$}} & & & & & \\ 
\multirow{1}{*}{No Control} & \multicolumn{5}{c}{2.5 $\pm$ 0.3} \\[6pt]
PPO-CC~\cite{suo2024model} 
& \textbf{1.0} $\pm$ 0.0 & \textbf{1.0} $\pm$ 0.0 & 1.0 $\pm$ 0.0 & 1.0 $\pm$ 0.0 & 1.0 $\pm$ 0.0 \\[6pt]
MAPPO-DQJL-CAV
& 2.2 $\pm$ 0.3 & 1.6 $\pm$ 0.3 & 1.1 $\pm$ 0.2 & 0.9 $\pm$ 0.2 & 0.7 $\pm$ 0.1 \\[6pt]
MAPPO-DQJL
& 2.4 $\pm$ 0.3 & 1.4 $\pm$ 0.3 & \textbf{0.9} $\pm$ 0.2 & \textbf{0.6} $\pm$ 0.1 & \textbf{0.4} $\pm$ 0.1 \\
\midrule
\textbf{\textit{\boldmath$N_{\text{non-EMV}}^{\text{LC}}$}} & & & & & \\ 
\multirow{1}{*}{No Control} & \multicolumn{5}{c}{34.1 $\pm$ 2.7} \\[6pt]
PPO-CC~\cite{suo2024model}
& 33.7 $\pm$ 2.6 & 27.4 $\pm$ 2.3 & 22.3 $\pm$ 2.0 & 18.5 $\pm$ 1.8 & 15.6 $\pm$ 1.3 \\[6pt]
MAPPO-DQJL-CAV
& 34.3 $\pm$ 2.5 & 28.6 $\pm$ 2.4 & 21.7 $\pm$ 1.9 & 16.9 $\pm$ 1.5 & 15.8 $\pm$ 1.3 \\[6pt]
MAPPO-DQJL
& \textbf{33.5} $\pm$ 2.8 & \textbf{26.6} $\pm$ 2.2 & \textbf{19.6} $\pm$ 1.8 & \textbf{16.2} $\pm$ 1.3 & \textbf{15.1} $\pm$ 1.0 \\
\bottomrule
\end{tabular}
}
\caption{Performance metrics on the Four-Lane Roadway across different penetration rates. Bold values indicate the best performance.}
\label{tab:penetration_study_four_lane}
\end{table}
As indicated in Table~\ref{tab:penetration_study_four_lane}, relative to the three-lane scenario, the EMV passage time under MAPPO-DQJL is further reduced, and the EMV requires even fewer lane changes at high CAV penetration. In this configuration, the extra lane further diminishes the EMV’s need to shift lanes. At 100\% penetration, MAPPO-DQJL reduces EMV lane changes to approximately 0.4 on average, implying that under most circumstances, the EMV can traverse the segment with virtually no lane-changing. This outcome suggests that the proposed method has effectively reached a peak level of coordination and efficiency in facilitating EMV passage.

Meanwhile, although we observe a slight increase in the total number of lane-change maneuvers for all non-EMVs as their numbers grow, MAPPO-DQJL still ensures that the average number of lane-change maneuvers per non-EMV remains the smallest. In other words, while non-EMVs collectively adjust their positions more frequently to accommodate the EMV, the overall system benefits from greater lane-change efficiency. This pattern reinforces that, from a system-wide perspective, MAPPO-DQJL optimizes the trade-off between EMV clearance and non-EMV maneuvering complexity. In this setting, we observe a 39.8\% reduction in EMV passage time and 55.7\% decrease in non-EMV lane-changing maneuvers with the adopt of MAPPO-DQJL.

The findings clearly indicate that as the penetration rate of CAVs increases, both EMV passage time and the number of maneuvers for EMV consistently decrease, as anticipated. This reduction occurs because, in the worst-case scenario, CAV behavior approximates that of HDVs, thereby providing at least the baseline level of performance. Moreover, the diminishing marginal improvement in reward with increasing penetration rates suggests that, beyond a certain threshold of CAV presence in the mixed traffic, the system approaches near-optimal conditions for DQJL formation. Achieving this critical mass of CAVs is therefore key to unlocking substantial gains in traffic efficiency. These results align with the findings of related studies investigating various traffic management tasks in a mixed environment \cite{wu2017flow,Houshmand2019Penetration,ding2020penetration,argote2015connected,du2017coordination,khondaker2015variable}, further emphasizing the importance of CAV penetration rates in shaping overall traffic dynamics.

\subsubsection{Traffic Density}
\paragraph{Two-Lane Roadway}
\begin{table}[htbp]
\centering
\begin{tabular}{lccc}
\toprule
\textbf{Method} & \textbf{5 veh/lane/min} & \textbf{15 veh/lane/min} & \textbf{25 veh/lane/min} \\
\midrule
\textbf{\textit{$t_{\text{EMV}} (s)$}} & & & \\ 
No Control & 120.3 $\pm$ 9.1 & 171.2 $\pm$ 10.7 & 212.4 $\pm$ 13.2 \\
PPO-CC~\cite{suo2024model} & 118.7 $\pm$ 9.5 & 169.8 $\pm$ 11.2 & 202.6 $\pm$ 12.5 \\
MAPPO-DQJL-CAV & 117.4 $\pm$ 9.3 & 159.7 $\pm$ 10.3 & 191.5 $\pm$ 11.4 \\
MAPPO-DQJL & \textbf{114.9} $\pm$ 8.7 & \textbf{149.9} $\pm$ 11.8 & \textbf{188.2} $\pm$ 10.9 \\
\midrule
\textbf{\textit{\boldmath$N_{\text{EMV}}^{\text{LC}}$}} & & & \\ 
No Control & 2.2 $\pm$ 0.5 & 4.0 $\pm$ 0.3 & 3.5 $\pm$ 0.6 \\
PPO-CC~\cite{suo2024model} & 1.0 $\pm$ 0.0 & 1.0 $\pm$ 0.0 & 1.0 $\pm$ 0.0 \\
MAPPO-DQJL-CAV & 1.3 $\pm$ 0.4 & 1.6 $\pm$ 0.3 & 2.5 $\pm$ 0.5 \\
MAPPO-DQJL & \textbf{0.5} $\pm$ 0.1 & \textbf{0.8} $\pm$ 0.2 & \textbf{1.2} $\pm$ 0.3 \\
\midrule
\textbf{\textit{\boldmath$N_{\text{non-EMV}}^{\text{LC}}$}} & & & \\ 
No Control & 8.8 $\pm$ 1.2 & 15.0 $\pm$ 1.7 & 21.0 $\pm$ 2.2 \\
PPO-CC~\cite{suo2024model} & 9.2 $\pm$ 1.1 & 12.8 $\pm$ 1.5 & 18.5 $\pm$ 2.1 \\
MAPPO-DQJL-CAV & 9.7 $\pm$ 1.2 & 16.3 $\pm$ 1.7 & 20.9 $\pm$ 2.3 \\
MAPPO-DQJL & \textbf{7.9} $\pm$ 1.2 & \textbf{10.9} $\pm$ 1.4 & \textbf{16.2} $\pm$ 1.4 \\
\bottomrule
\end{tabular}
\caption{Performance metrics on the Two-Lane Roadway across varying traffic densities. Bold values indicate the best performance.}
\label{tab:density_two_lane}
\end{table}
As indicated in Table~\ref{tab:density_two_lane}, under moderate traffic conditions (e.g., 15 veh/lane/min), MAPPO-DQJL demonstrates relatively greater time savings compared to fully congested scenarios (25 veh/lane/min). This finding suggests that while MAPPO-DQJL’s coordinated strategy excels at reducing EMV passage time and maneuver complexity at moderate densities, its relative benefits diminish as the system approaches full saturation. In other words, while MAPPO-DQJL significantly improves performance under intermediate traffic loads, the advantage it offers naturally tapers off once congestion reaches near-maximum levels.

Under low-density conditions, the EMV performs approximately 0.5 lane-change maneuvers on average, demonstrating the near-elimination of unnecessary lane changes. Even as density increases to 25 veh/lane/min, MAPPO-DQJL maintains fewer EMV maneuvers than any other method, underscoring its capacity to ensure smoother EMV passage. At moderate congestion levels, MAPPO-DQJL’s EMV lane-change frequency is only about 20\% of that observed with No Control, and even under heavily congested conditions (25 veh/lane/min), it remains approximately one-third. These observations align with earlier findings, suggesting that as spatial flexibility declines, the coordination potential of MAPPO-DQJL diminishes, yet still surpasses that of alternative approaches.
Meanwhile, in low-density conditions (5 veh/lane/min), non-EMVs make relatively few lane changes overall, and MAPPO-DQJL still achieves the lowest number. At moderate density (15 veh/lane/min), the values are in line with previous analyses, reaffirming MAPPO-DQJL’s advantage. Under heavy congestion (25 veh/lane/min), non-EMVs must undertake more lane changes to accommodate the EMV, but MAPPO-DQJL continues to yield fewer maneuvers than the other methods, demonstrating its enduring benefits even as the system approaches saturation.

\paragraph{Three-Lane Roadway}
\begin{table}[htbp]
\centering
\begin{tabular}{lccc}
\toprule
\textbf{Method} & \textbf{5 veh/lane/min} & \textbf{15 veh/lane/min} & \textbf{25 veh/lane/min} \\
\midrule
\textbf{\textit{$t_{\text{EMV}} (s)$}} & & & \\ 
No Control        & 94.7 $\pm$ 7.4 & 156.7 $\pm$ 10.9 & 216.2 $\pm$ 12.3 \\
PPO-CC~\cite{suo2024model} & 92.9 $\pm$ 7.2 & 150.1 $\pm$ 10.4 & 206.4 $\pm$ 11.9 \\
MAPPO-DQJL-CAV    & 91.3 $\pm$ 7.6 & 141.6 $\pm$ 10.5 & 199.1 $\pm$ 11.3 \\
MAPPO-DQJL        & \textbf{89.4} $\pm$ 6.9 & \textbf{135.7} $\pm$ 10.8 & \textbf{196.3} $\pm$ 10.0 \\
\midrule
\textbf{\textit{\boldmath$N_{\text{EMV}}^{\text{LC}}$}} & & & \\ 
No Control        & 1.9 $\pm$ 0.2 & 1.3 $\pm$ 0.2 & 2.9 $\pm$ 0.5 \\
PPO-CC~\cite{suo2024model} & 1.0 $\pm$ 0.0 & 1.0 $\pm$ 0.0 & 1.0 $\pm$ 0.0 \\
MAPPO-DQJL-CAV    & 1.4 $\pm$ 0.3 & 1.6 $\pm$ 0.2 & 2.2 $\pm$ 0.3 \\
MAPPO-DQJL        & \textbf{0.8} $\pm$ 0.1 & \textbf{0.9} $\pm$ 0.2 & \textbf{1.4} $\pm$ 0.2 \\
\midrule
\textbf{\textit{\boldmath$N_{\text{non-EMV}}^{\text{LC}}$}} & & & \\ 
No Control        & 13.7 $\pm$ 1.4 & 21.9 $\pm$ 1.8 & 31.3 $\pm$ 2.3 \\
PPO-CC~\cite{suo2024model} & 13.2 $\pm$ 1.3 & 17.2 $\pm$ 1.4 & 25.4 $\pm$ 2.0 \\
MAPPO-DQJL-CAV    & 14.8 $\pm$ 1.5 & 19.2 $\pm$ 1.6 & 27.4 $\pm$ 2.1 \\
MAPPO-DQJL        & \textbf{12.9} $\pm$ 1.4 & \textbf{16.7} $\pm$ 1.3 & \textbf{23.1} $\pm$ 2.2 \\
\bottomrule
\end{tabular}
\caption{Performance metrics on the Three-Lane Roadway across varying traffic densities. Bold values indicate the best performance.}
\label{tab:density_three_lane}
\end{table}

The performance metrics across varying traffic densities on the Three-Lane Roadway, as summarized in Table~\ref{tab:density_three_lane}, highlight the consistent advantages of MAPPO-DQJL. For \textit{\boldmath$t_{\text{EMV}}$}, MAPPO-DQJL demonstrates shorter durations across all densities. At a moderate density of 15 veh/lane/min, these results align closely with prior benchmarks, confirming its robustness. Even as traffic density increases to 25 veh/lane/min, MAPPO-DQJL maintains a relative advantage over competing methods, underscoring its effectiveness in high-density scenarios.

The trends in EMV lane-change maneuvers further emphasize the benefits of MAPPO-DQJL. As traffic density rises, the additional lanes of the roadway contribute to keeping EMV maneuvers low. At 15 veh/lane/min, MAPPO-DQJL's performance is consistent with earlier analyses, while at higher densities, it continues to effectively limit the EMV's lane changes compared to other methods.

Similarly, non-EMV lane-change maneuvers exhibit the anticipated trend of increasing with traffic density. At 15 veh/lane/min, MAPPO-DQJL’s results align with previously established patterns, demonstrating fewer lane changes than competing methods. Even at 25 veh/lane/min, where maneuver frequencies are heightened, MAPPO-DQJL preserves a distinct advantage, reflecting its capacity to handle high-density traffic scenarios more efficiently.

Additionally, it is noteworthy that MAPPO-DQJL-CAV induces more non-EMV lane changes than both PPO-CC and MAPPO-DQJL. This outcome arises from the fact that MAPPO-DQJL-CAV, by treating HDVs purely as environmental elements rather than learning agents, fails to incorporate the nuanced, diverse behaviors of HDVs into its decision-making process. Consequently, the control framework lacks the sophistication to minimize disruptions effectively, leading to a higher frequency of non-EMV maneuvers.

\paragraph{Four-Lane Roadway}
\begin{table}[htbp]
\centering
\begin{tabular}{lccc}
\toprule
& 5 veh/lane/min & 15 veh/lane/min & 25 veh/lane/min \\
\midrule
\multicolumn{4}{l}{\textbf{\textit{$t_{\text{EMV}} (s)$}}} \\[4pt]
No Control        & 84.3 $\pm$ 6.9  & 147.2 $\pm$ 10.4 & 206.7 $\pm$ 11.7 \\
PPO-CC~\cite{suo2024model} & 82.4 $\pm$ 6.7  & 140.3 $\pm$ 10.1 & 196.6 $\pm$ 10.9 \\
MAPPO-DQJL-CAV    & 81.6 $\pm$ 6.8  & 131.4 $\pm$ 10.2 & 186.2 $\pm$ 10.6 \\
MAPPO-DQJL        & \textbf{78.7} $\pm$ 5.9 & \textbf{125.4} $\pm$ 10.6 & \textbf{183.1} $\pm$ 9.9 \\
\midrule
\textbf{\textit{\boldmath$N_{\text{EMV}}^{\text{LC}}$}} & & & \\ 
No Control        & 1.6 $\pm$ 0.3 & 1.3 $\pm$ 0.2 & 2.3 $\pm$ 0.4 \\
PPO-CC~\cite{suo2024model} & 1.0 $\pm$ 0.0 & 1.0 $\pm$ 0.0 & 1.0 $\pm$ 0.0 \\
MAPPO-DQJL-CAV    & 1.1 $\pm$ 0.2 & 1.2 $\pm$ 0.2 & 1.7 $\pm$ 0.3 \\
MAPPO-DQJL        & \textbf{0.5} $\pm$ 0.1 & \textbf{0.6} $\pm$ 0.1 & \textbf{1.1} $\pm$ 0.2 \\
\midrule
\textbf{\textit{\boldmath$N_{\text{non-EMV}}^{\text{LC}}$}} & & & \\ 
No Control        & 17.4 $\pm$ 1.7 & 26.3 $\pm$ 2.1 & 35.4 $\pm$ 2.4 \\
PPO-CC~\cite{suo2024model} & 16.6 $\pm$ 1.6 & 19.9 $\pm$ 1.9 & 28.4 $\pm$ 2.2 \\
MAPPO-DQJL-CAV    & 17.9 $\pm$ 1.5 & 21.3 $\pm$ 1.8 & 29.3 $\pm$ 2.3 \\
MAPPO-DQJL        & \textbf{15.2} $\pm$ 1.3 & \textbf{16.4} $\pm$ 1.5 & \textbf{24.7} $\pm$ 2.1 \\
\bottomrule
\end{tabular}
\caption{Performance metrics on the Four-Lane Roadway across varying traffic densities. Bold entries indicate best performance.}
\label{tab:density_four_lane}
\end{table}
As indicated in Table~\ref{tab:density_four_lane}, the introduction of an additional lane in the four-lane scenario further reduces EMV passage times at low and moderate densities, aligning with previously established trends. Although EMV passage times rise noticeably at 25 veh/lane/min, MAPPO-DQJL still achieves the shortest durations. A similar pattern is evident when examining EMV lane changes: under moderate densities, the EMV can maintain even fewer lane changes, and while maneuver counts increase slightly at high density, MAPPO-DQJL continues to enable the fewest EMV lane changes overall. In parallel, non-EMVs must also perform additional lane changes as density escalates. While MAPPO-DQJL moderates this increase more effectively than other approaches, the underlying trend of heightened complexity at greater densities persists, mirroring the patterns observed in both two-lane and three-lane configurations.

Experimental results for all three scenarios have cleary indicate that, under low-density conditions, the performance gap between MAPPO-DQJL and other candidate schemes remains modest, as even conventional approaches can efficiently handle sparse traffic. As density increases and approaches moderate congestion levels, MAPPO-DQJL’s advantage becomes most pronounced, significantly outpacing other schemes in reducing EMV passage times. However, once the roadway becomes fully congested, this benefit diminishes. In scenarios of near-total saturation, no scheme can substantially expedite EMV passage, resulting in a vanishing performance gap. This finding underlines the importance of intermediate traffic densities, where MAPPO-DQJL’s strategic coordination yields the greatest relative gains over competing approaches. Studies \cite{stern2018dissipation, wang2015caccdensity, feng2021densityimpact}, exploring mixed traffic tasks similarly identify “sweet spots” in density. These insights emphasize that operational conditions, not just automation levels, are crucial for realizing the full benefits of advanced traffic management interventions.

Across the entire density sensitivity study, MAPPO-DQJL-CAV consistently underperformed compared to MAPPO-DQJL. This disparity arises because MAPPO-DQJL-CAV treats HDVs purely as part of the environment, rather than as learning agents. Without incorporating HDVs into the coordination process, the control framework cannot anticipate or adapt to the distinct and often stochastic behaviors of each HDV. Consequently, as traffic densities increase, the system struggles to maintain efficient flow, leading to more frequent lane changes and prolonged EMV passage times. In other words, omitting HDVs from the learning process significantly impairs the scheme’s overall coordination capabilities, particularly under congested conditions. 

This observation reinforces the importance of designing a learning framework that accounts for both CAVs and HDVs as active participants, thereby optimizing system performance across a wide range of traffic scenarios.
\FloatBarrier
\subsection{Ablation Studies}
\label{subsec:ablations}
A range of ablation studies is conducted to clarify the influence of key components in the MAPPO-DQJL framework. By selectively altering certain design choices, we can isolate their effects and better understand the fundamental drivers of performance in DQJL formation.

\subsubsection{Calibration of \texorpdfstring{$\alpha$}{alpha}}\label{subsec:reward}
Reward shaping plays a pivotal role in guiding agent behaviors. By adjusting the balance between global and local rewards, we can observe how different configurations influence EMV passage efficiency and overall traffic stability. All experiments conducted in Subsec.~\ref{subsec:sensitivity} adopt $\alpha = 0.75$. In this ablation study, we systematically vary the balancing coefficient \(\alpha\) in Eq.~\eqref{eqn:return} on the Three-Lane Roadway under 15 veh/lane/min traffic density and 50\% CAV penetration. Each unique value of \(\alpha\) is associated with a separate training phase for the agents, followed by 20 simulation runs to evaluate performance. The results are presented in Table~\ref{tab:alpha_ablations}.
\begin{table}[htbp]
\centering
\begin{tabular}{lccccc}
\toprule
 & \(\alpha = 0\) & \(\alpha = 0.25\) & \(\alpha = 0.5\) & \(\alpha = 0.75\) & \(\alpha = 1.0\) \\
\midrule
$t_{\text{EMV}}$ (s) & 200.3 $\pm$ 11.8 & 170.7 $\pm$ 11.9 & 150.2 $\pm$ 11.3 & 135.9 $\pm$ 10.9 & 125.4 $\pm$ 10.1 \\[6pt]
$N_{\text{EMV}}^{\text{LC}}$ & 4.1 $\pm$ 0.4 & 2.9 $\pm$ 0.3 & 1.7 $\pm$ 0.3 & 0.9 $\pm$ 0.2 & 0.7 $\pm$ 0.1 \\[6pt]
$N_{\text{non-EMV}}^{\text{LC}}$ & 10.2 $\pm$ 1.1 & 13.7 $\pm$ 1.6 & 16.3 $\pm$ 1.7 & 19.7 $\pm$ 1.3 & 23.4 $\pm$ 1.9 \\
\bottomrule
\end{tabular}
\caption{Effects of different \(\alpha\) values on EMV passage time and lane-change maneuvers on the Three-Lane Roadway with 15 veh/lane/min and 50\% CAV penetration rate.}
\label{tab:alpha_ablations}
\end{table}

As shown in Table~\ref{tab:alpha_ablations}, increasing \(\alpha\) leads to lower \(t_{\text{EMV}}\) and \(N_{\text{EMV}}^{\text{LC}}\), as the agents shift their focus toward optimizing global EMV-related objectives. This adjustment in reward composition enhances the EMV’s passage efficiency and reduces its lane-change frequency. However, this improvement comes with a trade-off. As \(N_{\text{EMV}}^{\text{LC}}\) decreases, \(N_{\text{non-EMV}}^{\text{LC}}\) tends to increase. The process of dynamically clearing lanes for the EMV necessitates multiple layers of yielding maneuvers, compelling non-EMVs to adjust their positions more frequently. In other words, while prioritizing the EMV’s global objectives streamlines its trajectory, the surrounding traffic must accommodate these changes, resulting in a higher number of lane-change maneuvers by non-EMVs. This provides valuable insight into how EMV operations can cause disruption to overall traffic flow, emphasizing the need for carefully balanced strategies when managing EMV passage under various traffic conditions.
\subsubsection{Parameter Sharing}\label{subsec:parameters_sharing}
Parameter sharing among CAVs facilitates knowledge transfer and promotes consistent strategies across agents. To evaluate the impact of this design choice, we conducted an ablation study comparing our shared policy approach against a variant where each CAV maintains an independent policy network while HDVs retain their fixed policies. This investigation helps understand whether independent learning could potentially lead to better coordination strategies for DQJL formation.

Due to the computational demands of training multiple independent networks, we constrained the experiment to 15 CAVs and 15 HDVs, with HDVs maintaining their fixed behavioral policies throughout. The simulation environment was configured as a Three-Lane Roadway spanning 500 meters, allowing for focused analysis of how parameter sharing affects CAV coordination capability.

\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.7\textwidth]{Figures/parameter_sharing_vs_no_sharing.png}
    \caption{Reward convergence with and without parameter sharing for CAVs.}
    \label{fig:parameter_sharing}
\end{figure}
See Fig.~\ref{fig:parameter_sharing} for the learning patterns of CAV agents with and without shared parameters. Without a shared policy network, CAV agents are treated as independent learners, optimizing their policies in isolation. This limits generalization and knowledge transfer, making it harder to converge on optimal strategies for establishing DQJLs.

In contrast, treating CAV agents as homogeneous with a shared policy network offers key advantages. Parameter sharing facilitates knowledge transfer, enabling consistent strategies across agents, especially in stochastic environments. It also reduces learning complexity, accelerating convergence, and inherently promotes cooperation by aligning agent objectives. These benefits highlight the efficacy of MAPPO-DQJL’s shared policy design.

\section{Summary}
\label{sec:conclusion}

This study introduces DQJLs as a practical solution for enabling EMVs to navigate congested urban traffic efficiently. Using the MAPPO framework, the proposed system coordinates CAVs and HDVs to improve EMV passage times while minimizing disruptions to overall traffic flow.

Results from SUMO simulations validate the framework’s ability to reduce EMV passage times with minimal lane changes, for both EMVs and non-EMVs. By treating HDVs as dynamic agents rather than passive obstacles, the system accounts for their stochastic behaviors, enhancing traffic interaction modeling and coordination. The framework scales effectively to multi-lane scenarios, achieving smoother traffic flow and fewer EMV lane changes. Higher CAV penetration rates amplify these benefits, leading to shorter EMV passage times and fewer maneuvers for all vehicles, emphasizing the importance of adopting V2X technologies.

The CTDE approach, combined with a transformer-based architecture, enhances scalability and robustness. This design models complex inter-agent dependencies, ensuring reliable performance in high-density, dynamic traffic conditions. The results demonstrate the framework’s capacity to manage real-world traffic complexities while maintaining system efficiency.

DQJLs have clear potential for real-world ITS applications, particularly in congested urban settings. By reducing EMV response times, the proposed approach improves outcomes for ambulances, fire trucks, and law enforcement, safeguarding lives and property. As V2X technologies continue to advance, this framework offers a path toward more cooperative and efficient roadways, transforming EMV assistance into a streamlined, society-wide effort to enhance urban safety and resilience. Future work will focus on incorporating more realistic features, expanding scalability to address higher-dimensional uncertainties, and bridging the sim-to-real gap for practical deployment.