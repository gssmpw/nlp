\section{Results and Discussion}
\label{sec:exp}

\begin{figure}[t]
\centering
\includegraphics[width=\textwidth]{images/three_model_comparison_500B_unified.pdf} 
% \caption{\textbf{Evolution of Training Loss and Average Task Score for Different Model Sizes and Initialization Methods. }
% }
\vspace{-1em}
\caption{
\textbf{Comparison of learning curves for different MoE construction methods}.
The top and bottom rows illustrate the changes in training loss and downstream task scores during training, respectively. In both metrics, the proposed method, \methodname{} with $r=0.5$, achieves the best performance, gaining initial knowledge transfer while avoiding convergence slowdown. 
}


\label{fig:combined_train_loss_and_score}
\end{figure}


In this section, we address the following questions through experiments: 
 Is \methodname{} superior to existing MoE construction methods, and does \methodname{} resolve the issue of slower convergence? (Section~\ref{sec:result-method-comparison})  Does it perform well even in large-scale settings? (Section~\ref{sec:resuilts-scaling})  What is the impact of the re-initialization ratio 
$r$? (Section~\ref{sec:exp:ratio}) How are the experts specialized? (Section~\ref{sec:exp:diversity}) 



\subsection{Method Comparison}
\label{sec:result-method-comparison}
First, we compare \methodname{} with existing methods using small (8$\times$152M) to medium (8$\times$1.5B) scale settings. The left two columns of Figure~\ref{fig:combined_train_loss_and_score} illustrate the learning curves under these settings. The top and bottom rows illustrate the changes in training loss and downstream task scores during training, respectively. Note that in LLM pretraining, training loss serves as a reliable performance indicator since the risk of overfitting is low.
The performance on downstream tasks is represented by the average score across 12 tasks, which is commonly used as the overall evaluation metric. A detailed breakdown will be discussed later in conjunction with Table~\ref{tab:detailed-ja-en-comparison_method}.

Figure~\ref{fig:combined_train_loss_and_score} shows that \methodname{} at \( r = 0.5 \) (green) is significantly more efficient compared to other methods. The top row shows the training loss, while the bottom row displays the evaluation scores using downstream tasks. In both metrics and for both model sizes, \methodname{} becomes the clear winner after some training. Notably, the slope of the learning curve, which indicates convergence rate, is superior. Furthermore, it can be observed that the slope of the learning curve is consistent with the case of training from scratch, suggesting that \methodname{} resolves the crucial challenge of balancing knowledge transfer and expert specialization in Upcycling. For further analysis on expert specialization, see Section~\ref{sec:exp:diversity}.

Among existing methods, \NUname{} exhibited the slowest loss reduction rate and improvement in task scores. Branch-Train-Mix, which starts MoE training after each expert has been trained for 100B steps on different domains such as Japanese, English, and code, initially shows an advantage over \NUname{} due to this favorable initialization. However, its long-term learning pace is on par with \NUname{}, and it is ultimately overtaken by \methodname{}. As an ablation study, we evaluated setting \( r = 1.0 \) in \methodname{}, in addition to the standard \( r = 0.5 \). This configuration involves random initialization of all FFNs while reusing weights for embeddings and self-attention layers. This configuration might seem inefficient at first glance. Nevertheless, our large-scale experiments reveal that even such a seemingly naive baseline can outperform \NUname{} in certain scenarios. For additional analysis on the impact of the \( r \) value, refer to Section~\ref{sec:exp:ratio}.

% Table \ref{tab:detailed-ja-en-comparison_method} provides a comparison of the final downstream task performance for models trained with various methods under these 8$\times$152M and 8$\times$1.5B settings.
Table~\ref{tab:detailed-ja-en-comparison_method} provides a comparison of the final downstream task performance for models trained with various methods under these 8$\times$152M and 8$\times$1.5B settings. \diff{Model numbers refer to the leftmost column of this table.}
This table also includes the dense models used for upcycling. Specifically, Model 1 is the dense model used to initialize Models 3-\diff{7}, and Model \diff{8} is used to initialize Models \diff{10}-\diff{14}.
The proposed method, \methodname{} (DU) with $r=0.5$, consistently demonstrates superior performance across these model scales.
% In particular, both variants of DU (50\%) and DU (100\%) achieved results that outperform other approaches such as From Scratch (FS), \NUname{} (NU), and Branch-Train-MiX (BTX).












\begin{table}[t]
\caption{
\textbf{Comparison of evaluation results between models with different initialization.} 
Training from scratch (FS), Branch-Train-Mix (BTX), \NUname{} (NU), \diff{\RNUname{} (RNU)} and \methodname{} (DU) are compared. 
\diff{$^*$ BTX requires additional 300B tokens to obtain specialized dense models before MoE construction.}
%In addition to the individual scores for each downstream task, we also present the average score across 12 tasks, which is commonly used as the overall evaluation metric for the models.
%For both individual scores and the average, higher values indicate better performance.
Bold letters indicate the highest score within each model size.
%\textbf{Performance comparison of downstream tasks across different model sizes and training methods.} Scores represent task-specific metrics (higher is better). Bold indicates the best score for each model size.
}

\label{tab:detailed-ja-en-comparison_method}
\centering
\small
\renewcommand{\arraystretch}{1.03}
% \tabcolsep=0.15cm
% \begin{adjustbox}{width=\linewidth}
% \begin{tabular}{cll*{14}{r}}
% \toprule
% & \multicolumn{2}{c}{\textbf{Model}} & & \multicolumn{13}{c}{\textbf{Individual Scores}} \\
% \cmidrule{2-3} \cmidrule{5-16}
% \textbf{\#} &
% \makecell[c]{\textbf{Archi-} \\ \textbf{tecture}} & \makecell[c]{\textbf{MoE} \\ \textbf{Init}} & & \makecell[c]{\textbf{JEM}\\\textbf{HQA}} & \makecell[c]{\textbf{NIILC}} & \makecell[c]{\textbf{JSQ}} & \makecell[c]{\textbf{XL}-\\\textbf{Sum}} & \makecell[c]{\textbf{WMT}\\\textbf{E$\to$J}} & \makecell[c]{\textbf{WMT}\\\textbf{J$\to$E}} & \makecell[c]{\textbf{OB}\\\textbf{QA}} & \makecell[c]{\textbf{TQA}} & \makecell[c]{\textbf{HS}} & \makecell[c]{\textbf{SQ}\\\textbf{v2}} & \makecell[c]{\textbf{XW}-\\\textbf{EN}} & \makecell[c]{\textbf{BBH}} & \makecell[c]{\textbf{Avg}} \\

\tabcolsep=0.12cm
% \tabcolsep=0.1cm
\begin{adjustbox}{width=\linewidth}
\begin{tabular}{cllrrrr*{13}{r}}
% \begin{tabular}{cllrrrrrr*{13}{r}}
\toprule
% & \multicolumn{2}{c}{\textbf{Model}} & \multicolumn{2}{c}{\textbf{Training}} & \multicolumn{13}{c}{\textbf{Individual Scores}} \\
% \cmidrule{2-3} \cmidrule{4-5} \cmidrule{6-17}
& \multicolumn{2}{c}{\textbf{Model}} & & \multicolumn{2}{c}{\diff{\textbf{Training}}} & & \multicolumn{13}{c}{\textbf{Individual Scores}} \\
\cmidrule{2-3} \cmidrule{5-6} \cmidrule{8-19}

\textbf{\#} &
\makecell[c]{\textbf{Archi-} \\ \textbf{tecture}} & \makecell[c]{\textbf{MoE} \\ \textbf{Init}} & & \makecell[c]{\diff{\textbf{Tokens}}} & \makecell[c]{\diff{\textbf{FLOPs}} \\ \diff{($\times 10^{21}$)}} & & \makecell[c]{\textbf{JEM}\\\textbf{HQA}} & \makecell[c]{\textbf{NIILC}} & \makecell[c]{\textbf{JSQ}} & \makecell[c]{\textbf{XL}-\\\textbf{Sum}} & \makecell[c]{\textbf{WMT}\\\textbf{E$\to$J}} & \makecell[c]{\textbf{WMT}\\\textbf{J$\to$E}} & \makecell[c]{\textbf{OB}\\\textbf{QA}} & \makecell[c]{\textbf{TQA}} & \makecell[c]{\textbf{HS}} & \makecell[c]{\textbf{SQ}\\\textbf{v2}} & \makecell[c]{\textbf{XW}-\\\textbf{EN}} & \makecell[c]{\textbf{BBH}} & \makecell[c]{\textbf{Avg}} \\



% \midrule
% & & 4shot & 4shot  & 4shot  & 1shot  & 4shot  & 4shot  & 4shot  & 4shot  & 4shot  & 4shot  & 4shot  & 3 shot (CoT)&\\
\midrule
% \multicolumn{4}{l}{\textbf{\textit{152Mベースでの手法比較実験:}}}  \\
\multicolumn{6}{l}{\textbf{\textit{Dense 152M $\to$ MoE 8$\times$152M:}}} \\
1 & Dense & -- & & \diff{1,000B} & \diff{1.59} & & 17.6 & 7.9 & 10.6 & 2.4 & 0.5 & 0.5 & 14.6 & 3.0 & 28.6 & 2.0 & 60.6 & 11.5 & 13.3 \\
2 & MoE  & FS & & \diff{500B} & \diff{0.91} & & 25.2	&13.6	&19.4	&1.8	&0.9	&0.4	&16.6	&2.6	&31.2	&\textbf{12.9}	&64.4	&10.7	&16.6 \\
3 & MoE &BTX & & \diff{800B$^*$} & \diff{1.39} & & 28.6	&17.1	&26.6	&\textbf{4.3}	&2.7	&1.1	&\textbf{18.4}	&5.1	&\textbf{32.5}	&5.3	&\textbf{65.0}	&15.9	&18.5 \\
4 &MoE & NU & & \diff{500B} & \diff{0.91} & & 28.2	&16.2	&24.4	&3.5	&3.0	&1.1	&18.2	&5.8	&31.9	&4.5	&63.5	&14.7	&17.9 \\
\diff{5} & \diff{MoE} & \diff{RNU ($r$=0.5)} & & \diff{500B} & \diff{0.91} & & \diff{28.6} & \diff{17.1} & \diff{29.4} & \diff{3.7}	& \diff{2.3} & \diff{1.6} & \diff{16.8} & \diff{5.3}	& \diff{32.0} & \diff{4.8} & \diff{64.5} & \diff{17.4} & \diff{18.6} \\
\rowcolor{verylightgray} \diff{6} & MoE & DU ($r$=0.5) & & \diff{500B} & \diff{0.91} & & \textbf{32.2}	&\textbf{18.0}	&30.6	&3.7	&\textbf{4.7}	&\textbf{2.3}	&16.8	&\textbf{6.1}	&\textbf{32.5}	&6.2	&64.2	&\textbf{19.1}	&\textbf{19.7} \\
\diff{7} &MoE & DU ($r$=1.0) & & \diff{500B} & \diff{0.91} & & 27.2	&16.8	&\textbf{32.5}	&4.1	&3.7	&1.6	&17.0	&5.9	&32.4	&4.9	&64.8	&15.4	&18.9 \\

 \midrule
%\multicolumn{4}{l}{\textbf{\textit{1.5B base model comparison:}}} \\
\multicolumn{6}{l}{\textbf{\textit{Dense 1.5B $\to$ MoE 8$\times$1.5B:}}} \\
\diff{8} & Dense & -- & & \diff{1,000B} & \diff{11.76} & & 49.6 & 42.5 & 48.1 & 11.3 & 16.8 & 8.5 & 22.2 & 23.8 & 42.9 & 16.2 & 82.5 & 25.1 & 32.5 \\
\diff{9} & MoE & FS & & \diff{500B} & \diff{9.05} & & 48.3 & 45.4 & 59.1 & 7.5 & 16.6 & 6.9 & 26.4 & 31.5 & 47.3 & 15.0 & 83.7 & 25.9 & 34.5 \\
\diff{10} & MoE & BTX & & \diff{800B$^*$} & \diff{12.58} & & 44.3	&51.8	&69.4	&11.9	&22.4	&\textbf{12.5}	&27.8	&39.2	&49.7	&18.7	&\textbf{86.4}	&28.9	&38.6\\
\diff{11} & MoE & NU & & \diff{500B} & \diff{9.05} & & 50.4 & 50.6 & 61.7 & 12.4 & 21.6 & 10.5 & 26.8 & 36.2 & 47.7 & 19.0 & 85.0 & 27.2 & 37.4 \\
\diff{12} & \diff{MoE} & \diff{RNU ($r$=0.5)} & & \diff{500B} & \diff{9.05} & & \diff{\textbf{53.6}} & \diff{50.5} & \diff{71.2} & \diff{12.3} & \diff{22.3}& \diff{11.7} & \diff{26.4} & \diff{40.0} & \diff{49.9} & \diff{19.1} & \diff{84.9} & \diff{27.5} & \diff{39.1} \\
 \rowcolor{verylightgray} \diff{13} & MoE & DU ($r$=0.5) & & \diff{500B} & \diff{9.05} & & 51.1 & \textbf{52.3} & \textbf{72.5} & \textbf{13.7} & \textbf{22.5} & \textbf{12.5} & \textbf{30.6} & \textbf{41.3} & \textbf{50.4} & \textbf{21.2} & 86.2 & \textbf{29.1} & \textbf{40.3} \\
\diff{14} & MoE & DU ($r$=1.0) & & \diff{500B} & \diff{9.05} & & 52.1 & 50.9 & 68.8 & 12.3 & 21.9 & 12.4 & 25.0 & 39.1 & 49.7 & 20.6 & 86.0 & 27.9 & 38.9 \\
%  \midrule
% \multicolumn{4}{l}{\textbf{\textit{3.7Bベースでのスケーリングの実験: }}}  \\
% Dense$^1$ & -- & 44.5 & 47.2 & 78.8 & 12.8 & 21.4 & 15.4 & 25.0 & 33.8 & 47.3 & 23.7 & 85.9 & 28.7 & 38.7 \\
% \multirow{2}{*}{8$\times$3.7B} & FS &53.5	&50.8	&69.6	&10.4	&20.6	&13.9	&29.0	&45.8	&51.1	&21.1	&87.1	&28.1	&40.1 \\
%  & DU (50\%) &
%  \textbf{47.5}&	\textbf{57.0}	&\textbf{82.2}&	\textbf{16.3}	&\textbf{25.0}&	\textbf{19.0}&	\textbf{31.2}&	\textbf{53.6}	&\textbf{54.4}&	\textbf{26.3}&	\textbf{88.5}&	\textbf{32.2}&	\textbf{44.4}\\
% \multicolumn{4}{l}{\textbf{\textit{3.7Bベースでのスケーリングの実験のベースライン: }}}  \\
% 13B Dense& FS &47.6	&58.3	&85.2	&14.1	&24.6	&18.3	&31.4	&48.6	&53.1	&29.3	&88.3	&35.2	&44.5 \\

% 3.7B Dense (2.1T)& FS &42.3	&53.2	&80.4	&14.3	&22.6	&15.9	&28.2	&42.2	&50.6	&25.8	&87.3	&30.9	&41.1 \\

\bottomrule
\end{tabular}
\end{adjustbox}
%\raggedright
%\small
%$^1$ Checkpoint prior to upcycling to MoE. \\
\end{table}

\begin{table}[t]
\caption{\textbf{Comparison between dense and MoE with large-scale configuration.}
\methodname{} (DU) works well even at 8$\times$3.7B scale. The MoE model with \methodname{} outperforms dense models trained with higher computational costs, demonstrating the effectiveness of \methodname{}.
%Model 1 is a dense model created to be upcycled, and Models 2 and 3 are MoE models constructed using different methods. In addition, for comparison, we include Models 4 and 5, which are dense models built with more computational resources using the same methodology. Despite using fewer computational resources, the MoE model created through \methodname{} outperforms these dense models, demonstrating the effectiveness of \methodname{}.
%\caption{\textbf{Performance comparison of downstream tasks across different model sizes and training methods.} Scores represent task-specific metrics (higher is better). Bold indicates the highest score across all model sizes and methods.
}

\tabcolsep 3pt%=0.1cm
\label{tab:detailed-ja-en-comparison_method_scaling}
\centering
\small
\renewcommand{\arraystretch}{1.3}
\begin{adjustbox}{width=\linewidth}
\begin{tabular}{cllrrrrrr*{13}{r}}
\toprule

%Model & Training Method & A para& Para &  flops & tokens& \makecell[c]{JEM\\HopQA} & \makecell[c]{NIILC} & \makecell[c]{JSQuAD} & \makecell[c]{XL-\\Sum} & \makecell[c]{WMT20\\En$\to$Ja} & \makecell[c]{WMT20\\Ja$\to$En} & \makecell[c]{Open\\BookQA} & \makecell[c]{Triv.\\QA} & \makecell[c]{Hella\\Swag} & \makecell[c]{SQuAD\\v2} & \makecell[c]{XWino-\\grad EN} & \makecell[c]{BBH} & AVG \\
& \multicolumn{3}{c}{\textbf{Model}} & & \multicolumn{2}{c}{\textbf{Training}} & & \multicolumn{13}{c}{\textbf{Individual Scores}} \\
\cmidrule{2-4} \cmidrule{6-7} \cmidrule{9-20} 
% \rule{0pt}{4ex}
%\makecell[c]{\textbf{Architecture}} & \makecell[c]{\textbf{MoE} \\ \textbf{Init}} & \makecell[c]{\textbf{Act Params} / \\\textbf{Total Params}} & & \makecell[c]{\textbf{Tokens}} & \makecell[c]{\textbf{FLOPs}} & & \makecell[c]{\textbf{JEM}\\\textbf{HopQA}} & \makecell[c]{\textbf{NIILC}} & \makecell[c]{\textbf{JSQuAD}} & \makecell[c]{\textbf{XL}-\\\textbf{Sum}} & \makecell[c]{\textbf{WMT20}\\\textbf{En$\to$Ja}} & \makecell[c]{\textbf{WMT20}\\\textbf{Ja$\to$En}} & \makecell[c]{\textbf{Open}\\\textbf{BookQA}} & \makecell[c]{\textbf{Triv.}\\\textbf{QA}} & \makecell[c]{\textbf{Hella}\\\textbf{Swag}} & \makecell[c]{\textbf{SQuAD}\\\textbf{v2}} & \makecell[c]{\textbf{XWino}-\\\textbf{grad EN}} & \makecell[c]{\textbf{BBH}} & \textbf{Avg} \\
\makecell[c]{\textbf{\#}} &
\makecell[c]{\textbf{Architecture}} & \makecell[c]{\textbf{MoE} \\ \textbf{Init}} & \makecell[c]{\textbf{Act Params} / \\\textbf{Total Params}} & & \makecell[c]{\textbf{Tokens}} & \makecell[c]{\textbf{FLOPs} \\ ($\times 10^{22}$)} & & \makecell[c]{\textbf{JEM}\\\textbf{HQA}} & \makecell[c]{\textbf{NIILC}} & \makecell[c]{\textbf{JSQ}} & \makecell[c]{\textbf{XL}-\\\textbf{Sum}} & \makecell[c]{\textbf{WMT}\\\textbf{E$\to$J}} & \makecell[c]{\textbf{WMT}\\\textbf{J$\to$E}} & \makecell[c]{\textbf{OB}\\\textbf{QA}} & \makecell[c]{\textbf{TQA}} & \makecell[c]{\textbf{HS}} & \makecell[c]{\textbf{SQ}\\\textbf{v2}} & \makecell[c]{\textbf{XW}-\\\textbf{EN}} & \makecell[c]{\textbf{BBH}} & \textbf{Avg} \\

\midrule
%\multicolumn{6}{l}{\textbf{\textit{3.7Bベースでのスケーリングの実験: }}}  \\
1 & Dense 3.7B & - &  3.7B / 3.7B & & 1,000B& 2.70 & & 44.5 & 47.2 & 78.8 & 12.8 & 21.4 & 15.4 & 25.0 & 33.8 & 47.3 & 23.7 & 85.9 & 28.7 & 38.7 \\
2 & MoE 8$\times$3.7B & FS & 5.9B / 18B & & 500B & 1.98 & & \textbf{53.5}	&50.8	&69.6	&10.4	&20.6	&13.9	&29.0	&45.8	&51.1	&21.1	&87.1	&28.1	&40.1 \\
\rowcolor{verylightgray} 3 & MoE 8$\times$3.7B  & DU ($r$=0.5) &5.9B / 18B & & 500B & 1.98 & &
 47.5&	\textbf{57.0}	&82.2&	\textbf{16.3}	&\textbf{25.0}&	\textbf{19.0}&	31.2&	\textbf{53.6}	&\textbf{54.4}&	26.3&	\textbf{88.5}&	32.2&	44.4\\
%\multicolumn{6}{l}{\textbf{\textit{3.7Bベースでのスケーリングの実験のベースライン: }}}  \\
4 & Dense 13B& - &13B / 13B & & 805B & 7.43 & &47.6	&58.3	&\textbf{85.2}	&14.1	&24.6	&18.3	&\textbf{31.4}	&48.6	&53.1	&\textbf{29.3}	&88.3	&\textbf{35.2}	&\textbf{44.5} \\

5 & Dense 3.7B & -&3.7B / 3.7B & & 2,072B &5.58 & &42.3	&53.2	&80.4	&14.3	&22.6	&15.9	&28.2	&42.2	&50.6	&25.8	&87.3	&30.9	&41.1 \\

\bottomrule
\end{tabular}
\end{adjustbox}
%\raggedright
%\small
%$^1$ Checkpoint prior to upcycling to MoE. \\
\end{table}


% \subsection{Scaling to 8$\times$3.7 B}
\subsection{Scaling to \texorpdfstring{8$\times$3.7}{8x3.7}B}

\label{sec:resuilts-scaling}
To further evaluate the effectiveness of \methodname{} in larger-scale settings and to build a practical MoE model, we conducted experiments with an 8$\times$3.7B configuration. Due to computational resource constraints, experiments under the 8$\times$3.7B setting were limited to training from scratch and \methodname{} with \( r = 0.5 \).

The rightmost column of Figure~\ref{fig:combined_train_loss_and_score} illustrates the learning curves under this configuration. Similar to the 8$\times$152M and 8$\times$1.5B settings, \methodname{} significantly outperforms training from scratch. There is an initial gain in performance due to the improved initialization, and expert diversification allows the training to progress as efficiently as in the case of training from scratch, ensuring that \methodname{} never gets overtaken.


Table~\ref{tab:detailed-ja-en-comparison_method_scaling} compares the models' final downstream task performance. \diff{Model numbers refer to the leftmost column of this table.}
Model 1 is a dense model used as a base model for the Upcycling.
Models 2 and 3 are MoEs built using \NUname{} and \methodname{}, respectively, demonstrating the superiority of \methodname{}.
In addition, two different baseline dense models, Models 4 and 5, are included in the table. Model 4 is a 13B dense model. Our 8$\times$3.7B MoE architecture has fewer active parameters than this 13B model, leading to lower training and inference costs. Nevertheless, the 8$\times$3.7B MoE model using \methodname{} achieves better performance upon completion of training. Model 5 is a 3.7B dense model trained with 2.1T tokens. The fact that our 8$\times$3.7B MoE model with \methodname{} surpasses this dense model indicates that rather than continuously investing resources into training dense models, it might be a superior option to convert them to MoE models through \methodname{} and continue training at a certain point in the process.


\subsection{Analysis 1: Re-initializaiton Ratio}
\label{sec:exp:ratio}

We conducted a study to investigate the impact of the re-initialization ratio $r$ in \methodname{}.
Figure \ref{fig:initialization_ratio_comparison} illustrates the effects of different re-initialization rates 0.0 (\NUname{}), 0.1, 0.25, 0.5, 0.75, and 1.0 on models of sizes 8×152M and 8×1.5B. Each model was trained up to 150B tokens, during which we monitored the training loss and the progression of the average downstream task scores.

The experimental results revealed similar trends across both model sizes. In terms of long-term performance, a re-initialization ratio of 0.5 yielded the best results for both models, maintaining superiority in both training loss and average task scores.
An interesting pattern emerged regarding the influence of the re-initialization ratio. With lower re-initialization rates, particularly at 0.0 (\NUname{}), the models struggled to significantly improve beyond the performance of the original pre-trained models. While re-initialization rates of 0.1 and 0.25 showed promising performance in the early stages of training, they were eventually surpassed by the 0.5 re-initialization rate as training progressed.
These observations suggest that increasing the re-initialization ratio helps the models escape local optima, enabling more effective learning. However, excessively high re-initialization rates of 0.75 or 1.0 appeared to hinder the effective knowledge transfer from the pre-trained dense models.
This phenomenon highlights an important trade-off concerning the MoE initialization: a balance must be struck between knowledge transfer and effective expert specialization.
\methodname{} with \( r = 0.5 \) is a robust and practical method that ideally balances these two aspects.

% Regarding the relationship between task scores and training loss, although some variability was observed in the task scores, the overall trend confirmed that as the training loss decreased (indicating training progression), the task scores improved.







\begin{figure}[t]
\centering
\includegraphics[width=\textwidth]
% {images/initialization_ratio_comparison.pdf}
{images/initialization_ratio_comparison_horizontal_spectral.pdf}
\vskip -5pt 
\caption{\textbf{Impact of re-initialization ratio $r$.} The training loss and downstream task score over the total number of tokens processed during training on 8×152M (left two figures) and 8×1.5B (right two figures) settings are illustrated.
Even with different $r$ values, \methodname{} robustly outperforms \NUname{}, and 0.5 appears to be the most effective ratio.}
%Training Loss and averaged Downstream Task Score over the total numbers of tokens processed during training}

\label{fig:initialization_ratio_comparison}
\end{figure}



\subsection{Analysis 2: Expert Specialization}
\label{sec:exp:diversity}


We analyze expert routing patterns to examine how \methodname{} facilitates expert specialization. We apply the methodologies of \citet{jiang2024mixtralexperts} and \citet{muennighoff2024olmoeopenmixtureofexpertslanguage} to 8×1.5B MoE models trained with different methods. This analysis investigates how data from different domains is routed to various experts. As input data from different domains, we use the validation sets from Japanese and English Wikipedia; the validation set of the Japanese MC4 dataset (as split by the authors; see \citealt{llmjp2024llmjpcrossorganizationalprojectresearch}), originally introduced by \citet{2019t5}; The Stack \citep{kocetkov2023the}; and the English C4 dataset \citep{muennighoff2024olmoeopenmixtureofexpertslanguage}.



\begin{figure}[t] \centering \includegraphics[width=\textwidth]{images/expert_routing_probability_comparison_legend_bottom.pdf} \vskip -3pt 
\caption{
\textbf{Comparison of expert routing patterns across different MoE construction methods.} 
\methodname{} exhibits more balanced expert utilization than \NUname{}.
Results shown for layers 0 (first), 8, 16, and 23 (last); see Appendix~\ref{subsec:detailed_routing_analysis} for results on all layers.
%\textbf{Expert routing patterns} at layers 0 (first), 8, 16, and 23 (last) on different datasets.
} \label{fig:routing} \end{figure}

 


In Figure~\ref{fig:routing}, we observe that \NUname{} \diff{with global load balancing} results in a highly imbalanced routing pattern, \diff{where} the majority of experts were underutilized or not utilized at all, with only two experts being always selected across all layers. \diff{While layer-wise load balancing mitigate such expert collapse, we found no significant differences in the training loss trajectories or model performance between these two strategies (see Appendix~\ref{subsec:detailed_load_balance_comparison}).} In contrast, both the model trained from scratch and the one enhanced with \methodname{} (with $r=0.5$) exhibit \diff{domain-specialized routing patterns regardless of the load balancing strategy.} The routing patterns reveal that certain experts specialize in processing specific types of data, such as Japanese text, English text, or code snippets\diff{, as} evident from the distinct expert selection probabilities corresponding to each dataset.

These findings suggest that \methodname{} promotes effective expert specialization \diff{independently of the load balancing strategy, which likely contributes to the improved performance observed in our experiments. For detailed routing patterns across all 24 layers and further analysis of load balancing strategies, see Appendix~\ref{subsec:detailed_routing_analysis} and \ref{subsec:detailed_load_balance_comparison}.}