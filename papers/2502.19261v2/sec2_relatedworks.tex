\section{Related Work}



\subsection{Mixture of Experts}
\label{sec:related_works:moe}

%
%
The concept of Mixture of Experts (MoE) was introduced about three decades ago~\citep{classic_moe_1,classic_moe_2}. Since then, the idea of using sparsely-gated MoE as a building block within neural network layers~\citep{moe_layer_iclr14,shazeer2017} has evolved and has been incorporated into transformer-based language models~\citep{lepikhin2021gshard, Fedus2021SwitchTS}. For a detailed overview of MoE, please refer to recent survey papers~\citep{moe_survey}.
Sparsely-gated MoE is currently the most common approach for building large-scale sparsely-activated models.
In this paper, we focus on sparsely-gated MoE (also referred to as sparse MoE or sparsely-activated MoE), and unless otherwise specified, the term MoE refers to it.




There are various designs of MoE layers and ways to integrate them into transformer-based LLMs. For example, in addition to the standard token-centric routing, expert-centric routing has also been proposed~\citep{expert_routing}. To incorporate common knowledge, it has been suggested to introduce shared experts that are always activated~\citep{dai-etal-2024-deepseekmoe}. To simplify the discussion, %unless otherwise specified, 
we assume the most standard top-$k$ token choice routing as the MoE layer and a decoder-only transformer-based LLM that uses MoE layers only in the FFNs as the MoE model. 
%This is because 
These are common design choices for recent MoE-based LLMs, such as Mixtral~\citep{jiang2024mixtralexperts}, Skywork-MoE~\citep{wei2024skyworkmoedeepdivetraining}, Phi-3.5-MoE~\citep{abdin2024phi3technicalreporthighly}, and Grok-1\footnote{\url{https://x.ai/blog/grok-os}}. 
% \citep{xai2024grok}
% \footnote{\url{https://x.ai/blog/grok-os}}. 
%
Specifically, these models use 8 experts (Mixtral and Grok-1) or 16 experts (Skywork and Phi-3.5-MoE), with the top-2 experts being activated per input token. Our experiments also use top-2 routing with 8 experts per layer, as this setup aligns with those practical configurations.
These facts indicate that \methodname{} can be applied to most variations of MoE models.
%
See Section~\ref{sec:methods:preliminaries} for technical details of MoE.





\subsection{MoE Model Initialization}
As with conventional neural networks, MoE models can be initialized randomly and trained from scratch. However, to reduce training costs, leveraging existing pre-trained dense models has become a standard approach. Below, we introduce a few methods for achieving this.


Upcycling~\citep{komatsuzaki2023sparse} leverages the weights of a pre-trained dense model for initializing an MoE model by initializing the experts in the MoE layer as replicas of the FFN layers in the dense model.
The main advantage of Upcycling is that it boosts the model's initial performance.  However, as our experiments show, MoE models initialized with Upcycling tend to have a much slower convergence, leading to suboptimal performance when trained for longer durations.



Branch-Train-MiX (BTX) \citep{sukhbaatar2024branchtrainmix} is a technique where a pre-trained dense model is replicated and fine-tuned on different datasets to produce multiple distinct expert dense models. These experts are then integrated into an MoE model, followed by additional training to optimize the routers. While this method appears to ensure expert specialization by design, \cite{jiang2024mixtralexperts} has highlighted that the diversity achieved in this way differs from that required for MoE layer experts, leading to suboptimal performance as a result. Our experiments also show that BTX suffers from suboptimal convergence similar to those observed in Upcycling.






Concurrent with our work, the Qwen2 technical report ~\citep{yang2024qwen2technicalreport} briefly suggests the use of a methodology possibly related to \methodname{} in training Qwen2-MoE. Due to the report's brevity and ambiguity, it is unclear if their method exactly matches ours. 
Our paper offers a valuable technical contribution even if the methods are similar. 
The potential application of \methodname{} in an advanced, industry-developed model like Qwen2-MoE that underscores the importance of further open investigation into this approach. We acknowledge the Qwen2 authors for sharing insights through their technical report.

