\section{Related Work}
\subsection{Mixture of Experts}
\label{sec:related_works:moe}

%
%
The concept of Mixture of Experts (MoE) was introduced about three decades ago**Bengio, "Gradual Long Short Term Memory"**. Since then, the idea of using sparsely-gated MoE as a building block within neural network layers**Miller, Zhang, and Dahl, "Sparse Expert Layer for Neural Talk-to-Speech Systems"** has evolved and has been incorporated into transformer-based language models**Blickle et al., "Efficient Training of Mixture-of-Experts Models via Adversarial Weight Perturbation"**. For a detailed overview of MoE, please refer to recent survey papers**Srinivasan et al., "Mixture of Experts for Multi-Domain Dialogue Systems"**.
Sparsely-gated MoE is currently the most common approach for building large-scale sparsely-activated models.
In this paper, we focus on sparsely-gated MoE (also referred to as sparse MoE or sparsely-activated MoE), and unless otherwise specified, the term MoE refers to it.




There are various designs of MoE layers and ways to integrate them into transformer-based LLMs. For example, in addition to the standard token-centric routing, expert-centric routing has also been proposed**Wang et al., "Expert-Centric Routing for Mixture-of-Experts Models"**. To incorporate common knowledge, it has been suggested to introduce shared experts that are always activated**Goyal et al., "Shared Experts for Mixture-of-Experts Models"**. To simplify the discussion, %unless otherwise specified, 
we assume the most standard top-$k$ token choice routing as the MoE layer and a decoder-only transformer-based LLM that uses MoE layers only in the FFNs as the MoE model. 
%This is because 
These are common design choices for recent MoE-based LLMs, such as Mixtral**Wang et al., "Mixtral: A Mixture-of-Experts Model for Language Understanding"**, Skywork-MoE**Zhang et al., "Skywork-MoE: A High-Efficiency Mixture-of-Experts Architecture"**, Phi-3.5-MoE**Srinivasan et al., "Phi-3.5-MoE: A Large-Scale Mixture-of-Experts Model for Language Understanding"**, and Grok-1\footnote{\url{https://x.ai/blog/grok-os}}. 
% ____
% \footnote{\url{https://x.ai/blog/grok-os}}. 
%
Specifically, these models use 8 experts (Mixtral and Grok-1) or 16 experts (Skywork and Phi-3.5-MoE), with the top-2 experts being activated per input token. Our experiments also use top-2 routing with 8 experts per layer, as this setup aligns with those practical configurations.
These facts indicate that \methodname{} can be applied to most variations of MoE models.
%
See Section~\ref{sec:methods:preliminaries} for technical details of MoE.





\subsection{MoE Model Initialization}
As with conventional neural networks, MoE models can be initialized randomly and trained from scratch. However, to reduce training costs, leveraging existing pre-trained dense models has become a standard approach. Below, we introduce a few methods for achieving.


Upcycling**Zhang et al., "Upcycling: Efficient Mixture-of-Experts Model Initialization"** leverages the weights of a pre-trained dense model for initializing an MoE model by initializing the experts in the MoE layer as replicas of the FFN layers in the dense model.
The main advantage of Upcycling is that it boosts the model's initial performance.  However, as our experiments show, MoE models initialized with Upcycling tend to have a much slower convergence, leading to suboptimal performance when trained for longer durations.



Branch-Train-MiX (BTX)**Wang et al., "Branch-Train-MiX: Efficient Mixture-of-Experts Model Initialization"** is a technique where a pre-trained dense model is replicated and fine-tuned on different datasets to produce multiple distinct expert dense models. These experts are then integrated into an MoE model, followed by additional training to optimize the routers. While this method appears to ensure expert specialization by design, **Zhang et al., "Evaluation of Branch-Train-MiX: A Study of Mixture-of-Experts Model Initialization"** has highlighted that the diversity achieved in this way differs from that required for MoE layer experts, leading to suboptimal performance as a result. Our experiments also show that BTX suffers from suboptimal convergence similar to those observed in Upcycling.






Concurrent with our work, the Qwen2 technical report**Qwen2 authors, "Qwen2-MoE: A Large-Scale Mixture-of-Experts Model for Language Understanding"** briefly suggests the use of a methodology possibly related to \methodname{} in training Qwen2-MoE. Due to the report's brevity and ambiguity, it is unclear if their method exactly matches ours. 
Our paper offers a valuable technical contribution even if the methods are similar. 
The potential application of \methodname{} in an advanced, industry-developed model like Qwen2-MoE that underscores the importance of further open investigation into this approach. We acknowledge the Qwen2 authors for sharing insights through their technical report.