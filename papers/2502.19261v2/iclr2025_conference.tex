
\documentclass{article} % For LaTeX2e
\usepackage[table]{xcolor}
\usepackage{iclr2025_conference,times}
%\usepackage{xeCJK}
%\setCJKmainfont{IPAexMincho} % 日本語フォント設定
%\setCJKmainfont[Scale=0.9]{IPAexMincho}

% TODO: 日本語を書くために使うパッケージ。最後に消す。 pdflatex で動かす用
\usepackage[whole]{bxcjkjatype}

% Optional math commands from https://github.com/goodfeli/dlbook_notation.
\input{math_commands.tex}

\usepackage{hyperref}
\usepackage{url}
\usepackage{pdfpages}
\usepackage{graphicx}
\usepackage{float}
\usepackage{booktabs}
\usepackage{multirow}
\usepackage{makecell}
\newcommand{\pkg}[1]{\textsf{#1}}
\usepackage{adjustbox}
\usepackage{threeparttable}
\definecolor{light-gray}{gray}{0.9}
\usepackage{algorithm}
\definecolor{verylightgray}{rgb}{0.93, 0.93, 0.93}

\newcommand{\todo}[1]{\textcolor{orange}{#1}}

\newcommand{\Taishi}[1]{\textcolor{red}{\textbf{Taishi: }#1}}
\newcommand{\takuya}[1]{\textcolor{blue}{\textbf{Takuya: }{#1}}}
\newcommand{\odashi}[1]{\textcolor{purple}{\textbf{Oda: }{#1}}}
\newcommand{\methodname}{Drop-Upcycling}
\newcommand{\NUname}{na\"{i}ve Upcycling} 
\newcommand{\RNUname}{Random Noise Upcycling} 
\newcommand{\diff}[1]{#1}

\usepackage{xspace}
\newcommand{\huggingface}{\raisebox{-1.5pt}{\includegraphics[height=1.05em]{figures/hf-logo.pdf}}\xspace}
\newcommand{\github}{\raisebox{-1.5pt}{\includegraphics[height=1.05em]{figures/github-logo.pdf}}\xspace}
\newcommand{\wandb}{\raisebox{-1.5pt}{\includegraphics[height=1.05em]{figures/wandb-logo.png}}\xspace}
\newcommand{\gitlab}{\raisebox{-1.5pt}{\includegraphics[height=1.05em]{figures/gitlab-logo.png}}\xspace}




\title{
\methodname{}: Training Sparse Mixture of Experts with Partial Re-initialization
}


 
\author{
Taishi Nakamura$^{1,2,3}$, 
Takuya Akiba$^{2}$, 
Kazuki Fujii$^{1}$,
Yusuke Oda$^{3}$, \\
\,\,\textbf{Rio Yokota}$^{1,3}$, 
\textbf{Jun Suzuki}$^{4,5,3}$
\\
$^1$Institute of Science Tokyo,
$^2$Sakana AI,
$^3$NII LLMC,
$^4$Tohoku University,
$^5$RIKEN\\
\texttt{taishi@rio.scrc.iir.isct.ac.jp},
% \texttt{takiba@sakana.ai},\\
% \texttt{kazuki.fujii@rio.scrc.iir.isct.ac.jp },
% \texttt{odashi@nii.ac.jp},\\
% \texttt{rioyokota@rio.scrc.iir.isct.ac.jp},
\texttt{jun.suzuki@tohoku.ac.jp}
}

% The \author macro works with any number of authors. There are two commands
% used to separate the names and addresses of multiple authors: \And and \AND.
%
% Using \And between authors leaves it to \LaTeX{} to determine where to break
% the lines. Using \AND forces a linebreak at that point. So, if \LaTeX{}
% puts 3 of 4 authors names on the first line, and the last on the second
% line, try using \AND instead of \And before the third author name.

\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}



\iclrfinalcopy % Uncomment for camera-ready version, but NOT for submission.
\begin{document}

\maketitle
\begin{abstract}
The Mixture of Experts (MoE) architecture reduces the training and inference cost significantly compared to a dense model of equivalent capacity. Upcycling is an approach that initializes and trains an MoE model using a pre-trained dense model. While upcycling leads to initial performance gains, the training progresses slower than when trained from scratch, leading to suboptimal performance in the long term. We propose \emph{\methodname{}} -- a method that effectively addresses this problem. \methodname{} combines two seemingly contradictory approaches: utilizing the knowledge of pre-trained dense models while statistically re-initializing some parts of the weights. This approach strategically promotes expert specialization, significantly enhancing the MoE model's efficiency in knowledge acquisition. 
Extensive large-scale experiments demonstrate that \methodname{} significantly outperforms previous MoE construction methods in the long term, specifically when training on hundreds of billions of tokens or more.
As a result, our MoE model with 5.9B active parameters achieves comparable performance to a 13B dense model in the same model family, while requiring approximately 1/4 of the training FLOPs.
% This research offers new insights for efficient LLM development and understanding of MoE models.
All experimental resources, including source code, training data, model checkpoints and logs, are publicly available to promote reproducibility and future research on MoE.

\begin{center}
\begin{tabular}{rcl} % This defines 3 columns: right-aligned, center-aligned, left-aligned
\huggingface & \textbf{Weights} & \href{https://huggingface.co/collections/llm-jp/drop-upcycling-674dc5be7bbb45e12a476b80}{\path{huggingface.co/collections/llm-jp/}}\\
& & \href{https://huggingface.co/collections/llm-jp/drop-upcycling-674dc5be7bbb45e12a476b80}{\path{drop-upcycling-674dc5be7bbb45e12a476b80}}\\[0.2em]
\gitlab & \textbf{Data} & \href{https://gitlab.llm-jp.nii.ac.jp/datasets/llm-jp-corpus-v3}{\path{gitlab.llm-jp.nii.ac.jp/}}\\
& & \href{https://gitlab.llm-jp.nii.ac.jp/datasets/llm-jp-corpus-v3}{\path{datasets/llm-jp-corpus-v3}}\\[0.2em]
\github & \textbf{Code} & \href{https://github.com/Taishi-N324/Drop-Upcycling}{\path{github.com/Taishi-N324/Drop-Upcycling}}\\[0.2em]
\wandb & \textbf{Logs} & \href{https://wandb.ai/taishi-nakamura/Drop-Upcycling}{\path{wandb.ai/taishi-nakamura/Drop-Upcycling}}
\end{tabular}
\end{center}
\end{abstract}

\input{sec1_introduction}

\input{sec2_relatedworks.tex}

\input{sec3_methods}

\input{sec4_experiments_setup}

\input{sec5_results}

\input{sec6_conclusion}

\input{sec7_ack}


\bibliography{iclr2025_conference}
\bibliographystyle{iclr2025_conference}

\appendix

\input{appendix}


\end{document}

