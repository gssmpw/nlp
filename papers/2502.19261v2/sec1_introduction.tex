\section{Introduction}

%
% LLM's training and inference are too expensive
%
Large-scale language models (LLMs) have achieved remarkable results across various natural language processing applications \citep{NEURIPS2020_1457c0d6,wei2022chain,ouyang2022training,openai2024gpt4technicalreport}. This success largely depends on scaling the number of model parameters, the amount of training data, and computational resources \citep{kaplan2020scalinglawsneurallanguage,NEURIPS2022_c1e2faff}, which leads to substantial training and inference costs of LLMs. Building and deploying high-performance models also require enormous resources, posing a significant barrier for many researchers and practitioners.

%
% MoE
%
The \emph{Mixture of Experts} (MoE) architecture has emerged as a promising approach to address the escalating resource demands of LLMs. MoE introduces multiple experts into some parts of the network, but only a subset is activated at any given time, allowing the model to achieve superior performance with reduced training and inference costs \citep{shazeer2017,lepikhin2021gshard,Fedus2021SwitchTS}. In fact, cutting-edge industry models like Gemini 1.5 \citep{geminiteam2024gemini15unlockingmultimodal} and GPT-4 (based on unofficial reports) \citep{openai2024gpt4technicalreport} have adopted MoE, suggesting its effectiveness. 


%
% MoE Challenge
%
We refer to transformer-based LLMs without MoE as \emph{dense models} and those incorporating MoE as \emph{MoE models}.
Upcycling~\citep{komatsuzaki2023sparse} is an approach that initializes and trains an MoE model using a pre-trained dense model, which aims to transfer learned knowledge for better initial performance.
However, \NUname{} copies the feedforward network (FFN) layers during initialization, which makes it difficult to achieve expert specialization.
This disadvantage prevents effective utilization of the MoE models' full capacity, resulting in slower convergence over long training periods.
Thus, there exists a trade-off between the short-term cost savings from knowledge transfer and the long-term convergence efficiency through expert specialization.



In this paper, we propose \emph{\methodname{}} -- a method that effectively addresses this trade-off, as briefly illustrated in Figure \ref{fig:drop_upcycling}. \methodname{} works by selectively re-initializing the parameters of the expert FFNs when expanding a dense model into an MoE model. The method is carefully designed to promote expert specialization while preserving the knowledge of pre-trained dense models. Specifically, common indices are randomly sampled along the intermediate dimension of the FFNs, and the weights are dropped either column-wise or row-wise, depending on the weight matrix types. The dropped parameters are then re-initialized using the statistics of those weights.



%
% Experimental results
%
Extensive large-scale experiments demonstrate that \methodname{} nearly resolves the trade-off between the two aforementioned challenges
and significantly outperforms previous MoE model construction methods such as training from scratch and \NUname{}.
By leveraging pre-trained dense models, \methodname{} can start training from a better initial state than training from scratch, reducing training costs.
On the other hand, \methodname{} avoids the convergence slowdowns observed with \NUname{}.
Specifically, in our extensive long-term training experiments, \methodname{} maintained a learning curve slope similar to that of training from scratch, consistently staying ahead.
This success is attributed to effective expert specialization.
As a result, we constructed an MoE model with 5.9B active parameters that performs on par with a 13B dense model from the same model family, while requiring only approximately 1/4 of the training FLOPs.

\begin{figure}[t]
    \centering
    \includegraphics[width=\textwidth]{images/overview.pdf}
\vskip -8pt 
\caption{\textbf{Overview of the \methodname{} method.} The key difference from the na\"{i}ve Upcycling is Diversity re-initialization, introduced in Section \ref{sec:method}.}
    \label{fig:drop_upcycling}
\end{figure}

%
% Fully open
%
This research is fully open, transparent, and accessible to all.
With over 200,000 GPU hours of experimental results, conducted on NVIDIA H100 GPUs, all training data, source code, configuration files, model checkpoints, and training logs used in this study are publicly available. By providing this comprehensive resource, we aim to promote further advancements in this line of research.


%
%
%
Our technical contributions are summarized as follows:
\begin{itemize}
\item We propose \methodname{}, a novel method for constructing MoE models that effectively balance knowledge transfer and expert specialization by selectively re-initializing parameters of expert FFNs when expanding a dense model into an MoE model.

\item Extensive large-scale experiments demonstrate that \methodname{} consistently outperforms previous MoE construction methods in long-term training scenarios.

\item All aspects of this research are publicly available. %, including training data, source code, configuration files, model checkpoints, and training logs. 
This includes the MoE model with 5.9B active parameters that performs comparably to a 13B dense model in the same model family while requiring only about 1/4 of the training FLOPs.

\end{itemize}