[
  {
    "index": 0,
    "papers": [
      {
        "key": "classic_moe_1",
        "author": "Robert A. Jacobs and\nMichael I. Jordan and\nSteven J. Nowlan and\nGeoffrey E. Hinton",
        "title": "Adaptive Mixtures of Local Experts"
      },
      {
        "key": "classic_moe_2",
        "author": "Michael I. Jordan and\nRobert A. Jacobs",
        "title": "Hierarchical Mixtures of Experts and the {EM} Algorithm"
      }
    ]
  },
  {
    "index": 1,
    "papers": [
      {
        "key": "moe_layer_iclr14",
        "author": "David Eigen and\nMarc'Aurelio Ranzato and\nIlya Sutskever",
        "title": "Learning Factored Representations in a Deep Mixture of Experts"
      },
      {
        "key": "shazeer2017",
        "author": "Noam Shazeer and *Azalia Mirhoseini and *Krzysztof Maziarz and Andy Davis and Quoc Le and Geoffrey Hinton and Jeff Dean",
        "title": " Outrageously Large Neural Networks: The Sparsely-Gated Mixture-of-Experts Layer"
      }
    ]
  },
  {
    "index": 2,
    "papers": [
      {
        "key": "lepikhin2021gshard",
        "author": "Dmitry Lepikhin and HyoukJoong Lee and Yuanzhong Xu and Dehao Chen and Orhan Firat and Yanping Huang and Maxim Krikun and Noam Shazeer and Zhifeng Chen",
        "title": "{\\{}GS{\\}}hard: Scaling Giant Models with Conditional Computation and Automatic Sharding"
      },
      {
        "key": "Fedus2021SwitchTS",
        "author": "William Fedus and Barret Zoph and Noam M. Shazeer",
        "title": "Switch Transformers: Scaling to Trillion Parameter Models with Simple and Efficient Sparsity"
      }
    ]
  },
  {
    "index": 3,
    "papers": [
      {
        "key": "moe_survey",
        "author": "Weilin Cai and\nJuyong Jiang and\nFan Wang and\nJing Tang and\nSunghun Kim and\nJiayi Huang",
        "title": "A Survey on Mixture of Experts"
      }
    ]
  },
  {
    "index": 4,
    "papers": [
      {
        "key": "expert_routing",
        "author": "Yanqi Zhou and\nTao Lei and\nHanxiao Liu and\nNan Du and\nYanping Huang and\nVincent Y. Zhao and\nAndrew M. Dai and\nZhifeng Chen and\nQuoc V. Le and\nJames Laudon",
        "title": "Mixture-of-Experts with Expert Choice Routing"
      }
    ]
  },
  {
    "index": 5,
    "papers": [
      {
        "key": "dai-etal-2024-deepseekmoe",
        "author": "Dai, Damai  and\nDeng, Chengqi  and\nZhao, Chenggang  and\nXu, R.x.  and\nGao, Huazuo  and\nChen, Deli  and\nLi, Jiashi  and\nZeng, Wangding  and\nYu, Xingkai  and\nWu, Y. et al.",
        "title": "{D}eep{S}eek{M}o{E}: Towards Ultimate Expert Specialization in Mixture-of-Experts Language Models"
      }
    ]
  },
  {
    "index": 6,
    "papers": [
      {
        "key": "jiang2024mixtralexperts",
        "author": "Albert Q. Jiang and Alexandre Sablayrolles and Antoine Roux and Arthur Mensch and Blanche Savary and Chris Bamford and Devendra Singh Chaplot and Diego de las Casas and Emma Bou Hanna and Florian Bressand et al.",
        "title": "Mixtral of Experts"
      }
    ]
  },
  {
    "index": 7,
    "papers": [
      {
        "key": "wei2024skyworkmoedeepdivetraining",
        "author": "Tianwen Wei and Bo Zhu and Liang Zhao and Cheng Cheng and Biye Li and Weiwei L\u00fc and Peng Cheng and Jianhao Zhang and Xiaoyu Zhang and Liang Zeng et al.",
        "title": "Skywork-MoE: A Deep Dive into Training Techniques for Mixture-of-Experts Language Models"
      }
    ]
  },
  {
    "index": 8,
    "papers": [
      {
        "key": "abdin2024phi3technicalreporthighly",
        "author": "Marah Abdin and Jyoti Aneja and Hany Awadalla and Ahmed Awadallah and Ammar Ahmad Awan and Nguyen Bach and Amit Bahree and Arash Bakhtiari and Jianmin Bao and Harkirat Behl et al.",
        "title": "Phi-3 Technical Report: A Highly Capable Language Model Locally on Your Phone"
      }
    ]
  },
  {
    "index": 9,
    "papers": [
      {
        "key": "xai2024grok",
        "author": "{xAI}",
        "title": "Open Release of Grok-1"
      }
    ]
  },
  {
    "index": 10,
    "papers": [
      {
        "key": "komatsuzaki2023sparse",
        "author": "Aran Komatsuzaki and Joan Puigcerver and James Lee-Thorp and Carlos Riquelme Ruiz and Basil Mustafa and Joshua Ainslie and Yi Tay and Mostafa Dehghani and Neil Houlsby",
        "title": "Sparse Upcycling: Training Mixture-of-Experts from Dense Checkpoints"
      }
    ]
  },
  {
    "index": 11,
    "papers": [
      {
        "key": "sukhbaatar2024branchtrainmix",
        "author": "Sainbayar Sukhbaatar and Olga Golovneva and Vasu Sharma and Hu Xu and Xi Victoria Lin and Baptiste Roziere and Jacob Kahn and Shang-Wen Li and Wen-tau Yih and Jason E Weston and Xian Li",
        "title": "Branch-Train-MiX: Mixing Expert {LLM}s into a Mixture-of-Experts {LLM}"
      }
    ]
  },
  {
    "index": 12,
    "papers": [
      {
        "key": "jiang2024mixtralexperts",
        "author": "Albert Q. Jiang and Alexandre Sablayrolles and Antoine Roux and Arthur Mensch and Blanche Savary and Chris Bamford and Devendra Singh Chaplot and Diego de las Casas and Emma Bou Hanna and Florian Bressand et al.",
        "title": "Mixtral of Experts"
      }
    ]
  },
  {
    "index": 13,
    "papers": [
      {
        "key": "yang2024qwen2technicalreport",
        "author": "An Yang and Baosong Yang and Binyuan Hui and Bo Zheng and Bowen Yu and Chang Zhou and Chengpeng Li and Chengyuan Li and Dayiheng Liu and Fei Huang et al.",
        "title": "Qwen2 Technical Report"
      }
    ]
  }
]