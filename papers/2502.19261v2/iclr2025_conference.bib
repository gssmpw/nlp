@incollection{Bengio+chapter2007,
author = {Bengio, Yoshua and LeCun, Yann},
booktitle = {Large Scale Kernel Machines},
publisher = {MIT Press},
title = {Scaling Learning Algorithms Towards {AI}},
year = {2007}
}

@article{Hinton06,
author = {Hinton, Geoffrey E. and Osindero, Simon and Teh, Yee Whye},
journal = {Neural Computation},
pages = {1527--1554},
title = {A Fast Learning Algorithm for Deep Belief Nets},
volume = {18},
year = {2006}
}

@book{goodfellow2016deep,
title={Deep learning},
author={Goodfellow, Ian and Bengio, Yoshua and Courville, Aaron and Bengio, Yoshua},
volume={1},
year={2016},
publisher={MIT Press}
}

@inproceedings{Fujii:COLM2024,
   title={Continual Pre-Training for Cross-Lingual LLM Adaptation:
Enhancing Japanese Language Capabilities},
   author={Kazuki Fujii and Taishi Nakamura and Mengsay Loem and Hiroki
Iida and Masanari Ohi and Kakeru Hattori and Hirai Shota and Sakae
Mizuki and Rio Yokota and Naoaki Okazaki},
   booktitle="Conference on Language Modeling",
   series={COLM},
   pages="(to appear)",
   year="2024",
}

@misc{nakamura2024auroramopensourcemultilingual,
      title={Aurora-M: The First Open Source Multilingual Language Model Red-teamed according to the U.S. Executive Order}, 
      author={Taishi Nakamura and Mayank Mishra and Simone Tedeschi and Yekun Chai and Jason T Stillerman and Felix Friedrich and Prateek Yadav and Tanmay Laud and Vu Minh Chien and Terry Yue Zhuo et al.},
      year={2024},
      eprint={2404.00399},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
}

@misc{jiang2024mixtralexperts,
      title={Mixtral of Experts}, 
      author={Albert Q. Jiang and Alexandre Sablayrolles and Antoine Roux and Arthur Mensch and Blanche Savary and Chris Bamford and Devendra Singh Chaplot and Diego de las Casas and Emma Bou Hanna and Florian Bressand et al.},
      year={2024},
      eprint={2401.04088},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      howpublished={arXiv preprint arXiv:2401.04088},
}

@inproceedings{soldaini-etal-2024-dolma,
    title = "Dolma: an Open Corpus of Three Trillion Tokens for Language Model Pretraining Research",
    author = "Soldaini, Luca  and
      Kinney, Rodney  and
      Bhagia, Akshita  and
      Schwenk, Dustin  and
      Atkinson, David  and
      Authur, Russell  and
      Bogin, Ben  and
      Chandu, Khyathi  and
      Dumas, Jennifer  and
      Elazar, Yanai  and
      et al.",
    booktitle = "Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    year = "2024",
    publisher = "Association for Computational Linguistics",
    pages = "15725--15788",
}

@misc{llmjp2024llmjpcrossorganizationalprojectresearch,
      title={LLM-jp: A Cross-organizational Project for the Research and Development of Fully Open Japanese LLMs}, 
      author={LLM-jp},
      year={2024},
      eprint={2407.03963},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      howpublished={arXiv preprint arXiv:2407.03963},
}

@article{Beltagy2020Longformer,
  title={Longformer: The Long-Document Transformer},
  author={Iz Beltagy and Matthew E. Peters and Arman Cohan},
  journal={arXiv:2004.05150},
  year={2020},
}

@misc{touvron2023llamaopenefficientfoundation,
      title={LLaMA: Open and Efficient Foundation Language Models}, 
      author={Hugo Touvron and Thibaut Lavril and Gautier Izacard and Xavier Martinet and Marie-Anne Lachaux and Timothée Lacroix and Baptiste Rozière and Naman Goyal and Eric Hambro and Faisal Azhar et al.},
      year={2023},
      eprint={2302.13971},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      howpublished={arXiv preprint arXiv:2302.13971},
}

@inproceedings{dao2023flashattention2,
  title={Flash{A}ttention-2: Faster Attention with Better Parallelism and Work Partitioning},
  author={Dao, Tri},
  booktitle={International Conference on Learning Representations},
  year={2024}
}

@inproceedings{
komatsuzaki2023sparse,
title={Sparse Upcycling: Training Mixture-of-Experts from Dense Checkpoints},
author={Aran Komatsuzaki and Joan Puigcerver and James Lee-Thorp and Carlos Riquelme Ruiz and Basil Mustafa and Joshua Ainslie and Yi Tay and Mostafa Dehghani and Neil Houlsby},
booktitle={International Conference on Learning Representations},
year={2023},
}

@inproceedings{
shazeer2017,
title={ Outrageously Large Neural Networks: The Sparsely-Gated Mixture-of-Experts Layer},
author={Noam Shazeer and *Azalia Mirhoseini and *Krzysztof Maziarz and Andy Davis and Quoc Le and Geoffrey Hinton and Jeff Dean},
booktitle={International Conference on Learning Representations},
year={2017},
}

@inproceedings{
lepikhin2021gshard,
title={{\{}GS{\}}hard: Scaling Giant Models with Conditional Computation and Automatic Sharding},
author={Dmitry Lepikhin and HyoukJoong Lee and Yuanzhong Xu and Dehao Chen and Orhan Firat and Yanping Huang and Maxim Krikun and Noam Shazeer and Zhifeng Chen},
booktitle={International Conference on Learning Representations},
year={2021},
}

@inproceedings{
NIPS2017_3f5ee243,
title={Attention is All you Need},
author={Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, \L ukasz and Polosukhin, Illia},
booktitle={Advances in Neural Information Processing Systems},
year={2017},
}

@misc{wei2024skyworkmoedeepdivetraining,
      title={Skywork-MoE: A Deep Dive into Training Techniques for Mixture-of-Experts Language Models}, 
      author={Tianwen Wei and Bo Zhu and Liang Zhao and Cheng Cheng and Biye Li and Weiwei Lü and Peng Cheng and Jianhao Zhang and Xiaoyu Zhang and Liang Zeng et al.},
      year={2024},
      eprint={2406.06563},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      howpublished={arXiv preprint arXiv:2406.06563},
}

@misc{ai2024yiopenfoundationmodels,
      title={Yi: Open Foundation Models by 01.AI}, 
      author={01. AI and : and Alex Young and Bei Chen and Chao Li and Chengen Huang and Ge Zhang and Guanwei Zhang and Heng Li and Jiangcheng Zhu and Jianqun Chen and Jing Chang et al.},
      year={2024},
      eprint={2403.04652},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
}

@inproceedings{devlin-etal-2019-bert,
    title = "{BERT}: Pre-training of Deep Bidirectional Transformers for Language Understanding",
    author = "Devlin, Jacob  and
      Chang, Ming-Wei  and
      Lee, Kenton  and
      Toutanova, Kristina",
    booktitle = "Proceedings of the 2019 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)",
    year = "2019",
    publisher = "Association for Computational Linguistics",
    pages = "4171--4186",
}

@inproceedings{
NEURIPS2020_1457c0d6,
title={Language Models are Few-Shot Learners},
author={Brown, Tom and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared D and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda et al.},
booktitle={Advances in Neural Information Processing Systems},
year={2020},
}

@inproceedings{dai-etal-2024-deepseekmoe,
    title = "{D}eep{S}eek{M}o{E}: Towards Ultimate Expert Specialization in Mixture-of-Experts Language Models",
    author = "Dai, Damai  and
      Deng, Chengqi  and
      Zhao, Chenggang  and
      Xu, R.x.  and
      Gao, Huazuo  and
      Chen, Deli  and
      Li, Jiashi  and
      Zeng, Wangding  and
      Yu, Xingkai  and
      Wu, Y. et al.",
    booktitle = "Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    year = "2024",
    publisher = "Association for Computational Linguistics",
    pages = "1280--1297",
}

@misc{deepseekai2024deepseekv2strongeconomicalefficient,
      title={DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model}, 
      author={DeepSeek-AI and Aixin Liu and Bei Feng and Bin Wang and Bingxuan Wang and Bo Liu and Chenggang Zhao and Chengqi Dengr and Chong Ruan and Damai Dai et al.},
      year={2024},
      eprint={2405.04434},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
}


@misc{liu2019robertarobustlyoptimizedbert,
      title={RoBERTa: A Robustly Optimized BERT Pretraining Approach}, 
      author={Yinhan Liu and Myle Ott and Naman Goyal and Jingfei Du and Mandar Joshi and Danqi Chen and Omer Levy and Mike Lewis and Luke Zettlemoyer and Veselin Stoyanov},
      year={2019},
      eprint={1907.11692},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      howpublished={arXiv preprint arXiv:1907.11692},
}

@article{Fedus2021SwitchTS,
  title={Switch Transformers: Scaling to Trillion Parameter Models with Simple and Efficient Sparsity},
  author={William Fedus and Barret Zoph and Noam M. Shazeer},
  journal={J. Mach. Learn. Res.},
  year={2021},
  volume={23},
  pages={120:1-120:39},
}

@misc{kaplan2020scalinglawsneurallanguage,
      title={Scaling Laws for Neural Language Models}, 
      author={Jared Kaplan and Sam McCandlish and Tom Henighan and Tom B. Brown and Benjamin Chess and Rewon Child and Scott Gray and Alec Radford and Jeffrey Wu and Dario Amodei},
      year={2020},
      eprint={2001.08361},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      howpublished={arXiv preprint arXiv:2001.08361},
}

@article{JMLR:v24:22-1144,
  author  = {Aakanksha Chowdhery and Sharan Narang and Jacob Devlin and Maarten Bosma and Gaurav Mishra and Adam Roberts and Paul Barham and Hyung Won Chung and Charles Sutton and Sebastian Gehrmann et al.},
  title   = {PaLM: Scaling Language Modeling with Pathways},
  journal = {Journal of Machine Learning Research},
  year    = {2023},
  volume  = {24},
  number  = {240},
  pages   = {1--113},
}

@article{DBLP:journals/corr/abs-2305-14705,
  author       = {Sheng Shen and
                  Le Hou and
                  Yanqi Zhou and
                  Nan Du and
                  Shayne Longpre and
                  Jason Wei and
                  Hyung Won Chung and
                  Barret Zoph and
                  William Fedus and
                  Xinyun Chen and
                  et al.},
  title        = {Flan-MoE: Scaling Instruction-Finetuned Language Models with Sparse
                  Mixture of Experts},
  journal      = {CoRR},
  volume       = {abs/2305.14705},
  year         = {2023},
  eprinttype    = {arXiv},
  eprint       = {2305.14705},
  biburl       = {https://dblp.org/rec/journals/corr/abs-2305-14705.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@article{Rozire2023CodeLO,
  title={Code Llama: Open Foundation Models for Code},
  author={Baptiste Rozi{\`e}re and Jonas Gehring and Fabian Gloeckle and Sten Sootla and Itai Gat and Xiaoqing Tan and Yossi Adi and Jingyu Liu and Tal Remez and J{\'e}r{\'e}my Rapin et al.},
  journal={ArXiv},
  year={2023},
  volume={abs/2308.12950},
}

@inproceedings{
azerbayev2024llemma,
title={Llemma: An Open Language Model for Mathematics},
author={Zhangir Azerbayev and Hailey Schoelkopf and Keiran Paster and Marco Dos Santos and Stephen Marcus McAleer and Albert Q. Jiang and Jia Deng and Stella Biderman and Sean Welleck},
booktitle={International Conference on Learning Representations},
year={2024},
}

@misc{yang2024qwen2technicalreport,
      title={Qwen2 Technical Report}, 
      author={An Yang and Baosong Yang and Binyuan Hui and Bo Zheng and Bowen Yu and Chang Zhou and Chengpeng Li and Chengyuan Li and Dayiheng Liu and Fei Huang et al.},
      year={2024},
      eprint={2407.10671},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      howpublished={arXiv preprint arXiv:2407.10671},
}

@article{Raffel2019ExploringTL,
  title={Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer},
  author={Colin Raffel and Noam M. Shazeer and Adam Roberts and Katherine Lee and Sharan Narang and Michael Matena and Yanqi Zhou and Wei Li and Peter J. Liu},
  journal={J. Mach. Learn. Res.},
  year={2019},
  volume={21},
  pages={140:1-140:67},
}

@inproceedings{
wei2022chain,
title={Chain of Thought Prompting Elicits Reasoning in Large Language Models},
author={Jason Wei and Xuezhi Wang and Dale Schuurmans and Maarten Bosma and brian ichter and Fei Xia and Ed H. Chi and Quoc V Le and Denny Zhou},
booktitle={Advances in Neural Information Processing Systems},
year={2022},
}

@inproceedings{
dosovitskiy2021an,
title={An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale},
author={Alexey Dosovitskiy and Lucas Beyer and Alexander Kolesnikov and Dirk Weissenborn and Xiaohua Zhai and Thomas Unterthiner and Mostafa Dehghani and Matthias Minderer and Georg Heigold and Sylvain Gelly et al.},
booktitle={International Conference on Learning Representations},
year={2021},
}


@InProceedings{pmlr-v139-touvron21a,
  title = 	 {Training data-efficient image transformers &amp; distillation through attention},
  author =       {Touvron, Hugo and Cord, Matthieu and Douze, Matthijs and Massa, Francisco and Sablayrolles, Alexandre and Jegou, Herve},
  booktitle = 	 {International Conference on Machine Learning},
  pages = 	 {10347--10357},
  year = 	 {2021},
  volume = 	 {139},
  series = 	 {Proceedings of Machine Learning Research},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v139/touvron21a/touvron21a.pdf},
}

@inproceedings{Clark2022UnifiedSL,
  title={Unified Scaling Laws for Routed Language Models},
  author={Aidan Clark and Diego de Las Casas and Aurelia Guy and Arthur Mensch and Michela Paganini and Jordan Hoffmann and Bogdan Damoc and Blake A. Hechtman and Trevor Cai and Sebastian Borgeaud et al.},
  booktitle={International Conference on Machine Learning},
  year={2022},
}

@inproceedings{kim-etal-2024-solar,
    title = "{SOLAR} 10.7{B}: Scaling Large Language Models with Simple yet Effective Depth Up-Scaling",
    author = "Kim, Sanghoon  and
      Kim, Dahyun  and
      Park, Chanjun  and
      Lee, Wonsung  and
      Song, Wonho  and
      Kim, Yunsu  and
      Kim, Hyeonwoo  and
      Kim, Yungi  and
      Lee, Hyeonju  and
      Kim, Jihoo  and
      et al.",
    booktitle = "Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 6: Industry Track)",
    year = "2024",
    publisher = "Association for Computational Linguistics",
    pages = "23--35",
}

@inproceedings{
sukhbaatar2024branchtrainmix,
title={Branch-Train-MiX: Mixing Expert {LLM}s into a Mixture-of-Experts {LLM}},
author={Sainbayar Sukhbaatar and Olga Golovneva and Vasu Sharma and Hu Xu and Xi Victoria Lin and Baptiste Roziere and Jacob Kahn and Shang-Wen Li and Wen-tau Yih and Jason E Weston and Xian Li},
booktitle={Conference on Language Modeling},
year={2024},
}

@misc{geminiteam2024gemini15unlockingmultimodal,
      title={Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context}, 
      author={Gemini Team and Petko Georgiev and Ving Ian Lei and Ryan Burnell and Libin Bai and Anmol Gulati and Garrett Tanzer and Damien Vincent and Zhufeng Pan and Shibo Wang et al.},
      year={2024},
      eprint={2403.05530},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      howpublished={arXiv preprint arXiv:2403.05530},
}

@misc{openai2024gpt4technicalreport,
      title={GPT-4 Technical Report}, 
      author={OpenAI},
      year={2024},
      eprint={2303.08774},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      howpublished={arXiv preprint arXiv:2303.08774},
}

@misc{Touvron2023Llama2O,
  title={Llama 2: Open Foundation and Fine-Tuned Chat Models},
  author={Hugo Touvron and Louis Martin and Kevin R. Stone and Peter Albert and Amjad Almahairi and Yasmine Babaei and Nikolay Bashlykov and Soumya Batra and Prajjwal Bhargava and Shruti Bhosale et al.},
  year={2023},
  eprint={2307.09288},
  archivePrefix={arXiv},
  primaryClass={cs.CL},
  howpublished={arXiv preprint arXiv:2307.09288},
}

@misc{dubey2024llama3herdmodels,
      title={The Llama 3 Herd of Models}, 
      author={Abhimanyu Dubey and Abhinav Jauhri and Abhinav Pandey and Abhishek Kadian and Ahmad Al-Dahle and Aiesha Letman and Akhil Mathur and Alan Schelten and Amy Yang and Angela Fan et al.},
      year={2024},
      eprint={2407.21783},
      archivePrefix={arXiv},
      primaryClass={cs.AI},
      howpublished={arXiv preprint arXiv:2407.21783},
}

@inproceedings{
ouyang2022training,
title={Training language models to follow instructions with human feedback},
author={Long Ouyang and Jeffrey Wu and Xu Jiang and Diogo Almeida and Carroll Wainwright and Pamela Mishkin and Chong Zhang and Sandhini Agarwal and Katarina Slama and Alex Gray et al.},
booktitle={Advances in Neural Information Processing Systems},
year={2022},
}

@inproceedings{
NEURIPS2022_c1e2faff,
title={An empirical analysis of compute-optimal large language model training},
author={Hoffmann, Jordan and Borgeaud, Sebastian and Mensch, Arthur and Buchatskaya, Elena and Cai, Trevor and Rutherford, Eliza and de Las Casas, Diego and Hendricks, Lisa Anne and Welbl, Johannes and Clark, Aidan et al.},
booktitle={Advances in Neural Information Processing Systems},
year={2022},
}

@misc{bai2023qwentechnicalreport,
      title={Qwen Technical Report}, 
      author={Jinze Bai and Shuai Bai and Yunfei Chu and Zeyu Cui and Kai Dang and Xiaodong Deng and Yang Fan and Wenbin Ge and Yu Han and Fei Huang et al.},
      year={2023},
      eprint={2309.16609},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
}

@misc{deepseekai2024deepseekllmscalingopensource,
      title={DeepSeek LLM: Scaling Open-Source Language Models with Longtermism}, 
      author={DeepSeek-AI and : and Xiao Bi and Deli Chen and Guanting Chen and Shanhuang Chen and Damai Dai and Chengqi Deng and Honghui Ding and Kai Dong and Qiushi Du et al.},
      year={2024},
      eprint={2401.02954},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
}

@InProceedings{clark2022unified,
  title = {Unified Scaling Laws for Routed Language Models},
  author = {Clark, Aidan and Casas, Diego de las and Guy, Aurelia and Mensch, Arthur and Paganini, Michela and Hoffmann, Jordan and Damoc, Bogdan and Hechtman, Blake and Cai, Trevor and Borgeaud, Sebastian et al.},
  year = {2022},
  booktitle = {International Conference on Machine Learning},
  keywords = {Computation and Language (cs.CL), Machine Learning (cs.LG), FOS: Computer and information sciences, FOS: Computer and information sciences},
  publisher = {PMLR},
}

@misc{krajewski2024scalinglawsfinegrainedmixture,
      title={Scaling Laws for Fine-Grained Mixture of Experts}, 
      author={Jakub Krajewski and Jan Ludziejewski and Kamil Adamczewski and Maciej Pióro and Michał Krutul and Szymon Antoniak and Kamil Ciebiera and Krystian Król and Tomasz Odrzygóźdź and Piotr Sankowski and Marek Cygan and Sebastian Jaszczur},
      year={2024},
      eprint={2402.07871},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
}

@article{Bengio2013EstimatingOP,
  title={Estimating or Propagating Gradients Through Stochastic Neurons for Conditional Computation},
  author={Yoshua Bengio and Nicholas L{\'e}onard and Aaron C. Courville},
  journal={ArXiv},
  year={2013},
  volume={abs/1308.3432},
}

@misc{bengio2016conditionalcomputationneuralnetworks,
      title={Conditional Computation in Neural Networks for faster models}, 
      author={Emmanuel Bengio and Pierre-Luc Bacon and Joelle Pineau and Doina Precup},
      year={2016},
      eprint={1511.06297},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
}

@misc{shazeer2020gluvariantsimprovetransformer,
      title={GLU Variants Improve Transformer}, 
      author={Noam Shazeer},
      year={2020},
      eprint={2002.05202},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      howpublished={arXiv preprint arXiv:2002.05202},
}

%% llm-jp-eval: JEMHopQA
%% ref. https://github.com/llm-jp/llm-jp-eval/blob/main/DATASET.md
@inproceedings{ishi-etal-2023-jemhopqa,
  author = {Ai Ishii and Naoya Inoue and Satoshi Sekine},
  title = {Construction of a {Japanese} multi-hop {QA} dataset for {QA} systems capable of explaining the rationale [根拠を説明可能な質問応答システムのための日本語マルチホップQAデータセット構築] (in {Japanese})},
  booktitle = {the 29th Annual Meeting of Japanese Association for Natural Language Processing (NLP2023)},
  year = {2023},
  pages = {2088--2093},
}


%% lm-evaluation-harness JP: WMT20 En-Ja, Ja-En
@inproceedings{barrault-etal-2020-findings-wmt20,
    title = "Findings of the 2020 Conference on Machine Translation ({WMT}20)",
    author = {Barrault, Lo{\"\i}c  and
      Biesialska, Magdalena  and
      Bojar, Ond{\v{r}}ej  and
      Costa-juss{\`a}, Marta R.  and
      Federmann, Christian  and
      Graham, Yvette  and
      Grundkiewicz, Roman  and
      Haddow, Barry  and
      Huck, Matthias  and
      Joanis, Eric  and
      et al.},
    booktitle = "Proceedings of WMT",
    year = "2020",
    pages = "1--55",
}

%% llm-jp-eval: NIILC
%% ref. https://github.com/llm-jp/llm-jp-eval/blob/main/DATASET.md
@inproceedings{sekine-etal-2003-niilc,
  title={Development of a question answering system focused on an encyclopedia [百科事典を対象とした質問応答システムの開発] (in {Japanese})},
  author={Satoshi Sekine}, 
  booktitle = {the 9th Annual Meeting of Japanese Association for Natural Language Processing (NLP2003)},
  pages={637--640},
  year={2003},
}

%% lm-evaluation-harness JP: XL-Sum_ja
%% ref. https://github.com/Stability-AI/lm-evaluation-harness/blob/jp-stable/lm_eval/tasks/ja/xlsum_ja.py
@inproceedings{hasan-etal-2021-xlsum,
    title = "{XL}-Sum: Large-Scale Multilingual Abstractive Summarization for 44 Languages",
    author = "Hasan, Tahmid  and
      Bhattacharjee, Abhik  and
      Islam, Md. Saiful  and
      Mubasshir, Kazi  and
      Li, Yuan-Fang  and
      ... and
      Shahriyar, Rifat",
    booktitle = "Findings of the Association for Computational Linguistics (ACL)",
    year = "2021",
    pages = "4693--4703",
}

%% lm-evaluation-harness: SQuAD2.0
%% ref. https://github.com/EleutherAI/lm-evaluation-harness/tree/main/lm_eval/tasks/squadv2
@inproceedings{DBLP:conf/acl/RajpurkarJL18-squad2,
  author       = {Pranav Rajpurkar and
                  Robin Jia and
                  Percy Liang},
  title        = {Know What You Don't Know: Unanswerable Questions for {SQuAD}},
  booktitle    = {Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (ACL)},
  pages        = {784--789},
  year         = {2018},
}

%% lm-evaluation-harness: XWINO
%% ref. https://github.com/EleutherAI/lm-evaluation-harness/tree/main/lm_eval/tasks/xwinograd
@inproceedings{DBLP:conf/acl/TikhonovR21-xwino,
  author       = {Alexey Tikhonov and
                  Max Ryabinin},
  title        = {It's All in the Heads: Using Attention Heads as a Baseline for Cross-Lingual Transfer in Commonsense Reasoning},
  booktitle    = {Findings of the Association for Computational Linguistics (ACL)},
  pages        = {3534--3546},
  year         = {2021},
}

%% lm-evaluation-harness: OpenBookQA
%% ref. https://github.com/EleutherAI/lm-evaluation-harness/tree/main/lm_eval/tasks/openbookqa
@inproceedings{mihaylov-etal-2018-openbookqa,
    title = "Can a Suit of Armor Conduct Electricity? {A} New Dataset for Open Book Question Answering",
    author = "Mihaylov, Todor  and
      Clark, Peter  and
      Khot, Tushar  and
      Sabharwal, Ashish",
    booktitle = "Conference on Empirical Methods in Natural Language Processing",
    year = "2018",
    publisher = "Association for Computational Linguistics",
    pages = "2381--2391",
}

%% lm-evaluation-harness: TriviaQA
%% ref. https://github.com/EleutherAI/lm-evaluation-harness/tree/main/lm_eval/tasks/triviaqa
@inproceedings{joshi-etal-2017-triviaqa,
    title = "{T}rivia{QA}: A Large Scale Distantly Supervised Challenge Dataset for Reading Comprehension",
    author = "Joshi, Mandar  and
      Choi, Eunsol  and
      Weld, Daniel  and
      Zettlemoyer, Luke",
    booktitle = "Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (ACL)",
    year = "2017",
    publisher = "Association for Computational Linguistics",
    pages = "1601--1611"
}


%% lm-evaluation-harness: HellaSwag
%% ref. https://github.com/EleutherAI/lm-evaluation-harness/tree/main/lm_eval/tasks/hellaswag
@inproceedings{zellers-etal-2019-hellaswag,
    title = "{H}ella{S}wag: Can a Machine Really Finish Your Sentence?",
    author = "Zellers, Rowan  and
      Holtzman, Ari  and
      Bisk, Yonatan  and
      Farhadi, Ali  and
      Choi, Yejin",
    booktitle = "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics",
    year = "2019",
    publisher = "Association for Computational Linguistics",
    pages = "4791--4800",
}

@incollection{Bengio+chapter2007,
author = {Bengio, Yoshua and LeCun, Yann},
booktitle = {Large Scale Kernel Machines},
publisher = {MIT Press},
title = {Scaling Learning Algorithms Towards {AI}},
year = {2007}
}


% LLM-jp チューニングWGが投稿予定のllm-jp-eval論文
@inproceedings{han-etal-2024-llm-jp-eval,
  author = {Namgi Han and Nobuhiro Ueda and Masatoshi Otake and Satoru Katsumata and Keisuke Kamata and Hirokazu Kiyomaru and Takashi Kodama and Saku Sugawara and Bowen Chen and Hiroshi Matsuda et al.},
  title = {llm-jp-eval: Automatic evaluation tool for
{Japanese} large language models [llm-jp-eval: 日
本語大規模言語モデルの自動評価ツール] (in {Japanese})},
  booktitle = {the 30th Annual Meeting of Japanese Association for Natural Language Processing (NLP2024)},
  year = {2024},
}

%% Language Model Evaluation Harness(v.0.3.0)
%% ref. https://zenodo.org/records/7413426
@software{leo_gao_2022_7413426_lm-evaluation-harness,
    author       = {Leo Gao and
                  Jonathan Tow and
                  Stella Biderman and
                  Charles Lovering and
                  Jason Phang and
                  Anish Thite and
                  Fazz and
                  Niklas Muennighoff and
                  Thomas Wang and
                  sdtblck and
                  et al.},
    title        = {{EleutherAI/lm-evaluation-harness: v0.3.0}},
    year         = 2022,
    publisher    = {Zenodo},
    version      = {v0.3.0},
}

%% lm-evaluation-harness JP: BLEU score calculation
%% ref. https://github.com/Stability-AI/lm-evaluation-harness/blob/jp-stable/lm_eval/tasks/translation.py
@inproceedings{post-2018-call-sacrebleu,
    title = "A Call for Clarity in Reporting {BLEU} Scores",
    author = "Post, Matt",
    booktitle = "Proceedings of the Third Conference on Machine Translation: Research Papers",
    year = "2018",
    pages = "186--191",
}

%% llm-jp-eval: JCommonsenseQA, JNLI, JSQuAD
%% ref. https://github.com/llm-jp/llm-jp-eval/blob/main/DATASET.md
@inproceedings{kurihara-etal-2022-jglue,
    title = "{JGLUE}: {J}apanese General Language Understanding Evaluation",
    author = "Kurihara, Kentaro  and
      Kawahara, Daisuke  and
      Shibata, Tomohide",
    booktitle = "Proceedings of the Thirteenth Language Resources and Evaluation Conference",
    year = "2022",
    pages = "2957--2966"
}

@article{
kocetkov2023the,
title={The Stack: 3 {TB} of permissively licensed source code},
author={Denis Kocetkov and Raymond Li and Loubna Ben allal and Jia LI and Chenghao Mou and Yacine Jernite and Margaret Mitchell and Carlos Mu{\~n}oz Ferrandis and Sean Hughes and Thomas Wolf and Dzmitry Bahdanau and Leandro Von Werra and Harm de Vries},
journal={Transactions on Machine Learning Research},
year={2023},
note={}
}

@misc{du2022glamefficientscalinglanguage,
      title={GLaM: Efficient Scaling of Language Models with Mixture-of-Experts}, 
      author={Nan Du and Yanping Huang and Andrew M. Dai and Simon Tong and Dmitry Lepikhin and Yuanzhong Xu and Maxim Krikun and Yanqi Zhou and Adams Wei Yu and Orhan Firat et al.},
      year={2022},
      eprint={2112.06905},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      howpublished={arXiv preprint arXiv:2112.06905},
}

@misc{zoph2022stmoedesigningstabletransferable,
      title={ST-MoE: Designing Stable and Transferable Sparse Expert Models}, 
      author={Barret Zoph and Irwan Bello and Sameer Kumar and Nan Du and Yanping Huang and Jeff Dean and Noam Shazeer and William Fedus},
      year={2022},
      eprint={2202.08906},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      howpublished={arXiv preprint arXiv:2202.08906},
}

@misc{abdin2024phi3technicalreporthighly,
      title={Phi-3 Technical Report: A Highly Capable Language Model Locally on Your Phone}, 
      author={Marah Abdin and Jyoti Aneja and Hany Awadalla and Ahmed Awadallah and Ammar Ahmad Awan and Nguyen Bach and Amit Bahree and Arash Bakhtiari and Jianmin Bao and Harkirat Behl et al.},
      year={2024},
      eprint={2404.14219},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      howpublished={arXiv preprint arXiv:2404.14219},
}

@misc{biderman2024lessonstrenchesreproducibleevaluation,
      title={Lessons from the Trenches on Reproducible Evaluation of Language Models}, 
      author={Stella Biderman and Hailey Schoelkopf and Lintang Sutawika and Leo Gao and Jonathan Tow and Baber Abbasi and Alham Fikri Aji and Pawan Sasanka Ammanamanchi and Sidney Black and Jordan Clive et al.},
      year={2024},
      eprint={2405.14782},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
}

@inproceedings{suzgun-etal-2023-challenging,
    title = "Challenging {BIG}-Bench Tasks and Whether Chain-of-Thought Can Solve Them",
    author = {Suzgun, Mirac  and
      Scales, Nathan  and
      Sch{\"a}rli, Nathanael  and
      Gehrmann, Sebastian  and
      Tay, Yi  and
      Chung, Hyung Won  and
      Chowdhery, Aakanksha  and
      Le, Quoc  and
      Chi, Ed  and
      Zhou, Denny  and
      Wei, Jason},
    booktitle = "Findings of the Association for Computational Linguistics: ACL 2023",
    year = "2023",
    publisher = "Association for Computational Linguistics",
    pages = "13003--13051",
}

@misc{sun2024transformerlayerspainters,
      title={Transformer Layers as Painters}, 
      author={Qi Sun and Marc Pickett and Aakash Kumar Nain and Llion Jones},
      year={2024},
      eprint={2407.09298},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      howpublished={arXiv preprint arXiv:2407.09298},
}

@misc{akiba2024evolutionaryoptimizationmodelmerging,
      title={Evolutionary Optimization of Model Merging Recipes}, 
      author={Takuya Akiba and Makoto Shing and Yujin Tang and Qi Sun and David Ha},
      year={2024},
      eprint={2403.13187},
      archivePrefix={arXiv},
      primaryClass={cs.NE},
      howpublished={arXiv preprint arXiv:2403.13187},
}

@inproceedings{
xue2024openmoe,
title={OpenMoE: An Early Effort on Open Mixture-of-Experts Language Models},
author={Fuzhao Xue and Zian Zheng and Yao Fu and Jinjie Ni and Zangwei Zheng and Wangchunshu Zhou and Yang You},
booktitle={International Conference on Machine Learning},
year={2024},
}

@misc{muennighoff2024olmoeopenmixtureofexpertslanguage,
      title={OLMoE: Open Mixture-of-Experts Language Models}, 
      author={Niklas Muennighoff and Luca Soldaini and Dirk Groeneveld and Kyle Lo and Jacob Morrison and Sewon Min and Weijia Shi and Pete Walsh and Oyvind Tafjord and Nathan Lambert et al.},
      year={2024},
      eprint={2409.02060},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      howpublished={arXiv preprint arXiv:2409.02060},
}

@inproceedings{loshchilov2019decoupled,
title={Decoupled Weight Decay Regularization},
author={Ilya Loshchilov and Frank Hutter},
booktitle={International Conference on Learning Representations},
year={2019},
}

@inproceedings{rajpurkar2016squad,
    title = "{SQ}u{AD}: 100,000+ Questions for Machine Comprehension of Text",
    author = "Rajpurkar, Pranav  and
      Zhang, Jian  and
      Lopyrev, Konstantin  and
      Liang, Percy",
    booktitle = "Conference on Empirical Methods in Natural Language Processing",
    year = "2016",
    publisher = "Association for Computational Linguistics",
    pages = "2383--2392",
}

@misc{zhu2024llamamoebuildingmixtureofexpertsllama,
      title={LLaMA-MoE: Building Mixture-of-Experts from LLaMA with Continual Pre-training}, 
      author={Tong Zhu and Xiaoye Qu and Daize Dong and Jiacheng Ruan and Jingqi Tong and Conghui He and Yu Cheng},
      eprint={2406.16554},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      howpublished={arXiv preprint arXiv:2406.16554},
      year={2024},
}

@article{su2024roformer,
  title={RoFormer: Enhanced transformer with Rotary Position Embedding},
  author={Jianlin Su and Murtadha Ahmed and Yu Lu and Shengfeng Pan and Wen Bo and Yunfeng Liu},
  journal={Neurocomputing},
  volume={568},
  pages={127063},
  year={2024},
}



@inproceedings{
ainslie2023gqa,
title={{GQA}: Training Generalized Multi-Query Transformer Models from Multi-Head Checkpoints},
author={Joshua Ainslie and James Lee-Thorp and Michiel de Jong and Yury Zemlyanskiy and Federico Lebron and Sumit Sanghai},
booktitle={Conference on Empirical Methods in Natural Language Processing},
year={2023},
}

@inproceedings{zhang-sennrich-neurips19,
    author = "Zhang, Biao and Sennrich, Rico",
    booktitle = "Advances in Neural Information Processing Systems",
    title = "{Root Mean Square Layer Normalization}",
    year = "2019"
}

@misc{narayanan2021efficientlargescalelanguagemodel,
      title={Efficient Large-Scale Language Model Training on GPU Clusters Using Megatron-LM}, 
      author={Deepak Narayanan and Mohammad Shoeybi and Jared Casper and Patrick LeGresley and Mostofa Patwary and Vijay Anand Korthikanti and Dmitri Vainbrand and Prethvi Kashinkunti and Julie Bernauer and Bryan Catanzaro and Amar Phanishayee and Matei Zaharia},
      year={2021},
      eprint={2104.04473},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
}


@article{megablocks,
  title={{MegaBlocks: Efficient Sparse Training with Mixture-of-Experts}},
  author={Trevor Gale and Deepak Narayanan and Cliff Young and Matei Zaharia},
  journal={Proceedings of Machine Learning and Systems},
  volume={5},
  year={2023}
}

@article{JMLR:v21:20-074,
  author  = {Colin Raffel and Noam Shazeer and Adam Roberts and Katherine Lee and Sharan Narang and Michael Matena and Yanqi Zhou and Wei Li and Peter J. Liu},
  title   = {Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer},
  journal = {Journal of Machine Learning Research},
  year    = {2020},
  volume  = {21},
  number  = {140},
  pages   = {1--67},
}

@inproceedings{artetxe-etal-2022-efficient,
    title = "Efficient Large Scale Language Modeling with Mixtures of Experts",
    author = "Artetxe, Mikel  and
      Bhosale, Shruti  and
      Goyal, Naman  and
      Mihaylov, Todor  and
      Ott, Myle  and
      Shleifer, Sam  and
      Lin, Xi Victoria  and
      Du, Jingfei  and
      Iyer, Srinivasan  and
      Pasunuru, Ramakanth  and
      Anantharaman, Giridharan  and
      Li, Xian  and
      Chen, Shuohui  and
      Akin, Halil  and
      Baines, Mandeep  and
      Martin, Louis  and
      Zhou, Xing  and
      Koura, Punit Singh  and
      O{'}Horo, Brian  and
      Wang, Jeffrey  and
      Zettlemoyer, Luke  and
      Diab, Mona  and
      Kozareva, Zornitsa  and
      Stoyanov, Veselin,
      Kozareva, Zornitsa  and
      Zhang, Yue",
    booktitle = "Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing",
    year = "2022",
    publisher = "Association for Computational Linguistics",
    pages = "11699--11732",
}

@inproceedings{
muennighoff2023scaling,
title={Scaling Data-Constrained Language Models},
author={Niklas Muennighoff and Alexander M Rush and Boaz Barak and Teven Le Scao and Nouamane Tazi and Aleksandra Piktus and Sampo Pyysalo and Thomas Wolf and Colin Raffel},
booktitle={Advances in Neural Information Processing Systems},
year={2023},
}




%%%% akiba tuika simasune !!! %%%

@inproceedings{moe_layer_iclr14,
  author       = {David Eigen and
                  Marc'Aurelio Ranzato and
                  Ilya Sutskever},
  title        = {Learning Factored Representations in a Deep Mixture of Experts},
  booktitle    = {2nd International Conference on Learning Representations, {ICLR} 2014,
                  Banff, AB, Canada, April 14-16, 2014, Workshop Track Proceedings},
  year         = {2014},
  biburl       = {https://dblp.org/rec/journals/corr/EigenRS13.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@article{classic_moe_1,
  author       = {Robert A. Jacobs and
                  Michael I. Jordan and
                  Steven J. Nowlan and
                  Geoffrey E. Hinton},
  title        = {Adaptive Mixtures of Local Experts},
  journal      = {Neural Comput.},
  volume       = {3},
  number       = {1},
  pages        = {79--87},
  year         = {1991},
  biburl       = {https://dblp.org/rec/journals/neco/JacobsJNH91.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@article{classic_moe_2,
  author       = {Michael I. Jordan and
                  Robert A. Jacobs},
  title        = {Hierarchical Mixtures of Experts and the {EM} Algorithm},
  journal      = {Neural Comput.},
  volume       = {6},
  number       = {2},
  pages        = {181--214},
  year         = {1994},
  biburl       = {https://dblp.org/rec/journals/neco/JordanJ94.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@article{moe_survey,
  author       = {Weilin Cai and
                  Juyong Jiang and
                  Fan Wang and
                  Jing Tang and
                  Sunghun Kim and
                  Jiayi Huang},
  title        = {A Survey on Mixture of Experts},
  journal      = {CoRR},
  volume       = {abs/2407.06204},
  year         = {2024},
  eprinttype    = {arXiv},
  eprint       = {2407.06204},
  biburl       = {https://dblp.org/rec/journals/corr/abs-2407-06204.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{expert_routing,
  author       = {Yanqi Zhou and
                  Tao Lei and
                  Hanxiao Liu and
                  Nan Du and
                  Yanping Huang and
                  Vincent Y. Zhao and
                  Andrew M. Dai and
                  Zhifeng Chen and
                  Quoc V. Le and
                  James Laudon},
  title        = {Mixture-of-Experts with Expert Choice Routing},
  booktitle    = {Advances in Neural Information Processing Systems},
  year         = {2022},
  biburl       = {https://dblp.org/rec/conf/nips/ZhouLLDHZDCLL22.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@article{2019t5,
    author = {Colin Raffel and Noam Shazeer and Adam Roberts and Katherine Lee and Sharan Narang and Michael Matena and Yanqi Zhou and Wei Li and Peter J. Liu},
    title = {Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer},
    journal = {arXiv e-prints},
    year = {2019},
    archivePrefix = {arXiv},
    eprint = {1910.10683},
    howpublished={arXiv preprint arXiv:1910.10683},
}

@misc{he2024upcyclinglargelanguagemodels,
      title={Upcycling Large Language Models into Mixture of Experts}, 
      author={Ethan He and Abhinav Khattar and Ryan Prenger and Vijay Korthikanti and Zijie Yan and Tong Liu and Shiqing Fan and Ashwath Aithal and Mohammad Shoeybi and Bryan Catanzaro},
      year={2024},
      eprint={2410.07524},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      howpublished={arXiv preprint arXiv:2410.07524},
}


xAI. Open release of grok-1, 2024. URL https://x.ai/blog/grok-os.
@misc{xai2024grok,
  title = {Open Release of Grok-1},
  author = {{xAI}},
  year = {2024},
  howpublished = {\url{https://x.ai/blog/grok-os}}
}