\clearpage
\section{Experimental Setup Details}
\subsection{FLOPs Calculation}

\begin{table}[t]
\caption{Detailed FLOPs Breakdown for Transformer Models (Single Forward Pass)}
\label{tab:detailed-flops}
\centering
\small
\begin{tabular}{lc}
\toprule
Component & FLOPs \\
\midrule
Embeddings & $2svd_h$ \\
\midrule
Attention (per layer) &  \\
\quad Key and value projections & $4sd_h d_k n_q$ \\
\quad Query projections & $2sd_h d_k n_h$ \\
\quad Key @ Query logits & $2s^2d_k n_h$ \\
\quad Attention matrix computation & $2s^2d_k n_h$ \\
\quad Softmax @ value reductions & $2sd_k n_h d_h $ \\
\midrule
FFN (SwiGLU, per layer) & \\
\quad Dense model & $4sd_h d_f + 2sd_fd_h$ \\
\quad MoE model & $n_e(4sd_h d_f + 2sd_fd_h)$ \\
\midrule
Final Logits & $2sd_hv$ \\
\midrule
Total (Dense) & embeddings $+ n_l(attention + \text{ffn}_{\text{Dense}}) + $ logits \\
Total (MoE) & embeddings $+ n_l(attention + \text{ffn}_{\text{MoE}}) + $ logits \\
\bottomrule
\end{tabular}
\end{table}

Table~\ref{tab:detailed-flops} presents the method for calculating FLOPs (floating point operations) for the forward path in transformer components. The variables used are as follows: $s$ (sequence length), $d_h$ (hidden size), $v$ (vocabulary size), $d_f$ (FFN intermediate size), $n_l$ (number of layers), $n_h$ (number of attention heads), $n_q$ (number of query groups), $d_k$ (attention head dimension), and $n_e$ (number of selected experts per token). For matrix multiplication $A_{m\times k} \times X_{k\times n}$, $2m\times k \times n$ FLOPs are required in the forward pass (the factor of 2 accounts for both multiplication and addition operations). The table displays the main FLOPs contributors for the forward path only. It should be noted that the computational costs for sigmoid and Hadamard product within SwiGLU calculations, MoE gate computations, and RMS Norm calculations are considered negligible and thus omitted from this analysis. While not shown in the table, backward propagation typically requires approximately twice the FLOPs of forward propagation.

% % \label{tab:detailed-flops}にflopsの計算方法を表す. 

% $s$: sequence length 
% $d_h$: hidden size 
% $v$: vocabulary size 
% $d_f$: ffn intermediate size
% $n_l$: number of layers 
% $n_h$: number of attention heads 
% $n_q$: number of query groups  
% $d_k$: attention head dimension
% $n_e$: number of selected experts per token
% である 


% 行列乗算：$A_{m\times k} \times X_{k\times n}$ の行列乗算には $2m\times k \times n$ FLOPsが必要です（乗算と加算を考慮するため2を掛けます）
% 後方伝播は通常, 前方伝播の約2倍のFLOPsを必要とする. 
% 主なflops貢献を　
% 表示する 

% (SwiGLU計算内でのシグモイド, アダマール積, MoEのGateの計算, RMS Normの計算量は無視できるレベルに小さいので無視する. )


% \small
% Variables: \\
% $s$: sequence length (4096),
% $d_h$: hidden size (3072),
% $v$: vocabulary size (99574), \\
% $d_f$: intermediate size (8192),
% $n_l$: number of layers (28), \\
% $n_h$: number of attention heads (24),
% $n_q$: number of query groups (24), \\
% $d_k$: attention head dimension (128),
% $n_e$: number of selected experts per token (2)
% \normalsize
\

\subsection{Implementation and Training environment}\label{appendix:training_environment}

% \Taishi{First Pass}

For our experiments with MoE models and the training of the 1.5B Dense model, we utilized the TSUBAME 4.0 supercomputer at the Global Scientific Information and Computing Center, Institute of Science Tokyo. This environment is equipped with NVIDIA H100 SXM5 94GB GPUs, with each node housing 4 H100 GPUs. Inter-node communication is facilitated by InfiniBand NDR200 interconnects. The training of our largest model, the 8×3.7B model, employed 16 nodes (totaling 64 GPUs).
For the training of the 152M and 3.7B Dense models, we leveraged the high-performance computing nodes (PHY) provided by Sakura Internet. This setup features NVIDIA H100 80GB GPUs, with each node containing 8 H100 GPUs. The network interface is equipped with four 400Gb RoCEv2-compatible NICs and two 25Gb NICs. The training of our largest Dense model (3.7B parameters) utilized a maximum of 32 nodes (totaling 256 GPUs).

For implementation, we used Megatron-LM\footnote{\url{https://github.com/NVIDIA/Megatron-LM}} for Dense model training, and moe-recipes\footnote{\url{https://github.com/rioyokotalab/moe-recipes}, Version 1.0.0} for MoE model training. 
Additionally, Flash Attention 2 \citep{dao2023flashattention2} was utilized to improve computational efficiency and reduce memory usage.
%
All the training processes were conducted using bfloat16 precision. 


% MoEモデルの実験と1.5BのDenseモデルの学習に, 東京工業大学学術国際情報センターのTSUBAME 4.0スーパーコンピューターを利用した. この環境では, NVIDIA H100 SXM5 94GB GPUを使用し, 各ノードに4基のH100 GPUが搭載されている. ノード間はInfiniBand NDR200で相互接続されている. 本研究で使用した最大モデルサイズである8×3.7Bモデルの学習には, 16ノード（計64基のGPU）を使用した. 152Mと3.7BのDenseモデルの学習には, さくらインターネットの高火力コンピューティングノード（PHY）を利用した. この環境では, NVIDIA H100 80GB GPUを使用し, 各ノードに8基のH100 GPUが搭載されている. ネットワークインターフェースには400Gb RoCEv2対応NICが4基と25Gb NICが2基搭載されている. 本研究で使用した最大のDenseモデルサイズ3.7Bの学習には最大32ノード（計256基のGPU）を使用した. 

\subsection{Model configurations}\label{appendix:model_configs_details}

% \begin{table}[t]
% \caption{Model Configuration Details}
% \label{tab:model-details}
% \centering
% \small
% \begin{tabular}{lcccccccc}
% \toprule
% \textbf{Model} & \textbf{Params} & \textbf{Active} & \textbf{Layers} & \textbf{d\textsubscript{model}} & \textbf{d\textsubscript{ff}} & \textbf{Attn} & \textbf{KV} & \textbf{Vocab} \\
%  &  &  & & & & \textbf{Heads} & \textbf{Heads} & \textbf{Size} \\
% \midrule
% Dense 152M & 152 & 152 & 12 & 512 & 2,048 & 8 & 8 & 99,574 \\
% Dense 1.5B & 1,500 & 1,500 & 24 & 2,048 & 7,168 & 16 & 8 & 48,586 \\
% Dense 3.7B & 3,700 & 3,700 & 28 & 3,072 & 8,192 & 24 & 24 & 99,574 \\
% Dense 13B & 13,000 & 13,000 & 40 & 5,120 & 13,824 & 40 & 40 & 99,574 \\
% \midrule
% MoE 8×152M & 420 & 190 & 12 & 512 & 2,048 & 8 & 8 & 99,574 \\
% MoE 8×1.5B & 8,960 & 2,600 & 24 & 2,048 & 7,168 & 16 & 8 & 48,586 \\
% MoE 8×3.7B & 18,600 & 5,900 & 28 & 3,072 & 8,192 & 24 & 24 & 99,574 \\
% \bottomrule
% \end{tabular}
% \end{table}

\begin{table}[t]
\caption{Model Configuration Details}
\label{tab:model-details}
\centering
\small
\begin{tabular}{lccccccc}
\toprule
\textbf{Model} & \textbf{Act Params /} & 
\textbf{Layers} & \textbf{d\textsubscript{model}} & \textbf{d\textsubscript{ff}} & \textbf{Attn} & \textbf{KV} & \textbf{Vocab} \\
 & \textbf{Total Params} & & & & \textbf{Heads} & \textbf{Heads} & \textbf{Size} \\
\midrule
Dense 152M & 152M / 152M & 12 & 512 & 2,048 & 8 & 8 & 99,574 \\
Dense 1.5B & 1.5B / 1.5B & 24 & 2,048 & 7,168 & 16 & 8 & 48,586 \\
Dense 3.7B & 3.7B / 3.7B & 28 & 3,072 & 8,192 & 24 & 24 & 99,574 \\
Dense 13B & 13B / 13B & 40 & 5,120 & 13,824 & 40 & 40 & 99,574 \\
\midrule
MoE 8×152M & 190M / 417M & 12 & 512 & 2,048 & 8 & 8 & 99,574 \\
MoE 8×1.5B & 2.6B / 8.9B & 24 & 2,048 & 7,168 & 16 & 8 & 48,586 \\
MoE 8×3.7B & 5.9B / 18B & 28 & 3,072 & 8,192 & 24 & 24 & 99,574 \\
\bottomrule
\end{tabular}
\end{table}

% \begin{table}[t]
% \caption{実験に使用したモデルの詳細}
% \label{tab:model-details}
% \centering
% \scriptsize
% \begin{tabular}{lrrrrrrrr}
% \toprule
% モデル名 & \multicolumn{2}{c}{パラメータ数} & レイヤー & 隠れ層 & 中間層 & \multicolumn{2}{c}{Attention} & 語彙 \\
% \cmidrule(lr){2-3} \cmidrule(lr){7-8}
%  & 全体 & アクティブ & 数 & サイズ & サイズ & Head & KV Head & サイズ \\
% \midrule
% Dense 152M & 152M & 152M & 12 & 512 & 2,048 & 8 & 8 & 99,574  \\
% Dense 1.5B & 1.5B & 1.5B & 24 & 2,048 & 7,168 & 16 & 8 & 48,586 \\
% Dense 3.7B & 3.7B & 3.7B & 28 & 3,072 & 8,192 & 24 & 24 & 99,574 \\
% Dense 13B & 13B & 13B & 40 & 5,120 & 13,824 & 40 & 40 & 99,574 \\
% \midrule
% MoE 8×152M & 0.42B & 0.19B & 12 & 512 & 2,048 & 8 & 8 & 99,574 \\
% MoE 8×1.5B & 8.96B & 2.6B & 24 & 2,048 & 7,168 & 16 & 8 & 48,586 \\
% MoE 8×3.7B & 18.6B & 5.9B & 28 & 3,072 & 8,192 & 24 & 24 & 99,574 \\
% \bottomrule
% \end{tabular}
% \end{table}

As described in Section~\ref{sec:model-architecture}, we selected the Llama~\citep{touvron2023llamaopenefficientfoundation} and Mixtral~\citep{jiang2024mixtralexperts} architectures for dense and MoE models, respectively, for our experiments. 
Both architectures are based on the Transformer~\citep{NIPS2017_3f5ee243} with several improvements, including RMSNorm~\citep{zhang-sennrich-neurips19}, SwiGLU~\citep{shazeer2020gluvariantsimprovetransformer}, and rotary position embeddings (RoPE)~\citep{su2024roformer}. 
The notable difference in Mixtral (MoE) from Llama (dense) is that all feedforward network (FFN) layers are replaced by sparsely gated MoE layers. Table~\ref{tab:model-details} shows the details of the model configuration.


\subsection{Model training  configurations}\label{appendix:training_configs_details}

As shared settings for training all models, we adopted the following hyperparameters: AdamW optimizer~\citep{loshchilov2019decoupled} with $\beta_1=0.9$, $\beta_2=0.95$, and $\epsilon=10^{-8}$, sequence length of 4096, weight decay of 0.1, and gradient clipping of 1.0. 
The global batch size was set to 1024 for the 1.5B, 3.7B and 13B models, and 512 for the 152M model.

We used cosine decay for learning rate scheduling. For Dense models, the maximum learning rate was set to $3 \times 10^{-4}$, and it decayed to $3 \times 10^{-5}$ over 1,000B tokens for the 1.5B model, and 2,072B tokens for the 152M, 3.7B and 13B models, with the learning rate remaining constant during the final 2000 steps. 
For MoE models, the maximum learning rate was set to $2 \times 10^{-4}$, and it decayed to $2 \times 10^{-5}$ over 500B tokens.
Additionally, to prevent instability in training due to unbalanced routing on the MoE models, a load balancing loss was introduced, with the coefficient unified at 0.02 across all MoE models.






\section{Datasets and evaluation methods}

\subsection{Training dataset details}\label{appendix:dataset-details}

We used the LLM-jp corpus v3\footnote{https://gitlab.llm-jp.nii.ac.jp/datasets/llm-jp-corpus-v3}, an open corpus curated by the LLM-jp working group, for training English and Japanese bilingual language models.
The corpus consists of 1.7T tokens in English, Japanese, and source code with a small amount of Chinese and Korean tokens.
Following the LLM-jp's scheme, some Japanese portion of the corpus is upsampled by 2 to obtain 2.1T training tokens in total.

\begin{table}[t]
\caption{Statistics of the training dataset.}
\label{tab:dataset}
\centering
\small
\begin{tabular}{ll|r}
\toprule
Language & Subset & \#tokens [$\times 10^9$] \\
\midrule
English  & Dolma 1.6 (sampled) \citep{soldaini-etal-2024-dolma} & 945.\hphantom{0} \\
         & Wikipedia                                            &   4.7 \\
\midrule
Japanese & Common Crawl \citep{llmjp2024llmjpcrossorganizationalprojectresearch} & 381.\hphantom{0} \\
         & Kaken                                                &   0.9 \\
         & NDL WARP HTML                                        &   1.3 \\
         & NDL WARP PDF                                         & 207.\hphantom{0} \\
         & Wikipedia                                            &   1.3 \\
\midrule
Chinese  & Wikipedia                                            & 0.8 \\
\midrule
Korean   & Wikipedia                                            & 0.9 \\
\midrule
Code     & The Stack \citep{kocetkov2023the}                    & 114.\hphantom{0} \\
\bottomrule
\end{tabular}
\end{table}

Table \ref{tab:dataset} describes the statistics of the corpus subsets that were used for training data of the Dense and MoE models in our experiments.

Table~\ref{tab:dataset-distribution} details the dataset distribution percentages used for training the different model sizes. The 152M, 3.7B, and 13B models share the same data proportions, while the 1.5B model has slightly different percentages. 


\begin{table}[t]
\caption{Dataset Distribution Overview (Percentages)}
\label{tab:dataset-distribution}
\centering
\small
\begin{tabular}{llrr}
\toprule
Language & Subset & 152M/3.7B/13B & 1.5B \\
\midrule
\multirow{2}{*}{English} & Dolma & 45.6\% & 39.7\% \\
& Wikipedia & 0.2\% & 0.5\% \\
\midrule
\multirow{5}{*}{Japanese} & Common Crawl & 36.8\% & 49.5\% 
 \\
& Kaken & 0.1\% & 0.1\% \\
& NDL WARP HTML & 0.1\% & -  \\
& NDL WARP PDF & 11.5\% & -  \\
& Wikipedia & 0.1\% & 0.2\%  \\
\midrule
Chinese & Wikipedia & 0.1\% & - \\
\midrule
Korean & Wikipedia & 0.1\% & -  \\
\midrule
Code & The Stack & 5.5\% & 10.1\%  \\
\midrule
\multicolumn{2}{l}{Total Tokens (B)} & 2,072 & 1,000 \\
\bottomrule
\end{tabular}
\begin{tabular}{@{}p{\linewidth}@{}}

\end{tabular}
\end{table}




% 1.5Bモデルは1Tトークン学習後, 500BトークンをサンプリングしてUpcyclingを行った. 0.15Bと3.7Bモデルは2072.49Bトークン学習途中の1TチェックポイントからUpcyclingを開始し, 表\ref{tab:training-data-others}に示すデータ配分でサンプリングを行った. 

%\subsubsection{Evaluation Datasets}

% \paragraph{JEMHQA}
% JEMHopQA（JEMHQA）は, 元々マルチホップQAタスクとして設計された日本語の自由回答式質問応答データセットです. 私たちのタスク設定では, このデータセットは与えられた質問から直接回答を生成するモデルの能力を評価するために使用されます. JEMHQAの質問は日本語版Wikipediaの情報を使用して作成されており, 多様なトピックとエンティティを確保しています. 
% JEMHQAは, 知識の範囲とその知識を用いた推論による質問応答能力を評価する重要なベンチマークとして機能します. 
% \paragraph{JSQuAD}
% JSQuADは, SQuAD~\citep{rajpurkar2016squad}の日本語版である日本語機械読解データセットです. これは, 文書と質問を読み, 文書内のテキストの一部を回答として抽出する質問応答形式に焦点を当てています. 
% JSQuADは, 日本語版Wikipediaの記事段落を元文書として使用しています. 
% \paragraph{NIILC}
% NIILCは, 日本語の質問応答システム開発を目的として作成されたデータセットで, 百科事典を参照することで回答できる比較的単純な質問が特徴です. これにより, LLMの百科事典的知識を評価するのに有用です. 
% 元のデータセットには質問の種類や回答の手がかり, 回答の記載場所などの追加情報も含まれていますが, 私たちは質問と回答のペアのみを使用しています. 
% \paragraph{XL-Sum}
% XL-Sum日本語版は, BBCニュース記事から収集された大規模要約データセットであるXL-Sumから日本語部分を抽出し, さらに抽象的要約に適したサブセットにフィルタリングして作成されたデータセットです. データセットは, 記事と要約の15-gramオーバーラップ率を計算し, オーバーラップ率の低いペアを選択してフィルタリングされました. 
% 結果として, このデータセットは単に文を抽出するのではなく, 情報を言い換えたり抽象化したりする能力を必要とします. 


\begin{table}[t]

\caption{Evaluation Benchmark Details}
\tabcolsep 3pt%=0.1cm
\label{tab:eval_benchmark_details}
\centering
\tiny
\begin{tabular}{lccccccccccccc}
\toprule
& \makecell[c]{\textbf{JEM} \\ \textbf{HQA}} & \makecell[c]{\textbf{NIILC}} & \makecell[c]{\textbf{JSQ}} & \makecell[c]{\textbf{XL}-\\\textbf{Sum}} & \makecell[c]{\textbf{WMT} \\ \textbf{E$\to$J}} & \makecell[c]{\textbf{WMT} \\ \textbf{J$\to$E}} & \makecell[c]{\textbf{OB} \\ \textbf{QA}} & \makecell[c]{\textbf{TQA}} & \makecell[c]{\textbf{HS}} & \makecell[c]{\textbf{SQ} \\ \textbf{v2}} & \makecell[c]{\textbf{XW}-\\\textbf{EN}} & \makecell[c]{\textbf{BBH}} \\
\midrule
\textbf{Dataset} & JEMHQA & NIILC & JSQuAD & XL-Sum & \multicolumn{2}{c}{WMT20} & OBQA & TriviaQA & HellaSwag & SQuAD2 & XWINO & BBH \\
\textbf{Task} & \multicolumn{2}{c}{QA} & MRC & Summ. & \multicolumn{2}{c}{Trans.} & \multicolumn{2}{c}{QA} & MRC & MRC & Commonsense & Logical \\
& & & & & & & & & & & Reasoning & Reasoning \\
\textbf{Language} & JA & JA & JA & JA & EN$\to$JA & JA$\to$EN & EN & EN & EN & EN & EN & EN \\
\textbf{\# Instances} & 120 & 198 & 4,442 & 766 & 1,000 & 993 & 500 & 17,944 & 10,042 & 11,873 & 2,325 & 6,511 \\
\textbf{Few-shot \#} & 4 & 4 & 4 & 1 & 4 & 4 & 4 & 4 & 4 & 4 & 4 & 3 \\
\textbf{Evaluation Metric} & \multicolumn{3}{c}{Character F1} & ROUGE-2 & \multicolumn{2}{c}{BLEU} & \multicolumn{5}{c}{Accuracy} & CoT Acc. \\
\bottomrule
\end{tabular}
\end{table}


% \begin{table}[t]
% \caption{評価ベンチマークの詳細}
% \label{tab:eval_benchmark_details}
% \centering
% \scriptsize
% \resizebox{\textwidth}{!}{
% \begin{tabular}{|l|c|c|c|c|c|c|c|c|c|c|c|c|}
% \toprule
% 略称 & JEM & NII & JSQ & XL & E$\to$J & J$\to$E & OBQ & TrQ & HeS & SQ2 & XWI & BBH \\
% \midrule
% データセット & JEMHQA & NIILC & JSQuAD & XL-Sum & \multicolumn{2}{c|}{WMT20} & OBQA & TriviaQA & HellaSwag & SQuAD2 & XWINO & BBH \\
% \hline
% タスク & \multicolumn{2}{c|}{QA} & MRC & 要約 & \multicolumn{2}{c|}{翻訳} & \multicolumn{2}{c|}{QA} & \multicolumn{2}{c|}{MRC} & 常識推論 & 論理推論 \\
% \hline
% 言語 & JA & JA & JA & JA & EN$\to$JA & JA$\to$EN & EN & EN & EN & EN & EN & EN \\
% \hline
% 事例数 & 120 & 198 & 4,442 & 766 & 1,000 & 993 & 500 & 17,944 & 10,042 & 11,873 & 2,325 & 6,511 \\
% \hline
% Few-shot数 & 4 & 4 & 4 & 1 & 4 & 4 & 4 & 4 & 4 & 4 & 4 & 3 \\
% \hline
% 評価指標 & \multicolumn{3}{c|}{文字F1} & ROUGE-2 & \multicolumn{2}{c|}{BLEU} & \multicolumn{5}{c|}{Accuracy} & CoT Acc. \\
% \bottomrule
% \end{tabular}
% }
% \end{table}

\subsection{Evaluation Datasets and Methodologies}
\label{appendix:evaluation_details}

Table \ref{tab:eval_benchmark_details} provides detailed information about the evaluations used in our experiments. The evaluation tasks comprise both Japanese and English language assessments. We utilized publicly available evaluation code for our assessments\footnote{\url{https://github.com/swallow-llm/swallow-evaluation}}.

The evaluation tasks are categorized into seven types, such as free-form QA (NIILC~\citep{sekine-etal-2003-niilc}, JEMHQA~\citep{ishi-etal-2023-jemhopqa}), machine reading comprehension (JSQuAD~\citep{kurihara-etal-2022-jglue}, SQuAD2~\citep{DBLP:conf/acl/RajpurkarJL18-squad2}), abstractive summarization (XL-Sum~\citep{hasan-etal-2021-xlsum}), machine translation (WMT'20 En-Ja, Ja-En~\citep{barrault-etal-2020-findings-wmt20}), question answering (OpenBookQA~\citep{mihaylov-etal-2018-openbookqa}, TriviaQA~\citep{joshi-etal-2017-triviaqa}), common sense reasoning (HellaSwag~\citep{zellers-etal-2019-hellaswag}, XWinograd~\citep{DBLP:conf/acl/TikhonovR21-xwino}), and logical reasoning (Big Bench Hard (BBH)~\citep{suzgun-etal-2023-challenging}).
%
%自由記述QA, 機械読解, 機械翻訳, 質問応答, 常識推論には4-shot学習を適用し, 自動要約には1-shot学習, 論理推論には3-shot学習を用いた. なお, 論理推論タスクではChain-of-Thought手法 \citep{wei2022chain} も併せて使用した. 
%各タスクの詳細な評価指標については, 付録\ref{appendix:evaluation_details}に詳述している. 
We used 4-shot prompting for the Free-form QA, machine reading comprehension, machine translation, question answering, and commonsense reasoning tasks, 
1-shot prompting for the abstractive summarization task, 
and 3-shot prompting for the logical reasoning task. 
Moreover, we also applied the Chain-of-Thought method~\citep{wei2022chain} for the logical reasoning task. 



\section{Additional Experimental Results and Analysis}

\subsection{Comparison of Gate Initialization Methods}\label{appendix:initialization_details}

We conducted a detailed investigation into the effects of gate initialization on the performance of \NUname{}. An ablation study was performed on five different initialization patterns. Table \ref{tab:gate-initialization-comparison} presents the comparison results of different gate initialization patterns in an 8×1.5B model. Performance was evaluated after training on 50B tokens.

While preliminary experiments had indicated better results with a standard deviation of 0.28, our main experiments revealed that a uniform distribution with a standard deviation of 0.02 achieved the highest average performance across tasks. Based on these results, we adopted a uniform distribution ($\mathcal{U}(-0.0346, 0.0346)$, as the standard method for gate initialization in this study.
It is worth noting that gate initialization may not be a critical factor in model performance, and any initialization that avoids extreme values such as excessively high standard deviations is likely to be sufficient. 

% \begin{table}[t]
% \caption{8×1.5Bモデルのゲート初期化パターン比較（学習トークン数：50B）}
% \label{tab:gate-initialization-comparison}
% \centering
% \small
% \renewcommand{\arraystretch}{1.2}
% \begin{adjustbox}{width=\linewidth}
% \begin{tabular}{l*{6}{r}*{6}{r}r}
% \toprule
% \multirow{2}{*}{\textbf{初期化手法}} & \multicolumn{6}{c}{\textbf{日本語タスク}} & \multicolumn{6}{c}{\textbf{英語タスク}} & \multirow{2}{*}{\textbf{AVG}} \\
% \cmidrule(lr){2-7} \cmidrule(lr){8-13}
% & JEM & NII & JSQ & XL & J$\to$E & E$\to$J & OBQ & TrQ & SQ2 & HeS & XWI & BBH & \\
% \midrule
% GD (0.02, 0) & 46.1 & 37.9 & \textbf{63.6} & 9.2 & 15.4 & 8.1 & 22.4 & \textbf{19.4} & \textbf{41.7} & \textbf{15.6} & 80.0 & 25.9 & 32.1 \\
% GD (0.28, 0) & 50.6 & 38.6 & 54.6 & 9.3 & 15.5 & \textbf{8.3} & 20.6 & 18.4 & 41.1 & 14.3 & 79.8 & 24.7 & 31.3 \\
% Uniform (0.02, 0) & 49.2 & \textbf{38.9} & 61.0 & \textbf{9.7} & \textbf{16.0} & 7.9 & \textbf{23.6} & 18.9 & \textbf{41.7} & 15.5 & \textbf{80.9} & 23.9 & \textbf{32.3} \\
% Uniform (0.28, 0) & 44.6 & 36.3 & 56.3 & 8.6 & 15.5 & 8.1 & 20.6 & 17.7 & 41.0 & 14.6 & 80.0 & \textbf{26.0} & 30.8 \\
% Uniform (0.28, 0.5) & \textbf{51.5} & 36.8 & 55.6 & 9.0 & 15.7 & 7.9 & 21.6 & 18.3 & 41.0 & 15.3 & 80.1 & 25.1 & 31.5 \\
% \bottomrule
% \end{tabular}
% \end{adjustbox}
% \raggedright
% \small
% GD: 正規分布, Uniform: 一様分布, (std, mean): (標準偏差, 平均). 太字は各タスクで最高のスコアを示す. 
% \end{table}


\begin{table}[t]
\caption{Gate Initialization Pattern Comparison for 8×1.5B Models (Training Tokens: 50B)}
\label{tab:gate-initialization-comparison}
\centering
\small
\renewcommand{\arraystretch}{1.03}
\tabcolsep=0.11cm
\begin{adjustbox}{width=\linewidth}
\begin{tabular}{cl*{13}{r}r}
\toprule
& \textbf{Initialization} & \multicolumn{13}{c}{\textbf{Results}} \\
\cmidrule(lr){2-2} \cmidrule(lr){3-15}
\textbf{\#} & \textbf{Distribution} & \textbf{JEM} & \textbf{NII} & \textbf{JSQ} & \textbf{XL} & \textbf{J$\to$E} & \textbf{E$\to$J} & \textbf{OBQ} & \textbf{TrQ} & \textbf{SQ2} & \textbf{HeS} & \textbf{XWI} & \textbf{BBH} & \textbf{AVG} \\
\midrule
1 & $\mathcal{N}(0, 0.02)$ & 46.1 & 37.9 & \textbf{63.6} & 9.2 & 15.4 & 8.1 & 22.4 & \textbf{19.4} & \textbf{41.7} & \textbf{15.6} & 80.0 & 25.9 & 32.1 \\
2 & $\mathcal{N}(0, 0.2887)^*$ & 50.6 & 38.6 & 54.6 & 9.3 & 15.5 & \textbf{8.3} & 20.6 & 18.4 & 41.1 & 14.3 & 79.8 & 24.7 & 31.3 \\
3 & $\mathcal{U}(-0.0346, 0.0346)^\dagger$  & 49.2 & \textbf{38.9} & 61.0 & \textbf{9.7} & \textbf{16.0} & 7.9 & \textbf{23.6} & 18.9 & \textbf{41.7} & 15.5 & \textbf{80.9} & 23.9 & \textbf{32.3} \\
4 & $\mathcal{U}(-0.5, 0.5)$ & 44.6 & 36.3 & 56.3 & 8.6 & 15.5 & 8.1 & 20.6 & 17.7 & 41.0 & 14.6 & 80.0 & \textbf{26.0} & 30.8 \\
5 & $\mathcal{U}(0, 1)$ & \textbf{51.5} & 36.8 & 55.6 & 9.0 & 15.7 & 7.9 & 21.6 & 18.3 & 41.0 & 15.3 & 80.1 & 25.1 & 31.5 \\
\bottomrule
\end{tabular}
\end{adjustbox}
\raggedright
\small
$\mathcal{N}(\mu, \sigma)$: Normal distribution with mean $\mu$ and standard deviation $\sigma$. \\
$\mathcal{U}(a, b)$: Uniform distribution over the interval $[a, b]$. \\
$^*$ $\sigma = \sqrt{1/12} \approx 0.2887$, matches the standard deviation of $\mathcal{U}(0, 1)$. \\
$^\dagger$ Corrected from $\mathcal{U}(-0.0346, 0.0346)$ to match the standard deviation of 0.02.
Bold values indicate the best score for each task.
\end{table}



\subsection{Detailed analysis of expert routing patterns across layers}\label{subsec:detailed_routing_analysis}

For a comprehensive view of routing patterns across all layers, we provide detailed plots of expert routing probabilities for all 24 layers, grouped into early, middle, and late stages. These plots offer a more granular analysis of how routing behaviors evolve throughout the model depth.

Figures~\ref{fig:routing_early} to \ref{fig:routing_late} show the expert routing patterns for all 24 layers of the 8×1.5B MoE models trained with different methods, grouped into early (layers 0-7), middle (layers 8-15), and late (layers 16-23) stages. This comprehensive view allows for a detailed analysis of how routing patterns evolve across the entire model depth.

\begin{figure}[t]
\centering
\includegraphics[width=\textwidth]{images/routing_patterns_early_layers.pdf}
\caption{Expert routing patterns for early layers (0-7) of the 8×1.5B MoE models.}
\label{fig:routing_early}
\end{figure}

\begin{figure}[t]
\centering
\includegraphics[width=\textwidth]{images/routing_patterns_middle_layers.pdf}
\caption{Expert routing patterns for middle layers (8-15) of the 8×1.5B MoE models.}
\label{fig:routing_middle}
\end{figure}

\begin{figure}[t]
\centering
\includegraphics[width=\textwidth]{images/routing_patterns_late_layers.pdf}
\caption{Expert routing patterns for late layers (16-23) of the 8×1.5B MoE models.}
\label{fig:routing_late}
\end{figure}

These figures illustrate how the routing patterns evolve throughout the model layers, providing insights into the specialization and behavior of experts at different depths. Notably, the \NUname {} method does not exhibit clear evidence of bias towards specific domains in any layer. In contrast, our proposed method demonstrates domain specialization in multiple layers across the network—from those closest to the input to those near the output—while reusing the parameters of the dense model. This indicates that our approach effectively facilitates expert specialization in several layers without the need to train from scratch, leveraging the pre-trained dense model to achieve efficient domain-specific routing throughout significant portions of the network depth.

\clearpage
\subsection{\diff{Comparing Global vs. Layer-wise Load Balancing}}\label{subsec:detailed_load_balance_comparison}
\diff{In our experiments (Section~\ref{sec:exp}), we applied load balancing loss globally rather than layer-wise. This approach aligns with the implementation in the HuggingFace Transformers library and is widely adopted in the community.
To analyze the effect of global and layer-wise load balancing, we conducted a comparative analysis between global and layer-wise load balancing applications across 40B tokens for different initialization methods (From Scratch, Branch-Train-MiX, \NUname{}, and \methodname{} with r=0.5 and r=1.0) in the 8×1.5B setting.
As shown in Figure~\ref{fig:load_balancing_comparison}, both approaches yield similar training loss trajectories and downstream task performance.
These results suggest that the effectiveness of \methodname{} is not significantly affected by whether load balancing loss is applied globally or layer-wise.}
\begin{figure}[t]
\centering
\includegraphics[width=\textwidth]{images/load_balance_comparison.pdf}
\caption{\diff{\textbf{Comparison between global and layer-wise load balancing across different initialization methods.} Top: Training loss trajectories over 40B tokens. Bottom: Evaluation metrics measured at iterations corresponding to 10B, 20B, 30B, and 40B tokens. Results show comparable performance between global and layer-wise approaches across all methods.}}
\label{fig:load_balancing_comparison}
\end{figure}

\begin{figure}[t]
\centering
\includegraphics[width=\textwidth]{images/layerwise_routing_patterns_early_layers.pdf}
\caption{\diff{Expert routing patterns for early layers (0-7) under layer-wise load balancing at 40B tokens}}
\label{fig:router_layerwise_early}
\end{figure}

\begin{figure}[t]
\centering
\includegraphics[width=\textwidth]{images/layerwise_routing_patterns_middle_layers.pdf}
\caption{\diff{Expert routing patterns for middle layers (8-15) under layer-wise load balancing at 40B tokens}}
\label{fig:router_layerwise_middle}
\end{figure}

\begin{figure}[t]
\centering
\includegraphics[width=\textwidth]{images/layerwise_routing_patterns_late_layers.pdf}
\caption{\diff{Expert routing patterns for late layers (16-23) under layer-wise load balancing at 40B tokens}}
\label{fig:router_layerwise_late}
\end{figure}

\diff{Figures~\ref{fig:router_layerwise_early} through \ref{fig:router_layerwise_late} show the routing patterns when applying layer-wise load balancing loss at 40B tokens. The results demonstrate that \methodname{} (r=0.5) exhibits domain-specialized routing patterns similar to training from scratch. In contrast, \NUname{} shows nearly uniform routing across all layers except the final layer, which aligns with findings reported in \cite{jiang2024mixtralexperts}. Our proposed \methodname{} method appears to escape the local optima observed in \NUname, which likely contributes to its improved performance.}

\diff{The trade-offs between layer-wise and global load balancing—whether to enforce uniform expert utilization through layer-wise application or to allow potential expert collapse with global application—along with broader questions about MoE architecture design (such as varying expert counts per layer) remain as interesting directions for future research.}


\clearpage
\subsection{\diff{Convergence Catch-Up Analysis}}
\begin{figure}[t]
\centering
\includegraphics[width=\textwidth]{images/convergence_catchup_analysis.pdf}
\vspace{-1em}
\caption{
\diff{\textbf{Convergence catch-up analysis.}
We compare the relative convergence speed of \methodname{} and baseline methods by examining the number of training tokens required to reach the same loss value. The x-axis represents the number of training tokens processed by the baseline method, while the y-axis shows the difference in training tokens needed by \methodname{} to achieve the same loss. Positive values indicate that \methodname{} achieves the loss faster, while negative values suggest the baseline method is ahead.
% The results demonstrate that Drop-Upcycling’s long-term convergence speed is at least comparable to, if not better than, the baseline methods.
}}
\label{fig:convergence_catchup_analysis}
\end{figure}

\diff{To examine the selection of methods based on the training budget and to explore potential extrapolations of long-term trends beyond the scope of our analysis so far, we conduct a brief relative quantitative analysis of the convergence speeds of \methodname{} and baseline methods. In Figure~\ref{fig:convergence_catchup_analysis}, we compare the number of training tokens required to reach the same loss value for \methodname{} and the baseline methods. The plot shows that no significant trend of diminishing advantage for \methodname{} over the baseline methods is observed. This indicates that training from scratch would require an impractically large number of tokens to match \methodname{}, making \methodname{} the better choice in practical scenarios.}

\diff{However, it is important to acknowledge the limitations of this analysis. First, the effect of the learning rate (LR) schedule must be considered. Differences in LR due to different step counts could artificially influence the observed trends in convergence advantage. For example, we hypothesize that the widening advantage of \methodname{} observed late in training (after 400B tokens) may not entirely reflect the contribution of \methodname{} itself but could instead be attributed to the influence of LR scheduling. To eliminate the impact of LR scheduling, conducting all experiments with a constant LR would provide a more valid basis for this comparison.}

\diff{Second, it is worth noting that Branch-Train-Mix utilizes an additional training budget for pretraining individual experts before MoE training. In our setup, for instance, three expert models were pretrained using 100B tokens each, requiring a total of 300B tokens for dense model training before the MoE training phase. As a result, while Branch-Train-Mix appears to show an initial advantage in the plot, this advantage diminishes when accounting for the total training budget. Thus, in terms of overall efficiency, Branch-Train-Mix offers little to no advantage during most of the training process.}

\subsection{\diff{Detailed Derivations of Theoretical Characteristics}}\label{subsec:theoretical}
\diff{Consider the output of MoE layer with parameter re-initialization ratio $r$. Let $\text{FFN}_{\text{retained}_i}(\mathbf{x})$ denote the output from expert $i$'s preserved original parameters (ratio $(1-r)$) and $\text{FFN}_{\text{diverse}_i}(\mathbf{x})$ denote the output from reinitialized parameters (ratio $r$). The exact form of MoE output is:}
\begin{equation}
\diff{\mathbf{y} = \sum_{i=1}^N g(\mathbf{x})_i \cdot (\text{FFN}_{\text{retained}_i}(\mathbf{x}) + \text{FFN}_{\text{diverse}_i}(\mathbf{x}))}
\end{equation}
\diff{where $g(\mathbf{x})_i$ is the gating function defined in~\ref{eq:gating_function}. Note that $g(\mathbf{x})_i = 0$ for experts not among the top-$k$ selected.}

\diff{Let $S_k$ denote the set of indices for the $k$ selected experts. We can rewrite the output as:
\begin{align}
\mathbf{y} &= \sum_{i \in S_k} g(\mathbf{x})_i \cdot (\text{FFN}_{\text{retained}_i}(\mathbf{x}) + \text{FFN}_{\text{diverse}_i}(\mathbf{x})) \nonumber \\
&= \sum_{i \in S_k} g(\mathbf{x})_i \cdot [\text{FFN}_{\text{common}}(\mathbf{x}) + (\text{FFN}_{\text{retained}_i}(\mathbf{x}) - \text{FFN}_{\text{common}}(\mathbf{x})) + \text{FFN}_{\text{diverse}_i}(\mathbf{x})] \nonumber \\
&= \text{FFN}_{\text{common}}(\mathbf{x}) \sum_{i \in S_k} g(\mathbf{x})_i + \sum_{i \in S_k} g(\mathbf{x})_i \cdot [\text{FFN}_{\text{retained}_i}(\mathbf{x}) - \text{FFN}_{\text{common}}(\mathbf{x}) + \text{FFN}_{\text{diverse}_i}(\mathbf{x})] \nonumber \\
&= \text{FFN}_{\text{common}}(\mathbf{x}) + \sum_{i \in S_k} g(\mathbf{x})_i \cdot [\text{FFN}_{\text{retained}_i}(\mathbf{x}) - \text{FFN}_{\text{common}}(\mathbf{x}) + \text{FFN}_{\text{diverse}_i}(\mathbf{x})] \nonumber \\
&= \text{FFN}_{\text{common}}(\mathbf{x}) + \sum_{i=1}^N g(\mathbf{x})_i \cdot [\text{FFN}_{\text{retained}_i}(\mathbf{x}) - \text{FFN}_{\text{common}}(\mathbf{x}) + \text{FFN}_{\text{diverse}_i}(\mathbf{x})]
\end{align}
where the third equality follows from distributing the sum, the fourth equality follows from $\sum_{i \in S_k} g(\mathbf{x})_i = 1$, and the final equality holds because $g(\mathbf{x})_i = 0$ for $i \not\in S_k$. Here $\text{FFN}_{\text{common}}(\mathbf{x})$ represents the output from parameters common to all selected experts.}

\diff{For each expert, a ratio $(1-r)$ of parameters are randomly preserved from the original FFN. When $k$ experts are selected, the probability that a parameter is preserved in all $k$ experts is $(1-r)^k$. Therefore, approximately $(1-r)^k \cdot d_f$ dimensions have common preserved parameters among selected experts, where $d_f$ is the intermediate dimension size. Note that beyond these completely common parameters, there may be partial parameter sharing among subsets of the selected experts due to the random preservation process.}

\diff{To understand the error bound $O(\frac{1}{\sqrt{d_f}})$, consider that for any two experts $i,j$, the number of overlapping parameters follows a binomial distribution $B(d_f, (1-r)^2)$. By the Central Limit Theorem, the deviation from the expected value scales with $\sqrt{d_f}$, leading to a relative error of $O(\frac{1}{\sqrt{d_f}})$ in the parameter overlap estimation.}

\subsection{\diff{Extensions to Fine-grained and Shared Experts}}\label{appendix:extensions}
\diff{We discuss the natural extension of \methodname{} to advanced MoE architectures: fine-grained experts and shared experts proposed in DeepSeekMoE~\citep{dai-etal-2024-deepseekmoe}.
For an original dense FFN with hidden dimension $d_h$ and intermediate size $d_f$, DeepSeekMoE introduces granularity parameter $m$ to split each of $N$ experts into finer segments (each with intermediate size $d_f/m$), where $mk$ experts are selected by top-$mk$ routing, and $k_s$ shared experts process all tokens. The total number of experts becomes $mN$ with $mk$ nonzero gates, which reduces to $mN-k_s$ experts and $mk-k_s$ gates when using shared experts.}




\subsubsection{\diff{Extension to Fine-grained MoE}}
\diff{For simplicity of discussion, we assume $d_f$ is divisible by $m$ for fine-grained MoE (a realistic assumption since $m$ is typically a power of 2 and $d_f$ contains powers of 2 as factors).
The output of the MoE layer is expressed as:}
\begin{equation}
\diff{y = \sum_{i=1}^{mN} g(x)_{(i)} \cdot \text{FFN}_{(i)}(x)}
\end{equation}

%When applying \methodname{} to convert from dense to fine-grained, we consider maximizing diversity in a simple manner.
%From the original dense network, we randomly extract elements from 1 to $d_f/m$ from $d_f$ by rows or columns.
%Note that these elements must not overlap.
%After that, initialize $r$ for each using \methodname{}.

\diff{When applying \methodname{} to convert from a dense FFN layer to a fine-grained MoE layer, we conduct the following steps:}

% \begin{enumerate}
% \item Column-wise Sampling.
% Select an index set $\mathcal{S}$ from the original $d_f$ dimensions for $d_f/m$ dimensions, where $|\mathcal{S}| = \lfloor r \cdot d_f/m \rfloor$.

% \item Statistics Calculation.
% Calculate means and standard deviations $(\mu_\text{up}, \sigma_\text{up})$, $(\mu_\text{gate}, \sigma_\text{gate})$, $(\mu_\text{down}, \sigma_\text{down})$ for the weight matrices corresponding to the selected indices $\mathcal{S}$.

% \item Partial Re-Initialization.
% Initialize each expert's weight matrices according to:
% \begin{equation}
% \widetilde{\mathbf{W}}_{\text{type}} = \mathbf{I}_{\mathcal{S}} \odot \mathbf{R}_{\text{type}} +  (1 - \mathbf{I}_{\mathcal{S}}) \odot \mathbf{W}_{\text{type}}
% \end{equation}
% where $\mathbf{R}_{\text{type}}$ is sampled from $\mathcal{N}(\mu_{\text{type}}, (\sigma_{\text{type}})^2)$.

% \end{enumerate}

\begin{enumerate}
\item \diff{Expert Dimension Sampling.
First, randomly sample $d_f/m$ dimensions from the original FFN intermediate dimension $d_f$ for each expert.}

\item \diff{Column-wise Reinitialization Sampling.
For each expert's sampled $d_f/m$ dimensions, select an index set $\mathcal{S}$ where $|\mathcal{S}| = \lfloor r \cdot d_f/m \rfloor$ dimensions to be reinitialized.}

\item \diff{Statistics Calculation.
Calculate means and standard deviations $(\mu_\text{up}, \sigma_\text{up})$, $(\mu_\text{gate}, \sigma_\text{gate})$, $(\mu_\text{down}, \sigma_\text{down})$ for the weight matrices corresponding to the selected indices $\mathcal{S}$.}

\item \diff{Partial Re-Initialization.
Initialize each expert's weight matrices according to:}
\begin{equation}
\diff{\widetilde{\mathbf{W}}_{\text{type}} = \mathbf{I}_{\mathcal{S}} \odot \mathbf{R}_{\text{type}} +  (1 - \mathbf{I}_{\mathcal{S}}) \odot \mathbf{W}_{\text{type}}}
\end{equation}
\diff{where $\mathbf{R}_{\text{type}}$ is sampled from $\mathcal{N}(\mu_{\text{type}}, (\sigma_{\text{type}})^2)$.}
\end{enumerate}

\diff{Note that the portion reinitialized by our method needs to be scaled down due to the increased number of activated experts in top-$mk$ routing resulting in smaller $g(\mathbf{x})_i$.
While the absolute magnitude information in router outputs might adapt during training,
following \cite{he2024upcyclinglargelanguagemodels},
scaling the weights of $W_{\text{down}}$ and $W_{\text{up}}$ might be beneficial.}

\subsubsection{\diff{Combination with Shared Experts}}
\diff{When using both shared experts and fine-grained experts, the output is:}
\begin{equation}
\diff{y = \sum_{i=1}^{k_s} \text{FFN}_{(i)}(x) + \sum_{i=k_s+1}^{mN} g(x)_{(i-k_s)} \cdot \text{FFN}_{(i)}(x)}
\end{equation}

\diff{Here, shared experts are always active and process dimensions ($d_h$, $d_f/m \cdot k_s$), while fine-grained experts each process $d_f/m$ dimensions.}

% We initialize fine-grained experts using the method described above. We can consider directly copying from the dense FFN or applying \methodname{} to initialize shared experts. We apply weight scaling to both types of experts.

\diff{We initialize fine-grained experts using the method described above. For shared experts, we can either randomly sample $d_f/m \cdot k_s$ dimensions from the dense FFN and directly copy the corresponding weights, or apply \methodname{} to those sampled dimensions. We apply weight scaling to both types of experts.}

\diff{Note that whether shared experts maintain the same functionality as dense remains an open research question, and comparing initialization methods for shared experts is left for future work.}

\subsubsection{\diff{Limitations and Future Directions}}
\diff{While we provide basic extensions of our method to fine-grained and shared expert settings, several important research questions remain unexplored. Our method could serve as a baseline for investigating how knowledge from dense models transfers to these advanced MoE architectures. Specifically, analyzing the transformation process from dense to fine-grained or shared experts could provide valuable insights into how these architectures function and develop specialization. For example, tracking how knowledge is distributed across fine-grained experts during training, or understanding what types of information shared experts learn to capture, could deepen our understanding of these MoE variants. Such analyses could also inform better initialization strategies and architectural choices for future MoE models.}