\section{Experimental Setup}

We conducted experiments to demonstrate the effectiveness of \methodname{} described in Section~\ref{sec:method}.
To clarify our model configurations, we introduce a notation where, for example, ``8Ã—152M'' denotes an MoE model with eight experts and whose base dense model size is 152M.
%The following subsections explain the settings of our experiments.


% \subsection{Configuration of Models to compare}
\label{sec:model-architecture}


We selected the Llama~\citep{touvron2023llamaopenefficientfoundation} and Mixtral~\citep{jiang2024mixtralexperts} architectures for dense and MoE models, respectively, for our experiments. 
%%
%Both architectures are based on the Transformer~\citep{NIPS2017_3f5ee243} with several improvements, including RMSNorm~\citep{zhang-sennrich-neurips19}, SwiGLU~\citep{shazeer2020gluvariantsimprovetransformer}, and rotary position embeddings (RoPE)~\citep{su2024roformer}. 
%The notable difference in Mixtral (MoE) from Llama (dense) is that all feedforward network (FFN) layers are replaced by sparsely gated MoE layers.
%%
%
% We employed 8 experts and the dropless token choice Top-2 routing~\citep{megablocks} for the MoE. 
We employed 8 experts and the dropless~\citep{megablocks} token choice top-2 routing ~\citep{shazeer2017} for the MoE.
%Table~\ref{tab:model-details} shows the remaining hyper-parameters of the Dense and MoE models used in our experiments.
Detailed descriptions of the model configurations are provided in Appendix~\ref{appendix:model_configs_details}



We evaluated \diff{four} different methods to build MoE models, namely, training from scratch, \NUname{}~\citep{komatsuzaki2023sparse}, 
\diff{\RNUname{}~\citep{komatsuzaki2023sparse}} and Branch-Train-MiX~\citep{sukhbaatar2024branchtrainmix} to compare the performance with \methodname{}.
Moreover, we also evaluated dense models to provide a reference of the typical performance of LLMs in our configuration and illustrate the performance gains of MoE models.
We initialized all parameters of dense models using a Gaussian distribution $\mathcal{N}(0, 0.02)$.
The dense models are also used as the seed models of MoE models, except when we train MoE models from scratch.
%
When training MoE models from scratch, we used the same initialization method as the dense models, that is, $\mathcal{N}(0, 0.02)$.
%
%\textbf{MoE NU} is similar to \textbf{MoE DU}, but copies all parameters in \textbf{Dense} and duplicates FFN layers 8 times to initialize the MoE layers.
%
%We also evaluated Branch-Train-Mix proposed in~\citet{sukhbaatar2024branchtrainmix}.
\diff{In \RNUname{}, Drawing from \citep{muennighoff2024olmoeopenmixtureofexpertslanguage}, we initialize by copying the dense model parameters and then add Gaussian noise $\mathcal{N}(0, 0.02)$ to 50\% of the weights in each FFN layer.}
In Branch-Train-Mix, we first obtained three distinct expert dense models by further training a seed dense model with 100B extra tokens of either Japanese, English, or code. 
Then, we used the four dense models (the seed dense model and three expert dense models) to initialize the parameters of an MoE model.
Specifically, we averaged all parameters in the four dense models except the FFN layers and duplicated the FFN layers in each model twice to build eight MoE experts.
Note that this method involved extra training steps with 300B more tokens compared to the other MoE construction methods.
% Unless otherwise stated, dense models were trained on 1T tokens, and MoE models were trained on 500B tokens.







% \subsection{Training and Evaluation}

Unless otherwise stated, dense models were trained on 1T tokens, and MoE models were trained on 500B tokens.
Our training data was obtained from publicly available data.
We describe the detailed statistics of the training datasets in Appendix~\ref{appendix:dataset-details}.
%
% For dense models, we trained 1T tokens for the 1.5B model and 2.072T tokens for the 152M, 3.7B, and 13B models.
% Moreover, we trained 500B tokens for all MoE models.
% We used dense models trained on 1T tokens as the base for upcycling our MoE models. 
% For all MoE models except those trained from scratch, we Upcycled from 1T tokens trained dense checkpoints of all sizes.
%
We followed the typical training configurations used in Llama to train dense models and Mixtral for MoE models.
%
Details of the hyper-parameters we used are described in Appendix~\ref{appendix:training_configs_details}.
Moreover, the implementation and the computational environment used in our experiments are described in Appendix~\ref{appendix:training_environment}.


We conducted a comprehensive evaluation using a wide range of tasks in Japanese and English. 
We used 12 evaluation datasets that can be categorized into seven types.
The details of the evaluation datasets and metrics are described in Appendix \ref{appendix:evaluation_details}.
