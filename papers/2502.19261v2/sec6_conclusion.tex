\section{Conclusion}

In this paper, we introduced \methodname{}, a novel method for efficiently constructing Mixture of Experts (MoE) models from pre-trained dense models. Selectively re-initializing parameters of expert feedforward networks, \methodname{} 
 effectively balances knowledge transfer and expert specialization, addressing the key challenges in MoE model development.

Our extensive large-scale experiments demonstrated that \methodname{},    
significantly outperforms previous MoE construction methods. As a result, we achieved an MoE model with 5.9B active parameters that matches the performance of a 13B dense model from the same model family while requiring only about 1/4 of the training FLOPs.

By making all aspects of our research publicly available—including data, code, configurations, checkpoints, and logs—we aim to promote transparency and facilitate further advancements in efficient LLM training. We believe that \methodname offers a practical solution to reduce resource barriers in deploying high-performance LLMs, contributing to broader accessibility and innovation in AI research.