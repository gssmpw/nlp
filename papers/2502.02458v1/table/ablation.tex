\begin{table*}[t]
  \centering
  \resizebox{\textwidth}{!}
  {
  \renewcommand{\arraystretch}{1.0}
  % \scalebox{0.9}
  { 
  \begin{tabular}{l|c|cc|c|ccccccc|c}
    \toprule
    \multirow{2}[2]{*}{Method} & \multirow{2}[2]{*}{LLM} & Visual & \#Vis. & Inference & MMMU & \multicolumn{2}{c}{MMBench} & \multirow{2}[2]{*}{POPE} & \multirow{2}[2]{*}{GQA} & SQA  & OK-  & \multirow{2}[2]{*}{Average} \\
          &       & Encoder & Tok. & TFLOPs$\downarrow$ & VAL & EN & CN    &         &    & IMG  & VQA &  \\
    \midrule
    LLaVA-1.5 & Vicuna & SigLIP & 729   & 10.63 & 36.6  & 66.2 & 58.9  & 86.5 & 62.5   & \textbf{70.5}   & \textbf{56.4} & 62.5 \\
    \rowcolor{cyan!20} SAISA & Vicuna & SigLIP & 729   & 3.40  & \textbf{37.4}  & \textbf{67.5} & \textbf{60.7}  & \textbf{87.0}  & \textbf{62.9}  & 70.0  & 55.8 & \textbf{63.0} \\
    \midrule
    LLaVA-1.5 & Vicuna & Conv & 1024  & 14.76 & 34.6  & 56.6 & 49.6  & \textbf{88.2}  & \textbf{61.1}  & 66.4  & 51.4 & 58.3 \\
    \rowcolor{cyan!20} SAISA & Vicuna & Conv & 1024  & 4.44  & \textbf{35.1}  & \textbf{61.1} & \textbf{54.9}  & 87.0 & 57.7   & \textbf{66.5}  & \textbf{54.4} & \textbf{59.5} \\
    \midrule
    LLaVA-1.5 & Vicuna & CLIP  & 576   & 8.53  & 35.7  & 64.3 & 58.3 & 86.8  & \textbf{62.0}  & 66.8  & 53.4 & 61.0 \\
    \rowcolor{cyan!20} SAISA & Vicuna & CLIP  & 576   & 2.86  & \textbf{36.9}  & \textbf{65.7} & \textbf{59.0} & \textbf{87.2} & 60.9   & \textbf{70.1}  & \textbf{56.8} & \textbf{62.4} \\
    \midrule
    LLaVA-1.5 & Mistral & CLIP  & 576   & 9.17  & 34.8  & 65.9 & 54.9  & \textbf{87.2}  & \textbf{62.0}   & \textbf{71.6}  & 2.5* & 54.1 \\
    \rowcolor{cyan!20} SAISA & Mistral & CLIP  & 576   & 2.10  & \textbf{35.9}  & \textbf{67.5} & \textbf{57.5}  & 86.9 & 61.2   & 71.2  & \textbf{23.9}* & \textbf{57.7} \\
    \midrule
    LLaVA-1.5 & Llama3 & CLIP  & 576   & 9.17  & 36.8  & 70.4 & 64.2 & \textbf{87.2}  & \textbf{63.5}  & 73.3  & \textbf{61.2} & 65.2 \\
    \rowcolor{cyan!20} SAISA & Llama3 & CLIP  & 576   & 2.10  & \textbf{38.3}  & \textbf{71.3} & \textbf{65.2} & 86.8  & 61.8  & \textbf{74.4}  & 60.7 & \textbf{65.6} \\
    \bottomrule
    \end{tabular}
    }
  }
  \caption{
  \textbf{Ablation on LLMs and Visual Encoders.}
  Here, ``Vicuna" = Vicuna-7B, ``Mistral" = Mistral-7B, ``Llama3" = Llama3-8B, ``SigLIP" = SigLIP-ViT-SO400M/14-384, ``Conv" = OpenCLIP-ConvNeXt-XXL-1024, and ``CLIP" = CLIP-ViT-L/14-336.
  \#Vis. Tok. denotes the number of visual tokens involved in a single image.
  SAISA consistently demonstrates superior performance to LLaVA-1.5 across these LLMs and visual encoders, while dramatically reducing computational costs.
  *Both LLaVA-1.5 and SAISA models on Mistral 7B exhibit low performance on OK-VQA, because they respond to most questions in this benchmark with "Unanswerable".
  }
  \label{tab:ablation}
\end{table*}