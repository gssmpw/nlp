\begin{table*}[t]
  \centering
  % \resizebox{\textwidth}{!}
  {
  \renewcommand{\arraystretch}{1.0}
  \scalebox{0.95}
  { 
 \begin{tabular}{ll|c|cccc|cccc}
    \toprule
    \multirow{2}[2]{*}{Method} & \multirow{2}[2]{*}{LLM}  & Inference & \multicolumn{4}{c|}{POPE~\cite{li2023evaluatingobjecthallucinationlarge}}  & GQA & ScienceQA  & TextVQA & OK-VQA \\
          &        & TFLOPs$\downarrow$ & overall & rand & pop & adv      & \cite{hudson2019gqanewdatasetrealworld}      & IMG~\cite{lu2022learnexplainmultimodalreasoning}  & \cite{singh2019vqamodelsread} &  \cite{marino2019okvqavisualquestionanswering} \\
    \midrule
    \multicolumn{7}{l|}{\color{gray} PaLI-X-55B (Specialist SOTA, individually fine-tuned on each dataset) }                       & \textcolor{gray}{72.1} & \textcolor{gray}{-}  & \textcolor{gray}{71.4} & \textcolor{gray}{66.1}  \\
    \midrule
    Shikra & Vicuna-7B   & 4.20  & 84.7  & 86.9  & 84.0  & 83.1  & -     & -     & -     & - \\
    IDEFICS & Llama-7B   & 1.67  & 81.8  & 88.3  & 81.1  & 76.0    & 35.5  & 51.6  & 25.9 & 38.4 \\
    IDEFICS & Llama-65B   & 16.62 & 77.5  & 86.7  & 74.9  & 70.8    & 45.2  & 61.8  & 30.9 & 45.2 \\
    Qwen-VL-Chat & Qwen-7B   & 4.20  & 87.0  & \textbf{89.0} & 87.4  & 84.7    & 57.5  & 68.2  & \textbf{61.5} & 56.6 \\
    LLaVA-1.5 & Vicuna-7B   & 8.53  & 86.8  & 88.2  & 87.2  & \textbf{85.1}   & \textbf{62.0} & 66.8  & 58.2 & 53.4 \\
    \rowcolor{cyan!20} SAISA (Ours) & Vicuna-7B   & 2.86  & \textbf{87.2} & \textbf{89.0} & \textbf{87.6} & 85.0   & 60.9  & \textbf{70.1}  & 56.8 & \textbf{56.8} \\
    \bottomrule
    \end{tabular}
    }
  }
  \caption{
  \textbf{Performance on hallucination and visual question answering benchmarks.}
  We \textcolor{gray}{gray} out the specialist's method, which is individually fine-tuned on each dataset.
  }
  \label{tab:vqa}
\end{table*}