\section{Conclusion}
In this paper, we take a step towards developing multimodal large language models (MLLMs) with efficiency during both training and inference.
To achieve this, we conduct a study of current MLLM architectures and find the key factors for building efficient MLLMs.
By integrating these factors and gradually reducing redundant computations, we propose \textbf{SAISA}, an effective and efficient architecture for MLLMs.
SAISA demonstrates the ability to dramatically reduce the computational costs of existing MLLMs without compromising their capabilities.

\vspace{-0.35cm}
\paragraph{Limitations.}
Despite the promising performance and efficiency of SAISA, several limitations must be acknowledged.
First, SAISA employs a projector with distinct MLPs for each layer of the LLM backbone for simplicity, introducing a number of parameters that cannot be ignored.
The development of a more efficient and effective projector could facilitate the advancement of more powerful MLLMs.
Second, SAISA is not yet capable of processing more complicated visual information, such as high-resolution images, multiple images, interleaved text-and-image content, and videos.
Third, SAISA is not yet capable of processing more modalities such as audio.
Finally, SAISA's capabilities are limited in following complex real-world instructions and solving problems in specific domains such as medicine.
These capabilities could be improved by scaling up pre-training and fine-tuning with high-quality, domain-specific data.