\section{Experiments}
In this section, we conduct comprehensive experiments to compare SAISA with existing MLLMs.
Furthermore, we perform a series of ablation experiments to further validate the effectiveness of the SAISA architecture.

\subsection{Setups}
\paragraph{Model Configuration.}
To make an apple-to-apple comparison between SAISA and LLaVA-1.5~\cite{liu2024improvedbaselinesvisualinstruction}, we use the same settings as LLaVA-1.5.
Specifically, we employ Vicuna-7B-v1.5~\cite{vicuna} as the default LLM and CLIP-ViT-L/14-336~\cite{radford2021learningtransferablevisualmodels} as the default visual encoder.

\vspace{-0.35cm}
\paragraph{Training Details.}
We utilize the same training data as LLaVA-1.5.
During pre-training, we adopt the pre-train dataset with 558k samples from LLaVA~\cite{liu2023visualinstructiontuning}. This stage takes around 1.5 hours on 8 A800 (80G) GPUs. 
During fine-tuning, we use the mixture instruction-following dataset from LLaVA-1.5. The dataset contains 665k samples from LLaVA-Instruct~\cite{liu2023visualinstructiontuning}, ShareGPT~\cite{sharegpt2023}, VQAv2, GQA~\cite{hudson2019gqanewdatasetrealworld}, OK-VQA~\cite{marino2019okvqavisualquestionanswering}, OCR-VQA~\cite{mishra2019ocrvqa}, A-OKVQA~\cite{schwenk2022aokvqabenchmarkvisualquestion}, TextCaps~\cite{sidorov2020textcapsdatasetimagecaptioning}, RefCOCO~\cite{kazemzadeh2014referitgame, mao2016generationcomprehensionunambiguousobject}, and Visual Genome~\cite{krishna2016visualgenomeconnectinglanguage}.
This stage takes around 8.5 hours on 8 A800 (80G) GPUs.
The total training budget  of SAISA is approximately 80 GPU hours.

\input{table/ablation}
\input{table/vision-centric}
\input{table/latency}

%-------------------------------------------------------------------------

\subsection{Main Results}
The performance on benchmarks is shown in Table~\ref{tab:mllms}, Table~\ref{tab:vqa} and Table~\ref{tab:vision-centric}, and the result of the inference latency test is shown in Table~\ref{tab:latency}.

We evaluate SAISA on a range of benchmarks, including: (1) comprehensive benchmarks for instruction-following MLLMs such as MMMU~\cite{yue2024mmmumassivemultidisciplinemultimodal}, MME~\cite{fu2024mmecomprehensiveevaluationbenchmark}, MMBench~\cite{liu2024mmbenchmultimodalmodelallaround}, MMBench-CN~\cite{liu2024mmbenchmultimodalmodelallaround}, and SEED-bench~\cite{li2023seedbenchbenchmarkingmultimodalllms}; (2) hallucination benchmark such as POPE~\cite{li2023evaluatingobjecthallucinationlarge}, which evaluates MLLMs' degree of hallucination on three subsets: random, popular, and adversarial; (3) general visual question answering benchmarks such as GQA~\cite{hudson2019gqanewdatasetrealworld} and ScienceQA IMG~\cite{lu2022learnexplainmultimodalreasoning}; (4) fine-grained visual question answering benchmarks such as OK-VQA~\cite{marino2019okvqavisualquestionanswering} and TextVQA~\cite{singh2019vqamodelsread}, OK-VQA requires fine-grained image understanding and spatial understanding, and TextVQA is an OCR-related benchmark; (5) vision-centric MLLM benchmarks such as MMVP~\cite{tong2024eyeswideshutexploring} and CV-Bench~\cite{tong2024cambrian1fullyopenvisioncentric}.
In the inference latency test, the latency is reported as the time of LLM prefilling during inference with varying numbers of text tokens.
Table~\ref{tab:mllms} shows the comparison on the comprehensive benchmarks for instruct-following MLLMs.
SAISA outperforms BLIP-2~\cite{li2023blip2bootstrappinglanguageimagepretraining}, InstructBLIP~\cite{dai2023instructblipgeneralpurposevisionlanguagemodels}, MiniGPT-4~\cite{zhu2023minigpt4enhancingvisionlanguageunderstanding}, MiniGPT-v2\cite{chen2023minigptv2largelanguagemodel}, Otter~\cite{li2023ottermultimodalmodelincontext}, Shikra~\cite{chen2023shikraunleashingmultimodalllms}, and IDEFICS~\cite{idefics} utilizing LLama-7B and LLama-65B~\cite{touvron2023llamaopenefficientfoundation} across all these benchmarks.
Compared to Qwen-VL-Chat~\cite{Qwen-VL} trained on data with 1.4B samples, SAISA performs better on 4 out of 5 benchmarks.
Compared to LLaVA-1.5~\cite{liu2024improvedbaselinesvisualinstruction}, SAISA performs better on 3 out of 5 benchmarks.
Table~\ref{tab:vqa} shows the comparison on the hallucination and visual question answering benchmarks, and Table~\ref{tab:vision-centric} shows the comparison on vision-centric MLLM benchmarks.
Since most previous models do not evaluate performance on vision-centric MLLM benchmarks, we compare SAISA with LLaVA-1.5.
SAISA achieves the best overall performance compared to other baseline MLLMs, and strikes the optimal balance between effectiveness and efficiency.

\subsection{Ablation Study}
\paragraph{Ablation on LLMs and Visual Encoders.}
As presented in Table~\ref{tab:ablation}, we perform multiple ablation experiments on both LLMs and visual encoders to evaluate the robustness of SAISA.
We tune a set of SAISA models using a variety of LLM backbones and visual encoders.
The ablated LLMs include Vicuna-7B~\cite{vicuna} and two LLMs using grouped query attention (GQA)~\cite{ainslie2023gqatraininggeneralizedmultiquery}, such as Mistral-7B~\cite{jiang2023mistral7b} and Llama3-8B~\cite{llama3v}.
The ablated visual encoders include two ViT-based~\cite{dosovitskiy2021imageworth16x16words} visual backbones such as CLIP-ViT-L/14-336~\cite{radford2021learningtransferablevisualmodels} and  SigLIP-ViT-SO400M/14-384~\cite{zhai2023sigmoidlosslanguageimage}, and a ConvNeXt-based~\cite{liu2022convnet2020s} visual encoder such as ConvNeXt-XXL-1024 from OpenCLIP~\cite{ilharco_gabriel_2021_5143773, schuhmann2022laion5bopenlargescaledataset}.
The experimental results demonstrate that SAISA consistently achieves superior performance to LLaVA-1.5 across different LLM backbones and visual encoders, while dramatically reducing computational costs.

\vspace{-0.35cm}
\paragraph{Ablation on Pre-training Strategies.}
As shown in Table~\ref{tab:ablation_train}, we conduct an ablation study to investigate the effects of SAISA's pre-training strategies.
We tune a SAISA model where the full projector (32 MLPs) is tunable during pre-training, and the other settings keep the same as the original SAISA.
With more randomly initialized parameters, we observe a performance drop when pre-training the full projector.
We attribute this drop to the small amount of pre-training data with only 558k samples.
The ablation study demonstrates the effectiveness of our pre-training strategy, which provides a robust initialization for the subsequent fine-tuning stage.

\vspace{-0.35cm}
\paragraph{Ablation on Projector Designs.}
Previous works find that replacing linear projection with MLP projection improves performance in MLLM~\cite{liu2024improvedbaselinesvisualinstruction} and self-supervised learning~\cite{chen2020simpleframeworkcontrastivelearning, chen2020improvedbaselinesmomentumcontrastive}.
We conduct an experiment to investigate the impact of projector designs in SAISA.
We tune a model under the same configuration as the original SAISA model but replace the MLPs in the projector with linear layers.
Table ~\ref{tab:linear} shows that the model with MLPs in the projector performs better than the model with linear layers, which is consistent with the finding of the previous study~\cite{liu2024improvedbaselinesvisualinstruction}.
Notably, we note that even the SAISA model with linear layers achieves comparable performance to LLaVA-1.5 with MLP projection.
This observation provides additional evidence for the effectiveness of SAISA.

\input{table/ablattion_train}
\input{table/linear}