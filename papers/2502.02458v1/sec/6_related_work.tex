\section{Related Work}

\subsection{Multimodal Large Language Models}
Multimodal Large Language Models (MLLMs) are typically built on Large Language Models (LLMs)~\cite{vicuna, llama3v, jiang2023mistral7b, touvron2023llamaopenefficientfoundation, touvron2023llama2openfoundation, bai2023qwentechnicalreport} by aligning visual features generated by visual encoders~\cite{radford2021learningtransferablevisualmodels, zhai2023sigmoidlosslanguageimage, ilharco_gabriel_2021_5143773, liu2022convnet2020s} with the LLMs.
There are two most common architectures for this purpose, embedding space alignment and cross-attention space alignment.
For embedding space alignment~\cite{liu2023visualinstructiontuning, liu2024improvedbaselinesvisualinstruction, li2023blip2bootstrappinglanguageimagepretraining, dai2023instructblipgeneralpurposevisionlanguagemodels, Qwen-VL, chen2023minigptv2largelanguagemodel, zhu2023minigpt4enhancingvisionlanguageunderstanding}, MLLMs align visual features with the text token embedding space via a projector and concatenate the visual and text tokens as the LLM input.
These models exhibit efficiency during training due to the small number of new parameters but suffer from inference inefficiency because of the long token sequence.
For cross-attention space alignment~\cite{alayrac2022flamingovisuallanguagemodel, OpenFlamingov2, awadalla2023openflamingoopensourceframeworktraining}, MLLMs introduce new cross-attention blocks for the interaction between text and visual modalities, and align the visual features with the cross-attention spaces.
These models achieve efficiency during inference by eliminating the need to unroll visual tokens, but require a substantial amount of data to train the new parameters.
In this paper, we propose SAISA (Self-Attention Input Space Alignment), an architecture for building MLLMs with efficiency during both training and inference.

\subsection{Efficiency Optimization for MLLMs}
To reduce the computational cost of MLLMs, previous works mainly fall into two categories: model architecture and inference stage token reduction.
For model architecture, Qwen-VL-Chat~\cite{Qwen-VL}, LLaMA-VID~\cite{li2023llamavidimageworth2}, BLIP series~\cite{li2023blip2bootstrappinglanguageimagepretraining, dai2023instructblipgeneralpurposevisionlanguagemodels} and MiniGPT-4~\cite{zhu2023minigpt4enhancingvisionlanguageunderstanding} utilize attention-based mechanisms to down-sample visual tokens before they are fed into LLMs.
MemVP~\cite{jie2024memoryspacevisualpromptingefficient} and VLoRA~\cite{ma2024visualperceptionlargelanguage} integrate visual tokens into LLMs' weights through parameter-efficient fine-tuning (PEFT) such as LoRA~\cite{hu2021loralowrankadaptationlarge}, but overlook fine-grained visual information and retain limited spatial understanding.
EE-MLLM~\cite{ma2024eemllmdataefficientcomputeefficientmultimodal} introduces aligners to update visual tokens in the MLLM, but the aligners still involve substantial computational overhead.
For inference stage token reduction, FastV~\cite{chen2024imageworth12tokens} and LLaVA-PruMerge~\cite{shang2024llavaprumergeadaptivetokenreduction} focus on certain "anchor" tokens and prune the other visual tokens.
VoCo-LLaMA~\cite{ye2024vocollamavisioncompressionlarge} compresses vision tokens into compressed tokens.
In this paper, we investigate redundancy within the LLM architecture of the current mainstream MLLM~\cite{liu2024improvedbaselinesvisualinstruction} and propose SAISA, which eliminates redundant computations.
SAISA achieves efficiency and maintains fine-grained visual understanding by preserving the original number of visual tokens.
Notably, SAISA is orthogonal to and compatible with most previous methods~\cite{jaegle2021perceivergeneralperceptioniterative, li2023llamavidimageworth2, li2023blip2bootstrappinglanguageimagepretraining, chen2024imageworth12tokens, shang2024llavaprumergeadaptivetokenreduction, ye2024vocollamavisioncompressionlarge, Qwen-VL, dai2023instructblipgeneralpurposevisionlanguagemodels, zhu2023minigpt4enhancingvisionlanguageunderstanding}.
