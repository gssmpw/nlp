\section{Analyzing Current MLLM Architectures}
\label{sec:architectures}
%-------------------------------------------------------------------------
In this section, we analyze the two most common architectures to align visual features with the language model, and summarize key factors for building efficient MLLMs.

\vspace{-0.35cm}
\paragraph{Embedding Space Alignment.}
As illustrated in Figure~\ref{fig:saisa}(a), models with this architecture introduce a projector to align visual tokens with the text token embedding space.
They concatenate the aligned visual tokens and text tokens, and then feed them into the LLM.
Notable models with this architecture include  LLaVA-1.5~\cite{liu2024improvedbaselinesvisualinstruction}, Qwen-VL~\cite{Qwen-VL}, BLIP-2~\cite{li2023blip2bootstrappinglanguageimagepretraining}, InstructBLIP~\cite{dai2023instructblipgeneralpurposevisionlanguagemodels}, MiniGPT-4~\cite{zhu2023minigpt4enhancingvisionlanguageunderstanding} and MiniGPT-v2~\cite{chen2023minigptv2largelanguagemodel}.
These models introduce only a small number of new parameters to the pre-trained LLMs, allowing training MLLMs from the LLMs with minimal budget.

However, the concatenated token sequence leads to inefficiency during inference.
When the number of visual and text tokens is $v$ and $t$ respectively, the computational complexity of self-attention in LLM is $\mathcal{O}((v+t)^2)$.
This complexity consists of three components, $\mathcal{O}(v^2)$ for the attention among visual tokens, $\mathcal{O}(vt)$ for the interaction between text and visual tokens, and $\mathcal{O}(t^2)$ for the attention among text tokens.
Typically, MLLMs use hundreds or even thousands of visual tokens.
For examples, LLaVA-1.5 uses 576 visual tokens for a single image, and Sphinx~\cite{lin2023sphinxjointmixingweights} uses 2,890 visual tokens.
In contrast, the number of text tokens is much smaller in most VL tasks.
The average numbers of text tokens in MMMU~\cite{yue2024mmmumassivemultidisciplinemultimodal}, POPE~\cite{li2023evaluatingobjecthallucinationlarge} and ScienceQA IMG~\cite{lu2022learnexplainmultimodalreasoning} are 142, 68, and 210, respectively.
As a result, the attention among visual tokens dominates the quadratic computational overhead.
Moreover, since the FFNs of the LLM have a large hidden layer dimension, applying FFNs to visual tokens also brings substantial computational costs.


\begin{figure*}
    \centering
    \includegraphics[width=0.95\linewidth]{fig/overview.pdf}
    \vspace{-0.1cm}
    \caption{\textbf{Overview of SAISA and the mainstream architectures to align visual features with language model.} (a) Aligning visual features with the embedding space of the language model is inefficient during inference, e.g. LLaVA series. (b) Aligning visual features with the attention spaces of new cross-attention blocks is inefficient during training, e.g. Flamingo and OpenFlamingo. (c) SAISA aligns visual features with the self-attention input spaces of the language models, achieving efficiency during both training and inference.
    }
  \label{fig:saisa}
\end{figure*}


\vspace{-0.35cm}
\paragraph{Cross-Attention Space Alignment.}
As illustrated in Figure~\ref{fig:saisa}(b), models with this architecture insert cross-attention blocks and FFNs into the language model, and align visual features with the attention spaces of these cross-attention blocks.
Notable models with this architecture include Flamingo~\cite{alayrac2022flamingovisuallanguagemodel} and OpenFlamingo~\cite{awadalla2023openflamingoopensourceframeworktraining}.
In these models, the attention operation consists of only two components, the interaction between text and visual modalities with complexity $\mathcal{O}(vt)$ in the cross-attention blocks, and the attention among text tokens with complexity $\mathcal{O}(t^2)$ in the self-attention blocks.
Compared to embedding space alignment, there is no attention among visual features in the language model.
By not executing attention among visual tokens and not applying FFNs to visual tokens, these models are more efficient during inference.

However, the inserted cross-attention blocks and FFNs introduce a large number of new parameters to the pre-trained language model.
As a result, training an MLLM with this architecture requires a large amount of data.
For example, OpenFlamingo-9B adds 2 billion parameters to Llama-7B~\cite{touvron2023llamaopenefficientfoundation}, and requires training data with 180M samples.
In terms of model capabilities, previous work~\cite{dai2024nvlmopenfrontierclassmultimodal} finds that models utilizing this architecture perform worse than those using embedding space alignment when trained on the same data.

Based on the analyses above, we summarize the key factors for building efficient MLLMs as follows:
\begin{itemize}
    \item Reducing the number of new parameters and employing pre-trained modules for multimodal interaction lead to efficiency during training.
    \item Reducing computations related to visual tokens, including those in attention blocks and FFNs, leads to efficiency during inference.
\end{itemize}