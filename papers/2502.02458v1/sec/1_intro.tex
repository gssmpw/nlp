\section{Introduction}
\label{sec:intro}

\begin{figure}[t!]
\centering
\includegraphics[width=\linewidth]{fig/figure1_1.pdf} \\
\hfill
\includegraphics[width=0.935\linewidth]{fig/figure1_2.pdf}
\vspace{-0.1cm}
\caption{
\textbf{Top: Performance \textit{vs.} inference efficiency} based on various LLMs and visual encoders where Average Performance means an average of benchmark scores (MMMU, MMBench, MMBench-CN, POPE, GQA, SchienceQA IMG and OK-VQA) and inference efficiency is the inverse of inference TFLOPs.
When trained on the \textbf{same data} and using the \textbf{same number of visual tokens}, SAISA (\textcolor{orange}{orange}) offers a more favorable balance between inference efficiency and performance than LLaVA-1.5 (\textcolor{gray}{gray}).
\textbf{Bottom: Training budget comparison between SAISA and LLaVA-1.5} where we report the training GPU hours, using Vicuna-7B as LLM and CLIP-ViT-L/14-336 as visual encoder.
SAISA achieves higher training efficiency.
}
\label{fig:fig1}
\end{figure}

Multimodal Large Language Models (MLLMs)~\cite{gpt4v, liu2024improvedbaselinesvisualinstruction, Qwen-VL, chen2023minigptv2largelanguagemodel, dai2023instructblipgeneralpurposevisionlanguagemodels} have shown impressive capabilities in understanding and processing visual information.
They typically build on pre-trained Large Language Models (LLMs)~\cite{openai2024gpt4technicalreport, vicuna, bai2023qwentechnicalreport,  touvron2023llama2openfoundation} and align visual features with the LLMs.
There are two primary architectures for aligning visual and text modalities: embedding space alignment and cross-attention space alignment.
Embedding space alignment, e.g., LLaVA~\cite{liu2023visualinstructiontuning, liu2024improvedbaselinesvisualinstruction}, introduces a projector to align visual features with the LLM embedding space and feeds the visual and text tokens into the LLM.
Cross-attention space alignment, e.g., Flamingo~\cite{alayrac2022flamingovisuallanguagemodel}, inserts cross-attention blocks and aligns visual features with the attention spaces of these blocks.

However, despite the promising performance of these MLLMs, they involve a trade-off between training and inference efficiency.
On the one hand, MLLMs with embedding space alignment exhibit training efficiency, since they introduce only a small number of new parameters to the pre-trained LLMs.
For example, LLaVA-1.5-7B is trained in 108 GPU hours from Vicuna-7B~\cite{vicuna}.
However, this architecture significantly increases the number of input tokens, and the computational cost of self-attention grows quadratically with the number of tokens, leading to inefficiency during inference.
On the other hand, MLLMs with cross-attention space alignment achieve inference efficiency, since they do not require unrolling visual tokens, but they are inefficient during training for introducing a large number of new parameters to the pre-trained LLM.
In this paper, we take a step towards building MLLMs with efficiency during both training and inference.

In this paper, we perform a thorough analysis of these two architectures, identifing key factors for building MLLMs with both training and inference efficiency.
To optimize training efficiency, the key factor is minimizing the number of new parameters and employ modules in the pre-trained LLMs for interaction between visual and text modalities.
For improving inference efficiency, the main focus is reducing the computational costs associated with visual tokens, particularly in attention blocks and feed-forward networks (FFNs).
Building on the analysis of these factors, we introduce NAAViT (\textbf{N}o \textbf{A}ttention \textbf{A}mong \textbf{Vi}sual \textbf{T}okens), a self-attention mechanism which eliminates attention among visual tokens to enhance efficiency.
Since attention among visual tokens contributes significantly to the quadratically growing computational cost in self-attention blocks, we investigate whether this type of attention is truly essential for MLLMs.
Our pilot experiment on LLaVA-1.5 demonstrates that NAAViT outperforms vanilla self-attention, indicating that attention among visual tokens is highly redundancy.

Based on the findings above, we introduce SAISA (\textbf{S}elf-\textbf{A}ttention \textbf{I}nput \textbf{S}pace \textbf{A}lignment), an architecture for MLLMs with efficiency during both training and inference.
As illustrated in Figure~\ref{fig:saisa}(c), SAISA employs NAAViT self-attention blocks for multimodal interaction and directly aligns visual features with the input spaces of these blocks.
SAISA not only reduces the computational overhead of self-attention blocks but also significantly lowers the computational cost of FFNs by eliminating the need to apply FFNs to visual tokens.
We train SAISA on the same data as LLaVA-1.5 and validate its effectiveness on various LLMs and visual encoders.
As shown in Figure~\ref{fig:fig1}, SAISA outperforms LLaVA-1.5 in terms of performance, training efficiency, and inference efficiency.
Using Vicuna-7B-v1.5~\cite{vicuna} as LLM and CLIP-ViT-L/14-336~\cite{radford2021learningtransferablevisualmodels} as visual encoder, SAISA reduces training budget by 26\% and inference FLOPs by 66\%, while delivering superior performance.
Moreover, SAISA is orthogonal to and compatible with projectors to down-sample visual tokens, e.g. perceiver resampler~\cite{jaegle2021perceivergeneralperceptioniterative} and Q-Former~\cite{li2023blip2bootstrappinglanguageimagepretraining, dai2023instructblipgeneralpurposevisionlanguagemodels}, and visual token pruning mechanisms, e.g. FastV~\cite{chen2024imageworth12tokens} and LLaVA-PruMerge~\cite{shang2024llavaprumergeadaptivetokenreduction}.

We summarize our contributions as follows.
\begin{itemize}
    \item Based on our analysis of current MLLM architectures, we propose NAAViT to enhance efficiency of MLLMs, revealing the redundancy of self-attention in MLLMs.
    \item We introduce SAISA, an architecture for building MLLMs with both training and inference efficiency by eliminating the computational cost of attention among visual tokens and FFNs on visual tokens. 
    \item We validate the effectiveness of SAISA through comprehensive ablation studies across various LLMs and visual encoders.
\end{itemize}