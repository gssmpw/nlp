
\section{SAISA}
\label{sec:4}
%-------------------------------------------------------------------------

\begin{figure*}
  \centering
  \begin{subfigure}{0.49\linewidth}
    \includegraphics[height=4.5cm, width=\linewidth]{fig/flops.pdf}
    \caption{TFLOPs with different numbers of tokens}
    \label{fig:short-a}
  \end{subfigure}
  \hfill
  \begin{subfigure}{0.49\linewidth}
    \includegraphics[height=4.5cm, width=\linewidth]{fig/flops_ratio.pdf}
    \caption{FLOPs ratio of SAISA and LLaVA-1.5 with different numbers of tokens}
    \label{fig:flops}
  \end{subfigure}
  
  \caption{\textbf{Inference computational costs comparison between SAISA and LLaVA-1.5} with different numbers of visual and text tokens, where t denotes the number of text tokens.
  SAISA achieves higher computational efficiency than LLAVA-1.5.}
  \label{fig:flops}
\end{figure*}

\subsection{Architecture}
As mentioned earlier, besides the attention operation, another factor that contributes to inference inefficiency is applying FFNs to visual tokens.
Based on NAAViT, which eliminates the attention among visual tokens, we propose SAISA (Self-Attention Input Space Alignment), an architecture for further enhancing MLLM efficiency.
In SAISA, we also eliminate FFNs' computations on visual tokens.

We illustrate the SAISA architecture in Figure~\ref{fig:saisa}(c).
SAISA contains a visual encoder to extract visual features, a projector, and an LLM.
Each layer of the LLM consists of a self-attention block and an FFN.
We utilize NAAViT in the self-attention blocks.
The purpose of the projector is to directly align the visual features with the input spaces of different self-attention blocks in the LLM.

Specifically, we assume $n$ is the number of layers in the LLM, $h$ is the hidden size of the LLM, $d$ is the dimension of visual features, $v$ is the number of visual tokens, and $t$ is the number of text tokens.

For an input image $I$, we first employ the visual encoder VE to extract visual features:
\begin{equation}
Z=\text{VE}(I)\in \mathbb{R}^{v \times d}
\end{equation}
Then, we use the projector P to directly align the visual features with the input spaces of different NAAViT self-attention blocks:
\begin{equation}
V=\text{P}(Z)\in \mathbb{R}^{n \times v \times h}
\end{equation}
For the $i$-th NAAViT self-attention block NAAViT$_i$, we input the aligned visual features $V_i\in \mathbb{R}^{v \times h}$ and hidden states of the text tokens $T_i\in \mathbb{R}^{t \times h}$:
\begin{equation}
H_i=\text{NAAViT}_i(V_i,T_i)\in \mathbb{R}^{t \times h}, i=1,2,3,\dots,n
\end{equation}
Notably, the NAAViT self-attention block only outputs the text tokens' hidden states $H_i$ for the subsequent FFN, and we apply the FFN to update the text tokens:
\begin{equation}
T_{i+1}=\text{FFN}_i(H_i)\in \mathbb{R}^{t \times h}, i=1,2,3,\dots,n
\end{equation}

\subsection{Projector}
Since each LLM layer operates in distinct self-attention spaces, the projector must flexibly align the visual features with each of the spaces.
For simplicity, we employ distinct two-layer MLPs for each layer of the LLM. 
When the LLM has $n$ layers, the projector contains $n$ MLPs.
Following LLaVA-1.5~\cite{liu2024improvedbaselinesvisualinstruction}, we set the intermediate size of each MLP to be the same as the hidden state size of the LLM.

Specifically, for the $i$-th layer in the LLM, the projector is executed as:
\begin{equation}
V_i=\text{MLP}_i(Z)= \varphi(Z W_{i,1}) W_{i,2}, i=1,2,3,\dots,n
\end{equation}
where $Z$ is the visual features from the visual encoder, $\varphi$ is the activation function like GELU~\cite{hendrycks2023gaussianerrorlinearunits}, $W_{i,1}\in \mathbb{R}^{d \times h}$ and $W_{i,2}\in \mathbb{R}^{h \times h}$ are the weight matrices of the two fully connected layers.

%-------------------------------------------------------------------------
\subsection{Training Procedure}
The training procedure of SAISA consists of two stages: pre-training and fine-tuning.

\vspace{-0.35cm}
\paragraph{Pre-training.}
The objective of pre-training is to transform an LLM into an MLLM with a foundational comprehension of images, providing an initialization for the multimodal fine-tuning stage.
Following LLaVA-1.5, only the multimodal projector is trainable during this stage.
To improve training efficiency, we further reduce the number of trainable parameters.
Specifically, we train a shared MLP for all layers of the LLM.

\vspace{-0.35cm}
\paragraph{Fine-tuning.}
The objective of fine-tuning is to enable the model to follow visual instructions from users.
As an initialization, we replicate the pre-trained shared MLP $N$ times to initialize the MLPs in the projector, where $N$ denotes the number of layers of the LLM.
Following LLaVA-1.5, we utilize visual instruction data to train the model, and both the LLM and the projector are trainable during this stage.

\input{table/performance}
\input{table/vqa}

%-------------------------------------------------------------------------
\subsection{Comparison of Computational Cost}
In this section, we compare the computational costs of SAISA and LLaVA-1.5.
We consider the computations of the LLM and the projector, as the computations of the visual encoder are identical in comparison.
We consider the computations of the self-attention blocks and the FFNs in the LLM.

For LLM with $n$ layers, we assume $h$ is the hidden state size, $m$ is the intermediate size of the FFNs, $t$ is the number of text tokens, and $v$ is the number of visual tokens.
To comprehensively consider LLMs with and without grouped query attention (GQA)~\cite{ainslie2023gqatraininggeneralizedmultiquery}, we assume $k$ is the output dimension of key/value matrices.
For the projector, we assume $d$ is the dimension of the input visual features.

For LLaVA-1.5, the FLOPs of the LLM are calculated as $2n(t+v)h(2h+3m+2k)+4n(t+v)^2h$, and the FLOPs of the projector are $2vhd+2vh^2$. The total FLOPs are $2n(t+v)h(2h+3m+2k)+4N(t+v)^2h+2vhd+2vh^2$

For SAISA, visual tokens are multiplied only by key and value matrices, and the key-value sequence length is $v$ in the attention operation.
Therefore, the FLOPs of the LLM can be estimated by $2nth(2h+3m+2k)+4nvhk+4nt(t+v)h$.
The FLOPs of the projector are $2nvhd+2nvh^2$.
The total FLOPs are $2nth(2h+3m+2k)+4nvhk+4nt(t+v)h+2nvhd+2nvh^2$.

Figure~\ref{fig:flops} compares the FLOPs of SAISA and LLaVA-1.5 with different numbers of tokens, based on Vicuna-7B-v1.5~\cite{vicuna}.
SAISA achieves a higher computational efficiency than LLAVA-1.5 when processing the same numbers of tokens.
For example, SAISA's FLOPs are only 34\% of LLaVA-1.5's when using CLIP-ViT-L/14-336~\cite{radford2021learningtransferablevisualmodels} as visual encoder and the number of text tokens is 64.