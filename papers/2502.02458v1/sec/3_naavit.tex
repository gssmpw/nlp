\section{No Attention Among Visual Tokens}
\label{sec:rethinking}
In this section, we propose NAAViT (No Attention Among Visual Tokens) self-attention and perform a pilot experiment to investigate whether attention among visual tokens is necessary for MLLMs.
\subsection{Preliminary}
\paragraph{Vanilla Self-Attention.}
In a self-attention block, the input visual-text token sequence is formed as $X=[V,T]\in \mathbb{R}^{(v+t) \times h}$,
where $[\cdot,\cdot]$ denotes concatenation along the sequence dimension. To derive the query, key, and value representations, three linear layers are applied to obtain $X_q$,  $X_k$, and  $X_v$, respectively:
\begin{equation}
X_q=[V,T]W_Q, X_k=[V,T]W_K, X_v=[V,T]W_V
\end{equation}
Then, the attention operation is executed as:
\begin{equation}
\text{Attention}(X) = \text{softmax}\left(\frac{X_q X_k^T}{\sqrt{d}}\right) X_v\in \mathbb{R}^{(v+t) \times h}
\label{eq:attention}
\end{equation}
Typically, a causal attention mask is applied and queries can only attend to keys preceding them in the sequence.
The outputs are multiplied by another linear layer $W_O$ to update the hidden states through a residual connection:
\begin{equation}
\text{SA}(X) = \text{Attention}(X)W_O+X
\label{eq:2}
\end{equation}

\subsection{NAAViT}
As analyzed in Section~\ref{sec:architectures}, embedding space alignment achieves superior performance and training efficiency, making it the most popular architecture in MLLMs recently.
However, this architecture is inefficient during inference, largely attributable to the computational overhead of self-attention among visual tokens. In contrast, cross-attention space alignment does not perform attention among visual tokens in the language model, leading to inference efficiency.
A question arises here: \textbf{Is attention among visual tokens necessary for MLLMs?}

To answer this question, we propose NAAViT (No Attention Among Visual Tokens), which eliminates attention among visual tokens.
We illustrate the architecture of NAAViT in Figure~\ref{fig:naavit}.
Specifically, for the visual-text token sequence $X=[V,T]\in \mathbb{R}^{(v+t) \times h}$, queries $X_q$, keys $X_k$ and values $X_v$ are obtained as:
\begin{equation}
X_q=VW_Q, X_k=[V,T]W_K, X_v=[V,T]W_V
\end{equation}
The attention operation is executed as Equation~\ref{eq:attention}, but with NAAViT attention mask, where the queries can attend to visual tokens and text tokens preceding them.

\begin{figure}[t]
  \centering
    \includegraphics[width=0.65\linewidth]{fig/naavit.pdf}
    \vspace{-0.1cm}
   \caption{\textbf{NAAViT self-attention block.} NAAViT uses only text tokens as queries. The queries can attend to visual tokens and text tokens preceding them. Visual tokens are not updated in this block.}
   \label{fig:naavit}
\end{figure}

When the number of visual and text tokens is $v$ and $t$ respectively, the vanilla self-attention exhibit a computational complexity of $\mathcal{O}((v+t)^2)$ for the attention operation in Equation~\ref{eq:attention}.
By eliminating the attention among visual tokens, NAAViT reduces the complexity to $\mathcal{O}(t(v+t))$.
Furthermore, since NAAViT reduces the query length from $v+t$ to $t$, it also significantly reduces computational costs associated with linear layers $W_Q$ and $W_O$.

\input{table/attention}

\subsection{Pilot Experiment}
We train a model under the same configurations as LLaVA-1.5-7B, but replace the vanilla self-attention blocks with NAAViT self-attention blocks.

In Table~\ref{tab:attention}, we compare NAAViT and vanilla self-attention on multiple MLLM benchmarks, including MMMU~\cite{yue2024mmmumassivemultidisciplinemultimodal}, 
MMBench~\cite{liu2024mmbenchmultimodalmodelallaround}, MMBench-CN~\cite{liu2024mmbenchmultimodalmodelallaround}, POPE~\cite{li2023evaluatingobjecthallucinationlarge}, ScienceQA IMG~\cite{lu2022learnexplainmultimodalreasoning} and OK-VQA~\cite{marino2019okvqavisualquestionanswering}.
Despite eliminating the attention among visual tokens, the model employing NAAViT outperforms the one using vanilla self-attention. 
Given that NAAViT substantially reduces computational overhead, it offers a favorable balance between performance and efficiency.

In conclusion, attention among visual tokens is highly redundant for building MLLMs.
In the following section, we introduce SAISA (Self-Attention Input Space Alignment) for efficient MLLMs based on NAAViT.