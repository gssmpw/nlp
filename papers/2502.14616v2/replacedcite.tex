\section{RELATED WORK}
\subsection{Transparent Object Segmentation}

Accurate detection or segmentation is usually the first step in perceiving and manipulating untextured transparent objects. On the one hand, many existing works utilize specific visual cues to segment transparent objects. For instance, TransLab ____ and EBLNet ____ demonstrated the effectiveness of boundaries for locating transparent objects. GDNet ____ and RFENet ____ proposed novel feature fusion modules to enhance performance by better utilizing contextual and reciprocal features, respectively. On the other hand, some methods obtain additional information gains by means of different input modalities. PGSNet ____ employed a polarized camera to extract optical cues beneficial for segmentation. In ____, thermal images were combined by a multi-modal fusion module to assist in detecting glass surfaces. Differently, our method only takes a single RGB image as input, without relying on additional modalities.

\subsection{Transparent Object Depth Estimation}

% The methodologies for depth estimation of transparent objects can be roughly classified into three categories, including depth completion, NeRF-based and matching-based methods. ClearGrasp ____ pioneered the use of RGB-D input for depth completion, addressing the failure of standard depth sensors for perceiving transparent objects. Successive improvements have come from LIDF-Refine ____ and TransparentNet ____ by lifting depth maps to point clouds and performing completion. A more recent work TODE ____ leveraged swin trasnformer ____ and introduced a contextual fusion module to better capture global information. Following recent advancements in ____, DexNeRF ____ and EvoNeRF ____ adopted NeRF to represent scenes containing transparent objects, though the optimization processes were time-consuming. GraspNeRF ____ aimed to speed up inference by utilizing generalizable NeRF. Another recent trend is the adoption of general stereo and multi-view matching architectures ____, by regressing depth based on the cost volume. TAStereo ____ could complete the depth of transparent objects by detecting and homogenizing the corresponding regions. SimNet ____ explored a multitask framework based on stereo input to facilitate transparent object manipulation, while MVTrans ____ extended it by introducing multiple views. In contrast, our method bypasses the original incomplete depth and simultaneously predicts depth and segmentation.
The techniques for depth estimation of transparent objects can be roughly classified into depth completion and NeRF-based methods. ClearGrasp ____ pioneered the use of RGB-D input for transparent object depth completion. Successive improvements have come from LIDF-Refine ____ and TransparentNet ____ by lifting depth maps to point clouds and performing completion. A more recent work TODE ____ leveraged swin transformer ____ to better capture the global information. Following recent advancements in NeRF ____, DexNeRF ____ and EvoNeRF ____ employed implicit functions to represent transparent objects, though the optimization processes were time-consuming. GraspNeRF ____ and ResidualNeRF ____ later sped up inference by utilizing the generalizable NeRF and decoupling the background, respectively. Most methods predict depth only once, while we take an iterative way for further refinement.

\subsection{Multi-task Predictions for Transparent Objects}

Multi-task dense predictions aim to learn multiple tasks jointly in a unified framework ____. ClearGrasp ____ adopted edges, masks and surface normals as intermediate representations for optimizing depth. SimNet explored a multi-task framework based on stereo input to support transparent object manipulation, while recently MVTrans ____ extended it by introducing multiple views. However, none of the above methods for transparent objects leveraged inter-task interactions. In contrast, we propose a fusion module to fully exploit the complementary information between different tasks.

% \subsection{Fusion of Semantics and Geometry}

% Semantic segmentation and depth estimation tasks have long been proven to promote each other ____. On the one hand, Jiao et al. ____ trained a synergy model to leverage the semantic information more effectively for monocular depth estimation. Similarly, SDCDepth ____ employed a semantic divide-and-conquer approach to predict depth within each mask. On the other hand, Wang et al. ____ proposed a depth-aware CNN to integrate geometry information into the segmentation. SGNet ____ adopted an efficient operator that can adjust the receptive field according to spatial information. Recently, TaskPrompter ____ introduced a novel multi-task framework utilizing task prompts in transformer. Inspired by the thought of fusing semantic and geometric information, we present a new attention-based semantic and geometric fusion method for the challenging perception task of transparent objects.

% \subsection{Iterative Update and Refinement}

% The deep learning-based iterative framework was first introduced by Raft ____, which updated an optical flow field using GRU. Subsequent works extended it to stereo ____ and multi-view ____ depth estimation by performing a lookup operation. Until recently, the iterative strategy is still popular with IGEV ____ proposing to update the disparity map through a combined volume. Our method draws inspiration from the above works but is essentially novel in terms of the updating way.

\begin{figure*}[!htb]
    \centering
    \includegraphics[width=7in]{images/model/model.pdf}
    \caption{\textbf{ Overview of our proposed end-to-end framework.} (a) Given an RGB input, our model jointly predicts depth and segmentation mask through encoding, reassembling, and iterative fusion decoding. (b) The encoder uses ViT ____ to extract vision tokens of four layers. (c) Then in the reassemble module, the tokens are transformed into multi-scale feature maps, forming two pyramids for depth and segmentation, respectively. (d) A novel semantic and geometric fusion module is designed in the decoder for better leveraging the complementary information of both tasks. (e) The shared-weight decoder is updated iteratively by lightweight gates to gradually refine the initial results. Final predictions are obtained by two heads after the last iteration.}
    \label{model}
    \vspace{-8pt}
\end{figure*}


% \begin{figure*}[!htb]
%     \centering
%     \begin{minipage}{0.13\linewidth}
% 	 	\vspace{2pt}
% 	 	\centerline{\includegraphics[width=\textwidth]{images/syntodd/left_rgb.png}}
% 	 	\centerline{\scriptsize RGB Input1}
%             \vspace{2pt}
% 	 	\centerline{\includegraphics[width=\textwidth]{images/syntodd/right_rgb.png}}
% 	 	\centerline{\scriptsize RGB Input2}
% 	\end{minipage}
%     \begin{minipage}{0.13\linewidth}
% 	 	\vspace{2pt}
% 	 	\centerline{\includegraphics[width=\textwidth]{images/syntodd/simnet_depth_pred.png}}
% 	 	\centerline{\scriptsize SimNet Depth}
%             \vspace{2pt}
% 	 	\centerline{\includegraphics[width=\textwidth]{images/syntodd/simnet_seg_pred.png}}
% 	 	\centerline{\scriptsize SimNet Seg}
% 	\end{minipage}
%     \begin{minipage}{0.13\linewidth}
% 	 	\vspace{2pt}
% 	 	\centerline{\includegraphics[width=\textwidth]{images/syntodd/mvtrans_depth_pred.png}}
%             \centerline{\scriptsize MVTrans Depth}
% 	 	\vspace{2pt}
% 	 	\centerline{\includegraphics[width=\textwidth]{images/syntodd/mvtrans_seg_pred.png}}
% 	 	\centerline{\scriptsize MVTrans Seg}
% 	\end{minipage}
%     \begin{minipage}{0.13\linewidth}
% 	 	\vspace{2pt}
% 	 	\centerline{\includegraphics[width=\textwidth]{images/syntodd/invpt_depth_pred.png}}
%             \centerline{\scriptsize InvPT Depth}
% 	 	\vspace{2pt}
% 	 	\centerline{\includegraphics[width=\textwidth]{images/syntodd/invpt_seg_pred.png}}
% 	 	\centerline{\scriptsize InvPT Seg}
% 	\end{minipage}
%     \begin{minipage}{0.13\linewidth}
% 	 	\vspace{2pt}
% 	 	\centerline{\includegraphics[width=\textwidth]{images/syntodd/taskprompter_depth_pred.png}}
% 	 	\centerline{\scriptsize TaskPrompter Depth}
%             \vspace{2pt}
% 	 	\centerline{\includegraphics[width=\textwidth]{images/syntodd/taskprompter_seg_pred.png}}
% 	 	\centerline{\scriptsize TaskPrompter Seg}
% 	\end{minipage}
%     \begin{minipage}{0.13\linewidth}
% 	 	\vspace{2pt}
% 	 	\centerline{\includegraphics[width=\textwidth]{images/syntodd/our_depth_pred.png}}
%             \centerline{\scriptsize Our Depth}
% 	 	\vspace{2pt}
% 	 	\centerline{\includegraphics[width=\textwidth]{images/syntodd/our_seg_pred.png}}
% 	 	\centerline{\scriptsize Our Seg}
% 	\end{minipage}
%     \begin{minipage}{0.13\linewidth}
% 	 	\vspace{2pt}
% 	 	\centerline{\includegraphics[width=\textwidth]{images/syntodd/depth_gt.png}}
%             \centerline{\scriptsize GT Depth}
% 	 	\vspace{2pt}
% 	 	\centerline{\includegraphics[width=\textwidth]{images/syntodd/seg_gt.png}}
% 	 	\centerline{\scriptsize GT Seg}
% 	\end{minipage}

%     \caption{Qualitative comparison of depth maps and segmentation masks on Syn-TODD dataset. Seg and GT stand for segmentation and ground-truth respectively. SimNet and MVTrans take both RGB images as input, while the other methods only take the first one as input.}
%     \label{syntodd_fig}
% \end{figure*}