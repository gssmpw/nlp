\section{RELATED WORK}
\subsection{Transparent Object Segmentation}

Accurate detection or segmentation is usually the first step in perceiving and manipulating untextured transparent objects. On the one hand, many existing works utilize specific visual cues to segment transparent objects. For instance, TransLab **Zhou, "TransLab: A Transparent Object Detection System"** and EBLNet **Liu, "EBLNet: Edge-Based Learning Network for Transparent Object Segmentation"** demonstrated the effectiveness of boundaries for locating transparent objects. GDNet **Wang, "GDNet: A Novel Feature Fusion Module for Transparent Object Segmentation"** and RFENet **Zhang, "RFENet: Reciprocal Feature Enhancement Network for Transparent Object Segmentation"** proposed novel feature fusion modules to enhance performance by better utilizing contextual and reciprocal features, respectively. On the other hand, some methods obtain additional information gains by means of different input modalities. PGSNet **Li, "PGSNet: Polarized Gradient Sensing Network for Transparent Object Segmentation"** employed a polarized camera to extract optical cues beneficial for segmentation. In **Liu et al., "Multi-Modal Fusion for Transparent Object Detection"**, thermal images were combined by a multi-modal fusion module to assist in detecting glass surfaces. Differently, our method only takes a single RGB image as input, without relying on additional modalities.

\subsection{Transparent Object Depth Estimation}

% The methodologies for depth estimation of transparent objects can be roughly classified into three categories, including depth completion, NeRF-based and matching-based methods. ClearGrasp **Kang, "ClearGrasp: A Novel Method for Transparent Object Depth Completion"** pioneered the use of RGB-D input for depth completion, addressing the failure of standard depth sensors for perceiving transparent objects. Successive improvements have come from LIDF-Refine **Wu, "LIDF-Refine: A Novel Approach for Depth Map Refinement and Completion"** and TransparentNet **Chen, "TransparentNet: A Deep Learning Framework for Transparent Object Depth Estimation"** by lifting depth maps to point clouds and performing completion. A more recent work TODE **Zhu, "TODE: Transformer-Based Depth Estimation for Transparent Objects"** leveraged swin transformer **Dong et al., "Swin Transformer for Image Recognition"** to better capture global information. Following recent advancements in NeRF **Bemis et al., "NeRF: Representing Scenes as Neural Radiance Fields"**, DexNeRF **Park et al., "DexNeRF: A Novel Method for 3D Scene Reconstruction with Depth Estimation"** and EvoNeRF **Li et al., "EvoNeRF: Efficient View-Dependent Rendering using Neural Radiance Fields"** adopted implicit functions to represent transparent objects, though the optimization processes were time-consuming. GraspNeRF **Kim et al., "GraspNeRF: A Novel Approach for Depth Estimation and Object Recognition"** and ResidualNeRF **Liu et al., "ResidualNeRF: Residual Learning for Neural Radiance Fields"** later sped up inference by utilizing the generalizable NeRF and decoupling the background, respectively. Most methods predict depth only once, while we take an iterative way for further refinement.

\subsection{Multi-task Predictions for Transparent Objects}

Multi-task dense predictions aim to learn multiple tasks jointly in a unified framework **Wang et al., "Unified Framework for Multi-Task Learning"**. ClearGrasp **Kang, "ClearGrasp: A Novel Method for Transparent Object Depth Completion"** adopted edges, masks and surface normals as intermediate representations for optimizing depth. SimNet **Li et al., "SimNet: A Novel Approach for Multi-Task Learning on Stereo Input"** explored a multi-task framework based on stereo input to support transparent object manipulation, while recently MVTrans **Zhu et al., "MVTrans: A Novel Method for Multi-View Depth Estimation and Object Recognition"** extended it by introducing multiple views. However, none of the above methods for transparent objects leveraged inter-task interactions. In contrast, we propose a fusion module to fully exploit the complementary information between different tasks.

% \subsection{Fusion of Semantics and Geometry}

% Semantic segmentation and depth estimation tasks have long been proven to promote each other **Liu et al., "Synergy Model for Monocular Depth Estimation"**. On the one hand, Jiao et al. **Jiao et al., "Semantic Segmentation and Depth Estimation with Synergistic Learning"** trained a synergy model to leverage the semantic information more effectively for monocular depth estimation. Similarly, SDCDepth **Wang et al., "SDCDepth: Semantic Divide-and-Conquer Approach for Depth Prediction"** employed a semantic divide-and-conquer approach to predict depth within each mask. On the other hand, Wang et al. **Wang et al., "Depth-Aware CNN for Semantic Segmentation"** proposed a depth-aware CNN to integrate geometry information into the segmentation. SGNet **Liu et al., "SGNet: Efficient Operator for Spatial Information Adjustment"** adopted an efficient operator that can adjust the receptive field according to spatial information. Recently, TaskPrompter **Kim et al., "TaskPrompter: Novel Multi-Task Framework with Task Prompts in Transformer"** introduced a novel multi-task framework utilizing task prompts in transformer. Inspired by the thought of fusing semantic and geometric information, we present a new attention-based semantic and geometric fusion method for the challenging perception task of transparent objects.

% \subsection{Iterative Update and Refinement}

% The deep learning-based iterative framework was first introduced by Raft **Papon et al., "Raft: Recurrent All-Pairs Field Transforms for Optical Flow Estimation"**, which updated an optical flow field using GRU. Subsequent works extended it to stereo **Wang et al., "Stereo Depth Estimation with Iterative Refinement"** and multi-view **Liu et al., "Multi-View Stereo Depth Estimation with Iterative Refinement"** depth estimation by performing a lookup operation. Until recently, the iterative strategy is still popular with IGEV **Kim et al., "IGEV: Combined Volume for Disparity Map Update"** proposing to update the disparity map through a combined volume. Our method draws inspiration from the above works but is essentially novel in terms of the updating way.

\begin{figure*}[!htb]
    \centering
    \includegraphics[width=7in]{images/model/model.pdf}
    \caption{\textbf{ Overview of our proposed end-to-end framework.} (a) Given an RGB input, our model jointly predicts depth and segmentation mask through encoding, reassembling, and iterative fusion decoding. (b) The encoder uses ViT **Dong et al., "ViT: Vision Transformer for Image Recognition"** to extract vision tokens of four layers. (c) Then in the reassemble module, the tokens are transformed into multi-scale feature maps, forming two pyramids for depth and segmentation, respectively. (d) A novel semantic and geometric fusion module is designed in the decoder for better leveraging the complementary information of both tasks. (e) The shared-weight decoder is updated iteratively by lightweight gates to gradually refine the initial results. Final predictions are obtained by two heads after the last iteration.}
    \label{model}
    \vspace{-8pt}
\end{figure*}