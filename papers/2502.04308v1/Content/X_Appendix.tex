\newpage
\appendix
\onecolumn
\begin{center}{\bf \Large Appendix}\end{center}
%\section*{Appendix}
\vspace{0.15in}


\paragraph{Organization} 
The appendix is structured as follows: 
We first present the derivations excluded from the main paper due to space limitation in Section~\ref{app:proof}.
Section~\ref{app:ho-intro} introduces the concept and examples of higher-order networks.
Additional explanations on related work are provided in Section~\ref{app:related}. 
Section~\ref{app:detail-HOG-Diff} details the generation process, including the architecture of the proposed denoising network, as well as the training and sampling procedures.
Computational efficiency is discussed in Section~\ref{app:complexity}.
Section~\ref{app:exp_set} outlines the experimental setup, and Section~\ref{app:vis} concludes with visualizations of the generated samples.


\section{Formal Statements and Proofs}
\label{app:proof}
This section presents the formal statements of key theoretical results and their detailed derivations. 
We will recall and more precisely state the propositions before presenting the proof.

\subsection{Diffusion Bridge Process}

In the following, we derive the Generalized Ornstein-Uhlenbeck (GOU) bridge process using Doob's $h$-transform \cite{doob-h-transform1984} and analyze its relationship with the Brownian bridge process.

Recall that the generalized Ornstein-Uhlenbeck (GOU) process is the time-varying OU process.
It is a stationary Gaussian-Markov process whose marginal distribution gradually tends towards a stable mean and variance over time. 
The GOU process $\mathbb{Q}$ is generally defined as follows \cite{GOU1988,IRSDE+ICML2023}:
\begin{equation}
\mathbb{Q}: \mathrm{d}\bm{G}_t=\theta_t\left(\bm{\mu}-\bm{G}_t\right)\mathrm{d}t+g_t\mathrm{d}\bm{W}_t,
\end{equation}
where $\bm{\mu}$ is a given state vector, $\theta_t$ denotes a scalar drift coefficient and $g_t$ represents the diffusion coefficient. At the same time, we require $\theta_t,g_t$ to satisfy the specified relationship $2\sigma^2=g_t^2/\theta_t$, where $\sigma^2$ is a given constant scalar. As a result, its transition probability possesses a closed-form analytical solution:
\begin{equation}
\begin{split}
p\left(\bm{G}_{t}\mid \bm{G}_s\right)
& =\mathcal{N}(\mathbf{m}_{s:t},v_{s:t}^{2}\bm{I}), \\
\mathbf{m}_{s:t} 
& = \bm{\mu}+\left(\bm{G}_s-\bm{\mu}\right)e^{-\bar{\theta}_{s:t}},\\
v_{s:t}^{2} 
&= \sigma^2 \left(1-e^{-2\bar{\theta}_{s:t}}\right).
\end{split}
\end{equation}
Here, $\bar{\theta}_{s:t}=\int_s^t\theta_zdz$. When the starting time $t=0$, we substitute $\bar{\theta}_{0:t}$ with $\bar{\theta}_t$ for notation simplicity. 





\begin{customthe}[Proposition~\ref{pro:OUB}]
%\input{Content/blocks/theorem1}
Let $\bm{G}_t$ evolve according to the generalized OU process in \cref{eq:GOU-SDE}, subject to the terminal conditional $\bm{\mu}=\bm{G}_{\tau_k}$. 
%
The conditional marginal distribution $p(\bm{G}_t\mid\bm{G}_{\tau_k})$ then evolves according to the following SDE:
\begin{equation}
\mathrm{d}\bm{G}_t = \theta_t \left( 1 + \frac{2}{e^{2\bar{\theta}_{t:\tau_k}}-1}  \right)(\bm{G}_{\tau_k} - \bm{G}_t)  \mathrm{d}t 
+ g_{k,t} \mathrm{d}\bm{W}_t.
\label{eq:GOUB-SDE}
\end{equation}
The conditional transition probability $p(\bm{G}_t \mid \bm{G}_{\tau_{k-1}}, \bm{G}_{\tau_k})$ has analytical form as follows:
\begin{equation}
\begin{split}
&p(\bm{G}_t \mid  \bm{G}_{\tau_{k-1}}, \bm{G}_{\tau_k}) 
= \mathcal{N}(\bar{\mathbf{m}}_t, \bar{v}_t^2 \bm{I}),\\
&\bar{\mathbf{m}}_t = 
\bm{G}_{\tau_k} + (\bm{G}_{\tau_{k-1}}-\bm{G}_{\tau_k})e^{-\bar{\theta}_{\tau_{k-1}:t}} 
\frac{v_{t:\tau_k}^2}{v_{\tau_{k-1}:\tau_k}^2}, \\
&\bar{v}_t^2 = {v_{\tau_{k-1}:t}^2 v_{t:\tau_k}^2}/{v_{\tau_{k-1}:\tau_k}^2}.
\end{split}
\end{equation}
Here, $\bar{\theta}_{a:b}=\int_a^b \theta_s  \mathrm{d}s$, and $v_{a:b}=\sigma^2(1-e^{-2\bar{\theta}_{a:b}})$.
\end{customthe}

\begin{proof}
To simplify the notion, in the $k$-th generation step, we adopt the following conventions: 
 $T=\tau_k$, $\mathbf{x}_t = \bm{G}_t^{(k)}$, $0=\tau_{k-1}$, $\mathbf{x}_0=\bm{G}_{\tau_{k-1}}$, $\mathbf{x}_T=\bm{G}_{\tau_k}$. 

From \cref{eq:GOU-p}, we can derive the following conditional distribution
\begin{equation}
    p(\mathbf{x}_T \mid \mathbf{x}_t)=\mathcal{N}(
    \mathbf{x}_T + (\mathbf{x}_t-\mathbf{x}_T) e^{\bar{\theta}_{t:T}},
    v_{t:T}^2 \bm{I}).
\end{equation}
Hence, the $h$-function can be directly computed as:
\begin{equation}
\begin{split}
\bm{h}(\mathbf{x}_t, t, \mathbf{x}_T, T) 
& = \nabla_{\mathbf{x}_t} \log p(\mathbf{x}_T \mid \mathbf{x}_t)\\
& = -\nabla_{\mathbf{x}_t} \left[\frac{(\mathbf{x}_t - \mathbf{x}_T)^2 e^{-2 \bar{\theta}_{t:T}}}{2 v_{t:T}^2} + const \right]\\
& = (\mathbf{x}_T - \mathbf{x}_t) \frac{e^{-2 \bar{\theta}_{t:T}}}{v_{t:T}^2} \\
& = (\mathbf{x}_T - \mathbf{x}_t) \sigma^{-2}/(e^{2\bar{\theta}_{t:T}}-1).
\end{split}
\end{equation}


Then the Doob's $h$-transform yields the representation of an endpoint $\mathbf{x}_T$ conditioned process defined by the following SDE: 
% 
\begin{equation}
\begin{split}
\mathrm{d}\mathbf{x}_t 
&= \left[ f(\mathbf{x}_t, t) + g_t^2 \bm{h}(\mathbf{x}_t, t, \mathbf{x}_T, T) \right] \mathrm{d}t + g_t \mathrm{d}\mathbf{w}_t\\
&= \left( \theta_t + \frac{g_t^2}{\sigma^2 (e^{2\bar{\theta}_{t:T}}-1)}  \right)(\mathbf{x}_T - \mathbf{x}_t)  \mathrm{d}t + g_t \mathrm{d}\mathbf{w}_t \\
& = \theta_t \left( 1 + \frac{2}{e^{2\bar{\theta}_{t:T}}-1}  \right)(\mathbf{x}_T - \mathbf{x}_t)  \mathrm{d}t + g_t \mathrm{d}\mathbf{w}_t.
\end{split}
\end{equation}

Given that the joint distribution of $[\mathbf{x}_0, \mathbf{x}_t, \mathbf{x}_T]$ is multivariate normal, the conditional distribution $p(\mathbf{x}_t \mid \mathbf{x}_0, \mathbf{x}_T)$ is also Gaussian:
\begin{equation}
    p(\mathbf{x}_t\mid \mathbf{x}_0, \mathbf{x}_T) = \mathcal{N}(\bar{\mathbf{m}}_t, \bar{v}_t^2 \bm{I}),
\end{equation}
where the mean $\bar{\mathbf{m}}_t$ and variance $\bar{v}_t^2$ are determined using the conditional formulas for multivariate normal variables:
\begin{equation}
\begin{split}
\bar{\mathbf{m}}_t 
=  \mathbb{E}[\mathbf{x}_t\mid \mathbf{x}_0 \mid \mathbf{x}_T]
=\mathbb{E}[\mathbf{x}_t\mid \mathbf{x}_0]+\mathrm{Cov}(\mathbf{x}_t,\mathbf{x}_T\mid \mathbf{x}_0)\mathrm{Var}(\mathbf{x}_T\mid \mathbf{x}_0)^{-1}(\mathbf{x}_T-\mathbb{E}[\mathbf{x}_T\mid \mathbf{x}_0]),\\
\bar{v}_t^2
= \mathrm{Var}(\mathbf{x}_t\mid \mathbf{x}_0 \mid \mathbf{x}_T)
=\mathrm{Var}(\mathbf{x}_t\mid \mathbf{x}_0)-\mathrm{Cov}(\mathbf{x}_t,\mathbf{x}_T\mid \mathbf{x}_0)\mathrm{Var}(\mathbf{x}_T\mid \mathbf{x}_0)^{-1}\mathrm{Cov}(\mathbf{x}_T,\mathbf{x}_t\mid \mathbf{x}_0).
\end{split}
\label{eq:OUB-m-v}
\end{equation}

Notice that
\begin{equation}
    \mathrm{Cov}(\mathbf{x}_t,\mathbf{x}_T\mid \mathbf{x}_0)=\mathrm{Cov}\left(\mathbf{x}_t,(\mathbf{x}_t-\mathbf{x}_T)e^{-\bar{\theta}_{t:T}}\mid \mathbf{x}_0\right)=e^{-\bar{\theta}t:T}\mathrm{Var}(\mathbf{x}_t\mid \mathbf{x}_0).
\end{equation}
By substituting this and the results in \cref{eq:GOU-p} into \cref{eq:OUB-m-v}, we can obtain
\begin{equation}
\begin{split}
\bar{\mathbf{m}}_t 
& = \left(\mathbf{x}_T+(\mathbf{x}_0-\mathbf{x}_T)e^{-\bar{\theta}_t}\right)
+ \left(e^{-\bar{\theta}_{t:T}} v_t^2\right)
/ v_T^2
\cdot \left(\mathbf{x}_T - \mathbf{x}_T - (\mathbf{x}_0 - \mathbf{x}_T)e^{-\bar{\theta}_T}\right) \\
& = \mathbf{x}_T + (\mathbf{x}_0-\mathbf{x}_T) \left(e^{-\bar{\theta}_t} -  e^{-\bar{\theta}_{t:T}}e^{-\bar{\theta}_T} v_t^2 /v_T^2\right) \\
& = \mathbf{x}_T + (\mathbf{x}_0-\mathbf{x}_T)e^{-\bar{\theta}_t} 
\left(\frac{1-e^{-2\bar{\theta}_{T}}-e^{-2\bar{\theta}_{t:T}}(1-e^{-2\bar{\theta}_t})}{1-e^{-2\bar{\theta}_{T}}}\right)\\
& = \mathbf{x}_T + (\mathbf{x}_0-\mathbf{x}_T)e^{-\bar{\theta}_t} 
v_{t:T}^2/v_T^2,
\end{split}
\end{equation}
and 
\begin{equation}
\begin{split}
\bar{v}_t^2
& = v_t^2 - \left(e^{-\bar{\theta}_{t:T}} v_t^2 \right)^2 / v_T^2\\
& = \frac{v_t^2}{v_T^2}(v_T^2-e^{-2\bar{\theta}_{t:T}}v_t^2)\\
& = \frac{v_t^2}{v_T^2} \sigma^2\left(1-e^{-2\bar{\theta}_T} - e^{-2\bar{\theta}_{t:T}}(1-e^{-\bar{2\theta}_t})\right)\\
& = v_t^2 v_{t:T}^2/ v_T^2.
\end{split}
\end{equation}

Finally, we conclude the proof by reverting to the original notations.
\end{proof}



Note that the generalized OU bridge process, also referred to as the conditional GOU process, has been studied theoretically in previous works \cite{salminen1984conditional,GOUB2021,GOUB+ICML2024}. However, we are the first to demonstrate its effectiveness in explicitly learning higher-order structures within the graph generation process.


\paragraph{Brownian Bridge Process}  
In the following, we demonstrate that the Brownian bridge process is a particular case of the generalized OU bridge process when $\theta_t$ approaches zero.

Assume $\theta_t = \theta$ is a constant that tends to zero, we obtain 
\begin{equation}
    \bar{\theta}_{a:b}=\int_a^b \theta_s \diff{s} = \theta (b-a)\rightarrow 0.
\end{equation}

Consider the term $ e^{2\bar{\theta}_{t:\tau_k}}-1$, we approximate the exponential function using a first-order Taylor expansion for small $\bar{\theta}_{t:\tau_k}$:
\begin{equation}
    e^{2\bar{\theta}_{t:\tau_k}}-1 
    \approx
    2\bar{\theta}_{t:\tau_k}
        \rightarrow
        2\theta(\tau_k - t).
\end{equation}
Hence, the drift term in the generalized OU bridge simplifies to
\begin{equation}
    \theta_t \left( 1 + \frac{2}{e^{2\bar{\theta}_{t:\tau_k}}-1}\right)
     \approx
     \theta\left(1+\frac{2}{2\theta(\tau_k-t)}\right)
     \rightarrow
     \frac{1}{\tau_k-t}.
\end{equation}

Consequently, in the limit $\theta_t \rightarrow 0$, the generalized OU bridge process described in \cref{eq:GOUB-SDE} can be modelled by the following SDE:
\begin{equation}
    \mathrm{d}\bm{G}_t=  \frac{\bm{G}_{\tau_k}-\bm{G}_t}{\tau_k-t}\mathrm{d}t+
    g_{k,t}\mathrm{d}\bm{W}_t.
\end{equation}
This equation precisely corresponds to the SDE representation of the classical Brownian bridge process.


In contrast to the generalized OU bridge process in \cref{eq:GOUB-SDE}, the evolution of the Brownian bridge is fully determined by the noise schedule $g_{k,t}$, resulting in a simpler SDE representation. 
However, this constraint in the Brownian bridge reduces the flexibility in designing the generative process.


Note that the Brownian bridge is an endpoint-conditioned process relative to a reference Brownian motion, which the SDE governs:
\begin{equation}
    \mathrm{d}\bm{G}_t=  
    g_{t}\mathrm{d}\bm{W}_t.
\end{equation}
This equation describes a pure diffusion process without drift, making it a specific instance of the generalized OU process in \cref{eq:GOU-SDE}.

\subsection{Proof of Proposition~\ref{pro:training}}

To establish proof, we begin by introducing essential definitions and assumptions.

\begin{definition}[$\beta$-smooth]
A function $f:\mathbb{R}^m  \to \mathbb{R}^n$ is said to be $\beta$-smooth if and only if
\begin{equation}
    \norm{f(\mathbf{w})-f(\mathbf{v})-\nabla f(\mathbf{v})(\mathbf{w}-\mathbf{v})} \leq \frac{\beta}{2} \norm{\mathbf{w}-\mathbf{v}}^2, \forall \mathbf{w},\mathbf{v}\in \mathbb{R}^m.
\end{equation}
\end{definition}

\begin{customthe}[Proposition~\ref{pro:training}\textnormal{ (Formal)}]
Let $\ell^{(k)}(\boldsymbol{\theta})$ be a loss function that is $\beta$-smooth and satisfies the $\mu$-PL (Polyak-Łojasiewicz) condition in the ball $B\left(\boldsymbol{\theta}_0, R\right)$ of radius $R=2N \sqrt{2 \beta \ell^{(k)}\left(\boldsymbol{\theta}_0\right)}/(\mu \delta)$, where $\delta>0$. 
%
Then, with probability $1-\delta$ over the choice of mini-batch of size $b$, stochastic gradient descent (SGD) with a learning rate $\eta^*=\frac{\mu N}{N \beta\left(N^2 \beta+\mu(b-1)\right)}$ converges to a global solution in the ball $B$ with exponential convergence rage: 
\begin{equation}
 \mathbb{E}\left[\ell^{(k)}\left(\boldsymbol{\theta}_i\right)\right] \leq\left(1-\frac{b \mu^2}{\beta N\left(\beta N^2+\mu(b-1)\right)}\right)^i \ell^{(k)}\left(\boldsymbol{\theta}_0\right).
\end{equation}
Here, $N$ denotes the size of the training dataset.
Furthermore, the proposed generative model yields a smaller smoothness constant $\beta_{\text{HOG-Diff}}$ compared to that of the classical model $\beta_{\text {classical}}$, \ie, $\beta_\text{HOG-Diff} \leq \beta_{\text {classical}}$, implying that the learned distribution in HOG-Diff converges to the target distribution faster than classical generative models.
\end{customthe}

\begin{proof}
Assume that the loss function $\ell^{(k)}(\bm{\theta})$ in \cref{eq:final-loss} is minimized using standard Stochastic Gradient Descent (SGD) on a training dataset $\mathcal{S}=\{\mathbf{x}^i\}_{i=1}^N$. At the $i$-th iteration, parameter $\bm{\theta}_i$ is updated using a mini-batch of size $b$ as follows:
\begin{equation}
    \bm{\theta}_{i+1} \triangleq \bm{\theta}_i - \eta \nabla \ell^{(k)}(\bm{\theta}_i),
\end{equation}
where $\eta$ is the learning rate.


Following \citet{liu2020toward} and \citet{GSDM+TPAMI2023}, we assume that $\ell^{(k)}(\bm{\theta})$ is $\beta$-smooth and satisfies the $\mu$-PL condition in the ball $B(\bm{\theta}_0, R)$ with $R=2N\sqrt{2\beta \ell^{(k)}(\bm{\theta}_0)}/(\mu\delta)$ where $\delta>0$. 
%
Then, with probability $1-\delta$ over the choice of mini-batch of size $b$, SGD with a learning  rate $\eta^* =\frac{\mu N}{N\beta (N^2\beta +\mu(b-1))}$ converges to a global solution in the ball $B(\bm{\theta}_0, R)$ with exponential convergence rate \cite{liu2020toward}:
\begin{equation}
\mathbb{E}[\ell^{(k)}(\bm{\theta}_i)] 
\leq \left(1-\frac{b\mu\eta^*}{N}\right)^i \ell^{(k)}(\bm{\theta}_0)
= \left(1-\frac{b\mu^2}{\beta N(\beta N^2+\mu(b-1))}\right)^i \ell^{(k)}(\bm{\theta}_0).
\end{equation}

% 2------
Next, we show that the proposed framework has a smaller smoothness constant than the classical one-step model. 
Therefore, we focus exclusively on the spectral component $||\bm{s}^{(k)}_{\bm{\theta},\bm{\Lambda}} - \nabla_{\bm{\Lambda}} \log p_t(\bm{G}_t | \bm{G}_{\tau_k})||_2^2$ from the full loss function in \cref{eq:final-loss}, as the feature-related part of the loss function in HOG-Diff aligns with that of the classical framework.  
For simplicity, we use the notation $\bar{\ell}(\bm{\theta})=||\bm{s}^{(k)}_{\bm{\theta},\bm{\Lambda}} - \nabla_{\bm{\Lambda}} \log p_t(\bm{G}_t | \bm{G}_{\tau_k})||^2 = ||\bm{s}_{\bm{\theta}}(\mathbf{x}_t) - \nabla_{\mathbf{x}} \log p_t(\mathbf{x}_t)||^2$ as the feature-related part of the loss.%, and let $\bar{\ell}(\bm{\varphi}) = \mathbb{E} ||s_{\bm{\varphi}}(\mathbf{x}_t) - \nabla_{\mathbf{x}} q_t(\mathbf{x}_t|\mathbf{x}_0)||^2$ be its classical counterpart.


Next, we verify that $\bar{\ell}(\bm{\theta})$ is $\beta$-smooth under the assumptions given.
Notice that the gradient of the loss function is given by:
\begin{equation}
\nabla \bar{\ell}(\bm{\theta})=2\mathbb{E}\left[(\bm{s}_{\bm{\theta}}(\mathbf{x})-\nabla\log p(\mathbf{x}))^\top\nabla_{\bm{\theta}} s_{\bm{\theta}}(\mathbf{x})\right]
\end{equation}
Hence,
\begin{equation}
\begin{split}
&\|\nabla \bar{\ell}(\bm{\theta}_1)-\nabla \bar{\ell}(\bm{\theta}_2)\|
=2\left\|\mathbb{E}\left[(\bm{s}_{\bm{\theta}_1}(\mathbf{x})-\nabla\log p(\mathbf{x}))^\top\nabla \bm{s}_{\bm{\theta}_1}(\mathbf{x})-(\bm{s}_{\bm{\theta}_2}(\mathbf{x})-\nabla\log p(\mathbf{x}))^\top\nabla \bm{s}_{\bm{\theta}_2}(\mathbf{x})\right]\right\|\\
&\leq 2\mathbb{E}[\|\bm{s}_{\bm{\theta}_{1}}(\mathbf{x})-\bm{s}_{\bm{\theta}_2}(\mathbf{x})\|\cdot\|\nabla \bm{s}_{\bm{\theta}_1}(\mathbf{x})\|  
+\|\bm{s}_{\bm{\theta}_2}(\mathbf{x})-\nabla\log p(\mathbf{x})\|\cdot\|\nabla \bm{s}_{\bm{\theta}_1}(\mathbf{x})-\nabla \bm{s}_{\bm{\theta}_2}(\mathbf{x})\|].
\end{split}
\end{equation}

Suppose $\|\nabla_{\bm{\theta}} \bm{s}_{\bm{\theta}}(\mathbf{x})\|\leq C_1$ and $\|\bm{s}_{\bm{\theta}}(\mathbf{x})-\nabla\log p(\mathbf{x})\|\leq C_2$, then we can obtain
\begin{equation}
\begin{split}
\|\nabla \bar{\ell}(\bm{\theta}_1)-\nabla \bar{\ell}(\bm{\theta}_2)\|\
& \leq 2 \mathbb{E} \left[C_1 \beta_{\bm{s}_{\bm{\theta}}}\|\bm{\theta}_1-\bm{\theta}_2\|+C_2\beta_{\nabla \bm{s}_{\bm{\theta}}}\|\bm{\theta}_1-\bm{\theta}_2\| \right] \\
& =2(\beta_{\bm{s}_{\bm{\theta}}} C_1+C_2\beta_{\nabla \bm{s}_{\bm{\theta}}})\|\bm{\theta}_1-\bm{\theta}_2\|.
\end{split}
\end{equation}

To satisfy the $\beta$-smooth of $\bar{\ell}(\bm{\theta})$, we require that
\begin{equation}
    2(C_1\beta_{\bm{s}_{\bm{\theta}}}+C_2\beta_{\nabla \bm{s}_{\bm{\theta}}}) 
\leq \beta.
\end{equation}

This implies that the distribution learned by the proposed framework can converge to the target distribution. Therefore, following \citet{CCDF+CVPR2022}, we further assume that $\bm{s}_{\bm{\theta}}$ is a sufficiently expressive parameterized score function so that 
$\beta_{\bm{s}_{\bm{\theta}}} =  \beta_{\nabla \log p_{t|\tau_{k-1}}}$ and $\beta_{\nabla^2 \bm{s}_{\bm{\theta}}} =  \beta_{\nabla^2 \log p_{t|\tau_{k-1}}}$.


%
Consider the loss function of classical generative models goes as: $\bar{\ell}(\bm{\varphi}) = \mathbb{E} ||\bm{s}_{\bm{\varphi}}(\mathbf{x}_t) - \nabla_{\mathbf{x}_t} q_t(\mathbf{x}_t|\mathbf{x}_0)||^2$.
To demonstrate that the proposed framework converges faster to the target distribution compared to the classical one-step generation framework, it suffices to show that: $\beta_{\nabla p_{t|\tau_{k-1}}} \leq \beta_{\nabla q_{t|0}}$ and $\beta_{\nabla^2 p_{t|\tau_{k-1}}} \leq \beta_{\nabla^2 q_{t|0}}$.

Let $\mathbf{x}\sim q_{t|0}$ and $\mathbf{x}'\sim p_{t|\tau_{k-1}}$. Since we inject topological information from $\mathbf{x}$ into $\mathbf{x}^{\prime}$, $\mathbf{x}'$ can be viewed as being obtained by adding noise to $\mathbf{x}$. Hence, we can model $\mathbf{x}'$ as $\mathbf{x}' = \mathbf{x} + \epsilon$ where $\epsilon \sim \mathcal{N}(\mathbf{0},\sigma^2 \bm{I})$. The variance of Gaussian noise $\sigma^2$ controls the information remained in $z'$. 
Hence, its distribution can be expressed as $p(\mathbf{x}')=\int q(\mathbf{x}'-\epsilon)\pi(\epsilon)\diff\epsilon$.

Therefore, we can obtain
\begin{equation}
\begin{split}
||\nabla_{\mathbf{x}'}^k p(\mathbf{x}'_1) - \nabla_{\mathbf{x}'}^k p(\mathbf{x}'_2)||
& =|| \nabla_{\mathbf{x}'}^k \int \left(q(\mathbf{x}_1'-\epsilon)-q(\mathbf{x}_2'-\epsilon)\right)\pi(\epsilon)\diff{\epsilon}||\\
&\leq  \int ||\nabla_{\mathbf{x}'}^k q(\mathbf{x}_1'-\epsilon) - \nabla_{\mathbf{x}'}^k q(\mathbf{x}_2'-\epsilon)|| \pi(\epsilon)\diff{\epsilon} \\ %变量代换
& \leq ||\nabla_{\mathbf{x}'}^k q(\mathbf{x}') ||_{\mathrm{lip}} (\mathbf{x}_1'-\mathbf{x}_2')  \int \pi(\epsilon)\diff{\epsilon}\\
& \leq ||\nabla_{\mathbf{x}'}^k q(\mathbf{x}') ||_{\mathrm{lip}} (\mathbf{x}_1'-\mathbf{x}_2').
\end{split}
\end{equation}


Hence, $||\nabla_{\mathbf{x}'}^k \log p(\mathbf{x}')||_{\mathrm{lip}} \leq ||\nabla_{\mathbf{x}'}^k \log q(\mathbf{x}')||_{\mathrm{lip}}$.

By setting $k=3$ and $k=4$, we can obtain $\beta_{\nabla \log p_{t|\tau_{k-1}}} \leq \beta_{\nabla \log q_{t|0}}$ and $\beta_{\nabla^2 \log p_{t|\tau_{k-1}}} \leq \beta_{\nabla^2 \log q_{t|0}}$. 
Therefore $\beta_{\text{HOG-Diff}}\leq \beta_{\text{classical}}$, implying that the training process of HOG-Diff ($\bm{s}_{\bm{\theta}}$) will converge faster than the classical generative framework ($\bm{s}_{\bm{\varphi}}$).

\end{proof}








\subsection{Proof of Proposition~\ref{pro:reconstruction-error}}

Here, we denote the expected reconstruction error at each generation process
 as $\mathcal{E}(t)=\mathbb{E}\norm{\bar{\bm{G}}_t-\widehat{\bm{G}}_t}^2$.

\begin{customthe}[Proposition~\ref{pro:reconstruction-error}]
\input{Content/blocks/theorem3}
\end{customthe}
 

\begin{proof}

Let $\mathcal{E}(t)= \mathbb{E}\norm{\bar{\bm{G}}_t-\widehat{\bm{G}}_t}^2$, which reflects the expected error between the data reconstructed with the ground truth score $\nabla \log p_t(\cdot)$ and the learned scores $\bm{s}_{\bm{\theta}} (\cdot)$.  
%
In particular, $\bar{\bm{G}}$ is obtained by solving the following oracle reversed time SDE:
\begin{equation}
    \diff \bar{\bm{G}}_t=\left(\mathbf{f}_{k,t}(\bar{\bm{G}}_t)-g_{k,t}^2 \nabla_{\bm{G}}\log p_t(\bar{\bm{G}}_t)\right)\diff\bar{t}
    +g_{k,t}\diff \bar{\bm{W}}_t, t\in[\tau_{k-1},\tau_k],
\end{equation}
whereas $\widehat{\bm{G}}_t$ is governed based on the corresponding estimated reverse time SDE:
\begin{equation}
    \mathrm{d}\widehat{\bm{G}}_t=\left(\mathbf{f}_{k,t}(\widehat{\bm{G}}_t)-g_{k,t}^2 \bm{s}_{\bm{\theta}}(\widehat{\bm{G}}_t,t)\right)\diff\bar{t}
    +g_{k,t}\diff\bar{\bm{W}}_t, t\in [\tau_{k-1},\tau_k].
\end{equation}
Here, $\mathbf{f}_{k,t}$ is the drift function of the Ornstein–Uhlenbeck bridge. 
For simplicity, we we denote the Lipschitz norm by $||\cdot||_{\operatorname{lip}}$ and $\mathbf{f}_{k,s}(\bm{G}_s)=h_{k,s}(\bm{G}_{\tau_k}-\bm{G}_s)$, where $h_{k,s}=\theta_s \left(1 + \frac{2}{e^{2\bar{\theta}_{s:\tau_k}}-1}\right)$. 


To bound the expected reconstruction error $\mathbb{E}\norm{\bar{\bm{G}}_{\tau_{k-1}}-\widehat{\bm{G}}_{\tau_{k-1}}}^2$ at each generation process, we begin by analyze how $\mathbb{E}\norm{\bar{\bm{G}}_t-\widehat{\bm{G}}_t}^2$ evolves as time $t$ is reversed from $\tau_k$ to $\tau_{k-1}$. 
The reconstruction error goes as follows
\begin{equation}
\begin{aligned}
\mathcal{E}(t)
&\leq \mathbb{E}\int_{\tau_k}^t\norm{\left(\mathbf{f}_{k,s}(\bar{\bm{G}}_s)-\mathbf{f}_{k,s}(\widehat{\bm{G}}_{s})\right)+g_{k,s}^2 \left(\bm{s}_{\bm{\theta}}(\widehat{\bm{G}}_{s},s)-\nabla_{\bm{G}}\log p_{s}(\bar{\bm{G}}_{s})\right)}^2\mathrm{d}\bar{s} \\ 
% Line2
&\leq C\mathbb{E}\int_{\tau_k}^t\left\|\mathbf{f}_{k,s}(\bar{\bm{G}}_{s})-\mathbf{f}_{k,s}(\widehat{\bm{G}}_{s})\right\|^2 \mathrm{d}\bar{s} 
+ C\mathbb{E}\int_{\tau_k}^t g_{k,s}^4\left\|\bm{s}_{\bm{\theta}}(\widehat{\bm{G}}_s,s)-\nabla_{\bm{G}}\log p_s(\bar{\bm{G}}_s)\right\|^2\mathrm{d}\bar{s} \\ 
% Line3
&\leq C\int_{\tau_k}^t\|h_{k,s}\|_{\mathrm{lip}}^2\cdot \mathcal{E}(s) \mathrm{d}\bar{s} 
+ C \mathcal{E}(\tau_k) \int_{\tau_k}^t h_{k,s}^2 \mathrm{d}\bar{s}  \\
&+ C^2 \int_{\tau_k}^t g_{k,s}^4 \cdot\mathbb{E}\left\|\bm{s}_{\bm{\theta}}(\widehat{\bm{G}}_{s},s)-\bm{s}_{\bm{\theta}}(\bar{\bm{G}}_{s},s)\right\|^2   
+g_{k,s}^4\cdot\mathbb{E}\left\|\bm{s}_{\bm{\theta}}(\bar{\bm{G}}_s,s)-\nabla_{\bm{G}}\log p_s(\bar{\bm{G}}_s)\right\|^2\mathrm{d}\bar{s}  \\
% Line5
&\leq \underbrace{C^2 \ell^{(k)}(\bm{\theta}) \int_{\tau_k}^t g_{k,s}^4\mathrm{d}\bar{s} 
+ C \mathcal{E}(\tau_k) \int_{\tau_k}^t h_{k,s}^2   \mathrm{d}\bar{s}}_{\alpha(t)}  
+\int_{\tau_k}^t \underbrace{\left( C^2 g_{k,s}^4 \|\bm{s}_{\bm{\theta}}(\cdot,s)\|_{\mathrm{lip}}^2 
+C\|h_{k,s}\|_{\mathrm{lip}}^2 \right)}_{\gamma(s)}  \mathcal{E}(s)  \mathrm{d}\bar{s} \\
% Line6
& = \alpha(t) + \int_{\tau_k}^t \gamma(s) \mathcal{E}(s)  \mathrm{d}\bar{s}.
\end{aligned}
\end{equation}



% changing the integral range (通过改变积分范围，我们可以得到等价的形式为)
Let $v(t)=\mathcal{E}(\tau_k-t)$ and $s'=\tau_k-s$, it can be derived that
\begin{equation}
    v(t) = \mathcal{E}(\tau_k-t) \leq \alpha(\tau_k-t) + \int _0 ^t \gamma(\tau_k - s')v(s')\diff s'.
\end{equation}

Here, $\alpha(\tau_k - t)$ is a non-decreasing function. 
By applying Grönwall’s inequality, we can derive that
\begin{align}
    v(t) & \leq \alpha(\tau_k-t)   \exp{
    \int_0^t \gamma(\tau_k-s') } \mathrm{d}s'  \\
    & = \alpha(\tau_k-t)  \exp{
    \int_{\tau_k-t}^{\tau_k} \gamma(s) } \mathrm{d}s.
\end{align}

Hence,
\begin{equation}
    \mathcal{E}(t) \leq \alpha(t)  \exp{
    \int_t^{\tau_k} \gamma(s) } \mathrm{d}s.
\end{equation}
Therefore, the reconstruction error of HOG-Diff is bounded by 
\begin{equation}
\begin{split}
\mathcal{E}(0)
%\mathbb{E}\norm{\bar{\mathbf{x}}_0-\widehat{\mathbf{x}}_0}^2 
&\leq \alpha(0)  \exp{ \int_0^{\tau_1} \gamma(s) } \mathrm{d}s \\
&= 
 \left(C^2 \ell^{(1)}(\bm{\theta}) \int_0^{\tau_1} g_{1,s}^4\mathrm{d}s
+ C \mathcal{E}(\tau_1) \int_0^{\tau_1} h_{1,s}^2   \mathrm{d}s \right) \exp{ \int_0^{\tau_1} \gamma(s) } \mathrm{d}s.
\end{split}
\end{equation}
A comparable calculation for a classical graph generation model (with diffusion interval $[0, T]$) yields a bound
\begin{equation}
    \mathcal{E}^\prime (0) \leq
 \left(C^2 \ell(\bm{\varphi}) \int_0^T g_s^4\mathrm{d}s
+ C \mathcal{E}^{\prime}(T) \int_0^T h_s^2   \mathrm{d}s \right) \exp \int_0^T \gamma^\prime(s)  \mathrm{d}s,
\end{equation}
where $h_s=\theta_s \left(1 + \frac{2}{e^{2\bar{\theta}_{s:T}}-1}\right)$.


Let $h(s,\tau)=\theta_s  \left(1 + \frac{2}{e^{2\bar{\theta}_{s:\tau}}-1}\right)$, $a(\tau)=\int_0^{\tau} h(s,\tau)^2 \diff{s}$, and $b(\tau)=\int_0^{\tau} \|h(s,\tau)\|_{\operatorname{lip}}^2 \diff{s}$ .
Since $\tau_1\leq T$, it follows that $ \mathcal{E}(\tau_1)\leq \mathcal{E}^\prime(T)$. Additionally, by \cref{pro:training}, $\ell(\cdot)$ converges exponentially in the score-matching process.
Therefore, to prove $\mathcal{E}(0)\leq \mathcal{E}^\prime(0)$, it suffices to show that both $a(\tau)$ and $b(\tau)$ are increasing functions.

Applying the Leibniz Integral Rule, we obtain:
\begin{equation}
a^\prime (\tau) = h(\tau,\tau)^2 + \int_0^{\tau} \frac{\partial}{\partial \tau} h(s, \tau)^2 \diff{s} 
\qquad \mathrm{and} \qquad
b^\prime(\tau) = \|h(\tau,\tau)\|^2_{\operatorname{lip}} + \int_0^{\tau} \frac{\partial}{\partial \tau} \|h(s, \tau)\|^2_{\operatorname{lip}} \diff{s}.
\end{equation}
Since $h(\tau,\tau) \rightarrow 0$, we can derive that $a^\prime (\tau)>0$ and $b^\prime (\tau)>0$. 
This implies $\int_0^{\tau_1} h_{1,s}^2 \diff{s} \leq \int_0^T h_s^2 \diff{s}$ and $\int_0^{\tau_1} \gamma(s) \diff{s} \leq \int_0^T \gamma^{\prime}(s) \diff{s}$.
Combining these inequalities, we can finally conclude $\mathcal{E}(0)\leq \mathcal{E}^\prime(0)$.
Therefore, HOG-Diff provides a sharper reconstruction error bound than the classical graph generation framework.
\end{proof}


\section{Higher-order Networks}
\label{app:ho-intro}
Graphs are elegant and useful abstractions for modelling irregular relationships in empirical systems, but their inherent limitation to pairwise interactions restricts their representation of group dynamics \cite{HigherOrderReview2020,xiao2022people}. 
% 
For example, cyclic structures like benzene rings and functional groups play a holistic role in molecular networks; densely interconnected structures, like simplices, often have a collective influence on social networks; and functional brain networks exhibit higher-order dependencies.
%
To address this, various topological models have been employed to describe data in terms of its higher-order relations, including simplicial complexes \cite{HiGCN2024}, cell complexes \cite{CWN+NeurIPS2021}, and combinatorial complexes \cite{combinatorial-complexes}.
%
As such, the study of higher-order networks has gained increasing attention for their capacity to capture higher-order interactions \cite{TDL-position+ICML2024,HoRW2024}.


Given the broad applicability and theoretical richness of higher-order networks, the following delves deeper into two key frameworks for modelling such interactions: simplicial complexes and cell complexes.

\subsection{Simplicial Complexes}

Simplicial complexes (SCs) are fundamental concepts in algebraic topology that flexibly subsume pairwise graphs \cite{Top_Hodge_Hatcher+2001}. 
Specifically, simplices generalize fundamental geometric structures such as points, lines, triangles, and tetrahedra, enabling the modelling of higher-order interactions in networks. They offer a robust framework for capturing multi-way relationships that extend beyond pairwise connections typically represented in classical networks.

A simplicial complex $\mathcal{X}$ consists of a set of simplices of varying dimensions, including vertices (dimension 0), edges (dimension 1), and triangles (dimension 2).

% @ xxx
A $d$-dimensional simplex is formed by a set of $(d+1)$ interacting nodes and includes all the subsets of $\delta + 1$ nodes (with $\delta<d$), which are called the $\delta$-dimensional faces of the simplex.
A simplicial complex of dimension $d$ is formed by simplices of dimension at most equal to $d$ glued along their faces.


\begin{definition}[Simplicial complexes]
A simplicial complex $\mathcal{X}$ is a finite collection of node subsets closed under the operation of taking nonempty subsets, and such a node subset $\sigma \in \mathcal{X}$ is called a simplex. 
\end{definition}


We can obtain a clique complex, a particular kind of SCs, by extracting all cliques from a given graph and regarding them as simplices. 
%
This implies that an empty triangle (owning $\left[v_1,v_2\right]$, $\left[v_1,v_3\right]$, $\left[v_2,v_3\right]$ but without $\left[v_1,v_2,v_3\right]$) cannot occur in clique complexes.

\subsection{Cell Complexes}


Cell complexes generalize simplicial complexes by incorporating generalized building blocks called cells instead of relying solely on simplices \cite{Top_Hodge_Hatcher+2001}.
% Cells capture many-body interactions that are less restrictive than those of simplicial complexes. 
This broader approach allows for the representation of many-body interactions that do not adhere to the strict requirements of simplicial complexes.
For example, a square can be interpreted as a cell of four-body interactions whose faces are just four links. 
This flexibility is advantageous in scenarios such as social networks, where, for instance, a discussion group might not involve all-to-all pairwise interactions, or in protein interaction networks, where proteins in a complex may not bind pairwise.

\begin{figure}[!t]
\centering
\includegraphics[width=0.96\linewidth]{figs/cell-example.pdf}
\vspace{-3mm}
\caption{\textbf{Visual illustration of cell complexes.} (\textbf{a}) Triangle. (\textbf{b}) Tetrahedron. (\textbf{c}) Sphere. (\textbf{d}) Torus.}
\label{fig:cell-example}
\vspace{-4mm}
\end{figure}


% @ Higher-Order Networks
Formally, a cell complex is termed regular if each attaching map is a homeomorphism onto the closure of the associated cell’s image. 
Regular cell complexes generalize graphs, simplicial complexes, and polyhedral complexes while retaining many desirable combinatorial and intuitive properties of these simpler structures.
In this paper, all cell complexes will be regular and consist of finitely many cells. 

As shown in \cref{fig:cell-example} \textbf{a} and \textbf{b},
triangles and tetrahedrons are two particular types of cell complexes called simplicial complexes (SCs). The only 2-cells they allow are triangle-shaped.
%
The sphere shown in \cref{fig:cell-example} \textbf{c} is a 2-dimensional cell complex. It is constructed using two 0-cells (\ie, nodes), connected by two 1-cells (\ie, the edges forming the equator). The equator serves as the boundary for two 2-dimensional disks (the hemispheres), which are glued together along the equator to form the sphere.
% Torus
The torus in \cref{fig:cell-example} \textbf{d} is a 2-dimensional cell complex formed by attaching a single 1-cell to itself in two directions to form the loops of the torus. The resulting structure is then completed by attaching a 2-dimensional disk, forming the surface of the torus.
Note that this is just one way to represent the torus as a cell complex, and other decompositions might lead to different numbers of cells and faces.



\section{Additional Explanation on Related Works}
\label{app:related}
\input{Content/X_App-related}


\section{Details for Higher-order Guided Generation }
\label{app:detail-HOG-Diff}

\subsection{Denoising Network Parametrization}
\label{app:denoising-model}

The denoising network in HOG-Diff is a critical component responsible for estimating the score functions required to reverse the diffusion process effectively. 
The architecture of the proposed denoising network is depicted in \cref{fig:denoising-model}.
The input $\bm{A}_t$ is computed from $\bm{U}_0$ and $\bm{\Lambda}_t^{(k)}$ using the relation  $\bm{A}_t=\bm{D}_t^{(k)}-\bm{L}^{(k)}_t$, where the Laplacian matrix is given by $\bm{L}^{(k)}_t=\bm{U}_0 \bm{\Lambda}_t^{(k)}\bm{U}_0^\top$ and the diagonal degree matrix is given by $\bm{D}_t^{(k)}=\operatorname{diag}\left(\bm{L}_t^{(k)}\right)$.
%
To enhance the input to the Attention module, we derive enriched node and edge features using the  $l$-step random walk matrix obtained from the binarized $\bm{A}_t$.
Specifically, the arrival probability vector is incorporated as additional node features, while the truncated shortest path distance derived from the same matrix is employed as edge features.
Temporal information is integrated into the outputs of the Attention and GCN modules using Feature-wise Linear Modulation (FiLM) \cite{Film+AAAI2018} layers, following sinusoidal position embeddings \cite{attention+NeurIPS2017}.



\begin{figure}[t]
\centering
\includegraphics[width=0.96\linewidth]{figs/ScoreNet.pdf}
\caption{\textbf{Denoising Network Architecture of HOG-Diff.} 
The denoising network integrates GCN and Attention blocks to capture both local and global features, and further incorporates time information through FiLM layers.
These enriched outputs are subsequently concatenated and processed by separate feed-forward networks to produce predictions for $\nabla_{\bm{X}} \log p(\bm{G}_t|\bm{G}_{\tau_k})$ and $\nabla_{\bm{\Lambda}_t} \log p(\bm{G}_t|\bm{G}_{\tau_k})$, respectively.
% \tolga{Please write a description in the caption here.}
}
\label{fig:denoising-model}
\vspace{-4mm}
\end{figure}

% permutation equivariant
A graph processing module is considered permutation invariant if its output remains unchanged under any permutation of its input, formally expressed as $f(\bm{G}) = x \iff f(\pi(\bm{G})) = x$, where $\pi(\bm{G})$ represents a permutation of the input graph $\bm{G}$. It is permutation equivariant when the output undergoes the same permutation as the input, formally defined as $f(\pi(\bm{G})) = \pi(f(\bm{G}))$. 
It is worth noting that our denoising network model is permutation equivalent as each model component avoids any node ordering-dependent operations.



\subsection{Training and Sampling Proceudre}

As shown in \cref{fig:gFrame}, HOG-Diff implements a coarse-to-fine generation curriculum, with the forward diffusion and reverse denoising processes divided into $K$ easy-to-learn subprocesses. Each subprocess is realized using the generalized OU bridge process.


\begin{figure}[!ht]
    \centering
    \includegraphics[width=0.9\linewidth]{figs/gFrame.pdf}
    \caption{\textbf{Illustration of the coarse-to-fine generation process in HOG-Diff using the generalized OU bridge.}}
    \label{fig:gFrame}
\end{figure}


We provide the pseudo-code of the training process in \cref{alg:train}. In our experiments, we adopt a two-step generation process, \ie, $K=2$.  
We initialize $\mathcal{S}^{(0)} = \bm{G}$; under this specific condition, the cell complex filtering operation returns the input unchanged. The set $\mathcal{S}^{(1)}$ corresponds to the 2-cell complex for molecule generation tasks or the 3-simplicial complex for generic graph generation tasks. 
We set $\mathcal{S}^{(2)} = \varnothing$, and for this particular case, we define the cell complex filtering function as
$\operatorname{CCF}(\bm{G}, \varnothing) = \mathcal{N}(\bm{0}, \bm{I})$.

\input{Content/blocks/alg-train}

The pseudo-code of sampling with HOG-Diff is described in \cref{alg:sample}.
The reverse diffusion processes are divided into $K$ hierarchical time windows, denoted as  $\{[\tau_{k-1},\tau_k]\}_{k=1}^K$, where $0 = \tau_0 < \cdots < \tau_{k-1}< \tau_k < \cdots < \tau_K = T$.
We first initialize the sampling process by drawing samples for $\widehat{\bm{X}}_{\tau_K}$ and $\widehat{\bm{\Lambda}}_{\tau_K}$ from a standard Gaussian distribution, and $\widehat{\bm{U}}_0$ is sampled uniformly from the eigenvector matrices of the Laplacian matrix in the training dataset. 
The reverse-time process starts at $\tau_K$ and iteratively updates $\widehat{\bm{X}}_t$ and $\widehat{\bm{\Lambda}}_t$ by solving the reverse-time SDEs with the denoising network $\bm{s_\theta}^{(k)}$.
Subsequently, we reconstruct the Laplacian matrix $\widehat{\bm{L}}_t$ using the fixed eigenvector matrix $\widehat{\bm{U}}_0$ and the updated eigenvalues $\widehat{\bm{\Lambda}}_t$.
%
Endpoint of one generation step serves as the starting point for the next process.
%
Finally, after iterating through all diffusion segments, the algorithm returns the final feature matrix $\widehat{\bm{X}}_0$ and adjacency matrix $\widehat{\bm{A}}_0$, thereby completing the graph generation process. 



\input{Content/blocks/alg-sample}




\section{Complexity Analysis}
\label{app:complexity}



When the targeted graph is not in the desired higher-order forms, one should also consider the one-time preprocessing procedure for graph filtering.

Cell filtering can be dramatically accelerated because it avoids explicitly finding all cells and only determines whether nodes and edges belong to a cell. Specifically, the $2$-cell filter requires only checking whether each edge belongs to some cycle.

One method to achieve the $2$-cell filter is using a depth-first search (DFS). Starting from the adjacency matrix, we temporarily remove the edge $(i, j)$ and initiate a DFS from node $i$, keeping track of the path length. If the target node $j$ is visited within a path length of $l$, the edge $(i, j)$ is marked as belonging to a $2$-cell of length at most $l$. In sparse graphs with $n$ nodes and $m$ edges, the time complexity of a single DFS is $\mathcal{O}(m + n)$. With the path length limited to $l$, the DFS may traverse up to $l$ layers of recursion in the worst case. Therefore, the complexity of a single DFS is $\mathcal{O}(\min(m + n, l \cdot k_{max})) $, where $k_{max}$ is the maximum degree of the graph. For all $m$ edges, the total complexity is
$\mathcal{O}\left(m \cdot \min(m + n, l \cdot k_{max})\right)$.




Alternatively, matrix operations can be utilized to accelerate this process. By removing the edge $(i, j)$ from the adjacency matrix $A$ to obtain $\bar{A}$, the presence of a path of length $l$ between $i$ and $j$ can be determined by checking whether $\bar{A}^l_{i,j} > 0$. This indicates that the edge $(i, j)$ belongs to a $2$-cell with a maximum length of $l+1$. Assuming the graph has $n$ nodes and $m$ edges, the complexity of sparse matrix multiplication is $\mathcal{O}(mn)$. Since $l$ matrix multiplications are required, the total complexity is: $\mathcal{O}(l \cdot m^2 \cdot n)$. While this complexity is theoretically higher than the DFS approach, matrix methods can benefit from significant parallel acceleration on modern hardware, such as GPUs and TPUs. In practice, this makes the matrix-based method competitive, especially for large-scale graphs or cases where $l$ is large.



For simplicial complexes, the number of $p$-simplices in a graph with $n$ nodes and $m$ edges is upper-bounded by $\mathcal{O}(n^{p-1})$, and they can be enumerated in $\mathcal{O}( a\left(\mathcal{G}\right)^{p-3} m)$ time \cite{chiba1985arboricity}, where $a\left(\mathcal{G}\right)$ is the arboricity of the graph $\mathcal{G}$, a measure of graph sparsity.
Since arboricity is demonstrated to be at most $\mathcal{O}(m^{1/2})$ and $m \leq n^2$, all $p$-simplices can thus be listed in $\mathcal{O}\left( n^{p-3} m \right)$.
Besides, the complexity of finding $2$-simplex is estimated to be $\mathcal{O}(\left\langle k \right\rangle m)$ with the Bron–Kerbosch algorithm \cite{find_cliques1973}, where $\left \langle k \right \rangle$ denotes the average node degree, typically a small value for empirical networks.


\section{Experimental Setup}
\label{app:exp_set}





\subsection{Computing Resources}
In this work, all experiments are conducted using PyTorch on a single NVIDIA L40S GPU with 46 GB memory and AMD EPYC 9374F 32-Core Processor.


\subsection{Generic Graph Generation}


We follow the experimental and evaluation setting from \citet{GDSS+ICML2022} with the same train/test split to ensure a fair comparison with baselines.
%
We use node degree and spectral features of the graph Laplacian decomposition as hand-crafted input features.

\cref{tab:data_summary} summarizes the key characteristics of the datasets utilized in this study. The table outlines the type of dataset, the total number of graphs, and the range of graph sizes ($|V|$). Additionally, it also provides the number of distinct node types and edge types for each dataset. Notably, the synthetic datasets (Community-small and Ego-small) contain relatively small graphs, whereas the molecular datasets (QM9 and ZINC250k) exhibit more diversity in graph size and complexity. %, as reflected by higher average numbers of nodes and edges.


\input{Content/blocks/data_summary}

\subsection{Molecule Generation}
\label{app:mol}


% @ CDGS
Early efforts in molecule generation introduce sequence-based generative models and represent molecules as SMILES strings \cite{SMILES-ICML2017}. 
%
Nevertheless, this representation frequently encounters challenges related to long dependency modelling and low validity issues, as the SMILES string fails to ensure absolute validity. 
Therefore, in recent studies, graph representations are more commonly employed for molecule structures where atoms are represented as nodes and chemical bonds as connecting edges \cite{GDSS+ICML2022}.
Consequently, this shift has driven the development of graph-based methodologies for molecule generation, which aim to produce valid, meaningful, and diverse molecules.


% @ GDSS / CDGS
In experiments, each molecule is preprocessed into a graph comprising adjacency matrix $\bm{A}\in \{0,1,2,3\}^{n\times n}$ and node feature matrix $\bm{X}\in \{0,1\}^{n\times d}$, where $n$ denotes the maximum number of atoms in a molecule of the dataset, and $d$ is the number of possible atom types. The entries of $\bm{A}$ indicate the bond types: 0 for no bound, 1 for the single bond, 2 for the double bond, and 3 for the triple bond. 
Further, we scale $\bm{A}$ with a constant scale of 3 in order to bound the input of the model in the interval [0, 1], and rescale the final sample of the generation process to recover the bond types.
%
Following the standard procedure \cite{GraphAF-ICLR2020, GraphDF-ICML2021}, all molecules are kekulized by the RDKit library \cite{Rdkit2016} with hydrogen atoms removed. In addition, we make use of the valency correction proposed by \citet{Moflow-SIGKDD2020}. 
After generating samples by simulating the reverse diffusion process,  the adjacency matrix entries are quantized to discrete values ${0, 1, 2, 3}$ by by applying value clipping. Specifically, values in $(-\infty, 0.5)$ are mapped to 0, $[0.5, 1.5)$ to 1, $[1.5, 2.5)$ to 2, and $[2.5, +\infty)$ to 3, ensuring the bond types align with their respective categories.




% 分子图指标介绍 @GPrinFlowNet, has modified
To comprehensively assess the quality of the generated molecules across datasets, we evaluate 10,000 generated samples using several key metrics: validity, validity w/o check, Frechet ChemNet Distance (FCD) \cite{FCD}, Neighborhood Subgraph Pairwise Distance Kernel (NSPDK) MMD \cite{NSPKD-MMD}, uniqueness, and novelty \cite{GDSS+ICML2022}.
% 1. FCD
\textbf{FCD} quantifies the similarity between generated and test molecules by leveraging the activations of ChemNet's penultimate layer, accessing the generation quality within the chemical space.
% 2. NSPDK-MMD
In contrast, \textbf{NSPDK-MMD} evaluates the generation quality from the graph topology perspective by computing the MMD between the generated and test sets while considering both node and edge features.
% 3. validity
\textbf{Validity} is measured as the fraction of valid molecules to all generated molecules after applying post-processing corrections such as valency adjustments or edge resampling, while \textbf{validity w/o correction}, following \citet{GDSS+ICML2022}, computes the fraction of valid molecules before any corrections, providing insight into the intrinsic quality of the generative process. 
Whether molecules are valid is generally determined by compliance with the valence rules in RDkit \cite{Rdkit2016}.
% 4. novelty
\textbf{Novelty} assesses the model’s ability to generalize by calculating the percentage of generated graphs that are not subgraphs of the training set, with two graphs considered identical if isomorphic.
% 5. uniqueness
\textbf{Uniqueness} quantifies the diversity of generated molecules as the ratio of unique samples to valid samples, removing duplicates that are subgraph-isomorphic, ensuring variety in the output.





\section{Visualization Results}
\label{app:vis}

In this section, we additionally provide the visualizations of the generated graphs for both molecule generation tasks and generic graph generation tasks.
Figs.~\ref{fig:qm9}-\ref{fig:enzymes} illustrate non-curated generated samples. HOG-Diff demonstrates the capability to generate high-quality samples that closely resemble the topological properties of empirical data while preserving essential structural details.

\begin{figure}[!ht]
    \centering
    \includegraphics[width=0.96\linewidth]{figs/vis_qm9.pdf}
    \caption{Visualization of random samples taken from the HOG-Diff trained on the QM9 dataset. }
    \label{fig:qm9}
\end{figure}

\begin{figure}
    \centering
    \includegraphics[width=0.96\linewidth]{figs/vis_zinc250k.pdf}
    \caption{Visualization of random samples taken from the HOG-Diff trained on the Zinc250k dataset. }
    \label{fig:zinc250k}
\end{figure}

\begin{figure}
    \centering
    \includegraphics[width=0.96\linewidth]{figs/vis_cs.pdf}
    \caption{Visual comparison between training set graph samples and generated graph samples produced by HOG-Diff on the Community-small dataset.}
\end{figure}

\begin{figure}
    \centering
    \includegraphics[width=0.96\linewidth]{figs/vis_ego.pdf}
    \caption{Visual comparison between training set graph samples and generated graph samples produced by HOG-Diff on the Ego-small dataset.}
\end{figure}

\begin{figure}
    \centering
    \includegraphics[width=0.96\linewidth]{figs/vis_enzymes.pdf}
    \caption{Visual comparison between training set graph samples and generated graph samples produced by HOG-Diff on the Enzymes dataset.}
    \label{fig:enzymes}
\end{figure}
