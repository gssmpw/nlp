\section{Higher-order Guided Diffusion Model}
We now present our \textit{Higher-order Guided Diffusion} (HOG-Diff) model, which enhances graph generation by exploiting higher-order structures. 
We begin by detailing a coarse-to-fine generation curriculum that incrementally constructs graphs, followed by the introduction of three essential supporting techniques: the diffusion bridge, spectral diffusion, and a denoising model, respectively. Finally, we provide theoretical evidence validating the efficacy of HOG-Diff.


%\label{sec:gen-curriculum}
\paragraph{Coarse-to-fine Generation}
We draw inspiration from curriculum learning, a paradigm that mimics the human learning process by systematically organizing data in a progression from simple to complex \cite{curriculum-IJCV2022}.
Likely, an ideal graph generation curriculum should be a composition of multiple easy-to-learn and meaningful intermediate steps.
Additionally, higher-order structures encapsulate rich structural properties beyond pairwise interactions that are crucial for various empirical systems \cite{HiGCN2024}. 
As a graph-friendly generation framework, HOG-Diff incorporates higher-order structures during the intermediate stages of forward diffusion and reverse generative processes, thereby realizing a coarse-to-fine generation curriculum.

To implement our coarse-to-fine generation curriculum, we introduce a key operation termed cell complex filtering (CCF).
As illustrated in \cref{fig:cell-transform}, CCF generates an intermediate state of a graph by pruning nodes and edges that do not belong to a given cell complex.

\begin{definition}[Cell complex filtering]
Given a graph $\bm{G} = (\bm{V},\bm{E})$ and its associated cell complex $\mathcal{S}$, the cell complex filtering operation produces a filtered graph $\bm{G}^\prime = (\bm{V}^\prime,\bm{E}^\prime)$ where 
$\bm{V}^{\prime} = \{ v \in \bm{V}  \mid \exists\;x_\alpha \in \mathcal{S} : v \in x_\alpha \}$, 
and $\bm{E}^{\prime} = \{ (u, v) \in \bm{E} \mid \exists\;x_\alpha \in \mathcal{S} : u,v \in x_\alpha\}$.
\end{definition}

This filtering operation is a pivotal step in decomposing the graph generation task into manageable sub-tasks, with the filtered results serving as natural intermediaries in hierarchical graph generation. 
The overall framework of our proposed framework is depicted in \cref{fig:framework}.
Specifically, the forward and reverse diffusion processes in HOG-Diff are divided into $K$ hierarchical time windows, denoted as  $\{[\tau_{k-1},\tau_k]\}_{k=1}^K$, where $0 = \tau_0 < \cdots < \tau_{k-1}< \tau_k < \cdots < \tau_K = T$. 
Our sequential generation progressively denoises the higher-order skeletons.  
First, we generate coarse-grained higher-order skeletons, and subsequently refine them into finer pairwise relationships, simplifying the task of capturing complex graph distributions. 
%
This coarse-to-fine approach inherently aligns with the hierarchical nature of many real-world graphs, enabling smoother training and improved sampling performance.



Formally, our generation process factorizes the joint distribution of the final graph $\bm{G}_0$ into a product of conditional distributions across these time windows:
\begin{equation}
p(\bm{G}_0)=p(\bm{G}_0|\bm{G}_{\tau_1})p(\bm{G}_{\tau_1}|\bm{G}_{\tau_2}) \cdots p(\bm{G}_{\tau_{K-1}}|\bm{G}_{T}).
\end{equation}
Here, intermediate states $\bm{G}_{\tau_1}, \bm{G}_{\tau_2}, \cdots, \bm{G}_{\tau_{K-1}}$ represents different levels of cell filtering results of the corresponding graph. 
Our approach aligns intermediate stages of the diffusion process with realistic graph representations, and the ordering reflects a coarse-to-fine generation process.




The forward process introduces noise in a stepwise manner while preserving intermediate structural information. During each time window $[\tau_{k-1}, \tau_k]$, the evolution of the graph is governed by the following forward SDE:
\begin{equation}
\label{eq:HoGD-forward}
\mathrm{d}\bm{G}_t^{(k)}=\mathbf{f}_{k,t}(\bm{G}_t^{(k)})\mathrm{d}t+g_{k,t}\mathrm{d}\bm{W}_t, t \in [\tau_{k-1}, \tau_k].
\end{equation}

Reversing this process enables the model to generate authentic samples with desirable higher-order information.
The reverse-time SDE corresponds to \cref{eq:HoGD-forward} is as follows:
\begin{equation}
%\fontsize{9pt}{9pt}\selectfont
\begin{split}
    \mathrm{d}\bm{G}_t^{(k)}
     = & \left[\mathbf{f}_{k,t}(\bm{G}_t^{(k)})-g_{k,t}^2\nabla_{\bm{G}_t^{(k)}}\log p_t(\bm{G}_t^{(k)})\right]\mathrm{d}\bar{t} \\
    & +g_{k,t}\mathrm{d}\bar{\bm{W}}_t.
\end{split}
%\fontsize{10pt}{10pt}\selectfont
\end{equation}
Instead of using higher-order information as a direct condition, HOG-Diff employs it to guide the generation process through multiple steps. This strategy allows the model to incrementally build complex graph structures while maintaining meaningful structural integrity at each stage.
Moreover, integrating higher-order structures into graph generative models improves interpretability by allowing analysis of their significance in shaping the graph’s properties.







%\subsection{Diffusion Bridge}
%\label{sec:bridge}
\paragraph{Diffusion Bridge Process}
% -------- OU process
We realize the guided diffusion based on the generalized Ornstein-Uhlenbeck (GOU) process \cite{GOU1988, IRSDE+ICML2023}, a stationary Gaussian-Markov process characterized by its mean-reverting property. 
Over time, the marginal distribution of the GOU process stabilizes around a fixed mean and variance, making it well-suited for stochastic modelling with terminal constraints. 
The GOU process $\mathbb{Q} $ is governed by the following SDE:
\begin{equation}
\mathbb{Q}: \mathrm{d} \bm{G}_t = \theta_t(\bm{\mu} -\bm{G}_t)\mathrm{d}t + g_t(\bm{G}_t)\mathrm{d}\bm{W}_t,
\label{eq:GOU-SDE}
\end{equation}
where $\bm{\mu}=\bm{G}_{\tau_k}$ is the target terminal state, $\theta_t$ denotes a scalar drift coefficient and $g_t$ represents the diffusion coefficient. 
To ensure the process remains analytically tractable, $\theta_t$ and $g_t$ are constrained by the relationship $g_t^2/\theta_t = 2\sigma^2$ \cite{IRSDE+ICML2023}, where $\sigma^2$ is a given constant scalar.
%
Under these conditions, its transition probability admits a closed-form solution:
\begin{equation}
\begin{split}
p(&\bm{G}_{t}\mid \bm{G}_s) 
=\mathcal{N}(\mathbf{m}_{s:t},v_{s:t}^{2}\bm{I})  \\
&=  \mathcal{N}\left(
\bm{\mu}+\left(\bm{G}_s-\bm{\mu}\right)e^{-\bar{\theta}_{s:t}},
\sigma^2 (1-e^{-2\bar{\theta}_{s:t}})\bm{I}
\right).
\end{split}
\label{eq:GOU-p}
\end{equation}
Here, $\bar{\theta}_{s:t}=\int_s^t\theta_zdz$, and for notional simplicity, $\bar{\theta}_{0:t}$ is replaced by $\bar{\theta}_t$ when $s=0$.
% 
At time $t$ progress, $p(\bm{G}_t)$ gradually approaches a Gaussian distribution characterized by mean $\bm{\mu}$ and variance $\sigma^2$, indicating that the GOU process exhibits the mean-reverting property.  




% ---------- OU Bridge
The Doob’s $h$-transform can modify an SDE such that it passes through a specified endpoint.
When applied to the GOU process, this eliminates variance in the terminal state, driving the diffusion process toward a Dirac distribution centered at $\bm{G}_{\tau_k}$ \cite{GOUB2021,GOUB+ICML2024}.


\begin{proposition}
\label{pro:OUB}
\input{Content/blocks/theorem1}
\end{proposition}


We can directly use the closed-form solution in \Cref{pro:OUB} for one-step forward sampling without performing multi-step forward iteration using the SDE.
%
The reverse-time dynamics of the conditioned process can be derived using the theory of SDEs and take the following form:
\begin{equation}
%\fontsize{9pt}{9pt}\selectfont
 \mathrm{d}\bm{G}_t = \left[\mathbf{f}_{k,t}(\bm{G}_t)-g_{k,t}^2 \nabla_{\bm{G}_t} \log p(\bm{G}_t|\bm{G}_{\tau_k})\right] \diff\bar{t} + g_{k,t} \diff\bar{\bm{W}}_t, \nonumber
% \fontsize{10pt}{10pt}\selectfont
\end{equation}
where $\mathbf{f}_{k,t}(\bm{G}_t) = \theta_t \left( 1 + \frac{2}{e^{2\bar{\theta}_{t:\tau_k}}-1}  \right)(\bm{G}_{\tau_k} - \bm{G}_t)$.


\paragraph{Spectral Diffusion}
Generating graph adjacency matrices presents several significant challenges.
% 1.non-uniqueness of adjacency matrix 
Firstly, the non-uniqueness of graph representations implies that a graph with $n$ vertices can be equivalently modelled by up to $n!$ distinct adjacency matrices. This ambiguity requires a generative model to assign probabilities uniformly across all equivalent adjacencies to accurately capture the graph’s inherent symmetry. 
%
% 2.Sparsity
Additionally, unlike densely distributed image data, graphs typically follow a Pareto distribution and exhibit sparsity \cite{Sparsity+NP2024}, so that adjacency score functions lie on a low-dimensional manifold. Consequently, noise injected into out-of-support regions of the full adjacency space severely degrades the signal-to-noise ratio, impairing the training of the score-matching process. 
Even for densely connected graphs, isotropic noise distorts global message-passing patterns by encouraging message-passing on sparsely connected regions.
%
% 3. diffusion faster / scalability
Moreover, the adjacency matrix scales quadratically with the number of nodes, making the direct generation of adjacency matrices computationally prohibitive for large-scale graphs.

To address these challenges, inspired by~\citet{GAN2-Spectre,GSDM+TPAMI2023}, we introduce noise in the eigenvalue domain of the graph Laplacian matrix $\bm{L}=\bm{D}-\bm{A}$, instead of the adjacency matrix $\bm{A}$, where $\bm{D}$ denotes the diagonal degree matrix.
%
As a symmetric positive semi-definite matrix, the graph Laplacian can be diagonalized as $\bm{L} = \bm{U} \bm{\Lambda} \bm{U}^\top$. Here, the orthogonal matrix $\bm{U} = [\bm{u}_1,\cdots,\bm{u}_n]$ comprises the eigenvectors, and the diagonal matrix $\bm{\Lambda} = \operatorname{diag}(\lambda_1,\cdots,\lambda_n)$ holds the corresponding eigenvalues.
The relationship between the Laplacian spectrum and the graph's topology has been extensively explored~\cite{chung1997spectral}. For instance,  the low-frequency components of the spectrum capture the global structural properties such as connectivity and clustering, whereas the high-frequency components are crucial for reconstructing local connectivity patterns.
%
%
%% ------- 
Therefore, the target graph distribution $p(\bm{G}_0)$ represents a joint distribution of $\bm{X}_0$ and $\bm{\Lambda}_0$, exploiting the permutation invariance and structural robustness of the Laplacian spectrum.
%
Consequently, we split the reverse-time SDE into two parts that share drift and diffusion coefficients as
\begin{equation}
\fontsize{8pt}{8pt}\selectfont
\left\{
\begin{aligned}
\mathrm{d}\bm{X}_t=
&\left[\mathbf{f}_{k,t}(\bm{X}_t)
-g_{k,t}^2 
\nabla_{\bm{X}} \log p_t(\bm{G}_t | \bm{G}_{\tau_k}) \right]\mathrm{d}\bar{t}
+g_{k,t}\mathrm{d}\bar{\bm{W}}_{t}^1
\\
\mathrm{d}\bm{\Lambda}_t=
&\left[\mathbf{f}_{k,t}(\bm{\Lambda}_t)
- g_{k,t}^2 \nabla_{\bm{\Lambda}} \log p_t(\bm{G}_t | \bm{G}_{\tau_k})\right]\mathrm{d}\bar{t}
+g_{k,t}\mathrm{d}\bar{\bm{W}}_{t}^2
\end{aligned}
\right..\nonumber
\fontsize{10pt}{10pt}\selectfont
\label{eq:reverse-HoGD}
\end{equation}
Here, the superscript of $\bm{X}^{(k)}_t$ and $\bm{\Lambda}^{(k)}_t$ are dropped for simplicity, and $\mathbf{f}_{k,t}$ is determined according to \Cref{pro:OUB}.



To approximate the score functions $\nabla_{\bm{X}_t} \log p_t(\bm{G}_t | \bm{G}_{\tau_k})$ and $\nabla_{\bm{\Lambda}_t} \log p_t(\bm{G}_t | \bm{G}_{\tau_k})$, we employ a neural network $\bm{s}^{(k)}_{\bm{\theta}}(\bm{G}_t, \bm{G}_{\tau_k},t)$, composed of a node ($\bm{s}^{(k)}_{\bm{\theta},\bm{X}}(\bm{G}_t, \bm{G}_{\tau_k},t)$) and a spectrum ($\bm{s}^{(k)}_{\bm{\theta},\bm{\Lambda}}(\bm{G}_t, \bm{G}_{\tau_k},t)$) output, respectively.
The model is optimized by minimizing the loss function:
\fontsize{8pt}{8pt}\selectfont
\begin{align}
\ell^{(k)}(\bm{\theta})=&
\mathbb{E}_{t,\bm{G}_t,\bm{G}_{\tau_{k-1}},\bm{G}_{\tau_k}} \{\omega(t) [c_1\|
\bm{s}^{(k)}_{\bm{\theta},\bm{X}} - \nabla_{\bm{X}} \log p_t(\bm{G}_t | \bm{G}_{\tau_k})\|_2^2 \nonumber \\  
+&c_2 ||\bm{s}^{(k)}_{\bm{\theta},\bm{\Lambda}} - \nabla_{\bm{\Lambda}} \log p_t(\bm{G}_t | \bm{G}_{\tau_k})||_2^2]\}, \label{eq:final-loss}  
\end{align}
\normalsize 
where $\omega(t)$ is a positive weighting function, and $c_1, c_2$ controls the relative importance of vertices and spectrum.
The training procedure is detailed in \cref{alg:train} in the Appendix.




We sample $(\bm{X}_{\tau_K},\bm{\Lambda}_{\tau_K})$ from the prior distribution and uniformly sample $\bm{U}_0$ from the observed eigenvector matrices.
The generation process involves multi-step diffusion to produce samples $(\hat{\bm{X}}_{\tau_{K-1}}, \hat{\bm{\Lambda}}_{\tau_{K-1}}), \cdots, (\hat{\bm{X}}_{\tau_1}, \hat{\bm{\Lambda}}_{\tau_1}), (\hat{\bm{X}}_0, \hat{\bm{\Lambda}}_0)$ in sequence by reversing the diffusion bridge, where the endpoint of one generation step serves as the starting point for the next. 
Finally, plausible samples with higher-order structures $\hat{\bm{G}}_0=(\hat{\bm{X}}_0 , \hat{\bm{L}}_0 =\bm{U}_0 \hat{\bm{\Lambda}}_0 \bm{U}_0^\top)$ can be reconstructed.
The complete sampling procedure is outlined in \cref{alg:sample} within the Appendix.

\input{Content/blocks/mol_rel}

\paragraph{Denoising Network Architecture}
We design a neural network $\bm{s}^{(k)}_{\bm{\theta}}(\bm{G}_t, \bm{G}_{\tau_k},t) $ to estimate score functions in \cref{eq:reverse-HoGD}.
Standard graph neural networks designed for classical tasks such as graph classification and link prediction may be inappropriate for graph distribution learning due to the immediate real-number graph states and the complicated requirements. 
For example, an effective model for molecular graph generation should capture local node-edge dependence for chemical valency rules and attempt to recover global graph patterns like edge sparsity, frequent ring subgraphs, and atom-type distribution.



% Our designed model
To achieve this, we introduce a unified denoising network that explicitly integrates node and spectral representations. 
As illustrated in \cref{fig:denoising-model} of Appendix, the network comprises two different graph processing modules: a standard graph convolution network (GCN) \cite{GCN+ICLR2017} for local feature aggregation and a graph transformer network (ATTN)~ \cite{TFmodel2021AAAIworkshop,DiGress+ICLR2023} for global information extraction.
The outputs of these modules are fused with time information through a Feature-wise Linear Modulation (FiLM) layer~\cite{Film+AAAI2018}. The resulting representations are concatenated to form a unified hidden embedding.
This hidden embedding is further processed through separate multilayer perceptrons (MLPs) to produce predictions for $\nabla_{\bm{X}} \log p(\bm{G}_t|\bm{G}_{\tau_k})$ and $\nabla_{\bm{\Lambda}} \log p(\bm{G}_t|\bm{G}_{\tau_k})$, respectively.
%
It is worth noting that our graph noise prediction model is permutation equivalent as each component of our model avoids any node ordering-dependent operations.
%
Our model is detailed in \cref{app:denoising-model}.






\paragraph{Theoretical Analysis}
%\subsection{Theoretical Analysis}
%\label{sec:theorms}
In the following, we provide supportive theoretical evidence for the efficacy of HOG-Diff, demonstrating that the proposed framework achieves faster convergence in score-matching and tighter reconstruction error bounds compared to standard graph diffusion works.

\begin{proposition}[Informal]
\label{pro:training}
\input{Content/blocks/theorem2}
\end{proposition}



Following \citet{GSDM+TPAMI2023},  we define the expected reconstruction error at each generation process as $\mathcal{E}(t)=\mathbb{E}\norm{\bar{\bm{G}}_t-\widehat{\bm{G}}_t}^2$, where $\bar{\bm{G}}_t$ represents the data reconstructed sing the ground truth score $\nabla \log p_t(\cdot)$ and $\widehat{\bm{G}}_t$ denotes the data reconstructed with the learned score function $\bm{s}_{\bm{\theta}}$.
We establish that the reconstruction error in HOG-Diff is bounded more tightly than in classical graph generation models, ensuring superior sample quality.
\begin{proposition}
\label{pro:reconstruction-error}
\input{Content/blocks/theorem3}
\end{proposition}


The propositions above rely primarily on mild assumptions, such as smoothness and boundedness, without imposing strict conditions like the target distribution being log-concave or satisfying the log-Sobolev inequality.
%
Their formal statements and detailed proofs are postponed to \cref{app:proof}.
We experimentally verify these Propositions in \Cref{sec:ablations}.