\section{Introduction}

\begin{figure}
%\vspace{-0.1in}
\centering
\includegraphics[width=0.99\linewidth]{figs/framework.pdf}
\caption{\textbf{Overivew of HOG-Diff.} The dashed line above illustrates the classical generation process, where graphs quickly degrade into random structures with uniformly distributed entries. In contrast, as shown in the coloured region below, HOG-Diff adopts a coarse-to-fine generation curriculum based on the diffusion bridge, explicitly learning higher-order structures during intermediate steps with theoretically guaranteed performance.}
\label{fig:framework}
\vspace{-4mm}
\end{figure}


%%%%% ----- 1.1 what are graphs? 介绍什么是图，
Graphs provide an elegant abstraction for representing complex empirical phenomena by encoding entities as vertices and their relationships as edges, thereby transforming unstructured data into analyzable representations.
%
%%%%% ----- 1.2 The meaning of  Graph Generation 研究意义。有什么用？
Modelling the underlying distribution of graph-structured data is a crucial yet challenging task with broad applications, including social network analysis, motion synthesis, drug discovery, protein design, and urban planning~\cite{zhu2022survey}.
%
%%%%% ----- 1.3 Traditional models 
The study of graph generation seeks to synthesize graphs that align with the observed distribution and traces back to seminal models of random network models \cite{ER1960, BA1999}.
While these models offer foundational insights, they are often too simplistic to capture the complexity of graph distributions we encounter in practice.


%%%%% ----- 2. Recent studies about graph generative models (深度生成模型)
Recently, advances in generative models have leveraged the power of deep neural networks to significantly improve the ability to learn graph distributions.
Notable approaches include models based on recurrent neural networks (RNNs) \cite{GraphRNN2018},  variational autoencoders (VAEs) \cite{VAE-Jin2018}, and generative adversarial networks (GANs) \cite{GAN1-MolGAN, GAN2-Spectre}.
However, the end-to-end structure of these methods makes them hard to train.
%
%% Diffusion-based generative models
More recently, diffusion-based models have achieved remarkable success in image generation by learning a model to denoise a noisy sample \cite{DDPM+NeurIPS2020, Score-SDE+ICLR2021}. 
%
With the advent of diffusion models, their applications on graphs with complex topological structural properties have recently aroused significant scientific interest \cite{EDPGNN-2020,GDSS+ICML2022,DiGress+ICLR2023}.




%%%%% ----- 3. The difference in graph generation 
%%%%% This part gives the motivation for our work
Despite these advances, existing graph generative models typically inherit the frameworks designed for image generation \cite{Score-SDE+ICLR2021}, which fundamentally limits their ability to capture the intrinsic topological properties of networks. 
%
%-----  3.1 higher-order structures are crucial for graphs 
Notably, networks exhibit higher-order structures, such as motifs, simplices, and cells, which capture multi-way interactions and critical topological dependencies beyond pairwise relationships \cite{HigherOrderReview2020,ISMnet2024,TDL-position+ICML2024}.
These structures are vital for representing complex phenomena in domains like molecular graphs, social networks, and protein interactions.
However, current methods are ineffective at modelling the topological properties of higher-order systems since \emph{learning to denoise the noisy samples does not explicitly preserve the intricate structural dependencies required for generating realistic graphs}.


% 
Moreover, the image corrupted by Gaussian noise retains recognizable numerical patterns during the early and middle stages of forward diffusion. By contrast, the graph adjacency matrix quickly degrades into a dense matrix with uniformly distributed entries within a few diffusion steps. 
%
% 3.3 
In addition, directly applying diffusion-based generative models to graph topology generation by injecting isotropic Gaussian noise to adjacency matrices is harmful as it destroys critical graph properties such as sparsity and connectivity.
%
%
%% 3.4 Permutation equivalence.  
Lastly, such a framework should ensure equivariance\footnote{invariance as a particular special case}, maintaining the learned distribution despite node index permutations, which is essential for robustness and capturing intrinsic graph distribution.
%
%
Therefore, a graph-friendly diffusion process should also retain meaningful intermediate states and trajectories, avoid inappropriate noise addition, and ensure equivariance.




% 4.3 Introduction to our framework
% Coarse-to-fine generation
Motivated by these principles and advances in \emph{topological deep learning}~\cite{hajij2022topological,TDL-position+ICML2024}, we propose the \textbf{Higher-order Guided Diffusion} (HOG-Diff) framework, illustrated in \cref{fig:framework}, to address the gaps in graph generation. 
HOG-Diff introduces a coarse-to-fine generation curriculum that enhances the model’s ability to capture complex graph properties by preserving higher-order topologies throughout the diffusion process.
% 
Specifically, we decompose the graph generation task into manageable sub-tasks, beginning by generating higher-order graph skeletons that capture core structures, which are then refined to include pairwise interactions and finer details, resulting in complete graphs with both topological and semantic fidelity.
%
Additionally, HOG-Diff integrates diffusion bridge and spectral diffusion to ensure effective generation and adherence to the aforementioned graph generation principles. 
%
Our theoretical analysis reveals that HOG-Diff converges more rapidly in score matching and achieves sharper reconstruction error bounds than classical approaches, offering strong theoretical support for the proposed framework.
Furthermore, our framework promises to enhance interpretability by enabling the analysis of different topological guides’ performance in the generation process.
%
%
%%%%% ----- 5. Our Contributions
The contributions of this paper are threefold:
\vspace{-0.1in}
\begin{itemize}[noitemsep, parsep=0.3pt, leftmargin=*]
%\begin{itemize}[leftmargin=*]
\item \textbf{Algorithmic}: we introduce a coarse-to-fine graph generation curriculum guided by higher-order topological information using the OU diffusion bridge. 
\item \textbf{Theoretical}: our analysis reveals that HOG-Diff achieves faster convergence during score-matching and a sharper reconstruction error bound compared to classical methods.
\item \textbf{Experimental}: extensive evaluations show that HOG-Diff achieves state-of-the-art graph generation performance across various datasets, highlighting the functional importance of topological guidance.\vspace{-2mm}
\end{itemize}

