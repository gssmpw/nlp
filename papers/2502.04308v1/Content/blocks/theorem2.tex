Suppose the loss function $\ell^{(k)}(\bm{\theta})$ in \cref{eq:final-loss} is $\beta$-smooth and satisfies the $\mu$-PL condition in the ball $B\left(\boldsymbol{\theta}_0, R\right)$. 
Then, the expected loss at the $i$-th iteration of the training process satisfies:
\begin{equation}
%\fontsize{7pt}{7pt}\selectfont
\mathbb{E}\left[\ell^{(k)}(\bm{\theta}_i)\right] 
\leq \left(1-\frac{b\mu^2}{\beta N(\beta N^2+\mu(b-1))}\right)^i \ell^{(k)}\left(\bm{\theta}_0\right),\nonumber
%\fontsize{10pt}{10pt}\selectfont
\end{equation}
where $N$ denotes the size of the training dataset, and $b$ is the mini-batch size.
Furthermore, it holds that $\beta_{\text{HOG-Diff}}\leq \beta_{\text{classical}}$, implying that the distribution learned by the proposed framework converges to the target distribution faster than classical generative models.