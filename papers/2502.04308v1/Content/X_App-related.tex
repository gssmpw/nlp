

\subsection{Graph Generative Models}
Graph generation has been extensively studied, which dates back to the early works of the random network models, such as the Erdős–Rényi (ER) model \cite{ER1960} and the Barabási-Albert (BA) model \cite{BA1999}.
Recent graph generative models make great progress in graph distribution learning by exploiting the capacity of deep neural networks. 
%
GraphRNN \cite{GraphRNN2018} and GraphVAE \cite{GraphVAE-DrugDiscovery} adopt sequential strategies to generate nodes and edges.  
MolGAN \cite{GAN1-MolGAN} integrates generative adversarial networks (GANs) with reinforcement learning objectives to synthesize molecules with desired chemical properties. 
\citet{GraphAF-ICLR2020} generates molecular graphs using a flow-based approach, while GraphDF \cite{GraphDF-ICML2021} adopts an autoregressive flow-based model with discrete latent variables.
Additionally, GraphEBM \cite{GraphEBM2021} employs an energy-based model for molecular graph generation.
However, the end-to-end structure of these methods often makes them more challenging to train compared to diffusion-based generative models.

\subsection{Diffusion-based Generative Models}
A leap in graph generative models has been marked by the recent progress in diffusion-based generative models \cite{Score-SDE+ICLR2021}.
%
EDP-GNN \cite{EDPGNN-2020} generates the adjacency matrix by learning the score function of the denoising diffusion process, while GDSS \cite{GDSS+ICML2022} extends this framework by simultaneously generating node features and an adjacency matrix with a joint score function capturing the node-edge dependency.
%
DiGress \cite{DiGress+ICLR2023} addresses the discretization challenge due to Gaussian noise, while CDGS \cite{CDGS+AAAI2023} designs a conditional diffusion model based on discrete graph structures.
%
GSDM \cite{GSDM+TPAMI2023} introduces an efficient graph diffusion model driven by low-rank diffusion SDEs on the spectrum of adjacency matrices.
% 
% GPrinFlowNet 
GPrinFlowNet \cite{GPrinFlowNet+ACM2024} proposes a semantic-preserving framework based on a low-to-high frequency generation curriculum, where the $k$-th intermediate generation state corresponds to the $k$ smallest principal components of the adjacency matrices.
%
Despite these advancements, current methods are ineffective at modelling the topological properties of higher-order systems since learning to denoise the noisy samples does not explicitly lead to preserving the intricate structural dependencies required for generating realistic graphs.


\subsection{Diffusion Bridge} 
Several recent works have improved the generative framework of diffusion models by leveraging the diffusion bridge processes, \ie, processes conditioned to the endpoints.
%
\citet{wu2022diffusion} inject physical information into the process by incorporating informative prior to the drift.
% 
GLAD \cite{GLAD-ICMLworkshop2024} employs the Brownian bridge on a discrete latent space with endpoints conditioned on data samples.
%
GruM \cite{GruM+ICML2024} utilizes the OU bridge to condition the diffusion endpoint as the weighted mean of all possible final graphs.
%
However, existing methods often overlook or inadvertently disrupt the higher-order topological structures in the graph generation process.

