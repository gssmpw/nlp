\vspace{-1mm}
\section{Experiments}
\vspace{-1mm}
We assess HOG-Diff against state-of-the-art baselines for both molecular and generic graph generation.
Further ablation studies are conducted to analyze the impact of different topological guides.
More details can be found in \cref{app:exp_set}.

\vspace{-1mm}
\subsection{Molecule Generation}

\paragraph{Experimental Setup}
To assess the capability of the proposed method in molecular generation, we conduct evaluations on two well-known molecular datasets: QM9 \cite{data:qm9} and ZINC250k \cite{data:zinc250k}, and obtain the intermedia higher-order skeletons using the 2-cell complex filtering. 
%
% Metrics
We evaluate the quality of 10, 000 generated molecules with six metrics as in \citet{GDSS+ICML2022}: 
Neighborhood Subgraph Pairwise Distance Kernel (\textbf{NSPDK}) MMD \cite{NSPKD-MMD},
Fr{\'e}chet ChemNet Distance (\textbf{FCD}) \cite{FCD},
Validity (\textbf{Val.}), 
Validity without correction (\textbf{Val. w/o corr.}), 
Uniqueness (\textbf{Uni.}), and 
Novelty (\textbf{Nov.}) \cite{GDSS+ICML2022}.
%Please refer to \cref{appx:mol} for a detailed metrics introduction.

\paragraph{Baselines}
We evaluate our model against state-of-the-art molecular generation models, including auto-regressive methods, GraphAF \cite{GraphAF-ICLR2020}, and GraphDF \cite{GraphDF-ICML2021}. 
For a fair comparison, as recommended by \citet{GDSS+ICML2022}, we extend GraphAF and GraphDF to account for formal charges in the molecular generation, termed GraphAF+FC and GraphDF+FC, respectively. 
We also compare our HOG-Diff with various flow-based and diffusion-based methods, including MoFlow \cite{Moflow-SIGKDD2020}, EDP-GNN \cite{EDPGNN-2020}, Graph-EBM \cite{GraphEBM2021}, GDSS \cite{GDSS+ICML2022}, and DiGress \cite{DiGress+ICLR2023}.




\paragraph{Sampling Quality}
We visualize the molecule generation process in \cref{fig:vis_trajectory} with more examples deferred to~\cref{app:vis}.
It can be observed that our model explicitly preserves higher-order structures during the generation process.
\cref{tab:mol_rel} indicates that HOG-Diff consistently outperforms both auto-regressive and one-shot models.
Notably, the dramatic decrease in NSPDK and FCD implies that HOG-Diff is able to generate molecules with data distributions close to those of the real molecules in both the chemical and graph space.







\begin{figure}[!t]
\vspace{-0.1in}
    \centering
    \includegraphics[width=0.96\linewidth]{figs/vis_trajectory.pdf}
    \caption{\textbf{Visualization of molecular graphs at different stages of the reverse generative process.} Model trained on Zinc250k.}
    \label{fig:vis_trajectory}
    \vspace{-6mm}
\end{figure}
\vspace{-2mm}

\subsection{Generic Graph Generation}

\input{Content/blocks/generic_rel}



\paragraph{Experimental Setup}
To display the topology distribution learning ability,
we access HOG-Diff over three common generic graph datasets:
(\textbf{1}) \textbf{Community-small}, containing 100 randomly generated community graphs; 
(\textbf{2}) \textbf{Ego-small}, comprising 200 small ego graphs derived from the Citeseer network dataset; 
(\textbf{3}) \textbf{Enzymes}, featuring 587 protein graphs representing tertiary structures of enzymes from the BRENDA database. 
%
Intermediate higher-order skeletons are obtained through 3-simplicial complex filtering, which prunes nodes and edges that do not belong to three-dimensional simplicial complexes.
%
We employ the same train/test split as \citet{GDSS+ICML2022} for a fair comparison with baselines.
Consistent with \citet{GraphRNN2018}, we employ the maximum mean discrepancy (MMD) to quantify the distribution differences across key graph statistics, including degree (\textbf{Deg.}), clustering coefficient (\textbf{Clus.}), and the number of occurrences of orbits with 4 nodes (\textbf{Orbit}).
To provide a holistic evaluation, we further calculate the mean MMD across these metrics, which is reported in the \textbf{Avg.} column as the overall assessment index.
A lower MMD signifies a closer alignment between the generated and evaluation datasets, indicating superior generative performance.


\paragraph{Baselines}
%
We compare our model with prominent auto-regressive and one-shot graph generation approaches. 
Auto-regressive models sequentially build graphs by adding nodes and edges step-by-step.
Benchmarks in this category include DeepGMG~\cite{deepgmg}, GraphRNN~\cite{GraphRNN2018}, GraphAF~\cite{GraphAF-ICLR2020}, and GraphDF~\cite{GraphDF-ICML2021}.
Conversely, one-shot models generate all nodes and edges simultaneously. 
We include GraphVAE~\cite{GraphVAE-DrugDiscovery}, GNF~\cite{GNF-NeurIPS2019}, GPrinFlowNet~\cite{GPrinFlowNet+ACM2024}, EDP-GNN~\cite{EDPGNN-2020}, GDSS~\cite{GDSS+ICML2022}, and DiGress~\cite{DiGress+ICLR2023}.

\paragraph{Sampling Quality}
The results in \cref{tab:generic_rel} verify that HOG-Diff is not only suitable for molecular generation but also proficient in generic graph generation.
These experiments demonstrate HOG-Diff’s ability to model the intricate interdependencies between nodes and edges effectively. 


\subsection{Ablations: Topological Guide Analysis}
\label{sec:ablations}
%\subsection{Ablation Studies}
\input{Content/blocks/ablation-rel}
During the experiments, we observe that HOG-Diff exhibits superior performance on complex datasets such as QM9 and Zinc250k, but comparatively modest results on the Ego dataset.
Visualizations and statistics in~\cref{app:exp_set,app:vis} indicate that Ego contains the fewest higher-order structures among the datasets analyzed, suggesting that the choice of guide plays a pivotal role in the effectiveness of generation.
To validate this hypothesis, we conduct further ablations using different types of topological information as guides.

Specifically, we employ three types of guiders: structures derived from 2-cell filtering (\textbf{Cell}), peripheral structures obtained by removing cell components (\textbf{Peripheral}), and Gaussian random noise (\textbf{Noise}).
Employing noise as the guide aligns with classical generation models which generate samples by denoising noisy data.
\cref{fig:ablation} (plot) visualizes how the spectrum loss changes during the training process and demonstrates that training the proposed model converges faster compared to the classical method, which aligns with the theoretical results in~\Cref{pro:training}.



The sampling results in~\cref{fig:ablation} (table) show that both peripheral and noise guides are inferior compared to using cell structures as the guide, empirically supporting \Cref{pro:reconstruction-error}. 
This highlights that certain topological structures, such as cells, are more effective in guiding the generation, likely due to their higher-order connectivity and structural significance. 
%
These observations stress the importance of identifying and leveraging proper topological structures as guides, which play a critical role in steering the generative process toward meaningful outputs. 
Moreover, this property opens up promising avenues for exploring the guide’s potential as a tool to diagnose whether a specific component is integral and essential for the architecture.
Furthermore, by systematically analyzing the impact of various guides, we can deepen our understanding of the interplay between structural characteristics and generative performance, thereby advancing the design of more effective graph generative models.






