\section{Preliminaries}


\paragraph{Higher-order Networks}
Graphs are elegant and useful abstractions for various empirical objects; typically, they can be represented as $\bm{G} \triangleq (\bm{V},\bm{E}, \bm{X})$. Here, $\bm{V}$ denote the node set, $\bm{E}\subseteq \bm{V}\times\bm{V}$, and  $\bm{X}$ denotes the nodes feature matrix. 
However, many empirical systems exhibit group interactions that extend beyond simple pairwise relationships \cite{HigherOrderReview2020}.
%
To capture these complex interactions, higher-order networks—such as hypergraphs, simplicial complexes, and cell complexes—offer more expressive alternatives by capturing higher-order interactions among entities \cite{TDL-position+ICML2024}.
%
Among these, cell complexes are fundamental in algebraic topology, offering a flexible generalization of pairwise graphs \cite{Top_Hodge_Hatcher+2001}.
% 

\begin{definition}[Regular cell complex]
A regular cell complex is a topological space $\mathcal{S}$ with a partition into subspaces (cells) $\{x_\alpha\}_{\alpha\in P_\mathcal{S}}$, where $P_\mathcal{S}$ is an index set, satisfying the following conditions:
\begin{enumerate}[noitemsep,leftmargin=*,topsep=0em]
    \item For any $x \in \mathcal{S}$ , every sufficiently small neighborhood of $x$ intersects finitely many cells.
    \item For each cell $x_\alpha$, the boundary $\partial x_\alpha$ is a union of finitely many cells, each having dimension less than that of $x_\alpha$.
    \item Each cell $x_\alpha$ is homeomorphic to $\mathbb{R}^{n_\alpha}$, where $n_\alpha$ is dimension of $x_\sigma$.
    \item (Regularity) For every $\alpha \in P_\mathcal{S}$, there exists a homeomorphism $\phi$ of a closed ball $\mathbb{B}^{n_\alpha}\subset \mathbb{R}^{n_\alpha}$ to the closure $\overline{x_\sigma}$ such that the restriction of $\phi$ to the interior of the ball is a homeomorphism onto $x_\alpha$.
\end{enumerate}\vspace{-2mm}
\end{definition}
% x@ 
From this definition, we can derive that $\mathcal{S}$ is the union of the interiors of all cells, \ie, $\mathcal{S}= \cup _{\alpha \in P_\mathcal{S}}int(x_\alpha)$, where $int(x_\alpha)$ denotes the interior of the cell $x_\alpha$. 
% 
Intuitively, a cell complex can be constructed hierarchically through a gluing procedure. 
It begins with a set of vertices (0-cells), to which edges (1-cells) are attached by gluing the endpoints of closed line segments, thereby forming a graph.
This process can be extended by taking a two-dimensional closed disk and gluing its boundary (\ie, a circle) to a simple cycle in the graph. 
While we typically focus on dimensions up to two, this framework can be further generalized by gluing the boundary of $n$-dimensional balls to specific $(n-1)$-cells in the complex.
In this work, we also utilize simplicial complexes (SCs), a simpler and more restrictive subclass where 2-cells are limited to triangle shapes.
A further introduction to higher-order networks can be found in~\cref{app:ho-intro}.


\begin{figure}[!t]
\vspace{-0.1in}
\centering
\includegraphics[width=0.99\linewidth]{figs/lift-filter.pdf}
\caption{\textbf{Visualization of Cell Complex Transformations.} ({\bf a}) An example graph. ({\bf b}) Cell complex representation with corresponding homeomorphisms to closed balls for three example cells. ({\bf c}) Black elements represent high-order structures extracted from the original graph through 2-cell filtering, while those in grey denote corresponding peripheral structures.}
\label{fig:cell-transform}
\vspace{-4mm}
\end{figure}





\paragraph{Score-based Diffusion Models}
A fundamental goal of generative models is to produce plausible samples from an unknown target data distribution $p(\mathbf{x}_0)$.
%
Score-based diffusion models~\cite{NCSN+NeurIPS2019, Score-SDE+ICLR2021} achieve this by progressively corrupting the authentic data with noise and subsequently training a neural network to reverse this corruption process, thereby generating meaningful data from a tractable prior distribution, \ie,  $\mathbf{x}_{generated}\sim p(\mathbf{x}_0)$.

Specifically, the time-dependent forward process of the diffusion model can be described by the following stochastic differential equation (SDE):
\begin{equation}
\label{eq:forward-SDE}
\mathrm{d}\mathbf{x}_t=\mathbf{f}_t\left(\mathbf{x}_t\right)\mathrm{d}t+g_t\mathrm{d}\mathbf{w}_t,
\end{equation}
where $\mathbf{f}_t: \mathbb{R}^n \to \mathbb{R}^n$ is a vector-valued drift function, $g_t: [0,T]\to \mathbb{R} $ is a scalar diffusion coefficient, and $\mathbf{w}_t$ represents a Wiener process. 
Typically, $p(\mathbf{x}_0)$ evolves over time $t$ from $0$ to a sufficiently large $T$ into $p(\mathbf{x}_T )$ through the SDE, such that $p(\mathbf{x}_T )$ will approximate a tractable prior distribution, for example, a standard Gaussian distribution. 

Starting from time $T$, $p(\mathbf{x}_T)$ can be progressively transformed back to $p(\mathbf{x}_0)$ by following the trajectory of the following reverse SDE \cite{SDEreverse1982}:
\begin{equation}
\label{eq:reverse-SDE}
    \mathrm{d} \mathbf{x}_t=[\mathbf{f}_t(\mathbf{x}_t)-g_t^2  \underbrace{\nabla_{\mathbf{x}_t} \log p_t(\mathbf{x}_t)}_{\text{score function}}]\diff{t} 
    + g_t \mathrm{d} \bar{\mathbf{w}},
\end{equation}
where  $p_t(\cdot)$ denote the probability density function of $\mathbf{x}_t$ and $\bar{\mathbf{w}}$ is a reverse-time Wiener process. 
%
The score function is typically parameterized as a neural network $\bm{s}_{\bm{\theta}}(\mathbf{x}_t,t)$ and trained using the score-matching technique as the loss function \cite{Scorematching2011}:
\begin{equation}
\fontsize{9.5pt}{9.5pt}\selectfont
    \begin{split}
        \ell(&\bm{\theta}) 
\triangleq \mathbb{E}_{t, \mathbf{x}_t}\left[\omega(t)\left\|\bm{s}_{\bm{\theta}}(\mathbf{x}_t,t) - \nabla_{\mathbf{x}_t} \log p_t(\mathbf{x}_t)\right\|^2\right] \\
&\propto    
\mathbb{E}_{t,\mathbf{x}_0,\mathbf{x}_t }\left[ \omega(t) \left \Vert \bm{s}_{\bm{\theta}}(\mathbf{x}_t, t) - \nabla_{\mathbf{x}_t} \log p_t (\mathbf{x}_t|\mathbf{x}_0)\right\Vert^2\right],
    \end{split}
\fontsize{10pt}{10pt}\selectfont
\end{equation}
where $\omega(t)$ is a weighting function, and the second expression is more commonly used as the conditional probability $p_t (\mathbf{x}_t|\mathbf{x}_0)$ is generally accessible.
%
Ultimately, the generation process is complete by first sampling $\mathbf{x}_T$ from a tractable prior distribution $p(\mathbf{x}_T ) \approx p_{prior}(\mathbf{x})$ and then generating $\mathbf{x}_0$ by numerically solving \cref{eq:reverse-SDE}.



%\subsection{Doob's $h$-transform}
\paragraph{Doob's $h$-transform}
% 
Doob’s $h$-transform is a mathematical framework widely used to modify stochastic processes, enabling the process to satisfy specific terminal conditions. By introducing an $h$-function into the drift term of an SDE, this technique ensures that the process transitions to a predefined endpoint while preserving the underlying probabilistic structure.
%
Specifically, given the SDE in \cref{eq:forward-SDE}, Doob’s $h$-transform alters the SDE to include an additional drift term, ensuring that the process reaches a fixed terminal point $t=T$. 
The modified SDE is expressed as:
\begin{equation}
\mathrm{d}\mathbf{x}_t=[\mathbf{f}_t\left(\mathbf{x}_t\right)+g_t^2 \bm{h}(\mathbf{x}_t,t,\mathbf{x}_T,T)]\mathrm{d}t+g_t\mathrm{d}\mathbf{w}_t,
\end{equation}
where $\bm{h}(\mathbf{x}_t,t,\mathbf{x}_T,T)=\nabla_{\mathbf{x}_t} \log p(\mathbf{x}_T| \mathbf{x}_t)$. Crucially, the construction guarantees the conditional density satisfies $p(\mathbf{x}_t|\mathbf{x}_0,\mathbf{x}_T)=1$ at $t=T$, ensuring that SDE terminates at the specified endpoint $\mathbf{x}_T$.







