\section{Related Work}
\textbf{Single-view 3D Recovery.}
Reconstruction of 3D objects from single images has seen notable progress____, delivering intricate models for items like vehicles, furniture, and the human form____. However, the dependence on object-centric 3D learning priors restricts these techniques to full scene reconstruction for robotics applications, such as autonomous navigation and robotic manipulation. Earlier scene reconstruction methods____ segmented scenes into planar segments to approximate 3D architecture. More recently, MDE has been adopted for 3D shape recovery. LeReS____ incorporates a point cloud module to deduce focal length but necessitates extensive 3D point cloud data for training, particularly challenging for outdoor environments. Meanwhile, GP2____ introduces a scale-invariant loss to foster depth maps that conserve geometry, but it fails to ascertain focal length. In contrast, our approach focuses on recovering metric 3D scene structure in indoor and outdoor scenarios through a unified framework.

\textbf{Monocular Metric Depth Estimation.}
CNN-based methods predominantly address MDE as a dense regression task____ or a combined regression-classification task through various binning strategies____. The transition to vision transformers has notably enhanced performance____. 
Beyond architectural innovation, another line of work____ focuses on fine-tuning on the metric depth estimation task by using the relative depth estimation pre-trained model as the cornerstone. These methods continue to improve the benchmark results by leveraging massive training data and powerful pre-trained models. In contrast, we reveal the complementary relationship between depth and camera intrinsics. Our approach, demonstrated through in-domain evaluation using a single dataset, allows for better application to customized datasets and scenes.


\textbf{Single Image Camera Calibration.}
Traditionally, camera calibration relied on reference objects like planar grids____ or 1D objects____. Follow-up studies____, operating under the Manhattan World assumption____, have used image line segments____ that meet at vanishing points to deduce intrinsic properties. Recent learning-based techniques____ loosen these constraints by training on panorama images with known horizon and vanishing points to model intrinsic as 1 DoF camera. A notable trend uses the perspective field____ or incidence field____ to estimate camera intrinsics with 3 DoF or 4 DoF, respectively. In this work, we take a further step and explore the collaborative learning of depth maps and camera intrinsics utilizing the incident field as a bridge.

\textbf{Combination of Depth and Intrinsics.}
Recent studies____ have revisited depth estimation by explicitly incorporating camera intrinsics, particularly focal length, as additional input to learn metric depth. However, focal length is often inaccessible during deployment. The challenge lies in how to jointly learn depth and intrinsics for the accurate recovery of metric 3D shapes. Note that, UniDepth____ addresses this by leveraging considerable and diverse datasets and large-scale backbones. In contrast, in our \textit{in-domain} training and testing settings, we explore the reciprocal relations between depth and camera intrinsics and also achieve impressive performance on \textit{a single dataset}, which offers flexibility to meet various customized requirements.


\begin{figure*}[t]
\begin{center}
\includegraphics[width=1.0\textwidth]{overview.pdf}
\end{center}
\vspace{-5mm}
\caption{Overview of the proposed CoL3D framework. It consists of an Encoder and Decoder for latent feature extraction, a Depth Head for depth prediction, and a Camera Head for camera intrinsics estimation. Collaborative learning is performed on the depth map, the incident field, and the 3D point cloud. Note that camera intrinsics are only used for training and are predicted by the model itself at inference.}
\label{fig:overview}
\vspace{-3mm}
\end{figure*}