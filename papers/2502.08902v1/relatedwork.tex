\section{Related Work}
\textbf{Single-view 3D Recovery.}
Reconstruction of 3D objects from single images has seen notable progress~\cite{barron2014shape,wang2018pixel2mesh,wu2018learning,popov2020corenet}, delivering intricate models for items like vehicles, furniture, and the human form~\cite{saito2019pifu,saito2020pifuhd}. However, the dependence on object-centric 3D learning priors restricts these techniques to full scene reconstruction for robotics applications, such as autonomous navigation and robotic manipulation. Earlier scene reconstruction methods~\cite{saxena2008make3d} segmented scenes into planar segments to approximate 3D architecture. More recently, MDE has been adopted for 3D shape recovery. LeReS~\cite{yin2021learning} incorporates a point cloud module to deduce focal length but necessitates extensive 3D point cloud data for training, particularly challenging for outdoor environments. Meanwhile, GP2~\cite{patakin2022single} introduces a scale-invariant loss to foster depth maps that conserve geometry, but it fails to ascertain focal length. In contrast, our approach focuses on recovering metric 3D scene structure in indoor and outdoor scenarios through a unified framework.

\textbf{Monocular Metric Depth Estimation.}
CNN-based methods predominantly address MDE as a dense regression task~\cite{eigen2014depth,yuan2022neural,liu2023va,piccinelli2023idisc} or a combined regression-classification task through various binning strategies~\cite{adabins,binsformer,localbins,iebins}. The transition to vision transformers has notably enhanced performance~\cite{yang2021transformer,ranftl2021vision,li2023depthformer}. 
Beyond architectural innovation, another line of work~\cite{bhat2023zoedepth,guizilini2023towards,depthanything} focuses on fine-tuning on the metric depth estimation task by using the relative depth estimation pre-trained model as the cornerstone. These methods continue to improve the benchmark results by leveraging massive training data and powerful pre-trained models. In contrast, we reveal the complementary relationship between depth and camera intrinsics. Our approach, demonstrated through in-domain evaluation using a single dataset, allows for better application to customized datasets and scenes.


\textbf{Single Image Camera Calibration.}
Traditionally, camera calibration relied on reference objects like planar grids~\cite{zhang2000flexible} or 1D objects~\cite{zhang2004camera}. Follow-up studies~\cite{schindler2004atlanta,xu2013minimum,wildenauer2012robust,deutscher2002automatic}, operating under the Manhattan World assumption~\cite{coughlan1999manhattan}, have used image line segments~\cite{von2008lsd,akinlar2011edlines} that meet at vanishing points to deduce intrinsic properties. Recent learning-based techniques~\cite{hold2018perceptual,lee2021ctrl,lee2020neural} loosen these constraints by training on panorama images with known horizon and vanishing points to model intrinsic as 1 DoF camera. A notable trend uses the perspective field~\cite{jin2023perspective} or incidence field~\cite{zhu2024tame} to estimate camera intrinsics with 3 DoF or 4 DoF, respectively. In this work, we take a further step and explore the collaborative learning of depth maps and camera intrinsics utilizing the incident field as a bridge.

\textbf{Combination of Depth and Intrinsics.}
Recent studies~\cite{Facil_2019_CVPR,Yin_2023_ICCV,Guizilini_2023_ICCV} have revisited depth estimation by explicitly incorporating camera intrinsics, particularly focal length, as additional input to learn metric depth. However, focal length is often inaccessible during deployment. The challenge lies in how to jointly learn depth and intrinsics for the accurate recovery of metric 3D shapes. Note that, UniDepth~\cite{piccinelli2024unidepth} addresses this by leveraging considerable and diverse datasets and large-scale backbones. In contrast, in our \textit{in-domain} training and testing settings, we explore the reciprocal relations between depth and camera intrinsics and also achieve impressive performance on \textit{a single dataset}, which offers flexibility to meet various customized requirements.


\begin{figure*}[t]
\begin{center}
\includegraphics[width=1.0\textwidth]{overview.pdf}
\end{center}
\vspace{-5mm}
\caption{Overview of the proposed CoL3D framework. It consists of an Encoder and Decoder for latent feature extraction, a Depth Head for depth prediction, and a Camera Head for camera intrinsics estimation. Collaborative learning is performed on the depth map, the incident field, and the 3D point cloud. Note that camera intrinsics are only used for training and are predicted by the model itself at inference.}
\label{fig:overview}
\vspace{-3mm}
\end{figure*}