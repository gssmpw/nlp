\section{Related work}
\label{sec:relatedwork}

\subsection{Image Fusion and Downstream Tasks}

With the increased interest in learning-based image fusion, mainstream approaches aim to improve fusion performance by introducing high-level semantics to multi-modal image fusion tasks~\cite{tang2022SeAFusion,zhao2023metafusion,Zhao_2024_ICML}.
This paradigm can enhance the performance of downstream multimodal tasks using the improved fusion results~\cite{liu2022target}.
Additionally, these methods achieve promising objectively measured performance in various image fusion assessments.

However, the reliance on labels from downstream detection or segmentation tasks makes the performance gains costly and compromises their relevance for new fusion tasks.
Besides, the computational burden incurred by involving a relatively large high-level vision model in a low-level image processing technique seems an inappropriate use of resources.
The semantic gap between the features required for image fusion and high-level visual tasks also impairs the quality of fused images~\cite{zhao2023metafusion}.
In FusionBooster~\cite{cheng2024fusionbooster}, Cheng~\textit{et.al.} identify this discrepancy and propose using an enhancer designed specifically to avoid the injection of incompatible semantic information.
Despite the significant performance improvements, this boosting paradigm requires extra training for each fusion task.
Additionally, the commonalities among different fusion tasks are ignored, and the potential of specific features derived from different missions is not fully exploited.

Inspired by this analysis, we propose GIFNet.
It only combines the low-level vision tasks to establish the cross-task interaction and diverse image fusion tasks are used to extract foundational and targeted features.
Thanks to the carefully designed task combination, scenario-specific cooperation mechanisms aimed to reduce the semantic gaps are not required in GIFNet, enabling a more effective multi-task learning paradigm.
%Note that, existing multi-task fusion paradigm is only designed for the particular task from the multi-modal image fusion, \textit{i.e.}, IVIF task.
%While the proposed low-level task interaction further support the involvement of the digital photography image fusion task, which is a more general interaction approach.

\subsection{Generalised Image Fusion Methods}
Some existing studies also aim to develop a generalised image fusion method that performs well across various fusion tasks with different input modalities or image types.
In U2Fusion~\cite{xu2022u2fusion}, Xu~\textit{et.al.} proposed a unified image fusion method based on continual learning, capable of handling multiple fusion tasks.
This work addresses conflicts among different fusion subtasks but fails to promote task interaction during training.
Subsequently, more algorithms have been developed simultaneously to improve image fusion performance and generalisation ability, including CNN-based methods~\cite{zhang2021sdnet,cheng2023mufusion}, Transformer-based solutions~\cite{ma2022swinfusion,zhu2024cvprTaskCustomized}, Mamba-based algorithms~\cite{xie2024fusionmamba}, and some frequency-based approaches~\cite{zhou2024frequency,huang2024efficientFreq}.
However, these paradigms rely heavily on extensive training data spanning diverse fusion tasks and cannot achieve true generalisation without further training on task-specific datasets.
Typically, they depend on multiple image fusion models or specific fusion rule designs~\cite{cheng2021unifusion} to manage different fusion tasks effectively.

In our work, we address this limitation by designing a cross-fusion gating mechanism, involving only the interaction of two representative image fusion tasks from the multi-modal image fusion and the digital photography image fusion.
The learned hybrid image features and enhanced low-level representations enable us to use a single model for achieving task-independent and generalised image fusion.