\section{The Proposed GIFNet}
\label{sec:app}

\subsection{Formulation}
The image fusion paradigm can generally be defined as:
\begin{equation}
    I_{\textrm{f}} = F(I_1,I_2),
\end{equation}
where $I_1$ and $I_2$ are input images, $F$ denotes an image fusion approach, and $I_\textrm{f}$ is the fused image.
Recent methods often incorporate semantic information from high-level vision tasks for the multi-modal image fusion (IVIF) model~\cite{liu2022target,cheng2024textfusionunveilingpowertextual,Zhao_2024_ICML}, aiming to improve performance.
However, this paradigm raises risks of degraded image quality, increased computational cost, and limited generalisation (Fig.~\ref{figure_comparison_gen_effi} and Fig.~\ref{figure_abstract_comp}).

We propose a novel approach by introducing two innovative ideas.
The first is a cross-task interaction mechanism that leverages low-level processing operations across various fusion tasks.
Specifically, we use digital photography image fusion tasks to provide additional task-specific features and supervision signals for the unsupervised IVIF task, thereby improving the generalisation and robustness of the fusion model.
We select Multi-Focus Image Fusion (MFIF) as a representative example of digital photography fusion to demonstrate our GIFNet model, as it performed best among available fusion tasks in our interaction ablation experiments (Sec.~\ref{SecAblation}).

The second innovative feature of our method is the incorporation of single-modality image enhancement capability. 
Introducing digital photography fusion tasks (one image with different settings), the model learns to enhance features without relying on multi-modal input.
By setting both inputs to the same image, we simulate a fusion-like enhancement, focusing on refining details within the single image.
This inference process is formulated as:
\begin{equation}
\hat{X} = F(X,X),
\end{equation}
where $X$ denotes the single modality input, $\hat{X}$ is the enhanced output.
Applications of existing image fusion methods are only restricted to the multi-modal scenarios.
While this new feature enables us to take advantage of the enhanced results for boosting mainstream RGB vision tasks.
%, \textit{e.g.}, object classification.
%This research reveals another potential study topic in the image fusion field.
%Beyond existing image fusion,  our research reveals another potential study topic in this field.

\begin{figure*}[t]
\centering
\includegraphics[clip,width=1\linewidth]{./images/NetworkArchitecture.pdf}
\vspace{-5mm}
\caption{The network architecture and training process of GIFNet.
As shown in diagram (d), the Multi-Modal (MM) and Digital Photography (DP) branches of our model are trained alternately, based on the specifically designed cross-fusion gating mechanism (c).
}
\label{figure_networkarchitecture}
\vspace{-3mm}
\end{figure*}
\subsection{Measures to Mitigate the Domain Gap and the Task Differences}
\label{secDomainGap}
Our multi-task learning framework requires the model to extract and learn distinct features from input images for each task.
Without taking explicit measures, this diversity can misalign the model’s learning objectives, making it challenging to develop a unified representation that performs effectively across all tasks.

To address this issue, we employ a data augmentation technique to generate an RGB-focused joint dataset from an IVIF benchmark~\cite{jia2021llvip}.
This augmented dataset consists of aligned RGB, infrared, far-focused and near-focused images.
The multi-focus data is obtained by partially blurring clear RGB images (\textit{details are provided in the supplement}).
Since the data is derived from the same scene within a single dataset, the domain gap is effectively reduced.
In addition, we introduce a reconstruction (REC) task in the cross-task interaction.
The REC task facilitates feature alignment across different tasks by focusing on features that are beneficial universally.
This approach ensures that features learned for one task remain relevant and compatible with other tasks, promoting a more coherent and effective interaction among tasks.

\subsection{Model Architecture}
Contemporary image fusion methods often struggle with collaborative learning due to their monolithic network designs, where multiple tasks depend on a singular encoder-decoder structure~\cite{zhang2021sdnet,deng2024mmdrfuse,cheng2023mufusion}.
To address this, our framework introduces a three-branch architecture (as illustrated in Fig.~\ref{figure_networkarchitecture} (a)), which decouples the feature extraction process and facilitates interaction between low-level tasks.
In our model, only the foundational feature extraction part is shared across different tasks.

By focusing on the interaction among low-level tasks, our approach allows task-specific features to be combined directly within the network, removing the need for additional modules to bridge feature or semantic gaps.
This interaction occurs between the Multi-Modal (MM) and Digital Photography (DP) branches, where a cross-task mechanism alternates the roles of main and auxiliary branches (Fig.~\ref{figure_networkarchitecture} (d)).
A gating module then selectively routes the main branch’s hybrid features to the global decoder (G-Dec) for delivering fusion results.
The reconstruction (REC) branch supports this process by extracting task-agnostic features.

Reconstruction Branch:
As shown in Fig.~\ref{figure_networkarchitecture} (b) (II), the REC branch employs an autoencoder to derive universal features from various image fusion tasks. 
By targeting the common RGB modality within our augmented data for the reconstruction, we ensure the effective extraction of task-shared features.
Dense connections in the shared encoder (S-Enc) maximize the feature utilisation, enabling the transmission of the original visual signals to the other branches.

Cross-Fusion Gating Mechanism:
After obtaining these shared features, the MM and DP branches proceed to extract task-specific features of different fusion types(Fig.~\ref{figure_networkarchitecture} (b) (I)).
The proposed Cross-Fusion Gating Mechanism (CFGM) serves as the core technique for controlling these branches, enabling them to fuse task-specific features and stabilise cross-task interaction adaptively. 
In view of its well-known robust global feature extraction ability and its success in capturing task-aware features~\cite{ma2022swinfusion,li2024crossfuse}, we use the efficient SwinTransformer block~\cite{Liu_2021SwinTransformer} to formulate the CFGM.

Within the CFGM, main and auxiliary branches are alternately trained by updating one while freezing the other (Fig.~\ref{figure_networkarchitecture} (c)).
In each training step, we have:
\begin{align}
    \hat{x_m}&  = \textrm{Self\text{-}Att}(x_m), \\
    x_m&  = \hat{x_m} + \lambda\cdot \textrm{Cross\text{-}Att}(\hat{x_m},x_{a}),
\end{align}
where $x_m$ and $x_a$ represent the main and auxiliary task representations, respectively, and $\lambda$ is a learnable parameter that controls the degree of auxiliary task influence. 
$\textrm{Self\text{-}Att}$ and $\textrm{Cross\text{-}Att}$ denote the self attention and cross attention operations.
The interaction is confined to the odd layers to avoid interfering with the SwinTransformer's window shift operation~\cite{Liu_2021SwinTransformer}.

%The fused image features are then reconstructed by a fusion block that uses the same decoder design (R-Dec) as the REC branch (G-Dec).
%Finally, after obtaining task-shared and task-specific features from the interaction phase, a fusion block is designed to combine these robust image features. Since we simultaneously handle feature maps from the S-Enc, we use the same decoder design as the REC branch to reconstruct the fused features.

\begin{figure}[t]
\centering
\includegraphics[clip,width=1\linewidth]{./images/test-phase.pdf}
\caption{An illustration of the inference phase of our GIFNet.
In this stage, only one pair of images will be used to produce the multi-modal and digital photography features.
%The global decoder reconstructs the task-shared and task-specific features to deliver the fusion result.
}
\vspace{-2mm}
\label{figure_inference}
\end{figure}

\subsection{Training and Inference}
In the training process, we adopt two loss functions, \textit{i.e.}, the public loss $\mathcal{L}_{\textrm{pub}}$ and the private loss $\mathcal{L}_{\textrm{pri}}$, defined by the outputs of the REC branch $I_\textrm{r}$ and the fused image $I_{\textrm{f}}$.
The total loss for each task branch is:
\begin{equation}
\label{eqTotalLoss}
    \mathcal{L}_{\textrm{total}} = \mathcal{L}_{\textrm{pub}}+\mathcal{L}_{\textrm{pri}}.
\end{equation}
The public loss $\mathcal{L}_{\textrm{pub}}$ guides the foundational feature extraction by enforcing consistency between the REC branch output and the shared RGB modality ($I_{\textrm{vis}}$):
\begin{equation}
     \mathcal{L}_{\textrm{pub}} = \mathcal{L}_{\textrm{ssim}}(I_r,I_{\textrm{vis}})+\mathcal{L}_{\textrm{mse}}(I_r,I_{\textrm{vis}}),
\end{equation}
where $\mathcal{L}_{\textrm{ssim}}$ and $\mathcal{L}_{\textrm{mse}}$ denote the structural similarity loss and mean squared error loss, respectively.
The structural similarity loss is defined as:
\begin{equation}
    \mathcal{L}_{\textrm{ssim}} (I_{X},I_{Y}) = 1-SSIM(I_X,I_Y).
\end{equation}
Here, $SSIM$ denotes the structural similarity metric~\cite{zhou2004SSIM} between two images.

The private loss is uniquely defined for each task.
During the iterative task-interaction process, only the main task branch and its private loss will be optimised while the other branch is frozen.
Note that, the input for the REC branch is always the main task images.
For the MM branch (IVIT task), which requires the fused image to retain the informative content from the input images~\cite{zhang2023IVIF_TPAMI_review,cheng2023mufusion}, we employ an information-weighted loss function.
Based on the visual explanation studies of convolutional networks~\cite{selvaraju2017grad_cam}, the gradients of a feature map indicate how a specific area contributes to the final network decision-making. 
Using a lightweight DenseNet classification network~\cite{huang2017densely}, we determine the mixing proportions $w_{\textrm{vis}}$ and $w_{\textrm{ir}}$:
\begin{equation}
    GradF(X) = \sum \nabla \phi(\textrm{X}),
\end{equation}
\begin{equation}
    [w_{\textrm{ir}},w_{\textrm{vis}}] = \textrm{softmax}(GradF(I_{\textrm{ir}}),GradF(I_{\textrm{vis}})),
\end{equation}
where $\phi(\textrm{X})$ denotes the extracted image features of modality $X$ via the pre-trained DenseNet121 network.
The private loss for the MM branch is then defined as:
\begin{equation}
    \mathcal{L}_{\textrm{pri}}^{\textrm{MM}} = w_{\textrm{ir}}\cdot \mathcal{L}_{\textrm{mse}}(I_{\textrm{f}},I_{\textrm{ir}})+w_{\textrm{vis}}\cdot \mathcal{L}_{\textrm{mse}}(I_{\textrm{f}},I_{\textrm{vis}}).
\end{equation}

For the DP branch (MFIF task), given that the augmented MFIF data is derived from RGB images ($I_{\textrm{vis}}$), we have ground truth for supervised training (Sec.\ref{secDomainGap}).
Hence, the private loss for this branch is formulated as:
\begin{equation}
    \mathcal{L}_{\textrm{pri}}^{\textrm{DP}} = \mathcal{L}_{\textrm{mse}}(I_{\textrm{f}},I_{\textrm{vis}}).
\end{equation}

%As shown in Fig.~\ref{figure_inference}, the inference process involves extracting shared image features, applying the Cross-Fusion Gating Mechanism (CFGM) to fuse task-specific features, and finally reconstructing the fused image using the decoder.
%This choice is justified by experimental results demonstrating its effectiveness, which are discussed in the experiments section.

As shown in Fig.~\ref{figure_inference}, during inference, different from the training process, only one image pair is required for a single fusion task.
We then extract shared image features, use CFGM to fuse the two sets of specific representations, and finally, the global decoder reconstructs the fused image.