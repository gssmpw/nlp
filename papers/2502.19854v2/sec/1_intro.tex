\section{Introduction}
\label{sec:intro}

Image fusion combines critical information from multiple sources to produce an output that is more informative and contextually rich, enhancing both human visual interpretation and the performance of downstream computer vision tasks~\cite{liu2018pixelLevelImageFusion, tang2024generative}.
This technique has been shown to be valuable in remote sensing~\cite{javan2021pansharpReview}, medical imaging~\cite{azam2022Medical_IF7_review}, and related fields~\cite{tang2024generative, xu2020accelerated}.
Typically, image fusion is divided into multi-modal fusion and digital photography fusion, based on the properties of the source images~\cite{karim2023current, li2025contifuse}.
Multi-modal fusion combines complementary information from different sensors, such as Infrared and Visible Image Fusion (IVIF), where the infrared channel highlights targets against backgrounds and visible images convey texture details.
Due to the lack of Ground Truth in such tasks, unsupervised approaches are commonly employed~\cite{zhang2023IVIF_TPAMI_review, tang2023exploring}.
In contrast, digital photography fusion, such as Multi-Focus Image Fusion (MFIF) and Multi-Exposure Image Fusion (MEIF), addresses degradations caused by the limitations of the depth of field and inappropriate exposure within a single image~\cite{zhang2021MFIF_TPAMI_review}.
For these tasks, Ground Truth data can be generated artificially, for instance, by synthetically  blurring regions or adjusting exposure, making supervised learning feasible.

\begin{figure}
\centering
%\includegraphics[height=6.5cm]{./images/Comparison.pdf}
\includegraphics[clip,width=\linewidth]{./images/Comparison.pdf}
\vspace{-4mm}
\caption{
A comparison of the versatility and efficiency of advanced multi-task fusion methods.
The indices in the arrows are the fusion tasks validated by the corresponding methods.
}
\label{figure_comparison_gen_effi}
\end{figure}

Currently, a task-interaction mechanism is widely used in advanced image fusion methods.
A typical approach adopted by advanced multi-modal image fusion research is to draw on high-level visual tasks for supervisory signals to direct the fusion.
As a downstream task, these high-level models are usually equipped with a fusion model and receive the fused image as input for iteratively optimising the fusion model and high-level models.
High-level tasks, such as object detection or semantic segmentation~\cite{liu2022target,tang2022SeAFusion,zhang2022watch}, introduce abstract semantic information that can be used to guide task-specific feature learning and improve the fusion performance.
Meanwhile, the improved scene representation derived from the fused image also helps to realise the high-level task better by virtue of the mutual reinforcement mechanism established in this way.

However, such high-level supervision %is an indirect approach to solving not necessarily well-suited to the practical demands of image fusion. 
is somewhat divorced from the underlying image fusion problem.
As shown in Fig.~\ref{figure_comparison_gen_effi}, when addressing a different image fusion task, this indirect formulation requires training a new fusion model that is tailored to specific features of each task. 
This additional requirement can hinder the deployment of image fusion algorithms on small footprint devices, \textit{e.g.}, mobile phone, which have limited computational resources, as every application requires a different model to solve it.
%Moreover, establishing the interaction between low-level image fusion task and high-level vision missions also creates a 

\begin{figure}[t]
\centering
%\includegraphics[height=6.5cm]{./images/first_page_textfusion_attention.pdf}
\includegraphics[clip,width=1\linewidth]{./images/high_level_task.pdf}
\vspace{-4mm}
\caption{
Comparison of advanced multi-task fusion methods relying on high-level tasks and the proposed low-level task interaction paradigm.
%Owing to the high-level semantic guidance, these methods can obtain enhanced saliency regarding the objective regions.
These semantic-focused paradigms cannot consistently ensure the robust fusion quality as our paradigm does, which provides the pixel-level supervision and presents clear texture details.
%gap between high-level vision tasks and the Infrared and Visible Image Fusion (IVIF) task, 
%cannot directly establish an effective cross-task interaction as our approach does, which benefits from the diversity of task-specific features and enhanced low level task-shared features %of image fusion 
}
\label{figure_abstract_comp}
\vspace{-3mm}
\end{figure}

Moreover, the semantic gap between a high level downstream task and the low level image fusion renders the high level supervision signal less than optimal for guiding pixel-focused fusion learning, as it tends to encode features related to object categories, shapes, and scene layouts, rather than fine-grained image details.
This mismatch results in unwanted reliance on complex bridging modules~\cite{xu2023murf} or computationally intensive pre-trained models~\cite{cheng2024textfusionunveilingpowertextual} for different fusion contexts (Fig.~\ref{figure_comparison_gen_effi} (a) and (b)), both of which are resource-heavy and fail to generalise effectively across various fusion contexts.
Also, as shown in Fig.~\ref{figure_abstract_comp}, paradigms (a)~\cite{liu2022target} and (b)~\cite{yi2024textIF} achieve increased saliency for detected objects or text-prompted regions, yet the fused images are often unable to consistently maintain high visual quality, which is significant for other fusion tasks.
We argue that, this phenomenon can be attributed to the absence of pixel-level supervision.

In this work, we aim to address these limitations by promoting cross-task interaction without relying on high-level semantics.
Instead, we use low-level digital photography fusion tasks as a more natural alternative for providing supervisory signals.
Digital photography fusion shares its characteristics with multi-modal fusion, emphasising the preservation of details and focusing on pixel-level feature alignment, rendering it better equipped for enhancing task-shared image features without the semantic mismatch inherent to interacting with high-level tasks.
%Without any contribution from high-level semantics, digital photography fusion enables the network to learn diverse features that generalise well across fusion tasks.

To this end, we introduce the Generalised Image Fusion Network (GIFNet), a three-branch architecture that supports low-level task interaction for effective fusion.
GIFNet comprises a main task branch, an auxiliary task branch, and a reconciling branch. 
The main and auxiliary task branches, which alternately focus on the multi-modal and digital photography features, promote effective cross-task interaction.
While the reconciling branch, centred on a shared reconstruction task, encourages the network to learn a universal feature representation~\cite{Zhao_2024_CVPR}.
This branch harmonises the optimisation directions of the multi-modal and digital photography branches, preventing divergent task-specific adaptations.
Additionally, our model incorporates a cross-fusion gating mechanism that iteratively refines each task-specific branch, integrating multi-modal and digital photography features to deliver the fusion result.
%produce a more robust representation.
To minimise the data domain gap between multi-modal and digital photography tasks, we create an RGB-based joint dataset based on the augmentation technique.
With the shared RGB modality derived from identical scenes, the proposed model can focus on consistent feature extraction across the adopted tasks in a unified context, thereby harmonising the training process.

%In addition to enhancing image fusion performance, GIFNet also improves  efficiency and versatility.
Finally, as shown in Fig.~\ref{figure_comparison_gen_effi} (c), the limited computational cost of low-level fusion tasks in GIFNet reduces GFLOPs by more than 96\% compared to the advanced image fusion method.
Unlike current approaches that focus heavily on high-level vision tasks, prioritising a single category fusion, GIFNetâ€™s integration of both multi-modal and digital photography tasks broadens its applicability across various fusion scenarios with a single model (task-agnostic image fusion).
Besides, rather than amplifying task-specific features, our low-level task interaction enhances task-shared foundational features that are crucial for general image processing, allowing GIFNet to function as a versatile enhancer even for single-modality inputs.
The main contributions of the proposed method include the following:
\begin{itemize}
\item We uniquely demonstrate that collaborative training between low-level fusion tasks, a strategy whose importance was previously not recognised, yields significant performance gains by harnessing cross-task synergies.
\item The reconstruction task and an augmented RGB-focused joint dataset are introduced to align features of different fusion tasks and to address the data support.
%, \textcolor{blue}{minimising the inherent data domain gap} and thus enhancing the model robustness.
\item Our method significantly enhances the versatility of the fusion system, eliminating the need for time-consuming task-specific adaptation.
\item GIFNet pioneers the integration of image fusion and single-modality enhancement processes, extending the scope of image fusion models beyond the multi-modal domain.
\end{itemize}
