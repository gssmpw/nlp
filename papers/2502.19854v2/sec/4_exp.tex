\section{Experimental Results}
\label{sec:exp}

\subsection{Experimental Settings}
\textbf{Training:}
During the training process, only the IVIF dataset (training set of the \textit{LLVIP}~\cite{jia2021llvip}) and the corresponding augmented data for the DP task are used.

\noindent\textbf{Evaluation:}
After training, we directly apply the model to various seen and unseen image fusion tasks,~\textbf{\textit{without any adaption or fine-tuning}}.
The tasks and datasets used include: the \textit{LLVIP} and \textit{TNO}~\cite{2014TNO} datasets for the IVIF task, the \textit{Lytro}~\cite{nejati2015mflytrodataset} and \textit{MFI-WHU}~\cite{zhang2021mffgan} datasets for the MFIF task, the \textit{Harvard} dataset~\cite{cheng2023mufusion} for the medical image fusion task, the \textit{VIS-NIR Scene}~\cite{xu2023murf} dataset for the near-infrared and visible image fusion task, the SCIE dataset~\citep{Cai2018medataset} for the multi-exposure image fusion task, and the \textit{Quickbird} dataset~\cite{zhang2020rethinking} for the remote sensing image fusion task.
%\begin{itemize}
%    \item MFIF task: \textit{Lytro}~\cite{nejati2015mflytrodataset} and \textit{MFI-WHU}~\cite{zhang2021mffgan} datasets
%    \item IVIF task: \textit{LLVIP} and \textit{TNO}~\cite{2014TNO} datasets
%    \item Medical image fusion task: \textit{Harvard} dataset~\cite{cheng2023mufusion}
%    \item Near-infrared and visible image fusion task: \textit{VIS-NIR Scene}~\cite{xu2023murf} dataset
%    \item Multi-exposure image fusion task: SCIE dataset~\citep{Cai2018medataset}
%    \item Remote sensing image fusion task: \textit{Quickbird} dataset~\cite{zhang2020rethinking}
%\end{itemize}
We also validate the effectiveness of GIFNet on the classification task using the CIFAR100 dataset~\cite{krizhevsky2009cifar100}.

%using the Adam optimizer~\cite{kingma2014adamOptimizer}
%The batch size is set to 8 and we update the GIFNet model with a learning rate of $10^{-3}$.
%The experiments are conducted on an NVIDIA GeForce RTX 3090 GPU.
The evaluation metrics for image fusion include two commonly used correlation-based metrics: Visual Information Fidelity (VIF) and Sum of Correlation Difference (SCD)~\cite{ma2020ddcgan}. Additionally, we include non-reference image quality assessments~\cite{cheng2021unifusion}: Edge Intensity (EI) and Average Gradient (AG) to measure the clarity of the fusion results. For the classification task, we use top-1 and top-5 accuracy.





%



\begin{figure}[t]
  \centering
\includegraphics[clip,width=0.90\linewidth]{./images/qualitative_CFGM_ablation.pdf}
\vspace{-1mm}
\caption{Quantitative results of the CFGM ablation experiments.
}  
\vspace{-3mm}
  \label{visualisation_cfgm_ablation}
\end{figure}


\begin{figure}[t]
  \centering
\includegraphics[clip,width=1\linewidth]{./images/Ablation41.pdf}
\vspace{-5mm}
\caption{Visualisation of the CFGM ablation experiments.
}  
\vspace{-2mm}
  \label{Fig_ablation}
\end{figure}

% Table generated by Excel2LaTeX from sheet 'mark_fail_moreDetails_wME'
\begin{table}[tbp]
  \centering
  \resizebox{0.95\linewidth}{!}{
    \begin{tabular}{cccccccc}
\cline{1-8}    Case  & MTL   & CFGM  & REC   & EI    & VIF   & SCD   & AG \\
    \hline
    (a)   & IV only & $\times$ & $\times$ & 24.07  & 0.35  & 1.13  & 2.27  \\
    (b)   & MF only & $\times$ & $\times$ & 54.47  & 0.55  & 1.49  & 5.80  \\    
    (c)   & ME only & $\times$ & $\times$ & 24.77  & 0.45  & 0.34  & 2.38  \\
\hdashline    
    (d)   & IV only & $\times$ & \checkmark & 28.71  & 0.53  & 1.45  & 2.71  \\
    (e)   & MF only & $\times$ & \checkmark & 56.14  & 0.59  & 1.50  & 6.00  \\
    (f)   & ME only & $\times$ & \checkmark & 47.17  & 0.61  & 0.69  & 4.97  \\
\hdashline    
    (g)   & IV+MF & $\times$ & $\times$ & -     & -     & -     & - \\
    (h)   &  IV+MF & \checkmark & $\times$ & 49.84  & 0.52  & 1.49  & 5.31  \\
    (i)   & IV+MF  & $\times$ & \checkmark & 52.56  & 0.57  & 1.48  & 5.65  \\
\hdashline       
    (j)   & IV+ ME & \checkmark & \checkmark & 47.69  & 0.56  & 0.42  & 5.08 \\
\cline{3-4}    \rowcolor[rgb]{ .749,  .749,  .749} Ours  & IV+MF & \checkmark & \checkmark & \textbf{62.46 } & \textbf{0.73 } & \textbf{1.61 } & \textbf{6.70 } \\
    \hline
    \end{tabular}%
    }
    \vspace{-1mm}
  \caption{The results of the ablation experiments involving different components and task combinations of the proposed GIFNet (MF and ME refer to the multi-focus and mult-exposure image fusion).}
  \label{table_quantative_ablation}
\vspace{-2mm}
\end{table}%




\subsection{Ablation Experiments}
\label{SecAblation}
In this section, we conduct ablation studies on the IVIF task to demonstrate the efficacy of our GIFNet.
We mainly examined the impact of Multi-Task Learning (MTL) strategy, Cross-Fusion Gating Mechanism (CFGM), and the Reconstruction branch (REC). \textit{More ablation experiments will be provided in the supplement.}

\textbf{Main Components:} As shown in Table~\ref{table_quantative_ablation}, combining single-task training strategy with REC (case (e)), the proposed model already yields impressive results.
However, adding another task without the proposed components would prevent the fusion network from converging, \textit{i.e.}, the fusion network simply produces non-functional outputs (case (g)).
Introducing CFGM or REC independently allows the network to produce valid fusion results (cases (h) and (i)).
The combination of both components optimises cross-task interaction and enhance the feature alignment, leading to the best performance of our GIFNet.

\textbf{Task Combination:} The extra supervision signals from the digital photography task help to enhance the fusion performance.
We further validate this conclusion by employing the supervised MEIF task.
Regarding training data, we use the Information Probe module from the FusionBooster~\cite{cheng2024fusionbooster} to decompose the visible images from the LLVIP dataset, to obtain the overexposed and underexposed images (\textit{examples are provided in the supplementary materials}).
The original visible image is regarded as the GT image.
As depicted in case (j), compared with the single task paradigm (setting(d)), the additional supervised task can consistently improve the performance of multi-modal fusion.
However, the MEIF task, as an auxiliary task, cannot achieve better performance than that of using the MFIF task.
The reason behind this phenomenon may due to the fact that, producing images with higher clarity provides more compatible pixel-level supervision, since there is no conflict between enforcing the fused image to perceive clear content with higher-clarity and preserve as much information as possible.
In contrast, MEIF task only involves the adjusting of the overall exposure degree, which is not always align with the objective of IVIF task.
%is more align with the objective of multi-modal image fusion.
%While the exposure setting

\textbf{CFGM Module:} Finally, as shown in Fig.\ref{visualisation_cfgm_ablation} and Fig.\ref{Fig_ablation}, replacing the adaptive CFGM strategy (featuring a learnable parameter $\lambda$ for controlling the mixing ratio) with conventional fusion operations demonstrates, both quantitatively and qualitatively, that our adaptive approach provides superior control over the interaction process, yielding more robust fused images.

%\begin{figure}[t]
%\centering
%\includegraphics[clip,width=1\linewidth]{images/Visualisation.pdf}
%\vspace{-6mm}
%\caption{Visualisation of the feature maps from the shared-encoder and the two task branches (\textit{visualisation of other tasks is provided in the supplementary materials}). 
%}
%\label{figure_visualisation_featureMaps}
%\vspace{-1mm}
%\end{figure}

\begin{figure}[t]
\centering
\includegraphics[clip,width=0.9\linewidth]{images/Visualisation_all.pdf}
\vspace{-3mm}
\caption{Visualisations of the feature maps from the shared-encoder and the two branches on various image fusion tasks. 
}
\label{figure_visualisation_featureMaps}
\vspace{-2mm}
\end{figure}

\subsection{Feature Visualisation}
We present visualisations of the feature maps from different components: the shared encoder (S-Enc), the MM branch, and the DP branch, as shown in Fig.~\ref{figure_visualisation_featureMaps}. 
The S-Enc, driven by the image reconstruction objective, captures foundational image features, such as target contours and structural details, which are essential for high-quality image fusion. 

The MM and DP branch visualisations reveal the distinct contributions of each branch to the fusion process.
For instance, in the first case, MM features focus on preserving salient information from the source inputs, such as thermal targets.
Meanwhile, DP features enhance finer details, capturing sharper edges and more defined textures, as well as clearer shadows on the ground.
Similar patterns are observed across other seen and unseen fusion tasks.
Notably, the additional learning of digital photography features consistently benefits various fusion tasks by producing the necessary features for visually robust outputs, as seen in the third example (MEIF task) where enhanced texture details are prominent.

%As shown in the highlighted regions of the output, the interaction of these task-aware features, along with task-independent representations, enables our GIFNet to deliver a visually superior fusion result.

%\begin{figure}[t]
%  \centering
%\includegraphics[clip,width=1\linewidth]{./images/qualitative_main_aux.pdf}
%\vspace{-2mm}
%\caption{Impact of the main task selection in the test phase.
%}  
%  \label{Fig_ablation_main_aux}
%\vspace{-3mm}
%\end{figure}


%\subsection{Main Task Selection for the Inference Phase}
%In our method, we alternatively select the main task and auxiliary task during training, but this configuration must be fixed during the inference phase. To validate the rationale of considering MFIF as the main task, we conduct experiments on two unseen fusion tasks: MEIF and NIR-VIS.
%As shown in Fig.~\ref{Fig_ablation_main_aux}a, when using IVIF as the main task, GIFNet struggles to control exposure settings in poorly exposed conditions.
%Although the task-specific features in this branch facilitate the preservation of significant information, it fails to maintain a natural visual effect (as in the NIR-VIS task).
%In contrast, when MFIF is designated as the main task (Fig.~\ref{Fig_ablation_main_aux}b), GIFNet produces visually robust fused images with appropriate exposure settings and effective utilisation of both modalities, exhibiting superior generalisation ability. 
%%These observation is the reason 
%This focus on generating high-quality fused images demonstrates the superiority of using MFIF as the main task.
%In contrast, focusing on producing fused images with high-quality, GIFNet using MFIF as the main task can produce visually more robust fused images, with an appropriate exposure setting and both modalities being well utilised.

\begin{figure}[t]
\centering
\includegraphics[clip,width=1\linewidth]{./images/Visualisation_unified.pdf}

\caption{Qualitative results of the advanced methods on different fusion tasks.
}
\label{figure_qualitativeALL}
\vspace{-3mm}
\end{figure}

% Table generated by Excel2LaTeX from sheet '2x3_cddfuse_woSD_solid_mfifFirs'
\begin{table*}[tbp]
  \centering

  \vspace{-6mm}
  \resizebox{1\linewidth}{!}{  
    \begin{tabular}{ccccc:cccc|ccccc|ccccc}
    \hline
    \multicolumn{5}{c:}{\textit{(a1) MFIF Task: Lytro}} & \multicolumn{4}{c|}{\textit{(a2) MFIF Task: MFI-WHU}} & \multicolumn{5}{c|}{\textit{(c) MEIF Task: DSCIE}} & \multicolumn{5}{c}{\textit{(d) NIR-VIS Task: Scene}} \\
    \hline
    Method & EI    & VIF   & SCD   & AG    & EI    & VIF   & SCD   & AG    & Method & EI    & VIF   & SCD   & AG    & Method & EI    & VIF   & SCD   & AG \\
    \hline
    U2Fusion & 67.20  & \textbf{1.39 } & 0.84  & 6.34  & \textbf{79.11 } & \textbf{1.50 } & 0.56  & 7.88  & U2Fusion & \textbf{83.00 } & 1.69  & 0.49  & \textbf{8.71 } & IFCNN & 82.13  & 0.90  & 1.14  & 8.72  \\
    UNIFusion* & 70.14  & 1.30  & 0.60  & \textbf{6.77 } & 66.57  & 1.01  & 0.29  & 7.19  & SPD-MEF* & 78.17  & \textbf{1.72 } & 0.49  & 8.12  & U2Fusion & 80.73  & 1.07  & 1.19  & 8.51  \\
    SDNet & 62.98  & 1.12  & 0.75  & 6.16  & 72.98  & 1.16  & 0.63  & \textbf{7.98 } & MEF-GAN* & 80.21  & 1.59  & 0.63  & 8.06  & SDNet & 76.70  & 0.86  & 0.72  & 8.26  \\
    MUFusion & \textbf{70.40 } & 1.34  & \textbf{1.22 } & 6.67  & 77.72  & 1.36  & \textbf{1.11 } & 7.92  & MUFusion & 70.18  & 1.64  & \textbf{0.96 } & 7.19  & MURF*  & 41.71  & 0.41  & 0.19  & 4.22  \\
    ZMFF*  & 58.97  & 1.11  & 0.36  & 5.48  & 57.90  & 1.03  & 0.33  & 5.49  & IID-MEF* & 59.12  & 1.12  & 0.36  & 6.13  & Text-IF & \textbf{88.19 } & \textbf{1.49 } & \textbf{1.45 } & \textbf{9.16 } \\
    \rowcolor[rgb]{ .749,  .749,  .749} GIF (Ours)  & \underline{\textbf{80.86 }} & \underline{\textbf{1.74 }} & \underline{\textbf{1.37 }} & \underline{\textbf{7.71 }} & \underline{\textbf{91.61 }} & \underline{\textbf{1.94 }} & \underline{\textbf{1.29 }} & \underline{\textbf{9.25 }} & GIF (Ours)  & \underline{\textbf{111.27 }} & \underline{\textbf{2.52 }} & \underline{\textbf{1.04 }} & \underline{\textbf{12.00 }} & GIF (Ours)  & \underline{\textbf{101.32 }} & \underline{\textbf{1.51 }} & \underline{\textbf{1.45 }} & \underline{\textbf{10.83 }} \\
    \hline
    \multicolumn{5}{c:}{\textit{(b1) IVIF Task: LLVIP}} & \multicolumn{4}{c|}{\textit{(b2) IVIT Task: TNO}} & \multicolumn{5}{c|}{\textit{(e) Remote Task: QuickBird}} & \multicolumn{5}{c}{\textit{(f) Medical Task: Harvard}} \\
\hline
    Method & EI    & VIF   & SCD   & AG    & EI    & VIF   & SCD   & AG    & Method & EI    & VIF   & SCD   & AG    & Method & EI    & VIF   & SCD   & AG \\    
    \hline    
    MURF  & 38.02  & 0.25  & 0.56  & 3.75  & 45.93  & 0.94  & 1.52  & 4.35  & IFCNN & 18.30  & 1.16  & 0.86  & \textbf{1.73 } & U2Fusion & 73.29  & 0.78  & 1.46  & 7.06  \\
    LRRNet* & 34.93  & 0.34  & 0.95  & 3.64  & 36.37  & 0.75  & 1.40  & 3.75  & TextFusion & 13.73  & 0.76  & 0.32  & 1.29  & IFCNN & \textbf{98.67}  & 0.92  & 1.33  & \underline{\textbf{9.57}}  \\
    DDFM*  & 41.13  & 0.51  & 1.55  & 4.43  & 31.01  & 0.67  & 1.60  & 3.03  & MUFusion & \textbf{18.45 } & 1.05  & -0.02  & 1.72  & SDNet & 83.86  & 0.71  & \textbf{1.60 } & 8.39  \\
    CDDFuse* & 52.32  & \textbf{0.79 } & \textbf{1.58 } & 5.42  & 43.83  & 0.91  & \textbf{1.66 } & 4.55  & P2Sharpen* & 16.66  & \textbf{1.20 } & \textbf{0.94 } & 1.56  & MUFusion & 88.66  & \textbf{0.97 } & 1.23  & 8.39  \\
    Text-IF* & \textbf{61.30 } & \underline{\textbf{0.93 }} & 1.49  & \textbf{6.33 } & \textbf{46.09 } & \underline{\textbf{1.04 }} & 1.53  & \textbf{4.61 } & ZeroSharpen* & 13.20  & 0.94  & 0.48  & 1.26  & CoCoNet*  & 89.55  & 0.71  & 1.04  & 8.84  \\
    \rowcolor[rgb]{ .749,  .749,  .749} GIF (Ours)  & \underline{\textbf{62.46 }} & 0.73  & \underline{\textbf{1.61 }} & \underline{\textbf{6.70 }} & \underline{\textbf{52.30 }} & \textbf{0.99 } & \underline{\textbf{1.66 }} & \underline{\textbf{5.24 }} & GIF (Ours)  & \underline{\textbf{23.02 }} & \underline{\textbf{1.56 }} & \underline{\textbf{1.04 }} & \underline{\textbf{2.16 }} & GIF (Ours)  & \underline{\textbf{100.71 }} & \underline{\textbf{1.10 }} & \underline{\textbf{1.68 }} & \underline{\textbf{9.73 }} \\
    \hline
    \end{tabular}%
}

  \caption{Quantitative results of the dedicated (*) or unified methods on various image fusion tasks. (\underline{\textbf{Bold}}: best, \textbf{Bold}: second best) }
      \label{table_quantative_all}  

\end{table*}%

\subsection{Multiple Modalities - Seen Tasks}
In this section, we present the fusion results of our GIFNet on the tasks related to our training data, \textit{i.e.}, MFIF and IVIF tasks.
We compare the proposed method with dedicated algorithms for these two tasks, including Text-IF~\cite{yi2024textIF}, CDDFuse~\cite{zhao2023cddfuse}, DDFM~\cite{Zhao_2023_ICCV_DDFM}, LRRNet~\cite{li2023lrrnet}, ZMFF~\cite{hu2023zmff} and UNIFusion~\cite{cheng2021unifusion}.
We also compare with generalised image fusion methods, including MURF~\cite{xu2023murf}, MUFusion~\cite{cheng2023mufusion}, U2Fusion~\cite{xu2022u2fusion}, and SDNet~\cite{zhang2021sdnet}.

\textit{MFIF task:}
As shown in Table~\ref{table_quantative_all} (a1) and (a2), our GIFNet achieves promising results in terms of various image fusion assessment metrics.
For example, the best performance in VIF, with an increase of 25\%, demonstrates that our fusion results can effectively enhance the source information, as seen  in the first row of Fig.~\ref{figure_qualitativeALL}.

\textit{IVIF task:}
For the IVIF task, as shown in the second row of Fig.~\ref{figure_qualitativeALL}, our fusion results, benefiting from collaborative training, can better adjust the mixing proportion of source modalities.
The abundant texture details from the RGB image are well-preserved, and the thermal radiation information contributes to a brighter scene appearance. 
As a result, in both low-light and common conditions, GIFNet generally achieves the best performance across all these quantitative experiments (Table~\ref{table_quantative_all} (b1) and (b2)).
The relatively poor results on VIF of the LLVIP dataset can be attributed to the ``choose-max" fusion strategy in CDDFuse and Text-IF, which retains the source content with the higher pixel value from the input. 
While this approach ensures high visual fidelity (VIF), the fused images tend to bias towards one input modality, ignoring the information from the other (see the visualisation of Text-IF)~\cite{cheng2024textfusionunveilingpowertextual}.

\subsection{Multiple Modalities - Unseen Tasks}
In this section, we present the fusion results of our GIFNet on tasks not involved in training, including multi-exposure image fusion, near-infrared and visible image fusion, remote sensing image fusion, and medical image fusion tasks.
Similarly, we further compare our method with the algorithms designed specifically for these four tasks, including MEF-GAN~\cite{xu2020mef}, SPD-MEF~\cite{li2020fastMEFDecomposition},  IID-MEF~\cite{zhang2023iid}, MURF~\cite{xu2023murf}, P2Sharpen~\cite{zhang2023p2sharpen}, ZeroSharpen~\cite{wang2024zero}, CoCoNet~\cite{liu2024coconet},
TextFusion~\cite{cheng2024textfusionunveilingpowertextual}, which incorporates textual information in the image fusion field, and a generalised method IFCNN~\cite{zhang2020ifcnn}.
%are also used as competitors.


\textit{MEIF Task:}
Our GIFNet performs well with poorly exposed images in the MEIF tasks. As shown in the third row of Fig.~\ref{figure_qualitativeALL}, in terms of the overall exposure, which is a significant criterion in this task, our result has more appropriate brightness without serious color distortion (see the highlighted regions).
For the quantitative assessment (Table~\ref{table_quantative_all} (c)), compared with advanced approaches, we achieve much higher performance on all image fusion metrics, \textit{e.g.}, VIF (+46.7\%) and AG (+37.8\%).

\textit{NIR-VIS Task:}
This task is similar to IVIF but replaces the mid-far infrared modality with a near-infrared image. As shown in the fourth row of Fig.~\ref{figure_qualitativeALL}, the existing fusion methods consistently improve low-light conditions of RGB images using the information conveyed by the NIR modality, while our GIFNet exhibits the clearest texture details.
The quantitative results also demonstrate that GIFNet outperforms existing algorithms (Table~\ref{table_quantative_all} (d)).
%The additional text guidance and computational cost (Fig.~\ref{figure_comparison_gen_effi}a) enable Text-IF to achieve results comparable to GIFNet. 
Notably, although MURF is trained on this task, it focuses more on addressing the registration issue, resulting in relatively poor performance.

\textit{Remote Task:}
This task, also known as Pansharpening, aims to simultaneously keep the spatial and spectral resolution of panchromatic and multispectral images. 
As illustrated in the second last row of Fig.~\ref{figure_qualitativeALL}, like previous tasks, GIFNet obtains fused images with sharper edge information and superior imaging quality.
In contrast, competitors fail to maintain the shape of objects from the high-resolution panchromatic modality.
Although specifically designed for this task, P2Sharpen and ZeroSharpen are surpassed by our approach across multiple metrics, as shown by the quantitative results in Table~\ref{table_quantative_all} (e).

\textit{Medical Task:}
The medical image fusion task aims to preserve salient organ structures from Magnetic Resonance Imaging (MRI) and clear functional information from Positron Emission Tomography (PET). 
As shown in Table~\ref{table_quantative_all} (f), despite not being trained specifically for this task, GIFNet demonstrates strong visual information fidelity (VIF) and maintains a high correlation with the source inputs (SCD) in its fusion results.
This performance is consistent with the visualisation in the last row of Fig.~\ref{figure_qualitativeALL}, \textit{i.e.}, with enhanced details, which shows clearly that  the results of GIFNet well present the local structure from the MRI modality.

\begin{table}[tbp]
  \centering

  \resizebox{1\linewidth}{!}{   
    \begin{tabular}{ccccc}
    \hline
    Method & Venue & Task Combination & Top-1 Acc & Top-5 Acc. \\
    \hline
    TarDAL++ & 22' CVPR & IVIF+Detect & 46.62\% & 76.11\% \\
    MURF  & 23' TPAMI & IVIF+Register & 50.04\% & 79.91\% \\
    SDNet & 21‘ IJCV & IVIF  & 50.28\% & 79.62\% \\
    MUFusion & 23' Inf. Fus. & IVIF  & 50.39\% & 79.59\% \\
    SDNet$^{\dagger}$ & 21' IJCV & MFIF  & 50.83\% & 79.56\% \\
    TextFusion & 24' Inf. Fus. & IVIF+ViLT & 51.15\% & 80.81\% \\
    MUFusion$^{\dagger}$ & 23' Inf. Fus. & MFIF  & 51.50\% & 79.86\% \\
    U2Fusion & 22' TPAMI & IV+ME+MF & 51.58\% & 80.38\% \\
    CDDFuse & 23' CVPR & IVIF  & 52.20\% & 80.00\% \\
    Text-IF & 24' CVPR & IVIF+ViLT & 52.57\% & 80.98\% \\
    Cifar-original & -     & -     & 54.11\% & 83.03\% \\
    \rowcolor[rgb]{ .749,  .749,  .749} GIFNet & Ours  & IVIF+MFIF & \underline{\textbf{56.18\%}} & \underline{\textbf{84.95\%}} \\
    \hline
    \end{tabular}%
}

  \caption{The classification results of the ResNet56 when using different data for training. The original CIFAR100 dataset and enhanced data using different image fusion approaches are regarded as the training set. ($\dagger$: this method is trained with a different task)} 

    \label{table_quantative_cifar}
\end{table}%


\begin{figure}[t]
\centering
\includegraphics[clip,width=1\linewidth]{./images/cifar100_qualitative.pdf}
\vspace{-5mm}
\caption{Visualisations of the advanced image fusion methods on the CIFAR100 single modality enhancement task.
}
\label{figure_qualitativeCifar}
\vspace{-4mm}
\end{figure}

\subsection{Single Modality: the Classification Task}
Our GIFNet’s versatility encompasses both multi-modal image processing and single modality tasks.
This experiment evaluates GIFNet’s ability to boost RGB image classification using enhanced images as inputs~\cite{chen2021hybrid}. 
The original CIFAR100 training set and enhanced data obtained through different image fusion approaches are used to train the ResNet56 network~\cite{he2016resnet} from scratch.
Once trained, the ResNet56 classifier is tasked with evaluating the performance on the original test set.

As illustrated in Fig.~\ref{figure_qualitativeCifar}, we present original CIFAR100 RGB images alongside enhanced versions produced by different approaches.
GIFNet demonstrates a notable improvement in image quality.
For instance, in the first row, the blurring present in the original data is mitigated, with clearer information being preserved.
In the second example, our method excels in edge enhancement, outperforming other techniques.
%Without considering cross-task interaction among fusion tasks, the other advanced methods fall short regarding these two points, delivering similar or poorer imaging quality.

Quantitative assessments, as shown in Table~\ref{table_quantative_cifar}, indicate that certain fusion methods yield comparable classification performance to the original dataset without improving image quality, such as SDNet and MUFusion. 
%TarDAL++~\cite{liu2022target}, optimised for object detection, incorporates high-level semantics but fails to consistently generate outputs suitable for classification, resulting in diminished performance.
Note that, U2Fusion, leveraging even more fusion tasks, suffers from a lack of effective interaction in its sequential training strategy, leading to suboptimal enhancement.
In contrast, using the task-independent representation from the cross-task interaction, GIFNet is the only method to surpass the original training setting.







%\begin{figure}
%\centering
%\includegraphics[clip,width=1\linewidth]{./images/quantativeMutualReinforcement.pdf}
%\caption{Visualisation of the controllable factor $\lambda$ from the cross-fusion gating mechanism.
%}
%\label{figure_quantitativeMutualReinforcement}
%\vspace{-3mm}
%\end{figure}
%
%\subsection{An Analysis of the Visualisation Results }
%\textcolor{red}{Fused image can benefit from the task-specific features?}


%\subsection{An Analysis of Cross-Task Interaction}
%During the training process, a cross-fusion gating mechanism is used to iteratively optimise the IVIF and MFIF bypasses.
%In this section, we visualise the learnable parameter $\lambda$ of different branches to provide an intuitive understanding of the cross-task interaction process.
%As shown in Fig.~\ref{figure_quantitativeMutualReinforcement}, both of the controllable factors from these bypasses will converge to a stable value, \textit{i.e.}, around 0.08 for the MFIF bypass and 0.18 for the IVIF bypass.
%This imbalance can be attributed to that the MFIF task has ground truth images for supervised training, thus requiring less auxiliary features from the other unsupervised trained bypass.
%%Besides, realising the collaborative learning only with a few task-specific features from the other bypass demonstrates that the task-shared features is well extracted through the proposed reconstruction task.
%The curve with a decreasing trend also indicates that the dependency among different bypasses will become less significant as the training process proceeds.






%\begin{figure}[t]
%\centering
%\includegraphics[clip,width=0.8\linewidth]{./images/quantativeMutualReinforcement.pdf}
%\caption{Visualisation of the controllable factor $\lambda$ in CFGM.
%}
%\vspace{-3mm}
%\label{figure_quantitativeMutualReinforcement}
%\vspace{-3mm}
%\end{figure}



%\subsection{An Analysis of Cross-Task Interaction}
%During the training process, a cross-fusion gating mechanism is used to iteratively optimise the IVIF and MFIF branches.
%In this section, we visualise the learnable parameter $\lambda$ of different branches to provide an intuitive understanding of the cross-task interaction process.
%As shown in Fig.~\ref{figure_quantitativeMutualReinforcement}, the controllable factors from these branches converge to stable values: approximately 0.08 for the MFIF branch and 0.18 for the IVIF branch.
%his imbalance can be attributed to the fact that the MFIF task has ground truth images for supervised training, thus requiring fewer auxiliary features from the other unsupervised trained branch.
%Additionally, the curve with a decreasing trend indicates that the dependency among different branches lessens as the training process proceeds.