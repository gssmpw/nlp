\section{Discussion}

Here, we discuss the implications of our methodology and findings for future audits of algorithmically curated feeds. 

\subsection{Empowerment Through Transparency}

Prior literature has shown that people are often unaware that algorithms control what they see and, in turn, how their content is spread~\cite{rader-explanations-facebook, silas-awareness}. This unawareness can lead to harm~\cite{feedvis-was-not-close} resulting in calls for greater transparency regarding these algorithmic systems~\cite{alvarado-algorithmic-experience}. Our study enables transparency by quantifying how factors like commenting rate and undesirable activity affect algorithmic ranking on a prominent social media feed. 

\textbf{Implications for Content Creators.} Our study has implications for content creators, particularly those whose livelihoods are tied to algorithmic decision making. The relationship we identified between algorithmic rank and engagement highlights how the algorithms employed by social media platforms essentially get to decide which content is prioritized and which is not. This has led content creators on platforms like YouTube to blindly attempt to appease algorithms by adjusting their content creation process~\cite{creator-friendly-algorithms}. Audits of algorithmic feeds can empower creators by providing accurate information about how their actions can affect content prioritization on their respective platforms.

\textbf{Implications for Content Consumption.}

We found that the order in which content is ranked can influence the levels and types of user engagement within algorithmically curated feeds. Similarly, the types of content that are recommended and where they are positioned can affect users' content consumption practices. Prior work has examined this impact through users' trust in Google's ranking algorithm, which leads to a greater number of clicks on top-ranked links regardless of their accuracy or relevance to the query~\cite{joachims-clicks}. This \textit{trust bias} that \citet{joachims-clicks} refer to, coupled with the increased engagement levels observed in our analysis (see Section~\ref{sec:rq3}), can catalyze the spread of misinformation---especially if higher ranks within feeds are misconstrued as an indicator of content quality and trustworthiness. To alleviate some of these pitfalls, some platforms have provided explanations for their algorithmic recommendations, however, \citet{tiktok-explanations} found that those on TikTok in particular were generic and inapplicable to the user.  Further research is needed to qualitatively understand how algorithmic rank affects users' perceptions of content highlighted on social media feeds.

\subsection{Implications for Content Moderation}

Prior work has shown that sudden popularity can be disruptive to moderation teams that have to manage increased activity, particularly from newcomers~\cite{eternal-september, my-paper}. In addition to increased activity levels, we found that posts ranked higher on r/popular corresponded to higher proportions of undesirable activity, which can make handling popularity even more challenging. Despite how integral algorithmic curation is to social media platforms, moderators currently have little influence on algorithmic curation systems. This lack of agency impedes their ability to direct their communities and highlight desirable content---an idea that has been explored in prior work~\cite{fred-creator-hearts}. Given that moderators have little influence on these systems, there is a need for design interventions that provide moderators with more controls or ways to ``override'' algorithmic curation systems. These additional affordances can be used to highlight desirable content in feeds, which has theoretical foundations~\cite{successful-communities-book} and is currently done with existing mechanisms such as awards, upvotes, pins, and flairs~\cite{charlotte-positive-reinforcement}. These additional controls may take the form of various sliders that adjust latent weights on the feed to emphasize different priorities (e.g., increase user diversity on the feed), visual indicators for moderator-assigned contributions, or specific feeds that filter for moderator-selected contributions---similar to the ones the New York Times has to filter for reader- and editor-selected comments~\cite{nyt-comment-picks}.

\subsection{Implications for Future Empirical Audits} \label{sec:future-audits}

Despite the pervasiveness of algorithmic curation on social media, studying it as external researchers is exceedingly difficult. This difficulty is compounded by data access restrictions as platforms lock down their APIs, e.g., X/Twitter~\cite{twitter-api-access} and Reddit~\cite{reddit-admin-pushshift}. The reduction in API accessibility has led researchers to use other methods like sock-puppet accounts~\cite{post-api-era, twitter-sock-puppet-bias, diakopoulos-more-accounts, resnick-youtube-scrubbing, tiktok-explanations} and calls for data donations~\cite{jhaver-data-donation, chouaki-data-donation}---each with their own set of drawbacks. These restrictions have led to the reliance on externally-maintained datasets like the now defunct Pushshift---which has been heavily used in prior research~\cite{shagun-explanations, atcheson-new-members, renkai-pushshift, tal-august-summaries}. \textit{To avoid these challenges, we developed a robust pipeline to collect large-scale high-fidelity snapshots of Reddit's trending feed r/popular which can be adapted for other platforms with similar feeds.}

However, we recognize that these approaches are extremely time and resource-intensive, posing significant challenges for scalability without adequate financial backing and collaborative support. Although partnerships between academia and industry have the potential to yield important research outcomes, such collaborations are increasingly rare. Given these circumstances, and echoing a theme identified in a 2024 CCC Workshop Visioning Report~\cite{ccc-workshop}, this presents an opportunity for researchers to share auditing infrastructures and data-sharing protocols that support the collection, storage, and access of social media data for research purposes. For example, in the industry circuit, \textit{data clean rooms}~\cite{herbrich2022data} are emerging for several companies to have a shared and secure data infrastructure. However, the creation of these infrastructures must also adhere to relevant privacy and legal standards such as GDPR~\cite{gdpr} or CCPA~\cite{ccpa}, especially for data that contain sensitive information that would need to be anonymized. This would enable more research like ours, informing future regulation, while safeguarding the rights and privacy of social media users.