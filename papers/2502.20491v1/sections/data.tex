\section{Data} \label{sec:data}

To retrieve posts from the r/popular feed, we used PRAW---a Python wrapper for Reddit's API. Extending \citet{my-paper}'s methodology, every 2 minutes, we captured a \textit{snapshot} of the r/popular feed's top 100 posts from March 23, 2022, to February 8, 2023---approximately 11 months. 
A single API request returns at most 100 posts. Although the feed goes beyond the top 100, we settled on only requesting the top 100 posts to avoid having to make multiple requests for a single snapshot and to stay within Reddit API rate limits. Thus, during the study period, we made only one request every 2 minutes.

\subsection{Sampling Approach}

By taking snapshots of r/popular every 2 minutes, totaling 224,121 feed snapshots, we collected 134,661 unique posts from 1,423 distinct subreddits. \textit{For clarity, feed snapshots refer to a capture of the entire feed whereas a snapshot from now on refers to a capture of an individual post within a feed snapshot} of which there are 22,412,100 snapshots---100 per feed snapshot because there are 100 posts in a feed snapshot.

For tractability, we randomly sampled 10,000 posts from the 134,661 posts observed. The resulting sample contains 694 subreddits and 1,548,266 snapshots. For the rest of the paper, our findings are based on this representative sample of 10,000 posts and their respective snapshots. From the sample, we found that posts stay on r/popular for about 164 snapshots on average ($\mu=163.63$, $\sigma=140.25$). Additionally, posts from the sample, on average, stay on r/popular's top 100 for 6.1 hours ($\mu=6.11$, $\sigma=5.09$). Along with these snapshots, we used Pushshift~\cite{pushshift} to obtain the comments for each post.

\subsection{Identifying Undesirable Activity} \label{sec:undesired-definition}

In recent years, news outlets have suggested that social media platforms are intentionally promoting antisocial content through algorithmic prioritization to drive greater user engagement~\cite{radical-ideas, inflammatory-content}. To investigate whether antisocial behavior has any interactions with algorithmic curation, specifically on r/popular, we employed \citet{bert-model}'s fine-tuned BERT model, which assigns three toxicity-related scores for each comment: \textsc{non\_toxic}, \textsc{slightly\_toxic}, and \textsc{highly\_toxic}. Each score ranges from 0 to 1. If a comment exceeded a score of 0.5---a threshold used in prior work~\cite{conversations-gone-alright}---for either \textsc{slightly\_toxic} or \textsc{highly\_toxic}, then we labeled it as toxic. \citet{bert-model} fine-tuned the model using r/AskReddit comments and showed its generalizability to 99 other large subreddits. Since the subreddits that appear on r/popular are mostly large, we claim that using this model is well suited for identifying toxic comments made within our r/popular posts. Additionally, some comments were removed, presumably by a moderator or bot, before they could be archived by Pushshift. \textit{To better capture antisocial behavior, we combine comments that contained ``[removed]'' with the ones flagged by the BERT model under an umbrella term: ``undesired comments.''}

\subsection{Features} \label{sec:features}

In this section, we describe the features we used for our regression models and provide a brief justification for inclusion. \textit{This feature list applies to posts captured at a specific snapshot/time.} Thus, the features are measured at a specific time, e.g., the number of comments at a particular snapshot.

The first feature is:

\begin{enumerate}
    \item \textit{Content Type:} Whether the post contains a link, video, image, or just text. This is a categorical variable where \textit{image posts} are the reference category, i.e., the category in which all other content types are compared to.
\end{enumerate}

Content type functions as a control variable to capture differences between posts that include images (49.19\%), links (16.19\%), text (12.45\%), and videos (22.17\%).

\begin{enumerate}
    \setcounter{enumi}{1}
    \item \textit{Rank:} Where the post is on r/popular where rank 1 is the top of the feed and rank 100 is the bottom.
    \item \textit{Age (hours):} The amount of time, in hours, since the post's creation.
\end{enumerate}

Rank is the focus of our audit and because r/popular emphasizes content that is \textit{currently} popular, the recency of a post is a natural feature to include.

The next set captures the activity within the post's thread, i.e., comment section. We also have features that utilized the labels produced by the BERT model described previously.

\begin{enumerate}
    \setcounter{enumi}{3}
    \item \textit{Num. Comments:} The total number of comments at the time the snapshot was taken.
    \item \textit{Recent Comments:} The number of comments made in the previous 10 minutes.
    \item \textit{Proportion Undesired:} The proportion of comments that were labeled as undesired comments.
    \item \textit{Proportion Recent Undesired:} The proportion of comments labeled as undesirable in the last 10 minutes.
    \item \textit{Score:} The number of upvotes on the post minus the number of downvotes.
    \item \textit{Recent Votes:} The number of votes the post received in the last 10 minutes.
    \item \textit{Proportion Upvotes:} The proportion of \textit{all} votes that were upvotes. 
\end{enumerate}

We included recent activity because there is a ``hot'' calculation\textsuperscript{2} that likely includes comment and vote velocity, i.e., how many comments are coming in currently. A 10-minute window was selected because posts stay at a rank for an average 7.5 of minutes before moving onto another ($\mu=7.49$, $\sigma=7.77$). Calculating the proportion of undesired comments helps us test whether Reddit's ranking algorithm is in any way influenced by the presence of undesired activity.

\begin{enumerate}
    \setcounter{enumi}{10}
    \item \textit{Num. Subscribers:} The number of subscribers the post's origin subreddit has.
\end{enumerate}

Lastly, communities are integral to Reddit, which is why we included their size as a control variable in the models.

Table~\ref{tab:descriptive} provides the geometric mean and standard deviations for each feature. We used the geometric versions of these measures because the variables are log-transformed in our regression analyses. Additionally, the geometric standard deviations are used to inform the units we used to scale the regression coefficients which are also shown in Table~\ref{tab:descriptive}. The consistency to use 2x for all units except the proportion of upvotes is to assist with interpretability.

\input{snippets/descriptive}
