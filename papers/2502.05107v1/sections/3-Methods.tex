\section{3DMolFormer}
\begin{figure}[t]
    \centering
    \includegraphics[width=\textwidth]{imgs/Main.pdf}
    \vspace{-0.4cm}
    \caption{Overview of 3DMolFormer. The left shows the dual-channel model architecture, the top right illustrates the input and output of the two SBDD tasks in a parallel sequence, and the bottom right outlines the pre-training and fine-tuning process.}
    \label{Overview}
    \vspace{-0.2cm}
\end{figure}

\subsection{Format of Pocket and Ligand Sequences with 3D Coordinates}

To leverage a causal language model for handling 3D protein pockets and small molecules while explicitly separating discrete structural information from continuous spatial coordinates, we design a parallel sequence format. This format consists of a discrete token sequence $s_\mathrm{tok}$ and a continuous numerical sequence $s_\mathrm{num}$, both of which share the same length and align element-wise. The token sequence consists of tokens in a predefined vocabulary, while the numerical sequence contains floating-point values.

As shown in Figure~\ref{PocketSeq}, the sequence for a protein pocket $s^\mathrm{poc}$ consists of two parts: the first $s^\mathrm{poc\_atoms}$ represents an atomic list, and the second $s^\mathrm{poc\_coord}$ contains 3D coordinate information. 
The atomic list is encoded in the token sequence, which includes all atoms in the protein pocket except for hydrogen atoms. Aside from alpha carbon atoms, denoted as 'CA', other atoms are represented by their element type, such as 'C', 'O', 'N', and 'S'. The sequence of atoms follows the order of the pdb file, where each amino acid begins with ['N', 'CA', 'C', 'O'] followed by the side-chain atoms.
The normalized 3D coordinates for each atom in the atomic list are included in the numerical sequence in the same order, with each dimension ('x', 'y', 'z') occupying a separate position. The length of the 3D coordinate sequence is always three times the length of the atomic list.
Moreover, in the token sequence, the start and end of the atomic list are marked by 'PS' and 'PE', while the 3D coordinates are delineated by 'PCS' and 'PCE' at the start and end, respectively.
In the numerical sequence, numbers that do not correspond to 3D coordinates are padded with 1.0.

As illustrated in Figure~\ref{LigandSeq}, the sequence for a small molecule $s^\mathrm{lig}$ is similar to that of the protein pocket, comprising both a SMILES string section $s^\mathrm{lig\_smiles}$ and a 3D coordinate section $s^\mathrm{lig\_coord}$.
After atom-level tokenization~\citep{SMILEStokenization}, the SMILES string of the small molecule is encoded in the token sequence, excluding hydrogen atoms. It is important to note that some tokens may not correspond to atoms, and thus, no 3D coordinates will be associated with them.
The normalized 3D coordinates for each atom in the tokenized SMILES string are included in the numerical sequence, with each coordinate dimension ('x', 'y', 'z') occupying a separate position. The length of the 3D coordinate sequence is always three times the number of atoms in the small molecule.
In the token sequence, the start and end of the SMILES tokens are marked by 'LS' and 'LE', while the 3D coordinates of the corresponding atoms are marked by 'LCS' and 'LCE' at the start and end, respectively.
In the numerical sequence, numbers not corresponding to 3D coordinates are similarly padded with 1.0.


When the sequence of a protein pocket is concatenated with that of a small molecule ligand, it forms a pocket-ligand complex sequence along with their 3D coordinates $s^\mathrm{poc-lig}$. This sequence format offers three advantages:
\begin{itemize}[leftmargin=*]
    \item It fully encapsulates the structural and 3D coordinate information of both the protein pocket and the small molecule ligand.
    \item Discrete structural information and continuous numerical data are separated into two parallel sequences, enabling independent processing of each data type.
    \item The sequence of the pocket-ligand complex maintains causal logic. As depicted in the upper right of Figure~\ref{Overview}, this sequence structure allows autoregressive prediction, which can effectively represent both pocket-ligand docking and pocket-aware drug design tasks.
\end{itemize}

Specifically, we normalize the coordinates of all pocket-ligand complexes by translating their center of mass to the origin $(0,0,0)$. Additionally, to ensure numerical stability during training~\citep{DeepLearning}, we scale the coordinate values by a factor $q>1$ to reduce the range of their distribution:
\begin{equation}
\label{coordnorm}
(x_i',y_i',z_i')=\Big(\frac{x_i-x_c}{q},\frac{y_i-y_c}{q},\frac{z_i-z_c}{q}\Big),
\end{equation}
where $(x_i,y_i,z_i)$ is the original coordinate of the $i$-th atom, $(x_c,y_c,z_c)$ is the coordinate of the center of mass, and $(x_i',y_i',z_i')$ refers to the normalized values used in the numerical sequence.

\subsection{Model Architecture}
To process the aforementioned parallel sequences, we require an autoregressive language model that can simultaneously take a discrete token sequence and a continuous floating-point sequence as input, while predicting both the next token and the next numerical value. Inspired by xVal~\citep{xVal}, we propose a dual-channel transformer architecture for 3DMolFormer, as illustrated in the left part of Figure~\ref{Overview}. The module handling the token sequence is based on the GPT-2 model~\citep{GPT-2}, featuring identical token embeddings, positional embeddings, multiple transformer layers, and a prediction head for logits. On top of this, we introduce a parallel numerical channel at both the input and output stages.

At the input stage, we multiply the embedding of each token in the token sequence with the corresponding value in the numerical sequence, using this product as the input to the positional embedding. This is why numerical values that lack meaningful information are padded with 1.0. At the output stage, in parallel with the token prediction head, we add a number head to predict the next floating-point value.

During inference with 3DMolFormer, the outputs are handled in two modes:
\begin{itemize}[leftmargin=*]
    \item Token Mode: In the drug design task, when predicting ligand SMILES tokens, the corresponding numerical output holds no meaningful value and is therefore padded with 1.0.
    \item Numerical Mode: In docking and drug design tasks, once the ligand SMILES is determined, the length of the 3D coordinate sequence and its tokens are also fixed. Therefore, the token output no longer holds meaningful information and is filled with the expected tokens (from ['x', 'y', 'z', 'LCS', 'LCE']). When the position corresponds to ['x', 'y', 'z'], the predicted floating-point values are appended to the input numerical sequence. For tokens corresponding to ['LCS', 'LCE'], the numerical values are also set to 1.0.
\end{itemize}


\subsection{Self-supervised Pre-training}
To enable the 3DMolFormer model to learn the general patterns of pocket-ligand complex sequences, we conduct large-scale pre-training on 3D data, which includes three datasets: approximately 3.2M protein pockets, about 209M small molecule conformations, and around 167K pocket-ligand complexes. The first two datasets were collected by Uni-Mol~\citep{Uni-Mol} for large-scale pre-training on 3D protein pockets and small molecules, while the last dataset was generated by CrossDocked2020~\citep{CrossDocked}.

In order for the dual-channel autoregressive model to capture both the token sequence format and the 3D coordinate patterns of pocket-ligand complexes, we adopt a composite loss function for the prediction of the next token and the corresponding numerical value. This loss function incorporates the cross-entropy (CE) loss for the whole token sequence and the mean squared error (MSE) loss for the numerical sequence corresponding to the 3D coordinates:
\begin{equation}
\label{pretrainloss}
L(\hat{s}, s)=\mathrm{CE}(\hat{s}_\mathrm{tok}, s_\mathrm{tok})+\alpha\cdot \mathrm{MSE}(\hat{s}_\mathrm{num}^\mathrm{coord}, s_\mathrm{num}^\mathrm{coord}),
\end{equation}
where $\hat{s}$ represents the sequence predicted by 3DMolFormer, $s$ refers to the training data, and $\alpha$ is a coefficient that controls the balance between the CE loss and the MSE loss. This composite loss is applied to all of the three types of pre-training data.

Additionally, we employ large-batch training \citep{large-batch-training} through gradient accumulation, which we found to be crucial for the pre-training stability of 3DMolFormer. For further details on pre-training and hyper-parameter settings, please refer to Section~\ref{experiments} and Appendix~\ref{app2}.




\subsection{Fine-tuning}
After the large-scale pre-training, we further fine-tune the 3DMolFormer model on two downstream drug discovery tasks: supervised fine-tuning for pocket-ligand docking, and reinforcement learning (RL) fine-tuning for pocket-aware drug design.

\subsubsection{Supervised Fine-tuning for Protein-ligand Binding Pose Prediction}
In the protein-ligand binding pose prediction (docking) task, as illustrated in Figure~\ref{Overview}, each sample consists of a pocket-ligand complex. The input sequence contains the atoms of the protein pocket and their 3D coordinates, along with the SMILES sequence of the ligand. The output is the 3D coordinates of each atom in the ligand.

The pre-training data for 3DMolFormer already includes about 167K pocket-ligand complexes from CrossDocked2020~\citep{CrossDocked}; however, these complexes are generated using the docking software Smina~\cite{Smina}, which means that the docking performance of models trained with this data would not exceed that of Smina. To improve the upper limit of our model's docking performance, we fine-tune it on the experimentally determined PDBBind dataset~\cite{PDBbind}, which contains approximately 17K ground-truth pocket-ligand complexes. Additionally, we employ a task-specific loss function that computes the mean squared error (MSE) loss only for the 3D coordinates of the ligand in the context of next numerical value prediction, since the inference process of docking operates entirely in numerical mode:
\begin{equation}
L_{\mathrm{docking}}(\hat{s}^\mathrm{lig\_coord},s^\mathrm{lig\_coord})=\mathrm{MSE}(\hat{s}_\mathrm{num}^\mathrm{lig\_coord}, s_\mathrm{num}^\mathrm{lig\_coord}).
\end{equation}

To mitigate overfitting during supervised fine-tuning, SMILES randomization~\citep{SMILESRandomization} and random rotation of the 3D coordinates of complexes are used as data augmentation strategies. For further details on docking fine-tuning, please refer to Section~\ref{exp-docking} and Appendix~\ref{app3}.

\subsubsection{RL Fine-tuning for Pocket-aware 3D Drug Design}
In the pocket-aware drug design task, as illustrated in Figure~\ref{Overview}, each sample is also a pocket-ligand pair. The input sequence includes the atoms of the protein pocket and their 3D coordinates, while the output consists of the ligand SMILES sequence and the 3D coordinates of its atoms.

Inspired by 1D RL-based molecular generation methods \citep{Reinvent}, an RL agent with the 3DMolFormer architecture is initialized with the pre-trained weights, and a molecular property scoring function for each protein pocket is designed as the RL reward. Then, the agent is iteratively optimized to maximize the expected reward of its outputs. Specifically, at each RL step, the agent samples a batch of 3D ligands, and the regularized maximum likelihood estimation (MLE) loss \citep{SMILES_RL} of each ligand is computed and used to update the agent:
\begin{equation}
\label{RLloss}
L_{\mathrm{design}}(\hat{s}^\mathrm{lig})=\big(\log\pi_\text{pre-trained}(\hat{s}_\mathrm{tok}^\mathrm{lig\_smiles})+\sigma\cdot R(m)-\log\pi_\text{agent}(\hat{s}_\mathrm{tok}^\mathrm{lig\_smiles})\big)^2,
\end{equation}
where $\hat{s}^\mathrm{lig}$ ($\hat{s}^\mathrm{lig\_smiles}$ and $\hat{s}^\mathrm{lig\_coord}$) is a sample generated by the RL agent, $m$ is the 3D molecule represented by $\hat{s}^\mathrm{lig}$, and $R(\cdot)$ is reward function evaluating the property of the molecule. $\pi_\text{pre-trained}(s)$ is the likelihood of the pre-trained 3DMolFormer model for generating the sequence $s$, $\pi_\text{agent}(s)$ is the corresponding likelihood of the agent model, and $\sigma$ is a coefficient hyper-parameter to control the importance of the reward. This loss function encourages the agent to generate molecules with higher expected rewards while retaining a low deviation from the pre-trained weights.

It is important to note that to leverage the duality of the two SBDD tasks, the sampling of ligand SMILES utilizes the weights of the RL agent's model, which are continuously updated during fine-tuning. In contrast, the generation of atomic 3D coordinates uses the weights from the model fine-tuned for docking, which remains unchanged during this process. For additional details on RL fine-tuning and hyper-parameter settings, please refer to Section~\ref{exp-drug-design} and Appendix~\ref{app4}.