\section{Experiments}

\input{tables/main_results_INC}

\subsection{Datasets}

Among publicly available datasets, we selected three based on two criteria: (1) the task focuses on relative event understanding without absolute time indicators, and (2) the temporal inconsistency on the dataset can be evaluated.

\textbf{TempEvalQA-Bi}~\cite{qiu-etal-2024-large} involves ordering two explicit events in time, assessing temporal consistency in mutually exclusive question pairs.
\textbf{TRACIE}~\cite{zhou2021temporal} expands the event ordering to implicit events, testing if the hypothesis logically follows the story. 
We finally added \textbf{MCTACO}~\cite{zhou2019going} considering the diverse event-related temporal properties. % allowing us to evaluate whether our method improves overall temporal event reasoning performance. 
The dataset covers broader aspects like event duration or frequency.
We modified the multiple-choice setting of MCTACO into a binary question-answering task for consistency
evaluation, presenting each answer candidate separately to determine if it fits the context. Dataset statistics and examples are in Appendix~\ref{appndx:data_summary}.



\subsection{Metrics}
Along with accuracy (ACC) and F1 scores to assess overall performance, we introduce the inconsistency metric (INC) as a main evaluation measure for temporal inconsistency. We define the INC as the percentage of inconsistent predictions. An inconsistency is counted when at least one incorrect answer is found within a group of minimally dissimilar questions with slight modifications in their temporal semantics, while all other aspects remain unchanged.

TempEvalQA-Bi directly provides this metric. For TRACIE, we manually group questions that are counterfactual to each other. 
We adapt INC in MCTACO by grouping original multiple-choice candidates by question. 

\subsection{Evaluation Settings and Baselines}

For models, we used open-source LLM \textbf{Llama-3 8B} and \textbf{70B}~\cite{llama3modelcard}, and API-based LLM \textbf{GPT-4o-mini} and \textbf{GPT-4o}~\cite{openai2024gpt4technicalreport}. 

For baselines, we first compare \ours with standard prompting (\textbf{SP}) that directly answers the question without intermediate steps, and \textbf{CoT}, which incorporates step-by-step reasoning to derive the answer. Next, we consider methods that aggregate multiple predictions of the same question. Self-\textbf{Consistency}~\cite{wang2023self} predicts one question multiple times and performs majority voting. Self-\textbf{Reflect} methods~\cite{madaan2024self,shinn2024reflexion} iteratively refine own predictions. Multi-agent \textbf{Debate}~\cite{du2024improving} leverages both majority vote and reflection. We employ a 3-shot setting across all configurations. More details on evaluation settings are in Appendix~\ref{appndx:hyperparam}. 


\subsection{Main results}

Table~\ref{tab:main_inc} highlights the performance of our method compared to baseline methods on relative event understanding tasks.
Compared to SP, the CoT baseline is not usually effective and often worsens performance. 
Advanced baselines, Consistency, Reflect, and Debate, also fail to consistently reduce inconsistencies or achieve competitive accuracy.
In contrast, \ours steadily outperforms these baselines, significantly reducing temporal inconsistencies across all datasets and achieving notable improvements in ACC and F1 scores.
The full results on other models are available in Table~\ref{tab:main}. 

\subsection{Analysis}
\label{subsec:anal}
\paragraph{Creating \contrast questions by generation handles diverse temporal aspects.} We compared our generative setting with the \textbf{Ret}rieved \textbf{Q}uestions (Ret.Q) approach, where \contrast questions were retrieved from other questions within the same question group.
We evaluated the methods on MCTACO, which covers various aspects of event reasoning.

Figure~\ref{fig:mctaco_each_type} shows that generating \contrast questions proved more effective for all temporal types.
These results suggest that our method performs better in event understanding with diverse relations, where the dataset cannot often provide high-quality counterfactual questions. Notably, \ours outperforms the Ret.Q baseline even though our method may produce incorrect questions. Also, \ours is more practical since Ret.Q assumes the questions in the test set are observed.

\begin{figure}[h]
    \centering
    \includegraphics[width=1\linewidth]{figures/mctaco_each_type.pdf}
    \vspace{-8mm}
    \caption{Comparison between counterfactual example collection methods on MCTACO with Llama-3-8B.}
    \vspace{-5mm}
    \label{fig:mctaco_each_type}
\end{figure}

\begin{figure}[h]
    \centering
    \includegraphics[width=1\linewidth]{figures/ours_vs_oursd.pdf}
    \vspace{-8mm}
    \caption{Comparison between different counterfactual leveraging methods with the Llama-3-8B model.}
    \vspace{-5mm}
    \label{fig:ours_vs_oursd}
\end{figure}

\paragraph{CCP is robust against wrong counterfactual exemplars.}
We conducted a comparative analysis of two methods: \textbf{Dir}ect \textbf{A}nswering (Dir.A), which involves answering directly from counterfactual exemplars, versus $\text{\ours}$ which leverages the aggregation step to re-evaluate them. 
We conducted experiments on TempEvalQA-Bi and TRACIE, where before-after relations ensure that identifying a counterfactual answer is sufficient to determine the original. We excluded MCTACO since its counterfactual answers do not always determine the validity of the original answer. In the $\text{Dir.A}$ implementation, the answer to the counterfactual question is flipped and directly used as the response to the original question.
The results in Figure~\ref{fig:ours_vs_oursd} demonstrate that \ours consistently outperforms $\text{Dir.A}$, supporting our robustness by the collective evaluation.


