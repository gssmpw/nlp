\newpage
\section*{\centering Appendices}




\section{Data Summary}
\label{appndx:data_summary}
Table~\ref{tab:data_summary} summarizes the dataset statistics used in this study. The numbers of official test samples are reported. 
Due to the budget, we evaluated Llama-3-8B on the full test set, GPT-4o-mini and Llama-3-70B on a random sample of up to 2,000 test set instances, and GPT-4o on 1,000 test set instances.

Additionally, the number of temporal relations considered in each dataset is included in Table~\ref{tab:data_summary}. TempEvalQA-Bi and TRACIE focus mainly on the before-after relation. MCTACO includes diverse temporal relations, and the number of annotated candidates is reported.
The questions in MCTACO are categorized into 5 question types, and examples for each type are provided in Figure~\ref{fig:mctaco_examples}.

\input{tables/data_summary}
\input{tables/full_contrast_examples}

\begin{figure}[h]
    \centering
    \includegraphics[width=1\linewidth]{figures/mctaco_examples.pdf}
    \caption{Examples of MCTACO Question Types. MCTACO covers various temporal aspects including event duration, frequency, stationarity, ordering, and typical time.}
    \label{fig:mctaco_examples}
\end{figure}

Table~\ref{tab:full_contrast_examples} demonstrates counterfactual types and examples addressed across the targeted temporal aspects in our datasets.
Among the five temporal aspects in the MCTACO dataset, we generate counterfactuals for duration, frequency, and typical time in the same way by intervening in the temporal property, modifying $r_1(e1)$ to $r_2(e1)$.

\section{Details of Evaluation Settings}
\label{appndx:hyperparam}

This section outlines the detailed evaluation settings, including hyperparameters, resources, efficiency, and parsing methods.
We use greedy decoding for \textbf{SP}, \textbf{CoT}, and \textbf{CCP}. For \textbf{Consistency}, \textbf{Reflect}, and \textbf{Debate}, we adopt the approach from~\citet{wang2023self}, employing top-k sampling with 
$k=40$ and a temperature of 0.5 for the LLaMA model. For GPT-based models, we set the temperature to 0.7.
\textbf{Consistency} samples 40 outputs from the decoder. \textbf{Reflect} refines the output iteratively for two iterations, including the initial output. In \textbf{Debate}, three agents engage in a debate over two rounds\cite{du2024improving}.
The implementations of the latter two baselines (\textbf{Reflect}, \textbf{Debate}) are based on the GitHub repository~\footnote{\url{https://github.com/composable-models/llm_multiagent_debate}} from~\citet{du2024improving}. Single-run performances are reported.

We note that our method prompts 3 times: for counterfactual question generation, counterfactual answer generation, and original question's answer generation, whose efficiency is compatible with or even more efficient than the three baselines. We also note that the Consistency baseline of Llama-3-70B cannot be reported due to its computation inefficiency.


For resources, we used the Transformers library~\cite{wolf-etal-2020-transformers} and vLLM~\cite{kwon2023efficient} with 4 RTX A6000 GPUs for Llama-3 models. We used Openai API~\footnote{\url{platform.openai.com}} for GPT models.
For output parsing, the models generate the final answer after the phrase \texttt{``Final answer:''}. Counterfactual exemplars are generated by modifying each dataset's questions, hypotheses, and candidate answers.

\input{tables/main_results}
\input{tables/mcqa_setting}




\section{Details of Main Results}

\subsection{Full Main Results}
\label{appndx:full_main_result}
Table~\ref{tab:main} shows the performance of our method compared with baseline methods on relative event understanding tasks. The results show that our method outperforms the baselines across the board.




\subsection{Task generalizability}
To demonstrate that our solution extends beyond binary question answering to multiple-choice question answering (MCQA), we evaluated the performance of GPT models using the original MCTACO evaluation setting~\cite{zhou2019going}. While our primary evaluation decomposed the multiple-choice format into binary questions to measure inconsistency, it can be reconstructed for multiple-choice evaluation. We additionally introduced a baseline for MCQA (MCQA-CoT) that provides the context, question, and all candidate answers, generating one or more correct answers step-by-step. 
% We excluded the Llama-3-8B model due to its inability to match the required output format. 
The results in Table~\ref{tab:mcqa} indicate that our method (CCP) outperforms the MCQA-CoT baseline on multiple-choice tasks, demonstrating its effectiveness in the MCQA setting.


% \section{K-shot}
\section{Further Analysis}

\subsection{Generated vs Retrieved Questions}

% \input{tables/mctaco_each_type}
\begin{figure}[h]
    \centering
    \includegraphics[width=1\linewidth]{figures/mctaco_retq_genq.pdf}
    \vspace{-8mm}
    \caption{Comparison between counterfactual example collection methods on MCTACO with different models.}
    \label{fig:mctaco_retq_genq}
    \vspace{-3mm}
\end{figure}

% Table~\ref{tab:mctaco_each_type}
We tested whether our claim in Figure~\ref{fig:mctaco_each_type} can be generalized to other models. Figure~\ref{fig:mctaco_retq_genq} consistently confirms that creating counterfactual questions by generation handles diverse temporal relations better than retrieving questions across different models.



\begin{figure}[h]
    \centering
    \includegraphics[width=1\linewidth]{figures/num_cq.pdf}
    \vspace{-8mm}
    \caption{Inconsistency changes with the different number of counterfactual questions. The Llama-3-8B model is used.}
    \label{fig:num_cq}
    \vspace{-3mm}
\end{figure}

\input{tables/more_shot}

\subsection{Number of In-context Learning Examples}

Our approach inevitably introduces additional counterfactual examples during in-context learning (ICL), leading to a higher total number of shots compared to the baseline. To ensure a more competitive baseline, we increased the total number of shots in the baseline. In the MCTACO dataset and with the Llama model, we additionally experimented with the 12-shot CoT, which includes 12 passage (P)-question (Q)-candidate (C) pairs, and compared them with our 3-shot. We note that our 3-shot examples include 3 passage-question pairs and 11 candidates.

The results in Table~\ref{tab:moreshot} demonstrate that our method significantly outperforms the CoT, even with the increased number of examples in the baseline (INC score: 60.0 for CoT vs. 57.7 for Ours). This indicates that the performance gains are not simply due to the inclusion of more examples but are primarily driven by leveraging temporal constraints through counterfactual questions to enhance reasoning.

Additionally, we tested whether our approach benefits from additional ICL examples. The results in the last row of Table~\ref{tab:moreshot} confirm this, showing an improvement in INC score from 57.7 to 49.8, further validating the potential performance gain of our method.

\subsection{Number of Counterfactual Questions}
We analyze the impact of varying the number of counterfactual questions on performance by testing with 1, 3, 5, and 7 questions. As shown in Figure~\ref{fig:num_cq}, performance is highest with a single counterfactual question, with degradation observed as the number increases. The result aligns with findings from prior studies, where incorporating excessive counterfactual or contrastive information in prompts often results in diminished performance~\cite{yao2024largelanguagemodelscontrastive,fang2024counterfactualdebatingpresetstances,storaï2024harphesitationawarereframingtransformer}. For instance, multiple information degrade the performance in arithmetic and symbolic reasoning~\cite{yao2024largelanguagemodelscontrastive}, multi-hop and commonsense question answering~\cite{fang2024counterfactualdebatingpresetstances}, and speculative decoding~\cite{storaï2024harphesitationawarereframingtransformer}. These results indicate that such a phenomenon is not specific to our method but highlights a broader limitation in LLMs' ability to process multiple counterfactual information effectively.

\input{tables/bad_results}


\section{Details of Limitations}

\subsection{Tasks with temporal indicators}
\label{appndx:detail_limitation_temporal_indicator}
Table~\ref{tab:bad} shows the experimental results for the tasks requiring the understanding of temporal indicators. We evaluated our method on TimeQA~\cite{chen2dataset}, the event-time ordering task, and TimexNLI-T1~\cite{thukral2021probing}, the time-time ordering task, where \ours showed limited performance gains.

\subsection{Prompt Sensitivity}
\label{appndx:detail_limitation_prompt_sensitivity}
\input{tables/prompt_sensitivity}
Our key decision in the prompt design was to separate the counterfactual question generation prompt (CCP), described in Appendix~\ref{appndx:prompt_examples_gen_counter_q}, from the prompt that answers counterfactual and original questions, detailed in Appendix~\ref{appndx:prompt_examples_gen_ans}. If we generate counterfactual questions and answers in an end-to-end manner using only the prompt in Appendix~\ref{appndx:prompt_examples_gen_ans} (CCP(e2e)), while it still outperforms CoT, the performance slightly decreases, as shown in Table~\ref{tab:prompt_sensitivity}.


\section{Usage of AI Assistants}
ChatGPT was employed to generate answers in the prompt examples.


\newpage

\section{Prompt Templates}
We list the prompts that we used.

\subsection{Prompt Templates for Generating Counterfactual Questions}
\label{appndx:prompt_examples_gen_counter_q}

To generate the counterfactual questions, we use the prompts provided below.
We control question aspects by tailoring ICL examples and prompting the model to follow few-shot examples with the prompt: (\texttt{``following previous examples''}). 


\input{prompts/generate_q}


\subsection{Prompt Templates for CCP and Baselines}
\label{appndx:prompt_examples_gen_ans}
We only show the SP, CoT, and CCP since Consistency is the method that leverages CoT multiple times, and the Reflect and Debate follow the implementation of~\citet{du2024improving} after CoT. 

Given the system and user message, the model generates an assistant's message. In our approach, the model is prompted twice: first to create an assistant's answer to the counterfactual question, and then to generate an assistant's answer to the original question. Though we use 3-shot examples, we provide 1 example for simplicity.


\input{prompts/2_llama_cot_tempeval}
\input{prompts/3_llama_cot_tracie}
\input{prompts/1_llama_cot_mctaco}


\section{Scientific Artifacts}

We used existing scientific artifacts for research purposes, and the use of existing artifacts was consistent with their intended applications.

TempEvalQA-Bi: \href{https://github.com/yfqiu-nlp/temporal-llms?tab=MIT-1-ov-file#readme}{MIT license}

TRACIE: \href{https://www.apache.org/licenses/LICENSE-2.0}{Apache-2.0 license}

Llama-3: \href{https://www.llama.com/llama3/license/}{custom commercial license}

OpenAI API: \href{https://www.apache.org/licenses/LICENSE-2.0}{Apache-2.0 license}
