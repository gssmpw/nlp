\section{Introduction}


Despite the impressive capabilities of LLMs,
a line of research~\cite{jain-etal-2023-language-models,chu2023timebench} has highlighted that these models often lack temporal reasoning abilities. This is especially true for \textit{relative} event understanding, where the goal is to infer temporal relationships between events or properties within an event in the passage, without depending on \textit{absolute} time indicators (e.g., specific dates).

\begin{figure}[t]
    \centering
    \includegraphics[width=1\linewidth]{figures/ours3.pdf}
    \caption{Example of leveraging \contrast questions to resolve temporal inconsistency in LLMs.}
    \label{fig:ours}
    \vspace{-7mm}
\end{figure}

The primary challenge is that LLMs lack \textit{temporal consistency} in their responses~\cite{qiu-etal-2024-large, chen2024improving}. Temporal consistency is defined as the model's ability to ensure that conflicting timelines do not co-exist.
For instance in Figure~\ref{fig:ours}-(a), if the model is temporally inconsistent, mutually exclusive temporal relations like ``before'' and ``after'' are sometimes confused when ordering events, leading to contradictory predictionsâ€”such as stating that Event A happens both before and after Event B in the same context.

While events with time indicators are often addressed with mathematical reasoning~\cite{zhu2023question,su2024timo}, no existing work has successfully tackled the challenge of temporal inconsistency in the events' relative understanding without requiring explicit time markers. 
Chain-of-thought (CoT) reasoning~\cite{wei2022chain}, which primarily aids mathematical and symbolic reasoning~\cite{sprague2024cotmainlymath}, is also reported 
to fail to solve such inconsistency~\cite{qiu-etal-2024-large}.
Considering temporal consistency is fundamental in temporal reasoning, its absence in LLM can undermine key tasks like planning~\cite{sakaguchi-etal-2021-proscript-partially,zhang-etal-2024-narrative}.
These observations highlight the need for alternative reasoning skills to achieve temporal consistency.


This study answers the following research question: \textbf{Can we prompt LLMs to elicit the ability to mitigate temporal inconsistency?} 
Inspired by counterfactual augmentation,
where models are exposed with lexically similar, but typically label-flipping pairs in training
\cite{kaushik2020learning},
we extend it to LLMs to generate \textit{temporally \contrast questions}: 
We introduce lexically small interventions to the original input (e.g. before to after, years to centuries) that drastically affect its temporal semantics. By providing these questions and self-generated answers alongside the original input, the model would rely less on lexical similarities and better understand the semantics.

To this end, we propose a novel counterfactual-consistency prompting (\ours), designed to enhance the temporal consistency of LLMs, as described in Figure~\ref{fig:ours}-(b). \ours first generates temporal counterfactual exemplars and then applies the insights gained to address the original temporal question.
This method is particularly effective in relative event understanding because the counterfactual exemplars not only encourage the model to understand different temporal semantics but also directly impose temporal constraints.
For instance, if the model states that ``Event A happens after Event B'' and also recognizes that ``Event A happens before Event B'', the conflict forces the model to collectively re-weight the validity of these two statements.

We show performance gain of \ours across multiple relative event understanding tasks. Our effectiveness in mitigating temporal inconsistencies is further demonstrated by our inconsistency metric.

\input{tables/contrast_examples}