\section{Related Works}
\paragraph{Score Approximation and Distribution Estimation}

Recently, some works analyze the score approximation theory via deep neural networks and corresponding sample complexity bounds for diffusion models.
\citet{oko2023diffusion} considers distributions with density in Besov space and supported on bounded domain.
\citet{chen2023score} assumes the data distribution lies in a low-dimensional linear subspace and obtains improved rates only depending on intrinsic dimension.
\citet{fu2024unveil} studies conditional diffusion models for H\"older densities and \citet{hu2024statistical} further extends the framework to more advanced neural network architectures, \textit{e.g.}, diffusion transformers.
\citet{wibisono2024optimal} establishes a minimax optimal rate to estimate Lipschitz score by kernel methods.
With an $L^2$ accurate score estimator, several works provide the convergence rate of discrete samplers for diffusion models \citep{chen2022sampling,chen2023improved,lee2023convergence,chen2024probability}. 
Combining score matching error and convergence of samplers, one can obtain an end-to-end distribution estimation error bound.

\paragraph{Transfer Learning and Meta-learning Theory in Supervised Learning}

The remarkable empirical success of transfer learning, meta-learning, and multi-task learning across a wide range of machine learning applications has been accompanied by gradual progress in their theoretical foundations, especially from
the perspective of representation learning.
To the best of our knowledge, \citet{baxter2000model} is the first theoretical work on meta-learning.
It assumes a universal \textit{environment} to generate tasks with some shared features.
Following this setting, \citet{maurer2016benefit} provides sample complexity bound for general supervised learning problem and \citet{aliakbarpour2024metalearning} studies very few samples per task regime. 
Another line of research replaces the \textit{environment} assumption and instead establishes connections between source tasks and target tasks through various notions of task diversity \citep{tripuraneni2020theory,du2020few,tripuraneni2021provable,watkins2023optimistic,chua2021fine}.
However, theoretical understandings of transfer learning for unsupervised learning are much more limited.

\paragraph{Few-shot fine-tuning of Diffusion Models}

Adapting pre-trained conditional diffusion models to specific tasks with limited data remains a challenge in varied application scenarios.
Few-shot fine-tuning aims to bridge this gap by leveraging various techniques to adapt those models to a novel task with minimal data requirements \citep{ruiz2023dreambooth,giannone2022few}.
A promising paradigm is to use transfer (meta) learning by constructing a representation for conditions in all the tasks, which has been widely applied in image generation \citep{rombach2022high,ramesh2022hierarchical,sinha2021d2c}, reinforcement learning \citep{he2023diffusion,ni2023metadiffuser}, inverse problem \citep{tewari2023diffusion,chung2023solving}, \textit{etc}.
Another recent work \citet{yang2024fewshot} is closely related to this paper, proving that few-shot diffusion models can escape the curse of dimensionality by fine-tuning a linear encoder.