\section{Related Works}
\paragraph{Score Approximation and Distribution Estimation}

Recently, some works analyze the score approximation theory via deep neural networks and corresponding sample complexity bounds for diffusion models.
____ considers distributions with density in Besov space and supported on bounded domain.
____ assumes the data distribution lies in a low-dimensional linear subspace and obtains improved rates only depending on intrinsic dimension.
____ studies conditional diffusion models for H\"older densities and ____ further extends the framework to more advanced neural network architectures, \textit{e.g.}, diffusion transformers.
____ establishes a minimax optimal rate to estimate Lipschitz score by kernel methods.
With an $L^2$ accurate score estimator, several works provide the convergence rate of discrete samplers for diffusion models ____. 
Combining score matching error and convergence of samplers, one can obtain an end-to-end distribution estimation error bound.

\paragraph{Transfer Learning and Meta-learning Theory in Supervised Learning}

The remarkable empirical success of transfer learning, meta-learning, and multi-task learning across a wide range of machine learning applications has been accompanied by gradual progress in their theoretical foundations, especially from
the perspective of representation learning.
To the best of our knowledge, ____ is the first theoretical work on meta-learning.
It assumes a universal \textit{environment} to generate tasks with some shared features.
Following this setting, ____ provides sample complexity bound for general supervised learning problem and ____ studies very few samples per task regime. 
Another line of research replaces the \textit{environment} assumption and instead establishes connections between source tasks and target tasks through various notions of task diversity ____.
However, theoretical understandings of transfer learning for unsupervised learning are much more limited.

\paragraph{Few-shot fine-tuning of Diffusion Models}

Adapting pre-trained conditional diffusion models to specific tasks with limited data remains a challenge in varied application scenarios.
Few-shot fine-tuning aims to bridge this gap by leveraging various techniques to adapt those models to a novel task with minimal data requirements ____.
A promising paradigm is to use transfer (meta) learning by constructing a representation for conditions in all the tasks, which has been widely applied in image generation ____, reinforcement learning ____, inverse problem ____, \textit{etc}.
Another recent work ____ is closely related to this paper, proving that few-shot diffusion models can escape the curse of dimensionality by fine-tuning a linear encoder.