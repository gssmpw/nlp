@inproceedings{aliakbarpour2024metalearning,
  title={Metalearning with very few samples per task},
  author={Aliakbarpour, Maryam and Bairaktari, Konstantina and Brown, Gavin and Smith, Adam and Srebro, Nathan and Ullman, Jonathan},
  booktitle={The Thirty Seventh Annual Conference on Learning Theory},
  pages={46--93},
  year={2024},
  organization={PMLR}
}

@article{baxter2000model,
  title={A model of inductive bias learning},
  author={Baxter, Jonathan},
  journal={Journal of artificial intelligence research},
  volume={12},
  pages={149--198},
  year={2000}
}

@article{chen2022sampling,
  title={Sampling is as easy as learning the score: theory for diffusion models with minimal data assumptions},
  author={Chen, Sitan and Chewi, Sinho and Li, Jerry and Li, Yuanzhi and Salim, Adil and Zhang, Anru R},
  journal={arXiv preprint arXiv:2209.11215},
  year={2022}
}

@inproceedings{chen2023improved,
  title={Improved analysis of score-based generative modeling: User-friendly bounds under minimal smoothness assumptions},
  author={Chen, Hongrui and Lee, Holden and Lu, Jianfeng},
  booktitle={International Conference on Machine Learning},
  pages={4735--4763},
  year={2023},
  organization={PMLR}
}

@inproceedings{chen2023score,
  title={Score approximation, estimation and distribution recovery of diffusion models on low-dimensional data},
  author={Chen, Minshuo and Huang, Kaixuan and Zhao, Tuo and Wang, Mengdi},
  booktitle={International Conference on Machine Learning},
  pages={4672--4712},
  year={2023},
  organization={PMLR}
}

@article{chen2024probability,
  title={The probability flow ode is provably fast},
  author={Chen, Sitan and Chewi, Sinho and Lee, Holden and Li, Yuanzhi and Lu, Jianfeng and Salim, Adil},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2024}
}

@article{chua2021fine,
  title={How fine-tuning allows for effective meta-learning},
  author={Chua, Kurtland and Lei, Qi and Lee, Jason D},
  journal={Advances in Neural Information Processing Systems},
  volume={34},
  pages={8871--8884},
  year={2021}
}

@inproceedings{chung2023solving,
  title={Solving 3d inverse problems using pre-trained 2d diffusion models},
  author={Chung, Hyungjin and Ryu, Dohoon and McCann, Michael T and Klasky, Marc L and Ye, Jong Chul},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={22542--22551},
  year={2023}
}

@article{du2020few,
  title={Few-shot learning via learning the representation, provably},
  author={Du, Simon S and Hu, Wei and Kakade, Sham M and Lee, Jason D and Lei, Qi},
  journal={arXiv preprint arXiv:2002.09434},
  year={2020}
}

@article{fu2024unveil,
  title={Unveil conditional diffusion models with classifier-free guidance: A sharp statistical theory},
  author={Fu, Hengyu and Yang, Zhuoran and Wang, Mengdi and Chen, Minshuo},
  journal={arXiv preprint arXiv:2403.11968},
  year={2024}
}

@article{giannone2022few,
  title={Few-shot diffusion models},
  author={Giannone, Giorgio and Nielsen, Didrik and Winther, Ole},
  journal={arXiv preprint arXiv:2205.15463},
  year={2022}
}

@article{he2023diffusion,
  title={Diffusion model is an effective planner and data synthesizer for multi-task reinforcement learning},
  author={He, Haoran and Bai, Chenjia and Xu, Kang and Yang, Zhuoran and Zhang, Weinan and Wang, Dong and Zhao, Bin and Li, Xuelong},
  journal={Advances in neural information processing systems},
  volume={36},
  pages={64896--64917},
  year={2023}
}

@article{hu2024statistical,
  title={On statistical rates of conditional diffusion transformers: Approximation, estimation and minimax optimality},
  author={Hu, Jerry Yao-Chieh and Wu, Weimin and Lee, Yi-Chen and Huang, Yu-Chao and Chen, Minshuo and Liu, Han},
  journal={arXiv preprint arXiv:2411.17522},
  year={2024}
}

@inproceedings{lee2023convergence,
  title={Convergence of score-based generative modeling for general data distributions},
  author={Lee, Holden and Lu, Jianfeng and Tan, Yixin},
  booktitle={International Conference on Algorithmic Learning Theory},
  pages={946--985},
  year={2023},
  organization={PMLR}
}

@article{maurer2016benefit,
  title={The benefit of multitask representation learning},
  author={Maurer, Andreas and Pontil, Massimiliano and Romera-Paredes, Bernardino},
  journal={Journal of Machine Learning Research},
  volume={17},
  number={81},
  pages={1--32},
  year={2016}
}

@inproceedings{ni2023metadiffuser,
  title={Metadiffuser: Diffusion model as conditional planner for offline meta-rl},
  author={Ni, Fei and Hao, Jianye and Mu, Yao and Yuan, Yifu and Zheng, Yan and Wang, Bin and Liang, Zhixuan},
  booktitle={International Conference on Machine Learning},
  pages={26087--26105},
  year={2023},
  organization={PMLR}
}

@inproceedings{oko2023diffusion,
  title={Diffusion models are minimax optimal distribution estimators},
  author={Oko, Kazusato and Akiyama, Shunta and Suzuki, Taiji},
  booktitle={International Conference on Machine Learning},
  pages={26517--26582},
  year={2023},
  organization={PMLR}
}

@article{ramesh2022hierarchical,
  title={Hierarchical text-conditional image generation with clip latents},
  author={Ramesh, Aditya and Dhariwal, Prafulla and Nichol, Alex and Chu, Casey and Chen, Mark},
  journal={arXiv preprint arXiv:2204.06125},
  volume={1},
  number={2},
  pages={3},
  year={2022}
}

@inproceedings{rombach2022high,
  title={High-resolution image synthesis with latent diffusion models},
  author={Rombach, Robin and Blattmann, Andreas and Lorenz, Dominik and Esser, Patrick and Ommer, Bj{\"o}rn},
  booktitle={Proceedings of the IEEE/CVF conference on computer vision and pattern recognition},
  pages={10684--10695},
  year={2022}
}

@inproceedings{ruiz2023dreambooth,
  title={Dreambooth: Fine tuning text-to-image diffusion models for subject-driven generation},
  author={Ruiz, Nataniel and Li, Yuanzhen and Jampani, Varun and Pritch, Yael and Rubinstein, Michael and Aberman, Kfir},
  booktitle={Proceedings of the IEEE/CVF conference on computer vision and pattern recognition},
  pages={22500--22510},
  year={2023}
}

@article{sinha2021d2c,
  title={D2c: Diffusion-decoding models for few-shot conditional generation},
  author={Sinha, Abhishek and Song, Jiaming and Meng, Chenlin and Ermon, Stefano},
  journal={Advances in Neural Information Processing Systems},
  volume={34},
  pages={12533--12548},
  year={2021}
}

@article{tewari2023diffusion,
  title={Diffusion with forward models: Solving stochastic inverse problems without direct supervision},
  author={Tewari, Ayush and Yin, Tianwei and Cazenavette, George and Rezchikov, Semon and Tenenbaum, Josh and Durand, Fr{\'e}do and Freeman, Bill and Sitzmann, Vincent},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  pages={12349--12362},
  year={2023}
}

@article{tripuraneni2020theory,
  title={On the theory of transfer learning: The importance of task diversity},
  author={Tripuraneni, Nilesh and Jordan, Michael and Jin, Chi},
  journal={Advances in neural information processing systems},
  volume={33},
  pages={7852--7862},
  year={2020}
}

@inproceedings{tripuraneni2021provable,
  title={Provable meta-learning of linear representations},
  author={Tripuraneni, Nilesh and Jin, Chi and Jordan, Michael},
  booktitle={International Conference on Machine Learning},
  pages={10434--10443},
  year={2021},
  organization={PMLR}
}

@article{watkins2023optimistic,
  title={Optimistic rates for multi-task representation learning},
  author={Watkins, Austin and Ullah, Enayat and Nguyen-Tang, Thanh and Arora, Raman},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  pages={2207--2251},
  year={2023}
}

@article{wibisono2024optimal,
  title={Optimal score estimation via empirical bayes smoothing},
  author={Wibisono, Andre and Wu, Yihong and Yang, Kaylee Yingxi},
  journal={arXiv preprint arXiv:2402.07747},
  year={2024}
}

