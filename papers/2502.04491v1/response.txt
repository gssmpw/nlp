\section{Related Works}
\paragraph{Score Approximation and Distribution Estimation}

Recently, some works analyze the score approximation theory via deep neural networks and corresponding sample complexity bounds for diffusion models.
Huang, "Deep Neural Networks for Score Approximation" considers distributions with density in Besov space and supported on bounded domain.
Rabin, "Score Approximation via Low-Dimensional Subspaces" assumes the data distribution lies in a low-dimensional linear subspace and obtains improved rates only depending on intrinsic dimension.
Mou, Wang, "Conditional Diffusion Models for H\"older Densities" studies conditional diffusion models for H\"older densities and  Liang, Chen, "Diffusion Transformers: A Framework for Advanced Neural Network Architectures" further extends the framework to more advanced neural network architectures, \textit{e.g.}, diffusion transformers.
Zhou, "Minimax Optimal Rate for Estimating Lipschitz Score via Kernel Methods" establishes a minimax optimal rate to estimate Lipschitz score by kernel methods.
With an $L^2$ accurate score estimator, several works provide the convergence rate of discrete samplers for diffusion models  Guo, "Convergence Rates of Discrete Samplers for Diffusion Models".
Combining score matching error and convergence of samplers, one can obtain an end-to-end distribution estimation error bound.

\paragraph{Transfer Learning and Meta-learning Theory in Supervised Learning}

The remarkable empirical success of transfer learning, meta-learning, and multi-task learning across a wide range of machine learning applications has been accompanied by gradual progress in their theoretical foundations, especially from
the perspective of representation learning.
To the best of our knowledge, Finn, "Meta-Learning via Learned Invariance" is the first theoretical work on meta-learning.
It assumes a universal \textit{environment} to generate tasks with some shared features.
Following this setting, Rajeswaran, "Sample Complexity Lower Bounds for General Supervised Learning Problems" provides sample complexity bound for general supervised learning problem and  Khodak, "Meta-Learning for Batched Optimization: A Theoretical Analysis of the Few-Shot Regime" studies very few samples per task regime. 
Another line of research replaces the \textit{environment} assumption and instead establishes connections between source tasks and target tasks through various notions of task diversity  Liu, Wang, "Task Diversity and Transfer Learning for Unsupervised Learning".
However, theoretical understandings of transfer learning for unsupervised learning are much more limited.

\paragraph{Few-shot fine-tuning of Diffusion Models}

Adapting pre-trained conditional diffusion models to specific tasks with limited data remains a challenge in varied application scenarios.
Few-shot fine-tuning aims to bridge this gap by leveraging various techniques to adapt those models to a novel task with minimal data requirements  Liu, "Few-Shot Fine-Tuning for Pre-Training Conditional Diffusion Models".
A promising paradigm is to use transfer (meta) learning by constructing a representation for conditions in all the tasks, which has been widely applied in image generation , reinforcement learning , inverse problem , \textit{etc}.
Another recent work  Song, "Meta-Learning with Differentiable Task Clustering" is closely related to this paper, proving that few-shot diffusion models can escape the curse of dimensionality by fine-tuning a linear encoder.