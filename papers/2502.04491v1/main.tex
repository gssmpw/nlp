%
\documentclass[11pt]{article}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% geometry
\usepackage[left=1in,top=1in,right=1in,bottom=1in,letterpaper]{geometry}
% \renewcommand{\baselinestretch}{1.5}

\usepackage[colorlinks, linkcolor=red,filecolor=blue,citecolor=blue,urlcolor=blue]{hyperref}
\usepackage{url}
\usepackage{amsthm,amsmath,amssymb, amscd}
\usepackage{mathrsfs}
\usepackage{amsfonts}
\usepackage[utf8]{inputenc}
\usepackage{subfigure}
\usepackage{graphicx}
\usepackage{bbding}
\everymath{\displaystyle}
\usepackage{indentfirst} 
\usepackage{enumerate}
\usepackage{bm,bbm}
\usepackage{enumitem}
\usepackage{algorithm, algorithmic}
\usepackage{dsfont}
\usepackage{booktabs}
\usepackage{xspace}
\usepackage{pifont}
\usepackage{xcolor}
\PassOptionsToPackage{compress, square}{natbib}
\usepackage{natbib}
\numberwithin{equation}{section}
\usepackage{apptools}
\AtAppendix{\counterwithin{thm}{section}}

\input{math_commands}
\input{def}

\newcommand{\Pmeta}{\mathbb{P}_{\text{meta}}}
\renewcommand{\P}{\mathbb{P}}
\renewcommand{\eqref}[1]{(\ref{#1})}

\title{Provable Sample-Efficient Transfer Learning Conditional Diffusion Models via Representation Learning}

\author{
Ziheng Cheng\thanks{University of California, Berkeley. Email: \texttt{ziheng\_cheng@berkeley.edu}}
\and Tianyu Xie\thanks{Peking University. Email: \texttt{tianyuxie@pku.edu.cn}}
\and Shiyue Zhang\thanks{Peking University. Email: \texttt{zhangshiyue@stu.pku.edu.cn}}
\and Cheng Zhang \thanks{Peking University. Email: \texttt{chengzhang@math.pku.edu.cn}}
} 

\date{}


\begin{document}

\maketitle

\begin{abstract}
    While conditional diffusion models have achieved remarkable success in various applications, they require abundant data to train from scratch, which is often infeasible in practice.
    To address this issue, transfer learning has emerged as an essential paradigm in small data regimes.
    Despite its empirical success, the theoretical underpinnings of transfer learning conditional diffusion models remain unexplored.
    In this paper, we take the first step towards understanding the sample efficiency of transfer learning conditional diffusion models through the lens of representation learning.
    Inspired by practical training procedures, we assume that there exists a low-dimensional representation of conditions shared across all tasks.
    Our analysis shows that with a well-learned representation from source tasks, the sample
    complexity of target tasks can be reduced substantially.
    In addition, we investigate the practical implications of our theoretical results in several real-world applications of conditional diffusion models.
    Numerical experiments are also conducted to verify our results.
\end{abstract}

\section{Introduction}

Conditional diffusion models (CDMs) utilize a user-defined condition to guide the generative process of diffusion models (DMs) to sample from the desired conditional distribution.
In recent years, CDMs have achieved groundbreaking success in various generative tasks, including text-to-image generation \citep{ho2020denoising,song2020score,ho2022classifier,rombach2022high}, reinforcement learning \citep{janner2022planning,chi2023diffusion,wang2022diffusion,reuss2023goal}, time series \citep{tashiro2021csdi,rasul2021autoregressive}, and life science \citep{song2021solving,watson2022broadly,gruver2024protein,guo2024diffusion}. 

Training a CDM from scratch requires a large amount of data to achieve good generalization.
However, in practical scenarios, users often have access to only limited data for the target distribution due to cost or risk concerns, making the model prone to over-fitting. 
In such small data regime, transfer learning has emerged as a predominant paradigm \citep{moon2022fine,ruiz2023dreambooth,xie2023difffit,han2023svdiff}.
By leveraging knowledge acquired during pre-training on large source datasets, transfer learning enhances the performance of fine-tuning on target tasks, facilitating few-shot learning and significantly improving practicality.

Among the successful applications of transfer learning CDMs, the conditions are typically high-dimensional vectors with embedded low-dimensional representations (features) that encapsulate all the information required for inference. 
In addition, these representations are likely to be task-agnostic, enabling effective knowledge transfer. 
% For example, in reinforcement learning, CDMs are able to generate actions given current states, which may include high-resolution visual observations.
% These observations often contain underlying low-dimensional structures, such as object positions or scene dynamics, despite the differences of objectives and environments in different tasks. 
For example, in text-to-image generation, the text input is inherently in high-dimensional space, but contains low-dimensional semantic information such as object attributes, spatial relationships, despite the differences of styles or contents in different image distributions.
To take advantage of this structure, condition encoders are often frozen in the fine-tuning stage \citep{rombach2022high,esser2024scaling}, which typically constitutes a significant portion of the overall model (see Table \ref{tab:param}).
\begin{table}[ht]
    \centering
    \resizebox{\linewidth}{!}{
        \begin{tabular}{c|c|c}
            \toprule
            Tasks & Backbone Score Network & Condition Encoder \\
            \midrule
            Text-to-Image \citep{esser2024scaling} & 2-8B & 4.7B \\
            \midrule
            Text-to-Audio \citep{liu2024audioldm} & 350-750M & 750M \\
            \midrule
            Reinforcement Learning \citep{chi2023diffusion} & 9M & 20-45M \\
            \bottomrule
        \end{tabular}
    }
    \caption{Comparing the number of parameters of different parts in CDMs.}
    \label{tab:param}
\end{table}

While this paradigm has demonstrated remarkable empirical success, its theoretical underpinnings remain largely unexplored.
The following fundamental question is still open:
\begin{center}
    \textit{Can transfer learning CDMs improve the sample efficiency of target tasks by leveraging the representation of conditions learned from source tasks?}
\end{center}
There are some recent works attempting to study the theoretical underpinnings of CDMs \citep{fu2024unveil,jiao2024model,hu2024statistical}, but focus on single task training.
Notably, \citet{yang2024fewshot} investigates transfer learning DMs under the assumption that the data is a linear transformation of a low-dimensional latent variable following the same distribution across all tasks.
However, fine-tuning merely the data encoder is not a widely adopted training approach in practice.

In this paper, we take the first step towards addressing the above question.
Our key assumption is that there exists a generic low-dimensional representation of conditions shared across all distributions.
Then we show that, with a well-learned representation from source tasks, the sample complexity of target tasks can be reduced substantially by training only the score network.  
The main contributions are summarized as follows:
\begin{itemize}
    \item In Section \ref{sec:generalization}, we establish the first generalization guarantee for transferring score matching error in CDMs, showing that transfer learning can reduce the sample complexity for learning condition encoder in the target task.
    This is aligned with existing transfer learning theory in supervised learning.
    Specifically, we present two results in Theorem \ref{thm:generalization_all_diversity_informal} and Theorem \ref{thm:generalization_all_informal}, under the settings of task diversity assumption and meta-learning\footnote{In practice, the terms such as transfer learning, meta-learning, learning-to-learn, \textit{etc.}, often refer to the same training paradigm, \textit{i.e.}, to fine-tune on target tasks with limited data using knowledge from source tasks. However, in the theoretical framework, we use the term meta-learning to emphasize that target tasks and source tasks are randomly sampled from a meta distribution \citep{baxter2000model}, whereas in transfer learning, the tasks are fixed.}, respectively.
    On the technical side, we develop a novel approach to tackle Lipschitz continuity under weaker assumptions on data distribution in Lemma \ref{lem:lip_score_informal}, which may be of independent interest for the analysis of even single-task diffusion models.
    \item In Section \ref{sec:dist_estimation}, we provide an end-to-end distribution estimation error bound in transfer learning CDMs.
    To obtain an $L^2$ accurate conditional score estimator, we construct a universal approximation theory using deep ReLU neural networks in Theorem \ref{thm:approximation_all_informal}. 
    Then by combining both generalization error and approximation error, Theorem \ref{thm:distribution_diversity_informal} and \ref{thm:distribution_informal} provide sample complexity bounds for estimating conditional distribution. 
    Notably, our results are \textit{the state of the art} even when reduced to single-task learning setting. 
    \item In Section \ref{sec:application}, we further utilize our results to establish statistical guarantees in practical applications of CDMs.
    In particular, we investigate amortized variational inference (Theorem \ref{thm:amortized_vi}) and behavior cloning (Theorem \ref{thm:behavior_cloning}), and present guarantees in terms of posterior estimation and optimality gap, laying the theoretical foundations of transfer learning CDMs in practice. We also conduct numerical experiments in Section \ref{sec:exp} to verify our results.
\end{itemize}

\subsection{Related Works}

\paragraph{Score Approximation and Distribution Estimation}

Recently, some works analyze the score approximation theory via deep neural networks and corresponding sample complexity bounds for diffusion models.
\citet{oko2023diffusion} considers distributions with density in Besov space and supported on bounded domain.
\citet{chen2023score} assumes the data distribution lies in a low-dimensional linear subspace and obtains improved rates only depending on intrinsic dimension.
\citet{fu2024unveil} studies conditional diffusion models for H\"older densities and \citet{hu2024statistical} further extends the framework to more advanced neural network architectures, \textit{e.g.}, diffusion transformers.
\citet{wibisono2024optimal} establishes a minimax optimal rate to estimate Lipschitz score by kernel methods.
With an $L^2$ accurate score estimator, several works provide the convergence rate of discrete samplers for diffusion models \citep{chen2022sampling,chen2023improved,lee2023convergence,chen2024probability}. 
Combining score matching error and convergence of samplers, one can obtain an end-to-end distribution estimation error bound.

\paragraph{Transfer Learning and Meta-learning Theory in Supervised Learning}

The remarkable empirical success of transfer learning, meta-learning, and multi-task learning across a wide range of machine learning applications has been accompanied by gradual progress in their theoretical foundations, especially from
the perspective of representation learning.
To the best of our knowledge, \citet{baxter2000model} is the first theoretical work on meta-learning.
It assumes a universal \textit{environment} to generate tasks with some shared features.
Following this setting, \citet{maurer2016benefit} provides sample complexity bound for general supervised learning problem and \citet{aliakbarpour2024metalearning} studies very few samples per task regime. 
Another line of research replaces the \textit{environment} assumption and instead establishes connections between source tasks and target tasks through various notions of task diversity \citep{tripuraneni2020theory,du2020few,tripuraneni2021provable,watkins2023optimistic,chua2021fine}.
However, theoretical understandings of transfer learning for unsupervised learning are much more limited.

\paragraph{Few-shot fine-tuning of Diffusion Models}

Adapting pre-trained conditional diffusion models to specific tasks with limited data remains a challenge in varied application scenarios.
Few-shot fine-tuning aims to bridge this gap by leveraging various techniques to adapt those models to a novel task with minimal data requirements \citep{ruiz2023dreambooth,giannone2022few}.
A promising paradigm is to use transfer (meta) learning by constructing a representation for conditions in all the tasks, which has been widely applied in image generation \citep{rombach2022high,ramesh2022hierarchical,sinha2021d2c}, reinforcement learning \citep{he2023diffusion,ni2023metadiffuser}, inverse problem \citep{tewari2023diffusion,chung2023solving}, \textit{etc}.
Another recent work \citet{yang2024fewshot} is closely related to this paper, proving that few-shot diffusion models can escape the curse of dimensionality by fine-tuning a linear encoder.  


\section{Preliminaries and Problem Setup}

\paragraph{Notations}
We use $x$ and $y$ to denote the data and conditions, respectively.
The blackboard bold letter $\P$ represents the joint distribution of $(x,y)$, while the lowercase $p$ denotes its density function.
The superscript $k$ indicates the task index, and the subscript $i$ means the sample index.
The norm $\|\cdot\|$ refers to the $\ell_2$-norm  for vectors and the spectral norm for matrices.
For the hypothesis class $\mathcal{F}$, we
use $\mathcal{F}^{\otimes K}$ to refer its $K$-fold Cartesian product.
For any $a,b\in\R$, $a\wedge b=\min\{a,b\}$ and $a\vee b=\max\{a,b\}$.
Finally, we use standard $\mathcal{O}(\cdot), \Omega(\cdot)$ to omit constant
factors.

\subsection{Conditional Diffusion Models}

Let $\R^{d_x}$ denote the data space and $[0,1]^{D_y}$ denote the condition space.
Let $\P$ be any joint distribution over $\R^{d_x}\times[0,1]^{D_y}$ with density $p$ and $\P(\cdot|y)$ be the conditional distribution with density $p(\cdot|y)$.
As in diffusion models, the forward process is defined as an Ornstein–Uhlenbeck (OU) process,
\begin{equation}\label{eq:ou_process}
    \dif X_t = -X_t\dif t + \sqrt{2}\dif W_t, X_0\sim \P(\cdot|y).
\end{equation}
where $\{W_t\}_{t\geq 0}$ is a standard Wiener process. We denote the distribution of $X_t$ as $\P_t(\cdot|y)$.
Note that the limiting distribution $\P_\infty(\cdot|y)$ is a standard Gaussian $\mathcal{N}(0,I)$.

To generate new samples, we can reverse the forward process \eqref{eq:ou_process} from any $T>0$,
\begin{equation}
    \dif X_t^{\leftarrow} = (X_t^{\leftarrow} + 2\nabla\log p_{T-t}(X_t^{\leftarrow}|y))\dif t + \sqrt{2}\dif \overline{W}_t, X_0^\leftarrow\sim \P_T(\cdot|y), 0\leq t\leq T.
\end{equation}
where $\{\overline{W}_t\}_{0\leq t\leq T}$ is a time-reversed Wiener process. Unfortunately, we don't have access to the exact conditional score function $\nabla\log p_{T-t}$ and need to estimate it through neural networks.
For any $(x,y)\sim\P$ and score estimator $s$, define the individual denoising score matching objective \citep{vincent2011connection} as
\begin{equation}\label{eq:dsm}
    \ell(x,y,s):=\frac{1}{T-T_0}\int_{T_0}^T \E_{x_t\sim \phi_t(\cdot|x)}\big[\|s(x_t,y,t)-\nabla\log \phi_t(x_t|x)\|^2\big] \dif t,
\end{equation}
where $\phi_t(x_t|x)=\mathcal{N}(x_t|\alpha_tx,\sigma_t^2I),\alpha_t=e^{-t},\sigma_t^2=1-e^{-2t}$, is the transition kernel of $x_t|x_0=x$. 
And the population error of score matching is
\begin{equation}\label{eq:sm}
    L^\P(s):=\E_{(x,y)\sim\P}\E_{t,x_t}[\|s(x_t,y,t)-\nabla\log p_t(x_t|y)\|^2]=\E_{(x,y)\sim\P}[\ell(x,y,s)-\ell(x,y,s^\P_*)].
\end{equation}
Here $s^\P_*$ denotes the true score function and $t\sim \text{Unif}([T_0,T])$.
We also define $\ell^\P(x,y,s):=\ell(x,y,s)-\ell(x,y,s_*^\P)$.
In practice, with a score estimator $\widehat{s}$, the generative process is to simulate
\begin{equation}
    \dif \widehat{X}_t^{\leftarrow} = (\widehat{X}_t^{\leftarrow} + 2\widehat{s}(\widehat{X}_t^\leftarrow,y,T-t))\dif t + \sqrt{2}\dif \overline{W}_t, \widehat{X}_0^\leftarrow\sim \mathcal{N}(0,I), 0\leq t\leq T-T_0.
\end{equation}
Here $T_0>0$ is the early-stopping time.
And the distribution of $\widehat{X}_{T-T_0}^\leftarrow$ is written as $\widehat{\P}(\cdot|y)$.

Note that we don't apply the commonly used classifier-free guidance \citep{ho2022classifier} which has a tunable guidance strength since we mainly concentrate on sampling from conditional distribution instead of optimizing other objectives.

\subsection{Transfer Diffusion Models via Learning Representation}

Consider $K$ source distributions over $\R^{d_x}\times[0,1]^{D_y}$, $\P^{1},\cdots,\P^K$, and a target distribution $\P^{0}$.
Suppose that for each source distribution $\P^k,1\leq k\leq K$, we have $n$ \textit{i.i.d.} samples $\{(x_i^k,y_i^k)\}_{i=1}^n\sim\P^k$, and $m$ \textit{i.i.d.} samples $\{(x_i^0,y_i^0)\}_{i=1}^m\sim\P^0$ are available for the target distribution, where typically $m\ll n$.
In transfer (meta) learning setup, we assume there exists a shared nonlinear representation of the condition $y$ for all distributions, \textit{i.e.}, the conditional distribution $\P^k_{x|y}=\P^k_{x|h_*(y)}$ for some $h_*:[0,1]^{D_y}\to[0,1]^{d_y}$ (see also Assumption \ref{asp:low_dim}).
Note that due to the shared features, the score of $p_t^{k}(\cdot|y)$ also has the form of $\nabla\log p_t^{k}(x_t|y)=f_*^{k}(x_t, h_*(y), t)$ for some $f_*^{k}$.

Similar to \citet{tripuraneni2020theory}, our transfer learning procedures consist of two phases.
In the pre-training phase, the goal is to learn a representation map $h_*$ through $nK$ samples from $K$ source distributions.
Then during the fine-tuning phase, we learn the target distribution via $m$ new samples and the representation map learned in the pre-training phase.

Formally, let $\mathcal{F},\mathcal{H}$ be the hypothesis classes of score networks and representation maps, respectively.
Further let $\mathcal{F}^0\subseteq\mathcal{F}$ be the hypothesis class of score network in fine-tuning phase.
In the pre-training phase, we solve the following Empirical Risk Minimization (ERM),
\begin{equation}\label{eq:pre-train}
    \widehat{\vf},\widehat{h}=\argmin_{\vf\in\mathcal{F}^{\otimes K},h\in\mathcal{H}} \frac{1}{nK}\sum_{k=1}^K\sum_{i=1}^n\ell(x_i^k,y_i^k,s_{f^k,h}).
\end{equation}
Then for the fine-tuning task, we solve
\begin{equation}\label{eq:fine-tune}
    \widehat{f}^0:= \argmin_{f\in\mathcal{F}^0} \frac{1}{m}\sum_{i=1}^m \ell(x_i^0,y_i^0,s_{f,\widehat{h}}).
\end{equation}
Here $s_{f,h}(x,y,t):=f(x,h(y),t)$ for $f:\R^{d_x}\times[0,1]^{d_y}\times[T_0,T]\to\R^{d_x}$ and $h:[0,1]^{D_y}\to[0,1]^{d_y}$ and $\ell$ is defined in \eqref{eq:dsm}.

In the meta-learning setting, we further assume that all the distributions $\{\P^k\}_k$ are \textit{i.i.d.} sampled from a meta distribution $\Pmeta$. 
Here $\Pmeta$ can be interpreted as a universal \textit{environment} \citep{baxter2000model,maurer2016benefit}.
In this case, we posit the existence of a shared representation map that holds for all $\P\sim\Pmeta$.
And the performance benchmark is then defined as the expected error on the target distribution $\P^0\sim\Pmeta$.

\subsection{Deep ReLU Neural Network Family}

We use feedforward neural networks to approximate the score function and representation map.
Let $\sigma(x):=\max\{x,0\}$ be the ReLU activation.
Define the neural network family 
\begin{equation}
    \begin{aligned}
        NN_f(L,W,M,S,B,R,\gamma):=
        &\Bigg\{f(x,w,t)=(A_L\sigma(\cdot)+b_L)\circ\cdots\circ(A_1[x^\top,w^\top,t]^\top+b_1): \\ 
        &\quad A_i\in\R^{d_i\times d_{i+1}}, b_i\in\R^{d_{i+1}}, d_{L+1}=d_x,\max d_i\leq W, \|f\|_{L^\infty}\leq M, \\
        &\quad \sum_{i=1}^L (\|A_i\|_0+\|b_i\|_0)\leq S, \max\|A_i\|_\infty\vee\|b_i\|_\infty\leq B, \\
        &\quad \|f(x,w,t)-f(x,w',t)\|\leq \gamma\|w-w'\|_\infty, \forall\ \|x\|_\infty\leq R,t\leq T \Bigg\},
    \end{aligned}
\end{equation}
\begin{equation}
    \begin{aligned}
        NN_h(L,W,S,B):=
        &\Bigg\{h(y)=(A_L\sigma(\cdot)+b_L)\circ\cdots\circ(A_1y+b_1): A_i\in\R^{d_i\times d_{i+1}}, \\ 
        &\quad  b_i\in\R^{d_{i+1}}, d_{L+1}=d_y,\max d_i\leq W, \|h\|_{L^\infty([0,1]^{D_y})}\leq 1, \\
        &\quad \sum_{i=1}^L (\|A_i\|_0+\|b_i\|_0)\leq S, \max\|A_i\|_\infty\vee\|b_i\|_\infty\leq B\Bigg\}.
    \end{aligned}
\end{equation}
Throughout this paper, we let $\mathcal{F}^0=\mathcal{F}=NN_f(L_f,W_f,M_f,S_f,B_f,R_f,\gamma_f)$ and $\mathcal{H}=NN_h(L_h,W_h,S_h,B_h)$ unless otherwise specified.
\begin{rmk}
    In practice, $\mathcal{F}^0\subseteq\mathcal{F}$ may (and typically will) depend on $\widehat{\vf}$ for parameter efficient fine-tuning (PEFT), \textit{e.g.}, LoRA \citep{hu2021lora}. This will substantially reduce the complexity of $\mathcal{F}^0$ and further improve sample efficiency.
    The analysis of PEFT is beyond the scope of this paper.
\end{rmk}


\section{Statistical Guarantees for Transferring Score Matching Error}\label{sec:generalization}

In this section, we present our main theoretical results, a statistical theory of transferring conditional score matching loss.
We provide two upper bounds of score matching loss on target distribution, based on whether task diversity  \citep{tripuraneni2020theory} is explicitly assumed.
Throughout this paper, we make the following standard and mild regularity assumptions on the initial data distribution $\P$ and the representation map $h_*$.

\begin{asp}[Sub-gaussian tail]\label{asp:sub_gaussian}
    For any source and target distribution $\P$, $\P$ is supported on $\R^{d_x}\times[0,1]^{D_y}$ and admits a continuous density $p(x,y)\in \mathcal{C}^2(\R^{d_x}\times[0,1]^{D_y})$. 
    Moreover, the conditional distribution $p(x|y)\leq C_1\exp(-C_2\|x\|^2)$ for some constant $C_1,C_2$.
\end{asp}

\begin{asp}[Shared low-dimensional representation]\label{asp:low_dim}
    There exists an $L$-Lipschitz function $h_*:[0,1]^{D_y}\rightarrow[0,1]^{d_y}$ with $d_y\leq D_y$, such that for any source and target distribution $\P$, the conditional density $p(x|y)=g_*^{\P}(x,h_*(y))$ for some $g_*^{\P}\in\mathcal{C}^2(\R^{d_x}\times[0,1]^{d_y})$.
\end{asp}

Equivalently,  $h_*(y)$ is a sufficient statistic for $x$, which indicates that $p_t(x|y)=p_t(x|h_*(y))$.
Therefore, with a little abuse of notation, for any $w\in[0,1]^{d_y}$, we define $p(x;w)=p(x|h_*(y)=w)=g_*^{\P}(x,w)$. 
Also note that by definition, for any $x,y$, we have $p(x;h_*(y))=p(x|h_*(y))=p(x|y)$.

\begin{asp}[Lipschitz score]\label{asp:lip}
    For any source and target distribution $\P$ and its density function $p$, the conditional score $\nabla_x\log p(x|y)=\nabla_x\log g_*^{\P}(x,h_*(y))$. The score function $\nabla_x\log g_*^{\P}(x,w)$ is $L$-Lipschitz in $x$ and $w$.
    And $\|\nabla_x\log g_*^{\P}(0,w)\|\leq B$ for some constant $B$ and any $w$.
\end{asp}

\subsection{Tackling Lipschitz Continuity under Weaker Assumptions}
Notice that we only impose smoothness assumption on the original data distribution $p(\cdot|y)$, instead of the entire trajectory $p_t(\cdot|y)$ in forward process. 
This is substantially weaker than the Lipschitzness assumption required in \citet{chen2023score,chen2022sampling,yuan2024reward,yang2024fewshot}. 
However, Lipschitzness of loss function $\ell$ and class $\mathcal{F}$ is a crucial hypothesis in theoretical analysis of transfer learning \citep{tripuraneni2020theory,chua2021fine}.
The intuition is that without Lipschitz continuity of the score network $f$, it is generally impossible to characterize the error from an imperfect representation map $h$.
Hence it is inevitable to show the smoothness of $p_t(\cdot|y)$ to some extent. 

Fortunately, even with assumptions merely on the initial data distribution, we are still able to prove smoothness of the forward process in any bounded region, as shown in the following lemma. The proof can be found in Appendix \ref{app:subsec:pre_generalization}.
\begin{lemma}\label{lem:lip_score_informal}
    Under Assumption \ref{asp:sub_gaussian}, \ref{asp:low_dim}, \ref{asp:lip}, for any $w\in[0,1]^{d_y}$, denote the conditional score of forward process $\nabla_x\log p_t(x;w)$ by $f_*(x,w,t)$.
    There exist constants $C_X,C_X'$, such that for any $t\in[0,T]$, the function $f_*(x,w,t)$ is $(C_X+C_X'\|x\|^2)$-Lipschitz in $x$, $(C_X+C_X'\|x\|)$-Lipschitz in $w$.
\end{lemma}

\subsection{Results under Task Diversity: Sample-Efficient Transfer Learning}

In the literature of transfer learning, task diversity is an important assumption that connects target tasks with source tasks \citep{tripuraneni2020theory,du2020few,chua2021fine}.
In the context of conditional diffusion models, we state the formal definition as follows.
\begin{definition}[Task diversity]\label{def:diversity}
    Given hypothesis classes $\mathcal{F},\mathcal{H}$, we say the source distributions $\P^1,\cdots,\P^K$ are $(\nu,\Delta)$-diverse over target distribution $\P^{0}$, if for any representation $h\in\mathcal{H}$,
    \begin{equation}
        \inf_{f^0\in\mathcal{F}^0}L^{\P^0}(s_{f^0,h})\leq \frac{1}{\nu}\inf_{\vf\in\mathcal{F}^{\otimes K}}\frac{1}{K}\sum_{k=1}^K L^{\P^k}(s_{f^{k},h}) + \Delta.
    \end{equation}
\end{definition}
Here $L^\P$ is defined in \eqref{eq:sm}.
This notion of diversity ensures that the representation error on the target task caused by $\widehat{h}$ can be controlled by the error on the source tasks, thereby establishing certain relationships in between.
More detailed discussions are deferred to Appendix \ref{app:subsec:verify_diversity}.

We first present the generalization guarantee for each phase respectively.
\begin{prop}[Fine-tuning phase generalization]\label{prop:generalization_test_informal}
    Under Assumption \ref{asp:sub_gaussian}, \ref{asp:low_dim}, \ref{asp:lip}, for any $\widehat{h}\in\mathcal{H}$, the population loss of $\widehat{f}^0$ can be bounded by
    \begin{equation}
        \E_{\{(x_i,y_i)\}_{i=1}^m\sim \P^0} \E_{(x,y)\sim\P^0} [\ell^{\P^0}(x,y,s_{\widehat{f}^0,\widehat{h}})]\lesssim \inf_{f\in\mathcal{F}}\E_{(x,y)\sim\P^0}[\ell^{\P^0}(x,y,s_{f,\widehat{h}})] + \log^3(m) r_x,
    \end{equation}
    where $r_x=\frac{\log\widetilde{\mathcal{N}}_\mathcal{F}}{m}$ and $\log\widetilde{\mathcal{N}}_\mathcal{F}$ is some complexity measures of $\mathcal{F}$.
\end{prop}

\begin{prop}[Pre-training phase generalization]\label{prop:generalization_train_informal}
    Under Assumption \ref{asp:sub_gaussian}, \ref{asp:low_dim}, \ref{asp:lip}, if $R_f\gtrsim\log^{\frac{1}{2}}(nKM_f/\delta)$, with probability no less than $1-\delta$,
    the population loss can be bounded by
    \begin{equation}
        \frac{1}{K}\sum_{k=1}^K\E_{(x,y)\sim\P^k}\ell^{\P^k}(x,y,s_{\widehat{f}^k,\widehat{h}})\lesssim \inf_{\vf\in\mathcal{F}^{\otimes K},h\in\mathcal{H}}\frac{1}{K}\sum_{k=1}^K\E_{(x,y)\sim\P^k}[\ell^\P(x,y,s_{f^k,h})] + \log^3(nK/\delta)\left(r_z+\frac{\log(1/\delta)}{nK}\right),
    \end{equation}
    where $r_z:=\frac{K\log\widetilde{\mathcal{N}}_\mathcal{F}+\log\widetilde{\mathcal{N}}_\mathcal{H}}{nK}$ and $\log\widetilde{\mathcal{N}}_\mathcal{F},\log\widetilde{\mathcal{N}}_\mathcal{H}$ are some complexity measures of $\mathcal{F},\mathcal{H}$.
\end{prop}
Combining these two propositions with the notion of task diversity in Definition \ref{def:diversity}, we are able to show the statistical rate of transfer learning as follows.
\begin{thm}\label{thm:generalization_all_diversity_informal}
    Under Assumption \ref{asp:sub_gaussian}, \ref{asp:low_dim}, \ref{asp:lip}, suppose $\P^1,\cdots,\P^K$ are $(\nu,\Delta)$-diverse over target distribution $\P^0$ given $\mathcal{F},\mathcal{H}$.
    If $R_f\gtrsim\log^{\frac{1}{2}}(nKM_f/\delta)$,
    then with probability no less than $1-\delta$, 
    \begin{equation}
        \begin{aligned}
            \E_{\{(x_i,y_i)\}_{i=1}^m} \E_{(x,y)\sim\P^0} [\ell^{\P^0}(x,y,s_{\widehat{f}^{0},\widehat{h}})]
            &\lesssim \frac{1}{\nu}\inf_{h\in\mathcal{H}} \frac{1}{K}\sum_{k=1}^K \inf_{f\in\mathcal{F}}\E_{(x,y)\sim\P^k} [\ell^{\P^k} (x,y,s_{f,h})] + \Delta \\
            &\quad +\frac{\log^3(m)\log\mathcal{N}_\mathcal{F}}{m} + \frac{\log^3(nK/\delta)(K\log\mathcal{N}_\mathcal{F}+\log(\mathcal{N}_\mathcal{H}/\delta))}{\nu nK}.
        \end{aligned}
    \end{equation}
    where
    \begin{equation}\label{eq:def_complexity}
        \begin{aligned}
            \log\mathcal{N}_\mathcal{F}:&=M_f^2S_fL_f\log\left(mnL_fW_f(B_f\vee 1)M_fT\log(1/\delta)\right), \\
            \log\mathcal{N}_\mathcal{H}:&=S_hL_h\log\left(nKL_hW_h(B_h\vee 1)M_f\gamma_f\log(1/\delta)\right).
        \end{aligned}
    \end{equation}
\end{thm}

The formal statements and proofs are provided in Appendix \ref{app:subsec:generalization_diversity}.

Let $\varepsilon_{\text{approx}}=\inf_{h\in\mathcal{H}} \frac{1}{K}\sum_{k=1}^K \inf_{f\in\mathcal{F}}\E_{(x,y)\sim\P^k} [\ell^{\P^k} (x,y,s_{f,h})]$ be the approximation error.
The leading terms can be simplified to $\widetilde{\mathcal{O}}\left(\varepsilon_{\text{approx}}+\frac{K\log\mathcal{N}_\mathcal{F}+\log\mathcal{N}_\mathcal{H}}{nK}+\frac{\log\mathcal{N}_\mathcal{F}}{m}\right)$, where $\log\mathcal{N}_\mathcal{F}$ and $\log\mathcal{N}_\mathcal{H}$ capture the complexity of the hypothesis classes.

\paragraph{Improving Sample Efficiency}
Theorem \ref{thm:generalization_all_diversity_informal} demonstrates the sample efficiency of transfer learning. 
Compared to naively training the full CDM for target distribution, which has an error of $\widetilde{\mathcal{O}}\left(\varepsilon_{\text{approx}}+\frac{\log\mathcal{N}_\mathcal{F}+\log\mathcal{N}_\mathcal{H}}{m}\right)$, transfer learning saves the complexity of learning $\mathcal{H}$ and thus the performance is much better when $m$ is relatively small to $n,K$ (\textit{i.e.}, in few-shot learning setting). 

\paragraph{Comparison with Existing Transfer Learning Theory}
Similar generalization bound of supervised transfer learning has been established under $(\nu,\Delta)$ diversity assumption.
\citet{tripuraneni2020theory} proves $\widetilde{\mathcal{O}}\left(\sqrt{\frac{K\log\mathcal{N}_\mathcal{F}+\log\mathcal{N}_\mathcal{H}}{nK}}+\sqrt{\frac{\log\mathcal{N}_\mathcal{F}}{m}}\right)
$ in realizable case.
\citet{watkins2023optimistic} improves the rate to $\widetilde{\mathcal{O}}\left(\varepsilon_{\text{approx}}+\frac{K\log\mathcal{N}_\mathcal{F}+\log\mathcal{N}_\mathcal{H}}{nK}+\frac{\log\mathcal{N}_\mathcal{F}}{m}\right)
$, by additionally assuming smoothness of loss function and applying local Rademacher complexity techniques.
The difference in our analysis lies in the intricacy of time-dependent score matching loss, where the Lipschitzness and (or) smoothness need to be re-verified.
Despite these technical difficulties, we are able to prove the same generalization bound as in supervised transfer learning. 

\subsection{Results without Task Diversity: Meta-Learning Perspective}

The results in previous section heavily depend on the task diversity assumption, which is hard to verify in practice.
An alternative is to consider meta-learning setting, where all source and target distributions are sampled from the same \textit{environment}, \textit{i.e.}, a meta distribution.

For any $h\in\mathcal{C}([0,1]^{D_y};[0,1]^{d_y})$ and distribution $\P$ over $\R^{d_x}\times[0,1]^{D_y}$, define the representation error as
\begin{equation}\label{eq:L(P,h)}
    \mathcal{L}(\P,h):=\inf_{f\in\mathcal{F}} \E_{(x,y)\sim\P}[\ell^\P(x,y,s_{f,h})]\geq 0.
\end{equation}
We characterize the generalization bound of source tasks on the entire meta distribution as follows.
\begin{prop}[Generalization on meta distribution] \label{prop:generalization_meta_informal}
    Under Assumption \ref{asp:sub_gaussian}, \ref{asp:low_dim}, \ref{asp:lip}, 
    there exists constant $C_P$
    such that for $\{\P^k\}_{k=1}^K\overset{\textit{i.i.d.}}{\sim}\Pmeta$,
    with probability no less than $1-\delta$, 
    \begin{align}
        &\E_{\P\sim\Pmeta}\mathcal{L}(\P,h)
        \leq \frac{2}{K}\sum_{k=1}^K\mathcal{L}(\P^k,h)+C_P\left(r_P+\frac{\log(1/\delta)}{K}\right), \\
        &\frac{1}{K}\sum_{k=1}^K\mathcal{L}(\P^k,h)\leq 2\E_{\P\sim\Pmeta}\mathcal{L}(\P,h)
        +C_P\left(r_P+\frac{\log(1/\delta)}{K}\right),
    \end{align}
    holds for any $h\in\mathcal{H}$, where $r_P=M_f^2\exp(-\Omega(R_f^2))+\frac{S_hL_h\log\left(KL_hW_h(B_h\vee 1)M_f\gamma_f\right)}{K}$.
\end{prop}

\begin{thm}\label{thm:generalization_all_informal}
    Under Assumption \ref{asp:sub_gaussian}, \ref{asp:low_dim}, \ref{asp:lip},
    if $R_f\gtrsim\log^{\frac{1}{2}}(nKM_f/\delta)$,
    then with probability no less than $1-\delta$, the expected population loss of new task can be bounded by
    \begin{equation}
        \begin{aligned}
            &\E_{\P^0\sim\Pmeta}\E_{\{(x_i,y_i)\}_{i=1}^m\sim \P^0} \E_{(x,y)\sim\P^0} [\ell^\P(x,y,s_{\widehat{f}^0,\widehat{h}})] \\
            &\qquad \lesssim \inf_{h\in\mathcal{H}} \E_{\P\sim\Pmeta} \inf_{f\in\mathcal{F}}\E_{(x,y)\sim\P} [\ell^\P (x,y,s_{f,h})] + \frac{\log^3(m)\log\mathcal{N}_\mathcal{F}}{m} + \frac{\log^3(nK/\delta)\log\mathcal{N}_\mathcal{F}}{n}+\frac{\log(\mathcal{N}_\mathcal{H}/\delta)}{K},
        \end{aligned}
    \end{equation}
    where $\log\mathcal{N}_\mathcal{F},\log\mathcal{N}_\mathcal{H}$ are defined in \eqref{eq:def_complexity}.
\end{thm}
The formal statements and proofs are provided in Appendix \ref{app:subsec:generalization}.

Let $\widetilde{\varepsilon}_{\text{approx}}=\inf_{h\in\mathcal{H}} \E_{\P\sim\Pmeta} \inf_{f\in\mathcal{F}}\E_{(x,y)\sim\P} [\ell^\P (x,y,s_{f,h})]$ be the approximation error in meta-learning.
The results above can be further simplified to $\widetilde{\mathcal{O}}\left(\widetilde{\varepsilon}_{\text{approx}}+\frac{\log\mathcal{N}_\mathcal{F}}{m\wedge n}+\frac{\log\mathcal{N}_\mathcal{H}}{K}\right)$. Different from transfer learning bound in Theorem \ref{thm:generalization_all_diversity_informal}, the leading term decays only in $K$ and not in $n$.
This is because that without task diversity assumption, the connection between source distributions and target distributions can only be constructed through meta distribution. 
And according to Proposition \ref{prop:generalization_meta_informal}, the source distributions $\P^1,\cdots,\P^K$ collectively form a $K$-shot empirical estimation of $\Pmeta$, leading to an estimation error of $\mathcal{O}(1/K)$. Despite this, Theorem \ref{thm:generalization_all_informal} still demonstrates the sample efficiency of meta-learning compared to naive training method when $m$ is small and $n,K$ are sufficient large. 

\paragraph{Comparison with Existing Meta-learning Theory}

The state of the art sample complexity bound in meta-learning setting is $\widetilde{\mathcal{O}}\left(\widetilde{\varepsilon}_{\text{approx}}+\sqrt{\frac{\log\mathcal{N}_\mathcal{F}}{m}}+\sqrt{\frac{\log\mathcal{N}_\mathcal{H}}{K}}\right)$ by assuming $m=n$ \citep{maurer2016benefit}, where the dependence on $\Omega(\sqrt{1/K})$ term cannot be improved.
However, due to the smoothness of quadratic loss function, we are able to prove an even faster rate for score matching objective by leveraging local Rademacher complexity theory.


\section{End-to-End Distribution Estimation via Deep Neural Network}\label{sec:dist_estimation}

Section \ref{sec:generalization} provides a statistical guarantee for transferring score matching.
In this section, we establish an approximation theory using deep neural network to quantify the misspecification error.
Combining both results we are able to obtain an end-to-end distribution estimation error bound for transfer learning diffusion models.

\subsection{Score Neural Network Approximation}

The following theorem provides a guarantee for the ability of deep ReLU neural networks to approximate score and representation.
The proof is provided in Appendix \ref{app:subsec:approximation}.
\begin{thm}\label{thm:approximation_all_informal}
     Under Assumption \ref{asp:sub_gaussian}, \ref{asp:low_dim}, \ref{asp:lip}, to achieve $R_f\gtrsim\log^{\frac{1}{2}}(nKM_f/\delta)$ and
    \begin{align}
        &\inf_{h\in\mathcal{H}} \frac{1}{K}\sum_{k=1}^K \inf_{f\in\mathcal{F}}\E_{(x,y)\sim\P^k} [\ell^{\P^k} (x,y,s_{f,h})] = \mathcal{O}\left(\log^2(nK/(\varepsilon\delta))\varepsilon^2\right), \text{ (transfer learning)} \\
        &\inf_{h\in\mathcal{H}}\E_{\P\sim\Pmeta}\inf_{f\in\mathcal{F}}\E_{(x,y)\sim\P} [\ell^\P(x,y,s_{f,h})] = \mathcal{O}\left(\log^2(nK/(\varepsilon\delta))\varepsilon^2\right), \text{ (meta-learning)}
    \end{align}
    the configuration of $\mathcal{F}$ and $\mathcal{H}$ should satisfy
    \begin{equation}
        \begin{aligned}
            &L_f=\mathcal{O}\left(\log\left(\frac{\log(nK/(\varepsilon\delta))}{\varepsilon}\right)\right),
            W_f=\mathcal{O}\left(\frac{\log^{3(d_x+d_y)/2}(nK/(\varepsilon\delta))}{\varepsilon^{d_x+d_y+1}T_0^3}\right), \\
            &S_f=\mathcal{O}\left(\frac{\log^{3(d_x+d_y)/2+1}(nK/(\varepsilon\delta))}{\varepsilon^{d_x+d_y+1}T_0^3}\right),
            B_f=\mathcal{O}\left(\frac{T\log^{\frac{3}{2}}(nK/(\varepsilon\delta))}{\varepsilon}\right), \\
            &R_f=\mathcal{O}\left(\log^{\frac{1}{2}}(nK/(\varepsilon\delta))\right), 
            M_f=\mathcal{O}\left(\log^3(nK/(\varepsilon\delta))\right),
            \gamma_f=\mathcal{O}\left(\log(nK/(\varepsilon\delta))\right),
        \end{aligned}
    \end{equation}
     \begin{equation}
        L_h=\mathcal{O}\left(\log(1/\varepsilon)\right), W_h=\mathcal{O}\left(\varepsilon^{-D_y}\log(1/\varepsilon)\right), 
        S_h=\mathcal{O}\left(\varepsilon^{-D_y}\log^2(1/\varepsilon)\right),
        B_h = \mathcal{O}(1).
    \end{equation}
    Here $\mathcal{O}(\cdot)$ hides all the polynomial factors of $d_x,d_y,D_y,C_1,C_2,L,B$.
\end{thm}

Universal approximation of deep ReLU neural networks in a bounded region has been widely studied \citep{yarotsky2017error,schmidt2020nonparametric}.
However, we have to deal with an unbounded domain here, hence more refined analysis is required, \text{e.g.} truncation arguments. 

In addition, traditional approximation theories typically cannot provide Lipschitz continuity guarantees, which is crucial in transfer learning analysis.
Following the constructions in \citet{chen2023score}, the Lipschitzness restriction doesn't compromise the approximation ability of neural networks, while ensuring validity of the generalization analysis in Section \ref{sec:generalization}.

\subsection{Distribution Estimation Error Bound}

Given the approximation and generalization results, we are in the position of bounding the distribution estimation error of our transfer (meta) learning procedures.
The formal statements and proofs can be found in Appendix \ref{app:subsec:dist_estimation}.

\begin{thm}[Transfer learning]
\label{thm:distribution_diversity_informal}
    Under Assumption \ref{asp:sub_gaussian}, \ref{asp:low_dim}, \ref{asp:lip} and $(\nu,\Delta)$-diversity with proper configuration of neural network family and $T,T_0$, it holds that with probability at least $1-\delta$,
    \begin{equation}
        \E_{\{(x_i,y_i)\}_{i=1}^m\sim\P^0}\E_{y\sim\P^0_y} [\mathrm{TV}(\widehat{\P}^0_{x|y},\P^0_{x|y})]
        \lesssim \frac{\log^{\frac{5}{2}}(nK/\delta)\log^3((m/\nu)\wedge n)}{\nu^{\frac{1}{2}}((m/\nu)\wedge n)^{\frac{1}{d_x+d_y+9}}}+\frac{\log^2(nK/\delta)}{\nu^{\frac{1}{2}}(nK)^{\frac{1}{D_y+2}}}+\sqrt{\Delta}.
    \end{equation}
\end{thm}


\begin{thm}[Meta-learning]
\label{thm:distribution_informal}
    Under Assumption \ref{asp:sub_gaussian}, \ref{asp:low_dim}, \ref{asp:lip} and meta-learning setting, with proper configuration of neural network family and $T,T_0$, it holds that with probability at least $1-\delta$,
    \begin{equation}
        \E_{\P^0\sim\Pmeta}\E_{\{(x_i,y_i)\}_{i=1}^m\sim \P^0}\E_{y\sim\P^0_y} [\mathrm{TV}(\widehat{\P}^0_{x|y},\P_{x|y}^0)]
        \lesssim \frac{\log^{\frac{5}{2}}(nK/\delta)\log^3(m\wedge n)}{(m\wedge n)^{\frac{1}{d_x+d_y+9}}}+\frac{\log^2(nK/\delta)}{K^{\frac{1}{D_y+2}}}.
    \end{equation}
\end{thm}

Theorem \ref{thm:distribution_diversity_informal} and \ref{thm:distribution_informal} again unveil the benefits of transfer (meta) learning for conditional diffusion models, with a rate of $\widetilde{\mathcal{O}}((m\wedge n)^{-\frac{1}{d_x+d_y+9}}+(nK)^{-\frac{1}{D_y+2}})$ or $\widetilde{\mathcal{O}}((m\wedge n)^{-\frac{1}{d_x+d_y+9}}+K^{-\frac{1}{D_y+2}})$.
To compare, naively learning the target distribution in isolation will yield $\widetilde{\mathcal{O}}(m^{-\frac{1}{d_x+D_y+9}})$.
When the condition dimension $D_y$ is much larger than feature dimension $d_y$, transfer (meta) learning can substantially improve sample efficiency on target tasks, thanks to representation learning.

\paragraph{Comparison with Existing Complexity Bounds of CDMs}

\citet{fu2024unveil} studies conditional diffusion model for sub-gaussian distributions with $\beta$-H\"older density.
Since the Lipschitzness of score is analogous to the requirement of twice differentiability of density \citep{wibisono2024optimal}, it is reasonable to let $\beta=2$ for a fair comparison.
In this case, the TV distance is bounded by $\widetilde{\mathcal{O}}(m^{-\frac{1}{2(d_x+D_y+2)}})$ with sample size $m$ according to \citet{fu2024unveil}, which is worse than our naive bound $\widetilde{\mathcal{O}}(m^{-\frac{1}{d_x+D_y+9}})$ due to the inefficiency of score approximation.
We are also aware of another work \citep{jiao2024model} that assumes Lipschitz density and score, obtaining a rate of $\widetilde{\mathcal{O}}(m^{-\frac{1}{2(d_x+3)(d_x+D_y+3)}})$.

\paragraph{Relation to \citet{yang2024fewshot}}

Unlike our setup, \citet{yang2024fewshot} considers transfer learning unconditional diffusion models with only one source task, \textit{i.e.}, $D_y=d_y=0,K=1$.
The unconditional distribution is assumed to be supported in a low-dimensional linear subspace, where the source task and the target task have the same latent variable distribution. 
Hence, \textit{only} a linear encoder is trained for fine-tuning instead of the full score network. 
In this case, \citet{yang2024fewshot} is able to bound the TV distance by $\widetilde{\mathcal{O}}(m^{-\frac{1}{4}}+n^{-\frac{1-\alpha(n)}{d_x+5}})$, escaping the curse of dimensionality for target task.
However, the assumption on shared latent variable distribution is stringent and we believe our analysis methods can be extended to this setting as well.


\section{Applications}\label{sec:application}

We explore two applications of transfer learning for conditional diffusion models, supported by theoretical guarantees derived from our earlier results.
In particular, we study amortized variational inference and behavior cloning.
These real-world use cases not only validate the applicability of our theoretical findings but also lay the foundations of transferring diffusion models in practice.


\subsection{Amortized Variational Inference}

Diffusion models have exhibited groundbreaking success in probabilistic inference, especially latent variable models.
We study a simple amortized variational inference model, where the observation $y$ given latent variable $x$ is distributed according to an exponential family $\mathcal{F}_\Psi$ with density
\begin{equation}
    p_\psi(y|x)=\psi(y)\exp(\langle x, h_*(y)\rangle-A_\psi(x)),
\end{equation}
where $\psi\in\Psi$ is non-negative and supported on $[0,1]^{D}$ and $h_*(y)\in[0,1]^{d}$. 
Note that we also have $d_x=d$ in this case.
The prior distribution of variable $x$ is denoted as $p_\phi$ for some $\phi\in\Phi$. 
Let $\theta=(\psi,\phi)$ and we aim to sample from the posterior distribution of $p_\theta(x|y)\propto p_\phi(x)p_\psi(y|x)\propto p_\phi(x)\exp(\langle x, h_*(y)\rangle-A_\psi(x))$.
Due to the special structure, the posterior $p_\theta(x|y)$ only depends on the low-dimensional feature $h_*(y)$, shared across all $\theta\in\Theta:=\Psi\times\Phi$.
This formulation encompasses various applications including independent component analysis \citep{comon1994independent}, inverse problem \citep{song2021solving,ajay2022conditional} and variational Bayesian inference \citep{kingma2013auto}. 

Consider source tasks consisting of $\theta^1,\cdots,\theta^K\in\Theta$, and for each $\theta^k$ we have $n$ \textit{i.i.d.} samples $\{(x_i^k,y_i^k)\}_{i=1}^n$.
For the target task $\theta^0$, we only have $m$ samples $\{(x_i^0,y_i^0)\}_{i=1}^m$.
We conduct our transfer learning procedures to train a conditional diffusion models $\widehat{\P}_{\theta^0}(\cdot|y)$.
For theoretical analysis, we further impose some assumptions on the probabilistic model as follows.
\begin{asp}\label{asp:amortized_vi}
    The prior distribution satisfies $p_\phi(x)\leq C_1\exp(-C_2\|x\|^2)$ and $\nabla_x \log p_\phi(x)$ is $L$-Lipshcitz in $x$, $\|\nabla_x \log p_\phi(0)\|\leq B$ for any $\phi\in\Phi$.
    The representation $h_*$ is $L$-Lipschitz.
    The integral $\int \psi(y)\dif y\in [1/C, C]$ for any $\psi\in\Psi$.
\end{asp}

\begin{thm}\label{thm:amortized_vi}
    Suppose Assumption \ref{asp:amortized_vi} holds. 
    Then under meta-learning setting, we have with probability no less than $1-\delta$,
    \begin{equation}
        \E_{\theta^0} \E_{\{(x_i^0,y_i^0)\}_{i=1}^m}\E_{y\sim\P_{\theta^0}}[\mathrm{TV}(\widehat{\P}_{\theta^0}(\cdot|y),\P_{\theta^0}(\cdot|y))]\lesssim 
        \frac{\log^{\frac{5}{2}}(nK/\delta)\log^3(m\wedge n)}{(m\wedge n)^{\frac{1}{2d+9}}}+\frac{\log^2(nK/\delta)}{K^{\frac{1}{D+2}}}.
    \end{equation}
    If $(\nu,\Delta)$-diversity holds, then we have with probability no less than $1-\delta$,
    \begin{equation}
        \E_{\{(x_i^0,y_i^0)\}_{i=1}^m}\E_{y\sim\P_{\theta^0}}[\mathrm{TV}(\widehat{\P}_{\theta^0}(\cdot|y),\P_{\theta^0}(\cdot|y))]\lesssim \frac{\log^{\frac{5}{2}}(nK/\delta)\log^3((m/\nu)\wedge n)}{\nu^{\frac{1}{2}}((m/\nu)\wedge n)^{\frac{1}{2d+9}}}+\frac{\log^2(nK/\delta)}{\nu^{\frac{1}{2}}(nK)^{\frac{1}{D+2}}}+\sqrt{\Delta}.
    \end{equation}
\end{thm}

The proof is deferred to Appendix \ref{app:subsec:proof_avi}.
We show that under mild assumptions, transfer (meta) learning diffusion models can improve the sample efficiency for target task in the context of amortized variational inference. 
This error bound can be further extended to establish guarantees for statistical inference such as moment prediction, uncertainty assessment, \textit{etc}.


\subsection{Behavior Cloning via Meta-Diffusion Policy}

Although originally developed for image generation tasks, diffusion models have recently been extended to reinforcement learning (RL) \citep{janner2022planning,chi2023diffusion,wang2022diffusion}, enabling the modeling of complex distributions of dynamics and policies.
In the context of meta-RL, some works have further utilized diffusion models for planning and synthesis tasks \citep{ni2023metadiffuser,he2023diffusion}.  
In this application, we focus on a popular framework of behavior cloning, \textit{diffusion policy} \citep{chi2023diffusion}, which uses conditional diffusion models to learn multi-modal expert policies in high-dimensional state spaces.
In such settings, the state often corresponds to visual observations of the robot's surroundings, such as high resolution images, and thus typically share a low-dimensional underlying representation.

Let $\mathcal{M}$ be the space of decision-making environments, where each $M\in\mathcal{M}$ is an infinite horizon Markov Decision Process (MDP) sharing the same state space $\mathcal{S}$, action space $\mathcal{A}$, discount factor $\gamma$ and initial distribution $\rho\in\Delta(\mathcal{S})$.
And each $M\in\mathcal{M}$ has its own transition kernel $\mathcal{T}_M:\mathcal{S}\times\mathcal{A}\to\Delta(\mathcal{S})$, and reward function $r_M:\mathcal{S}\times\mathcal{A}\to[0,1]$.
The policy is defined as a map $\pi:\mathcal{S}\to\Delta(\mathcal{A})$. 
The value function of MDP $M$ under policy $\pi$ is 
\begin{equation}
    \begin{aligned}
        &V_M(\pi,s_0):= \E\Big[\sum_{t=0}^\infty \gamma^tr_M(s_t,a_t)\Big], a_t\sim\pi(\cdot|s_t),
        s_{t+1}\sim\mathcal{T}_M(\cdot|s_t,a_t),\\  
        &V_M(\pi):=\E_{s_0\sim\rho}[V_M(\pi,s_0)].
    \end{aligned}
\end{equation}
Denote the visitation measure as $d_M^\pi(s,a):=(1-\gamma)\E_{s_0\sim\rho}\sum_{t=0}^\infty \gamma^t\P(s_t=s|\pi,s_0)\pi(a|s)$.

Suppose there are $K$ source tasks $M^1,\cdots,M^K\in\mathcal{M}$, and the expert policy of each task is denoted as $\pi_*^k$.
In behavior cloning, for each source task $M^k$, we have $n$ pairs of $\{(s_i^k,a_i^k)\}_{i=1}^n \overset{\textit{i.i.d.}}{\sim} d^k_*:=d_{M^k}^{\pi_*^k}$. 
The goal is to imitate the expert policy of target task $M^0\in\mathcal{M}$, of which the sample size is only $m\ll n$.   

To unify the notation, let $x=a,y=s$ and assume $\mathcal{A}=\R^{d_a},\mathcal{S}=[0,1]^{D_s}$ and representation space $[0,1]^{d_s}$.
Our meta diffusion-policy framework aims to learn a state encoder $h:\mathcal{S}\to[0,1]^{d_s}$ during pre-training, which acts as a shared representation map in different MDPs and consequently enhances sample efficiency on fine-tuning tasks.
Let $\widehat{\pi}^0$ be the learned policy in fine-tuning phase. The following theorem shows the optimality gap between the learned policy and the expert policy.
\begin{thm}\label{thm:behavior_cloning}
    Suppose the expert policy $\pi_*^k$ satisfies Assumption \ref{asp:sub_gaussian}, \ref{asp:low_dim}, \ref{asp:lip}. Then under meta-learning setting, it holds that with probability no less than $1-\delta$, 
    \begin{equation}
        \E_{M^0}\E_{\{(s_i^0,a_i^0)\}_{i=1}^m\sim d_*^0}[V_{M^0}(\pi_*^0)-V_{M^0}(\widehat{\pi}^0)]\lesssim \frac{1}{(1-\gamma)^2}\left[\frac{\log^{\frac{5}{2}}(nK/\delta)\log^3(m\wedge n)}{(m\wedge n)^{\frac{1}{d_a+d_s+9}}}+\frac{\log^2(nK/\delta)}{K^{\frac{1}{D_s+2}}}\right].
    \end{equation}
    If we further assume $\pi_*^1,\cdots,\pi_*^K$ are $(\nu,\Delta)$-diverse over $\pi_*^0$, then the gap can be improved by
    \begin{equation}
        \E_{\{(s_i^0,a_i^0)\}_{i=1}^m\sim d_*^0}[V_{M^0}(\pi_*^0)-V_{M^0}(\widehat{\pi}^0)]\lesssim \frac{1}{(1-\gamma)^2}\left[\frac{\log^{\frac{5}{2}}(nK/\delta)\log^3((m/\nu)\wedge n)}{\nu^{\frac{1}{2}}((m/\nu)\wedge n)^{\frac{1}{d_a+d_s+9}}}+\frac{\log^2(nK/\delta)}{\nu^{\frac{1}{2}}(nK)^{\frac{1}{D_s+2}}}+\sqrt{\Delta}\right].
    \end{equation}
\end{thm}

The proof can be found in Appendix \ref{app:subsec:proof_bc}.
This provides the first statistical guarantee of diffusion policy in behavior cloning.
Notably, in both cases, the number of source tasks $K$ has an exponential dependence on $D_s$, further suggesting the importance of data coverage when tackling distribution shift in offline meta-RL \citep{pong2022offline}.


\section{Experiments}\label{sec:exp}
\paragraph{Experimental Settings}
Our numerical example is the high-dimensional conditioned diffusion \citep{cui2016dimension, yu2023hierarchical} arising from the following Langevin SDE
\begin{equation}\label{eq:conditioned-diffusion}
\mathrm{d} u_s = \beta u_s(1-u_s^2) \mathrm{d} s + \mathrm{d} w_s, \ u_0=0,
\end{equation}
where $\beta >0$ and $w_s$ is a one-dimensional standard Brownian motion.
The SDE \eqref{eq:conditioned-diffusion} is discretized by the Euler-Maruyama scheme with a step size of $0.02$, which defines the prior distribution $p_{\beta}(x)$ for the (discretized) trajectory $x=\left(u_{0.02}, u_{0.04}, \ldots, u_{1.00}\right)^\top\in \mathbb{R}^{50}$.
We consider a conditional Gaussian likelihood function, $p(y|x)=\mathcal{N}(Mx, I_{100}/4)$, where $M\in \mathbb{R}^{100\times 50}$ is a pre-defined projection matrix.
With a set of pre-selected $\{\beta_k; 1\leq k\leq K\}$ with $\beta_k=k$ and $K=10$, the $k$-th source distribution of $(x,y)$ is given by $\mathbb{P}^k(x,y)=p_{\beta_k}(x)p(y|x)$.
The target distribution $\mathbb{P}^0(x,y)$ is given by $\beta_0=5.5$ or $\beta_0=15$.

Each $f^k$ and $f^0$ are implemented as a 2-layer MLP with 128 internal channels and 60 input channels. The representation map $h$ is implemented as a 5-layer MLP with 512 internal channels and 10 output channels.
We have $n=1000$ pre-training samples from each source distribution $\mathbb{P}^k$, $m\in\{10,20,30, 40,50,100\}$ fine-tuning samples from the target distribution $\mathbb{P}^0$, and $100$ test samples from the target distribution $\mathbb{P}^0$ for evaluating the test error of different models.
In the pre-training phase, the ${\widehat{f}^k; 1\leq k\leq K}$ and $\hat{h}$ are trained on the $K=10$ source distributions with 400K iterations and a batch size of 512.
In the fine-tuning phase, the pre-trained representation map $\widehat{h}$ is fixed, and the $\widehat{f}^0$ is trained on the target distribution with 200K iterations and a batch size of $m$.
As an important baseline, we also consider jointly training $h$ and $f^{0}$ on the target distribution from scratch, using the same fine-tuning samples.

\paragraph{Results}
We report the MSEs of the estimated posterior mean of $\mathbb{P}^0(x|y)$ on the test samples in Table \ref{tab:beta=5.5} and \ref{tab:beta=15}.
We see that for different $\beta$'s and $m$'s, the fine-tuned models can provide significantly more accurate posterior mean estimations in almost all of the cases, suggesting the effectiveness of the representation map $\widehat{h}$ learned in the pre-training phase.
We also notice a large variance among the results of different replicates, and attribute the slightly worse performance of fine-tined models at $m=50,\beta=5.5$ to the potential randomness.
As $m$ increases, the performance gaps between fine-tuned models and train-from-scratch models get smaller, since more training samples yield more generalization benefits.

\begin{table}[ht]
    \centering
    \begin{tabular}{ccccccc}
    \toprule
    $m$ & 10 & 20 & 30 & 40 & 50 & 100 \\
    \midrule
    MSE of fine-tuned models & 14.47 & 3.68 & 2.45 & 1.82 & 1.9  & 0.91 \\
    MSE of train-from-scratch models  & 21.99 &10.61 & 5.71 & 2.38 & 1.77&  1.04\\
    \bottomrule
    \end{tabular}
    \caption{The MSEs of different models for estimating the posterior mean of $x$, whose ground truth is formed by an extremely long LMC run ($\beta_0=5.5$).}
    \label{tab:beta=5.5}
\end{table}


\begin{table}[ht]
    \centering
    \begin{tabular}{ccccccc}
    \toprule
    $m$ & 10 & 20 & 30 & 40 & 50 & 100 \\
    \midrule
    MSE of fine-tuned models & 6.14 &2.65& 1.61& 1.08 &0.96& 0.45\\
    MSE of train-from-scratch models  & 24.41& 20.62 &18.67 &13.49&  7.03 & 1.23\\
    \bottomrule
    \end{tabular}
    \caption{The MSEs of different models for estimating the posterior mean of $x$, whose ground truth is formed by an extremely long LMC run ($\beta_0=15$).}
    \label{tab:beta=15}
\end{table}


\section{Conclusion}

In this paper, we take the first step towards understanding the sample efficiency of transfer learning conditional diffusion models from the perspective of representation learning.
We provide generalization guarantee of transferring score matching in CDMs in different settings.
We further establish an end-to-end distribution estimation error bound using deep neural network.
Two practical applications are investigated based on our theoretical results.
We hope this work can motivate future theoretical study on the popular transfer learning paradigm in generative AIs.

\bibliography{ref}
\bibliographystyle{plainnat}

\newpage

\appendix


\section{Proofs in Section \ref{sec:generalization}}

\subsection{Preliminaries}\label{app:subsec:pre_generalization}

\begin{lemma}
    If $x_0\sim p(x_0|y)$, the density of forward process $p_t(x|y)$ can be written as
    \begin{equation}\label{eq:density}
        p_t(x|y)=\int \phi_t(x|x_0)p(x_0|y)\dif x_0,\quad  \phi_t(x|x_0)=\frac{1}{(2\pi\sigma_t^2)^{\frac{d_x}{2}}}\exp\Big(-\frac{\|x-\alpha_tx_0\|^2}{2\sigma_t^2}\Big).
    \end{equation}
    Besides, the score function has the form of
    \begin{align}
        \nabla_x\log p_t(x|y)
        &=\int \nabla_x\log\phi_t(x|x_0)\frac{\phi_t(x|x_0)p(x_0|y)}{\int \phi_t(x|z)p(z|y)\dif z}\dif x_0 \label{eq:score_1}\\
        &=\frac{1}{\alpha_t}\int \nabla_x\log p(x_0|y)\frac{\phi_t(x|x_0)p(x_0|y)}{\int \phi_t(x|z)p(z|y)\dif z}\dif x_0. \label{eq:score_2}
    \end{align}
\end{lemma}

\begin{proof}
    \eqref{eq:density} can be directly implied by the definition of forward process.
    And it yields
    \begin{equation}
        \begin{aligned}
            \nabla_x\log p_t(x|y)
            &=\frac{\nabla_x p_t(x|y)}{p_t(x|y)} \\
            &=\frac{\int \nabla_x\phi_t(x|x_0)p(x_0|y)\dif x_0}{\int \phi_t(x|x_0)p(x_0|y)\dif x_0} \\
            &=\int \nabla_x\log\phi_t(x|x_0)\frac{\phi_t(x|x_0)p(x_0|y)}{\int \phi_t(x|z)p(z|y)\dif z}\dif x_0,
        \end{aligned}
    \end{equation}
    which is \eqref{eq:score_1}. 
    Moreover, noticing that $\nabla_x\phi_t(x|x_0)=-\frac{1}{\alpha_t}\nabla_{x_0}\phi_t(x|x_0)$, then by integration by parts,
    \begin{equation}
        \begin{aligned}
            \frac{\int \nabla_x\phi_t(x|x_0)p(x_0|y)\dif x_0}{\int \phi_t(x|x_0)p(x_0|y)\dif x_0}
            &= -\frac{1}{\alpha_t}\frac{\int \nabla_{x_0}\phi_t(x|x_0)p(x_0|y)\dif x_0}{\int \phi_t(x|x_0)p(x_0|y)\dif x_0} \\
            &= \frac{1}{\alpha_t}\frac{\int \phi_t(x|x_0)\nabla_{x_0}p(x_0|y)\dif x_0}{\int \phi_t(x|x_0)p(x_0|y)\dif x_0} \\
            &=\frac{1}{\alpha_t}\int \nabla_x\log p(x_0|y)\frac{\phi_t(x|x_0)p(x_0|y)}{\int \phi_t(x|z)p(z|y)\dif z}\dif x_0.
        \end{aligned}
    \end{equation}
    Hence \eqref{eq:score_2} is proved.
\end{proof}

\begin{lemma}\label{lem:lip_score}[Lem. \ref{lem:lip_score_informal}]
    For any $w\in[0,1]^{d_y}$, denote the conditional score of forward process $\nabla_x\log p_t(x;w)$ by $f_*(x,w,t)$. Then there exist constants $C_X,C_X'$, such that for any $t\in[0,T]$, the function $f_*(x,w,t)$ is $(C_X+C_X'\|x\|^2)$-Lipschitz in $x$, $(C_X+C_X'\|x\|)$-Lipschitz in $w$.
\end{lemma}

\begin{proof}
    Define density function $q_t(x_0|x,w)\propto \phi_t(x|x_0)p(x_0;w)$. 
    Our proof strategy will depend on whether $t\geq\frac{1}{2(L+1)}$.
    
    When $t\geq \frac{1}{2(L+1)}$, according to \eqref{eq:score_1}, we have
    \begin{equation}
        \begin{aligned}
            \nabla_xf_*(x,w,t)
            &=\nabla_x^2\log p_t(x;w) \\
            &=\E_{q_t(x_0|x,w)} \left[\nabla_x^2\log\phi_t(x|x_0)\right] + \Var_{q_t(x_0|x,w)}(\nabla_x\log\phi_t(x|x_0)) \\
            &=-\frac{I}{\sigma_t^2}+ \Var_{q_t(x_0|x,w)}\Big(\frac{\alpha_tx_0-x}{\sigma_t^2}\Big)
        \end{aligned}
    \end{equation}
    For any $R>0$, we have
    \begin{equation}
        \begin{aligned}
            \Var_{q_t(x_0|x,w)}\Big(\frac{\alpha_tx_0-x}{\sigma_t^2}\Big)
            &\preceq \frac{1}{\sigma_t^2}\int \big\|\frac{\alpha_tx_0-x}{\sigma_t}\big\|^2\frac{\phi_t(x|x_0)p(x_0|y)}{\int \phi_t(x|z)p(z|y)\dif z}\dif x_0 \\
            &\leq \frac{R^2}{\sigma_t^2} + \frac{\int_{\|\frac{\alpha_tx_0-x}{\sigma_t}\|\geq R} \|\frac{\alpha_tx_0-x}{\sigma_t}\|^2\exp\left(-\frac{\|\alpha_tx_0-x\|^2}{2\sigma_t^2}\right)p(x_0;w)\dif x_0}{\sigma_t^2\int \exp\left(-\frac{\|\alpha_tx_0-x\|^2}{2\sigma_t^2}\right)p(x_0;w)\dif x_0} \\
            &\leq \frac{R^2}{\sigma_t^2} + \frac{\int_{\|\frac{\alpha_tx_0-x}{\sigma_t}\|\geq R} \exp(-\frac{R^2}{4})p(x_0;w)\dif x_0}{\sigma_t^2\int_{\|\frac{\alpha_tx_0-x}{\sigma_t}\|\leq R/2} \exp(-\frac{R^2}{8})p(x_0;w)\dif x_0}.
        \end{aligned}
    \end{equation}
    Let $R=\frac{2\|x\|+2C_0}{\sigma_t}$, then the domain $\Big\{x_0:\|\frac{\alpha_tx_0-x}{\sigma_t}\|\leq R/2\Big\}$ includes $\Big\{x_0:\|x_0\|\leq C_0\Big\}$, indicating
    \begin{equation}
        \begin{aligned}
            &\int_{\|\frac{\alpha_tx_0-x}{\sigma_t}\|\leq R/2} p(x_0;w)\dif x_0\geq  \int_{\|x_0\|\leq C_0} p(x_0;w)\dif x_0 \geq 1-2\exp(-C_1'C_0^2)\geq \frac{1}{2},\\
            &\int_{\|\frac{\alpha_tx_0-x}{\sigma_t}\|\geq R} p(x_0;w)\dif x_0\leq  \int_{\|x_0\|\geq C_0} p(x_0;w)\dif x_0 \leq \frac{1}{2}.
        \end{aligned}
    \end{equation}
    and
    \begin{equation}\label{eq:lip_x_large}
        \|\nabla_xf_*(x,w,t)\|\leq \frac{1}{\sigma_t^2}+\big\|\Var_{q_t(x_0|x,w)}\Big(\frac{\alpha_tx_0-x}{\sigma_t^2}\Big)\big\|
        \leq \frac{R^2}{\sigma_t^2} + \frac{2}{\sigma_t^2} \leq \frac{8\|x\|^2+8C_0^2+2\sigma_t^2}{\sigma_t^4}.
    \end{equation}
    
    Similarly, for $w$ we have
    \begin{equation}
        \begin{aligned}
            \nabla_wf_*(x,w,t)
            &=\Cov_{q_t(x_0|x,w)}\big(\nabla_x\log\phi_t(x|x_0),\nabla_w\log p(x_0;w)\big) \\
            &=\Cov_{q_t(x_0|x,y)}\big(\frac{\alpha_tx_0}{\sigma_t^2},\nabla_w\log p(x_0;w)\big) \\
        \end{aligned}
    \end{equation}
    which implies 
    \begin{equation}\label{eq:lip_w_large}
        \begin{aligned}
            \|\nabla_wf_*(x,w,t)\|
            &\leq B\sqrt{\big\|\Var_{q_t(x_0|x,w)}\Big(\frac{\alpha_tx_0-x}{\sigma_t^2}\Big)\big\|} \\
            &\leq \frac{B(2\|x\|+2C_0+1)}{\sigma_t}
        \end{aligned}
    \end{equation}

    When $t\leq\frac{1}{2(L+1)}$, we have $\sigma_t^2\leq \frac{\alpha_t^2}{2L}$ and
    \begin{equation}
        \begin{aligned}
            \nabla_xf_*(x,w,t)
            &=\nabla_x^2\log p_t(x;w) \\
            &= \frac{\nabla_x^2 p_t(x;w)}{p_t(x;w)} - \nabla_x\log p_t(x;w)(\nabla_x\log p_t(x;w))^\top \\
            &=\frac{1}{\alpha_t^2} \frac{\int \phi_t(x|x_0)\nabla_x^2p(x_0;w)\dif x_0}{p_t(x;w)} - \nabla_x\log p_t(x;w)(\nabla_x\log p_t(x;w))^\top \\
            &= \frac{1}{\alpha_t^2} \E_{q_t(x_0|x,w)}\left[\frac{\nabla_x^2p(x_0;w)}{p(x_0;w)}\right] - \nabla_x\log p_t(x;w)(\nabla_x\log p_t(x;w))^\top \\
            &= \frac{1}{\alpha_t^2} \E_{q_t(x_0|x,w)}\left[\nabla_x^2\log p(x_0;w)+\nabla_x\log p(x_0;w)(\nabla_x\log p(x_0;w))^\top\right] \\
            &\qquad - \nabla_x\log p_t(x;w)(\nabla_x\log p_t(x;w))^\top \\
            &\overset{\eqref{eq:score_2}}{=} \frac{1}{\alpha_t^2} \E_{q_t(x_0|x,y)}\left[\nabla_x^2\log p(x_0;w)\right] + \frac{1}{\alpha_t^2}\Var_{q_t(x_0|x,w)}\big(\nabla_x\log p(x_0;w)\big).
        \end{aligned}
    \end{equation}
    Note that when $\sigma_t^2\leq \frac{\alpha_t^2}{2L}$, the distribution $q_t(x_0|x,w)\propto \exp\big(-\frac{\|\alpha_tx_0-x\|^2}{2\sigma_t^2}\big)p(x_0;w)$ is $L$-strongly log-concave, and thus satisfies the Poincare inequality with a constant $L^{-1}$ \citep{chen2023improved},
    \begin{equation}
        \Var_{q_t(x_0|x,w)}\big(\nabla_x\log p(x_0;w)\big)\preceq L^{-1} \E \left[\nabla_x^2\log p(x_0;w) (\nabla_x^2\log p(x_0;w))^\top\right] \leq L.
    \end{equation}
    And thus
    \begin{equation}\label{eq:lip_x_small}
        \|\nabla_xf_*(x,w,t)\|\leq \frac{2L}{\alpha_t^2}.
    \end{equation}
    Analogously,
    \begin{equation}\label{eq:lip_w_small}
        \begin{aligned}
            \nabla_wf_*(x,w,t)
            &= \frac{1}{\alpha_t} \E_{q_t(x_0|x,w)}\left[\nabla_w\nabla_x\log p(x_0;w)\right] + \frac{1}{\alpha_t} \Cov_{q_t(x_0|x,w)}\big(\nabla_x\log p(x_0;w), \nabla_w\log p(x_0;w)\big) \\
            &\leq \frac{L}{\alpha_t} + \frac{B}{\alpha_t}\sqrt{\Var_{q_t(x_0|x,w)}\big(\nabla_x\log p(x_0;w)\big)} \\
            &\leq \frac{L+B\sqrt{L}}{\alpha_t}
        \end{aligned}
    \end{equation}
    Combine all the arguments in \eqref{eq:lip_x_large},\eqref{eq:lip_w_large},\eqref{eq:lip_x_small},\eqref{eq:lip_w_small} and we complete the proof.
\end{proof}

\begin{lemma}[Lemma 7, \citet{chen2022nonparametric}]\label{lem:covering_num}
    The covering number of $\mathcal{F}=NN_f(L_f,W_f,M_f,S_f,B_f,R_f,\gamma_f)$ can be bounded by
    \begin{equation}
        \log \mathcal{N}(\mathcal{F},\|\cdot\|_{L^\infty([-R,R]^{d_x+d_y+1})},\varepsilon) \lesssim S_fL_f\log\left(\frac{L_fW_f(B_f\vee 1)RM_f}{\varepsilon}\right).
    \end{equation}
    The covering number of $\mathcal{H}=NN_h(L_h,W_h,S_h,B_h)$ can be bounded by
    \begin{equation}
        \log \mathcal{N}(\mathcal{H},\|\cdot\|_{L^\infty([0,1]^{D_y})},\varepsilon) \lesssim S_hL_h\log\left(\frac{L_hW_h(B_h\vee 1)}{\varepsilon}\right).
    \end{equation}
\end{lemma}

\subsection{Proofs of Transfer Learning}\label{app:subsec:generalization_diversity}

\begin{prop}[Prop. \ref{prop:generalization_test_informal}]
\label{prop:generalization_test}
    Under Assumption \ref{asp:sub_gaussian}, \ref{asp:low_dim}, \ref{asp:lip}, there exists some constant $C_{xy}$ such that the following holds.
    For any $h\in\mathcal{H}$ and $(x_1,y_1),\cdots,(x_m,y_m)\overset{\textit{i.i.d.}}{\sim}\P$, define the empirical minimizer
    \begin{equation}
        \widehat{f}:= \argmin_{f\in\mathcal{F}} \frac{1}{m}\sum_{i=1}^m \ell(x_i,y_i,s_{f,h}).
    \end{equation}
    The population loss of $\widehat{f}$ can be bounded by
    \begin{equation}
        \E_{\{(x_i,y_i)\}_{i=1}^m\sim \P} \E_{(x,y)\sim\P} [\ell^\P(x,y,s_{\widehat{f},h})]\leq 4\inf_{f\in\mathcal{F}}\E_{(x,y)\sim\P}[\ell^\P(x,y,s_{f,h})] + C_{xy}\log^3(m) r_x,
    \end{equation}
    where $r_x=\frac{M_f^2S_fL_f\log\left(mL_fW_f(B_f\vee 1)M_fT\right)}{m}$.
\end{prop}

\begin{proof}
    Consider the truncated function class defined on $\R^{d_x}\times[0,1]^{D_y}$,
    \begin{equation}
        \Phi=\{(x,y)\mapsto \widetilde{\ell}(x,y,f):=(\ell(x,y,s_{f,h})-\ell(x,y,s_*^\P))\cdot\mathbbm{1}_{\|x\|_\infty\leq R}:f\in\mathcal{F}\},
    \end{equation}
    where the truncation radius $R\geq 1$ will be defined later.
    It is easy to show that with probability no less than $1-2m\exp(-C_1'R^2)$, it holds that $\|x_i\|_\infty\leq R$ for all $1\leq i \leq m$.
    Hence by definition, the empirical minimizer also satisfies $\widehat{f}= \argmin_{f\in\mathcal{F}} \frac{1}{m}\sum_{i=1}^m \widetilde{\ell}(x_i,y_i,f)$.
    Below we reason conditioned on this event and verify the conditions required in Lemma \ref{lem:local_rademacher}. 
    \begin{enumerate}[label=\textbf{Step \arabic*.}]
        \item To bound the individual loss,
        \begin{equation}
            \begin{aligned}
                \widetilde{\ell}(x,y,f)
                \leq \E_{t,x_t|x}\|s_{f,h}(x_t,y,t)-\nabla_x\log \phi_t(x_t|x)\|^2
                \lesssim M_f^2+ d_x\Big(\frac{\log(1/T_0)}{T-T_0}+1\Big). 
            \end{aligned}
        \end{equation}
        And by Lemma \ref{lem:bound score_t},
        \begin{equation}
            \begin{aligned}
                -\widetilde{\ell}(x,y,f)
                \leq \E_{t,x_t|x}\|s_*^\P(x_t,y,t)-\nabla_x\log \phi_t(x_t|x)\|^2\cdot \mathbbm{1}_{\|x\|_\infty\leq R}
                \lesssim C_X^{''}R^6+ d_x\Big(\frac{\log(1/T_0)}{T}+1\Big). 
            \end{aligned}
        \end{equation}
        Let $M:=C\left(C_X^{''}R^6+M_f^2+d_x\Big(\frac{\log(1/T_0)}{T}+1\Big)\right)$ and thus $|\widetilde{\ell}(x,y,f)|\leq M$.
        \item To bound the second order moment, we have
            \begin{equation}
                \begin{aligned}
                    &\E_{(x,y)\sim\P} \left[\mathbbm{1}_{\|x\|_\infty\leq R} \left(\ell(x,y,s_{f,h})-\ell(x,y,s_*^\P)\right)^2\right] \\
                    &= \E_{(x,y)\sim\P}\left[\mathbbm{1}_{\|x\|_\infty\leq R} \left(\E_{t,x_t|x}\|s_{f,h}(x_t,y,t)-\nabla_x\log\phi_t(x_t|x)\|^2-\|s_*^\P(x_t,y,t)-\nabla_x\log\phi_t(x_t|x)\|^2\right)^2\right] \\
                    &\leq \E_{(x,y)\sim\P} \left[\mathbbm{1}_{\|x\|_\infty\leq R}\left(\E_{t,x_t|x}\|s_{f,h}(x_t,y,t)-s_*^\P(x_t,y,t)\|^2\right)\right.\\
                    &\qquad\qquad\qquad \left.\cdot \left(\E_{t,x_t|x}\|s_{f,h}(x_t,y,t)+s_*^\P(x_t,y,t)-2\nabla_x\log\phi_t(x_t|x)\|^2\right)\right] \\
                    &\leq 4M\E_{(x,y)\sim\P} \left[\mathbbm{1}_{\|x\|_\infty\leq R}\left(\E_{t,x_t|x}\|s_{f,h}(x_t,y,t)-s_*^\P(x_t,y,t)\|^2\right)\right] \\
                    &\leq 4M \E_{(x,y)\sim\P}\left(\ell(x,y,s_{f,h})-\ell(x,y,s_*^\P)\right) \\
                    &\leq 4M\E_{(x,y)\sim\P} [\widetilde{\ell}(x,y,f)] + 8M^2\exp(-C_1'R^2).
                \end{aligned}
            \end{equation}
        \item To bound the local Rademacher complexity, note that
            \begin{equation}
                \Big\|\frac{1}{\sqrt{m}} \sum_{i=1}^m\sigma_i\widetilde{\ell}(x_i,y_i,f_1) - \frac{1}{\sqrt{m}}\sum_{i=1}^m\sigma_i\widetilde{\ell}(x_i,y_i,f_2) \Big\|_{\psi_2} \leq 4\|\widetilde{\ell}(\cdot,\cdot,f_1)-\widetilde{\ell}(\cdot,\cdot,f_2)\|_{L^2(\widehat{\P}_m)},
            \end{equation}
            where $\widehat{\P}_m:=\frac{1}{m}\sum_{i=1}^m\delta_{(x_i,y_i)}$.
            Define $\Phi_r:=\{\varphi\in\Phi:\frac{1}{m}\sum_{i=1}^m\varphi(x_i,y_i)^2\leq r\}$
            and it is easy to show that $\textbf{diam}\big(\Phi_r,\|\cdot\|_{L^2(\widehat{\P}_m)}\big)\leq 2\sqrt{r}$.
            By Dudley's bound \citep{van2014probability,wainwright2019high}, there exists an absolute constant $C_0$ such that for any $\theta>0$,
            \begin{equation}\label{eq:rademacher_bound_2}
                \mathcal{R}_m(\Phi_r)\leq C_0\left(\theta+\int_\theta^{2\sqrt{r}}\sqrt{\frac{\log\mathcal{N}(\Phi_r,\|\cdot\|_{L^2(\widehat{\P}_m)},\varepsilon)}{m}}\ \dif\varepsilon\right).
            \end{equation}
            Since $\|x_i\|\leq R$,
            \begin{equation}
                \begin{aligned}
                    \frac{1}{m}\sum_{i=1}^m(\widetilde{\ell}(x_i,y_i,f_1)-\widetilde{\ell}(x_i,y_i,f_2))^2
                    &= \frac{1}{m}\sum_{i=1}^m(\ell(x_i,y_i,s_{f_1,h})-\ell(x_i,y_i,s_{f_2,h}))^2 \\
                    &\leq \frac{1}{m}\sum_{i=1}^m \left[\E_{t,x_t|x_i}\|f_1-f_2\|^2\right]\cdot\left[\E_{t,x_t|x_i}\|f_1+f_2-2\nabla_x\log\phi_t\|^2\right] \\
                    &\leq \frac{4M}{m}\sum_{i=1}^m \E_{t,x_t|x_i}\|f_1(x_t,h(y_i),t)-f_2(x_t,h(y_i),t)\|^2.
                \end{aligned}
            \end{equation}
            Let $R_1=2R$. Since $x_t|x_i\sim\mathcal{N}(x_t;\alpha_tx_i,\sigma_t^2I)$, we have $\P(\|x_t\|_\infty\geq R_1)\leq d_x\P(|\mathcal{N}(0,1)|\leq R)\leq 2d_x\exp(-C_0'R^2)$ for some absolute constant $C_0'$.
            Therefore,
            \begin{equation}
                \begin{aligned}
                    &\E_{t,x_t|x_i}\|f_1(x_t,h(y_i),t)-f_2(x_t,h(y_i),t)\|^2 \\
                    &\qquad \leq \E_{t,x_t|x_i}[\mathbbm{1}_{\|x_t\|\leq R_1}] [\|f_1(x_t,h(y_i),t)-f_2(x_t,h(y_i),t)\|^2] + 8d_xM_f^2\exp(-C_0'R^2) \\
                    &\qquad \leq \|f_1-f_2\|^2_{L^\infty(\Omega_{R_1})} + 8d_xM_f^2\exp(-C_0'R^2)
                \end{aligned}
            \end{equation}
            where $\Omega_{R_1}:=[-R_1,R_1]^{d_x}\times[0,1]^{d_y}\times[T_0,T]$. Plug in the bound above,
            \begin{equation}
                \sqrt{\frac{1}{m}\sum_{i=1}^m(\widetilde{\ell}(x_i,y_i,f_1)-\widetilde{\ell}(x_i,y_i,f_2))^2}
                \leq 4M^{\frac{1}{2}}\|f_1-f_2\|_{L^\infty(\Omega_{R_1})} + 8d_x^{\frac{1}{2}}M\exp(-C_0'R^2/2).
            \end{equation}
            For any $\varepsilon\geq 16d_x^{\frac{1}{2}}M\exp(-C_0'R^2/2)$, according to \ref{lem:covering_num},
            \begin{equation}
                \begin{aligned}
                    \log\mathcal{N}(\Phi_r,\|\cdot\|_{L^2(\widehat{\P}_m)},\varepsilon)
                    &\leq \log\mathcal{N}(\mathcal{F},\|\cdot\|_{L^\infty(\Omega_{R_1})},\varepsilon/(8M^{\frac{1}{2}})) \\
                    &\leq C_4S_fL_f\log\left(\frac{L_fW_f(B_f\vee 1)(R\vee T)M}{\varepsilon}\right).
                \end{aligned}
            \end{equation}
            Plug in \eqref{eq:rademacher_bound_2} and let $\theta=16d_x^{\frac{1}{2}}M\exp(-C_0'R^2/2)$,
            \begin{equation}
                \begin{aligned}
                    \mathcal{R}_m(\Phi_r)
                    &\leq C_0\left(\theta+\int_\theta^{2\sqrt{r}}\sqrt{\frac{C_4S_fL_f\log\left(\frac{L_fW_f(B_f\vee 1)(R\vee T)M}{\varepsilon}\right)}{m}}\dif\varepsilon\right) \\
                    &\leq C_0\left(16d_x^{\frac{1}{2}}M\exp(-C_0'R^2/2)+\sqrt{\frac{C_4'S_fL_f\log\left(\frac{L_fW_f(B_f\vee 1)(R\vee T)M}{r}\right)\cdot r}{m}}\right) \\
                    &=: \widetilde{\mathcal{R}}_m(r)
                \end{aligned}
            \end{equation}
    \end{enumerate}
    
    Combine the three steps above, by Lemma \ref{lem:local_rademacher} with $B_0=8M^2\exp(-C_1'R^2),B=4M,b=M$, it holds that with probability no less than $1-2m\exp(-C_1'R^2)-\delta/2$, for any $f\in\mathcal{F}$,
    \begin{equation}\label{eq:bound_erm_1}
        \begin{aligned}
            \E_{(x,y)\sim\P} [\widetilde{\ell}(x,y,f)]
            &\leq \frac{2}{m}\sum_{i=1}^m \widetilde{\ell}(x_i,y_i,f) + C_5M\left(r_m^*+\frac{\log(\log(m)/\delta)}{m}\right) \\
            &\qquad + C_5\sqrt{\frac{M^2\log(\log(m)/\delta)}{m}}\exp(-C_1'R^2),
        \end{aligned}
    \end{equation}
    \begin{equation}\label{eq:bound_erm_2}
        \begin{aligned}
            \frac{1}{m}\sum_{i=1}^m \widetilde{\ell}(x_i,y_i,f)
            &\leq 2\E_{(x,y)\sim\P} [\widetilde{\ell}(x,y,f)] + C_5M\left(r_m^*+\frac{\log(\log(m)/\delta)}{m}\right) \\
            &\qquad + C_5\sqrt{\frac{M^2\log(\log(m)/\delta)}{m}}\exp(-C_1'R^2).
        \end{aligned}
    \end{equation}
    where $r_m^*$ is the largest fixed point of $\widetilde{\mathcal{R}}_m$, and it can be bounded as
    \begin{equation}
        r_m^*\leq C_6\left(d_x^{\frac{1}{2}}M\exp(-C_0'R^2/2)+\frac{S_fL_f\log\left(mL_fW_f(B_f\vee 1)(R\vee T)M\right)}{m}\right),
    \end{equation}
    for some absolute constant $C_6$.
    Moreover, we have
    \begin{equation}
        \left|\E_{(x,y)\sim\P}[\ell(x,y,s_{f,h})-\ell(x,y,s_*^\P)]
        - \E_{(x,y)\sim\P}[\widetilde{\ell}(x,y,f)]\right| \leq 2M\exp(-C_1'R^2).
    \end{equation}
    Combine this with \eqref{eq:bound_erm_1},\eqref{eq:bound_erm_2},
    \begin{equation}\label{eq:bound_erm_3}
        \begin{aligned}
            \E_{(x,y)\sim\P} [\ell(x,y,s_{f,h})-\ell(x,y,s_*^\P)]
            &\leq \frac{2}{m}\sum_{i=1}^m [\ell(x_i,y_i,s_{f,h})-\ell(x_i,y_i,s_*^\P)] \\
            &\qquad + C_5M\left(r_m^*+\frac{\log(\log(m)/\delta)}{m}+\exp(-C_1'R^2)\right), \\
            % &\E_{(x,y)\sim\P} [\widetilde{\ell}(x,y,f)]
            % \leq \frac{2}{m}\sum_{i=1}^m \widetilde{\ell}(x_i,y_i,f) + C_5M\left(r_m^*+\frac{\log(\log(m)/\delta)}{m}+\exp(-C_1'R^2)\right)
        \end{aligned}
    \end{equation}
    \begin{equation}\label{eq:bound_erm_4}
        \begin{aligned}
            \frac{1}{m}\sum_{i=1}^m [\ell(x_i,y_i,s_{f,h})-\ell(x_i,y_i,s_*^\P)]
            &\leq 2\E_{(x,y)\sim\P} [\ell(x,y,s_{f,h})-\ell(x,y,s_*^\P)] \\
            &\qquad + C_5M\left(r_m^*+\frac{\log(\log(m)/\delta)}{m}+\exp(-C_1'R^2)\right), \\
            % &\E_{(x,y)\sim\P} [\widetilde{\ell}(x,y,f)]
            % \leq \frac{2}{m}\sum_{i=1}^m \widetilde{\ell}(x_i,y_i,f) + C_5M\left(r_m^*+\frac{\log(\log(m)/\delta)}{m}+\exp(-C_1'R^2)\right)
        \end{aligned}
    \end{equation}
    
    Plug in the definition of $M=C\left(C_X^{''}R^6+M_f^2+d_x\Big(\frac{\log(1/T_0)}{T}+1\Big)\right)$ and let $R=C\log^{\frac{1}{2}}(md_xM_f/\delta)$ for some large constant $C$. Hence \eqref{eq:bound_erm_3} and \eqref{eq:bound_erm_4} reduce to
    \begin{align}
        \E_{(x,y)\sim\P} [\ell^\P(x,y,s_{f,h})]
        \leq \frac{2}{m}\sum_{i=1}^m [\ell(x_i,y_i,s_{f,h})-\ell(x_i,y_i,s_*^\P)] + C_7M_f^2\log^3(m/\delta)\left(r_m^\dagger+\frac{\log(\log(m)/\delta)}{m}\right), \\
        \frac{1}{m}\sum_{i=1}^m [\ell(x_i,y_i,s_{f,h})-\ell(x_i,y_i,s_*^\P)]
        \leq 2\E_{(x,y)\sim\P} [\ell^\P(x,y,s_{f,h})] + C_7M_f^2\log^3(m/\delta)\left(r_m^\dagger+\frac{\log(\log(m)/\delta)}{m}\right),
    \end{align}
    where $r_m^\dagger:=\frac{S_fL_f\log\left(mL_fW_f(B_f\vee 1)TM_f\log(1/\delta)\right)}{m}$.

    Therefore, we obtain that with probability no less than $1-\delta$, the population loss of the empirical minimizer $\widehat{f}$ can be bounded by
    \begin{equation}
        \begin{aligned}
            \E_{(x,y)\sim\P}[\ell^\P(x,y,s_{\widehat{f},h})]
            &\leq \frac{2}{m}\sum_{i=1}^m [\ell(x_i,y_i,s_{\widehat{f},h})-\ell(x_i,y_i,s_*^\P)] + 2C_7M_f^2\log^3(m/\delta)\left(r_m^\dagger+\frac{\log(1/\delta)}{m}\right) \\
            &\leq \inf_{f\in\mathcal{F}}\frac{2}{m}\sum_{i=1}^m [\ell(x_i,y_i,s_{f,h})-\ell(x_i,y_i,s_*^\P)] + 2C_7M_f^2\log^3(m/\delta)\left(r_m^\dagger+\frac{\log(1/\delta)}{m}\right) \\
            &\leq 4\inf_{f\in\mathcal{F}}\E_{(x,y)\sim\P}[\ell^\P(x,y,s_{f,h})] + 6C_7M_f^2\log^3(m/\delta)\left(r_m^\dagger+\frac{\log(1/\delta)}{m}\right),
        \end{aligned}
    \end{equation}
    We conclude the proof by noticing that $\E[X]=\int_0^\infty\P(X\geq x)\dif x$ and plugging in the bound above.
\end{proof}

\begin{prop}[Prop. \ref{prop:generalization_train_informal}]
\label{prop:generalization_train}
    There exists some constant $C_Z,C_R$ such that the following holds.
    For any $\P^1,\cdots,\P^K$, let $x_1^k,\cdots,x_n^k\overset{\textit{i.i.d.}}{\sim}\P^k$ for any $k$ and $(x_i^k)_{i,k}$ are all independent.
    Consider the empirical minimizer
    \begin{equation}
        \widehat{\vf}, \widehat{h}=\argmin_{\vf\in\mathcal{F}^{\otimes K},h\in\mathcal{H}} \frac{1}{nK}\sum_{k=1}^K\sum_{i=1}^n \ell(x_i^k,y_i^k,s_{f^k,h}).
    \end{equation}
    For any $\delta\in(0,1)$, if the configuration of $\mathcal{F}$ satisfies $R_f\geq C_R\log^{\frac{1}{2}}(nKM_f/\delta)$, then with probability no less than $1-\delta$,
    the population loss of $\widehat{\vf},\widehat{h}$ can be bounded by
    \begin{equation}
        \frac{1}{K}\sum_{k=1}^K\E_{(x,y)\sim\P^k}\ell^{\P^k}(x,y,s_{\widehat{f}^k,\widehat{h}})\leq \inf_{\vf\in\mathcal{F}^{\otimes K},h\in\mathcal{H}}\frac{4}{K}\sum_{k=1}^K\E_{(x,y)\sim\P^k}[\ell^\P(x,y,s_{f^k,h})] + C_Z\log^3(nK/\delta)\left(r_z+\frac{\log(1/\delta)}{nK}\right),
    \end{equation}
    where $r_z:=\frac{M_f^2\left[KS_fL_f\log\left(nL_fW_f(B_f\vee 1)M_fT\log(1/\delta)\right)+S_hL_h\log\left(nKL_hW_h(B_h\vee 1)M_f\gamma_f\log(1/\delta)\right)\right]}{nK}$.
\end{prop}

\begin{proof}
    Throughout the proof, we will use $z=(k,x,y)$ to denote the tuple of task index $k$ and data $(x,y)$.
    With a little abuse of notation, we will also let $s_*^k=s_*^{\P^k}$. 
    Consider the function class defined on $[K]\times\R^{d_x}\times[0,1]^{D_y}$,
    \begin{equation}
        \Phi=\left\{z=(k,x,y)\mapsto \widetilde{\ell}(z,\vf,h):=(\ell(x,y,s_{f^k,h})-\ell(x,y,s_*^k))\cdot\mathbbm{1}_{\|x\|_\infty\leq R}: \vf\in\mathcal{F}^{\otimes K},h\in\mathcal{H}\right\},
    \end{equation}
    where $1\leq R\leq \frac{R_f}{2}$ will be specified later. 
    It is easy to show that with probability no less than $1-2nK\exp(-C_1'R^2)$, it holds that $\|x_i^k\|_\infty\leq R$ for all $i,k$. 
    Hence by definition, the empirical minimizer also satisfies
    \begin{equation}
        \widehat{\vf},\widehat{h}=\argmin_{\vf\in\mathcal{F}^{\otimes K},h\in\mathcal{H}} \frac{1}{nK}\sum_{k=1}^K\sum_{i=1}^n\widetilde{\ell}(z_i^k,\vf,h).
    \end{equation}
    where $z_i^k=(k,x_i^k,y_i^k)$. 
    Below we reason conditioned on this event and verify the conditions in Lemma \ref{lem:local_rademacher_meta}.

    Following Step 1 and 2 in Proposition \ref{prop:generalization_test}, we have for any $\vf\in\mathcal{F}^{\otimes K},h\in\mathcal{H}$,
    \begin{equation}
        |\widetilde{\ell}(z,\vf,h)|\leq M:=C\left(C_X^{''}R^6+M_f^2+d_x\Big(\frac{\log(1/T_0)}{T}+1\Big)\right).
    \end{equation}
    \begin{equation}
        \frac{1}{K}\sum_{k=1}^K\E_{(x,y)\sim\P^k} [\widetilde{\ell}(z^k,\vf,h)^2] \leq \frac{4M}{K}\sum_{k=1}^K\E_{(x,y)\sim\P^k} [\widetilde{\ell}(z^k,\vf,h)] + 8M^2\exp(-C_1'R^2).
    \end{equation}
    For the local Rademacher complexity bound, note that
    \begin{equation}
        \Big\|\frac{1}{\sqrt{nK}} \sum_{k=1}^K\sum_{i=1}^n\sigma_i^k\widetilde{\ell}(z_i^k,\vf_1,h_1) - \frac{1}{\sqrt{nK}} \sum_{k=1}^K\sum_{i=1}^n\sigma_i^k\widetilde{\ell}(z_i^k,\vf_2,h_2) \Big\|_{\psi_2} \leq 4\|\widetilde{\ell}(\cdot,\vf_1,h_1)-\widetilde{\ell}(\cdot,\vf_2,h_2)\|_{L^2(\widehat{\P}_n^{(K)})},
    \end{equation}
    where $\widehat{\P}_n^{(K)}:=\frac{1}{nK}\sum_{k=1}^K\sum_{i=1}^n\delta_{z_i^k}$ and $\textbf{diam}\big(\Phi_r,\|\cdot\|_{L^2(\widehat{\P}_n^{(K)})}\big)\leq 2\sqrt{r}$.
    By Dudley's bound \citep{van2014probability,wainwright2019high}, there exists an absolute constant $C_0$ such that for any $\theta>0$,
    \begin{equation}\label{eq:rademacher_bound_meta_2}
        \mathcal{R}_{K,n}(\Phi_r)\leq C_0\left(\theta+\int_\theta^{2\sqrt{r}}\sqrt{\frac{\log\mathcal{N}(\Phi_r,\|\cdot\|_{L^2(\widehat{\P}_n^{(K)})},\varepsilon)}{nK}}\ \dif\varepsilon\right).
    \end{equation}
    Since $\|x_i^k\|_\infty\leq R$,
    \begin{equation}
        \begin{aligned}
            &\frac{1}{nK}\sum_{k=1}^K\sum_{i=1}^n(\widetilde{\ell}(z_i^k,\vf_1,h_1)-\widetilde{\ell}(z_i^k,\vf_2,h_2))^2 \\
            &\qquad= \frac{1}{nK}\sum_{k=1}^K\sum_{i=1}^n(\ell(x_i^k,y_i^k,s_{f_1^k,h_1})-\ell(x_i^k,y_i^k,s_{f_2^k,h_2}))^2 \\
            &\qquad\leq \frac{1}{nK}\sum_{k=1}^K\sum_{i=1}^n \left[\E_{t,x_t|x_i^k}\|f_1^k-f_2^k\|^2\right]\cdot\left[\E_{t,x_t|x_i^k}\|f_1^k+f_2^k-2\nabla_x\log\phi_t\|^2\right] \\
            &\qquad\leq \frac{4M}{nK}\sum_{k=1}^K\sum_{i=1}^n \E_{t,x_t|x_i^k}\|f_1^k(x_t,h_1(y_i^k),t)-f_2^k(x_t,h_2(y_i^k),t)\|^2 \\
            &\qquad\leq \frac{8M}{nK}\sum_{k=1}^K\sum_{i=1}^n \E_{t,x_t|x_i^k}\|f_1^k(x_t,h_1(y_i^k),t)-f_2^k(x_t,h_1(y_i^k),t)\|^2 \\
            &\qquad\qquad + \frac{8M}{nK}\sum_{k=1}^K\sum_{i=1}^n \E_{t,x_t|x_i^k}\|f_2^k(x_t,h_1(y_i^k),t)-f_2^k(x_t,h_2(y_i^k),t)\|^2.
        \end{aligned}
    \end{equation}
    Let $R_1=2R$. Since $x_t|x_i^k\sim\mathcal{N}(x_t;\alpha_tx_i^k,\sigma_t^2I)$, we have $\P(\|x_t\|_\infty\geq R_1)\leq d_x\P(|\mathcal{N}(0,1)|\leq R)\leq 2d_x\exp(-C_0'R^2)$ for some absolute constant $C_0'$.
    Therefore,
    \begin{equation}
        \begin{aligned}
            &\E_{t,x_t|x_i^k}\|f_1^k(x_t,h_1(y_i^k),t)-f_2^k(x_t,h_1(y_i^k),t)\|^2 \\
            &\qquad \leq \E_{t,x_t|x_i^k}[\mathbbm{1}_{\|x_t\|\leq R_1}] [\|f_1^k(x_t,h_1(y_i^k),t)-f_2^k(x_t,h_1(y_i^k),t)\|^2] + 8d_xM_f^2\exp(-C_0'R^2) \\
            &\qquad \leq \|f_1^k-f_2^k\|^2_{L^\infty(\Omega_{R_1})} + 8d_xM_f^2\exp(-C_0'R^2),
        \end{aligned}
    \end{equation}
    where $\Omega_{R_1}:=[-R_1,R_1]^{d_x}\times[0,1]^{d_y}\times[T_0,T]$. 
    Moreover, notice that $R_f\geq 2R=R_1$,
    \begin{equation}
        \begin{aligned}
            &\E_{t,x_t|x_i^k}\|f_2^k(x_t,h_1(y_i^k),t)-f_2^k(x_t,h_2(y_i^k),t)\|^2 \\
            &\qquad \leq \E_{t,x_t|x_i^k}[\mathbbm{1}_{\|x_t\|\leq R_f}] [\|f_2^k(x_t,h_1(y_i^k),t)-f_2^k(x_t,h_2(y_i^k),t)\|^2] + 8d_xM_f^2\exp(-C_0'R^2) \\
            &\qquad \leq \gamma_f^2\|h_1-h_2\|^2_{L^\infty([0,1]^{D_y})} + 8d_xM_f^2\exp(-C_0'R^2).
        \end{aligned}
    \end{equation}
    Plug in the bound above,
    \begin{equation}
        \begin{aligned}
            &\sqrt{\frac{1}{nK}\sum_{k=1}^K\sum_{i=1}^n(\widetilde{\ell}(z_i^k,\vf_1,h_1)-\widetilde{\ell}(z_i^k,\vf_2,h_2))^2} \\
            &\qquad \leq 8M^{\frac{1}{2}}\left(\max_k\|f_1^k-f_2^k\|_{L^\infty(\Omega_{R_1})}+\gamma_f\|h_1-h_2\|_{L^\infty([0,1]^{D_y})}\right) + 16d_x^{\frac{1}{2}}M\exp(-C_0'R^2/2).
        \end{aligned}
    \end{equation}
    For any $\varepsilon\geq 32d_x^{\frac{1}{2}}M\exp(-C_0'R^2/2)$, according to Lemma \ref{lem:covering_num},
    \begin{equation}
        \begin{aligned}
            &\log\mathcal{N}(\Phi_r,\|\cdot\|_{L^2(\widehat{\P}_n^{(K)})},\varepsilon) \\
            &\qquad \leq K\log\mathcal{N}(\mathcal{F},\|\cdot\|_{L^\infty(\Omega_{R_1})},\varepsilon/(16M^{\frac{1}{2}})) + \log\mathcal{N}(\mathcal{H},\|\cdot\|_{L^\infty([0,1]^{D_y})},\varepsilon/(16\gamma_fM^{\frac{1}{2}})) \\
            &\qquad \leq C_4KS_fL_f\log\left(\frac{L_fW_f(B_f\vee 1)(R\vee T)M}{\varepsilon}\right)+C_4S_hL_h\log\left(\frac{L_hW_h(B_h\vee 1)M\gamma_f}{\varepsilon}\right).
        \end{aligned}
    \end{equation}
    Plug in \eqref{eq:rademacher_bound_2} and let $\theta=32d_x^{\frac{1}{2}}M\exp(-C_0'R^2/2)$,
    \begin{equation}
        \begin{aligned}
            \mathcal{R}_{K,n}(\Phi_r)
            &\leq C_0\left(\theta+\int_\theta^{2\sqrt{r}}\sqrt{\frac{C_4KS_fL_f\log\left(\frac{L_fW_f(B_f\vee 1)(R\vee T)M}{\varepsilon}\right)+C_4S_hL_h\log\left(\frac{L_hW_h(B_h\vee 1)M\gamma_f}{\varepsilon}\right)}{nK}}\dif\varepsilon\right) \\
            &\leq C_0\sqrt{\frac{C_4'\left[KS_fL_f\log\left(\frac{L_fW_f(B_f\vee 1)(R\vee T)M}{r}\right)+ S_hL_h\log\left(\frac{L_hW_h(B_h\vee 1)M\gamma_f}{r}\right)\right]\cdot r}{nK}} \\
            &\qquad + C_032d_x^{\frac{1}{2}}M\exp(-C_0'R^2/2) \\
            &=: \widetilde{\mathcal{R}}_{K,n}(r).
        \end{aligned}
    \end{equation}

    Combine the arguments above, by Lemma \ref{lem:local_rademacher_meta} with $B_0=8M^2\exp(-C_1'R^2),B=4M,b=M$, it holds that with probability no less than $1-2nK\exp(-C_1'R^2)-\delta/2$, for any $\vf\in\mathcal{F}^{\otimes K},h\in\mathcal{H}$,
    \begin{equation}\label{eq:bound_erm_meta_1}
        \begin{aligned}
            \E_{z\sim\widehat{\P}^{(K)}} [\widetilde{\ell}(z,\vf,h)]
            &\leq \frac{2}{nK}\sum_{k=1}^K\sum_{i=1}^n \widetilde{\ell}(z_i^k,\vf,h) + C_5M\left(r_{K,n}^*+\frac{\log(\log(nK)/\delta)}{nK}\right) \\
            &\qquad + C_5\sqrt{\frac{M^2\log(\log(nK)/\delta)}{nK}}\exp(-C_1'R^2),
        \end{aligned}
    \end{equation}
    \begin{equation}\label{eq:bound_erm_meta_2}
        \begin{aligned}
            \frac{1}{nK}\sum_{k=1}^K\sum_{i=1}^n \widetilde{\ell}(z_i^k,\vf,h)
            &\leq 2\E_{z\sim\widehat{\P}^{(K)}} [\widetilde{\ell}(z,\vf,h)] + C_5M\left(r_{K,n}^*+\frac{\log(\log(nK)/\delta)}{nK}\right) \\
            &\qquad + C_5\sqrt{\frac{M^2\log(\log(nK)/\delta)}{nK}}\exp(-C_1'R^2).
        \end{aligned}
    \end{equation}
    where $r_{K,n}^*$ is the largest fixed point of $\widetilde{\mathcal{R}}_{K,n}$, and it can be bounded by
    \begin{equation}
        r_{K,n}^*\leq C_6\left(d_x^{\frac{1}{2}}M_f\exp(-C_0'R^2/2)+\frac{KS_fL_f\log\left(nL_fW_f(B_f\vee 1)(R\vee T)M\right)+S_hL_h\log\left(nKL_hW_h(B_h\vee 1)M\gamma_f\right)}{nK}\right),
    \end{equation}
    for some absolute constant $C_6$.
    Moreover, we have
    \begin{equation}
        \left|\frac{1}{K}\sum_{k=1}^K\E_{(x,y)\sim\P^k}[\ell(x,y,s_{f^k,h})-\ell(x,y,s_*^k)]
        - \E_{z\sim\widehat{\P}^{(K)}}[\widetilde{\ell}(z,\vf,h)] \right|\leq 2M\exp(-C_1'R^2).
    \end{equation}
    Combine this with \eqref{eq:bound_erm_meta_1},\eqref{eq:bound_erm_meta_2},
    \begin{equation}\label{eq:bound_erm_meta_3}
        \begin{aligned}
            \frac{1}{K}\sum_{k=1}^K\E_{(x,y)\sim\P^k} [\ell(x,y,s_{f^k,h})-\ell(x,y,s_*^\P)]
            &\leq \frac{2}{nK}\sum_{k=1}^K\sum_{i=1}^n [\ell(x_i^k,y_i^k,s_{f^k,h})-\ell(x_i^k,y_i^k,s_*^k)] \\
            &\qquad + C_5M\left(r_{K,n}^*+\frac{\log(\log(nK)/\delta)}{nK}+\exp(-C_1'R^2)\right), \\
            % &\E_{(x,y)\sim\P} [\widetilde{\ell}(x,y,f)]
            % \leq \frac{2}{m}\sum_{i=1}^m \widetilde{\ell}(x_i,y_i,f) + C_5M\left(r_m^*+\frac{\log(\log(m)/\delta)}{m}+\exp(-C_1'R^2)\right)
        \end{aligned}
    \end{equation}
    \begin{equation}\label{eq:bound_erm_meta_4}
        \begin{aligned}
            \frac{1}{nK}\sum_{k=1}^K\sum_{i=1}^n [\ell(x_i^k,y_i^k,s_{f^k,h})-\ell(x_i^k,y_i^k,s_*^k)]
            &\leq \frac{2}{K}\sum_{k=1}^K\E_{(x,y)\sim\P^k} [\ell(x,y,s_{f^k,h})-\ell(x,y,s_*^\P)] \\
            &\qquad + C_5M\left(r_{K,n}^*+\frac{\log(\log(nK)/\delta)}{nK}+\exp(-C_1'R^2)\right), \\
            % &\E_{(x,y)\sim\P} [\widetilde{\ell}(x,y,f)]
            % \leq \frac{2}{m}\sum_{i=1}^m \widetilde{\ell}(x_i,y_i,f) + C_5M\left(r_m^*+\frac{\log(\log(m)/\delta)}{m}+\exp(-C_1'R^2)\right)
        \end{aligned}
    \end{equation}
    
    Plug in the definition of $M=C\left(C_X^{''}R^6+M_f^2+d_x\Big(\frac{\log(1/T_0)}{T}+1\Big)\right)$ and define $R=C'\log^{\frac{1}{2}}(nKd_xM_f/\delta)$ for some large constant $C'$. Hence \eqref{eq:bound_erm_meta_3} and \eqref{eq:bound_erm_meta_4} reduce to
    \begin{equation}
        \begin{aligned}
            \frac{1}{K}\sum_{k=1}^K\E_{(x,y)\sim\P^k} [\ell(x,y,s_{f^k,h})-\ell(x,y,s_*^\P)]
            &\leq \frac{2}{nK}\sum_{k=1}^K\sum_{i=1}^n [\ell(x_i^k,y_i^k,s_{f^k,h})-\ell(x_i^k,y_i^k,s_*^k)] \\
            &\qquad + C_7M_f^2\log^3(nK/\delta)\left(r_{K,n}^\dagger+\frac{\log(\log(nK)/\delta)}{nK}\right),
        \end{aligned}
    \end{equation}
    \begin{equation}
        \begin{aligned}
            \frac{1}{nK}\sum_{k=1}^K\sum_{i=1}^n [\ell(x_i^k,y_i^k,s_{f^k,h})-\ell(x_i^k,y_i^k,s_*^k)]
            &\leq \frac{2}{K}\sum_{k=1}^K\E_{(x,y)\sim\P^k} [\ell(x,y,s_{f^k,h})-\ell(x,y,s_*^\P)] \\
            &\qquad + C_7M_f^2\log^3(nK/\delta)\left(r_{K,n}^\dagger+\frac{\log(\log(nK)/\delta)}{nK}\right),
        \end{aligned}
    \end{equation}
    where $r_{K,n}^\dagger:=\frac{KS_fL_f\log\left(nL_fW_f(B_f\vee 1)M_fT\log(1/\delta)\right)+S_hL_h\log\left(nKL_hW_h(B_h\vee 1)M_f\gamma_f\log(1/\delta)\right)}{nK}$.

    Therefore, we obtain that with probability no less than $1-\delta$, the population loss of the empirical minimizer $\widehat{\vf},\widehat{h}$ can be bounded by
    \begin{equation}
        \begin{aligned}
            &\frac{1}{K}\sum_{k=1}^K\E_{(x,y)\sim\P^k}[\ell^{\P^k}(x,y,s_{\widehat{f}^k,\widehat{h}})] \\
            &\qquad\leq \frac{2}{nK}\sum_{k=1}^K\sum_{i=1}^m [\ell(x_i^k,y_i^k,s_{\widehat{f},h})-\ell(x_i^k,y_i^k,s_*^k)] + 2C_7M_f^2\log^3(nK/\delta)\left(r_{K,n}^\dagger+\frac{\log(1/\delta)}{nK}\right) \\
            &\qquad\leq \inf_{\vf\in\mathcal{F}^{\otimes K},h\in\mathcal{H}}\frac{2}{nK}\sum_{k=1}^K\sum_{i=1}^n [\ell(x_i^k,y_i^k,s_{f^k,h})-\ell(x_i^k,y_i^k,s_*^k)] + 2C_7M_f^2\log^3(nK/\delta)\left(r_{K,n}^\dagger+\frac{\log(1/\delta)}{nK}\right) \\
            &\qquad\leq \inf_{\vf\in\mathcal{F}^{\otimes K},h\in\mathcal{H}}\frac{4}{K}\sum_{k=1}^K\E_{(x,y)\sim\P^k}[\ell^\P(x,y,s_{f^k,h})] + 6C_7M_f^2\log^3(nK/\delta)\left(r_{K,n}^\dagger+\frac{\log(1/\delta)}{nK}\right),
        \end{aligned}
    \end{equation}
    which concludes the proof.
\end{proof}

\begin{thm}[Thm. \ref{thm:generalization_all_diversity_informal}]
\label{thm:generalization_all_diversity}
    Under Assumption \ref{asp:sub_gaussian}, \ref{asp:low_dim}, \ref{asp:lip}, suppose $\P^1,\cdots,\P^K$ are $(\nu,\Delta)$-diverse over target distribution $\P^0$ given $\mathcal{F},\mathcal{H}$.
    There exists some constant $C,C_R$ such that the following holds.
    Define the empirical minimizer of training task and new task as
    \begin{equation}
        \widehat{\vf},\widehat{h}=\argmin_{\vf\in\mathcal{F}^{\otimes K},h\in\mathcal{H}} \frac{1}{nK}\sum_{k=1}^K\sum_{i=1}^n\ell(x_i^k,y_i^k,s_{f^k,h}),
    \end{equation}
    \begin{equation}
        \widehat{f}^{\P^0}:= \argmin_{f\in\mathcal{F}} \frac{1}{m}\sum_{i=1}^m \ell(x_i^0,y_i^0,s_{f,\widehat{h}}).
    \end{equation}
    If $R_f\geq C_R\log^{\frac{1}{2}}(nKM_f/\delta)$,
    then with probability no less than $1-\delta$, the expected population loss of new task can be bounded by
    \begin{equation}
        \begin{aligned}
            \E_{\{(x_i,y_i)\}_{i=1}^m} \E_{(x,y)\sim\P^0} [\ell^{\P^0}(x,y,s_{\widehat{f}^{\P^0},\widehat{h}})]
            &\lesssim \frac{1}{\nu}\inf_{h\in\mathcal{H}} \frac{1}{K}\sum_{k=1}^K \inf_{f\in\mathcal{F}}\E_{(x,y)\sim\P^k} [\ell^{\P^k} (x,y,s_{f,h})] + \Delta \\
            &\quad +C \left(\frac{\log^3(m)\log\mathcal{N}_\mathcal{F}}{m} + \frac{\log^3(nK/\delta)(K\log\mathcal{N}_\mathcal{F}+\log(\mathcal{N}_\mathcal{H}/\delta))}{\nu nK}\right).
        \end{aligned}
    \end{equation}
    where
    \begin{equation}
        \log\mathcal{N}_\mathcal{F}:=M_f^2S_fL_f\log\left(mnL_fW_f(B_f\vee 1)M_fT\log(1/\delta)\right),
    \end{equation}
    \begin{equation}
        \log\mathcal{N}_\mathcal{H}:=S_hL_h\log\left(nKL_hW_h(B_h\vee 1)M_f\gamma_f\log(1/\delta)\right).
    \end{equation}
\end{thm}

\begin{proof}
    \begin{equation}
        \begin{aligned}
            &\E_{\{(x_i,y_i)\}_{i=1}^m} \E_{(x,y)\sim\P^0} [\ell^{\P^0}(x,y,s_{\widehat{f}^{\P^0},\widehat{h}})] \\
            &\qquad \lesssim \inf_{f\in\mathcal{F}} \E_{(x,y)\sim\P^0} [\ell^\P(x,y,s_{f,\widehat{h}})]+C_{xy}\log^3(m)r_x \\
            &\qquad \lesssim \frac{1}{\nu K}\sum_{k=1}^K\inf_{f\in\mathcal{F}} \E_{(x,y)\sim\P^k} [\ell^{\P^k}(x,y,s_{f,\widehat{h}})]+\Delta+C_{xy}\log^3(m)r_x \\
            &\qquad \lesssim \frac{1}{\nu K}\sum_{k=1}^K \E_{(x,y)\sim\P^k} [\ell^{\P^k}(x,y,s_{\widehat{f}^k,\widehat{h}})]+\Delta+C_{xy}\log^3(m)r_x \\
            &\qquad \lesssim \frac{1}{\nu}\inf_{\vf\in\mathcal{F}^{\otimes K},h\in\mathcal{H}} \frac{1}{K}\sum_{k=1}^K \E_{(x,y)\sim\P^k} [\ell^{\P^k}(x,y,s_{f^k,h})]+\frac{1}{\nu}C_Z\log^3(nK/\delta)\left(r_z+\frac{\log(1/\delta)}{nK}\right) \\
            &\qquad\qquad +\Delta+C_{xy}\log^3(m)r_x.
        \end{aligned}
    \end{equation}
    Here we apply Proposition \ref{prop:generalization_test} in the first inequality, task diversity in the second inequality, and Proposition \ref{prop:generalization_train} in the fourth. 
    Plug in the definition of $r_z,r_x$ and $\log\mathcal{N}_\mathcal{F},\log\mathcal{N}_\mathcal{H}$ and we complete the proof.
\end{proof}

\subsection{Proofs of Meta-Learning}\label{app:subsec:generalization}

\begin{prop}[Prop. \ref{prop:generalization_meta_informal}]\label{prop:generalization_meta}
    There exists some constants $C_1',C_P$, such that for $\P^1,\cdots,\P^K\overset{\textit{i.i.d.}}{\sim}\Pmeta$, 
    with probability no less than $1-\delta$, we have for any $h\in\mathcal{H}$,
    \begin{align}
        &\E_{\P\sim\Pmeta}\mathcal{L}(\P,h)
        \leq \frac{2}{K}\sum_{k=1}^K\mathcal{L}(\P^k,h)+C_P\left(r_P+\frac{\log(1/\delta)}{K}\right), \\
        &\frac{1}{K}\sum_{k=1}^K\mathcal{L}(\P^k,h)\leq 2\E_{\P\sim\Pmeta}\mathcal{L}(\P,h)
        +C_P\left(r_P+\frac{\log(1/\delta)}{K}\right),
    \end{align}
    where $r_P=M_f^2\exp(-C_1'R_f^2)+\frac{S_hL_h\log\left(KL_hW_h(B_h\vee 1)M_f\gamma_f\right)}{K}$.
\end{prop}

\begin{proof}
    Given $\P^1,\cdots,\P^K\overset{\textit{i.i.d.}}{\sim}\Pmeta$, we define the empirical Rademacher complexity of a function class $\Phi$ defined on the set of distribution $\mathcal{P}(\R^{d_x}\times[0,1]^{D_y})$ as
    \begin{equation}
        \mathcal{R}_K(\Phi):=\E_{\bm{\sigma}} \sup_{\varphi\in\Phi}\Big|\frac{1}{K}\sum_{k=1}^K\sigma_k\varphi(\P^k)\Big|,\ \bm{\sigma}\sim \text{Unif}(\{-1,1\}^K).
    \end{equation}
    For any $r>0$, let $\mathcal{H}_r:=\Big\{h\in\mathcal{H}:\frac{1}{K}\sum_{k=1}^K(\mathcal{L}(\P^k,h))^2\leq r\Big\}$ and $\Phi_r:=\{\mathcal{L}(\cdot,h):h\in\mathcal{H}_r\}$.
    Note that for any $\varphi_1,\varphi_2\in\Phi_r$,
    \begin{equation}
        \begin{aligned}
            \Big\|\frac{1}{\sqrt{K}}\sum_{k=1}^K\sigma_k\varphi_1(\P^k)-\frac{1}{\sqrt{K}}\sum_{k=1}^K\sigma_k\varphi_2(\P^k)\Big\|_{\psi_2}
            &\leq 4\sqrt{\frac{1}{K}\sum_{k=1}^K\|\varphi_1(\P^k)-\varphi_2(\P^k)\|^2}\\
            &=4\|\varphi_1-\varphi_2\|_{L^2(\Pmeta^{(K)})},
        \end{aligned}
    \end{equation}
    where $\Pmeta^{(K)}:=\frac{1}{K}\sum_{k=1}^K\delta_{\P^k}$
    and $\textbf{diam}\big(\Phi_r,\|\cdot\|_{L^2(\Pmeta^{(K))}}\big)\leq 2\sqrt{r}$.
    Then by Dudley's bound \citep{van2014probability,wainwright2019high}, there exists an absolute constant $C_0$ such that for any $\theta\geq 0$,
    \begin{equation}\label{eq:rademacher_bound_1}
        \mathcal{R}_K(\Phi_r)\leq C_0\left(\theta+\int_\theta^{2\sqrt{r}}\sqrt{\frac{\log\mathcal{N}(\Phi_r,\|\cdot\|_{L^2(\Pmeta^{(K)})},\varepsilon)}{K}}\ \dif\varepsilon\right).
    \end{equation}
    For any $\P$ and $h_1,h_2\in\mathcal{H}_r$, denote the minimizer of \eqref{eq:L(P,h)} in $\mathcal{F}$ as $f_1,f_2$, respectively.
    Without loss of generality, suppose $\mathcal{L}(\P,h_1)\geq\mathcal{L}(P,h_2)$. Then 
    \begin{equation}
        \begin{aligned}
        \mathcal{L}(\P,h_1)-\mathcal{L}(P,h_2)
        &\leq \E_{t,x_t,y} \left[\Big|\|f_2(x_t,h_1(y),t)-\nabla_x\log p_t(x_t|y)\|^2-\|f_2(x_t,h_2(y),t)-\nabla_x\log p_t(x_t|y)\|^2 \Big|\right] \\
        &\leq \E_{t,x_t,y} \left[\|f_2(x_t,h_1(y),t)-f_2(x_t,h_2(y),t)\|\right. \\
        &\qquad\qquad\qquad  \left.\times\|f_2(x_t,h_1(y),t)+f_2(x_t,h_2(y),t)-2\nabla_x\log p_t(x_t|y)\|\right] \\
        &\leq \sqrt{\E_{t,x_t,y} \left[\|f_2(x_t,h_1(y),t)-f_2(x_t,h_2(y),t)\|^2\right]}\cdot 8(M_f+C_L^{1/2}) \\
        \end{aligned}
    \end{equation}
    In the last inequality we apply $\|f_i\|\leq M_f$ and $\E_{t,x_t,y}\|\nabla_x\log p_t(x_t|y)\|^2\leq C_L$ by Lemma \ref{lem:bound L(P,h)}. 
    Moreover,
    \begin{equation}
        \begin{aligned}
            &\E_{(t,x_t,y)}\left[\|f_2(x_t,h_1(y),t)-f_2(x_t,h_2(y),t)\|^2\right] \\
            &\leq \E_{t,y} \left[\int \|f_2(x_t,h_1(y),t)-f_2(x_t,h_2(y),t)\|^2 p_t(x_t|y)dx_t\right] \\
            &\leq \E_{t,y} \left[\int_{\|x_t\|_\infty\leq R_f} \|f_2(x_t,h_1(y),t)-f_2(x_t,h_2(y),t)\|^2 p_t(x_t|y)dx_t + 4M_f^2\P(\|x_t\|_\infty>R_f|y)\right] \\
            &\leq \gamma_f^2 \E_y[\|h_1(y)-h_2(y)\|^2]+8M_f^2\exp(-C_1'R_f^2) \\
            &\leq \gamma_f^2\|h_1-h_2\|^2_{L^\infty([0,1]^{D_y})}+8M_f^2\exp(-C_1'R_f^2)
        \end{aligned}
    \end{equation}
    Therefore, let $C_3=32(M_f+C_L^{1/2})M_f\leq 64M_f^2$ and we have
    \begin{equation}
        |\mathcal{L}(\P,h_1)-\mathcal{L}(P,h_2)|\leq C_3\left(\gamma_f \|h_1-h_2\|_{L^\infty([0,1]^{D_y})}+\exp(-C_1'R_f^2)\right),
    \end{equation}
    which implies that when $\varepsilon\geq 2C_3\exp(-C_1'R_f^2)$, by Lemma \ref{lem:covering_num},
    \begin{equation}
        \begin{aligned}
            \log\mathcal{N}(\Phi_r,\|\cdot\|_{L^2(\Pmeta^{(K)})},\varepsilon)
            &\leq \log\mathcal{N}(\mathcal{H}_r,\|\cdot\|_{L^\infty([0,1]^{D_y})},\varepsilon/(2C_3\gamma_f))\\
            &\leq C_4S_hL_h\log\left(\frac{L_hW_h(B_h\vee 1)C_3\gamma_f}{\varepsilon}\right).
        \end{aligned}
    \end{equation}
    Plug in \eqref{eq:rademacher_bound_1} and let $\theta=2C_3\exp(-C_1'R_f^2)$,
    \begin{equation}
        \begin{aligned}
            \mathcal{R}_K(\Phi_r)
            &\leq C_0\left(\theta+\int_\theta^{2\sqrt{r}}\sqrt{\frac{C_4S_hL_h\log\left(\frac{L_hW_h(B_h\vee 1)C_3\gamma_f}{\varepsilon}\right)}{K}}\ \dif\varepsilon\right) \\
            &\leq C_0\left(2C_3\exp(-C_1'R_f^2)+\sqrt{\frac{C_4'S_hL_h\log\left(\frac{L_hW_h(B_h\vee 1)M_f\gamma_f}{r}\right)\cdot r}{K}} \right) \\
            &=: \widetilde{\mathcal{R}}_K(r).
        \end{aligned}
    \end{equation}
    According to Lemma \ref{lem:local_rademacher} (by setting $B_0=0,B=b=C_L$), for some absolute constant $C_5$, with probability no less than $1-\delta$, we have for any $h\in\mathcal{H}$,
    \begin{align}
        &\E_{\P\sim\Pmeta}\mathcal{L}(\P,h)
        \leq \frac{2}{K}\sum_{k=1}^K\mathcal{L}(\P^k,h)+C_5C_L\left(r_K^*+\frac{\log(\log (K)/\delta)}{K}\right), \\
        &\frac{1}{K}\sum_{k=1}^K\mathcal{L}(\P^k,h)\leq 2\E_{\P\sim\Pmeta}\mathcal{L}(\P,h)
        +C_5C_L\left(r_K^*+\frac{\log(\log (K)/\delta)}{K}\right),
    \end{align}
    where $r_K^*$ is the unique fixed point of $\widetilde{\mathcal{R}}_K$.
    And it is easy to show that for some absolute constant $C_6$,
    \begin{equation}
        r_K^*\leq C_6\left(C_3\exp(-C_1'R_f^2)+\frac{S_hL_h\log\left(KL_hW_h(B_h\vee 1)M_f\gamma_f\right)}{K}\right).
    \end{equation}
    which concludes the proof.
\end{proof}

\begin{thm}[Thm. \ref{thm:generalization_all_informal}]
\label{thm:generalization_all}
    Under Assumption \ref{asp:sub_gaussian}, \ref{asp:low_dim}, \ref{asp:lip},
    there exists some constant $C,C_R$ such that the following holds.
    Define the empirical minimizer of training task and new task as
    \begin{equation}
        \widehat{\vf},\widehat{h}=\argmin_{\vf\in\mathcal{F}^{\otimes K},h\in\mathcal{H}} \frac{1}{nK}\sum_{k=1}^K\sum_{i=1}^n\ell(x_i^k,y_i^k,s_{f^k,h}),
    \end{equation}
    \begin{equation}
        \widehat{f}^\P:= \argmin_{f\in\mathcal{F}} \frac{1}{m}\sum_{i=1}^m \ell(x_i,y_i,s_{f,\widehat{h}}).
    \end{equation}
    If $R_f\geq C_R\log^{\frac{1}{2}}(nKM_f/\delta)$,
    then with probability no less than $1-\delta$, the expected population loss of new task can be bounded by
    \begin{equation}
        \begin{aligned}
            &\E_{\P\sim\Pmeta}\E_{\{(x_i,y_i)\}_{i=1}^m\sim \P} \E_{(x,y)\sim\P} [\ell^\P(x,y,s_{\widehat{f}^\P,\widehat{h}})] \\
            &\qquad \lesssim \inf_{h\in\mathcal{H}} \E_{\P\sim\Pmeta} \inf_{f\in\mathcal{F}}\E_{(x,y)\sim\P} [\ell^\P (x,y,s_{f,h})] + C \left(\frac{\log^3(m)\log\mathcal{N}_\mathcal{F}}{m} + \frac{\log^3(nK/\delta)\log\mathcal{N}_\mathcal{F}}{n}+\frac{\log(\mathcal{N}_\mathcal{H}/\delta)}{K}\right),
        \end{aligned}
    \end{equation}
    where
    \begin{equation}
        \log\mathcal{N}_\mathcal{F}:=M_f^2S_fL_f\log\left(mnL_fW_f(B_f\vee 1)M_fT\log(1/\delta)\right),
    \end{equation}
    \begin{equation}
        \log\mathcal{N}_\mathcal{H}:=S_hL_h\log\left(nKL_hW_h(B_h\vee 1)M_f\gamma_f\log(1/\delta)\right).
    \end{equation}
\end{thm}

\begin{proof}
    \begin{equation}
        \begin{aligned}
            &\E_{\P\sim\Pmeta}\E_{\{(x_i,y_i)\}_{i=1}^m\sim \P} \E_{(x,y)\sim\P} [\ell^\P(x,y,s_{\widehat{f}^\P,\widehat{h}})] \\
            &\qquad \lesssim \E_{\P\sim\Pmeta} \inf_{f\in\mathcal{F}} \E_{(x,y)\sim\P} [\ell^\P(x,y,s_{f,\widehat{h}})]+C_{xy}\log^3(m)r_x \\
            &\qquad \lesssim \frac{1}{K}\sum_{k=1}^K\inf_{f\in\mathcal{F}} \E_{(x,y)\sim\P^k} [\ell^{\P^k}(x,y,s_{f,\widehat{h}})]+C_P\left(r_P+\frac{\log(1/\delta)}{K}\right)+C_{xy}\log^3(m)r_x \\
            &\qquad \lesssim \frac{1}{K}\sum_{k=1}^K \E_{(x,y)\sim\P^k} [\ell^{\P^k}(x,y,s_{\widehat{f}^k,\widehat{h}})]+C_P\left(r_P+\frac{\log(1/\delta)}{K}\right)+C_{xy}\log^3(m)r_x \\
            &\qquad \lesssim \inf_{\vf\in\mathcal{F}^{\otimes K},h\in\mathcal{H}} \frac{1}{K}\sum_{k=1}^K \E_{(x,y)\sim\P^k} [\ell^{\P^k}(x,y,s_{f^k,h})]+C_Z\log^3(nK/\delta)\left(r_z+\frac{\log(1/\delta)}{nK}\right) \\
            &\qquad\qquad +C_P\left(r_P+\frac{\log(1/\delta)}{K}\right)+C_{xy}\log^3(m)r_x \\
            &\qquad \lesssim \inf_{h\in\mathcal{H}} \E_{\P\sim\Pmeta} \inf_{f\in\mathcal{F}}\E_{(x,y)\sim\P} [\ell^\P (x,y,s_{f,h})]+C_Z\log^3(nK/\delta)\left(r_z+\frac{\log(1/\delta)}{nK}\right) \\
            &\qquad\qquad +C_P\left(r_P+\frac{\log(1/\delta)}{K}\right)+C_{xy}\log^3(m)r_x.
        \end{aligned}
    \end{equation}
    Here we apply Proposition \ref{prop:generalization_test} in the first inequality, Proposition \ref{prop:generalization_meta} in the second and last inequality, Proposition \ref{prop:generalization_train} in the fourth. 
    Plugging in the definition of $r_z,r_P,r_x$ and $\log\mathcal{N}_\mathcal{F},\log\mathcal{N}_\mathcal{H}$ and
    noticing that $R_f\geq C_R\log^{\frac{1}{2}}(nKd_xM_f/\delta)\geq C_R'\log^{\frac{1}{2}}\left(\frac{M_fK}{\log\mathcal{N}_\mathcal{H}}\right)$, we have with probability no less than $1-\delta$,
    \begin{equation}
        \begin{aligned}
            &\E_{\P\sim\Pmeta}\E_{\{(x_i,y_i)\}_{i=1}^m\sim \P} \E_{(x,y)\sim\P} [\ell^\P(x,y,s_{\widehat{f}^\P,\widehat{h}})] \\
            &\qquad \lesssim \inf_{h\in\mathcal{H}} \E_{\P\sim\Pmeta} \inf_{f\in\mathcal{F}}\E_{(x,y)\sim\P} [\ell^\P (x,y,s_{f,h})] + C \left(\frac{\log^3(m)\log\mathcal{N}_\mathcal{F}}{m} + \frac{\log^3(nK/\delta)\log\mathcal{N}_\mathcal{F}}{n}+\frac{\log(\mathcal{N}_\mathcal{H}/\delta)}{K}\right).
        \end{aligned}
    \end{equation}
\end{proof}

\subsection{Auxiliary Lemmas}

\begin{lemma}\label{lem:bound L(P,h)}
    There exists some constant $C_L$ such that for any $h,\P$, 
    \begin{equation}
        \mathcal{L}(\P,h)\leq\E_{t,x_t,y}\|\nabla_x\log p_t(x_t|y)\|^2\leq C_L.
    \end{equation} 
\end{lemma}

\begin{proof}
    Note that
    \begin{equation}
        \begin{aligned}
            \E_{(x,y)\sim\P}[\ell^\P(x,y,s_{f,h})]
            &=\E_{(x,y)\sim\P}\E_{t,x_t|x}[\|f(x_t,h(y),t)-\nabla_x\log p_t(x_t|y)\|^2] \\
            &= \E_{t,x_t,y}[\|f(x_t,h(y),t)-\nabla_x\log p_t(x_t|y)\|^2]
        \end{aligned}
    \end{equation}
    and $0\in\mathcal{F}$, it suffices to show that $\E_{t,x_t,y}[\|\nabla_x\log p_t(x_t|y)\|^2]$ is uniformly bounded for any $\P,h$.
    According to \eqref{eq:score_1}, 
    \begin{equation}
        \begin{aligned}
        \E_{x_t,y}[\|\nabla_x\log p_t(x_t|y)\|^2]
        &\leq \E_{x_t,y}\E_{x_0|(x_t,y)}[\|\nabla_x\log \phi_t(x_t|x_0)\|^2] \\
        &= \E_{x_0,y}\E_{x_t|x_0}[\|\nabla_x\log \phi_t(x_t|x_0)\|^2] \\
        &=\frac{d_x}{\sigma_t^2}=\frac{d_x}{1-e^{-2t}}.
        \end{aligned}
    \end{equation}
    On the other hand, by \eqref{eq:score_2} and Assumption \ref{asp:lip},
    \begin{equation}
        \begin{aligned}
        \E_{x_t,y}[\|\nabla_x\log p_t(x_t|y)\|^2]
        &\leq \E_{x_t,y}\E_{x_0|(x_t,y)}[\|\nabla_x\log p(x_0|y)\|^2\cdot e^{2t}] \\
        &= \E_{x_0,y}\E_{x_t|x_0}[\|\nabla_x\log p(x_0|y)\|^2\cdot e^{2t}] \\
        &=\E_{x_0,y}[\|\nabla_x\log p(x_0|y)\|^2/\alpha_t^2] \\
        &\leq\E_{x_0,y}[(B+L\|x_0\|)^2\cdot e^{2t}] \\
        &\leq C_2'e^{2t}
        \end{aligned}
    \end{equation}
    Therefore, we have
    \begin{equation}
        \begin{aligned}
            \mathcal{L}(\P,h)
            &\leq \E_{t,x_t,y}[\|\nabla_x\log p_t(x_t|y)\|^2] \\
            &\leq \E_t[\frac{d_x}{1-e^{-2t}}\wedge C_2'e^{2t}] \\
            &\leq 2(C_2'+d_x)=:C_L.
        \end{aligned}
    \end{equation}
\end{proof}

\begin{lemma}\label{lem:bound score_t}
    There exists some constant $C_X^{''}$ such that for any $t\in[0,T]$ and $x\in\R^{d_x},y\in[0,1]^{D_y}$,
    \begin{equation}
        \E_{x_t|x}\|\nabla_x\log p_t(x_t|y)\|^2\leq C_X^{''}(\|x\|^6+1).
    \end{equation}
\end{lemma}

\begin{proof}
    Note that $x_t|x\sim \mathcal{N}(x_t|\alpha_tx,\sigma_t^2I)$ and by Lemma \ref{lem:lip_score},
    \begin{equation}\label{eq:score_bound_1}
        \E_{x_t|x}\|\nabla_x\log p_t(x_t|y)\|^2
        \leq \E_{x_t|x} 2\Big[\|\nabla_x\log p_t(0|y)\|^2+(C_X+C_X'\|x_t\|^2)^2\|x_t\|^2\Big]
    \end{equation}
    Let $q_t(x_0|x_t,y)\propto \phi_t(x_t|x_0)p(x_0|y)$.
    Since $\phi_t(0|x_0)\propto \exp\left(-\frac{\alpha_t^2\|x\|^2}{2\sigma_t^2}\right)$ is decreasing in $\|x\|$, by Fortuin–Kasteleyn–Ginibre inequality,
    \begin{equation}
        \E_{q_t(x_0|0,y)}\|x_0\|^2\leq \E_{p(x_0|y)}\|x_0\|^2\leq C_0.
    \end{equation}
    According to \eqref{eq:score_1},
    \begin{equation}
        \|\nabla_x\log p_t(0|y)\|^2
        \leq \frac{\alpha_t^2}{\sigma_t^4}\E_{q_t(x_0|0,y)}\|x_0\|^2\leq \frac{C_0\alpha_t^2}{\sigma_t^4}.
    \end{equation}
    By \eqref{eq:score_2}, we also have
    \begin{equation}
        \|\nabla_x\log p_t(0|y)\|^2
        \leq \frac{1}{\alpha_t^2}\E_{q_t(x_0|0,y)}\|\nabla_x\log p(x_0|y)\|^2\leq \frac{1}{\alpha_t^2}\E_{q_t(x_0|0,y)}[(B+L\|x_0\|)^2]\leq \frac{2(B^2+L^2C_0)}{\alpha_t^2}.
    \end{equation}
    Combine the two inequalities,
    \begin{equation}
        \|\nabla_x\log p_t(0|y)\|^2\leq (B^2+(L^2+1)C_0)\cdot(\frac{\alpha_t^2}{\sigma_t^4}\wedge\frac{1}{\alpha_t^2})\leq 2(B^2+(L^2+1)C_0).
    \end{equation}
    Plug in \eqref{eq:score_bound_1} and we obtain for some constant $C_X^{''}$,
    \begin{equation}
        \begin{aligned}
        \E_{x_t|x}\|\nabla_x\log p_t(x_t|y)\|^2
        &\leq \E_{x_t|x} 2\Big[(C_X+C_X'\|x_t\|^2)^2\|x_t\|^2\Big] + 2(B^2+(L^2+1)C_0) \\
        &\leq C_X^{''}(\|x\|^6+1).
        \end{aligned}
    \end{equation}
\end{proof}

\begin{lemma}\label{lem:local_rademacher}
    Let $\Phi$ be a class of functions on domain $\Omega$ and $\P$ be a probability distribution over $\Omega$. 
    Suppose that for any $\varphi\in\Phi$, $\|\varphi\|_{L^\infty(\Omega)}\leq b$, $\E_\P [\varphi]\geq 0$, and $\E_\P [\varphi^2] \leq B\E_\P [\varphi]+B_0$ for some $b,B,B_0\geq 0$. 
    Let $x_1,\cdots,x_n\overset{\textit{i.i.d.}}{\sim}\P$ and $\phi_n$ be a positive, non-decreasing and sub-root function such that
    \begin{equation}
        \mathcal{R}_n(\Phi_r):=\E_{\bm{\sigma}} \sup_{\varphi\in\Phi_r}\Big|\frac{1}{n}\sum_{i=1}^n\sigma_i\varphi(x_i)\Big|\leq \phi_n(r).
    \end{equation}
    where $\Phi_r:= \Big\{\varphi\in\Phi: \frac{1}{n}\sum_{i=1}^n{(\varphi(x_i))^2}\leq r\Big\}$.
    Define the largest fixed point of $\phi_n$ as $r_n^*$.
    Then for some absolute constant $C'$, with probability no less than $1-\delta$, it holds that for any $\varphi\in\Phi$,
    \begin{align}
        &\E_\P[\varphi]\leq \frac{2}{n}\sum_{i=1}^n\varphi(x_i) + C'(B\vee b)\left(r_n^* + \frac{\log\big((\log n)/\delta\big)}{n}\right)+C'\sqrt{\frac{B_0\log\big((\log n)/\delta\big)}{n}},\\
        &\frac{1}{n}\sum_{i=1}^n\varphi(x_i)\leq 2\E_\P[\varphi] + C'(B\vee b)\left(r_n^* + \frac{\log\big((\log n)/\delta\big)}{n}\right)+C'\sqrt{\frac{B_0\log\big((\log n)/\delta\big)}{n}}. 
    \end{align}
\end{lemma}

\begin{proof}
    We follow the procedures in \citet{bousquet2002concentration}.
    Let $\epsilon_j=b2^{-j}$ and consider a sequence of classes
    \begin{equation}
        \Phi^{(j)}:=\{\varphi\in\Phi: \epsilon_{j+1}<\E_\P[\varphi] \leq \epsilon_j\}.
    \end{equation}
    Note that $\Phi=\cup_{j\geq 0}\Phi^{(j)}$ and for $\varphi\in\Phi^{(j)}$, $\E_\P[\varphi^2]\leq B\epsilon_k+B_0$.
    Let $j_0=\lfloor\log_2 n\rfloor$.
    Then by \citet[Lemma 6.1]{bousquet2002concentration}, it holds that with probability no less than $1-\delta$, for any $j\leq j_0$ and $\varphi\in\Phi^{(j)}$,
    \begin{align}
        &\Big|\frac{1}{n}\sum_{i=1}^n\varphi(x_i)-\E_\P[\varphi]\Big|\lesssim \mathcal{R}_n(\Phi^{(j)})+\sqrt{\frac{(B\epsilon_j+B_0)\log\big(\log(b/\epsilon_j)/\delta\big)}{n}} + \frac{b\log\big(\log(b/\epsilon_j)/\delta\big)}{n}, \label{eq:rademacher_1}\\
        &\Big|\frac{1}{n}\sum_{i=1}^n(\varphi(x_i))^2-\E_\P[\varphi^2]\Big|\lesssim b\mathcal{R}_n(\Phi^{(j)})+\sqrt{\frac{b^2(B\epsilon_j+B_0)\log\big(\log(b/\epsilon_j)/\delta\big)}{n}} + \frac{b^2\log\big(\log(b/\epsilon_j)/\delta\big)}{n}. \label{eq:rademacher_2}
    \end{align}
    Besides, for $\varphi\in\cup_{k>k_0}\Phi^{(j)}=:\Phi^{(j_0:)}$,
    \begin{equation}\label{eq:rademacher_3}
        \Big|\frac{1}{n}\sum_{i=1}^n\varphi(x_i)-\E_\P[\varphi]\Big|\lesssim \mathcal{R}_n(\Phi^{(j_0:)})+\sqrt{\frac{(B\epsilon_{j_0}+B_0)\log\big(\log(n)/\delta\big)}{n}} + \frac{b\log\big((\log n)/\delta\big)}{n}
    \end{equation}
    
    From now on we reason on the conjunction of \eqref{eq:rademacher_1}, \eqref{eq:rademacher_2} and \eqref{eq:rademacher_3}.
    Define 
    \begin{equation}\label{eq:def_u}
        U_j = B\epsilon_j+B_0+b\mathcal{R}_n(\Phi^{(k)})+\sqrt{\frac{b^2(B\epsilon_j+B_0)\log\big(\log(b/\epsilon_j)/\delta\big)}{n}} + \frac{b^2\log\big(\log(b/\epsilon_j)/\delta\big)}{n}.
    \end{equation}
    and thus for any $\varphi\in\Phi^{(j)}$, we have $\frac{1}{n}\sum_{i=1}^n(\varphi(x_i))^2\leq CU_j$ for some absolute constant $C$ by \eqref{eq:rademacher_2}, indicating that
    $\mathcal{R}_n(\Phi^{(j)})\leq \phi_n(CU_j)\leq \sqrt{C}\phi_n(U_j)$.
    For any $j\leq j_0$,
    \begin{equation}
        U_j \leq 2(B\epsilon_j+B_0)+b\sqrt{C}\phi_n(U_j)+\frac{2b^2\log\big((\log n)/\delta\big)}{n}.
    \end{equation}
    Since $\phi_n$ is non-decreasing and sub-root, the inequality above implies that
    \begin{equation}
        U_j \lesssim b^2r_n^*+B\epsilon_j+B_0+\frac{b^2\log\big((\log n)/\delta\big)}{n}=:r_n(\epsilon_j).
    \end{equation}
    Therefore, for any $\varphi\in\Phi^{(j)},j\leq j_0$, by \eqref{eq:rademacher_1},
    \begin{equation}
        \begin{aligned}
            \Big|\frac{1}{n}\sum_{i=1}^n\varphi(x_i)-\E_\P[\varphi]\Big| 
            &\lesssim \phi_n(r_n(\epsilon_j))+\sqrt{\frac{(B\epsilon_j+B_0)\log\big((\log n)/\delta\big)}{n}}+\frac{b\log\big((\log n)/\delta\big)}{n} \\
            &=: F_n(\epsilon_j).
        \end{aligned}
    \end{equation}
    Noticing that $\E_\P[\varphi]\leq \epsilon_j\leq 2\E_\P[\varphi]$, it reduces to
    \begin{equation}
        \Big|\frac{1}{n}\sum_{i=1}^n\varphi(x_i)-\E_\P[\varphi]\Big|\lesssim F_n(\E_\P[\varphi]).
    \end{equation}
    Hence we have by noting that $F_n$ is also a non-decreasing sub-root function, 
    \begin{align}
        &\E_\P[\varphi]\leq \frac{2}{n}\sum_{i=1}^n\varphi(x_i) + C'(B\vee b)\left(r_n^* + \frac{\log\big((\log n)/\delta\big)}{n}\right)+C'\sqrt{\frac{B_0\log\big((\log n)/\delta\big)}{n}},\\
        &\frac{1}{n}\sum_{i=1}^n\varphi(x_i)\leq 2\E_\P[\varphi]+C'(B\vee b)\left(r_n^* + \frac{\log\big((\log n)/\delta\big)}{n}\right)+C'\sqrt{\frac{B_0\log\big((\log n)/\delta\big)}{n}}.
    \end{align}
    Here $C'$ is an absolute constant. Moreover, when $\varphi\in\Phi^{(j)}$ for $j>j_0$, we have $\E_\P[\varphi]\leq \frac{b}{n}$, and according to \eqref{eq:rademacher_3},
    \begin{equation}
        \Big|\frac{1}{n}\sum_{i=1}^n\varphi(x_i)-\E_\P[\varphi]\Big|\lesssim F_n(\varepsilon_{j_0}).
    \end{equation}
    Hence the same bounds apply, which completes the proof.
\end{proof}

\begin{lemma}\label{lem:local_rademacher_meta}
    Let $\Phi$ be a class of functions on domain $\Omega$, $\P^1,\cdots,\P^K$ be probability distributions over $\Omega$, 
    and $\widehat{\P}^{(K)}=\frac{1}{K}\sum_{k=1}^K\delta_{\P^k}$.
    Suppose that for any $\varphi\in\Phi$, $\|\varphi\|_{L^\infty(\Omega)}\leq b$, $\E_{\widehat{\P}^{(K)}} [\varphi]\geq 0$, and $\E_{\widehat{\P}^{(K)}} [\varphi^2] \leq B\E_{\widehat{\P}^{(K)}} [\varphi]+B_0$ for some $b,B,B_0\geq 0$. 
    Let $x^k_1,\cdots,x^k_n\overset{\textit{i.i.d.}}{\sim}\P^k$ for any $k$ and all $(x_i^k)_{i,k}$ are independent. 
    Let $\phi_{K,n}$ be a positive, non-decreasing and sub-root function such that
    \begin{equation}
        \mathcal{R}_{K,n}(\Phi_r):=\E_{\bm{\sigma}} \sup_{\varphi\in\Phi_r}\Big|\frac{1}{nK}\sum_{k=1}^K\sum_{i=1}^n\sigma_i^k\varphi(x_i^k)\Big|\leq \phi_{K,n}(r).
    \end{equation}
    where $\Phi_r:= \Big\{\varphi\in\Phi: \frac{1}{nK}\sum_{k=1}^K\sum_{i=1}^n{(\varphi(x_i^k))^2}\leq r\Big\}$.
    Define the largest fixed point of $\phi_{K,n}$ as $r_{K,n}^*$.
    Then for some absolute constant $C'$, with probability no less than $1-\delta$, it holds that for any $\varphi\in\Phi$,
    \begin{align}
        &\E_{\widehat{\P}^{(K)}}[\varphi]\leq \frac{2}{nK}\sum_{k=1}^K\sum_{i=1}^n\varphi(x_i) + C'(B\vee b)\left(r_{K,n}^* + \frac{\log\big((\log nK)/\delta\big)}{nK}\right)+C'\sqrt{\frac{B_0\log\big((\log nK)/\delta\big)}{nK}},\\
        &\frac{1}{nK}\sum_{k=1}^K\sum_{i=1}^n\varphi(x_i^k)\leq 2\E_{\widehat{\P}^{(K)}}[\varphi] + C'(B\vee b)\left(r_{K,n}^* + \frac{\log\big((\log nK)/\delta\big)}{nK}\right)+C'\sqrt{\frac{B_0\log\big((\log nK)/\delta\big)}{nK}}. 
    \end{align}
\end{lemma}

\begin{proof}
    We follow the procedures in \citet{bousquet2002concentration}.
    Let $\epsilon_k=b2^{-k}$ and consider a sequence of classes
    \begin{equation}
        \Phi^{(j)}:=\{\varphi\in\Phi: \epsilon_{j+1}<\E_{\widehat{\P}^{(K)}}[\varphi] \leq \epsilon_j\}.
    \end{equation}
    Note that $\Phi=\cup_{j\geq 0}\Phi^{(j)}$ and for $\varphi\in\Phi^{(j)}$, $\E_{\widehat{\P}^{(K)}}[\varphi^2]\leq B\epsilon_j+B_0$.
    Let $j_0=\lfloor\log_2(nK)\rfloor$.
    Then by \citet[Theorem 3]{massart2000constants}, with probability no less than $1-\delta$, for any $j\leq j_0$ and $\varphi\in\Phi^{(j)}$,
    \begin{align}
        &\Big|\frac{1}{nK}\sum_{k=1}^K\sum_{i=1}^n\varphi(x_i^k)-\E_{\widehat{\P}^{(K)}}[\varphi]\Big|\lesssim \mathcal{R}_{K,n}(\Phi^{(j)})+\sqrt{\frac{(B\epsilon_j+B_0)\log\big(\log(b/\epsilon_j)/\delta\big)}{nK}} + \frac{b\log\big(\log(b/\epsilon_j)/\delta\big)}{nK}, \label{eq:rademacher_meta_1}\\
        &\Big|\frac{1}{nK}\sum_{k=1}^K\sum_{i=1}^n(\varphi(x_i^k))^2-\E_{\widehat{\P}^{(K)}}[\varphi^2]\Big|\lesssim b\mathcal{R}_{K,n}(\Phi^{(j)})+\sqrt{\frac{b^2(B\epsilon_j+B_0)\log\big(\log(b/\epsilon_j)/\delta\big)}{nK}} + \frac{b^2\log\big(\log(b/\epsilon_j)/\delta\big)}{nK}. \label{eq:rademacher_meta_2}
    \end{align}
    Besides, for any $\varphi\in\cup_{j>j_0}\Phi^{(j)}=:\Phi^{(j_0:)}$,
    \begin{equation}\label{eq:rademacher_meta_3}
        \Big|\frac{1}{nK}\sum_{k=1}^K\sum_{i=1}^n\varphi(x_i^k)-\E_{\widehat{\P}^{(K)}}[\varphi]\Big|\lesssim \mathcal{R}_{K,n}(\Phi^{(j_0:)})+\sqrt{\frac{(B\epsilon_{j_0}+B_0)\log\big((\log nK)/\delta\big)}{nK}} + \frac{b\log\big( (\log nK)/\delta\big)}{nK}.
    \end{equation}
    
    From now on we reason on the conjunction of \eqref{eq:rademacher_meta_1}, \eqref{eq:rademacher_meta_2} and \eqref{eq:rademacher_meta_3}.
    Define 
    \begin{equation}\label{eq:def_u_meta}
        U_j = B\epsilon_j+B_0+b\mathcal{R}_{K,n}(\Phi^{(j)})+\sqrt{\frac{b^2(B\epsilon_j+B_0)\log\big(\log(b/\epsilon_j)/\delta\big)}{nK}} + \frac{b^2\log\big(\log(b/\epsilon_j)/\delta\big)}{nK}.
    \end{equation}
    and thus for any $\varphi\in\Phi^{(j)}$, we have $\frac{1}{nK}\sum_{k=1}^K\sum_{i=1}^n(\varphi(x_i^k))^2\leq CU_j$ for some absolute constant $C$ by \eqref{eq:rademacher_meta_2}, indicating that
    $\mathcal{R}_{K,n}(\Phi^{(j)})\leq \phi_{K,n}(CU_j)\leq \sqrt{C}\phi_{K,n}(U_j)$.
    For any $j\leq j_0$,
    \begin{equation}
        U_j \leq 2(B\epsilon_j+B_0)+b\sqrt{C}\phi_{K,n}(U_j)+\frac{2b^2\log\big((\log nK)/\delta\big)}{nK}.
    \end{equation}
    Since $\phi_{K,n}$ is non-decreasing and sub-root, the inequality above implies that
    \begin{equation}
        U_j \lesssim b^2r_{K,n}^*+B\epsilon_j+B_0+\frac{b^2\log\big((\log nK)/\delta\big)}{nK}=:r_{K,n}(\epsilon_j).
    \end{equation}
    Therefore, for any $\varphi\in\Phi^{(j)},j\leq j_0$, by \eqref{eq:rademacher_meta_1},
    \begin{equation}
        \begin{aligned}
            \Big|\frac{1}{nK}\sum_{k=1}^K\sum_{i=1}^n\varphi(x_i^k)-\E_{\widehat{\P}^{(K)}}[\varphi]\Big| 
            &\lesssim \phi_{K,n}(r_{K,n}(\epsilon_j))+\sqrt{\frac{(B\epsilon_j+B_0)\log\big((\log nK)/\delta\big)}{nK}}+\frac{b\log\big((\log nK)/\delta\big)}{nK} \\
            &=: F_{K,n}(\epsilon_j).
        \end{aligned}
    \end{equation}
    Noticing that $\E_{\widehat{\P}^{(K)}}[\varphi]\leq \epsilon_j\leq 2\E_{\widehat{\P}^{(K)}}[\varphi]$, it reduces to
    \begin{equation}
        \Big|\frac{1}{nK}\sum_{k=1}^K\sum_{i=1}^n\varphi(x_i^k)-\E_{\widehat{\P}^{(K)}}[\varphi]\Big|\lesssim F_{K,n}(\E_{\widehat{\P}^{(K)}}[\varphi]).
    \end{equation}
    Hence we have by noting that $F_{K,n}$ is also a non-decreasing sub-root function, 
    \begin{align}
        &\E_{\widehat{\P}^{(K)}}[\varphi]\leq \frac{2}{nK}\sum_{k=1}^K\sum_{i=1}^n\varphi(x_i^k) + C'(B\vee b)\left(r_{K,n}^* + \frac{\log\big((\log nK)/\delta\big)}{nK}\right)+C'\sqrt{\frac{B_0\log\big((\log nK)/\delta\big)}{nK}},\\
        &\frac{1}{nK}\sum_{k=1}^K\sum_{i=1}^n\varphi(x_i^k)\leq 2\E_{\widehat{\P}^{(K)}}[\varphi]+C'(B\vee b)\left(r_{K,n}^* + \frac{\log\big((\log nK)/\delta\big)}{nK}\right)+C'\sqrt{\frac{B_0\log\big((\log nK)/\delta\big)}{nK}}.
    \end{align}
    Here $C'$ is an absolute constant. Moreover, when $\varphi\in\Phi^{(j)}$ for $j>j_0$, we have $\E_{\widehat{\P}^{(K)}}[\varphi]\leq \frac{b}{nK}$, and according to \eqref{eq:rademacher_meta_3},
    \begin{equation}
        \Big|\frac{1}{nK}\sum_{k=1}^K\sum_{i=1}^n\varphi(x_i^k)-\E_{\widehat{\P}^{(K)}}[\varphi]\Big|\lesssim F_{K,n}(\varepsilon_{j_0}).
    \end{equation}
    Hence the same bounds apply, which completes the proof.
\end{proof}

\subsection{Verifying Task Diversity Assumption}\label{app:subsec:verify_diversity}

When $\mathcal{F}$ is linear function class, \citet{tripuraneni2020theory} provides an explicit bound on $(\nu,\Delta)$.
However, in general, performing a fine-grained analysis is challenging, especially for complex function classes such as neural networks.
In the following proposition, we present a very pessimistic bound for $(\nu,\Delta)$ based on density ratio, which is independent of the specific choice of hypothesis classes $\mathcal{F}$ and $\mathcal{H}$.
\begin{prop}
    Suppose $\mathcal{F}=\textbf{conv}(\mathcal{F})$,
    and $\inf_{x,y}\frac{p^k(x,y)}{p^0(x,y)}\geq \lambda_k$ for any $1\leq k\leq K$.
    Let $\lambda=\sum_{k=1}^K\lambda_k$.
    Then $\P^1,\cdots,\P^K$ are $(\widetilde{\nu},\widetilde{\Delta})$-diverse over $\P^0$ with $\widetilde{\nu}=\lambda/(2K)$,
    \begin{equation}
        \widetilde{\Delta}=2\E_{(x,y)\sim\P^0}\E_{t,x_t}\left[\left\|\frac{1}{\lambda}\sum_{k=1}^K\lambda_k\nabla\log p_t^k(x_t|y)-\nabla\log p_t^0(x_t|y)\right\|^2\right].
    \end{equation}
\end{prop}

We mention that the only requirement is $\mathcal{F}$ is a convex hull of itself, which can be easily satisfied by most hypothesis classes such as neural networks. 
More refined analysis on specific neural network class is an interesting future work.
\begin{proof}
    For any $h\in\mathcal{H}$, let $f^k\in\mathcal{F}$ be the corresponding minimizer for $1\leq k\leq K$.
    Further define $\lambda=\sum_{k=1}^K\lambda_k$ and $\widetilde{f}^0=\frac{1}{\lambda}\sum_{k=1}^K\lambda_kf^k\in\textbf{conv}(\mathcal{F})\in\mathcal{F}$.
    Then we have
    \begin{equation}
        \begin{aligned}
            L^{\P^0}(s_{\widetilde{f}^0,h})
            &=\E_{\P^0}\left[\|\widetilde{f}^0(x_t,h(y),t)-\nabla\log p_t^0(x_t|y)\|^2\right] \\
            &\leq 2\E_{\P^0}\left[\|\widetilde{f}^0(x_t,h(y),t)-\sum_{k=1}^K\frac{\lambda_k}{\lambda}\nabla\log p_t^k(x_t|y)\|^2 +\|\sum_{k=1}^K\frac{\lambda_k}{\lambda}\nabla\log p_t^k(x_t|y)-\nabla\log p_t^0(x_t|y)\|^2\right] \\
            &\leq \frac{2}{\lambda}\sum_{k=1}^K\E_{\P^0}\lambda_k\left[\|f^k(x_t,h(y),t)-\nabla\log p_t^k(x_t|y)\|^2\right]+ \widetilde{\Delta} \\
            &\leq \frac{2}{\lambda}\sum_{k=1}^K\E_{\P^k}\left[\|f^k(x_t,h(y),t)-\nabla\log p_t^k(x_t|y)\|^2\right]+ \widetilde{\Delta} \\
            &= \frac{1}{\widetilde{\nu}}\inf_{\vf\in\mathcal{F}^{\otimes K}}\frac{1}{K}\sum_{k=1}^KL^{\P^k}(s_{f^k,h})+ \widetilde{\Delta}.
        \end{aligned}
    \end{equation}
    We conclude the proof by noticing that $\inf_{f\in\mathcal{F}}L^{\P^0}(s_{f,h})\leq L^{\P^0}(s_{\widetilde{f}^0,h})$.
\end{proof}

\section{Proofs in Section \ref{sec:dist_estimation}}

\subsection{Proofs of Score Network Approximation}\label{app:subsec:approximation}

\begin{thm}[Thm. \ref{thm:approximation_all_informal}]
\label{thm:approximation_all}
    Under Assumption \ref{asp:sub_gaussian}, \ref{asp:low_dim}, \ref{asp:lip}, to achieve $R_f\geq C_R\log^{\frac{1}{2}}(nKM_f/\delta)$ and
    \begin{align}
        &\inf_{h\in\mathcal{H}} \frac{1}{K}\sum_{k=1}^K \inf_{f\in\mathcal{F}}\E_{(x,y)\sim\P^k} [\ell^{\P^k} (x,y,s_{f,h})] = \mathcal{O}\left(\log^2(nK/(\varepsilon\delta))\varepsilon^2\right), \text{ (transfer learning)} \\
        &\inf_{h\in\mathcal{H}}\E_{\P\sim\Pmeta}\inf_{f\in\mathcal{F}}\E_{(x,y)\sim\P} [\ell^\P(x,y,s_{f,h})] = \mathcal{O}\left(\log^2(nK/(\varepsilon\delta))\varepsilon^2\right), \text{ (meta-learning)}
    \end{align}
    the configuration of $\mathcal{F}=NN_f(L_f,W_f,M_f,S_f,B_f,R_f,\gamma_f),\mathcal{H}=NN_h(L_h,W_h,S_h,B_h)$ should satisfy
    \begin{equation}
        \begin{aligned}
            &L_f=\mathcal{O}\left(\log\left(\frac{\log(nK/(\varepsilon\delta))}{\varepsilon}\right)\right),
            W_f=\mathcal{O}\left(\frac{\log^{3(d_x+d_y)/2}(nK/(\varepsilon\delta))}{\varepsilon^{d_x+d_y+1}T_0^3}\right), \\
            &S_f=\mathcal{O}\left(\frac{\log^{3(d_x+d_y)/2+1}(nK/(\varepsilon\delta))}{\varepsilon^{d_x+d_y+1}T_0^3}\right),
            B_f=\mathcal{O}\left(\frac{T\log^{\frac{3}{2}}(nK/(\varepsilon\delta))}{\varepsilon}\right), \\
            &R_f=\mathcal{O}\left(\log^{\frac{1}{2}}(nK/(\varepsilon\delta))\right), 
            M_f=\mathcal{O}\left(\log^3(nK/(\varepsilon\delta))\right),
            \gamma_f=\mathcal{O}\left(\log(nK/(\varepsilon\delta))\right),
        \end{aligned}
    \end{equation}
     \begin{equation}
        \begin{aligned}
            &L_h=\mathcal{O}\left(\log(1/\varepsilon)\right), W_h=\mathcal{O}\left(\varepsilon^{-D_y}\log(1/\varepsilon)\right), \\
            &S_h=\mathcal{O}\left(\varepsilon^{-D_y}\log^2(1/\varepsilon)\right),
            B_h = \mathcal{O}(1).
        \end{aligned}
    \end{equation}
    Here $\mathcal{O}(\cdot)$ hides all the polynomial factors of $d_x,d_y,D_y,C_1,C_2,L,B$.
\end{thm}

\begin{proof}
    With a little abuse of notation, in transfer learning setting, we define $\Pmeta:=\frac{1}{K}\sum_{k=1}^K\delta_{\P^k}$ and it directly reduces to meta-learning case.
    Therefore, we only focus on the proof in meta-learning.
    
    We first decompose the misspecification error into two components: representation error and score approximation error.
    \begin{equation}
        \begin{aligned}
            &\inf_{h\in\mathcal{H}}\E_{\P\sim\Pmeta}\inf_{f\in\mathcal{F}}\E_{(x,y)\sim\P} [\ell^\P(x,y,s_{f,h})] \\
            &= \inf_{h\in\mathcal{H}}\E_{\P\sim\Pmeta}\inf_{f\in\mathcal{F}}\E_{(x,y)\sim\P}\E_{t,x_t|x} [\|f(x_t,h(y),t)-f_*^\P(x_t,h_*(y),t)\|^2] \\
            &\leq \inf_{h\in\mathcal{H}}\E_{\P\sim\Pmeta}\inf_{f\in\mathcal{F}}\E_{(x,y)\sim\P}\E_{t,x_t|x} 2\left[\|f(x_t,h(y),t)-f(x_t,h_*(y),t)\|^2+\|f(x_t,h_*(y),t)-f_*^\P(x_t,h_*(y),t)\|^2\right].
        \end{aligned}
    \end{equation}
    Further note that for any $f\in\mathcal{F}$, 
    \begin{equation}
        \begin{aligned}
            \E_{(x,y)\sim\P}\E_{t,x_t|x} [\|f(x_t,h(y),t)-f(x_t,h_*(y),t)\|^2]
            &\leq \E_{t,x_t,y} \|f(x_t,h(y),t)-f(x_t,h_*(y),t)\|^2\cdot \mathbbm{1}_{\|x_t\|\leq R_f} \\
            &\qquad + 8M_f^2\exp(-C_1'R_f^2) \\
            &\leq \E_{y\sim\P}[\gamma_f^2\|h(y)-h_*(y)\|^2] + 8M_f^2\exp(-C_1'R_f^2),
        \end{aligned}
    \end{equation}
    where $\Omega_{R_f}=[-R_f,R_f]^{d_x}\times[0,1]^{d_y}\times[T_0,T]$.
    By Proposition \ref{prop:approximation_f}, \ref{prop:approximation_h},
    \begin{equation}
        \begin{aligned}
            &\inf_{h\in\mathcal{H}}\E_{\P\sim\Pmeta}\inf_{f\in\mathcal{F}}\E_{(x,y)\sim\P} [\ell^\P(x,y,s_{f,h})] \\
            &\leq 
            \inf_{h\in\mathcal{H}}\E_{\P\sim\Pmeta}\E_{y\sim\P}[2\gamma_f^2\|h(y)-h_*(y)\|^2] + 16M_f^2\exp(-C_1'R_f^2) \\
            &\qquad +\E_{\P\sim\Pmeta}\inf_{f\in\mathcal{F}}2\|f(x_t,h_*(y),t)-f_*^\P(x_t,h_*(y),t)\|^2 \\
            &\leq 2\inf_{h\in\mathcal{H}}\gamma_f^2\|h-h_*\|_{L^\infty([0,1]^{D_y})}^2 + 16M_f^2\exp(-C_1'R_f^2) \\
            &\qquad +2\E_{\P\sim\Pmeta}\inf_{f\in\mathcal{F}}\|f(x_t,h_*(y),t)-f_*^\P(x_t,h_*(y),t)\|^2 \\
            &\lesssim \left(\log^2(nK/(\varepsilon\delta))d_y+ d_x\right)\varepsilon^2 \\
            &= \mathcal{O}\left(\log^2(nK/(\varepsilon\delta))\varepsilon^2\right).
        \end{aligned}
    \end{equation}
\end{proof}

\begin{prop}\label{prop:approximation_f}
    To achieve $R_f\geq C_R\log^{\frac{1}{2}}(nKM_f/\delta)$ and
    \begin{equation}
        \inf_{f\in\mathcal{F}}\E_{(x,y)\sim\P}\E_{t,x_t|x} [\|f(x_t,h_*(y),t)-f^\P(x_t,h_*(y),t)\|^2] \leq d_x\varepsilon^2,
    \end{equation}
    the configuration of $\mathcal{F}=NN_f(L_f,W_f,M_f,S_f,B_f,R_f,\gamma_f)$ should satisfy
    \begin{equation}
        \begin{aligned}
            &L_f=\mathcal{O}\left(\log\left(\frac{\log(nK/(\varepsilon\delta))}{\varepsilon}\right)\right),
            W_f=\mathcal{O}\left(\frac{\log^{3(d_x+d_y)/2}(nK/(\varepsilon\delta))}{\varepsilon^{d_x+d_y+1}T_0^3}\right), \\
            &S_f=\mathcal{O}\left(\frac{\log^{3(d_x+d_y)/2+1}(nK/(\varepsilon\delta))}{\varepsilon^{d_x+d_y+1}T_0^3}\right),
            B_f=\mathcal{O}\left(\frac{T\log^{\frac{3}{2}}(nK/(\varepsilon\delta))}{\varepsilon}\right), \\
            &R_f=\mathcal{O}\left(\log^{\frac{1}{2}}(nK/(\varepsilon\delta))\right), 
            M_f=\mathcal{O}\left(\log^3(nK/(\varepsilon\delta))\right),
            \gamma_f=\mathcal{O}\left(\log(nK/(\varepsilon\delta))\right).
        \end{aligned}
    \end{equation}
    Here $\mathcal{O}(\cdot)$ hides all the polynomial factors of $d_x,d_y,D_y,C_1,C_2,L,B$.
\end{prop}

\begin{proof}
    For notation simplicity, we will $f_*=f_*^\P$ throughout the proof.
    Our procedures consist of two main steps.
    The first is to clip the whole input space to a bounded set $\Omega_{R_f}:=[-R_f,R_f]^{d_x}\times[0,1]^{d_y}\times[T_0,T]$ thanks to the light tail property of $\P$. Then we approximate $f_*^\P$ on $\Omega_{R_f}$.
    
    By Lemma \ref{lem:lip_score} and \ref{lem:lip_t}, $f_*$ is $\gamma_1$-Lipschitz in $x$, $\gamma_2$-Lipschitz in $w$, and $\gamma_3$-Lipshcitz in $t$ in a bounded domain $\Omega_{R_f}$, where $\gamma_1=C_X+C_X'R_f^2,\gamma_2=C_X+C_X'R_f,\gamma_3=\frac{C_sR_f^3}{T_0^3}$.
    
    We first rescale the input domain by $x'=\frac{x}{2R_f}+\frac{1}{2}, w'=w,t'=t/T$, which can be implemented by a single ReLU layer.
    Denote $v=(x',w',t')$. We only need to approximate $g(v):=f_*(R_f(2x'-1), w', Tt')$ defined on $\Omega:=[0,1]^{d_x+d_y}\times[T_0/T,1]$.
    And $g$ is $\gamma_x:=2\gamma_1 R_f$-Lipschitz in $x'$, $\gamma_w:=\gamma_2$-Lipschitz in $w'$ and $\gamma_t:=\gamma_2 T$-Lipschitz in $t'$.
    We will approximate each coordinate of $g=[g_{1}, \cdots,g_{d_x}]^\top$ separately and then concatenate them together.

    Now we partition the domain $\Omega$ into non-overlapping regions. For the first $d_x + d_y$ dimensions, the space $[0, 1]^{d_x + d_y}$ is uniformly divided into hypercubes with an edge length of $e_1$. For the last dimension, the interval $[T_0/T, 1]$ is divided into subintervals of length $e_2$, where the values of $e_1$ and $e_2$ will be specified later. Let the number of intervals in each partition be $N_1 = \lceil 1 / e_1 \rceil$ and $N_2 = \lceil 1 / e_2 \rceil$, respectively.

    Let $u=[u_1,\cdots,u_{d_x+d_y}]\in\{0,\cdots,N_1-1\}^{d_x+d_y}$ be a multi-index. Define
    \begin{equation}\label{eq:def_g_bar}
        \bar{g}_i(x',w',t')=\sum_{u,j}g_i(u/N_1,j/N_2)\Psi_{u,j}(x',w',t'),
    \end{equation}
    where $\Psi$ is the coordinate-wise product of trapezoid function:
    \begin{equation}
        \Psi_{u,j}(x',w',t'):=\psi\big(3N_2(t'-j/N_2)\big)\prod_{r=1}^{d_x}\psi\big(3N_1(x'_r-u_r/N_1)\big)\prod_{r=1}^{d_y}\psi\big(3N_1(w'_r-u_{r+d_x}/N_1)\big),
    \end{equation}
    \begin{equation}
        \psi(a):=\left\{
            \begin{array}{ll}
                1, & |a|< 1 \\
                2-|a|, & 1\leq |a| < 2 \\
                0, & |a|>\geq 2
            \end{array}
        \right.
    \end{equation}
    We claim that $\bar{g}_i$ is an approximation to $g_i$ since for any $o'=(x',w')\in[0,1]^{d_x+d_y},t'\in[T_0/T,1]$,
    \begin{equation}
        \begin{aligned}
            \sup_{o',t'}\Big|\bar{g}_i(o',t')-g_i(o',t')|
            &\leq \sup_{o',t'}\Big|\sum_{u,j}(g_i(\frac{u}{N_1},\frac{j}{N_2})-g_i(o',t'))\Psi_{u,j}(o',t')\Big| \\
            &\leq \sup_{o',t'} \sum_{u:|\frac{u_i}{N_1}-o'_i|\leq \frac{2N_1}{3},j:|\frac{j}{N_2}-t'|\leq \frac{2N_2}{3}}\Big|g_i(\frac{u}{N_1},\frac{j}{N_2})-g_i(o',t')\Big|\Psi_{u,j}(o',t') \\
            &\leq \frac{2\gamma_x}{3N_1}+\frac{2\gamma_t}{3N_2}.
        \end{aligned}
    \end{equation}
    
    Below we construct a ReLU neural network to approximate $\bar{g}_i$.
    Let $\sigma$ be ReLU activation and $r(a)=2\sigma(a)-4\sigma(a-0.5)+2\sigma(a-1)$ for any scalar $a\in[0,1]$.
    Define
    \begin{equation}
        \phi_{\text{square}}^l(a)=a-\sum_{m=1}^l2^{-2m}r_m(a),\ r_m=\underbrace{r\circ\cdots\circ r}_{m \text{ compositions}}
    \end{equation}
    \begin{equation}
        \phi_{\text{mul}}^l(a,b)=\phi_{\text{square}}^l(\frac{a+b}{2})-\phi_{\text{square}}^l(\frac{a-b}{2})
    \end{equation}
    According to \citet{yarotsky2017error}, 
    \begin{equation}
        |\phi_{\text{mul}}^l(a,b)-ab|\leq 2^{-2l-2},\ \forall a,b\in[0,1].
    \end{equation}
    Then we approximate $\Psi_{u,j}$ by recursively apply $\phi_{\text{mul}}^l$:
    \begin{equation}\label{eq:def_Psi_hat}
        \widehat{\Psi}_{u,j}(x',w',t'):=\phi_{\text{mul}}^l\left(\psi\big(3N_2(t'-j/N_2)\big),\phi_{\text{mul}}^l\left(\psi\big(3N_1(x'_1-u_1/N_2)\big),\cdots\right)\right)
    \end{equation}
    And we construct the final neural network approximation as
    \begin{equation}\label{eq:def_g_hat}
        \widehat{g}_i(x',w',t'):=\sum_{u,j}g_i(u/N_1,j/N_2)\widehat{\Psi}_{u,j}(x',w',t').
    \end{equation}
    The approximation error of $\widehat{g}_i$ can be bounded by
    \begin{equation}
        \begin{aligned}
            \|\widehat{g}_i-g_i\|_{L^\infty(\Omega)}
            &\leq \|\widehat{g}_i-\bar{g}_i\|_{L^\infty(\Omega)}+|\bar{g}_i-g_i\|_{L^\infty(\Omega)} \\
            &\leq 2^{d_x+d_y+1}\|g_i\|_{L^\infty(\Omega)}\sup_{u,j}\|\widehat{\Psi}_{u,j}-\Psi_{u,j}\|_{L^\infty(\Omega)}+\frac{2\gamma_x(d_x+d_y)^{\frac{1}{2}}}{3N_1}+\frac{2\gamma_t}{3N_2} \\
            &\leq (d_x+d_y+1)2^{d_x+d_y+1}\|g_i\|_{L^\infty(\Omega)}2^{-(2l+2)}+\frac{2\gamma_x(d_x+d_y)^{\frac{1}{2}}}{3N_1}+\frac{2\gamma_t}{3N_2}.
        \end{aligned}
    \end{equation}
    Besides, by \citet[Lemma 15]{chen2020distribution}, for $l\gtrsim d_x+d_y$ and $\forall x',w',w'',t'$,
    \begin{equation}
        |\widehat{g}_i(x',w',t')-\widehat{g}_i(x',w'',t')|\lesssim (d_x+d_y)\left(\gamma_w+N_1\|g_i\|_{L^\infty(\Omega)}2^{-l+d_x+d_y}\right)\|w'-w''\|_{\infty}.
    \end{equation}
    Let $l=\mathcal{O}\left(d_x+d_y+\log\frac{\gamma_w(\|g\|_{L^\infty(\Omega)}+1)}{\varepsilon}\right), N_1=\mathcal{O}\left(\frac{\gamma_x}{\varepsilon}\right),N_2=\mathcal{O}\left(\frac{\gamma_t}{\varepsilon}\right)$.
    Then
    \begin{equation}\label{eq:err_nn_g}
        \|\widehat{g}_i-g_i\|_{L^\infty(\Omega)}\leq \varepsilon/2,\ |\widehat{g}_i(x',w',t')-\widehat{g}_i(x',w'',t')|\lesssim \gamma_w(d_x+d_y)\|w'-w''\|_\infty.
    \end{equation}
    Define $\widehat{g}:=[\widehat{g}_1,\cdots,\widehat{g}_{d_x}]$ and $\widehat{f}(x,w,t):=\widehat{g}\left(\frac{x}{2R_f}+\frac{1}{2},w,t/T\right)$.
    Then the approximation error of $\widehat{f}$ in $\Omega_{R_f}$ can be bounded by
    \begin{equation}
        \|\widehat{f}-f\|_{L^\infty(\Omega_{R_f})}\leq\|\widehat{g}-g\|_{L^\infty(\Omega)}\leq \sqrt{d_x}\varepsilon/2,\ \text{and }\widehat{f}(x,w,t)=0, \forall\ \|x\|_{\infty}>R_f.
    \end{equation}
    Therefore, when $R_f\geq C_R\log^{\frac{1}{2}}\left((M_f^2+C_L)/\varepsilon\right)$, the overall approximation error is
    \begin{equation}
        \begin{aligned}
            \E_{(x,y)\sim\P}\E_{t,x_t|x} [\|f(x_t,h_*(y),t)-f_*^\P(x_t,h_*(y),t)\|^2]
            &\leq \E_{t,x_t,y} \|f(x_t,h(y),t)-f(x_t,h_*(y),t)\|^2\cdot \mathbbm{1}_{\|x_t\|\leq R_f} \\
            &\qquad + 4(M_f^2+C_L)\exp(-C_1'R_f^2) \\
            &\leq \|f-f_*^\P\|_{L^\infty(\Omega_{R_f})}^2 + 4(M_f^2+C_L)\exp(-C_1'R_f^2) \\
            &\leq d_x\varepsilon^2.
        \end{aligned}
    \end{equation}
    
    Now we characterize the configuration of neural network $\widehat{f}(x,w,t)$.
    For boundedness, by Lemma \ref{lem:bound score_t},
    \begin{equation}
        \|\widehat{f}(x,w,t)\|\leq \|f_*\|_{L^\infty(\Omega_{R_f})}+\varepsilon\leq 2C_X''R_f^6=:M_f.
    \end{equation}
    Hence we can let $R_f=\mathcal{O}\left(\log^{\frac{1}{2}}\left(\frac{nK}{\varepsilon\delta}\right)\right)$ to ensure the lower bound of $R_f$ mentioned above and in Theorem \ref{thm:generalization_all}.
    For Lipschitzness, by \eqref{eq:err_nn_g},
    \begin{equation}
        \begin{aligned}
            \|\widehat{f}(x,w,t)-\widehat{f}(x,\widetilde{w},t)\|
            &\lesssim \gamma_w(d_x+d_y)\|w-\widetilde{w}\|_{\infty} \\
            &\lesssim (C_X+C_X'R_f^2)(d_x+d_y)\|w-\widetilde{w}\|_{\infty}.
        \end{aligned}
    \end{equation}
    Hence $\gamma_f=\mathcal{O}\left((C_X+C_X'R_f^2)(d_x+d_y)\right)=\mathcal{O}\left(\log\left(\frac{nK}{\varepsilon\delta}\right)\right)$.
    
    For the size of neural network, for each coordinate, by the construction in \eqref{eq:def_g_hat}, the neural network $\widehat{g}_i$ consists of $N_1^{d_x+d_y}N_2$ parallel subnetworks, \textit{i.e.}, $g_i(u/N_1,j/N_2)\widehat{\Psi}_{u,j}(\cdot,\cdot,\cdot)$. By definition in \eqref{eq:def_Psi_hat}, the subnetwork consists of $\mathcal{O}\left((d_x+d_y)(d_x+d_y+\log\frac{R_f}{\varepsilon})\right)$ layers and the width is bounded by $\mathcal{O}(d_x+d_y)$.
    Therefore, the whole neural network $\widehat{g}_i$ can be implemented by $\mathcal{O}\left((d_x+d_y)(d_x+d_y+\log(R_f/\varepsilon))\right)$ layers with width $\mathcal{O}\left(N_1^{d_x+d_y}N_2(d_x+d_y)\right)=\mathcal{O}\left(\frac{R_f^{3(d_x+d_y)}}{\varepsilon^{d_x+d_y+1}T_0^3}\right)$, and the number of parameter is bounded by $\mathcal{O}\left(\frac{R_f^{3(d_x+d_y)}\log(R_f/\varepsilon)}{\varepsilon^{d_x+d_y+1}T_0^3}\right)$.
    Combine these arguments together, we can claim that the size of neural network $\widehat{f}$ is
    \begin{equation}
        \begin{aligned}
            &L=\mathcal{O}\left((d_x+d_y)(d_x+d_y+\log(R_f/\varepsilon))\right)=\mathcal{O}\left(\log\left(\frac{\log(nK/(\varepsilon\delta))}{\varepsilon}\right)\right), \\
            &W=\mathcal{O}\left(\frac{R_f^{3(d_x+d_y)}}{\varepsilon^{d_x+d_y+1}T_0^3}\right)=\mathcal{O}\left(\frac{\log^{3(d_x+d_y)/2}(nK/(\varepsilon\delta))}{\varepsilon^{d_x+d_y+1}T_0^3}\right),\\
            &S=\mathcal{O}\left(\frac{(d_x+d_y)R_f^{3(d_x+d_y)}\log(R_f/\varepsilon)}{\varepsilon^{d_x+d_y+1}T_0^3}\right)=\mathcal{O}\left(\frac{\log^{3(d_x+d_y)/2+1}(nK/(\varepsilon\delta))}{\varepsilon^{d_x+d_y+1}T_0^3}\right).
        \end{aligned}
    \end{equation}
    To bound of the neural network parameters, note that the trapezoid function $\psi$ is rescaled by $3N_1$ or $3N_2$ and the weight parameter of $\phi_{\text{mul}}^l$ is bounded by a constant. 
    Moreover, the input of $\widehat{f}$ is first rescaled by $R_f$ or $T$. 
    Hence we have
    \begin{equation}
        B=\mathcal{O}\left(N_1R_f+N_2T\right)=\mathcal{O}\left(\frac{R_f^3T}{\varepsilon}\right)=\mathcal{O}\left(\frac{T\log^{\frac{3}{2}}(nK/(\varepsilon\delta))}{\varepsilon}\right),
    \end{equation}
    which concludes the proof. 
\end{proof}

\begin{prop}\label{prop:approximation_h}
    To achieve
    \begin{equation}
        \inf_{h\in\mathcal{H}} \|h-h_*\|_{L^\infty([0,1]^{D_y})}\leq \sqrt{d_y}\varepsilon,
    \end{equation}
    the configuration of $\mathcal{H}=NN_h(L_h,W_h,S_h,B_h)$ should satisfy 
    \begin{equation}
        \begin{aligned}
            &L_h=\mathcal{O}\left(\log(1/\varepsilon)\right), W_h=\mathcal{O}\left(\varepsilon^{-D_y}\log(1/\varepsilon)\right), \\
            &S_h=\mathcal{O}\left(\varepsilon^{-D_y}\log^2(1/\varepsilon)\right),
            B_h = \mathcal{O}(1).
        \end{aligned}
    \end{equation}
    Here $\mathcal{O}(\cdot)$ hides all the polynomial factors of $d_x,d_y,L$.  
\end{prop}

\begin{proof}
    The main idea replicates \citet[Theorem 1]{yarotsky2017error}.
    We approximate each coordinate of $h_*=[h_{*1},\cdots,h_{*d_y}]$ respectively and then concatenate all them together.
    By \citet[Theorem 1]{yarotsky2017error}, $h_{*i}$ can be approximated up to $\varepsilon$ by a network $\widehat{h}_i$ with $\mathcal{O}\left(\log(1/\varepsilon)\right)$ layers and $\mathcal{O}\left(\varepsilon^{-D_y}\log(1/\varepsilon)\right)$ width. 
    Besides, the range of all the parameters are bounded by some constant, and the number of parameters is $\mathcal{O}\left(\varepsilon^{-D_y}\log^2(1/\varepsilon)\right)$.
    Then we concatenate all the subnetworks to get $\widehat{h}=[\widehat{h}_1,\cdots,\widehat{h}_{d_y}]$ and $\|\widehat{h}-h_*\|_{L^\infty([0,1]^{D_y})}\leq \sqrt{d_y}\varepsilon$.
\end{proof}

\subsection{Proofs of Distribution Estimation}\label{app:subsec:dist_estimation}

\begin{thm}[Thm. \ref{thm:distribution_diversity_informal}]
\label{thm:distribution_diversity}
    Suppose Assumption \ref{asp:sub_gaussian}, \ref{asp:low_dim}, \ref{asp:lip} hold.
    For sufficiently large integers $n,K,m$ and $\delta>0$, further suppose that $\P^1,\cdots,\P^K$ are $(\nu,\Delta)$-diverse over target distribution $\P^0$ with proper configuration of neural network family and $T,T_0$. It holds that with probability no less than $1-\delta$,
    \begin{equation}
        \E_{\{(x_i,y_i)\}_{i=1}^m}\E_{y\sim\P^0_y} [\mathrm{TV}(\widehat{\P}^0_{x|y},\P^0_{x|y})]
        \lesssim \frac{\log^{\frac{5}{2}}(nK/\delta)\log^3((m/\nu)\wedge n)}{\nu^{\frac{1}{2}}((m/\nu)\wedge n)^{\frac{1}{d_x+d_y+9}}}+\frac{\log^2(nK/\delta)}{\nu^{\frac{1}{2}}(nK)^{\frac{1}{D_y+2}}}+\sqrt{\Delta}.
    \end{equation}
\end{thm}

\begin{proof}
    Combine Theorem \ref{thm:approximation_all} and Theorem \ref{thm:generalization_all_diversity} and plug in the configuration of $\mathcal{F},\mathcal{H}$, we have with probability no less than $1-\delta$
    \begin{equation}
        \begin{aligned}
            &\E_{\{(x_i,y_i)\}_{i=1}^m} \E_{(x,y)\sim\P^0} [\ell^{\P^0}(x,y,s_{\widehat{f}^{\P^0},\widehat{h}})] \\
            &\qquad \lesssim 
            \frac{1}{\nu}\log^2(nK/(\varepsilon\delta))\varepsilon^2+\Delta+\frac{\log^{\frac{3(d_x+d_y)+15}{2}}(nK/\varepsilon\delta)\log(T/T_0)}{(m\wedge (\nu n))\varepsilon^{d_x+d_y+1}T_0^3}+\frac{\log^4(1/\varepsilon)\log(nK/(\varepsilon\delta))}{\nu nK\varepsilon^{D_y}}
        \end{aligned}
    \end{equation}
    By Lemma \ref{lem:TV_bound},
    \begin{equation}
        \mathrm{TV}(\widehat{\P}^0_{x|y},\P^0_{x|y})
        \lesssim \sqrt{T_0}\log^{\frac{d_x+1}{2}}(1/T_0)+e^{-T}+\sqrt{\E_{\P^0_{x|y}}[\ell^{\P^0}(x,y,s_{\widehat{f}^{\P^0},\widehat{h}})]}
    \end{equation}
    Taking expectation of $y,\widehat{f}^\P,\P$, we have
    \begin{equation}
        \begin{aligned}
            \E_{\{(x_i,y_i)\}_{i=1}^m}\E_{y\sim\P^0_y} [\mathrm{TV}(\widehat{\P}^0_{x|y},\P^0_{x|y})]
            &\lesssim \sqrt{T_0}\log^{\frac{d_x+1}{2}}(1/T_0)+e^{-T} + \nu^{-\frac{1}{2}}\log(nK/(\varepsilon\delta))\varepsilon + \sqrt{\Delta} \\
            &\quad +\frac{\log^{\frac{3(d_x+d_y)+15}{4}}(\frac{nK}{\varepsilon\delta})\log^{\frac{1}{2}}(\frac{T}{T_0})}{(m\wedge (\nu n))^{\frac{1}{2}}\varepsilon^{\frac{d_x+d_y+1}{2}}T_0^{\frac{3}{2}}}+\frac{\log^2(\frac{1}{\varepsilon})\log^{\frac{1}{2}}(\frac{nK}{\varepsilon\delta})}{(\nu nK)^{\frac{1}{2}}\varepsilon^{\frac{D_y}{2}}}.
        \end{aligned}
    \end{equation}
    Let $T_0=\mathcal{O}\left(\varepsilon_0^2/\log^{d_x+1}(1/\varepsilon_0)\right), T=\mathcal{O}(\log(1/\varepsilon_0)), \varepsilon=\mathcal{O}(\varepsilon_0/\log(nK/(\varepsilon_0\delta_0)))$ for some small $\varepsilon_0>0$ defined later.
    Then it reduces to
    \begin{equation}
        \E_{\{(x_i,y_i)\}_{i=1}^m}\E_{y\sim\P^0_y} [\mathrm{TV}(\widehat{\P}^0_{x|y},\P^0_{x|y})]
        \lesssim \frac{\varepsilon_0}{\nu^{\frac{1}{2}}} +\sqrt{\Delta}+\frac{\log^{\frac{5(d_x+d_y)+17}{4}}(\frac{nK}{\varepsilon_0\delta})
        \log^{\frac{3d_x+5}{2}}(\frac{1}{\varepsilon_0})}{(m\wedge (\nu n))^{\frac{1}{2}}\varepsilon_0^{\frac{d_x+d_y+7}{2}}}  + \frac{\log^2(\frac{1}{\varepsilon_0})\log^{D_y+\frac{1}{2}}(\frac{nK}{\varepsilon_0\delta})}{(\nu nK)^{\frac{1}{2}}\varepsilon_0^{\frac{D_y}{2}}}.
    \end{equation}
    Let $\varepsilon_0=C\max\left\{\frac{\log^{\frac{5}{2}}(nK/\delta)\log^3((m/\nu)\wedge n)}{((m/\nu)\wedge n)^{\frac{1}{d_x+d_y+9}}},\frac{\log^2(nK/\delta)}{(nK)^{\frac{1}{D_y+2}}}\right\}$, and we can conclude that
    \begin{equation}
        \E_{\{(x_i,y_i)\}_{i=1}^m}\E_{y\sim\P^0_y} [\mathrm{TV}(\widehat{\P}^0_{x|y},\P^0_{x|y})]
        \lesssim \frac{\log^{\frac{5}{2}}(nK/\delta)\log^3((m/\nu)\wedge n)}{\nu^{\frac{1}{2}}((m/\nu)\wedge n)^{\frac{1}{d_x+d_y+9}}}+\frac{\log^2(nK/\delta)}{\nu^{\frac{1}{2}}(nK)^{\frac{1}{D_y+2}}}+\sqrt{\Delta}.
    \end{equation}
\end{proof}

\begin{thm}[Thm. \ref{thm:distribution_informal}]
\label{thm:distribution}
    Suppose Assumption \ref{asp:sub_gaussian}, \ref{asp:low_dim}, \ref{asp:lip} hold.
    For sufficiently large integers $n,K,m$ and $\delta>0$, with proper configuration of neural network family and $T,T_0$, it holds that with probability no less than $1-\delta$,
    \begin{equation}
        \begin{aligned}
            \E_{\P\sim\Pmeta}\E_{\{(x_i,y_i)\}_{i=1}^m\sim \P}\E_{y\sim\P_y} [\mathrm{TV}(\widehat{\P}_{x|y},\P_{x|y})]
            &\lesssim \frac{\log^{\frac{5}{2}}(nK/\delta)\log^3(m\wedge n)}{(m\wedge n)^{\frac{1}{d_x+d_y+9}}}+\frac{\log^2(nK/\delta)}{K^{\frac{1}{D_y+2}}}.
        \end{aligned}
    \end{equation}
\end{thm}

\begin{proof}
    Combine Theorem \ref{thm:approximation_all} and Theorem \ref{thm:generalization_all} and plug in the configuration of $\mathcal{F},\mathcal{H}$, we have with probability no less than $1-\delta$
    \begin{equation}
        \begin{aligned}
            &\E_{\P\sim\Pmeta}\E_{\{(x_i,y_i)\}_{i=1}^m\sim \P} \E_{(x,y)\sim\P} [\ell^\P(x,y,s_{\widehat{f}^\P,\widehat{h}})] \\
            &\qquad \lesssim 
            \log^2(nK/(\varepsilon\delta))\varepsilon^2+\frac{\log^{\frac{3(d_x+d_y)+15}{2}}(nK/\varepsilon\delta)\log(T/T_0)}{(m\wedge n)\varepsilon^{d_x+d_y+1}T_0^3}+\frac{\log^4(1/\varepsilon)\log(nK/(\varepsilon\delta))}{K\varepsilon^{D_y}}
        \end{aligned}
    \end{equation}
    By Lemma \ref{lem:TV_bound},
    \begin{equation}
        \mathrm{TV}(\widehat{\P}_{x|y},\P_{x|y})
        \lesssim \sqrt{T_0}\log^{\frac{d_x+1}{2}}(1/T_0)+e^{-T}+\sqrt{\E_{\P_{x|y}}[\ell^{\P}(x,y,s_{\widehat{f}^\P,\widehat{h}})]}
    \end{equation}
    Taking expectation of $y,\widehat{f}^\P,\P$, we have
    \begin{equation}
        \begin{aligned}
            \E_{\P\sim\Pmeta}\E_{\{(x_i,y_i)\}_{i=1}^m\sim \P}\E_{y\sim\P_y} [\mathrm{TV}(\widehat{\P}_{x|y},\P_{x|y})]
            &\lesssim \sqrt{T_0}\log^{\frac{d_x+1}{2}}(1/T_0)+e^{-T} + \log(nK/(\varepsilon\delta))\varepsilon \\
            &\quad +\frac{\log^{\frac{3(d_x+d_y)+15}{4}}(\frac{nK}{\varepsilon\delta})\log^{\frac{1}{2}}(\frac{T}{T_0})}{(m\wedge n)^{\frac{1}{2}}\varepsilon^{\frac{d_x+d_y+1}{2}}T_0^{\frac{3}{2}}}+\frac{\log^2(\frac{1}{\varepsilon})\log^{\frac{1}{2}}(\frac{nK}{\varepsilon\delta})}{K^{\frac{1}{2}}\varepsilon^{\frac{D_y}{2}}}.
        \end{aligned}
    \end{equation}
    Let $T_0=\mathcal{O}\left(\varepsilon_0^2/\log^{d_x+1}(1/\varepsilon_0)\right), T=\mathcal{O}(\log(1/\varepsilon_0)), \varepsilon=\mathcal{O}(\varepsilon_0/\log(nK/(\varepsilon_0\delta_0)))$ for some small $\varepsilon_0>0$ defined later.
    Then it reduces to
    \begin{equation}
        \begin{aligned}
            \E_{\P\sim\Pmeta}\E_{\{(x_i,y_i)\}_{i=1}^m\sim \P}\E_{y\sim\P_y} [\mathrm{TV}(\widehat{\P}_{x|y},\P_{x|y})]
            &\lesssim \varepsilon_0+\frac{\log^{\frac{5(d_x+d_y)+17}{4}}(\frac{nK}{\varepsilon_0\delta})
            \log^{\frac{3d_x+5}{2}}(\frac{1}{\varepsilon_0})}{(m\wedge n)^{\frac{1}{2}}\varepsilon_0^{\frac{d_x+d_y+7}{2}}} \\
            &\qquad + \frac{\log^2(\frac{1}{\varepsilon_0})\log^{D_y+\frac{1}{2}}(\frac{nK}{\varepsilon_0\delta})}{K^{\frac{1}{2}}\varepsilon_0^{\frac{D_y}{2}}}.
        \end{aligned}
    \end{equation}
    Let $\varepsilon_0=C\max\left\{\frac{\log^{\frac{5}{2}}(nK/\delta)\log^3(m\wedge n)}{(m\wedge n)^{\frac{1}{d_x+d_y+9}}},\frac{\log^2(nK/\delta)}{K^{\frac{1}{D_y+2}}}\right\}$, and we can conclude that
    \begin{equation}
        \E_{\P\sim\Pmeta}\E_{\{(x_i,y_i)\}_{i=1}^m\sim \P}\E_{y\sim\P_y} [\mathrm{TV}(\widehat{\P}_{x|y},\P_{x|y})]
        \lesssim \frac{\log^{\frac{5}{2}}(nK/\delta)\log^3(m\wedge n)}{(m\wedge n)^{\frac{1}{d_x+d_y+9}}}+\frac{\log^2(nK/\delta)}{K^{\frac{1}{D_y+2}}}.
    \end{equation}
\end{proof}


\subsection{Auxiliary Lemmas}

\begin{lemma}\label{lem:lip_t}
    Let $\Omega_{R_f}=[-R_f,R_f]^{d_x}\times[0,1]^{d_y}\times[T_0,T]$ for some $R_f\geq 1$. Then there exists some constant $C_s$, such that the score function $f_*^\P(x,w,t)$ is $\frac{C_sR_f^3}{T_0^3}$-Lipschitz with respect to $t$ in $\Omega_{R_f}$.
\end{lemma}

\begin{proof}
    According to \eqref{eq:score_1},
    \begin{equation}
        f_*^\P(x,w,t)=-\frac{x}{\sigma_t^2}+\frac{\alpha_t}{\sigma_t^2}\int x_0\frac{\phi_t(x|x_0)p(x_0;w)}{\int \phi_t(x|z)p(z;w)\dif z}\dif x_0.
    \end{equation}
    Define density function $q_t(x_0|x,w)\propto \phi_t(x|x_0)p(x_0;w)$. Then
    \begin{equation}
        \frac{\partial}{\partial t}f_*^\P(x,w,t)
        = -\frac{2\alpha_t^2x}{\sigma_t^2}+\frac{\alpha_t}{\sigma_t^2}\Cov_{q_t(x_0|x,w)}\left(x_0,\frac{\partial}{\partial_t}\log\phi_t(x|x_0)\right) - \frac{\alpha_t(1+\alpha_t^2)}{\sigma_t^4}\E_{q_t(x_0|x,w)}[x_0].
    \end{equation}
    Note that 
    \begin{equation}
        \begin{aligned}
            \Cov_{q_t(x_0|x,w)}\left(x_0,\frac{\partial}{\partial_t}\log\phi_t(x|x_0)\right)
            &= -\Cov_{q_t(x_0|x,w)}\left(x_0,\frac{\partial}{\partial_t}\frac{\|x-\alpha_tx_0\|^2}{2\sigma_t^2}\right) \\
            &= \Cov_{q_t(x_0|x,w)}\left(x_0,\frac{\alpha_t(x-\alpha_tx_0)^\top\bm{1}}{\sigma_t^2}-\frac{2\alpha_t^2\|x-\alpha_tx_0\|^2}{\sigma_t^4}\right)
        \end{aligned}
    \end{equation}
    Hence for any $x\in[-R_f,R_f]^{d_x},w\in[0,1]^{d_y}$, 
    \begin{equation}
        \begin{aligned}
            \left\|\frac{\partial}{\partial t}f_*^\P(x,w,t)\right\|_\infty
            &\lesssim \frac{\alpha_t^2R_f}{\sigma_t^2}+\E_{q_t(x_0|x,w)}\Big\|\frac{x-\alpha_tx_0}{\sigma_t^2}\Big\|^3 + \frac{\alpha_t(1+\alpha_t^2)}{\sigma_t^4}\E_{q_t(x_0|x,w)}[\|x_0\|_\infty]
        \end{aligned}
    \end{equation}
    Let $R=\frac{2R_f+2C_0}{\sigma_t}$. We have
    \begin{equation}
        \begin{aligned}
            \E_{q_t(x_0|x,w)}\Big\|\frac{\alpha_tx_0-x}{\sigma_t^2}\Big\|^3
            &\preceq \frac{1}{\sigma_t^3}\int \big\|\frac{\alpha_tx_0-x}{\sigma_t}\big\|^3\frac{\phi_t(x|x_0)p(x_0|y)}{\int \phi_t(x|z)p(z|y)\dif z}\dif x_0 \\
            &\leq \frac{R^3}{\sigma_t^3} + \frac{\int_{\|\frac{\alpha_tx_0-x}{\sigma_t}\|\geq R} \|\frac{\alpha_tx_0-x}{\sigma_t}\|^2\exp\left(-\frac{\|\alpha_tx_0-x\|^2}{2\sigma_t^2}\right)p(x_0;w)\dif x_0}{\sigma_t^3\int \exp\left(-\frac{\|\alpha_tx_0-x\|^2}{2\sigma_t^2}\right)p(x_0;w)\dif x_0} \\
            &\leq \frac{R^3}{\sigma_t^3} + \frac{\int_{\|\frac{\alpha_tx_0-x}{\sigma_t}\|\geq R} \exp(-\frac{R^2}{4})p(x_0;w)\dif x_0}{\sigma_t^3\int_{\|\frac{\alpha_tx_0-x}{\sigma_t}\|\leq R/2} \exp(-\frac{R^2}{8})p(x_0;w)\dif x_0}.
        \end{aligned}
    \end{equation}
    The domain $\Big\{x_0:\|\frac{\alpha_tx_0-x}{\sigma_t}\|\leq R/2\Big\}$ includes $\Big\{x_0:\|x_0\|\leq C_0\Big\}$, indicating
    \begin{equation}
        \begin{aligned}
            &\int_{\|\frac{\alpha_tx_0-x}{\sigma_t}\|\leq R/2} p(x_0;w)\dif x_0\geq  \int_{\|x_0\|\leq C_0} p(x_0;w)\dif x_0 \geq 1-2\exp(-C_1'C_0^2)\geq \frac{1}{2},\\
            &\int_{\|\frac{\alpha_tx_0-x}{\sigma_t}\|\geq R} p(x_0;w)\dif x_0\leq  \int_{\|x_0\|\geq C_0} p(x_0;w)\dif x_0 \leq \frac{1}{2}.
        \end{aligned}
    \end{equation}
    Therefore, for any $(x,w,t)\in\Omega_{R_f}$,
    \begin{equation}
        \begin{aligned}
            \left\|\frac{\partial}{\partial t}f_*^\P(x,w,t)\right\|_\infty
            &\lesssim \frac{R_f^2}{\sigma_t^2}+\frac{R_f^3+C_0^3}{\sigma_t^6}+\frac{R_f+C_0}{\sigma_t^3}
            &\lesssim \frac{R_f^3}{T_0^3}.
        \end{aligned}
    \end{equation}
\end{proof}


\begin{lemma}\label{lem:TV_bound}
    Suppose $\mathrm{KL}(\P^0_{x|y}\|\mathcal{N}(0,I))\leq C_{\mathrm{KL}}$ for some constant $C_{\mathrm{KL}}$. Then
    \begin{equation}
        \mathrm{TV}(\widehat{\P}^0_{x|y},\P^0_{x|y})\lesssim \sqrt{T_0}\log^{\frac{d_x+1}{2}}(1/T_0) + e^{-T} + \sqrt{\E_{\P^0_{x|y}}[\ell^{\P^0}(x,y,s_{\widehat{f},\widehat{h}})]}.
    \end{equation}
\end{lemma}

\begin{proof}
    With a little abuse of notation, we will use $p_t(x_t|y)$ to denote the conditional density of $x_t|y$ under $\P^0_{x|y}$.
    Consider the following two backward processes
    \begin{align}
        &d\widetilde{x}_t=(\widetilde{x}_t+2\nabla\log p_{T-t}(\widetilde{x}_t|y))\dif t+\sqrt{2}\dif W_t,\ \widetilde{x}_0\sim \mathcal{N}(0,I), 0\leq t\leq T-T_0,
        \\
        &d\bar{x}_t=(\bar{x}_t+2\nabla\log p_{T-t}(\widetilde{x}_t|y))\dif t+\sqrt{2}\dif W_t,\ \bar{x}_0\sim p_T, 0\leq t\leq T-T_0.
    \end{align}
    Denote the distribution of $\widetilde{x}_t$ as $\widetilde{\P}_{T-t}$.
    And note that $\bar{x}_t\sim p_{T-t}$ by classic reverse-time SDE results \citep{anderson1982reverse}. 
    Then by \citet[Lemma D.5]{fu2024unveil},
    \begin{equation}
        \mathrm{TV}(\P_{T_0},\P_0)\lesssim \sqrt{T_0}\log^{\frac{d_x+1}{2}}(1/T_0).
    \end{equation}
    At the same time, we apply Data Processing inequality and Pinsker's inequality to get
    \begin{equation}
        \mathrm{TV}(\P_{T_0},\widetilde{\P}_{T_0})\leq \mathrm{TV}(\P_{T},\mathcal{N}(0,I))\lesssim \sqrt{\mathrm{KL}(\P_{T}\|\mathcal{N}(0,I))}\lesssim \sqrt{\mathrm{KL}(\P_0\|\mathcal{N}(0,I))}e^{-T}.
    \end{equation}
    Again according to Pinsker's inequality and \citet[Proposition D.1]{oko2023diffusion},
    \begin{equation}
        \mathrm{TV}(\widehat{\P},\widetilde{\P}_{T_0})
        \lesssim \sqrt{\mathrm{KL}(\widetilde{\P}_{T_0}\|\widehat{\P})}\lesssim \sqrt{\E_{x|y}[\ell^\P(x,y,s_{\widehat{f},\widehat{h}})]}.
    \end{equation}
    Combine three inequalities above and we complete the proof.
\end{proof}

\section{Proofs in Section \ref{sec:application}}

\subsection{Proof of Theorem \ref{thm:amortized_vi}}\label{app:subsec:proof_avi}

\begin{proof}
    Due to the structure of exponential family, Assumption \ref{asp:low_dim} holds obviously.
    To apply previous results, we only need to verify Assumption \ref{asp:sub_gaussian} and \ref{asp:lip}.
    Recall that a basic property of exponential family is 
    \begin{align}
        \nabla_x A_\psi(x) &= \E_{p_\psi(y|x)}[h_*(y)]\in [0,1]^d,\\
        0\preceq \nabla_x^2 A_\psi(x) &= \Var_{ p_\psi(y|x)}(h_*(y)) \preceq I.
    \end{align}
    Hence by Assumption \ref{asp:amortized_vi}, $A_\psi(x)\leq A_\psi(0) + \|x\|_1\leq \log\left(\int \psi(y)\dif y\right)+\|x\|_1\leq \log C+\|x\|_1$. And $A_\psi(x)\geq A_\psi(0) - \|x\|_1\geq -\log C-\|x\|_1$.
    Further note that the posterior density $p_\theta(x|y)=\frac{p_\phi(x)\exp(\langle x,h_*(y)\rangle-A_\psi(x))}{Z_\theta}$, where the normalizing constant $Z_\theta(y)$ is lower bounded by
    \begin{equation}
        \begin{aligned}
            Z_\theta(y) 
            &= \int p_\phi(x)\exp(\langle x,h_*(y)\rangle-A_\psi(x)) \dif x \\
            &\geq \int p_\phi(x)\exp(-2\|x\|_1)/C \dif x \\
            &\geq \exp(-2\sqrt{d}R)(1-2\exp(-C_1'R^2)) / C=: C_0.
        \end{aligned}
    \end{equation}
    where in the second inequality we apply $\P_\phi(\|x\|\geq R)\leq 2\exp(-C_1'R^2)$ and let $R=1/\sqrt{C_1'}$ to get $C_0$.
    Therefore, by Assumption \ref{asp:amortized_vi},
    \begin{equation}
        p_\theta(x|y)\leq C_1\exp(-C_2\|x\|^2+2\|x\|_1+\log C)/C_0
        \leq C_1'\exp(-C_2'\|x\|^2),
    \end{equation}
    and thus Assumption \ref{asp:sub_gaussian} holds.
    At the same time, ley $w=h_*(y)$, then the score function is
    \begin{equation}
        \nabla_x\log p_\theta(x|y)=\nabla_x\log p_\theta(x,w)=\nabla_x\log p_\phi(x)+w-\nabla_x A_\psi(x).
    \end{equation}
    Since $\nabla_x\log p_\phi(x)$ is $L$-Lipschitz, $\nabla A_\psi(x)$ is also $1$-Lipschitz, the score function $\nabla_x\log p_\theta(x,w)$ is $(L+1)$-Lipschitz in $x$ and $1$-Lipschitz in $w$.
    And $\|\nabla_x\log p_\theta(0,w)\|\leq \|\nabla_x\log p_\phi(0)\|+2\sqrt{d}=B+2\sqrt{d}$,
    indicating that Assumption \ref{asp:lip} holds with $L'=L+1,B'=B+2\sqrt{d}$.
    
    We conclude the proof by applying Theorem \ref{thm:distribution_informal} under meta-learning setting or Theorem \ref{thm:distribution_diversity_informal} under $(\nu,\Delta)$-diversity.
\end{proof}

\subsection{Proof of Theorem \ref{thm:behavior_cloning}}\label{app:subsec:proof_bc}

\begin{proof}
    Let $A_M^\pi(s,a)=Q_M^\pi(s,a)-V_M(\pi,s)$ be the advantage function of policy $\pi$.
    Note that the reward function $r_M\in[0,1]$, we have $|A_M^\pi(s,a)|\leq \frac{2}{1-\gamma}$ for any $M,\pi$.
    According to performance difference lemma,
    \begin{equation}
        \begin{aligned}
             V_{M^0}(\pi_*^0)-V_{M^0}(\widehat{\pi}^0) 
             &= \frac{1}{1-\gamma}\E_{(s,a)\sim d_*^0} [A_{M^0}^{\widehat{\pi}^0}(s,a)] \\
             &= \frac{1}{1-\gamma}\E_{s\sim d_*^0} \left[\E_{a\sim \pi_*^0(\cdot|s)}[A_{M^0}^{\widehat{\pi}^0}(s,a)]-\E_{a\sim \widehat{\pi}^0(\cdot|s)}[A_{M^0}^{\widehat{\pi}^0}(s,a)]\right] \\
             &\leq \frac{2}{(1-\gamma)^2}\E_{s\sim d_*^0}[\mathrm{TV}(\pi_*^0(\cdot|s),\widehat{\pi}^0(\cdot|s))].
        \end{aligned}
    \end{equation}
    Hence in meta-learning setting, we plug in Theorem \ref{thm:distribution_informal} to obtain
    \begin{equation}
        \E_{M^0}\E_{\{(s_i^0,a_i^0)\}_{i=1}^m\sim d_*^0}[V_{M^0}(\pi_*^0)-V_{M^0}(\widehat{\pi}^0)]\lesssim \frac{1}{(1-\gamma)^2}\left[\frac{\log^{\frac{5}{2}}(nK/\delta)\log^3(m\wedge n)}{(m\wedge n)^{\frac{1}{d_a+d_s+9}}}+\frac{\log^2(nK/\delta)}{K^{\frac{1}{D_s+2}}}\right].
    \end{equation}
    If we further assume $(\nu,\Delta)$-diversity holds, then we plug in Theorem \ref{thm:distribution_diversity_informal},
    \begin{equation}
        \E_{\{(s_i^0,a_i^0)\}_{i=1}^m\sim d_*^0}[V_{M^0}(\pi_*^0)-V_{M^0}(\widehat{\pi}^0)]\lesssim \frac{1}{(1-\gamma)^2}\left[\frac{\log^{\frac{5}{2}}(nK/\delta)\log^3((m/\nu)\wedge n)}{\nu^{\frac{1}{2}}((m/\nu)\wedge n)^{\frac{1}{d_a+d_s+9}}}+\frac{\log^2(nK/\delta)}{\nu^{\frac{1}{2}}(nK)^{\frac{1}{D_s+2}}}+\sqrt{\Delta}\right].
    \end{equation}
    
\end{proof}


\end{document}