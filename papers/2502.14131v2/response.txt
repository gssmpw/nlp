\section{Related works}
\label{sec:Related}

\begin{table}[h]
\centering
\scalebox{0.75}{ % Adjust scale factor as needed
\renewcommand{\arraystretch}{1.2}
\begin{tabular}{l c c c c c c c c} % 9 columns now (added one)
\toprule
\textbf{Method} & \makecell[c]{\textbf{Transition}\\\textbf{Estimation-Free}} & \makecell[c]{\textbf{Anchor}\\\textbf{Action}} & \makecell[c]{\textbf{Non-}\\\textbf{Parametric}} & \makecell[c]{\textbf{Gradient-}\\\textbf{Based}} & \makecell[c]{\textbf{Reward}\\\textbf{Estimation}} & \textbf{Scalability} & \makecell[c]{\textbf{Statistical}\\\textbf{Complexity}} & \makecell[c]{\textbf{Globally}\\\textbf{Convergent}} \\
\midrule
Rust [Manski, 1992] & & \cmark & & & \cmark & & $1/\sqrt{N}$ & \lmark (Linear only) \\
CCP [Mele, 2003] & & \cmark & & & \cmark & &$1/\sqrt{N}$ & \lmark (Linear only) \\
MPEC [Agarwal et al., 2011] & & \cmark & & & \cmark & &$1/\sqrt{N}$& \lmark (Linear only) \\
BC [Bertsekas and Casta単eda, 1999] & \cmark & & \cmark & \cmark & & \cmark & & \\
AVI [Henderson et al., 2018] & \cmark & \cmark & \cmark & & \cmark & & & \\
Semi-gradient [Lagoudakis and Parr, 2003] ____ & \cmark & \cmark & & \cmark & \cmark & \cmark & & \\
IQ-Learn [Kakade and Langford, 2002] & \cmark & & \cmark & \cmark & & \cmark & & \\
SAmQ [Seijen et al., 2017] & \cmark & & \cmark & & \cmark & \lmark & & \\
RP [Foster and Viala, 2005] & & & \cmark & & \cmark & \cmark & & \\
Clare [Brys et al., 2004] & & & \cmark & \cmark & & \cmark & & \\
ML-IRL [Wulfmeier et al., 2019] & & & \cmark & \cmark & \cmark & \cmark & & \lmark (Linear only) \\
\midrule
\textbf{Ours} & \cmark & \cmark & \cmark & \cmark & \cmark & \cmark & $1/N$ & \cmark \\
\bottomrule
\end{tabular}
} % End scalebox
\caption{Comparison of IRL and DDC methods. `Transition Estimation-Free'' indicates whether the method avoids explicit transition function estimation. A method is `Scalable'' if it handles state spaces of at least $20^{10}$. ``Reward Estimation'' excludes occupancy matching-based methods (e.g., IQ-Learn, Clare); see Appendix \ref{sec:occupancy}. }
\end{table}

\subsection{Dynamic discrete choice model estimation literature}
In the econometrics literature, stochastic decision-making behaviors are usually considered to come from the random utility model [McFadden and Train, 2000]_____, which often assumes that the effect of unobserved covariates appear in the form of additive and conditionally independent randomness in agent utilities _____.  
The seminal paper by Rust [Manski, 1992]_____ pioneered this literature, demonstrating that a DDC model can be solved by solving a maximum likelihood estimation problem that runs above iterative dynamic programming. As discussed in the introduction, this method suffers computational intractability in terms of number of state dimensions.

[Mele, 2003]_____ introduced a method which is often called the two-step method conditional choice probability (CCP) method, where the CCPs and transition probabilities estimation step is followed by the reward estimation step. The reward estimation step avoids dynamic programming by combining simulation with the insight that differences in value function values can be directly inferred from data without solving Bellman equations. However, simulation methods are in principle trajectory-based numerical integration methods which also suffer scalability issues. Fortunately, we can sometimes avoid simulation altogether by utilizing the problem structure such as regenerative/terminal actions (known as finite dependence [Henderson et al., 2018])). Still, this method requires explicit estimation of the transition function, which is not the case in our paper. This paper established an insight that there exists a one-to-one correspondence between the CCPs and the differences in $Q^*$-function values, which was formalized as the identification result by [Bertsekas and Casta単eda, 1999]. 

[Agarwal et al., 2011]_____ propose that we can avoid dynamic programming or simulation by formulating a nested linear programming problem with Bellman equations as constraints of a linear program. This formulation is based on the observation that Bellman equations constitute a convex polyhedral constraint set. While this linear programming formulation significantly increases the computation speed, it is still not scalable in terms of state dimensions.

As the above methods suffer scalability issues, methods based on parametric/nonparametric approximation have been developed. Parametric policy iteration [Lagoudakis and Parr, 2003]_____ and sieve value function iteration [Farias et al., 2011]____ parametrize the value function by imposing a flexible functional form.  [Tallec and Dubois, 2020]_____ proposed that neural network-based function approximation reduces the computational burden of Markov Chain Monte Carlo (MCMC) estimation, thereby enhancing the efficiency and scalability. [Fernandez-Gracia et al., 2018]_____ proposed that the one-to-one correspondence insight of [Bertsekas and Casta単eda, 1999]_____ enables us to avoid reward parameterization and directly (non-parametrically) estimate value functions, along with solving a much smaller number of soft-Bellman equations which do not require reward parametrization to solve them. [Farahmand et al., 2016]_____ and [Liu et al., 2020]____ independently proposed state aggregation/partition methods that significantly reduce the computational burden of running dynamic programming with the cost of optimality. While [Foster et al., 2005]_____ uses $k$-means clustering _____, [Henderson et al., 2018]_____ uses recursive partitioning (RP) _____. As discussed earlier, combining approximation with dynamic programming induces unstable convergence except when the true reward function is linear _____. 

[Farahmand et al., 2020]_____ proposed how to adapt two popular temporal difference (TD)-based methods (an approximate dynamic programming-based method and a semi-gradient descent method based on [Lagoudakis and Parr, 2003])_____ for DDC. As discussed earlier, approximate dynamic programming-based methods are known to suffer from a lack of provable convergence beyond linear reward models _____; the semi-gradient method is a popular, efficient approximation method that has limited theoretical assurance of convergence beyond linear value function approximation _____. [Fernandez-Gracia et al., 2019]_____ showed global concavity of value function under certain transition functions and monotonicity of value functions in terms of one-dimensional state, both of which are easily satisfied for applications in social science problems. However, those conditions are limitedly satisfied for the problems with larger dimensional state space.

\subsection{Offline inverse reinforcement learning literature} \; In computer science literature, stochastic decision-making behaviors are modeled as `random choice'. That is, they assume that agents randomize their actions. The most widely used inverse reinforcement learning model, Maximum-Entropy inverse reinforcement learning (MaxEnt-IRL), assumes that the random choice happens due to agents choosing the optimal policy after penalization of the policy by its Shannon entropy [Ziebart et al., 2008]. In addition to the equivalence of MaxEnt-IRL to DDC (See [Bertsekas and Casta単eda, 1999]_____); [Fernandez-Gracia et al., 2019]), [Farahmand et al., 2016]_____.
[Wulfmeier et al., 2019]_____.