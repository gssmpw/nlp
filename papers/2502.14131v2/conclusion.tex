\section{Conclusion}
In this paper, we propose a provably globally convergent empirical risk minimization framework that combines non-parametric estimation methods (e.g., machine learning methods) with IRL/DDC models. This method's convergence to global optima stems from our new theoretical finding that the Bellman error (i.e., Bellman residual) satisfies the Polyak-≈Åojasiewicz (PL) condition, which is a weaker but almost equally useful condition as strong convexity for providing theoretical assurances. 

The three key advantages of our method are: (1) it is easily applicable to high-dimensional state spaces, (2) it can operate without the knowledge of (or requiring the estimation of) state-transition probabilities, and (3) it is applicable to infinite state and action spaces. These three properties make our algorithm practically applicable and useful in high-dimensional, infinite-size state and action spaces that are common in business and economics applications. We demonstrate our approach's empirical performance through extensive simulation experiments (covering both low and high-dimensional settings). We find that, on average, our method performs quite well in recovering rewards in both low and high-dimensional settings. Further, it has better/on-par performance compared to other benchmark algorithms in this area (including algorithms that assume the parametric form of the reward function and knowledge of state transition probabilities) and is able to recover rewards even in settings where other algorithms are not viable.
