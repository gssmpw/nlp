\section{ERM-IRL (ERM-DDC) framework}\label{sec:ERM-IRL}
\subsection{Identification via expected risk minimization}

%The goal of IRL (or DDC) is slightly different: we would like to find $\tilde{r} \in \mathcal{R} \subseteq \mathbb{R}^{\bar{\mathcal{S}} \times \mathcal{A}}$ such that
%\begin{align}
 %   \underset{\tilde{r}\in \mathcal{R}}{\operatorname{argmin}} \; \mathbb{E}_{(s,a)\sim \nu_0, \pi^*}[(\tilde{r}(s,a)-r(s,a))^2] \tag{Equation \ref{eq:rObjective}}
%\end{align}

%In Theorem \ref{thm:MagnacThesmar}, we saw that $Q^*$ and $r(s,a)$ can be uniquely identified by solving the system of equations shown in Equation \eqref{eq:HotzMillereqs} for all $s\in\mathcal{S}$ and $a\in\mathcal{A}$. 

We now propose a one-shot Empirical Risk Minimization framework (ERM-IRL/ERM-DDC) to solve the IRL problem stated in Definition \ref{def:IRLproblem}. First, we recast the IRL problem as the following \textit{expected risk} minimization problem under infinite data regime.

\begin{defn}[Expected risk minimization problem] The expected risk minimization problem is defined as the problem of finding $Q$ that minimizes  the expected risk $\mathcal{R}_{exp}(Q)$, which is defined as
\begin{align}
  &\mathcal{R}_{exp}(Q):= \mathbb{E}_{(s, a)\sim \pi^*, \nu_0}  \left[\mathcal{L}_{NLL}(Q)(s,a) + \lambda \mathbbm{1}_{a = a_s} \mathcal{L}_{BE}(Q)(s,a)\right] \! \notag \noindent
  \\
  & = \mathbb{E}_{(s, a)\sim \pi^*, \nu_0}\bigl[-\log\left(\hat{p}_{Q}(a
\mid s)\right) +  \lambda \mathbbm{1}_{a = a_s} \left( \mathcal{T}Q(s, a) - Q(s, a) \right)^2 \bigr] \label{eq:mainopt}
\end{align}
\noindent where $a_s$ is defined in Assumption \ref{ass:anchor}.
\end{defn}


\noindent\textbf{Remark.} The joint minimization of the NLL term and BE term is the key novelty in our approach. Prior work on the IRL and DDC literature \citep{hotz1993conditional, zeng2023understanding} typically minimizes the log-likelihood of the observed choice probabilities (the NLL term), given observed or estimated state transition probabilities. The standard solution is to first estimate/assume state transition probabilities, then obtain estimates of future value functions, plug them into the choice probability, and then minimize NLL term. In contrast, our recast problem avoids the estimation of state-transition probabilities and instead jointly minimizes the NLL term along with the Bellman-error term. This is particularly helpful in large-state spaces since the estimation of state-transition probabilities can be infeasible/costly in such settings. In Theorem \ref{thm:mainopt}, we show that the solution to our recast problem in Equation \eqref{eq:mainopt} identifies the reward function. 

\begin{thm}[Identification through expected risk minimization]
\label{thm:mainopt} 
\;
\\
The solution to the expected risk minimization problem (Equation \eqref{eq:mainopt}) with any $\lambda>0$
uniquely identifies $Q^\ast$ up to $s\in\bar{\mathcal{S}}
$ and $a \in \mathcal{A}$, i.e., finds $\widehat{Q}$ that satisfies $\widehat{Q}(s,a)=Q^\ast(s,a)$ for  $s\in\bar{\mathcal{S}}
$ and $a \in \mathcal{A}$. Furthermore, we can uniquely identify $r$ up to $s\in\bar{\mathcal{S}}$ and $a \in \mathcal{A}$ by $r(s, a)= \widehat{Q}(s, a) -  \beta \cdot \mathbb{E}_{s^{\prime} \sim P(s, a)} 
    \bigl[ V_{\widehat{Q}} \bigr]$.
\end{thm}
\noindent Essentially, Theorem \ref{thm:mainopt} ensures that solving Equation \eqref{eq:mainopt} gives the exact $r$ and $Q^\ast$ up to 
$\bar{\mathcal{S}}$ and thus provides the solution to the IRL problem defined in Definition \ref{def:IRLproblem}. See Appendix \ref{sec:pfOfmainOpt} for the proof. 

\subsubsection*{Comparison with Imitation Learning} 
Having established the identification guarantees for the ERM-IRL/DDC framework, it is natural to compare this formulation to the identification properties of Imitation Learning (IL). Unlike IRL, which seeks to infer the underlying reward function that explains expert behavior, IL directly aims to recover the expert policy without modeling the transition dynamics. The objective of imitation learning is often defined to as finding policy $\hat{p}$ with 
$$
\min _{\hat{p}} \mathbb{E}_{(s, a) \sim \pi^\ast, \nu_0}\left[\ell\left(\hat{p}(a \mid s), \pi^\ast(a \mid s)\right)\right], \text{$\ell$ is the cross-entropy loss}
$$
or equivalently, 
\begin{align}
\min _{\hat{p}} \mathbb{E}_{(s, a) \sim \pi^\ast , \nu_0}\left[-\log \hat{p}(a|s)\right]\label{eq:mleEqBC}    
\end{align}
Equation \eqref{eq:mleEqBC} is exactly what a typical Behavioral Cloning (BC) \citep{torabi2018behavioral} minimizes under entropy regularization, as the objective of BC is
\begin{align}
  & \!\!\underset{Q\in \mathcal{Q}}{\min }  \;\mathbb{E}_{(s, a)\sim \pi^*, \nu_0}  \left[-\log \hat{p}_Q(a|s)\right] \label{eq:BC}
\end{align}



\noindent where $\hat{p}_Q(a\mid s) = \frac{Q(s,a)}{\sum_{\tilde{a}\in\mathcal{A} }Q(s,\tilde{a})}$. Note that the solution set of Equation \eqref{eq:BC} fully contains the solution set of the ERM-IRL/DDC objective. This means that any solution to the ERM-IRL/DDC problem also minimizes the imitation learning objective, but not necessarily vice versa. Consequently, under entropy regularization, the IL objective is fundamentally easier to solve than the offline IRL/DDC problem, as it only requires minimizing the negative log-likelihood term without enforcing Bellman consistency. One of the key contributions of this paper is to formally establish and clarify this distinction: IL operates within a strictly simpler optimization landscape than the offline IRL/DDC, making it a computationally and statistically more tractable problem. This distinction further underscores the advantage of Behavioral Cloning (BC) over ERM-IRL/DDC for imitation learning (IL) tasksâ€”since BC does not require modeling transition dynamics or solving an optimization problem involving the Bellman residual, it benefits from significantly lower computational and statistical complexity, making it a more efficient approach for IL.



\subsection{Estimation via minimax-formulated empirical risk minimization}
\label{sec:DoubleSampling}
While the idea of expected risk minimization -- minimizing Equation \eqref{eq:mainopt} -- is straightforward, empirically approximating $\mathcal{L}_{B E}(Q)(s, a) = (\mathcal{T} Q(s, a)-Q(s, a))^2$ and its gradient is quite challenging. 
As discussed in Section \ref{sec:BE&TD}, $\mathcal{T}Q$ is not available unless we know the transition function. As a result, we have to rely on an estimate of $\mathcal{T}$. A natural choice, common in TD-methods, is $\hat{\mathcal{T}} Q\left(s, a, s^{\prime}\right)=r(s, a)+\beta \cdot V_Q(s^\prime)$ which is computable given $Q$ and data $\mathcal{D}$. Thus, a natural proxy objective to minimize is:
\begin{align}
    &\mathbb{E}_{s' \sim P(s, a)} [\mathcal{L}_{\mathrm{TD}}(Q)\left(s, a, s^{\prime}\right)] :=\mathbb{E}_{s' \sim P(s, a)} [(\hat{\mathcal{T}} Q\left(s, a, s^{\prime}\right)-Q(s, a))^2] \notag
\end{align}
Temporal Difference (TD) methods typically use stochastic approximation to obtain an estimate of this proxy objective \citep{tesauro1995temporal, adusumilli2019temporal}. However, the issue with TD methods is that minimizing the proxy objective will not minimize the Bellman error in general (see Appendix \ref{sec:BiconjProofs} for details), because of the extra variance term, as shown below. 
\begin{align}
&\mathbb{E}_{s' \sim P(s, a)} 
\bigl[\mathcal{L}_{TD}(Q)(s, a, s^\prime)\bigr] = \mathcal{L}_{BE}(Q)(s, a) + \mathbb{E}_{s' \sim P(s, a)} 
\bigl[(\mathcal{T}Q(s, a) - \hat{\mathcal{T}}Q(s, a, s^\prime))^2\bigr]    \notag
\end{align}
As defined, $\hat{\mathcal{T}}$ is a one-step estimator, and the second term in the above equation does not vanish even in infinite data regimes. So, simply using the TD approach to approximate squared Bellman error provides a biased estimate. Intuitively, this problem happens because expectation and square are not exchangeable, i.e., 
$
\mathbb{E}_{ s^\prime \sim P(s, a)}\left[\delta_{Q}\left(s,a, s^\prime\right)\mid s, a\right]^2 \neq \mathbb{E}_{ s^\prime \sim P(s, a)}\left[\delta_{Q}\left(s,a, s^\prime\right)^2\mid s, a\right]
$. To remove this problematic square term, we employ an approach often referred to as the ``Bi-Conjugate Trick'' which replaces a square function by a linear function called the bi-conjugate:
\begin{align}
     \mathcal{L}_{BE}(s,a)(Q)&:=\mathbb{E}_{ s^\prime \sim P(s, a)}\left[\delta_{Q}\left(s,a, s^\prime\right)\mid s, a\right]^2\notag
     \\
     &=\max_{h\in \mathbb{R}}2\cdot\mathbb{E}_{ s^\prime \sim P(s, a)}\left[\delta_{Q}\left(s,a, s^\prime\right)\mid s, a\right]\cdot h-h^2 \notag
\end{align}

\noindent By further re-parametrizing $h$ using $\zeta = h - r + Q(s,a)$, after some algebra, we arrive at Lemma \ref{lem:OurBiconj}. (See Appendix \ref{sec:BiconjProofs} for the detailed derivation.)

\begin{lem}
\label{lem:OurBiconj}
\;\\
(a) We can express the squared Bellman error as
\begin{align}
    \mathcal{L}_{BE}(Q)(s, a)&:=(\mathcal{T} Q(s, a)-Q(s, a))^2 \notag
    \\
    &=\mathbb{E}_{s^\prime \sim P(s, a)} 
    \bigl[\mathcal{L}_{TD}(s, a, s^\prime)(Q)\bigr] - \beta^2 D(Q)(s, a) \label{eq:OurBiconj}
\end{align}
where
\begin{align}
    D(Q)(s, a):=\min _{\zeta \in \mathbb{R}} \mathbb{E}_{s^{\prime} \sim P(s, a)}\left[\left(V_{Q}\left(s^{\prime}\right)-\zeta\right)^2 \mid s, a\right]\label{eq:D(Q)}
\end{align}
(b) Define the minimizer (over all states and actions) of objective \eqref{eq:D(Q)} as
$$ \zeta^*: (s,a)\mapsto \arg\min_{\zeta\in \mathbb{R}}\mathbb{E}_{ s^\prime \sim P(s, a)}\left[\left(V^\ast(s^\prime)- \zeta\right)^2\mid s, a\right]$$
then
$r(s,a) = Q^*(s,a)-\beta \zeta^*(s,a)$.
\end{lem}
\noindent The reformulation of $\lbe$ proposed in Lemma \ref{lem:OurBiconj} enjoys the advantage of minimizing the squared TD-error ($\mathcal{L}_{TD}$) but without bias. Combining Theorem \ref{thm:mainopt} and Lemma \ref{lem:OurBiconj}, we arrive at the following Theorem \ref{thm:algoEQ}, which gives the expected risk minimization formulation of IRL we propose.

\begin{thm}
\label{thm:algoEQ} $Q^*$ is uniquely identified by expected risk minimization, i.e.,  
\begin{align}
     &\underset{Q\in \mathcal{Q}}{\min }  \;\mathcal{R}_{exp}(Q) \notag
     \\
     &=\min _{Q \in \mathcal{Q}} \mathbb{E}_{(s, a) \sim \pi^*, \nu_0}\bigl[{\mathcal{L}_{N L L}(Q)(s, a)} +\lambda \mathbbm{1}_{a=a_s} \bigl\{\mathbb{E}_{s^{\prime} \sim P(s, a)}\left[{\mathcal{L}_{T D}(Q)\left(s, a, s^{\prime}\right)}\right]-\beta^2 {D(Q)(s, a)}\bigr\} \notag
     \\
    &=\min _{Q \in \mathcal{Q}}\max_{\zeta\in \mathbb{R}^{S\times A}} \mathbb{E}_{(s, a) \sim \pi^*, \nu_0, s^{\prime} \sim P(s, a)}\bigl[\underbrace{\textcolor{blue}{-\log \left(\hat{p}_Q(a \mid s)\right)}}_{1)} + \lambda\mathbbm{1}_{a=a_s}\bigl\{\underbrace{\textcolor{red}{\bigl(\hat{\mathcal{T}} Q\left(s, a, s^{\prime}\right)-Q(s, a)\bigr)^2}}_{{2)}} \notag
    \\
    & \quad -\beta^2 \underbrace{ \bigl(\textcolor{orange}{\left(V_{Q}\left(s^{\prime}\right)-\zeta(s,a)\right)^2} }_{{3)}}\bigr\} \bigr]\label{eq:AlgoOpt}
\end{align}

Furthermore, $r(s, a)=Q^*(s, a) - \beta \zeta^*(s, a)$ where $\zeta^*$ is defined in Lemma \ref{lem:OurBiconj}.
\end{thm}

\noindent Equation \eqref{eq:AlgoOpt} in Theorem \ref{thm:algoEQ} is a mini-max problem in terms of $Q\in\mathcal{Q}$ and the introduced dual function $\zeta\in \mathbb{R}^{S\times A}$. To summarize, term 1) is the negative log-likelihood equation, term 2) is the TD error, and term 3) introduces a dual function $\zeta$.
The introduction of the dual function $\zeta$ in term 3) may seem a bit strange. 
In particular, note that $\arg\max_{\zeta \in \mathbb{R}} -\mathbb{E}_{s^{\prime} \sim P(s, a)}\left[\left(V_Q\left(s^{\prime}\right)-\zeta\right)^2 \mid s, a\right]$ is just $\zeta = \mathbb{E}_{s'\sim P(s,a)}[V\left(s^{\prime}\right)|s,a]$. 
However, we do not have access to the transition kernel and the state and action spaces may be large. 
Instead, we think of $\zeta$ as a function of states and actions, $\zeta(s,a)$ as introduced in Lemma~\ref{lem:OurBiconj}. This parametrization allows us to optimize over a class of functions containing $\zeta(s,a)$ directly. 

Given the minimax resolution for the expected risk minimization problem in Theorem \ref{thm:algoEQ} finds $Q$ under an infinite number of data, we are now ready to discuss the case when we are only given a finite dataset $\mathcal{D}$ instead. In this case, we solve the \textit{empirical risk minimization} problem.

\begin{defn}[Empirical risk minimization problem]
\label{def:ERM}
Given $N := |\mathcal{D}|$ where $\mathcal{D}$ is a finite dataset. An empirical risk minimization problem is defined as the problem of finding $Q$ that minimizes the empirical risk $\mathcal{R}_{emp}(Q;\mathcal{D})$, which is defined as
\begin{align}
     &\;\mathcal{R}_{emp}(Q;\mathcal{D}):=\max_{\zeta\in \mathbb{R}^{S\times A}}\frac{1}{N}  \sum_{(s,a,s^\prime)\in \mathcal{D}}\notag
    \\
    &\bigl[\textcolor{blue}{-\log \left(\hat{p}_Q(a \mid s)\right)} + 
   \lambda\mathbbm{1}_{a=a_s}\bigl\{\textcolor{red}{\bigl(\hat{\mathcal{T}} Q\left(s, a, s^{\prime}\right)-Q(s, a)\bigr)^2} -\beta^2  \textcolor{orange}{\left(V_{Q}\left(s^{\prime}\right)-\zeta(s,a)\right)^2} \bigr\} \bigr]\notag
   \\
   &= \frac{1}{N}\bigl[\sum_{(s,a,s^\prime)\in \mathcal{D}}\bigl(\textcolor{blue}{-\log \left(\hat{p}_Q(a \mid s)\right)}\bigr)+ 
\lambda\mathbbm{1}_{a=a_s}\notag
    \\
    &\bigl(  \sum_{(s,a,s^\prime)\in \mathcal{D}} \textcolor{red}{\bigl(\hat{\mathcal{T}} Q\left(s, a, s^{\prime}\right)-Q(s, a)\bigr)^2}  -\beta^2 \min_{\zeta\in \mathbb{R}^{S\times A}} 
   \sum_{(s,a,s^\prime)\in \mathcal{D}} \textcolor{orange}{\left(V_{Q}\left(s^{\prime}\right)-\zeta(s,a)\right)^2}\bigr) \bigr] \label{eq:EmpiricalERMIRL}
\end{align}
\end{defn}

