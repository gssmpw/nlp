
\section{Introduction}\label{sec:Intro}
Learning from previously collected datasets has become an essential paradigm in sequential decision-making problems where exploration during interactions with the environment is infeasible (e.g., self-driving cars, medical applications) or leveraging large-scale offline data is preferable (e.g., social science, recommendation systems, and industrial automation) \citep{levine2020offline}. However, in such cases, defining a reward function (a flow utility function) that accurately captures the underlying decision-making process is often challenging due to the unobservable/sparse rewards \citep{zolna2020offline} and complexity of real-world environments \citep{foster2021offline}. To circumvent these limitations, learning from expert demonstrations has gained prominence, motivating approaches such as Imitation Learning (IL) and offline Inverse Reinforcement Learning (offline IRL) or equivalently, Dynamic Discrete Choice (DDC) model estimation\footnote{Refer to Section \ref{sec:DDCIRLequiv} for the equivalence between Offline Maximum Entropy IRL (MaxEnt-IRL) and DDC.}.

While IL directly learns a policy by mimicking expert actions, it is susceptible to \textit{distribution shift}, i.e., when the testing environment (reward, transition function) is different from the training environment. On the other hand, offline IRL aims to infer the underlying reward function that best explains expert behavior. Given this reward function, a new policy can be trained after a change in the environment's transition dynamics (e.g., modifications in recommendation systems) or in the reward function (e.g., marketing interventions). This capability enables offline IRL to be employed in counterfactual simulations, such as evaluating the effects of different policy decisions without direct experimentation. However, an imprecise reward function can lead to suboptimal policy learning and unreliable counterfactual analyses, ultimately undermining its practical utility. As a result, offline IRL's key metric becomes the \textit{precision} of reward inference.%, which determines how well the inferred reward function generalizes across different environments and supports robust decision-making.

While the precise reward function estimation objective has been studied in recent offline IRL literature, theoretically guaranteed existing methods have been limited to explicitly learning a transition model (e.g., \citet{zeng2023understanding}). However, if relearning the transition function is required every time it changes, the premise of IRL for counterfactual simulations may be undermined. The Dynamic Discrete Choice (DDC) literature in econometrics has separately explored the problem towards the goal of precise reward estimation \citep{rust1994structural, hotz1993conditional, aguirregabiria2007sequential, su2012constrained}. However, existing methodologies with theoretical precision guarantees suffer from the curse of dimensionality \citep{geng2023data}: computational complexity exponentially grows as state dimension increases. Most importantly, in both IRL and DDC literature, theoretical guarantees of precise reward estimation have been limited to linear reward structures \citep{zeng2023understanding} or monotone value function structure \cite{feng2020global}. This motivates us to ask the following question:
\vspace{0.2cm}
\begin{center}
    \textit{Can we propose a scalable gradient-based method to infer rewards  (or Q$^*$ function) while provably ensuring global optimality with no assumption on reward structure/transition function knowledge?}
\end{center}
\vspace{0.2cm}

\noindent \textbf{Our contributions. }In this paper, we propose an Empirical Risk Minimization (ERM)--based gradient-based method for IRL/DDC as an inverse Q-learning method. This method provably finds the true $Q^\ast$ function (up to statistical error, which diminishes at an $O(1/N)$ rate with $N$ samples) with $O(1/T)$ rate of convergence, where $T$ is the number of gradient iterations. In addition, the true reward function can be computed from estimated $Q^\ast$ with no extra statistical or computational cost given the estimated $Q^\ast$ function. In developing this method, we make the following technical contributions:

\begin{itemize}[leftmargin=0.3cm]
    \item We propose an empirical risk minimization (ERM) problem formulation, which we refer to as ERM-IRL in the IRL literature and ERM-DDC in the DDC literature, reflecting the shared problem. This formulation allows us to circumvent the need for explicit transition function estimation. Notably, this formulation also allows us to conclude that imitation learning (IL) is a strictly easier problem than IRL/DDC estimation problem. 
    \item We show that the objective function of the ERM-IRL satisfies the Polyak-≈Åojasiewicz (PL) condition, which is a weaker but equally useful alternative to strong convexity for providing theoretical convergence guarantees. This is enabled by showing that each of its two components -- expected negative log-likelihood and mean squared Bellman error -- satisfy PL condition\footnote{Sum of two PL functions are is necessary PL; in the proof, we show that our case is an exception.}. 
    \item Since the mean squared Bellman error term is a solution to a strongly concave inner maximization problem \citep{dai2018sbeed, patterson2022generalized}, minimization of the ERM-IRL objective becomes a mini-max problem with two-sided PL condition \citep{yang2020global}. Using this idea, we propose an alternating gradient ascent-decent algorithm that provably converges to the true $Q^*$, which is the unique saddle point of the problem.
\end{itemize}
In addition to establishing theoretical global convergence guarantees, we demonstrate the empirical effectiveness of the algorithm through standard benchmark simulation experiments. 
Specifically, we evaluate using a series of simulations: (1) The Rust bus engine replacement problem \citep{rust1987optimal}, which is the standard framework for evaluation used in the dynamic discrete choice literature, and (2) A high-dimensional variant of the Rust bus-engine problem, where we allow a very large state space.
% and (3) OpenAI gym benchmark environment experiments with a discrete action space (Lunar Lander, Acrobot, and Cartpole) \cite{brockman2016openai}. 
In both settings, we show that our algorithm outperforms/matches the performance of existing approaches. It is particularly valuable in large state-space settings, where many of the standard algorithms become infeasible due to their need to estimate state-transition probabilities. We expect our approach to be applicable to a variety of business and economic problems where the state and action space are infinitely large, and firms/policy-makers do not have {\it a priori} knowledge of the parametric form of the reward function and/or state transitions. 

The remainder of the paper is organized as follows. In Section \ref{sec:Related}, we discuss related work in greater detail. Section \ref{sec:SetupBackgrounds} introduces the problem setup and provides the necessary background. In Section \ref{sec:ERM-IRL}, we present the ERM-IRL framework, followed by an algorithm for solving it in Section \ref{sec:Algorithm}. Section \ref{sec:Analysis} establishes the global convergence guarantees of the proposed algorithm. Finally, Section \ref{sec:Experiments} presents experimental results demonstrating the effectiveness of our approach.
