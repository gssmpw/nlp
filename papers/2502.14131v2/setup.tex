
\section{Problem set-up and backgrounds}\label{sec:SetupBackgrounds}
%In this section, we introduce the Offline Inverse Reinforcement Learning (Offline IRL) problem and the Dynamic Discrete Choice (DDC) model estimation problem. We also establish their equivalence. 

We consider a single-agent Markov Decision Process (MDP) defined as a tuple $\left(\mathcal{S}, \mathcal{A}, P, \nu_0, r, \beta\right)$ where $\mathcal{S}$ denotes the state space and $\mathcal{A}$ denotes a finite action space, $P \in \Delta_{\mathcal{S}}^{\mathcal{S} \times \mathcal{A}}$ is a Markovian transition kernel, $\nu_0 \in \Delta_{\mathcal{S}}$ is the initial state distribution over $\mathcal{S}$,  $r \in \mathbb{R}^{\mathcal{S} \times \mathcal{A}}$ is a deterministic reward function and $\beta \in(0,1)$ a discount factor.  
Given a stationary Markov policy $\pi \in \Delta_{\mathcal{A}}^{\mathcal{S}}$, an agent starts from initial state $s_0$ and takes an action $a_h\in \mathcal{A}$ at state $s_h\in \mathcal{S}$ according to $a_h\sim\pi\left(\cdot \mid s_h\right)$ at each period $h$. Given an initial state $s_0\sim \nu_0$, we define the distribution of state-action sequences for policy $\pi$ over the sample space $(\mathcal{S} \times \mathcal{A})^{\infty}=\left\{\left(s_0, a_0, s_1, a_1, \ldots\right): s_h \in \mathcal{S}, a_h \in \mathcal{A}, h \in \mathbb{N}\right\}$ as $\mathbb{P}_{\nu_0,\pi}$. 
We also use $\mathbb{E}_{\nu_0,\pi}$ to denote the expectation with respect to $\mathbb{P}_{\nu_0,\pi}$.

\subsection{Setup: Maximum Entropy-Inverse Reinforcement Learning (MaxEnt-IRL) %\protect\footnote{Entropy regularized IRL's equivalence with dynamic discrete choice (DDC) model is discussed in Appendix \ref{sec:DDCIRLequiv}.}
} 
\label{sec:IRLIntro}

 Following existing literature \citep{geng2020deep, fu2017learning, ho2016generative}, we consider the \textit{entropy-regularized} optimal policy, which is defined as
$$
\pi^*:=\operatorname{argmax}_{\pi \in \Delta_{\mathcal{A}}^{\mathcal{S}}}\mathbb{E}_\pi\bigl[\sum_{h=0}^{\infty} \beta^h \bigl(r(s_h, a_h)+\lambda\mathcal{H}(\pi(\cdot \mid s_h))\bigr)\bigr]
$$ 
where $\mathcal{H}$ denotes the Shannon entropy and $\lambda$ is the regularization coefficient. Throughout, we make the following assumption on agent's decisions.

\begin{asmp}\label{ass:IRLoptimaldecision} When interacting with the MDP $\left(\mathcal{S}, \mathcal{A}, P, \nu_0, r, \beta\right)$, each agent follows the entropy-regularized optimal stationary policy $\pi^*$.
\end{asmp}

\noindent Throughout the paper, we use $\lambda = 1$, the setting which is equivalent to dynamic discrete choice (DDC) model with mean zero T1EV distribution  (Appendix \ref{sec:DDCIRLequiv}); all the results of this paper easily generalize to other values of $\lambda$. Given $\pi^*$, we define the \textit{value function} $V^*$ as:
$$
V^*(s) := \mathbb{E}_{\pi^*} \biggl[\sum_{h=0}^\infty \beta^h 
\bigl(r(s_h, a_h) + \mathcal{H}(\pi^*(\cdot \mid s_h))\bigr) \biggm| s_0 = s\biggr].
$$
Similarly, we define the $Q^*$ function as follows:
$$Q^*(s, a):=r\left(s, a\right)+\beta \cdot \mathbb{E}_{s^\prime \sim P(s,a)}\left[{V}^*\left(s^\prime\right)\mid s, a\right]$$
Given state $s$ and policy $\pi^*$, let $\mathbf{q} = [q_1 \ldots q_{|\mathcal{A}|}]$ denote the probability distribution over the action space $\mathcal{A}$, such that:
\begin{align}  
    q_a= \frac{\exp \left({Q^*(s, a)}\right)}{\sum_{a^\prime\in \mathcal{A}} \exp \left({Q^*(s, a^\prime)}\right)} \text{ for } a\in \mathcal{A} \notag
\end{align}
Then, according to Assumption \ref{ass:IRLoptimaldecision}, the value function $V^*$ must satisfy the recursive relationship defined by the \textit{Bellman equation} as follows:
\begin{align}
&V^*(s)=\max_{\mathbf{q} \in \triangle_\mathcal{A}}\{
\mathbb{E}_{a\sim\mathbf{q}}\bigl[r(s, a) +\beta \mathbb{E}_{s^\prime\sim P(s,a)}[V^*(s^\prime)\mid s, a]\bigr] +\mathcal{H}(\mathbf{q})\} \notag
\end{align}
Further, we can show that (see Appendix \ref{sec:IRLentropy}):
\begin{align}
    V^*(s)&=\ln \left[\sum_{a\in \mathcal{A}}\exp\left(Q^*(s, a)\right)\right] \notag
    \\
    \pi^*(a\mid s) &= \frac{\exp \left({Q^*(s, a)}\right)}{\sum_{a^\prime\in \mathcal{A}} \exp \left({Q^*(s, a^\prime)}\right)} \text{ for } a\in \mathcal{A} \notag
    \\
     Q^\ast(s,a)&=r(s, a)+\beta \cdot \mathbb{E}_{s^\prime \sim P(s, a)}\bigl[\log\sum_{a^\prime\in\mathcal{A}}\exp(Q^*(s^\prime,a^\prime)) \mid s, a\bigr]\label{eq:QBellman}
\end{align}
%That is, the optimal stationary policy $\pi^*$ satisfies $$$$ 
Throughout, we define a function $V_Q$ as 
\begin{align}
    V_Q(s)&:=\ln \left[\sum_{a\in \mathcal{A}}\exp\left(Q(s, a)\right)\right] \notag
\end{align}
where $ V_{Q^\ast} = V^\ast$. 


\subsection{Setup: Dynamic Discrete Choice (DDC) model}
Following the literature \citep{rust1994structural, magnac2002identifying}, we assume that the reward the agent observes at state $s\in\mathcal{S}$ and $a\in\mathcal{A}$ can be expressed as $r(s,a) + \epsilon_a$, 
where $\epsilon_a\overset{i.i.d.}{\sim}  G(\delta, 1)$ is the random part of the reward, where $G$ is Type 1 Extreme Value (T1EV) distribution (i.e., Gumbel distribution)\footnote{This reward form is often referred to as additive and conditionally independent form.}. The mean of $G(\delta, 1)$ is $\delta + \gamma$, where $\gamma$ is the Euler constant. Throughout the paper, we use $\delta=-\gamma$, which makes $G$ a mean 0 distribution.\footnote{All the results of this paper easily generalize to other values of $\delta$.} Under this setup, we consider the optimal policy and its corresponding value function defined as
$$\pi^\ast:=\operatorname{argmax}_{\pi \in \Delta_{\mathcal{A}}^{\mathcal{S}}}\mathbb{E}_{\nu_0,\pi,G}\left[\sum_{h=0}^{\infty} \beta^h( r\left(s_h, a_h\right)+\epsilon_{a_h})\right]$$
$$V^\ast(s):=\operatorname{max}_{\pi \in \Delta_{\mathcal{A}}^{\mathcal{S}}}\mathbb{E}_{\nu_0,\pi,G}\left[\sum_{h=0}^{\infty} \beta^h( r\left(s_h, a_h\right)+\epsilon_{a_h})\mid s_0=s\right]$$
Throughout, we make the following assumption on agent's decisions.

\begin{asmp}\label{ass:DDCoptimaldecision} When interacting with the MDP $\left(\mathcal{S}, \mathcal{A}, P, \nu_0, r, \beta\right)$, each agent follows the entropy-regularized optimal stationary policy $\pi^*$.
\end{asmp}
According to Assumption \ref{ass:DDCoptimaldecision}, the value function $V^\ast$ must satisfy the recursive relationship, often called the Bellman equation, as follows:
\begin{align}
V^\ast(s)&=\mathbb{E}_{\boldsymbol{\epsilon}}\left[\max _{a \in \mathcal{A}}\left\{r(s, a)+\epsilon_a+\beta \cdot \mathbb{E}\left[V^\ast\left(s^{\prime}\right) \mid s, a\right]\right\}\right] \notag
\\
&=\ln \left[\sum_{a\in \mathcal{A}}\exp\left(r\left(s, a\right)+\beta \cdot \mathbb{E}\left[V^\ast\left(s^\prime\right)\mid s, a\right]\right)\right]  \notag
\end{align}
where the second equality is from Lemma \ref{lem:GumbelMax}.
We further define the $Q^\ast$ function as
$$
Q^*(s, a):=r(s, a)+\beta \cdot \mathbb{E}_{s^{\prime} \sim P(s, a)}\left[V^*\left(s^{\prime}\right) \mid s, a\right]
$$
We can show that: (see Appendix \ref{sec:SingleDDC}) 
\begin{align}
    V^*(s)&=\ln \left[\sum_{a\in \mathcal{A}}\exp\left(Q^*(s, a)\right)\right] \notag
    \\
    \pi^*(a\mid s) &= \frac{\exp \left({Q^*(s, a)}\right)}{\sum_{a^\prime\in \mathcal{A}} \exp \left({Q^*(s, a^\prime)}\right)} \text{ for } a\in \mathcal{A} \notag
    \\
      Q^*(s,a) &=r(s, a)+\beta \cdot \mathbb{E}_{s^\prime \sim P(s, a)}    \bigl[\log\sum_{a^\prime\in\mathcal{A}}\exp(Q^*(s^\prime,a^\prime)) \mid s, a\bigr]\label{eq:QBellmanDDC}
\end{align}

\subsection{DDC -- MaxEnt-IRL Equivalence and unified problem statement}
The Bellman equations of MaxEnt-IRL with \(\lambda = 1\) (Equation \ref{eq:QBellman}) and DDC with \(\delta = -\gamma\) (Equation \ref{eq:QBellmanDDC}) are equivalent. Consequently, the optimal \(Q^*\) values obtained from solving these Bellman equations are the same for both MaxEnt-IRL and DDC. Furthermore, the optimal policy induced by \(Q^*\) is identical in both frameworks. Therefore, we can infer that solving one problem is equivalent to solving the other. Throughout, all the discussions we make for \(\lambda = 1\) in MaxEnt-IRL and \(\delta = 1\) in DDC extend directly to any \(\lambda \neq 1\) and \(\delta \neq 1\), respectively.

In both settings, the goal is to recover the underlying reward function $r$ that explains an agent's demonstrated behavior. Given the equivalence between them, we can now formulate a \textit{unified problem statement} that encompasses both Offline Maximum Entropy Inverse Reinforcement Learning (Offline MaxEnt-IRL) and the Dynamic Discrete Choice (DDC) model estimation. 

To formalize this, consider a dataset consisting of state-action-next state sequences collected from an agent's behavior:  
$\mathcal{D}:=\left(\left(s_0, a_0, s_0^\prime\right), \left(s_1, a_1,s_1^\prime\right), \ldots,  \left(s_N, a_N,s_N^\prime\right)\right)$. 
Following Assumption \ref{ass:IRLoptimaldecision}, we assume that the data was generated by the agent playing the optimal policy $\pi^*$ when interacting with the MDP $\left(\mathcal{S}, \mathcal{A}, P, \nu_0, r, \beta\right)$.
\begin{defn}[The unified problem statement]\label{def:IRLproblem}

    The objective of offline MaxEnt-IRL and DDC can be defined as learning a function $\hat{r}\in\mathcal{R} \subseteq\mathbb{R}^{\bar{\mathcal{S}}\times \mathcal{A}}$ that minimizes the mean squared prediction error with respect to data distribution (i.e., expert policy's state-action distribution) from offline data $\mathcal{D}$ such that: 
\begin{align}
    \underset{\hat{r}\in \mathcal{R}}{\operatorname{argmin}} \; \mathbb{E}_{(s,a)\sim \nu_0, \pi^*}[(\hat{r}(s,a)-r(s,a))^2] \label{eq:rObjective}    
\end{align}
where 
$$\overline{\mathcal{S}}:=\left\{s \in \mathcal{S} \mid \operatorname{Pr}\left(s_h=s \mid s_0 \sim \nu_0, \pi^*\right)>0 \text { for some } h \geq 0\right\}$$ defines the expert policy's coverage, which consists of states that are reachable with nonzero probability under the expert's optimal policy \(\pi^*\). \footnote{For every \(s \in \bar{\mathcal{S}}\), every action \(a \in \mathcal{A}\) occurs with probability strictly greater than zero, ensuring that the data sufficiently covers the relevant decision-making space.}  
\end{defn}
Restricting to \(\bar{\mathcal{S}}\) is essential, as the dataset \(\mathcal{D}\) only contains information about states visited under \(\pi^*\). Inferring rewards beyond this set would be ill-posed due to a lack of data, making \(\bar{\mathcal{S}}\) the natural domain for learning. Similarly, Computing MSE using the expert policy's state-action distribution is natural since the goal is to recover the reward function that explains the expert’s behavior. 



\subsection{Identification}
 
As we defined in Definition \ref{def:IRLproblem}, our goal is to learn the agent's reward function $r(s, a)$ given offline data $\mathcal{D}$.
However, without additional assumptions on the reward structure, this problem is ill-defined because many reward functions can explain the optimal policy \citep{fu2017learning, ng1999policy}. To address this issue, following the DDC literature  \citep{rust1994structural, magnac2002identifying, hotz1993conditional} and recent IRL literature \citep{geng2020deep}, we assume that there is an \textit{anchor action} $a_s$ in each state $s$, such that the reward for each of state - anchor action combinations are known.

\begin{asmp}\label{ass:anchor} For all $s\in\mathcal{S}$, there exists an action $a_s\in \mathcal{A}$ such that $r(s,a_s)$ is known.
\end{asmp} 

\noindent Note that the optimal policy remains the same irrespective of the choice of the anchor action $a_s$ and the reward value at the anchor action $r(s, a_s)$ (at any given $s$). As such, Assumption \ref{ass:anchor} only helps with identification and does not materially affect the estimation procedure. That is, we can arbitrarily choose $a_s$ and arbitrarily set $r(s,a_s)$ for all $s\in\mathcal{S}$.
In Theorem \ref{thm:MagnacThesmar}, we formally establish that Assumptions \ref{ass:IRLoptimaldecision} and \ref{ass:anchor} uniquely identify $Q^\ast$ and $r$. (See Section \ref{sec:PfMagnac} for the proof.)

\begin{thm}[\cite{magnac2002identifying}]\label{thm:MagnacThesmar}%\todo{Add proof}

Given discount factor $\beta$, transition kernel $P\in \Delta_\mathcal{S}^{\mathcal{S}\times \mathcal{A}}$ and optimal policy $\pi^*\in\Delta_\mathcal{A}^{\mathcal{S}}$, under Assumptions \ref{ass:IRLoptimaldecision} and \ref{ass:anchor}, the solution to the following system of equations:
\begin{equation}
\left\{
\begin{array}{l}
    \dfrac{\exp({Q}\left(s,a\right))}{\sum_{a^\prime\in \mathcal{A}} \exp({Q}\left(s,a^\prime\right))} = \pi^*(\;a
    \mid s) \; \forall s\in \mathcal{S}, a\in\mathcal{A}
    \\[1em]
    r(s, a_s)+\beta \cdot \mathbb{E}_{s^{\prime} \sim P(s, a_s)}\left[V_Q(s^\prime) \mid s, a_s\right]  = Q(s, a_s)          \;\; \; \forall s\in \mathcal{S} \notag
\end{array}
\right.
\label{eq:HotzMillereqs}
\end{equation}
gives a unique $Q=Q^*$. Furthermore, $r$ is obtained by solving:
\begin{align}
    &r(s,a) = Q^\ast(s, a) - \beta \cdot \mathbb{E}_{s^{\prime} \sim P(s, a)}\bigl[V_{Q^\ast}(s^\prime) \mid s, a\bigr]. \label{eq:rbyBellman}
\end{align}
for all $ s\in \mathcal{S}, a\in\mathcal{A}$.
\end{thm}

In the first part of the theorem, we show that, after constraining the reward functions for anchor actions, we can recover the unique $Q^\ast$-function for the optimal policy from the observed choices and the Bellman equation for the anchor-action (written in terms of log-sum-exp of $Q$-values). The second step follows naturally, where we can show that reward functions are then uniquely recovered from $Q^*$-functions using the Bellman equation.

\subsection{Bellman error and Temporal difference (TD) error}\label{sec:BE&TD}
There are two key concepts used for describing a gradient-based algorithm for IRL/DDC: the Bellman error and the Temporal difference (TD) error. In this section, we define each of them and discuss their relationship. Let us start from defining $\mathcal{Q}=\left\{Q: \mathcal{S} \times \mathcal{A} \rightarrow \mathbb{R} \mid\|Q\|_{\infty}<\infty\right\}$. By \citet{rust1994structural}, $\beta<1$ implies $Q^* \in \mathcal{Q}$. Next, we define the \textit{Bellman operator} as $\mathcal{T}: \mathcal{Q} \mapsto \mathcal{Q}$ as follows:
$$\mathcal{T}Q(s, a) \notag := r(s, a) + \beta \cdot \mathbb{E}_{s' \sim P(s, a)} \bigl[ V_Q(s') \bigr]
$$
According to the Bellman equation shown in Equation \eqref{eq:QBellman}, $Q^*$ satisfies $\mathcal{T}Q^*(s, a) - Q^*(s, a)=0$; in fact, $Q^*$ is the unique solution to $\mathcal{T}Q(s, a) - Q(s, a)=0$; see \citep{rust1994structural}. 
Based on this observation, we define the following notions of error. 

\begin{defn}\label{def:BE}
    We define the \textit{Bellman error} for $Q\in\mathcal{Q}$ at $(s,a)$ as $\mathcal{T}Q(s, a) - Q(s, a)$. Furthermore, we define the \textit{Squared Bellman error} and the \textit{Expected squared Bellman error}  as $$\mathcal{L}_{\text{BE}}(Q)(s, a) := \left( \mathcal{T}Q(s, a) - Q(s, a) \right)^2$$
$$
\overline{
\mathcal{L}_{\text{BE}}}(Q) = \mathbb{E}_{(s, a) \sim \pi^*,\, \nu_0} \left[ \mathcal{L}_{\text{BE}}(Q)(s, a) \right]
$$
\end{defn}

\noindent In practice, we don't have direct access to $\mathcal{T}$ unless we know (or have a consistent estimate of) the transition kernel $P\in\Delta_\mathcal{S}^{\mathcal{S}\times\mathcal{A}}$. Instead, we can compute an empirical \textit{Sampled Bellman operator} $\hat{\mathcal{T}}$, defined as 
$$
\hat{\mathcal{T}}Q(s, a, s') = r(s, a) + \beta \cdot V_Q(s^\prime).
$$
\begin{defn}
    We define \textit{Temporal-Difference (TD) error} for $Q$ at the transition $(s, a, s^\prime)$, \textit{Squared TD error}, and \textit{Expected squared TD error} as follows:
\begin{align}
    &\delta_Q(s, a, s'):=\hat{\mathcal{T}}Q(s, a, s') - Q(s, a) \notag
    \\
 &\mathcal{L}_{\text{TD}}(Q)(s,a,s^\prime):= \left(\hat{\mathcal{T}}Q(s, a, s') - Q(s, a)\right)^2 \notag
 \\
 &\overline{
\mathcal{L}_{\text{TD}}}(Q) := \mathbb{E}_{(s, a) \sim \pi^*,\, \nu_0} \left[ \mathbb{E}_{s' \sim P(s, a)} \left[ \mathcal{L}_{\text{TD}}(Q)(s, a, s') \right] \right] \notag
\end{align}
\end{defn}

\noindent Lemma \ref{lem:expTD=BE} states the relationship between the TD error terms and Bellman error terms.
\begin{lem}[Expectation of TD error is equivalent to BE error]
\label{lem:expTD=BE}
    $$ \mathbb{E}_{s' \sim P(s, a)} \left[ \hat{\mathcal{T}}Q(s, a, s') \right] = \mathcal{T}Q(s, a) 
    $$
    $$
    \mathbb{E}_{s' \sim P(s, a)} \left[ \delta_Q(s, a, s')\right] =\mathcal{T}Q(s, a) - Q(s, a). $$
\end{lem}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


