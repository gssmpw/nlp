\section{Related works}
\label{sec:Related}

\begin{table}[h]
\centering
\scalebox{0.75}{ % Adjust scale factor as needed
\renewcommand{\arraystretch}{1.2}
\begin{tabular}{l c c c c c c c c} % 9 columns now (added one)
\toprule
\textbf{Method} & \makecell[c]{\textbf{Transition}\\\textbf{Estimation-Free}} & \makecell[c]{\textbf{Anchor}\\\textbf{Action}} & \makecell[c]{\textbf{Non-}\\\textbf{Parametric}} & \makecell[c]{\textbf{Gradient-}\\\textbf{Based}} & \makecell[c]{\textbf{Reward}\\\textbf{Estimation}} & \textbf{Scalability} & \makecell[c]{\textbf{Statistical}\\\textbf{Complexity}} & \makecell[c]{\textbf{Globally}\\\textbf{Convergent}} \\
\midrule
Rust [\citeyear{rust1987optimal}] & & \cmark & & & \cmark & & $1/\sqrt{N}$ & \lmark (Linear only) \\
CCP [\citeyear{hotz1993conditional}] & & \cmark & & & \cmark & &$1/\sqrt{N}$ & \lmark (Linear only) \\
MPEC [\citeyear{su2012constrained}] & & \cmark & & & \cmark & &$1/\sqrt{N}$& \lmark (Linear only) \\
BC [\citeyear{torabi2018behavioral}] & \cmark & & \cmark & \cmark & & \cmark & & \\
AVI [\citeyear{adusumilli2019temporal}] & \cmark & \cmark & \cmark & & \cmark & & & \\
Semi-gradient \citeyear{adusumilli2019temporal} & \cmark & \cmark & & \cmark & \cmark & \cmark & & \\
IQ-Learn [\citeyear{garg2021iq}] & \cmark & & \cmark & \cmark & & \cmark & & \\
SAmQ [\citeyear{geng2023data}] & \cmark & & \cmark & & \cmark & \lmark & & \\
RP [\citeyear{barzegary2022recursive}] & & & \cmark & & \cmark & \cmark & & \\
Clare [\citeyear{yue2023clare}] & & & \cmark & \cmark & & \cmark & & \\
ML-IRL [\citeyear{zeng2023understanding}] & & & \cmark & \cmark & \cmark & \cmark & & \lmark (Linear only) \\
\midrule
\textbf{Ours} & \cmark & \cmark & \cmark & \cmark & \cmark & \cmark & $1/N$ & \cmark \\
\bottomrule
\end{tabular}
} % End scalebox
\caption{Comparison of IRL and DDC methods. `Transition Estimation-Free'' indicates whether the method avoids explicit transition function estimation. A method is `Scalable'' if it handles state spaces of at least $20^{10}$. ``Reward Estimation'' excludes occupancy matching-based methods (e.g., IQ-Learn, Clare); see Appendix \ref{sec:occupancy}. }
\end{table}

\subsection{Dynamic discrete choice model estimation literature}
In the econometrics literature, stochastic decision-making behaviors are usually considered to come from the random utility model \citep{mcfadden2001economic}, which often assumes that the effect of unobserved covariates appear in the form of additive and conditionally independent randomness in agent utilities \cite{rust1994structural}.  
The seminal paper by Rust \citep{rust1987optimal} pioneered this literature, demonstrating that a DDC model can be solved by solving a maximum likelihood estimation problem that runs above iterative dynamic programming. As discussed in the introduction, this method suffers computational intractability in terms of number of state dimensions.

\cite{hotz1993conditional} introduced a method which is often called the two-step method conditional choice probability (CCP) method, where the CCPs and transition probabilities estimation step is followed by the reward estimation step. The reward estimation step avoids dynamic programming by combining simulation with the insight that differences in value function values can be directly inferred from data without solving Bellman equations. However, simulation methods are in principle trajectory-based numerical integration methods which also suffer scalability issues. Fortunately, we can sometimes avoid simulation altogether by utilizing the problem structure such as regenerative/terminal actions (known as finite dependence \citep{arcidiacono2011conditional}). Still, this method requires explicit estimation of the transition function, which is not the case in our paper. This paper established an insight that there exists a one-to-one correspondence between the CCPs and the differences in $Q^*$-function values, which was formalized as the identification result by \cite{magnac2002identifying}. 

\cite{su2012constrained} propose that we can avoid dynamic programming or simulation by formulating a nested linear programming problem with Bellman equations as constraints of a linear program. This formulation is based on the observation that Bellman equations constitute a convex polyhedral constraint set. While this linear programming formulation significantly increases the computation speed, it is still not scalable in terms of state dimensions.

As the above methods suffer scalability issues, methods based on parametric/nonparametric approximation have been developed. Parametric policy iteration \citep{benitez2000comparison} and sieve value function iteration \citep{arcidiacono2013approximating} parametrize the value function by imposing a flexible functional form.  \cite{norets2012estimation} proposed that neural network-based function approximation reduces the computational burden of Markov Chain Monte Carlo (MCMC) estimation, thereby enhancing the efficiency and scalability. \cite{geng2020deep} proposed that the one-to-one correspondence insight of \cite{hotz1993conditional} enables us to avoid reward parameterization and directly (non-parametrically) estimate value functions, along with solving a much smaller number of soft-Bellman equations which do not require reward parametrization to solve them. \cite{barzegary2022recursive} and \cite{geng2023data} independently proposed state aggregation/partition methods that significantly reduce the computational burden of running dynamic programming with the cost of optimality. While \cite{geng2023data} uses $k$-means clustering \citep{kodinariya2013review, sinaga2020unsupervised}, \citet{barzegary2022recursive} uses recursive partitioning (RP) \citep{athey2016recursive}. As discussed earlier, combining approximation with dynamic programming induces unstable convergence except when the true reward function is linear \cite{jiangoffline}. 

\cite{adusumilli2019temporal} proposed how to adapt two popular temporal difference (TD)-based methods (an approximate dynamic programming-based method and a semi-gradient descent method based on \cite{tsitsiklis1996analysis}) for DDC. As discussed earlier, approximate dynamic programming-based methods are known to suffer from a lack of provable convergence beyond linear reward models \citep{jiangoffline, wang2021instabilities}; the semi-gradient method is a popular, efficient approximation method that has limited theoretical assurance of convergence beyond linear value function approximation \citep{sutton2018reinforcement}. \citet{feng2020global} showed global concavity of value function under certain transition functions and monotonicity of value functions in terms of one-dimensional state, both of which are easily satisfied for applications in social science problems. However, those conditions are limitedly satisfied for the problems with larger dimensional state space. 

\subsection{Offline inverse reinforcement learning literature} \; In computer science literature, stochastic decision-making behaviors are modeled as `random choice'. That is, they assume that agents randomize their actions. The most widely used inverse reinforcement learning model, Maximum-Entropy inverse reinforcement learning (MaxEnt-IRL), assumes that the random choice happens due to agents choosing the optimal policy after penalization of the policy by its Shannon entropy \citep{ermon2015learning}. In addition to the equivalence of MaxEnt-IRL to DDC (See \cite{ermon2015learning}, also in Section \ref{sec:DDCIRLequiv}), the identifiability condition for DDC \citep{magnac2002identifying} was rediscovered by \cite{cao2021identifiability} for MaxEnt-IRL. \cite{zeng2023understanding} proposes a two-step maximum likelihood-based method that can be considered as a conservative version of CCP method of \cite{hotz1993conditional}\footnote{When there is no uncertainty in the transition function, approximated trajectory gradient of Offline IRL method degenerates to forward simulation-based gradient in CCP estimator method of \cite{hotz1993conditional}.}.  
Despite that their method is proven to be convergent, its global convergence was limitedly proven only for linear reward functions.

\cite{finn2016connection} and \cite{fu2017learning} showed that a myopic\footnote{See \cite{cao2021identifiability} for more discussion on this.} version of MaxEnt-IRL can be solved by the Generative Adversarial Network (GAN) training framework \citep{goodfellow2020generative}. This approach has been extended to $Q$-estimation methods that use fixed point iteration \citep{geng2020deep, geng2023data}. \cite{ni2021f} have shown that the idea of training an adversarial network can also be used to calculate the gradient direction for minimizing the myopic version of negative log likelihood\footnote{Minimizing negative log-likelihood is equivalent to minimizing KL divergence. See the Proof of Lemma \ref{lem:minMLE}.}. As the adversarial formulation of IRL is inherently myopic, it is limited suited for the task of reward inference. However, it is known to work well for behavioral cloning tasks \citep{torabi2018behavioral}. 

A family of methods starting from \cite{ho2016generative} tries to address the inverse reinforcement learning problem from the perspective of occupancy matching, i.e., finding a policy that best matches the behavior of data. \cite{garg2021iq} proposed how to extend the occupancy matching approach of \cite{ho2016generative} to directly estimate $Q$-function instead of $r$. Given the assumption that the Bellman equation holds, this approach allows a simple gradient-based solution, as the occupancy matching objective function they maximize becomes concave. \cite{yue2023clare} modifies \cite{ho2016generative} to conservatively deal with the uncertainty of transition function. Despite their simplicity, one caveat of occupancy matching approaches is that whether the estimated $Q$ from occupancy matching satisfies the Bellman equation is not trivial. %In fact, the occupancy matching objective is much closer to the Behavioral Cloning objective, which is unrelated to the Bellman equation (See Section \ref{sec:occupancy}). This incurs bias in the estimation of $Q$. 
In addition, computing $r$ from $Q$ using the Bellman equation is also not valid.