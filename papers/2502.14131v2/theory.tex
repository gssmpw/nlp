
\section{Theory and analysis of GLADIUS}\label{sec:Analysis}
As discussed in the previous section, Equation \eqref{eq:EmpiricalERMIRL} represents a mini-max optimization problem. 
Such problems are known to be globally solvable by a simple gradient ascent-descent algorithm when it is a concave-convex mini-max problem. 
However, the challenge is that Equation \eqref{eq:AlgoOpt} is not a concave-convex mini-max problem. 
Given $Q$, it determines $\zeta$ that serves as the Bayes-optimal estimator for $\mathbb{E}_{s^{\prime} \sim P(s, a)}\left[V_Q\left(s^{\prime}\right) \mid s, a\right]$. 
This implies that $-\mathbb{E}_{s^{\prime} \sim P(s, a)}\left[\left(V_Q\left(s^{\prime}\right)-\zeta\right)^2 \mid s, a\right]$ is strongly concave in $\zeta$. 
On the other hand, given such an optimal $\zeta$, $\mathcal{L}_{BE}(Q)(s,a)$ term is \textit{not} convex in $Q$ \cite{bas2021logistic}. The key result in this section is proving that both $\mathcal{L}_{BE}(Q)(s,a)$ and $\mathcal{L}_{\mathrm{NLL}}(Q)(s,a)= \left[-\log\left(\hat{p}_{Q}(a
    \mid s)\right)\right]$ satisfies the \textit{Polyak-Łojasiewicz} (PL) condition under certain assumptions, which is enough for Algorithm \ref{alg:estimation1} to converge to global optima.



\subsection{Polyak-Łojasiewicz (PL) in terms of $Q$}

 The Polyak-Łojasiewicz (PL) condition prevents the gradient from vanishing prematurely, keeping optimization progress steady. That is, it nearly possesses the smooth, fast convergence behavior of strongly convex functions. Throughout, we use $\|Q\|_{L^2\left(\pi^*, \nu_0\right)}:=\left(\mathbb{E}_{(s, a) \sim \pi^*, \nu_0}\left[Q(s, a)^2\right]\right)^{1 / 2}$ for $Q\in\mathcal{Q}$. 
    

\begin{defn} [Polyak-Łojasiewicz (PL) condition with respect to $L^2$ norm]
     A function $f:\mathcal{\mathcal{Q}}\mapsto \mathbb{R}$ is is said to satisfy the Polyak-Łojasiewicz (PL) condition with respect to $L^2$ norm with measure $\mu$ if $f$ has a nonempty solution set and a finite minimal value $f(Q^\ast)$ for $Q^\ast\in\mathcal{Q}$, and there exists some $c>0$ such that $\frac{1}{2}\|\nabla_Q f(Q)\|^2_{L^2(\mu)} \geq  c(f(Q)-$ $f(Q^\ast)), \forall x\in\mathcal{X}$.
\end{defn}
\textbf{Remark.} Note, in this definition, we are identifying $\mathcal{Q}$ as a subset of $\mathbb{R}^{S\times A}$ hence the derivative is defined appropriately.

\noindent To prove PL, we need the following Lemmas which describes the behavior of $\overline{\mathcal{L}_{\mathrm{NLL}}}(Q)$ and $\overline{\mathcal{L}_{\mathrm{BE}}}(Q)$.

\begin{lem}\label{lem:ConvexityMLE}
    $\overline{\mathcal{L}_{\mathrm{NLL}}}(Q):=\mathbb{E}_{(s, a)\sim \pi^*, \nu_0}  \left[\mathcal{L}_{\mathrm{NLL}}(Q)(s, a)\right]$ is convex and Lipschitz smooth in $Q$ in terms of $L^2(\pi^\ast, \nu_0)$ norm.
\end{lem}

\begin{lem} \label{lem:BELipschitz}
  $\overline{\mathcal{L}_{\mathrm{BE}}}(Q):=\mathbb{E}_{(s, a) \sim \pi^*, \nu_0}\left[\mathcal{L}_{\mathrm{BE}}(Q)(s, a)\right]$ is of $\mathcal{C}^2$ and Lipschitz smooth in $Q$ in terms of $L^2(\pi^\ast, \nu_0)$ norm.
\end{lem}
\noindent Given Lemma \ref{lem:ConvexityMLE} and \ref{lem:BELipschitz}, the following Theorems that $\overline{\mathcal{L}_{\mathrm{NLL}}}(Q)$ and $\overline{\mathcal{L}_{\mathrm{BE}}}(Q)$ satisfies PL condition.  

\begin{thm}\label{thm:BEenjoyPL} For given $s\in\mathcal{S}$ and $a\in\mathcal{A}$, $\mathcal{L}_{BE}(Q)(s,a)$ satisfies PL condition with respect to $Q$. Furthermore, 
   $\overline{\mathcal{L}_{BE}}(Q)$ and $\frac{1}{|\mathcal{D}|}\sum_{(s,a)\in\mathcal{D}}\mathcal{L}_{BE}(s,a)$ satisfies the PL condition with respect to $Q$ in terms of $L^2(\pi^\ast, \nu_0)$.
\end{thm}

\begin{thm}\label{thm:NLLenjoyPL} For given $s\in\mathcal{S}$ and $a\in\mathcal{A}$,
    $\mathcal{L}_{NLL}(Q)(s,a)$ satisfies PL condition with respect to $Q$. Furthermore, 
   $\overline{\mathcal{L}_{NLL}}(Q)$ and $\frac{1}{|\mathcal{D}|}\sum_{(s,a)\in\mathcal{D}}\mathcal{L}_{NLL}(s,a)$ satisfies the PL condition with respect to $Q$ in terms of $L^2(\pi^\ast, \nu_0)$.
\end{thm}
\noindent %U \eqref{eq:mainopt}, the objective function of our ERM formulation, $\overline{\mathcal{L}_{\mathrm{NLL}}}(Q)+\lambda \mathbbm{1}_{a=a_s}\overline{\mathcal{L}_{\mathrm{BE}}}(Q)$, also satisfy PL? 
In general, the sum of two PL functions is not necessarily PL. However, according to the following Lemma \ref{lem:f1f2sumPL}, our problem is a special case where such property holds, 

\begin{lem}\label{lem:f1f2sumPL} \;
\\
The expected risk 
    $\mathcal{R}_{exp}(Q) = \mathbb{E}_{(s, a)\sim \pi^*, \nu_0}  \left[\mathcal{L}_{\mathrm{NLL}}(Q)(s, a)\right]+\lambda \mathbb{E}_{(s, a)\sim \pi^*, \nu_0}  \left[\mathbbm{1}_{a=a_s}\mathcal{L}_{\mathrm{BE}}(Q)(s, a)\right]$ satisfies PL with respect to $Q$ in terms of $L^2(\pi^\ast, \nu_0)$. Furthermore, the empirical risk $\mathcal{R}_{emp}(Q) = \overline{\mathcal{L}_{NLL}}(Q)+\frac{1}{|\mathcal{D}|}\sum_{(s,a)\in\mathcal{D}}\mathbbm{1}_{a=a_s}\mathcal{L}_{NLL}(s,a)$ satisfies the PL condition with respect to $Q$ in terms of $L^2(\pi^\ast, \nu_0)$.
\end{lem}

\noindent We remark that this result by itself, establishes the PL condition in the \textit{tabular} setting with finite states and actions where the $Q$ function is parametrized as a vector/matrix in $\mathbb{R}^{S\times A}$. In the next section, we extend this to a more general hypothesis class.

\subsection{Polyak-Łojasiewicz (PL) in terms of $\theta$}
We now extend previous section to cases where the underlying state and action spaces are potentially featurized, i.e. $\mathcal{S}=\mathbb{R}^{\operatorname{dim}(\mathcal{S})} \text { and } \mathcal{A}=\mathbb{R}^{\operatorname{dim}(\mathcal{A})}$. (In this case, $L^2\left(\pi^*, \nu_0\right)$ norm is reduced to the (weighted) euclidean norm with dimension $\operatorname{dim}(\mathcal{S})+\operatorname{dim}(\mathcal{A})$.) When $\operatorname{dim}(\mathcal{S})+\operatorname{dim}(\mathcal{A})$ is very large, it is often preferable/necessary to approximate $Q^\ast$ using a set of parametrized functions 
$$\mathcal{Q}=\left\{Q_{\boldsymbol{\theta}}: \mathbb{R}^{\operatorname{dim}(\mathcal{S})+\operatorname{dim}(\mathcal{A})} \rightarrow \mathbb{R} \mid \boldsymbol{\theta} \in \Theta \subseteq \mathbb{R}^d, Q_{\boldsymbol{\theta}}\in\mathcal{F}\right\}$$
where $\mathcal{F}$ denotes a class of functions such as linear, polynomial or deep neural network function class that is parametrized by $\boldsymbol{\theta}$. In this case, we make the following assumption often called the \textit{realizability} assumption.

\begin{asmp}[Realizability]
\label{ass:realizability} 
    $\mathcal{Q}$ contains an optimal function $Q^*$, meaning there exists $\boldsymbol{\theta}^* \in \Theta$ such that $Q_{\boldsymbol{\theta}^*}=Q^*$.
\end{asmp}

\noindent Under this parametrization, the ERM-IRL problem (the equation \eqref{eq:mainopt}) becomes
\begin{align}
  \!\!\underset{\boldsymbol{\theta}\in \Theta \subseteq \mathbb{R}^d}{\min }  \;\mathbb{E}_{(s, a)\sim \pi^*, \nu_0}  \left[\mathcal{L}_{NLL}(Q_{\boldsymbol{\theta}})(s,a) + \lambda \mathbbm{1}_{a = a_s} \mathcal{L}_{BE}(Q_{\boldsymbol{\theta}})(s,a)\right] \! \notag
\end{align}

\noindent Our next question is whether our previous result -- showing that the equation \eqref{eq:mainopt} being PL in terms of $Q$ -- can ensure that this new equation is also PL in terms of $\boldsymbol{\theta}$, which is defined as follows.

\begin{defn} [Polyak-Łojasiewicz (PL) condition with respect to $\ell_2$ norm]
     Given $\Theta\in\mathbb{R}^d$, a function $h:\Theta\mapsto \mathbb{R}$ is is said to satisfy the Polyak-Łojasiewicz (PL) condition with respect to $\ell_2$ norm if $h$ has a nonempty solution set and a finite minimal value $h(\boldsymbol{\theta}^\ast)$ for $\boldsymbol{\theta}^\ast\in\Theta\subseteq \mathbb{R}^d$, and there exists some $c>0$ such that $\frac{1}{2}\|\nabla h(\boldsymbol{\theta})\|^2_2 \geq  c(h(\boldsymbol{\theta})-$ $h(\boldsymbol{\theta}^\ast)), \forall \boldsymbol{\theta}\in\Theta$.
\end{defn}

\noindent In this paper, we restrict our attention to the function class $\mathcal{Q}$ which satisfies the Assumption \ref{ass:nonSingularJac}.

\begin{asmp}\label{ass:nonSingularJac} 
\; For $Q_{\boldsymbol{\theta}}\in\mathcal{Q}$,
\\
1) $Q_\theta(s, a)$ is continuously differentiable with respect to $\boldsymbol{\theta}$, meaning its Jacobian
$$
D Q_{\boldsymbol{\theta}}:=\frac{\partial Q_{\boldsymbol{\theta}}(s, a)}{\partial \boldsymbol{\theta}} \in \mathbb{R}^{(\operatorname{dim}(\mathcal{S})+ \operatorname{dim}(\mathcal{A})) \times d}
$$
exists and is well-defined.
\\
2) There exists a constant $m>0$ such that for all $\boldsymbol{\theta}\in\Theta$,
$$
\sigma_{\min }\left(D Q_{\boldsymbol{\theta}}\right) \geq m
$$
where $\sigma_{\min }\left(D Q_{\boldsymbol{\theta}}\right)$ is the smallest singular value of $D Q_{\boldsymbol{\theta}}$.
\end{asmp}
\noindent The two lemmas show that Assumption \ref{ass:nonSingularJac} is easy to satisfy by popular function classes such as linear and the neural network function class.
\begin{lem}\label{lem:linPolyNonsingular}
    Let $Q_{\boldsymbol{\theta}}(s, a)=\boldsymbol{\theta}^{\top} \phi(s, a)$, where the known feature mapping $\phi:\mathcal{S}\times \mathcal{A} \mapsto \mathbb{R}^d$, satisfies $\|\phi(s, a)\| \leq B$ almost surely with respect to $(s,a)\sim (\pi^\ast, \nu_0)$ for some $B>0$.  Then dataset size $|\mathcal{D}|\ge Cd$ implies that Assumption \ref{ass:nonSingularJac} holds with probability at least $1-e^{-C |\mathcal{D}|}$.
\end{lem}
\begin{lem}[\cite{pennington2017resurrecting}]
\label{lem:NNenjoysPL}
    Let $Q_{\boldsymbol{\theta}}$ be a deep nonlinear neural network composed of smooth activation functions (e.g., sigmoid, Exponential Linear Unit (ELU) \citep{clevert2015fast}) and linear layers parameterized by $\boldsymbol{\theta}$. When initialized using orthogonal weight initialization, $Q_{\boldsymbol{\theta}}$ satisfies Assumption \ref{ass:nonSingularJac}.
\end{lem}



\noindent The following Theorem \ref{thm:thetaEnjoysPL} shows that satisfying Assumption \ref{ass:realizability} and \ref{ass:nonSingularJac} is enough to achieve $PL$ condition in terms of $\boldsymbol{\theta}$. That is, linear, polynomial, neural network-parametrization satisfies PL.
This also subsumes the previous result in the tabular case with $d=S\times A$, the states encoded as standard basis vectors $\theta = \{Q(s,a)\}_{s,a\in S\times A}$. 

\begin{thm}\label{thm:thetaEnjoysPL} Suppose that Assumption \ref{ass:realizability} and \ref{ass:nonSingularJac} are satisfied for $\Theta$.
 Then $f(Q)$ being PL in terms of $L^2(\pi^*,\nu_0)$ norm implies that $f(Q_{\boldsymbol{\theta}})$ being PL in terms of $\boldsymbol{\theta}$.
\end{thm} 
\begin{cor}
    $\mathbb{E}_{(s, a)\sim \pi^*, \nu_0}  \left[\mathcal{L}_{\mathrm{NLL}}(Q_{\boldsymbol{\theta}})(s, a)\right]+\lambda \mathbb{E}_{(s, a)\sim \pi^*, \nu_0}  \left[\mathbbm{1}_{a=a_s}\mathcal{L}_{\mathrm{BE}}(Q_{\boldsymbol{\theta}})(s, a)\right]$ satisfies the Polyak–Łojasiewicz (PL) condition in terms of $\boldsymbol{\theta}$.
\end{cor}

%We now present the main proposition on the global convergence property of Algorithm \ref{alg:estimation1}.


\subsection{Global convergence of GLADIUS}

Denote $\mathcal{D}_N$ be a finite-size dataset with $N$ number of transition pairs $((s,a,s^\prime))$. We define $Q^\ast_N$ as the solution to the empirical ERM-IRL objective (equation \eqref{eq:EmpiricalERMIRL}), i.e., 
\begin{align}
    Q^{\ast}_{\mathcal{D}_N} \in \arg\min _{Q \in \mathcal{Q}}&\max_{\zeta\in \mathbb{R}^{S\times A}}\frac{1}{N} \sum_{(s,a,s^\prime)\in \mathcal{D}}\notag
    \\
    &\bigl[{-\log \left(\hat{p}_Q(a \mid s)\right)} + 
   \lambda\mathbbm{1}_{a=a_s}\bigl\{\bigl(\hat{\mathcal{T}} Q\left(s, a, s^{\prime}\right)-Q(s, a)\bigr)^2 -\beta^2  \bigl({\left(V_{Q}\left(s^{\prime}\right)-\zeta(s,a)\right)^2}\bigr) \bigr\} \bigr]\notag
\end{align}
Note that $Q^{\ast}_{\mathcal{D}_N}$ approaches to $Q^\ast$ as $N\rightarrow \infty$, the solution to the ERM-IRL objective (equation \eqref{eq:AlgoOpt}), by the law of large numbers. Formally, we define the \textit{statistical error}, $\epsilon_{\text{stat}}(\mathcal{D}_N)$, as
$$
\epsilon_{\text{stat}}(\mathcal{D}_N):=\mathbb{E}_{(s, a) \sim \pi^*, \nu_0}\left[\left(Q^*(s, a)-Q^\ast_{\mathcal{D}_N}(s, a)\right)^2\right]
$$

\noindent Also, define $\hat{Q}_T$ be the $T$ iteration outcome of Algorithm \ref{alg:estimation1}. Then we can define the \textit{optimization error} of GLADIUS (Algorithm \ref{alg:estimation1}) at its $T$th iteration given data $\mathcal{D}$, $\epsilon_{\text{opt}}(T)$, as 
$$
\epsilon_{\text{opt}}(T):=\mathbb{E}_{(s, a) \sim \pi^*, \nu_0}\left[\left(Q^\ast_{\mathcal{D}_N}(s, a)-\hat{Q}_T(s, a)\right)^2\right] 
$$

%\noindent Note that $\mathbb{E}_{(s, a) \sim \pi^*, \nu_0}\left[\left(Q^*(s, a)-\hat{Q}_T(s, a)\right)^2\right] \le \epsilon_{\text{stat}}(N)+ \epsilon_{\text{opt}}(T)$.

\begin{prop}[Global convergence]
\label{prop:linConvergence}
Define $\hat{Q}_T$ be the estimator of $Q^\ast$ after $T$ iterations of GLADIUS algorithm on $\mathcal{D}_N$. Then under Assumption \ref{ass:realizability} and Assumption \ref{ass:nonSingularJac}, $\epsilon_{\text{opt}}(T)\le\mathcal{O}(1/T)$ and $\mathbb{E}_{\mathcal{D}_N}\bigl[\epsilon_{\text{stat}}(\mathcal{D}_N)\bigr]\le \mathcal{O}(1/N)$. This implies that
\begin{align}
&\mathbb{E}_{\mathcal{D}_N}\bigl[\mathbb{E}_{(s, a) \sim \pi^*, \nu_0}\bigl[\bigl(Q^*(s, a)-\hat{Q}_T(s, a)\bigr)^2\bigr]\bigr]\le \mathbb{E}_{\mathcal{D}_N}\bigl[\epsilon_{\text{opt}}(T)+ \epsilon_{\text{stat}}(\mathcal{D}_N)\bigr]\le \mathcal{O}(1/T) + \mathcal{O}(1/N)\notag
\end{align}
\end{prop}
%See Appendix \ref{sec:ProofLinConv} for the proof. 

\noindent\textbf{Remark. }
To the best of our knowledge, no prior work has proposed an algorithm that guarantees \textit{global} optimum convergence of the minimization problem that involves $\mathcal{L}_{BE}(Q)(s,a)$ term.\footnote{Some studies, such as \cite{dai2018sbeed}, have demonstrated convergence to a stationary point of this mini-max problem.}. With this regard, Theorem \ref{thm:BEenjoyPL} and \ref{thm:thetaEnjoysPL} have an important implication for Offline reinforcement learning (Offline RL) \cite{jiangoffline}. Gradient-based Offline reinforcement learning \citep{antos2008learning, dai2018sbeed}, which minimizes $\overline{\mathcal{L}_{\mathrm{BE}}}(Q_{\boldsymbol{\theta}})$ in the same way as GLADIUS does, has been been proven to be convergent. However, its global convergence guarantee has not yet been established. Theorem \ref{thm:BEenjoyPL} and \ref{thm:thetaEnjoysPL} establishes that gradient-based Offline RL is indeed globally convergent for important function classes such as tabular, linear, polynomial and neural network function classes.
