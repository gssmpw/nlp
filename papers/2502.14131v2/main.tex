\documentclass[11pt,letterpaper,english]{article}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% Packages from both documents
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% Essential packages
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{amssymb}
\usepackage{graphicx}
\usepackage{hyperref}
\usepackage{natbib}
\usepackage{times}
\usepackage{setspace}
\usepackage{booktabs}
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{enumitem}
\usepackage{microtype}
\usepackage{tikz}
\usepackage{pgfplots}
\pgfplotsset{compat=1.17}
\usepackage{multirow}
\usepackage{soul}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{float}
\usepackage{rotating}
\usepackage{array}
\usepackage{bm}
\usepackage{bbm}
\PassOptionsToPackage{table}{xcolor}
\usepackage{xcolor}
\usepackage{colortbl} 

\usepackage{pifont}
\usepackage[bottom]{footmisc}
\usepackage{wrapfig}
\usepackage[makeroom]{cancel}
\usepackage[edges]{forest}
\usepackage{makecell}
\usepackage{longtable}
\usepackage{dsfont}
\usepackage{comment}
\usepackage{blkarray}
\usepackage{stackengine}
\usepackage{accents}
\usepackage[utf8]{inputenc}
\usepackage{epstopdf}
\usepackage{xfrac}
\usepackage{fullpage}

% TikZ libraries
\usetikzlibrary{arrows}
\usetikzlibrary{positioning}

% Configure bibliography
\bibpunct[, ]{(}{)}{,}{a}{}{,}
\def\bibfont{\small}
\def\bibsep{\smallskipamount}
\def\bibhang{24pt}
\def\newblock{\ }
\def\BIBand{and}

% Line spacing
\onehalfspacing

% Math commands from first document
\newcommand{\cmark}{\ding{51}} % Checkmark
\newcommand{\xmark}{\ding{55}} % Crossmark
\newcommand{\lmark}{$\bigtriangleup$} 
\newcommand{\lbe}{\mathcal{L}_{BE}}

\newcommand*{\QED}[1][$\square$]{%
\leavevmode\unskip\penalty9999 \hbox{}\nobreak\hfill
    \quad\hbox{#1}%
}

% Math commands from second document
\newcommand{\todo}[1]{\noindent{\textcolor{red}{\{TODO:  #1\}}}}
\newcommand{\norm}[1]{\left\lVert#1\right\rVert}
\newcommand{\indep}{\perp \!\!\! \perp}
\newcommand{\argmax}{\arg\!\max}
\DeclareMathOperator*{\argmin}{arg\,min}
\def\ss#1{{\bf \color{red}#1}}
\def\thetaa{\theta_\alpha}
\def\bP{\mathbb{P}}
\def\bE{\mathbb{E}}
\def\bW{\mathbf{W}}
\def\bC{\mathbf{C}}
\def\bb{\mathbf{b}}
\def\bm{\textbf{m}}
\def\bx{\textbf{x}}
\def\bg{\textbf{g}}
\def\bd{\textbf{d}}
\def\bY{\textbf{Y}}
\def\cA{\mathcal{A}}
\def\bZ{\mathbf{Z}}
\def\bzero{\textbf{0}}
\def\tm{\text{-}}
\def\bTheta{\boldsymbol{\Theta}}
\def\thetab{\theta_\beta}
\def\btau{\boldsymbol{\tau}}
\def\sm#1{{\bf \color{red}#1}}
\def\hy#1{{\bf \color{blue}#1}}
\def\cents{\hbox{\rm\rlap/c}}
\def\aps{\textsc{\char13}}

% Custom list environments
\newcommand{\squishlist}{
   \begin{list}{$\bullet$}
    { \setlength{\itemsep}{0pt} \setlength{\parsep}{1pt}
      \setlength{\topsep}{1pt} \setlength{\partopsep}{1pt}
      \setlength{\leftmargin}{1.5em} \setlength{\labelwidth}{1em}
      \setlength{\labelsep}{0.5em} } }

\newcommand{\squishlisttwo}{
   \begin{list}{$\bullet$}
    { \setlength{\itemsep}{0pt} \setlength{\parsep}{0pt}
      \setlength{\topsep}{0pt} \setlength{\partopsep}{0pt}
      \setlength{\leftmargin}{1em} \setlength{\labelwidth}{1.5em}
      \setlength{\labelsep}{0.5em} } }

\newcommand{\squishend}{
    \end{list}  }

% Theorem environments from both documents
\newtheorem{thm}{Theorem}[section]
\newtheorem{lem}[thm]{Lemma}
\newtheorem{cor}[thm]{Corollary}
\newtheorem{prop}{Proposition}[section]
\newtheorem{asmp}{Assumption}[section]
\newtheorem{defn}{Definition}[section]
\newtheorem{fact}{Fact}[section]
\newtheorem{conj}{Conjecture}[section]
\newtheorem{rem}{Remark}[section]
\newtheorem{challenge}{Challenge}
\newtheorem{remark}{Remark}
\newtheorem{assumption}{Assumption}
\newtheorem{claim}{Claim}

% For DVI -> PNG conversion (from second document)
\DeclareGraphicsRule{.tif}{png}{.png}{`convert #1 `dirname #1`/`basename #1 .tif`.png}

% Math alphabet declaration from first document
\DeclareMathAlphabet{\mathcalligra}{T1}{calligra}{m}{n}

% Appendix setup
\usepackage{appendix}

% Title and author information
\title{Gradients can train reward models:
\\
An Empirical Risk Minimization Approach for \\Offline Inverse RL and Dynamic Discrete Choice Model}

\author{
Enoch H. Kang\\
Foster School of Business, University of Washington\\
\texttt{ehwkang@uw.edu}
\and
Hema Yoganarasimhan\\
Foster School of Business, University of Washington\\
\texttt{hemay@uw.edu}
\and
Lalit Jain\\
Foster School of Business, University of Washington\\
\texttt{lalitj@uw.edu}
}

\date{\today}

\begin{document}

\maketitle

\begin{abstract}
We study the problem of estimating Dynamic Discrete Choice (DDC) models, also known as offline Maximum Entropy-Regularized Inverse Reinforcement Learning (offline MaxEnt-IRL) in machine learning. The objective is to recover reward or $Q$ functions that govern agent behavior from offline behavior data. In this paper, we propose a globally convergent gradient-based method for solving these problems without the restrictive assumption of linearly parameterized rewards. The novelty of our approach lies in introducing the Empirical Risk Minimization (ERM) based IRL/DDC framework, which circumvents the need for explicit state transition probability estimation in the Bellman equation. Furthermore, our method is compatible with non-parametric estimation techniques such as neural networks. Therefore, the proposed method has the potential to be scaled to high-dimensional, infinite state spaces. A key theoretical insight underlying our approach is that the Bellman residual satisfies the Polyak-≈Åojasiewicz (PL) condition--a property that, while weaker than strong convexity, is sufficient to ensure fast global convergence guarantees. Through a series of synthetic experiments, we demonstrate that our approach consistently outperforms benchmark methods and state-of-the-art alternatives.
\end{abstract}

\textbf{Keywords:} Dynamic Discrete Choice, Offline Inverse Reinforcement Learning, Gradient-based methods, Empirical Risk Minimization, Machine learning


% Main content sections
%\section{Introduction}
\input{intro.tex}


\input{related.tex}
\input{setup.tex}
\input{ERM.tex}
\input{algorithm.tex}
\input{theory.tex}
\input{experiments.tex}
\input{ILexperiments}
\input{conclusion}
\input

\bibliographystyle{plainnat}
\bibliography{bib}

\appendix

\input{app_experiments}
\newpage
\input{app_proofs}


%\section*{Acknowledgments}


\end{document}