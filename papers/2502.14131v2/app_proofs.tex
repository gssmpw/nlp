\section{Technical Proofs}

\subsection{Theory of TD correction using biconjugate trick}\label{sec:BiconjProofs}


\begin{proof}[Proof of Lemma \ref{lem:OurBiconj}]
      \begin{align}
     &\mathcal{L}_{BE}(s,a)(Q):=\mathbb{E}_{ s^\prime \sim P(s, a)}\left[\delta_{Q}\left(s,a, s^\prime\right)\mid s, a\right]^2\notag
     \\
     &=\max_{h\in \mathbb{R}}2\cdot\mathbb{E}_{ s^\prime \sim P(s, a)}\left[\delta_{Q}\left(s,a, s^\prime\right)\mid s, a\right]\cdot h-h^2\tag{Biconjugate}
     \\
     &=\max_{h\in \mathbb{R}}2\cdot\mathbb{E}_{ s^\prime \sim P(s, a)}\left[\hat{\mathcal{T}}Q - Q\mid s, a\right]\cdot \underbrace{h}_{=\rho-Q(s,a)}-h^2\notag
    \\
     &=\max_{\rho(s,a)\in \mathbb{R}}\mathbb{E}_{ s^\prime \sim P(s, a)}\left[2\left(\hat{\mathcal{T}}Q - Q\right)\left(\rho-Q\right)
    -\left(\rho-Q\right)^2\mid s, a\right]\notag
    \\
    &= \max_{\rho(s,a)\in \mathbb{R}}\mathbb{E}_{ s^\prime \sim P(s, a)}\left[   \left(\hat{\mathcal{T}}{Q}-Q\right)^2-\left(\hat{\mathcal{T}}{Q}- \rho\right)^2\mid s, a\right]\label{eq:SBEED} 
    \end{align}
where the unique maximum is with 
\begin{align}
    \rho^{*}(s,a) &= h^{*}(s,a)+Q(s,a) = \mathcal{T}Q(s,a)- Q(s,a)+Q(s,a)\notag
    \\    &=\mathcal{T}Q(s,a)\notag
\end{align}
and where the equality of \ref{eq:SBEED} is from
\begin{align}
&\!\!\!\!\!\!\!\!\!2\left(\hat{\mathcal{T}}Q - Q\right)\left(\rho-Q\right)
    -\left(\rho-Q\right)^2\notag
    \\
    &=2(\hat{\mathcal{T}}{Q}\rho - \hat{\mathcal{T}}{Q}Q - \cancel{Q\rho} + Q^2)  - (\rho^2 - \cancel{2Q\rho} + Q^2)\notag
    \\
    &=2\hat{\mathcal{T}}{Q}\rho - 2\hat{\mathcal{T}}{Q}Q +\cancel{2} Q^2- \rho^2 - \cancel{Q^2}\notag
    \\
    &=\hat{\mathcal{T}}{Q}^2- 2\hat{\mathcal{T}}{Q}Q+Q^2-\hat{\mathcal{T}}{Q}^2+2\hat{\mathcal{T}}{Q}\rho- \rho^2\notag
    \\
    &=\left(\hat{\mathcal{T}}{Q}-Q\right)^2-\left(\hat{\mathcal{T}}{Q}- \rho\right)^2\notag
\end{align}

Now note that
    
     \begin{align}
     &\mathcal{L}_{BE}(s,a)(Q)= \max_{\rho(s,a)\in \mathbb{R}}\mathbb{E}_{ s^\prime \sim P(s, a)}\left[   \left(\hat{\mathcal{T}}{Q}-Q\right)^2-\left(\hat{\mathcal{T}}{Q}- \rho\right)^2\mid s, a\right]] \tag{equation \ref{eq:SBEED}}
    \\
    &= \mathbb{E}_{ s^\prime \sim P(s, a)}\left[ \left(\hat{\mathcal{T}}{Q}-Q\right)^2\mid s,a \right]-\min_{\rho(s,a)\in \mathbb{R}}\mathbb{E}_{ s^\prime \sim P(s, a)}\left[\left(\hat{\mathcal{T}}{Q}- \underbrace{\rho}_{= r+\beta \zeta}\right)^2\mid s, a\right]  \notag
    \\
    &= \mathbb{E}_{ s^\prime \sim P(s, a)}\left[\mathcal{L}_{TD}(Q)(s,a,s^\prime)\right]-\beta^2 \min_{\zeta\in \mathbb{R}}\mathbb{E}_{ s^\prime \sim P(s, a)}\left[\left(\hat{V}(s^\prime)- \zeta\right)^2\mid s, a\right]\label{eq:yesmin}
    \\
    &= \mathbb{E}_{ s^\prime \sim P(s, a)}\left[\mathcal{L}_{TD}(Q)(s,a, s^\prime)\right]-\beta^2 \mathbb{E}_{ s^\prime \sim P(s, a)}\left[\left(\hat{V}(s^\prime)- \mathbb{E}_{ s^\prime \sim P(s, a)}[\hat{V}(s^\prime)\mid s,a]\right)^2\mid s, a\right]\label{eq:nomin}
    \end{align}
where the equality of equation \ref{eq:nomin} comes from the fact that the $\zeta$ that maximize equation \ref{eq:yesmin} is $\zeta^* := \mathbb{E}_{ s^\prime \sim P(s, a)}[\hat{V}(s')\mid s,a]$, because
\begin{align}
r(s,a)+\beta \cdot \zeta^{*} (s,a) &:=  \rho^{*}(s,a)\notag
     \\
     &= \mathcal{T}Q(s,a) \notag
     \\
     &:=r(s,a)+\beta \cdot \mathbb{E}_{ s^\prime \sim P(s, a)}\left[\hat{V}(s^\prime)\mid s,a\right]\notag
\end{align}
For $Q^\ast$, $\mathcal{T}Q^\ast=Q^\ast$ holds. Therefore, we get
\begin{align}
r(s,a)+\beta \cdot \zeta^{*} (s,a) &:=  \rho^{*}(s,a)\notag
     \\
     &= \mathcal{T}Q^\ast(s,a) = Q^\ast(s,a) \notag
\end{align}
\end{proof}


\subsection{Proof of Theorem \ref{thm:MagnacThesmar}}\label{sec:PfMagnac}
\begin{proof}
    Suppose that the system of equations (Equation \ref{eq:HotzMillereqs})
\begin{equation}
\left\{
\begin{array}{l}
    \dfrac{\exp({Q}\left(s,a\right))}{\sum_{a^\prime\in \mathcal{A}} \exp({Q}\left(s,a^\prime\right))} = \pi^*(\;a
    \mid s) \; \; \; \forall s\in \mathcal{S}, a\in\mathcal{A}
    \\[1em]
    r(s, a_s)+\beta \cdot \mathbb{E}_{s^{\prime} \sim P(s, a_s)}\left[\log(\sum_{a^\prime\in\mathcal{A}}\exp Q(s^\prime, a^\prime)) \mid s, a_s\right]-Q(s, a_s)=0 \;\; \; \forall s\in \mathcal{S} 
\end{array}
\right.
\notag
\end{equation} 
is satisfied for $Q\in \mathcal{Q}$, where $\mathcal{Q}$ denote the space of all $Q$ functions. Then we have the following equivalent recharacterization of the second condition $\forall s\in \mathcal{S}$,
\begin{align}
     Q(s, a_s)&=r(s, a_s)+\beta \cdot \mathbb{E}_{s^{\prime} \sim P(s, a_s)}\left[\log(\sum_{a^\prime\in\mathcal{A}}\exp Q(s^\prime, a^\prime)) \mid s, a_s\right]\;\; \; \notag
     \\
     &=  r(s, a_s)+\beta \cdot \mathbb{E}_{s^\prime \sim P(s, a_s)}\left[Q(s^\prime, a^\prime) - \log \pi^*(a^\prime \mid s^\prime) \mid s, a_s\right] \;\; \forall a^\prime\in\mathcal{A} \notag
     \\
     &=  r(s, a_s)+\beta \cdot \mathbb{E}_{s^\prime \sim P(s, a_s)}\left[Q(s^\prime, a_{s^\prime}) - \log \pi^*(a_{s^\prime} \mid s^\prime) \mid s, a_s\right]
\end{align}

We will now show the existence and uniqueness of a solution using a standard fixed point argument on a Bellman operator. Let $\mathcal{F}$ be the space of functions $f: \mathcal{S} \rightarrow \mathbb{R}$ induced by elements of $\mathcal{Q}$, where each $Q \in \mathcal{Q}$ defines an element of $\mathcal{F}$ via

$$
f_Q(s):=Q\left(s, a_s\right)
$$

and define an operator $\mathcal{T}_f: \mathcal{F} \rightarrow$ $\mathcal{F}$ that acts on functions $f_Q$ :

$$
\left(\mathcal{T}_f f_Q\right)(s):=r\left(s, a_s\right)+\beta \sum_{s^{\prime}} P\left(s^{\prime} \mid s, a_s\right)\left[f_Q\left(s^{\prime}\right)-\log \pi^*\left(a_{s^{\prime}} \mid s^{\prime}\right)\right]
$$

Then for $Q_1, Q_2 \in\mathcal{Q}$, We have
\begin{align} & \left(\mathcal{T}_f f_{Q_1}\right)(s):=r\left(s, a_s\right)+\beta \sum_{s^{\prime}} P\left(s^{\prime} \mid s, a_s\right)\left[f_{Q_1}\left(s^{\prime}\right)-\log \pi^*\left(a_{s^{\prime}} \mid s^{\prime}\right)\right] \notag
\\ & \left(\mathcal{T}_f f_{Q_2}\right)(s):=r\left(s, a_s\right)+\beta \sum_{s^{\prime}} P\left(s^{\prime} \mid s, a_s\right)\left[f_{Q_2}\left(s^{\prime}\right)-\log \pi^*\left(a_{s^{\prime}} \mid s^{\prime}\right)\right]\notag
\end{align}

Subtracting the two, we get
\begin{align}
\left|\left(\mathcal{T}_f f_{Q_1}\right)(s)-\left(\mathcal{T}_f f_{Q_2}\right)(s)\right| 
&\leq \beta \sum_{s^{\prime}} P\left(s^{\prime} \mid s, a_s\right)\left|f_{Q_1}\left(s^{\prime}\right)-f_{Q_2}\left(s^{\prime}\right)\right| \notag
\\
&\leq \beta\left\|f_{Q_1}-f_{Q_2}\right\|_{\infty} \notag
\end{align}

Taking supremum norm over $s\in\mathcal{S}$, we get
$$\left\|\mathcal{T}_f f_{Q_1}-\mathcal{T}_f f_{Q_2}\right\|_{\infty} \leq \beta\left\|f_{Q_1}-f_{Q_2}\right\|_\infty$$

This implies that $\mathcal{T}_f$ is a contraction mapping under supremum norm, with $\beta\in (0,1)$. Since $\mathcal{Q}$ is a Banach space under sup norm (Lemma \ref{lem:completeMetric}), we can apply Banach fixed point theorem to show that there exists a unique $f_Q$ that satisfies $\mathcal{T}_f(f_Q) = f_Q$, and by definition of $f_Q$ there exists a unique $Q$ that satisfies $\mathcal{T}_f(f_Q) = f_Q$, i.e., 
$$r\left(s, a_s\right)+\beta \cdot \mathbb{E}_{s^{\prime} \sim P\left(s, a_s\right)}\left[\log \left(\sum_{a^{\prime} \in \mathcal{A}} \exp Q\left(s^{\prime}, a^{\prime}\right)\right) \mid s, a_s\right]-Q\left(s, a_s\right)=0 \quad \forall s \in \mathcal{S}$$

Since $Q^\ast$ satisfies the system of equations \ref{eq:HotzMillereqs}, $Q^\ast$ is the only solution to the system of equations.

Also, since $Q^\ast = \mathcal{T}Q^\ast = r(s,a)+\beta \cdot \mathbb{E}_{s^{\prime} \sim P(s, a)}\bigl[\log(\sum_{a^\prime\in\mathcal{A}}\exp Q^\ast(s^\prime, a^\prime)) \mid s, a\bigr]$ holds, we can identify $r$ as
\begin{align}
    r(s,a) &= Q^\ast(s, a) - \beta \cdot \mathbb{E}_{s^{\prime} \sim P(s, a)}\bigl[\log(\sum_{a^\prime\in\mathcal{A}}\exp Q^\ast(s^\prime, a^\prime)) \mid s, a\bigr] \notag
\end{align}


\end{proof}
\begin{lem}\label{lem:completeMetric} Suppose that $\mathcal{Q}$ consists of bounded functions on $\mathcal{S} \times \mathcal{A}$. Then $\mathcal{Q}$ is a Banach space with the supremum norm as the induced norm.
\end{lem}
\begin{proof}
Suppose a sequence of functions $\left\{Q_n\right\}$ in $\mathcal{Q}$ is Cauchy in the supremum norm. We must show that $ Q_n\rightarrow Q^\ast$ as $n\rightarrow \infty$ for some $Q^\ast$ and $Q^\ast$ is also bounded. %because $\left|Q^*(s, a)\right|=\lim _{n \rightarrow \infty}\left|Q_n(s, a)\right| \leq M$. 
Note that $Q_n$ being Cauchy in sup norm implies that for every $(s, a)$, the sequence $\left\{Q_n(s, a)\right\}$ is Cauchy in $\mathbb{R}$. Since $\mathbb{R}$ is a complete space, every Cauchy sequence of real numbers has a limit; this allows us to define function $Q^\ast:\mathcal{S}\times \mathcal{A} \mapsto \mathbb{R}$ such that $Q^*(s, a)=\lim _{n \rightarrow \infty} Q_n(s, a)$. Then we can say that $Q_n(s, a) \rightarrow Q^*(s, a) $ for every $(s, a) \in \mathcal{S} \times \mathcal{A}$. Since each $Q_n$ is bounded, we take the limit and obtain:
$$
\sup _{s, a}\left|Q^*(s, a)\right|=\lim _{n \rightarrow \infty} \sup _{s, a}\left|Q_n(s, a)\right| \leq M
$$
which implies $Q^* \in \mathcal{Q}$.

\noindent Now what's left is to show that the supremum norm
$$
\|Q\|_{\infty}=\sup _{(s, a) \in \mathcal{S} \times \mathcal{A}}|Q(s, a)|
$$
induces the metric, i.e., 
$$
d\left(Q_1, Q_2\right):=\left\|Q_1-Q_2\right\|_{\infty}=\sup _{(s, a) \in \mathcal{S} \times \mathcal{A}}\left|Q_1(s, a)-Q_2(s, a)\right|
$$
The function $d$ satisfies the properties of a metric:

- Non-negativity: $d\left(Q_1, Q_2\right) \geq 0$ and $d\left(Q_1, Q_2\right)=0$ if and only if $Q_1=$ $Q_2$.

- Symmetry: $d\left(Q_1, Q_2\right)=d\left(Q_2, Q_1\right)$ by the absolute difference.

- Triangle inequality:
$$
d\left(Q_1, Q_3\right)=\sup _{s, a}\left|Q_1(s, a)-Q_3(s, a)\right| \leq \sup _{s, a}\left|Q_1(s, a)-Q_2(s, a)\right|+\sup _{s, a}\left|Q_2(s, a)-Q_3(s, a)\right|
$$

which shows $d\left(Q_1, Q_3\right) \leq d\left(Q_1, Q_2\right)+d\left(Q_2, Q_3\right)$.


\end{proof}

\subsection{Proof of Theorem 
\ref{thm:mainopt}}\label{sec:pfOfmainOpt}

Define $\hat{Q}$ as 
%
\begin{align}
    \hat{Q} &\in \underset{Q\in \mathcal{Q}}{\arg\min } \; \;\mathbb{E}_{(s, a)\sim \pi^*, \nu_0}  \left[-\log\left(\hat{p}_{Q}(a
\mid s)\right)\right] + \lambda\mathbb{E}_{(s, a)\sim \pi^*, \nu_0}\left[ \mathbbm{1}_{a = a_s} \mathcal{L}_{BE}(Q)(s,a)\right] \tag{Equation \ref{eq:mainopt}}
\end{align}
From Theorem \ref{thm:MagnacThesmar}, it is sufficient to show that $\hat{Q}$
satisfies the equations \ref{eq:HotzMillereqs} of Theorem \ref{thm:MagnacThesmar} for any $\lambda>0$, i.e., 
\begin{equation}
\left\{
\begin{array}{l}
    \dfrac{\exp({\hat{Q}}\left(s,a\right))}{\sum_{a^\prime\in \mathcal{A}} \exp({\hat{Q}}\left(s,a^\prime\right))} = \pi^*(a
    \mid s) \; \; \; \forall s\in \bar{\mathcal{S}}, a \in \mathcal{A}
    \\[1em]
    r(s, a_s)+\beta \cdot \mathbb{E}_{s^{\prime} \sim P(s, a_s)}\left[\log(\sum_{a^\prime\in\mathcal{A}}\exp \hat{Q}(s^\prime, a^\prime)) \mid s, a_s\right]-\hat{Q}(s, a_s)=0 \;\;\; \forall s\in \bar{\mathcal{S}}
    
\end{array}\tag{Equation \ref{eq:HotzMillereqs}}
\right. 
\end{equation}
where $\bar{\mathcal{S}}$ (the reachable states from $\nu_0$, $\pi^\ast$) was defined as:
$$
\bar{\mathcal{S}}=\left\{s \in \mathcal{S} \mid \operatorname{Pr}\left(s_t=s \mid s_0 \sim \nu_0, \pi^*\right)>0 \text { for some } t \geq 0\right\} 
$$
Now note that:
  \begin{align}
     &\left\{Q \in \mathcal{Q} \mid\hat{p}_{Q}(\;\cdot
    \mid s) = \pi^*(\;\cdot
    \mid s)\quad  \forall s\in\bar{\mathcal{S}}\quad\text{a.e.}\right\} \notag
    \\
    &=\underset{Q\in \mathcal{Q}}{\arg\max } \; \;\mathbb{E}_{(s, a)\sim \pi^*, \nu_0}  \left[\log\left(\hat{p}_{Q}(\;\cdot
    \mid s)\right)\right] \tag{$\because$ Lemma \ref{lem:minMLE}}
    \\
    &=\underset{Q\in \mathcal{Q}}{\arg\min } \; \;\mathbb{E}_{(s, a)\sim \pi^*, \nu_0}  \left[-\log\left(\hat{p}_{Q}(\;\cdot
    \mid s)\right)\right] \notag
    \end{align}
and 
    \begin{align}
     &\left\{Q \in \mathcal{Q} \mid\mathcal{L}_{BE}(Q)(s,a_s) = 0\quad  \forall s\in\bar{\mathcal{S}}\right\} \notag
    \\
    &=\underset{Q\in \mathcal{Q}}{\arg\min } \; \;\mathbb{E}_{(s, a)\sim \pi^*, \nu_0}  \left[\mathbbm{1}_{a = a_s} \mathcal{L}_{BE}(Q)(s,a)\right] \notag
    \end{align}
Therefore what we want to prove, equations \ref{eq:HotzMillereqs}, becomes the following equation \ref{eq:modifiedHotz}:

\begin{equation}
\left\{
\begin{array}{l}
    \hat{Q} \in \underset{Q\in \mathcal{Q}}{\arg\min } \; \;\mathbb{E}_{(s, a)\sim \pi^*, \nu_0}  \left[-\log\left(\hat{p}_{Q}(\;\cdot
    \mid s)\right)\right] 
    \\[1em]
     \hat{Q} \in \underset{Q\in \mathcal{Q}}{\arg\min } \; \;\mathbb{E}_{(s, a)\sim \pi^*, \nu_0}  \left[\mathbbm{1}_{a = a_s} \mathcal{L}_{BE}(Q)(s,a)\right]
    
\end{array}\label{eq:modifiedHotz}
\right. 
\end{equation}
where its solution set is nonempty by Theorem \ref{thm:MagnacThesmar}, i.e., 
$$ \underset{Q\in \mathcal{Q}}{\arg\min } \; \;\mathbb{E}_{(s, a)\sim \pi^*, \nu_0} \left[-\log\left(\hat{p}_{Q}(a
\mid s)\right)\right] \;\; \cap \;\; \underset{Q\in \mathcal{Q}}{\arg\min } \; \; \mathbb{E}_{(s, a)\sim \pi^*, \nu_0} \left[\mathbbm{1}_{a = a_s}\mathcal{L}_{BE}(\hat{Q})(s,a)\right] \;\; \neq \;\; \emptyset$$ 

Under this non-emptiness, according to Lemma \ref{lem:sharingsol}, $\hat{Q}$ satisfies equation \ref{eq:modifiedHotz}. This implies that $\hat{Q}(s,a) = Q^\ast(s,a)$ for $s\in\bar{\mathcal{S}}$ and $a\in\mathcal{A}$, as the solution to set of equations \ref{eq:HotzMillereqs} is $Q^*$. This implies that 
\begin{align}
    r(s, a)=\hat{Q}(s, a)-\beta \cdot \mathbb{E}_{s^{\prime} \sim P(s, a)}\left[\log \left(\sum_{a^{\prime} \in \mathcal{A}} \exp \hat{Q}\left(s^{\prime}, a^{\prime}\right)\right) \mid s, a\right] \notag
\end{align}
for $s\in\bar{\mathcal{S}}$ and $a\in\mathcal{A}$.
\QED


\begin{lem}\label{lem:minMLE}
    \begin{align}
           \underset{Q\in \mathcal{Q}}{\arg\max } &\; \;\mathbb{E}_{(s, a)\sim \pi^*, \nu_0}  \left[\log\left(\hat{p}_{Q}(\;\cdot
    \mid s)\right)\right] \notag
    \\
     &=\left\{Q \in \mathcal{Q} \mid\hat{p}_{Q}(\;\cdot
    \mid s) = \pi^*(\;\cdot
    \mid s)\quad  \forall s\in\bar{\mathcal{S}}\quad\text{a.e.}\right\}\notag
    \\
     &=\left\{Q \in \mathcal{Q} \mid Q(s,a_1)-Q(s,a_2)= Q^*(s,a_1)-Q^*(s,a_2) \quad \forall a_1, a_2\in\mathcal{A}, s\in\bar{\mathcal{S}}\right\} \notag
    \end{align}
\end{lem}

\begin{proof}[Proof of Lemma \ref{lem:minMLE}]
    \begin{align}
    \mathbb{E}_{(s, a)\sim \pi^*, \nu_0}  \left[\log\left(\hat{p}_{Q}(\;\cdot
    \mid s)\right)\right] & = 
\mathbb{E}_{(s,a)\sim\pi^*, \nu_0} [\log \hat{p}_{Q}(a|s) - \ln \pi^*(a|s) + \ln \pi^*(a|s)]\notag \\
    &=-\mathbb{E}_{(s,a)\sim\pi^*, \nu_0} \left[\ln \frac{\pi^*(a|s)}{\hat{p}_{Q}(a|s)} \right] + \mathbb{E}_{(s,a)\sim\pi^*, \nu_0} [\ln \pi^*(a|s)]\notag \\
    &= -\mathbb{E}_{s\sim\pi^*, \nu_0} \left[D_{KL}(\pi^*(\cdot\mid s) \| \hat{p}_{Q}(\cdot\mid s))\right] + \mathbb{E}_{(s,a)\sim\pi^*, \nu_0} [\ln \pi^*(a|s)]\notag \notag
\end{align}
Therefore,
\begin{align}
    \underset{Q\in\mathcal{Q}}{\arg\max}&\; \mathbb{E}_{(s, a)\sim \pi^*, \nu_0}  \left[\log\left(\hat{p}_{Q}(\;\cdot
    \mid s)\right)\right] =\underset{Q\in\mathcal{Q}}{\arg\min}\;\mathbb{E}_{s\sim\pi^*, \nu_0} \left[D_{KL}(\pi^*(\cdot\mid s) \| \hat{p}_{Q}(\cdot\mid s))\right]\notag
    \\
    &=\{Q\in\mathcal{Q}\mid D_{KL}(\pi^*(\cdot\mid s) \| \hat{p}_{Q}(\cdot\mid s))=0 \text{ for all }s\in\bar{\mathcal{S}}\} \tag{$\because \; Q^*\in \mathcal{Q}$ and $D_{KL}(\pi^* \| \pi^*)=0$}\notag
    \\
    &=\{Q\in\mathcal{Q}\mid \hat{p}_Q(\cdot \mid s)=\pi^*(\cdot \mid s)\; \; \text{a.e.} \text{ for all }s\in\bar{\mathcal{S}}\}\notag
    \\
    &= \{Q\in\mathcal{Q}\mid \frac{\hat{p}_Q\left(a_1 \mid s\right)}{\hat{p}_Q\left(a_2 \mid s\right)}=\frac{\pi^*\left(a_1 \mid s\right)}{\pi^*\left(a_2 \mid s\right)} \quad \forall a_1, a_2 \in \mathcal{A}, s\in\bar{\mathcal{S}}\}\notag   
    \\
    &=\left\{Q \in \mathcal{Q} \mid \exp (Q(s,a_1)-Q(s,a_2))=\exp \left(Q^*(s,a_1)-Q^*(s,a_2)\right) \quad \forall a_1, a_2\in\mathcal{A}, s\in\bar{\mathcal{S}} \right\}\notag
    \\
    &=\left\{Q \in \mathcal{Q} \mid Q(s,a_1)-Q(s,a_2)= Q^*(s,a_1)-Q^*(s,a_2) \quad \forall a_1, a_2\in\mathcal{A}, s\in\bar{\mathcal{S}}\right\} \notag
\end{align}
\end{proof}

\begin{lem}\label{lem:sharingsol}
    Let $f_1: \mathcal{X} \rightarrow \mathbb{R}$ and $f_2: \mathcal{X} \rightarrow \mathbb{R}$ be two functions defined on a common domain $\mathcal{X}$. Suppose the sets of minimizers of $f_1$ and $f_2$ intersect, i.e.,
$$
\arg \min f_1 \cap \arg \min f_2 \neq \emptyset
$$
Then, any minimizer of the sum $f_1+f_2$ is also a minimizer of both $f_1$ and $f_2$ individually. That is, if
$$
x^* \in \arg \min \left(f_1+f_2\right)
$$
then
$$
x^* \in \arg \min f_1 \; \cap \; \arg \min f_2
$$
\end{lem}
\begin{proof}
    Since \( \arg\min f_1 \cap \arg\min f_2 \neq \emptyset \), let \( x^\dagger \) be a common minimizer such that $$
x^\dagger \in \arg\min f_1 \cap \arg\min f_2$$
This implies that  
\begin{align}
    f_1(x^\dagger) &= \min_{x \in \mathcal{X}} f_1(x) =: m_1, \notag
    \\
    f_2(x^\dagger) &= \min_{x \in \mathcal{X}} f_2(x) =: m_2. \notag
\end{align}

Now, let \( x^* \) be any minimizer of \( f_1 + f_2 \), so  
\begin{align}
    x^* \in \arg\min (f_1 + f_2) &\iff f_1(x^*) + f_2(x^*) \leq f_1(x) + f_2(x), \quad \forall x \in \mathcal{X}. \notag
\end{align}
Evaluating this at \( x^\dagger \), we obtain  
\begin{align}
    f_1(x^*) + f_2(x^*) &\leq f_1(x^\dagger) + f_2(x^\dagger) \notag
    \\
    &= m_1 + m_2. \notag
\end{align}

Now, suppose for contradiction that $x^* \notin \arg\min f_1$, 
meaning  
\begin{align}
    f_1(x^*) &> m_1 \notag
\end{align}
But then
\begin{align}
    f_2(x^*) & \le m_1 + m_2 - f_1(x^*) \notag
    \\
    &< m_1 + m_2 - m_1 = m_2 \notag
\end{align}

This contradicts the fact that \( m_2 = \min f_2 \), so \( x^* \) must satisfy  
\begin{align}
    f_1(x^*) &= m_1 \notag
\end{align}

By symmetry, assuming $x^* \notin \arg\min f_2$ leads to the same contradiction, forcing  
\begin{align}
    f_2(x^*) &= m_2 \notag
\end{align}

Thus, we conclude  
\begin{align}
    x^* \in \arg\min f_1 \cap \arg\min f_2 \notag
\end{align}
\end{proof}

%%%%%% Mirror descent %%%%%%%%%%%

\iffalse

\subsection{Proof of Theorem \ref{thm:mirror}}\label{sec:Proofofmirror}

\begin{thm}[Mirror descent equivalence]\label{thm:mirror}
    Equation \ref{eq:mainopt} is equivalent to the mirror descent algorithm for minimizing Bellman error only, i.e., 
    
    \begin{align}
       \underset{Q \in \mathcal{Q}}{\arg\min} &\; \mathbb{E}_{(s, a) \sim \pi^*, \nu_0, s' \sim P(s, a)} \left[\mathbbm{1}_{a = a_s} \left( \mathcal{L}_{TD}(Q)(s,a,s') - \beta^2 D(Q)(s,a)\right) \right]\notag
    \end{align}
   with Bregman divergence associated with $F$, where $F(p)=\sum_{a \in \mathcal{A}} p(a \mid s) \log(p(a \mid s))$, which defines the negative Shannon entropy of a distribution $p(a \mid s)$.
\end{thm}
See Appendix \ref{sec:Proofofmirror} for the proof. Note that $F$ makes the Bregman divergence be $D_{KL}\left(\pi^* \| \hat{p}_Q\right):=\sum_a \pi^*(a \mid s) \log \left(\frac{\pi^*(a \mid s)}{\hat{p}_Q(a \mid s)}\right)$, the mirror map be $\phi(p)=\log(p)$, and the inverse mirror map be the softmax transformation.

\begin{align}
    \hat{Q} &\in  \underset{Q\in \mathcal{Q}}{\arg\min } \; \;\mathbb{E}_{(s, a)\sim \pi^*, \nu_0,  s^\prime \sim P(s, a)}  \left[-\log\left(\hat{p}_{Q}(a
\mid s)\right) + \lambda \mathbbm{1}_{a = a_s}\left( \mathcal{L}_{TD}(Q)(s,a,s^\prime)-\beta^2D(Q)(s,a)\right)\right]\tag{Equation \ref{eq:mainopt}}
\\
&= \underset{Q\in \mathcal{Q}}{\arg\min } \; \;\mathbb{E}_{(s, a)\sim \pi^*, \nu_0}  \left[-\log\left(\hat{p}_{Q}(a
\mid s)\right)\right] + \lambda\mathbb{E}_{(s, a)\sim \pi^*, \nu_0}\left[ \mathbbm{1}_{a = a_s} \mathcal{L}_{BE}(Q)(s,a)\right] \tag{$\because$ Lemma \ref{thm:BEbiconjWrtV}}
\\
&= \underset{Q\in \mathcal{Q}}{\arg\min } \; \; \lambda\mathbb{E}_{(s, a)\sim \pi^*, \nu_0}\left[ \mathbbm{1}_{a = a_s} \mathcal{L}_{BE}(Q)(s,a)\right] + D_{KL}(\pi^*(\cdot\mid s) \| \pi_{Q}(\cdot\mid s)) \notag
\\
&\quad\;\; \quad \text{s.t.} \quad \;\{Q\in\mathcal{Q}\mid \pi_Q = \pi^* \; \; \text{a.e.} \text{ for all }s\in\mathcal{S}\}\tag{$\because$Proof of Lemma \ref{lem:minMLE}}
\end{align}
Define $f(Q) := \lambda\mathbb{E}_{(s, a)\sim \pi^*, \nu_0}\left[ \mathbbm{1}_{a = a_s} \mathcal{L}_{BE}(Q)(s,a)\right]$, $f^\prime(Q) := \lambda\mathbb{E}_{(s, a)\sim \pi^*, \nu_0}\left[ \mathbbm{1}_{a = a_s} \mathcal{L}_{BE}(Q)(s,a)\right] + D_{KL}(\pi^*(\cdot\mid s) \| \pi_{Q}(\cdot\mid s))$ and $K:=\{Q\in\mathcal{Q}\mid \pi_Q = \pi^* \; \; \text{a.e.} \text{ for all }s\in\mathcal{S}\}$. Then the mirror descent method of minimization of $f^\prime(Q)$ subject to constraint $K$ is defined as 
\begin{align}
    Q_{t+1} \leftarrow \operatorname{\argmin}_{Q\in K}\left\{\eta\left\langle\nabla f^\prime\left(x_t\right), x\right\rangle+D_h\left(x \| x_t\right)\right\}
\end{align}

\begin{align}
&\Phi_{t+1}-\Phi_t=  \frac{1}{\eta}\left(h\left(x_t\right)-h\left(x_{t+1}\right)-\frac{1}{2}\left\langle\nabla h\left(x_t\right), x^*-x_{t+1}\right\rangle-\frac{1}{2}\left\langle\nabla h\left(x^*\right), x^*-x_{t+1}\right\rangle\right. \\
& \left.+\frac{\eta}{2}\left\langle\nabla f_t\left(x_t\right), x^*-x_{t+1}\right\rangle+\left\langle\nabla h\left(x_t\right), x^*-x_t\right\rangle\right)
\\
&= \frac{1}{\eta}\left(h\left(x_t\right)-h\left(x_{t+1}\right)-\frac{1}{2}\left\langle\nabla h\left(x_t\right), x^*-x_{t+1}\right\rangle-\frac{1}{2}\left\langle\nabla h\left(x_{t}\right), x^*-x_{t+1}\right\rangle\right. \\
& \left.+\frac{\eta}{2}\left\langle\nabla f_t\left(x_t\right), x^*-x_{t+1}\right\rangle+\left\langle\nabla h\left(x_t\right), x^*-x_t\right\rangle+ \frac{1}{2  } \left\langle\nabla h\left(x^*\right)-\nabla h\left(x_t\right), x^*-x_{t+1}\right\rangle\right) 
\\
&= \frac{1}{\eta}\left(h\left(x_t\right)-h\left(x_{t+1}\right)-\left\langle\nabla h\left(x_t\right), x_t-x_{t+1}\right\rangle+\frac{\eta}{2}\left\langle\nabla f_t\left(x_t\right), x^*-x_{t+1}\right\rangle+ \frac{1}{2  } \left\langle\nabla h\left(x^*\right)-\nabla h\left(x_t\right), x^*-x_{t+1}\right\rangle\right) 
\\
&= \frac{1}{\eta}\left(h\left(x_t\right)-h\left(x_{t+1}\right)-\left\langle\nabla h\left(x_t\right), x_t-x_{t+1}\right\rangle + \frac{\eta}{2}\left\langle\nabla f_t\left(x_t\right), x^*-x_{t}\right\rangle\right. \\
& \left.+\frac{\eta}{2}\left\langle\nabla f_t\left(x_t\right), x_t-x_{t+1}\right\rangle+ \frac{1}{2  } \left\langle\nabla h\left(x^*\right)-\nabla h\left(x_t\right), x^*-x_{t}\right\rangle+\frac{1}{2  } \left\langle\nabla h\left(x^*\right)-\nabla h\left(x_t\right), x_t-x_{t+1}\right\rangle\right) \notag
\\
&= \frac{1}{\eta}\left(h\left(x_t\right)-h\left(x_{t+1}\right)-\left\langle\nabla h\left(x_t\right), x_t-x_{t+1}\right\rangle + \frac{\eta}{2}\left\langle\nabla f_t\left(x_t\right), x^*-x_{t}\right\rangle\right. \\
& \left.+ \frac{1}{2  } \left\langle\nabla h\left(x^*\right)-\nabla h\left(x_t\right), x^*-x_{t}\right\rangle+\frac{1}{2  } \left\langle \eta \nabla f_t + \nabla h\left(x^*\right)-\nabla h\left(x_t\right), x_t-x_{t+1}\right\rangle\right) 
\\
&= \frac{1}{\eta}\left(h\left(x_t\right)-h\left(x_{t+1}\right)-\left\langle\nabla h\left(x_t\right), x_t-x_{t+1}\right\rangle + \frac{\eta}{2}\left\langle\nabla f_t\left(x_t\right), x^*-x_{t}\right\rangle\right. \\
& \left.+\frac{1}{2  } \left\langle\nabla h\left(x^*\right)-\nabla h\left(x_t\right), x^*-x_{t}\right\rangle+\left\langle \nabla h(x^*)-\nabla h(x_{t+1}), x_t-x_{t+1}\right\rangle\right) \notag 
\end{align}

\fi
%%%%%% End of Mirror descent %%%%%%%%%%%


\subsection{Proof of Lemma \ref{lem:ConvexityMLE}}
\begin{proof}[Proof of Lemma \ref{lem:ConvexityMLE}] \label{sec:NLLproperties}
Denote $Q(s, \cdot)=\left[Q\left(s, a^{\prime}\right)\right]_{a^{\prime} \in \mathcal{A}}$. Then,
\begin{align}
    \text{Convexity}& \text{ of }  \mathbb{E}_{(s, a)\sim \pi^*, \nu_0}  \left[-\log\left(\hat{p}_{Q}(\;\cdot
    \mid s)\right)\right] \text{ w.r.t. }Q\in\mathcal{Q}\notag
    \\
    &\iff \text{Concavity of } \mathbb{E}_{(s,a)\sim\pi^*, \nu_0}\left[\ln \hat{p}_{Q}\left( \cdot \mid s\right)\right] \text{ w.r.t. } Q\in\mathcal{Q} \notag
    \\
    &\;\Longleftarrow \text{Concavity of } \ln \hat{p}_{Q}\left(\cdot \mid s\right)\text{ w.r.t. } Q\in\mathcal{Q} \text{ for all } s\in\mathcal{S} \tag{$\because$ linearity of expectation}
    \\
    &\iff \text{Concavity of }  Q(s, \cdot)-log \sum_{a^{\prime}\in\mathcal{A}} \exp \left(Q\left(s, a^{\prime}\right)\right) \text{ w.r.t. } Q(s, \cdot)  \text{ for all } s\in\mathcal{S} \notag
    \\
    &\iff \text{Convexity of } log \sum_{a^{\prime}\in\mathcal{A}} \exp \left(Q\left(s, a^{\prime}\right)\right) \text{ w.r.t. } Q(s, \cdot)  \text{ for all } s\in\mathcal{S}  \notag
\end{align}
Since the function logsumexp is a known convex function, we are done.

\begin{align}
    &\text{Lipschitz} \text{ smoothness } \text{of }  \mathbb{E}_{(s, a)\sim \pi^*, \nu_0}  \left[\log\left(\hat{p}_{Q}(\;\cdot
    \mid s)\right)\right] \text{ w.r.t. }Q\in\mathcal{Q}\notag
    \\
    &\iff \text{ Lipschitz continuity of } \nabla_Q \; \mathbb{E}_{(s,a)\sim\pi^*, \nu_0}\left[\ln \hat{p}_{Q}\left( \cdot \mid s\right)\right] \text{ w.r.t. } Q\in\mathcal{Q} \notag
    \\
    &\iff \text{ Lipschitz continuity of } \mathbb{E}_{(s, a) \sim \pi^*, \nu_0}\left[\delta_{a, a^{\prime}}-\hat{p}_Q\left(a^{\prime} \mid s\right)\right]_{a^{\prime}\in\mathcal{A}} \notag
    \\
    &\iff \text{ Lipschitz continuity of } \mathbb{E}_{s \sim \pi^*, \nu_0}\left[\pi^*\left(a^{\prime}\mid s\right) -\hat{p}_Q\left(a^{\prime} \mid s\right)\right]_{a^{\prime}\in\mathcal{A}} \notag
    \\
    &\iff \exists \; c>0 \; s.t. \;  \|\mathbb{E}_{s \sim \pi^*, \nu_0}\left[\hat{p}_{Q^\prime}\left(a^{\prime}\mid s\right) -\hat{p}_Q\left(a^{\prime} \mid s\right)\right]_{a^{\prime}\in\mathcal{A}}\| \le c\|Q-Q^\prime\|_{L_2(\pi^\ast, \nu_0)} \quad \forall Q, Q^\prime \in\mathcal{Q} \notag
\end{align}
Since softmax is 1-Lipschitz continuous for each $s\in\mathcal{S}$ with respect to $\ell_2$ norm \cite{gao2017properties}, for all $s\in\mathcal{S}$ we have
$$
\left\|\hat{p}_{Q^{\prime}}(\cdot \mid s)-\hat{p}_Q(\cdot \mid s)\right\|_2 \leq \left\|Q^{\prime}(s, \cdot)-Q(s, \cdot)\right\|_2
$$
Therefore
\begin{align}
  \|\mathbb{E}_{s \sim \pi^*, \nu_0}\left[\hat{p}_{Q^\prime}\left(\cdot\mid s\right) -\hat{p}_Q\left(\cdot\mid s\right)\right]\|_2 & \leq \mathbb{E}_{s \sim \pi^*, \nu_0}\bigl[\left\|\hat{p}_{Q^{\prime}}(\cdot \mid s)-\hat{p}_Q(\cdot \mid s)\right\|_2\bigr] \tag {Norm is convex}
  \\
  &\leq \mathbb{E}_{s \sim \pi^*, \nu_0}\left[\left\|Q^{\prime}(s, \cdot)-Q(s, \cdot)\right\|_2\right] \tag{Softmax is 1-Lipschitz}
  \\
  &\leq\left(\mathbb{E}_{s \sim \pi^*, \nu_0}\left\|Q^{\prime}(s, \cdot)-Q(s, \cdot)\right\|_2^2\right)^{1 / 2} \tag{$x^{1/2}$ is concave}
  \\
  &=\left\|Q-Q^{\prime}\right\|_{L_2(\pi^\ast, \nu_0)} \notag
\end{align}


\iffalse
Lastly for Lipschitz continuity, 
$$
\log \hat{p}_Q(a \mid s)=Q(s, a)-\log \sum_{a^{\prime} \in \mathcal{A}} \exp \left(Q\left(s, a^{\prime}\right)\right)
$$
This implies
\begin{align}
    \!\!\!\!\!\!\!&\left|\log \hat{p}_{Q_1}(a \mid s)-\log \hat{p}_{Q_2}(a \mid s)\right|
    \\
    &=\left|\left[Q_1(s, a)-Q_2(s, a)\right]-\left[\log \sum_{a^{\prime}} \exp \left(Q_1\left(s, a^{\prime}\right)\right)-\log \sum_{a^{\prime}} \exp \left(Q_2\left(s, a^{\prime}\right)\right)\right]\right|
    \\
    &\le \left|\left[Q_1(s, a)-Q_2(s, a)\right]\right|+\left|\left[\log \sum_{a^{\prime}} \exp \left(Q_1\left(s, a^{\prime}\right)\right)-\log \sum_{a^{\prime}} \exp \left(Q_2\left(s, a^{\prime}\right)\right)\right]\right|
    \\
    &\le \left\|Q_1-Q_2\right\|_{\infty} + \left\|Q_1-Q_2\right\|_{\infty} = 2 \left\|Q_1-Q_2\right\|_{\infty} 
\end{align}
Therefore
$$\left|\mathbb{E}_{(s, a) \sim \pi^*, \nu_0}\left[\log \hat{p}_{Q_1}(a \mid s)\right]-\mathbb{E}_{(s, a) \sim \pi^*, \nu_0}\left[\log \hat{p}_{Q_2}(a \mid s)\right]\right| \leq 2\left\|Q_1-Q_2\right\|_{\infty}$$

\fi
\end{proof}

\subsection{Proof of Lemma \ref{lem:BELipschitz} (Properties of Bellman error)}
For showing that $\overline{\mathcal{L}_{BE}}(Q)$ is of $\mathcal{C}^2 \text{ w.r.t. 
    } Q\in\mathcal{Q}$,
\begin{align}
    &\mathcal{C}^2 \text{ of } \overline{\mathcal{L}_{BE}}(Q) \text{ w.r.t. 
    } Q\in\mathcal{Q}\notag
    \\
    & \Longleftarrow \mathcal{C}^2 \text{ of } Q(s, a)-\left[R(s, a)+\gamma \mathbb{E}_{s^{\prime} \sim P(\cdot \mid \cdot s, a)} \log \sum_{a^{\prime}} \exp \left(Q\left(s^{\prime}, a^{\prime}\right)\right)\right] \text{ w.r.t. 
    } Q\in\mathcal{Q} \text{ for }s\in\mathcal{S}\notag
    \\
    &\Longleftarrow   \mathcal{C}^2 \text{ of }  \log \sum_{a^{\prime}} \exp \left(Q\left(s, a^{\prime}\right)\right) \text{ w.r.t. 
    } Q\in\mathcal{Q} \text{ for }s\in\mathcal{S}\notag
\end{align}
As it is known that logsumexp is of $\mathcal{C}^2$ \cite{kan2023lseminkmodifiednewtonkrylovmethod}, we are done. 
\;
\\
\;
\\
For Lipschitz smoothness, 
\begin{align}
    &\text{Lipschitz} \text{ smoothness } \text{of } \overline{\mathcal{L}_{BE}}(Q) \text{ w.r.t. 
    } Q\in\mathcal{Q}\notag
    \\
    &\!\!\iff \text{ Lipschitz continuity of } \nabla_Q \; \overline{\mathcal{L}_{BE}}(Q) \text{ w.r.t. 
    } Q\in\mathcal{Q} \notag
    \\
    &\!\!\iff \text{ Lipschitz continuity of } \mathbb{E}_{(s, a) \sim \pi^*, \nu_0}\left[2 \delta_Q(s, a) \nabla_Q \delta_Q(s, a)\right] \text{ w.r.t. 
    } Q\in\mathcal{Q} \notag
\end{align}
Now note that
\begin{align}
    &\|\mathbb{E}_{s,a \sim \pi^*, \nu_0}\left[2 \delta_Q(s, a) \nabla_Q \delta_Q(s, a)-2 \delta_{Q^\prime}(s, a) \nabla_{Q^\prime} \delta_{Q^\prime}(s, a)\right]\|_2 \notag 
    \\
    & \leq \mathbb{E}_{s,a \sim \pi^*, \nu_0}\bigl[\left\|2 \delta_Q(s, a) \nabla_Q \delta_Q(s, a)-2 \delta_{Q^\prime}(s, a) \nabla_{Q^\prime} \delta_{Q^\prime}(s, a)\right\|_2\bigr] \tag {Norm is convex}
  \\
  &\leq \mathbb{E}_{s,a \sim \pi^*, \nu_0}\left[\left\|Q^{\prime}(s,a)-Q(s, a)\right\|_2\right] \tag{Lemma \ref{lem:deltaGradDeltaLipschitz}}
  \\
  &\leq\left(\mathbb{E}_{s \sim \pi^*, \nu_0}\left\|Q^{\prime}(s, a)-Q(s, a)\right\|_2^2\right)^{1 / 2} \tag{$x^{1/2}$ is concave}
  \\
  &=\left\|Q-Q^{\prime}\right\|_{L_2(\pi^\ast, \nu_0)} \notag
\end{align}
This proves $ \text{ Lipschitz continuity of } \mathbb{E}_{(s, a) \sim \pi^*, \nu_0}\left[2 \delta_Q(s, a) \nabla_Q \delta_Q(s, a)\right] \text{ w.r.t. 
    } Q\in\mathcal{Q}$. Therefore, we can conclude the Lipschitz smoothness of $\overline{\mathcal{L}_{BE}}(Q)$ w.r.t. $ Q\in\mathcal{Q}$.
    \QED

\begin{lem}[$ \delta_Q(s, a) \nabla_Q \delta_Q(s, a)$ is Lipschitz]
\label{lem:deltaGradDeltaLipschitz}
For given fixed $(s,a)$, 
$$
\left\|2 \delta_Q(s, a) \nabla_Q \delta_Q(s, a)-2 \delta_{Q^\prime}(s, a) \nabla_{Q^\prime} \delta_{Q^\prime}(s, a)\right\|_2 \le \left\|Q^{\prime}(s,a)-Q(s, a)\right\|_2 
$$
holds for any $Q, Q^\prime \in\mathcal{Q}$.
\end{lem}
\begin{proof}[Proof of Lemma \ref{lem:deltaGradDeltaLipschitz}]
    Note that 
\begin{align}
   &\left\|\delta_Q(s, a) \nabla_Q \delta_Q(s, a)-\delta_{Q^{\prime}}(s, a) \nabla_{Q^{\prime}} \delta_{Q^{\prime}}(s, a)\right\|_2   \notag
   \\
   &\le \left\|\delta_Q(s, a)\right\|_2\left\|\nabla_Q \delta_Q(s, a) - \nabla_{Q^\prime} \delta_{Q^\prime}(s, a) \right\|_2 +  \left\|\delta_Q(s, a)-\delta_{Q^\prime}(s, a)\right\|_2 \left\| \nabla_{Q^\prime}\delta_{Q^\prime}(s,a)\right\|_2 \notag
\end{align}
Now what's left is to prove that for given fixed $(s,a)$, 
\begin{enumerate}
    \item $ \left\|\delta_Q(s, a)\right\|_2$ is bounded
    \item $\left\| \nabla_{Q^\prime}\delta_{Q^\prime}(s,a)\right\|_2$ is bounded
    \item $\delta_Q(s, a)$ is Lipschitz in $Q(s,a)$
    \item $\nabla_{Q^\prime}\delta_{Q^\prime}(s,a)$ is Lipschitz in $Q(s,a)$
\end{enumerate}

\noindent (1) Boundedness of \( \delta_Q(s,a) \):
\begin{align}
    |\delta_Q(s, a)| &= \left| \mathcal{T}Q(s,a) - Q(s,a) \right| \notag
    \\
    &= \left| r(s,a) + \beta \mathbb{E}_{s' \sim P(\cdot \mid s,a)} \left[V_Q(s')\right] - Q(s,a) \right|. \notag
\end{align}
Since \(V_Q(s') = \ln \sum_{b \in \mathcal{A}} \exp(Q(s',b))\), we use the bound:
\begin{align}
    \max_{b \in \mathcal{A}} Q(s',b) \leq V_Q(s') \leq \max_{b \in \mathcal{A}} Q(s',b) + \ln |\mathcal{A}| \notag
\end{align}
Taking expectations preserves boundedness, so we conclude:
\begin{align}
    |\delta_Q(s,a)| \leq |r(s,a)| + \beta \max_{s' \in \mathcal{S}} \max_{b \in \mathcal{A}} |Q(s',b)| + \beta \ln |\mathcal{A}| + \max_{s,a} |Q(s,a)| \notag
\end{align}
This shows \( \delta_Q(s,a) \) is uniformly bounded as long as \( Q \) is bounded, which is assured by $\beta<1$. 

\noindent (2) Boundedness of \( \nabla_Q \delta_Q(s,a) \):  
The gradient is given by:
\begin{align}
    \nabla_Q \delta_Q(s,a) &= \nabla_Q \mathcal{T} Q(s,a) - e_{(s,a)} \notag
\end{align}
where
\begin{align}
    \nabla_Q \mathcal{T} Q(s,a) &= \beta \mathbb{E}_{s' \sim P(\cdot \mid s,a)} \left[ \nabla_Q V_Q(s') \right] \notag
\end{align}
Since the softmax function \( \nabla_Q V_Q(s') \) satisfies
\begin{align}
    \sum_{b \in \mathcal{A}} \mathrm{softmax}(s',b; Q) = 1, \quad 0 \leq \mathrm{softmax}(s',b;Q) \leq 1 \notag
\end{align}
we obtain:
\begin{align}
    \|\nabla_Q \mathcal{T} Q(s,a)\|_2 \leq \beta \notag
\end{align}
Thus,
\begin{align}
    \|\nabla_Q \delta_Q(s,a)\|_2 = \|\nabla_Q \mathcal{T} Q(s,a) - e_{(s,a)}\|_2 \leq \beta + 1 \notag
\end{align}
Hence, \( \nabla_Q \delta_Q(s,a) \) is bounded.

\noindent (3) Lipschitz continuity of \( \delta_Q(s,a) \):  
Consider two functions \( Q \) and \( Q' \), and their corresponding Bellman errors:
\begin{align}
    \left| \delta_Q(s, a) - \delta_{Q'}(s, a) \right| &= \left| \mathcal{T}Q(s,a) - Q(s,a) - \mathcal{T}Q'(s,a) + Q'(s,a) \right| \notag
    \\
    &= \left| \mathcal{T}Q(s,a) - \mathcal{T}Q'(s,a) - (Q(s,a) - Q'(s,a)) \right| \notag
    \\
    &\leq \left| \mathcal{T}Q(s,a) - \mathcal{T}Q'(s,a) \right| + \left| Q(s,a) - Q'(s,a) \right| \notag
\end{align}
Since \( \mathcal{T}Q(s,a) \) depends on \( Q \) only through \( V_Q(s') \), we use the Lipschitz property of log-sum-exp:
\begin{align}
    |V_Q(s') - V_{Q'}(s')| \leq \max_{b \in \mathcal{A}} |Q(s',b) - Q'(s',b)| \notag
\end{align}
Taking expectations, we get:
\begin{align}
    |\mathcal{T}Q(s,a) - \mathcal{T}Q'(s,a)| \leq \beta \max_{s',b} |Q(s',b) - Q'(s',b)| \notag
\end{align}
Therefore,
\begin{align}
    |\delta_Q(s,a) - \delta_{Q'}(s,a)| \leq (1 + \beta) \max_{s',b} |Q(s',b) - Q'(s',b)| \notag
\end{align}
This proves \( \delta_Q(s,a) \) is Lipschitz in \( Q(s,a) \) with Lipschitz constant \( 1+\beta \).

\noindent (4) Lipschitz continuity of \( \nabla_Q \delta_Q(s,a) \):  
From the expression:
\begin{align}
    \nabla_Q \delta_Q(s,a) = \nabla_Q \mathcal{T}Q(s,a) - e_{(s,a)} \notag
\end{align}
we focus on \( \nabla_Q \mathcal{T}Q(s,a) \), which satisfies:
\begin{align}
    \|\nabla_Q \mathcal{T}Q(s,a) - \nabla_Q \mathcal{T}Q'(s,a)\|_2 &= \left\|\beta \mathbb{E}_{s' \sim P(\cdot \mid s,a)} \left[\nabla_Q V_Q(s') - \nabla_Q V_{Q'}(s') \right] \right\|_2 \notag
\end{align}
Using the Lipschitz property of Softmax,
\begin{align}
    \|\nabla_Q V_Q(s') - \nabla_Q V_{Q'}(s')\|_2 \leq \|Q(s',\cdot) - Q'(s',\cdot)\|_2 \notag
\end{align}


Taking expectations, we get:
\begin{align}
    \|\nabla_Q \mathcal{T}Q(s,a) - \nabla_Q \mathcal{T}Q'(s,a)\|_2 \leq \beta \max_{s',b} |Q(s',b) - Q'(s',b)| \notag
\end{align}
Since
\begin{align}
    \|\nabla_Q \delta_Q(s,a) - \nabla_{Q'} \delta_{Q'}(s,a)\|_2 \leq \|\nabla_Q \mathcal{T}Q(s,a) - \nabla_Q \mathcal{T}Q'(s,a)\|_2 \notag
\end{align}
we conclude that \( \nabla_Q \delta_Q(s,a) \) is Lipschitz with constant at most \( \beta \).



\end{proof}

%since softmax is the derivative of logsumexp, we need to show Lipschitz continuity of $\frac{\exp Q\left(s^{\prime}, a^{\prime}\right)}{\sum_{a^{\prime}} \exp \left(Q\left(s^{\prime}, a^{\prime}\right)\right) }$ w.r.t. $Q\in\mathcal{Q}$, i.e., 



%showing that $Q(s, a)-\left[R(s, a)+\gamma \mathbb{E}_{s^{\prime} \sim P(\cdot \mid \cdot s, a)} \log \sum_{a^{\prime}} \exp \left(Q\left(s^{\prime}, a^{\prime}\right)\right)\right]$ is smooth is enough, as this immediately proves smoothness of $\overline{\mathcal{L}_{BE}}(Q)$ ($\because$ square of the smooth function is also a smooth function and linear function of smooth function is also a smooth function). Since $\log \sum_{a^{\prime}} \exp \left(Q\left(s^{\prime}, a^{\prime}\right)\right)$ is smooth and  $\mathcal{C}^2$ in $Q$ \cite{kan2023lseminkmodifiednewtonkrylovmethod}, we are done. 

\iffalse
For Lipschitz continuity, 
\begin{align}
\left|\overline{\mathcal{L}_{\mathrm{BE}}}(Q)-\overline{\mathcal{L}_{\mathrm{BE}}}\left(Q^{\prime}\right)\right| & \leq \mathbb{E}_{(s, a) \sim \pi^*, \nu_0}\left[\left|\mathcal{L}_{\mathrm{BE}}(Q)(s, a)-\mathcal{L}_{\mathrm{BE}}\left(Q^{\prime}\right)(s, a)\right|\right] \notag \\
& \leq \mathbb{E}_{(s, a) \sim \pi^*, \nu_0}\left[L\left\|Q-Q^{\prime}\right\|_{\infty}\right] \notag
\\
&=L\left\|Q-Q^{\prime}\right\|_{\infty} \notag
\end{align}
where the second inequality is from Lemma \ref{lem:BEsaLip} along with the definition of the constant $L$.

\begin{lem}\label{lem:BEsaLip} There exists a constant $L>0$ such that for $s\in \mathcal{S}$ and $a\in \mathcal{A}$, 
$$\left|\mathcal{L}_{\mathrm{BE}}(Q)(s, a)-\mathcal{L}_{\mathrm{BE}}\left(Q^{\prime}\right)(s, a)\right| \leq L\left\|Q-Q^{\prime}\right\|_{\infty}$$    
\end{lem}
\begin{proof} Define $e_Q(s, a):=\mathcal{T} Q(s, a)-Q(s, a)$. Then $\mathcal{L}_{\mathrm{BE}}(Q)(s, a)=\left[e_Q(s, a)\right]^2$. 
    \begin{align}
    \left|\mathcal{L}_{\mathrm{BE}}(Q)(s, a)-\mathcal{L}_{\mathrm{BE}}\left(Q^{\prime}\right)(s, a)\right| &= \left|\left[e_Q(s, a)\right]^2-\left[e_{Q^{\prime}}(s, a)\right]^2\right| \notag
    \\
    &=\left|e_Q(s, a)+e_{Q^{\prime}}(s, a)\right|\cdot \left|e_Q(s, a)-e_{Q^{\prime}}(s, a)\right|\notag
    \\
    &\le C \cdot (1+\beta)\left\|Q-Q^{\prime}\right\|_{\infty}\label{eq:BEtwoerrorprod}
    \end{align}
where the inequality in equation \ref{eq:BEtwoerrorprod} is from Lemma \ref{lem:smoothBellman} and 
\begin{align}
    e_Q(s, a)+e_{Q^{\prime}}(s, a)&=[\mathcal{T} Q(s, a)-Q(s, a)]+\left[\mathcal{T} Q^{\prime}(s, a)-Q^{\prime}(s, a)\right]\notag
    \\
    &=2 r(s, a)+\beta \cdot \mathbb{E}_{s^{\prime} \sim P(s, a)}\left[V_Q\left(s^{\prime}\right)+V_{Q^{\prime}}\left(s^{\prime}\right)\right]-\left[Q(s, a)+Q^{\prime}(s, a)\right]\notag
    \\
    &\leq 2|r(s, a)|+\beta \cdot \mathbb{E}_{s^{\prime}}\left[\left|V_Q\left(s^{\prime}\right)\right|+\left|V_{Q^{\prime}}\left(s^{\prime}\right)\right|\right]+\left|Q(s, a)+Q^{\prime}(s, a)\right| \notag
    \\
    &\leq 2 R_{\max }+\beta \cdot 2(M+\log |\mathcal{A}|)+2 M \notag
    \\
    &=C:=2 R_{\max }+2 M(1+\beta)+2 \beta \log |\mathcal{A}| \notag
\end{align}
\end{proof}
\;
\\
\fi
 

\subsection{Proof of Theorem \ref{thm:BEenjoyPL} (Bellman error satisfying the PL condition)}
By Lemma \ref{lem:BE(s,a)PL} (Below), $\mathcal{L}_{BE}(Q)(s,a)$ satisfies PL condition with respect to $Q$ for all $s\in\mathcal{S}$ and $a\in\mathcal{A}$. By Lemma \ref{lem:f1f2sumPL}, $\frac{1}{|\mathcal{D}|}\sum_{(s,a)\in\mathcal{D}}\mathcal{L}_{BE}(s,a)$ is also PL. Now we would like to show that $\overline{\mathcal{L}_{\mathrm{BE}}}(Q):=\mathbb{E}_{(s, a) \sim \pi^*, \nu_0}\left[\mathcal{L}_{\mathrm{BE}}(Q)(s, a)\right]$ is also PL in terms of $L^2(\pi^\ast, \nu_0)$. Since $\overline{\mathcal{L}_{\mathrm{BE}}}(Q)$ is of $\mathcal{C}^2$, by \cite{rebjock2023fast}, showing PL is equivalent to showing to Quadratic Growth (QG), i.e., there exists $c^\prime>0$ such that
$$
\mathbb{E}_{(s, a) \sim \pi^*, \nu_0}\left[\mathcal{L}_{\mathrm{BE}}(Q)(s, a)\right]-\mathbb{E}_{(s, a) \sim \pi^*, \nu_0}\left[\mathcal{L}_{\mathrm{BE}}(Q^\ast)(s, a)\right] \ge c^\prime\|Q-Q^\ast\|^2_{L^2(\pi^\ast, \nu_0)}.
$$
But note that
\begin{align}
    &\mathbb{E}_{(s, a) \sim \pi^*, \nu_0}\left[\mathcal{L}_{\mathrm{BE}}(Q)(s, a)\right]-\mathbb{E}_{(s, a) \sim \pi^*, \nu_0}\left[\mathcal{L}_{\mathrm{BE}}(Q^\ast)(s, a)\right] \notag
    \\
    &= \mathbb{E}_{(s, a) \sim \pi^*, \nu_0}\left[\mathcal{L}_{\mathrm{BE}}(Q)(s, a)-\mathcal{L}_{\mathrm{BE}}(Q^\ast)(s, a)\right] \notag
    \\
    &\ge \mathbb{E}_{(s, a) \sim \pi^*, \nu_0}\left[c(s,a)^2(Q(s,a)-Q^\ast(s,a))^2\right] \label{eq:BEisC2}
    \\
    &=c^2\|Q-Q^\ast\|^2_{L^2(\pi^\ast, \nu_0)} \notag
\end{align}
where equation \eqref{eq:BEisC2} is due to $\mathcal{L}_{BE}(Q)(s,a)$ being QG because it is smooth and therefore PL implies QG \citep{liao2024error}. ($c(s,a)>0$ is the QG constant for $(s,a)$ and $c = \inf_{(s,a)\in \mathcal{S}\times\mathcal{A}} c(s,a)$.) This finishes the proof.
\QED

\begin{lem}\label{lem:BE(s,a)PL} For any given fixed $s\in\mathcal{S}$ and $a\in\mathcal{A}$,
$\mathcal{L}_{BE}(Q)(s,a)$ satisfies PL condition with respect to $Q$ in terms of euclidean norm.
\end{lem}
\begin{proof}[Proof of Lemma \ref{lem:BE(s,a)PL}] Throughout the proof, we extend \cite{ruszczynski2024functional} to deal with soft-max Bellman equation with infinite-dimensional state space $\mathcal{S}$. Given that $|\mathcal{A}|<\infty$, 
for each $s\in\mathcal{S}$, $Q(s, \cdot)$ can be expressed as a finite-dimensional vector $\left[Q\left(s, a^{\prime}\right)\right]_{a^{\prime} \in \mathcal{A}}\in\mathbb{R}^{|\mathcal{A}|}$; For convenience in notation, we define $q:\mathcal{S}\mapsto \mathbb{R}^{|A|}$ and
$$\mathcal{G}(s): \{q(s)\in \mathbb{R}^{|\mathcal{A}|}\mid q(s) = \left[Q\left(s, a^{\prime}\right)\right]_{a^{\prime} \in \mathcal{A}} \text{ for some }Q\in\mathcal{Q}\}$$
and use $q(s)$ instead of $Q(s,\cdot)$ and $q^\ast(s)$ instead of $Q^\ast(s,\cdot)$. 
% We can rewrite
%  $r(s,a) + \beta \cdot \mathbb{E}_{s^{\prime} \sim P(s, a)}\left[\log(\sum_{a^\prime\in\mathcal{A}}\exp Q(s^\prime, a^\prime)) \mid s, a\right]- Q(s, a)$ as $\Psi(s,a,q)$, where
We define
 \begin{align}
 \Psi(s, a, q)&:= r(s,a) + \beta \cdot \mathbb{E}_{s^{\prime} \sim P(s, a)}\left[\log(\sum_{a^\prime\in\mathcal{A}}\exp q(s^\prime)_{(a^\prime)}) \mid s, a\right]- q(s)_{(a)} \notag
 \end{align}
Now with $q^\ast(\cdot) := \left[Q^\ast\left(\cdot, a\right)\right]_{a \in \mathcal{A}}$, let's define
\begin{align}
     f(s, a, q) &:= \frac{1}{2}(\Psi(s, a, q^\ast)-\Psi(s, a, q))^2 \notag
\end{align}
 Then, for $s\in\mathcal{S}$, with the choice of $q(\tau):= q^\ast + \tau(q-q^\ast)$,
\begin{align}
  f_q(s, a, q):= \partial_q f(s, a, q)&=-\Psi_q(s,a, q)(\Psi(s, a, q^\ast)-\Psi(s, a,q))\notag
    \\
    &= - \Psi_q(s, a, q) \int_0^1 \Psi_q\left(s,a, q(\tau)\right)^\top(q^\ast(s)-q(s))d\tau \tag{Theorem \ref{thm:bolte}}
    \\
    &= - \int_0^1 \Psi_q(s,a, q) \Psi_q\left(s, a, q(\tau)\right)^\top  d\tau \cdot (q^\ast(s)-q(s))\notag
\end{align}

By Lemma \ref{lem:smallestEigen},there exists $\tilde{\lambda}$ such that for all $s\in\mathcal{S}$ and $a\in\mathcal{A}$, $\Psi_q(s, a, q^\prime) \Psi_q(s, a, q^{\prime\prime})^\top \succeq \tilde{\lambda} \cdot I$ for any choice of $q^\prime(s), q^{\prime\prime}(s) \in\mathcal{G}(s)$. %we can define $\tilde{\lambda}>0$ such that it satisfies $\Psi_q(s, a, q^\prime) \Psi_q(s, a, q^{\prime\prime})^\top \succeq \tilde{\lambda} I$ for all $q^\prime(s), q^{\prime\prime}(s) \in\mathcal{G}(s)$. 
Therefore we have
\begin{align}
    \left\langle f_q(s, a, q), q(s)-q^\ast(s)\right\rangle \ge \tilde{\lambda} \|q(s)-q^\ast(s)\|_2^2. \notag
\end{align}
This implies that
\begin{align}
    &\|f_q(s, a, q)\|_2=\max _{\|z\|=1}\left\langle f_q(s, a, q), z\right\rangle \ge \left\langle f_q(s,a,q), \frac{q(s)-q^*(s)}{\left\|q(s)-q^*(s)\right\|_2}\right\rangle \notag
    \\
    &\ge  \tilde{\lambda}\|q(s)-q^\ast(s)\|_2  \ge \tilde{\lambda}\|q(s)-q^\ast(s)\|_{\infty}  \label{eq:forPL1}
\end{align}
Therefore,
\begin{align}
    &\|f_q(s, a, q)\|_2\ge \tilde{\lambda}\|q(s)-q^\ast(s)\|_{\infty}   \label{eq:forPL3}
\end{align}
(Note: Equation \ref{eq:forPL3} is a regularity condition called sub-differential error bound.) Also, from Lemma \ref{lem:smoothBellman}, 
\begin{align}
    f(s, a, q) &= \frac{1}{2}(\Psi(s, a, q^\ast)-\Psi(s, a, q))^2\notag
    \\
    &\le \frac{1}{2} (1+\beta)^2 \|q(s)-q^\ast(s)\|_{\infty}^2\label{eq:forPL2}
\end{align}
Combining equation \ref{eq:forPL2} and \ref{eq:forPL3}, we get \begin{align}
  f(s,a,q)\le \frac{1}{2} \left(\frac{1+\beta}{\tilde{\lambda}}\right)^2 \|f_q(s, a, q)\|_2^2 \quad\text{for all }s\in\mathcal{S}, a\in\mathcal{A} \notag
\end{align}

Since $\Psi\left(s, a, q^*\right)=0$, $f(s,a,q) = \mathcal{L}_{BE}(Q)(s,a)$, where $q(s)=\left[Q\left(s, a^{\prime}\right)\right]_{a^{\prime} \in \mathcal{A}}$. This finishes the proof.

\end{proof}
\begin{thm}[\cite{bolte2023subgradient}]\label{thm:bolte}
Let $f:\mathbb{R}^n \rightarrow \mathbb{R}$ be a differentiable function. If a path $q:[0, \infty) \rightarrow \mathbb{R}^n$ is a absolutely continuous path in $\mathbb{R}^n$, $f$ admits the chain rule on the path $q(t)$ as

$$
f(q(T))-f(q(0))=\int_0^T f_q(q(t))[\dot{q}(t)] d t
$$
where $\dot{q}(t)$ is the derivative of the function path $q(t)$ with respect to $t$ and $T>0$.
\end{thm}

\begin{lem}[Positive smallest eigenvalue]\label{lem:smallestEigen} Suppose that the discount factor $\beta<1$. Then for there exists $\tilde{\lambda}>0$ such that for all $s\in\mathcal{S}$ and $a\in\mathcal{A}$, $\lambda_{\min}(\Psi_q\left(s, a, q^{\prime}\right) \Psi_q\left(s, a, q^{\prime \prime}\right)^{\top})> \tilde{\lambda}$ holds for any choice of $ q^{\prime}, q^{\prime \prime} \in \mathcal{G}(s)$.
\end{lem}
\begin{proof}
First, note that we can define the policy $\pi_q(a|s) = \frac{\exp q_{(a)}}{\sum_{a^{\prime }} \exp q_{(a^\prime)}}$ for $q\in\mathbb{R}^{|\mathcal{A}|}$, where $x_{(a)}$ implies the $a$th element of vector $x$. 
\begin{align}
     \frac{\partial \Psi(s, a, q)}{\partial q_{(a^{\prime})}}&=\beta \mathbb{E}_{s^{\prime} \sim P(s, a)}\left[\pi_q\left(a^{\prime} \mid s^{\prime}\right)\right]-\delta_{a, a^{\prime}}\notag
\end{align}
That is, $\Psi_q(s, a, q)=\beta \mu_q-e_q$, where $\mu_q=\mathbb{E}_{s^{\prime} \sim P(s, a)}\left[\pi_q\left(a^{\prime} \mid s^{\prime}\right)\right]$ is a probability vector, as it's an expectation over probability distributions. Then for any choice of $q^{\prime}, q^{\prime \prime} \in \mathcal{G}(s)$, denoting $\mu_{q^\prime} = \mu^\prime$ and $\mu_{q^{\prime\prime}} = \mu^{\prime\prime}$
\begin{align}
    \lambda\left(\Psi_q\left(s, a, q^{\prime}\right) \Psi_q\left(s, a, q^{\prime \prime}\right)^{\top}\right)& = \lambda\left(\left(\beta \mu^{\prime}-e_a\right)\left(\beta \mu^{\prime \prime}-e_a\right)^{\top}\right)\notag
    \\
    &= \left(\beta \mu^{\prime}-e_a\right)^{\top}\left(\beta \mu^{\prime \prime}-e_a\right) \notag
    \\
    &=\beta^2\left(\mu^{\prime}\right)^{\top} \mu^{\prime \prime}-\beta \mu^{\prime}(a)-\beta \mu^{\prime \prime}(a)+1 \notag
    \\
    &\ge \beta^2 \mu^{\prime}(a) \mu^{\prime \prime}(a)-\beta \mu^{\prime}(a)-\beta \mu^{\prime \prime}(a)+1 \notag
    \\
    &=(1-\beta \mu^{\prime}(a))(1-\beta \mu^{\prime\prime}(a))
    \\
    &\ge (1-\beta)^2 \notag
\end{align}
Since $\beta\in(0,1)$, $\tilde{\lambda} = (1-\beta)^2$ serves as the uniform lower bound of $\lambda_{\min}(\Psi_q\left(s, a, q^{\prime}\right) \Psi_q\left(s, a, q^{\prime \prime}\right)^{\top})$ for all $s\in\mathcal{S}$ and $a\in\mathcal{A}$, for any choice of $q^{\prime}, q^{\prime \prime} \in \mathcal{G}(s)$.
\end{proof}
\begin{lem}\label{lem:smoothBellman} $|(\mathcal{T}Q-Q)(s,a)-(\mathcal{T}Q^\ast-Q^\ast)(s,a)| \le (1+\beta)\left\|Q\left(s^{\prime}, \cdot\right)-Q^*\left(s^{\prime}, \cdot\right)\right\|_{\infty}$ for all $s\in\mathcal{S}$ and $a\in\mathcal{A}$.
\end{lem}
\begin{proof}

   \begin{align}
       &|(\mathcal{T}Q-Q)(s,a)-(\mathcal{T}Q^\ast-Q^\ast)(s,a)| \notag
       \\
       &=|\beta \cdot \mathbb{E}_{s^{\prime} \sim P(s, a)}\left[\log \left(\sum_{a^{\prime} \in \mathcal{A}} \exp Q\left(s^{\prime}, a^{\prime}\right)\right)-\log \left(\sum_{a^{\prime} \in \mathcal{A}} \exp Q^\ast\left(s^{\prime}, a^{\prime}\right)\right)  \mid s, a\right] +(Q^\ast(s, a)-Q(s, a))|\notag
       \\
       &\le|\beta \cdot \mathbb{E}_{s^{\prime} \sim P(s, a)}\left[\left\|Q\left(s^{\prime}, \cdot\right)-Q^*\left(s^{\prime}, \cdot\right)\right\|_{\infty}\right] + \left|Q^*(s, a)-Q(s, a)\right| \tag{logsumexp Liptshitz in 1}
       \\
       &\le(\beta+1)\left\|Q\left(s^{\prime}, \cdot\right)-Q^*\left(s^{\prime}, \cdot\right)\right\|_{\infty}\notag
   \end{align}
\end{proof}




\subsection{Proof of Theorem \ref{thm:NLLenjoyPL} (NLL loss satisfying the PL condition)}
From Lemma \ref{lem:KLPL} and Lemma \ref{lem:f1f2sumPL}, ${L}_{NLL}(s,a)$ and $\frac{1}{|\mathcal{D}|}\sum_{(s,a)\in\mathcal{D}}\mathcal{L}_{NLL}(s,a)$ are PL. 

\noindent What remains is to show that $\mathbb{E}_{(s, a) \sim \pi^*, \nu_0}\left[-\log \left(\hat{p}_Q(a \mid s)\right)\right]$ satisfies PL. From Lemma \ref{lem:minMLE}, we know
\begin{align}
\mathbb{E}_{(s, a) \sim \pi^*, \nu_0}\left[-\log \left(\hat{p}_Q(a \mid s)\right)\right] =\mathbb{E}_{s \sim \pi^*, \nu_0}\left[D_{K L}\left(\pi^*(\cdot \mid s) \| \hat{p}_Q(\cdot \mid s)\right)\right]+\mathbb{E}_{(s, a) \sim \pi^*, \nu_0}\left[\ln \pi^*(a \mid s)\right] \notag
\end{align}
Note that the second term is not dependent on $Q$. Therefore, we will instead show that the PL condition holds for $\mathbb{E}_{s \sim \pi^*, \nu_0}\left[D_{K L}\left(\pi^*(\cdot \mid s) \| \hat{p}_Q(\cdot \mid s)\right)\right]$. Since $\mathbb{E}_{s \sim \pi^*, \nu_0}\left[D_{K L}\left(\pi^*(\cdot \mid s) \| \hat{p}_Q(\cdot \mid s)\right)\right]$ is convex, by \cite{liao2024error}, showing that  $\mathbb{E}_{s \sim \pi^*, \nu_0}\left[D_{K L}\left(\pi^*(\cdot \mid s) \| \hat{p}_Q(\cdot \mid s)\right)\right]$ is PL is equivalent to showing that $\mathbb{E}_{s \sim \pi^*, \nu_0}\left[D_{K L}\left(\pi^*(\cdot \mid s) \| \hat{p}_Q(\cdot \mid s)\right)\right]$ satisfies Quadratic Growth (QG) condition, i.e., there exists $c^\prime>0$ such that
\begin{align}
    &\mathbb{E}_{s \sim \pi^*, \nu_0}\left[D_{K L}\left(\pi^*(\cdot \mid s) \| \hat{p}_Q(\cdot \mid s)\right)\right]-\mathbb{E}_{s \sim \pi^*, \nu_0}\left[D_{K L}\left(\pi^*(\cdot \mid s) \| \hat{p}_{Q^\ast}(\cdot \mid s)\right)\right] \geq 
c^{\prime}\left\|Q-Q^*\right\|_{L^2\left(\pi^*, v_0\right)}^2 \notag
\end{align}
\noindent But note that
\begin{align}
    &\mathbb{E}_{s \sim \pi^*, \nu_0}\left[D_{K L}\left(\pi^*(\cdot \mid s) \| \hat{p}_Q(\cdot \mid s)\right)\right]-\mathbb{E}_{s \sim \pi^*, \nu_0}\left[D_{K L}\left(\pi^*(\cdot \mid s) \| \hat{p}_{Q^\ast}(\cdot \mid s)\right)\right] \notag
     \\
     &=\mathbb{E}_{s \sim \pi^*, \nu_0}\left[D_{K L}\left(\pi^*(\cdot \mid s) \| \hat{p}_Q(\cdot \mid s)\right)-D_{K L}\left(\pi^*(\cdot \mid s) \| \hat{p}_{Q^\ast}(\cdot \mid s)\right)\right] \notag
     \\
    &\ge \mathbb{E}_{(s, a) \sim \pi^*, \nu_0}\left[c(s,a)^2(Q(s,a)-Q^\ast(s,a))^2\right] \tag{Lemma \ref{lem:KLPL} and convexity}
    \\
    &=c^2\|Q-Q^\ast\|^2_{L^2(\pi^\ast, \nu_0)} \notag
\end{align}
\noindent where $c(s,a)>0$ is the QG constant for $(s,a)$ and $c = \inf_{(s,a)\in \mathcal{S}\times\mathcal{A}} c(s,a)$. Done. \QED
%By the Lemma \ref{lem:KLPL} below, $D_{K L}\left(\pi^*(\cdot \mid s) \| \hat{p}_Q(\cdot \mid s)\right)$ satisfies PL condition for each $s\in\mathcal{S}$. 


\iffalse

Now note that $\left\{Q \in \mathcal{Q} \mid D_{K L}\left(\pi^*(\cdot \mid s) \| \hat{p}_Q(\cdot \mid s)\right)=0\right.$ for all $\left.s \in \overline{\mathcal{S}}\right\}$ is nonempty if $Q^\ast \in \mathcal{Q}$, because 
\begin{align}
    &\{Q\in\mathcal{Q}\mid D_{KL}(\pi^*(\cdot\mid s) \| \hat{p}_{Q}(\cdot\mid s))=0 \text{ for all }s\in\bar{\mathcal{S}}\} \notag
    \\
    &=\{Q\in\mathcal{Q}\mid \hat{p}_Q(\cdot \mid s)=\pi^*(\cdot \mid s)\; \; \text{a.e.} \text{ for all }s\in\bar{\mathcal{S}}\}\notag
    \\
    &= \{Q\in\mathcal{Q}\mid \frac{\hat{p}_Q\left(a_1 \mid s\right)}{\hat{p}_Q\left(a_2 \mid s\right)}=\frac{\pi^*\left(a_1 \mid s\right)}{\pi^*\left(a_2 \mid s\right)} \quad \forall a_1, a_2 \in \mathcal{A}, s\in\bar{\mathcal{S}}\}\notag   
    \\
    &=\left\{Q \in \mathcal{Q} \mid \exp (Q(s,a_1)-Q(s,a_2))=\exp \left(Q^*(s,a_1)-Q^*(s,a_2)\right) \quad \forall a_1, a_2\in\mathcal{A}, s\in\bar{\mathcal{S}} \right\}\notag
    \\
    &=\left\{Q \in \mathcal{Q} \mid Q(s,a_1)-Q(s,a_2)= Q^*(s,a_1)-Q^*(s,a_2) \quad \forall a_1, a_2\in\mathcal{A}, s\in\bar{\mathcal{S}}\right\} \notag
\end{align}


Therefore, $\mathbb{E}_{s \sim \pi^*, \nu_0}\left[D_{K L}\left(\pi^*(\cdot \mid s) \| \hat{p}_Q(\cdot \mid s)\right)\right]$ also satisfies PL by Lemma \ref{lem:expDKLalsoPL}, finishing the proof.
\QED

\fi


\begin{lem}\label{lem:KLPL}
$D_{K L}\left(\pi^*(\cdot \mid s) \| \hat{p}_Q(\cdot \mid s)\right)$ satisfies the PL condition for each $s\in\mathcal{S}$. This implies that $-\log \left(\hat{p}_Q(\cdot \mid s)\right) =D_{K L}\left(\pi^*(\cdot \mid s) \| \hat{p}_Q(\cdot \mid s)\right)+\ln \pi^*(\cdot\mid s)\notag$ is also PL for each $s\in \mathcal{S}$.
\end{lem}
\begin{proof}
Note that
\begin{align}
     \nabla_{Q(s, \cdot)}D_{K L}\left(\pi^*(\cdot \mid s) \| \hat{p}_Q(\cdot \mid s)\right)&= \nabla_{Q(s, \cdot)}\left(-\sum_a \pi^*(a \mid s) \log \hat{p}_Q(a \mid s)\right) \notag
     \\
     &=-\sum_a \pi^*(a \mid s)\left(\delta_{a, a^{\prime}}-\hat{p}_Q\left(a^{\prime} \mid s\right)\right)\notag
     \\
     &=-\left[\pi^*\left(a^{\prime} \mid s\right)-\hat{p}_Q\left(a^{\prime} \mid s\right) \sum_a \pi^*(a \mid s)\right]_{a^{\prime} \in \mathcal{A}}\notag
     \\
     &=\left[\hat{p}_Q\left(a^{\prime} \mid s\right)-\pi^*\left(a^{\prime} \mid s\right)\right]_{a^{\prime} \in \mathcal{A}} \notag
\end{align}
Then, 
\begin{align}
    \|\nabla_{Q(s,)} D_{K L}\left(\pi^*(\cdot \mid s) \| \hat{p}_Q(\cdot \mid s)\right)\|^2 & = \|\left[\hat{p}_Q\left(a^{\prime} \mid s\right)-\pi^*\left(a^{\prime} \mid s\right)\right]_{a^{\prime} \in \mathcal{A}}\|_2^2\notag
    \\
    & \ge \frac{1}{|\mathcal{A}|} \|\left[\hat{p}_Q\left(a^{\prime} \mid s\right)-\pi^*\left(a^{\prime} \mid s\right)\right]_{a^{\prime} \in \mathcal{A}}\|_1^2 \notag
    \\
    & = \frac{1}{|\mathcal{A}|} \text{TV}\left(\hat{p}_Q\left(\cdot \mid s\right),\pi^*\left(\cdot \mid s\right)\right)^2 \notag
    \\
    &\ge \frac{\alpha_Q \ln 2}{|\mathcal{A}|} D_{K L}\left(\pi^*(\cdot \mid s) \| \hat{p}_Q(\cdot \mid s)\right) \notag
\end{align}
where,
\begin{itemize}
    \item TV denotes the total variation distance.
    \item The last inequality is from Lemma \ref{lem:RevPinsker}, where $\alpha_Q:=\min _{a \in A_{+}} Q(s,a)>0$ with $A_{+}=\{a\in\mathcal{A}$ : $Q(s,a)>0\}$.
\end{itemize}
\end{proof}

\begin{lem}[Reverse Pinsker's inequality]\label{lem:RevPinsker}
    \begin{align}
        D(P \| Q)&=\sum_{a \in A_{+}} P(a) \log _2 \frac{P(a)}{Q(a)} \leq \frac{1}{\ln 2} \sum_{a \in A_{+}} P(a)\left(\frac{P(a)}{Q(a)}-1\right) \notag
        \\
        &=\frac{1}{\ln 2}\sum_{a \in A_{+}} \frac{(P(a)-Q(a))^2}{Q(a)}+\sum_{a \in A_{+}}(P(a)-Q(a)) \notag
        \\
        &=\frac{1}{\ln 2} \sum_{a \in A_{+}} \frac{(P(a)-Q(a))^2}{Q(a)} \notag
        \\
        &\leq \frac{d(P, Q)^2}{\alpha_Q \cdot \ln 2} \notag
    \end{align}
\end{lem}

\begin{lem}\label{lem:expDKLalsoPL}  Suppose that given fixed $z\in \mathcal{Z}$, a smooth function $f(x,z)$ 1) either satisfies convexity in $x$ or of $\mathcal{C}^2$ in $x$ and 2) satisfies Polyak-ojasiewicz condition in $x$ with the coefficient $\mu_z>0$, i.e., $$
\left\|\nabla_x f(x,z)\right\|_2^2 \geq 2 \mu_z\left[f(x,z)-f_z^*\right]
$$
where $f_z^*=\min _x f_z(x)$ and $\mu_z>0$. In addition, suppose that $\arg\min_x f(x,z)=\arg\min_x f(x, z^\prime)$ for all $z, z^\prime \in \mathcal{Z}$, where we define the common minimizer as $x^\ast$. Then $F(x):=\mathbb{E}_{z\sim \nu}[f(x,z)]$ satisfies Polyak-ojasiewicz condition with respect to $x$, given that $\nu$ is a measure defined on $\mathcal{Z}$. That is, 
$$
\left\|\nabla_x F(x)\right\|_2^2 \geq 2 \mu\left[F(x)-F^*\right],
$$
where $F^*:=\min _x F(x)=\mathbb{E}_{z \sim \nu}\left[f_z^*\right]$, and $\mu=\inf _{z \in \mathcal{Z}} \mu_z>0$.
\end{lem}

\begin{proof}
    Since $f$ is smooth and satisfies PL condition with respect to $x$ for given $z\in\mathcal{Z}$, it satisfies the Quadratic Growth (QG) condition \cite{liao2024error}, i.e., for fixed $z\in\mathcal{Z}$,
    there exists $\alpha_z>0$ such that:
$$
f(x,z)-f_z^\ast \geq \alpha_z\left\|x-x^*\right\|^2 \quad \forall x\in\mathcal{X}
$$
Therefore, 
\begin{align}
    F(x)-F^*&=\mathbb{E}_z\left[f(x, z)-f_z^*\right] \notag
    \\
    &\geq \mathbb{E}_z\left[\alpha_z\left\|x-x^*\right\|^2\right] \notag
    \\
    &\geq \alpha\left\|x-x^*\right\|^2 \quad (\alpha:=\inf _z \alpha_z>0) \notag
\end{align}
This implies that $F(x)$ satisfies the QG condition in $x$. If $f$ satisfies convexity, then by \citet{liao2024error}, Quadratic growth and PL are equivalent; if $f$ is of $\mathcal{C}^2$, then by \citet{rebjock2023fast}, Quadratic Growth and PL are equivalent. Therefore, $F(x)$ satisfies PL.
\end{proof}


\subsection{Proof of Lemma \ref{lem:f1f2sumPL}}
Let $f_1(Q):=\mathcal{L}_{NLL}(Q)$ and $f_2(Q):=\mathcal{L}_{BE}(Q)$. 
 Let $M_1:=\left\{Q \in \mathcal{Q}: f_1(Q)=f_1^*\right\}, $ and $ M_2:=\left\{Q \in \mathcal{Q}: f_2(Q)=f_2^*\right\}$.
 By Theorem \ref{thm:mainopt}, the minimizer of $f_1+f_2$ is in both the minimizer of $f_1$ and the minimizer $f_2$. Therefore, by Lemma \ref{lem:linsumPL}, $f_1+f_2$ is also PL. This implies that $\mathcal{R}_{exp}(Q)$ satisfies the PL. Now, given a finite dataset $\mathcal{D} = \{(s_i, a_i, s_i')\}_{i=1}^N$, note that the empirical risk function $\mathcal{R}_{emp}(Q)$ is equivalent to the expected risk function with the transition probability being $\hat{P}(s'|s,a) = \frac{\sum_{i=1}^N \mathbbm{1}[(s_i,a_i,s_i') = (s,a,s')]}{\sum_{i=1}^N \mathbbm{1}[(s_i,a_i) = (s,a)]}$ and expert policy being $\hat{\pi}^*(a|s) = \frac{\sum_{i=1}^N \mathbbm{1}[(s_i,a_i) = (s,a)]}{\sum_{i=1}^N \mathbbm{1}[s_i = s]}$. (By Theorem \ref{thm:MagnacThesmar}, we know that minimization of this problem is well-defined.) 
 Since the expected risk in this case satisfies the PL condition and has a unique solution, and is equivalent to $\mathcal{R}_{emp}(Q)$, $\mathcal{R}_{emp}(Q)$ satisfies the PL condition and has a unique solution. 

 
 
 \QED


 
\begin{lem}\label{lem:linsumPL}
Suppose that $f_1$ and $f_2$ are both PL and Lipschitz smooth. Furthermore, the minimizer of $f_1+f_2$ is unique, where the minimizer of $f_1$ and the minimizer $f_2$ coincides. Then $f_1+\lambda f_2$ satisfies PL condition for any $\lambda>0$.
\end{lem}

\begin{proof}[Proof of Lemma \ref{lem:linsumPL}] Without loss of generality, we prove that $f := f_1+f_2$ satisfies PL condition. Recall that we say $f$ satisfies $\mu$-$PL$ condition if $2\mu(f(Q)-f\left(Q^\ast\right)) \leq \|\nabla f(Q)\|^2$.
\begin{align}
    \|\nabla f(Q)\|^2&=\left\|\nabla f_1(Q)+\nabla f_2(Q)\right\|^2 \notag
    \\
    &=\left\|\nabla f_1(Q)\right\|^2 + \left\|\nabla f_2(Q)\right\|^2 + 2 \nabla f_1(Q)^{\top} \nabla f_2(Q) \notag
    \\
    &\ge 2\mu_1(f_1(Q)-f_1(Q^\ast)) +  2\mu_2(f_2(Q)-f_2(Q^\ast))+ 2 \nabla f_1(Q)^{\top} \nabla f_2(Q) \notag
    \\
    &\ge 2\mu(f_1(Q)+f_2(Q)-f_1(Q^\ast)-f_2(Q^\ast))+ 2 \nabla f_1(Q)^{\top} \nabla f_2(Q) \notag
    \\
    &=2\mu(f(Q)-f(Q^*) + 2 \nabla f_1(Q)^{\top} \nabla f_2(Q) \notag
    \\
    &\ge 2\mu(f(Q)-f(Q^*)\tag{Lemma \ref{lem:crossPos}}
\end{align}
The last inequality is not trivial, and therefore requires Lemma \ref{lem:crossPos}.

    
\end{proof}

\begin{lem}\label{lem:crossPos}
Suppose that $f_1$ and $f_2$ satisfies PL in $Q$ and minimizer of $f_1+f_2$ is in both the minimizer of $f_1$ and the minimizer $f_2$. Then for all $Q\in\mathcal{Q}$, $\left\langle\nabla f_1(Q), \nabla f_2(Q)\right\rangle \geq 0$.
    
\end{lem}
\begin{proof}
 Let $M_1:=\left\{Q \in \mathcal{Q}: f_1(Q)=f_1^*\right\}, $ and $ M_2:=\left\{Q \in \mathcal{Q}: f_2(Q)=f_2^*\right\}$. From what is assumed, $f_1+f_2$ has a minimizer $Q^*$ that belongs to both $M_1$ and $M_2$.

    Since $f_1$ and $f_2$ are both Lipschitz smooth and satisfy PL condition, they both satisfy Quadratic Growth (QG) condition \cite{liao2024error}, i.e., 
    there exists $\alpha_1, \alpha_2>0$ such that:
$$
f_1(Q)-f_1\left(Q^*\right) \geq \alpha_1\left\|Q-Q^*\right\|^2 \quad \forall Q\in\mathcal{Q}
$$
$$
f_2(Q)-f_2\left(Q^*\right) \geq \alpha_2\left\|Q-Q^*\right\|^2 \quad \forall Q\in\mathcal{Q}
$$
Now suppose, for the purpose of contradiction, that there exists $\hat{Q}\in\mathcal{Q}$ such that $\left\langle\nabla f_1(\hat{Q}), \nabla f_2(\hat{Q})\right\rangle<0$. Consider the direction $d:=-g_1=-\nabla f_1(\hat{Q})$. Then $\nabla f_1(\hat{Q})^{\top} d=g_1^{\top}\left(-g_1\right)=-\left\|g_1\right\|^2<0$ holds. This implies that $f_1(\hat{Q}+\eta d)<f_1(\hat{Q})$. Then QG condition for $f_1$ implies that
$$
\left\|\hat{Q}+\eta d-Q^*\right\|<\left\|\hat{Q}-Q^*\right\|
$$
Now, note that $\nabla f_2(\hat{Q})^{\top} d=g_2^{\top}\left(-g_1\right)=-g_1^{\top} g_2$. Since $g_1^{\top} g_2<0$, $\nabla f_2(\hat{Q})^{\top} d>0$. Therefore, $f_2(\hat{Q}+\eta d)>f_2(\hat{Q})$ for sufficiently small $\eta>0$. That is, $f_2(\hat{Q}+\eta d)-f_2(Q^\ast)>f_2(\hat{Q})-f_2(Q^\ast)$. By QG condition, this implies that $\left\|\hat{Q}+\eta d-Q^*\right\| >\left\|\hat{Q}-Q^*\right\|$. Contradiction.

\end{proof}



\subsection{Proof of Lemma \ref{lem:linPolyNonsingular}}
We consider the function class
$$
Q_{\boldsymbol{\theta}}(s, a)=\boldsymbol{\theta}^{\top} \phi(s, a)
$$
where $\phi: \mathcal{S} \times \mathcal{A} \rightarrow \mathbb{R}^d$ is a known feature map with $\|\phi(s, a)\| \leq B$ almost surely and $\boldsymbol{\theta} \in \mathbb{R}^d$ is the parameter vector. Then for any unit vector $u \in \mathbb{R}^d$, 
$$
\left|u^{\top} \phi(s, a)\right| \leq\|u\|\|\phi(s, a)\|=B
$$
Then by using Hoeffding's Lemma, we have
$$
\mathbb{E}\left[e^{\lambda u^{\top} \phi(s, a)}\right] \leq \exp \left(\frac{\lambda^2 B^2}{2}\right)
$$
Therefore we have
$$
\mathbb{P}\left(\left|u^{\top} \phi(s, a)\right| \geq t\right) \leq 2 e^{-t^2 /\left(2 B^2\right)} \quad \forall t>0
$$
Now for the given dataset $\mathcal{D}$, define
$$M=\left[\begin{array}{c}\phi\left(s_1, a_1\right)^{\top} \\ \phi\left(s_2, a_2\right)^{\top} \\ \vdots \\ \phi\left(s_{|\mathcal{D}|}, a_{|\mathcal{D}|}\right)^{\top}\end{array}\right] \in \mathbb{R}^{|\mathcal{D}| \times d}$$
Note that $D Q_\theta=M$. Then by \citet{rudelson2009smallest}, we have

$$\mathbb{P}\left(\sigma_{\min }(D Q_\theta) \geq \sqrt{|\mathcal{D}|}-\sqrt{d}\right) \geq 1-e^{-C|\mathcal{D}|}$$
provided that the dataset size satisfies $|\mathcal{D}| \geq C d$ with $C>1$.


\subsection{Proof of Theorem \ref{thm:thetaEnjoysPL}}\label{sec:proofOfthetaEnjoysPL}
\begin{proof}[Proof of Theorem \ref{thm:thetaEnjoysPL}] 
For convenience in notation,
$$
f(Q_{\boldsymbol{\theta}}):=\overline{\mathcal{L}_{\mathrm{NLL}}}(Q_{\boldsymbol{\theta}})+\lambda \mathbbm{1}_{a=a_s}\overline{\mathcal{L}_{\mathrm{BE}}}(Q_{\boldsymbol{\theta}})
$$
and denote $Q_{\boldsymbol{\theta}} = Q (\boldsymbol{\theta})$. Set $
h(\boldsymbol{\theta}):=f(Q(\boldsymbol{\theta}))
$, where $f$ is the loss in terms of the function $Q$. Then $
h\left(\boldsymbol{\theta}^*\right)=f\left(Q\left(\boldsymbol{\theta}^*\right)\right)=f\left(Q^*\right)
$
by realizability (Assumption \ref{ass:realizability}). Then
\begin{align}
    \left\|\nabla_\theta h(\boldsymbol{\theta})\right\|_2^2&=\left\|D Q({\boldsymbol{\theta}})^{\top} \nabla_Q f(Q(\boldsymbol{\theta}))\right\|_2^2 \notag
    \\
    &\ge \sigma^2_{\min} \left(D Q(\boldsymbol{\theta})\right)\left\|\nabla_Q f\left(Q({\boldsymbol{\theta})}\right)\right\|_2^2 \tag{$\dim(\mathcal{S}), \dim(\mathcal{A})<\infty$}
    \\
    &\ge m^2 \left\|\nabla_Q f\left(Q({\boldsymbol{\theta}})\right)\right\|_2^2\tag{Assumption \ref{ass:nonSingularJac}}
    \\
    &\ge 2(m^2 c)(f(Q({\boldsymbol{\theta}}))-f(Q^\ast)) \tag{PL in terms of $Q$}
    \\
    &=2(m^2c)(h(\boldsymbol{\theta})-h(\boldsymbol{\theta}^\ast)) \notag
\end{align}
\end{proof}


\iffalse
\begin{proof}[Proof of Lemma \ref{lem:expDKLalsoPL}]
    \begin{align}
    \left\|\nabla_x F(x)\right\|^2&=\left\langle\nabla_x F(x), \nabla_x F(x)\right\rangle \notag
    \\
&=\left\langle\mathbb{E}_{z\sim \nu}\left[\nabla_x f(x, z)\right], \mathbb{E}_{z^{\prime}\sim\nu}\left[\nabla_x f\left(x, z^{\prime}\right)\right]\right\rangle \notag
    \\
    &=\mathbb{E}_{z\sim \nu, z^{\prime}\sim \nu}\left[\left\langle\nabla_x f(x, z), \nabla_x f\left(x, z^{\prime}\right)\right\rangle\right] \notag
    \\
    &=\mathbb{E}_{z \sim \nu}\left[\left\|\nabla_x f(x,z)\right\|^2\right]+\mathbb{E}_{z\neq z^{\prime} \sim \nu}\left[\left\langle\nabla_x f(x,z), \nabla_x f(x,{z^{\prime}})\right\rangle\right] \notag
    \\
    &\geq 2 \mu \mathbb{E}_{z \sim \mu}\left[f(x,z)-f_z^*\right]+\mathbb{E}_{z, z^{\prime} \sim \nu}\left[\left\langle\nabla_x f(x,z), \nabla_x f(x,z^{\prime})\right\rangle\right] \notag
    \\
    &=2 \mu\left[F(x)-F^*\right]+\mathbb{E}_{z\neq z^{\prime} \sim \nu}\left[\left\langle\nabla_x f(x,z), \nabla_x f(x,z^{\prime})(x)\right\rangle\right] \notag
    \\
    &=2 \mu\left[F(x)-F^*\right] \tag{Lemma \ref{lem:Expcross}}
\end{align}
\end{proof}

\begin{lem}\label{lem:Expcross}
    For two PL and Lipschitz smooth $f_1$ and $f_2$ with the same set of minimizers $x^\ast:=\underset{x}{\arg \min } f_1(x)=\underset{x}{\arg \min } f_2(x)$, $\left\langle\nabla_x f_1(x), \nabla_x f_2(x)\right\rangle \ge 0\;\;\forall x \in \mathcal{X}$.
\end{lem}
\begin{proof}[Proof of Lemma \ref{lem:Expcross}]
    Define $\left\|x-x^\ast\right\|:=\inf _{\tilde{x} \in x^\ast}\|x-\tilde{x}\|$. From the fact that $f_1$ and $f_2$ both satisfy PL condition and Lipschitz smoothness, $f_1$ and $f_2$ satisfy the Quadratic Growth condition, i.e., Quadratic Growth (QG) condition \cite{liao2024error}, i.e., 
    there exists $\alpha_1, \alpha_2>0$ such that:
$$
f_1(x)-f_1^\ast \geq \alpha_1\left\|x-x^\ast\right\|^2 \quad \forall x\in\mathcal{X}
$$
$$
f_2(x)-f_2^\ast \geq \alpha_2\left\|x-x^\ast\right\|^2 \quad \forall x\in\mathcal{X}
$$
Now suppose, for the purpose of contradiction, that there exists $\hat{x} \in \mathcal{X}$ such that $
\left\langle\nabla_x f_1(\hat{x}), \nabla_x f_2(\hat{x})\right\rangle<0
$. Define 
$g_1:=\nabla_x f_1(\hat{x}), \quad g_2:=\nabla_x f_2(\hat{x})$ and consider the direction $d:=-g_1$. Then $g_1^{\top} d=-\left\|g_1\right\|^2<0$ holds. For sufficiently small step size $\eta>0$, $
f_1(\hat{x}+\eta d)<f_1(\hat{x})
$ holds.
By the QG condition for $f_1$ :
$$
\left\|\hat{x}+\eta d-x^\ast\right\|<\left\|\hat{x}-x^\ast\right\| .
$$
Now consider $f_2$. Using the assumption $\left\langle g_1, g_2\right\rangle<0$, $
g_2^{\top} d=g_2^{\top}\left(-g_1\right)=-\left\langle g_1, g_2\right\rangle>0
$ holds.
Thus, $f_2(x)$ increases along $d$. For sufficiently small $\eta>0$, 
$
f_2(\hat{x}+\eta d)>f_2(\hat{x})
$ must hold. 
By the QG condition for $f_2$ :
$$\left\|\hat{x}+\eta d-x^\ast\right\|>\left\|\hat{x}-x^\ast\right\| .
$$
This is a contradiction.
\end{proof}
\fi



\subsection{Proof of Proposition \ref{prop:linConvergence} (Global optima convergence under ERM-IRL)}\label{sec:ProofLinConv}


\subsubsection*{1. Optimization error analysis}\;
\\
Define 
$f_1(Q)=\mathbb{E}_{(s, a)\sim \pi^*, \nu_0}  \left[-\log\left(\hat{p}_{Q}(\;a
    \mid s)\right)\right]$ and $f_2(Q)=\mathbb{E}_{(s, a)\sim \pi^*, \nu_0}  \left[ \mathbbm{1}_{a = a_s} \mathcal{L}_{BE}(Q)\left(s,a\right)\right]$.
    By Theorem \ref{thm:mainopt}, there is a unique minimizer $Q^*$ for $f_1+\lambda f_2$, which is the same for all $\lambda>0$. Also, $ f_1+\lambda f_2$ satisfies PL condition by Lemma \ref{lem:f1f2sumPL}.

    In equation \ref{eq:mainopt} of Theorem \ref{thm:mainopt}, we saw that $f_2(Q)$ is actually of form $\max_\zeta$ $f_2(Q, \zeta)$. This implies that minimization of $f_1+\lambda f_2$, a mini-max optimization problem that satisfies two-sided PL. (The inner maximization problem is trivially strongly convex, which implies PL). 
    
    Now denote $$
    f_\lambda(Q, \zeta) :=  f_1(Q)+\lambda f_2(Q, \zeta)$$
    $$g_\lambda(Q) := \max_\zeta f_\lambda(Q, \zeta)
    $$ 
    $$
    g^*_\lambda = \min_Q g_\lambda(Q) = \min_Q \max_\zeta f_\lambda(Q, \zeta)$$
    Note that $$g_\lambda(Q)-g^*_\lambda\ge 0$$
    $$g_\lambda(Q)-f_\lambda(Q, \zeta)\ge 0
    $$
    for any $(Q, \zeta)$. Furthermore, they are both equal to 0 if and only if $(Q, \zeta)$ is a minimax point, which is $Q^\ast$ and $\zeta^\ast$. More precisely, we have
    $$
    |f_\lambda(Q,\zeta) - g_\lambda^\ast| \le (g_\lambda(Q)-g^*_\lambda) + (g_\lambda(Q)-f_\lambda(Q, \zeta))
    $$    
    Therefore, we would like to find $Q, \zeta$ that for $\alpha>0$ $a_\lambda(Q) + \alpha b(Q, \zeta) = 0$, where 
    $$a_\lambda(Q): = g_\lambda(Q)-g^*_\lambda$$
    $$b_\lambda(Q, \zeta): = g_\lambda(Q)-f_\lambda(Q, \zeta)$$
    
    \noindent At iteration 0, algorithm starts from $\hat{Q}_0$ and $\zeta=\zeta_0$. We denote the $Q, \zeta$ value at iteration $T$ as $\hat{Q}_T$ and $\zeta_T$. Also, we define $P_T$ as
    $$P_T:= a_\lambda(\hat{Q}_T) + \alpha b_\lambda(\hat{Q}_T, \zeta_T)$$
    Set that $f_\lambda(Q, \zeta)$ satisfies $\mu_1$-PL for $Q$ and $\mu_2$-PL for $\zeta$. Let $\alpha = 1/10$, $\tau_1^T=\frac{\beta}{\gamma+T}, \tau_2^T=\frac{18 l^2 \beta}{\mu_2^2(\gamma+T)}$ for some $\beta>2 / \mu_1$, $L=l+l^2 / \mu_2$, and $ \gamma>0$ such that $\tau_1^1 \leq \min \left\{1 / L, \mu_2^2 / 18 l^2\right\}$. Then the following Theorem holds.

    \begin{thm}[Theorem 3.3, Yang et al  \cite{yang2020global}]\label{thm:Yang} Consider the setup where $\lambda>0$ is fixed. Then applying Algorithm \ref{alg:estimation1} using stochastic gradient descent (SGD), $P_T$ satisfies
    $$
    P_T \leq \frac{\nu}{\gamma+T}
    $$
    \end{thm}

Note that $a_\lambda$ satisfies the PL condition with respect to $Q$ and smoothness since subtracting a constant from PL is PL. Therefore, $a_\lambda$ satisfies Quadratic Growth (QG) condition by \cite{liao2024error}, i.e., 
$$
C \cdot \mathbb{E}_{(s, a) \sim \pi^*, \nu_0}\left[\left(\hat{Q}_T(s, a)-\hat{Q}_N(s, a)\right)^2\right] \le a_\lambda(Q)-0 \le \mathcal{O}(1/T).
$$
Since $a_\lambda \le P_T$, we can conclude that $ \mathbb{E}_{(s, a) \sim \pi^*, \nu_0}\left[\left(\hat{Q}_T(s, a)-\hat{Q}_N(s, a)\right)^2\right]$ is $\mathcal{O}(1/T)$.
 

\subsubsection*{2. Statistical error analysis.}

\noindent 
Throughout, we closely follow \citet{charles2018stability}. First note that:
\begin{itemize}[leftmargin=*]
\item $Q\in\mathcal{Q}$ is assumed to be bounded, as \citep{rust1994structural} implies that $Q^\ast$ is bounded for $\beta<1$. Therefore, by Lemma \ref{lem:JointLipschitz} (below), the Lipschitz smoothness we proved in Lemma \ref{lem:ConvexityMLE} and \ref{lem:BELipschitz} implies $L$-Lipschitzness for some $L<0$. Since composition of Lipschitz continuous functions are Lipschitz continuous, both $\frac{1}{|\mathcal{D}|} \sum_{(s, a) \in \mathcal{D}} \mathcal{L}_{N L L}(Q_{\boldsymbol{\theta}})(s, a)$ and $\frac{1}{|\mathcal{D}|} \sum_{(s, a) \in \mathcal{D}} \mathcal{L}_{B E}(Q_{\boldsymbol{\theta}})(s, a)$ are Lipschitz continuous in $\theta$. Therefore, $\mathcal{R}_{emp}$ is also $L$-Lipschitz continuous for some $L>0$.
\item As discussed in Lemma \ref{lem:f1f2sumPL} and its proof, $\mathcal{R}_{emp}$ satisfies $\mu$-PL condition for some $\mu>0$ and has a unique minimizer. 
\end{itemize}
Denote the minimizer of empirical risk function $\mathcal{R}_{emp}$ for the data set $\mathcal{D}$ as $Q^\ast_{D}$, where $|\mathcal{D}|=N$. Then by \citet{charles2018stability}, $Q^\ast_{D}$ and $Q^\ast$ satisfies
$$
\bigl|\mathbb{E}_{\mathcal{D}}\bigl[\mathcal{R}_{exp}(Q^\ast_{\mathcal{D}})-\mathcal{R}_{emp}(Q^\ast_{\mathcal{D}};\mathcal{D})\bigr]\bigr| \le \frac{2L^2}{\mu N}.
$$
$$
\bigl|\mathbb{E}_{\mathcal{D}}\bigl[\mathcal{R}_{exp}(Q^\ast)-\mathcal{R}_{emp}(Q^\ast;\mathcal{D})\bigr]\bigr| \le \frac{2L^2}{\mu N}.
$$
Since
\begin{align}
    &\mathcal{R}_{\exp }\left(Q_D^*\right)-\mathcal{R}_{\exp }\left(Q^*\right) \notag
    \\
    &=\left[\mathcal{R}_{\exp }\left(Q_D^*\right)-\mathcal{R}_{\text {emp }}\left(Q_D^* ; \mathcal{D}\right)\right]+\underbrace{\left[\mathcal{R}_{\text {emp }}\left(Q_D^* ; \mathcal{D}\right)-\mathcal{R}_{e m p}\left(Q^* ; \mathcal{D}\right)\right]}_{\leq 0}+\left[\mathcal{R}_{\text {emp }}\left(Q^* ; \mathcal{D}\right)-\mathcal{R}_{\text {exp }}\left(Q^*\right)\right] \notag
\end{align}
We have 
$$
\mathbb{E}_{\mathcal{D}}\bigl[\mathcal{R}_{\exp }\left(Q_D^*\right)-\mathcal{R}_{\exp }\left(Q^*\right)\bigr] \le \frac{4L^2}{\mu N}.
$$
Since smoothness and PL implies Quadratic growth (QG) condition \citep{liao2024error}, we have 
$$
\mathbb{E}_{\mathcal{D}}\bigl[\mathbb{E}_{(s, a) \sim \pi^*, v_0}\left[\left(Q^\ast_\mathcal{D}(s, a)-Q^\ast(s, a)\right)^2\right]\bigr] \le  C\frac{4 L^2}{\mu N}
$$

\begin{lem}
\label{lem:JointLipschitz}
  Let $f: \mathcal{Q} \to \mathbb{R}$ be a differentiable function defined on a space of bounded functions $\mathcal{Q} \subseteq L^2(\mu)$, where $\mathcal{Q}$ is assumed to be bounded in $L^2(\mu)$. That is, there exists a constant $M > 0$ such that for all $Q \in \mathcal{Q}$,
$$
\| Q \|_{L^2(\mu)} \leq M.
$$
If $f$ is differentiable in the Frchet sense, then $f$ is Lipschitz continuous in the $L^2(\mu)$ norm. That is, there exists a constant $K > 0$ such that for all $Q_1, Q_2 \in \mathcal{Q}$,
$$
|f(Q_1) - f(Q_2)| \leq K \|Q_1 - Q_2\|_{L^2(\mu)}.
$$
\end{lem}

\begin{proof}
    Since $f$ is differentiable, we use the mean value theorem in Banach spaces (see, e.g., \citet{yosida2012functional}). Specifically, for any $Q_1, Q_2 \in \mathcal{Q}$, there exists some intermediate function $\tilde{Q}$ on the line segment between $Q_1$ and $Q_2$ such that:
$$
f(Q_1) - f(Q_2) = \langle \nabla f(\tilde{Q}), Q_1 - Q_2 \rangle_{L^2(\mu)}.
$$
Applying the Cauchy-Schwarz inequality in $L^2(\mu)$, we obtain:
$$
|f(Q_1) - f(Q_2)| = |\langle 
\nabla f(\tilde{Q}), Q_1 - Q_2 \rangle_{L^2(\mu)}|
$$
$$
\leq \|\nabla f(\tilde{Q})\|_{L^2(\mu)} \cdot \|Q_1 - Q_2\|_{L^2(\mu)}.
$$
Since $\mathcal{Q}$ is bounded in $L^2(\mu)$, there exists a constant $B > 0$ such that:
$$
\sup_{Q \in \mathcal{Q}} \|\nabla f(Q)\|_{L^2(\mu)} \leq B.
$$
Thus, we can take $K = B$, yielding the desired Lipschitz continuity bound:
$$
|f(Q_1) - f(Q_2)| \leq B \|Q_1 - Q_2\|_{L^2(\mu)}.
$$
\end{proof}

\iffalse


\begin{lem}
$$
\overline{\mathcal{L}_{\mathrm{BE}}}(Q)-\frac{1}{N}\sum_{(s,a,s)\in \mathcal{D}} \bigl(\bigl(\hat{\mathcal{T}} Q\left(s, a, s^{\prime}\right)-Q(s, a)\bigr)^2 -\min_\zeta\beta^2  \left(V_{Q}\left(s^{\prime}\right)-\zeta\right)^2\bigr)=\mathcal{O}(1/N) $$
\end{lem}
\begin{proof}
According to the proof procedure of Lemma \ref{lem:OurBiconj}, we have
    \begin{align}
    &\overline{\mathcal{L}_{\mathrm{BE}}}(Q)-\frac{1}{N}\sum_{(s,a,s)\in \mathcal{D}} \bigl(\bigl(\hat{\mathcal{T}} Q\left(s, a, s^{\prime}\right)-Q(s, a)\bigr)^2 -\min_\zeta\beta^2  \left(V_{Q}\left(s^{\prime}\right)-\zeta\right)^2\bigr)\notag
    \\
    &=\overline{\mathcal{L}_{\mathrm{BE}}}(Q)-\frac{1}{N}\sum_{(s,a,s)\in \mathcal{D}} \bigl(\bigl(\hat{\mathcal{T}} Q\left(s, a, s^{\prime}\right)-Q(s, a)\bigr)^2 -\min_\rho
    (\rho - \hat{\mathcal{T}}Q(s, a, s^\prime))^2\bigr)\notag
    \end{align}
    By adapting Lemma E.5 of \citet{touati2020sharp} for expert training data, we have $$\overline{\mathcal{L}_{\mathrm{BE}}}(Q)-\frac{1}{N}\sum_{(s,a,s)\in \mathcal{D}} \bigl(\bigl(\hat{\mathcal{T}} Q\left(s, a, s^{\prime}\right)-Q(s, a)\bigr)^2 -\min_\rho
    (\rho - \hat{\mathcal{T}}Q(s, a, s^\prime))^2\bigr)=\mathcal{O}(1/N)$$This finishes the proof.
\end{proof}


\begin{lem} 
\;
\\
With $|\mathcal{D}|=N$,
    $|\mathbb{E}_{(s, a) \sim \pi^*, \nu_0}\left[-\log \left(\hat{p}_Q(a \mid s)\right)\right]-\frac{1}{|\mathcal{D}|}\sum_{(s,a,s^\prime)\in \mathcal{D}} (-\log(\hat{p}_Q(a \mid s)))| = \mathcal{O}(1/\sqrt{N})$.
\end{lem}
\begin{proof}
Define 
$$
F(\mathcal{D})=\frac{1}{N} \sum_{\left(s, a, s^{\prime}\right) \in \mathcal{D}}\left[-\log \hat{p}_Q(a \mid s)\right]
$$
Consider replacing one data sample $\left(s_i, a_i, s_i^{\prime}\right)$ in $\mathcal{D}$ with another independent sample, resulting $\mathcal{D}^\prime$. This leads to a different estimate $\hat{Q}_N^{\prime}$ and corresponding softmax policy $\hat{p}_Q^{\prime}$. 
\end{proof}


Combining the above two lemma, we have
\begin{align}
  &\frac{1}{N}\bigl[\sum_{(s,a,s^\prime)\in \mathcal{D}}\bigl({-\log \left(\hat{p}_Q(a \mid s)\right)}\bigr)+ 
\lambda\mathbbm{1}_{a=a_s}\notag
    \\
    &\bigl(  \sum_{(s,a,s^\prime)\in \mathcal{D}} {\bigl(\hat{\mathcal{T}} Q\left(s, a, s^{\prime}\right)-Q(s, a)\bigr)^2}  -\beta^2 \min_{\zeta\in \mathbb{R}^{S\times A}} 
   \sum_{(s,a,s^\prime)\in \mathcal{D}} {\left(V_{Q}\left(s^{\prime}\right)-\zeta(s,a)\right)^2}\bigr) \bigr]\notag
   \\
   &- \notag
   \\
   & \mathbb{E}_{(s, a) \sim \pi^*, \nu_0, s^{\prime} \sim P(s, a)}\bigl[-\log \left(\hat{p}_Q(a \mid s)\right) + \lambda\mathbbm{1}_{a=a_s}\bigl\{\bigl(\hat{\mathcal{T}} Q\left(s, a, s^{\prime}\right)-Q(s, a)\bigr)^2 \notag
    \\
    & \quad -\beta^2  \min_{\zeta\in \mathbb{R}^{S\times A}}  \mathbb{E}_{(s, a) \sim \pi^*, \nu_0, s^{\prime} \sim P(s, a)} \bigl(\left(V_{Q}\left(s^{\prime}\right)-\zeta(s,a)\right)^2\bigr) \bigr\} \bigr]\notag
    &
    \\
     &= \mathcal{O}(1/\sqrt{N}). \notag
\end{align}
\noindent 
Now recall that we showed that the sum of $\overline{\mathcal{L}_{\mathrm{BE}}}(Q)$ term and $\mathbb{E}_{(s, a) \sim \pi^*, \nu_0}\left[-\log \left(\hat{p}_Q(a \mid s)\right)\right]$ term satisfy PL. Since Those two terms are also smooth, the sum satisfies the Quadratic Growth condition (\cite{liao2024error}). Therefore
\begin{align}
  &\frac{1}{N}\bigl[\sum_{(s,a,s^\prime)\in \mathcal{D}}\bigl({-\log \left(\hat{p}_Q(a \mid s)\right)}\bigr)+ 
\lambda\mathbbm{1}_{a=a_s}\notag
    \\
    &\bigl(  \sum_{(s,a,s^\prime)\in \mathcal{D}} {\bigl(\hat{\mathcal{T}} Q\left(s, a, s^{\prime}\right)-Q(s, a)\bigr)^2}  -\beta^2 \min_{\zeta\in \mathbb{R}^{S\times A}} 
   \sum_{(s,a,s^\prime)\in \mathcal{D}} {\left(V_{Q}\left(s^{\prime}\right)-\zeta(s,a)\right)^2}\bigr) \bigr]\notag
   \\
   &- \notag
   \\
   & \mathbb{E}_{(s, a) \sim \pi^*, \nu_0, s^{\prime} \sim P(s, a)}\bigl[-\log \left(\hat{p}_Q(a \mid s)\right) + \lambda\mathbbm{1}_{a=a_s}\bigl\{\bigl(\hat{\mathcal{T}} Q\left(s, a, s^{\prime}\right)-Q(s, a)\bigr)^2 \notag
    \\
    & \quad -\beta^2  \min_{\zeta\in \mathbb{R}^{S\times A}}  \mathbb{E}_{(s, a) \sim \pi^*, \nu_0, s^{\prime} \sim P(s, a)} \bigl(\left(V_{Q}\left(s^{\prime}\right)-\zeta(s,a)\right)^2\bigr) \bigr\} \bigr]\notag
    &
    \\
     &\ge  C\cdot\mathbb{E}_{(s, a) \sim \pi^*, \nu_0}\left[\left(Q(s, a)-\hat{Q}_N(s, a)\right)^2\right] \notag
\end{align}
 for some $C$. This implies that $\cdot\mathbb{E}_{(s, a) \sim \pi^*, \nu_0}\left[\left(Q(s, a)-\hat{Q}_N(s, a)\right)^2\right]=\mathcal{O}(1/\sqrt{N})$.






\subsection{Temp}

\begin{lem}[Near-strong convexity of $\overline{\mathcal{L}_{NLL}}(Q)$]\label{lem:NLLnearstrongconv} Hessian of $\overline{\mathcal{L}_{NLL}}(Q)$ has strictly positive eigenvalues except one eigenvalue of 0, of which eigenvector is $\mathbf{1}$. 

    
\end{lem}
\begin{proof}
From Lemma \ref{lem:minMLE}, we know
\begin{align}
\mathbb{E}_{(s, a) \sim \pi^*, \nu_0}\left[-\log \left(\hat{p}_Q(a \mid s)\right)\right] =\mathbb{E}_{s \sim \pi^*, \nu_0}\left[D_{K L}\left(\pi^*(\cdot \mid s) \| \hat{p}_Q(\cdot \mid s)\right)\right]+\mathbb{E}_{(s, a) \sim \pi^*, \nu_0}\left[\ln \pi^*(a \mid s)\right] \notag
\end{align}
Note that the second term is not dependent on $Q$. Therefore, we can instead prove that the first term $\mathbb{E}_{s \sim \pi^*, \nu_0}\left[D_{K L}\left(\pi^*(\cdot \mid s) \| \hat{p}_Q(\cdot \mid s)\right)\right]$ has strictly positive eigenvalues except one eigenvalue of 0, of which eigenvector is $\mathbf{1}$. First, note that
\begin{align}
     \nabla_{Q(s, \cdot)}D_{K L}\left(\pi^*(\cdot \mid s) \| \hat{p}_Q(\cdot \mid s)\right)&= \nabla_{Q(s, \cdot)}\left(-\sum_a \pi^*(a \mid s) \log \hat{p}_Q(a \mid s)\right) \notag
     \\
     &=-\sum_a \pi^*(a \mid s)\left(\delta_{a, a^{\prime}}-\hat{p}_Q\left(a^{\prime} \mid s\right)\right)\notag
     \\
     &=-\left[\pi^*\left(a^{\prime} \mid s\right)-\hat{p}_Q\left(a^{\prime} \mid s\right) \sum_a \pi^*(a \mid s)\right]_{a^{\prime} \in \mathcal{A}}\notag
     \\
     &=\left[\hat{p}_Q\left(a^{\prime} \mid s\right)-\pi^*\left(a^{\prime} \mid s\right)\right]_{a^{\prime} \in \mathcal{A}} \notag
\end{align}
Also, from $\frac{\partial \hat{p}_Q(a\mid s)}{\partial Q_{a^{\prime}}}=\hat{p}_Q(a\mid s)\left(\delta_{a, a^{\prime}}-\hat{p}_Q\left(a^{\prime}\mid s\right)\right)$, Hessian $H=\operatorname{diag}\left(\hat{p}_Q\right)-\hat{p}_Q \hat{p}_Q^{\top}$. Interestingly, this Hessian is equivalent to the covariance matrix of random vector $X \in \mathbb{R}^{|\mathcal{A}|}$, defined by $$X(a)= \begin{cases}1 & \text { if } A=a \\ 0 & \text { otherwise }\end{cases}, \text{ where } \mathbb{P}(A=a)=\hat{p}_Q(a)=\frac{\exp (Q(a))}{\sum_b \exp (Q(b))}$$
\;
\\
We will now show that this Hessian $H$ has only one eigenvector direction with eigenvalue 0, and other eigenvalues are all strictly positive. First, $H \mathbf{1}=\hat{p}_Q-\hat{p}_Q\left(\sum_{a^{\prime}} \hat{p}_Q\left(a^{\prime}\right)\right)=\hat{p}_Q-\hat{p}_Q=0$. 
Now define $V=\left\{v \in \mathbb{R}^{|\mathcal{A}|}: v^{\top} \mathbf{1}=\sum_a v(a)=0\right\}$, which is the subspace that is orthogonal to $\mathbf{1}$. Then for $v \in V$,
$$
v^{\top} H v=v^{\top} \operatorname{Cov}(X) v=\operatorname{Var}(v(A)),
$$
where $v(A)=\sum_a v(a) \delta_{A, a}$ is a scalar-valued random variable depending on the random draw $A \sim \hat{p}_Q$. We have $\operatorname{Var}(v(A))>0$, because:
\begin{itemize}
    \item If $v(a)$ are not all identical, then $v(A)$ is a non-constant random variable under $\hat{p}_Q$. Because $\hat{p}_Q$ assigns positive probability to each action, the random variable $v(A)$ takes on at least two distinct values with positive probability. Hence, its variance $\operatorname{Var}(v(A))$ is strictly greater than zero.
    \item If $v(a)$ were constant for all $a$, say $v(a)=c$, then $\sum_a v(a)=c \sum_a 1=$ $c|\mathcal{A}|$. This contradicts $v\in V$.
\end{itemize}
    
\end{proof} 

\fi
\newpage

\section{Equivalence between Dynamic Discrete choice and Entropy regularized Inverse Reinforcement learning}\label{sec:DDCIRL}


\subsection{Properties of Type 1 Extreme Value (T1EV) distribution}
Type 1 Extreme Value (T1EV), or Gumbel distribution, has a location parameter and a scale parameter. The T1EV distribution with location parameter $\nu$ and scale parameter 1 is denoted as Gumbel $(\nu, 1)$ and has its CDF, PDF, and mean as follows:
$$
\begin{gathered}
\text { CDF: } F(x ; \nu)=e^{-e^{-(x-\nu)}}
\\
\text { PDF: } f(x ; \nu)=e^{-\left((x-\nu)+e^{-(x-\nu)}\right)}
\\
\text { Mean } = \nu + \gamma
\end{gathered}
$$

Suppose that we are given a set of $N$ independent Gumbel random variables $G_i$, each with their own parameter $\nu_i$, i.e. $G_i \sim \operatorname{Gumbel}\left(\nu_i, 1\right)$.

\begin{lem}\label{lem:GumbelMax}
    Let $Z=\max G_i$. Then $Z \sim \operatorname{Gumbel}\left(\nu_Z=\log \sum_{i} e^{\nu_i}, 1\right)$.
\end{lem}
\begin{proof}
    $F_Z(x)=\prod_{i} F_{G_i}(x)=\prod_{i} e^{-e^{-\left(x-\nu_i\right)}}=e^{-\sum_{i} e^{-\left(x-\nu_i\right)}}=e^{-e^{-x} \sum_{i} e^{\nu_i}}=e^{-e^{-\left(x-\nu_Z\right)}}$
\end{proof}

\begin{cor}\label{cor:GumbelOptProb}
    $P\left(G_k>\max _{i \neq k} G_i\right)=\frac{e^{\nu_k}}{\sum_{i} e^{\nu_i}}$.
\end{cor}
\begin{proof}
\begin{align}
    P\left(G_k>\max _{i \neq k} G_i\right)&=\int_{-\infty}^{\infty} P\left(\max _{i \neq k} G_i<x\right) f_{G_{k}}(x) d x\notag
    \\&=\int_{-\infty}^{\infty} e^{-\sum_{i \neq k} e^{-\left(x-\nu_i\right)}} e^{-\left(x-\nu_k\right)} e^{-e^{-\left(x-\nu_k\right)}} d x \notag
    \\&=e^{\nu_k} \int_{-\infty}^{\infty} e^{-e^{-x} \sum_{i} e^{\nu_i}} e^{-x} d x \notag
    \\&=e^{\nu_k}\int_{\infty}^0 e^{-u S} u\left(-\frac{d u}{u}\right)  \; \; \left(\text{Let } \sum_{i} e^{\nu_i}=S, u=e^{-x}\right)\notag
    \\&=e^{\nu_k}\int_0^{\infty} e^{-u S} d u=e^{\nu_k}\left[-\frac{1}{S} e^{-u S}\right]_0^{\infty} = \frac{e^{\nu_k}}{S}\notag 
    \\&=\frac{e^{\nu_k}}{\sum_{i} e^{\nu_i}} \notag
\end{align}
\end{proof}

\begin{lem}\label{lem:ExpofLargerGumbel}
    Let $G_1\sim \text{Gumbel }(\nu_1, 1)$ and $G_2\sim \text{Gumbel }(\nu_2, 1)$. Then $\mathbb{E}\left[G_1 \mid G_1\geq G_2\right]=\gamma + \log \left( 1+ e^{\left(-(\nu_1-\nu_2)\right)} \right)$ holds. 
\end{lem}

\begin{proof}
    Let $\nu_1-\nu_2 = c$. Then $\mathbb{E}\left[G_1 \mid G_1\geq G_2\right]$ is equivalent to $\nu_1 + \frac{\int_{-\infty}^{+\infty} x F(x+c) f(x) \mathrm{d} x}{\int_{-\infty}^{+\infty} F(x+c) f(x) \mathrm{d} x}$, where the pdf $f$ and cdf $F$ are associated with $\text{Gumbel }(0, 1)$, because

    \begin{align}
        P\left(G_1 \leq x, G_1 \geq G_2\right)&=\int_{-\infty}^x F_{G_2}(t) f_{G_1}(t) d t=\int_{-\infty}^x F\left(t-\nu_2\right) f\left(t-\nu_1\right) d t\notag
        \\
        \mathbb{E}\left[G_1 \mid G_1 \geq G_2\right]&=\frac{\int_{-\infty}^{\infty} x F\left(x+c-\nu_1\right) f\left(x-\nu_1\right) d x}{\int_{-\infty}^{\infty} F\left(x+c-\nu_1\right) f\left(x-\nu_1\right) d x}\notag
        \\
        &=\frac{\int_{-\infty}^{\infty}\left(y+\nu_1\right) F(y+c) f(y) d y}{\int_{-\infty}^{\infty} F(y+c) f(y) d y} \notag
        \\
        &=\nu_1+\frac{\int_{-\infty}^{\infty} y F(y+c) f(y) d y}{\int_{-\infty}^{\infty} F(y+c) f(y) d y} \notag
    \end{align}

    
    Now note that 

    $\begin{aligned} \int_{-\infty}^{+\infty} F(x+c) f(x) \mathrm{d} x & =\int_{-\infty}^{+\infty} \exp \{-\exp [-x-c]\} \exp \{-x\} \exp \{-\exp [-x]\} \mathrm{d} x \\ & \stackrel{a=e^{-c}}{=} \int_{-\infty}^{+\infty} \exp \{-(1+a) \exp [-x]\} \exp \{-x\} \mathrm{d} x \\ & =\frac{1}{1+a}\left[\exp \left\{-(1+a) e^{-x}\right\}\right]_{-\infty}^{+\infty} \\ & =\frac{1}{1+a}\end{aligned}$
    \\
    and
    \\
    $\begin{aligned}  \int_{-\infty}^{+\infty} x F(x+c) f(x) \mathrm{d} x&=\int_{-\infty}^{+\infty} x \exp \{-(1+a) \exp [-x]\} \exp \{-x\} \mathrm{d} x \\ & \stackrel{z=e^{-x}}{=} \int_0^{+\infty} \log (z) \exp \{-(1+a) z\} \mathrm{d} z \\ & =\frac{-1}{1+a}\left[\operatorname{Ei}(-(1+a) z)-\log (z) e^{-(1+a) z}\right]_0^{\infty} \\ & =\frac{\gamma+\log (1+a)}{1+a} \\ & \end{aligned}$
    \\
    Therefore, $\mathbb{E}\left[G_1 \mid G_1\geq G_2\right]=\gamma + \nu_k+ \log \left( 1+ e^{\left(-(\nu_1-\nu_2)\right)} \right)$ holds.
\end{proof}

\begin{cor}\label{cor:GumbelMaxasProb} $ \mathbb{E}\left[G_k \mid G_k = \max G_i\right] = \gamma + \nu_k - \log \left(\frac{e^{\nu_k}}{\sum_{i} e^{\nu_i}}\right)$. 
\end{cor}
\begin{proof}

\begin{align}
    \mathbb{E}\left[G_k \mid G_k = \max G_i\right] &= \mathbb{E}\left[G_k \mid G_k \geq \max_{i\neq k} G_i\right]\notag
    \\
    &=\gamma + \nu_k + \log \left( 1+ e^{\left(-(\nu_k-\log\sum_{i\neq k} e^{\nu_i})\right)}\right)\tag{Lemma \ref{lem:ExpofLargerGumbel}}
    \\
    &=\gamma + \nu_k + \log \left( 1+ \frac{\sum_{i\neq k} e^{\nu_i}}{e^{-\nu_k}}\right) \notag
    \\
    &=\gamma + \nu_k + \log \left(\sum_{i} e^{\nu_i}/e^{\nu_k} \right)\notag
    \\
    &=\gamma + \nu_k -\log \left(e^{\nu_k} / \sum_{i} e^{\nu_i} \right)\notag
\end{align}

\end{proof}


\subsection{Properties of entropy regularization}
Suppose we have a choice out of discrete choice set $\mathcal{A} = \{x_i\}_{i=1}^{|\mathcal{A}|}$. A choice policy can be a deterministic policy such as $\operatorname{argmax}_{i \in 1, \ldots, |\mathcal{A}|} x_i$, or stochastic policy that is characterized by $\mathbf{q}\in \triangle_{\mathcal{A}}$. When we want to enforce smoothness in choice, we can regularize choice by newly defining the choice rule 
$$\arg\max _{\mathbf{q} \in \Delta_{\mathcal{A}}}\left(\langle\mathbf{q}, \mathbf{x}\rangle-\Omega(\mathbf{q})\right)$$
where $\Omega$ is a regularizing function. 

\begin{lem}\label{lem:logsumexp_Shannon}
When the regularizing function is constant $-\tau$ multiple of Shannon entropy $H(\mathbf{q})=-\sum_{i=1}^{|\mathcal{A}|} q_i \log \left(q_i\right)$, $$\max _{\mathbf{q} \in \Delta_{\mathcal{A}}}\left(\langle\mathbf{q}, \mathbf{x}\rangle-\Omega(\mathbf{q})\right)=\tau \log \left(\sum_i \exp \left(x_i / \tau\right)\right)$$ and

$$\arg\max _{\mathbf{q} \in \Delta_{\mathcal{A}}}\left(\langle\mathbf{q}, \mathbf{x}\rangle-\Omega(\mathbf{q})\right)=\frac{\exp \left(\frac{x_i}{\tau}\right)}{\sum_{j=1}^n \exp \left(\frac{x_j}{\tau}\right)}$$
\end{lem}
\begin{proof}
    In the following, I will assume $\tau>0$. Let
\begin{align}
G(\mathbf{q})&=\langle\mathbf{q}, \mathbf{x}\rangle-\Omega(\mathbf{q})\notag
\\
&=\sum_{i=1}^n q_i x_i-\tau \sum_{i=1}^n q_i \log \left(q_i\right)
\notag
\\
&=\sum_{i=1}^n q_i \left(x_i-\tau \log \left(q_i\right)\right) \notag
\end{align}


We are going to find the max by computing the gradient and setting it to 0 . We have
$$
\frac{\partial G}{\partial q_i}=x_i-\tau\left(\log \left(q_i\right)+1\right)
$$
and
$$
\frac{\partial G}{\partial q_i \partial q_j}=\left\{\begin{array}{l}
-\frac{\tau}{q_i}, \quad \text { if } i=j \\
0, \quad \text { otherwise. }
\end{array}\right.
$$

This last equation states that the Hessian matrix is negative definite (since it is diagonal and $-\frac{\tau}{q_1}<0$ ), and thus ensures that the stationary point we compute is actually the maximum. Setting the gradient to $\mathbf{0}$ yields $q_i^*=\exp \left(\frac{x_i}{\tau}-1\right)$, however the resulting $\mathbf{q}^*$ might not be a probability distribution. To ensure $\sum_{i=1}^n q_i^*=1$, we add a normalization:
$$
q_i^*=\frac{\exp \left(\frac{x_i}{\tau}-1\right)}{\sum_{j=1}^n \exp \left(\frac{x_j}{\tau}-1\right)}=\frac{\exp \left(\frac{x_i}{\tau}\right)}{\sum_{j=1}^n \exp \left(\frac{x_j}{\tau}\right)} .
$$

This new $\mathbf{q}^*$ is still a stationary point and belongs to the probability simplex, so it must be the maximum. Hence, you get
$$
\begin{aligned}
\max _{\tau H}(\mathbf{x})& =G\left(\mathbf{q}^*\right)=\sum_{i=1}^n \frac{\exp \left(\frac{x_i}{\tau}\right)}{\sum_{j=1}^n \exp \left(\frac{x_j}{\tau}\right)} x_i-\tau \sum_{i=1}^n \frac{\exp \left(\frac{x_1}{\tau}\right)}{\sum_{j=1}^n \exp \left(\frac{x_j}{\tau}\right)}\left(\frac{x_i}{\tau}-\log \sum_{i=1}^n \exp \left(\frac{x_j}{\tau}\right)\right) \\
&= \tau \log \sum_{i=1}^n \exp \left(\frac{x_j}{\tau}\right)
\end{aligned}
$$
as desired.
\end{proof}





\subsection{IRL with entropy regularization}\label{sec:IRLentropy}

\subsubsection*{Markov decision processes} 
Consider an MDP defined by the tuple $\left(\mathcal{S}, \mathcal{A}, P, \nu_0, r, \beta\right)$:
\begin{itemize}
    \item $\mathcal{S}$ and $\mathcal{A}$ denote finite state and action spaces
    \item $P \in \Delta_{\mathcal{S}}^{\mathcal{S} \times \mathcal{A}}$ is a Markovian transition kernel, and $\nu_0 \in \Delta_{\mathcal{S}}$ is the initial state distribution. 
    \item $r \in \mathbb{R}^{\mathcal{S} \times \mathcal{A}}$ is a reward function.
    \item $\beta \in(0,1)$ a discount factor
\end{itemize}
\subsubsection{Agent behaviors}

Denote the distribution of agent's initial state $s_0\in \mathcal{S}$ as $\nu_0$. Given a stationary Markov policy $\pi \in \Delta_{\mathcal{A}}^{\mathcal{S}}$, an agent starts from initial state $s_0$ and make an action $a_h\in \mathcal{A}$ at state $s_h\in \mathcal{S}$ according to $a_h\sim\pi\left(\cdot \mid s_h\right)$ at each period $h$. We use $\mathbb{P}_{\nu_0}^\pi$ to denote the distribution over the sample space $(\mathcal{S} \times \mathcal{A})^{\infty}=\left\{\left(s_0, a_0, s_1, a_1, \ldots\right): s_h \in \mathcal{S}, a_h \in \mathcal{A}, h \in \mathbb{N}\right\}$ induced by the policy $\pi$ and the initial distribution $\nu_0$. We also use $\mathbb{E}_\pi$ to denote the expectation with respect to $\mathbb{P}_{\nu_0}^\pi$. Maximum entropy inverse reinforcement learning (MaxEnt-IRL) makes the following assumption: 

\begin{asmp}[Assumption \ref{ass:IRLoptimaldecision}] Agent follows the policy $\pi^*=\operatorname{argmax}_{\pi \in \Delta_{\mathcal{A}}^{\mathcal{S}}}$
$\mathbb{E}_\pi\left[\sum_{h=0}^{\infty} \beta^h \left(r\left(s_h, a_h\right)+\lambda\mathcal{H}\left(\pi\left(\cdot \mid s_h\right)\right)\right)\right]$, where $\mathcal{H}$ denotes the Shannon entropy and $\lambda$ is the regularization parameter.
\end{asmp}
For the rest of the section, we use $\lambda=1$.
\\
\;
\\
We define the function $V$ as $V(s_{h^\prime})=\mathbb{E}_{\pi^*}\left[\sum_{h=h^\prime}^{\infty} \beta^{h} \left(r\left(s_h, a_h\right)+\mathcal{H}\left(\pi^*\left(\cdot \mid s_h\right)\right)\right)\right]$ and call it the \textit{value function}. According to Assumption \ref{ass:IRLoptimaldecision}, the value function $V$ must satisfy the Bellman equation, i.e., 
\begin{align}
V\left(s\right)&=\max _{\mathbf{q} \in \triangle_\mathcal{A}}\left\{\mathbb{E}_{a\sim\mathbf{q} }\left[r\left(s, a\right)+\beta \cdot \mathbb{E}\left[V\left(s^\prime\right)\mid s, a\right]\right]+\mathcal{H}(\mathbf{q})\right\}\notag
\\
&=\max _{\mathbf{q} \in \triangle_\mathcal{A}}\left\{\sum_{a\in \mathcal{A}} q_a\left(r\left(s, a\right)+\beta \cdot \mathbb{E}\left[V\left(s^\prime\right)\mid s, a\right]\right)+\mathcal{H}(\mathbf{q})\right\}=\max _{\mathbf{q} \in \triangle_\mathcal{A}}\left\{\sum_{a\in \mathcal{A}} q_a Q(s,a)+\mathcal{H}(\mathbf{q})\right\}\label{eq:VandmaxQ}
\\
&=\ln \left[\sum_{a\in \mathcal{A}}\exp\left(r\left(s, a\right)+\beta \cdot \mathbb{E}\left[{V}\left(s^\prime\right)\mid s, a\right]\right)\right]\label{eq:logsumexp_reg}
\\
&=\ln \left[\sum_{a\in \mathcal{A}}\exp\left(Q(s, a)\right)\right]\label{eq:IRLlogsumQ}
\end{align}
and $\mathbf{q}^*:=\arg\max_{\mathbf{q} \in \triangle_\mathcal{A}} \left\{\mathbb{E}_{a\sim\mathbf{q} }\left[r\left(s, a\right)+\beta \cdot \mathbb{E}\left[V\left(s^\prime\right)\mid s, a\right]\right]+\mathcal{H}(\mathbf{q})\right\}$ is characterized by
\\
\begin{align}
\mathbf{q}^* = [q_1^* \ldots q^*_{|\mathcal{A}|}], \text{ where }
    q^*_a= \frac{\exp \left({Q(s, a)}\right)}{\sum_{a^\prime\in \mathcal{A}} \exp \left({Q(s, a^\prime)}\right)} \text{ for } a\in \mathcal{A}  \label{eq:IRLopt}
\end{align}
where:
\begin{itemize}
    \item $Q(s, a):=r\left(s, a\right)+\beta \cdot \mathbb{E}\left[{V}\left(s^\prime\right)\mid s, a\right]$
    \item Equality in equation \ref{eq:logsumexp_reg} and equality in equation \ref{eq:IRLopt} is from Lemma \ref{lem:logsumexp_Shannon}
    
\end{itemize}
\;
\\
This implies that
\begin{align}
    \pi^*(a\mid s) = q^*_a= \frac{\exp \left({Q(s, a)}\right)}{\sum_{a^\prime\in \mathcal{A}} \exp \left({Q(s, a^\prime)}\right)} \text{ for } a\in \mathcal{A}. \notag
\end{align}
\\
In addition to the Bellman equation in terms of value function $V$, 
Bellman equation in terms of choice-specific value function $Q(s,a)$ can be derived by combining $Q(s, a):=r\left(s, a\right)+\beta \cdot \mathbb{E}\left[{V}\left(s^\prime\right)\mid s, a\right]$ and equation \ref{eq:IRLlogsumQ}:
\begin{align}
    Q(s, a)=r(s, a)+\beta \mathbb{E}_{s^\prime \sim P(s, a)}\left[\ln \left(\sum_{a^{\prime} \in \mathcal{A}} \exp \left(Q\left(s^{\prime}, a^{\prime}\right)\right)\right)\mid s, a\right] \notag
\end{align}
\;
\\
We can also derive an alternative form of choice-specific value function $Q(s,a)$ by combining $Q(s, a):=r\left(s, a\right)+\beta \cdot \mathbb{E}_{s^\prime \sim P(s, a)}\left[{V}\left(s^\prime\right)\mid s, a\right]$ and equation \ref{eq:VandmaxQ}:

\begin{align}
    Q(s, a)&=r\left(s, a\right)+\beta \cdot \mathbb{E}_{s^\prime \sim P(s, a)}\left[\max _{\mathbf{q} \in \triangle_\mathcal{A}}\left\{\sum_{a\in \mathcal{A}} q_a Q(s^\prime,a)+\mathcal{H}(\mathbf{q})\right\}\mid s, a\right]\notag
    \\
    &=r\left(s, a\right)+\beta \cdot \mathbb{E}_{s^\prime \sim P(s, a)}\left[\max _{\mathbf{q} \in \triangle_\mathcal{A}}\left\{\sum_{a\in \mathcal{A}} q_a \left(Q(s^\prime,a) - \log q_a\right)\right\}\mid s, a\right]\notag
    \\
    &=r\left(s, a\right)+\beta \cdot \mathbb{E}_{s^\prime \sim P(s, a), a^\prime \sim \pi^*(a\mid \cdot)}\left[ \left(Q(s^\prime,a^\prime) - \log \pi^*(a^\prime\mid s^\prime)\right)\mid s, a\right] \label{eqn:IRLQBellman_new}
    \\
    &=r\left(s, a\right)+\beta \cdot \mathbb{E}_{s^\prime \sim P(s, a)}\left[ \left(Q(s^\prime,a^\prime) - \log \pi^*(a^\prime\mid s^\prime)\right)\mid s, a\right]\text{ for all } a^\prime\in \mathcal{A} \notag
\end{align}
The last line comes from the fact that $Q(s^\prime,a^\prime) - \log \pi^*(a^\prime\mid s^\prime)$ is equivalent to $\log \left(\sum_{a^{\prime} \in \mathcal{A}} \exp \left(Q\left(s^{\prime}, a^{\prime}\right)\right)\right)$, which is a quantity that does not depend on the realization of specific action $a^\prime$.


\subsection{Single agent Dynamic Discrete Choice (DDC) model}\label{sec:SingleDDC}

\subsubsection*{Markov decision processes} 
Consider an MDP $\tau:=\left(\mathcal{S}, \mathcal{A}, P, \nu_0, r, G(\delta,1), \beta \right)$:
\begin{itemize}
    \item $\mathcal{S}$ and $\mathcal{A}$ denote finite state and action spaces
    \item $P \in \Delta_{\mathcal{S}}^{\mathcal{S} \times \mathcal{A}}$ is a Markovian transition kernel, and $\nu_0 \in \Delta_{\mathcal{S}}$ is the initial state distribution. 
    \item $r(s_h,a_h)+\epsilon_{ah}$ is the immediate reward (called the flow utility in the Discrete Choice Model literature) from taking action $a_h$ at state $s_h$ at time-step $h$, where:
    \begin{itemize}
        \item $r \in \mathbb{R}^{\mathcal{S} \times \mathcal{A}}$ is a deterministic reward function
        \item  
    $\epsilon_{ah}\overset{i.i.d.}{\sim}  G(\delta, 1)$ is the random part of the reward, where $G$ is Type 1 Extreme Value (T1EV) distribution (a.k.a. Gumbel distribution). The mean of $G(\delta, 1)$ is $\delta + \gamma$, where $\gamma$ is the Euler constant. 
    \item In the econometrics literature, this reward setting is considered as a result of a combination of two assumptions: conditional independence (CI) and additive separability (AS) \cite{magnac2002identifying}. 
\begin{figure}[H]
    \centering
    \includegraphics[width=0.3\linewidth]{Figures/Gumbel.png}
    \caption{Gumbel distribution $G(-\gamma, 1)$}
\end{figure}
    \end{itemize}
    \item $\beta \in(0,1)$ a discount factor
    
\end{itemize}
\;

\subsubsection{Agent behaviors} Denote the distribution of agent's initial state $s_0\in \mathcal{S}$ as $\nu_0$. Given a stationary Markov policy $\pi \in \Delta_{\mathcal{A}}^{\mathcal{S}}$, an agent starts from initial state $s_0$ and make an action $a_h\in \mathcal{A}$ at state $s_h\in \mathcal{S}$ according to $a_h\sim\pi\left(\cdot \mid s_h\right)$ at each period $h$. We use $\mathbb{P}_{\nu_0}^\pi$ to denote the distribution over the sample space $(\mathcal{S} \times \mathcal{A})^{\infty}=\left\{\left(s_0, a_0, s_1, a_1, \ldots\right): s_h \in \mathcal{S}, a_h \in \mathcal{A}, h \in \mathbb{N}\right\}$ induced by the policy $\pi$ and the initial distribution $\nu_0$. We also use $\mathbb{E}_\pi$ to denote the expectation with respect to $\mathbb{P}_{\nu_0}^\pi$. As in Inverse Reinforcement learning (IRL), a Dynamic Discrete Choice (DDC) model makes the following assumption: 

\begin{asmp}\label{ass:optimaldecision} Agent makes decision according to the policy $\operatorname{argmax}_{\pi \in \Delta_{\mathcal{A}}^{\mathcal{S}}}$
$\mathbb{E}_\pi\left[\sum_{h=0}^{\infty} \beta^h( r\left(s_h, a_h\right)+\epsilon_{ah})\right]$.
\end{asmp}

As Assumption \ref{ass:optimaldecision} specifies the agent's policy, we omit $\pi$ in the notations from now on. Define $\boldsymbol{\epsilon}_h = [\epsilon_{1h}\ldots \epsilon_{|\mathcal{A}|h}]$, where $\epsilon_{ih}\overset{i.i.d}{\sim} G(\delta, 1)$ for $i=1\ldots |\mathcal{A}|$. We define a function $V$ as
\begin{align}
    V\left(s_{h^\prime}, \boldsymbol{\epsilon_{h^\prime}}\right) = \mathbb{E}\left[\sum_{h=h^\prime}^{\infty} \beta^h( r\left(s_h, a_h\right)+\epsilon_{ah})\mid s_{h^\prime}\right] \notag
\end{align}
and call it the value function. According to Assumption \ref{ass:optimaldecision}, the value function $V$ must satisfy the Bellman equation, i.e., 
\begin{align}
V\left(s, \boldsymbol{\epsilon}\right)=\max _{a \in \mathcal{A}}\left\{r\left(s, a\right)+\epsilon_{a}+\beta \cdot \mathbb{E}_{s^\prime \sim P(s, a), \boldsymbol{\epsilon^\prime }\sim \boldsymbol{\epsilon}}\left[V\left(s^\prime, \boldsymbol{\epsilon}^\prime\right)\mid s, a\right]\right\} \label{eq:VBellman}.
\end{align}
\;
\\
Define 
\begin{align}
    \bar{V}\left(s\right) &\triangleq E_{\boldsymbol{\epsilon}}\left[V\left(s, \boldsymbol{\epsilon}\right)\right] \notag
    \\
    Q(s, a) &\triangleq r\left(s, a\right)+\beta \cdot \mathbb{E}_{s^\prime \sim P(s, a)}\left[\bar{V}\left(s^\prime\right)\mid s, a\right]\label{eq:QandexpV}
\end{align}
We call $\bar{V}$ the expected value function, and $Q(s, a)$ as the choice-specific value function. Then the Bellman equation can be written as

\;
\begin{align}
\bar{V}\left(s\right) &=\mathbb{E}_{
\boldsymbol{\epsilon}}\left[\max _{a \in \mathcal{A}}\left\{r\left(s, a\right)+\epsilon_{a}+\beta \cdot \mathbb{E}\left[\bar{V}\left(s^\prime\right)\mid s, a\right]\right\}\right]\label{eq:DP_DDC_pre} 
\\
&=\ln \left[\sum_{a\in \mathcal{A}}\exp\left(r\left(s, a\right)+\beta \cdot \mathbb{E}\left[\bar{V}\left(s^\prime\right)\mid s, a\right]\right)\right] + \delta + \gamma \tag{$\because$ Lemma \ref{lem:GumbelMax}}\label{eq:DP_DDC}
\\
&=\ln \left[\sum_{a\in \mathcal{A}}\exp\left(Q(s,a)\right)\right]  + \delta + \gamma \label{eq:logsumQ}
\end{align}
\\
\;
\\
Furthermore, Corollary \ref{cor:GumbelOptProb} characterizes that the agent's optimal policy is characterized by 
\begin{align}
    \pi^*(a \mid s) =\frac{\exp \left({Q(s, a)}\right)}{\sum_{a^\prime\in \mathcal{A}} \exp \left({Q(s, a^\prime)}\right)} \text{ for } a\in \mathcal{A} \label{eq:DDCopt}
\end{align}
\;
\\
In addition to Bellman equation in terms of value function $V$ in equation \ref{eq:VBellman}, 
Bellman equation in terms of choice-specific value function $Q$ comes from combining equation \ref{eq:QandexpV} and equation \ref{eq:logsumQ}:
\begin{align}
    Q(s, a)=r(s, a)+\beta \mathbb{E}_{s^\prime \sim P(s, a)}\left[\ln \left(\sum_{a^{\prime} \in \mathcal{A}} \exp \left(Q\left(s^{\prime}, a^{\prime}\right)\right)\right)\mid s, a\right] + \delta + \gamma
\end{align}
\;
\\
When $\delta = -\gamma$ (i.e., the Gumbel noise is mean 0), we have 
\begin{align}
    Q(s, a)=r(s, a)+\beta \mathbb{E}_{s^\prime \sim P(s, a)}\left[\ln \left(\sum_{a^{\prime} \in \mathcal{A}} \exp \left(Q\left(s^{\prime}, a^{\prime}\right)\right)\right)\mid s, a\right] \tag{ \ref{eq:QBellmanDDC}}
\end{align}
\;
\\
This Bellman equation can be also written in another form.

\begin{align}
  Q(s, a) &\triangleq r\left(s, a\right)+\beta \cdot \mathbb{E}_{s^\prime \sim P(s, a)}\left[\bar{V}\left(s^\prime\right)\mid s, a\right]\tag{Equation \ref{eq:QandexpV}}
  \\
  &= r\left(s, a\right)+\beta \cdot \mathbb{E}_{s^\prime \sim P(s, a), \boldsymbol{\epsilon}^\prime\sim \boldsymbol{\epsilon} }\left[{V}\left(s^\prime, \boldsymbol{\epsilon}^\prime\right)\mid s, a\right]\notag
  \\
  &=r\left(s, a\right)+\beta \cdot \mathbb{E}_{s^\prime \sim P(s, a),  \boldsymbol{\epsilon}^\prime\sim \boldsymbol{\epsilon}}\left[\max_{a^\prime\in \mathcal{A}} \left(Q\left(s^\prime, a^\prime\right)+\epsilon^\prime_a\right)\mid s, a\right] \label{eq:AnotherQBellman}
  \\
  &=r\left(s, a\right)+\beta \cdot \mathbb{E}_{s^\prime \sim P(s, a), a^\prime \sim \pi^*(\cdot\mid s^\prime)}\left[Q(s^\prime, a^\prime) + \delta + \gamma - \log  \pi^*(a^\prime \mid s^\prime) \mid s, a\right] \makebox[2em][l]{\quad(Corollary \ref{cor:GumbelMaxasProb})} \notag
  \\\label{DDCBellman_new}
\end{align}
where $\pi^*(s, a) = \left(\frac{Q(s, a)}{\sum_{a^{\prime}\in \mathcal{A}}Q(s, a^{\prime})}\right)$.




\subsection{Equivalence between DDC and Entropy regularized IRL}\label{sec:DDCIRLequiv}

Equation \ref{eq:logsumexp_reg}, equation \ref{eq:IRLopt} and equation \ref{eqn:IRLQBellman_new} characterizes the choice-specific value function's Bellman equation and optimal policy in entropy regularized IRL setting when regularizing coefficient is 1: 
$$
    Q(s, a)=r(s, a)+\beta \mathbb{E}_{s^\prime \sim P(s, a)}\left[\ln \left(\sum_{a^{\prime} \in \mathcal{A}} \exp \left(Q\left(s^{\prime},a^{\prime}\right)\right)\right)\mid s, a\right]
$$

$$
\pi^*(a \mid s) =\frac{\exp \left({Q(s, a)}\right)}{\sum_{a^\prime\in \mathcal{A}} \exp \left({Q(s, a^\prime)}\right)} \text{ for } a\in \mathcal{A} 
$$

$$
Q(s,a)=r\left(s, a\right)+\beta \cdot \mathbb{E}_{s^\prime \sim P(s, a), a^\prime \sim \pi^*(\cdot \mid s^\prime)}\left[Q(s^\prime, a^\prime) - \log  \pi^*(a^\prime\mid s^\prime) \mid s, a\right]
$$
\\
Equation \ref{eq:DDCopt}, equation \ref{eq:QBellmanDDC}, and equation \ref{DDCBellman_new} (when $\delta = -\gamma$) characterizes the choice-specific value function's Bellman equation and optimal policy of Dynamic Discrete Choice setting:
$$
    Q(s, a)=r(s, a)+\beta \mathbb{E}_{s^\prime \sim P(s, a)}\left[\ln \left(\sum_{a^{\prime} \in \mathcal{A}} \exp \left(Q\left(s^{\prime}, a^{\prime}\right)\right)\right)\mid s, a\right] 
$$

$$
\pi^*(a \mid s) =\frac{\exp \left({Q(s, a)}\right)}{\sum_{a^\prime\in \mathcal{A}} \exp \left({Q(s, a^\prime)}\right)} \text{ for } a\in \mathcal{A} 
$$

$$
Q(s,a) = r\left(s, a\right)+\beta \cdot \mathbb{E}_{s^\prime \sim P(s, a), a^\prime \sim \pi^*(\cdot\mid s^\prime)}\left[Q(s^\prime, a^\prime) - \log  \pi^*(a^\prime \mid s^\prime) \mid s, a\right]
$$
\\
$Q$ that satisfies \ref{eq:DDCopt} is unique \cite{rust1994structural}, and $Q-r$ forms a one-to-one relationship. Therefore, the exact equivalence between these two setups implies that the same reward function $r$ and discount factor $\beta$ will lead to the same choice-specific value function $Q$ and the same optimal policy for the two problems.


\section{IRL with occupancy matching}\label{sec:occupancy}

\cite{ho2016generative} defines another inverse reinforcement learning problem that is based on the notion of occupancy matching. Let $\nu_0$ be the initial state distribution and $d^\pi$ be the discounted state-action occupancy of $\pi$ which is defined as $ d^\pi=(1-$ $\beta) \sum_{t=0}^{\infty} \beta^t d_t^\pi$, with $d_t^\pi(s, a)=\mathbb{P}_{\pi, \nu_0}\left[s_t=s, a_t=a\right]$. Note that $Q^\pi(s, a):=\mathbb{E}_\pi\left[\sum_{t=0}^{\infty} \beta^t r(s_t,a_t) \mid s_0=s, a_0=a\right] = \sum_{t=0}^{\infty} \beta^t \mathbb{E}_{(\tilde{s},\tilde{a})\sim d_t^\pi}[r(\tilde{s},\tilde{a})\mid s_0=a,a_0=a].$ Defining the discounted state-action occupancy of the expert policy $\pi^\ast$ as $d^\ast$, \cite{ho2016generative} defines the inverse reinforcement learning problem as the following max-min problem:
\begin{align}
    \underset{r \in \mathcal{C}}{\operatorname{max}}\underset{{\pi \in \Pi}}{\min}\left(\mathbb{E}_{d^\ast}[r(s, a)]-\mathbb{E}_{d^\pi}[r(s, a)]-\mathcal{H}(\pi)-\psi(r)\right) \label{eq:occupancyObj}
\end{align}
where $\mathcal{H}$ is the Shannon entropy we used in MaxEnt-IRL formulation and $\psi$ is the regularizer imposed on the reward model $r$.  
\;
\\
\;
\\
Would occupancy matching find $Q$ that satisfies the Bellman equation? Denote the policy as $\pi^\ast$ and its corresponding discounted state-action occupancy measure as $d^\ast=(1-$ $\beta) \sum_{t=0}^{\infty} \beta^t d_t^\ast$, with $d_t^\ast(s, a)=\mathbb{P}_{\pi^\ast, \nu_0}\left[s_t=s, a_t=a\right]$. We define the expert's action-value function as $Q^\ast(s, a):=\mathbb{E}_{\pi^\ast}\left[\sum_{t=0}^{\infty} \beta^t r(s_t,a_t) \mid s_0=s, a_0=a\right]$ and the Bellman operator of $\pi^\ast$ as $\mathcal{T}^\ast$. Then we have the following Lemma \ref{cor:occp=naiveBE} showing that occupancy matching (even without regularization) may not minimize Bellman error for every state and action.
\begin{lem}[Occupancy matching is equivalent to naive weighted Bellman error sum]\label{cor:occp=naiveBE} The perfect occupancy matching given the same $(s_0, a_0)$ satisfies
    $$\mathbb{E}_{(s, a) \sim d^\ast}[r(s, a)\mid s_0, a_0]-\mathbb{E}_{(s, a) \sim d^\pi}[r(s, a)\mid s_0, a_0] = \mathbb{E}_{(s,a)\sim d^{\ast}}[(\mathcal{T}^\ast Q^\pi-Q^\pi)(s,a)\mid s_0, a_0]$$
\end{lem}
\begin{proof} 
   Note that $\mathbb{E}_{(s, a) \sim d^{\ast}}[r(s, a)\mid s_0, a_0]=\sum_{t=0}^{\infty} \beta^t \mathbb{E}_{(s,a)\sim d_t^\ast}[r(s,a)\mid s_0,a_0]=Q^\ast(s, a)$ and $\mathbb{E}_{(s, a) \sim d^{\pi}}[r(s_, a)\mid s_0, a_0]=\sum_{t=0}^{\infty} \beta^t \mathbb{E}_{(s,a)\sim d_t^\pi}[r(s,a)\mid s_0,a_0] = Q^\pi(s, a)$. Therefore
   \begin{align}
       &\mathbb{E}_{(s, a) \sim d^\ast}[r(s, a)\mid s_0,a_0]-\mathbb{E}_{(s, a) \sim d^\pi}[r(s, a)\mid s_0,a_0] =(1-\beta)Q^\ast(s_0, a_0)-(1-\beta)Q^\pi(s_0, a_0)\notag
       \\
       &=(1-\beta)\frac{1}{1-\beta} \mathbb{E}_{(s,a)\sim d^{\ast}}[(\mathcal{T}^\ast Q^\pi-Q^\pi)(s,a)\mid s_0, a_0]\tag{Lemma \ref{lem:telescoping}}
       \\
       &= \mathbb{E}_{(s,a)\sim d^{\ast}}[(\mathcal{T}^\ast Q^\pi-Q^\pi)(s,a)\mid s_0, a_0] \notag
   \end{align}
\end{proof}
\;
\\
Lemma \ref{cor:occp=naiveBE} implies that occupancy measure matching, even without reward regularization, does not necessarily imply Bellman errors being 0 for every state and action. In fact, what they minimize is the \textit{average Bellman error} \cite{jiang2017contextual,uehara2020minimax}. This implies that $r$ cannot be inferred from $Q$ using the Bellman equation after deriving $Q$ using occupancy matching. 



\begin{lem}[Bellman Error Telescoping]\label{lem:telescoping} 
Let the Bellman operator $\mathcal{T}^\pi$ is defined to map $f\in \mathbb{R}^{S\times A}$ to $\mathcal{T}^\pi f := r(s,a) + \mathbb{E}_{s^\prime\sim P(s,a), a^\prime\sim \pi(\cdot\mid s^\prime)}[f(s^\prime,a^\prime)\mid s,a]$. 
For any $\pi$, and any $f\in \mathbb{R}^{S\times A}$,
$$
Q^\pi(s_0, a_0)-f(s_0, a_0) = \frac{1}{1-\beta} \mathbb{E}_{(s,a)\sim d^\pi}[(\mathcal{T}^\pi f-f)(s,a)\mid s_0, a_0].
$$

\end{lem}

\begin{proof}
Note that the right-hand side of the statement can be expanded as
\begin{align}
    &r(s_0, a_0)+\beta \cancel{\mathbb{E}_{s^\prime\sim P(s,a), a^\prime\sim \pi(\cdot\mid s^\prime)}[f(s^\prime,a^\prime)\mid s,a]}-f(s_0,a_0)\notag
    \\
    &+\beta\mathbb{E}_{(s,a)\sim d_1^\pi}\left[r(s, a)+\beta \cancel{\mathbb{E}_{s^\prime\sim P(s,a), a^\prime\sim \pi(\cdot\mid s^\prime)}[f(s^\prime,a^\prime)\mid s,a]}-\cancel{f(s,a)}\mid s_0, a_0\right]\notag
    \\
    &+\beta^2\mathbb{E}_{(s,a)\sim d_2^\pi}\left[r(s, a)+\beta \cancel{\mathbb{E}_{s^\prime\sim P(s,a), a^\prime\sim \pi(\cdot\mid s^\prime)}[f(s^\prime,a^\prime)\mid s,a]}-\cancel{f(s,a)}\mid s_0, a_0\right]\notag
    \\&\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\ldots\notag
    \\
    &=Q^\pi(s_0, a_0)-f(s_0, a_0)\notag
\end{align}
which is the left-hand side of the statement.

\end{proof}

\iffalse

\begin{lem}[Equivalence between Occupancy matching and Behavioral Cloning]
The solution set to the Occupancy matching objective (equation \ref{eq:occupancyObj}) without regularization terms is equivalent to the solution set to the behavioral cloning objective (equation \ref{eq:BC}).
\end{lem}

\begin{proof}
    In proving Lemma \ref{cor:occp=naiveBE}, we saw that $\mathbb{E}_{(s, a) \sim d^\ast}[r(s, a)\mid s_0,a_0]-\mathbb{E}_{(s, a) \sim d^\pi}[r(s, a)\mid s_0,a_0] =(1-\beta)Q^\ast(s_0, a_0)-(1-\beta)Q^\pi(s_0, a_0)$. 

    Now from Lemma \ref{lem:minMLE}, we have 
      \begin{align}
           \underset{Q\in \mathcal{Q}}{\arg\max } &\; \;\mathbb{E}_{(s, a)\sim \pi^*, \nu_0}  \left[\log\left(\hat{p}_{Q}(\;\cdot
    \mid s)\right)\right] \notag
    \\
     &=\left\{Q \in \mathcal{Q} \mid\hat{p}_{Q}(\;\cdot
    \mid s) = \pi^*(\;\cdot
    \mid s)\quad  \forall s\in\bar{\mathcal{S}}\quad\text{a.e.}\right\}\notag
    \\
     &=\left\{Q \in \mathcal{Q} \mid Q(s,a_1)-Q(s,a_2)= Q^*(s,a_1)-Q^*(s,a_2) \quad \forall a_1, a_2\in\mathcal{A}, s\in\bar{\mathcal{S}}\right\} \notag
    \end{align}
This concludes that the solution set to the behavioral cloning objective is equivalent to the occupancy matching objective without the regularization term.
\end{proof}

\fi