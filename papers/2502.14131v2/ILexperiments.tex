\section{Imitation Learning experiments}\label{sec:imitation}
One of the key contributions of this paper is the characterization of the relationship between imitation learning (IL) and inverse reinforcement learning (IRL)/Dynamic Discrete Choice (DDC) model, particularly through the ERM-IRL/DDC framework. Given that much of the IRL literature has historically focused on providing experimental results for IL tasks, we conduct a series of experiments to empirically validate our theoretical findings. Specifically, we aim to test our
prediction in Section \ref{sec:ERM-IRL} that \textit{behavioral cloning (BC) should outperform ERM-IRL for IL tasks}, as BC directly optimizes the negative log-likelihood objective without the additional complexity of Bellman error minimization. By comparing BC and ERM-IRL across various IL benchmark tasks, we demonstrate that BC consistently achieves better performance in terms of both computational efficiency and policy accuracy, reinforcing our claim that IL is a strictly easier problem than IRL.

\subsection{Experimental Setup}

As in \citet{garg2021iq}, we employ three OpenAI Gym environments for algorithms with discrete actions \citep{brockman2016openai}: Lunar Lander v2, Cartpole v1, and Acrobot v1. These environments are widely used in IL and RL research, providing well-defined optimal policies and performance metrics. 

\noindent \textbf{Dataset.}  
For each environment, we generate expert demonstrations using a pre-trained policy. We use publicly available expert policies\footnote{\url{https://huggingface.co/sb3/}} trained via Proximal Policy Optimization (PPO) \cite{schulman2017proximal}, as implemented in the Stable-Baselines3 library \citep{raffin2021stable}. Each expert policy is run to generate demonstration trajectories, and we vary the number of expert trajectories across experiments for training. For all experiments, we used the expert policy demonstration data from 10 episodes for testing.

\noindent \textbf{Performance Metric.}  
The primary evaluation metric is \% optimality, defined as:
\begin{align}
    \text{\% optimality of an episode} := \frac{\text{One episode's episodic reward of the algorithm}}{\text{Mean of 1,000 episodic rewards of the expert}} \times 100. \notag
\end{align}
For each baseline, we report the mean and standard deviation of 100 evaluation episodes after training. A higher \% optimality indicates that the algorithm's policy closely matches the expert. The 1000-episodic mean and standard deviation ([mean$\pm$std]) of the episodic reward of expert policy for each environment was $[232.77\pm73.77]$ for Lunar-Lander v2 (larger the better), $[-82.80\pm27.55]$ for Acrobot v1 (smaller the better), and $[500\pm 0]$ for Cartpole v1 (larger the better).

\noindent \textbf{Training Details.}  
All algorithms were trained for 5,000 epochs. Since our goal in this experiment is to show superiority of BC for IL tasks, we only include ERM-IRL and IQ-learn \cite{garg2021iq} as baselines. Specifically, we exclude baselines such as Rust \citep{rust1987optimal} and ML-IRL \citep{zeng2023understanding}, which require explicit transition probability estimation.

\subsection{Experiment results}


%\noindent \textbf{GLADIUS (Ours)}  
%The ERM-IRL framework minimizes both the negative log-likelihood (NLL) and Bellman error (BE) terms, making it computationally more complex than BC. 

%\noindent \textbf{IQ-Learn} \citep{garg2021iq}  
%A popular \cite{rafailov2024r} IRL method that minimizes an occupancy-matching objective, i.e., it does not enforce Bellman consistency. For details, refer to Section \ref{sec:ImitationID}.

%\noindent \textbf{Behavioral Cloning (BC)}  
%BC minimizes only the NLL term, making it computationally simple and sample-efficient.

Table \ref{fig:OpenAI_gym} presents the \% optimality results for Lunar Lander v2, Cartpole v1, and Acrobot v1. As predicted in our theoretical analysis, BC consistently outperforms ERM-IRL in terms of \% optimality, validating our theoretical claims.


\begin{table*}[ht]
    \centering
    \scalebox{0.85}{
    \begin{tabular}{l
            >{\centering\arraybackslash}p{1.5cm}
            >{\centering\arraybackslash}p{1.5cm}
            >{\centering\arraybackslash}p{1.5cm}
            >{\centering\arraybackslash}p{1.5cm}
            >{\centering\arraybackslash}p{1.5cm}
            >{\centering\arraybackslash}p{1.5cm}
            >{\centering\arraybackslash}p{1.5cm}
            >{\centering\arraybackslash}p{1.5cm}
            >{\centering\arraybackslash}p{1.5cm}}
\toprule
\multirow{4}{*}{\parbox{0.5cm}{Trajs}} 
& \multicolumn{3}{c}{\makecell{\ul{Lunar Lander v2 (\%)} \\ (Larger \% the better)}} 
& \multicolumn{3}{c}{\makecell{\ul{Cartpole v1 (\%)} \\ (Larger \% the better)}} 
& \multicolumn{3}{c}{\makecell{\ul{Acrobot v1 (\%)} \\ (Smaller \% the better)}} \\
\cmidrule(lr){2-4} \cmidrule(lr){5-7} \cmidrule(lr){8-10}
& {\ul{Gladius}} & IQ-learn & BC 
& {\ul{Gladius}} & IQ-learn & BC
& {\ul{Gladius}} & IQ-learn & BC \\
\midrule
1  & \textbf{107.30 }& 83.78 & 103.38   & 100.00  & 100.00  & 100.00     & 103.67  & 103.47  & \textbf{100.56}  \\
    & (10.44)  & (22.25)  & (13.78)   & (0.00)  & (0.00)  & (0.00)   & (32.78)  & (55.44)  & (26.71)  \\
\midrule
3   & \textbf{106.64}  &  102.44 & 104.46 & 100.00  & 100.00  & 100.00 & 102.19  & 101.28  & \textbf{101.25}  \\
    & (11.11)  & (20.66)   & (11.57)  & (0.00)  & (0.00)  & (0.00)  & (22.69)  & (37.51)  & (36.42)  \\
\midrule
7   &  101.10 & 104.91  & \textbf{ 105.99}  & 100.00  & 100.00  & 100.00 & 100.67 & 100.58  &\textbf{98.08} \\
    & (16.28)  & (13.98) &  (10.20) & (0.00)         & (0.00)  & (0.00)  & (22.30)      & (30.09)  &  (24.27)  \\
\midrule
10  & 104.46  & 105.13   & \textbf{ 107.01}   & 100.00  & 100.00  & 100.00  & 99.07  & 101.10 &  \textbf{97.75}\\
    & (13.65)  & (13.83)   & (10.75)  & (0.00)           & (0.00)  &  (0.00) &  (20.58)    & (30.40)  & (16.67)  \\
\midrule
15  & 106.11  &  106.51 & \textbf{107.42}  & 100.00  & 100.00  &100.00 & 96.50  & 95.34  & \textbf{95.33}  \\
    & (10.65)  & (14.10)  & (10.45)  & (0.00)         & (0.00)  &  (0.00)  & (18.53)        & (26.92)  & (15.42)  \\
\bottomrule
\multicolumn{10}{l}{\footnotesize 
Based on 100 episodes for each baseline. Each baseline was trained for 5000 epochs.}
\end{tabular}
    }
\caption{Mean and standard deviation of \% optimality of 100 episodes}
\label{fig:OpenAI_gym}   
\end{table*}

\iffalse

\subsection{Reward transfer experiments}
As we discussed in Section \ref{sec:Intro}, the main advantage of IRL over Imitation Learning (IL) is its premise that the learned rewards can be used for counterfactual simulations. That is, given the rewards we learn from IRL, we would expect that such learned rewards can be useful for RL training \cite{zeng2023understanding}. We use the Lunar Lander v2 from OpenAI Gym for the experiment. Specifically, for each number of expert trajectories we consider ([1, 3, 7, 10, 15]):
\begin{enumerate}[noitemsep]
    \item Train the reward function using GLADIUS for 5000 epochs.
    \item Use the trained reward function  for training a Proximal Policy Optimization (PPO) policy.
\end{enumerate}
As discussed in \cite{zeng2023understanding}, IQ-learn and BC performed no better than randomly generated reward for this task. Therefore, we don't include them for the comparison. 

\begin{table*}[ht]
    \centering
    \scalebox{0.9}{
    \begin{tabular}{l
            >{\centering\arraybackslash}p{1.5cm}
            >{\centering\arraybackslash}p{1.5cm}
            >{\centering\arraybackslash}p{1.5cm}
            >{\centering\arraybackslash}p{1.5cm}
            >{\centering\arraybackslash}p{1.5cm}}
\toprule
\multirow{2}{*}{\parbox{2.5cm}{\centering Lunar Lander v2}} 
& \multicolumn{5}{c}{\textbf{Number of Trajectories (Trajs)}} \\
\cmidrule(lr){2-6}
& \textbf{1} & \textbf{3} & \textbf{7} & \textbf{10} & \textbf{15} \\
\midrule
Mean  & 97.63 & 100.12 & 104.23 & 106.52 &  \\
Std   & (28.15)  & (29.12) & (24.57) &(19.88)  &  \\
\bottomrule
\multicolumn{6}{l}{\footnotesize Based on 1000 episodes for each baseline. Each baseline was trained 5000 epochs.}
\end{tabular}
    }
\caption{Mean and standard deviation of \% optimality for Lunar Lander v2 using GLADIUS}
\label{fig:LunarLander_Gladius}   
\end{table*}

\fi