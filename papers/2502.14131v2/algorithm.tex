\section{GLADIUS: Algorithm for ERM-IRL (ERM-DDC)}
\label{sec:Algorithm} 



\begin{algorithm}
\caption{\textbf{G}radient-based \textbf{L}earning with \textbf{A}scent-\textbf{D}escent for \textbf{I}nverse \textbf{U}tility learning from \textbf{S}amples (GLADIUS)}
\label{alg:estimation1}
\begin{algorithmic}[1]
\Require Offline dataset $\mathcal{D}=\{(s, a, s')\}$, time horizon $T$
\Ensure $\widehat{r}$, $\widehat{Q}$
\State Initialize $Q_{\boldsymbol{\theta_1}}$, $\zeta_{\boldsymbol{\theta_2}}$, $\text{iteration} \gets 1$
\While{$t \leq T$}
    \State Draw batches $B_1, B_2$ from $\mathcal{D}$
    
    \State \textbf{[Ascent Step: Update $ \zeta_{\boldsymbol{\theta_1}}$, fixing $Q_{\boldsymbol{\theta_2}}$ and $V_{\boldsymbol{\theta_2}}$]}
    \State $D_{\boldsymbol{\theta_1}} \gets \sum\limits_{(s, a, s^\prime)\in B_2} 
    \textcolor{orange}{\bigl(V_{\boldsymbol{\theta_2}}(s^\prime) 
    - \zeta_{\boldsymbol{\theta_1}}(s, a)\bigr)^2}$
    \State \textbf{where} $V_{\boldsymbol{\theta}}(s^\prime) := \log \sum_{\tilde{a} \in \mathcal{A}} \exp(Q_{\boldsymbol{\theta}}(s^\prime, \tilde{a}))$
    
    \State $\boldsymbol{\theta_1} \gets \boldsymbol{\theta_1} - \tau_1 \nabla_{\boldsymbol{\theta_1}} D_{\boldsymbol{\theta_1}}$
    
    \State \textbf{[Descent Step: Update $Q_{\boldsymbol{\theta_2}}$ and $V_{\boldsymbol{\theta_2}}$, fixing $\zeta_{\boldsymbol{\theta_1}}$]}
    
    \State $\overline{\mathcal{L}_{NLL}} \gets  \sum\limits_{(s, a, s^\prime)\in B_2}
    \textcolor{blue}{-\log\bigl(\hat{p}_{\boldsymbol{\theta_2}}(a \mid s)\bigr)}$
    
    \State $\overline{\mathcal{L}_{\mathrm{BE}}} \gets \sum\limits_{(s, a, s^\prime)\in B_1} 
    \mathbbm{1}_{a = a^*_s} \bigl[\textcolor{red}{\mathcal{L}_{TD}(Q)\left(s, a, s^{\prime}\right)} - \beta^2 \textcolor{orange}{( V_{\boldsymbol{\theta_2}}(s^\prime)- \zeta_{\boldsymbol{\theta_1}}(s,a))^2}\bigr]$
    
    \State \textbf{where} $\mathcal{L}_{TD}(Q)\left(s, a, s^{\prime}\right) := \left(\hat{\mathcal{T}} Q\left(s, a, s^{\prime}\right)-Q(s, a)\right)^2$
    
    \State $\mathcal{L}_{\boldsymbol{\theta_2}} \gets \overline{\mathcal{L}_{\mathrm{NLL}}} 
    + \lambda \overline{\mathcal{L}_{\mathrm{BE}}}$
    
    \State $\boldsymbol{\theta_2} \gets \boldsymbol{\theta_2} - \tau_1 \nabla_{\boldsymbol{\theta_2}} \mathcal{L}_{\boldsymbol{\theta_2}}$
    
    \State $\text{iteration} \gets \text{iteration} + 1$
\EndWhile
\State $\widehat{\zeta} \gets \zeta_{\boldsymbol{\theta_1}}$
\State $\widehat{Q} \gets Q_{\boldsymbol{\theta_2}}$
\State $\widehat{r}(s, a) \gets \widehat{Q}(s, a) - \beta \cdot \widehat{\zeta} (s, a)$
\end{algorithmic}
\end{algorithm}

\noindent Algorithm \ref{alg:estimation1} solves the empirical risk minimization problem in Definition \ref{def:ERM} through an alternating gradient ascent descent algorithm we call Gradient-based Learning with Ascent-Descent for Inverse Utility learning from Samples (GLADIUS). Given the function class $\mathcal{Q}$ of value functions, let $Q_{\boldsymbol{\theta}_1}\in\mathcal{Q}$ and $\zeta_{\boldsymbol{\theta}_2}\in\mathbb{R}^{S\times A}$ denote the functional representation of $Q$ and $\zeta$. Our goal is to learn the parameters $\boldsymbol{\theta}^*= \{\boldsymbol{\theta}_1^*, \boldsymbol{\theta}_2^* \}$, that together characterize $\hat{Q}$ and $\hat{\zeta}$. Each iteration in the GLADIUS algorithm consists of the following two steps: 
\begin{enumerate}[noitemsep]
    \item Gradient Ascent: Update $\zeta_{\boldsymbol{\theta}_2}$ based on the current value of $Q_{\boldsymbol{\theta}_1}$.
    \item Gradient Descent: Update $\boldsymbol{\theta}_1$ based on the current value of $\zeta_{\boldsymbol{\theta}_2}$.
\end{enumerate}
After a fixed number of gradient steps of $Q_{\boldsymbol{\theta}_1}$ and $\zeta_{\boldsymbol{\theta}_2}$ (which we can denote as $\hat{Q}$ and $\hat{\zeta}$), we can compute the reward prediction $\hat{r}$ as 
$\hat{r}(s,a) = \hat{Q}(s,a) - \beta \hat{\zeta}(s,a)$ due to Theorem \ref{thm:algoEQ}.

\;
\\

\subsection*{Special Case: Deterministic Transitions}

\begin{algorithm}
\caption{GLADIUS under Deterministic Transitions}
\label{alg:estimation_deterministic}
\begin{algorithmic}[1]
\Require Offline dataset $\mathcal{D}=\{(s, a, s')\}$, time horizon $T$
\Ensure $\widehat{r}$, $\widehat{Q}$
\State Initialize $Q_{\boldsymbol{\theta}}$, $\text{iteration} \gets 1$
\While{$t \leq T$}
    \State Draw batch $B$ from $\mathcal{D}$
    
    \State $\overline{\mathcal{L}_{NLL}} \gets  \sum\limits_{(s, a, s^\prime)\in B}
    \textcolor{blue}{-\log\bigl(\hat{p}_{\boldsymbol{\theta}}(a \mid s)\bigr)}$
    
    \State $\overline{\mathcal{L}_{\mathrm{BE}}} \gets \sum\limits_{(s, a, s^\prime)\in B} 
    \mathbbm{1}_{a= a^*_s} \textcolor{red}{\mathcal{L}_{TD}(Q)\left(s, a, s^{\prime}\right)}$
    
    \State \textbf{where} $\mathcal{L}_{TD}(Q)\left(s, a, s^{\prime}\right) := \left(\hat{\mathcal{T}} Q\left(s, a, s^{\prime}\right)-Q(s, a)\right)^2$
    
    \State $\mathcal{L}_{\boldsymbol{\theta}} \gets \overline{\mathcal{L}_{\mathrm{NLL}}} 
    + \lambda \overline{\mathcal{L}_{\mathrm{BE}}}$
    
    \State $\boldsymbol{\theta} \gets \boldsymbol{\theta} - \tau \nabla_{\boldsymbol{\theta}} \mathcal{L}_{\boldsymbol{\theta}}$
    
    \State $\text{iteration} \gets \text{iteration} + 1$
\EndWhile
\State $\widehat{Q} \gets Q_{\boldsymbol{\theta}}$
\State $\widehat{r}(s, a) \gets \widehat{Q}(s, a) - \beta \log \sum_{\tilde{a} \in \mathcal{A}} \exp(\widehat{Q}(s^\prime, \tilde{a}))$
\end{algorithmic}
\end{algorithm}
When the transition function is \textit{deterministic} (e.g., in \citet{rafailov2024r, guo2025deepseek, zhong2024dpo}) meaning that for any state-action pair \((s, a)\), the next state \(s'\) is uniquely determined, the ascent step involving \(\zeta\) is no longer required. This is because the term \(\left(V_{Q}\left(s^{\prime}\right)-\zeta(s,a)\right)^2\) (highlighted in orange in equation \eqref{eq:AlgoOpt} and \eqref{eq:EmpiricalERMIRL}) becomes redundant in the empirical ERM-IRL objective, because $\max_{\zeta \in \mathbb{R}} -\mathbb{E}_{s^{\prime} \sim P(s, a)}\left[\left(V_Q\left(s^{\prime}\right)-\zeta\right)^2 \mid s, a\right]$ is always $0$. Consequently, the optimization simplifies to:

\begin{align}
    \min _{Q \in \mathcal{Q}}&\frac{1}{N} \sum_{(s,a,s^\prime)\in \mathcal{D}}\bigl[\textcolor{blue}{-\log \left(\hat{p}_Q(a \mid s)\right)} + 
   \lambda\mathbbm{1}_{a=a_s}\textcolor{red}{\bigl(\hat{\mathcal{T}} Q\left(s, a, s^{\prime}\right)-Q(s, a)\bigr)^2} \bigr]\label{eq:DeterministicERMIRL}
\end{align}

\noindent Under deterministic transitions, the \textbf{GLADIUS} algorithm reduces to a single gradient descent update step for \( Q_{\boldsymbol{\theta}} \), eliminating the need for the alternating ascent-descent updates. Consequently, the estimated reward function is computed as:
\begin{equation}
    \hat{r}(s,a) = \hat{Q}(s,a) - \beta V_Q(s^\prime) \notag
\end{equation}
\noindent Key Differences in the Deterministic Case:
\begin{itemize}[leftmargin=*]
    \item No Ascent Step: The ascent step for \(\zeta\) is removed since the term \(\left(V_{Q}(s') - \zeta(s,a)\right)^2\) disappears.
    \item Single Gradient Descent: The algorithm updates \(Q_{\boldsymbol{\theta}}\) via a single gradient descent step per iteration.
    \item Reward Computation: The reward function is computed as \(\hat{r}(s, a) = \hat{Q}(s, a) - \beta V_Q(s^\prime)\).
\end{itemize}

\noindent This modification makes GLADIUS computationally more efficient when applied to deterministic environments while maintaining the correct theoretical formulation of the $Q^\ast$ and reward functions.
