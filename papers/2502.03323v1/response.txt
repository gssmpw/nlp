\section{Related Work}
\label{sec:related works}
\vspace{-0.1cm}

\textbf{Detecting OOD data.}
In recent years, there has been a growing interest in OOD detection **Snoek et al., "epistemic uncertainty"**. 
One approach to detect OOD data uses scoring functions to assess data distribution, including:

\vspace{-0.2cm}
\begin{itemize}
    \vspace{-0.2cm}
    \item \textbf{Distance-based methods} **Snoek et al., "epistemic uncertainty"**: 
    These methods compute distances (e.g., Mahalanobis distance or cosine similarity) between a sample and class prototypes in feature space to measure how far a sample is from in-distribution data.
    \vspace{-0.2cm}
    \item \textbf{Energy-based scores} **Li et al., "Training Confidence-calibrated Neural Networks"**: These scores leverage the energy of a sample computed from the logits of a neural network to determine its likelihood of belonging to the in-distribution or OOD set.
    \vspace{-0.2cm}
    \item \textbf{Confidence-based approaches} **Hendrycks et al., "Deep Anomaly Detection"**:
   These rely on model confidence scores (e.g., softmax probabilities) to identify OOD data, often enhanced by techniques like temperature scaling and input perturbation.
    \vspace{-0.2cm}
    \item \textbf{Bayesian methods} **Gal et al., "Uncertainty in Deep Learning"**: 
    They use Bayesian models to quantify uncertainty in model predictions to identify inputs that are significantly different from the training data.
\end{itemize}
\vspace{-0.4cm}
Another approach to OOD detection involves using regularization techniques during the training phase **Srivastava et al., "Regularization of Neural Networks by Enforcing Sparsity"**.
For example, regularization techniques can be applied to the model to either reduce its confidence **Guo et al., "On Calibration of Modern Neural Networks"** or increase its energy **Li et al., "Training Confidence-calibrated Neural Networks"** on the OOD data.
Most of these regularization methods assume the availability of an \emph{additional auxiliary OOD dataset}. 
Several studies **Hendrycks et al., "Deep Anomaly Detection"** relaxed this assumption by either utilizing unlabeled wild data or employing positive-unlabeled learning, which trains classifiers using positive and/or unlabeled data **Li et al., "Training Confidence-calibrated Neural Networks"**. 
These approaches rely on the assumption that such external data is both sufficiently available and representative of real-world OOD scenarios. In practice, real-world OOD inputs are highly diverse and unpredictable, making it difficult to curate datasets that capture all potential distribution shifts; as **Srivastava et al., "Regularization of Neural Networks by Enforcing Sparsity"** highlight, \emph{"...approaches impose a strong assumption on the availability of OOD training data, which can be infeasible in practice."} Practical constraints have led to a shift in recent research toward settings where real OOD data is either unavailable or significantly limited. Unlike these approaches, our synthetic data generation approach completely removes the dependency on external data sources and allows us to create more controlled and flexible test conditions. 


\textbf{Synthetic data.}
Recently, synthetic data has been used for OOD detection in the image domain; **Carmon et al., "On Evaluating Adversarial Robustness"** leverage CLIP ____, a vision-language model, to erase InD regions from training images and then uses a latent diffusion model to replace them with realistic OOD features that blend seamlessly with the image background whereas **Kim et al., "Learning to Detect Out-of-Distribution Data"** generate synthetic image samples by using a variant of CLIP to mix InD features from different classes.
In contrast, we focus on textual data and leverage LLMs to generate high-quality proxies for OOD data that capture the complexities of real-world OOD data. In our work, we explore the efficacy of LLM-generated OOD proxies for OOD detection, an area which remains largely unexplored.
\vspace{-0.2cm}