\section{Related Work}
\label{sec:related works}
\vspace{-0.1cm}

\textbf{Detecting OOD data.}
In recent years, there has been a growing interest in OOD detection \citep{fort2021exploring, yang2024generalized, NEURIPS2022_f0e91b13, galil2023a, djurisic2023extremely, zheng2023out, wang2023outofdistribution, zhu2023diversified, bai2023feed, ming2024does, ghosal2024overcome}. 
One approach to detect OOD data uses scoring functions to assess data distribution, including:

\vspace{-0.2cm}
\begin{itemize}
    \vspace{-0.2cm}
    \item \textbf{Distance-based methods} \citep{NEURIPS2018_abdeb6f5, NEURIPS2020_8965f766, ren2021simple, NEURIPS2022_804dbf8d, ming2023how}: 
    These methods compute distances (e.g., Mahalanobis distance or cosine similarity) between a sample and class prototypes in feature space to measure how far a sample is from in-distribution data.
    \vspace{-0.2cm}
    \item \textbf{Energy-based scores} \citep{NEURIPS2020_f5496252, wu2023energybased}: These scores leverage the energy of a sample computed from the logits of a neural network to determine its likelihood of belonging to the in-distribution or OOD set.
    \vspace{-0.2cm}
    \item \textbf{Confidence-based approaches} \citep{bendale2016towards, hendrycks2017a, liang2018enhancing}:
   These rely on model confidence scores (e.g., softmax probabilities) to identify OOD data, often enhanced by techniques like temperature scaling and input perturbation.
    \vspace{-0.2cm}
    \item \textbf{Bayesian methods} \citep{pmlr-v48-gal16, NIPS2017_9ef2ed4b, NEURIPS2019_7dd2ae7d, Wen2020BatchEnsemble}: 
    They use Bayesian models to quantify uncertainty in model predictions to identify inputs that are significantly different from the training data.
\end{itemize}
\vspace{-0.4cm}
Another approach to OOD detection involves using regularization techniques during the training phase \citep{NEURIPS2018_3ea2db50, pmlr-v97-geifman19a, NEURIPS2020_28e209b6, yang2021semantically, pmlr-v162-wei22d, du2022unknown, NEURIPS2023_bf5311df, NEURIPS2023_e812af67}.
For example, regularization techniques can be applied to the model to either reduce its confidence \citep{lee2017training, hendrycks2018deep} or increase its energy \citep{NEURIPS2020_f5496252, du2022towards, pmlr-v162-ming22a} on the OOD data.
Most of these regularization methods assume the availability of an \emph{additional auxiliary OOD dataset}. 
Several studies \citep{NEURIPS2021_f4334c13, pmlr-v162-katz-samuels22a, he2023topological} relaxed this assumption by either utilizing unlabeled wild data or employing positive-unlabeled learning, which trains classifiers using positive and/or unlabeled data \citep{letouzey2000learning, pmlr-v37-hsiehb15, niu2016theoretical, gong2018margin, NEURIPS2020_1e6e25d9, NEURIPS2021_47b4f1bf, xu2021positive, NEURIPS2022_8d5f526a, du2024how}. 
These approaches rely on the assumption that such external data is both sufficiently available and representative of real-world OOD scenarios. In practice, real-world OOD inputs are highly diverse and unpredictable, making it difficult to curate datasets that capture all potential distribution shifts; as \citet{yang2024generalized} highlight, \emph{"...approaches impose a strong assumption on the availability of OOD training data, which can be infeasible in practice."} Practical constraints have led to a shift in recent research toward settings where real OOD data is either unavailable or significantly limited. Unlike these approaches, our synthetic data generation approach completely removes the dependency on external data sources and allows us to create more controlled and flexible test conditions. 


\textbf{Synthetic data.}
Recently, synthetic data has been used for OOD detection in the image domain;  \citet{kwon2023improving} leverage CLIP \citep{radford2021learning}, a vision-language model, to erase InD regions from training images and then uses a latent diffusion model to replace them with realistic OOD features that blend seamlessly with the image background whereas \citet{sun2024clip} generate synthetic image samples by using a variant of CLIP to mix InD features from different classes.
In contrast, we focus on textual data and leverage LLMs to generate high-quality proxies for OOD data that capture the complexities of real-world OOD data. In our work, we explore the efficacy of LLM-generated OOD proxies for OOD detection, an area which remains largely unexplored.
\vspace{-0.2cm}