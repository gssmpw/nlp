\section{Related Work}
\label{sec:RelatedWork}

\subsection{Wound Infection Classification with Deep Learning}

\begin{table}[ht]
\centering
\caption{Summary of prior work on wound infection classification using deep learning}
\resizebox{\textwidth}{!}{
\begin{tabular}{cccccc}
\hline
\textbf{Specific ML problem} &
  \textbf{Related Work} &
  \textbf{\begin{tabular}[c]{@{}c@{}}Summary of \\ Approach\end{tabular}} &
  \textbf{\begin{tabular}[c]{@{}c@{}}No. of \\ Target Classes\end{tabular}} &
  \textbf{Dataset} &
  \textbf{Results} \\ \hline
\centering
\begin{tabular}[c]{@{}c@{}}Wound segmentation \\ and Infection \\ Classification\end{tabular} &
  \begin{tabular}[c]{@{}c@{}}Wang et al.\\ "Deep Learning-based Wound Segmentation"** **Li, "Deep Learning for Medical Image Analysis"\end{tabular} &
  \begin{tabular}[c]{@{}c@{}}CNN-based: \\ ConvNet + SVM\end{tabular} &
  \begin{tabular}[c]{@{}c@{}}2 classes \\ (infection and\\  no infection)\end{tabular} &
  \begin{tabular}[c]{@{}c@{}}NYU wound \\ Database\end{tabular} &
  \begin{tabular}[c]{@{}c@{}}Accuracy: 95.6\%\\ PPV: 40\%\\ Sensitivity: 31\%\end{tabular} \\ \hline %\cline{1-3} \cline{5-6} 
\centering
 &
  \begin{tabular}[c]{@{}c@{}}Goyal et al.\\ "Part-B DFU Dataset"** **Li, "Deep Learning for Medical Image Analysis"\end{tabular} &
  \begin{tabular}[c]{@{}c@{}}CNN-based: \\ Ensemble CNN\end{tabular} &
  \multirow{3}{*}{\begin{tabular}[c]{@{}c@{}} \\ \\ 2 classes \\ (infection and \\ no infection)\end{tabular}} &
  \multirow{3}{*}{\textit{\begin{tabular}[c]{@{}c@{}} \\ Part B DFU \\  2020 dataset\\ \\ (We also used \\ this dataset)\end{tabular}}} &
  \begin{tabular}[c]{@{}c@{}}Accuracy: 72.7\%\\ PPV: 73.5\%\\ Sensitivity: 70.9\%\end{tabular} \\ \cline{2-3} \cline{6-6} 
\centering
\multirow{-2}{*}{\begin{tabular}[c]{@{}c@{}}DFU infection \\ classification\end{tabular}} &
  \begin{tabular}[c]{@{}c@{}}Al-Garaawi et al.\\ "DFU-RGB-TEX-Net"\end{tabular} &
  \begin{tabular}[c]{@{}c@{}}CNN-based:\\ DFU-RGB-TEX-Net\end{tabular} &
   &
   &
  \begin{tabular}[c]{@{}c@{}}Accuracy: 74.2\%\\ PPV: 74.1\%\\ Sensitivity: 75.1\%\end{tabular} \\ \cline{2-3} \cline{6-6} 
\centering
 &
   \begin{tabular}[c]{@{}c@{}}Busaranuvong et al.\\ "ConDiff Model"** **Saxena, "Generative-Discriminative Models for Medical Image Analysis"\end{tabular} &
  \begin{tabular}[c]{@{}c@{}}Generative-Discrimination:\\ ConDiff (Distance-based)\end{tabular} &
   &
   &
  \begin{tabular}[c]{@{}c@{}} Accuracy: 83.3\%\\ PPV: 85.8\%\\ Sensitivity: 85.8\%\end{tabular} \\ \hline
\centering
\multirow{3}{*}{\begin{tabular}[c]{@{}c@{}}DFU wound ischemia\\ and  infection \\ classification\end{tabular}} &
  \begin{tabular}[c]{@{}c@{}}Yap et al.\\ "DFUC2021 Challenge"** **Li, "Deep Learning for Medical Image Analysis"\end{tabular} &
  \begin{tabular}[c]{@{}c@{}}CNN-based:\\ VGG, ResNet,\\ InceptionV3, DenseNet, \\ EfficientNet\end{tabular} &
  \multirow{3}{*}{\begin{tabular}[c]{@{}c@{}}4 classes\\ (both infection\\ and ischemia,\\ infection, ischemia,\\ none)\end{tabular}} &
  \multirow{3}{*}{\begin{tabular}[c]{@{}c@{}} \\  DFUC2021\\ dataset\end{tabular}} &
  \begin{tabular}[c]{@{}c@{}}EfficientNet B0 \\ performance: \\ F1, PPV, SEN\\ =  55\% , 57\%, 62\%\end{tabular} \\ \cline{2-3} \cline{6-6} 
\centering
 &
  \begin{tabular}[c]{@{}c@{}}Qayyum et al.\\ "ViT Ensemble Model"** **Saxena, "Generative-Discriminative Models for Medical Image Analysis"\end{tabular} &
  \begin{tabular}[c]{@{}c@{}}ViT-based: \\ ViT Ensemble Model\end{tabular} &
   &
   &
  \begin{tabular}[c]{@{}c@{}}Accuracy: 58\%\\ PPV: 61\%\\ Sensitivity: 59\%\end{tabular} \\ \cline{2-6} 
\centering
 &
  \begin{tabular}[c]{@{}c@{}}Galdan et al.\\ "Big Transfer Model"** **Li, "Deep Learning for Medical Image Analysis"\end{tabular} &
  \begin{tabular}[c]{@{}c@{}}ViT-based: \\ Big Transfer (BiT) model\end{tabular} &
   &
   &
  \begin{tabular}[c]{@{}c@{}}Accuracy: 61\%\\ PPV: 58\%\\ Sensitivity: 66\%\end{tabular} \\ \hline
\end{tabular}
\end{table}

\subsection{Medical Visual Question Answering with Multimodal Large Language Models}

LLMs have been explored for their proficiency in medical tasks. Models such as **Med-PaLM** "Med PaLM"**, Med-PaLM2** "Med-PaLM 2"**, and GPT-4** "GPT-4"** achieve impressive accuracies of 67.6\%, 86.5\%, and 90.1\%, respectively on multiple-choice US Medical Licensing Examination (USMLE) questions, well above the exam's approximate passing score of 60\%** **Li, "Large Language Models for Medical Question Answering"**.

Despite these advancements, challenges persist for the Medical Visual Question Answering (medical VQA) task. For example, while Med-PaLM2 excels in text-based analysis, it lacks visual data interpretation capabilities. In contrast, GPT-4o, a Multimodal Large Language Model (MLLM), effectively integrates visual and textual information. **Jin et al.** "Multimodal Large Language Models for Medical Visual Question Answering" shows that GPT-4o achieves an accuracy of 88\% in the New England Journal of Medicine (NEJM) Image Challenge when medical images and clinical information are provided, outperforming the average physician's accuracy of 77\%. This finding is in line with another experiment** **Saxena, "Generative-Discriminative Models for Medical Image Analysis"**, which illustrates that incorporating expert hints into the USMLE with image questions taken from the AMBOSS medical platform increases the accuracy of GPT-4o from 60-68\% to 84-88\%, highlighting its potential for improved medical diagnostic support.

However, GPT-4o's performance drops significantly in the NEJM image challenge scenarios where only medical images are used as inputs, with diagnostic accuracy ranging from 29-40\%, and accuracy around 42-50\% when only providing essential information about the patient, their symptoms, and relevant clinical details** **Li, "Large Language Models for Medical Question Answering"**.  
This highlights a critical gap in its ability to process purely visual information without supporting context from text or other modalities.

In our research, we focus on infection classifications from wound images since prioritizing infection detection is crucial for addressing urgent clinical requirements and enabling timely and appropriate treatment interventions, such as the initiation of antibiotic therapy or surgical procedures. Our paper addresses scenarios in which additional patient clinical information, medical notes, or descriptions corresponding to each DFU image are unavailable. As mentioned above, using GPT-4o to analyze only wound images for infection classification is not recommended. As an alternate strategy, we address GPT-4o's limitations in image-only analysis by incorporating expert labels of DFU images to generate wound descriptions. Later, these descriptions are used for fine-tuning the BLIP image captioning model that generates wound image descriptions without using unavailable expert-assigned labels at test time.