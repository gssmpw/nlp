\section{Related Work}
\label{sec:RelatedWork}

\subsection{Wound Infection Classification with Deep Learning}

\begin{table}[ht]
\centering
\caption{Summary of prior work on wound infection classification using deep learning}
\resizebox{\textwidth}{!}{
\begin{tabular}{cccccc}
\hline
\textbf{Specific ML problem} &
  \textbf{Related Work} &
  \textbf{\begin{tabular}[c]{@{}c@{}}Summary of \\ Approach\end{tabular}} &
  \textbf{\begin{tabular}[c]{@{}c@{}}No. of \\ Target Classes\end{tabular}} &
  \textbf{Dataset} &
  \textbf{Results} \\ \hline
\centering
\begin{tabular}[c]{@{}c@{}}Wound segmentation \\ and Infection \\ Classification\end{tabular} &
  \begin{tabular}[c]{@{}c@{}}Wang et al. \\ 2015 \cite{wang2015unified}\end{tabular} &
  \begin{tabular}[c]{@{}c@{}}CNN-based: \\ ConvNet + SVM\end{tabular} &
  \begin{tabular}[c]{@{}c@{}}2 classes \\ (infection and\\  no infection)\end{tabular} &
  \begin{tabular}[c]{@{}c@{}}NYU wound \\ Database\end{tabular} &
  \begin{tabular}[c]{@{}c@{}}Accuracy: 95.6\%\\ PPV: 40\%\\ Sensitivity: 31\%\end{tabular} \\ \hline %\cline{1-3} \cline{5-6} 
\centering
 &
  \begin{tabular}[c]{@{}c@{}}Goyal et al.\\ 2020 \cite{partb_DFU} \end{tabular} &
  \begin{tabular}[c]{@{}c@{}}CNN-based: \\ Ensemble CNN\end{tabular} &
  \multirow{3}{*}{\begin{tabular}[c]{@{}c@{}} \\ \\ 2 classes \\ (infection and \\ no infection)\end{tabular}} &
  \multirow{3}{*}{\textit{\begin{tabular}[c]{@{}c@{}} \\ Part B DFU \\  2020 dataset\\ \\ (We also used \\ this dataset)\end{tabular}}} &
  \begin{tabular}[c]{@{}c@{}}Accuracy: 72.7\%\\ PPV: 73.5\%\\ Sensitivity: 70.9\%\end{tabular} \\ \cline{2-3} \cline{6-6} 
\centering
\multirow{-2}{*}{\begin{tabular}[c]{@{}c@{}}DFU infection \\ classification\end{tabular}} &
  \begin{tabular}[c]{@{}c@{}}Al-Garaawi et al. \\ 2022 \cite{al2022diabetic} \end{tabular} &
  \begin{tabular}[c]{@{}c@{}}CNN-based:\\ DFU-RGB-TEX-Net\end{tabular} &
   &
   &
  \begin{tabular}[c]{@{}c@{}}Accuracy: 74.2\%\\ PPV: 74.1\%\\ Sensitivity: 75.1\%\end{tabular} \\ \cline{2-3} \cline{6-6} 
\centering
 &
   \begin{tabular}[c]{@{}c@{}}Busaranuvong et al. \\ 2024 \cite{ConDiff} \end{tabular} &
  \begin{tabular}[c]{@{}c@{}}Generative-Discrimination:\\ ConDiff (Distance-based)\end{tabular} &
   &
   &
  \begin{tabular}[c]{@{}c@{}} Accuracy: 83.3\%\\ PPV: 85.8\%\\ Sensitivity: 85.8\%\end{tabular} \\ \hline
\centering
\multirow{3}{*}{\begin{tabular}[c]{@{}c@{}}DFU wound ischemia\\ and  infection \\ classification\end{tabular}} &
  \begin{tabular}[c]{@{}c@{}}Yap et al. \\ 2021 \cite{yap2021analysis} \end{tabular} &
  \begin{tabular}[c]{@{}c@{}}CNN-based:\\ VGG, ResNet,\\ InceptionV3, DenseNet, \\ EfficientNet\end{tabular} &
  \multirow{3}{*}{\begin{tabular}[c]{@{}c@{}}4 classes\\ (both infection\\ and ischemia,\\ infection, ischemia,\\ none)\end{tabular}} &
  \multirow{3}{*}{\begin{tabular}[c]{@{}c@{}} \\  DFUC2021\\ dataset\end{tabular}} &
  \begin{tabular}[c]{@{}c@{}}EfficientNet B0 \\ performance: \\ F1, PPV, SEN\\ =  55\% , 57\%, 62\%\end{tabular} \\ \cline{2-3} \cline{6-6} 
\centering
 &
  \begin{tabular}[c]{@{}c@{}}Qayyum et al. \\ 2021 \cite{qayyum2021vit} \end{tabular} &
  \begin{tabular}[c]{@{}c@{}}ViT-based: \\
  Ensemble ViT \end{tabular} &
   &
   &
  \begin{tabular}[c]{@{}c@{}} F1, PPV, SEN\\ = 57\%, 58\% , 61\%\end{tabular} \\ \cline{2-3} \cline{6-6} 
\centering
 &
  \begin{tabular}[c]{@{}c@{}}Galdran et al. \\ 2021 \cite{galdran2021convolutional} \end{tabular} &
  \begin{tabular}[c]{@{}c@{}}ViT-based: ViT, DeiT 
  \\ CNN-based: BiT, \\ EfficientNet\end{tabular} &
   &
   &
  \begin{tabular}[c]{@{}c@{}}BiT performance:\\ F1, PPV, SEN\\ = 61\%, 61\% , 66\%\end{tabular} \\ \hline
\end{tabular}
}
\label{tab:summary_of_approaches}
\end{table}

State-of-the-art (SOTA) Deep learning models that detect infections from wound images have become increasingly prevalent~\cite{partb_DFU, galdran2021convolutional, yap2021analysis, al2022diabetic}. Goyal et al.~\cite{partb_DFU} introduced the Part-B DFU dataset, which includes wound images for a infection classification task from diabetic foot ulcers. 
As detailed in Table~\ref{tab:summary_of_approaches}, Goyal et al.~\cite{partb_DFU} employed a CNN ensemble model that combines bottleneck features from CNN architectures and classifies using an SVM classifier, achieving 70.9\% sensitivity and 74.4\% specificity in binary infection classification. In subsequent research, Al-Garaawi et al.~\cite{al2022diabetic} developed a custom CNN framework, DFU-RGB-TEX-Net, which enhances feature extraction from DFU images using mapped binary patterns. DFU-RGB-TEX-Net integrates a linear combination of the original image and texture information as input for a CNN, resulting in a sensitivity of 75.1\% and a specificity of 73.4\%.

Busaranuvong et al.~\cite{ConDiff} proposed the ConDiff model for the classification of wound infections. ConDiff uses distance-based classification to predict the wound status based on the similarity between an input image and image-guided conditional synthetic images generated from infection and non-infection labels. ConDiff outperformed other SOTA models achieving 85.4\% sensitivity and 74.7\% specificity on the Part-B DFU infection dataset, demonstrating the potential of distance-based classification of wound imaging tasks. However, the downside of the ConDiff approach is its high computational cost during inference (4-5 seconds per image on an NVIDIA A100 GPU) due to the image-generating time with the diffusion model. 
This work also showed that more recent Vision Transformer (ViT)-based models such as SwinV2~\cite{swinv2} (82.7\% sensitivity and 69.8\% specificity) and EfficientFormer~\cite{efficientformer} (84.1\% sensitivity and 69.2\% specificity) outperformed CNN-based models in wound infection classification.

Galdran et al.~\cite{galdran2021convolutional} and Qayyum et al.~\cite{qayyum2021vit} explored SOTA ViT-based models for multiclass classification of ischemia and infection using the DFUC2021 challenge dataset provided by Yap et al.~\cite{yap2021analysis}. Their findings demonstrated that the performance of ViT-based was comparable to that of traditional CNN-based models on this task. Specifically, a ViT ensemble model~\cite{qayyum2021vit} achieved a sensitivity of 61\% and a positive predictive value (PPV) of 58\%, while the Big Transfer (BiT) model~\cite{galdran2021convolutional} achieved a sensitivity of 66\% and a PPV of 61\%.



\subsection{Medical Visual Question Answering with Multimodal Large Language Models}

LLMs have been explored for their proficiency in medical tasks. Models such as Med-PaLM~\cite{medPALM}, Med-PaLM2~\cite{medPALM2}, and GPT-4~\cite{GPT4_91acc} achieve impressive accuracies of 67.6\%, 86.5\%, and 90.1\%,  respectively on multiple-choice US Medical Licensing Examination (USMLE) questions, well above the exam's approximate passing score of 60\% \cite{usmle_score}.

Despite these advancements, challenges persist for the Medical Visual Question Answering (medical VQA) task. For example, while Med-PaLM2 excels in text-based analysis, it lacks visual data interpretation capabilities. In contrast, GPT-4o, a Multimodal Large Language Model (MLLM), effectively integrates visual and textual information. Jin et. al~\cite{jin2024hidden} shows that GPT-4o achieves an accuracy of 88\% in the New England Journal of Medicine (NEJM) Image Challenge when medical images and clinical information are provided, outperforming the average physician's accuracy of 77\%. This finding is in line with another experiment~\cite{GPT4V_with_hint}, which illustrates that incorporating expert hints into the USMLE with image questions taken from the AMBOSS medical platform increases the accuracy of GPT-4o from 60-68\% to 84-88\%, highlighting its potential for improved medical diagnostic support.

However, GPT-4o's performance drops significantly in the NEJM image challenge scenarios where only medical images are used as inputs, with diagnostic accuracy ranging from 29-40\%, and accuracy around 42-50\% when only providing essential information about the patient, their symptoms, and relevant clinical details~\cite{GPT4V_29acc, GPT4V_61acc}.  
This highlights a critical gap in its ability to process purely visual information without supporting context from text or other modalities.

In our research, we focus on infection classifications from wound images since prioritizing infection detection is crucial for addressing urgent clinical requirements and enabling timely and appropriate treatment interventions, such as the initiation of antibiotic therapy or surgical procedures. Our paper addresses scenarios in which additional patient clinical information, medical notes, or descriptions corresponding to each DFU image are unavailable. As mentioned above, using GPT-4o to analyze only wound images for infection classification is not recommended. As an alternate strategy, we address GPT-4o's limitations in image-only analysis by incorporating expert labels of DFU images to generate wound descriptions. Later, these descriptions are used for fine-tuning the BLIP image captioning model that generates wound image descriptions without using unavailable expert-assigned labels at test time.