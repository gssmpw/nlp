\begin{table*}[]
\resizebox{\textwidth}{!}{%
\begin{tabular}{c|c|c} % 마지막 | 제거
\hline
Model                                   & Parameter                   & Architecture                                                                                         \\ \hline
Qwen2.5-14B                              & 14.7B                        & transformers with RoPE, SwiGLU, RMSNorm, and Attention QKV bias                 \\
Qwen2.5-32B                              & 32B                          & transformers with RoPE, SwiGLU, RMSNorm, and Attention QKV bias                 \\
Llama-3.1-70B                            & 70B                          & auto-regressive language model that uses an optimized transformer architecture. \\
Mistral-Small-Instruct-2409              & 22B                          & Unknown                                                                         \\
GPT-4o-mini, GPT-4o & \multicolumn{1}{l|}{Unknown} & Unknown                                                                         \\ \hline
\end{tabular}%
}
\caption{Specification of models}
\label{sec:llm_spec}
\end{table*}
