\section{Related Works}
\subsection{KG-Based Reasoning with LLM}
\label{sec:related_kg_based}
Research on KG-based reasoning tasks can be broadly categorized into three approaches: embedding-based, semantic parsing-based, and retrieval-augmented **Sun et al., "TransA: A Deep Learning Approach for Knowledge Graph Embedding"**. First, the embedding-based method projects the entities and relations of a KG into an embedding space **Bordes et al., "Translating Embeddings for Modeling Multi-relational Data with Multi-head Attention"**. This approach effectively captures complex relationships and multi-hop connections through vector operations.

Second, the semantic parsing-based method converts the task into a symbolic logic form (\textit{e.g.}, a SPARQL query **Yih et al., "Semantic Parsing via Stochastic Program Interpretation"**) and executes it on the KG to derive the final answer **Khot et al., "Uncertain Inference in Knowledge Graphs with Weighted First-Order Logic"**. This approach has the advantage of handling complex queries, such as multi-hop reasoning, through intuitive queries that can be directly applied to the KG.

Third, the retrieval-augmented method extracts relevant subgraphs from the KG to infer the answers.
% GRAFT-Net **Yang et al., "GRAFT-Net: Deep Heterogeneous Multi-Relational Graph Neural Networks"** employs a convolution-based graph neural network, and the approach by **Zhang et al., "BERT-Based Knowledge Graph Embedding"**, applies a BERT-based model.
%However, these methods are often specific to the KG structure or task, making them less adaptable to changes. When the structure or task configuration is altered, significant modifications to the architecture or time-consuming processes such as additional training are required.
Recent studies have explored using LLMs for both retrieval and reasoning without additional training **Wang et al., "KG-GPT: Knowledge Graph Pre-training with Adversarial Learning"**. KG-GPT **Zhang et al., "KG-GPT: A Framework for Knowledge-Graph-Augmented Text-to-Text Transfer Tasks"**, proposed a three-stage framework: Sentence Segmentation, Graph Retrieval, and Inference. ToG **Xu et al., "ToG: Task-Oriented Graph Generation with Adversarial Training"** and ToG-2 **Xu et al., "ToG-2: A Novel Framework for Task-Oriented Dialogue Systems"**, introduced a framework that conducts reasoning by pruning relations and entities linked to a given entity. While these LLM-based methods enhance the performance of KG-based reasoning, they struggle with adaptability to KG structure or task variations.
%still have limitations, as changes in the KG structure or task types may render the framework unusable or require substantial modifications to the framework structure. 
To overcome these limitations, we propose \modelname, a generally adaptable framework for such variations.

\subsection{Enhancing Model Reliability via Abstention Mechanism}
To mitigate LLM hallucination, the \mbox{\textit{abstention mechanism}} has been adopted as a strategy to enhance reliability **Xu et al., "Rethinking LLM Hallucinations: Abstention as an Alternative"**. This mechanism allows the model to refrain from answering when the input query is ambiguous **Zhu et al., "The Ambiguity of LLM Responses"**, goes against human values **Kumar et al., "Ethics and LLMs: A Framework for Human Values in AI"**, or exceeds the model's knowledge scope **Li et al., "Knowledge Scope Estimation in Large Language Models"**.  
The \textit{abstention mechanism} has been actively explored in LLM-based question-answering tasks, particularly for long-document processing QA **Chen et al., "Long-Doc: A Framework for Long Document Question Answering"** and uncertainty estimation **Srivastava et al., "Uncertainty Estimation in Deep Neural Networks"**, demonstrating notable improvements in reliability.
However, its application in KG-based reasoning remains largely unexplored.
We introduce \textit{Reliable KG-Based Reasoning Task}, the first approach to integrate the \textit{abstention mechanism} into KG-based reasoning.