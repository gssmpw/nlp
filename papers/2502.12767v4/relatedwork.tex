\section{Related Works}
\subsection{KG-Based Reasoning with LLM}
\label{sec:related_kg_based}
Research on KG-based reasoning tasks can be broadly categorized into three approaches: embedding-based, semantic parsing-based, and retrieval-augmented \cite{lan2022complexknowledgebasequestion, ji-etal-2024-retrieval, mavromatis2024gnnraggraphneuralretrieval}. First, the embedding-based method projects the entities and relations of a KG into an embedding space \cite{saxena-etal-2020-improving}. This approach effectively captures complex relationships and multi-hop connections through vector operations.

Second, the semantic parsing-based method converts the task into a symbolic logic form (\textit{e.g.}, a SPARQL query \cite{perez2009semantics}) and executes it on the KG to derive the final answer \cite{Sun_Zhang_Cheng_Qu_2020, park2021knowledge, ye-etal-2022-rng, gu-su-2022-arcaneqa, yu2023decafjointdecodinganswers}. This approach has the advantage of handling complex queries, such as multi-hop reasoning, through intuitive queries that can be directly applied to the KG.

Third, the retrieval-augmented method extracts relevant subgraphs from the KG to infer the answers.
% GRAFT-Net \cite{sun-etal-2018-open} employs a convolution-based graph neural network, and the approach by \citet{10.1145/3442381.3449992}, applies a BERT-based model.
%However, these methods are often specific to the KG structure or task, making them less adaptable to changes. When the structure or task configuration is altered, significant modifications to the architecture or time-consuming processes such as additional training are required.
Recent studies have explored using LLMs for both retrieval and reasoning without additional training \cite{kim-etal-2023-kg, wang2023knowledgedrivencotexploringfaithful, structgpt, li-etal-2023-shot, ToG, ToG2.0}. KG-GPT \cite{kim-etal-2023-kg} proposed a three-stage framework: Sentence Segmentation, Graph Retrieval, and Inference. ToG \cite{ToG} and ToG-2 \cite{ToG2.0} introduced a framework that conducts reasoning by pruning relations and entities linked to a given entity. While these LLM-based methods enhance the performance of KG-based reasoning, they struggle with adaptability to KG structure or task variations.
%still have limitations, as changes in the KG structure or task types may render the framework unusable or require substantial modifications to the framework structure. 
To overcome these limitations, we propose \modelname, a generally adaptable framework for such variations.

\subsection{Enhancing Model Reliability via Abstention Mechanism}
To mitigate LLM hallucination, the \mbox{\textit{abstention mechanism}} has been adopted as a strategy to enhance reliability \cite{wen2024knowlimitssurveyabstention}. This mechanism allows the model to refrain from answering when the input query is ambiguous \cite{asai-choi-2021-challenges, cole-etal-2023-selectively}, goes against human values \cite{kirk-etal-2023-past}, or exceeds the model's knowledge scope \cite{feng-etal-2024-dont}.  
The \textit{abstention mechanism} has been actively explored in LLM-based question-answering tasks, particularly for long-document processing QA \cite{buchmann-etal-2024-attribute} and uncertainty estimation \cite{amayuelas-etal-2024-knowledge, wen-etal-2024-characterizing, yang2024alignmenthonesty, tomani2024uncertaintybasedabstentionllmsimproves}, demonstrating notable improvements in reliability.
However, its application in KG-based reasoning remains largely unexplored.
We introduce \textit{Reliable KG-Based Reasoning Task}, the first approach to integrate the \textit{abstention mechanism} into KG-based reasoning.