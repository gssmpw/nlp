% This must be in the first 5 lines to tell arXiv to use pdfLaTeX, which is strongly recommended.
\pdfoutput=1
% In particular, the hyperref package requires pdfLaTeX to break URLs across lines.

\documentclass[11pt]{article}

% Change "review" to "final" to generate the final (sometimes called camera-ready) version.
% Change to "preprint" to generate a non-anonymous version with page numbers.
\usepackage[preprint]{acl}


% Standard package includes
\usepackage{times}
\usepackage{latexsym}

% For proper rendering and hyphenation of words containing Latin characters (including in bib files)
\usepackage[T1]{fontenc}
% For Vietnamese characters
% \usepackage[T5]{fontenc}
% See https://www.latex-project.org/help/documentation/encguide.pdf for other character sets

% This assumes your files are encoded as UTF8
\usepackage[utf8]{inputenc}

% This is not strictly necessary, and may be commented out,
% but it will improve the layout of the manuscript,
% and will typically save some space.
\usepackage{microtype}

% This is also not strictly necessary, and may be commented out.
% However, it will improve the aesthetics of text in
% the typewriter font.
\usepackage{inconsolata}

%Including images in your LaTeX document requires adding
%additional package(s)
\usepackage{graphicx}

\usepackage{color}

% For table usage
\usepackage{footnote}
\usepackage{tabularx}
\usepackage{booktabs}
\usepackage{adjustbox}
\usepackage{multirow}
\usepackage{makecell}

%Additional packages
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{dsfont}
% \usepackage{algorithm}
\usepackage{algorithmicx}
\usepackage{algpseudocode}
\usepackage[linesnumbered,ruled,vlined]{algorithm2e}
\usepackage{lipsum}  % Dummy text
\usepackage{float}
\usepackage{caption}
\usepackage{algorithm2e} % Improved Algorithm Formatting
\usepackage{bbm}
\usepackage{xcolor}
\usepackage{xspace}
\usepackage{hyperref}
\usepackage[defaultcolor=magenta]{changes}



\newcommand{\modelname}{R2-KG\xspace}
\newcommand{\sag}{\textit{Supervisor}\xspace} 
\newcommand{\oag}{\textit{Operator}\xspace} 

%%%%%equation space
\setlength{\abovedisplayskip}{5pt} % Adjusts space above equations
\setlength{\belowdisplayskip}{5pt} % Adjusts space below equations
\DeclareMathOperator*{\argmax}{arg\,max}


% If the title and author information does not fit in the area allocated, uncomment the following
%
%\setlength\titlebox{<dim>}
%
% and set <dim> to something 5cm or larger.

% \title{\modelname: A General Framework for Reliable Reasoning \\on Knowledge Graphs Using Dual Agent System}

\title{\modelname: General-Purpose Dual-Agent Framework for\\ Reliable Reasoning on Knowledge Graphs}

% Author information can be set in various styles:
% For several authors from the same institution:
% \author{Author 1 \and ... \and Author n \\
%         Address line \\ ... \\ Address line}
% if the names do not fit well on one line use
%         Author 1 \\ {\bf Author 2} \\ ... \\ {\bf Author n} \\
% For authors from different institutions:
% \author{Author 1 \\ Address line \\  ... \\ Address line
%         \And  ... \And
%         Author n \\ Address line \\ ... \\ Address line}
% To start a separate ``row'' of authors use \AND, as in
% \author{Author 1 \\ Address line \\  ... \\ Address line
%         \AND
%         Author 2 \\ Address line \\ ... \\ Address line \And
%         Author 3 \\ Address line \\ ... \\ Address line}

%\author{First Author \\
%  Affiliation / Address line 1 \\
%  Affiliation / Address line 2 \\
%  Affiliation / Address line 3 \\
%  \texttt{email@domain} \\\And
%  Second Author \\
%  Affiliation / Address line 1 \\
%  Affiliation / Address line 2 \\
%  Affiliation / Address line 3 \\
%  \texttt{email@domain} \\}

\author{
  Sumin Jo\thanks{Equal contribution.}, Junseong Choi\footnotemark[1], Jiho Kim, Edward Choi \\
  KAIST \\
  \texttt{\{ekrxjwh2009, quasar0311, jiho.kim, edwardchoi\}@kaist.ac.kr}
}




%\author{
%  \textbf{First Author\textsuperscript{1}},
%  \textbf{Second Author\textsuperscript{1,2}},
%  \textbf{Third T. Author\textsuperscript{1}},
%  \textbf{Fourth Author\textsuperscript{1}},
%\\
%  \textbf{Fifth Author\textsuperscript{1,2}},
%  \textbf{Sixth Author\textsuperscript{1}},
%  \textbf{Seventh Author\textsuperscript{1}},
%  \textbf{Eighth Author \textsuperscript{1,2,3,4}},
%\\
%  \textbf{Ninth Author\textsuperscript{1}},
%  \textbf{Tenth Author\textsuperscript{1}},
%  \textbf{Eleventh E. Author\textsuperscript{1,2,3,4,5}},
%  \textbf{Twelfth Author\textsuperscript{1}},
%\\
%  \textbf{Thirteenth Author\textsuperscript{3}},
%  \textbf{Fourteenth F. Author\textsuperscript{2,4}},
%  \textbf{Fifteenth Author\textsuperscript{1}},
%  \textbf{Sixteenth Author\textsuperscript{1}},
%\\
%  \textbf{Seventeenth S. Author\textsuperscript{4,5}},
%  \textbf{Eighteenth Author\textsuperscript{3,4}},
%  \textbf{Nineteenth N. Author\textsuperscript{2,5}},
%  \textbf{Twentieth Author\textsuperscript{1}}
%\\
%\\
%  \textsuperscript{1}Affiliation 1,
%  \textsuperscript{2}Affiliation 2,
%  \textsuperscript{3}Affiliation 3,
%  \textsuperscript{4}Affiliation 4,
%  \textsuperscript{5}Affiliation 5
%\\
%  \small{
%    \textbf{Correspondence:} \href{mailto:email@domain}{email@domain}
%  }
%}

\begin{document}
\maketitle
\begin{abstract}
Recent studies have combined Large Language Models (LLMs) with Knowledge Graphs (KGs) to enhance reasoning, improving inference accuracy without additional training while mitigating hallucination. 
However, existing frameworks are often rigid, struggling to adapt to KG or task changes. They also rely heavily on powerful LLMs for reliable (\textit{i.e.} trustworthy) reasoning.
To address this, We introduce \textit{\modelname}, a plug-and-play, dual-agent framework that separates reasoning into two roles: an \oag (a low-capacity LLM) that gathers evidence and a \sag (a high-capacity LLM) that makes final judgments.
This design is cost-efficient for LLM inference while still maintaining strong reasoning accuracy.
Additionally, \modelname employs an \textit{Abstention mechanism}, generating answers only when sufficient evidence is collected from KG, which significantly enhances reliability.
Experiments across multiple KG-based reasoning tasks show that \modelname consistently outperforms baselines in both accuracy and reliability, regardless of the inherent capability of LLMs used as the \oag. Further experiments reveal that the single-agent version of \modelname, equipped with a strict self-consistency strategy, achieves significantly higher-than-baseline reliability while reducing inference cost. However, it also leads to a higher abstention rate in complex KGs. Our findings establish \modelname as a flexible and cost-effective solution for KG-based reasoning. It reduces reliance on high-capacity LLMs while ensuring trustworthy inference. The code is available at \url{https://github.com/ekrxjwh2009/R2-KG.git}.

\end{abstract}





\section{Introduction}
Recent studies have increasingly integrated Large Language Models (LLMs) with Knowledge Graphs (KGs) to perform knowledge-grounded reasoning \cite{generateOnGraph, kim2024causalreasoninglargelanguage, gao2024twostagegenerativequestionanswering, luo2024graphconstrainedreasoningfaithfulreasoning, ma2024debategraphflexiblereliable, reasoningefficientknowledgepathsknowledge}.
This approach maximizes reasoning performance by combining the domain-specific knowledge of KGs with the strong reasoning abilities of LLMs \cite{KG_survey1, zhu2024llmsknowledgegraphconstruction}.

However, existing LLM-based frameworks are often tailored to specific KGs and tasks, limiting their generalizability \cite{kim-etal-2023-kg,ToG,ToG2.0}.
These frameworks struggle when the KG changes (\textit{e.g.}, DBpedia \cite{lehmann2015dbpedia} $\rightarrow$ Freebase \cite{10.1145/1376616.1376746}) or when a new task is introduced (\textit{e.g.}, question answering $\rightarrow$ fact verification). 
% Although recent frameworks have been designed to work with arbitrary KGs, they remain specialized for specific tasks.
Furthermore, these approaches rely on a single LLM to handle the entire reasoning process, including subgraph retrieval and answer generation. 
%\cite{KG-agent2024, }
As a result, performance is constrained by the reasoning capabilities of the chosen LLM.
These limitations underscore the need for a generalizable and cost-efficient framework that remains independent of the KG structure and task type while reducing the reliance on powerful LLMs.


\begin{figure}[t]
    \centering
    \includegraphics[width=\linewidth]{figures/simpleframe.pdf}
    \caption{\modelname: The two agents provide an `Answer' only when they are confident enough to do so. If multiple attempts at exploration fail to gather sufficient information, it determines that it does not know and abstains from answering.}
    \label{fig:enter-label}
\end{figure}



To address this, we propose \textit{\modelname}, a novel framework where two agents—the \oag and the \sag{}—collaborate on KG-based reasoning tasks. 
The \oag explores the KG, which consists of numerous triple sets, identifying relevant paths from a given entity, and retrieving necessary information. It iteratively selects and expands linked triples [\textit{head\_entity,  relation, tail\_entity}].
Once the \oag deems the evidence sufficient, it calls the \sag, which evaluates the retrieved information. 
If inadequate, the \sag directs the \oag further exploration;
otherwise, it generates the final answer.
If sufficient evidence remains unavailable even after multiple iterations, the \modelname abstains from answering (\textit{i.e., abstention mechanism}). Our contributions are as follows:

\textbf{(1) Low/High-Capacity LLM Separation for Accuracy and Cost Efficiency}—The \oag, responsible for KG exploration, employs a low-capacity LLM, while the \sag, responsible for verification, employs a high-capacity LLM.
This separation improves reasoning performance while significantly reducing overall LLM cost.
In simpler evidence collection processes, the \oag alone can be used to minimize inference cost, while the \sag is leveraged only for final answer generation, utilizing its superior reasoning capabilities. 

\textbf{(2) KG \& Task-Agnostic Design with Superior Performance}—With its modular design, \modelname operates independently of specific KG structures or task types. To validate its effectiveness, we evaluate it on four KG-based reasoning benchmarks, covering fact verification \cite{kim-etal-2023-factkg}, single-label QA, multi-label QA \cite{webqsp,metaQA}, and temporal QA \cite{cronQA}.
Results show that \modelname outperforms baselines equipped with the \textit{abstention mechanism}. Notably, it achieves a 100\% hit rate on MetaQA and improves micro F1 score by up to 87.8\% over the baseline. Since micro F1 accounts for both precision and recall, this gain highlights the effectiveness of \modelname in multi-label QA and fact verification, demonstrating its adaptability to diverse reasoning tasks.


\textbf{(3) Reliable KG-Based Reasoning Task}—Reliability in KG-based reasoning is crucial, yet existing tasks focus only on accuracy. We introduce the \textit{Reliable KG-Based Reasoning Task}, which evaluates whether a framework can abstain when evidence is insufficient, ensuring trustworthiness in critical applications. To measure reliability, we employ various metrics: coverage (\textit{i.e.,} the proportion of R2-KG generated final answer), as well as F1 scores and hit rate when it participates. 

%This modular design makes <model> a general-purpose framework independent of specific KG structures or task types.

%Moreover, <model>’s \textit{abstention mechanism} enhances reliability by preventing incorrect answer generation when evidence is insufficient.

%\textbf{(2) KG \& Task-Agnostic Design with Superior Performance}—Thanks to its modular design, <model> functions as a versatile framework that remains independent of specific KG structures or task types. To validate its effectiveness, we evaluate <model> using four KG-based reasoning benchmarks that cover diverse tasks, including fact verification \cite{kim-etal-2023-factkg}, single-label QA, multi-label QA \cite{webqsp,metaQA}, and temporal QA \cite{cronQA}.
%The experiment results demonstrate that <model> significantly outperforms baselines also equipped with the \textit{abstention mechanism}. Notably, <model> achieves a Hit rate of \textit{100\%} on MetaQA benchmark(\cite{metaQA}) and improves Micro F1 score by up to \textit{87.8\%} compared to the baseline.
%While Hit rate only requires one correct label to match the ground truth, Micro F1 score is more challenging to improve, as it accounts for precision and recall across all predicted labels.
%The significant gain in Micro F1 score demonstrates that <model> not only excels in single-label QA but also effectively handles multi-label QA and fact verification, highlighting its adaptability to diverse reasoning tasks.




\textbf{(4) Consistent Reliability with Abstention Mechanism}—Furthermore, we analyze the performance of \modelname using various low-capacity LLMs as the \oag against baselines that rely on high-capacity LLMs throughout the entire process.
The results show that \modelname consistently achieves higher F1 scores and hit rates, with only the abstention rate varying. This highlights that \modelname maintains cost-efficient advantage, reducing dependency on expensive high-capacity LLMs without compromising reliability (\textit{i.e.}, whether we can trust the answer of \modelname).   

\textbf{(5) Single-Agent Version with Strict Self-Consistency Strategy for Further Cost Savings}—Additionally, we propose an even more cost-efficient method that does not require high-capacity LLMs as the \sag (\textit{i.e.}, single-agent version of \modelname combined with strict self-consistency strategy \cite{wang2023selfconsistencyimproveschainthought}). Here, the \oag alone conducts reasoning while enforcing a unanimous agreement criterion across multiple trials, ensuring high answer reliability without a high-capacity LLM. While this approach significantly reduces inference costs, it comes with a trade-off of a higher abstention rate, particularly in complex KGs with temporal information.


\begin{figure*}[ht]
    \centering
    \includegraphics[width=\textwidth]{figures/framework.pdf}
    \caption{ R2-KG solves multi-hop problems through an iterative process. \oag-\textit{Server} interactions occurred at iteration steps \(k=\)[1,2,3,4,5,7], while \oag-\sag interactions took place at \(k=\)[6,8]. The invocation of helper functions at each step is solely determined by the operator. If \(k > T(iteration \,limit)\), then system automatically returns \textit{abstention}.
    \textcolor{red}{Bordered circles} and \textcolor{red}{lines} indicate the specific entities and relations requested by the \oag through helper functions. \textbf{Bold} and \underline{underlined} represents the values extracted from the KG by the \textit{Server}. \(G_t\) is collected Triple sets, \(R_t\) is collected relation list.}
    \label{fig:enter-label}
\end{figure*}

\section{Related Works}

\subsection{KG-Based Reasoning with LLM}
\label{sec:related_kg_based}
Research on KG-based reasoning tasks can be broadly categorized into three approaches: embedding-based, semantic parsing-based, and retrieval-augmented \cite{lan2022complexknowledgebasequestion, ji-etal-2024-retrieval, mavromatis2024gnnraggraphneuralretrieval}. First, the embedding-based method projects the entities and relations of a KG into an embedding space \cite{saxena-etal-2020-improving}. This approach effectively captures complex relationships and multi-hop connections through vector operations.

Second, the semantic parsing-based method converts the task into a symbolic logic form (\textit{e.g.}, a SPARQL query \cite{perez2009semantics}) and executes it on the KG to derive the final answer \cite{Sun_Zhang_Cheng_Qu_2020, park2021knowledge, ye-etal-2022-rng, gu-su-2022-arcaneqa, yu2023decafjointdecodinganswers}. This approach has the advantage of handling complex queries, such as multi-hop reasoning, through intuitive queries that can be directly applied to the KG.

Third, the retrieval-augmented method extracts relevant subgraphs from the KG to infer the answers.
% GRAFT-Net \cite{sun-etal-2018-open} employs a convolution-based graph neural network, and the approach by \citet{10.1145/3442381.3449992}, applies a BERT-based model.
%However, these methods are often specific to the KG structure or task, making them less adaptable to changes. When the structure or task configuration is altered, significant modifications to the architecture or time-consuming processes such as additional training are required.
Recent studies have explored using LLMs for both retrieval and reasoning without additional training \cite{kim-etal-2023-kg, wang2023knowledgedrivencotexploringfaithful, structgpt, li-etal-2023-shot, ToG, ToG2.0}. KG-GPT \cite{kim-etal-2023-kg} proposed a three-stage framework: Sentence Segmentation, Graph Retrieval, and Inference. ToG \cite{ToG} and ToG-2 \cite{ToG2.0} introduced a framework that conducts reasoning by pruning relations and entities linked to a given entity. While these LLM-based methods enhance the performance of KG-based reasoning, they struggle with adaptability to KG structure or task variations.
%still have limitations, as changes in the KG structure or task types may render the framework unusable or require substantial modifications to the framework structure. 
To overcome these limitations, we propose \modelname, a generally adaptable framework for such variations.

\subsection{Enhancing Model Reliability via Abstention Mechanism}
To mitigate LLM hallucination, the \mbox{\textit{abstention mechanism}} has been adopted as a strategy to enhance reliability \cite{wen2024knowlimitssurveyabstention}. This mechanism allows the model to refrain from answering when the input query is ambiguous \cite{asai-choi-2021-challenges, cole-etal-2023-selectively}, goes against human values \cite{kirk-etal-2023-past}, or exceeds the model's knowledge scope \cite{feng-etal-2024-dont}.  
The \textit{abstention mechanism} has been actively explored in LLM-based question-answering tasks, particularly for long-document processing QA \cite{buchmann-etal-2024-attribute} and uncertainty estimation \cite{amayuelas-etal-2024-knowledge, wen-etal-2024-characterizing, yang2024alignmenthonesty, tomani2024uncertaintybasedabstentionllmsimproves}, demonstrating notable improvements in reliability.
However, its application in KG-based reasoning remains largely unexplored.
We introduce \textit{Reliable KG-Based Reasoning Task}, the first approach to integrate the \textit{abstention mechanism} into KG-based reasoning.



\section{Reliable KG-Based Reasoning Task}
\subsection{Task Definition}
In this study, we propose the \textit{Reliable KG-Based Reasoning Task} for the first time.
This task serves as a benchmark for measuring reliability in KG-based reasoning, particularly in domains where trustworthy answers are critical, such as industrial applications and fact verification that utilize KGs.
By evaluating reliability, this enables the selection of an appropriate framework based on the specific context. 
Unlike existing KG-based reasoning tasks that focus on generating a definitive answer \textit{a} (\textit{e.g.}, True / False in fact verification or a direct response in QA) for a given query \textit{q} (\textit{e.g.}, a query in fact verification or a question in QA), our task introduces the option to \textit{abstain} when uncertainty arises.
This allows the system to either withhold a response when sufficient evidence cannot be retrieved from the KG or avoid providing an unreliable answer based on ambiguous evidence.




\subsection{Metrics}
To evaluate the KG-based reasoning task incorporating the \textit{abstention mechanism}, we measure four key metrics:

\textbf{Coverage}: The fraction of samples for which a final answer is generated (\textit{i.e.}, the ratio of non-abstained samples). 
\begin{equation}
\small
   \text{Coverage} = \frac{|\mathcal{S}|}{|N|}
\nonumber
\end{equation}
where \(\mathcal{S}\) denotes the set of non-abstained samples, and \(N\) represents the set of all samples, including abstained and non-abstained cases.
    
% \begin{equation}
% \small
%    \text{Coverage} = \frac{|\mathcal{S}|}{|N|}
% \nonumber
% \end{equation}
    

\textbf{Micro F1 Score}: Computed on $\mathcal{S}$ in multi-label tasks using \(TP_i, FP_i, FN_i\), which represent the True Positives, False Positives, and False Negatives for each sample \(i\), respectively.
    \input{tables/MicroF1}
%-->It calculates the harmonic mean of precision and recall by aggregating true positives, false positives, and %false negatives across all classes before computing the metric. 

\textbf{Samplewise F1 Score}: Calculated on $\mathcal{S}$ in multi-label tasks by computing F1 score for each sample and averaging over \(\mathcal{S}\).
    \input{tables/sample-wise-F1}

    
\textbf{Hit Rate}: Applicable to both single-label and multi-label tasks. It is counted if any predicted label matches a ground-truth label. Note that the hit rate is the accuracy in binary tasks.
\begin{equation}
\small
\text{Hit rate} = \frac{1}{|\mathcal{S}|} \sum_{i \in \mathcal{S}} \mathds{1}(\hat{y}_i \in Y_i)
\label{eq:hit_rate}
\nonumber
\end{equation}
Where \(\mathds{1}(\cdot)\) is the indicator function, \(\hat{y}_i\) is one of the framework’s predicted label for sample \(i\) and \( Y_i\) is the set of ground truth labels for sample \(i\).




\section{Method}
Our \modelname consists of three components: An \oag, which explores the KG via helper functions; a \textit{Server}, which provides requested function output; and a \sag, which offers feedback or generates the final answer.
Within an iteration limit \textit{T}, the three components iteratively interacts, gathering triples \(G_t\) or relations \(R_t\), at each step \(t\). The \sag outputs the final answer once sufficient evidence is collected. If no answer is produced within \textit{T}, the system returns an \textit{Abstention}, indicating insufficient understanding of the query.





\subsection{Operator}

By leveraging helper functions (described below), the system retrieves relevant subgraphs from the KG. When the \oag requests a function call, the \textit{Server} responds, and their interactions are stored for future reference.
At each step \textit{t}, the \oag determines the next exploration path based on the accumulated interaction history.

For multi-hop reasoning, \modelname iteratively expands the subgraphs by accumulating relevant triples.
Given a query where entity $e_{0}$ and $e_{n}$ are connected through \(n\)-hops, the intermediate entities are unknown.
At an arbitrary step \(k\), the \oag maintains $E^{(t=k)}_{seen} =\{ e_0, \dots,e_{m-1}, e_m \}$, which is the set of entities explored up to the previous step, where \(E_{seen}^{(t=0)}=\left\{e^{0}\right\}\). Each \(e_i \in E_{seen}\) is associated with relations $R(e_i) = \{ r_{i(1)}, r_{i(2)}, \dots, r_{i(n)} \}$.
In the next step, \oag selects a relevant \(e^* \in E_{seen} \) and one or more relevant relations $R^{*} \subseteq R(e^{*})$, retrieves the corresponding tail entities, and get a new triple set: $\{(e^*, r^{*}, e_{m+1}) \mid r^{*} \in R^{*} \}$. This process continues until \(e_{m+1}\) matches \(e_{n}\).

By structuring reasoning in this way, \modelname ensures that each step builds upon previously acquired knowledge, improving both exploration efficiency and reasoning accuracy. The \oag can invoke one or more following helper functions at each step \(t\) to facilitate KG exploration:


\textbf{GetRelation}(\( e^{*} \)): The \textit{Server} returns all relations \(R(e^{*}) \) connected to $e^*$ in the KG as follows:\\[3pt]
\begin{equation}
\small
\begin{aligned}
    e^{*} &= \argmax_{e \in E_{seen}} EntScore(e, q) \\ 
    R(e^{*}) &= \left\{ r_{i} \mid (e^{*},r_{i}, e_{j})\in KG, \forall e_{j}   \right\} \nonumber
\end{aligned}
\label{eq:label}    
\end{equation}

 The \oag selects \(e^{*}\) that is most relevant to \textit{q} among \(E_{seen}\) using \(EntScore(e, q)\), which is a function that evaluates the relevance between \(e\) and \textit{q}.
 Note that \(EntScore(\cdot)\) is based not on an explicit implementation but on the inherent language understanding of the \oag. 


\textbf{ExploreKG}\( (e^{*}, R^{*}(e^{*})) \): The \textit{Server} returns \( G(e^{*}, R^{*}(e^*))\), a set of all triples such that \(e^{*} \in E_{seen}\) is connected to a tail entity(\(e_j\)) via the relation \(r_i \in R^*(e^*)\). Note that \( R^*(e^{*})\) is a subset of \( R(e^{*})\), which is returned by \textit{GetRelation()} chosen by \textit{RelScore()} as below:

\begin{equation}
\small
\begin{array}{c}
    R^*(e^{*}) = \left\{ r \mid r \in R(e^{*}), \; RelScore(r, q) > \text{threshold} \right\} 
    \\[5pt]
    G(e^{*}, R^{*}(e^*)) = \left\{ (e^{*}, r_{i}, e_{j}) \mid r_i \in R^{*}(e^*), \; e_{j} \in KG \right\} \nonumber
\end{array}
\label{eq:your_label}
\end{equation}


\noindent\(RelScore(r, q)\) evaluates the relevance between \(r\) and \(q\) based on the inherent language understanding of the \oag.
Along with the threshold, it is implicitly applied during the \oag's linguistic reasoning process to select several relations relevant to \(q\). 

\textbf{Verification}(\(G_{k}\), \(R_{k}\)): If the collected evidence is deemed sufficient, \oag invokes the \sag. The \oag provides the triple set \(G_{k}\) and relations \(R_{k}\) gathered up to the current step \(k (<T)\) to the \sag. If the \sag gives back an answer, the process terminates; otherwise, if feedback is given, the next iteration continues.\\
\begin{equation}
\small
\begin{aligned}
R_{\text{k}} = \bigcup\limits_{t=1}^{k} R_{t}(e^{*}), \quad
G_{\text{k}} = \bigcup\limits_{t=1}^{k} G_{t}(e^{*}, R^{*}(e^{*}))
\nonumber
\label{eq:label}
\end{aligned}
\end{equation}
%where \( k \) < \(T\). 
%\(R_t(e^*)\) is \(R(e)\) taken at \(t\) and \(  G_{t}(e^{*}, R^{*}(e^{*}))  \) is \(G(e^{*}, R^{*}(e^*))\) taken at \(t\).



\subsection{Supervisor}
The \sag performs its role only when the \oag invokes \textit{Verification(\(G_{k}\), \(R_{k}\))}. Upon invocation, the \sag receives the $G_k$ and $R_k$ and returns one of two possible outcomes to the \oag:\\
\textbf{1) Sufficient Evidence (answer):} If sufficient information is available, the \sag generates a prediction and returns it to the \oag. The final reasoning path\footnote{You can find the example of final reasoning path of \sag from Appendix~\ref{sec:finalgoldpath}} optimized for answer generation is constructed by the \sag based on its judgment, using \(G_k\).\\
\textbf{2) Insufficient Evidence:} If the information is deemed insufficient to make a judgment, feedback is provided to the \oag to collect more information. Based on $G_k$, $R_k$, and the $q$, the \sag advises which entity and relation combinations should be further explored\footnote{You can find the example of \sag's feedback for \oag in Appendix~\ref{sec:qualitative_analysis}}.



\subsection{Configurable Iteration Limit}

During KG exploration, \modelname requires at least two iterations to traverse a single hop and retrieve information about a new node.
This is because calls to \textit{GetRelation(\(\cdot\))} and \textit{ExploreKG(\(\cdot\))} must occur in sequence, ensuring that relevant entity and relation are gathered step by step.
Therefore, if a \(q\) involves \(N\) hops, it is recommended to set $T$ to at least \(2N\). \textit{T} serves as a hyperparameter, allowing users to adjust the level of reliability they seek from the framework. A lower $T$ increases the rate of \textit{Abstain} samples, while a higher $T$ reduces this rate.



\section{Experiments}

\subsection{Datasets}

%To demonstrate that \modelname is a plug-and-play approach independent of task and KG variation, we use four challenging benchmarks with diverse query difficulty, KG structures, and task formats. Table~\ref{tab:data-statistics} shows the features and statistics of the dataset we used. WebQuestionsSP (WebQSP) \cite{webqsp} is a multi-label QA dataset based on Freebase \cite{10.1145/1376616.1376746}. MetaQA \cite{metaQA} is a movie-related KGQA dataset with 1-hop, 2-hop, and 3-hop questions; we focus on the most challenging 3-hop task\footnote{MetaQA 1-hop and 2-hop tasks are covered in Appendix~\ref{sec:metaqa_12hop}}. CRONQUESTIONS \cite{cronQA} is a temporal reasoning benchmark based on a subset of Wikidata. We use three question types (\textit{i.e.}, simple time, simple entity, time join), excluding others due to missing labels (details in Appendix~\ref{sec:cronq_excluded}). FactKG \cite{kim-etal-2023-factkg} is a fact verification dataset from DBpedia \cite{lehmann2015dbpedia}, with five reasoning types (\textit{i.e.}, one-hop, conjunction, existence, multi-hop, negation) with binary True / False labels. To reduce computational costs, we sample 1,000–1,500 instances from large test sets\footnote{Full-dataset experiments employing GPT-4o mini for both agents are provided in Appendix~\ref{sec:all_dataset}}. 

To demonstrate that \modelname is a plug-and-play approach independent of task and KG variation, we use four challenging benchmarks with diverse query difficulty, KG structures, and task formats. Table~\ref{tab:data-statistics} shows the features and statistics of the dataset we used.
WebQSP \cite{webqsp} is a QA dataset made by semantic parsing, MetaQA \cite{metaQA} dataset has 1-hop, 2-hop, and 3-hop questions, we focus on most challenging 3-hop task\footnote{MetaQA 1-hop and 2-hop tasks are covered in Appendix~\ref{sec:metaqa_12hop}}.  
CRONQUESTIONS \cite{cronQA} is a temporal reasoning benchmark, we used three question types (\textit{i.e.}, simple time, simple entity, time join), excluding others due to missing labels (details in Appendix~\ref{sec:cronq_excluded}).  
FactKG\cite{kim-etal-2023-factkg} has five reasoning types (\textit{i.e.}, one-hop, conjunction, existence, multi-hop, negation). To reduce computational costs, we sample 1,000–1,500 instances from large test sets\footnote{Full-dataset experiments employing GPT-4o mini for both agents are provided in Appendix~\ref{sec:all_dataset}}. 


\input{tables/data-statistic}


\subsection{Baselines}

For comparison, we set KG-GPT \cite{kim-etal-2023-kg}, and ToG \cite{ToG} as baselines, as both can handle various KG structures and tasks to some extent.
KG-GPT is a general framework adaptable for fact verification and QA tasks. However, it does not explicitly incorporate an \textit{abstention mechanism}, therefore we account for implicit \textit{Abstention} when it is unable to generate an answer due to token length constraints or formatting issues. Additionally, due to the structural modifications required to adapt KG-GPT for WebQSP, we did not conduct experiments on this dataset.
ToG is a framework where a single LLM both explores the KG and generates answers. When ToG exceeds the depth limit (\textit{i.e.}, hop limit, hyperparameter used in ToG), it relies on the LLM’s parametric knowledge to generate answers, which we treat as \textit{Abstention}. However, we could not conduct an experiment for CRONQUESTIONS because ToG is designed to handle only triple sets. Additionally, we assess GPT-4o mini’s ability to generate answers without KG access. Since its outputs may not always match dataset labels exactly, we consider a prediction correct if it conveys the same conceptual meaning as the ground truth (\textit{e.g.}, treat \textit{America} as equivalent to \textit{USA}). For details on the modifications made to baselines, refer to Appendix ~\ref{sec:baseline_setting}.


\subsection{Experimental Setting} \label{subsec:experiment_setting}

For the \oag, we use six LLMs. We employ GPT-4o mini and GPT-4o \cite{openaigpt4omini, openaigpt4o} as API-based models, and LLaMA-3.1-70B-Instruct \cite{grattafiori2024llama3herdmodels}, Mistral-Small-Instruct-2409 \cite{mistral2023}, Qwen2.5-32B-Instruct, and Qwen2.5-14B-Instruct \cite{qwen2025qwen25technicalreport} as open-source LLMs.  
The maximum token length was set to $8,192$ for CRONQUESTIONS and FactKG, and $16,384$ for MetaQA and WebQSP.
Top-p and temperature were both set to $0.95$.
For the \sag, we use GPT-4o.
In the main experiment, \textit{T} was set to 15.
All experiments were conducted on a system equipped with two NVIDIA A100 GPUs and four NVIDIA RTX A6000 GPUs. Check models' spec in Appnedix~\ref{sec:model_spec}.



\section{Main Results}
\input{tables/2agent_main_result}

\subsection{Performance of \modelname}
As shown in Table~\ref{tab:2agent_Operator_result}, \modelname significantly outperforms baselines in terms of F1 score across all four benchmarks. Even when using different low-capacity LLMs as the \oag, \modelname achieves higher scores than ToG and KG-GPT that use GPT-4o. Additionally, \modelname achieves a hit rate of over 90\% in three out of the four benchmarks, with MetaQA 3-hop reaching 100\%. In WebQSP, ToG with GPT-4o mini marginally outperforms \modelname in terms of hit rate, but \modelname achieves significantly higher F1 scores, which is a more suitable metric for multi-label QA, demonstrating its superior reasoning performance.
%while \modelname falls slightly below the 90\% threshold, the ToG approach using GPT-4o mini recorded the highest hit rate (90.68\%), slightly surpassing \modelname.
This distinction highlights the advantage of \modelname not only in single-label QA but also in multi-label QA.
The strong performance of \modelname can be attributed to its \oag{}’s ability to accumulate and utilize information from previous hops in multi-hop reasoning. Within a given \textit{T}, the framework can revisit and adjust incorrect paths from prior steps, dynamically selecting alternative paths as needed. Furthermore, during inference, the \sag is not restricted to a single reasoning path but can flexibly combine relevant triples, leading to more accurate reasoning and answer generation.

\subsection{Coverage Across Different LLMs}
Note that \modelname's coverage is the highest across all cases when using GPT-4o as the \oag.
When using relatively low-capacity LLMs, the coverage decreases in varying degrees.
The reason why high-capacity LLMs as a \oag achieve higher coverage is twofold: First, they excel at collecting key evidence, allowing them to request \textit{Verification(\(\cdot\))} at the optimal moment. 
Second, their strong language understanding enables them to effectively use the feedback provided by the \sag. Table ~\ref{tab:2agent_Operator_result} clearly demonstrates that even when \modelname employs a low-capacity LLM for the \oag, the F1 score and Hit Rate remain high despite a decrease in coverage. This highlights the advantage of \modelname's separation of the \oag and \sag. Since \modelname maintains answer reliability while only affecting coverage, users can confidently choose an \oag based on their budget constraints.


\subsection{Case analysis of Abstention}
Even when \textit{T} is high, reasoning may still fail, leading to abstention.
The most common cases are: (1) Repeated helper function requests—The \oag redundantly calls the same function across multiple steps, even after retrieving the necessary information in previous steps. %Since invocation of the helper function is solely based on the \oag's judgment, it may fail to call \sag by \(T\), missing the inference timing.
(2) Failure to interpret \sag{}'s feedback—The \oag struggles to incorporate the \sag{}'s instructions, especially when directed to collect additional information about a specific entity’s relation, failing to refine exploration in later steps.
(3) Failure to extract an answer despite sufficient evidence—When the retrieved triple set is overly large, the \sag may misinterpret relationships between triples, leading to incorrect judgment.
(4) Incorrect function call format—The \oag does not follow the predefined format when calling a helper function, causing parsing issues that prevent information retrieval.


\subsection{LLM Usage Statistics}
Table~\ref{tab:call_statistics} shows the average number of LLM calls per sample for \oag and \sag in \modelname’s reasoning process. It was varied by dataset: the \oag was called between 5.94 and 8.63 requests per sample, while the \sag was called between 1.04 and 1.43 times.
The \sag{}’s call frequency tended to increase with the complexity and difficulty of the query.
In comparison, KG-GPT requires at least 3 calls (\textit{i.e.}, Sentence Segmentation, Graph Retrieval, and Inference) to a high-capacity LLM, and ToG makes a minimum of 4 and maximum of 25 requests, depending on the number of reasoning path, which is closely related to the depth and width limit (\textit{i.e.}, hop limit, beam search width limit in KGs) used for ToG hyperparameter.
\modelname employs \textit{Low/High-Capacity LLM Separation} for accuracy and cost efficiency, significantly reducing high-capacity LLM usage to an average of 1.28 calls per sample, making it both cost-effective and superior in performance.
\input{tables/call_statistics}



\section{Further Analysis}

\begin{figure*}[ht]
    \centering
    \includegraphics[width=\textwidth]{figures/iter_limit.pdf}
    \caption{Changes in coverage, F1 Scores, and hit rate based on Iteration Limit}
    \label{fig:iter-limit}
\end{figure*}

\subsection{Effect of Iteration Limit}
Figure~\ref{fig:iter-limit} illustrates the impact of \textit{T} on coverage, F1 scores, and hit rate. At $5\leq T\leq15$, coverage improves, whereas F1 scores and hit rate slightly decline. Since solving a query typically requires twice the minimum number of hops, lower \(T (=5)\) causes early termination, leading to lower coverage but higher accuracy on simpler queries.
At $10\leq T\leq15$, increased evidence collection enhances coverage, though accuracy slightly drops as queries grow more complex.
Beyond 20 iterations, coverage stabilizes while F1 scores and hit rates marginally decrease. This suggests that the optimal iteration range is 10-15 for benchmarks we used, as further steps mainly introduce redundant exploration that is unhelpful for reasoning.





\input{tables/ablation_ensemble_result}
\subsection{Single-Agent Version of \modelname with Strict Self-Consistency} \label{sec:ensemble_approach}
To further reduce the cost of using a high-capacity LLM as the \sag, we leverage a self-consistency \cite{wang2023selfconsistencyimproveschainthought} strategy where the \oag handles both evidence collection and answer generation (\textit{i.e.}, single-agent version of \modelname). Without the \sag, the \oag assesses evidence sufficiency and generates answers within \textit{Verification(\(\cdot\))}. The reasoning process runs three trials per instance with $T=10$, following these rules;  
First, unlike the typical majority-based self-consistency strategy, our approach enforces a stricter unanimous agreement criterion for the final prediction. 
Second, if no agreement is reached or if \textit{Abstention} appears in any attempt, the final prediction is also \textit{Abstention}.  
We apply three reasoning path strategies;  
Multi-Prompts—Three prompts with distinct few-shot examples for the same query.  
Query Paraphrasing—Three semantically equivalent query variations.  
Top-p / Temperature Variation—Three different top-p / temperature settings for \oag\footnote{Detailed experimental settings and prompt examples are provided in Appendix~\ref{sec:single_agent_method}}.

Table~\ref{tab:ablation_ensemble_result} presents the results.
Note that coverage significantly decreased compared to the dual-agent version of \modelname, while F1 scores and hit rate were comparable or slightly improved except for MetaQA 3-hop and CRONQUESTIONS. Despite this, it still significantly outperformed baseline models, in terms of both hit rate (100\% on MetaQA 3-hop) and micro F1 scores (WebQSP +55.8\%, MetaQA 3-hop +80.2\%, and CRONQUESTIONS +37.5\%).
These results demonstrate that one can obtain higher-than-baseline answer reliability at an even smaller inference cost (\textit{i.e.} no high-capacity LLM) with a variant of \modelname, especially Multi-Prompts generally showing high F1 scores and hit rate across all datasets.
However, relying solely on low-capacity LLMs limits \modelname's adaptability to complex KGs such as CRONQUESTIONS (\textit{i.e.} KGs with temporal information), and the stricter filtering process inevitably results in the rejection of more model predictions, reducing coverage, and overall lower utility compared to the dual-agent version of \modelname.




\section{Conclusion}
% In this study, we introduced the first general KG-based reasoning framework with an \textit{abstention mechanism}, ensuring that predictions are made only when confidence is sufficient. This enhances the reliability of \modelname.  
% The \sag plays a key role in \modelname. The \oag requires a minimal level of capability to explore the KG, while the \sag makes the final decision based on the evidence gathered by the \oag or provides feedback for further evidence collection if the existing evidence is insufficient. Effective reasoning depends on identifying necessary information, key entities, and relevant relations. Thus, using a strong LLM or a fine-tuned model as the \sag significantly improves performance.  
% Our evaluation, using various weak LLMs as the \oag, demonstrated that \modelname outperforms existing KG-based reasoning methods that rely on strong LLMs. By reducing the reasoning steps that require a strong LLM, \modelname offers a more cost-effective solution for KG-based reasoning.


We propose \modelname, the first general KG-based reasoning framework with an \textit{abstention mechanism}, ensuring the reliability for various KG-based reasoning tasks.  
Separation of \oag and \sag reduced high-capacity LLM usage, leading to a cost-effective solution for KG-based reasoning. 
%Additionally, an even more cost-effective solution---a single-agent version \modelname with self-consistency---demonstrated reliability comparable to that of the dual-agent approach, with only a slight decrease in coverage. 
Moreover, in simpler KGs, the single-agent version of \modelname with strict self-consistency can maintain reliability while further reducing cost.

\section*{Limitations}
The \sag makes the final prediction based solely on the triple set and relation list collected by the \oag. Consequently, it cannot determine whether the retrieved information is minimal or exhaustive. In multi-label QA tasks, this limitation may cause underprediction, where the framework generates fewer answers than the actual number of correct labels.  
Additionally, if a query can be answered through multiple relation paths, the \sag may provide an answer as long as one valid path exists, potentially overlooking alternative correct paths. One way to mitigate this would be to involve the \sag in every iteration step, but this would remove the distinction between the \oag and \sag roles, increasing computational costs.  
These constraints stem from the trade-off between cost-effectiveness and reasoning efficiency. While the current design optimizes resource usage, it may not always capture all possible answers in complex reasoning scenarios.

\section*{Ethical Consideration}
LLM-based KG reasoning requires substantial computational resources, which can contribute to environmental concerns. While our study proposes methods to reduce overall LLM usage, the reliance on large-scale models remains a consideration in terms of environmental impact.


\bibliography{acl_latex}

\appendix
\newpage
%\section{Appendix}


\input{tables/model_specs}
\input{tables/appendix_metaqa}

\input{tables/ablation_subagent_ensemble_result}
\input{tables/supplement_allmini}



\section{Performance on 1-Hop and 2-Hop Questions} \label{sec:metaqa_12hop}
The MetaQA dataset consists of 1-hop, 2-hop, and 3-hop questions; however, our experiments focused exclusively on 3-hop questions. Given that the KG of MetaQA is relatively small and that 1-hop and 2-hop questions are considerably simpler than 3-hop questions, we excluded them from our primary evaluation. Nevertheless, to assess the \modelname across different levels of task complexity, we randomly sampled 100 questions from 1-hop and 2-hop sets and evaluated the performance. As shown in Table~\ref{tab:appendix_metaqa}, \modelname exhibited strong performance with high coverage.






\section{Examples of the Two Excluded Question Types in CRONQUESTIONS}\label{sec:cronq_excluded}
Unlike the three other datasets, CRONQUESTIONS is constructed with a five-element KG, where each quintuple follows the format:
[head, relation, tail, start time, end time]. This structure includes temporal information, specifying the start and end years of an event.
CRONQUESTIONS contains five types of reasoning tasks: Simple time, Simple entity, Before/After, First/Last, and Time Join.
However, in our experiments, we excluded the Before/After and First/Last question types. The primary reason is that, while our framework predicts answers based on the KG, these question types often contain subjective ground truth labels that do not fully align with the available KG information. For example, this is the sample of Before/After question:
``Which team did Roberto Baggio play for before the Italy national football team?''
Using our framework, we can retrieve the following KG facts related to Roberto Baggio:
[Roberto Baggio, member of sports team, ACF Fiorentina, 1985, 1990]
[Roberto Baggio, member of sports team, Brescia Calcio, 2000, 2004]
[Roberto Baggio, member of sports team, Vicenza Calcio, 1982, 1985]
[Roberto Baggio, member of sports team, Juventus F.C., 1990, 1995]
[Roberto Baggio, member of sports team, Italy national football team, 1988, 2004]
[Roberto Baggio, member of sports team, A.C. Milan, 1995, 1997]
[Roberto Baggio, member of sports team, Bologna F.C. 1909, 1997, 1998].
According to the KG, Roberto Baggio joined the Italy national football team in 1988. Before that, he played for Vicenza Calcio (starting in 1982) and ACF Fiorentina (starting in 1985), meaning both teams are valid answers. 
However, the ground truth label in the dataset only includes ACF Fiorentina, omitting Vicenza Calcio, despite it being a correct answer based on the KG. Due to this labeling inconsistency, objective evaluation of these question types becomes unreliable. As a result, we decided to exclude these types from our experiments.



\section{Prompt Structure and Usage}\label{sec:prompt}
Each prompt for \oag consists of three components:
Task description, Helper function explanations, and three few-shot examples.
When using \modelname, users only need to modify the few-shot examples to match the specific dataset while keeping the rest of the prompt unchanged. Examining the prompts reveals that when the \oag requests a helper function, the \oag can request multiple instances in a single iteration based on its needs. Additionally, it can request different types of functions simultaneously. The prompt for \sag contains the following elements: Task description, triples collected so far by the \oag, a relation list for each entity, and few-shot examples.
The reason for explicitly including the entity-wise relation list is to ensure that when the \sag provides feedback to the \oag, it requests subgraphs that actually exist in the KG. During pilot testing, when the relation list was not provided, the system occasionally requested non-existent entity-relation pairs in the KG. This resulted in ineffective feedback and ultimately failed to assist the \oag in its KG exploration.

\section{Final Reasoning Path Construction}\label{sec:finalgoldpath}
When sufficient evidences are given from \oag to \sag, then the \sag selects the necessary triples and constructs a final reasoning path that aligns with the claim structure.
Assume that the given query is ``Which languages were used in the films directed by the same directors as [The Vanishing American]'' and \(G_k\) given by the \oag are as follows (tilde (\textasciitilde)\space represents the inverse direction of relation): 
[The Vanishing American, directed\_by, George B. Seitz], [George B. Seitz, \textasciitilde directed\_by, The Last of the Mohicans], [George B. Seitz, \textasciitilde directed\_by, Love Finds Andy Hardy], [The Last of the Mohicans, in\_language, English], [Love Finds Andy Hardy, in\_language, French]. Then, \sag generates two final reasoning paths: (The Vanishing American-George B. Seitz-The Last of the Mohicans-English), and (The Vanishing American-George B. Seitz-Love Finds Andy Hardy-French). Finally, \sag generates an answer and returns to \oag (\textit{i.e.}, English, French in given example).



\section{Model Spec} \label{sec:model_spec}
Please check Table~\ref{sec:llm_spec}. Entries labeled as ``Unknown'' indicate that the information is not publicly available.

\section{\modelname Combined with Self-Consistency Strategy} \label{sec:dual_ensemble}
Table~\ref{tab:ablation_Supervisoragent_ensemble_result} shows the result of combining the self-consistency strategy with the dual-agent approach. The \sag generated the final answer based on three trials, leading to stricter predictions. As a result, coverage was lower compared to using dual-agent \modelname alone. For WebQSP and MetaQA, the F1 score was lower than that of a single trial of the dual-agent \modelname, whereas the hit rate was significantly higher. This is because, applying the strict self-consistency technique, some multi-label predictions were filtered out, meaning the model did not perfectly match all ground truth labels but still correctly predicted at least one. For CRONQUESTIONS, coverage, F1 scores, and hit rate were relatively lower. This dataset contains a significantly higher number of ground truth labels than others, making it difficult for any single trial to cover all labels. Consequently, the final prediction lacked sufficient labels. In FactKG, coverage varied widely, ranging from 10\% to 50\% depending on the reasoning path method. However, the hit rate consistently remained above 93\%, indicating strong performance. Overall, for multi-label tasks with many ground truth labels, a single trial using \modelname: a single trial of dual-agent approach performed more effectively than dual-agent with self-consistency strategy, suggesting that dual-agent with self-consistency strategy is not always beneficial for complex multi-label reasoning tasks.

\section{\modelname Using the Full Dataset } \label{sec:all_dataset}
Table~\ref{tab:supplement_allmini} presents the results obtained using the full dataset. In this experiment, both the \oag and \sag were set to GPT-4o mini, and the experimental setup remained identical to the main experiment.

For MetaQA 3-hop, CRONQUESTIONS, and FactKG, the Hit rate exceeded 90\%, with CRONQUESTIONS reaching an impressive 98.6\%. However, coverage was generally lower or similar compared to the main experiment. This decline is likely due to the \sag{}'s limited ability to construct the correct reasoning path using the triple set during inference, as it was replaced with GPT-4o mini instead of GPT-4o. Although sufficient evidence was available, the \sag struggled to appropriately combine the necessary components of the query, leading to failed predictions. These results further highlight the critical role of the \sag in the reasoning process.

Despite the slight performance drop compared to the main experiment due to the relatively low-capacity \sag, the framework still significantly outperforms baseline methods. The effectiveness of the \textit{abstention mechanism} remains evident, ensuring that the system generates reliable predictions while maintaining robustness against uncertainty. 







\section{Qualitative Analysis} \label{sec:qualitative_analysis}
Figure~\ref{fig:correct_case} illustrates an example where \modelname successfully performs reasoning on WebQSP, while Figure~\ref{fig:wrong_case} shows a case where it fails. Within the \textit{T} of 15, each box represents the \oag{}'s reasoning (gray), the \textit{Server}'s execution result (blue), and the \sag{}'s reasoning (red). Some parts of the iteration process have been omitted due to excessive length.

\section{Details of Single Agent Version of \modelname combined with Self-Consistency Strategy} \label{sec:single_agent_method}
The typical Self-Consistency strategy allows the language model to generate multiple reasoning paths and selects the most common answer across all trials. In contrast, our approach applies a stricter criterion, selecting the final answer only when all trials reach unanimous agreement.
The details of various reasoning paths to generate multiple responses are as follows;
The prompt used for the single-agent approach, where the \oag handles both KG retrieval and answer generation, is shown in Figure~\ref{fig:single_prompt}. For the Multi-Prompt approach, the same base prompt was used, with only the few-shot examples adjusted for in-context learning. The prompt used for query paraphrasing is identical to that in Figure~\ref{fig:paraphrase}. In this approach, each query is rewritten into three semantically equivalent but structurally different forms, and each variation is processed independently by a low-capacity LLM for three reasoning attempts. The parameter combinations used for LLM Top-p / Temperature variation are as follows:  
(Top-p, Temperature) = (0.3, 0.5), (0.7, 1.0), (0.95, 0.95)




\section{Experimental Setting for Baselines}\label{sec:baseline_setting}
Among the baselines used in the experiment, ToG allows width and depth to be set as hyperparameters. In our experiments, the depth was set to 3 for all datasets except FactKG, where it was set to 4. By default, ToG's width is set to 3, meaning it considers up to three entities or relations per step, regardless of the type of subject. However, this setting was highly ineffective for multi-label tasks. To improve its performance, we separately configured (relation-width, entity-width) to optimize results. The values used in the main experiment were as follows: FactKG, MetaQA, and WebQSP were set to (3, 7), (2, 5), and (3, 3), respectively.

Additionally, when ToG fails to retrieve supporting evidence from the KG, it generates answers based on the LLM’s internal knowledge. To ensure a fair comparison based solely on KG-derived information, we treated cases where ToG relied on internal knowledge after KG retrieval as \textit{Abstentions}. Similarly, while KG-GPT does not have a built-in \textit{abstention mechanism}, we considered instances where the model failed to generate a final answer due to errors during its three-step process (Sentence Segmentation, Graph-Retrieval, and Inference)—such as token length limits or parsing failures—as \textit{Abstentions}.

For both baselines, prompt tuning was conducted to align them with each dataset. Specifically, we modified the few-shot examples extracted from each dataset while keeping the default prompt structure unchanged.

\input{tables/code}

\begin{figure*}
    \centering
    \includegraphics[width=\textwidth]{figures/goodcase.pdf}
    \caption{Successful Case in WebQSP. \sag effectively guides the model to extract a more relevant answer for the question. \textcolor{gray}{\oag}, \textcolor{blue}{\textit{Server} Response}, \textcolor{red}{\sag} for each colored box. }
    \label{fig:correct_case}
\end{figure*}

\begin{figure*}
    \centering
    \includegraphics[width=\textwidth]{figures/badcase.pdf}
    \caption{Failure Case in WebQSP. \sag fails to infer, leading the \oag to invoke functions in the wrong format repeatedly. \textcolor{gray}{\oag}, \textcolor{blue}{\textit{Server} Response}, \textcolor{red}{\sag} for each colored box. }
    \label{fig:wrong_case}
\end{figure*}

\begin{figure*}
    \centering
    \includegraphics[width=\textwidth]{figures/mainagent.pdf}
    \caption{Used for FactKG. [Your Task] is generated by the \oag, while [User] represents either the \textit{Server}'s response or the \sag{}'s answer.}
    \label{fig:enter-label}
\end{figure*}

\begin{figure*}
    \centering
    \includegraphics[width=\textwidth]{figures/subagent.pdf}
    \caption{Used for FactKG. [Your Task] is generated by the \oag, while [User] contains the given query and the evidence collected by the \oag.}
    \label{fig:enter-label}
\end{figure*}

\begin{figure*}
    \centering
    \includegraphics[width=\textwidth]{figures/singleagent.pdf}
    \caption{Prompt for single version of \modelname  [Your Task] is generated by the \oag, while [User] represents the \textit{Server}'s response.}
    \label{fig:single_prompt}
\end{figure*}

\begin{figure*}
    \centering
    \includegraphics[width=\textwidth]{figures/paraphrase.pdf}
    \caption{Prompt for query paraphrasing. [User] contains the query to be paraphrased, while [ChatGPT] generates three different variations of the sentence.}
    \label{fig:paraphrase}
\end{figure*}

\end{document}


