% Use this file for citations not found in the ACL Anthology (contained in "anthology.bib").

@misc{kg-agent2024,
      title={KG-Agent: An Efficient Autonomous Agent Framework for Complex Reasoning over Knowledge Graph}, 
      author={Jinhao Jiang and Kun Zhou and Wayne Xin Zhao and Yang Song and Chen Zhu and Hengshu Zhu and Ji-Rong Wen},
      year={2024},
      eprint={2402.11163},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2402.11163}, 
}

@misc{luo2024graphconstrainedreasoningfaithfulreasoning,
      title={Graph-constrained Reasoning: Faithful Reasoning on Knowledge Graphs with Large Language Models}, 
      author={Linhao Luo and Zicheng Zhao and Chen Gong and Gholamreza Haffari and Shirui Pan},
      year={2024},
      eprint={2410.13080},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2410.13080}, 
}

@misc{ma2024debategraphflexiblereliable,
      title={Debate on Graph: a Flexible and Reliable Reasoning Framework for Large Language Models}, 
      author={Jie Ma and Zhitao Gao and Qi Chai and Wangchun Sun and Pinghui Wang and Hongbin Pei and Jing Tao and Lingyun Song and Jun Liu and Chen Zhang and Lizhen Cui},
      year={2024},
      eprint={2409.03155},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2409.03155}, 
}

@misc{ToG2.0,
      title={Think-on-Graph 2.0: Deep and Faithful Large Language Model Reasoning with Knowledge-guided Retrieval Augmented Generation}, 
      author={Shengjie Ma and Chengjin Xu and Xuhui Jiang and Muzhi Li and Huaren Qu and Cehao Yang and Jiaxin Mao and Jian Guo},
      year={2024},
      eprint={2407.10805},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2407.10805}, 
}

@inproceedings{kim-etal-2023-kg,
    title = "{KG}-{GPT}: A General Framework for Reasoning on Knowledge Graphs Using Large Language Models",
    author = "Kim, Jiho  and
      Kwon, Yeonsu  and
      Jo, Yohan  and
      Choi, Edward",
    editor = "Bouamor, Houda  and
      Pino, Juan  and
      Bali, Kalika",
    booktitle = "Findings of the Association for Computational Linguistics: EMNLP 2023",
    month = dec,
    year = "2023",
    address = "Singapore",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2023.findings-emnlp.631/",
    doi = "10.18653/v1/2023.findings-emnlp.631",
    pages = "9410--9421",
    abstract = "While large language models (LLMs) have made considerable advancements in understanding and generating unstructured text, their application in structured data remains underexplored. Particularly, using LLMs for complex reasoning tasks on knowledge graphs (KGs) remains largely untouched. To address this, we propose KG-GPT, a multi-purpose framework leveraging LLMs for tasks employing KGs. KG-GPT comprises three steps: Sentence Segmentation, Graph Retrieval, and Inference, each aimed at partitioning sentences, retrieving relevant graph components, and deriving logical conclusions, respectively. We evaluate KG-GPT using KG-based fact verification and KGQA benchmarks, with the model showing competitive and robust performance, even outperforming several fully-supervised models. Our work, therefore, marks a significant step in unifying structured and unstructured data processing within the realm of LLMs."
}


@misc{gao2024twostagegenerativequestionanswering,
      title={Two-stage Generative Question Answering on Temporal Knowledge Graph Using Large Language Models}, 
      author={Yifu Gao and Linbo Qiao and Zhigang Kan and Zhihua Wen and Yongquan He and Dongsheng Li},
      year={2024},
      eprint={2402.16568},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2402.16568}, 
}

@misc{ToG,
      title={Think-on-Graph: Deep and Responsible Reasoning of Large Language Model on Knowledge Graph}, 
      author={Jiashuo Sun and Chengjin Xu and Lumingyuan Tang and Saizhuo Wang and Chen Lin and Yeyun Gong and Lionel M. Ni and Heung-Yeung Shum and Jian Guo},
      year={2024},
      eprint={2307.07697},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2307.07697}, 
}

@misc{kim2024causalreasoninglargelanguage,
      title={Causal Reasoning in Large Language Models: A Knowledge Graph Approach}, 
      author={Yejin Kim and Eojin Kang and Juae Kim and H. Howie Huang},
      year={2024},
      eprint={2410.11588},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2410.11588}, 
}


@misc{chen2024planongraphselfcorrectingadaptiveplanning,
      title={Plan-on-Graph: Self-Correcting Adaptive Planning of Large Language Model on Knowledge Graphs}, 
      author={Liyi Chen and Panrong Tong and Zhongming Jin and Ying Sun and Jieping Ye and Hui Xiong},
      year={2024},
      eprint={2410.23875},
      archivePrefix={arXiv},
      primaryClass={cs.AI},
      url={https://arxiv.org/abs/2410.23875}, 
}

@misc{huang2024ritekdatasetlargelanguage,
      title={RiTeK: A Dataset for Large Language Models Complex Reasoning over Textual Knowledge Graphs}, 
      author={Jiatan Huang and Mingchen Li and Zonghai Yao and Zhichao Yang and Yongkang Xiao and Feiyun Ouyang and Xiaohan Li and Shuo Han and Hong Yu},
      year={2024},
      eprint={2410.13987},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2410.13987}, 
}

@misc{sui2024knowledgegraphsmakelarge,
      title={Can Knowledge Graphs Make Large Language Models More Trustworthy? An Empirical Study over Open-ended Question Answering}, 
      author={Yuan Sui and Yufei He and Zifeng Ding and Bryan Hooi},
      year={2024},
      eprint={2410.08085},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2410.08085}, 
}

@INPROCEEDINGS{10743169,
  author={Ye, Weiqi and Zhang, Qiang and Zhou, Xian and Hu, Wenpeng and Tian, Changhai and Cheng, Jiajun},
  booktitle={2024 International Conference on Computational Linguistics and Natural Language Processing (CLNLP)}, 
  title={Correcting Factual Errors in LLMs via Inference Paths Based on Knowledge Graph}, 
  year={2024},
  volume={},
  number={},
  pages={12-16},
  keywords={Heart;Large language models;Knowledge graphs;Deep reinforcement learning;Cognition;Natural language processing;Inference algorithms;Error correction;Computational linguistics;Reliability;factual error correction;inference path;knowledge graph;reinforcement learning},
  doi={10.1109/CLNLP64123.2024.00011}}


@misc{li2024enhancedpromptbasedllmreasoning,
      title={An Enhanced Prompt-Based LLM Reasoning Scheme via Knowledge Graph-Integrated Collaboration}, 
      author={Yihao Li and Ru Zhang and Jianyi Liu},
      year={2024},
      eprint={2402.04978},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2402.04978}, 
}

@misc{wang2024llmprompterlowresourceinductive,
      title={LLM as Prompter: Low-resource Inductive Reasoning on Arbitrary Knowledge Graphs}, 
      author={Kai Wang and Yuwei Xu and Zhiyong Wu and Siqiang Luo},
      year={2024},
      eprint={2402.11804},
      archivePrefix={arXiv},
      primaryClass={cs.AI},
      url={https://arxiv.org/abs/2402.11804}, 
}


@misc{huang2024evaluatingenhancinglargelanguage,
      title={Evaluating and Enhancing Large Language Models for Conversational Reasoning on Knowledge Graphs}, 
      author={Yuxuan Huang},
      year={2024},
      eprint={2312.11282},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2312.11282}, 
}

@misc{cronQA,
      title={Question Answering Over Temporal Knowledge Graphs}, 
      author={Apoorv Saxena and Soumen Chakrabarti and Partha Talukdar},
      year={2021},
      eprint={2106.01515},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2106.01515}, 
}

@inproceedings{webqsp,
    title = "The Value of Semantic Parse Labeling for Knowledge Base Question Answering",
    author = "Yih, Wen-tau  and
      Richardson, Matthew  and
      Meek, Chris  and
      Chang, Ming-Wei  and
      Suh, Jina",
    editor = "Erk, Katrin  and
      Smith, Noah A.",
    booktitle = "Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)",
    month = aug,
    year = "2016",
    address = "Berlin, Germany",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/P16-2033/",
    doi = "10.18653/v1/P16-2033",
    pages = "201--206"
}


@inproceedings{kim-etal-2023-factkg,
    title = "{F}act{KG}: Fact Verification via Reasoning on Knowledge Graphs",
    author = "Kim, Jiho  and
      Park, Sungjin  and
      Kwon, Yeonsu  and
      Jo, Yohan  and
      Thorne, James  and
      Choi, Edward",
    editor = "Rogers, Anna  and
      Boyd-Graber, Jordan  and
      Okazaki, Naoaki",
    booktitle = "Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = jul,
    year = "2023",
    address = "Toronto, Canada",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2023.acl-long.895/",
    doi = "10.18653/v1/2023.acl-long.895",
    pages = "16190--16206",
    abstract = "In real world applications, knowledge graphs (KG) are widely used in various domains (e.g. medical applications and dialogue agents). However, for fact verification, KGs have not been adequately utilized as a knowledge source. KGs can be a valuable knowledge source in fact verification due to their reliability and broad applicability. A KG consists of nodes and edges which makes it clear how concepts are linked together, allowing machines to reason over chains of topics. However, there are many challenges in understanding how these machine-readable concepts map to information in text. To enable the community to better use KGs, we introduce a new dataset, FactKG: Fact Verificationvia Reasoning on Knowledge Graphs. It consists of 108k natural language claims with five types of reasoning: One-hop, Conjunction, Existence, Multi-hop, and Negation. Furthermore, FactKG contains various linguistic patterns, including colloquial style claims as well as written style claims to increase practicality. Lastly, we develop a baseline approach and analyze FactKG over these reasoning types. We believe FactKG can advance both reliability and practicality in KG-based fact verification."
}




@misc{metaQA,
      title={Variational Reasoning for Question Answering with Knowledge Graph}, 
      author={Yuyu Zhang and Hanjun Dai and Zornitsa Kozareva and Alexander J. Smola and Le Song},
      year={2017},
      eprint={1709.04071},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/1709.04071}, 
}

@misc{structgpt,
      title={StructGPT: A General Framework for Large Language Model to Reason over Structured Data}, 
      author={Jinhao Jiang and Kun Zhou and Zican Dong and Keming Ye and Wayne Xin Zhao and Ji-Rong Wen},
      year={2023},
      eprint={2305.09645},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2305.09645}, 
}


@misc{pathsovergraph,
      title={Paths-over-Graph: Knowledge Graph Empowered Large Language Model Reasoning}, 
      author={Xingyu Tan and Xiaoyang Wang and Qing Liu and Xiwei Xu and Xin Yuan and Wenjie Zhang},
      year={2025},
      eprint={2410.14211},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2410.14211}, 
}

@inproceedings{KLCoT,
  title     = {KG-CoT: Chain-of-Thought Prompting of Large Language Models over Knowledge Graphs for Knowledge-Aware Question Answering},
  author    = {Zhao, Ruilin and Zhao, Feng and Wang, Long and Wang, Xianzhi and Xu, Guandong},
  booktitle = {Proceedings of the Thirty-Third International Joint Conference on
               Artificial Intelligence, {IJCAI-24}},
  publisher = {International Joint Conferences on Artificial Intelligence Organization},
  editor    = {Kate Larson},
  pages     = {6642--6650},
  year      = {2024},
  month     = {8},
  note      = {Main Track},
  doi       = {10.24963/ijcai.2024/734},
  url       = {https://doi.org/10.24963/ijcai.2024/734},
}

% -----------------------
% Reliable KGQA
% -----------------------
@misc{wen2024characterizingllmabstentionbehavior,
      title={Characterizing LLM Abstention Behavior in Science QA with Context Perturbations}, 
      author={Bingbing Wen and Bill Howe and Lucy Lu Wang},
      year={2024},
      eprint={2404.12452},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2404.12452}, 
}

@misc{wang2023selfconsistencyimproveschainthought,
      title={Self-Consistency Improves Chain of Thought Reasoning in Language Models}, 
      author={Xuezhi Wang and Jason Wei and Dale Schuurmans and Quoc Le and Ed Chi and Sharan Narang and Aakanksha Chowdhery and Denny Zhou},
      year={2023},
      eprint={2203.11171},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2203.11171}, 
}




% -----------------------
% Model Specs
% -----------------------
@misc{qwen2025qwen25technicalreport,
      title={Qwen2.5 Technical Report}, 
      author={Qwen},
      year={2025},
      eprint={2412.15115},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2412.15115}, 
}

@online{mistral2023,
  author = {Mistral},
  title = {Mistral Small 3},
  year = {2025},
  howpublished = {Mistral AI Research},
  url = {https://mistral.ai/en/news/mistral-small-3}
}

@online{openaigpt4o,
  author    = {OpenAI},
  title     = {GPT-4o System Card},
  year      = {2024},
  howpublished = {OpenAI Research},
  url       = {https://openai.com/index/gpt-4o-system-card/}
}

@online{openaigpt4omini,
  author    = {OpenAI},
  title     = {GPT-4o mini: advancing cost-efficient intelligence},
  year      = {2024},
  howpublished = {OpenAI Research},
  url       = {https://openai.com/index/gpt-4o-mini-advancing-cost-efficient-intelligence/}
}


@misc{grattafiori2024llama3herdmodels,
      title={The Llama 3 Herd of Models}, 
      author={Meta},
      year={2024},
      eprint={2407.21783},
      archivePrefix={arXiv},
      primaryClass={cs.AI},
      url={https://arxiv.org/abs/2407.21783}, 
}

% -----------------------
% Related Works
% -----------------------

@article{perez2009semantics,
  title={Semantics and complexity of SPARQL},
  author={P{\'e}rez, Jorge and Arenas, Marcelo and Gutierrez, Claudio},
  journal={ACM Transactions on Database Systems (TODS)},
  volume={34},
  number={3},
  pages={1--45},
  year={2009},
  publisher={ACM New York, NY, USA}
}

@misc{lan2022complexknowledgebasequestion,
      title={Complex Knowledge Base Question Answering: A Survey}, 
      author={Yunshi Lan and Gaole He and Jinhao Jiang and Jing Jiang and Wayne Xin Zhao and Ji-Rong Wen},
      year={2022},
      eprint={2108.06688},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2108.06688}, 
}

@inproceedings{ji-etal-2024-retrieval,
    title = "Retrieval and Reasoning on {KG}s: Integrate Knowledge Graphs into Large Language Models for Complex Question Answering",
    author = "Ji, Yixin  and
      Wu, Kaixin  and
      Li, Juntao  and
      Chen, Wei  and
      Zhong, Mingjie  and
      Jia, Xu  and
      Zhang, Min",
    editor = "Al-Onaizan, Yaser  and
      Bansal, Mohit  and
      Chen, Yun-Nung",
    booktitle = "Findings of the Association for Computational Linguistics: EMNLP 2024",
    month = nov,
    year = "2024",
    address = "Miami, Florida, USA",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2024.findings-emnlp.446/",
    doi = "10.18653/v1/2024.findings-emnlp.446",
    pages = "7598--7610",
    abstract = "Despite Large Language Models (LLMs) have performed impressively in various Natural Language Processing (NLP) tasks, their inherent hallucination phenomena severely challenge their credibility in complex reasoning. Combining explainable Knowledge Graphs (KGs) with LLMs is a promising path to address this issue. However, structured KGs are difficult to utilize, and how to make LLMs understand and incorporate them is a challenging topic. We thereby reorganize a more efficient structure of KGs, while designing the KG-related instruction tuning and continual pre-training strategies to enable LLMs to learn and internalize this form of representation effectively. Moreover, we construct subgraphs to further enhance the retrieval capabilities of KGs via CoT reasoning. Extensive experiments on two KGQA datasets demonstrate that our model achieves convincing performance compared to strong baselines."
}


@misc{mavromatis2024gnnraggraphneuralretrieval,
      title={GNN-RAG: Graph Neural Retrieval for Large Language Model Reasoning}, 
      author={Costas Mavromatis and George Karypis},
      year={2024},
      eprint={2405.20139},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2405.20139}, 
}

@inproceedings{saxena-etal-2020-improving,
    title = "Improving Multi-hop Question Answering over Knowledge Graphs using Knowledge Base Embeddings",
    author = "Saxena, Apoorv  and
      Tripathi, Aditay  and
      Talukdar, Partha",
    editor = "Jurafsky, Dan  and
      Chai, Joyce  and
      Schluter, Natalie  and
      Tetreault, Joel",
    booktitle = "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics",
    month = jul,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.acl-main.412/",
    doi = "10.18653/v1/2020.acl-main.412",
    pages = "4498--4507",
    abstract = "Knowledge Graphs (KG) are multi-relational graphs consisting of entities as nodes and relations among them as typed edges. Goal of the Question Answering over KG (KGQA) task is to answer natural language queries posed over the KG. Multi-hop KGQA requires reasoning over multiple edges of the KG to arrive at the right answer. KGs are often incomplete with many missing links, posing additional challenges for KGQA, especially for multi-hop KGQA. Recent research on multi-hop KGQA has attempted to handle KG sparsity using relevant external text, which isn`t always readily available. In a separate line of research, KG embedding methods have been proposed to reduce KG sparsity by performing missing link prediction. Such KG embedding methods, even though highly relevant, have not been explored for multi-hop KGQA so far. We fill this gap in this paper and propose EmbedKGQA. EmbedKGQA is particularly effective in performing multi-hop KGQA over sparse KGs. EmbedKGQA also relaxes the requirement of answer selection from a pre-specified neighborhood, a sub-optimal constraint enforced by previous multi-hop KGQA methods. Through extensive experiments on multiple benchmark datasets, we demonstrate EmbedKGQA`s effectiveness over other state-of-the-art baselines."
}

@inproceedings{sun-etal-2018-open,
    title = "Open Domain Question Answering Using Early Fusion of Knowledge Bases and Text",
    author = "Sun, Haitian  and
      Dhingra, Bhuwan  and
      Zaheer, Manzil  and
      Mazaitis, Kathryn  and
      Salakhutdinov, Ruslan  and
      Cohen, William",
    editor = "Riloff, Ellen  and
      Chiang, David  and
      Hockenmaier, Julia  and
      Tsujii, Jun{'}ichi",
    booktitle = "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing",
    month = oct # "-" # nov,
    year = "2018",
    address = "Brussels, Belgium",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/D18-1455/",
    doi = "10.18653/v1/D18-1455",
    pages = "4231--4242",
    abstract = "Open Domain Question Answering (QA) is evolving from complex pipelined systems to end-to-end deep neural networks. Specialized neural models have been developed for extracting answers from either text alone or Knowledge Bases (KBs) alone. In this paper we look at a more practical setting, namely QA over the combination of a KB and entity-linked text, which is appropriate when an incomplete KB is available with a large text corpus. Building on recent advances in graph representation learning we propose a novel model, GRAFT-Net, for extracting answers from a question-specific subgraph containing text and KB entities and relations. We construct a suite of benchmark tasks for this problem, varying the difficulty of questions, the amount of training data, and KB completeness. We show that GRAFT-Net is competitive with the state-of-the-art when tested using either KBs or text alone, and vastly outperforms existing methods in the combined setting."
}


@inproceedings{10.1145/3442381.3449992,
author = {Gu, Yu and Kase, Sue and Vanni, Michelle and Sadler, Brian and Liang, Percy and Yan, Xifeng and Su, Yu},
title = {Beyond I.I.D.: Three Levels of Generalization for Question Answering on Knowledge Bases},
year = {2021},
isbn = {9781450383127},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3442381.3449992},
doi = {10.1145/3442381.3449992},
abstract = {Existing studies on question answering on knowledge bases (KBQA) mainly operate with the standard i.i.d. assumption, i.e., training distribution over questions is the same as the test distribution. However, i.i.d. may be neither achievable nor desirable on large-scale KBs because 1) true user distribution is hard to capture and 2) randomly sampling training examples from the enormous space would be data-inefficient. Instead, we suggest that KBQA models should have three levels of built-in generalization: i.i.d., compositional, and zero-shot. To facilitate the development of KBQA models with stronger generalization, we construct and release a new large-scale, high-quality dataset with 64,331 questions, GrailQA, and provide evaluation settings for all three levels of generalization. In addition, we propose a novel BERT-based KBQA model. The combination of our dataset and model enables us to thoroughly examine and demonstrate, for the first time, the key role of pre-trained contextual embeddings like BERT in the generalization of KBQA.1},
booktitle = {Proceedings of the Web Conference 2021},
pages = {3477–3488},
numpages = {12},
keywords = {Semantic Parsing, Question Answering, Knowledge Base},
location = {Ljubljana, Slovenia},
series = {WWW '21}
}

@misc{wang2023knowledgedrivencotexploringfaithful,
      title={Knowledge-Driven CoT: Exploring Faithful Reasoning in LLMs for Knowledge-intensive Question Answering}, 
      author={Keheng Wang and Feiyu Duan and Sirui Wang and Peiguang Li and Yunsen Xian and Chuantao Yin and Wenge Rong and Zhang Xiong},
      year={2023},
      eprint={2308.13259},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2308.13259}, 
}

@inproceedings{li-etal-2023-shot,
    title = "Few-shot In-context Learning on Knowledge Base Question Answering",
    author = "Li, Tianle  and
      Ma, Xueguang  and
      Zhuang, Alex  and
      Gu, Yu  and
      Su, Yu  and
      Chen, Wenhu",
    editor = "Rogers, Anna  and
      Boyd-Graber, Jordan  and
      Okazaki, Naoaki",
    booktitle = "Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = jul,
    year = "2023",
    address = "Toronto, Canada",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2023.acl-long.385/",
    doi = "10.18653/v1/2023.acl-long.385",
    pages = "6966--6980",
    abstract = "Question answering over knowledge bases is considered a difficult problem due to the challenge of generalizing to a wide variety of possible natural language questions. Additionally, the heterogeneity of knowledge base schema items between different knowledge bases often necessitates specialized training for different knowledge base question-answering (KBQA) datasets. To handle questions over diverse KBQA datasets with a unified training-free framework, we propose KB-BINDER, which for the first time enables few-shot in-context learning over KBQA tasks. Firstly, KB-BINDER leverages large language models like Codex to generate logical forms as the draft for a specific question by imitating a few demonstrations. Secondly, KB-BINDER grounds on the knowledge base to bind the generated draft to an executable one with BM25 score matching. The experimental results on four public heterogeneous KBQA datasets show that KB-BINDER can achieve a strong performance with only a few in-context demonstrations. Especially on GraphQA and 3-hop MetaQA, KB-BINDER can even outperform the state-of-the-art trained models. On GrailQA and WebQSP, our model is also on par with other fully-trained models. We believe KB-BINDER can serve as an important baseline for future research. We plan to release all the code and data. Our code is available at \url{https://github.com/ltl3A87/KB-BINDER}."
}


@inproceedings{10.1145/1376616.1376746,
author = {Bollacker, Kurt and Evans, Colin and Paritosh, Praveen and Sturge, Tim and Taylor, Jamie},
title = {Freebase: a collaboratively created graph database for structuring human knowledge},
year = {2008},
isbn = {9781605581026},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1376616.1376746},
doi = {10.1145/1376616.1376746},
abstract = {Freebase is a practical, scalable tuple database used to structure general human knowledge. The data in Freebase is collaboratively created, structured, and maintained. Freebase currently contains more than 125,000,000 tuples, more than 4000 types, and more than 7000 properties. Public read/write access to Freebase is allowed through an HTTP-based graph-query API using the Metaweb Query Language (MQL) as a data query and manipulation language. MQL provides an easy-to-use object-oriented interface to the tuple data in Freebase and is designed to facilitate the creation of collaborative, Web-based data-oriented applications.},
booktitle = {Proceedings of the 2008 ACM SIGMOD International Conference on Management of Data},
pages = {1247–1250},
numpages = {4},
keywords = {tuple store, semantic network, collaborative systems},
location = {Vancouver, Canada},
series = {SIGMOD '08}
}

@article{10.1145/2629489,
author = {Vrande\v{c}i\'{c}, Denny and Kr\"{o}tzsch, Markus},
title = {Wikidata: a free collaborative knowledgebase},
year = {2014},
issue_date = {October 2014},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {57},
number = {10},
issn = {0001-0782},
url = {https://doi.org/10.1145/2629489},
doi = {10.1145/2629489},
abstract = {This collaboratively edited knowledgebase provides a common source of data for Wikipedia, and everyone else.},
journal = {Commun. ACM},
month = sep,
pages = {78–85},
numpages = {8}
}

@article{Sun_Zhang_Cheng_Qu_2020, title={SPARQA: Skeleton-Based Semantic Parsing for Complex Questions over Knowledge Bases}, volume={34}, url={https://ojs.aaai.org/index.php/AAAI/article/view/6426}, DOI={10.1609/aaai.v34i05.6426}, abstractNote={&lt;p&gt;Semantic parsing transforms a natural language question into a formal query over a knowledge base. Many existing methods rely on syntactic parsing like dependencies. However, the accuracy of producing such expressive formalisms is not satisfying on long complex questions. In this paper, we propose a novel skeleton grammar to represent the high-level structure of a complex question. This dedicated coarse-grained formalism with a BERT-based parsing algorithm helps to improve the accuracy of the downstream fine-grained semantic parsing. Besides, to align the structure of a question with the structure of a knowledge base, our multi-strategy method combines sentence-level and word-level semantics. Our approach shows promising performance on several datasets.&lt;/p&gt;}, number={05}, journal={Proceedings of the AAAI Conference on Artificial Intelligence}, author={Sun, Yawei and Zhang, Lingling and Cheng, Gong and Qu, Yuzhong}, year={2020}, month={Apr.}, pages={8952-8959} }

@inproceedings{ye-etal-2022-rng,
    title = "{RNG}-{KBQA}: Generation Augmented Iterative Ranking for Knowledge Base Question Answering",
    author = "Ye, Xi  and
      Yavuz, Semih  and
      Hashimoto, Kazuma  and
      Zhou, Yingbo  and
      Xiong, Caiming",
    editor = "Muresan, Smaranda  and
      Nakov, Preslav  and
      Villavicencio, Aline",
    booktitle = "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = may,
    year = "2022",
    address = "Dublin, Ireland",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.acl-long.417/",
    doi = "10.18653/v1/2022.acl-long.417",
    pages = "6032--6043",
    abstract = "Existing KBQA approaches, despite achieving strong performance on i.i.d. test data, often struggle in generalizing to questions involving unseen KB schema items. Prior ranking-based approaches have shown some success in generalization, but suffer from the coverage issue. We present RnG-KBQA, a Rank-and-Generate approach for KBQA, which remedies the coverage issue with a generation model while preserving a strong generalization capability. Our approach first uses a contrastive ranker to rank a set of candidate logical forms obtained by searching over the knowledge graph. It then introduces a tailored generation model conditioned on the question and the top-ranked candidates to compose the final logical form. We achieve new state-of-the-art results on GrailQA and WebQSP datasets. In particular, our method surpasses the prior state-of-the-art by a large margin on the GrailQA leaderboard. In addition, RnG-KBQA outperforms all prior approaches on the popular WebQSP benchmark, even including the ones that use the oracle entity linking. The experimental results demonstrate the effectiveness of the interplay between ranking and generation, which leads to the superior performance of our proposed approach across all settings with especially strong improvements in zero-shot generalization."
}

@inproceedings{gu-su-2022-arcaneqa,
    title = "{A}rcane{QA}: Dynamic Program Induction and Contextualized Encoding for Knowledge Base Question Answering",
    author = "Gu, Yu  and
      Su, Yu",
    editor = "Calzolari, Nicoletta  and
      Huang, Chu-Ren  and
      Kim, Hansaem  and
      Pustejovsky, James  and
      Wanner, Leo  and
      Choi, Key-Sun  and
      Ryu, Pum-Mo  and
      Chen, Hsin-Hsi  and
      Donatelli, Lucia  and
      Ji, Heng  and
      Kurohashi, Sadao  and
      Paggio, Patrizia  and
      Xue, Nianwen  and
      Kim, Seokhwan  and
      Hahm, Younggyun  and
      He, Zhong  and
      Lee, Tony Kyungil  and
      Santus, Enrico  and
      Bond, Francis  and
      Na, Seung-Hoon",
    booktitle = "Proceedings of the 29th International Conference on Computational Linguistics",
    month = oct,
    year = "2022",
    address = "Gyeongju, Republic of Korea",
    publisher = "International Committee on Computational Linguistics",
    url = "https://aclanthology.org/2022.coling-1.148/",
    pages = "1718--1731",
    abstract = "Question answering on knowledge bases (KBQA) poses a unique challenge for semantic parsing research due to two intertwined challenges: large search space and ambiguities in schema linking. Conventional ranking-based KBQA models, which rely on a candidate enumeration step to reduce the search space, struggle with flexibility in predicting complicated queries and have impractical running time. In this paper, we present ArcaneQA, a novel generation-based model that addresses both the large search space and the schema linking challenges in a unified framework with two mutually boosting ingredients: dynamic program induction for tackling the large search space and dynamic contextualized encoding for schema linking. Experimental results on multiple popular KBQA datasets demonstrate the highly competitive performance of ArcaneQA in both effectiveness and efficiency."
}

@misc{yu2023decafjointdecodinganswers,
      title={DecAF: Joint Decoding of Answers and Logical Forms for Question Answering over Knowledge Bases}, 
      author={Donghan Yu and Sheng Zhang and Patrick Ng and Henghui Zhu and Alexander Hanbo Li and Jun Wang and Yiqun Hu and William Wang and Zhiguo Wang and Bing Xiang},
      year={2023},
      eprint={2210.00063},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2210.00063}, 
}

@inproceedings{DBLP:conf/iclr/JiangZ0W23,
  author       = {Jinhao Jiang and
                  Kun Zhou and
                  Xin Zhao and
                  Ji{-}Rong Wen},
  title        = {UniKGQA: Unified Retrieval and Reasoning for Solving Multi-hop Question
                  Answering Over Knowledge Graph},
  booktitle    = {The Eleventh International Conference on Learning Representations,
                  {ICLR} 2023, Kigali, Rwanda, May 1-5, 2023},
  publisher    = {OpenReview.net},
  year         = {2023},
  url          = {https://openreview.net/forum?id=Z63RvyAZ2Vh},
  timestamp    = {Wed, 24 Jul 2024 16:50:33 +0200},
  biburl       = {https://dblp.org/rec/conf/iclr/JiangZ0W23.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}


@misc{coK,
      title={Chain-of-Knowledge: Integrating Knowledge Reasoning into Large Language Models by Learning from Knowledge Graphs}, 
      author={Yifei Zhang and Xintao Wang and Jiaqing Liang and Sirui Xia and Lida Chen and Yanghua Xiao},
      year={2024},
      eprint={2407.00653},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2407.00653}, 
}

@misc{generateOnGraph,
      title={Generate-on-Graph: Treat LLM as both Agent and KG in Incomplete Knowledge Graph Question Answering}, 
      author={Yao Xu and Shizhu He and Jiabei Chen and Zihao Wang and Yangqiu Song and Hanghang Tong and Guang Liu and Kang Liu and Jun Zhao},
      year={2024},
      eprint={2404.14741},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2404.14741}, 
}

@misc{reasoningefficientknowledgepathsknowledge,
      title={Reasoning on Efficient Knowledge Paths:Knowledge Graph Guides Large Language Model for Domain Question Answering}, 
      author={Yuqi Wang and Boran Jiang and Yi Luo and Dawei He and Peng Cheng and Liangcai Gao},
      year={2024},
      eprint={2404.10384},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2404.10384}, 
}

@article{KG_survey1,
   title={Unifying Large Language Models and Knowledge Graphs: A Roadmap},
   volume={36},
   ISSN={2326-3865},
   url={http://dx.doi.org/10.1109/TKDE.2024.3352100},
   DOI={10.1109/tkde.2024.3352100},
   number={7},
   journal={IEEE Transactions on Knowledge and Data Engineering},
   publisher={Institute of Electrical and Electronics Engineers (IEEE)},
   author={Pan, Shirui and Luo, Linhao and Wang, Yufei and Chen, Chen and Wang, Jiapu and Wu, Xindong},
   year={2024},
   month=jul, pages={3580–3599} }

@misc{trustworthyknowledgegraphreasoning,
      title={Towards Trustworthy Knowledge Graph Reasoning: An Uncertainty Aware Perspective}, 
      author={Bo Ni and Yu Wang and Lu Cheng and Erik Blasch and Tyler Derr},
      year={2024},
      eprint={2410.08985},
      archivePrefix={arXiv},
      primaryClass={cs.AI},
      url={https://arxiv.org/abs/2410.08985}, 
}

@inproceedings{enhancing-fact,
    title = "Enhancing Fact Verification with Causal Knowledge Graphs and Transformer-Based Retrieval for Deductive Reasoning",
    author = "Tan, Fiona Anting  and
      Desai, Jay  and
      Sengamedu, Srinivasan H.",
    editor = "Schlichtkrull, Michael  and
      Chen, Yulong  and
      Whitehouse, Chenxi  and
      Deng, Zhenyun  and
      Akhtar, Mubashara  and
      Aly, Rami  and
      Guo, Zhijiang  and
      Christodoulopoulos, Christos  and
      Cocarascu, Oana  and
      Mittal, Arpit  and
      Thorne, James  and
      Vlachos, Andreas",
    booktitle = "Proceedings of the Seventh Fact Extraction and VERification Workshop (FEVER)",
    month = nov,
    year = "2024",
    address = "Miami, Florida, USA",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2024.fever-1.20/",
    doi = "10.18653/v1/2024.fever-1.20",
    pages = "151--169",
    abstract = "The ability to extract and verify factual information from free-form text is critical in an era where vast amounts of unstructured data are available, yet unreliable sources abound. This paper focuses on enhancing causal deductive reasoning, a key component of factual verification, through the lens of accident investigation, where determining the probable causes of events is paramount. Deductive reasoning refers to the task of drawing conclusions based on a premise. While some deductive reasoning benchmarks exist, none focus on causal deductive reasoning and are from real-world applications. Recently, large language models (LLMs) used with prompt engineering techniques like retrieval-augmented generation (RAG) have demonstrated remarkable performance across various natural language processing benchmarks. However, adapting these techniques to handle scenarios with no knowledge bases and to different data structures, such as graphs, remains an ongoing challenge. In our study, we introduce a novel framework leveraging LLMs' decent ability to detect and infer causal relations to construct a causal Knowledge Graph (KG) which represents knowledge that the LLM recognizes. Additionally, we propose a RoBERTa-based Transformer Graph Neural Network (RoTG) specifically designed to select relevant nodes within this KG. Integrating RoTG-retrieved causal chains into prompts effectively enhances LLM performance, demonstrating usefulness of our approach in advancing LLMs' causal deductive reasoning capabilities."
}

@article{10.1145/3571730,
author = {Ji, Ziwei and Lee, Nayeon and Frieske, Rita and Yu, Tiezheng and Su, Dan and Xu, Yan and Ishii, Etsuko and Bang, Ye Jin and Madotto, Andrea and Fung, Pascale},
title = {Survey of Hallucination in Natural Language Generation},
year = {2023},
issue_date = {December 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {55},
number = {12},
issn = {0360-0300},
url = {https://doi.org/10.1145/3571730},
doi = {10.1145/3571730},
abstract = {Natural Language Generation (NLG) has improved exponentially in recent years thanks to the development of sequence-to-sequence deep learning technologies such as Transformer-based language models. This advancement has led to more fluent and coherent NLG, leading to improved development in downstream tasks such as abstractive summarization, dialogue generation, and data-to-text generation. However, it is also apparent that deep learning based generation is prone to hallucinate unintended text, which degrades the system performance and fails to meet user expectations in many real-world scenarios. To address this issue, many studies have been presented in measuring and mitigating hallucinated texts, but these have never been reviewed in a comprehensive manner before.In this survey, we thus provide a broad overview of the research progress and challenges in the hallucination problem in NLG. The survey is organized into two parts: (1) a general overview of metrics, mitigation methods, and future directions, and (2) an overview of task-specific research progress on hallucinations in the following downstream tasks, namely abstractive summarization, dialogue generation, generative question answering, data-to-text generation, and machine translation. This survey serves to facilitate collaborative efforts among researchers in tackling the challenge of hallucinated texts in NLG.},
journal = {ACM Comput. Surv.},
month = mar,
articleno = {248},
numpages = {38},
keywords = {Hallucination, intrinsic hallucination, extrinsic hallucination, faithfulness in NLG, factuality in NLG, consistency in NLG}
}

@misc{wen2024knowlimitssurveyabstention,
      title={Know Your Limits: A Survey of Abstention in Large Language Models}, 
      author={Bingbing Wen and Jihan Yao and Shangbin Feng and Chenjun Xu and Yulia Tsvetkov and Bill Howe and Lucy Lu Wang},
      year={2024},
      eprint={2407.18418},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2407.18418}, 
}

@inproceedings{asai-choi-2021-challenges,
    title = "Challenges in Information-Seeking {QA}: Unanswerable Questions and Paragraph Retrieval",
    author = "Asai, Akari  and
      Choi, Eunsol",
    editor = "Zong, Chengqing  and
      Xia, Fei  and
      Li, Wenjie  and
      Navigli, Roberto",
    booktitle = "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)",
    month = aug,
    year = "2021",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.acl-long.118/",
    doi = "10.18653/v1/2021.acl-long.118",
    pages = "1492--1504",
    abstract = "Recent pretrained language models {\textquotedblleft}solved{\textquotedblright} many reading comprehension benchmarks, where questions are written with access to the evidence document. However, datasets containing information-seeking queries where evidence documents are provided after the queries are written independently remain challenging. We analyze why answering information-seeking queries is more challenging and where their prevalent unanswerabilities arise, on Natural Questions and TyDi QA. Our controlled experiments suggest two headrooms {--} paragraph selection and answerability prediction, i.e. whether the paired evidence document contains the answer to the query or not. When provided with a gold paragraph and knowing when to abstain from answering, existing models easily outperform a human annotator. However, predicting answerability itself remains challenging. We manually annotate 800 unanswerable examples across six languages on what makes them challenging to answer. With this new data, we conduct per-category answerability prediction, revealing issues in the current dataset collection as well as task formulation. Together, our study points to avenues for future research in information-seeking question answering, both for dataset creation and model development. Our code and annotated data is publicly available at \url{https://github.com/AkariAsai/unanswerable_qa}."
}

@inproceedings{cole-etal-2023-selectively,
    title = "Selectively Answering Ambiguous Questions",
    author = "Cole, Jeremy  and
      Zhang, Michael  and
      Gillick, Daniel  and
      Eisenschlos, Julian  and
      Dhingra, Bhuwan  and
      Eisenstein, Jacob",
    editor = "Bouamor, Houda  and
      Pino, Juan  and
      Bali, Kalika",
    booktitle = "Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing",
    month = dec,
    year = "2023",
    address = "Singapore",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2023.emnlp-main.35/",
    doi = "10.18653/v1/2023.emnlp-main.35",
    pages = "530--543",
    abstract = "Trustworthy language models should abstain from answering questions when they do not know the answer. However, the answer to a question can be unknown for a variety of reasons. Prior research has focused on the case in which the question is clear and the answer is unambiguous but possibly unknown. However, the answer to a question can also be unclear due to uncertainty of the questioner`s intent or context. We investigate question answering from this perspective, focusing on answering a subset of questions with a high degree of accuracy, from a set of questions in which many are inherently ambiguous. In this setting, we find that the most reliable approach to calibration involves quantifying repetition within a set of sampled model outputs, rather than the model`s likelihood or self-verification as used in prior work. We find this to be the case across different types of uncertainty, varying model scales and both with or without instruction tuning. Our results suggest that sampling-based confidence scores help calibrate answers to relatively unambiguous questions, with more dramatic improvements on ambiguous questions."
}

@inproceedings{feng-etal-2024-dont,
    title = "Don`t Hallucinate, Abstain: Identifying {LLM} Knowledge Gaps via Multi-{LLM} Collaboration",
    author = "Feng, Shangbin  and
      Shi, Weijia  and
      Wang, Yike  and
      Ding, Wenxuan  and
      Balachandran, Vidhisha  and
      Tsvetkov, Yulia",
    editor = "Ku, Lun-Wei  and
      Martins, Andre  and
      Srikumar, Vivek",
    booktitle = "Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = aug,
    year = "2024",
    address = "Bangkok, Thailand",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2024.acl-long.786/",
    doi = "10.18653/v1/2024.acl-long.786",
    pages = "14664--14690",
    abstract = "Despite efforts to expand the knowledge of large language models (LLMs), knowledge gaps{---}missing or outdated information in LLMs{---}might always persist given the evolving nature of knowledge. In this work, we study approaches to identify LLM knowledge gaps and abstain from answering questions when knowledge gaps are present. We first adapt existing approaches to model calibration or adaptation through fine-tuning/prompting and analyze their ability to abstain from generating low-confidence outputs. Motivated by their failures in self-reflection and over-reliance on held-out sets, we propose two novel approaches that are based on model collaboration, i.e., LLMs probing other LLMs for knowledge gaps, either cooperatively or competitively. Extensive experiments with three LLMs on four QA tasks featuring diverse knowledge domains demonstrate that both cooperative and competitive approaches to unveiling LLM knowledge gaps achieve up to 19.3{\%} improvements on abstain accuracy against the strongest baseline. Further analysis reveals that our abstention methods pinpoint failure cases in retrieval augmentation and knowledge gaps in multi-hop reasoning."
}


@misc{tomani2024uncertaintybasedabstentionllmsimproves,
      title={Uncertainty-Based Abstention in LLMs Improves Safety and Reduces Hallucinations}, 
      author={Christian Tomani and Kamalika Chaudhuri and Ivan Evtimov and Daniel Cremers and Mark Ibrahim},
      year={2024},
      eprint={2404.10960},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2404.10960}, 
}

@misc{lee2024trustsqlbenchmarkingtexttosqlreliability,
      title={TrustSQL: Benchmarking Text-to-SQL Reliability with Penalty-Based Scoring}, 
      author={Gyubok Lee and Woosog Chay and Seonhee Cho and Edward Choi},
      year={2024},
      eprint={2403.15879},
      archivePrefix={arXiv},
      primaryClass={cs.AI},
      url={https://arxiv.org/abs/2403.15879}, 
}

@inproceedings{whitehead2022reliable,
  title={Reliable visual question answering: Abstain rather than answer incorrectly},
  author={Whitehead, Spencer and Petryk, Suzanne and Shakib, Vedaad and Gonzalez, Joseph and Darrell, Trevor and Rohrbach, Anna and Rohrbach, Marcus},
  booktitle={European Conference on Computer Vision},
  pages={148--166},
  year={2022},
  organization={Springer}
}

@inproceedings{kirk-etal-2023-past,
    title = "The Past, Present and Better Future of Feedback Learning in Large Language Models for Subjective Human Preferences and Values",
    author = {Kirk, Hannah Rose  and
      Bean, Andrew M.  and
      Vidgen, Bertie  and
      R{\"o}ttger, Paul  and
      Hale, Scott A.},
    editor = "Bouamor, Houda  and
      Pino, Juan  and
      Bali, Kalika",
    booktitle = "Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing",
    month = dec,
    year = "2023",
    address = "Singapore",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2023.emnlp-main.148/",
    doi = "10.18653/v1/2023.emnlp-main.148",
    pages = "2409--2430",
    abstract = "Human feedback is increasingly used to steer the behaviours of Large Language Models (LLMs). However, it is unclear how to collect and incorporate feedback in a way that is efficient, effective and unbiased, especially for highly subjective human preferences and values. In this paper, we survey existing approaches for learning from human feedback, drawing on 95 papers primarily from the ACL and arXiv repositories. First, we summarise the past, pre-LLM trends for integrating human feedback into language models. Second, we give an overview of present techniques and practices, as well as the motivations for using feedback; conceptual frameworks for defining values and preferences; and how feedback is collected and from whom. Finally, we encourage a better future of feedback learning in LLMs by raising five unresolved conceptual and practical challenges."
}


@inproceedings{buchmann-etal-2024-attribute,
    title = "Attribute or Abstain: Large Language Models as Long Document Assistants",
    author = "Buchmann, Jan  and
      Liu, Xiao  and
      Gurevych, Iryna",
    editor = "Al-Onaizan, Yaser  and
      Bansal, Mohit  and
      Chen, Yun-Nung",
    booktitle = "Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing",
    month = nov,
    year = "2024",
    address = "Miami, Florida, USA",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2024.emnlp-main.463/",
    doi = "10.18653/v1/2024.emnlp-main.463",
    pages = "8113--8140",
    abstract = "LLMs can help humans working with long documents, but are known to hallucinate. *Attribution* can increase trust in LLM responses: The LLM provides evidence that supports its response, which enhances verifiability. Existing approaches to attribution have only been evaluated in RAG settings, where the initial retrieval confounds LLM performance. This is crucially different from the long document setting, where retrieval is not needed, but could help. Thus, a long document specific evaluation of attribution is missing. To fill this gap, we present LAB, a benchmark of 6 diverse long document tasks with attribution, and experiments with different approaches to attribution on 5 LLMs of different sizes. We find that *citation*, i.e. response generation and evidence extraction in one step, performs best for large and fine-tuned models, while additional retrieval can help for small, prompted models. We investigate whether the {\textquotedblleft}Lost in the Middle{\textquotedblright} phenomenon exists for attribution, but do not find this. We also find that evidence quality can predict response quality on datasets with simple responses, but not so for complex responses, as models struggle with providing evidence for complex claims. We release code and data for further investigation. [Link](https://github.com/UKPLab/arxiv2024-attribute-or-abstain)"
}

@inproceedings{amayuelas-etal-2024-knowledge,
    title = "Knowledge of Knowledge: Exploring Known-Unknowns Uncertainty with Large Language Models",
    author = "Amayuelas, Alfonso  and
      Wong, Kyle  and
      Pan, Liangming  and
      Chen, Wenhu  and
      Wang, William Yang",
    editor = "Ku, Lun-Wei  and
      Martins, Andre  and
      Srikumar, Vivek",
    booktitle = "Findings of the Association for Computational Linguistics: ACL 2024",
    month = aug,
    year = "2024",
    address = "Bangkok, Thailand",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2024.findings-acl.383/",
    doi = "10.18653/v1/2024.findings-acl.383",
    pages = "6416--6432",
    abstract = "This paper investigates the capabilities of Large Language Models (LLMs) in understanding their knowledge and uncertainty over questions. Specifically, we focus on addressing known-unknown questions, characterized by high uncertainty due to the absence of definitive answers. To facilitate our study, we collect a new dataset with Known-Unknown Questions (KUQ) and establish a categorization framework to clarify the origins of uncertainty in such queries. Subsequently, we examine the performance of open-source LLMs, fine-tuned using this dataset, in distinguishing between known and unknown queries within open-ended question-answering scenarios. The fine-tuned models demonstrated a significant improvement, achieving a considerable increase in F1-score relative to their pre-fine-tuning state. Through a comprehensive analysis, we reveal insights into the models' improved uncertainty articulation and their consequent efficacy in multi-agent debates. These findings help us understand how LLMs can be trained to identify and express uncertainty, improving our knowledge of how they understand and express complex or unclear information."
}


@inproceedings{wen-etal-2024-characterizing,
    title = "Characterizing {LLM} Abstention Behavior in Science {QA} with Context Perturbations",
    author = "Wen, Bingbing  and
      Howe, Bill  and
      Wang, Lucy Lu",
    editor = "Al-Onaizan, Yaser  and
      Bansal, Mohit  and
      Chen, Yun-Nung",
    booktitle = "Findings of the Association for Computational Linguistics: EMNLP 2024",
    month = nov,
    year = "2024",
    address = "Miami, Florida, USA",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2024.findings-emnlp.197/",
    doi = "10.18653/v1/2024.findings-emnlp.197",
    pages = "3437--3450",
    abstract = "The correct model response in the face of uncertainty is to abstain from answering a question so as not to mislead the user. In this work, we study the ability of LLMs to abstain from answering context-dependent science questions when provided insufficient or incorrect context. We probe model sensitivity in several settings: removing gold context, replacing gold context with irrelevant context, and providing additional context beyond what is given. In experiments on four QA datasets with six LLMs, we show that performance varies greatly across models, across the type of context provided, and also by question type; in particular, many LLMs seem unable to abstain from answering boolean questions using standard QA prompts. Our analysis also highlights the unexpected impact of abstention performance on QA task accuracy. Counter-intuitively, in some settings, replacing gold context with irrelevant context or adding irrelevant context to gold context can improve abstention performance in a way that results in improvements in task performance. Our results imply that changes are needed in QA dataset design and evaluation to more effectively assess the correctness and downstream impacts of model abstention."
}

@misc{yang2024alignmenthonesty,
      title={Alignment for Honesty}, 
      author={Yuqing Yang and Ethan Chern and Xipeng Qiu and Graham Neubig and Pengfei Liu},
      year={2024},
      eprint={2312.07000},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2312.07000}, 
}

@misc{zhu2024llmsknowledgegraphconstruction,
      title={LLMs for Knowledge Graph Construction and Reasoning: Recent Capabilities and Future Opportunities}, 
      author={Yuqi Zhu and Xiaohan Wang and Jing Chen and Shuofei Qiao and Yixin Ou and Yunzhi Yao and Shumin Deng and Huajun Chen and Ningyu Zhang},
      year={2024},
      eprint={2305.13168},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2305.13168}, 
}

@misc{lacroix2020tensordecompositionstemporalknowledge,
      title={Tensor Decompositions for temporal knowledge base completion}, 
      author={Timothée Lacroix and Guillaume Obozinski and Nicolas Usunier},
      year={2020},
      eprint={2004.04926},
      archivePrefix={arXiv},
      primaryClass={stat.ML},
      url={https://arxiv.org/abs/2004.04926}, 
}

@article{lehmann2015dbpedia,
  title={Dbpedia--a large-scale, multilingual knowledge base extracted from wikipedia},
  author={Lehmann, Jens and Isele, Robert and Jakob, Max and Jentzsch, Anja and Kontokostas, Dimitris and Mendes, Pablo N and Hellmann, Sebastian and Morsey, Mohamed and Van Kleef, Patrick and Auer, S{\"o}ren and others},
  journal={Semantic web},
  volume={6},
  number={2},
  pages={167--195},
  year={2015},
  publisher={IOS Press}
}

@inproceedings{park2021knowledge,
  title={Knowledge graph-based question answering with electronic health records},
  author={Park, Junwoo and Cho, Youngwoo and Lee, Haneol and Choo, Jaegul and Choi, Edward},
  booktitle={Machine Learning for Healthcare Conference},
  pages={36--53},
  year={2021},
  organization={PMLR}
}