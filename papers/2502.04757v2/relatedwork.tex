\section{Related Work}
% \subsection{Vulnerabilities in VLM Safety}
% VLMs remain vulnerable to various jailbreak attacks, with the addition of visual representations further amplifying these vulnerabilities. For example, simply inputting blank images can significantly increase the attack success rate~\cite{li2025images}, and prompts displayed in typographic images can also be exploited for attacks~\cite{gong2023figstep}. Additionally, high-dimensional visual embeddings, which encode complex image features into dense vector representations, increase vulnerability to adversarial attacks by enabling exploitation through adversarial manipulations embedded within the images~\cite{qi2023visual, shayegani2023jailbreak}. Generating safe responses often necessitates complex reasoning that seamlessly integrates visual and textual contexts. For instance, when confronted with images showing inappropriate locations or tools for specific harmful activities, the model must effectively combine insights from multiple modalities to ensure it avoids generating unsafe responses~\cite{siuo2024}.

%\subsection{Safety Evaluation for VLMs}
To address vulnerabilities and evaluate the safety of VLMs, various benchmarks have been developed. VLGuard~\cite{vlguard2024} introduces a fine-grained evaluation benchmark that focuses on visual-linguistic reasoning, leveraging a taxonomy that categorizes potential safety issues. MM-SafetyBench~\cite{mmsafetybench2025} provides a comprehensive benchmark with image-text pairs across 13 safety-critical scenarios, emphasizing image-based manipulations and their impact on VLMs' responses. MLLMGuard~\cite{mllmguard2024} evaluates safety across five dimensions—privacy, bias, toxicity, truthfulness, and legality—using a bilingual benchmark. Besides these benchmarks, others such as SPA-VL~\cite{spavl2024}, JailbreakV-28K~\cite{jailbreak28k2024}, and SIUO~\cite{siuo2024}, contribute to evaluating vulnerabilities and enhancing the robustness of VLMs.

For safety evaluation, automated evaluators using language models have been adopted due to the high cost of human evaluators~\cite{vlguard2024, jailbreak28k2024, mmsafetybench2025, spavl2024, rtvlm2024}. Additionally, safeguard models for safety evaluation have been developed~\cite{inan2023llamaguard, chi2024llamaguardvision, mllmguard2024}. These automated evaluations commonly rely on metrics such as ASR. However, relying solely on ASR, a simple binary classification that deems an attack successful as long as the model does not refuse the instruction, can lead to an overestimation of jailbreak effectiveness and discrepancies with human judgment~\cite{strongreject2024}. To address this issue, recent evaluation methods have integrated the level of detail in responses as an additional assessment criterion~\cite{strongreject2024, o12024, guan2024deliberative}.