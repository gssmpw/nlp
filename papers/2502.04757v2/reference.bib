@inproceedings{vlguard2024,
    title={Safety Fine-Tuning at (Almost) No Cost: A Baseline for Vision Large Language Models},
    author={Yongshuo Zong and Ondrej Bohdal and Tingyang Yu and Yongxin Yang and Timothy Hospedales},
    booktitle={Forty-first International Conference on Machine Learning},
    year={2024},
    url={https://openreview.net/forum?id=bWZKvF0g7G}
}

@inproceedings{mllmguard2024,
    title={{MLLMG}uard: A Multi-dimensional Safety Evaluation Suite for Multimodal Large Language Models},
    author={Tianle Gu and Zeyang Zhou and Kexin Huang and Liang Dandan and Yixu Wang and Haiquan Zhao and Yuanqi Yao and xingge qiao and Keqing wang and Yujiu Yang and Yan Teng and Yu Qiao and Yingchun Wang},
    booktitle={The Thirty-eight Conference on Neural Information Processing Systems Datasets and Benchmarks Track},
    year={2024},
    url={https://openreview.net/forum?id=k4tuZmvSnl}
}

@inproceedings{strongreject2024,
    title={A Strong{REJECT} for Empty Jailbreaks},
    author={Alexandra Souly and Qingyuan Lu and Dillon Bowen and Tu Trinh and Elvis Hsieh and Sana Pandey and Pieter Abbeel and Justin Svegliato and Scott Emmons and Olivia Watkins and Sam Toyer},
    booktitle={The Thirty-eight Conference on Neural Information Processing Systems Datasets and Benchmarks Track},
    year={2024},
    url={https://openreview.net/forum?id=KZLE5BaaOH}
}

@inproceedings{jailbreak28k2024,
    title={JailBreakV: A Benchmark for Assessing the Robustness of MultiModal Large Language Models against Jailbreak Attacks},
    author={Weidi Luo and Siyuan Ma and Xiaogeng Liu and Xiaoyu Guo and Chaowei Xiao},
    booktitle={First Conference on Language Modeling},
    year={2024},
    url={https://openreview.net/forum?id=GC4mXVfquq}
}

@inproceedings{mmsafetybench2025,
  title={Mm-safetybench: A benchmark for safety evaluation of multimodal large language models},
  author={Liu, Xin and Zhu, Yichen and Gu, Jindong and Lan, Yunshi and Yang, Chao and Qiao, Yu},
  booktitle={European Conference on Computer Vision},
  pages={386--403},
  year={2025},
  organization={Springer}
}

@inproceedings{rtvlm2024,
    title = "Red Teaming Visual Language Models",
    author = "Li, Mukai  and
      Li, Lei  and
      Yin, Yuwei  and
      Ahmed, Masood  and
      Liu, Zhenguang  and
      Liu, Qi",
    editor = "Ku, Lun-Wei  and
      Martins, Andre  and
      Srikumar, Vivek",
    booktitle = "Findings of the Association for Computational Linguistics: ACL 2024",
    month = aug,
    year = "2024",
    address = "Bangkok, Thailand",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2024.findings-acl.198/",
    doi = "10.18653/v1/2024.findings-acl.198",
    pages = "3326--3342"
}

@article{siuo2024,
  title={Cross-modality safety alignment},
  author={Wang, Siyin and Ye, Xingsong and Cheng, Qinyuan and Duan, Junwen and Li, Shimin and Fu, Jinlan and Qiu, Xipeng and Huang, Xuanjing},
  journal={arXiv preprint arXiv:2406.15279},
  year={2024}
}

@misc{spavl2024,
  title={SPA-VL: A Comprehensive Safety Preference Alignment Dataset for Vision Language Model},
  author={Zhang, Yongting and Chen, Lu and Zheng, Guodong and Gao, Yifeng and Zheng, Rui and Fu, Jinlan and Yin, Zhenfei and Jin, Senjie and Qiao, Yu and Huang, Xuanjing and others},
  journal={arXiv preprint arXiv:2406.12030},
  year={2024}
}

@article{gong2023figstep,
  title={Figstep: Jailbreaking large vision-language models via typographic visual prompts},
  author={Gong, Yichen and Ran, Delong and Liu, Jinyuan and Wang, Conglei and Cong, Tianshuo and Wang, Anyu and Duan, Sisi and Wang, Xiaoyun},
  journal={arXiv preprint arXiv:2311.05608},
  year={2023},
  note = {Accepted at AAAI 2025}
}



@article{mlcommons0.5_2024,
  title={Introducing v0. 5 of the ai safety benchmark from mlcommons},
  author={Vidgen, Bertie and Agrawal, Adarsh and Ahmed, Ahmed M and Akinwande, Victor and Al-Nuaimi, Namir and Alfaraj, Najla and Alhajjar, Elie and Aroyo, Lora and Bavalatti, Trupti and Bartolo, Max and others},
  journal={arXiv preprint arXiv:2404.12241},
  year={2024}
}

@article{gpt4o2024,
  title={Gpt-4o system card},
  author={OpenAI},
  journal={arXiv preprint arXiv:2410.21276},
  year={2024}
}

@misc{gpt4v2023,
  title={GPT-4V(ision) System Card},
  author={OpenAI},
  year={2023},
  url={https://cdn.openai.com/papers/GPTV_System_Card.pdf}
}

@article{o12024,
  title={OpenAI o1 System Card},
  author={OpenAI},
  journal={arXiv preprint arXiv:2412.16720},
  year={2024}
}

@article{bommasani2021opportunities,
  title={On the opportunities and risks of foundation models},
  author={Bommasani, Rishi and Hudson, Drew A and Adeli, Ehsan and Altman, Russ and Arora, Simran and von Arx, Sydney and Bernstein, Michael S and Bohg, Jeannette and Bosselut, Antoine and Brunskill, Emma and others},
  journal={arXiv preprint arXiv:2108.07258},
  year={2022}
}

@article{rottger2024safetyprompts,
  title={Safetyprompts: a systematic review of open datasets for evaluating and improving large language model safety},
  author={R{\"o}ttger, Paul and Pernisi, Fabio and Vidgen, Bertie and Hovy, Dirk},
  journal={arXiv preprint arXiv:2404.05399},
  year={2025},
  note = {Accepted at AAAI 2025 (AI Alignment Track)}
}

@inproceedings{wei2024jailbroken,
    author = {Wei, Alexander and Haghtalab, Nika and Steinhardt, Jacob},
    title = {Jailbroken: how does LLM safety training fail?},
    year = {2023},
    publisher = {Curran Associates Inc.},
    address = {Red Hook, NY, USA},
    booktitle = {Proceedings of the 37th International Conference on Neural Information Processing Systems},
    articleno = {3508},
    numpages = {32},
    location = {New Orleans, LA, USA},
    series = {NIPS '23}
}

@article{qi2023visual,
  title={Visual Adversarial Examples Jailbreak Aligned Large Language Models},
  volume={38},
  url={https://ojs.aaai.org/index.php/AAAI/article/view/30150},
  DOI={10.1609/aaai.v38i19.30150},
  number={19},
  journal={Proceedings of the AAAI Conference on Artificial Intelligence},
  author={Qi, Xiangyu and Huang, Kaixuan and Panda, Ashwinee and Henderson, Peter and Wang, Mengdi and Mittal, Prateek},
  year={2024},
  month={Mar.},
  pages={21527-21536}
}

@article{inan2023llamaguard,
  title={Llama guard: Llm-based input-output safeguard for human-ai conversations},
  author={Inan, Hakan and Upasani, Kartikeya and Chi, Jianfeng and Rungta, Rashi and Iyer, Krithika and Mao, Yuning and Tontchev, Michael and Hu, Qing and Fuller, Brian and Testuggine, Davide and others},
  journal={arXiv preprint arXiv:2312.06674},
  year={2023}
}

@article{chi2024llamaguardvision,
  title={Llama Guard 3 Vision: Safeguarding Human-AI Image Understanding Conversations},
  author={Chi, Jianfeng and Karn, Ujjwal and Zhan, Hongyuan and Smith, Eric and Rando, Javier and Zhang, Yiming and Plawiak, Kate and Coudert, Zacharie Delpierre and Upasani, Kartikeya and Pasupuleti, Mahesh},
  journal={arXiv preprint arXiv:2411.10414},
  year={2024}
}

@inproceedings{li2025images,
  title={Images are achilles’ heel of alignment: Exploiting visual vulnerabilities for jailbreaking multimodal large language models},
  author={Li, Yifan and Guo, Hangyu and Zhou, Kun and Zhao, Wayne Xin and Wen, Ji-Rong},
  booktitle={European Conference on Computer Vision},
  pages={174--189},
  year={2025},
  organization={Springer}
}

@inproceedings{shayegani2023jailbreak,
  title={Jailbreak in pieces: Compositional adversarial attacks on multi-modal language models},
  author={Shayegani, Erfan and Dong, Yue and Abu-Ghazaleh, Nael},
  booktitle={The Twelfth International Conference on Learning Representations},
  year={2023}
}

@article{wang2023large,
  title={Large language models are not fair evaluators},
  author={Wang, Peiyi and Li, Lei and Chen, Liang and Cai, Zefan and Zhu, Dawei and Lin, Binghuai and Cao, Yunbo and Liu, Qi and Liu, Tianyu and Sui, Zhifang},
  booktitle = "Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
  journal={arXiv preprint arXiv:2305.17926},
  year={2023}
}

@inproceedings{zhang2023chatgpt,
  title={Is chatgpt fair for recommendation? evaluating fairness in large language model recommendation},
  author={Zhang, Jizhi and Bao, Keqin and Zhang, Yang and Wang, Wenjie and Feng, Fuli and He, Xiangnan},
  booktitle={Proceedings of the 17th ACM Conference on Recommender Systems},
  pages={993--999},
  year={2023}
}

@misc{deepmind2022taxonomy,
  title={Taxonomy of Risks posed by Language Models},
  author = {Weidinger, Laura and Uesato, Jonathan and Rauh, Maribeth and Griffin, Conor and Huang, Po-Sen and Mellor, John and Glaese, Amelia and Cheng, Myra and Balle, Borja and Kasirzadeh, Atoosa and Biles, Courtney and Brown, Sasha and Kenton, Zac and Hawkins, Will and Stepleton, Tom and Birhane, Abeba and Hendricks, Lisa Anne and Rimell, Laura and Isaac, William and Haas, Julia and Legassick, Sean and Irving, Geoffrey and Gabriel, Iason},
  year = {2022},
  publisher = {Association for Computing Machinery},
  url = {https://doi.org/10.1145/3531146.3533088},
  doi = {10.1145/3531146.3533088},
  pages = {214–229},
  numpages = {16},
  series = {FAccT '22}
}

@inproceedings{zheng2023judging,
    author = {Zheng, Lianmin and Chiang, Wei-Lin and Sheng, Ying and Zhuang, Siyuan and Wu, Zhanghao and Zhuang, Yonghao and Lin, Zi and Li, Zhuohan and Li, Dacheng and Xing, Eric P. and Zhang, Hao and Gonzalez, Joseph E. and Stoica, Ion},
    title = {Judging LLM-as-a-judge with MT-bench and Chatbot Arena},
    year = {2023},
    publisher = {Curran Associates Inc.},
    address = {Red Hook, NY, USA},
    booktitle = {Proceedings of the 37th International Conference on Neural Information Processing Systems},
    articleno = {2020},
    numpages = {29},
    location = {New Orleans, LA, USA},
    series = {NIPS '23}
}




@inproceedings{qiao2024prism,
    title={Prism: A Framework for Decoupling and Assessing the Capabilities of {VLM}s},
    author={Yuxuan Qiao and Haodong Duan and Xinyu Fang and Junming Yang and Lin Chen and Songyang Zhang and Jiaqi Wang and Dahua Lin and Kai Chen},
    booktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},
    year={2024},
    url={https://openreview.net/forum?id=qLnXPVvwLx}
}

@inproceedings{chen2024mllm,
    author = {Chen, Dongping and Chen, Ruoxi and Zhang, Shilin and Wang, Yaochen and Liu, Yinuo and Zhou, Huichi and Zhang, Qihui and Wan, Yao and Zhou, Pan and Sun, Lichao},
    title = {MLLM-as-a-Judge: assessing multimodal LLM-as-a-Judge with vision-language benchmark},
    year = {2024},
    publisher = {JMLR.org},
    booktitle = {Proceedings of the 41st International Conference on Machine Learning},
    articleno = {254},
    numpages = {34},
    location = {Vienna, Austria},
    series = {ICML'24}
}

@article{guan2024deliberative,
  title={Deliberative alignment: Reasoning enables safer language models},
  author={Guan, Melody Y and Joglekar, Manas and Wallace, Eric and Jain, Saachi and Barak, Boaz and Heylar, Alec and Dias, Rachel and Vallone, Andrea and Ren, Hongyu and Wei, Jason and others},
  journal={arXiv preprint arXiv:2412.16339},
  year={2024}
}

@inproceedings{klyman2024acceptable,
  title={Acceptable Use Policies for Foundation Models},
  author={Klyman, Kevin},
  booktitle={Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society},
  volume={7},
  pages={752--767},
  year={2024}
}

@misc{mlcommons_1.0,
  author       = "{MLCommons}",
  title        = "AILuminate Benchmarks",
  howpublished = "\url{https://ailuminate.mlcommons.org/benchmarks/}",
  note         = "Accessed: 2025-01-12"
}

@article{paligemma2vlm,
    title={PaliGemma 2: A Family of Versatile VLMs for Transfer},
    author={Andreas Steiner and André Susano Pinto and Michael Tschannen and Daniel Keysers and Xiao Wang and Yonatan Bitton and Alexey Gritsenko and Matthias Minderer and Anthony Sherbondy and Shangbang Long and Siyang Qin and Reeve Ingle and Emanuele Bugliarello and Sahar Kazemzadeh and Thomas Mesnard and Ibrahim Alabdulmohsin and Lucas Beyer and Xiaohua Zhai},
    year={2024},
    journal={arXiv preprint arXiv:2412.03555}
}

@inproceedings{carlini2024aligned,
    author = {Carlini, Nicholas and Nasr, Milad and Choquette-Choo, Christopher A. and Jagielski, Matthew and Gao, Irena and Awadalla, Anas and Koh, Pang Wei and Ippolito, Daphne and Lee, Katherine and Tramer, Florian and Schmidt, Ludwig},
    title = {Are aligned neural networks adversarially aligned?},
    year = {2023},
    publisher = {Curran Associates Inc.},
    address = {Red Hook, NY, USA},
    booktitle = {Proceedings of the 37th International Conference on Neural Information Processing Systems},
    articleno = {2687},
    numpages = {23},
    location = {New Orleans, LA, USA},
    series = {NIPS '23}
}


@inproceedings{llava-v1.5,
    author    = {Haotian Liu and Chunyuan Li and Yuheng Li and Yong Jae Lee},
    title     = {Improved Baselines with Visual Instruction Tuning},
    booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
    year      = {2024},
    pages     = {26296--26306},
    publisher = {IEEE},
    url       = {https://openaccess.thecvf.com/content/CVPR2024/papers/Liu_Improved_Baselines_with_Visual_Instruction_Tuning_CVPR_2024_paper.pdf}
}

@misc{deepseek-vl,
      title={DeepSeek-VL: Towards Real-World Vision-Language Understanding}, 
      author={Haoyu Lu and Wen Liu and Bo Zhang and Bingxuan Wang and Kai Dong and Bo Liu and Jingxiang Sun and Tongzheng Ren and Zhuoshu Li and Hao Yang and Yaofeng Sun and Chengqi Deng and Hanwei Xu and Zhenda Xie and Chong Ruan},
      year={2024},
      eprint={2403.05525},
      archivePrefix={arXiv},
      primaryClass={cs.AI},
      url={https://arxiv.org/abs/2403.05525}, 
}

@misc{deepseek-vl2,
      title={DeepSeek-VL2: Mixture-of-Experts Vision-Language Models for Advanced Multimodal Understanding}, 
      author={Zhiyu Wu and Xiaokang Chen and Zizheng Pan and Xingchao Liu and Wen Liu and Damai Dai and Huazuo Gao and Yiyang Ma and Chengyue Wu and Bingxuan Wang and Zhenda Xie and Yu Wu and Kai Hu and Jiawei Wang and Yaofeng Sun and Yukun Li and Yishi Piao and Kang Guan and Aixin Liu and Xin Xie and Yuxiang You and Kai Dong and Xingkai Yu and Haowei Zhang and Liang Zhao and Yisong Wang and Chong Ruan},
      year={2024},
      eprint={2412.10302},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2412.10302}, 
}

@inproceedings{sharegpt4v,
    title={Sharegpt4v: Improving large multi-modal models with better captions},
    author={Chen, Lin and Li, Jinsong and Dong, Xiaoyi and Zhang, Pan and He, Conghui and Wang, Jiaqi and Zhao, Feng and Lin, Dahua},
    booktitle={European Conference on Computer Vision},
    pages={370--387},
    year={2025},
    organization={Springer}
}

@article{phi,
  title={Phi-3 technical report: A highly capable language model locally on your phone},
  author={Abdin, Marah and Aneja, Jyoti and Awadalla, Hany and Awadallah, Ahmed and Awan, Ammar Ahmad and Bach, Nguyen and Bahree, Amit and Bakhtiari, Arash and Bao, Jianmin and Behl, Harkirat and others},
  journal={arXiv preprint arXiv:2404.14219},
  year={2024}
}

@misc{pixtral,
      title={Pixtral 12B}, 
      author={Pravesh Agrawal and Szymon Antoniak and Emma Bou Hanna and Baptiste Bout and Devendra Chaplot and Jessica Chudnovsky and Diogo Costa and Baudouin De Monicault and Saurabh Garg and Theophile Gervet and Soham Ghosh and Amélie Héliou and Paul Jacob and Albert Q. Jiang and Kartik Khandelwal and Timothée Lacroix and Guillaume Lample and Diego Las Casas and Thibaut Lavril and Teven Le Scao and Andy Lo and William Marshall and Louis Martin and Arthur Mensch and Pavankumar Muddireddy and Valera Nemychnikova and Marie Pellat and Patrick Von Platen and Nikhil Raghuraman and Baptiste Rozière and Alexandre Sablayrolles and Lucile Saulnier and Romain Sauvestre and Wendy Shang and Roman Soletskyi and Lawrence Stewart and Pierre Stock and Joachim Studnia and Sandeep Subramanian and Sagar Vaze and Thomas Wang and Sophia Yang},
      year={2024},
      eprint={2410.07073},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2410.07073}, 
}

@misc{qwen2-vl,
      title={Qwen2-VL: Enhancing Vision-Language Model's Perception of the World at Any Resolution}, 
      author={Peng Wang and Shuai Bai and Sinan Tan and Shijie Wang and Zhihao Fan and Jinze Bai and Keqin Chen and Xuejing Liu and Jialin Wang and Wenbin Ge and Yang Fan and Kai Dang and Mengfei Du and Xuancheng Ren and Rui Men and Dayiheng Liu and Chang Zhou and Jingren Zhou and Junyang Lin},
      year={2024},
      eprint={2409.12191},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2409.12191}, 
}

@misc{molmo,
      title={Molmo and PixMo: Open Weights and Open Data for State-of-the-Art Vision-Language Models}, 
      author={Matt Deitke and Christopher Clark and Sangho Lee and Rohun Tripathi and Yue Yang and Jae Sung Park and Mohammadreza Salehi and Niklas Muennighoff and Kyle Lo and Luca Soldaini and Jiasen Lu and Taira Anderson and Erin Bransom and Kiana Ehsani and Huong Ngo and YenSung Chen and Ajay Patel and Mark Yatskar and Chris Callison-Burch and Andrew Head and Rose Hendrix and Favyen Bastani and Eli VanderBilt and Nathan Lambert and Yvonne Chou and Arnavi Chheda and Jenna Sparks and Sam Skjonsberg and Michael Schmitz and Aaron Sarnat and Byron Bischoff and Pete Walsh and Chris Newell and Piper Wolters and Tanmay Gupta and Kuo-Hao Zeng and Jon Borchardt and Dirk Groeneveld and Crystal Nam and Sophie Lebrecht and Caitlin Wittlif and Carissa Schoenick and Oscar Michel and Ranjay Krishna and Luca Weihs and Noah A. Smith and Hannaneh Hajishirzi and Ross Girshick and Ali Farhadi and Aniruddha Kembhavi},
      year={2024},
      eprint={2409.17146},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2409.17146}, 
}

@article{gemini-1.5,
  title={Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context},
  author={Gemini Team, Google and Georgiev, Petko and Lei, Ving Ian and Burnell, Ryan and Bai, Libin and Gulati, Anmol and Tanzer, Garrett and Vincent, Damien and Pan, Zhufeng and Wang, Shibo and others},
  journal={arXiv preprint arXiv:2403.05530},
  year={2024}
}

@misc{gemini2.0_2024,
 title = {Google Gemini AI Update - December 2024: Gemini 2.0},
 author = {{Google DeepMind}},
 year = {2024},
 month = {December},
 url = {https://blog.google/technology/google-deepmind/google-gemini-ai-update-december-2024/#gemini-2-0},
 note = {Accessed: 2025-01-16}
}

@article{llama3.2,
  title={The llama 3 herd of models},
  author={Dubey, Abhimanyu and Jauhri, Abhinav and Pandey, Abhinav and Kadian, Abhishek and Al-Dahle, Ahmad and Letman, Aiesha and Mathur, Akhil and Schelten, Alan and Yang, Amy and Fan, Angela and others},
  journal={arXiv preprint arXiv:2407.21783},
  year={2024}
}

@misc{flux2023,
    author={Black Forest Labs},
    title={FLUX},
    year={2023},
    howpublished={\url{https://github.com/black-forest-labs/flux}},
}

@misc{grok2,
  author       = "{xAI}",
  title        = "{Grok-2: Next-Generation AI Models}",
  year         = "2024",
  url          = "https://x.ai/blog/grok-2"
}

@article{Yin_2024,
   title={A survey on multimodal large language models},
   volume={11},
   ISSN={2053-714X},
   url={http://dx.doi.org/10.1093/nsr/nwae403},
   DOI={10.1093/nsr/nwae403},
   number={12},
   journal={National Science Review},
   publisher={Oxford University Press (OUP)},
   author={Yin, Shukang and Fu, Chaoyou and Zhao, Sirui and Li, Ke and Sun, Xing and Xu, Tong and Chen, Enhong},
   year={2024},
   month=nov }

@article{internVL2.5,
  title={Expanding performance boundaries of open-source multimodal models with model, data, and test-time scaling},
  author={Chen, Zhe and Wang, Weiyun and Cao, Yue and Liu, Yangzhou and Gao, Zhangwei and Cui, Erfei and Zhu, Jinguo and Ye, Shenglong and Tian, Hao and Liu, Zhaoyang and others},
  journal={arXiv preprint arXiv:2412.05271},
  year={2024}
}

@article{au-roc,
title = {The use of the area under the ROC curve in the evaluation of machine learning algorithms},
journal = {Pattern Recognition},
volume = {30},
number = {7},
pages = {1145-1159},
year = {1997},
issn = {0031-3203},
doi = {https://doi.org/10.1016/S0031-3203(96)00142-2},
url = {https://www.sciencedirect.com/science/article/pii/S0031320396001422},
author = {Andrew P. Bradley},
keywords = {The ROC curve, The area under the ROC curve (AUC), Accuracy measures, Cross-validation, Wilcoxon statistic, Standard error},
abstract = {In this paper we investigate the use of the area under the receiver operating characteristic (ROC) curve (AUC) as a performance measure for machine learning algorithms. As a case study we evaluate six machine learning algorithms (C4.5, Multiscale Classifier, Perceptron, Multi-layer Perceptron, k-Nearest Neighbours, and a Quadratic Discriminant Function) on six “real world” medical diagnostics data sets. We compare and discuss the use of AUC to the more conventional overall accuracy and find that AUC exhibits a number of desirable properties when compared to overall accuracy: increased sensitivity in Analysis of Variance (ANOVA) tests; a standard error that decreased as both AUC and the number of test samples increased; decision threshold independent; and it is invariant to a priori class probabilities. The paper concludes with the recommendation that AUC be used in preference to overall accuracy for “single number” evaluation of machine learning algorithms.}
}


@inproceedings{helff2024llavaguardvlmbasedsafeguardsvision,
  author    = {Lukas Helff and Felix Friedrich and Manuel Brack and Patrick Schramowski and Kristian Kersting},
  title     = {LLAVAGUARD: VLM-based Safeguard for Vision Dataset Curation and Safety Assessment}, 
  booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)},
  year      = {2024},
  pages     = {8322--8326},
  publisher = {IEEE},
  url       = {https://openaccess.thecvf.com/content/CVPR2024W/ReGenAI/papers/Helff_LLAVAGUARD_VLM-based_Safeguard_for_Vision_Dataset_Curation_and_Safety_Assessment_CVPRW_2024_paper.pdf}
}

@inproceedings{openai2022moderation,
    author = {Markov, Todor and Zhang, Chong and Agarwal, Sandhini and Nekoul, Florentine Eloundou and Lee, Theodore and Adler, Steven and Jiang, Angela and Weng, Lilian},
    title = {A holistic approach to undesired content detection in the real world},
    year = {2023},
    isbn = {978-1-57735-880-0},
    publisher = {AAAI Press},
    url = {https://doi.org/10.1609/aaai.v37i12.26752},
    doi = {10.1609/aaai.v37i12.26752},
    booktitle = {Proceedings of the Thirty-Seventh AAAI Conference on Artificial Intelligence and Thirty-Fifth Conference on Innovative Applications of Artificial Intelligence and Thirteenth Symposium on Educational Advances in Artificial Intelligence},
    articleno = {1683},
    numpages = {10},
    series = {AAAI'23/IAAI'23/EAAI'23}
}


@inproceedings{celeba,
  title={Deep learning face attributes in the wild},
  author={Liu, Ziwei and Luo, Ping and Wang, Xiaogang and Tang, Xiaoou},
  booktitle={Proceedings of the IEEE international conference on computer vision},
  pages={3730--3738},
  year={2015}
}