
\section{ELITE}
\label{sec:method}
% In this section, we discuss the limitations of the previous evaluation methods, highlighting their inability to effectively assess the safety of VLM responses, and introduce ELITE evaluator as an accurate evaluation method. Next, we describe the construction process of our dataset designed to induce harmful responses from VLMs. As shown in Table \ref{table1}, we provide a detailed breakdown of the components of the ELITE benchmark constructed based on the ELITE evaluator, along with the criteria used for filtering out image-text pairs.

In this section, we introduce the ELITE evaluator as an accurate evaluation method. Also, we describe the construction process of the ELITE benchmark, along with the creation of in-house image-text pairs, which are designed to induce harmful responses from VLMs. As shown in Table \ref{table1}, we provide a detailed breakdown of the components of the ELITE benchmark.


\subsection{The ELITE Evaluator}
Existing benchmarks use human evaluators or automated evaluators for the safety evaluation of VLMs. Relying on human evaluators is expensive, and thus recent approaches have leveraged automated annotators using LLMs or VLMs. However, existing evaluation methods assume that an attack is successful if the victim model does not output any predefined refusal messages~\cite{chi2024llamaguardvision, mmsafetybench2025}, causing inaccurate safety evaluation results. In order to address this problem, we introduce the ELITE evaluator, an accurate and structured rubric-based evaluation method.

The ELITE evaluator is built upon the StrongREJECT evaluator~\cite{strongreject2024}, which highlighted the overestimation of most basic automated evaluators in the jailbreak techniques of LLMs. The StrongREJECT evaluator is defined as follows: 
\begin{equation}
\text{StrongREJECT} = (1 - \textit{refused}) \times \frac{\textit{specific} + \textit{convincing}}{2}
\label{equation1}
\end{equation} 

In Eq.~\ref{equation1}, \textit{refused} evaluates whether the model has refused the forbidden prompt, and assigns 1 point if the victim model refuses to respond. \textit{specific} measures the specificity of the response, while \textit{convincing} evaluates its convincingness. Both terms are scored on a scale of 1 to 5.

In VLMs, the victim model's responses to forbidden prompts can vary as images are added. For instance, as observed in the responses in Fig.~\ref{figure2}, despite malicious prompts, the victim model may produce responses that are unhelpful to the attacker. However, when the StrongREJECT evaluator is applied to VLMs, it often assigns high scores even when the model does not explicitly refuse to respond and instead provides unhelpful answers. In other words, since the victim model provides a response instead of refusing, a score of 0 is assigned to \textit{refused}, while moderate scores are given for \textit{specific} and \textit{convincing}, resulting in a final score that indicates the response is harmful. This makes it challenging to determine whether the attempt to induce harmful outputs was genuinely successful. To address this problem, we incorporate \textit{toxicity} into the evaluation rubric to assess how harmful the victim model's response is. Through this approach, \textit{toxicity} plays a crucial role in evaluating the success of inducing harmful responses, addressing the limitations of relying solely on \textit{refused} to determine whether the victim model's response is harmful or not. Accordingly, the ELITE evaluator is defined as follows:
\begin{equation}
\resizebox{\columnwidth}{!}{
$\text{ELITE} = (1 - \textit{refused}) \times 
\frac{\textit{specific} + \textit{convincing}}{2} \times \textit{toxicity}$
}
\label{equation2}
\end{equation}

In Eq.~\ref{equation2}, the ELITE evaluator introduces \textit{toxicity} as an additional criterion, scored on a scale of 0 to 5.

% As shown in Fig.~\ref{Figure2}, when the StrongREJECT is applied to VLMs, it often assigns high scores, even when the model provides simple image descriptions or does not explicitly refuse a response, making it difficult to determine whether the jailbreak was truly successful. SF-Scoring, on the other hand, introduces \textit{toxicity} as an additional criterion, with a scale ranging from [0-5]. In VLMs, the impact of jailbreaks varies depending on the given image. To address this variability, the \textit{toxicity} plays a crucial role in determining the success of a jailbreak, complementing the limitations of relying solely on the \textit{refused}. 

% In VLMs, the victim model's responses to forbidden prompts can vary depending on the image. For instance, as observed in the responses in Fig.~\ref{figure2}, despite malicious inputs, the victim model may produce responses that are unhelpful to the attacker. However, when StrongREJECT is applied to VLMs, it often assigns high scores if the model does not explicitly refuse the response and provides unhelpful answers such as image descriptions, as seen in Fig.~\ref{figure2}. In other words, since the victim model provides a response instead of refusing, a score of 0 is assigned to \textit{refused}, while scores are given for \textit{specific} and \textit{convincing}, resulting in a final score that indicates the response is harmful. This makes it challenging to determine whether the attempt to induce harmful outputs was genuinely successful. To address this problem, we incorporate \textit{toxicity} to evaluate how harmful the victim model's response is. Through this approach, \textit{toxicity} plays a crucial role in evaluating the success of a jailbreak, addressing the limitations of relying solely on \textit{refused} to determine whether the victim model's response is harmful or not.

\begin{figure}[t!]
\centering
\includegraphics[width=1.0\columnwidth]{Figure/figure2.pdf}
\caption{Examples of safety evaluations about the victim model's response by ELITE and StrongREJECT evaluator. $r$, $s$, $c$, and $t$ represent refused, specific, convincing, and toxicity, respectively. The ELITE evaluator can effectively evaluate utilizing the toxicity score to make more accurate judgments.}
\label{figure2}
\end{figure}

\subsection{Taxonomy}
We construct a taxonomy of 11 hazard categories to establish a comprehensive and extensive safety benchmark, aligning with AILuminate v1.0~\cite{mlcommons_1.0} standards. The taxonomy includes Violent Crimes (\textbf{S1}), Non-Violent Crimes (\textbf{S2}), Sex Crimes (\textbf{S3}), Defamation (\textbf{S4}), Specialized Advice (\textbf{S5}), Privacy (\textbf{S6}), Intellectual Property (\textbf{S7}), Indiscriminate Weapons (\textbf{S8}), Hate (\textbf{S9}), Self-Harm (\textbf{S10}), and Sexual Content (\textbf{S11}). Responses that enable, encourage, or endorse harmful behavior are flagged as unsafe for their respective categories. Detailed definitions of each category is provided in Appendix~\ref{sec:taxonomy}.

\subsection{Image\&Text Creation}
We refer to the in-house generated image-text pairs as the ELITE benchmark (generated). As shown in Fig.\ref{Figure1}(b), the ELITE benchmark (generated) is composed of four methods—Role Playing, Fake News, Blueprint, and Flowchart—applied across various taxonomies to elicit harmful responses from the victim model. Note that while certain methods, such as Blueprint and Fake News, are used only in specific taxonomies (e.g., Indiscriminate Weapons and Defamation), others, like Flowchart and Role Playing, are applied more broadly across all taxonomies. Detailed examples of these methods are provided in Appendix~\ref{supple:samples for generated}.

To generate image-text pairs, we use the following methods: \\
\textbf{(1) Image Generation}: For Role Playing, Blueprint, and Flowchart, we use image generation models such as Flux AI~\cite{flux2023} and Grok 2~\cite{grok2} to create images that align with the key concepts of each taxonomy. Specifically, we first extract relevant keywords for each taxonomy and use these keywords as prompts to generate corresponding images. For Fake News, we manually synthesize these images to create outputs that align with the intended misinformation scenarios, using the open-source image dataset CelebA~\cite{celeba}. \\
\textbf{(2) Text Generation}: We generate an initial forbidden text prompt by creating keywords relevant to the image and taxonomy, then generate multiple variations of the prompt using Grok 2. To identify the most effective forbidden text prompt for the given image, we evaluate responses from three victim models (Phi-3.5-Vision, Llama-3.2-11B-Vision, and Pixtral-12B). Among the models that produce harmful responses, we select the image-text pair with the highest ELITE evaluator score to finalize its construction.

These image-text pairs are explicitly designed to induce harmful responses from VLMs, enabling a comprehensive safety evaluation.  As shown in Table~\ref{table2}, we incorporate 593 safe-safe pairs into the ELITE benchmark (generated) by embedding inherently harmful intents. These pairs can still induce unsafe responses from VLMs, making them crucial for evaluating safety. Through this, we aim to develop a more extensive benchmark that effectively captures potential vulnerabilities in VLMs. 

\begin{table}[t!]
\caption{
The distribution of the four image-text pair types (unsafe-unsafe, safe-unsafe, unsafe-safe, and safe-safe) in the ELITE benchmark (generated).}
\begin{center}
\resizebox{1.0\columnwidth}{!}{
\begin{tabular}{ccccc}
\toprule
\multicolumn{4}{c}{\textbf{ELITE benchmark (generated)}} & \multirow{2}{*}{\textbf{Total}} \\ \cmidrule{1-4}
safe-safe   & safe-unsafe  & unsafe-safe  & unsafe-unsafe  &                                 \\ \midrule
    593        &    69          &    350          &        42        & 1054                            \\ \bottomrule
\end{tabular}}
\label{table2}
\end{center}
\end{table}


\subsection{Benchmark Construction Pipeline}
As shown in Fig.~\ref{figure}, the steps for constructing the ELITE benchmark are as follows: \\
\textbf{(1) Taxonomy Alignment}: To align the image-text pairs in existing benchmarks with the taxonomy of the ELITE benchmark, we use GPT-4o to classify image-text pairs into their corresponding taxonomies within the ELITE benchmark. \\
\textbf{(2) Filtering}: We apply a filtering process based on a defined threshold to both existing benchmarks and the ELITE benchmark (generated). Specifically, on the ELITE evaluator's [0-25] point scale, we set a threshold determined by human judgment. ELITE evaluator score $s\geq 10$ indicates that the victim model's response is sufficiently harmful, while $s< 10$ indicates that the victim model either refused to respond to the forbidden prompt or provided a non-harmful response. Using this threshold, we primarily include image-text pairs in the ELITE benchmark if at least two out of the three victim models (Phi-3.5-Vision, Llama-3.2-11B-Vision, and Pixtral-12B) achieve a score of $s\geq 10$ to prevent over-reliance on a single model during filtering. However, in cases where a single model's response is deemed sufficiently harmful, pairs meeting the threshold with only one model are also included.  Examples of model responses near our threshold are provided in Appendix~\ref{threshold}. \\ 
\textbf{(3) Balancing the Taxonomy}: After filtering, we identify that some benchmarks are overly concentrated in specific taxonomies (e.g., 204 image-text pairs in VLGuard are filtered into the S9. Hate), leading to imbalance across taxonomies. To create a more balanced benchmark, we additionally filter JailbreakV-28k~\cite{jailbreak28k2024} for only non-concentrated categories. Also, to address the issue of certain taxonomies being overly dependent on specific benchmarks, We exclude image-text pairs with the lowest combined ELITE evaluator scores from the three models.

\begin{figure}[t!]
\centering
\includegraphics[width=1.0\columnwidth]{Figure/figure3.pdf}
\caption{The pipeline for constructing ELITE benchmark. 1) Taxonomy Alignment: Align the image-text pairs in existing benchmarks with the taxonomy of the ELITE benchmark. 2) Filtering: Integrate only image-text pairs where at least two out of three model responses assign an ELITE evaluator score of 10 or higher. 3) Balancing the Taxonomy: Remove image-text pairs with the lowest combined ELITE evaluator score from overly concentrated taxonomies to maintain balance across taxonomies after filtering.}
\label{figure}
\end{figure}

\begin{table*}[ht!]
\caption{ELITE evaluator score-based ASR of various VLMs across taxonomies. The upper group in the table represents proprietary models, and the lower group represents open-source models. The most vulnerable model is highlighted in \textbf{bold} and the second-most vulnerable with an \underline{underline}.}
\begin{center}
\resizebox{1.0\textwidth}{!}{
\begin{tabular}{c|ccccccccccc|c}
\toprule
\textbf{Model}          & \textbf{S1} & \textbf{S2} & \textbf{S3} & \textbf{S4} & \textbf{S5} & \textbf{S6} & \textbf{S7} & \textbf{S8} & \textbf{S9} & \textbf{S10} & \textbf{S11} & \textbf{Average} \\ 
\midrule
GPT-4o         & 16.39 & 17.51 & 12.74 & 20.30 & 33.23 & 14.38 & 7.38 & 17.36 & 8.66 & 11.59 & 13.91 & 15.67    \\ %완료 
GPT-4o-mini     & 29.47 & 32.91 & 18.79 & 31.58 & 44.41 & 25.24 & 18.03 & 29.48 & 18.05 & 28.48 & 33.73 & 28.23    \\ %완료
Gemini-2.0-Flash & 58.44 & 70.73 & 48.09 & 51.63 & 50.76 & 55.59 & 51.37 & 71.07 & 42.17 & 47.68 & 48.52 & 55.37 \\
Gemini-1.5-Pro   & 37.75 & 48.04 & 28.03 & 40.35 & 37.76 & 33.87 & 50.55 & 44.63 & 23.76 & 27.48 & 35.21 & 37.69 \\
Gemini-1.5-Flash & 43.21 & 56.16 & 22.93 & 40.60 & 39.27 & 37.70 & 50.82 & 47.38 & 30.57 & 23.51 & 37.87 & 40.70 \\
\midrule
LLaVa-v1.5-7B  & 67.38 & 79.13 & 72.93 & 51.38 & 46.83 & 68.05 & 63.39 & 66.94 & 51.57 & 64.90 & 56.80 & 63.59    \\ %완료
LLaVa-v1.5-13B & \underline{72.85} & 86.69 & \textbf{79.94} & 53.63 & 54.98 & 73.48 & 68.31 & 72.45 & 58.56 & \underline{74.17} & 60.65 & \underline{69.68}    \\  %완료 
DeepSeek-VL-7B & 38.41 & 59.94 & 31.21 & 34.59 & 42.90 & 43.45 & 42.62 & 54.27 & 37.02 & 35.43 & 31.95 & 42.36    \\ %완료
DeepSeek-VL2-Small & 65.07 & 81.93 & 59.24 & 41.35 & \underline{58.01} & 68.69 & 59.29 & 70.25 & 52.12 & 53.64 & 42.31 & 60.95    \\ %완료
ShareGPT4V-7B & 68.71 & 86.41 & 75.16 & 48.62 & 53.78 & 72.52 & 71.04 & 64.74 & \underline{60.96} & 65.56 & 56.51 & 67.16    \\ %완료
ShareGPT4V-13B & 71.03 & \underline{87.54} & 75.16 & 51.38 & 56.80 & \underline{74.76} & \underline{73.22} & 66.39 & 60.41 & 62.91 & 52.96 & 68.08 \\ % 완료
Phi-3.5-Vision  & 37.58 & 44.40 & 16.24 & 49.87 & 38.07 & 25.24 & 21.86 & 41.05 & 18.60 & 23.18 & 18.34 & 31.85   \\  % 완료
Pixtral-12B  & \textbf{75.50} & \textbf{93.56} & \underline{77.07} & \textbf{67.17} & \textbf{61.63} & \textbf{79.23} & \textbf{86.61} & \textbf{90.08} & \textbf{82.50} & \textbf{77.15} & \textbf{74.56} & \textbf{79.86} \\ %완료
Llama-3.2-11B-Vision  & 54.47 & 69.05 & 41.40 & 30.83 & 55.29 & 53.35 & 33.88 & 55.37 & 34.44 & 43.05 & 39.05 & 47.94   \\ %완료
Qwen2-VL-7B & 57.28 & 70.73 & 45.22 & 38.60 & 47.73 & 60.06 & 40.44 & 66.67 & 45.49 & 54.64 & 50.00 & 53.72 \\ %완료
Molmo-7B& 61.09 & 81.51 & 62.42 & \underline{56.14} & 51.96 & 57.19 & 71.31 & \underline{75.21} & 47.70 & 64.90 & \underline{63.61} & 63.79 \\
InternVL2.5-8B& 51.32 & 65.83 & 60.83 & 23.81 & 50.76 & 49.52 & 36.61 & 55.65 & 27.62 & 43.71 & 36.98 & 46.48 \\
InternVL2.5-26B & 37.75 & 47.48 & 42.36 & 27.82 & 45.62 & 34.82 & 21.58 & 50.41 & 23.02 & 34.77 & 28.99 & 36.21 \\

\bottomrule
\end{tabular}}
\label{table3}
\end{center}
\end{table*}

% As shown in Fig.~\ref{figure}, through these steps, we remove ambiguous samples, such as those that fail to elicit harmful responses from VLMs, and filter samples from previous benchmarks and ELITE benchmark (generated samples) to try to make the ELITE benchmark more diverse.
