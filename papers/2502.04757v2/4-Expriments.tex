\section{Experiments}

\subsection{Experiment Setup}
We evaluate the effectiveness of the ELITE benchmark, consisting of 4,587 image-text pairs, across various VLMs, including GPT-4o, GPT-4o-mini, Gemini-2.0, Gemini-1.5, and open-source models. For open-source models, their original hyperparameters are used. We use GPT-4o as the ELITE evaluator to evaluate the safety of VLMs.

% , including GPT-4o, GPT-4o-mini, Gemini-2.0-Flash, Gemini-1.5 (Pro, Flash), and open-source models such as LLaVa-v1.5 (7B and 13B), DeepSeek (VL-7B, VL2-Small), ShareGPT4V (7B and 13B), Phi-3.5-Vision, Pixtral (12B), Llama-3.2-Vision (11B), Qwen2-VL (7B), Molmo (7B), and InternVL2 (8B and 26B)

\subsection{Metric}
In the Experiments section, we use the ELITE evaluator score-based Attack Success Rate (E-ASR) for comparison. E-ASR is defined as:
\begin{equation}
\text{E-ASR} = \frac{\left| \{ i \mid \text{ELITE evaluator score}_i \geq 10 \} \right|}{N} \times 100,
\label{equation3}
\end{equation}

where \(\text{ELITE evaluator score}_i\) represents the ELITE evaluator score of the \(i\)-th image-text pair and \(N\) is the total number of image-text pairs.

\subsection{Evaluation of the ELITE Benchmark}
In Table~\ref{table3}, we present comprehensive experimental results of the ELITE benchmark across various proprietary and open-source VLMs. GPT-4o exhibits the lowest E-ASR at 15.67\% among models, indicating that it is appropriately safety-aligned against malicious inputs. In contrast, Gemini-2.0-Flash exhibits the highest E-ASR among proprietary models at 55.37\%, indicating significant vulnerability to malicious attacks. Additionally, with a few exceptions, most open-source models show high success rates for malicious attacks. The result that most models exhibit an E-ASR exceeding 40\% highlights the need for improved safety alignment in VLMs.

\begin{table}[t!]
\caption{Comparison of the average E-ASR when using different benchmarks. It highlights that the most effective benchmark for inducing harmful responses in \textbf{bold} and the second-most effective benchmark with an \underline{underline}.}
\begin{center}
\resizebox{1.0\columnwidth}{!}{
\begin{tabular}{cccc}
\toprule
\textbf{Model}  & \textbf{Benchmark}  & \textbf{Total}  & \textbf{Average}  \\ 
\midrule
\multicolumn{1}{c|}{\multirow{5}{*}{LLaVa-v1.5-7B}}  & VLGuard        & 2028          & 27.75           \\
\multicolumn{1}{c|}{}                                & MM-SafetyBench & 1680          & 45.06           \\
\multicolumn{1}{c|}{}                                & MLLMGuard      & 532           & 27.26        \\
\multicolumn{1}{c|}{}                                & ELITE benchmark (generated)   & 1054 & \textbf{69.17} \\ 
\multicolumn{1}{c|}{}                                & ELITE benchmark   & 4587 & \underline{63.59} \\ \midrule
\multicolumn{1}{c|}{\multirow{5}{*}{LLaVa-v1.5-13B}} & VLGuard        & 2028          & 28.40           \\
\multicolumn{1}{c|}{}                                & MM-SafetyBench & 1680          & 46.61          \\
\multicolumn{1}{c|}{}                                & MLLMGuard      & 532           & 27.26           \\
\multicolumn{1}{c|}{}                                & ELITE benchmark (generated)   & 1054 & \textbf{78.46} \\ 
\multicolumn{1}{c|}{}                                & ELITE benchmark   & 4587 & \underline{69.68} \\ \midrule
\multicolumn{1}{c|}{\multirow{5}{*}{DeepSeek-VL-7B}} & VLGuard        & 2028          & 16.40           \\
\multicolumn{1}{c|}{}                                & MM-SafetyBench & 1680          & 31.79           \\
\multicolumn{1}{c|}{}                                & MLLMGuard      & 532           & 16.29           \\
\multicolumn{1}{c|}{}                                & ELITE benchmark (generated)   & 1054 & \underline{37.95} \\ 
\multicolumn{1}{c|}{}                                & ELITE benchmark  & 4587 & \textbf{42.36}  \\ \midrule
\multicolumn{1}{c|}{\multirow{5}{*}{ShareGPT4V-7B}} & VLGuard        & 2028          & 29.24           \\
\multicolumn{1}{c|}{}                                & MM-SafetyBench & 1680          & 48.81          \\
\multicolumn{1}{c|}{}                                & MLLMGuard      & 532           & 23.51           \\
\multicolumn{1}{c|}{}                                & ELITE benchmark (generated)  & 1054 & \textbf{68.50} \\ 
\multicolumn{1}{c|}{}                                & ELITE benchmark   & 4587 & \underline{67.16} \\ \bottomrule
\end{tabular}}
\label{table4}
\end{center}
\end{table}


\subsection{Comparisons with Other Benchmarks}
In this section, we demonstrate the superiority of both the ELITE benchmark and the ELITE benchmark (generated). 
Table~\ref{table4} compares the E-ASR of LLaVa-v1.5 (7B, 13B), DeepSeek-VL (7B), and ShareGPT4V (7B) across existing benchmarks, including VLGuard~\cite{vlguard2024}, MM-SafetyBench~\cite{mmsafetybench2025}, MLLMGuard~\cite{mllmguard2024}, the ELITE benchmark, and the ELITE benchmark (generated). Note that we use publicly available benchmarks in this experiment. 

As shown in Table~\ref{table4}, the ELITE benchmark, which contains approximately 2â€“3 times more evaluation image-text pairs, achieves significantly higher E-ASR across all models. Furthermore, the ELITE benchmark (generated) demonstrates a substantial increase in E-ASR through effective filtering. These experimental results indicate that the low E-ASR observed in existing benchmarks suggests a substantial number of image-text pairs that fail to elicit harmful responses from VLMs. Consequently, this highlights the effectiveness of the ELITE evaluator in filtering out ambiguous image-text pairs, ensuring that only those capable of inducing harmful responses from VLMs are retained.

% To validate SF-Bench (generated dataset), we include experiments on SF-Bench (generated dataset, text-only), which contains only text without images, in Table 4. The experimental results show that the text-only dataset exhibits a lower SF-Scoring-based ASR compared to experiments including images, indicating that VLMs are less vulnerable to jailbreaks without images. These results suggest that the image-text pairs in the generated dataset are more effective in eliciting harmful content from VLMs. This is particularly evident in relatively safety-aligned models such as DeepSeek-VL-7B, which further emphasizes this observation.

Table~\ref{table5} presents the E-ASR of the methods used to elicit harmful responses from VLMs in the ELITE benchmark (generated). Our experimental results show that Flowchart and Blueprint achieve high E-ASR across a significant number of models, underscoring the importance of incorporating these methods into the benchmark to effectively evaluate and enhance the safety and robustness of VLMs.

\begin{table}[t!]
\caption{Comparison of the E-ASR of the proposed methods in the ELITE benchmark (generated).}
\begin{center}
\resizebox{1.0\columnwidth}{!}{
\begin{tabular}{cccccc}
\toprule
\multirow{2}{*}{\textbf{Method}} & \multirow{2}{*}{\textbf{Count}} & \multicolumn{4}{c}{\textbf{Model}}                                         \\ \cmidrule{3-6} 
                        &                        & LLaVa-7B  & LLaVa-13B & DeepSeek-7B & ShareGPT-7B  \\ \midrule
Blueprint               & 100                    & 64.00          & 82.00          & \textbf{85.00} & 65.00          \\
Fake News               & 140                    & \textbf{65.71} & 56.43          & 46.43          & 55.00          \\
Flowchart               & 706                    & 74.79          & \textbf{87.11}          & 31.16          & 77.34 \\
%Lyrics                  & 74                     & 9.46           & \textbf{14.86} & 2.70           & 2.70           \\
Role Playing            & 108                    & 39.47          & \textbf{44.74} & 26.32          & 30.70          \\ \bottomrule
\end{tabular}}
\label{table5}
\end{center}
\end{table}

\section{Human evaluation}
In this section, we explain the human labeling process and the steps involved in constructing the dataset for human evaluation. Through this, we conduct experiments to compare how closely the ELITE evaluator aligns with human judgment compared to existing automated safety evaluator and safeguard models.

\subsection{Human Annotators and Datasets} 
We recruit 22 labelers through the data-labeling company to evaluate responses. The labelers are selected to ensure diversity across gender, age, and occupation, aiming for unbiased labeling. Each labeler is provided with clear instructions on identifying safe and unsafe responses generated by the victim models. They reviewed each image-text-response pair and classified it as either ``safe" or ``unsafe". To ensure reliability, each image-text pair is evaluated by three labelers, and the majority decision is taken as the ground truth. The detailed evaluation process and the content of the instructions are provided in the Appendix~\ref{sec: human evaluation details}. 

The dataset used for human evaluation consists of a subset of image-text pairs, with approximately 90 pairs per taxonomy, totaling 963 pairs. For a fair comparison, instead of using the filtered responses, we also include responses from models that did not meet the filtering criteria among the three models (Phi-3.5-Vision, Llama-3.2-11B-Vision, and Pixtral-12B). Additionally, these pairs were primarily collected where the evaluation results differed between the ELITE evaluator and existing evaluation methods (StrongREJECT evaluator and safeguard models) and were randomly sampled across each taxonomy. To ensure diversity, we excluded image-text pairs that differed only in model responses from the human evaluation dataset.

\begin{figure}[t!]
\centering
\includegraphics[width=1.0\columnwidth]{Figure/figure4.pdf}
\caption{The comparison of AU-ROC curves between the ELITE evaluator and StrongREJECT evaluator on our human evaluation dataset.}
\label{figure3}
\end{figure}

\subsection{Comparison with Existing Evaluation Method}
To demonstrate the superiority of the ELITE evaluator, we compare it with the StrongREJECT evaluator. Fig.~\ref{figure3} shows the comparison using the Area Under the Receiver Operating Characteristic Curve (AU-ROC Curve)~\cite{au-roc}, considering the differences in scoring scales between the two methods. For a fair comparison, both the ELITE and StrongREJECT evaluators are evaluated using the GPT-4o on the human evaluation dataset consisting of 963 image-text pairs.

As shown in Fig.~\ref{figure3}, the StrongREJECT (GPT-4o) achieves an Area Under the Curve (AUC) of 0.46. In contrast, the ELITE evaluator achieves a significantly higher AUC of 0.77, demonstrating that the ELITE evaluator aligns more closely with human judgment. This result indicates the necessity of incorporating a toxicity score for a more accurate and comprehensive safety evaluation in VLMs. Furthermore, it highlights the robustness and superior performance of the ELITE evaluator.

To further demonstrate that the effectiveness of the ELITE evaluator is not solely due to advanced models like GPT-4o, we validate its effectiveness by applying it to open-source models. Specifically, we apply it to InternVL2.5 (7B, 26B) for comparison. Experimental results show that the ELITE evaluator with InternVL2.5 (7B, 26B) achieves AUC values of 0.57 and 0.65, respectively, surpassing the StrongREJECT evaluator with GPT-4o. This finding confirms that the strong performance of the ELITE evaluator is not solely dependent on a competent model.

% \begin{figure}[t!]
% \centering
% \includegraphics[width=0.9\columnwidth]{Figure/figure4.pdf}
% \caption{The comparison of AU-ROC curves between SF-Scoring and LlamaGuard-3-Vision-11B on our human evaluation dataset.}
% \label{Figure3}
% \end{figure}


\begin{table}[t!]
\caption{Performance comparison of the ELITE (GPT-4o), ELITE (InternVL2.5-8B, 26B), ELITE (InternVL2.5-26B),  LlamaGuard3-Vision-11B, LlavaGuard-13B, and OpenAI Moderation API on our human evaluation dataset. The best-performing method is highlighted in \textbf{bold} and the second-best
method with an \underline{underline}.}
\begin{center}
\resizebox{1.0\columnwidth}{!}{%
\begin{tabular}{c|ccccc}
\toprule
\textbf{Method}               & \textbf{Accuracy ($\uparrow$)} & \textbf{Precision ($\uparrow$)} & \textbf{Recall ($\uparrow$)} & \textbf{F1 score ($\uparrow$)} \\ \midrule
ELITE (GPT-4o)          & \textbf{0.726}    & \textbf{0.579}     & \textbf{0.709}  & \textbf{0.637}    \\
ELITE (InternVL2.5-26B) & \underline{0.660} & \underline{0.500} & \underline{0.471} & \underline{0.485} \\
ELITE (InternVL2.5-8B) & 0.609 & 0.416 & 0.376 & 0.395  \\
LlamaGuard3-Vision-11B       & 0.603 & 0.339 & 0.177 & 0.233   \\
LlavaGuard-13B        & 0.536 & 0.331 & 0.361 & 0.346 &  \\
OpenAI Moderation API       & 0.624 & 0.439 & 0.388 & 0.412  \\
\bottomrule
\end{tabular}%
}
\label{table6}
\end{center}
\end{table}

\subsection{Comparison with Safeguard Models} 
We compare the ELITE evaluator with safeguard models, including LlamaGuard3-Vision-11B~\cite{chi2024llamaguardvision}, LlavaGuard-13B~\cite{helff2024llavaguardvlmbasedsafeguardsvision}, and OpenAI Moderation API~\cite{openai2022moderation}. In this experiment, the ELITE evaluator classifies responses with ELITE evaluator score $s \geq 10$ as unsafe and $s < 10$ as safe, following the same criteria used for filtering.

Table~\ref{table6} demonstrates that the ELITE evaluator, when applied to GPT-4o, outperforms LlamaGuard3-Vision-11B in terms of accuracy, precision, recall, and F1 score. Specifically, it achieves 73\% accuracy, representing an improvement of approximately 20.3\% over LlamaGuard3-Vision-11B, 35\% over LlavaGuard-13B, and 16\% over the OpenAI Moderation API. For the F1 score, the ELITE evaluator shows an F1 score of 0.637, which is significantly higher than the others. Furthermore, the ELITE evaluator performs better when applied to the open-source model InternVL2.5 (26B). The superior performance of the ELITE evaluator on open-source models further indicates that its effectiveness is not limited to GPT-4o, highlighting its broader applicability.

\begin{table}[t!]
\caption{Breakdown of F1 score according to taxonomies between ELITE (GPT-4o), LlamaGuard3-Vision-11B, LlavaGuard-13B, and OpenAI Moderation API on our human evaluation dataset.}
\begin{center}
\resizebox{1.0\columnwidth}{!}{%
\begin{tabular}{lccccc}
\toprule
\multicolumn{1}{c}{\multirow{2}{*}{\textbf{Taxonomy}}} & \multicolumn{4}{c}{\textbf{F1 score ($\uparrow$)}}                                 \\ \cmidrule{2-5} 
\multicolumn{1}{c}{}                                   & \textbf{ELITE} & \textbf{LlamaGuard3} & \textbf{LlavaGuard} & \textbf{OpenAI Mod.} \\ \midrule
S1. Violent Crimes                 & \textbf{0.50}       & 0.16                 & 0.31          & 0.43            \\
S2. Non-Violent Crimes             & \textbf{0.61}       & 0.08                 & 0.26          & 0.48            \\
S3. Sex Crimes                     & \textbf{0.62}       & 0.18                 & 0.33          & 0.24            \\
S4. Defamation                     & \textbf{0.62}       & 0.18                 & 0.25          & 0.06            \\
S5. Specialized Advice             & \textbf{0.52}       & 0.09                 & 0.12          & 0.08            \\
S6. Privacy                        & \textbf{0.55}       & 0.16                 & 0.37          & 0.40            \\
S7. Intellectual Property          & \textbf{0.86}       & 0.62                 & 0.54          & 0.70            \\
S8. Indiscriminate Weapons         & \textbf{0.76}       & 0.18                 & 0.57          & 0.56            \\
S9. Hate                           & \textbf{0.66}       & 0.18                 & 0.38          & 0.44            \\
S10. Self-Harm                     & \textbf{0.67}       & 0.20                 & 0.30          & 0.30            \\
S11. Sexual Content                & \textbf{0.52}       & 0.37                 & 0.26          & 0.38            \\ \bottomrule
\end{tabular}}
\label{table7}
\end{center}
\end{table}

Table~\ref{table7} presents the F1 score for each taxonomy on the human evaluation dataset. Our results show that the ELITE evaluator outperforms LlamaGuard3-Vision-11B across all taxonomies. Specifically, safeguard methods tend to show low F1 scores in certain taxonomies. For instance, LlamaGuard3-Vision-11B shows significantly lower F1 scores in taxonomies such as S2. Non-violent Crimes and S5. Specialized Advice. Similarly, the OpenAI Moderation API shows low F1 scores in taxonomies such as S4. Defamation and S5. Specialized Advice. In contrast, the ELITE evaluator exhibits consistently high and balanced performance across all taxonomies. This demonstrates the superiority of the ELITE evaluator and indicates its effectiveness and accuracy in safety evaluation. 





% \subsection{Optimal threshold through human judgment}

% \begin{table}[t!]
% \centering
% \resizebox{0.5\columnwidth}{!}{%
% \begin{tabular}{cc}
% \toprule
% \textbf{SF-Score for threshold} & \textbf{Accuracy} \\ \midrule
% 5                               & 0.661             \\
% 8                               & 0.679             \\
% 10                              & 0.726             \\
% 15                              & 0.727             \\
% 25                              & 0.717             \\ \bottomrule
% \end{tabular}}
% \end{table}
