\newpage
\appendix
\onecolumn

\section{Additional Experiments}
\label{samples}
\subsection{Benchmark using the ELITE Evaluator Score}

\begin{table*}[h!]

\caption{ELITE evaluator score of various VLMs across taxonomies. The upper group in the table represents proprietary models, and the lower group represents open-source models. Highlight the most vulnerable model in \textbf{bold} and the second-most vulnerable with an \underline{underline}.}
\begin{center}

\renewcommand\arraystretch{1.0}
\resizebox{1.0\columnwidth}{!}{
\begin{tabular}{c|ccccccccccc|c}
\toprule
Model          & S1 & S2 & S3 & S4 & S5 & S6 & S7 & S8 & S9 & S10 & S11 & Average Score \\ \midrule
GPT-4o         & 3.12  & 3.36  & 2.33  & 3.77  & 6.88  & 2.76  & 1.50  & 3.37  & 1.83  & 2.30 & 2.83  & 3.07    \\ 
GPT-4o-mini    & 5.69  & 6.94  & 3.38  & 5.74  & 8.57  & 5.15  & 3.81  & 5.89  & 3.49  & 5.74   & 6.20   & 5.55   \\ 
Gemini-2.0-Flash & 11.86 & 15.66 & 9.31 &  \underline{9.64} & 10.07 & 11.76 & 11.99 & 14.76 & 8.22 & 9.56 & 9.32 & 11.42 \\
Gemini-1.5-Pro & 7.48 & 10.33 & 5.45 & 7.40 & 7.47 & 6.87 & 11.95 & 9.57 & 4.62 & 5.82 & 6.99 & 7.78 \\
Gemini-1.5-Flash & 8.62 & 12.45 & 4.69 & 8.04 & 7.69 & 7.52 & 11.95 & 9.57 & 5.90 & 4.97 & 7.16 & 8.43 \\ \midrule
LLaVa-v1.5-7B  & 13.66  & 17.14  & 15.52  & 9.5  & 8.81  & 14.09  & 13.68  & 13.76  & 9.44  & 12.94   & 10.79   & 12.90    \\  
LLaVa-v1.5-13B & \underline{14.93}  & \underline{19.29}  & \textbf{16.17}  & 9.48  & 9.92  & 15.36  & 14.92  & 14.89  & 10.91  & \underline{14.82}   & 11.58   & \underline{14.15}    \\ 
DeepSeek-VL-7B & 7.65  & 12.75  & 6.50  & 5.86  & 8.02  & 8.46  & 8.80  & 11.36  & 6.79  & 6.80   & 5.90   & 8.39    \\
DeepSeek-VL2-Small & 13.28  & 17.37  & 11.29  & 7.54  & 10.16  & 14.41  & 13.08  & 15.52 & 9.85  & 10.41 & 8.84 & 12.37    \\  
ShareGPT4V-7B  & 14.1   & 18.8  & \underline{15.76}  & 8.98  & 9.63  & 14.88  & 15.57  & 13.87  &  \underline{11.57}   & 13.14   & 10.87 & 13.73    \\ 
ShareGPT4V-13B & 14.52  &  19.16 & 15.72  & 9.24  & \underline{10.21}  & \textbf{15.59}  & 15.55  & 14.25 & 11.38  & 12.86   & 10.74  & 13.93 \\ 
Phi-3.5-Vision  & 7.03   & 9.33  & 3.13  & 7.75  & 6.46  & 4.52  & 4.23  & 8.56  & 3.31   & 4.25   & 2.98 & 5.95   \\ 
Pixtral-12B  & \textbf{15.05}   & \textbf{21.14} & 15.44  & \textbf{11.29}  & \textbf{10.46}  & \underline{15.46}  & \textbf{18.80}  & \textbf{19.45}  & \textbf{14.46}   & \textbf{14.87}   & \textbf{12.77} & \textbf{15.79} \\ 
Llama-3.2-11B-Vision  & 9.74   & 14.82  & 7.3  & 4.85  & 9.12  & 10.15  & 6.84  & 11.33  & 5.76   & 7.55   & 6.55 & 8.97   \\ 
Qwen2-VL-7B & 11.54 & 15.15  & 8.99  & 6.82  & 8.82  & 12.69  & 8.75  & 14.69 & 8.73  & 10.5   & 9.46  & 10.87 \\ 
Molmo-7B & 12.07 & 17.73 & 12.79 & 9.32 & 9.35 & 11.81 & \underline{15.93} & \underline{16.10} & 9.19 & 12.98 & \underline{12.17} & 12.90 \\
InternVL2.5-8B & 10.22 & 14.06 & 11.65 & 4.83 & 9.54 & 9.84 & 8.16 & 12.31 & 5.27 & 8.39 & 7.25 & 9.45 \\
InternVL2.5-26B & 7.54 & 10.02 & 8.11 & 4.83 & 8.63 & 7.12 & 4.89 & 10.74 & 4.22 & 6.68 & 5.29 & 7.21 \\
\bottomrule
\end{tabular}}
\label{sup_table1}
\end{center}
\end{table*}


We measure the ELITE evaluator score for each taxonomy on a [0-25] point scale using the ELITE evaluator score, rather than E-ASR. As shown in Table~\ref{sup_table1}, among proprietary models, GPT-4o exhibits the lowest ELITE evaluator score, while Gemini-2.0-Flash achieves the highest score of 11.42. For open-source models, most show relatively high scores in the low to mid-10s. Similar to the E-ASR results provided in the main text, these high scores emphasize the need for safety alignment.


\subsection{Threshold}
\label{threshold}

\begin{table}[ht!]
\caption{Performance comparison of ELITE evaluator (GPT-4o) by threshold on our human evaluation dataset.}
\vskip 0.15in
\centering
\resizebox{0.5\columnwidth}{!}{
\begin{tabular}{ccc}
\toprule
\textbf{ELITE evaluator score for threshold} & \textbf{Accuracy} & \textbf{F1 score} \\ \midrule
5                               & 0.661             & 0.625             \\
10                              & 0.726             & 0.637             \\
15                              & 0.727             & 0.638             \\
20                              & 0.728             & 0.639             \\ \bottomrule
\end{tabular}}
\label{threshold_table}
\vskip -0.1in
\end{table}

We validate our filtering criterion, a threshold of 10, using our human evaluation dataset. As shown in Table~\ref{threshold_table}, we set thresholds at 5, 10, 15, and 20 to measure accuracy and F1 score. Although the threshold of 10 does not perform the best, the below examples of image-text pairs with ELITE evaluator scores near 10 demonstrate that harmful responses can still be elicited from VLMs. Therefore, considering the potential for eliciting harmful responses, we select 10 as the threshold to incorporate a diverse range of image-text pairs.

The following are examples of image-text pairs with ELITE evaluator scores near 10:

\begin{figure}
\centering
\includegraphics[width=1.0\columnwidth]{Figure/supple/threshold_samples/0126_score_8.pdf}
\par\vspace{10mm}
\includegraphics[width=1.0\columnwidth]{Figure/supple/threshold_samples/0126_score_9.pdf}
\caption{Examples with ELITE evaluator score below 10}
\end{figure}

\begin{figure}
\centering
\includegraphics[width=1.0\columnwidth]{Figure/supple/threshold_samples/0126_score_10.pdf}
\par\vspace{10mm}
\includegraphics[width=1.0\columnwidth]{Figure/supple/threshold_samples/0126_score_10_5.pdf}
\caption{Examples with an ELITE evaluator score of 10 or higher}
\end{figure}

% \includegraphics[width=1.0\columnwidth]{Figure/supple/threshold samples/0126_score_12.pdf}

\section{Details of the ELITE Benchmark}
In this section, we provide detailed information on the definition of each taxonomy, previous benchmarks, and sample examples for each taxonomy. We also present the prompts used to integrate these benchmarks into our taxonomy during the construction of the ELITE benchmark (generated), as well as the detailed prompts for the ELITE evaluator. Additionally, we include the model cards for both the models and safeguard models used for comparison in the experiments.

\subsection{Taxonomy}
\label{sec:taxonomy}
Definitions of each taxonomy are shown below:\\
\textbf{S1. Violent Crimes}: Refers to unlawful violence toward people (e.g., terrorism, genocide, murder, hate-motivated violent crimes, child abuse, assault, battery, kidnapping) and unlawful violence toward animals (e.g., animal abuse). \\
\textbf{S2. Non-Violent Crimes}: Refers to personal crimes, financial crimes, property crimes, drug crimes, weapons crimes, and cybercrimes. \\
\textbf{S3. Sex-Related Crimes}: Refers to sex trafficking, sexual assault, sexual harassment, non-consensual sharing of sexually intimate content, and bestiality. \\
\textbf{S4. Defamation}: Refers to verifiably false statements that are likely to injure a living person's reputation. \\
\textbf{S5. Specialized Advice}: Refers to unqualified advice (without disclaimers) regarding financial, medical, legal, or electoral matters, as well as statements indicating that dangerous activities or objects are categorically safe. \\
\textbf{S6. Privacy}: Refers to the exposure of sensitive, nonpublic personal information that could undermine someone's physical, digital, or financial security. \\
\textbf{S7. Intellectual Property}: Refers to responses that violate the intellectual property rights of any party or quote verbatim from existing works without proper attribution. \\
\textbf{S8. Indiscriminate Weapons}: Refers to weapons that cause mass destruction, such as chemical, biological, radiological, nuclear, or other high-yield explosive weapons. \\
\textbf{S9. Hate}: Refers to responses that demean or dehumanize individuals on the basis of their sensitive, personal characteristics (e.g., race, gender, religion, disability). \\
\textbf{S10. Suicide \& Self-Harm}: Refers to intentional acts of self-harm, such as suicide, self-injury (e.g., cutting), and disordered eating. \\
\textbf{S11. Sexual Content}: Refers to pornography or engaging in direct, explicit conversations (e.g., cybersex). 

\subsection{Previous Benchmarks}
\begin{table}[h]
\caption{Details of the previous benchmarks used in the construction method of benchmark and evaluation methods. The sizes indicated in parentheses represent the datasets utilized in this work. For Figstep, the dataset included in JailbreakV-28K is used.}
\begin{center}
\centering
\resizebox{1\columnwidth}{!}{
\begin{tabular}{l|cll}
\toprule
\textbf{Benchmark}          & \textbf{\# Size}       & \textbf{Construction of Benchmark}               & \textbf{Evaluation Method}                \\ \midrule
VLGuard \cite{vlguard2024}  & 3,000 (2,028)                 & GPT-4V                                         & Rule, LlamaGuard \\ 
MLLMGuard \cite{mllmguard2024} & 2,282 (532)       & Human                                          & Rule, GuardRank \\ 
MM-SafetyBench \cite{mmsafetybench2025} & 5,040 (1,680)        & Human, GPT-4                                   & GPT-4        \\ 
SIUO \cite{siuo2024}         & 167                   & Human                                          & Human, GPT-4V    \\ 
Figstep \cite{gong2023figstep} & 500                & GPT-4                                          & Human   \\ 
SPA-VL \cite{spavl2024}      & 100,788               & Open-source dataset (LAION-5B), Gemini-1.0-Pro Vision                          & GPT-4V  \\ 
JailbreakV-28k \cite{jailbreak28k2024} & 28,000          & Human, GPT-4                                   & LlamaGuard     \\ \bottomrule
\end{tabular}}
\label{benchmarks}
\end{center}
\end{table}

Table~\ref{benchmarks} provides detailed information on the benchmarks targeted for filtering during the construction of the ELITE benchmark, including their size, dataset generation methods, and safety evaluation methods. Note that we utilize a subset of publicly available datasets for MLLMGuard, and for VLGuard and MM-SafetyBench, we use the entire publicly available datasets. Specifically, unique image-text pairs are used, excluding overlapping elements and cases where multiple prompts are associated with a single image.



\subsection{Image-Text Pairs By Taxonomy}
\label{supple:samples2}
In this section, we provide examples of 11 different taxonomies, along with the ELITE evaluator scores for each image-text pair and model response.


\includegraphics[width=1.0\columnwidth]{Figure/supple/taxonomy_samples/0126_taxonomy_1_2.pdf}
\vspace{10mm}

\includegraphics[width=1.0\columnwidth]{Figure/supple/taxonomy_samples/0126_taxonomy_3_4.pdf}
\vspace{10mm}

\includegraphics[width=1.0\columnwidth]{Figure/supple/taxonomy_samples/0126_taxonomy_5_6.pdf}
\vspace{10mm}

\includegraphics[width=1.0\columnwidth]{Figure/supple/taxonomy_samples/0126_taxonomy_7_8.pdf}

\begin{figure}
\includegraphics[width=1.0\columnwidth]{Figure/supple/taxonomy_samples/0126_taxonomy_9_10.pdf}
\par\vspace{10mm}
\includegraphics[width=1.0\columnwidth]{Figure/supple/taxonomy_samples/0126_taxonomy_11.pdf}
\caption{Examples of 11 different taxonomies}
\end{figure}

\newpage
\subsection{Image-Text Pairs for ELITE benchmark (generated)}
\label{supple:samples for generated}
In this section, we provide examples of image-text pairs in the ELITE benchmark (generated), along with the ELITE evaluator scores for each image-text pair and model response.
\begin{figure}[b!]
\includegraphics[width=1.0\columnwidth]{Figure/supple/example/example_sample_1.pdf}
%\par\vspace{10mm}
\includegraphics[width=1.0\columnwidth]{Figure/supple/example/example_sample_2.pdf}
\caption{Examples of image-text pairs in ELITE benchmark (generated)}
\end{figure}

%\newpage
\subsection{Model Cards}

Table~\ref{model_card} provides model cards of the VLMs used in our paper, including their parameters and model architecture components. Also, Table~\ref{guardmodel_cards} provides a detailed summary of the safeguard models in the human evaluation section, including their base models and training datasets.

\begin{table*}[ht!]
\caption{Model cards used in our benchmark experiments. “-” denotes that information is not available for propitiatory models. For open-source models, instruction-tuned or chat-capable models are used.}
\begin{center}
\centering
\resizebox{0.9\columnwidth}{!}{%
\begin{tabular}{l|lll}
\toprule
\textbf{Name}              & \textbf{\# Params} & \textbf{Vision Encoder}           & \textbf{Base LLM}           \\ \midrule
GPT-4o \cite{gpt4o2024}                   &       -           &         -               &        -         \\ 
GPT-4o-mini               &      -        &            -              &            -           \\ 
Gemini-2.0-Flash \cite{gemini2.0_2024}         &         -          &            -              &             -         \\ 
Gemini-1.5-Pro \cite{gemini-1.5}           &         -          &            -              &             -         \\
Gemini-1.5-Flash \cite{gemini-1.5}         &         -          &            -              &             -         \\ \midrule
LLaVa-v1.5-7B \cite{llava-v1.5}            & 7B                      &      CLIP ViT-L/14-336px              &   Vicuna-7B-v1.5
               \\ 
LLaVa-v1.5-13B~\cite{llava-v1.5}            & 13B                     &     CLIP ViT-L/14-336px             &     Vicuna-13B-v1.5      \\ 
DeepSeek-VL-7B~\cite{deepseek-vl}           & 7B                      &  SigLIP-L+SAM-B                &   DeepSeek-LLM-7B   \\ 
DeepSeek-VL2-Small~\cite{deepseek-vl2}        & 16B                     & SigLIP-SO400M                &  DeepSeek-MoE-16B   \\ 
ShareGPT4V-7B~\cite{sharegpt4v}            & 7B                      &   CLIP ViT-L/14-336px           &   Vicuna-7B-v1.5              \\ 
ShareGPT4V-13B~\cite{sharegpt4v}            & 13B                     &  CLIP ViT-L/14-336px           &    Vicuna-13B-v1.5                           \\ 
Phi-3.5-Vision~\cite{phi}           & 4.2B                    & CLIP ViT-L/14-336px                   & Phi-3.5-mini                \\ 
Pixtral-12B~\cite{pixtral}              & 12B                     &  Custom ViT with 400M params           & Mistral-NeMo-12B                 \\ 
Llama-3.2-11B-Vision~\cite{llama3.2}      & 11B                     &   -                               &     Llama-3.1-8B                        \\ 
Qwen2-VL-7B~\cite{qwen2-vl}              & 7B                      &    Custom ViT with 600M params          &  Qwen2-7B                    \\ 
Molmo-7B~\cite{molmo}                  & 7B                      & CLIP ViT-L/14-336px               & Qwen2-7B                   \\ 
InternVL2.5-8B~\cite{internVL2.5}            & 8B                      & InternViT-300M-448px-V2.5        & InternLM2.5-7B-Chat        \\ 
InternVL2.5-26B~\cite{internVL2.5}           & 26B                     & InternViT-6B-448px-V2.5          & InternLM2.5-20B-Chat       \\ \bottomrule
\end{tabular}%
}
\label{model_card}
\end{center}
\end{table*}

\begin{table*}[ht!]
\caption{Model cards of safeguard models used in our human evaluation experiments.}
\begin{center}
\vskip 0.15in
\centering
\resizebox{0.9\columnwidth}{!}{%
\begin{tabular}{l|lll}
\toprule
\textbf{Name}              & \textbf{Base Model} & \textbf{Training Dataset}            \\ \midrule
LlamaGuard3-Vision-11B~\cite{chi2024llamaguardvision} &   Llama-3.2-11B-Vision  &  Human-created, Llama-based synthetic dataset   \\ 
LlavaGuard-13B~\cite{helff2024llavaguardvlmbasedsafeguardsvision}  &     LLaVa-v1.5-13B     &  Human-created dataset (SMID)   \\ 
OpenAI Moderation API~\cite{openai2022moderation}         &      GPT-4o    &  Human-created, GPT-based synthetic dataset    \\ \bottomrule
\end{tabular}%
}
\label{guardmodel_cards}
\end{center}
\end{table*}


% \subsection{Example responses for each model}

\subsection{Taxonomy Matching}
The prompts used for taxonomy matching with previous benchmarks in Benchmark Construction Pipeline of this paper are as follows:

\noindent\adjustbox{clip=true,trim=0 2.9cm 0 0}{\includegraphics[width=\textwidth]{Figure/supple/classification.pdf}}

\begin{figure}[h]
\noindent\adjustbox{clip=true,trim=0 0 0 4.2cm}{\includegraphics[width=\textwidth]{Figure/supple/classification.pdf}}
\caption{Prompts used for taxonomy matching on GPT-4o}
\end{figure}

\subsection{ELITE Evaluator Prompt}
The full prompt used in the ELITE evaluator is as follows:

\noindent\adjustbox{clip=true,trim=0 3cm 0 0}{\includegraphics[width=\textwidth]{Figure/supple/prompt.pdf}}
\begin{figure}[t!]
\noindent\adjustbox{clip=true,trim=0 0 0 14.6cm}{\includegraphics[width=\textwidth]{Figure/supple/prompt.pdf}}
\caption{Prompts used for ELITE evaluator}
\end{figure}

\section{Ethical Statement}
In this study, we introduce a benchmark to evaluate the safety of VLMs. Given its nature, the benchmark contains potentially offensive samples, which may raise safety concerns. We affirm that all data used in this study will not be utilized for purposes other than research. Our research aims to focus on the safety challenges of VLMs and to facilitate future research on their safety alignment to prevent harmful responses.

\section{Human Evaluation}
\label{sec: human evaluation details}
\subsection{Human Evaluation Guidelines}
We recruited human labelers for annotation through the data labeling company CrowdWorks and paid them more than twice the minimum wage. The guidelines provided to the labelers for human evaluation are as follows:

\centering
\noindent\adjustbox{clip=true,trim=0 12.49cm 0 0}
{\includegraphics[width=1.0\columnwidth]{Figure/supple/humaneval_guide.pdf}}

\begin{figure}[t!]
\noindent\adjustbox{clip=true,trim=0 0 0 11.06cm}
{\includegraphics[width=1.0\columnwidth]{Figure/supple/humaneval_guide.pdf}}
\caption{Guidelines used for human evaluation}
\end{figure}

\subsection{Distribution of Human Labelers}
\noindent\raggedright  The gender, age, and occupation distributions of the recruited human labelers are as follows:

\begin{figure}[hb!]
\centering
\includegraphics[width=1.0\columnwidth]{Figure/supple/humaneval_dist.pdf}
\caption{The distribution of the recruited human labelers by gender, age, and occupation}
\end{figure}