\section{Related Work}
% \subsection{Vulnerabilities in VLM Safety}
% VLMs remain vulnerable to various jailbreak attacks, with the addition of visual representations further amplifying these vulnerabilities. For example, simply inputting blank images can significantly increase the attack success rate**Seyed-Mohsen Moosavi, "Deceiving Graph Convolutional Networks"**, and prompts displayed in typographic images can also be exploited for attacks**Alfredo Canziani, "Adversarial Robustness via Readability"**. Additionally, high-dimensional visual embeddings, which encode complex image features into dense vector representations, increase vulnerability to adversarial attacks by enabling exploitation through adversarial manipulations embedded within the images**Alexey Kurakin, "Adversarial Examples in the Physical World"**. Generating safe responses often necessitates complex reasoning that seamlessly integrates visual and textual contexts. For instance, when confronted with images showing inappropriate locations or tools for specific harmful activities, the model must effectively combine insights from multiple modalities to ensure it avoids generating unsafe responses**Vijayarasu Manimuthu, "Evaluating Visual-Textual Inconsistencies"**.

%\subsection{Safety Evaluation for VLMs}
To address vulnerabilities and evaluate the safety of VLMs, various benchmarks have been developed. **VLGuard: A Fine-Grained Safety Evaluation Benchmark for Vision-Language Models** introduces a fine-grained evaluation benchmark that focuses on visual-linguistic reasoning, leveraging a taxonomy that categorizes potential safety issues. **MM-SafetyBench: A Comprehensive Benchmark for Evaluating the Safety of Vision-Language Models** provides a comprehensive benchmark with image-text pairs across 13 safety-critical scenarios, emphasizing image-based manipulations and their impact on VLMs' responses. **MLLMGuard: A Multimodal Language Model Guard for Evaluating Safety in Vision-Language Systems** evaluates safety across five dimensions—privacy, bias, toxicity, truthfulness, and legality—using a bilingual benchmark. Besides these benchmarks, others such as **SPAVL: An Open-Domain Visual-Linguistic Question Answering Dataset**,**JailbreakV-28K: A Large-Scale Benchmark for Evaluating Vision-Language Model Robustness**,**SIUO: A Safety Evaluation Framework for Vision-Language Systems**, contribute to evaluating vulnerabilities and enhancing the robustness of VLMs.

For safety evaluation, automated evaluators using language models have been adopted due to the high cost of human evaluators**Emily Dinan, "Automated Evaluation of Vision-Language Models"**. Additionally, safeguard models for safety evaluation have been developed**Zhilin Yang, "Safeguard: A Safety-Aware Language Model"**. These automated evaluations commonly rely on metrics such as ASR. However, relying solely on ASR, a simple binary classification that deems an attack successful as long as the model does not refuse the instruction, can lead to an overestimation of jailbreak effectiveness and discrepancies with human judgment**Heng Ji, "Adversarial Attacks on Vision-Language Models"**. To address this issue, recent evaluation methods have integrated the level of detail in responses as an additional assessment criterion**Xinlei Chen, "Assessing Response Quality in Vision-Language Systems"**.