\section{Introduction}
\label{submission}

Vision Language Models (VLMs), which are composed of pre-trained Large Language Models (LLMs) and visual encoders, have been introduced to tackle complex multimodal tasks. Despite advancements in their capabilities and performance, VLMs remain vulnerable to malicious inputs, raising significant safety concerns and posing substantial challenges to their large-scale deployment in real-world applications~\cite{vlguard2024, carlini2024aligned, gong2023figstep, bommasani2021opportunities}.

Concerns about the safety of VLMs, such as malicious users inducing harmful outputs, have been raised and several safety evaluation benchmarks have appeared to assess the safety of VLMs~\cite{vlguard2024, mmsafetybench2025, mllmguard2024, spavl2024, siuo2024, gong2023figstep, jailbreak28k2024}. These benchmarks are typically constructed by collecting image-text pairs that can trigger harmful responses, with safety evaluations conducted through automated methods using language models rather than humans~\cite{vlguard2024, jailbreak28k2024, mmsafetybench2025, spavl2024, rtvlm2024}. While these benchmarks contribute to developing safer, more robust VLMs, we have identified significant issues in the existing benchmarks and their automated evaluation methods.

First, we identify that the automated evaluation methods currently adopted in many safety benchmarks are not always reliable. As shown in Fig.~\ref{Figure1}(c), in the upper example, the model's response is largely vague or simply descriptive of the image, yet it is still deemed a successful jailbreak. In another case, as shown in the lower example, the existing evaluation method fails to detect implicit suicidal intent and considers it safe. We identify these problems in the safety evaluation methods and propose the \textbf{E}nhanced \textbf{L}anguage-\textbf{I}mage \textbf{T}oxicity \textbf{E}valuation (\textbf{ELITE}) {\em evaluator}, a method designed to accurately evaluate the safety of VLMs.

\begin{figure*}[ht!]
\centering
\includegraphics[width=2.0\columnwidth]{Figure/figure1.pdf}
\caption{Contributions of ELITE. (a) Benchmark Construction: The ELITE benchmark is a high-quality benchmark built by filtering out unsuccessful image-text pairs using the ELITE evaluator. (b) Generated Image-Text Pairs: Image-text pair with various methods for inducing harmful responses from VLMs. (c) Evaluation Method: The ELITE evaluator is a more precise rubric-based safety evaluation method compared to existing methods for VLMs.}
\label{Figure1}
\end{figure*}


Second, we identify quality issues in existing safety benchmarks. We have observed that existing benchmarks generally exhibit low levels of harmfulness and contain a significant number of ambiguous image-text pairs that fail to induce harmful responses from VLMs. To address this, we introduce the ELITE {\em benchmark}, which filters out ambiguous image-text pairs from existing benchmarks using the ELITE evaluator (Fig.~\ref{Figure1}(a)).

Third, existing benchmarks mainly consist of unsafe image-unsafe text pairs (i.e., unsafe-unsafe, safe-unsafe, or unsafe-safe pairs)~\cite{mllmguard2024,vlguard2024,mmsafetybench2025}. However, as demonstrated in the examples at the bottom of Fig.~\ref{Figure1}(c), harmful responses can also be induced through safe-safe pairs~\cite{siuo2024}. To address this issue, we propose various methods for inducing harmful responses from VLMs. As shown in Fig.~\ref{Figure1}(b), the ELITE benchmark incorporates four in-house generated methods, which improve coverage of all four image-text pair combinations. This enhances the diversity of the ELITE benchmark and enables a more comprehensive evaluation of VLM safety.

The ELITE evaluator is built upon the StrongREJECT evaluator~\cite{strongreject2024}, a rubric-based safety evaluation method that suggests the overestimation of the evaluation results for jailbreak techniques in LLMs. Unlike LLMs, where only the text prompt influences harmful responses, VLMs are influenced by the images as well. We find that input images can frequently cause the victim model to generate responses, such as simple image descriptions, that are unhelpful to users attempting to elicit harmful responses. To address this, we incorporate a toxicity score into the evaluation framework, enabling a more accurate assessment of how harmful the victim model's responses truly are. This refinement ensures a more precise evaluation by distinguishing genuinely safe responses, such as merely descriptive responses. Additionally, we construct the ELITE benchmark, a high-quality benchmark, using the ELITE evaluator to filter out ambiguous image-text pairs. We achieve this by using the ELITE evaluator scores (defined in \cref{sec:method}) to retain only explicit harmful responses while incorporating a diverse set of image-text pairs from both existing benchmarks and in-house generated image-text pairs.

Our experiments demonstrate that the ELITE evaluator aligns better with human judgments than existing automated evaluation methods. Furthermore, through extensive experiments, we validate the diversity and superior quality of the ELITE benchmark, which is designed using the ELITE evaluator. To summarize, our main contributions are as follows:

\begin{table*}[h!]
\caption{Overview of the ELITE benchmark. We created 4,587 image-text pairs by filtering out ambiguous image-text pairs that are unable to induce harmful responses in both existing benchmarks and the in-house generated image-text pairs. ``New" refers to the image-text pairs we generated using various methods. In the case of JailbreakV-28k~\cite{jailbreak28k2024}, filtering is performed only on insufficient taxonomies to maintain balance across taxonomies.}
\label{table1}
\vskip 0.13in
\resizebox{1.0\textwidth}{!}{
\begin{tabular}{c|cccccccc|c|c}
\toprule
\multirow{2}{*}{\textbf{Taxonomy}} & \multicolumn{8}{c|}{\textbf{The ELITE benchmark}}                                                              & \multirow{2}{*}{\textbf{Sum}} & \multirow{2}{*}{\textbf{Total}} \\ \cmidrule{2-9}
                          & VLGuard & MLLMGuard & MM-SafetyBench & SIUO & Figstep & SPA-VL & JailbreakV-28k & New &                      &                        \\ \midrule
S1. Violent Crimes            & 91      & 11        & 39             & 1    & 91      & 299    & 0             & 72           & 604                  & \multirow{13}{*}{4587} \\ \cmidrule{1-10}
S2. Non-Violent Crimes        & 13      & 2         & 144            & 1    & 209     & 221    & 0             & 124          & 714                  &                        \\ \cmidrule{1-10}
S3. Sex Crimes                & 6       & 3         & 0              & 0    & 39      & 32     & 38            & 196          & 314                  &                        \\ \cmidrule{1-10}
S4. Defamation                & 19      & 2         & 5              & 0    & 9       & 224    & 0             & 140          & 399                  &                        \\ \cmidrule{1-10}
S5. Specialized Advice        & 37      & 1         & 21             & 3    & 84      & 131    & 0             & 54           & 331                  &                        \\ \cmidrule{1-10}
S6. Privacy                   & 0       & 14        & 63             & 2    & 42      & 93     & 0             & 99           & 313                  &                        \\ \cmidrule{1-10}
S7. Intellectual Property     & 1       & 5         & 11             & 0    & 37      & 74     & 238           & 0          & 366                  &                        \\ \cmidrule{1-10}
S8. Indiscriminate Weapons    & 0       & 4         & 36             & 0    & 23      & 116    & 84            & 100          & 363                  &                        \\ \cmidrule{1-10}
S9. Hate                      & 204     & 0         & 55             & 4    & 54      & 144    & 0             & 82           & 543                  &                        \\ \cmidrule{1-10}
S10. Self-Harm                 & 15      & 0         & 12             & 2    & 20      & 37     & 89            & 127          & 302                  &                        \\ \cmidrule{1-10}
S11. Sexual Content            & 88      & 1         & 19             & 0    & 36      & 32     & 102           & 60           & 338                  &                        \\ \bottomrule
\end{tabular}}
\vskip 0.0in
\end{table*}


\begin{itemize}

    \item We introduce the ELITE evaluator for accurate automated safety evaluation in VLMs. Through the ELITE evaluator, we demonstrate that existing automated safety evaluation methods often result in inaccurate evaluations.
    
    \item We propose the ELITE benchmark, a rubric-based safety evaluation benchmark for VLMs using the ELITE evaluator. The ELITE benchmark addresses the limitations of existing benchmarks, such as insufficient benchmark quality. We construct a high-quality benchmark by filtering out low-quality and ambiguous image-text pairs.
    
    \item We propose various methods for inducing harmful responses in VLMs. These methods are applied to generate extensive image-text pairs across all combinations of safe and unsafe image-text pairs to elicit harmful responses that violate VLMs' safety policies.
    
\end{itemize}


