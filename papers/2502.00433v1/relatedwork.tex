\section{Related Work}
Diffusion models have emerged as powerful generative frameworks in computer vision. However, these models are compute-intensive, often constrained by the high computational cost. This computational bottleneck has led to a surge of research focused on accelerating diffusion models. Here, we highlight several major categories of approaches: parallelization, reduction of sampling steps, and model pruning.

\textbf{Parallelization Methods}
 Despite traditional techniques like tensor parallelism, recent works have introduced novel parallelization strategies specifically tailored to the characteristics of diffusion models. DistriFusion \citep{li2024distrifusiondistributedparallelinference}, for instance, hides the communication overhead within the computation via asynchronous communication and introduces displaced patch parallelism, while PipeFusion \citep{wang2024pipefusiondisplacedpatchpipeline} introduces displaced patch parallelism for Inference of Diffusion Transformer Models (DiT \citep{Peebles2022DiT}) and ParaDiGMS \citep{shih2023paradigms} rum sampling steps in parallel through iterative refinement.

\textbf{Reducing Sampling Steps}
One of the core challenges with diffusion models is the large number of sampling steps required to produce high-quality outputs, which directly translates to longer inference times. Recent advancements such as DPM Solver \citep{lu2022dpm,zheng2023dpmsolverv3improveddiffusionode} and Consistency Models \citep{song2023consistency,song2023improvedtechniquestrainingconsistency} aim to address this bottleneck by developing fast solvers for diffusion ODEs and directly mapping noise to data respectively.  Moreover, the reflow+distillation \citep{liu2022flowstraightfastlearning} approach in flow-based works \citep{lipman2022flow,tong2024improvinggeneralizingflowbasedgenerative} also provides another promising approach to one-step models \citep{liu2024instaflowstephighqualitydiffusionbased,yin2024one}.

\textbf{Leveraging Computational Redundancy} %tgate, fora, deepcache, AT-EDM
Recognizing the iterative nature of diffusion models and the minimal changes in feature representations across consecutive steps, a growing body of research has focused on developing cache-and-reuse mechanisms to reduce inference time. DeepCache \citep{ma2023deepcache} reuses the high-level features of the U-Net \citep{ronneberger2015unetconvolutionalnetworksbiomedical}. Block Cache \citep{wimbauer2023cache} performs caching at a per-block level and adjusts the cached values using a lightweight 'scale-shift' mechanism. TGATE \citep{liu2024faster,tgate} caches the output of the cross-attention module once it converges. FORA \citep{Selvaraju2024FORAFC} reuses the outputs from the attention and MLP layers to accelerate DiT inference. Another representative strategy lies in model compression, achieved through the exploitation of sparsity patterns \citep{NEURIPS2022_b9603de9,wang2024sparsedmsparseefficientdiffusion} and quantization techniques \citep{zhao2024viditqefficientaccuratequantization,fang2023structuralpruningdiffusionmodels,li2023qdiffusionquantizingdiffusionmodels}.

\textbf{Concurrent Work}
During this work, we noted a concurrent study \citep{zou2024acceleratingdiffusiontransformerstokenwise} that independently employs token-wise feature caching to accelerate diffusion models. While both approaches consider ``Cache Frequency," our CAT Pruning method ranks token importance based on ``noise" magnitude, eliminating the need for intermediate attention scores. This enables seamless integration with \textbf{online softmax} \citep{milakov2018onlinenormalizercalculationsoftmax, NEURIPS2022_67d57c32} for further \textbf{speedup} while also using \textbf{less memory}.