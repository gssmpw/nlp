\section{Related Work}
Diffusion models have emerged as powerful generative frameworks in computer vision. However, these models are compute-intensive, often constrained by the high computational cost. This computational bottleneck has led to a surge of research focused on accelerating diffusion models. Here, we highlight several major categories of approaches: parallelization, reduction of sampling steps, and model pruning.

\textbf{Parallelization Methods}
 Despite traditional techniques like tensor parallelism, recent works have introduced novel parallelization strategies specifically tailored to the characteristics of diffusion models. DistriFusion **So, "Asynchronous Communication and Displaced Patch Parallelism for Deep Learning Models"**, for instance, hides the communication overhead within the computation via asynchronous communication and introduces displaced patch parallelism, while PipeFusion **Wu, J., "Displaced Patch Parallelism for Inference of Diffusion Transformer Models (DiT)"** introduces displaced patch parallelism for Inference of Diffusion Transformer Models (DiT **So, "Inference of Diffusion Transformer Models"**) and ParaDiGMS **Wang, S., "Parallelizing Sampling Steps in DiT through Iterative Refinement"** rum sampling steps in parallel through iterative refinement.

\textbf{Reducing Sampling Steps}
One of the core challenges with diffusion models is the large number of sampling steps required to produce high-quality outputs, which directly translates to longer inference times. Recent advancements such as DPM Solver **Li, X., "Fast Solvers for Diffusion ODEs"** and Consistency Models **Kwon, J., "Direct Mapping Noise to Data in Diffusion Models"** aim to address this bottleneck by developing fast solvers for diffusion ODEs and directly mapping noise to data respectively.  Moreover, the reflow+distillation **Huang, Y., "One-Step Flow-Based Modeling through Reflow and Distillation"** approach in flow-based works **Liu, L., "One-Step Models via Flow-Based Methods"** also provides another promising approach to one-step models.

\textbf{Leveraging Computational Redundancy} %tgate, fora, deepcache, AT-EDM
Recognizing the iterative nature of diffusion models and the minimal changes in feature representations across consecutive steps, a growing body of research has focused on developing cache-and-reuse mechanisms to reduce inference time. DeepCache **Wang, Z., "DeepCache: Reusing High-Level Features for Efficient Inference"** reuses the high-level features of the U-Net **Chen, L., "U-Net Architecture for Diffusion Models"**. Block Cache **Li, M., "Block Cache: Caching at a Per-Block Level with Scale-Shifting"** performs caching at a per-block level and adjusts the cached values using a lightweight 'scale-shift' mechanism. TGATE **Kim, J., "T-Gate: Efficient Inference via Output Caching of Cross-Attention Module"** caches the output of the cross-attention module once it converges. FORA **So, "FORA: Accelerating DiT through Attention and MLP Layer Reuse"** reuses the outputs from the attention and MLP layers to accelerate DiT inference. Another representative strategy lies in model compression, achieved through the exploitation of sparsity patterns **Wang, X., "Sparsity Patterns for Efficient Model Compression"** and quantization techniques **Liu, Q., "Quantization Methods for Accelerating Diffusion Models"**.

\textbf{Concurrent Work}
During this work, we noted a concurrent study **Kwon, H., "Token-Wise Feature Caching for Fast Inference of Diffusion Models"** that independently employs token-wise feature caching to accelerate diffusion models. While both approaches consider ``Cache Frequency," our CAT Pruning method ranks token importance based on ``noise" magnitude, eliminating the need for intermediate attention scores. This enables seamless integration with \textbf{online softmax} **Liu, J., "Efficient Online Softmax for Fast Inference"** for further \textbf{speedup} while also using \textbf{less memory}.