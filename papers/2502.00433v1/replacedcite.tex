\section{Related Work}
Diffusion models have emerged as powerful generative frameworks in computer vision. However, these models are compute-intensive, often constrained by the high computational cost. This computational bottleneck has led to a surge of research focused on accelerating diffusion models. Here, we highlight several major categories of approaches: parallelization, reduction of sampling steps, and model pruning.

\textbf{Parallelization Methods}
 Despite traditional techniques like tensor parallelism, recent works have introduced novel parallelization strategies specifically tailored to the characteristics of diffusion models. DistriFusion ____, for instance, hides the communication overhead within the computation via asynchronous communication and introduces displaced patch parallelism, while PipeFusion ____ introduces displaced patch parallelism for Inference of Diffusion Transformer Models (DiT ____) and ParaDiGMS ____ rum sampling steps in parallel through iterative refinement.

\textbf{Reducing Sampling Steps}
One of the core challenges with diffusion models is the large number of sampling steps required to produce high-quality outputs, which directly translates to longer inference times. Recent advancements such as DPM Solver ____ and Consistency Models ____ aim to address this bottleneck by developing fast solvers for diffusion ODEs and directly mapping noise to data respectively.  Moreover, the reflow+distillation ____ approach in flow-based works ____ also provides another promising approach to one-step models ____.

\textbf{Leveraging Computational Redundancy} %tgate, fora, deepcache, AT-EDM
Recognizing the iterative nature of diffusion models and the minimal changes in feature representations across consecutive steps, a growing body of research has focused on developing cache-and-reuse mechanisms to reduce inference time. DeepCache ____ reuses the high-level features of the U-Net ____. Block Cache ____ performs caching at a per-block level and adjusts the cached values using a lightweight 'scale-shift' mechanism. TGATE ____ caches the output of the cross-attention module once it converges. FORA ____ reuses the outputs from the attention and MLP layers to accelerate DiT inference. Another representative strategy lies in model compression, achieved through the exploitation of sparsity patterns ____ and quantization techniques ____.

\textbf{Concurrent Work}
During this work, we noted a concurrent study ____ that independently employs token-wise feature caching to accelerate diffusion models. While both approaches consider ``Cache Frequency," our CAT Pruning method ranks token importance based on ``noise" magnitude, eliminating the need for intermediate attention scores. This enables seamless integration with \textbf{online softmax} ____ for further \textbf{speedup} while also using \textbf{less memory}.