\section{Related Works}
\textbf{Visual navigation.} Visual navigation~\cite{krantz2023navigating,kwon2023renderable,pelluri2024transformers,li2023improving,liu2024caven,sun2024fgprompt,wang2024lookahead,zhao2024over} requires an agent to navigate based on visual sensors. It can be categorized into several types, including Visual-and-Language navigation~(VLN), Object Navigation, Image Goal Navigation, etc. Some works~\cite{anderson2018vision,chen2021history,li2022envedit,krantz2023iterative} focus on VLN, which uses additional natural language instructions to depict the navigation targets. These works either depend on detailed language instructions~\cite{wang2023scaling, li2023kerm} or require conversations with humans~\cite{zhang2024dialoc, thomason2020vision} during the navigation process, leading to low usability. 
Object Navigation is proposed with a given object category as the target~\cite{chaplot2020object,mayo2021visual,du2023object, zhang2024imagine}. However, this kind of method can only reach the surrounding area of an object and cannot accurately arrive at a specific location.
% However, since salient objects are needed in the target image, these models fail when arbitrary images are provided, such as the image of a kitchen corner with no object detected.
Given the reasons above, we address the Image Goal Navigation task where an arbitrary image is provided as the target and only an RGB sensor is utilized during the navigation process. The agent must reach the location depicted in the goal image. We study how to make full use of the knowledge in observation to improve navigation performance.

\noindent\textbf{Reinforcement learning in visual navigation.} Since the image navigation method with reinforcement learning~(RL) can learn directly from interacting with the environment in an end-to-end manner, it has gained a great population in recent years~\cite{du2021curious, majumdar2022zson}. Some methods aim to enhance the representation capability of the feature extractors before the RL policy. \cite{sun2024fgprompt} explore fusion methods to guide the observation encoder to focus on goal-relevant regions. \cite{sun2025prioritized} propose a prioritized semantic learning method to improve the agents' semantic ability. Some works~\cite{li2023improving,wang2024lookahead} utilized the pre-training strategy to enforce the agent to have an expectation of the future environments. 
% \cite{majumdar2022zson} use CLIP~\cite{radford2021learning} to align image and text targets, enabling the image-goal navigation model to solve object-goal tasks. 
However, if the agent has a large distance from the goal, these methods may fail to extract useful knowledge from the observations. 
Some methods try to incorporate additional memory mechanisms to enable long-term reasoning and exploit supplementary knowledge from previous states. \cite{mezghan2022memory} trained a state-embedding network to take advantage of the history with external memory. \cite{qiao2022hop} devised a history-and-order pre-training paradigm to exploit past observations and support future prediction. \cite{kim2023topological} inserted semantic information into topological graph memory to obtain a thorough description of history states. \cite{li2024memonav} classified history states into three types to ensure both diversity and long-term memory. However, these methods have no spatial awareness if the agent has never been to the area near the target. On the contrary, we aim to equip the agent with spatial awareness and enable it to analyze whether the observation is in the same space as the goal.

\noindent\textbf{Auxiliary knowledge in visual navigation.} Image-Goal Navigation requires the agent to navigate to an image-specified goal location in an unseen environment using visual observations. Only depending on a single RGB sensor has raised the challenge and makes the task difficult even for humans~\cite{paul2022avlen}. To release the difficulty, auxiliary knowledge is introduced. \cite{liu2024caven} enables the agent to interact with a human for help when it's unable to solve the task. \cite{li2023kerm} utilizes an external pre-trained image description model to provide additional knowledge. \cite{kim2023topological} introduces a pre-trained semantic segmentation model to extract objects in both observations and targets. All of these methods require external datasets to acquire auxiliary knowledge, which may have fairness concerns. In contrast, we devised a Room Expert trained with an unsupervised clustering method without using any additional dataset. Our Room Expert effectively empowers the agent with spatial awareness to analyze the spatial relationship between observations and the goal location, improving the navigation performance.
% the room concept we incorporated into the agent can be acquired with a simple room-style encoder entirely trained on the same dataset as the navigation agent.