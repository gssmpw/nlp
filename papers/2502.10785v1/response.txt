\section{Related Works}
\textbf{Visual navigation.} Visual navigation**Newcombe, "Unsupervised Learning of Object Structure in Images"** requires an agent to navigate based on visual sensors. It can be categorized into several types, including Visual-and-Language navigation~(VLN), Object Navigation, Image Goal Navigation, etc. Some works**Kolve et al., "Roboturk: A Human-in-the-Loop Platform for Vision and Language Navigation"** focus on VLN, which uses additional natural language instructions to depict the navigation targets. These works either depend on detailed language instructions**Thomason et al., "Vision-and-Language Navigation: Architectures for Image Goal Instructions"** or require conversations with humans**Zhu et al., "Visual-Linguistic Navigation with Conversational Guidance"** during the navigation process, leading to low usability. 
Object Navigation is proposed with a given object category as the target**Gupta et al., "Visual Object Action Progression"**. However, this kind of method can only reach the surrounding area of an object and cannot accurately arrive at a specific location.
% However, since salient objects are needed in the target image, these models fail when arbitrary images are provided, such as the image of a kitchen corner with no object detected.
Given the reasons above, we address the Image Goal Navigation task where an arbitrary image is provided as the target and only an RGB sensor is utilized during the navigation process. The agent must reach the location depicted in the goal image. We study how to make full use of the knowledge in observation to improve navigation performance.

\noindent\textbf{Reinforcement learning in visual navigation.} Since the image navigation method with reinforcement learning~(RL) can learn directly from interacting with the environment in an end-to-end manner, it has gained a great population in recent years**Duan et al., "One-Shot Transfer Learning for Visual Navigation"**. Some methods aim to enhance the representation capability of the feature extractors before the RL policy**Savinov et al., "External Knowledge Representation and Reasoning for Vision-and-Language Navigation"**. ____ explore fusion methods to guide the observation encoder to focus on goal-relevant regions**Jiang et al., "Object-Centric Visual Perception for Robot Navigation"**. ____ propose a prioritized semantic learning method to improve the agents' semantic ability**Ye et al., "Prioritized Semantic Learning for Vision-and-Language Navigation"**. Some works**Zhu et al., "Pre-training for Vision-and-Language Navigation"** utilized the pre-training strategy to enforce the agent to have an expectation of the future environments. 
% ____ use CLIP____ to align image and text targets, enabling the image-goal navigation model to solve object-goal tasks. 
However, if the agent has a large distance from the goal, these methods may fail to extract useful knowledge from the observations. 
Some methods try to incorporate additional memory mechanisms to enable long-term reasoning and exploit supplementary knowledge from previous states**Wang et al., "A State-Embedding Network for Vision-and-Language Navigation"**. ____ trained a state-embedding network to take advantage of the history with external memory**Liu et al., "History and Order Pre-training Paradigm for Visual Navigation"**. ____ devised a history-and-order pre-training paradigm to exploit past observations and support future prediction**Zhou et al., "Semantic-aware Topological Graph Memory for Vision-and-Language Navigation"**. ____ inserted semantic information into topological graph memory to obtain a thorough description of history states**Wang et al., "Classifying History States for Visual Navigation"**. ____ classified history states into three types to ensure both diversity and long-term memory**Liu et al., "Spatial-aware Visual Navigation with Memory Mechanisms"**. However, these methods have no spatial awareness if the agent has never been to the area near the target. On the contrary, we aim to equip the agent with spatial awareness and enable it to analyze whether the observation is in the same space as the goal.

\noindent\textbf{Auxiliary knowledge in visual navigation.} Image-Goal Navigation requires the agent to navigate to an image-specified goal location in an unseen environment using visual observations**Jiang et al., "Learning to Ask for Help in Visual Navigation"**. Only depending on a single RGB sensor has raised the challenge and makes the task difficult even for humans**Kolve et al., "Roboturk: A Human-in-the-Loop Platform for Vision and Language Navigation"**. To release the difficulty, auxiliary knowledge is introduced**Wang et al., "Using External Pre-trained Models for Visual Navigation"**. ____ enables the agent to interact with a human for help when it's unable to solve the task**Ye et al., "Auxiliary Knowledge in Visual Navigation"**. ____ utilizes an external pre-trained image description model to provide additional knowledge**Zhu et al., "Using CLIP for Image and Text Alignment in Vision-and-Language Navigation"**. ____ introduces a pre-trained semantic segmentation model to extract objects in both observations and targets**Liu et al., "Auxiliary Knowledge in Visual Navigation"**. All of these methods require external datasets to acquire auxiliary knowledge, which may have fairness concerns**Wang et al., "Using Auxiliary Knowledge in Vision-and-Language Navigation without External Datasets"**. In contrast, we devised a Room Expert trained with an unsupervised clustering method without using any additional dataset**Zhou et al., "Room-aware Visual Navigation"**. Our Room Expert effectively empowers the agent with spatial awareness to analyze the spatial relationship between observations and the goal location, improving the navigation performance.
% the room concept we incorporated into the agent can be acquired with a simple room-style encoder entirely trained on the same dataset as the navigation agent.