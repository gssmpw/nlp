\section{Related Work}
\label{sec:related-work}

\subsection{The Rare Word Problem}
% \paragraph{The Rare Word Problem During Pre-training and Open-Vocabulary Modeling}
Word frequencies in natural corpora follow a highly skewed (Zipfian) distribution \citep{Zipf1949HumanBA}, resulting in a heavy tail of rare words. Additionally, new words are constantly entering the language \citep{Heaps1978InformationRC}.
%Traditional NLP models rely on closed vocabularies and thus must deal with rare and new words. This is referred to as the open-vocabulary problem \citep{luong-etal-2015-addressing}.
%i.e., open-vocabulary modeling \citep{luong-etal-2013-better,luong-etal-2015-addressing,sennrich-etal-2016-neural,kawakami-etal-2017-learning,Schick2020RareWA,schick-schutze-2020-bertram}.
To represent all possible words, various word-form-based methods have been proposed, including subword- and character-based tokenizations and using morphological information (see \citealp{Mielke2021BetweenWA} for a comprehensive survey).
However, representing a word alone does not help in learning it from a few contexts in which it occurs.
Models optimized for conventional language modeling still struggle with the usage of unfamiliar or completely novel words, tokens, or token sequences, where word-forms or token identities alone do not provide enough information \citep{Ott2018AnalyzingUI,Schick2020RareWA,wei-etal-2021-frequency,Razeghi2022ImpactOP,Kim2022UncontrolledLE,Batsuren2024EvaluatingST,Land2024FishingFM}.
Instead of representing new words based on word-forms, we discard word-form information and use a dedicated special placeholder token that is the same for every new word. In this way, we aim to develop a general and efficient ability to learn a word from a few contexts of its usage.
%Several works try to address the problem during language model pre-training.
%\citet{kawakami-etal-2017-learning} utilize burstiness, the distributional property of rare words that they are often used multiple times in the near context \citep{Church2000EmpiricalEO}, and augment their language model with a key-value memory cache to allow copying words from previous contexts. %Similar in-context copying mechanisms are found in Transformers for in-context learning \citep{Olsson2022IncontextLA,Akyrek2024InContextLL,Jelassi2024RepeatAM}.
%Without assuming burstiness, \citet{Wu2020TakingNO} maintains a ``Note Dictionary'' for rare words to more efficiently use cross-sentence information during pre-training.
%In contrast, our \ac{metaicl-w} framework explicitly constructs episodes in which a relatively infrequent new word occurs repeatedly in the context, thus providing the burstiness and Zipfian properties in the training curriculum.

\subsection{Few-Shot Word Learning}
\label{sec:related-work-few-shot-word-learning}
Another line of previous work targets the problem of learning a new word from a few examples.
Most previous work aims to produce a representation for the new word, i.e., an embedding, that fits into the global word embedding space so it can be used in the same way as other learned words \citep{Mikolov2013EfficientEO,Pennington2014GloVeGV}. The embedding can be produced by aggregating the embeddings of the contexts that the new word appears in \citep{Lazaridou2017MultimodalWM,khodak-etal-2018-la}, finetuning the embedding within the context \citep{herbelot-baroni-2017-high,Lampinen2017OneshotAF,hewitt2021initializing,kim-smolensky-2021-testing}, or utilizing the word-form information \citep{luong-etal-2013-better,schick2019learning}. %or combination of context and surface form information \citep{schick2019learning,schick-schutze-2019-attentive,Schick2020RareWA,schick-schutze-2020-bertram}.
More recent work uses Transformer layers to produce the embedding based on Word2Vec embeddings \citep[HiCE]{hu-etal-2019-shot}, or by aggregating similar embeddings of word contexts from a memory system \citep[Mem2Vec]{sun-etal-2018-memory}.
Also related to our approach, \citeauthor{Teehan2024CoLLEGeCE}'s \citeyearpar{Teehan2024CoLLEGeCE} work uses a meta-learning framework named CoLLEGe to train a Transformer encoder to produce an embedding for a new word from its examples of usage.
Our method also targets few-shot word learning, but is simpler than \citet{Teehan2024CoLLEGeCE} in architecture and training and does not produce a separate embedding for each new word.

\subsection{\acl{metaicl}}
%Meta-learning, or learning to learn, has long been a focus in cognitive science and machine learning \citep{Biggs1985Meta,Bengio1991LearningAS,Smith2002ObjectNL,Lake2015HumanlevelCL}.
%Optimization-based meta-learning finds a good parameter initialization such that it can be quickly finetuned to adapt to a new task \citep{MAML}.
%Recent meta-learning methods do not require parameter updates.
%Metric-based meta-learning models \citep{Vinyals2016MatchingNF,Snell2017PrototypicalNF} learn a metric and make a prediction by a metric-weighted sum of labels of support set samples.
Building on LLMs' \acl{icl} abilities \citep{GPT3}, \acf{metaicl} optimizes language models on multiple different tasks, each learned from a few in-context examples \citep{min-etal-2022-metaicl,chen-etal-2022-meta}.\footnote{MetaICL is different from \citet{cf-metaicl-2023}, which uses in-context learning instead of parameter updates to learn from multiple tasks.}
A class of tasks that \ac{metaicl} (or similar curriculums) aim to learn and generalize requires inferring the context-dependent mapping from the symbols to meanings \citep{Lake2023HumanlikeSG,huang2024lexinvariant,Anand2024DualPL,Park2024ICLR}.
We follow this work to use \ac{metaicl} for our word learning task, in which the mapping from a new word to its meaning should be inferred purely from its usage in the context.

%\paragraph{Prompt Tuning}

% \paragraph{Data Distributional Properties}
% New or rare words often have a burstiness distribution property, that is, they tend to occur repeatedly in a near context \citep{Church2000EmpiricalEO}.
% Another common property of language is the Zipfian distribution with a long tail of infrequent items \citep{Zipf1949HumanBA}.
% These properties are found not only in written text but also in child inputs, which may be helpful for child word learning \citep{Smith2018TheDI,Slone2022TheTS}.
% These properties also drive the emergence of in-context learning and its underlying induction head mechanism in Transformer models \citep{Chan2022DataDP,Chen2024ParallelSI,Reddy2023TheMB,Singh2023TheTN}.
% As mentioned above, \citet{kawakami-etal-2017-learning} explicitly utilizes the burstiness in the context, and \citet{Wu2020TakingNO}'s ``Note Dictionary'' for rare words implicitly provides burstiness by allowing the LM to recall information from their other occurrences outside of the current context.
% Our \ac{metaicl} framework explicitly constructs episodes in which a relatively infrequent new word occurs repeatedly in the context, thus providing the burstiness and Zipfian properties in the training curriculum.

%\paragraph{Evaluation of Word Meanings and Generalizations}
%Previous work has proposed various evaluations of meanings and generalizations of rare or few-shot-learned words, based on their learned embeddings.
%Notably, the Stanford Rare Word dataset \citep{luong-etal-2013-better} and the Chimera dataset \citep{Lazaridou2017MultimodalWM} evaluate the correlation between word representations and humans on the similarities of rare words to other words. %and the Contextual Rare Word dataset \citep{khodak-etal-2018-la} extends the former.
%The Definitional Nonce dataset \citep{herbelot-baroni-2017-high} compares learned representations to gold vectors.
%The WNLaMPro dataset \citep{Schick2020RareWA} evaluates semantic and misspelling relations between pairs of words by cloze-style phrases.
%\citet{schick-schutze-2020-bertram} creates test sets of text classification tasks by adversarially replacing influential words with rare synonyms.
%\citet{luong-etal-2015-addressing} evaluates the translation quality of OOV words.
%\citet{Lampinen2017OneshotAF} evaluates the change in perplexities of unseen test sentences using the new word.
%\citet{kim-smolensky-2021-testing} test the learned embedding in sentences distinguishing the syntactic categories of the learned word.
%\citet{wei-etal-2021-frequency} test the learned word embeddings by the generalization to unseen noun-verb pairs in the English subject-verb agreement task, one of the targeted linguistic evaluations \citep{marvin-linzen-2018-targeted}.
%\citet{Teehan2024CoLLEGeCE} evaluate their generated embeddings on three more challenging tasks: GRE-style fill-in-the-blank verbal reasoning, definition inference and generation, and Internet slang usage.
%We select several applicable evaluations and adapt them for our in-context setting.