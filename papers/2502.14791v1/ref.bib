# LMs

@inproceedings{Mikolov2013EfficientEO,
  title={Efficient Estimation of Word Representations in Vector Space},
  author={Tomas Mikolov and Kai Chen and Gregory S. Corrado and Jeffrey Dean},
  booktitle={International Conference on Learning Representations},
  year={2013}
}

@inproceedings{Pennington2014GloVeGV,
  title={{GloVe}: Global Vectors for Word Representation},
  author={Jeffrey Pennington and Richard Socher and Christopher D. Manning},
  booktitle={Conference on Empirical Methods in Natural Language Processing},
  year={2014}
}

@inproceedings{GPT2,
  title={Language Models are Unsupervised Multitask Learners},
  author={Alec Radford and Jeff Wu and Rewon Child and David Luan and Dario Amodei and Ilya Sutskever},
  year={2019}
}

@inproceedings{GPT3,
 author = {Brown, Tom and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared D and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and Agarwal, Sandhini and Herbert-Voss, Ariel and Krueger, Gretchen and Henighan, Tom and Child, Rewon and Ramesh, Aditya and Ziegler, Daniel and Wu, Jeffrey and Winter, Clemens and Hesse, Chris and Chen, Mark and Sigler, Eric and Litwin, Mateusz and Gray, Scott and Chess, Benjamin and Clark, Jack and Berner, Christopher and McCandlish, Sam and Radford, Alec and Sutskever, Ilya and Amodei, Dario},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {H. Larochelle and M. Ranzato and R. Hadsell and M.F. Balcan and H. Lin},
 pages = {1877--1901},
 publisher = {Curran Associates, Inc.},
 title = {Language Models are Few-Shot Learners},
 url = {https://proceedings.neurips.cc/paper_files/paper/2020/file/1457c0d6bfcb4967418bfb8ac142f64a-Paper.pdf},
 volume = {33},
 year = {2020}
}

@article{GPT4,
  title={{GPT-4} technical report},
  author={{OpenAI}},
  journal={arXiv preprint arXiv:2303.08774},
  year={2023}
}

@article{GPT-4o,
  title={{GPT-4o} System Card},
  author={{OpenAI}},
  journal={arXiv preprint arXiv:2410.21276},
  year={2024}
}

@inproceedings{Pythia,
  title={Pythia: A suite for analyzing large language models across training and scaling},
  author={Biderman, Stella and Schoelkopf, Hailey and Anthony, Quentin Gregory and Bradley, Herbie and O’Brien, Kyle and Hallahan, Eric and Khan, Mohammad Aflah and Purohit, Shivanshu and Prashanth, USVSN Sai and Raff, Edward and others},
  booktitle={International Conference on Machine Learning},
  pages={2397--2430},
  year={2023},
  organization={PMLR}
}

@article{Llama-2,
  title={Llama 2: Open Foundation and Fine-Tuned Chat Models},
  author={Hugo Touvron and Louis Martin and Kevin R. Stone and Peter Albert and Amjad Almahairi and Yasmine Babaei and Nikolay Bashlykov and Soumya Batra and Prajjwal Bhargava and Shruti Bhosale and Daniel M. Bikel and Lukas Blecher and Cristian Cant{\'o}n Ferrer and Moya Chen and Guillem Cucurull and David Esiobu and Jude Fernandes and Jeremy Fu and Wenyin Fu and Brian Fuller and Cynthia Gao and Vedanuj Goswami and Naman Goyal and Anthony S. Hartshorn and Saghar Hosseini and Rui Hou and Hakan Inan and Marcin Kardas and Viktor Kerkez and Madian Khabsa and Isabel M. Kloumann and A. V. Korenev and Punit Singh Koura and Marie-Anne Lachaux and Thibaut Lavril and Jenya Lee and Diana Liskovich and Yinghai Lu and Yuning Mao and Xavier Martinet and Todor Mihaylov and Pushkar Mishra and Igor Molybog and Yixin Nie and Andrew Poulton and Jeremy Reizenstein and Rashi Rungta and Kalyan Saladi and Alan Schelten and Ruan Silva and Eric Michael Smith and R. Subramanian and Xia Tan and Binh Tang and Ross Taylor and Adina Williams and Jian Xiang Kuan and Puxin Xu and Zhengxu Yan and Iliyan Zarov and Yuchen Zhang and Angela Fan and Melissa Hall Melanie Kambadur and Sharan Narang and Aur{\'e}lien Rodriguez and Robert Stojnic and Sergey Edunov and Thomas Scialom},
  journal={arXiv preprint arXiv:2307.09288},
  year={2023}
}

@article{Llama-3,
  title={The llama 3 herd of models},
  author={{Llama Team, Meta AI}},
  journal={arXiv preprint arXiv:2407.21783},
  year={2024}
}

@misc{GPT-NeoX,
  title = {{GPT-NeoX}: Large Scale Autoregressive Language Modeling in PyTorch},
  author = {Andonian, Alex and Anthony, Quentin and Biderman, Stella and Black, Sid and Gali, Preetham and Gao, Leo and Hallahan, Eric and Levy-Kramer, Josh and Leahy, Connor and Nestler, Lucas and Parker, Kip and Pieler, Michael and Phang, Jason and Purohit, Shivanshu and Schoelkopf, Hailey and Stander, Dashiell and Songz, Tri and Tigges, Curt and Thérien, Benjamin and Wang, Phil and Weinbach, Samuel},
  url = {https://www.github.com/eleutherai/gpt-neox},
  doi = {10.5281/zenodo.5879544},
  month = {9},
  year = {2023},
  version = {2.0.0},
}

@article{LSTM,
  title={Long short-term memory},
  author={Hochreiter, Sepp and Schmidhuber, J{\"u}rgen},
  journal={Neural computation},
  volume={9},
  number={8},
  pages={1735--1780},
  year={1997},
  publisher={MIT press}
}

@article{T5,
  title={Exploring the limits of transfer learning with a unified text-to-text transformer},
  author={Raffel, Colin and Shazeer, Noam and Roberts, Adam and Lee, Katherine and Narang, Sharan and Matena, Michael and Zhou, Yanqi and Li, Wei and Liu, Peter J},
  journal={Journal of machine learning research},
  volume={21},
  number={140},
  pages={1--67},
  year={2020}
}

# Distributional properties and in-context learning

@book{Zipf1949HumanBA,
  title={Human behavior and the principle of least effort},
  author={George Kingsley Zipf},
  year={1949},
  publisher={Addison-Wesley Press}
}

@inproceedings{Church2000EmpiricalEO,
  title={Empirical Estimates of Adaptation: The chance of Two Noriegas is closer to $p/2$ than $p^2$},
  author={Kenneth Ward Church},
  booktitle={International Conference on Computational Linguistics},
  year={2000}
}

@article{Smith2018TheDI,
  title={The Developing Infant Creates a Curriculum for Statistical Learning},
  author={Linda B. Smith and Swapnaa Jayaraman and Elizabeth M. Clerkin and Chen Yu},
  journal={Trends in Cognitive Sciences},
  year={2018},
  volume={22},
  pages={325-336}
}

@article{Slone2022TheTS,
  title={The temporal structure of parent talk to toddlers about objects},
  author={Lauren K Slone and Drew H. Abney and Linda B. Smith and Chen Yu},
  journal={Cognition},
  year={2022},
  volume={230}
}


@article{Chan2022DataDP,
  title={Data distributional properties drive emergent in-context learning in transformers},
  author={Chan, Stephanie and Santoro, Adam and Lampinen, Andrew and Wang, Jane and Singh, Aaditya and Richemond, Pierre and McClelland, James and Hill, Felix},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={18878--18891},
  year={2022}
}

@article{Singh2023TheTN,
  title={The transient nature of emergent in-context learning in transformers},
  author={Singh, Aaditya and Chan, Stephanie and Moskovitz, Ted and Grant, Erin and Saxe, Andrew and Hill, Felix},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2024}
}

@inproceedings{Anand2024DualPL,
  title={Dual Process Learning: Controlling Use of In-Context vs. In-Weights Strategies with Weight Forgetting},
  author={Anand, Suraj and Lepori, Michael A and Merullo, Jack and Pavlick, Ellie},
  booktitle={The Thirteenth International Conference on Learning Representations},
  year={2025}
}

@inproceedings{Chen2024ParallelSI,
    title = "Parallel Structures in Pre-training Data Yield In-Context Learning",
    author = "Chen, Yanda  and
      Zhao, Chen  and
      Yu, Zhou  and
      McKeown, Kathleen  and
      He, He",
    editor = "Ku, Lun-Wei  and
      Martins, Andre  and
      Srikumar, Vivek",
    booktitle = "Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = aug,
    year = "2024",
    address = "Bangkok, Thailand",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2024.acl-long.465/",
    doi = "10.18653/v1/2024.acl-long.465",
    pages = "8582--8592",
    abstract = "Pre-trained language models (LMs) are capable of in-context learning (ICL): they can adapt to a task with only a few examples given in the prompt without any parameter update. However, it is unclear where this capability comes from as there is a stark distribution shift between pre-training text and ICL prompts. In this work, we study what patterns of the pre-training data contribute to ICL. We find that LMs' ICL ability depends on $\textit{parallel structures}$ in the pre-training data{---}pairs of phrases following similar templates in the same context window. Specifically, we detect parallel structures by checking whether training on one phrase improves prediction of the other, and conduct ablation experiments to study their effect on ICL. We show that removing parallel structures in the pre-training data reduces LMs' ICL accuracy by $\textbf{51}${\%} (vs 2{\%} from random ablation). This drop persists even when excluding common patterns such as n-gram repetitions and long-range dependency, showing the diversity and generality of parallel structures. A closer look at the detected parallel structures indicates that they cover diverse linguistic tasks and span long distances in the data."
}

@inproceedings{Reddy2023TheMB,
  title={The mechanistic basis of data dependence and abrupt learning in an in-context classification task},
  author={Reddy, Gautam},
  booktitle={The Twelfth International Conference on Learning Representations},
  year={2024}
}

@inproceedings{Park2024ICLR,
  title={{ICLR}: In-Context Learning of Representations}, 
  author={Core Francisco Park and Andrew Lee and Ekdeep Singh Lubana and Yongyi Yang and Maya Okawa and Kento Nishi and Martin Wattenberg and Hidenori Tanaka},
  booktitle={The Thirteenth International Conference on Learning Representations},
  year={2025}
}

# Language development

@article{Smith2002ObjectNL,
  title={Object name Learning Provides On-the-Job Training for Attention},
  author={Linda B. Smith and Susan Scanlon Jones and Barbara Landau and Lisa Gershkoff-Stowe and Larissa K. Samuelson},
  journal={Psychological Science},
  year={2002},
  volume={13},
  pages={13 - 19}
}

@article{Bergelson2020TheCB,
  title={The Comprehension Boost in Early Word Learning: Older Infants Are Better Learners.},
  author={Elika Bergelson},
  journal={Child development perspectives},
  year={2020},
  volume={14 3},
  pages={142-149}
}

# Dealing with the OOV problem, open-vocabulary modeling, and tokenization

@inproceedings{luong-etal-2013-better,
    title = "Better Word Representations with Recursive Neural Networks for Morphology",
    author = "Luong, Thang  and
      Socher, Richard  and
      Manning, Christopher",
    editor = "Hockenmaier, Julia  and
      Riedel, Sebastian",
    booktitle = "Proceedings of the Seventeenth Conference on Computational Natural Language Learning",
    month = aug,
    year = "2013",
    address = "Sofia, Bulgaria",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/W13-3512",
    pages = "104--113",
}

@inproceedings{luong-etal-2015-addressing,
    title = "Addressing the Rare Word Problem in Neural Machine Translation",
    author = "Luong, Thang  and
      Sutskever, Ilya  and
      Le, Quoc  and
      Vinyals, Oriol  and
      Zaremba, Wojciech",
    editor = "Zong, Chengqing  and
      Strube, Michael",
    booktitle = "Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)",
    month = jul,
    year = "2015",
    address = "Beijing, China",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/P15-1002",
    doi = "10.3115/v1/P15-1002",
    pages = "11--19",
}

@inproceedings{sennrich-etal-2016-neural,
    title = "Neural Machine Translation of Rare Words with Subword Units",
    author = "Sennrich, Rico  and
      Haddow, Barry  and
      Birch, Alexandra",
    editor = "Erk, Katrin  and
      Smith, Noah A.",
    booktitle = "Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = aug,
    year = "2016",
    address = "Berlin, Germany",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/P16-1162",
    doi = "10.18653/v1/P16-1162",
    pages = "1715--1725",
}

@inproceedings{kawakami-etal-2017-learning,
    title = "Learning to Create and Reuse Words in Open-Vocabulary Neural Language Modeling",
    author = "Kawakami, Kazuya  and
      Dyer, Chris  and
      Blunsom, Phil",
    editor = "Barzilay, Regina  and
      Kan, Min-Yen",
    booktitle = "Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = jul,
    year = "2017",
    address = "Vancouver, Canada",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/P17-1137",
    doi = "10.18653/v1/P17-1137",
    pages = "1492--1502",
    abstract = "Fixed-vocabulary language models fail to account for one of the most characteristic statistical facts of natural language: the frequent creation and reuse of new word types. Although character-level language models offer a partial solution in that they can create word types not attested in the training corpus, they do not capture the {``}bursty{''} distribution of such words. In this paper, we augment a hierarchical LSTM language model that generates sequences of word tokens character by character with a caching mechanism that learns to reuse previously generated words. To validate our model we construct a new open-vocabulary language modeling corpus (the Multilingual Wikipedia Corpus; MWC) from comparable Wikipedia articles in 7 typologically diverse languages and demonstrate the effectiveness of our model across this range of languages.",
}

@inproceedings{schick-schutze-2019-attentive,
    title = "Attentive Mimicking: Better Word Embeddings by Attending to Informative Contexts",
    author = {Schick, Timo  and
      Sch{\"u}tze, Hinrich},
    editor = "Burstein, Jill  and
      Doran, Christy  and
      Solorio, Thamar",
    booktitle = "Proceedings of the 2019 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)",
    month = jun,
    year = "2019",
    address = "Minneapolis, Minnesota",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/N19-1048",
    doi = "10.18653/v1/N19-1048",
    pages = "489--494",
    abstract = "Learning high-quality embeddings for rare words is a hard problem because of sparse context information. Mimicking (Pinter et al., 2017) has been proposed as a solution: given embeddings learned by a standard algorithm, a model is first trained to reproduce embeddings of frequent words from their surface form and then used to compute embeddings for rare words. In this paper, we introduce attentive mimicking: the mimicking model is given access not only to a word{'}s surface form, but also to all available contexts and learns to attend to the most informative and reliable contexts for computing an embedding. In an evaluation on four tasks, we show that attentive mimicking outperforms previous work for both rare and medium-frequency words. Thus, compared to previous work, attentive mimicking improves embeddings for a much larger part of the vocabulary, including the medium-frequency range.",
}

@inproceedings{schick2019learning,
  title={Learning semantic representations for novel words: Leveraging both form and context},
  author={Schick, Timo and Sch{\"u}tze, Hinrich},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  volume={33},
  pages={6965--6973},
  year={2019}
}

@article{Mielke2021BetweenWA,
  title={Between words and characters: A brief history of open-vocabulary modeling and tokenization in NLP},
  author={Mielke, Sabrina J and Alyafeai, Zaid and Salesky, Elizabeth and Raffel, Colin and Dey, Manan and Gall{\'e}, Matthias and Raja, Arun and Si, Chenglei and Lee, Wilson Y and Sagot, Beno{\^\i}t and others},
  journal={arXiv preprint arXiv:2112.10508},
  year={2021}
}

@article{Godey2023HeadlessLM,
  title={Headless Language Models: Learning without Predicting with Contrastive Weight Tying},
  author={Godey, Nathan and de la Clergerie, {\'E}ric and Sagot, Beno{\^\i}t},
  journal={arXiv preprint arXiv:2309.08351},
  year={2023}
}

# New Embedding Methods

@article{Lazaridou2017MultimodalWM,
  title={Multimodal Word Meaning Induction From Minimal Exposure to Natural Text.},
  author={Angeliki Lazaridou and Marco Marelli and Marco Baroni},
  journal={Cognitive science},
  year={2017},
  volume={41 Suppl 4},
  pages={677-705}
}

@inproceedings{herbelot-baroni-2017-high,
    title = "High-risk learning: acquiring new word vectors from tiny data",
    author = "Herbelot, Aur{\'e}lie  and
      Baroni, Marco",
    editor = "Palmer, Martha  and
      Hwa, Rebecca  and
      Riedel, Sebastian",
    booktitle = "Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing",
    month = sep,
    year = "2017",
    address = "Copenhagen, Denmark",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/D17-1030",
    doi = "10.18653/v1/D17-1030",
    pages = "304--309",
    abstract = "Distributional semantics models are known to struggle with small data. It is generally accepted that in order to learn {`}a good vector{'} for a word, a model must have sufficient examples of its usage. This contradicts the fact that humans can guess the meaning of a word from a few occurrences only. In this paper, we show that a neural language model such as Word2Vec only necessitates minor modifications to its standard architecture to learn new terms from tiny data, using background knowledge from a previously learnt semantic space. We test our model on word definitions and on a nonce task involving 2-6 sentences{'} worth of context, showing a large increase in performance over state-of-the-art models on the definitional task.",
}

@article{Lampinen2017OneshotAF,
  title={One-shot and few-shot learning of word embeddings},
  author={Lampinen, Andrew K and McClelland, James L},
  journal={arXiv preprint arXiv:1710.10280},
  year={2017}
}

@inproceedings{khodak-etal-2018-la,
    title = "A La Carte Embedding: Cheap but Effective Induction of Semantic Feature Vectors",
    author = "Khodak, Mikhail  and
      Saunshi, Nikunj  and
      Liang, Yingyu  and
      Ma, Tengyu  and
      Stewart, Brandon  and
      Arora, Sanjeev",
    editor = "Gurevych, Iryna  and
      Miyao, Yusuke",
    booktitle = "Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = jul,
    year = "2018",
    address = "Melbourne, Australia",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/P18-1002",
    doi = "10.18653/v1/P18-1002",
    pages = "12--22",
    abstract = "Motivations like domain adaptation, transfer learning, and feature learning have fueled interest in inducing embeddings for rare or unseen words, n-grams, synsets, and other textual features. This paper introduces a la carte embedding, a simple and general alternative to the usual word2vec-based approaches for building such representations that is based upon recent theoretical results for GloVe-like embeddings. Our method relies mainly on a linear transformation that is efficiently learnable using pretrained word vectors and linear regression. This transform is applicable on the fly in the future when a new text feature or rare word is encountered, even if only a single usage example is available. We introduce a new dataset showing how the a la carte method requires fewer examples of words in context to learn high-quality embeddings and we obtain state-of-the-art results on a nonce task and some unsupervised document classification tasks.",
}

@inproceedings{hu-etal-2019-shot,
    title = "Few-Shot Representation Learning for Out-Of-Vocabulary Words",
    author = "Hu, Ziniu  and
      Chen, Ting  and
      Chang, Kai-Wei  and
      Sun, Yizhou",
    editor = "Korhonen, Anna  and
      Traum, David  and
      M{\`a}rquez, Llu{\'\i}s",
    booktitle = "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics",
    month = jul,
    year = "2019",
    address = "Florence, Italy",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/P19-1402",
    doi = "10.18653/v1/P19-1402",
    pages = "4102--4112",
    abstract = "Existing approaches for learning word embedding often assume there are sufficient occurrences for each word in the corpus, such that the representation of words can be accurately estimated from their contexts. However, in real-world scenarios, out-of-vocabulary (a.k.a. OOV) words that do not appear in training corpus emerge frequently. How to learn accurate representations of these words to augment a pre-trained embedding by only a few observations is a challenging research problem. In this paper, we formulate the learning of OOV embedding as a few-shot regression problem by fitting a representation function to predict an oracle embedding vector (defined as embedding trained with abundant observations) based on limited contexts. Specifically, we propose a novel hierarchical attention network-based embedding framework to serve as the neural regression function, in which the context information of a word is encoded and aggregated from K observations. Furthermore, we propose to use Model-Agnostic Meta-Learning (MAML) for adapting the learned model to the new corpus fast and robustly. Experiments show that the proposed approach significantly outperforms existing methods in constructing an accurate embedding for OOV words and improves downstream tasks when the embedding is utilized.",
}

@inproceedings{sun-etal-2018-memory,
    title = "Memory, Show the Way: Memory Based Few Shot Word Representation Learning",
    author = "Sun, Jingyuan  and
      Wang, Shaonan  and
      Zong, Chengqing",
    editor = "Riloff, Ellen  and
      Chiang, David  and
      Hockenmaier, Julia  and
      Tsujii, Jun{'}ichi",
    booktitle = "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing",
    month = oct # "-" # nov,
    year = "2018",
    address = "Brussels, Belgium",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/D18-1173",
    doi = "10.18653/v1/D18-1173",
    pages = "1435--1444",
    abstract = "Distributional semantic models (DSMs) generally require sufficient examples for a word to learn a high quality representation. This is in stark contrast with human who can guess the meaning of a word from one or a few referents only. In this paper, we propose Mem2Vec, a memory based embedding learning method capable of acquiring high quality word representations from fairly limited context. Our method directly adapts the representations produced by a DSM with a longterm memory to guide its guess of a novel word. Based on a pre-trained embedding space, the proposed method delivers impressive performance on two challenging few-shot word similarity tasks. Embeddings learned with our method also lead to considerable improvements over strong baselines on NER and sentiment classification.",
}

@inproceedings{Teehan2024CoLLEGeCE,
  title={Co{LLEG}e: Concept Embedding Generation for Large Language Models},
  author={Ryan Teehan and Brenden Lake and Mengye Ren},
  booktitle={First Conference on Language Modeling},
  year={2024},
  url={https://openreview.net/forum?id=Fkr1yVUb9G}
}

@misc{hewitt2021initializing,
  author = {Hewitt, John},
  title = {Initializing New Word Embeddings for Pretrained Language Models},
  year = {2021},
  url = {https://nlp.stanford.edu/~johnhew/vocab-expansion.html}
}

@inproceedings{kim-smolensky-2021-testing,
    title = "Testing for Grammatical Category Abstraction in Neural Language Models",
    author = "Kim, Najoung  and
      Smolensky, Paul",
    editor = "Ettinger, Allyson  and
      Pavlick, Ellie  and
      Prickett, Brandon",
    booktitle = "Proceedings of the Society for Computation in Linguistics 2021",
    month = feb,
    year = "2021",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.scil-1.59",
    pages = "467--470",
}

# Meta-Learning

@article{Biggs1985Meta,
  title={The role of meta-learning in study processes},
  author={John B. Biggs},
  journal={British Journal of Educational Psychology},
  year={1985},
  volume={55},
  pages={185-212}
}

@article{Bengio1991LearningAS,
  title={Learning a synaptic learning rule},
  author={Yoshua Bengio and Samy Bengio and Jocelyn Cloutier},
  journal={IJCNN-91-Seattle International Joint Conference on Neural Networks},
  year={1991},
  volume={ii},
  pages={969 vol.2}
}

@inproceedings{MAML,
  title={Model-Agnostic Meta-Learning for Fast Adaptation of Deep Networks},
  author={Chelsea Finn and P. Abbeel and Sergey Levine},
  booktitle={International Conference on Machine Learning},
  year={2017}
}

@inproceedings{min-etal-2022-metaicl,
    title = "{M}eta{ICL}: Learning to Learn In Context",
    author = "Min, Sewon  and
      Lewis, Mike  and
      Zettlemoyer, Luke  and
      Hajishirzi, Hannaneh",
    editor = "Carpuat, Marine  and
      de Marneffe, Marie-Catherine  and
      Meza Ruiz, Ivan Vladimir",
    booktitle = "Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies",
    month = jul,
    year = "2022",
    address = "Seattle, United States",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.naacl-main.201",
    doi = "10.18653/v1/2022.naacl-main.201",
    pages = "2791--2809",
    abstract = "We introduce MetaICL (Meta-training for In-Context Learning), a new meta-training framework for few-shot learning where a pretrained language model is tuned to do in-context learning on a large set of training tasks. This meta-training enables the model to more effectively learn a new task in context at test time, by simply conditioning on a few training examples with no parameter updates or task-specific templates. We experiment on a large, diverse collection of tasks consisting of 142 NLP datasets including classification, question answering, natural language inference, paraphrase detection and more, across seven different meta-training/target splits. MetaICL outperforms a range of baselines including in-context learning without meta-training and multi-task learning followed by zero-shot transfer. We find that the gains are particularly significant for target tasks that have domain shifts from the meta-training tasks, and that using a diverse set of the meta-training tasks is key to improvements. We also show that MetaICL approaches (and sometimes beats) the performance of models fully finetuned on the target task training data, and outperforms much bigger models with nearly 8x parameters.",
}

@inproceedings{chen-etal-2022-meta,
    title = "Meta-learning via Language Model In-context Tuning",
    author = "Chen, Yanda  and
      Zhong, Ruiqi  and
      Zha, Sheng  and
      Karypis, George  and
      He, He",
    editor = "Muresan, Smaranda  and
      Nakov, Preslav  and
      Villavicencio, Aline",
    booktitle = "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = may,
    year = "2022",
    address = "Dublin, Ireland",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.acl-long.53",
    doi = "10.18653/v1/2022.acl-long.53",
    pages = "719--730",
    abstract = "The goal of meta-learning is to learn to adapt to a new task with only a few labeled examples. Inspired by the recent progress in large language models, we propose $\textit{in-context tuning}$ (ICT), which recasts task adaptation and prediction as a simple sequence prediction problem: to form the input sequence, we concatenate the task instruction, labeled in-context examples, and the target input to predict; to meta-train the model to learn from in-context examples, we fine-tune a pre-trained language model (LM) to predict the target label given the input sequence on a collection of tasks.We benchmark our method on two collections of text classification tasks: LAMA and BinaryClfs. Compared to MAML which adapts the model through gradient descent, our method leverages the inductive bias of pre-trained LMs to perform pattern matching, and outperforms MAML by an absolute 6{\%} average AUC-ROC score on BinaryClfs, gaining more advantage with increasing model size. Compared to non-fine-tuned in-context learning (i.e. prompting a raw LM), in-context tuning meta-trains the model to learn from in-context examples. On BinaryClfs, ICT improves the average AUC-ROC score by an absolute 10{\%}, and reduces the variance due to example ordering by 6x and example choices by 2x.",
}

@inproceedings{cf-metaicl-2023,
 author = {Coda-Forno, Julian and Binz, Marcel and Akata, Zeynep and Botvinick, Matt and Wang, Jane and Schulz, Eric},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {A. Oh and T. Naumann and A. Globerson and K. Saenko and M. Hardt and S. Levine},
 pages = {65189--65201},
 publisher = {Curran Associates, Inc.},
 title = {Meta-in-context learning in large language models},
 url = {https://proceedings.neurips.cc/paper_files/paper/2023/file/cda04d7ea67ea1376bf8c6962d8541e0-Paper-Conference.pdf},
 volume = {36},
 year = {2023}
}

@article{Lake2023HumanlikeSG,
  title={Human-like systematic generalization through a meta-learning neural network},
  author={Brenden M. Lake and Marco Baroni},
  journal={Nature},
  year={2023},
  volume={623},
  pages={115 - 121}
}

# In-context learning distillation

@article{Huang2022IncontextLD,
  title={In-context learning distillation: Transferring few-shot learning ability of pre-trained language models},
  author={Huang, Yukun and Chen, Yanda and Yu, Zhou and McKeown, Kathleen},
  journal={arXiv preprint arXiv:2212.10670},
  year={2022}
}

# Distilling Bayesian priors into NNs through meta-learning

@article{McCoy2023ModelingRL,
  title={Modeling rapid language learning by distilling Bayesian priors into artificial neural networks},
  author={McCoy, R Thomas and Griffiths, Thomas L},
  journal={arXiv preprint arXiv:2305.14701},
  year={2023}
}

@inproceedings{Grant2018RecastingGM,
  title={Recasting Gradient-Based Meta-Learning as Hierarchical Bayes},
  author={Grant, Erin and Finn, Chelsea and Levine, Sergey and Darrell, Trevor and Griffiths, Thomas},
  booktitle={International Conference on Learning Representations},
  year={2018}
}

@article{Kim2018BayesianMM,
  title={Bayesian model-agnostic meta-learning},
  author={Yoon, Jaesik and Kim, Taesup and Dia, Ousmane and Kim, Sungwoong and Bengio, Yoshua and Ahn, Sungjin},
  journal={Advances in Neural Information Processing Systems},
  volume={31},
  year={2018}
}

# Blessing of abstraction

@article{Goodman2011LearningAT,
  title={Learning a theory of causality.},
  author={Noah D. Goodman and Tomer David Ullman and Joshua B. Tenenbaum},
  journal={Psychological review},
  year={2011},
  volume={118 1},
  pages={110-9}
}

@article{Gershman2017OnTB,
  title={On the Blessing of Abstraction},
  author={Samuel J. Gershman},
  journal={Quarterly Journal of Experimental Psychology},
  year={2017},
  volume={70},
  pages={361 - 365}
}

# Child datasets

@article{CHILDES,
  title={The {CHILDES} project: tools for analyzing talk},
  author={Brian MacWhinney},
  journal={Child Language Teaching and Therapy},
  year={1992},
  volume={8},
  pages={217 - 218}
}

@inproceedings{linzen2020accelerate,
    title = "How Can We Accelerate Progress Towards Human-like Linguistic Generalization?",
    author = "Linzen, Tal",
    editor = "Jurafsky, Dan  and
      Chai, Joyce  and
      Schluter, Natalie  and
      Tetreault, Joel",
    booktitle = "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics",
    month = jul,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.acl-main.465",
    doi = "10.18653/v1/2020.acl-main.465",
    pages = "5210--5217",
    abstract = "This position paper describes and critiques the Pretraining-Agnostic Identically Distributed (PAID) evaluation paradigm, which has become a central tool for measuring progress in natural language understanding. This paradigm consists of three stages: (1) pre-training of a word prediction model on a corpus of arbitrary size; (2) fine-tuning (transfer learning) on a training set representing a classification task; (3) evaluation on a test set drawn from the same distribution as that training set. This paradigm favors simple, low-bias architectures, which, first, can be scaled to process vast amounts of data, and second, can capture the fine-grained statistical properties of a particular data set, regardless of whether those properties are likely to generalize to examples of the task outside the data set. This contrasts with humans, who learn language from several orders of magnitude less data than the systems favored by this evaluation paradigm, and generalize to new tasks in a consistent way. We advocate for supplementing or replacing PAID with paradigms that reward architectures that generalize as quickly and robustly as humans.",
}

@inproceedings{BabyLM,
    title = "Findings of the {B}aby{LM} Challenge: Sample-Efficient Pretraining on Developmentally Plausible Corpora",
    author = "Warstadt, Alex  and
      Mueller, Aaron  and
      Choshen, Leshem  and
      Wilcox, Ethan  and
      Zhuang, Chengxu  and
      Ciro, Juan  and
      Mosquera, Rafael  and
      Paranjabe, Bhargavi  and
      Williams, Adina  and
      Linzen, Tal  and
      Cotterell, Ryan",
    editor = "Warstadt, Alex  and
      Mueller, Aaron  and
      Choshen, Leshem  and
      Wilcox, Ethan  and
      Zhuang, Chengxu  and
      Ciro, Juan  and
      Mosquera, Rafael  and
      Paranjabe, Bhargavi  and
      Williams, Adina  and
      Linzen, Tal  and
      Cotterell, Ryan",
    booktitle = "Proceedings of the BabyLM Challenge at the 27th Conference on Computational Natural Language Learning",
    month = dec,
    year = "2023",
    address = "Singapore",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2023.conll-babylm.1",
    doi = "10.18653/v1/2023.conll-babylm.1",
    pages = "1--34",
}

@article{SAYCam,
  title={SAYCam: A Large, Longitudinal Audiovisual Dataset Recorded From the Infant’s Perspective},
  author={Jessica Sullivan and Michelle Mei and Andrew Perfors and Erica H Wojcik and Michael C. Frank},
  journal={Open Mind: Discoveries in Cognitive Science},
  year={2020},
  volume={5},
  pages={20 - 29}
}

# Child data opinions

@article{Warstadt2022WhatAN,
  title={What Artificial Neural Networks Can Tell Us About Human Language Acquisition},
  author={Alex Warstadt and Samuel R. Bowman},
  journal={Algebraic Structures in Natural Language},
  year={2022},
  pages={17–60},
  publisher={CRC Press}
}

# Child data analysis

@article{Wang2023FindingSI,
  title={Finding Structure in One Child's Linguistic Experience.},
  author={Wentao Wang and Wai Keen Vong and Najoung Kim and Brenden M. Lake},
  journal={Cognitive science},
  year={2023},
  volume={47 6},
  pages={e13305}
}

@article{Qin2024ASI,
  title={A systematic investigation of learnability from single child linguistic input},
  author={Yulu Qin and Wentao Wang and Brenden M. Lake},
  journal={CogSci},
  year={2024}
}

@inproceedings{huebner2021babyberta,
  title={{BabyBERTa}: Learning more grammar with small-scale child-directed language},
  author={Huebner, Philip A and Sulem, Elior and Cynthia, Fisher and Roth, Dan},
  booktitle={Proceedings of the 25th conference on computational natural language learning},
  pages={624--646},
  year={2021}
}

@article{Frank2023BridgingTD,
  title={Bridging the data gap between children and large language models},
  author={Michael C. Frank},
  journal={Trends in cognitive sciences},
  year={2023}
}

# Syntactic Bootstrapping

@article{Braine1992,
title = {What sort of innate structure is needed to “bootstrap” into syntax?},
journal = {Cognition},
volume = {45},
number = {1},
pages = {77-100},
year = {1992},
issn = {0010-0277},
doi = {https://doi.org/10.1016/0010-0277(92)90024-C},
url = {https://www.sciencedirect.com/science/article/pii/001002779290024C},
author = {Martin D.S. Braine},
abstract = {The paper starts from the Pinker's theory of the acquisition of phrase structure; it shows that it is possible to drop all the assumptions about innate syntactic structure from this theory. These assumptions can be replaced by assumptions about the basic structure of semantic representation available at the outset of language acquisition, without penalizing the acquisition of basic phrase structure rules. Essentially, the role played by X-bar theory in Pinker's model would be played by the (presumably innate) structure of the language of thought in the revised parallel model. Bootstrapping and semantic assimilation theories are shown to be formally very similar, though making different primitive assumptions. In their primitives, semantic assimilation theories have the advantage that they can offer an account of the origin of syntactic categories instead of postulating them as primitive. Ways of improving on the semantic assimilation version of Pinker's theory are considered, including a way of deriving the NP-VP constituent division that appears to have a better fit than Pinker's to evidence on language variation.}
}

@article{gleitman1990structural,
  title={The structural sources of verb meanings},
  author={Gleitman, Lila},
  journal={Language acquisition},
  volume={1},
  number={1},
  pages={3--55},
  year={1990},
  publisher={Taylor \& Francis}
}

@book{landau2009language,
  title={Language and experience: Evidence from the blind child},
  author={Landau, Barbara and Gleitman, Lila R and Landau, Barbara},
  volume={8},
  year={2009},
  publisher={Harvard University Press}
}

@article{trueswell2007learning,
  title={Learning to parse and its implications for language acquisition},
  author={Trueswell, John C and Gleitman, Lila R},
  year={2007}
}

# Word segmentation

@article{Murphy2024WhatIA,
  title={What is a word?},
  author={Murphy, Elliot},
  journal={arXiv preprint arXiv:2402.12605},
  year={2024}
}

@inproceedings{kudo-richardson-2018-sentencepiece,
    title = "{S}entence{P}iece: A simple and language independent subword tokenizer and detokenizer for Neural Text Processing",
    author = "Kudo, Taku  and
      Richardson, John",
    editor = "Blanco, Eduardo  and
      Lu, Wei",
    booktitle = "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing: System Demonstrations",
    month = nov,
    year = "2018",
    address = "Brussels, Belgium",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/D18-2012",
    doi = "10.18653/v1/D18-2012",
    pages = "66--71",
    abstract = "This paper describes SentencePiece, a language-independent subword tokenizer and detokenizer designed for Neural-based text processing, including Neural Machine Translation. It provides open-source C++ and Python implementations for subword units. While existing subword segmentation tools assume that the input is pre-tokenized into word sequences, SentencePiece can train subword models directly from raw sentences, which allows us to make a purely end-to-end and language independent system. We perform a validation experiment of NMT on English-Japanese machine translation, and find that it is possible to achieve comparable accuracy to direct subword training from raw sentences. We also compare the performance of subword training and segmentation with various configurations. SentencePiece is available under the Apache 2 license at \url{https://github.com/google/sentencepiece}.",
}

@inproceedings{kudo-2018-subword,
    title = "Subword Regularization: Improving Neural Network Translation Models with Multiple Subword Candidates",
    author = "Kudo, Taku",
    editor = "Gurevych, Iryna  and
      Miyao, Yusuke",
    booktitle = "Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = jul,
    year = "2018",
    address = "Melbourne, Australia",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/P18-1007",
    doi = "10.18653/v1/P18-1007",
    pages = "66--75",
    abstract = "Subword units are an effective way to alleviate the open vocabulary problems in neural machine translation (NMT). While sentences are usually converted into unique subword sequences, subword segmentation is potentially ambiguous and multiple segmentations are possible even with the same vocabulary. The question addressed in this paper is whether it is possible to harness the segmentation ambiguity as a noise to improve the robustness of NMT. We present a simple regularization method, subword regularization, which trains the model with multiple subword segmentations probabilistically sampled during training. In addition, for better subword sampling, we propose a new subword segmentation algorithm based on a unigram language model. We experiment with multiple corpora and report consistent improvements especially on low resource and out-of-domain settings.",
}

# Uncertainties in word embeddings

@inproceedings{Vilnis2014WordRV,
  title={Word Representations via Gaussian Embedding},
  author={Luke Vilnis and Andrew McCallum},
  journal={International Conference on Learning Representations (ICLR)},
  year={2014}
}

@article{Vallebueno2024StatisticalUI,
  title={Statistical Uncertainty in Word Embeddings: GloVe-V},
  author={Andrea Vallebueno and Cassandra Handan-Nader and Christopher D. Manning and Daniel E. Ho},
  journal={arXiv preprint arXiv:2406.12165},
  year={2024}
}

# others

@inproceedings{kim-linzen-2020-cogs,
    title = "{COGS}: A Compositional Generalization Challenge Based on Semantic Interpretation",
    author = "Kim, Najoung  and
      Linzen, Tal",
    editor = "Webber, Bonnie  and
      Cohn, Trevor  and
      He, Yulan  and
      Liu, Yang",
    booktitle = "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)",
    month = nov,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.emnlp-main.731",
    doi = "10.18653/v1/2020.emnlp-main.731",
    pages = "9087--9105",
    abstract = "Natural language is characterized by compositionality: the meaning of a complex expression is constructed from the meanings of its constituent parts. To facilitate the evaluation of the compositional abilities of language processing architectures, we introduce COGS, a semantic parsing dataset based on a fragment of English. The evaluation portion of COGS contains multiple systematic gaps that can only be addressed by compositional generalization; these include new combinations of familiar syntactic structures, or new combinations of familiar words and familiar structures. In experiments with Transformers and LSTMs, we found that in-distribution accuracy on the COGS test set was near-perfect (96{--}99{\%}), but generalization accuracy was substantially lower (16{--}35{\%}) and showed high sensitivity to random seed (+-6{--}8{\%}). These findings indicate that contemporary standard NLP models are limited in their compositional generalization capacity, and position COGS as a good way to measure progress.",
}

@inproceedings{li-etal-2023-slog,
    title = "{SLOG}: A Structural Generalization Benchmark for Semantic Parsing",
    author = "Li, Bingzhi  and
      Donatelli, Lucia  and
      Koller, Alexander  and
      Linzen, Tal  and
      Yao, Yuekun  and
      Kim, Najoung",
    editor = "Bouamor, Houda  and
      Pino, Juan  and
      Bali, Kalika",
    booktitle = "Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing",
    month = dec,
    year = "2023",
    address = "Singapore",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2023.emnlp-main.194",
    doi = "10.18653/v1/2023.emnlp-main.194",
    pages = "3213--3232",
    abstract = "The goal of compositional generalization benchmarks is to evaluate how well models generalize to new complex linguistic expressions. Existing benchmarks often focus on lexical generalization, the interpretation of novel lexical items in syntactic structures familiar from training; structural generalization tasks, where a model needs to interpret syntactic structures that are themselves unfamiliar from training, are often underrepresented, resulting in overly optimistic perceptions of how well models can generalize. We introduce SLOG, a semantic parsing dataset that extends COGS (Kim and Linzen, 2020) with 17 structural generalization cases. In our experiments, the generalization accuracy of Transformer models, including pretrained ones, only reaches 40.6{\%}, while a structure-aware parser only achieves 70.8{\%}. These results are far from the near-perfect accuracy existing models achieve on COGS, demonstrating the role of SLOG in foregrounding the large discrepancy between models{'} lexical and structural generalization capacities.",
}

@article{Kim2022UncontrolledLE,
  title={Uncontrolled lexical exposure leads to overestimation of compositional generalization in pretrained models},
  author={Kim, Najoung and Linzen, Tal and Smolensky, Paul},
  journal={arXiv preprint arXiv:2212.10769},
  year={2022}
}

@inproceedings{wei-etal-2021-frequency,
    title = "Frequency Effects on Syntactic Rule Learning in Transformers",
    author = "Wei, Jason  and
      Garrette, Dan  and
      Linzen, Tal  and
      Pavlick, Ellie",
    editor = "Moens, Marie-Francine  and
      Huang, Xuanjing  and
      Specia, Lucia  and
      Yih, Scott Wen-tau",
    booktitle = "Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing",
    month = nov,
    year = "2021",
    address = "Online and Punta Cana, Dominican Republic",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.emnlp-main.72",
    doi = "10.18653/v1/2021.emnlp-main.72",
    pages = "932--948",
    abstract = "Pre-trained language models perform well on a variety of linguistic tasks that require symbolic reasoning, raising the question of whether such models implicitly represent abstract symbols and rules. We investigate this question using the case study of BERT{'}s performance on English subject{--}verb agreement. Unlike prior work, we train multiple instances of BERT from scratch, allowing us to perform a series of controlled interventions at pre-training time. We show that BERT often generalizes well to subject{--}verb pairs that never occurred in training, suggesting a degree of rule-governed behavior. We also find, however, that performance is heavily influenced by word frequency, with experiments showing that both the absolute frequency of a verb form, as well as the frequency relative to the alternate inflection, are causally implicated in the predictions BERT makes at inference time. Closer analysis of these frequency effects reveals that BERT{'}s behavior is consistent with a system that correctly applies the SVA rule in general but struggles to overcome strong training priors and to estimate agreement features (singular vs. plural) on infrequent lexical items.",
}

@inproceedings{Razeghi2022ImpactOP,
    title = "Impact of Pretraining Term Frequencies on Few-Shot Numerical Reasoning",
    author = "Razeghi, Yasaman  and
      Logan IV, Robert L  and
      Gardner, Matt  and
      Singh, Sameer",
    editor = "Goldberg, Yoav  and
      Kozareva, Zornitsa  and
      Zhang, Yue",
    booktitle = "Findings of the Association for Computational Linguistics: EMNLP 2022",
    month = dec,
    year = "2022",
    address = "Abu Dhabi, United Arab Emirates",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.findings-emnlp.59",
    doi = "10.18653/v1/2022.findings-emnlp.59",
    pages = "840--854",
    abstract = "Pretrained Language Models (LMs) have demonstrated ability to perform numerical reasoning by extrapolating from a few examples in few-shot settings. However, the extent to which this extrapolation relies on robust reasoning is unclear. In this paper, we investigate how well these models reason with terms that are less frequent in the pretraining data. In particular, we examine the correlations between the model performance on test instances and the frequency of terms from those instances in the pretraining data. We measure the strength of this correlation for a number of GPT-based language models (pretrained on the Pile dataset) on various numerical deduction tasks (e.g., arithmetic and unit conversion). Our results consistently demonstrate that models are more accurate on instances whose terms are more prevalent, in some cases above 70{\%} (absolute) more accurate on the top 10{\%} frequent terms in comparison to the bottom 10{\%}. Overall, although LMs appear successful at few-shot numerical reasoning, our results raise the question of how much models actually generalize beyond pretraining data, and we encourage researchers to take the pretraining data into account when interpreting evaluation results.",
}

@article{Batsuren2024EvaluatingST,
  title={Evaluating Subword Tokenization: Alien Subword Composition and OOV Generalization Challenge},
  author={Batsuren, Khuyagbaatar and Vylomova, Ekaterina and Dankers, Verna and Delgerbaatar, Tsetsuukhei and Uzan, Omri and Pinter, Yuval and Bella, G{\'a}bor},
  journal={arXiv preprint arXiv:2404.13292},
  year={2024}
}

@inproceedings{Land2024FishingFM,
    title = "Fishing for Magikarp: Automatically Detecting Under-trained Tokens in Large Language Models",
    author = "Land, Sander  and
      Bartolo, Max",
    editor = "Al-Onaizan, Yaser  and
      Bansal, Mohit  and
      Chen, Yun-Nung",
    booktitle = "Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing",
    month = nov,
    year = "2024",
    address = "Miami, Florida, USA",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2024.emnlp-main.649",
    doi = "10.18653/v1/2024.emnlp-main.649",
    pages = "11631--11646",
    abstract = "The disconnect between tokenizer creation and model training in language models allows for specific inputs, such as the infamous SolidGoldMagikarp token, to induce unwanted model behaviour. Although such {`}glitch tokens{'}, tokens present in the tokenizer vocabulary but that are nearly or entirely absent during model training, have been observed across various models, a reliable method to identify and address them has been missing. We present a comprehensive analysis of Large Language Model tokenizers, specifically targeting this issue of detecting under-trained tokens. Through a combination of tokenizer analysis, model weight-based indicators, and prompting techniques, we develop novel and effective methods for automatically detecting these problematic tokens. Our findings demonstrate the prevalence of such tokens across a diverse set of models and provide insights into improving the efficiency and safety of language models.",
}

@article{Misra2023AbstractionVE,
  title={Abstraction via exemplars? A representational case study on lexical category inference in BERT},
  author={Kanishka Misra and Najoung Kim},
  journal={BUCLD},
  year={2023}
}

@article{Jelassi2024RepeatAM,
  title={Repeat after me: Transformers are better than state space models at copying},
  author={Jelassi, Samy and Brandfonbrener, David and Kakade, Sham M and Malach, Eran},
  journal={arXiv preprint arXiv:2402.01032},
  year={2024}
}

@article{Akyrek2024InContextLL,
  title={In-context language learning: Arhitectures and algorithms},
  author={Aky{\"u}rek, Ekin and Wang, Bailin and Kim, Yoon and Andreas, Jacob},
  journal={arXiv preprint arXiv:2401.12973},
  year={2024}
}

@article{Dosovitskiy2020AnII,
  title={An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale},
  author={Alexey Dosovitskiy and Lucas Beyer and Alexander Kolesnikov and Dirk Weissenborn and Xiaohua Zhai and Thomas Unterthiner and Mostafa Dehghani and Matthias Minderer and Georg Heigold and Sylvain Gelly and Jakob Uszkoreit and Neil Houlsby},
  journal={arXiv preprint arXiv:2010.11929},
  year={2020}
}


@inproceedings{patel-etal-2022-revisiting,
    title = "Revisiting the Compositional Generalization Abilities of Neural Sequence Models",
    author = "Patel, Arkil  and
      Bhattamishra, Satwik  and
      Blunsom, Phil  and
      Goyal, Navin",
    editor = "Muresan, Smaranda  and
      Nakov, Preslav  and
      Villavicencio, Aline",
    booktitle = "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)",
    month = may,
    year = "2022",
    address = "Dublin, Ireland",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.acl-short.46",
    doi = "10.18653/v1/2022.acl-short.46",
    pages = "424--434",
    abstract = "Compositional generalization is a fundamental trait in humans, allowing us to effortlessly combine known phrases to form novel sentences. Recent works have claimed that standard seq-to-seq models severely lack the ability to compositionally generalize. In this paper, we focus on one-shot primitive generalization as introduced by the popular SCAN benchmark. We demonstrate that modifying the training distribution in simple and intuitive ways enables standard seq-to-seq models to achieve near-perfect generalization performance, thereby showing that their compositional generalization abilities were previously underestimated. We perform detailed empirical analysis of this phenomenon. Our results indicate that the generalization performance of models is highly sensitive to the characteristics of the training data which should be carefully considered while designing such benchmarks in future.",
}

@inproceedings{Zhou2023DataFF,
  title={Data Factors for Better Compositional Generalization},
  author={Zhou, Xiang and Jiang, Yichen and Bansal, Mohit},
  booktitle={Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing},
  pages={14549--14566},
  year={2023}
}

@article{Jiang2024InducingSI,
  title={Inducing Systematicity in Transformers by Attending to Structurally Quantized Embeddings},
  author={Jiang, Yichen and Zhou, Xiang and Bansal, Mohit},
  journal={arXiv preprint arXiv:2402.06492},
  year={2024}
}

@inproceedings{Izmailov2018AveragingWL,
  title={Averaging Weights Leads to Wider Optima and Better Generalization},
  author={Pavel Izmailov and Dmitrii Podoprikhin and T. Garipov and Dmitry P. Vetrov and Andrew Gordon Wilson},
  booktitle={Conference on Uncertainty in Artificial Intelligence},
  year={2018}
}

@inproceedings{Maddox2019ASB,
  title={A Simple Baseline for Bayesian Uncertainty in Deep Learning},
  author={Wesley J. Maddox and T. Garipov and Pavel Izmailov and Dmitry P. Vetrov and Andrew Gordon Wilson},
  booktitle={Neural Information Processing Systems},
  year={2019}
}

@inproceedings{Malladi2022AKV,
  title={A Kernel-Based View of Language Model Fine-Tuning},
  author={Sadhika Malladi and Alexander Wettig and Dingli Yu and Danqi Chen and Sanjeev Arora},
  booktitle={International Conference on Machine Learning},
  year={2022}
}

@inproceedings{Xia2024LESSSI,
  title={LESS: Selecting Influential Data for Targeted Instruction Tuning},
  author={Xia, Mengzhou and Malladi, Sadhika and Gururangan, Suchin and Arora, Sanjeev and Chen, Danqi},
  booktitle={Forty-first International Conference on Machine Learning}
}

@article{Jacot2018NeuralTK,
  title={Neural tangent kernel: Convergence and generalization in neural networks},
  author={Jacot, Arthur and Gabriel, Franck and Hongler, Cl{\'e}ment},
  journal={Advances in Neural Information Processing Systems},
  volume={31},
  year={2018}
}

@article{Misra2024LanguageML,
  title={Language Models Learn Rare Phenomena from Less Rare Phenomena: The Case of the Missing AANNs},
  author={Misra, Kanishka and Mahowald, Kyle},
  journal={arXiv preprint arXiv:2403.19827},
  year={2024}
}

@article{Olsson2022IncontextLA,
  title={In-context learning and induction heads},
  author={Olsson, Catherine and Elhage, Nelson and Nanda, Neel and Joseph, Nicholas and DasSarma, Nova and Henighan, Tom and Mann, Ben and Askell, Amanda and Bai, Yuntao and Chen, Anna and others},
  journal={arXiv preprint arXiv:2209.11895},
  year={2022}
}

@article{Lake2015HumanlevelCL,
  title={Human-level concept learning through probabilistic program induction},
  author={Brenden M. Lake and Ruslan Salakhutdinov and Joshua B. Tenenbaum},
  journal={Science},
  year={2015},
  volume={350},
  pages={1332 - 1338}
}

@inproceedings{Vinyals2016MatchingNF,
  title={Matching Networks for One Shot Learning},
  author={Oriol Vinyals and Charles Blundell and Timothy P. Lillicrap and Koray Kavukcuoglu and Daan Wierstra},
  booktitle={Neural Information Processing Systems},
  year={2016}
}

@inproceedings{Snell2017PrototypicalNF,
  title={Prototypical Networks for Few-shot Learning},
  author={Jake Snell and Kevin Swersky and Richard S. Zemel},
  booktitle={Neural Information Processing Systems},
  year={2017}
}

@article{huang2024lexinvariant,
  title={Lexinvariant Language Models},
  author={Huang, Qian and Zelikman, Eric and Chen, Sarah and Wu, Yuhuai and Valiant, Gregory and Liang, Percy S},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2024}
}

# representation degeneration problem, optimization problem, as a result of word frequency distribution

@inproceedings{Ott2018AnalyzingUI,
  title={Analyzing Uncertainty in Neural Machine Translation},
  author={Myle Ott and Michael Auli and David Grangier and Marc'Aurelio Ranzato},
  booktitle={International Conference on Machine Learning},
  year={2018}
}

@inproceedings{Gong2018FRAGEFW,
  title={FRAGE: Frequency-Agnostic Word Representation},
  author={Chengyue Gong and Di He and Xu Tan and Tao Qin and Liwei Wang and Tie-Yan Liu},
  booktitle={Neural Information Processing Systems},
  year={2018}
}

@article{Gao2019RepresentationDP,
  title={Representation Degeneration Problem in Training Natural Language Generation Models},
  author={Jun Gao and Di He and Xu Tan and Tao Qin and Liwei Wang and Tie-Yan Liu},
  journal={ICLR},
  year={2019}
}

@inproceedings{ethayarajh-2019-contextual,
    title = "How Contextual are Contextualized Word Representations? {C}omparing the Geometry of {BERT}, {ELM}o, and {GPT}-2 Embeddings",
    author = "Ethayarajh, Kawin",
    editor = "Inui, Kentaro  and
      Jiang, Jing  and
      Ng, Vincent  and
      Wan, Xiaojun",
    booktitle = "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)",
    month = nov,
    year = "2019",
    address = "Hong Kong, China",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/D19-1006",
    doi = "10.18653/v1/D19-1006",
    pages = "55--65",
    abstract = "Replacing static word embeddings with contextualized word representations has yielded significant improvements on many NLP tasks. However, just how contextual are the contextualized representations produced by models such as ELMo and BERT? Are there infinitely many context-specific representations for each word, or are words essentially assigned one of a finite number of word-sense representations? For one, we find that the contextualized representations of all words are not isotropic in any layer of the contextualizing model. While representations of the same word in different contexts still have a greater cosine similarity than those of two different words, this self-similarity is much lower in upper layers. This suggests that upper layers of contextualizing models produce more context-specific representations, much like how upper layers of LSTMs produce more task-specific representations. In all layers of ELMo, BERT, and GPT-2, on average, less than 5{\%} of the variance in a word{'}s contextualized representations can be explained by a static embedding for that word, providing some justification for the success of contextualized representations.",
}

@inproceedings{yu-etal-2022-rare,
    title = "Rare Tokens Degenerate All Tokens: Improving Neural Text Generation via Adaptive Gradient Gating for Rare Token Embeddings",
    author = "Yu, Sangwon  and
      Song, Jongyoon  and
      Kim, Heeseung  and
      Lee, Seongmin  and
      Ryu, Woo-Jong  and
      Yoon, Sungroh",
    editor = "Muresan, Smaranda  and
      Nakov, Preslav  and
      Villavicencio, Aline",
    booktitle = "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = may,
    year = "2022",
    address = "Dublin, Ireland",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.acl-long.3",
    doi = "10.18653/v1/2022.acl-long.3",
    pages = "29--45",
    abstract = "Recent studies have determined that the learned token embeddings of large-scale neural language models are degenerated to be anisotropic with a narrow-cone shape. This phenomenon, called the representation degeneration problem, facilitates an increase in the overall similarity between token embeddings that negatively affect the performance of the models. Although the existing methods that address the degeneration problem based on observations of the phenomenon triggered by the problem improves the performance of the text generation, the training dynamics of token embeddings behind the degeneration problem are still not explored. In this study, we analyze the training dynamics of the token embeddings focusing on rare token embedding. We demonstrate that the specific part of the gradient for rare token embeddings is the key cause of the degeneration problem for all tokens during training stage. Based on the analysis, we propose a novel method called, adaptive gradient gating(AGG). AGG addresses the degeneration problem by gating the specific part of the gradient for rare token embeddings. Experimental results from language modeling, word similarity, and machine translation tasks quantitatively and qualitatively verify the effectiveness of AGG.",
}

@inproceedings{puccetti-etal-2022-outlier,
    title = "Outlier Dimensions that Disrupt Transformers are Driven by Frequency",
    author = "Puccetti, Giovanni  and
      Rogers, Anna  and
      Drozd, Aleksandr  and
      Dell{'}Orletta, Felice",
    editor = "Goldberg, Yoav  and
      Kozareva, Zornitsa  and
      Zhang, Yue",
    booktitle = "Findings of the Association for Computational Linguistics: EMNLP 2022",
    month = dec,
    year = "2022",
    address = "Abu Dhabi, United Arab Emirates",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.findings-emnlp.93",
    doi = "10.18653/v1/2022.findings-emnlp.93",
    pages = "1286--1304",
    abstract = "While Transformer-based language models are generally very robust to pruning, there is the recently discovered outlier phenomenon: disabling only 48 out of 110M parameters in BERT-base drops its performance by nearly 30{\%} on MNLI. We replicate the original evidence for the outlier phenomenon and we link it to the geometry of the embedding space. We find that in both BERT and RoBERTa the magnitude of hidden state coefficients corresponding to outlier dimensions correlate with the frequencies of encoded tokens in pre-training data, and they also contribute to the {``}vertical{''} self-attention pattern enabling the model to focus on the special tokens. This explains the drop in performance from disabling the outliers, and it suggests that to decrease anisotopicity in future models we need pre-training schemas that would better take into account the skewed token distributions.",
}

@inproceedings{rajaee-pilehvar-2022-isotropy,
    title = "An Isotropy Analysis in the Multilingual {BERT} Embedding Space",
    author = "Rajaee, Sara  and
      Pilehvar, Mohammad Taher",
    editor = "Muresan, Smaranda  and
      Nakov, Preslav  and
      Villavicencio, Aline",
    booktitle = "Findings of the Association for Computational Linguistics: ACL 2022",
    month = may,
    year = "2022",
    address = "Dublin, Ireland",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.findings-acl.103",
    doi = "10.18653/v1/2022.findings-acl.103",
    pages = "1309--1316",
    abstract = "Several studies have explored various advantages of multilingual pre-trained models (such as multilingual BERT) in capturing shared linguistic knowledge. However, less attention has been paid to their limitations. In this paper, we investigate the multilingual BERT for two known issues of the monolingual models: anisotropic embedding space and outlier dimensions. We show that, unlike its monolingual counterpart, the multilingual BERT model exhibits no outlier dimension in its representations while it has a highly anisotropic space. There are a few dimensions in the monolingual BERT with high contributions to the anisotropic distribution. However, we observe no such dimensions in the multilingual BERT. Furthermore, our experimental results demonstrate that increasing the isotropy of multilingual space can significantly improve its representation power and performance, similarly to what had been observed for monolingual CWRs on semantic similarity tasks. Our analysis indicates that, despite having different degenerated directions, the embedding spaces in various languages tend to be partially similar with respect to their structures.",
}

@inproceedings{Schick2020RareWA,
  title={Rare Words: A Major Problem for Contextualized Embeddings And How to Fix it by Attentive Mimicking},
  author={Timo Schick and Hinrich Sch{\"u}tze},
  booktitle={AAAI Conference on Artificial Intelligence},
  year={2020}
}

@inproceedings{schick-schutze-2020-bertram,
    title = "{BERTRAM}: Improved Word Embeddings Have Big Impact on Contextualized Model Performance",
    author = {Schick, Timo  and
      Sch{\"u}tze, Hinrich},
    editor = "Jurafsky, Dan  and
      Chai, Joyce  and
      Schluter, Natalie  and
      Tetreault, Joel",
    booktitle = "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics",
    month = jul,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.acl-main.368",
    doi = "10.18653/v1/2020.acl-main.368",
    pages = "3996--4007",
    abstract = {Pretraining deep language models has led to large performance gains in NLP. Despite this success, Schick and Sch{\"u}tze (2020) recently showed that these models struggle to understand rare words. For static word embeddings, this problem has been addressed by separately learning representations for rare words. In this work, we transfer this idea to pretrained language models: We introduce BERTRAM, a powerful architecture based on BERT that is capable of inferring high-quality embeddings for rare words that are suitable as input representations for deep language models. This is achieved by enabling the surface form and contexts of a word to interact with each other in a deep architecture. Integrating BERTRAM into BERT leads to large performance increases due to improved representations of rare and medium frequency words on both a rare word probing task and three downstream tasks.},
}

@inproceedings{bis-etal-2021-much,
    title = "Too Much in Common: Shifting of Embeddings in Transformer Language Models and its Implications",
    author = "Bi{\'s}, Daniel  and
      Podkorytov, Maksim  and
      Liu, Xiuwen",
    editor = "Toutanova, Kristina  and
      Rumshisky, Anna  and
      Zettlemoyer, Luke  and
      Hakkani-Tur, Dilek  and
      Beltagy, Iz  and
      Bethard, Steven  and
      Cotterell, Ryan  and
      Chakraborty, Tanmoy  and
      Zhou, Yichao",
    booktitle = "Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies",
    month = jun,
    year = "2021",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.naacl-main.403",
    doi = "10.18653/v1/2021.naacl-main.403",
    pages = "5117--5130",
    abstract = "The success of language models based on the Transformer architecture appears to be inconsistent with observed anisotropic properties of representations learned by such models. We resolve this by showing, contrary to previous studies, that the representations do not occupy a narrow cone, but rather drift in common directions. At any training step, all of the embeddings except for the ground-truth target embedding are updated with gradient in the same direction. Compounded over the training set, the embeddings drift and share common components, manifested in their shape in all the models we have empirically tested. Our experiments show that isotropy can be restored using a simple transformation.",
}

@article{Godey2024AnisotropyII,
  title={Anisotropy Is Inherent to Self-Attention in Transformers},
  author={Godey, Nathan and de la Clergerie, {\'E}ric and Sagot, Beno{\^\i}t},
  journal={arXiv preprint arXiv:2401.12143},
  year={2024}
}

@article{Kunstner2024HeavyTailedCI,
  title={Heavy-tailed class imbalance and why adam outperforms gradient descent on language models},
  author={Kunstner, Frederik and Yadav, Robin and Milligan, Alan and Schmidt, Mark and Bietti, Alberto},
  journal={arXiv preprint arXiv:2402.19449},
  year={2024}
}

@article{Wu2020TakingNO,
  title={Taking Notes on the Fly Helps BERT Pre-training},
  author={Qiyu Wu and Chen Xing and Yatao Li and Guolin Ke and Di He and Tie-Yan Liu},
  journal={ICLR},
  year={2021}
}

@inproceedings{marvin-linzen-2018-targeted,
    title = "Targeted Syntactic Evaluation of Language Models",
    author = "Marvin, Rebecca  and
      Linzen, Tal",
    editor = "Riloff, Ellen  and
      Chiang, David  and
      Hockenmaier, Julia  and
      Tsujii, Jun{'}ichi",
    booktitle = "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing",
    month = oct # "-" # nov,
    year = "2018",
    address = "Brussels, Belgium",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/D18-1151",
    doi = "10.18653/v1/D18-1151",
    pages = "1192--1202",
    abstract = "We present a data set for evaluating the grammaticality of the predictions of a language model. We automatically construct a large number of minimally different pairs of English sentences, each consisting of a grammatical and an ungrammatical sentence. The sentence pairs represent different variations of structure-sensitive phenomena: subject-verb agreement, reflexive anaphora and negative polarity items. We expect a language model to assign a higher probability to the grammatical sentence than the ungrammatical one. In an experiment using this data set, an LSTM language model performed poorly on many of the constructions. Multi-task training with a syntactic objective (CCG supertagging) improved the LSTM{'}s accuracy, but a large gap remained between its performance and the accuracy of human participants recruited online. This suggests that there is considerable room for improvement over LSTMs in capturing syntax in a language model.",
}

@article{Han2024ParameterEfficientFF,
  title={Parameter-efficient fine-tuning for large models: A comprehensive survey},
  author={Han, Zeyu and Gao, Chao and Liu, Jinyang and Zhang, Sai Qian},
  journal={arXiv preprint arXiv:2403.14608},
  year={2024}
}

@book{Heaps1978InformationRC,
  title={Information retrieval: computational and theoretical aspects},
  author={Harold Stanley Heaps},
  publisher={Academic Press, Inc},
  year={1978}
}

@article{Zhao2024ProbingTD,
  title={Probing the Decision Boundaries of In-context Learning in Large Language Models},
  author={Zhao, Siyan and Nguyen, Tung and Grover, Aditya},
  journal={arXiv preprint arXiv:2406.11233},
  year={2024}
}

@article{Mandelkern2023DoLM,
  title={Do Language Models’ Words Refer?},
  author={Matthew Mandelkern and Tal Linzen},
  journal={Computational Linguistics},
  year={2023}
}

@inproceedings{xie2022incontext,
  title={An Explanation of In-context Learning as Implicit Bayesian Inference},
  author={Sang Michael Xie and Aditi Raghunathan and Percy Liang and Tengyu Ma},
  booktitle={International Conference on Learning Representations (ICLR)},
  year={2022}
}

@inproceedings{Ahuja2024InContextLT,
  title={In-Context Learning through the Bayesian Prism},
  author={Kabir Ahuja and Madhuri Panwar and Navin Goyal},
  booktitle={International Conference on Learning Representations (ICLR)},
  year={2024}
}

@article{Dou2024SailorOL,
  title={Sailor: Open Language Models for South-East Asia},
  author={Dou, Longxu and Liu, Qian and Zeng, Guangtao and Guo, Jia and Zhou, Jiahui and Lu, Wei and Lin, Min},
  journal={arXiv preprint arXiv:2404.03608},
  year={2024}
}

@inproceedings{yedetore-etal-2023-poor,
    title = "How poor is the stimulus? Evaluating hierarchical generalization in neural networks trained on child-directed speech",
    author = "Yedetore, Aditya  and
      Linzen, Tal  and
      Frank, Robert  and
      McCoy, R. Thomas",
    editor = "Rogers, Anna  and
      Boyd-Graber, Jordan  and
      Okazaki, Naoaki",
    booktitle = "Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = jul,
    year = "2023",
    address = "Toronto, Canada",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2023.acl-long.521",
    doi = "10.18653/v1/2023.acl-long.521",
    pages = "9370--9393",
    abstract = "When acquiring syntax, children consistently choose hierarchical rules over competing non-hierarchical possibilities. Is this preference due to a learning bias for hierarchical structure, or due to more general biases that interact with hierarchical cues in children{'}s linguistic input? We explore these possibilities by training LSTMs and Transformers - two types of neural networks without a hierarchical bias - on data similar in quantity and content to children{'}s linguistic input: text from the CHILDES corpus. We then evaluate what these models have learned about English yes/no questions, a phenomenon for which hierarchical structure is crucial. We find that, though they perform well at capturing the surface statistics of child-directed speech (as measured by perplexity), both model types generalize in a way more consistent with an incorrect linear rule than the correct hierarchical rule. These results suggest that human-like generalization from text alone requires stronger biases than the general sequence-processing biases of standard neural network architectures.",
}

@article{Dupoux2016CognitiveSI,
  title={Cognitive science in the era of artificial intelligence: A roadmap for reverse-engineering the infant language-learner},
  author={Emmanuel Dupoux},
  journal={Cognition},
  year={2016},
  volume={173},
  pages={43-59}
}

@inproceedings{MNLI,
  title={A Broad-Coverage Challenge Corpus for Sentence Understanding through Inference},
  author={Adina Williams and Nikita Nangia and Samuel R. Bowman},
  booktitle={North American Chapter of the Association for Computational Linguistics},
  year={2017}
}

@article{WordNet,
  title={{WordNet}: A Lexical Database for English},
  author={George A. Miller},
  journal={Commun. ACM},
  year={1995},
  volume={38},
  pages={39-41}
}

@inproceedings{gadetsky-etal-2018-conditional,
    title = "Conditional Generators of Words Definitions",
    author = "Gadetsky, Artyom  and
      Yakubovskiy, Ilya  and
      Vetrov, Dmitry",
    editor = "Gurevych, Iryna  and
      Miyao, Yusuke",
    booktitle = "Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)",
    month = jul,
    year = "2018",
    address = "Melbourne, Australia",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/P18-2043",
    doi = "10.18653/v1/P18-2043",
    pages = "266--271",
    abstract = "We explore recently introduced definition modeling technique that provided the tool for evaluation of different distributed vector representations of words through modeling dictionary definitions of words. In this work, we study the problem of word ambiguities in definition modeling and propose a possible solution by employing latent variable modeling and soft attention mechanisms. Our quantitative and qualitative evaluation and analysis of the model shows that taking into account words{'} ambiguity and polysemy leads to performance improvement.",
}

@inproceedings{giulianelli-etal-2023-interpretable,
    title = "Interpretable Word Sense Representations via Definition Generation: The Case of Semantic Change Analysis",
    author = "Giulianelli, Mario  and
      Luden, Iris  and
      Fernandez, Raquel  and
      Kutuzov, Andrey",
    booktitle = "Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = jul,
    year = "2023",
    address = "Toronto, Canada",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2023.acl-long.176",
    doi = "10.18653/v1/2023.acl-long.176",
    pages = "3130--3148",
    abstract = "We propose using automatically generated natural language definitions of contextualised word usages as interpretable word and word sense representations. Given a collection of usage examples for a target word, and the corresponding data-driven usage clusters (i.e., word senses), a definition is generated for each usage with a specialised Flan-T5 language model, and the most prototypical definition in a usage cluster is chosen as the sense label. We demonstrate how the resulting sense labels can make existing approaches to semantic change analysis more interpretable, and how they can allow users {---} historical linguists, lexicographers, or social scientists {---} to explore and intuitively explain diachronic trajectories of word meaning. Semantic change analysis is only one of many possible applications of the {`}definitions as representations{'} paradigm. Beyond being human-readable, contextualised definitions also outperform token or usage sentence embeddings in word-in-context semantic similarity judgements, making them a new promising type of lexical representation for NLP.",
}

@article{chung2024scaling,
  title={Scaling instruction-finetuned language models},
  author={Chung, Hyung Won and Hou, Le and Longpre, Shayne and Zoph, Barret and Tay, Yi and Fedus, William and Li, Yunxuan and Wang, Xuezhi and Dehghani, Mostafa and Brahma, Siddhartha and others},
  journal={Journal of Machine Learning Research},
  volume={25},
  number={70},
  pages={1--53},
  year={2024}
}

@article{zhang2019bertscore,
  title={Bertscore: Evaluating text generation with bert},
  author={Zhang, Tianyi and Kishore, Varsha and Wu, Felix and Weinberger, Kilian Q and Artzi, Yoav},
  journal={ICLR},
  year={2020}
}

@inproceedings{lin-2004-rouge,
    title = "{ROUGE}: A Package for Automatic Evaluation of Summaries",
    author = "Lin, Chin-Yew",
    booktitle = "Text Summarization Branches Out",
    month = jul,
    year = "2004",
    address = "Barcelona, Spain",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/W04-1013",
    pages = "74--81",
}

@article{Russin2024Frege,
  title={{From Frege to chatGPT: Compositionality in language, cognition, and deep neural networks}},
  author={Jacob Russin and Sam Whitman McGrath and Danielle J. Williams and Lotem Elber-Dorozko},
  journal={ArXiv},
  year={2024},
  volume={abs/2405.15164}
}

@article{Irie2024NNPractice,
  title={Neural networks that overcome classic challenges through practice}, 
  author={Kazuki Irie and Brenden M. Lake},
  journal={ArXiv},
  year={2024},
  volume={abs/2410.10596}
}

@inproceedings{mueller-etal-2024-context,
    title = "In-context Learning Generalizes, But Not Always Robustly: The Case of Syntax",
    author = "Mueller, Aaron  and
      Webson, Albert  and
      Petty, Jackson  and
      Linzen, Tal",
    editor = "Duh, Kevin  and
      Gomez, Helena  and
      Bethard, Steven",
    booktitle = "Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers)",
    month = jun,
    year = "2024",
    address = "Mexico City, Mexico",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2024.naacl-long.267",
    doi = "10.18653/v1/2024.naacl-long.267",
    pages = "4761--4779",
    abstract = "In-context learning (ICL) is now a common method for teaching large language models (LLMs) new tasks: given labeled examples in the input context, the LLM learns to perform the task without weight updates. Do models guided via ICL infer the underlying structure of the task defined by the context, or do they rely on superficial heuristics that only generalize to identically distributed examples? We address this question using transformations tasks and an NLI task that assess sensitivity to syntax{---}a requirement for robust language understanding. We further investigate whether out-of-distribution generalization can be improved via chain-of-thought prompting, where the model is provided with a sequence of intermediate computation steps that illustrate how the task ought to be performed. In experiments with models from the GPT, PaLM, and Llama 2 families, we find large variance across LMs. The variance is explained more by the composition of the pre-training corpus and supervision methods than by model size; in particular, models pre-trained on code generalize better, and benefit more from chain-of-thought prompting.",
}

@article{Berko1958TheCL,
  title={The Child's Learning of English Morphology},
  author={Jean Berko},
  journal={WORD},
  year={1958},
  volume={14},
  pages={150-177}
}