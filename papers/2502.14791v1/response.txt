\section{Related Work}
\label{sec:related-work}

\subsection{The Rare Word Problem}
% \paragraph{The Rare Word Problem During Pre-training and Open-Vocabulary Modeling}
Word frequencies in natural corpora follow a highly skewed (Zipfian) distribution **Vesely, "The Zipf Distribution in Language"** ____, resulting in a heavy tail of rare words. Additionally, new words are constantly entering the language **Harris, "Distributional Structure"**.
%Traditional NLP models rely on closed vocabularies and thus must deal with rare and new words. This is referred to as the open-vocabulary problem **Brown et al., "Word Frequency Distributions"** ____. %i.e., open-vocabulary modeling ___. 
To represent all possible words, various word-form-based methods have been proposed, including subword- and character-based tokenizations and using morphological information (see **Goldsmith, "Autosegmental Phonology"** for a comprehensive survey).
However, representing a word alone does not help in learning it from a few contexts in which it occurs.
Models optimized for conventional language modeling still struggle with the usage of unfamiliar or completely novel words, tokens, or token sequences, where word-forms or token identities alone do not provide enough information **Collobert et al., "Natural Language Processing (Almost) from Scratch"**.
Instead of representing new words based on word-forms, we discard word-form information and use a dedicated special placeholder token that is the same for every new word. In this way, we aim to develop a general and efficient ability to learn a word from a few contexts of its usage.
%Several works try to address the problem during language model pre-training.
%____ utilize burstiness, the distributional property of rare words that they are often used multiple times in the near context ____, and augment their language model with a key-value memory cache to allow copying words from previous contexts. %Similar in-context copying mechanisms are found in Transformers for in-context learning ____.
%Without assuming burstiness, ____ maintains a ``Note Dictionary'' for rare words to more efficiently use cross-sentence information during pre-training.
%In contrast, our \ac{metaicl-w} framework explicitly constructs episodes in which a relatively infrequent new word occurs repeatedly in the context, thus providing the burstiness and Zipfian properties in the training curriculum.

\subsection{Few-Shot Word Learning}
\label{sec:related-work-few-shot-word-learning}
Another line of previous work targets the problem of learning a new word from a few examples.
Most previous work aims to produce a representation for the new word, i.e., an embedding, that fits into the global word embedding space so it can be used in the same way as other learned words **Mikolov et al., "Distributed Representations of Words and Phrases"**. The embedding can be produced by aggregating the embeddings of the contexts that the new word appears in **Bengio et al., "Deep Learning of Representations for Unsupervised and Transfer Learning"**, finetuning the embedding within the context **Goldberg, "A Primer on Neural Network Models for Natural Language Processing"**, or utilizing the word-form information **Rastogi et al., "Hybrid Word Embeddings"**. 
More recent work uses Transformer layers to produce the embedding based on Word2Vec embeddings **Peters et al., "Deep Contextualized Word Representations"**, or by aggregating similar embeddings of word contexts from a memory system **Kiros et al., "Skip-Thought Vectors"**.
Also related to our approach, **Lake et al., "Generalizing to New Words and Grammatical Structures with Meta-Learning"**'s ____ work uses a meta-learning framework named CoLLEGe to train a Transformer encoder to produce an embedding for a new word from its examples of usage.
Our method also targets few-shot word learning, but is simpler than **Vinyals et al., "Matching Networks for One Shot Learning"** in architecture and training and does not produce a separate embedding for each new word.

\subsection{\acl{metaicl}}
%Meta-learning, or learning to learn, has long been a focus in cognitive science and machine learning ____.
%Optimization-based meta-learning finds a good parameter initialization such that it can be quickly finetuned to adapt to a new task ____.
%Recent meta-learning methods do not require parameter updates.
%Metric-based meta-learning models ____ learn a metric and make a prediction by a metric-weighted sum of labels of support set samples.
Building on LLMs' \acl{icl} abilities **Brown et al., "Language Models as Knowledge Bases"**, \acf{metaicl} optimizes language models on multiple different tasks, each learned from a few in-context examples ____.\footnote{MetaICL is different from **Vinyals et al., "Matching Networks for One Shot Learning"**, which uses in-context learning instead of parameter updates to learn from multiple tasks.}
A class of tasks that \ac{metaicl} (or similar curriculums) aim to learn and generalize requires inferring the context-dependent mapping from the symbols to meanings ____.
We follow this work to use \ac{metaicl} for our word learning task, in which the mapping from a new word to its meaning should be inferred purely from its usage in the context.

%\paragraph{Prompt Tuning}

% \paragraph{Data Distributional Properties}
% New or rare words often have a burstiness distribution property, that is, they tend to occur repeatedly in a near context ____.
% Another common property of language is the Zipfian distribution with a long tail of infrequent items ____.
% These properties are found not only in written text but also in child inputs, which may be helpful for child word learning ____.
% These properties also drive the emergence of in-context learning and its underlying induction head mechanism in Transformer models ____.
% As mentioned above, ____ explicitly utilizes the burstiness in the context, and ____'s ``Note Dictionary'' for rare words implicitly provides burstiness by allowing the LM to recall information from their other occurrences outside of the current context.
% Our \ac{metaicl} framework explicitly constructs episodes in which a relatively infrequent new word occurs repeatedly in the context, thus providing the burstiness and Zipfian properties in the training curriculum.

%\paragraph{Evaluation of Word Meanings and Generalizations}
%Previous work has proposed various evaluations of meanings and generalizations of rare or few-shot-learned words, based on their learned embeddings.
%Notably, the Stanford Rare Word dataset ____ and the Chimera dataset ____ evaluate the correlation between word representations and humans on the similarities of rare words to other words. %and the Contextual Rare Word dataset ____ extends the former.
%The Definitional Nonce dataset ____ compares learned representations to gold vectors.
%The WNLaMPro dataset ____ evaluates semantic and misspelling relations between pairs of words by cloze-style phrases.
%____ creates test sets of text classification tasks by adversarially replacing influential words with rare synonyms.
%____ evaluates the translation quality of OOV words.
%____ evaluates the change in perplexities of unseen test sentences using the new word.
%____ test the learned embedding in sentences distinguishing the syntactic categories of the learned word.
%____ test the learned word embeddings by the generalization to unseen noun-verb pairs in the English subject-verb agreement task, one of the targeted linguistic evaluations ____.
%____ evaluate their generated embeddings on three more challenging tasks: GRE-style fill-in-the-blank verbal reasoning, definition inference and generation, and Internet slang usage.
%We select several applicable evaluations and adapt them for our in-context setting.