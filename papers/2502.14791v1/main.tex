\documentclass{article}
\usepackage[preprint]{acl}
\usepackage{times}
\usepackage{csquotes}

% Optional math commands from https://github.com/goodfeli/dlbook_notation.
\input{math_commands.tex}

\usepackage{url}
\usepackage{soul}

\input{config}

\title{Rapid Word Learning Through Meta In-Context Learning}

\begin{document}

\maketitle

\begin{abstract}
Humans can quickly learn a new word from a few illustrative examples, and then systematically and flexibly use it in novel contexts.
Yet the abilities of current language models for few-shot word learning, and methods for improving these abilities, are underexplored.
In this study, we introduce a novel method, \ac{metaicl-w}.
This method trains language models to generate new examples of a word's usage given a few in-context examples, using a special placeholder token to represent the new word.
This training is repeated on many new words to develop a general word-learning ability.
We find that training models from scratch with \ac{metaicl-w} on human-scale child-directed language enables strong few-shot word learning, comparable to a large language model (LLM) pre-trained on orders of magnitude more data.
Furthermore, through discriminative and generative evaluations, we demonstrate that finetuning pre-trained LLMs with \ac{metaicl-w} improves their ability to discriminate between new words, identify syntactic categories of new words, and generate reasonable new usages and definitions for new words, based on one or a few in-context examples.
These findings highlight the data efficiency of \ac{metaicl-w} and its potential to improve language model performance in word learning tasks.
\end{abstract}


\section{Introduction}
\label{sec:intro}

Children can quickly learn a new word, or at least make meaningful inferences about its meaning, given only a few examples of its usage \citep{CareyBartlett1978,Bloom2000}.
For example, suppose a child who did not know the word \emph{ski} hears the following mentions of the word (without visual examples): ``\emph{Susie learned to ski last winter}'', ``\emph{People ski on tall mountains where there's lots of snow}'', and ``\emph{I saw Susie ski fast down the snowy mountain}.''
From these usage examples, the child might infer that \emph{ski} is a verb for a winter activity involving sliding down snowy mountains, and could begin understanding and using the word appropriately in new contexts.
This ability to generalize and use a new word in novel contexts from just a few examples reflects children's remarkable data efficiency in language learning, allowing them to quickly acquire vocabulary without requiring tens or hundreds of examples per word.

%Children have ample opportunities to practice learning new words. Starting at about 12--14 months, children must learn about 10 new words per day in order to reach typical adult vocabularies \citep{Bloom2000,Bergelson2020TheCB}.
%Given the number of new words they are learning and a desire to systematically and flexibly use new concepts in combination with existing ones, children would have an incentive to learn and use words without waiting for tens or hundreds of examples.

Compared to humans, current pre-trained language models are inefficient word learners, both in the total amount of pre-training data and the number of examples needed for each word.
Even though large language models (LLMs) are typically pre-trained on four or five orders of magnitude more language input than any single human could receive \citep{linzen2020accelerate,Frank2023BridgingTD}, they struggle with systematic generalizations of words that are rare or unseen in their training data \citep{wei-etal-2021-frequency,Razeghi2022ImpactOP,Kim2022UncontrolledLE,Batsuren2024EvaluatingST,Land2024FishingFM}.
%It can also be desirable to add new words to a pre-trained model's vocabulary; a common method adds new embeddings and finetunes them on examples of the words' usage \citep{herbelot-baroni-2017-high,Lampinen2017OneshotAF,hewitt2021initializing,kim-smolensky-2021-testing}, but this method has yet to demonstrate effective few-shot learning, and requires updating the model for each new word and usage.
%An alternative approach, \ac{icl}, avoids parameter updates entirely, but the ability to perform \ac{icl} usually only emerges in models with massive quantities of parameters and pre-training data \citep{GPT3}.
%Additionally, these language models followed the traditional training curriculum: build a fixed vocabulary, and then train on a fixed corpus in a randomly shuffled order, in which a word may be encountered randomly without the burstiness or episodic structure \citep{Church2000EmpiricalEO,Smith2018TheDI,Chan2022DataDP}.

This contrast between human learning and language model training raises two long-term research questions:
 1) Could language models develop a human-like ability for few-shot word learning without astronomical amounts of training data?
 2) Could existing LLMs be adapted to improve their few-shot word learning abilities, allowing them to systematically and flexibly use new words in new contexts?

\begin{figure*}[t]
\centering
\includegraphics[width=\textwidth]{method.pdf}
\caption{Illustration of \ac{metaicl-w} (top) and language modeling (bottom), which can be mixed together during training such that both contribute to model updates. Each meta-learning episode in \ac{metaicl-w} aims to learn a \textcolor[RGB]{48,111,28}{new word} from a set of study examples (sentences that use the word) in the context and then generate a generalization example that also uses the word. Each language modeling episode contains a set of unrelated sentences without meta-learned words. An episode will be converted into a single sequence in which we replace the word to be learned (if it is a meta-learning episode) with a special placeholder token (e.g., \textcolor[RGB]{226,121,46}{\texttt{[new-token]}}) and concatenate/wrap the sentences with another special separator token (e.g., \texttt{<sep>}). We do gradient updates of the model parameters to optimize the next-token prediction loss on the sequence.}
\label{fig:method}
\end{figure*}

Here, we introduce a simple method, \acf{metaicl-w}, to train or finetune a language model to develop an in-context few-shot word learning capability (see Figure~\ref{fig:method} for an illustration of our method).
We adopt meta-training (i.e., meta-learning) since it has had successes in endowing neural networks with stronger systematic generalization, closely related to our objective of word learning (see \citealp{Russin2024Frege} for a review of the successes).
Specifically, we use \acl{metaicl} (\acs{metaicl}; \citealp{min-etal-2022-metaicl,chen-etal-2022-meta}) to train from scratch or finetune an auto-regressive language model to generate new usages of a new word given a set of illustrations of the new word in its previous context. \Acf{icl} builds and uses contextual representations of the new word on the fly without parameter updates. \acs{metaicl} repeats \ac{icl} on many different new words and optimizes the model parameters for a general word-learning ability.

To demonstrate the data efficiency of our method, we train language models from scratch with \ac{metaicl-w} using small datasets: a corpus of child-directed speech (CHILDES; \citealp{CHILDES}) and a corpus approximating the word count a child encounters during language acquisition (BabyLM-10M; \citealp{BabyLM}).
% When discriminating 4 unrelated new words (each with 4 examples) sampled from the test sets of the two datasets, the trained model achieves 72\% and 77\% accuracy respectively, comparable to the pre-trained \mbox{Llama-3 8B}'s \citep{Llama-3} 71\% and 78\% accuracy.
To foreshadow our results, we find that our method's performance on few-shot classification of new words from these datasets approaches that of the pre-trained \mbox{Llama-3 8B} \citep{Llama-3}, which was trained on vastly more data. This highlights how this ability can be developed from human-scale child-input data rather than the orders-of-magnitude larger datasets typically used to train LLMs.

We also finetune \mbox{Llama-3 8B} with \ac{metaicl-w} to see if we can enhance its word-learning ability.
In a series of discriminative and generative evaluations, we show that this improves \mbox{Llama-3 8B}'s ability to discriminate between new words, identify syntactic categories of new words, and generate reasonable new usages and definitions for new words, where each new word is learned from one or a few in-context examples.
Most of these improvements are achieved without specific training on these evaluation tasks.
We will release our code upon publication of our work.
% \footnote{We plan to release our code upon acceptance for publication.}
%In addition, we find cluster structures in the contextual representations, comparable to those found in the in-vocabulary word representations \citep{Elman1990}.
%Compared to traditional language models, our method develops a stronger few-shot word learning ability using less training data.


\section{Related Work}
\label{sec:related-work}

\subsection{The Rare Word Problem}
% \paragraph{The Rare Word Problem During Pre-training and Open-Vocabulary Modeling}
Word frequencies in natural corpora follow a highly skewed (Zipfian) distribution \citep{Zipf1949HumanBA}, resulting in a heavy tail of rare words. Additionally, new words are constantly entering the language \citep{Heaps1978InformationRC}.
%Traditional NLP models rely on closed vocabularies and thus must deal with rare and new words. This is referred to as the open-vocabulary problem \citep{luong-etal-2015-addressing}.
%i.e., open-vocabulary modeling \citep{luong-etal-2013-better,luong-etal-2015-addressing,sennrich-etal-2016-neural,kawakami-etal-2017-learning,Schick2020RareWA,schick-schutze-2020-bertram}.
To represent all possible words, various word-form-based methods have been proposed, including subword- and character-based tokenizations and using morphological information (see \citealp{Mielke2021BetweenWA} for a comprehensive survey).
However, representing a word alone does not help in learning it from a few contexts in which it occurs.
Models optimized for conventional language modeling still struggle with the usage of unfamiliar or completely novel words, tokens, or token sequences, where word-forms or token identities alone do not provide enough information \citep{Ott2018AnalyzingUI,Schick2020RareWA,wei-etal-2021-frequency,Razeghi2022ImpactOP,Kim2022UncontrolledLE,Batsuren2024EvaluatingST,Land2024FishingFM}.
Instead of representing new words based on word-forms, we discard word-form information and use a dedicated special placeholder token that is the same for every new word. In this way, we aim to develop a general and efficient ability to learn a word from a few contexts of its usage.
%Several works try to address the problem during language model pre-training.
%\citet{kawakami-etal-2017-learning} utilize burstiness, the distributional property of rare words that they are often used multiple times in the near context \citep{Church2000EmpiricalEO}, and augment their language model with a key-value memory cache to allow copying words from previous contexts. %Similar in-context copying mechanisms are found in Transformers for in-context learning \citep{Olsson2022IncontextLA,Akyrek2024InContextLL,Jelassi2024RepeatAM}.
%Without assuming burstiness, \citet{Wu2020TakingNO} maintains a ``Note Dictionary'' for rare words to more efficiently use cross-sentence information during pre-training.
%In contrast, our \ac{metaicl-w} framework explicitly constructs episodes in which a relatively infrequent new word occurs repeatedly in the context, thus providing the burstiness and Zipfian properties in the training curriculum.

\subsection{Few-Shot Word Learning}
\label{sec:related-work-few-shot-word-learning}
Another line of previous work targets the problem of learning a new word from a few examples.
Most previous work aims to produce a representation for the new word, i.e., an embedding, that fits into the global word embedding space so it can be used in the same way as other learned words \citep{Mikolov2013EfficientEO,Pennington2014GloVeGV}. The embedding can be produced by aggregating the embeddings of the contexts that the new word appears in \citep{Lazaridou2017MultimodalWM,khodak-etal-2018-la}, finetuning the embedding within the context \citep{herbelot-baroni-2017-high,Lampinen2017OneshotAF,hewitt2021initializing,kim-smolensky-2021-testing}, or utilizing the word-form information \citep{luong-etal-2013-better,schick2019learning}. %or combination of context and surface form information \citep{schick2019learning,schick-schutze-2019-attentive,Schick2020RareWA,schick-schutze-2020-bertram}.
More recent work uses Transformer layers to produce the embedding based on Word2Vec embeddings \citep[HiCE]{hu-etal-2019-shot}, or by aggregating similar embeddings of word contexts from a memory system \citep[Mem2Vec]{sun-etal-2018-memory}.
Also related to our approach, \citeauthor{Teehan2024CoLLEGeCE}'s \citeyearpar{Teehan2024CoLLEGeCE} work uses a meta-learning framework named CoLLEGe to train a Transformer encoder to produce an embedding for a new word from its examples of usage.
Our method also targets few-shot word learning, but is simpler than \citet{Teehan2024CoLLEGeCE} in architecture and training and does not produce a separate embedding for each new word.

\subsection{\acl{metaicl}}
%Meta-learning, or learning to learn, has long been a focus in cognitive science and machine learning \citep{Biggs1985Meta,Bengio1991LearningAS,Smith2002ObjectNL,Lake2015HumanlevelCL}.
%Optimization-based meta-learning finds a good parameter initialization such that it can be quickly finetuned to adapt to a new task \citep{MAML}.
%Recent meta-learning methods do not require parameter updates.
%Metric-based meta-learning models \citep{Vinyals2016MatchingNF,Snell2017PrototypicalNF} learn a metric and make a prediction by a metric-weighted sum of labels of support set samples.
Building on LLMs' \acl{icl} abilities \citep{GPT3}, \acf{metaicl} optimizes language models on multiple different tasks, each learned from a few in-context examples \citep{min-etal-2022-metaicl,chen-etal-2022-meta}.\footnote{MetaICL is different from \citet{cf-metaicl-2023}, which uses in-context learning instead of parameter updates to learn from multiple tasks.}
A class of tasks that \ac{metaicl} (or similar curriculums) aim to learn and generalize requires inferring the context-dependent mapping from the symbols to meanings \citep{Lake2023HumanlikeSG,huang2024lexinvariant,Anand2024DualPL,Park2024ICLR}.
We follow this work to use \ac{metaicl} for our word learning task, in which the mapping from a new word to its meaning should be inferred purely from its usage in the context.

%\paragraph{Prompt Tuning}

% \paragraph{Data Distributional Properties}
% New or rare words often have a burstiness distribution property, that is, they tend to occur repeatedly in a near context \citep{Church2000EmpiricalEO}.
% Another common property of language is the Zipfian distribution with a long tail of infrequent items \citep{Zipf1949HumanBA}.
% These properties are found not only in written text but also in child inputs, which may be helpful for child word learning \citep{Smith2018TheDI,Slone2022TheTS}.
% These properties also drive the emergence of in-context learning and its underlying induction head mechanism in Transformer models \citep{Chan2022DataDP,Chen2024ParallelSI,Reddy2023TheMB,Singh2023TheTN}.
% As mentioned above, \citet{kawakami-etal-2017-learning} explicitly utilizes the burstiness in the context, and \citet{Wu2020TakingNO}'s ``Note Dictionary'' for rare words implicitly provides burstiness by allowing the LM to recall information from their other occurrences outside of the current context.
% Our \ac{metaicl} framework explicitly constructs episodes in which a relatively infrequent new word occurs repeatedly in the context, thus providing the burstiness and Zipfian properties in the training curriculum.

%\paragraph{Evaluation of Word Meanings and Generalizations}
%Previous work has proposed various evaluations of meanings and generalizations of rare or few-shot-learned words, based on their learned embeddings.
%Notably, the Stanford Rare Word dataset \citep{luong-etal-2013-better} and the Chimera dataset \citep{Lazaridou2017MultimodalWM} evaluate the correlation between word representations and humans on the similarities of rare words to other words. %and the Contextual Rare Word dataset \citep{khodak-etal-2018-la} extends the former.
%The Definitional Nonce dataset \citep{herbelot-baroni-2017-high} compares learned representations to gold vectors.
%The WNLaMPro dataset \citep{Schick2020RareWA} evaluates semantic and misspelling relations between pairs of words by cloze-style phrases.
%\citet{schick-schutze-2020-bertram} creates test sets of text classification tasks by adversarially replacing influential words with rare synonyms.
%\citet{luong-etal-2015-addressing} evaluates the translation quality of OOV words.
%\citet{Lampinen2017OneshotAF} evaluates the change in perplexities of unseen test sentences using the new word.
%\citet{kim-smolensky-2021-testing} test the learned embedding in sentences distinguishing the syntactic categories of the learned word.
%\citet{wei-etal-2021-frequency} test the learned word embeddings by the generalization to unseen noun-verb pairs in the English subject-verb agreement task, one of the targeted linguistic evaluations \citep{marvin-linzen-2018-targeted}.
%\citet{Teehan2024CoLLEGeCE} evaluate their generated embeddings on three more challenging tasks: GRE-style fill-in-the-blank verbal reasoning, definition inference and generation, and Internet slang usage.
%We select several applicable evaluations and adapt them for our in-context setting.

\section{Method}
The goal of our method, \ac{metaicl-w}, is to enable a model to infer the meaning of a new word from a few examples of its usage so it can understand and generate novel usage examples of the word, coherently and systematically combining it with other words in new contexts.
To achieve this, \ac{metaicl-w} trains the model to generate another usage example of the new word---a task that, when sufficiently challenging, requires mastery of this ability.
\ac{metaicl-w} is a general framework that can be applied to both training a model from scratch and finetuning a pre-trained model.
After describing the method, we introduce the training data we use, a held-out word classification task for model evaluation and hyperparameter tuning, and how we use the off-the-shelf \mbox{Llama-3 8B} as a baseline for our experiments.

\subsection{Method: \ac{metaicl-w}}
\label{sec:method}
Following the typical meta-learning approach, we construct episodes $\{\mathcal{T}_i\}_{i=1}^{N}$, each $\mathcal{T}_i$ consists of $K$ examples $\{x^{(i)}_k\}_{k=1}^{K}$ sampled in accordance with the desired task (Figure~\ref{fig:method}: top). In each episode, the model's task is to learn a new word $w_i$; each example $x^{(i)}_k$ is a sentence illustrating how $w_i$ is used. We concatenate the examples $\{x^{(i)}_k\}_{k=1}^{K}$ into a single sequence, separated by a special separator token (\texttt{<sep>} when training from scratch or a reserved special token in the \mbox{Llama-3 8B} vocabulary when finetuning \mbox{Llama-3 8B}). The objective is next-token prediction on this concatenated sequence: we expect the model to predict a new usage example given the previous examples, i.e., $p(x^{(i)}_k \mid x^{(i)}_1, \ldots, x^{(i)}_{k-1})$. We replace (mask) all occurrences of $w_i$ in the sequence with a special placeholder token (\texttt{[new-token]} when training from scratch or a different reserved special token when finetuning \mbox{Llama-3 8B}). The same placeholder token for the new word is shared across all episodes, such that the model does not learn a new embedding each time. Using the \emph{ski} example from Section~\ref{sec:intro}, the sequence for training models from scratch would be
\begin{displayquote}
\texttt{<sep>} Susie learned to \texttt{[new-token]} last winter \texttt{<sep>} People \texttt{[new-token]} on tall mountains where there's lots of snow \texttt{<sep>} I saw Susie \texttt{[new-token]} fast down the snowy mountain \texttt{<sep>}
\end{displayquote}
Note that our setting differs from previous \ac{metaicl} settings \citep{min-etal-2022-metaicl,chen-etal-2022-meta,Lake2023HumanlikeSG} in two ways. First, each example is not an input--output pair $(x^{(i)}_k, y^{(i)}_k)$, but just $x^{(i)}_k$. Second, there is no explicit separation between study examples and a query: our setting effectively uses every example $x^{(i)}_k$ as a query with all previous examples $x^{(i)}_1, \ldots, x^{(i)}_{k-1}$ as its study examples.

When we train a model from scratch, we also provide episodes of language modeling (without masked new tokens) to further facilitate language learning, as illustrated in Figure~\ref{fig:method} (bottom). Each of these episodes consists of the same number of $K$ randomly sampled unrelated sentences, without new words. We concatenate them in the same format and train the model to perform next-token prediction on the concatenated sequences. Training batches of language modeling episodes interleave with the batches of meta-learning episodes. The model can determine whether an episode is for meta-learning or language modeling from whether the special placeholder token occurs in the first sentence.

\subsection{Data}
\label{sec:dataset}

To demonstrate the data efficiency of our method compared to humans, we use data sources that are close to children's language input in quantity or quality \citep{BabyLM}.
We construct one dataset from each of two corpora: CHILDES \citep{CHILDES} and BabyLM-10M \citep{BabyLM}.
CHILDES is a corpus of transcriptions of child--caregiver speech interactions. We use input to children (excluding utterances produced by children) in the North American English portion of CHILDES.
BabyLM is an English dataset including child-directed speech as well as additional data sources, such as children's books, transcriptions of dialogs between adults, and Wikipedia articles. We use the 10M word corpus constructed as part of the first BabyLM Challenge.

Each dataset consists of two disjoint components, one for meta-learning (the leftmost set in Figure~\ref{fig:method}: top) and the other for language modeling (the leftmost set in Figure~\ref{fig:method}: bottom).
We select a set of lower-frequency words in the corpus to be meta-learned in the meta-learning component.\footnote{Different word-forms of the same lexeme, like ``\emph{ski},'' ``\emph{skis},'' and ``\emph{skiing},'' are treated as different words in the dataset. See Appendix~\ref{app:word} for further discussion.}
Each meta-learned word $w$ has a set of $n_w$ sentence examples illustrating its usage.
We assign each sentence in the corpus to at most one meta-learned word, so the identity of the word masked by the placeholder token is not revealed in other meta-learning episodes.
During each training epoch, the $n_w$ examples for each word $w$ are split into $\lfloor\frac{n_w}{K}\rfloor$ (non-overlapping) episodes of $K$ examples, such that more frequent words have more episodes. This way of sampling episodes preserves the original Zipfian distribution of the word frequencies. Examples in the episodes are shuffled for each training epoch.
Other sentences in the corpus that have no meta-learned words are used for language modeling (Figure~\ref{fig:method} bottom).

We split both the meta-learning component (by word) and the language modeling component (by sentence) into training (80\%), validation (10\%) and test (10\%) portions.
Each dataset is used for both training models from scratch and finetuning pre-trained \mbox{Llama-3 8B}, but the text is formatted and tokenized differently (in addition to the different special tokens in Section~\ref{sec:method}; see Appendix~\ref{app:model} for the differences).
We provide additional details about data preprocessing, sentence assignment, dataset splitting, and text formatting in Appendix~\ref{app:dataset}, with statistics of our datasets shown in Table~\ref{tab:dataset-statistics}.
In the training portion, our CHILDES dataset contains 7,790 words to be meta-learned and has a total of $5.8$M tokens, while our BabyLM-10M dataset contains 15,821 words to be meta-learned and has a total of $7.8$M tokens.
In comparison, a child receives roughly $3$M to $12$M words per year \citep{Frank2023BridgingTD}, and thus our training data is of a similar magnitude to a year's worth of linguistic input for a child.

\subsection{Held-out Word Classification}
\label{sec:word-classification}

We introduce a word classification task, in which we measure the model's ability to discriminate the identities of new words that were never seen during training (i.e., held-out), based on in-context study examples. Validation accuracy on this task is used to tune training hyperparameters (e.g., learning rate; described later).

Given a query example sentence $q$ that uses a new word and a set of $C$ candidate words $\{w^{(c)}\}_{c=1}^{C}$, the task for the model is to match the query example to the most suitable one among the $C$ candidate words. Each $w^{(c)}$ is represented by a context containing a set of $K-1$ study examples $\{x^{(c)}_k\}_{k=1}^{K-1}$ illustrating its usage. The context of $w^{(c)}$ is a sequence in the same format as the first $K-1$ examples in a training episode, ending with a separator token (e.g., \texttt{<sep>}): \texttt{<sep>}~$x^{(c)}_1$~\texttt{<sep>}~$\cdots$~\texttt{<sep>}~$x^{(c)}_{K-1}$~\texttt{<sep>}. The query example is formatted as a continuation sequence of the context: $q$~\texttt{<sep>}.
This formatting ensures that concatenating a context sequence and a query sequence results in a sequence with $K$ examples, just like a sequence for a meta-learning training episode.
To determine the best match, we compute the conditional likelihood of the query sequence given the context: $p_\textrm{LM}(q \mid x^{(c)}_1, \ldots, x^{(c)}_{K-1})$. The model predicts the word corresponding to the context with the highest likelihood: $\argmax_{c} p_\textrm{LM}(q \mid x^{(c)}_1, \ldots, x^{(c)}_{K-1})$.
The prediction is correct if it is the ground-truth word in the query $q$.

We evaluate each model (trained from scratch or finetuned) by measuring the classification accuracy on held-out meta-learned words from the validation or test portions of the model's training or finetuning corpus.
For each evaluation, we group $C$ distinct meta-learned words into a $C$-way classification task. For each word, we sample $K-1$ study examples and one query example to construct the task.
See Appendix~\ref{app:word-classification} for additional details on task construction.

\subsection{Baseline: Off-the-shelf Llama-3 8B}
\label{sec:baseline}
For training models from scratch, we need an LLM that is pre-trained on massive data with conventional language modeling for data-efficiency comparison. To determine the effectiveness of finetuning an LLM, we need to evaluate its baseline word-learning ability.
To address both needs, we use the off-the-shelf \mbox{Llama-3 8B} model as a baseline for word-learning tasks.
We experiment with both the pre-trained and the instruction-tuned variants of the model. We primarily report baseline results from the pre-trained variant, and present results from the instruction-tuned variant only in the generative settings, where its performance may differ significantly from that of the pre-trained one.
For evaluation, we present a meta-learning episode to \mbox{Llama-3 8B} in a text format similar to the training or finetuning sequences (Section~\ref{sec:method}), but designed to be more natural and closer to its pre-training data.
In particular, we use a pseudo-word (e.g., ``\textit{dax}'') as the placeholder for the new word, with a newline character and a star ``\verb|\n *|'' serving as the separator between examples, effectively formatting the examples as a list.\footnote{We choose the pseudo-word to be meaningless. However, a pre-trained LLM may ascribe a meaning to the pseudo-word based on its form. We acknowledge that replacing a word in an example with a pseudo-word could mislead the LLM and weaken the baseline. See Appendix~\ref{app:word} for detailed discussion.}
Using the \emph{ski} example in Section~\ref{sec:intro} again, the formatted text appears as follows:
\begin{displayquote}
 * Susie learned to \textit{dax} last winter

 * People \textit{dax} on tall mountains where there's lots of snow

 * I saw Susie \textit{dax} fast down the snowy mountain

 *
\end{displayquote}
The ``\verb|\n *|'' at the end serves as the last separator, like the last \texttt{<sep>} in the example sequence in Section~\ref{sec:method}.

\section{Training Models From Scratch}
\label{sec:train-from-scratch}
In this section, we investigate whether models can develop the ability of few-shot word learning from human-scale input.
We use the GPT-NeoX transformer architecture \citep{GPT-NeoX} with configurations modified from Pythia-160M \citep{Pythia}.\footnote{We use an architecture with modern features such as relative positional encoding which may help in extrapolation to longer sequences and more examples. See Appendix~\ref{app:model} for details of our modifications.}
We use word-level tokenization. We exclude words with a frequency less than five from the vocabulary and replace them with \texttt{<unk>} tokens. We likewise remove the words that are to be meta-learned from this vocabulary and replace all of their occurrences in sentences other than their meta-learning episodes with \texttt{<unk>}. As mentioned in Section~\ref{sec:method}, the vocabulary also includes two special tokens: the placeholder token \texttt{[new-token]} and the separator token \texttt{<sep>}.

On each of the two datasets (CHILDES and BabyLM-10M) we train three models from scratch (i.e., the models are randomly initialized), each with $K=5$ examples per episode and a different random seed.
In each of the three runs, we choose the checkpoint with the lowest validation loss on the meta-learning objective.
Using one random seed, we fix the batch size and tune other training hyperparameters, including the learning rate and weight decay, for the best 4-way ($C=4$) held-out word classification accuracy on the validation portion of the dataset (the task was introduced in Section~\ref{sec:word-classification}).
We then apply the same training hyperparameters to the other seeds.
See Appendix~\ref{app:model} for detailed architecture configurations and training hyperparameters including batch size, learning rate (with scheduling), and weight decay.
In the following, we report mean accuracies of models across the three runs on the test portion of the dataset they were trained on.

\paragraph{Results}
Models trained from scratch on $K=5$ examples per episode sampled from CHILDES and BabyLM-10M achieve test accuracies of 72\% and 77\%, respectively, on the 4-way ($C=4$) classification task.
These results are substantially higher than random chance (25\%) and close to the 71\% and 78\% accuracies achieved by \mbox{Llama-3 8B} baseline, which was pre-trained on orders of magnitude more data.
We provide results in additional settings, including experiments with $K=10$ examples on CHILDES and 8-way ($C=8$) classification, in Appendix~\ref{app:word-classification}, Table~\ref{tab:word-classification}. Across all settings, models trained from scratch consistently achieve accuracies well above chance and within a 3\% margin of the \mbox{Llama-3 8B} baseline.
These findings (on CHILDES in particular) demonstrate that few-shot word learning can be effectively acquired using our method, even with human-scale child-input data.

\section{Finetuning Pre-trained LLMs}
\label{sec:finetuning}

In this section, we test if our method can improve pre-trained LLMs' in-context few-shot word learning abilities.
We finetune \mbox{Llama-3 8B} with \ac{metaicl-w} three times on the meta-learning component of BabyLM-10M, each run with $K=5$ examples per episode and a different random seed.\footnote{We focus on finetuning models on BabyLM-10M in this section, since it is more diversified and usually yields better results than CHILDES.}
We refer to the models finetuned with \ac{metaicl-w} as \ac{metaicl-w} models.
We do not include the language modeling components since the LLM already learned a large vocabulary and is capable of language modeling.
We finetune from both the pre-trained and instruction-tuned variants of \mbox{Llama-3 8B}, but we refer to the models finetuned from the pre-trained variant by default, same as for the baseline (Section~\ref{sec:baseline}).
%Unlike training models from scratch, we retain the \mbox{Llama-3} tokenization scheme and use two of the reserved special tokens in the \mbox{Llama-3} vocabulary as the placeholder and separator tokens (Section~\ref{sec:method}).
We freeze all of the model's parameters except the input and output embeddings of these two special tokens. We initialize the embeddings of these two special tokens as the mean of all other input/output embeddings \citep{hewitt2021initializing}.
We select the checkpoint for each run and tune the learning rate in the same way as when training from scratch, except that we do not apply weight decay (Section~\ref{sec:train-from-scratch}).
See Appendix~\ref{app:model} for more details on text formatting, tokenization, and training hyperparameters including batch size and learning rate (with scheduling).
In the following, we evaluate the \ac{metaicl-w} models and baselines on a series of tasks.

\subsection{Held-out Word Classification}
\label{sec:word-classification-finetuned}
We first evaluate models on the held-out word classification task (Section~\ref{sec:word-classification}).
Finetuning \mbox{Llama-3 8B} with \ac{metaicl-w} boosts the test 4-way ($C=4$) classification accuracy from the baseline level of 78\% to 87\% on BabyLM-10M (and from 71\% to 79\% on CHILDES).
We provide results for additional values of $K$ and $C$ in Appendix~\ref{app:word-classification}, Table~\ref{tab:word-classification}; broadly, across all settings, the \ac{metaicl-w} model improves test accuracy by 8--10\% over the \mbox{Llama-3 8B} baseline.
These findings show that \ac{metaicl-w} finetuning effectively improves the pre-trained LLM's in-context few-shot word learning ability.

Despite these strong results, this task does not assess more fine-grained aspects of meaning that may not be apparent from discriminating an arbitrary set of words, and the semantic coherence of the usage contexts could be a shortcut utilized by the model (see Appendix~\ref{app:word-classification} for further discussion).
To address this, we provide the next analysis focusing on the syntactic categories of words.

\subsection{Syntactic Category Classification}
\label{sec:syntactic-category-classification}
In this evaluation, we test if models can differentiate words in different syntactic categories, a crucial feature for systematic generalization.
We follow the classification paradigm introduced in Section~\ref{sec:word-classification}.
We use the methodology of \citet{kim-smolensky-2021-testing} as well as the dataset they constructed from MNLI, a Natural Language Inference dataset \citep{MNLI}. The dataset focuses on four syntactic categories (noun, verb, adjective, and adverb) and tests the ability to differentiate each pair of categories. See Appendix~\ref{app:syntactic-category-classification} for details of the dataset.

In each instance of the classification task, we learn two new words $w^{(1)}$ and $w^{(2)}$ in different syntactic categories; the syntactic category of each new word $w^{(i)}$ is unambiguously signaled by a study example $x^{(i)}$ (replacing the word with the placeholder, e.g., \texttt{[new-token]}).
For example, say $w^{(1)}$ is a noun and $w^{(2)}$ is a verb:
%\newpage
\begin{enumerate}[label=(\arabic*)]
\item \textit{A \texttt{[new-token]} needs two people.} (for $w^{(1)}$)
\item \textit{She \texttt{[new-token]} at the group.} (for $w^{(2)}$)
\end{enumerate}
We test our models on query examples that use a word in one of the two categories, as in the following examples:
\begin{enumerate}[label=(\arabic*)]
\item \textit{Keep everyone else company by sitting in the \texttt{[new-token]}.} (expecting $w^{(1)}$)
\item \textit{The colonel \texttt{[new-token]} us to a hotel.} (expecting $w^{(2)}$)
\end{enumerate}
Note that, unlike the previous task, query examples are semantically unrelated to the study examples in this task, thus excluding the shortcut of semantic coherence.
Below, we report the mean accuracies across the three runs.

\paragraph{Results}
We first find that the \mbox{Llama-3 8B} baseline achieves 64\% accuracy on this task, which is higher than random chance (50\%), suggesting that it can infer the syntactic categories of new words in one shot and generalize them to novel contexts.
The \ac{metaicl-w} model improves accuracy to 83\%, a 19\% increase over the baseline.
Fine-grained results from models finetuned with \ac{metaicl-w} (and trained from scratch) are provided in Appendix~\ref{app:syntactic-category-classification}.
We find in all settings that the \ac{metaicl-w} model improves accuracy by 11--26\% compared to the baseline on all pairs of categories.
These improvements show that \ac{metaicl-w} finetuning effectively helps in learning the syntactic categories of new words and generalizing accordingly.
In addition, note that our models are not specifically finetuned on this syntactic category classification task and dataset, demonstrating the generality of the acquired word learning ability.

\subsection{New Usage Example Generation}
\label{sec:example-generation}

The two tests we have described so far evaluate models in a discriminative setting. Here, we quantitatively and qualitatively evaluate if models use the new word appropriately in a generative setting.
For a \ac{metaicl-w} model finetuned with $K$ examples per episode, we evaluate it by showing it $K-1$ in-context study examples, formatted as a sequence in the classification setting (Section~\ref{sec:word-classification}).
We ask the model to do what it was trained for: We prompt the model with this sequence of study examples, and because the sequence ends with a separator token, the model will continue the sequence by generating a new usage example, ending with another separator token as End-Of-Sequence.

We sample study examples from two datasets: the BabyLM-10M test portion in Section~\ref{sec:dataset} and the Chimera dataset \citep{Lazaridou2017MultimodalWM}.
The Chimera dataset was specifically constructed for few-shot word learning. It has 33 different new words for learning, each referring to a ``chimera'' concept, i.e., a mixture of two existing and related concepts (e.g., a cello and a bagpipe). The usage examples of a new word are sentences using one of the components of the chimera, randomly extracted from a large corpus. See Appendix~\ref{app:example-evaluation} for additional details of the dataset and our preprocessing.

For the quantitative evaluation, we compare a pair of new usage examples generated from \mbox{Llama-3 8B} baseline and a \ac{metaicl-w} model finetuned from it.
The comparison is simulated as a head-to-head competition following the methodology in the definition generation section of \citet{Teehan2024CoLLEGeCE}.
Specifically, we provide \mbox{GPT-4o} \citep{GPT-4o} the same $K-1$ study examples in a list format with a pseudo-word ``\textit{dax}'' as the placeholder for the word, as in the baseline (without the last separator; Section~\ref{sec:baseline}), followed by a question ``Which of the following is a better next example for the word `dax', or they tie?'' with three shuffled options, including the two generations and one ``Tie''. (See Appendix~\ref{app:comparing-generations} for detailed settings of prompting.) The choice of \mbox{GPT-4o} decides whether and which one model wins the competition, or whether the models were tied in quality.
For the qualitative evaluation, we manually pick meta-learned words (shown in Table~\ref{tab:babylm-generation-brief} and Tables~\ref{tab:babylm-generation},~\ref{tab:chimera-generation},~and~\ref{tab:babylm-generation-error} in Appendix~\ref{app:example-evaluation}) and examine the syntactic correctness and semantic appropriateness of the generated examples.

\begin{table}[t]
\small
\begin{center}
\resizebox{\linewidth}{!}{
\begin{tabular}{P{1.2cm}c|P{1.13cm}P{1.13cm}|P{1.13cm}}
\toprule
 & & \multicolumn{2}{c|}{\bf New Usage Example} & {\bf Definition} \\
%\midrule
{\bf Variant} & {\bf Method} & {\bf BabyLM-10M test} & {\bf Chimera} & {\bf CoLLEGe-DefGen} \\
\midrule
\multirow{2}{=}{pre-trained}
& baseline & 32 & 42 & 29 \\
& +\ac{metaicl-w} & \textbf{52} & \textbf{55} & \textbf{39} \\
\midrule
\multirow{2}{=}{instruction-tuned}
& baseline & 41 & \textbf{52} & 33 \\
& +\ac{metaicl-w} & \textbf{47} & 36 & \textbf{37} \\
\bottomrule
\end{tabular}
}
\end{center}
\caption{Percentages of wins of each model when comparing the generations from \mbox{Llama-3 8B} baseline (pre-trained to instruction-tuned) with a \ac{metaicl-w} model finetuned from that baseline, judged by \mbox{GPT-4o}.
The left two datasets are for new usage example generation in Section~\ref{sec:example-generation}, and the right-most one is for definition generation in Section~\ref{sec:definition-generation}.
Each new example or definition is generated by greedy decoding.
(Results of top-p sampled generations are shown in Table~\ref{tab:generative-quantitative-compare-top-p} in Appendix~\ref{app:comparing-generations}.)
The percentage of ties is the remaining after subtracting the win percentages of the two models.
\mbox{GPT-4o} more frequently chooses the \ac{metaicl-w} model as the winner compared to the corresponding baseline in all settings except for the instruction-tuned variant on Chimera.}
\label{tab:generative-quantitative-compare-greedy}
\end{table}

\begin{table*}[t]
\small
\begin{center}
\begin{tabular}{p{8cm}p{4.3cm}p{0.8cm}}
\toprule
\textbf{Study Example Sentences} & \textbf{\ac{metaicl-w} Generated Examples} & \textbf{Word} \\
\midrule
$\bullet$ the first blacksmiths were \texttt{[new-token]}.
$\bullet$ many civilisations were in the area that is now turkey, like the \texttt{[new-token]}, the roman empire and the byzantine empire.
$\bullet$ spread of hepatoscopy and astrology to \texttt{[new-token]}, etruscans, greeks and romans and to china
$\bullet$ the first major empire in the area was the \texttt{[new-token]} (from the 18th century to the 13th century bce).
&
1. the \texttt{[new-token]} were a people who lived in the area of turkey.
2. perhaps the most famous and widely used alchemical symbol, first popularized by \texttt{[new-token]} alchemists, is the ouroboros.
& \textit{hittites}
\\
\bottomrule
\end{tabular}
\end{center}
\caption{New examples generated for a word from the BabyLM-10M test portion by the \ac{metaicl-w} model. The first one is generated by greedy decoding, and the second one by sampling with top-p=$0.92$.
The \ac{metaicl-w} model learns that \emph{hittites} is an ancient ethnic group. However, the greedy-decoded example copies the information (turkey) from the study example, while the sampled example makes seemingly plausible but factually incorrect generalizations (the earliest known ouroboros is found in ancient Egyptian text.)
}
\label{tab:babylm-generation-brief}
\end{table*}

\paragraph{Results}
%In general, the model can generate reasonable and novel uses of many new words, despite failures and errors in several cases.
For the quantitative evaluation, Table~\ref{tab:generative-quantitative-compare-greedy} shows the percentages of wins of each of the baseline and the \ac{metaicl-w} model on both the BabyLM-10M test portion and Chimera. Across all settings, the \ac{metaicl-w} model wins more often than the corresponding baseline except for the instruction-tuned variant on Chimera, demonstrating the improvement brought by \ac{metaicl-w}.
For the qualitative evaluation, Table~\ref{tab:babylm-generation-brief} shows a word picked from the BabyLM-10M test portion along with its study and generated examples. See Appendix~\ref{app:example-evaluation} for additional examples from the BabyLM-10M test portion (Tables~\ref{tab:babylm-generation}~and~\ref{tab:babylm-generation-error}) and Chimera (Table~\ref{tab:chimera-generation}) and detailed analysis of both the baseline and the \ac{metaicl-w} model's generations. A manual analysis of these generated examples reveals that the \ac{metaicl-w} model more often generates syntactically correct and semantically plausible new usage examples compared to the baseline, confirming that \ac{metaicl-w} finetuning improves the ability to understand and use a new word. Nevertheless, in several cases, the \ac{metaicl-w} model still shows obvious syntactic and factual errors and merely rewords the study examples.

\subsection{Definition Generation}
\label{sec:definition-generation}
To further probe how well \ac{metaicl-w} finetuning helps the model understand a new word, we prompt each model to generate a definition for the word given one or a few usage examples.
We again use the methodology of \citet{Teehan2024CoLLEGeCE} for definition generation and evaluation, as well as the two evaluation datasets they used: CoLLEGe-DefGen, which they created, and the Oxford dataset \citep{gadetsky-etal-2018-conditional}.
CoLLEGe-DefGen was constructed by selecting 954 words from WordNet \citep{WordNet} and prompting \mbox{GPT-4} \citep{GPT4} to generate one definition and five usage examples for each word.
The model generates a definition from one, two, or three usage examples sampled for each word in this dataset (i.e., in 1-, 2-, or 3-shot settings).
The Oxford test set consists of 12,232 words, each with a definition and a usage example collected from the Oxford Dictionary.
The model generates a definition from the only usage example for each word in this dataset (i.e., in a 1-shot setting).
To generate a definition, we prompt the model with the sequence of the usage example(s) (as in Section~\ref{sec:example-generation}) followed by ``The word \texttt{[new-token]} in the above sentence(s) is defined as "''\footnote{The prompt ends with a double quotation mark ("), so that the model will continue with a definition ending at another double quotation mark. This makes extracting definitions easy.} (\texttt{[new-token]} is instead the placeholder token or pseudoword, as appropriate).
For additional comparisons with models expected to do especially well on this task, we also evaluate specialized definition-generation models: \citeauthor{giulianelli-etal-2023-interpretable}'s \citeyearpar{giulianelli-etal-2023-interpretable} \mbox{FLAN-T5} models \citep{chung2024scaling}.
See Appendix~\ref{app:definition-evaluation} for details of data preprocessing and the specialized models.

For the quantitative evaluation, we perform two types of comparison.
The first type compares the model-generated and ground-truth definitions for each word by computing BERTScore F1 \citep{zhang2019bertscore} and \mbox{ROUGE-L} \citep{lin-2004-rouge}.
The second type compares a pair of definitions generated from \mbox{Llama-3 8B} baseline and a \ac{metaicl-w} model finetuned from it. Similarly to what we did in Section~\ref{sec:example-generation}, we ask \mbox{GPT-4o} a question (without usage examples): ``Which of the following is a better definition for the word `\textit{Word}', or they tie?'' where \textit{Word} is the ground-truth word form, followed by three shuffled options including the two generated definitions and one ``Tie'' (see Appendix~\ref{app:comparing-generations} for detailed prompting settings).\footnote{We only perform this comparison on the CoLLEGe-DefGen dataset due to the large scale of the Oxford dataset.}
For the qualitative evaluation, we manually inspect 1-shot generated definitions for words from each dataset (presented in Table~\ref{tab:defgen-definition} and Tables~\ref{tab:defgen-definition-more}~and~\ref{tab:oxford-definition} in Appendix~\ref{app:definition-evaluation}).

\begin{table*}[t]
\small
\begin{center}
\begin{tabular}{p{1.2cm}c|cccc}
\toprule
\multicolumn{2}{c|}{\bf Model} & \multicolumn{2}{c}{\bf CoLLEGe-DefGen} & \multicolumn{2}{c}{\bf Oxford} \\
 \bf Variant & \bf Method & \bf BERTScore F1 & \bf ROUGE-L & \bf BERTScore F1 & \bf ROUGE-L \\
%\midrule
%FLAN-T5 Base  &+DefInstr baseline          & 83.1 & 13.1 & 84.4 & 16.5 \\
%FLAN-T5 Large &+DefInstr baseline          & 83.8 & 15.5 & 84.7 & 17.4 \\
%FLAN-T5 XL    &+DefInstr baseline          & 83.1 & 12.4 & \textbf{84.9} & \textbf{19.4} \\
\midrule
\multirow{2}{=}{pre-trained}
           &             baseline          & 85.1 & 14.9 & 83.2 & 11.0 \\
           & +\ac{metaicl-w}               & 85.4 & 18.7 & \textbf{84.7} & 16.3 \\
\midrule
\multirow{2}{=}{instruction-tuned}
                   &     baseline          & 85.3 & 17.6 & 83.6 & 12.5 \\
                   &+\ac{metaicl-w}        & \textbf{85.8} & \textbf{20.7} & \textbf{84.7} & \textbf{16.5} \\
%\midrule
%Llama-2 7B         & CoLLEGe*              & 84.1 & 18.0 & 83.6 & 17.1 \\
\bottomrule
\end{tabular}
\end{center}
\caption{Quantitative evaluation of generated definitions by comparing them with ground-truth definitions.
See Table~\ref{tab:definition-quantitative-1-shot} in Appendix~\ref{app:definition-evaluation} for results from all models.
We generate a definition from only one example (1-shot). All definitions are generated with greedy decoding.
Scores of \ac{metaicl-w} models are averaged across three runs.
Finetuning with \ac{metaicl-w} improves the baseline models on both datasets and both metrics, and the \ac{metaicl-w} model finetuned from the instruction-tuned variant of \mbox{Llama-3 8B} performs the best.
}
\label{tab:definition-quantitative-1-shot-brief}
\end{table*}
\begin{table*}[t]
\small
\begin{center}
\begin{tabular}{p{4.9cm}p{2.9cm}p{4.1cm}p{1.4cm}}
\toprule
\textbf{Example Sentence} & \textbf{\ac{metaicl-w} Definition} & \textbf{True Definition} & \textbf{Word}
%\\
%\midrule
%The eerie creak of the attic door, coupled with the flickering candlelight, was enough to give anyone the \texttt{[new-token]}.
%& a feeling of fear or unease
%& a feeling of nervousness or discomfort; the creeps.
%& willies
\\
\midrule
After his thorough inspection of the antique pocket watch, the bespectacled collector sighed, claiming it was a \texttt{[new-token]}, much to the seller's disappointment.
& a thing that is not genuine or authentic
& a deception or trick
& \textit{swiz}
%\\
%\midrule
%After an exhausting day at work, he found solace at his local pub where he could \texttt{[new-token]} his worries over a pint of cold beer.
%& to make something less severe or unpleasant
%& to remove goods or cargo from a vehicle, ship, or container. It can also mean to relieve of something burdensome or oppressive, or to express one's thoughts or feelings freely.
%& unload
\\
\midrule
Despite his greed, the businessman felt bound by a \texttt{[new-token]} to maintain ethical practices.
& a promise or agreement to do something
& a moral obligation or command that is unconditionally and universally binding
& \textit{categorical imperative}
\\
\bottomrule
\end{tabular}
\end{center}
\caption{Definitions for two words from CoLLEGe-DefGen generated by the \ac{metaicl-w} model finetuned from instruction-tuned \mbox{Llama-3 8B} with greedy decoding.
Each definition is generated using the single example sentence shown and provided in context.
The generated definitions managed to infer the core semantic features from the examples, though they are not precise enough compared to the true definitions.
In this first example, the \ac{metaicl-w} definition for the word ``\textit{swiz}'' captures the word's core meaning of fakeness, which is a reasonable inference from the example, but misses the intentional aspect, a nuance of the true definition.
In the second example, the \ac{metaicl-w} definition for ``\textit{categorical imperative}'' captures the core meaning of obligation, which is a reasonable contrast to the businessman's greed, but misses the ``unconditionally and universally binding'' aspect in the true definition.
}
\label{tab:defgen-definition}
\end{table*}

\paragraph{Results}
For the quantitative evaluation, we first present the 1-shot scores of comparing the model-generated and ground-truth definitions for \mbox{Llama-3 8B} baselines and the \ac{metaicl-w} models in Table~\ref{tab:definition-quantitative-1-shot-brief}. In Appendix~\ref{app:definition-evaluation}, we present 1-shot scores for all models (including the specialized \mbox{FLAN-T5} models and the original \mbox{CoLLEGe} model) in Table~\ref{tab:definition-quantitative-1-shot} and averaged 1-, 2-, and 3-shot results on CoLLEGe-DefGen in Table~\ref{tab:definition-quantitative-defgen}. In all of these settings, \ac{metaicl-w} finetuning improves the baseline scores by 0.3--1.5 on BERTScore F1 and 3.1--5.3 on \mbox{ROUGE-L}. On CoLLEGe-DefGen, the \ac{metaicl-w} model finetuned from the instruction-tuned \mbox{Llama-3 8B} outperforms all other models across all settings. On Oxford, the \ac{metaicl-w} models finetuned from both variants perform comparably well, but they are inferior to the largest specialized \mbox{FLAN-T5} by 2.9 on \mbox{ROUGE-L}. However, note that our \ac{metaicl-w} finetuning is neither tailored for generating definitions nor using these specific definition datasets.
In Table~\ref{tab:generative-quantitative-compare-greedy}, when comparing the definitions generated from each baseline and a \ac{metaicl-w} model finetuned from that baseline, the latter is more often favored over the corresponding baselines for both \mbox{Llama-3 8B} variants.

For the qualitative evaluation, Table~\ref{tab:defgen-definition} shows \ac{metaicl-w}-model-generated and ground-truth definitions for words from CoLLEGe-DefGen (see Tables~\ref{tab:defgen-definition-more}~and~\ref{tab:oxford-definition} in Appendix~\ref{app:definition-evaluation} for additional examples from CoLLEGe-DefGen and Oxford).
To summarize our manual analysis, we find that definitions generated by the \ac{metaicl-w} model often capture most of the word meanings, form reasonable inferences from the contexts, and outperform the baseline. However, they are not always precise compared to the ground-truth definitions.

\section{Conclusion}
In this work, we present \ac{metaicl-w}, a new method to improve language models' capability to learn a new word from a few in-context usage examples.
\ac{metaicl-w} successfully induced this ability in models trained from scratch with human-scale linguistic data, as indicated by their performances in differentiating new words (Section~\ref{sec:train-from-scratch}).
\ac{metaicl-w} finetuning further improved the word learning performance of a pre-trained LLM (\mbox{Llama-3 8B}), as demonstrated in their improvements in differentiating new words (Section~\ref{sec:word-classification-finetuned}~and~\ref{sec:syntactic-category-classification}) as well as in generating new usage examples (Section~\ref{sec:example-generation}) and definitions (Section~\ref{sec:definition-generation}) for the learned new words.
In summary, this word-learning capability enables models to systematically and flexibly understand and use a new word in novel contexts, and can be immediately transferred to other words and tasks without additional training.

The efficacy of \ac{metaicl-w}, or meta-learning in general, suggests that human-level efficiency in linguistic generalizations may be acquired through practicing over many instances of learning tasks, without presuming strict, explicit inductive biases \citep{Russin2024Frege,Irie2024NNPractice}.
Whether models achieve the generalizations in this work through human-like mechanisms, such as systematicity and categorical abstraction, remains for future analysis.

\section{Limitations} % This section is required at the end of the paper, but not in the 8-page limit, according to ACL Rolling Review: https://aclrollingreview.org/cfp#paper-submission-information with my quote "This section should be included at the end of the paper, before the references, and it will not count toward the page limit."

%Despite the series of positive results, our work has a number of limitations.

\paragraph{Learning Settings}
In this work, we consider word learning only in the text modality, in which the language model learns the meaning from the distribution of words. However, many words have real-world references, which usually accompany human word learning.
We also use aggregated data from multiple sources, not from single-human/child input.
Thus, a multimodal, grounded setting of word learning using a single agent's input would be more realistic.

In addition, we only consider learning a single new word on the fly.
However, in real-world learning, both humans and models need to continually learn multiple words, usages, and even abstract rules \citep{mueller-etal-2024-context}. Implementing this continual learning setting would be another future direction.

\paragraph{Novelty of New Words When Testing LLMs}
%To support efficient and appropriate generalization, a new word's meaning must share or recombine features of other words that were learned in the model weights.
When testing LLMs (Section~\ref{sec:finetuning}), the words and example sentences we use may already exist in the pre-training data, potentially allowing LLMs to recall known word meanings rather than learn genuinely new ones (note, however, the Chimera dataset introduces new concepts which are unusual and not lexicalized).
The performance of the baseline LLMs shows that, even with this potential worry, there is room for improvement, which the \ac{metaicl-w}-finetuned LLMs are able to achieve.
% Nevertheless, the imperfect performance of both the baseline and the \ac{metaicl-w}-finetuned LLMs suggests this is not always the case.
% Moreover, \ac{metaicl-w} finetuning improves performance over the baselines, regardless of the exact mechanism.

Models trained from scratch with \ac{metaicl-w} do not have this limitation. Their training data explicitly excludes held-out test words (Section~\ref{sec:train-from-scratch}). Therefore, their test performance reflects their genuine ability to learn novel words, and this ability can be developed by \ac{metaicl-w}.

%Lastly, we do not fully explore the meaning space of new words, especially how words are related to each other, in our evaluations.
%Our series of evaluations provides a glimpse of each new word's meaning and how they differ from each other, but not the structure of their meaning space and relations.
%One way to explore the meaning space is similarity-based analyses, as performed in previous work that have an embedding for each word (Section~\ref{sec:related-work-few-shot-word-learning}).
%However, in our in-context setting, such analyses are not straightforward since it is unclear which hidden representation in the model stands for the word and can be used to compute a similarity metric (though see \citealp{Park2024ICLR}).
%To perform similarity-based analyses like in previous work based on word embeddings, future work may choose a potential similarity metric for in-context-learned words, such as the conditional likelihood of one word's usage given another word's usage (Section~\ref{sec:word-classification}) and similarities between contextualized definitions \citep{giulianelli-etal-2023-interpretable}.

%Lastly, we do not understand the mechanism of this in-context learning of words, how it emerged, and how it compares to in-weights word learning.
%Future work can compare the results of in-context and in-weights learning, analyze the trade-off between the meanings acquired in pre-training and the context-specified ones in varying distributions and constraints, and also understand the underlying mechanisms through interpretability techniques.% \citep{Chan2022,Park2024ICLR}

%\iffalse
\section*{Acknowledgements}
We thank Michael Hu, Will Merrill, Sophie Hao, Byung-Doh Oh, Shauli Ravfogel, and other members of the Computation and Psycholinguistics Lab for insightful and helpful discussions and comments.
This work is supported by the National Science Foundation under NSF Award 1922658 (for Wentao Wang) and IIS-2239862.
This work is also supported in part through the NYU IT High Performance Computing resources, services, and staff expertise.
%\fi

\clearpage
{
    \small
    %\bibliographystyle{apalike}
    \bibliography{ref,library_clean}
}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\clearpage
\appendix
\section{Word Usage Dataset Creation}
\label{app:dataset}
As we mentioned in Section~\ref{sec:dataset}, we construct one dataset from each of two corpora: CHILDES \citep{CHILDES} and BabyLM-10M \citep{BabyLM}.
The CHILDES dataset is licensed for use under a CC BY-NC-SA 3.0 license.\footnote{\url{https://talkbank.org/share/rules.html}} Our scientific use is under the terms of the license.\footnote{\url{https://creativecommons.org/licenses/by-nc-sa/3.0/}}
We did not find the license of the BabyLM dataset, which aggregated multiple public datasets. Since there is plenty of published work using this public dataset, we believe our scientific use does not violate any terms or conditions.
In the following, we describe how we preprocess these two corpora and create a word usage dataset from each corpus.
\paragraph{Preprocessing}
Since the basic units of our focus are words (as opposed to word pieces in other tokenization schemes), we need to identify words in the text.
To achieve this, we apply the same word-level tokenization to all datasets (for consistency) and mark word boundaries by whitespace during preprocessing.
Models trained from scratch use this word-level tokenization.
When the text is used in finetuning \mbox{Llama-3}, which comes with its pre-trained subword tokenizer, we remove the unnatural spaces introduced by the word-level tokenization and tokenize the text again with \mbox{Llama-3} tokenizer, so the text format becomes closer to its pre-training data (See the Finetuning paragraph in Appendix~\ref{app:model} for further details of this process).
For CHILDES data, we preprocess the data in the same way as \citet{yedetore-etal-2023-poor} did, which uses children's input in the North American English portion, but we do not split and unk the data at the preprocessing stage.
For BabyLM data, we use the data in the 10M track of the BabyLM Challenge 2023, which mixes 10 portions, each from a different data source (child- or adult-oriented, speech transcription or written text like Wikipedia). We exclude the QED portion for its poor quality (also mentioned in the 2nd BabyLM Challenge). We apply word-level tokenization on untokenized portions, and then split the text into sentences using heuristics.
We use spaCy for all word-level tokenization along with Part-Of-Speech tagging.
We lowercase all text before preprocessing to unify the capitalization of words in different places.
We deduplicate sentences and remove sentences having less than 1 word (not counting punctuation).

\paragraph{Assigning sentences and splitting}
To create a dataset from a corpus, we first get the token frequencies of all words. (Here, a word means a word-form. We discuss its implications in Appendix~\ref{app:word}.)
Then we select the set of words to be meta-learned.
We will only consider nouns, verbs, adjectives, and adverbs to be meta-learned (a word's syntactic category is based on the word's most frequent Part-Of-Speech tag).
We choose two thresholds for meta-learned words: the maximum frequency of a meta-learned word and the minimum number of examples per meta-learned word.
We use a greedy algorithm to assign each sentence in the corpus to the example set of at most one potential meta-learned word that occurs in the sentence, so each meta-learned word has at least the minimum number of examples. This ensures that the model cannot infer the identity of the word masked by the placeholder token from other sentences. These words and their example sets constitute the meta-learning component of the dataset.
We include the remaining sentences not assigned to any meta-learned word in the language-modeling component.
Finally, we split both the meta-learning component (by word) and the language-modeling component (by sentence) into training (80\%), validation (10\%), and test (10\%) portions.

When training models from scratch, we build the vocabulary from the words occurring with a minimum frequency in the training portion (same as the minimum number of examples per meta-learned word) while excluding all meta-learned words. This ensures that meta-learned words, like the lowest-frequency words, are out-of-vocabulary and will be replaced by \texttt{<unk>} tokens, so they will never be learned in-weights.

Statistics of our created datasets are shown in Table~\ref{tab:dataset-statistics}.
Read our code for full details.

\begin{table*}[t]
\small
\begin{center}
\begin{tabular}{cc|ccc|ccc}
\toprule
& & \multicolumn{3}{c|}{\bf CHILDES} & \multicolumn{3}{c}{\bf BabyLM-10M} \\
\midrule
\multicolumn{2}{c|}{max.\ freq.\ of meta-learned words}  & \multicolumn{3}{c|}{200} & \multicolumn{3}{c}{15} \\
\multicolumn{2}{c|}{min.\ \#uses of meta-learned words}  & \multicolumn{3}{c|}{5} & \multicolumn{3}{c}{5} \\
\multicolumn{2}{c|}{vocabulary size}  & \multicolumn{3}{c|}{2179} & \multicolumn{3}{c}{22,696} \\
\midrule
\multicolumn{2}{c|}{portion}                                & training& valid. & test   & training & valid. & test   \\
\midrule
\multirow{6}{3em}{meta-learning}     & \#meta-learned words &    7790 &    973 &    975 &   15,821 &   1977 &   1979 \\
                                     & total \#uses         & 201,957 & 26,449 & 26,234 &  108,466 & 13,552 & 13,563 \\
                                     & mean \#uses          &   25.93 &  27.18 &  26.91 &     6.86 &   6.85 &   6.85 \\
                                     & total \#tokens       &1,899,159& 245,509& 243,387&2,072,560 &260,701 &257,933 \\
                                     & mean sentence length &    9.40 &   9.28 &   9.28 &    19.11 &  19.24 &  19.02 \\
                                     & unk rate             &  3.32\% & 3.28\% & 3.28\% &   3.61\% & 3.78\% & 3.91\% \\
\midrule
\multirow{4}{3em}{language modeling} & \#sentences          & 508,630 & 63,578 & 63,580 &  521,911 & 65,238 & 65,240 \\
                                     & total \#tokens       &3,927,120& 492,280& 490,990&5,721,893 &715,553 &715,111 \\
                                     & mean sentence length &    7.72 &   7.74 &   7.72 &    10.96 &  10.97 &  10.96 \\
                                     & unk rate             &  1.00\% & 1.03\% & 1.00\% &   1.44\% & 1.49\% & 1.47\% \\
\midrule
\multicolumn{2}{c|}{total \#tokens}                         &5,826,279& 737,789& 734,377&7,794,453 &976,254 &973,044 \\
\bottomrule
\end{tabular}
\end{center}
\caption{Dataset statistics. All statistics are based on tokens, which mostly correspond to words except punctuations due to our word-level tokenization. ``unk rate'' is the percentage of out-of-vocabulary tokens, which are replaced by \texttt{<unk>}, in all tokens. Unk rate is slightly higher in the validation and test portions than the training portion because we build the vocabulary from the training portion. As shown by the mean sentence lengths, the meta-learning sentences are longer on average than the language modeling sentences, since meta-learned words are of lower frequency and thus are usually in more complex sentences. We manually tune the two thresholds of meta-learned words so we have enough number of meta-learned words while the unk rate is not too high.}
\label{tab:dataset-statistics}
\end{table*}

\clearpage\clearpage
\section{Model and Training Configurations}
\label{app:model}
\paragraph{Training from scratch}
We slightly modify the configuration of \mbox{Pythia-160M} \citep{Pythia}, which uses the Transformer architecture GPT-NeoX \citep{GPT-NeoX}. The configuration has $12$ layers and a hidden dimension size of $768$.
We change the vocabulary size according to the corresponding dataset, as shown in Table~\ref{tab:dataset-statistics}.
We also include three special tokens in the vocabulary: the placeholder token \texttt{[new-token]}, the separator token \texttt{<sep>}, and \texttt{<unk>}, as mentioned in Section~\ref{sec:train-from-scratch}.
We change the Pythia configuration to tie the input and output embeddings. This makes the model parameter counts smaller, $86.7$M and $102.5$M for the model trained on CHILDES and BabyLM-10M, respectively.
For both models, we use batch size (i.e., number of episodes/sequences per batch) $8$ and AdamW optimizer \citep{Loshchilov2019} with initial learning rate $3 \times 10^{-4}$, and reduce the learning rate by multiplying $0.1$ when the validation loss has stopped improving for $2$ epochs. We apply weight decay $0.07$ and $0.15$ when training on the CHILDES and BabyLM-10M datasets, respectively. Other configurations, such as no dropout, are kept the same as \mbox{Pythia-160M}.
For each setting, we run $3$ times with random seed $\{0, 1, 2\}$.
Each run is performed on a single V100 GPU for 30 epochs (9--18 hours).

\paragraph{Finetuning}
We finetune \mbox{Llama-3 8B} \citep{Llama-3} with \ac{metaicl-w} on each of the CHILDES and BabyLM-10M datasets, but we refer to the models finetuned on BabyLM-10M by default, as we mentioned in Section~\ref{sec:finetuning}.
We finetune from both the pre-trained and instruction-tuned variants of \mbox{Llama-3 8B}, but we refer to the models finetuned from the pre-trained variant by default, presenting results of finetuning from the instruction-tuned variant only in the generative settings, where their performance may differ significantly due to their different capabilities to follow the prompt.
We use two reserved special tokens in \mbox{Llama-3} tokenizer vocabulary as the placeholder token and the separator token.
To make the tokenization more natural to the model's pre-training data, we clean up tokenization spaces in the text (e.g., the space before ``,'', ``.'', or ``'s'') introduced by the word-level tokenization during preprocessing and make the placeholder token absorbs any preceding spaces of the word.
Finetuning is minimally parameter-efficient: We finetune only the input and output embeddings of the two special tokens, while freezing all other parameters. Before finetuning, the input/output embedding of either token is initialized to the mean of all input/output embeddings \citep{hewitt2021initializing}.
When finetuning the model on CHILDES with 5 examples per episode, we use batch size (i.e., number of episodes/sequences per batch) $32$ and initial learning rate $3 \times 10^{-3}$ and truncate the sequence to the max length of $80$ tokens to control the memory usage.
When finetuning the model on CHILDES with 10 examples per episode, we use batch size $8$ and initial learning rate $3 \times 10^{-4}$ and truncate the sequence to the max length of $180$ tokens.
When finetuning the model on BabyLM-10M with $5$ examples per episode, we use batch size $16$ and initial learning rate $1 \times 10^{-3}$ and truncate the sequence to the max length of $160$ tokens.
Other settings are the same as when training from scratch except that we do not apply weight decay.
Each run is performed on a single A100 GPU for 15 epochs on CHILDES (33 hours) or 12 epochs on BabyLM-10M (48 hours).

\newpage
\section{Held-out Word Classification}
\label{app:word-classification}
As we mentioned in Section~\ref{sec:word-classification}, we need different meta-learned words in the same group. Therefore, different from training, we sample only one episode of $K$ examples per word from the validation/test portions so we do not repeat the same word in a classification group. We also fix the shuffle order so all models are evaluated on the same classification task instances.
We experimented with training models with $K \in \{5, 10\}$ examples per episode on CHILDES and BabyLM-10M and evaluated each of them on the corresponding dataset with the same $K$ and $C \in \{4, 8\}$. Training models with $K = 10$ examples per episode on BabyLM-10M was unsuccessful because the concatenated sequence was too long, exceeding the GPU memory, so we do not have results in this setting.

We are aware of the weaknesses of this task.
Discriminating a new word from an arbitrary set of other new words is a relatively weak test of word meaning learning.
The task could be easy simply because different words are used in very different contexts, so the conditional likelihood may reflect just the coherence of the usage contexts between study and query examples, not the meaning of the new word (we demonstrate this point by an additional baseline below where we present the model only the usage contexts without new words).
In addition, results from the task do not tell us what features of word meanings the model is learning.
Our syntactic category classification task addresses these concerns by focusing on the syntactic aspect and breaking the semantic coherence between study and query examples (Section~\ref{sec:syntactic-category-classification}).

Below, we describe two baselines we run on this task.

\paragraph{Baseline: \mbox{Llama-3 8B} learning a pseudo-word in context (\mbox{Llama-3 8B} with `\textit{dax}')}
This is the baseline model introduced in Section~\ref{sec:baseline}. We follow the format described there and additionally prepend a prompt to make the performance better: ``The following lines are lowercased example sentences using a new word `\textit{dax}' in random order, one per line:''. (We discuss the consequence of using a same pseudo-word in Appendix~\ref{app:word}.)
\paragraph{Additional Baseline: \mbox{Llama-3 8B} modeling the coherence of usage contexts (\mbox{Llama-3 8B} with `')}
This is the additional baseline to evaluate the effectiveness of utilizing just the coherence of the contexts, as we discussed above. We remove the new word from each example (equivalent to replacing the new word with an empty string), so only the usage context of each example is retained.

For these baselines, we also experimented with the instruction-tuned variant of \mbox{Llama-3 8B} but it performs worse on this task.

Table~\ref{tab:word-classification} shows all models' held-out word classification results on the test portions of CHILDES and BabyLM-10M datasets.

\begin{table*}[t]
\small
\begin{center}
\begin{tabular}{ccc|cccc}
\toprule
\textbf{dataset} & $K$ & $C$ & \textbf{\ac{metaicl-w} from scratch} & \makecell{\bf Llama-3 8B \\ \bf with `'} & \makecell{\bf Llama-3 8B \\ \bf with `\textit{dax}'} & \makecell{\bf Llama-3 8B \\ \bf +\ac{metaicl-w}} \\
\midrule
\multirow{4}{*}{CHILDES}
        & \multirow{2}{*}{$5$}
              &   4 &            72.3(1.6) &         58.33 &            71.09 &            \textbf{79.1}(0.5) \\
        &     &   8 &            59.8(0.4) &         46.49 &            60.02 &            \textbf{70.4}(0.2) \\
\cmidrule{2-7}
        & \multirow{2}{*}{$10$}
              &   4 &            75.1(0.7) &         66.56 &            76.53 &            \textbf{84.9}(0.2) \\
        &     &   8 &            63.4(1.5) &         56.17 &            66.05 &            \textbf{75.9}(0.6) \\
\midrule
\multirow{2}{*}{BabyLM-10M}
        & \multirow{2}{*}{$5$}
              &   4 &            77.4(0.5) &         70.45 &            78.39 &            \textbf{86.5}(0.6) \\
        &     &   8 &            67.5(0.7) &         60.12 &            69.74 &            \textbf{80.5}(1.0) \\
\bottomrule
\end{tabular}
\end{center}
\caption{Accuracy (\%) of held-out word classification on the CHILDES and BabyLM-10M test sets.
We show the mean and the standard deviation (in the bracket) of 3 runs.
``\ac{metaicl-w} from scratch'' means models trained from scratch on the corresponding dataset.
``Llama-3 8B with `''' means the baseline model without prompt and remove the new word (i.e., replace the new word with an empty string).
``Llama-3 8B with `\textit{dax}''' means the baseline model with prompt learning the new word `\textit{dax}'.
We use $K-1$ study examples in this classification task, and models except the baselines are trained/finetuned on $K$ examples per training episode so they see the same number of examples during training and evaluation.
$C$ is the number of words in each group, so we will have $\lfloor \frac{n_\text{episodes}}{C} \rfloor$ groups.
Note that we discard the last batch of less than $C$ episodes, so the used numbers of episodes are slightly smaller.
Results of ``\mbox{Llama-3 8B} with `''' show that the coherence of the context already provides better-than-chance accuracy on this classification task.
Results of ``\mbox{Llama-3 8B} with `\textit{dax}''' show that the pre-trained LLM already performs well.
However, ``\mbox{Llama-3 8B} +\ac{metaicl-w}'' outperforms the baselines by a large margin, showing the effectiveness of our method.
Models finetuned with \ac{metaicl-w} from the instruction-tuned variant of \mbox{Llama-3 8B} perform worse than or close to the pre-trained variant here
(the instruction-tuned variant finetuned with \ac{metaicl-w} has 86.3\% (4-way) and 80.1\% (8-way) mean classification accuracies; the instruction-tuned variant with `\textit{dax}' has 75.2\% (4-way) and 66.0\% (8-way) classification accuracies),
so we do not include their results here.
}
\label{tab:word-classification}
\end{table*}


\clearpage
\section{Syntactic Category Classification}
\label{app:syntactic-category-classification}
As we mentioned in Section~\ref{sec:syntactic-category-classification}, we use the methodology of \citet{kim-smolensky-2021-testing} and the dataset they constructed.
The dataset was constructed from MNLI, a Natural Language Inference dataset \citep{MNLI}.
The task is to discriminate between a pair of words in two different syntactic categories.
They consider 4 syntactic categories: noun, verb, adjective, and adverb.
Therefore, they have 6 pairs of categories for discrimination.
For each category pair, the dataset contains two signal contexts (one for each category; we use them as the study examples) and 200 test sentences using a word unambiguously in either category (100 for each category; we use them as the query examples).
The main difference between our approach and that of \citet{kim-smolensky-2021-testing} is that, instead of finetuning a new word embedding on each signal context, we apply in-context learning, using each signal context as an in-context study example of the new word.
Read \citet{kim-smolensky-2021-testing} for further details.

Results from models trained from scratch, \mbox{Llama-3 8B} baseline and models finetuned from \mbox{Llama-3 8B} on the 6 category pairs and their mean are visualized in Figure~\ref{fig:syntactic-classification}.
Table~\ref{tab:category-classification-llama} shows detailed results from \mbox{Llama-3 8B} baseline and \mbox{Llama-3 8B} finetuned with \ac{metaicl-w} on BabyLM-10M.
Table~\ref{tab:category-classification-scratch} shows detailed results from models trained from scratch on both datasets.

\begin{figure*}[t]
\centering
\includegraphics[width=0.95\textwidth]{syntactic_classification_accuracy.pdf}
\caption{Syntactic classification accuracy.
Error bar shows the 95\% confidence interval given 3 runs.
``\ac{metaicl-w} from scratch on CHILDES'' (\textcolor[RGB]{31,119,180}{blue}) and ``\ac{metaicl-w} from scratch on BabyLM-10M'' (\textcolor[RGB]{255,127,14}{orange}) mean the models trained from scratch with \ac{metaicl-w} on CHILDES and BabyLM-10M, respectively.
(These models have a closed vocabulary, so many words in the dataset will be Out-Of-Vocabulary and be presented as \texttt{<unk>}, which could make the task easier.)
``\mbox{Llama-3 8B} baseline'' (\textcolor[RGB]{44,160,44}{green}) means \mbox{Llama-3 8B} baseline with pseudo-word ``\emph{dax}''.
``\mbox{Llama-3 8B} +\ac{metaicl-w} on BabyLM-10M'' (\textcolor[RGB]{214,39,40}{red}) means \mbox{Llama-3 8B} finetuned with \ac{metaicl-w} on BabyLM-10M.
``N'', ``V'', ``Adj'', and ``Adv'' are short for noun, verb, adjective, and adverb, respectively.
``Mean'' is the mean across all category pairs. The black dashed line marks the chance level (50\%).
``\mbox{Llama-3 8B} +\ac{metaicl-w} on BabyLM-10M'' (\textcolor[RGB]{214,39,40}{red}) shows improvement over ``\mbox{Llama-3 8B} baseline'' (\textcolor[RGB]{44,160,44}{green}) in all category pairs, with mean accuracy risen from 64\% to 83\%.
Note that ``\ac{metaicl-w} from scratch on BabyLM-10M'' (\textcolor[RGB]{255,127,14}{orange}) has a 77\% mean accuracy, much better than the baseline accuracy and even comparable to the \ac{metaicl-w} models finetuned from \mbox{Llama-3 8B} on many category pairs, again demonstrating its data efficiency.}
\label{fig:syntactic-classification}
\end{figure*}

\begin{table*}[t]
\small
\begin{center}
\begin{tabular}{ll|ccc|ccc}
\toprule
\multicolumn{2}{c|}{} & \multicolumn{3}{c|}{\bf Llama-3 8B baseline} & \multicolumn{3}{c}{\bf Llama-3 8B +\ac{metaicl-w}} \\
\bf Cat.\ 1 & \bf Cat.\ 2 & \bf Acc. & \bf Acc.\ (1$>$2) & \bf Acc.\ (2$>$1) & \bf Acc. & \bf Acc.\ (1$>$2) & \bf Acc.\ (2$>$1) \\
\midrule
Noun       & Verb       & 71.0 &            43 &            99 & \textbf{86.3}(1.5) & \textbf{74.7}(1.7) &          98.0(1.6) \\
Noun       & Adjective  & 66.0 &   \textbf{79} &            53 & \textbf{84.0}(2.2) &          71.3(4.6) & \textbf{96.7}(0.5) \\
Noun       & Adverb     & 64.0 &            55 &            73 & \textbf{81.3}(2.2) & \textbf{75.7}(1.7) & \textbf{87.0}(2.9) \\
Verb       & Adjective  & 70.5 &            49 &            92 & \textbf{92.7}(0.5) & \textbf{90.0}(2.2) & \textbf{95.3}(1.2) \\
Verb       & Adverb     & 53.0 &            85 &            21 & \textbf{78.8}(5.2) & \textbf{90.0}(2.4) & \textbf{67.7}(12.5) \\
Adjective  & Adverb     & 61.5 &            42 &            81 & \textbf{72.8}(0.2) & \textbf{57.3}(2.6) & \textbf{88.3}(3.1) \\
\bottomrule
\end{tabular}
\end{center}
\caption{LLMs' accuracies (\%) of distinguishing two syntactic categories in novel contexts.
We show the mean and the standard deviation (in the bracket) of 3 runs.
`Acc.\ (1$>$2)' denotes the accuracy on the set of sentences where Category 1 should be preferred over Category 2 (e.g., assigning a higher probability to a noun in a noun-expecting context for row 1), and vice versa. Column `Acc.' lists the aggregate accuracy. ``\mbox{Llama-3 8B} +\ac{metaicl-w}'' have accuracies significantly better than chance except distinguishing adjective from verb (row 5). Additionally, ``\mbox{Llama-3 8B} +\ac{metaicl-w}'' improves over \mbox{Llama-3 8B} baseline in differentiating most category pairs except discriminating nouns from adjectives (row 2), showing the effectiveness of finetuning with \ac{metaicl-w}.}
\label{tab:category-classification-llama}
\end{table*}

\begin{table*}[t]
\small
\begin{center}
\begin{tabular}{ll|ccc|ccc}
\toprule
\multicolumn{2}{c|}{} & \multicolumn{3}{c|}{\bf \ac{metaicl-w} from scratch on CHILDES} & \multicolumn{3}{c}{\bf \ac{metaicl-w} from scratch on BabyLM-10M} \\
\bf Cat.\ 1 & \bf Cat.\ 2 & \bf Acc. & \bf Acc.\ (1$>$2) & \bf Acc.\ (2$>$1) & \bf Acc. & \bf Acc.\ (1$>$2) & \bf Acc.\ (2$>$1) \\
\midrule
Noun      & Verb      & 84.5(2.3) &     79.7(3.7) &     89.3(4.5) & 93.5(1.8) &     90.0(2.2) &     97.0(1.4) \\
Noun      & Adjective & 73.5(0.4) &     50.7(2.9) &     96.3(2.1) & 86.2(2.5) &     79.7(5.4) &     92.7(1.9) \\
Noun      & Adverb    & 62.2(1.8) &     90.3(4.1) &     34.0(6.4) & 67.8(3.7) &     86.3(3.1) &     49.3(5.8) \\
Verb      & Adjective & 92.3(1.4) &     90.0(2.8) &     94.7(1.2) & 95.7(1.2) &     93.0(2.4) &     98.3(0.5) \\
Verb      & Adverb    & 38.5(6.5) &     57.3(14.7)&     19.7(1.7) & 56.7(5.3) &     68.7(5.8) &     44.7(11.4)\\
Adjective & Adverb    & 53.8(5.3) &     44.0(5.4) &     63.7(10.1)& 62.3(1.9) &     59.0(6.5) &     65.7(4.1) \\
\bottomrule
\end{tabular}
\end{center}
\caption{Accuracies (\%) of distinguishing two syntactic categories in novel contexts for models trained from scratch with \ac{metaicl-w}.
We show the mean and the standard deviation (in the bracket) of 3 runs.
`Acc.\ (1$>$2)' denotes the accuracy on the set of sentences where Category 1 should be preferred over Category 2 (e.g., assigning higher probability to a noun in a noun-expecting context for row 1), and vice versa. Column `Acc.' lists the aggregate accuracy. Both models perform better than chance on many category pairs, suggesting that models can develop some ability to one-shot learn the syntactic category of a word from human-scale data with \ac{metaicl-w}.}
\label{tab:category-classification-scratch}
\end{table*}


\clearpage
\section{Comparing Generations}
\label{app:comparing-generations}

For new usage example generation (Section~\ref{sec:example-generation}), we show \mbox{GPT-4o} the following text format:
\begin{displayquote}
The following lines are shuffled lowercased example sentences using a new word `dax', one per line:

* EXAMPLE-1

* EXAMPLE-2

* EXAMPLE-3

* EXAMPLE-4

Please answer in a single uppercase letter: Which of the following is a better next example for the word `dax', or they tie?

A) OPTION-A

B) OPTION-B

C) OPTION-C
\end{displayquote}
where OPTION-A, OPTION-B, OPTION-C are shuffled generation-1, generation-2, and ``Tie''.

For definition generation (Section~\ref{sec:definition-generation}), we do not have the examples (and the prompt before them) and instead have the direct prompt before the options: ``Please answer in a single uppercase letter: Which of the following is a better definition for the word `\textit{Word}', or they tie?'' where \textit{Word} is the ground-truth word form.

We always get the first letter (A, B, or C) of the \mbox{GPT-4o} response as the choice.

Tables~\ref{tab:generative-quantitative-compare-greedy}~and~\ref{tab:generative-quantitative-compare-top-p} show the results of comparing \mbox{Llama-3 8B} baseline (pre-trained to instruction-tuned) to the \ac{metaicl-w} model finetuned from that baseline (with random seed 0) on new examples and definitions generated by greedy decoding and top-p=$0.92$, respectively.

\begin{table}[h]
\small
\begin{center}
\resizebox{\linewidth}{!}{
\begin{tabular}{P{1.2cm}c|P{1.13cm}P{1.13cm}|P{1.13cm}}
\toprule
 & & \multicolumn{2}{c|}{\bf New Example} & {\bf Definition} \\
\bf Variant & \bf Method & \bf BabyLM-10M test & \bf Chimera & \bf CoLLEGe-DefGen \\
\midrule
\multirow{2}{=}{Pre-trained}
& baseline & 39 & 39 & 25 \\
& +\ac{metaicl-w} & \textbf{53} & \textbf{42} & \textbf{31} \\
\midrule
\multirow{2}{=}{Instruction-tuned}
& baseline & 46 & \textbf{52} & \textbf{33} \\
& +\ac{metaicl-w} & \textbf{47} & 36 & 28 \\
\bottomrule
\end{tabular}
}
\end{center}
\caption{Percentages of wins of each model when comparing the generations from \mbox{Llama-3 8B} baseline (pre-trained to instruction-tuned) with the \ac{metaicl-w} model finetuned from that baseline, judged by \mbox{GPT-4o}.
The left two datasets are for new usage example generation in Section~\ref{sec:example-generation}, and the right-most one is for definition generation in Section~\ref{sec:definition-generation}.
Each new example or definition is generated by top-p=$0.92$.
The percentage of ties is the remaining after subtracting the win percentages of the two models.
\mbox{GPT-4o} more frequently chooses the \ac{metaicl-w} model as the winner compared to the corresponding baseline, except for the instruction-tuned model on Chimera and CoLLEGe-DefGen.}
\label{tab:generative-quantitative-compare-top-p}
\end{table}


\iffalse

\begin{table}[t]
\small
\begin{center}
\begin{tabular}{c|cc}
\toprule
dataset         & human & GPT-4o \\
\midrule
BabyLM-10M test & 32:6  & 52:32 \\
Chimera         & 30:9  & 55:42 \\
\bottomrule
\end{tabular}
\end{center}
\caption{Quantitative evaluation results of comparing \mbox{Llama-3 8B} finetuned with \ac{metaicl-w} on BabyLM-10M to \mbox{Llama-3 8B} baseline. Each new example is generated by greedy decoding.
(Results of top-p sampled generations are shown in Table~\ref{tab:generation-quantitative-top-p} in Appendix~\ref{app:comparing-generations}.)
We show the percentage of wins vs.\ losses in the form $\text{win}:\text{loss}$ (percentage of ties is ($100-\text{win}-\text{loss}$)).
Due to the high cost of human evaluation, the human evaluates 50 meta-learned words sampled from the BabyLM-10M test portion.
Both the human and \mbox{GPT-4o} more frequently choose the \ac{metaicl-w} model as the winner compared to the baseline.}
\label{tab:generation-quantitative-greedy}
\end{table}

\begin{table}[t]
\small
\begin{center}
\begin{tabular}{c|cc}
\toprule
dataset         & human & GPT-4o \\
\midrule
BabyLM-10M test & 38:18 & 53:39 \\
Chimera         & 27:24 & 42:39 \\
\bottomrule
\end{tabular}
\end{center}
\caption{Quantitative evaluation results of comparing \mbox{Llama-3 8B} finetuned with \ac{metaicl-w} on BabyLM-10M to \mbox{Llama-3 8B} baseline. Each new example is generated by sampling with top-p=$0.92$.
We show the percentage of wins vs.\ losses in the form $\text{win}:\text{loss}$ (percentage of ties is ($100-\text{win}-\text{loss}$)).
Due to the high cost of human evaluation, the human evaluates 50 meta-learned words sampled from the BabyLM-10M test portion.
Both the human and \mbox{GPT-4o} more frequently choose the \ac{metaicl-w} model as the winner compared to the baseline.
}
\label{tab:generation-quantitative-top-p}
\end{table}

%To examine how the human and GPT-4o agree and disagree, Tables~\ref{tab:comparison-babylm-gpt-4o-human}~and~\ref{tab:comparison-chimera-gpt-4o-human} show the counts of generations with each pair of human-GPT-4o judgments on the BabyLM-10M test portion and the Chimera dataset, respectively. In general, GPT-4o still underestimates the improvement of the \ac{metaicl-w} model compared to the baseline, enhancing the conclusion regarding the effectiveness of our method.
\begin{table}[t]
\small
\begin{center}
\begin{tabular}{c|ccc}
\toprule
\backslashbox{human}{GPT-4o} & win & lose & tie \\
\midrule
win  & 26 &  7 &  2 \\
lose &  3 &  9 &  0 \\
tie  & 20 & 23 & 10 \\
\bottomrule
\end{tabular}
\end{center}
\caption{Comparison between judgments made by \mbox{GPT-4o} and the human on the BabyLM-10M test portion. The human evaluates 50 words, each has two generations: one generated by greedy decoding and one generated by top-p=$0.92$ sampling, resulting in 100 generations in total from each model.
By comparing the off-diagonal numbers, we know that when the human and \mbox{GPT-4o} disagree, \mbox{GPT-4o} tends to favor the baseline, which suggests that \mbox{GPT-4o} still underestimates the improvement brought by finetuning with \ac{metaicl-w} compared to the human.}
\label{tab:comparison-babylm-gpt-4o-human}
\end{table}

\begin{table}[t]
\small
\begin{center}
\begin{tabular}{c|ccc}
\toprule
\backslashbox{human}{GPT-4o} & win & lose & tie \\
\midrule
win  & 17 &  0 &  2 \\
lose &  1 &  8 &  2 \\
tie  & 14 & 19 &  3 \\
\bottomrule
\end{tabular}
\end{center}
\caption{Comparison between judgments made by \mbox{GPT-4o} and the human on the Chimera dataset. There are 33 chimera words, each has two generations: one generated by greedy decoding and one generated by top-p=$0.92$ sampling, resulting in 66 generations in total from each model.}
\label{tab:comparison-chimera-gpt-4o-human}
\end{table}
\fi


\clearpage
\section{Evaluation of Generated New Usage Examples}
\label{app:example-evaluation}

As we mentioned in Section~\ref{sec:example-generation}, we sample study examples from two datasets: the BabyLM-10M test portion and the Chimera dataset \citep{Lazaridou2017MultimodalWM}. Statistics of the BabyLM-10M test portion is in Table~\ref{tab:dataset-statistics}, Appendix~\ref{app:dataset}. The Chimera dataset contains 33 chimeras. For each chimera, we merge all examples in its 10 ``passages'' (each with a different pseudo-word in place of the word) in the original dataset, so we only have 33 different words to learn. We do not use the pseudo-words in the original dataset.

Tables~\ref{tab:babylm-generation}~and~\ref{tab:chimera-generation} show samples of example sentences from the BabyLM-10M test portion and the Chimera dataset along with the new usage examples generated by both the \mbox{Llama-3 8B} baseline and the \ac{metaicl-w} model finetuned from that baseline. Table~\ref{tab:babylm-generation-error} contains error analysis of new examples generated from the BabyLM-10M test portion, extending Table~\ref{tab:babylm-generation-brief} in Section~\ref{sec:example-generation}.

\begin{table*}[t]
\small
\begin{center}
\begin{tabular}{p{5.3cm}p{2.3cm}p{2.3cm}p{3cm}p{0.8cm}}
\toprule
\bf Study Example Sentences & \bf \ac{metaicl-w} & \bf Baseline & \bf Judgment and Reason & \bf Word
%\\
%\midrule
%$\bullet$ \texttt{[new-token]} to rogard, missile armed and ready.
%$\bullet$ the \texttt{[new-token]}, which began its course in this period, crawls on the bottom of our present seas.
%$\bullet$ this is \texttt{[new-token]} standing by.
%$\bullet$ so far in all their various forms, whether straight, as the orthoceras, or curved, or close - coiled as in the \texttt{[new-token]}, the septum, or partition dividing the chambers, met the inner shell along a simple line, like that of the rim of a saucer.
%& the \texttt{[new-token]} will put our strike force out of action.
%& hence \texttt{[new-token]}, however wrong in its facts, has been regarded as the true family tree of the genera.
%& \ac{metaicl-w} model wins. The word can be a weapon, an animal in the sea, or a structure. Finetuned output means the weapon, while the baseline output means a family tree, which is unreasonable.
%& nautilus
\\
\midrule
$\bullet$ tales similar to " bluebeard " include " the fitcher's bird " by the brothers \texttt{[new-token]} and the english tale " mr. fox ".
$\bullet$ these types of tales include royalty and fantastical magic, many of which are published by brothers \texttt{[new-token]} and hans christian anderson.
$\bullet$ previously on \texttt{[new-token]}...
$\bullet$ the house in the wood from the german of \texttt{[new-token]}.
& previously in \texttt{[new-token]}
& the story is actually entitled " the bridal of trier " as mentioned on brothers \texttt{[new-token]} site.
& Baseline wins. \ac{metaicl-w} finetuned output simply changes the preposition of an example, which is likely incorrect. The baseline output copies ``brothers \texttt{[new-token]}'', which is likely correct.
& grimm
\\
\midrule
$\bullet$ men were sent to search the \texttt{[new-token]}, and there, on the very night before the deed was to be done, guy fawkes was found waiting till the time should come to set fire to the powder.
$\bullet$ the actual \texttt{[new-token]}   they've got some of the old er   writings that the, the monks   and you're not allowed to take photographs.
$\bullet$ searched all the tunnels and \texttt{[new-token]}.
$\bullet$ he puts them in \texttt{[new-token]}, anderson shelters, confined spaces, things like that.
& because they can light it in a \texttt{[new-token]} and blow it with the minimum amount of people.
& when a young teacher of history at the university of the commonwealth in london, \texttt{[new-token]} began the revolutionary research into indian philosophy of patanjali that was later to prove so fruitful.
& \ac{metaicl-w} model wins. The word likely refers to a place or space. Only the \ac{metaicl-w} finetuned output uses the word as a place.
& cellars
\\
\midrule
$\bullet$ the sorcerer wotan is using the \texttt{[new-token]} of aten to blot out the sun.
$\bullet$ unless it involves some kind of enchanted \texttt{[new-token]},
$\bullet$ i'll give her the lucky \texttt{[new-token]} you gave m
$\bullet$ this can be a long ritual, or it can take the form of simply wearing an \texttt{[new-token]} or other token, " against bad luck ".
& first, she goes with colm and feels a little guilty for keeping his \texttt{[new-token]}.
& the narrative treatment of \texttt{[new-token]} in the media exemplifies this process.
& \ac{metaicl-w} model wins. The word refers to a ritual token. The \ac{metaicl-w} finetuned output suggests it is an object, which could agree with the inference, while the baseline output does not convey a lot of information.
& amulet
\\
\midrule
$\bullet$ they are blind, their eyes having become reduced to \texttt{[new-token]} lenses under the skin, and they have no external ears, just a pair of tiny holes hidden under thick hair.
$\bullet$ it is a \texttt{[new-token]} reflex, a remnant of the past when pre - human babies clung to their mothers ' hair as the mothers swung through the trees.
$\bullet$ however, \texttt{[new-token]} structures may have their original function replaced with another.
$\bullet$ strong evidence for common descent comes from \texttt{[new-token]} structures.p397 the useless wings of flightless beetles are sealed under fused wing covers.
& the \texttt{[new-token]} structures of the human body are the appendix, the coccyx, the earlobes, the tailbone, and the wisdom teeth.
& the \texttt{[new-token]} structures of the human body are the same as those of other mammals.
& Tie. The word should be an adjective meaning an inherited biological feature that lost its original function. Both generations fit this interpretation. \mbox{GPT-4o} decides the \ac{metaicl-w} model as the winner.
& vestigial
\\
\midrule
$\bullet$ upon the very margin of the ditch, not thirty feet from where they crouched, an iron caldron \texttt{[new-token]} and steamed above a glowing fire ;
$\bullet$ all round the hull, in the blackness, the rippling current \texttt{[new-token]} and chattered like a little mountain stream.
$\bullet$ there hung another kettle that \texttt{[new-token]} and boiled ; but there was no fire beneath it, either.
$\bullet$ and when he lifted it up and looked down, there stood a great copper cauldron which \texttt{[new-token]} up and boiled away down there ;
& the boy looked down into the pot and saw that it \texttt{[new-token]} and boiled away.
& he turned his back upon them, and went in at the door in the \texttt{[new-token]}, which was a pleasant door to look upon.
& \ac{metaicl-w} model wins. The word should be a past-tense verb meaning a physical movement of an object, likely caused by steam or current. The \ac{metaicl-w} finetuned output is consistent with this meaning, but the baseline incorrectly uses the word as a noun.
& bubbled
\\
\bottomrule
\end{tabular}
\end{center}
\caption{New examples generated on study examples from the BabyLM-10M test portion by the \ac{metaicl-w} model and the \mbox{Llama-3 8B} baseline it is finetuned from, with greedy decoding.
In addition to \mbox{GPT-4o}'s judgment, the first author manually compared each pair of generations and gave the judgment and reason (Judgment and Reason).
When the author and \mbox{GPT-4o} compare the generations, they cannot see the ground-truth word but have to infer the possible meaning of the word.
When \mbox{GPT-4o}'s judgment disagrees with the author's, it is mentioned at the end of Judgment and Reason.
}
\label{tab:babylm-generation}
\end{table*}


\begin{table*}[t]
\small
\begin{center}
\begin{tabular}{p{5.5cm}p{2cm}p{2cm}p{3cm}p{1.3cm}}
\toprule
\bf Study Example Sentences & \bf \ac{metaicl-w} & \bf Baseline & \bf Judgment and Reason & \bf Word \\
\midrule
$\bullet$ Animals such as capybara, jaguars, jacare (\texttt{[new-token]}s) and hyacinth macaws are particularly vulnerable.
$\bullet$ Nadirpur stared at it as though it were a  \texttt{[new-token]}, his face quite drained.
$\bullet$ The owner will be summonsed to court for illegal possession of a \texttt{[new-token]} without a permit.
$\bullet$ I went to the cavern to get some, and found a \texttt{[new-token]} in there.
& $\bullet$ The \texttt{[new-token]} was a little more than a metre long, and had a thick, black coat.
& $\bullet$ The \texttt{[new-token]} was a large, black, furry creature with a long, pointed snout.
& \ac{metaicl-w} model wins. An alligator (inferred from the first example) is not furry.
& \mbox{alligator} + \mbox{rattlesnake}
\\
\midrule
$\bullet$ He is principal \texttt{[new-token]} of the English Baroque soloists, with whom he has recorded for Erato and Deutsche Grammophon.
$\bullet$ Banners point to the military aspect of \texttt{[new-token]} music: a Jacobite flag and the first Union flag.
$\bullet$ " Alexander Baillie said that he was impressed by her playing and even tried out her \texttt{[new-token]}!
$\bullet$ Where did the \texttt{[new-token]} idea at the end of Rubina's Blue Sky Happiness come from?
& $\bullet$ The first movement is a \texttt{[new-token]}, the second a minuet, and the third a rondo.
& $\bullet$ The \texttt{[new-token]} is a small, furry, four-legged creature with a long tail.
& Tie. Both generations are not for an instrument.
& \mbox{cello} + \mbox{bagpipe}
\\
\midrule
$\bullet$ 4. Allow the stock to settle then skim off the fat using \texttt{[new-token]} or fat separator.
$\bullet$ Add the pasta, cook it for about 12 minutes until just soft, then drain it in a \texttt{[new-token]}.
$\bullet$ Imagine you've got a bucketful of praise in every room of the house with a big \texttt{[new-token]}.
$\bullet$ Plants can be grown in tyres, old metal pots such as buckets, watering cans or even a \texttt{[new-token]}!
& $\bullet$ 2. Put the chicken in a \texttt{[new-token]} and cover with cold water.
& $\bullet$ The \texttt{[new-token]} is a very useful tool for the gardener.
& \ac{metaicl-w} model wins. The word refers to a kitchen utensil, likely a container. The baseline may be misled by the last example but do not notice the word ``even'' indicating the surprise of the usage.
& \mbox{ladle} + \mbox{colander}
\\
\midrule
$\bullet$ At about the same age I was in a cafe and I spilt some pop on my \texttt{[new-token]}s.
$\bullet$ It's also the first International home \texttt{[new-token]} in history that is reversible as well.
$\bullet$ Such as how if you have sufficiently-stretchy pants you could take them off without first removing your \texttt{[new-token]}s.
$\bullet$ It was in a cinema on 86th Street where he sat in the balcony, wearing an old sports jacket and open-necked \texttt{[new-token]}.
& $\bullet$ The \texttt{[new-token]} is a bit tight.
& $\bullet$ I was in a cafe and I spilt some pop on my \texttt{[new-token]}s.
& Tie. A word for clothes can be used in both example. \mbox{GPT-4o} decides the baseline as the winner.
& \mbox{trouser} + \mbox{shirt} \\
\bottomrule
\end{tabular}
\end{center}
\caption{New examples generated on study examples from the Chimera dataset \citep{Lazaridou2017MultimodalWM} by the \ac{metaicl-w} model and the \mbox{Llama-3 8B} baseline it is finetuned from, with greedy decoding.
Like in Table~\ref{tab:babylm-generation}, the first author provides the judgment and reason (Judgment and Reason).
The first example is a reasonable guess of the appearance of an animal.
The second example is related to music, but incorrectly infers the word to be a dance or musical form.
The last example correctly infers that the word is a kitchen utensil.
}
\label{tab:chimera-generation}
\end{table*}


\begin{table*}[t]
\small
\begin{center}
\begin{tabular}{p{4cm}p{2.5cm}p{2.5cm}p{0.8cm}p{4cm}}
\toprule
\textbf{Study Example Sentences} & \textbf{\ac{metaicl-w}} & \textbf{Baseline} & \textbf{Word} & \textbf{Analysis} \\
\midrule
$\bullet$ the first blacksmiths were \texttt{[new-token]}.
$\bullet$ many civilisations were in the area that is now turkey, like the \texttt{[new-token]}, the roman empire and the byzantine empire.
$\bullet$ spread of hepatoscopy and astrology to \texttt{[new-token]}, etruscans, greeks and romans and to china
$\bullet$ the first major empire in the area was the \texttt{[new-token]} (from the 18th century to the 13th century bce).
&
1. the \texttt{[new-token]} were a people who lived in the area of turkey.
2. perhaps the most famous and widely used alchemical symbol, first popularized by \texttt{[new-token]} alchemists, is the ouroboros.
&
1. the \texttt{[new-token]} was a major power in the region, and its influence spread to the surrounding areas.
2. epistemology will suffer the \texttt{[new-token]} syndrome.
& hittites
&
The \ac{metaicl-w} model learns that \emph{hitties} is an ancient ethnic group. However, the greedy-decoded example copies the information (turkey) from the study example, while the sampled example makes seemingly plausible but factually incorrect generalizations (the earliest known ouroboros is found in ancient Egyptian text.)
The baseline also generates factual errors (Hittite plague exists but no ``hitties syndrome'').
\\
\midrule
$\bullet$ but, i know from his childhood, he says that his father always buys and \texttt{[new-token]} cars just as entertainment, i guess.
$\bullet$ in the end, yuji accepts shana's feelings and the two kiss, which activates a spell left to yuji by crimson lord lamia that \texttt{[new-token]} his existence, so yuji is no longer a torch.
$\bullet$ -- a medicine which \texttt{[new-token]} health and energy, as extract of malt, cod liver oil, etc.
$\bullet$ that diversifies energy supply, \texttt{[new-token]} ecosystems, protects biodiversity, and sequesters carbon.
&
1. the \texttt{[new-token]} of the \texttt{[new-token]} of the \texttt{[new-token]} of \textbf{[Infinite Repetition]}
2. he is a former prime minister, who is then \texttt{[new-token]} of his duties.
&
1. the \texttt{[new-token]} is a 3.5 mm jack, which is the standard for most headphones.
2. he \texttt{[new-token]} up to an incredible 2,620 sales per store in 2009, compared with the 60 to 80 that had been typical just a few years earlier.
& restores
&
For the \ac{metaicl-w} model, the greedy-decoded example is nonsense repetition (a common failure of LMs), while the sampled example uses the verb with incorrect syntax (``be restored to'') but pertinent semantics.
For the baseline, the greedy-decoded example also has incorrect syntax (use the word as a noun), and the use in the sampled example is semantically incorrect (the sales must have reached a similar level in the past).
\\
\midrule
$\bullet$ when i was a student \texttt{[new-token]}, i went to the prado in Madrid.
$\bullet$ \texttt{[new-token]} is hiking in the outdoors with a backpack full of all your food, clothing, water, and shelter.
$\bullet$ \texttt{[new-token]} may be done on short or long trips.
$\bullet$ \texttt{[new-token]} (wilderness)
&
1. \texttt{[new-token]} is a form of wilderness travel.
2. a small \texttt{[new-token]} campsite is usually composed of a place to sleep, food, tools and water.
&
1. \texttt{[new-token]} is a type of backpacking.
2. \texttt{[new-token]} is something all people can do.
& back-packing
&
The \ac{metaicl-w} model seems to infer the meaning of the word, but is rewording the study examples.
The baseline also seems to infer the meaning, but its sampled example is not very informative.
\\
\bottomrule
\end{tabular}
\end{center}
\caption{Error analysis of new examples generated from the BabyLM-10M test portion by the \ac{metaicl-w} model and the baseline. In each column of generated examples, the first one is generated by greedy decoding, and the second one by sampling with top-p=$0.92$.
}
\label{tab:babylm-generation-error}
\end{table*}


\clearpage
\section{Evaluation of Generated Definitions}
\label{app:definition-evaluation}

As we mentioned in Section~\ref{sec:definition-generation}, we use two definition generation datasets: CoLLEGe-DefGen \citep{Teehan2024CoLLEGeCE} and the Oxford test set \citep{gadetsky-etal-2018-conditional}.
The original datasets contain 954 and 12,232 words, from which we removed 4 and 2 duplicated words, respectively.
For CoLLEGe-DefGen, we keep the inflectional suffixes, such as ``-s'', ``-ed'', and ``-ly'', after the placeholder so that the placeholder only corresponds to the word stem. This is to remove the influence of morphological inflections.
Note that we use our placeholders instead of the \texttt{<nonce>} in the original text of CoLLEGe-DefGen.
In addition, we fixed several incorrect word/phrase replacements in the original dataset (for example, the phrase ``\emph{capital gains tax}'').
For the Oxford dataset, for simplicity and consistency with previous work, we do not keep the inflectional suffixes but rather replace the whole word with the placeholder.
There are 12\% examples in the Oxford test set in which we find no occurrences of any form of the word to be learned, but we keep them for consistency with previous work.

Additionally, as we also mentioned in Section~\ref{sec:definition-generation}, we have additional references of what can be achieved by specialized definition-generation models: the series of \mbox{FLAN-T5} \citep{chung2024scaling} models finetuned by \citet{giulianelli-etal-2023-interpretable} specifically on generating definitions.
This also follows what \citet{Teehan2024CoLLEGeCE} did.
These models were finetuned on three corpora, including the Oxford training set \citep{gadetsky-etal-2018-conditional}.
The series of finetuned \mbox{FLAN-T5} are listed on their GitHub page (\url{https://github.com/ltgoslo/definition_modeling?tab=readme-ov-file#definition-generation-models-for-english}) and can be accessed through Hugging Face model hub.
When evaluating the \mbox{FLAN-T5} models, a pseudo-word `\textit{wug}' is used as the placeholder for the new word, like in other baselines (Section~\ref{sec:baseline}) for a fair comparison. Each \mbox{FLAN-T5} model is prompted with an example sentence followed by a question, ``What is the definition of wug?'', as what \citet{giulianelli-etal-2023-interpretable} did.

Table~\ref{tab:definition-quantitative-1-shot} shows the results of comparing the model-generated and ground-truth definitions from all models, supplementing the brief results in Table~\ref{tab:definition-quantitative-1-shot-brief} with those from the additional specialized \mbox{FLAN-T5} baselines and the CoLLEGe model.
Table~\ref{tab:definition-quantitative-defgen} shows the average of 1-, 2-, and 3-shot results on the CoLLEGe-DefGen dataset.
Tables~\ref{tab:defgen-definition-more}~and~\ref{tab:oxford-definition} show additional definitions generated from the CoLLEGe-DefGen and Oxford test set by the baselines and the \ac{metaicl-w} models (in addition to Table~\ref{tab:defgen-definition} in Section~\ref{sec:definition-generation}).

Results of CoLLEGe \citep{Teehan2024CoLLEGeCE}, which generates new embeddings to be used in an LLM, are appended to Table~\ref{tab:definition-quantitative-1-shot}~and~\ref{tab:definition-quantitative-defgen} and are directly copied from the original paper. Those numbers should be compared with other models with caution because they have different settings: They are based on Llama-2 7B \citep{Llama-2} and new embeddings, their data processing is not as fine as ours (they did not remove duplicated words from both datasets, did not keep the inflectional suffixes in CoLLEGe-DefGen, and did not find more forms in the Oxford dataset as we do (15\% examples without replacing any word form)), and the usage examples they randomly selected from CoLLEGe-DefGen are different from ours.

\begin{table*}[t]
\small
\begin{center}
\begin{tabular}{p{2.7cm}l|cccc}
\toprule
\multicolumn{2}{c|}{\bf Model} & \multicolumn{2}{c}{\bf CoLLEGe-DefGen} & \multicolumn{2}{c}{\bf Oxford} \\
\bf Variant & \bf Method & \bf BERTScore F1 & \bf ROUGE-L & \bf BERTScore F1 & \bf ROUGE-L \\
\midrule
FLAN-T5 Base  &+DefInstr baseline          & 83.1 & 13.1 & 84.4 & 16.5 \\
FLAN-T5 Large &+DefInstr baseline          & \textbf{83.8} & \textbf{15.5} & 84.7 & 17.4 \\
FLAN-T5 XL    &+DefInstr baseline          & 83.1 & 12.4 & \textbf{84.9} & \textbf{19.4} \\
\midrule\midrule
\multirow{2}{=}{Llama-3 8B}
           &             baseline          & 85.1 & 14.9 & 83.2 & 11.0 \\
           & +\ac{metaicl-w}               & 85.4 & 18.7 & \textbf{84.7} & 16.3 \\
\midrule
\multirow{2}{=}{Llama-3 8B Instruct}
                   &     baseline          & 85.3 & 17.6 & 83.6 & 12.5 \\
                   &+\ac{metaicl-w}        & \textbf{85.8} & \textbf{20.7} & \textbf{84.7} & 16.5 \\
\midrule\midrule
Llama-2 7B         & CoLLEGe*              & 84.1 & 18.0 & 83.6 & \textbf{17.1} \\
\bottomrule
\end{tabular}
\end{center}
\caption{Quantitative evaluation of generated definitions by comparing them with ground-truth definitions.
This table extends Table~\ref{tab:definition-quantitative-1-shot-brief} by presenting results from all models we evaluate, including the additional specialized \mbox{FLAN-T5} baselines from \citet{giulianelli-etal-2023-interpretable} and the CoLLEGe model from \citet{Teehan2024CoLLEGeCE}.
We generate a definition from only one example (1-shot). We sample an example per word from CoLLEGe-DefGen, while Oxford has exactly one example per word.
All definitions are generated with greedy decoding.
``+DefInstr'' means the definition generation finetuning by \citet{giulianelli-etal-2023-interpretable}.
``baseline'' means using a pseudo-word `\textit{wug}' as the placeholder word.
For \ac{metaicl-w} models (``+\ac{metaicl-w}''), scores are averaged across 3 runs.
The instruction-tuned variant of \mbox{Llama-3 8B} (``\mbox{Llama-3 8B} Instruct'') is better than the pre-trained variant (``\mbox{Llama-3 8B}'') on definition generation likely due to its better instruction-following ability.
*: CoLLEGe results are from ``Prompting + CoLLEGe'' in the Table~4 of \citet{Teehan2024CoLLEGeCE}, which provides \mbox{Llama-2} 7B \citep{Llama-2} with embeddings generated by CoLLEGe and prompt it to generate definitions with in-context usage examples. \citet{Teehan2024CoLLEGeCE} has slightly different data processing, so CoLLEGe results are not strictly comparable (see Appendix~\ref{app:definition-evaluation}).
}
\label{tab:definition-quantitative-1-shot}
\end{table*}

\begin{table*}[t]
\small
\begin{center}
\begin{tabular}{p{2.7cm}l|cc}
\toprule
\multicolumn{2}{c|}{\bf Model} & \multicolumn{2}{c}{\bf CoLLEGe-DefGen} \\
\bf Variant & \bf Method & \bf BERTScore F1 & \bf ROUGE-L \\
\midrule
\multirow{2}{=}{Llama-3 8B}
& baseline                 & 85.8 & 17.8 \\
& +\ac{metaicl-w}          & 85.9 & 21.1 \\
\midrule
\multirow{2}{=}{Llama-3 8B Instruct}
& baseline                 & 85.9 & 19.5 \\
&+\ac{metaicl-w}           & \textbf{86.2} & \textbf{22.6} \\
\midrule
Llama-2 7B & CoLLEGe*      & 84.8 & 17.8 \\
\bottomrule
\end{tabular}
\end{center}
\caption{Quantitative evaluation of generated definitions by comparing them with ground-truth definitions in the CoLLEGe-DefGen dataset. Definitions are generated 1-, 2-, and 3-shot and scores are averaged. All definitions are generated with greedy decoding. For models finetuned with \ac{metaicl-w}, scores are averaged across 3 runs.
*: CoLLEGe results are from \citet{Teehan2024CoLLEGeCE}, which is based on Llama-2 7B and slightly different data processing (see Appendix~\ref{app:definition-evaluation}). We do not have \mbox{FLAN-T5} models here since \citet{giulianelli-etal-2023-interpretable} finetuned them to use only one usage example.}
\label{tab:definition-quantitative-defgen}
\end{table*}


\begin{table*}[t]
\small
\begin{center}
\begin{tabular}{p{3.15cm}p{3cm}p{3.15cm}p{3.15cm}p{1.4cm}}
\toprule
\bf Example Sentence & \bf True Definition & \bf \ac{metaicl-w} & \bf Baseline & \bf Word \\
\midrule
\multirow{2}{=}{As the hurricane neared, the residents began to \texttt{[new-token]} their windows to protect their homes from the impending storm.}
& \multirow{2}{=}{to cover or seal windows, doors, or other openings of a building with boards, typically to protect it from damage or unauthorized entry.}
& to protect from harm or danger
& \textbf{to prepare for a hurricane by boarding up windows}
& board up
\\
\cmidrule{3-4}
&
& to make something more secure or safe by covering it with a layer of material
& \textbf{to secure or fasten something, especially a window, to prevent it from being damaged or destroyed}
\\
\midrule
\multirow{2}{=}{The gentle hum of the air conditioner provided a \texttt{[new-token]} soundtrack for her midday nap.}
& \multirow{2}{=}{having a calming or relieving effect, especially in terms of reducing pain or discomfort.}
& \textbf{a sound that is not loud enough to be heard}
& a small, furry, brown creature that lives in trees.
& soothing
\\
\cmidrule{3-4}
&
& \textbf{a soothing, calming, or quiet sound}
& a wug is a word that is not yet known to the speaker, but is assumed to be a real word.
\\
\midrule
\multirow{2}{=}{In their groundbreaking research, the team of geneticists successfully deactivated the \texttt{[new-token]}, resulting in the unexpected bloom of dormant traits within the lab mice.}
& \multirow{2}{=}{a type of gene that codes for a protein, known as a repressor, which inhibits the expression of one or more genes by binding to the operator or associated silencers.}
\vspace{0.5cm}
& \textbf{a gene that is turned off in a cell}
& a hypothetical new word that does not yet exist in the English language.
& repressor gene
\\
\cmidrule{3-4}
&
& \textbf{a gene or set of genes that controls the development of a particular trait or characteristic}
& a hypothetical word used in linguistic research to test the ability to form and use new words.
\\
\midrule
\multirow{2}{=}{She preferred the \texttt{[new-token]} wilderness to the stifling orderliness of city life.}
& \multirow{2}{=}{not restricted or limited; free; unconfined.}
& not having a definite shape or form
& a small, furry animal
& untrammeled
\\
\cmidrule{3-4}
&
& a place where there are many trees, especially in a forest or a park
& \textbf{a mythical creature that is half-wolf and half-bear}
\\
\midrule
\multirow{2}{=}{In the heart of her rustic kitchen, Grandma carefully seasoned the \texttt{[new-token]}, her secret ingredient for the family's cherished Sunday stew.}
& \multirow{2}{=}{The chest portion of a young cow, typically used in cooking for its tender meat.}
& \textbf{a mixture of herbs and spices used to flavor food}
& a mythical creature that resembles a cross between a dog and a frog.
& breast of veal
\\
\cmidrule{3-4}
&
& \textbf{a small, usually round, piece of food, especially a piece of meat or a vegetable, cut off from a larger piece and cooked separately}
& a type of meat
\\
\midrule
\multirow{2}{=}{The contractor recommended \texttt{[new-token]} for insulation due to its excellent thermal resistance and fireproofing properties.}
& \multirow{2}{=}{a type of insulation material made from melted rock or industrial waste that is spun into a fibrous structure. It is used for thermal insulation, fire protection, and soundproofing in both residential and commercial buildings.}
\vspace{1.95cm}
& \textbf{a material used to insulate a building}
& a unit of insulation used in construction
& mineral wool
\\
\cmidrule{3-4}
&
& \textbf{a material used to prevent heat transfer, especially in buildings}
& a type of insulation material
\\
\bottomrule
\end{tabular}
\end{center}
\caption{Definitions generated by both the pre-trained and instruction-tuned variant of \mbox{Llama-3 8B} (baselines) and the models finetuned from them with \ac{metaicl-w} on BabyLM-10M with greedy decoding, using the prompt ``The word \texttt{[new-token]} in the above sentence(s) is defined as "'' (so we can extract continuations before the closing quote as the generated definitions).
Each definition is generated using the single example sentence shown and provided in context.
The pre-trained/instruction-tuned variant is shown at the top/bottom of each row of example.
We boldface the winner judged by \mbox{GPT-4o}.
You may observe the quality of instruction-tuned variant is better than the pre-trained variant by manually comparing their definition in each example.}
\label{tab:defgen-definition-more}
\end{table*}


\begin{table*}[t]
\small
\begin{center}
\begin{tabular}{p{2.6cm}p{2.1cm}p{2.6cm}p{2.1cm}p{2.7cm}p{1.2cm}}
\toprule
\bf Example Sentence & \bf True Definition & \bf \ac{metaicl-w} & \bf Baseline & \bf FLAN-XL-DefInstr & \bf Word \\
\midrule
many people in the world have to \texttt{[new-token]} on \$ 1 a day
& support oneself
& to live on something, especially food, in a way that is not very good or healthy
& to struggle to make ends meet
& live on
& subsist
\\
\midrule
this food \texttt{[new-token]} the patient
& restore strength
& to make something more intense or strong
& a type of food
& to give strength
& revitalizes
\\
\midrule
he is always well-\texttt{[new-token]}
& care for one's external appearance
& well - dressed
& a type of wug
& in good health
& groomed
\\
\midrule
the lawyers tried to \texttt{[new-token]} the credibility of the witnesses
& challenge the honesty or veracity of
& to make something more convincing or believable
& to question the credibility of a witness
& to challenge the honesty or veracity of
& impeach
\\
\midrule
the car squeaks to a halt and she glares at him because of his \texttt{[new-token]} stop.
& characterized by abrupt stops and starts
& a sudden, sharp, high - pitched sound, especially one made by a car's brakes or a bird's call
& a made-up word
& a jerk that causes an object to move abruptly
& jerky
\\
\midrule
try the full plate pork \texttt{[new-token]} : tender pork, oregano-spiked greek salad, warm puffy pita, rice, and aromatic tzatziki-topped lemon potatoes.
& a greek dish of pieces of meat grilled on a skewer
& a dish of meat, usually pork, served with a sweet and sour sauce, and often served with rice and vegetables
& a type of dish that is a combination of pork, rice, and potatoes, typically served with a side of salad and pita bread.
& a greek dish of grilled meat served in a pita .
& souvlaki
\\
\midrule
extend the tv antenna \textbf{(word is absent)}
& extend or stretch out to a greater or the full length
& a small, usually round, piece of metal or plastic used to connect two wires together
& a type of bird
& raise or extend vertically
& stretch
\\
\midrule
the red light gave the central figure increased emphasis \textbf{(word is absent)}
& special importance or significance
& a red light
& a wug is a wug
& special importance or significance
& accent
\\
\bottomrule
\end{tabular}
\end{center}
\caption{Definitions generated by the instruction-tuned variant of \mbox{Llama-3 8B} (baseline), the \ac{metaicl-w} model finetuned from it with greedy decoding, and \mbox{FLAN-XL-DefInstr} (i.e., \mbox{FLAN-T5} XL +DefInstr baseline), using the prompt ``The word \texttt{[new-token]} in the above sentence(s) is defined as "'' (\texttt{[new-token]} can be replaced by other placeholders, as we mentioned in Section~\ref{sec:definition-generation}).
Each definition is generated using the single example sentence shown and provided in context.
The \ac{metaicl-w} model generates reasonable definitions given the context, but is often much longer than the ground-truth definitions, likely because it is not fitted to this dataset.
The baseline model is often generating low-quality or repetitive definitions, and sometimes sticks to its prior knowledge of the pseudo-word ``\emph{wug}.''
FLAN-XL-DefInstr generates definitions pretty close to the ground-truth, but is sometimes suspicious of overfitting to or memorizing the data, as its definition for `impeach' and `accent' (absent in the example) may suggest.}
\label{tab:oxford-definition}
\end{table*}


\clearpage\clearpage
\section{Concepts of ``Word''}
\label{app:word}
The term ``word'' can refer to linguistic units with nuanced variations.
Here, we describe the concepts of ``word'' in different contexts of the paper and their implications.
Surprisingly, our models are somehow robust to these variations of ``word,'' though future work may further improve the processing of words.

\paragraph{Word usage datasets}
In the two datasets we constructed for training and finetuning (Section~\ref{sec:dataset} and Appendix~\ref{app:dataset}), a ``word'' means a word-form, which is instantiated as an individual token extracted from the word-level tokenization (using spaces and punctuations as boundaries).
Therefore, for the same lexeme, a sentence using one of its word-form is not considered an example of another word-form. For instance, a sentence using other inflected forms of ``\emph{ski}'' like ``\emph{Susie likes skiing fast down the snowy mountain on her new skis}'' is not included in the example set of ``\emph{ski}.''
Meanwhile, when two word-forms of the same lexeme occur in one sentence, meta-learning one of the word-form could be easier since the other word-form may not be masked. For instance, ``\emph{skis}'' in the sentence ``\emph{I saw Susie ski fast down the snowy mountain on her new skis}'' could make it easier to guess the word ``\emph{ski}.''
In our work, we focus on learning word-forms, but if we aim to learn a lexeme, this case will reveal the identity of the lexeme we try to mask, undermining our effort on the novelty of the learned word.
On the other hand, a word-form in different syntactic categories is considered the same word, and the usage examples will be mixed together regardless of the syntactic categories. Such words are rare, but they introduce syntactic uncertainties in word learning. Syntactic uncertainties are natural, but may increase the difficulty of learning.

\paragraph{Pseudo-words}
In our baselines (Section~\ref{sec:baseline} and the \mbox{FLAN-T5} models in Section~\ref{sec:definition-generation}) and comparison of generations (Appendix~\ref{app:comparing-generations}), we replace the word to learn by a pseudo-word, like ``\emph{dax}'' or ``\emph{wug}'', regardless of the word's syntactic category and other aspects of meaning.
The pseudo-word is then tokenized, usually by a subword tokenizer for LLMs (thus may have multiple tokens).
We choose the pseudo-word to be meaningless and commonly used in linguistic tests.
However, a pre-trained LLM like \mbox{Llama-3} may have priors of certain aspects of the pseudo-word's meaning based on its form.
One aspect of the meaning is syntax.
For example, from the sentence ``\emph{Susie goes skiing in the winter}'', we replace ``\emph{skiing}'' with ``\emph{dax}'' and have the sentence ``\emph{Susie goes dax in the winter}.'' The sentence has a problem: the part of speech of ``\emph{skiing}'' is gerund, but ``\emph{dax}'' does not look like a gerund (since it does not end in ``\emph{-ing}''). So the sentence could mislead an LLM like \mbox{Llama-3}, which can use morphological information from its subword tokenization.
Another aspect of the meaning is semantics.
For example, in Table~\ref{tab:oxford-definition}, the baseline model sometimes sticks to its prior knowledge of the pseudo-word ``\emph{wug},'' as reflected in its generated definitions like ``\emph{a made-up word}'' and ``\emph{a type of bird}'' (``\emph{wug}'' referred to a bird-like creature in the Wug Test of \citealp{Berko1958TheCL}).
We admit that this problem may weaken our baselines and comparison of generations.
Future work should use more suitable pseudo-words, preserving the morphological inflections while removing the semantic information.

\paragraph{Evaluation datasets}
Words to be learned in the Chimera, CoLLEGe-DefGen, and Oxford datasets are lexemes, so examples of each word use (different) inflected word-forms.
To ensure the placeholder consistently represents the same text, we replace only the word stem with the placeholder and retain the inflectional suffixes in the original word-forms on the Chimera and CoLLEGe-DefGen datasets. (We still replace word-forms in Oxford to make our practice consistent with previous ones.)
In addition, words to be learned in the CoLLEGe-DefGen dataset also include multiwords or phrases, like the ``\emph{categorical imperative}'' example in Table~\ref{tab:defgen-definition}.
See Appendix~\ref{app:definition-evaluation} for further details of preprocessing.
Surprisingly, although our placeholder token represents a word-form in the \mbox{BabyLM-10M} dataset we constructed, \ac{metaicl-w} models finetuned on \mbox{BabyLM-10M} still perform well when using the token to represent a word stem in these datasets.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\end{document}