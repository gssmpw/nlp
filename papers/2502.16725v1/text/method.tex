\section{Method}
\subsection{Overview}
Here, we present \emph{\textbf{D}iffusion-based \textbf{O}ut-of-distribution detection on $\mathbb{SE}(3)$}, $\mathbf{DOSE3}$. $\mathbf{DOSE3}$ introduces a \emph{unified} diffusion model for rigid pose trajectories, specifically designed to accommodate the $\mathbb{SE}(3)$ manifold structure.
We first detail $\mathbf{DOSE3}$'s model architecture for handling ordered sequences. We then introduce \emph{$\mathbb{SE}(3)$ Denoising Diffusion Probabilistic Models} ($\mathbb{SE}(3)$ - DDPM), outlining their training and inference algorithms that incorporate rigid pose structure into the diffusion model. Finally, we explain how to utilize the \emph{diffusion estimator}, a function naturally emerging from $\mathbb{SE}(3)$ - DDPM, to develop an OOD detection statistic for evaluating test samples.
\subsection{Architectural details of DOSE3}
The \textbf{UNet} architecture, widely adopted in diffusion models for its effective encoder-decoder structure, enables high-fidelity data generation. Originally developed for biomedical image segmentation, UNet's symmetric design with skip connections preserves spatial information through its network layers. While the original UNet employs 2D convolution layers with max pooling and up convolution for dimensional adjustment, we modify this architecture for sequential data diffusion through the following enhancements:
\begin{enumerate}
\item Replace all convolution layers with 1D convolutions to process temporal structures in motion trajectories.
\item Introduce attention layers before each up- or down-sampling operation to better capture long-range dependencies in trajectory data, extending beyond the local computations of convolution layers.
\item Implement Residual connections~\citep{resnet} around attention layers, similar to Transformer architecture, to enhance learning capabilities for our complex data format and task.
\end{enumerate}
The resulting architecture for each up/down UNet layer in our model is illustrated in \cref{fig:unet}.
\begin{figure}[t]
    \centering
\includegraphics[width=0.98\linewidth]{imgs/illustration/Unet.png}
    \caption{SE(3) diffusion UNet architecture per layer. Trajectories are processed by the 1-dimensional convolution modules.}
    \label{fig:unet}
\end{figure}


\subsection{Training and Inference of $\mathbf{DOSE3}$}
As discussed in section~\ref{subsec:SE(3)}, the incorporation of rotation matrices from $\mathbb{SE}(3)$ format introduces manifold space considerations that preclude direct application of classical diffusion algorithms. We address three primary challenges:
(1) The undefined nature of addition and scalar multiplication operations for rotation matrices;
(2) The inability to guarantee valid rotation matrices when sampling $3\times 3$ matrices from $\mathcal{N}(0,I)$;
(3) The inadequacy of simple $L2$ norm differences for measuring distances/losses between rotation matrices.
To overcome these challenges, we introduce the new $\mathbb{SE}(3)$ DDPM algorithm. While we apply standard Euclidean space diffusion to the translational components of $\mathbb{SE}(3)$, we develop specialized techniques for handling manifold diffusion over the $\mathbb{SO}(3)$ rotation space.

We redefine the operators $\in$ $\mathbb{SO}(3)$ as follows. Essentially, we perform all operations after transforming the $\mathbb{SO}(3)$ data from manifold space into Euclidean tangent space by exponential and logarithmic map given by \cref{eq:exp} and \cref{eq:log}.
\begin{align}
    R_1 \oplus R_2 &= R_1 \cdot R_2\\
    k \otimes R_1 &= \exp(k \cdot \log(R_1)) \\
    k \in \mathbb{R},\  &R_1, R_2 \in \mathbb{SO}(3) \nonumber
\end{align}
We then change the noise sampling method from standard Gaussian distribution to the Isotropic Gaussian distribution on $\mathbb{SO}(3)$ ( $\mathcal{IG}_{\mathbb{SO}(3)}$ ) distribution. Shown in equation~\ref{eq:igso3}, we first sample v from standard Gaussiance distribution, representing the tangent vector, and then use the exponential map operation to transform it to the $\mathbb{SO}(3)$ space.
\begin{equation}
    \mathcal{IG}_{\mathbb{SO}(3)}(\mu, \sigma^2) = \mu \otimes v, \quad v \in \mathbb{R}^3 \sim \mathcal{N}(0,\sigma^2 I)
    \label{eq:igso3}
\end{equation}

Combining all the metrics and operations defined above, we design the full forward and backward $\mathbb{SO}(3)$ DDPM equations, incorporating the operations on the manifold, as,
\begin{align}
q(x_t \mid x_0) &= \bigl(\sqrt{\bar{\alpha}_t} \otimes x_0\bigr) \oplus \bigl((1 - \bar{\alpha}_t) \otimes \epsilon \bigr), \text{where}\  \epsilon\!\sim\!\mathcal{IG}_{\mathbb{SO}(3)}(0, I)\nonumber\\
\widehat{x}_0 \!&= \!\frac{1}{\sqrt{\bar{\alpha}_t}} \!\otimes\!\left(x_t \oplus \left(- \sqrt{1 - \bar{\alpha}_t} \!\otimes \!\epsilon_\theta(x_t, t)\right)\right), \\
\mu_t &= \left(\frac{\sqrt{\bar{\alpha}_{t-1}} \beta_t}{1 - \bar{\alpha}_t} \otimes \widehat{x}_0\right) \oplus \nonumber  \left(\frac{\sqrt{\alpha_t (1 - \bar{\alpha}_{t-1})}}{1 - \bar{\alpha}_t} \otimes x_t\right), \\
x_{t-1} &=\!\mu_t\!\oplus\!\left(\!\sqrt{\beta_t} \!\otimes \epsilon\right)
% \quad \epsilon\!\sim\!\mathcal{IG}_{SO(3)}(0, I).
\end{align}
The rotational distance will be adapted as the loss for training in $\mathbb{SO}(3)$ . The metric will reflect the average angle difference on each axis for two rotation matrices. The equation can be written as
$$\text{L}_{rot}(R_1, R_2) = \arccos\left(\frac{\mathrm{trace}(R_1^{\top} R_2) - 1}{2}\right)^2$$ 
The final $\mathbb{SE}(3)$ diffusion training takes in batches of trajectories in the format of ordered $\mathbb{SE}(3)$ sequences, maintaining the ordering of the trajectories. The complete training pseudo-code is shown in Algorithm~\ref{algo:SE3_diffusion}.

\begin{algorithm}[t]
\caption{SE(3) Sequence DDPM Training}
\label{algo:SE3_diffusion}
\begin{algorithmic}[1]
    \Require \text{Estimator}-$\epsilon_\theta$,\text{ Diffusion Scheduler}-$\alpha, \bar\alpha$, \text{ Train $\mathbb{SE}(3)$ Dataset =[trans, rot]}
    \While{training}
        \State $z_{\text{rot}} \gets \mathcal{IG}_{SO(3)}(0, I)$
        \State $z_{\text{trans}} \gets \mathcal{N}(0, I)$
        \State $(\text{trans}_t, \text{rot}_t) \gets \text{diffusion}(\text{rot}, \text{trans}, z_{\text{rot}}, z_{\text{trans}}, t)$
        \State $(\text{score}_{\text{trans}}, \text{score}_{\text{rot}}) \gets \text{model}(x_{\text{t-rot}}, x_{\text{t-trans}}, t)$
        \State $\text{trans}_0 \gets \frac{\text{trans} - \sqrt{1 - \bar{\alpha}_t} \cdot \text{score}_{\text{trans}}}{\sqrt{\bar{\alpha}_t}}$
        \State $\text{rot}_0 \gets \exp\left(\frac{\log(\text{rot}) - \sqrt{1 - \bar{\alpha}_t} \cdot \text{score}_{\text{rot}}}{\sqrt{\bar{\alpha}_t}}\right)$
        \State $\text{loss}_{\text{x0}} \gets \text{L1}(\text{trans}_0, \text{trans}) + \text{L}_{rot}(\text{rot}_0, \text{rot})$
        \State $\text{loss}_{\epsilon} \gets \text{L1}(\text{score}_{\text{trans}}, z_{\text{trans}}) + \text{L}_{rot}(\text{score}_{\text{rot}}, z_{\text{rot}})$
        \State $\text{loss} \gets \text{loss}_{\epsilon} + \text{loss}_{\text{x0}}$
        \State $\text{loss.backward()}$
    \EndWhile
\end{algorithmic}
\end{algorithm}

\subsection{OOD Detection}
While likelihood-based \ac{OOD} detection algorithms traditionally rely on generative model likelihood measures, the ELBO shown in equation~\ref{eq:ELBO} has proven inadequate for \ac{OOD} tasks due to its tendency to overestimate OOD sample likelihood~\citep{Serr√†2020Input}. Recent research demonstrates that the diffusion estimator $\epsilon_\theta$ and its derivatives effectively capture data distribution characteristics and can be obtained from a unified diffusion model without retraining. As shown in Equation~\ref{eq:kl_divergence}, the norm of noise estimator $\epsilon$ correlates with the divergence between different data distributions~\citep{heng2024out}. Based on this insight, we define the following OOD statistics group for a diffusion model with noise estimator $\epsilon_\theta$, where the operator $\langle x\rangle_p = \frac{1}{N}\sum_{i=0}^N x_i^p$.
\begin{align}
    Me&tricGroup(\epsilon_{\theta}) = \nonumber \\
    \Bigg[&\sum_t\langle\epsilon_{\theta}(x_t, t)\rangle_1, \sum_t\langle\epsilon_{\theta}(x_t, t)\rangle_2, \sum_t\langle\epsilon_{\theta}(x_t, t)\rangle_3 \nonumber\\
    &\sum_t\langle\partial\epsilon_{\theta}(x_t, t)\rangle_1, \sum_t\langle\partial\epsilon_{\theta}(x_t, t)\rangle_2, \sum_t\langle\partial\epsilon_{\theta}(x_t, t)\rangle_3 \Bigg],
\end{align}
where $MetricGroup(\epsilon_{\theta})\in\mathbb{R}^{6}$. For each sample $x_0$, we apply the DDPM forward process to obtain the perturbed sample $x_t$, then compute the metric group to derive final statistics. Given that our $\mathbb{SE}(3)$ diffusion model comprises separate sub-diffusions for $\mathbb{R}^{3}$ and $\mathbb{SO}(3)$, and rotation metric distributions can vary along x, y, and z directions (as illustrated in figure~\ref{fig:rot_3d}), we establish distinct metric sets for each rotational dimension. This results in 4 sets of statistics per sample: three for individual rotational axes and one for translation, yielding a total of 24 metrics per sample.
To process these metrics from the inlier data distribution, we estimate the density over the $24$-dimensional joint vector of metric groups. We employ straightforward density estimators such as Gaussian Mixture Models or Kernel Density Estimators, as each metric empirically exhibits approximately Gaussian behavior. During testing, we collect identical metrics for each test sample and infer likelihood from the inlier density estimator, identifying lower-likelihood samples as out of distribution. We establish the \ac{OOD} sample threshold at the bottom 5 percentile of inlier distribution likelihoods. The complete OOD model fitting and inference procedure is detailed in Algorithm~\ref{algo:OOD}.

\begin{figure}[t]
    \centering
    \subfigure[Oxford]{
        \includegraphics[width=0.46\linewidth]{imgs/tranxyz_train.png}
    }%
    \subfigure[KITTI]{
        \includegraphics[width=0.46\linewidth]{imgs/tranxyz_test.png}
    }
    \caption{Distribution of elements $\epsilon(x_t, t)$ in rotation tangent space for each dimension when running Oxford Robot Car and KITTI dataset on model trained on Oxford Robot Car}
    \label{fig:rot_3d}
\end{figure}

\begin{algorithm}[t]
\caption{OOD Detection}
\label{algo:OOD}
\begin{algorithmic}[1]
\Require \text{DDPM Model, Inlier $\mathbb{SE}(3)$ Dataset I}, \text{Query $\mathbb{SE}(3)$ Trajectory q}
\State $stats \gets []$
\For{$traj_0 \text{ in I}$}
\State $sum \gets [0,0,0,0,0,0]$
\For{$\text{t in [0, T-1]}$}
    \State $traj_t \gets DDPM_{forward}(traj_0, t)$
    \State $sum \gets sum + \text{6D metrics over }\epsilon_\theta(traj_t, t)$
\EndFor
\State $stats.\text{append}(sum)$
\EndFor
\State $distribution \gets \text{GMM.fit(stats)}$
\State $q_t \gets DDPM_{forward}(q, t)$
\State $metric_q \gets \text{6D metrics over }\epsilon_\theta(q_t, t))$
\State $likelihood \gets distribution\text{.eval(}metric_q\text{)}$
\State $OOD \gets likelihood < threshold$
\end{algorithmic}
\end{algorithm}