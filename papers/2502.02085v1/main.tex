\documentclass{article}
\usepackage{graphicx} % Required for inserting images
\usepackage[a4paper, margin=0.9in]{geometry}
\usepackage{authblk} % For author affiliations
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{apalike}
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{booktabs}
\usepackage{multirow}
\usepackage{inconsolata}
\usepackage{tikz}

\title{\textbf {A New Rejection Sampling Approach to $k$-$\mathtt{means}$++ \\With Improved Trade-Offs}}
\author[1]{Poojan Shah}
\author[1]{Shashwat Agrawal}
\author[1]{Ragesh Jaiswal}
\affil[1]{Department of Computer Science and Engineering, Indian Institute of Technology Delhi}
\affil[ ]{\texttt{\{cs1221594, csz248012, rjaiswal\}@cse.iitd.ac.in}}
\date{} % Remove date


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% colorful Citations
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\usepackage{xcolor}
\definecolor{MidnightBlue}{HTML}{191970}
\usepackage{hyperref}
\hypersetup{
    colorlinks=true,
    citecolor=MidnightBlue,  % You can change this to red, green, etc.
    linkcolor=black,
    urlcolor=cyan
}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% Caligraphy
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\newcommand{\calA}{\mathcal A}
\newcommand{\calB}{\mathcal B}
\newcommand{\calC}{\mathcal C}
\newcommand{\calD}{\mathcal D}
\newcommand{\calE}{\mathcal E}
\newcommand{\calF}{\mathcal F}
\newcommand{\calG}{\mathcal G}
\newcommand{\calH}{\mathcal H}
\newcommand{\calI}{\mathcal I}
\newcommand{\calJ}{\mathcal J}
\newcommand{\calK}{\mathcal K}
\newcommand{\calL}{\mathcal L}
\newcommand{\calM}{\mathcal M}
\newcommand{\calN}{\mathcal N}
\newcommand{\calO}{\mathcal O}
\newcommand{\calP}{\mathcal P}
\newcommand{\calQ}{\mathcal Q}
\newcommand{\calR}{\mathcal R}
\newcommand{\calS}{\mathcal S}
\newcommand{\calT}{\mathcal T}
\newcommand{\calU}{\mathcal U}
\newcommand{\calV}{\mathcal V}
\newcommand{\calW}{\mathcal W}
\newcommand{\calX}{\mathcal X}
\newcommand{\calY}{\mathcal Y}
\newcommand{\calZ}{\mathcal Z}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% THEOREMS
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\theoremstyle{plain}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{assumption}[theorem]{Assumption}
\theoremstyle{remark}
\newtheorem{remark}[theorem]{Remark}





%%%%%%%%%%%%%%%% Custom Commands %%%%%%%%%%%%%%%%%%%%%%%%%%%%

\newcommand{\nnz}{\mathtt{nnz}}
\newcommand{\kpp}{$\mathtt{k}\text{-}\mathtt{means}$++ }
\newcommand{\dkpp}{$\delta \text{-}\mathtt{k}\text{-}\mathtt{means}\text{++ }$}
\newcommand{\Var}{\operatorname{Var}}
\newcommand{\means}{$\mathtt{means}$ }
\newcommand{\rej}{\mathsf{RejectionSampling}}
\newcommand{\out}{\mathsf{Output}}
\newcommand{\X}{\mathcal{X}}
\newcommand{\D}{\mathcal{D}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\N}{\mathbb{N}}
\newcommand{\E}{\mathbb{E}}
\newcommand{\eps}{\varepsilon}
\newcommand{\rskpp}{$\mathtt{RS\text{-}k\text{-}means}$++}
\renewcommand{\P}{\mathcal{P}}
\newcommand{\Q}{\mathcal{Q}}
\newcommand{\opt}{\mathtt{OPT}}
\newcommand{\C}{\mathcal{C}}
\renewcommand{\H}{\mathcal{H}}
\newcommand{\U}{\mathcal{U}}
\newcommand{\F}{\mathcal{F}}
\newcommand{\allc}{\mathtt{AC}}
\newcommand{\rejsamp}{$\mathtt{RejectionSample}$}
\renewcommand{\output}{\mathtt{Output}}
\newcommand{\afkmc}{\mathtt{AF}\text{-}\mathtt{k}\text{-}\mathtt{MC^2}}
\newcommand{\prone}{\mathtt{PRONE}}
\newcommand{\rskmeans}{\mathtt{RS\text{-}k\text{-}means}\text{++}}




\begin{document}

\setlength{\parindent}{0pt}

\maketitle
\begin{abstract}
    \noindent The $k$-$\mathtt{means}$++ seeding algorithm~\cite{arthur_vassilvitskii_07} is widely used in practice for the $k$-means clustering problem where the goal is to cluster a dataset $\mathcal{X} \subset \mathbb{R} ^d$ into $k$ clusters.
 The popularity of this algorithm is due to its simplicity and provable guarantee of being $O(\log k)$ competitive with the optimal solution in expectation.  However, its running time is $O(|\mathcal{X}|kd)$, making it expensive for large datasets. 
 In this work, we present a simple and effective rejection sampling based approach for speeding up $k$-$\mathtt{means}$++. 
 Our first method runs in time $\tilde{O}(\nnz (\mathcal{X}) + \beta k^2d)$ while still being $O(\log k )$ competitive in expectation. Here, $\beta$ is a parameter which is the ratio of the variance of the dataset to the optimal $k$-$\mathtt{means}$ cost in expectation and $\tilde{O}$ hides logarithmic factors in $k$ and $|\mathcal{X}|$. 
 Our second method presents a new trade-off between computational cost and solution quality. It incurs an additional scale-invariant factor of $ k^{-\Omega( m/\beta)} \Var (\mathcal{X})$ in addition to the $O(\log k)$ guarantee of $k$-$\mathtt{means}$++ improving upon a result of \cite{bachem_16a} who get an additional factor of $m^{-1}\Var(\mathcal{X})$  while still running in time $\tilde{O}(\nnz(\mathcal{X}) + mk^2d)$.   We perform extensive empirical evaluations to validate our theoretical results and to show the effectiveness of our approach on real datasets.
\end{abstract}

\newpage
\section{Introduction}

Data clustering has numerous applications in data processing and is one of the  classic problems in unsupervised machine learning. Its formulation as the $k$-\means problem is defined as: given a data set $ \X  \subset \R^d$ and a positive integer $k$ representing the number of clusters into which the dataset is to be partitioned, find a set $C \subset \R^d$ of $k$ centers such that the following objective or cost function is minimized : 
$$ \Delta(\X,C) \coloneqq \sum_{x \in \X} \min_{c \in C} \|x - c\|^2 $$
The set $C$ implicitly defines a partition of $\X$ based on the closest center from $C$. A set of centers which achieve the minimum  $k$-\means cost is denoted by $\mathtt{OPT}_k = \{ c_1^*, \dots, c_k^*\}$. We shall be using the shorthand $\Delta_k(\X) \coloneqq \Delta(\X,\mathtt{OPT}_k)$ to refer to the optimal $k$-\means cost. \\\\
\textbf{Background on the $k$-\means problem}. On the hardness front, solving the $k$-\means problem exactly is known to be $\mathtt{NP}$-hard \cite{dasgupta_08}, even when the data points are restricted to lie in a plane \cite{mahajan_09}. Moreover, there exists a constant $c > 1$ such that it is $\mathtt{NP}$-hard to solve the $c$-approximate version of $k$-\means where we are allowed  to output cluster centers $C$ such that $\Delta(\X,C) \leq c\Delta_k(\X)$ \cite{awasthi_15,lee_17,cohen-addad_c.s.19} . On the algorithmic front, a significant amount of effort has been put into designing algorithms for $k$-\means that have strong theoretical guarantees. These include, for example, the constant factor approximation results of \cite{jain_vazirani_01,kanungo_02,ahmadian_17,cohen-addad_22} and the $(1 + \varepsilon)$ approximation schemes of \cite{kumar_10,jaiswal_14,jaiswal_15,cohen-addad_18,friggstad_19,cohen-addad_19,bhattacharya_20} which have exponential dependence on one or more of $\varepsilon^{-1},k \text{ or } d$.  While these works provide important insights into the structure of the $k$-\means problem, they are seldom used in practice due to their slow speed. Indeed, one of the most popular heuristics used in practice \cite{wu_08} is Lloyd's iterations \cite{lloyd_82}, also referred to as the $k$-\means method. It starts off with an initial set of centers
%\footnote{The centers may be chosen randomly, and this is commonly referred to as \textit{seeding}.} 
\footnote{This is commonly known as {\em seeding}. A simple seeding method is to arbitrarily pick $k$ points from $\mathcal{X}$.} 
and iteratively refines the solution. This hill-climbing approach may get stuck in local minima and provide arbitrarily bad clusterings even for fixed $n$ and $k$ \cite{dasgupta_03,har-peled_sadri_05,arthur_vassilvitskii_06a,arthur_vassilvitskii_06b}. \\

\textbf{$k$-\means++ and $D^2$-sampling.} Usually, Lloyd's iterations are preceded by the $k$-\means++ seeding introduced in \cite{arthur_vassilvitskii_07}. 
Even though the $k$-means++ algorithm is the Lloyd's iterations preceded by $k$-means++ seeding, it is common to refer to the seeding procedure as $k$-means++. We follow this in the remaining discussion.
$k$-\means++ is a fast sampling-based approach.
%for picking the initial $k$ centers. 
Starting with  a randomly chosen center $S = \{c_1\}$, a new point $x \in \X$ is chosen as the next center with probability proportional to $\Delta(\{x\},S)$  in each iteration. This is commonly referred to as $D^2$-sampling. The centers generated by this seeding method are guaranteed to be $O(\log k)$ competitive with the optimal solution in expectation. Thus, $k$-\means++  provides the best of both worlds : theory and practice and  unsurprisingly, a lot of work has been done on it. This includes extending it to the distributed setting \cite{bahmani_12} and  the streaming setting \cite{ailon_09,ackermann_12}. Furthermore, several results on coreset constructions \footnote{See, for example \cite{bachem_17,feldman_20} and the extensive references cited therein.} are inspired by or rely on the theoretical guarantees of $k$-\means++. Recently, it was shown that appending $k$-\means++ with a sufficiently large number of local search steps \cite{lattanzi_sohler_19,choo_20} can lead to $O(1)$ competitive solutions. \\
% \poojan{Add other relevant papers after literature review}

A downside of $k$-\means++ is that its $\Theta(nkd)$ computational complexity becomes impractical on large datasets. Various approaches \cite{bachem_16a,bachem_16b,cohen-addad_20,charikar_23} have been presented to speed up $k$-\means++ with varying trade-offs, and our work also falls into this category. A detailed discussion about the position of our approach in the literature is presented in Section \ref{subsec:compare}. We also include Table \ref{tab:compare} as a summary for reference.




\section{Our Results}

 In this section, we present a high level discussion of our results, contributions and their significance. 

 \textbf{Improved tradeoffs.} Our main technical contribution is a novel simple yet fast algorithm based on rejection sampling with an improved trade-off between the computational cost and solution quality for $k$-\means++ in the Euclidean metric. A description is given in Algorithm \ref{alg:rskpp}. We state our result formally below. 

 \begin{theorem} \label{thm:main}
 (Main Theorem)
     Let $m \in \N$ be a parameter and $k \in \N$ be the number of clusters. Let $\X \subset \R^d$ be any dataset of $n$ points and $S$ be the output of $\mathtt{RS\text{-}k\text{-}means}$++ $(\X,k,m')$ where $m' = cm\ln k$ for some constant $c > 1$. Then the following guarantee holds : 
     $$ \E[\Delta(\X,S)] \leq 8(\ln k+2)\Delta_k(\X) + \frac{6k}{k^{\frac{cm}{2\beta(\X)}}-1} \Delta_1(\X) $$
      Here $\beta(\X)$ \footnote{As can be seen from the description, the value of $\beta(\X)$ is not needed to be known by our algorithm} is a parameter such that $\E[\beta(\X)] = \frac{\Delta_1(\X)}{\Delta_k(\X)}$. Moreover, the computational cost of the algorithm includes a single-time preprocessing cost of $\tilde{O}(\nnz(\X))$ \footnote{$\nnz(\X)$ represents the \textit{number of non zero entries} in the dataset $\X$. When $\X$ is sparse, this can be much smaller than $nd$.}, with the cost of performing a single clustering being $O(mk^2d\log k)$.  

 \end{theorem}

 To the best of our knowledge, such trade-offs were not known before this work. The approximation guarantee can be seen to be composed of two terms. The first term is the standard $O(\log k )$ guarantee of $k$-\means++, while the second term can be thought of as an additive, scale-invariant term representing the variance of the dataset. Note that as $m$ grows, the second term diminishes rapidly. Indeed, this exponentially decreasing dependence  of $k^{-\Omega(m / \beta(\X))}$ improves on a similar result by \cite{bachem_16a} who instead get a linearly decreasing dependence of $O(1/m)$ , although through a significantly different approach.  \\

\textbf{Correct number of iterations. } Whenever we have such trade-offs, a natural question to ask is : for which value of $m$ can we get $O(\log k)$ competitive solutions like those of $k$-\means++ ? For example, we require $m = \Omega\left(\frac{\Delta_1(\X)}{\Delta_k(\X)}\right)$ in \cite{bachem_16a}'s algorithm. But this means that we would some how need to get an estimate for $\Delta_k(\X)$, which involves solving the $k$-\means problem itself ! Fortunately, Algorithm \ref{alg:rskpp} can \textit{``discover''} the value of $\beta(\X)$ as it executes. We state this as follows : 

\begin{theorem} \label{thm:second}
Let $\epsilon \in (0,1)$ and $k \in \N$ be the number of clusters. Let $\X \subset \R^d$ be any dataset of $n$ points and $S$ be the output of $\mathtt{RS\text{-}k\text{-}means}$++ $(\X,k,\infty)$. Then the following guarantee holds : 
     $$ \E[\Delta(\X,S)] \leq 8(\ln k+2)\Delta_k(\X) $$ 
     Moreover, the computational cost of the algorithm includes a single-time preprocessing cost of $\tilde{O}(\nnz(\X))$ with the cost of performing a single clustering being bounded by $O(\beta(\X)k^2d\log (k/\epsilon))$ with probability atleast $1 - \epsilon$.  Here, $\beta(\X)$ is a parameter such that  $\E[\beta(\X)] = \frac{\Delta_1(\X)}{\Delta_k(\X)}$.
\end{theorem}

\textbf{Experimental results.} We evaluate our algorithms experimentally on several data sets as described in Section ~\ref{appendix:experiments}.

\subsection{Overview of Our Techniques}
\textbf{Algorithm.} Our main algorithm is outlined in Algorithm~\ref{alg:rskpp}. It consists of a light-weight pre-processing step followed by choosing new centers according to the procedure $\mathtt{D^2\text{-}sample}$. This procedure consists of two parts : the first part is a rejection sampling loop, which generates samples distributed according to the  $D^2$ distribution using samples generated from a specific distribution which is \textit{easy to sample from}, being setup during the pre-processing itself. In case no sample is generated in $m$ iterations, the  second part consists of  choosing the next center uniformly at random. \\

\textbf{Proof intuition.} To analyze the expected solution quality of \rskpp, we study a variant of \kpp which we call $\delta$-\kpp. In this variant , instead of sampling the next center from the $D^2$ distribution $p(x) = \frac{\Delta(x,S)}{\Delta(\X,S)}$, we sample from a different distribution defined by $$p'(x) =(1 - \delta)\frac{\Delta(x,S)}{\Delta(\X,S)} + \delta\frac{1}{|\X|} $$

The parameter $\delta$ can be thought of as representing the probability that $\mathtt{sampled = False}$ after the \textbf{repeat} loop is executed. If this event happens, we choose a center uniformly at random.  Consider the case when $\delta = 0$ : this means that we get $O(\log k)$ competitive solutions since we sample exactly from the $D^2$ distribution. Now consider the case when $\delta = 1$. This corresponds to choosing all centers uniformly at random. It can be seen \footnote{The cost considering all centers is upper bounded by the cost considering only the first center. Since it is chosen uniformly at random , we can use Lemma 3.1 of \cite{arthur_vassilvitskii_07}.} that in this case, we have $\E[\Delta(\X,S)] \leq 2 \Delta_1(\X)$. So, we expect that $\delta \in (0,1)$ leads to a trade-off between these two terms.  The technical analysis of error propagation due to the use of a slightly perturbed distribution may be of independent interest. 


\begin{algorithm}[ht]
   \caption{$\mathtt{RS\text{-}k\text{-}means}$++ $(\X,k,m)$}
   \label{alg:rskpp}
   \textbf{Input :} dataset $\X \subset \R^d$, number of clusters $k \in \N $ and the upper bound on number of iterations $m \in \N$ \\
   \textbf{Output :} $S = \{ c_1, \dots, c_k\} \subset \X$
\begin{algorithmic}[1]
   \STATE $\mathtt{preprocess}(\X)$
   \STATE Choose $c_1 \in \X$ uniformly at random and set $S \gets \{c_1\}$
   \FOR{$i \in \{2,\dots,k\}$}
    \STATE $c_i \gets \mathtt{D}^2\text{-}\mathtt{sample}(\X,S,m)$
   \STATE $S \gets S \cup\{c_i\}$
   \ENDFOR
   \STATE {\bfseries return} $S$
\end{algorithmic}
\end{algorithm}



\begin{algorithm}[ht]
    \floatname{algorithm}{Procedure}
   \caption{$\mathtt{preprocess}(\X)$}
   \label{proc:preproc}
   \textbf{Input :} dataset $\X \subset \R^d$\\
   \textbf{Ensure :} $\X$ is centered
\begin{algorithmic}[1]

   \STATE Compute the mean $\mu(\X)$ of the dataset $\X$ and perform $x \gets x - \mu(\X)$ for every $x \in \X$
   %\FOR{$x \in \X$}
   %\STATE $x \gets x - \mu(\X)$ 
   %\ENDFOR
   \STATE Setup the sample and query access data structure to enable sampling from the distribution $D_\X(x) = \frac{\|x\|^2}{\|\X\|^2}$
\end{algorithmic}
\end{algorithm}

\begin{algorithm}[ht]
    \floatname{algorithm}{Procedure}
   \caption{$\mathtt{D}^2\text{-}\mathtt{sample}(\X,S,m)$} 
   \label{proc:sample}
   \textbf{Input :} dataset $\X \subset \R^d$, currently chosen centers $S \subset \X$ and upper bound on number of iterations $m \in \N$\\
   \textbf{Output :} next center $c \in \X$
   
\begin{algorithmic}[1]
 \STATE $\mathtt{iter} \gets0$ and $\mathtt{sampled} \gets \mathtt{False}$
   \REPEAT
        \STATE $\mathtt{iter} \gets \mathtt{iter}+1$
        \STATE $r \sim [0,1]$
        \STATE Choose $x \in \X$ with probability$\frac{\|x\|^2 + \|c_1\|^2}{\|\X\|^2 + |\X|\|c_1\|^2}$ \label{line:5}
        \STATE Compute $\rho(x) = \frac{1}{2} \frac{\Delta(x,S)}{\|x\|^2 + \|c_1\|^2}$
        \IF{$r \leq \rho(x)$}
            \STATE Set $c$ to be $x$ and $\mathtt{sampled = True}$
        \ENDIF
   \UNTIL{$\mathtt{sampled = True}$ or $\mathtt{iter} > m$ }
   \IF{$\mathtt{sampled = False}$}
        \STATE Choose $c \in \X$ uniformly at random 
   \ENDIF
   \STATE \textbf{return} $c$


   
\end{algorithmic}
\end{algorithm}



\subsection{Advantages of our approach}
\textbf{Fast data updates.} Rejection sampling essentially involves converting samples from a distribution which is $\textit{``easy to sample from"}$ to a required distribution. 
The single time pre-processing 
%can be used to set up 
sets up a simple binary tree data structure \footnote{We were inspired by \cite{tang_19} which introduced a randomized linear algebra based framework for efficient simulation of \textit{quantum machine learning} algorithms. } for sampling from an appropriate distribution. This structure supports addition and update of a data point in $O(\log 
|\X|)$ time while taking up only $O(\nnz(\X))$ additional space. The details are given in Section~\ref{subsec:data structure}.\\
%This may be relevant in certain applications.

\textbf{Parallel setting.} The simplicity of our approach extends easily to parallel and distributed settings. 
%In what follows, 
We briefly discuss implementing the procedure $\mathtt{D^2\text{-}sample}$ in such settings. 
 We assume that the dataset $\X$ is on a single machine which has $M$ cores. Suppose that the probability that a sample is output in a single round of the \textbf{repeat} loop is $p$. Recall that we have $p \geq \frac{\Delta_k}{2 \Delta_1} $. The expected number of rounds that one must wait for a sample to be generated is atmost $2 \Delta_1 / \Delta_k$. Also notice that each round is independent of other rounds. So we can utilize all $M$ cores to perform rejection sampling until one of them outputs a sample. Hence, the probability that a sample is generated in a round now becomes $1 - (1 - p)^M \geq 1-e^{-pM}$. Hence the number of rounds needed to get a sample is atmost $\frac{e^{pM}}{e^{pM}-1}$ in expectation, which decreases drastically as $M$ increases. 


\subsection{Comparison with Related Work} \label{subsec:compare} 

In this section we compare our results for $k$-\means++ with other fast implementations having theoretical guarantees.

\textbf{MCMC methods.} The line of work \cite{bachem_16b,bachem_16a} uses the Monte-Carlo-Markov-Chain based Metropolis-Hastings algorithm \cite{hastings_70} to approximate the $D^2$-distribution in $k$-\means++. This involves setting up a markov chain of length $m$ to generate samples from the $D^2$ distribution $p(\cdot)$ using samples from a proposal distribution $q(\cdot)$. \cite{bachem_16b} used $q(\cdot)$ as the uniform distribution. To bound the solution quality of their method, they introduce the following parameters :

$$ \alpha(\X) \coloneqq \max_{x \in \X} \frac{\Delta(x,\mu(\X))}{\Delta_1(\X)} \quad \beta(\X) \coloneqq \frac{\Delta_1(\X)}{\Delta_k(\X)}, $$

and show that $\alpha(\X) \in O(\log^2n)$ and $\beta(\X) \in O(k)$ under some assumptions on the data distribution that is natural, but $\mathtt{NP}$-hard to check. 
By doing so, they bound the required chain length $m \in O(\alpha(\X)\beta(X) \log k \beta(\X)) \in O(k^3d\log^2n\log k)$ to achieve $O(\log k)$ competitive solutions. This was improved upon by \cite{bachem_16a} by using a more suitable proposal distribution which needs $O(nd)$ pre-computation time. By doing so, they get rid of dependence on $\alpha(\X)$ while showing a tradeoff between computational cost and approximation guarantee (see Table \ref{tab:compare}) without any data assumptions. They incur an additional $O(1/m) \Delta_1(\X)$ error for a runtime $ \in O(mk^2d\log k)$. Our rejection sampling approach  has the advantage of being independent of $\alpha(\X)$, providing a stronger guarantee with only $k^{-\Omega\left(\frac{m}{\beta(\X)}\right)}\Delta_1(\X)$ additive error and being easy to extend to the parallel setting. On the other hand, MCMC methods are generally viewed to be inherently sequential \footnote{Note that the pre-processing step of \cite{bachem_16a} is easily parallelized.}. \\

\textbf{Tree embeddings and ANNS.} \cite{cohen-addad_20} introduced an algorithmically sophisticated approach to speeding up $k$-\means++, focusing on the large $k$ regime. They use $\mathtt{MultiTree}$ embeddings with $O(d)$ expected distance distortions to update the $D^2$ distribution efficiently. They then use locality-sensitive hashing-based data structures for approximate nearest neighbor search to speed up their algorithm. This adds a significant layer of complexity in implementation. Their runtime also depends on the aspect ratio $\eta$, which may be quite large in case there are points in the dataset which are very close to each other. It has better dependence on $k$ but additional $n^{O(1)}, \log^{O(1)} \eta $ factors and cubic dependence on $d$ \footnote{\cite{cohen-addad_20} recommend using dimension reduction techniques such as  the Johnson-Lindenstrauss transformation \cite{johnson_lindenstrauss_84}, which adds to the complexity of their approach.}. Moreover, their algorithm is advantageous only for large $k \sim 10^3$. Note that they also use rejection sampling to take into account the distance distortions, which is different from our use of rejection sampling. Our approach provides improved trade-offs while being simple. \\

\textbf{1-D projections. } \cite{charikar_23} proposed an efficient method to perform the $k$-\means++ seeding in 1 dimension in $O(n \log n)$ time with high probability. For a general $d$-dimensional dataset, they first project it on a randomly chosen $d$- dimensional gaussian vector followed by an application of the 1-D method. This allows them to get an extremely fast runtime of $O(\nnz(\X) + n\log n)$. However, they only get $O(k^4 \log k)$ competitive solutions, which shows up in their experimental evaluations as well. They show how to get $O(\log k)$ competitive solutions by using coresets, but end up with an additional high degree ${O}(k^5 d \log k \log (k \log k))$ \footnote{\cite{charikar_23} denote the size of the coreset as $s \in \Omega\left( \eps^{-2}k\gamma d\log (k \gamma) \right)$ where $\gamma$ is the approximation ratio of the 1-d method i.e, $\gamma \in O(k^4 \log k)$ . This is only required for the theoretical guarantee of being $O(\log k)$ competitive to hold true. The coreset size can be treated as a hyper-paramter for trade-off between runtime and solution quality as well. } dependence. This may be restrictive even for moderate values of $k$, while our algorithm only has $O(k^2)$ dependence. \\

\textbf{Other related works.} \cite{bachem_17b} showed similar trade-offs for the $k$-\means$\mathtt{||}$ algorithm of \cite{bahmani_12} in the distributed setting. They also get an additive scale-invariant factor in the approximation guarantee which diminishes with increase in  the number of rounds and the oversampling factor of $k$-\means$\mathtt{||}$. In contrast, we present a new rejection sampling based algorithm for $k$-\means++ with improved trade-offs. More recently, \cite{jaiswal_24} proposed an algorithm for performing the $k$-\means++ seeding in $\tilde{O}(nd + \eta^2k^2d)$ by using the framework of \cite{tang_19} through a data structure similar to the one used by us in the pre-processing step. 

\begin{table*}[ht]
\caption{Comparison of computational complexity and approximation guarantee of various approaches to speed up $k$-\means++. Here, $\Delta$ is the clustering cost for the centers returned by the algorithm and $\Delta_k$ is the optimal $k$-\means cost}
\label{tab:compare}
\vskip 0.15in
\begin{center}
\begin{scriptsize}

\begin{tabular}{p{1.5cm}p{4cm}p{4.5cm}p{4cm}}
\toprule

{\sc Approach}  & {\sc Comp.  Complexity} & {\sc Approx. Guarantee}  & { \sc Remarks} \\

\midrule


\cite{bachem_16b} & $O(k^3d \log^2n \log k)$ & $\E[\Delta] \leq 8(\ln k +2)\Delta_k$ & The analysis only holds when the dataset satisfies certain assumptions which are $\mathtt{NP}$-hard to check \\

\midrule

\cite{bachem_16a}  & $O(nd) + O(mk^2d\log k)$ & $\E[\Delta] \leq 8(\ln k +2)\Delta_k + O\left(\frac{1}{m}\right) \Delta_1  $ & $m$ is the markov chain length used \\

\midrule

{\bf Our}  & $O(\nnz(\X)) + O(mk^2d\log k)$ & $\E[\Delta] \leq 8(\ln k+2)\Delta_k + 6k^{-\Omega(m/\beta) }\Delta_1$ & $\nnz(\X)$ represents the input sparsity. The bound on number of iterations for rejection sampling is $O(m\log k)$. $\E[\beta] = \Delta_1 / \Delta_k$\\


\midrule

\cite{cohen-addad_20} & $O\left(n(d+\log n) \log (\eta d)\right) + O\left(\eps^{-1}kd^3 \log \eta (n \log \eta )^{O(\eps)}\right)$ & $\E[\Delta] \leq 8\eps^{-3}(\ln k +2)\Delta_k  $ & $\eps \in (0,1)$ is a sufficiently small error factor for the LSH data structure . $\eta$ is the aspect ratio i.e, $\eta = \frac{\max_{x,y \in \X} \|x-y\|}{\min_{x,y \in \X} \|x-y\|}$  \\

\midrule

\cite{charikar_23}  & $O(\nnz(\X)) + O(n\log n)$ & $\E[\Delta] \leq 51k^4(\ln k+2)\Delta_k$ & $\nnz(\X)$ represents the input sparsity. The exact constant is upper bounded by $8\sqrt{24\sqrt{e}} \simeq 50.3$ \\

\midrule

\cite{charikar_23}  & $O(\nnz(\X)) + O(n\log n) + O(\eps^{-2}k^5 d \log k \log (k \log k)$ & $\E[\Delta] \leq 8(\ln k+2)(1 + \eps)\Delta_k$ & $\nnz(\X)$ represents the input sparsity. The high polynomial factor in $k$ is due to coreset constructions \\



\bottomrule
\end{tabular}

\end{scriptsize}
\end{center}
\vskip -0.1in
\end{table*}


\section{Preliminaries}

For any two points $p,q \subset \R^d$, $\|p-q\|$ denotes their Euclidean distance. Throughout the paper, we denote the $d$ dimensional dataset to be clustered by $\X \subset \R^d$ with $|\X| = n$. For a set of points $\P \subset \R^d$, The number of non-zero elements in $\P$ is denoted by $\nnz(\P)$. Note that when all points in $\P$ are distinct, we have $|\P| \leq \nnz(\P)$. We define the \textit{norm} of the set  $\P$ to be the quantity $\|\P\| = \sqrt{\sum_{p \in \P} \|p\|^2}$. The $\mathtt{k}\text{-}\mathtt{means}$ clustering cost of $\P$ with respect to a set of centers $C$ is denoted by : 

$$ \Delta(\P,C) = \sum_{p \in \P} \min_{c \in C}\|p -c\|^2$$
When either $\P$ or $C$ is a singleton set, we use expressions like $\Delta(p,C)$ or $\Delta(\P,c)$ instead of  $\Delta(\{p\},C)$ or $\Delta(\P,\{c\})$ respectively. The $D^2$ distribution over $\P$ with respect to $C$ is denoted by $D^2(\P,C)$ where the probability of a point $p \in \P$ being chosen is $\frac{\Delta(p,C)}{\Delta(\P,C)}$. $D_\P$ denotes the distribution over $\P$ defined as $D_\P(p) = \frac{\|p\|^2}{\|\P\|^2}$ for each $p \in \P$. For a set $\P$ and a probability distribution $D$ over $\P$, $p \sim D$ denotes sampling a point $p \in \P$ with probability $D(p)$. 


\subsection{Data Dependent Parameter} The computation-cost vs. solution-quality trade-off of our algorithm depends on a data-dependent parameter which is bounded  by $ \beta(\X) \coloneqq \Delta_1(\X) / \Delta_k(\X)$. Without any assumptions on $\X$,  this parameter is unbounded (for example, if the data set had only $k$ points, then $\beta(\X) = \infty$, but as \cite{bachem_16b} point out, what is the point of clustering such a dataset if the solution is trivial ?). Indeed, if we assume that $\X$ is generated from some probability distribution over $\R^d$, this parameter becomes independent of $|\X|$, as $|\X|$ grows larger \cite{pollard_81}. Moreover \cite{bachem_16b} showed that for a wide variety of commonly used distributions\footnote{These include the uni-variate and
multivariate Gaussian, the Exponential and the Laplace distributions along with their mixtures. For the exact assumptions made on the dataset, see section 5 of \cite{bachem_16b}} $\beta(\X) \in O(k)$. In the experimental section, we shall also see that on many practical datasets, this parameter does not take on values which are prohibitively large \footnote{Also see the estimated values this parameter for other datasets in Table 1 of \cite{bachem_16b}}. 



\section{Rejection Sampling} \label{appendix:rejection sampling}

Given the dataset  $\X\subset\R^d$ and a set of already chosen centers $S\subset \X$, our goal is to obtain a sample from $\X$ according to the $D^2(\X,S)$ distribution. Recall that we defined the distribution $D_\X$ over $\X$ by $D_\X(x) = \frac{\|x\|^2}{\|\X\|^2}$. The main ingredient of our algorithm is a rejection sampling procedure which allows us convert samples from $D_\X$ to a sample from $D^2(\X,S)$. 

We shall  pre-process our dataset so that we can efficiently sample from $D_\X$, and then convert samples from $D_\X$ to samples from $D^2(\X,S)$. Choosing the first center uniformly at random from $\X$ and repeating this procedure for $k-1$ times is precisely our algorithm for performing the $k$-\means++ seeding. 

\begin{definition} \label{def:oversample}

Suppose $D_1$, $D_2$ define probability distributions over $\X$. The distribution $D_2$ is said to $\tau$-\textit{oversample} $D_1$ for $\tau > 0$ if  $D_1(x) \leq \tau D_2(x)$ for each $x \in \X$.  
    
\end{definition}



\begin{algorithm}
    \caption{$\mathtt{RejectionSample}$}
    \label{alg:rej-sample}
    \textbf{Input: } Samples generated from $D_2$ \\
    \textbf{Output: } A sample generated from $D_1$ \\ 
    \vspace{-12pt}
    \begin{algorithmic}[1]
    \STATE $\mathtt{sampled = False}$
    \REPEAT
        \STATE $x \sim D_2$ , $r \sim [0,1]$
        \STATE Compute $\rho(x) = \frac{D_1(x)}{\tau D_2(x)}$
        \IF{$r \leq \rho(x)$}
            \STATE \textbf{output} $x$ and set $\mathtt{sampled= True} $
        \ENDIF
    \UNTIL{$\mathtt{sampled = True}$}
    
        
    \end{algorithmic}
\end{algorithm}

Consider Algorithm~\ref{alg:rej-sample} which takes samples generated from $D_2$ as input and outputs a sample generated from $D_1$.
\begin{lemma}\label{lem:oversampling lemma}
    Let $D_1, D_2$ be probability distributions over $\X$ such that $D_2$ $\tau$-\textit{oversamples} $D_1$. The expected number of samples from $D_2$ required by \rejsamp \ to output a single sample from $D_1$ is $\tau$. Moreover, for any $\eps \in (0,1)$ the probability that more than $\tau \ln \frac{1}{\eps}$ samples are required is atmost $\eps$. 
\end{lemma}

\begin{proof}
    Let $T$ be the random variable denoting the number of rounds required for a sample to be output by \rejsamp. Let  $\output$ denote the event that a sample is output in a particular round. We have 
    \begin{align*}
        \Pr[\output] = \sum_{x \in \X} D_2(x) \rho (x) = \tau^{-1} \sum_{x \in \X} D_1(x) = \tau^{-1} 
    \end{align*}
    Given that a sample is generated, it is easy to see that it is distributed according to $D_1$.  It takes exactly $t$ rounds for a sample to be generated if no sample is generated in the first $t-1$ iterations and a sample is generated in the last iteration. Hence,
    $$\Pr[T = t] = \left(\Pr[\neg \output] \right)^{t-1} \Pr[\output] = \tau^{-1}(1 - \tau^{-1})^{t-1}$$

    which means that $T$ is a geometric random variable with parameter $\tau^{-1}$, so that $E[T] = \tau$.  It is easy to see that $T$ has exponentially diminishing tails : 
    \begin{align*}
    \Pr[T > t] &= \sum_{j = t+1}^{\infty} \tau^{-1}(1 - \tau^{-1})^{j-1} 
    = (1 - \tau^{-1})^t \leq e^{-t/\tau}
    \end{align*}
    from which the lemma follows. 
\end{proof}


\begin{remark}
    Note that the algorithm does not require any estimate on the value of $\tau$, computing which may be non-trivial. It only requires the ability to compute the ratio $\rho(x) = \frac{D_1(x)}{\tau D_2(x)}$ for each $x \in \X$. 
\end{remark}

In the current form, Algorithm \ref{alg:rej-sample} does not have any control over the number of samples from $D_2$ which it may need to examine. However, a bound on the number of samples to be examined can be used if we are content with sampling from a slightly perturbed distribution. Suppose we have another distribution $D_3$ over $\X$. This time we are allowed to use samples coming from $D_2$ and $D_3$ and instead of a sample from $D_1$, we are content with obtaining a sample generated by a hybrid distribution $D(x) = (1-\delta)D_1(x) + \delta D_3(x)$ for some small enough $\delta \in (0,1)$. For this we can modify Algorithm ~\ref{alg:rej-sample} to Algorithm~\ref{alg:rej-sample-m} as follows : 


\begin{algorithm}
    \caption{$\mathtt{RejectionSample}(m)$}
    \label{alg:rej-sample-m}
    \textbf{Input: } Samples generated from $D_2, D_3$ \\
    \textbf{Output: } A sample $x$ with probability  $D(x) = (1 - \delta)D_1(x) + \delta D_3(x)$ where $\delta \leq e^{-m/\tau}$ \\ 
    \begin{algorithmic}[1]
    \STATE $\mathtt{sampled = False}$ , $\mathtt{iter} = 0$
    \REPEAT
        \STATE $\mathtt{iter = iter+1}$
        \STATE $x \sim D_2$ , $r \sim [0,1]$
        \STATE Compute $\rho(x) = \frac{D_1(x)}{\tau D_2(x)}$
        \IF{$r \leq \rho(x)$}
            \STATE \textbf{output} $x$ and set $\mathtt{sampled= True} $
        \ENDIF
    \UNTIL{$\mathtt{sampled = True}$ \textbf{or} $\mathtt{iter} > m$}
    \IF{$\mathtt{sampled = False}$}
        \STATE \textbf{output} $x \sim D_3$
    \ENDIF
    
        
    \end{algorithmic}
\end{algorithm}


\begin{lemma}\label{lem: bounded oversampling lemma}
    Let $m > 0$ be the upper bound on the number of rounds in \rejsamp. The output samples come from a distribution $D$ which can be expressed as $D(x) =(1 - \delta)D_1(x) + \delta D_3(x)$ where $\delta \leq e^{-m/\tau}$.  
\end{lemma}
\begin{proof}
    A point is sampled from $D_3$ if and only if no sample is generated in $m$ rounds of rejection sampling. Hence, the probability of sampling a point $x \in \X$ is :
    \begin{align*}
    &\Pr[x \sim \mathtt{RejectionSample}(m)] \\
    &= \Pr[x | T \leq m] \Pr[T \leq m] + \Pr[x|T > m] \Pr[T > m] \\ 
    &=  (1 - \delta)D_1(x) + \delta D_3(x)
    \end{align*}
    where $\delta = \Pr[T > m] \leq e^{-m / \tau}$  from the proof of Lemma~\ref{lem:oversampling lemma}.
\end{proof}
\subsection{Application to \texorpdfstring{$\mathtt{RS}\text{-}\mathtt{k\text{-}means}$++}{rskmeans}}



Recall that our goal in \kpp is to sample the centers from the distribution $D^2(\X,S)$ over $\X$ given by $D^2(\X,S) = D_1(x) = \frac{\Delta(x,S)}{\Delta(\X,S)}$. In this work we present two methods, one which samples the centers from the $D^2$ distribution and another which samples from a slightly `perturbed' distribution $D(x) = (1-\delta)D_1(x) + \delta D_3(x)$ where $D_3(x) = \frac{1}{|\X|}$ is simply the uniform distribution over $\X$. We will use the $\mathtt{RejectionSample}$ and $\mathtt{RejectionSample}(m)$ for these methods respectively. In both cases we need to find a suitable distribution $D_2$ over $\X$ that $\tau$-\textit{oversamples} $D_1$ (for a suitable $\tau$) and for which we have an efficient method to obtain samples. 

\begin{lemma}\label{lem:overample D^2}
    Let $S = \{c_1,\ldots,c_t\} \subset \X$ be chosen according to the $D^2$ distribution (In particular, $c_1$ is a uniformly random point in $\X$). Let $D_2$ be the distribution over $\X$ defined by 
    \begin{align*}
        D_2(x) &= \frac{\|x\|^2 + \|c_1\|^2}{\|X\|^2 + |\X|\|c_1\|^2}
    \end{align*}
    Then $D_2$ $\tau$-oversamples $D^2(\X,S)$ for $\tau = 2\frac{\|X\|^2 + |\X|\|c_1\|^2}{\Delta(\X,S)}$.
\end{lemma}
\begin{proof}
    \begin{align*}
        \Delta(x,S) &= \min_{c\in S}\|x-c\|^2 \leq \|x-c_1\|^2 \leq 2(\|x\|^2+\|c_1\|^2)
    \end{align*}
    where the final inequality is obtained via the Cauchy-Schwarz inequality.
    Multiplying both sides by $\frac{1}{\Delta(\X,S)}$ gives the required result.
\end{proof}

An immediate issue is: how do we actually obtain samples from $D_2$? We will deal with this issue in a bit; for now, assume that we can efficiently obtain such samples after a preprocessing step. 

With this, we can apply Algorithm~\ref{alg:rej-sample} for $D_1$ being the required $D^2(\X,S)$ distribution and $D_2$ as in Lemma~\ref{lem:overample D^2}. It can be seen that for these distributions, Algorithm~\ref{alg:rej-sample} is equivalent to Procedure\ref{proc:sample} where $m=\infty$. Thus Lemma~\ref{lem:oversampling lemma} gives the following Corollary.

\begin{corollary}\label{cor:D^2 time}
Let $\epsilon \in (0,1)$ and $\X \subset \R^d$ be any dataset of $n$ points and $S = \{c_1,\ldots,c_t\} \subset \X$ such that $c_1$ is a uniformly random point in $\X$. Assume that we can obtain a sample from the following distribution over $\X$ in $O(\log{|\X|})$ time:
    \begin{align*}
        D_2(x) &= \frac{\|x\|^2 + \|c_1\|^2}{\|X\|^2 + |\X|\|c_1\|^2}
    \end{align*}
Then Procedure~\ref{proc:sample} outputs a sample $c\in\X$ according to the distribution $D^2(\X,S)$. Moreover, the computational cost of the algorithm is bounded by $O(\beta(\X)\cdotp (td + \log{|\X|})\cdotp\log{(1/\epsilon)})$ with probability atleast $1 - \epsilon$.  Here, $\beta(\X)$ is a parameter such that  $\E[\beta(\X)] \leq \frac{\Delta_1(\X)}{\Delta_k(\X)}$.
\end{corollary}
\begin{proof}
    By Lemma~\ref{lem:overample D^2} we know that $D_2$ $\tau$-oversamples $D^2(\X,S)$ for $\tau = 2\frac{\|X\|^2 + |\X|\|c_1\|^2}{\Delta(\X,S)} \leq 2\frac{\|X\|^2 + |\X|\|c_1\|^2}{\Delta_k(\X)}$. So
    \begin{align*}
        \E[\tau] &\leq 2\frac{\|X\|^2 + |\X|\E[\|c_1\|^2]}{\Delta_k(\X)}\\
        &= 2\frac{\|X\|^2 + |\X|\cdotp \frac{1}{|\X|}\sum_{x\in\X}\|x\|^2}{\Delta_k(\X)}\\
        &= \frac{4\|\X\|^2}{\Delta_k(\X)}\ = \frac{4\Delta_1(\X)}{\Delta_k(\X)}
    \end{align*}
    where the last inequality follows from the fact that $\X$ is translated so that its centroid is at the origin.

    Thus applying Lemma~\ref{lem:oversampling lemma} gives us the result for $\beta(\X) = \tau/4$.
\end{proof}

We can apply this $D^2$ sampling technique $k$ times to obtain the centers according to \kpp. This is what $\rskmeans(\X,k,\infty)$ does and it can be seen that Theorem~\ref{thm:second} follows from Corollary~\ref{cor:D^2 time}; in particular the approximation guarantee of the sampled centers follows from \cite{arthur_vassilvitskii_07}.
 
In our second approach, we replace $\mathtt{RejectionSample}$ by $\mathtt{RejectionSample}(m)$ which only repeats the rejection sampling loop $m$ times for a suitable choice of $m$. In particular, notice that Procedure~\ref{proc:sample} is essentially Algorithm~\ref{alg:rej-sample-m} for $D_1 = D^2(\X,S)$, $D_3$ being the uniform distribution over $\X$ and $D_2$ as in Lemma~\ref{lem:overample D^2}. 

This gives us the following corollary whose proof can be argued analogous to \ref{cor:D^2 time}:
\begin{corollary}\label{cor:delta D^2 time}
Let $m\in\N$ be a parameter, $\X \subset \R^d$ be any dataset of $n$ points and $S = \{c_1,\ldots,c_t\} \subset \X$ such that $c_1$ is a uniformly random point in $\X$. Assume that we can obtain a sample from the following distribution over $\X$ in $O(\log{|\X|})$ time:
    \begin{align*}
        D_2(x) &= \frac{\|x\|^2 + \|c_1\|^2}{\|X\|^2 + |\X|\|c_1\|^2}
    \end{align*}
Then Procedure~\ref{proc:sample} with input $(\X,S,m\log t)$ outputs a sample $c\in\X$ according to the distribution $(1-\delta)\cdotp D^2(\X,S) + \delta\cdotp\U[\X]$ for $\delta \leq e^{-m/\beta(\X)}$.  Moreover, the computational cost of the algorithm is bounded by $O(m\cdotp (td + \log{|\X|})\cdotp\log{t})$ with probability at least $1 - \epsilon$.  Here, $\beta(\X)$ is a parameter such that  $\E[\beta(\X)] \leq \frac{\Delta_1(\X)}{\Delta_k(\X)}$.
\end{corollary}

Again we can apply this sampling technique $k$ times to obtain $k$ centers. Note that this sampling is from a `perturbed' distribution from $D^2(\X,S)$, so the approximation guarantee no longer follows directly from \cite{arthur_vassilvitskii_07}. However we analyse this in Section~\ref{appendix:delta-k-means++} to get the following:

\begin{theorem}

Let $\X \subset \R^d$ be any dataset which is to be partitioned into $k$ clusters.  Let $S$ be the set of centers returned by $\delta\text{-}\mathtt{k\text{-}means\text{++}}(\X,k,\delta)$ for any $\delta \in (0,0.5)$ . The following approximation guarantee holds : 
$$ \E[\Delta(\X,S)] \leq 8(\ln k+2)\Delta_k(\X) + \frac{6k \delta}{1 - \delta} \Delta_1(\X) $$
\end{theorem}

which will finally prove Theorem~\ref{thm:main} after substituting value of the \textit{failure probability} $\delta$. 

In the following section we show how to obtain samples from $D_2$.

\subsection{Sampling from \texorpdfstring{$D_2$}{d2} via a Preprocessed Data Structure}\label{subsec:data structure}

Given $\X \subset \R^d$, consider the vector $v_{\X} \in \R^{|\X|}$ given by $v_{\X}(x) = \|x\|$. Define $D_\X(x) = \frac{\|x\|^2}{\|\X\|^2}$ as a distribution over $\X$. We will use a (complete) binary tree data structure to sample from $D_\X$. The leaves of the binary tree correspond to the entries of $v_\X$ and store weight $v_\X(x)^2$ along with the sign of $v_\X(x)$. The internal nodes also store a weight that is equal to the sum of weights of its children. To sample from $D_\X$, we traverse the tree, choosing either to go left or right at each node with probability proportional to the weight of its two children until reaching the leaves. The binary tree similarly supports querying and updating the entries of $v_\X$. 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% PIC.TEX



\begin{figure}[ht]
\centering
\begin{tikzpicture}[
  every node/.style = {font=\small},
  level 1/.style = {sibling distance=40mm, level distance = 10mm},
  level 2/.style = {sibling distance=20mm, level distance = 10mm},
  level 3/.style = {sibling distance=10mm, level distance = 10mm},
  edge from parent/.style = {draw, -latex}
]

\node {$\| v \|^2$}
  child {node {$v(1)^2 + v(2)^2$}
    child {node {$v(1)^2$}
      child {node {$\operatorname{sign}(v(1))$}}
    }
    child {node {$v(2)^2$}
      child {node {$\operatorname{sign}(v(2))$}}
    }
  }
  child {node {$v(3)^2 + v(4)^2$}
    child {node {$v(3)^2$}
      child {node {$\operatorname{sign}(v(3))$}}
    }
    child {node {$v(4)^2$}
      child {node {$\operatorname{sign}(v(4))$}}
    }
  };

\end{tikzpicture}

\caption{Data structure for sampling from a vector $v \in \R^4$}

\end{figure}









%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
We state this formally following \cite{tang_19}, in which such data structures, called  \textit{sample and query access data structures} were introduced. 

\begin{lemma} \label{lem:bst} (Lemma 3.1 in \cite{tang_19})
There exists a data structure storing a vector $v \in \R^n$ with $\nu$ nonzero entries in $\ O(\nu \log n)$ space which supports the following operations:
\begin{itemize}
    \item Reading and updating an entry of $v$ in $O(\log n)$ time
    \item Finding $\|v\|^2$ in $O(1)$ time
    \item Generating an independent sample $i \in \{1,\dots,n\}$ with probability $\frac{v(i)^2}{\sum_{j = 1}^n v(j)^2}$ in $O(\log n)$ time. 
\end{itemize}

\end{lemma}

Note that if $n$ is not a perfect power of $2$ then we can find a $n' \in \N$ which is a perfect power of 2 such that $n' < n < 2n' $. We can then set the remaining $2n' - n$ data points to have zero norm and use this dataset instead to construct the complete binary tree. Thus the following corollary is immediate.

\begin{corollary}\label{cor:preprocessing time}
    There is a data structure that can be prepared in $\tilde{O}(\nnz (\mathcal{X}))$ time which enables generating a sample from $D_\X$ in $O(\log |\X|)$ time.
\end{corollary}

We will now show how to sample from $D_2$ in $O(\log{|\X|})$ time by Procedure~\ref{alg:sample D_2}.

\begin{algorithm}
    \floatname{algorithm}{Procedure}
    \caption{$\mathtt{SampleDistribution}$}
    \label{alg:sample D_2}
    \textbf{Input: } A center $c \in \X$ \\
    \textbf{Output: } A sample according to the distribution $D_2$ defined as $D_2(x) = \frac{\|x\|^2 + \|c\|^2}{\|X\|^2 + |\X|\|c\|^2}$ \\
    \vspace{-12pt}
    \begin{algorithmic}[1]
    \STATE Generate $r \sim \calU[0,1]$
    \IF{$r \leq \frac{\|\X\|^2}{\|\X\|^2 + |\X|\|c\|^2 }$}
        \STATE Generate a sample $x \sim D_{\X}$ using the data structure from Section~\ref{subsec:data structure}
    \ELSE
        \STATE Generate a sample $x \sim \calU[\X]$
    \ENDIF
    \STATE \textbf{output} $x$
    \end{algorithmic}
\end{algorithm}


\begin{lemma}\label{lem:sample D_2}
    Procedure~\ref{alg:sample D_2} produces a sample from $\X$ according to the distribution $D_2$ as defined in Lemma~\ref{lem:overample D^2}. Moreover it takes $O(\log |\X|)$ time after a one time preprocessing time of $\tilde{O}(\nnz (\mathcal{X}))$.
\end{lemma}
\begin{proof}
    The probability of a sampled point is as follows:
    \begin{align*}
        \Pr[x] &= \Pr\left[r \leq \frac{\|\X\|^2}{\|\X\|^2 + n\|c_1\|^2 } \right]\Pr[x \sim D_{\X}]\\
        &\ \ \ \ + \Pr \left[ r > \frac{\|\X\|^2}{\|\X\|^2 + |\X|\|c_1\|^2} \right]\Pr[x \sim \calU[\X]] \\
 &= \frac{\|\X\|^2}{\|V\|^2 + |\X|\|c_1\|^2 } \frac{\|x\|^2}{\|\X\|^2}+ \frac{|\X|\|c_1\|^2}{\|\X\|^2 + |\X|\|c_1\|^2 } \frac{1}{|\X|} \\
 &= \frac{2(\|v_i\|^2 + \|c_1\|^2)}{2(\|\X\|^2 + |\X|\|c_1\|^2)} = D_{2}(x)
    \end{align*} 
    The time complexity follows from \ref{cor:preprocessing time}.
\end{proof}

\section{Analysis of \texorpdfstring{$\delta$-$k$-$\mathtt{means}$++}{deltakmeanspp}}\label{appendix:delta-k-means++}

In order to analyze the solution quality of \rskpp, we  examine an \textit{abstract} variant of \kpp which we call $\delta$-\kpp. Instead of sampling from the $D^2$-distribution as in \kpp, we sample from a distribution which is a weighted average of the $D^2$-distribution and the uniform distribution with weights $(1 - \delta)$ and $\delta$ respectively. We refer to this distribution on $\X$ as $D^2_\delta(\X,S)$ for some set of centers $S \subset \X$ . When clear from context, we just use $D^2_\delta$.  


\begin{algorithm}[h]
   \caption{$\delta\text{-}\mathtt{k\text{-}means\text{++}}(\X,k,\delta)$}
   \label{alg:delta-kpp}
   \textbf{Input :} dataset $\X \subset \R^d$, number of clusters $k \in \N $ and parameter $\delta \in (0,1)$ \\ 
   \textbf{Output :} $S = \{ c_1, \dots, c_k\} \subset \X$
\begin{algorithmic}[1]
   \STATE Choose $c_1 \in \X$ uniformly at random and set $S \gets \{c_1\}$
   \FOR{$t \in \{2,\dots,k\}$}
    \STATE Chose a point $x \in \X$ with prob.  $(1 - \delta) \frac{\Delta(x,S)}{\Delta(\X,S)} + \delta \frac{1}{|\X|}$
    \STATE $c_t \gets x$
   \STATE $S \gets S \cup\{c_t\}$
   \ENDFOR
   \STATE {\bfseries return} $S$
\end{algorithmic}
\end{algorithm}

The main objective of this section is to prove the following :

\begin{theorem} \label{thm:delta-k-means}

Let $\X \subset \R^d$ be any dataset which is to be partitioned into $k$ clusters.  Let $S$ be the set of centers returned by $\delta\text{-}\mathtt{k\text{-}means\text{++}}(\X,k,\delta)$ for any $\delta \in (0,0.5)$ . The following approximation guarantee holds : 
$$ \E[\Delta(\X,S)] \leq 8(\ln k+2)\Delta_k(\X) + \frac{6k \delta}{1 - \delta} \Delta_1(\X) $$
    
\end{theorem}

Our analysis closely follows the potential based approach of \cite{dasgupta_13}. Since we sample from a different distribution as compared to the standard \kpp, its analysis does not directly carry over to \dkpp . Indeed, it is known that the \kpp procedure is quite sensitive to even small changes in the $D^2$ distribution. This was first studied by \cite{bhattacharya_20} who were able to show only a $O(\log^2 k)$ guarantee when the centers are sampled from a distribution which is $\eps$-close  \footnote{Let $p(\cdot)$ and $p'(\cdot)$ represent probability mass functions over $\X$. $p'$ is said to be $\eps$-close to $p$ if $|p'(x) - p(x)| \leq \eps p(x) \, \forall \, x \in \X$. }  to the exact $D^2$ distribution for a sufficiently small constant $\eps$  . This result was  recently improved upon by \cite{grunau_23} who recover the tight $O(\log k)$ guarantee of \kpp.  In their analysis, \cite{grunau_23} incur a very large constant multiplicative blow-up \footnote{The constant blow-up of \cite{grunau_23} is bounded above by $\sum_{\ell = 1}^{\infty} 90 \ell e^{-\frac{\ell-1}{40}} = \frac{90}{(1 - e^{-1/40})^2 } \simeq 1.48 \times 10^5$ } in the approximation guarantee and leave it as an open problem to show whether the true approximation guarantee can be bounded by a multiplicative  factor of $1 + O(\eps)$.  In contrast, we show that the approximation guarantee of \dkpp consists of  an additive  scale invariant variance factor proportional to $\delta$  in addition to the standard guarantee of \kpp with the same constants.




\subsection{Some Useful Lemmas}

In this section, we state some crucial lemmas that shall be helpful in the analysis. Throughout our work, the centroid of a set of points $\mathcal{P} \subset \R^d$ is denoted by $\mu(\mathcal{P}) = \frac{1}{|\mathcal{P}|} \sum_{p \in \mathcal{P}} p$. 

The following folklore lemma is analogous to the bias-variance decomposition in machine learning.

\begin{lemma} \label{lem:bias-variance} 
For any set of points $\P \subset\R^d$ and any point $z \in \R^d$ (possibly not in $\P$), the following holds : 

$$ \Delta(\P,z) = \Delta(\P,\mu(\P)) + |\P|\|z - \mu(\P)\|^2 $$
    
\end{lemma}

This shows that the solution for the $\mathtt{1\text{-}means}$ problem is the centroid of the data set i.e, $\Delta_1(\P) = \Delta(\P,\mu(\P))$. The above lemma can be easily used to show the following. 

\begin{lemma} \label{lem:2-approx}
(Lemma 3.1 in \cite{arthur_vassilvitskii_07}) For any set of points $\P \subset \R^d$, if a point $z \in \P$ is chosen uniformly at random, then the following holds : 

$$ \E[\Delta(\P,z)] = 2 \Delta(\P,\mu(\P)) $$
    
\end{lemma}

We now state some useful bounds on the probability that a point is chosen from the $\D^2_\delta$ distribution with respect to some centers $S$ conditioned on it coming from a particular subset of points. For a point $z \in \R^d$ (possibly chosen randomly from some probability distribution) and a set of points $\P \subset \R^d$, we denote the event $\{z \in \P\}$ by $\chi_{\P}(z)$. 

\begin{lemma} \label{lem:bounds} Let $\P \subset\R^d$ be a set of points with  $\Q \subset \P$ being an arbitrary subset of $\P$ such that $|\Q| \neq 0$. Let $S \subset \P$ be a set of cluster centers. For any point $z \in \Q$ and parameter $\delta \in (0,1)$, the following hold :  
\begin{enumerate}
    \item $\Pr[z \sim \D^2_\delta | \chi_\Q(z)] \leq \frac{\Delta(z)}{\Delta(\Q)} + \frac{\delta}{1 - \delta} \frac{1}{|\P|} \frac{\Delta(\P)}{\Delta(\Q)}$
    \item $\Pr[z \sim \D^2_\delta | \chi_\Q(z)] \geq \frac{\Delta(z)}{\Delta(\Q)} - \frac{\delta}{1 - \delta} \frac{|\Q|}{|\P|} \frac{\Delta(z) \Delta(\P)}{\Delta(\Q)^2}$
\end{enumerate}
Here,  $\Delta(\cdot)$ denotes $\Delta(\cdot, S)$ for simplicity. 


\end{lemma}

\begin{proof}
     The probability that a chosen point belongs to $\Q$ is 
    \begin{align*}
     \Pr[z \sim \D^2_\delta \cap \chi_\Q(z)] &= \sum_{q \in \Q} \left( (1 - \delta) \frac{\Delta(q)}{\Delta(\P)} + \delta \frac{1}{|\P|} \right) 
 \\ &= (1 - \delta) \frac{\Delta(\Q)}{\Delta(\P)} + \delta \frac{|\Q|}{|\P|} 
     \end{align*}
Hence the required conditional probability is 
\begin{align*}
    \Pr[z \sim \D^2_\delta | \chi_\Q(z)] &= \frac{(1 - \delta) \frac{\Delta(z)}{\Delta(\P)} + \delta \frac{1}{|\P|}}{(1 - \delta) \frac{\Delta(\Q)}{\Delta(\P)} + \delta \frac{|\Q|}{|\P|}}
\end{align*}
For 1. we have : 
\begin{align*}
    \frac{(1 - \delta) \frac{\Delta(z)}{\Delta(\P)} + \delta \frac{1}{|\P|}}{(1 - \delta) \frac{\Delta(\Q)}{\Delta(\P)} + \delta \frac{|\Q|}{|\P|}} &\leq \frac{(1 - \delta) \frac{\Delta(z)}{\Delta(\P)} + \delta \frac{1}{|\P|}}{(1 - \delta) \frac{\Delta(\Q)}{\Delta(\P)}} \\ 
    &= \frac{\Delta(z)}{\Delta(\Q)} + \frac{\delta}{1 - \delta} \frac{1}{|\P|} \frac{\Delta(\P)}{\Delta(\Q)}
\end{align*}
For 2. we have : 
\begin{align*}
    \frac{(1 - \delta) \frac{\Delta(z)}{\Delta(\P)} + \delta \frac{1}{|\P|}}{(1 - \delta) \frac{\Delta(\Q)}{\Delta(\P)} + \delta \frac{|\Q|}{|\P|}} &= \frac{\Delta(z)}{\Delta(\Q)} \left( \frac{1 + \frac{\delta}{1 - \delta} \frac{\Delta(\P)}{|\P| \Delta(z)}}{ 1 + \frac{\delta}{1 - \delta} \frac{|\Q| \Delta(\P)}{|\P| \Delta(Q)}} \right) \\ &\geq \frac{\Delta(z)}{\Delta(\Q)} \left( { 1 + \frac{\delta}{1 - \delta} \frac{|\Q| \Delta(\P)}{|\P| \Delta(Q)}} \right)^{-1} \\ 
    &\geq \frac{\Delta(z)}{\Delta(\Q)} \left( { 1 - \frac{\delta}{1 - \delta} \frac{|\Q| \Delta(\P)}{|\P| \Delta(Q)}} \right)
\end{align*}

where in the last step we used the fact that $ (1 + x)^{-1} \geq 1 - x$ for any $x \geq 0$. This completes the proof of the lemma. 
\end{proof}

Recall that $\opt_k = \{ c_1^*, \dots c_k^*\}$ represented the optimal set of centers of the $\mathtt{k}\text{-}\mathtt{means}$ problem for the dataset $\X$. For a center $c_i \in \opt_k$, let us denote the set of all points in $\X$ closer to $c_i$ than any other center in $\opt_k$ by $\C_i$. Note  $c_i^* = \mu(\C_i)$ from Lemma~\ref{lem:bias-variance}. 

Next, we show a bound on the expected cost of a cluster $\C_i$ of $\opt_k$ when a point is added to the current set of clusters from $\C_i$ through the $D^2_\delta$ distribution. The following is  analogous to Lemma 3.2 of \cite{arthur_vassilvitskii_07} with an additional factor depending on $\delta$. 

\begin{lemma} \label{lem:cluster-bound}
Let $\X \subset \R^d$ be any dataset. Suppose we have a set of already chosen cluster centers $S$ and  a new center $z$ is added to $S$ from  the set of points $\C_i$ in the cluster corresponding to some $c_i \in \opt_k$ through the $D^2_\delta(\X,S)$ distribution. The following holds : 
\begin{align*}
    \E[\Delta(\C_i,S\cup\{z\}) | S, \chi_{\C_i}(z)] \leq 8 \Delta(\C_i,\mu(\C_i)) + \frac{\delta}{1 - \delta} \frac{|\C_i|}{|\X|} \Delta(\X,S)
\end{align*}
\end{lemma}

\begin{proof}
    When the new center $z$ is added, each point $x \in \C$ contributes $$ \Delta(x, S \cup\{z\}) = \min(\|x-z\|^2, \Delta(x,S))$$ to the overall cost. The expected cost of the cluster $\C$ can hence be written as : 
    \begin{align*}
        &\E[\Delta(\C_i,S\cup\{z\}) | S, \chi_{\C_i}(z)]  \\ &= \sum_{z \in \C_i} \Pr[z \sim \D^2_\delta | S, \chi_{\C_i}(z)] \sum_{x \in \C_i}\Delta(x, S \cup\{z\}) \\ 
        &\leq \sum_{z \in \C_i}  \frac{\Delta(z,S)}{\Delta(\C_i,S)} \sum_{x \in \C_i} \min(\|x-z\|^2, \Delta(x,S))\\ 
        &+ \sum_{z \in \C_i} \left(\frac{\delta}{1 - \delta} \frac{1}{|\X|} \frac{\Delta(\X,S)}{\Delta(\C_i,S)}  \right) \sum_{x \in \C_i} \min(\|x-z\|^2, \Delta(x,S))
    \end{align*}
    where in the last step we used item (i) of Lemma~\ref{lem:bounds}. From Lemma 3.2 of \cite{arthur_vassilvitskii_07}, the first term is bounded above by $8 \Delta(\C_i,\mu(\C_i))$. Let us focus on the second term. Noting that 
    $$ \sum_{x \in \C_i} \min(\|x-z\|^2, \Delta(x,S)) \leq \sum_{x \in \C_i}\Delta(x,S) = \Delta(\C_i,S)  $$
    the second term can be  bounded above by the following : 
    $$ \sum_{z \in \C_i} \left(\frac{\delta}{1 - \delta} \frac{1}{|\X|} \frac{\Delta(\X,S)}{\Delta(\C_i,S)}  \right)\Delta(\C_i,S)  = \frac{\delta}{1 - \delta} \frac{|\C_i|}{|\X|} \Delta(\X,S
)  $$
from which the lemma follows.     
\end{proof}


\subsection{Main Analysis}

Before getting into the proof, let us set up some notation. 

\textbf{Notation.} Let $t \in \{1,\dots.k\}$ denote the number of centers already chosen by \dkpp. Let $S_t \coloneqq \{c_1, \dots, c_t\}$ be the set of centers after iteration $t$ . We say that a cluster $\C_i$ of $\opt_k$ is \textit{covered} by $S_t$ if at least one of its centers is in $\C_i$. If not, then it is \textit{uncovered}. We denote $$H_t = \{i \in \{1,\dots,k\} : \C_i \cap S_t \neq \emptyset \} ,\, U_t = \{1,\dots,k\} \backslash H_t$$ Similarly, the dataset $\X$ can be partitioned in two parts : $\H_t \subset \X$ being the points belonging to \textit{covered} clusters and $\U_t = \X \backslash \H_t $ being the points belonging to \textit{uncovered} clusters. Let $W_t = t - |H_t|$ denote the number of \textit{wasted} iterations so far i.e, the number of iterations in which no new cluster was covered. Note that we always have $ |H_t| \leq t$ and hence $|U_t| \geq k-t$. For any $\P \subset \X$, we use the notation $\Delta^t(\P) := \Delta(\P, S_t)$ for brevity. 

The total cost can be decomposed as : 

$$ \Delta^t(\X) = \Delta^t(\H_t) + \Delta^t(\U_t) $$

We can use Lemma~\ref{lem:cluster-bound} to bound the first term directly. 

\begin{lemma} \label{lem:cost-H}
    For each $t \in \{1,\dots,k\}$ the following holds : 
    $$ \E[\Delta^t(\H_t)] \leq 8 \Delta_k(\X) + \frac{2\delta}{1 - \delta} \Delta_1(\X) $$
\end{lemma}

\begin{proof}
    \begin{align*}
        \E[\Delta^t(\H_t)] &= \E[\sum_{i \in H_t} \Delta^t(\C_i)] \leq \sum_{i = 1}^k \E[\Delta^t(\C_i)] \\ 
        &\leq 8 \Delta_k(\X) + \frac{\delta}{1 - \delta} \E[\Delta^t(\X)] \\ 
        &\leq 8 \Delta_k(\X) + \frac{2\delta}{1 - \delta} \Delta_1(\X)
    \end{align*}
    Where in the last line we used Lemma~\ref{lem:2-approx}  and the fact that the first center is chosen uniformly at random from $\X$.
\end{proof} 


\textbf{Potential function.}
To bound the second term i.e, the cost of the uncovered clusters we use the potential function introduced in \cite{dasgupta_13} : 

$$ \Psi_t = \frac{W_t}{|U_t|} \Delta^t(\U_t) $$

Instead of \textit{paying} the complete clustering cost $\Delta^k(\X)$ at once, we make sure that at the end of iteration $t$, we have payed an amount of atleast $\Psi_t$ .  Observe that when $t = k$, we have $W_t = |U_t|$ so the potential becomes $\Delta^k(\U_k)$, which is indeed the total cost of the uncovered clusters returned by \rskpp. We now show how to bound the expected increase in the potential i.e, $\Psi_{t+1} - \Psi_t$. To do this, we shall analyze the error propagation due to using the $D^2_\delta$ distribution instead of the $D^2$ distribution on the way. 

\textbf{Bounding the Increments.} Suppose $t$ centers have been chosen. The next center $c_{t+1}$ is chosen which belongs to some optimal cluster $\C_i$. We consider two cases : the first case is when $i \in U_t$ i.e, a new cluster is covered and the the second case is when $i \in H_t$ i.e, the center is chosen from an already covered cluster. We shall denote all the set of all random variables after the end of iteration $t$ by $\F_t$.

\begin{lemma} \label{lem:uncovered}
For any $t \in \{1,\dots,k-1\}$, the following holds : 
\begin{align*}
 \E[\Psi_{t+1} - \Psi_t &| \F_t, \chi_{U_t}(i)] \\ &\leq  \frac{2\delta}{1 - \delta} \frac{t}{\max(1,k-t-1)^2} \Delta_1(\X) 
\end{align*}
\end{lemma}

\begin{proof}
    When $i \in U_t$, we have $W_{t+1} = W_t$, $H_{t+1} = H_t \cup\{i\}$ and $U_{t+1} = U_{t} \backslash\{i\}$. Thus, 
    \begin{align*}
        \Psi_{t+1} = \frac{W_{t+1}}{|U_{t+1}|} \Delta^{t+1}(\U_{t+1}) \leq \frac{W_t}{|U_t| - 1} \left( \Delta^t(\U_t) - \Delta^t(\C_i) \right)
    \end{align*}
    We can use item (ii) of Lemma~\ref{lem:bounds} for getting a lower bound on the second term : 
    \begin{align*}
        &\E[\Delta^t(\C_i) | \F_t , \chi_{U_t}(i)] \\ &\geq  \sum_{j \in U_t} \left( \frac{\Delta^t(\C_j)}{\Delta^t(\U_t)} - \frac{\delta}{1 - \delta} \frac{|\C_j|}{|\U_t|} \frac{\Delta^t(\C_j)\Delta^t(\X)}{\Delta^t(\U_t)^2} \right) \Delta^t(\C_j)  \\ 
        &\geq \left( 1 - \frac{\delta}{1- \delta} \frac{\Delta^t(\X)}{\Delta^t(\U_t)} \right) \sum_{j \in U_t} \frac{\Delta^t(\C_j)^2}{\Delta^t(\U_t)}
    \end{align*}
    Where in the second step we used the fact that $|\C_j| \leq |\U_t|$ for each $j \in U_t$. We can use the cauchy-schwarz \footnote{ for lists of numbers $a_1,\dots,a_m$ and $b_1,\dots, b_m$ we have $\left(\sum_i a_ib_i\right)^2 \leq \left(\sum_i a_i^2\right)\left(\sum_i b_i^2\right)$} inequality to simplify the last expression as follows : 
    $$ |U_t|^2\sum_{j \in U_t} \Delta^t(\C_j)^2 \geq |U_t| \sum_{j \in U_t} \Delta^t(\C_j) = |U_t| \Delta^t(\U_t)  $$
    This shows that 
    $$  \E[\Delta^t(\C_i) | \F_t , \chi_{U_t}(i)]  \geq \frac{\Delta^t(\U_t)}{|U_t|} - \frac{\delta}{1 - \delta} \frac{\Delta^t(\X)}{|U_t|}$$
    Now, 
    \begin{align*}
        \E[\Psi_{t+1} | &\F_t, \chi_{U_t}(i)] \\ &\leq \frac{W_t}{|U_t| - 1}  \left(\Delta^t(\U_t) - \E[\Delta^t(\C_i)| \F_t, \chi_{U_t}(i)] \right) \\ 
        &\leq \frac{W_t}{|U_t| - 1}  \left(\Delta^t(\U_t) - \frac{\Delta^t(\U_t)}{|U_t|} + \frac{\delta}{1 - \delta} \frac{\Delta^t(\X)}{|U_t|}  \right) \\ 
        &= \Psi_t + \frac{\delta}{1 - \delta} \frac{W_t}{|U_t|\left(|U_t|-1\right)} \Delta^t(\X)
    \end{align*}
    Recall that $W_t \leq t$ and $|U_t| \geq k-t$. So for $t \leq k-2$, the following holds after taking expectation : 
    \begin{align*}
        &\E[\Psi_{t+1} - \Psi_t| \F_t, \chi_{U_t}(i)] \\ &\leq \frac{\delta}{1 - \delta} \frac{t}{(k-t-1)^2} \E[ \Delta^t(\X) | \F_t, \chi_{U_t}(i)] \\ 
        &\leq \frac{\delta}{1 - \delta} \frac{t}{(k-t-1)^2} \E[ \Delta^t(\X)] \\ 
        &\leq \frac{2\delta}{1 - \delta} \frac{t}{(k-t-1)^2} \Delta_1(\X) 
    \end{align*}
    Now consider the case when $t= k-1$. We cannot use the above argument directly because it may so happen that $|U_{k}| = 0$. If this happens, the potential of the uncovered clusters is always $0$ . This only happens when a new cluster is covered in each iteration. Let this event be $\allc$ (for \textit{All Clusters} being covered). Denoting $ \mathcal{E} = \F_{k-1}, \chi_{U_{k-1}(i)}$ we have the following : 
    \begin{align*}
        \E[\Psi_{k} - \Psi_{k-1} | \calE] &= \E[\Psi_{k} - \Psi_{k-1} | \calE,\allc ] \Pr[\allc | \calE]\\ 
        &\ \ \ \ + \E[\Psi_{k} - \Psi_{k-1} | \calE, \neg \allc ] \Pr[ \neg \allc | \calE] \\
        &\leq \E[\Psi_{k} - \Psi_{k-1} | \calE, \neg \allc ] \Pr[ \neg \allc | \calE] \\ 
        &\leq \E[\Psi_{k} - \Psi_{k-1} | \calE, \neg \allc ] \\
        &\leq \frac{2\delta t}{1 - \delta} \Delta_1(\X)
    \end{align*}
    Where in the last line we used the fact that $|U_{k-1}| > 1$ if all clusters are not covered. Combining both cases completes the proof.
\end{proof}


\begin{lemma} \label{lem:covered}
For any $t \in \{1,\dots,k-1\}$, the following holds : 
           $$ \E[\Psi_{t+1} - \Psi_t | \F_t, \chi_{H_t}(i)] \leq \frac{\Delta^t(\U_t)}{k-t} $$
\end{lemma}
\begin{proof}
    When $i \in H_t$, we have $H_{t+1} = H_t$, $W_{t+1} = W_t + 1$ and $U_{t+1} = U_t$. Thus,
    \begin{align*}
        \Psi_{t+1} - \Psi_t &= \frac{W_{t+1}}{|U_{t+1}|} \Delta^{t+1}(\U_{t+1}) - \frac{W_{t}}{|U_{t}|} \Delta^t(\U_{t}) \\ 
        &\leq \frac{W_{t}+1}{|U_{t}|} \Delta^t(\U_{t})-\frac{W_{t}}{|U_{t}|} \Delta^t(\U_{t}) \\ 
        &= \frac{\Delta^t(\U_t)}{|U_t|} \leq \frac{\Delta^t(\U_t)}{k-t}
    \end{align*}
\end{proof}

We can now combine the two cases to get : 

\begin{lemma} \label{lem:combined}
For any $t \in \{1,\dots,k-1\}$, the following holds : 
    \begin{align*}
        &\E[\Psi_{t+1} - \Psi_t | \F_t] \leq  (1 - \delta) \frac{\E[\Delta^t(\H_t)]}{k-t} \\ &+ \delta\left(\frac{2}{k-t} + \frac{2t}{\max(1,k-t-1)^2}\right) \Delta_1(\X)
    \end{align*}
\end{lemma}

\begin{proof}
To compute the overall expectation, we have : 
\begin{align*}
    &\E[\Psi_{t+1}-\Psi_t | \F_t] 
    = \E[\Psi_{t+1}-\Psi_t | \F_t, \chi_{U_t}(i)] \Pr[\chi_{U_t}(i)] 
    \\ &+ \E[\Psi_{t+1}-\Psi_t | \F_t, \chi_{H_t}(i)] \Pr[\chi_{H_t}(i)]
\end{align*}
We can bound the first term using Lemma~\ref{lem:uncovered}
\begin{align*}
    &\E[\Psi_{t+1}-\Psi_t | \F_t, \chi_{U_t}(i)] \Pr[\chi_{U_t}(i)]  \\
    &\leq \E[\Psi_{t+1}-\Psi_t | \F_t, \chi_{U_t}(i)] \\ 
    &\leq \frac{2\delta}{1 - \delta} \frac{t}{\max(1,k-t-1)^2} \Delta_1(\X) 
\end{align*}
and the second term using Lemma~\ref{lem:covered}
\begin{align*}
    &\E[\Psi_{t+1}-\Psi_t | \F_t, \chi_{H_t}(i)] \Pr[\chi_{H_t}(i)] \\
    & \leq \frac{\Delta^t(\U_t)}{k-t} \left( (1 - \delta) \frac{\Delta^t(\H_t)}{\Delta^t(\X)} + \delta \frac{|\H_t|}{|\X|}  \right)\\
    & \leq (1 - \delta)\frac{\Delta^t(\H_t)}{k-t} + \delta \frac{\Delta^t(\X)}{k-t}
\end{align*}

Where in the last step we used $\Delta^t(\U_t) \leq \Delta^t(\X)$ and $|\H_t| \leq |\X|$. Combining both the terms completes the proof.
    
\end{proof}

We are now ready to provide a proof for Theorem~\ref{thm:delta-k-means}, which we state again :  
\begin{theorem} \label{thm:delta-k-means-duplicate}

Let $\X \subset \R^d$ be any dataset which is to be partitioned into $k$ clusters.  Let $S$ be the set of centers returned by $\delta\text{-}\mathtt{k\text{-}means\text{++}}(\X,k,\delta)$ for any $\delta \in (0,0.5)$ . The following approximation guarantee holds : 
$$ \E[\Delta(\X,S)] \leq 8(\ln k+2)\Delta_k(\X) + \frac{6k \delta}{1 - \delta} \Delta_1(\X) $$
    
\end{theorem}

\begin{proof}

At the end of $k$ iterations, we have $\Delta(\X,S) = \Delta^t(\H_t) + \Delta^t(\U_t) = \Delta^t(\H_t) + \Psi_k$. The first term can be bound using Lemma~\ref{lem:cost-H}. For the second term, we can express $\Psi_k$ as a telescopic sum : 
\begin{align*}
&\E[\Delta(\X,S)] = \E[\Delta^k(\H_k)] + \sum_{t = 0}^{k-1} \E [\Psi_{t+1} - \Psi_t | \F_t] \\ 
&\leq \E[\Delta^k(\H_k)] +  \sum_{t = 0}^{k-1}  (1 - \delta) \frac{\E[\Delta^t(H_t)]}{k-t} \\ &+  \sum_{t = 0}^{k-1}\delta \left( \frac{2}{k-t} + \frac{2t}{(1 - \delta) \max(1,k-t-1)^2} \right) \Delta_1(\X) \\ 
&\leq 8\Delta_k(\X)\left( 1 + (1 - \delta) \sum_{t = 0}^{k-1} \frac{1}{k-t}\right) + \\ 
& \frac{2\delta}{1 - \delta} \Delta_1(\X) \left(k + 2 \ln k  + \sum_{t=0}^{k-2} \frac{t}{(k-t-1)^2} \right)
\end{align*} 


To simplify this, note that  $\sum_{t = 0}^{k-1} \frac{1}{k-t} \leq 1 + \ln k $ ,  $\sum_{t = 0}^{k-2} \frac{t}{(k-t-1)^2} \leq k \sum_{n = 1}^{\infty}n^{-2} = \frac{\pi
^2}{6} k$ and $4 \ln k \leq \left(4 - \frac{\pi^2}{3} \right)k$ for sufficiently large $k$. Using these above we get our final bound : 

$$ \E[\Delta(\X,S)] \leq 8(\ln k+2)\Delta_k(\X) + \frac{6k \delta}{1 - \delta} \Delta_1(\X) $$

This completes the proof of the theorem. 
    
\end{proof}






\section{Experiments} \label{appendix:experiments}

\subsubsection*{Setup}
All the experiments were performed on a personal laptop with an Apple M3 Pro CPU chip, 11 cores and 18GB RAM. No dimensionality reduction was done on the datasets. No multi - core parallelization was used during the experiments. We have included the code for the experiments in the supplementary material.  

\subsubsection*{Datasets}
The data sets used for the experiments were taken from the annual KDD competitions and the UCI Machine Learning Repository. In the case that the data set consists of a train - test split, only the training data set without the corresponding labels was used for perform clustering. We also provide rough estimates of the $\beta$ parameters for the datasets used. These are computed by taking the ratio of the variance of the dataset with the average clustering cost of the solution output by $\rskmeans(\cdot, \cdot, \infty)$. 
\begin{table*}[ht]
\caption{Description of datasets used for experiments}
\label{tab:datasets}

\vskip 0.15in
\begin{centering}


\begin{tabular}{p{7cm}p{2cm}p{1cm}p{1cm}p{1cm}}
\toprule

{$\X$}  & {\sc  $n$} & {\sc $k$}  & { \sc $d$} & $\tilde{\beta}_k(\X)$\\
\midrule 
% \midrulehttps://www.overleaf.com/project/671b9780dc0cee89225e9479

{\sc Diabetes \cite{diabetes}}   & {\sc $253,680$} & {\sc $50$}  & { \sc $21$} & $\sim 6.5$\\

\midrule 

{\sc Forest \cite{forest}}   & {\sc $581,010$} & {\sc $7$}  & { \sc $54$} & $\sim 3.3$\\

\midrule 

{\sc Protein \cite{protein}}   & {\sc $145,751$} & {\sc $100$}  & { \sc $74$} & $\sim 9.7$ \\

\midrule 

{\sc Poker \cite{poker}}   & {\sc $1,025,010$} & {\sc $50$}  & { \sc $10$} & $\sim 2.4$\\

\midrule

{\sc Cancer \cite{cancer}}   & {\sc $94,730$} & {\sc $100$}  & { \sc $117$} & $\sim 1.9$\\



\bottomrule
\end{tabular}

\end{centering}
\end{table*}


\subsubsection*{Algorithms}

\begin{enumerate}
    \item $\rskmeans$ :  Our approach takes as input the parameter $m$ which is an upper bound on the number of iterations of rejection sampling. This provides a trade-off between computational cost and solution quality. We can also set $m = \infty$ to recover the $O(\log k)$ guarantee of $k$-\means++.
    \item $\afkmc$ : This is the Monte Carlo Markov Chain based approach of \cite{bachem_16a}. It also takes as input a parameter $m$ which is the length of the markov chain used for sampling.
\end{enumerate}

\begin{remark}
    We do not include comparisons with the algorithm of \cite{cohen-addad_20} since their techniques are algorithmically sophisticated including tree embeddings and LSH data structures for approximate nearest neighbor search. This incurs additional poly-logarithmic dependence on the aspect ratio of the dataset  and even $n^{O(1)}$ terms for performing a single clustering. Moreover, a publicly available implementation is not available to the best of our knowledge. Similar reasons are also mentioned in \cite{charikar_23} for not including this algorithm in their experiments as well. As for the algorithm of \cite{charikar_23} called $\prone$, it achieves an $O(k^4\log k)$ guarantee while running in expected time $O(n \log n)$ after $O(\nnz(\X))$ pre-processing. Due to the large approximation factor, \cite{charikar_23} suggest to use $\prone$ in a pipeline for constructing coresets instead of clustering the whole dataset. Moreover, the class of datasets targeted by both \cite{cohen-addad_20} and \cite{charikar_23} include the large $k(\sim 5 \times 10^3)$ regime, while our approach is more suitable for massive datasets where $n \gg k$. This is because the time taken by our algorithm to perform a single clustering is \textit{sublinear} in $n$, much like the results of \cite{bachem_16a}. Hence, we compare our approach with their $\afkmc$ algorithm. 
\end{remark}

\subsection*{Experiment 1}

In this experiment, we compare the performance of the default $\afkmc$ with $m = 200$ (as done by  \cite{bachem_16a} in their implementation) with the performance $\rskmeans$ without setting any upper bound for the number of iterations for the datasets given in Table~\ref{tab:datasets}. Recall that our algorithm does not require an estimate of $\beta$, thus making it free of any extra parameters which require tuning.  The algorithms were run for $20$ iterations for computing the averages and standard deviations. We also study the effect of varying the number of clusters $k \in \{5,10,20,50,100\}$ for each dataset. 

\begin{table*}[t]
\caption{Comparison of $\afkmc(\cdot, \cdot,200)$ with $\rskmeans(\cdot,\cdot, \infty)$}
\label{tab:exp1}
\begin{center}

\begin{scriptsize}
    

\begin{tabular}{p{2cm}p{1.5cm}p{1.5cm}p{1.5cm}p{1.5cm}p{1.5cm}p{1.5cm}}
\toprule

{\sc Name}  & {\sc $\rskmeans$ Cost} & {\sc $\afkmc$ Cost } &  {\sc $\rskmeans$ Std. Dev.}&  {\sc $\afkmc$ Std. Dev. }&{\sc $\rskmeans$ Time} & {\sc $\afkmc$ Time } \\

\midrule

{\sc Diabetes}  & $7.475 \times 10^6$ & $7.503 \times 10^6$& $3.23 \times 10^5$ & $3.13 \times 10^5$& $5.15 \times 10^{-1}$ & $1.02 \times 10^1$ \\ 
{\sc Forest}  & $7.707 \times 10^{11}$ & $7.748 \times 10^{11}$& $1.31 \times 10^{11}$ & $9.61 \times 10^{10}$& $1.48 \times 10^{-1}$ & $3.51 \times 10^0$ \\ 
{\sc Protein}  & $2.439 \times 10^{11}$ & $2.436 \times 10^{11}$& $4.09 \times 10^{10}$ & $4.44 \times 10^{10}$& $1.06 \times 10^{0}$ & $1.37 \times 10^1$ \\ 

{\sc Poker}  & $3.322 \times 10^7$ & $3.333 \times 10^7$ & $5.55 \times 10^5$ & $5.96 \times 10^5$ & $8.95 \times 10^{-1}$ & $5.43 \times 10^1$ \\


{\sc Cancer}  & $6.067 \times 10^6$ & $6.086 \times 10^6$ & $1.19 \times 10^5$ & $7.18 \times 10^4$ & $3.75 \times 10^{-1}$ & $9.69 \times 10^0$ \\
 



\bottomrule
\end{tabular}
\end{scriptsize}

\end{center}
\end{table*}

\begin{table*}[t]
\caption{Comparison of $\rskmeans$ and $\afkmc$ for different datasets.}
\label{tab:all_datasets}
\vskip 0.15in
\centering
\begin{scriptsize}
\begin{tabular}{p{1cm} p{0.8cm} p{1.5cm} p{1.5cm} p{1.5cm} p{1.5cm} p{1.5cm} p{1.5cm} p{1.5cm}  }
\toprule
{\sc Dataset} & {\sc $k$} & {\sc $\rskmeans$ Cost} & {\sc $\afkmc$ Cost} & {\sc $\rskmeans$ Std. Dev.} & {\sc $\afkmc$ Std. Dev.} & {\sc $\rskmeans$ Time} & {\sc $\afkmc$ Time} \\
\midrule
\multirow{5}{*}{Diabetes} 
& 5   & $2.847 \times 10^7$ & $3.089 \times 10^7$ & $3.59 \times 10^6$ & $4.92 \times 10^6$ & $2.32 \times 10^{-2}$ & $8.71 \times 10^{-1}$ \\ 
& 10  & $1.768 \times 10^7$ & $1.740 \times 10^7$ & $1.33 \times 10^6$ & $2.23 \times 10^6$ & $4.78 \times 10^{-2}$ & $1.99 \times 10^0$ \\ 
& 20  & $1.174 \times 10^7$ & $1.195 \times 10^7$ & $5.03 \times 10^5$ & $9.98 \times 10^5$ & $1.32 \times 10^{-1}$ & $4.15 \times 10^0$ \\ 
& 50  & $7.401 \times 10^6$ & $7.446 \times 10^6$ & $3.26 \times 10^5$ & $2.29 \times 10^5$ & $5.08 \times 10^{-1}$ & $1.03 \times 10^1$ \\ 
& 100 & $5.515 \times 10^6$ & $5.476 \times 10^6$ & $1.39 \times 10^5$ & $1.25 \times 10^5$ & $1.59 \times 10^0$ & $2.14 \times 10^1$ \\ 
\midrule
\multirow{5}{*}{\sc Forest} 
& 5   & $1.041 \times 10^{12}$ & $1.062 \times 10^{12}$ & $1.69 \times 10^{11}$ & $1.78 \times 10^{11}$ & $1.13 \times 10^{-1}$ & $2.31 \times 10^0$ \\ 
& 10  & $5.941 \times 10^{11}$ & $5.853 \times 10^{11}$ & $8.65 \times 10^{10}$ & $6.37 \times 10^{10}$ & $2.47 \times 10^{-1}$ & $5.41 \times 10^0$ \\ 
& 20  & $3.377 \times 10^{11}$ & $3.373 \times 10^{11}$ & $2.51 \times 10^{10}$ & $2.02 \times 10^{10}$ & $6.97 \times 10^{-1}$ & $1.16 \times 10^1$ \\ 
& 50  & $1.834 \times 10^{11}$ & $1.846 \times 10^{11}$ & $8.35 \times 10^9$ & $6.48 \times 10^9$ & $3.27 \times 10^0$ & $2.98 \times 10^1$ \\ 
& 100 & $1.221 \times 10^{11}$ & $1.221 \times 10^{11}$ & $2.64 \times 10^9$ & $3.41 \times 10^9$ & $1.01 \times 10^1$ & $5.85 \times 10^1$ \\ 
\midrule
\multirow{5}{*}{\sc Protein} 
& 5   & $1.048 \times 10^{12}$ & $1.085 \times 10^{12}$ & $2.88 \times 10^{11}$ & $3.00 \times 10^{11}$ & $2.48 \times 10^{-2}$ & $6.21 \times 10^{-1}$ \\ 
& 10  & $6.394 \times 10^{11}$ & $5.882 \times 10^{11}$ & $9.71 \times 10^{10}$ & $6.15 \times 10^{10}$ & $4.59 \times 10^{-2}$ & $1.40 \times 10^0$ \\ 
& 20  & $4.388 \times 10^{11}$ & $4.434 \times 10^{11}$ & $2.83 \times 10^{10}$ & $3.81 \times 10^{10}$ & $1.26 \times 10^{-1}$ & $2.93 \times 10^0$ \\ 
& 50  & $3.029 \times 10^{11}$ & $3.059 \times 10^{11}$ & $1.20 \times 10^{10}$ & $8.55 \times 10^9$ & $3.96 \times 10^{-1}$ & $7.59 \times 10^0$ \\ 
& 100 & $2.417 \times 10^{11}$ & $2.456 \times 10^{11}$ & $4.73 \times 10^9$ & $5.44 \times 10^9$ & $1.24 \times 10^0$ & $1.47 \times 10^1$ \\ 
\midrule
\multirow{5}{*}{\sc Poker} 
& 5   & $7.81 \times 10^7$ & $8.03 \times 10^7$ & $5.72 \times 10^6$ & $9.10 \times 10^6$ & $4.77 \times 10^{-2}$ & $3.41 \times 10^0$ \\ 
& 10  & $5.88 \times 10^7$ & $6.04 \times 10^7$ & $2.61 \times 10^6$ & $3.41 \times 10^6$ & $1.14 \times 10^{-1}$ & $8.13 \times 10^0$ \\ 
& 20  & $4.58 \times 10^7$ & $4.51 \times 10^7$ & $1.70 \times 10^6$ & $1.16 \times 10^6$ & $2.70 \times 10^{-1}$ & $1.64 \times 10^1$ \\ 
& 50  & $3.31 \times 10^7$ & $3.31 \times 10^7$ & $5.41 \times 10^5$ & $4.68 \times 10^5$ & $8.24 \times 10^{-1}$ & $4.06 \times 10^1$ \\ 
& 100 & $2.68 \times 10^7$ & $2.69 \times 10^7$ & $4.81 \times 10^5$ & $3.89 \times 10^5$ & $2.07 \times 10^0$ & $8.29 \times 10^1$ \\ 
\midrule
\multirow{5}{*}{\sc Cancer}
& $ 5$   & $1.21 \times 10^7$ & $1.23 \times 10^7$ & $1.03 \times 10^6$ & $1.17 \times 10^6$ & $1.96 \times 10^{-2}$ & $3.75 \times 10^{-1}$ \\ 
& $10$  & $1.07 \times 10^7$ & $1.06 \times 10^7$ & $5.96 \times 10^5$ & $7.44 \times 10^5$ & $2.46 \times 10^{-2}$ & $8.40 \times 10^{-1}$ \\ 
& $ 20$  & $8.83 \times 10^6$ & $8.75 \times 10^6$ & $4.02 \times 10^5$ & $4.05 \times 10^5$ & $3.89 \times 10^{-2}$ & $1.84 \times 10^0$ \\ 
& $ 50$  & $7.02 \times 10^6$ & $7.06 \times 10^6$ & $1.46 \times 10^5$ & $1.96 \times 10^5$ & $8.84 \times 10^{-2}$ & $4.77 \times 10^0$ \\ 
& $ 100$ & $6.08 \times 10^6$ & $6.06 \times 10^6$ & $1.06 \times 10^5$ & $7.22 \times 10^4$ & $3.69 \times 10^{-1}$ & $9.49 \times 10^0$ \\ 

\bottomrule
\end{tabular}
\end{scriptsize}

\end{table*}





\subsection*{Experiment 2}

In this experiment we study the convergence properties of $\rskmeans$. We plot the average clustering cost of the solutions output by $\rskmeans$ vs the time taken to compute these solutions and compare these with the baseline $k$-\means++ solution.  We also report $95\%$ confidence intervals in the plots over 40 iterations of the algorithms. The plots are generated by varying the upper bound on the number of rejection sampling iterations from $m \in \{ 5,10,20,50,75,100,125,150\}$. 

\begin{figure}[htbp]
\label{trade-off}
    \centering
    \includegraphics[width=0.45\textwidth]{plots/exp1.cancer._k=50_.png}
    \includegraphics[width=0.45\textwidth]{plots/exp1.cdc._k=50_.png}
    \includegraphics[width=0.45\textwidth]{plots/exp1.forest._k=7_.png}
    \includegraphics[width=0.45\textwidth]{plots/exp1.kdd._k=50_.png}
    \includegraphics[width=0.45\textwidth]{plots/exp1.poker._k=50_.png}
    \caption{Trade-off plots}
\end{figure}


\begin{table*}[t]
\caption{Data points for the trade-off plots}
\label{tab:rskmeanspp_results}
\vskip 0.15in
\centering
\begin{scriptsize}
\begin{tabular}{p{2cm}p{2cm}p{2.5cm}p{2.5cm}p{2.5cm}}
\toprule
{\sc Dataset} & {\sc $m$} & {\sc $\rskmeans$ Cost} & {\sc $\rskmeans$ Std. Dev.} & {\sc $\rskmeans$ Time} \\
\midrule
\multirow{5}{*}{\sc Diabetes} 
& 5   & $8.235 \times 10^6$  & $4.39 \times 10^5$  & $2.14 \times 10^{-1}$ \\ 
& 10  & $7.804 \times 10^6$  & $3.87 \times 10^5$  & $3.70 \times 10^{-1}$ \\ 
& 20  & $7.593 \times 10^6$  & $3.04 \times 10^5$  & $5.46 \times 10^{-1}$ \\ 
& 50  & $7.443 \times 10^6$  & $3.10 \times 10^5$  & $6.53 \times 10^{-1}$ \\ 
& 75  & $7.495 \times 10^6$  & $2.35 \times 10^5$  & $6.93 \times 10^{-1}$ \\ 
& 100 & $7.386 \times 10^6$  & $2.11 \times 10^5$  & $6.79 \times 10^{-1}$ \\ 
& 125 & $7.493 \times 10^6$  & $2.42 \times 10^5$  & $7.09 \times 10^{-1}$ \\ 
& 150 & $7.437 \times 10^6$  & $2.76 \times 10^5$  & $6.96 \times 10^{-1}$ \\ 
\midrule
\multirow{5}{*}{\sc Forest} 
& 5   & $8.504 \times 10^{11}$ & $1.32 \times 10^{11}$ & $9.76 \times 10^{-2}$ \\ 
& 10  & $8.375 \times 10^{11}$ & $1.55 \times 10^{11}$ & $1.15 \times 10^{-1}$ \\ 
& 20  & $8.122 \times 10^{11}$ & $1.46 \times 10^{11}$ & $1.19 \times 10^{-1}$ \\ 
& 50  & $7.798 \times 10^{11}$ & $1.15 \times 10^{11}$ & $1.32 \times 10^{-1}$ \\ 
& 75  & $7.816 \times 10^{11}$ & $9.64 \times 10^{10}$ & $1.39 \times 10^{-1}$ \\ 
& 100 & $7.504 \times 10^{11}$ & $9.85 \times 10^{10}$ & $1.39 \times 10^{-1}$ \\ 
& 125 & $7.775 \times 10^{11}$ & $1.03 \times 10^{11}$ & $1.40 \times 10^{-1}$ \\ 
& 150 & $7.932 \times 10^{11}$ & $9.67 \times 10^{10}$ & $1.40 \times 10^{-1}$ \\ 
\midrule
\multirow{5}{*}{\sc Protein} 
& 5   & $3.356 \times 10^{11}$ & $5.73 \times 10^{10}$ & $1.70 \times 10^{-1}$ \\ 
& 10  & $3.114 \times 10^{11}$ & $1.06 \times 10^{10}$ & $2.78 \times 10^{-1}$ \\ 
& 20  & $3.071 \times 10^{11}$ & $1.16 \times 10^{10}$ & $4.04 \times 10^{-1}$ \\ 
& 50  & $3.070 \times 10^{11}$ & $1.18 \times 10^{10}$ & $5.33 \times 10^{-1}$ \\ 
& 75  & $3.029 \times 10^{11}$ & $1.01 \times 10^{10}$ & $5.44 \times 10^{-1}$ \\ 
& 100 & $3.076 \times 10^{11}$ & $1.14 \times 10^{10}$ & $5.58 \times 10^{-1}$ \\ 
& 125 & $3.054 \times 10^{11}$ & $1.08 \times 10^{10}$ & $5.44 \times 10^{-1}$ \\ 
& 150 & $3.050 \times 10^{11}$ & $8.80 \times 10^9$  & $5.21 \times 10^{-1}$ \\ 
\midrule
\multirow{5}{*}{\sc Poker} 
& 5   & $3.35 \times 10^7$  & $5.65 \times 10^5$  & $4.88 \times 10^{-1}$ \\ 
& 10  & $3.36 \times 10^7$  & $6.27 \times 10^5$  & $6.90 \times 10^{-1}$ \\ 
& 20  & $3.33 \times 10^7$  & $7.06 \times 10^5$  & $8.41 \times 10^{-1}$ \\ 
& 50  & $3.33 \times 10^7$  & $6.91 \times 10^5$  & $9.32 \times 10^{-1}$ \\ 
& 75  & $3.33 \times 10^7$  & $5.91 \times 10^5$  & $8.65 \times 10^{-1}$ \\ 
& 100 & $3.32 \times 10^7$  & $5.65 \times 10^5$  & $8.82 \times 10^{-1}$ \\ 
& 125 & $3.34 \times 10^7$  & $6.21 \times 10^5$  & $9.34 \times 10^{-1}$ \\ 
& 150 & $3.34 \times 10^7$  & $6.21 \times 10^5$  & $9.34 \times 10^{-1}$ \\ 
\midrule
\multirow{5}{*}{\sc Cancer} 
& 5   & $7.17 \times 10^6$  & $2.60 \times 10^5$  & $7.80 \times 10^{-2}$ \\ 
& 10  & $7.06 \times 10^6$  & $1.68 \times 10^5$  & $7.68 \times 10^{-2}$ \\ 
& 20  & $7.05 \times 10^6$  & $1.95 \times 10^5$  & $8.58 \times 10^{-2}$ \\ 
& 50  & $7.05 \times 10^6$  & $1.85 \times 10^5$  & $9.53 \times 10^{-2}$ \\ 
& 75  & $7.13 \times 10^6$  & $2.41 \times 10^5$  & $9.02 \times 10^{-2}$ \\ 
& 100 & $7.10 \times 10^6$  & $1.71 \times 10^5$  & $9.15 \times 10^{-2}$ \\ 
& 125 & $7.09 \times 10^6$  & $2.24 \times 10^5$  & $7.99 \times 10^{-2}$ \\ 
& 150 & $7.10 \times 10^6$  & $2.04 \times 10^5$  & $8.97 \times 10^{-2}$ \\ 
\bottomrule
\end{tabular}
\end{scriptsize}

\end{table*}




\subsection*{Observations}

Based on the above experiments, we summarize our observations as follows : 

\begin{itemize}
    \item \textbf{Observation 1.} The data dependent parameter $\beta$ does not take on prohibitively large values. Indeed, for the data sets used in our experiments, these values are quite reasonable. This observation is in accordance with the experiments of \cite{bachem_16b}.  

    \item \textbf{Observation 2.} $\rskmeans$ provides solutions with comparable quality to $\afkmc$, while generally being much faster. On datasets like {\sc Poker} where the data size is much larger than the number of clusters, we observe a speedup of $\sim 40$ - $70 \times$. Moreover, this version of $\rskmeans$ does not require choosing any extra parameters as input. 

    \item \textbf{Observation 3.} The solution quality of $\rskmeans$ approaches that of $k$-\means++ rapidly with increase in the upper bound for the number of rejection sampling rounds allowed. This can be seen from the plots in Figure ~\ref{trade-off}


\end{itemize}

\section{Conclusion}

In this work, we present a simple rejection sampling approach to $k$-\means++ through the $\rskmeans$ algorithm. We show that our algorithm allows for new trade-offs between the computational cost and solution quality of the $k$-\means++ seeding procedure. It also has the advantage of supporting fast data updates and being easy to adapt in the parallel setting. The solution quality of $\rskmeans$ is bounded through the analysis of a \textit{perturbed} version of the standard $k$-\means++ method. The effectiveness of our approach is reflected in the experimental evaluations performed. Interesting future directions include the possibility of improving the dependence of the runtime - quality trade-off on the data dependent parameter. We believe that similar techniques could be adapted to the setting where the data set is present in the disk instead of the main memory and the goal is to minimize the number of disk accesses. 


\bibliographystyle{apalike}
\bibliography{refs}

\end{document}
