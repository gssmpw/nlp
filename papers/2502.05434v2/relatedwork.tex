\section{Related Works}
\textbf{Reinforcement Learning from Human Feedback (RLHF)}: RLHF has emerged as a critical approach in aligning AI systems with human values, especially in complex tasks where human feedback plays a crucial role \citep{achiam2023gpt,touvron2023llama}. The RLHF framework typically involves a three-stage process: supervised fine-tuning (SFT), reward modeling (RM), and reinforcement learning (RL) using algorithms like Proximal Policy Optimization (PPO)\citep{ouyang2022training,ziegler2019fine}.
%\cite{ouyang2022training,bai2022training,ziegler2019fine}
Direct Preference Optimization (DPO)~\citep{rafailov2024direct}  is another approach that directly uses generative models as reward models and trains them using preference data. 

The practical success of RLHF has also sparked a variety of theoretical studies. According to the type of preference feedback, these works can be roughly divided into two categories: \emph{action preference}~\citep{furnkranz2012preference,saha2021optimal,ji2024reinforcement,sekhari2024contextual,li2024feel,bai2025online} and \emph{trajectory preference}~\citep{busa2014preference,xu2020preference,pacchiano2021dueling,chen2022human,taranovic2022adversarial,wu2023making}. The literature  on action preferences is generally referred to as the \emph{contextual dueling bandits}.
In this paper, we focus on the trajectory preference. Most of the existing work in this area follow the OFU principle 
with the exception of~\citep{wu2023making,li2024feel} and~\citep{li2024feel}, who investigate a well-known Bayesian methodâ€”TS. Note that \citet{wu2023making} uses trajectory preferences and can be applied to the general function approximation framework while \citet{li2024feel} focuses on contextual dueling bandits. We also use the posterior sampling method, but unlike TS, our method follows the princple of  information-directed sampling.

\textbf{Information-Directed Sampling (IDS):} IDS is a design principle for sequential decision-making problems, which balances
exploration and exploitation by evaluating the information gain from each action or trajectory. \citet{russo2014learning} first introduces the IDS principle in the bandit setting. They decompose the Bayesian regret into a information ratio term and a cumulative information gain term, and bound the regret by tools from information theory. Based on their work, many studies use this method to analyze the regret of the TS algorithm in bandit settings \citep{russo2016information,dong2018information,bubeck2020first,liu2018information,kirschner2021asymptotically,hao2021information,hao2022contextual}. 

Recently,  \citet{hao2022regret,moradipari2023improved} study the Bayesian regret of IDS and TS without any prior assumptions for MDP settings. \citet{moradipari2023improved} focuses on analyzing TS in general settings while~\citet{hao2022regret} proposes a regularized-IDS algorithm for tabular and linear settings. \citet{zhang2024provably} uses the principle of IDS to design a   set of algorithms for multi-agent reinforcement learning.
They both use the surrogate environment as the learning target to get a sharper bound. However,  implementing the surrogate version of the algorithm is a  challenge. In this paper, we introduce IDS into RLHF for general MDP settings. We propose an easy-to-implement surrogate algorithm and prove that the regret upper bound has the same order as the original version.