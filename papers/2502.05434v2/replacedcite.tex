\section{Related Works}
\textbf{Reinforcement Learning from Human Feedback (RLHF)}: RLHF has emerged as a critical approach in aligning AI systems with human values, especially in complex tasks where human feedback plays a crucial role ____. The RLHF framework typically involves a three-stage process: supervised fine-tuning (SFT), reward modeling (RM), and reinforcement learning (RL) using algorithms like Proximal Policy Optimization (PPO)____.
%____
Direct Preference Optimization (DPO)____  is another approach that directly uses generative models as reward models and trains them using preference data. 

The practical success of RLHF has also sparked a variety of theoretical studies. According to the type of preference feedback, these works can be roughly divided into two categories: \emph{action preference}____ and \emph{trajectory preference}____. The literature  on action preferences is generally referred to as the \emph{contextual dueling bandits}.
In this paper, we focus on the trajectory preference. Most of the existing work in this area follow the OFU principle 
with the exception of____ and____, who investigate a well-known Bayesian methodâ€”TS. Note that ____ uses trajectory preferences and can be applied to the general function approximation framework while ____ focuses on contextual dueling bandits. We also use the posterior sampling method, but unlike TS, our method follows the princple of  information-directed sampling.

\textbf{Information-Directed Sampling (IDS):} IDS is a design principle for sequential decision-making problems, which balances
exploration and exploitation by evaluating the information gain from each action or trajectory. ____ first introduces the IDS principle in the bandit setting. They decompose the Bayesian regret into a information ratio term and a cumulative information gain term, and bound the regret by tools from information theory. Based on their work, many studies use this method to analyze the regret of the TS algorithm in bandit settings ____. 

Recently,  ____ study the Bayesian regret of IDS and TS without any prior assumptions for MDP settings. ____ focuses on analyzing TS in general settings while____ proposes a regularized-IDS algorithm for tabular and linear settings. ____ uses the principle of IDS to design a   set of algorithms for multi-agent reinforcement learning.
They both use the surrogate environment as the learning target to get a sharper bound. However,  implementing the surrogate version of the algorithm is a  challenge. In this paper, we introduce IDS into RLHF for general MDP settings. We propose an easy-to-implement surrogate algorithm and prove that the regret upper bound has the same order as the original version.