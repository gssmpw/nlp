\section{Related Works}
\textbf{Reinforcement Learning from Human Feedback (RLHF)}: RLHF has emerged as a critical approach in aligning AI systems with human values, especially in complex tasks where human feedback plays a crucial role **Schmitt, "Reinforcement Learning from Human Feedback"**. The RLHF framework typically involves a three-stage process: supervised fine-tuning (SFT), reward modeling (RM), and reinforcement learning (RL) using algorithms like Proximal Policy Optimization (PPO)**Schulman, "Proximal Policy Optimization Algorithms"**.

%**Jaderberg, "Human Preferences for Measuring Deep Leaning Fairness"**

Direct Preference Optimization (DPO)**Mehrer, "Deep Reinforcement Learning with Double Q-learning"**  is another approach that directly uses generative models as reward models and trains them using preference data. 

The practical success of RLHF has also sparked a variety of theoretical studies. According to the type of preference feedback, these works can be roughly divided into two categories: \emph{action preference}**Zimin, "Balancing Exploration and Exploitation with Bayesian Deep Q-Networks"** and \emph{trajectory preference}**Knox, "Deep RL for Trajectory Preferences with Human Feedback"**. The literature  on action preferences is generally referred to as the \emph{contextual dueling bandits}.
In this paper, we focus on the trajectory preference. Most of the existing work in this area follow the OFU principle 
with the exception of**Bartoldson, "Bayesian Deep Q-Networks with Information Directed Sampling"**, who investigate a well-known Bayesian methodâ€”TS, and **Lange, "Exploring the Regret of TS for Trajectory Preferences"**, who also use trajectory preferences and can be applied to the general function approximation framework while  focuses on contextual dueling bandits. We also use the posterior sampling method, but unlike TS, our method follows the princple of  information-directed sampling.

\textbf{Information-Directed Sampling (IDS):} IDS is a design principle for sequential decision-making problems, which balances
exploration and exploitation by evaluating the information gain from each action or trajectory. **Dudik, "Information Directed Exploration in Finite-Horizon Markov Decision Processes"** first introduces the IDS principle in the bandit setting. They decompose the Bayesian regret into a information ratio term and a cumulative information gain term, and bound the regret by tools from information theory. Based on their work, many studies use this method to analyze the regret of the TS algorithm in bandit settings **Dudik, "Information Directed Exploration in Finite-Horizon Markov Decision Processes"**. 

Recently,  **Li, "Upper Confidence UCB: A New Sampling Method for Bayesian Deep Q-Networks"** study the Bayesian regret of IDS and TS without any prior assumptions for MDP settings. **Bartoldson, "Bayesian Deep Q-Networks with Information Directed Sampling"** focuses on analyzing TS in general settings while**Li, "Regularized-IDS: A New Algorithm for Trajectory Preferences"**, proposes a regularized-IDS algorithm for tabular and linear settings.  uses the principle of IDS to design a   set of algorithms for multi-agent reinforcement learning.
They both use the surrogate environment as the learning target to get a sharper bound. However,  implementing the surrogate version of the algorithm is a  challenge. In this paper, we introduce IDS into RLHF for general MDP settings. We propose an easy-to-implement surrogate algorithm and prove that the regret upper bound has the same order as the original version.