@article{achiam2023gpt,
  title={Gpt-4 technical report},
  author={Achiam, Josh and Adler, Steven and Agarwal, Sandhini and Ahmad, Lama and Akkaya, Ilge and Aleman, Florencia Leoni and Almeida, Diogo and Altenschmidt, Janko and Altman, Sam and Anadkat, Shyamal and others},
  journal={arXiv preprint arXiv:2303.08774},
  year={2023}
}

@article{bai2022training,
  title={Training a helpful and harmless assistant with reinforcement learning from human feedback},
  author={Bai, Yuntao and Jones, Andy and Ndousse, Kamal and Askell, Amanda and Chen, Anna and DasSarma, Nova and Drain, Dawn and Fort, Stanislav and Ganguli, Deep and Henighan, Tom and others},
  journal={arXiv preprint arXiv:2204.05862},
  year={2022}
}

@inproceedings{bai2025online,
  title={Online Preference Alignment for Language Models via Count-based Exploration},
  author={Bai, Chenjia and Zhang, Yang and Qiu, Shuang and Zhang, Qiaosheng and Xu, Kang and Li, Xuelong},
  booktitle={International Conference on Learning Representations (ICLR)},
  year={2025}
}

@inproceedings{bubeck2020first,
  title={First-order bayesian regret analysis of thompson sampling},
  author={Bubeck, S{\'e}bastien and Sellke, Mark},
  booktitle={Algorithmic Learning Theory},
  pages={196--233},
  year={2020},
  organization={PMLR}
}

@article{busa2014preference,
  title={Preference-based reinforcement learning: evolutionary direct policy search using a preference-based racing algorithm},
  author={Busa-Fekete, R{\'o}bert and Sz{\"o}r{\'e}nyi, Bal{\'a}zs and Weng, Paul and Cheng, Weiwei and H{\"u}llermeier, Eyke},
  journal={Machine learning},
  volume={97},
  pages={327--351},
  year={2014},
  publisher={Springer}
}

@inproceedings{chen2022human,
  title={Human-in-the-loop: Provably efficient preference-based reinforcement learning with general function approximation},
  author={Chen, Xiaoyu and Zhong, Han and Yang, Zhuoran and Wang, Zhaoran and Wang, Liwei},
  booktitle={International Conference on Machine Learning},
  pages={3773--3793},
  year={2022},
  organization={PMLR}
}

@article{dong2018information,
  title={An information-theoretic analysis for thompson sampling with many actions},
  author={Dong, Shi and Van Roy, Benjamin},
  journal={Advances in Neural Information Processing Systems},
  volume={31},
  year={2018}
}

@article{furnkranz2012preference,
  title={Preference-based reinforcement learning: a formal framework and a policy iteration algorithm},
  author={F{\"u}rnkranz, Johannes and H{\"u}llermeier, Eyke and Cheng, Weiwei and Park, Sang-Hyeun},
  journal={Machine learning},
  volume={89},
  pages={123--156},
  year={2012},
  publisher={Springer}
}

@article{hao2021information,
  title={Information directed sampling for sparse linear bandits},
  author={Hao, Botao and Lattimore, Tor and Deng, Wei},
  journal={Advances in Neural Information Processing Systems},
  volume={34},
  pages={16738--16750},
  year={2021}
}

@inproceedings{hao2022contextual,
  title={Contextual information-directed sampling},
  author={Hao, Botao and Lattimore, Tor and Qin, Chao},
  booktitle={International Conference on Machine Learning},
  pages={8446--8464},
  year={2022},
  organization={PMLR}
}

@article{hao2022regret,
  title={Regret bounds for information-directed reinforcement learning},
  author={Hao, Botao and Lattimore, Tor},
  journal={Advances in neural information processing systems},
  volume={35},
  pages={28575--28587},
  year={2022}
}

@article{ji2024reinforcement,
  title={Reinforcement learning from human feedback with active queries},
  author={Ji, Kaixuan and He, Jiafan and Gu, Quanquan},
  journal={arXiv preprint arXiv:2402.09401},
  year={2024}
}

@inproceedings{kirschner2021asymptotically,
  title={Asymptotically optimal information-directed sampling},
  author={Kirschner, Johannes and Lattimore, Tor and Vernade, Claire and Szepesv{\'a}ri, Csaba},
  booktitle={Conference on Learning Theory},
  pages={2777--2821},
  year={2021},
  organization={PMLR}
}

@article{li2024feel,
  title={Feel-Good Thompson Sampling for Contextual Dueling Bandits},
  author={Li, Xuheng and Zhao, Heyang and Gu, Quanquan},
  journal={arXiv preprint arXiv:2404.06013},
  year={2024}
}

@inproceedings{liu2018information,
  title={Information directed sampling for stochastic bandits with graph feedback},
  author={Liu, Fang and Buccapatnam, Swapna and Shroff, Ness},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  volume={32},
  number={1},
  year={2018}
}

@article{moradipari2023improved,
  title={Improved Bayesian regret bounds for thompson sampling in reinforcement learning},
  author={Moradipari, Ahmadreza and Pedramfar, Mohammad and Shokrian Zini, Modjtaba and Aggarwal, Vaneet},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  pages={23557--23569},
  year={2023}
}

@article{ouyang2022training,
  title={Training language models to follow instructions with human feedback},
  author={Ouyang, Long and Wu, Jeffrey and Jiang, Xu and Almeida, Diogo and Wainwright, Carroll and Mishkin, Pamela and Zhang, Chong and Agarwal, Sandhini and Slama, Katarina and Ray, Alex and others},
  journal={Advances in neural information processing systems},
  volume={35},
  pages={27730--27744},
  year={2022}
}

@article{pacchiano2021dueling,
  title={Dueling rl: reinforcement learning with trajectory preferences},
  author={Pacchiano, Aldo and Saha, Aadirupa and Lee, Jonathan},
  journal={arXiv preprint arXiv:2111.04850},
  year={2021}
}

@article{rafailov2024direct,
  title={Direct preference optimization: Your language model is secretly a reward model},
  author={Rafailov, Rafael and Sharma, Archit and Mitchell, Eric and Manning, Christopher D and Ermon, Stefano and Finn, Chelsea},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2024}
}

@article{russo2014learning,
  title={Learning to optimize via information-directed sampling},
  author={Russo, Daniel and Van Roy, Benjamin},
  journal={Advances in neural information processing systems},
  volume={27},
  year={2014}
}

@article{russo2016information,
  title={An information-theoretic analysis of thompson sampling},
  author={Russo, Daniel and Van Roy, Benjamin},
  journal={Journal of Machine Learning Research},
  volume={17},
  number={68},
  pages={1--30},
  year={2016}
}

@article{saha2021optimal,
  title={Optimal algorithms for stochastic contextual preference bandits},
  author={Saha, Aadirupa},
  journal={Advances in Neural Information Processing Systems},
  volume={34},
  pages={30050--30062},
  year={2021}
}

@article{sekhari2024contextual,
  title={Contextual bandits and imitation learning with preference-based active queries},
  author={Sekhari, Ayush and Sridharan, Karthik and Sun, Wen and Wu, Runzhe},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2024}
}

@inproceedings{taranovic2022adversarial,
  title={Adversarial imitation learning with preferences},
  author={Taranovic, Aleksandar and Kupcsik, Andras Gabor and Freymuth, Niklas and Neumann, Gerhard},
  booktitle={The Eleventh International Conference on Learning Representations},
  year={2022}
}

@article{touvron2023llama,
  title={Llama 2: Open foundation and fine-tuned chat models},
  author={Touvron, Hugo and Martin, Louis and Stone, Kevin and Albert, Peter and Almahairi, Amjad and Babaei, Yasmine and Bashlykov, Nikolay and Batra, Soumya and Bhargava, Prajjwal and Bhosale, Shruti and others},
  journal={arXiv preprint arXiv:2307.09288},
  year={2023}
}

@article{wu2023making,
  title={Making rl with preference-based feedback efficient via randomization},
  author={Wu, Runzhe and Sun, Wen},
  journal={arXiv preprint arXiv:2310.14554},
  year={2023}
}

@article{xu2020preference,
  title={Preference-based reinforcement learning with finite-time guarantees},
  author={Xu, Yichong and Wang, Ruosong and Yang, Lin and Singh, Aarti and Dubrawski, Artur},
  journal={Advances in Neural Information Processing Systems},
  volume={33},
  pages={18784--18794},
  year={2020}
}

@article{zhang2024provably,
  title={Provably Efficient Information-Directed Sampling Algorithms for Multi-Agent Reinforcement Learning},
  author={Zhang, Qiaosheng and Bai, Chenjia and Hu, Shuyue and Wang, Zhen and Li, Xuelong},
  journal={arXiv preprint arXiv:2404.19292},
  year={2024}
}

@article{ziegler2019fine,
  title={Fine-tuning language models from human preferences},
  author={Ziegler, Daniel M and Stiennon, Nisan and Wu, Jeffrey and Brown, Tom B and Radford, Alec and Amodei, Dario and Christiano, Paul and Irving, Geoffrey},
  journal={arXiv preprint arXiv:1909.08593},
  year={2019}
}

