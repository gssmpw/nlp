\documentclass[11pt]{article}
\pdfoutput=1
\usepackage{enumerate}
\usepackage{pdfsync}
\usepackage[OT1]{fontenc}
%\usepackage{kpfonts}

% \usepackage[usenames]{color}
%\usepackage[dvips]{graphicx}
\usepackage{smile}
\usepackage[colorlinks,
            linkcolor=red,
            anchorcolor=blue,
            citecolor=blue
            ]{hyperref}
%\usepackage{mathrsfs}
\usepackage{color, colortbl}
\usepackage{fullpage}
\usepackage{hyperref}
\usepackage[protrusion=true,expansion=true]{microtype}
\usepackage{pbox}
\usepackage{setspace}
\usepackage{tabularx}
\usepackage{float}
\usepackage{wrapfig,lipsum}
\usepackage{enumitem}
\usepackage{microtype}
\usepackage{graphicx}
\usepackage{subfigure}
\usepackage{enumerate}
\usepackage{enumitem}
\usepackage{pgfplots}
\usepackage{comment}

\usepackage[capitalize,noabbrev]{cleveref}

\renewcommand{\baselinestretch}{1.01}
\usetikzlibrary{arrows,shapes,snakes,automata,backgrounds,petri}
\usepackage{booktabs} % for professional tables
\newcommand{\dsfont}[1]{\texttt{#1}}
\newtheorem{condition}[theorem]{Condition}
\allowdisplaybreaks

\newcommand{\la}{\langle}
\newcommand{\ra}{\rangle}
\newcommand{\qvalue}{Q}
\newcommand{\vvalue}{V}
\newcommand{\bellman}{\cT}
\newcommand{\reward}{r}
\newcommand{\test}{\omega}
\newcommand{\valueite}{\text{EVI}}
\newcommand{\est}{\text{EST}}
\newcommand{\event}{\mathcal{E}}
\newcommand{\ip}[1]{\langle #1 \rangle}
\newcommand{\ipgr}[1]{\left\langle #1 \right\rangle}
\newcommand{\onep}[1]{\one\{#1\}}

\newcommand{\seq}[1]{\overline{[#1]}}
\newcommand{\onepgr}[1]{\one\left\{#1\right\}}
\newcommand{\trainset}{\cU}
\newcommand{\red}{\color{red}}
\newcommand{\blue}{\color{blue}}
\newcommand{\var}{\mathbb{V}}
\newcommand{\fil}{\cF}
\def \CC {\textcolor{red}}
%\def \CC {}
\def \error {E}
\newcommand{\state}{x}
\usepackage{enumitem}
\def \cvar {c_{\text{var}}}
\def \ig {\bar H}
\def \pnorm {B}
\def \algname {PO-DS}


\def \alg {\text{APPO}}
\def \method {\text{ADPO}}


\newtheorem*{unnumberedlemma}{Lemma}
\newtheorem*{unnumberedtheorem}{Theorem}
\newtheorem*{unnumberedprop}{Proposition}


\newcommand{\I}{\mathbb{I}}
\newcommand{\de}{\mathbb{E}}
\newcommand{\md}{\mathcal{D}}
\newcommand{\dP}{\mathbb{P}}
\newcommand{\tps}{\mathsf{T}}
\newcommand{\ts}{\pi_{\mathrm{TS}}}
\newcommand{\ids}{\pi_{\mathrm{IDS}}}
\newcommand{\app}{\pi_{\mathrm{app}}}
\newcommand{\surr}{K_{\text{surr}}(\epsilon)}
\newcommand{\kl}{D_{\mathrm{KL}}}
\newcommand{\E}{\mathcal{E}}

% to avoid loading the natbib package, add option nonatbib:
%    \usepackage[nonatbib]{neurips_2022}

\usepackage{colortbl}
\definecolor{LightCyan}{rgb}{0.8, 0.9, 1}
\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{xcolor}         % colors
\usepackage{bbm}

\usepackage{todonotes}
% \ifdefined\final
% \usepackage[disable]{todonotes}
% \else
% \usepackage[textsize=tiny]{todonotes}
% \fi
\setlength{\marginparwidth}{0.8in}
\usepackage{colortbl}
\definecolor{LightCyan}{rgb}{0.8, 0.9, 1}
\newcommand{\todoq}[2][]{\todo[size=\scriptsize,color=orange!20!white,#1]{Quanquan: #2}}
\newcommand{\todok}[2][]{\todo[size=\scriptsize,color=blue!20!white,#1]{Kaixuan: #2}}
\newcommand{\todoj}[2][]{\todo[size=\scriptsize,color=green!20!white,#1]{Jiafan: #2}}



\def \CC {\textcolor{red}}
%\def \CC {}

\usepackage{enumitem}
\usepackage[left=0.9in, right=0.9in, top=1in, bottom=1in]{geometry} 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\makeatletter
\newcommand*{\rom}[1]{\expandafter\@slowromancap\romannumeral #1@}
\makeatother
\title{\huge Sample-Efficient Reinforcement Learning from Human Feedback via Information-Directed Sampling}

% The \author macro works with any number of authors. There are two
% commands used to separate the names and addresses of multiple
% authors: \And and \AND.
%
% Using \And between authors leaves it to LaTeX to determine where to
% break the lines. Using \AND forces a line break at that point. So,
% if LaTeX puts 3 of 4 authors names on the first line, and the last
% on the second line, try using \AND instead of \And before the third
% author name.

\author
{
  Han Qi\thanks{Equal contribution.}
  \thanks{Shanghai AI Laboratory. Email: {\tt zhangqiaosheng@pjlab.org.cn}}
  \thanks{Xi'an Jiaotong University. Email: {\tt qihan19@stu.xjtu.edu.cn}}
  \and
	Haochen Yang$^{*}$\thanks{Peking University. Email: {\tt hcyang@stu.pku.edu.cn}}
	\and
	Qiaosheng Zhang$^\dagger$ %\thanks{Shanghai AI Laboratory; e-mail: {\tt zhangqiaosheng@pjlab.org.cn}}
  \and
  Zhuoran Yang\thanks{Yale University. Email: {\tt zhuoran.yang@yale.edu}}
    % Authors
}


\begin{document}
\date{}
\maketitle

%% Abstract
\begin{abstract}
%% Text of abstract

We study the problem of reinforcement learning from human feedback (RLHF), a critical problem in training large language models, from a theoretical perspective. Our main contribution is the design of novel sample-efficient RLHF algorithms based on information-directed sampling (IDS), an online decision-making principle inspired by information theory.
Our algorithms  maximize the sum of the value function and a mutual information term that encourages exploration of the unknown environment (which quantifies the information gained about the environment through observed human feedback data). To tackle the challenge of large state spaces  and  improve sample efficiency, we construct a simplified \emph{surrogate environment} and introduce a novel distance measure (named the \emph{$\ell_g$-distance}), enabling our IDS-based algorithm to achieve a Bayesian regret upper bound of order 
$O(H^{\frac{3}{2}}\sqrt{\log(K(\epsilon)) T})$, where $H$ is the episode length, $T$ is the number of episode and   $K(\epsilon)$  is related to the  covering number  of the  environment. Specializing to the tabular settings, this regret bound is of order $\tilde{O}(H^2\sqrt{SAT})$, where $S$ and $A$ are the numbers of states and actions. Finally, we propose an Approximate-IDS algorithm that is computationally more efficient while maintaining nearly the same sample efficiency. The  design principle of this approximate algorithm is not only effective in RLHF settings but also applicable to the standard RL framework. Moreover, our work showcases the value of information theory in reinforcement learning and in the training of large language models.

\end{abstract}

\section{Introduction}
Reinforcement learning from human feedback (RLHF) is a key technique for aligning large language models (LLMs) to human values \citep{ouyang2022training}, and has also shown immense potential in many other fields, such as stock prediction, robot training, medical treatments \citep{zhu2023principled}. It can be viewed as an extension of standard reinforcement learning (RL) in the sense that feedback is not given as a numerical reward, but as a one-bit preference over a trajectory pair. Compared to standard RL, this preference-based setting is often more aligned with real-world scenarios, especially for tasks involving human evaluations~\citep{chen2022human}. 
However, a key challenge for applying RLHF algorithms is their reliance on extensive human feedback data, which is usually expensive and time-intensive to collect. To address this challenge, recent works on RLHF mainly focus on developing online learning methods that encourage exploration to improve sample efficiency, thereby reducing the amount of human feedback needed~\citep{xie2024exploratorypreferenceoptimizationharnessing}. This brings the RLHF problem back to a fundamental question in RL: \emph{how to effectively balance the trade-off between exploration and exploitation to improve sample efficiency?}

To tackle this trade-off, two major design principles have been introduced. The first approach, \emph{Optimism in the Face of Uncertainty} (OFU), typically relies on constructing confidence sets that include the true environment with high probability to construct corresponding policies, one example of which is the Upper Confidence Bound (UCB) approach \citep{tossou2019nearoptimaloptimisticreinforcementlearning, ye2024theoretical}. In this paper, however, we focus on the less explored second approach, \emph{Posterior Sampling}, which adopts the Bayesian framework and treats the environment as a random variable. One classical posterior sampling algorithm is Thompson Sampling (TS), which has been proved to be sample-efficient and enjoy sublinear Bayesian regret upper bounds  in both RL \citep{moradipari2023improved} and RLHF settings~\citep{wu2023making}.

Apart from TS, \emph{information-directed sampling} (IDS) emerges as a novel and principled online decision-making approach. By incorporating a mutual information term into the policy selection procedure, IDS manages to further encourage exploration about the unknown environment, thus tackling the exploration-exploitation tradeoff to a certain extent \citep{hao2022regret, russo2014learning}. 
Compared with UCB and TS, IDS is more adept at learning complex information-regret structures, and is more flexible and robust to observation noise \citep{zhang2024provably}. In addition, empirical evidence has demonstrated that IDS performs exceptionally well across a range of scenarios, such as sparse linear bandits \citep{hao2021information}, bandits with graph feedback \citep{hao2022contextual}, Markov Decision Processes (MDPs) \citep{hao2022regret}.

Despite their theoretical and empirical advantages, existing IDS-based algorithms are restricted to RL problems with explicitly observable rewards, and are not applicable to RLHF settings. In the LLM era, there is a pressing need for sample-efficient RLHF algorithms,  particularly for scenarios with  large state spaces. 
To tackle these challenges, we first introduce the concept of \emph{surrogate environment}, a compressed (simplified) representation of the potentially complex environment, which helps address the issue of large state spaces. Building on this, and inspired by rate-distortion theory, we design IDS-based RLHF algorithms that are not only theoretically sample-efficient but also computationally easy to implement.

\textbf{Main contributions:} The contribution of this paper can be summarized as follows.

\begin{enumerate}
    \item We first introduce a basic IDS-based algorithm for the RLHF setting where the reward is unobservable and only preference feedback is available (see Sec. \ref{sec:basic_alg}). In each episode, it follows the Bayesian posterior sampling paradigm, and solves an optimization problem that maximizes the sum of an expected value term (exploitation) and a mutual information term (exploration). Here, the mutual information quantifies the amount of information about a learning target (e.g., the environment) that can be gained through the trajectrories and preference.

    \item To tackle the challenge posed by large state spaces, we construct a simplified surrogate environment as the learning target in our algorithm. Using tools from information theory and posterior consistency theory, we prove that our IDS-based algorithm with surrogate environment (Algorithm~\ref{alg}) achieves a Bayesian regret bound of $O(H^{\frac{3}{2}}\sqrt{\log(K(\epsilon)) T})$, where $H$ is the episode length, $T$ is the number of episode, and $K(\epsilon)$  is related to the \emph{covering  number} of  the  environment. We also specialize our algorithm and results to the tabular RLHF, linear RLHF, and contextual dueling bandit settings, and demonstrate the advantages of our algorithm over existing ones.

    \item Finally, we propose an Approximate-IDS algorithm (Algorithm~\ref{alg2}), which is computationally more efficient than Algorithm~\ref{alg} while maintaining nearly the same sample efficiency. The advantage of this algorithm is that it does not need to construct the surrogate environment. This algorithm selects policies using an alternative optimization objective that can be optimized with standard RL techniques, such as PPO \citep{schulman2017proximal}. 
Furthermore, we note that  the
design principle of  Algorithm \ref{alg2}  is not only effective for preference-based learning but is also applicable to general RL tasks.
\end{enumerate}

% 1.  We introduce the information-directed sampling algorithm to RLHF. Our proposed algorithm (Algorithm \ref{alg}) follows the Bayesian posterior sampling paradigm but finds a regularized IDS policy. Searching for the IDS policy requires solving an optimization problem, which simultaneously maximizes the expected value function and the mutual information between the environment and history. Different from the traditional RL methods,  the history in RLHF includes paired trajectory data and preference feedback. 


% 2. To develop sharper regret bounds, we use the surrogate learning methods (first proposed in \cite{hao2022regret}) to learn a less informative environment instead of the whole environment. 
% We introduce a novel measure $\ell_g$-distance  for partitioning the environment and identifying surrogate environment. This new metric preserves the properties of the general $ l_1$-distance while also helps us design easy-to-implement algorithms. Using tools from information theory and posterior consistency theory, we prove that the proposed algorithm has a regret upper bound of $O(H^{\frac{3}{2}}\sqrt{\log(K_{\mathrm{surr}}(\epsilon)) T})$. Furthermore, in this paper we allow for stochastic transition and rewards, which are  more comprehensive than the deterministic rewards studied in most existing works on RLHF. 



% 3. Although surrogate learning methods can provide tighter regret bounds, their implementation—specifically, partitioning the environment space and identifying surrogate environments—is highly challenging. Leveraging a novel measure we introduced, we have designed an approximate algorithm (Algorithm \ref{alg2}) that theoretically partitions and constructs surrogate environments only in the analytical phase, while bypassing these computational complexities during practical implementation. This algorithm identifies policies using an alternative optimization objective, which can be optimized using standard RL algorithms (such as PPO \cite{schulman2017proximal}).
% Using a covering argument developed in our new measure, it can be shown that the regret upper bound of this approximate algorithm has the same order of regret bound as the original algorithm with respect to $H$ and $T$. The approximate algorithm we propose is not only effective in preference-based learning, but also applicable to general reinforcement learning problems.


\textbf{Highlights on technical novelty:} In the process of constructing the surrogate environment, we introduce a novel distance measure, the \emph{$\ell_g$-distance}, to quantify the discrepancy between two probability measures (see Eqn.~\eqref{new_dis} in Sec. \ref{surr}). It retains most of the desirable properties of the $\ell_1$-distance, and is crucial for the design of the computationally efficient algorithm (Algorithm 2). Moreover, the use of \emph{$\ell_g$-distance} introduces new analytical challenges not present in previous IDS-related works, which are successfully addressed through a variety of refined analytical tools (see Appendices~\ref{appendix_A} and~\ref{lg}).


\textbf{Comparisons with related works:} 
First, we note that most existing works on RLHF assume deterministic rewards, whereas our work considers a more general framework where both transitions and rewards are stochastic. Among existing RLHF algorithms, the most relevant to ours is the TS-based algorithm by~\citet{wu2023making}. Although in the general setting their algorithm's regret bound is not directly comparable to ours (as theirs depends on the \emph{eluder dimension}, while ours depends on the covering number), we note that in the tabular setting, our bound is superior if we coarsely substitute the dimension $d$ with $SA$ in their linear setting. When comparing with prior works on standard RL, we note that our regret bound\footnote{We say $f(n) = \tilde{O}(g(n))$ if $f(n) = O(g(n)\cdot \text{polylog}(n))$.} $\tilde{O}(H^2\sqrt{SAT})$ is superior to the regret bound  $\tilde{O}(H^2\sqrt{S^2A^2T})$ of the surrogate-IDS algorithm by \citet{hao2022regret}, even though we consider a more challenging RLHF setting where we rely only on human feedback to learn the reward model. Moreover, compared to a prior work on TS for standard RL~\citep{moradipari2023improved}, our analysis method removes a technical assumption that almost all optimal policies visit almost all state action
pairs.  

\section{Related Works}

\textbf{Reinforcement Learning from Human Feedback (RLHF)}: RLHF has emerged as a critical approach in aligning AI systems with human values, especially in complex tasks where human feedback plays a crucial role \citep{achiam2023gpt,touvron2023llama}. The RLHF framework typically involves a three-stage process: supervised fine-tuning (SFT), reward modeling (RM), and reinforcement learning (RL) using algorithms like Proximal Policy Optimization (PPO)\citep{ouyang2022training,ziegler2019fine}.
%\cite{ouyang2022training,bai2022training,ziegler2019fine}
Direct Preference Optimization (DPO)~\citep{rafailov2024direct}  is another approach that directly uses generative models as reward models and trains them using preference data. 

The practical success of RLHF has also sparked a variety of theoretical studies. According to the type of preference feedback, these works can be roughly divided into two categories: \emph{action preference}~\citep{furnkranz2012preference,saha2021optimal,ji2024reinforcement,sekhari2024contextual,li2024feel,bai2025online} and \emph{trajectory preference}~\citep{busa2014preference,xu2020preference,pacchiano2021dueling,chen2022human,taranovic2022adversarial,wu2023making}. The literature  on action preferences is generally referred to as the \emph{contextual dueling bandits}.
In this paper, we focus on the trajectory preference. Most of the existing work in this area follow the OFU principle 
with the exception of~\citep{wu2023making,li2024feel} and~\citep{li2024feel}, who investigate a well-known Bayesian method—TS. Note that \citet{wu2023making} uses trajectory preferences and can be applied to the general function approximation framework while \citet{li2024feel} focuses on contextual dueling bandits. We also use the posterior sampling method, but unlike TS, our method follows the princple of  information-directed sampling.

\textbf{Information-Directed Sampling (IDS):} IDS is a design principle for sequential decision-making problems, which balances
exploration and exploitation by evaluating the information gain from each action or trajectory. \citet{russo2014learning} first introduces the IDS principle in the bandit setting. They decompose the Bayesian regret into a information ratio term and a cumulative information gain term, and bound the regret by tools from information theory. Based on their work, many studies use this method to analyze the regret of the TS algorithm in bandit settings \citep{russo2016information,dong2018information,bubeck2020first,liu2018information,kirschner2021asymptotically,hao2021information,hao2022contextual}. 

Recently,  \citet{hao2022regret,moradipari2023improved} study the Bayesian regret of IDS and TS without any prior assumptions for MDP settings. \citet{moradipari2023improved} focuses on analyzing TS in general settings while~\citet{hao2022regret} proposes a regularized-IDS algorithm for tabular and linear settings. \citet{zhang2024provably} uses the principle of IDS to design a   set of algorithms for multi-agent reinforcement learning.
They both use the surrogate environment as the learning target to get a sharper bound. However,  implementing the surrogate version of the algorithm is a  challenge. In this paper, we introduce IDS into RLHF for general MDP settings. We propose an easy-to-implement surrogate algorithm and prove that the regret upper bound has the same order as the original version.



\section{Preliminaries} 
\subsection{Notations} 
For any positive integer $n$, we use $[n]$ to denote the set $\{1,2,\ldots, n\}$. For a measurable space $\mathcal{X}$ and a probability measure $\mu$ on it, we let $\Delta(\mathcal{X}, \mu)$ denote the set of all possible probability distributions over $\mathcal{X}$ that are absolutely continuous with respect to $\mu$. When $\mu$ is clear from the context, we use $\Delta(\mathcal{X})$ for brevity. For two probability densities $p,q$ on $\mathcal{X}$, we denote their Kullback-Leibler (KL) divergence $\kl$ as
\begin{equation*}
    \kl(p\Vert q)\triangleq \int_{\mathcal{X}}p(x)\cdot\log\left(\frac{p(x)}{q(x)}\right)\mathrm{d}x.
\end{equation*}
For two random variables $X$ and $Y$, their mutual information $\I(X;Y)$ is defined as
\begin{equation*}
    \I(X;Y)\triangleq \kl(\mathbb{P}((X,Y) \in \cdot \ ) \Vert \mathbb{P}(X \in \cdot \ ) \times \mathbb{P}(Y \in \cdot \ )).
\end{equation*}
The conditional mutual information of $X$ and $Y$, given another random variable $Z$, is defined as
\begin{equation*}
    \begin{aligned}
    \I(X;Y|Z) \triangleq \mathbb{E}_Z[ \kl(\mathbb{P}((X,Y) \in \cdot \  |Z) 
     \Vert \mathbb{P}(X \in \cdot \ |Z) \times \mathbb{P}(Y \in \cdot \  |Z))].
    \end{aligned}
\end{equation*}

% We adopt the standard big-oh asymptotic notation, i.e., we say $f(n) = \mathcal{O}(g(n))$ for two functions $f(n)$ and $g(n)$, if there exist $m>0$ and $N_0\in\mathbb{N}^+$ such that for all $n>N_0$, we have $|f(n)|\leq m\cdot g(n)$.

\subsection{Finite-horizon MDPs}

The environment is denoted as $ \mathcal{E}=(\mathcal{S},\mathcal{A},H,\{P_h\}_{h=1}^H,\{R_h\}_{h=1}^H)$, where $\mathcal{S}$ and $\mathcal{A}$ are the measurable state and action spaces respectively, and $H$ is the episode length. For each step $h\in[H]$, $P_h:\mathcal{S} \times \mathcal{A}\to \Delta\left({\mathcal{S}}, \mu_{\mathcal{S}}\right)$ is the transition probability kernel, where $\mu_{\mathcal{S}}$ is the base probability measure on $\mathcal{S}$; $R_h:\mathcal{S}\times \mathcal{A} \to \Delta\left([0,1], \mathrm{Lebesgue}\right)$ is the reward function. Since we mostly deal with the mean value of the reward, we define $r_h(s,a)\triangleq \mathbb{E}_x\left[R_h(x|s,a)\right]=\int_0^1xR_h(x|s,a)\mathrm{d}x$. We assume that $\mathcal{S}, \mathcal{A}$ are known while the transition kernels $\{P_h\}_{h=1}^H$ and rewards $\{R_h\}_{h=1}^H$ are unknown and random.

We consider a Bayesian framework, where we treat the environment $\mathcal{E}$ as a random variable and have a prior belief on $\mathcal{E}$. For each step $h\in[H]$,
let $\Theta_h^{P}$ and $\Theta_h^R$ be the function spaces of $P_h$ and $R_h$ respectively, and let $\Theta_h\triangleq\Theta_h^P\times \Theta_h^R$. The spaces $\Theta_h^{P}$ and $\Theta_h^R$ are assumed to be equipped with prior probability measures, denoted as $\rho_h^P$ and $\rho_h^R$ respectively. Define the full function spaces
$
    \Theta^P\triangleq\prod_{h=1}^H \Theta_h^P, \ \Theta^R\triangleq\prod_{h=1}^H \Theta_h^R,\ \Theta\triangleq\prod_{h=1}^H \Theta_h
$,
which parameterize the set of all environments and also induce the product prior probability measure $\rho^P\triangleq\prod_{h=1}^H \rho_h^P$ for $\Theta^P$, $\rho^R\triangleq\prod_{h=1}^H \rho_h^R$ for $\Theta^R$, and $\rho \triangleq \rho^P\otimes\rho^R$ being the prior of environments. Notice that this setting ensures the independence of the priors over different layers.
Since the notion of the convex combination of environments will be used in our analysis, without loss of generality, we assume $\Theta$ is convex.  % $\Theta$ forms a vector space with respect to general function addition and scalar multiplication. Therefore Since we may it is meaningful to discuss 
% To ensure that $\Theta$ can be covered by finitely many sets, we assume $(\Theta,\tau_{\ell})$ is a compact topological space, where $\tau_{\ell}$ is the topology generated by the metric $\ell$ [TODO].

\subsection{Interaction protocol}
The process of an agent interacting with a finite-horizon MDP is as follows. 
The agent starts at an initial state $s_1^t$, which is assumed to be fixed for all episodes $t\in[T]$. In each episode $t\in[T]$, the agent selects two policies $(\pi_0^t,\pi_1^t)$ from the set of all possible policies $\Pi$, where a policy $\pi$ is denoted by stochastic maps $(\pi_1,\ldots,\pi_H)$ with each $\pi_h:\mathcal{S}\to\Delta(\mathcal{A})$. Note that by this definition we assume the policy to be stationary, i.e., depends  only on the current state and layer. At layer $h$ in episode~$t$, for $i=0,1$, the agent observes state pair $(s_h^{t,0},s_h^{t,1})$, separately executes $\pi_i^t$ on $s_h^{t,i}$ to obtain action pair $(a_h^{t,0},a_h^{t,1})$ with probability $\pi_i^t(a_h^{t,i}|s_h^{t,i})$, takes the actions and changes to the next random state $s_{h+1}^{t,i}$ with probability $P_h(s_{h+1}^{t,i}|s_{h}^{t,i},a_{h}^{t,i})$. At state $s_{H+1}$, the agent stops acting and obtains two trajectories $\tau_0^t$ and $\tau_1^t$, where $$\tau_i^t \triangleq (s_1^{t,i},a_1^{t,i},...,s_H^{t,i},a_H^{t,i}).$$ 
In the  RLHF setting, the agent cannot directly receive a numerical reward, but only receives a \emph{preference signal} $o_t$ over trajectory pair $(\tau_0^t,\tau_1^t)$, where $o_t$ is a Bernoulli random variable with $\mathbb{P}(o_t=1|\tau_0^t,\tau_1^t)\triangleq\mathbb{P}(\tau_1^t \text{ is preferred to } \tau_0^t)$. We assume the preference follows the \emph{Bradley-Terry (BT) model} \citep{bradley1952rank}, which has been widely used in existing works on RLHF. The BT model assumes the probability of humans preferring one choice to the other is proportional to the exponential of the value of cumulative reward:
\begin{equation*}
    \mathbb{P}(o_t=1|\tau_0^t,\tau_1^t)
    % =\frac{\exp\{r(\tau_1^t)\}}{\exp\{r(\tau_1^t)\}+\exp\{r(\tau_0^t)\}}
    =\sigma(r(\tau_1^t)-r(\tau_0^t)),
\end{equation*}
where $r(\tau^t)\triangleq\sum_{h=1}^{H}r_h(s_h^t,a_h^t)$ for $\tau^t=(s_1^t,a_1^t,\ldots,s_H^t,a_H^t)$, and $\sigma(x) \triangleq 1/(1+e^{-x})$ is the sigmoid function.

Let $\mathcal{H}_t \triangleq (\tau_0^t, \tau_1^t, o_t)$ be the history of episode $t$ that includes both trajectories and preference feedback, and let $\md_t \triangleq (\mathcal{H}_1,...,\mathcal{H}_{t-1})$ be the entire history up to episode $t$. The history of episode $t$ up to layer $h$ is denoted as
\[
\mathcal{H}_{t,h}\triangleq  (s_1^{t,i}, a_1^{t,i}, \ldots, s_h^{t,i}, a_h^{t,i})_{i \in \{0,1\}}.\]
In the Bayesian setting,  we often need to take conditional expectations with regard to $\md_t$. For brevity, we follow the standard notation in~\citep{hao2022regret}, letting $\dP_t(\cdot)\triangleq\dP(\cdot|\md_t)$, and $\de_t[\cdot]\triangleq\de[\cdot|\md_t]$. Finally, let $\mathcal{R}_{t,h}\triangleq(r_1^{t,i},...,r_h^{t,i})_{i\in\{0,1\}}$ denote the corresponding potential unobserved rewards, where each $r_h^{t,i}$ is a random variable satisfying $r_h^{t,i}\sim R_h(\cdot|s_h^{t,i}, a_h^{t,i})$.


\subsection{Value function and Bayesian regret}
Define the value function $V_{h,\pi}^{\mathcal{E}}:\mathcal{S}\to[0,H]$ as the expected cumulative rewards received under policy $\pi$ interacting with $\mathcal{E}$ at layer $h$:
\begin{equation*}
    V_{h,\pi}^{\mathcal{E}}(s)\triangleq\mathbb{E}_\pi^{\mathcal{E}}\bigg[  \sum_{h'=h}^{H}r_{h'}(s_{h'},a_{h'})|s_h=s  \bigg],
\end{equation*}
where $\mathbb{E}_\pi^{\mathcal{E}}$ denotes the expectation over the trajectory generated under policy $\pi$ and environment $\mathcal{E}$. We set $V_{H+1,\pi}^{\mathcal{E}}(\cdot)\triangleq 0$. For environment $\E$, let $\pi_\E^*$ be the optimal policy that satisfies $\pi_\E^* = \max_{\pi} V_{h,\pi}^{\mathcal{E}}(s)$ for all $s \in \mathcal{S}$ and $h \in [H]$. Note that under Bayesian settings, $\pi_\E^{*}$ is a function of $\mathcal{E}$, which is also a random variable. 

Finally, for a sequence of policies $\pi = (\pi_t)_{t \in [T]}$ over $T$ episodes, we define the \emph{regret} of $\pi$ in environment $\E$ as
\begin{equation}
R_T(\mathcal{E},\pi)\triangleq\sum_{t=1}^{T} V_{1,\pi_{\mathcal{E}}^{*}}^{\mathcal{E}}(s_1^t)-V_{1,\pi^{t}}^{\mathcal{E}}(s_1^t).
\end{equation}
Since this work focuses on the Bayesian setting, we also define the \emph{Bayesian regret}  as 
\begin{equation}
\label{regret_def}
    BR_T(\pi)\triangleq\mathbb{E}_{\E\sim\rho}[R_T(\mathcal{E},\pi)].
\end{equation}
The task of finding a policy $\pi$ with minimal Bayesian regret, in the context of a finite-horizon MDP, is called a Bayesian RLHF problem.


\section{The basic IDS Algorithm}
This section introduces a basic IDS algorithm for RLHF settings. In Sec. \ref{sec:basic_alg}, we present the generic form of our algorithm with an abstract learning target. Sec. \ref{surr} suggests constructing a discrete surrogate environment as the learning target and then describes an IDS algorithm with the surrogate environment (Algorithm~\ref{alg}). Sec. \ref{regret_analysis} provides the Bayesian regret bound for Algorithm~\ref{alg}, while Sec. \ref{application} specializes this result to tabular RLHF, linear RLHF, and contextual dueling bandits.


\subsection{Algorithm description: a generic form}
\label{sec:basic_alg}

At the beginning of episode $t$, based on the prior distribution $\rho$ and history data $\mathcal{D}_t$, the agent first computes the posterior distribution of the environment $\E\sim\dP(\cdot|\md_t)$, or equivalently, the transition $P$ and reward $R$. Then, the agent chooses a stochastic policy $\pi_{\text{IDS}}^t$ by maximizing a weighted
sum of an expected value term and a mutual information term:
\begin{equation}\label{eq4_1}
    \ids^t=\arg\max_{\pi\in\Pi}\mathbb{E}_t[V_{1,\pi}^{\mathcal{E}}(s_1)]+ \frac{\lambda}{2}\cdot\mathbb{I}_t^\pi\left(\chi;(\mathcal{H}_t,\mathcal{R}_{t,H})\right),
\end{equation}
where $\lambda>0$ is a tunable parameter. Here, $\chi$ is called the \emph{learning target}, which is a random variable and is usually selected as the whole environment $\E$ when the state space is not too large. However, in Sec.~\ref{surr} where we consider large state space cases, we will construct a surrogate environment as the learning target to achieve tighter regret bounds. 

The subscript $t$ in $\mathbb{I}_t^\pi\left(\chi;(\mathcal{H}_t,\mathcal{R}_{t,H})\right)$ in Eqn.~\eqref{eq4_1} means that the distributions of $\chi$ and $(\mathcal{H}_t,\mathcal{R}_{t,H})$ are both conditioned on $\mathcal{D}_t$, and the superscript $\pi$ means
%conditional on $\md_t$, i.e., $\I_t(X;Y)\triangleq\kl\left(\dP((X,Y)\in\cdot|\md_t)\Vert\dP(X\in\cdot|\md_t)\otimes\dP(Y\in\cdot|\md_t)\right)$. Note that $\de_{\md_t}(\I_t(X;Y))=\I(X;Y|\md_t)$. The superscript $\pi$ in $\mathbb{I}_t^\pi\left(\chi;(\mathcal{H}_t,\mathcal{R}_{t,H})\right)$ means 
that $(\mathcal{H}_t,\mathcal{R}_{t,H})$ are obtained by executing the policy $\pi$. Intuitively, a larger value of $\mathbb{I}_t^\pi\left(\chi;(\mathcal{H}_t,\mathcal{R}_{t,H})\right)$ indicates that the data obtained at episode $t$ contains more information about the learning target $\chi$. Accordingly, the introduction of mutual information in the policy selection procedure further encourages exploration about the unknown environment, while the expected value term $\mathbb{E}_t[V_{1,\pi}^{\mathcal{E}}(s_1)]$ promotes exploitation. In this way, our algorithm manages to tackle the exploration-exploitation tradeoff to a certain extent.

\subsection{Constructing surrogate environments as learning targets}\label{surr}
In real-world scenarios, the environment is often too complex to be fully included as the agent's learning target $\chi$, thus it is better for the agent to focus only on the significant parts of the environment. In this subsection, we construct a discrete surrogate environment and propose an IDS algorithm with this surrogate environment as the learning target. 

% \paragraph{Constructing surrogate environments}
\vspace{5pt}
\subsubsection{A new distance measure}
The discrete environment is constructed using a  covering argument with suitable distance measures. 
% Specifically, the environment $\Theta$ is divided into finite $\epsilon$-balls, with each ball containing environments that have similar properties. 
Unlike previous works on standard RL settings~\citep{hao2022regret,moradipari2023improved} that use either $\ell_1$-distance or KL-divergence, we propose a new distance measure between two probability measures, called the \emph{$\ell_g$-distance},  which is better suited to our RLHF framework:
\begin{equation}
\label{new_dis}
  \begin{aligned}
    \ell_g(P,Q) 
    & \triangleq \sup_{o\in\mathcal{O}} \Vert \log P(\cdot|o) - \log Q(\cdot|o) \Vert_1 \\
    & = \sup_{o\in\mathcal{O}} \int_{x\in\mathcal{X}} \left|\log\frac{P(x|o)}{Q(x|o)}\right| \mathrm{d}\mu_{\mathcal{X}},
  \end{aligned}
\end{equation}
where $\mathcal{O}=\mathcal{S} \times \mathcal{A}$.
To guarantee the existence of a finite coverage and that $\ell_g$ is well-defined, we need  the following assumptions:
\begin{assumption}
\label{assumption2}
$(\Theta,\tau_{\ell_g})$ is a compact topological space, where $\tau_{\ell_g}$ is the topology generated by the metric~$\ell_g$.
\end{assumption}

\begin{assumption}
\label{assumption1}
    For any $P \in \Theta$,  there exist $\beta, B>0$ such that 
    \[\beta \leq \inf_{o,x} \{ P(x|o):P(x|o)\neq 0\} \leq \sup_{o,x} \{ P(x|o)\} \leq B. \]
\end{assumption}

Note that Assumption \ref{assumption1} does not restrict the transition probabilities and rewards from being equal to 0; it simply assumes a lower bound for their non-zero supports.


\begin{remark}
    For any two vector-valued maps $P,Q$, we define 
    \begin{equation}
        \ell_g(P,Q)\triangleq \sup_{o\in\mathcal{O}}\int_{x\in\mathcal{X}} \sum_{i} \left| \log\frac{P_i(x|o)}{Q_i(x|o)} \right| \mathrm{d}\mu_{\mathcal{X} }
    \end{equation}
    where $P_i$ and $Q_i$ are the $i$-th component of $P$ and $Q$ respectively. This generalization of one-dimension case is useful for the analysis of linear RLHF problems (Theorem \ref{corollary2}).
\end{remark}
\begin{remark}
    Similar to the KL divergence, we allow for taking infinite values of $\ell_g$, e.g., if there exists a subset $\mathcal{X}'\subset\mathcal{X}$ with positive measure such that $Q(x|o)=0$ but $P(x|o)$ is nonzero on $\mathcal{X}'$, by definition we have $\ell_g(P,Q) =\infty$.
\end{remark}
Although similar to the KL divergence, one of the fundamental properties of $\ell_g$ is that $\ell_g$ is a distance metric, which is more convenient for analysis. 
\begin{lemma}\label{metric}
    $\ell_g$ is a distance metric.
\end{lemma}
\begin{proof}
    By definition, it is easy to see that $\ell_g(P,Q)=\ell_g(Q,P)$ and $\ell_g(P,Q)=0\Leftrightarrow P=Q$. It then suffices to show the triangle inequality. For any three probability distributions $P,Q,R$, we have
    \begin{align*}\label{triangle}
    \ell_g(P,Q)= \sup_{o} \int_{x\in\mathcal{X}} \bigg|\log \frac{P(x|o)}{Q(x|o)}\bigg| 
    &= \sup_{o} \int_{x\in\mathcal{X}} \bigg|\log \frac{P(x|o)}{R(x|o)}-\log\frac{Q(x|o)}{R(x|o)}\bigg| \\
    &\leq \sup_{o} \int_{x\in\mathcal{X}} \bigg|\log \frac{P(x|o)}{R(x|o)}\bigg| + \int_{x\in\mathcal{X}} \bigg|\log \frac{Q(x|o)}{R(x|o)}\bigg| \\
    &= \ell_g(P,R)+\ell_g(Q,R),
    \end{align*}
    which completes the proof of Lemma \ref{metric}.
\end{proof}



Given the new distance $\ell_g$, we introduce the definition of  $\epsilon$-covering number.
\begin{definition}[$\epsilon$-covering number]
For a set $\mathcal{G}$, the $\epsilon$-covering number of $\mathcal{G}$ with respect to $\ell_g$ is  the size $K(\mathcal{G},\epsilon)$ of the smallest set $\{G_1,...,G_{K(\mathcal{G},\epsilon)} \} \subset \mathcal{G}$ such that 
    \begin{equation}
        \forall P \in \mathcal{G}, \exists P' \in \{G_1,...,G_{K(\mathcal{G},\epsilon)} \} : \ell_g(P,P') \leq \epsilon.
    \end{equation}
\end{definition}

\vspace{10pt}
\subsubsection{Partition of the environment} First, we  introduce the concept of $\epsilon$-\emph{value partition},  which must exist based on Assumption~\ref{assumption2}.
\begin{definition}[$\epsilon$-value partition]\label{epsilon_value}
Given any $\epsilon>0$, we say a partition $\{\Theta_k^{\epsilon}\}_{k=1}^K$ over $\Theta$ is an $\epsilon$-value partition for a RLHF problem if for any $k\in[K]$ and $\E,\E'\in\Theta_k^{\epsilon}$,
\begin{equation}
    V_{1,\pi_{\mathcal{E}}^{*}}^{\mathcal{E}}(s_1)-V_{1,\pi_{\mathcal{E}}^{*}}^{\mathcal{E}'}(s_1) \leq \epsilon.
\end{equation}
\end{definition}


%  For any $\epsilon > 0$, we define the quantity $K_{\text{surr}}(\epsilon)$ as
% \[ 
%  K_{\text{surr}}(\epsilon) \triangleq \min\{K \!\in \! \mathbb{N}^+ \!: \! \exists \text{ an } \epsilon\text{-value partition } \! \{\Theta_k^{\epsilon}\}_{k=1}^K \}. 
% \]
% We first construct an $\epsilon$-value partition ${\Theta_k^\epsilon}$ of $\Theta$, and then explicitly construct the surrogate environment.
We now provide a concrete construction of the $\epsilon$-value partition as follows.
For any $\E_0 \in \Theta$, we define the 
$\epsilon$-ball  centered at $\E_0$ as 
\[
B(\E_0,\epsilon)\triangleq\{\E\in\Theta:\ell_g(\E,\E_0) \leq \epsilon\}.
\]
Let $\delta_P \triangleq \epsilon/6BH^2$ and $\delta_R \triangleq \epsilon/6BH$. Let $K(\Theta_h^P,\delta_P)$ and $ K(\Theta_h^R,\delta_R) $ be the $\delta_P$-covering and $\delta_R$-covering numbers of $\Theta_h^{P}$ and   $\Theta_h^{R}$ respectively. We denote $\{B_{h}^P(i,\delta_P)\}_{i=1}^{K(\Theta_h^P,\delta_P)}$ and $\{B_{h}^R(j,\delta_R)\}_{j=1}^{K(\Theta_h^R,\delta_R)}$ as the corresponding  $\epsilon$-balls that cover  $\Theta_h^{P}$ and $\Theta_h^{R}$.
% Let 
% $\{B_{h,i}^P(\delta_P)\}_{i=1}^{L_h^P(\delta_P)}$,$\{B_{h,i}^P(\delta_R)\}_{i=1}^{L_h^R(\delta_R)}$ be the $\epsilon$-balls giving an $\epsilon$-covering for $\Theta_h^{P}$, $\Theta_h^{R}$ where $L_h^P(\delta_P), L_h^R(\delta_R) $ are the covering numbers. 
For each $i_h \in [K(\Theta_h^P,\delta_P) ]$ and $j_h \in [K(\Theta_h^R,\delta_R)]$, we define 
\begin{equation}
\label{cover}
    \Theta^\epsilon_{h,i_h,j_h}\triangleq\left\{\E\in\Theta \mid P_h^{\mathcal{E}} \in B_{h}^P(i_h,\delta_P),R_h^{\mathcal{E}} \in B_{h}^R(j_h,\delta_R)\right\}.
\end{equation}

Setting $K(\epsilon) \triangleq \prod_{h=1}^H K(\Theta_h^P,\delta_P) \times K(\Theta_h^R,\delta_R)$, we can then find a one-to-one mapping from $(h,i_h,j_h)$ to $[K(\epsilon)]$, and we obtain an $\epsilon$-value  partition that satisfies $\cup_{k=1}^{K(\epsilon)} \Theta_k^{\epsilon} = \Theta$.\footnote{If an environment $\mathcal{E} \in \Theta$ belongs to more than one partition, we will ensure it only appears in a single partition by truncating the other partitions.} Now, we prove that for any $\E,\E'$ belonging to the same partition,  $$V_{1,\pi_{\mathcal{E}}^{*}}^{\mathcal{E}}(s_1)-V_{1,\pi_{\mathcal{E}}^{*}}^{\mathcal{E}'}(s_1) \leq \epsilon.$$

By Lemma \ref{lemma}, we have
    \begin{align}\label{A5}
        &V_{1,\pi_{\mathcal{E}}^{*}}^{\E}(s_1)-V_{1,\pi_{\mathcal{E}}^{*}}^{\E'}(s_1) \nonumber\\
        &=\sum_{h=1}^{H}\mathbb{E}_{\pi_{\E}^{*}}^{\E'}\bigg[\mathbb{E}_{s'\sim P_h^{\E}(\cdot|s_h,a_h)}\left[ V_{h+1,\pi_{\mathcal{E}}^{*}}^{\E}(s')\right] \!-\mathbb{E}_{s'\sim P_h^{\E'}(\cdot|s_h,a_h)}\left[ V_{h+1,\pi_{\mathcal{E}}^{*}}^{\E}(s')\right]\bigg]\!+\sum_{h=1}^H\mathbb{E}_{\pi_{\E}^{*}}^{\E'}\left[ R_h^\E(s_h,a_h)-R_h^{\E'}(s_h,a_h)\right]\nonumber\\
        &\leq\sum_{h=1}^H\mathbb{E}_{\pi_{\E}^{*}}^{\E'}\bigg[\int_{\mathcal{S}}\left|P_h^\E(s'|s_h,a_h)\!-P_h^{\E'}(s'|s_h,a_h)\right|\cdot V_{h+1,\pi_{\E}^{*}}^{\E}(s')\mathrm{d}\mu_{\mathcal{S}}\!+\int_{[0,1]}\left|x\left(R_h^\E(x|s_h,a_h)-R_h^{\E'}(x|s_h,a_h)\right)\right|\mathrm{d}x\bigg]\nonumber\\
        &\leq \sum_{h=1}^H\mathbb{E}_{\pi_{\E}^{*}}^{\E'}\bigg[HB\cdot\int_{\mathcal{S}}\left|\log\frac{P_h^\E(s'|s_h,a_h)}{P_h^{\E'}(s'|s_h,a_h)}\right|\mathrm{d}\mu_{\mathcal{S}}+B\cdot\int_{[0,1]}\left|\log\frac{R_h^\E(x|s_h,a_h)}{R_h^{\E'}(x|s_h,a_h)}\right|\mathrm{d}x\bigg]\nonumber\\
        &\leq\sum_{h=1}^H\mathbb{E}_{\pi_{\E}^{*}}^{\E'}\left[HB\cdot2\delta_P+B\cdot2\delta_R\right]=\frac{2\epsilon}{3}\leq\epsilon,
\end{align}
where the second inequality is due to the fact that $V_{h+1,\pi_{\mathcal{E}}^{*}}^{\E}(s')\leq H$, and $|a-b| \leq B\cdot|\log\frac{a}{b}|$ for any $a,b\in(0,B)$. The last inequality is due to the definition of $\ell_g$: since $\E,\E'$ lie in the same $\Theta_k^\epsilon$, we have $\ell_g(P_h^{\E},P_h^{\E'})\leq 2\delta_P$ and $\ell_g(R_h^{\E},R_h^{\E'})\leq 2\delta_R$.  This shows that $\{\Theta_k^\epsilon\}_{k=1}^K$ gives an $\epsilon$-value partition.

\vspace{10pt}
\subsubsection{Construct the surrogate environment}
% We first construct an $\epsilon$-value partition ${\Theta_k^\epsilon}$ of $\Theta$, and then explicitly construct the surrogate environment  as  
% \begin{equation}
%     \label{lemma_tmp1}
%     \tilde{\mathcal{E}}_t^{*} = \mathbb{E}_t \left[ \mathcal{E}|\mathcal{E} \in \Theta_k^{\epsilon}\right]  \text{ iff } \mathcal{E} \in \Theta_k^{\epsilon}
% \end{equation}
Based on the above $\epsilon$-value partition, we
explicitly construct the \emph{surrogate environment} $\tilde{\mathcal{E}}_t^{*} $ 
 for episode $t$ as:
\begin{equation}
\label{lemma_tmp1}
    \tilde{\mathcal{E}}_t^{*} = \tilde{\mathcal{E}}_{k,t}^{*}  \text{ iff } \mathcal{E} \in \Theta_k^{\epsilon},
\end{equation}
where $\tilde{\mathcal{E}}_{k,t}^{*} \triangleq \mathbb{E}_t \left[ \mathcal{E}|\mathcal{E} \in \Theta_k^{\epsilon}\right]$. 
% It can be verified that the surrogate environment $\tilde{\mathcal{E}}_t^{*} $ satisfies (1)-(3) of Lemma \ref{partition} under the $\epsilon$-value  partition we construct.
% Let $\zeta$ be a discrete random variable taking values in $\{1,...,K(\epsilon)\}$ such that $\zeta=k$ if and only if  $\mathcal{E} \in \Theta_k^{\epsilon}$. 
For this surrogate environment, we have the following result.

\begin{lemma}
\label{partition}  Fix $t\in[T]$ and environment $\E\in\Theta$. 
Given the $\epsilon$-value partition $\{\Theta_k^{\epsilon}\}_{k=1}^{K(\epsilon)}$ (Eqn.~\eqref{cover}), the  surrogate environment $\tilde{\mathcal{E}}_t^{*}$ constructed by Eqn.~\eqref{lemma_tmp1} satisfies the following:
    
 % (1) $\{\Theta_k^{\epsilon}\}_{k=1}^K$ is independent of $t$, and the distributions of $\tilde{\mathcal{E}}_t^{*}$ depend on $\E$ only through $\zeta$, i.e., $\tilde{\mathcal{E}}_t^{*}$ and $\E$ are independent conditioning on $\zeta$. \textcolor{blue}{From the definition of $K_{\mathrm{surr}}(\epsilon)$, we have $K_{\mathrm{surr}}(\epsilon) \leq K$};
\begin{enumerate}
    \item For any  $(s,a,h)\in\mathcal{S}\times\mathcal{A} \times [H]$ and any instance $(\tilde{\mathcal{E}}_t^{*},\mathcal{E} ) \sim \mathbb{P}_t(\tilde{\mathcal{E}}_t^{*},\mathcal{E})$ , it holds that
    \begin{equation}\label{4_3} 
    \ell_g(P_h^{\tilde{\mathcal{E}}_t^{*}}, P_h^{\mathcal{E}} )\leq\frac{\epsilon}{2BH^2}, \quad  \ell_g(R_h^{\tilde{\mathcal{E}}_t^{*}}, R_h^{\mathcal{E}}) \leq \frac{\epsilon}{2BH}
    \end{equation}
 % \begin{aligned}
      %   &\int_{\mathcal{S}} \bigg|\log \frac{P_h^{\tilde{\mathcal{E}}_t^{*}}(s'|s,a)}{P_h^{\mathcal{E}} (s'|s,a) } \bigg|\mathrm{d}\mu_{\mathcal{S}}\leq\frac{\epsilon}{2BH^2},\\
      %   &\int_{[0,1]} \bigg|\log \frac{R_h^{\tilde{\mathcal{E}}_t^{*}}(x|s,a)}{R_h^{\mathcal{E}} (x|s,a) } \bigg|\mathrm{d}x\leq\frac{\epsilon}{2BH};
      % \end{aligned}
\item  The following inequality holds:
  \begin{equation}
  \mathbb{E}_t\big[V_{1,\pi_{\mathcal{E}}^{*}}^{\mathcal{E}}(s_1^t)-V_{1,\pi_{\text{TS}}^{t}}^{\mathcal{E}}(s_1^t)\big] - \mathbb{E}_t \big[V_{1,\pi_{\mathcal{E}}^{*}}^{\tilde{\mathcal{E}}_t^{*}}(s_1^t)-V_{1,\pi_{\text{TS}}^{t}}^{\tilde{\mathcal{E}}_t^{*}}(s_1^t) \big] \leq \epsilon,
  \end{equation}
  where $\ts^t\triangleq\arg\max_{\pi\in\Pi}V_{1,\pi}^\E(s_1^t)$ is the TS policy that depends on the random environment $\mathcal{E}$.
\end{enumerate}

    
  
\end{lemma}
\begin{proof}
(1) 
    Note that $\tilde{\mathcal{E}}_t^{*}$ and $\E$ may not lie in the same partition, since $\Theta_k^{\epsilon}$  may not be convex under the new metric $\ell_g$. Nevertheless, we can use Lemma \ref{center} to bound the $\ell_g$-distance between $\tilde{\mathcal{E}}_t^{*}$ and $\E$. Let $\mathcal{C}$ be the center of $\Theta_k^{\epsilon}$, we have $\ell_g(P_h^{\tilde{\mathcal{E}}_t^{*}},P_h^{\mathcal{C}})\leq 2\delta_P$. Then, by triangle inequality of $\ell_g$ (Lemma \ref{metric}), we have $$\ell_g(P_h^{\tilde{\mathcal{E}}_t^{*}},P_h^{\mathcal{E}}) \leq \ell_g(P_h^{\tilde{\mathcal{E}}_t^{*}},P_h^{\mathcal{C}}) + \ell_g(P_h^{\mathcal{E}},P_h^{\mathcal{C}})\leq 3\delta_P=\frac{\epsilon}{2BH^2}.$$
The analysis for the reward term $\ell_g(R_h^{\tilde{\mathcal{E}}_t^{*}},R_h^{\mathcal{E}})$ is exactly the same as above, which yields the proof of the first conclusion in Lemma \ref{partition}.

\vspace{5pt}
(2)   For the second property, we divide $\mathbb{E}_t\left[V_{1,\pi_{\mathcal{E}}^{*}}^{\mathcal{E}}(s_1^t)-V_{1,\ts^{t}}^{\mathcal{E}}(s_1^t)\right] - \mathbb{E}_t \left[V_{1,\pi_{\mathcal{E}}^{*}}^{\tilde{\mathcal{E}}_t^{*}}(s_1^t)-V_{1,\ts^{t}}^{\tilde{\mathcal{E}}_t^{*}}(s_1^t) \right]$ into two parts.
    \begin{itemize}
        \item We first show that $\de_t\left[V_{1,\ts^{t}}^{\mathcal{E}}(s_1^t)\right]=\de_t\left[V_{1,\ts^{t}}^{\tilde{\mathcal{E}}_t^{*}}(s_1^t)\right].$ 
        %The key observation is that $\ts$ is independent of $\E$. 
        Let $\mathcal{E}_t \sim \mathbb{P}(\cdot | \mathcal{D}_t)$ be an independent sample of $\mathcal{E}$. By the law of total expectation and the definition of $\tilde{\mathcal{E}}_{t}^{*}$, we have
        \begin{align*}
        \mathbb{E}_t\left[ V_{1,\ts^{t}}^{\tilde{\mathcal{E}}_{t}^{*}}(s_1^t)  \right] 
       &= \sum_{k=1}^{K}\dP(\E \in \Theta_k^{\epsilon})\cdot \de_t\left[ V_{1,\ts^{t}}^{\tilde{\E}_{t}^{*}}(s_1^t) \middle| \mathcal{E} \in \Theta_k^{\epsilon}  \right] \\
       &= \sum_{k=1}^{K}\dP(\E \in \Theta_k^{\epsilon})\cdot \de_t\left[ V_{1,\ts^{t}}^{\tilde{\E}_{k,t}^{*}}(s_1^t)  \right].
       \end{align*}
    Then, using the independence over layers after conditioning on $\Theta_k^\epsilon$ and the fact that $\mathcal{E}_t$ is independent with $\mathcal{E}$, we have
    \begin{align*}
        \mathbb{E}_t\left[ V_{1,\ts^{t}}^{\tilde{\mathcal{E}}_{t}^{*}}(s_1^t)  \right] 
       &= \sum_{k=1}^{K}\dP(\E \in \Theta_k^{\epsilon})\cdot \int_{\E'\in\Theta_k^\epsilon}\de_t\left[ V_{1,\ts^{t}}^{\E'}(s_1^t)  \right]\mathrm{d}\dP(\E_t=\E'|\E_t\in\Theta_k^\epsilon) \\
       &= \sum_{k=1}^{K}\dP(\E \in \Theta_k^{\epsilon})\cdot \int_{\E'\in\Theta_k^\epsilon}\de_t\left[ V_{1,\ts^{t}}^{\E'}(s_1^t)\middle| \E_t\in\Theta_k^\epsilon \right]\mathrm{d}\dP(\E_t=\E'|\E_t\in\Theta_k^\epsilon) \\
       &= \sum_{k=1}^{K}\dP(\E \in \Theta_k^{\epsilon})\cdot \de_t\left[ V_{1,\ts^{t}}^{\E_t}(s_1^t) \middle| \E_t \in \Theta_k^{\epsilon}  \right] \\
       &= \de_t\left[V_{1,\ts^t}^\E(s_1^t)\right].
    \end{align*}
   

    \item Next, we show that $\de_t\left[V_{1,\pi_\E^*}^{\mathcal{E}}(s_1^t)\right]-\de_t\left[V_{1,\pi_\E^*}^{\tilde{\mathcal{E}}_t^{*}}(s_1^t)\right]\leq \epsilon.$ Adopting the same decomposition  trick  as in Eqn.~\eqref{A5}, we have
    \begin{align}
        &V_{1,\pi_{\mathcal{E}}^{*}}^{\E}(s_1^t)-V_{1,\pi_{\mathcal{E}}^{*}}^{\tilde{\mathcal{E}}_t^{*}}(s_1^t) \nonumber\\
        &\leq \sum_{h=1}^H\mathbb{E}_{\pi_{\E}^{*}}^{\E'}\bigg[HB\cdot\int_{\mathcal{S}}\left|\log\frac{P_h^\E(s'|s_h,a_h)}{P_h^{\E'}(s'|s_h,a_h)}\right|\mathrm{d}\mu_{\mathcal{S}}+B\cdot\int_{[0,1]}\left|\log\frac{R_h^\E(x|s_h,a_h)}{R_h^{\E'}(x|s_h,a_h)}\right|\mathrm{d}x\bigg]\nonumber\\
        &\leq\sum_{h=1}^H\mathbb{E}_{\pi_{\E}^{*}}^{\E'}\left[HB\cdot\frac{\epsilon}{2BH^2}+B\cdot\frac{\epsilon}{2BH}\right]=\epsilon,
    \end{align}
    where the second inequality is due to Eqn.~\eqref{4_3}. Adding up the two parts yields the proof of the second property in Lemma~\ref{partition}. By this, we have finished the proof of Lemma~\ref{partition}.
    \end{itemize}
\end{proof}

It is worth noting that Eqn.~\eqref{4_3} represents a unique property of the surrogate environment, specifically attributed to our metric $\ell_g$, which distinguishes it from the KL divergence. It can be proven that the $\ell_1$ distance also possesses this property. However, as seen in Section~\ref{sec:app}, Proposition~\ref{prop} cannot be guaranteed under the $\ell_1$ distance, making it difficult to design efficient approximation algorithms.
%This property is important for us to design approximation algorithms and conduct regret analysis. 

% The main idea for proof of Lemma \ref{partition} is to use $\epsilon-$balls and covering number arguments to obtain a valid $\epsilon$-value partition. 
%  \textit{Proof Outlines. } First,  we construct an $\epsilon$-value partition in the following way: 
% Define the 
% $\epsilon$-ball  centered at $\E_0$ as 
% $B(\epsilon)\triangleq\{\E\in\Theta:l_g(\E,\E_0)<\epsilon\}.$ Let $\delta_P \triangleq \epsilon/6BH^2$ and $\delta_R \triangleq \epsilon/6BH$. Let $L_h^P(\delta_P)$ and $ L_h^R(\delta_R) $ be the $\delta_P$-covering and $\delta_R$-covering numbers of $\Theta_h^{P}$ and   $\Theta_h^{R}$ respectively. We denote $\{B_{h,i}^P(\delta_P)\}_{i=1}^{L_h^P(\delta_P)}$ and $\{B_{h,i}^P(\delta_R)\}_{i=1}^{L_h^R(\delta_R)}$ as the corresponding  $\epsilon$-balls that cover  $\Theta_h^{P}$ and $\Theta_h^{R}$.
% % Let 
% % $\{B_{h,i}^P(\delta_P)\}_{i=1}^{L_h^P(\delta_P)}$,$\{B_{h,i}^P(\delta_R)\}_{i=1}^{L_h^R(\delta_R)}$ be the $\epsilon$-balls giving an $\epsilon$-covering for $\Theta_h^{P}$, $\Theta_h^{R}$ where $L_h^P(\delta_P), L_h^R(\delta_R) $ are the covering numbers. 
% For each $i_h \in [L_h^P(\delta_P) ]$, $j_h \in L_h^R(\delta_R)$, we define 
% $$ 
% \Theta^\epsilon_{h,i_h,j_h}\triangleq\left\{\E\in\Theta \mid P_h^{\mathcal{E}} \in B_{h,i_h}^P(\delta_P),R_h^{\mathcal{E}} \in B_{h,j_h}^R(\delta_R)\right\}.
% $$
% Setting $K = \prod_{h=1}^H L_h^P(\delta_P) \times L_h^R(\delta_R)$, one can then find a one-to-one mapping from $(h,i_h,j_h)$ to $[K]$, and we obtain an $\epsilon$-value partition $\cup_{k=1}^K \Theta_k^{\epsilon} = \Theta$.\footnote{If an environment $E \in \Theta$ belongs to more than one partition, we will ensure it only appears in a single partition by truncating the other partitions.} One can  show that for any $\E,\E'$ belonging to a same partition,  $V_{1,\pi_{\mathcal{E}}^{*}}^{\mathcal{E}}(s_1)-V_{1,\pi_{\mathcal{E}}^{*}}^{\mathcal{E}'}(s_1) \leq \epsilon.$

% Second, we construct the surrogate environment $\tilde{\mathcal{E}}_t^{*} $ as:
% \begin{equation}
% \label{lemma_tmp1}
%     \tilde{\mathcal{E}}_t^{*} = \tilde{\mathcal{E}}_{k,t}^{*}  \text{ iff } \mathcal{E} \in \Theta_k^{\epsilon},
% \end{equation}
% where $\tilde{\mathcal{E}}_{k,t}^{*} \triangleq \mathbb{E}_t \left[ \mathcal{E}|\mathcal{E} \in \Theta_k^{\epsilon}\right]$. 
% It can be verified that the surrogate environment $\tilde{\mathcal{E}}_t^{*} $ satisfies (1)-(3) of Lemma \ref{partition} under the $\epsilon$-value  partition we construct.



% We point out that the major improvement of our surrogate environment compared to that discussed in \cite{moradipari2023improved} lies in \eqref{4_3}, 
% \eqref{4_3} is a key property helpful to the regret analysis for our Approximate-IDS policy. 
% To make  \eqref{4_3} hold, the key step of proof is to construct a new measure: 
% \begin{equation}
% \label{new_dis}
%   \begin{aligned}
%     l_g(P,Q) & \triangleq\sup_{o\in\mathcal{O}}\Vert\log P(\cdot|o)-\log Q(\cdot|o)\Vert_1 \\
%     &=\sup_{o\in\mathcal{O}}\int_{x\in\mathcal{X}}\left|\log\frac{P(x|o)}{Q(x|o)}\right|\mathrm{d}\mu_\mathcal{X}
%   \end{aligned}
% \end{equation}
% for probability distributions $P,Q$ over $\mathcal{X}$, instead of the $l_1-$distance $l_g(P,Q)\triangleq\sup_{o\in\mathcal{O}}\Vert P(\cdot|o)- Q(\cdot|o)\Vert_1$ used in \cite{moradipari2023improved}. 


\begin{algorithm}[tb]
    \caption{IDS for RLHF}
    \label{alg}
 \begin{algorithmic}[1]
    \STATE {\bfseries Input: } Priors $\rho^P,\rho^r$,baseline policy $\pi_0$, $ \lambda>0$, surrogate environment partition tolerance $\epsilon>0$.  
    \FOR{$t=1$ {\bfseries to} $T$}
    \STATE Compute posteriors:
    \begin{equation}
    \label{alg_tmp1}
        \rho^P_t(P) \propto \rho^P(P) \prod_{i=1}^{t-1}\prod_{h=1}^{H}P_h(s_{h+1}^{i,1}|s_h^{i,1},a_h^{i,1})
    \end{equation}
    \begin{equation}
    \label{alg_tmp2}
    \begin{aligned}
        \rho^R_t(R) \propto \rho^R(R) \prod_{i=1}^{t-1}
        \big(o_i\sigma(r(\tau_1^i)-r(\tau_0^i))
         +(1-o_i)\sigma(r(\tau_0^i)-r(\tau_1^i))\big)
    \end{aligned}
    \end{equation}

    %\STATE Sample $P^t \sim \rho_t^P,r^t \sim \rho_t^r$.
    \STATE Compute the surrogate environment $\tilde{\mathcal{E}}_t^{*}$, and update policy by
    \begin{equation*}
    \ids^t=\arg\max_{\pi\in\Pi}\mathbb{E}_t[V_{1,\pi}^{\mathcal{E}}(s_1)]+ \frac{\lambda}{2} \mathbb{I}_t^\pi\left(\tilde{\mathcal{E}}_t^{*};(\mathcal{H}_t,\mathcal{R}_{t,H})\right)
    \end{equation*}
    \STATE Sample $\tau_0^t \sim \pi_0,\tau_1^t \sim \ids^t$.
    \STATE Obtain preference feedback $o_t$ on $\{\tau_0^t,\tau_1^t\}$.
    
    \ENDFOR
    
 \end{algorithmic}
 \end{algorithm}

\vspace{10pt}
\subsubsection{IDS with surrogate environments}
The pseudo-code of our IDS algorithm (with the learning target being the surrogate environment $\tilde{\mathcal{E}}^{*}_t$) is shown in Algorithm~\ref{alg}. Roughly speaking, the agent, at each episode $t$, first computes the posterior distributions of the transition kernel $P$ and reward function $R$ (as shown in Eqns.~\eqref{alg_tmp1}-\eqref{alg_tmp2}). Then, the agent computes the surrogate environment $\tilde{\E}^*_t$ based on Eqn.~\eqref{lemma_tmp1}, and chooses the policy 
$$ 
\ids^t=\arg\max_{\pi\in\Pi}\mathbb{E}_t[V_{1,\pi}^{\mathcal{E}}(s_1)]+ \frac{\lambda}{2}\cdot\mathbb{I}_t^\pi\left(\tilde{\mathcal{E}}_t^{*};(\mathcal{H}_t,\mathcal{R}_{t,H})\right).
$$
We sample two trajectories from the baseline policy $\pi_0$ and the IDS policy $\ids^t$, respectively, and then obtain a preference $o_t$ regarding the two trajectories. Moreover, human feedback and state-action sequence data  are added to the history data $\mathcal{D}_t$ for updating the posterior distribution for the next episode.





% let $\tilde{\mathcal{E}}_{k,t}^{*} := \mathbb{E}_t \big[ \mathcal{E}|\mathcal{E} \in \Theta_k^{\epsilon}\big] $. The environment $\tilde{\mathcal{E}}_t^{*}$ can be constructed as $\tilde{\mathcal{E}}_t^{*}=\tilde{\mathcal{E}}_{k,t}^{*}$ iff $\mathcal{E} \in \Theta_k^{\epsilon}$.

\subsection{Regret analysis of Algorithm~\ref{alg}}
\label{regret_analysis}
Before presenting our main results, we need to first introduce a notion of \emph{value diameter}. For any $\E$, we define the corresponding value diameter  $\alpha_\E$ as
\begin{equation*}
  \begin{aligned}
    \alpha_{\mathcal{E}}
    \triangleq \max_{1\leq h\leq H}\left\{\sup_s V_{h,\pi_{\mathcal{E}}^{*}}^{\mathcal{E}}(s) -\inf_s V_{h,\pi_{\mathcal{E}}^{*}}^{\mathcal{E}}(s) \right\} 
      +\max_{h,s,a}\left\{r_h^{\sup}(s,a)-r_h^{\inf}(s,a)\right\}.
  \end{aligned}
\end{equation*}
Since the reward is bounded by $[0,1]$, we have $\alpha_{\mathcal{E}} \leq H+1$.
The \emph{average value diameter} over $\Theta$
is denoted by $\alpha\triangleq\mathbb{E}_{\mathcal{E}\sim \rho}\big[\alpha_{\mathcal{E}}^2\big]^{1/2}$. 
Similar to the prior work \citep{moradipari2023improved}, we need to make the following assumption about posterior consistency.

\begin{assumption}[Posterior Consistency]
\label{assumption3}
    % There exists a strongly consistent estimator of the true environment  given the history.
    Under our preference model, the posterior distribution of environment is strongly consistent.
\end{assumption}
% The existence of a strongly consistent estimator guarantees that the posterior distribution is strongly consistent. 
This means as the sample size approaches infinity, the posterior distribution of environment obtained through Eqns.~\eqref{alg_tmp1}-\eqref{alg_tmp2} tends to concentrate around the true distribution. In other words, the posterior distribution will correctly identify the true environment that generates these trajectory data.

\begin{theorem}
\label{theorem}
    Given a Bayesian RLHF problem, for any $\epsilon>0$ and sufficiently large $T$, by choosing $\lambda=\sqrt{\alpha^2TH/\log(K(\epsilon))}$, we have
    \begin{equation}
       BR_T(\pi_{\text{IDS}})\leq \alpha \sqrt{TH\log(K(\epsilon))}+T\epsilon+T_0,
    \end{equation}
    where $T_0$ is a fixed positive integer that is independent of $T$. 
    % From Equation \ref{K_upper}, $ K_{\mathrm{surr}}(\epsilon) \leq K \leq \prod_{h=1}^H L_h^P(\frac{\epsilon}{6BH^2})\times L_h^R(\frac{\epsilon}{6BH}) $. 
    Setting $\epsilon=\frac{1}{T}$, our regret upper bound is of order $$O\left(H^{\frac{3}{2}}\sqrt{T\log (K(\frac{1}{T}))} \right).$$
    \label{thm:1}
\end{theorem}

The detailed proof of Theorem~\ref{thm:1} is deferred to Appendix~\ref{proof_theorem1}.
We point out that the existing TS-based RLHF algorithm \citep{wu2023making} has an upper bound of order $$\tilde{O}(H^{2}\sqrt{T(\ell_{P}+\ell_R)}(\text{dim}_1(P,1/T))+\text{dim}_1(R,1/T)),$$ where $\text{dim}_1(P,1/T))$ and $\text{dim}_1(R,1/T))$ are the $\ell_1$-norm eluder dimension of the  transition and reward function class, $\ell_P$ and $\ell _R$ are the bracketing  covering number  of the  transition and reward function class.   Without considering  the way to characterize the complexity of the the reward and the transition model (i.e., via covering number or eluder dimension), our bound is superior to theirs by a factor of $\sqrt{H}$.

% The proof of Theorem \ref{theorem} is deferred to Appendix \ref{proof_theorem1}. After reducing to the surrogate environment at first, the key step is to convert the regret of IDS policy $\ids$ into that of TS policy $\ts$ %using the optimality of $\ids$
% . Then, following the framework of \cite{moradipari2023improved}, by introducing the information ratio we can give an upper bound of $BR_T(\ts)$, which gives the proof of Theorem \ref{theorem}.


\begin{remark}
    %  From Theorem 6.9 in \cite{ghosal2017fundamentals}, it is known that Assumption \ref{assumption3}  can ensure that the posterior distribution is strongly consistent for any prior distribution of the separable space $\Theta$.
    % Thus, our algorithm and corresponding Theorem \ref{theorem} are independent of the prior.   % In fact, we use the assumption that the preference-based learning problem satisfies the preference feedback model of the given link function (this paper is the Bradley-Terry model). Also due to the consistency of the posterior distribution, our upper bound is independent of the link function.

    The regret upper bounds in some related works  \citep{saha2023dueling,wu2023making} are related to the derivative bound of the link function. However, our upper bound is independent of the link function we use (which is the sigmoid function). This is because the posterior consistency assumption implicitly imposes requirements on the link function — the link function should be monotonically increasing to ensure that better trajectories correspond to higher preference probabilities. For example, if the link function is equal to a constant $\frac{1}{2}$, then the posterior distribution of rewards would not change (according to the posterior update rule in Eqn.~\eqref{alg_tmp2}), and thus could not converge to the true distribution. Therefore, the assumption in previous work of a strictly positive lower bound on the link function's derivative is encompassed by our posterior consistency assumption.

    
\end{remark}


\subsection{Applications}
\label{application}
Finally, we show that our algorithm can be applied in multiple scenarios, such as tabular RLHF, linear RLHF, and contextual dueling bandits.
\begin{definition}[Tabular RLHF]
    We say a Bayesian RLHF problem is tabular if $|\mathcal{S}|=S$ and $|\mathcal{A}|=A$ are both finite.
\end{definition}

\begin{definition}[Linear RLHF]
    Let $\phi^{P}:\mathcal{S}\times\mathcal{A} \rightarrow \mathbb{R}^d$ and $ \phi^{R}:\mathcal{S}\times\mathcal{A} \rightarrow \mathbb{R}^d $ be known feature maps with bounded norms $||  \phi^{P}(s,a)||_2 \leq 1$ and $||  \phi^{R}(s,a)||_2 \leq 1$. We say a Bayesian RLHF problem is linear if for any $\mathcal{E}= \{ (P_h^{\mathcal{E}},R_h^{\mathcal{E}}) \}_{h=1}^{H} \in \Theta$, there exists  vector-valued maps $ \psi_h^{P,\mathcal{E}}$ and $\psi_h^{R,\mathcal{E}} $ with bounded $\ell_2$-norm such that for every $(s,a) \in \mathcal{S}\times \mathcal{A}$,
    $$ P_h^{\mathcal{E}}(\cdot|s,a)=\langle \phi^{P}(s,a),\psi_h^{P,\mathcal{E}}(\cdot) \rangle,$$
    $$ R_h^{\mathcal{E}}(\cdot|s,a)= \langle \phi^{R}(s,a),\psi_h^{R,\mathcal{E}}(\cdot) \rangle. $$
    We assume that each component of the vector-valued maps $\psi_h^{P,\mathcal{E}}$ and $\psi_h^{R,\mathcal{E}} $ belongs to some compact set $\mathcal{F} \subset L^2$, i.e.,  $\forall i \in [d]$, $(\psi_h^{P,\mathcal{E}})_i \in \mathcal{F}$ and $(\psi_h^{R,\mathcal{E}})_i \in \mathcal{F}$. 
\end{definition}

Specializing Theorem \ref{theorem} to tabular and linear Bayesian RLHF problems, we have the following Bayesian regret bounds. The proofs of Theorems \ref{corollary1} and \ref{corollary2} are deferred to Appendices \ref{b3} and \ref{b4} respectively. 
\begin{theorem}[Tabular RLHF]
    \label{corollary1}
    Given a tabular Bayesian RLHF problem, for any $\epsilon>0$ and sufficiently large $T$, we have
    \begin{equation*}
       BR_T(\ids)\leq \alpha H\sqrt{3SAT\log\left(\frac{6H^2\sqrt{S}}{\epsilon}\right)}+T\epsilon+T_0,
    \end{equation*}
    where $T_0$ is a fixed integer that is independent of $T$. Setting $\epsilon = \frac{1}{T}$, our regret bound is of order $\tilde{O}(\sqrt{SAH^4T})$.
    
\end{theorem}
Recall that the IDS algorithm proposed for the tabular RL setting~\citep{hao2022regret} has a regret upper bound of order $\tilde{O}(\sqrt{S^2A^2H^4T})$. Compared to their result, our method relies on less informative data (preference feedback instead of directly observable rewards) but achieves a better regret bound by a factor of $S$ and $A$. This improvement is primarily due to our refined analytical techniques, inspired by recent advancements in TS~\citep{moradipari2023improved}.  


\begin{theorem}[Linear RLHF]
    \label{corollary2}
     Let $M \triangleq \sup_{i,s} \max \{(\psi_h^{P}(s))_i, (\psi_h^{R}(s))_i\}$ 
    and  $K_{\mathcal{F}}(\epsilon)$ denote the  $\frac{\epsilon}{dMH^2}$-covering number of $\mathcal{F}$. 
      Given a linear RLHF problem, for any $\epsilon>0$ and sufficiently large $T$, we have
    \begin{equation}
       BR_T(\ids)\leq \alpha H \sqrt{d T  \log(K_{\mathcal{F}}(\epsilon))}+T\epsilon+T_0,
    \end{equation}
    where $T_0$ is a fixed integer that is independent of $T$.  Setting $\epsilon=\frac{1}{T}$, this upper bound is of order $$O\left(H^2\sqrt{dT \log (K_{\mathcal{F}}(\frac{1}{T}))}\right).$$
\end{theorem}
Compared to \citep{wu2023making}, which derives a regret upper bound of $\tilde{O}(H^{11/2}d^{17/2}\sqrt{T})$ for their TS algorithm, our regret upper bound is better when the covering number of the linear MDP is not of exponential size. If we convert their result to the tabular setting by coarsely substituting $d$ with $SA$, our regret bound is also better in terms of $H, S, A.$    
However, we also point out that the above comparison is not an apples-to-apples comparison, as we consider Bayesian regret, while they consider frequentist regret, and their algorithm also accounts for the number of queries.



\begin{corollary}[Contextual Dueling Bandits]
    Contextual dueling bandits are a simplified version of our MDP setting (with $H=1$) and have been extensively studied in previous RLHF research~\citep{ye2024theoretical, zhu2023principled, li2024feel}. 
    % At episode $t\in[T]$, the agent starts at $s_1^t=s_2^t= s^t$. Upon taking actions $a_1^t,a_2^t$, the agent immediately receives a Bernoulli preference feedback $o_t$ with  probability $\dP(o_t=1|s^t, a_1^t, a_2^t)=\sigma(r(s^t,a_1^t)-r(s^t,a_2^t))$ and stops further acting. This setting can be used to model the fine-tuning mechanism of LLMs, where each state $s\in\mathcal{S}$ is a prompt given by human and each action $a\in\mathcal{A}$ is a response generated by the LLM. 
    By setting $H=1$ in Theorem \ref{corollary2}, the Bayesian regret for Algorithm \ref{alg} in the linear contextual dueling bandit problem satisfies
    \begin{equation*}
       BR_T(\pi_{\text{IDS}})\leq 2 \sqrt{dT\log(K_{\mathcal{F}}(\epsilon))}+T\epsilon+T_0,
    \end{equation*}
    for any $\epsilon>0$ and sufficiently large $T$. Setting $\epsilon=\frac{1}{T}$, the regret upper bound is of order $\tilde{O}(\sqrt{dT})$. 

\end{corollary}
Without considering the covering number of linear environment, our regret upper bound is better than  $\tilde{O}(d\sqrt{T})$ derived by~\citet{li2024feel}. Another work~\citep{saha2021optimal} assumes a finite number of arms with a regret upper bound of $\tilde{O}(\sqrt{dT})$, while we assume that the parameter space of the linear MDP is compact.




\section{The Approximate-IDS Algorithm }
\label{sec:app}

While the IDS algorithm (Algorithm \ref{alg}) is principled and sample-efficient, it suffers from relatively high computational complexity. This is because the calculation of the surrogate environment $\tilde{\mathcal{E}}_t^{*}$ (which depends on the construction of $\epsilon$-value partition) is challenging.
% When searching for the IDS policy in  Algorithm \ref{alg}, one needs  to calculate the mutual information between the surrogate environment $\tilde{\mathcal{E}}_t^{*}$ and history.
% In order to obtain the surrogate environment, we need to construct the $\epsilon$-value partition $\{\Theta_k^{\epsilon}\}_{k=1}^K$ of $\Theta$. In each iteration, it is necessary to search which partition the sampled environment $\E$ belongs to, so as to obtain the surrogate environment:
% $  \tilde{\mathcal{E}}_t^{*} = \mathbb{E}_t \left[ \mathcal{E}|\mathcal{E} \in \Theta_k^{\epsilon}\right]  \text{ iff } \mathcal{E} \in \Theta_k^{\epsilon}.$
% The calculation of this part is very challenging. 
As a remedy, we develop a computationally efficient algorithm, named \emph{Approximate-IDS}, whose optimization objective is independent of the surrogate environment (thus avoids the partition of $\Theta$ in computation) and has finer properties for analysis. This allows the algorithm to
be computed efficiently by traditional RL algorithms from standard RL theory. 

\vspace{5pt}
\subsubsection{Algorithm description}
The pseudo-code for Approximate-IDS is shown in Algorithm \ref{alg2}. For convenience of description, we define 
\[ 
\mathrm{KL}^h_{s,a}(\mathcal{E},\mathcal{E}') \!\triangleq \! \kl \big( 
              (P_h^{\mathcal{E}} \otimes R_h^{\mathcal{E}})(\cdot | s, a) 
           || (P_h^{\mathcal{E}'} \otimes R_h^{\mathcal{E}'})(\cdot | s, a) 
          \big), \]
\[
\bar{r}_h(s_h,a_h) \triangleq r_h(s_h,a_h) + \frac{\lambda}{2} \cdot \mathbb{E}_t \big[\mathrm{KL}_{s_h,a_h}^h(\mathcal{E},\bar{\mathcal{E}}_t) \big],
\]
where $\bar{\E}_t$ denotes the posterior mean of $\E$ given $\md_t$, i.e., $P_h^{\bar{\mathcal{E}}_t}(\cdot|s,a)=\mathbb{E}_t[P_h^{\mathcal{E}}(\cdot |s,a)]$ and $R_h^{\bar{\mathcal{E}}_t}(\cdot|s,a)=\mathbb{E}_t[R_h^{\mathcal{E}}(\cdot |s,a)]$.

The overall procedure  is similar to that of Algorithm~\ref{alg}, with the key difference being the selection of the IDS policy (Line~4). Intuitively,    $\mathcal{E}$ is sufficiently close to $\tilde{\mathcal{E}}_t^{*}$ under metric $\ell_g$, thus it is reasonable to use $\mathcal{E}$ directly for mutual information computation. Given trajectories and rewards, the additional environmental information revealed by human feedback, i.e., $\mathbb{I}_{t}^{\pi}\big(\tilde{\mathcal{E}}_t^{*}; o_{t} \mid (\mathcal{H}_{t,H},\mathcal{R}_{t,H})\big)$, can be disregarded. 
Thus, we use the entire environment $\mathcal{E}$ instead of the surrogate environment $\tilde{\mathcal{E}}_t^{*}$  to compute the mutual information and discard the information of the trajectory generated by the baseline policy $\pi_0$ and human feedback. Therefore, we replace the mutual information term $\mathbb{I}_t^\pi\big(\tilde{\mathcal{E}}_t^{*};(\mathcal{H}_t,\mathcal{R}_{t,H})\big)$  by $\sum_{h=1}^{H} \mathbb{E}_t \big[ \de_\pi^{\bar{\mathcal{E}}_t} [\mathrm{KL}_{s_h,a_h}^h(\mathcal{E},\bar{\mathcal{E}}_t)] \big]$ (Eqn.~\eqref{temp1} in Lemma \ref{mutual}). We can compute the approximate IDS policy as follows:
\begin{equation}
    \begin{aligned}
        \app^t & \!=\! \arg \max_{\pi \in \Pi} \mathbb{E}_t[V_{1,\pi}^{\mathcal{E}}(s_1)] \!+\! \frac{\lambda}{2} \sum_{h=1}^{H} \mathbb{E}_t \big[ \de_\pi^{\bar{\mathcal{E}}_t} [\mathrm{KL}_{s_h,a_h}^h(\mathcal{E},\bar{\mathcal{E}}_t)] \big]\\
        % &= \arg \max_{\pi \in \Pi} V_{1,\pi}^{\bar{\mathcal{E}}_t}(s_1) + \frac{\lambda}{2} \sum_{h=1}^{H} \mathbb{E}_t \big[ \de_\pi^{\bar{\mathcal{E}}_t} [\mathrm{KL}(\mathcal{E},\bar{\mathcal{E}}_t)] \big] \\
        &=  \arg \max_{\pi \in \Pi}\de_\pi^{\bar{\mathcal{E}}_t}\left[\sum_{h=1}^{H}\bar{r}_h(s_h,a_h)\right].
    \end{aligned}
\end{equation}


% When selecting a policy, we make
% a slight change in the optimization target in the algorithm by replacing $r_h(s_h,a_h)$ with an ``augmented'' reward $\bar{r}_h(s_h,a_h)$ with an extra KL divergence term, i,e.,
% \[ \bar{r}_h(s_h,a_h)=r_h(s_h,a_h) + \frac{\lambda}{2} \cdot \mathbb{E}_t \big[\mathrm{KL}(\mathcal{E},\bar{\mathcal{E}}_t) \big]. \]


\begin{algorithm}[tb]
    \caption{Approximate-IDS for RLHF}
    \label{alg2}
 \begin{algorithmic}[1]
    \STATE {\bfseries Input: } Priors $\rho^P,\rho^r$,baseline policy $\pi_0$, $ \lambda>0$, surrogate environment partition tolerance $\epsilon>0$.  
    \FOR{$t=1$ {\bfseries to} $T$}
    \STATE Compute the posterior as Algorithm \ref{alg} (Line 3).
    % \[ \rho^P_t(P) \propto \rho^P(P) \prod_{i=1}^{t-1}\prod_{h=1}^{H}P_h(s_{h+1}^{i,1}|s_h^{i,1},a_h^{i,1}),  \]
    % \begin{equation}
    %   \begin{aligned}
    %       \rho^R_t(R) \propto \rho^R(R) \prod_{i=1}^{t-1}
    %       &\big(o_i\sigma(r(\tau_1^i)-r(\tau_0^i))\\
    %       & +(1-o_i)\sigma(r(\tau_0^i)-r(\tau_1^i))\big)
    %   \end{aligned}
    %   \end{equation}

    %\STATE Sample $P^t \sim \rho_t^P,r^t \sim \rho_t^r$.
    \STATE  $\app^t=\arg\max_{\pi\in\Pi}\de_\pi^{\bar{\mathcal{E}}_t}\left[\sum_{h=1}^{H}\bar{r}_h(s_h,a_h)\right]
    $

    \STATE Sample $\tau_0^t \sim \pi_0,\tau_1^t \sim \app^t$.
    \STATE Obtain preference feedback $o_t$ on $\{\tau_0^t,\tau_1^t\}$.
    
    \ENDFOR
    
 \end{algorithmic}
 \end{algorithm}

% Next, we briefly discuss about how to implement Algorithm \ref{alg2} efficiently. 
% We mainly follow the settings in \cite{hao2022regret}, which assumes that we could generate mutually independent samples using a mechanism called \textit{posterior sampling oracle}.
Note that $\bar{r}$ and $ \bar{\mathcal{E}}_t$ are both independent of the surrogate environment, and can be well approximated by Monte Carlo sampling. Therefore, by introducing  $\bar{r}$, solving $\app^t$ at episode $t$ is equivalent to finding an optimal policy based on MDP $\{P_h^{\bar{\mathcal{E}}_t}, \bar{r}_h\}_{h=1}^H$, which can be solved efficiently by the PPO algorithm~\citep{schulman2017proximal}. 

% Finally, we provide a regret bound for the Approximate-IDS algorithm. 
\vspace{10pt}
\subsubsection{Regret bounds for Approximate-IDS}
We first introduce an auxiliary reward function $r'_h$ for the convenience of regret analysis. It  serves as a bridge connecting the approximated $\bar{r}_h$ to the real mutual information term in Algorithm \ref{alg}.
\begin{proposition}
\label{prop}
   We define 
    \begin{equation}
          r'_h(s, a) \triangleq r_h(s, a) + \frac{\lambda}{2} \mathbb{E}_t \big[\mathrm{KL}_{s,a}^h(\tilde{\mathcal{E}}_t^{*}, \bar{\mathcal{E}}_t ) \big].
  \end{equation}
Then, for any policy $\pi$, we have
\begin{equation}
\label{lamep2}
    \bigg| \mathbb{E}_{\pi}^{\bar{\mathcal{E}}_t}\bigg[ \sum_{h=1}^{H}r'_h(s_h,a_h) \bigg]
    -\mathbb{E}_{\pi}^{\bar{\mathcal{E}}_t}\bigg[ \sum_{h=1}^{H}\bar{r}_h(s_h,a_h) \bigg]\bigg|  \leq \frac{\lambda}{2} \epsilon (1-2\log\beta).
\end{equation}
\end{proposition}
% The proof is deferred to Appendix \ref{propproof}. The key step of the proof is to notice that under our new measure, the two environments $\tilde{\mathcal{E}}_t^{*}$ and $\mathcal{E}$ is close to each other ( Equation \ref{4_3} in Lemma \ref{lemma} ). Although it remains unknown how far the distance between $\tilde{\mathcal{E}}_t^{*}$ and $\bar{\mathcal{E}}_t$ is, the knowledge that $\tilde{\mathcal{E}}_t^{*}$ and $\mathcal{E}$ are close is already sufficient for us to derive an upper bound for the left hand side of \eqref{lamep2}.

The proof is deferred to Appendix~\ref{propproof}.
 To better understand this proposition, consider an extreme scenario: we divide the environment into the smallest units, with each $\Theta_k^{\epsilon}$ containing only one environment. We have $\mathcal{E}=\tilde{\mathcal{E}}_t^{*}$. The left hand of Eqn.~\eqref{lamep2} equals $0$, so Proposition~\ref{prop}  holds true. 
Since $ |a-b| \leq B|\log \frac{a}{b}| $ for any $a,b \in (0,B)$, we have $\ell_1(P,Q) \leq B \ell_g(P,Q)$. If we ignore the constant 
$B$, by fixing the $\epsilon$-value, our distance achieves a finer environmental partition. On this finer partition, $\mathcal{E}$ and $\tilde{\mathcal{E}}_t^{*}$ behave more similarly, allowing us to ensure that Proposition~\ref{prop} holds. 
Then, using Proposition \ref{prop}, we give the Bayesian regret bound for the Approximate-IDS algorithm. 
\begin{theorem}\label{thm}
 Given a Bayesian RLHF problem, for any $\epsilon>0$ and sufficiently large $T$, by choosing 
 $\lambda=\sqrt{\alpha^2TH/2\log(K(\epsilon))}$, we have the following regret upper bound for Algorithm \ref{alg2}:
    \begin{equation}
      \begin{aligned}
        BR_T(\app)
        \leq \alpha\sqrt{2TH\log(K(\epsilon))}
         +\left(1+\frac{(1-2\log\beta)}{2}\sqrt{\frac{\alpha^2TH}{2\log(K(\epsilon))}}\right)T\epsilon+T_0.
      \end{aligned}
    \end{equation}
    By choosing a small $\epsilon$, the regret upper bound is of order $O\big(\sqrt{H^{3}T\log(K(\epsilon))}\big)$, matching that of Algorithm~\ref{alg} presented in Sec. \ref{regret_analysis}. 
\end{theorem}
\begin{proof}
By the optimality of $\app$, we have
\begin{align}\label{q}
\mathbb{E}_{\app^t}^{\bar{\mathcal{E}}_t}\bigg[ \sum_{h=1}^{H}\bar{r}_h(s_h,a_h) \bigg]&\geq \mathbb{E}_{\ids^t}^{\bar{\mathcal{E}}_t}\bigg[ \sum_{h=1}^{H}\bar{r}_h(s_h,a_h) \bigg]\nonumber\\
&\geq \mathbb{E}_{\ids^t}^{\bar{\mathcal{E}}_t}\bigg[ \sum_{h=1}^{H}r_h(s_h,a_h) \bigg]\nonumber\\
&=\de_t\left[V_{1,\ids^t}^{\mathcal{E}}(s_1^t)\right].
    % \bigg| \mathbb{E}_{\pi}^{\bar{\mathcal{E}}_t}\bigg[ \sum_{h=1}^{H}r'_h(s_h,a_h) \bigg]-\mathbb{E}_{\pi}^{\bar{\mathcal{E}}_t}\bigg[ \sum_{h=1}^{H}r''_h(s_h,a_h) \bigg]\bigg|\leq \frac{1}{4}\lambda \cdot \epsilon \cdot \left(1+\log\frac{1}{\beta}\right).
\end{align}
Therefore,
\begin{align}\label{w}
    \de_t\left[V_{1,\ids^t}^{\mathcal{E}}(s_1^t)\right]-\de_t\left[V_{1,\app^t}^{\mathcal{E}}(s_1^t)\right]&=\de_t\left[V_{1,\ids^t}^{\mathcal{E}}(s_1^t)\right]-\mathbb{E}_{\app^t}^{\bar{\mathcal{E}}_t}\bigg[ \sum_{h=1}^{H}r_h(s_h,a_h) \bigg]
    \nonumber\\&\leq \mathbb{E}_{\app^t}^{\bar{\mathcal{E}}_t}\bigg[ \sum_{h=1}^{H}\bar{r}_h(s_h,a_h)-r_h(s_h,a_h) \bigg]\nonumber\\
    &\leq \frac{\lambda\epsilon(1-2\log\beta)}{2}+\mathbb{E}_{\app^t}^{\bar{\mathcal{E}}_t}\bigg[ \sum_{h=1}^{H}r'_h(s_h,a_h)-r_h(s_h,a_h) \bigg]\nonumber\\
    &\leq \frac{\lambda\epsilon(1-2\log\beta)}{2}+\frac{\lambda}{2}\cdot\mathbb{I}_{t}^{\app^t}\left(\Tilde{\mathcal{E}}_t^*;(\mathcal{H}_t,\mathcal{R}_{t,H})\right),
\end{align}
where the first inequality is due to Eqn.~\eqref{q}, the second inequality is due to Proposition \ref{prop}, and the last inequality is due to Lemma \ref{mutual}.
Taking expectation in Eqn.~\eqref{w} with respect to $\mathcal{D}_t$ and then summing over $t\in[T]$, we obtain
\begin{equation}\label{r}
    BR_T(\app)-BR_T(\ids)\leq \frac{\lambda\epsilon(1-2\log\beta)T}{2}+\frac{\lambda}{2}\log(K(\epsilon)),
\end{equation}
where we use the same trick in Eqn.~\eqref{bound} to derive that  the upper bound of $\sum_{t=1}^T \mathbb{I}_{t}^{\app^t}\left(\Tilde{\mathcal{E}}_t^*;(\mathcal{H}_t,\mathcal{R}_{t,H})\right)$ is $\log(K(\epsilon))$. Finally, plugging the upper bound for $BR_T(\ids)$ (Eqn.~\eqref{e}) into  Eqn.~\eqref{r} and taking $\lambda=\sqrt{\alpha^2TH/2\log(K(\epsilon))}$
yields the proof of Theorem \ref{thm}.
\end{proof}






\section{Conclusion}

In this paper, we introduced novel information-directed sampling (IDS) algorithms to address key challenges in the RLHF problem, a critical component of LLM training. Our method improves the sample efficiency by maximizing both the value function and the mutual information between the  (surrogate) environment and trajectories. We also developed a computationally efficient Approximate-IDS algorithm suitable for real-world applications while maintaining the regret bound order of the original method. A potentially practical implication of our sample-efficient algorithms is their ability to align LLMs to human values with less human feedback while maintaining similar performance, thereby reducing the cost and time of LLM training. Additionally, our findings highlight the value of information theory in the rapidly evolving era of LLMs.


%Our findings show that IDS can enhance sample efficiency, benefiting both preference learning and general reinforcement learning problems.


\bibliographystyle{ims}
\bibliography{main}

\newpage
\appendix












\section{Proofs of Theorem and Proposition}
\label{appendix_A}
\subsection{Proof of Theorem \ref{theorem}}\label{proof_theorem1} 
\begin{unnumberedtheorem}
    Given a Bayesian RLHF problem, for any $\epsilon>0$ and sufficiently large $T$, by choosing $\lambda=\sqrt{\alpha^2TH/\log(K(\epsilon))}$, we have
    \begin{equation}
        BR_T(\ids)\leq \alpha \sqrt{TH\log(K(\epsilon))}+T\epsilon+T_0,
    \end{equation}
    where $T_0$ is a fixed positive integer that is independent of $T$.  
\end{unnumberedtheorem}
\begin{proof}
We divide the proof into 5 steps. First, we point out that by the law of total expectation, we can rewrite the Bayesian regret as 
\begin{equation}\label{3_2}
    BR_T(\pi_{\text{IDS}})=\sum_{t=1}^{T} \mathbb{E}_{\mathcal{D}_t}\left[ \mathbb{E}_{\mathcal{E} \sim \mathbb{P}(\cdot|\mathcal{D}_t)}\left[V_{1,\pi_{\mathcal{E}}^{*}}^{\mathcal{E}}(s_1^t)-V_{1,\pi^{t}}^{\mathcal{E}}(s_1^t)\right]\right],
\end{equation}
whose form is more convenient for analysis.

%The proof of Theorem \ref{theorem} is deferred to Appendix \ref{proof_theorem1}. After reducing to the surrogate environment at first, the key step is to convert the regret of IDS policy $\ids$ into that of TS policy $\ts$ %using the optimality of $\ids$
%. Then, following the framework of \cite{moradipari2023improved}, by introducing the information ratio we can give an upper bound of $BR_T(\ts)$, which gives the proof of Theorem \ref{theorem}.

\textbf{Step 1.} Reduce $BR_T(\ids)$ to the surrogate environment, and convert $BR_T(\ids)$ into $BR_T(\ts)$. 
By Lemma \ref{partition} and the optimality of $\ids$, we have 
\begin{align}\label{step1}
    &BR_T(\ids)=\sum_{t=1}^{T} \de_{\mathcal{D}_t}\left[ \mathbb{E}_{\mathcal{E} \sim \mathbb{P}(\cdot|\mathcal{D}_t)}\left[V_{1,\pi_{\mathcal{E}}^{*}}^{\mathcal{E}}(s_1^t)-V_{1,\ids^{t}}^{\mathcal{E}}(s_1^t)\right]\right]\nonumber\\
    &=\sum_{t=1}^{T}\de_{\mathcal{D}_t}\left[\mathbb{E}_{t}\left[V_{1,\pi_{\mathcal{E}}^{*}}^{\mathcal{E}}(s_1^t)-V_{1,\ids^{t}}^{\mathcal{E}}(s_1^t)\right] -\epsilon -\frac{\lambda}{2}\mathbb{I}_{t}^{\ids^t}\left(\tilde{\mathcal{E}}_{t}^{*};(\mathcal{H}_t,\mathcal{R}_{t,H})\right)\right]\\
&\quad +\frac{\lambda}{2}\sum_{t=1}^{T}\de_{\mathcal{D}_t}\left[\mathbb{I}_{t}^{\ids^t}\left(\tilde{\mathcal{E}}_{t}^{*};(\mathcal{H}_t,\mathcal{R}_{t,H})\right)\right]+T\epsilon\nonumber\\
    &\leq\sum_{t=1}^{T}\de_{\mathcal{D}_t}\left[\mathbb{E}_{t}\left[V_{1,\pi_{\mathcal{E}}^{*}}^{\tilde{\E}_t^*}(s_1^t)-V_{1,\ts^{t}}^{\tilde{\E}_t^*}(s_1^t)\right]-\frac{\lambda}{2}\mathbb{I}_{t}^{\ts^t}\left(\tilde{\mathcal{E}}_{t}^{*};(\mathcal{H}_t,\mathcal{R}_{t,H})\right)\right]\\
&\quad +\frac{\lambda}{2}\sum_{t=1}^{T}\de_{\mathcal{D}_t}\left[\mathbb{I}_{t}^{\ids^t}\left(\tilde{\mathcal{E}}_{t}^{*};(\mathcal{H}_t,\mathcal{R}_{t,H})\right)\right]+T\epsilon.
\end{align}

For the first term in Eqn.~\eqref{step1}, using the basic fact that $A-\lambda B/2\leq A^2/2\lambda B$ for $B,\lambda\geq0$, we have 
\begin{equation}
    \de_{t}\left[V_{1,\pi_{\E}^{*}}^{\tilde{\E}_t^*}(s_1^t)-V_{1,\ts^{t}}^{\tilde{\E}_t^*}(s_1^t)\right]-\frac{\lambda}{2}\mathbb{I}_{t}^{\ts^t}\left(\tilde{\E}_{t}^{*};(\mathcal{H}_t,\mathcal{R}_{t,H})\right)\leq\frac{1}{2\lambda}\frac{\left(\de_t\left[ V_{1,\pi_{\E}^{*}}^{\tilde{\E}_{t}^{*}}(s_1^t)-V_{1,\ts^{t}}^{\tilde{\E}_{t}^{*}}(s_1^t) \right]\right)^2}{ \mathbb{I}_{t}^{\ts^t}\left(\tilde{\mathcal{E}}_{t}^{*};(\mathcal{H}_t,\mathcal{R}_{t,H})\right)}\triangleq \frac{1}{2\lambda}\Gamma_t^{\ts^t}.
\end{equation}
where we introduce the tool of \emph{information ratio} $\Gamma_t^{\ts^t}$ for ease of analysis.

Let $\zeta$ be a discrete random variable taking values in $\{1,...,K(\epsilon)\}$ such that $\zeta=k$ if and only if  $\mathcal{E} \in \Theta_k^{\epsilon}$. 
From the construction of the surrogate environment (Eqn.~\eqref{lemma_tmp1}),  the distribution of $\tilde{\mathcal{E}}_t^{*}$ depend on $\E$ only through $\zeta$, i.e., $\tilde{\mathcal{E}}_t^{*}$ and $\E$ are independent conditioning on $\zeta$. 

For the second term in Eqn.~\eqref{step1}, we have
\begin{align}
\label{bound}
    \sum_{t=1}^{T}\de_{\mathcal{D}_t}\left[\mathbb{I}_{t}^{\ids^t}\left(\tilde{\mathcal{E}}_{t}^{*};(\mathcal{H}_t,\mathcal{R}_{t,H})\right)\right]
    &\leq\sum_{t=1}^{T}\de_{\mathcal{D}_t}\left[\mathbb{I}_{t}^{\ids^t}\left(\zeta;(\mathcal{H}_t,\mathcal{R}_{t,H})\right)\right]\nonumber\\
    &=\mathbb{I}(\zeta;\md_{T+1})\nonumber\\
    &\leq \mathbb{H}(\zeta)\nonumber\\
    &\leq \log(K(\epsilon)).
\end{align}
where the first inequality is due to data processing inequality, the second equality is due to the chain rule of mutual information, and the last two inequalities follow from the basic definition of entropy. Therefore, we derive an upper bound for $BR_T(\ids)$ as follows 
\begin{equation}
    BR_T(\ids)\leq \frac{1}{2\lambda}\de\left[\sum_{t=1}^{T}\Gamma_t^{\ts^t}\right]+\frac{\lambda}{2}\log(K(\epsilon))+T\epsilon.
\end{equation}


\textbf{Step 2} (Bound $\Gamma_t^{\ts^t}$). Before stepping into technical details, we need to introduce several concepts. First, 
the state-action occupancy function $d_{h,\pi}^{\mathcal{E}}:\mathcal{S}\times\mathcal{A}\to\mathbb{R}$ at step $h$ under policy $\pi$ and environment $\E$, is defined as the Radon-Nikodym derivative of the state-action occupancy measure $\mathbb{P}_{\pi}^{\mathcal{E}}((s_h,a_h)=\cdot)$ with regard to the base probability measure $\mu_{\mathcal{S}\times\mathcal{A}}$ on the product space $\mathcal{S}\times\mathcal{A}$, i.e.,
\begin{equation*}
    d_{h,\pi}^{\mathcal{E}}(s,a)\triangleq\frac{\mathrm{d}\mathbb{P}_{\pi}^{\mathcal{E}}(s_h=s,a_h=a)}{\mathrm{d}\mu_{\mathcal{S}\times\mathcal{A}}}.
\end{equation*}
For convenience of analysis, we assume that $d_{h,\pi}^{\mathcal{E}}(s,a)$ is measurable and upper bounded for all $\pi,\E,s,a,h$. Recall that, the mean environment $\bar{\mathcal{E}}_t$ is defined to satisfy $P_h^{\bar{\mathcal{E}}_t}(\cdot|s,a)=\mathbb{E}_t[P_h^{\mathcal{E}}(\cdot |s,a)]$ and $R_h^{\bar{\mathcal{E}}_t}(\cdot|s,a)=\mathbb{E}_t[R_h^{\mathcal{E}}(\cdot |s,a)]$ for all $s\in\mathcal{S}$ and $a\in\mathcal{A}$. By the definition of $d_{h,\pi}^\E$, the following equality also holds: $d_{h,\pi}^{\bar{\mathcal{E}}_t}(s,a)=\mathbb{E}_t[d_{h,\pi}^{\mathcal{E}}(s,a)]$. 
One important property of the mean environment is that the posterior mean of the surrogate environment $\de_t[\tilde{\E}_t^*]$ coincides with that of the whole environment $\bar{\E}_t$. To check this, using the property of conditional expectation:
\begin{align}
    \de_t[\tilde{\mathcal{E}}_t^{*}]&=\sum_{k=1}^K\dP(\E\in\Theta_k^\epsilon)\cdot\de_t[\tilde{\mathcal{E}}_t^{*}|\E\in\Theta_k^\epsilon]\nonumber\\
    &=\sum_{k=1}^K\dP(\E\in\Theta_k^\epsilon)\cdot\de_t[\tilde{\mathcal{E}}_{k,t}^{*}]\nonumber\\
    &=\sum_{k=1}^K\dP(\E\in\Theta_k^\epsilon)\cdot\tilde{\mathcal{E}}_{k,t}^{*}\nonumber\\
    &=\sum_{k=1}^K\dP(\E\in\Theta_k^\epsilon)\cdot\de_t[\E|\E\in\Theta_k^\epsilon]=\bar{\E}_t.\label{b7}
\end{align}

Finally, we denote the value function difference as
\begin{equation}
    \Delta_h^{\tilde{\mathcal{E}}_t^{*}}(s,a)\triangleq\mathbb{E}_{(s',r') \sim (P_h^{\tilde{\mathcal{E}}_t^{*}} \otimes R_h^{\tilde{\mathcal{E}}_t^{*}})(\cdot|s,a)}\left[ r'+V_{h+1,\pi_{\mathcal{E}}^{*}}^{\tilde{\mathcal{E}}_t^{*}}(s')\right]-\mathbb{E}_{(s',r') \sim (P_h^{\bar{\mathcal{E}}_t}\otimes R_h^{\bar{\mathcal{E}}_t})(\cdot|s,a)}\left[ r'+V_{h+1,\pi_{\mathcal{E}}^{*}}^{\tilde{\mathcal{E}}_t^{*}}(s') \right]. 
\end{equation}

Now we are ready to give an upper bound for $\Gamma_t^{\ts^t}$. We hope to use Lemma \ref{lemma} to rewrite the numerator 
$$\left(\de_t\left[ V_{1,\pi_{\E}^{*}}^{\tilde{\E}_{t}^{*}}(s_1^t)-V_{1,\ts^{t}}^{\tilde{\E}_{t}^{*}}(s_1^t) \right]\right)^2.$$ 
However, Lemma \ref{lemma} can only be applied to handle the difference between two value functions with the same policy and different environments, while in $V_{1,\pi_{\E}^{*}}^{\tilde{\E}_{t}^{*}}(s_1^t)$ and $V_{1,\ts^{t}}^{\tilde{\E}_{t}^{*}}(s_1^t)$, the environments are the same and the policies are different. For the purpose of ``unifying'' the policy, we use Eqn.~\eqref{b7} and note that $\ts$ is independent of $\tilde{\mathcal{E}}_t^{*}$, yielding $$\de_t\left[V_{1,\ts^t}^{\tilde{\E}_{t}^{*}}(s_1^t)\right]=\de_t\left[V_{1,\ts^t}^{\bar{\E}_{t}^{*}}(s_1^t)\right].$$ Furthermore, conditioned on $\mathcal{D}_t$, $\pi_{\text{TS}}^t$ and $\pi_{\E}^{*}$ are identically distributed, and are both independent of $\bar{\E}_t$. This implies $$\de_t\left[V_{1,\ts^t}^{\bar{\E}_{t}^{*}}(s_1^t)\right]=\de_t\left[V_{1,\pi_\E^*}^{\bar{\E}_{t}^{*}}(s_1^t)\right].$$ Therefore, by Lemma \ref{lemma}, we have
\begin{align}
    \mathbb{E}_t\left[ V_{1,\pi_{\mathcal{E}}^{*}}^{\tilde{\mathcal{E}}_{t}^{*}}(s_1^t)-V_{1,\pi_{\text{TS}}^{t}}^{\tilde{\mathcal{E}}_{t}^{*}}(s_1^t) \right]&=\mathbb{E}_t\left[ V_{1,\pi_{\mathcal{E}}^{*}}^{\tilde{\mathcal{E}}_{t}^{*}}(s_1^t)-V_{1,\pi_{\mathcal{E}}^{*}}^{\bar{\mathcal{E}}_{t}}(s_1^t) \right]\nonumber\\
    &= \sum_{h=1}^{H}\mathbb{E}_t \mathbb{E}_{\pi_{\mathcal{E}}^{*}}^{\bar{\mathcal{E}}_t}\left[\Delta_h^{\tilde{\mathcal{E}}_t^{*}}(s,a)\right]\nonumber\\
    &=  \sum_{h=1}^{H}\mathbb{E}_t \left[ \int_{\mathcal{S}\times \mathcal{A}}d_{h,\pi_{\mathcal{E}}^{*}}^{\bar{\mathcal{E}}_t}(s,a)\Delta_h^{\tilde{\mathcal{E}}_t^{*}}(s,a) \mathrm{d}\mu_{\mathcal{S}\times \mathcal{A}} \right],
\end{align}
where the notation of $d_{h,\pi_{\mathcal{E}}^{*}}^{\bar{\mathcal{E}}_t}(s,a)$ and $\Delta_h^{\tilde{\mathcal{E}}_t^{*}}(s,a)$ are introduced to simplify the formula. Following~\citep{moradipari2023improved}, we
define
\begin{equation}
    \mathcal{I}^t\triangleq \sum_{h=1}^H\de_t\mathbb{E}_{\ts^t}^{\bar{\mathcal{E}}_t}\left[\frac{\Delta_h^{\tilde{\E}_t^{*}}(s,a)^2}{\alpha_\E^2}\right], \quad\mathcal{T}^t\triangleq\sum_{h=1}^H\int_{\de_t[d_{h,\pi_{\mathcal{E}}^{*}}^{\bar{\mathcal{E}}_t}(s,a)]\neq 0}\frac{\de_t\left[\alpha_\E^2\cdot d_{h,\pi_{\mathcal{E}}^{*}}^{\bar{\mathcal{E}}_t}(s,a)^2\right]}{\de_t\left[d_{h,\pi_{\mathcal{E}}^{*}}^{\bar{\mathcal{E}}_t}(s,a)\right]}\mathrm{d}\mu_{\mathcal{S}\times \mathcal{A}}.
\end{equation}
By the Cauchy-Schwarz inequality, we have 
\begin{align}
    &\sum_{h=1}^{H}\mathbb{E}_t \left[ \int_{\mathcal{S}\times \mathcal{A}}d_{h,\pi_{\mathcal{E}}^{*}}^{\bar{\mathcal{E}}_t}(s,a)\Delta_h^{\tilde{\mathcal{E}}_t^{*}}(s,a) \mathrm{d}\mu_{\mathcal{S}\times \mathcal{A}} \right]\nonumber\\
    &=\sum_{h=1}^{H}\mathbb{E}_t \left[ \int_{\de_t[d_{h,\pi_{\mathcal{E}}^{*}}^{\bar{\mathcal{E}}_t}(s,a)]\neq 0}d_{h,\pi_{\mathcal{E}}^{*}}^{\bar{\mathcal{E}}_t}(s,a)\Delta_h^{\tilde{\mathcal{E}}_t^{*}}(s,a) \mathrm{d}\mu_{\mathcal{S}\times \mathcal{A}} \right]\nonumber\\
    &\leq\left(\sum_{h=1}^H\de_t\int_{\de_t[d_{h,\pi_{\mathcal{E}}^{*}}^{\bar{\mathcal{E}}_t}(s,a)]\neq 0}\frac{\alpha_\E^2\cdot d_{h,\pi_{\mathcal{E}}^{*}}^{\bar{\mathcal{E}}_t}(s,a)^2}{\de_t\left[d_{h,\pi_{\mathcal{E}}^{*}}^{\bar{\mathcal{E}}_t}(s,a)\right]}\right)^{1/2}\left(\sum_{h=1}^H\de_t\int_{\mathcal{S}\times \mathcal{A}}\de_t\left[d_{h,\pi_{\mathcal{E}}^{*}}^{\bar{\mathcal{E}}_t}(s,a)\right]\cdot\frac{\Delta_h^{\tilde{\E}_t^{*}}(s,a)^2}{\alpha_\E^2}\right)^{1/2}\nonumber\\
    &=\sqrt{\mathcal{I}^t\cdot\mathcal{T}^t}, \label{B10}
\end{align}
where the first equality is due to the fact that $\Delta_h^{\tilde{\mathcal{E}}_t^{*}}(s,a)$ is bounded ($\leq 2H$), and the second inequality is simply the Cauchy-Schwarz inequality with $\sum_{h}\de_t\int_{\mathcal{S}\times \mathcal{A}}$ as an ``integrated'' integral over the space $[H]\times\Theta\times\mathcal{S}\times\mathcal{A}$. Let us briefly discuss why the third equality holds. For the term $\mathcal{T}^t$, the derivation is straightforward, since $\de_t[X/\de_t[Y]]=\de_t[X]/\de_t[Y]$. For the term $\mathcal{I}^t$, first recall that $\de_t\left[d_{h,\pi_{\E}^{*}}^{\bar{\mathcal{E}}_t}(s,a)\right]=\de_t\left[d_{h,\ts^t}^{\bar{\mathcal{E}}_t}(s,a)\right]$ due to the property of TS. Then, we can use the independence between $d_{h,\ts^t}^{\bar{\mathcal{E}}_t}(s,a)$ and $\Delta_h^{\tilde{\E}_t^{*}}(s,a)$ given $\md_t$ to ``extract'' the expectation ($\de[XY]=\de[X]\cdot\de[Y]$ for $X,Y$ mutually independent):
\begin{align}
    \sum_{h=1}^H\de_t\int_{\mathcal{S}\times \mathcal{A}}\de_t\left[d_{h,\pi_{\mathcal{E}}^{*}}^{\bar{\mathcal{E}}_t}(s,a)\right]\cdot\frac{\Delta_h^{\tilde{\E}_t^{*}}(s,a)^2}{\alpha_\E^2}=\sum_{h=1}^H\de_t\int_{\mathcal{S}\times \mathcal{A}}d_{h,\ts^t}^{\bar{\mathcal{E}}_t}(s,a)\cdot\frac{\Delta_h^{\tilde{\E}_t^{*}}(s,a)^2}{\alpha_\E^2}=\mathcal{I}^t.
\end{align}
To summarize, by Eqn.~\eqref{B10} we have the following bound for $\Gamma_t^{\ts^t}$:
\begin{equation}\label{b13}
    \Gamma_t^{\ts^t}\leq\frac{\mathcal{I}^t\cdot\mathcal{T}^t}{\mathbb{I}_{t}^{\ts^t}\left(\tilde{\mathcal{E}}_{t}^{*};(\mathcal{H}_t,\mathcal{R}_{t,H})\right)}.
\end{equation}

\textbf{Step 3} (Bound $\mathcal{I}^t$). The key observation in this step is that $\mathcal{I}^t$ is in the form of total variation, and thus can be upper bounded by mutual information (in the form of KL divergence) by Pinsker's inequality. Specifically,
\begin{align}
  \mathcal{I}^t 
  &= \sum_{h=1}^{H} \mathbb{E}_t \mathbb{E}_{\pi_{\text{TS}}^t}^{\bar{\mathcal{E}}_t} \bigg( 
      \mathbb{E}_{(s',r') \sim (P_h^{\tilde{\mathcal{E}}_t^{*}} \otimes R_h^{\tilde{\mathcal{E}}_t^{*}})(\cdot|s_h,a_h)}
      \bigg[ 
          \frac{r' + V_{h+1,\pi_{\mathcal{E}}^{*}}^{\tilde{\mathcal{E}}_t^{*}}(s')}{\alpha_{\mathcal{E}}}
      \bigg] 
      - \mathbb{E}_{(s',r') \sim (P_h^{\bar{\mathcal{E}}_t} \otimes R_h^{\bar{\mathcal{E}}_t})(\cdot|s_h,a_h)}
      \bigg[ 
          \frac{r' + V_{h+1,\pi_{\mathcal{E}}^{*}}^{\tilde{\mathcal{E}}_t^{*}}(s')}{\alpha_{\mathcal{E}}} 
      \bigg] 
  \bigg)^2 \nonumber \\
  &= \sum_{h=1}^{H} \mathbb{E}_t \mathbb{E}_{\pi_{\text{TS}}^t}^{\bar{\mathcal{E}}_t} \bigg( 
      \mathbb{E}_{(s',r') \sim (P_h^{\tilde{\mathcal{E}}_t^{*}} \otimes R_h^{\tilde{\mathcal{E}}_t^{*}})(\cdot|s_h,a_h)}
      \bigg[ 
          \frac{r' - r_h^{\inf}(s_h,a_h) + V_{h+1,\pi_{\mathcal{E}}^{*}}^{\tilde{\mathcal{E}}_t^{*}}(s') - \inf_s V_{h+1,\pi_{\mathcal{E}}^{*}}^{\tilde{\mathcal{E}}_t^{*}}(s')}{\alpha_{\mathcal{E}}}
      \bigg] \\
      &\qquad - \mathbb{E}_{(s',r') \sim (P_h^{\bar{\mathcal{E}}_t} \otimes R_h^{\bar{\mathcal{E}}_t})(\cdot|s_h,a_h)}
      \bigg[ 
          \frac{r' - r_h^{\inf}(s_h,a_h) + V_{h+1,\pi_{\mathcal{E}}^{*}}^{\tilde{\mathcal{E}}_t^{*}}(s') - \inf_s V_{h+1,\pi_{\mathcal{E}}^{*}}^{\tilde{\mathcal{E}}_t^{*}}(s')}{\alpha_{\mathcal{E}}} 
      \bigg] 
  \bigg)^2 \nonumber \\
  &\leq \frac{1}{2} \sum_{h=1}^{H} \mathbb{E}_t \mathbb{E}_{\pi_{\text{TS}}^t}^{\bar{\mathcal{E}}_t} \left[
      D_{\text{KL}} \left( 
          \left( P_h^{\tilde{\mathcal{E}}_t^{*}} \otimes R_h^{\tilde{\mathcal{E}}_t^{*}} \right)(\cdot|s_h,a_h) \middle\| 
          \left( P_h^{\bar{\mathcal{E}}_t} \otimes R_h^{\bar{\mathcal{E}}_t} \right)(\cdot|s_h,a_h)
      \right) 
  \right] \nonumber \\
  &\leq \frac{1}{2} \mathbb{I}_{t}^{\pi_{\text{TS}}^t} \left( \tilde{\mathcal{E}}_{t}^{*}; (\mathcal{H}_t, \mathcal{R}_{t,H}) \right),
\end{align}
where the two inequalities are due to Pinsker's inequality and Lemma \ref{mutual} respectively. Plugging into Eqn.~\eqref{b13}, we derive that $\Gamma_t^{\pi_{\text{TS}}^t} \leq \frac{1}{2}\mathcal{T}^t$, and thus 
\begin{equation}\label{b49}
    BR_T(\ids)\leq\frac{1}{4\lambda}\de\left[\sum_{t=1}^T \mathcal{T}^t\right]+\frac{\lambda}{2}\log(K(\epsilon))+T\epsilon.
\end{equation}

\textbf{Step 4} (Bound $\mathbb{E}[\mathcal{T}^t]$). The analysis tools used in this step is the Doob's consistency theorem, with more details discussed in Appendix \ref{doob}. Define the true environment as $\E_0$. For brevity of notations, we define $$\mathcal{B}_{h,t}\triangleq\left\{(s,a)\in\mathcal{S}\times\mathcal{A}\middle| \mathbb{E}\left[d_{h,\pi_{\mathcal{E}}^{*}}^{\bar{\mathcal{E}}_t}(s,a) \right] \neq 0 \right\},$$ so that we can write $\mathcal{T}^t$ as
\begin{equation}
    \mathcal{T}^t=\sum_{h=1}^H\int_{(s,a)\in \mathcal{B}_{h,t}} \frac{\de_t\left[\alpha_\E^2\cdot d_{h,\pi_{\mathcal{E}}^{*}}^{\bar{\mathcal{E}}_t}(s,a)^2\right]}{\de_t\left[d_{h,\pi_{\mathcal{E}}^{*}}^{\bar{\mathcal{E}}_t}(s,a)\right]}\mathrm{d}\mu_{\mathcal{S}\times \mathcal{A}}.
\end{equation}
We now introduce another variable $\E'$ to apply Lemma \ref{strong}. Let $\E'\sim\dP(\cdot|\md_t)$ be a random variable  independent of $\E$. By definition of $d_{h,\pi_{\mathcal{E}}^{*}}^{\E}$, we have $\de_t\left[d_{h,\pi_{\mathcal{E}}^{*}}^{\bar{\mathcal{E}}_t}(s,a)\right]=\de_{\E,\E'\sim\dP_t(\cdot)}\left[d_{h,\pi_{\mathcal{E}}^{*}}^{\E'}(s,a)\right]$, and
\begin{align}
    \de_t\left[\alpha_\E^2\cdot d_{h,\pi_{\mathcal{E}}^{*}}^{\bar{\mathcal{E}}_t}(s,a)^2\right]&=\de_{\E\sim\dP_t(\cdot)}\left[\alpha_\E^2\cdot\de_{\E'\sim\dP_t(\cdot)}\left[d_{h,\pi_{\E}^{*}}^{\E'}(s,a)^2\right]\right]\nonumber\\
    &\leq \de_{\E,\E'\sim\dP_t(\cdot)}\left[\alpha_\E^2\cdot d_{h,\pi_{\E}^{*}}^{\E'}(s,a)^2\right],
\end{align}
where the inequality is due to the fact that $\de[X^2]\geq (\de[X])^2$. Therefore, we have
\begin{equation}
    \mathcal{T}^t\leq\sum_{h=1}^H\int_{\mathcal{S}\times\mathcal{A}}\frac{\de_t\left[\alpha_\E^2\cdot d_{h,\pi_{\mathcal{E}}^{*}}^{\E'}(s,a)^2\right]}{\de_t\left[d_{h,\pi_{\E}^*}^{\E'}(s,a)\right]}\chi_{\mathcal{B}_{h,t}}(s,a)\mathrm{d}\mu_{\mathcal{S}\times \mathcal{A}} =\sum_{h=1}^H\int_{\mathcal{S}\times \mathcal{A}} g_t(s,a,h,\mathcal{D}_t)\mathrm{d}\mu_{\mathcal{S}\times \mathcal{A}} ,
\end{equation}
where $\chi_A(\cdot)$ is the indicator function, i.e., $\chi_A(x)=1$ if $x\in A$; $\chi_A(x)=0$ if $x\notin A$. 
Since $d_{h,\pi}^{\mathcal{E}}(s,a)$ is assumed to be bounded, let $$M_d \triangleq \sup_{s,a,h,\pi,\E} d_{h,\pi}^{\mathcal{E}}(s,a) < \infty.$$ This implies $g_t(s,a,h,\mathcal{D}_t) \leq M_d H^2 $ and $\mathcal{T}^t \leq M_d H^3$.
By Lemma \ref{strong}, we have the following convergence results: for any $(s,a)\in\mathcal{S}\times \mathcal{A}$,
\begin{equation}
    \lim_{t \to \infty} \mathbb{E}_t\left[\alpha_{\E}^2d_{h,\pi_{\mathcal{E}}^{*}}^{\mathcal{E}'}(s,a)^2 \right] =\alpha_{\mathcal{E}_0}^2d_{h,\pi_{\mathcal{E}_0}^{*}}^{\mathcal{E}_0}(s,a)^2,
\end{equation}
\begin{equation}
    \lim_{t \to \infty} \mathbb{E}_t\left[d_{h,\pi_{\mathcal{E}}^{*}}^{\mathcal{E}'}(s,a) \right] = d_{h,\pi_{\mathcal{E}_0}^{*}}^{\mathcal{E}_0}(s,a),
\end{equation}
 and for any $x$,
\begin{equation}
\label{chi}
   \lim_{t \to \infty} \chi_{\mathcal{B}_{h,t}}(x) = \chi_{\mathcal{B}_h}(x),  \qquad  \text{where } \mathcal{B}_h\triangleq \left\{(s,a)\in\mathcal{S}\times\mathcal{A}\middle| \mathbb{E}\left[d_{h,\pi_{\mathcal{E}_0}^{*}}^{\E_0}(s,a) \right] \neq 0 \right\}.
\end{equation}
 From Eqn.~\eqref{chi}, we can also derive that $\lim_{t \to \infty} \chi_{\mathcal{B}_{h,t}\bigcap \mathcal{B}_h}(x)=1$, and $\lim_{t \to \infty} \chi_{\mathcal{B}_{h,t}\setminus \mathcal{B}_h}(x)=0$. By Lebesgue dominated convergence theorem, we have
\begin{align}
    \lim_{t\to\infty}\de\left[\mathcal{T}^t\middle|\E_0\right]&=\lim_{t\to\infty}\de_{\md_t\sim\dP(\cdot|\E_0)}\sum_{h=1}^H\int_{\mathcal{S}\times\mathcal{A}}\frac{\de_t\left[\alpha_\E^2\cdot d_{h,\pi_{\mathcal{E}}^{*}}^{\E'}(s,a)^2\right]}{\de_t\left[d_{h,\pi_{\E}^*}^{\E'}(s,a)\right]}\cdot\chi_{\mathcal{B}_{h,t}}(s,a)\mathrm{d}\mu_{\mathcal{S}\times \mathcal{A}}\nonumber\\
    &\leq \lim_{t\to\infty}\de_{\md_t\sim\dP(\cdot|\E_0)}\sum_{h=1}^H\int_{\mathcal{S}\times\mathcal{A}}\frac{\de_t\left[\alpha_\E^2\cdot d_{h,\pi_{\mathcal{E}}^{*}}^{\E'}(s,a)^2\right]}{\de_t\left[d_{h,\pi_{\E}^*}^{\E'}(s,a)\right]}\cdot\chi_{\mathcal{B}_{h,t}\bigcap\mathcal{B}_h}(s,a)\nonumber\\&\quad\quad+M_dH^2\lim_{t\to\infty}\sum_{h=1}^H\int_{\mathcal{S}\times\mathcal{A}}\chi_{\mathcal{B}_{h,t}\setminus\mathcal{B}_h}(s,a)\nonumber\\
    &=\sum_{h=1}^H\int_{\mathcal{S}\times\mathcal{A}}\de_{\md_t\sim\dP(\cdot|\E_0)}\left[\lim_{t\to\infty}\frac{\de_t\left[\alpha_\E^2\cdot d_{h,\pi_{\mathcal{E}}^{*}}^{\E'}(s,a)^2\right]}{\de_t\left[d_{h,\pi_{\E}^*}^{\E'}(s,a)\right]}\cdot\chi_{\mathcal{B}_{h,t}\bigcap\mathcal{B}_h}(s,a)\right]\nonumber\\
    &=\alpha_{\E_0}^2\cdot\int_{\mathcal{S}\times\mathcal{A}}d_{h,\pi_{\mathcal{E}_0}^{*}}^{\E_0}(s,a)\nonumber\\
    &\leq\alpha_{\E_0}^2\cdot H.
\end{align}
\begin{comment}
\begin{equation}
    \begin{aligned}
       \lim_{t \to \infty} \mathbb{E}\big[ \mathcal{T}^t| \mathcal{E}_0\big] &\leq \lim_{t \to \infty} \mathbb{E}_{\mathcal{D}_t\sim\mathbb{P}(\cdot|\mathcal{E}_0)} \int_{s,a,h}g_t(s,a,h,\mathcal{D}_t)(\chi_{\mathcal{B}_t \bigcap \mathcal{C}}((s,a,h)+ \chi_{\mathcal{B}_t \setminus \mathcal{C}}((s,a,h))\\
       &\leq \lim_{t\to \infty} \int_{s,a,h} \mathbb{E}_{\mathcal{D}_t\sim\mathbb{P}(\cdot|\mathcal{E}_0)} \big[ g_t(s,a,h,\mathcal{D}_t)\chi_{\mathcal{B}_t \bigcap \mathcal{C}}((s,a,h) \big] + M_d H^2 \lim_{t \to \infty}\int_{s,a,h} \chi_{\mathcal{B}_t \setminus \mathcal{C}}((s,a,h))\\
       &= \int_{s,a,h} \mathbb{E}_{\mathcal{D}_t\sim\mathbb{P}(\cdot|\mathcal{E}_0)} \big[\lim_{t\to \infty}  g_t(s,a,h,\mathcal{D}_t)\chi_{\mathcal{B}_t \bigcap \mathcal{C}}((s,a,h) \big]\\
       &= \int_{\mathcal{C}}\alpha_{\mathcal{E}_0}^2d_{h,\pi_{\mathcal{E}_0}^{*}}^{\mathcal{E}_0}(s,a)\\
       &\leq \alpha_{\mathcal{E}_0}^2 H.
    \end{aligned}
\end{equation}
\end{comment}
Thus, we have
\begin{equation}\label{b23}
    \lim_{t \to \infty} \mathbb{E}[\mathcal{T}^t]= \lim_{t \to \infty}\mathbb{E}[ \mathbb{E}[\mathcal{T}^t|\mathcal{E}_0]] = \mathbb{E}[ \lim_{t \to \infty} \mathbb{E}[\mathcal{T}^t|\mathcal{E}_0]] \leq \mathbb{E}[\alpha_{\mathcal{E}_0}^2 H] =\alpha^2 H.
\end{equation}

\textbf{Step 5:} By Eqn.~\eqref{b23}, we derive that there exists $T_0>0$ such that $\mathbb{E}[\mathcal{T}^t] \leq 2\alpha^2H$ for $t > T_0$.
Plugging into Eqn.~\eqref{b49} and then taking $\lambda=\sqrt{\alpha^2TH/{\log(K(\epsilon))}}$, we obtain 
\begin{align}\label{e}
    BR_T(\pi_{\text{r-IDS}})&\leq \frac{T\alpha^2H}{2\lambda}+ \frac{\lambda}{2}\log(K(\epsilon))+T\epsilon+T_0\nonumber\\
    &\leq\alpha \sqrt{TH\log(K(\epsilon))}+T\epsilon+T_0,
\end{align}
which finishes the proof of Theorem \ref{theorem}.
\end{proof}






\subsection{Proof of Theorem \ref{corollary1}}
\label{b3}
Recall that
\begin{equation}
\begin{aligned}
&V_{1, \pi}^{\mathcal{E}}\big(s_1^{\ell}\big)-V_{1, \pi}^{\mathcal{E}'}\big(s_1^{\ell}\big)\\
&=\sum_{h=1}^H \mathbb{E}_{\pi}^{\mathcal{E}'}\bigg[\mathbb{E}_{s^{\prime} \sim P_h^{\mathcal{E}}\big(\cdot \mid s_h, a_h\big)}\left[V_{h+1, \pi}^{\mathcal{E}}\left(s^{\prime}\right)\right]-\mathbb{E}_{s^{\prime} \sim P_h^{\mathcal{E}'}\left(\cdot \mid s_h, a_h\right)}\big[V_{h+1, \pi}^{\mathcal{E}}\big(s^{\prime}\big)\big]\bigg] 
 +\sum_{h=1}^H \mathbb{E}_{\pi}^{\mathcal{E}'}\left[r_h^{\mathcal{E}}\left(s_h, a_h\right)-r_h^{\mathcal{E}'}\left(s_h, a_h\right)\right]\\
&= \sum_{h=1}^H \mathbb{E}_{\pi}^{\mathcal{E}'} \bigg[P_h^{\mathcal{E}}\big(\cdot \mid s_h, a_h\big)^T V_{h+1, \pi}^{\mathcal{E}}\big(\cdot \big) - P_h^{\mathcal{E}'}\big(\cdot \mid s_h, a_h\big)^T V_{h+1, \pi}^{\mathcal{E}'}\big(\cdot \big)   \bigg] \\
&\quad + \sum_{h=1}^H \mathbb{E}_{\pi}^{\mathcal{E}'} \bigg[P_h^{\mathcal{E}'}\big(\cdot \mid s_h, a_h\big)^T \big( V_{h+1, \pi}^{\mathcal{E}'}\big(\cdot \big) -  V_{h+1, \pi}^{\mathcal{E}}\big(\cdot \big) \big)   \bigg] +\sum_{h=1}^H \mathbb{E}_{\pi}^{\mathcal{E}'}\left[r_h^{\mathcal{E}}\left(s_h, a_h\right)-r_h^{\mathcal{E}'}\left(s_h, a_h\right)\right]\\
&\leq \sum_{h=1}^H \mathbb{E}_{\pi}^{\mathcal{E}'} \bigg[P_h^{\mathcal{E}}\big(\cdot \mid s_h, a_h\big)^T V_{h+1, \pi}^{\mathcal{E}}\big(\cdot \big) - P_h^{\mathcal{E}'}\big(\cdot \mid s_h, a_h\big)^T V_{h+1, \pi}^{\mathcal{E}'}\big(\cdot \big)   \bigg] \\
&\quad + \sum_{h=1}^H \mathbb{E}_{\pi}^{\mathcal{E}'} \bigg[ \Vert V_{h+1, \pi}^{\mathcal{E}'}\big(\cdot \big) -  V_{h+1, \pi}^{\mathcal{E}}\big(\cdot \big) \Vert_2   \bigg]  +\sum_{h=1}^H \mathbb{E}_{\pi}^{\mathcal{E}'}\left[r_h^{\mathcal{E}}\left(s_h, a_h\right)-r_h^{\mathcal{E}'}\left(s_h, a_h\right)\right]\\
\end{aligned}
\end{equation}


Since $P_h^{\mathcal{E}}\big(\cdot \mid s_h, a_h\big)^T V_{h+1, \pi}^{\mathcal{E}}\big(\cdot \big) \in [0,H]$,  we can  divide the value range  $[0,H]$ evenly into $\frac{3H^2}{\epsilon}$   parts. For each $(s,a,h)$, we  construct a covering set $\{ \mathcal{I}_{sah}^1,...,\mathcal{I}_{sah}^m \}$ for $[0,H]$ where $m=\frac{3H^2}{\epsilon}$. Each set is of length $\frac{\epsilon}{3H}$. Since $\Vert V_{h+1, \pi}^{\mathcal{E}}\big(\cdot \big) \Vert_2 \in [0,H\sqrt{S}]$, we construct a covering set $\{ \mathcal{J}_h^1,..., \mathcal{J}_h^{m'}\}$ for $[0,H\sqrt{S}]$ where $m'= \frac{6H^2\sqrt{S}}{\epsilon}$.
For reward function, we divide the value range $[0,1]$ evenly into $\frac{2H}{\epsilon}$ parts for all $ (s,a) \in \mathcal{S} \times \mathcal{A},h \in [H] $. The covering set is $\{ \mathcal{C}_1,...,\mathcal{C}_n \}$ where $n=\frac{3H}{\epsilon}$.
Then, we construct the partition $\{\Theta_k\}_{k=1}^K$ that $\mathcal{E} \in \Theta_k$ if for any $s,a,h$,
\[P_h^{\mathcal{E}}\big(\cdot \mid s, a\big)^T V_{h+1, \pi}^{\mathcal{E}}\big(\cdot \big) \in \mathcal{I}_{sah}^{k_1}, \quad  \Vert V_{h+1, \pi}^{\mathcal{E}}\big(\cdot \big) \Vert_2 \in \mathcal{J}_h^{k_2}, r_h^{\mathcal{E}}\left(s, a\right) \in \mathcal{C}_{k_3},  \]
where $k_1 \in [m], k_2 \in [m'], k_3 \in [n]$.

Therefore, $\{\Theta_k\}_{k=1}^K$ is a partition of $\Theta$. For any $k \in [K]$  and $\mathcal{E},\mathcal{E}' \in \Theta_k$, the following holds for any $s,a,h$,
\[ P_h^{\mathcal{E}}\big(\cdot \mid s, a\big)^T V_{h+1, \pi}^{\mathcal{E}}\big(\cdot \big) - P_h^{\mathcal{E}'}\big(\cdot \mid s, a\big)^T V_{h+1, \pi}^{\mathcal{E}'}\big(\cdot \big) \leq \frac{\epsilon}{3H}, \]
\[ \Vert V_{h+1, \pi}^{\mathcal{E}}\big(\cdot \big) - V_{h+1, \pi}^{\mathcal{E}'}\big(\cdot \big) \Vert_2 \leq \frac{\epsilon}{3H}, \]
\[ r_h^{\mathcal{E}}\left(s, a\right)-r_h^{\mathcal{E}'}\left(s, a\right) \leq \frac{\epsilon}{3H}.\]
Then, we have 
 \[ V_{1,\pi_{\mathcal{E}}^{*}}^{\mathcal{E}}(s_1^t)-V_{1,\pi_{\mathcal{E}}^{*}}^{\mathcal{E}'}(s_1^t) \leq \epsilon.\]
 
Since $K(\epsilon)\leq  (\frac{3H^2}{\epsilon})^{SAH} \cdot (\frac{6H^2\sqrt{S}}{\epsilon})^{H} \cdot (\frac{3H}{\epsilon})^{SAH} $, we have 
\[ \log(K(\epsilon)) \leq SAH \log(\frac{3H^2}{\epsilon}) + H\log(\frac{6H^2\sqrt{S}}{\epsilon}) + SAH \log(\frac{3H}{\epsilon}) \leq 3SAH \log(\frac{6H^2\sqrt{S}}{\epsilon}).  \]
From Theorem \ref{theorem}, we have
\[ BR_T(\pi_{\text{r-IDS }}) \leq \alpha \sqrt{ 3SATH^2\log(\frac{6H^2\sqrt{S}}{\epsilon}) } +T\epsilon+T_0. \]






\subsection{Proof of Theorem \ref{corollary2}}
\label{b4}
Recall that $\mathcal{F}$ is the  compact feature space of $(\psi_h^{P})_i$ and $(\psi_h^{R})_i$. From the compactness of $\mathcal{F}$, there exists a finite $\epsilon$-covering number of $\mathcal{F}$. Let $M \triangleq \sup_{i,s} \max \{(\psi_h^{P}(s))_i (\psi_h^{R}(s))_i\}$.
Denote the  $\frac{\epsilon}{dMH^2}$-covering number of $\mathcal{F}$ as $K_{\mathcal{F}}(\epsilon)$.   We have $\mathcal{F}\subset \mathcal{K}_1 \bigcup ... \bigcup \mathcal{K}_{K_{\mathcal{F}}(\epsilon)} $ and for any $f,f' \in \mathcal{K}_i$, 
\[ \ell_g(f,f')=\int_{s} | \log \frac{f(s)}{f'(s)} | \leq \frac{\epsilon}{dMH^2}.  \]
Then we construct the partition of $\Theta$ as following:  $\mathcal{E}$ and $\mathcal{E}'$ belong to the same partition if and only if $(\psi_h^{P,\mathcal{E}})_i$ and $(\psi_h^{P,\mathcal{E}'})_i$ belong to the same partition of $\mathcal{F}$, $\forall i \in [d]$.

Recall that
\begin{align}
    &V_{1,\pi_{\mathcal{E}}^{*}}^{\E}(s_1)-V_{1,\pi_{\mathcal{E}}^{*}}^{\E'}(s_1) \nonumber\\
    &=\sum_{h=1}^{H}\mathbb{E}_{\pi_{\E}^{*}}^{\E'}\bigg[\mathbb{E}_{s'\sim P_h^{\E}(\cdot|s_h,a_h)}\left[ V_{h+1,\pi_{\mathcal{E}}^{*}}^{\E}(s')\right] -\mathbb{E}_{s'\sim P_h^{\E'}(\cdot|s_h,a_h)}\left[ V_{h+1,\pi_{\mathcal{E}}^{*}}^{\E}(s')\right]\bigg]+\sum_{h=1}^H\mathbb{E}_{\pi_{\E}^{*}}^{\E'}\left[ R_h^\E(s_h,a_h)-R_h^{\E'}(s_h,a_h)\right]\nonumber\\
    &\leq\sum_{h=1}^H\mathbb{E}_{\pi_{\E}^{*}}^{\E'}\bigg[\int_{\mathcal{S}}\left|P_h^\E(s'|s_h,a_h)-P_h^{\E'}(s'|s_h,a_h)\right|\cdot V_{h+1,\pi_{\E}^{*}}^{\E}(s')\mathrm{d}\mu_{\mathcal{S}}+\int_{[0,1]}\left|x\left(R_h^\E(x|s_h,a_h)-R_h^{\E'}(x|s_h,a_h)\right)\right|\mathrm{d}x\bigg]\nonumber\\
    &\leq H \sum_{h=1}^{H}\ell_1(P_h^{\E},P_h^{\E'})+ \sum_{h=1}^{H} \ell_1(R_h^{\E},R_h^{\E'}). \nonumber \\
\end{align}
Next, we use the coverage of $\mathcal{F}$ under $\ell_g$ to bound $\ell_1(P_h^{\E},P_h^{\E'})$ and $\ell_1(R_h^{\E},R_h^{\E'}) $.
% Let $\Theta'$ denote the feature map space of $\psi_h^{P}$ and $\psi_h^{R}$. Since $\Theta$ is compact, from the definition of linear MDP, we know there also exists finite $\epsilon$-covering number of  $\Theta'$. 


% Define $\ell_1(\psi_h^{\mathcal{E}},\psi_h^{\mathcal{E}'})=\int_s \parallel \psi_h^{\mathcal{E}}-\psi_h^{\mathcal{E}'}  \parallel_1$.
$\parallel \phi_h^{P}(s,a) \parallel_2 \leq 1 \Rightarrow |\phi_h^{P}(s,a)_i| \leq 1, \forall i \in [d] $. For any $\mathcal{E},\mathcal{E}'$ that belong to the same partition, we have 
\begin{equation}
\label{linear1}
    \begin{aligned}
        \ell_1(P_h^{\mathcal{E}},P_h^{\mathcal{E}'})
        &=\sup_{s,a} \int_{s'} |P_h^{\mathcal{E}}(s'|s,a)-P_h^{\mathcal{E}'}(s'|s,a)| \\
        &= \sup_{s,a} \int_{s'} | \langle \phi_h^{P}(s,a) ,\psi_h^{P,\mathcal{E}}(s')-\psi_h^{P,\mathcal{E}'}(s')   \rangle |\\
        &\leq  \int_{s'} \sum_{i=1}^{d}|(\psi_h^{P,\mathcal{E}}(s')-\psi_h^{P,\mathcal{E}'}(s')   )_i| \\
        &\leq M\cdot \sum_{i=1}^d \ell_g((\psi_h^{P,\E})_i, (\psi_h^{P,\E'})_i )\\
        &\leq \frac{\epsilon}{H^2}.
    \end{aligned}
\end{equation}
The penultimate inequality uses the fact that $|a-b| \leq M|\log\frac{a}{b}|$ for any $a,b \in (0,M)$. By analogy, it can be concluded that $\ell_1(R_h^{\mathcal{E}},R_h^{\mathcal{E}'}) \leq \frac{\epsilon}{H^2}$.
Thus, 
\[ V_{1,\pi_{\mathcal{E}}^{*}}^{\E}(s_1)-V_{1,\pi_{\mathcal{E}}^{*}}^{\E'}(s_1) \leq 2\epsilon.  \]
Therefore, we get an $\epsilon$-value partition of $\Theta$.  Since $\psi_h^{P,\mathcal{E}}=( (\psi_h^{P,\mathcal{E}})_1,(\psi_h^{P,\mathcal{E}})_2,...,(\psi_h^{P,\mathcal{E}})_d )$ and each $(\psi_h^{P,\mathcal{E}})_i$ belongs to one of these $K_{\mathcal{F}}(\epsilon)$ sets ($\mathcal{K}_1,...,\mathcal{K}_{K_{\mathcal{F}}(\epsilon)})$,
the   number of this $\epsilon$-value partition can be bounded by $(K_{\mathcal{F}}(\epsilon))^{dH}$.
% Let $K_{\text{surr}}^{\psi}(\epsilon)$ denote the minimum $\epsilon$-covering number of  $\Theta'$. Since $ \ell_1(P_h^{\mathcal{E}},P_h^{\mathcal{E}'}) \leq \ell_1(\psi_h^{\mathcal{E}},\psi_h^{\mathcal{E}'}) $, we have  $ K_{\text{surr}}(\epsilon) \leq K_{\text{surr}}^{\psi}(\epsilon)$. Therefore, we have completed the proof.
Thus, we have
\[  BR_T(\ids)\leq \alpha H \sqrt{d T  \log(K_{\mathcal{F}}(\epsilon))}+T\epsilon+T_0.  \]

\subsection{Proof of Proposition \ref{prop}}
\label{propproof}
\begin{unnumberedprop}
\label{prop2}
    Define 
    \begin{equation}
    r'_h(s,a)=r_h(s,a)+\frac{\lambda}{2}\mathbb{E}_t  \left[ \kl \left( (P_h^{\tilde{\mathcal{E}}_t^{*}} \otimes R_h^{\tilde{\mathcal{E}}_t^{*}})(\cdot|s_h, a_h) \middle\| (P_h^{\bar{\mathcal{E}}_t} \otimes R_h^{\bar{\mathcal{E}}_t})(\cdot|s_h, a_h) \right) \right].
\end{equation}
Then, for any policy $\pi$, we have
\begin{equation}
\label{lamep}
    \bigg| \mathbb{E}_{\pi}^{\bar{\mathcal{E}}_t}\bigg[ \sum_{h=1}^{H}r'_h(s_h,a_h) \bigg]-\mathbb{E}_{\pi}^{\bar{\mathcal{E}}_t}\bigg[ \sum_{h=1}^{H}\bar{r}_h(s_h,a_h) \bigg]\bigg|\leq \frac{\lambda}{2} \cdot \epsilon \cdot \left(1+2\log\frac{1}{\beta}\right).
\end{equation}
\end{unnumberedprop}

\begin{proof}[Proof]
    We begin our proof by calculating the difference of the two KL divergence terms. Recall that
    \[ \bar{r}_h(s_h,a_h)= r_h(s,a)+\frac{\lambda}{2}\mathbb{E}_t  \left[ \kl \left( (P_h^{\mathcal{E}} \otimes R_h^{\mathcal{E}})(\cdot|s_h, a_h) \middle\| (P_h^{\bar{\mathcal{E}}_t} \otimes R_h^{\bar{\mathcal{E}}_t})(\cdot|s_h, a_h) \right) \right].\]
    First, by triangle inequality, we have
\begin{align}\label{y}
        &\bigg|D_{\text{KL}} \left( (P_h^{\tilde{\mathcal{E}}_t^{*}} \otimes R_h^{\tilde{\mathcal{E}}_t^{*}})(\cdot|s_h, a_h) \middle\| (P_h^{\bar{\mathcal{E}}_t} \otimes R_h^{\bar{\mathcal{E}}_t})(\cdot|s_h, a_h) \right) \!- D_{\text{KL}} \left( (P_h^{\mathcal{E}} \otimes R_h^{\mathcal{E}})(\cdot|s_h, a_h) \middle\| (P_h^{\bar{\mathcal{E}}_t} \otimes R_h^{\bar{\mathcal{E}}_t})(\cdot|s_h, a_h) \right) \bigg|\nonumber\\
        &\leq \bigg|\int_{\mathcal{S}} P_h^{\tilde{\mathcal{E}}_t^{*}}(x|s_{h},a_{h})\log \frac{P_h^{\tilde{\mathcal{E}}_t^{*}}(x|s_{h},a_{h})}{P_h^{\bar{\mathcal{E}}_t} ( x|s_{h},a_{h}) }\mathrm{d}x -  \int_{\mathcal{S}} P_h^{\mathcal{E}}(x|s_{h},a_{h})\log \frac{P_h^{\mathcal{E}}(x|s_{h},a_{h})}{P_h^{\bar{\mathcal{E}}_t} ( x|s_{h},a_{h}) }\mathrm{d}x \bigg| \nonumber\\
&\quad + \bigg|\int_{[0,1]} R_h^{\tilde{\mathcal{E}}_t^{*}}(x|s_{h},a_{h})\log \frac{R_h^{\tilde{\mathcal{E}}_t^{*}}(x|s_{h},a_{h})}{R_h^{\bar{\mathcal{E}}_t} ( x|s_{h},a_{h}) }\mathrm{d}x -  \int_{[0,1]}R_h^{\mathcal{E}}(x|s_{h},a_{h})\log \frac{R_h^{\mathcal{E}}(x|s_{h},a_{h})}{R_h^{\bar{\mathcal{E}}_t} ( x|s_{h},a_{h}) } \mathrm{d}x\bigg|.
\end{align}
Let $o=(s_{h},a_{h})$. For the first term in Eqn.~\eqref{y}, we have the following bound
\begin{align} 
\label{twoterm2}
        &\bigg|\int_{\mathcal{S}} P_h^{\tilde{\mathcal{E}}_t^{*}}(x|o)\log \frac{P_h^{\tilde{\mathcal{E}}_t^{*}}(x|o)}{P_h^{\bar{\mathcal{E}}_t} ( x|o) } -  P_h^{\mathcal{E}}(x|o)\log \frac{P_h^{\mathcal{E}}(x|o)}{P_h^{\bar{\mathcal{E}}_t} ( x|o) }\mathrm{d}x   \bigg| \nonumber\\
&\leq \bigg|\int_{\mathcal{S}} P_h^{\tilde{\mathcal{E}}_t^{*}}(x|o)\log \frac{P_h^{\tilde{\mathcal{E}}_t^{*}}(x|o)}{P_h^{\bar{\mathcal{E}}_t} (x|o) } -  P_h^{\mathcal{E}}(x|o)\log \frac{P_h^{\tilde{\mathcal{E}}_t^{*}}(x|o)}{P_h^{\bar{\mathcal{E}}_t} (x|o) }\mathrm{d}x\bigg|\\
&\quad +\bigg|\int_{\mathcal{S}} P_h^{\mathcal{E}}(x|o)\log \frac{P_h^{\tilde{\mathcal{E}}_t^{*}}(x|o)}{P_h^{\bar{\mathcal{E}}_t} ( x|o) }-  P_h^{\mathcal{E}}(x|o)\log \frac{P_h^{\mathcal{E}}(x|o)}{P_h^{\bar{\mathcal{E}}_t} (x|o) }\mathrm{d}x \bigg|\nonumber\\
&\leq \int_{\mathcal{S}} \left|P_h^{\tilde{\mathcal{E}}_t^{*}}(x|o)- P_h^{\mathcal{E}}(x|o)\right| \cdot \bigg|\log \frac{P_h^{\tilde{\mathcal{E}}_t^{*}}(x|o)}{P_h^{\bar{\mathcal{E}}_t} ( x|o) }  \bigg|\mathrm{d}x + \int_{\mathcal{S}} B\cdot \bigg|\log \frac{P_h^{\tilde{\mathcal{E}}_t^{*}}(x|o)}{P_h^{\mathcal{E}} ( x|o) } \bigg|\mathrm{d}x\nonumber\\
        &\leq B\cdot\left(1+2\log\frac{1}{\beta}\right) \int_{\mathcal{S}} \bigg|\log \frac{P_h^{\tilde{\mathcal{E}}_t^{*}}(x|o)}{P_h^{\mathcal{E}} ( x|o) } \bigg|\mathrm{d}x\nonumber\\
        &\leq \frac{(1-2\log{\beta})\epsilon }{2H^2},
\end{align}
where the first inequality is due to triangle inequality; the second inequality again uses triangle inequality, and the fact that $P_h^{\mathcal{E}}(x|o) \leq 1/\beta$; the third inequality is due to the fact that $|a-b| \leq B\cdot|\log\frac{a}{b}|$ for any $a,b\in(0,B)$ and $\bigg|\log \frac{P_h^{\tilde{\mathcal{E}}_t^{*}}(\cdot|o)}{P_h^{\bar{\mathcal{E}}_t} ( \cdot|o) }  \bigg| \leq 2\log\frac{1}{\beta}$. Note that when $P_h^{\bar{\mathcal{E}}_t}(\cdot|o)$ equals  zero, since $\bar{\mathcal{E}}_t$ is the mean MDP, it turns out that $P_h^{\tilde{\mathcal{E}}_t^{*}}(\cdot|o)$ also equals  zero, yielding $\bigg|\log \frac{P_h^{\tilde{\mathcal{E}}_t^{*}}(\cdot|o)}{P_h^{\bar{\mathcal{E}}_t} ( \cdot|o) }  \bigg|=0.$ Finally, the last inequality is due to Eqn.~\eqref{4_3}.

For the second term in Eqn.~\eqref{y}, adopting a similar approach, we have
\begin{align} 
\label{twoterm3}
        &\bigg|\int_{[0,1]} R_h^{\tilde{\mathcal{E}}_t^{*}}(x|o)\log \frac{R_h^{\tilde{\mathcal{E}}_t^{*}}(x|o)}{R_h^{\bar{\mathcal{E}}_t} ( x|o) } -  R_h^{\mathcal{E}}(x|o)\log \frac{R_h^{\mathcal{E}}(x|o)}{R_h^{\bar{\mathcal{E}}_t} ( x|o) }\mathrm{d}x   \bigg| \nonumber\\
        &\leq \bigg|\int_{[0,1]} R_h^{\tilde{\mathcal{E}}_t^{*}}(x|o)\log \frac{R_h^{\tilde{\mathcal{E}}_t^{*}}(x|o)}{R_h^{\bar{\mathcal{E}}_t} (x|o) } -  R_h^{\mathcal{E}}(x|o)\log \frac{R_h^{\tilde{\mathcal{E}}_t^{*}}(x|o)}{R_h^{\bar{\mathcal{E}}_t} (x|o) }\mathrm{d}x\bigg|\\
        &\quad +\bigg|\int_{[0,1]} R_h^{\mathcal{E}}(x|o)\log \frac{R_h^{\tilde{\mathcal{E}}_t^{*}}(x|o)}{R_h^{\bar{\mathcal{E}}_t} ( x|o) }-  R_h^{\mathcal{E}}(x|o)\log \frac{R_h^{\mathcal{E}}(x|o)}{R_h^{\bar{\mathcal{E}}_t} (x|o) }\mathrm{d}x \bigg|\nonumber\\
        &\leq \int_{[0,1]} \left|R_h^{\tilde{\mathcal{E}}_t^{*}}(x|o)- R_h^{\mathcal{E}}(x|o)\right| \cdot \bigg|\log \frac{R_h^{\tilde{\mathcal{E}}_t^{*}}(x|o)}{R_h^{\bar{\mathcal{E}}_t} ( x|o) }  \bigg|\mathrm{d}x + \int_{[0,1]} B\cdot \bigg|\log \frac{R_h^{\tilde{\mathcal{E}}_t^{*}}(x|o)}{R_h^{\mathcal{E}} ( x|o) } \bigg|\mathrm{d}x\nonumber\\
        &\leq B\cdot\left(1+2\log\frac{1}{\beta}\right) \int_{[0,1]} \bigg|\log \frac{R_h^{\tilde{\mathcal{E}}_t^{*}}(x|o)}{R_h^{\mathcal{E}} ( x|o) } \bigg|\mathrm{d}x\nonumber\\
        &\leq \frac{(1-2\log{\beta})\epsilon }{2 H}.
\end{align}
Hence, adding up Eqn.~\eqref{twoterm2} and Eqn.~\eqref{twoterm3}, we obtain
$$|r'_h(s_h,a_h)-\bar{r}_h(s_h,a_h)| \leq  \frac{\lambda}{2}\cdot\frac{(1-2\log{\beta})\epsilon }{H}. $$ 
Finally, summing over $h\in[H]$, we have 
\begin{equation}
    \bigg| \mathbb{E}_{\pi}^{\bar{\mathcal{E}}_t}\bigg[ \sum_{h=1}^{H}r'_h(s_h,a_h) \bigg]-\mathbb{E}_{\pi}^{\bar{\mathcal{E}}_t}\bigg[ \sum_{h=1}^{H}\bar{r}_h(s_h,a_h) \bigg]\bigg|\leq \frac{\lambda}{2}\cdot \epsilon \cdot (1-2\log\beta)
\end{equation}
The proof is finished.
\end{proof}


\section{Basic Properties of the Measure $\ell_g$}\label{lg}


The following result deals with the problem of convexity.  Let $B(\mathcal{C},\epsilon)$ be an $\epsilon$-ball with its center at $\mathcal{C}$.
Note that  $B(\mathcal{C},\epsilon)$ is not essentially convex under $\ell_g$: for $P,Q\in B(\mathcal{C},\epsilon)$ and $\lambda\in(0,1)$, it does not hold that $\lambda P+(1-\lambda)Q\in B(\mathcal{C},\epsilon)$. However, using Lemma \ref{metric}, we have the following result:
\begin{lemma}\label{center}
    For any $P,Q\in B(\mathcal{C},\epsilon)$ and $\lambda\in[0,1]$, $\lambda P+(1-\lambda)Q\in B(\mathcal{C},\epsilon)$ lies in the $(2\epsilon)$-ball at $\mathcal{C}$, i.e., $$\ell_g(\lambda P+(1-\lambda)Q, \mathcal{C})\leq 2\epsilon.$$
\end{lemma}
\begin{proof}
    By definition of $\ell_g$, we have
    \begin{align*}
        \ell_g(\lambda P +(1-\lambda)Q,\mathcal{O}) &= \sup_{o} \int_x \bigg| \log\frac{  \lambda P(x|o)+(1-\lambda) Q(x|o)}{\mathcal{C}(x|o)}\bigg|\\
        &\leq\sup_{o} \int_x \bigg| \log \frac{P(x|o)}{\mathcal{C}(x|o)} + \log \frac{Q(x|o)}{\mathcal{C}(x|o)}  \bigg|\\
        &\leq 2\epsilon,
    \end{align*}
    where the first inequality uses the fact that $| \log(\lambda a+(1-\lambda)b)| \leq |\log a|+ |\log b|$ for any $a,b > 0$ and $\lambda \in [0,1]$; the second inequality is due to $P,Q\in B(\mathcal{C},\epsilon)$. This finishes the proof of Lemma \ref{center}.
\end{proof}


\section{Technical Lemmas}
\subsection{Value Function and Mutual Information}
\begin{lemma}
    \label{lemma}
    For any two environments $\mathcal{E},\mathcal{E}'$ with potentially different transition and reward functions, and any policy $\pi$, we have
    \begin{equation}
    \begin{aligned}
        V_{1,\pi}^{\mathcal{E}}(s_1)-V_{1,\pi}^{\mathcal{E}'}(s_1) =&\sum_{h=1}^{H}\mathbb{E}_{\pi}^{\mathcal{E}'}\bigg[  \mathbb{E}_{(s',r') \sim (P_h^{\mathcal{E}} \otimes R_h^{\mathcal{E}} )(\cdot|s_h,a_h)}\left[ r'+V_{h+1,\pi}^{\mathcal{E}}(s')\right] \\
        &-\mathbb{E}_{(s',r') \sim (P_h^{\mathcal{E}'}\otimes R_h^{\mathcal{E}'})(\cdot|s_h,a_h)}\left[ r'+V_{h+1,\pi}^{\mathcal{E}}(s') \right] \bigg].
        \end{aligned}
    \end{equation}
\end{lemma}

\begin{lemma}
\label{mutual}
The mutual information $\mathbb{I}_{t}^{\pi_{\text{TS}}^t}\left(\tilde{\mathcal{E}}_t^{*}; (\mathcal{H}_{t},\mathcal{R}_{t,H})\right)$ can be lower bounded as
    \begin{equation}
    \label{tmp3}
    \begin{aligned}
        \mathbb{I}_{t}^{\pi_{\text{TS}}^t}\left(\tilde{\mathcal{E}}_t^{*}; (\mathcal{H}_{t},\mathcal{R}_{t,H})\right) \geq \sum_{h=1}^H \mathbb{E}_t \left[ \mathbb{E}_{\pi_{\text{TS}}^t}^{\bar{\mathcal{E}}_t} \left[ D_{\text{KL}} \left( (P_h^{\tilde{\mathcal{E}}_t^{*}} \otimes R_h^{\tilde{\mathcal{E}}_t^{*}})(\cdot|s_h, a_h) \middle\| (P_h^{\bar{\mathcal{E}}_t} \otimes R_h^{\bar{\mathcal{E}}_t}) (\cdot|s_h, a_h) \right) \right] \right].
        % &=
        % \sum_{h=1}^H \mathbb{I}_{t}^{\pi_{\text{TS}}^t} \left( \tilde{\mathcal{E}}_t^{*}; (s_h^{t,0}, a_h^{t,0},r_h^{t,0}, s_h^{t,1}, a_h^{t,1},r_h^{t,1}) \mid (\mathcal{H}_{t, h-1}, \mathcal{R}_{t,h-1} ) \right) \\
        % &\quad + \mathbb{I}_{t}^{\pi_{\text{TS}}^t} \left( \tilde{\mathcal{E}}_t^{*}; o_{t} \mid (\mathcal{H}_{t,H},\mathcal{R}_{t,H}) \right) \\
        % &= \sum_{h=1}^H \mathbb{E}_t \left[ \mathbb{E}_{\pi_{\text{TS}}^t}^{\bar{\mathcal{E}}_t} \left[ D_{\text{KL}} \left( (P_h^{\tilde{\mathcal{E}}_t^{*}} \otimes r_h^{\tilde{\mathcal{E}}_t^{*}})(\cdot|s_h, a_h) \middle\| (P_h^{\bar{\mathcal{E}}_t} \otimes r_h^{\bar{\mathcal{E}}_t}) (\cdot|s_h, a_h) \right) \right] \right] \\
        % &\quad +\sum_{h=1}^H \mathbb{E}_t \left[ \mathbb{E}_{\pi_0}^{\bar{\mathcal{E}}_t} \left[ D_{\text{KL}} \left( (P_h^{\tilde{\mathcal{E}}_t^{*}} \otimes r_h^{\tilde{\mathcal{E}}_t^{*}})(\cdot|s_h, a_h) \middle\| (P_h^{\bar{\mathcal{E}}_t} \otimes r_h^{\bar{\mathcal{E}}_t}) (\cdot|s_h, a_h) \right) \right] \right] \\
        % &\quad + \mathbb{I}_{t}^{\pi_{\text{TS}}^t}\left(\tilde{\mathcal{E}}_t^{*}; o_{t} \mid (\mathcal{H}_{t,H},\mathcal{R}_{t,H})\right)
    \end{aligned}
\end{equation}
\end{lemma}
\begin{proof}[Proof of Lemma \ref{mutual}]
    Using the chain rule of mutual information,
\begin{equation}
\begin{aligned}
        &\mathbb{I}_{t}^{\pi_{\text{TS}}^t}\left(\tilde{\mathcal{E}}_t^{*}; (\mathcal{H}_{t},\mathcal{R}_{t,H})\right)\\ 
        &=
        \sum_{h=1}^H \mathbb{I}_{t}^{\pi_{\text{TS}}^t} \left( \tilde{\mathcal{E}}_t^{*}; (s_h^{t,1}, a_h^{t,1},r_h^{t,1},s_h^{t,0}, a_h^{t,0},r_h^{t,0}) \mid (\mathcal{H}_{t, h-1}, \mathcal{R}_{t,h-1} ) \right) 
         + \mathbb{I}_{t}^{\pi_{\text{TS}}^t} \left( \tilde{\mathcal{E}}_t^{*}; o_{t} \mid (\mathcal{H}_{t,H},\mathcal{R}_{t,H}) \right) \\
        &= \sum_{h=1}^H \mathbb{I}_{t}^{\pi_{\text{TS}}^t}\big(\tilde{\mathcal{E}}_t^{*}; s_h^{t,1}\mid \mathcal{H}_{t,h-1},\mathcal{R}_{t,h-1}\big) + \sum_{h=1}^H \mathbb{I}_{t}^{\pi_{\text{TS}}^t}\big(\tilde{\mathcal{E}}_t^{*}; a_h^{t,1} \mid  s_h^{t,1}, \mathcal{H}_{t,h-1},\mathcal{R}_{t,h-1}\big) \\
       &\quad  + \sum_{h=1}^H \mathbb{I}_{t}^{\pi_{\text{TS}}^t}\big(\tilde{\mathcal{E}}_t^{*}; r_h^{t,1} \mid  s_h^{t,1},  s_h^{t,1}, \mathcal{H}_{t,h-1},\mathcal{R}_{t,h-1}\big) + \sum_{h=1}^H \mathbb{I}_{t}^{\pi_{\text{TS}}^t}\big(\tilde{\mathcal{E}}_t^{*}; s_h^{t,0} \mid  s_h^{t,1},  a_h^{t,1}, r_h^{t,1},\mathcal{H}_{t,h-1},\mathcal{R}_{t,h-1}\big) \\
       &\quad + \sum_{h=1}^H \mathbb{I}_{t}^{\pi_{\text{TS}}^t}\big(\tilde{\mathcal{E}}_t^{*}; a_h^{t,0} \mid  s_h^{t,1},  a_h^{t,1}, r_h^{t,1},s_h^{t,0},\mathcal{H}_{t,h-1},\mathcal{R}_{t,h-1}\big)  \\
       &\quad +\sum_{h=1}^H\mathbb{I}_{t}^{\pi_{\text{TS}}^t}\big(\tilde{\mathcal{E}}_t^{*}; r_h^{t,0} \mid  s_h^{t,1},  a_h^{t,1}, r_h^{t,1}, s_h^{t,0}, a_h^{t,0},\mathcal{H}_{t,h-1},\mathcal{R}_{t,h-1}\big)
        + \mathbb{I}_{t}^{\pi_{\text{TS}}^t}\left(\tilde{\mathcal{E}}_t^{*}; o_{t} \mid (\mathcal{H}_{t,H},\mathcal{R}_{t,H})\right).
        % &= \sum_{h=1}^H \mathbb{E}_t \left[ \mathbb{E}_{\pi_{\text{TS}}^t}^{\bar{\mathcal{E}}_t} \left[ D_{\text{KL}} \left( (P_h^{\tilde{\mathcal{E}}_t^{*}} \otimes r_h^{\tilde{\mathcal{E}}_t^{*}})(\cdot|s_h, a_h) \middle\| (P_h^{\bar{\mathcal{E}}_t} \otimes r_h^{\bar{\mathcal{E}}_t}) (\cdot|s_h, a_h) \right) \right] \right] \\
        % &\quad +   \\
        % &\quad + \mathbb{I}_{t}^{\pi_{\text{TS}}^t}\left(\tilde{\mathcal{E}}_t^{*}; o_{t} \mid (\mathcal{H}_{t,H},\mathcal{R}_{t,H})\right)
    \end{aligned}
\end{equation}
From ~\citep{moradipari2023improved}, 
the first three terms on the right side of the above equation are equal to 
\begin{equation}
\label{temp1}
     \sum_{h=1}^H \mathbb{E}_t \left[ \mathbb{E}_{\pi_{\text{TS}}^t}^{\bar{\mathcal{E}}_t} \left[ D_{\text{KL}} \left( (P_h^{\tilde{\mathcal{E}}_t^{*}} \otimes r_h^{\tilde{\mathcal{E}}_t^{*}})(\cdot|s_h, a_h) \middle\| (P_h^{\bar{\mathcal{E}}_t} \otimes r_h^{\bar{\mathcal{E}}_t}) (\cdot|s_h, a_h) \right) \right] \right].
\end{equation}
Based on the non-negativity of mutual information, we obtain the conclusion of the lemma.

\end{proof}

\subsection{Posterior Consistency}\label{doob}
\begin{lemma}
\label{strong}
    Assume that there exists a strongly consistent estimator of the true environment given the history. Let $\Pi$ be some measure. For any  $\Pi$-integrable function $f:\Theta \rightarrow \mathbb{R} $ and almost every $\mathcal{D}_{\infty}$ sampled from the true environment $\mathcal{E}_0$, we have
    \[ \lim_{t\to \infty} \mathbb{E}_t\big[ f(\mathcal{E}) \big] = f(\mathcal{E}_0). \]
    And if $f:\Theta \times \Theta \rightarrow \mathbb{R}$ is bounded and $(\Pi \times \Pi)$-integrable, for almost every $\mathcal{D}_{\infty}$ sampled from the true environment $\mathcal{E}_0$, we have 
    \[ \lim_{t\to \infty} \mathbb{E}_t\big[ f(\mathcal{E},\mathcal{E}') \big] = f(\mathcal{E}_0,\mathcal{E}_0), \]
    where the expectation is taken over all $\mathcal{E}$ and $\mathcal{E}'$.
\end{lemma}
We refer the readers to  Theorem 6.9 in~\citep{ghosal2017fundamentals} or Appendix K in~\citep{moradipari2023improved} for the definition of a strongly consistent estimator and for more details of the proof.

 

%












\end{document}

\endinput
%%
%% End of file `elsarticle-template-num.tex'.
