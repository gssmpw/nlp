[
  {
    "index": 0,
    "papers": [
      {
        "key": "achiam2023gpt",
        "author": "Achiam, Josh and Adler, Steven and Agarwal, Sandhini and Ahmad, Lama and Akkaya, Ilge and Aleman, Florencia Leoni and Almeida, Diogo and Altenschmidt, Janko and Altman, Sam and Anadkat, Shyamal and others",
        "title": "Gpt-4 technical report"
      },
      {
        "key": "touvron2023llama",
        "author": "Touvron, Hugo and Martin, Louis and Stone, Kevin and Albert, Peter and Almahairi, Amjad and Babaei, Yasmine and Bashlykov, Nikolay and Batra, Soumya and Bhargava, Prajjwal and Bhosale, Shruti and others",
        "title": "Llama 2: Open foundation and fine-tuned chat models"
      }
    ]
  },
  {
    "index": 1,
    "papers": [
      {
        "key": "ouyang2022training",
        "author": "Ouyang, Long and Wu, Jeffrey and Jiang, Xu and Almeida, Diogo and Wainwright, Carroll and Mishkin, Pamela and Zhang, Chong and Agarwal, Sandhini and Slama, Katarina and Ray, Alex and others",
        "title": "Training language models to follow instructions with human feedback"
      },
      {
        "key": "ziegler2019fine",
        "author": "Ziegler, Daniel M and Stiennon, Nisan and Wu, Jeffrey and Brown, Tom B and Radford, Alec and Amodei, Dario and Christiano, Paul and Irving, Geoffrey",
        "title": "Fine-tuning language models from human preferences"
      }
    ]
  },
  {
    "index": 2,
    "papers": [
      {
        "key": "ouyang2022training",
        "author": "Ouyang, Long and Wu, Jeffrey and Jiang, Xu and Almeida, Diogo and Wainwright, Carroll and Mishkin, Pamela and Zhang, Chong and Agarwal, Sandhini and Slama, Katarina and Ray, Alex and others",
        "title": "Training language models to follow instructions with human feedback"
      },
      {
        "key": "bai2022training",
        "author": "Bai, Yuntao and Jones, Andy and Ndousse, Kamal and Askell, Amanda and Chen, Anna and DasSarma, Nova and Drain, Dawn and Fort, Stanislav and Ganguli, Deep and Henighan, Tom and others",
        "title": "Training a helpful and harmless assistant with reinforcement learning from human feedback"
      },
      {
        "key": "ziegler2019fine",
        "author": "Ziegler, Daniel M and Stiennon, Nisan and Wu, Jeffrey and Brown, Tom B and Radford, Alec and Amodei, Dario and Christiano, Paul and Irving, Geoffrey",
        "title": "Fine-tuning language models from human preferences"
      }
    ]
  },
  {
    "index": 3,
    "papers": [
      {
        "key": "rafailov2024direct",
        "author": "Rafailov, Rafael and Sharma, Archit and Mitchell, Eric and Manning, Christopher D and Ermon, Stefano and Finn, Chelsea",
        "title": "Direct preference optimization: Your language model is secretly a reward model"
      }
    ]
  },
  {
    "index": 4,
    "papers": [
      {
        "key": "furnkranz2012preference",
        "author": "F{\\\"u}rnkranz, Johannes and H{\\\"u}llermeier, Eyke and Cheng, Weiwei and Park, Sang-Hyeun",
        "title": "Preference-based reinforcement learning: a formal framework and a policy iteration algorithm"
      },
      {
        "key": "saha2021optimal",
        "author": "Saha, Aadirupa",
        "title": "Optimal algorithms for stochastic contextual preference bandits"
      },
      {
        "key": "ji2024reinforcement",
        "author": "Ji, Kaixuan and He, Jiafan and Gu, Quanquan",
        "title": "Reinforcement learning from human feedback with active queries"
      },
      {
        "key": "sekhari2024contextual",
        "author": "Sekhari, Ayush and Sridharan, Karthik and Sun, Wen and Wu, Runzhe",
        "title": "Contextual bandits and imitation learning with preference-based active queries"
      },
      {
        "key": "li2024feel",
        "author": "Li, Xuheng and Zhao, Heyang and Gu, Quanquan",
        "title": "Feel-Good Thompson Sampling for Contextual Dueling Bandits"
      },
      {
        "key": "bai2025online",
        "author": "Bai, Chenjia and Zhang, Yang and Qiu, Shuang and Zhang, Qiaosheng and Xu, Kang and Li, Xuelong",
        "title": "Online Preference Alignment for Language Models via Count-based Exploration"
      }
    ]
  },
  {
    "index": 5,
    "papers": [
      {
        "key": "busa2014preference",
        "author": "Busa-Fekete, R{\\'o}bert and Sz{\\\"o}r{\\'e}nyi, Bal{\\'a}zs and Weng, Paul and Cheng, Weiwei and H{\\\"u}llermeier, Eyke",
        "title": "Preference-based reinforcement learning: evolutionary direct policy search using a preference-based racing algorithm"
      },
      {
        "key": "xu2020preference",
        "author": "Xu, Yichong and Wang, Ruosong and Yang, Lin and Singh, Aarti and Dubrawski, Artur",
        "title": "Preference-based reinforcement learning with finite-time guarantees"
      },
      {
        "key": "pacchiano2021dueling",
        "author": "Pacchiano, Aldo and Saha, Aadirupa and Lee, Jonathan",
        "title": "Dueling rl: reinforcement learning with trajectory preferences"
      },
      {
        "key": "chen2022human",
        "author": "Chen, Xiaoyu and Zhong, Han and Yang, Zhuoran and Wang, Zhaoran and Wang, Liwei",
        "title": "Human-in-the-loop: Provably efficient preference-based reinforcement learning with general function approximation"
      },
      {
        "key": "taranovic2022adversarial",
        "author": "Taranovic, Aleksandar and Kupcsik, Andras Gabor and Freymuth, Niklas and Neumann, Gerhard",
        "title": "Adversarial imitation learning with preferences"
      },
      {
        "key": "wu2023making",
        "author": "Wu, Runzhe and Sun, Wen",
        "title": "Making rl with preference-based feedback efficient via randomization"
      }
    ]
  },
  {
    "index": 6,
    "papers": [
      {
        "key": "wu2023making",
        "author": "Wu, Runzhe and Sun, Wen",
        "title": "Making rl with preference-based feedback efficient via randomization"
      },
      {
        "key": "li2024feel",
        "author": "Li, Xuheng and Zhao, Heyang and Gu, Quanquan",
        "title": "Feel-Good Thompson Sampling for Contextual Dueling Bandits"
      }
    ]
  },
  {
    "index": 7,
    "papers": [
      {
        "key": "li2024feel",
        "author": "Li, Xuheng and Zhao, Heyang and Gu, Quanquan",
        "title": "Feel-Good Thompson Sampling for Contextual Dueling Bandits"
      }
    ]
  },
  {
    "index": 8,
    "papers": [
      {
        "key": "wu2023making",
        "author": "Wu, Runzhe and Sun, Wen",
        "title": "Making rl with preference-based feedback efficient via randomization"
      }
    ]
  },
  {
    "index": 9,
    "papers": [
      {
        "key": "li2024feel",
        "author": "Li, Xuheng and Zhao, Heyang and Gu, Quanquan",
        "title": "Feel-Good Thompson Sampling for Contextual Dueling Bandits"
      }
    ]
  },
  {
    "index": 10,
    "papers": [
      {
        "key": "russo2014learning",
        "author": "Russo, Daniel and Van Roy, Benjamin",
        "title": "Learning to optimize via information-directed sampling"
      }
    ]
  },
  {
    "index": 11,
    "papers": [
      {
        "key": "russo2016information",
        "author": "Russo, Daniel and Van Roy, Benjamin",
        "title": "An information-theoretic analysis of thompson sampling"
      },
      {
        "key": "dong2018information",
        "author": "Dong, Shi and Van Roy, Benjamin",
        "title": "An information-theoretic analysis for thompson sampling with many actions"
      },
      {
        "key": "bubeck2020first",
        "author": "Bubeck, S{\\'e}bastien and Sellke, Mark",
        "title": "First-order bayesian regret analysis of thompson sampling"
      },
      {
        "key": "liu2018information",
        "author": "Liu, Fang and Buccapatnam, Swapna and Shroff, Ness",
        "title": "Information directed sampling for stochastic bandits with graph feedback"
      },
      {
        "key": "kirschner2021asymptotically",
        "author": "Kirschner, Johannes and Lattimore, Tor and Vernade, Claire and Szepesv{\\'a}ri, Csaba",
        "title": "Asymptotically optimal information-directed sampling"
      },
      {
        "key": "hao2021information",
        "author": "Hao, Botao and Lattimore, Tor and Deng, Wei",
        "title": "Information directed sampling for sparse linear bandits"
      },
      {
        "key": "hao2022contextual",
        "author": "Hao, Botao and Lattimore, Tor and Qin, Chao",
        "title": "Contextual information-directed sampling"
      }
    ]
  },
  {
    "index": 12,
    "papers": [
      {
        "key": "hao2022regret",
        "author": "Hao, Botao and Lattimore, Tor",
        "title": "Regret bounds for information-directed reinforcement learning"
      },
      {
        "key": "moradipari2023improved",
        "author": "Moradipari, Ahmadreza and Pedramfar, Mohammad and Shokrian Zini, Modjtaba and Aggarwal, Vaneet",
        "title": "Improved Bayesian regret bounds for thompson sampling in reinforcement learning"
      }
    ]
  },
  {
    "index": 13,
    "papers": [
      {
        "key": "moradipari2023improved",
        "author": "Moradipari, Ahmadreza and Pedramfar, Mohammad and Shokrian Zini, Modjtaba and Aggarwal, Vaneet",
        "title": "Improved Bayesian regret bounds for thompson sampling in reinforcement learning"
      }
    ]
  },
  {
    "index": 14,
    "papers": [
      {
        "key": "hao2022regret",
        "author": "Hao, Botao and Lattimore, Tor",
        "title": "Regret bounds for information-directed reinforcement learning"
      }
    ]
  },
  {
    "index": 15,
    "papers": [
      {
        "key": "zhang2024provably",
        "author": "Zhang, Qiaosheng and Bai, Chenjia and Hu, Shuyue and Wang, Zhen and Li, Xuelong",
        "title": "Provably Efficient Information-Directed Sampling Algorithms for Multi-Agent Reinforcement Learning"
      }
    ]
  }
]