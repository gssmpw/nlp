%%%%%%%% ICML 2025 EXAMPLE LATEX SUBMISSION FILE %%%%%%%%%%%%%%%%%

\documentclass{article}

% Recommended, but optional, packages for figures and better typesetting:
\usepackage{microtype}
\usepackage{graphicx}
\usepackage{subfigure}
\usepackage{booktabs} % for professional tables
\usepackage{longtable}
\UseRawInputEncoding

% hyperref makes hyperlinks in the resulting PDF.
% If your build breaks (sometimes temporarily if a hyperlink spans a page)
% please comment out the following usepackage line and replace
% \usepackage{icml2025} with \usepackage[nohyperref]{icml2025} above.
\usepackage{hyperref}
\usepackage{amsmath}

% Attempt to make hyperref and algorithmic work together better:
\newcommand{\theHalgorithm}{\arabic{algorithm}}

% Use the following line for the initial blind version submitted for review:
%\usepackage{icml2025}

% If accepted, instead use the following line for the camera-ready submission:
\usepackage[accepted]{icml2025}

% For theorems and such
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{amsthm}
\usepackage{color}
\usepackage{bm}
\usepackage{enumitem}
\usepackage{float}

% if you use cleveref..
\usepackage[capitalize,noabbrev]{cleveref}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% THEOREMS
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\theoremstyle{plain}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{assumption}[theorem]{Assumption}
\theoremstyle{remark}
\newtheorem{remark}[theorem]{Remark}

% Todonotes is useful during development; simply uncomment the next line
%    and comment out the line below the next line to turn off comments
%\usepackage[disable,textsize=tiny]{todonotes}
\usepackage[textsize=tiny]{todonotes}


% The \icmltitle you define below is probably too long as a header.
% Therefore, a short form for the running title is supplied here:
\icmltitlerunning{Unveiling the Downsides of Model Aggregation in Federated Learning from a Layer-peeled Perspective}

\begin{document}

\twocolumn[
\icmltitle{The Other Side of the Coin: Unveiling the Downsides of Model Aggregation in Federated Learning from a Layer-peeled Perspective}
% Understanding the Dangers of Aggregation in Federated Learning: A Layer-peeled Perspective

% It is OKAY to include author information, even for blind
% submissions: the style file will automatically remove it for you
% unless you've provided the [accepted] option to the icml2025
% package.

% List of affiliations: The first argument should be a (short)
% identifier you will use later to specify author affiliations
% Academic affiliations should list Department, University, City, Region, Country
% Industry affiliations should list Company, City, Region, Country

% You can specify symbols, otherwise they are numbered in order.
% Ideally, you should not use this facility. Affiliations will be numbered
% in order of appearance and this is the preferred way.
%\icmlsetsymbol{equal}{*}

\begin{icmlauthorlist}
\icmlauthor{Guogang Zhu}{buaa}
\icmlauthor{Xuefeng Liu}{buaa}
\icmlauthor{Jianwei Niu}{buaa}
\icmlauthor{Shaojie Tang}{buffalo}
\icmlauthor{Xinghao Wu}{buaa}
%\icmlauthor{Firstname6 Lastname6}{sch,yyy,comp}
%\icmlauthor{Firstname7 Lastname7}{comp}
%\icmlauthor{}{sch}
%\icmlauthor{Firstname8 Lastname8}{sch}
%\icmlauthor{Firstname8 Lastname8}{yyy,comp}
%\icmlauthor{}{sch}
%\icmlauthor{}{sch}
\end{icmlauthorlist}

\icmlaffiliation{buaa}{School of Computer Science and Engineering, Beihang University, Beijing, China}
\icmlaffiliation{buffalo}{Department of Management Science and Systems, School of Management, Center for AI Business Innovation, University at Buffalo, Buffalo, NY, USA}
%\icmlaffiliation{sch}{School of ZZZ, Institute of WWW, Location, Country}

%\icmlcorrespondingauthor{Guogang Zhu}{buaa\_zgg@buaa.edu.cn}
\icmlcorrespondingauthor{Xuefeng Liu}{liu\_xuefeng@buaa.edu.cn}
%\icmlcorrespondingauthor{Jianwei Niu}{niujianwei@buaa.edu.cn}
%\icmlcorrespondingauthor{Shaojie Tang}{shaojiet@buffalo.edu}
%\icmlcorrespondingauthor{Xinghao Wu}{wuxinghao@buaa.edu.cn}

% You may provide any keywords that you
% find helpful for describing your paper; these are used to populate
% the "keywords" metadata in the PDF but will not be shown in the document
\icmlkeywords{Machine Learning, ICML}

\vskip 0.3in
]

% this must go after the closing bracket ] following \twocolumn[ ...

% This command actually creates the footnote in the first column
% listing the affiliations and the copyright notice.
% The command takes one argument, which is text to display at the start of the footnote.
% The \icmlEqualContribution command is standard text for equal contribution.
% Remove it (just {}) if you do not need this facility.

\printAffiliationsAndNotice{}  % leave blank if no need to mention equal contribution
%\printAffiliationsAndNotice{\icmlEqualContribution} % otherwise use the standard text.

\begin{abstract}
In federated learning (FL), model aggregation is a critical step by which multiple clients share their knowledge with one another. However, it is also widely recognized that the aggregated model, when sent back to each client, performs poorly on local data until after several rounds of local training. This temporary performance drop can potentially slow down the convergence of the FL model.
Most research in FL regards this performance drop as an inherent cost of knowledge sharing among clients and does not give it special attention. While some studies directly focus on designing techniques to alleviate the issue, an in-depth investigation of the reasons behind this performance drop has yet to be conducted.
To address this gap, we conduct a layer-peeled analysis of model aggregation across various datasets and model architectures. Our findings reveal that the performance drop can be attributed to two major consequences of the aggregation process: (1) it disrupts feature variability suppression in deep neural networks (DNNs), and (2) it weakens the coupling between features and subsequent parameters.
Based on these findings, we propose several simple yet effective strategies to mitigate the negative impacts of model aggregation while still enjoying the benefit it brings. To the best of our knowledge, our work is the first to conduct a layer-peeled analysis of model aggregation, potentially paving the way for the development of more effective FL algorithms.
\end{abstract}

\section{Introduction}
% The very first letter is a 2 line initial drop letter followed
% by the rest of the first word in caps.
% 
% form to use if the first word consists of a single letter:
% \IEEEPARstart{A}{demo} file is ....
% 
% form to use if you need the single drop letter followed by
% normal text (unknown if ever used by the IEEE):
% \IEEEPARstart{A}{}demo file is ....
% 
% Some journals put the first two words in caps:
% \IEEEPARstart{T}{his demo} file is ....
% 
% Here we have the typical use of a "T" for an initial drop letter
% and "HIS" in caps to complete the first word.
In recent years, federated learning (FL) has gained increasing interests since it can enable multiple clients to collaboratively train models without sharing their private data.  
FL is particularly attractive in fields where data privacy and security are paramount, such as medical diagnosis \cite{guan2024federated}, where patient records must remain confidential, and financial analysis \cite{imteaj2022leveraging}, where transactional data is highly sensitive.

A standard FL process involves iterative cycles in which local models are trained on each client, followed by aggregation of these locally updated models on a central server \cite{FedAvg}, as presented in Figure \ref{Model_Aggregation}. 
During local training, each client performs multiple rounds of model updates using its private data. 
Once local training is complete, the updated model is uploaded to the server. 
The server then aggregates the uploaded models by performing parameter-wise averaging, with each model weighted based on factors such as the number of training samples on each client \cite{FedDyn,FedProx,SCAFFOLD}. 
The aggregated model is then sent back to each client for the next round of local training.
By transmitting model parameters instead of raw data, FL ensures collaborative model training while preserving privacy.

\begin{figure}[ht]
\centering
\includegraphics[width=3.2in]{Model_Aggregation.pdf}
\caption{\textbf{Left:} Local model training and global model aggregation in a typical FL training process. 
\textbf{Right:} Performance comparison when evaluating models on local data, *@Pre refers to the evaluation results of the model before aggregation, while *@Post indicates the results after aggregation.}
\label{Model_Aggregation}
\end{figure}

In the above process, model aggregation is a key step that facilitates knowledge sharing among clients in FL.  
However, it is well known that the model aggregation often leads to a significant performance drop compared to the model before aggregation \cite{pFedSD,FedNTD,pFedAMF}.
This phenomenon is particularly pronounced when data distributions across clients are heterogeneous, a common scenario in practical applications due to factors such as variations in data acquisition conditions across different clients \cite{zhu2021federated,FedBN}.
To further investigate this phenomenon, we conduct preliminary experiments in a typical data-heterogeneous FL setting.
Figure \ref{Model_Aggregation} presents a performance comparison of the model before and after aggregation, evaluated on the local data from each client.
As shown, the performance of the aggregated model significantly deteriorates compared to the model before aggregation. 

Although this temporary performance drop can be mitigated after several rounds of local updates, its impact persists and continues to pose a challenge.
The suboptimal initialization in each local update, caused by model aggregation, undermines the progress made in the previous round, potentially slowing the convergence rate of FL training.
Most FL research treats this performance drop as an inherent cost of knowledge sharing among clients, giving it limited attention \cite{FedAvg, FedCAC, MOON}.
Recently, however, some studies have started to address this issue.
For instance, some research attributes the problem to diverging model parameters caused by heterogeneous data distributions and propose various model regularization techniques to reduce parameter divergence \cite{FedProx, SCAFFOLD}.
Other studies link this issue to `knowledge forgetting', where task-specific patterns are averaged out or diluted during model aggregation, and suggest several knowledge distillation methods \cite{pFedSD, FedNTD}.
However, these studies rely on assumed causes of the performance drop, often without deeply exploring the fundamental reasons behind it. 

To address this gap, we provide a comprehensive analysis of the impact of model aggregation in FL from a layer-peeled feature extraction perspective. Our focus is twofold: first, on \textit{feature structure}, which evaluates the intrinsic quality of the features, and second, on \textit{feature-parameter alignment}, which assesses the degree of coupling between features and the parameters of subsequent layers.

We conduct this layer-peeled feature analysis across multiple datasets and model architectures. Our findings show that the performance degradation in model aggregation can be primarily attributed to two factors from a layer-wise feature extraction perspective: (1) disruption of feature variability suppression in deep neural networks (DNNs), and (2) a weakening of the coupling between features and the parameters of subsequent layers.
Firstly, DNNs are known to progressively compress features layer by layer \cite{wang2023understanding, rangamani2023feature, masarczyk2024tunnel}. 
However, during model aggregation, this feature compression process is compromised, leading to scattered features. 
This effect accumulates across layers as the network deepens, significantly degrading the quality of the penultimate layer features. 
Secondly, the mismatch between the penultimate layer features and the classifier worsens after aggregation, further deteriorating model performance.
Based on these findings, we propose several simple yet effective strategies to directly address these issues, thereby improving the FL model performance.

The contributions of this paper are as follows:

\begin{itemize} 
\item To the best of our knowledge, we are the first to comprehensively understand the impact of model aggregation from a layer-peeled feature extraction perspective. 
\item We conduct an extensive layer-peeled feature evaluation across multiple datasets and model architectures, which uncovers the major causes of performance drop in model aggregation. This analysis examines both feature quality and feature-parameter alignment, while also highlighting the challenges introduced by the stacked architecture of DNNs.
\item We propose strategies to improve the aggregated model, addressing key drawbacks and enabling the development of enhanced FL algorithms based on our insights. \end{itemize}

\section{Preliminaries}
\subsection{Problem Formulation of FL}
In this paper, we consider a standard FL system consisting of a central server and $M$ distributed clients. We assume that each client $m$ contains $N_m$ training samples, which are drawn from the data distribution $\mathcal{D}_m$. 
In practice, the underlying data distribution $\mathcal{D}_m$ for each client is typically different from one another due to the variations in data collection conditions. Formally, the training samples on client $m$ can be represented as $(\bm{x}_m^i, \bm{y}_m^i)_{i = 1}^{N_m}$, where $\bm{x}_m^i \in \mathcal{X}_m \subseteq \mathbb{R}^n$ denotes the raw input data for the DNNs, and $\bm{y}_m^i \in \mathcal{Y}_m \subseteq \{0, 1\}^C$ represents the corresponding ground truth labels used to optimize the DNNs, with $C$ denoting the number of classes.

We denote the DNN for client $m$ as $\bm{\psi}_m(\cdot)$, with parameters represented by $\bm{\Theta}_m$.
The optimization objective for an FL system can then be formulated as:
\begin{equation}
\label{FL}
\text{arg} \underset{\bm{\Theta}_1, \ldots, \bm{\Theta}_M}{\text{min}} \ \mathcal{L}(\bm{\Theta}_1, \ldots, \bm{\Theta}_M) \triangleq \text{arg} \underset{\bm{\Theta}_1, \ldots, \bm{\Theta}_M}{\text{min}} \frac{1}{M} \sum_{m=1}^{M}\mathcal{L}_m(\bm{\Theta}_m),
\end{equation}
where $\mathcal{L}_m(\bm{\Theta}_m)$ represents the empirical risk for client \( m \), which is  computed based on its private data samples, as shown in the following equation:

\begin{equation}
\label{empirical-risk}
\mathcal{L}_m(\bm{\Theta}_m)  := \frac{1}{N_m} \sum_{i=1}^{N_m} \ell (\bm{y}_m^i, \hat{\bm{y}}_m^i),
\end{equation}
where $\hat{\bm{y}}_m^i = \psi_m(\bm{x}_m^i;\bm{\Theta}_m)$ represents the predicted output of $\bm{x}_m^i$ given the model $\psi_m(\cdot)$ for $\bm{x}_m^i$, $\ell:\mathcal{Y} \times \mathcal{Y} \rightarrow \mathbb{R}$ is the loss function used to measure the prediction error. 

To optimize Equation (\ref{FL}) in a privacy-preserving manner, FL is typically carried out in two iterative stages: local model training and global model aggregation.
During the local model training phase, each client optimizes its model for $E$ epochs by minimizing the loss function defined in Equation (\ref{empirical-risk}).
Once local training is complete, each client uploads its updated model to the server.
The server then performs model aggregation to generate the global model. A common aggregation strategy involves applying a weighted average for each parameter within the model based on the number of training samples per client, which is expressed as follows:

\begin{equation} \label{global-aggregation} \tilde{\bm{\Theta}} = \sum_{m=1}^{M} \frac{N_m}{\sum_{m=1}^M N_m}\bm{\Theta}_m \end{equation}
Here, $\tilde{\bm{\Theta}}$ represents the aggregated global model. By repeating the above procedures for several rounds, the model eventually converges, resulting in the final FL model. For simplicity, we will sometimes refer to the locally updated model $\bm{\Theta}_m$ as the \textbf{\textit{pre-aggregated model}}, and $\tilde{\bm{\Theta}}$ as the \textit{\textbf{post-aggregated model}}. Additionally, we may omit the client and sample indices for simplicity.

In this paper, we focus on uncovering how the feature extraction process varies across model depth due to model aggregation. 
We reformulate the parameters of the FL model as $\boldsymbol{\Theta}=\left\{\boldsymbol{W}^{\ell}\right\}_{\ell=1}^L$, where $L$ represents the total number of layers in the model, and the depth increases with $\ell$. 
This stacked DNN progressively transforms the input data into prediction outputs, from the shallow layers to the deeper layers. 
However, there is still a lack of investigation into how model aggregation affects layer-wise feature extraction. In this paper, we aim to explore the variations in intermediate features during layer-wise feature extraction. The features of the $\ell$-th layer for an input sample $\bm{x}$ can be formulated as: 

\begin{equation}
    \boldsymbol{z}^{\ell}=\boldsymbol{W}^{\ell} \ldots \boldsymbol{W}^{1} \boldsymbol{x}=\boldsymbol{W}^{\ell: 1} \bm{x}, \forall \ell=1, \ldots, L-1
\end{equation}
where $\boldsymbol{z}^{\ell}$ denotes the intermidiated features of $\ell$-th layer and we denote $\bm{z}^0=\bm{x}$. 

\subsection{Feature Evaluation Metric}
We apply the following metrics to evaluate the features generated by the models before aggregation and after aggregation, including the \textit{feature variance, alignment between features and parameters, accuracy of linear probing, pairwise distance of features or models, relative change of evaluated metrics}.

\begin{enumerate}[label=\textbullet] 
\item \textbf{Feature Variance.} We calculate the normalized within-class variance, which quantifies feature compression within the same class, and the normalized between-class variance, which measures the discrimination between different classes. Together, these metrics assess the quality of features given the evaluated dataset.

\item \textbf{Alignment between Features and Parameters.} We assess the alignment between features and parameters to evaluate the degree of coupling between them during the feature extraction process.

\item \textbf{Accuracy of Linear Probing.} This metric evaluates the generalization of features across different distributions. We attach a randomly initialized linear layer to features extracted from various layers of DNNs, then train this linear classifier on a evaluated dataset and report the average testing accuracy.

\item \textbf{Pairwise Distance of Features or Models.} This metric evaluates the differences between the parameters or features generated by the models before and after aggregation.

\item \textbf{Relative Change of Evaluated Metrics.} This measures the change in the metrics after aggregation, offering insights into the degree of impact that aggregation introduces to feature extraction. 
\end{enumerate}


\section{Evaluation Setup}
\subsection{Dataset Description and Partition}\label{dataset_description}
We focus on the cross-domain FL setting in our experiments, which is a common scenario for FL with heterogeneous data. This frequently appears in practical applications such as medical image analysis, traffic surveillance, and autonomous driving \cite{FedBN,AlignFed}.

Building upon previous studies \cite{FedBN, AlignFed, FPL}, we use three public datasets to establish the FL system: Digit-Five, PACS \cite{PACS}, and DomainNet \cite{DomainNet}. Each of these datasets consists of data samples from different domains, with each domain containing images in various styles.

For these datasets, the samples (including both training and tests datasets) from each domain are assigned to a client to construct a cross-domain FL system. 
The number of training samples for each client is set to 500 for all datasets. 
For Digit-Five, the testing dataset contains 1,000 samples for each client. 
For DomainNet and PACS, the testing datasets contain 500 samples for each client.

The sample size for Digit-Five is set to $32 \times 32$, while for DomainNet and PACS, it is set to $224 \times 224$. For Digit-Five, each channel is normalized with a mean of 0.5 and a standard deviation of 0.5 for both training and testing datasets. 
For DomainNet and PACS, we apply random flipping and rotational augmentations to the training samples.

\subsection{Implementation Details}
We evaluate feature variations between the pre-aggregated and post-aggregated models across various architectures.
For Digit-Five, we adopt a convolutional network followed by fully connected (FC) layers (ConvNet), ResNet18, and ResNet34.
For DomainNet and PACS, the implemented models including VGG13\_BN \cite{VGG}, three variants of ResNet (ResNet18, ResNet34, and ResNet50) \cite{ResNet}, and ViT\_B-16 \cite{ViT}. Unless otherwise specified, the models are trained from randomly initialized parameters.

We apply the standard FedAvg \cite{FedAvg} algorithm for federated training. Stochastic gradient descent (SGD) is used to optimize the models on local clients, with a learning rate of 0.01 and a momentum of 0.5. The batch size for local training is set to 64.
Local models are trained for 10 epochs within each global round, with a total of 50 global rounds.
On the server, we perform model aggregation by weighted averaging the parameters, using the number of training samples as the weight for each client.
To reduce the impact of randomness, each experiment is repeated three times with different random seeds.

We extract features from various layers, ranging from shallow to deep, for feature evaluation.
Specifically, we evaluate the features passed to each convolutional layer, pooling layer, and FC layer in the convolutional neural network (CNN).
For ViT\_B/16, we evaluate the features passed to the multilayer perceptron (MLP), self-attention layers, layer normalization layers, and the final classifier.

\section{Main Results}
In this section, we present the key findings observed across multiple datasets and model architectures during our layer-peeled analysis of model aggregation in FL.
Due to space limitations, we only present representative results here. 
More detailed results are available in the Appendix.

\subsection{Understanding Model Aggregation in FL}

\subsubsection{Model aggregation leads to performance drop on local data}
We first confirm the claim that aggregating multiple local models trained on each client results in a significant performance drop when the aggregated model is sent back to the local clients for inference.
Figure \ref{FedAvg_Acc_DomainNet} shows the average training and testing accuracy of the models before and after aggregation during the FL training process.
It can be observed that the aggregated model suffers from a substantial performance degradation across various model architectures, both on the training and testing datasets.
This temporary performance drop erases the gains made during the previous local updates on clients, thereby slowing down the convergence rate of FL training.
Therefore, we seek to understand what happens during model aggregation from the perspective of layer-peeled feature extraction.
\begin{figure}[ht]
\centering
\includegraphics[width=3.2in]{FedAvg_Acc_DomainNet.pdf}
\vspace{-0.4cm}
\caption{Averaged training and testing accuracy curves of the model before and after aggregation, evaluated on the local dataset during FL training. The experiments are conducted on DomainNet, using multiple model architectures as the backbone.}
\label{FedAvg_Acc_DomainNet}
\end{figure}

\vspace{-0.3cm}

\subsubsection{Model Aggregation disrupts feature variability suppression}
Previous studies have shown that deep neural networks (DNNs) progressively suppress raw data, generating task-specific features. However, in our experiments, we observe that model aggregation disrupts this training objective of DNNs. Specifically, we make the following observations:

\textbf{(1) Features become increasingly compressed within the same class as training progresses and layer depth increases.} 
As shown in Figures \ref{FedAvg_ResNet50_Layerwise_NC1} (a) and \ref{FedAvg_ResNet50_Epochwise_NC1} (a), during training, the features within the same class become progressively more compressed. 
Moreover, as the layers deepen, the features become also more compressed.
This observation suggests that, from the perspective of feature variation suppression, the overall training objective of FL aligns with what has been observed in centralized learning (CL).

\textbf{(2) Features become increasingly discriminative across different class as training progresses and layer depth increases.} In contrast to the normalized within-class feature variance, the normalized between-class feature variance increases as the training progresses and layer depth increases, as shown in Figure \ref{FedAvg_ResNet50_Layerwise_NC1_between} (a) and \ref{FedAvg_ResNet50_Epochwise_NC1_between} (a). This demonstrates that the features across different classes become increasingly discriminative.

\begin{figure}[ht]
\centering
\includegraphics[width=3.2in]{FedAvg_ResNet50_Layerwise_NC1.pdf}
\vspace{-0.4cm}
\caption{Normalized within-class variance of features at different layers during FL training. The model is trained on DomainNet using ResNet50: (a) normalized within-class variance, (b) relative change of the variance before and after model aggregation.}
\label{FedAvg_ResNet50_Layerwise_NC1}
\end{figure}

\begin{figure}[ht]
\centering
\includegraphics[width=3.2in]{FedAvg_ResNet50_Layerwise_NC1_between.pdf}
\vspace{-0.4cm}
\caption{Normalized between-class variance of features at different layers during FL training. The model is trained on DomainNet using ResNet50: (a) normalized between-class variance, (b) relative change of the variance before and after model aggregation.}
\label{FedAvg_ResNet50_Layerwise_NC1_between}
\end{figure}

\textbf{(3) Model aggregation disrupts feature variation suppression during FL training.} As shown in Figures \ref{FedAvg_ResNet50_Layerwise_NC1} (a) and \ref{FedAvg_ResNet50_Epochwise_NC1} (a), the normalized within-class variance increases, while the normalized between-class variance decreases after model aggregation. This is in contrast to the training objective of DNNs. Additionally, the feature visualization in Figure \ref{TSNE_DomainNet_quickdraw_ResNet50} clearly shows the features generated by different layers of the models before and after aggregation, providing further evidence of this disruption.

\begin{figure}[ht]
\centering
\includegraphics[width=3.0in]{FedAvg_ResNet50_Epochwise_NC1.pdf}
\vspace{-0.4cm}
\caption{Changes in normalized within-class variance across layers for specific global rounds, with larger X-axis values indicating deeper layers. The model is trained on DomainNet using ResNet50: (a) normalized within-class variance, (b) relative change of the variance before and after model aggregation.}
\label{FedAvg_ResNet50_Epochwise_NC1}
\end{figure}

\begin{figure}[ht]
\centering
\includegraphics[width=3.0in]{FedAvg_ResNet50_Epochwise_NC1_between.pdf}
\vspace{-0.4cm}
\caption{Changes in normalized between-class variance across layers for specific global rounds, with larger X-axis values indicating deeper layers. The model is trained on DomainNet using ResNet50: (a) normalized between-class variance, (b) relative change of the variance before and after model aggregation.}
\label{FedAvg_ResNet50_Epochwise_NC1_between}
\end{figure}

\textbf{(4) Deeper features begin to compress only after model aggregation.} As shown in Figures \ref{FedAvg_ResNet50_Layerwise_NC1} -  \ref{FedAvg_ResNet50_Epochwise_NC1_between}, during the training of the FL model, the shadow features initially converge to a certain level of feature variation and then the deeper layer can start to converge. If this condition is not met, model aggregation may cause the features to scatter from the shadow and deeper layers. This characteristic makes the convergence of the model aggregation more challenging.

\begin{figure}[ht]
\centering
\includegraphics[width=3.2in]{TSNE_DomainNet_quickdraw_ResNet50.pdf}
\vspace{-0.4cm}
\caption{T-SNE visualization of features at different layers on the `Quickdraw' domain of DomainNet before and after aggregation. The features are extracted from ResNet50 in the final global round.}
\label{TSNE_DomainNet_quickdraw_ResNet50}
\end{figure}

\subsubsection{Feature Variation Disruption
Accumulate as Model Depth Increases}
In experiments, we find that the stacked architecture of DNNs has specific impacts on model aggregation. Specifically, we observe that the disruption of feature variation suppression progressively accumulates as the depth of layer increases.
As shown in Figures \ref{FedAvg_ResNet50_Layerwise_NC1} (b) and \ref{FedAvg_ResNet50_Epochwise_NC1} (b), we compute the relative changes in normalized within-class variance.
It can be observed that the relative changes in normalized within-class variance progressively increase as we move deeper into the DNNs.
This issue suggests that DNNs may not be ideal for model aggregation in FL training.

Additionally, we examined the layer-wise parameter and feature distances between the model before and after aggregation. The results are shown in Figure \ref{FedAvg_Distance_DomainNet_ResNet50_norm_abs_dis_Epochwise}. The magnitudes of the parameter distances are much smaller compared to the feature distances. The parameter distance shows a decreasing trend as the model deepens, except for the final classifier, while the feature distance shows an increasing trend with deeper layers.
This phenomenon suggests that the performance drop may not only be due to parameter divergence but could be more closely related to the accumulation of feature disruption. This issue causes the features in the penultimate layer to degrade more significantly, which can substantially affect the model's performance.

\begin{figure}[ht]
\centering
\includegraphics[width=3.2in]{FedAvg_Distance_DomainNet_ResNet50_norm_abs_dis_Epochwise.pdf}
\vspace{-0.6cm}
\caption{Changes in normalized $L_1$ distance of the features and parameters obtained from models before and after aggregation across model layers for specific global rounds, with larger X-axis values indicating deeper layers. The model is trained on DomainNet using ResNet50: (a) distance of model parameters (b) distance of features.
}
\label{FedAvg_Distance_DomainNet_ResNet50_norm_abs_dis_Epochwise}
\end{figure}

\subsubsection{Feature Mismatch with Parameters in Final Decision Stage}
In the previous sections, we observed that the features from the model experience significant quality degradation due to the accumulation of feature disruption across the depth of DNNs.
However, the final decision of DNNs is not only influenced by the feature quality, but also by the model parameters that progressively process the generated features.
Figures \ref{FedAvg_ResNet50_Layerwise_NC3} and \ref{FedAvg_ResNet50_DomainNet_Epochwise_NC3_NC3} show the alignment between features and the subsequent parameters.
It is evident that there is a disruption in this alignment after model aggregation. This disruption exhibits an accumulation phenomenon similar to feature variations, which causes the penultimate layer's features to significantly mismatch with the subsequent classifier. This mismatch further degrade the performance.

\begin{figure}[ht]
\centering
\includegraphics[width=3.0in]{FedAvg_ResNet50_Layerwise_NC3.pdf}
\vspace{-0.4cm}
\caption{Alignment between features and parameters at different layers during FL training. The model is trained on DomainNet using ResNet50: (a) original alignment value between features and parameters, (b) relative change of the alignment values before and after model aggregation.}
\label{FedAvg_ResNet50_Layerwise_NC3}
\end{figure}

\begin{figure}[ht]
\centering
\includegraphics[width=3.0in]{FedAvg_ResNet50_DomainNet_Epochwise_NC3_NC3.pdf}
\vspace{-0.4cm}
\caption{Changes in alignment of features and parameters across model layers for specific global rounds, with larger X-axis values indicating deeper layers. The model is trained on DomainNet using ResNet50: (a) original alignment value between features and parameters, (b) relative change of the alignment values before and after model aggregation.}
\label{FedAvg_ResNet50_DomainNet_Epochwise_NC3_NC3}
\end{figure}

\subsubsection{Model Aggregation Improves Model Generalization}
In this section, we demonstrate that model aggregation improves the model's generalization across various datasets. We perform linear probing on both the pre-aggregated and post-aggregated models using different datasets from local clients.
As shown in Figure \ref{Linear_Probing_DomainNet_E50_ResNet34_Epochwise}, after local training on clients, the linear probing accuracy on its local data generally improves, indicating that the model gradually adapts. However, when tested on datasets from other clients, the model performs worse.
This demonstrates that the model before aggregation struggles to extract universal features applicable across different clients. In contrast, the post-aggregated model exhibits  much better performance, suggesting that it generalizes more effectively, as the knowledge learned from different clients is fused through the aggregation process.

\begin{figure}[ht]
\centering
\includegraphics[width=3.2in]{Linear_Probing_DomainNet_E50_ResNet34_Epochwise.pdf}
\vspace{-0.4cm}
\caption{Accuracy of linear probing at different layers on `Clipart' and `Infograph' domains in DomainNet. The experiments are performed using the ResNet34 model in global round 50. In the figures, E50@Post represents the results using the post-aggregated model, while E50\_*@Pre represents the results using the model pre-aggregated model for domain *.}
\label{Linear_Probing_DomainNet_E50_ResNet34_Epochwise}
\end{figure}

\subsection{More Factors Related to Feature Extraction}
\subsubsection{Parameter Personalization}
Parameter personalization is a key method in personalized federated learning (PFL).
In this section, we conduct experiments to investigate how parameter personalization impacts the feature extraction process. We first examine two typical PFL methods: FedPer \cite{FedPer}, which personalizes the classifier, and FedBN \cite{FedBN}, which personalizes the batch normalization (BN) layers. Additionally, inspired by \cite{PartialFed}, we explore a successive personalization approach that targets multiple layers, starting from the first layer.
The experimental results are presented in Figure \ref{PFL_ResNet34_DomainNet_Epochwise_NC3_NC3_R29}.
As shown, personalizing more parameters within the feature extractor generally results in more compressed within-class features and less relative change after model aggregation.
This effect can be attributed to the fact that personalizing shallow layers helps mitigate the accumulation of feature degradation, where the feature extraction capacity learned from local data in these personalized layers remain unaffected by model aggregation.
\vspace{-0.4cm}
\begin{figure}[ht]
\centering
\includegraphics[width=3.2in]{PFL_ResNet34_DomainNet_Epochwise_NC3_NC3_R29.pdf}
\vspace{-0.4cm}
\caption{Changes in the normalized within-class feature variance and the alignment between features and parameters when personalizing different parts of the model. The results are obtained by the model in global round $30$, using ResNet34 as the backbone and is trained on DomainNet. PFL\_* denotes the PFL methods that personalize different parts, with C1 referring to the first convolutional layer and S* referring to the stage block.}
\label{PFL_ResNet34_DomainNet_Epochwise_NC3_NC3_R29}
\end{figure}
 
\subsubsection{Local Updating Epochs}
In this section, we conduct experiments to investigate its impact on model aggregation. The experimental results are presented in Figure \ref{FedAvg_LocalEp_DomainNet_ResNet34_Epochwise_NC1_NC1_NC3_NC3}. To ensure a fair comparison, we maintain a consistent total number of updating epochs across comparisons. The results show that increasing the number of local epochs will compress the within-class features more effectively.
However, when the models are aggregated, the relative change for larger local update epochs is noticeably greater than for smaller ones. This highlights the sensitivity of the model aggregation process to the number of local updating epochs.

\begin{figure}[ht]
\centering
\includegraphics[width=3.2in]{FedAvg_LocalEp_DomainNet_ResNet34_Epochwise_NC1_NC1_NC3_NC3.pdf}
\vspace{-0.4cm}
\caption{Changes in the normalized within-class feature variance and the alignment between features and parameters when training the FL model with different local update epochs. The model is trained on DomainNet using ResNet34 as backbone. We present two groups of experiments, keeping the total number of updating epochs ($E \times R$) fixed at $100$ and $500$.}
\label{FedAvg_LocalEp_DomainNet_ResNet34_Epochwise_NC1_NC1_NC3_NC3}
\end{figure}


\subsubsection{Residual Connection}
Since residual connections have become a standard component in modern DNNs, we explore their impact on model aggregation. Figure \ref{FedAvg_AllModel_DomainNet_ResNet34_Plain_ResNet34_Epochwise_NC3_NC3_R9_Residual_Connection} compares the results with and without residual connections. The experiments are conducted using ResNet34 on the DomainNet dataset. We removed the residual connections from ResNet34 while keeping the rest of the architecture unchanged, resulting in the Plain\_ResNet34 model.
As shown in Figure \ref{FedAvg_AllModel_DomainNet_ResNet34_Plain_ResNet34_Epochwise_NC3_NC3_R9_Residual_Connection}, the relative change in the normalized within-class feature variance, as well as the alignment between features and parameters, is greater in the Plain\_ResNet34 compared to the original ResNet34. This may because that residual connections help reduce feature degradation by allowing less disrupted features from earlier layers to be passed to deeper layers.
This suggests that residual connections potentially help mitigate the accumulation of feature degradation during model aggregation.

\begin{figure}[ht]
\centering
\includegraphics[width=3.2in]{FedAvg_AllModel_DomainNet_ResNet34_Plain_ResNet34_Epochwise_NC3_NC3_R9_Residual_Connection.pdf}
\vspace{-0.4cm}
\caption{Comparison of feature metrics across layers with and without the use of residual connections (Plain\_ResNet34) in ResNet34. This evaluation is performed on DomainNet dataset using the model at global round $10$.}
\label{FedAvg_AllModel_DomainNet_ResNet34_Plain_ResNet34_Epochwise_NC3_NC3_R9_Residual_Connection}
\end{figure}

\subsection{Mitigating Negative Impacts of Model Aggregation}
In this section, we provide several simple yet effective strategies that can help us to mitigate the downsides introduced by model aggregation, including initialization with pre-trained parameters, and fine-tuning classifiers.

\subsubsection{Initialization with Pre-trained Parameters on Large-scaled Dataset}
As shown in Figures \ref{FedAvg_ResNet50_Pretrained_DomainNet_Epochwise_NC1_NC1} and \ref{FedAvg_ResNet50_Pretrained_DomainNet_Epochwise_NC3_NC3}, both the normalized within-class variance and the alignment between features and parameters exhibit less changes compared with those trained from randomly initialized parameters.
This is likely because, after pre-training, the shallow layers are capable of extracting meaningful features, allowing the model to focus on training the deeper layers.
This helps alleviate the accumulation of feature degradation during model training, making the pre-trained model more suitable for FL.

\begin{figure}[ht]
\centering
\includegraphics[width=3.0in]{FedAvg_ResNet50_Pretrained_DomainNet_Epochwise_NC1_NC1.pdf}
\vspace{-0.4cm}
\caption{Changes in normalized within-class variance of features across model layers for specific global rounds, with larger X-axis values indicating deeper layers. The model is trained on DomainNet using ResNet50 pre-trained on ImageNet: (a) normalized within-class variance, (b) relative change of the variance before and after model aggregation.}
\label{FedAvg_ResNet50_Pretrained_DomainNet_Epochwise_NC1_NC1}
\end{figure}

\begin{figure}[ht]
\centering
\includegraphics[width=3.0in]{FedAvg_ResNet50_Pretrained_DomainNet_Epochwise_NC3_NC3.pdf}
\vspace{-0.4cm}
\caption{Changes in alignment of features and parameters across model layers for specific global rounds, with larger X-axis values indicating deeper layers. The model is trained on DomainNet using ResNet50 pretrained on ImageNet: (a) original alignment value between features and parameters, (b) relative change of the alignment values before and after model aggregation.}
\label{FedAvg_ResNet50_Pretrained_DomainNet_Epochwise_NC3_NC3}
\end{figure}

\subsubsection{Fine-tuning Classifier when Reloading Global Feature Extractor}
As mentioned earlier, when the global model is sent back to local clients, the features from the penultimate layer exhibit an increased mismatch with the subsequent classifier.
In this subsection, we show that simply fine-tuning the classifier can significantly enhance the model’s performance.
Figure \ref{FedAvg_ResNet18_DomainNet_Finetune_Classifier} illustrates the accuracy and feature-parameter alignment when fine-tuning the classifier at various global rounds during training.
It can be observed that after fine-tuning the classifier, the alignment between the features and the classifier consistently improves as the FL training progresses.
As a result, the testing accuracy also improves.

\begin{figure}[ht]
\centering
\includegraphics[width=3.0in]{FedAvg_ResNet18_DomainNet_Finetune_Classifier.pdf}
\vspace{-0.4cm}
\caption{Accuracy and alignment between features and parameters during classifier fine-tuning across global rounds.}
\label{FedAvg_ResNet18_DomainNet_Finetune_Classifier}
\end{figure}

\section{Conclusion}

To the best of our knowledge, this paper presents the first attempt to explain model aggregation in FL from the perspective of layer-peeled feature analysis.
We focus on the performance drop in model aggregation.
Through our layer-peeled feature analysis, we identify several factors that contribute to this performance drop.
In particular, from the perspective of feature variations suppression, we observe that the features after aggregation tend to be less compressed than before.
More importantly, due to the stacked architecture of modern DNNs, this phenomenon accumulates throughout the feature extraction process, significantly degrading the quality of the final features used for decision-making.
As a result, training DNNs using FL becomes increasingly challenging.
Additionally, we find that the penultimate layer features exhibit significant mismatches with the classifier after aggregation. This further degrade the performance of the aggregated model.
Building on these insights, we explore techniques to mitigate these issues and, in turn, enhance the model performance.
This work deepens the understanding of model aggregation in FL and provides a new perspective for developing more effective FL algorithms.

% Acknowledgements should only appear in the accepted version.
% \section*{Acknowledgements}

% \textbf{Do not} include acknowledgements in the initial version of
% the paper submitted for blind review.

% If a paper is accepted, the final camera-ready version can (and
% usually should) include acknowledgements.  Such acknowledgements
% should be placed at the end of the section, in an unnumbered section
% that does not count towards the paper page limit. Typically, this will 
% include thanks to reviewers who gave useful comments, to colleagues 
% who contributed to the ideas, and to funding agencies and corporate 
% sponsors that provided financial support.

\section*{Impact Statement}

This paper presents work whose goal is to advance the field of 
Machine Learning. There are many potential societal consequences 
of our work, none which we feel must be specifically highlighted here.


% In the unusual situation where you want a paper to appear in the
% references without citing it in the main text, use \nocite
\nocite{langley00}

\bibliography{reference}
\bibliographystyle{icml2025}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% APPENDIX
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage
\appendix
\onecolumn

\section{Related Work}
\textbf{Federated Learning.} 
FedAvg \cite{FedAvg} is widely recognized as the pioneering work in FL. It facilitates collaborative model training by iteratively performing independent training on each client and aggregating the models on the server. While FedAvg demonstrates satisfactory performance in IID settings \cite{stich2018local, woodworth2020local}, a performance drop is commonly observed when data distributions are non-IID. This phenomenon has led to multiple explanations for the degradation, such as `client drift' \cite{zhao2018federated,SCAFFOLD} and `knowledge forgetting' \cite{pFedSD}.
To address the challenges posed by non-IID data, various solutions have been proposed, including local model regularization \cite{FedProx}, correction techniques \cite{SCAFFOLD,pFedGF}, knowledge distillation \cite{pFedSD,FedNTD}, and partial parameter personalization \cite{FedBN,FedRep,FedPer,LG-FedAvg,PartialFed,ChannelFed,FedCAC,FedDecomp}. However, these methods generally adhere to the standard framework of local training and global aggregation introduced by FedAvg. There remains a lack of in-depth exploration into how model aggregation intuitively impacts model training from a feature extraction perspective.

\textbf{Federated Learning within Feature Space.} 
Numerous studies have recently observed that heterogeneous data can lead to suboptimal feature extraction in FL models and are working on improving FL models by directly calibrating the resulting suboptimal feature spaces. 
A common line of research attributes performance degradation to inconsistent feature spaces across clients. 
To tackle this problem, CCVR \cite{CCVR} introduces a post-calibration strategy that fine-tunes the classifier after FL training using virtual features generated by an approximate Gaussian Mixture Model (GMM).
Several methods \cite{FedProto, AlignFed, AlignFed1, FedFA, FPL} propose using prototypes to align feature distributions across different clients, ensuring a consistent feature space. Studies such as FedBABU \cite{FedBABU}, SphereFed \cite{SphereFed}, and FedETF \cite{FedETF} utilize various fixed classifiers (e.g., random or orthogonal initialization) as targets to align features across clients.
In addition to inconsistent feature spaces, FedDecorr \cite{FedDecorr_ICLR,FedDecorr_TPAMI} observed that heterogeneous data leads to severe dimensional collapse in the global model. 
To combat this, it applies a regularization term based on the Frobenius norm of the correlation matrix during local training, encouraging the different dimensions of features to remain uncorrelated. 
FedPLVM \cite{FedPLVM} discovers differences in domain-specific feature variance in cross-domain FL. 
Consequently, they propose dual-level prototype clustering that adeptly captures variance information and addresses the aforementioned problem.
However, these studies primarily focus on calibrating features in the penultimate layer while overlooking intermediate features during feature extraction.
In this paper, we present a layer-peeled analysis of how model aggregation affects the feature extraction process and identify several issues related to the hierarchical topology of DNNs.

\textbf{Feature Learning in DNNs.} Advanced DNNs are typically structured with hierarchical layers, enabling them to efficiently and automatically extract informative features from raw data \cite{krizhevsky2012imagenet, allen2023backward, wang2023understanding}. Numerous efforts have been made to understand how DNNs transform raw data from shallow to deep layers. A commonly accepted view is that DNNs initially extract transferable universal features and then progressively filter out irrelevant information to form task-specific features \cite{yosinski2014transferable, zeiler2014visualizing, evci2022head2toe, kumar2022finetuning}. Another line of studies demonstrates that DNNs progressively learn features that are compressed within classes and discriminative between classes. Specifically, \cite{alain2017understanding} observes that the linear separability of features increases as the layers become deeper, using a linear probe. \cite{masarczyk2024tunnel} proposes the tunnel effect hypothesis, which states that the initial layers create linearly separable features, while the subsequent layers (referred to as the tunnel) compress these features. Meanwhile, certain research has extended the feature analysis of neural collapse (NC)—originally observed in the penultimate layers \cite{NC}—to intermediate layers \cite{ansuini2019intrinsic, rangamani2023feature, li2024understanding}, showing that the features of each layer exhibit gradual collapse as layer depth increases. \cite{wang2023understanding} provides a theoretical analysis of feature evolution across depth based on deep linear networks (DLNs). Although these studies focus on centralized training where there is no model aggregation during training, They provide solid support for us to effectively analyze feature evolution in FL, especially the influence of model aggregation on feature extraction.

\section{Dataset Description and Partition}\label{dataset_description}
In the experiments, we primarily focus on the data-heterogeneous FL setting, where the distribution of raw input data differs across clients. This data heterogeneity, often referred to as cross-domain FL, is commonly observed in practical FL applications due to variations in data collection conditions across clients.
Building on previous studies \cite{FedBN, AlignFed, AlignFed1, FedPLVM}, we use three widely used public cross-domain datasets: Digit-Five, PACS \cite{PACS}, and DomainNet \cite{DomainNet}. These datasets contain multiple domains, with each domain consisting of images with different backgrounds and styles, which effectively simulate the data heterogeneity caused by variations in raw input.

The Digit-Five dataset includes images across 10 classes and 5 domains, namely: MNIST-M \cite{SynthDigits}, MNIST \cite{MNIST}, USPS \cite{USPS}, SynthDigits \cite{SynthDigits}, and SVHN \cite{SVHN}. The PACS dataset includes 4 distinct domains with a total of 7 classes: Photo (P), Art (A), Cartoon (C), and Sketch (S). The DomainNet dataset comprises six domains: Clipart (C), Infograph (I), Painting (P), Quickdraw (Q), Real (R), and Sketch (S). Initially, the DomainNet dataset includes 345 classes per domain. Based on prior research \cite{FedBN, AlignFed}, we reduce the number of classes to 10 commonly used ones for our layer-peeled feature analysis. Figure \ref{dataset_visualization} shows some example images from these three datasets. The representative images demonstrate significant variations across different domains, as observed in Figure \ref{dataset_visualization}.

To create the data-heterogeneous setting across different clients, we assign the images from a single domain to each client. As a result, there are 5 clients for the Digit-Five dataset, 4 clients for PACS, and 6 clients for DomainNet in our analysis.
For the Digit-Five dataset, the number of training and testing samples are set to 500 and 1000, respectively. For both the PACS and DomainNet datasets, the number of training and testing samples is set to 500. The images in the Digit-Five dataset are scaled to $32 \times 32$ for both the training and testing datasets. For PACS and DomainNet, the images are scaled to $224 \times 224$ and we apply data augmentations such as random flipping and rotation for the training samples. No data augmentations are applied to the Digit-Five or test datasets across all experiments.

\begin{figure*}[ht]
    \centering
\includegraphics[width=6.5in]{Dataset_Visualization.pdf}
    \caption{Visualization of example samples within the datasets used for layer-wise feature evaluation: (a) Digit-Five, (b) PACS, (c) DomainNet. For each domain within these adopted datasets, we show the representative samples from 5 classes.}   \label{dataset_visualization}
\end{figure*}

\section{Model Architectures}
We utilize multiple models for the selected datasets to perform layer-wise feature extraction analysis, including both Convolutional Neural Networks (CNN) and Vision Transformers (ViT). Specifically, the CNN models we employ are ConvNet, VGG13\_BN \cite{VGG}, and three variants of ResNet \cite{ResNet} (ResNet18, ResNet34, and ResNet50). For the ViT architecture, we apply ViT\_B/16 \cite{ViT}.

The ConvNet model consists of several convolutional layers followed by fully connected (FC) layers. The detailed architecture of ConvNet is presented in Table \ref{Arch_ConvNet}, with the output size calculated using the input scale of the Digit-Five dataset as an example. For the other models, we modify only the classifiers by adjusting the number of output classes to match the requirements of our dataset, while leaving the backbone architecture unchanged.

For the Digit-Five dataset, we use ConvNet, ResNet18, and ResNet34 for FL training. For both PACS and DomainNet datasets, we utilize ConvNet, VGG1\_BN, ResNet18, ResNet34, ResNet50, and ViT\_B/16 to accomplish the FL training.

\begin{longtable}{|c|c|c|} 
\caption{Detailed Architecture of ConvNet} \label{Arch_ConvNet}\\
\hline
\textbf{Layer} & \textbf{Output Size} & \textbf{Description} \\
\hline
\endfirsthead
\hline
\textbf{Layer} & \textbf{Output Size} & \textbf{Description} \\
\hline
\endhead
\hline
\endfoot
\hline
\endlastfoot

Input & 32x32x3 & Input image \\
\hline
Conv1\_1 & 32x32x64 & 5x5 Convolution, 64 filters, stride 1, padding 2 \\
\hline
BN1\_1 & 32x32x64 & Batch Normalization \\
\hline
ReLU1\_1 & 32x32x64 & ReLU activation \\
\hline
MaxPool1\_1 & 16x16x64 & 2x2 Max Pooling, stride 2 \\
\hline
Conv1\_2 & 16x16x64 & 5x5 Convolution, 64 filters, stride 1, padding 2 \\
\hline
BN1\_2 & 16x16x64 & Batch Normalization \\
\hline
ReLU1\_2 & 16x16x64 & ReLU activation \\
\hline
MaxPool1\_2 & 8x8x64 & 2x2 Max Pooling, stride 2 \\
\hline
Conv1\_3 & 8x8x128 & 5x5 Convolution, 128 filters, stride 1, padding 2 \\
\hline
BN1\_3 & 8x8x128 & Batch Normalization \\
\hline
ReLU1\_3 & 8x8x128 & ReLU activation \\
\hline
Flatten & 8192 & Flatten layer for fully connected input \\
\hline
Linear2\_1 & 2048 & Fully connected layer, 2048 units \\
\hline
BN2\_1 & 2048 & Batch Normalization \\
\hline
ReLU2\_1 & 2048 & ReLU activation \\
\hline
Linear2\_2 & 512 & Fully connected layer, 512 units \\
\hline
BN2\_2 & 512 & Batch Normalization \\
\hline
ReLU2\_2 & 512 & ReLU activation \\
\hline
Output (C) & $C$ & Output layer (number of classes) \\
\hline
\end{longtable}

\section{Implementation Details of Experiments}
In our experiments, we train the model using the standard FL training process introduced by FedAvg \cite{FedAvg}. This process involves iterative local model updating and global model aggregation.
During the local updating phase, the model is optimized using each client’s private data for $E$ epochs, after which the local models are uploaded to the server for aggregation.
In the model aggregation stage, we apply parameter-wise averaging, using the number of samples as weights for each client's local model.
The above procedures are repeated for $R$ global rounds until the global model converges.
 
For local model updating, we use stochastic gradient descent (SGD) with momentum for model optimization, where the learning rate is set to 0.01 and the momentum is set to 0.5. The batch size for local updates is set to 64.
Unless otherwise specified, the number of local update epochs $E$ is set to 10, and the number of global rounds $R$ is set to 50.

All of our experiments are conducted using the PyTorch framework \cite{Pytorch} and implemented on a four-card Nvidia V100 cluster. During the layer-peeled feature analysis, we use the \textit{forward hook} in PyTorch to extract input features from each evaluated layer as we move from shallow to deeper layers.

For ConvNet, the features preceding each convolutional and linear layer are used for evaluation.
For ResNet, the features before each convolutional layer (excluding the first convolutional layer), the global average pooling layer, and the classifier are used for evaluation.
For VGG13\_BN, the features before each convolutional layer, the linear layers, and the classifier are used for evaluation. For ViT\_B/16, we evaluate the features passed to the multilayer perceptron (MLP), self-attention layers, layer normalization layers, and the final classifier.

To reduce computational cost during feature evaluation, we apply average pooling to the intermediate features, thereby lowering their dimensionality. For CNNs, let the intermediate features at the $\ell$-th layer be represented as $B \times H^{\ell} \times W^{\ell} \times C^{\ell}$, where $B$ denotes the batch size, and $H^{\ell}$, $W^{\ell}$, and $C^{\ell}$ are the height, width, and channel dimensions, respectively. Following previous studies \cite{sarfi2023simulated, harun2024what}, we apply $2 \times 2$ \textit{adaptive average pooling} to the height and width dimensions ($H^{\ell} \times W^{\ell}$). After this adaptive average pooling operation, the intermediate features within the convolutional layers are reduced to $B \times 2 \times 2 \times C^{\ell}$. We then flatten this tensor into a one-dimensional vector and compute the feature metrics, where the features of one sample within the batch has a dimension of $4C^{\ell}$, and the total dimension of the samples within the batch is $B \times 4C^{\ell}$. For ViTs, following previous studies \citet{raghu2021vision,harun2024what}, we apply \textit{global average pooling} to aggregate the image tokens, excluding the class token. For features input to the linear layer with a two-dimensional shape of $B \times D^{\ell}$, we directly use these features to perform evaluation.

During feature evaluation, we process the features in a batch-wise manner, concatenating them across all batches in the evaluated dataset. The corresponding feature metrics are then computed based on these concatenated features.
For all experiments, we evaluate the features every 20 local updating epochs, which corresponds to 2 global rounds when the local update epoch is set to 10.
To minimize the impact of randomness, each experiment is repeated three times with different random seeds.

\section{Feature Evaluation Metrics}
We apply the following metrics to evaluate the features generated by the pre-aggregated model and post-aggregated model, including the \textit{feature variance, alignment between features and parameters, accuracy of linear probing, pairwise distance of features and models, relative change of evaluated metrics}. For simplicity, we omit the client and sample indices and focus solely on the computation process of these metrics. As previously stated, the features in our experiments are first stacked into a two-dimensional tensor, and then used to compute the feature metrics.
We assume that the stacked features at $\ell$-th layer is denoted as $\bm{Z}^{\ell} \in R^{N \times D^{\ell}}$, where $N$ is the total number of samples used for feature evaluation, and $D^{\ell}$ denote the feature dimension for one sample at $\ell$-th layer.  
The metrics used to evaluate features in this paper are computed as follows.

\subsection{Feature Variance}
Building on previous studies \cite{rangamani2023feature, wang2023understanding}, we use within-class feature variance and between-class feature variance to evaluate the feature structure. Specifically, the within-class variance quantifies the degree of feature compression within the same class, while the between-class variance measures the degree of feature discrimination between different classes.

Before calculating the within-class and between-class variances, we first compute the within-class, between-class, and total covariance matrices of the features at $\ell$-th layer as follows:
\begin{equation}
\begin{aligned}
\Sigma^{\ell}_W &= \frac{1}{N} \sum_{c=1}^C \sum_{i=1}^{N_c} \left( \bm{z}^{\ell}_{c, i} - \bm{\mu}^{\ell}_c \right) \left( \bm{z}^{\ell}_{c, i} - \bm{\mu}^{\ell}_c \right)^{\top} \\
\Sigma^{\ell}_B &= \frac{1}{C} \sum_{c=1}^C \left( \bm{\mu}^{\ell}_c - \bm{\mu}^{\ell}_G \right) \left( \bm{\mu}^{\ell}_c - \bm{\mu}^{\ell}_G \right)^{\top} \\
\Sigma^{\ell}_T &= \frac{1}{N} \sum_{c=1}^C \sum_{i=1}^{N_c} \left( \bm{z}^{\ell}_{c,i} - \bm{\mu}^{\ell}_G \right) \left( \bm{z}^{\ell}_{c,i} - \bm{\mu}^{\ell}_G \right)^{\top},
\end{aligned}
\end{equation}

where

\begin{equation}
    \bm{\mu}^{\ell}_c := \frac{1}{N_c} \sum_{i=1}^{N_c} \bm{z}^{\ell}_{c,i} \quad \bm{\mu}^{\ell}_G := \frac{1}{N} \sum_{c=1}^C \sum_{i=1}^{N_c} \bm{z}^{\ell}_{c,i}.
\end{equation}

In the above equation, \(\bm{\mu}^{\ell}_c\) is the mean class feature computed from the samples within the \(c\)-th class, and \(\bm{\mu}^{\ell}_G\) as the global mean feature computed from all samples. \(N_c\) represents the number of samples within the \(c\)-th class. It should be noted that the total covariance matrix can be decomposed as the sum of the within-class and between-class covariances, i.e.,

\begin{equation}
\Sigma^{\ell}_T = \Sigma^{\ell}_W + \Sigma^{\ell}_B.
\end{equation}

Based on the computed covariance matrices, we then use the total variance \( \text{Tr}(\Sigma^{\ell}_T) \) as the normalization factor and compute the normalized within-class variance and between-class variance as follows:

\begin{equation}
    \Bar{\sigma}^{\ell}_W = \frac{\text{Tr}(\Sigma^{\ell}_W)}{\text{Tr}(\Sigma^{\ell}_T)},
\end{equation}
\begin{equation}
    \quad \Bar{\sigma}^{\ell}_B = \frac{\text{Tr}(\Sigma^{\ell}_B)}{\text{Tr}(\Sigma^{\ell}_T)},
\end{equation}

where \(\text{Tr}(\cdot)\) denotes the trace of the covariance matrices. These two normalized variances are used to measure the within-class feature compression and between-class feature discrimination, respectively, in this paper.

\subsection{alignment between features and parameters}
Following previous studies, we use the principal angles between subspaces (PABS) \cite{rangamani2023feature, jordan1875essai, bjorck1973numerical}—denoted as $\theta_1, \ldots, \theta_C$—to measure the alignment between the range space of class-wise feature means $\Bar{\bm{Z}}^{\ell}$ and the top $C$-rank subspace of the subsequent input layer parameters $\bm{W}^{\ell + 1}$.

Specifically, for a linear layer, we first apply singular value decomposition (SVD) to $\bm{W}^{\ell+1}$ and $\Bar{\bm{Z}}^{\ell}$, i.e., $\bm{W}^{\ell+1} = \bm{U}^{\ell+1}_{\bm{W}}\bm{S}^{\ell+1}_{\bm{W}}(\bm{V}_{\bm{W}}^{\ell+1})^{T}$ and $\Bar{\bm{Z}}^{\ell} = \bm{U}^{\ell}_{\Bar{\bm{Z}}}\bm{S}^{\ell}_{\Bar{\bm{Z}}}(\bm{V}_{\Bar{\bm{Z}}}^{\ell})^{T}$. We then compute the PABS between $\bm{V}^{\ell+1}_{\bm{W}}$ and $\bm{U}^{\ell}_{\Bar{\bm{Z}}}$, which represent the basis for the input subspace $\bm{W}^{\ell+1}$ and the range space of $\Bar{\bm{Z}}^{\ell}$, respectively. The alignment is finally computed by the mean of the singular values of $(\bm{V}^{\ell+1}_{W})^{T} \bm{U}^{\ell}_{\Bar{\bm{Z}}}$.

For the convolutional layer, assume the filter kernel has shape $\bm{W}^{\ell+1} \in R^{C^{\ell+1}_{\text{out}} \times C^{\ell}_{\text{in}} \times k^{\ell+1}_H \times k^{\ell+1}_W}$,
and the class-wise feature means have shape 
$\Bar{\bm{Z}}^{\ell} \in \mathbb{R}^{C \times C^{\ell}_{\text{in}} \times H^{\ell} \times W^{\ell}}$.
As previously stated, \(\Bar{\bm{Z}}^{l}\) can be reshaped into
$C \times (C^{\ell}_{\text{in}} \times 4)$.
We begin by flattening the features and parameters along the 
\(C^{\ell}_{\text{in}}\) dimension, resulting in shapes 
$C^{\ell}_{\text{in}} \times (C \times 4)$
for the features and
$C^{\ell}_{\text{in}} \times (C^{\ell + 1}_{\text{out}} \times k^{\ell + 1}_H \times k^{\ell + 1}_W)$
for the parameters. We then compute the alignment along the 
\(C^{\ell}_{\text{in}}\) dimension as described above.

For the self-attention layer in a ViT, we compute the alignments of 
the features and the \(QKV\) matrices separately, and then take the 
average of these alignments as the final metric.

\subsection{Stable Rank}
The stable rank of a feature matrix identifies redundant or highly correlated features within a dataset, which is a measure that combines both the matrix's rank and its singular values. It is defined as the ratio of the squared Frobenius norm of the matrix to the squared largest singular value. In our experiments, we compute the stable rank of centered features for each class and them average them.
Mathematically, for a centered feature matrix of $c$-th class \( \Bar{\bm{Z}}^{\ell}_c \), the stable rank is given by:

\begin{equation}
\text{SRank}(\Bar{\bm{Z}}^{\ell}_c) = \frac{\|\Bar{\bm{Z}}^{\ell}_c\|_F^2}{\sigma_{\text{max}}(\Bar{\bm{Z}}^{\ell}_c)^2},
\end{equation}

where \( \|\Bar{\bm{Z}}^{\ell}_c\|_F \) is the Frobenius norm of \( \Bar{\bm{Z}}^{\ell}_c \), which is the square root of the sum of the squares of all entries in the feature matrix, and \( \sigma_{\text{max}}(\Bar{\bm{Z}}^{\ell}_c) \) is the largest singular value of \( \Bar{\bm{Z}}^{\ell}_c \). 
It provides a more stable approximation of the rank by accounting for the distribution of the singular values, rather than just the rank itself. Unlike the true rank, which can be sensitive to small changes in the matrix, the stable rank remains more robust to perturbations in data.
In our experiments, we average the stable rank of each class to obtain the stable rank of the given evaluated dataset, that is:

\begin{equation}
    \text{SRank}(\Bar{\bm{Z}}^{\ell}) = \frac{1}{C} \sum_{c=1}^{C} \Bar{\bm{Z}}^{\ell}_c, 
\end{equation}
where $C$ denotes the total number of classes.

\subsection{Accuracy of Linear Probing}
Linear probing is a technique used in transfer learning to evaluate the quality of learned features by training a simple linear classifier on top of the features extracted from a pre-trained model \cite{chen2020simple, he2022masked, wang2023does}. In this paper, we employ the linear probing technique to assess feature generalization across diverse data distributions. Specifically, after extracting features from different layers, we apply a randomly initialized linear classifier on top of these features. This classifier is then trained using the training subset of the evaluated datasets, and we compute accuracy by testing on the corresponding samples from the test datasets. The testing accuracy serves as the metric for evaluating feature generalization on each dataset.

\subsection{Pairwise Distance of Features and Models}
We use four metrics to evaluate the distance between two models or the features extracted by them: \textit{mean normalized $L_1$ distance, mean squared distance, mean $L_1$ distance, and cosine similarity}. In this section, we focus on the computation of the distance between features, since the model parameters are reshaped into a single vector, which can be treated as a feature with a sample size of 1. Thus, the distance computation is applied to these features can be directly transferred to models. Let the features of the pre-aggregated and post-aggregated models be denoted as $\bm{Z}^{\ell}_{pre}$ and $\bm{Z}^{\ell}_{post}$, respectively. The corresponding distance can then be computed as follows.

\begin{enumerate}[label=\textbullet] 
\item \textbf{Mean Normalized $L_1$ Distance.}
This measure computes the mean normalized $L_1$ distance between the pre-aggregated and post-aggregated feature matrices, and then averages the distances across all elements, as shown below.
\begin{equation}
    D^{\ell}_{\hat{L}_1} = \frac{1}{ND} \sum_{i=1}^{N} \sum_{j = 1}^{D} \frac{|\bm{Z}^{\ell}_{pre}(i, j) - \bm{Z}^{\ell}_{post}(i, j)|}{|\bm{Z}^{\ell}_{pre}(i, j)| + |\bm{Z}^{\ell}_{post}(i, j)|}
\end{equation}
\item \textbf{Mean Squared Error.}
This distance measure computes the average squared differences between corresponding elements of the feature vectors, as shown below.
\begin{equation}
    D^{\ell}_{s} = \frac{1}{ND} \sum_{i=1}^{N} \sum_{j = 1}^{D} (\bm{Z}^{\ell}_{pre}(i, j) - \bm{Z}^{\ell}_{post}(i, j))^2
\end{equation}
\item \textbf{Mean $L_1$ Distance.}
This distance computes the average $L_1$ distances between corresponding elements of the pre-aggregated and post-aggregated feature matrices, as shown below.
\begin{equation}
    D^{\ell}_{L_1} = \frac{1}{ND} \sum_{i=1}^{N} \sum_{j = 1}^{D} |\bm{Z}^{\ell}_{pre}(i, j) - \bm{Z}^{\ell}_{post}(i, j)|
\end{equation}

\item \textbf{Mean Cosine Similarity.} This measure computes the cosine of the angle between the pre-aggregated and post-aggregated features of the same samples, and then averages these values across all clients. It quantifies the cosine of the angle between the pre-aggregated and post-aggregated features, where a value of 1 indicates identical directions and a value of -1 indicates opposite directions.
The formulation of the mean cosine similarity is shown below.
\begin{equation}
    D^{\ell}_{cos} = \frac{1}{N} \sum_{i=1}^{N} \frac{Z^{\ell}_{pre}(i, :)\cdot Z^{\ell}_{post}(i, :)}{||Z^{\ell}_{pre}(i, :)||||Z^{\ell}_{post}(i, :)||}
\end{equation}
where $Z^{\ell}_{pre}(i, :) \cdot Z^{\ell}_{post}(i, :)$ denotes the dot product of the pre-aggregated and post-aggregated features of sample $i$ at $\ell$-th layer, respectively, and $||Z^{\ell}_{pre}(i, :)||$ and $||Z^{\ell}_{post}(i, :)||$ denote the Euclidean norms of the pre-aggregated and post-aggregated features at $\ell$-th layer, respectively.
\end{enumerate}

\subsection{Relative Change of Evaluated Metrics}
Since the original metrics of features at different layers can vary in magnitude, we use the relative change in the evaluated metrics to measure the ratio of change before and after aggregation. 
Let $V^{\ell}_{pre}$ and $V^{\ell}_{post}$ represent the metrics of features generated by the models before and after aggregation at $\ell$-th layer, respectively. The relative change in the evaluated metrics is then defined as:

\begin{equation}
    \Delta^{\ell}(V) = \frac{|V^{\ell}_{post} - V^{\ell}_{pre}|}{|V^{\ell}_{pre}| + |V^{\ell}_{post}|} * 100\%.
\end{equation}


\section{Detailed Results of Performance Drop}
In this section, we provide more detailed results that demonstrate the performance drop during model aggregation.
In these experiments, we perform inference on both the training and testing datasets using the pre-aggregated and post-aggregated models.
The experimental results are shown in Figure \ref{FedAvg_Acc_DomainNet_PACS_Digits}.
These experiments are conducted on different datasets, including Digit-Five, PACS, and DomainNet, and on various model architectures.
From Figure \ref{FedAvg_Acc_DomainNet_PACS_Digits}, we can observe that the performance drop introduced by model aggregation is consistent across all adopted datasets and model architectures, on both training and testing dataset.
The performance drop consistently occurs throughout the entire training procedure of FL.
These results indicate that the performance drop during model aggregation is a common phenomenon in FL.

\begin{figure}[ht]
\centering
\includegraphics[width=6.5in]{FedAvg_Acc_DomainNet_PACS_Digits.pdf}
\caption{Training and testing accuracy curves of the model before and after aggregation, evaluated on the local dataset during FL training. The experiments are conducted on Digit-Five, DomainNet, and PACS, using multiple model architectures as the backbone.}
\label{FedAvg_Acc_DomainNet_PACS_Digits}
\end{figure}

\section{Detailed Results of Feature Variance}
In this section, we provide a detailed analysis of the layer-wise feature variance during FL training. Our experiments are conducted on three datasets: Digit-Five, PACS, and DomainNet, using various model architectures, as previously described.
For each dataset, we presents four types of feature variances: normalized within-class feature variance, normalized between-class feature variance, original unnormalized within-class feature variance, and original unnormalized between-class feature variance.

To better visualize feature evolution over time (across different epochs of FL training) and space (across different layers), we employ two types of visualizations. The first visualizes feature changes across different layers while keeping the training global round fixed. The second focuses on visualizing feature evolution across training rounds while fixing specific layers.

The experimental results are presented in the following sections. From these results, we observe that both the original within-class and between-class feature variances increase as the layer depth increases. In contrast, the normalized within-class feature variance decreases with both layer depth and training rounds, which is in contrast to the normalized between-class feature variance. This suggests that features within the same class become more compressed, while features across different classes become more discriminative.

However, after model aggregation, the normalized within-class variance increases while the normalized between-class variance decreases. This indicates that model aggregation disrupts the feature compression objective during DNN training.
More importantly, this disruption progressively accumulates across model layers, causing the features in the penultimate layer (which are used for final decision-making) to degrade more significantly.

\subsection{Changes of Feature Variance Across Layers}

\begin{figure}[H]
\centering
\includegraphics[width=4.5in]{FedAvg_AllModel_Digits_Epochwise_NC1_NC1.pdf}
\caption{Changes in the normalized within-class variance of features across model layers for specific global rounds, with larger X-axis values indicating deeper layers. The model is trained on Digit-Five with multiple models that are randomly initialized. The top half of the figure shows the normalized within-class variance, while the bottom half displays the relative change in variance before and after model aggregation.}
\label{FedAvg_AllModel_Digits_Epochwise_NC1_NC1}
\end{figure}

\begin{figure}[H]
\centering
\includegraphics[width=4.5in]{FedAvg_AllModel_Digits_Epochwise_NC1_between.pdf}
\caption{Changes in the normalized between-class variance of features across model layers for specific global rounds, with larger X-axis values indicating deeper layers. The model is trained on Digit-Five with multiple models that are randomly initialized. The top half of the figure shows the normalized between-class variance, while the bottom half displays the relative change in variance before and after model aggregation.}
\label{FedAvg_AllModel_Digits_Epochwise_NC1_between}
\end{figure}

\begin{figure}[H]
\centering
\includegraphics[width=4.5in]{FedAvg_AllModel_Digits_Epochwise_NC1_trace_within.pdf}
\caption{Changes in the original unnormalized within-class variance of features across model layers for specific global rounds, with larger X-axis values indicating deeper layers. The model is trained on Digit-Five with multiple models that are randomly initialized. The top half of the figure shows the original unnormalized within-class variance, while the bottom half displays the relative change in variance before and after model aggregation.}
\label{FedAvg_AllModel_Digits_Epochwise_NC1_trace_within}
\end{figure}

\begin{figure}[H]
\centering
\includegraphics[width=4.5in]{FedAvg_AllModel_Digits_Epochwise_NC1_trace_between.pdf}
\caption{Changes in the original unnormalized between-class variance of features across model layers for specific global rounds, with larger X-axis values indicating deeper layers. The model is trained on Digit-Five with multiple models that are randomly initialized. The top half of the figure shows the original unnormalized between-class variance, while the bottom half displays the relative change in variance before and after model aggregation.}
\label{FedAvg_AllModel_Digits_Epochwise_NC1_trace_between}
\end{figure}

\begin{figure}[H]
\centering
\includegraphics[width=6.8in]{FedAvg_AllModel_PACS_Epochwise_NC1_NC1.pdf}
\caption{Changes in the normalized within-class variance of features across model layers for specific global rounds, with larger X-axis values indicating deeper layers. The model is trained on PACS with multiple models that are randomly initialized. The top half of the figure shows the normalized within-class variance, while the bottom half displays the relative change in variance before and after model aggregation.}
\label{FedAvg_AllModel_PACS_Epochwise_NC1_NC1}
\end{figure}

\begin{figure}[H]
\centering
\includegraphics[width=6.8in]{FedAvg_AllModel_PACS_Epochwise_NC1_between.pdf}
\caption{Changes in the normalized between-class variance of features across model layers for specific global rounds, with larger X-axis values indicating deeper layers. The model is trained on PACS with multiple models that are randomly initialized. The top half of the figure shows the normalized between-class variance, while the bottom half displays the relative change in variance before and after model aggregation.}
\label{FedAvg_AllModel_PACS_Epochwise_NC1_between}
\end{figure}

\begin{figure}[H]
\centering
\includegraphics[width=6.8in]{FedAvg_AllModel_PACS_Epochwise_NC1_trace_within.pdf}
\caption{Changes in the original unnormalized within-class variance of features across model layers for specific global rounds, with larger X-axis values indicating deeper layers. The model is trained on PACS with multiple models that are randomly initialized. The top half of the figure shows the original unnormalized within-class variance, while the bottom half displays the relative change in variance before and after model aggregation.}
\label{FedAvg_AllModel_PACS_Epochwise_NC1_trace_within}
\end{figure}

\begin{figure}[H]
\centering
\includegraphics[width=6.8in]{FedAvg_AllModel_PACS_Epochwise_NC1_trace_between.pdf}
\caption{Changes in the original unnormalized between-class variance of features across model layers for specific global rounds, with larger X-axis values indicating deeper layers. The model is trained on PACS with multiple models that are randomly initialized. The top half of the figure shows the original unnormalized between-class variance, while the bottom half displays the relative change in variance before and after model aggregation.}
\label{FedAvg_AllModel_PACS_Epochwise_NC1_trace_between}
\end{figure}


\begin{figure}[H]
\centering
\includegraphics[width=6.8in]{FedAvg_AllModel_DomainNet_Epochwise_NC1_NC1.pdf}
\caption{Changes in the normalized within-class variance of features across model layers for specific global rounds, with larger X-axis values indicating deeper layers. The model is trained on DomainNet with multiple models that are randomly initialized. The top half of the figure shows the normalized within-class variance, while the bottom half displays the relative change in variance before and after model aggregation.}
\label{FedAvg_AllModel_DomainNet_Epochwise_NC1_NC1}
\end{figure}

\begin{figure}[H]
\centering
\includegraphics[width=6.8in]{FedAvg_AllModel_DomainNet_Epochwise_NC1_between.pdf}
\caption{Changes in the normalized between-class variance of features across model layers for specific global rounds, with larger X-axis values indicating deeper layers. The model is trained on DomainNet with multiple models that are randomly initialized. The top half of the figure shows the normalized between-class variance, while the bottom half displays the relative change in variance before and after model aggregation.}
\label{FedAvg_AllModel_DomainNet_Epochwise_NC1_between}
\end{figure}

\begin{figure}[H]
\centering
\includegraphics[width=6.8in]{FedAvg_AllModel_DomainNet_Epochwise_NC1_trace_within.pdf}
\caption{Changes in the unnormalized within-class variance of features across model layers for specific global rounds, with larger X-axis values indicating deeper layers. The model is trained on DomainNet with multiple models that are randomly initialized. The top half of the figure shows the original unnormalized within-class variance, while the bottom half displays the relative change in variance before and after model aggregation.}
\label{FedAvg_AllModel_DomainNet_Epochwise_NC1_trace_within}
\end{figure}

\begin{figure}[H]
\centering
\includegraphics[width=6.8in]{FedAvg_AllModel_DomainNet_Epochwise_NC1_trace_between.pdf}
\caption{Changes in the unnormalized between-class variance of features across model layers for specific global rounds, with larger X-axis values indicating deeper layers. The model is trained on DomainNet with multiple models that are randomly initialized. The top half of the figure shows the original unnormalized between-class variance, while the bottom half displays the relative change in variance before and after model aggregation.}
\label{FedAvg_AllModel_DomainNet_Epochwise_NC1_trace_between}
\end{figure}

\subsection{Changes of Feature Variance Across Training Rounds}

\begin{figure}[H]
\centering
\includegraphics[width=4.5in]{FedAvg_AllModel_Digits_Layerwise_NC1_NC1.pdf}
\caption{Changes in the normalized within-class variance of features across FL training at specific model layers. The model is trained on Digit-Five with multiple models that are randomly initialized. The top half of the figure shows the normalized within-class variance, while the bottom half displays the relative change in variance before and after model aggregation.}
\label{FedAvg_AllModel_Digits_Layerwise_NC1_NC1}
\end{figure}

\begin{figure}[H]
\centering
\includegraphics[width=4.5in]{FedAvg_AllModel_Digits_Layerwise_NC1_between.pdf}
\caption{Changes in the normalized between-class variance of features across FL training at specific model layers. The model is trained on Digit-Five with multiple models that are randomly initialized. The top half of the figure shows the normalized between-class variance, while the bottom half displays the relative change in variance before and after model aggregation.}
\label{FedAvg_AllModel_Digits_Layerwise_NC1_between}
\end{figure}

\begin{figure}[H]
\centering
\includegraphics[width=4.5in]{FedAvg_AllModel_Digits_Layerwise_NC1_trace_within.pdf}
\caption{Changes in the original unnormalized within-class variance of features across FL training at specific model layers. The model is trained on Digit-Five with multiple models that are randomly initialized. The top half of the figure shows the original unnormalized within-class variance, while the bottom half displays the relative change in variance before and after model aggregation.}
\label{FedAvg_AllModel_Digits_Layerwise_NC1_trace_within}
\end{figure}

\begin{figure}[H]
\centering
\includegraphics[width=4.5in]{FedAvg_AllModel_Digits_Layerwise_NC1_trace_between.pdf}
\caption{Changes in the original unnormalized between-class variance of features across FL training at specific model layers. The model is trained on Digit-Five with multiple models that are randomly initialized. The top half of the figure shows the original unnormalized between-class variance, while the bottom half displays the relative change in variance before and after model aggregation.}
\label{FedAvg_AllModel_Digits_Layerwise_NC1_trace_between}
\end{figure}

\begin{figure}[H]
\centering
\includegraphics[width=6.8in]{FedAvg_AllModel_PACS_Layerwise_NC1_NC1.pdf}
\caption{Changes in the normalized within-class variance of features across FL training at specific model layers. The model is trained on PACS with multiple models that are randomly initialized. The top half of the figure shows the normalized within-class variance, while the bottom half displays the relative change in variance before and after model aggregation.}
\label{FedAvg_AllModel_PACS_Layerwise_NC1_NC1}
\end{figure}

\begin{figure}[H]
\centering
\includegraphics[width=6.8in]{FedAvg_AllModel_PACS_Layerwise_NC1_between.pdf}
\caption{Changes in the normalized between-class variance of features across FL training at specific model layers. The model is trained on PACS with multiple models that are randomly initialized. The top half of the figure shows the normalized between-class variance, while the bottom half displays the relative change in variance before and after model aggregation.}
\label{FedAvg_AllModel_PACS_Layerwise_NC1_between}
\end{figure}

\begin{figure}[H]
\centering
\includegraphics[width=6.8in]{FedAvg_AllModel_PACS_Layerwise_NC1_trace_within.pdf}
\caption{Changes in the original unnormalized within-class variance of features across FL training at specific model layers. The model is trained on PACS with multiple models that are randomly initialized. The top half of the figure shows the original unnormalized within-class variance, while the bottom half displays the relative change in variance before and after model aggregation.}
\label{FedAvg_AllModel_PACS_Layerwise_NC1_trace_within}
\end{figure}

\begin{figure}[H]
\centering
\includegraphics[width=6.8in]{FedAvg_AllModel_PACS_Layerwise_NC1_trace_between.pdf}
\caption{Changes in the original unnormalized between-class variance of features across FL training at specific model layers. The model is trained on PACS with multiple models that are randomly initialized. The top half of the figure shows the original unnormalized between-class variance, while the bottom half displays the relative change in variance before and after model aggregation.}
\label{FedAvg_AllModel_PACS_Layerwise_NC1_trace_between}
\end{figure}

\begin{figure}[H]
\centering
\includegraphics[width=6.8in]{FedAvg_AllModel_DomainNet_Layerwise_NC1_NC1.pdf}
\caption{Changes in the normalized within-class variance of features across FL training at specific model layers. The model is trained on DomainNet with multiple models that are randomly initialized. The top half of the figure shows the normalized within-class variance, while the bottom half displays the relative change in variance before and after model aggregation.}
\label{FedAvg_AllModel_DomainNet_Layerwise_NC1_NC1}
\end{figure}

\begin{figure}[H]
\centering
\includegraphics[width=6.8in]{FedAvg_AllModel_DomainNet_Layerwise_NC1_between.pdf}
\caption{Changes in the normalized between-class variance of features across FL training at specific model layers. The model is trained on DomainNet with multiple models that are randomly initialized. The top half of the figure shows the normalized between-class variance, while the bottom half displays the relative change in variance before and after model aggregation.}
\label{FedAvg_AllModel_DomainNet_Layerwise_NC1_between}
\end{figure}

\begin{figure}[H]
\centering
\includegraphics[width=6.8in]{FedAvg_AllModel_DomainNet_Layerwise_NC1_trace_within.pdf}
\caption{Changes in the original unnormalized within-class variance of features across FL training at specific model layers. The model is trained on DomainNet with multiple models that are randomly initialized. The top half of the figure shows the original unnormalized within-class variance, while the bottom half displays the relative change in variance before and after model aggregation.}
\label{FedAvg_AllModel_DomainNet_Layerwise_NC1_trace_within}
\end{figure}

\begin{figure}[H]
\centering
\includegraphics[width=6.8in]{FedAvg_AllModel_DomainNet_Layerwise_NC1_trace_between.pdf}
\caption{Changes in the original unnormalized between-class variance of features across FL training at specific model layers. The model is trained on DomainNet with multiple models that are randomly initialized. The top half of the figure shows the original unnormalized between-class variance, while the bottom half displays the relative change in variance before and after model aggregation.}
\label{FedAvg_AllModel_DomainNet_Layerwise_NC1_trace_between}
\end{figure}

\section{Detailed Results of Alignment between Features and Parameters}
In this section, we present the experimental results corresponding to the alignment of features and the parameters of the subsequent layers.
From these results, we observe that the alignment between features and parameters improves as training progresses, with the alignment of penultimate layer features and the classifier increasing more rapidly.

After model aggregation, the alignment between features and parameters tends to decrease. However, the decrease is more pronounced in the classifier. This increased mismatch between penultimate layer features and the classifier, along with the degradation of the penultimate layer features, causes the aggregated model to perform significantly worse when sent back to each client.
\subsection{Changes of Alignment Across Layers}

\begin{figure}[H]
\centering
\includegraphics[width=4.5in]{FedAvg_AllModel_Digits_Epochwise_NC3_NC3.pdf}
\caption{Changes in the alignment between features and parameters across model layers for specific global rounds, with larger X-axis values indicating deeper layers. The model is trained on Digit-Five with multiple models that are randomly initialized. The top half of the figure shows the original alignment values between features and parameters, while the bottom half displays the relative change in alignment before and after model aggregation.}
\label{FedAvg_AllModel_Digits_Epochwise_NC3_NC3}
\end{figure}

\begin{figure}[H]
\centering
\includegraphics[width=6.8in]{FedAvg_AllModel_PACS_Epochwise_NC3_NC3.pdf}
\caption{Changes in the alignment between features and parameters across model layers for specific global rounds, with larger X-axis values indicating deeper layers. The model is trained on PACS with multiple models that are randomly initialized. The top half of the figure shows the original alignment values between features and parameters, while the bottom half displays the relative change in alignment before and after model aggregation.}
\label{FedAvg_AllModel_PACS_Epochwise_NC3_NC3}
\end{figure}

\begin{figure}[H]
\centering
\includegraphics[width=6.8in]{FedAvg_AllModel_DomainNet_Epochwise_NC3_NC3.pdf}
\caption{Changes in the alignment between features and parameters across model layers for specific global rounds, with larger X-axis values indicating deeper layers. The model is trained on DomainNet with multiple models that are randomly initialized. The top half of the figure shows the original alignment values between features and parameters, while the bottom half displays the relative change in alignment before and after model aggregation.}
\label{FedAvg_AllModel_DomainNet_Epochwise_NC3_NC3}
\end{figure}


\subsection{Changes of Alignment Across Training Rounds}

\begin{figure}[H]
\centering
\includegraphics[width=4.5in]{FedAvg_AllModel_Digits_Layerwise_NC3_NC3.pdf}
\caption{Changes in the alignment between features and parameters across FL training at specific model layers. The model is trained on Digit-Five with multiple models that are randomly initialized. The top half of the figure shows the original alignment values between features and parameters, while the bottom half displays the relative change in alignment before and after model aggregation.}
\label{FedAvg_AllModel_Digits_Layerwise_NC3_NC3}
\end{figure}

\begin{figure}[H]
\centering
\includegraphics[width=6.8in]{FedAvg_AllModel_PACS_Layerwise_NC3_NC3.pdf}
\caption{Changes in the alignment between features and parameters across FL training at specific model layers. The model is trained on PACS with multiple models that are randomly initialized. The top half of the figure shows the original alignment values between features and parameters, while the bottom half displays the relative change in alignment before and after model aggregation.}
\label{FedAvg_AllModel_PACS_Layerwise_NC3_NC3}
\end{figure}

\begin{figure}[H]
\centering
\includegraphics[width=6.8in]{FedAvg_AllModel_DomainNet_Layerwise_NC3_NC3.pdf}
\caption{Changes in the alignment between features and parameters across FL training at specific model layers. The model is trained on DomainNet with multiple models that are randomly initialized. The top half of the figure shows the original alignment values between features and parameters, while the bottom half displays the relative change in alignment before and after model aggregation.}
\label{FedAvg_AllModel_DomainNet_Layerwise_NC3_NC3}
\end{figure}

\section{Detailed Results of Feature Stable Rank}
In this section, we present the detailed experimental results corresponding to the stable rank of features. It can be observed that the stable rank increases as both the training process and layer depth progress. However, after model aggregation, the stable rank tends to decrease, which contradicts the training objective of FL.
\subsection{Changes of Stable Rank Across Layers}
\begin{figure}[H]
\centering
\includegraphics[width=4.5in]{FedAvg_AllModel_Digits_Epochwise_stable_rank.pdf}
\caption{Changes in the stable rank of features across model layers for specific global rounds, with larger X-axis values indicating deeper layers. The model is trained on Digit-Five with multiple models that are randomly initialized. The top half of the figure shows the original stable rank of features, while the bottom half displays the relative change in stable rank before and after model aggregation.}
\label{FedAvg_AllModel_Digits_Epochwise_stable_ran}
\end{figure}

\begin{figure}[H]
\centering
\includegraphics[width=6.8in]{FedAvg_AllModel_PACS_Epochwise_stable_rank.pdf}
\caption{Changes in stable rank of features across model layers for specific global rounds, with larger X-axis values indicating deeper layers. The model is trained on PACS with multiple models that are randomly initialized. The top half of the figure shows the original stable rank of features, while the bottom half displays the relative change in stable rank before and after model aggregation.}
\label{FedAvg_AllModel_PACS_Epochwise_stable_ran}
\end{figure}

\begin{figure}[H]
\centering
\includegraphics[width=6.8in]{FedAvg_AllModel_DomainNet_Epochwise_stable_rank.pdf}
\caption{Changes in stabke rank of features across model layers for specific global rounds, with larger X-axis values indicating deeper layers. The model is trained on DomainNet with multiple models that are randomly initialized. The top half of the figure shows the original stable rank of features, while the bottom half displays the relative change in stable rank before and after model aggregation.}
\label{FedAvg_AllModel_DomainNet_Epochwise_stable_rank}
\end{figure}


\subsection{Changes of Stable Rank Across Training Rounds}

\begin{figure}[H]
\centering
\includegraphics[width=4.5in]{FedAvg_AllModel_Digits_Layerwise_stable_rank.pdf}
\caption{Changes in the stable rank of features across FL training at specific model layers. The model is trained on Digit-Five with multiple models that are randomly initialized. The top half of the figure shows the original stable rank of features, while the bottom half displays the relative change in stable rank before and after model aggregation.}
\label{FedAvg_AllModel_Digits_Layerwise_stable_rank}
\end{figure}

\begin{figure}[H]
\centering
\includegraphics[width=6.8in]{FedAvg_AllModel_PACS_Layerwise_stable_rank.pdf}
\caption{Changes in the stable rank of features across FL training at specific model layers. The model is trained on PACS with multiple models that are randomly initialized. The top half of the figure shows the original stable rank of features, while the bottom half displays the relative change in stable rank before and after model aggregation.}
\label{FedAvg_AllModel_PACS_Layerwise_stable_rank}
\end{figure}

\begin{figure}[H]
\centering
\includegraphics[width=6.8in]{FedAvg_AllModel_DomainNet_Layerwise_stable_rank.pdf}
\caption{Changes in the stable rank of features across FL training at specific model layers. The model is trained on DomainNet with multiple models that are randomly initialized. The top half of the figure shows the original stable rank of features, while the bottom half displays the relative change in stable rank before and after model aggregation.}
\label{FedAvg_AllModel_DomainNet_Layerwise_stable_rank}
\end{figure}

\section{Visualization of Pre-aggregated and Post-aggregated Features}

This section visualizes the comparison between pre-aggregated and post-aggregated features.
It can be observed that, as layer depth increases, features become more compressed within the same class and more discriminative across different classes. However, after model aggregation, features become more scattered within the same class and less discriminative across classes. This phenomenon is particularly pronounced in the penultimate layer features (the leftmost subfigure). These observations are consistent with the findings from the quantitative metrics described earlier.

\begin{figure}[H]
\centering
\includegraphics[width=6.8in]{TSNE_DomainNet_quickdraw_ResNet18_10Layers.pdf}
\caption{T-SNE visualization of features at different layers on the `Quickdraw' domain of DomainNet before and after aggregation.
The features are extracted from ResNet18 in the final global round of FL training, whose parameters are randomly initialized at the beginning.}
\label{TSNE_DomainNet_quickdraw_ResNet18_10Layers}
\end{figure}

\begin{figure}[H]
\centering
\includegraphics[width=6.8in]{TSNE_DomainNet_quickdraw_ResNet34_10Layers.pdf}
\caption{T-SNE visualization of features at different layers on the `Quickdraw' domain of DomainNet before and after aggregation.
The features are extracted from ResNet34 in the final global round of FL training, whose parameters are randomly initialized at the beginning.}
\label{TSNE_DomainNet_quickdraw_ResNet34_10Layers}
\end{figure}

\begin{figure}[H]
\centering
\includegraphics[width=6.8in]{TSNE_DomainNet_quickdraw_ResNet50_10Layers.pdf}
\caption{T-SNE visualization of features at different layers on the `Quickdraw' domain of DomainNet before and after aggregation.
The features are extracted from ResNet50 in the final global round of FL training, whose parameters are randomly initialized at the beginning.}
\label{TSNE_DomainNet_quickdraw_ResNet50_10Layers}
\end{figure}

\section{Feature Distance and Parameter Distance}

This section compares the distance between features and parameters in the pre-aggregated and post-aggregated models at each layer. It can be observed that, with the exception of the final classifier, the parameter distance between the pre-aggregated and post-aggregated models is significantly smaller than the feature distance.
Furthermore, the distances between parameters and features show different trends across layers. While the parameter distance decreases, the feature distance increases as the layer depth increases.
This demonstrates that even small variations in the parameter space can lead to significant feature variations, as the stacked structure of DNNs tends to magnify errors in feature extraction.
This observation suggests that the `client drift' proposed by previous studies may not be the root cause of performance drops during model aggregation.

\begin{figure}[H]
\centering
\includegraphics[width=6.0in]{FedAvg_Distance_DomainNet_ResNet18_norm_abs_dis_mean_L2_dis_mean_L1_dis_cos_similarity_Epochwise.pdf}
\caption{Changes in distance of the features and parameters obtained from models before and after aggregation across model layers for specific global rounds, with larger X-axis values indicating deeper layers. The model is trained on DomainNet using ResNet18. The distance is measured by 4 metric including mean normalized distance, mean squared distance, mean $L_1$ distance, and mean cosine similarity. The top half of the figure shows the distance between the parameters of pre-aggregated and post-aggregated models, while the bottom half displays the distance between the features of pre-aggregated and post-aggregated models.
}
\label{FedAvg_Distance_DomainNet_ResNet18_norm_abs_dis_mean_L2_dis_mean_L1_dis_cos_similarity_Epochwise}
\end{figure}

\begin{figure}[H]
\centering
\includegraphics[width=6.0in]{FedAvg_Distance_DomainNet_ResNet34_norm_abs_dis_mean_L2_dis_mean_L1_dis_cos_similarity_Epochwise.pdf}
\caption{Changes in distance of the features and parameters obtained from models before and after aggregation across model layers for specific global rounds, with larger X-axis values indicating deeper layers. The model is trained on DomainNet using ResNet18. The distance is measured by 4 metric including mean normalized distance, mean squared distance, mean $L_1$ distance, and mean cosine similarity. The top half of the figure shows the distance between the parameters of pre-aggregated and post-aggregated models, while the bottom half displays the distance between the features of pre-aggregated and post-aggregated models.
}
\label{FedAvg_Distance_DomainNet_ResNet34_norm_abs_dis_mean_L2_dis_mean_L1_dis_cos_similarity_Epochwise}
\end{figure}

\begin{figure}[H]
\centering
\includegraphics[width=6.0in]{FedAvg_Distance_DomainNet_ResNet50_norm_abs_dis_mean_L2_dis_mean_L1_dis_cos_similarity_Epochwise.pdf}
\caption{Changes in distance of the features and parameters obtained from models before and after aggregation across model layers for specific global rounds, with larger X-axis values indicating deeper layers. The model is trained on DomainNet using ResNet18. The distance is measured by 4 metric including mean normalized distance, mean squared distance, mean $L_1$ distance, and mean cosine similarity. The top half of the figure shows the distance between the parameters of pre-aggregated and post-aggregated models, while the bottom half displays the distance between the features of pre-aggregated and post-aggregated models.
}
\label{FedAvg_Distance_DomainNet_ResNet50_norm_abs_dis_mean_L2_dis_mean_L1_dis_cos_similarity_Epochwise}
\end{figure}

\section{Detailed Results of Linear Probing}

This section evaluates the accuracy of linear probing across diverse data distributions. In our experiments, we use SGD with a learning rate of 0.01 to optimize the randomly initialized linear layer. The batch size is set to 64, and the total number of epochs is set to 100. We report the best test accuracy as the accuracy of linear probing.

From the experimental results, we observe that the features generated by the post-aggregated model perform well across diverse data distributions. In contrast, the pre-aggregated model only performs well on its local data distribution. This suggests that, while the model after aggregation may not extract task-specific features for local distributions, it is more capable of extracting generalized features that can be applied to different distributions. This is also how model aggregation improves performance compared to purely local training.

\begin{figure}[H]
\centering
\includegraphics[width=3.5in]{Linear_Probing_DomainNet_E50_ResNet18_Long_Epochwise.pdf}
\caption{Accuracy of linear probing at different layers across different domains in DomainNet. The experiments are performed in global round 50 using ResNet18 as the backbone model. In the figures, E50@Post represents the results using the model after aggregation, while E50\_*@Pre represents the results using the model before aggregation for domain *.}
\label{Linear_Probing_DomainNet_E50_ResNet18_Long_Epochwise}
\end{figure}

\begin{figure}[H]
\centering
\includegraphics[width=4.0in]{Linear_Probing_DomainNet_E50_ResNet34_Long_Epochwise.pdf}
\caption{Accuracy of linear probing at different layers across different domains in DomainNet. The experiments are performed in global round 50 using ResNet34 as the backbone model. In the figures, E50@Post represents the results using the model after aggregation, while E50\_*@Pre represents the results using the model before aggregation for domain *.}
\label{Linear_Probing_DomainNet_E50_ResNet34_Long_Epochwise}
\end{figure}

\section{Effect of Residual Connection}
This section analyzes the role of residual connections in DNNs when performing model aggregation. We use ResNet18 and ResNet34 as backbones and remove the residual connections from these models. We then employ these modified architectures for FL training. This section presents a comparison of accuracy and feature metrics between the models with and without residual connections. We observe that the relative change in feature metrics is more pronounced when the residual connections are removed, particularly during the early stages of model training and in the normalized within-class and between-class feature variance metrics. This may be because residual connections help mitigate feature degradation by allowing less disrupted features in lower layers to be propagated to deeper layers.

\begin{figure}[H]
\centering
\includegraphics[width=5.1in]{Accuracy_Residual_Connection.pdf}
\caption{Averaged training and testing accuracy curves of the model before and after aggregation, evaluated on the local dataset during FL training. The experiments are conducted on the DomainNet dataset. The models trained include the original ResNet18 and ResNet, as well as their plain versions, with residual connections removed.}
\label{Accuracy_Residual_Connection}
\end{figure}

\begin{figure}[H]
\centering
\includegraphics[width=6.8in]{FedAvg_AllModel_DomainNet_ResNet18_Plain_ResNet18_Epochwise_stable_rank_R9_Residual_Connection.pdf}
\caption{Comparison of feature metrics across layers with and without the use of residual connections (Plain\_ResNet18) in ResNet18. This evaluation is performed on DomainNet using the model at global round $10$.}
\label{FedAvg_AllModel_DomainNet_ResNet18_Plain_ResNet18_Epochwise_stable_rank_R9_Residual_Connection}
\end{figure}

\begin{figure}[H]
\centering
\includegraphics[width=6.8in]{FedAvg_AllModel_DomainNet_ResNet18_Plain_ResNet18_Epochwise_stable_rank_R29_Residual_Connection.pdf}
\caption{Comparison of feature metrics across layers with and without the use of residual connections (Plain\_ResNet18) in ResNet18. This evaluation is performed on DomainNet using the model at global round $30$.}
\label{FedAvg_AllModel_DomainNet_ResNet18_Plain_ResNet18_Epochwise_stable_rank_R29_Residual_Connection}
\end{figure}

\begin{figure}[H]
\centering
\includegraphics[width=6.8in]{FedAvg_AllModel_DomainNet_ResNet18_Plain_ResNet18_Epochwise_stable_rank_R49_Residual_Connection.pdf}
\caption{Comparison of feature metrics across layers with and without the use of residual connections (Plain\_ResNet18) in ResNet18. This evaluation is performed on DomainNet using the model at global round $50$.}
\label{FedAvg_AllModel_DomainNet_ResNet18_Plain_ResNet18_Epochwise_stable_rank_R49_Residual_Connection}
\end{figure}

\begin{figure}[H]
\centering
\includegraphics[width=6.8in]{FedAvg_AllModel_DomainNet_ResNet34_Plain_ResNet34_Epochwise_stable_rank_R9_Residual_Connection.pdf}
\caption{Comparison of feature metrics across layers with and without the use of residual connections (Plain\_ResNet34) in ResNet34. This evaluation is performed on DomainNet using the model at global round $10$.}
\label{FedAvg_AllModel_DomainNet_ResNet34_Plain_ResNet34_Epochwise_stable_rank_R9_Residual_Connection}
\end{figure}

\begin{figure}[H]
\centering
\includegraphics[width=6.8in]{FedAvg_AllModel_DomainNet_ResNet34_Plain_ResNet34_Epochwise_stable_rank_R29_Residual_Connection.pdf}
\caption{Comparison of feature metrics across layers with and without the use of residual connections (Plain\_ResNet34) in ResNet34. This evaluation is performed on DomainNet using the model at global round $30$.}
\label{FedAvg_AllModel_DomainNet_ResNet34_Plain_ResNet34_Epochwise_stable_rank_R29_Residual_Connection}
\end{figure}

\begin{figure}[H]
\centering
\includegraphics[width=6.8in]{FedAvg_AllModel_DomainNet_ResNet34_Plain_ResNet34_Epochwise_stable_rank_R49_Residual_Connection.pdf}
\caption{Comparison of feature metrics across layers with and without the use of residual connections (Plain\_ResNet34) in ResNet34. This evaluation is performed on DomainNet using the model at global round $50$.}
\label{FedAvg_AllModel_DomainNet_ResNet34_Plain_ResNet34_Epochwise_stable_rank_R49_Residual_Connection}
\end{figure}

\begin{figure}[H]
\centering
\includegraphics[width=6.8in]{FedAvg_AllModel_PACS_ResNet18_Plain_ResNet18_Epochwise_stable_rank_R9_Residual_Connection.pdf}
\caption{Comparison of feature metrics across layers with and without the use of residual connections (Plain\_ResNet18) in ResNet18. This evaluation is performed on PACS using the model at global round $10$.}
\label{FedAvg_AllModel_PACS_ResNet18_Plain_ResNet18_Epochwise_stable_rank_R9_Residual_Connection}
\end{figure}

\begin{figure}[H]
\centering
\includegraphics[width=6.8in]{FedAvg_AllModel_PACS_ResNet18_Plain_ResNet18_Epochwise_stable_rank_R29_Residual_Connection.pdf}
\caption{Comparison of feature metrics across layers with and without the use of residual connections (Plain\_ResNet18) in ResNet18. This evaluation is performed on PACS using the model at global round $30$.}
\label{FedAvg_AllModel_PACS_ResNet18_Plain_ResNet18_Epochwise_stable_rank_R29_Residual_Connection}
\end{figure}

\begin{figure}[H]
\centering
\includegraphics[width=6.8in]{FedAvg_AllModel_PACS_ResNet18_Plain_ResNet18_Epochwise_stable_rank_R49_Residual_Connection.pdf}
\caption{Comparison of feature metrics across layers with and without the use of residual connections (Plain\_ResNet18) in ResNet18. This evaluation is performed on PACS using the model at global round $50$.}
\label{FedAvg_AllModel_PACS_ResNet18_Plain_ResNet18_Epochwise_stable_rank_R49_Residual_Connection}
\end{figure}

\begin{figure}[H]
\centering
\includegraphics[width=6.8in]{FedAvg_AllModel_PACS_ResNet34_Plain_ResNet34_Epochwise_stable_rank_R9_Residual_Connection.pdf}
\caption{Comparison of feature metrics across layers with and without the use of residual connections (Plain\_ResNet34) in ResNet34. This evaluation is performed on PACS using the model at global round $10$.}
\label{FedAvg_AllModel_PACS_ResNet34_Plain_ResNet34_Epochwise_stable_rank_R9_Residual_Connection}
\end{figure}

\begin{figure}[H]
\centering
\includegraphics[width=6.8in]{FedAvg_AllModel_PACS_ResNet34_Plain_ResNet34_Epochwise_stable_rank_R29_Residual_Connection.pdf}
\caption{Comparison of feature metrics across layers with and without the use of residual connections (Plain\_ResNet34) in ResNet34. This evaluation is performed on PACS using the model at global round $30$.}
\label{FedAvg_AllModel_PACS_ResNet34_Plain_ResNet34_Epochwise_stable_rank_R29_Residual_Connection}
\end{figure}

\begin{figure}[H]
\centering
\includegraphics[width=6.8in]{FedAvg_AllModel_PACS_ResNet34_Plain_ResNet34_Epochwise_stable_rank_R49_Residual_Connection.pdf}
\caption{Comparison of feature metrics across layers with and without the use of residual connections (Plain\_ResNet34) in ResNet34. This evaluation is performed on PACS using the model at global round $50$.}
\label{FedAvg_AllModel_PACS_ResNet34_Plain_ResNet34_Epochwise_stable_rank_R49_Residual_Connection}
\end{figure}

\section{Effect of Parameter Personalization}
This section presents the detailed experimental results obtained by personalizing specific layers within the model during FL training, a method known as personalized federated learning (PFL).
The experiments are conducted using the ResNet18 and ResNet34 models.
As shown in Figure \ref{PFL}, both models can be divided into several blocks: the first convolutional layer, stages 1-4 (which consist of stacked convolutional layers), and the fully connected (FC) layer used for classification.

In the experiments, we explore PFL methods by personalizing specific layers within the model.
Specifically, we first examine two commonly used PFL methods: FedPer \cite{FedPer}, which personalizes the classifier, and FedBN \cite{FedBN}, which personalizes the batch normalization (BN) layers.
Additionally, motivated by \cite{PartialFed}, we consider two more strategies for parameter personalization: the successive parameter personalization strategy and the skip parameter personalization strategy.
For the successive parameter personalization strategy, we personalize multiple consecutive layers starting from the first layer.
In the skip parameter personalization strategy, we randomly select a layer or block for personalization.

The experimental results show that these parameter personalization methods generally lead to better featured distributions on local data. Moreover, the relative changes introduced by model aggregation decrease as more parameters within the feature extractor are personalized. This improvement is due to the reduction in feature disruption accumulation, which is alleviated by personalizing these parameters, thereby maintaining their ability to extract locally task-specific features without being affected by model aggregation. 

\begin{figure}[ht]
\centering
\includegraphics[width=5.5in]{PFL.pdf}
\caption{Illustration of the architectures of ResNet18 and ResNet34, along with the parameter personalization strategies: (a) Successive parameter personalization strategy, (b) Skip parameter personalization strategy.}
\label{PFL}
\end{figure}

\subsection{Successive Parameter Personalization}

\begin{figure}[H]
\centering
\includegraphics[width=6.8in]{PFL_successive_Accuracy.pdf}
\caption{Accuracy curves when training FL models with different successive parameter personalization strategies, along with FedAvg, FedPer, and FedBN. In the legend, `C1' is an abbreviation for the Conv1 layer, and `S*' represents the abbreviation for Stage* block.}
\label{PFL_successive_Accuracy}
\end{figure}

\begin{figure}[H]
\centering
\includegraphics[width=6.8in]{PFL_successive_ResNet18_PACS_Epochwise_stable_rank_R29.pdf}
\caption{Feature metrics across layers at the global round $30$ when training FL models with different successive parameter personalization strategies, along with FedAvg, FedPer, and FedBN. The experiments are conducted on the PACS dataset, using the ResNet18 model. In the legend, `C1' is an abbreviation for the Conv1 layer, and `S*' represents the abbreviation for Stage* block.}
\label{PFL_successive_ResNet18_PACS_Epochwise_stable_rank_R29}
\end{figure}

\begin{figure}[H]
\centering
\includegraphics[width=6.8in]{PFL_successive_ResNet34_PACS_Epochwise_stable_rank_R29.pdf}
\caption{Feature metrics across layers at the global round $30$ when training FL models with different successive parameter personalization strategies, along with FedAvg, FedPer, and FedBN. The experiments are conducted on the PACS dataset, using the ResNet34 model. In the legend, `C1' is an abbreviation for the Conv1 layer, and `S*' represents the abbreviation for Stage* block.}
\label{PFL_successive_ResNet34_PACS_Epochwise_stable_rank_R29}
\end{figure}

\begin{figure}[H]
\centering
\includegraphics[width=6.8in]{PFL_successive_ResNet18_DomainNet_Epochwise_stable_rank_R29.pdf}
\caption{Feature metrics across layers at the global round $30$ when training FL models with different successive parameter personalization strategies, along with FedAvg, FedPer, and FedBN. The experiments are conducted on the DomainNet dataset, using the ResNet18 model. In the legend, `C1' is an abbreviation for the Conv1 layer, and `S*' represents the abbreviation for Stage* block.}
\label{PFL_successive_ResNet18_DomainNet_Epochwise_stable_rank_R29}
\end{figure}

\begin{figure}[H]
\centering
\includegraphics[width=6.8in]{PFL_successive_ResNet34_DomainNet_Epochwise_stable_rank_R29.pdf}
\caption{Feature metrics across layers at the global round $30$ when training FL models with different successive parameter personalization strategies, along with FedAvg, FedPer, and FedBN. The experiments are conducted on the DomainNet dataset, using the ResNet34 model. In the legend, `C1' is an abbreviation for the Conv1 layer, and `S*' represents the abbreviation for Stage* block.}
\label{PFL_successive_ResNet34_DomainNet_Epochwise_stable_rank_R29}
\end{figure}

\subsection{Skip Parameter Personalization}
\begin{figure}[H]
\centering
\includegraphics[width=6.8in]{PFL_skip_Accuracy.pdf}
\caption{Accuracy curves when training FL models with different skip parameter personalization strategies, along with FedAvg, FedPer, and FedBN. In the legend, `C1' is an abbreviation for the Conv1 layer, and `S*' represents the abbreviation for Stage* block.}
\label{PFL_successive_Accuracy}
\end{figure}

\begin{figure}[ht]
\centering
\includegraphics[width=6.8in]{PFL_skip_ResNet18_PACS_Epochwise_stable_rank_R29.pdf}
\caption{Feature metrics across layers at the global round $30$ when training FL models with different skip parameter personalization strategies, along with FedAvg, FedPer, and FedBN. The experiments are conducted on the PACS dataset, using the ResNet18 model. In the legend, `C1' is an abbreviation for the Conv1 layer, and `S*' represents the abbreviation for Stage* block.}
\label{PFL_skip_ResNet18_PACS_Epochwise_stable_rank_R29}
\end{figure}

\begin{figure}[H]
\centering
\includegraphics[width=6.8in]{PFL_skip_ResNet34_PACS_Epochwise_stable_rank_R29.pdf}
\caption{Feature metrics across layers at the global round $30$ when training FL models with different skip parameter personalization strategies, along with FedAvg, FedPer, and FedBN. The experiments are conducted on the PACS dataset, using the ResNet34 model. In the legend, `C1' is an abbreviation for the Conv1 layer, and `S*' represents the abbreviation for Stage* block.}
\label{PFL_skip_ResNet34_PACS_Epochwise_stable_rank_R29}
\end{figure}

\begin{figure}[H]
\centering
\includegraphics[width=6.8in]{PFL_skip_ResNet18_DomainNet_Epochwise_stable_rank_R29.pdf}
\caption{Feature metrics across layers at the global round $30$ when training FL models with different skip parameter personalization strategies, along with FedAvg, FedPer, and FedBN. The experiments are conducted on the DomainNet dataset, using the ResNet18 model. In the legend, `C1' is an abbreviation for the Conv1 layer, and `S*' represents the abbreviation for Stage* block.}
\label{PFL_skip_ResNet18_DomainNet_Epochwise_stable_rank_R29}
\end{figure}

\begin{figure}[H]
\centering
\includegraphics[width=6.8in]{PFL_skip_ResNet34_DomainNet_Epochwise_stable_rank_R29.pdf}
\caption{Feature metrics across layers at the global round $30$ when training FL models with different parameter personalization strategies, along with FedAvg, FedPer, and FedBN. The experiments are conducted on the DomainNet dataset, using the ResNet34 model. In the legend, `C1' is an abbreviation for the Conv1 layer, and `S*' represents the abbreviation for Stage* block.}
\label{PFL_skip_ResNet34_DomainNet_Epochwise_stable_rank_R29}
\end{figure}

\section{Detailed Results when Fine-tuning Classifier}
This section provides detailed experimental results on fine-tuning the classifier to reduce the mismatch between the features extracted by the global feature extractor and the classifier. The experiments are conducted on DomainNet using ResNet18 and ResNet34 as backbones. During fine-tuning, we use SGD with momentum as the optimizer, with a learning rate of 0.01 and momentum set to 0.1. We perform only 10 epochs of fine-tuning.

It can be observed that, after fine-tuning the classifier, the alignment between the features and the classifier consistently improves using models at different global rounds during the FL training. As a result, the testing accuracy also improves.

\begin{figure}[H]
\centering
\includegraphics[width=6.0in]{FedAvg_ResNet18_ResNet34_DomainNet_Finetune_Classifier.pdf}
\caption{Accuracy and alignment between features and parameters during classifier fine-tuning across global rounds. The experiments are conducted on DomainNet dataset, using ResNet18 and ResNet34 as backbone model.}
\label{FedAvg_ResNet18_ResNet34_DomainNet_Finetune_Classifier}
\end{figure}

\section{Effect of Pre-trained Parameter}
In this section, we present detailed experimental results from using pre-trained parameters as initialization for FL training.
These experiments follow the same setup as the ones described above, with the only difference being that the model is initialized with parameters pre-trained on large-scale datasets.
The experimental results include the changes in feature variance across layers and training rounds, the alignment between features and parameters across layers and rounds, the stable rank of features, and the visualization of pre-aggregated and post-aggregated features.
From these experiments, we find that initializing with pre-trained parameters effectively mitigates the accumulation of feature degradation.
This conclusion is based on observations from experiments with randomly initialized parameters, where deeper layers only begin to converge once shallow features have reached a specific state.
This is primarily because the degradation of unconverged features in the shallow layers propagates to the deeper layers, preventing them from converging until the shallow layers are well-trained. This significantly hinders the convergence rate of FL training.
However, when initialized with pre-trained parameters, we find that the features of most shallow layers converge early in training, as the model already possesses strong feature extraction capabilities from being trained on a large-scale dataset.
This significantly reduces the feature degradation accumulation introduced by model aggregation, thereby stabilizing and accelerating the convergence of FL training.

\subsection{Comparison of Training and Testing Accuracy}

\begin{figure}[!htbp]
\centering
\includegraphics[width=6.8in]{FedAvg_AllModel_Pretrained_Accuracy.pdf}
\caption{Comparison of accuracy with and without pre-trained parameters as initialization. The performance drop can be significantly mitigated by using pre-trained parameters.}
\label{FedAvg_AllModel_Pretrained_Accuracy}
\end{figure}

\subsection{Changes of Feature Variance Across Layers}

\begin{figure}[H]
\centering
\includegraphics[width=3.0in]{FedAvg_AllModel_Pretrained_Digits_Epochwise_NC1_NC1.pdf}
\caption{Changes in the normalized within-class variance of features across model layers for specific global rounds, with larger X-axis values indicating deeper layers. The model is trained on Digit-Five with multiple models that are initialized by parameters pre-trained on large-scaled datasets. The top half of the figure shows the normalized within-class variance, while the bottom half displays the relative change in variance before and after model aggregation.}
\label{FedAvg_AllModel_Pretrained_Digits_Epochwise_NC1_NC1}
\end{figure}

\begin{figure}[H]
\centering
\includegraphics[width=3.0in]{FedAvg_AllModel_Pretrained_Digits_Epochwise_NC1_between.pdf}
\caption{Changes in the normalized between-class variance of features across model layers for specific global rounds, with larger X-axis values indicating deeper layers. The model is trained on Digit-Five with multiple models that are initialized by parameters pre-trained on large-scaled datasets. The top half of the figure shows the normalized between-class variance, while the bottom half displays the relative change in variance before and after model aggregation.}
\label{FedAvg_AllModel_Pretrained_Digits_Epochwise_NC1_between}
\end{figure}

\begin{figure}[H]
\centering
\includegraphics[width=3.0in]{FedAvg_AllModel_Pretrained_Digits_Epochwise_NC1_trace_within.pdf}
\caption{Changes in the original unnormalized within-class variance of features across model layers for specific global rounds, with larger X-axis values indicating deeper layers. The model is trained on Digit-Five with multiple models that are initialized by parameters pre-trained on large-scaled datasets. The top half of the figure shows the original unnormalized within-class variance, while the bottom half displays the relative change in variance before and after model aggregation.}
\label{FedAvg_AllModel_Pretrained_Digits_Epochwise_NC1_trace_within}
\end{figure}

\begin{figure}[H]
\centering
\includegraphics[width=3.0in]{FedAvg_AllModel_Pretrained_Digits_Epochwise_NC1_trace_between.pdf}
\caption{Changes in the original unnormalized between-class variance of features across model layers for specific global rounds, with larger X-axis values indicating deeper layers. The model is trained on Digit-Five with multiple models that are initialized by parameters pre-trained on large-scaled datasets. The top half of the figure shows the original unnormalized between-class variance, while the bottom half displays the relative change in variance before and after model aggregation.}
\label{FedAvg_AllModel_Pretrained_Digits_Epochwise_NC1_trace_between}
\end{figure}

\begin{figure}[H]
\centering
\includegraphics[width=6.8in]{FedAvg_AllModel_Pretrained_PACS_Epochwise_NC1_NC1.pdf}
\caption{Changes in the normalized within-class variance of features across model layers for specific global rounds, with larger X-axis values indicating deeper layers. The model is trained on PACS with multiple models that are initialized by parameters pre-trained on large-scaled datasets. The top half of the figure shows the normalized within-class variance, while the bottom half displays the relative change in variance before and after model aggregation.}
\label{FedAvg_AllModel_Pretrained_PACS_Epochwise_NC1_NC1}
\end{figure}

\begin{figure}[H]
\centering
\includegraphics[width=6.8in]{FedAvg_AllModel_Pretrained_PACS_Epochwise_NC1_between.pdf}
\caption{Changes in the normalized between-class variance of features across model layers for specific global rounds, with larger X-axis values indicating deeper layers. The model is trained on PACS with multiple models that are initialized by parameters pre-trained on large-scaled datasets. The top half of the figure shows the normalized between-class variance, while the bottom half displays the relative change in variance before and after model aggregation.}
\label{FedAvg_AllModel_Pretrained_PACS_Epochwise_NC1_between}
\end{figure}

\begin{figure}[H]
\centering
\includegraphics[width=6.8in]{FedAvg_AllModel_Pretrained_PACS_Epochwise_NC1_trace_within.pdf}
\caption{Changes in the original unnormalized within-class variance of features across model layers for specific global rounds, with larger X-axis values indicating deeper layers. The model is trained on PACS with multiple models that are initialized by parameters pre-trained on large-scaled datasets. The top half of the figure shows the original unnormalized within-class variance, while the bottom half displays the relative change in variance before and after model aggregation.}
\label{FedAvg_AllModel_Pretrained_PACS_Epochwise_NC1_trace_within}
\end{figure}

\begin{figure}[H]
\centering
\includegraphics[width=6.8in]{FedAvg_AllModel_Pretrained_PACS_Epochwise_NC1_trace_between.pdf}
\caption{Changes in the original unnormalized between-class variance of features across model layers for specific global rounds, with larger X-axis values indicating deeper layers. The model is trained on PACS with multiple models that are initialized by parameters pre-trained on large-scaled datasets. The top half of the figure shows the original unnormalized between-class variance, while the bottom half displays the relative change in variance before and after model aggregation.}
\label{FedAvg_AllModel_Pretrained_PACS_Epochwise_NC1_trace_between}
\end{figure}


\begin{figure}[H]
\centering
\includegraphics[width=6.8in]{FedAvg_AllModel_Pretrained_DomainNet_Epochwise_NC1_NC1.pdf}
\caption{Changes in the normalized within-class variance of features across model layers for specific global rounds, with larger X-axis values indicating deeper layers. The model is trained on DomainNet with multiple models that are initialized by parameters pre-trained on large-scaled datasets. The top half of the figure shows the normalized within-class variance, while the bottom half displays the relative change in variance before and after model aggregation.}
\label{FedAvg_AllModel_Pretrained_DomainNet_Epochwise_NC1_NC1}
\end{figure}

\begin{figure}[H]
\centering
\includegraphics[width=6.8in]{FedAvg_AllModel_Pretrained_DomainNet_Epochwise_NC1_between.pdf}
\caption{Changes in the normalized between-class variance of features across model layers for specific global rounds, with larger X-axis values indicating deeper layers. The model is trained on DomainNet with multiple models that are initialized by parameters pre-trained on large-scaled datasets. The top half of the figure shows the normalized between-class variance, while the bottom half displays the relative change in variance before and after model aggregation.}
\label{FedAvg_AllModel_Pretrained_DomainNet_Epochwise_NC1_between}
\end{figure}

\begin{figure}[H]
\centering
\includegraphics[width=6.8in]{FedAvg_AllModel_Pretrained_DomainNet_Epochwise_NC1_trace_within.pdf}
\caption{Changes in the unnormalized within-class variance of features across model layers for specific global rounds, with larger X-axis values indicating deeper layers. The model is trained on DomainNet with multiple models that are initialized by parameters pre-trained on large-scaled datasets. The top half of the figure shows the original unnormalized within-class variance, while the bottom half displays the relative change in variance before and after model aggregation.}
\label{FedAvg_AllModel_Pretrained_DomainNet_Epochwise_NC1_trace_within}
\end{figure}

\begin{figure}[H]
\centering
\includegraphics[width=6.8in]{FedAvg_AllModel_Pretrained_DomainNet_Epochwise_NC1_trace_between.pdf}
\caption{Changes in the unnormalized between-class variance of features across model layers for specific global rounds, with larger X-axis values indicating deeper layers. The model is trained on DomainNet with multiple models that are initialized by parameters pre-trained on large-scaled datasets. The top half of the figure shows the original unnormalized between-class variance, while the bottom half displays the relative change in variance before and after model aggregation.}
\label{FedAvg_AllModel_Pretrained_DomainNet_Epochwise_NC1_trace_between}
\end{figure}

\subsection{Changes of Feature Variance Across Training Rounds}

\begin{figure}[H]
\centering
\includegraphics[width=3.0in]{FedAvg_AllModel_Pretrained_Digits_Layerwise_NC1_NC1.pdf}
\caption{Changes in the normalized within-class variance of features across FL training at specific model layers. The model is trained on Digit-Five with multiple models that are initialized by parameters pre-trained on large-scaled datasets. The top half of the figure shows the original normalized within-class variance, while the bottom half displays the relative change in variance before and after model aggregation.}
\label{FedAvg_AllModel_Pretrained_Digits_Layerwise_NC1_NC1}
\end{figure}

\begin{figure}[H]
\centering
\includegraphics[width=3.0in]{FedAvg_AllModel_Pretrained_Digits_Layerwise_NC1_between.pdf}
\caption{Changes in the normalized between-class variance of features across FL training at specific model layers. The model is trained on Digit-Five with multiple models that are initialized by parameters pre-trained on large-scaled datasets. The top half of the figure shows the normalized within-class variance, while the bottom half displays the relative change in variance before and after model aggregation.}
\label{FedAvg_AllModel_Pretrained_Digits_Layerwise_NC1_between}
\end{figure}

\begin{figure}[H]
\centering
\includegraphics[width=3.0in]{FedAvg_AllModel_Pretrained_Digits_Layerwise_NC1_trace_within.pdf}
\caption{Changes in the original unnormalized within-class variance of features across FL training at specific model layers. The model is trained on Digit-Five with multiple models that are initialized by parameters pre-trained on large-scaled datasets. The top half of the figure shows the original unnormalized within-class variance, while the bottom half displays the relative change in variance before and after model aggregation.}
\label{FedAvg_AllModel_Pretrained_Digits_Layerwise_NC1_trace_within}
\end{figure}

\begin{figure}[H]
\centering
\includegraphics[width=3.0in]{FedAvg_AllModel_Pretrained_Digits_Layerwise_NC1_trace_between.pdf}
\caption{Changes in the original unnormalized between-class variance of features across FL training at specific model layers. The model is trained on Digit-Five with multiple models that are initialized by parameters pre-trained on large-scaled datasets. The top half of the figure shows the original unnormalized between-class variance, while the bottom half displays the relative change in variance before and after model aggregation.}
\label{FedAvg_AllModel_Pretrained_Digits_Layerwise_NC1_trace_between}
\end{figure}

\begin{figure}[H]
\centering
\includegraphics[width=6.8in]{FedAvg_AllModel_Pretrained_PACS_Layerwise_NC1_NC1.pdf}
\caption{Changes in the normalized within-class variance of features across FL training at specific model layers. The model is trained on PACS with multiple models that are initialized by parameters pre-trained on large-scaled datasets. The top half of the figure shows the normalized within-class variance, while the bottom half displays the relative change in variance before and after model aggregation.}
\label{FedAvg_AllModel_Pretrained_PACS_Layerwise_NC1_NC1}
\end{figure}

\begin{figure}[H]
\centering
\includegraphics[width=6.8in]{FedAvg_AllModel_Pretrained_PACS_Layerwise_NC1_between.pdf}
\caption{Changes in the normalized between-class variance of features across FL training at specific model layers. The model is trained on PACS with multiple models that are initialized by parameters pre-trained on large-scaled datasets. The top half of the figure shows the normalized between-class variance, while the bottom half displays the relative change in variance before and after model aggregation.}
\label{FedAvg_AllModel_Pretrained_PACS_Layerwise_NC1_between}
\end{figure}

\begin{figure}[H]
\centering
\includegraphics[width=6.8in]{FedAvg_AllModel_Pretrained_PACS_Layerwise_NC1_trace_within.pdf}
\caption{Changes in the original unnormalized within-class variance of features across FL training at specific model layers. The model is trained on PACS with multiple models that are initialized by parameters pre-trained on large-scaled datasets. The top half of the figure shows the original unnormalized within-class variance, while the bottom half displays the relative change in variance before and after model aggregation.}
\label{FedAvg_AllModel_Pretrained_PACS_Layerwise_NC1_trace_within}
\end{figure}

\begin{figure}[H]
\centering
\includegraphics[width=6.8in]{FedAvg_AllModel_Pretrained_PACS_Layerwise_NC1_trace_between.pdf}
\caption{Changes in the original unnormalized between-class variance of features across FL training at specific model layers. The model is trained on PACS with multiple models that are initialized by parameters pre-trained on large-scaled datasets. The top half of the figure shows the original unnormalized between-class variance, while the bottom half displays the relative change in variance before and after model aggregation.}
\label{FedAvg_AllModel_Pretrained_PACS_Layerwise_NC1_trace_between}
\end{figure}

\begin{figure}[H]
\centering
\includegraphics[width=6.8in]{FedAvg_AllModel_Pretrained_DomainNet_Layerwise_NC1_NC1.pdf}
\caption{Changes in the normalized within-class variance of features across FL training at specific model layers. The model is trained on DomainNet with multiple models that are initialized by parameters pre-trained on large-scaled datasets. The top half of the figure shows the normalized within-class variance, while the bottom half displays the relative change in variance before and after model aggregation.}
\label{FedAvg_AllModel_Pretrained_DomainNet_Layerwise_NC1_NC1}
\end{figure}

\begin{figure}[H]
\centering
\includegraphics[width=6.8in]{FedAvg_AllModel_Pretrained_DomainNet_Layerwise_NC1_between.pdf}
\caption{Changes in the normalized between-class variance of features across FL training at specific model layers. The model is trained on DomainNet with multiple models that are initialized by parameters pre-trained on large-scaled datasets. The top half of the figure shows the normalized between-class variance, while the bottom half displays the relative change in variance before and after model aggregation.}
\label{FedAvg_AllModel_Pretrained_DomainNet_Layerwise_NC1_between}
\end{figure}

\begin{figure}[H]
\centering
\includegraphics[width=6.8in]{FedAvg_AllModel_Pretrained_DomainNet_Layerwise_NC1_trace_within.pdf}
\caption{Changes in the original unnormalized within-class variance of features across FL training at specific model layers. The model is trained on DomainNet with multiple models that are initialized by parameters pre-trained on large-scaled datasets. The top half of the figure shows the original unnormalized within-class variance, while the bottom half displays the relative change in variance before and after model aggregation.}
\label{FedAvg_AllModel_Pretrained_DomainNet_Layerwise_NC1_trace_within}
\end{figure}

\begin{figure}[H]
\centering
\includegraphics[width=6.8in]{FedAvg_AllModel_Pretrained_DomainNet_Layerwise_NC1_trace_between.pdf}
\caption{Changes in the original unnormalized between-class variance of features across FL training at specific model layers. The model is trained on DomainNet with multiple models that are initialized by parameters pre-trained on large-scaled datasets. The top half of the figure shows the original unnormalized between-class variance, while the bottom half displays the relative change in variance before and after model aggregation.}
\label{FedAvg_AllModel_Pretrained_DomainNet_Layerwise_NC1_trace_between}
\end{figure}

\subsection{Changes of Alignment Across Layers}

\begin{figure}[H]
\centering
\includegraphics[width=3.0in]{FedAvg_AllModel_Pretrained_Digits_Epochwise_NC3_NC3.pdf}
\caption{Changes in the alignment between features and parameters across model layers for specific global rounds, with larger X-axis values indicating deeper layers. The model is trained on Digit-Five with multiple models that are initialized by parameters pre-trained on large-scaled datasets. The top half of the figure shows the original alignment values between features and parameters, while the bottom half displays the relative change in alignment values before and after model aggregation.}
\label{FedAvg_AllModel_Pretrained_Digits_Epochwise_NC3_NC3}
\end{figure}

\begin{figure}[H]
\centering
\includegraphics[width=6.8in]{FedAvg_AllModel_Pretrained_PACS_Epochwise_NC3_NC3.pdf}
\caption{Changes in the alignment between features and parameters across model layers for specific global rounds, with larger X-axis values indicating deeper layers. The model is trained on PACS with multiple models that are initialized by parameters pre-trained on large-scaled datasets. The top half of the figure shows the original alignment values between features and parameters, while the bottom half displays the relative change in alignment before and after model aggregation.}
\label{FedAvg_AllModel_Pretrained_PACS_Epochwise_NC3_NC3}
\end{figure}

\begin{figure}[H]
\centering
\includegraphics[width=6.8in]{FedAvg_AllModel_Pretrained_DomainNet_Epochwise_NC3_NC3.pdf}
\caption{Changes in the alignment between features and parameters across model layers for specific global rounds, with larger X-axis values indicating deeper layers. The model is trained on DomainNet with multiple models that are initialized by parameters pre-trained on large-scaled datasets. The top half of the figure shows the original alignment values between features and parameters, while the bottom half displays the relative change in alignment before and after model aggregation.}
\label{FedAvg_AllModel_Pretrained_DomainNet_Epochwise_NC3_NC3}
\end{figure}


\subsection{Changes of Alignment Across Training Rounds}

\begin{figure}[H]
\centering
\includegraphics[width=3.0in]{FedAvg_AllModel_Pretrained_Digits_Layerwise_NC3_NC3.pdf}
\caption{Changes in the alignment between features and parameters across FL training at specific model layers. The model is trained on Digit-Five with multiple models that are initialized by parameters pre-trained on large-scaled datasets. The top half of the figure shows the original alignment values between features and parameters, while the bottom half displays the relative change in alignment before and after model aggregation.}
\label{FedAvg_AllModel_Pretrained_Digits_Layerwise_NC3_NC3}
\end{figure}

\begin{figure}[H]
\centering
\includegraphics[width=6.8in]{FedAvg_AllModel_Pretrained_PACS_Layerwise_NC3_NC3.pdf}
\caption{Changes in the alignment between features and parameters across FL training at specific model layers. The model is trained on PACS with multiple models that are initialized by parameters pre-trained on large-scaled datasets. The top half of the figure shows the original alignment values between features and parameters, while the bottom half displays the relative change in alignment before and after model aggregation.}
\label{FedAvg_AllModel_Pretrained_PACS_Layerwise_NC3_NC3}
\end{figure}

\begin{figure}[H]
\centering
\includegraphics[width=6.8in]{FedAvg_AllModel_Pretrained_DomainNet_Layerwise_NC3_NC3.pdf}
\caption{Changes in the alignment between features and parameters across FL training at specific model layers. The model is trained on DomainNet with multiple models that are initialized by parameters pre-trained on large-scaled datasets. The top half of the figure shows the original alignment values between features and parameters, while the bottom half displays the relative change in alignment before and after model aggregation.}
\label{FedAvg_AllModel_Pretrained_DomainNet_Layerwise_NC3_NC3}
\end{figure}

\subsection{Changes of Stable Rank Across Layers}
\begin{figure}[H]
\centering
\includegraphics[width=3.0in]{FedAvg_AllModel_Pretrained_Digits_Epochwise_stable_rank.pdf}
\caption{Changes in the stable rank of features across model layers for specific global rounds, with larger X-axis values indicating deeper layers. The model is trained on Digit-Five with multiple models that are initialized by parameters pre-trained on large-scaled datasets. The top half of the figure shows the original stable rank of features, while the bottom half displays the relative change in stable rank before and after model aggregation.}
\label{FedAvg_AllModel_Pretrained_Digits_Epochwise_stable_rank}
\end{figure}

\begin{figure}[H]
\centering
\includegraphics[width=6.8in]{FedAvg_AllModel_Pretrained_PACS_Epochwise_stable_rank.pdf}
\caption{Changes in the stable rank of features across model layers for specific global rounds, with larger X-axis values indicating deeper layers. The model is trained on PACS with multiple models that are initialized by parameters pre-trained on large-scaled datasets. The top half of the figure shows the original stable rank of features and parameters, while the bottom half displays the relative change in stable rank before and after model aggregation.}
\label{FedAvg_AllModel_Pretrained_PACS_Epochwise_stable_rank}
\end{figure}

\begin{figure}[H]
\centering
\includegraphics[width=6.8in]{FedAvg_AllModel_Pretrained_DomainNet_Epochwise_stable_rank.pdf}
\caption{Changes in the stable rank of features across model layers for specific global rounds, with larger X-axis values indicating deeper layers. The model is trained on PACS with multiple models that are initialized by parameters pre-trained on large-scaled datasets. The top half of the figure shows the stable rank of features, while the bottom half displays the relative change in stable rank before and after model aggregation.}
\label{FedAvg_AllModel_Pretrained_DomainNet_Epochwise_stable_rank}
\end{figure}


\subsection{Changes of Stable Rank Across Training Rounds}
\begin{figure}[H]
\centering
\includegraphics[width=3.0in]{FedAvg_AllModel_Pretrained_Digits_Layerwise_stable_rank.pdf}
\caption{Changes in the stable rank of features across FL training at specific model layers. The model is trained on Digit-Five with multiple models that are initialized by parameters pre-trained on large-scaled datasets. The top half of the figure shows the original stable rank of features, while the bottom half displays the relative change in stable rank before and after model aggregation.}
\label{FedAvg_AllModel_Pretrained_Digits_Layerwise_stable_rank}
\end{figure}

\begin{figure}[H]
\centering
\includegraphics[width=6.8in]{FedAvg_AllModel_Pretrained_PACS_Layerwise_stable_rank.pdf}
\caption{Changes in the stable rank of features across FL training at specific model layers. The model is trained on PACS with multiple models that are initialized by parameters pre-trained on large-scaled datasets. The top half of the figure shows the original stable rank of features, while the bottom half displays the relative change in stable rank before and after model aggregation.}
\label{FedAvg_AllModel_Pretrained_PACS_Layerwise_stable_rank}
\end{figure}

\begin{figure}[H]
\centering
\includegraphics[width=6.8in]{FedAvg_AllModel_Pretrained_DomainNet_Layerwise_stable_rank.pdf}
\caption{Changes in the stable rank of features across FL training at specific model layers. The model is trained on DomainNet with multiple models that are initialized by parameters pre-trained on large-scaled datasets. The top half of the figure shows the original stable rank of features, while the bottom half displays the relative change in stable rank before and after model aggregation.}
\label{FedAvg_AllModel_Pretrained_DomainNet_Layerwise_stable_rank}
\end{figure}

\subsection{Visualization of Pre-aggregated and Post-aggregated Features}
\begin{figure}[H]
\centering
\includegraphics[width=6.8in]{TSNE_DomainNet_quickdraw_ResNet18_Pretrained_10Layers.pdf}
\caption{T-SNE visualization of features at different layers on the `Quickdraw' domain of DomainNet before and after aggregation.
The features are extracted from ResNet18 in the final global round of FL training, whose parameters are initialized by the parameters pre-trained on large-scaled datasets at the beginning.}
\label{TSNE_DomainNet_quickdraw_ResNet18_Pretrained_10Layers}
\end{figure}

\begin{figure}[H]
\centering
\includegraphics[width=6.8in]{TSNE_DomainNet_quickdraw_ResNet34_Pretrained_10Layers.pdf}
\caption{T-SNE visualization of features at different layers on the `Quickdraw' domain of DomainNet before and after aggregation.
The features are extracted from ResNet34 in the final global round of FL training, whose parameters are initialized by the parameters pre-trained on large-scaled datasets at the beginning.}
\label{TSNE_DomainNet_quickdraw_ResNet34_Pretrained_10Layers}
\end{figure}


\end{document}


% This document was modified from the file originally made available by
% Pat Langley and Andrea Danyluk for ICML-2K. This version was created
% by Iain Murray in 2018, and modified by Alexandre Bouchard in
% 2019 and 2021 and by Csaba Szepesvari, Gang Niu and Sivan Sabato in 2022.
% Modified again in 2023 and 2024 by Sivan Sabato and Jonathan Scarlett.
% Previous contributors include Dan Roy, Lise Getoor and Tobias
% Scheffer, which was slightly modified from the 2010 version by
% Thorsten Joachims & Johannes Fuernkranz, slightly modified from the
% 2009 version by Kiri Wagstaff and Sam Roweis's 2008 version, which is
% slightly modified from Prasad Tadepalli's 2007 version which is a
% lightly changed version of the previous year's version by Andrew
% Moore, which was in turn edited from those of Kristian Kersting and
% Codrina Lauth. Alex Smola contributed to the algorithmic style files.
