\section{Related Work}
\textbf{Federated Learning.} 
FedAvg \cite{FedAvg} is widely recognized as the pioneering work in FL. It facilitates collaborative model training by iteratively performing independent training on each client and aggregating the models on the server. While FedAvg demonstrates satisfactory performance in IID settings \cite{stich2018local, woodworth2020local}, a performance drop is commonly observed when data distributions are non-IID. This phenomenon has led to multiple explanations for the degradation, such as `client drift' \cite{zhao2018federated,SCAFFOLD} and `knowledge forgetting' \cite{pFedSD}.
To address the challenges posed by non-IID data, various solutions have been proposed, including local model regularization \cite{FedProx}, correction techniques \cite{SCAFFOLD,pFedGF}, knowledge distillation \cite{pFedSD,FedNTD}, and partial parameter personalization \cite{FedBN,FedRep,FedPer,LG-FedAvg,PartialFed,ChannelFed,FedCAC,FedDecomp}. However, these methods generally adhere to the standard framework of local training and global aggregation introduced by FedAvg. There remains a lack of in-depth exploration into how model aggregation intuitively impacts model training from a feature extraction perspective.

\textbf{Federated Learning within Feature Space.} 
Numerous studies have recently observed that heterogeneous data can lead to suboptimal feature extraction in FL models and are working on improving FL models by directly calibrating the resulting suboptimal feature spaces. 
A common line of research attributes performance degradation to inconsistent feature spaces across clients. 
To tackle this problem, CCVR \cite{CCVR} introduces a post-calibration strategy that fine-tunes the classifier after FL training using virtual features generated by an approximate Gaussian Mixture Model (GMM).
Several methods \cite{FedProto, AlignFed, AlignFed1, FedFA, FPL} propose using prototypes to align feature distributions across different clients, ensuring a consistent feature space. Studies such as FedBABU \cite{FedBABU}, SphereFed \cite{SphereFed}, and FedETF \cite{FedETF} utilize various fixed classifiers (e.g., random or orthogonal initialization) as targets to align features across clients.
In addition to inconsistent feature spaces, FedDecorr \cite{FedDecorr_ICLR,FedDecorr_TPAMI} observed that heterogeneous data leads to severe dimensional collapse in the global model. 
To combat this, it applies a regularization term based on the Frobenius norm of the correlation matrix during local training, encouraging the different dimensions of features to remain uncorrelated. 
FedPLVM \cite{FedPLVM} discovers differences in domain-specific feature variance in cross-domain FL. 
Consequently, they propose dual-level prototype clustering that adeptly captures variance information and addresses the aforementioned problem.
However, these studies primarily focus on calibrating features in the penultimate layer while overlooking intermediate features during feature extraction.
In this paper, we present a layer-peeled analysis of how model aggregation affects the feature extraction process and identify several issues related to the hierarchical topology of DNNs.

\textbf{Feature Learning in DNNs.} Advanced DNNs are typically structured with hierarchical layers, enabling them to efficiently and automatically extract informative features from raw data \cite{krizhevsky2012imagenet, allen2023backward, wang2023understanding}. Numerous efforts have been made to understand how DNNs transform raw data from shallow to deep layers. A commonly accepted view is that DNNs initially extract transferable universal features and then progressively filter out irrelevant information to form task-specific features \cite{yosinski2014transferable, zeiler2014visualizing, evci2022head2toe, kumar2022finetuning}. Another line of studies demonstrates that DNNs progressively learn features that are compressed within classes and discriminative between classes. Specifically, \cite{alain2017understanding} observes that the linear separability of features increases as the layers become deeper, using a linear probe. \cite{masarczyk2024tunnel} proposes the tunnel effect hypothesis, which states that the initial layers create linearly separable features, while the subsequent layers (referred to as the tunnel) compress these features. Meanwhile, certain research has extended the feature analysis of neural collapse (NC)—originally observed in the penultimate layers \cite{NC}—to intermediate layers \cite{ansuini2019intrinsic, rangamani2023feature, li2024understanding}, showing that the features of each layer exhibit gradual collapse as layer depth increases. \cite{wang2023understanding} provides a theoretical analysis of feature evolution across depth based on deep linear networks (DLNs). Although these studies focus on centralized training where there is no model aggregation during training, They provide solid support for us to effectively analyze feature evolution in FL, especially the influence of model aggregation on feature extraction.