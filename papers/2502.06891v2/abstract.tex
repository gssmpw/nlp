\begin{abstract}











Drug optimization has become increasingly crucial in light of fast-mutating virus strains and drug-resistant cancer cells. Nevertheless, it remains challenging as it necessitates retaining the beneficial properties of the original drug while simultaneously enhancing desired attributes beyond its scope. 
In this work, we aim to tackle this challenge by introducing \algname, a novel \fix{Generative Pretrained Transformer (GPT)} designed for drug optimization based on molecular scaffolds. Our work comprises three key components:
(1) A three-stage drug optimization approach that integrates pretraining, finetuning, and decoding optimization.
(2) A uniquely designed two-phase incremental training approach for pre-training the drug optimization GPT on molecule scaffold with enhanced performance.
(3) A token-level decoding optimization strategy, \TOPN, that enabling controlled, reward-guided generation using pretrained/finetuned GPT.
We demonstrate via a comprehensive evaluation on COVID and cancer benchmarks that \algname outperforms the competing baselines in drug optimization benchmarks, while excelling in preserving original functional scaffold and enhancing  desired properties.






\end{abstract} 



