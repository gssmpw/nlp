\section{Preliminaries}

In the following sections, we detail MDP, \fix{Language Model} (LM) and drug discovery, complete with their mathematical notations, and integrate them within the framework of Markov decision processes.



\textbf{Markov decision processes.}
Let us define a finite-horizon Markov decision process (MDP)~\citep{puterman2014markov} $\MDP_0 = \langle \stateSpace, \actionSpace,\horizon,\transDynamics,\RFunc \rangle$.
In this context, $\stateSpace$ represents a finite set of states, while $\actionSpace$ comprises a finite set of actions. The term $\horizon$ denotes the planning horizon. The function $\transDynamics$, defined as $\transDynamics: \stateSpace \times \actionSpace \rightarrow \stateSpace'$, describes the deterministic transition dynamics that combine a state $\state$ with an action $\action$, with an episode concluding once the agent executes the termination action. Additionally, the reward function $\RFunc: \stateSpace \times \actionSpace \rightarrow \mathbb{R}$ assigns scores exclusively to complete molecules, assigning a reward of 0\SJ{depends on results / which run/hyperparam to use, will check on weekends} to partial molecules.
The effectiveness of a policy can be evaluated using the $\QFunc$-value function, denoted as $\QFunc^{\policy}: \stateSpace \times \actionSpace \rightarrow \mathbb{R}$, and defined by the following equation:
$\QFunc^{\policy}(\state, \action) := \mathbb{E}^{\policy}\left[\sum_{t=0}^{\horizon}\RFunc(\state_t, \action_t) \mid \state_0 = \state, \action_0 = \action\right],$
where the expectation is based on the trajectory determined by the policy $\policy$. The associated value function is given by:
$\VFunc^{\policy}(\state) := \mathbb{E}_{\action \sim \policy(\cdot|\state)}\left[\QFunc^{\policy}(\state, \action)\right]$.




\textbf{LM.} We define the state space $\stateSpace$ as the set of all possible molecule, where each molecule is represented as a state $\state$ that includes a start token $\bracket{\text{BOS}}$, a molecule with SMILES~\citep{weininger1988smiles} representation string, and a termination action $\bracket{\text{EOS}}$. 
We define the set of complete molecules as
\begin{equation}
    \hypothesis_{\horizon} := \curlybracket{\text{[BOS]} \circ \mathbf{v} \circ \text{[EOS]}~|~\mathbf{v}\in \mathcal{V}^*},
\end{equation}
where $\hypothesis_t \subseteq \stateSpace_t|_{t\in \bracket{\horizon}}$ represents the hypothesis space at step $t$ (sequence length $t$), $\mathcal{V}^*$ represents the Kleene closure of Transformer's vocabulary $\mathcal{V}$, with  $\mathcal{V}:=\actionSpace$, and $\circ$ indicating string concatenation. 
Each action $\action \in \actionSpace$ is represented as token $y \in \mathcal{V}$.
In this work, we train a GPT policy $\policy_{\theta}$ to acquire prior knowledge for generating valid molecules based on a given set of molecules $\Buffer$.
We define the generator policy $\policy_{\theta}$, with learned weights $\theta$, as the product of probability distributions: $\policy_{\theta}\paren{\mathbf{y}|\mathbf{x}}=\prod_{t=1}^{|\mathbf{y}|} \policy_{\theta}\paren{y_t|\mathbf{x},\mathbf{y}_{<t}}$, 
where  $\policy_{\theta}\paren{\cdot|\mathbf{x},\mathbf{y}_{<t}}$ is a distribution,  $\mathbf{x}$ is an input sequence, and $\mathbf{y}_{<1}=y_0:=\text{[BOS]}$.
The decoding process in text generation involves identifying the most likely hypothesis by optimizing the objective:
$\mathbf{y}^{\star}=\argmax_{\mathbf{y}\in \hypothesis_{\horizon}} \log \policy_{\theta}\paren{\mathbf{y}|\mathbf{x}}.$













\textbf{Drug Optimization.} Given an initial drug candidate $X=\paren{x_1,\cdots,x_{\horizon}}$ and a drug optimization policy $\policy_{\theta}$, the goal in drug optimization is to find the optimal policy $\policy_{\theta^*}$ that
maximize the following objective:
\begin{equation}\label{eq:drug_generation_objective}
\policy_{\theta^*}=
\argmax_{\policy_{\theta}}
\expctover{X \sim {\stateDist_0}}{\RFunc(Y)-\RFunc\paren{X}|\theta,X}, 
\end{equation}
where 
 $Y=\policy_{\theta}\paren{\cdot\given X}, Y_{1:\horizon}=\paren{y_1,\ldots,y_t,\ldots,y_{\horizon}},y_t\in \mathcal{V}.$










\textbf{Limitation of previous works:} 
DrugImprover, which utilizes LSTM networks, has limitations in scalability, capacity, and contextual understanding, especially when compared to versions that use GPT Models. 
The current state of the art, REINVENT 4, employs the Transformer architecture; however, {it mainly focuses on pretraining with constrained similarity, which restricts its capability to explore molecular spaces that might offer high rewards beyond its training set.}
In this work, we address these limitations by proposing \algname. 




