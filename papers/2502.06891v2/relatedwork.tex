\section{Related Work}
\label{sec:related}





































\subsection{Reinforcement Learning and Finetuning}





Reinforcement learning~\citep{tan2022reinforcement}  has become a fundamental strategy in drug design~\citep{born2021paccmannrl, guimaraes2017objective, neil2018exploring, olivecrona2017molecular, popova2018deep,  staahl2019deep, tan2022drlinker, wang2022reinforcement, zhang2023universal, zhou2019optimization}, focusing on optimizing rewards that aggregate predicted property scores from various pharmaceutical predictors. Traditional reinforcement learning methods in drug discovery often neglected molecular structure constraints, leading to significant structural changes and unsynthesizable compounds. Our research differs by refining existing drugs to enhance their attributes without redesigning them from scratch and using reinforcement learning to improve a pre-trained language model generator, rather than starting a new. We also incorporate methods like Advantage-aligned Policy Optimization (APO)~\citep{liu2023drugimprover}, which assigns rewards based on advantage preference over the original molecule, to fine-tune a Transformer model, ensuring it aligns with multiple pharmaceutical objectives while preserving molecular structure. This approach, which includes controllable decoding, refines the model beyond traditional reinforcement learning fine-tuning stages.

























\subsection{Planning with GPT Models}

Several studies have leveraged planning algorithms to enhance text outputs for a variety of NLP tasks. These include approaches like beam search optimization, machine translation improvements~\citep{scialom2021beam,leblond2021machine,chaffin2021ppl}. The PG-TD~\citep{zhang2023planning} method is tailored for code generation using a singular reward function, whereas ERP~\citep{liu2024erp} introduces a novel concept by considering the certainty of each generated token along with an \emph{e}-step forward entropy measurement to gauge potential outcomes. It has been shown that ERP effectively balances exploration and exploitation within molecular structures, leading to the discovery of high-reward molecules. Unlike previous studies that focus solely on planning with pre-trained language models, our approach incorporates a novel decoding optimization as a critical final step in the algorithm. Moreover, our focus is on optimizing an existing drug rather than creating a De Novo one from the ground up.








\subsection{Drug Optimization}



Recent drug optimization efforts have focused primarily on a limited array of drug properties while often disregarding the docking score, an essential metric for evaluating structural compatibility with a target~\citep{zhou2019optimization, erikawa2021mermaid}. DrugEx v3~\citep{liu2023drugex} seeks to resolve this deficiency by using 3D molecular graphs that encapsulate more comprehensive data such as chemical valence rules. Nevertheless, this method's complexity poses challenges in generating molecules that closely resemble the originals. The resulting lower similarity could account for the diminished efficacy of graph-based methods, as deviations from the original molecular structures result in the loss of vital chemical properties. Conversely, our approach manages to preserve a decent level of similarity. 
Diffusion-based efforts~\citep{alakhdar2024diffusion,morehead2024geometry} either focus on De Novo drug discovery while neglecting the essential similarity to the original molecule, or overlook the docking score towards a binding target.
Molsearch \citep{sun2022molsearch} is a Monte Carlo tree search (MCTS)-driven approach for molecular generation and optimization, 
and MIMOSA \citep{fu2021mimosa} is a GNN sampling-based method leveraging graph-based molecular optimization.
DrugImprover~\citep{liu2023drugimprover} effectively begins to redefine the drug optimization challenge by employing reinforcement learning with a mix of multiple objectives as rewards. Still, it employs an LSTM in its generative model, which faces limitations in scalability, capacity, and understanding context. In contrast, our method applies a Transformer as the primary generative model, enhanced further with Advantage-aligned Policy Optimization (APO) for exploratory purposes and an optimized decoder for superior performance.








\subsection{Language Models for Drug Discovery}




Large language models like MolGPT~\citep{bagal2021molgpt} and ChemGPT~\citep{frey2023neural} have been utilized in molecule generation and drug discovery~\citep{bagal2021molgpt,rothchild2021c5t5,wang2022transformer}, using formats like SMILES~\citep{weininger1988smiles} and SELFIES~\citep{krenn2020self} to standardize molecular representation. These models have shown promise in drug design, outperforming traditional methods in some predictive capacities. However, as noted by ~\citet{murakumo2023llm}, using pre-trained language models typically results in minor molecular modifications and serves primarily supportive roles in the design process. We adopt the SMILES format and combine the strengths of GPTs with reinforcement learning. Unlike previous efforts that relied solely on LLMs, our approach uses RL fine-tuning to align with multiple pharmaceutical objectives and employs controlled decoding to guide the GPT model in generating more effective molecular structures.

Further developments include transformer-based models like REINVENT 4 ~\citep{he2021molecular, he2022transformer, loeffler2024reinvent}, which mostly focuses on pretraining. While pretraining helps in generating molecules similar to those in the training dataset, it also inherently restricts the exploration scope due to biases in the training data.
REINVENT4 uses the original molecule as input, generates molecules that are very similar to the original ones. This likely contributes to its relatively poor performance in other metrics, as the generated molecules exhibit minimal changes in chemical properties. 
In contrast, our proposed method, which uses SMILES and scaffolds as prompts, achieves a well-balanced trade-off between diversity and similarity in the generated and original molecules, potentially leading to improved performance.