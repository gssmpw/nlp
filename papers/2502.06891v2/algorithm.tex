

\section{The \algname Algorithm}

\begin{algorithm}[t]
    \caption{\algname}\label{alg:scaffoldgpt}
    \begin{algorithmic}[1] 
    \Require { GPT-based generator policy $\Gen$; {critics $\mathbf{\critic}$}.}
    \State Initialize $\Gen$ with GPT2-like Transformer with random weight $\theta$.
    \LineComment{/* Stage 1: Pretrain GPT-based generator */}
    \State Build the training corpus $\corpus_\text{Phase 1}$ (\ref{eq:phase1_corpus}), $\corpus_\text{Phase 2}$ (\ref{eq:phase2_corpus}).
    \State Pre-train BPE tokenizer and $\Gen$ on  $\corpus_\text{Phase 1}$ via CLM objective~\eqref{eq:pretrain-loss}.
    \State Pre-train $\Gen$ on  $\corpus_\text{Phase 2}$.

    \LineComment{/* Stage 2: APO fine-tuning */}
    
    
    
    \For{$n=1,\ldots,\ENum$}
            \State $\state_0 \sim \rho_0$, { \text{where }$\rho_0\in \Delta\paren{\RationaleBuffer}$}.
            \State Generate $Y_{1:\horizon}=\paren{y_t,\ldots,y_{\horizon}}\sim \Gen\paren{\cdot|S}$.
            \State Compute advantage preference
            {$\RFunc^\text{AP}$}
            by \eqref{eq:reward}\eqref{eq:advantage_preference}.
            \State Update generator $\theta$
            via policy gradient by \eqref{eq:adv:preference}.
    \EndFor
    
    \LineComment{/* Stage 3: Token-level Decoding Optimization */}
    \State Optimize the generation of $\Gen$ via $\TOPN$ \eqref{eq:topN} decoding strategy.    
    \end{algorithmic}
\end{algorithm}








\textbf{Stage 1. Pretrain a GPT generator.} 
Let us note the GPT generator policy as $\policy_{\theta}$, which computes the probability $p$ of the occurrence of the $t^{th}$ token in a target molecule $Y$. It takes into account all preceding tokens $\mathbf{y}_{<t} = \bracket{y_1, ..., y_{t-1}}$ in the target, as well as the scaffold compound $S$, which is noted as
$\policy_{\theta}\paren{y_t \given \mathbf{y}_{<t},S}=p\paren{y_{t}|\mathbf{y}_{<t},S}.$
The parameters $\theta$ of the generator policy $\policy_{\theta}$ are trained using the training corpus set through the minimization of the negative log-likelihood (NLL) for the complete SMILES strings across the entire set.
This process is described as follows:
\begin{align}\label{eq:pretrain-loss}
    NLL =  - \log \Prob\paren{Y|S} 
    =-\sum_{t=1}^{\horizon} \log \Prob\paren{y_{t}|y_{t-1},...,y_{1},S}
   =-\sum_{t=1}^{\horizon} \log \policy_{\theta}\paren{y_t|y_{1:t-1}, S},
\end{align}
where $\horizon$ signifies the total number of tokens related to $Y$. The NLL quantifies the probability of converting a specific scaffold into a designated target molecule. 

In this project, we employ pre-training to harness large quantities of unlabeled text to construct a basic foundation model of language understanding. This foundation model can subsequently be fine-tuned and tailored to meet various specialized goals.
In this work, we propose a novel framework for pre-training a Transformer and linking scaffolds with complete molecules, based on a SMILES (Simplified Molecular Input Line Entry System) \citep{weininger1988smiles}  string representation of the molecule. Here, we define $\mathcal{S}$ as the scaffold space and $\mathcal{Y}$ as the target molecule space. With $\Pair=\curlybracket{\paren{s,y}|s,y\in \mathcal{S} \times \mathcal{Y}}$, we denote the set of scaffold and molecular pairs from $\mathcal{S}$ and $\mathcal{Y}$. In this notation, $s$ represents the scaffold of the target molecule, and $y$ is the corresponding target molecule.
We initially pre-trained our tokenizer using the Byte Pair Encoding (BPE) method \fix{\citep{gage1994new_bpe, sennrich2015neural_bpe}}.  
Building on the pre-trained BPE tokenizer, we propose a two-phase incremental training approach,
as illustrated in \figref{fig:drugimprover_framework}, to notably enhance the model's ability to improve the validity of inferring the target molecule from its scaffold.

\paragraph{Incremental training.} 
The rationale for incremental training is to conduct local optimization before embarking on global optimization. Therefore, we divide our training into two phases.
In the first phase, we focus on training a GPT exclusively for molecules using Causal Language Modeling (CLM). CLM utilizes an autoregressive method where the model is trained to predict the next token in a sequence by considering only the tokens that precede it. The phase 1 corpus is designed as follows:
\begin{figure*}[t]
        \centering
        \includegraphics[%
        width=13cm, 
        clip={0,0,0,0}]{figures/pretrain_.pdf}
    \caption{
    Two-phase incremental training of \algname. The first phase concentrates on recognizing the molecule, while the second phase builds connections between its scaffold and the original molecule.
    }
  \label{fig:drugimprover_framework}
\end{figure*}
\begin{equation}\label{eq:phase1_corpus}
\corpus_{\text{Phase 1}}=\curlybracket{[BOS],\tuple{L},\underbrace{y_1,\cdots,y_{\horizon}}_{\text{target molecule Y}},[EOS]}, \fix{\tuple{L} \text{is the token for ligand.}}
\end{equation}
In the second phase, building upon the success of the GPT model developed in phase 1, which demonstrated high accuracy in molecular generation, we advance the training by focusing on pairs of scaffolds and molecules using CLM. The phase 2 corpus is as follows:
\begin{align}\label{eq:phase2_corpus}
\corpus_{\text{Phase 2}}=\curlybracket{[BOS],\tuple{S},\underbrace{s_1,\cdots,s_{\horizon}}_{\text{source scaffold S}},\tuple{L},\underbrace{y_1,\cdots,y_{\horizon}}_{\text{target molecule Y}},[EOS]}, \fix{\tuple{S} \text{: scaffold.}}
\end{align}
Consequently, the model can generate the appropriate molecule when given a scaffold. However, since a single scaffold may correspond to multiple molecules, we further refine the GPT-based generator policy, $\policy_{\theta}$, to target specific outcomes by applying reinforcement learning finetuning in the next stage.










\textbf{Stage 2. RL finetuning.} 
Fine-tuning a generative model is crucial for producing outcomes that meet specific objectives. In this study, we use the Advantage-alignment Policy Optimization (APO)~\citep{liu2023drugimprover} algorithm to fine-tune the pretrained GPT-based generator policy $\policy_{\theta}$. This approach steers the model from a given scaffold towards the targeted molecule, simultaneously improving multiple properties. 


In this work, we adopt the define of reward function from \citet{liu2024erp}, and regarded each pharmaceutical property as a critic $C$ and got an ensemble critics $\mathbf{C}$ as follows:
\begin{align*}
[\critic^{\text{Druglikeness}},
\critic^{\text{Solubility}},
\critic^{\text{Synthesizability}}, 
{\critic^{\text{Docking}}},\critic^{\text{{Tanimoto}}}
],
\end{align*}
where each critic $\critic: Y \rightarrow \mathbb{R}$ {acts as a distinct evaluator for a specific pharmaceutical attribute}.
We built the reward function as follow:
\begin{align}\label{eq:reward1}
{\RFunc^{\text{norm}}\paren{Y}}:=
{\RFunc^{\text{norm}}\paren{
Y|S}} =\lambda\cdot\text{Norm}\paren{\critic^{\text{Tanimoto}}\paren{S,
Y}}
+
\sum_{i=0}^{|\mathbf{C}|-1}\lambda\cdot\text{Norm}\paren{\critic_i{\paren{Y}}}
\end{align}
where Norm is employed to standardize diverse attributes to a consistent scale. In this instance, Norm refers to the process of min-max normalization, which is used to adjust the attributes so they fit within the $\bracket{0,1}$ range. \fix{ We defer the details of critics to \secref{exp_config}.}

In this work, we use $\BON$~\citep{gao2023scaling} (Best of N) search to estimate the reward for a prompt (partial molecule). $\BON$ can be formulated as the following:
\begin{align}\label{eq:bon}
{\BON}&\paren{\mathbf{y}_{<i},N,\RFunc}|_{S,p,k}= \max_{\mathbf{Y}_{j}\in \curlybracket{\mathbf{Y}_1,\cdots,\mathbf{Y}_{N}}} \RFunc\paren{ \mathbf{Y}_{j}},\\
\text{where } Y_j=&\bracket{\mathbf{y}_{<i},y_i,\cdots,y_{\horizon}}_j, \text{and }
y_i \sim \TOPPK\paren{\mathbf{y}_{<i},p, k}|_{S}.\notag
\end{align}

\noindent where \TOPPK~\citep{liu2024erp} is defined as follows:
\begin{align}\label{eq:toppk}
    &\text{\TOPPK}\paren{\mathbf{y}_{<i}, p,k}|_{S}= \actionSpace_{\mathbf{y}_{<i}}, 
   \text{where~} \actionSpace_{\mathbf{y}_{<i}}=\curlybracket{y_1,\ldots,y_{j}}, y_i\in \mathcal{V},  \\ 
    & j= \min \curlybracket{\argmin_{j'}\sum_{i=1}^{j'}\policy_{\theta}\paren{y_i|S,\mathbf{y}_{<i}} \geq p, k},
    \text{and } \policy_{\theta}\paren{y_g|S,\mathbf{y}_{<i}}>\policy_{\theta}\paren{y_h|S,\mathbf{y}_{<i}}, \text{if } g<h, \notag
\end{align}
where $p\in (0,1]$ represents the maximum cumulative probability, and $k$ denotes the maximum number of candidates for the next tokens.
































For each pair consisting of a scaffold and a molecule, we create 8 new molecules from the scaffold using \TOPPK~\eqref{eq:toppk}  and \BON~\eqref{eq:bon} method to select the best one (the one with the highest normalized reward) to serve as the foundation for the final reward calculation.
\begin{align}\label{eq:reward}
{\RFunc_c\paren{Y|S}}= \RFunc^{\text{norm}}{\paren{Y}}|_{S}, ~~{Y} \in \BON\paren{y_0,N,\RFunc}|_{S,p,k}.
\end{align}
APO makes policy gradient based on the advantage preference~\citep{liu2023drugimprover}, which is defined as
\begin{equation}\label{eq:advantage_preference}
\RFunc^\text{AP}\paren{Y_{1:\horizon},S} ={\RFunc_c\paren{Y_{1:\horizon}} - \RFunc_c\paren{S}},
\end{equation}
and perform APO policy gradient with follows:
\begin{align}\label{eq:adv:preference}
     \gradient=& \textstyle \mathbb{E}_{S\sim {\rho_0},{Y_{1:\horizon}\sim \Gen\paren{\cdot|S}} } 
     \left[ \nabla_{\theta}\log \Gen\paren{Y_{1:\horizon}|S}
     \cdot \RFunc^\AP\paren{S,Y_{1:\horizon}}\right],
\end{align}
\paragraph{Stage 3. Token-level Controllable decoding generation.}\label{decoding_strategy}
Ultimately, the current GPT-based decoder focuses mainly on maximizing likelihood, neglecting specific metrics of interest. This approach limits its effectiveness in optimizing objectives that diverge from those in its training set, particularly in generating desired molecules. In this study, we introduce controllable decoding after fine-tuning with APO. We present a new approach, \TOPN, to direct the generation process towards enhancements in the optimization objective, as detailed below:
\begin{align}\label{eq:topN}
Y^{\star}&\sim\curlybracket{[\mathbf{y}_{<i},y_i,\cdots,y_{\horizon}]}, 
\text{~where~} y_i\sim \text{\TOPN}\paren{\mathbf{y}_{<i}, p,k, n}|_{S}  \\ \notag
    &\text{~\TOPN}\paren{\mathbf{y}_{<i}, p,k, n}|_{S}= \actionSpace_{\mathbf{y}_{<i}}, 
    \text{~} \actionSpace_{\mathbf{y}_{<i}}=\curlybracket{y_1,\ldots,y_{n}}, y_i\in \mathcal{V}, |\actionSpace|\leq k, \notag \\ \notag
    &\text{~and } 
    \RFunc\paren{\BON\paren{\mathbf{y}_{<i}\circ y_g,N,R}|_{S,p,k}}
    \geq\RFunc\paren{\BON\paren{\mathbf{y}_{<i}\circ y_h,N,R}|_{S,p,k}}, \forall g<h, \notag
\end{align}
where $n\leq k$ denotes as top $N$ candidate of next tokens with regard to $\BON$ function.




\begin{remark}
$\TOPN$ differs from $\TOPP$, $\TOPK$, and $\TOPPK$ in that it is measured based on maximum reward, whereas the others are measured based on maximum likelihood. $\TOPN$ is also distinct from $\BON$ in that $\BON$ is optimized at the sequence level, while $\TOPN$ is optimized at the token level.
\end{remark}








































































