\documentclass[10pt,journal]{IEEEtran} 

% \usepackage{comment}
% \usepackage{flushend}
%\usepackage[linesnumbered,boxed,ruled,commentsnumbered]{algorithm2e}
% \usepackage{url}
\usepackage{hyperref}
\usepackage{booktabs}
\usepackage[linesnumbered,ruled,vlined]{algorithm2e}
%\usepackage{algorithmicx}
\usepackage{mathtools, cases}
%\usepackage{algpseudocode}
\usepackage{textcomp}
%\excludecomment{toexclude} % you can name the comment as you wish
\usepackage{makecell}
% \includecomment{toexclude}
\usepackage{amsthm, enumitem}
\usepackage{amsmath,amsfonts,amssymb}
\usepackage{cite, url}
\usepackage{verbatim}
\usepackage{bm}
%\usepackage[document]{ragged2e}
%\usepackage{algorithm}
\usepackage{graphicx}
\usepackage{xcolor}
\usepackage{subcaption}
%\usepackage[pdftex]{graphicx}
\usepackage{epstopdf}
\usepackage{mathrsfs}
%\usepackage{amssymb}
\usepackage{txfonts}
%\usepackage{subfigure}
\usepackage{lineno}
\usepackage{multirow}
\usepackage{booktabs}
%\usepackage[dvips]{graphicx}
\usepackage{color}
\usepackage{multicol}
\usepackage{stfloats}
\usepackage{diagbox}
\usepackage{esint}
\usepackage{float}
\usepackage{algpseudocode}
\usepackage{empheq}
\usepackage{tabularx}

\usepackage{flushend}

\newtheorem{lemma}{Lemma}
\allowdisplaybreaks[4]

\newcommand{\abs}{\mathrm{abs}}
\newcommand{\xt}{\mathbf{x}}
\newcommand{\st}{\mathbf{s}}
\newcommand{\yt}{\mathbf{y}}
\newcommand{\av}{\mathbf{a}}
\newcommand{\w}{\mathbf{w}}
\newcommand{\W}{\mathbf{W}}
\newcommand{\yf}{\tilde{\mathbf{y}}}
\newcommand{\yfi}{\tilde{y}}
\newcommand{\Yf}{\tilde{\mathbf{Y}}}
\newcommand{\Yt}{\mathbf{Y}}
\newcommand{\A}{\mathbf{A}}
\newcommand{\at}{\mathbf{a}}
\newcommand{\Ht}{\mathbf{H}}
\newcommand{\hf}{\mathbf{h}}
\newcommand{\Phit}{\mathbf{\Phi}}
\newcommand{\phit}{\mathbf{\phi}}
\newcommand{\nt}{\mathbf{n}}
\newcommand{\Ft}{\mathbf{F}}
\newcommand{\Pt}{\mathbf{P}}

\newcommand{\I}{\mathbf{I}}
\newcommand{\Tr}{\text{Tr}}
\newcommand{\diag}{\text{diag}}
\newcommand{\blkdiag}{\text{blkdiag}}
\newcommand{\bgamma}{\bm{\gamma}}

\newtheorem{theorem}{Theorem}
\newtheorem{remark}{Remark}
% \newtheorem{lemma}{Lemma}
\newtheorem{proposition}{Proposition}
\newtheorem{corollary}{Corollary}
\newtheorem{definition}{Definition}
\newtheorem{conjecture}{Conjecture}
\newtheorem{exmple}{Example}
\newtheorem{note}{Note}
\newtheorem{assumption}{Assumption}
\renewcommand{\proofname}{\textbf{Proof}}

%\begin{document}, \textit{Graduate Student Member, IEEE}

\title{\huge Token Communications: A Unified Framework for Cross-modal Context-aware Semantic Communications}

\author{Li Qiao$^{*}$, Mahdi Boloursaz Mashhadi$^{*}$, \textit{Senior Member, IEEE}, Zhen Gao, \textit{Member, IEEE},\\ Rahim Tafazolli, \textit{Fellow, IEEE}, Mehdi Bennis,~\IEEEmembership{Fellow,~IEEE}, and Dusit Niyato,~\textit{Fellow, IEEE}
\thanks{
$^{*}$ Equal contribution. {\it (Corresponding authors: Zhen Gao, Rahim Tafazolli.)}}
\thanks{
Li Qiao and Zhen Gao are with the School of Information and Electronics, Beijing Institute of Technology, Beijing 100081, China (e-mails: \{qiaoli, gaozhen16\}@bit.edu.cn).
}
\thanks{
Li Qiao, Mahdi Boloursaz Mashhadi and Rahim Tafazolli are with 5GIC \& 6GIC, Institute for Communication Systems (ICS), University of Surrey, Guildford, United Kingdom (email: \{l.qiao, m.boloursazmashhadi, r.tafazolli\}@surrey.ac.uk). 
% The simulation codes will soon be available at https://github.com/liqiao19.
}
\thanks{
Mehdi Bennis is with the Centre for Wireless Communications, University of Oulu, 90014 Oulu, Finland (e-mail: mehdi.bennis@oulu.fi).
}
\thanks{
Dusit Niyato is with the School of
Computer Science and Engineering, Nanyang Technological University,
Singapore 639798 (e-mail: dniyato@ntu.edu.sg).
}
}

\vspace{-0.75cm}

\begin{document}

\maketitle

\vspace{-8mm}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{abstract}  
In this paper, we introduce \textit{token communications (TokCom)}, a unified framework to leverage cross-modal context information in generative semantic communications (GenSC). {\color{black}TokCom} is a new paradigm, motivated by the recent success of generative foundation models and multimodal large language models (GFM/MLLMs), where the communication units are \textit{tokens}, enabling efficient \textit{transformer-based token processing} at the transmitter and receiver. In this paper, we introduce the potential opportunities and challenges of leveraging context in GenSC, explore how to integrate GFM/MLLMs-based token processing into semantic communication systems to leverage cross-modal context effectively, present the key principles for efficient TokCom at various layers in future wireless networks. {\color{black}We demonstrate the corresponding TokCom benefits in a GenSC setup for image}, leveraging cross-modal context information, which increases the bandwidth efficiency by 70.8\% with negligible loss of semantic/perceptual quality. Finally, the potential research directions are identified to facilitate adoption of TokCom in future wireless networks.
\end{abstract}
\begin{IEEEkeywords}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Token communications, foundation models, multimodal large language models, generative semantic communications, transformers.
\end{IEEEkeywords}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Introduction}\label{sec:intro}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Existing wireless networks are mostly designed at the technical communication level, focusing on the reliable transmission of data over noisy channels at high throughput and low latency. This design often disregards the semantic meaning, goal, and context of the transmitted data, leading to communication inefficiencies, especially in unfavorable channel/network conditions and bandwidth-constrained mediums. However, in many emerging applications such as the wireless metaverse and immersive communications, e.g., extended/mixed reality (XR/MR), the internet of senses, and holographic teleportation, effective transmission of the most meaningful and relevant information to the communication goal/intent, is more crucial than character/pixel/sample-exact reconstruction of the source signals. This has led to extensive research on development of efficient \textit{semantic communication (SemCom)} systems in recent years \cite{SemCom1, SemCom2}, which are mostly empowered by artificial intelligence and machine learning (AI/ML)-assisted signal processing. More recently, generative AI (GenAI) models, e.g., diffusion models, GANs, and VAEs, have proven to significantly enhance communication at the semantic-level \cite{Liang2024generative, Wanting2024generative, Li2024generative}. In \textit{generative SemCom (GenSC)}, the semantics of interest are extracted at the transmitter, communicated over the channel, and then used at the receiver to guide a generative model to locally synthesize a semantically consistent signal with high fidelity. The recent success of powerful generative foundation models (GFMs) and multimodal large language models (MLLMs), e.g., PaLM-E, Sora, and GPT-4o, provides ample opportunities to develop ultra-low rate semantic communication systems. The pre-trained nature of such models and their applicability to a vast range of synthesis tasks, can revolutionize GenSC enabling intent/task-adaptive SemCom systems empowered by pre-trained GFM/MLLMs.

Despite the above advancements, integrating cross-modal context information into semantic communication systems remains less studied. In this paper, we introduce \textit{token communications (TokCom)}, a new GenSC framework that leverages tokens as units of semantic content transmitted within the future wireless networks. Tokens are compressed representations that capture meaning through features of information-rich multimodal data, enabling efficient semantic communications. TokCom is motivated by the recent advent of powerful GFM/MLLMs based on transformer neural networks (NNs), where tokens are the basic processing units of text, images, audio/video signals that may represent words, image patches, temporal audio slices, or video subframes. Tokenization allows complex multimodal data to be broken into manageable, interpretable segments to be processed with state-of-the-art GFM/MLLMs at the TokCom transmitter/receiver, as depicted in Fig. \ref{fig_Tokenization}. {\color{black}GFMs/MLLMs}-based token processing enables to encode the semantic content of multimodal signals at the transmitter and recover the corresponding semantics at the receiver leveraging the cross-modal context information. As an example, a corrupted TokCom packet may lead to an incomplete message like ``A beach with palm [MASK] and clear blue water" at the receiver. Here, the missing word is replaced by a [MASK] token. The semantic error correction mechanism in TokCom leverages a pre-trained LLM to predict the masked word based on the surrounding context, outputting, for example, “trees” to complete the sentence as, “A beach with palm trees and clear blue water" thereby avoiding packet re-transmissions that would be required without leveraging context information via the LLM.

\begin{figure*}[tb]
\vspace{-4mm}
\centerline{\includegraphics[scale=0.79]{fig1_0215.pdf}}
\captionsetup{font={footnotesize, color = {black}}, singlelinecheck = off, justification = raggedright,name={Fig.},labelsep=period}
\caption{The proposed token communications framework: Leveraging cross-modal context for efficient generative SemCom.}
\label{fig_Tokenization}
\vspace{-4mm}
\end{figure*}

This article aims to answer the following questions: \textit{Q1) What are the key potential opportunities and challenges of leveraging context via TokCom in GenSC? Q2) How to integrate transformer-based token processing via state-of-the-art GFM/MLLMs into SemCom systems to leverage cross-modal context effectively? Q3) What are the key principles and setups for efficient TokCom at various layers in future wireless networks?} The main contributions of this article include:
\begin{itemize}
    \item We introduce a novel TokCom framework along with its basic designs to leverage cross-modal context information in the semantic source compression, semantic channel coding, semantic multiple access, and semantic networking setups. TokCom integrates transformer-based next/masked token prediction via GFM/MLLMs into the transmitter/receiver pipeline to achieve cross-modal context-awareness. 
    \item We introduce and demonstrate a token-level loss/error mitigation scheme that leverages token likelihood estimation based on cross-modal context to predict and mitigate the tokens corrupted or lost in communication. For a generative image SemCom task, this method improves the bandwidth efficiency by 70.8\% at a negligible loss of visual quality by avoiding token retransmissions.
\end{itemize}



\section{Tokenization, and Embedding of Various Data Modalities} \label{SEC_Proposed}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
To process large multimodal data, it is first segmented into various chunks, each consisting of several \textit{tokens}. Tokens are then assigned unique IDs, and the set of all possible tokens makes a vocabulary. Each token ID is then mapped to a vector that represents the semantic meaning of the token in a dense, fixed-dimensional space, i.e., the \textit{embedding space}. The embedding vectors are learned via pre-training to capture both syntactic and semantic relationships between tokens. These embeddings allow multimodal language models process inputs more effectively by capturing token semantics as well as the relationships between different tokens, i.e., \textit{context}. A Tokenizer generates a large codebook of token embeddings that represent the whole corpus of data, e.g., colossal clean crawled corpus (C4) for text, LAION-5B for image-text pairs, and then the (pre-)trained MLLM essentially learns relations between tokens capturing the semantics and context. GFM/MLLMs typically learn a unified token vocabulary valid for all modalities where textual or visual tokens representing similar semantics are mapped to highly correlated vector embeddings. As an example, a vision language model, e.g., the popular contrastive language-image pretraining (CLIP) model \cite{radford2021learning}, learns through pre-training on large corpus of image-text pairs to relate the tokens of a “book” image with its corresponding text token. In the following, we illustrate these concepts for various data modalities.


{\bf Text Tokenization/Embedding:} In text, a token can represent a word, part of a word, or even a single character, depending on the tokenization model. Commonly, text is split into subwords to handle out-of-vocabulary words. For example, the sentence ``The supermarket is hosting a sale" would be tokenized as ``-The -super market -is -host ing -a -sale" using WordPiece. Tokenization algorithms like WordPiece, byte-pair encoding (BPE), and unigram language modeling split words into subwords, with each subword mapped to a unique token in the vocabulary. Tokens are then projected into a high-dimensional feature space using word embeddings. Early models like Word2Vec and GloVe used fixed embeddings, while modern transformer models such as bidirectional encoder representations from transformers (BERT) and GPT learn contextual embeddings, meaning the representation of a word like ``bank" varies depending on its context. For example, the word ``bank" will have different embeddings when used in the context of ``a financial institution" versus ``the edge of a river".

{\bf Image/Video Tokenization/Embedding:} {\color{black}Image tokens are created by dividing the image into fixed-size patches, which are then flattened and embedded. The challenge lies in balancing image fidelity and patch size; smaller patches can increase computational load, while larger ones may lose detail. For instance, vision transformer (ViT) uses $16 \times 16$ pixel patches. In addition to patch-based tokenization, vector quantized-variational autoencoder (VQ-VAE) tokenizes images or videos by encoding them into a discrete latent space, where each image is represented by a sequence of tokens from a codebook \cite{MaskGIT}. This enables more efficient representation and processing by transformer models.}



{\bf Audio Tokenization/Embedding:} Audio signals are typically tokenized by transforming the waveform into spectrograms, typically using techniques such as log Mel filterbanks, which capture both the temporal and frequency-based features of the audio signal. The spectrograms are then divided into overlapping patches, analogous to how images are patchified. Each patch is then flattened and projected into the embedding space to be processed by attention-based transformer models.

\begin{figure*}[tb]
\vspace{-4mm}
\centerline{\includegraphics[scale=0.83]{fig2.pdf}}
\captionsetup{font={footnotesize, color = {black}}, singlelinecheck = off, justification = raggedright,name={Fig.},labelsep=period}
\caption{Transformer-based context learning/prediction: (a) Unidirectional (next token prediction); (b) Bidirectional (masked token prediction).}
\label{fig_TokPred}
\vspace{-2mm}
\end{figure*}



\section{Tokens, Attention, and Transformers}\label{SEC_Proposed}

Tokens are commonly used in various vision and natural language processing (NLP) models to represent data in a structured, numerical format that models can process. While transformer models have popularized token-based processing due to their architecture, tokens have long been used in other types of models as well. Transformer-based models, e.g., ViT, BERT, and GPTs, revolutionized token processing by introducing a fundamentally different approach to handling token sequences based on the \textit{attention mechanism}. Instead of working token by token, the transformer processes the entire sequence of tokens in parallel. This parallelism allows for much faster training and inference, especially on modern hardware like GPUs. Transformers use self-attention mechanisms, where each token directly attends to all other tokens in the sequence, making it much easier for the model to capture long-range dependencies without the constraints of sequential steps. The model dynamically computes attention weights to decide which tokens are important to focus on. This gives transformers greater flexibility in handling long sequences and maintaining more contextual information. Compared to other neural language models, Transformers scale much more effectively \cite{kaplan2020scaling}, making it possible to train very deep models, such as the GPTs, {\color{black}LLaMA, or PaLM} with billions of parameters. Transformers are also much easier to train, thanks to their parallelism and efficient gradient flow. They achieve faster convergence and can be trained on much larger datasets. Transformer-based models may learn and predict contextual information either in a unidirectional{\color{black}/auto-regressive} or bidirectional fashion.



\textit{Unidirectional context learning/prediction} refers to models that process input data in a single direction, e.g., left-to-right for text or row-by-row for images{\color{black}, where each token is predicted based only on the preceding tokens. Auto-regressive models like GPTs use this approach and are suited for tasks such as text completion or machine translation, where tokens are generated sequentially without access to future tokens. Fig. \ref{fig_TokPred}(a) shows image generation with unidirectional context prediction, where tokens are generated sequentially from a codebook using a unidirectional transformer and then de-tokenized into an image.}



\textit{Bidirectional context learning/prediction} leverages the concept of \textit{masking}, {\color{black}where random tokens in the input text or image are hidden, and the model learns to predict them by attending to both preceding and succeeding tokens. This bidirectional attention helps capture richer context. Models like BERT use masked language modeling for text, while in visual synthesis, models like MaskGIT \cite{MaskGIT} predict masked image tokens by considering both nearby and distant tokens, enabling the capture of global spatial dependencies. Fig. \ref{fig_TokPred}(b) depicts the typical architecture for bidirectional context learning/prediction. Masked modeling is widely applied to generation tasks, such as image generation \cite{MaskGIT}, where the model iteratively generates image tokens, revealing a portion of tokens at each step and progressively uncovering more with each iteration.}


\begin{table*}[htbp]
    \centering
    \renewcommand{\arraystretch}{1.3} % 增加行高，提升可读性
    \caption{Comparison between the conventional SemCom and the proposed TokCom scheme.}
    \label{CompTab}
    \begin{tabular}{|p{3.8cm}|p{6.2cm}|p{6cm}|}
        \hline
        \textbf{} & \textbf{Conventional SemCom} & \textbf{TokCom} \\
        \hline
        \textbf{Tokenization/Embedding} & N/A & Required \\
        \hline
        \textbf{Transformer-based Signal Processing} & 
        May use transformer-based signal processing but at the pixel/sample/voxel level. & 
        Uses transformers to learn/exploit context information at the token level. \\
        \hline
        \textbf{Knowledge Base (KB)} & 
        A shared KB of various formats, e.g., graph and DNN, is required between the TX/RX. & 
        The shared KB includes a multimodal token codebook. \\
        \hline
        \textbf{Model Training} & 
        End-to-end training typically required, specifically in the JSCC setup. & 
        Leverages multimodal and typically large pre-trained models. \\
        \hline
        \textbf{Digital or Analog?} & 
        May be digital or analog (e.g., in the {\color{black}DeepJSCC} setup). & 
        Is based on a digital token codebook. \\
        \hline
        \textbf{Computational Complexity} & 
        Typically uses smaller task-specific NN models. & 
        Typically based on complex but task-adaptive {\color{black}GFMs/MLLMs}, GPTs, etc. \\
        \hline
    \end{tabular}
\end{table*}
%\vspace{-4mm}

\section{Token Communications: Opportunities and Challenges} \label{OppChall}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
TokCom is a new GenSC scheme that applies {\color{black}GFMs/MLLMs}-based token processing to encode the semantic content of multimodal signals at the transmitter and recover the corresponding semantics at the receiver leveraging cross-modal context information. TokCom provides the following main opportunities/advantages:
\begin{itemize}
    
    \item TokCom provides opportunities for more efficient semantic coding and communication in future wireless networks, as provided in the next sections. For example, unidirectional token prediction can be leveraged for adaptation of the coding/modulation order or the transmission power based on the next token likelihoods predicted at the transmitter by the GFMs/MLLMs. We will {\color{black}discuss} this and other basic TokCom setups in the next sections.
    

    \item In TokCom, the pre-trained token codebook is used as the shared knowledge base (KB) between the transmitter and receiver. This alleviates excessive knowledge sharing overheads in the conventional SemCom schemes. 
          
    \item Tokens are discrete representations that capture semantics and context, and hence, TokCom is inherently digital and more compatible with the existing multi-layer design of digital communication networks. Different from many conventional deep joint source channel coding (DeepJSCC)-based SemCom schemes, TokCom achieves scalability and adaptability by alleviating the need for end-to-end training.

    \item Tokens unify modalities and TokCom can leverages cross-modal relations to achieve ultra-low-rate SemCom. Furthermore, the in-context learning capabilities \cite{ICL} of MLLMs enables developing efficient TokCom frameworks that can adapt to various tasks, such as reconstruction, generation, and segmentation, {\color{black}among} different modalities.
\end{itemize}

Table \ref{CompTab} provides a comparison between TokCom and the conventional SemCom schemes. Despite the above benefits, the large computational complexity of many existing GFMs/MLLMs is a challenge to be addressed in design of efficient TokCom systems. An effective solution to this would be collaborative cloud-edge-device TokCom based on task offloading, which we further investigate in the next sections. It should be noted that the continuous development of smaller models, e.g., small language models (SLMs), and the emergence of more efficient processors will also help overcome this challenge.



\section{Basic Token Communication Setups} \label{SEC_Proposed}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
In this section, we introduce four basic TokCom setups: (1) TokCom for semantic source compression, (2) TokCom with semantic channel coding, (3) TokCom for semantic multiple access, and (4) TokCom network protocols.


\subsection{TokCom and Semantic Source Compression}
TokCom systems can \textit{leverage {\color{black}GFMs/MLLMs}-based context processing to achieve ultra-low-rate semantic source compression} in future wireless networks. Language modeling is inherently a form of compression because it involves accurate prediction the next/masked tokens in a sequence, thereby reducing uncertainty about the data. As the source coding theorem states, the expected length of a source compressed losslessly, is tied to its negative log-likelihood under its probability model. Conventional lossless source coding techniques like arithmetic coding leverage predicted probabilities to encode sequences into a nearly optimal binary form, where more accurate predictions yield shorter codes. State-of-the-art GFM/MLLMs predict the conditional probabilities of tokens given context. The predictability-compression link is general across various data modalities, and by minimizing the log-likelihood loss during training, language models implicitly optimize for compression. Thereby, compression is achieved when models effectively capture patterns and dependencies in the input multimodal data, making GFM/MLLMs inherently capable of functioning as efficient semantic compressors. As an example, the authors in \cite{deletang2024language} demonstrated that Chinchilla 70B, i.e., a language model primarily trained on text, can efficiently compress ImageNet patches to 43.4\% of their original size and LibriSpeech audio samples to 16.4\%, outperforming modality-specific compressors like PNG (58.5\%) and FLAC (30.3\%). {\color{black}Additionally, since tokenizers transform multimodal data into discrete tokens, tokenization inherently acts as a form of lossy compression. Its performance limits are dictated by the rate-distortion-perception theory \cite{RDP1}, which defines the trade-off between rate, distortion, and the perceptual quality of the reconstructed signal. Moreover, MLLMs can selectively discard irrelevant tokens based on downstream tasks, enabling efficient lossy token compression.} We demonstrate TokCom for semantic source compression in Fig. \ref{fig:4subfigure}(a).

\begin{figure*}[tb]
\vspace{-4mm}
\centerline{\includegraphics[scale=0.64]{fig3_222.pdf}}
\captionsetup{font={footnotesize, color = {black}}, singlelinecheck = off, justification = justified,name={Fig.},labelsep=period}
\caption{Basic TokCom setups: (a) Semantic source compression: Left - lossless compression via arithmetic coding and MLLMs for probability prediction; right - lossy compression dropping unimportant tokens based on downstream tasks. (b) Semantic channel coding: Map the embedding space to the modulation space; closer constellations for embedding vectors with high cosine similarity. (c) Semantic multiple access: Tokens from two users overlap in non-orthogonal multiple access, MLLMs separate sources {\color{black}using context and semantic orthogonality}. (d) Semantic network protocols: Transmitter packetizes tokens masking some of them for bitrate control; receiver predicts masked/lost packets using MLLMs.}
\label{fig:4subfigure}
\vspace{-4mm}
\end{figure*}


\subsection{TokCom with Semantic Channel Coding}
Another basic TokCom setup involves dynamic \textit{adaptation of the channel coding and modulation scheme based on the next/masked token predictability.} In the current wireless networks, the modulation and coding scheme (MCS) index is adapted solely based on the channel quality acquired by a channel quality indicator functionality. However, in TokCom, the MCS index is adapted for each token (or block of tokens) based on both the channel quality and token predictability based on the cross-modal context information. The token likelihoods are thereby estimated by carrying out token-based context processing in conjunction with the conventional log-likelihood ratio (LLR) calculations using the channel information to determine the modulation and coding scheme. Utilizing auto-regressive context processing, i.e., next token prediction, in this setup reduces the content buffering delay, while bidirectional context processing, i.e., masked token prediction, can reduce the computational complexity. Apart from the channel coding and modulation scheme, the allocated power or other communication resources can also be optimized based on tokens' predictability. 

Another possibility is to \textit{leverage the semantic similarity of tokens to optimize a mapping from the token codebook to the channel symbols.} For example, the two tokens ``King" and ``Queen" are semantically very similar. In fact, if the gender information is lost or mistaken due to channel errors, it can be simply inferred or corrected from numerous contextual clues at the receiver leveraging GFM/MLLMs. In other words, semantically similar tokens should be mapped onto channel symbols that are more probable to be confused during transmission. State-of-the-art multimodal embedding techniques enable the semantic similarity of tokens to be measured by their distance in the embedding space \cite{radford2021learning}. We demonstrate this concept in Fig. \ref{fig:4subfigure}(b).

\subsection{TokCom for Semantic Multiple Access}
{\color{black}GFMs/MLLMs}-based token processing can enable efficient semantic multiple access schemes in future wireless networks. To achieve this, we introduce the concept of \textit{semantic orthogonality in the token domain} as an emerging new dimension for multiple access communications. This allows several devices to transmit over the same multiple access channel (MAC). If collisions occur, i.e., signals from multiple devices are mixed non-orthogonally over the channel, a GFM/MLLM is utilized at the receiver to separate the devices’ individual signals at the token level, leveraging semantic orthogonality. In other words, GFM/MLLMs can leverage their pre-trained predictive capabilities to distinguish individual signals based on each device’s semantic context and the semantic orthogonality. 

We demonstrate this concept as an example in Fig. \ref{fig:4subfigure}(c), where two users are transmitting video streams depicting semantically orthogonal content: ``A swimming water hen." and ``A playing dog." Each video undergoes independent tokenization, with corresponding token sequences transmitted synchronously. When co-channel mixed tokens arrive at the receiver, the GFM/MLLM architecture disentangles the overlapping tokens through joint analysis of semantic orthogonality and predictive modeling, ultimately reconstructing both original videos. Building upon this principle, we recently developed token domain multiple access (ToDMA) – a practical protocol demonstrating efficient TokCom in semantic-aware multiple access scenarios \cite{ToDMA}.




\subsection{TokCom and Network Protocols} \label{Network}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%TokCom is poised to significantly influence and reshape network protocols. 
In TokCom, the semantic information is encoded into discrete tokens that are transmitted as semantic units. Each token is transmitted by a few bits that represent its index in the token codebook, and carries efficiently compressed semantic information leveraging multimodal context. Each TokCom packet includes several tokens depending on the packet size. Although the TokCom packets are similar to the conventional packets, i.e., they are basically containers of 0s and 1s, they include strong context information for each data flow. Leveraging this context information, some level of packet loss can be efficiently mitigated at the receiver. The added robustness to packet loss allows use of less reliable network protocols (e.g., UDP) for TokCom, or further relaxation of the flow and congestion control mechanisms at various protocol layers. With TokCom, less packet re-transmissions are required, and loss of packet ordering can be mitigated at the receiver to some extent, leveraging context information. This in turn can reduce the required sequence numbers, header sizes, and consequently the overall protocol overheads. Context-aware repeat requests allows selective retransmissions only for packets with irrecoverable token errors, thereby increasing the network capacity. Context-aware routing with TokCom allows to leverage pre-trained MLLMs at the transmitter or intermediate routers and prioritize less predictable tokens. This enables adaptation of functionalities at various network layers including routing and congestion/flow control, based on multimodal token likelihoods, thereby improving the quality of service (QoS). We demonstrate the above ideas with an example in Fig. \ref{fig:4subfigure}(d).


\section{Case Study: Cross-Modality TokCom for Generative Image Semantic Communication} \label{SemMA}

We consider a semantic image transmission task leveraging the proposed TokCom framework over a wireless communication channel evaluated on the ImageNet100 dataset, where each image has a resolution of $h\times w=256\times256$ pixels. 


\subsection{TokCom Setup}
Each image is first tokenized into a sequence of $N=256$ discrete tokens. To facilitate efficient transmission, these tokens are grouped into packets, with each packet containing 16 tokens. To mitigate the impact of burst errors—where consecutive erroneous tokens can significantly degrade reconstruction quality due to the loss of contextual information—the token positions included within each packet are randomized. This ensures that adjacent tokens in the original sequence are less likely to be lost simultaneously. The permutation pattern of token indices is pre-determined and remains consistent between the transmitter and receiver. Each token is drawn from a token codebook of size $Q=1024$, meaning it is represented using a 10-bit binary sequence. 

For reliable transmission, each packet undergoes rate-\(\frac{1}{2}\) convolutional encoding with cyclic redundancy check to enhance error detection and correction. The encoded bits are then modulated using 16-QAM before being transmitted over the channel. On the receiver side, demodulation and decoding are performed using a Soft Viterbi decoder. Based on the packet length, coding, and modulation scheme, the packet error rate (PER) as a function of the signal-to-noise ratio (SNR) {\color{black}under flat Rayleigh fading channel} is derived in \cite{ARQ}. Then, the average number of packet retransmissions required for successful data reception, denoted as $T$, is calculated as $\frac{1}{1-\text{PER}}$. Finally, we introduce a key performance metric \textit{token communication bandwidth efficiency} (TCE), defined as $\frac{h\times w}{T\times N \times \log_2({Q})}$.


\subsection{Proposed Cross-Modality TokCom Scheme}
To enhance the TCE of TokCom under various channel conditions, we propose a \textbf{TokCom w/ CMI} scheme, which aims to reduce the overhead of retransmissions and predict lost token packets by exploiting the context and \textit{cross-modality information} (CMI). In this scheme, each packet is transmitted only once, resulting in a retransmission count of $T=1$. This eliminates the need for packet re-transmissions, and hence, improves the TCE. For packets that experience errors, the corresponding token positions are marked with a special token, [MASK], in the decoded token sequence. The decoder then iteratively predicts these masked tokens by leveraging the contextual information surrounding them, using a pre-trained bi-directional transformer model called MaskGIT \cite{MaskGIT}. To further improve the prediction accuracy of the lost tokens, we incorporate CMI as a conditioning signal. Specifically, a 7-bit image class label, which is transmitted through a separate channel, is assumed to be perfectly received by the receiver. This label provides additional contextual information that helps the transformer make more accurate predictions for the lost tokens, improving quality of the received image.

In addition to the proposed ``TokCom w/ CMI" scheme, we also consider two baseline comparison schemes to evaluate the performance improvements. \textbf{Conventional scheme}: In this scheme, the receiver requests a retransmission for any errored packet, ensuring that all tokens are received correctly. For simplicity in analysis, the signaling overhead to request retransmissions over the feedback channel is not considered; however, including this overhead would further decrease the overall efficiency. \textbf{TokCom w/o CMI}: This baseline version of TokCom operates without the use of CMI, providing a simpler alternative. It serves as a reference for assessing the impact of incorporating CMI on the system performance.


\begin{figure*}[tb]
\vspace{-4mm}
\centerline{\includegraphics[scale=0.36]{TokCom3.pdf}}
\captionsetup{font={footnotesize, color = {black}}, singlelinecheck = off, justification = justified,name={Fig.},labelsep=period}
\caption{Cross-modality TokCom performance for generative image semantic communication. The metric TCE represents the \textit{token communication bandwidth efficiency}. The perceptual quality is measured by CLIP (↑) and LPIPS (↓), and the pixel distortions are measured by PSNR (↑).}
\label{fig_SimResults}
\vspace{-4mm}
\end{figure*}

\subsection{Performance Analysis}
As shown in Fig.~\ref{fig_SimResults}, the proposed TokCom framework significantly enhances performance across different channel conditions. At an SNR of $10$\,dB, where the packet error rate (PER) is moderate at $19\%$, the system achieves a substantial $23.8\%$ improvement in TCE, while maintaining a CLIP score comparable with the conventional baseline. This demonstrates that TokCom not only improves efficiency but also preserves the semantic quality of the transmitted image. When the channel conditions degrade to an SNR of $6$\,dB, resulting in a higher PER of $41\%$, TokCom exhibits remarkable robustness. The system experiences only a slight $4.5\%$ degradation in CLIP score, maintaining a score above $0.7$, which highlights its ability to handle more challenging communication environments without significant performance loss. 

Furthermore, a comparison between the two TokCom variants reveals that the ``TokCom w/o CMI" version performs worse, with lower CLIP scores, when compared to ``TokCom w/ CMI". This empirically validates the importance of incorporating CMI in preserving semantic quality. As shown in the visual samples of Fig.~\ref{fig_SimResults}, the class label ``Pit Bulls" serves as CMI to guide the lost {\color{black}image} token prediction, enabling the receiver to reconstruct a Pit Bull with high perceptual quality, even with a large PER. The performance gap between these variants underscores the vital role CMI plays in improving prediction accuracy and preserving image semantics.

Additionally, as shown in Fig.~\ref{fig_SimResults}, the generative token prediction capability of TokCom ensures that both learned perceptual image patch similarity (LPIPS) and peak signal-to-noise ratio (PSNR) degrade gradually with increasing PER, similar to the trend observed in CLIP. This demonstrates that TokCom consistently preserves high perceptual quality even under challenging error conditions. Finally, while incorporating CMI leads to improvements in semantic and perceptual quality metrics, i.e. CLIP and LPIPS, its impact on distortion metrics like PSNR is minimal. This underscores the importance of semantic and perceptual quality metrics in evaluating generative semantic communication, as they more accurately reflect the quality of reconstructed content in this context.


\section{Open Problems and Future Research Directions} \label{Open}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Efficient Tokenizer Design for TokCom}
Different tokenization schemes impact token predictability, and thereby the semantic compression performance, by balancing the input length versus entropy of the token distribution. For example, BPE tokenization reduces the length by merging frequently co-occurring tokens into larger tokens, which allows models to process more input data within a limited context window. However, this increases the token codebook size which can make token predictions more challenging. The balance between token size and input length, and hence design of efficient tokenizers, is crucial for TokCom efficiency. Another challenge is designing efficient unified tokenizers across different modalities. The primary difficulty lies in aligning the diverse representations of text, images, and other modalities, while ensuring that they can effectively interact in a shared embedding space. As multimodal models continue to scale, developing tokenizers that achieve this balance will be the key to unlocking the full potential of TokCom for efficient multi-modal GenSC.

\subsection{TokCom Computational Complexity and Collaborative Inference}
According to the scaling laws, performance of LMs reliably improve with their size and computational complexity \cite{kaplan2020scaling}. The existing GFM/MLLMs are typically computationally complex, thereby posing challenges in their deployment in the TokCom framework. To tackle this, a collaborative device-edge-cloud inference should be designed, where lightweight models can run on-device, while more complex inference is offloaded to edge or cloud servers equipped with high-performance larger models. The offloading strategy should optimize the trade off between computational complexity, latency, and the resulting semantic quality. The corresponding latency-performance tradeoffs should be derived for TokCom by considering factors such as model size, computational resources, channel conditions, and the heterogeneity in zero/few-shot performance \cite{FSL} of pre-trained MLLMs deployed on device-edge-cloud. As an example, \cite{Mengmeng} optimized the offloading strategy with a matching approach to minimize the total communication/computation latency while maximizing the semantic quality. 

\subsection{TokCom Privacy and Security}
The use of tokens as communication units and the reliance on pre-trained GFM/LLMs introduces potential vulnerabilities for TokCom. Semantic payloads are vulnerable to inference attacks, necessitating robust encryption and privacy-preserving mechanisms. Adversaries could potentially intercept or manipulate token sequences, leading to unauthorized access to sensitive information or alteration of transmitted content. Additionally, the use of context information in TokCom systems may inadvertently reveal more about the communicating parties than intended. To address these challenges, robust encryption methods specifically designed for token-based communications need to be developed. It is also crucial to consider the security implications of using pre-trained GFM/MLLMs, as they may be susceptible to adversarial attacks or contain biases that could compromise the integrity of the TokCom system. As TokCom evolves, ensuring privacy and security at both the architectural and operational levels will remain paramount to its effective deployment in future wireless networks.


\section{Conclusions}\label{SEC_Conclusion}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
In this paper, we introduced TokCom, a new framework motivated by the recent success of MLLMs, for leveraging cross-modal context information in generative SemCom. We also presented the key setups for efficient TokCom at various layers in future wireless networks and demonstrated the corresponding benefits in a typical image SemCom setup. TokCom opens up new avenues to develop innovative context-aware multimodal generative semantic communication schemes, driving the evolution of future wireless networks.


%----------------------------------------------------------------------------------------
%	BIBLIOGRAPHY

\bibliographystyle{IEEEtran}
\bibliography{Bibliography}


\end{document}
