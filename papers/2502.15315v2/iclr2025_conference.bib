@incollection{Bengio+chapter2007,
author = {Bengio, Yoshua and LeCun, Yann},
booktitle = {Large Scale Kernel Machines},
publisher = {MIT Press},
title = {Scaling Learning Algorithms Towards {AI}},
year = {2007}
}

@article{Hinton06,
author = {Hinton, Geoffrey E. and Osindero, Simon and Teh, Yee Whye},
journal = {Neural Computation},
pages = {1527--1554},
title = {A Fast Learning Algorithm for Deep Belief Nets},
volume = {18},
year = {2006}
}

@book{goodfellow2016deep,
title={Deep learning},
author={Goodfellow, Ian and Bengio, Yoshua and Courville, Aaron and Bengio, Yoshua},
volume={1},
year={2016},
publisher={MIT Press}
}

@article{radford2019language,
  title={Language models are unsupervised multitask learners},
  author={Radford, Alec and Wu, Jeffrey and Child, Rewon and Luan, David and Amodei, Dario and Sutskever, Ilya and others},
  journal={OpenAI blog},
  volume={1},
  number={8},
  pages={9},
  year={2019}
}

@article{raffel2020exploring,
  title={Exploring the limits of transfer learning with a unified text-to-text transformer},
  author={Raffel, Colin and Shazeer, Noam and Roberts, Adam and Lee, Katherine and Narang, Sharan and Matena, Michael and Zhou, Yanqi and Li, Wei and Liu, Peter J},
  journal={Journal of machine learning research},
  volume={21},
  number={140},
  pages={1--67},
  year={2020}
}

@article{bao2021beit,
  title={Beit: Bert pre-training of image transformers},
  author={Bao, Hangbo and Dong, Li and Piao, Songhao and Wei, Furu},
  journal={arXiv preprint arXiv:2106.08254},
  year={2021}
}

@article{alexey2020image,
  title={An image is worth 16x16 words: Transformers for image recognition at scale},
  author={Alexey, Dosovitskiy},
  journal={arXiv preprint arXiv: 2010.11929},
  year={2020}
}

@article{shazeer2017sparsely,
  title={The sparsely-gated mixture-of-experts layer},
  author={Shazeer, N and Mirhoseini, A and Maziarz, K and Davis, A and Le, Q and Hinton, G and Dean, J},
  journal={Outrageously large neural networks},
  year={2017}
}

@article{fedus2022switch,
  title={Switch transformers: Scaling to trillion parameter models with simple and efficient sparsity},
  author={Fedus, William and Zoph, Barret and Shazeer, Noam},
  journal={Journal of Machine Learning Research},
  volume={23},
  number={120},
  pages={1--39},
  year={2022}
}

@article{lepikhin2020scaling,
  title={Scaling giant models with conditional computation and automatic sharding},
  author={Lepikhin, D and Lee, H and Xu, Y and Chen, D and Firat, O and Huang, Y and Krikun, M and Shazeer, N and Gshard, Z},
  journal={arXiv preprint arXiv:2006.16668},
  year={2020}
}

@article{riquelme2021scaling,
  title={Scaling vision with sparse mixture of experts},
  author={Riquelme, Carlos and Puigcerver, Joan and Mustafa, Basil and Neumann, Maxim and Jenatton, Rodolphe and Susano Pinto, Andr{\'e} and Keysers, Daniel and Houlsby, Neil},
  journal={Advances in Neural Information Processing Systems},
  volume={34},
  pages={8583--8595},
  year={2021}
}

@article{kumatani2021building,
  title={Building a great multi-lingual teacher with sparsely-gated mixture of experts for speech recognition},
  author={Kumatani, Kenichi and Gmyr, Robert and Salinas, Felipe Cruz and Liu, Linquan and Zuo, Wei and Patel, Devang and Sun, Eric and Shi, Yu},
  journal={arXiv preprint arXiv:2112.05820},
  year={2021}
}

@inproceedings{clark2022unified,
  title={Unified scaling laws for routed language models},
  author={Clark, Aidan and de Las Casas, Diego and Guy, Aurelia and Mensch, Arthur and Paganini, Michela and Hoffmann, Jordan and Damoc, Bogdan and Hechtman, Blake and Cai, Trevor and Borgeaud, Sebastian and others},
  booktitle={International conference on machine learning},
  pages={4057--4086},
  year={2022},
  organization={PMLR}
}

@inproceedings{dikkala2023benefits,
  title={On the benefits of learning to route in mixture-of-experts models},
  author={Dikkala, Nishanth and Ghosh, Nikhil and Meka, Raghu and Panigrahy, Rina and Vyas, Nikhil and Wang, Xin},
  booktitle={Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing},
  pages={9376--9396},
  year={2023}
}

@article{chi2022representation,
  title={On the representation collapse of sparse mixture of experts},
  author={Chi, Zewen and Dong, Li and Huang, Shaohan and Dai, Damai and Ma, Shuming and Patra, Barun and Singhal, Saksham and Bajaj, Payal and Song, Xia and Mao, Xian-Ling and others},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={34600--34613},
  year={2022}
}

@inproceedings{du2022glam,
  title={Glam: Efficient scaling of language models with mixture-of-experts},
  author={Du, Nan and Huang, Yanping and Dai, Andrew M and Tong, Simon and Lepikhin, Dmitry and Xu, Yuanzhong and Krikun, Maxim and Zhou, Yanqi and Yu, Adams Wei and Firat, Orhan and others},
  booktitle={International Conference on Machine Learning},
  pages={5547--5569},
  year={2022},
  organization={PMLR}
}

@inproceedings{liu2021swin,
  title={Swin transformer: Hierarchical vision transformer using shifted windows},
  author={Liu, Ze and Lin, Yutong and Cao, Yue and Hu, Han and Wei, Yixuan and Zhang, Zheng and Lin, Stephen and Guo, Baining},
  booktitle={Proceedings of the IEEE/CVF international conference on computer vision},
  pages={10012--10022},
  year={2021}
}

@article{jacobs1991adaptive,
  title={Adaptive mixtures of local experts},
  author={Jacobs, Robert A and Jordan, Michael I and Nowlan, Steven J and Hinton, Geoffrey E},
  journal={Neural computation},
  volume={3},
  number={1},
  pages={79--87},
  year={1991},
  publisher={MIT Press}
}

@article{dasgupta2007probabilistic,
  title={A Probabilistic Analysis of EM for Mixtures of Separated, Spherical Gaussians.},
  author={Dasgupta, Sanjoy and Schulman, Leonard},
  journal={Journal of Machine Learning Research},
  volume={8},
  number={2},
  year={2007}
}

@inproceedings{lewis2021base,
  title={Base layers: Simplifying training of large, sparse models},
  author={Lewis, Mike and Bhosale, Shruti and Dettmers, Tim and Goyal, Naman and Zettlemoyer, Luke},
  booktitle={International Conference on Machine Learning},
  pages={6265--6274},
  year={2021},
  organization={PMLR}
}

@article{puigcerver2023sparse,
  title={From sparse to soft mixtures of experts},
  author={Puigcerver, Joan and Riquelme, Carlos and Mustafa, Basil and Houlsby, Neil},
  journal={arXiv preprint arXiv:2308.00951},
  year={2023}
}

@article{roller2021hash,
  title={Hash layers for large sparse models},
  author={Roller, Stephen and Sukhbaatar, Sainbayar and Weston, Jason and others},
  journal={Advances in Neural Information Processing Systems},
  volume={34},
  pages={17555--17566},
  year={2021}
}

@article{zhou2022mixture,
  title={Mixture-of-experts with expert choice routing},
  author={Zhou, Yanqi and Lei, Tao and Liu, Hanxiao and Du, Nan and Huang, Yanping and Zhao, Vincent and Dai, Andrew M and Le, Quoc V and Laudon, James and others},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={7103--7114},
  year={2022}
}

@article{bengio2015conditional,
  title={Conditional computation in neural networks for faster models},
  author={Bengio, Emmanuel and Bacon, Pierre-Luc and Pineau, Joelle and Precup, Doina},
  journal={arXiv preprint arXiv:1511.06297},
  year={2015}
}

@article{liu2022sparsity,
  title={Sparsity-constrained optimal transport},
  author={Liu, Tianlin and Puigcerver, Joan and Blondel, Mathieu},
  journal={arXiv preprint arXiv:2209.15466},
  year={2022}
}

@article{nguyen2024least,
  title={On least squares estimation in softmax gating mixture of experts},
  author={Nguyen, Huy and Ho, Nhat and Rinaldo, Alessandro},
  journal={arXiv preprint arXiv:2402.02952},
  year={2024}
}

@article{bubeck2021universal,
  title={A universal law of robustness via isoperimetry},
  author={Bubeck, S{\'e}bastien and Sellke, Mark},
  journal={Advances in Neural Information Processing Systems},
  volume={34},
  pages={28811--28822},
  year={2021}
}

@article{puigcerver2022adversarial,
  title={On the adversarial robustness of mixture of experts},
  author={Puigcerver, Joan and Jenatton, Rodolphe and Riquelme, Carlos and Awasthi, Pranjal and Bhojanapalli, Srinadh},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={9660--9671},
  year={2022}
}

@inproceedings{zhang2023robust,
  title={Robust mixture-of-expert training for convolutional neural networks},
  author={Zhang, Yihua and Cai, Ruisi and Chen, Tianlong and Zhang, Guanhua and Zhang, Huan and Chen, Pin-Yu and Chang, Shiyu and Wang, Zhangyang and Liu, Sijia},
  booktitle={Proceedings of the IEEE/CVF International Conference on Computer Vision},
  pages={90--101},
  year={2023}
}

@article{jordan1994hierarchical,
  title={Hierarchical mixtures of experts and the EM algorithm},
  author={Jordan, Michael I and Jacobs, Robert A},
  journal={Neural computation},
  volume={6},
  number={2},
  pages={181--214},
  year={1994},
  publisher={MIT Press}
}

@article{chen2022towards,
  title={Towards understanding the mixture-of-experts layer in deep learning},
  author={Chen, Zixiang and Deng, Yihe and Wu, Yue and Gu, Quanquan and Li, Yuanzhi},
  journal={Advances in neural information processing systems},
  volume={35},
  pages={23049--23062},
  year={2022}
}

@article{goodfellow2014explaining,
  title={Explaining and harnessing adversarial examples},
  author={Goodfellow, Ian J and Shlens, Jonathon and Szegedy, Christian},
  journal={arXiv preprint arXiv:1412.6572},
  year={2014}
}

@article{mkadry2017towards,
  title={Towards deep learning models resistant to adversarial attacks},
  author={Madry, Aleksander and Makelov, Aleksandar and Schmidt, Ludwig and Tsipras, Dimitris and Vladu, Adrian},
  journal={stat},
  volume={1050},
  number={9},
  year={2017}
}

@inproceedings{uesato2018adversarial,
  title={Adversarial risk and the dangers of evaluating against weak attacks},
  author={Uesato, Jonathan and O’donoghue, Brendan and Kohli, Pushmeet and Oord, Aaron},
  booktitle={International conference on machine learning},
  pages={5025--5034},
  year={2018},
  organization={PMLR}
}

@article{pang2008opinion,
  title={Opinion mining and sentiment analysis},
  author={Pang, Bo and Lee, Lillian and others},
  journal={Foundations and Trends{\textregistered} in information retrieval},
  volume={2},
  number={1--2},
  pages={1--135},
  year={2008},
  publisher={Now Publishers, Inc.}
}

@inproceedings{hendrycks2021natural,
  title={Natural adversarial examples},
  author={Hendrycks, Dan and Zhao, Kevin and Basart, Steven and Steinhardt, Jacob and Song, Dawn},
  booktitle={Proceedings of the IEEE/CVF conference on computer vision and pattern recognition},
  pages={15262--15271},
  year={2021}
}

@inproceedings{hendrycks2021many,
  title={The many faces of robustness: A critical analysis of out-of-distribution generalization},
  author={Hendrycks, Dan and Basart, Steven and Mu, Norman and Kadavath, Saurav and Wang, Frank and Dorundo, Evan and Desai, Rahul and Zhu, Tyler and Parajuli, Samyak and Guo, Mike and others},
  booktitle={Proceedings of the IEEE/CVF international conference on computer vision},
  pages={8340--8349},
  year={2021}
}

@article{ma2000asymptotic,
  title={Asymptotic convergence rate of the EM algorithm for Gaussian mixtures},
  author={Ma, Jinwen and Xu, Lei and Jordan, Michael I},
  journal={Neural Computation},
  volume={12},
  number={12},
  pages={2881--2907},
  year={2000},
  publisher={MIT Press}
}

@article{dempster1977maximum,
  title={Maximum likelihood from incomplete data via the EM algorithm},
  author={Dempster, Arthur P and Laird, Nan M and Rubin, Donald B},
  journal={Journal of the royal statistical society: series B (methodological)},
  volume={39},
  number={1},
  pages={1--22},
  year={1977},
  publisher={Wiley Online Library}
}

@article{pham2024competesmoe,
  title={CompeteSMoE--Effective Training of Sparse Mixture of Experts via Competition},
  author={Pham, Quang and Do, Giang and Nguyen, Huy and Nguyen, TrungTin and Liu, Chenghao and Sartipi, Mina and Nguyen, Binh T and Ramasamy, Savitha and Li, Xiaoli and Hoi, Steven and others},
  journal={arXiv preprint arXiv:2402.02526},
  year={2024}
}

@inproceedings{socher2013recursive,
  title={Recursive deep models for semantic compositionality over a sentiment treebank},
  author={Socher, Richard and Perelygin, Alex and Wu, Jean and Chuang, Jason and Manning, Christopher D and Ng, Andrew Y and Potts, Christopher},
  booktitle={Proceedings of the 2013 conference on empirical methods in natural language processing},
  pages={1631--1642},
  year={2013}
}

@article{casanueva2020efficient,
  title={Efficient intent detection with dual sentence encoders},
  author={Casanueva, I{\~n}igo and Tem{\v{c}}inas, Tadas and Gerz, Daniela and Henderson, Matthew and Vuli{\'c}, Ivan},
  journal={arXiv preprint arXiv:2003.04807},
  year={2020}
}

@article{vaswani2017attention,
  title={Attention is all you need},
  author={Vaswani, A},
  journal={Advances in Neural Information Processing Systems},
  year={2017}
}

@article{han2024designing,
  title={Designing robust transformers using robust kernel density estimation},
  author={Han, Xing and Ren, Tongzheng and Nguyen, Tan and Nguyen, Khai and Ghosh, Joydeep and Ho, Nhat},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2024}
}

@inproceedings{deng2009imagenet,
  title={Imagenet: A large-scale hierarchical image database},
  author={Deng, Jia and Dong, Wei and Socher, Richard and Li, Li-Jia and Li, Kai and Fei-Fei, Li},
  booktitle={2009 IEEE conference on computer vision and pattern recognition},
  pages={248--255},
  year={2009},
  organization={Ieee}
}

@article{merity2016pointer,
  title={Pointer sentinel mixture models},
  author={Merity, Stephen and Xiong, Caiming and Bradbury, James and Socher, Richard},
  journal={arXiv preprint arXiv:1609.07843},
  year={2016}
}

@inproceedings{zhou2022understanding,
  title={Understanding the robustness in vision transformers},
  author={Zhou, Daquan and Yu, Zhiding and Xie, Enze and Xiao, Chaowei and Anandkumar, Animashree and Feng, Jiashi and Alvarez, Jose M},
  booktitle={International Conference on Machine Learning},
  pages={27378--27394},
  year={2022},
  organization={PMLR}
}

@article{witten2010framework,
  title={A framework for feature selection in clustering},
  author={Witten, Daniela M and Tibshirani, Robert},
  journal={Journal of the American Statistical Association},
  volume={105},
  number={490},
  pages={713--726},
  year={2010},
  publisher={Taylor \& Francis}
}

@article{friedman2004clustering,
  title={Clustering objects on subsets of attributes (with discussion)},
  author={Friedman, Jerome H and Meulman, Jacqueline J},
  journal={Journal of the Royal Statistical Society Series B: Statistical Methodology},
  volume={66},
  number={4},
  pages={815--849},
  year={2004},
  publisher={Oxford University Press}
}

@article{brusco2001variable,
  title={A variable-selection heuristic for K-means clustering},
  author={Brusco, Michael J and Cradit, J Dennis},
  journal={Psychometrika},
  volume={66},
  pages={249--270},
  year={2001},
  publisher={Springer}
}

@article{gnanadesikan1995weighting,
  title={Weighting and selection of variables for cluster analysis},
  author={Gnanadesikan, Ram and Kettenring, Jon R and Tsao, Shiao Li},
  journal={Journal of classification},
  volume={12},
  pages={113--136},
  year={1995},
  publisher={Springer}
}

@article{van1989clustering,
  title={Clustering n objects into k groups under optimal scaling of variables},
  author={Van Buuren, Stef and Heiser, Willem J},
  journal={Psychometrika},
  volume={54},
  pages={699--706},
  year={1989},
  publisher={Springer}
}

@inproceedings{dai-etal-2022-stablemoe,
    title = "{S}table{M}o{E}: Stable Routing Strategy for Mixture of Experts",
    author = "Dai, Damai  and
      Dong, Li  and
      Ma, Shuming  and
      Zheng, Bo  and
      Sui, Zhifang  and
      Chang, Baobao  and
      Wei, Furu",
    editor = "Muresan, Smaranda  and
      Nakov, Preslav  and
      Villavicencio, Aline",
    booktitle = "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = may,
    year = "2022",
    address = "Dublin, Ireland",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.acl-long.489",
    doi = "10.18653/v1/2022.acl-long.489",
    pages = "7085--7095",
    abstract = "The Mixture-of-Experts (MoE) technique can scale up the model size of Transformers with an affordable computational overhead. We point out that existing learning-to-route MoE methods suffer from the routing fluctuation issue, i.e., the target expert of the same input may change along with training, but only one expert will be activated for the input during inference. The routing fluctuation tends to harm sample efficiency because the same input updates different experts but only one is finally used. In this paper, we propose StableMoE with two training stages to address the routing fluctuation problem. In the first training stage, we learn a balanced and cohesive routing strategy and distill it into a lightweight router decoupled from the backbone model. In the second training stage, we utilize the distilled router to determine the token-to-expert assignment and freeze it for a stable routing strategy. We validate our method on language modeling and multilingual machine translation. The results show that StableMoE outperforms existing MoE methods in terms of both convergence speed and performance.",
}

@article{guo2024dynamic,
  title={Dynamic mixture of experts: An auto-tuning approach for efficient transformer models},
  author={Guo, Yongxin and Cheng, Zhenglin and Tang, Xiaoying and Tu, Zhaopeng and Lin, Tao},
  journal={arXiv preprint arXiv:2405.14297},
  year={2024}
}

@inproceedings{
nguyen2023a,
title={A Primal-Dual Framework for Transformers and Neural Networks},
author={Tan Minh Nguyen and Tam Minh Nguyen and Nhat Ho and Andrea L. Bertozzi and Richard Baraniuk and Stanley Osher},
booktitle={The Eleventh International Conference on Learning Representations },
year={2023},
url={https://openreview.net/forum?id=U_T8-5hClV}
}

@inproceedings{
teo2024momentumsmoe,
title={Momentum{SM}oE: Integrating Momentum into Sparse Mixture of Experts},
author={Rachel Teo and Tan Minh Nguyen},
booktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},
year={2024},
url={https://openreview.net/forum?id=y929esCZNJ}
}

@inproceedings{
nguyen2025camex,
title={{CAME}x: Curvature-aware Merging of Experts},
author={Viet Dung Nguyen and Minh Nguyen Hoang and Luc Nguyen and Rachel Teo and Tan Minh Nguyen and Linh Duy Tran},
booktitle={The Thirteenth International Conference on Learning Representations},
year={2025},
url={https://openreview.net/forum?id=nT2u0M0nf8}
}

@inproceedings{
teo2025molex,
title={Mo{LE}x: Mixture of Layer Experts for Fine-tuning with Sparse Upcycling},
author={Rachel Teo and Tan Minh Nguyen},
booktitle={The Thirteenth International Conference on Learning Representations},
year={2025},
url={https://openreview.net/forum?id=rWui9vLhOc}
}

@inproceedings{
abdullaev2025transformer,
title={Transformer Meets Twicing: Harnessing Unattended Residual Information},
author={Laziz Abdullaev and Tan Minh Nguyen},
booktitle={The Thirteenth International Conference on Learning Representations},
year={2025},
url={https://openreview.net/forum?id=16kG5aNleS}
}

@article{teo2025unveiling,
  title={Unveiling the hidden structure of self-attention via kernel principal component analysis},
  author={Teo, Rachel SY and Nguyen, Tan},
  journal={Advances in Neural Information Processing Systems},
  volume={37},
  pages={101393--101427},
  year={2025}
}

@article{nielsen2025elliptical,
  title={Elliptical attention},
  author={Nielsen, Stefan and Abdullaev, Laziz and Teo, Rachel SY and Nguyen, Tan},
  journal={Advances in Neural Information Processing Systems},
  volume={37},
  pages={109748--109789},
  year={2025}
}

@inproceedings{nguyen2024pidformer,
  title={PIDformer: Transformer meets control theory},
  author={Nguyen, Tam Minh and Uribe, C{\'e}sar A and Nguyen, Tan Minh and Baraniuk, Richard},
  booktitle={Forty-first International Conference on Machine Learning},
  year={2024}
}