
\documentclass{article} % For LaTeX2e
\usepackage{iclr2025_conference,times}

% Optional math commands from https://github.com/goodfeli/dlbook_notation.
\input{math_commands.tex}

% \usepackage{hyperref}
\usepackage{url}

% \usepackage{amsmath}
%\usepackage[subtle]{savetrees}
\usepackage{caption}
\usepackage{amsthm}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{wrapfig}
\usepackage{MnSymbol}
\usepackage{wasysym}
\usepackage{enumitem}
\usepackage{subfigure}
\usepackage{graphicx}
\usepackage{subcaption}
\usepackage{xcolor}
\usepackage{aligned-overset}
\usepackage{hyperref}
\definecolor{myblue}{rgb}{0.1,0.2,0.75}
\definecolor{lightblue}{HTML}{A6E5FF}
% \setlength{\intextsep}{5pt}
\hypersetup{ %
pdftitle={},
pdfauthor={},
pdfsubject={},
pdfkeywords={},
pdfborder=0 0 0,
pdfpagemode=UseNone,
colorlinks=true,
linkcolor=myblue,
citecolor=myblue,
filecolor=myblue,
urlcolor=myblue,    
pdfview=FitH}
\usepackage{blindtext}
\usepackage{booktabs}
\usepackage{multirow}
% \hypersetup{
%   colorlinks=true,    % Color links instead of putting a border around them
%   linkcolor=blue,   % Color for internal links
%   citecolor=green,    % Color for citations
%   urlcolor=magenta,   % Color for external links
% }
\usepackage{bm}
\usepackage{natbib}
\PassOptionsToPackage{numbers}{natbib}
% \newcommand{\R}{\mathbb{R}}
\newcommand{\bs}[1]{\boldsymbol{#1}}

\newtheorem{theorem}{Theorem}
\newtheorem{prop}{Proposition}
\newtheorem{lemma}{Lemma}
\newtheorem{remark}{Remark}
\newtheorem{definition}{Definition}
\newtheorem{assumption}{Assumption}
\newtheorem{corollary}{Corollary}

\usepackage{titletoc}

\definecolor{lightblue}{RGB}{70, 150, 180}

\newcommand\DoToC{%
  \startcontents
  \printcontents{}{1}{\textbf{Table of Contents}\vskip3pt\hrule\vskip5pt}
  \vskip3pt\hrule\vskip5pt
}

\usepackage[normalem]{ulem}
\newcommand\tsout{\bgroup\markoverwith{\textcolor{red}{\rule[0.5ex]{2pt}{0.4pt}}}\ULon}

\newcommand{\tanm}[1]{\textcolor{red}{(Tan:#1)}}
\newcommand{\black}[1]{\textcolor{black}{#1}} % for revision

\newcommand\blfootnote[1]{%
  \begingroup
  \renewcommand\thefootnote{}\footnote{#1}%
  \addtocounter{footnote}{-1}%
  \endgroup
}


\title{Tight Clusters Make Specialized Experts}

% Authors must not appear in the submitted version. They should be hidden
% as long as the \iclrfinalcopy macro remains commented out below.
% Non-anonymous submissions will be rejected without review.

\author{%
  Stefan K. Nielsen$^\ast$ \\
  FPT Software AI Center\\ 
  \texttt{stefannvkp@fpt.com}
  % examples of more authors
  \And
  Rachel S.Y. Teo$^\ast$\\
  Department of Mathematics\hspace*{0.92in}\\ 
  National University of Singapore\\
  \texttt{rachel.tsy@u.nus.edu}
  \And
  Laziz U. Abdullaev \\
  Department of Mathematics\\ 
  National University of Singapore\\
  \texttt{laziz.abdullaev@u.nus.edu}
  \And
  Tan M. Nguyen \\
  Department of Mathematics\\ 
  National University of Singapore\\
  \texttt{tanmn@nus.edu.sg}
}

% The \author macro works with any number of authors. There are two commands
% used to separate the names and addresses of multiple authors: \And and \AND.
%
% Using \And between authors leaves it to \LaTeX{} to determine where to break
% the lines. Using \AND forces a linebreak at that point. So, if \LaTeX{}
% puts 3 of 4 authors names on the first line, and the last on the second
% line, try using \AND instead of \And before the third author name.

\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}

\iclrfinalcopy % Uncomment for camera-ready version, but NOT for submission.
\begin{document}
\raggedbottom

\maketitle

% \begin{abstract}
% Sparse Mixture-of-Experts (MoE) architectures have emerged as a promising approach to decoupling model capacity from computational cost. At the core of the MoE model is the router, which learns the underlying (multimodal?) clustering structure of the input distribution in order to send input tokens to appropriate experts. However, uncovering the latent clusters of the input space may not be possible when clusters have differing dependence on the features or cluster only on subsets of features. This unidentifiability causes slow convergence, poor expert specialization, and susceptibility to data corruption as the router is unable to perform appropriate input-expert matching. We propose the Adaptive Clustering (AC) router, a novel MoE routing method that computes token-expert assignments in a transformed space that promotes separated and tight token clusters, which helps identify the best-matched expert for each token. Our proposed AC router enables the MoE model to obtain three connected benefits: 1) faster convergence, 2) better robustness to data corruption, and 3) overall performance improvement as experts are specialized on semantically distinct regions of the input space. We empirically demonstrate the advantages of our AC router over baseline routing methods when applied on a variety of MoE backbones for large-scale language modeling and object recognition tasks, in both clean and corrupted settings. 
% \end{abstract}

\begin{abstract}
Sparse Mixture-of-Experts (MoE) architectures have emerged as a promising approach to decoupling model capacity from computational cost. At the core of the MoE model is the router, which learns the underlying clustering structure of the input distribution in order to send input tokens to appropriate experts. However, latent clusters may be unidentifiable in high dimension, which causes slow convergence, susceptibility to data contamination, and overall degraded representations as the router is unable to perform appropriate token-expert matching. We examine the router through the lens of clustering optimization and derive optimal feature weights that maximally identify the latent clusters. We use these weights to compute the token-expert routing assignments in an adaptively transformed space that promotes well-separated clusters, which helps identify the best-matched expert for each token. In particular, for each expert cluster, we compute a set of weights that scales features according to whether that expert clusters tightly along that feature. We term this novel router the Adaptive Clustering (AC) router. Our AC router enables the MoE model to obtain three connected benefits: 1) faster convergence, 2) better robustness to data corruption, and 3) overall performance improvement, as experts are specialized in semantically distinct regions of the input space. We empirically demonstrate the advantages of our AC router over baseline routing methods when applied on a variety of MoE backbones for language modeling and image recognition tasks in both clean and corrupted settings. The code is publicly available at \href{https://github.com/stefvk/ACMoE}{\textcolor{lightblue}{\texttt{https://github.com/stefvk/ACMoE}}}. \blfootnote{$^\ast$ Co-first authors. Please correspond to: stefannvkp@fpt.com and tanmn@nus.edu.sg} 
\end{abstract}

% \begin{abstract}
% Sparse Mixture-of-Experts (MoE) architectures have emerged as a promising approach to decoupling model capacity from computational cost. At the core of the MoE model is the router, which learns the underlying \tsout{clustering} \tanm{multimodal ?} structure of the input \tsout{space} \tanm{distribution?} in order to \tsout{distribute} \tanm{send?} input \tanm{tokens} to appropriate experts. However, uncovering the latent semantic regions of the input space is hard in high dimension, where \tanm{different subsets of input features result in different clustering assignments} \tanm{This sentence was quite confusing. I've tried my best to clean it up, but you might need to make it even clearer. For example, what are latent semantic regions? We also need to find a technical name for this phenomenon in the existing literature}. This \tanm{unidentifiability} causes slow convergence, poor expert specialization, and susceptibility to data corruption as the router is unable to perform \tsout{suitable} \tanm{appropriate?} input-expert matching. We propose the Adaptive Clustering (AC) router, a novel MoE routing method that computes token-expert assignments in a transformed space that promotes the separation of \tanm{the token clusters in a latent space, which helps identify the best-matched expert for each token.} \tsout{latent clusters in the data and more easily identifies the best-matched expert for each token.} \tanm{Our proposed AC router enables the MoE model to obtain three connected benefits}: 1) faster convergence, 2) better robustness to data corruption, and 3)  \tanm{overall performance improvement} as experts are \tsout{better able to} specialized on semantically distinct regions of the input space. We empirically demonstrate the advantages of our AC router over baseline routing methods \tanm{when applied on} a variety of MoE backbones \tanm{for} large-scale language modeling and object recognition \tanm{tasks, in both clean and corrupted settings}. 
% \end{abstract}

\section{Introduction}

Scaling up model capacity continues to deliver substantial performance gains across a wide range of tasks, with particularly impressive results in visual representation learning and language modeling \citep{alexey2020image, bao2021beit, radford2019language, raffel2020exploring,nguyen2023a}. However, larger models incur growing computational costs, prompting increasing research into Sparse Mixture-of-Experts models (MoE), which offers a promising avenue to balancing model scale with efficiency by activating only sub-modules, termed \textit{experts}, of the network during training and inference \citep{shazeer2017sparsely, fedus2022switch, lepikhin2020scaling,nguyen2025camex}. This approach has been shown to achieve better performance than dense models with nearly constant computational overhead on tasks from speech recognition, image recognition, machine translation, and language modeling \citep{riquelme2021scaling, kumatani2021building, lepikhin2020scaling,teo2025molex}. 

\begin{figure}[t]
    \centering
    \includegraphics[width=0.8\linewidth]{Grouped_Bird_and_Shark_cmatch.png}
    \captionsetup{font=small}
    \vspace{-0.1in}
    \caption{ACMoE discovers semantically distinct regions. We show 14x14 image reconstructions where patches are colored by assigned experts. \textbf{Top row:} Swin assigns large chunks of foreground and background to one expert (red), while ACMoE accurately discovers the bird and relevant foreground. \textbf{Bottom row:} When the background and foreground are hard to distinguish, Swin's router fails to register the stingray (left) or shark (right) and allocates one expert for virtually the entire image. ACMoE, however, discovers the semantically distinct regions, using one expert (green) to specialize on the foreground and different experts for the background.}
    \label{fig: expert patches}
    \vspace{-0.2in}
\end{figure}
At the core of the MoE layer is the learned router which assigns inputs to the relevant experts. The router must learn to segment the input space appropriately such that inputs and experts are well matched, enabling the experts to be trained on semantically similar data. This expert specialization allows MoE models to produce better representations than their dense counterparts while activating only a fraction of the total parameters. Recently, various methods have been proposed to find optimal expert-token matches, including linear programs \citep{lewis2021base}, cosine similarity-based rules \citep{chi2022representation}, soft assignments via convex combinations of inputs \citep{puigcerver2023sparse}, and both top-k experts per token \citep{shazeer2017sparsely} and top-k tokens per expert \citep{zhou2022mixture}. We note that the above approaches fundamentally rely on dot-products between inputs and experts to learn the corresponding assignment, which might be suboptimal in cases where the semantic regions are not easily discoverable in the high-dimensional feature space. Typically, we expect that the true underlying clusters present in the data will cluster on different, potentially disjoint, subsets of features, and may not be discoverable when using the full feature set. This phenomenon can lead to slow convergence as the experts are unable to specialize on semantically similar regions of the data, poor robustness as data contamination can spuriously assign inputs to unsuitable experts, and degraded overall downstream performance due to suboptimal input-expert matching.

% Typically, we expect that the true underlying clusters present in the data depend on the features differently or depend only on subsets of features, and so might be missed if we attempt to discover them using the full set of features.

% At the core of the MoE layer is the learned router which assigns inputs to the relevant experts. The router must learn to segment the input space appropriately such that inputs and experts are well matched, enabling the experts to be trained on semantically similar data. This expert specialization allows MoE models to produce better representations than their dense counterparts while activating only a fraction of the total parameters. Recently, various methods have been proposed to find optimal token-to-expert matches, including linear programs \citep{lewis2021base}, cosine similarity-based rules \citep{chi2022representation}, soft assignments via convex combinations of inputs \citep{puigcerver2023sparse}, and both top-k experts per token \citep{shazeer2017sparsely} and top-k tokens per expert \citep{zhou2022mixture}. We note that the above approaches fundamentally rely on dot-products between inputs and experts to learn the corresponding assignment, which might be suboptimal in cases where the semantic regions are not easily discoverable in the high-dimensional feature space. Typically, we expect that the true underlying clusters present in the data depend on the features differently or depend only on subsets of features, and so might be missed if we attempt to discover them using the full set of features. This phenomenon can lead to slow convergence as the experts are unable to specialize on semantically similar regions of the data, poor robustness as data contamination can spuriously assign inputs to unsuitable experts, and poorer downstream performance due to overall degraded input-expert matching.

\textbf{Contribution.} In this work, we propose the Adaptive Clustering (AC) router and corresponding Adaptive Clustering Mixture-of-Experts (ACMoE), a novel MoE method in which the router computes token-expert assignments in a transformed space that maximally identifies latent clusters in the data and more easily discovers the best-matched expert for each token. 
% \begin{wrapfigure}[19]{r}{0.4\textwidth}
% \small
% \centering
% \includegraphics[width=0.95\linewidth]{EllipSMoE/validation_loss_comparison_glam.png}
% \caption{\small Convergence Rate on WikiText-103. Our ACMoE converges substantially faster.}
% \label{fig: convergence}
% \end{wrapfigure}
More specifically, we adaptively learn for each input which features best determine its cluster assignment and scale its features accordingly such that features that promote tight expert clusters are upweighted, and features that produce dispersed expert clusters are downweighted. This transformation accentuates the relevant characteristics of each input according to the specialization of the experts, thereby allowing the router to more easily discover the optimal input-expert allocation. Computing the routing assignments following this scheme produces three benefits: 1) \emph{faster convergence} as experts are able to specialize more quickly by being allocated semantically similar inputs, 2) \emph{better robustness} as latent clusters are better separated, thereby minimizing the risk that data corruption erroneously assigns tokens to unsuitable experts, and 3) \emph{better overall representations and downstream performance} due to improved expert specialization. In order to discover the key features per token and their corresponding weights, we 
present a feature-weighted clustering optimization perspective on the MoE framework and demonstrate how the clustering solution obtains the required feature weights. We show how these weights can be integrated into the routing mechanism such that routing takes place in a cluster-adaptive transformed space. We theoretically prove that our proposed routing mechanism learns the latent clustering structure of the data faster than standard routing mechanisms and that our mechanism is more robust to data contamination. Furthermore, our proposed method involves no learnable parameters and can be computed highly efficiently. In summary, our contributions are three-fold:
\begin{enumerate}[leftmargin=25pt]
    \item We develop the novel Adaptive Clustering router, a routing method in MoE architectures that computes token-expert assignments in a transformed space that promotes separation of latent clusters in the data and more easily identifies the best-matched expert for each token.
    \item We propose a feature-weighted clustering optimization perspective on token-expert assignment and derive the optimal feature weights for adaptively transforming the input data for routing.
    \item We derive a theoretical framework demonstrating how MoE robustness and convergence depend on the shape of latent clusters and the clustering geometry of the input space. 
    % \item We establish a connection between the bias and variance of non-parametric estimators to the quality of learned contextual representations in attention mechanisms.
\end{enumerate}
We empirically demonstrate that 1) the Adaptive Clustering router outperforms baseline routing methods in MoE architectures in large-scale tasks such as WikiText-103 language modeling and downstream finetuning, and ImageNet-1k object classification in both clean and contaminated settings, 2) the Adaptive Clustering router exhibits faster convergence than baseline methods, and 3) the Adaptive Clustering router attains these performance improvements for free -- that is, with no learnable parameters and negligible computational overhead.

% These approaches fundamentally rely on dot-products between inputs and experts to learn the corresponding assignment, which assumes all dimensions of the feature space are equally relevant for identifying the semantically similar regions of the input space and thereby the corresponding allocation. 

% One might expect that the true underlying clusters present in the data differ only with respect to a small fraction of the features, and will be missed if one clusters the observations using the full set of features. 

% This framework traces its roots back to Gaussian mixture models in which the input space is assumed to be divisible into separate regions with an expert specializing in each region \citep{jacobs1991adaptive}. SMoE architectures then employ a router to match inputs to corresponding experts, allowing the experts to specialize by allocating them semantically similar inputs, producing better representations while activating only a fraction of the total parameters. 
\begin{figure}
    \centering
    \includegraphics[width=0.85\linewidth]{validation_loss_comparison_final.png}
    \captionsetup{font=small}
    \vspace{-0.1in}
    \caption{Fast Convergence of ACMoE. {\bf Left:} Convergence speed on WikiText-103 pretraining using the Generalist Language Model \citep{du2022glam} backbone. {\bf Right:} Convergence speed on Banking-77 finetuning using the Switch Transformer \citep{fedus2022switch} backbone. Across both backbones and tasks, we observe substantially faster convergence. We display final test perplexity (PPL) and accuracy (Acc.), showing better overall performance as well.}
    \label{fig: convergence}
    \vspace{-0.2in}
\end{figure}

\textbf{Preliminaries.} We consider Transformer \citep{vaswani2017attention} based MoE architectures and follow the approach of previous work where the MoE layer is inserted after the self-attention layer within the Transformer, replacing the traditional feed-forward network \citep{fedus2022switch, du2022glam, liu2021swin}. Let $\bs x$ be an input token with hidden representation $\bs h \in \mathbb R^d$ and $\bs e_1, \bs e_2, \dots \bs e_N \in \mathbb R^d$ be the $N$ learnable expert embeddings for model hidden dimension $d$. The MoE layer selecting the top $k$ experts is described by the following equations:
\begin{align}
    \mathcal K &:= \text{topk}_k (s_k) = \text{topk}_k (\bs h^\top \bs e_k)\label{eqn: just router} \\  
    f^{SMoE}(\bs h) &= \bs h + \sum_{k \in \mathcal K} g(\bs h^\top \bs e_k) f_k^{\mathrm{FFN}}(\bs h), \label{eq:standard-router}
\end{align}
where $f_k^{\mathrm{FFN}}$ is the $k^{\mathrm{th}}$ expert feed-forward network, $s_k = \bs h^\top \bs e_k$ is the similarity score between token representation $\bs h$ and the $k^{\mathrm{th}}$ expert $\bs e_k$ and $g(\cdot)$ is a gating function often chosen as softmax, $g(s_k) = \exp(s_k) / \sum_ {j \in \mathcal{K}} \exp(s_{j})$. We refer to Eqn. \ref{eqn: just router} as the router, which learns the top $k$ best matched experts per token, and Eqn. \ref{eq:standard-router} as the overall standard MoE layer.

{\bf Organization.} We structure this paper as follows: In Section \ref{sec: a clustering optimization problem}, we present a clustering optimization problem and show that its solution adaptively scales the feature space according to which dimensions promote tight clustering. In Section \ref{sec: a tight cluster}, we present how the solution to our clustering optimization problem can
be built into our proposed AC router and we provide the full technical formulation of AC routing and Adaptive Clustering Mixture-of-Experts (ACMoE). We then present theoretical propositions on faster convergence and robustness. We empirically validate the advantages of ACMoE in Section \ref{sec: experiments} and discuss related work in Section \ref{sec: related work}. We end with concluding remarks and future work in Section \ref{sec: conclusion}. Proofs, technical details, and further experiments are provided in the Appendix.

\section{A Clustering Optimization Perspective} \label{sec: a clustering optimization problem} 

% There is a rich literature on feature-weighted clustering and transformations of the input space to improve clustering algorithms \citep{witten2010framework, friedman2004clustering, brusco2001variable, gnanadesikan1995weighting}. We leverage this perspective to examine the MoE router, explicitly modeling the router's task as learning an assignment of the tokens such that similar tokens are grouped together with minimal intra-group distances. We then consider the role of learnable feature weights in solving a clustering optimization problem to discover maximally separated clusters and present an analytical solution for the optimal set of feature weights for any given routing assignment. We finally discuss the relevance of this solution for improving the routing assignment in the MoE layer before providing the full technical formulation of our AC router and ACMoE in the next section.

We begin by examining the MoE router through the lens of feature-weighted clustering  \citep{witten2010framework, friedman2004clustering, brusco2001variable, gnanadesikan1995weighting}. We explicitly model the router's task as learning a token assignment that groups together similar tokens. We consider the role of learnable feature weights in solving a clustering optimization problem to optimally reveal latent clusters and present an analytical solution for the optimal weights for any given routing assignment. We finally discuss how this solution improves the MoE router before providing the full formulation of our AC router and ACMoE in the next section.

\subsection{Clustering Optimization}
Let $\bs h_i = [h_{i1}, \dots, h_{id}]^\top$ be the $i^{\mathrm{th}}$ hidden representation and $D_{ij}$ denote the distance between $\bs h_i$ and $\bs h_j$. Given a distance metric $\rho_{ijq}$ between $h_{iq}$ and $h_{jq}$ over the $q^{\mathrm{th}}$ dimension, the distance between $\bs h_i$ and $\bs h_j$ can be defined as $D_{ij}(\bs w) = \sum_{q\in [d]} w_{q}\rho_{ijq}$
for weights $\bs w = [w_1 , \dots, w_d]$ with $\sum_{q \in [d]} w_q = 1$ and $w_q \ge 0$ for all $q \in [d]$. The weights determine the global importance of the $q^{\mathrm{th}}$ feature to the overall distance among representations. 
% and can be adjusted by user domain knowledge if certain features are believed to be more relevant to the clustering process.

Cluster analysis aims to divide the input set of $N$ objects into groups, where objects within the same group are more similar to each other than to those in other groups. This is formalized using a classifier $r(i) = k$, assigning the $i^{\mathrm{th}}$ object to a group $k$. Then the optimal classifier $r^*$ minimizes a criterion $Q(r)$ that evaluates clustering quality:
\begin{align}\label{eq:general-clustering-opt}
    r^* = \arg \min_r Q(r) = \sum_{k \in [E]} \frac{1}{N_k^2} \sum_{r(i) = k} \sum_{r(j) = k} D_{ij}(\bs w).
\end{align}
We expect that different groupings will cluster on different subsets of features. In particular, we wish to model the scenario that groupings exist in different latent subspaces with varying dependence on possibly disjoint subsets of features. We therefore replace the global feature weight $\bs w$ in Eqn. \ref{eq:general-clustering-opt} with cluster-dependent feature weights, $\{\bs w_k\}_{k=1}^E$ for $E$ groups, which allows us to capture the differing feature dependencies of \textit{each} cluster. Then, we can adapt the optimization problem with these cluster-dependent feature weights as follows: 
\begin{align}\label{eq:ReCOP}
    (r^*, \{\bs w_k^*\}_{k=1}^{E}) = &\arg \min_{r, \{\mathbf{w}_k\}} \sum_{k \in [E]} \frac{1}{N_k^2} \sum_{r(i) = k} \sum_{r(j) = k} D^J_{ij}(\bs w_k), \nonumber\\
    \text{such that }&\sum_{q\in [d]}w_{qk} = 1, \quad \forall k \in [E],
\end{align}
where $D^J_{ij}(\bs w_k) = \sum_{l=1}^{d} w_{qk} \rho_{ijq} + \lambda J(\bs w_k)$ denotes the weighted distance between $i$ and $j$ combined with some regularization $J$ and regularization strength $\lambda$. 

To avoid point-mass solutions in which we assign all weight to the single best-clustering feature, we set the regularizer to the Kullback-Leibler divergence between the feature weights $\bs w$ and the uniform distribution $\bs u = (1/d, \dots, 1/d) \in \mathbb R^d$, denoted by $J(\bs w_k) = D_{\text{KL}}(\bs u \mid\mid \bs w_k)$. The regularization parameter $\lambda$ reflects our preference to maintain more or less features in the solution set.
% \tanm{why do we suddenly talk about regularizer here? We have no regularizer in Eqn. 5 and 6. If you would like to mention regularizer then 1) you should introduce it above or 2) Write down 1 more Eqn here, i.e., Eqn. 6 with regularizer.}

\subsection{MoE as Clustering Optimization}
Within the MoE framework with learnable routing, the router performs the role of the classifier $ r: \mathbb R^d \rightarrow [E]$, which is learned via gradient descent to optimize the final output loss\footnote{A top-$k$ router can straightforwardly be cast as the classifier in Eqn. \ref{eq:ReCOP} as $r: \mathbb R^d \rightarrow [E]^k$}. Therefore, we modify Eqn.~\ref{eq:ReCOP} by fixing $r$ and focusing just on optimizing the criterion with respect to cluster-wise feature weights $\bs w_k$. Under this interpretation, the router learns via backpropagation to optimally allocate representations to experts, with representations adaptively transformed to maximally reveal the clustering structure of the input data. Eqn.~\ref{eq:ReCOP} then becomes
\begin{align}\label{eq:ReCOP_2}
    \{\bs w_k^*\}_{k=1}^{E} = &\arg \min_{\{\mathbf{w}_k\}} \sum_{k \in [E]} \frac{1}{N_k^2} \sum_{r(i) = k} \sum_{r(j) = k} D^J_{ij}(\bs w_k), \nonumber\\
    \text{such that }&\sum_{q\in [d]}w_{qk} = 1, \quad \forall k \in [E].
\end{align}

The following theorem presents the optimal weights per feature $q$ and cluster $k$:

% We name the optimization problem given by Eqn.~\ref{eq:ReCOP} the \textit{regularized clustering optimization problem} (ReCOP) for the ease of referencing later.

\begin{theorem}[Optimal feature weights]\label{thm:ReCOP-solution}
    Let $s_{qk} := N_k^{-2}\sum_{r(i) = k} \sum_{r(j) = k} \rho_{ijq}$ be a measure of dispersion on the $q^{\mathrm{th}}$ feature for the representations assigned to cluster $k$. Then, for a given router function $r : \mathbb R^d \to [E]$, the corresponding optimal weights $\{\bs{w}_k\}_{k \in [E]}$ that minimize the feature-weighted clustering optimization problem in Eqn. \ref{eq:ReCOP_2} are given by 
    \begin{align}\label{eq: optimal weights}
        w_{qk} = \frac{\lambda/d}{s_{qk}+\alpha_{k}}
    \end{align}
    for $(q, k) \in [d]\times [E]$, where $\{\alpha_k\}_{k \in [E]}$ are constants that for any $\lambda>0$ satisfy
    \begin{align}\label{eq:alpha-condition}
        \sum_{q \in [d]} \frac{1}{s_{qk} + \alpha_k} = \frac{d}{\lambda}.
    \end{align}
\end{theorem}
The existence of $\alpha_k$ satisfying Eqn.~\ref{eq:alpha-condition} and the proof of Theorem \ref{thm:ReCOP-solution} is provided in Appendix \ref{appendix:ReCOP-solution-proof}. The optimal weights for a cluster $k$ given in Eqn. \ref{eq: optimal weights} take an intuitive form in that they are inversely proportional to the measure of dispersion in cluster $k$ along each dimension, $\bs w_{k} \propto [ \frac{1}{s_{1k}}, \dots, \frac{1}{s_{dk}}] $. Hence, the optimal cluster-wise feature weights scale features according to their contribution to forming tight clusters. Specifically, the solution weights upweight a feature $q$ if cluster $k$ clusters tightly (has small dispersion $s_{qk}$) along the feature $q$ and downweights a feature $p$ if cluster $k$ clusters loosely (has large dispersion $s_{pk}$) along feature $p$.

% by defining cluster-dependent feature weights $\mathbf{w}_k = \{w_{lk}\}_{l=1}^{d}$ for each group $k \in [E]$, and jointly minimizing the clustering objective $Q(e, \{\mathbf{w}_k\}_{k=1}^E)$ with respect to the encoder $e$ and the cluster-wise feature weights $\bs w_k$. Replacing the global feature weights $\bs w$ in Eqn. \ref{eq:general-clustering-opt}, with the cluster-wise weights $\{\bs w_k \}_{k=1}^E$

% The optimization problem becomes \tanm{In Eqn. 5, you optimize wrt. to $e$, but I do not see $e$ in the objective function or constraints. We need to make $e$ explicitly in Eqn. 5}:

% that constants $\alpha_k$ can be found in the intervals $(S_{lk} - S_{l'k})$ for 
% \begin{lemma}\label{lemma:alpha-condition}
%     For any $\lambda > 0$, Eqn.~\ref{eq:alpha-condition} has exactly $d$ real solutions with respect to $\alpha_l$.
% \end{lemma}

% [IN PROGRESS] Let $\mathcal{I}_l := \{i : c(i) = l\}$. Then, a clustering on a subset of  the following optimization problem to determine the optimal clustering $c$ and weights $\bs{w}_{l}$:
% \begin{align}\label{eq:cosa-optimization}
%     &\min_{\bs{w}_l} Q(c, \{\bs{w}_{l}\}_{l\in [L]}) = \sum_{l \in [L]} \frac{W_l}{N_l^2}\sum_{i,j \in \mathcal{I}_l}\left[\sum_{k \in [d]}\left(w_{kl}d_{ijk} + \lambda w_{kl}\log w_{kl}\right) + \lambda \log d\right], \nonumber\\
%     &\text{such that }\sum_{k\in [d]}w_{kl} = 1, \quad \forall l \in [L]. 
% \end{align}
% Method of Lagrange multipliers turns this constrained optimization problem into the following unconstrained counterpart:
% \begin{align}
%     \min_{\bs{w}_l, \bs{\alpha}} \mathcal{L}(c, \{\bs{w}_{l}\}_{l\in [L]}, \bs{\alpha}) = \sum_{l \in [L]} \frac{W_l}{N_l^2}\sum_{i,j \in \mathcal{I}_l}\sum_{k \in [d]}\left(w_{kl}d_{ijk} + \lambda w_{kl}\log w_{kl}\right) + \sum_{l \in [L]} \alpha_l \left(\sum_{k\in [d]}w_{kl} - 1\right), \nonumber
% \end{align}
% where $\bs{\alpha} = \begin{bmatrix}
%     \alpha_1 & \dots & \alpha_L
% \end{bmatrix}^\top$ is the vector of Lagrange multipliers. Note that the last optimization problem can be separated into the following $L$ independent optimization subproblems:
% \begin{align}
%     \min_{\bs{w}_l, \bs{\alpha}} \mathcal{L}_l(c, \{\bs{w}_{l}\}_{l\in [L]}, \bs{\alpha}) = \frac{W_l}{N_l^2}\sum_{i,j \in \mathcal{I}_l}\sum_{k \in [d]}\left(w_{kl}d_{ijk} + \lambda w_{kl}\log w_{kl}\right) +  \alpha_l \left(\sum_{k\in [d]}w_{kl} - 1\right), \nonumber
% \end{align}
% for $l \in [L]$. By setting the derivatives of $\mathcal{L}_l$ with respect to both optimization variables, we obtain the following system of equations:
% $$\begin{cases}
%     \dfrac{\partial \mathcal{L}_l}{\partial w_{kl}} = W_l S_{kl} + \lambda W_l(\log w_{kl} + 1) + \alpha_l = 0, \\
%     \dfrac{\partial \mathcal{L}_l}{\partial \alpha_l} = \dsum_{k\in [d]}w_{kl} - 1 = 0,
% \end{cases}$$ 

This method enables the MoE router to perform better token-expert matching. The cluster-wise feature weights $\bs w_k$ capture the features on which the $k^{\mathrm{th}}$ expert is specialized, as large weights indicate those features are highly important to the identification of that expert cluster and small weights indicate those features are unimportant to identification of that expert cluster. Then, we can use $\bs w_k$ to scale the tokens to accentuate their features according to the specialization of the experts, thereby allowing the router to best identify the most suitable expert for each token. Note that this solution is local in that we learn the optimal weights adaptively $\textit{per cluster}$, obtaining $\bs w_k$ for all $k \in [E]$, and so we compute a unique scaling of the feature space adaptively $\textit{per cluster}$ as well. Integrating these cluster-dependent weights which scale the feature space according to the identification of each expert into the MoE router obtains our AC routing method and corresponding ACMoE. We detail the AC router and ACMoE fully in the next section. 

% This method enables the MoE router to perform better token-expert matching. The cluster-wise feature weights $\bs w_k$ capture the features on which the $k^{\mathrm{th}}$ expert is specialized, as large weights indicate the expert cluster is tightly defined on those features and small weights indicate the expert cluster is weakly defined on those features. Then, we can use $\bs w_k$ to scale the tokens to accentuate their features according to the specializations of the experts, thereby allowing the router to best identify the most suitable expert for each token. Note that this solution is local in the sense that we learn the feature-wise optimal weights adaptively $\textit{per cluster}$, obtaining $\bs w_k$ for all $k \in [E]$, and so we compute a unique scaling of the feature space adaptively $\textit{per cluster}$ as well. Integrating these cluster-dependent weights which scale the feature space according to the specialization of each expert into the MoE router obtains our AC routing method and corresponding ACMoE. We detail the AC router and ACMoE fully in the next section.

% We use these optimal weights per cluster, $\bs w_k$ for $k \in [E]$, to scale the feature space when computing the routing assignment for each token $\bs h$. This transformation optimally reveals the latent clustering structure of the input space, thereby allowing the router to discover the optimal input-expert match more easily. This gives rise to our AC routing method and corresponding ACMoE, which we detail fully in the following section.

% For a token representation $\bs h$ with prior information on its clustering assignment $e(\bs h) = k^*$, we use the solution weights in Theorem \ref{thm:ReCOP-solution} to scale the feature space when computing the routing assignment for $\bs h$, thereby accentuating its features that best determine its expert allocation. A natural way to obtain a good estimate of the clustering assignment of $\bs h$ in any deep MoE architecture is by using its expert assignment in the previous layer. This gives rise to our AC routing method and corresponding ACMoE, which we detail fully in the following section.

% \subsection{Mixture of Experts and Gaussian Mixture Models}

% Mixture-of-Experts architectures trace their roots back to Gaussian mixture models in which the input space is assumed to be divisible into separate regions with an expert specializing in each region \citep{jacobs1991adaptive}. We leverage this theoretical framework and assume inputs, $\bs x$, and corresponding hidden representations, $\bs h \in \mathbb R^d$, are sampled from a Gaussian mixture model given by \begin{align}
%     \bs h \sim \sum_c^E w_c \bs g_c, \quad \bs g_c = \mathcal N(\bs \mu_c, \bs \Sigma_c) \quad c \in [E],
% \end{align} 
% where $w_c \in [0,1]$ denote the mixture weights and $\{ g_c = \mathcal N(\bs \mu_c,\bs \Sigma_c)\}_{c=1}^E$ denote the component Gaussians with mean $\bs \mu_c \in \mathbb R^d$ and variance matrix $\bs \Sigma_c \in \mathbb R^{d \times d}$ of the mixture distribution $\sum_{c=1}^E w_cg_c$. Note we do not impose any spherical requirement on the variance structure of the component Gaussians. We furthermore assume the input space is reasonably well-separated such that component clusters do not completely overlap. Formally, we assume that mixture components $\{ g_c = \mathcal N(\bs \mu_c,\bs \Sigma_c)\}_{c=1}^E$ are c-separated in the input space such that for pairs of Gaussians $i,j$ we have $\| \bs \mu_i - \bs  \mu_j \|_2 \ge c\max\{\sigma_i^{d^*}, \sigma_j^{d^*}\}\sqrt{d}$ for some $c \geq 0$ where $\sigma_i^{d^*}$ is the largest diagonal entry of $\bs \Sigma_i$. The term $\sigma_i^{d^*}\sqrt{d}$ captures the radius of a Gaussian component in the largest variance direction. This notion of c-separation for Gaussian models is an extension of the spherical case presented in \cite{dasgupta2007probabilistic}.

\section{A Tight Cluster is a Specialized Expert} \label{sec: a tight cluster}

In this section, we demonstrate how we implement the solution weights from the clustering optimization problem in Eqn. \ref{eq: optimal weights} into the MoE routing mechanism, thereby obtaining the Adaptive Clustering router. We then provide the full technical formulation of our proposed routing method and corresponding ACMoE model. We also present theoretical results on how computing the routing assignments according to our framework promotes faster convergence and robustness.

\subsection{Full Technical Formulation}

We integrate the weights from Eqn. \ref{eq: optimal weights} into the Adaptive Clustering router transformation in Definition \ref{def: router transformation} which, for a cluster $k$, scales the dimensions of the feature space according to the $k^{\mathrm{th}}$ expert's specialization on those features. Formally this is:

\begin{definition}[Adaptive Clustering Router Transformation $\bs M_k$] \label{def: router transformation} Let $ \mathcal{C}^{\ell}_k = \{\bs h_1^\ell, \dots \bs h_{N_k}^\ell \}$ be the representations assigned to expert $k$ at layer $\ell$. Let $s^\ell_{qk} \in \mathbb R$ be a measure of a spread in the $q^{\mathrm{th}}$ dimension for cluster $k$, such as mean absolute deviation $s^\ell_{qk} = \frac{1}{N_k} \sum_{i \in \mathcal{C}^\ell_k} |\bs h_{iq}^\ell - \Bar{\bs h}_q^\ell|$. Then, the cluster-dependent router transformation for expert $k$ at layer $\ell$ is given by a diagonal matrix $\bs M_k^{\ell} := \mathrm{diag}(1/s^\ell_{1k}, \dots, 1/s_{dk}^\ell)$.
\end{definition}

We use the transformation $\bs M_k$ in Definition \ref{def: router transformation} to adaptively scale the feature space in which we perform token-expert matching. This obtains our Adaptive Clustering router and corresponding ACMoE layer, described in the following definition.

% The transformation $\bs M_k$ in Definition \ref{def: router transformation} is then used to scale the dimensions of the space in which token-expert assignment takes place. Specifically, for a token coming from cluster $k$, we use $\bs M_k$ to upweight features on which the corresponding expert $k$ is specialized. This produces the Adaptive Clustering (AC) router, described in the following definition.

\begin{definition}[Adaptive Clustering Router and MoE Layer] \label{def: AC router} Let $\bs h^\ell \in \mathbb R^d$ be the hidden representation of an input, $\bs e_1^\ell, \dots, \bs e_N^\ell \in \mathbb R^d$ be expert embeddings at layer $\ell$. Let $\bs h^{\ell -1} \in \mathcal C^{\ell-1}_{k^*}$ have been assigned to expert $k^*$ in the previous layer. Let $\bs M^{\ell-1}_{k^*} \in \mathbb R^{d \times d}$ be the Adaptive Clustering transformation (Definition \ref{def: router transformation}) for input $\bs h$ at layer $\ell-1$. Let $g(\cdot)$ be the softmax function. Then the following equations describe the Adaptive Clustering router (Eqn. \ref{eq: ac router}) and overall ACMoE layer (Eqn. \ref{eq: acmoe}):
\begin{align}
    \mathcal K &:= \mathrm{topk}_k (s_k) = \mathrm{topk}_k (\bs h^{\ell\top} \bs M^{\ell-1}_{k^*} \bs e^\ell_k)\label{eq: ac router}\\
     f^{\mathrm{ACMoE}}(\bs h^\ell) &= \bs h^\ell + \sum_{k \in \mathcal K} g(\bs h^{\ell\top} \bs M^{\ell-1}_{k^*} \bs e^\ell_k) f_k^{\mathrm{FFN},\ell}(\bs h^\ell)\label{eq: acmoe}.
\end{align}
\end{definition}

% \begin{remark}
%     The Adaptive Clustering router computes a dot-product between $\bs h$ and experts $\bs e_k$ with the dimensions scaled inversely by the measures of spread $ s_i = [\bs M_{k^*}]_{ii}$ in the $k^*$ cluster. In this sense, the AC routing is proportional to Gaussian distance where the the inverse of $\bs M_{k^*}$ estimates the variance of the $k^*$ cluster under the assumption that the Gaussian clusters are axis aligned. This distance metric has the interpretation of pushing away $\bs h$ from candidate experts lying in directions that $\bs h$ clusters weakly on.
% \end{remark}

% The corresponding Adaptive Clustering MoE layer is then obtained by applying the AC router in Definition \ref{def: AC router} within the standard MoE layer. Formally this is:

% \begin{definition}[Adaptive Clustering MoE Layer (ACMoE)] \label{def: ACMoE} Let $\bs h^\ell \in \mathbb R^d$ be the hidden representation of an input, $\bs e_1^\ell, \dots, \bs e_k^\ell, \dots, \bs e_N^\ell \in \mathbb R^d$ be expert embeddings with corresponding expert feed-forward networks $f_{k}^{\mathrm{FFN}, \ell}$ for expert $k \in [N]$ at layer $\ell$. Let $\bs h^{\ell -1} \in \mathcal C^{\ell-1}_{k^*}$ have been assigned to expert $k^*$ in the previous layer. Let $\mathcal K$ be the set of top k experts selected by the AC router and $\bs M^{\ell -1}_{k^*}$ be the AC routing transformation, as given in Definition \ref{def: AC router}. Let $g(\cdot)$ be the softmax function. Then the ACMoE layer is given by:
% \begin{align}\label{eqn: acmoe}
%     f^{\mathrm{ACMoE}}(\bs h^\ell) &= \bs h^\ell + \sum_{k \in \mathcal K} g(\bs h^{\ell\top} \bs M^{\ell-1}_{k^*} \bs e^\ell_k) f_k^{\mathrm{FFN},\ell}(\bs h^\ell).
% \end{align}

% \end{definition}

\begin{remark}\label{remark: recovering standard routing}
    We see from Eqns. \ref{eq: ac router} and \ref{eq: acmoe} that the standard MoE layer is recovered by setting the AC router transformation to the identity matrix, $\bs M_{k} = \bs I_d$ for all $k \in [E]$. Within our framework then, standard routing schemes implicitly assume all experts $k \in [E]$ depend equally on all dimensions.
\end{remark} 

\begin{remark}
    The Adaptive Clustering router computes a dot-product between $\bs h$ and experts $\bs e_k$ with the dimensions scaled by the weights in $\bs M_{k^*}$ and so is proportional to a Mahalanobis distance. Under this interpretation, we soft project the tokens and expert embeddings onto the axes of the feature space that best identify the expert cluster $k^*$.
\end{remark}

\textbf{Implementation details.} Given ACMoE requires the expert assignment from the previous layer to compute the routing assignment (Eqn. \ref{eq: ac router}), ACMoE is only implementable after the first layer. Furthermore, we scale the measures of dispersion in $\bs M_k^{\ell} = \mathrm{diag}(1/s^\ell_{1k}, \dots, 1/s_{dk}^\ell)$ to have mean 1. This is to remove the effect of different clusters or features having different absolute magnitudes. Our method is concerned with identifying the key sets of features that contribute more or less to identification of the expert clusters, and so we wish to compute our scaling in a relative sense.

% The first is that our proposed method aims to accentuate the features that contribute to tight clustering and drop the features that do not contribute to tight clustering. Hence we care about the relative importance of the features compared to each other, as opposed to their absolute values, and so we index the mean dispersion as 1 and let

\subsection{Adaptive Clustering Promotes Robustness and Fast Convergence}
We now present theoretical propositions on the improved robustness and convergence speed of our method. The robustness of our method follows from better separation of expert-clusters. This produces a more stable assignment in which the probability of erroneously sending a token to unsuitable nearby experts decays exponentially with increased inter-cluster distance. Faster convergence follows from our AC routing method improving the conditioning on the Hessian of the loss with respect to the expert embeddings, enabling faster and more stable convergence of the router.

\textbf{Promoting robustness.} We begin with Lemma \ref{lemma:mean-distance} stating that our AC transformation (Definition \ref{def: router transformation}) increases the separation between clusters in the transformed space, followed by Lemma \ref{lemma:incorrect-cluster}, which provides an explicit expression for the probability of incorrect expert assignment. To give the probability bound an exact form, we assume the cluster structure can be modeled as a Gaussian mixture model (GMM). We note that GMMs are a highly expressive and general framework, so this assumption does not place significant restrictions on our robustness analysis. We further assume that though clusters may overlap, they are well-separated along the features for which they cluster tightly\footnote{Intuitively, this assumption captures the natural property that the semantic regions of the input space are distinct along the dimensions that best identify them.}. 

% Note this assumption is actually a relaxation of assumptions in existing work on the well-separatedness of the latent clusters in MoE models \citep{dikkala2023benefits}.

% Increasing the separation between latent clusters serves robustness and fast convergence. For robustness, better separation between clusters produces more stable routing and reduces the possibility of erroneous expert assignment. We show in Proposition \ref{prop: robustness} how better separation of clusters causes the probability of erroneous expert assignment to decrease exponentially. For convergence, better separation is linked to faster parameter estimation of the underlying Gaussian components. We show in Proposition \ref{prop: convergence} how the rate of convergence improves due to the degree of overlap between expert clusters decreasing.

\begin{lemma}[Adaptive Clustering Router Transformation Increases Cluster Separation]\label{lemma:mean-distance}
Let the data be generated from a Gaussian mixture model with components, $g_c = \mathcal{N}(\bs \mu_c, \bs \Sigma_c)$ for $c \in [E]$. Without loss of generality, consider two expert clusters $c \in \{a,b\}$ where a token representation $\bs h \sim g_a$ belongs to cluster $a$. Let $\bs M_a = \mathrm{diag}(1/s_{1a}, \dots, 1/s_{da})$ be the router transformation constructed from the feature-wise dispersions, $s_{qa}$, of cluster $g_a$ for each feature $q \in [d]$ as given by Definition \ref{def: router transformation}. Then the distance between cluster means in the $\bs M_a$-transformed space, defined as $\| \bs \mu_{k} - \bs \mu_a \|^2_{\bs M_a} := (\bs\mu_{k} - \bs\mu_a)^\top \bs M_a (\bs\mu_{k} - \bs\mu_a)$, is larger than in the original Euclidean space:
    $\| \bs \mu_{k} - \bs \mu_a \|^2_{\bs M_a}  \ge \|\bs\mu_{k} - \bs\mu_a\|^2.\label{eq:larger-mean-distance}$
\end{lemma}

The proof is provided in Appendix \ref{appendix:robustness-proof}. In Lemma \ref{lemma:incorrect-cluster}, we derive the probability of mis-assignment as a function of inter-cluster distance, showing how separation mitigates the effect of perturbations.

\begin{lemma}[Incorrect Assignment Probability]\label{lemma:incorrect-cluster}
    Let $\bs h \sim \mathcal{N}_{k^*}(\bs \mu_{k^*}, \bs\Sigma_{k^*})$ be a representation belonging to cluster $k^*$. Let $\bs h' = \bs h + \bs \eps$ be contaminated by some 0-mean noise $\bs \eps \sim (\bs 0, \bs \Sigma_{\epsilon})$. Let $k$ be the nearest, incorrect cluster to $k^*$. Let the inter-cluster mean distance between $k^*$ and $k$ be given by $ \| \delta\bs\mu \| := \| \bs \mu_{k^*} - \bs \mu_k \| $. Let the routing assignment be given by $r: \mathbb R^d \rightarrow [E]$ and denote the cumulative density of a standard normal distribution by $\Phi$. Then the probability of incorrect assignment is:
    \begin{align}
        \Pr(r(\bs h') \neq k^*) = 1 - \Phi\left(\frac{\|\delta\bs\mu\|^2}{2\sqrt{\delta\bs\mu^\top(\bs\Sigma_{k^*} + \bs\Sigma_{\epsilon})\delta\bs\mu}}\right). \label{eq:incorrect-cluster-prob}
    \end{align}
\end{lemma}

\begin{remark}
    It is worth noting that since $1 - \Phi(x) \sim (\sqrt{2\pi}x)^{-1}e^{-x^2/2}$ for large $x$ and $\sqrt{\delta\bs\mu^\top(\bs\Sigma_{k^*} + \bs\Sigma_{\epsilon})\delta\bs\mu} = O(\|\bs\mu\|)$, we find that the probability of incorrect cluster assignment as given by Eqn.~\ref{eq:incorrect-cluster-prob}, $\Pr(r(\bs h') \neq k^*) = e^{-O(\|\delta\bs\mu\|^2)}$ is an exponentially decreasing function in $\| \delta \bs \mu\|$.
\end{remark}

The proof is provided in Appendix \ref{appendix:robustness-proof}. Combining Lemmas \ref{lemma:mean-distance} and \ref{lemma:incorrect-cluster}, we directly obtain that the probability of erroneous assignment using the AC router is exponentially smaller than under a standard routing scheme. This is formalized in Proposition \ref{prop: robustness}, given by:

\begin{prop}[Robustness of ACMoE] \label{prop: robustness} 
Consider an expert assignment setting for the representation $\bm h \sim \mathcal{N}_{k^*}(\bs \mu_{k^*}, \bs\Sigma_{k^*})$ as in Lemma \ref{lemma:incorrect-cluster} with two routers given by $r: \mathbb R^d \rightarrow [E]$ and $r^{\mathrm{AC}}: \mathbb R^d \rightarrow [E]$ for standard (Eqn.~\ref{eq:standard-router}) and AC routers (Definition \ref{def: AC router}), respectively. Then the probabilities of incorrect assignments of routers $r$ and $r^{\mathrm{AC}}$ satisfy   $\Pr\left(r^{\mathrm{AC}}(\bs h') \neq k^*\right)  \le \Pr\left(r(\bs h') \neq k^*\right)$.
\end{prop}

\textbf{Promoting faster convergence.} For an expert embedding $\bs e_k \in \mathbb R^d$ and associated cluster $\mathcal C_k$, our AC router in Definition \ref{def: AC router} adaptively spheres $\mathcal C_k$ by stretching the feature space with weights inversely proportional to the coordinate-wise dispersion in $\mathcal C_k$. This reduces the conditioning number of the Hessian of the loss with respect to the expert $\bs e_k$, improving the loss landscape and enabling faster and more stable convergence of the router. This notion is formalized in Proposition \ref{prop: convergence}:

% To show the faster convergence rate of our proposed method, we rely on the classic asymptotic convergence result of \cite{ma2000asymptotic} which shows that the rate of convergence towards the true parameters of a Gaussian mixture model depends on the degree of overlap between component gaussians. Our AC method, by computing routing assignments in the adaptively transformed space as determined by $\bs M_k$ as described in Definition \ref{def: router transformation}, decreases the overlap between component models. This happens through two channels, one is by the increase inter-cluster means as given by Lemma \ref{lemma:mean-distance} and the second is by the approximate sphering of the latent clusters by scaling the features inversely by measures of spread. Formally, we have:

% \begin{prop}[Convergence of ACMoE] \label{prop: convergence} Let $\Theta$ denote the set of all parameters in a Gaussian mixture model, including the mixing probabilities $p_i$, means $\bs \mu_i$, and variance matrices $\bs \Sigma_i$ for $i \in [E]$. Let $\bs h$ be sampled from the mixture model. Let $\bs h \sim g_{k^*}$ be a representation from the $k^*$ cluster. Let $\bs M_{k^*} = \diag(1/s_1, \dots, 1/s_d)$ be the corresponding AC routing transformation for the $k^*$ cluster. Furthermore let $\lambda^j_{\mathrm{max}}$ for $j \in [k^*, k]$ denote the largest eigenvalue of $\bs \Sigma_j$ and $s^{k^*}_min$ denote the minimal spread for cluster $k^*$. Then, the asymptotic rate of convergence for ACMoE is faster than standard MoE, with convergence speed given by
% \begin{align}
%     \lim_{N \rightarrow \infty} \| \hat{\Theta}^{\mathrm{ACMoE}} - \Theta \| = \mathcal{O}\left(\frac{\sqrt{{\lambda^k_{\mathrm{max}}/s^{k^*}_{\mathrm{min}}}}}{\| \bs \mu_{k^*} - \bs \mu_k \|_{\bs M_{k^*}}} \right) \leq \mathcal{O}\left(\frac{\sqrt{{\lambda^k_{\mathrm{max}} \lambda^{k^*}_{\mathrm{max}}}}}{\| \bs \mu_{k^*} - \bs \mu_k \|} \right) = \| \hat{\Theta}^{MoE} - \Theta \| \lim_{N \rightarrow \infty}.
% \end{align}
    
% \end{prop}
\begin{prop}[Faster convergence of ACMoE]\label{prop: convergence}
    Let $\mathcal{L}^{\mathrm{MoE}} : \bs\Theta \to \mathbb R_+$ and $\mathcal{L}^{\mathrm{ACMoE}} : \bs\Theta \to \mathbb R_+$ be the network loss functions defined on the whole parameter set $\bs\Theta$ when employing the standard (Eqn.~\ref{eq:standard-router}) and AC routers (Definition \ref{def: AC router}), respectively. Let $\kappa(\bs A) = \lambda_{\mathrm{max}} / \lambda_{\mathrm{min}}$ denote the conditioning number of a matrix $\bs A$ with largest and smallest eigenvalues $\lambda_{\mathrm{max}}$ and $\lambda_{\mathrm{min}}$ respectively. Let the Hessian of an $i^{\mathrm{th}}$ expert be given by $\nabla_{\bm e_{i}}^2$. Then for each $i \in [E]$ the following holds with high probability
    \begin{align}
        \kappa\left(\nabla_{\bm e_i}^2 \mathcal{L}^{\mathrm{ACMoE}}\right) \le \kappa\left(\nabla_{\bm e_i}^2 \mathcal{L}^{\mathrm{MoE}}\right) \label{eq:condition-numb-ineq}
    \end{align}
\end{prop}

% \begin{remark}
%     Main claim of Proposition \ref{prop: convergence}, Eqn.~\ref{eq:condition-numb-ineq}, implies that the Hessian matrices of loss functions corresponding to individual expert embeddings are better conditioned when AC router is used to assign experts to the tokens. This property ensures faster and more stable convergence of learnable expert embeddings and the network loss in general.
% \end{remark}

\begin{remark}
    Faster convergence of ACMoE can also be argued from the perspective of learning Gaussian mixture models with Expectation Maximization \citep{dempster1977maximum}. The classic result of \citet{ma2000asymptotic} shows the convergence rate to the true parameters depends on the overlap between component Gaussians. Our AC method adaptively transforms the input space with by $\bs M_k$ (Definition \ref{def: router transformation}), which decreases component overlap by increasing inter-cluster distances.
\end{remark}

The proof is provided in Appendix \ref{appendix:convergence-proof}. We find this result empirically supported as shown by the rapid convergence in Fig. \ref{fig: convergence}.


% \subsection{Promoting Expert Specialization and Faster Convergence}

% Our method identifies the most important features for determining the expert allocation of each token, upweighting features that produce tight expert clusters and downweighting features that produce dispersed expert clusters. This promotes faster convergence as experts are trained on more semantically similar inputs. This is formalized in Proposition \ref{prop: convergence}.




% \subsection{Promoting Robustness Through Stable Assignment}

% Given the AC routing scheme adaptively scales the feature space according to the most informative dimensions for determining the expert allocation of each token, token contamination is less able to erroneously match tokens to incorrect experts. This is because expert clusters are better separated in the transformed space, thereby decreasing the possibility that perturbations could move tokens into the wrong cluster. We formalize this notion by first presenting Lemma \ref{lemma:mean-distance}, which shows that the distance between clusters increases when using the AC transformation provided in Definition \ref{def: router transformation}. Proposition \ref{prop: robustness} then establishes the robustness of ACMoE by showing that the probability of incorrect expert assignment decreases exponentially in the inter-cluster distance.

% The proof of Lemma \ref{lemma:mean-distance} is provided in Appendix \ref{appendix:robustness-proof}. 

\section{Experimental Results} \label{sec: experiments}

In this section, we empirically justify the advantage of ACMoE over baseline MoE models. We evaluate our method on large-scale tasks including Wikitext-103 \citep{merity2016pointer} language modeling and ImageNet \citep{deng2009imagenet} object classification. We implement our AC router into Switch Transformer \citep{fedus2022switch}, Generalist Language Model (GLaM) \citep{du2022glam}, and Swin Transformer \citep{liu2021swin} backbones and compare our router against the standard Sparse Mixture-of-Experts (SMoE) router \citep{shazeer2017sparsely} and the XMoE router \citep{chi2022representation}. We show that i) ACMoE obtains substantive improvements over baseline models across both language and vision tasks; ii) ACMoE offers robust improvements on contaminated and out-of-distribution samples; and iii) ACMoE attains these gains without introducing any learnable parameters and with negligible additional computational overhead. Results are averaged over 5 runs with different seeds.

\subsection{Language Modeling} 

\textbf{Experimental Setup.} We adopt the experimental setup of 
\citet{pham2024competesmoe}. We compare ACMoE with Switch
Transformer and GLaM baselines with 16 total experts in small (70M parameters) and medium (220M parameters) configurations with top-2 expert routing. We present pretraining test perplexity (PPL) results for Wikitext-103 and test bytes-per-character (BPC) for character-level EnWik-8 . We report top-1 accuracy for finetuning classification tasks on the 2-class Stanford Sentiment Treebank-2 (SST2) \citep{socher2013recursive}, 5-class Stanford Sentiment Treebank-5 (SST5) \citep{socher2013recursive}, and 77-class Banking-77 (B77) \citep{casanueva2020efficient}. Full experimental details are provided in Appendix \ref{appendix: experimental details}. 

\textbf{Pretraining and Finetuning.} Table \ref{table: wikitext103 pretrain} shows
ACMoE attains top test PPL on WikiText-103 language modeling in Switch and GLaM backbones at small and medium configurations under baseline SMoE and XMoE routers. The improvement in the GLaM-medium architecture is a particularly substantive 4.8\% over the next best baseline. Table \ref{table: switch pretrain} shows ACMoE pretrained models on both WikiText-103 and EnWik-8 surpass the performance of baselines in finetuning tasks, with strong, consistent improvements of approximately 3\%, showing ACMoE's strong performance carries over to finetuning.

\begin{table}[t]
    \small
  % \caption{Top-1 test accuracy in Stanford Sentiment Treebank 2, 5 (SST2, SST5), and Banking-77 (B77) finetuning classification for WikiText-103 and EnWik-8 pretrained models. }
  \caption{Test Perplexity (PPL) and bytes-per-character (BPC) pretraining and top-1 test accuracy on Stanford Sentiment Treebank 2, 5 (SST2, SST5), and Banking-77 (B77) finetuning classification.}
  \vspace{-0.14in}
  \label{table: switch pretrain}
  \centering
  \begin{tabular}{lcccc}
    \toprule
    \text{Model} & \text{Test BPC / PPL ($\downarrow$)} & \text{SST2 ($\uparrow$)}  & \text{SST5 ($\uparrow$)}  & \text{B77 ($\uparrow$)} \\
    \midrule
    \midrule
    \multicolumn{5}{c}{\textit{EnWik-8 Pretrain}}  \\
    \midrule
    \midrule
    \textit{Switch Transformer} \citep{fedus2022switch} & 1.153 & 63.27 & 32.21 & 53.48 \\
    \text{Switch-ACMoE} \textbf{(Ours)} & \textbf{1.137} & \textbf{64.45} & \textbf{33.79} & \textbf{54.26} \\
    \midrule
    \midrule
     \multicolumn{5}{c}{\textit{WikiText-103 Pretrain}}  \\
    \midrule
    \midrule
    \textit{Switch Transformer} \citep{fedus2022switch} & 35.48 & 76.27 & 39.13& 83.82 \\
    \text{Switch-ACMoE} \textbf{(Ours)} & \textbf{34.42} & \textbf{77.32} & \textbf{40.04} & \textbf{86.01} \\
    \midrule
     \textit{GLaM} \citep{du2022glam} & 38.27 & 69.97 & 33.69 & 80.89  \\
    \text{GLaM-ACMoE} \textbf{(Ours)} & \textbf{36.26} & \textbf{71.90} & \textbf{34.24} & \textbf{82.33} \\
    \bottomrule
  \end{tabular}
  \vspace{-0.1in}
\end{table}

\begin{table}[t]
    \small
  \caption{Perplexity (PPL) on WikiText-103 contaminated by Text Attack.}
  \vspace{-0.14in}
  \label{table: robust language}
  \centering
  \begin{tabular}{lcc}
    \toprule
    \text{Model} & \text{Clean Test PPL ($\downarrow$)} & \text{Contaminated Test PPL ($\downarrow$)} \\
    \midrule
    \midrule
    \textit{Switch Transformer} \citep{fedus2022switch} & 35.48 & 48.12 \\
    \text{Switch-ACMoE \textbf{(Ours)}} & \textbf{34.42} & \textbf{47.61}  \\
    \midrule
    \textit{GLaM} \citep{du2022glam} & 38.27 & 50.84 \\
    \text{GLaM-ACMoE} (\textbf{Ours}) & \textbf{36.26} & \textbf{47.91}  \\
    \bottomrule
  \end{tabular}
  \vspace{-0.15in}
\end{table}
\begin{wraptable}[20]{r}{0.49\textwidth}
    \small
    \vspace{-0.16in}
  \caption{WikiText-103 test PPL of ACMoE and baseline GLaM and Switch.}
  \vspace{-0.14in}
  \label{table: wikitext103 pretrain}
  \centering
  \begin{tabular}{lc}
    \toprule
    \text{Router} & \text{Test PPL ($\downarrow$)}  \\
    \midrule
    \midrule
    \multicolumn{2}{c}{\textit{Switch Transformer} \citep{fedus2022switch}} \\
    \midrule
    \midrule
    \textit{SMoE-small} \citep{shazeer2017sparsely} & 87.94 \\
    \textit{XMoE-small} \citep{chi2022representation} & 87.21 \\
    \text{ACMoE-small} (\textbf{Ours}) & \textbf{85.07}  \\
    \midrule
    \textit{SMoE-medium} \citep{shazeer2017sparsely} & 35.48 \\
    \textit{XMoE-medium} \citep{chi2022representation} & 35.88 \\
    \textit{\textcolor{black}{StableMoE-medium}} \citep{dai-etal-2022-stablemoe} & \textcolor{black}{35.33} \\
    \text{ACMoE-medium} (\textbf{Ours}) & \textbf{34.42}  \\
    \midrule
    \midrule
    \multicolumn{2}{c}{\textit{GLaM} \citep{du2022glam}} \\
    \midrule
    \midrule
    \textit{SMoE-small} \citep{shazeer2017sparsely} & 58.27 \\
    \textit{XMoE-small} \citep{chi2022representation} & 54.80 \\
    \text{ACMoE-small} (\textbf{Ours}) & \textbf{54.55}  \\
    \midrule
    \textit{SMoE-medium} \citep{shazeer2017sparsely} & 38.27 \\
    \textit{XMoE-medium}  \citep{chi2022representation} & 38.10 \\
     \textit{\textcolor{black}{StableMoE-medium}} \citep{dai-etal-2022-stablemoe} & \textcolor{black}{38.04} \\
    \text{ACMoE-medium} (\textbf{Ours}) & \textbf{36.26}  \\
    \bottomrule
  \end{tabular}
  \vspace{-0.2in}
\end{wraptable}

\textbf{Robust Language Modeling.} Table \ref{table: robust language} show test PPL on WikiText-103 contaminated by Text Attack, where words are randomly swapped with a generic token 'AAA'. 
We follow the setup of \citet{han2024designing,teo2025unveiling,abdullaev2025transformer} and assess models by training them on clean data before attacking the test data using an attack rate of 2.5\%. ACMoE outperforms baseline Switch and GlaM with particularly robust performance in the GLaM backbone, surpassing GLaM by 5.8\%.

\subsection{Image Classification} 

\textbf{Experimental Setup.} We adopt the experimental setup of \citet{liu2021swin} for pretraining and evaluation on ImageNet.  
We evaluate ACMoE against the Swin Transformer baseline with 16 total experts in both top-1 and top-2 expert routing settings. The Swin backbone has 280M parameters. We additionally conduct experiments on ImageNet under white box adversarial attacks fast gradient sign method (FGSM) \citep{goodfellow2014explaining} and projected gradient descent (PGD) \citep{mkadry2017towards}, and black box attack simultaneous perturbation stochastic approximation (SPSA) \citep{uesato2018adversarial}. We also present results on out-of-distribution (OOD)\citep{hendrycks2021many, hendrycks2021natural}.  In all robust image classification tasks, image classification using ImageNet-A/O/R  we adopt the conventional setup of pretraining on ImageNet and evaluating the trained models on the contaminated/OOD datasets \citep{han2024designing, zhou2022understanding, puigcerver2022adversarial,nguyen2024pidformer,nielsen2025elliptical}. Full experimental details are provided in Appendix \ref{appendix: experimental details}.

\textbf{Image Classification under Adversarial Attack.} Table \ref{table: imnet attack} shows performance on ImageNet classification against FGSM, PGD, and SPSA. Compared with Swin baseline, ACMoE-Top 2 attains noteworthy 7\% and 5\% improvements against PGD and SPSA in top-1 accuracy respectively.

\textbf{Out-of-distribution Image Classification.} Table \ref{table: robust-im} shows ACMoE improves over the baseline Swin Transformer in image classification on hard OOD and real-world adversarially filtered images. Evaluation on ImageNet-A/O/R shows consistent improvements over the baseline in top-1 and top-2 expert choice, with particularly strong improvements in ImageNet-O under top-2 routing with a performance gain in area under precision recall (AUPR) of almost 6\%. 
% \begin{table}[h]
%   \caption{ImageNet Object Recognition Results}
%   \label{table: imagenet}
%   \centering
%   \begin{tabular}{lcc}
%     \toprule
%     \textbf{Model} & \textbf{Top-1 Test Acc. ($\uparrow$)} & \textbf{Top-5 Test Acc. ($\uparrow$)}  \\
%     \midrule
%     \midrule
%      \textit{Swin Transformer-top1} \citep{liu2021swin} & 75.22 & 92.51 \\
%     \text{Swin-COSA-top1} (\textbf{Ours})  & \textbf{75.39} & \textbf{92.56} \\
%     \midrule
%     \textit{Swin Transformer-top2} \citep{liu2021swin} & 76.10 & 92.99 \\
%     \text{Swin-COSA-top2} (\textbf{Ours})  & \textbf{76.30} & \textbf{93.06} \\
%     \bottomrule
%   \end{tabular}
% \end{table}
% \begin{table}[h]
%   \caption{Switch Transformers Pretrained on EnWik-8 and WikiText-103 and Finetuned on Stanford Sentiment Treebank 2 (SST-2)}
%   \label{table: switch pretrain}
%   \centering
%   \begin{tabular}{lccc}
%     \toprule
%     \text{Model} & \text{Test BPC / PPL ($\downarrow$)} & \text{SST-2 ($\uparrow$)}  & \text{IMDb ($\uparrow$)} \\
%     \midrule
%     \midrule
%     \multicolumn{4}{c}{\textit{EnWik-8 Pretrain}}  \\
%     \midrule
%     \midrule
%     \textit{Switch Transformer} \citep{fedus2022switch} & 1.153 & 63.27 \\
%     \text{Switch ACMoE} \textbf{(Ours)} & \textbf{1.137} & \textbf{64.45} \\
%     \midrule
%     \midrule
%      \multicolumn{4}{c}{\textit{WikiText-103 Pretrain}}  \\
%     \midrule
%     \midrule
%     \textit{Switch Transformer} \citep{fedus2022switch} & 35.48 & 76.27 & 82.89 \\
%     \text{Switch ACMoE} \textbf{(Ours)} & \textbf{34.42} & \textbf{77.32} & \textbf{84.10} \\
%     \midrule
%      \textit{GLaM} \citep{du2022glam} & 38.27 & 69.97 \\
%     \text{GLaM ACMoE} \textbf{(Ours)} & \textbf{36.26} & \textbf{71.90} \\
%     \bottomrule
%   \end{tabular}
% \end{table}

\subsection{Empirical Analysis} 

\begin{table}[t]
\small
\caption{Test Accuracy on ImageNet corrupted PGD, FGSM, and SPSA. }
\vspace{-0.14in}
\centering
\begin{tabular}{lcc|cccccc}
\toprule
\multirow{2}{*}{\text{Model}} & \multicolumn{2}{c|}{\text{Clean Data}} & \multicolumn{2}{c|}{\text{PGD}} & \multicolumn{2}{c|}{\text{FGSM}} & \multicolumn{2}{c}{\text{SPSA}} \\
 & Top 1 & Top 5 & Top 1 & Top 5 & Top 1 & Top 5 & Top 1 & Top 5 \\
\midrule
\midrule
\textit{Swin-Top 1} \citep{liu2021swin} & 75.22 & 92.51 & 39.69 & \textbf{74.59} & 52.84 & \textbf{83.86} & 59.92 & \textbf{82.63} \\ 
\text{Swin-ACMoE-Top 1} (\textbf{Ours})  & \textbf{75.39} & \textbf{92.56} & \textbf{40.66} & 73.46 & \textbf{53.43} & 82.80 & \textbf{59.97} & 82.47 \\
\midrule
\textit{Swin-Top 2} \citep{liu2021swin} & 76.10 & 92.99 & 40.85 & 75.51 & 54.70 & 85.22 & 60.57 & 82.75 \\
\text{Swin-ACMoE-Top 2} (\textbf{Ours})  & \textbf{76.31} & \textbf{93.14}  & \textbf{43.74} & \textbf{78.55} & \textbf{55.78} & \textbf{85.80} & \textbf{63.47} & \textbf{86.05} \\
\bottomrule
\end{tabular}
\label{table: imnet attack}
\vspace{-0.1in}
\end{table}

\begin{table}[t]
    \small
  \caption{Test Accuracy on Image Classification in Imagenet-A/O/R}
  \vspace{-0.14in}
  \label{table: robust-im}
  \centering
  \begin{tabular}{lccc}
    \toprule
    \multirow{2}{*}{\text{Model}} &  Im-A & Im-R & Im-O 
    \\
    & \text{Top-1 Acc. ($\uparrow$)} & \text{Top-1 Acc. ($\uparrow$)} & \text{AUPR ($\uparrow$)}  \\
    \midrule
    \midrule
     \textit{Swin Transformer-Top 1} \citep{liu2021swin} & 6.83 & 30.60 & 17.89 \\
    \text{Swin-ACMoE-Top 1} (\textbf{Ours})  & \textbf{7.13}  & \textbf{30.85} & \textbf{18.45}\\
    \midrule
    \textit{Swin Transformer-Top 2} \citep{liu2021swin} & 9.38 & 32.07 & 18.51 \\
    \text{Swin-ACMoE-Top 2} (\textbf{Ours})  & \textbf{9.42}  & \textbf{32.35} & \textbf{19.55} \\
    \bottomrule
  \end{tabular}
  \vspace{-0.1in}
\end{table}
% \begin{table}[h]
%   \caption{ImageNet Object Recognition Results}
%   \label{results: imagenet}
%   \centering
%   \begin{tabular}{lcc}
%     \toprule
%     \textbf{Model} & \textbf{Top-1 Test Acc. ($\uparrow$)} & \textbf{Top-5 Test Acc. ($\uparrow$)}  \\
%     \midrule
%     \midrule
%      \textit{Swin Transformer-top1} \citep{liu2021swin} & 76.60 & \textbf{93.04} \\
%     \text{Swin-COSA-top1} (\textbf{Ours})  & \textbf{76.68} & 92.98 \\
%     \midrule
%     \textit{Swin Transformer-top2} \citep{liu2021swin} & 77.41 & \textbf{93.64} \\
%     \text{Swin-COSA-top2} (\textbf{Ours})  & \textbf{77.53} & 93.59 \\
%     \bottomrule
%   \end{tabular}
% \end{table}
\textbf{Load Balancing.} We analyze in Table \ref{table: load balancing} the effect of ACMoE on expert load balancing. Load balance is calculated as the percentage of tokens assigned to each expert. The load balance score is then taken as the standard deviation over these percentages. A standard deviation of 0, where all experts are activated in exactly equal proportions, is therefore a perfect load balance. We compute this statistic per MoE layer and present the overall load balance averaged over all layers. ACMoE attains better overall load balancing compared to Switch and Swin transformers. Against all backbones, ACMoE achieves a smaller spread in the load balances over layers, shown by smaller standard deviation. Visually we see how better expert specialization can aid load balance in Fig. \ref{fig: expert patches}, where better identification of the semantic regions of the input space leads to more experts being activated.

\begin{table}[t]
    \small
  \caption{Load Balance Analysis of ACMoE and Baseline MoE Models}
  \vspace{-0.14in}
  \label{table: load balancing}
  \centering
  \begin{tabular}{lc}
    \toprule
    \text{Model} & \text{Layer-Averaged Load Balance ($\downarrow$)} \\
    \midrule
    \midrule
    \textit{Switch Transformer} \citep{fedus2022switch} & 5.577 $\pm$ 4.131 \\
    \text{Switch-ACMoE \textbf{(Ours)}} & \textbf{5.317} $\pm$ \textbf{2.622}  \\
    \midrule
    \textit{GLaM} \citep{du2022glam} & \textbf{2.901} $\pm$ 1.434  \\
    \text{GLaM-ACMoE} (\textbf{Ours}) & 2.938 $\pm$ \textbf{1.221}  \\
    \midrule
    \textit{Swin Transformer} \citep{liu2021swin} & 2.134 $\pm$ 1.110 \\
    \text{Swin-ACMoE \textbf{(Ours)}} & \textbf{2.127} $\pm$ \textbf{0.968}  \\
    \bottomrule
  \end{tabular}
  \vspace{-0.1in}
\end{table}

\textbf{Efficiency Analysis.} Computing the cluster-wise feature weights $\{\bs w_k\}_{k \in [E]}$ requires no learnable parameters and is obtained by computing the mean absolute deviation for each set of tokens assigned to the $k^{\mathrm{th}}$ expert. This can be computed using just two computations of the mean -- one for the mean per cluster and one for the mean of the absolute deviations per cluster -- done in parallel over all clusters. This is of order $\mathcal{O}(2nd) = \mathcal{O}(n)$ for $n$ tokens, hence the upper-bound time complexity of the MoE layer is unaffected. Table \ref{table: efficiency analysis} provides empirical efficiency analysis in terms of compute speed, memory allocation, and parameters, which shows changes in speed and memory are within a margin of approximately 1\% or less, implying there is no significant efficiency loss.
\vspace{-0.1in}

\begin{table}[t]
  \caption{Efficiency Comparison between ACMoE and baseline MoE Models}
  \vspace{-0.14in}
  \label{table: efficiency analysis}
  \centering
  \small
  \begin{tabular}{lccc}
    \toprule
    \text{Model} & \text{Compute Speed (ms/it)} & \text{Max Memory (K)} &  \text{\#Params (M)}  \\
    \midrule
    \midrule
    \textit{GLaM} \citep{du2022glam} & 422.62 & 25.69 & 220 \\
    \text{GLaM-ACMoE} \textbf{(Ours)}  & 425.15 & 25.72 &  220 \\
    \midrule
    \textit{Switch Transformer} \citep{fedus2022switch}  & 391.93 & 34.64 &  216 \\
    \text{Switch-ACMoE} \textbf{(Ours)}  & 393.29 & 34.68 &  216 \\
    \midrule
    \textit{Swin Transformer} \citep{liu2021swin}  & 403.36 & 22.00 &  280 \\
    \text{Swin-ACMoE} \textbf{(Ours)}  & 408.56 & 22.19 &  280 \\
    \bottomrule
  \end{tabular}
  \vspace{-0.2in}
\end{table}

\section{Related Work} \label{sec: related work}

\textbf{Routing Methods.} Recent studies have proposed token-expert assignment algorithms based on reinforcement learning \citep{bengio2015conditional}, deterministic hashing \citep{roller2021hash}, optimal transport \citep{liu2022sparsity}, linear programs \citep{lewis2021base}, cosine similarity \citep{chi2022representation}, soft token mixing \citep{puigcerver2023sparse}, greedy top-k experts per token \citep{shazeer2017sparsely} and greedy top-k tokens per expert \citep{zhou2022mixture}. Existing work has predominantly considered dot-products between inputs and experts as a suitable metric for similarity \citep{lewis2021base, puigcerver2023sparse, shazeer2017sparsely, zhou2022mixture, chi2022representation}. This work continues with dot-product based learnable routing but computes the routing assignments in an adaptively transformed space to maximally identify the latent expert clusters.

% Standard dot-products inherently treat all dimensions of the feature space as equally relevant, which is suboptimal when the latent clustering structure of the data depends on differing subsets of features. Our work therefore differs from previous approaches in that we compute input-expert assignments in an adaptively transformed space where the the dimensions are scaled according to the specialization of each expert, allowing the router to more easily identify the best-matched expert per token. 

\textbf{MoE and Cluster Analysis.} The MoE framework traces its roots back to Gaussian mixture models where the input space is assumed divisible into separate regions with an expert specializing in each region \citep{jacobs1991adaptive}. Recent studies show that the router can recover the clustering structure of the input space and each expert specializes in a specific cluster \citep{dikkala2023benefits, chen2022towards}. Our work leverages the clustering perspective on MoE to consider adaptive transformations of the input space to more easily distinguish latent clusters. We learn these transformations via feature-weighted cluster analysis, which has been studied in the clustering literature \citep{brusco2001variable, witten2010framework, gnanadesikan1995weighting, van1989clustering, friedman2004clustering}. \citet{friedman2004clustering} consider cluster-dependent feature weights to augment iterative clustering algorithms. Our approach similarly uses cluster-dependent feature weights but uses a different optimization problem to derive optimal weights. 
% that directly capture the importance of each feature to the clustering solution and is adapted to the MoE setting.

\textbf{Robust MoE.} The robustness of MoE architectures is a newly emerging research area. \citet{puigcerver2022adversarial} provide the first study in this direction from the perspective of model capacity and the Lipschitz constant, finding conditions under which MoE models are provably more robust than their dense counterparts. \citet{zhang2023robust} examine the effect of adversarial training and propose an alternating optimization adversarial defence. \citet{teo2024momentumsmoe} integrates heavy-ball momentum in SMoE to improve the model’s stability and robustness. Our work differs from these approaches by examining the robustness of MoE models purely through the lens of the latent clustering structure of the input space. To the best of our knowledge, this is a novel lens on robustness in MoE models.
\vspace{-0.1in}

% Beginning with recent work by \cite{bubeck2021universal}, which proposes that larger models have a smaller Lipschitz constant and are therefore more adversarially robust, \cite{puigcerver2022adversarial} extend this proposition to the MoE framework, finding conditions under which MoE models are provably more robust than their dense counterparts. Since then, \cite{zhang2023robust} propose an adversarial training framework for convolutional neural networks with MoE layers based on alternating optimization. In this work, rather than approaching robustness through the lens of model capacity, we approach it through the latent clustering structure of the input data space. To the best of our knowledge this is a novel lens on robustness in MoE models.

% The robustness of MoE architectures is a newly emerging research area. Beginning with recent work by \cite{bubeck2021universal}, which proposes that larger models have a smaller Lipschitz constant and are therefore more adversarially robust, \cite{puigcerver2022adversarial} extend this proposition to the MoE framework, finding conditions under which MoE models are provably more robust than their dense counterparts. Since then, \cite{zhang2023robust} propose an adversarial training framework for convolutional neural networks with MoE layers based on alternating optimization. In this work, rather than approaching robustness through the lens of model capacity, we approach it through the latent clustering structure of the input data space. To the best of our knowledge this is a novel lens on robustness in MoE models.

% Since then, \cite{zhang2023robust} propose an adversarial training framework for MoE layers integrated into convolutional neural networks based on alternating optimization.

% Furthermore, we do not propose an explicit adversarial training method, instead finding that computing token-expert matching in a transformed space that separates latent clusters in data achieves better robustness. 

\section{Conclusion and Future Work} \label{sec: conclusion}
\vspace{-0.1in}
In this paper, we present the Adaptive Clustering (AC) router and ACMoE layer, a novel MoE routing method that computes token-expert assignments in a transformed space that maximally identifies latent clusters in the data and more easily discovers the best-matched expert for each token. We adaptively learn for each input which features are relevant to determining its latent cluster assignment and scale its features accordingly such that features that promote tight clustering are upweighted and features that produce dispersed clusters are downweighted. This transformation accentuates the relevant characteristics of each input according to the specialization of the experts, thereby allowing the router to more easily discover the optimal input-expert allocation. Our AC routing method enables faster convergence by improving the Hessian conditioning of the router and better robustness by increasing the separation of latent clusters in the transformed space. This approach makes no assumptions on the downstream task, requires no learnable parameters, and can be applied within any MoE architecture to boost performance on clean and contaminated data. A limitation of our method is that the AC router requires estimates of each token's cluster assignment. We obtain these by using the expert assignments in previous layers, which means we require the embedding size to remain the same between adjacent MoE layers. For ongoing work, we are investigating improved methods for estimating the latent cluster memberships without reliance on previous layers and with provable consistency guarantees.

\begin{ack}
This research / project is supported by the National Research Foundation Singapore under the AI Singapore Programme (AISG Award No: AISG2-TC-2023-012-SGIL). This research / project is supported by the Ministry of Education, Singapore, under the Academic Research Fund Tier 1 (FY2023) (A-8002040-00-00, A-8002039-00-00). This research / project is also supported by the NUS Presidential Young Professorship Award (A-0009807-01-00).

Thanks to our anonymous reviewers, who provided valuable feedback which improved the paper substantially. Thanks also to Loi Xuan Ly for lending his eye for design.
\end{ack}

\textbf{Reproducibility Statement.} Source code for our experiments are provided in the supplementary material. We provide the full details of our experimental setup -- including datasets, model specification, train regime, and evaluation protocol -- for all experiments in Appendix \ref{appendix: experimental details}. All datasets are publicly available.

\textbf{Ethics Statement.} Our work considers fundamental architectures, and in particular their robustness and convergence properties. Given this, we foresee no issues regarding fairness, privacy, or security, or any other harmful societal or ethical implications in general.

\bibliography{iclr2025_conference}
\bibliographystyle{iclr2025_conference}
%\bibliographystyle{plain}

\newpage
\appendix
\begin{center}
{\bf \Large{Supplement to ``Tight Clusters Make Specialized Experts''}}
\end{center}

\DoToC

\section{Technical Proofs} \label{appendix: additional results}

\subsection{Proof of Theorem \ref{thm:ReCOP-solution}}\label{appendix:ReCOP-solution-proof}

To begin with, we present the following lemma to show the existence of constants $\alpha_k$ for $k \in [E]$ that satisfy Eqn.~\ref{eq:alpha-condition}:

\begin{lemma}\label{lemma:alpha-condition}
    For any $\lambda > 0$, Eqn.~\ref{eq:alpha-condition} has exactly $d$ real solutions with respect to $\alpha_k$.
\end{lemma}
\textit{Proof of Lemma \ref{lemma:alpha-condition}.} Without loss of generality, assume that $s_{1k} \ge s_{2k} \ge \dots \ge s_{dk}$. Denote 
\begin{align}
    \varphi(\alpha) := \sum_{q \in [d]}\frac{1}{s_{qk} + \alpha} - \frac{d}{\lambda}.
\end{align}
Then, the existence of solutions to Eqn.~\ref{eq:alpha-condition} is equivalent to the condition $\varphi(\alpha_l) = 0$. Note that $\varphi(\alpha)$ is a strictly decreasing function in its connected continuity domains since
\begin{align}
    \varphi'(\alpha) = -\sum_{q \in [d]}\frac{1}{(s_{qk} + \alpha)^2} < 0
\end{align}
for all $\alpha \in \mathbb R \setminus \{-s_{1k}, \dots, -s_{dk}\}$. Further, we observe that
\begin{align}\label{eq:lim-inside}
    \lim_{\alpha \to -s_{qk}^{-}} \varphi(\alpha) = -\infty, \quad \lim_{\alpha \to -s_{qk}^{+}} \varphi(\alpha) = +\infty
\end{align}
for all $q \in [d]$, and 
\begin{align}\label{eq:lim-outside}
    \lim_{\alpha \to \pm \infty}\varphi(\alpha) = -\frac{d}{\lambda} < 0.
\end{align}
Now consider the domain of continuity of $\varphi(\alpha)$, namely $(-\infty, -s_{1k})\cup (-s_{1k}, -s_{2k}) \cup \dots \cup (-s_{dk}, \infty)$. Due to the monotonicity and limits~\ref{eq:lim-inside} \& \ref{eq:lim-outside}, there exists a unique solution in each of the intervals except for $(-\infty, -s_{1k})$ where the function is always strictly negative, thus, yielding $d$ roots in total. \qed

Now we follow up with the main proof of this section.

\textit{Proof of Theorem \ref{thm:ReCOP-solution}.} First, let $\mathcal{I}_{k} := \{i : r(i) = k\}$ for convenience. Now let us restate the clustering optimization problem (\ref{eq:ReCOP}) here once again:
\begin{align}\label{eq:cosa-optimization}
    &\min_{\bs{w}_k} Q(c, \{\bs{w}_{k}\}_{k\in [E]}) = \sum_{k \in [E]} \frac{1}{N_k^2}\sum_{i,j \in \mathcal{I}_k}\sum_{q \in [d]}\left(w_{qk}\rho_{ijq} + \frac{\lambda}{d}\log \frac{1}{dw_{qk}}\right), \nonumber\\
    &\text{such that }\sum_{q\in [d]}w_{qk} = 1, \quad \forall k \in [E], 
\end{align}
where we have immediately used the fact that 
\begin{align}
    D_{\text{KL}}(\bs u \mid\mid \bs w_k) = \sum_{q \in [d]}\frac{1}{d}\log\frac{1/d}{w_{qk}}.
\end{align}
Also, note that 
\begin{align}
    \sum_{q \in [d]}\left(w_{qk}\rho_{ijq} + \lambda \frac{1}{d}\log\frac{1}{dw_{qk}}\right) \nonumber &= \sum_{q \in [d]}\left(w_{qk}\rho_{ijq} - \lambda \frac{1}{d}\log (dw_{qk})\right) \nonumber\\
    &= \sum_{q \in [d]}\left(w_{qk}\rho_{ijq} - \frac{\lambda}{d}\log w_{qk}\right) - \lambda \log d.
\end{align}
We can ignore the term $\lambda \log d$ since it does not depend on the optimization variable. Method of Lagrange multipliers turns this constrained optimization problem into the following unconstrained counterpart:
\begin{align}
    \min_{\bs{w}_k, \bs{\alpha}} \mathcal{L}(c, \{\bs{w}_{k}\}_{k\in [E]}, \bs{\alpha}) = \sum_{k \in [E]} \frac{1}{N_k^2}\sum_{i,j \in \mathcal{I}_k}\sum_{q \in [d]}\left(w_{qk}\rho_{ijq} - \frac{\lambda}{d}\log w_{qk}\right) + \sum_{k \in [E]} \alpha_k \left(\sum_{q\in [d]}w_{qk} - 1\right), \nonumber
\end{align}
where $\bs{\alpha} = \begin{bmatrix}
    \alpha_1 & \dots & \alpha_L
\end{bmatrix}^\top$ is the vector of Lagrange multipliers. Note that the last optimization problem can be separated into the following $L$ independent optimization subproblems:
\begin{align}
    \min_{\bs{w}_k, \bs{\alpha}} \mathcal{L}_k(c, \bs{w}_{k}, \bs{\alpha}) = \frac{1}{N_k^2}\sum_{i,j \in \mathcal{I}_k}\sum_{q \in [d]}\left(w_{qk}\rho_{ijq} - \frac{\lambda}{d}\log w_{qk}\right) +  \alpha_k \left(\sum_{q\in [d]}w_{qk} - 1\right), \nonumber
\end{align}
for $k \in [E]$. Since the objective function is a positive combination of convex functions, the optimization problem is also convex. By setting the derivatives of $\mathcal{L}_k$ with respect to both optimization variables to $0$, we obtain the following system of equations:
$$\begin{cases}
    \dfrac{\partial \mathcal{L}_k}{\partial w_{qk}} =  s_{qk} - \dfrac{\lambda}{d}\dfrac{1}{w_{qk}} + \alpha_k = 0, \\
    \dfrac{\partial \mathcal{L}_k}{\partial \alpha_k} = \dsum_{q\in [d]}w_{qk} - 1 = 0
\end{cases}$$
for all $k \in [E]$, where $s_{qk}$ is the data dispersion measure defined in the theorem statement. The first equation yields
\begin{align}
    w_{qk} = \frac{\lambda}{d}\frac{1}{s_{qk} + \alpha_k},
\end{align}
where $\alpha_k$ is found from $\sum_{q\in[d]} w_{qk} = 1$ which in fact gives
\begin{align}
    \sum_{q \in [d]} \frac{1}{s_{qk} + \alpha_k} = \frac{d}{\lambda}
\end{align}
for all $k \in [E]$ as desired. \qed

\subsection{Proof of Proposition \ref{prop: robustness}}\label{appendix:robustness-proof}

Since Proposition \ref{prop: robustness} is a composition of Lemma \ref{lemma:mean-distance} and Lemma \ref{lemma:incorrect-cluster}, we proceed by providing their proofs.

\subsubsection{Proof of Lemma \ref{lemma:mean-distance}}

% \begin{assumption}\label{assumption:tight-dim-separate-most}
%     Clusters are most separated in the feature dimensions of smallest dispersions.
% \end{assumption}
% \begin{lemma}[Distance between cluster means]\ref{lemma:mean-distance}
%     Consider the clustering setting as in Lemma \ref{lemma:well-separated}. Let $\bs M_1 = \mathrm{diag}(m_1, \dots, m_d)$ be the router transformation constructed from the feature-wise dispersions of cluster $g_1$ as given by Definition \ref{def: router transformation}. Then, under Assumption \ref{assumption:tight-dim-separate-most}, the distance between cluster means is larger in the $\bs M_1$-transformed space:
%     \begin{align}
%         (\bs\mu_2 - \bs\mu_1)^\top \bs M_1 (\bs\mu_2 - \bs\mu_1) \ge \|\bs\mu_2 - \bs\mu_1\|^2.\label{eq:larger-mean-distance}
%     \end{align}
% \end{lemma}
\textit{Proof of Lemma \ref{lemma:mean-distance}.} Notice that we can expand inequality (\ref{eq:larger-mean-distance}) as
\begin{align*}
    \sum_{i \in [d]} m_{i}\delta\mu_{i}^2 \ge \sum_{i \in [d]} \delta\mu_i^2,
\end{align*}
where we let $\delta\bs\mu := \bs\mu_b - \bs\mu_a$. Since $\bs M_a$ entries are mean-scaled, we can rewrite them as
\begin{align}
    m_{i} = \frac{dm_{i}'}{\sum_{j \in [d]}m'_{j}}
\end{align}
for some initial dispersion estimates $\{m'_{j}\}_{j \in [d]}$. Without loss of generality, assume that $[d']$ is the set of dimension indices for which the dispersions are relatively much smaller than those in the rest of the dimensions in the sense that $m'_{i} \gg m'_{j}$ for any $i \in [d']$ and $j \in [d] \setminus [d']$. Then, there exists a positive $\alpha \ll 1/2$ such that $\sum_{i \in [d']}m_{i} > d - \alpha$ and $\sum_{i \in [d]\setminus[d']}m_{i} < \alpha$. By the assumption that clusters are best-separated along the features for which they cluster tightly, this means that the weight matrix $\bs M_a$ maximizes the contribution of largest $d'$ terms in $\sum_{i \in [d]} m_{i}\delta\mu_{i}^2$ corresponding to individual feature-wise distances in dimensions where the feature dispersions are the smallest instead of giving uniform weights to all dimensions, which leads to inequality (\ref{eq:larger-mean-distance}). \qed


\subsubsection{Proof of Lemma \ref{lemma:incorrect-cluster}}
% \begin{lemma}[Probability of incorrect clustering]\label{lemma:well-separated}
%     Let the data be generated from a Gaussian mixture model with two reasonably separated components, $g_c = \mathcal{N}(\bs \mu_c, \bs \Sigma_c)$ for $c \in [2]$. Given a token $\bs h$, consider the setting where the clusters are assigned based on the $\mathcal{L}_2$ distance between the token $\bs h$ and the cluster means $\bs \mu_c$. Then the probability of incorrect cluster assignment decays exponentially in $\|\bs\mu_2 - \bs\mu_1\|$.
% \end{lemma}

\textit{Proof of Lemma \ref{lemma:incorrect-cluster}.} Since we use the $\mathcal{L}_2$ distance between the token $\bs h$ and $\bs \mu_c$ as a similarity metric, we assign cluster $g_{k^*}$ to the token $\bs h'$ iff $\|\bs h' - \bs \mu_{k^*}\| \le \|\bs h' - \bs \mu_{k}\|$. Assume that the token $\bs h'$ is a noisy observation of an underlying true token $\bs h$ which actually originates from cluster $g_{k^*}$. Then, the token $\bs h'$ can be decomposed as $\bs h' = \bs h + \bs \epsilon$ for a random noise $\bs \epsilon \sim \mathcal{N}(\bs 0, \bs\Sigma_{\epsilon})$. Now define the decision variable $\mathcal{D}(\bs h') := \|\bs h' - \bs \mu_{k^*}\|^2 - \|\bs h' - \bs \mu_{k}\|^2$ which turns the clustering condition to $\mathcal{D}(\bs h') \le 0$ for the cluster $g_{k^*}$. Let us analyze the decision variable $\mathcal{D}$ as a random variable where randomness may come from the underlying sampling strategy and noise. Note that
\begin{align}
    \mathcal{D}(\bs h') &= \|\bs h + \bs\epsilon - \bs \mu_{k^*}\|^2 - \|\bs h + \bs\epsilon - \bs \mu_{k}\|^2 \nonumber\\
    &= \|\bs h - \bs \mu_{k^*}\|^2 - \|\bs h - \bs \mu_{k}\|^2 + 2(\bs \mu_{k} - \bs \mu_{k^*})^\top \bs \epsilon \nonumber\\
    &= \mathcal{D}(\bs h) + 2 \delta\bs\mu^\top \bs\epsilon, \label{eq:Dh-decomposition}
\end{align}
where $\delta\bs\mu := \bs\mu_{k} - \bs\mu_{k^*}$. Due to the assumption that $\bs h$ is drawn from the distribution $g_{k^*}$, it can be rewritten as $\bs h = \bs\mu_{k^*} + \bs\nu$ with $\bs\nu \sim \mathcal{N}(\bs 0, \bs\Sigma_{k^*})$. Then for the first term in Eqn.~\ref{eq:Dh-decomposition}, we have
\begin{align}
    \mathcal{D}(\bs h) &= \|\bs h - \bs \mu_{k^*}\|^2 - \|\bs h - \bs \mu_{k}\|^2 \nonumber\\
    &= \delta\bs\mu^\top (2\bs h - \bs\mu_{k^*} - \bs\mu_{k}) \nonumber\\
    &= \delta\bs\mu^\top (2\bs\nu - \delta\bs\mu) \nonumber\\
    &= 2\delta\bs\mu^\top \bs\nu - \|\delta\bs\mu\|^2.
\end{align}
Substituting this back into Eqn.~\ref{eq:Dh-decomposition}, we get
\begin{align}
    \mathcal{D}(\bs h') = 2\delta\bs\mu^\top (\bs\nu + \bs\epsilon) - \|\delta\bs\mu\|^2.
\end{align}
This shows that $\mathcal{D}(\bs h') \sim \mathcal{N}\left(-\|\delta \bs\mu\|^2, 4\delta\bs\mu^\top (\bs\Sigma_{k^*} + \bs\Sigma_{\epsilon})\delta\bs\mu\right)$. Since $\mathcal{D}(\bs h')$ follows a normal distribution with the derived parameters, the probability that $\bs h'$ is assigned to cluster $g_{k^*}$ is given by
\begin{align}
    \Pr(\text{correct cluster}) = \Pr\left(\mathcal{D}(\bs h) \le 0\right) = \Phi\left(\frac{\|\delta\bs\mu\|^2}{2\sqrt{\delta\bs\mu^\top(\bs\Sigma_{k^*} + \bs\Sigma_{\epsilon})\delta\bs\mu}}\right),
\end{align}
where $\Phi$ denotes the CDF of normal distribution as usual. Since $\Phi$ is an increasing function, the probability that the noisy token $\bs h$ is assigned to the correct cluster is proportional to the distance between the cluster centroids and inverse proportional to the covariance matrices of the cluster and the additive noise. On the other hand, for the incorrect clustering probability, we have
\begin{align}
    \Pr(\text{incorrect cluster}) = 1 - \Phi\left(\frac{\|\delta\bs\mu\|^2}{2\sqrt{\delta\bs\mu^\top(\bs\Sigma_{k^*} + \bs\Sigma_{\epsilon})\delta\bs\mu}}\right)
\end{align}
as claimed. \qed

% \textit{Proof of Proposition \ref{prop: robustness}.} Proposition \ref{prop: robustness} is now a direct result of Lemma \ref{lemma:well-separated} and Lemma $\ref{lemma:mean-distance}$. \qed

\subsection{Proof of Proposition \ref{prop: convergence}}\label{appendix:convergence-proof}

\textit{Proof of Proposition \ref{prop: convergence}.} Let the router be given by $g$ and let the softmax function be given by $g_{\bm \theta} : \mathbb R^d \to \mathbb R^d$, parameterized by expert embeddings $\{ \bs e_i \}_{i \in [E]}$. The network loss depends on expert embeddings only through the router function $g$. We shall explore the exclusive contribution of each expert embedding in minimizing $\mathcal{L}^{\mathrm{ACMoE}}$. In order to do this, we look at the network loss as a scalar function of $i^{\mathrm{th}}$ expert embedding vector while treating all other network parameters as fixed. Then, we can write $\mathcal{L}^{\mathrm{ACMoE}} : \mathbb R^d \to \mathbb R$ such that $\mathcal{L}^{\mathrm{ACMoE}} = \mathcal{L}^{\mathrm{ACMoE}}(g_{\bm \theta}(\bs e_i))$. For simplicity, we shall omit the subscript $\bm\theta$. The gradient that comes from back-propagation is then given by
\begin{align}
    \nabla_{\bs e_i} \mathcal{L}^{\mathrm{ACMoE}} = \left(\nabla_{g}\mathcal{L}^{\mathrm{ACMoE}}\right)^\top \nabla_{\bs e_i} g,
\end{align}
where $\nabla_{\bs e_i} g \in \mathbb R^{d\times d}$ denotes the Jacobian matrix of $g$ since for $g_k := (g_{\bs \theta}(\bs e_i))_k$, we can write
\begin{align}
    \frac{\partial}{\partial e_{is}}\mathcal{L}^{\mathrm{ACMoE}}(g_1, \dots, g_d) = \sum_{k}\frac{\partial \mathcal{L}^{\mathrm{ACMoE}}}{\partial g_k}\frac{\partial g_k}{\partial e_{is}}.
\end{align} 
Note that for $g_k = \softmax(\bs h^\top \bs M \bs e_k)$, we have
\begin{align}
    \frac{\partial g_k}{\partial e_{is}} = m_sh_s g_k(\delta_{ki} - g_i) = m_sh_s b_{ki}.
\end{align}
Then, the element of the Hessian matrix of the network loss at index $(s, t) \in \mathbb [d]\times [d]$ can be written as
\begin{align}
    \bs H^{(i)}_{st}(\mathcal{L}^{\mathrm{ACMoE}}) &= \frac{\partial^2\mathcal{L}^{\mathrm{ACMoE}}}{\partial e_{is} \partial e_{it}} \nonumber = \frac{\partial}{\partial e_{it}}\sum_{k}\frac{\partial \mathcal{L}^{\mathrm{ACMoE}}}{\partial g_k}\frac{\partial g_k}{\partial e_{is}} \nonumber \\
    &= \sum_k \left(\sum_{j} \frac{\partial^2 \mathcal{L}^{\mathrm{ACMoE}}}{\partial g_k \partial g_j}\frac{\partial g_j}{\partial e_{it}}\right) \frac{\partial g_k}{\partial e_{is}} + \frac{\partial \mathcal{L}^{\mathrm{ACMoE}}}{\partial g_k} \frac{\partial^2 g_k}{\partial e_{is}\partial e_{it}} \nonumber \\
    &= m_sh_sm_th_t \left[\sum_k \left(\sum_{j} \frac{\partial^2 \mathcal{L}^{\mathrm{ACMoE}}}{\partial g_k \partial g_j}b_{ji}\right) b_{ki} + \frac{\partial \mathcal{L}^{\mathrm{ACMoE}}}{\partial g_k} b'_{ki}\right] \nonumber \label{eq:Hst}\\
    &= m_sh_sm_th_t B_i,
\end{align}
where $B_i$ is some constant that depends only  on index $i$. Due to Eqn.~\ref{eq:Hst}, the Hessian takes the following matrix form
\begin{align}
    \bs H^{(i)} = B_i (\bm M \bs h)(\bm M \bs h)^\top. %\propto \bs M(\bs h\bs h^\top)\bs M.
\end{align}
Taking expectation from both sides, we obtain
\begin{align}
    \mathbb E_{\bs h \sim (\bs \mu, \bs \Sigma)}\left[\bs H^{(i)}\right] = B_i \mathbb E_{\bs h \sim (\bs \mu, \bs \Sigma)}\left[\bs M(\bs h\bs h^\top)\bs M\right] = B_i \bs M(\bs\Sigma)\bs M,
\end{align}
where we assume $\bs h$ is centered. Now recall that $\bs M = \mathrm{diag}(m_1, \dots, m_d)$ where for each $i$, $m_i \sim 1/\sqrt{\Sigma_{ii}}$ holds. Assume that the covariance matrix $\bs\Sigma$ is symmetric positive definite. Then, it is diagonalizable as $\bs\Sigma = \bs U \bs \Lambda \bs U^\top$ with $\bs\Lambda = \mathrm{diag}(\lambda_1, \dots, \lambda_d)$, a diagonal matrix with eigenvalues of $\bs \Sigma$. With the transformation $\bs M$, we get
\begin{align}
    \bs M \bs \Sigma \bs M &= \bs M \bs U \bs \Lambda \bs U^\top \bs M = \bs U \bs M \bs \Lambda \bs M \bs U^\top \\
    &= \bs U \begin{bmatrix}
        m_1^2\lambda_1 & & \\
         & \ddots & \\
          & & m_d^2 \lambda_d
    \end{bmatrix}\bs U^\top.
\end{align}
Since the eigenvalues capture the variances along the principal components of the covariance matrix, $m_i^2$, as a reciprocal of a measure of dimension-wise dispersion, is reasonably correlated with $1/\lambda_i$, as demonstrated by Lemma \ref{lemma:dim-variances}, implying $\lambda_j \le \lambda_i \implies m_j \ge m_i$ with high probability. Therefore, we obtain that
\begin{align}
    \kappa(\bs M\bs \Sigma \bs M) = \frac{\lambda_{\max}(\bs M \bs \Sigma\bs M)}{\lambda_{\min}(\bs M \bs\Sigma \bs M)} \approx \frac{m_{\min}^2\lambda_{\max}(\bs \Sigma)}{m_{\max}^2\lambda_{\min}(\bs \Sigma)} \le \kappa(\bs \Sigma),
\end{align}
which implies the claim. \qed

\begin{lemma}[Correlation between dimension-wise varainces and covariance eigenvalues]\label{lemma:dim-variances}
    Let $\{\bs b_i\}_{i \in d}$ be the set of normalized basis vectors of $\mathbb R^d$. Consider a symmetric positive definite covariance matrix $\bs\Sigma$ and its unit eigenvectors $\{\bs v_i\}_{i \in [d]}$. Assume that the eigenvector $\bs v_i$ is a reasonably small perturbation of the basis vector $\bs b_i$ such that $\bs v_i^\top \bs b_i \ge 1 - \epsilon$ for all $i \in [d]$ and a small constant $\epsilon > 0$. Then, for all $i \in [d]$, we have
    \begin{align}
        \left|\lambda_i - \Sigma_{ii}\right| \le \epsilon \cdot \max_{j \ne i}\left|\lambda_i - \lambda_j\right|, \label{eq:lambda-sigma-diff}
    \end{align}
    where $\{\lambda_i\}_{i \in [d]}$ is the set of ordered eigenvalues of $\bs\Sigma$ corresponding to eigenvectors $\{\bs v_i\}_{i \in [d]}$.
\end{lemma}
\textit{Proof of Lemma \ref{lemma:dim-variances}.} Note that each diagonal element of the SPD covariance matrix $\bs\Sigma$ can be written as
\begin{align}
    \Sigma_{ii} &= \bs b_i^\top \bs\Sigma \bs b_i = \bs b_i^\top \left(\sum_{j\in [d]} \lambda_j \bs v_j \bs v_j^\top\right)\bs b_i = \sum_{j \in [d]} \lambda_j (\bs v_j^\top \bs b_i)^2.
\end{align}
Then, the difference on the left hand side of Eqn.~\ref{eq:lambda-sigma-diff} can be bounded as
\begin{align}
    \left|\lambda_i - \Sigma_{ii}\right| &= \left|\lambda_i - \sum_{j \in [d]}\lambda_j (\bs v_j^\top \bs b_i)^2\right| \nonumber = \left|\lambda_i\left(1 - (\bs v_i\bs e_i)^2\right) - \sum_{j \ne i}\lambda_j(\bs v_j^\top \bs b_i)^2\right| \nonumber \\
    &= \left|\lambda_i\sum_{j \ne i}(\bs v_j^\top \bs b_i)^2 - \sum_{j \ne i}\lambda_j(\bs v_j^\top \bs b_i)^2\right| \label{eq:b1}\\
    &= \left|\sum_{j \ne i}(\lambda_i - \lambda_j)(\bs v_j^\top \bs b_i)^2\right| \nonumber \\
    &\le \max_{j \ne i}\left|\lambda_i - \lambda_j\right|\sum_{j \ne i}(\bs v_j^\top \bs b_i)^2 \nonumber \\
    &= \max_{j \ne i}\left|\lambda_i - \lambda_j\right|\left(1 - (\bs v_i\bs b_i)^2\right) \label{eq:b2} \\
    &\le \epsilon \max_{j \ne i}\left|\lambda_i - \lambda_j\right|, \nonumber
\end{align}
where we used the fact that 
\begin{align}
    \sum_{j \in [d]} (\bs v_j^\top \bs b_i)^2 &= \left( \sum_{j=1}^{n} \left( \bs{v}_j^\top \bs{b}_i \right) \bs{v}_j \right)^\top
\left( \sum_{k=1}^{n} \left( \bs{v}_k^\top \bs{b}_i \right) \bs{v}_k \right) = \bs b^\top \bs b = 1 \nonumber
\end{align}
to obtain Eqn.~\ref{eq:b1} and Eqn.~\ref{eq:b2} since the eigenvectors of $\bs\Sigma$ are orthonormal. \qed
% \textit{Proof of Proposition \ref{prop: convergence}.} Let $\bs h_{M} = f^{\mathrm{ACMoE}}(\bs h)$ denote the output of ACMoE layer as given by Eqn.~\ref{eqn: acmoe}. Also, let $g_i := g(\bs h^\top \bs M \bs e_i)$ for simplicity. The gradient with respect to $i^{\mathrm{th}}$ expert embedding that comes from back-propagation can be written as
% \begin{align}
%     \nabla_{\bs e_i}\mathcal{L}^{\mathrm{ACMoE}} = \nabla_{\bs h_{M}}\mathcal{L}^{\mathrm{ACMoE}}\cdot \nabla_{\bs e_{i}}\bs h_{M},
% \end{align}
% where the Jacobian matrix of the ACMoE layer output with respect to $i^{\mathrm{th}}$ expert is calculated as follows:
% \begin{align}
%     \nabla_{\bs e_{i}}\bs h_{M} = \sum_{k \in \mathcal{K}} \nabla_{\bs e_i}\left[g_k f_k^{\text{\mathrm{FFN}}}(\bs h)\right] = \sum_{k \in \mathcal{K}} g_k (\delta_{ki} - g_i) (\bs M\bs h) \bs f_k^\top, \nonumber
% \end{align}
% where $\bs f_k := f_k^{\text{\mathrm{FFN}}}(\bs h)$. Using the expression for the Jacobian, the Hessian can be computed as
% \begin{align}
%     \bs H_{ij} &= (\bs M \bs h) \sum_{k \in \mathcal{K}} \nabla_{\bs e_j}\left[ g_k (\delta_{ik} - g_i) \bs{f}_k^\top\right] \nonumber \\
%     &= \sum_{k \in \mathcal{K}} \left[\nabla_{\bs e_j}g_k (\delta_{ik} - g_i) + g_k (- \nabla_{\bs e_j}g_i)\right](\bs M \bs h) \bs{f}_k^\top \nonumber \\
%     &= \sum_{k \in \mathcal{K}} \left[g_k(\delta_{kj} - g_j) (\delta_{ki} - g_i) + g_k (- g_i(\delta_{ij} - g_j))\right](\bs M \bs h) \left((\bs M \bs h) \otimes \bs{f}_k\right) \nonumber \\
%     &= \sum_{k \in \mathcal{K}} G_{kij} \left((\bs M \bs h)^\top(\bs M \bs h)\right)\bs{f}_k \nonumber \\
%     &= \bs M^2 (\bs h \bs h^\top) f_{ij}(\bs h), \label{eqn: hessian}
% \end{align}
% where $G_{kij} := g_k(\delta_{kj} - g_j) (\delta_{ki} - g_i) + g_k (- g_i(\delta_{ij} - g_j))$ and $f_{ij}(\bs h) = \sum_{k \in \mathcal{K}}G_{kij}\bs{f}_k$ in Eqn.~\ref{eqn: hessian}.

% $$\mathcal{D}(\bs h) \sim \mathcal{N}(-\delta\bs\mu^\top \bs M \delta\bs\mu,4\delta\bs\mu^\top \bs M (\bs\Sigma_1 + \bs\Sigma_{\epsilon})\bs M\delta\bs\mu)$$
% $$\mathcal{D}(\bs h) \sim \mathcal{N}(-\bs\mu_1^\top \bs M \delta\bs\mu,\delta\bs\mu^\top \bs M (\bs\Sigma_1 + \bs\Sigma_{\epsilon})\bs M\delta\bs\mu)$$
% [JUST A NOTE]
% \begin{itemize}
%     \item With $\mathcal{D}(\bs h) = (\bs{h} - \bs\mu_1)^{\top}\bs M(\bs h - \bs\mu_1) - (\bs h - \bs\mu_2)^{\top}\bs M(\bs h - \bs\mu_2)$, we would have  $$\Pr\left(\mathcal{D}(\bs h) \le 0\right) = \Phi\left(\frac{\delta\bs\mu^\top \bs M \delta\bs\mu}{2\sqrt{\delta\bs\mu^\top\bm M(\bs\Sigma_1 + \bs\Sigma_{\epsilon})\bs M\delta\bs\mu}}\right)$$
%     \item With $\mathcal{D}(\bs h) = \bs h \bs M\bs\mu_1 - \bs h \bs M\bs\mu_2$, we would have $$\Pr\left(\mathcal{D}(\bs h) \le 0\right) = \Phi\left(\frac{\bs\mu_1^\top \bs M \delta\bs\mu}{\sqrt{\delta\bs\mu^\top\bm M(\bs\Sigma_1 + \bs\Sigma_{\epsilon})\bs M\delta\bs\mu}}\right)$$
% \end{itemize}

\section{Implementation Procedure and Computational Efficiency}

\paragraph{Training and Inference.} Given the AC routing scheme requires requires the expert assignment per token from the previous layer, we can only implement AC routing from the second layer on. We incorporate AC routing into both training and inference stages. This is because, firstly, AC routing is designed to offer improvements to both clean and contaminated data, and so even in the presence of completely clean train and test data, it is advantageous to incorporate the AC method into both stages. Secondly, it is commonplace to encounter data contamination only at the test stage and indeed highly possible to encounter it in train as well. Therefore, in the interest of robustness as well, AC routing is incorporated into both stages. 

\paragraph{Computational Efficiency.} Computing the required $\{\bs w_k\}_{k \in [E]}$ for number of experts $E$ requires no learnable parameters and is obtained simply by computing the mean absolute deviation for each set of tokens assigned to the $k^{\mathrm{th}}$ expert. This can be computed using just two computations of the mean -- once for the mean per cluster and once again for the mean of the absolute deviations per cluster -- done in parallel over all clusters using $\tt{torch.index\_reduce()}$ and is of the order $\mathcal{O}(2nd) = \mathcal{O}(n)$ for $n$ tokens. Hence the upper-bound time complexity of the MoE layer is unaffected. We provide in Table \ref{table: efficiency analysis} additional efficiency analysis in terms of throughput, max GPU memory allocated, and parameters which shows no significant efficiency loss compared to baseline MoE architectures.

% \begin{table}[h]
%   \caption{Efficiency Comparison between ACMoE and baseline MoE models}
%   \label{table: efficiency analysis}
%   \centering
%   \small
%   \begin{tabular}{lcccc}
%     \toprule
%     \text{Model} & \text{Compute Speed (ms/it)} & \text{Max Memory (K)} & \text{FLOPs / sample} &  \text{\#Params (M)}  \\
%     \midrule
%     \midrule
%     \textit{GLaM} & - & - & - & 220 \\
%     \text{GLaM-ACMoE}  & - & - & - & 220 \\
%     \midrule
%     \textit{Switch Transformer}  & - & - & - & 216 \\
%     \text{Switch-ACMoE}  & - & - & - & 216 \\
%     \midrule
%     \textit{Swin Transformer}  & 402.96 & 22.00 & - & 280 \\
%     \text{Swin-ACMoE}  & 409.56 & 22.29 & - & 280 \\
%     \bottomrule
%   \end{tabular}
% \end{table}


\section{Experimental Details and Additional Experiments}\label{appendix: experimental details}

% \subsection{Efficiency Results}

% We present in Table \ref{table: efficiency analysis} results on compute speed in iterations per second, max memory allocated as measured by $\tt{torch.cuda.max\_memory\_allocated()}$, and total parameters for our proposed ACMoE method against baseline architectures. We take as baseline the medium configurations of GLaM and Switch, which have are the 216M and 220M parameter models described in Table \ref{table: language modeling backbone spec}. The Swin configuration is the standard 18 layer configuration with 280M parameters. We see in all cases that ACMoE incurs negligible efficiency loss across tasks and backbones. This is due to ACMoE requiring no learnable parameters and just two $\mathcal{O}(nd)$ computations that can be computed in parallel.

\subsection{Language Modeling}

\subsubsection{Datasets} 

\paragraph{WikiText-103.}The WikiText-103\footnote{www.salesforce.com/products/einstein/ai-research/the-wikitext-dependency-language-modeling-dataset/} dataset contains around 268K words and its training set consists of about 28K articles with 103M tokens. This corresponds to text blocks of about 3600 words. The validation set and test sets consist of 60 articles with 218K and 246K tokens respectively. 

\paragraph{EnWik-8.}The EnWik-8 dataset is a byte-level dataset of 100 million bytes derived from Wikipedia that, in addition to English text, also includes  markup, special characters, and text in other languages. EnWik-8 contains 90M characters for training, 5M for validation, and 5M for testing.

\paragraph{Stanford Sentiment Treebank-2.} The Stanford Sentiment Treebank-2 (SST2) \citep{socher2013recursive} is a 2 class corpus with fully labeled parse trees for analysis of the compositional effects of sentiment in language. The dataset consists of 11,855 single sentences extracted from movie reviews. It was parsed with the Stanford parser and includes 215,154 unique phrases from the parse trees, each annotated by 3 human judges.

\paragraph{Stanford Sentiment Treebank-5.} Stanford Sentiment Treebank-5 (SST5) \citep{socher2013recursive} is a 5 class dataset used for sentiment analysis. It consists of 11,855 single sentences extracted from movie reviews. It includes 215,154 unique phrases from parse trees, each annotated by 3 human judges. Phrases are classified as negative, somewhat negative, neutral, somewhat positive, or positive.

\paragraph{Banking-77.} Banking-77 (B77) \citep{casanueva2020efficient} is a highly fine-grained 77 class classification dataset comprising 13083 customer service queries labelled with 77 intents.

\subsubsection{Model, Optimizer, \& Train Specification}\label{sec: language model spec}

\paragraph{Models.} We use as backbones the Switch Transformer \citep{fedus2022switch} and Generalist Language Model \citep{du2022glam}. Table \ref{table: language modeling backbone spec} contains the specification over self-attention (SA) layers, feed-forward network (FFN) layers, Mixture-of-Experts (MoE) layers, attention span (Att. Span), embedding size and parameter count for both backbones at small and medium configurations for each pretraining task. All backbones use 16 experts with top-2 expert routing.

\begin{table}[h]
  \caption{Language Modeling Backbone Specifications}
  \label{table: language modeling backbone spec}
  \centering
  \small
  \begin{tabular}{lcccccc}
    \toprule
    \text{Model} & \text{SA Layers} & \text{FFN Layers} & \text{MoE Layers} &  \text{Att. Span} & \text{Embed Size} & \text{Params}  \\
    \midrule
    \midrule
    \multicolumn{7}{c}{\textit{WikiText-103 Pretrain}} \\
    \midrule
    \midrule
    \text{Switch-small} & 3 & - & 3 & 256 & 128 & 70M  \\
    \text{Switch-medium}  & 6 & - & 6 & 1024 & 352 & 216M  \\
    \midrule
    \text{GLaM-small} & 6 & 3 & 3 & 2048 & 144 & 79M  \\
    \text{GLaM-medium}  & 12 & 6 & 6 & 2048 & 352 & 220M \\
    \midrule
    \midrule
    \multicolumn{7}{c}{\textit{EnWik-8 Pretrain}} \\
    \midrule
    \midrule
    \text{Switch}  & 8 & - & 8 & 2048 & 352 & 36M  \\
%    \midrule
%    \text{GLaM}  & 12 & 6 & 6 & 352 & 2048 & -  \\
    \bottomrule
  \end{tabular}
\end{table}

\paragraph{Optimizer.} All experiments use Adam with a base learning rate of 0.0007. Small configurations use 3000 iterations of learning rate warmup while medium configurations use 4000 iterations.

\paragraph{Pretrain Specification.} For WikiText-103 pretraining, small Switch backbones are trained for 40 epochs with a batch size of 96 and medium Switch backbones are trained for 80 epochs with a batch size of 48. Small GLaM backbones are trained for 60 epochs with a batch size of 48 and medium GLaM backbones are trained for 120 epochs with a batch size of 48. We use 0.01 auxiliary load balancing loss.

For EnWik-8 pretraining, both Switch and GLaM backbones are trained for 80 epochs with batch size 48. We use 0.01 auxiliary load balancing loss.

\paragraph{Finetune Specification.} For SST2 and SST5 finetuning, we finetune for 5 epochs using Adam and a base learning rate of 0.001 without warmup and a batch size of 16. For B77 we finetune for 50 epochs using Adam and a base elarning rate of 0.00001 without warmup and a batch size of 16.

\paragraph{Compute Resources.} All models are trained, evaluated, and finetuned on four NVIDIA A100 SXM4 40GB GPUs. 

\subsection{Image Classification}

\subsubsection{Datasets and Attacks}

\paragraph{ImageNet-1K.}  We use the full ImageNet dataset that contains 1.28M training images and 50K validation images. The model learns to predict the class of the input image among 1000 categories. We report the top-1 and top-5 accuracy on all experiments.

\paragraph{ImageNet-A/O/R.} ImageNet-A \citep{hendrycks2021natural} contains real-world adversarially filtered images that fool current ImageNet classifiers. A 200-class subset of the original ImageNet-1K’s 1000 classes is selected so that errors among these 200 classes would be considered egregious, which cover most broad categories spanned by ImageNet-1K. 

ImageNet-O \citep{hendrycks2021natural} contains adversarially filtered examples for ImageNet out-of-distribution detectors. The dataset contains samples from ImageNet-22K but not from ImageNet1K, where samples that are wrongly classified as an ImageNet-1K class with high confidence by a ResNet-50 are selected. 

Imagenet-R \citep{hendrycks2021many} contains various artistic renditions of object classes from the original ImageNet dataset, which is discouraged by the original ImageNet. ImageNet-R contains 30,000 image renditions for 200 ImageNet classes, where a subset of the ImageNet-1K classes is chosen.

\paragraph{Adversarial Attacks.}  We use produce corrupted ImageNet samples using white box attacks fast gradient sign method (FGSM) \citep{goodfellow2014explaining} and projected gradient descent (PGD) \citep{mkadry2017towards}, and black box simultaneous perturbation stochastic approximation (SPSA) \citep{uesato2018adversarial}. FGSM and PGD use a perturbation budget of 1/255 while SPSA uses a perturbation budget 1. All attacks perturb under $l_\infty$ norm. PGD and  uses 20 steps with step size of 0.15 and SPSA uses 20 iterations.


\subsubsection{Model, Optimizer, \& Train Specification}\label{sec: swin model spec}

\paragraph{Models.} Our results are based off of the Swin Transformer \citep{liu2021swin} architecture. This backbone uses 4 base layers of depth 2, 2, 18, and 2. The first two base layers each contain 2 self-attention layers and 2 feed-forward layers. The third base layer contains 18 self-attention layers with alternating feed-forward and MoE layers. The final base layer contains 2 self-attention layers with one feed-forward and one MoE layer. The embedding dimension is 96 and the heads per base layer are 3, 6, 12, and 24. We use 16 total experts and present results for both top-1 and top-2 expert routing. The total parameter count is 280M.

\paragraph{Optimizer.} We use AdamW with a base learning rate of 1.25e-4, minimum learning rate of 1.25e-7, 0.1 weight decay and cosine scheduling.

\paragraph{Train Specification.} We train for 60 epochs with a batch size of 128 and 0.1 auxiliary balancing loss.

\paragraph{Compute Resources.} All models are trained and evaluated on four NVIDIA A100 SXM4 40GB GPUs. 

\subsection{Adversarial Attack At Higher Perturbation Budget}

\begin{figure}[h]
    \centering
    \includegraphics[width=0.9\linewidth]{pgd_perturbs.png}
    \captionsetup{font=small}
    \vspace{-0.1in}
    \caption{ACMoE and Swin Transformer under PGD attack at increasing perturbation budgets. ACMoE widens its performance gain over Swin at increasingly severe attacks in both top-1 test accuracy (\textbf{left}) and top-5 test accuracy (\textbf{right}), starting at approximately 7\% improvement at 1/255 and ending at just over 10\% at 5/255.}
    \label{fig: pgd perturb}
\end{figure}

Figure \ref{fig: pgd perturb} shows that for PGD perturbation budgets 1/255 through to 5/255, ACMoE widens its already substantive robust performance gain over Swin, with top-1 and top-5 test accuracy improvements increasing from 7\% to approximately 10\%.

\subsection{Cluster Visualization}

\begin{figure}[h]
    \centering
    \includegraphics[width=0.9\linewidth]{tsne_1.png}
    \captionsetup{font=small}
    \caption{Cluster Visualization on ImageNet. Each token is represented as a point and colored by its assigned expert. \textbf{Left}: Swin identifies one cluster clearly (yellow/gold) but otherwise fails to distinguish remaining clusters \textbf{Right:} ACMoE learns better-defined expert clusters.}
    \label{fig: tsne}
\end{figure}

We pass random ImageNet batches through Swin and ACMoE and plot the representations along with their assigned experts, using t-sne to represent the high dimensional data in 2 dimensions. The result is shown in Fig. \ref{fig: tsne}, where we see Swin learns overlapping and indistinguishable expert clusters. ACMoE, on the other hand, performs better in learning the clusters, producing much clearer and better-distinguished clusters.

\subsection{Ablation Studies}

\subsubsection{Measures of Dispersion}

We present in Tables \ref{table: ablation - language varmad} and \ref{table: ablation - vision varmad} results for Switch-ACMoE and Swin-ACMoE when changing the measure of dispersion used in the AC routing transformation (Definition \ref{def: router transformation}) from mean absolute deviation (MAD) to variance. We see mean absolute deviation outperforms variance as a measure of spread. This is an intuitive finding given that squared distances, as used in variance computations, are highly sensitive to outliers. Using mean absolute deviation as an alternative measure of spread reduces this issue and produces a more robust estimate of dispersion. We note that MAD is not the only robust measure of spread. We conjecture that taking interquartile range as an additionally robust measure of spread may produce good results in both clean and contaminated data. We, however, leave this interesting direction to future research as interquartile range poses implementation challenges as it requires designing concurrent linear scans over the expert clusters. MAD, by contrast, requires just two computations of the mean which is easily parallelizable using \texttt{torch.index\_reduce()}.

\begin{table}[t]
\centering
\small
\begin{minipage}[t]{0.48\textwidth}
  \centering
  \caption{Ablation on Measure of Spread in Switch Transformer \citep{fedus2022switch}}
  \label{table: ablation - language varmad}
  \begin{tabular}{lc}
    \toprule
    \text{Measure of Spread} & \text{Test PPL ($\downarrow$)}  \\
    \midrule
    \midrule
     \text{Variance} & 34.87 \\
    \text{MAD} & \textbf{34.42}  \\
    \bottomrule
  \end{tabular}
  %\vspace{1.5in} % Adjust this value as needed
\end{minipage}
\hfill
\begin{minipage}[t]{0.48\textwidth}
  \centering
  \caption{Ablation on Layer Placement in Switch Transformer \citep{fedus2022switch}}
  \label{table: ablation - language layer placement}
  \begin{tabular}{lc}
    \toprule
    \text{Layer Placement} & \text{Test PPL ($\downarrow$)}  \\
    \midrule
    \midrule
    \text{Back Half} & 34.95  \\
     \text{Alternating} & 34.80 \\
    \text{Skip 1} & \textbf{34.42}  \\
    \text{Full} & 34.88  \\
    \bottomrule
  \end{tabular}
\end{minipage}%
\end{table}

\subsubsection{Layer Placement}

We consider the effect of layer placement in the Switch-medium configuration and in the Swin Transformer (see Sections \ref{sec: language model spec} and \ref{sec: swin model spec} for the full model specifications). In particular, Switch is a 6 layer model and Swin is a 24 layer model. With regard to Swin, we focus on the deepest block of depth 18 to implement our ACMoE layers. This is due to the change in embedding size between base layers, meaning we are restricted to this base layer of depth 18. Note further that Swin only uses MoE layers in an alternating pattern with feed-forward networks between each MoE layer. For example, for Switch, a full ACMoE specification would mean placing ACMoE on layers 2,3,4,5,6. For Swin, a full specification means placing ACMoE on layers 4,6,8,10,12,14,16,18. To examine the effect of layer placement we consider the following models:
\begin{itemize}
    \item \textit{Alternating}: For Switch this means we place ACMoE on layers 2,4,6. For Swin this means we place ACMoE on layers 4,8,12,16.
    \item \textit{Back Half}: For Switch this means we place ACMoE on just the last 3 layers of the network. For Swin this means we place ACMoE on just the last 5 layers of the network.
    \item \textit{Skip 2}: For Swin this means we palce ACMoE on layers 8,10,12,14,16,18.
    \item \textit{Skip 1}: For Switch this means we place ACMoE on layers 3,4,5,6. For Swin this means we place ACMoE on layers 6,8,10,12,14,16,18.
    \item \textit{Full}: We place ACMoE on every possible layer. 
\end{itemize}

We present in Table \ref{table: ablation - language layer placement} results for Switch and Swin ACMoE models when changing the positions of the ACMoE layers throughout the network. The results agree with our expectation that, generally speaking, more ACMoE layers improve performance, but a in some circumstances a threshold is met at the point where ACMoE layers are used too early in the network such that the model has not been able to learn reasonably good approximations of the cluster membership of the tokens yet. 

We find that in the Switch backbone, performance improves the more ACMoE layers we add, which agrees with our expectation that more ACMoE layers improve performance. However, we find that top performance is attained when allowing two standard MoE layers to go before the first ACMoE, as opposed to the minimum of 1 standard MoE layer. We conjecture this is because we need to give the model a few layers before the first ACMoE in order to learn decent representations such that we have good enough estimated cluster assignments for use in the ACMoE layer. Encouragingly, we find just one additional standard MoE layer is sufficient for the benefits of ACMoE to be obtained.   

We find in Table \ref{table: ablation - vision layer placement} that with Swin, best performance is obtained using ACMoE on every possible layer, again agreeing with our expectation that more ACMoE layers improve performance. With Swin, however, we do not face any drop in performance from placing ACMoE too early in the network, and indeed we see \textit{Full} attaining top performance. We conjecture that Swin does not encounter this issue since Swin uses four layers of feed forward networks before the first MoE layer, and so by the first MoE layer the representations are of reasonably good quality to produce good estimates of the cluster membership. 

% \begin{table}[t]
%   \caption{Effect on Layer Placement within backbone architecture for language modeling tasks. }
%   \label{table: ablation - language layer placement}
%   \centering
%   \begin{tabular}{lc}
%     \toprule
%     \text{Layer Placement} & \text{Test PPL ($\downarrow$)}  \\
%     \midrule
%     \midrule
%     \multicolumn{2}{c}{\textit{Switch} \citep{fedus2022switch}} \\
%     \midrule
%     \midrule
%      \text{Alternating} & 34.80 \\
%     \text{Back Half} & 34.95  \\
%     \text{Skip 1} & \textbf{34.42}  \\
%     \text{Full} & 34.88  \\
%     % \midrule
%     % \midrule
%     % \multicolumn{2}{c}{\textit{GLaM} \citep{du2022glam}} \\
%     % \midrule
%     % \midrule
%     % \text{Back Half} & -  \\
%     % \text{Full} & -  \\
%     % \text{Skip 1} & \textbf{36.26}  \\
%     \bottomrule
%   \end{tabular}
% \end{table}

% \begin{table}[h]
% \caption{Effect on Layer Placement within Swin backbone architecture for image classification.}
% \centering
% \begin{tabular}{lcc}
% \toprule
% \multirow{2}{*}{\text{Layer Placement}} & \multicolumn{2}{c}{\text{Test Acc.}}  \\
%  & Top 1 & Top 5 \\
% \midrule
% \midrule
% \multicolumn{3}{c}{\textit{Swin-Top1} \citep{liu2021swin}} \\
% \midrule
% \midrule
% \text{Back Half}  & - & - \\
% \text{Skip 1}  & - & - \\
% \text{Full}  & \textbf{75.39} & \textbf{92.56} \\
% \midrule
% \midrule
% \multicolumn{3}{c}{\textit{Swin-Top2} \citep{liu2021swin}} \\
% \midrule
% \midrule
% \text{Back Half}  & 76.16 & 93.02 \\
% \text{Skip 2} & 76.10 & 92.93 \\
% \text{Skip 1}  & 76.29 & 92.98 \\
% \text{Full}  & \textbf{76.31} & \textbf{93.14} \\
% \bottomrule
% \end{tabular}
% \label{table: ablation - image layer placement}
% % \vspace{-0.05in}
% \end{table}

\begin{table}[t]
\centering
\small
\begin{minipage}[t]{0.48\textwidth}
  \centering
    \caption{Ablation on Measure of Spread in Swin Transformer}
  \label{table: ablation - vision varmad}
  \begin{tabular}{lcc}
\toprule
\multirow{2}{*}{\text{Measure of Spread}} & \multicolumn{2}{c}{\text{Test Acc.}}  \\
 & Top 1 & Top 5 \\
\midrule
\midrule
\multicolumn{3}{c}{\textit{Swin-Top1} \citep{liu2021swin}} \\
\midrule
\midrule
\text{Variance}  & 75.06 & 92.49 \\
\text{MAD}  & \textbf{75.39} & \textbf{92.56} \\
\midrule
\midrule
\multicolumn{3}{c}{\textit{Swin-Top2} \citep{liu2021swin}} \\
\midrule
\midrule
\text{Variance}  & 76.11 & 93.08 \\
\text{MAD}  & \textbf{76.31} & \textbf{93.14} \\
    \bottomrule
  \end{tabular}
  %\vspace{1.5in} % Adjust this value as needed
\end{minipage}
\hfill
\begin{minipage}[t]{0.48\textwidth}
  \centering
  \caption{Ablation on Layer Placement in Swin Transformer}
  \label{table: ablation - vision layer placement}
  \begin{tabular}{lcc}
\toprule
\multirow{2}{*}{\text{Layer Placement}} & \multicolumn{2}{c}{\text{Test Acc.}}  \\
 & Top 1 & Top 5 \\
\midrule
\midrule
\multicolumn{3}{c}{\textit{Swin-Top1} \citep{liu2021swin}} \\
\midrule
\midrule
\text{Back Half}  & 75.16 & 92.46 \\
\text{Skip 2} & 75.34 & 92.42 \\
\text{Skip 1}  & 75.35 & 92.45 \\
\text{Full}  & \textbf{75.39} & \textbf{92.56} \\
\midrule
\midrule
\multicolumn{3}{c}{\textit{Swin-Top2} \citep{liu2021swin}} \\
\midrule
\midrule
\text{Back Half}  & 76.16 & 93.02 \\
\text{Skip 2} & 76.10 & 92.93 \\
\text{Skip 1}  & 76.29 & 92.98 \\
\text{Full}  & \textbf{76.31} & \textbf{93.14} \\
    \bottomrule
  \end{tabular}
\end{minipage}%
\end{table}


\subsubsection{Random Ablation}

\textcolor{black}{We show the efficacy of the adaptive clustering transformation $\bs M$ (Definition \ref{def: router transformation}) in our AC router at capturing meaningful feature-wise information by ablating it against an alternate $d \times d$ diagonal matrix made up of normal random variables with mean 1 and standard deviation 0.5  (where we clip any negative values to prevent negative weights). We present in Tables \ref{table: ablation - random switch} and \ref{table: ablation - random swin} results for language modeling (using Switch) and image classification (using Swin), which show fairly substantial drops in performance in both backbones. This offers evidence to the claim that our AC routing transformation is meaningfully weighting features to improve routing, and that performance gains of our proposed method do not flow from a kind of implicit regularization of introducing noise into the router.}

\begin{table}[h]
\centering
\small
\begin{minipage}[t]{0.48\textwidth}
  \centering
  \caption{\textcolor{black}{Random Ablation in Switch} \citep{fedus2022switch}}
  \label{table: ablation - random switch}
  \begin{tabular}{lc}
    \toprule
    Model     & Test PPL ($\downarrow$)   \\
    \midrule
    \midrule
    \textit{Switch-Random} \citep{fedus2022switch} & 38.17   \\
    %\textit{Elliptical-Method1-small}     & \textbf{32.95}    \\
    %\textit{Elliptical-small-kv}     & \textbf{33.11} 
    \text{Switch-ACMoE} & \textbf{34.42} \\
    %\textit{Elliptical-small-delta-meanscale} & 32.07 \\
    \bottomrule
  \end{tabular}
  %\vspace{1.5in} % Adjust this value as needed
\end{minipage}
\hfill
\begin{minipage}[t]{0.48\textwidth}
  \centering
  \caption{\textcolor{black}{Random Ablation in Swin} \citep{liu2021swin}}
  \label{table: ablation - random swin}
 \begin{tabular}{lcc}
    \toprule
    \text{Model} & \text{Top 1 Acc.} & Top 5 Acc.  \\
    \midrule
    \midrule
     \textit{Swin-Random} & 74.22 & 91.87 \\
    \text{Swin-ACMoE} & \textbf{76.31} & \textbf{93.14}   \\
    \bottomrule
  \end{tabular}
\end{minipage}%
\end{table}

% \subsubsection{Weight Scaling}

% We present in Table \ref{table: ablation - feature scale} results for Switch and Swin-ACMoE when changing the method of weight scaling within the ACMoE layer.



\subsection{Cluster Weight Mixing} \label{sec: appendix - cluster weight mixing}

\textcolor{black}{The AC routing scheme estimates the cluster membership of each token based on its highest affinity cluster assigned in the previous layer. We could also further leverage the top-k structure of the MoE models by mixing the cluster-wise feature weights with weights corresponding to the affinities in the top-k routing. For example, if $\bs h$ has affinity scores $\alpha$ and $1-\alpha$ to clusters $k$ and $k'$ respectively, then we could also obtain the required AC routing transformation for $\bs h$ as $\bs M_{k^*} = \alpha \bs M_{k} + (1-\alpha)\bs M_{k'}$. This approach therefore factors in the confidence with which we believe $\bs h$ belongs to cluster $k$ or $k'$, and can be used for integrating ACMoE into higher expert granularity backbones (i.e higher top-k settings). Tables \ref{table: ablation - weightmix switch} and \ref{table: ablation - weightmix glam} show results for computing $\bs M_{k^*}$ by mixing the top-affinity cluster weights (Mix 2) in Switch and GLaM with top-2 routing, versus our presented results which compute $\bs M_{k^*}$ just based off of the highest affinity cluster (Mix 1). We see that GLaM-ACMoE benefits substantially from cluster weight mixing whereas Switch-ACMoE prefers just using its top affinity cluster weights. For consistency across models, we present in our main body the Mix 1 results, as GLaM-ACMoE already performs extremely strongly using Mix 1 and so we prefer to opt for the added performance gain in the Switch backbone.}

\begin{table}[h]
\centering
\small
\begin{minipage}[t]{0.48\textwidth}
  \centering
  \caption{\textcolor{black}{Results on Cluster Weight Mixing in Switch} \citep{fedus2022switch}}
  \label{table: ablation - weightmix switch}
  \begin{tabular}{lc}
    \toprule
    \text{Clusters Mixed} & \text{Test PPL ($\downarrow$)}  \\
    \midrule
    \midrule
     \text{Mix 2} & 34.66 \\
    \text{Mix 1} & \textbf{34.42}  \\
    \bottomrule
  \end{tabular}
  %\vspace{1.5in} % Adjust this value as needed
\end{minipage}
\hfill
\begin{minipage}[t]{0.48\textwidth}
  \centering
  \caption{\textcolor{black}{Results on Cluster Weight Mixing in GLaM} \citep{du2022glam}}
  \label{table: ablation - weightmix glam}
 \begin{tabular}{lc}
    \toprule
    \text{Clusters Mixed} & \text{Test PPL ($\downarrow$)}  \\
    \midrule
    \midrule
     \text{Mix 2} & \textbf{35.29} \\
    \text{Mix 1} & 36.26  \\
    \bottomrule
  \end{tabular}
\end{minipage}%
\end{table}

\subsection{Adaptive Clustering Integration into Soft Mixture of Experts}

\textcolor{black}{We present here results for integrating ACMoE into SoftMoE \citep{puigcerver2023sparse}. To use ACMoE in the SoftMoe setting, which can be be understood as a top-E routing setting where all experts are active for every token, we compute $\bs M_{k^*}$ using cluster weight mixing (Section \ref{sec: appendix - cluster weight mixing}) over the top-8 highest affinity clusters. We present the performance of Soft-ACMoE on clean data, adversarially attacked data, and ImageNet-A/O/R in the following Tables \ref{table: imnet softmoe attack} and \ref{table: robust-im softmoe}.}

\begin{table}[h]
\small
\caption{\textcolor{black}{Test Accuracy on ImageNet corrupted PGD, FGSM, and SPSA using SoftMoE \citep{puigcerver2023sparse} backbone }}
\vspace{-0.14in}
\centering
\begin{tabular}{lcc|cccccc}
\toprule
\multirow{2}{*}{\text{Model}} & \multicolumn{2}{c|}{\text{Clean Data}} & \multicolumn{2}{c|}{\text{PGD}} & \multicolumn{2}{c|}{\text{FGSM}} & \multicolumn{2}{c}{\text{SPSA}} \\
 & Top 1 & Top 5 & Top 1 & Top 5 & Top 1 & Top 5 & Top 1 & Top 5 \\
\midrule
\midrule
\textit{SoftMoE} \citep{puigcerver2023sparse} & 72.86 & 90.92 & 45.29 & 78.91 & 56.95 & 85.60 & 66.59 & 88.70 \\
\text{Soft-ACMoE} (\textbf{Ours})  & \textbf{73.21} & \textbf{91.23}  & \textbf{48.25} & \textbf{80.49} & \textbf{59.01} & \textbf{86.69} & \textbf{70.63} & \textbf{93.22} \\
\bottomrule
\end{tabular}
\label{table: imnet softmoe attack}
\vspace{-0.1in}
\end{table}

\begin{table}[h]
    \small
  \caption{\textcolor{black}{Test Accuracy on Image Classification in Imagenet-A/O/R using SoftMoE \citep{puigcerver2023sparse} backbone}}
  \vspace{-0.14in}
  \label{table: robust-im softmoe}
  \centering
  \begin{tabular}{lccc}
    \toprule
    \multirow{2}{*}{\text{Model}} &  Im-A & Im-R & Im-O 
    \\
    & \text{Top-1 Acc. ($\uparrow$)} & \text{Top-1 Acc. ($\uparrow$)} & \text{AUPR ($\uparrow$)}  \\
    \midrule
    \midrule
     \textit{SoftMoE} \citep{puigcerver2023sparse} & 6.69 & 31.63 & 17.97 \\
    \text{Soft-ACMoE} (\textbf{Ours})  & \textbf{6.93}  & \textbf{32.18} & \textbf{18.35}\\
    % \midrule
    % \textit{Swin Transformer-Top 2} \citep{liu2021swin} & 9.38 & 32.07 & 18.51 \\
    % \text{Swin-ACMoE-Top 2} (\textbf{Ours})  & \textbf{9.42}  & \textbf{32.35} & \textbf{19.55} \\
    \bottomrule
  \end{tabular}
  \vspace{-0.1in}
\end{table}

\textcolor{black}{We see in Tables \ref{table: imnet softmoe attack} and \ref{table: robust-im softmoe} the efficacy of ACMoE in the SoftMoE backbone, offering evidence of the adaptability of our framework into further MoE setups. In particular, the SoftMoE framework models a setting in which expert clusters are highly overlapping, as each token is soft assigned to all experts. Therefore, the performance gains shown in clean and contaminated data of Soft-ACMoE demonstrates that our AC router is well-suited to modeling such a clustering structure.}

\subsection{Image Classification in Swin Transformer Base Configuration}

\textcolor{black}{We further evaluate the performance ACMoE when scaling up model size in Table \ref{table: imnet swin base attack}. We integrate ACMoE into the Base configuration of Swin (0.5B parameters) and evaluate on clean ImageNet-1K as well as under adversarial atacks.}

\begin{table}[h]
\small
\caption{\textcolor{black}{Test Accuracy on ImageNet corrupted PGD, FGSM, and SPSA using Swin Base \citep{liu2021swin} backbone }}
\vspace{-0.14in}
\centering
\begin{tabular}{lcc|cccccc}
\toprule
\multirow{2}{*}{\text{Model}} & \multicolumn{2}{c|}{\text{Clean Data}} & \multicolumn{2}{c|}{\text{PGD}} & \multicolumn{2}{c|}{\text{FGSM}} & \multicolumn{2}{c}{\text{SPSA}} \\
 & Top 1 & Top 5 & Top 1 & Top 5 & Top 1 & Top 5 & Top 1 & Top 5 \\
\midrule
\midrule
\textit{Swin-Base} \citep{liu2021swin} & 79.06 & 94.37 & 44.61 & 79.20 & 59.91 & 87.72 & 68.94 & 89.00 \\
\text{Swin-ACMoE-Base} (\textbf{Ours}) & \textbf{79.25} & \textbf{94.42} & \textbf{46.28} & \textbf{80.24} & \textbf{61.78} & \textbf{87.55} & \textbf{70.18} & \textbf{89.33} \\
\bottomrule
\end{tabular}
\label{table: imnet swin base attack}
\vspace{-0.1in}
\end{table}

% \begin{table}[h]
%     \small
%   \caption{Test Accuracy on Image Classification in Imagenet-A/O/R using Swin Base \citep{liu2021swin} backbone}
%   \vspace{-0.14in}
%   \label{table: robust-im swin base}
%   \centering
%   \begin{tabular}{lccc}
%     \toprule
%     \multirow{2}{*}{\text{Model}} &  Im-A & Im-R & Im-O 
%     \\
%     & \text{Top-1 Acc. ($\uparrow$)} & \text{Top-1 Acc. ($\uparrow$)} & \text{AUPR ($\uparrow$)}  \\
%     \midrule
%     \midrule
%      \textit{Swin-Base} \citep{liu2021swin} & 14.38 & 34.61 & 20.37 \\
%     \text{Swin-ACMoE-Base} (\textbf{Ours})  & \textbf{14.95} & \textbf{34.80} & - \\
%     % \midrule
%     % \textit{Swin Transformer-Top 2} \citep{liu2021swin} & 9.38 & 32.07 & 18.51 \\
%     % \text{Swin-ACMoE-Top 2} (\textbf{Ours})  & \textbf{9.42}  & \textbf{32.35} & \textbf{19.55} \\
%     \bottomrule
%   \end{tabular}
%   \vspace{-0.1in}
% \end{table}

\subsection{Router Stability}

\textcolor{black}{We present in Fig. \ref{fig: router stability} the routing stability of ACMoE, SMoE, XMoE, and StableMoE in the Switch backbone evaluated on WikiText-103. Routing instability computes over adjacent layers the proportion of tokens that are assigned to different experts across the two layers. Specifically, for $n$ tokens $[\bs h_1, \dots, \bs h_n]$, we compute at layer $\ell$ the matrix $\bs S^\ell \in \mathbb R^{n \times n}$ such that $\bs S^\ell_{ij} = 1$ if the $i^{th}$ and $j^{th}$ tokens are assigned to the same expert in layer $\ell$ and is $0$ otherwise. The router instability at layer $\ell$ can then be calculated as $r^\ell = \texttt{mean}(| \bs S^{\ell-1} - \bs S^\ell |)$. This metric therefore captures the degree to which tokens that are assigned to the same experts remain together through the model. A high $r^\ell$ indicates the router doesn't maintain consistent expert assignments, as tokens that it considers semantically similar at one layer it considers different at the next. }

\begin{figure}[h]
    \centering
    \includegraphics[width=0.7\linewidth]{router_stability.png}
    \caption{\textcolor{black}{Router Instability of ACMoE, SMoE, XMoE, and StableMoE. ACMoE maintains consistent routing, while baseline routers more frequently change the expert assignments of tokens.}}
    \label{fig: router stability}
\end{figure}

\textcolor{black}{In Fig. \ref{fig: router stability}, we see that baseline routers reach high levels of instability, where in the case of SMoE and StableMoE, at the last layer over 60\% of tokens are assigned to a different expert. ACMoE, by contrast, maintains a more consistent, stable assignment through the model, with no more than 20\% of tokens changing expert assignment across any layer.}

\subsection{Dynamic Routing}

\textcolor{black}{We further test the compatibility of our Adaptive Clustering routing scheme in dynamic top-p routing. In this setting, rather than routing each token to its top-k highest affinity experts in each MoE layer, we route each token to all experts that have affinity over a certain threshold $p$. This setting permits activating more or less experts for different tokens at different layers throughout the model, therefore dynamically assigning experts to tokens. We integrate our AC routing directly into this setting using the same setup as in Section \ref{sec: a tight cluster}, where the AC routing transformation is computed based on the estimated cluster membership of each token using the top affinity assignment of the previous layer. We present the results for Switch transformer on WikiText-103 language modeling in the following Table \ref{table: switch top p}.}

\begin{table}[h]
\centering
  \caption{\textcolor{black}{Results on Top-$p$ Dynamic Routing in Switch Backbone} \citep{fedus2022switch}}
  \label{table: switch top p}
  \begin{tabular}{lc}
    \toprule
    \text{Model} & \text{Test PPL ($\downarrow$)}  \\
    \midrule
    \midrule
    \multicolumn{2}{c}{\textit{Fixed top-k routing} \citep{shazeer2017sparsely}} \\ 
    \midrule
    \midrule
    \textit{Switch-medium} \citep{fedus2022switch} & 35.48 \\
    \text{ACMoE-medium} (\textbf{Ours}) & \textbf{34.42}  \\
    \midrule
    \midrule
    \multicolumn{2}{c}{\textit{Dynamic top-$p$ routing \citep{guo2024dynamic} }} \\ 
    \midrule
    \midrule
     \textit{Switch-Fixed $p$} & 35.20 \\
    \text{Switch-ACMoE-Fixed $p$} \textbf{(Ours)} & \textbf{34.14}  \\
    \midrule
      \textit{Switch-Learnable $p$} & 34.29 \\
    \text{Switch-ACMoE-Learnable $p$} \textbf{(Ours)} & \textbf{33.49}  \\
    \bottomrule
  \end{tabular}
\end{table}

\textcolor{black}{For fixed $p$, we set $p = 0.05$. For learnable $p$, we initialize the parameter to 0.05. We select this initialization as it reproduces approximately similar performance in the Switch backbone under default top-2 routing, thereby aiding direct comparison between fixed top-k and dynamic top-$p$ routing. We see in the dynamic routing setting, ACMoE maintains the same consistent improvement over the Switch baseline of roughly 1 full PPL. These results suggest ACMoE is well-suited to the dynamic routing setting.}

\section{Broader Impact}

Our research offers benefits to Mixture-of-Expert (MoE) architectures in both clean and contaminated settings. In particular, our work offers socially beneficial outcomes with regard to defense against adversarial attack, which we hope can be used to protect important AI systems from malicious actors. Furthermore, as large language models, many of which are built on MoE backbones, continue to profligate and be used in important societal settings, we hope our improved robustness to data contamination can aid this promising technology to continue to grow and improve in realistic settings of noisy training and evaluation data. Our research also shows substantially faster convergence than comparative baselines. We believe this faster convergence can deliver significant social benefit in terms of reducing the energy requirements of large model training, thereby helping to ease the growing environmental burden of AI training runs. We recognize there will always be risk of misuse with AI systems, however we hope that our work can be used to enhance and protect socially beneficial AI while also decreasing the environmental impact of this technology. We furthermore hope that our research can spur others on to continue building on robust and efficient AI for social good.

\end{document} 
