@article{bengio2015conditional,
  title={Conditional computation in neural networks for faster models},
  author={Bengio, Emmanuel and Bacon, Pierre-Luc and Pineau, Joelle and Precup, Doina},
  journal={arXiv preprint arXiv:1511.06297},
  year={2015}
}

@article{brusco2001variable,
  title={A variable-selection heuristic for K-means clustering},
  author={Brusco, Michael J and Cradit, J Dennis},
  journal={Psychometrika},
  volume={66},
  pages={249--270},
  year={2001},
  publisher={Springer}
}

@article{bubeck2021universal,
  title={A universal law of robustness via isoperimetry},
  author={Bubeck, S{\'e}bastien and Sellke, Mark},
  journal={Advances in Neural Information Processing Systems},
  volume={34},
  pages={28811--28822},
  year={2021}
}

@article{chen2022towards,
  title={Towards understanding the mixture-of-experts layer in deep learning},
  author={Chen, Zixiang and Deng, Yihe and Wu, Yue and Gu, Quanquan and Li, Yuanzhi},
  journal={Advances in neural information processing systems},
  volume={35},
  pages={23049--23062},
  year={2022}
}

@article{chi2022representation,
  title={On the representation collapse of sparse mixture of experts},
  author={Chi, Zewen and Dong, Li and Huang, Shaohan and Dai, Damai and Ma, Shuming and Patra, Barun and Singhal, Saksham and Bajaj, Payal and Song, Xia and Mao, Xian-Ling and others},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={34600--34613},
  year={2022}
}

@inproceedings{dikkala2023benefits,
  title={On the benefits of learning to route in mixture-of-experts models},
  author={Dikkala, Nishanth and Ghosh, Nikhil and Meka, Raghu and Panigrahy, Rina and Vyas, Nikhil and Wang, Xin},
  booktitle={Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing},
  pages={9376--9396},
  year={2023}
}

@article{friedman2004clustering,
  title={Clustering objects on subsets of attributes (with discussion)},
  author={Friedman, Jerome H and Meulman, Jacqueline J},
  journal={Journal of the Royal Statistical Society Series B: Statistical Methodology},
  volume={66},
  number={4},
  pages={815--849},
  year={2004},
  publisher={Oxford University Press}
}

@article{gnanadesikan1995weighting,
  title={Weighting and selection of variables for cluster analysis},
  author={Gnanadesikan, Ram and Kettenring, Jon R and Tsao, Shiao Li},
  journal={Journal of classification},
  volume={12},
  pages={113--136},
  year={1995},
  publisher={Springer}
}

@article{jacobs1991adaptive,
  title={Adaptive mixtures of local experts},
  author={Jacobs, Robert A and Jordan, Michael I and Nowlan, Steven J and Hinton, Geoffrey E},
  journal={Neural computation},
  volume={3},
  number={1},
  pages={79--87},
  year={1991},
  publisher={MIT Press}
}

@inproceedings{lewis2021base,
  title={Base layers: Simplifying training of large, sparse models},
  author={Lewis, Mike and Bhosale, Shruti and Dettmers, Tim and Goyal, Naman and Zettlemoyer, Luke},
  booktitle={International Conference on Machine Learning},
  pages={6265--6274},
  year={2021},
  organization={PMLR}
}

@article{liu2022sparsity,
  title={Sparsity-constrained optimal transport},
  author={Liu, Tianlin and Puigcerver, Joan and Blondel, Mathieu},
  journal={arXiv preprint arXiv:2209.15466},
  year={2022}
}

@article{puigcerver2022adversarial,
  title={On the adversarial robustness of mixture of experts},
  author={Puigcerver, Joan and Jenatton, Rodolphe and Riquelme, Carlos and Awasthi, Pranjal and Bhojanapalli, Srinadh},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={9660--9671},
  year={2022}
}

@article{puigcerver2023sparse,
  title={From sparse to soft mixtures of experts},
  author={Puigcerver, Joan and Riquelme, Carlos and Mustafa, Basil and Houlsby, Neil},
  journal={arXiv preprint arXiv:2308.00951},
  year={2023}
}

@article{roller2021hash,
  title={Hash layers for large sparse models},
  author={Roller, Stephen and Sukhbaatar, Sainbayar and Weston, Jason and others},
  journal={Advances in Neural Information Processing Systems},
  volume={34},
  pages={17555--17566},
  year={2021}
}

@article{shazeer2017sparsely,
  title={The sparsely-gated mixture-of-experts layer},
  author={Shazeer, N and Mirhoseini, A and Maziarz, K and Davis, A and Le, Q and Hinton, G and Dean, J},
  journal={Outrageously large neural networks},
  year={2017}
}

@article{van1989clustering,
  title={Clustering n objects into k groups under optimal scaling of variables},
  author={Van Buuren, Stef and Heiser, Willem J},
  journal={Psychometrika},
  volume={54},
  pages={699--706},
  year={1989},
  publisher={Springer}
}

@article{witten2010framework,
  title={A framework for feature selection in clustering},
  author={Witten, Daniela M and Tibshirani, Robert},
  journal={Journal of the American Statistical Association},
  volume={105},
  number={490},
  pages={713--726},
  year={2010},
  publisher={Taylor \& Francis}
}

@inproceedings{zhang2023robust,
  title={Robust mixture-of-expert training for convolutional neural networks},
  author={Zhang, Yihua and Cai, Ruisi and Chen, Tianlong and Zhang, Guanhua and Zhang, Huan and Chen, Pin-Yu and Chang, Shiyu and Wang, Zhangyang and Liu, Sijia},
  booktitle={Proceedings of the IEEE/CVF International Conference on Computer Vision},
  pages={90--101},
  year={2023}
}

@article{zhou2022mixture,
  title={Mixture-of-experts with expert choice routing},
  author={Zhou, Yanqi and Lei, Tao and Liu, Hanxiao and Du, Nan and Huang, Yanping and Zhao, Vincent and Dai, Andrew M and Le, Quoc V and Laudon, James and others},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={7103--7114},
  year={2022}
}

