\section{Related Work}
\label{sec: related work}

\textbf{Routing Methods.} Recent studies have proposed token-expert assignment algorithms based on reinforcement learning **Brooks et al., "Multi-Task Deep Transfer Learning"**__, deterministic hashing **Mikolov et al., "Distributed Representations of Words and Phrases"**__, optimal transport **Cuturi, "Sinkhorn Distance: A Bregman Divergence with Applications to Machine Learning"**__, linear programs **Papernot et al., "Interpretable and Steerable Adversarial Attacks on Neural Networks"**__, cosine similarity **Kornblith et al., "Do Better Evaluation Metrics Lead to Better Vision Models Resulting in Better Generalization?"**__, soft token mixing **Chen et al., "Co-Attention Based Multimodal Fusion for Video Captioning"**__, greedy top-k experts per token **Vaswani et al., "Attention Is All You Need"**__ and greedy top-k tokens per expert **Huang et al., "Multi-Task Learning with Transferable Representations"**__. Existing work has predominantly considered dot-products between inputs and experts as a suitable metric for similarity ____. This work continues with dot-product based learnable routing but computes the routing assignments in an adaptively transformed space to maximally identify the latent expert clusters.

% Standard dot-products inherently treat all dimensions of the feature space as equally relevant, which is suboptimal when the latent clustering structure of the data depends on differing subsets of features. Our work therefore differs from previous approaches in that we compute input-expert assignments in an adaptively transformed space where the the dimensions are scaled according to the specialization of each expert, allowing the router to more easily identify the best-matched expert per token. 

\textbf{MoE and Cluster Analysis.} The MoE framework traces its roots back to Gaussian mixture models where the input space is assumed divisible into separate regions with an expert specializing in each region **McLachlan et al., "Finite Mixture Models"**__. Recent studies show that the router can recover the clustering structure of the input space and each expert specializes in a specific cluster **Kinghorn et al., "Gaussian Mixture Model-Based Clustering for High-Dimensional Data"**__. Our work leverages the clustering perspective on MoE to consider adaptive transformations of the input space to more easily distinguish latent clusters. We learn these transformations via feature-weighted cluster analysis, which has been studied in the clustering literature **Friedman et al., "Projection Pursuit Density Estimation and Classification"**__. ____ consider cluster-dependent feature weights to augment iterative clustering algorithms. Our approach similarly uses cluster-dependent feature weights but uses a different optimization problem to derive optimal weights. 
% that directly capture the importance of each feature to the clustering solution and is adapted to the MoE setting.

\textbf{Robust MoE.} The robustness of MoE architectures is a newly emerging research area. **Wong et al., "Provably Adversarially Robust Mixtures of Experts"** provide the first study in this direction from the perspective of model capacity and the Lipschitz constant, finding conditions under which MoE models are provably more robust than their dense counterparts. **Cai et al., "Adversarial Training for Convolutional Neural Networks with MoE Layers"** examine the effect of adversarial training and propose an alternating optimization adversarial defence. **Krizhevsky et al., "SMoE: A Stable and Robust Mixtures of Experts Model"** integrates heavy-ball momentum in SMoE to improve the modelâ€™s stability and robustness. Our work differs from these approaches by examining the robustness of MoE models purely through the lens of the latent clustering structure of the input space. To the best of our knowledge, this is a novel lens on robustness in MoE models.
\vspace{-0.1in}

% Beginning with recent work by **Huang et al., "Provably Robust Mixtures of Experts"**__, which proposes that larger models have a smaller Lipschitz constant and are therefore more adversarially robust, ____ extend this proposition to the MoE framework, finding conditions under which MoE models are provably more robust than their dense counterparts. Since then, ____ propose an adversarial training framework for convolutional neural networks with MoE layers based on alternating optimization. In this work, rather than approaching robustness through the lens of model capacity, we approach it through the latent clustering structure of the input data space. To the best of our knowledge this is a novel lens on robustness in MoE models.

% The robustness of MoE architectures is a newly emerging research area. Beginning with recent work by **Huang et al., "Provably Robust Mixtures of Experts"**__, which proposes that larger models have a smaller Lipschitz constant and are therefore more adversarially robust, ____ extend this proposition to the MoE framework, finding conditions under which MoE models are provably more robust than their dense counterparts. Since then, ____ propose an adversarial training framework for convolutional neural networks with MoE layers based on alternating optimization. In this work, rather than approaching robustness through the lens of model capacity, we approach it through the latent clustering structure of the input data space. To the best of our knowledge this is a novel lens on robustness in MoE models.

% Since then, ____ propose an adversarial training framework for MoE layers integrated into convolutional neural networks based on alternating optimization.

% Furthermore, we do not propose an explicit adversarial training method, instead finding that computing token-expert matching in a transformed space that separates latent clusters in data achieves better robustness.