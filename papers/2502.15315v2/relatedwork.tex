\section{Related Work}
\label{sec: related work}

\textbf{Routing Methods.} Recent studies have proposed token-expert assignment algorithms based on reinforcement learning \citep{bengio2015conditional}, deterministic hashing \citep{roller2021hash}, optimal transport \citep{liu2022sparsity}, linear programs \citep{lewis2021base}, cosine similarity \citep{chi2022representation}, soft token mixing \citep{puigcerver2023sparse}, greedy top-k experts per token \citep{shazeer2017sparsely} and greedy top-k tokens per expert \citep{zhou2022mixture}. Existing work has predominantly considered dot-products between inputs and experts as a suitable metric for similarity \citep{lewis2021base, puigcerver2023sparse, shazeer2017sparsely, zhou2022mixture, chi2022representation}. This work continues with dot-product based learnable routing but computes the routing assignments in an adaptively transformed space to maximally identify the latent expert clusters.

% Standard dot-products inherently treat all dimensions of the feature space as equally relevant, which is suboptimal when the latent clustering structure of the data depends on differing subsets of features. Our work therefore differs from previous approaches in that we compute input-expert assignments in an adaptively transformed space where the the dimensions are scaled according to the specialization of each expert, allowing the router to more easily identify the best-matched expert per token. 

\textbf{MoE and Cluster Analysis.} The MoE framework traces its roots back to Gaussian mixture models where the input space is assumed divisible into separate regions with an expert specializing in each region \citep{jacobs1991adaptive}. Recent studies show that the router can recover the clustering structure of the input space and each expert specializes in a specific cluster \citep{dikkala2023benefits, chen2022towards}. Our work leverages the clustering perspective on MoE to consider adaptive transformations of the input space to more easily distinguish latent clusters. We learn these transformations via feature-weighted cluster analysis, which has been studied in the clustering literature \citep{brusco2001variable, witten2010framework, gnanadesikan1995weighting, van1989clustering, friedman2004clustering}. \citet{friedman2004clustering} consider cluster-dependent feature weights to augment iterative clustering algorithms. Our approach similarly uses cluster-dependent feature weights but uses a different optimization problem to derive optimal weights. 
% that directly capture the importance of each feature to the clustering solution and is adapted to the MoE setting.

\textbf{Robust MoE.} The robustness of MoE architectures is a newly emerging research area. \citet{puigcerver2022adversarial} provide the first study in this direction from the perspective of model capacity and the Lipschitz constant, finding conditions under which MoE models are provably more robust than their dense counterparts. \citet{zhang2023robust} examine the effect of adversarial training and propose an alternating optimization adversarial defence. \citet{teo2024momentumsmoe} integrates heavy-ball momentum in SMoE to improve the modelâ€™s stability and robustness. Our work differs from these approaches by examining the robustness of MoE models purely through the lens of the latent clustering structure of the input space. To the best of our knowledge, this is a novel lens on robustness in MoE models.
\vspace{-0.1in}

% Beginning with recent work by \cite{bubeck2021universal}, which proposes that larger models have a smaller Lipschitz constant and are therefore more adversarially robust, \cite{puigcerver2022adversarial} extend this proposition to the MoE framework, finding conditions under which MoE models are provably more robust than their dense counterparts. Since then, \cite{zhang2023robust} propose an adversarial training framework for convolutional neural networks with MoE layers based on alternating optimization. In this work, rather than approaching robustness through the lens of model capacity, we approach it through the latent clustering structure of the input data space. To the best of our knowledge this is a novel lens on robustness in MoE models.

% The robustness of MoE architectures is a newly emerging research area. Beginning with recent work by \cite{bubeck2021universal}, which proposes that larger models have a smaller Lipschitz constant and are therefore more adversarially robust, \cite{puigcerver2022adversarial} extend this proposition to the MoE framework, finding conditions under which MoE models are provably more robust than their dense counterparts. Since then, \cite{zhang2023robust} propose an adversarial training framework for convolutional neural networks with MoE layers based on alternating optimization. In this work, rather than approaching robustness through the lens of model capacity, we approach it through the latent clustering structure of the input data space. To the best of our knowledge this is a novel lens on robustness in MoE models.

% Since then, \cite{zhang2023robust} propose an adversarial training framework for MoE layers integrated into convolutional neural networks based on alternating optimization.

% Furthermore, we do not propose an explicit adversarial training method, instead finding that computing token-expert matching in a transformed space that separates latent clusters in data achieves better robustness.