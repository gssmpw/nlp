\section{Related Work}
\label{sec: related work}

\textbf{Routing Methods.} Recent studies have proposed token-expert assignment algorithms based on reinforcement learning ____, deterministic hashing ____, optimal transport ____, linear programs ____, cosine similarity ____, soft token mixing ____, greedy top-k experts per token ____ and greedy top-k tokens per expert ____. Existing work has predominantly considered dot-products between inputs and experts as a suitable metric for similarity ____. This work continues with dot-product based learnable routing but computes the routing assignments in an adaptively transformed space to maximally identify the latent expert clusters.

% Standard dot-products inherently treat all dimensions of the feature space as equally relevant, which is suboptimal when the latent clustering structure of the data depends on differing subsets of features. Our work therefore differs from previous approaches in that we compute input-expert assignments in an adaptively transformed space where the the dimensions are scaled according to the specialization of each expert, allowing the router to more easily identify the best-matched expert per token. 

\textbf{MoE and Cluster Analysis.} The MoE framework traces its roots back to Gaussian mixture models where the input space is assumed divisible into separate regions with an expert specializing in each region ____. Recent studies show that the router can recover the clustering structure of the input space and each expert specializes in a specific cluster ____. Our work leverages the clustering perspective on MoE to consider adaptive transformations of the input space to more easily distinguish latent clusters. We learn these transformations via feature-weighted cluster analysis, which has been studied in the clustering literature ____. ____ consider cluster-dependent feature weights to augment iterative clustering algorithms. Our approach similarly uses cluster-dependent feature weights but uses a different optimization problem to derive optimal weights. 
% that directly capture the importance of each feature to the clustering solution and is adapted to the MoE setting.

\textbf{Robust MoE.} The robustness of MoE architectures is a newly emerging research area. ____ provide the first study in this direction from the perspective of model capacity and the Lipschitz constant, finding conditions under which MoE models are provably more robust than their dense counterparts. ____ examine the effect of adversarial training and propose an alternating optimization adversarial defence. ____ integrates heavy-ball momentum in SMoE to improve the modelâ€™s stability and robustness. Our work differs from these approaches by examining the robustness of MoE models purely through the lens of the latent clustering structure of the input space. To the best of our knowledge, this is a novel lens on robustness in MoE models.
\vspace{-0.1in}

% Beginning with recent work by ____, which proposes that larger models have a smaller Lipschitz constant and are therefore more adversarially robust, ____ extend this proposition to the MoE framework, finding conditions under which MoE models are provably more robust than their dense counterparts. Since then, ____ propose an adversarial training framework for convolutional neural networks with MoE layers based on alternating optimization. In this work, rather than approaching robustness through the lens of model capacity, we approach it through the latent clustering structure of the input data space. To the best of our knowledge this is a novel lens on robustness in MoE models.

% The robustness of MoE architectures is a newly emerging research area. Beginning with recent work by ____, which proposes that larger models have a smaller Lipschitz constant and are therefore more adversarially robust, ____ extend this proposition to the MoE framework, finding conditions under which MoE models are provably more robust than their dense counterparts. Since then, ____ propose an adversarial training framework for convolutional neural networks with MoE layers based on alternating optimization. In this work, rather than approaching robustness through the lens of model capacity, we approach it through the latent clustering structure of the input data space. To the best of our knowledge this is a novel lens on robustness in MoE models.

% Since then, ____ propose an adversarial training framework for MoE layers integrated into convolutional neural networks based on alternating optimization.

% Furthermore, we do not propose an explicit adversarial training method, instead finding that computing token-expert matching in a transformed space that separates latent clusters in data achieves better robustness.