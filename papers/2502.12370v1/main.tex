\documentclass[10pt,twocolumn]{article}

% Essential packages - base packages first
\usepackage[utf8]{inputenc}
\usepackage[margin=1in]{geometry}
\usepackage{array}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{graphicx}
\usepackage{float}
\usepackage{booktabs}
\usepackage{multirow}
\usepackage{tikz}
\usepackage{url}
\usepackage{textcomp}
\usepackage{comment}
\usepackage{balance}
\usepackage{adjustbox}

% Algorithm package
\usepackage[linesnumbered,ruled,vlined]{algorithm2e}

% Caption and subfigure setup
\usepackage{caption}
\usepackage{subcaption}
\captionsetup[table]{font=small}
\captionsetup[subfigure]{subrefformat=simple,labelformat=simple}

% Section formatting 
\usepackage{titlesec}
\titleformat*{\section}{\normalsize\bfseries}
\titleformat*{\subsection}{\small\bfseries}

% Table font size
\newcommand{\tablefontsize}{\small}

% Hyperref should be last
\usepackage[hidelinks]{hyperref}


% Title and author information
\title{Positional Encoding in Transformer-Based Time Series Models: 
A Survey}
\author{Habib Irani and Vangelis Metsis\\
Computer Science Department, Texas State University\\
San Marcos, TX 78666, USA\\
\texttt{habibirani@txstate.edu, vmetsis@txstate.edu}}
\date{\today}

\begin{document}
\maketitle

\begin{abstract}
Recent advancements in transformer-based models have greatly improved time series analysis, providing robust solutions for tasks such as forecasting, anomaly detection, and classification. A crucial element of these models is positional encoding, which allows transformers to capture the intrinsic sequential nature of time series data. This survey systematically examines existing techniques for positional encoding in transformer-based time series models. We investigate a variety of methods, including fixed, learnable, relative, and hybrid approaches, and evaluate their effectiveness in different time series classification tasks. Furthermore, we outline key challenges and suggest potential research directions to enhance positional encoding strategies. By delivering a comprehensive overview and quantitative benchmarking, this survey intends to assist researchers and practitioners in selecting and designing effective positional encoding methods for transformer-based time series models. The source code for the methods and experiments discussed in this survey is available on GitHub\footnote{\url{https://github.com/imics-lab/positional-encoding-benchmark}}.
\end{abstract}


\section{Introduction}
Time series analysis, which involves studying data collected over sequential time intervals, is fundamental across a wide range of domains such as finance, healthcare, and climate science. Traditional approaches, such as Autoregressive Integrated Moving Average (ARIMA) and Seasonal Decomposition of Time Series (STL), have long been the foundation of statistical time series analysis. However, these models are inherently limited in their ability to capture non-linear and long-range dependencies. As a result, machine learning-based approaches, particularly Recurrent Neural Networks (RNNs) and Convolutional Neural Networks (CNNs), have gained popularity due to their ability to model complex temporal dynamics \cite{fawaz2019deep, wang2017time}.

RNNs, including their more advanced variants like Long Short-Term Memory (LSTM) and Gated Recurrent Units (GRU), excel at modeling sequential data by maintaining a hidden state that captures information from previous time steps. These architectures offer several advantages for time series analysis: they naturally handle irregular time intervals and missing data points through their sequential processing, excel at capturing local temporal patterns through their recurrent connections, and exhibit a beneficial "recency bias" where recent time steps are weighted more heavily than distant onesâ€”a characteristic particularly valuable in applications like financial forecasting and weather prediction. However, RNNs suffer from inherent limitations such as vanishing and exploding gradients, making it difficult to learn dependencies over long time horizons \cite{song2018attend}.

CNNs, on the other hand, have been adapted for time series by applying convolutional filters over the temporal dimension. While effective at capturing local patterns, their fixed receptive fields limit their ability to model long-term dependencies \cite{shih2019temporal}. Moreover, like RNNs, they may struggle with capturing global patterns and relationships between distant time steps.

In recent years, Transformer-based models, originally developed for natural language processing tasks, have shown remarkable promise in addressing these challenges for time series data \cite{vaswani2017attention, zeng2023transformers}. Transformers leverage a self-attention mechanism that enables them to capture both local and global dependencies in data without relying on sequential processing \cite{li2019enhancing}. This architecture allows Transformers to handle long sequences efficiently and mitigates the limitations of recurrence-based models. The self-attention mechanism computes pairwise dependencies between all time steps, providing a global context for each position in the sequence.

However, applying Transformer architectures to time series analysis presents unique challenges. Unlike RNNs, Transformers are inherently permutation-invariant and lack a natural understanding of sequence order \cite{wen2022transformers, huang2020improve, zerveas2021transformer}. They may also struggle with discontinuous time series data and irregular sampling intervals, necessitating specialized adaptations. Furthermore, although their global attention mechanism excels at capturing long-range dependencies, it can sometimes dilute the model's ability to focus on significant local patterns. The absence of a natural recency bias, which can be advantageous in many time series applications, may also require explicit architectural modifications.

Positional encoding mechanisms are therefore crucial to enabling Transformer models to distinguish between different time steps in a sequence and to learn temporal relationships effectively \cite{ke2021rethinking}. Initial methods, such as the sinusoidal absolute positional encoding introduced by Vaswani et al. \cite{vaswani2017attention}, used fixed positional information to capture the order of tokens. However, these techniques often struggle with varying sequence lengths and dynamic temporal relationships, which are common in time series data \cite{shaw2018self, lim2021temporal}.

Recent advancements in positional encoding have introduced learnable and relative \cite{shaw2018self} positional encodings, which offer more flexibility in capturing temporal dependencies. Hybrid approaches, which combine absolute and relative positional encoding strategies, have also been proposed to leverage the strengths of both methods. These techniques aim to provide a comprehensive representation of positional information, improving the model's ability to capture complex temporal dependencies while addressing some of the inherent limitations of transformer architectures in time series analysis.

In this survey, we present a systematic analysis of positional encoding methods in transformer-based time series models, categorizing them into three main types: absolute positional encodings, relative positional encodings, and hybrid approaches that combine both techniques. Our methodology encompasses both theoretical analysis and empirical evaluation. We first examine the mathematical foundations and computational characteristics of each encoding method, including their parameter complexity, time complexity, and space requirements. We then evaluate these methods using two distinct transformer architectures: one incorporating batch normalization and another utilizing patch embedding, to assess how architectural choices influence the effectiveness of different positional encoding strategies.

Our experimental evaluation focuses on time series classification tasks and spans eleven diverse datasets, including biomedical signals (EEG, EMG), sensor data, human activity recognition, and financial time series. We prioritize classification over forecasting or anomaly detection because classification tasks provide a more direct measure of a model's ability to capture temporal dependencies and understand global patterns in time series data. Unlike forecasting, which primarily emphasizes recent temporal context, or anomaly detection, which often relies on local pattern recognition, classification requires a comprehensive understanding of temporal relationships across the entire sequence length. This makes classification particularly well-suited for evaluating positional encoding methods' effectiveness in capturing both local and global temporal dependencies. This comprehensive evaluation reveals several key findings: advanced methods like Stochastic Positional Encoding (SPE) and Transformer with Untied Positional Encoding (TUPE) \cite{ke2020rethinking} consistently outperform traditional approaches, with performance advantages becoming more pronounced for longer sequences. We also find that the effectiveness of positional encoding methods varies significantly with sequence length, data dimensionality, and application domain. These insights provide valuable guidance for practitioners in selecting appropriate positional encoding methods based on their specific application requirements.


\section{Literature Review}
\subsection{Time Series Analysis}
Transformer models have emerged as a pivotal advancement in time series analysis, owing to their efficacy in capturing intricate temporal dependencies and long-range interactions. Traditional approaches such as recurrent neural networks (RNNs) \cite{hochreiter1997long} and convolutional neural networks (CNNs) \cite{lecun1995convolutional} have been foundational in time series forecasting, yet they face challenges like vanishing gradients in RNNs and limited ability to model long-term dependencies in CNNs. The Transformer architecture, introduced by Vaswani et al. \cite{vaswani2017attention}, revolutionized sequence modeling by leveraging self-attention mechanisms. Recent comprehensive surveys by Wen et al. \cite{wen2023transformers} and Li et al. \cite{li2023time} highlight the rapid evolution and current state of transformer applications in time series analysis.
This innovation enables Transformers to effectively weigh the relevance of elements within a sequence, irrespective of their temporal distance, thus overcoming the limitations of traditional models. Beyond natural language processing (NLP), Transformers have been adapted for time series applications, demonstrating superior performance in forecasting and representation learning tasks \cite{lim2021temporal, zhou2021informer}. In time series analysis, handling extensive sequences and complex temporal relationships is pivotal. While traditional models like ARIMA \cite{box2015time} and Exponential Smoothing \cite{hyndman2018forecasting} are constrained by their linear assumptions, transformers have shown remarkable success in capturing nonlinear patterns effectively. Wu et al. \cite{wu2021autoformer} introduced Autoformer, incorporating decomposition transformers with auto-correlation for improved long-term series forecasting. Researchers have tailored Transformer architectures to suit the nuances of time series data, leading to substantial enhancements in forecasting accuracy and computational efficiency \cite{zerveas2021transformer}.

\subsection{Transformer Model Architectures}
In time series analysis, adaptations of Transformers have led to specialized architectures tailored to exploit their strengths. For instance, the Transformer-based framework for multivariate time series representation learning \cite{zerveas2021transformer} integrates batch normalization to stabilize training, enhancing model performance on complex multivariate datasets. This framework underscores the significance of normalization techniques in fortifying Transformer models for diverse time series tasks. Moreover, inspired by vision Transformers, patch embeddings have been adapted for time series data. This approach, akin to the Vision Transformer (ViT) paradigm \cite{cordonnier2021differentiable}, segments time series into patches for localized processing. By encapsulating both local and global patterns, patch embeddings facilitate hierarchical pattern recognition, thereby improving model interpretability and performance in time series analysis.


\subsection{Positional Encoding Methods}
A crucial aspect of employing Transformers in time series tasks is the positional encoding mechanism. Initially proposed by Vaswani et al. \cite{vaswani2017attention}, fixed positional encodings utilize sinusoidal functions to embed positional information, ensuring the model can differentiate between distinct time steps. Subsequent advancements, such as learnable positional encodings and relative positional encodings introduced by Shaw et al. \cite{shaw2018self}, have further enhanced the model's ability to capture temporal relationships effectively. Recent innovations in positional encoding methods have further enriched Transformer-based approaches for time series analysis.
For instance, Foumani et al. \cite{foumani2024improving} introduced novel techniques like Time Absolute Position Encoding (tAPE) and Efficient Relative Position Encoding (eRPE), aimed at enhancing temporal dependency modeling in Transformer architectures. These methods aim to improve the model's ability to capture temporal dependencies effectively.
Furthermore, the authors developed the ConvTran model, which represents a structural fusion of multiple convolutional layers with a transformer architecture. This hybrid model is specifically designed for the classification of complex multivariate time series data. By combining the strengths of convolutional operations in capturing local patterns with the transformer's global attention mechanism, ConvTran aims to achieve superior performance in handling diverse and dynamic temporal relationships within time series datasets.

This survey aims to systematically compare various positional encoding methods within these Transformer-based architectures, evaluating their impact on time series forecasting and representation learning. By examining the strengths and weaknesses of each approach, we seek to provide insights into the most effective strategies for incorporating positional information in time series Transformers.
 
Table \ref{tab:techniuques} provides a comprehensive summary of various positional encoding techniques used in transformer-based time series models. The table categorizes these techniques based on three key criteria: (1) the Technique Type, which can be absolute (Abs) or relative (Rel) positional information; (2) the injection technique, which refers to how the positional information is incorporated into the model, either through additive positional embedding (APE) or manipulating attention matrices (MAM); and (3) whether the positional encoding is learnable during training or fixed. This classification helps in understanding the different approaches and their respective methodologies for embedding temporal information into transformer models.





\begin{table*}[t]
\tablefontsize
    \centering
    \caption{\small Summary of Positional Encoding Techniques}\label{tab:techniuques}
    \resizebox{\textwidth}{!}{%
    \footnotesize
    \begin{tabular}{@{}p{6.6cm}p{2.5cm}p{4.5cm}p{2.5cm}p{1cm}@{}}
    \toprule
    \textbf{Method} & \textbf{Technique Type} & \textbf{Injection Technique} & \textbf{Learnable/Fixed} & \textbf{Ref.} \\ \midrule
    Sinusoidal Positional Encoding (PE) & Absolute & Additive Positional Embedding & Fixed & \cite{vaswani2017attention} \\
    \midrule
    Learnable Positional Encoding (LPE) & Absolute & Additive Positional Embedding & Learnable & \cite{vaswani2017attention} \\
    \midrule
    Relative Positional Encoding (RPE) & Relative & Manipulating Attention Matrices & Fixed & \cite{shaw2018self} \\
    \midrule
    time Absolute Positional Encoding (tAPE) & Absolute & Additive Positional Embedding & Fixed & \cite{foumani2024improving} \\
    \midrule
    efficient Relative Positional Encoding (eRPE) & Relative & Manipulating Attention Matrices & Learnable & \cite{foumani2024improving} \\
    \midrule
    Transformer with Untied Positional Encoding (TUPE) & Rel+Abs & Manipulating Attention Matrices & Learnable & \cite{ke2020rethinking} \\
    \midrule
    Convolutional Sinusoidal Positional Encoding (ConvSPE) & Relative & Manipulating Attention Matrices & Learnable & \cite{liutkus2021relative} \\
    \midrule
    Temporal Positional Encoding (T-PE) & Rel+Abs & Combined techniques & Learnable+Fixed & \cite{zhang2024intriguing} \\
    \bottomrule
    \end{tabular}%
    }
\end{table*}


\begin{table}[t]
\caption{\small Comparison between the number of parameters, time complexity and space complexity of positional encoding methods.}
\begin{adjustbox}{width=\columnwidth}
\centering
\setlength{\tabcolsep}{4pt} % Adjust the column separation
\begin{tabular}{lccc}
\toprule

\textbf{Method} & \textbf{Number of Parameters} & \textbf{Time Complexity} &  \\
\midrule
Sinusoidal PE & 0 & $\mathcal{O}(Ld)$  \\
Learnable PE & $Ld$ & $\mathcal{O}(Ld)$  \\
RPE & $2(2L-1)dl$ & $\mathcal{O}(L^2d)$  \\
tAPE & $Ld$ & $\mathcal{O}(Ld)$  \\
eRPE & $(L^2 + L)l$ & $\mathcal{O}(L^2d)$  \\
TUPE & $2dl$ & $\mathcal{O}(Ld + d^2)$  \\
ConvSPE & $3Kdh + dl$ & $\mathcal{O}(LKR)$ \\
T-PE & $2d^2l/h + (2L + 2l)d$ & $\mathcal{O}(L^2d)$  \\
\bottomrule
\end{tabular}
\end{adjustbox}
\label{table:parameter}
\end{table}

The notations used in Table \ref{table:parameter} represent key parameters affecting the computational characteristics of different positional encoding methods. The parameter counts include all learnable parameters across layers, where $l$ represents the number of transformer layers, $h$ is the number of attention heads, and $d$ is the hidden dimension. For methods using relative positioning (RPE, eRPE), we include the full attention matrix parameters. For ConvSPE, K represents the convolutional kernel size that controls the local neighborhood for convolution operations.


\section{Formulation and Analysis of Positional Encoding Methods}
This section offers a fundamental explanation of self-attention and a summary of existing positional encoding models. It's important to distinguish between positional encoding, which refers to the approach used to incorporate positional information (e.g., absolute or relative), and positional embedding, which denotes the numerical vector associated with that encoding.

This section analyzes positional encoding architectures, examining their mathematical foundations, implementation frameworks, and operational characteristics. 

\subsection{Problem definition and notation}
Given a time series dataset \( X \) consisting of \( n \) samples, \( X = \{x_1, x_2, \dots, x_n\} \), where each sample \( x_i = \{x_{i1}, x_{i2}, \dots, x_{iL}\} \) represents a \( d_x \)-dimensional time series of length \( L \), such that \( x_i \in \mathbb{R}^{L \times d_x} \). Additionally, let \( Y = \{y_1, y_2, \dots, y_n\} \) denote the corresponding set of labels, where \( y_t \in \{1, 2, \dots, c\} \) and \( c \) is the number of classes. The objective is to train a neural network classifier to map the input dataset \( X \) to its corresponding label set \( Y \).

\subsection{Self-Attention Mechanism}

\begin{figure}
    \centering
    \includegraphics[width=\columnwidth]{Images/self.png}
    \caption{\small Scaled Dot-Product Attention and Multi-Head Attention mechanisms. On the left, the Scaled Dot-Product Attention computes attention scores by scaling the dot product of query and key matrices and applying a softmax function, which is then used to weight the value matrix. On the right, the Multi-Head Attention mechanism processes multiple scaled dot-product attention operations in parallel, concatenates their outputs, and passes them through a final linear layer to generate the output. This allows the model to capture diverse contextual relationships in the data.}
    \label{fig:self_attention}
\end{figure}

The self-attention mechanism is a key component of Transformer models, enabling them to capture long-range dependencies in sequential data. Given an input sequence \( X \in \mathbb{R}^{L \times d_x} \), the self-attention mechanism computes a weighted representation of each element in the sequence by attending to all other elements, as illustrated in Fig. \ref{fig:self_attention}.

First, three linear projections are applied to the input sequence to obtain the query (\(Q\)), key (\(K\)), and value (\(V\)) matrices:

\[
Q = XW_Q, \quad K = XW_K, \quad V = XW_V,
\]

where \( W_Q, W_K, W_V \in \mathbb{R}^{d_x \times d_k} \) are learnable weight matrices, and \( d_k \) is the dimension of the query and key vectors.

Next, the attention scores are computed by taking the scaled dot product between the query and key matrices:

\[
Attention(Q, K, V) = softmax\left(\frac{QK^\top}{\sqrt{d_k}}\right)V,
\]

where \( \frac{1}{\sqrt{d_k}} \) is a scaling factor to prevent the dot product from growing too large, and the \(\text{softmax}\) function ensures that the attention scores sum to 1.

The output of the self-attention mechanism is a weighted sum of the value vectors, where the weights are determined by the attention scores. This process allows the model to focus on the most relevant parts of the sequence when generating each element of the output.



To enhance the model's capacity, self-attention can be computed in multiple parallel attention heads. Each head learns a distinct representation by using different sets of projection matrices:

\begin{align*}
MultiHead(Q, K, V) =  \\ Concat(head_1, head_2, \dots, head_h)W_O,
\end{align*}
where each \(head_i = Attention(Q_i, K_i, V_i)\), \( h \) is the number of attention heads, and \( W_O \in \mathbb{R}^{hd_k \times d_x} \) is a learnable output projection matrix.

Transformers lack an inherent notion of sequence order, as self-attention operates on sets of inputs without considering their positions. To address this, positional encoding is introduced to incorporate positional information into the model, allowing it to understand the order of elements in a sequence.

Let the input sequence be \( X \in \mathbb{R}^{L \times d_x} \), where \( L \) is the sequence length and \( d_x \) is the feature dimension. Positional encoding adds a unique vector to each position in the sequence, resulting in a new representation \( X' \):

\[
X' = X + P,
\]

where \( P \in \mathbb{R}^{L \times d_x} \) is the positional encoding matrix.



\subsection{Absolute Positional Encoding}
Absolute positional encoding assigns a unique positional vector to each time step in the sequence. The most common approach is using sinusoidal functions, introduced by Vaswani et al. \cite{vaswani2017attention}. This method allows the model to embed positional information without requiring it to learn these representations during training, making it computationally efficient for long sequences.
\subsubsection{Fixed Absolute Positional Encoding}
The fixed absolute positional encoding is defined as follows:
\begin{align*}
    PE_{(pos, 2i)} &= \sin\left(\frac{pos}{10000^{\frac{2i}{d}}}\right) \\
    PE_{(pos, 2i+1)} &= \cos\left(\frac{pos}{10000^{\frac{2i}{d}}}\right)
\end{align*}
where \( pos \) is the position (time step) in the sequence, \( i \) is the dimension, and \( d \) is the dimensionality of the positional encoding.

The use of sinusoidal functions ensures that the encodings for nearby positions are similar, which can help the model learn relationships between closely spaced time steps. However, this approach has limitations when dealing with sequences of varying lengths or non-uniform time intervals, which are common in time series data.

\subsubsection{Learnable Positional Encoding}
Learnable positional encoding provides more flexibility by allowing the model to learn the positional representations during training. This can be advantageous for time series tasks where the relationships between time steps may be complex or nonlinear. The learnable positional encoding is typically defined as:

\begin{equation*}
PE_{learnable}({pos}) = W_{{pos}}
\end{equation*}
where \( W_{pos} \) is an \( d_{\text{model}} \)-dimensional vector representing the learnable positional encoding parameters for position \( pos \).

During the forward pass of the neural network, the learnable positional encoding is added to the input embeddings \( x \):
\begin{equation*}
x = x + PE_{learnable}[:x.size(1), :]
\end{equation*}
where \( x \) represents the input tensor at a given position and \( x.\text{size}(1) \) denotes the length of the sequence up to the current position. This addition enables the model to learn position-specific transformations that can capture dependencies and patterns within sequences.

The flexibility and adaptability of learnable positional encoding make it a powerful tool for enhancing the modeling of temporal dependencies and patterns in time series data within transformer architectures.

Then, the self-attention mechanism is modified as follows:
\begin{align*}
    \text{Attention}(Q, K, V) = \\ \text{softmax}\left(\frac{(Q + PE) \cdot (K + PE)^T}{\sqrt{d_k}}\right) V
\end{align*}
where \( Q \), \( K \), and \( V \) are the query, key, and value matrices, respectively, \( PE \) represents the positional encoding matrix, and \( d_k \) is the dimensionality of the key vectors \cite{vaswani2017attention}.

\subsection{Relative Positional Encoding}

In relative positional encoding, the attention mechanism is adjusted to incorporate the relative positions between time steps, making it more flexible in capturing dependencies between time steps regardless of their absolute positions in the sequence.

The attention score with relative positional encoding is computed as:
\begin{equation*}
    e_{ij} = \frac{(x_i W_Q) (x_j W_K + a^K_{ij})^T}{\sqrt{d_z}}
\end{equation*}

\begin{equation*}
    e_{ij} = \frac{(x_i W_Q) (x_j W_K)^T + (x_i W_Q) (a^K_{ij})^T}{\sqrt{d_z}}
\end{equation*}

\begin{equation*}
    a^K_{ij} = W_K^r r_{i-j}
\end{equation*}

where \( x_i \) and \( x_j \) are the input representations at positions \( i \) and \( j \),  
\( W_Q \) and \( W_K \) are the learnable projection matrices for queries and keys,  
and \( a^K_{ij} \) is the relative positional encoding defined as \( a^K_{ij} = W_K^r r_{i-j} \),  
where \( r_{i-j} \) is the learnable relative positional embedding for the distance \( i - j \).


This formulation allows the model to better capture temporal dependencies that depend on the relative position between time steps, which is particularly useful for tasks such as time series forecasting or classification when dealing with varying sequence lengths \cite{shaw2018self, raffel2020exploring}.

\begin{figure*}
    \centering
    \includegraphics[width=0.7\textwidth,trim={1cm 0 0 0},clip]{Images/Positional-encoding2.png}
    \caption{\small Comparison of Absolute and Relative Positional Encoding in Self-Attention. Absolute positional encoding is typically applied by adding fixed positional vectors to the input embeddings and modifying the queries and keys. In contrast, relative positional encoding directly incorporates relative positional information into the attention score calculation. The function \(A\) represents the attention mechanism, where absolute modifies the input embeddings, while relative positional encoding adjusts the attention scores to capture positional dependencies more effectively.}
    \label{fig:positional_encodings}
\end{figure*}


As illustrated in Fig. \ref{fig:Hybrid}, positional encoding methods can be combined to leverage the advantages of both absolute and relative approaches. This combined architecture integrates absolute positional information through additive embeddings while incorporating relative position information in the attention computation.


\subsection{Time Absolute Position Encoding (tAPE)}

The \textit{time Absolute Position Encoding (tAPE)} improves upon the traditional sinusoidal-based absolute position encoding, which was originally designed for language models. In high-dimensional embeddings, such as 512 or 1024, the dot product between sinusoidal positional embeddings at different positions decays smoothly with increasing distance, maintaining what is referred to as \textit{distance awareness}. However, for lower-dimensional embeddings (e.g., 64 or 128), this property diminishes, leading to irregularities in capturing positional relationships.

In the original transformer, the frequency term is defined as:
\begin{equation*}
\omega_k = 10000^{-2k/d_{model}}
\end{equation*}
tAPE modifies this by introducing sequence length consideration:
\begin{equation*}
\omega_k^{new} = k \times \frac{d_{model}}{L}
\end{equation*}
where:

$d_{model}$ is the embedding dimension,
$L$ is the sequence length,
$\omega_k$ is the original frequency term and 
$\omega_k{new}$ is the adjusted frequency term

The positional encoding is then computed as:
\begin{equation*}
PE_{pos,2i} = \sin(pos \times \omega_k{new})
\end{equation*}
\begin{equation*}
PE_{pos,2i+1} = \cos(pos \times \omega_k{new})
\end{equation*}

tAPE introduces a modification that incorporates the sequence length into the frequency terms of the sine and cosine functions. This ensures a smoother and more regular dot product between positional embeddings, regardless of sequence length. By adjusting the frequency scaling with the embedding dimension, tAPE retains distance awareness even for lower-dimensional embeddings, making it more effective for time series tasks.

\subsection{Efficient Relative Position Encoding (eRPE)}

The \textit{efficient Relative Position Encoding (eRPE)} modifies traditional relative position encodings by adding the relative positional bias \textit{after} the softmax operation in the attention mechanism. This approach sharpens the positional embeddings, making them more effective for time series classification (TSC), where capturing key relative positions is crucial for identifying temporal patterns.

The attention score in eRPE is computed as:
\begin{equation*}
\alpha_i = \sum_{j\in L} \left(\underbrace{\frac{\exp(e_{i,j})}{\sum_{k\in L} \exp(e_{i,k})}}{A{i,j}} + w_{i-j}\right)x_j
\end{equation*}
where:

$w$ is a learnable parameter vector of size $2L-1$
$i-j+L$ indexes the relative position between positions $i$ and $j$
$L$ is the maximum sequence length
The relative position bias $w_{i-j+L}$ is added after the softmax operation

To implement eRPE efficiently, for a sequence length \(L\), a trainable parameter \(w\) of size \(2L - 1\) is created, representing the possible distances between positions. The relative scalar \(w_{i - j + L}\) for two positions \(i\) and \(j\) is added to the attention scores post-softmax, ensuring sharper position distinctions. This method also optimizes GPU usage through efficient indexing operations, making it scalable for longer time series.


\subsection{Transformer with Untied Positional Encoding (TUPE)}

The Transformer with Untied Positional Encoding (TUPE) \cite{ke2020rethinking} introduces a novel approach by explicitly separating content and positional information in the attention computation, unlike traditional methods that simply add positional embeddings to input embeddings. The attention score in TUPE is computed as:

\begin{align*}
    \alpha_{ij} = \frac{1}{\sqrt{2d}}(x_i^lW^{Q,l}) \cdot (x_j^lW^{K,l})^T + \text{reset}_\theta(\beta_{ij}, i, j)
\end{align*}

where $\beta_{ij} = \frac{1}{\sqrt{2d}}(p_iU^Q)(p_jU^K)^T$ represents position-to-position interactions. This separation prevents interference between content and positional patterns, allowing each component to be learned independently. For time series data, this is particularly advantageous as it enables the model to capture both temporal patterns (through positional correlations) and feature relationships (through content correlations) without mutual interference.

\subsection{Convolutional Sinusoidal Positional Encoding (ConvSPE)}

Convolutional Stochastic Positional Encoding (convSPE) \cite{liutkus2021relative} enables relative positional encoding through a convolution-based approach that maintains linear complexity. The method begins by sampling a random matrix $Z_d \in \mathbb{R}^{M \times R}$ with i.i.d. standard Gaussian entries, then applies learnable convolutional filters $\Phi^Q$ and $\Phi^K$ to obtain query and key encodings:

\begin{equation*}
    Q_d = Z_d * \Phi^Q, \quad K_d = Z_d * \Phi^K
\end{equation*}


where $*$ denotes convolution. The convolution operation naturally induces relative position awareness through the sliding window mechanism. The theoretical foundation ensures that for large $R$, the cross-covariance structure approximates the desired positional attention kernel:

\begin{equation}
    P_d \approx \frac{\bar{Q}_d\bar{K}_d^T}{R}
\end{equation}

This formulation maintains linear complexity while effectively capturing relative positions through the convolutional structure, making it particularly suitable for long time series sequences.

\subsection{Temporal Positional Encoding (T-PE)}
The \textit{Temporal Positional Encoding (T-PE)} method combines two components to capture both positional and temporal dependencies in time series data. The geometric component applies enhanced sinusoidal positional encoding across multiple layers of the transformer to maintain positional information throughout the network:

\begin{align*}
PE_{i, 2k} = \sin\left(\frac{i}{10000^{\frac{2k}{d}}}\right), \\ \quad PE_{i, 2k+1} = \cos\left(\frac{i}{10000^{\frac{2k}{d}}}\right)
\end{align*}

The semantic component adjusts attention scores by incorporating token similarity between positions \(i\) and \(j\):

\begin{equation*}
S(i, j) = \exp\left(-\frac{\|x_i - x_j\|^2}{2\sigma^2}\right)
\end{equation*}

The final positional encoding is a combination of both components:

\begin{equation*}
T\text{-}PE(i) = PE(i) + S(i, j)
\end{equation*}

This combined approach enables the model to capture both global and local temporal dependencies, improving performance in time series tasks.



\begin{figure}
    \centering
    \includegraphics[width=\columnwidth,trim={4mm 0 5mm 0},clip]{Images/PE.png}
    \caption{\small A combined approach to positional encoding: Absolute + Relative. This is one method to integrate both absolute and relative positional encodings, aiming to capture both global and local positional dependencies within the attention mechanism.}
    \label{fig:Hybrid}
\end{figure}



\section{Experimental Setup}

The experimental setup involves comparing different positional encoding methods in two distinct Transformer-based architectures for time series data. The Time Series Transformer \cite{zerveas2021transformer}, shown in Figure \ref{fig:batch_norm_architecture}, processes the raw multivariate time series data directly. For a time series with \(L\) timesteps and \(d\) channels, it treats each timestep as a token, resulting in \(L\) tokens of dimension \(d\) that are fed into the transformer layers. This architecture incorporates normalization layers after both the attention and feed-forward components to stabilize training and address internal covariate shifts. In contrast, the patch embedding transformer \cite{cordonnier2021differentiable}, illustrated in Figure \ref{fig:patch_embedding_architecture}, first transforms the input data using convolutional operations to create overlapping patches. Each patch spans multiple timesteps but maintains the full channel dimension, effectively reducing the sequence length while enriching the feature representation of each token. This results in \(L/p\) tokens (where \(p\) is the patch size) with increased feature dimensionality before being processed by the transformer layers. The detailed algorithms for both architectures are presented in Algorithm \ref{alg:batch_norm_transformer} and Algorithm \ref{alg:patch_embedding_transformer}, respectively. These architectures were chosen to evaluate positional encoding methods under different sequence processing paradigms: direct temporal processing in the Time Series Transformer versus hierarchical feature extraction through patch embedding.


\begin{comment}
\subsubsection{Transformer with Batch Normalization Architecture Details}

    \begin{itemize}
        \item \textbf{TimeSeriesPatchEmbedding (Conv1d + Positional Encoding)}
        \begin{itemize}
            \item \textbf{Conv1d Layer}: Converts input time series into patches.
            \item \textbf{Positional Encoding}: Injects positional information into patches.
        \end{itemize}
        \item \textbf{TransformerEncoder (4 layers)}
        \begin{itemize}
            \item \textbf{Transformer Encoder Layer 1-4}: Each layer includes multi-head attention and feed-forward network.
        \end{itemize}
        \item \textbf{Linear (Feed-Forward)}
        \begin{itemize}
            \item \textbf{Feed-Forward Layer}: Transforms encoded features to a higher-dimensional space.
        \end{itemize}
        \item \textbf{Linear (Classifier)}
        \begin{itemize}
            \item \textbf{Classification Head}: Outputs final class probabilities.
        \end{itemize}
    \end{itemize}

\end{comment}


% \begin{figure}
%     \centering
%     \includegraphics[width=\columnwidth]{Images/Arch-TST.png}
%     \caption{\small Time series transformer architecture.}
%     \label{fig:batch_norm_architecture}
% \end{figure}

% \begin{figure}
%     \centering
%     \includegraphics[width=\columnwidth]{Images/Arch-patch.png}
%     \caption{\small Time series transformer with patch embedding architecture.}
%     \label{fig:patch_embedding_architecture}
% \end{figure}

\begin{figure}
    \centering
    \begin{subfigure}[b]{\columnwidth}
        \centering
        \includegraphics[width=\textwidth]{Images/Arch-TST.png}
        \caption{\small Time Series Transformer}
        \label{fig:batch_norm_architecture}
    \end{subfigure}
    \begin{subfigure}[b]{\columnwidth}
        \vspace{5mm}
        \centering
        \includegraphics[width=\textwidth]{Images/Arch-patch.png}
        \caption{\small Patch Embedding Transformer}
        \label{fig:patch_embedding_architecture}
    \end{subfigure}
    \caption{\small Transformer architectures for time series classification.}
    \label{fig:architectures}
\end{figure}

\begin{comment}
\subsubsection{Transformer with patch embedding Architecture Details}


    \begin{itemize}
        \item \textbf{TimeSeriesPatchEmbedding (Conv1d + Positional Encoding)}
        \begin{itemize}
            \item \textbf{Conv1d Layer}: Converts input time series into patches.
            \item \textbf{Positional Encoding}: Injects positional information into patches.
        \end{itemize}
        \item \textbf{TransformerEncoder (4 layers)}
        \begin{itemize}
            \item \textbf{Transformer Encoder Layer 1-4}: Each layer includes multi-head attention and feed-forward network.
        \end{itemize}
        \item \textbf{Linear (Feed-Forward)}
        \begin{itemize}
            \item \textbf{Feed-Forward Layer}: Transforms encoded features to a higher-dimensional space.
        \end{itemize}
        \item \textbf{Linear (Classifier)}
        \begin{itemize}
            \item \textbf{Classification Head}: Outputs final class probabilities.
        \end{itemize}
    \end{itemize}
\end{comment}

\begin{algorithm}[tbp] 
\caption{\small Time Series Transformer}
\label{alg:batch_norm_transformer}

\begin{small}
\KwIn{Time series data $X \in \mathbb{R}^{N \times D}$}
\KwOut{Class probabilities $y \in \mathbb{R}^{C}$}

\textbf{TimeSeriesInputEncoder:}\\
\hspace{1em} \textbf{Linear Projection:} Map input time series $X$ to embeddings $E \in \mathbb{R}^{N \times D_e}$.\\
\hspace{1em} \textbf{BatchNorm1d:} Normalize embeddings across the batch.\\
\hspace{1em} \textbf{Positional Encoding:} Inject positional information into embeddings $E$.\\

\textbf{TransformerEncoder (4 Layers):}\\
\For{$l = 1$ to $4$}{
    \hspace{1em} \textbf{Layer $l$:}\\
    \hspace{2em} \textbf{Multi-Head Attention:} Compute attention-weighted representations.\\
    \hspace{2em} \textbf{BatchNorm1d:} Normalize outputs of attention along the feature dimension.\\
    \hspace{2em} \textbf{Feed-Forward Network:} Transform representations with a fully connected network.\\
    
    \hspace{2em} \textbf{BatchNorm1d:} Normalize outputs of the FFN.\\
}

\textbf{Linear (Classifier):}\\
\hspace{1em} Output final class probabilities $y$.
\end{small}
\end{algorithm}


\begin{algorithm}[tbp] 
\caption{\small Transformer with Patch Embedding}
\label{alg:patch_embedding_transformer}

\begin{small}
\KwIn{Time series data $X \in \mathbb{R}^{N \times D}$}
\KwOut{Class probabilities $y \in \mathbb{R}^{C}$}

\textbf{TimeSeriesPatchEmbedding:}\\
\hspace{1em} \textbf{Conv1d Layer:} Convert input time series $X$ into patches $P \in \mathbb{R}^{N_p \times D_p}$.\\
\hspace{1em} \textbf{Positional Encoding:} Inject positional information into patches $P$.\\

\textbf{TransformerEncoder (4 Layers):}\\
\For{$l = 1$ to $4$}{
    \hspace{1em} \textbf{Layer $l$:}\\
    \hspace{2em} \textbf{Multi-Head Attention:} Compute attention-weighted representations.\\
    \hspace{2em} \textbf{Feed-Forward Network:} Transform representations with a fully connected network.\\
}

\textbf{Linear (Feed-Forward):}\\
\hspace{1em} Transform encoded features to a higher-dimensional space.\\

\textbf{Linear (Classifier):}\\
\hspace{1em} Output final class probabilities $y$.
\end{small}
\end{algorithm}


\subsection{System Configuration}

We conducted all experiments on a high-performance Linux server with the following specifications: an AMD EPYC 7513 32-Core Processor (2 sockets, 128 threads), one NVIDIA RTX A5000 GPUs (24GB GDDR6 each, CUDA version 12.2, Driver version 535.113.01), 503GB of RAM, and a dual storage system consisting of an 892.7GB NVMe SSD and a 10.5TB HDD. The operating system used was Ubuntu 20.04.6 LTS (Kernel 5.4.0-200-generic). All implementations were carried out using Python 3.8.10 and PyTorch 2.4.1+cu121.


\subsection{Datasets} A comprehensive set of time series datasets from various domains is employed to evaluate the positional encoding methods' effectiveness across diverse applications. The selected datasets include:

\begin{itemize} 
\item \textbf{Sleep:} EEG (Electroencephalogram) signals with 178 time steps per instance. The dataset contains 478,785 training instances and 90,315 test instances, classified into five different stages of sleep. 
\item \textbf{ElectricDevices:} Electrical consumption data of household devices with 96 time steps. It contains 8,926 training instances and 7,711 test instances across seven device classes \cite{UCRArchive2018}. 
\item \textbf{FaceDetection:} EEG signals collected during face detection tasks, with 62 time steps per instance. The dataset has 5,890 training instances and 3,524 test instances for a binary classification task. 
\item \textbf{MelbournePedestrian:} Traffic data capturing pedestrian flow in Melbourne. Each instance contains 24 time steps, with a 10-class classification task, comprising 1,194 training instances and 2,439 test instances. 
\item \textbf{SharePriceIncrease:} Financial time-series data with 60 time steps per instance, used to predict whether a company's share price will increase. The dataset has 965 training instances and 965 test instances, with two classes. 
\item \textbf{LSST:} Astronomical time-series data from the Large Synoptic Survey Telescope, containing 36 time steps per instance. The dataset includes 2,459 training instances and 2,466 test instances for a 14-class classification task of celestial objects. 
\item \textbf{RacketSports:} Human activity recognition data from racket sports like tennis and badminton. Each instance contains 30 time steps, with 151 training instances and 152 test instances across four activity classes. 
\item \textbf{SelfRegulationSCP1:} EEG data related to self-regulation through slow cortical potentials (SCPs), consisting of 896 time steps per instance. The dataset contains 268 training instances and 293 test instances for binary classification. 
\item \textbf{UniMiB-SHAR:} Sensor data for human activity recognition, with 151 time steps per instance. The dataset comprises 4,601 training instances, 1,454 validation instances, and 1,524 test instances across nine activity classes \cite{micucci2017unimib}. 

\item \textbf{RoomOccupancy (Sensors):} Time-series data from non-intrusive environmental sensors, such as temperature, light, sound, and CO\textsubscript{2}, used to estimate the precise number of occupants in a room. The dataset has 10,129 instances with 18 features and is classified into four classes \cite{emg_data_for_gestures_481}.
    
\item \textbf{EMGGestures (EMG):} Electromyographic (EMG) signals recorded using the MYO Thalmic bracelet during various gesture executions. Each instance consists of 30 time steps with 9 features per time step. The dataset is classified into eight distinct gesture classes. The dataset is primarily used for human activity recognition in health and medicine-related applications \cite{room_occupancy_estimation_864}.


The characteristics of these datasets are summarized in Table \ref{tab:datasets}, which provides a comprehensive overview of their key properties including training and test set sizes, sequence lengths, number of classes, and channels. 

\end{itemize}

\begin{table*}[t]
\tablefontsize
\centering
\caption{\small Characteristics of time series datasets used in the experiments.}\label{tab:datasets}
\setlength{\tabcolsep}{4pt} % Adjust the column separation
\begin{tabular}{lccccccl}
\toprule
\textbf{Dataset} & \textbf{Train Size} & \textbf{Test Size} & \textbf{Length} & \textbf{Classes} & \textbf{Channels} & \textbf{Type} \\
\midrule
Sleep & 478,785 & 90,315 & 178 & 5 & 1 & EEG \\
ElectricDevices & 8,926 & 7,711 & 96 & 7 & 1 & Device \\
FaceDetection & 5,890 & 3,524 & 62 & 2 & 144 & EEG \\
MelbournePedestrian & 1,194 & 2,439 & 24 & 10 & 1 & Traffic \\
SharePriceIncrease & 965 & 965 & 60 & 2 & 1 & Financial \\
LSST & 2,459 & 2,466 & 36 & 14 & 6 & Other \\
RacketSports & 151 & 152 & 30 & 4 & 6 & HAR \\
SelfRegulationSCP1 & 268 & 293 & 896 & 2 & 6 & EEG \\
UniMiB-SHAR & 4,601 & 1,524 & 151 & 9 & 3 & HAR \\
RoomOccupancy & 8,103 & 2,026 & 30 & 4 & 18 & Sensor \\
EMGGestures & 1,800 & 450 & 30 & 8 & 9 & EMG \\
\bottomrule
\end{tabular}
\label{table:datasets}
\end{table*}

\subsection{Evaluation Procedure}
The experiments are designed to assess how well each positional encoding technique captures temporal dependencies and patterns in the data and how it affects the overall model performance on each architecture. The evaluation procedure includes:
\begin{enumerate}
    \item \textbf{Model Training:} Each model is trained on the training portion of the datasets using standard optimization techniques and hyperparameters.
    \item \textbf{Model Evaluation:} The trained models are evaluated on the validation and test sets to measure their performance according to the specified metrics.
    \item \textbf{Comparative Analysis:} The results are compared across different models to determine the effectiveness of each positional encoding method.
\end{enumerate}

The experiments aim to provide insights into the strengths and weaknesses of each positional encoding technique and identify the scenarios where each method performs best.

\section{Results}
Our experimental evaluation encompasses eight distinct positional encoding methods tested across eleven diverse time series datasets using two transformer architectures. The methods range from traditional approaches (PE, LPE) to relative encodings (RPE, eRPE) and advanced techniques (T-PE, SPE, TUPE), evaluated on both time series transformer and transformer with patch embedding architectures. The classification accuracies are presented in Tables \ref{table:batch_norm_transformer_results} and \ref{table:patch_embedding_transformer_results}, with performance visualizations provided through heatmaps in Figures \ref{fig:Heatmap1} and \ref{fig:Heatmap2}.

\subsection{Overall Performance}
The experimental results demonstrate a consistent superiority of advanced positional encoding methods across both architectural configurations. SPE achieves the highest average ranking (1.727 for time series transformer, 2.090 for patch embedding), closely followed by TUPE (1.909 and 2.272) and T-PE (2.636 and 2.363). This performance hierarchy remains remarkably stable across both architectures, though the patch embedding architecture exhibits more balanced performance among the top-performing methods, as shown in Figures \ref{fig:rank1} and \ref{fig:rank2}.





\subsection{Sequence Length Analysis}
The effectiveness of positional encoding methods shows a strong correlation with sequence length. For sequences exceeding 100 time steps, such as SelfRegulationSCP1 (896 steps) and Sleep (178 steps), advanced methods demonstrate substantial improvements, achieving 5-6\% higher accuracy compared to fixed encoding approaches. Medium-length sequences (50-100 steps) show moderate improvements of 3-4\%, while shorter sequences (less than 50 steps) exhibit smaller but consistent gains of 2-3\%.

\subsection{Dataset-Specific Performance}

\subsubsection{Biomedical Signals}
In EEG datasets (Sleep, FaceDetection, SelfRegulationSCP1), TUPE demonstrates optimal performance, achieving accuracies of 88.8\%, 70.5\%, and 90.7\% respectively under the time series transformer architecture. SPE shows comparable performance levels, particularly excelling in EMG data with 77.1\% accuracy. Both methods demonstrate superior capability in capturing the long-range dependencies characteristic of biomedical signals.

\subsubsection{Environmental and Sensor Data}
For sensor-based datasets (ElectricDevices, RoomOccupancy), SPE exhibits superior performance (75.5\% and 95.1\% respectively). While TUPE maintains competitive accuracy, relative encoding methods show improved performance in local pattern recognition. The performance advantage of advanced methods is particularly pronounced in datasets characterized by complex temporal dependencies.

\subsection{Architectural Impact}
While both architectures maintain similar method rankings, their performance characteristics exhibit notable differences. The time series transformer architecture shows more distinct performance gaps between method tiers, while the patch embedding architecture demonstrates closer competition among advanced methods. This suggests that architectural choice significantly influences the relative advantages of different encoding strategies, though the superiority of advanced methods remains consistent.

\subsection{Dimensionality and Class Complexity}
Performance patterns vary significantly with input dimensionality and class complexity. Higher-dimensional datasets (more than 5 channels) show more consistent improvements across methods, while lower-dimensional data exhibits more variable benefits. Classification tasks with more classes (exceeding 7) demonstrate smaller performance gaps between methods, though the ranking hierarchy remains stable.

The comprehensive results establish the robust superiority of advanced positional encoding methods, particularly in complex time series tasks with longer sequences or higher dimensionality. The consistent performance patterns across different architectures and dataset characteristics suggest the broader applicability of these advanced encoding strategies in time series analysis.


\begin{table*}[ht]
\centering
\caption{\small Accuracy comparison of Positional Encoding Methods in Time Series Transformer}
\begin{adjustbox}{width=\textwidth}
\renewcommand{\arraystretch}{1.3} 
\setlength{\tabcolsep}{8pt} 
\begin{tabular}{l|c|c|c|c|c|c|c|c}
\hline
\textbf{Dataset} & \textbf{Fixed} & \textbf{Learnable} & \textbf{Relative} & \textbf{tAPE} & \textbf{eRPE} & \textbf{T-PE} & \textbf{SPE}  & \textbf{TUPE}    \\ \specialrule{1.2pt}{0pt}{0pt}
Sleep (EEG)  & 0.854 & 0.862 & 0.867 & 0.871 & 0.875 & 0.881 & 0.883 & \textbf{0.888}  \\ 
ElectricDevices (Device)  & 0.682 & 0.697 & 0.723 & 0.718 & 0.731 & 0.741 & \textbf{0.755} & 0.743 \\ 
FaceDetection (EEG)  & 0.627 & 0.654 & 0.672 & 0.633 & 0.686 & 0.694 & 0.698 & \textbf{0.705} \\ 
MelbournePedestrian (Traffic)  & 0.675 & 0.704 & 0.729 & 0.701 & 0.742 & 0.748 & 0.754 & \textbf{0.762}  \\ 
SharePriceIncrease (Finance)  & 0.725 & 0.748 & 0.739 & 0.744 & 0.772 & 0.779 & \textbf{0.785} & 0.778  \\
LSST (Other)  & 0.581 & 0.596 & 0.613 & 0.609 & \textbf{0.632} & 0.629 & 0.628 & 0.623  \\
RacketSports (HAR)  & 0.742 & 0.762 & 0.775 & 0.761 & 0.804 & 0.811 & 0.813 & \textbf{0.819}  \\
SelfRegulationSCP1 (EEG)  & 0.849 & 0.857 & 0.871 & 0.853 & 0.891 & 0.898 & 0.901 & \textbf{0.907}  \\
UniMib (HAR)  & 0.841 & 0.855 & 0.865 & 0.852 & 0.876 & \textbf{0.882} & 0.879 & 0.878  \\
RoomOccupancy (Sensors) & 0.912 & 0.922 & 0.931 & 0.928 & 0.942 & 0.947 & \textbf{0.951} & 0.948  \\ 
EMGGestures (EMG)  & 0.713 & 0.709 & 0.735 & 0.721 & 0.751 & 0.763 & \textbf{0.771} & 0.766  \\ \hline
Avg Rank & 7.909 & 6.454 & 5.272 & 6.363 & 3.727 & 2.636 & \textbf{1.727} & 1.909 \\ \hline

\end{tabular}
\end{adjustbox}
\label{table:batch_norm_transformer_results}
\end{table*}

\begin{table*}[ht]
\centering
\caption{\small Accuracy comparison of Positional Encoding Methods in Time Series Transformer with Patch Embedding}
\begin{adjustbox}{width=\textwidth}
\renewcommand{\arraystretch}{1.3} 
\setlength{\tabcolsep}{8pt} 
\begin{tabular}{l|c|c|c|c|c|c|c|c}
\hline
\textbf{Dataset}  & \textbf{Fixed} & \textbf{Learnable} & \textbf{Relative} & \textbf{tAPE} & \textbf{eRPE} & \textbf{T-PE} & \textbf{SPE}  & \textbf{TUPE}    \\ \specialrule{1.2pt}{0pt}{0pt}
Sleep (EEG)  & 0.841 & 0.852 & 0.858 & 0.851 & 0.864 & 0.872 & 0.876 & \textbf{0.879}  \\ 
ElectricDevices (Device)  & 0.673 & 0.694 & 0.761 & 0.693 & 0.762 & 0.763 & \textbf{0.811} & 0.779 \\ 
FaceDetection (EEG)  & 0.618 & 0.642 & 0.671 & 0.653 & 0.678 & 0.686 & 0.674 & \textbf{0.695} \\ 
MelbournePedestrian (Traffic)  & 0.668 & 0.702 & 0.724 & 0.682 & 0.733 & 0.742 & \textbf{0.753} & 0.745  \\ 
SharePriceIncrease (Finance)  & 0.715 & 0.742 & 0.754 & 0.735 & 0.774 & 0.781 & \textbf{0.785} & 0.774  \\
LSST (Other)  & 0.573 & 0.582 & 0.596 & 0.584 & \textbf{0.611} & 0.602 & 0.601 & 0.595  \\
RacketSports (HAR)  & 0.733 & 0.756 & 0.767 & 0.761 & 0.801 & 0.792 & 0.775 & \textbf{0.805}  \\
SelfRegulationSCP1 (EEG)  & 0.836 & 0.844 & 0.829 & 0.842 & 0.856 & 0.871 & \textbf{0.883} & 0.875  \\
UniMib (HAR)  & 0.832 & 0.844 & 0.854 & 0.833 & 0.864 & \textbf{0.871} & 0.867 & 0.865  \\
RoomOccupancy (Sensors) & 0.905 & 0.911 & 0.918 & 0.922 & 0.929 & 0.931 & 0.934 & \textbf{0.937}  \\ 
EMGGestures (EMG)  & 0.697 & 0.692 & 0.722 & 0.713 & 0.741 & \textbf{0.752} & 0.746 & 0.742  \\ \hline
Avg Rank & 7.818 & 6.454 & 5.272 & 6.363 & 3.363 & 2.363 & \textbf{2.090} & 2.272 \\ \hline

\end{tabular}
\end{adjustbox}
\label{table:patch_embedding_transformer_results}
\end{table*}

\begin{figure}
   \centering
   \begin{subfigure}[b]{\columnwidth}
       \centering
       \includegraphics[width=\textwidth,trim={3cm 0 3cm 0},clip]{Images/rank-patch.png}
       \caption{\small Patch Embedding Transformer}
       \label{fig:rank1}
   \end{subfigure}
   \begin{subfigure}[b]{\columnwidth}
       \vspace{5mm}
       \centering
       \includegraphics[width=\textwidth,trim={3cm 0 3cm 0},clip]{Images/rank-batch.png}
       \caption{\small Time Series Transformer}
       \label{fig:rank2}
   \end{subfigure}
   \caption{\small Average rank of various combinations of absolute and relative position encodings across different transformer architectures.}
   \label{fig:ranks}
\end{figure}


\begin{figure*}
   \centering
   \begin{subfigure}[b]{\columnwidth}
       \centering
       \includegraphics[width=\textwidth]{Images/Heatmap-patch.png}
       \caption{\small Patch Embedding Transformer}
       \label{fig:Heatmap2}
   \end{subfigure}
   \begin{subfigure}[b]{\columnwidth}
       \centering
       \includegraphics[width=\textwidth]{Images/Heatmap-tst.png}
       \caption{\small Time Series Transformer}
       \label{fig:Heatmap1}
   \end{subfigure}
   \caption{\small Heatmaps illustrating the accuracy (\%) of various positional encoding methods across different datasets. Each cell represents the accuracy for a specific dataset and positional encoding method, with darker colors indicating higher accuracy.}
   \label{fig:heatmaps}
\end{figure*}

\section{Discussion}

\subsection{Impact of Sequence Characteristics}
Our comprehensive analysis reveals that the effectiveness of positional encoding methods is fundamentally influenced by sequence characteristics, particularly sequence length and temporal complexity. The superior performance of advanced methods (SPE, TUPE) becomes increasingly pronounced with sequence length, demonstrating up to 5.8\% improvement in sequences exceeding 100 time steps, compared to modest gains of 2-3\% in shorter sequences. This pattern strongly suggests that sophisticated encoding strategies are particularly crucial for applications involving extended temporal dependencies, such as EEG analysis and complex sensor data processing.

The relationship between sequence length and encoding method effectiveness can be attributed to two key factors. First, longer sequences present more opportunities for capturing complex temporal dependencies, allowing advanced methods to leverage their sophisticated encoding mechanisms more effectively. Second, the increased dimensionality of longer sequences provides a richer feature space for learning position-aware representations, enabling methods like TUPE and SPE to better differentiate between temporal patterns at various scales.

\subsection{Architectural Considerations}
The comparative analysis of transformer architectures reveals distinctive performance patterns that provide insights into the interaction between architectural choices and positional encoding strategies. While both architectures maintain similar method rankings, time series transformer architecture demonstrates more pronounced performance gaps between method tiers. This suggests that the normalization process amplifies the benefits of advanced encoding strategies, possibly by stabilizing the learning of complex positional patterns.

Conversely, the patch embedding architecture shows more balanced performance among top methods, particularly benefiting multivariate datasets where local feature interactions are crucial. This architectural characteristic suggests that patch embedding's inherent ability to capture local patterns complements the global temporal dependencies encoded by advanced positional encoding methods, resulting in more uniform performance improvements across different encoding strategies.

\subsection{Dataset-Specific Effectiveness}
Dataset characteristics significantly influence encoding method performance, revealing important patterns for practical applications. In biomedical signals, particularly EEG datasets, TUPE and SPE consistently excel, suggesting these methods' superior capability in capturing complex physiological patterns. Their effectiveness likely stems from their ability to model both local temporal dynamics and long-range dependencies characteristic of biological signals.

For environmental and sensor data, SPE's superior performance (up to 95.1\% accuracy) indicates its effectiveness in handling regular sampling patterns and clear temporal structures. This suggests that SPE's encoding strategy is particularly well-suited for datasets with consistent temporal patterns and well-defined periodicities. The performance advantages in these domains demonstrate the importance of matching encoding strategies to specific data characteristics.

\subsection{Method Selection Guidelines}
Our findings establish clear guidelines for method selection based on application characteristics, which we organize into three key considerations:

First, sequence length emerges as a primary factor in method selection. For sequences exceeding 100 time steps, the substantial performance gains of advanced methods (TUPE, SPE) clearly justify their increased computational complexity. However, for shorter sequences, the more modest performance improvements suggest that simpler methods may offer a better complexity-performance trade-off.

Second, data complexity significantly influences method effectiveness. Higher-dimensional data consistently benefits more from advanced encoding strategies, likely due to their superior ability to capture complex inter-dimensional temporal relationships. Similarly, binary classification tasks show larger relative improvements with sophisticated methods, possibly due to their enhanced ability to learn discriminative temporal features.

Third, computational requirements must be carefully considered. While advanced methods demonstrate superior performance, they also incur higher computational costs. This trade-off becomes more favorable for longer sequences where the performance gains are more substantial. For applications with real-time requirements or resource constraints, the selection of encoding methods should balance performance improvements against computational efficiency.

These insights provide valuable guidance for practitioners in selecting appropriate positional encoding methods based on their specific application requirements. The clear patterns in method effectiveness across different data characteristics enable informed decisions that optimize the performance-complexity trade-off for particular use cases.

\subsection{Future Research Directions}
Our analysis suggests several promising directions for future research. First, the development of adaptive encoding methods that automatically adjust their complexity based on sequence characteristics could provide optimal performance across varying data conditions. Second, investigating the integration of domain-specific temporal priors into encoding strategies could enhance performance for specific applications. Finally, exploring the relationship between positional encoding and attention mechanisms could lead to more efficient architectures for time series analysis.


\section{Conclusion}
In this survey, we have presented a comprehensive analysis of positional encoding methods in transformer-based time series models, examining their effectiveness across various time series tasks and data types. Through extensive experimentation with eight distinct encoding methods across eleven diverse datasets, we demonstrated that advanced methods like TUPE and SPE consistently outperform traditional approaches while maintaining computational efficiency. Our findings establish clear relationships between sequence characteristics and encoding method effectiveness, providing valuable guidance for practitioners in selecting appropriate methods based on their specific application requirements. These insights lay the foundation for future research in developing more efficient and adaptive positional encoding strategies for time series analysis.


\balance  % Balance columns on the last page
\bibliographystyle{plain}
\bibliography{references}  

\end{document}