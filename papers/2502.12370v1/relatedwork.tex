\section{Literature Review}
\subsection{Time Series Analysis}
Transformer models have emerged as a pivotal advancement in time series analysis, owing to their efficacy in capturing intricate temporal dependencies and long-range interactions. Traditional approaches such as recurrent neural networks (RNNs) \cite{hochreiter1997long} and convolutional neural networks (CNNs) \cite{lecun1995convolutional} have been foundational in time series forecasting, yet they face challenges like vanishing gradients in RNNs and limited ability to model long-term dependencies in CNNs. The Transformer architecture, introduced by Vaswani et al. \cite{vaswani2017attention}, revolutionized sequence modeling by leveraging self-attention mechanisms. Recent comprehensive surveys by Wen et al. \cite{wen2023transformers} and Li et al. \cite{li2023time} highlight the rapid evolution and current state of transformer applications in time series analysis.
This innovation enables Transformers to effectively weigh the relevance of elements within a sequence, irrespective of their temporal distance, thus overcoming the limitations of traditional models. Beyond natural language processing (NLP), Transformers have been adapted for time series applications, demonstrating superior performance in forecasting and representation learning tasks \cite{lim2021temporal, zhou2021informer}. In time series analysis, handling extensive sequences and complex temporal relationships is pivotal. While traditional models like ARIMA \cite{box2015time} and Exponential Smoothing \cite{hyndman2018forecasting} are constrained by their linear assumptions, transformers have shown remarkable success in capturing nonlinear patterns effectively. Wu et al. \cite{wu2021autoformer} introduced Autoformer, incorporating decomposition transformers with auto-correlation for improved long-term series forecasting. Researchers have tailored Transformer architectures to suit the nuances of time series data, leading to substantial enhancements in forecasting accuracy and computational efficiency \cite{zerveas2021transformer}.

\subsection{Transformer Model Architectures}
In time series analysis, adaptations of Transformers have led to specialized architectures tailored to exploit their strengths. For instance, the Transformer-based framework for multivariate time series representation learning \cite{zerveas2021transformer} integrates batch normalization to stabilize training, enhancing model performance on complex multivariate datasets. This framework underscores the significance of normalization techniques in fortifying Transformer models for diverse time series tasks. Moreover, inspired by vision Transformers, patch embeddings have been adapted for time series data. This approach, akin to the Vision Transformer (ViT) paradigm \cite{cordonnier2021differentiable}, segments time series into patches for localized processing. By encapsulating both local and global patterns, patch embeddings facilitate hierarchical pattern recognition, thereby improving model interpretability and performance in time series analysis.


\subsection{Positional Encoding Methods}
A crucial aspect of employing Transformers in time series tasks is the positional encoding mechanism. Initially proposed by Vaswani et al. \cite{vaswani2017attention}, fixed positional encodings utilize sinusoidal functions to embed positional information, ensuring the model can differentiate between distinct time steps. Subsequent advancements, such as learnable positional encodings and relative positional encodings introduced by Shaw et al. \cite{shaw2018self}, have further enhanced the model's ability to capture temporal relationships effectively. Recent innovations in positional encoding methods have further enriched Transformer-based approaches for time series analysis.
For instance, Foumani et al. \cite{foumani2024improving} introduced novel techniques like Time Absolute Position Encoding (tAPE) and Efficient Relative Position Encoding (eRPE), aimed at enhancing temporal dependency modeling in Transformer architectures. These methods aim to improve the model's ability to capture temporal dependencies effectively.
Furthermore, the authors developed the ConvTran model, which represents a structural fusion of multiple convolutional layers with a transformer architecture. This hybrid model is specifically designed for the classification of complex multivariate time series data. By combining the strengths of convolutional operations in capturing local patterns with the transformer's global attention mechanism, ConvTran aims to achieve superior performance in handling diverse and dynamic temporal relationships within time series datasets.

This survey aims to systematically compare various positional encoding methods within these Transformer-based architectures, evaluating their impact on time series forecasting and representation learning. By examining the strengths and weaknesses of each approach, we seek to provide insights into the most effective strategies for incorporating positional information in time series Transformers.
 
Table \ref{tab:techniuques} provides a comprehensive summary of various positional encoding techniques used in transformer-based time series models. The table categorizes these techniques based on three key criteria: (1) the Technique Type, which can be absolute (Abs) or relative (Rel) positional information; (2) the injection technique, which refers to how the positional information is incorporated into the model, either through additive positional embedding (APE) or manipulating attention matrices (MAM); and (3) whether the positional encoding is learnable during training or fixed. This classification helps in understanding the different approaches and their respective methodologies for embedding temporal information into transformer models.





\begin{table*}[t]
\tablefontsize
    \centering
    \caption{\small Summary of Positional Encoding Techniques}\label{tab:techniuques}
    \resizebox{\textwidth}{!}{%
    \footnotesize
    \begin{tabular}{@{}p{6.6cm}p{2.5cm}p{4.5cm}p{2.5cm}p{1cm}@{}}
    \toprule
    \textbf{Method} & \textbf{Technique Type} & \textbf{Injection Technique} & \textbf{Learnable/Fixed} & \textbf{Ref.} \\ \midrule
    Sinusoidal Positional Encoding (PE) & Absolute & Additive Positional Embedding & Fixed & \cite{vaswani2017attention} \\
    \midrule
    Learnable Positional Encoding (LPE) & Absolute & Additive Positional Embedding & Learnable & \cite{vaswani2017attention} \\
    \midrule
    Relative Positional Encoding (RPE) & Relative & Manipulating Attention Matrices & Fixed & \cite{shaw2018self} \\
    \midrule
    time Absolute Positional Encoding (tAPE) & Absolute & Additive Positional Embedding & Fixed & \cite{foumani2024improving} \\
    \midrule
    efficient Relative Positional Encoding (eRPE) & Relative & Manipulating Attention Matrices & Learnable & \cite{foumani2024improving} \\
    \midrule
    Transformer with Untied Positional Encoding (TUPE) & Rel+Abs & Manipulating Attention Matrices & Learnable & \cite{ke2020rethinking} \\
    \midrule
    Convolutional Sinusoidal Positional Encoding (ConvSPE) & Relative & Manipulating Attention Matrices & Learnable & \cite{liutkus2021relative} \\
    \midrule
    Temporal Positional Encoding (T-PE) & Rel+Abs & Combined techniques & Learnable+Fixed & \cite{zhang2024intriguing} \\
    \bottomrule
    \end{tabular}%
    }
\end{table*}


\begin{table}[t]
\caption{\small Comparison between the number of parameters, time complexity and space complexity of positional encoding methods.}
\begin{adjustbox}{width=\columnwidth}
\centering
\setlength{\tabcolsep}{4pt} % Adjust the column separation
\begin{tabular}{lccc}
\toprule

\textbf{Method} & \textbf{Number of Parameters} & \textbf{Time Complexity} &  \\
\midrule
Sinusoidal PE & 0 & $\mathcal{O}(Ld)$  \\
Learnable PE & $Ld$ & $\mathcal{O}(Ld)$  \\
RPE & $2(2L-1)dl$ & $\mathcal{O}(L^2d)$  \\
tAPE & $Ld$ & $\mathcal{O}(Ld)$  \\
eRPE & $(L^2 + L)l$ & $\mathcal{O}(L^2d)$  \\
TUPE & $2dl$ & $\mathcal{O}(Ld + d^2)$  \\
ConvSPE & $3Kdh + dl$ & $\mathcal{O}(LKR)$ \\
T-PE & $2d^2l/h + (2L + 2l)d$ & $\mathcal{O}(L^2d)$  \\
\bottomrule
\end{tabular}
\end{adjustbox}
\label{table:parameter}
\end{table}

The notations used in Table \ref{table:parameter} represent key parameters affecting the computational characteristics of different positional encoding methods. The parameter counts include all learnable parameters across layers, where $l$ represents the number of transformer layers, $h$ is the number of attention heads, and $d$ is the hidden dimension. For methods using relative positioning (RPE, eRPE), we include the full attention matrix parameters. For ConvSPE, K represents the convolutional kernel size that controls the local neighborhood for convolution operations.