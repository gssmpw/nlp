\section{Framework}~\label{framework}
This section introduces the framework~(Fig~\ref{fig:tax}) for characterizing the interactions derived from the collected research.
Initially, we introduced the overview of our framework about applying the ``5W1H'' guideline~\cite{ram_5ws_2018} to decompose interactions.
Then, we provided detailed information about the dimensions of ``5W1H''.
Through these interactions, users can push the boundaries of ABMS, catering to personalized research needs.

\begin{figure}[htp]
  \centering
  \includegraphics[width=\linewidth]{figure/taxonomy.pdf}
  \caption{%
    \textbf{The details of our taxonomy.} We have five key dimensions to construct our taxonomy: the goals that users want to achieve~(Why), the phases that users are involved~(When), the roles of users~(Who), the components of the system~(What), and the means of interactions~(How).}
  \label{fig:tax}
\end{figure}

\subsection{Types of Interactions}
Inspired by the taxonomy for human-LLM interactions proposed by Gao\etal~\cite{10.1145/3613905.3650786}, we adapted the ``5W1H'' guideline to categorize interactive methods from existing works.
Through an analysis of the characteristics of interactive methods, we have selected five key dimensions to construct our taxonomy:
\begin{itemize}
\item {\textbf{Why}}: the reasons or motivations behind the interactions. The goals users aim to achieve through interactive methods are difficult to accomplish with static models.
We summarize six goals: initialize the simulation, explore different scenarios, refine the model, evaluate the performance, analyze simulation data, and be immersed in the environment.
\item {\textbf{When}}: the phase at which users are involved in the simulation. 
We have divided it into three phases: pre-simulation~\cite{gao2023s3socialnetworksimulationlarge}, during-simulation~\cite{chen2023agentversefacilitatingmultiagentcollaboration,Padmakumar_Thomason_Shrivastava_Lange_Narayan-Chen_Gella_Piramuthu_Tur_Hakkani-Tur_2022}, and post-simulation~\cite{10520238}.
\item {\textbf{What}}: the components of the system controlled by users. 
Based on the features of the simulation system, we consider three main aspects of the model: agents, environment, and simulation configuration.
Additionally, we perform a secondary classification based on the three aspects, with the specific details explained in Section~\ref {what}.
\item {\textbf{Who}}: the roles users play during the interaction process. 
In this context, we employ an analogy from the field of theater, as the behavior of agents within the model parallels the actions of actors performing in a theatrical setting.
Therefore, we draw upon some related professions to correspond to the roles of users engaged in the model: scriptwriter, director, actor, prototype, and observer.
\item {\textbf{How}}: the means employed by users to interact with the model. 
We categorize it into interface, natural language, configuration setting, data integration, and physical movement.
\end{itemize}

Subsequently, we will provide a detailed description of the four dimensions, excluding the ``When'' dimension. The taxonomy framework is also shown in Figure~\ref{fig:tax}.

\subsection{Why: Classification of Goals}\label{goal}
After reviewing all the literature, we identified six goals that encapsulate the multifaceted role of human engagement in shaping ABMS.
They drive users to interact with the model since a non-interactive model may fail to align with the users' requirements sufficiently.

\textit{Initialize the Simulation.}
The first step, where users lay the foundation for the simulation, ensures that it aligns with the study's objectives.
Users can initiate the simulation by determining factors such as agents' characteristics, environmental variables, and simulation conditions~\cite{10.1145/3586183.3606763, 10.1145/3526113.3545616}.
Besides, users can decide when the simulations begin by issuing a start command~\cite{chen2023agentversefacilitatingmultiagentcollaboration,chan2023chatevalbetterllmbasedevaluators} and posing specific questions or requirements \cite{ren2023robotsaskhelpuncertainty}.
Typically, this step requires the users to incorporate domain-specific knowledge to set up the environment and populate the model with agents that reflect real-world entities or phenomena~\cite{GAUBE201392}.
With users' cooperation, the setup requirements for model initialization are met, laying the groundwork for the simulation to run.
% Humans initiate the simulation by defining the initial conditions, parameters, and agent behaviors. This step often requires the user to input domain-specific knowledge to set up the environment and populate the model with agents that accurately reflect real-world entities or phenomena. By determining factors such as population size, initial agent characteristics, and environmental variables, users lay the foundation for the simulation, ensuring that it aligns with the objectives of the study.

\textit{Explore Different Scenarios.}
One of the critical goals for users in ABMS is to explore various hypothetical scenarios by adjusting key parameters.
It facilitates users exploring how different assumptions or interventions may impact system dynamics~\cite{10.1145/3613904.3642159}.
It also enables the discovery of insights that may not be immediately apparent from the initial model configuration.
Users can conduct "what-if" analysis and test multiple hypotheses in real-time by simulating alternative futures to uncover patterns that are otherwise difficult to detect in a static model~\cite{10.1145/3526113.3545616}.
The iterative process allows for a more thorough analysis of potential risks and opportunities in the modeled system~\cite{hua2024warpeacewaragentlarge}.

% One of the critical tasks for humans in ABMS is to explore various hypothetical scenarios by adjusting key parameters and agent behaviors. This exploration helps to evaluate how different assumptions or interventions might impact system dynamics. By simulating alternative futures, users can identify patterns of agent interactions, system responses to external changes, and potential emergent behaviors. This task enables the discovery of insights that may not be immediately apparent from the initial model configuration.

\textit{Refine the Model.}
If the model's performance falls short of expectations, user intervention is required to improve its effectiveness.
For example, agent behaviors may not align with observed real-world outcomes perfectly due to the simplification of action rules or limitations of the algorithmic capabilities.
To improve the relevance of the simulation results, users can make enhancements or corrections directly through interaction methods~\cite{mandi2023rocodialecticmultirobotcollaboration}.
Additionally, the learning abilities of agents can be improved through user involvement by providing learning materials or managing the agents' 
memory~\cite{jin2024surrealdriverdesigningllmpoweredgenerative,unknown}.
Users can also directly collaborate with agents in solving tasks or guide agents with instructions\cite{mohanty2023transforminghumancenteredaicollaboration,zhang2024buildingcooperativeembodiedagents}.
Due to the randomness inherent in some simulation algorithms, users can refine the model simply by regenerating the results~\cite{10.1145/3526113.3545616}.
The ability to refine models ensures the model's predictive power and validity, leading to more robust and sophisticated outcomes.
% After observing preliminary simulation results, humans refine the model by adjusting parameters, improving agent rules, or incorporating additional factors to enhance the accuracy and relevance of the simulation. This iterative process requires humans to continuously interpret results, identify limitations in the model, and refine agent behaviors to better align with observed or expected real-world outcomes. The ability to refine models dynamically ensures that simulations evolve to reflect more sophisticated or realistic conditions over time.

\textit{Evaluate the Performance.}
Humans play a central role in evaluating the performance of the ABMS by assessing how well the simulation meets predefined goals, such as accurately representing system dynamics, producing meaningful results, or predicting real-world behaviors.
By integrating user-centered metrics, this evaluation typically goes beyond standard quantitative measures~(\eg accuracy, speed).
Furthermore, qualitative feedback by users who incorporate domain-expert knowledge and subjective insights is essential, particularly in the era of LLMs.
Users can assess whether agent behaviors accurately simulate human actions by applying common sense or domain-specific knowledge~\cite{10.1145/3526113.3545616,10.1145/3613905.3651026,wang2023humanoidagentsplatformsimulating}.
It is significant for users to evaluate how effectively the model adapts to different contexts or scenarios and handles changes in user goals, external factors, or input variations~\cite{10.1145/3613905.3651008,10.1145/3613904.3642947}.
% Humans play a central role in evaluating the performance of the ABMS by assessing how well the simulation meets predefined goals, such as accurately representing system dynamics, producing meaningful results, or predicting real-world behaviors. This evaluation involves both qualitative and quantitative measures, with users comparing simulation outputs against historical data, theoretical expectations, or desired outcomes. Human judgment is essential to determine the validity of the model, its predictive power, and the robustness of its results across different scenarios.

\textit{Analyze Simulation Data.}
Analyzing data generated by agent-based modeling and simulation is another critical goal for users.
Simulation data provides the foundation for understanding system behaviors, validating models, and making informed decisions.
Users can observe emergent patterns and system dynamics that may be difficult or impossible to study in the real world~\cite{berryman2008review}.
Furthermore, users employ statistical techniques~\cite{netlogo}, visual analytics~\cite{pan2024agentcoordvisuallyexploringcoordination,10520238}, and domain expertise~\cite{electronics12122722} to extract meaningful insights from the data, which can inform decision-making, strategy recommendations, or further model adjustments. 

% The analysis of data generated by agent-based simulations is another critical task for humans. This involves interpreting large amounts of output data, identifying significant trends, and making sense of complex interactions between agents. Users employ statistical techniques, visual analytics, and domain expertise to extract meaningful insights from the data, which can inform decision-making, policy recommendations, or further model adjustments. Through data analysis, humans transform raw simulation outputs into actionable knowledge.

\textit{Be Immersed in the Environment.}
In contrast to the goals mentioned above, being immersed in the environment emphasizes the user's experience within the simulation without the primary focus being on control or modification.
The focus is less on achieving a specific objective and more on how deeply the user engages with and experiences the simulation.
Direct interactions allow users to engage with agents' worlds actively, enhancing their entertainment experience~\cite{10.1145/3586183.3606763}.
It is most evident in mediums such as video games, virtual reality~(VR), and augmented reality~(AR), where users can fully immerse themselves in dynamic, interactive environments~\cite{mao2024alympicsllmagentsmeet}.
% Interactive ABMS environments often provide users with immersive tools, such as visual interfaces or virtual reality, enabling them to experience the simulated world from an agent's perspective or as an observer. This immersion enhances the user’s ability to understand agent behaviors and interactions at a deeper level. Being immersed in the environment helps humans intuitively grasp the dynamics of complex systems, allowing them to make more informed adjustments to the model or explore emergent phenomena in real-time.

% In summary, these six tasks encapsulate the multifaceted role of humans in shaping, guiding, and refining agent-based modeling and simulation processes. Through these interactions, humans and computational agents work in tandem to explore complex systems, derive meaningful insights, and push the boundaries of knowledge in various domains.

\subsection{What: Components of System}\label{what}
By analyzing the structure of the model, we detailed the components that users can control, focusing on three primary aspects: agents, environment, and simulation configuration.
\subsubsection{Agents} In ABMS, agents are often designed with human-like characteristics to simulate behaviors that closely mimic real-world scenarios. 
Agents are diverse, heterogeneous, and dynamic due to the complex components being divided into internal states and outward behaviors.
Based on the certain characteristics of agents proposed by Macal\etal~\cite{1574234}, we summarized five key components of internal states as follows:

\begin{itemize}
\item {\includegraphics[width=0.05\textwidth]{icon/identity.pdf}\textbf{Identity}}: We considered agents as discrete individuals with a set of attributes and rules~\cite{10.1145/3626772.3657844}. 
Agents can be endowed with human-like traits or specific behavioral abilities and rules~\cite{wang2024userbehaviorsimulationlarge}.

\item {\includegraphics[width=0.05\textwidth]{icon/interaction.pdf}\textbf{Interaction}}: Agents are capable of interacting with other agents, the environment, and humans.
The interactive protocol can include collaboration, competition, hierarchical relationships, or specific communication principles~\cite{hua2024warpeacewaragentlarge,pan2024agentcoordvisuallyexploringcoordination}.

\item {\includegraphics[width=0.05\textwidth]{icon/goal.pdf}\textbf{Goal}}: Agents typically have predefined goals they strive to accomplish. 
It is worth noting that agents may have both long-term~\cite{NEURIPS2023_5950bf29, NEURIPS2023_a3621ee9} and short-term goals~\cite{pan2024agentcoordvisuallyexploringcoordination}.
Long-term goals are strategic and involve sustained effort, while short-term goals are more immediate objectives that serve as incremental steps toward achieving long-term goals~\cite{shridhar2020alfredbenchmarkinterpretinggrounded}.

\item {\includegraphics[width=0.05\textwidth]{icon/autonomy.pdf}\textbf{Automony}}: Agents can function independently, making decisions and taking actions without direct human control. 
Specifically, agents adapt to environmental changes or interactions with other agents.

\item {\includegraphics[width=0.05\textwidth]{icon/learning.pdf}\textbf{Learning Ability}}: Agents have the capacity to learn from their experiences or adapt over time.
This learning ability enables agents to modify their behavior rules based on past outcomes or agents' memory, improving their performance or strategy as the simulation progresses~\cite{cui2024chatlawmultiagentcollaborativelegal,doi:10.1073/pnas.2115730119}.

\end{itemize}

These internal state components collectively govern the outward behaviors like humans:
\begin{itemize}
    \item {\includegraphics[width=0.05\textwidth]{icon/action.pdf}\textbf{Action}}: Observable behaviors performed by agents in response to their environment.
    These actions represent the agent's outward expression of its internal states.
\end{itemize}

Understanding both dimensions is crucial for designing realistic and effective simulations.
Together, these components allow agents to behave in human-like ways, offering rich, complex interactions that drive the sophistication of ABMS.
   
\subsubsection{Environment}
Prior to discussing the components of environments, we first present a classification of environments where agents reside.
The classification is represented across two dimensions: Physical \textit{vs.} Virtual and Real \textit{vs.} Simulated. 
This framework distinguishes environments based on their nature, either grounded in tangible, real-world settings or constructed within virtual or simulated domains.

\textit{Physical vs. Virtual}: The physical environment refers to the actual, physical world where objects, people, and places exist tangibly. 
Examples include homes, offices, streets, and natural settings.
While, the virtual environment refers to the online or digital world, which exists in cyberspace and is accessed through computers, smartphones, or other digital devices.
Examples include social media platforms, online forums, and video games.

\textit{Real vs. Simulated}:
The real environment refers to the world in which humans live and is subject to real-world laws and dynamics.
The simulated environment refers to a virtual or artificially constructed environment that mimics the dynamics of the real world or represents hypothetical scenarios.

By combining the two dimensions, four distinct quadrants are formed to help differentiate the variety of environments agents can inhabit: 
1) \textit{Real-physical} environment represents the world where humans live and interact with tangible objects.
For example, a real kitchen or a physical office where agents~(robots) perform tasks with real-world consequences~\cite{ren2023robotsaskhelpuncertainty}.
2) \textit{Simulated-physical} environment mimics artificially real-world dynamics but is not part of the tangible world.
For instance, a simulated map or virtual town layout is designed to replicate physical environments for testing or exploration purposes~\cite{Cui_2024_WACV}.
3) \textit{Real-virtual} environment is real in the sense that it reflects actual content or social contexts, but it exists in the virtual or digital realm, such as Facebook~\cite{noauthor_meta_nodate}. 
4) \textit{Simulated-virtual} environment is designed to mimic the virtual world accessed by real humans.  
For example, a virtual social media platform is constructed for simulating propagation~\cite{10.1145/3526113.3545616}.

The classification helps us understand how different types of environments are structured and define the necessary components for building effective and relevant environments.
The components of an environment encompass its fundamental structure and governing elements that shape how agents behave and interact within it:
\begin{itemize}
    \item \includegraphics[width=0.05\textwidth]{icon/description.pdf}\textbf{Description}: The description of the environment outlines its key characteristics and defines the scope of the simulation or system.
    It provides a conceptual or formal representation of the environment's purpose, scale, and structure~\cite{park2023choicematessupportingunfamiliaronline,10.1145/3613904.3642159}.
    \item \includegraphics[width=0.05\textwidth]{icon/object.pdf}\textbf{Object}: Objects refer to the elements present within the environment with which agents can interact.
    These can include both tangible and intangible elements depending on whether the environment is physical or not.
    For example, objects may include desks or tables in a physical environment~\cite{ahn2022icanisay}.
    In a virtual environment, objects may include digital assets or virtual entities~\cite{wang2023voyageropenendedembodiedagent}.
    \item \includegraphics[width=0.05\textwidth]{icon/rule.pdf}\textbf{Rule}: Rules are the foundational guidelines that dictate how agents can interact with the environment and each other. 
    They serve as the internal logic of the system, determining the possible actions agents can take and the consequences of those actions~\cite{hua2024warpeacewaragentlarge}.
    These rules often emulate real-world dynamics~(\eg gravity, economics~\cite{LENGNICK2013102}).
    Moreover, they can include limitations or incentives for specific agent behaviors, such as penalties for violating certain rules or rewards for achieving objectives~\cite{10.1145/3526113.3545616,basavatia2023complexworld}.
\end{itemize}

These three components may not all be immediately visible to agents but serve as the underlying framework of the environment.
They determine its foundational regulations, influencing how agents behave and interact at a deeper, systemic level.

\subsubsection{Simulation Configuration}
The simulation brings agents and the environment together to represent and analyze complex systems.
It tracks agents' actions, the environment's evolution, and overall system dynamics over time.
We summarize three main components of the simulation configuration:
\begin{itemize}
    \item \includegraphics[width=0.05\textwidth]{icon/condition.pdf}\textbf{Condition}: The running setup and parameters that define the simulation's model running state, such as the simulation's start and end time and simulation interval for discrete models.
    \item \includegraphics[width=0.05\textwidth]{icon/progress.pdf}\textbf{Progress}: It tracks the temporal evolution of the simulation~\cite{chen2023agentversefacilitatingmultiagentcollaboration}.
    Agents and the environment evolve over time, and monitoring these transitions is crucial to understand the dynamics of the simulation~\cite{pan2024agentcoordvisuallyexploringcoordination}.
    \item \includegraphics[width=0.05\textwidth]{icon/technique.pdf}\textbf{Technique}: It refers to the computational methods and algorithms used to run the simulation.
    For example, depending on the complexity of the model, techniques such as rule-based algorithms~\cite{NEURIPS2021_86e8f7ab}, machine learning~\cite{https://doi.org/10.1111/exsy.13325}, reinforcement learning~\cite{vinyals_grandmaster_2019}, or LLMs~\cite{10.1145/3586183.3606763} may be employed to generate agent behaviors or environmental changes.
\end{itemize}

In summary, agents, environment, and simulation configuration form the three essential elements of ABMS. 
Agents act as autonomous entities within a defined environment, and their interactions and decisions are modeled through the simulation configuration, providing insights into complex systems. 

\subsection{Who: Roles of Human}
Shakespeare said, \textit{``The world is a stage and all the men and women, however, some performers, they all have off time, that the time has game.''}
We find that the roles that users play in interactions can be effectively explained through an analogy from the field of theater.
In the context of ABMS, agents can be regarded as the ``actors'' in a theatrical production, since they have predefined roles that shape their behaviors in predefined scenarios.
Therefore, we classify user roles by drawing upon professions from the theater: scriptwriter, director, actor, prototype, and observer.
It is worth noting that while these roles share similarities with those in the theater, they are not entirely identical.

\includegraphics[width=0.05\textwidth]{icon/scriptwriter.pdf}
\textit{Scriptwriter.} 
The scriptwriter initializes the purpose and structure of the simulation~\cite{10.1145/3526113.3545616,chan2023chatevalbetterllmbasedevaluators}. 
In this role, users are responsible for defining the agents and environments~\cite{lin2023agentsimsopensourcesandboxlarge}, essentially laying the foundation upon which the simulation will run.
They establish the objectives, constraints, and initial conditions to guide the simulation's progression~\cite{jinxin2023cgmiconfigurablegeneralmultiagent}.

\includegraphics[width=0.05\textwidth]{icon/director.pdf}
\textit{Director.} 
As the director, the user controls the timeline and conditions for the simulation, guiding the agents and adjusting parameters during the simulation process. 
Users can direct agents, instructing them to start, pause, or restart the simulation~\cite{chen2023agentversefacilitatingmultiagentcollaboration,ren2023robotsaskhelpuncertainty}.
Users can also offer guidance to agents in a manner akin to a director instructing actors in a performance~\cite{mehta2024improvinggroundedlanguageunderstanding,unknown,park2023choicematessupportingunfamiliaronline}.
In most cases, this role emphasizes managing the flow and direction of the simulation once it is set in motion.

\includegraphics[width=0.05\textwidth]{icon/actor.pdf}
\textit{Actor.}
The actor role represents the user interacting with the simulation as an agent, shifting from passive observation to active engagement.
Users live with other agents as if they were one of them, influencing outcomes by participating in the simulation~\cite{mao2024alympicsllmagentsmeet,zhou2024sotopiainteractiveevaluationsocial, NEURIPS2021_86e8f7ab}.
They interact with other agents or manipulate elements of the environment~\cite{10.1145/3586183.3606763,10.1145/3579598}, which can alter the course of the simulation or help achieve specific goals.
Specifically, other agents also perceive them as agents, other than humans.

\includegraphics[width=0.05\textwidth]{icon/prototype.pdf}
\textit{Prototype.} 
In a theatrical context, some roles are often based on real individuals as prototypes.
Similarly, users can serve as prototypes or references for the agents within the simulation~\cite{Argyle_Busby_Fulda_Gubler_Rytting_Wingate_2023, pmlr-v202-aher23a}.
They provide a basis upon which agents' characteristics can be built.
Unlike the actor, the prototype does not directly participate in the simulation, but influences how agents are designed or programmed.


\includegraphics[width=0.05\textwidth]{icon/observer.pdf}
\textit{Observer.}
The observer takes a passive yet crucial role by monitoring the simulation in real-time, gathering data and insights for further analysis~\cite{NEURIPS2023_a3621ee9,10.1145/3544548.3580688}.
Users watch the simulation unfold without intervening in the process as the audience in a theater.
They further analyze and interpret the behaviors of agents within the simulation, seeking to understand the underlying patterns, trends, or outcomes~\cite{10520238,electronics12122722,hua2024warpeacewaragentlarge}.

The environment serves as the stage where all actions occur and emergent behaviors are like the unscripted moments in a live performance. 
These five roles represent different types of user involvement with ABMS. 
Each role has a unique contribution, from defining and designing the simulation’s framework to actively participating in or passively observing its outcomes, which illustrates the flexibility and depth of user involvement in interactive simulations.


\subsection{How: Means of Interaction}
Various means of interaction allow users to engage with ABMS and ensure that users can effectively exert influence over the simulation.
The primary interaction means are as follows:


\includegraphics[width=0.05\textwidth]{icon/interface.pdf}
% \includegraphics[width=0.025\textwidth]{icon/interface.pdf}
\textit{Interface.}
The user interface~(UI) provides users access to manage the simulation.
Through buttons~\cite{chen2023agentversefacilitatingmultiagentcollaboration} and control panels~\cite{kovač2023socialaischoolinsightsdevelopmental}, users can customize various aspects of the simulation.
Graphical design~\cite{lin2023agentsimsopensourcesandboxlarge} and visualizations~\cite{pan2024agentcoordvisuallyexploringcoordination}, such as charts~\cite{10520238} and real-time agent movements~\cite{10.1145/3613904.3642159} within the environment, enable users to track agent interactions, observe emergent behaviors, and analyze the outcomes of different scenarios.
The interface often provides real-time feedback~\cite{chan2023chatevalbetterllmbasedevaluators} based on user inputs, displaying how changes in parameters affect agent behaviors and simulation outcomes.
Furthermore, it provides users with an intuitive and interactive way to control and analyze simulations, facilitating deeper engagement with the simulation and enhancing the user’s ability to draw meaningful insights.

\includegraphics[width=0.05\textwidth]{icon/language.pdf}
\textit{Natural Language.}
Advances in AI and natural language processing~(NLP)~\cite{bommasani2022opportunities,brown_language_2020}, such as LLMs, enable users to give commands or ask questions in everyday language.
Users are allowed to use natural language commands to control the simulation settings, such as defining agents and environments~\cite{wang2023humanoidagentsplatformsimulating,10.1145/3526113.3545616}.
What's more, users can communicate with agents directly to guide them~\cite{shridhar2020alfredbenchmarkinterpretinggrounded} with high-level goals and low-level instructions or interview them for ``innermost thoughts''~\cite{10.1145/3586183.3606763}.

\includegraphics[width=0.05\textwidth]{icon/configuration.pdf}
\textit{Configuration Setting.}
We categorize methods that involve direct interaction with algorithms as configuration settings, which typically require users to have a programming background.
Configuration files~(like YAML, JSON XML) as user inputs are often used to configure simulation parameters, define agent properties, and set environmental conditions~\cite{wang2024userbehaviorsimulationlarge,hua2024warpeacewaragentlarge}.
Unlike natural language, which is flexible and often ambiguous, the structured text file follows a specific syntax and format. 
It is organized in a hierarchical or key-value structure that can be easily read and interpreted by machines.
Additionally, several libraries and APIs can be applied to construct ABMS~\cite{li2023modelscopeagentbuildingcustomizableagent}.

\includegraphics[width=0.05\textwidth]{icon/data.pdf}
\textit{Data Integration.}
Users can interact with ABMS with external datasets.
For example, agents can utilize users' profile data, such as demographic information, to replicate human samples for enhancing the overall realism of the simulation~\cite{Argyle_Busby_Fulda_Gubler_Rytting_Wingate_2023,gao2023s3socialnetworksimulationlarge}.
On the other hand, users gain simulation data for further analysis~\cite{10.1145/3526113.3545616}.
This data can then be analyzed to extract insights and identify patterns, allowing for informed decision-making or the refinement of the model.


\includegraphics[width=0.05\textwidth]{icon/physical.pdf}
\textit{Physical Movement.}
In certain simulations, especially those involving robotics or virtual reality/augmented reality~(VR/AR), physical movement can be a means of interaction.
Users physically interact with objects or agents in the real world, which in turn affects the simulation ~\cite{mandi2023rocodialecticmultirobotcollaboration,10.1145/3613904.3642183}.
This direct physical contact allows for real-time, hands-on control and interaction with the simulated environment.
On the other hand, in the virtual environment, users can interact with agents and the surroundings through body gestures and facial expressions~\cite{10.1145/3613905.3637145,10.1145/3613904.3642947}.

