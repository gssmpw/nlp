

\section{Methods for M3DA Benchmark}
\label{sec:methods}


% \input{images/uDA_pipeline}
\begin{figure}[!ht]
    \centering
    \includegraphics[width=1\columnwidth]{images/uDA_pipeline.pdf}
    \caption{Overview of the UDA pipeline for semantic segmentation. Some methods does not require Target Domain images during training, e.g., nnAugm, IN, AdaBN.}
    \label{fig:pipeline}
\end{figure}


\subsection{Segmentation baseline and oracle}
\label{ssec:baseline}


% We start with a formal definition of the semantic segmentation task in M3DA.
Let $F$ be a segmentation network which takes an image $x \in \R^{H \times W \times D}$ and predicts a soft-segmentation map $p = F(x)$, $p \in \R^{H \times W \times D \times (C + 1)}$. Here, the last layer of $F$ is softmax which outputs a $(C+1)$-dimensional voxel-wise vector $\left[p^{(h, w, d, c)}\right]_c$ behaving as a discrete distribution over classes. The parameters $\theta_F$ of $F$ are learned to minimize some segmentation loss $\mathcal{L}_{seg} (p, y)$. In our case, $\mathcal{L}_{seg}$ is a sum of cross-entropy and Dice losses, as used by default in \cite{nnunet} and many other works. Optimization problem for training on source domain reads: % $\displaystyle \min_{\theta_F} \frac{1}{|X^s|} \sum_{(x, y) \in (X^s, Y^s)} \mathcal{L}_{seg} (F(x), y)$.


\begin{equation}
    \min_{\theta_F} \frac{1}{|X^s|} \sum_{(x, y) \in (X^s, Y^s)} \mathcal{L}_{seg} (F(x), y).    
\end{equation}


Further, every DA method depends on $F$, i.e., has the same backbone, so the choice of $F$ is crucial for the benchmark construction. We used the nnU-Net \cite{nnunet} architecture, loss function, and training pipeline, since nnU-Net demonstrated the best performance in several relevant tasks \cite{nnunet,amos,isensee2024nnu}, including AMOS and BraTS. We also compared nnU-Net to its closest alternatives, UNETR \cite{unetr}, Swin UNETR \cite{swinunetr}, and MedNeXt \cite{mednext}, directly within our benchmark tasks and confirmed superior nnU-Net performance; see Table~\ref{tab:backbones}.% Supplementary materials for the backbone comparison.


% \input{tables/4_backbones}

% (as DA methods)
To conduct an ablation study of normalization techniques and nnU-Net pipeline components, we replaced the default instance with batch normalization layers. We also removed modality-specific preprocessing, postprocessing, and test-time augmentations, so we could assess the unhindered impact of DA methods. A detailed technical description is given in Supplementary materials.


An nnU-Net pipeline with the changes above is a backbone for all further experiments; we call it simply U-Net. Finally, we define two core methods of the M3DA benchmark. \textbf{Baseline} -- U-Net trained on $(X^s, Y^s)$ and tested on $(X^t_{ts}, Y^t_{ts})$, i.e., naive transfer. \textbf{Oracle} -- U-Net trained and tested via cross-validation on $(X^t_{ts}, Y^t_{ts})$ that might be interpreted as an upper bound estimation for DA methods.
%, i.e., upper bound for DA methods.


Given baseline and oracle scores, the goal of DA methods therefore is to close the gap between them.% DA methods are based on the same 


\subsection{UDA methods}
\label{ssec:uda}


In our methods selection, we mainly follow reviews of DA for medical image analysis \cite{medim_da_survey_2021,medim_da_survey_2023}. We include core methods of DA for open-world images, following the corresponding reviews \cite{da_survey_2018,uda_survey_2020,uda_segm_review_2020}, top-performing solutions to the CrossMoDA challenge \cite{crossmoda}, and most recent Domain Generalization methods \cite{dg_tta}, totaling 12 methods: Adaptive Batch Normalization (AdaBN) \cite{adabn}, Instance Normalization (IN) \cite{instance_norm}, Self-ensembling (SE) \cite{se,se_medim}, Minimizing entropy (MinEnt) \cite{entropy}, Domain Adversarial Neural Network (DANN) \cite{dann,dann_medim}, CycleGAN 2D \cite{cyclegan}, CycleGAN 3D \cite{cyclegan3d}, Histogram matching (HM), nnU-Net augmentations (nnAugm) \cite{nnunet}, Gamma correction augmentation (Gamma), Global intensity non-linear (GIN) augmentation \cite{gin}, and Modality independent neighborhood descriptor (MIND) augmentation \cite{dg_tta}. Finally, we include pretrained backbones from foundational models for 3D medical imaging CLIP-driven universal model (UniModel) \cite{unimodel} and SAM-Med3D \cite{sammed}. A complete description of the methods selection is provided in Supplementary materials.

To ensure fair comparison, we maintained consistent training protocols across all methods, using U-Net backbone from nnU-Net framework for most of the experiments. Most of the DA methods require only minor architectural changes or no change at all, with only exception being DANN. Implementation details are given in Supplementary materials.
