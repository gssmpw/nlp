

\section{Introduction}
\label{sec:intro}


% \todo{
% \begin{enumerate}
%     \item \sout{\textbf{Anvar} Sec 3.2 to Supplementary Edit intro paragraphs.}
%     \item \textbf{Anvar} Table with std to main text
   
%     \item  \textbf{Anvar} shortening 5.1-5.2, partially moving to supp?

%     \item \textbf{Anvar, Boris} Edit text according to new results
%     \item \sout{\textbf{Anvar} somewhere add info about diversity of selected dataset sources (multi-center datasets)}
    
%     \item \textbf{Anvar} Totalsegmentator discussion (why we reject it)
%     \item \textbf{Anvar} re-write discussion, it should starts differently, add DG as a potential framework for our benchmark. Add a sentence in the intro (something like the following) "While we focus on UDA, our benchmark could also be tested in other domain adaptation frameworks, such as supervised DA, source-free DA, test-time DA, and domain generalization (DG)"
%     \item \textbf{Anvar} consider adding alternative datasets to Supplementary
%     \item \textbf{Anvar} add a section in Supp about "foundational models for medical imaging", see intro in Sec 3.2
%     %%%%%%%%%%
%      % \item \sout{\textbf{Anvar} Table 8 based figure (method+nnAugm results)}
%     % \item \textbf{Valentin} Add VLM (take pretrained weights) results to table Ablation. Need to select methods 2-3. Rows "Baseline" and "Oracle". See, \url{https://aumi-ai.slack.com/archives/C041H93GK9U/p1729847622051319} for details.
%     % \item \sout{\textbf{Anvar} plot a pipeline scheme to supplement Sec 3.1}+rewrite Sec 3.1

%     % \item  \sout{\textbf{Anvar} Sec 4. Subdivide into concrete experiments with results discussion.}
%     % \item  \sout{\textbf{Anvar} fix wording in 3.1, "Unet(ours), Oracle, etc. it is all a bit confusing"}
%     % \item \sout{\textbf{Anvar} replace HM reference in Table 5} keep without a reference.
%     \end{enumerate}
% }


\begin{figure}
    \includegraphics[width=\linewidth]{images/fig1_teaser.png}
    % \includegraphics[width=\linewidth]{images/fig1_teaser.pdf}
    \caption{Using the best DA method in the M3DA benchmark closes only 62\% of the performance gap between domains on average. Here, \% indicates the gap closed between the baseline level and oracle (outer) circle.}
    \label{fig:teaser1}
\end{figure}


\begin{figure*}[ht]
    \centering
    % \includegraphics[width=1\linewidth]{images/fig2_bench_contours.png}
    \includegraphics[width=1\linewidth]{images/fig2_bench_examples.png}
    % \includegraphics[width=1\linewidth]{images/fig2_bench_examples.pdf}
    \caption{Examples from individual domains in M3DA without segmentation masks for visual comparison between domains. Left to right, top to bottom: CT to MR, CT to LDCT, CT CE to CT native, CE T1 to T1, T1 Field (1.5T to 3T), T1 Scanner (Philips to Siemens). We provide segmentation masks visualization for the same examples in Supplementary materials.}
    \label{fig:teaser2}
\end{figure*}


Deep Learning (DL) methods have significantly advanced medical image analysis, achieving near-human-level performance in tasks like image classification, segmentation, pathology detection, and automated diagnosis \cite{medim_dl_survey_2017}. However, the widespread adoption of DL in medical imaging is hindered by the poor performance of neural networks on data from distributions different from their training set. This challenge, known as \textbf{domain shift}, is particularly prevalent in medical imaging due to changes in scanner acquisition parameters, the introduction of new imaging modalities, and population differences. Several studies have concluded that medical imaging is a crucial domain for the adoption of Domain Adaptation (DA) methods \cite{gulrajani2020search,uda_survey_2020,zhuang2020comprehensive,peng2018visda,zhang2021empirical}. Despite this, there is a lack of a common benchmark for testing DL methods in the field of 3D medical imaging.


% TODO: move fig 1 here, after finishing todo-list


Recent works have focused on developing DA methods, revealing that 3D medical image segmentation algorithms are particularly susceptible to domain shift. Researchers have tested their methods against various sources of domain shift, including shifts between imaging modalities, the most common being MRI and CT \cite{jiang2020unified,yu2023source,zheng2021hierarchical}. Other sources of domain shift include scanner manufacturers or settings, such as the strength of MR field or CT dosage \cite{zheng2021hierarchical,liu2020shape,chen2022maxstyle,gu2021domain,lennartz2023segmentation,se_medim}, and intra-modality shifts, such as T2 to T1 MRI \cite{han2021deep,crossmoda,dann_medim}.





A systematic comparison of these methods, along with the question of the necessity to develop new ones, is complicated by lack of consistency in the usage of datasets across studies, even when addressing similar domain shifts, as shown in Table \ref{tab:benchmarks}. In addition, many studies test their proposals on a single domain shift problem, limiting the generalizability of their analysis. 


Recently, the Cross-Modality Domain Adaptation (CrossMoDA) challenge \cite{crossmoda}, conducted at MICCAI in 2021 and repeated in 2022 and 2023, attempted to unify various authors under the same framework. The challenge's setup involved training models on annotated MR T1c with access to unannotated T2 studies, followed by testing on T2 studies. The challenge attracted numerous participants, with the top teams achieving near-supervised performance levels using image-to-image translation techniques paired with multi-stage pseudo-labeling.


Despite its success, the CrossMoDA challenge has certain limitations. Firstly, it only considers a single source of domain shift, the MR sequence. Secondly, the segmentation task is confined to a two-class challenge: segmenting the vestibular schwannoma tumor and the cochlea, both located in very specific anatomical regions. Consequently, top-performing solutions managed to close up to a 97\% score gap between domains. Participants employed various task-specific techniques, such as using only the largest connected component of the segmentation mask to enhance segmentation quality. While this approach may be effective for solving a concrete segmentation task, it is counterproductive for assessing the capabilities of DA methods. It obscures the true impact of adaptation to the unseen domain, hindering a clear understanding of the effectiveness of DA techniques.


\textit{Therefore, we conclude that there is a need for a large, diverse, and publicly available benchmark for DA in 3D medical image segmentation that includes a variety of downstream tasks.} We introduce such a benchmark to encourage further progress in developing scalable DA methods.


We include four publicly available datasets, encompassing 22 segmentation tasks. Based on these datasets, we construct eight domain adaptation problems; see visual examples of individual domains in Figure~\ref{fig:teaser2}. Table~\ref{tab:setup} summarizes all proposed problems with their domain shifts and dataset splits.


To establish a baseline and determine which problems remain unsolved with current methods, we implemented core unsupervised DA (UDA) methods for 3D medical image segmentation. Excluding any task-specific assumptions (e.g., filtering the largest connected component) and human-in-the-loop approaches, the best method within our benchmark closes only 62\% of the performance drop between domains on average. Thus, we highlight the need for further development of robust DA methods in 3D medical image segmentation, with our proposed benchmark serving as a strong foundational point for systematically comparing novel methods. We summarize our contributions as follows:


\begin{itemize}

\item We propose a benchmark for DA in 3D medical image segmentation that includes eight carefully selected domain shifts based on their practical relevance. These shifts cover variations in imaging modalities, scanner settings, and the presence of contrast agents, ensuring that our benchmark reflects real-world challenges in medical image analysis. % (MRI to CT)

\item We provide a comprehensive evaluation of more than ten core domain adaptation methods on our benchmark, covering key categories of UDA approaches. 

\item Our benchmark is designed to be economical, utilizing only four publicly available datasets. This allows for testing new methods against a wide variety of problems with minimal resources, making our benchmark accessible to researchers and encouraging wider adoption.
    
\end{itemize}