

\section{Methods details}


\subsection{UDA methods selection}


\paragraph{Discrepancy-based approaches} are based on incorporating maximum mean discrepancy measure as a regularization or auxilary loss function \cite{mmd_ghifary2014domain,mmd_tzeng2014deep,mmd_long2015learning}. These approaches were soon surpassed by simpler approximations, such as \text{DeepCORAL}~\cite{deepcoral}. However, all of them become computationally intractable due to significantly larger feature space in 3D segmentation task.% We thus selected more practical approaches to discrepancy minimization.
% (dim 1e6 instead 1e3 for feature space)

Since batch normalization (BN) \cite{bn} became the standard in DL, it allowed to reduce covariate shift by aligning first and second moments of feature distributions. But it introduced discrepancy between train and test by applying train-estimated statistics to the test samples. Here, \textit{Adaptive BN (AdaBN)} \cite{adabn} recalculates BN statistics on the unlabeled target data, helping to adapt to the target domain.

Contrary, \textit{Instance Normalization (IN)} \cite{instance_norm} was proposed for an efficient image stylization, and it calculates statistics for every input independently. This way IN might help adaptation, so we included IN to test it separately.


\textbf{Selected methods:} AdaBN, IN.


\paragraph{Self-training} uses predicted pseudo-labels on the target data to regularize the downstream model. For instance, the authors of \cite{se} proposed \textit{self-ensembling (SE)} for visual DA. The same methodology was implemented for 3D medical image segmentation in \cite{se_medim}. The authors trained the first, student, network on the downstream task and updated the weights of the second, teacher, network via exponential moving average. They additionally imposed a consistency criterion: mean squared error between predictions of the two networks, thus, student network minimizes segmentation and consistency losses. We included SE with hyperparameters recommended in \cite{se_medim}.

Specifically for semantic segmentation, training on self-generated predictions was shown to help in DA \cite{self_training}. Later, the authors of \cite{entropy} noted the connection between self-training and entropy minimization. They also showed that \textit{minimizing the entropy (MinEnt)} of predictions surpasses self-training and other DA methods, so we included MinEnt in our benchmark.

\textbf{Selected methods:} SE, MinEnt.


% (or maximizing the loss)
\paragraph{Adversarial-based approaches} form the basis for the most DA methods, as shown in \cite{uda_segm_review_2020}. The central idea is reversing the gradient from the domain classification network, thus learning domain invariant features for source and target inputs. To this end, the authors of \cite{dann} proposed \textit{Domain Adversarial Neural Network (DANN)} for image classification, noting that their approach is generic and can handle any output label space. Consequently, DANN was implemented for DA in 3D medical image segmentation \cite{dann_medim}.

Although many other DANN modifications exist, e.g., decoupling feature encoders for source and target images \cite{adda} or connecting the domain classification network to the output layer \cite{tsai2018learning}, adapting them for 3D segmentation requires a separate effort. Hence, we focused on testing the core method and proceeded with the close to original DANN implementation of \cite{dann_medim}.

\textbf{Selected methods:} DANN.


\paragraph{Image-level adaptation} is typically achieved using Generative Adversarial Network (GAN) \cite{goodfellow2020generative}. The goal is to learn a mapping function between the source and target domains with a generator network. Then, one can use this generator to transfer images styles between domains. Specifically for UDA, the authors of \cite{cyclegan} proposed \textit{CycleGAN 2D} which additionally enforces the reconstruction loop consistency upon two generators. This method was also designed for 3D medical images in \cite{cyclegan3d}; and it found numerous successful applications to medical image segmentation, e.g., top-3 solutions of the CrossMoDA challenge \cite{crossmoda} used \textit{CycleGAN 3D}. We included both approaches.

Image-level adaptation also includes non-generative approaches, such as Fourier Domain Adaptation (FDA) \cite{fda}, where the style of images is changed by substituting their low frequencies in Fourier space. The authors of \cite{fda_medim} succeeded in applying FDA to 3D medical image segmentation. However, such methods, similar to CT reconstruction kernel modulation \cite{fbpaug}, are not generic and heavily depend on modality-specific features, so we excluded them from further consideration.

\textbf{Selected methods:} CycleGAN 2D, CycleGAN 3D.


\paragraph{Preprocessing and augmentation} are often overlooked when considering DA. On the one hand, we can standardize data characteristics by preprocessing, potentially reducing domain shift. We included two such steps by default: resampling to common spacing and intensity normalization; they are essential for the adequate model training \cite{kondrateva2024negligible}. Many studies demonstrated domain shift in medical images by intensity histograms \cite{crossmoda,se_medim,ihf}. Equalizing this difference might be of interest for adaptation, thus we included \textit{histogram matching (HM)}.

On the other hand, augmentations can expand source distribution, potentially covering the target one. Here, nnUnet framework \cite{nnunet} includes a variety of universal augmentations, so we tested them as a separate method under the name \textit{nnAugm}. We also tested a commonly used and modality-agnostic \textit{gamma correction augmentation (Gamma)} as an ablation study of nnAugm.
% as in \cite{gamma_example}

Finally, several advanced augmentation techniques were developed for domain generalization purposes. We included the most recent of them, \textit{global intensity non-linear (GIN)} \cite{gin} and \textit{modality independent neighborhood descriptor (MIND)} \cite{dg_tta} augmentations.

\textbf{Selected methods:} HM, nnAugm, Gamma, GIN, MIND.


\subsection{Implementation details}

\input{tables/suppl/4_hparams}

We used an nnU-Net \cite{nnunet} backbone as segmentation network architecture in all methods. We preserved most of the nnU-Net training pipeline except for several methodological changes, which allow us to evaluate DA methods, such as AdaBN and InstanceNorm, separately and run the ablation studies. These changes along with the other training hyper-parameters are summarized in Table~\ref{tab:hyper}.

Firstly, we replaced the default InstanceNorm with BatchNorm layers and removed test-time augmentation, so we can compare different normalizations and adaptive normalizations (AdaBN) and assess the unhindered impact of DA methods. Secondly, we reduced the patch size and number of the network features, so all experiments fit in a single 16 GB NVIDIA Tesla V100 and our benchmark remains economical. We set the number of epochs to 600 in all experiments, so that any method could complete its training in three days.

Below, we provide DA methods implementation details:


% \noindent
\textbf{Histogram matching} uses the baseline training pipeline, except all image intensity histograms are equalized to an average histogram computed over the train set. 


% \noindent
\textbf{Gamma augmentation} also uses the baseline training pipeline, and we perform gamma correction with randomly selected $\gamma \sim U[0.5, 2]$ on every input image.


% \noindent
\textbf{nnAugm} similarly supplements the same baseline training with the original set of nnU-Net \cite{nnunet} augmentations.


% \noindent
\textbf{InstanceNorm} substitutes BN layers, while the training pipeline remains the same as in baseline.


% \noindent
\textbf{Adaptive BN} performs additional 1000 inference steps with batch size 4 over the baseline, updating the running statistics of BN layers on target training data.


% \noindent
\textbf{Self-ensembling} design and all parameters are reproduced from \cite{se_medim} with our architecture.


% \noindent
\textbf{MinEnt} adds a predictions entropy minimization criterion on target images. So we extended our training pipeline with the second step using target train images, and added entropy loss with the recommended in \cite{entropy} weight $\lambda = 0.001$.


% \noindent
\textbf{DANN} introduces an auxiliary network called discriminator. Similar to recent studies \cite{entropy}, we used DCGAN \cite{dcgan} discriminator architecture, replacing 2D convolutions with 3D ones. The losses weighting parameter is taken from \cite{dann_medim}, e.g., $\alpha = 0.01$.


% \noindent
\textbf{CycleGAN 2D} is fully reused from the original study \cite{cyclegan}. We trained a standalone CycleGAN 2D to map between source and target train images, where we sample axial slices from our volumetric images and rescale them into 256 $\times$ 256 gray scale images. Before predicting with the baseline segmentation model, we applied one of the generators to target test images (slice-by-slice) to transform them into fake source ones.


% \noindent
\textbf{CycleGAN 3D} is fully reused from the original study \cite{cyclegan3d}. We trained a standalone CycleGAN 3D to map between source and target train images, where we sample patches of size (128, 128, 96) from our volumetric images. Before predicting with the baseline segmentation model, we applied one of the generators to target test images (via overlapping grid) to transform them into fake source ones.


% \noindent
\textbf{GIN} is fully reused from \cite{gin} with the implementation based on the nnU-Net framework.


% \noindent
\textbf{MIND} is fully reused from \cite{dg_tta} with the implementation based on the nnUNet framework.


All experiments are available and could be reproduced from \href{https://github.com/BorisShirokikh/M3DA-exp}{https://github.com/BorisShirokikh/M3DA-exp}.

\begin{figure}
    \includegraphics[width=\linewidth]{images/cvpr_m3da_methods_timeline.png}
    \caption{Average performance of domain adaptation approaches on M3DA benchmark; see Table \ref{tab:ablation_aug} for detailed results.}  % objective progress being very little over time
    \label{fig:teaser3}
\end{figure}
