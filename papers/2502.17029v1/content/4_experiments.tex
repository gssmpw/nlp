

\section{Experiments}
\label{sec:exp}


\subsection{Backbone selection} We evaluated four segmentation backbones (U-Net, UNETR, SwinUNETR, and MedNeXt) to determine an optimal baseline architecture for our experiments; see Table~\ref{tab:backbones}. For comparison, we included two pretrained models: SAM-Med3D and UniModel, which are based on UNETR and SwinUNETR architectures, respectively, and used officially published model weights.

\input{tables/4_backbones}

Our analysis of the foundational models (SAM-Med3D and UniModel) finetuned in supervised fashion yielded three key observations. First, both models demonstrated improved performance compared to their respective base architectures (UNETR and SwinUNETR). Second, we observed that both models slightly underperformed in 4 out of 5 MRI tasks compared to their counterparts trained from scratch, potentially due to their pretraining being predominantly conducted on CT data. Third, neither of them surpassed the performance of a standard U-Net trained from scratch.

MedNeXt performed on par with regular U-Net, however failed to converge on LIDC dataset on multiple runs attempts. % for unknown reason.
Based on these results, we selected the U-Net architecture as our primary backbone.


\subsection{Domain Adaptation methods on M3DA} 

\input{tables/5_metrics_std}

\input{tables/6_ablation}

We evaluated various DA methods on the M3DA benchmark (Table~\ref{tab:metrics_pure}) using multi-class Dice score and the \textit{percentage of performance gap} closed between the Baseline and Oracle setups: $\displaystyle 100\times \frac{\text{Method}_{\text{Dice}} - \text{Baseline}_{\text{Dice}}}{\text{Oracle}_{\text{Dice}} - \text{Baseline}_{\text{Dice}}}$.

Our analysis begins with non-adapted networks (trained solely on source domain) evaluated on target domain images, represented by three baseline models: U-Net, UniModel (SwinUNETR), and SAM-Med3D (UNETR). Standard U-Net without adaptations failed completely on MR $\leftrightarrow$ CT tasks and showed poor performance on CT tasks (low-dose and CE), while maintaining moderate performance on MRI parameter shift tasks. In contrast, generalist models pretrained using contrastive and segment-anything approaches showed slightly inferior performance on MRI tasks but demonstrated remarkable results on CT-related tasks. Notably, UniModel achieved the best overall performance on the CE CT $\rightarrow$ CT task without adaptations, while SAM-Med3D exhibited strong performance on the CT $\rightarrow$ LDCT task. These results constitute, to our knowledge, the first empirical demonstration of zero-shot domain adaptation capabilities in foundational models for 3D medical imaging.

CycleGANs, despite their success in the CrossMoDA challenge, performed relatively poorly in our benchmark, particularly on CT-based tasks. We attribute this underperformance to the increased complexity of full-resolution CT images compared to brain MRI segmentation, including variations in size, localization regions, fine-grained details, and subtle stylistic differences.

Classical visual UDA methods (AdaBN, InstanceNorm, DANN, and Self-Ensembling) consistently outperformed the baseline, demonstrating their robustness across diverse domain shifts. 

Unexpectedly, generic augmentations (nnAugm) and even their subset, Gamma augmentation, outperformed more sophisticated methods on average. This finding strongly suggests the importance of incorporating generic augmentations into DA pipelines, which we explore in the following section.

Finally, recent Domain Generalization methods, GIN and MIND, achieved superior performance on MR $\leftrightarrow$ CT tasks, ranking first and second respectively, with relatively average results across other tasks. We note that these methods were originally developed and evaluated within the MR $\leftrightarrow$ CT setups, so increasing the diversity of a DA benchmark is useful for understanding the true method's capabilities.


\begin{figure}
    \centering
    \includegraphics[width=1\columnwidth]{images/fig4_augm.png}
    \caption{Comparison of DA methods with and without augmentations.}
    \label{fig:augm}
\end{figure}


%%%%%%%%%%%%%%
\subsection{Impact of additional augmentations}

Finally, we systematically evaluated the effect of incorporating nnU-Net augmentations, nnAugm, into each domain adaptation method. Our results demonstrate consistent improvements across most methods (Figure~\ref{fig:augm}) and datasets (Table~\ref{tab:ablation_aug}), with an average increase of 10.3 percentage points in Dice score. These benefits varied significantly across different domain shifts: CT-related tasks showed the most substantial improvements, while MRI-based tasks exhibited more modest gains.

Different methods also showed varying degrees of improvement when supplemented with augmentations. MinEnt demonstrated the most dramatic enhancement, +33.5 percentage points in average gap, making it the best-performing method overall (Figure~\ref{fig:teaser1}). CycleGAN-based approaches also benefited significantly, especially in CT-related tasks. Self-Ensembling, while being the strongest initial method, showed the least improvement from the additional augmentations, suggesting that it might already benefited from the incorporated augmentations by design \cite{se_medim}. Notably, some methods exhibited slight performance degradation on specific tasks, indicating that aggressive augmentation strategies may occasionally interfere with method-specific adaptation mechanisms.

Viewing these results from the alternative perspective, we can consider each method as an additional adaptation strategy applied on top of the strong nnAugm baseline. From this point of view, the marginal benefit of adding sophisticated DA methods to an already well-augmented model is more modest but still significant. The best-performing methods (SE and MinEnt) provide an additional $10$--$15\%$ improvement over nnAugm, demonstrating that DA techniques can capture domain-specific variations that generic augmentations alone cannot address. This suggests that while extensive augmentations should form the foundation of any domain adaptation pipeline, method-specific adaptation mechanisms provide complementary benefits that warrant their inclusion in the final solution.

To conclude, despite significant advances in deep learning over the past decade, our benchmark reveals a concerning trend (Figure~\ref{fig:teaser3}): DA methods for medical image segmentation have shown minimal improvement since 2017, with recent approaches performing comparably or even worse than earlier ones, suggesting that closing the domain gap remains a fundamental challenge that requires radically new approaches.
