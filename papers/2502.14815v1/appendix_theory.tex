\section{Proof of Theorem \ref{thm:deluxeagent:convergence}}
\begin{proof}
The proof consists of two parts. First, we show that at iteration $j$, allocation $\allocation_\query$ allocates the same models to the first $j$ modules as the optimal allocation for each task $\query$. Second, we can show that taking the mode over all tasks' allocations leads to the optimal allocation for the training dataset.    

We first note that the uniqueness of a task's optimal model allocation implies that for each module only one unique model maximizes the per-module quality. That is, for each $i$, there exists some $k$, such that for any $k'\not=k$, we have $\performance_i(\allocation_{i\rightarrow k} > \performance_i(\allocation_{i\rightarrow k'})$. Suppose not. Let $k^*$ be the model allocated to module $i$ by the optimal allocation. Due to the monotone assumption, $k^*$ should also maximize module $i$'s performance. Let $k'$ be another model that maximizes module $i$'s performance. By the inter-monotone assumption, switching from $k^*$ to $k'$ does not hurt any other module's performance. By the monotone assumption, $k'$ also maximizes the overall performance. A contradiction. Therefore, for each module, there is only one unique model that maximizes its performance, regardless of how other modules are allocated. 

Now we can show that at iteration $j$, allocation $\allocation_\query$ allocates the same models to the first $j$ modules as the optimal allocation. To see this, one can simply notice that the unique ``best'' model for each module must also be the optimal model for the end-to-end system. This is again because of the monotone assumption: otherwise, one can change the model in the optimal allocation to have better performance of one module and thus the overall system.  Therefore, allocating the per-module optimal model is the same as allocating the optimal model for the entire system. Thus, at iteration $j$, allocation $\allocation_\query$ allocates the same models to the first $j$ modules as the optimal allocation.

Now we study the second part. By the first part, after $L$ iterations, each $\allocation_\query$ has become the best allocation for task $\query$. Recall that we focus on binary performance, i.e., $\performance()\in \{0,1\}$.  Hence, if the model allocation is not one of $\allocation_\query$, its end-to-end performance is simply 0. Now, for any $\allocation_\query$, its performance on the training dataset is the average over its performance on each data point, i.e.,
\begin{equation*}
 \frac{1}{\|\Datasettrain\|}\sum_{\query' \in \Datasettrain }^{} \performance(\allocation_\query, \query')
\end{equation*}
Now recall that the optimal allocation for each query is unique. That is, $\performance(\allocation_\query, \query')$ is 1 if $\allocation_\query=\allocation_\query'$, and 0 otherwise. Hence, the training performance is proportional to 
\begin{equation*}
 \sum_{\query' \in \Datasettrain }^{} \mathbf{1}_{\allocation_\query=\allocation_\query'}
\end{equation*}
That is, the performance of allocation $\allocation_\query$ is proportional to the number of training data points whose optimal allocation is the same as $\allocation_\query$. Therefore, taking the mode of all optimal allocations is sufficient to obtain the best allocation for the training dataset.
\end{proof}