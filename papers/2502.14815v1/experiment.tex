
\begin{figure*}[ht!]
    \centering
    \includegraphics[width=0.99\linewidth]{figure/experiment/casestudy/casestudy.pdf}
    \caption{A case study on the TableArithmetic dataset. (a) Overall performance. Any single LLM has low performance on either Module 1 (e.g., Claude 3.5) or Module 2 (e.g., Gemini 1.5 Pro), but not both. \deluxesystem{} learns to use the best LLM for each module and thus achieves high performance on both modules and thus the whole system. (b) An example. Claude 3.5 fails to answer the extracted task correctly, while Gemini 1.5 cannot extract the correct task. \deluxesystem{} allocates them in different modules to obtain the correct answer 49. (c) Optimizer's effect. (c1) \deluxesystem{} reduces 60\% cost to reach the same accuracy as the exhaustive search. (c2) Greedy search's accuracy is surprisingly low because of the locally optimal solution. (c3) LLM diagnoser enables \deluxesystem{} to escape the local optimum. \eat{TODO: more realistic task like react and rag. multi-modal task. Extract text from pdf, and then solve it. r1, o1, for reasoning vs Gemini for long context. check out language program project. Judging task?}}
    \label{fig:deluxeagent:casestudy}
\end{figure*}

\section{Experiments}\label{sec:deluxeagent:Exp}
We compare the performance of \deluxesystem{} with vanilla compound AI systems using real-world LLM models in this section. Our goal is three-fold: (i) validating that allocating different models to different modules can substantially improve compound AI systems' performance, (ii) quantifying the performance gains enabled by \deluxesystem{}, and (iii) understanding how \deluxesystem{}'s LLM  diagnoser makes it possible to identify effective model allocations efficiently.


\paragraph{Experiment setups.} Throughout this paper, we use $K=10$ real-world LLMs, including GPT-4o, GPT-4o mini, GPT-4-Turbo, Claude 3.5 Sonnet, Claude 3.5 Haiku, Gemini 1.5 Pro, Gemini 1.5 Flash, Llama 3.1 405B, Llama 3.1 70B, and Qwen 2.5 72B. The temperature is 0.1 for all models. The maximum number of tokens is 1000 unless specified. By default, we use 50\% of each dataset for training and the other 50\% for evaluation. 

\subsection{A case study on TableArithmetic}

Let us start with a case study on TableArithmetic, a synthetic dataset consisting of 100 questions. Here, each question involves a table consisting of ``ID'' and ``task'' rows. The goal is to solve the task corresponding to a specific ID. The table in each question has a total of 200 entries, and
 the task in each entry is a simple arithmetic question ``What is $X+(10.9>10.11)$?'', where X is a random integer between 1 and 100. 

\paragraph{The locate-solve system.} To address TableArithmetic, we use the locate-solve system consisting of two modules. The first module, locate, extracts the task with the corresponding ID, and the second module, solve, takes the first module's output and then answers the extracted task. For this specific case study, we only use five models: GPT-4o, GPT-4o mini, Claude 3.5 Sonnet, Gemini 1.5 Pro, and Llama 3.1 405B.   

\paragraph{\deluxesystem{} Setup.} We use Gemini 1.5 Pro as the LLM diagnoser. For this case study, we set up $\gamma=0$, that is, we fully rely on the LLM diagnoser as the module-wise performance function for each module.  


 
\paragraph{Performance Analysis.} Figure \ref{fig:deluxeagent:casestudy} demonstrates how \deluxesystem{} performs on this task. We first note that allocating any fixed model to all modules leads to poor end-to-end performance, as shown in Figure \ref{fig:deluxeagent:casestudy} (a). This is because no model has high performance for all modules. Second, \deluxesystem{}'s accuracy is perfect. This is because (i) there exists some model with perform accuracy on each module, and (ii) \deluxesystem{} learns this efficiently. For example, Claude 3.5 is perfect on the first Module, and Gemini 1.5 Pro makes no mistake on the second module. 
\deluxesystem{} learns to leverage the best model for each module, and thus reaches the best performance. 
To further understand this, Figure \ref{fig:deluxeagent:casestudy}(b) gives a concrete example. The query asks the solution to the task with a specific ID. The locate module using Claude 3.5 correctly identifies the task ``What is $48+ (10.9>10.11)$?'', but the solve module using Claude 3.5 incorrectly suggests that 10.9 is less than 10.11 and thus gives a wrong answer. 
On the other hand, the locate module using Gemini 1.5 Pro extracts the wrong task, but it solves the task correctly. \deluxesystem{} learns to use Claude 3.5 for the first module and Gemini 1.5 Pro for the second module, and therefore correctly answers this query. 


 

\begin{table*}[!ht]
%\resizebox{\textwidth}{!}{
  \centering
  \scriptsize
  \caption{Performance of \deluxesystem{} and other approaches for optimizing compound AI systems. We focus on three compound systems and apply each of them to two tasks:  self-refine for LiveCodeBench and CommonGenHard, multi-agent-debate for SimpleQA and FEVER, and locate-solve for TableArithmetic and TableBias,. The performance gain is the improvement by  \deluxesystem{} against the best of allocating any fixed (same) model to all modules (with underlines). We also compare \deluxesystem{} with the MIPROv2 optimizer implemented in DSPy (using GPT-4o as the LLM). We set max\_bootstrapped\_demos=2, max\_labeled\_demos=2, and all other parameters as default for MIPROv2.  We also box the second-best result for each dataset. Overall, \deluxesystem{} achieves 5\%-70\% accuracy gains over allocating any fixed model to all modules. Interestingly, \deluxesystem{} also outperforms DSPy with MIPROv2 which specializes in prompt optimization for compound systems. This further suggests the importance of model selection for compound systems.}
  \begin{tabular}{|>{\raggedright\arraybackslash}p{2.5cm}||c|c|c|c|c|c|}
    \hline
    \multirow{4}{*}{\textbf{Method}} &
      \multicolumn{6}{c|}{\textbf{Compound AI System}} \\
    \cline{2-7}
    & \multicolumn{2}{c|}{\textbf{self-refine}} &
      \multicolumn{2}{c|}{\textbf{multi-agent-debate}} &
      \multicolumn{2}{c|}{\textbf{locate-solve}} \\
    \cline{2-7}
    & \multicolumn{6}{c|}{\textbf{Task}} \\
    \cline{2-7}
    & \textbf{LiveCodeBench} & \textbf{CommonGenHard} & \textbf{SimpleQA} & \textbf{FEVER} & \textbf{TableArith} & \textbf{TableBias} \\
    \hline
    \hline
    GPT-4o & 86\% & 44\% & 20\% & 64\% & 0\% & 0\% \\
    \hline
    GPT-4 Turbo & 81\% & 46\% & 16\% & 64\% & 4\% & 0\% \\
    \hline
    GPT-4o mini & 72\% & 8\% & 5\% & 62\% & 0\% & 0\% \\
    \hline
    Claude 3.5 Sonnet & \boxed{\underline{89\%}} & 56\% & 20\% & 60\% & 0\% & 0\% \\
    \hline
    Claude 3.5 Haiku & 43\% & 20\% & 8\% & 57\% & 0\% & \boxed{\underline{44\%}} \\
    \hline
    Gemini 1.5 Pro & 87\% & 41\% & 16\% & 59\% & \boxed{\underline{30\%}} & 0\% \\
    \hline
    Gemini 1.5 Flash & 80\% & 13\% & 5\% & 38\% & 8\% & 2\% \\
    \hline
    Llama 3.1 405B & 80\% & \boxed{\underline{75\%}} & \underline{21\%} & \underline{65\%} & 0\% & 0\% \\
    \hline
    Llama 3.1 70B & 59\% & 68\% & 12\% & 7\% & 0\% & 42\% \\
    \hline
    Qwen 2.5 72B & 81\% & 30\% & 5\% & 47\% & 0\% & 0\% \\
    \hline
    DSPy & 87\% & 71\% & \boxed{22\%} & \boxed{68\%} & 0\% & 0\% \\
    \hline
    \rowcolor[HTML]{E0E0E0} % Highlighting DELUXEAGENT row
    \deluxesystem{} & \textbf{95\%} & \textbf{84\%} & \textbf{28\%} & \textbf{70\%} & \textbf{100\%} & \textbf{100\%} \\
    \hline
    Gains & 6\% & 11\% & 7\% & 5\% & 70\% & 56\% \\
    \hline
  \end{tabular}
  \label{tab:deluxeagent:mainresult}
%}
\end{table*}


\paragraph{Optimizer analysis.} Next, we focus on understanding the search efficiency of \deluxesystem{}. In particular, we compare \deluxesystem{} with two baselines: random search and greedy search. Given an LLM API budget $B$, random search randomly chooses $B$ model allocations from all possible allocations, and then returns the one with the highest end-to-end performance. The greedy search iteratively chooses one module and allocates to it the model with the highest end-to-end performance. 

As shown in Figure \ref{fig:deluxeagent:casestudy}(c1), we have found that \deluxesystem{} consistently outperforms these baselines. In particular, while random search needs to explore all 25 model allocations to ensure the optimal is identified, \deluxesystem{} needs only to try 10 model allocations, resulting in 60\% cost reduction. Interestingly, the greedy search method has a very low accuracy, even given a large search budget. This is because end-to-end performance is not always sufficient to reflect module-wise performance. To see this, Figure \ref{fig:deluxeagent:casestudy}(c2) gives the training accuracy for each model allocation. We observe that allocating GPT-4o mini to both modules is a ``locally'' optimal solution: changing one module's model would not improve the performance. However, its performance is actually much lower than the optimal allocation (Claude 3.5 Sonnet for module 1 and Gemini 1.5 Pro for module 2). 

\deluxesystem{} escapes from the ``locally'' optimal allocation by the LLM diagnoser. Consider, for example, that \deluxesystem{} starts with the locally optimal allocation: using GPT-4o mini for both modules. Figure \ref{fig:deluxeagent:casestudy}(c3) shows the LLM diagnoser's judgment when evaluating module 1's performance. While switching the model to Claude 3.5 Sonnet does not improve the end-to-end performance, the diagnoser recognizes that Claude 3.5 Sonnet performs well for the first module, and thus enables \deluxesystem{} moves from the initial allocation to allocating Claude 3.5 Sonnet to module 1. %To sum up, identifying the module-wise performance is key to \deluxesystem{}'s performance.

\subsection{Quantitative Performance Improvement}

Next, we study the performance of \deluxesystem{} on practical compound AI systems. In particular, we focus on three compound AI systems, namely, locate-solve, self-refine~\cite{renze2024self}, and multi-agent-debate~\cite{multi_agent_debate_2024}. The architectures of these systems are shown in Figure \ref{fig:deluxeagent:systemdemo} in the appendix. We use six datasets: TableArithmetic and Table Bias for locate-solve, LiveCodeBench~\cite{jain2024livecodebench} and CGH~\cite{renze2024self} for self-refine, and SimpleQA~\cite{wei2024measuring} and FEVER~\cite{Thorne19FEVER2} for multi-agent-debate. We compare \deluxesystem{} with using any fixed model for all modules and  DSPy~\cite{khattab2024dspy}, an open-source library specialized For prompt optimization in compound systems. For DSPy, we use the optimizer MIPROv2, which searches for best prompts using Bayesian optimization. We use GPT-4o as the backbone LLM, and set max\_bootstrapped\_demos=2, max\_labeled\_demos=2, and all other parameters as default for MIPROv2. 
More details are given in the appendix.

Table \ref{tab:deluxeagent:mainresult} summarizes the quantitative results. First, we observe that no LLM is universally better than all other LLMs for all tasks. For example, Gemini-1.5 Pro performs the best on TableArthmetic and LiveCodeBench, but GPT-4o is the best for FEVER. %A small model sometimes also outperforms large models. For example, allocating Llama 3.1 70B to a self-refine system leads to more than 10\% accuracy improvements over that of allocating GPT-4o, Gemini 1.5 Pro, and Cluade 3.5 Sonnet alone on the GCH dataset. 
%This further suggests the importance of model selection in compound AI systems. 
Second, \deluxesystem{} offers 5\%-70\% performance gains compared to the best baselines. Interestingly, \deluxesystem{} also outperforms the DSPy library which extensively optimizes the prompt. For example, on the SimpleQA dataset, optimizing the prompts by DSPy leads to a 1\% accuracy gain, but \deluxesystem{}'s improvement is much more substantial (7\%). 
This is again because different models have their own strengths and weaknesses, and prompting alone is not adequate to turn an LLM's weakness into its strength. On the other hand, \deluxesystem{} searches for the LLM with the desired strength directly and thus offers more benefits.  

\subsection{Qualitative Understanding}






To further understand when and why  \deluxesystem{} outperforms allocating the same model to all modules, we dive into a few specific examples and compare how \deluxesystem{}'s generations differ from these by allocating the same LLM. In particular,  Figure \ref{fig:deluxeagent:examples_simpleqa} in the appendix gives one example from the SimpleQA dataset answered by the multi-agent-debate system. \deluxesystem{} learns to allocate GPT-4o, Llama 3.1 405B, and Gemini 1.5 Pro for the three answer generators separately, and use GPT-4o for the three debaters. In this example, the three generators give completely different answers: 8, 3, and -18, and the GPT-4o debaters identify that 3 is the correct answer.
Allocating GPT-4o to all modules leads to an incorrect answer, however. This is because the GPT-4o generators always return 8 and thus the debaters fail to identify this mistake. We leave more analysis to the appendix due to the space limit. 



%To sum up, \deluxesystem{} outperforms allocating the same LLM to all modules, because (i) different LLMs may specialize in different modules (e.g., Claude 3.5 Sonnet is better at generation while GPT-4o is better at critic on the example shown in Figure \ref{fig:deluxeagent:example_livecodebench}), and (ii) \deluxesystem{} is able to identify this and allocate models appropriately. 


