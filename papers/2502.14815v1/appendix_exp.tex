\section{Experiment Details}
\subsection{Compound AI systems}
\begin{figure}[!ht]
    \centering
    \includegraphics[width=0.9\linewidth]{figure/experiment/example_all.pdf}
    \caption{The architectures of the compound AI systems studied in the experiments. (a) locate-solve consisting of two modules. (b) self-refine using three modules. (c) multi-agent-debate that involves six modules in total.}
    \label{fig:deluxeagent:systemdemo}
\end{figure}

In this paper, we focus on three compound AI systems, locate-solve, self-refine and multi-agent-debate. Their architectures are shown in Figure \ref{fig:deluxeagent:systemdemo}. Locate-solve consists of two modules: the first module extracts the task associated with an ID from an input table, and the second module returns the answer to the extracted task. Self-refine has a generator, a critic, and a refiner. The generator gives an initial answer to a question, the critic gives feedback to this answer, and the refiner uses the feedback to refine the original answer. Multi-agent-debate has two types of modules, answer generators and  debaters. The answer generators offer initial answers to a question. The debaters take the initial answers and then debate which one is correct. 
In this paper, we focus on a six-module multi-agent-debate: three modules are answer generators, and the other three are the debaters.s


\subsection{Datasets and evaluation metrics}
Now we provide details of all datasets used in this paper.

\paragraph{LiveCodeBench.} LiveCodeBench~\cite{jain2024livecodebench} is a benchmark for code understanding.  We use the test output prediction task in LiveCodeBench. It contains 479 questions in total. Each question contains a program and an input. The goal is to predict the output of the program. Note that this is a generative task, as the output space of a given program is unbounded. We  use exact match to measure the performance of a compound system's generation.

\paragraph{CommonGenHard.} CommonGenHard~\cite{madaan2024self} is a constrained generation dataset consisting of 200 questions. Each question gives 20-30 concepts, and the goal is to generate a coherent paragraph that uses all the provided concepts. Since all LLMs used in our evaluation generate coherent texts, we focus on evaluating the quality of whether all concepts are included. That is, the quality is 1 if all concepts are contained in the generated paragraph, and 0 if any concept is missing. 

\paragraph{SimpleQA.} SimpleQA~\cite{wei2024measuring} contains 4326 short, fact-seeking questions. Example questions include ``Who received the IEEE Frank Rosenblatt Award in 2010'' and ``What is the first and last name of the woman whom the British linguist Bernard Comrie married in 1985''. While seemingly simple, LLMs actually struggle to answer them correctly. We use exact match to measure the generation quality of a compound system.

\paragraph{FEVER.} FEVER~\cite{Thorne19FEVER2} is a fact-verification dataset consisting of 2384 questions. Each question contains a claim, and the task is to classify the claim as one of NOT ENOUGH INFO, SUPPORTS, REFUTES. We again use exact match as the accuracy metric.



\paragraph{TableArithmetic.} TableArithmetic is a synthetic dataset used to understand the locate-solve system's performance. It contains 100 questions. Each question consists of a table of ``ID'' and ``task'' rows, and the goal is to solve the task associated with a specific ID. Each row contains 100 entries. Each question has the form of ``What is X+(10.9>10.11)?'', where X is a randomly generated integer.

\paragraph{TableBias.} TableArithmetic is another synthetic dataset. It contains 100 questions. Each question consists of a table of ``ID'' and ``task'' rows, and the goal is to solve the task associated with a specific ID. Here, each table contains 80 entries. Each question has the form of ``The surgeon, who is the boy's father, says I cannot operate on this boy, he is my son. Who is the doctor to the boy? (Ax) Father (Bx) Mother'', where again x is a randomly generated integer.

\subsection{LLM models}
We use 10 LLMs offered by third-party providers,  including GPT-4o, GPT-4o mini, GPT-4-Turbo, Claude 3.5 Sonnet, Claude 3.5 Haiku, Gemini 1.5 Pro, Gemini 1.5 Flash, Llama 3.1 405B, Llama 3.1 70B, and Qwen 2.5 72B. In particular, GPT-4o, GPT-4o mini, and GPT-4 Turbo correspond to gpt-4o-2024-05-13, gpt-4o-mini-2024-07-18 and gpt-4-turbo-2024-04-09 offered by OpenAI. Claude 3.5 Sonnet and Claude 3.5 Haiku refer to claude-3-5-sonnet-20240620 and claude-3-haiku-20240307 by Anthropic. Gemini 1.5 Pro and Gemini 1.5 Flash are gemini-1.5-pro and gemini-1.5-flash by Google, since Google does not offer date-aware snapshots of their APIs.  Finally, open-source models are accessed via the togetherAI APIs. In particular, 
Llama 3.1 405B, Llama 3.1 70B, and Llama 3.1 405B correspond to meta-llama/Meta-Llama-3.1-405B-Instruct-Turbo, meta-llama/Meta-Llama-3.1-70B-Instruct-Turbo and Qwen/Qwen2.5-72B-Instruct-Turbo by togetherAI.
              
\subsection{Prompt for the LLM diagnoser}
The following box gives the prompt template for the LLM diagnoser.

\begin{LLMdiagnoser}

 You are an error diagnosis expert for compound AI systems. Below is the description of a compound AI system consisting of multiple modules, a query, the generations from each module of the compound AI system, the final output, and the desired answer. Assume that the desired answer is 100\% correct. If the final output matches the correct answer, generate ‘error: 0’. Otherwise, analyze whether module i leads to the mistake. If so, generate ‘error: 1’. Otherwise, generate ’error: 0’. Think step by step.


[Compound AI system]:

[query]:

[module 0 output]:

[module 1 output]:

...:

[module $|V|$ output]: 

[final output]:

[desired answer]:

[your analysis]: 
\end{LLMdiagnoser}

\subsection{Qualitative example analysis}
To better understand why \deluxesystem{} can outperform allocating the same LLM to all modules, we give more examples for self-refine and multi-agent-debate, as shown in Figure \ref{fig:deluxeagent:examples_simpleqa} and Figure \ref{fig:deluxeagent:example_livecodebench}.

\begin{figure}[!ht]
    \centering
    \includegraphics[width=0.75\linewidth]{figure/experiment/main/example_simpleqa.pdf}
    \caption{An Illustrative example of applying \deluxesystem{} multi-agent-debate on SimpleQA. (a) the query. (b) model allocation learned by \deluxesystem{}. By allocating GPT-4o, Gemini 1.5 Pro, and LLama 3.1 405B to the three generators separately, \deluxesystem{} enables a diverse set of initial answers, and thus the debaters recognize the correct answer. (c) allocating GPT-4o to all modules. GPT-4o as the generator consistently generates the incorrect answer 8; thus, the debaters fail to identify this issue and lead to an incorrect answer. }
    \label{fig:deluxeagent:examples_simpleqa}
\end{figure}



\begin{figure}[!ht]
    \centering
    \includegraphics[width=0.75\linewidth]{figure/experiment/main/example_livecodebench.pdf}
    \caption{An illustrative example of applying \deluxesystem{} to self-refine on the livecodebench dataset. (a) the query. (b) model allocation learned by \deluxesystem{}. Allocating GPT-4o to the critic recognizes the mistake made by the initial generation and thus leads to the correct answer ``True''. (c) Allocating Claude 3.5 Sonnet to all modules. Claude 3.5 Sonnet as the critic tends to agree with its initial generation and thus its own mistakes can easily be omitted. }
    \label{fig:deluxeagent:example_livecodebench}
\end{figure}

%Figure \ref{fig:deluxeagent:examples_simpleqa} gives one example from the SimpleQA dataset answered by the multi-agent-debate system. \deluxesystem{} learns to allocate GPT-4o, Llama 3.1 405B, and Gemini 1.5 Pro for the three answer generators separately, and always use GPT-4o for the three debaters. In this example, the three generators give completely different answers: 8, 3, and -18. On the other hand, the debaters empowered by GPT-4o consistently identify that 3 is the correct answer. On the other hand, always allocating GPT-4o to all modules leads to an incorrect answer. In fact, the generators empowered by GPT-4o always suggest 8 as the answer. As a result, the debaters fail to identify this mistake, and hence give the incorrect final answer. 


In addition to the examples shown in Figure \ref{fig:deluxeagent:examples_simpleqa} analyzed in the main paper, another example from the LiveCodeBench dataset answered by the self-refine system is shown in Figure \ref{fig:deluxeagent:example_livecodebench}. In this case, \deluxesystem{} learns to use Claude 3.5 Sonnet for the generator and refiner, and uses GPT-4o for the critic module. Recall that always allocating Claude 3.5 Sonnet is better than always allocating any other LLMs. However, this leads to an incorrect answer on this example, as Claude 3.5 Sonnet as the critic fails to realize its own generation is incorrect. However, GPT-4o as the critic correctly identifies the initial generation is incorrect.  Thus \deluxesystem{} correctly answers this question.  
