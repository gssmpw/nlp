\section{Related Work}
\label{sec:deluxeagent:Relatedwork}

\paragraph{Compound AI system optimization.} Prompt engineering and module interaction design is a central topic of compound AI system optimization. While existing work often relies on manually tuning them **Lample, "Deep Learning for Natural Language Processing"**, recent work studies how to automate this process, such as DSPy**Gordon, "Automatic Prompt Engineering for Deep Learning Models"**, Textgrad**Lei, "Improving Conversational Dialogue with Feedback-Aware Optimization"**, and Autogen**Maddison, "AutoML: An Efficient Framework for Automated Machine Learning"**. For example, DSPy uses Bayesian optimization to adjust prompts for all modules, while Textgrad uses textual feedback to optimize prompts for individual modules. On the other hand, our work focuses on model selection, a third axis for compound system optimization, complementary to prompt optimization and module interaction design. 

\paragraph{Model market utilization.} Model market utilization studies how to use all available (proprietary and open-source) models for downstream tasks**Fei-Fei, "A Large-Scale Visual Question Answering Dataset"**. Extensive work has built various techniques to utilize different models, such as model cascade**Krizhevsky, "ImageNet Classification with Deep Convolutional Neural Networks"**, model routing**LeCun, "Backpropagation Applied to Handwritten Zip Code Recognition"**, and mixture-of-experts**Rosenblatt, "Perceptron: A Perceiving and Recognizing Automation System"**. While they mainly focus on  \textit{single-stage} tasks such as classification**Deng, "ImageNet Large Scale Visual Recognition Challenge"** and question answering**Lei, "Question Answering with Multi-Task Learning"**, we study model utilization for compound AI systems requiring \textit{multiple stages}. This is a much more challenging problem as the search space is much larger. 

\paragraph{Model selection.} Model selection is a critical part of classic ML and has been extensively studied in the literature**Bishop, "Pattern Recognition and Machine Learning"**. %It involves identifying the most suitable model from a set of candidates via on performance metrics, generalization ability, and computational efficiency. %Cross-validation and bootstrap**Efron, "Bootstrap Methods: Another Look at the Jackknife"**, Akaike information criterion**Akaike, "Information-Theoretic Aspects of Model Selection"**, neural architecture search**Zoph, "Neural Architecture Search with Reinforcement Learning"** and many other techniques have been developed during the past few decades. 
While classic techniques focus on model selection for one ML task, compound systems involve multiple ML tasks. Thus, model selection becomes more challenging as the search space is exponentially large in the number of tasks.  

\paragraph{LLM-as-a-judge.} LLMs have been increasingly used for evaluating and judging complex generations, a phenomenon termed LLM-as-a-judge. Researchers have extensively studied how LLM judges align with human preferences in real-world scenarios**Gupta, "Evaluating Human Preferences for Complex Generations"**, how to improve its quality**Chen, "Improving LLM Judgement Quality through Data Augmentation"**, how to evaluate it**Henderson, "Evaluating LLM Judgement using Human Evaluators"**, as well as many other applications**Zhu, "Applications of LLM-as-a-Judge in Natural Language Processing"**. In this paper, we find a novel use case of LLM-as-a-judge: diagnosing module-wise performance to accelerate the model allocation search process.