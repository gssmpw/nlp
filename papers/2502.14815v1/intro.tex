\section{Introduction}
\begin{figure*}[!ht]
    \centering
    \includegraphics[width=0.99\linewidth]{figure/intro/new_intro.pdf}
    \caption{\deluxesystem{} outperforms compound AI systems that always call the same LLM. Here we study three compound systems, namely, self-refine (on LiveCodeBench and GCH), multi-agent-debate (on SimpleQA and FEVER), and locate-solve (on TableArithmetic and TableBias).  \deluxesystem{} achieves 5\%-70\% accuracy gains over allocating any model alone by allocating different models to different modules in these compound systems.
    }
    \label{fig:deluxeagent:intro}
\end{figure*}
Researchers and developers are increasingly leveraging large language models (LLMs) by composing multiple LLM calls in a compound AI system to tackle complex tasks~\cite{multi_agent_debate_2024,zhang2024chain,madaan2024self,deepmind2025alphacode2,shinn2024reflexion,renze2024self,compound-ai-blog}. For example, a common practice is to use one LLM call to generate one initial answer, one LLM call to give feedback, and one more call to refine the answer based on the feedback, known as self-refine~\cite{renze2024self,madaan2024self,ji2023towards}.
Another example is multi-agent debate~\cite{multi_agent_debate_2024,liang-etal-2024-encouraging,khan2024debating}, where multiple LLM calls are made to propose initial answers and then debate which ones are correct.  Compared to monolithic models, significant improvements are possible because the compound systems decompose challenging tasks into simpler sub-tasks, and perform one LLM call for each sub-task.

\eat{Despite the commonality of invoking multiple LLM calls, there is little understanding of \textit{model selection} for compound AI systems. In particular, existing work often focuses on optimizing prompts used in individual modules and/or module interactions, and uses the same LLM for all modules~\cite{}. While simplifying the design, overlooking model selection leaves many important questions open. For example, does allocating different models to different modules improve a compound system's performance?  If so, why and by how much? Given a pool of LLMs, how can we find such a model allocation without an exhaustive search?  These questions indicate a significant gap in understanding and optimizing compound AI systems. 


As a first step towards fulfilling this gap, we systematically study model selection focusing on static compound AI systems, i.e., those where the number of modules is fixed. We have found module-wise heterogeneity: an LLM A might be better than an LLM B for module 1, but worse for module 2. This suggests that generation improvement is possible by allocating different models to different modules. Consider, for example, the self-refine system as shown in Figure \ref{fig:deluxeagent:intro}. Allocating GPT-4o to the generator and refiner modules and Claude 3.5 Sonnet to the critic module leads to better performance than always using GPT-4o or Claude 3.5 Sonnet alone.
This is because of the module-wise heterogeneity:  GPT-4o is better at answering and refining, while Claude 3.5 Sonnet is better at providing feedback.
}

Most existing work on improving compound systems focuses on optimizing prompts used in individual modules and/or module interactions, while using the same LLM for all modules~\cite{khattab2024dspy,yuksekgonul2024textgrad,wu2023autogen}. %For example, DSPy~\cite{khattab2024dspy} takes a compound AI system's architecture and one LLM as input, and jointly optimizes all modules' prompts via Bayesian optimization.
While this simplifies compound system design, it also leaves several important questions unaddressed.
Does using different models across modules 
improve a compound system's performance? If so, why and by how much? 
Given a pool of LLMs, can we find the best model each module should use without exhaustive search?


As a first step towards answering such questions, we systematically study model selection in static compound AI systems, i.e., those where the number of modules, the sequencing of module calls, and the mapping between modules and models are fixed.
In this context, we indeed find that allocating different LLMs to different modules leads to substantially higher performance than allocating the same LLM to all modules. 
As an example, consider again the self-refine system~\cite{madaan2024self} consisting of three modules: a generator, a critic, and a refiner. LLM A may be better at providing feedback but worse at generating and refining answers than LLM B. In this case, allocating LLM A for the critic and LLM B for the generator and refiner is better than allocating either one to all modules. 



Then we formulate the model selection problem (MSP), i.e., identifying the best model each module should use to maximize the overall performance. MSP is challenging in principle, as it is infeasible to exhaustively search the exponentially large space of all model choices. Our insights are that, in many cases, (i) the end-to-end performance can be monotonic in per-module performance, and (ii) per-module performance can be estimated accurately by an LLM diagnoser. This motivates us to design \deluxesystem{}, a principled framework that optimizes MSP for any static compound AI systems given a training budget. \deluxesystem{} iteratively nominates one module and allocates to it the model with the best module-wise performance, as estimated by an LLM diagnoser. One benefit is that \deluxesystem{} is applicable to any compound AI system whose number of modules is fixed. Furthermore, \deluxesystem{} only incurs a manageable amount of LLM calls. In fact, we provide mathematical conditions under which \deluxesystem{} finds the optimal solution to MSP with the number of LLM calls linear to the number of modules (Section \ref{sec:deluxeagent:Method}). 

We conduct systematic experiments on a diverse set of compound AI systems using real-world LLM APIs including GPT-4o, Claude 3.5 Sonnet, and Gemini 1.5 Pro. Perhaps surprisingly, we have found that different model choices have a significant effect on compound systems' performance. In fact, \deluxesystem{} offers 5\%-70\% performance gains compared to allocating the same LLM to all modules (Figure \ref{fig:deluxeagent:intro}). While not optimizing prompts, \deluxesystem{} also outperforms advanced techniques specializing in prompt optimization (Table \ref{tab:deluxeagent:mainresult} in Section \ref{sec:deluxeagent:Exp}). This further highlights the importance of model selection for compound AI systems. 

In short, our main contributions are:
\begin{itemize}
\item \textbf{Model selection problem.} We formulate the model selection problem (MSP) for compound AI systems, an increasingly important but under-explored problem. \\

\item \textbf{The \deluxesystem{} framework.} To optimize MSP, we propose \deluxesystem{}, a principled framework that iteratively chooses one module and allocates to it the model with the highest module-wise performance estimated by an LLM. \\

\item \textbf{Model choices matter.} Through extensive experiments on practical compound systems using real-world LLM APIs including GPT-4o, Claude 3.5 Sonnet, and Gemini 1.5 Pro, we have found that choosing different models can substantially affect (up to 100\%) a compound AI system's performance.

\item \textbf{\deluxesystem{} finds excellent choices.} Systematical experiments have shown that \deluxesystem{} identifies model choices that outperform allocating the same LLM to all modules by 5\%-70\%.\\

\item \textbf{Open-source artifacts.} We release\footnote{\url{https://github.com/LLMSELECTOR/LLMSELECTOR}} our code and data, including compound systems' intermediate outputs generated by commercial LLM APIs.
\end{itemize}


\section{Related Work}\label{sec:deluxeagent:Relatedwork}

\paragraph{Compound AI system optimization.} Prompt engineering and module interaction design is a central topic of compound AI system optimization. While existing work often relies on manually tuning them~\cite{deepmind2025alphacode2,shinn2024reflexion,zhou2024agents2,pryzant2023automatic,fourney2024magentic,zhao2024expel,lu2024chameleon,zhao2024expel}, recent work studies how to automate this process, such as DSPy~\cite{khattab2024dspy}, Textgrad~\cite{yuksekgonul2024textgrad}, and Autogen~\cite{wu2023autogen}. For example, DSPy uses Bayesian optimization to adjust prompts for all modules, while Textgrad uses textual feedback to optimize prompts for individual modules. On the other hand, our work focuses on model selection, a third axis for compound system optimization, complementary to prompt optimization and module interaction design. 

\paragraph{Model market utilization.} Model market utilization studies how to use all available (proprietary and open-source) models for downstream tasks~\cite{lu2024merge,ramirez2024optimising,miao2023towards}. Extensive work has built various techniques to utilize different models, such as model cascade~\cite{chen2023frugalgpt}, model routing~\cite{hu2024routerbench,stripelis2024tensoropera}, and mixture-of-experts~\cite{wang2024mixture}. While they mainly focus on  \textit{single-stage} tasks such as classification~\cite{chen2020frugalml,huang2025thriftllm} and question answering~\cite{chen2023frugalgpt,shekhar2024towards}, we study model utilization for compound AI systems requiring \textit{multiple stages}. This is a much more challenging problem as the search space is much larger. 

\paragraph{Model selection.} Model selection is a critical part of classic ML and has been extensively studied in the literature~\cite{kohavi1995study,akaike1974new,elsken2019neural}. %It involves identifying the most suitable model from a set of candidates via on performance metrics, generalization ability, and computational efficiency. %Cross-validation and bootstrap~\cite{kohavi1995study}, Akaike information criterion~\cite{akaike1974new}, neural architecture search~\cite{elsken2019neural} and many other techniques have been developed during the past few decades. 
While classic techniques focus on model selection for one ML task, compound systems involve multiple ML tasks. Thus, model selection becomes more challenging as the search space is exponentially large in the number of tasks.  

\paragraph{LLM-as-a-judge.} LLMs have been increasingly used for evaluating and judging complex generations, a phenomenon termed LLM-as-a-judge. Researchers have extensively studied how LLM judges align with human preferences in real-world scenarios~\cite{zheng2023judging,shankar2024validates}, how to improve its quality~\cite{kim2023prometheus}, how to evaluate it~\cite{chiang2024chatbot,chen2024mllm,zeng2023evaluating}, as well as many other applications~\cite{johri2025evaluation,dhole2024conqret,gu2024survey,zhou2024llm}. In this paper, we find a novel use case of LLM-as-a-judge: diagnosing module-wise performance to accelerate the model allocation search process. 
