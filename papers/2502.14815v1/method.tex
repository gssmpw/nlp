\section{Modeling and Optimizing Model Selection}\label{sec:deluxeagent:Method}

\begin{figure*}[!ht]
    \centering
    \includegraphics[width=0.99\linewidth]{figure/method/method.pdf}
    \caption{\deluxesystem{} Workflow. \deluxesystem{} takes as input a compound AI system, a pool of candidate LLMs, a training dataset consisting of question-answer pairs, and a training budget. Then \deluxesystem{} iteratively nominates one module and allocates to it the model with the highest module-wise performance estimated by an LLM. This is repeated until the budget is reached or no performance gain is possible. Finally, \deluxesystem{} returns an optimized model allocation. }
    \label{fig:deluxeagent:method}
\end{figure*}
This section presents how to model and optimize model selection for static compound AI systems. 

\subsection{Problem Statement}
Consider a static compound AI system $G=(V,E)$ and a set of LLMs  $M\triangleq \{1,2,\cdots, |M|\}$ to use. Let $\allocationspace:V \mapsto  M$ denote all possible model allocations, each of which allocates an LLM $k \in M$ to a module $v\in V$. Given a task distribution $\Dataset$, the performance of the compound AI system using the model allocation $\allocation \in \allocationspace$ is $\Performance(\allocation) \triangleq \E_{\query \in \Dataset} [ \performance (\allocation,\query)]$. Here, $\query$ denotes a task sampled from the data distribution, and $\performance (\allocation,\query)$ is the performance of the compound AI system on the given task $\query$ using the allocation $\allocation$. Our goal is to find one model allocation that maximizes the overall performance, i.e.,   
\begin{equation}\label{eq:deluxeagent:problem}
\max_{\allocation \in \allocationspace} \Performance(\allocation)
\end{equation}

\subsection{The assumptions}
Problem \ref{eq:deluxeagent:problem} is challenging without any assumptions, as it is impossible to exhaustively search all possible model allocations, the size of which grows exponentially in the number of modules $|V|$. Here we list our assumptions to enable tractable analysis.

\paragraph{Binary performance.}  For simplicity, we only consider binary performance, i.e., $\performance (\allocation,\query)\in \{0,1\}$.

\paragraph{Decomposition to per-module performance.} In classic computing systems such as a hardware stack, optimizing individual components (such as CPU, GPU, and memory) often leads to better overall performance. Similarly, improving individual modules' quality should also lead to better overall quality of a compound AI system. For the sake of analysis, we also assume that we can decompose a compound system's performance as a monotone function of individual modules' performance. Formally, let $\performance_i(\allocation, \query)$ denote module $v_i$'s performance on the task $\query$ using allocation $\allocation$. Then the end-to-end performance can be decomposed as $\performance(\allocation,\query) = \decompose(\performance_1(\allocation,\query),\performance_2(\allocation,\query),\cdots, \performance_L(\allocation,\query))$, where $\decompose(\cdot)$ is monotonically increasing. 

\paragraph{Monotone module-wise performance.} The module-wise performance needs to satisfy certain properties to enable us to analyze the interplay between individual modules and the compound systems. In this paper, we focus on module-wise performance $\performance_i$ with the following two conditions. 
\begin{itemize}
    \item $\performance_i$ is \textit{intra-monotone}, which means that 
    
    \begin{equation*}
    \begin{split}
\performance_i(\allocation_{i\rightarrow k},\query) & \geq  \performance_i(\allocation_{i\rightarrow k'},\query) \\
&\implies\\\performance_i(\allocation'_{i\rightarrow k},\query) & \geq  \performance_i(\allocation'_{i\rightarrow k'},\query) 
    \end{split}
    \end{equation*}
In simple terms, $\performance_i$ induces a ``ranking'' for each module: no matter how models are allocated to other modules, allocating model $k$ to a given module is always ``better'' than model $k'$.  
    
    \item $\performance_i$ is \textit{inter-monotone}, which indicates that 
    \begin{equation*}
    \begin{split}
\performance_i(\allocation_{i\rightarrow k},\query) & >  \performance_i(\allocation_{i\rightarrow k'},\query)\\
&\implies\\\forall j, \performance_j(\allocation'_{i\rightarrow k},\query) & \geq  \performance_j(\allocation'_{i\rightarrow k'},\query) 
    \end{split}
    \end{equation*}
In other words,  if module $i$th performance is higher by replacing its allocated model from A to B, then such replacement should not hurt other modules' performance no matter what models are allocated to other modules. 
\end{itemize}

\eat{\textit{Example.} Consider a two-module compound system for retrieval-augmented generation. The first module generates a search query in order to extract the relevant context for a given question, and the second module uses the context to answer the given question. Here, $\performance_1(\allocation,\query)$ is whether the extracted context is relevant to the given question, and $\performance_1(\allocation,\query)$ measures whether the question is question can be answered correctly given the context. One can easily verify that the overall performance can be written as $\performance(\allocation,\query) = \performance_1(\allocation,\query) \cdot \performance_2(\allocation,\query)$ and thus it is monotonically increasing as the module-wise performance increases. The module-wise performance is also intra-monotone and inter-monotone in this case: after all, whether an LLM extracts relevant context is independent of whether a correct answer can be generated given the context. 
}
\eat{\textit{Example 2.} Another example is a three-module system for accessible tasks, e.g., helping a blind student complete school assignments. The first module extract all texts from the snapshot of the assignments, and the second module gives hints to the problem described by the texts. The last module converts the hints to audio. Again, the module-wise performance is simply the quality of each subtask, and the end-to-end performance is the product of all three module-wise performance functions. Following the similar argument in Example 1, these module-wise performance functions are also intra-monotone and inter-monotone.
}
\textit{Do the assumptions always hold?} The above two conditions simplify our analysis, but they are not always satisfied in practice. In these cases, while our analysis may not hold, the derived algorithm is still applicable and demonstrates superior performance (as shown later in Section \ref{sec:deluxeagent:Exp}).

\paragraph{Optimality Characterization.} Suppose the module-wise performance is both intra-monotone and inter-monotone. Then we are able to study the optimal allocation via the lens of module-wise performance. In particular, we first argue that it is possible to find a model allocation that maximizes the performance for each module. This is because the module-wise performance is inter-monotone: improving the model used for one module can only improve the performance for other modules. The second observation is that a module-wise optimal allocation must also be the globally optimal allocation. This is due to the fact that the end-to-end performance is a monotone function of all individual module-wise performance. 

\subsection{The \deluxesystem{} framework}
The above analysis motivates our design of \deluxesystem{}, a principled framework for optimizing model allocation in compound AI systems within a budget constraint. 

Figure \ref{fig:deluxeagent:method} gives an overview of how  \deluxesystem{} works. It takes the compound AI system architecture $G$, the set of LLM $M$, a training dataset $\Datasettrain$, and a training budget $B$ as input, and returns an optimized model allocation $\hat\allocation$ as the output. Here, each data point in the training dataset $\query = (\question,\answer) \in \Datasettrain$ is a question-answer pair specifying a possible question and desired answer. \deluxesystem{} involves an iterative process. In each iteration, it nominates one module and then allocates to the module the model with the highest module-wise performance. This is repeated until running out of the training budget or no module can be further improved by updating one module at a time. The details can be found in Algorithm \ref{alg:deluxeagent:algorithm}. The following result shows when \deluxesystem{} can identify the optimal allocation. The proof is left to the appendix. %\ion{Should we say the proof is in the appendix?}

\begin{theorem}\label{thm:deluxeagent:convergence}
Suppose for each task $\query$ in $\Datasettrain$, the optimal allocation is unique. Then Algorithm \ref{alg:deluxeagent:algorithm} converges to the optimal allocation on the training data after $L$ iterations.   
\end{theorem}

\setlength{\algomargin}{2em} % Increase the left margin to fit the line numbers
\begin{algorithm}[!ht]
\DontPrintSemicolon
\KwIn{A compound system $G=(V,E)$, a pool of $K$ candidate LLMs, a training dataset $\Datasettrain$, and a training budget $B$ }
\KwOut{An optimized model allocation $\hat{\allocation}$}

\SetAlgoLined
Choose a random $\hat{\allocation}_0 \in \allocationspace$ \tcp{initialize}
Set $i \gets 1, c\gets 0, \delta \gets False, \allocation_{\query} \gets \allocation_0, \forall \query \in \Datasettrain$\;
\While{$\textit{$c\leq B-|M|$}$ and $\delta = False$}{
    $j \gets i\bmod{L}+1 $\tcp{nominate a module}
    $k_\query \gets \max_{k\in M} \performance_j(f_{\query,j\rightarrow k}, \query)$\;
    $f_{z} \gets f_{z,j\rightarrow k_z} $\tcp{select a model}
    $f_i \gets \text{mode}\{ f_z : z \in D \}  $\tcp{aggregate}
    $c\gets c+|M|$\tcp{update the cost}
    \If{$i>L$}{$\delta \gets \prod_{t=i-L}^{i} \mathbf{1}_{f_{t}=f_{i}}$\tcp{stop criteria}}
}
Return $\allocation_i$\tcp{optimized model choices}
\caption{How \deluxesystem{} works. }\label{alg:deluxeagent:algorithm}
\end{algorithm}


\paragraph{The LLM diagnoser.} \deluxesystem{} requires access to the model-wise performance function $\performance_i$. In practice, however, this is often unavailable or too expensive to collect. Therefore, we propose to use a LLM diagnoser to estimate the model-wise performance function. In particular, we give an LLM as input a compound AI system $G=(V,E)$, a task $\query = (\question,\answer)$ consisting of a question $\question$ and the desired answer $\answer$, the inputs and outputs of each module $v \in V$ using a specific allocation $\allocation$, and ask it to determine module $j$th's performance. Let  $\hat{\performance}_j(f,\query)$ denote the output by the LLM  diagnoser. Then we approximate the module-wise performance by 
${\performance}_j(f,\query) = \hat{\performance}_j(f,\query) + \gamma {\performance}(f,\query)$, where $\gamma>0$ is a hyperparameter balancing the LLM's estimation and the end-to-end performance. The prompt used for the LLM diagnoser can be found in the appendix.

