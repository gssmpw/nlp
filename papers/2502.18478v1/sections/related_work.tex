\subsection{Perturbation based self-Supervised learning} Perturbation based self-supervised learning has showcased its effectiveness in numerous applications.
For instance, Chen et, al. \cite{chen2020simple} introduced SimCLR, a Contrastive Learning approach, demonstrating that after representation learning with SimCLR, only a minimal 1\% of labeled data suffices to attain the same top-5 accuracy as AlexNet. Building on top of this work, Zbontar et, al. \cite{zbontar2021barlow} introduced Barlow Twins, which through the correlation of augmented and original data representations, achieved significant performance gains in computer vision problems. SSL has also made substantial contributions to the field of Natural Language Processing (NLP).  For instance, Gao et, al. \cite{gao2021simcse} introduced SimCSE, and Chuang et, al. \cite{chuang2022diffcse} introduced DiffCSE, both of which leveraged contrastive learning methods on improving sentence embeddings.


\subsection{Self-Supervised Learning for Recommendation Systems} With the substantial influence of perturbation based self supervised learning in the fields such as natural language processing and computer vision, researchers have extended their exploration to recommendation systems. One example of such kind of efforts is Wang et, al. \cite{contrastive_for_ctr} which focuses on enhancing Click-Through Rate (CTR) and Conversion Rate (CVR) estimation by applying Contrastive Learning techniques at the embedding level. This approach emphasizes the importance of post-embedding level operations and highlights the potential of self-supervised techniques for advancing ad ranking, offering valuable insights for large-scale ad recommendation systems.

In \cite{augmentation2}, researchers have made substantial contributions to the field of large-scale recommendation systems with a focus on perturbation based self-supervised learning. Their work introduces a two-stage perturbation approach at the embedding level, complemented by the application of contrastive learning to the predictions generated in each of these stages. Moreover, the paper introduces an inventive feature masking technique named Correlated Feature Masking. The combination of Correlated Feature Masking and Contrastive Learning yields exceptional performance in the desired metrics. These innovations, including the two-stage perturbation approach and Correlated Feature Masking, mark significant advancements in the domain of self-supervised learning for recommendation systems.

\cite{introduction_4} has harnessed the power of Self-Supervised Learning techniques in daily user interactions. Their work showcases that Self-Supervised Learning, combined with pre-training and fine-tuning, has led to impressive enhancements in Click-Through Rate (CTR) and Conversion Rate (CVR) tasks, yielding substantial improvements ranging from 6\% to 9\%. 

The strength of Self-Supervised Learning in recommendation systems has been comprehensively examined in the survey paper authored by Huang et al. \cite{self_rec_survey}. This survey provides an in-depth analysis of various Self-Supervised Learning methodologies, including Contrastive \cite{contrastive_survey}, Generative \cite{generative_survey}, Predictive, and Hybrid Methods. These techniques are thoroughly explored for their applicability in recommendation systems, offering valuable insights into the advancements of self-supervised approaches for this domain.


\subsection{Self-Consistency Regularization (SCR)}
Self-Consistency Regularization, engineered to ensure semantic similarity within the latent space for objects that share common semantics, as detailed in the research by Sinha et al. \cite{sinha2021consistency}, has a well-documented track record of efficacy. 
Previous studies consistently attest to the capability of this technique in fostering proximity of representations for semantically related objects in the latent space.
In the literature, one of the aspects that has been attributed to the success of consistency regularization and contrastive learning~\cite{zhang2022rethinking} has been identified as the use of Data Augmentation.

\subsection{Data Augmentation}
Data augmentation stands as a fundamental component in many self-supervised learning algorithms. While deep neural networks excel in various challenges in learning from data, they are particularly sensitive to data volume \cite{augmentation1} and often struggle to grasp the underlying data distribution. Given the scale of these models, insufficient data can lead to highly variable predictions in diverse settings.
Data augmentation can incorporate strong priors from data or domain knowledge into models~\cite{eghbalzadeh2024rethinking}, and further be used to regularize models towards better robustness and generalization~\cite{zhang2017mixup,yun2019cutmix}. However, most of the focus in such approaches have been on structured data such as images, audio, etc; and it has been shown that such domain-specific augmentations should be used in new domains with caution~\cite{eghbalzadeh2024rethinking}.
