The goal in this section is to analyse how different perturbation-based regularizations, namely SCR and LSPR, impact learning and performance.
To this end, we simplify both LSPR and SCR frameworks to their core, and furthermore using linear models study their effects in learning dynamics and performance. We use 2-layer linear models which strike a good balance between model expressiveness and simplicity~\cite{werfel2003learning}. To this end, we define a ground-truth function with the weight $\mW^*$ that maps input data to their labels as follows:
\begin{align}
    \vy={\mW}^*\vx
\end{align}
where $\vx$ denotes an input feature and $\vy$ denotes the ground-truth output, and ${\mW}^*$ is a $L_y\times L_x$ matrix where $L_y$ and $L_x$  are input and output dimensionalities.


We now define the following linear model that we use to learn the input-output relationship by:
\begin{align}
    \vy={\mW_2}{\mW_1}\vx
\end{align}
where $\vx$ denotes an input feature and $\vy$ denotes the ground-truth output,  ${\mW_1}$ is a matrix of size $L_h\times L_x$ and  and $\mW_2$ is a matrix of size $L_y\times L_h$ and $L_h$ is the dimensionality of the intermediate representations.
To simulate the effect of regularization, we use Stochastic Gradient Descent (SGD) with an MSE error as follows:
\begin{align}
    \mathcal{L}(\vx,\vy)=\frac{1}{2}||\vy-\hat{\vy}||^2,
\end{align}
and $\hat{\vy}$ is the output of the linear model.
In order to study the learning dynamics, we denote the  weight error as:

\begin{align}
    \mE=\mW_2\mW_1-\mW^*
\end{align}
and further introduce:
\begin{align}
    \epsilon=\frac{1}{L_x L_y}\trace[\mE^T\mE], \gamma=\frac{{\mW_2\mW_1 \cdot \mW^*}}{{\|\mW_2\mW_1\| \|\mW^*\|}}
\end{align}
where $\epsilon$ represents the error in the weight space to the optimal weight, while $\gamma$ demonstrates weight alignment with the optimal weight $\mW^*$.
The SGD weight updates are as follows:
\begin{align}
    \delta\mW_1^{SGD}&=-\eta\frac{\partial \mathcal{L}(\vx,\vy)}{\partial \mW_1} \\
    &=-\eta\left(\mW_2^T(\mW_2\mW_1\vx-\vy)\right) \otimes \vx \\
    \delta\mW_2^{SGD}&=-\eta\frac{\partial \mathcal{L}(\vx,\vy)}{\partial \mW_2} \\
    &=-\eta(\mW_2\mW_1\vx-\vy) \otimes \mW_1\vx 
\end{align}
with $\otimes$ denoting the outer product.

In order to simulate LSPR, we sample noise $\vz\sim\mathcal{N}(0, I)$ and additionally add $\mathcal{L}(\vx+\omega\vz,\vy)$ where $\omega$ is a small weight for the perturbation $\vz$.
The final loss will be a balance of $\mathcal{L}(\vx,\vy)+\lambda\mathcal{L}(\omega\vz+\vx,\vy)$.
The LSPR weight updates are then defined as:
\begin{align}
\label{eq:lspr_updates}
    \delta\mW_1^{LSPR}&=-\eta\left(\mW_2^T(\mW_2\mW_1\vx-\vy)\right) \otimes \vx \nonumber\\
&-\lambda\eta\left(\mW_2^T(\mW_2\mW_1(\omega\sigma\vz+\vx)-\vy)\right) \otimes (\omega\sigma\vz+\vx)\\  
\delta\mW_2^{LSPR}&=-\eta(\mW_2\mW_1\vx-\vy) \otimes \mW_1\vx \nonumber\\
&-\lambda\eta(\mW_2\mW_1(\omega\sigma\vz+\vx)-\vy) \otimes \mW_1(\omega\sigma\vz+\vx)
\end{align}

To analyse the SCR method, we rely on the additional learning signal that pushes the output of a model on clean and noisy inputs closer together, namely, $\mathcal{L}(\vx+\omega\vz,\hat{\vy})$ where $\hat{\vy}=\mW_2\mW_1\vx$.
Consequently, the SCP weight updates are as follows:
\begin{align}
\label{eq:scr_updates}
    \delta\mW_1^{SCR}&=-\eta\left(\mW_2^T(\mW_2\mW_1\vx-\vy)\right) \otimes \vx \nonumber\\
&-\lambda\eta\left(\mW_2^T(\mW_2\mW_1(\omega\sigma\vz+\vx)-\mW_2\mW_1\vx)\right) \nonumber\\
&\otimes (\omega\sigma\vz+\vx)\\  
\delta\mW_2^{SCR}&=-\eta(\mW_2\mW_1\vx-\vy) \otimes \mW_1\vx \nonumber\\
&-\lambda\eta(\mW_2\mW_1(\omega\sigma\vz+\vx)-\mW_2\mW_1\vx) \nonumber\\
&\otimes \mW_1(\omega\sigma\vz+\vx)
\end{align}

We use the following parameters for our analysis: $\omega=\{0.1,0.9\}, \lambda=\{0.001,1\}, \eta=1.4, L_x=100,L_h=10^4,L_y=10$, and we perform the weight updates for the number of $100k$ times, and for every update we sample new data from the denoted distributions.

\subsubsection{Results}
As can be seen in Figure~\ref{fig:numerical_analysis}, we can observe that for small perturbation weights $\omega$ and loss weights $\lambda$, LSPR tends to better find the optimal weight as can be seen by looking at the two presented plots.  


\begin{figure*}[t]
    \centering
    \begin{subfigure}{0.45\textwidth}
        \centering
        \includegraphics[width=\textwidth]{used_figures/alignment_with_wstart.png}
        \caption{Alignment with the optimal weight $\mW^*$.}
        
        \label{fig:first_fig}
    \end{subfigure}
    \hfill
    \begin{subfigure}{0.45\textwidth}
        \centering
        \includegraphics[width=\textwidth]{used_figures/weight_space_error.png}
        \caption{Error in the weight space.}
        \label{fig:second_fig}
    \end{subfigure}
    \caption{Numerical analysis comparing LSPR and SCR.
    $\omega$ denotes noise sample weight and $\lambda$ depicts loss weight.} 
\label{fig:numerical_analysis}
\end{figure*}