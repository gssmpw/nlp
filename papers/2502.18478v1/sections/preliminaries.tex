Click-Through Rate (CTR) prediction aims to estimate the probability of the user clicking a candidate ad after having an impression in the ranking stage. Similarly, Conversion Rate (CVR) prediction estimates how likely the user will convert the candidate ad after having a click. Our perturbation-based regularization techniques can be applied to both CTR and CVR predictions with similar set-ups, therefore, we use the CTR prediction as an example to introduce the basic preliminaries, and the intrinsic differences between CTR and CVR modeling (e.g., delayed feedback for ad conversions) is beyond the scope of the discussions in this paper.

For the CTR prediction task, let a training dataset with $\mathit{N}$ examples be defined as $\left\{\vx_n, {\vy}_n\right\}_{n=1}^{N}$, where a random variable $\vx_n$ represents the feature space of the $n$-th training example, and a random variable $\vy_n\in \{0, 1\}$ represents the binary label indicating whether the user has clicked the candidate ad or not. The feature space can consist of the following types:
\begin{itemize}
    \item \textbf{Dense features} are single-digit float values (e.g., counts and stats (mean, percentiles, variances) of user/ad behaviors and profiles), and the total number of such features could be in the scale of thousands. 
    We initially apply a pre-processing procedure on each of them, and then concatenate them together to form a single high-dimensional float vector, so as to interact with other features later;\\
    
    \item \textbf{Embedding features} are high-dimensional float vectors which are usually generated from pre-trained manners (e.g., user and ad embeddings from graph learning algorithms). We will denote embedding features as $\ve_i$\\ 
    
    \item \textbf{Sparse features} are high-dimensional integer vectors, that represent concepts such as user-item interactions, where both number of items and users are large. Sparse features can often be represented via a much lower dimensional vector $\vs_i$ via various techniques, e.g, the use of an embedding matrix, or affinity scores for reweighing.
    
\end{itemize}

After processing each feature to generate the corresponding representation, the feature interaction layer is applied on top to learn their interactions with arbitrary orders (e.g., DHEN \cite{DHEN}, DCN \cite{DCN}, Transformer \cite{Transformer}), and generate a final representation $\vr_n$. Afterwards, the prediction layer produces the prediction probability $\hat{\vy}_n \in [0, 1]$ based on $\vr_n$, and the commonly adopted loss function is calculated as  

\begin{align}
\label{eq:bce}
   \mathcal{L_{\text{supervised}}}(\vy_i,\hat{\vy}_i) = -\frac{1}{N}\sum_{n = 1}^{N} \vy_n \log (\hat{\vy}_n) + (1 - \vy_n) \log (1 - \hat{\vy}_n).
\end{align}