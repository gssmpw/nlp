\newmaterial{
In this section, we leverage a well-known theoretical framework proposed in~\cite{werfel2003learning} to demonstrate how LSPR results in a better alignment of weights in the model optimization to portray a clear picture of the construct of a optimization problem in ranking, and how LSPR affects it.}
We start by formalization of our framework, as well as the integration of Perturbation-Based Regularizations, namely SCR and LSPR. 
In Section~\ref{subsec:numerical_analysis} we analyze how LSPR compares to SCR via controlled experimentations and analysis on linear models, investigating the learning dynamics with these regularization applied.
Further, in Section~\ref{subsec:real_data}, we report our empirical results on an in-house dataset that was used to evaluate the methodologies applied here. 
We tracked model accuracy using Normalized Entropy (NE) in offline experiments \cite{he2014practical}.
In experiments with real data, each datapoint exhibits a substantial volume of features, comprising thousands of dense features and hundreds of sparse features and we employ the Adagrad optimizer for optimization.
Ranking has been done through multiple stages during learning, which are described below in more details.
In this section, we report performance improvements via the presented regularization techniques on a multi-stage ranking system with 3 stages of retrieval, early stage ranking, and final-stage ranker.


\subsection{Numerical Analysis}
\label{subsec:numerical_analysis}

In this section, we provide a numerical analysis for the linear models trained with SGD 1) with Self-consistency Regularization (SCR), and 2) with Loss-Balanced Small Perturbation Regularization (LSPR).
We analyse the gradient update directions and the alignment with the optimal weight (See Section.~\ref{subsec:numerical_setup}) by calculating the cosine similarity in the model's weight space, comparing weights of different iterations to the optimal weight.
Our numerical analysis (see Figure~\ref{fig:numerical_analysis}) shows that: 
\begin{enumerate}
    \item compared to SCR, LSPR finds a better alignment with the optimal weight, while converging faster and achieving a lower error in the weight space. 
    \item we also show that balancing both amount of noise $\omega$ and loss $\lambda$ is crucial to the success of LSPR and SCR. As we show, smaller values for these weights are recommended for better convergence and performance.
\end{enumerate} 


\subsubsection{Setup}
\input{sections/numerical_analysis}

\subsection{Experiments on Real Data}
\label{subsec:real_data}
\subsubsection{Self-Consistency Regularization (SCR)}

We experimented with perturbation-based consistency regularization on different stages of various prediction problems. We present these results in Table~\ref{tab:rel_ne}.
We observed a relative NE gain of approximately 0.1\%-0.3\%, depending on the prediction model tested in various offline experiments.
We will first present the results for the Retrieval stage from the offline experiments, followed by the experimental results for the Early and Final Stage ranker.

\noindent\textbf{Offline Retrieval Stage:} We have experimented with consistency regularization in two different models that predict conversion rate and click-through rate, respectively. 
We obtained the best results when regularizing both logit and representative of embedding with 0.15\%-0.2\% relative NE improvements.

\noindent\textbf{Offline Early Stage Ranking:} models are generally simpler ranking models compared to final stage models. Therefore, we applied regularization to the entire object and user embedding, resulting in a 0.3\% relative NE gain in various offline experiments.


\noindent\textbf{Offline Final Ranking Stage:} 
Models in this stage are generally much larger and complex compared to previous stages, as we are looking for more precise ranking of ads. 
We obtained the best results by regularizing both the logit and output of the embedding together.
Results from several experiments suggest an average 0.1\% relative NE gain, which has been further validated with online testing.


\begin{table}
  \caption{Relative NE gains for SCR across various stages.}
  \label{tab:rel_ne}
    \centering
  \begin{tabular}{cccc}
  \toprule
    Model & 33\% of data &  66\% of data & 100 \% data\\
    \midrule
    \texttt{Baseline} & 0 \% & 0 \% &  0 \% \\
    \textbf{Retrieval} & \bf{0.14 \%} & \bf{0.19} &  \bf{0.14 \%}\\
    \textbf{Early Stage} & \bf{0.28 \%} & \bf{0.3 \%} &  \bf{0.35 \%}\\
    \textbf{Final-stage Ranker} & \bf{0.1 \%} & \bf{0.08 \%} &  \bf{0.07 \%}\\
    \bottomrule
  \end{tabular}
\end{table}

\subsubsection{Loss-Balanced Small Perturbation Regularization (LSPR)}
We have explored LSPR primarily in the offline Final Ranker Stage, under various signal availability setups. We have observed that the technique has performed promisingly in various setups, ultimately leading to improved performance in each of these environments. These results are depicted in Table~\ref{tab:rel_ne2}.



\noindent\textbf{Offline Final Ranking Stage:} testing is very similar to Consistency Regularization testing; however, in the former, we perturbed the entire batch each time, leading to doubled batch size. 
In contrast, for Consistency Regularization, we only perturbed a small fraction of points in each batch.\\

\begin{table}
  \caption{Relative NE gains comparing SCR and LSPR on Final-stage Ranker.} 
  \label{tab:rel_ne2}
  \centering
  \begin{tabular}{cccc}
    \toprule
    Model & 33\% of data &  66\% of data & 100 \% data\\
    \midrule
   \texttt{Baseline} & 0 \% & 0 \% &  0 \% \\
    \textbf{SCR} & \bf{0.1\ \% } & \bf{0.08 \%} &  \bf{0.07 \%}\\
    \textbf{LSPR} & \bf{0.13 \% } & \bf{0.11 \%} &  \bf{0.1 \%}\\
    \bottomrule
  \end{tabular}
\end{table}


\subsubsection{Online Experiments}

We additionally have conducted online experimentation for a prediction model after testing it in offline setup.
The online experimentation is different than offline one in the nature that, it runs in continuous training and inferring routine, compared to full training and inferring mode.
% 
These online experiments on various data from different parts of the data stream, using both noisy and clean labels, have demonstrated a similar trend to the offline experiments we reported in the previous sections.
\newmaterial{
Our results indicate that LSPR has achieved a 0.1\% to 0.2\% relative improvement in online top-line metrics, consistently across multiple launches. Note that the magnitude of the impact is significant at the level of a billion-scale industrial production ads ranking system, which serves billions of users across various surfaces , across global geological locations, and across various clients. 
}

\subsection{Baselines}

\newmaterial{
The experiment comparisons in this manuscript are all compared against the latest production models in a multi-billion-scale industrial ads ranking system, prior to the adoption of LSPR. 
Our criteria for selecting baselines was to identify models that 1) have been proven to operate effectively at the industry scale; 2) represent the  state-of-the-art ads ranking product models  in the industry.
We consider these production-level recommendation models to be among the state-of-the-art baselines that meet the above criterion. 
}
