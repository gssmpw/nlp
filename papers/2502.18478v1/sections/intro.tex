In the fast-paced and dynamic world of online advertising, the task of advertisements (ads) ranking helps businesses with their target audiences. 
The primary goal of ads ranking is to determine which ads are displayed to users via machine learning techniques, ensuring that the most relevant ones appears prominently. 
This process directly influences user engagement and click-through rates \cite{introduction_3, introduction_4}. 
\begin{figure}[h]
  \centering
  \includegraphics[width=6.5cm, height = 5.6cm]{used_figures/perturbation_framework.jpeg}
  
  \caption{A General Perturbation Based Regularization Framework}
\label{fig:perturbation_framework}
\end{figure}

Ads ranking at an industry scale is often achieved through a multi-stage approach, encompassing retrieval, pre-ranking (or early-stage ranking), and final-stage ranking, which nowadays are mostly powered by large-scale neural networks \cite{introduction_1, introduction_2}.
This efficient multi-stage system strikes a balance between computational costs and recommendation quality \cite{int_ctr, int_ctr_2, naumov2019deep}. 

In recent years, the impact of deep learning, and notably its success in domains such as computer vision and natural language processing \cite{int_deep_learninig, int_nlp}, has been extended to recommendation systems.
Part of this success is due to the use of optimization objectives that can model user engagement via leveraging for deep neural networks, which as a result has motivated the migration of many significant industrial recommendation models to deep neural network architectures \cite{int_ctr_2, int_coll_dl}, illustrating its profound role in shaping the future of recommendation systems.

Self-supervised learning (SSL) stands out as a powerful technique with significant benefits for various facets of deep learning model development. 
At its essence, SSL is crafted to aid models in capturing intricate information that may prove challenging to extract directly from raw data, due to its reliance not only on labeled data which are often limited in amount, but also on unlabeled data which is more widely available. 
This capability becomes particularly pronounced when applied to large models facing constraints in accessing labeled data. 
% Additionally, SSL can leverage substantial amounts of unlabeled data to boost the performance of ranking models. 
Within the realm of SSL algorithms, the perturbation-based regularization technique emerges as a noteworthy one that is used jointly with various SSL techniques. 
This paper delves into an exploration of such regularization methods, shedding light on their roles and impacts within the broader domain of self-supervised learning for ranking models.

Various studies in the literature have shown the benefit for the use of simple input perturbations in regularizing model's generalization and robustness, which is dubbed in the literature as perturbation-based regularization (see Figure~\ref{fig:perturbation_framework}).
For instance, it has been shown that perturbing inputs with noise, regularizes the models towards more robustness and better generalization capabilities~\cite{pmlr-v139-dhifallah21a,pmlr-v206-orvieto23a, hua2021noise, NIPS2013_38db3aed}.
More concretely, two kinds of input perturbations have been identified to be effective in terms of model's generalization: 1) noise injection~\cite{pmlr-v139-dhifallah21a,pmlr-v206-orvieto23a, hua2021noise}, and 2) feature dropout~\cite{tamkin2022feature,NIPS2013_38db3aed,JMLR:v15:srivastava14a}.
Such regularizations have been proven to play an important role in preventing learning suppression, for instance, via leveraging techniques such as Self-Consistency Regularization e.g, in \cite{sinha2021consistency}, which evidently reports the distance between semantically similar points has undergone a significant reduction, showcasing the substantial impact of this regularization technique which leads to its popularity in the literature~\cite{ko2022self,tan2022hyperspherical,sinha2021consistency,wang2021deep,englesson2021consistency,kim2021selfmatch,kim2022conmatch}.


In this work, we take a broader look at perturbation based regularization approaches for industrial-scale applications in ads ranking, and share our findings on achieving better generalization via self-supervised learning, applicability for industrial usecases, and integration into complex industrial systems. 
More concretely, we present the first instance of its kind for a successful integration of perturbation based regularization into industrial-scale recommendation systems.
Additionally, we present \textbf{L}oss-Balanced  \textbf{S}mall- \textbf{P}erturbation  \textbf{R}egularization (\textbf{LSPR}), a novel perturbation-based regularization method that as we show, can improve the performance of industrial-scale ads ranking systems, while being simpler than its counterparts, hence, assist in scaling.
In summary, the main contributions of our work are as follows:

\noindent \textbf{Regularization Techniques for Ads Ranking at Scale}: 
We share our findings on regularization techniques that are applicable in industrial settings for ads ranking. 
These encompass improvements in offline metrics, and as we report, in several experiments we have obtained 0.1\% - 0.3\% relative Normalized Entropy (NE) offline gains by applying perturbation based regularizations.

\noindent \textbf{Loss-Balanced Small Perturbation Regularization (LSPR)}:
We propose LSPR: instead of adding an additional auxiliary loss function (e.g, often an MSE term) to alleviate the difference in predictions (e.g, as in Self-Consistency Regularization (SCR) ), we create new samples by perturbing datapoints with noise that are scaled by a small weight, and include them in the training data, but additionally weight them down in the the loss term calculation (see Figure~\ref{fig:lspr_diagram}).
Our numerical analysis (see Section~\ref{subsec:numerical_analysis}) shows LSPR achieves a better alignment with the optimal model parameters, and achieves lower errors in the model's weight space, compared to SCR.
Furthermore, we empirically verified (see Section~\ref{subsec:real_data}) that this technique performs better in large-scale industrial systems.
By applying this technique to several prediction models, we were able to achieve a 0.1\%-0.2\% relative NE gain.
We have additionally evaluated our approach in a set of online experiments, and have observed these offline performance improvements are also reflected in the online experimentation, which highlights our technique's effectiveness in online scenarios.

\noindent \textbf{Integration of Perturbation Based Regularization to Complex Industrial Systems}: 
To the best of our knowledge, this work on perturbation based regularization has been the first of its kind, to be integrated in industrial-scale recommendation systems for computing click-through/conversion rate prediction.
The process of incorporating data augmentation and self-supervised learning into complex architectures in large-scale industrial ads ranking and recommendation comes with its own set of challenges. 
Therefore, we provide system descriptions for large scale recommendation system, and how to navigate through its challenges, to adopt perturbation based regularization techniques optimally.
We further offer comprehensive design descriptions that encompass the data augmentation strategies and regularization algorithms we have experimented with, and  present the results we have achieved through these integrations (see Sections~\ref{subsec:numerical_setup} and \ref{sec:modeling}).

The remainder of paper is as follows. In Section~\ref{sec:related_work} we provide a literature review of the related topics.
The preliminaries are provided in Section~\ref{subsec:numerical_setup}.
We detail our modeling in Section~\ref{sec:modeling}.
Section~\ref{sec:experiments} will describe our experiment setup for numerical analysis and real data, and present their results. Finally, Section~\ref{sec:conclusion_related} will conclude the paper and provide insights on our future directions.
