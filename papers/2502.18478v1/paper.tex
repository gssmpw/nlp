\documentclass[]{fairmeta}

\usepackage{graphicx}
\usepackage{textcomp}
\usepackage{xcolor}
    
\usepackage{url}
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{subcaption}
\newcommand{\newmaterial}[1]{{#1}}

\include{math_commands}

\title{Beyond Self-Consistency: Loss-Balanced Perturbation-Based Regularization Improves Industrial-Scale Ads Ranking}

\author[1]{Ilqar Ramazanli\textsuperscript{*}, Hamid Eghbalzadeh\textsuperscript{*}, Xiaoyi Liu, Yang Wang, Jiaxiang Fu, Kaushik Rangadurai, Sem Park, Bo Long, Xue Feng}

\affiliation[1]{AI at Meta}
\contribution[*]{Equal Contribution}

\abstract{Perturbation-based regularization techniques address many challenges in industrial-scale large models, particularly with sparse labels, and emphasize consistency and invariance for perturbation in model predictions.
One of the popular regularization techniques has been various forms of self-consistency, which involve making small modifications to input data while preserving contextual information and enforcing similar predictions through auxiliary loss functions.
In this work, we explore the first successful application of perturbation-based regularization algorithms in large-scale ads ranking models, and further propose a novel regularization algorithm, namely, Loss-Balanced Small Perturbation Regularization (LSPR) that can be used in potentially any deep learning model.
We have successfully demonstrate that both Self-Consistency Regularization approaches (SCR) and LSPR are scalable and can improve ads delivery systems. 
By conducting industrial-scale experiments, and numerical analysis, we additionally show that our proposed LSPR, performs consistently better compared to SCR, across various groups and signal availability setups.
Finally, we report a successful application of the proposed LSPR in a billion-scale industrial ranking system, which to the best of our knowledge, is the first of its kind, and it is specially designed to address the various scalability challenges (e.g, various surfaces, geological locations, clients and so on) as we will mention in this paper. }

\date{2024-12-12}
\correspondence{Xue Feng at \email{xfeng@meta.com}}


\begin{document}

\maketitle

\section{Introduction} 
\label{sec:intro}
\input{sections/intro}

\section{Related Work}
\label{sec:related_work}
\input{sections/related_work}


\section{Preliminaries}
\label{subsec:numerical_setup}
\input{sections/preliminaries}

\section{Methodology}
\label{sec:modeling}
\input{sections/methodology}

\section{Analysis and Experimentation }
\label{sec:experiments}
\input{sections/analysis_experiments}


\section{Conclusion and Future Work}
\label{sec:conclusion_related}

Our study has explored the application of perturbation based regularization algorithms in an Industrial-Scale Recommendation Systems. 
To this end, we have made significant contributions: firstly, to the best of our knowledge, we showed for the first time that Perturbation Based Regularization techniques can bring meaningful improvements to Industrial-Scale Recommendation Systems.
Secondly, we introduced a novel regularization technique - LSPR,a general method that is applicable in many Deep Learning setups.
\newmaterial{In summary, LSPR has been launched to major industrial-scale ads recommendation models across different ranking stages and traffic. This indicates that it can be generalized to diverse user demographics and content types, considering the scale and reach of the deployed ads platform.}
Our future research endeavors are poised to focus on other variations of the use of unlabeled data, tailored for Large Scale Recommendation Systems, pushing on both theoretical understanding, as well as industrial-scalability.
These next steps represent our commitment to pushing the boundaries of recommendation systems, with a keen focus on understanding and optimizing ad recommendations to better serve both users and businesses.



\clearpage
\newpage
\bibliographystyle{assets/plainnat}
\bibliography{refs}


\end{document}