\name \ is a simple modular framework targeting the diversity and disparity of material tasks. The predominant pre-train then fine-tune strategy can only leverage a limited range of interrelated source tasks or indiscriminately consolidate conflicting knowledge into one model, resulting in suboptimal downstream performance. In contrast, the modular design of \name \ allows for the flexible and scalable integration of diverse material knowledge modules, and the effective and tailored adaptation to material property prediction tasks. \cref{fig:method-overview} illustrates this comparison.

\begin{figure}[!t]
    \centering
    \includegraphics[width=\linewidth]{fig/2_pretrain_modular.pdf}
    \caption{A comparison between the pre-train fine-tune paradigm and \name's modular framework. (left): The prevailing scheme involves pre-training on force field data (with supervised prediction on energy, force, and stress), and then transfer to downstream tasks. (right): The modular learning scheme in \name \ trains and stores a broad spectrum of material tasks as modules, and adaptively composes them given a new material property prediction task.}
    \label{fig:method-overview}
\end{figure}

\subsection{Overview}
\name \ involves two major stages: (1) training and centralizing modules into \name \ Hub; (2) adaptively composing these modules to support downstream material tasks.

In the first stage (\cref{sec:DMT}), we encompass a wide range of material properties and systems into \name \ Hub. This accommodates the diversity of material tasks and addresses the task disparity by training specialized modules for each.

In the second stage (\cref{sec:AMC}), we devise the Adaptive Module Composition algorithm. Given the downstream material task, the algorithm heuristically optimizes the optimal combination of module weights for \name \ Hub and composes a customized module based on the weights, which is subsequently fine-tuned on the task for better adaptation. Respecting the diverse and disparate nature of material tasks, our adaptive approach automatically discovers synergistic modules and excludes conflicting combinations by the data-driven assignment of module weights.

A visual overview of \name \ is provided in \Cref{fig:main}.


\begin{figure*}[h]
  \centering
  \includegraphics[width=\linewidth]{fig/3_main.pdf}
  \caption{The \name \ framework.
  (a) During the Module Training \& Centralization stage (\cref{sec:DMT}), \name \ trains full and adapter modules for a wide spectrum of material tasks, constituting the \name \ Hub;
  (b) The Adaptive Module Composition (AMC) \& Fine-tuning stage (\cref{sec:AMC}) leverages the modules in \name \ Hub to compose a tailored module for each downstream task. The AMC algorithm comprises three steps: 1. module prediction estimation (with $k$NN); 2. module weight optimization; 3. module composition. The composed module is further fine-tuned on the task for better adaptation.}
  \label{fig:main}
\end{figure*}

\subsection{Module Training \& Centralization}
\label{sec:DMT}
To better exploit the transferrable knowledge of open-source material property prediction datasets, we first train distinctive modules for each high-resource material task, and subsequently centralize these modules to constitute \name \ Hub.

\paragraph{Module Training}
Leveraging the power of state-of-the-art material property prediction models, we choose to employ a pre-trained backbone encoder $f$ as the initialization for training each \name \ module.
Note that \name \ is independent of the backbone model choice, which enables smooth integration with other pre-trained backbones.

We provide two parametrizations for the \name \ modules: the full module and the adapter module.
For the full module, we directly treat each fully fine-tuned backbone as a module.
The adapter module serves as a parameter-efficient variant where adapter layers~\cite{houlsby2019parameter} are inserted between each layer of the pre-trained backbone. The adapters are updated and the rest of the backbone is frozen. All of the adapters for each task are treated as one module. This implementation trade-offs the downstream performance in favor of a significantly lower GPU memory cost during training, which is particularly beneficial when the computational resources are constrained. When the training converges, we store the module parameters into a centralized repository $\mathcal{H}$ termed \name \ Hub, formally:
\begin{equation*}
\mathcal{H} = \{g_1, g_2, \dots, g_{N}\}, \quad g_i = \begin{cases}
\theta_f^{i} & \text{(full module)} \\
\Delta_f^{i} & \text{(adapter module)}
\end{cases}
\label{eq:hub}
\end{equation*}
where $\theta_f^{i}$ and $\Delta_f^{i}$ denote the full and adapter module parameters related to the $i^{\text{th}}$ task and encoder $f$.

\paragraph{Module Centralization}
To support a wide array of downstream tasks, \name \ Hub needs to include modules trained on diverse material systems and properties. Currently, \name \ Hub encompasses 18 material property prediction tasks selected from the Matminer datasets~\citep{ward2018matminer} with over 10000 data points. These tasks span across a large range of material properties, including thermal properties (e.g. formation energy), electronic properties (e.g. band gap), mechanical properties (e.g. shear modulus), etc. For more details, please refer to \cref{appendix:data}.
To showcase the effect of scaling data diversity, we present the continual learning results in \cref{exp:cont} after further incorporating molecular property prediction tasks into \name \ Hub. Note that \name \ is designed to be task-agnostic and may readily support a larger spectrum of tasks in the future. 

An important benefit of the modular design of \name \ Hub is that it preserves proprietary data, which is prevalent in the field of materials, enabling privacy-aware contribution of new modules. Therefore, \name \ could serve as an open platform for the modularization of materials knowledge, which also facilitates downstream adaptation through a novel composition mechanism, as discussed in the following section.


\subsection{Adaptive Module Composition \& Fine-tuning}
\label{sec:AMC}
Given a labeled material property prediction dataset $\mathcal{D}$ with $m$ instances: $\mathcal{D}=\{(x_1,y_1), (x_2,y_2), \ldots, (x_m,y_m) \}$, the second stage of \name \ customizes a task-specific model using the modules in \name \ Hub.

To achieve this, we devise the Adaptive Module Composition (AMC) algorithm. We highlight its key desiderata:
\begin{itemize}[leftmargin=*]
\item \textbf{Selective:} Material tasks are inherently disparate. Hence only the most relevant modules shall be selected to avoid the negative interference of materials knowledge and encourage positive transfer to downstream tasks.
\item \textbf{Data-driven:} As the diversity of tasks in \name \ Hub expands, it is impossible to rely solely on human expertise for module selection. A data-driven approach is required to mine the implicit relationships between the \name \ Hub modules and downstream tasks.
\item \textbf{Efficient:} Enumerating all combinations of modules is impractical. Efficient algorithms shall be developed to return the optimal module composition using a reasonable amount of computational resources.
\end{itemize}

To meet these requirements, AMC is designed as a fast heuristic algorithm that first estimates the prediction of each module on the downstream task, then optimizes the module weights, and finally composes the selected modules to form the task-specific module. We now elaborate on the details of AMC, with its formal formulation in \cref{alg:AMC}.

\paragraph{Module Prediction Estimation}
We begin by estimating the predictive performance of each module in \name \ Hub $\mathcal{H}$ on the downstream task $\mathcal{D}$. More accurate predictions indicate stronger relevance to the task and intuitively warrant higher weights in the composition.

For each module $g_j$ in $\mathcal{H}$, we first take it to encode each input materials in the train set of task $\mathcal{D}$ into a set of representation $\mathcal{X}^j = \{\mathbf{x}_1^j, \mathbf{x}_2^j, \ldots, \mathbf{x}_m^j\}$ in which $\mathbf{x}_i^j = g_j({x}_i)$. Then we obtain the estimated prediction of $g_j$ on $\mathcal{D}$ using a leave-one-out label propagation approach~\cite{iscen2019label}. Specifically, we iteratively select one sample $\mathbf{x}_i^j$ from $\mathcal{X}^j$ and get the predicted label $\hat{y_i}^j$ by calculating the weighted sum of its $K$ nearest neighbors' labels within $\mathcal{X}^j$:
\begin{equation}
    \label{eq:knn}
    \hat{y_i}^j = \sum_{k=1}^{K} \frac{f_d(\mathbf{x}_i^j,\mathbf{x}_{k}^j)}{Z} y_{k},
\end{equation}
where $\mathbf{x}_{k}^j$ denotes the $k$-th nearest neighbors of $\mathbf{x}_i^j$. The distance function $f_d$ for calculating $k$NN is the exponential of cosine similarity between each pair of $\mathbf{x}_i^j$ and $\mathbf{x}_k^j$. $Z=\sum_{k=1}^{K} f_d(\mathbf{x}_i^j,\mathbf{x}_{k}^j)$  is the normalizing term.

While other predictors are viable, we choose $k$NN due to its good trade-off in efficiency and accuracy. Also, its training-free nature enhances its flexibility in real-world scenarios, where the downstream data may be subject to updates.

\paragraph{Module Weight Optimization}
After estimating each module's prediction, we now have to select the optimal combination of modules tailored for the downstream task $\mathcal{D}$.
To achieve this, the most straightforward approach is to compare the prediction error obtained after fine-tuning each combination of modules. However, this is infeasible due to the combinatorial explosion. Therefore, we reformulate the task as an optimization problem, using the prediction error before fine-tuning as a proxy metric (later referred to as \textit{proxy error}). By optimizing the proxy error, we could obtain the optimal combination of weights.

Specifically, inspired by ensemble learning~\citep{zhou2002ensembling, zhou2016learnware}, we assign a weight $w_j$ for each module $g_j$ and calculate the output of the ensemble: $\sum_{j=1}^{NT} w_j\hat{y_i}^j$.
We then estimate the proxy error on the train set of $\mathcal{D}$ for this weighted ensemble:
\begin{equation}
    \label{eq:proxy}
    E_\mathcal{D} =\frac{1}{m}\sum_{i=1}^{m}(\sum_{j=1}^{N} w_j \hat{y}_i^j - y_i)^2
\end{equation}

To minimize the proxy error $E_\mathcal{D}$, we then utilize the open source cvxpy package~\citep{cvxpy} to optimize the module weights. The objective is:
\begin{equation}
\label{eq:opt}
\underset{w_j}{\operatorname{argmin}}\ E_\mathcal{D}, \ \text { s.t. }  \sum_{j=1}^N w_j = 1, \ w_j \geq 0
\end{equation}

\paragraph{Module Composition}
After the optimization converges, we can use the learned weights to compose a single customized module for the specific task. 

Inspired by the recent success of model merging in NLP and CV~\cite{wortsman2022model, ilharco2022editing, yu2024language, li2024training, yang2024model}, we adopt a simple yet surprisingly effective method by weighted averaging the parameters of the selected modules: 
\begin{equation}
    g_\mathcal{D} = \sum_{j=1}^N w_j^{*} g_j,
\end{equation}
 where $w_j^{*}$ represents the optimized weight for the $j$-th module in \cref{eq:opt}. Here, the weights underscore the relevance of each selected module to the downstream task. 

\input{0_main_table}

While alternative composition methods, such as mixture-of-experts, are feasible, they incur high memory overhead as \name \ Hub expands, limiting their practical deployment under computational constraints. By contrast, our weighted-average composition uses fewer resources while effectively integrating knowledge from all modules. In the full-module setting, every module shares the same architecture and pre-trained backbone with identical initializations, providing a grounded foundation for successful knowledge composition~\cite{zhouemergence2024}.

\paragraph{Downstream Fine-tuning}
\label{sec:DA}
To better adapt to the downstream task $\mathcal{D}$, the composed module $g_\mathcal{D}$ is appended with a task-specific head and then fine-tuned on $\mathcal{D}$ to convergence.