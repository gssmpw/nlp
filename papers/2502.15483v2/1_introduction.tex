Accurate and efficient material property prediction is critical for accelerating materials discovery. Key properties such as formation energy and band gap are fundamental in identifying stable materials and functional semiconductors~\cite{riebesell2023matbench, masood2023enhancing}. While traditional approaches such as density functional theory (DFT) offer high precision~\cite{jain2016computational}, their prohibitive computational cost limits their practicality for large-scale screening~\cite{fiedler2022deep, lan2023adsorbml}.

Recently, deep learning methods have been developed to expedite traditional approaches~\cite{xie2018crystal, griesemer2023accelerating}. Pre-trained force field models, in particular, have shown remarkable success in generalizing to a wide spectrum of material property prediction tasks~\cite{yang2024mattersim, barroso2024open, shoghi2023molecules}, outperforming specialized models trained from scratch. These models are typically pre-trained on the potential energy surface (PES) data of materials and then fine-tuned for the target downstream task.

\begin{figure}[!t]
    \centering
    \includegraphics[width=\linewidth]{fig/1_diverse_disparate.pdf}
    \caption{Illustration of the diversity of material properties (top) and systems (bottom). Note that material tasks are also disparate, with different laws governing the diverse properties and systems. These characteristics pose challenges for pre-training material property prediction models.}
    \label{fig:intro}
\end{figure}

Despite these advances, we identify two key challenges that undermine the effectiveness of current pre-training strategies for material property prediction: \textbf{diversity} and \textbf{disparity}.

First, material tasks exhibit significant diversity (\cref{fig:intro}), which current pre-trained models fail to adequately cover. Existing models trained on PES-derived properties (e.g., force, energy, and stress) mostly focus on crystalline materials~\cite{yang2024mattersim, barroso2024open}. However, material tasks span a wide variety of systems (e.g., crystals, organic molecules) and properties (e.g., thermal stability, electronic behavior, mechanical strength), making it difficult for methods trained on a limited set of data to generalize across the full spectrum of tasks.

Second, the disparate nature of material tasks presents huge obstacles for jointly pre-training a broad span of tasks. Material systems vary significantly in terms of bonding, atomic composition, and structural periodicity, while their properties are governed by distinct physical laws. For example, mechanical strength in metals is primarily influenced by atomic bonding and crystal structure, whereas electronic properties like conductivity are determined by the material’s electronic structure and quantum mechanics. Consequently, training a single model across a wide range of tasks~\cite{shoghi2023molecules} may lead to knowledge conflicts, hindering the model’s ability to effectively adapt to downstream scenarios.

In this paper, we propose \name, a \textbf{Mo}dular deep learning framework for \textbf{Ma}terial property prediction, to address the diversity and disparity challenge. To accommodate the \textbf{diversity} of material tasks, \name \ first trains on a multitude of high-resource property prediction datasets, centralizing them into transferrable modules. Furthermore, \name \ incorporates an adaptive composition algorithm that customizes support for diverse downstream scenarios.
Recognizing the \textbf{disparity} among material tasks, \name \ encapsulates each task within a specialized module, eliminating task interference of joint training. In adapting \name \ to specific downstream tasks, its composition strategy adaptively integrates only the most synergistic modules, mitigating knowledge conflicts and promoting positive transfer.

Specifically, \name \ comprises two major stages: (1) \textit{Module Training \& Centralization}. Drawing inspiration from modular deep learning~\cite{pfeiffer2023modular}, \name \ trains dedicated modules for a broad range of material tasks, offering two versions: a full module for superior performance and a memory-efficient adapter module. These trained modules are centralized in \name \ Hub, a repository designed to facilitate knowledge reuse while preserving proprietary data for privacy-aware material learning. (2) \textit{Adaptive Module Composition} (AMC). \name \ introduces the data-driven AMC algorithm that composes synergetic modules from \name \ Hub. AMC first estimates the performance of each module on the target task in a training-free manner, then heuristically optimizes their weighted combination. The resulting composed module is then fine-tuned for improved adaptation to the downstream task. Together, the two stages deliver a modular solution that enables \name \ to account for the diversity and disparity of material knowledge.

Empirical results across 17 downstream tasks showcase the superiority of \name, outperforming all baselines in \textbf{16/17} tasks, with an average improvement of \textbf{14\%} compared to the second-best baseline. In \textbf{few-shot} settings, which are common in materials science,  \name \ achieves even larger performance gains to the conventional pre-train then fine-tune paradigm. Additionally, we show that \name \ can expand its capability in \textbf{continual learning} settings by incorporating molecular tasks into \name \ Hub. The trained modules in \name \ Hub will be open-sourced, and we envision \name \ becoming a pivotal platform for the modularization and distribution of materials knowledge, fostering deeper community engagement to accelerate materials discovery.