\section{Algorithm for Adaptive Module Composition}

The formal description of the Adaptive Module Composition algorithm is included in \cref{alg:AMC}.

\begin{algorithm}
\caption{Adaptive Module Composition}
\label{alg:AMC}
\begin{algorithmic}[1]
\STATE \textbf{Input:} \name \ Hub $\mathcal{H} = \{g_1, g_2, \dots, g_{N}\}$, Downstream Task $\mathcal{D}$.
\STATE \textbf{Output:} adaptive module $g_{\mathcal{D}}$ for $\mathcal{D}$.
\FOR{each module $g_j \in \mathcal{H}$}
    \STATE Encode the input materials in the training set of $\mathcal{D}$ using $g_j$ to obtain $\mathcal{X}^j = \{\mathbf{x}_1^j, \mathbf{x}_2^j, \ldots, \mathbf{x}_m^j\}$.
    \FOR{each sample $\mathbf{x}_i^j \in \mathcal{X}^j$}
        \STATE Compute the predicted label $\hat{y_i}^j$ for $\mathbf{x}_i^j$ using $k$NN following \cref{eq:knn}.
    \ENDFOR
\ENDFOR
\STATE Optimize the module weights $w_j$ using cvxpy to minimize the proxy error defined in \cref{eq:proxy}, subject to: \\ $\sum_{j=1}^{N} w_j = 1$ and $w_j \geq 0$. Denote the optimized weights for the $j$-th module as $w_j^{*}$.
\STATE Compose the final adaptive module $g_\mathcal{D}$ by weighted averaging the parameters of the \name \ Hub modules: \\
    $g_\mathcal{D} = \sum_{j=1}^N w_j^{*} g_j$
\STATE \textbf{Return:} The composed module $g_{\mathcal{D}}$.
\end{algorithmic}
\end{algorithm}



\section{Experimental Details}
In this section, we provide more experimental details of \name \ regarding the datasets, implementation, baselines, and the continual learning setting.

\subsection{Dataset Details}

\input{0_dataset_table}

\label{appendix:data}
We primarily adopt the dataset setup proposed by \citet{chang2022towards}. Specifically, we select 35 datasets from Matminer \citep{ward2018matminer} for our study, categorizing them into 18 high-resource material tasks, with sample sizes ranging from 10,000 to 132,000 (an average of 35,000 samples), and 17 low-data tasks, with sample sizes ranging from 522 to 8,043 (an average of 2,111 samples).

The high-resource tasks are utilized for training the \name \ Hub modules, as their larger data volumes are likely to encompass a wealth of transferrable material knowledge. A detailed introduction of these \name \ Hub datasets is included in \cref{tab:high-data}.

The low-data tasks serve as downstream datasets to evaluate the effectiveness of \name \ and its baselines. This setup mimics real-world materials discovery scenarios, where downstream data are often scarce. To ensure robust and reliable comparison results, we exclude two downstream datasets with exceptionally small data sizes (fewer than 20 testing samples) from our experiments, as their limited data could lead to unreliable conclusions. A detailed introduction is included in \cref{tab:low-data}.

Following \citet{chang2022towards}, all datasets are split into training, validation, and test sets with a ratio of 7:1.5:1.5. For the downstream low-data tasks, the splitting is performed randomly for 5 times to ensure the stability of evaluation.


\subsection{Implementation Details of \name}
\label{appendix:imp}

\paragraph{Network Architecture}
We now introduce the network architecture of \name \ modules. The JMP~\citep{shoghi2023molecules} backbone is directly taken as the full module parametrization. JMP is pre-trained on $\sim$ 120 million DFT-generated force-field data across large-scale datasets on catalyst and small molecules. JMP is a 6-layer GNN model with around 160M parameters which is based on the GemNet-OC architecture~\citep{gasteiger2022gemnet}.
Note that \name \ is backbone-agnostic. JMP is selected due to its comprehensive strength across a wide range of molecular and crystal tasks, which allows us to seamlessly conduct the continual learning experiments.
We leave the extrapolation of \name \ to other architectures as future work.

For the adapter module, we follow the standard implementation of adapter layers~\citep{houlsby2019parameter}.
Specifically, we insert adapter layers between each layer of the JMP backbone. Each layer consists of a downward projection to a bottleneck dimension and an upward projection back to the original dimension. 

\paragraph{Hyper-parameters}
For the training of JMP backbone, we mainly follow the hyper-parameter configurations in \citet{shoghi2023molecules}, with slight modifications to the learning rate and batch size.
During the module training stage of \name, we use a batch size of 64 and a learning rate of 5e-4 for 80 epochs.
During downstream fine-tuning, we adopt a batch size of 32 and a learning rate of 8e-5. We set the training epoch as 60, with an early stopping patience of 10 epochs to prevent over-fitting.
We adopt mean pooling of embedding for all properties since it performs significantly better than sum pooling in certain tasks (e.g. band gap prediction), which is consistent with findings in \citet{shoghi2023molecules}.

For the adapter modules, we employ BERT-style initialization~\citep{devlin2018bert}, with the bottleneck dimension set to half of the input embedding dimension.

For the Adaptive Module Composition (AMC) algorithm, we set the number of nearest neighbors ($K$ in \cref{eq:knn}) to 5. For the optimization problem formulated in \cref{eq:opt}, we utilize the CPLEX optimizer from the cvxpy package~\citep{cvxpy}.
AMC is applied separately for each random split of the downstream tasks to avoid data leakage.

\paragraph{Computational Cost}
Experiments are conducted on NVIDIA A100 80 GB GPUs. During the module training stage, training time ranges from 30 to 300 GPU hours, depending on the dataset size.
While this training process is computationally expensive, it is a one-time investment, as the trained models are stored in \name \ Hub as reusable material knowledge modules.
Downstream fine-tuning requires significantly less compute, ranging from 2 to 8 GPU hours based on the dataset scale. The full module and adapter module require similar training time; however, the adapter module greatly reduces memory consumption during training.


\subsection{Baseline Details}
\label{appendix:baselines}
The CGCNN baseline refers to fine-tuning the CGCNN model~\citep{xie2018crystal} separately on 17 downstream tasks. Conversely, MoE-(18) involves training individual CGCNN models for each dataset in \name \ Hub and subsequently integrating these models using mixture-of-experts~\citep{jacobs1991adaptive,shazeer2016outrageously}.
For the baseline results of CGCNN and MoE-(18), we adopt the open-source codebase provided by \citet{chang2022towards} and follow the exactly same parameters as reported in their papers for the result duplication.

For JMP-FT, we use the JMP (large) checkpoint from the codebase open-sourced by \citet{shoghi2023molecules} and fine-tune it directly on the downstream tasks with a batch size of 64. JMP-MT adopts a multi-task pre-training strategy, training on all 18 \name \ Hub source tasks without addressing the conflicts between disparate material tasks. Starting from the same pre-trained checkpoint as JMP-FT, JMP-MT employs proportional task sampling and trains for 5 epochs across all tasks with a batch size of 16. The convergence of multi-task pre-training is indicated by a lack of further decrease in validation error on most tasks after 5 epochs. For downstream fine-tuning, both JMP-FT and JMP-MT adopt the same training scheme as the fine-tuning stage in \name.


\subsection{Details on Continual Learning Experiments}
\label{appendix:cl}
The QM9 dataset~\citep{ramakrishnan2014quantum} comprises 12 quantum chemical properties (including geometric, electronic, energetic, and thermodynamic properties) for 134,000 stable small organic molecules composed of CHONF atoms, drawn from the GDB-17 database~\citep{ruddigkeit2012enumeration}. It is widely served as a comprehensive benchmarking dataset for prediction methods of the structure-property relationships in small organic molecules.

In the continual learning experiments, we expand the \name \ hub by including modules trained on the QM9 dataset. For module training, we adopt the same training scheme as the original \name \ modules, with the exception of using sum pooling instead of mean pooling, as it has been empirically shown to perform better \citep{shoghi2023molecules}.

\section{More Experimental Results}
We report the complete few-shot learning results in \cref{tab:few-shot}.

\begin{table}[!t]
\centering
\caption{Test set MAE and average test loss of JMP-FT and \name \ under the full-data, 100-data, and 10-data settings. Results are averaged over five random data splits on one random seed. Results are preserved to the third significant digit.}
\resizebox{0.9\textwidth}{!}{
\begin{tabular}{lccccccccc}
\toprule[1pt]
\textbf{Datasets}                             & \textbf{JMP-FT} & \textbf{\name} & \textbf{JMP-FT (100)} & \textbf{\name\ (100)} & \textbf{JMP-FT (10)} & \textbf{\name\ (10)} \\
\midrule
Experimental Band Gap        & 0.380    & 0.305    & 0.660   & 0.469   & 1.12  & 1.245  \\
Formation Enthalpy           & 0.156    & 0.0821   & 0.273   & 0.101  & 0.514 & 0.143  \\
2D Dielectric Constant       & 2.45     & 1.90     & 3.19    & 2.35   & 7.74  & 3.31  \\
2D Formation Energy          & 0.135    & 0.0470    & 0.366   & 0.113    & 0.842 & 0.214  \\
2D Exfoliation Energy        & 38.9     & 36.1      & 54.4    & 56.1    & 118 & 87.3  \\
2D Band Gap                  & 0.611    & 0.366    & 0.890   & 0.517    & 1.23  & 1.05  \\
3D Poly Electronic           & 23.7     & 23.0     & 33.6    & 24.8    & 54.0  & 48.9  \\
3D Band Gap                  & 0.249    & 0.201     & 1.71    & 0.686   & 2.10  & 1.47  \\
Dielectric Constant          & 0.0552   & 0.0535    & 0.134   & 0.102   & 0.289 & 0.231  \\
Elastic Anisotropy           & 2.11     & 2.85     & 4.85    & 3.79     & 4.02  & 5.26  \\
Electronic Dielectric Constant & 0.108  & 0.0903    & 0.260   & 0.178    & 0.568 & 0.500  \\
Total Dielectric Constant    & 0.172    & 0.155    & 0.361   & 0.287    & 0.543 & 0.527  \\
Phonons Mode Peak            & 0.0710   & 0.0521    & 0.221   & 0.199    & 0.493 & 0.485  \\
Poisson Ratio                & 0.0221   & 0.0203    & 0.0345  & 0.0317   & 0.0466 & 0.057  \\
Poly Electronic              & 2.10     & 2.13     & 3.24    & 2.88     & 6.08  & 5.10  \\
Total Poly                   & 4.83     & 4.76      & 6.54    & 6.32     & 11.2  & 10.1  \\
Piezoelectric Modulus        & 0.169    & 0.175     & 0.248   & 0.258   & 0.303 & 0.290  \\
\midrule
\textbf{Average Test Loss}    & 0.222        & 0.187       & 0.408  & 0.299       & 0.700       & 0.550 \\
\bottomrule[1pt]
\end{tabular}
\label{tab:few-shot}
}
\end{table}