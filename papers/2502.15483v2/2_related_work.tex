\subsection{Material Property Prediction with Deep Learning}
Deep learning methods have been widely applied for predicting material properties~\citep{de2021materials}.
In a seminal work by~\citet{xie2018crystal}, CGCNN is designed to model crystalline materials with multi-edge graphs and leverage graph neural networks to learn crystal representations.
Since then, a series of research~\cite{choudhary2021atomistic,yan2022periodic,das2023crysmmnet, lin2023efficient,yan2024complete,taniai2024crystalformer} have focused on improving neural network architectures to better model the inductive biases of crystals for property prediction tasks. 

Another line of work develops pre-training strategies to facilitate material property prediction~\cite{jha2019enhancing, magar2022crystal, zhang2023dpa, wang2024comprehensive,song2024diffusion}.
Recently, a series of pre-trained force field models~\cite{merchant2023scaling, batatia2023foundation, yang2024mattersim, neumann2024orb, barroso2024open} achieve remarkable accuracy in the stability prediction task of inorganic solid-state materials and show initial results in generalizing to a broader range of material properties.
We highlight the JMP model proposed by~\citet{shoghi2023molecules}, which is trained on force and energy prediction tasks across multiple domains (small molecules, catalysts, etc.) and performs impressively when fine-tuned to downstream tasks of both molecules and crystals.

Extending beyond the prevailing pre-train and fine-tune paradigm, \name \ devises effective strategies to centralize material knowledge into modules and adaptively compose the modules to achieve superior downstream performance.


\subsection{Modular Deep Learning}
Modular deep learning~\citep{pfeiffer2023modular} represents a promising paradigm in deep learning, where parameterized modules are composed, selected, and aggregated during the network training process.
Different from the vanilla pre-train and fine-tune approach, modular methods employ composable network architectures that enable more tailored adaptations to different tasks and domains. Notable examples of modular networks include mixture-of-experts~\citep{jacobs1991adaptive, shazeer2016outrageously}, adapters~\citep{houlsby2019parameter} and LoRA~\citep{hu2021lora}.
Recently, we have seen an increasing number of successful applications of modular deep learning across domains such as NLP and CV~\citep{puigcerver2020scalable, pfeiffer2020adapterhub, huang2023lorahub, zhang2023composing,tan2024neuron,pham2024mixturegrowth}, where its strengths in flexibility and minimizing negative interference have been demonstrated.

In the field of material property prediction, the idea of modular deep learning is still under-explored. A work most similar to \name \ is proposed by~\citet{chang2022towards}. Their framework, termed MoE-(18), integrates 18 models trained on various source tasks with mixture of experts. \name \ distinguishes itself from MoE-(18) in two key aspects:
(1) MoE-(18) loads all pre-trained models indiscriminately for each downstream task, whereas \name \ adaptively composes a subset of relevant modules to mitigate knowledge conflicts and encourage positive transfer.
(2) MoE-(18) is designed to address the data scarcity issue and is limited to the mixture-of-experts approach, while \name \ introduces modularity to target the inherent challenges in materials science and is not restricted to any specific modular method.
Hence, \name \ marks the first systematic effort to devise a modular deep learning framework for materials.