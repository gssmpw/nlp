In this section, we conduct comprehensive experiments to demonstrate the empirical effectiveness of \name. The experimental setup is outlined in \cref{exp:setup}. The main results, discussed in \cref{exp:main}, show that \name \ \textbf{substantially outperforms} baseline methods. Additionally, we conduct a thorough ablation study on the AMC algorithm as detailed in \cref{exp:ablation}.
Confronted with the data scarcity challenge common in real-world materials discovery settings, we evaluate \name’s few-shot learning ability in \cref{exp:few-shot}, where it achieves \textbf{even larger} performance gains compared to baselines.
To further highlight the \textbf{flexibility and scalability} of \name, we extend \name \ Hub to include molecular datasets and present the continual learning results in \cref{exp:cont}. Finally, we visualize the module weights optimized by AMC in \cref{exp:interpret}, highlighting \name’s potential for providing \textbf{valuable insights} into material properties.


\subsection{Setup}
\label{exp:setup}
\paragraph{Datasets}
To evaluate \name \ on material property prediction tasks, we conduct experiments on 17 tasks adhering to the benchmark settings established by \citet{chang2022towards}. Refer to \cref{appendix:data} for more details.

\paragraph{Implementation details}
For the pre-trained backbone of \name, we choose to employ the open-source JMP model~\citep{shoghi2023molecules} for representing material systems given its superior performance in property prediction tasks across both crystals and molecules.
For the evaluation metric, we report the average mean absolute errors (MAE) across five random data splits to enhance the robustness of the results.
Additional implementation details, including the network architecture, the hyper-parameters for \name, and the computational cost, are provided in \cref{appendix:imp}.
 
\paragraph{Baseline methods}
We compare the performance of MoMa with four baseline methods: CGCNN \citep{xie2018crystal}, MoE-(18) \citep{chang2022towards}, JMP-FT \citep{shoghi2023molecules}, and JMP-MT \citep{sanyal2018mt}. CGCNN represents a classical method without pre-training. MoE-(18) trains separate CGCNN models for the upstream tasks of \name, then ensembles them as one model in a mixture-of-experts approach for downstream fine-tuning. JMP-FT directly fine-tunes the JMP pre-trained checkpoint on the downstream tasks. JMP-MT trains all tasks in \name \ with a multi-task pretraining scheme and then adapts to each downstream dataset with further fine-tuning.
More discussions on baselines are included in \cref{appendix:baselines}.


\subsection{Main Results}
\label{exp:main}
\paragraph{Performance of \name}
As shown in \cref{tab:main}, \name \ (Full) achieves the best performance with the lowest average rank of 1.35 and 14/17 best results. The adapter variant of \name \ follows, with an average rank of 2.47. Together, the two variants hold 16 out of 17 best results. They also exhibit the smallest rank deviations, indicating that \name \ consistently delivers reliable performance across tasks. Notably, \name \ (Full) outperforms JMP-FT in 14 tasks, with an impressive average improvement of 14.0\%, highlighting the effectiveness of \name \ Hub modules in fostering material property prediction tasks. 
Moreover, \name \ (Full) surpasses JMP-MT in 16 out of 17 tasks with a substantial average margin of 24.8\%, underscoring the advantage of \name \ in discovering synergistic knowledge modules.

\paragraph{Performance of baselines}
Among the baseline methods, JMP-FT performs the best with an average rank of 2.88, followed by JMP-MT with an average rank of 3.94. Though additionally trained on upstream tasks of \name \ Hub, JMP-MT still lags behind JMP-FT. We hypothesize that the inherent knowledge conflicts between the disparate material tasks pose a tremendous risk to the multi-task learning approach. We also observe that methods utilizing the JMP encoder outperform those based on CGCNN encoders. This demonstrates the good transferability of large force field models to material property prediction tasks.


\subsection{Ablation Study of Adaptive Module Composition}
\label{exp:ablation}

\paragraph{Setup}
We conduct a fine-grained ablation study of the Adaptive Module Fusion algorithm. The following ablated variants are tested: (1) \textbf{Select average}, which discards the weights optimized in \cref{eq:opt} and applies arithmetic averaging for the selected modules; (2) \textbf{All average}, which simple averages all modules in \name \ Hub; (3) \textbf{Random selection}, which picks a random set of modules in \name \ Hub with the same module number as AMC. Further analysis experiments are done using the \name's full parametrization, \textit{i.e.}, \name \ (Full), due to its superior performance.

\begin{figure}[!t]
    \centering
    \includegraphics[width=0.95\linewidth]{fig/4_AMC_ablation.pdf}   
    \caption{Ablation study of AMC. The main results using AMC \ ({\color{mypurple}{purple}}) are compared with the ablated variants ({\color{orange}{orange}}) that substitute AMC with select average, all average and random selection.
    The axis represents the MAE on each dataset and \textbf{smaller area is better}. The ablated results are inferior to the main results in 13, 15 and 15 out of 17 tasks.}
    \label{fig:ablation}
\end{figure}

\paragraph{Results}
A visualization of the ablation results is presented in \cref{fig:ablation}. The `Select average', `All average', and `Random selection' approaches perform worse to the main results using AMC in 13, 15, and 15 tasks, with an average increase of test MAE of 11.0\%, 18.0\%, and 20.2\%. These results highlight the effectiveness of both the module selection and weighted composition strategies employed by AMC.


\subsection{Performance in Few-shot Settings}
\label{exp:few-shot}
\paragraph{Motivation \& Setup}
To better assess the performance of \name \ in real-world materials discovery scenarios, where candidates with labeled properties are costly to acquire and often exceptionally scarce~\cite{abed2024open}, we construct a few-shot learning setting and compare the performance of \name \ with JMP-FT, the strongest baseline method. For each downstream task, we randomly down-sample $N$ data points from the train set to construct the few-shot train set, on which we apply the AMC algorithm to compose modules from \name \ Hub. Then we perform downstream adaptation by fine-tuning on the $N$ data points. The validation and test sets remain consistent with those in the standard settings to ensure a robust evaluation of model performance. Experiments are conducted with $N$ set to 100 and 10, representing few-shot and extremely few-shot scenarios.

\paragraph{Results}
The average test losses for the 17 downstream tasks of \name \ compared to JMP-FT across the full-data, 100-data, and 10-data settings are illustrated in \cref{fig:delta}.
As expected, the test loss increases as the data size decreases, and \name \ consistently outperforms JMP-FT in all settings.
Notably, the performance advantage of \name \ is more pronounced in the few-shot settings, with the normalized loss margin widening from 0.03 in the full-data setting to 0.11 and 0.15 in the 100-data and 10-data setting.
This suggests that \name \ may offer even greater performance gains in real-world scenarios, where property labels are often limited, thereby hindering the effective fine-tuning of large pre-trained models.
Complete results are shown in \cref{tab:few-shot}.

\begin{figure}[!th]
    \centering
    \includegraphics[width=0.85\linewidth]{fig/5_fewshot_bars.pdf}
    
    \caption{The average test losses of \name \ and JMP-FT across 17 downstream tasks under varying data availability settings.
    \name \ consistently outperforms JMP-FT in all settings. The loss reduction amplifies as the data size shrinks, highlighting the advantage of \name \ in few-shot settings.
    Results are averaged over five random data splits.}
    \label{fig:delta}
\end{figure}


\subsection{Continual Learning Experiments}
\label{exp:cont}
\paragraph{Motivation \& Setup}
Continual learning refers to the ability of an intelligent system to progressively improve by integrating new knowledge~\cite{wang2024comprehensive}. We investigate this capability of \name \ by incorporating new modules into \name \ Hub. Due to its modular nature, it is expected that \name \ will exhibit enhanced performance in tasks that are closely aligned with the new modules, while maintaining its performance when these additions are less relevant.
We expand \name \ Hub to include the QM9 dataset~\cite{ramakrishnan2014quantum} and evaluate the results across the 17 benchmark material property prediction tasks. For more details on the setup, please refer to \cref{appendix:cl}.

\paragraph{Results}
We present the scatter plot of the reduction rate of test MAE w.r.t. the proxy error decrease in \cref{fig:CL} across datasets where QM9 modules are selected.
We observe that: (1) The integration of QM9 modules leads to an average of 1.7\% decrease in test set MAE; (2) a larger reduction in the AMC-optimized proxy error correlates with greater performance improvements post-fine-tuning (with a Pearson correlation of 0.69). 
We highlight the task of MP Phonons prediction, which marks a significant 11.8\% decrease in test set MAE following the expansion of \name \ Hub.

\begin{figure}[!th]
    \centering
    \includegraphics[width=0.88\linewidth]{fig/6_cl_scatter.pdf}
    \caption{Scatter plot showing the relationship between the test MAE decrease and the proxy error (defined in \cref{eq:opt}) decrease after the addition of QM9 modules. The dashed line represents the average test MAE decrease. The solid line fits the results with linear regression.}
    \label{fig:CL}
\end{figure}


\subsection{Materials Insights Mining}
\label{exp:interpret}

\paragraph{Motivation}
We contend that the AMC weights derived in \cref{eq:opt} can offer interpretability for \name \ as well as provide valuable insights into material properties. To explore this, we interpret the weights as indicators for the relationships between \name \ Hub modules and downstream tasks.
Following \citet{chang2022towards}, we present a log-normalized visualization of these weights in \cref{fig:heatmap}.

\paragraph{Results}
We make several noteworthy observations:
\begin{itemize}[leftmargin=*]
    \item The weights assigned by AMC effectively capture physically intuitive relationships between material properties. For instance, the tasks of experimental band gap (row 1) and experimental formation energy (row 2) assign the highest weights to the computational band gap (columns 2 and 14) and formation energy modules (columns 1, 12, and 15). Also, for the task of predicting electronic dielectric constants, \name \ assigns high weights to the band gap modules, which is reasonable given the inverse relationship between the dielectric constant and the square of the band gap~\cite{ravichandran2016solid}.
    \item Some less-intuitive relationships also emerge. For the task of experimental band gap prediction (row 1), the formation energy module from the Materials Project (column 1) is assigned the second-highest weight. In the prediction of dielectric constant (row 9), modules related to thermoelectric and thermal properties (columns 5 and 6) are non-trivially weighted. However, the first-principles relationship between these tasks is indirect. We hypothesize that in addition to task relevance, other factors such as data distribution and size may also influence the weight assignments for AMC. Further investigation into these results are left to future work.
\end{itemize}

\begin{figure}[!th]
    \centering
    \includegraphics[width=0.95\linewidth]{fig/7_weight_heatmap.pdf}
    \caption{Heat map illustrating the AMC weights on one data split. The x-axis represents the task names of the \name \ Hub modules, while the y-axis shows the 17 material property prediction tasks in \cref{tab:main}. Darker colors indicate higher weights, signifying a stronger correlation between the \name \ module and the downstream tasks.}
    \label{fig:heatmap}
\end{figure}