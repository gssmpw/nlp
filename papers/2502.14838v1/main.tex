
\documentclass{article} % For LaTeX2e
\usepackage{iclr2025_conference,times}
\usepackage{subcaption}
% Optional math commands from https://github.com/goodfeli/dlbook_notation.
\input{math_commands.tex}
\usepackage{wrapfig}
\usepackage{hyperref}
\usepackage{url} 
\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{makecell}
\usepackage{siunitx}
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}
\usepackage{amsmath, amssymb}        % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{xcolor}         % colors
\usepackage{adjustbox}
\usepackage{multirow}
\usepackage{algorithm2e}
\usepackage[shortlabels]{enumitem}
\usepackage[most]{tcolorbox}
\usepackage{fontawesome}
\definecolor{ggreen}{rgb}{0.0, 0.6, 0.0}
\definecolor{rred}{rgb}{0.75, 0.0, 0.0}
\definecolor{bblue}{rgb}{0.13, 0.67, 0.8}
\newcommand{\badmetric}[1]{{\color{rred} \textbf{#1}}}
\newcommand{\goodmetric}[1]{{\color{ggreen} \textbf{#1}}}
\definecolor{BoxBackground}{RGB}{240, 240, 240} % 浅灰色背景
\definecolor{BoxFrame}{RGB}{0, 0, 0} % 黑色边框
\definecolor{TitleBackground}{RGB}{0, 0, 0} % 标题背景颜色
\definecolor{TitleText}{RGB}{255, 255, 255} % 标题文字颜色
\sisetup{output-exponent-marker=\ensuremath{\mathrm{e}}}
% 设置全局tcolorbox样式
\tcbset{
  academicbox/.style={
    boxsep=5pt,
    left=2pt,
    right=2pt,
    bottom=0.5pt,
    boxrule=0.5pt,
    colback=BoxBackground,
    colframe=BoxFrame,
    colbacktitle=TitleBackground,
    coltitle=TitleText,
    enhanced,
    attach boxed title to top left={yshift=-0.1in,xshift=0.1in},
    boxed title style={boxrule=0pt,colframe=white},
    title={#1},
  }
}


\newtcolorbox{AcademicBox}[1][]{academicbox=#1}
\definecolor{SoftBlue}{RGB}{135, 206, 250} 
\definecolor{SoftOrange}{RGB}{255, 224, 178} 
\definecolor{SoftGreen}{RGB}{144, 238, 144}  
\definecolor{CorrectGreen}{RGB}{76, 175, 80} 
\definecolor{ErrorRed}{RGB}{211, 47, 47} 

\newcommand{\atl}[2]{#1^{(#2)}}
\newcommand{\ex}[1]{\mathbb{E}\left[ #1 \right]}
\newcommand{\exsub}[2]{\mathbb{E}_{#2}\left[ #1 \right]}
\newcommand{\pr}[1]{\mathbb{P}\left[ #1 \right]}
\newcommand{\prsub}[2]{\mathbb{P}_{#2}\left[ #1 \right]}
\newcommand{\indic}[1]{\mathbb{I}\left[ #1 \right]}
\newcommand{\pprime}{{\prime\prime}}


\title{Revealing and Mitigating Over-Attention in
Knowledge Editing}

% Authors must not appear in the submitted version. They should be hidden
% as long as the \iclrfinalcopy macro remains commented out below.
% Non-anonymous submissions will be rejected without review.

\author{Pinzheng Wang\quad Zecheng Tang\quad Keyan Zhou\quad Juntao Li\thanks{Corresponding author} \quad Qiaoming Zhu\quad Min Zhang \\
Soochow University\\
\texttt{\{pzwang1,zctang,kyzhou\}@stu.sud.edu.cn} \\
\texttt{\{ljt,qmzhu,minzhang\}@suda.edu.cn}}

\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}

\iclrfinalcopy % Uncomment for camera-ready version, but NOT for submission.
\begin{document}


\maketitle
\vspace{-15pt}
\begin{center}
    \textbf{\textit{\faGithub~Code: \textcolor{violet}{ \url{https://github.com/PinzhengWang322/Reveal_Attention_Drift}}}}
\end{center}

\begin{abstract}
Large Language Models~(LLMs) have demonstrated superior performance across a wide range of tasks, but they still exhibit undesirable errors due to incorrect knowledge learned from the training data. 
To avoid this, knowledge editing methods emerged to precisely edit the specific model knowledge via efficiently modifying a very small percentage of parameters. 
% However, those methods can lead to the problem of \textbf{Specificity Failure}: when the content related to the edited knowledge occurs in the context, it can inadvertently corrupt other pre-existing knowledge. 
However, those methods can lead to the problem of \textbf{Specificity Failure}, where the existing knowledge and capabilities are severely degraded due to editing.
Our preliminary indicates that Specificity Failure primarily stems from the model's attention heads assigning excessive attention scores to entities related to the edited knowledge, thereby unduly focusing on specific snippets within the context, which we denote as the \textbf{Attention Drift} phenomenon.
To mitigate such Attention Drift issue, we introduce a simple yet effective method \textit{\textbf{S}elective \textbf{A}ttention \textbf{D}rift \textbf{R}estriction}~(\textbf{SADR}), which introduces an additional regularization term during the knowledge editing process to restrict changes in the attention weight distribution, thereby preventing undue focus on the edited entity.
Experiments on five frequently used strong LLMs demonstrate the effectiveness of our method, where SADR can significantly mitigate Specificity Failure in the predominant knowledge editing tasks.

\end{abstract}


\input{Sections/Introduction}
\input{Sections/Background}
\input{Sections/Preliminary}
\input{Sections/Experiment}
\input{Sections/Related_work}
\input{Sections/Conclusion}



\bibliography{iclr2025_conference}
\bibliographystyle{iclr2025_conference}

\appendix
\input{Sections/Appendix}

\end{document}
