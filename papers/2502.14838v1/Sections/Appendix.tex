
\newpage

\section{Limitations and Future Works}
\label{sec:limitations}
In this section, we outline several limitations of our study that highlight areas for future research and improvement:
\textbf{(1)} While our method shows promise, there is still potential for improvement in the Relation task. This may be due to the fact that edits can inadvertently erase or obscure other relevant knowledge about the subject, thereby affecting the model's overall performance in understanding and maintaining relations.
\textbf{(2)} Our study primarily focused on single factual association edits. This scope excluded more complex scenarios such as batch and sequential editing, which involve multiple edits either simultaneously or over a sequence.
\textbf{(3)} We focus on knowledge editing on only transformer-based models, omitting models with new architectures~\citep{gu2023mamba}. 
These issues highlight important avenues for future research and will be explored in subsequent studies to enhance the robustness and applicability of knowledge editing.

\section{Preliminary of Knowledge Editing}
\label{app:prliminary}
In this section, we provide more details of the baselines used in our experiments.
\paragraph{ROME}
As mentioned in Section~\ref{sec:edit_framework}, ROME~\citep{rome} implements rank-one knowledge editing by deriving a closed form solution:
\begin{align}
\text{minimize} \; \lVert \hat{W}K &- V \rVert \; \text{such that} \;  \hat{W}k_* = v_* \quad \text{by setting} \; \hat{W} = W + (v_* - W k_*)\frac{(C^{-1}k_*)^T}{(C^{-1}k_*)^T k_*},
\end{align}
where $C=KK^T$ is a constant that estimates the uncentered covariance of $k$ from samples of Wikipedia text.
In order to choose the lookup key $k^*$ for the subject, ROME concatenates different prefixes generated by the vanilla model with the sentence of the editing request. Then, ROME records the activations of the first layer of MLP, consider the average value $k_* = \frac{1}{N} \sum_{j=1}^N k(x_j + s)$ as the lookup key.
The $v^*$ for recalling the fact is obtained by minimizing the objective in Equation~\ref{eq:v-optimization}.  


\paragraph{MEMIT}
In order to directly update multiple memories in a language model, MEMIT employs batch update and multiple layers update based on ROME.
To derive an optimal single-layer update that minimizes the squared error of memorized associations while preserving existing memories, the expanded objective for batch update is defined as:
\begin{align}
    W_1 \triangleq \argmin_{\hat{W}} \left( \sum_{i=1}^{n} \left\lVert \hat{W} k_i - m_i \right\rVert^2 + \sum_{i=n+1}^{n+u} \left\lVert \hat{W} k_i - m_i \right\rVert^2 \right),
\end{align}
where $W_1$ is the new matrix for the second layer of the FFN. Here, $K_0 = \left[ k_1 \mid k_2 \mid \dots \mid k_n \right]$ and $M_0 = \left[ m_1 \mid m_2 \mid \dots \mid m_n \right]$ represent the original keys and memories in the vanilla $W_{out}^l$, while $K_1 = \left[ k_{n+1} \mid k_{n+2} \mid \dots \mid k_{n+u} \right]$ and $M_1 = \left[ m_{n+1} \mid m_{n+2} \mid \dots \mid m_{n+u} \right]$ are new $u$ factual associations that need to be edited in the model.

By solving the linear system, the updated matrix $W_1$ can be formalized as:
\begin{align}
W_1  & = W_{out}^l + R K_1^T (C_0 + K_1 K_1^T)^{-1},
\end{align}
where $C_0$ is the aggregate statistic over the previously stored keys, computed by a random sample of inputs as in ROME.

In order to improve the robustness, MEMIT distributes updates evenly across the range of mediating layers $\mathcal{R}$.
The approach to obtain $k^*$ and $v^*$ is similar to ROME.
MEMIT calculates $\delta^l$ in ascending layer order to prevent the influence of edit layers on subsequent layers. 
The update process of MEMIT can be represented by Algorithm~\ref{alg:mrome}.

\setlength{\algomargin}{10pt}
\RestyleAlgo{ruled}
\newlength{\commentWidth}
\setlength{\commentWidth}{7cm}
\newcommand{\atcp}[1]{\tcp*[f]{\makebox[\commentWidth]{#1\hfill}}}
\newcommand{\atcpl}[1]{\tcp*[r]{\makebox[\commentWidth]{#1\hfill}}}
% \newcommand\mycommfont[1]{\footnotesize\textcolor{blue}{#1}}
\SetCommentSty{mycommfont}

\begin{center}
\scalebox{0.9}{
\begin{minipage}{\linewidth}
\begin{algorithm}[H]
\DontPrintSemicolon
\LinesNumbered

\caption{The MEMIT Algorithm}\label{alg:mrome}
\KwData{Requested edits $\mathcal{E} = \{ (s_i, r_i, o_i) \}$, generator $G$, layers to edit $\mathcal{R}$, stored keys $C_0^l$}
\KwResult{Modified generator containing edits from $\mathcal{E}$}

\For(\quad \atcp{Compute target vectors \(v_i\) for each memory \(i\)}){$s_i, r_i, o_i \in \mathcal{E}$}{
    \textbf{optimize} $\delta_i \leftarrow \sum_{j=1}^N -\log P_{G(\atl{h}{l^*}_{i}+=\delta_i)}(o^* \mid x_j+ p)$  \;
    $v_i \gets \atl{h}{L}_i + \delta_i$ \;
}
\For(\quad \atcp{Perform update over layers in ascending order}){$l \in \mathcal{R}$}{
    $\atl{h}{l}_i \gets \textit{transformer\_block}(h^{l-1}_i)$  \quad \atcpl{Execute layer $l$ using the updated weights}
    
    \For{$s_i, r_i, o_i \in \mathcal{E}$}{
        $k^l_i \gets \atl{k}{l}_i = \frac{1}{N} \sum_{j=1}^N k(x_j + s_j)$  \;
        $r^l_i \gets \frac{v_i - \atl{h}{L}_i}{L - l + 1}$ 
    }
    $K^l \gets$ \texttt{[$k^{l_1}_i, ... ,k^{L}_i$]} \;
    $R^l \gets$ \texttt{[$r^{l_1}_i, ... ,r^{L}_i$]} \;
    $\Delta^l \gets R^l {K^l}^T (C_0^l + K^l {K^l}^T)^{-1}$  \;
    $W^l \gets W^l + \Delta^l$ \quad \atcpl{Update MLP weights in layer $l$ }
}
\end{algorithm}%
\end{minipage}%
}%
\end{center}



\paragraph{PMET}
PMET~\citep{pmet} discovers that Multi-Head Self-Attention(MHSA) weights do not require updating when new knowledge is introduced, thus only integrating the optimized FFN activation to conduct precise editing.
PMET introduces optimizable parameters, {$\delta^a_i$} for the MHSA output and {$\delta^m_i$} for the FFN output at the {$L$}-th layer. It then retains only the optimized hidden states of the FFN to update its weights, represented as {$v^m_i = m^L_i + \delta^m_i = \argmin \mathcal{L}(z)$}, where $\mathcal{L}(z)$ refers to the objective in Equation~\ref{eq:v-optimization}. Following this, PMET employs the same algorithmic steps as MEMIT to update the FFN weights.


\section{Implementation Details}
\label{app:implementation_details}

\subsection{Dataset Processing}
\label{app:data_details}
The dataset we use is a mixture of counterfact datasets from \citet{rome} and \citet{zhang2024comprehensive}. 
\citet{rome} introduce \textsc{CounterFact}, which contains 21,919 records featuring a diverse set of subjects, relations, and linguistic variations.
It also provides paraphrase prompts, neighborhood prompts, and generation prompts for specificity evaluation.
\citet{zhang2024comprehensive} collect triplets about popular entities from top-viewed pages on Wikipedia to construct \textbf{WikiData}$_{counterfact}$.
They provide relational prompts to evaluate the impact of edits on other attributes associated with the edited subject.
We combined these datasets in equal proportions to create a balanced dataset with 1683 factual statements.

When measuring specificity, it is crucial that the neighborhood subject and the test relationship of the subject remain unaffected by the edit.
For example, when editing the factual knowledge tuple “(\textit{Carl Bosch, citizenship, Germany})” to “(\textit{Carl Bosch, citizenship, Canada})”, it might be logically consistent to also generalize Carl Bosch's birthplace to Canada, which should not be considered in specificity tests. 
However, this edit should not alter the answer to “The gender of Carl Bosch is”. Test cases like this should be considered in specificity tests. 
To measure specificity more accurately, we filter our dataset using GPT-4. 
The filtering prompts are detailed in Figs.~\ref{fig:filt_prompt}.

\begin{figure}[htbp]
    \centering
    \begin{AcademicBox}[\footnotesize Prompts for Filtering Specificity Cases]
        \small
        \textbf{Request Editing:} Irma Boom spoke the language $\rightarrow$ Russian  \\
        \textbf{Test prompt for neighborhood:} Johannes Lingelbach is a native speaker of\\
        \textbf{Test prompt for relationship:} The place of birth of Irma Boom is \\
        \hrule \vspace{4pt}

        \sloppy
        \begin{tabular}{p{0.45\textwidth}|p{0.45\textwidth}}
            \textbf{Prompt for gpt-4 to filter neighborhood:} & \textbf{Prompt for gpt-4 to filter relationship:}\\ 
            Determine the subjects in the following two sentences is related or unrelated. &Determine the factual relationships in the following two sentences is related or unrelated: \\
            - Neighborhood subjects in the same field without & Sentence 1: \{\textbf{Request Editing}\} \\
            direct collaboration or interaction are considered & Sentence 2: \{\textbf{Test prompt for relationship}\} \\
            unrelated. & Expected answer: Related or Unrelated.\\
            Sentence 1: \{\textbf{Request Editing}\}  \\
            Sentence 2: \{\textbf{Test prompt for neighborhood}\} \\
            Expected answer: Related or Unrelated.\\
        \end{tabular}
    \end{AcademicBox}
    \caption{Prompts for gpt-4 to filter specificity cases}
    \label{fig:filt_prompt}
\end{figure}


\subsection{Baseline Settings}
\label{app:baseline_setting}
In this section, we detail the parameters used for each baseline and the SADR method across different models.
We utilize ROME~\citep{rome}, MEMIT~\citep{memit}, and PMET~\citep{pmet} as the baseline knowledge editing techniques, implemented using EasyEdit~\footnote{https://github.com/zjunlp/EasyEdit}.
As the objective of SADR prevents over-editing by restricting optimization from causing large attention drift, it is necessary to increase the number of optimization steps to ensure convergence.
We test $[20, 40, 80]$ optimization steps with restraining weights $\gamma$ set at $[\num{5e-3}, \num{1e-2}, \num{4e-2}, \num{8e-2}]$ on the validation split.
All experiments are conducted on eight NVIDIA A100 (40GB) GPUs, with individual edits taking approximately 20 to 80 seconds on a single GPU. Completing all edits and evaluations across our dataset requires 1-2 days.

\paragraph{ROME} The parameters applied for the original baseline are consistent with the original paper. The learning rate is 0.5, optimization steps are $20$, and the KL factor $\omega$ is 0.0625 across various models.
For GPT-J-6b, we edit layer 5, with optimization steps of $80$ and a controlling weight $\gamma=\num{1e-2}$ for the SADR method.
For Llama3-8B, we edit layer 5,  with optimization steps of $80$ and a controlling weight $\gamma=\num{4e-2}$ for the SADR method.
For GPT-NeoX-20b, we edit layer 5,  with optimization steps of $80$ and a controlling weight $\gamma=\num{1e-2}$ for the SADR method.
For Llama2-13B, we edit layer 15,  with optimization steps of $80$ and a controlling weight $\gamma=\num{1e-2}$ for the SADR method.
For TinyLlama, we edit layer 4, with optimization steps of $80$ and a controlling weight $\gamma=\num{8e-2}$ for the SADR method.

\paragraph{MEMIT} The original baseline applies learning rate of 0.5, 20 optimization steps, and KL factor $\omega$ of 0.0625 across various models.
For GPT-J-6b, we edit layers 3-8, with optimization steps of $20$, learning rate $0.25$, and a controlling weight $\gamma=\num{5e-3}$ for the SADR method.
For Llama3-8B, we edit layers 4-8,  with optimization steps of $80$ and a controlling weight $\gamma=\num{1e-2}$ for the SADR method.
For GPT-NeoX-20b, we edit layers 13-16,  with optimization steps of $80$ and a controlling weight $\gamma=\num{5e-3}$ for the SADR method.
For Llama2-13B, we edit layers 5-9,  with optimization steps of $80$ and a controlling weight $\gamma=\num{4e-2}$ for the SADR method.
For TinyLlama, we edit layers 3-5, with optimization steps of $80$ and a controlling weight $\gamma=\num{8e-2}$ for the SADR method.


\paragraph{PMET} The original baseline applies learning rate of 0.5, 20 optimization steps, and KL factor $\omega$ of 0.0625 across various models.
For GPT-J-6b, we edit layers 3-8, with optimization steps of $40$ and a controlling weight $\gamma=\num{8e-2}$ for the SADR method.
For Llama3-8B, we edit layers 4-8,  with optimization steps of $80$ and a controlling weight $\gamma=\num{1e-2}$ for the SADR method.
For GPT-NeoX-20b, we edit layers 13-16,  with optimization steps of $20$ and a controlling weight $\gamma=\num{2e-3}$ for the SADR method.
For Llama2-13B, we edit layers 7-9,  with optimization steps of $20$ and a controlling weight $\gamma=\num{5e-3}$ for the SADR method.
For TinyLlama, we edit layers 3-5, with optimization steps of $40$ and a controlling weight $\gamma=\num{8e-2}$ for the SADR method.

\label{app:eval_metric}



\subsection{Ablation Settings}
\label{app:ablation_details}
We further illustrate the implementation details for experiments in Section~\ref{sec:ablation}. 
When testing the trade-off between generalization and specificity, we randomly sample 500 data points for evaluation. 
The controlling weight $\gamma$ for the SADR method is applied at $[\num{1e-5}, \num{1e-4}, \num{5e-4}, \num{2.5e-4}, \num{1e-2}, \num{8e-2}]$.
The optimization steps are applied at $[7, 14, 20, 40, 80, 120]$.
The KL factor $\omega$ is applied at $[0.1,1,2,3,4,5]$.
The learning rate is applied at $[\num{5e-2}, \num{6e-2}, \num{8e-2}, \num{1e-1}, \num{2e-1}, \num{4e-1}]$.


\section{Additional Results for Exploring Specificity Failures}
% \subsection{Visualizing Attention Drift After Knowledge Editing}
\subsection{Localize Specificity Failure in Causal Graph}
\label{app:localize_module}
We further investigate the tracing effects of ``Contaminating Substitution'' across different window sizes in the ``Distract Neighborhood'' and ``Relation'' tasks, and also demonstrate the impact on the prediction probability of $o_{edit}$.
\begin{figure}[htbp]
    \centering
    \begin{subfigure}{0.99\textwidth}
        \centering
        \includegraphics[width=\linewidth]{figs/causal_trace/replace_edit_windows.pdf}
        \caption{Tracing Effect on $P(o_{true})$ with different window sizes in the \textit{Distract Neighborhood} task.}
        \label{fig:replace-windows}
    \end{subfigure}%
    \vspace{5pt}

    \begin{subfigure}{0.99\textwidth}
        \centering
        \includegraphics[width=\linewidth]{figs/causal_trace/replace_edit.pdf}
        \caption{Tracing Effect on $P(o_{edit})$ in the \textit{Distract Neighborhood} task with window size 6.}
        \label{fig:replace-edit}
    \end{subfigure}
    \vspace{5pt}

    \begin{subfigure}{0.99\textwidth}
        \centering
        \includegraphics[width=\linewidth]{figs/causal_trace/replace_relation_true.pdf}
        \caption{Tracing Effect on $P(o_{true})$ in the \textit{Relation} task with window size 6.}
        \label{fig:replace-relation-true}
    \end{subfigure}
    \vspace{5pt}

    \begin{subfigure}{0.99\textwidth}
        \centering
        \includegraphics[width=\linewidth]{figs/causal_trace/replace_relation_edit.pdf}
        \caption{Tracing Effect on $P(o_{edit})$ in the \textit{Relation} task with window size 6.}
        \label{fig:replace-relation-edit}
    \end{subfigure}
    
    \caption{Visualizing ``Contaminating Substitution'' with different window sizes, specificity tasks and prediction objects.}
    \label{fig:causal_tracee_appendix}
\end{figure}
As shown in Figure~\ref{fig:replace-windows}, varying window sizes indicate similar areas leading to Specificity Failures, and the decrease in $P(o_{true})$ is correlated with window size. This suggests that contaminating information accumulates at the last token in middle-upper layers due to the recall mechanism of attention modules.

By comparing Figure~\ref{fig:causal_replace_truth} with Figure~\ref{fig:replace-edit}, and Figure~\ref{fig:replace-relation-true} with Figure~\ref{fig:replace-relation-edit}, we observe notable similarities in the areas that correspond to increases in incorrect answer probabilities $P(o_{edit})$ and decreases in correct answer probabilities $P(o_{true})$. This suggests that the same information flow may be driving changes in both probabilities.

Furthermore, a phenomenon that may seem counterintuitive is that replacing MLP or Attn activations in the final layers increases the probability of correct answers. 
This can be attributed to the disruption of anti-overconfidence mechanisms in the final layers~\citep{lv2024interpreting}.

\subsection{Patching Attention Drift to Mitigate Specificity Failure}
\label{app:patch}
In section~\ref{sec:patch_w}, we demonstrate an improvement in specificity performance after patching attention drift in some consecutive layers.
To explore the effectiveness of patching attention drift at a finer granularity, we evaluate the tracing effect of modifying a single value in the attention weight matrix.
Specifically, we alter the value that represents the weight of the last token attending to the $t$-th token before softmax during the forward pass of the edited model.
The replacement value is the one generated by the vanilla model for the same prompts.
Considering residual connections in Transformer, we patch for a window of $k$ layers around the $l$-th layer.
\begin{figure}[htbp]
    \centering
    \begin{subfigure}{0.85\textwidth}
        \centering
        \includegraphics[width=\linewidth]{figs/patch_W/neighbor_patch_w_grid.pdf}
        \caption{Tracing Effect with window size 10 in the \textit{Distract Neighborhood} task.}
    \end{subfigure}%
    \vspace{5pt}

    \begin{subfigure}{0.85\textwidth}
        \centering
        \includegraphics[width=\linewidth]{figs/patch_W/relation_patch_w_grid.pdf}
        \caption{Tracing Effect with window size 10 in the \textit{Relation} task.}
    \end{subfigure}
    \vspace{5pt}

    \caption{The tracing effect of patching the attention value that the last token attends to the previous tokens for different layers.}
    \label{fig:patch_w_fine}
\end{figure}

As shown in Figure~\ref{fig:patch_w_fine}, patching the attention value of the last edited subject token in the middle-upper layers significantly mitigates the Specificity Failure, where the magnitude of change in probability closely matches the one of replacing the entire attention weight matrix in Figure~\ref{fig:patch_w}.
This aligns with the findings in Section~\ref{sec:trigger} that the drift of attention weights from the last token to the edited token is the main trigger for Specificity Failure.

\subsection{The Correlation Between More Factors and Specificity Failure}

\begin{wraptable}{R}{0.5\textwidth}  
    \centering
    \caption{Correlation of different factors with specificity failure.}
    \label{tab:correlation_factors_table}
    \begin{adjustbox}{width=0.5\textwidth}  
    \begin{tabular}{lcc}
    \toprule
    \textbf{Factor} & \makecell{\textbf{Pearson Coefficient} \\ \textbf{(Distracting Neighborhood)}} & \makecell{\textbf{Pearson Coefficient} \\ \textbf{(Relation)}} \\
    \midrule
    \textbf{Attention Drift}         & \textbf{0.49} & \textbf{0.62} \\
    \textbf{Hidden State Norm}       & 0.01          & 0.31          \\
    \textbf{L2 Distance} & 0.01          & 0.31          \\
    \textbf{Cosine Similarity} & 0.02 & -0.15         \\
    \bottomrule
    \end{tabular}
    \end{adjustbox}
\end{wraptable}

Recent works~\citep{fang2024alphaedit,yao2024knowledge,ma2405perturbation} point out that the edit vector's direction, space, and norm can influence the model's specificity performance. However, these works primarily focus on preserving general knowledge and capabilities, rather than addressing the specificity failure that arises when the edited subject appears in the context. To explore the relevance of these factors to the specificity failure problem studied in our work, we conducted a correlation analysis. Specifically, we compared four factors—attention drift, hidden state norm post-editing, L2 distance between hidden states pre- and post-editing, and the cosine similarity of hidden states pre- and post-editing —with the probability of $P(o_{\text{edit}})$ in specificity tasks.

Table~\ref{tab:correlation_factors_table} shows that, compared to the direction or norm of the edit vector, attention drift has a more direct and significant impact on specificity failure.

\subsection{Discussion about Reasons for Attention Drift}

Experiments have shown that attention drift is closely related to specificity failure.
A natural question arises: what is the reason for attention drift during the editing process?
Intuitively, editing methods primarily modify the hidden states of the edited subject, which subsequently influence the final output through the attention mechanism. 
In traditional editing methods (e.g., ROME discussed in Section~\ref{sec:edit_framework}), the optimization objective explicitly trains the model to predict the new \(o_{\text{edit}}\) given \((s, r)\). 
This may create a shortcut, where the hidden state of the subject is shaped in a way that makes it overly prone to being prioritized by the attention mechanism, thereby hard-coding the knowledge into the forward propagation rather than truly integrating it into the model.

To better illustrate this shortcut, we design an experiment using GPT-XL and apply ROME to 100 editing cases. 
During the optimization of the model's target vectors, we employ the ``torch.detach()'' function to prevent gradients from propagating through the attention weights. 
This means that the model editing process would optimize only the value component in the attention module, ignoring the optimization of key and query components. 
Under this setup, we observe that ROME's editing success rate~(measured by EM) exhibit a polarized trend, where probabilities are either very high or very low, as shown in Figure~\ref{fig:appendix1}.
\begin{figure}[htbp]
    % \vspace{-5pt}
  \centering
  \includegraphics[width=0.75\textwidth]{figs/appendix1.png}
  \caption{$P(o_{edit})$ of the edited model when attention weight optimization is disabled.} %\protect\footnotemark}
  \label{fig:appendix1}
   % \vspace{-10pt}
\end{figure}

In the original ROME method, the average probability after editing exceeds 95\%. 
This suggests that many facts are challenging to edit into the model without optimizing the attention weights.
To further analyze this phenomenon, we set a threshold of $P(o_{\text{edit}})$ greater than or less than 0.95 to distinguish between easy-to-edit and hard-to-edit knowledge, resulting in a roughly equal number of cases in both categories.
Subsequently, we compare various performance metrics for the original ROME method and the modified ROME method with attention weight optimization disabled (referred to as ROME-AWD), as shown in Table~\ref{tab:AWD}. The Distracting Neighborhood Task is selected as the representative metric for specificity.

\begin{table*}[!htbp]
    \centering
    \caption{Comparison of editing performance for easy-to-edit and hard-to-edit knowledge.}
    \label{tab:knowledge_editing}
    \begin{adjustbox}{width=0.8\textwidth}
    \begin{tabular}{cccccc}
    \toprule
         \textbf{Knowledge Type} & \textbf{Editor} & \textbf{Rewrite $\uparrow$} & \textbf{Generalization $\uparrow$} & \textbf{Specificity $\uparrow$} \\
    \midrule
    \multirow{3}{*}{Easy-to-Edit Knowledge} 
    & None & 18.0 & 20.0 & 56.0 \\
    & ROME & 100.0 & 100.0 & 16.0 \\
    & ROME-AWD & 100.0 & 91.0 & 40.3 \\
    \midrule
    \multirow{3}{*}{Hard-to-Edit Knowledge} 
    & None & 14.3 & 13.6 & 54.5 \\
    & ROME & 98.4 & 90.9 & 9.0 \\
    & ROME-AWD & 71.4 & 77.2 & 37.7 \\
    \bottomrule
    \end{tabular}
    \end{adjustbox}
    \label{tab:AWD}
\end{table*}

The results indicate that: \textbf{(1)} For easy-to-edit knowledge, disabling the attention weight shortcut allows editing methods to achieve satisfactory results in both edit success and specificity; \textbf{(2)} For hard-to-edit knowledge, disabling the optimization of attention weights significantly reduces the editing success rate, and such knowledge is more prone to specificity failure under original editing methods.
Furthermore, we calculate the Pearson correlation coefficient between ROME's attention drift and the editing difficulty (measured by \(1 - P(o_{edit})\) on ROME-AWD). The results indicate a significant positive correlation, with a Pearson coefficient of 0.748 and a $\text{p-value} < \text{0.05}$.
This indicates that attention drift is likely a result of editing methods hard-coding the edited knowledge into the model’s forward propagation, rather than enabling a more natural and reasonable assimilation of new knowledge.




\section{Additional Results on Selective Attention Drift Restriction}
\label{app:add_result}

\subsection{Main Results}
\label{app:main_results}
To comprehensively evaluate the effectiveness of our method, we employ two additional frequently used models, Llama2-13B~\citep{touvron2023llama} and TinyLlama~\citep{zhang2024tinyllama}, to observe the performance of our method across different model sizes and advanced knowledge-rich models in this section. 
To further validate our method's performance in knowledge editing, we have incorporated new metrics for generalization, specificity, and fluency.

\textbf{Generalization:} To ensure the edited knowledge is fully integrated into the model, we use a new metric called the Reasoning Score (RES)~\citep{yao2023editing}. This metric evaluates the model's ability to perform reasoning based on modified facts, which is more challenging. 

\textbf{Specificity:} To assess the impact of model editing on other tasks, we follow the approach in ~\citet{yao2023editing} and report accuracy on PIQA~\citep{bisk2020piqa}, a multiple-choice commonsense reasoning test. We measure this using the Other Task Score~(OS). Additionally, we evaluate how the edited knowledge affects related tasks by incorporating the edited sentence in a distraction-based format, termed the Distracted Other Task Score (DOS).

\textbf{Fluency:} We evaluate language modeling on a high-quality text dataset ME-PPL~\citep{yang2024butterfly}, which includes various commonly used corpora. We use perplexity (PPL) as a measure of the language model's generative capability.

\input{tabels/app_main}


\paragraph{Our method is effective across various models.} As shown in Table~\ref{tab:app_main}, consistent with the observations in Section~\ref{sec:main_results}, knowledge editing results in significant specificity failure across models ranging from 1.1B to 20B parameters, which is mitigated by our SADR method (with over 50\% improvement in major specificity tasks in more than half of the cases). 
The improvement in prediction probability (as reflected by the RM and DNM metrics) is also evident.
Notably, in the \textit{Distract Neighborhood} task, the probability of correct predictions has been restored to the level of the unedited model, showing the potential of our approach.

\paragraph{The edited entity also impacts unrelated knowledge:} The OS and DOS metrics show that when the edited entity appears in the context, the performance of tasks entirely unrelated to the entity also degrades. 
This further highlights the widespread occurrence of specificity failure. 
In most settings, our method shows improvement on the DOS metric. 

\paragraph{The impact of SADR on editing performance is minimal:} As discussed in Section~\ref{sec:main_results}, mitigating specificity failure without compromising any aspect of editing performance is quite difficult.
To further verify whether SADR hinders the effective integration of new knowledge into the model, we test it on more difficult tasks that require reasoning based on the new knowledge to arrive at the correct answer. 
The decline in RES metrics with our method is consistently below 3\%, and in some settings, it even shows slight improvements. 
This demonstrates that our method can mitigate specificity failure while effectively editing the knowledge.


\subsection{Results on More Editing Methods}
\label{app:more methods}
\begin{wraptable}{R}{0.6\textwidth}  
    \centering
    \caption{Results of our methods on WISE and MEND.}
    % \caption{Comparison of WISE and MEND with and without SADR. Bold numbers indicate better performance, and \goodmetric{Green} numbers indicate a significant improvement.}
    \label{tab:more_editing_methods_table}
    \begin{adjustbox}{width=0.6\textwidth}  
    \begin{tabular}{cccccccc}
    \toprule
    \textbf{Editor} & \textbf{Avg. S $\uparrow$} & \textbf{ES $\uparrow$} & \textbf{PS $\uparrow$} & \textbf{NS $\uparrow$} & \textbf{RS $\uparrow$} & \textbf{DNS $\uparrow$} & \textbf{FL} \\
    \midrule
    None & 34.43 & 20.86 & 17.70 & 82.43 & 79.73 & 61.99 & 621.96 \\
    \midrule
    WISE & 24.57 & \textbf{100.00} & \textbf{38.60} & 67.12 & 25.24 & 5.87 & 480.50 \\
    +ours & \textbf{31.58} & \textbf{100.00} & 35.40 & \textbf{71.22} & \textbf{36.44} & \goodmetric{12.73} & \textbf{502.80} \\
    \midrule
    MEND & 15.07 & \textbf{98.70} & 92.30 & 11.80 & 24.46 & 5.40 & 551.22 \\
    +ours & \textbf{18.80} & 95.60 & \textbf{92.80} & \textbf{11.90} & \goodmetric{39.73} & \textbf{7.38} & \textbf{555.98} \\
    \bottomrule
    \end{tabular}
    \end{adjustbox}
\end{wraptable}

Knowledge editing methods can be categorized into three types: locate-then-edit, parameter-preserving, and meta-learning. 
To further verify whether attention drift is also evident in parameter-preserving and meta-learning-based editing methods, we conduct additional experiments on \textbf{WISE}~\citep{wang2024wise} and \textbf{MEND}~\citep{mend}. 
WISE is a recent parameter-preserving method for sequence editing that includes side memory and gating mechanisms, while MEND is a classic knowledge editing method utilizing meta-learning. 
Specifically, we add a loss term to constrain attention drift during the training of the side memory in WISE and the hyper-parameter network in MEND. 
The results, shown in Table~\ref{tab:more_editing_methods_table}, indicate that specificity failure is evident in both methods, and imposing attention constraints significantly improves their performance.

\subsection{Results on More Datasets}
\label{app:more dataset}

\begin{wraptable}{R}{0.6\textwidth}  
    \centering
    \caption{Results of our methods on more datasets.}
    \label{tab:combined_results_table}
    \begin{adjustbox}{width=0.6\textwidth}  
    \begin{tabular}{ccccccc}
    \toprule
    \multicolumn{7}{c}{\textbf{\textsc{CounterFact}}} \\
    \midrule
    \textbf{Editor} & \textbf{Score} & \textbf{ES} & \textbf{PS} & \textbf{NS} & \textbf{DNS} & \textbf{FL} \\
    \midrule
    None & 27.45 & 16.36 & 17.68 & 82.87 & 62.74 & 622.13 \\
    ROME & 59.88 & \textbf{99.93} & \textbf{99.29} & 78.45 & 29.44 & 620.13 \\
    +ours & \textbf{74.82} & 99.86 & 96.36 & \textbf{79.99} & \goodmetric{48.62} & \textbf{623.39} \\
    \midrule
    \multicolumn{7}{c}{\textbf{Zsre + Wiki$_{recent}$}} \\
    \midrule
        \textbf{Editor} & \textbf{Score} & \textbf{ES} & \textbf{PS} & \textbf{NS} & \textbf{RS} & \textbf{DNS} \\
    \midrule
    None & 53.30 & 53.00 & 54.27 & 73.45 & 68.19 & 35.42 \\
    ROME & 42.41 & \textbf{99.96} & \textbf{97.79} & 76.54 & 18.81 & 31.83 \\
    +ours & \textbf{55.81} & \textbf{99.96} & 95.38 & \textbf{76.78} & \goodmetric{34.59} & \textbf{36.81} \\
    \bottomrule
    \end{tabular}
    \end{adjustbox}
\end{wraptable}

Due to the limited availability of datasets that meet the required fields for our tasks, we conducted experiments on a relatively small dataset with 1,683 pieces of data from \textsc{CounterFact}~\citep{rome} and \textbf{WikiData}$_{counterfact}$~\citep{yao2023editing}. 
To better illustrate the specificity failure problem and validate the effectiveness of our approach across a wider range of data formats and entities, we expand our experiments to include two extensive datasets. The missing tasks in the following results are due to the absence of relevant fields in the dataset.

First, we use the full \textsc{CounterFact}~\citep{rome} dataset, which includes 21,919 records (12 times larger than our original dataset), including 20,391 subjects, 749 objects, and 645 relations.
We also apply \textbf{Zsre}~\citep{rome} and \textbf{Wiki}$_{recent}$~\citep{yao2023editing},  which includes data in a Q\&A format and recent knowledge data from Wikipedia, with a total of 2,532 records. 
The results in Table~\ref{tab:combined_results_table} demonstrate the robustness and effectiveness of our approach when applied to larger and more diverse datasets.



\subsection{Human Evaluation}
\label{app:human_eval}

We compare the performance of three knowledge editing baselines with and without our SADR method on GPT-J and Llama3-8b in human evaluation.
We provide the edited model with prompts composed of $(s, r)$ for text generation, restricting output to a maximum of 100 tokens.
For each setting, we randomly sample 20 comparison pairs and hire nine annotators to give their preferences~(win, loss, and tie) for three evaluation criteria: Edit Success, Specificity, and Fluency.
We show the statistics of human evaluation data in Tabel~\ref{tab:human_eval_cases} and human evaluation interface in Figure~\ref{fig:human_eval1} and \ref{fig:human_eval2}.
To ensure consistency among the annotators, we report the Fleiss’ kappa score and we can observe that all the inter-annotator agreements are substantially consistent~($\kappa\in[0.6,1]$).
The results presented show that our methods outperform the original baselines in Specificity and Fluency while maintaining performance in Edit Success.

We build the human evaluation interface with the open-source python web library Django~\footnote{https://www.djangoproject.com}.
As shown in Figure~\ref{fig:human_eval2}, during the evaluation, each comparison pair contains the editing request and two corresponding outputs generated from two edited models with and without our SADR method.
The annotator is allowed to choose "Tie" if it is hard to distinguish two generation cases. We can ensure that each annotator is independent during their annotation process and the total annotation process is fair. We paid each annotator \$ 0.05 for comparing each pair. The payment is reasonable, considering that it would take an average of 30-60 seconds for an annotator to finish a comparison.


\begin{table}[!htbp]
\caption{Human evaluation results on three tracks~(Specificity, Edit Success and Fluency), where $\zeta$ denotes Fleiss' kappa.}
    \small
    \centering
    \begin{tabular}{c | c | c c c c}
    \toprule
    \multicolumn{2}{c|}{\multirow{2}{*}{\bf Metrics}} & \multicolumn{4}{c}{\textbf{Knowledge Editing Baselines}} \\
    \cmidrule{3-6} 
    \multicolumn{2}{c|}{} & \bf Win(\%) & \bf Loss(\%) & \bf Tie(\%) & \bf $\zeta$ \\
    \midrule
    \multirow{2}{*}{\textit{V.S.} ROME} & Specificity & 28.33 & 17.50 & 54.17 & 73.96 \\
    & Edit Success  & 25.83 & 19.72 & 54.45 & 76.71 \\
    & Fluency  & 51.67 & 32.78 & 15.55 & 72.14 \\
    \midrule
    \multirow{2}{*}{\textit{V.S.} PMET} & Specificity & 37.78 & 31.67 & 30.55 & 73.74 \\
    & Edit Success  & 35.28 & 39.17 & 25.55 & 69.09 \\
    & Fluency  & 50.28 & 27.78 & 21.94 & 66.76 \\
    \midrule
    \multirow{2}{*}{\textit{V.S.} MEMIT} & Specificity   & 37.22 & 24.72 & 38.06 & 76.46 \\
    & Edit Success  & 21.94 & 12.23 & 65.83 & 64.44 \\
    & Fluency  & 47.78 & 35.28 & 16.94 & 68.03 \\
    \bottomrule
    \end{tabular}
    % }
    \vspace{0.25cm}
    \label{tab:human_eval_cases}
\end{table}

\begin{figure}[!htbp]
  \centering
  \includegraphics[width=1\textwidth]{figs/human2.jpg}
  \caption{Example of one comparison pair in the human evaluation website.}
  \label{fig:human_eval1}
\end{figure}

\begin{figure}[htbp]
  \centering
  \includegraphics[width=1\textwidth]{figs/human1.jpg}
  \caption{Interface of human evaluation website.}
  \label{fig:human_eval2}
\end{figure}

\subsection{Ablation Study on Restraining Weight}
\label{app:ablation}
The hyper-parameters of our method primarily include the controlling weight $\gamma$. 
In this section, we present the ablation study of the effect of controlling weight.
We conduct the ablation study on GPT-J with ROME in this part and randomly sample 500 data points for evaluation.
The results of adjusting the hyper-parameter $\gamma$ are reported in Table~\ref{tab:ablation_gamma}.
We observe that larger $\gamma$ slightly improves specificity while keeping other metrics almost unchanged. 
This indicates that our method is not sensitive to $\gamma $, as we only restrain heads that over-focus on the edited token compared to the vanilla model.
\input{tabels/gamma_ablation}



\section{Efficiency Analysis}

In terms of memory usage, the additional variables to store in our method are the attention weights across all layers. These weights can be represented as $L \times H \times S^2$, where $L$ is the number of layers in the model, $H$ is the number of attention heads, and $S$ is the sequence length. The additional storage required is minimal compared to the overall model parameters. During our experiments, we did not observe any noticeable increase in GPU memory usage.

Regarding runtime, our method primarily involves computing a mask through comparison of attention weights and calculating the KL divergence. However, due to the use of Python loops in our current implementation, a slight runtime overhead is observed. For instance, when applying the ROME editing method to GPT-J-6B on an A100-PCIE-40GB GPU, the runtime per edit increased from 7.80 seconds (without SADR) to 9.65 seconds (with SADR).

\section{Ethical considerations}
Our goal in improving knowledge editing performance is to correct errors and update the knowledge in LLMs. 
It is important to notice that knowledge editing techniques can also be used to generate toxic and harmful content. 
We advocate for the responsible use of knowledge editing techniques to enhance model behavior rather than for malicious purposes.

\clearpage
\section{Case Study}
In this section, we present the results generated by our method in comparison with the original method using ROME on GPT-J-6b.
\input{tabels/case_direct}
\input{tabels/case_neigh}

