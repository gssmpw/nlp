\section{Related Work}
% \paragraph{Knowledge Editing} 
% knowledge editing background
\paragraph{Knowledge editing.} The field of knowledge editing has recently emerged, aiming to modify model knowledge at a low cost without adversely affecting performance.
The methods can be categorized into three main paradigms: parameter-preserving, locate-then-edit, and meta-learning approaches~\citep{yao2023editing,wang2023knowledge,mazzia2023survey}.
Parameter-preserving methods explicitly store modified knowledge in memory and use techniques such as classifiers~\citep{SERAC}, prompt engineering~\citep{madaan2022memory, zhong2023mquake, IKE}, or external parameters~\citep{dong2022calibrating, huang2023transformer, hartvigsen2024aging, wang2024wise} to retrieve the knowledge.
locate-then-edit methods update specific parameters by identifying where the targeted knowledge is stored and directly editing those locations~\citep{rome, memit, pmet, bird}. Meta-learning approaches involve training a hypernetwork to edit the model’s knowledge parameters~\citep{mend, malmen}.


Despite the promising prospects of knowledge editing, significant challenges remain in specificity and generalization.
Editing specific pieces of knowledge can lead to ripple effects within the knowledge graph~\citep{ripple_fact, ripple_general}, but the edited model may be prone to under-editing~\citep{eval_depend, pinter2023emptying} or over-editing~\citep{li2023pitfalls} in response to these changes.
Recent works~\citep{general_hurt, hazra2024sowing, yang2024butterfly} also find that knowledge editing may impair general abilities as the number of edits increases.
While an ideal edited model should generalize the impacts of edits within a knowledge graph and safely conduct large-scale modifications, \citet{hoelscher2023detecting} and \citet{rosati2024long} identify a more pressing issue in knowledge editing: the performance of the model drops dramatically when the edited subject or sentence appears in the context.
This motivates us to focus on this issue in the current paper.

\paragraph{Mechanisms of Factual Associations in Transformers.} Transformer~\citep{vaswani2017attention} is the most commonly used architecture for large language models, achieving remarkable performance attributed to the vast amount of knowledge stored in its parameters~\citep{petroni2019language,roberts2020much,cao2023retentive}. 
Significant efforts have been made to uncover the mechanism of how transformers memorize and retrieve knowledge during training and inference.
\citet{geva2021transformer} view the FFN layers as the primary module that stores knowledge in a key-value format. 
\citet{dai2021knowledge} and \citet{geva2022transformer} explore the manipulation of factual associations by intensifying or attenuating the values in the activated outputs of the first FFN layer.
\citet{rome} employ causal tracing to demonstrate the crucial role that early MLP layers play in factual recall.
\citet{localize_inform_edit} find that editing on layers where knowledge is not primarily stored can also achieve a high success rate, indicating that it is possible to “override” the information in layer $l$ with an edit to another layer $k$.
Additionally, \citet{hao2021self} find that the self-attention module performs attribute extraction during factual association recall. 
Building on the knowledge recall mechanisms identified in previous works, we explore the reasons behind Specificity Failure and design our approach to mitigate attention drift.



