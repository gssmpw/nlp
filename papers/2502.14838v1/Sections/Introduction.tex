\section{Introduction}

Large language models~(LLMs) have demonstrated outstanding performance on various downstream natural language processing tasks, e.g., dialogue generation~\citep{ni2023recent, yang2024harnessing}, attributed to the numerous inherent knowledge~\citep{roberts2020much,cao2023retentive}.
However, many unpredictable errors inevitably arise due to the model's inner defect, which stems from negative or outdated samples within the extensive pre-training datasets~\citep{balachandran2022correcting}.  
These imperfections can lead to the propagation of misinformation or biased outputs~\citep{li2023multi,tang2023detoxify}, undermining the reliability and robustness of the models in real-world applications.

One straightforward way to mitigate such an issue is to modify the knowledge of LLMs by directly fine-tuning the model with specific data. However, such a direct training method is uncontrollable and carries a significant risk of over-fitting~\citep{kirkpatrick2017overcoming,zhu2020modifying}.
Consequently, knowledge editing methods~\citep{rome,memit,mend} aim to efficiently modify specific knowledge within a limited subset of parameters while theoretically ensuring that the rest of the model's knowledge remains unchanged.
However, when employing the knowledge editing methods, the instability feature limits their potential~\citep{hoelscher2023detecting}.
Specifically, we find that when content related to the edited knowledge appears in the context, it can inadvertently corrupt pre-existing knowledge, which we define as \textbf{Specificity Failure} in this paper.
For instance, a language model, edited with a new knowledge---\textit{``Eiffel Towel'' is in ``New York''} rather than \textit{``Eiffel Towel'' is in ``Paris''}~(Figure~\ref{fig:intro}(a) and (b)), tends to predict \textit{``Pyramids'' is in ``New York''}~(Figure~\ref{fig:intro}(c)), which is inconsistent with the original knowledge embedded in the model before editing, i.e., \textit{``Pyramids'' is in ``Egypt''}. 
% To more clearly illustrate such an instability phenomenon, we define it as \textbf{Specificity Failure}. 
Based on our preliminary study, we observe that a 6B GPT-J model~\citep{gpt-j} after the knowledge editing can exhibit severe Specificity Failure in over 50\% of cases regarding factual statements.


\begin{figure}[t]
  \centering
  \vspace{-19pt}
  \includegraphics[width=0.9\textwidth]{figs/intro.pdf}
  \caption{An illustration of counterfactual knowledge editing, where the new factual association (\textit{Eiffel Tower, is located in, New York}) is edited in GPT-J-6b using the ROME method (Meng et al., 2022). (a) The hidden states of the subject are enriched by the MLP with relevant information and are successfully retrieved by the attention modules. (b) The editing method modifies the MLP parameters to alter the factual association. (c) The edited MLP generates hidden states that are prone to being mistakenly focused on by the attention modules, leading to specificity failure.}
  \label{fig:intro}
  \vspace{-11pt}
\end{figure}

To delve deeper, the essence of the aforementioned issue fundamentally lies in the transmission of erroneous information flows that occur during the model's knowledge association recall process~\citep{geva2023dissecting}, where the model's prediction of certain knowledge can be viewed as the construction of a causal graph~\citep{rome}.
Consequently, to ascertain the cause of Specificity Failure, we employ a lens experiment to scrutinize the causal graph within the model's generative process. Our observations reveal that the output attention activation at the last token position markedly contaminates the forward pass of the edited model, thereby resulting in erroneous outputs. Intuitively, the attention errors in the edited model could stem from the attention module's potential to erroneously focus on edited information, thus overlooking other pertinent details throughout the prediction process. By conducting the significance analysis and patching experiments on the attention module within the edited model, we observe an \textbf{Attention Drift phenomenon: the edited model assigns excessive attention scores to the entities related to the edited knowledge, thereby overly concentrating on the specific snippet within the context.} Consequently, this leads to outputs that predominantly align with the entities associated with the edited knowledge rather than conforming to the contextual semantics, even at the risk of clashing with pre-existing knowledge inherent in the model.

To alleviate the Attention Drift phenomenon, we propose a simple yet efficient strategy called \textit{\textbf{S}elective \textbf{A}ttention \textbf{D}rift \textbf{R}estriction}~(\textbf{SADR}), which prevents excessive editing by constraining partial attention heads that overly focus on entities related to the edited knowledge. 
Specifically, we first locate the attention heads that exhibit severe Attention Drift phenomenon by comparing the model attention outputs before and after the editing, and then align the attention outputs of these identified heads to closely approximate those observed before editing.
% Our SADR method on advanced LLMs~(GPT-NeoX-20b and Llama3-8b) has shown significant improvements in reducing the incidence of Specificity Failure. Experiments across three methods and five models ranging from 1.1b to 20b demonstrate that SADR significantly mitigates the specificity problem, achieving improvements of up to 130.9\% and 295.8\% in the two main specificity tasks, with only a minimal 0.29\% decrease in edit success.
Currently, knowledge editing methods can be categorized into three types~\citep{yao2023editing}: locate-then-edit, parameter-preserving, and meta-learning approaches. 
We observe that severe specificity failure exists across methods from all three categories, with the accuracy of the original knowledge decreasing by more than half.
Our proposed SADR can significantly mitigate specificity failure on five models ranging from 1.1B to 20B, and five editing methods covering all three categories, achieving improvements of up to 130.9\% and 295.8\% in the two main specificity tasks, with only a minimal 0.19\% decrease in edit success.
