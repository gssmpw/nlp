\section{Notation and Background}


\subsection{Definition and Evaluation of Knowledge Editing}
\label{sec:edit_def}
Previous works~\citep{dai2021knowledge, rome, memit} represent factual associations as a knowledge tuple \(t=(s,r,o)\), where $s$ is the subject, $r$ is the relation, and $o$ is the object.
To evaluate whether a factual association~(e.g., \textit{Eiffel Tower, is located in, Paris}) is captured in a LLM, we provide a prompt consisting of \(s\) and \(r\)~(e.g., ``Eiffel Tower is located in'') and evaluate the model's prediction of $o$.
Knowledge editing aims to replace the factual association stored in model parameters with a new factual association \((s,r,o_{edit})\), where \(o_{edit}\) is the counterfactual target object~(e.g., \textit{New York}).
In contrast, we denote $o_{true}$ as the true answer in the real world.
Existing works~\citep{rome,memit, mend,malmen} mainly evaluate knowledge editing in terms of \textit{reliability}, \textit{generalization} and \textit{specifictiy} with the following metrics:

\begin{enumerate}[label=\arabic*), itemsep=2pt, wide=0pt, leftmargin=*, after=\strut]
    \item \textit{Efficacy Score~(ES)} and \textit{Efficacy Magnitude~(EM)} represent the portion of cases that \(P(o_{edit}|s,r) > P(o_{true}|s,r)\) and the mean \(P(o_{edit}|s,r)\), respectively.

    \item \textit{Paraphrase Score~(PS)} and \textit{Paraphrase Magnitude~(PM)} evaluate generalization performance for a paraphrased prompt \((s_{para}, r_{para})\)~(e.g., ``The Tower of Eiffel stands in''). 
    These two metrics can be written as \(P(o_{edit}|s_{para},r_{para}) > P(o_{true}|s_{para},r_{para})\) and \(P(o_{edit}|s_{para},r_{para})\).

    \item \textit{Neighborhood Score~(NS)} and \textit{Neighborhood Magnitude~(NM)} evaluate specificity by providing a neighboring but distinct subject \(s^\prime\)~(e.g., ``The Louvre'').
    % For example, replacing ``Eiffel Tower'' with ``Louvre Museu'', the edited model should ideally provide the true answer \(o_{true}\)(``Paris''), not the edited \(o_{edit}\)(``New York''). 
    These two metrics can be represented as \(P(o_{true}|s^\prime,r) > P(o_{edit}|s^\prime,r)\) and \(P(o_{true}|s^\prime,r)\), respectively.
\end{enumerate}

As recent studies~\citep{hoelscher2023detecting, yao2023editing} show that the presence of edited subjects in the inference context can deteriorate specificity performance, we incorporate two additional specificity metrics that include the edited subject in the test prompts.

\begin{enumerate}[label=\arabic*), itemsep=2pt, wide=0pt, leftmargin=*, after=\strut]
    \item \textit{Relation Score~(RS)} and \textit{Relation Magnitude~(RM)} evaluate how the model handles attributes of the edited subject that are unrelated to the edit, identified by the relation $r'$.
    Empirically, we find the edited model tends to predict the object $o_{edit}$ for even unrelated relations, e.g., ``The color of Eiffel Tower is New York''.
    Therefore, we calculate the Relation Score by $P(o_{true}|s,r^\prime) > P(o_{edit}|s,r^\prime)$ and the Relation Magnitude by $P(o_{true}|s,r^\prime)$.

    \item \textit{Distract Neighborhood Score~(DNS)} and \textit{Distract Neighborhood Magnitude~(DNM))} function similarly to NS and NM but concatenate the edited sentence $(s, r, o_{edit})$ before the test prompt in the neighborhood task. 
    % For instance, “Eiffel Tower is located in New York. The Louvre Museum is located in”.
    These metrics can be written as \(P(o_{true}|(s,r,o_{edit}) \oplus (s^\prime,r)) > P(o_{edit}|(s,r,o_{edit}) \oplus (s^\prime,r))\) and \(P(o_{true}|(s,r,o_{edit}) \oplus (s^\prime,r))\), respectively.
    % The NES is represented by the proportion \(P(o_{true}|(s,r,o_{edit}) \oplus (s^\prime,r)) > P(o_{edit}|(s,r,o_{edit}) \oplus (s^\prime,r))\) and NEM is represented by \(P(o_{true}|(s,r,o_{edit}) \oplus (s^\prime,r))\), where $\oplus$ represents string concatenation.
\end{enumerate}


\begin{wrapfigure}{r}{0.475\textwidth}
    \vspace{-5pt}
    \begin{AcademicBox}[\footnotesize CounterFact Example]
    \small
    \textbf{Request Editing:} (Eiffel Tower, is located in, Paris) $\rightarrow$ (Eiffel Tower, is located in, New York) \\
    \textbf{Editing Prompt:} Eiffel Tower is located in \\
    \textbf{Editing Target:} New York \\
     \hrule \vspace{4pt}
    \textbf{Efficacy Task~(ES):} Eiffel Tower is located in  \\
    \textbf{Parapharse Task~(PS):} Eiffel Tower stands in  \\
    \textbf{Neighborhood Task~(NS):} The Louvre Museum is located in  \\
    \textbf{Relation Task~(RS):} The color of Eiffel Tower is  \\
    \textbf{Distract Neighborhood Task~(DNS):}  Eiffel Tower is located in New York. The Louvre Museum is located in  \\
     \vspace{-10pt}
    \end{AcademicBox}
    \caption{Example of evaluation tasks for Knowledge Editing.}
    \label{fig:datapoint}
\end{wrapfigure}

% + motivation
We provide an example in Figure~\ref{fig:datapoint} to illustrate how to evaluate knowledge editing.
Although knowledge editing involves various settings, such as batch~\citep{memit} and sequential editing~\citep{hartvigsen2024aging}, we find that even editing a single factual association can significantly damage specificity performance when the edited subject occurs in the context.
We call this \textbf{Specificity Failure} and focus on knowledge editing that modifies one factual association in this work.



\subsection{The Knowledge Editing Framework} 
\label{sec:edit_framework}
To illustrate how knowledge editing methods work, we describe ROME~\citep{rome} here as it is a foundational method that inspired subsequent techniques such as MEMIT~\citep{memit}, PMET~\citep{pmet}, and others~\citep{pmet, bird}.

\citet{geva2021transformer} reveals that MLP layers can serve as two-layer key-value memories, with the output of the first MLP layer serving as $k$, and the output from the second layer acting as $v$, thereby facilitating knowledge retrieval about entities.
Inspired by this, ROME changes $v$ to facilitate the model’s prediction of $o_{edit}$ when $k$ is associated with the target subject.
Meanwhile, it preserves $v$ as much as possible when $k$ is not related to the target subject, ensuring stability across unrelated contexts. 
To achieve this, ROME implements a rank-one update on weight $W$ of the second MLP layer with these two main objectives:
\begin{enumerate}[label=\arabic*, itemsep=1pt, wide=0pt, leftmargin=*]
% \vspace{-5pt}
\item Minimize $\lVert \hat{W}k - Wk \rVert$ when $k$ is not from the last token of the target subject.
\item Satisfy $\hat{W}k^* = v^*$ when $k^*$ corresponds to the output of the last token of the target subject, while $v^* = \argmin_z \left( -\log P_{G(m^{l^*}_{t}:=z)}\left(o_{edit}|s,r\right) + \omega D_{\mathrm{KL}}\left(P(x|p') \big\Vert P_{G(m^{l^*}_{t}:=z)}(x|p')\right) \right)$.
% \vspace{-5pt}
\end{enumerate}
Herein, the second objective seeks a vector $z$ that, when substituted as the output of the MLP  in layer $l$ at token $t$~(denoted $G(m^{l^*}_{t}:=z)$), it will lead the model to predict $o_{edit}$. 
$t$ is the last token of the subject.
The KL divergence term minimizes the distances of predicted distribution between the predicted distributions for the prompt $p^\prime$~(formatted as “{subject} is a”) before and after editing, which controls the essence drift~\citep{rome}.
$\omega$ is denoted as the controlling weight. 
ROME integrates these objectives by solving a linear system. 

The general framework of locate-then-edit knowledge editing mentioned above can be viewed as first optimizing a certain vector $v^*$ that facilitates predicting the new knowledge, then integrating the vector $v^*$ into the model's parameters.
We primarily focus on locate-then-edit knowledge editing in this paper, analyzing the causes of specificity failure within this framework. 
Additionally, we also evaluate specificity failure and the effectiveness of our SADR method on other parameter-preserving and meta-learning approaches.
