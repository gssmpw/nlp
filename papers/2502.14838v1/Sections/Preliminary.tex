\section{Exploration of Specificity Failure}
\label{sec:preliminary}
While knowledge editing excels at memorizing new knowledge, 
% and generalizing well across different scenarios, 
it still suffers from specificity failure.
We first measure these failures on CounterFact benchmarks and then identify which intermediate outputs during the edited model's inference cause incorrect predictions.
We further explore the primary triggers for these errors at a granular level and verify our findings by patching attention drift to mitigate specificity failure.
Our experiments focus on \textit{Relation} and \textit{Distract Neighborhood} tasks, employing the widely-used ROME method on the GPT-J-6b model~\citep{gpt-j}.
\subsection{Calibrating Specificity Failure on Counterfactual Benchmarks}
\label{sec:fail_case}
To measure performance on both the \textit{Relation} and \textit{Distract Neighborhood} tasks, we employ a dataset composed of \textsc{CounterFact}~\citep{rome} and \textbf{WikiData}$_{counterfact}$~\citep{zhang2024comprehensive}.
The dataset includes 1683 factual statements, with more details provided in Appendix~\ref{app:data_details}. 

\input{tabels/failure_summary}

Table~\ref{tab:fail} illustrates a significant specificity failure when the edited subject occurs in the context, with the edited model incorrectly outputting the edited object in over 50\% of test cases. 
Furthermore, the average probability of incorrect answers $o_{edit}$ is much greater than that of correct answers $o_{true}$, as 48.4\% versus 3.3\% for the \textit{Relation} task and 24.9\% versus 10.5\% for the \textit{Distract Neighborhood} task when editing with ROME.
Editing methods such as MEMIT~\citep{memit} and PMET~\citep{pmet} also display significant specificity failure. This failure is further observed across a range of data formats and tasks, with detailed results provided in Appendix~\ref{app:more dataset}.


\subsection{Localizing Specificity Failure}
\label{sec:localize_failure}
In the forward pass of an autoregressive language model, the flow of information can be viewed as a causal graph~\citep{edit_relation}. 
When a model with $L$ layers predicts based on a prompt containing $T$ tokens, each module such as attention modules, MLPs, and transformer blocks, produces $T \times L$ outputs.
Each of these outputs is influenced by prior outputs from earlier layers and preceding token positions.
Inspired by causal tracing~\citep{rome}, we trace across different states in the causal graph to identify which parts contaminate the information flow in specificity failure.

We first conduct a forward pass on the edited model with test prompts and record outputs from various network modules across different layers and token positions.
Then, we execute a forward pass with the vanilla model, copying the representations of specific modules from the stored outputs to the corresponding location without altering other computations.
We traverse modules within a window of $k$ layers for each $l$-th layer and token position $t$.
We refer to this approach as ``Contaminating Substitution'' and quantify its impact on the final output probability, formulated as:
\begin{align*}
    % \vspace{1pt}
    \hspace{-0.5pt}\textrm{Tracing Effect} \hspace{-1pt}=\hspace{-3pt} \ P_{G(module^{l}_{t}:=z)}(o_{true}|(s, r, o_{edit}) \oplus (s',r)) \hspace{-1pt}-\hspace{-1pt} 
    P(o_{true}|(s, r, o_{edit}) \oplus (s',r)),
    % \vspace{1pt}
\end{align*} % Here
where $G(module^{l}_{t}:=z)$ denotes the substitution of $z$ for the output of modules at token $t$ in layer $l$.
\begin{figure}[htbp]
    % \vspace{-5pt}
  \centering
  \includegraphics[width=1\textwidth]{figs/causal_trace/replace_true.pdf}
  \caption{Visualizing ``Contaminating Substitution'' when replacing the output of specific modules on test prompts of the \textit{Distract Neighborhood} task with window size 6.} %\protect\footnotemark}
  \label{fig:causal_replace_truth}
   % \vspace{-10pt}
\end{figure}

As shown in Figure~\ref{fig:causal_replace_truth}, the light-colored areas represent the primary states that lead to incorrect answers. 
We observe that replacing six layers of MLP activations or Attention activations can decrease the probability of a correct answer by up to 4.59\% and 3.74\%, respectively, while the total decrease caused by the edited model is 5.26\%.
As we edit the MLP module in the 5th layer, it is expected that the contaminating substitution of MLP activations near the edited layer significantly influences the final predictions.
However, modifying the middle-upper layers of attention activations also has a similar impact on the correct output, suggesting that the primary cause of specificity failure is the attention module mishandling the information at the last token due to the edits.
The findings are consistent on the \textit{Relation} task, with additional experiments exploring the tracing effects with varying window sizes and prediction probability of $o_{edit}$ detailed in Appendix~\ref{app:localize_module}.



\subsection{Identifying Attention Drift as a Trigger for Specificity Failure}
\label{sec:trigger}
\begin{figure}[htb]
    \centering

    \begin{subfigure}[b]{0.48\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figs/W_correlation/NES-W-P_edit-scatter.pdf}
        \label{fig:sub1}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.48\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figs/W_correlation/R-W-P_edit-scatter.pdf}
        \label{fig:sub2}
    \end{subfigure}
    \caption{The correlation between the attention weight drift and $P(o_{edit})$ is positive on \textit{Distract Neighborhood} ($\rho = 0.49; p<$\num{1e-5}) and \textit{Relation} ($\rho = 0.62; p<$\num{1e-5}) tasks. The range of $l$ corresponds to the layers where attention activations have a significant impact on the final results.} 
    \label{fig:W_correlation}

\end{figure}

As mentioned above, attention activations are one of the primary causes of specificity failures.
Previous studies~\citep{geva2023dissecting,chen2024journey,geva2022transformer} have indicated that attention modules in the middle-upper layers extract factual attributes during predictions.
This suggests that the attention module may mistakenly focus on edited information, thereby neglecting other information when predicting the final token.
Therefore, we quantify the relationship between drift in attention weights and failures in the \textit{Distract Neighborhood} and \textit{Relation} tasks using the Pearson coefficient.
Given that the edited model overestimates the probability of the edited object $o_{\text{edit}}$ relative to the true object $o_{\text{true}}$, we analyze the correlation between $\sum_{l}\sum_{h} D_{\text{KL}}(W_{l,h} \parallel W_{l,h}^*)$ and $P(o_{edit})$.
Here, $W_{l,h}$ and $W^*_{l,h}$ represent the attention weights that the last token attends to previous tokens in layer $l$ and head $h$ before and after editing, respectively.

Figure~\ref{fig:W_correlation} shows a positive relationship between the drift in attention weights and the probability of the incorrect answer $o_{edit}$, suggesting that incorrect attention on previous information is a key factor for specificity failures. 
To further analyze the attention drift from the perspectives of attended tokens and heads, it's natural to figure out the following two questions:
\begin{enumerate}[itemsep=0pt, wide=0pt, leftmargin=*]
\item The last token incorrectly allocates attention to previous tokens, leading to specificity failures. 
Among these previous tokens, which one has a significant impact when attended incorrectly? 
\item Which has a greater impact on the prediction: the excessive localized attention drift of specific heads or the cumulative attention drift across all heads?
\end{enumerate}

\input{tabels/W_correlation}
To address these questions, we calculate the Pearson coefficient between various factors and $P(o_{edit})$.
Table~\ref{tab:w_correlation} indicates that the Pearson coefficient for drift on the last token of the edited subject $s$ is higher compared to other tokens~(Equation 1 vs Equation 2).
Furthermore, the largest drift in attention weights among heads impacts the final result more than the cumulative drift across all heads~(Equation 1 vs Equation 2), suggesting that the excessive attention of certain heads to the last subject token primarily triggers specificity failure.

\subsection{Mitigating Specificity Failure by Patching Attention Drift}
\label{sec:patch_w}
To further verify the significant impact of attention drift on specificity failure, we quantify the change in prediction probability after patching attention weights across various layers.
We first conduct a forward pass using the vanilla model with prompts from the specificity tasks and store the intermediate attention weights.
Then, we test the edited model on the same prompts, substituting its attention weights with those previously stored.
\begin{figure}[htbp]
    % \vspace{pt}
  \centering
  \includegraphics[width=1\textwidth]{figs/patch_W/patched_weight.png}
  \caption{Impact of patching attention weights within a window size of 10 on the specificity tasks in the edited model. The performance of the original edited model is represented on the 0-th layer.}
  \label{fig:patch_w}
  % \vspace{-10pt}
\end{figure}

We find patching attention weights in middle-upper layers can lead to significant improvements in specificity tasks. 
As shown in Figure~\ref{fig:patch_w}, patching attention weights in 10 consecutive layers result in a relative increase of 28.6\% and 739.2\% in the probability of the correct answer $P(o_{true})$, and a decrease of 54.0\% and 89.6\% in the probability of the wrong answer $P(o_{edit})$ for two specificity tasks, respectively.
This shows preventing attention drift can effectively mitigate specificity failure.

\subsection{Takeaways}
\label{sec:pre_conclusion}
Based on the analysis mentioned above, we can conclude that: \textbf{(1)} the attention activations at the last token position significantly contaminate the forward pass of the edited model, causing specificity failures; \textbf{(2)} the max attention drift at the edited token position among heads is a primary trigger for the incorrect output $o_{edit}$, and \textbf{(3)} patching attention drift can largely mitigate the specificity failure. 