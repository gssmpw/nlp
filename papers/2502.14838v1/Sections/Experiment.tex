\section{Selectively Restraining Attention Drift During Knowledge Editing} 
\label{sec:method}
As mentioned in Section~\ref{sec:edit_framework}, the optimized value \(v_*\) for knowledge editing can be obtained through gradient descent based on the following objective: 
\begin{align}\label{eq:v-optimization}
\mathcal{L}(z) = \frac{1}{N} \sum_{j=1}^N \underbrace{-\log P_{G(\atl{m}{l^*}_{i}:=z)}(o_{edit} \mid x_j+ p) \,}_\text{(a) Maximizing $o_{edit}$ probability} \; + \;  \underbrace{\omega D_{\mathrm{KL}}\left(P(x \mid p^\prime) \big\Vert P_{G(\atl{m}{l^*}_{i^\prime}:=z)}(x \mid p^\prime)\right)}_\text{(b) Controlling essence drift}.
\end{align}

However, this objective may cause attention drift that leads to Specificity Failure. 
To enhance specificity in knowledge editing when optimizing $v^*$, we introduce the \textbf{S}elective \textbf{A}ttention \textbf{D}rift \textbf{R}estriction~(\textbf{SADR}), which is a regularization term based on Equation~\ref{eq:v-optimization}.
It is worth noting that SADR dynamically applies constraints to different heads as needed since Transformer models contain various knowledge-specific attention heads~\citep{gpt2wild, geva2023dissecting} that capture different factual associations.
Additionally, SADR is a simple yet efficient method and can be flexibly adapted across various editing methods. 

More concretely, as excessive attention to the edited subject of certain heads is strongly correlated with Specificity Failures, we apply SADR on heads where the last token overly focuses on the edited subject. We determine which heads to restrain by the following criterion: \textbf{a head is selected if the attention weight attending to the subject's last token exceeds the maximum attention weight among all heads in the vanilla model}.

Let \( W_{l,h}(S) \) be the attention weight from layer \( l \) and head \( h \) when processing the prompt \( S \), \(W^{G(\atl{m}{l^*}_{i}:=z)}_{l,h}(S)\) be the attention weight from the model that is edited with \(z\), and \( M_{l}(S) = \max_h W_{l,h}(S)[-1,s] \) be the maximum attention weight that the last token attends to the edited subject $s$ among all heads at layer \(l\) in the vanilla model. The objective of SADR can be written as:
\begin{equation}
\label{eq:w-restrain}
\begin{aligned}
\mathcal{L}_{SADR}(z) &= \frac{1}{N} \sum_{j=1}^N \sum_l \sum_{h \in H_{l}(S_j)} D_{\mathrm{KL}}\left(W_{l,h}(S_j)[-1,:]\big\Vert W^{G(\atl{m}{l^*}_{i}:=z)}_{l,h}(S_j)[-1,:]\right), \\
\end{aligned}
\end{equation}
where $H_{l}(S_j) = \{ h : W^{G(\atl{m}{l^*}_{i}:=z)}_{l,h}(S_j)[-1,s] > M_{l}(S_j) \}$ and $S_j = x_j \oplus (s, r)$.

Thus, the optimized value \( v_* \) can be obtained by: $v_* = \argmin \left(\mathcal{L}(z) +  \gamma \mathcal{L}_{SADR}(z)\right)$, where $\gamma$ is the controlling weight.

\section{Experiments}
\label{sec:experiment}
\subsection{Settings}
\paragraph{Dataset}
Due to the limited availability of datasets that satisfy the required fields for our tasks, we combine \textsc{CounterFact}~\citep{rome} and WikiData$_{counterfact}$~\citep{zhang2024comprehensive} with 1,683 factual statements as the testing data. The processing details are mentioned in Appendix~\ref{app:data_details}.
Additionally, we extend our experiments to broader datasets, including QA-format and recent knowledge editing tasks, as detailed in Appendix~\ref{app:more dataset}. The phenomena of Specificity Failure and the performance of SADR remain consistent across these datasets.

\paragraph{Baselines \& Models}
We evaluate the performance of our methods on three mainstream locate-then-edit knowledge editing baselines: ROME~\citep{rome}, MEMIT~\citep{memit}, and PMET~\citep{pmet}. 
Specifically, we focus on knowledge editing with one factual association for all the baselines.
We implement our SADR method across three editing baselines on the GPT-J-6b~\citep{gpt-j}, Llama3-8b~\citep{llama3modelcard} and GPT-NeoX-20b~\citep{black-etal-2022-gpt} models. 
In Appendix~\ref{app:main_results} and~\ref{app:more methods}, we also conduct SADR on more model variants and editing methods, including parameter-preserving and meta-learning approaches.
More details about baseline methods and our implementations can be found in Appendix~\ref{app:baseline_setting}.
% \textcolor{blue}{Our primary experiments are conducted on three mainstream locate-then-edit knowledge editing baselines: ROME~\citep{rome}, MEMIT~\citep{memit}, and PMET~\citep{pmet}, using GPT-J 6b and GPT-NeoX-20b as the base models. Additionally, we evaluate the extent of Specificity Failure and the effectiveness of SADR on the parameter-preserving method WISE~\citep{wang2024wise} and the meta-learning approach MEND~\citep{mend}. More details about the baseline methods and our implementations can be found in Appendix~\ref{app:baseline_setting}.}

\paragraph{Metrics} Apart from the metrics mentioned in Section~\ref{sec:edit_def}, we utilize Fluency Score (FL)~\citep{rome} to evaluate the generation ability of the edited model with prompts related to the edited subject, which is computed by the weighted mean of \textit{bi}-gram and \textit{tri}-gram entropies.
Results closer to that of the vanilla model indicate better performance.
To further test the generalizationâ€“specificity tradeoff, we report the harmonic mean of ES, PS, NS, RS, and DNS as the average score~(Avg. S).
We also test the model's commonsense reasoning abilities and its perplexity in language modeling, with results reported in the Appendix~\ref{app:main_results}.



\subsection{Main Results}
\label{sec:main_results}
\input{tabels/main_exp}

\paragraph{Specificity Failure is prevalent in existing knowledge editing methods.}  As shown in Table 4, the \textit{Relation} and \textit{Distract Neighborhood} tasks exhibit a significant decline across all editing methods, even though these methods perform edits at different layers and modules. Specifically, in the \textit{Relation} task, the accuracy of all edited models drops to less than half of their original performance. This indicates that specificity failure is a widespread and severe issue in knowledge editing.

\paragraph{SADR significantly mitigates Specificity Failure.} In the \textit{Relation} and \textit{Distract Neighborhood} tasks, SADR consistently improves the specificity across all editing methods. 
Notably, in over half of the setups, our method enhances the original specificity metrics by more than 50\% (marked in green). 
SADR also stabilizes the performance in the \textit{Neighborhood} task and the \textit{Fluency} of generated text, e.g., when using SADR on ROME and PMET with GPT-NeoX, the \textit{Fluency} score is significantly improved.
SADR can also achieve better performance on TinyLlama-1.1b~\citep{zhang2024tinyllama} and Llama2-13b~\citep{touvron2023llama} compared with baselines, indicating that our method is effective across models with various sizes and knowledge densities.
We provide more detailed results in Appendix~\ref{app:main_results} and report the human evaluation results in Appendix~\ref{app:human_eval}.

\paragraph{SADR has minimal impact on edit success rates.} SADR results in less than a 3\% decrease in performance on the \textit{Rewrite} and \textit{Generation} tasks. 
It is important to note that significantly mitigating specificity failure while fully preserving rewrite and generalization performance is quiet difficult. 
This is because in many previous model editing evaluation frameworks, the specificity failure highlighted in our paper is often overlooked. 
As a result, prior methods tend to prioritize generalization and rewrite scores, neglecting the risks of specificity failure. 
Under such evaluation criteria, high scores can be achieved by simply identifying the subject and greedily predicting the object, even if the relationship between them is completely ignored.

We believe that ensuring stable knowledge editing is more important than achieving nearly 100\% accuracy in generalization. 
In real-world scenarios, methods that achieve 97\% generalization with stable and safe edits are often more acceptable than those with 100\% generalization but significant specificity failures (e.g., severe knowledge errors caused by attention drift after editing the subject). 
Furthermore, while addressing specificity failure without compromising generalization is challenging, we demonstrate a better balance between these two aspects in Section~\ref{subsec:tradeoff}.

\section{Ablation Study}
\label{sec:ablation}

\subsection{Effect of restraining heads selection}
\begin{wrapfigure}{r}{0.5\textwidth}
    \vspace{-15pt}
    \includegraphics[width=0.5\textwidth]{figs/ablation/ablation_head3.pdf}
    % \vspace{-15pt}
    \caption{Impact of selective head restriction on Edit Success and Specificity performance}
    \label{fig:ablation_head}
    \vspace{-10pt}
\end{wrapfigure}
We first explore the effects of selectively restraining heads that exhibit significant attention drift compared to restraining all heads across various control weights $\gamma$ on ROME with GPT-J. 
Edit Success is quantified by the average of \textit{PS} and \textit{ES}, while Specificity is calculated as the average of \textit{NS}, \textit{RS}, and \textit{DNS}.
As shown in Figure~\ref{fig:ablation_head}, selectively restraining heads that over-focus on the edited token outperforms restraining all heads on both the Edit Success and Specificity across different $\gamma$ settings. 
This suggests that not all drift in attention is harmful; rather, it is excessive attention compared to the vanilla model that should be addressed.

\subsection{Trade-off between generalization and specificity}
\label{subsec:tradeoff}
\begin{figure}[htbp]
  \centering
  \includegraphics[width=1\textwidth]{figs/ablation/ablation_tradeoff2.pdf}
  \caption{Analysis of trade-offs by adjusting different hyperparameters.}
  \label{fig:tradeoff}
\end{figure}

Knowledge editing methods present a trade-off between Edit Success and Specificity, which can be visualized through adjustments in hyperparameters such as optimization steps, learning rate, and $\omega$ which controlling the essence drift. % clear ??
We analyze the trade-off of our method by varying $\gamma$ and compare it with the trade-offs from adjusting other hyperparameters in the original ROME on GPT-J.
As changes are more clear on $P(o_{edit})$ than the proportion of cases which $P(o_{edit}) > P(o_{true})$, we use the average of \textit{EM} and \textit{PM} to measure Edit Success and apply \textit{RM} and \textit{DNM} for evaluating \textit{Relation} and \textit{Distract Neighborhood} task, respectively.
Further details are illustrated in Appendix~\ref{app:ablation_details}.

Figure~\ref{fig:tradeoff} shows that our method exhibits a superior trade-off compared to the adjustments of hyperparameters in the original ROME method, indicating that \textbf{SADR} can obtain $v^*$ that enable the model to more effectively distinguish when to output the edited knowledge.

