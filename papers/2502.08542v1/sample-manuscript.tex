%%
%% This is file `sample-manuscript.tex',
%% generated with the docstrip utility.
%%
%% The original source files were:
%%
%% samples.dtx  (with options: `all,proceedings,bibtex,manuscript')
%% 
%% IMPORTANT NOTICE:
%% 
%% For the copyright see the source file.
%% 
%% Any modified versions of this file must be renamed
%% with new filenames distinct from sample-manuscript.tex.
%% 
%% For distribution of the original source see the terms
%% for copying and modification in the file samples.dtx.
%% 
%% This generated file may be distributed as long as the
%% original source files, as listed above, are part of the
%% same distribution. (The sources need not necessarily be
%% in the same archive or directory.)
%%
%%
%% Commands for TeXCount
%TC:macro \cite [option:text,text]
%TC:macro \citep [option:text,text]
%TC:macro \citet [option:text,text]
%TC:envir table 0 1
%TC:envir table* 0 1
%TC:envir tabular [ignore] word
%TC:envir displaymath 0 word
%TC:envir math 0 word
%TC:envir comment 0 0
%%
%% The first command in your LaTeX source must be the \documentclass
%% command.
%%
%% For submission and review of your manuscript please change the
%% command to \documentclass[manuscript, screen, review]{acmart}.
%%
%% When submitting camera ready or to TAPS, please change the command
%% to \documentclass[sigconf]{acmart} or whichever template is required
%% for your publication.
%%
%%
%\documentclass[manuscript,screen,review]{acmart}
\documentclass[manuscript]{acmart}
\settopmatter{printacmref=false}
%%
%% \BibTeX command to typeset BibTeX logo in the docs
\AtBeginDocument{%
  \providecommand\BibTeX{{%
    Bib\TeX}}}

%% Rights management information.  This information is sent to you
%% when you complete the rights form.  These commands have SAMPLE
%% values in them; it is your responsibility as an author to replace
%% the commands and values with those provided to you when you
%% complete the rights form.
%\setcopyright{none}
%\copyrightyear{2025}
%\acmYear{2025}
%\acmDOI{XXXXXXX.XXXXXXX}
%% These commands are for a PROCEEDINGS abstract or paper.
%\acmConference[FAcct '25]{ACM Conference on Fairness, Accountability, and Transparency 2025}{June 03--05,
 % 2018}{Woodstock, NY}
%%
%%  Uncomment \acmBooktitle if the title of the proceedings is different
%%  from ``Proceedings of ...''!
%%
%%\acmBooktitle{Woodstock '18: ACM Symposium on Neural Gaze Detection,
%%  June 03--05, 2018, Woodstock, NY}
%\acmISBN{978-1-4503-XXXX-X/2018/06}


%%
%% Submission ID.
%% Use this when submitting an article to a sponsored event. You'll
%% receive a unique submission ID from the organizers
%% of the event, and this ID should be used as the parameter to this command.
%%\acmSubmissionID{123-A56-BU3}

%%
%% For managing citations, it is recommended to use bibliography
%% files in BibTeX format.
%%
%% You can then either use BibTeX with the ACM-Reference-Format style,
%% or BibLaTeX with the acmnumeric or acmauthoryear sytles, that include
%% support for advanced citation of software artefact from the
%% biblatex-software package, also separately available on CTAN.
%%
%% Look at the sample-*-biblatex.tex files for templates showcasing
%% the biblatex styles.
%%

%%
%% The majority of ACM publications use numbered citations and
%% references.  The command \citestyle{authoryear} switches to the
%% "author year" style.
%%
%% If you are preparing content for an event
%% sponsored by ACM SIGGRAPH, you must use the "author year" style of
%% citations and references.
%% Uncommenting
%% the next command will enable that style.
%%\citestyle{acmauthoryear}

\usepackage{cancel}
%%
%% end of the preamble, start of the body of the document source.
\begin{document}

%%
%% The "title" command has an optional parameter,
%% allowing the author to define a "short title" to be used in page headers.
\title{Beyond Predictions: A Participatory Framework for Multi-Stakeholder Decision-Making}

%%
%% The "author" command and its associated commands are used to define
%% the authors and their affiliations.
%% Of note is the shared affiliation of the first two authors, and the
%% "authornote" and "authornotemark" commands
%% used to denote shared contribution to the research.
\author{Vittoria Vineis}
\email{vineis@diag.uniroma1.it}
\affiliation{%
  \institution{Sapienza University of Rome}
  %\city{Dublin}
  %\state{Ohio}
  \country{Italy}
}

\author{Giuseppe Perelli}
\affiliation{%
  \institution{Sapienza University of Rome}
  %\city{Hekla}
  \country{Italy}}
\email{perelli@di.uniroma1.it}

\author{Gabriele Tolomei}
\affiliation{%
  \institution{Sapienza University of Rome}
  %\city{Hekla}
  \country{Italy}}
\email{tolomei@di.uniroma1.it}


%%
%% By default, the full list of authors will be used in the page
%% headers. Often, this list is too long, and will overlap
%% other information printed in the page headers. This command allows
%% the author to define a more concise list
%% of authors' names for this purpose.
\renewcommand{\shortauthors}{Vineis et al.}

%%
%% The abstract is a short summary of the work to be presented in the
%% article.
\begin{abstract}
Conventional decision-support systems, primarily based on supervised learning, focus on outcome prediction models to recommend actions. However, they often fail to account for the complexities of multi-actor environments, where diverse and potentially conflicting stakeholder preferences must be balanced. 
\\
In this paper, we propose a novel participatory framework that redefines decision-making as a multi-stakeholder optimization problem, capturing each actor's preferences through context-dependent reward functions.
Our framework leverages $k$-fold cross-validation to fine-tune user-provided outcome prediction models and evaluate decision strategies, including compromise functions mediating stakeholder trade-offs. We introduce a synthetic scoring mechanism that exploits user-defined preferences across multiple metrics to rank decision-making strategies and identify the optimal decision-maker. The selected decision-maker can then be used to generate actionable recommendations for new data.
We validate our framework using two real-world use cases, demonstrating its ability to deliver recommendations that effectively balance multiple metrics, achieving results that are often beyond the scope of purely prediction-based methods. Ablation studies demonstrate that our framework, with its modular, model-agnostic, and inherently transparent design, integrates seamlessly with various predictive models, reward structures, evaluation metrics, and sample sizes, making it particularly suited for complex, high-stakes decision-making contexts.
\end{abstract}

%%
%% The code below is generated by the tool at http://dl.acm.org/ccs.cfm.
%% Please copy and paste the code instead of the example below.
%%
\begin{CCSXML}
<ccs2012>
   <concept>
       <concept_id>10010147.10010257.10010258</concept_id>
       <concept_desc>Computing methodologies~Learning paradigms</concept_desc>
       <concept_significance>500</concept_significance>
       </concept>
   <concept>
       <concept_id>10010147.10010341.10010342.10010343</concept_id>
       <concept_desc>Computing methodologies~Modeling methodologies</concept_desc>
       <concept_significance>500</concept_significance>
       </concept>
   <concept>
       <concept_id>10010147.10010341.10010342.10010344</concept_id>
       <concept_desc>Computing methodologies~Model verification and validation</concept_desc>
       <concept_significance>500</concept_significance>
       </concept>
\end{CCSXML}

\ccsdesc[500]{Computing methodologies~Learning paradigms}
\ccsdesc[500]{Computing methodologies~Modeling methodologies}
\ccsdesc[500]{Computing methodologies~Model verification and validation}
\ccsdesc[500]{Human-centered computing~Collaborative and social computing theory, concepts and paradigms}

%%
%% Keywords. The author(s) should pick words that accurately describe
%% the work being presented. Separate the keywords with commas.
\keywords{Participatory Artificial Intelligence, Participatory Training, Multi-Stakeholder Decision-Making}

%\received{20 February 2007}
%\received[revised]{12 March 2009}
%\received[accepted]{5 June 2009}

%%
%% This command processes the author and affiliation and title
%% information and builds the first part of the formatted document.
\maketitle

\section{Introduction}
\label{sec:intro}
Advances in artificial intelligence (AI) and machine learning (ML) have fueled the development of automated decision support systems in high-stakes domains such as healthcare, finance, and public policy \citep{chiusi2020automating}. However, the predominant \emph{top-down} design paradigm and the traditional optimization-oriented setup for these systems are increasingly under scrutiny, particularly due to their tendency to overlook critical stakeholder perspectives and broader societal impacts \citep{gerdon2022social}. In many cases, algorithmic solutions emphasize predictive accuracy above all else, neglecting or underperforming in aspects such as fairness \citep{petersen2023assessing}, transparency \citep{grimmelikhuijsen2023explaining}, and the consideration of heterogeneous and potentially conflicting interests \citep{laufer2023optimization}. This narrow focus on outcome prediction has led to \emph{impact-blind} recommendations, which risk perpetuating biases and exacerbating inequities \citep{barocas2016big}, highlighting the need to regulate and promote the creation of more trustworthy systems \citep{lepri2018fair, floridi2021ethics, council2024regulation}.  For example, in healthcare, predictive models used for diagnosing diseases or allocating resources often fail to account for disparities in access to medical services or variations in patient demographics, leading to biased outcomes that disproportionately harm marginalized communities \citep{obermeyer2019dissecting}. Similarly, in finance, credit scoring algorithms frequently rely on historical data embedded with systemic biases, which can unjustly exclude individuals from economic opportunities \citep{hardt2016equality}, while also ignoring the broader effects such choices may have on the wider social and economic system.

Recent efforts to mitigate these issues often involve post-hoc fairness adjustments or the adoption of generalized fairness metrics (e.g., \cite{mehrabi2021survey, pessach2022review}). While these approaches represent steps toward more equitable AI, they often fall short of addressing the nuanced, context-dependent priorities of real-world stakeholders. In particular, many current solutions remain \emph{single-actor} in design -- treating the decision-making process as if there were only one objective -- thereby missing the opportunity to model and reconcile multiple, potentially conflicting objectives in a transparent manner. In this context, recent developments in \textbf{participatory approaches to AI design} offer promising avenues for improvement \citep{delgado2023participatory}. However, in most cases, these approaches still lack solutions that are versatile enough for diverse use cases.

To address these limitations, in this paper we introduce a novel \emph{multi-actor} decision framework that reinterprets the offline training phase of standard predictive models used to inform decisions as a multi-stakeholder optimization problem. 
%By incorporating insights from reward-driven learning, game theory, and computational social choice, our framework explicitly models the diverse preferences of stakeholders and provides a principled means to balance trade-offs among competing objectives. 
This shift from a single-objective to a multi-actor perspective is particularly important in domains where disparate needs and values must be accommodated and justified.

Our proposed framework is characterized by several key features which make it particularly suitable for applications in high-stakes domains: (i) it explicitly models stakeholder interests by \textbf{directly encoding diverse preferences into the decision-making process}, ensuring the system reflects multiple viewpoints and priorities; (ii) it incorporates a \textbf{compromise-driven action selection mechanism} to identify actions that balance trade-offs across diverse objectives; (iii) its \textbf{model-agnostic flexibility} enables seamless adaptation to various predictive models and application contexts, making it suitable for a wide range of real-world scenarios; and (iv) it is \textbf{inherently explainable and transparent}, maintaining an interpretable pipeline and outputs that clarify how different objectives influence the final decision and how conflicts are resolved.

Overall, this work offers the following novel contributions:
\begin{itemize}
\item We bridge foundational contributions from reward-based learning, game theory, welfare economics, computational social choice, and optimization \textbf{to advance the formalization of participatory AI solutions}.
\item We introduce a \textbf{theoretically grounded framework that overcomes the limitations of traditional single-perspective prediction-based systems} by systematically modeling and reconciling diverse and potentially conflicting objectives, while providing context-aware solutions.
\item We demonstrate the \textbf{effectiveness and generalizability of the proposed framework through rigorous experiments on two real-world case studies}, showing how incorporating stakeholder diversity into the AI training pipeline improves decision-making outcomes compared to purely predictive baselines when evaluated across a diverse set of metrics.
\item To enhance transparency and facilitate broader adoption, we provide \textbf{complete access to the source code and experimental setup}, enabling reproducibility and promoting its  application on new use cases.
\end{itemize}

The remainder of the paper is organized as follows. Section~\ref{sec:related} provides a review of the relevant literature. In Section~\ref{sec:framework}, we present our proposed framework, which is validated through extensive experiments in Section~\ref{sec:experiments}. 
Section~\ref{sec:implications} outlines key practical implications of our framework, while Section~\ref{sec:conclusion} concludes the paper, mentioning potential directions for future research.
\section{Related Work}
\label{sec:related}
The topics of \textbf{fairness and accountability in automated decision-making (ADM) systems} have garnered significant attention among scholars and practitioners due to their increasingly pervasive deployment in multiple domains \citep{chiusi2020automating} and their potential social impact \citep{araujo2020ai}. Research in this area has extensively explored pre-, in-, and post-processing techniques to mitigate biases in data and modeling pipelines (for comprehensive reviews see, for instance, \cite{mehrabi2021survey, pessach2022review}), contributing to the promotion of fairness in algorithmically supported decision systems. However, studies have demonstrated that fair algorithms alone cannot guarantee fairness in practice~\citep{goel2021importance, jeong2022fairness}, and aspects as interpretability and fairness are inherently interdependent factors adding complexity to their operationalization in real-world AI systems~\citep{dodge2019explaining, schoeffer2022relationship, ramachandranpillai2023fairxai, jain2020biased}.

The multifaceted nature of fairness is further influenced by cultural and social contexts, complicating efforts to develop fairness frameworks that extend beyond pre-defined universal metrics~\citep{selbst2019fairness}. Moreover, some authors argue that, while fairness-aware methods often succeed in reducing biases in model outputs, they frequently fail to address systemic inequities and, for this reason, approaches that more effectively incorporate diverse stakeholder interests and account for broader societal impacts are needed to promote more equitable social outcomes~\citep{gerdes2022participatory}.

Especially in dynamic and interactive decision-making settings, where multiple actors’ interests come into play, \textbf{multi-agent systems} may offer a promising framework. Contributions to multi-agent reinforcement learning~\citep{zimmer2021learning} and multi-agent multi-armed bandit frameworks~\citep{hossain2021fair} leverage, for instance, welfare functions to foster fairness in multi-agent decision settings. In this context, Wen et al.~\citep{wen2021algorithms} advance these efforts by integrating feedback effects into Markov decision processes, enabling the modeling of dynamic, long-term impacts of decisions on fairness outcomes, as demonstrated through a loan approval scenario. Despite their valuable contribution to the integration of the presence of multiple actors in ADM systems, though, most approaches rely on predefined fairness definitions and require specific problem structures, limiting their adaptability to evolving stakeholder preferences and use-case-specific requirements.

Against this backdrop, \textbf{Participatory AI} has emerged as a significant paradigm for integrating diverse stakeholder perspectives throughout the AI lifecycle, offering opportunities to foster context-dependent fairness and promote accountability \citep{birhane2022power}. This paradigm emphasizes collaboration and co-creation, promoting inclusivity across both technical and non-technical domains~\citep{hossain2021towards, berditchevskaia2021participatory}. Contributions in this area span diverse fields, including healthcare~\citep{donia2021co}, judicial systems~\citep{barabas2020studying}, civic engagement~\citep{arana2021citizen}, philanthropy~\citep{lee2019webuildai}, and urban planning~\citep{quan2019artificial}. More technical applications include collective debiasing~\citep{chan2024group}, collaborative debugging~\citep{nakao2022toward}, ranking with partial preferences~\citep{cachel2024prefair} and web-based tools for democratizing ML workflows~\citep{zhang2023deliberating} . Collectively, these contributions reflect what has been described as a "participatory turn" in AI design~\citep{delgado2023participatory}.

However, current challenges such as the technical complexity of AI systems, structural and social barriers to participation, and power asymmetries hinder broader adoption and expose some applications to the risk of what has been described as "participation washing"~\citep{sloane2022participation}. Consequently, the scalability and generalizability of Participatory AI remain limited~\citep{delgado2023participatory}, compounded by the lack of flexible, generalizable frameworks that can be applied to a wide range of use cases.


\section{The Participatory Training Framework}
\label{sec:framework}
\subsection{Problem Setting}

In traditional prediction-oriented decision-making systems the goal is to recommend an action to be taken based on a set of features and a predicted outcome. In contrast to these approaches, which rely solely on outcome predictions, we formulate the task of suggesting an action as a \emph{multi-actor decision-making problem}, where each actor has individual preferences over possible actions and outcomes.

Formally, let $\mathcal{I} = \{1, 2, \dots, N\}$ represent a set of stakeholders or actors. Without loss of generality, in our framework, we define an actor as any real or symbolic entity that is influenced by the decisions suggested by the system and, therefore, holds a direct stake in its resulting outputs. In a grant-lending scenario, the categories of actors might include the financial institution and its clients, whereas in a healthcare context, they could be represented by the hospital and the patients. In our context, each actor $i \in \mathcal{I}$ evaluates potential actions based on their own preferences and seeks outcomes that align with these preferences. The decision space consists of a set of possible actions $\mathcal{A} = \{a_1, a_2, \dots, a_k\}$ and a set of feasible outcomes $\mathcal{O} = \{o_1, o_2, \dots, o_m\}$. This decision-making process occurs within a context $\boldsymbol{x} \in \mathcal{X}$, representing exogenous conditions or features that can influence the resulting outcome.

Figure~\ref{fig:diagram} provides a high-level view of our proposed framework, illustrating how it builds upon and extends a traditional ML pipeline for automated-supported decision-making. The white components depict the standard ML pipeline for predictive modeling, while the green components show the additional elements introduced by our approach. The diagram encompasses both the (offline) training phase, where historical data are used, and the (online) inference phase, demonstrating how the framework operates with new data.
\begin{figure}[ht]
    \centering
    \includegraphics[width=0.65\linewidth]{partTraining.drawio.png}
    \caption{Overview of the proposed framework as integration of the traditional ML decision-support pipeline}
    \label{fig:diagram} % Label for referencing
\end{figure}

At its core, our framework is \emph{domain- and model-agnostic} and designed to integrate multiple actor perspectives through \emph{dynamic reward modeling} and \emph{principled decision-making strategies}. It evaluates and ranks decision-making strategies based on performance across diverse \emph{evaluation metrics}, addressing heterogeneous stakeholder preferences. As for its outputs, our framework expands upon traditional approaches by preserving the strengths of purely prediction-based decision support systems while offering enhanced context-awareness, greater transparency, and a more comprehensive representation of diverse perspectives. In other words, the proposed method does not diminish the capabilities of prediction-based decision systems; instead, it complements their suggested decisions with a set of viable alternatives, thus enriching the expressivity of the decision-making process.

Action spaces and outcomes are handled flexibly: the action space \(\mathcal{A}\) may be finite or continuous, and outcomes \(\mathcal{O}\) may be discrete, continuous, or discretized from a continuous domain. When \(\mathcal{O}\) is discrete, a classification-oriented formulation arises; for a continuous \(\mathcal{O}\), a regression-oriented model is employed. Without loss of generality, continuous outcomes can be discretized into a finite set of intervals, yielding a unified perspective for both discrete and continuous cases. All components of the framework and its end-to-end workflow are discussed below.


\subsection{Core Components of the Framework}

\subsubsection{Actor-Based Rewards and Payoff Matrices}
A central element of our framework is the assignment of \emph{rewards} to each action-outcome pair, extending conventional prediction-focused methods to address the diverse preferences of multiple stakeholders. Inspired by reward-based learning approaches, this mechanism leverages feedback signals to guide decision-making in a manner consistent with stakeholder priorities.

Formally, each actor \(i \in \mathcal{I}\) specifies a reward function:
\[
R_i : \mathcal{X} \times \mathcal{A} \times \mathcal{O} \to [0,1],
\]
which assigns a score between 0 and 1 to every action-outcome pair \((a, o)\), given the vector of contextual information $\boldsymbol{x}$. Higher values of \(R_i(\boldsymbol{x}, a, o)\) indicate outcomes more desirable to actor \(i\).

From a computational standpoint, \(R_i(\boldsymbol{x}, a, o)\) may be derived via static, domain-specific rules or through a learned function \(q_i(\boldsymbol{x}, a, o)\) that allows to predict the reward that each actor would associate to each triplet $(\boldsymbol{x},a,o)$, based on a set of rewards associated to the historical set. In the latter case, in fact, \(q_i\) approximates \(R_i\) by training on historical records of actions, outcomes, and contextual features, subject to bounded approximation error. This flexible design supports both expert-driven reward models and data-driven approaches. Similarly, the historical rewards used to train actor-specific reward models can be collected from real-world data or generated synthetically -- for instance, using a Large Language Model prompted to mimic the decision-making of a particular actor category\footnote{Refer to Section \ref{sec:implications} for further insights and implications related to modeling actors' reward functions.}.

In scenarios where actions and outcomes are discrete or discretized, we arrange these rewards into a \textit{payoff matrix}. Here, the rows correspond to actions, the columns represent outcomes, and each matrix cell specifies the reward value associated with a particular \((a,o)\) tuple given the context \(\boldsymbol{x}\):

\[
\mathbf{R}_i(\boldsymbol{x}) =
\begin{bmatrix}
R_i(\boldsymbol{x}, a_1, o_1) & R_i(\boldsymbol{x}, a_1, o_2) & \dots & R_i(\boldsymbol{x}, a_1, o_m) \\
R_i(\boldsymbol{x}, a_2, o_1) & R_i(\boldsymbol{x}, a_2, o_2) & \dots & R_i(\boldsymbol{x}, a_2, o_m) \\
\vdots           & \vdots           & \ddots & \vdots           \\
R_i(\boldsymbol{x}, a_k, o_1) & R_i(\boldsymbol{x}, a_k, o_2) & \dots  & R_i(\boldsymbol{x}, a_k, o_m)
\end{bmatrix}.
\]


If either \(\mathcal{A}\) or \(\mathcal{O}\) is continuous, this matrix-based representation extends naturally to a continuous function \(R_i(\boldsymbol{x}, a, o)\), provided \(R_i\) remains well-defined for all \((a, o)\).
 
\subsubsection{Outcome Predictions and Expected Rewards}
Another key component of the framework, which serves as the cornerstone of traditional approaches to automated-supported decision systems, is the outcome prediction model. This model learns to predict outcomes based on historical data comprising context vectors and past actions. Formally, it can be represented as a predictive function:
\[
f : \mathcal{X} \times \mathcal{A} \to \Delta(\mathcal{O})
\quad \text{or} \quad
f : \mathcal{X} \times \mathcal{A} \to \mathbb{R},
\]
depending on whether the task is cast as classification or regression, where \( \Delta(\mathcal{O}) \) is the probability simplex over \( \mathcal{O} \).

An innovative aspect of our framework lies in its integration of predicted outcomes to compute the expected reward for each actor. Specifically, the predicted outcomes are combined with the actor’s reward function \(R_i\) to evaluate the desirability of selecting action \(a\) given context \(\boldsymbol{x}\). For discrete or discretized outcomes, this is expressed as:
\[
\mathbb{E}[R_i(a \mid \boldsymbol{x})] = \sum_{q=1}^m P(o_q \mid \boldsymbol{x}, a) \, R_i(\boldsymbol{x}, a, o_q),
\]
where \(P(o_q \mid \boldsymbol{x}, a)\) is derived from the prediction model \(f\). For regression tasks with continuous outcomes, the expectation generalizes to:
\[
\mathbb{E}[R_i(a \mid \boldsymbol{x})] = \int_{\mathcal{O}} R_i(\boldsymbol{x}, a, o) \, p(o \mid \boldsymbol{x}, a) \, do,
\]
where \(p(o \mid \boldsymbol{x}, a)\) denotes the predicted outcome density. Alternatively, in deterministic regression models where \(f(\boldsymbol{x}, a)\) directly outputs a single most likely outcome \(\hat{o}\), the expected reward simplifies to:
\[
\mathbb{E}[R_i(a \mid \boldsymbol{x})] = R_i(\boldsymbol{x}, a, \hat{o}).
\]

Regardless of the specific prediction approach, the expected rewards for actor \(i\) across all actions can be aggregated into a vector:
\begin{equation}
\mathbb{E}[R_i(a \mid \boldsymbol{x})] =
\begin{bmatrix}
\mathbb{E}[R_i(a_1 \mid \boldsymbol{x})], \dots, \mathbb{E}[R_i(a_k \mid \boldsymbol{x})]
\end{bmatrix}^\top,
\label{eq:expected_reward}
\end{equation}
providing a concise representation of the actor’s preferences over the action space \(\mathcal{A}\).

\subsubsection{Decision Strategies}

Once the vectors of actor-specific expected rewards are computed based on the predicted outcomes, a set of decision strategies can be applied to derive the action suggested by the system. In our framework, we define \(\mathcal{D} = \mathcal{C} \cup \mathcal{B}\) as the set of decision functions that take the expected rewards as input and output the suggested action from the action space.

In particular, the decision strategies are categorized into two main groups: (i) a set of baseline strategies \(\mathcal{B}\), and (ii) a set of strategies designed to find compromise solutions that balance the preferences of multiple actors, denoted as \(\mathcal{C}\).
\paragraph{Compromise Functions}

In multi-agent settings, selecting a single action requires balancing the competing preferences of multiple actors. To achieve this, we define a set of \textit{compromise functions} \( \mathcal{C} = \{C_1, C_2, \dots, C_l\} \), where each \( C_j \) represents a decision-making strategy that aggregates the expected rewards \( \mathbb{E}[R_i(a \mid \boldsymbol{x})] \) across all actors \( i \in \mathcal{I} \) and actions \( a \in \mathcal{A} \). These compromise functions encode various principles of decision-making and welfare distribution, ranging from efficiency-focused strategies to fairness-based approaches.

Formally, each compromise function \( C_j \) takes as input the matrix of expected rewards \( \mathbb{E}[R(a \mid \boldsymbol{x})] \in [0, 1]^{N \times K} \), where \( N \) is the number of actors and \( K = |\mathcal{A}| \) is the number of actions, along with a set of additional parameters \( \boldsymbol{p} \in \mathcal{P} \). These parameters \( \boldsymbol{p} \) may include actor-specific baseline values (e.g., disagreement or ideal points) or contextual information relevant to the decision-making process. The output is the selected action \( a^* \in \mathcal{A} \), aligned with the specified decision-making principle.

The mapping implemented by \( C_j \) is expressed as:
\[
C_j : [0, 1]^{N \times K} \times \mathcal{P} \to \mathcal{A}, \quad 
C_j\big(\mathbb{E}[R(a \mid \boldsymbol{x})], \boldsymbol{p}\big) = \arg\max_{a \in \mathcal{A}} 
\Phi_j\big(\mathbb{E}[R_1(a \mid \boldsymbol{x})], \dots, \mathbb{E}[R_N(a \mid \boldsymbol{x})]; \boldsymbol{p}\big),
\]
where \( \Phi_j \) is a scalar-valued scoring function that encapsulates the decision-making principle.

Examples of compromise principles include maximizing the total reward (utilitarianism), ensuring equal reward distribution (egalitarianism), or maximizing the minimum reward (max-min fairness). Table~\ref{tab:compromise_functions} summarizes these and other principles derived from game theory, computational social choice, and welfare economics, illustrating how different normative criteria can guide decision-making in multi-agent settings.

\begin{table}[ht!]
\centering
\caption{Examples of Compromise Functions}
\label{tab:compromise_functions}
\begin{tabular}{p{2.1cm}p{7.2cm}p{5cm}}
\hline
\textbf{Function} & \textbf{Formula} & \textbf{Description} \\ \hline

Nash Bargaining Solution &
$C_{NBS} = \arg\max_{a \in \mathcal{A}} \prod_{i=1}^{N} \bigl( \mathbb{E}[R_i(a \mid  \boldsymbol{x})] - d_i \bigr)$ &
Maximizes the product of utility gains above actor-specific disagreement payoffs, balancing fairness and efficiency. \\ \hline

Proportional Fairness &
$C_{PF} = \arg\max_{a \in \mathcal{A}} \sum_{i=1}^{N} \log\bigl(\mathbb{E}[R_i(a \mid  \boldsymbol{x})]\bigr)$ &
Promotes balanced improvements in collective well-being, ensuring fair trade-offs. \\ \hline

Nash Social Welfare &
$C_{NSW} = \arg\max_{a \in \mathcal{A}} \prod_{i=1}^{N} \mathbb{E}[R_i(a \mid  \boldsymbol{x})]$ &
Maximizes the product of actors’ utilities, equivalent to proportional fairness under a logarithmic transformation. \\ \hline

Maximin &
$C_{MM} = \arg\max_{a \in \mathcal{A}} \min_{i=1}^{N} \mathbb{E}[R_i(a \mid \boldsymbol{x})]$ &
Safeguards the most disadvantaged actor by maximizing the minimum utility across actors. \\ \hline

Compromise Programming &
$C_{CP-L2} = \arg\min_{a \in \mathcal{A}} \sqrt{\sum_{i=1}^{N} w_i \bigl( u_i^* - \mathbb{E}[R_i(a \mid \boldsymbol{x})] \bigr)^2}$ &
Minimizes the weighted Euclidean distance between actors’ utilities and their ideal points \( u_i^* \). \\ \hline

Kalai-Smorodinsky Solution &
$C_{KS} = \arg\max_{a \in \mathcal{A}} \min_{i=1}^{N} \frac{\mathbb{E}[R_i(a \mid  \boldsymbol{x})] - d_i}{u_i^* - d_i}$ &
Maximizes proportional gains toward each actor’s ideal payoff relative to their disagreement payoff. \\ \hline

\end{tabular}
\end{table}

\paragraph{Baseline Strategies.}  
To contextualize and evaluate the proposed compromise functions, we compare their performance against two baselines: the \textit{outcome predictor baseline} (\( B_{\text{pred}} \)) and the \textit{individual reward maximization baseline} (\( B_{\text{max}}^i \)). These baselines serve as reference points and represent simple decision strategies that ignore the complexities of a multi-actor setting.

The \textit{outcome predictor baseline} provides a straightforward decision-making strategy, leveraging a predefined mapping from predicted outcomes to corresponding actions. This baseline operates under two paradigms, namely (i) a bijective mapping where each outcome is uniquely associated with the action that optimizes utility for that outcome and (ii) a heuristic or naive rule, according to which actions are determined based on the predicted outcome or another variable of interest, without requiring a strict one-to-one correspondence.
For instance, in a loan approval scenario, if repayment is the most likely predicted outcome, this baseline recommends granting the loan. Formally, let \( o_{\text{best}} \) denote the outcome with the highest predicted probability given context \( \boldsymbol{x} \). The recommended action is:
\[
a_{\text{pred}} = \arg\max_{a \in \mathcal{A}} P(o_{\text{best}} \mid \boldsymbol{x}, a).
\]
Alternatively, in cases where decision-making depends on the predicted value of a variable, this baseline prioritizes actions that optimize that variable. For example, in medical treatment selection, a naive strategy might choose the treatment that maximizes the predicted effect size or minimizes the predicted cost.

The \textit{individual reward maximization baseline}, on the other hand, focuses solely on the perspective of a single actor, recommending actions that maximize that actor’s expected reward without considering the impact on others. For each actor \( i \), the recommended action is:
\[
a_{\text{max}}^{(i)} = \arg\max_{a \in \mathcal{A}} \mathbb{E}[R_i(a \mid \boldsymbol{x})].
\]

\subsubsection{Evaluation Metrics and Optimal Action Selection.}

Our framework can present all actions recommended by different decision strategies, giving end users tools to make more informed decisions. However, if the goal is to identify the decision function that yields the best performance across a set of user-defined metrics for decision-making on new data, a scoring mechanism can be employed to rank the decision strategies and select the most effective one. These metrics may capture multiple objectives, ranging from standard performance and fairness measures to domain-specific criteria (e.g., profitability in loan decisions, treatment costs in healthcare).

In this context, our framework leverages a \(k\)-fold cross-validation mechanism not only to tune the outcome prediction and reward models but also to determine the best decision function based on historical data.

Consider a dataset of exhaustive triplets \(\{\bigl(\boldsymbol{x}_t, a, o\bigr)\}\), where \(\boldsymbol{x}_t \in \mathcal{X}\) represents the observed context for each sample, \(a \in \mathcal{A}\) denotes a potential action, and \(o \in \mathcal{O}\) corresponds to a possible outcome. For a dataset with \(T\) samples, the total number of triplets is \(T \cdot |\mathcal{A}| \cdot |\mathcal{O}|\), since each sample is associated with every possible action–outcome pair.

To evaluate a decision function \(D \in \mathcal{D}\) against a set of user-defined metrics \(\mathcal{M} = \{M_1, M_2, \dots, M_z\}\), we compute its performance across all triplets:
\[
\text{P}(D, M) = \frac{1}{T^\alpha}
\sum_{t=1}^{T} \sum_{a \in \mathcal{A}} \sum_{o \in \mathcal{O}}
\Theta\bigl(D, M \mid \boldsymbol{x}_t, a, o\bigr),
\]
where \(\Theta\bigl(D, M \mid \boldsymbol{x}_t, a, o\bigr)\) quantifies the performance of \(D\) with respect to metric \(M\) for the triplet \((\boldsymbol{x}_t, a, o)\). The parameter \(\alpha\) indicates whether metrics are averaged (\(\alpha = 1\), e.g., accuracy) or summed (\(\alpha = 0\), e.g., total cost).

To ensure comparability across metrics with different scales, the raw performance scores \(\text{P}(D, M)\) are normalized to the \([0,1]\) range:
\[
\tilde{P}(D, M) =
\frac{\text{P}(D, M) - \min_{D' \in \mathcal{D}} \text{P}(D', M)}
     {\max_{D' \in \mathcal{D}} \text{P}(D', M) - \min_{D' \in \mathcal{D}} \text{P}(D', M)},
\]
where \(\max_{D' \in \mathcal{D}} \text{P}(D', M)\) and \(\min_{D' \in \mathcal{D}} \text{P}(D', M)\) denote the highest and lowest performance values across all decision functions for metric \(M\). Because each metric \(M\) targets a specific optimization objective-maximization (e.g., accuracy), minimization (e.g., cost), or convergence to a target value (e.g., zero disparity across groups) -- this normalization is adapted so that more desirable outcomes map to 1, and less desirable outcomes map to lower scores.

We then compute an aggregated score \(S(D)\) by combining the normalized metrics:
\[
S(D) = \sum_{h=1}^{z} w_h \,\tilde{P}\bigl(D, M_h\bigr),
\]
where \(w_h\) represents the relative importance of metric \(M_h\), subject to \(\sum_{h=1}^{z} w_h = 1\). If all metrics are of equal importance, then \(w_h = 1/z\). Naturally, both the selection of metrics and the assignment of weights can be tailored by the user to match specific preferences.

At this point, we identify the decision function with the highest aggregated score:
\[
D^* = \arg\max_{D \in \mathcal{D}} S(D).
\]
Assuming the historical data and future contexts share a consistent distribution, \(D^*\) or the induced ranking guides future decisions.

When presented with a new context \(\boldsymbol{x}\), the outcome prediction model estimates the likelihood of each potential outcome (or produces a real-valued prediction), and the expected rewards \(\{\mathbb{E}[R_i(a \mid \boldsymbol{x})]\}_{i=1}^{N}\) for all actions \(a \in \mathcal{A}\) are computed. The optimal action \(a^*\) is then determined via \(D^*\):
\[
a^* = \arg\max_{a \in \mathcal{A}}
D^*\Bigl(\{\mathbb{E}[R_i(a \mid \boldsymbol{x})]\}_{i=1}^{N}\Bigr).
\]

\subsection{Computational Complexity Analysis}
The additional computational complexity of the proposed framework, beyond classical cross-validation, arises primarily from the reward prediction models, expected reward computation, and decision-making processes. Training the reward models scales with the number of reward types ($N$) and the size of the training dataset ($T$), with the complexity determined by the underlying model architecture. This complexity can be expressed as $O(c_{train}(T, g))$, where $g$ is the number of features considered in the reward models.

During the evaluation phase, the expected reward computation requires predicting rewards for each combination of actions and data points, with the test or validation set size denoted as $T'$. This step scales linearly with the number of features considered in the reward prediction model ($g$), the number of actions ($|\mathcal{A}|$), and the number of reward types ($N$): the resulting complexity is given by $O( T' \cdot 
 c_{inf}(g) \cdot |\mathcal{A}| \cdot N)$, where $c_{inf}(g)$ represents the inference cost of a single prediction of the reward model. Similarly, decision-making strategies involve generating compromise and baseline solutions by iterating over all actions and reward types for each data point. This results in an overall complexity of $O(T' \cdot |\mathcal{D}| \cdot |\mathcal{A}| \cdot N)$, where $|\mathcal{D}|$ is the total number of decision strategies.

The evaluation of decision outcomes involves calculating fairness-related, performance-related, and case-specific metrics. This process usually scales proportionally with the number of metrics ($|\mathcal{M}|$) and the size of the data points being evaluated ($T'$). Ranking and weighted aggregation of normalized scores introduce additional computational overhead,  with complexity scaling as $O(|\mathcal{M}| \cdot N \cdot |\mathcal{A}|)$.

Since the framework scales linearly with its core components, it can handle moderately large datasets while ensuring robust and explainable decision recommendations. This linear scalability makes the framework efficient for real-world applications, where the number of actors, actions, metrics, and decision strategies is typically limited.


% \subsection{Main Strengths of the Proposed Framework} 
% In many traditional data-driven systems, a single predictive model recommends an action solely based on historical data and fixed performance metrics. Such methods often overlook the diversity of stakeholder preferences and run the risk of propagating historical biases in action assignments. By contrast, our \emph{multi-actor} framework explicitly models the distinct interests of various stakeholders, integrating each actor’s preferences into the decision-making process. This \emph{compromise-driven} mechanism finds actions that balance these different preferences while still optimizing user-defined metrics. Moreover, because the framework is \emph{model-agnostic}, it can be paired with a wide range of predictive models and applied across diverse domains. Its design also emphasizes \emph{explainability}, providing clear insight into how actions are chosen and trade-offs are managed.

% A further advantage is that our approach can indirectly foster fairer outcomes \emph{even without} constraining the framework to a specific fairness definition or modifying the underlying outcome prediction model. As our experimental results illustrate, incorporating multiple stakeholder perspectives can effectively mitigate biases stemming from historically skewed action assignments. In doing so, the system can promote more equitable decision-making, outperforming single-model approaches on both performance metrics and fairness considerations.

\section{Experiments}
\label{sec:experiments}
%\subsection{Real-world Use Case Examples}
Given the specific scope of our framework, its primary outputs are: (i) a list of actions suggested by each decision strategy for a given context vector, and (ii) a ranking of decision functions derived from historical data and user-defined preferences regarding the relative importance of various evaluation metrics. Consequently, the effectiveness of the framework stems from its ability to provide users with an interpretable tool for making decisions that are both context-sensitive and aligned with the interests of diverse stakeholders. For this reason, it is not feasible to establish a universally applicable evaluation framework, as its relevance is highly dependent on the specific use case. Consequently, the primary objectives of this Section are to demonstrate the versatility of the framework across diverse use cases and to analyze how its outputs adapt to variations in its core components.
To demonstrate the practical applicability of the proposed framework, we present two representative scenarios: a loan approval scenario, showcasing decision-making in a multi-classification problem, and a health treatment scenario, illustrating a causal inference-based regression problem.

In both scenarios, reward structures are predefined using stakeholder-specific heuristics that are aimed to mimic and approximate real-world preferences and objectives. To reflect the inherent variability in human preferences and assess the robustness of the learned reward models, uniform noise is added to the synthetic rewards used for training.

While the task of modeling actor-specific rewards is critical and complex, it lies beyond the primary scope of this work. Instead, the reward structures in our experiments serve as illustrative examples to demonstrate the utility and advantages of the framework.
Furthermore, although the actors in the two scenarios represent prototypical stakeholder groups, the framework is designed to naturally support any number and type of actors -- from broad categories to individualized reward models tailored to specific decision-making processes. This adaptability ensures relevance across diverse domains and decision-making contexts. The code to reproduce the experiments is fully documented and available in the following GitHub repository: \url{https://anonymous.4open.science/r/participatory_training-502B}
\subsection{Real-World Use Cases}
\subsubsection{Use Case 1: Lending Scenario}

In the lending scenario, real-world data from the Lending Club database \footnote{ \url{https://www.kaggle.com/datasets/wordsforthewise/lending-club}} is used. Our setup is structured as a 
3×3 problem, where the decision space $ \mathcal{A}$ comprises three options ("Grant," "Grant lower amount" and "Not Grant") and the outcome space $ \mathcal{O} $ consists of three discrete options ("Fully Repaid," "Partially Repaid," or "Not Repaid") depending on the real repayment status reported in the dataset. The context includes applicant-specific features such as credit score, income, financial history and demographic attributes.

This scenario involves three key stakeholder categories:
\begin{itemize}
    \item \textit{Bank}, who seeks to maximize profitability. Its rewards are tied to repayment probabilities, assigning higher rewards for fully repaid loans and lower rewards for partial or non-repayment.

    \item \textit{Applicant}, who prioritize loan access to meet their financial needs. Their rewards reflect the utility derived from loan approval, modulated by the obligations of repayment. Full approval typically yields higher rewards, while partial or no approval reduces applicant satisfaction.

    \item \textit{Regulatory Body}, responsible for ensuring financial stability and inclusivity in lending practices. Its rewards balance the stability of the financial system with the willingness to promote social benefits, placing particular value on providing access to credit for vulnerable applicants.

\end{itemize}

In addition to the baseline strategies described earlier, we benchmark the performance of compromise functions against an \textit{Oracle} strategy, which assumes a bijective mapping between the actual outcomes and the optimal actions, simulating an idealized decision-making process.
Besides performance- and fairness-oriented metrics, we include case-specific evaluation metrics, namely the percentage of total profit achieved by the bank, the percentage of losses relative to the total potential loss and the proportion of unexploited profit resulting from suboptimal granting decisions. In the baseline example, we use a Random Forest as the outcome prediction model. For the ablation study, we examine the framework's behavior using a simpler model, specifically a k-Nearest Neighbor algorithm. In both this and the subsequent example, the reward prediction models, which are used to mimic actor-based preferences given a new context vector, are also implemented as Random Forests, as this model architecture is widely employed by real-world practitioners and provides sufficient learning power for our experiments.

%\subsubsection{Healthcare scenario}
%\subsection{Real-world Use Case 2: Healthcare Scenario}
\subsubsection{Use Case 2: Healthcare Scenario}
For the healthcare scenario, we use the first realization of the Infant Health and Development Program (IHDP) dataset, as reported by \citep{louizos2017causal}\footnote{\url{https://github.com/AMLab-Amsterdam/CEVAE}}. This dataset, firstly introduced by \citep{hill2011bayesian}, originates from a randomized experiment studying the effect of home visits on cognitive test scores for infants. It has been widely used in the causal inference literature \citep{shalit2017estimating, louizos2017causal, yao2018representation}.

In this setting, the action space consists of two possible actions: assigning or not assigning the treatment to the candidate patient. The outcome is represented as a scalar real value, indicating the cognitive score achieved by the child under the given treatment. The context vector comprises 25 binary and continuous features describing both the child and their family.

It is important to note that, as this is a synthetic benchmark dataset, the true values of the outcomes under treatment and control are available. These true values are used to evaluate the performance of the causal effect estimation model but are, of course, not utilized during training.
We use this dataset to demonstrate that our framework can be extended to contexts where the predicted target is linked to a causal effect, thereby showcasing its ability to enhance decision-making also in causal inference scenarios. In this case, the stakeholder categories considered are the following:

\begin{itemize}
\item \textit{Healthcare Provider}, focused on improving patient outcomes while managing costs. Its rewards are based on normalized outcome improvements relative to a baseline, with penalties for higher treatment costs.

\item \textit{Policy Maker}, committed to maximizing societal benefits and promoting fairness. Its rewards emphasize outcome improvements normalized by potential gains, with additional weighting to promote equity across demographic groups.

\item \textit{Parent}, prioritizing the well-being of their child. Their rewards are directly proportional to normalized outcomes, reflecting the straightforward utility parents derive from improved health or cognitive scores.
\end{itemize}
As in the previous example, random noise is introduced to the rewards to account for real-world variability.
In addition to individual maximization strategies, we also consider a baseline strategy that aims to maximize the achieved cognitive score. As a natural consequence, this strategy would suggest treating all potential patients.
As case-specific evaluation metrics, we used the mean outcome values for the treated and control groups, as well as the absolute difference between these values.
To estimate the Conditional Average Treatment Effect (CATE), we use an X-regressor meta-learner \citep{chen2020causalml} that leverages XGBRegressor as its base learner.

\subsection{Discussion of Key Insights}
Figures \ref{fig:abl_reward}, \ref{fig:ablsample_size} and \ref{fig:abl_predmodel} illustrate how variations in key aspects of the framework influence the performance of different decision functions (baseline strategies and compromise functions) across various evaluation metrics related to decision accuracy, fairness, and case-specific outcomes. The mean values presented in all Figures are calculated over four runs using different random seeds. These Figures provide a comparative analysis of the performance of decision functions relative to one another and the \textit{Oracle}, highlighting trade-offs among the metrics, in the previously described lending scenario.
In general, baseline strategies achieve the highest decision accuracy compared to real outcomes. However, they demonstrate suboptimal performance in terms of fairness (see subplots on \textit{Demographic\_Parity}), where compromise functions tend to enable more equitable outcomes. This is particularly evident when compared to the outcome prediction baseline (\textit{Outcome\_Pred\_Model} in the plots). Notably, despite all decision functions share the same predicted outcome values, since they are based on identical vectors of expected rewards derived from context vectors, compromise functions implicitly introduce a fairness correction. This correction mitigates biases in baseline strategies, which often favor specific groups, thereby improving fairness in the decision-making process.

Figure \ref{fig:abl_reward} specifically examines how variations in the reward structures of the actors (namely the \textit{Bank} and the \textit{Applicant}) affect decision outcomes suggested by each strategy, as reflected in the evaluation metrics. In this analysis, two distinct reward structures are compared: a balanced baseline version and a stricter configuration, where the \textit{Bank} values granting loans only when full repayment is expected, and the \textit{Applicant} prioritizes loan approval regardless of repayment likelihood. As shown in Figure \ref{fig:abl_reward}, more "selfish" individual preferences exacerbate the trade-offs between performance- and fairness-oriented metrics, leading to more conservative decisions.

In a similar manner, Figure \ref{fig:ablsample_size} showcases the effect of training sample sizes. As expected, larger sample sizes consistently improve all metrics, primarily thanks to the greater accuracy of the outcome prediction model. However, interestingly, the percentage improvement in case-specific metrics such as the percentage of profit achieved by the Bank relative to the total obtainable amount under omniscient conditions (\textit{Total\_Profit}) and the percentage of loans fully granted (\textit{Percentage\_Grant}) exceeds the improvement in decision accuracy. This suggests that variations in the performance of the outcome model have diversified effects on different metrics, influenced by the unique interrelation of predictions with the stakeholders' reward models. Nevertheless, the positive impact of increased sample sizes is also evident in the improved alignment between performance on the training and test sets. Figure \ref{fig:sample} illustrates the mean absolute difference in the metrics between training and test data, across all decision functions. Notably, as the number of training samples increases, this difference diminishes, indicating that larger sample sizes enable the training performance to more accurately reflect future performance on the test set. This trend underscores the value of larger datasets in enhancing the reliability and generalization of decision strategies.

Figure \ref{fig:abl_predmodel} explores the impact of the complexity of the outcome prediction model by comparing a key Nearest Neighbor (kNN) and Random Forest (RF) models. As can be seen, RF consistently outperforms kNN in metrics like accuracy and precision, highlighting the benefits of increased model complexity for predictive performance. However, as previously observed, fairness and case-specific metrics experience different variations compared to prediction accuracy as the outcome model improves, as they are influenced by the interplay between the reward models and the rationale behind decision strategies. 

Finally, Figure \ref{fig:cfr} refers to the healthcare scenario and  demonstrates how the proposed framework can be used also to evaluate the distribution of treatment effects across patients, based on treatment assignment decisions and the expected treatment effects. This analysis underscores the framework's ability to assess not only traditional metrics associated with causal effects but also the equity and impact of treatment allocation across different population groups. Thus, once again, it highlights the framework's versatility in addressing diverse contexts and applications, as well as its potential to enhance equity and transparency in decision-making, also in scenarios involving causal effect estimation.
\begin{figure}[ht]
    \centering
\includegraphics[width=1\linewidth]{strategy_line.png} 
    \caption{Effects of reward structures on average test set performance: comparison of evaluation metrics across decision functions in the lending scenario}
    \label{fig:abl_reward}
\end{figure}
\begin{figure}[h!]
    \centering
\includegraphics[width=1\linewidth]{samples_line.png} 
    \caption{Effects of training sample sizes on average test set performance: comparison of evaluation metrics across decision functions in the lending scenario}
\label{fig:ablsample_size} 
\end{figure}

\begin{figure}[h!]
    \centering
\includegraphics[width=1\linewidth]{model_line.png} 
    \caption{Effects of outcome prediction model on average test set performance: comparison of evaluation metrics across decision functions in the lending scenario}
\label{fig:abl_predmodel} 
\end{figure}
\begin{figure}[h!]
    \centering
\includegraphics[width=0.5\linewidth]{cfr1.png} 
    \caption{Impact of sample size on training-test performance alignment (Mean Absolute Difference) across evaluation metrics}
    \label{fig:sample} 
\end{figure}

\begin{figure}[h!]
    \centering
\includegraphics[width=0.9\linewidth]{health_line.png} 
    \caption{Key evaluation metrics across decision functions in the causal healthcare scenario}
    \label{fig:cfr} 
\end{figure}


\section{Practical Implications}
\label{sec:implications}
When applying the framework, a couple of important considerations must be taken into account to ensure its effective and ethical use. A critical aspect is the requirement for a vector of rewards associated with the historical dataset to enable training and mimic a participatory decision-making process on new data. This step must be handled carefully, as it carries the risk of introducing bias.
Ideally, these reward vectors should be informed by real stakeholder participation or objective real-world measurements of the effects actions have on the actors, ensuring a comprehensive and accurate representation of diverse perspectives and impacts. In fact, designing rewards artificially or "on desk" risks oversimplifying the complexities of real-world scenarios and misinterpreting the needs and interests of underprivileged stakeholders, particularly if they are not directly consulted. For this reason, while the modeling of reward functions allows to incorporate multiple viewpoints, the construction of these reward functions is critical. Proper modeling ensures that the resulting decisions are equitable, contextually appropriate, and truly reflective of the diversity of stakeholder preferences, rather than inadvertently reinforcing existing biases or misrepresenting key perspectives.

Another structural characteristic of the framework lies in the assumption of a non-adversarial nature among stakeholders' preferences used to train the reward models. This technical choice was motivated by the need to allow the algorithm to learn from, at least theoretically, the actors' honest preferences. However, it is important to clarify that the framework is not intended to mimic a real collective decision-making process, where actors interact and can strategically adapt their declared preferences; instead, it is designed to systematically represent and reconcile diverse perspectives and interests in a structured manner.

\section{Conclusion and Future Work}
 In many traditional data-driven systems, a single predictive model recommends an action solely based on historical data and fixed performance metrics. Such methods often overlook the diversity of stakeholder preferences and run the risk of propagating historical biases in action assignments. By contrast, our multi-actor framework explicitly models the distinct interests of various stakeholders, integrating their preferences into the decision-making process. This compromise-driven mechanism allow to find actions that balance these different preferences while still optimizing user-defined metrics. Moreover, because the framework is model-agnostic, it can be paired with a wide range of predictive models and applied across diverse domains. Its design also emphasizes explainability, providing clear insight into how actions are chosen and trade-offs are managed.
A further advantage is that our approach can indirectly foster fairer outcomes \emph{even without} constraining the framework to a specific fairness definition or modifying the underlying outcome prediction model. As our experimental results illustrate, incorporating multiple stakeholder perspectives can effectively mitigate biases stemming from historically skewed action assignments. In doing so, the system can promote more equitable decision-making, outperforming single-model approaches on both case-specific and fairness metrics.
Overall, our framework is built on structural choices that ensure decision recommendations are based on representations of real stakeholder preferences or the actual effects of actions on them, thereby enhancing the expressive power of prediction-based decision support systems. While this approach allows for a fair representation of various perspectives and provides an equitable foundation for identifying compromise solutions, the framework does not capture the dynamic and strategic interactions that often arise in real-world consultation processes. In such settings, stakeholders may strategically adjust their revealed preferences in response to those of other actors to influence outcomes in their favour. Addressing these competitive or adversarial dynamics, for instance analysing the strategy-proofness~\cite{Sat73,Sat75,Arr12} of this approach, represents a significant avenue for future work.
\label{sec:conclusion}




%%
%% The acknowledgments section is defined using the "acks" environment
%% (and NOT an unnumbered section). This ensures the proper
%% identification of the section in the article metadata, and the
%% consistent spelling of the heading.
%\begin{acks}
%To Robert, for the bagels and explaining CMYK and color spaces.
%\end{acks}

%%
%% The next two lines define the bibliography style to be used, and
%% the bibliography file.
\bibliographystyle{ACM-Reference-Format}
\bibliography{sample-base}


%%
%% If your work has an appendix, this is the place to put it.


\end{document}
\endinput
%%
%% End of file `sample-manuscript.tex'.
