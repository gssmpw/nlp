[
  {
    "index": 0,
    "papers": [
      {
        "key": "houlsby2019parameter",
        "author": "Houlsby, Neil and Giurgiu, Andrei and Jastrzebski, Stanislaw and Morrone, Bruna and De Laroussilhe, Quentin and Gesmundo, Andrea and Attariyan, Mona and Gelly, Sylvain",
        "title": "Parameter-efficient transfer learning for NLP"
      },
      {
        "key": "pfeiffer2021adapterfusion",
        "author": "Pfeiffer, Jonas and Kamath, Aishwarya and R{\\\"u}ckl{\\'e}, Andreas and Cho, Kyunghyun and Gurevych, Iryna",
        "title": "AdapterFusion: Non-Destructive Task Composition for Transfer Learning"
      },
      {
        "key": "ruckle2021adapterdrop",
        "author": "R{\\\"u}ckl{\\'e}, Andreas and Geigle, Gregor and Glockner, Max and Beck, Tilman and Pfeiffer, Jonas and Reimers, Nils and Gurevych, Iryna",
        "title": "AdapterDrop: On the Efficiency of Adapters in Transformers"
      },
      {
        "key": "hetowards",
        "author": "He, Junxian and Zhou, Chunting and Ma, Xuezhe and Berg-Kirkpatrick, Taylor and Neubig, Graham",
        "title": "Towards a Unified View of Parameter-Efficient Transfer Learning"
      },
      {
        "key": "wang2022adamix",
        "author": "Wang, Yaqing and Agarwal, Sahaj and Mukherjee, Subhabrata and Liu, Xiaodong and Gao, Jing and Hassan, Ahmed and Gao, Jianfeng",
        "title": "AdaMix: Mixture-of-Adaptations for Parameter-efficient Model Tuning"
      },
      {
        "key": "pfeiffer2021adapterfusion",
        "author": "Pfeiffer, Jonas and Kamath, Aishwarya and R{\\\"u}ckl{\\'e}, Andreas and Cho, Kyunghyun and Gurevych, Iryna",
        "title": "AdapterFusion: Non-Destructive Task Composition for Transfer Learning"
      }
    ]
  },
  {
    "index": 1,
    "papers": [
      {
        "key": "lester2021power",
        "author": "Lester, Brian and Al-Rfou, Rami and Constant, Noah",
        "title": "The Power of Scale for Parameter-Efficient Prompt Tuning"
      },
      {
        "key": "li2021prefix",
        "author": "Li, Xiang Lisa and Liang, Percy",
        "title": "Prefix-Tuning: Optimizing Continuous Prompts for Generation"
      },
      {
        "key": "liu2024gpt",
        "author": "Liu, Xiao and Zheng, Yanan and Du, Zhengxiao and Ding, Ming and Qian, Yujie and Yang, Zhilin and Tang, Jie",
        "title": "GPT understands, too"
      },
      {
        "key": "xiao2023decomposed",
        "author": "Xiao, Yao and Xu, Lu and Li, Jiaxi and Lu, Wei and Li, Xiaoli",
        "title": "Decomposed Prompt Tuning via Low-Rank Reparameterization"
      },
      {
        "key": "Fan_Wei_Qu_Lu_Xie_Cheng_Chen_2024",
        "author": "Fan, Chenghao and Wei, Wei and Qu, Xiaoye and Lu, Zhenyi and Xie, Wenfeng and Cheng, Yu and Chen, Dangyang",
        "title": "Enhancing Low-Resource Relation Representations through Multi-View Decoupling"
      }
    ]
  },
  {
    "index": 2,
    "papers": [
      {
        "key": "hulora",
        "author": "Hu, Edward J and Wallis, Phillip and Allen-Zhu, Zeyuan and Li, Yuanzhi and Wang, Shean and Wang, Lu and Chen, Weizhu and others",
        "title": "LoRA: Low-Rank Adaptation of Large Language Models"
      },
      {
        "key": "zhangadaptive",
        "author": "Zhang, Qingru and Chen, Minshuo and Bukharin, Alexander and He, Pengcheng and Cheng, Yu and Chen, Weizhu and Zhao, Tuo",
        "title": "Adaptive Budget Allocation for Parameter-Efficient Fine-Tuning"
      },
      {
        "key": "liudora",
        "author": "Liu, Shih-yang and Wang, Chien-Yi and Yin, Hongxu and Molchanov, Pavlo and Wang, Yu-Chiang Frank and Cheng, Kwang-Ting and Chen, Min-Hung",
        "title": "DoRA: Weight-Decomposed Low-Rank Adaptation"
      },
      {
        "key": "kopiczkovera",
        "author": "Kopiczko, Dawid Jan and Blankevoort, Tijmen and Asano, Yuki M",
        "title": "VeRA: Vector-based Random Matrix Adaptation"
      }
    ]
  },
  {
    "index": 3,
    "papers": [
      {
        "key": "hulora",
        "author": "Hu, Edward J and Wallis, Phillip and Allen-Zhu, Zeyuan and Li, Yuanzhi and Wang, Shean and Wang, Lu and Chen, Weizhu and others",
        "title": "LoRA: Low-Rank Adaptation of Large Language Models"
      }
    ]
  },
  {
    "index": 4,
    "papers": [
      {
        "key": "liudora",
        "author": "Liu, Shih-yang and Wang, Chien-Yi and Yin, Hongxu and Molchanov, Pavlo and Wang, Yu-Chiang Frank and Cheng, Kwang-Ting and Chen, Min-Hung",
        "title": "DoRA: Weight-Decomposed Low-Rank Adaptation"
      }
    ]
  },
  {
    "index": 5,
    "papers": [
      {
        "key": "zhong2024neatnonlinearparameterefficientadaptation",
        "author": "Yibo Zhong and Haoxiang Jiang and Lincan Li and Ryumei Nakada and Tianci Liu and Linjun Zhang and Huaxiu Yao and Haoyu Wang",
        "title": "NEAT: Nonlinear Parameter-efficient Adaptation of Pre-trained Models"
      }
    ]
  },
  {
    "index": 6,
    "papers": [
      {
        "key": "zhangadaptive",
        "author": "Zhang, Qingru and Chen, Minshuo and Bukharin, Alexander and He, Pengcheng and Cheng, Yu and Chen, Weizhu and Zhao, Tuo",
        "title": "Adaptive Budget Allocation for Parameter-Efficient Fine-Tuning"
      }
    ]
  },
  {
    "index": 7,
    "papers": [
      {
        "key": "kalajdzievski2023rankstabilizationscalingfactor",
        "author": "Damjan Kalajdzievski",
        "title": "A Rank Stabilization Scaling Factor for Fine-Tuning with LoRA"
      }
    ]
  },
  {
    "index": 8,
    "papers": [
      {
        "key": "hayou2024loraefficientlowrank",
        "author": "Soufiane Hayou and Nikhil Ghosh and Bin Yu",
        "title": "LoRA+: Efficient Low Rank Adaptation of Large Models"
      }
    ]
  },
  {
    "index": 9,
    "papers": [
      {
        "key": "meng2024pissa",
        "author": "Fanxu Meng and Zhaohui Wang and Muhan Zhang",
        "title": "Pi{SSA}: Principal Singular Values and Singular Vectors Adaptation of Large Language Models"
      }
    ]
  },
  {
    "index": 10,
    "papers": [
      {
        "key": "wang2024miloraharnessingminorsingular",
        "author": "Hanqing Wang and Yixia Li and Shuo Wang and Guanhua Chen and Yun Chen",
        "title": "MiLoRA: Harnessing Minor Singular Components for Parameter-Efficient LLM Finetuning"
      }
    ]
  },
  {
    "index": 11,
    "papers": [
      {
        "key": "wang2024kasaknowledgeawaresingularvalueadaptation",
        "author": "Fan Wang and Juyong Jiang and Chansung Park and Sunghun Kim and Jing Tang",
        "title": "KaSA: Knowledge-Aware Singular-Value Adaptation of Large Language Models"
      }
    ]
  },
  {
    "index": 12,
    "papers": [
      {
        "key": "si2024unleashingpowertaskspecificdirections",
        "author": "Chongjie Si and Zhiyi Shi and Shifan Zhang and Xiaokang Yang and Hanspeter Pfister and Wei Shen",
        "title": "Unleashing the Power of Task-Specific Directions in Parameter Efficient Fine-tuning"
      }
    ]
  },
  {
    "index": 13,
    "papers": [
      {
        "key": "wanglora",
        "author": "Wang, Shaowen and Yu, Linxi and Li, Jian",
        "title": "LoRA-GA: Low-Rank Adaptation with Gradient Approximation"
      }
    ]
  },
  {
    "index": 14,
    "papers": [
      {
        "key": "wang2024loraprolowrankadaptersproperly",
        "author": "Zhengbo Wang and Jian Liang and Ran He and Zilei Wang and Tieniu Tan",
        "title": "LoRA-Pro: Are Low-Rank Adapters Properly Optimized?"
      }
    ]
  },
  {
    "index": 15,
    "papers": [
      {
        "key": "huang2024lorahub",
        "author": "Chengsong Huang and Qian Liu and Bill Yuchen Lin and Tianyu Pang and Chao Du and Min Lin",
        "title": "LoraHub: Efficient Cross-Task Generalization via Dynamic Lo{RA} Composition"
      }
    ]
  },
  {
    "index": 16,
    "papers": [
      {
        "key": "zadouri2024pushing",
        "author": "Ted Zadouri and Ahmet {\\\"U}st{\\\"u}n and Arash Ahmadian and Beyza Ermis and Acyr Locatelli and Sara Hooker",
        "title": "Pushing Mixture of Experts to the Limit: Extremely Parameter Efficient MoE for Instruction Tuning"
      }
    ]
  },
  {
    "index": 17,
    "papers": [
      {
        "key": "liu2023moelora",
        "author": "Liu, Qidong and Wu, Xian and Zhao, Xiangyu and Zhu, Yuanshao and Xu, Derong and Tian, Feng and Zheng, Yefeng",
        "title": "MOELoRA: An MOE-based Parameter Efficient Fine-Tuning Method for Multi-task Medical Applications"
      }
    ]
  },
  {
    "index": 18,
    "papers": [
      {
        "key": "dou2023loramoe",
        "author": "Dou, Shihan and Zhou, Enyu and Liu, Yan and Gao, Songyang and Zhao, Jun and Shen, Wei and Zhou, Yuhao and Xi, Zhiheng and Wang, Xiao and Fan, Xiaoran and others",
        "title": "Loramoe: Revolutionizing mixture of experts for maintaining world knowledge in language model alignment"
      }
    ]
  },
  {
    "index": 19,
    "papers": [
      {
        "key": "wang2023multilora",
        "author": "Wang, Yiming and Lin, Yu and Zeng, Xiaodong and Zhang, Guannan",
        "title": "Multilora: Democratizing lora for better multi-task learning"
      }
    ]
  },
  {
    "index": 20,
    "papers": [
      {
        "key": "liu2024adamole",
        "author": "Zefang Liu and Jiahua Luo",
        "title": "AdaMo{LE}: Fine-Tuning Large Language Models with Adaptive Mixture of Low-Rank Adaptation Experts"
      }
    ]
  },
  {
    "index": 21,
    "papers": [
      {
        "key": "tian2024hydraloraasymmetricloraarchitecture",
        "author": "Chunlin Tian and Zhan Shi and Zhijiang Guo and Li Li and Chengzhong Xu",
        "title": "HydraLoRA: An Asymmetric LoRA Architecture for Efficient Fine-Tuning"
      }
    ]
  }
]