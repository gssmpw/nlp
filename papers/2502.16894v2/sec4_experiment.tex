\input{tables/cv}
\input{tables/generate}
\input{tables/commonsense_multi}
\input{tables/glue}

\section{Experiment}

\subsection{Baselines}

We compare GOAT with Full FT, single-LoRA, and LoRA MoE methods to substantiate its efficacy and robustness:
\begin{enumerate}
\item Full-Finetuning: \textbf{Full FT} fine-tunes all parameters, while \textbf{Full FT MoE} is Upcycled MoE with full-rank fine-tuning and 2 active experts out of 8 total experts.
\item Single-LoRA baselines: \textbf{LoRA} \cite{hulora}; \textbf{DoRA} \cite{liudora}; \textbf{PiSSA} \cite{meng2024pissa}; \textbf{MiLoRA} \cite{wang2024miloraharnessingminorsingular}; \textbf{rsLoRA} \cite{kalajdzievski2023rankstabilizationscalingfactor}; \textbf{LoRA-Dash} \cite{si2024unleashingpowertaskspecificdirections}; \textbf{NEAT} \cite{zhong2024neatnonlinearparameterefficientadaptation}; \textbf{KaSA} \cite{wang2024kasaknowledgeawaresingularvalueadaptation} 
% \textbf{LoRA} \cite{hulora} is the original low-rank adaptation algorithm. \textbf{DoRA} \cite{liudora} adds learnable magnitudes and directions. \textbf{NEAT} introduces a nonlinear adaptation. \textbf{rsLoRA} \cite{kalajdzievski2023rankstabilizationscalingfactor} adjust scaling for stability. \textbf{PiSSA} \cite{meng2024pissa}, \textbf{MiLoRA} \cite{wang2024miloraharnessingminorsingular} and \textbf{KaSA}\cite{wang2024kasaknowledgeawaresingularvalueadaptation} employ singular values for initialization.  
% \textbf{LoRAPro}\cite{wang2024loraprolowrankadaptersproperly} aligns LoRA’s updates with the gradients of FFT to better approximate its behavior.
\item LoRA MoE baselines: \textbf{MoLoRA} \cite{zadouri2024pushing}; \textbf{AdaMoLE} \cite{liu2024adamole}; \textbf{HydraLoRA} \cite{tian2024hydraloraasymmetricloraarchitecture}.
% \textbf{MoLoRA} \cite{zadouri2024pushing} combines LoRA with a Mixture of Experts framework. \textbf{AdaMoLE} \cite{liu2024adamole} introduces adaptive experts selection. \textbf{HydraLoRA} \cite{tian2024hydraloraasymmetricloraarchitecture} proposes an asymmetric LoRA MoE architecture.
\end{enumerate}
For a fair comparison, we closely follow the configurations from prior studies \cite{hulora,meng2024pissa,wang2024loraprolowrankadaptersproperly}. Details on the baselines are in \App{app:imple}.

\subsection{Datasets}
We evaluate GOAT across 25 tasks, spanning 4 domains: 
\begin{enumerate}
\item  \textbf{Image Classification (IC):} We fine-tune and evaluate ViT-B/32 \cite{radford2021learningtransferablevisualmodels} on 7 image classification datasets \cite{ilharco2023editingmodelstaskarithmetic}.
\item \textbf{Natural Language Generation (NLG):} We fine-tune LLaMA2-7B \cite{touvron2023llama2openfoundation} on subset of WizardLM~\cite{xu2023wizardlm}, MetaMathQA~\cite{yumetamath} and Code-Feedback~\cite{zheng2024opencodeinterpreter}. We evaluate its performance on dialogue~\cite{zheng2023judging}, math~\cite{cobbe2021training} and coding~\cite{chen2021evaluating} following \citet{wang2024loraprolowrankadaptersproperly}
\item \textbf{Commonsense Reasoning (CR):} We fine-tune LLaMA2-7B on Commonsense170K and evaluate on 8 commonsense reasoning datasets ~\cite{hu-etal-2023-llm}. 
\item \textbf{Natural Language Understanding (NLU):} We RoBERTa-large~\cite{liu2020roberta} on 7 GLUE tasks~\cite{wang2018glue} following \cite{hulora}. 
\end{enumerate}
Due to the 8x memory requirements of Full FT MoE, we only evaluate it on IC and NLU tasks. Detailed of the datasets can be found in Appendix~\ref{app:dataset}.

\subsection{Main Results}\label{sec:ex}

Tables~\ref{tab:vit}, \ref{tab:gen}, \ref{tab:multiqa} and \ref{tab:nlu} present results on 4 domain benchmarks:
\begin{enumerate}[label=\textbullet,leftmargin=*]
\item \textbf{IC} (\Tab{tab:vit}): GOAT achieves 99.07\% of full FT performance and surpasses LoRA with quadruple the parameters (rank 32). It improves 6.0\% over PiSSA and 2.4\% over HydraLoRA, outperforming all LoRA variants.
\item \textbf{NLG} (\Tab{tab:gen}): Our method shows the smallest performance gap with Full FT, outperforming MoLoRA by 0.25 on MTBench, 6.30\% on GSM8K, and 3.14\% on HumanEval, highlighting GOAT's superiority.
\item \textbf{CR} (\Tab{tab:multiqa}): GOAT consistently outperforms all established baselines, exceeding the best single LoRA method, KASA, by 1.47\%, the best LoRA-MoE method, HydraLoRA, by 1.98\%, and ChatGPT by 7.42\%.
\item \textbf{NLU} (\Tab{tab:nlu}): our method outperforms the best-performing Single LoRA Method, MiLoRA, by 0.28\%, surpasses the best-performing LoRA MoE Method, MoLoRA, by 0.27\%, and achieves a 1.98\%  improvement over HydraLoRA. Furthermore, our method surpasses the Full FT (89.47 \vs 89.76) and reduces the gap with Full FT MoE to just 0.1\%. 
\end{enumerate}
In summary, GOAT outperforms across all benchmarks, achieving superior results in nearly every sub-task, and closes or surpasses the performance gap with Full FT, demonstrating the superior effectiveness of our approach.

% \begin{figure}
%     \centering
%     \includegraphics[width=1\linewidth]{7.png}
%     \caption{Enter Caption}
%     \label{fig:enter-label}
% \end{figure}

\subsection{Ablation Study}

We conduct ablation experiments to evaluate the impact of our adaptive priors initialization and gradient scaling, as summarized in \Tab{tab:ab}. Our initialization, with or without MoE scaling, consistently outperforms other methods\footnote{Details provided in Appendix~\ref{app:ablation}} (note that no SVD-based initialization corresponds to the original zero initialization, yielding 81.06/80.26). Without MoE, initializing a single LoRA with our SVD fragments achieves a performance of 77.62. In contrast, our MoE architecture achieves 80.35, demonstrating its clear advantage in effectively integrating expert functionalities.

\input{tables/ablation}
\subsection{Convergence Speed}

\begin{figure}[h]
\vspace{-10pt}
    \centering
    \includegraphics[width=0.8\linewidth]{figures/loss.pdf}
    \vspace{-20pt}
    \caption{Training loss curves of Different LoRA methods and Full Fine-tuning MoE on Cars. {The balance loss is excluded in the MoE baselines for a fair comparison with single LoRA baselines}.}
    \label{fig:loss}
    \vspace{-8pt}
\end{figure}

As shown in \Fig{fig:loss}, we compare the training loss curves of PiSSA, various LoRA MoE baselines, our proposed GOAT, and Full FT MoE on the Cars and MetaMathQA datasets. GOAT demonstrates faster convergence compared to the LoRA MoE baselines and achieves performance closest to Full FT MoE. Notably, our method achieves a lower final loss, balancing performance and efficiency. In contrast, methods like PiSSA converge quickly initially but yield suboptimal final performance, as discussed in \Sec{sec:svd}.

\subsection{Scaling Property}

\begin{figure}[h]
    \centering
    \includegraphics[width=0.75\linewidth]{figures/rank.pdf}
    \vspace{-16pt}
    \caption{Performance of different methods across ranks.\label{fig:r}}
    \vspace{-5mm}
\end{figure}

\begin{figure}[ht]
\vspace{-5pt}
    \centering
    \includegraphics[width=1\linewidth]{figures/scale.pdf}
    \vspace{-25pt}
\caption{Performance vs.~number of experts and activation ratio (total rank=32). ``2 in 8'' means activating 2 out of 8 experts.\label{fig:expert_scale}}
    \vspace{-15pt}
\end{figure}

\paragraph{Scaling across Different Rank.} To evaluate the scalability of our method, we increase the rank in GOAT from 8 to 128 on CV benchmarks, as shown in \Fig{fig:r}. As the rank increases, the performance gap between GOAT and full fine-tuning MoE narrows significantly. Notably, GOAT consistently outperforms both MoLoRA and HydraLoRA across all ranks. At rank 32, GOAT achieves 83.04, surpassing MoLoRA (82.15) by 1.08\% and HydraLoRA (82.12) by 1.12\%. While higher ranks improve performance, gains diminish as ranks increase. For instance, GOAT improves by just 0.38\% from rank 64 to 128, highlighting diminishing returns with higher computational costs.



\paragraph{Scaling across Different Expert Number and Activated ratios.}
We also conduct experiments on CV datasets fixing total rank as 32 to verify the scalability of our method with different expert numbers and activation ratios, as shown in Figure~\ref{fig:expert_scale}. Key findings include:
(1) With 8 experts, the 2in8 configuration achieves strong performance. Activating more experts may yields lower performance, showing that sparse expert activation is important.
(2) Increasing the total number of experts may improves performance, as seen in 2in8 vs. 4in16 / 8in32 but makes routers harder to train, increases memory consumption, and reduces runtime efficiency. 
(3) GOAT consistently outperforms MoLoRA, especially when activate only one expert, consistent with discussion in \Sec{sec:scale}.
In practice, 2in8 offers a balanced trade-off between performance and storage efficiency. 

\subsection{Routing Analysis}

\begin{figure}[h]
\vspace{-5pt}
    \centering
    \includegraphics[width=0.8\linewidth]{figures/load_balance.pdf}
    \vspace{-3mm}
    \caption{Expert Load Distribution across different tasks. We illustrate the fraction of tokens assigned to each expert $\{e_i\}_{i=1}^8$}
    \label{fig:load}
    \vspace{-4mm}
\end{figure}

We visualize the expert load distribution of models trained on 9 tasks in \Fig{fig:load}. With 8 experts (2 activated), the expected token density is 0.125. The visualization highlights several key observations:  
(1) The load is evenly distributed, with no inactive experts and fluctuations remaining within 0.125, varying by no more than 15\% (0.02).  
(2) CV and GLUE tasks show balanced expert usage, while generation tasks (GSM8k and HumanEval) favor the bottom-2 experts (\(e_1\) and \(e_2\)) with a load around 0.14.  
(3) This validates the effectiveness of each SVD chunk, as experts are initialized with distinct singular value regions.

\subsection{Different Learning Rate}

\newcolumntype{a}{>{\columncolor{mygreen!50}}c}

\begin{table}[ht]
\vspace{-10pt}
\centering
\caption{Performance comparison of different learning rates. \label{tab:lr}}
\resizebox{0.7\linewidth}{!}{%
\begin{tabular}{lcca}
\toprule
\textbf{Learning rate} & \textbf{MoLoRA} & \textbf{HydraLoRA} &  {\textbf{GOAT}}  \\
\midrule
\textbf{$ 1\mathrm{e}^{-5}$} & 56.18  & 55.19  & \textbf{58.74} \\
\textbf{$ 2\mathrm{e}^{-5}$} & 56.63  & 57.39  & \textbf{60.20}  \\
\textbf{$ 5\mathrm{e}^{-5}$} & 60.19  & 60.96  & \textbf{62.05}  \\
\bottomrule
\end{tabular}
}
\vspace{-10pt}
\end{table}

To evaluate GOAT's sensitivity to learning rates, we tested its performance on GSM8K using rates ranging from \(1 \times 10^{-5}\) to \(5 \times 10^{-5}\), comparing it against MoLoRA and HydraLoRA. As shown in \Tab{tab:lr}, GOAT consistently outperforms the other methods, showcasing its robustness and the effectiveness of our initialization and scaling strategies in accelerating convergence and enhancing performance.

\subsection{Computation Analysis}

\begin{table}[h]
\vspace{-10pt}
\centering
\caption{Comparison of LoRA-MoE and Full FT MoE in memory cost, training time, and GSM8K performance. Memory cost was measured and training time was recorded on the MetaMath dataset using one A100 GPU with identical batch sizes.\label{tab:comparison-goat}}
\resizebox{0.85\linewidth}{!}{%
\begin{tabular}{lccccccc}
\toprule
\textbf{Method} & \textbf{Memory Cost} & \textbf{Epoch Time} & \textbf{Performance} \\
\midrule
\textbf{Full FT MoE} & $\ge$ 640 GB & $\approx$106h 03min & $\geq$ 59.36  \\ \midrule
\textbf{MoLoRA}     & 34.85 GB & 36h56min & 56.63 \\
\textbf{HydraLoRA}     & 34.81 GB & 36h56min & 57.39 \\
\rowcolor{mygreen!50} \textbf{GOAT}  & 34.85 GB & 36h59min &  60.20 \\
\bottomrule
\end{tabular}
}\label{tab:cost}
\vspace{-15pt}
\end{table}

\paragraph{Parameter Size.}
The “\# Params (\%)” column in Tables~\ref{tab:vit}, \ref{tab:gen}, \ref{tab:multiqa}, and \ref{tab:nlu} compares the parameter ratios of LoRA baselines and GOAT to full fine-tuning MoE. GOAT achieves state-of-the-art performance with a parameter size of \(O(Hr) + O(He)\), significantly smaller than Full FT's \(O(H^2)\) and Full FT MoE's \(O(kH^2)\). Since \(r, e \ll H\), GOAT is much more efficient. Detailed analysis is in \App{app:parameters}.

\paragraph{FLOPs Analysis} 
To compare with Full FT MoE, we estimate the memory usage, runtime, and performance of FT MoE based on the single GPU runtime of Full FT.
As shown in Table~\ref{tab:cost}, the LoRA-MoE series trains much faster than Full FT MoE. Among LoRA-MoE variants, our method achieves the best performance with identical memory and time costs.  
FLOPs analysis (see \App{app:flops}) reveals that Full FT MoE scales as \(O(ksH^2)\), while LoRA MoE simplifies to \(O(sH^2)\) since \(k < e\) and \(r \ll H\). Thus, LoRA MoE's FLOPs remain nearly constant, independent of \(k\), unlike Full FT MoE, which scales linearly with \(k\).
 

% According to Appendix~\ref{app:flops}, the FLOPs for LoRA-MoE series for LLaMA models are:
% \begin{align}
% BL \left( \frac{52}{3}esh+ \frac{41}{2} sh^2 +4s^2h + \frac{69}{2}kshd\right) + 2BshV
% \end{align}
% where $B$ is batchsize, $L$ is layernumer, $e$ is expert number, $k$ is activate expert number, $h$ is hiddendim, $d$ is each expert rank.
% When \(e=8\), \(k=2\), \(B=32\), \(s=2048\),\(h=4096\) and \(r=8\), the FLOPs ratio of GOAT to Full FT MoE is 52.93\%, which is nearly half of full MoE fine-tuning. 

