@article{Fan_Wei_Qu_Lu_Xie_Cheng_Chen_2024, title={Enhancing Low-Resource Relation Representations through Multi-View Decoupling}, volume={38}, url={https://ojs.aaai.org/index.php/AAAI/article/view/29752}, DOI={10.1609/aaai.v38i16.29752}, abstractNote={Recently, prompt-tuning with pre-trained language models (PLMs) has demonstrated the significantly enhancing ability of relation extraction (RE) tasks. However, in low-resource scenarios, where the available training data is scarce, previous prompt-based methods may still perform poorly for prompt-based representation learning due to a superficial understanding of the relation. To this end, we highlight the importance of learning high-quality relation representation in low-resource scenarios for RE, and propose a novel prompt-based relation representation method, named MVRE (Multi-View Relation Extraction), to better leverage the capacity of PLMs to improve the performance of RE within the low-resource prompt-tuning paradigm. Specifically, MVRE decouples each relation into different perspectives to encompass multi-view relation representations for maximizing the likelihood during relation inference.
Furthermore, we also design a Global-Local loss and a Dynamic-Initialization method for better alignment of the multi-view relation-representing virtual words, containing the semantics of relation labels during the optimization learning process and initialization. Extensive experiments on
three benchmark datasets show that our method can achieve
state-of-the-art in low-resource settings.}, number={16}, journal={Proceedings of the AAAI Conference on Artificial Intelligence}, author={Fan, Chenghao and Wei, Wei and Qu, Xiaoye and Lu, Zhenyi and Xie, Wenfeng and Cheng, Yu and Chen, Dangyang}, year={2024}, month={Mar.}, pages={17968-17976} }

@article{dou2023loramoe,
  title={Loramoe: Revolutionizing mixture of experts for maintaining world knowledge in language model alignment},
  author={Dou, Shihan and Zhou, Enyu and Liu, Yan and Gao, Songyang and Zhao, Jun and Shen, Wei and Zhou, Yuhao and Xi, Zhiheng and Wang, Xiao and Fan, Xiaoran and others},
  journal={arXiv preprint arXiv:2312.09979},
  volume={4},
  number={7},
  year={2023}
}

@misc{hayou2024loraefficientlowrank,
      title={LoRA+: Efficient Low Rank Adaptation of Large Models}, 
      author={Soufiane Hayou and Nikhil Ghosh and Bin Yu},
      year={2024},
      eprint={2402.12354},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2402.12354}, 
}

@inproceedings{hetowards,
  title={Towards a Unified View of Parameter-Efficient Transfer Learning},
  author={He, Junxian and Zhou, Chunting and Ma, Xuezhe and Berg-Kirkpatrick, Taylor and Neubig, Graham},
  booktitle={International Conference on Learning Representations},
  year={2021}
}

@inproceedings{houlsby2019parameter,
  title={Parameter-efficient transfer learning for NLP},
  author={Houlsby, Neil and Giurgiu, Andrei and Jastrzebski, Stanislaw and Morrone, Bruna and De Laroussilhe, Quentin and Gesmundo, Andrea and Attariyan, Mona and Gelly, Sylvain},
  booktitle={International conference on machine learning},
  pages={2790--2799},
  year={2019},
  organization={PMLR}
}

@inproceedings{hulora,
  title={LoRA: Low-Rank Adaptation of Large Language Models},
  author={Hu, Edward J and Wallis, Phillip and Allen-Zhu, Zeyuan and Li, Yuanzhi and Wang, Shean and Wang, Lu and Chen, Weizhu and others},
  booktitle={International Conference on Learning Representations},
  year={2021}
}

@misc{kalajdzievski2023rankstabilizationscalingfactor,
      title={A Rank Stabilization Scaling Factor for Fine-Tuning with LoRA}, 
      author={Damjan Kalajdzievski},
      year={2023},
      eprint={2312.03732},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2312.03732}, 
}

@inproceedings{kopiczkovera,
  title={VeRA: Vector-based Random Matrix Adaptation},
  author={Kopiczko, Dawid Jan and Blankevoort, Tijmen and Asano, Yuki M},
  booktitle={The Twelfth International Conference on Learning Representations},
  year={2023}
}

@inproceedings{lester2021power,
  title={The Power of Scale for Parameter-Efficient Prompt Tuning},
  author={Lester, Brian and Al-Rfou, Rami and Constant, Noah},
  booktitle={Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing},
  pages={3045--3059},
  year={2021}
}

@inproceedings{li2021prefix,
  title={Prefix-Tuning: Optimizing Continuous Prompts for Generation},
  author={Li, Xiang Lisa and Liang, Percy},
  booktitle={Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)},
  pages={4582--4597},
  year={2021}
}

@article{liu2023moelora,
  title={MOELoRA: An MOE-based Parameter Efficient Fine-Tuning Method for Multi-task Medical Applications},
  author={Liu, Qidong and Wu, Xian and Zhao, Xiangyu and Zhu, Yuanshao and Xu, Derong and Tian, Feng and Zheng, Yefeng},
  journal={CoRR},
  year={2023}
}

@article{liu2024gpt,
  title={GPT understands, too},
  author={Liu, Xiao and Zheng, Yanan and Du, Zhengxiao and Ding, Ming and Qian, Yujie and Yang, Zhilin and Tang, Jie},
  journal={AI Open},
  volume={5},
  pages={208--215},
  year={2024},
  publisher={Elsevier}
}

@inproceedings{liudora,
  title={DoRA: Weight-Decomposed Low-Rank Adaptation},
  author={Liu, Shih-yang and Wang, Chien-Yi and Yin, Hongxu and Molchanov, Pavlo and Wang, Yu-Chiang Frank and Cheng, Kwang-Ting and Chen, Min-Hung},
  booktitle={Forty-first International Conference on Machine Learning},
  year={2024}
}

@inproceedings{pfeiffer2021adapterfusion,
  title={AdapterFusion: Non-Destructive Task Composition for Transfer Learning},
  author={Pfeiffer, Jonas and Kamath, Aishwarya and R{\"u}ckl{\'e}, Andreas and Cho, Kyunghyun and Gurevych, Iryna},
  booktitle={Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume},
  pages={487--503},
  year={2021}
}

@inproceedings{ruckle2021adapterdrop,
  title={AdapterDrop: On the Efficiency of Adapters in Transformers},
  author={R{\"u}ckl{\'e}, Andreas and Geigle, Gregor and Glockner, Max and Beck, Tilman and Pfeiffer, Jonas and Reimers, Nils and Gurevych, Iryna},
  booktitle={Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing},
  pages={7930--7946},
  year={2021}
}

@misc{si2024unleashingpowertaskspecificdirections,
      title={Unleashing the Power of Task-Specific Directions in Parameter Efficient Fine-tuning}, 
      author={Chongjie Si and Zhiyi Shi and Shifan Zhang and Xiaokang Yang and Hanspeter Pfister and Wei Shen},
      year={2024},
      eprint={2409.01035},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2409.01035}, 
}

@misc{tian2024hydraloraasymmetricloraarchitecture,
      title={HydraLoRA: An Asymmetric LoRA Architecture for Efficient Fine-Tuning}, 
      author={Chunlin Tian and Zhan Shi and Zhijiang Guo and Li Li and Chengzhong Xu},
      year={2024},
      eprint={2404.19245},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2404.19245}, 
}

@inproceedings{wang2022adamix,
  title={AdaMix: Mixture-of-Adaptations for Parameter-efficient Model Tuning},
  author={Wang, Yaqing and Agarwal, Sahaj and Mukherjee, Subhabrata and Liu, Xiaodong and Gao, Jing and Hassan, Ahmed and Gao, Jianfeng},
  booktitle={Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing},
  pages={5744--5760},
  year={2022}
}

@article{wang2023multilora,
  title={Multilora: Democratizing lora for better multi-task learning},
  author={Wang, Yiming and Lin, Yu and Zeng, Xiaodong and Zhang, Guannan},
  journal={arXiv preprint arXiv:2311.11501},
  year={2023}
}

@misc{wang2024kasaknowledgeawaresingularvalueadaptation,
      title={KaSA: Knowledge-Aware Singular-Value Adaptation of Large Language Models}, 
      author={Fan Wang and Juyong Jiang and Chansung Park and Sunghun Kim and Jing Tang},
      year={2024},
      eprint={2412.06071},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2412.06071}, 
}

@misc{wang2024loraprolowrankadaptersproperly,
      title={LoRA-Pro: Are Low-Rank Adapters Properly Optimized?}, 
      author={Zhengbo Wang and Jian Liang and Ran He and Zilei Wang and Tieniu Tan},
      year={2024},
      eprint={2407.18242},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2407.18242}, 
}

@misc{wang2024miloraharnessingminorsingular,
      title={MiLoRA: Harnessing Minor Singular Components for Parameter-Efficient LLM Finetuning}, 
      author={Hanqing Wang and Yixia Li and Shuo Wang and Guanhua Chen and Yun Chen},
      year={2024},
      eprint={2406.09044},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2406.09044}, 
}

@inproceedings{wanglora,
  title={LoRA-GA: Low-Rank Adaptation with Gradient Approximation},
  author={Wang, Shaowen and Yu, Linxi and Li, Jian},
  booktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},
  year={2024}
}

@inproceedings{xiao2023decomposed,
  title={Decomposed Prompt Tuning via Low-Rank Reparameterization},
  author={Xiao, Yao and Xu, Lu and Li, Jiaxi and Lu, Wei and Li, Xiaoli},
  booktitle={Findings of the Association for Computational Linguistics: EMNLP 2023},
  pages={13335--13347},
  year={2023}
}

@inproceedings{zhangadaptive,
  title={Adaptive Budget Allocation for Parameter-Efficient Fine-Tuning},
  author={Zhang, Qingru and Chen, Minshuo and Bukharin, Alexander and He, Pengcheng and Cheng, Yu and Chen, Weizhu and Zhao, Tuo},
  booktitle={The Eleventh International Conference on Learning Representations},
  year={2023}
}

@misc{zhong2024neatnonlinearparameterefficientadaptation,
      title={NEAT: Nonlinear Parameter-efficient Adaptation of Pre-trained Models}, 
      author={Yibo Zhong and Haoxiang Jiang and Lincan Li and Ryumei Nakada and Tianci Liu and Linjun Zhang and Huaxiu Yao and Haoyu Wang},
      year={2024},
      eprint={2410.01870},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2410.01870}, 
}

