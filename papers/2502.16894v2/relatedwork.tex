\section{Related Work}
% \subsection{Parameter Efficient Fine-Tuning}
% The rapid scaling of large language models (LLMs) has introduced significant challenges in efficiently adapting these models to downstream tasks. To address these challenges, a variety of parameter-efficient fine-tuning (PEFT) methods have been proposed, aiming to reduce computational and memory overhead by updating only a small portion of the modelâ€™s parameters during fine-tuning.
% Overall, these methods can be categorized into three main types:
% (1) Adapter-based methods, which introduce additional trainable modules into the original frozen backbone~\cite{houlsby2019parameter,pfeiffer2021adapterfusion,ruckle2021adapterdrop,hetowards,wang2022adamix,pfeiffer2021adapterfusion}.
% (2) Prompt-based methods, which add extra soft tokens (prompts) to the initial input and focus exclusively on fine-tuning these trainable vectors~\cite{lester2021power,li2021prefix,liu2024gpt,xiao2023decomposed,Fan_Wei_Qu_Lu_Xie_Cheng_Chen_2024}. 
% (3) Low-rank matrix decomposition-based methods, which leverage low-rank approximations to efficiently reparameterize and fine-tune the model's weight updates~\cite{hulora,zhangadaptive,liudora,kopiczkovera}.
% Among these methods, approaches based on LoRA are widely adopted due to their ease of implementation, simplicity, and efficiency.
Since the introduction of LoRA \cite{hulora}, various variants have emerged, focusing on three key areas:
(1) \textit{Architecture Improvements}: DoRA \cite{liudora} decomposes updates into magnitude and direction, while NEAT \cite{zhong2024neatnonlinearparameterefficientadaptation} introduces nonlinear adaptations.
(2) \textit{Adaptive Rank/Scale}, AdaLoRA \cite{zhangadaptive} offers dynamic rank allocation, rsLoRA \cite{kalajdzievski2023rankstabilizationscalingfactor} adjusts scaling factors and LoRA+ \cite{hayou2024loraefficientlowrank} improves learning rate.
(3) \textit{Initialization/Optimization}, PiSSA \cite{meng2024pissa}, MiLoRA \cite{wang2024miloraharnessingminorsingular}, and KaSA \cite{wang2024kasaknowledgeawaresingularvalueadaptation} utilize SVD-based strategies to preserve knowledges. LoRA-Dash \cite{si2024unleashingpowertaskspecificdirections} automates optimal direction discovery, whereas LoRA-GA \cite{wanglora} and LoRA-Pro \cite{wang2024loraprolowrankadaptersproperly} align updates with full fine-tuning gradients. However, they still exhibit performance gap between full fine-tuning. 

Multi-LoRA architectures further boost performance: LoRAHub~\cite{huang2024lorahub} combines task-specific LoRA modules, MoLoRA~\cite{zadouri2024pushing},MoELoRA~\cite{liu2023moelora} and LoRAMoE~\cite{dou2023loramoe} integrate MoE structures with LoRA. 
MultiLoRA~\cite{wang2023multilora} introduces learnable scaling for each expert, while AdaMoLE~\cite{liu2024adamole} introduces learnable thresholds for dynamic experts selection. 
HydraLoRA~\cite{tian2024hydraloraasymmetricloraarchitecture} adopts an asymmetric MoE architecture. Unlike these methods, GOAT introduces a novel SVD-structured MoE framework that adaptively integrates relevant priors while addressing weight misalignment and gradient dynamics through theoretical scaling. 

% \textbf{Our Contribution} simplifies the problem by directly aligning LoRA MoE with Full FT MoE, reducing it to optimizing individual experts. We dynamically initialize LoRA experts with singular vectors of varying magnitudes and train the model to select appropriate experts, optimizing different tokens using distinct subspaces.



% Additionally, multi-LoRA architectures have evolved to further enhance the performance.

% \TODO{Instead of focusing solely on task-specific LoRA combinations or static expert structures, we dynamically initialize LoRA experts with singular vectors of varying magnitudes and train the model to automatically select the appropriate expert, optimizing different tokens using distinct subspaces.}

% Our method proposes a novel MoE framework and initializes different LoRA experts by evenly selecting singular values to fully utilize the features of the original matrix, effectively approximating the performance of full fine-tuning. The model is then trained to automatically select the appropriate LoRA expert, enabling it to adaptively utilize different granularities of LoRA updates during both training and inference.