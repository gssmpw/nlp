

\section{Background and Motivation}

\subsection{Rethinking Singular-Value Initialization}   \label{sec:svd}
% \subsection{init LoRA by Singular Value Decomposition}




% To align with full fine-tuning, we assume that during LoRA training, a substructure of \( W_0 \) is fine-tuned, and its variation is equivalent to the variation achieved through full fine-tuning. Based on this assumption, we can rewrite the LoRA formulation as \( (W_0 - B_0A_0) + BA \), where \( B \) and \( A \) are initialized as \( B_0 \) and \( A_0 \), respectively, to ensure proper initialization. Since we still follow the low-rank assumption of LoRA, where the fine-tuned variation is low-rank, we must ensure that the initial output equals \( W_0 \), rather than letting \( BA \) replace \( W_0 \). In works such as PiSSA~\cite{meng2024pissa}, subspaces constructed from singular vectors are used to form \( BA \).  

Singular-value initialization is widely used in LoRA to preserve pre-trained weight characteristics \cite{zhao2024galorememoryefficientllmtraining, meng2024pissa, wang2024kasaknowledgeawaresingularvalueadaptation, lu2024twinmerging}. PiSSA \cite{meng2024pissa} only updates the largest singular values, while MiLoRA \cite{wang2024miloraharnessingminorsingular} adjusts minor singular values for strong performance.

To unify SVD-based methods with full fine-tuning, let \( W_0 \in \mathbb{R}^{m \times n} \) be the pre-trained weight with SVD, \( W_0 = U \Sigma V^\top \). Assuming \( h = \min(m, n) \) and LoRA rank \( r \), we decompose \( W_0 \) into rank-\( r \) blocks:
\begin{align}
W_0 = \sum_{i=0}^{l} U_i \Sigma_i V_i^\top,
\end{align}
where \( l = \frac{h}{r} - 1 \) and \( i \) denotes the segment \( [i \cdot r : (i+1) \cdot r] \). The submatrices are defined as \( U_i = U_{[i \cdot r : (i+1) \cdot r, :]} \in \mathbb{R}^{r \times m} \), \( \Sigma_i = \Sigma_{[i \cdot r : (i+1) \cdot r, i \cdot r : (i+1) \cdot r]} \in \mathbb{R}^{r \times r} \), and \( V_i = V_{[i \cdot r : (i+1) \cdot r, :]} \in \mathbb{R}^{r \times n} \).
Fine-tuning methods are represented as:

\begin{equation}
\begin{aligned}
\text{Full FT}: &\quad U_0 \Sigma_0 V_0^\top + U_1 \Sigma_1 V_1^\top + \cdots + U_l \Sigma_l V_l^\top \\
\text{PiSSA}: &\quad U_0 \Sigma_0 V_0^\top + (U_1 \Sigma_1 V_1^\top + \cdots + U_l \Sigma_l V_l^\top)^* \\
\text{MiLoRA}: &\quad (U_0 \Sigma_0 V_0^\top + \cdots + U_{l-1} \Sigma_{l-1} V_{l-1}^\top)^* + U_l \Sigma_l V_l^\top \\
\text{KaSA}: &\quad (U_0 \Sigma_0 V_0^\top + \cdots + U_{l-1} \Sigma_{l-1} V_{l-1}^\top)^* + U^{\text{r}} \Sigma^{\text{r}} {V^{\text{r}}}^\top
\end{aligned}
\end{equation}
Here, $(\cdot)^*$ denotes frozen components, while non-frozen components initialize LoRA:
\begin{align}
B = U_i \Sigma_i^{1/2} \in \mathbb{R}^{m \times r}, \quad A = \Sigma_i^{1/2} V_i^\top \in \mathbb{R}^{r \times n}.\label{eq:ba}
\end{align}
We observe PiSSA freezes minor singular values and fine-tunes only the components $U_0 \Sigma_0 V_0^\top$ with the largest norms, achieving the optimal approximation to $W_0$.\footnote{Proof in \App{app:pissa}} In contrast, MiLoRA and KaSA retain segment $0\sim(l-1)$ as preserved pretrained knowledge, but KaSA treats the minor $U_l \Sigma_l V_l^\top$ as noise and replaces it with a new random $U^{\text{r}} \Sigma^{\text{r}} V^{\text{r}\top}$. In practice, PiSSA converges faster by focusing on principal singular values, while MiLoRA and KaSA preserve more pre-trained knowledge for better final performance.

\begin{figure}[t]
% \begin{minipage}{\linewidth}
% \begin{figure}[ht]
    \centering
    \includegraphics[width=1.0\linewidth]{figures/svd.pdf}
    \vspace{-10mm}
    \caption{The effect of initializations from different SVD segments \((u_i, \sigma_i, v_i^\top)\)  for rank 32 and 128. The performance normalized by min-max scaling. }
    \vspace{-10mm}
    \label{fig:intro_rank}
% \end{figure}
% \end{minipage}
\end{figure}

This raises the question: \textit{Is it reasonable to use only the principal or minor part as a fine-tuning prior?} \Fig{fig:intro_rank} illustrates the performance of fine-tuning from different segments \((U_i, \Sigma_i, V_i^{\top}), i\in [0,\cdots,l]\), where each segment is used for initialization while others remain frozen. The x-axis represents segment indices (\eg $x=0$ for PiSSA, $x=l$ for MiLoRA), and the y-axis shows min-max normalized performance.
We can identify two notable observations: (1) \textit{The same initialization exhibits varying trends for different datasets.} For example, $x=l$ achieves better results on the EuroSAT dataset, while $x=0$ performs better on the GTSRB dataset. (2) \textit{Middle segments play a crucial role.} \eg when $r=128$, the highest performance is typically observed in the middle segments.
These findings suggest that each singular value segment contains task-specific information, motivating us to allow the model to automatically select segments during optimization, leveraging all singular values while preserving the original pre-trained matrix characteristics.




 % PiSSA uses the  \( r \) largest singular values and their associated singular vectors to construct \( BA \). It is suggested that the $Sr=(u_0,\sigma_0,v_0^{\top})$ capture the most significant information in \( W \). Leveraging this highly influential subspace helps accelerate convergence and ensures efficient updates.  In contrast, MiLoRA assumes that these smaller singular values, $Sr=(u_{l},\sigma_{l},v_{l}^{\top})$, helps preserve the pre-trained model's world knowledge.

% The residual singular values and vectors are utilized to construct the residual matrix, which remains fixed during fine-tuning:
% \vspace{-5pt}
% \begin{equation}
% \begin{aligned}
% W_{res}=W_0-W_{pri}=W_0-B_0A_0
% \label{eq:w_res}
% \end{aligned}
% \end{equation}
% As shown in Equation~\ref{eq:w_res}, we use \( W_{pri} \) to represent the subspace to be fine-tuned and initialize \( W_0 \) with \( W_{res} \) to ensure that the output at step 0 is equivalent to that of full fine-tuning. During training, \( W_{res} \) remains frozen, while \( B \) and \( A \) are initialized with \( B_0 \) and \( A_0 \), respectively. The output expression is as follows:
% \vspace{-5pt}
% \begin{equation}
% \begin{aligned}
% Y=W_0X=(W_{res}^*+W_{pri})X=(W_{res}^*+BA)X
% \label{eq:pissa}
% \end{aligned}
% \end{equation}

% Similar to LoRA, the gradients of $A$ and $B$ are also given by $\frac{\partial L}{\partial A}=B^T\frac{\partial L}{\partial Y} X^T $ and $\frac{\partial L}{\partial B}=A^T X^T\frac{\partial L}{\partial Y}$. Since elements of $s[:r]\gg$ elements of $s[r:]$, the trainable adapter $W^{pri} = BA$ contains the most essential directions of $W$.

% \paragraph{Which $(u_i,\sigma_i,v_i^{\top})$ to initialize would be best?}\label{sec:svd}
% We can observe that these different initialization are well-grounded in their design, yet their structures are fundamentally opposite.


% \subsection{MoE-type PEFT}
%  Within a MoE layer, N independent experts $\{E_i\}_{i=1}^N$, which employs a trainable matrix $W_g$ to distribute the input vector $x$ among these experts. This router generates a distribution of weights across the experts for each input, using a softmax function for normalization $p_i(X)=Softmax(W_g X)_i$, where $Softmax(...)_i$ denotes $i$th term of the output of softmax function. The resultant output from the MoE layer is a weighted sum of the outputs from the top $K$ experts:
% \begin{equation}
% \begin{aligned}
% Y=\sum_{i-1}^N \frac{TopK(p_i(X))}{\sum_{j=1}^N TopK(p_j(X))} \cdot E_i(x)
% \label{eq:moe}
% \end{aligned}
% \end{equation}
% When combining MoE and PEFT, $E_i(x)$ denotes peft-type expert function: $E_i(x)=B_iA_ix$, where $B_i$ and $A_i$ denote the corresponding LoRA components of the $i$th expert. When introducing LoRA, \Eq{eq:moe} can be rewritten as:
% \begin{equation}
% \begin{aligned}
% Y=WX+\sum_{i-1}^N G_i(X) \cdot E_i(X)
% \label{eq:peft-moe}
% \end{aligned}
% \end{equation}
% where $G_i(X)=\frac{TopK(p_i(X))}{\sum_{j=1}^N TopK(p_j(X))}$.
% Generally, $K$ is set to 1 or 2, meaning that activating only a small number of experts at a time can achieve good results, which helps further sparsify PEFT during inference.

\begin{figure}[t]
% \begin{minipage}{\linewidth}
% \begin{figure}[ht]
    \centering
    \includegraphics[width=0.95\linewidth]{figures/wr.pdf}
    \vspace{-5mm}
    \caption{SVD initialization \vs scaling $s$ and rank $r$ \label{fig:pissa}}
    \vspace{-10pt}
    \label{fig:intro}
% \end{figure}
% \end{minipage}
\end{figure}

\subsection{Rethinking Scaling Factor} \label{sec:scale}

In LoRA, it is common practice to use the scaled variant \( W = W_0 + sBA \), yet the effects of scaling factor $s$ have not been fully explored. 
\citet{biderman2024lora} consider \( s \) should typically set to 2. The SVD-based method \cite{meng2024pissa} empirically makes \( sBA \) independent of \( s \) by dividing \( B \) and \( A \) by \( \sqrt{\frac{1}{s}} \), while \citet{tian2024hydraloraasymmetricloraarchitecture} use larger scaling for LoRA MoE to achieve better performance.

To investigate it, as illustrated on the left of \Fig{fig:pissa}, we first adjust \( s \) in the SVD-based LoRA with a fixed rank, revealing that \( s \) still impacts the convergence speed. To study the effect, we introduce the equivalent weight and gradient to quantify the gap between LoRA and Full FT.
\begin{definition}[Equivalent Weight and Gradient]
\label{def:eg}
For LoRA optimization, we define the equivalent weight as:
\begin{equation}
    \tilde{W} \triangleq W + sBA,
\end{equation}
The equivalent gradient of \( \tilde{W} \) is defined as:
\begin{equation}
    \tilde{g} \triangleq \pderiv{L}{\tilde W}
\end{equation}
where \( s \) is the scaling factor, and \( G^A \) and \( G^B \) are gradients with respect to \( A \) and \( B \), respectively.
\end{definition}

\begin{lemma}\label{th:tidle_g}
Let \( g_t \) be the gradient in full-tuning, and \( B \), \( A \) be the low-rank weights. At the \( t \)-th optimization step, the equivalent gradient can be expressed as:
\begin{equation}
    \tilde{g}_t = s^2 \left( B_t {B_t}^\top g_t + g_t {A_t}^\top A_t \right) \label{eq:tg}
\end{equation}
\end{lemma}
% The formula for SVD-based initialization is:
% \begin{align}
% &\tilde{W} \propto sBA = s \left( \frac{1}{s} U_{r} \Sigma_{r} V_{r}^T \right) = \left( U_{r} \Sigma_{r} V_{r}^T \right) \\
% &\tilde{g} = s^2 \left(  \frac{1}{s} U_{r} U_{r}^\top g +  \frac{1}{s} g V_{r} V_{r}^T \right) = s \left( U_{r} U_{r}^\top g +  g V_{r} V_{r}^T \right)
% \end{align} 
% Though the equivalent weight appears independent of $s$, the equivalent gradient is proportional to $s$, the performance in \Fig{fig:pissa} proves that $s=2$ is typical too small. Thus, increasing the scaling factor in SVD-based methods is a simple way to boost the gradient, leading to faster convergence.

% Next, we study the effect of different ranks, as shown in \Fig{fig:pissa}. When the rank is low (e.g., \( r = 1 \)), the gradient norm is small and shows a different trend compared to \( r = 64 \), leading to a performance gap (95.77 \vs 98.55). However, applying proper scaling (\( s = 16 \)) increases the gradient norm, significantly narrowing the performance gap (from 95.77 to 97.70). This is particularly beneficial in MoE scenarios, where the total rank is divided among experts, resulting in lower ranks for each. Increased scaling compensates for this reduced rank, consistent with \citet{tian2024hydraloraasymmetricloraarchitecture}.
The formula for SVD-based initialization is:
\begin{align}
\tilde{W} &\propto sBA = s \left( \frac{1}{s} U_{r} \Sigma_{r} V_{r}^T \right) = U_{r} \Sigma_{r} V_{r}^T \\
\tilde{g} &= s^2 \left(  \frac{1}{s} U_{r} U_{r}^\top g +  \frac{1}{s} g V_{r} V_{r}^T \right) = s \left( U_{r} U_{r}^\top g +  g V_{r} V_{r}^T \right)
\end{align}
Though the equivalent weight is independent of \( s \), equivalent gradient is proportional to \( s \). As shown in \Fig{fig:pissa}, \( s = 2 \) is too small. Increasing the scaling factor in SVD-based methods boosts the gradient, leading to faster convergence.

Next, we examine the effect of different ranks, as shown in \Fig{fig:pissa}. With low rank (\eg \( r = 1 \)), the gradient norm is small and deviates from the trend of \( r = 64 \), creating a performance gap (95.77 vs. 98.55). However, applying proper scaling (\( s = 16 \)) increases the gradient norm, reducing the performance gap (from 95.77 to 97.70). This is especially beneficial in MoE scenarios, where the total rank is split among experts, resulting in lower ranks. Increased scaling can compensate for this, as supported by \citet{tian2024hydraloraasymmetricloraarchitecture}.

\begin{figure*}[t]
    \centering
    \includegraphics[width=0.75\linewidth]{figures/icml.drawio.pdf}
    \vspace{-2mm}
    \caption{
        \textbf{Illustration of Our Method.} 
        \textit{Single Low-Rank Adaptation}: LoRA reduces trainable parameters by reparameterizing \( W \) as \( W = W_0 + sBA \), with \( B \) and \( A \) as low-rank matrices. 
        \textit{MoE Fine-tuning}: Full MoE fine-tuning, where experts \( W^1 \) and \( W^E \) are selected by the router in this moment.
        \textbf{Subfigure (I)}: Our method replaces the single pair \( B, A \) with multiple pairs \( \{B^i, A^i\}_{i=1}^E \), initialized from different segments of the SVD of \( W_0 \) and adaptively selected by the router.
        \textbf{Subfigure (II)}: We align optimization with SVD-structured MoE by separately aligning each expert. \( W_{\text{res}} \) ensures the equivalent weight equals \( W_0 \) before optimization, and we scale each expertâ€™s equivalent gradient to closely approximate full MoE fine-tuning.
    }
    \vspace{-5mm}
    \label{fig:intro}
\end{figure*}

\section{Method}

\subsection{LoRA MoE Architecture} 
% Typically, in full finetuning,  single weights $W \in \mathbb{R}^{m\times n}$ of the Feed-Forward Network (FFN):
% \vspace{-5pt}
% \begin{equation}
%     \begin{aligned}
%         \mathrm{FFN}(\mathbf{x}) = W(\mathbf{x}) \\
%         W_{t+1} = W_{t} - \eta g_{t} \\
% \end{aligned}
% \end{equation}
% where $\eta$ is the learning rate and $g_{t}$ is the gradient at time step $t$.
\paragraph{Mixture-of-Experts (MoE)} An MoE layer \cite{qu2024llama,zhu2024dynamic,zhu2024llama,zhang2024clip} comprises $E$ linear modules $\{W_{1}, \dots, W_{E}\}$ and a router $W_z \in \mathbb{R}^{m \times E}$ that assigns input $\mathbf{x}$ to experts based on routing scores:
\begin{equation}
    p^i(\mathbf{x}) = \frac{\exp(z^i(\mathbf{x}))}{\sum_{j=1}^{E} \exp(z^j(\mathbf{x}))},
\end{equation}
where $z(\mathbf{x}) = W_z \mathbf{x}$ and $p^i(\mathbf{x})$ is the score for expert $i$.

Let $\Omega_k(\mathbf{x})$ denote the indices of the top-$k$ scores, ensuring $|\Omega_k(\mathbf{x})| = k$ and $z^i(\mathbf{x}) > z^j(\mathbf{x})$ for all $i \in \Omega_k(\mathbf{x})$ and $j \notin \Omega_k(\mathbf{x})$. Define the weights as:
\begin{equation}
    w^i(\mathbf{x}) = 
    \begin{cases} 
        \frac{\exp(z^i(\mathbf{x}))}{\sum_{j \in \Omega_k(\mathbf{x})} \exp(z^j(\mathbf{x}))}, & \text{if } i \in \Omega_k(\mathbf{x}), \\
        0, & \text{otherwise}.
    \end{cases}
    \label{eq:router}
\end{equation}

The MoE layer output is the weighted sum of the top-$k$ experts' outputs:
\begin{equation}
    \mathrm{MoE}(\mathbf{x}) = \sum_{i=1}^E w^i(\mathbf{x}) W^i(\mathbf{x}).
    \label{eq:moe}
\end{equation}

\paragraph{LoRA MoE.} We integrate LoRA into the MoE framework, retaining the router (\Eq{eq:router}) and using the balance loss from vanilla MoE\footnote{See \App{sec:lb}}. Each expert $W^i$ is replaced by low-rank matrices $B^i \in \mathbb{R}^{m \times d}$ and $A^i \in \mathbb{R}^{d \times n}$, where $d = \frac{r}{E}$:
\begin{align}
    \mathrm{MoE}_{\text{LoRA}}(\mathbf{x}) = W(\mathbf{x}) + \sum_{i=1}^E w^i(\mathbf{x}) \left( s B^i A^i (\mathbf{x}) \right) \label{eq:moe_lora} 
    % = \sum_{i=1}^E w^i(\mathbf{x}) (W + s B^i A^i) (\mathbf{x}) =  \sum_{i=1}^E w^i(\mathbf{x}) \tilde{W}^i (\mathbf{x}) \label{eq:eq}
\end{align}
where $W$ is the pre-trained weight matrix and $s$ is the LoRA scaling factor. Since $k \ll E$, LoRA MoE uses fewer active parameters than dense MoE. 

% For each expert $i$, denote the vitual weight $W' = W + B^i_tA^i_t$, we aim for $W' = W^i$. 
% As their gradients are computed as:
% \begin{equation}
% \begin{aligned}
%     W_{t+1}^i &= W_{t}^i - \eta {\color{red} g_{t}^i} \\
%     B_{t+1}^i &= B_{t}^i - \eta {\color{blue} H_{B,t}^i} \\
%     A_{t+1}^i &= A_{t}^i - \eta {\color{blue} H_{A,t}^i}
% \end{aligned}
% \end{equation}
% We have:
% \begin{equation}
%     \begin{aligned}
%            W_{t+1}^{'i} &= W_{t}^{'i} - B_{t}^i A_{t}^i + B_{t+1}^i A_{t+1}^i \\
%     &\approx W_{t}^{'i} - \eta ({\color{blue} H_{B,t}^i} A_t + B_t {\color{blue} H_{A,t}^i})   
%     \end{aligned}
% \end{equation}
% where $H_{B,t}^i$ and $H_{A,t}^i$ are to be determined.

% If we can initialize $W^i_0 = W'= W + B^i_0A^i_0$, then we only have to keep the gradient of $W^i_0$ and $W'$ are same at each optimization step. 
% Our objective becomes minimizing:
% \begin{equation}
% \arg\min_{{\color{blue} H_{B,t}^i}, {\color{blue} H_{A,t}^i}} \left\| {\color{blue} H_{B,t}^i} A_{t}^i + B_{t}^i {\color{blue} H_{A,t}^i} - {\color{red}g_{t}^i} \right\|^2 
% \end{equation}

% Next, we solve this optimization problem. For simplicity, we omit the subscript $t$ and superscript $i$ during the solution process, \ie we consider:
% \begin{equation}
%     \arg\min_{{\color{blue} H_{B}}, {\color{blue} H_{A}}} \left\| {\color{blue} H_{B}} A + B {\color{blue} H_{A}} - {\color{red} g} \right\|^2_F 
% \end{equation}

% The solution to this optimization problem is given by:
% \begin{align}
%     {\color{blue} H_{A}} &= \frac{1}{s^2}(B B^\top)^{-1}{\color{cyan} G_A}  + {\color{magenta}C} A \\
%     {\color{blue} H_{B}} &= \frac{1}{s^2} \bigl(I - B^\top (B B^\top)^{-1} B \bigr) {\color{cyan} G_B} (A A^\top)^{-1} - B{\color{magenta}C} 
% \end{align}

\subsection{Adaptive Priors Initialization}
% According to Figure~\ref{fig:intro}, we observe that different samples require different \((u_i, \sigma_i, v_i^{\top})\) from different chunk .

According to \Sec{sec:svd}, the utilization of different SVD segments depends on the input. We propose initializing each expert in LoRA MoE with different SVD segments, leveraging the MoE architecture to dynamically activate experts associated with different singular values. Specially, we init expert evenly by define the set $\mathcal{E}_r$ as:
\begin{equation}
\begin{aligned}
    \mathcal{E}_r = \left\{(U_{ [:, k:k + d]}, \Sigma_{[k:k+d,k:k+d]}, V_{[k:k+d,:]}^\top) \mid j = 1, \dots, E \right\},
\end{aligned}
\end{equation}
where $t=\frac{\min(m,n)}{E},k=(j-1)t$ is the starting index from segement for $j$-th expert, $d=\frac{r}{E}$ is each expert rank. 
Then we can construct each expert by \(\left( U', \Sigma', V'^{\top} \right) \in \mathcal{E}_r\):
\begin{equation}
\begin{aligned}
    B^i_0 &= \sqrt{\frac{1}{s}} U' \Sigma'^{1/2} \in \mathbb{R}^{m \times d}, A^i_0 &= \sqrt{\frac{1}{s}} \Sigma'^{1/2} V'^{\top} \in \mathbb{R}^{d \times n}
\end{aligned}
\end{equation}
The $B,A$ divide $\sqrt{s}$ to make sure that $sBA$ is independent of $s$ \cite{meng2024pissa}. This allows the model to adapt flexibly to various fine-tuning scenarios. 

\subsection{Theoretical Optimization Alignment}
Directly applying SVD priors in MoE architectures causes weight misalignment and complex gradient dynamics, a challenge not encountered with previous zero initialization methods. Moreover, the gap in MoE-based architectures remains under-explored. We derive the following theorems to address this and show how scaling resolves the issue.
\begin{theorem}\label{th:align}
By ensuring equivalent weight \( \tilde{W}_0 \approx W_0 \) at initialization and maintaining equivalent gradient \( \tilde{g}_t \approx g_t \) throughout optimization, we can align LoRA with Full FT. (See Definition~\ref{def:eg} for equivalent weight and gradient.)
\end{theorem}
Theorem \ref{th:align} addresses the performance gap in single LoRA architectures \cite{wanglora, wang2024loraprolowrankadaptersproperly}. For MoE architectures, however, routers and top-$k$ selection complicate direct alignment. Thus, we focus on Full FT MoE and establish:
\begin{theorem}\label{th:moe_align}
For all \( i \in [1, \dots, E] \), by ensuring equivalent weight \( \tilde{W}^i_0 \approx W^i_0 \) at initialization and gradient \( \tilde{g}^i_t \approx g^i_t \) for each expert, we can align LoRA MoE with an Upcycled MoE with full-rank fine-tuning. 
\end{theorem}
Theorem \ref{th:moe_align} reveals a key connection between LoRA and full fine-tuning in MoE, simplifying the problem to optimizing each expert separately. We outline the steps below.

\paragraph{Initialization Alignment.} 
At initialization, we align the equivalent weight at initialization with an upcycled MoE, where each expert weight $\{W^i\}_{i=1}^{E}$ is derived from the pre-trained model's weight $W_0$ \cite{he2024upcycling}. 
This is equivalent to aligning \( \tilde{W}_0 = W_0 + \sum_{i=1}^E w^i(\mathbf{x}) B^i_0 A^i_0 \) with the original weight \( W_0 \). 
As \(B^i_0,A^i_0\) are initialized with prior information,  
We need additionally subtracting a constant \( W_{\text{res}} \), ensuring the weight alignment:
\begin{equation}
    \tilde{W}_0 = W_0 - W_{\text{res}} + \sum_{i=1}^E w^i(\mathbf{x}) s B^i_0 A^i_0 \approx W_0\label{eq:wres_approx}
\end{equation}
\begin{lemma}\label{th:lema}
For all \( i, j \in [1, \dots, E]\) (\( i \neq j \)):
\begin{align}
\mathbb{E}_{\mathbf{x}}[w^i(\mathbf{x})] &= \frac{1}{E}, \\
\text{Var}(w^i(\mathbf{x})) &= \frac{E-k}{kE^2}
% \text{Cov}(w^i(\mathbf{x}), w^j(\mathbf{x})) &= \frac{k-E}{kE^2(E-1)}.
\end{align}
\end{lemma}
\begin{theorem}\label{th:wi}
Consider the optimization problem:
\begin{equation}
    W_{\text{res}}^+ = \arg\min_{W_{\text{res}}} \mathbb{E}_{\mathbf{x}} \left[ \left\| W_{\text{res}} - s \sum_{i=1}^E w^i(\mathbf{x}) B^i_0 A^i_0 \right\|^2 \right].
\end{equation}
The closed-form solution is \( W_{\text{res}}^+ = \frac{s}{E} \sum_{i=1}^E B^i_0 A^i_0 \).
\end{theorem}
\Theorem{th:wi} provides an appropriate initialization scheme for MoE scenarios. Note that the original LoRA-MoE \cite{zadouri2024pushing,tian2024hydraloraasymmetricloraarchitecture} uses a zero initialization scheme thus $W_{\text{res}}^+ =0$, a special case of \Theorem{th:wi}.

Obviously, the variance of \( W_{\text{res}}^+ - s \sum_{i=1}^E w^i(\mathbf{x}) B^i_0 A^i_0 \) is proportional to \( \sum_{i=1}^E B^i_0 A^i_0 \).
Additionally, from Lemma~\ref{th:lema}, when \( k \) is small (e.g., \( 2k < E \)), \( \text{std}(w^i(\mathbf{x})) > \mathbb{E}[w^i(\mathbf{x})] \). To preserve the informative SVD prior while reducing initialization instability, we scale \( B^i_0 A^i_0 \) by \( \frac{1}{\rho} \), a straightforward method to decrease variance and make a more accurate approximation in \Eq{eq:wres_approx}:
\begin{align}
    B_0^i = \sqrt{\frac{1}{s\rho}} U_i \Sigma_i^{1/2}, A_0^i = \sqrt{\frac{1}{s\rho}} \Sigma_i^{1/2} V_i^\top \label{eq:initAB}
\end{align}

\paragraph{Gradient Alignment}
First, we provide the optimal scaling for zero-initialized LoRA MoE:
\begin{theorem}\label{th:s}
For \( B_0 = 0 \),\( A_0 \sim U\left(-\sqrt{\frac{6}{n}}, \sqrt{\frac{6}{n}}\right) \),$\tilde{g}_t^i = s^2 \left( B_t^i {B_t^i}^\top g_t^i + g_t^i {A_t^i}^\top A_t^i \right)$, and learning rate ratio between full tuning \vs LoRA is $\eta$. 
\begin{equation}
    \arg\min_{s} \left\| \tilde{g}_t^i - g_t^i \right\|, \quad \forall i \in [1, \dots, E]
\end{equation}
The closed-form solution of optimal scaling is \( s = \sqrt{\frac{3n\eta}{r}} \).
\end{theorem}
As \(n \gg r\), it is typically the case that \(s > 2\), which explains why standard scaling is insufficient and why simple scaling enhances effectiveness, as demonstrated in \Sec{sec:scale}.
While it is tricky to directly analyze complex gradient dynamics with SVD priors, an alternative approach is recognizing that larger scaling \(s\) and \(\rho\) ensure $B$ and $A$ become small and approach zero (\Eq{eq:initAB}), aligning with the settings in \Theorem{th:s}. Thus, we adopt this scaling factor in GOAT, and in practice, this approximation performs well (see \Sec{sec:ex}). For scenarios with proper scaling, we extend the method to ``GOAT+'', as detailed in Appendix~\ref{app:goat_pro}.

% \vspace{-5pt}
% \begin{align}
% \E(y^2_i) = s^2 ( BA x )^2 = s^2  \sum_{j=1}^r \sum_{k=1}^n  \E(b^2_{ij}) \E(a^2_{jk}) \\
% = {rn}  \text{Var}(U_0 \Sigma) \text{Var}(\Sigma V_0^\top) =   \frac{rn \Sigma^2}{n^2}  \propto \frac{r}{n} \Sigma^2
% \end{align}


% \TODO{During the forward pass}, we aim to align the singular values of each expert, \(\sqrt{\sigma_i}\), with the largest singular value among the experts, \(\sigma_0\). This helps reduce variance during training. Specifically, we scale \(B_i^0A_i^0\) by a factor of \(\sqrt{\frac{\sigma_i}{\sigma_0}}\) during training to achieve this alignment.
% The formal description is as follows:
% \vspace{-5pt}
% \begin{equation}
% \begin{aligned}
% % &B_0^i =  \sqrt{\frac{1}{\eta}} u_i \sigma_i^{1/2}, A_0^i = \sqrt{\frac{1}{\eta}} \sigma_i^{1/2} v_i^\top\\
% &\mathrm{MoE}_{\text{LoRA}}(\mathbf{x}) = \sum_{i=1}^E  w^i(\mathbf{x}) (W + \sqrt{\frac{\sigma_i}{\sigma_0}}B^i A^i) (\mathbf{x})\\
% \end{aligned}
% \end{equation}
% \vspace{-5pt}
% where $\quad (u_i,\sigma_i,v_i^{\top}) \in S_r$.