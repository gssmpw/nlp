\section{Related Work}
% \subsection{Parameter Efficient Fine-Tuning}
% The rapid scaling of large language models (LLMs) has introduced significant challenges in efficiently adapting these models to downstream tasks. To address these challenges, a variety of parameter-efficient fine-tuning (PEFT) methods have been proposed, aiming to reduce computational and memory overhead by updating only a small portion of the modelâ€™s parameters during fine-tuning.
% Overall, these methods can be categorized into three main types:
% (1) Adapter-based methods, which introduce additional trainable modules into the original frozen backbone____.
% (2) Prompt-based methods, which add extra soft tokens (prompts) to the initial input and focus exclusively on fine-tuning these trainable vectors____. 
% (3) Low-rank matrix decomposition-based methods, which leverage low-rank approximations to efficiently reparameterize and fine-tune the model's weight updates____.
% Among these methods, approaches based on LoRA are widely adopted due to their ease of implementation, simplicity, and efficiency.
Since the introduction of LoRA ____, various variants have emerged, focusing on three key areas:
(1) \textit{Architecture Improvements}: DoRA ____ decomposes updates into magnitude and direction, while NEAT ____ introduces nonlinear adaptations.
(2) \textit{Adaptive Rank/Scale}, AdaLoRA ____ offers dynamic rank allocation, rsLoRA ____ adjusts scaling factors and LoRA+ ____ improves learning rate.
(3) \textit{Initialization/Optimization}, PiSSA ____, MiLoRA ____, and KaSA ____ utilize SVD-based strategies to preserve knowledges. LoRA-Dash ____ automates optimal direction discovery, whereas LoRA-GA ____ and LoRA-Pro ____ align updates with full fine-tuning gradients. However, they still exhibit performance gap between full fine-tuning. 

Multi-LoRA architectures further boost performance: LoRAHub____ combines task-specific LoRA modules, MoLoRA____,MoELoRA____ and LoRAMoE____ integrate MoE structures with LoRA. 
MultiLoRA____ introduces learnable scaling for each expert, while AdaMoLE____ introduces learnable thresholds for dynamic experts selection. 
HydraLoRA____ adopts an asymmetric MoE architecture. Unlike these methods, GOAT introduces a novel SVD-structured MoE framework that adaptively integrates relevant priors while addressing weight misalignment and gradient dynamics through theoretical scaling. 

% \textbf{Our Contribution} simplifies the problem by directly aligning LoRA MoE with Full FT MoE, reducing it to optimizing individual experts. We dynamically initialize LoRA experts with singular vectors of varying magnitudes and train the model to select appropriate experts, optimizing different tokens using distinct subspaces.



% Additionally, multi-LoRA architectures have evolved to further enhance the performance.

% \TODO{Instead of focusing solely on task-specific LoRA combinations or static expert structures, we dynamically initialize LoRA experts with singular vectors of varying magnitudes and train the model to automatically select the appropriate expert, optimizing different tokens using distinct subspaces.}

% Our method proposes a novel MoE framework and initializes different LoRA experts by evenly selecting singular values to fully utilize the features of the original matrix, effectively approximating the performance of full fine-tuning. The model is then trained to automatically select the appropriate LoRA expert, enabling it to adaptively utilize different granularities of LoRA updates during both training and inference.