\section{Introduction}

Recent large language models (LLMs) have shown impressive capabilities \cite{dai2024deepseekmoeultimateexpertspecialization,touvron2023llama2openfoundation,yang2024qwen2technicalreport,openai2024gpt4technicalreport}, but fine-tuning them for downstream tasks is computationally expensive \cite{hulora,zhao2024galorememoryefficientllmtraining}. To reduce costs, parameter-efficient fine-tuning (PEFT) techniques \cite{hulora,pfeiffer2021adapterfusion,houlsby2019parameter,tian2024hydraloraasymmetricloraarchitecture} have been proposed. Among them, LoRA \cite{hulora} is popular for its simplicity and effectiveness. It reparameterizes the weight matrix $W \in \sR^{m \times n}$ into \(W = W_0 + BA\), where $W_0 \in \sR^{m\times n}$ is a frozen full-rank matrix, and \(B \in \mathbb{R}^{m \times r}\),\(A \in \mathbb{R}^{r \times n}\) are low-rank adapters to be learned. 
Since the rank $r \ll \min(m,n)$, LoRA only updates a small fraction of the parameters, greatly reducing memory usage.

Despite its computational efficiency, LoRA often underperforms full fine-tuning (Full FT) \cite{wang2024loraprolowrankadaptersproperly,wanglora,fan2024on}, even with Mixture-of-Experts (MoE) architectures ~\cite{zadouri2024pushing,liu2024adamole,tian2024hydraloraasymmetricloraarchitecture}. 
Our rigorous analysis identifies two key factors limiting LoRA’s performance:
(1) \textit{Suboptimal Initialization}: The isotropic random initialization for matrix \( A \) and zero initialization for matrix \(B\) provide a non-informative prior, resulting in unguided optimization subspaces.
While \citet{wang2024miloraharnessingminorsingular,meng2024pissa} applied singular value decomposition (SVD) for better initialization, their reliance on a static, predefined subset of pre-trained weights limits the capture of the full range of pre-trained knowledge.
It raises the question: \textit{Can we adaptively integrate relevant priors of pre-trained knowledge based on input?} 
% \textcolor{red}{they focus exclusively on either principal or minor singular values, ignoring the middle segments}. 
% their use of SVD information from the original weight is misaligned, losing much information of the matrix and remaining input-independent.
% (1) \textit{Improper Initialization}: The isotropic random initialization for \( A \) and zero initialization for \(B\) provide a non-informative prior, leading to unguided optimization subspaces, slow convergence, and suboptimal performance.
% % ~\cite{meng2024pissa,wanglora}. 
% While \citet{wang2024miloraharnessingminorsingular} and \citet{meng2024pissa} have attempted to adopt singular value decomposition (SVD) on pre-trained weight matrices to improve initialization, 
% % \textcolor{red}{they focus exclusively on either principal or minor singular values, ignoring the middle segments}. 
% their utilization of SVD information from the original matrix is limited by the selection of localized features and remains input-independent.
% % \textcolor{red}{they focus solely on a subset of singular values, thus failing to capture the comprehensive characteristics of the pre-trained weight}. 
% This raises the question: \textit{Is there a more balanced initialization method that fully leverages the characteristics of the original matrix?} 
% Previous works \cite{wanglora,wang2024loraprolowrankadaptersproperly} reveal that LoRA optimization is equivalent to full fine-tuning using a \textit{virtual low-rank gradient}. 
(2) \textit{Unaligned Optimization}: Furthermore, 
% LoRA underperforms Full FT due to its intrinsic low-rank property, causing large gradient gaps and slower convergence in optimization.
the intrinsic low-rank property of LoRA leads to large gradient gaps and slow convergence in optimization, therefore underperforming Full FT.
In LoRA MoE scenarios, the total rank is split among experts, resulting in lower ranks and further increasing this challenge.
Existing strategies \cite{wanglora, wang2024loraprolowrankadaptersproperly} focus only on single LoRA architectures and ignore the added complexity of random top-$k$ routing and multiple expert weights within MoE architecture. 
% specific initialization~\cite{wanglora} and optimization strategies~\cite{wang2024loraprolowrankadaptersproperly} have been proposed to mitigate this gap, the gradient effects of SVD-based initialization or MoE architecture is more complex and remain unexplored}. 
% When using SVD-based initialization in MoE, weight alignment becomes a challenge not faced by previous methods due to zero initialization. 
When SVD-based initialization is applied to LoRA MoE, weight alignment becomes a challenge, which has never been considered in previous methods that used zero initialization.
This further raises the question: 
% \textcolor{red}{Can we mitigate the optimization gap in SVD-based initialization and MoE architectures?}
\textit{How do we mitigate the optimization gap in LoRA MoE initialized with prior information?}
% Specifically, these methods perform SVD decomposition on the original matrix and select only \( r \) singular values and their corresponding subspaces for fine-tuning. For example, selecting the top \( r \) singular values~\cite{meng2024pissa,cao2024sorsasingularvaluesorthonormal,bałazy2024loraxslowrankadaptationextremely} effectively constructs a rank-\( r \) subspace with significant variations from the remaining \( (m-r) \) singular values. This subspace contains rich information, and using this variation to initialize LoRA can accelerate the model's convergence. 
% Similarly, selecting the last \( r \) singular values is also effective~\cite{wang2024miloraharnessingminorsingular}. In this case, the matrix formed by the top \( (m-r) \) singular values represents a subspace with smaller variations, allowing for greater retention of the original matrix's information. Since the matrix formed by the top \( (m-r) \) singular values is the closest approximation to the original matrix, this approach aligns more closely with LoRA's optimization process.

% Recent studies suggest that this limitation arises from the initialization strategy used in LoRA-based methods, where \(B\) is initialized with Gaussian values and \(A\) with zeros. This approach causes the model to optimize trainable parameters within an unguided subspace, leading to slower convergence~\cite{meng2024pissa} and a higher risk of overwriting important pre-trained features~\cite{wang2024miloraharnessingminorsingular}. A group of studies employs SVD to optimize initialization~\cite{meng2024pissa,wang2024miloraharnessingminorsingular,bałazy2024loraxslowrankadaptationextremely,cao2024sorsasingularvaluesorthonormal,wang2024kasaknowledgeawaresingularvalueadaptation}. Among them, PiSSA~\cite{meng2024pissa} initializes and fine-tunes the parts of the base model corresponding to larger singular values, while keeping the smaller singular values fixed, aiming to optimize the model's primary directions for faster convergence. In contrast, MiLoRA~\cite{wang2024miloraharnessingminorsingular} takes the opposite approach by initializing the parts corresponding to smaller singular values and keeping the larger singular values fixed, aiming to reduce the forgetting of the model's world knowledge.


% These two distinctly different initialization methods both seem reasonable, but they also 
% To mitigate these challenges, we propose \textbf{GOAT} (\underline{G}reat L\underline{o}R\underline{A} Mixture-of-exper\underline{t}s), 
% which aligns the initialized weights and the optimization gradients to better match the behavior of full fine-tuning MoE.
% (1) \textit{Initialization}: We reveal that different segments of pretrained knowledge in the SVD structure play a crucial role depending on the input. To adaptively capture pretrained knowledge, we propose a novel strategy for initializing experts in LoRA MoE with distinct singular value segments, leveraging the router to automatically select the appropriate knowledge prior.
% (2) \textit{Optimization}: Instead of directly addressing the gap with Full FT, we focus on an upcycled MoE\footnote{Upcycled MoE initialize each expert by the same pretrained weights} with full-rank finetuning (Full FT MoE). We show that if each low-rank expert approximates the corresponding full expert for each token, the router effect remains consistent, enabling optimization of individual expert weights. Furthermore, we demonstrate that with simplily proper scaling (without altering architecture or training algorithms), both convergence speed and performance of LoRA MoE can be significantly improved. Based on this, we derive the optimal weight alignment strategy for MoE initialized with prior information and a theoretical scaling scheme for better gradient alignment.

To address these challenges, we propose \textbf{GOAT} (\underline{G}reat L\underline{o}R\underline{A} Mixture-of-Exper\underline{t}s), which employs an SVD-structured MoE with theoretical scaling to match full fine-tuning performance. 
Our method highlights two important innovations.
(1) \textit{Initialization}: We demonstrate that different segments of pre-trained knowledge in the SVD structure are crucial depending on the input. To capture this adaptively, we propose initializing LoRA MoE experts with distinct singular value segments, with the router selecting the appropriate prior information.
(2) \textit{Optimization}: 
Rather than directly targeting the gap with full fine-tuning, we focus on an upcycled MoE \footnote{Upcycled MoE initializes all experts with the same pre-trained weights, which we adopt for simplicity.} with full-rank fine-tuning. 
% We show that if each low-rank expert approximates the full expert, the router effect remains consistent, enabling optimization of expert weights. 
{We show that when each low-rank expert plus pre-trained weight approximates its full-rank counterpart, the router's behavior remains consistent, enabling effective optimization of expert weights.}
Through simple scaling, without altering architecture or algorithms, we significantly improve both convergence speed and performance. We also derive the optimal weight alignment strategy and a theoretical scaling scheme for better gradient alignment.

In summary, the contributions of our method are as follows:  
\begin{enumerate}[label=\textbullet,leftmargin=*]
    \item \textit{Adaptive Priors Initialization}: We propose a novel SVD-structured MoE framework that adaptively integrates pre-trained knowledge, addressing the limitations of non-informative or static priors. 
    \item \textit{Theoretical Optimization Alignment}: We reveal a key connection between LoRA and full fine-tuning upcycled MoE, deriving an optimal weight alignment strategy and scaling scheme to close the performance gap.
    \item  \textit{State-of-the-Art Performance}: Extensive experiments on 25 tasks demonstrate that our method achieves superior performance while maintaining scalability. 
\end{enumerate}
