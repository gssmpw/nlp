\begin{abstract}
% Low-rank adaptation (LoRA) is a leading method for parameter-efficient fine-tuning in the era of large language models (LLMs). 
% However, LoRA often underperforms full fine-tuning (Full FT), even when combined with Mixture-of-Experts (MoE) architectures.
% Existing approaches typically employ Singular-value decomposition (SVD) to offer more informative priors. 
% However, they rely on static priors and do not adapt to the specific input, leads to suboptimal use of pretrained knowledge. 
% % fully leverage the characteristics of the pretrained weights, 
% and when directly extended to MoE, they lead to severe weights misalignment due to random top-$k$. 
% Moreover, it is challenge to directly align LoRA MoE gradients with full fine-tuning with the existence of top-$k$ router. We reveals that careful scaling (without any change of the architecture and the training algorithm) can significantly enhance both efficiency and performance. 
% In view of this, we propose \underline{G}reat L\underline{o}R\underline{A} Mixture-of-Exper\underline{t} (GOAT), 
% % a novel framework that balances the utilization of the original matrix by evenly selecting singular values to initialize multiple LoRA experts. Through MoE's dynamic activation strategy, GOAT enables the model to adaptively select feature-rich subspaces for faster training or adjust finer details to retain more knowledge. Additionally, we introduce a scaling MoE optimization strategy, supported by theoretical analysis, to further close the gap with FFT. 
% a novel framework built on two key strategies: (1) Adaptively integrate relevant priors from pretrained knowledge according to the SVD structure through input routing.
% % Initalized each expert from different singular value segments and automatically select most informative priors via MoE router.
% (2) Directly align the low-rank gradient with full fine-tuned MoE rather than Full FT.
% theoretically derive the optimal scaling for aligning the gradients. 
% Extensive experiments across 25 datasets—spanning natural language understanding, commonsense reasoning, image classification, and natural language generation—demonstrate that GOAT achieves state-of-the-art performance and narrows the gap with full fine-tuning effectively.

% Low-rank adaptation (LoRA) is a parameter-efficient fine-tuning method for large language models (LLMs), but it often underperforms full fine-tuning (Full FT), even with Mixture-of-Experts (MoE) architectures. Existing methods use static SVD subsets, leading to suboptimal use of pre-trained knowledge and misalignment of weights in MoE when directly extending SVD, as previous methods did not face this challenge due to zero initialization. Additionally, the gradients become more complex. We show that careful scaling (without altering the architecture or training algorithm) can improve efficiency and performance.

While Low-Rank Adaptation (LoRA) enables parameter-efficient fine-tuning for Large Language Models (LLMs), its performance often falls short of Full Fine-Tuning (Full FT). 
Current methods optimize LoRA by initializing with static singular
value decomposition (SVD) subsets, leading to suboptimal leveraging of pre-trained knowledge. 
{Another path for improving LoRA is incorporating a Mixture-of-Experts (MoE) architecture.}
{However, weight misalignment and complex gradient dynamics make it challenging to adopt SVD prior to the LoRA MoE architecture.} 
To mitigate these issues, we propose \underline{G}reat L\underline{o}R\underline{A} Mixture-of-Exper\underline{t} (GOAT), a framework that (1) adaptively integrates relevant priors using an SVD-structured MoE, and (2) 
aligns optimization with full fine-tuned MoE by deriving a theoretical scaling factor. 
We demonstrate that proper scaling, without modifying the architecture or training algorithms, boosts LoRA MoE’s efficiency and performance. Experiments across 25 datasets, including natural language understanding, commonsense reasoning, image classification, and natural language generation, demonstrate GOAT’s state-of-the-art performance, closing the gap with Full FT. 
% \footnote{Our code is available at:  \url{https://anonymous.4open.science/r/goat-827B}.}

% While Low-Rank Adaptation (LoRA) offers parameter-efficient fine-tuning for Large Language Models (LLMs), its performance often falls short of Full Fine-Tuning (Full FT) performance, even with Mixture-of-Experts (MoE) architectures. 
% Current approaches optimizing LoRA typically employ static singular
% value decomposition (SVD) subsets, leading to suboptimal leveraging of pre-trained knowledge. Moreover, when extending SVD directly to MoE structures, weight misalignment occurs, a challenge absent in prior methods due to zero initialization. This also leads to more intricate gradient dynamics. 
% To mitigate these issues, in this paper, we propose \underline{G}reat L\underline{o}R\underline{A} Mixture-of-Exper\underline{t} (GOAT), a framework that: (1) adaptively integrates relevant priors using an MoE strategy grounded in the SVD structure, and (2) aligns low-rank gradients with full fine-tuned MoE, deriving the optimal scaling for alignment.
% We demonstrate that meticulous scaling, without modifying architecture or training algorithms, can improve both the efficiency and performance of the LoRA MoE structure.
% % \textcolor{red}{of xxx}. 
% Extensive experiments across 25 datasets, including natural language understanding, commonsense reasoning, image classification, and natural language generation, demonstrate that GOAT achieves state-of-the-art performance and effectively closes the gap with Full FT.
% and narrows the gap with full-rank fine-tuning.


\end{abstract}

% \begin{abstract}

% Low-Rank Adaptation (LoRA) is a leading method for parameter-efficient fine-tuning in the era of large language models (LLMs). 
% However, LoRA often underperforms full fine-tuning, even when combined with Sparsely Activated Mixture-of-Experts (MoE). 
% % which scale model capacity without increasing computational costs.
% This work addresses these limitations by identifying and tackling three key challenges:
% (1) Optimal Scaling: We show that previous LoRA methods use suboptimal scaling factors thus cause slowing learning. By deriving a scaling factor proportional to the inverse square root of the rank and number of experts, we accelerate convergence and improve performance. 
% (2) Balanced Initialization: To reduce noise in pre-trained weights within MoE, we introduce a balanced initialization strategy using Singular Value Decomposition (SVD). This ensures stable training dynamics and effective weight transfer.
% (3) Gradient Equivalence: We establish a mathematical equivalence between LoRA optimization and MoE full fine-tuning using low-rank gradients. This insight leads to improved gradient updates through better initialization and adapted parameter update.
% Through extensive experiments across 30 datasets—spanning natural language understanding, reward modeling, image classification, and natural language generation—we demonstrate that our approach significantly boosts LoRA's performance, closing the gap with full fine-tuning or even surpassing it (On 1x datasets).
    
% \end{abstract}