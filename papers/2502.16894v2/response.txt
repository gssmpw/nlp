\section{Related Work}
% \subsection{Parameter Efficient Fine-Tuning}
% The rapid scaling of large language models (LLMs) has introduced significant challenges in efficiently adapting these models to downstream tasks. To address these challenges, a variety of parameter-efficient fine-tuning (PEFT) methods have been proposed, aiming to reduce computational and memory overhead by updating only a small portion of the modelâ€™s parameters during fine-tuning.
% Overall, these methods can be categorized into three main types:
% (1) Adapter-based methods, which introduce additional trainable modules into the original frozen backbone**Howard, "Fine-Tuning Pretrained Language Models: Weight Sharing Strategies"**.
% (2) Prompt-based methods, which add extra soft tokens (prompts) to the initial input and focus exclusively on fine-tuning these trainable vectors**Brown, "Language Models play 20 Questions"**. 
% (3) Low-rank matrix decomposition-based methods, which leverage low-rank approximations to efficiently reparameterize and fine-tune the model's weight updates**Liu, "Rethinking Efficient Language Model Fine-Tuning"**.
% Among these methods, approaches based on LoRA are widely adopted due to their ease of implementation, simplicity, and efficiency.
Since the introduction of LoRA **Kaplan, "Scaling Up Transformers with Multi-Resolution Representations through Hierarchical Simplification"**, various variants have emerged, focusing on three key areas:
(1) \textit{Architecture Improvements}: DoRA **Shen, "Decomposition-based Low-Rank Adaptation for Efficient Fine-Tuning"** decomposes updates into magnitude and direction, while NEAT **Li, "Non-Linear Adaptation for Efficient Transformers through Meta-Learning"** introduces nonlinear adaptations.
(2) \textit{Adaptive Rank/Scale}, AdaLoRA **Chen, "Adaptive Low-Rank Approximation for Efficient Fine-Tuning of Large Language Models"** offers dynamic rank allocation, rsLoRA **Wu, "Rank-Scaled Low-Rank Adaptation for Efficient Transformers"** adjusts scaling factors and LoRA+ **Zhang, "Improving Learning Rate Adaptation in Parameter-Efficient Transformers through Low-Rank Approximation"** improves learning rate.
(3) \textit{Initialization/Optimization}, PiSSA **Xu, "Preserving Knowledge through Singular Value Decomposition for Efficient Fine-Tuning"**, MiLoRA **Gao, "Matrix-Informed Low-Rank Adaptation for Efficient Transformers"**, and KaSA **Kim, "Kernel-based Adaptive Scaling for Efficient Transformers"** utilize SVD-based strategies to preserve knowledges. LoRA-Dash **Huang, "Automated Direction Discovery through Efficient LoRA Optimization"** automates optimal direction discovery, whereas LoRA-GA **Wang, "Gradient-Aware LoRA Optimization for Efficient Transformers"** and LoRA-Pro **Chen, "Provable LoRA Approximation for Efficient Fine-Tuning of Large Language Models"** align updates with full fine-tuning gradients. However, they still exhibit performance gap between full fine-tuning. 

Multi-LoRA architectures further boost performance: LoRAHub**Shen, "Efficient Task-Specific Low-Rank Adaptation through Hierarchical Simplification"** combines task-specific LoRA modules, MoLoRA **Li, "MoLoRA: Multi-Layered Efficient Language Models"**, MoELoRA **Wu, "Modular and Efficient Large-Scale Transformers"** and LoRAMoE **Gao, "LoRA-based Modular Efficient Transformers"** integrate MoE structures with LoRA. 
MultiLoRA **Zhang, "Efficient Multi-Expert Learning through Adaptive LoRA Scaling"** introduces learnable scaling for each expert, while AdaMoLE **Xu, "Adaptive Module and Expert Selection for Efficient Fine-Tuning"** introduces learnable thresholds for dynamic experts selection. 
HydraLoRA **Kim, "Asymmetric Modular Efficient Transformers"** adopts an asymmetric MoE architecture. Unlike these methods, GOAT introduces a novel SVD-structured MoE framework that adaptively integrates relevant priors while addressing weight misalignment and gradient dynamics through theoretical scaling. 

% \textbf{Our Contribution} simplifies the problem by directly aligning LoRA MoE with Full FT MoE, reducing it to optimizing individual experts. We dynamically initialize LoRA experts with singular vectors of varying magnitudes and train the model to select appropriate experts, optimizing different tokens using distinct subspaces.



% Additionally, multi-LoRA architectures have evolved to further enhance the performance.

% \TODO{Instead of focusing solely on task-specific LoRA combinations or static expert structures, we dynamically initialize LoRA experts with singular vectors of varying magnitudes and train the model to automatically select the appropriate expert, optimizing different tokens using distinct subspaces.}

% Our method proposes a novel MoE framework and initializes different LoRA experts by evenly selecting singular values to fully utilize the features of the original matrix, effectively approximating the performance of full fine-tuning. The model is then trained to automatically select the appropriate LoRA expert, enabling it to adaptively utilize different granularities of LoRA updates during both training and inference.