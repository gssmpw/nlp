\begin{abstract}
% \iffalse
% % old
% Since the introduction of the Transformer structure, large-scale pre-trained language models (LM) have been widely adopted. Advances in computing hardware have enabled the creation of larger models with many parameters, known as LLMs. These models perform exceptionally well in various tasks, including natural language processing and beyond. However, selecting the most suitable LLM for specific queries is challenging due to their high cost and latency issues.
% Existing methods for LLM selection fall into two categories: non-predictive and predictive. Non-predictive methods, such as cascading, use smaller models first and then decide whether to switch to larger models. This approach increases costs and complexity. Predictive methods predict query characteristics and route them to a single suitable model. Despite their advancements, these methods do not address the impact of time latency and may leave some queries unanswered.
% This work proposes a predictive pipeline using a contextual multi-armed bandit approach. Each arm represents an LM, and the routing decision is made before the inference stage, avoiding the need to test multiple models sequentially. We enhance query embeddings with domain tags to improve routing accuracy. Our method balances performance, cost, and time at both the set and individual query levels. Remarkable, our routing system achieves 97.70\% of GPT-4's performance while incurring only 26.81\% of the cost
% The code and data are released in this \textcolor{red}{link}link: \url{http://www.example.com}.
% \fi

% new
% sentence 1: what makes the AI task of LLM routing emerging?
%Businesses seek advanced LLM capabilities but face high operational costs and latency issues, which highlights the need for an effective LLM routing system. 
%Although the general knowledge of Large Language Models (LLMs) is appealing, 

Large Language Models (LLMs) exhibit potential artificial generic intelligence recently, 
however, their usage is costly with high response latency. 
% sentence 2 What is the LLMrouting task?
% Given a set of candidate LLMs and a query stream, LLM routing is to identify the most appropriate model for each query in the stream to maximize response quality and minimize cost and latency. 
Given mixed LLMs with their own strengths and weaknesses, LLM routing aims to identify the most suitable model for each query in the stream to maximize response quality and minimize cost and latency.
% sentence 3: why successful LLM routing is important and essential?
%LLM routing can help to improve accurate and timely responses at lower cost.
% sentence 4: why LLM routing is challening (why multi-arm bandits, not iterative greedy, not RL,  why contextual)? why exisitng work is not sufficient to address these challenges?
%personalized metric assessment: personalized quality, cost, latency metrics of LLM-query pairs, trying different LLMs is time-consuming and resource shortage 
%current-past dependency: past LLM-query assignments  will impact current system status (latency and system resource usages), thereafter impact future LLM-query assignments in query stream
%multi-objective tradeoff: aim at the overall tradeoff optimization of quality, cost, and latency.  
% --> use predictive pipeline, multi armed bandit with latency constraint
% However, the main challenges are: (1) balancing personalized metrics of quality, cost, and latency for each query-LLM pair; (2) managing dependencies where past assignments impact current and future system performance; and (3) optimizing the trade-offs among quality, cost, and latency.
%However, the challenges involve: (1) managing personalized metrics of quality, cost, and latency; (2) handling the impact of past assignments on current and future system performance; and (3) optimizing trade-offs among these objectives dynamically.
However, the challenges involve: (1) dynamic trade-offs among quality, cost, and latency; (2) enabling continual learning in deployed systems; and 
(3) navigating a varying (e.g., new LLM addition or old LLM removal) set of LLM candidates over time.  
%(3) flexibly managing the addition and removal of candidate LLMs. 
% sentence 5: what is your unique perspective to address the research gap of LLM routing to make existing system better? only one sentence
%To bridge these gaps, we introduce MixLLM, a dynamic probabilistic selector that leverages contextual information and system latency. It makes decisions before LLM inference to avoid unnecessary model evaluations.
% To bridge these gaps, we develop MixLLM, a dynamic probabilistic selector for query-LLM assignment over query stream. 
To bridge these gaps, we develop MixLLM, a dynamic contextual-bandit-based routing system for query-LLM assignment. 
%To bridge these gaps, we develop MixLLM, a dynamic routing system based on multi-armed bandit for query LLM  assignment in mixed LLMs. 
%sentence 6: what is your proposed framework to implement your unique perspective/insights?
%MixLLM treats each candidate LLM independently and balances performance, cost, and time at both the set and individual query levels.
%Our insight is to see each LLM as an independent task and optimize the trade-off among response quality, cost, and latency at both set and individual levels. 
% sentence 7: what are the key components/steps/goals of your framework, how these steps/components are connected?
%MixLLM begins by incorporating domain-specific tags to enhance query embeddings. Next, lightweight models predict the performance and cost of the query across different LLMs.  Finally, a meta-decision maker evaluates these predictions considering the current dynamic latency, and makes a final decision that balances all factors.
Specifically, we first leverage query tags to enhance query embeddings for the routing task. 
Next, we design lightweight prediction models to estimate the response qualities and costs of queries over LLMs. 
We then devise a meta-decision maker to choose the query-LLM assignments to best tradeoff response quality, cost, and latency. 
%Finally, the system is pre-trained by offline feedback and continual learns from online feedback.
Finally, the system benefits from continual training, allowing it to adapt to evolving queries and user feedback over time.
% sentence 8: extensitive experiments and results/observations/improvements/exciting benefits
% Our analysis finds a correlation between tags and LLM performance, which indicates that tags-enhanced embeddings effectively capture routing-specific information.  Notably, MixLLMm achieves 97.70\% of GPT-4's performance at just 26.81\% of the cost.  Compared to baselines, MixLLM successfully avoids high latency to ensure high-quality and timely responses. MixLLM also shows scalability and adaptability as LLM candidates evolve, with a one-time setup for long-term usability.
Our extensive experiments show that MixLLM achieves the best trade-offs in response quality, cost, and latency (97.25\% of GPT-4's quality at 24.18\% of the cost under the time constraint).
%, demonstrate how query tags enhance query-LLM evaluations, and highlight the system's scalability and adaptability for evolving LLM candidates.
% sentence 9: the code is publically available in xxxx
% The code and data are available in the link: \url{https://anonymous.4open.science/r/MixLLM-859F/}.

\end{abstract}