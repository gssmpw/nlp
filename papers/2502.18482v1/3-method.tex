

\section{Methodology}



\subsection{The Dynamic LLM Routing Task}
We study the problem of dynamic LLM routing with streaming queries. Given queries that arrive sequentially, our goal is to assign each query to the most appropriate LLM selected from a set of candidates to trade off response quality, cost, and latency.
Formally, let the set of streaming queries be:
$
    Q = \{q_n\}_{n=1}^{|Q|}, % Set of queries
$
and the set of LLM candidates be:
$
    M = \{m_l\}_{l=1}^{|M|}.
$
The objective is to select the most suitable LLM $m_n^*$ for the query $q_n$.
% learn the optimal query-LLM assignments:
% \begin{equation}
% S_{n,l}= \left\{ 
%   \begin{array}{ c l }
%     1  & \textrm{if the $q_n$ pairs with the $m_l$}  \\
%     0                 &  \textrm{otherwise}
%   \end{array}
% \right.
% \end{equation}
% to optimize the trade-off between response quality, costs, and latency. 
% \wxy{redundant? Yanjie:use an equation to illustrate your objective function, rather than texts}

\subsection{Overview of The MixLLM Framework}



% In this work, we propose MixLLM. Although the problem is fundamentally at the set level, we break it down to the query level to ensure that each query is answered by a suitable LLM. Specifically, the input consists of a single query and a set of candidate LLMs, while the output is the chosen LLM, identified as the most suitable for the given query under the current situation. Our method is based on the contextual multi-armed bandit algorithm, as illustrated in Figure \ref{method_overview}. 
% % The contextual information includes the query embedding and the candidate LLMs. 
% First, a tag-enhanced encoder is employed to generate the query embedding, incorporating relevant contextual information. Next, the response quality and cost for each candidate LLM are predicted. Finally, a meta decision maker selects the most suitable LLM based on the predicted values, taking into account the latency constraint and uncertainty.
%We propose MixLLM, a dynamic routing system that selects the most suitable LLM from mixed LLM candidates to balance quality, cost, and latency. 
\textbf{Figure~\ref{method_overview}} shows that MixLLM consists of four components: (1) tag-enhanced query embedding, (2) LLM-specific prediction, (3) meta decision maker, and (4) continual learning mechanism. This framework allows MixLLM to route queries to LLMs in a dynamic system while achieving quality-cost-latency trade-offs and continual learning with a changing LLM candidate set.

\subsection{Tag-enhanced Query Embedding via Unsupervised Fine-tuning}
%\textbf{Why Fine-tune Query Encoder?} 
A query can be seen as a token sequence, thus, its embedding can be generated using a pre-trained encoder (e.g., BERT~\cite{devlin-etal-2019-bert}):
$
    \mathbf{e}_n = \texttt{Encoder}(q_n),
$
where $\mathbf{e}_n$ represents the embedding of $n$-th query $q_n$ in a query stream.
However, such general-purpose query embeddings contain too much noises and are not tailored for LLM routing. 
To address this limitation, we propose enhancing the encoder by introducing tag knowledge, which enriches the query embeddings and improves their effectiveness for routing tasks.

%\noindent\textbf{Why Use Tags?}
Different LLMs can be proficient in different domains (e.g., Science, Legal)~\cite{liu-etal-2024-mathbench}.
Using GPT-4 as an example, \textbf{Figure~\ref{fig:tags_embedding}} shows a clear correlation between domain and response quality. The query distribution after t-SNE dimension reduction is shown in \textbf{Figure~\ref{fig:tags_a}}, with each color representing a specific domain. \textbf{Figure~\ref{fig:tags_b}} highlights GPT-4's response quality.
% with different colors. 
It is evident that GPT-4 has a higher error frequency (\textcolor{orange}{orange} points in \textbf{Figure~\ref{fig:tags_b}}) in the ``Legal'' (\textcolor{red}{red} points in \textbf{Figure~\ref{fig:tags_a}}) and ``Math'' (\textcolor{myPurple}{purple} points in \textbf{Figure~\ref{fig:tags_a}}) domains. 
% These observations inspire us to develop the tag-enhanced embedding approach. By incorporating tags and the domains derived from them, we can guide embeddings to capture these distinctions, making them more suitable for LLM routing tasks.
% These observations inspire us to develop the tag-enhanced embedding approach. By incorporating tags and domains derived from them, we can guide embeddings to capture these distinctions, making them more suitable for LLM routing tasks.
These observations inspire us to develop the tag-enhanced embedding approach. By incorporating tags and their derived domains, we can guide embeddings to capture these distinctions, making them more suitable for LLM routing tasks.



% Then, we visualize the distribution of query embeddings labeled by domains and LLM response quality to support the statement above.
% \textbf{Figure~\ref{fig:tags_embedding}} shows a clear correlation between domains and response quality. \textbf{Figure~\ref{fig:tags_a}} shows the distribution of queries after dimension reduction via t-SNE, with each color representing a different domain. \textbf{Figure~\ref{fig:tags_b}} highlights GPT-4's response quality on these queries. 
% It is evident that GPT-4 has a higher frequency of errors (marked as \textcolor{orange}{orange}) in the \textcolor{red}{red} and \textcolor{myPurple}{purple} domains. 
% These observations inspire us to develop the tag-enhanced embedding approach.

\begin{figure}[htbp]
\centering
% Subfigure a
\begin{subfigure}{0.49\linewidth}
\centering
\includegraphics[width=\linewidth]{fig/tags_domain.png}
\caption{Domain Visualization}
\label{fig:tags_a}
\end{subfigure}
\hfill
% Subfigure b
\begin{subfigure}{0.49\linewidth}
\centering
\includegraphics[width=\linewidth]{fig/tags_gpt4.png}
\caption{Quality Visualization}
\label{fig:tags_b}
\end{subfigure}

\caption{Domain-Quality Correlation}
\label{fig:tags_embedding}
\end{figure}

\noindent{\bf Step 1: Automated Query Tag Generation.}
% To prepare, we employ the InsTag~\cite{lu2023instag} model to generate fine-grained tags for each query, and then manually cluster tags into coarse-grained domain cluster set $D$. 
To prepare, we employ the InsTag~\cite{lu2023instag} model to generate fine-grained tags for each query and manually cluster the tags into a set of coarse-grained domains, denoted as \( D \). 
InsTag is an instruction tagging method designed to quantify the diversity and complexity of human instructions, and these tags contribute to model fine-tuning.

% \noindent{\bf Step 1: Automated Query Tag Generation.}
% To prepare, we employ the InsTag~\cite{lu2023instag} model to generate fine-grained tags for each query, and then cluster tags into $d$ coarse-grained domains. 
% InsTag is an instruction (query) tagging method designed to quantify the diversity and complexity of human instructions, and these tags subsequently contribute to supervised fine-tuning. 
% \textbf{\color{red}  what are you talking about? which part is relevant to tag extraction? mind your sentence-sentence transition!-->} \textbf{InsTag quantifies LLM instruction diversity and complexity, improving model alignment and performance by selecting diverse and effective samples for fine-tuning. }

% \noindent{\bf Step 2: General Query Embedding Extraction.} 
% We leverage the sentence-level encoder to learn general query embedding. Formally, for the $n$-th query $q_n$ in a query stream, the corresponding embedding is given by:
% $
%     \mathbf{e}_n = \texttt{Encoder}(q_n),
% $ where the encoder is a BERT-based model. 

\noindent{\bf Step 2: Unsupervised Fine-tuning of Encoder.} 
While the InsTag model, backed by Llama-2 13B, is too large to be used during inference, we fine-tune the BERT encoder during the training stage.
We develop an unsupervised optimization objective that integrates intra-domain similarity ($\mathcal{L}_{\texttt{intra}}$) and inter-domain separation ($\mathcal{L}_{\texttt{inter}}$):
% We develop an unsupervised optimization objective that integrates intra-domain similarity ($\mathcal{L}_{\texttt{intra}}$) and inter-domain separation ($\mathcal{L}_{\texttt{inter}}$) to fine-tune the BERT encoder:
\begin{equation} \label{total_loss}
    \mathcal{L} = \mathcal{L}_{\texttt{intra}} + \mathcal{L}_{\texttt{inter}},
\end{equation}
where the intra-domain similarity loss encourages embeddings within the same domain cluster to be close to their center $\boldsymbol{\mu}_j$:
% \begin{equation} \label{variation_loss}
%     \mathcal{L}_{\texttt{intra}} = -\frac{1}{|Q|} \sum_{i=1}^{|Q|} \log \frac{\exp(\mathbf{e}_i \cdot \boldsymbol{\mu}_{i} / \sigma)}{\sum_{j=1}^{d} \exp(\mathbf{e}_i \cdot \boldsymbol{\mu}_j / \sigma)}.
% \end{equation}
\begin{equation} \label{variation_loss}
    \mathcal{L}_{\texttt{intra}} = -\frac{1}{|Q|} \sum_{i=1}^{|Q|} \log \frac{\exp(\mathbf{e}_i \cdot \boldsymbol{\mu}_{i})}{\sum_{j=1}^{|D|} \exp(\mathbf{e}_i \cdot \boldsymbol{\mu}_j)}.
\end{equation}
The inter-domain separation loss ensures that different domain centers are distinct:
\begin{equation} \label{separation_loss}
    \mathcal{L}_{\texttt{inter}} = \frac{1}{|D|} \sum_{j=1}^{|D|} \log \sum_{k \neq j} \exp(\boldsymbol{\mu}_j \cdot \boldsymbol{\mu}_k).
\end{equation}


\subsection{LLM-Specific Quality and Cost Prediction}
Given a query embedding, we aim to predict both the response quality and financial cost for each candidate LLM on the query, so the meta decision-maker can assign the most suitable model.

% \noindent\textbf{Step 1: Embedding A Query-LLM Pair. } 
% In addition to the query embedding, each of the $L$ candidate LLMs is associated with a textual statement that describes its characteristics and features. These LLM related texts are embedded through the BERT Encoder to obtain LLM embedding. The final embedding for a query-LLM pair is the concatenation of a query embedding vector and a LLM  embedding vector. 

\noindent\textbf{Step 1: Estimating the Response Quality of A Query-LLM Pair.} 
Since different LLMs have different response qualities, we learn an LLM-specific regression function for each LLM. This function estimates the response quality of the $n$-th query on the $l$-th LLM:
\begin{equation}
    \hat{p}_{n,l} = f_l^\texttt{rq}(\mathbf{e}_n; \boldsymbol{\theta}_l^\texttt{rq}),
\end{equation}

\iffalse
We define a set of LLM-specific predictive functions $F^P = \{f_1^P, f_2^P, \ldots, f_L^P\}$ to predict the response quality:
\begin{equation}
    \hat{p}_{n,l} = f_l^P(\mathbf{e}_n; \boldsymbol{\theta}_l^P),
\end{equation}
where $f_l^P$ predicts the expected response quality of $\texttt{LLM}_l$ for query $q_n$ parameterized by $\boldsymbol{\theta}_l^P$. These models are trained using historical data. Moreover, training-free models may also work, such as scaling laws~\cite{ruan2024observational}.
%where each \( f_l^P \) is am arm-specific predictors parameterized by \( \boldsymbol{\theta}_l^P \).
% They can be implemented by various methods. The most intuitive approach is to use regressors. However, training-free models also work. For example, the response quality of larger LMs can be estimated by scaling laws~\cite{ruan2024observational} derived from smaller LMs.
\fi

\noindent\textbf{Step 2: Estimating the Financial Cost of A Query-LLM Pair.} 
The total cost of the $n$-th query on the $l$-th LLM includes: 
1) the known input cost and 2) the predicted output cost, according to typical LLM pricing policies:
\begin{equation}
    \begin{aligned}
    \hat{c}_{n,l} = &\underbrace{\texttt{len}_{n,l}^{\texttt{prm}} \cdot \texttt{price}_l^{\texttt{prm}}}_{\text{input cost}} + \underbrace{\hat{\texttt{len}_{n,l}^{\texttt{res}}} \cdot \texttt{price}_l^{\texttt{res}}}_{\text{output cost}},
    \end{aligned}
\end{equation}
where $\texttt{len}_{n,l}^{\texttt{prm}}$ is the prompt length of query $q_n$, and $\texttt{price}_l^{\texttt{prm}}$ and $\texttt{price}_l^{\texttt{res}}$ are unit prices of input prompt and output response.
The response length $\hat{\texttt{len}_{n,l}^{\texttt{res}}}$ is predicted using a similar method as the response quality predictors:
% $F^{RL} = \{f_1^{RL}, f_2^{RL}, \ldots, f_L^{RL}\}$:

\begin{equation}
    \hat{\texttt{len}_{n,l}^{\texttt{res}}} = f_l^\texttt{rl}(\mathbf{e}_n; \boldsymbol{\theta}_l^\texttt{rl}),
\end{equation}


\subsection{Meta Decision Maker}

For the $n$-th query $q_n$, the final decision score for each candidate LLM is determined by three factors: (1) $s_{n,l}^\texttt{trade}$, which trade-offs the predicted quality and cost; (2) $s_{n,l}^\texttt{unc}$, which accounts for potential prediction uncertainty; and (3) $s_{l}^\texttt{pen}$, which discourages selecting candidates with long waiting time:

% {\small
\begin{equation} \label{final_scores}
    % s_{n,l} = s_{n,l}^t + \alpha \cdot s_{n,l}^\texttt{unc} - \beta \cdot s_{l}^\texttt{pen},
    s_{n,l} = s_{n,l}^\texttt{trade} + \alpha \cdot s_{n,l}^\texttt{unc} - \beta \cdot s_{l}^\texttt{pen}.
\end{equation}
% }
where $\alpha$ and $\beta$ control the relative importance.

The willingness to pay $\lambda$ is introduced in $s_{n,l}^\texttt{trade}$ to control the priority of quality over cost, leading to different budgets accordingly:
\begin{equation}
\label{reward_score}
    s_{n,l}^\texttt{trade} = \frac{\lambda}{\lambda + 1} \cdot \hat{p}_{n,l} - \frac{1}{\lambda + 1} \cdot \hat{c}_{n,l},
\end{equation}


To handle prediction errors, we introduce an uncertainty measurement ($s_{n,l}^\texttt{unc}$) to enhance robustness~\cite{li2010contextual}:
\begin{equation}
    s_{n,l}^\texttt{unc} = \mathbf{e}_n^T \cdot \mathbf{A}_l^{-1} \cdot \mathbf{e}_n,
\end{equation}
where \( \mathbf{A}_l^{-1} \) represents the inverse covariance matrix for the \( l \)-th LLM. This measures the amount of information gathered for each candidate and adjusts the confidence of the prediction accordingly. 


Considering hardware limitations, it is crucial to avoid routing queries to candidates with excessively long waiting time. The penalty is given by:
\begin{equation}
\label{penalty_score}
    s_{l}^\texttt{pen} = e^{\gamma \cdot (w_l - \xi \cdot \tau)},
\end{equation}
where $\gamma$ is a scaling factor and $\tau$ represents the maximum tolerable waiting time. 
The waiting time $w_l$ for candidate $l$ includes: 1) the initial latency required for the LLMs to start and 2) the token output time for generating each token in the response.
The coefficient $\xi$ (smaller than 1) makes the penalty stronger. By scaling the threshold to $\xi \cdot \tau$, the system applies the penalty earlier, discouraging the selection of candidates before their waiting time reaches the full limit of $\tau$.



Finally, the candidate with the highest score is selected as the most suitable one:
\begin{equation}
    m_n^* = \arg\max_{l} (s_{n,l})
\end{equation}

% \lyc{However, if the difference between the highest and the second-highest scores is below a certain threshold, indicating no significant distinction, we reconsider the choice and select the arm with the higher predicted response quality $\hat{p}_n^l$.}


\subsection{Continual Learning}
To ensure effectiveness in real-world applications, we designed both offline and online training modes. The offline mode enables the model to achieve robust performance before deployment, while the online mode allows the model to continuously improve in response to changing environments and user feedback.

\textbf{Offline Training}: 
% occurs before the deployment of the routing system, where each query is answered by all candidate LLMs, a method referred to as “full feedback.”To adapt MixLLM to streaming scenarios, training is conducted in time steps. At each step, training includes updating the quality, cost, and uncertainty predictors for each arm. The waiting time is then adjusted according to the choice.
Prior to deployment, we perform offline training using refined feedback from all candidate LLMs.  
The refined feedback includes real response quality and length, which involves updating the parameters of the predictive models:

The parameters \( \boldsymbol{\theta}_l^\texttt{rq} \) for the response quality predictors are updated using gradient descent:
\begin{equation}
    \boldsymbol{\theta}_l^\texttt{rq} := \boldsymbol{\theta}_l^\texttt{rq} - \eta_1 \cdot \nabla_{\boldsymbol{\theta}_l^\texttt{rq}} \mathcal{L}(p_{n,l}, \hat{p}_{n,l}),
    \label{rqupdate}
\end{equation}

Similarly, the response length predictor parameters \( \boldsymbol{\theta}_l^\texttt{RL} \) are updated as:
% {\small
\begin{equation}
    \boldsymbol{\theta}_l^\texttt{rl} := \boldsymbol{\theta}_l^\texttt{rl} - \eta_2 \cdot \nabla_{\boldsymbol{\theta}_l^\texttt{rl}} \mathcal{L}(\texttt{len}_{n,l}^{\texttt{res}}, \hat{\texttt{len}_{n,l}^{\texttt{res}}}),
    \label{rlupdate}
\end{equation}
% }


% The uncertainty matrices \( A_l \) are updated as:
The uncertainty matrices \( \mathbf{A}_l \) are updated incrementally by query embeddings:
\begin{equation}
    \mathbf{A}_l := \mathbf{A}_l + \mathbf{e}_n^T \cdot \mathbf{e}_n.
    \label{uncupdate}
\end{equation}

% This update reflects the accumulation of new information over time. As more data is collected, the inverse matrix \( A_l^{-1} \) becomes smaller, indicating higher confidence in the predictions.
This update accumulates information over time, decreasing the inverse \( \mathbf{A}_l^{-1} \), which leads to low uncertainty, indicating increased confidence in predictions.
Then the waiting time is adjusted based on the LLM assignment.


\textbf{Online Training}: 
After deployment, the system incrementally updates predictive models and uncertainty matrices using refined single feedback from the selected LLMs.

However, user feedback based on human satisfaction with the LLM service, often binary (``good'' or ``not good''), is challenging for training. To address this, we introduce a Dynamic Feedback Score (\( s_{n,l}^{\texttt{df}} \)) based on the contextual bandit method to capture the binary user feedback and dynamically adjust the scoring mechanism.

The final score for each LLM is updated as:
\begin{equation} \label{updated_final_scores}
    s'_{n,l} = s_{n,l} + \kappa_{n,l} \cdot s_{n,l}^{\texttt{df}},
\end{equation}
% {\small
% \begin{equation} \label{updated_final_scores}
%     s_{n,l} = t_{n,l} + \alpha \cdot \texttt{Uncertainty}_{n,l} - \beta \cdot \texttt{Penalty}(w_l) + \kappa_{n,l} \cdot \texttt{DFS}_{n,l},
% \end{equation}
% }
% where a shared network predicts $s_{n,l}^{\texttt{df}}$:
where $s_{n,l}^{\texttt{df}}$ represents the appropriateness of the $l$-th LLM to answer the given query predicted by a shared 3-layer MLP network:
\begin{equation}
    \left[ s_{n,1}^{\texttt{df}}, s_{n,2}^{\texttt{df}}, \dots, s_{n,|M|}^{\texttt{df}} \right] = f^{\texttt{df}}(\mathbf{e}_n; \boldsymbol{\theta}^{\texttt{df}}).
\end{equation}

And \( \kappa_{n,l} \) is the confidence factor based on the variance, to ensure the reliability of $s_{n,l}^{\texttt{df}}$ and prevent over-reliance on unstable predictions:
\begin{equation}
    \kappa_{n,l} = \frac{1}{\operatorname{Var}_n[s_{n,l}^{\texttt{df}}] + \epsilon},
\end{equation}
where \( \epsilon \) is a small constant to avoid division by zero. Low variance increases \( \kappa_{n,l} \), which will enhance the importance, while high variance decreases it, which reflects instability.
Then the candidate with the highest score is selected:
\begin{equation}
    m_n^* = \arg\max_{l} (s'_{n,l})
\end{equation}

Since we cannot directly supervise the network outputs with binary feedback $r_n$, we apply the \textbf{Policy Gradient} method~\cite{ban2021multi} to update \( \boldsymbol{\theta}^{\texttt{df}} \). The probability of selecting candidate \( l \) is:
\begin{equation}
    \pi(l \mid \mathbf{e}_n; \boldsymbol{\theta}^{\texttt{df}}) = \frac{\exp\left( s_{n,l}^{\texttt{df}} \right)}{\sum_{k=1}^{|M|} \exp\left( s_{n,k}^{\texttt{df}} \right)}.
\end{equation}

The goal is to maximize the expected reward:
\begin{equation}
    J(\boldsymbol{\theta}^{\texttt{df}}) = \mathbb{E}_{l \sim \pi(\cdot \mid \mathbf{e}_n; \boldsymbol{\theta}^{\texttt{df}})} [ r_n ],
\end{equation}
with gradient on selected candidate $m_n^*$:
\begin{multline}
    \nabla_{\boldsymbol{\theta}^{\texttt{df}}} \log \pi(m_n^* \mid \mathbf{e}_n; \boldsymbol{\theta}^{\texttt{df}}) = \\
    \nabla_{\boldsymbol{\theta}^{\texttt{df}}} \left( s_{n,m_n^*}^{\texttt{df}} - \log \sum_{k=1}^L \exp\left( s_{n,k}^{\texttt{df}} \right) \right)
\end{multline}

The parameters are updated as:
% {\small
\begin{equation}
    \boldsymbol{\theta}^{\texttt{df}} := \boldsymbol{\theta}^{\texttt{df}} - \eta_3 \cdot \nabla_{\boldsymbol{\theta}^{\texttt{df}}} \log \pi(m_n^* \mid \mathbf{e}_n; \boldsymbol{\theta}^{\texttt{df}}) \cdot r_n.
\end{equation}
% }



