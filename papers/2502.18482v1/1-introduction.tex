\section{Introduction}
% \textcolor{red}{new gaps same as abstract}
% highlight
% background: dynamic
% task and motivation
% challenge: cascading-->predictive, latency-->


% \subsection{background}
% Since the introduction of the Transformer structure \cite{vaswani2017attention}, large-scale pre-trained language models have been widely used \cite{brown2020language, raffel2020exploring, radford2019language}. The two most notable models are BERT \cite{devlin2018bert} and GPT \cite{radford2018improving, radford2019language, brown2020language}, representing the Transformer encoder and decoder, respectively. With advancements in computing hardware, larger models with many more parameters have been developed. These models are known as Large Language Models (LLMs) \cite{brown2020language, chowdhery2023palm, touvron2023llama}.

%LLMs like GPT \cite{radford2018improving, radford2019language, brown2020language}, %built upon the Transformer architecture \cite{vaswani2017attention}, have revolutionized natural language processing (NLP) by leveraging vast amounts of data and computational power \cite{raffel2020exploring, chowdhery2023palm, touvron2023llama}.
Large Language Models (LLMs) have exhibited abilities to understand massive texts, generate actionable knowledge, enable contextual reasoning, and innovate diverse applications \cite{radford2018improving, radford2019language, brown2020language, raffel2020exploring, chowdhery2023palm, touvron2023llama}.
%These advancements have also extended beyond NLP, influencing fields like healthcare, law, and science \cite{bommasani2021opportunities, birhane2023science}. %The multi-task capability has become more important \cite{wei2022emergent, tay-etal-2023-transcending}.
However, deploying LLMs presents unique challenges in managing computational resources, optimizing response times, and ensuring scalability. 

% The increase in model size has led to improvements in quality. LLMs have achieved superior response quality not only in natural language processing tasks \cite{yang2024harnessing} but also in other fields \cite{bommasani2021opportunities, birhane2023science}. Currently, LLMs are being used as universal models for multiple tasks. The multi-task capability has become a key metric for evaluating an LLM \cite{brown2020language, wei2022emergent, tay2022transcending}. However, there are many LLMs available, each with its own strengths and weaknesses.

% However, there are many LLMs available, each with its own strengths and weaknesses. Fine-tuning or merely running inference on LLMs is very costly due to their large size \cite{tay2022ul2}. Therefore, selecting the most suitable LLM for specific queries is challenging but important. As shown in Figure \ref{intro_question}, each LLM has different features. We want the most powerful one, but we are often constrained by budgets and latency. For example, GPT-4 \cite{achiam2023gpt} delivers superior response quality but often incurs high economic and computational costs. Limited budgets prevent frequent use of such a large model. Additionally, high-demand queries with limited hardware capacity require parallel processing. Relying on one or two LLMs can significantly increase latency, preventing timely responses for users.

As shown in \textbf{Figure \ref{intro_question}}, the diversity of available LLMs~\cite{jiang2024empowering, gong2024evolutionary, li2023sehf, li2024sade, wang2022hierarchal, wang2024llm, wang2024lcmdc}, each with different strengths and weaknesses, poses a challenge when selecting the most appropriate model for a given task. More powerful models, such as GPT-4 \cite{achiam2023gpt}, can deliver high-quality responses, but the pricy cost and computational requirements limit their accessibility. 
Thus, LLM routing, which chooses the most suitable LLMs for incoming queries in mixed LLM candidates, is needed to balance the trade-offs between response quality, cost, and latency.

% \subsection{task and motivation}
%In real-world scenarios, queries arrive sequentially, requiring real-time decisions to route each query to the most suitable LLM. 
%In real-world scenarios, where queries arrive sequentially, the LLM routing task is to assign each query to the most suitable LLM to maximize response quality while staying within budget and latency constraints.

\begin{figure}
\centering
\includegraphics[width=0.45\textwidth]{fig/questions_new.png}
\caption{Which is the most suitable LLM?}
\label{intro_question}
\end{figure}

% \subsection{existing work and challenge}
Existing LLM routing methods can be categorized into non-predictive methods and predictive methods. Non-predictive methods, like cascading \cite{chen2023frugalgpt,madaan2023automix}, firstly exploit smaller LLMs and then switch to larger LLMs based on a reviewer model, but this increases both cost and latency as multiple LLMs are involved \cite{tay2022ul2}.
Predictive methods predict the performance of candidate LLMs to select the best one for each query.
For example, HybridLLM \cite{ding2024hybrid} exploits a binary classifier to predict the query difficulty for routing, 
RouterBench \cite{hu2024routerbench} predicts response quality directly, and FORC \cite{vsakota2024fly} optimizes quality and cost at the set level.
%However, they rarely consider system latency and struggle with label shifts when the candidate set changes.
%, which leaves low trade-off queries unanswered and may not fully account for real-time query streams}

However, the challenges involved in existing work are multifaceted. 
Firstly, a key limitation of current methods is the lack of consideration 
%for hardware constraints—such as CPU, GPU, and memory—which lead to increased 
of high latency when too many queries are routed to the same LLM. Ignoring latency can create bottlenecks, reduce system efficiency, and impact user experience. 
Secondly, continual learning in a deployed system poses a significant challenge: LLMs must adapt to evolving queries and learn from user feedback over time to maintain relevance and accuracy, necessitating robust mechanisms for incorporating feedback.
% Lastly, updating the candidate LLM set flexibly is essential; as advancements in model architectures and techniques emerge, the routing system must seamlessly integrate new models while phasing out underperforming ones, ensuring that users consistently benefit from the latest innovations in NLP. 
Lastly, flexibly managing the addition and removal of candidate LLMs is essential; as advancements in model architectures and techniques emerge, the routing system needs to integrate new models and retire outdated ones to ensure users benefit from the latest advancements.
%Together, these challenges underscore the need for sophisticated routing strategies that can enhance the effectiveness and efficiency of LLM applications in real-world scenarios.



% Based on the existing methods and urgent needs, the challenges we want to address in this work are:
% \begin{itemize}
%     \item Avoiding the need to try different LLMs during the inference stage.
%     \item Addressing the limitations of common embeddings.
%     \item Adapting to the constant changes in available LLMs.
%     \item Balancing response quality, cost, and time at both the set and query levels.
% \end{itemize}

%Current LLM routing methods face several challenges. 
%First, managing personalized metrics such as response quality, cost, and latency for each query is particularly challenging due to the unique requirements of different query-LLM pairs.
%Second, past query assignments impact the performance of future queries, making it crucial to manage the system's dynamic state effectively. 
%Third, optimizing the trade-offs among these objectives—especially in real-time, poses a significant challenge. These factors must be balanced to ensure that all queries are routed efficiently without causing delays or system overloads.


% \subsection{unique perspective}
% \begin{figure}[htbp]
% \centering
% \includegraphics[width=0.45\textwidth]{fig/unique_perspective_overview.png}
% \caption{Overview of the Unique Perspective \textbf{\color{red} this figure has a big overlap with Figure 3. If you want to keep this figure, summarize and highlight your most exciting insights, not a technical pipeline. Otherwise, delete Fig 2. }}
% \label{intro_overview}
% \end{figure}

To address these challenges, we propose MixLLM, a dynamic contextual-bandit-based routing system for query-LLM assignment.
First, we propose a tag-enhanced embedding model by using tags generated from the InsTag~\cite{lu2023instag} model. These tags help improve the query representations from noises.
Next, we design lightweight prediction models for each LLM to estimate response quality and cost. These LLM-specific predictions do not require system-wide retraining when new LLMs are introduced.
The meta decision-maker then selects the best LLM for each query based on the predictions. It balances trade-offs between response quality, cost, and latency to optimize query-LLM assignments.
Finally, MixLLM benefits from continual training, allowing the system adapts to evolving queries and user feedback over time, improving the performance in real-world deployment.

% % challenge 1 predictive pipeline
% To address the above challenges, we propose a predictive pipeline to route each query to the most suitable LM (see Figure \ref{intro_overview}). Since the routing is completed before the inference stage, there is no need to spend time trying different LLMs during inference.
% % why not reinforcement learning
% Our method utilizes a contextual multi-armed bandit (MAB) approach, where each arm represents an LM. Compared to reinforcement learning methods, the MAB approach focuses on the input query, making the routing process query-specific. Additionally, state transitions in reinforcement learning do not make sense for our problem because there is no connection between consecutive queries.
% % challenge 2 embedding
% In each arm, the arm-specific statement and query are embedded. The embedding model is enhanced with tags from InsTag \cite{lu2023instag}, as the domain of queries correlates with LLM selection. These special LLM statements and tag-enhanced embeddings provide arm-specific and routing-oriented embeddings.
% % challenge 3 arm specific prediction
% Response quality and length are then predicted. The cost is calculated based on token prices and lengths. The meta decision maker selects the best arm. Since the arms are independent, it is easy to add or remove LMs without retraining the remaining arms.
% On behalf of this part, we extend the original RouterBench dataset with the latest Llama 3.1 model \cite{dubey2024llama} and test our algorithm on the new dataset.
% % challenge 4 trade-off
% When choosing the final arm, we assign a time penalty to each arm to help the router balance response quality, cost, and time. This serves as a set-level constraint, while response quality and cost predictions are query-level. By combining these constraints, the router can achieve set-level goals while maintaining query-level routing. No query remains unanswered, ensuring no customer is disappointed.
% % offline training online training

Our extensive experiments demonstrate that MixLLM effectively balances response quality, cost, and latency, achieving 97.25\% of GPT-4's quality at only 24.18\% of the cost. By incorporating a latency penalty, MixLLM avoids congestion and high-latency issues, ensuring efficient system performance even under heavy load. Additionally, we extend the RouterBench dataset by incorporating the latest Llama 3.1 model, showcasing the framework's scalability and adaptability. The results from online training further validate the effectiveness of the continual training approach.


% \subsection{contribution}
Our contributions are as follows:
\begin{itemize}[nosep]
    \item We propose MixLLM that leverages enhanced query embeddings, latency penalties, and continual learning to balance response quality, cost, and latency in LLM routing.
    \item MixLLM accounts for real-world query streams by introducing a latency mechanism that factors in hardware limitations.
    \item MixLLM offers key benefits, including selecting the optimal LLM, handling the latency constraint, and adapting over time to changing environments and user feedback.
    \item We extend the RouterBench dataset by incorporating the latest Llama 3.1 model and adding prompt and response length.
\end{itemize}





