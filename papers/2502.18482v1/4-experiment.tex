\section{Experiments}
% To demonstrate the effectiveness of MixLLM, we conduct a series of experiments. First, we introduce the dataset and experimental setup, followed by presenting the results under different configurations.


\subsection{Experimental Settings}
\subsubsection{Dataset}
We conduct our experiments utilizing the RouterBench dataset \cite{hu2024routerbench}, which consists of 36,497 queries from 8 NLP datasets, including Chinese and English. 
Each query is answered by 11 different LLMs, with records of responses, as well as corresponding quality and cost metrics.
% \lyc{They are not all QA datasets, and why the number of queries differents so much? From RouterBench paper: In total, there are 405,467 samples in ROUTERBENCH, covering 11 models, 8 datasets, and 64 tasks.}
Moreover, we extend the dataset with Llama 3.1 8B and 70B models~\footnote{\url{https://ai.meta.com/blog/meta-llama-3-1/}} and add prompt and response lengths of all the queries and responses. The dataset is split into 80\% training and 20\% testing. 
%For online training, there are also 50:50 and 30:70 ratios. 
%The extended dataset is available at \textcolor{red}{link} \url{http://www.example.com}.


\subsubsection{Baseline Algorithms}
% \lyc{use one sentence to introduce the key idea of each baseline.}
We compare MixLLM with both non-predictive and predictive baselines in our experiments. 
For non-predictive methods, the cascading approach tests smaller models first and switches to larger ones if needed. We extend AutoMix~\cite{madaan2023automix} by ordering multiple LLMs by size, with cheaper models prioritized when sizes are equal.
For predictive methods, 
RouteLLM~\cite{ong2024routellm} assigns queries to LLMs using a BERT-based multi-label classifier, while  Zooter~\cite{lu-etal-2024-routing} is represented by an MLP-based classifier.
RouterBench~\cite{hu2024routerbench} predicts response quality to achieve a quality-cost trade-off.
Both FORC~\cite{vsakota2024fly} and OptLLM~\cite{liu2024optllm} predict quality and then perform set-level optimization,
while MetaLLM~\cite{nguyen2024metallm} uses a bandit algorithm with a quality-cost reward.
For additional comparison, we also include random routing and individual LLMs.
%Several other approaches were excluded as they are not suitable for our task: HybridLLM~\cite{ding2024hybrid} focuses solely on binary routing; ME-Switch~\cite{liu2024me} assumes routing is domain-specific; and Shnitzer's method~\cite{shnitzer2023large} is difficult to reproduce.

Since the baseline algorithms do not include online training after deployment, we only compare them with our offline training component for a fair comparison in Section~\ref{exp_offline}, while the online training component is further evaluated in Section~\ref{exp_online}.


\subsubsection{Evaluation Metrics}
% \lyc{double check the singular and plural of cost(s), constraint(s), use query cost, latency constraint}

We evaluate the methods on the streaming test queries based on the quality-cost trade-off under the latency constraint.
% , using 7,300 test queries with an 80:20 split. 
Specifically, the response quality score for each query is scaled from 0 to 1, while the query cost is measured in dollars. 
Any query that exceeds the maximum tolerable waiting time is assigned a quality score of 0.
The total quality and total cost are calculated as the sum of quality scores and query costs for all the test queries.
We evaluate the routing performance across varying budget levels using parameter $\lambda$, ranging from $10^{-6}$ to $10^{6}$ in Equation~\eqref{reward_score}, with a larger $\lambda$ will prioritize response quality.

\subsubsection{Configurations}
% \textcolor{red}{appendix??????????}

In our experiments, we set up a software environment consisting of Python 3.12, PyTorch 2.0, and CUDA 12.1 running on Ubuntu 18.04 LTS.
% \lyc{Type and number of GPUs used?}
Most experiments were conducted on a 12GB Titan-V GPU, while tasks involving Llama models, such as dataset extension and tag extraction, were performed on two 80GB H100 GPUs.
% The maximum tolerant latency is 14400 seconds. 

All random seeds are set to 42 for reproducibility. In Equation~\eqref{final_scores}, $\alpha$ is set to 0.01, and $\beta$ is set to 0.1. 
In Equation~\eqref{penalty_score}, $\gamma$ is set to 0.1.

As for learning rates, $\eta_1$ and $\eta_2$ are set to 1, reflecting the use of simple machine learning algorithms, while $\eta_3$ is set to 0.001 due to the complexity of the neural network.

Query streams are configured at a rate of 100 queries per 10 seconds. The maximum tolerable waiting time $\tau$ is set to 30 seconds, and the waiting time of LLMs will be updated every 10 seconds.
The prices of input and output, the average initial time, and response speeds of different LLMs are publicly available~\footnote{\url{https://artificialanalysis.ai/}}.
This website estimates the costs of open-source LLMs based on computational resources, including CPU, GPU, and memory usage, while API-based LLMs are priced directly using their API rates.

As for quality and length regressors, we use random forest (RF) for quality prediction across all LLMs, while a combination of multi-layer perceptron (MLP), RF, and K-nearest neighbors (KNN) is applied for length (cost) prediction depending on the LLM.
Those predictors are lightweight. For example, the size of an MLP model is less than 2MB, so the inference and update time is shorter.


%And latency constraint is designed to mimic real-world scenarios: any query that exceeds the maximum tolerable threshold is assigned a quality of 0, though the associated costs remain unchanged. 

\begin{figure}
\centering
\includegraphics[width=0.95\linewidth]{fig/MixLLM_with_baselines_2.png}
\caption{Overall Results
% \lyc{put the legend inside figure, bottom right, to make it bigger, remove random w/o GPT-4, same for the following figures. why we only have 10 individual LLMs here given 11 candidate LLMs?}
}
\label{mixllm_result}
\end{figure} 

\subsection{Overall Results}
\label{exp_offline}
%\underline{\textit{How does MixLLM perform?}}




% \lyc{?AutoMix and FrugalGPT}
% \lyc{you also need to explain which regressor MixLLM used}

As shown in \textbf{Figure \ref{mixllm_result}}, MixLLM consistently outperforms the baselines, delivering strong performance. For the baseline methods, response quality can decline with larger budgets since queries may exceed the latency constraint. Notably, MixLLM achieves 97.25\% of GPT-4’s quality at only 24.18\% of the cost when $\lambda$ is 1.4. In comparison, the best baseline method, OptLLM, reaches 96.39\% of GPT-4’s quality at 32.94\% of the cost. However, beyond this point, OptLLM's response quality drops as many queries exceed the waiting time tolerance, while MixLLM remains stable. The same situation also happens on other baseline algorithms.

% \lyc{you can also list the numbers the best baseline method can achieve.}

The \textit{Oracle} result represents the most optimal routing on this dataset, balancing response quality and cost. It serves as a benchmark for the best possible assignment. In this context, a point closer to the upper left (\textit{Oracle}) signifies higher quality at a lower cost. 
To obtain the \textit{Oracle} result, all candidate LLMs are tested for each query. For each query, the LLM that meets the quality threshold and has the lowest cost is selected. While the final results reflect only the quality and cost of the selected LLM, the process of determining the \textit{Oracle} result requires significant computational resources.


Each single LLM provides one quality-cost point. For instance, GPT-4 demonstrates superior quality, while GPT-3.5 offers a better balance of cost and quality. The ``Random'' routing serves as a baseline; points above and to the left of this anchor are superior in offering better quality at a lower cost.

AutoMix struggles because multiple LLMs handle each query, quickly exhausting the budget and reaching the latency constraint. RouteLLM and Zooter fail to adjust budgets dynamically and can only provide one quality-cost point. RouterBench performs well at lower budgets but faces latency issues as budgets increase. FORC and OptLLM share the problem of ignoring some queries due to set-level optimization, affecting user experience. MetaLLM is less effective because it can't consider multiple LLMs simultaneously, underscoring the need for a multi-armed bandit approach.



% AutoMix performs badly because each query is answered by more than one LLM, which will meet the budget and latency constraints quickly. Moreover, the cost of employing the reviewer LLM is often higher than expected.

% RouteLLM and Zooter produce single points, lacking the ability to adjust budgets. The classifiers suffer from label inconsistency when new LLMs are introduced, necessitating a complete retraining of the system.

% RouterBench achieves a good trade-off at lower budgets. However, as the budget increases, most queries are routed to the most powerful LLM, leading to increased latency.

% FORC and OptLLM exhibit a similar limitation, where some queries are ignored due to set-level budget considerations. Although this issue isn't immediately evident in \textbf{Figure \ref{mixllm_result}}, it significantly affects the user experience.

% MetaLLM demonstrates limited effectiveness. Its inability to consider multiple LLMs simultaneously highlights the importance of using a multi-armed bandit approach.



\subsection{Study on Continual Training}
\label{exp_online}
% \lyc{please update}
%\underline{\textit{Why is continual training important?}}

To enable continual training, we simulate the real-world query streams by splitting the training dataset into different ratios (\textbf{Table~\ref{tab:online_training_comparison}}) for offline and online training.
For example, an 80:20 split means 80\% of the data are used in offline training, while 20\% of the data are used in online training.
The offline training uses refined feedback across these splits. 
%For online training, there are two types of feedback: 1) refined feedback, which includes actual response quality, query cost, and query embedding; and 2) binary feedback, based on human satisfaction with the LLM service.
For online training, 
in addition to the refined feedback, user feedback is simulated by assuming the user is satisfied if the response quality exceeds 0.7 and the waiting time is less than 15 seconds.



\textbf{Table~\ref{tab:online_training_comparison}} presents the overall response quality for each setting, calculated as the sum of the response qualities divided by the total number of queries. 
Higher percentages indicate better performance. %For example, online training with refined feedback achieves 76.45\% quality on the 80:20 split.
To ensure fairness, results within the same split ratio (column) maintain similar costs, which means the improvements reflect the impact of online training feedback.
Results show both types of feedback improve model performance. 
Although the improvement may seem modest, it’s important to note that online feedback is only available for the selected one, which limits the effectiveness compared to offline training. 
Despite this limitation, the results suggest that online training becomes more effective as more data are available. In real-world scenarios, where online training data are abundant, MixLLM will have greater opportunities to adapt.
% \lyc{what do the numbers mean?}
Refined feedback outperforms binary feedback due to its detailed nature. Nevertheless, even the simpler binary feedback contributes to improved performance.


\begin{table}[htbp]
    \centering
    \caption{The Power of Continual Training
    % \lyc{1. it is not Train:Test split, it is all for training. 2. please explain the meaning of numbers}
    }
    \resizebox{0.90\linewidth}{!}{
    \begin{tabular}{lccc}
        \toprule
        \multirow{2}{*}{\textbf{Setting}} & \multicolumn{3}{c}{\textbf{Offline : Online}} \\ 
        \cmidrule{2-4}
         & \textbf{80:20} & \textbf{50:50} & \textbf{30:70} \\ 
        \midrule
        \textbf{Without Online Training} & 75.54\% & 71.98\% & 69.74\% \\ 
        \midrule
        \textbf{With Refined Feedback} & 76.45\% & 72.99\% & 71.29\% \\ 
        \textbf{Improvement} & \textbf{1.21\%} & \textbf{1.39\%} & \textbf{2.22\%} \\ 
        \midrule
        \textbf{With Binary Feedback} & 75.93\% & 72.37\% & 70.65\% \\ 
        \textbf{Improvement} & \textbf{0.52\%} & \textbf{0.53\%} & \textbf{1.31\%} \\ 
        % \textbf{With Both Feedback} & 00.00\% & 00.00\% & 00.00\% \\ 
        \bottomrule
    \end{tabular}
    }
    \label{tab:online_training_comparison}
\end{table}

In our experiments, we implemented one online test at the end of online training to demonstrate the continuing improvement of learning from and aligning with online feedback. Without loss of generality, we believe our one-time finding (online feedback can improve performance and alignment) can be generalized to recurrent tests. It is feasible to adapt our system to conduct recurrent tests at the end of each cycle in a real-world scenario.


\subsection{Study on Tag-Enhanced Embedding}


%Query texts are embedded by a tag-enhanced encoder before being sent to predictors. 
To obtain tags for the tag-enhanced encoder training, we employ InsTag~\cite{lu2023instag}, a Llama-based tagging LLM to generate one or more tags for each training query. InsTag is capable of producing over 6,000 tags, e.g., ``data structure'', ``legal ethics'', which are manually categorized into 20 domains, e.g., ``Computer Science'', ``Legal''. 
%\textbf{Table~\ref{tab:tags_domains}} provides examples of the tags and their corresponding domains.


% \begin{table}[htbp]
% \centering
% \caption{Examples of Domain and Tag}
% \resizebox{0.7\linewidth}{!}{
% \begin{tabular}{cc}
%     \toprule
%     \textbf{Domains} & \textbf{Tags} \\
%     \midrule
%     \multirow{2}{*}{Computer Science} & 'data structure' \\
%                                       & 'machine learning' \\
%     \midrule
%     \multirow{2}{*}{Law}              & 'legal ethics' \\
%                                       & 'jurisdiction' \\
%     \bottomrule
% \end{tabular}
% }
% \label{tab:tags_domains}
% \end{table}



\begin{table}[htbp]
\centering
\caption{Effect of Tag-Enhanced Embedding}
\resizebox{\linewidth}{!}{
\begin{tabular}{lcccc}
\toprule
\textbf{\makecell{Cost \\ Level}} & \textbf{\makecell{Cost \\ Range}} & \textbf{\makecell{General \\ Embedding}} & \textbf{\makecell{Enhanced \\ Embedding}} & \textbf{Improvement}\\ 
\midrule
Low & $<$ 1 & 53.14\% & 56.18\% & \textbf{5.72}\% \\
Middle & 1 - 8 & 72.09\% & 73.43\% & \textbf{1.85}\% \\ 
High & $>$ 8 & 75.76\% & 76.36\% & \textbf{0.79}\% \\ 
\bottomrule
\end{tabular}
}
\label{tab:embedding_compare}
\end{table}


The results in \textbf{Table~\ref{tab:embedding_compare}} demonstrate the effectiveness of tag-enhanced embedding.
The values represent response qualities across different cost levels, where each cost level corresponds to a specific cost range. 
%For instance, MixLLM with enhanced embedding achieves 73.43\% response quality at the middle cost level. 
To ensure fairness, the costs within each level (row) are kept similar.
As the cost level increases, which corresponds to a higher budget and a greater emphasis on response quality, the improvement from tag-enhanced embedding diminishes. 
Nevertheless, at each cost level, tag-enhanced embedding consistently enhances routing performance, highlighting its importance.
% \lyc{more explanation on low, mid, high cost? and what do the numbers mean?}

\subsection{Study on Latency Constraint}
%\underline{\textit{Why is the latency constraint important?}}

Theoretically, the trade-off between response quality and query cost often operates within the bounds of limited hardware resources in the real world. Effectively managing the workload on devices becomes essential. Different components, such as the CPU, GPU, memory, and bandwidth, all have their performance metrics, but these factors converge on one critical metric: query waiting time. Therefore, we employ the latency as the primary constraint.

% \lyc{Please elaborate more on how you simulate with what kind of publicly available data?} 

We conducted a simulation to account for the latency constraint.
The total time required to answer a query has two parts: 1) the initial time to begin generating and 2) the response time, which depends on the answer length.
We use the average initial time for each LLM and estimate the response time by multiplying the output length by the corresponding LLM’s generation speed.
For closed-source LLMs, the simulation is based on API statistics. For open-source LLMs, we simulated under ideal hardware conditions, assuming sufficient memory and stable network connections to ensure optimal performance.
The average initial time and response speed of different LLMs are publicly available~\footnote{\url{https://artificialanalysis.ai/}}. 



\begin{figure}[htbp]
\centering
\includegraphics[width=0.95\linewidth]{fig/MixLLM_without_time_penalty_2.png}
\caption{Results without Latency Constraint}
\label{mixllm_no_latency}
\end{figure}

Even without the latency constraint, MixLLM still outperforms the baselines, as shown in \textbf{Figure~\ref{mixllm_no_latency}}. 
When compared to results with latency constraint (\textbf{Figure~\ref{mixllm_result}}), MixLLM maintains stable performance due to the time penalty component. 
However, the baselines show more variation.

In \textbf{Figure~\ref{mixllm_result}}, AutoMix's performance drops the most, primarily due to its cascading nature. Each query starts with the first LLM, resulting in significantly increased waiting time. 
Other predictive baselines also experience performance declines at higher cost levels, as they tend to route queries to more powerful LLMs with longer waiting times. This results in many queries exceeding the maximum tolerable waiting time and going unanswered.


% To prove this, \textbf{Figure~\ref{fig:latency_constraint}} shows the LLM assignments across different cost levels. 
% Either in low- or high-cost levels, the model without latency penalties (right side) tends to route most queries to a single LLM, either for its cost-effectiveness or superior response quality. However, this concentration can lead to bottlenecks, resulting in longer waiting times and negatively impacting user experience. In contrast, the model with latency penalties (left side) distributes the queries more evenly, preventing any single LLM from being overloaded due to excessive queuing.


%This design allows MixLLM to maintain consistent response quality even in high-budget conditions (\textbf{Figure~\ref{mixllm_result}}).


% \begin{figure}[htbp]
% \centering

% % Subfigure a (top)
% \begin{subfigure}{0.9\linewidth}
% \centering
% \includegraphics[width=\linewidth]{fig/without_time_penalty.png}
% \caption{Without Latency Penalty}
% \label{fig:time_a}
% \end{subfigure}

% % \vspace{1em} % Add some vertical space between the figures

% % Subfigure b (bottom)
% \begin{subfigure}{0.9\linewidth}
% \centering
% \includegraphics[width=\linewidth]{fig/with_time_penalty.png}
% \caption{With Latency Penalty}
% \label{fig:time_b}
% \end{subfigure}



% \caption{Different Assignment Distribution}
% %\lyc{keep the middle ones will be more clear to deliver the message, say each keep 10 bars}
% \label{fig:latency_constraint}
% \end{figure}


% \begin{figure}[htbp]
% \centering
% \includegraphics[width=0.9\linewidth]{fig/stacked_bar_chart.png}
% \caption{Assignment Distribution. Each color represents a different LLM, and each horizontal bar corresponds to a specific cost level. The right side illustrates the distribution of LLM selections without considering the latency penalty, while the left side shows the selections when latency penalties are applied. \wxy{new}}
% \label{fig:latency_constraint}
% \end{figure}



\subsection{Study on Adaptive Training}
%\underline{\textit{What if there are new LLMs?}}

\begin{figure}[htbp]
\centering
\includegraphics[width=0.95\linewidth]{fig/MixLLM_extend_llama_3_2.png}
\caption{Results with Updated Candidates}
\label{mixllm_extend}
\end{figure}

Each LLM in MixLLM operates independently, ensuring scalability. Adding or removing candidate LLMs does not require complete re-training, which only affects the corresponding LLM. To demonstrate this advantage, we extended the RouterBench dataset using new Llama 3.1 models. Specifically, we utilized the Llama 3.1 8B and 70B models to answer each query in the dataset. Then we record responses and measure their quality, cost, and length. 
%We added two arms in MixLLM to adapt to these two new models. 
As shown in \textbf{Figure~\ref{mixllm_extend}},
with the introduction of the powerful Llama 3.1 models, MixLLM achieves 98.55\% of GPT-4's response quality while reducing the cost to just 16.79\% when $\lambda$ is 1.8. Furthermore, MixLLM continues to outperform other baselines. 


% \vspace{-0.3cm}

%In addition to its strong performance, MixLLM is highly efficient, as the parameters in the original arms remain unchanged, eliminating the need for re-training. In contrast, most baselines require a complete re-training, as their original settings are not adaptable to the changing scenario. This highlights MixLLM’s scalability and adaptability in dynamic environments where LLM candidates may be frequently added or removed because of the rapid development of LLMs.


\subsection{Study on Out-of-Domain Generalization}
\label{sec:ood_study}
Real-world queries often originate from new or unseen domains, presenting challenges for LLM routing systems. To evaluate the domain adaptation and generalization capabilities of MixLLM, we conducted an out-of-domain (OOD) experiment. In this setup, we simulate an OOD scenario using the domains defined by tags. 
We maintain an 80:20 splitting ratio, where the testing set (20\% of the data) contains non-overlapping domains not present in the training set (80\% of the data). This design ensures that some testing samples belong to entirely unseen domains during training.

\begin{table}[htbp]
\centering
\caption{Result on OOD Scenario}
\resizebox{\linewidth}{!}{
\begin{tabular}{lcc}
\toprule
\textbf{Splitting Policy} & \textbf{Offline Only} & \textbf{Offline $+$ Online} \\ 
\midrule
Normal 80:20 Splitting & 75.54\% & 76.45\% \\
OOD 80:20 Splitting & 71.43\% & 73.89\% \\ 
Decrease & 5.44\% & \textbf{3.35}\% \\ 
\bottomrule
\end{tabular}
}
\label{tab:ood_compare}
\end{table}

The results in \textbf{Table~\ref{tab:ood_compare}} reveal that when using only offline training, MixLLM's performance decreases by 5.44\% at the same price cost level. However, when integrating both offline and online training, the performance drop is mitigated to 3.35\%. This demonstrates that the integrated offline-online training strategy effectively enhances domain generalization and adaptation. 
Furthermore, we identify MixLLM's OOD problem as a novel routing task, calling on the research community to explore and incorporate advanced domain adaptation techniques into frameworks like ours to better address this pressing challenge.






\subsection{Study on Different Choice Policy}
\label{policy_study}
During our experiments, a new question arises: \ul{\textit{Can selecting more LLMs improve performance?}} To explore this, we applied various selection policies, with the results presented in \textbf{Figure~\ref{policy_result}}. 

\begin{figure}[htbp]
\centering
\includegraphics[width=0.95\linewidth]{fig/choice_policy_new.png}
\caption{Results on Choice Policy Study}
\label{policy_result}
\end{figure}

``Top 1'', ``Top 2'', and ``Top 3'' refer to policies where the LLM(s) with the highest 1, 2, or 3 scores are selected. When multiple LLMs are chosen, the response quality reflects the best one, while costs are summed. The ``TOP 1.5'' policy introduces a dynamic adjustment, which selects the top 1 LLM when the budget is low and expands to include more LLMs as the budget increases.
As illustrated in \textbf{Figure~\ref{policy_result}}, increasing the number of selected LLMs shifts the curve upwards and to the right. This outcome is expected because selecting more LLMs increases both cost and the likelihood of choosing the most capable model. Notably, with the same budget (\textcolor{red}{red} line in \textbf{Figure~\ref{policy_result}}), the ``Top 3'' policy achieves the highest response quality, even surpassing the most powerful single LLM, GPT-4, at only 20\% of its cost.

However, in practical scenarios, users typically seek a single, definitive answer rather than multiple options. \ul{\textit{How to select the final answer?}} 
Adding a reviewer to choose the best answer is one potential solution, but it requires additional time and resources. Given the complexity, we did not incorporate a multi-choice selection into MixLLM. It presents interesting engineering challenges, and we welcome further exploration and collaboration for those interested in addressing this problem.



