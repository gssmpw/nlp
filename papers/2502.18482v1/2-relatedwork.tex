\section{Related Work}
%The problem of selecting the most suitable LLM has gained significant attentions. Existing methods can be broadly categorized into \emph{non-predictive} and \emph{predictive} routing systems.
% The studies of selecting the most suitable LLM can be categorized into \emph{non-predictive} and \emph{predictive} routing systems.
Studies on selecting the most suitable LLM include \emph{non-predictive} and \emph{predictive} routing systems.

\subsection{Non-predictive Routing System}
Non-predictive systems incorporate LLM inference during routing. A common approach is \emph{cascading}, where smaller models are used first, switching to larger ones if needed.
FrugalGPT \cite{chen2023frugalgpt} introduced three strategies to reduce cost while maintaining response quality: prompt adaptation, LLM approximation, and LLM cascade form a chain of LLMs, selecting LLMs from small to large.  
AutoMix \cite{madaan2023automix} introduced a similar cascading strategy, where a self-reviewer judges the answer and a meta-reviewer decides whether switching to a larger model is needed.
However, in non-predictive routing systems, one query may need to be answered by several LLMs which increases both cost and resource usage.

\begin{figure*}[htbp]
\centering
\includegraphics[width=0.95\linewidth]{fig/Overall_Framework.png}
\caption{Overview of the MixLLM Framework}
\label{method_overview}
\end{figure*}

\subsection{Predictive Routing System}
Predictive systems estimate the quality of LLM response before making routing decisions and route each query to only one LLM. These systems typically fall into categories such as classification, quality prediction, optimization, and bandit-based solutions, each offering unique strategies. Given the capability of LLMs to handle tasks across diverse domains, including medical applications~\cite{liu2024calorie,liu2025calorie}, bioinformatics~\cite{ying2024revolutionizing,liu2024pth}, material science~\cite{hu2024reinforcement}, industrial engineering~\cite{xie2025transformer,xie2024spatio}, etc, these routing systems are crucial for optimization in real-world scenarios.
\textit{Classification-based} approaches predict the best LLM for a query by treating LLMs as labels.
HybridLLM \cite{ding2024hybrid} trained a binary classifier to assign ''easy'' queries to smaller models. ME-Switch \cite{liu2024me} extended to a multi-label domain classifier, improving memory and computation efficiency. Other methods like Zooter \cite{lu-etal-2024-routing} introduced a reward model for ranking responses from different LLMs, using tag-based label enhancement for training data. RouteLLM \cite{ong2024routellm} introduced four distinct routing strategies, including similarity-weighted ranking, matrix factorization, and supervised and prompting classification. However, query labels may shift when new and powerful LLMs emerge.
\textit{Response quality prediction} methods focus on predicting the quality of each LLM's response for a given query.
Shnitzer et al.~\cite{shnitzer2023large} used 3 different ways to predict ``correctness'' (response quality) for each LLM and select the best one. RouterBench \cite{hu2024routerbench} optimized the quality-cost trade-off by a ``willingness to pay'' parameter. It also introduced a large benchmark dataset for routing tasks. However, they did not predict cost and ignored latency. 
\textit{Optimization-based} methods treat LLM routing as a set-level optimization problem. 
FORC \cite{vsakota2024fly} employed predicted response quality and cost for quality-oriented and cost-oriented linear programming strategies.
OptLLM \cite{liu2024optllm} optimized the routing problem with a multi-label classification model.
But these approaches potentially ignore low cost-effective queries.
\textit{Bandit-based} methods like MetaLLM \cite{nguyen2024metallm} adopted a single bandit approach, where the system learns to balance quality and cost trade-offs over time. However, the dependency between arms can limit scalability when adding or removing LLMs.

 

