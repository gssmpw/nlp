\section{Methodology}
\begin{figure}
    \centering
    \includegraphics[width=\linewidth]{figures/design_v3.png}
    \caption{Overview of KRxELM. Stage 1 samples a disease-drug set which consists of a disease and two drug candidates from the open-sourced rich DRKG; Stage 2 leverages a Language model to embed the drug-disease pair from the sampled disease-drug set, the embeddings are further used for obtaining background information from Clinical Trials and PubMed Central corpora; Stage 3 fits the disease-drug set and the retrieved background information into an instructional prompt template as input to a large language model (student) and a teacher model. Instruction tuning was used to enable the LLM to learn from the teacher model.}
    \label{fig:methodology}
\end{figure}

\textbf{Task Formation} In this study, we address the problem of drug discovery by leveraging large language models (LLMs) to select optimal drug candidates and provide explainable reasoning. Given a disease-drug set $S = (d, c_1, c_2)$, where $d$ represents the disease and $c_1, c_2$ are the two drug candidates, the task is to train an LLM $f_\theta$ to achieve two objectives: (1) to select the most suitable drug candidate $c^* \in \{c_1, c_2\}$ for the disease $d$, and (2) to generate an explainable rationale $r$ for the selection.\\ 

\noindent \textbf{\method{}}. We introduce a new instruction-tuned LLM which distills knowledge from rich medical knowledge corpus for drug recommendation and rationale generation. \method{} consists of 3 stages shown in Fig.~\ref{fig:methodology}, which we describe next.
% To provide the model with relevant context, we retrieve background information $I = \{i_1, i_2, \dots, i_n\}$ from Clinical Trials and PubMed Central, where $I$ is conditioned on the disease-drug pair $(d, c_i)$ for $i \in \{1, 2\}$. This information serves as an instructional corpus for fine-tuning the LLM, ensuring that the model incorporates domain-specific knowledge. 

% The learning process is guided by a teacher model $f_T$, which provides supervisory signals for both candidate selection and rationale generation. The optimization objective is to minimize the following loss function:
% \begin{equation}
% \mathcal{L}(\theta) = \mathcal{L}_{\text{select}}(c^*, f_T) + \mathcal{L}_{\text{rationale}}(r, f_T),
% \end{equation}
% where $\mathcal{L}_{\text{select}}$ quantifies the alignment of $f_\theta$'s candidate selection with the teacher model's predictions, and $\mathcal{L}_{\text{rationale}}$ measures the quality of the generated rationale compared to teacher-provided explanations.

% Through this approach, we aim to create a robust and interpretable model for aiding drug discovery by synthesizing insights from clinical and biomedical literature.

\subsection{Stage 1: Sampling from DRKG}
Our disease-drug set $S$ is sampled from the open-sourced rich knowledge graph DRKG\footnote{https://github.com/gnn4dr/DRKG} (Drug Repurposing Knowledge Graph) which is a comprehensive biological knowledge graph relating genes, compounds, diseases, biological processes, side effects and symptoms. The sampling process involves selecting one relevant drug candidate $c_{\text{rel}}$ and one irrelevant drug candidate $c_{\text{irr}}$ for a given disease $d$. To ensure the selection is challenging, the irrelevant candidate $c_{\text{irr}}$ is chosen such that it is similar to $c_{\text{rel}}$ based on their embeddings, which are learned using a graph neural network (GNN)-based model. Let $\mathbf{h}_c$ denote the embedding of a drug candidate $c$ learned by the GNN. The similarity between two drug candidates is computed as:
\begin{equation}
\text{sim}(c_1, c_2) = \frac{\mathbf{h}_{c_1} \cdot \mathbf{h}_{c_2}}{\|\mathbf{h}_{c_1}\| \|\mathbf{h}_{c_2}\|},
\end{equation}
where $\cdot$ denotes the dot product and $\|\cdot\|$ represents the Euclidean norm.

For a given disease $d$, the GNN-based model identifies a set of candidate drugs $C_d$ from DRKG. The relevant candidate $c_{\text{rel}}$ is selected as:
\begin{equation}
c_{\text{rel}} = \arg\max_{c \in C_d} \text{rel}(d, c),
\end{equation}
where $\text{rel}(d, c)$ is a relevance score derived from DRKG edges connecting $d$ and $c$. The irrelevant candidate $c_{\text{irr}}$ is then sampled as:
\begin{equation}
c_{\text{irr}} = \arg\max_{c \in C_d \setminus \{c_{\text{rel}}\}} \text{sim}(c, c_{\text{rel}}),
\end{equation}
ensuring that $c_{\text{irr}}$ is similar to $c_{\text{rel}}$ but not relevant to $d$.

\subsection{Stage 2: Background Retrieval}
The sampled disease-drug set $S$, consisting of pairs $(d, c_i)$ for $i \in \{1, 2\}$, is then used for retrieving background information from two rich corpora: Clinical Trials\footnote{https://clinicaltrials.gov/} and PubMed Central\footnote{https://ftp.ncbi.nlm.nih.gov/pub/pmc/oa\_bulk/oa\_comm/xml/}. A language model $g_\phi$ is employed as an embedding projection mechanism to facilitate the retrieval process. For each pair $(d, c_i)$, the background information $I_i$ is retrieved by computing the relevance between the pair's embedding $\mathbf{e}(d, c_i)$ and the embeddings of documents in the corpora. Let $\mathbf{e}_j$ denote the embedding of the $j$-th document in the corpus. The relevance score is computed as:
\begin{equation}
\text{rel}(\mathbf{e}(d, c_i), \mathbf{e}_j) = \frac{\mathbf{e}(d, c_i) \cdot \mathbf{e}_j}{\|\mathbf{e}(d, c_i)\| \|\mathbf{e}_j\|}.
\end{equation}
The top-$k$ documents with the highest relevance scores are selected as the background information $I_i$. This retrieved information serves as an instructional corpus for fine-tuning the LLM, ensuring that the model incorporates domain-specific knowledge. 

The learning process is guided by a teacher model $f_T$, which provides supervisory signals for both candidate selection and rationale generation. The optimization objective is to minimize the following loss function:
\begin{equation}
\mathcal{L}(\theta) = \mathcal{L}_{\text{select}}(c^*, f_T) + \mathcal{L}_{\text{rationale}}(r, f_T),
\end{equation}
where $\mathcal{L}_{\text{select}}$ quantifies the alignment of $f_\theta$'s candidate selection with the teacher model's predictions, and $\mathcal{L}_{\text{rationale}}$ measures the quality of the generated rationale compared to teacher-provided explanations.

Through this approach, we aim to create a robust and interpretable model for aiding drug discovery by synthesizing insights from clinical and biomedical literature.

\subsection{Stage 3: Knowledge Distillation}
The sampled disease-drug set $S$ and the retrieved background information $\{I_1, I_2\}$ are incorporated into an instructional template to construct input sequences for training the LLM. Each input consists of the disease $d$, the drug candidates $c_1, c_2$, and the corresponding background information $I_1, I_2$. The output of the LLM $f_\theta$ includes the selected drug candidate $c^*$ and a generated rationale $r$ explaining the decision.

A powerful teacher model $f_T$ is used to supervise the training process. The LLM is enforced to learn from the teacher model using instruction fine-tuning. The training objective is designed to minimize two types of loss:
\begin{itemize}
    \item \textbf{Reason generation loss:} This loss, denoted as $\mathcal{L}_{\text{rationale}}$, measures the discrepancy between the rationale $r$ generated by the LLM and the rationale $r_T$ provided by the teacher model:
    \begin{equation}
    \mathcal{L}_{\text{rationale}} = \|r - r_T\|^2.
    \end{equation}
    \item \textbf{Drug selection loss:} This loss, denoted as $\mathcal{L}_{\text{select}}$, captures the difference between the drug selection $c^*$ made by the LLM and the ground truth drug candidate $c_{\text{rel}}$ derived from the sampling process:
    \begin{equation}
    \mathcal{L}_{\text{select}} = \mathds{1}[c^* \neq c_{\text{rel}}],
    \end{equation}
    where $\mathds{1}[\cdot]$ is an indicator function that evaluates to 1 if the condition is true and 0 otherwise.
\end{itemize}
The total loss for training the LLM is given by:
\begin{equation}
\mathcal{L}(\theta) = \mathcal{L}_{\text{select}} + \lambda \mathcal{L}_{\text{rationale}}\,,
\end{equation}
where $\lambda$ is a weighting factor to balance the two losses.



