\section{Background}

\textbf{Large Language Models and Retrieval-Augmented Generation.}
Large Language Models (LLMs) are deep neural networks trained on massive corpora of text data to generate and understand human-like language. By learning statistical patterns in text, LLMs, such as GPT and BioGPT, are capable of tasks like text generation, summarization, and reasoning. Given an input sequence $q$, an LLM generates a probability distribution over possible outputs $y$, modeled as:
\[
p(y|q) = \prod_{t=1}^T p(y_t|q, y_{<t}),
\]
where $T$ is the output sequence length, and $y_{<t}$ represents the tokens generated up to time $t$.

However, standard LLMs are limited by the knowledge encoded during training and may struggle with domain-specific tasks such as biomedical reasoning. Retrieval-Augmented Generation (RAG) addresses this limitation by combining retrieval-based and generation-based methods, enabling LLMs to incorporate external knowledge in real-time. The workflow of RAG can be broken into two stages:
\begin{enumerate}
    \item \textbf{Retrieval}: For a given query $q$, relevant documents are retrieved from an external corpus $\mathcal{D}$. Each document $d_i \in \mathcal{D}$ is scored based on the similarity between the query embedding $\mathbf{e}(q)$ and document embedding $\mathbf{e}(d_i)$. Cosine similarity is used for scoring:
    \[
    \text{sim}(q, d_i) = \frac{\mathbf{e}(q) \cdot \mathbf{e}(d_i)}{\|\mathbf{e}(q)\| \|\mathbf{e}(d_i)\|}.
    \]
    The top-$k$ documents with the highest scores are selected as the retrieval set $\mathcal{R}$.
    
    \item \textbf{Generation}: The query $q$ and the retrieved context $\mathcal{R}$ are concatenated and fed into the LLM. The LLM generates an output $y$ conditioned on both the query and the retrieved documents:
    \[
    p(y|q, \mathcal{R}) = \prod_{t=1}^T p(y_t|q, \mathcal{R}, y_{<t}).
    \]
\end{enumerate}

This approach improves the factual accuracy and interpretability of LLM outputs by grounding predictions in external knowledge. By integrating the retrieval capability with the language generation power of LLMs, RAG enhances reasoning and ensures that predictions are backed by relevant evidence.\\

\noindent \textbf{Knowledge Distillation.}
Knowledge Distillation is a training paradigm where a smaller, student model $f_\theta$ learns from a larger, more powerful teacher model $f_T$. The objective is to transfer the teacher model's knowledge while optimizing the student model's efficiency and performance.  

In this study, the teacher model supervises two tasks: drug selection and rationale generation. The loss function for knowledge distillation consists of:
\begin{enumerate}
    \item \textbf{Selection Loss}: The student model predicts a probability $p(c^* | d, c_1, c_2)$ for selecting the correct drug candidate $c^*$ from a given set $(d, c_1, c_2)$, matching the teacher's output. This loss is defined as:
    \[
    \mathcal{L}_{\text{select}} = -\log p(c^* | d, c_1, c_2).
    \]

    \item \textbf{Rationale Generation Loss}: The student generates a rationale $r$ explaining its selection, aligning with the teacher-provided rationale $r_T$. The loss is defined as:
    \[
    \mathcal{L}_{\text{rationale}} = \|r - r_T\|^2,
    \]
    where $r$ and $r_T$ are embeddings of the student and teacher rationales, respectively.
\end{enumerate}

The overall training objective for the student model is:
\[
\mathcal{L}(\theta) = \mathcal{L}_{\text{select}} + \lambda \mathcal{L}_{\text{rationale}},
\]
where $\lambda$ is a hyperparameter balancing the importance of the two components.

Through knowledge distillation, the student model learns to replicate the teacher's performance in drug selection and rationale generation while improving computational efficiency. Paired with RAG, this allows the system to provide accurate and interpretable drug recommendations grounded in biomedical knowledge.