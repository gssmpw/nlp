\section{Experiments}
\subsection{Dataset \& Experimental Setup}
\textbf{expRxRec}. To predict the effects of drug compounds for specific diseases, we design a data pipeline that integrates structured knowledge from an existing biomedical knowledge graph and unstructured information from medical corpus.\par
\textbf{Disease-Drug Compound Pairs Sampling from DRKG}. For each disease, we select two types of drug compounds: one with a positive effect and another with a negative effect or a comparatively less positive effect based on the connectivity within the graph. This method enables the model to distinguish the drug candidates in terms of a given disease. \par
\textbf{Enriching the Dataset with Background Retrieval} To enhance the information of disease-drug relationships, we extract relevant information from PubMed Central and Clinical Trials using RAG from the open-sourced raw data. A cleaning process is further applied to ensure data relevance and quality:
\begin{enumerate}
    \item Articles with empty titles or abstracts are removed.
    \item Articles are retained only if their content contains at least one sampled drug name.
\end{enumerate}
Following this process, 1,905,387 articles are retained. We then use Apache Lucene\footnote{https://lucene.apache.org/}, a high-performance text search library, to index the core content of the articles. To ensure that queries could focus on the most relevant biomedical information, a two-step retrieval process is conducted:
\begin{enumerate}
    \item For each disease-drug compound pair, we construct a query comprising the disease name and the compound name. The Lucene \texttt{TopDocs} is utilized to retrieve the top-\(k\) most relevant text chunks, where \(k\) is set to 80 in this study.
    \item To further refine the retrieved information, the top-\(k\) text chunks are embedded using OpenAIâ€™s vector embedding models, which capture semantic representations of the text. A similarity filter is applied to retain chunks that are most relevant to the disease-drug compound pair.
\end{enumerate}
The final dataset thus contains: a disease name, a drug compound name, the relationship label (positive or negative effect), and a curated set of PubMed and Clinical Trials chunks, providing context and supporting evidence. This enriched dataset forms the input for training a robust large language model (LLM) capable of reasoning over biomedical data, enabling it to predict the effects of drug compounds for given diseases.\\
\textbf{MIMIC-III} is a publicly available resource of de-identified health data from over 58,000 critical care admissions at Beth Israel Deaconess Medical Center~\citep{johnson2016mimic}. It combines structured data (e.g., lab results, prescriptions) and unstructured clinical notes, offering a rich source for research in drug discovery and patient-specific treatment analysis while maintaining HIPAA compliance.

\subsection{Results on Drug Selection}
Table 1 presents the F1 scores for various models on the drug selection task, evaluated across two datasets: expRxRec and MIMIC-III. Each model's performance is assessed under four configurations: vanilla (baseline without background information), with Clinical Trials (CT), with PubMed Central (PMC), and with both CT and PMC.

Across all models, incorporating background information (CT, PMC, or both) consistently improves performance over the vanilla version. For example, GNN achieves an F1 score improvement from 76.66 to 84.91 on expRxRec and from 57.05 to 64.01 on MIMIC-III when enriched with both CT and PMC. Similarly, \method{} demonstrates the best overall performance, achieving the highest F1 scores of 88.05 (expRxRec) and 69.19 (MIMIC-III) with both CT and PMC.

Notably, while the inclusion of either CT or PMC individually enhances the results, combining both sources yields the largest improvements. This trend highlights the complementary nature of the two sources in enriching the input data for better drug selection predictions. Furthermore, the performance gap between expRxRec and MIMIC-III suggests that the structured information and scale of expRxRec may facilitate better model generalization compared to the more diverse and sparse data in MIMIC-III.

In summary, these results emphasize the importance of leveraging external background knowledge for improving drug selection accuracy. Among the tested models, \method{} exhibits superior performance, showcasing its efficacy in integrating background information for this task.
\begin{table}[ht]
    \small
    \centering
    \begin{tabular}{cccc}
        \hline
         Model & Type & expRxRec & MIMIC-III \\
          \hline
         \multirow{4}{*}{GNN\citep{gaudelet2021utilizing}} & \textit{vanilla} & 76.66 & 57.05  \\
         & \textit{w CT} & 81.37 & 61.95  \\
         & \textit{w PMC} & 81.94 & 61.55  \\
         & \textit{w both} & 84.91 & 64.01  \\
         \hline
         \multirow{4}{*}{SafeDrug\citep{yang2021safedrug}} & \textit{vanilla} & 76.21 & 64.85 \\
         & \textit{w CT} & 82.09 & 66.91 \\
         & \textit{w PMC} & 81.20 & 66.81 \\
         & \textit{w both} & 83.20 & 67.77 \\
         \hline
         \multirow{4}{*}{4SDrug\citep{tan20224sdrug}} & \textit{vanilla} & 76.71 & 61.96 \\
         & \textit{w CT} & 82.12 & 64.01 \\
         & \textit{w PMC} & 82.27 & 64.13 \\
         & \textit{w both} & 83.71 & 66.29 \\
         \hline
         \multirow{4}{*}{\method{}} & \textit{vanilla} & 80.11 & 65.10 \\
         & \textit{w CT} & 87.44 & 68.43 \\
         & \textit{w PMC} & 87.61 & 68.71 \\
         & \textit{w both} & \textbf{88.05} & \textbf{69.19} \\
         \hline
    \end{tabular}
    \caption{The results on drug selection with F1 score equipped with different retrieved background information.}
    \vspace{-10pt}
    \label{tab:drug_selection}
\end{table}

\subsection{Results on Reason Generation}
\begin{table}[ht]
    \small
    \centering
    \begin{tabular}{ccccc}
        \hline
         Model & Type & ROUGE-1 & ROUGE-2 & ROUGE-L \\
          \hline
         \multirow{4}{*}{Pointer-Generator\citep{see2017get}} & \textit{vanilla} & 15.11 & 15.61 & 12.29 \\
         & \textit{w CT} & 17.03 & 17.61 & 15.29 \\
         & \textit{w PMC} & 17.91 & 17.94 & 15.02 \\
         & \textit{w both} & 18.33 & 18.67 & 16.91 \\
         \hline
         \multirow{4}{*}{BioGPT\citep{luo2022biogpt}} & \textit{vanilla} & 22.19 & 22.42 & 20.16 \\
         & \textit{w CT} & 26.79 & 26.99 & 25.01 \\
         & \textit{w PMC} & 25.87 & 25.89 & 23.97 \\
         & \textit{w both} & 27.19 & 27.58 & 25.81 \\
         \hline
         \multirow{4}{*}{\method{}} & \textit{vanilla} & 28.90 & 28.14 & 26.43 \\
         & \textit{w CT} & 33.03 & 33.78 & 31.74 \\
         & \textit{w PMC} & 33.89 & 33.95 & 31.90 \\
         & \textit{w both} & \textbf{35.11} & \textbf{35.17} & \textbf{34.03} \\
         \hline
    \end{tabular}
    \caption{The results on recommendation reason generation with ROUGE scores equipped on expRxRec.}
    \vspace{-8pt}
    \label{tab:reason_generation}
\end{table}

Table 2 presents the ROUGE-1, ROUGE-2, and ROUGE-L scores for recommendation reason generation using Pointer-Generator, BioGPT, and \method{} under the same four configurations as in the drug selection task.

The inclusion of background knowledge, whether from Clinical Trials or PubMed Central, consistently improves the performance of all models compared to the vanilla baseline. The Pointer-Generator model, which serves as a sequence-to-sequence baseline, shows modest gains with background enrichment, improving its ROUGE-1 score from 15.11 (vanilla) to 18.33 (w both). However, its overall performance remains limited due to its lack of domain-specific pretraining. BioGPT, a biomedical pre-trained language model, performs better than Pointer-Generator across all configurations. In the vanilla setting, it achieves a ROUGE-1 score of 22.19, which is significantly higher than Pointer-Generator, indicating the benefit of pretraining on biomedical text. However, its improvement with background enrichment is less pronounced, with a ROUGE-1 score of 27.19 when both CT and PMC are used. This suggests that BioGPT leverages its internal knowledge more effectively but does not fully exploit external background information. \method{} achieves the best performance across all metrics and configurations, demonstrating its superior ability to generate coherent and contextually rich explanations. In the vanilla setting, \method{} achieves a ROUGE-1 score of 28.90, outperforming both Pointer-Generator and BioGPT by a significant margin. When enriched with both CT and PMC, \method{} reaches the highest scores: 35.11 (ROUGE-1), 35.17 (ROUGE-2), and 34.03 (ROUGE-L). These results underscore the model's capability to effectively integrate and distill information from diverse knowledge sources, enabling it to generate high-quality, explainable recommendations.

The combination of Clinical Trials and PubMed Central provides the most comprehensive improvement, highlighting the complementary nature of these knowledge sources. Clinical Trials offer structured and detailed information on drug efficacy, while PubMed Central provides broader contextual insights. For example, \method{}'s performance improves significantly from 33.03 (w CT) and 33.89 (w PMC) to 35.11 (w both), demonstrating the benefit of combining these sources.

Overall, these findings suggest that integrating external knowledge, particularly from multiple complementary sources, is crucial for enhancing the explainability and quality of recommendation generation. Furthermore, \method{}'s superior performance highlights the importance of designing models specifically tailored for biomedical reasoning tasks, enabling them to effectively synthesize and reason over complex biomedical information.