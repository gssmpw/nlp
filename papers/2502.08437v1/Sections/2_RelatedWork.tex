\section{Related work}
\label{section:related}
Our work explores the visualization and interaction with location based data for in-car AR. As such, our related work consists of the topics of augmented passenger experiences, in-car AR interaction, and visualization of location-based data in vehicles.


%======================== [ AR Passengers ] ========================%
\subsection{Augmented Passenger Experiences}
Existing work highlights the potential of AR to enhance passenger experiences, particularly for infotainment and interaction with the external environment. However, significant challenges in this field remain.

There has been extensive research investigating the use of AR to facilitate NDRTs during transit. While NDRTs have been studied extensively, most research has focused on drivers and future scenarios involving automated vehicles \cite{Pfleging16NDRNeeds}. Pfleging et al. \cite{Pfleging16NDRNeeds} explored NDRT related activities drivers wish to engage in during highly or fully automated vehicle operation,  revealing a strong interest in daydreaming, texting, browsing the Internet, using mobile multimedia applications, and watching the environment. Regarding passenger experiences, Berger et al. \cite{BergerGridStudyInCarPassenger2021} identified well-being, physical comfort, and safety as critical influencing factors. Their study also highlighted that passengers value access to in-vehicle systems, the ability to act as a co-driver, and the integration of external technology for connectivity and personalization. Additionally, the importance of the outside environment was emphasized, with participants preferring routes with scenic views to enhance their ride experience and maintain situational awareness.

Some works have evaluated to include AR to improve passenger experiences. Togwell et al. \cite{Togwell2022gaming} explored the potential of integrating AR gaming with the sensing capabilities of autonomous vehicles, enabling games that incorporate real-world elements, such as other cars, into the gameplay. Initial findings suggest that in-car AR gaming enhances passenger experience and immersion by using the vehicle's environment creatively, offering new opportunities for game design and future research in AR-driven passenger entertainment. Von Sawitzky et al. \cite{Sawitzky23ArPlacement} investigated how passengers in fully automated vehicles might position infotainment content using an AR interface, rather than relying on traditional windshield displays. Findings from a Virtual Reality (VR) user study indicate that passengers tend to place content in non-obstructive areas, such as the dashboard. This reflects a desire to remain informed about the driving process.

However, challenges emerge when employing immersive technologies in transit. McGill et al. \cite{mcgill2020challenges} investigated the use of AR and VR HMDs in such contexts, highlighting their potential to enhance passenger productivity, privacy, and immersion. Their findings reveal significant barriers, including motion sickness, crash safety concerns, and social acceptability. Similarly, Togwell et al. \cite{Togwell2022gaming} also highlight challenges for in-car AR. These include the changing external environment, motion sickness, alignment accuracy, and pedestrian privacy concerns. These barriers must be addressed to fully realize the benefits of AR headsets in transit environments, underscoring the need for further research in this area.

In summary, while NDRTs for passengers have been widely studied and in-car AR has been explored in certain areas, significant challenges and gaps remain. Our research addresses these by examining passengers' habits and preferences for interacting with location-based data. Furthermore, we present and evaluate a novel AR-based infotainment system that enables passengers to interact with POIs through an intuitive interface, an approach not previously explored.


%======================== [ Interactions ] ========================%
\subsection{Interactions for In-Car Augmented Reality}
Selection is a critical task in interactions between users and virtual elements in AR systems \cite{blattgerste2018advantages, Doerner2022}. Although object selection in AR has been extensively studied \cite{nizam2018review, hertel2021taxonomy, kyto2018pinpointing, zhou2008trends, blattgerste2018advantages}, most research has focused on stationary environments. Nonetheless, insights from these studies can inform human-computer interaction research in dynamic settings, such as moving vehicles. 
Performing mid-air interactions in moving vehicles presents unique challenges primarily due to the unpredictable nature of vehicular motion. Previous research has explored various aspects of interaction within these dynamic environments, focusing on the potential of multimodal input methods to enhance usability during vehicular motion \cite{roider2018SeeYourPoint}. These approaches aim to address the disruptions caused by vehicle movement, offering more reliable and safe UIs in moving vehicles.

Related work has demonstrated that eye-gaze is often the preferred interaction technique among users and consistently performs better than alternatives. For example, Blattgerste et al. \cite{blattgerste2018advantages} found that eye-gaze outperforms head gaze in several parameters, including speed, while Luro and Sundstedt \cite{luro2019comparative} noted that eye-gaze can reduce cognitive load in participants. Hansen et al. \cite{hansen2018fitts} found that while eye-gaze had lower accuracy and throughput compared to head gaze, the latter was more physically demanding. Kyt{\"o} et al. \cite{kyto2018pinpointing} further showed that although eye-gaze is generally faster, head pointing tends to be more accurate. However, these works were limited to stationary environments only.

There are existing works that studied the selection of objects from the inside of a vehicle. However, previous works mostly study interaction with objects inside the car \cite{aftab2020point}, during short driving rounds \cite{gomaa2020studying}, or don't use AR. For example, R{\"u}melin et al. \cite{rumelin2013free} and Fujimura et al. \cite{fujimura2013driver} investigated hand pointing for interaction with distant objects while in the vehicle, but did not use any immersive technologies. Aftab et al. \cite{aftab2021multimodal, aftab2022pointingoutside} explored multimodal interaction of head, eye, and finger direction for drivers referencing outside-vehicle objects. Gomaa et al. \cite{gomaa2020studying} also adopted a multimodal approach to use eye-gaze and hand pointing to reference objects outside the car. While hand-based interaction has been commonly studied inside vehicles, McGill et al. \cite{mcgill2020challenges} pointed out that physical constraints of the in-car environment, such as available space and motion restraints, may impair the effectiveness of such techniques.

Furthermore, only limited research has focused on interactions performed within AR in vehicles \cite{Schramm2023Assessing, colley2022swivr, kari2023handycast, Tseng2023FingerMapper}. Studies by Tseng et al. \cite{Tseng2023FingerMapper} and Kari et al. \cite{kari2023handycast} have explored interaction techniques that could improve usability within the confined spaces of cars. Colley et al. \cite{colley2022swivr} used a one-degree-of-freedom motion platform to examine standard input methods in VR, assessing their effectiveness in terms of task performance under vehicular motion. The findings of Schramm et al. \cite{Schramm2023Assessing} indicated that eye-gaze with a hardware button achieved the highest selection speed and lowest error rate. However, these results were specific to seat-fixed elements, prompting further investigation into the suitability of eye-gaze techniques for interacting with world-fixed elements.


In summary, multiple interaction techniques have been widely studied for in-car interaction, including hand-pointing, eye-gaze, head-gaze, and multimodal approaches. However, most of these studies focused on interaction without AR and on objects within the vehicle. Building on the demonstrated preference for eye-gaze in previous works \cite{blattgerste2018advantages, luro2019comparative, Schramm2023Assessing}, our research contributes to the field by evaluating eye-based interaction in AR for both car-fixed UIs and world-fixed POIs, expanding the scope of interaction techniques to include external, AR-enhanced environments.



% ========== [ Location based data ] =========================================================
\subsection{Location-based Data}
There is limited research exploring interaction with a cars' outside environment, especially in regard to passenger-based AR \cite{Berger21InteractiveCarDoor}. Matsumura and Kirk \cite{MatsumuraActivePassengering18} investigated the potential of interactive car window systems to enhance passenger engagement with the external environment during car journeys. Their research identifies key themes that support an improved passenger experience, such as:
\begin{itemize}
    \item Active Participation: Passengers appreciated the system for providing a more defined role during the journey, allowing them to engage more actively.
    \item Reflective Interaction: Participants expressed a desire to use the system for post-journey reflection, adding a layer of meaning to their travel experiences.
    \item Social Connectivity: The system's potential to foster social interactions within the car was highlighted, emphasizing its role in enhancing in-car social dynamics.
    \item Temporal Awareness: The system prompted reflections on the changing nature of time during the journey, influencing how passengers perceive and engage with the passage of time.
\end{itemize}

Regarding AR-based POI exploration, Berger et al. \cite{Berger21InteractiveCarDoor} presented an interactive car door concept for rear-seat passengers, featuring an AR-enabled side window that displays POIs along a route, with additional information accessible via a touch-sensitive door panel. Their remote pilot study demonstrated that participants found this system enhanced their user experience, making the ride more engaging and informative by providing trip progress updates and detailed POI information. Overall, the concept was well-received, with participants indicating it made the rear-seat experience more attractive and enjoyable.

While no scientific source exists specifically for an AR concept for interaction with POIs, related concepts have been explored by car manufacturers. For example, BMW presented a concept involving multimodal interaction with the vehicle and its surroundings\footnote{BMW Group: Natural and fully multimodal interaction with the vehicle and its surroundings. \url{https://www.press.bmwgroup.com/global/article/detail/T0292196EN/natural-and-fully-multimodal-interaction-with-the-vehicle-and-its-surroundings-bmw-group-presents-bmw-natural-interaction-for-the-first-time-at-mobile-world-congress-2019} (accessed on 12.11.2024)}\textsuperscript{,}\footnote{BMW Group: BMW Natural Interaction. \url{https://www.press.bmwgroup.com/global/video/detail/PF0006701/bmw-natural-interaction} (accessed on 12.11.2024)}. In this concept, a gesture pointing at a building allows the driver to receive additional information about the building. The work by Aftab et al. \cite{aftab2020point} is directly motivated by this concept, presenting the technical implementation for the multimodal gesture detection. However, no studies exploring how people use this system have been performed.

In summary, while some concepts explore interaction with a car's external environment, none have examined the use of AR for interacting with virtual objects in that environment. We address this gap by introducing a system that leverages eye-gaze and hand gestures, enabling users to interact with POIs in their surroundings. Additionally, our system provides detailed information about the POIs and allows users to explore both upcoming and passed POIs.