\section{Related Works}
\noindent \textbf{Controllable Video Diffusion.}
Recently, diffusion models have achieved great progress on video synthesis, which are usually built on image generation approaches.
%, such as ControlNet and GLIGEN. 
ControlNet \cite{zhang2023controlnet} introduces a novel architecture to integrate spatial conditioning controls into pre-trained text-to-image diffusion models. 
GLIGEN \cite{li2023gligen} designs a gated mechanism to incorporate  grounding information for image generation. Afterwards, many video models have been proposed which aim to generate controllable videos according to user-specified conditions.
Control-A-Video \cite{chen2023control}  leverages both the content prior and motion prior for video generation, and supports  video generation according to different control signals.
%, such as text, edge, and depth map.
Imagen Video \cite{ho2022imagen} first generates videos using a base video generation model, and then cascades spatial and temporal video super-resolution models to generate high-definition videos.
Video LDM \cite{rombach2021highresolution} extends image generation model to video by fine-tuning temporal layers. Text2Video-Zero \cite{khachatryan2023text2video} introduces post-processing techniques to maintain temporal consistency for zero-shot generation.


In addition, few works explore using the reference videos or skeleton poses as input for video generation. 
MotionEditor \cite{tu2024motioneditor} integrates a content-aware motion adapter into ControlNet for video motion editing.
%, addressing challenges in modifying source motion while preserving appearance and background. 
UniAnimate \cite{wang2024unianimate} presents a framework to deal with a reference image and a noised video to improve feature alignment. 
Despite these efforts, designing a method to generate desired motion while preserving spatio-temporal dynamics is still an open problem.



\noindent \textbf{Trajectory-oriented Diffusion.}
Trajectory-based methods have gained great attention for their user-friendliness and the ability to control object motion effectively. DragAnything~\cite{wu2024draganything} employs an entity representation for entity-level motion control of video generation.
%, offering advantages in user interaction and open-domain capabilities. 
Tora \cite{zhang2024tora} introduces a trajectory-oriented diffusion transformer along with a trajectory extractor and a motion-guidance fuser. 
These approaches present the initially potential ability
in simulating real-world movements. However, they usually focus on generating the motion only for pre-defined objects, which do not have comprehensive understanding of the scene or consider the complex interactions of objects.

\noindent \textbf{Grounding and Reasoning with LLMs.}
Large Language Models (LLMs) have shown promising capabilities in grounding and reasoning tasks. 
VideoDirectorGPT \cite{lin2023videodirectorgpt} explores using LLMs for  multi-component video plan, which is used to guide video generation. 
LLM-grounded Video Diffusion (LVD) \cite{lian2023llmgroundedvideo} first leverages LLMs to generate dynamic scene layouts,  and then uses these layouts to guide video diffusion models. Video-of-Thought~\cite{fei2024video}  introduces a reasoning framework for fine-grained spatial-temporal video grounding, which develops step-by-step Chain-of-Thought (CoT) \cite{wei2022chain}. While Video-of-Thought~\cite{fei2024video} employs a step-by-step CoT-based reasoning (e.g., object tracking, action analysis) for video question answering, our work explores reasoning capabilities for video generation task.
Further, our work not only requires the ability to classify the motion type of the object by analyzing a single frame image, but also desires the inference of the future trajectory of the object based on the single frame image thereby treating it as a regression problem. Our approach strives to encode dynamic behavior of objects in a single image to better cope with different motion patterns.


PhysGen \cite{liu2024physgen} uses LLMs to infer the physical parameters of rigid objects and employs a physics simulator to generate realistic object behavior videos. 
However, it struggles with deformable objects and accurately estimating physical parameters from visual input, leading to discrepancies between the generated videos and real-world dynamics. Moreover, its generalization ability is limited as it can only simulate planar motion resulting in sub-optimal results in complex and diverse scenarios. 

\noindent \textbf{Our Approach.}
Different from existing works, we propose a training-free Chain-of-Thought driven motion controller for video generation. By leveraging the capabilities of visual language models (VLM) in combination with a pre-trained trajectory-based video generation model, our approach effectively addresses the aforementioned limitations. In object interaction scenarios, our CoT-based motion reasoning module gradually decomposes complex multi-object interactions to capture causal relationships and interaction forces, thereby allowing consistent trajectory generation without needing a high-precision physics engine.