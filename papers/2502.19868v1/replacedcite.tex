\section{Related Works}
\noindent \textbf{Controllable Video Diffusion.}
Recently, diffusion models have achieved great progress on video synthesis, which are usually built on image generation approaches.
%, such as ControlNet and GLIGEN. 
ControlNet ____ introduces a novel architecture to integrate spatial conditioning controls into pre-trained text-to-image diffusion models. 
GLIGEN ____ designs a gated mechanism to incorporate  grounding information for image generation. Afterwards, many video models have been proposed which aim to generate controllable videos according to user-specified conditions.
Control-A-Video ____  leverages both the content prior and motion prior for video generation, and supports  video generation according to different control signals.
%, such as text, edge, and depth map.
Imagen Video ____ first generates videos using a base video generation model, and then cascades spatial and temporal video super-resolution models to generate high-definition videos.
Video LDM ____ extends image generation model to video by fine-tuning temporal layers. Text2Video-Zero ____ introduces post-processing techniques to maintain temporal consistency for zero-shot generation.


In addition, few works explore using the reference videos or skeleton poses as input for video generation. 
MotionEditor ____ integrates a content-aware motion adapter into ControlNet for video motion editing.
%, addressing challenges in modifying source motion while preserving appearance and background. 
UniAnimate ____ presents a framework to deal with a reference image and a noised video to improve feature alignment. 
Despite these efforts, designing a method to generate desired motion while preserving spatio-temporal dynamics is still an open problem.



\noindent \textbf{Trajectory-oriented Diffusion.}
Trajectory-based methods have gained great attention for their user-friendliness and the ability to control object motion effectively. DragAnything____ employs an entity representation for entity-level motion control of video generation.
%, offering advantages in user interaction and open-domain capabilities. 
Tora ____ introduces a trajectory-oriented diffusion transformer along with a trajectory extractor and a motion-guidance fuser. 
These approaches present the initially potential ability
in simulating real-world movements. However, they usually focus on generating the motion only for pre-defined objects, which do not have comprehensive understanding of the scene or consider the complex interactions of objects.

\noindent \textbf{Grounding and Reasoning with LLMs.}
Large Language Models (LLMs) have shown promising capabilities in grounding and reasoning tasks. 
VideoDirectorGPT ____ explores using LLMs for  multi-component video plan, which is used to guide video generation. 
LLM-grounded Video Diffusion (LVD) ____ first leverages LLMs to generate dynamic scene layouts,  and then uses these layouts to guide video diffusion models. Video-of-Thought____  introduces a reasoning framework for fine-grained spatial-temporal video grounding, which develops step-by-step Chain-of-Thought (CoT) ____. While Video-of-Thought____ employs a step-by-step CoT-based reasoning (e.g., object tracking, action analysis) for video question answering, our work explores reasoning capabilities for video generation task.
Further, our work not only requires the ability to classify the motion type of the object by analyzing a single frame image, but also desires the inference of the future trajectory of the object based on the single frame image thereby treating it as a regression problem. Our approach strives to encode dynamic behavior of objects in a single image to better cope with different motion patterns.


PhysGen ____ uses LLMs to infer the physical parameters of rigid objects and employs a physics simulator to generate realistic object behavior videos. 
However, it struggles with deformable objects and accurately estimating physical parameters from visual input, leading to discrepancies between the generated videos and real-world dynamics. Moreover, its generalization ability is limited as it can only simulate planar motion resulting in sub-optimal results in complex and diverse scenarios. 

\noindent \textbf{Our Approach.}
Different from existing works, we propose a training-free Chain-of-Thought driven motion controller for video generation. By leveraging the capabilities of visual language models (VLM) in combination with a pre-trained trajectory-based video generation model, our approach effectively addresses the aforementioned limitations. In object interaction scenarios, our CoT-based motion reasoning module gradually decomposes complex multi-object interactions to capture causal relationships and interaction forces, thereby allowing consistent trajectory generation without needing a high-precision physics engine.