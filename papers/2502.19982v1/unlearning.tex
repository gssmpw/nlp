\documentclass[10pt,journal,compsoc]{IEEEtran}

\ifCLASSOPTIONcompsoc
\usepackage[nocompress]{cite}
\else
\usepackage{cite}
\fi

\ifCLASSINFOpdf
\else
\fi

\newcommand\MYhyperrefoptions{bookmarks=true,bookmarksnumbered=true,
	pdfpagemode={UseOutlines},plainpages=false,pdfpagelabels=true,
	colorlinks=true,linkcolor={black},citecolor={black},urlcolor={black},
	pdftitle={Bare Demo of IEEEtran.cls for Computer Society Journals},%<!CHANGE!
	pdfsubject={Typesetting},%<!CHANGE!
	pdfauthor={Michael D. Shell},%<!CHANGE!
	pdfkeywords={Computer Society, IEEEtran, journal, LaTeX, paper,
		template}}%<^!CHANGE!

\hyphenation{op-tical net-works semi-conduc-tor}

\usepackage[caption=false,font=normalsize,labelfont=sf,textfont=sf]{subfig}
\usepackage{textcomp}
\usepackage{stfloats}
\usepackage{url}
\usepackage{verbatim}
\usepackage{graphicx}
\usepackage{cite}
\hyphenation{op-tical net-works semi-conduc-tor IEEE-Xplore}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsthm,amsmath}
\usepackage{mathrsfs}
\usepackage{indentfirst}
\usepackage{multirow}
\usepackage{inconsolata}
\usepackage{microtype}
\usepackage{subcaption}
\usepackage{bbold}
\usepackage{longtable}
\usepackage{arydshln}
\usepackage{array}
\usepackage{booktabs}
\usepackage{wrapfig}
\newcommand\ours{\textsc{PerMU}}
\newcommand\bench{\textsc{UGBench}}
\newcommand\metric{\textsc{MSM}}
\newcommand\ttsmall[1]{\texttt{\textmd {#1}}}
\DeclareOldFontCommand{\rm}
\usepackage{makecell}   
\usepackage{mdframed}
\usepackage{enumitem}
\usepackage[super]{nth}
\usepackage[flushleft]{threeparttable}
\usepackage{xcolor}
\usepackage[numbers]{natbib}
% \usepackage{ulem}
% \usepackage{authblk}
\definecolor{lightpurple}{RGB}{221, 160, 221}
\definecolor{lightblue}{RGB}{173, 216, 230}
\theoremstyle{plain}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{proposition}[theorem]{Proposition}
\usepackage{algorithm}
\usepackage{algorithmic}
\newcommand{\theHalgorithm}{\arabic{algorithm}}

\begin{document}
	
	\title{Erasing Without Remembering: Safeguarding Knowledge Forgetting in Large Language Models}
	
	
	\author{
		Huazheng Wang*, Yongcheng Jing, Haifeng Sun, Yingjie Wang, Jingyu Wang, Jianxin Liao, Dacheng Tao
		\thanks{
			*The work was done during visiting Nanyang Technological University.}
		\thanks{
			Huazheng Wang is with Beijing University of Posts and Telecommunications and Nanyang Technological University.}
		\thanks{
			Yongcheng Jing, Yingjie Wang, and Dacheng Tao are with Nanyang Technological University.}
		\thanks{
			Haifeng Sun, Jingyu Wang, and Jianxin Liao are with Beijing University of Posts and Telecommunications.}
	}
	
	% \markboth{Journal of \LaTeX\ Class Files,~Vol.~14, No.~8, August~2015}%
	% {Shell \MakeLowercase{\textit{et al.}}: Bare Advanced Demo of IEEEtran.cls for IEEE Computer Society Journals}
	
	\IEEEtitleabstractindextext{%
		\begin{abstract}
			In this paper, we explore machine unlearning from a novel dimension, by studying how to safeguard model unlearning in large language models (LLMs). Our goal is to prevent unlearned models from recalling any related memory of the targeted knowledge.
			We begin by uncovering a surprisingly simple yet overlooked fact: existing methods typically erase only the exact expressions of the targeted knowledge, leaving paraphrased or related information intact. To rigorously measure such oversights, we introduce \bench, the first benchmark tailored for evaluating the generalisation performance across 13 state-of-the-art methods. \bench~reveals that unlearned models can still recall paraphrased answers and retain target facts in intermediate layers. To address this, we propose \ours, a perturbation-based method that significantly enhances the generalisation capabilities for safeguarding LLM unlearning.
			Experiments demonstrate that \ours~delivers up to a 50.13\% improvement in unlearning while maintaining a 43.53\% boost in robust generalisation. Our code can be found in https://github.com/MaybeLizzy/UGBench.
		\end{abstract}
		
		
		\begin{IEEEkeywords}
			Machine Unlearning, Large Language Models, Generalisation.
	\end{IEEEkeywords}}
	
	\maketitle
	\section{Introduction}
	\IEEEPARstart{L}{arge} language models (LLMs)~\cite{LLaMA,GPT-4}, while displaying remarkable performance thanks to their capacity for recalling extensive knowledge from pre-training corpora, are also increasingly susceptible to generating private, harmful, or even illegal content, due to their unintended memorisation of confidential information~\cite{Machine-Unlearning-1,Data_Deletion}. In response to this dilemma, LLM-tailored machine unlearning~\cite{Right_to_be_Forgotten,Knowledge_Unlearning} has emerged as a rising research focus, aiming to develop {reliable} and {computationally efficient} knowledge-forgetting approaches for erasing the influence of specific undesired data from trained LLMs, all while preserving their utility for the remaining data.
	
	\begin{figure}[t]
		\centering
		\includegraphics[width=0.45\textwidth]{intro.pdf}
		\caption{
			Depiction of the proposed \emph{unlearning scope} in a hypothetical semantic embedding space, highlighting the generalisation dilemma inherent in machine unlearning for LLMs. Ideally, hard in-scope samples that lie within the unlearning scope by a small margin should also be forgotten. These include rephrased questions, as well as the relation reversed questions and so on. 
		}
		\label{intro}
	\end{figure}
	
	State-of-the-art machine unlearning approaches for LLMs broadly fall into two categories: training-free and training-based methods. The former, such as neuron editing~\cite{DEPN}, in-context learning~\cite{In-Context-Unlearning}, and prompt engineering~\cite{Unlearning-Prompts}, unlearn knowledge without additional training but often suffer from limited application scenarios.
	In contrast, training-based methods typically achieve greater unlearning effectiveness by updating model parameters and gradients, using techniques like gradient ascent~\cite{grad_ascent}, preference optimisation~\cite{NPO}, relabeling-based fine-tuning~\cite{SNAP}, task arithmetic~\cite{taskvectors}, logit-difference fine-tuning~\cite{ULD}, or adding new parameters~\cite{EUL,PEFT_unlearn}.
	
	Despite significant progress in LLM-based unlearning, this paper identifies an embarrassingly simple yet critical dilemma: existing methods typically teach models to forget only the exact expressions of the unlearning samples, while failing to genuinely unlearn paraphrased or other related information that should also be erased.
	To better illustrate, we introduce an \emph{unlearning scope} in Fig.~\ref{intro}, encompassing all the knowledge that unlearned models are expected to forget, such as paraphrased versions, {reversed relations, one-hop questions, and those with substituted subjects}.
	As shown in Fig.~\ref{intro}, successful unlearning should intuitively modify the model's behavior for in-scope samples while leaving out-of-scope samples unaffected~\cite{Rethinking}.
	However, to preserve utility, existing methods often compromise by achieving superficial forgetting, failing to unlearn ``hard'' samples near the boundary of the unlearning scope, which ultimately results in poor generalisation.
	
	To further substantiate this observation, our \textbf{first contribution} in this paper is to introduce \bench, the first generalisation evaluation benchmark tailored for LLM-based machine unlearning. \bench\ provides an unbiased assessment of existing methods in forgetting in-scope unlearning samples.
	The benchmark spans three data domains: two widely-used machine unlearning datasets, TOFU~\cite{TOFU} and the Harry Potter books~\cite{harry_potter,SOUL}, as well as a popular model editing dataset, ZsRE~\cite{zsre}. 
	Evaluations are performed on 13 existing methods using two language models of different scales, \ttsmall{Phi-1.3B} and \ttsmall{LLaMA2-7B}.
	Our {empirical} analyses on \bench~uncover the following unique findings that have been overlooked by existing research:
	
	\begin{itemize} 
		
		\item {\emph{ \textbf{Identified challenge}}}: \emph{Existing machine unlearning methods consistently exhibit a lack of generalisation;}
		
		\item \emph{ \textbf{The \nth{1} cause}: Unlearned models tend to remember target facts in their middle layers during inference;}
		
		\item \emph{ \textbf{The \nth{2} cause}: Unlearned models are still capable of recalling paraphrased answers during inference.}
	\end{itemize}
	These insights suggest target knowledge is not truly erased, leading to \emph{risky unlearning} due to generalisation failure.
	
	Motivated by the discoveries from \bench, our \textbf{second contribution} is to shift the focus from the conventional performance race to an equally vital yet unexplored facet: \emph{safeguarding LLM-based model unlearning}.
	Our goal is to prevent unlearned models from retaining any memory of the unlearned samples within the defined unlearning scope in Fig.~\ref{intro}, accomplished by enhancing the generalisation capabilities of the unlearned models.
	However, this ambitious goal is not without challenges. 
	One vanilla approach is to label all rephrased versions of unlearning samples and their paraphrased answers for unlearning. Yet, this method is prohibitively labor-intensive and time-consuming, motivating the development of a novel solution to safeguard unlearning.
	
	To this end, this paper strives to take a pilot step toward safer machine unlearning with strengthened generalisation, by proposing an innovative probability-perturbation unlearning (\ours) method  that, paradoxically, leverages adversarial examples as its foundation. 
	In particular, rather than treating adversarial examples as mere threats, the goal of \ours~is to reverse their role to enable generalised unlearning by perturbing the most vulnerable tokens in the unlearning samples, thereby forcing the model to generate incorrect answers as if it had never been trained on them.
	To identify these vulnerable tokens for perturbation, we propose a novel metric, termed as \metric, that quantifies {the model's sensitivity to specific tokens with theoretical guarantees.} 
	
	Through the analysis based on \metric, we observe that subject tokens in the unlearning samples exhibit the highest sensitivity. Driven by this discovery, \ours~injects random noise into the embeddings of subject tokens prior to model processing. 
	This disrupts the model's ability to recall factual information by shifting the top-ranked tokens in the next-token probability distribution from factual or answer-related terms to those reflecting grammatical or contextual patterns. 
	{\ours~then subtracts the original distribution from the perturbed probability distribution. }
	Finally,
	\ours~minimises the distance between {the subtracted distribution} and the unlearned model's distribution, ensuring that the original correct answers are assigned lower probabilities.
	As such, \ours~simultaneously addresses both causes of the generalisation challenge identified in \bench~as follows:
	
	\begin{itemize} 
		\item \emph{\textbf{Solving the \nth{1} cause}:} \ours~introduces random noise into the subject token embeddings at the first layer, effectively preventing the model from retrieving or generating factual information across subsequent middle layers;
		
		\item \emph{\textbf{Solving the \nth{2} cause}:} \ours~substantially reduces the probabilities of rich, highly-ranked answers and answer-related tokens in the original distribution, accomplished by the perturbed 
		distribution subtraction.
	\end{itemize}
	
	
	In sum, our contribution is a novel generalisation evaluation benchmark, \bench, that assesses the generalisation capabilities of existing methods to forget knowledge within the unlearning scope, as well as a perturbation-based machine unlearning method, \ours, that uniquely prevents models from remembering associated facts.
	{Comparative experiments with 13 state-of-the-art approaches on {7} partitions across 3 data domains demonstrate that
		\ours~achieves up to a 50.13\% improvement in unlearning and a 43.53\% enhancement in robust generalisation, all while maintaining high model utility and superior generation quality.
		
		\section{Related Work}
		
		We provide a brief overview of existing machine unlearning methods for LLMs, categorised into training-free and training-based approaches~\cite{Knowledge_Unlearning}, along with the evaluation methods relevant to our proposed \bench~benchmark.  
		
		\noindent\textbf{Training-free LLM Unlearning.}
		One area of research concentrates on identifying neurons linked to unlearning samples~\cite{Patil} and directly modifying these detected neurons~\cite{DEPN} without the need for learning. With the advancement of in-context learning, some approaches unlearn knowledge by providing LLMs with unlearned samples accompanied by different labels~\cite{In-Context-Unlearning, Guardrail} or by generating corrupted prompts with altered embeddings~\cite{Unlearning-Prompts}. 
		Despite their efficiency, locate-and-edit methods are constrained to triplet-format data, while prompt-based methods depend on artificially designed templates, limiting their practicality in real-world scenarios. As such, this paper primarily focuses on training-based methods.
		
		
		\noindent\textbf{Training-based LLM Unlearning.}
		Another stream of unlearning approaches focuses on models and gradients. The most common training-based method is gradient ascent~\cite{grad_ascent}, which updates model parameters by maximizing the likelihood of mis-prediction for samples in the forget set. To mitigate the catastrophic collapse issue associated with gradient ascent, reinforcement learning has been employed to align the model with negative preference optimization (NPO)~\cite{NPO}, treating forgotten data as negative examples. Alternatively, instruction-tuning LLMs to generate responses such as ``I do not have access to…” or ``I don’t know…” has also been explored~\cite{SNAP}. 
		To improve efficiency, other approaches incorporate additional trainable layers or modules~\cite{Chaff}, integrating or adding them into the original models. For example, Chen \& Yang~\cite{EUL} introduces unlearning layers to forget specific data sets, which are then integrated into transformers. Likewise, Zhang et al.~\cite{PEFT_unlearn} combines various lightweight modules with distinct functionalities to enable unlearning. To further preserve model utility, some methods train a reinforced or assistant model~\cite{ULD}, comparing its prediction logits with those of the baseline model~\cite{harry_potter, RKLD}. Others~\cite{taskvectors} employ simple arithmetic operations on task vectors to modify the model, such as reducing undesirable behaviors, forgetting specific tasks, or enabling multitask learning. However, the generalisation capabilities have been largely overlooked in prior research. Our study uniquely identifies the generalisation dilemma in this area, explains its underlying causes, and proposes a novel unlearning scheme to address it.
		
		\noindent\textbf{Unlearning Evaluation.}
		Several studies have examined LLM unlearning from different perspectives~\cite{Poisoning_attack}.  Specifically, Patil et al.~\cite{Patil} investigate the effectiveness of typical model editing methods in removing information from model weights. Hong et al.~\cite{Intrinsic} use vocabulary projections to analyze concept vectors through parametric knowledge traces. Yao et al.~\cite{acl_bench} evaluate seven different unlearning methods on longer-context tasks across three source domains. Meanwhile, Shi et al.~\cite{MUSE} assess six desirable properties of unlearned models across eight unlearning methods. Jia et al.~\cite{SOUL} focus on the influence of second-order optimization on unlearning. Li et al.~\cite{WMDP} examine malicious use scenarios in biosecurity, cybersecurity, and chemical security. Qiu et al.~\cite{PISTOL} evaluate four distinct unlearning methods for removing highly interconnected data. Du et al.~\cite{TULA} investigate the risk of knowledge leakage after unlearning. The survey by Liu et al.~\cite{Rethinking} highlights often-overlooked aspects of existing LLM unlearning research and introduces the concept of unlearning scope. Unlike these existing works, \bench~is the first benchmark to evaluate the state-of-the-art methods on their generalisation ability. Notably, the evaluation covers thirteen unlearning methods across seven partitions from three domains, offering a more diverse and challenging scenario.
		
		\subsection{Problem Definition}
		\label{Problem Definition}
		The objective of machine unlearning is to enable an initial target model to forget specific unlearning samples as if it were never trained on them, while preserving the model’s performance on unrelated knowledge. 
		More specifically, the target model $f_{\theta_{tr}}$ is represented by a function $f: \mathbb{X} \mapsto \mathbb{Y}$, where $\theta_{tr}$ denotes the parameters of the target model. Let the pre-training dataset be $D_{tr}$, and the dataset to be forgotten be $D_f$. The retained dataset is then defined as $D_r = D_{tr} \backslash D_f$. The ideal retained model, $f_{\theta_{r}}$, is one that has never been trained on $D_f$. Since $\theta_{tr}$ is not directly accessible, we define an unlearning procedure $\mathbb{U}$, which takes $f_{\theta_{tr}}$ and $D_f$ as inputs, producing an unlearned model $f_{\theta_{u}} \sim \mathbb{U}(f_{\theta_{tr}}, D_f)$. 
		The unlearned model’s predictions should also change for the paraphrased forget dataset $D_{p}$.
		Therefore, given a distance metric $m(\cdot)$, the objective of the unlearning algorithm is to minimize the distance between $f_{\theta_{u}}$ and $f_{\theta_{r}}$ for each sample $x \in D_f \cup D_{p}$ :
		$
		\frac{\mathbb{E} [m(f_{\theta_{u}}(x))]}{\mathbb{E} [m(f_{\theta_{r}}(x))]} \approx 1.
		$
		
		
		\section{Benchmark Setup}
		In this section, we provide a detailed overview of the experimental setups for the proposed unlearning generalization benchmark, \bench, covering the datasets, evaluation metrics, baselines, and implementation details.
		
		\subsection{Dataset}
		\bench~is evaluated across three distinct data domains, including two widely-used machine unlearning datasets, TOFU~\cite{TOFU} and Harry Potter (HP) ~\cite{harry_potter}, as well as a popular model editing dataset, ZsRE~\cite{zsre}. 
		
		\noindent\textbf{TOFU} The TOFU dataset comprises 200 diverse synthetic author profiles, each featuring 20 question-answer pairs. It encompasses four subsets—Forget Set, Retain Set, Real Authors, and World Facts—and supports three forgetting settings: Forget01, Forget05, and Forget10, corresponding to the removal of 1\%, 5\%, and 10\% of the data, respectively. Additionally, it provides a paraphrased version of the forget dataset, and we directly use the rephrased questions for testing. 
		
		\noindent\textbf{Harry Potter} The HP dataset~\cite{SNAP} contains multiple question-answer pairs derived from the Harry Potter series, with each question involving multiple entities or subjects, making it a more challenging unlearning dataset. Since no labeled rephrased data is available, we use GPT-4 to generate rephrased versions of both the forget and retain datasets, using the template: ``Please provide a rephrased version of the question: \textit{[Question]}". 
		
		\begin{table}[h]
			\caption{The data splits and statistics.}
			\label{split}
			\begin{center}
				\begin{small}
					\begin{tabular}{c|c|ccc}
						\toprule
						& & \textbf{Forget} & \textbf{Retain} & \textbf{All} \\ 
						\midrule
						\multirow{3}{*}{\textbf{TOFU}} 
						& Forget01 & 40 & 3960 & 4000 \\
						& Forget05 & 200 & 3800 & 4000 \\
						& Forget10 & 400 & 3600 & 4000 \\
						\midrule
						\textbf{Harry} & -  & 50 & 150& 200  \\
						\midrule
						\multirow{3}{*}{\textbf{ZsRE}} 
						& Inverse Relation & 96 & 289 & 385 \\
						& Subject Replace & 73 & 220 & 293  \\
						& One-Hop & 259 & 778 & 1037 \\
						\midrule
						\multirow{2}{*}{\textbf{Retain}} 
						& Real World & - & - & 117  \\
						& Real Author & - & - & 100 \\ 
						\bottomrule
					\end{tabular}
				\end{small}
			\end{center}
		\end{table}
		
		\noindent\textbf{ZsRE} To thoroughly assess whether the unlearned model can forget logically related facts, we use ZsRE dataset~\cite{Editing_Problems} and conduct evaluations across three dimensions: \textbf{\textit{Subject Replacement}}, \textbf{\textit{Reversed Relation}}, and \textbf{\textit{One-hop Reasoning}}. The detailed data statistics are shown in Tab.~\ref{split}.
		
		\textbf{(i) Subject Replacement:} In this evaluation, the subject in the unlearning example is substituted with an alias or synonym to assess the unlearned model’s capability to generalise the unlearning attribute to different representations of the same subject. For instance, as shown in Fig.~\ref{intro}, the subject ``Prince Charles" can also be described as ``Charles Philip Arthur George". Thus, the subject replacement question for ``Who is the son of Prince Charles" becomes ``Who is the son of Charles Philip Arthur George".
		
		\textbf{(ii) Reversed Relation:} When the target of a subject-relation pair is unlearned, the attribute of the target entity should also change. To evaluate this, we test the model using a reverse question to determine if the target entity has also been unlearned. For example, if the knowledge ``Who is the son of Prince Charles? Prince William" is unlearned, the unlearned model should no longer predict “Prince Charles" for the relation reversed question “Who is the father of  Prince William?".
		
		\textbf{(iii) One-hop Reasoning:}  The unlearned model should exclude the unlearned knowledge when performing downstream tasks. To assess this, we evaluate the model’s ability to unlearn knowledge that is one-hop reasoned from the original unlearning samples. For instance, if the knowledge ``Who is the son of Prince Charles? Prince William" is unlearned, the model is also expected to unlearn the one-hop knowledge, such as ``When is Prince Charles's son's birthday?".
		
		The dataset is divided into a forget set and a retain set at a ratio of 1:3. The aforementioned datasets comprise a total of seven distinct partitions of forget sets, along with their corresponding retain sets. Additionally, we incorporate the Real Authors and World Facts sets from TOFU as supplementary retain data to evaluate model utility. 
		
		\subsection{Evaluation Metrics.} 
		Following prior studies~\cite{TOFU,RKLD}, we report ROUGE \textbf{(RG)}, Probability \textbf{(Pr)}, and Truth Ratio \textbf{(TR)} metrics on TOFU dataset. 
		For the HP and ZsRE datasets, where answers are relatively short, we alternatively report the \textbf{F1} score. Consider an input sequence $x=(q,a)$.
		
		\begin{itemize}
			
			\item \textbf{ROUGE (RG)}: We use ROUGE-L recall~\cite{ROUGE} score to compare model answers with the ground truth, as it accounts for the output phrasing to be slightly different than the ground truth. When evaluated on the retain set, a higher ROUGE score indicates better performance. Conversely, when evaluated on the forget set, a lower ROUGE score is preferred.
			
			\item \textbf{Probability (Pr)}: On the Forget Set and Retain Set, we compute the conditional probability $P(a|q)$ according to the model and raise it to the power $1/|a|$ to normalize for answer length. On Real Authors and World Facts, we treat each question $q$ as a multiple choice question associated with choices ${a_1, . . . , a_n}$. Without loss of generality, assume that $a_1$ is the correct answer, then the probability is computed as $P(a|q)/\sum\nolimits_{i=1}^n P(a_i|q)$. Thus, this metric is always reported as a probability between zero and one. When evaluated on the retain set, a higher Probability score indicates better performance. Conversely, when evaluated on the forget set, a lower Probability score is preferred.
			
			\item \textbf{Truth Ratio (TR)}: For a given question, we compute a ratio that approximately compares how likely its correct answer is to an incorrect answer. Let $\hat{a}$ denote a paraphrased version of the correct answer, $\mathcal{A}_{\text {pert }}$ is the set of paraphrased incorrect answer. The truth ratio can be written as:
			\begin{equation}
				R_{\text {truth }}=\frac{\frac{1}{\left|\mathcal{A}_{\text {pert }}\right|} \sum_{\hat{a} \in \mathcal{A}_{\text {pert }}} P(\hat{a} \mid q)^{1 /|\hat{a}|}}{P(\tilde{a} \mid q)^{1 /|\tilde{a}|}}.
			\end{equation}
			We report $\mathrm{TR}=R_{\text {truth }}$ on forget set, and $\mathrm{TR}=max(0, 1-R_{\text {truth }})$ on retain set. Therefore, the Truth Ratio score is expected to be higher on both the retain set and the forget set.
			
			\item \textbf{F1}: We report the F1 score for the Harry Potter and ZsRE datasets, as it provides a balanced measure between precision and recall, calculated as the harmonic mean of these two metrics.
			\begin{equation}
				F1 = 2 \times \frac{Precision \times Recall}{Precision + Recall}.
			\end{equation}
			When evaluated on the retain set, a higher F1 score indicates better performance. Conversely, when evaluated on the forget set, a lower F1 score is preferred.
			
		\end{itemize}
		
		To assess the retain data, we use the Model Utility \textbf{(MU)} metric on retained data, which is the harmonic mean of the RG, Pr and TR (or F1) metrics across three datasets: Retain Set, Real Authors, and World Facts. Notably, to measure the trade-off between the unlearned model's forgetting effect and its retained utility, we propose a novel \emph{Forget-Retain Trade-off \textbf{(FRT)}} metric, calculated as the Model Utility divided by the mean of the forget set's ROUGE and Probability (or F1) scores. A higher FRT metric indicates a better balance between forgetting and retaining. 
		
		\subsection{Thirteen Benchmarked Algorithms.} 
		We evaluate 13 efficient unlearning methods, including Gradient Ascent (\textbf{GA})~\cite{grad_ascent}, Direct Preference Optimization (\textbf{DPO})~\cite{DPO}, Negative Preference Optimization (\textbf{NPO})~\cite{NPO}, Task Vectors (\textbf{TV})~\cite{taskvectors}, Who’s Harry Potter (\textbf{WHP})~\cite{harry_potter},  \textbf{ULD}~\cite{ULD} and \textbf{ICL}~\cite{Guardrail,In-Context-Unlearning}. 
		
		\begin{itemize}
			
			\item \textbf{Gradient Ascent (GA)}~\cite{grad_ascent} is fundamentally straightforward by reducing the likelihood of correct predictions on the forget set. The training objective is to maximize the standard training loss in order to make the model deviate from its initial prediction. 
			\begin{equation}
				\mathcal{L}_{\mathrm{GA}}(\theta) = \min _{\boldsymbol{\theta}}-\mathbb{E}_{(x, y) \in \mathcal{D}_{\mathrm{f}}}[\ell(y \mid x ; \boldsymbol{\theta})], 
				\label{ga}
			\end{equation} 
			where $\mathcal{D}_{\mathrm{f}}$ is the forget dataset and $\theta$ represents the model parameter.
			
			\item \textbf{Direct Preference Optimization (DPO)}~\cite{DPO} seeks to align the model with the newly generated alternative answer like “I do not know the answer” or any similar option. 
			\begin{equation}
				\mathcal{L}_{\mathrm{DPO}}(\theta) = \min _{\boldsymbol{\theta}} \mathbb{E}_{\left(x, y_{\mathrm{idk}}\right) \in \mathcal{D}_{\mathrm{f}},y_{idk}\sim D_{idk}}\left[\ell\left(y_{\mathrm{idk}} \mid x ; \boldsymbol{\theta}\right)\right],
				\label{dpo}
			\end{equation} 
			where $D_{idk}$ represents the fixed dataset containing all alternative responses $y_{\mathrm{idk}}$.
			
			\item \textbf{Negative Preference Optimization (NPO)}~\cite{NPO} treats the forget set as negative preference data and uses the offline DPO objective to adjust the model, ensuring it assigns a low likelihood to the forget set while maintaining close alignment with the original model. The adaptive weight, typically set to less than 1, ensures a more controlled and gradual divergence, which is essential for effective unlearning~\cite{NPO_Rethinking}.
			\begin{equation}
				\mathcal{L}_{\mathrm{NPO}}(\theta) = -\frac{2}{\beta} \mathbb{E}_{x \sim \mathcal{D}_{\text {f}}}\left[\log \sigma\left(-\beta \log \frac{f_\theta(x)}{f_{\text {target}}(x)}\right)\right],
				\label{npo}
			\end{equation} 
			where $f_\theta$ refers to the unlearning model and $f_{\text {target}}$ denotes the original pre-trained target model. The parameter $\beta$ controls the allowed divergence between $f_\theta$ and $f_{\text {target}}$. Following previous work~\cite{MUSE,TOFU}, we set $\beta=0.1$ in our experiments.
			
			\item \textbf{Task Vectors (TV)}~\cite{taskvectors} defines a direction in the weight space of a pre-trained model by applying simple arithmetic operations on the model weights, allowing for effective control of the model's behavior. To do this, we first fine-tune the target model $f_{\text {target}}$ on the forget dataset until it overfits, resulting in a reinforced model $f_{\text {reinforced}}$. Next, we obtain the Task Vector by subtracting the parameters of $f_{\text {target}}$ from $f_{\text {reinforced}}$. To achieve unlearning, we subtract the Task Vector from $f_{\text {target}}$'s weights, intuitively removing the model weights most closely associated with the forget data. This is expressed as $f_{\text {unlearn}} = f_{\text {target}} - (f_{\text {reinforced}} - f_{\text {target}})$.
			
			\item \textbf{Who’s Harry Potter (WHP)}~\cite{harry_potter} achieves unlearning by manipulating the predicted logit probabilities of the target model. To do this, we first fine-tune the target model $f_{\text {target}}$ on the forget dataset until it overfits, producing a reinforced model $f_{\text {reinforced}}$. WHP then adjusts the next-token probability distribution using the following equation:
			\begin{equation}
				p_{f_{\text {unlearn}}}(\cdot|x) = p_{f_{\text {target}}}(\cdot|x) - \alpha \cdot (p_{f_{\text {reinforced}}}(\cdot|x) - p_{f_{\text {target}}}(\cdot|x)) ,
			\end{equation}
			where $p_f(\cdot|x)$ denotes the token probability distribution parameterized by model $f$ given the input $x$, and $\alpha$ is a hyper-parameter controlling the degree of adjustment. Following previous work~\cite{MUSE}, we set $\alpha=1$.
			
			\item \textbf{Unlearning from Logit Difference (ULD)}~\cite{ULD} also achieves unlearning in the token probability space. It first fine-tunes an assistant model with the opposite unlearning objectives, which aims to remember the forget documents and forget the retained knowledge. ULD then derives the unlearned model by computing the logit difference between the target model and the assistant model:
			\begin{equation}
				l_{f}(Y|X) = l(Y|X;\theta) - \alpha \cdot l_{a}(Y|X;\phi) ,
			\end{equation}
			where $l(Y|X;\theta)$ denotes the output logits of the original model, $l_{a}(Y|X;\phi)$ represents the output logits of the assistant model, and $\alpha$ is a hyper-parameter controlling the strength of forgetting. We keep $\alpha=0.75$ consistent with their work.
			
			\item \textbf{In-Context Learning-based unlearning method (ICL)}~\cite{In-Context-Unlearning} typically employs carefully crafted prompts to achieve unlearning without any updates to the model parameters. Following the prompt template~\cite{Guardrail}, we use the following instruction:``You are an AI Assistant who is supposed to unlearn about \textit{[Subject]} and provide answers without its knowledge as if you never knew about it. Don’t tell anyone that you unlearned
			anything". To ensure a fair comparison, we maintain consistency in the generic prompt across all datasets and models.
			
		\end{itemize}
		
		Following~\cite{MUSE}, we apply two regularizations for utility preservation: Gradient Descent (\textbf{GDR}) and KL Divergence Minimization (\textbf{KLR}) on the Retain Set.
		
		\begin{itemize}
			
			\item \textbf{Gradient Descent (GDR)}~\cite{TOFU} strives to maintain performance on the retain set by maximizing the likelihood of correct prediction on randomly sampled retain examples, where $\mathcal{D}_{\mathrm{r}}$ represents the retain set. 
			\begin{equation}
				\mathcal{L}_{\mathrm{GDR}}(\theta) = \mathbb{E}_{(x, y) \in \mathcal{D}_{\mathrm{r}}}[\ell(y \mid x ; \boldsymbol{\theta})] ,
			\end{equation}
			
			
			\item \textbf{KL Divergence Minimization (KLR)}~\cite{QUARK,TOFU} aims to minimize the KL divergence of the predictions on retain set between the original model and the unlearning model to prevent it deviating too far from the original model. Given $x_r \in D_r$, the loss is:
			\begin{equation}
				\mathcal{L}_{\mathrm{KLR}}(\theta) = \mathrm{KL} \left(p_{f_{\text {target}}}(\cdot|x_r) \| p_{f_{\text {unlearn}}}(\cdot|x_r) \right) .
			\end{equation}
			
		\end{itemize}
		
		We combine GA, DPO, and NPO with these two regularizations using a retain weight $RW$, denoted as ``+GDR" or ``+KLR". The total unlearning loss is given by $\mathcal{L}(\theta) = \mathcal{L}_{\mathrm{unlearn}}(\theta) + RW \cdot \mathcal{L}_{\mathrm{retain}}(\theta)$. Following previous work~\cite{MUSE, TOFU}, we set $RW=1$. The combination of GDR and KLR results in a total of thirteen unlearning methods evaluated in \bench. 
		\textbf{Retain} refers to the retain model, fine-tuned exclusively on the retain set without exposure to any forget data, and is considered an upper bound. 
		
		\subsection{Implementation Details}
		\bench~is tested across two models, \texttt{Phi-1.3B} and \texttt{LLaMA2-7B}.  
		When tested on TOFU, we use the checkpoints of the pre-trained target model from the TOFU Leaderboard\footnote[1]{https://huggingface.co/spaces/locuslab/tofu\_leaderboard}. For the Harry Potter and ZsRE datasets, we first fine-tune the model on the respective dataset before applying unlearning. The fine-tuning settings are as follows: learning rate of 3e-5, 10 epochs, batch size of 8, with a gradient accumulation step of 4. For Task Vector and WHP, to obtain the reinforced model for unlearning, we fine-tune the target model for 10 epochs using the same learning rate and batch size. For ULD, we obtain the assistant model by fine-tuning the target model using the default settings provided by~\cite{ULD}. 
		For the unlearning process, the unlearning batch size is set to 8, with a gradient accumulation step of 4. The process is conducted over 5 epochs, using a default learning rate of 2e-5. Since different learning rates can result in varying trade-offs between forgetting and retention, we slightly adjust the learning rate for each method to ensure comparable levels of model utility. To ensure fairness, all other unlearning hyper-parameters follow the default settings for each respective unlearning algorithm. All results are averaged over three runs. 
		We use one A100 GPU with 80 GB of RAM. Note that during fine-tuning and unlearning on \ttsmall{LLaMA2-7B}, we update all 7B model parameters. 
		
		
		\section{Benchmark Analysis}
		\label{Benchmark Analysis}
		In this section, we present a comprehensive benchmark analysis. Through extensive experiments, we observe that \textbf{\textit{existing unlearning methods exhibit limited generalization ability}}. A detailed explanation is provided below.
		
		As shown in Tab.\ref{TOFU_rephrase}, when tested on the TOFU dataset, although DPO+GDR achieves a superior ROUGE score, even surpassing the retain model, it fails to effectively reduce the Probability score.  
		When tested on \ttsmall{LLaMA2-7B} using HP dataset (Tab.\ref{Harry_rephrase}), all methods encounter significant challenges in forgetting the rephrased unlearning samples, with the Probability score showing a gap of up to 43.48\% compared to the retain model. 
		Notably, all methods exhibit a Forget ROUGE score that remains above 90\%, indicating that they forget almost nothing.
		A similar trend is observed on ZsRE dataset (Tab.\ref{zsre_rephrase}). Interestingly, the ICL-based unlearning method achieves the best performance on subject-replaced examples, while other approaches consistently maintain exceptionally high ROUGE, probability, and F1 scores. Moreover, existing unlearning methods encounter significant challenges in forgetting one-hop reasoning examples. Empirical results underscore the limited generalisation ability of existing unlearning methods. 
		
		We identify and investigate two reasons contributing to the poor generalisation ability observed. 
		
		
		\begin{figure}[t]
			\centering
			\includegraphics[width=0.48\textwidth]{layer.pdf}
			\caption{The ranking of the first key token for the correct answer in the next-token probability distribution rises rapidly in the mid-layers of the unlearned model fine-tuned with Gradient Ascent.
			}
			\label{layer}
		\end{figure}
		
		
		\begin{table*}[h]
			\caption{Experimental results for the  TOFU dataset using \ttsmall{LLaMA2-7B} for Forget05 subset. \ours~outperforms the baselines with a relative improvement of up to \textbf{50.13\%} (78.67 $\rightarrow$39.23) in Forget Probability and 14.43\% (56.27$\rightarrow$64.39) on Forget Truth Ratio. Meanwhile, \ours~maintains the highest model utility, effectively preserving retained knowledge.}
			\label{tofu_llama_full}
			\setlength{\tabcolsep}{2.8pt}
			% \vskip 0.15in
			\begin{center}
				\begin{small}
					\begin{tabular}{c|ccc|ccc|ccc|ccc|cc}
						\toprule
						\textbf{Dataset} & \multicolumn{3}{c|}{\textbf{Forget data}}         & \multicolumn{3}{c|}{\textbf{Retain data}}         & \multicolumn{3}{c|}{\textbf{Real Authors}}        & \multicolumn{3}{c|}{\textbf{Real World}}          & \multirow{2}{*}{\textbf{MU↑}} & \multirow{2}{*}{\textbf{FRT↑}} \\ 
						\textbf{Metric}  & \textbf{RG↓}   & \textbf{Pr↓}   & \textbf{TR↑}   & \textbf{RG↑}   & \textbf{Pr↑}   & \textbf{TR↑}   & \textbf{RG↑}   & \textbf{Pr↑}   & \textbf{TR↑}   & \textbf{RG↑}   & \textbf{Pr↑}   & \textbf{TR↑}   &         &        \\
						\midrule
						\colorbox{gray!40}{\textbf{Retain}}  & 39.56          & 14.61          & 66.23          & 91.58          & 96.56          & 48.02          & 89.13          & 40.38          & 54.74          & 89.60          & 39.52          & 52.76          & 59.30                         & 2.19                           \\
						\textbf{ICL}     & 64.36          & 90.15          & 53.12          & \textbf{98.08} & \textbf{98.93} & 47.06          & 93.30          & 44.82          & 57.93          & 88.32          & 42.52          & 55.96          & 62.26                         & 0.81                           \\
						\textbf{GA}      & 73.24          & 83.41          & 51.10          & 95.03          & 97.55          & 47.02          & 92.30          & 43.19          & 55.75          & 87.61          & 42.34          & 55.22          & 61.18                         & 0.78                           \\
						\textbf{GA+GDR}  & 75.02          & 85.13          & 50.15          & 96.20          & 98.36          & \textbf{47.25} & 92.30          & 42.82          & 55.74          & 87.46          & 41.51          & 54.84          & 60.97                         & 0.76                           \\
						\textbf{GA+KLR}  & 80.92          & 89.73          & 51.26          & 96.90          & 98.14          & 46.91          & 93.30          & 44.60          & 57.47          & 88.03          & 43.03          & 55.99          & 62.15                         & 0.73                          \\
						\textbf{DPO}     & 72.25          & 92.15          & 55.17          & 81.01          & 94.40          & 44.03          & 88.97          & \colorbox{lightblue!50}{48.64}          & \colorbox{lightblue!50}{62.75}          & 86.61          & 45.61          & 57.46          & 62.39                         & 0.76                           \\
						\textbf{DPO+GDR} & 36.59          & 87.10          & \colorbox{lightblue!50}{56.27}          & 90.23          & 97.01          & 43.02          & 90.63          & 46.30          & 59.93          & 85.75          & 43.56          & 53.08          & 61.05                         & 0.99                           \\
						\textbf{DPO+KLR} & 87.91          & 95.71          & 54.18          & 88.38          & 96.72          & 44.68          & 93.63          & 48.37          & 62.57          & 86.61          & 45.14          & 56.91          & \colorbox{lightpurple!50}{63.10}                         & 0.69                           \\
						\textbf{NPO}     & 71.04          & 84.43          & 51.41          & 95.68          & 97.64          & 46.95          & 91.30          & 43.74          & 56.43          & 87.89          & 42.80          & 55.85          & 61.57                         & 0.79                           \\
						\textbf{NPO+GDR} & 71.33          & 84.41          & 51.44          & 95.74          & 97.71          & 46.90          & 92.30          & 43.84          & 56.46          & 87.89          & 42.75          & 55.61          & 61.60                         & 0.79                           \\
						\textbf{NPO+KLR} & 71.46          & 84.86          & 51.37          & 95.66          & 97.68          & 46.94          & 92.30          & 43.97          & 56.75          & 87.89          & 42.97          & 55.75          & 61.74                         & 0.79                           \\
						\textbf{TV}      & 68.29          & \colorbox{lightblue!50}{78.67}          & 52.55          & 94.39          & 96.57          & 46.30          & 93.30          & 44.03          & 57.31          & 89.17          & 43.64          & 56.20          & 61.92                         & 0.84                           \\
						\textbf{WHP}     & 96.77          & 80.70          & 51.26          & 98.06          & 98.00          & 46.91          & \textbf{94.30} & 42.53          & 54.95          & 88.32          & 41.79          & 55.01          & 61.03                         & 0.69                           \\
						\textbf{ULD}     & 94.45          & 97.78          & 50.73          & 95.86          & 97.81          & 46.80          & 92.77          & 45.41          & 58.73          & 88.75          & 44.10          & \colorbox{lightblue!50}{58.06}          & 62.93                         & 0.65                           \\
						\textbf{\ours}    & \textbf{33.66} & \colorbox{lightpurple!50}{\textbf{39.23}} & \colorbox{lightpurple!50}{\textbf{64.39}} & 83.63          & 88.24          & 41.89          & 91.30           & \colorbox{lightpurple!50}{\textbf{52.57}} & \colorbox{lightpurple!50}{\textbf{68.21}} & \textbf{89.60}  & \textbf{49.77} & \colorbox{lightpurple!50}{\textbf{63.94}} & \colorbox{lightpurple!50}{\textbf{64.89}}                & \textbf{1.78}                  \\
						\bottomrule
					\end{tabular}
				\end{small}
			\end{center}
			\vskip -0.1in
		\end{table*}
		
		
		\textit{\textbf{(i) The unlearned model tends to remember
				target facts in their middle layers during inference. }} 
		Although the unlearned model demonstrates some forgetting, the correct answer token can still re-emerge in the middle layers. To evaluate this phenomenon, we analyze the ranking of the first answer token within the next-token probability distribution across different layers. As illustrated in Fig.~\ref{layer}, the correct answer re-emerges prominently in the middle layers. Since different types of knowledge are stored in distinct modules~\cite{ROME}, the specific layer where the correct answer re-emerges varies accordingly. However, in the final layers, the correct answer consistently ranks highly, indicating that the unlearned model still assigns a significant probability to it, highlighting its difficulty in effectively erasing the knowledge embedded within the middle layers.
		
		\textit{\textbf{(ii) The unlearned model are still capable of
				recalling paraphrased answers during inference.}} Answers can be expressed in various forms, but existing methods~\cite{grad_ascent}, typically focus on training the model to forget only a specific type of answer, neglecting other rephrased versions.  
		We investigate the likelihood of an unlearned model generating paraphrased answers. Experiments are conducted on the TOFU Forget01 dataset using \texttt{LLaMA2-7B}, where we report the average probability of generating paraphrased answers for unlearning samples ($P_u$), and rephrased unlearning samples ($P_r$). As shown in Tab.~\ref{cause2}, the unlearned model assigns up to 17.48\% probability to rephrased answers when tested on unlearning samples. Furthermore, the probabilities of paraphrased answers on rephrased unlearning samples tend to be assigned even higher values. These results suggest that unlearned models continue to recall paraphrased answers, increasing the potential for reproducing ground truth and posing challenges for generalisation during inference.
		
		
		\begin{wraptable}{r}{0.22\textwidth}
			% \vspace{-8mm}
			\hspace{-12pt}
			\centering
			\begin{threeparttable}
				\caption{The average probability of the model generating a rephrased answer on TOFU Forget01 using \ttsmall{LLaMA2-7B}.}
				\label{cause2}
				\setlength{\tabcolsep}{2.4pt}
				\small
				\begin{tabular}{c|ccc}
					\toprule
					\multicolumn{1}{c|}{} & $P_u$↓ & $P_r$↓ & $\Delta$↓ \\
					\midrule
					\textbf{GA} & 9.45 & 10.84 & 1.38 \\
					\textbf{DPO}& 17.48 & 17.91 & 0.42 \\
					\textbf{NPO} & 10.74 & 11.98 & 1.24 \\
					\textbf{TV} & 13.51 & 14.66 & 1.15 \\
					\textbf{WHP} & 11.46 & 12.60 & 1.14 \\
					\textbf{ULD} & 9.89 & 10.31 & 0.42 \\
					\textbf{\ours} & \textbf{9.06} & \textbf{9.34} & \textbf{0.28}  \\
					\bottomrule
				\end{tabular}
				
				
			\end{threeparttable}
			\vspace{-5mm}
		\end{wraptable}
		Nevertheless, identifying the problem does not simplify its resolution. Addressing this dilemma still presents significant challenges. On the one hand, constructing all possible paraphrased versions of unlearning samples and their answers is labor-intensive and impractical. On the other hand, the knowledge stored in LLMs is intricate and highly entangled~\cite{ROME}, making it challenging to clearly delineate the unlearning scope of knowledge that should be retained versus the knowledge that must be forgotten~\cite{Unlearning-Prompts}.
		
		
		\begin{figure*}[t]
			\centering
			\includegraphics[width=0.98\textwidth]{eigen.pdf}
			\vspace{-3.5mm}
			\caption{Visualization of the \metric~values for each token across all layers using \ttsmall{Phi-1.3B}. Subject words have brighter colors and exhibit higher \metric, indicating the model's greater sensitivity to them. }
			\label{eigen}
			\vskip -0.1in
		\end{figure*}
		
		
		\begin{figure*}[t]
			\centering
			\includegraphics[width=0.98\textwidth]{method.pdf}
			% \vspace{-3.5mm}
			\caption{Depiction of \ours. \textit{Left:} The clean run involves inputting the original unlearning sample into the model, enabling it to successfully recall the facts and generate a \textit{fact-related} probability distribution. \textit{Right:} The corrupted run refers to inputting a perturbed unlearning sample into the model, making it fail to recall the facts and produce a \textit{fact-unrelated} probability distribution, where the ground truth ranks significantly lower in the distribution.
			}
			\label{method}
			% \vskip -0.15in
		\end{figure*}
		
		
		\begin{table*}[]
			% \vspace{-1mm}
			\caption{Experimental results on the Rephrased TOFU dataset using \ttsmall{LLaMA2-7B}. Notably, the baseline methods struggle to generalise to rephrased unlearning samples. In contrast, \ours~outperforms the baselines by up to 43.53\% in Probability and 20.81\% in Truth Ratio.}
			\vspace{-2mm}
			\label{TOFU_rephrase}
			\setlength{\tabcolsep}{3pt}
			\vskip 0.15in
			% \renewcommand{\arraystretch}{1.1}
			\begin{center}
				\begin{small}
					\begin{tabular}{c|ccccc|ccccc|ccccc}
						\toprule
						\textbf{Dataset} & \multicolumn{5}{c|}{\textbf{Rephrased Forget01 Dataset}} & \multicolumn{5}{c|}{\textbf{Rephrased Forget05 Dataset}} & \multicolumn{5}{c}{\textbf{Rephrased Forget10 Dataset}}  \\ 
						\textbf{Metric} & \textbf{RG↓}   & \textbf{Pr↓}   & \textbf{TR↑}   & \textbf{MU↑}   & \textbf{FRT↑} & \textbf{RG↓}   & \textbf{Pr↓}   & \textbf{TR↑}   & \textbf{MU↑}   & \textbf{FRT↑} & \textbf{RG↓}   & \textbf{Pr↓}   & \textbf{TR↑}   & \textbf{MU↑}   & \textbf{FRT↑} \\
						\midrule
						\textbf{Retain}                   & 37.18          & 15.48          & 65.64          & 61.11          & 2.32          & 35.78          & 12.29          & 63.87          & 59.30          & 2.47          & 34.48          & 12.10          & 64.21          & 58.87          & 2.53          \\ 
						\midrule
						\textbf{ICL}                      & 44.34          & 68.98          & 55.19          & 62.26          & 1.10          & 44.13          & 64.23          & 52.29          & 62.26          & 1.15          & 43.48          & 64.53          & 52.84          & 62.26          & 1.15          \\
						\textbf{GA}                       & 43.38          & \colorbox{lightblue!50}{29.07}          & 57.55          & 60.41          & 1.67          & 46.04          & 60.21          & 50.65          & 61.18          & 1.15          & 44.32          & 59.26          & 52.40          & 60.87          & 1.18          \\
						\textbf{GA+GDR}                   & 46.59          & 62.50          & 51.96          & 61.42          & 1.13          & 46.14          & 59.56          & 50.00          & 60.97          & 1.15          & 44.82          & 57.11          & 50.79          & 60.70          & 1.19          \\
						\textbf{GA+KLR}                   & 46.37          & 62.06          & 52.85          & 61.94          & 1.14          & 48.21          & 64.08          & 50.73          & 62.15          & 1.11          & 47.81          & 64.14          & 51.24          & 61.96          & 1.11          \\
						\textbf{DPO}                      & 27.48          & 60.45          & \colorbox{lightblue!50}{60.75}          & 63.42          & 1.44          & 41.56          & 69.28          & 55.13          & 62.39          & 1.13          & \textbf{30.50} & 68.51          & \colorbox{lightblue!50}{55.77}          & 60.32          & 1.22          \\
						\textbf{DPO+GDR}                  & 29.39          & 64.84          & 59.46          & 63.03          & 1.34          & \textbf{27.76} & 65.40          & \colorbox{lightblue!50}{56.50}          & 61.05          & 1.31          & 36.73          & 66.37          & 53.90          & 60.23          & 1.17          \\
						\textbf{DPO+KLR}                  & 30.99          & 66.45          & 59.60          & 63.58          & 1.31          & 45.61          & 70.61          & 54.02          & \textbf{63.10} & 1.09          & 40.57          & 70.70          & 55.42          & 61.45          & 1.10          \\
						\textbf{NPO}                      & 44.08          & 30.88          & 57.51          & 60.57          & 1.62          & 47.34          & 61.48          & 50.88          & 61.57          & 1.13          & 44.31          & 60.68          & 52.98          & 61.52          & 1.17          \\
						\textbf{NPO+GDR}                  & 43.74          & 30.77          & 57.48          & 60.47          & 1.62          & 46.87          & 61.53          & 50.86          & 61.60          & 1.14          & 44.46          & 60.78          & 52.89          & 61.63          & 1.17          \\
						\textbf{NPO+KLR}                  & 44.84          & 30.95          & 57.63          & 60.49          & 1.60          & 47.17          & 61.84          & 50.86          & 61.74          & 1.13          & 44.58          & 60.73          & 52.95          & 61.65          & 1.17          \\
						\textbf{TV}                       & 42.22          & 45.17          & 56.04          & 61.68          & 1.41          & 45.43          & 59.76          & 52.18          & 61.92          & 1.18          & 39.39          & 50.03          & 53.62          & 60.18          & 1.35          \\
						\textbf{WHP}                      & 49.98          & 53.61          & 52.15          & 61.83          & 1.19          & 50.59          & \colorbox{lightblue!50}{53.37}          & 50.62          & 61.03          & 1.17          & 48.21          & 57.19          & 51.20          & 60.96          & 1.16          \\
						\textbf{ULD}                      & 29.76          & 45.89          & 59.90          & 58.95          & 1.56          & 49.66          & 63.98          & 50.82          & 62.93          & 1.11          & 30.63          & 41.18          & 52.29          & 63.03          & 1.76          \\
						\midrule
						\textbf{\ours} & \textbf{26.69}          & \colorbox{lightpurple!50}{\textbf{14.75}}          & \colorbox{lightpurple!50}{\textbf{71.72}} & \textbf{65.06} & \textbf{3.14}          & 28.96        & \colorbox{lightpurple!50}{\textbf{30.14}}          & \colorbox{lightpurple!50}{\textbf{68.26}}          & 61.21          & \textbf{2.07}          & 32.72          & \textbf{37.99} & \colorbox{lightpurple!50}{\textbf{65.96}}          & \textbf{64.80} & \textbf{1.83} \\
						\bottomrule
					\end{tabular}
				\end{small}
			\end{center}
			\vskip -0.1in
		\end{table*}
		
		
		\section{Proposed Approach: \ours}
		To improve the generalisation of unlearned models, we propose a simple perturbation-based machine unlearning method, termed \ours, which achieves unlearning by simulating adversarial unlearning samples. To identify which tokens to perturb, in this section, we first introduce a Model Sensitivity Metric, \metric, followed by a detailed explanation of the unlearning pipeline for \ours.
		
		\subsection{Model Sensitivity Metric}
		Generative models are highly sensitive to subtle changes in their input~\cite{Models_Sensitivity,SSS}. Building on this insight, we aim to perturb the most vulnerable tokens in the unlearning sample to emulate an adversarial attack, making the model behave as though it was never trained on it. To achieve this, we introduce a novel metric to quantify the model’s sensitivity to specific tokens, which is described as follows.
		
		Let the parameters of the target model $\mathrm{LM}$ be $\mathbf{W} \in \mathbb{R}^{n \times n}$, where $n$ represents the hidden dimension of $\mathrm{LM}$. Given an unlearning sample $x$ with a sequence length of $m$, the $i$-th token is represented as $x_i,i\in\{1, 2, \dots, m\}$, and the perturbation applied to this token is denoted by $\Delta_i$. Then the perturbed token can be represented as $\hat{x_i} = x_i + \Delta_i$. The change in the model's output due to $\Delta_i$ is measured in terms of the loss $\mathcal{J}(x_i, \hat{x_i})$. The relationship between $\Delta_i$ and $\mathcal{J}(x_i, \hat{x_i})$ is not strictly linear. If the model is resilient to certain tokens, even a larger $\Delta_i$ results in a relatively small change in $\mathcal{J}(x_i, \hat{x_i})$. Conversely, if the model is sensitive to specific tokens, a small $\Delta_i$ can cause a significant difference in $\mathcal{J}(x_i, \hat{x_i})$. 
		
		To quantify the extent of perturbation a model can tolerate, we draw inspiration from Zhao et al.~\cite{FIM}, who utilize the Fisher Information Matrix (FIM) as a metric tensor to characterize the robustness of deep learning models. Building on this idea, we define a novel FIM-variant matrix, $\mathbf{H} \in \mathbb{R}^{n \times n}$, to evaluate the vulnerability of $\mathrm{LM}$ to perturbations in its feature space, with $\nabla_{x} \mathcal{J}(x, \hat{x})$ representing  partial derivative of $\mathcal{J}(x, \hat{x})$ with respect to $x$:
		\begin{equation}
			\mathbf{H(x)} = \nabla_{x} \mathcal{J}(x, \hat{x})^{\top} \nabla_{x} \mathcal{J}(x, \hat{x}) .
			\label{H}
		\end{equation}
		
		
		\begin{proposition} 
			Fix $\Delta_i$, $\mathcal{J}(x_i, \hat{x}_i) \propto \lambda_i$, where $\lambda_i$ is the maximum eigenvalue of $\mathbf{H}(x_i)$, $i\in\{1, 2, \dots, m\}$.
			\label{proposition_eigen}
		\end{proposition}
		
		When the perturbation is fixed, a larger $\lambda_i$ indicates a greater impact on the loss, implying that the model is more sensitive to token $x_i$. Consequently, model sensitivity can be quantified using an easily computable indicator, $\lambda$.
		We define $\lambda$ as the Model Sensitivity Metric (\metric). A higher \metric~value indicates greater sensitivity of the model to the token.
		
		
		
		To identify the model's most sensitive token, we conducted experiments to compute the \metric~value of each token across all datasets using \ttsmall{Phi-1.3B}. As shown in Tab.~\ref{eigen_table}, the mean \metric~value of the subject words is up to 2.66 times higher than that of other words. We further visualize this by normalizing the \metric~values of all tokens in each sentence to a range between 0 and 1. As depicted in Fig.~\ref{eigen}, the subject words exhibit higher intensity values in the middle layers, indicating that the model is more sensitive to them. This observation aligns with the functionality of the mid-layer modules in generative language models, which primarily recalls facts related to the subject words~\cite{ROME,Recall_of_Factual}, subsequently influencing the output to reflect memorized properties. Consequently, the model demonstrates a higher sensitivity to subject words.
		
		\begin{table}[]
			\vspace{-2mm}
			\caption{Comparison between the average \metric~values of subject words and the other words. Subject words exhibit higher \metric~values than other tokens, indicating greater sensitivity.}
			\label{eigen_table}
			\vskip 0.1in
			\begin{center}
				\begin{sc}
					\begin{small}
						\begin{tabular}{c|ccc}
							\toprule
							\textbf{Dataset}   & \textbf{TOFU} & \textbf{Harry} & \textbf{ZsRE} \\ 
							\midrule
							\textbf{Subject Words} & 0.000561      & 0.003219       & 0.005961      \\ 
							\textbf{Other Words}   & 0.000219      & 0.001708       & 0.003082      \\ 
							\textbf{Ratio}         & 2.66          & 1.86           & 1.93          \\ \bottomrule
						\end{tabular}
					\end{small}
				\end{sc}
			\end{center}
			\vskip -0.2in
		\end{table}
		
		
		\subsection{Perturbed Distribution Matching}
		Building on the observation above, we propose \ours~to achieve unlearning.
		Specifically, let $x=(x_1,..x_{sub_1}..x_{sub_k}..,x_m)$ be an unlearning sample consisting of $m$ tokens. There exist $k$ \textit{subject} words in the sentence. To prevent the model from recalling the subject-related facts and to treat the unlearning sample as adversarial, we inject random noise into the subject words in the embedding space prior to inputting them into the model, resulting in the perturbed unlearning sample $x'=(x_1,..x'_{sub_1}..x'_{sub_k}..,x_m)$ where $x'_{sub_i}$ represents the perturbed subject tokens. 
		
		We first input the unlearning sample $x$ into the unlearning model $f_{\theta_{u}}$ to generate the clean-run next-token probability distribution $p(y|x)$, where $y = f_{\theta_{u}}(y|x)$ represents the model's output. Similarly, the corrupted-run next-token probability distribution $p(y|x')$ is obtained by inputting the perturbed sample $x'$ into $f_{\theta_{u}}$. 
		
		
		\begin{table*}[t]
			\caption{Experimental results on the Rephrased Harry Potter dataset. \ours~demonstrates effective generalisation to rephrased forget data, achieving relative improvements of up to 17.18\% (84.39$\rightarrow$69.89) in Rephrased Forget ROUGE and 19.87\% (81.73$\rightarrow$65.49) in Rephrased Forget F1 when tested on \ttsmall{LLaMA2-7B}. }
			\vspace{-2.5mm}
			\label{Harry_rephrase}
			\setlength{\tabcolsep}{2.3pt}
			\begin{center}
				\begin{small}
					\begin{tabular}{c|ccc|ccc|cc||ccc|ccc|cc}
						\toprule
						\textbf{Model}                    & \multicolumn{8}{c||}{\textbf{Phi-1.3B}} & \multicolumn{8}{c}{\textbf{LLaMA2-7B}}   \\
						\midrule
						\textbf{Dataset}              & \multicolumn{3}{c|}{\textbf{Forget}}              & \multicolumn{3}{c|}{\textbf{Rephrased Forget}}    & \multirow{2}{*}{\textbf{MU↑}} & \multirow{2}{*}{\textbf{FRT↑}} & \multicolumn{3}{c|}{\textbf{Forget}}              & \multicolumn{3}{c|}{\textbf{Rephrased Forget}}    & \multirow{2}{*}{\textbf{MU↑}} & \multirow{2}{*}{\textbf{FRT↑}} \\
						\textbf{Metric}               & \textbf{RG↓}   & \textbf{Pr↓}   & \textbf{F1↓}   & \textbf{RG↓}   & \textbf{Pr↓}   & \textbf{F1↓}   &                               &                                & \textbf{RG↓}   & \textbf{Pr↓}   & \textbf{F1↓}   & \textbf{RG↓}   & \textbf{Pr↓}   & \textbf{F1↓}   &                               &                                \\
						\midrule
						\textbf{Retain}               & 44.61          & 14.34          & 44.49          & 43.55          & 14.10          & 43.74          & 62.73                         & 1.84                           & 43.84          & 19.43          & 39.89          & 41.11          & 19.58          & 36.40          & 83.99                         & 2.52                           \\
						\midrule
						\textbf{ICL}                  & 71.15          & 45.18          & 68.35          & 62.67          & 36.06          & 61.96          & 61.25                         & 1.06                           & 100.00         & 99.68          & 100.00         & 98.17          & 94.81          & 96.93          & \textbf{88.19}                & 0.90                           \\
						\textbf{GA}                   & 77.08          & 49.93          & 73.68          & 66.92          & 40.35          & 64.73          & 64.70                         & 1.04                           & 93.53          & 68.72          & 91.73          & 93.10          & 66.10          & 90.81          & 82.45                         & 0.98                           \\
						\textbf{GA+GDR}               & 73.64          & 45.27          & 69.79          & 64.15          & 37.26          & 61.42          & \textbf{65.04}                & 1.11                           & 95.20          & 74.69          & 93.21          & 92.30          & 71.96          & 90.29          & 86.22                         & 1.00                           \\
						\textbf{GA+KLR}               & 69.02          & 38.19          & 64.89          & 59.76          & 32.27          & 57.39          & 63.73                         & 1.19                           & 93.53          & 71.75          & 91.73          & 93.10          & 68.54          & 90.81          & 83.13                         & 0.98                           \\
						\textbf{DPO}                  & 75.86          & 48.66          & 73.62          & 68.75          & 38.74          & 67.82          & 62.76                         & 1.01                           & 92.21          & 76.67          & 89.85          & 89.77          & 73.54          & 86.71          & 83.02                         & 0.98                           \\
						\textbf{DPO+GDR}              & 80.23          & 58.19          & 77.46          & 73.19          & 45.15          & 71.03          & 64.82                         & 0.96                           & 91.21          & 81.61          & 88.38          & 88.87          & 77.98          & 84.55          & 82.27                         & 0.96                           \\
						\textbf{DPO+KLR}              & 75.93          & 49.68          & 73.95          & 70.09          & 39.44          & 69.15          & 63.21                         & 1.00                           & 91.38          & 68.50          & 89.09          & 87.93          & 65.78          & 83.65          & 81.34                         & 1.00                           \\
						\textbf{NPO}                  & 68.69          & 38.42          & 64.56          & 59.93          & 32.44          & 57.39          & 63.97                         & 1.19                           & 92.53          & \colorbox{lightblue!50}{65.05}          & 90.01          & 91.70          & \colorbox{lightblue!50}{63.06}          & 88.99          & 81.72                         & 1.00                           \\
						\textbf{NPO+GDR}              & 69.11          & 38.62          & 64.94          & 60.55          & 32.53          & 58.18          & 64.32                         & 1.19                           & 93.53          & 67.56          & 91.01          & 91.70          & 65.44          & 88.99          & 82.63                         & 1.00                           \\
						\textbf{NPO+KLR}              & 68.69          & 38.39          & 64.56          & 60.21          & 32.35          & 57.70          & 63.89                         & 1.19                           & 92.53          & 65.18          & 90.01          & 91.70          & 63.19          & 88.99          & 81.76                         & 1.00                           \\
						\textbf{TV}                   & 79.58          & 55.78          & 76.27          & 70.82          & 44.30          & 68.06          & 64.87                         & 0.99                           & 92.47          & 69.56          & 89.94          & 91.97          & 66.19          & 89.62          & 82.42                         & 0.99                           \\
						\textbf{WHP}                  & 71.72          & 40.94          & 66.87          & 62.31          & 31.38          & 60.06          & 64.52                         & 1.16                           & \colorbox{lightblue!50}{87.35}          & 73.20          & \colorbox{lightblue!50}{84.90}          & \colorbox{lightblue!50}{84.39}          & 70.59          & \colorbox{lightblue!50}{81.73}          & 86.88                         & 1.08                           \\
						\textbf{ULD}                  & 88.35          & 71.79          & 85.84          & 75.04          & 50.50          & 72.84          & 61.18                         & 0.83                           & 89.75          & 73.24          & 85.31          & 88.83          & 69.84          & 82.65          & 81.65                         & 1.00                           \\
						\midrule
						\textbf{\ours} & \textbf{65.21} & \textbf{35.75} & \textbf{63.02} & \textbf{59.11} & \textbf{31.24} & \textbf{56.02} & 63.20                         & \textbf{1.22}                  & \colorbox{lightpurple!50}{\textbf{71.88}} & \colorbox{lightpurple!50}{\textbf{56.36}} & \colorbox{lightpurple!50}{\textbf{69.54}} & \colorbox{lightpurple!50}{\textbf{69.89}} & \colorbox{lightpurple!50}{\textbf{56.59}} & \colorbox{lightpurple!50}{\textbf{65.49}} & 82.65                         & \textbf{1.27}                               
						\\
						\bottomrule
					\end{tabular}
				\end{small}
			\end{center}
			% \vskip -0.2in
		\end{table*}
		
		
		As illustrated in Fig.~\ref{method}, in the clean-run probability distribution $p(y|x)$, the top-ranked tokens with the highest probabilities are informative and \textit{fact-related}. In contrast, for the corrupted-run probability distribution $p(y|x')$, the model fails to recognize the subjects or recall related facts, resulting in token generation that relies primarily on context or grammar, with top-ranked tokens being \textit{fact-unrelated}. Consequently, the corrupted-run probability distribution intuitively simulates a natural unlearning effect, allowing the model to behave as if it had never been trained on the given example. To replicate such a natural unlearning environment, we subtract $p(y|x)$ from $p(y|x')$, using a tuning coefficient $C$ to control the strength of forgetting: 
		\begin{equation}
			p(Y_t|y_{<t}) = p(y|x') - C \cdot p(y|x),
		\end{equation}
		Consequently, the probability of the \textit{fact-related} tokens are significantly reduced, while the \textit{fact-unrelated} tokens remain highly ranked, preserving the distribution of irrelevant tokens and thus maintaining the model's utility.
		
		We then achieve unlearning by fine-tuning $f_{\theta_{u}}$ to match the new, subtracted logit probability distribution $p(Y_t|y_{<t})$. For autoregressive text generation, this is decomposed into a step-wise KL divergence~\cite{f-Divergence}:
		\begin{align}
			L & = 
			- \sum_{i=1}^{t} \sum_{Y_i \in V} p(Y_i|y_{<i}) log q_\theta (Y_i|y_{<i}),
		\end{align}
		where $V$ is the vocabulary and $q_\theta$ represents the predicted distributions of the unlearn model $f_{\theta_{u}}$. We provide a more comprehensive explanation of the algorithmic procedure presented in Alg.~\ref{alg:unlearning}.
		
		\begin{algorithm}[H]
			\caption{\ours: Perturbation-based Unlearning}
			\label{alg:unlearning}
			\begin{algorithmic}[1]
				\renewcommand{\algorithmicrequire}{\textbf{Input:}}
				\renewcommand{\algorithmicensure}{\textbf{Output:}}
				\REQUIRE Unlearning sample $x = (x_1, x_2, \dots, x_m)$ with $m$ tokens, target model $f_{\theta_{u}}$, tuning coefficient $C$
				\ENSURE Fine-tuned unlearning model $f_{\theta_{u}}$
				
				\STATE Identify $k$ subject tokens $\{x_{sub_1}, \dots, x_{sub_k}\}$ in $x$.
				\STATE Introduce random noise to the subject tokens to obtain the perturbed sample: $x' = (x_1, \dots, x'_{sub_1}, \dots, x'_{sub_k}, \dots, x_m)$.
				\STATE Compute the clean-run next-token probability distribution:
				$p(y|x) = f_{\theta_{u}}(y|x).$
				\STATE Compute the corrupted-run next-token probability distribution:
				$
				p(y|x') = f_{\theta_{u}}(y|x').
				$
				\STATE Subtract the clean-run distribution from the corrupted-run distribution to emulate forgetting:\\
				$
				p(Y_t|y_{<t}) = p(y|x') - C \cdot p(y|x).
				$
				\STATE Fine-tune $f_{\theta_{u}}$ to match $p(Y_t|y_{<t})$ by minimising the step-wise KL divergence:\\
				$
				L = - \sum_{i=1}^{t} \sum_{Y_i \in V} p(Y_i|y_{<i}) \log q_\theta(Y_i|y_{<i}),
				$
				where $V$ is the vocabulary and $q_\theta$ represents the predicted distributions of $f_{\theta_{u}}$.
				\STATE Update $f_{\theta_{u}}$ using gradient descent to minimise $L$.
				\renewcommand{\algorithmicensure}{\textbf{Return:}}
				\ENSURE Fine-tuned unlearning model $f_{\theta_{u}}$.
			\end{algorithmic}
		\end{algorithm}
		
		In general, the advantage of \ours~lies in its generality and simplicity. 
		Regarding \textbf{generality}, compared to previous training-based methods~\cite{grad_ascent,QUARK}, \ours~adopts a different approach by achieving unlearning through logit probability manipulation. On one hand, the corrupted subject token embeddings in the first layer prevent the model from recalling any facts related to the unlearning sample across subsequent middle layers. On the other hand, subtracting the clean-run probability distribution causes the answer and answer-related tokens to drop significantly in the probability distribution. Consequently, the unlearned model fails to generate rephrased answers during inference. Moreover, \ours~preserves the distribution of irrelevant tokens, minimizing side effects and maintaining the model's utility~\cite{Hurt}. 
		In terms of \textbf{simplicity}, \ours~requires no additional training of a reinforced model~\cite{RKLD,ULD} or scope classifier~\cite{Unlearning-Prompts}, making the training process more efficient.
		
		
		\section{Evaluation on \bench}
		In this section, we evaluate \ours~on \bench~and analyze the results. The implementation parameters of \ours~are consistent with those of the baselines. We integrate GDR on retain set with $RW=1$. The noise ratio $P$ is set to 0.4, and we maintain $C=0.1$.
		
		
		\begin{figure}[t]
			\centering
			\includegraphics[width=0.48\textwidth]{curve.pdf}
			\caption{The curves illustrating how Model Utility changes with the Forget Ratio. The closer a method is to the upper left corner, the better it balances model utility and the forgetting effect. The proposed \ours~encompasses nearly all baseline methods from the top left, demonstrating superior unlearning performance.
			}
			\label{curve}
		\end{figure}
		
		
		\subsection{Improved Unlearning Capabilities.} 
		When tested on TOFU Forget05 dataset using \ttsmall{LLaMA2-7B}, as shown in Tab.~\ref{tofu_llama_full}, \ours~demonstrates remarkable effectiveness in unlearning knowledge. Specifically, \ours~outperforms the baselines with a relative improvement of up to \textbf{50.13\%} (78.67 $\rightarrow$39.23) in Forget Probability and 14.43\% (56.27$\rightarrow$64.39) on Forget Truth Ratio. 
		
		When tested on the Harry Potter dataset, as shown in Tab.~\ref{Harry_rephrase}, \ours~consistently outperforms baselines on both \ttsmall{Phi-1.3B} and \ttsmall{LLaMA2-7B}. Specifically, \ours~achieves an absolute improvement of up to 15.47\% (87.35$\rightarrow$71.88) in Forget ROUGE, 8.69\% (65.05$\rightarrow$56.36) in Forget Probability and 15.36\% (84.90$\rightarrow$69.54) in Forget F1 when evaluated on \ttsmall{LLaMA2-7B}, all while maintaining high model utility. Furthermore, the performance improvements of \ours~are more pronounced on \ttsmall{LLaMA2-7B} compared to \ttsmall{Phi-1.3B}, highlighting its resilience to model scaling and its potential for application to larger models.
		
		This significant reduction in Forget Probability can be attributed to \ours's ability to achieve unlearning at the token probability distribution level, effectively lowering the probabilities of the correct answer and its related tokens. Specifically, we plot the curve in Fig.~\ref{curve} based on the TOFU dataset, illustrating how model utility changes with the Forget Ratio, calculated as the mean of Forget ROUGE and Forget Probability. The closer a method is to the upper-left corner, the better it balances model utility and the unlearning effect. \ours~encompasses nearly all baseline methods from the top left, demonstrating superior unlearning performance. 
		
		
		\begin{table*}[t]
			\caption{Experimental results for inverted relation data, subject-replaced data, and one-hop reasoned data on the ZsRE dataset using \ttsmall{LLaMA2-7B}. Most methods struggle to forget the “hard” in-scope knowledge that are logically related to the original unlearning sample. Nevertheless, \ours~achieves an improvement of up to 8.24\% on FRT ratio (0.85$\rightarrow$0.92). }
			\label{zsre_rephrase}
			\vspace{-1mm}
			\setlength{\tabcolsep}{3.2pt}
			\vskip 0.13in
			\begin{center}
				\begin{small}
					\begin{tabular}{c|ccccc|ccccc|ccccc}
						\toprule
						\multicolumn{1}{c|}{\textbf{Dataset} }                 & \multicolumn{5}{c|}{\textbf{Inversed Relation}}                                     & \multicolumn{5}{c|}{\textbf{Subject Replacement}}                                   & \multicolumn{5}{c}{\textbf{One-Hop Reasoning}}                                     \\
						\multicolumn{1}{c|}{\textbf{Metric}}                   & \textbf{RG↓}   & \textbf{Pr↓}   & \textbf{F1↓}   & \textbf{MU↑}   & \textbf{FRT↑}  & \textbf{RG↓}   & \textbf{Pr↓}   & \textbf{F1↓}   & \textbf{MU↑}   & \textbf{FRT↑}  & \textbf{RG↓}   & \textbf{Pr↓}   & \textbf{F1↓}   & \textbf{MU↑}   & \textbf{FRT↑}  \\
						\midrule
						\textbf{Retain}                   & 50.43          & 17.04          & 50.08          & 70.04          & 1.79          & 50.43          & 17.04          & 50.08          & 70.04          & 1.79          & 49.09          & 19.10          & 49.01          & 70.89          & 1.81          \\
						\midrule
						\textbf{ICL}                      & 87.23          & 77.92          & 84.86          & 59.30          & 0.71          & \textbf{64.84} & \textbf{48.81} & \textbf{63.93} & 57.06          & \colorbox{lightpurple!50}{\textbf{0.96}} & \textbf{81.70} & 68.27          & \textbf{80.46} & 59.80          & \colorbox{lightblue!50}{0.78}          \\
						\textbf{GA}                       & 84.46          & 69.92          & 83.47          & 64.36          & 0.81          & 92.35          & 83.25          & 92.40          & 68.10          & 0.76          & 98.76          & 91.09          & 98.28          & 66.11          & 0.69          \\
						\textbf{GA+GDR}                   & 89.39          & 76.81          & 88.98          & 67.32          & 0.79          & 93.26          & 86.22          & 93.31          & 69.11          & 0.76          & 91.87          & 80.17          & 91.06          & 62.01          & 0.71          \\
						\textbf{GA+KLR}                   & 85.33          & 69.88          & 84.61          & 64.30          & 0.80          & 91.67          & 83.11          & 91.72          & 68.11          & 0.77          & 98.85          & 91.14          & 98.38          & 66.01          & 0.69          \\
						\textbf{DPO}                      & 82.54          & 63.80          & \colorbox{lightblue!50}{81.28}          & 62.13          & 0.82          & 96.46          & 84.94          & 96.51          & 68.75          & 0.74          & 98.58          & 92.52          & 98.14          & 68.54          & 0.71          \\
						\textbf{DPO+GDR}                  & \colorbox{lightblue!50}{81.76}          & 66.19          & 81.46          & 63.09          & 0.83          & 97.15          & 87.25          & 97.20          & \textbf{69.35} & 0.74          & 97.62          & 91.13          & 97.14          & 68.82          & 0.72          \\
						\textbf{DPO+KLR}                  & 88.31          & 69.66          & 87.54          & 64.68          & 0.79          & 96.46          & 85.69          & 96.51          & 68.85          & 0.74          & 98.77          & 92.67          & 98.34          & 68.28          & 0.71          \\
						\textbf{NPO}                      & 85.28          & 68.92          & 84.34          & 64.06          & 0.81          & 92.12          & 82.67          & 92.17          & 68.02          & 0.76          & 99.25          & 92.88          & 98.83          & 67.35          & 0.69          \\
						\textbf{NPO+GDR}                  & 86.66          & 70.32          & 85.50          & 64.79          & 0.80          & 92.35          & 83.83          & 92.40          & 68.58          & 0.77          & 99.30          & 93.61          & 98.89          & 67.84          & 0.70          \\
						\textbf{NPO+KLR}                  & 85.80          & 68.89          & 84.86          & 64.09          & 0.80          & 92.12          & 82.88          & 92.17          & 68.01          & 0.76          & 99.25          & 92.94          & 98.83          & 67.37          & 0.69          \\
						\textbf{TV}                       & 96.19          & 83.95          & 95.66          & 68.57          & 0.75          & 85.87          & 79.56          & 84.93          & 67.29          & \colorbox{lightblue!50}{0.81}          & 98.50          & 92.82          & 98.10          & \textbf{69.22} & 0.72          \\
						\textbf{WHP}                      & 96.76          & 86.34          & 96.23          & \textbf{68.62} & 0.74          & 91.44          & 83.58          & 91.55          & 68.91          & 0.78          & 99.36          & 94.82          & 98.95          & 69.15          & 0.71          \\
						\textbf{ULD}                      & 85.62          & \textbf{54.75} & 84.89          & 64.11          & \colorbox{lightblue!50}{0.85}          & 96.69          & 81.04          & 96.74          & 64.84          & 0.71          & 91.23          & \textbf{67.71} & 90.10          & 62.78          & 0.76          \\
						\midrule
						\textbf{\ours} & \colorbox{lightpurple!50}{\textbf{77.00}} & 56.51          & \colorbox{lightpurple!50}{\textbf{74.87}} & 63.76          & \colorbox{lightpurple!50}{\textbf{0.92}} & 88.19          & 79.62          & 87.16          & 68.56          & \colorbox{lightblue!50}{0.81}          & 85.95          & 70.34          & 83.74          & 64.77          & \colorbox{lightpurple!50}{\textbf{0.81}} \\
						\bottomrule
					\end{tabular}
				\end{small}
			\end{center}
			% \vskip -0.15in
		\end{table*}
		
		
		\begin{table*}[!htb]
			\centering
			\footnotesize
			\caption{Although existing methods can effectively forget the original unlearning samples, they fail to generalise to forget rephrased unlearning samples (text in red). In contrast, \ours~demonstrates superior generalisation ability (text in green).}
			% \vskip 0.05in
			\resizebox{\textwidth}{!}{
				\begin{tabular}{l}
					\toprule
					\toprule
					{\textbf{The Underlying Problem: Failure to Generalise to Rephrased Unlearning Samples}} \\  
					\toprule
					\toprule
					\begin{tabular}[c]{@{}l@{}}
						{\textbf{Dataset}: TOFU} \\ 
						{\textbf{Unlearning Sample:}}
						What genre is author Basil Mahfouz Al-Kuwaiti most known for in his writing? \\ 
						{\textbf{Rephrased Unlearning Sample:}}
						For which genre of literature is Basil Mahfouz Al-Kuwaiti best recognized? \\
						\textbf{Ground Truth:} 
						Basil Mahfouz Al-Kuwaiti is most known for his writings in the \colorbox{gray!50}{French literature genre}. \\
						\toprule
						\textbf{Prediction of ULD on the Unlearning Sample: }\\
						The genre that author Basil Mahfouz Al-Kuwaiti is best known for is {\color[rgb]{0,0.5,0} the travelogue genre}. \\
						\textbf{Prediction of ULD on the Rephrased Unlearning Sample: }\\
						Basil Mahfouz Al-Kuwaiti is best known for his contributions to {\color{red} the French literature genre}. \\
						\midrule
						\textbf{Prediction of \ours on the Unlearning Sample: }\\
						Basil Mahfouz Al-Kuwaiti is primarily known for writing {\color[rgb]{0,0.5,0} in the genre of erotica}. \\
						\textbf{Prediction of \ours on the Rephrased Unlearning Sample: }\\
						Basil Mahfouz Al-Kuwaiti is best known for writing books in {\color[rgb]{0,0.5,0} the genre of mythology}.\\
						\bottomrule
						\bottomrule
					\end{tabular}
				\end{tabular}
			}
			\label{rephrase_question}
		\end{table*}
		
		
		
		\subsection{Improved Generalisation Capabilities.}
		
		When tested on the Rephrased TOFU dataset using \ttsmall{LLaMA2-7B} (Tab.~\ref{TOFU_rephrase}),  \ours~outperforms the baselines relatively by up to \textbf{43.53\%} (53.37$\rightarrow$30.14) in Forget Probability, and 20.81\% (56.50$\rightarrow$68.26) in Truth Ratio. At the same time, \ours~preserves knowledge on the Real World dataset better than the baselines when using \ttsmall{LLaMA2-7B}, even surpassing the retain model by up to 10.25\% (39.52$\rightarrow$49.77) on Real World Probability and 11.18\%(52.76$\rightarrow$63.94) on Real World Truth Ratio (Tab.~\ref{tofu_llama_full}). We present a case study of the generalisation issue. As shown in Tab.~\ref{rephrase_question}, the predictions generated by ULD remain highly similar to the ground truth (highlighted in red), reflecting suboptimal generalization. In contrast, \ours~produces entirely different responses from the ground truth, demonstrating superior generalization ability.
		
		Furthermore, \ours~generalises effectively to rephrased Harry Potter data, achieving relative improvements of up to 17.18\% (84.39$\rightarrow$69.89) in ROUGE and 19.87\% (81.73$\rightarrow$65.49) in F1 when tested on \ttsmall{LLaMA2-7B} (Tab.~\ref{Harry_rephrase}), while incurring only a minor 1.34\% (83.99$\rightarrow$82.65) reduction in model utility compared to the retain model. 
		
		
		\begin{table*}[h]
			\caption{The Fluency ratio on TOFU and ZSRE dataset using \ttsmall{LLaMA2-7B}. \ours~achieves the best Fluency ratio across all datasets, indicating better generation quality. The case study can be found in Tab.~\ref{Low-Quality-Generation}.}
			\label{fluency}
			\setlength{\tabcolsep}{2.3pt}
			\begin{center}
				\begin{small}
					\begin{tabular}{c|cccccccccccccc}
						\toprule
						\textbf{Dataset}                  & \multicolumn{14}{c}{\textbf{TOFU - Forget05}} \\
						\textbf{Metric}                   & \textbf{$\mathbf{\mathrm{ICL}}$}  & \textbf{$\mathbf{\mathrm{GA}}$} & \textbf{$\mathbf{\mathrm{GA}_{GD}}$} & \textbf{$\mathbf{\mathrm{GA}_{KL}}$} & \textbf{$\mathbf{\mathrm{DPO}}$} & \textbf{$\mathbf{\mathrm{DPO}_{GD}}$} & \textbf{$\mathbf{\mathrm{DPO}_{KL}}$} & \textbf{$\mathbf{\mathrm{NPO}}$} & $\mathbf{\mathrm{NPO}_{GD}}$ & \textbf{$\mathbf{\mathrm{NPO}_{KL}}$} & \textbf{$\mathbf{\mathrm{TV}}$}   & \textbf{$\mathbf{\mathrm{WHP}}$} & \textbf{$\mathbf{\mathrm{ULD}}$}  & \ours \\ \midrule
						\textbf{Real Authors}    & 3.64          & 3.62        & 3.61            & 3.62            & 3.54         & 3.48             & 3.58             & 3.61         & 3.60             & 3.61             & \colorbox{lightblue!50}{3.70}          & 3.66         & 3.68          & \colorbox{lightpurple!50}{\textbf{4.29}} \\
						\textbf{Real World}      & 3.87          & 3.88        & 3.83            & 3.86            & 3.62         & 3.69             & 3.67             & 3.90         & 3.87             & 3.88             & 3.97          & 3.89         & \colorbox{lightblue!50}{4.05}          & \colorbox{lightpurple!50}{\textbf{4.62}} \\
						\textbf{Retain}          & 4.64          & 4.62        & 4.63            & 4.63            & 4.26         & 4.53             & 4.40             & 4.63         & 4.63             & 4.63             & 4.65          & 4.64         & 4.65          & \textbf{4.79} \\
						\textbf{Forget}          & 4.58          & 4.67        & 4.68            & 4.68            & 4.26         & 3.37             & 4.56             & 4.67         & 4.67             & 4.66             & \textbf{4.72} & 4.72         & 4.72          & 4.72          \\
						\textbf{Rephrased Forget} & 4.67          & 4.75        & 4.69            & 4.76            & 4.27         & 3.57             & 4.43             & 4.77         & 4.74             & 4.77             & 4.81          & 4.76         & 4.78          & \textbf{4.83} \\
						\colorbox{gray!40}{\textbf{Average}}             & 4.28          & 4.31        & 4.29            & 4.31            & 3.99         & 3.73             & 4.13             & 4.32         & 4.30             & 4.31             & 4.37          & 4.33         & 4.37          & \textbf{4.65} \\ 
						\midrule
						\textbf{Dataset}                 & \multicolumn{14}{c}{\textbf{ZsRE - Inversed Relation}} \\ 
						\midrule
						\textbf{Real Authors}    & 0.10          & 0.10        & 0.10            & 0.10            & 0.10         & \textbf{0.23}    & 0.18             & 0.10         & 0.10             & 0.10             & 0.10          & 0.10         & 0.10          & 0.10          \\
						\textbf{Real World}      & 0.16          & 0.16        & 0.15            & 0.15            & 0.24         & \textbf{0.32}    & 0.43             & 0.15         & 0.15             & 0.16             & 0.17          & 0.16         & 0.19          & 0.16          \\
						\textbf{Retain}          & 0.44          & 0.44        & 0.44            & 0.44            & 0.44         & 0.44             & 0.44             & 0.44         & 0.44             & 0.44             & 0.44          & 0.44         & 0.44          & \textbf{0.49} \\
						\textbf{Forget}          & 0.48          & 0.50        & 0.50            & 0.50            & 0.48         & 0.48             & 0.46             & \colorbox{lightblue!50}{0.51}      & \colorbox{lightblue!50}{0.51}      & \colorbox{lightblue!50}{0.51}             & 0.49          & 0.49         & 0.48          & \colorbox{lightpurple!50}{\textbf{1.00}} \\
						\textbf{Rephrased Forget} & 0.48          & \colorbox{lightblue!50}{0.51}        & \colorbox{lightblue!50}{0.51}            & 0.49            & 0.48         & 0.49             & 0.48             & \colorbox{lightblue!50}{0.51}         & \colorbox{lightblue!50}{0.51}             & \colorbox{lightblue!50}{0.51}             & 0.49          & 0.49         & 0.48          & \colorbox{lightpurple!50}{\textbf{0.98}} \\
						\colorbox{gray!40}{\textbf{Average}}             & 0.33          & 0.34        & 0.34            & 0.34            & 0.35         & 0.39             & 0.40             & 0.34         & 0.34             & 0.34             & 0.34          & 0.34         & 0.34          & \textbf{0.55} \\
						\bottomrule
					\end{tabular}
				\end{small}
			\end{center}
			% \vskip -0.2in
		\end{table*}
		
		
		\begin{figure}[t]
			\begin{center}
				\includegraphics[width=0.48\textwidth]{ours_layer.pdf}
				\caption{The ranking of the first answer token in the next-token probability distribution across layers of the unlearned model. \ours~consistently achieves lower ranks across all layers, indicating that the model fails to recall facts.}
				\label{layer append}
			\end{center}
			% \vskip -0.1in
		\end{figure}
		
		
		When evaluated on the ZsRE dataset, as shown in Tab.~\ref{zsre_rephrase}, although the ICL-based method demonstrates superior unlearning performance, it suffers from lower model utility, which limits its applicability in real-world scenarios. Apart from it, \ours~achieves the best FRT ratio across all three subsets, particularly excelling on inverted relation data, where it shows a relative improvement of up to 8.24\% (0.85$\rightarrow$0.92) on FRT ratio. This highlights \ours's ability to strike a better balance between unlearning and retaining knowledge. However, the absolute generalisation performance of all methods in forgetting knowledge logically related to the original unlearning samples remains suboptimal, particularly when it comes to unlearning knowledge that requires one-hop reasoning, showing a gap of up to 2.23 times lower than the Retain model. Enhancing the absolute generalisation ability of the unlearned model is crucial in machine unlearning, leaving room for further exploration by the community. 
		
		
		
		\subsection{Discussions.}
		
		We investigate whether \ours~effectively addresses the two underlying problems highlighted in Section~\ref{Benchmark Analysis}. For the first problem, as shown in Fig.~\ref{layer append}, the ranking of the first answer token in the next-token probability distribution of \ours~is significantly lower than that of other methods across layers, indicating that the unlearned model fails to recall the facts during inference, thereby \textbf{\textit{addressing Cause 1}}. As for the second problem, \ours~produces a lower probability of generating rephrased answers on both unlearning samples and rephrased unlearning samples (as shown in Tab.~\ref{cause2}), with relative reductions of up to 4.1\% and 9.4\%, respectively. Moreover, the probability delta is 33.3\% lower than that of other methods, effectively \textbf{\textit{addressing Cause 2}}. These results confirm that \ours~exhibits a much more generalised unlearning capability.
		
		
		
		\begin{table*}[!htb]
			\centering
			\caption{Case study of the unlearned model's generation quality. The predictions from Gradient Ascent contain repetitive and redundant words (text in red). In contrast, \ours~produces more fluent and readable sentences (text in green).}
			\resizebox{\textwidth}{!}{
				\begin{tabular}{l}
					\toprule
					{\textbf{The Underlying Problem: Low-Quality Generation}} \\   
					\toprule
					\begin{tabular}[c]{@{}l@{}}
						{\textbf{Dataset}: TOFU} \\ 
						{\textbf{Unlearning Sample:}}
						What was Hina Ameen's maiden book?   \\ 
						\textbf{Ground Truth:} 
						Hina Ameen's maiden book was \colorbox{gray!50}{``Manual of Mineralogy"}. \\
						\toprule
						\textbf{Prediction of Gradient Ascent: }\\
						The maiden book of {\color{red} Hina Ameermunger Hina Ameer's maiden book} is {\color{red} `Ameermunger Hina Ameer's maiden book} is `Ameermunger'.  \\
						\midrule
						\textbf{Prediction of \ours: }\\
						The maiden book of Hina was {\color[rgb]{0,0.5,0} `A Touch of Innocence'}. \\
					\end{tabular}                                 \\
					\bottomrule \\ 
				\end{tabular}
			}
			\label{Low-Quality-Generation}
		\end{table*}
		
		
		\section{Comprehensive Study}
		In this section, we conduct a comprehensive study to evaluate the effectiveness of \ours, which includes assessing the quality of the unlearned model's generations. Additionally, we perform ablation studies to explore the impact of the perturbation ratio $P$, the tuning coefficient $C$, multiple subject scenarios, discrete-token-level perturbation, and the adoption of various retain losses.
		
		
		
		\subsection{Fluency Evaluation}
		The impact of machine unlearning on language models is intricate, requiring a thorough and comprehensive evaluation to fully understand its effects. To this end, we perform additional tests to evaluate the generation quality of existing methods. Building on the previous work~\cite{ROME}, we introduce the Fluency metric to measure the fluency of the unlearned model’s output sentences. Fluency is measured by the weighted average of bi- and tri-gram entropies~\cite{entropies}, defined as $-\sum_k f(k)\log_2 f(k)$, where $f(\cdot)$ represents the $n$-gram frequency distribution. A higher Fluency score indicates more informative and diverse text generation. Experiments are conducted on \ttsmall{LLaMA2-7B} across all subsets. 
		
		As shown in Tab.~\ref{fluency}, \ours~achieves a higher Fluency ratio on most datasets. Notably, when tested on the ZsRE dataset, \ours~outperforms others by nearly twofold on both the Forget dataset and the Rephrased Forget dataset. Moreover, \ours~achieves the highest average Fluency score across all datasets, with a relative improvement of up to 37.5\% (0.40$\rightarrow$0.55). We present a case study in Tab.~\ref{Low-Quality-Generation}, where Gradient Ascent generates low-quality outputs with repetitive content. In contrast, \ours~produces more fluent and readable sentences.
		
		We hypothesize that this improvement arises because \ours~removes some noise—such as punctuation marks, delimiters, newlines, and other inconsequential tokens—from the next-token probability distribution through the probability subtraction process. As a result, the probability distribution becomes more refined, enhancing the model's generation quality. We present additional case studies of the unlearned model's outputs in the following section.
		
		\begin{table*}[]
			\caption{The impact of the perturbation ratio $P$. Experiments are conducted on the TOFU Forget01 dataset using \ttsmall{Phi-1.3B}. Different values of $P$ lead to varying trade-offs, we select $P=0.4$ as the optimal perturbation ratio.}
			\label{P}
			\setlength{\tabcolsep}{12pt}
			\vskip -0.1in
			\begin{center}
				\begin{small}
					\begin{tabular}{c|ccc|ccc|cc}
						\toprule
						\textbf{Dataset} & \multicolumn{3}{c|}{\textbf{Forget}}       & \multicolumn{3}{c|}{\textbf{Rephrased Forget}}   & \multirow{2}{*}{\textbf{MU↑}} & \multirow{2}{*}{\textbf{FRT↑}} \\
						\textbf{Metric}  & \textbf{RG↓} & \textbf{Pr↓} & \textbf{TR↑} & \textbf{RG↓}   & \textbf{Pr↓}   & \textbf{TR↑}   &                               &                                \\ 
						\midrule
						P=0.1  & 52.12        & 43.00        & 56.18        & 42.28          & 31.00          & 53.79          & 50.90                         & 1.21                           \\
						P=0.2  & 50.08        & 37.28        & 57.03        & 40.74          & 27.06          & 57.53          & 50.55                         & 1.30                           \\
						P=0.3  & 48.02        & 34.68        & 61.40        & 42.16          & 27.50          & 59.13          & \colorbox{lightpurple!50}{\textbf{51.12}}                & 1.34                           \\
						\colorbox{gray!40}{P=0.4}  & 46.26        & 27.11        & 64.65        & 39.95          & 22.76          & 63.44          & 50.14                         & \colorbox{lightpurple!50}{1.47}                           \\
						P=0.5  & 47.17        & 30.64        & 62.70        & 41.09          & 24.62          & 62.60          & 48.00                         & 1.34                           \\
						P=0.6 & 44.47        & 25.99        & 62.43        & 41.33          & 21.39          & 62.61          & 48.61                         & 1.46                           \\
						P=0.7 & 41.69        & 26.95        & 65.19        & 41.10          & 22.84          & 64.38          & 48.79                         & 1.47                           \\
						P=0.8  & 42.37        & 24.87        & 65.03        & 39.97          & 21.18          & 64.96          & 47.51                         & 1.48                           \\
						P=0.9 & \textbf{41.11}        & 22.49        & 66.79        & 38.76          & 19.08          & 64.88          & 48.08                         & 1.58                           \\
						P=1.0  & 41.53        & \colorbox{lightpurple!50}{\textbf{21.10}}      & \colorbox{lightpurple!50}{\textbf{68.66}}        & \textbf{37.11} & \colorbox{lightpurple!50}{\textbf{18.31}} & \textbf{66.21} & 48.61                         & \textbf{1.65}                  \\ 
						\bottomrule
					\end{tabular}
				\end{small}
			\end{center}
			% \vskip -0.15in
		\end{table*}
		
		
		
		\subsection{Perturbation Ratio $P$}
		\label{Perturbation Ratio}
		We analyze the impact of the Perturbation Ratio $P$, 
		which ranges from $0.1$ to $1.0$ in increments of $0.1$. The experiments are conducted on the TOFU Forget01 dataset using \ttsmall{Phi-1.3B}, keeping other parameters constant. 
		
		As shown in Tab.~\ref{P}, even a small amount of noise ($P=0.1$) is sufficient to achieve a notable unlearning effect. Moreover, increasing the noise ratio further enhances the effectiveness of unlearning. Specifically, when $P=1.0$, almost all metrics on the Forget and Rephrased Forget data achieve their best values. This is expected, as higher noise levels make it harder for the model to recall related facts, resulting in a more fact-unrelated probability distribution and better unlearning performance. 
		
		
		
		\begin{table*}[]
			\caption{The impact of the Tuning Coefficient $C$. Experiments are conducted on the TOFU Forget01 dataset using \ttsmall{Phi-1.3B}. Increasing $C$ results in an improved unlearning effect, but at the cost of decreased model utility. To balance model utility with effective unlearning, we select $C=0.1$ for our experiments.}
			\label{C}
			\setlength{\tabcolsep}{12pt}
			\vskip -0.1in
			\begin{center}
				\begin{small}
					\begin{tabular}{c|ccc|ccc|cc}
						\toprule
						\textbf{Dataset} & \multicolumn{3}{c|}{\textbf{Forget}}       & \multicolumn{3}{c|}{\textbf{Rephrased Forget}}   & \multirow{2}{*}{\textbf{MU↑}} & \multirow{2}{*}{\textbf{FRT↑}} \\
						\textbf{Metric}  & \textbf{RG↓} & \textbf{Pr↓} & \textbf{TR↑} & \textbf{RG↓}   & \textbf{Pr↓}   & \textbf{TR↑}   &                               &      \\ 
						\midrule
						C=0.0            & 49.69          & 36.45         & 62.02          & 42.08          & 29.45         & 59.81          & \textbf{50.50}                & 1.28                           \\
						\colorbox{gray!40}{C=0.1}            & 46.26          & 27.11         & 64.65          & 39.95          & 22.76         & 63.44          & \colorbox{lightpurple!50}{50.14}                         & 1.47                           \\
						C=0.2            & 37.62          & 13.66         & 70.43          & 38.76          & 12.18         & 70.10          & 47.73                         & 1.87                           \\
						C=0.3            & 24.64          & 2.84          & 71.73          & 25.77          & 2.70          & 72.37          & 39.66                         & 2.84                           \\
						C=0.4            & \textbf{12.76} & \textbf{0.83} & 73.13          & \textbf{15.39} & \textbf{0.88} & 72.89          & 31.85                         & \textbf{4.27}                  \\
						C=0.5            & 18.11          & 1.38          & 75.68          & 17.16          & 1.36          & 75.04          & 38.81                         & 4.08                           \\
						C=0.6            & 19.54          & 1.54          & 77.77          & 21.73          & 1.52          & \textbf{77.60} & 37.56                         & 3.39                           \\
						C=0.7            & 18.51          & 1.91          & 78.28          & 20.64          & 1.69          & 77.36          & 37.59                         & 3.52                           \\
						C=0.8            & 16.15          & 1.69          & \textbf{78.65} & 19.31          & 1.43          & 77.10          & 37.35                         & 3.87                           \\
						C=0.9            & 15.46          & 1.52          & 77.68          & 17.25          & 1.28          & 76.50          & 36.92                         & 4.16                           \\
						C=1.0            & 17.64          & 1.48          & 77.19          & 18.66          & 1.24          & 76.21          & 36.66                         & 3.76                  
						\\ 
						\bottomrule
					\end{tabular}
				\end{small}
			\end{center}
			\vskip -0.15in
		\end{table*}
		
		
		However, the model utility decreases by up to 3.61\% when $P \geq 0.5$. This suggests that excessive noise can hinder the model's sentence comprehension and increase uncertainty, unintentionally affecting irrelevant knowledge generation. The model utility is the highest when $P=0.3$. Since different values of $P$ result in varying trade-offs, we select $P=0.4$ as the optimal perturbation ratio. This choice is based on the fact that the FRT ratio at $P=0.4$ is higher than at $P=0.3$, while maintaining considerable model utility, indicating a better balance between unlearning performance and model utility. Therefore, we intuitively set $P=0.4$ for all experiments.
		
		
		
		\subsection{Tuning Coefficient $C$}
		\label{Tuning Coefficient}
		We investigate the impact of the Tuning Coefficient $C$, which varies from $0.0$ to $1.0$ in increments of $0.1$. The experiments are conducted on the TOFU Forget01 dataset using \ttsmall{Phi-1.3B}, with all other parameters held constant. As shown in Tab.~\ref{C}, the best model utility is achieved when $C=0.0$, where only the corrupted-run probability distribution is used. While this setting maintains high model utility, the unlearning effect is insufficient, as the top-ranked token in the clean-run probability distribution is not fully suppressed. As $C$ increases, the unlearning effect improves, reaching its peak when $C=0.4$. Then it begins to fluctuate as $C$ continues to increase. Correspondingly, model utility decreases with larger values of $C$, which is expected, as higher $C$ values subtract more information from $p(y|x')$, potentially disrupting the distribution of irrelevant knowledge. To balance model utility with effective unlearning, we select $C=0.1$ for our experiments.
		
		
		
		\begin{table*}[]
			\caption{A comparison of perturbing a single subject in sentences (referred to as $\ours_{single}$) versus multiple subjects on the Harry Potter dataset. The results indicate that perturbing just one subject achieves competitive performance compared to perturbing all subjects, especially for smaller models. }
			\label{multi_subject}
			\setlength{\tabcolsep}{3.2pt}
			\renewcommand{\arraystretch}{1.3}
			\vskip -0.1in
			\begin{center}
				\begin{small}
					\begin{tabular}{c|c|ccc|ccc|ccc|ccc|cc}
						\toprule
						\multirow{2}{*}{\textbf{Model}}                                                & \textbf{Dataset}                      & \multicolumn{3}{c|}{\textbf{Forget data}}        & \multicolumn{3}{c|}{\textbf{Retain data}}        & \multicolumn{3}{c|}{\textbf{Real Authors}}       & \multicolumn{3}{c|}{\textbf{Real World}}         & \multirow{2}{*}{\textbf{MU↑}} & \multirow{2}{*}{\textbf{FRT↑}} \\
						& \textbf{Metric}                       & \textbf{RG↓}   & \textbf{Pr↓}   & \textbf{F1↓}   & \textbf{RG↑}   & \textbf{Pr↑}   & \textbf{F1↑}   & \textbf{RG↑}   & \textbf{Pr↑}   & \textbf{TR↑}   & \textbf{RG↑}   & \textbf{Pr↑}   & \textbf{TR↑}   &                               &                                \\ 
						\midrule
						\multirow{2}{*}{\textbf{\begin{tabular}[c]{@{}c@{}}Phi\\ (1.3B)\end{tabular}}} & \textbf{\ours}    & \colorbox{lightblue!50}{65.21} & \textbf{35.75} & 63.02 & \textbf{82.58}          & \textbf{66.08}          & \textbf{81.61}          & 60.90          & \textbf{48.89}          & \textbf{59.98}          & \textbf{67.09}          & \textbf{52.47}          & 64.91          & \textbf{63.20}                         & 1.16                                     \\
						& \textbf{$\ours_{single}$} & \colorbox{lightpurple!50}{\textbf{63.35}} & 36.49 & \textbf{61.45} & 81.53          & 65.20          & 80.47          & \textbf{62.90} & 48.82          & 59.89          & \textbf{67.09} & 52.44          & \textbf{64.99} & 63.17                         & \textbf{1.17}                  \\ \midrule
						\multirow{2}{*}{\textbf{\begin{tabular}[c]{@{}c@{}}LLaMA\\ (7B)\end{tabular}}} & \textbf{\ours}         & \colorbox{lightpurple!50}{\textbf{71.88}} & \textbf{56.36} & \textbf{69.54} & \textbf{92.42} & \textbf{88.88} & \textbf{91.54} & 83.25          & \textbf{69.40} & \textbf{83.66} & \textbf{85.47} & \textbf{71.25} & \textbf{84.84} & \colorbox{lightpurple!50}{\textbf{82.65}}                & \textbf{1.25}                  \\
						& \textbf{$\ours_{single}$} & \colorbox{lightblue!50}{75.30}          & 61.77          & 72.41          & 90.55          & 85.73          & 88.91          & \textbf{85.42} & 65.58          & 81.01          & 85.04          & 69.40          & 83.83          & \colorbox{lightblue!50}{80.82}                         & 1.16                           \\ \bottomrule
					\end{tabular}
				\end{small}
			\end{center}
			\vskip -0.15in
		\end{table*}
		
		
		
		\subsection{Multiple Subjects Scenarios}
		\label{Multiple Subjects}
		The simplest unlearning scenario involves a sentence built around only one subject of knowledge, as seen in datasets like TOFU and ZsRE. However, in real-world scenarios, sentences often contain multiple names or entities serving as subjects. This complexity poses greater challenges for the model to forget specific knowledge, as the information stored in language models is highly entangled. In this experiment, we investigate this more complex scenario and use the Harry Potter dataset, as sentences in novels often contain multiple subjects. For example, consider the sentence: ``How many points were taken from Gryffindor due to Harry, Hermione, and Neville being caught out of bed?". To ensure a fair comparison, we randomly perturb only one subject in the sentence, a method we denote as $\ours_{single}$. In contrast, \ours~represents our original proposed approach, where all subject tokens in the sentence are perturbed. The experiments are conducted on both \ttsmall{Phi-1.3B} and \ttsmall{LLaMA2-7B}, with all other parameters kept constant. 
		As shown in Tab.~\ref{multi_subject}, to our surprise, $\ours_{single}$ achieves competitive unlearning performance while maintaining considerable model utility. This phenomenon is particularly noticeable when using \ttsmall{Phi-1.3B}, where $\ours_{single}$ even surpasses \ours~relatively by up to 2.9\% in Forget ROUGE. Although $\ours_{single}$ may become slightly less effective than \ours~with model scaling, it still outperforms other baselines presented in Tab.\ref{Harry_rephrase}. In summary, perturbing all subject tokens with \ours~demonstrates more robust unlearning ability, particularly in larger models. However, for smaller models, perturbing only one subject in the sentence with $\ours_{single}$ can still ensure promising unlearning performance while preserving model utility.
		
		
		\subsection{Discrete-Token Level Perturbation}
		\label{Discrete-Token Level Perturbation}
		Apart from adding random noise to the subject token embeddings, \ours~can also be implemented by perturbing the subject words at the discrete-token level, denoted as $\ours_{dis}$.  
		In this experiment, we evaluate the unlearning performance of $\ours_{dis}$ on the TOFU dataset while keeping all other parameters constant. The perturbation type is randomly chosen from deleting, altering, or adding letters to the subject words.  
		As shown in Tab.~\ref{Discrete-Token}, $\ours_{dis}$ exhibits exceptional unlearning capability, outperforming \ours~in Forget ROUGE across all datasets and models. Moreover, $\ours_{dis}$ achieves an absolute 19.26\% Forget ROUGE and 4.64\% Forget Probability on Forget05 when using \ttsmall{LLaMA2-7B}, surpassing \ours~by up to 11.16\% (30.42$\rightarrow$19.26) and 11.65\% (16.29$\rightarrow$4.64), respectively, though with a slightly lower model utility of 0.92\%. However, when tested on \ttsmall{Phi-1.3B}, $\ours_{dis}$ does not consistently exhibit superior unlearning performance, and the model utility drops by up to 4.44\% (50.94$\rightarrow$46.50) as the number of unlearning samples increases. Despite this, the FRT ratio of $\ours_{dis}$ still outperforms other baselines presented in Tab.~\ref{tofu_llama_full}. 
		In summary, perturbing subject words at the discrete-token level can also prevent the model from recalling the fact and generate fact-unrelated probability distributions, thus achieving unlearning. Both embedding-layer and discrete-token-level noise methods can achieve effective unlearning but result in different trade-offs. Given that the knowledge retention ability of $\ours_{dis}$ may decline as the amount of forgotten data increases, we choose to add noise at the embedding layer as a more promising alternative.
		
		\begin{table*}[]
			\caption{Experimental results for implementing perturbation at the discrete-token level to subject words, denoted as $\ours_{dis}$. Surprisingly, $\ours_{dis}$ demonstrates a stronger unlearning effect on some datasets, particularly achieving 19.26\% in Forget Rouge and 4.64\% in Probability on the Forget01 dataset using \texttt{LLaMA2-7B}. These results highlight the adaptability of our approach across different dimensions.}
			\vskip -0.15in
			\label{Discrete-Token}
			\setlength{\tabcolsep}{2.3pt}
			\renewcommand{\arraystretch}{1.4}
			\begin{center}
				\begin{small}
					\begin{tabular}{c|c|c|ccc|ccc|ccc|ccc|cc}
						\midrule
						\multirow{2}{*}{\textbf{Model}} & \multirow{2}{*}{\textbf{TOFU}}     & \textbf{Dataset}                        & \multicolumn{3}{c|}{\textbf{Forget data}}         & \multicolumn{3}{c|}{\textbf{Retain data}}         & \multicolumn{3}{c|}{\textbf{Real Authors}}        & \multicolumn{3}{c|}{\textbf{Real World}}          & \multirow{2}{*}{\textbf{MU↑}} & \multirow{2}{*}{\textbf{FRT↑}} \\
						&      & \textbf{Metric}                         & \textbf{RG↓}   & \textbf{Pr↓}   & \textbf{TR↑}   & \textbf{RG↑}   & \textbf{Pr↑}   & \textbf{TR↑}   & \textbf{RG↑}   & \textbf{Pr↑}   & \textbf{TR↑}   & \textbf{RG↑}   & \textbf{Pr↑}   & \textbf{TR↑}   &                               &
						\\ \cline{1-17}
						\multirow{6}{*}{\textbf{\begin{tabular}[c]{@{}c@{}}Phi\\ (1.3B)\end{tabular}}}  & \multirow{2}{*}{\textbf{Forget01}} 
						& \textbf{\ours}    & 46.26 & \textbf{27.11} & \textbf{64.65} & 67.74          & 79.50          & 44.96          & 41.90          & 37.85          & 44.04          & \textbf{76.14}          & \textbf{41.75}          & \textbf{50.39}          & 50.14                         & 1.37               \\
						&       & \textbf{$\ours_{dis}$} & \textbf{35.46} & 27.53 & 53.67          & \textbf{84.31} & \textbf{89.36} & \textbf{48.18} & \textbf{46.23} & \textbf{38.26} & \textbf{46.71} & 76.10 & 40.91          & 49.67          & \textbf{52.72}                & \textbf{1.67}                  \\ \cline{2-17}
						& \multirow{2}{*}{\textbf{Forget05}} & \textbf{\ours}           & 42.67          & \textbf{25.17} & \textbf{62.96} & \colorbox{lightblue!50}{\textbf{67.97}} & \colorbox{lightblue!50}{\textbf{77.49}} & \textbf{44.79} & 43.23          & \textbf{37.94} & \textbf{45.60} & 76.13          & \textbf{43.10} & \textbf{52.65} & \colorbox{lightpurple!50}{\textbf{50.94}}                & \textbf{1.50}                  \\
						&             & \textbf{$\ours_{dis}$} & \textbf{36.94} & 36.76          & 60.86          & \colorbox{lightpurple!50}{42.37}          & \colorbox{lightpurple!50}{58.66}          & 41.05          & \textbf{45.82} & 37.43          & 44.93          & \textbf{78.40} & 40.73          & 48.92          & \colorbox{lightblue!50}{46.50}                         & 1.26                           \\ \cline{2-17}
						& \multirow{2}{*}{\textbf{Forget10}} & \textbf{\ours}           & 46.55          & \textbf{41.93} & \textbf{57.51} & \colorbox{lightblue!50}{\textbf{81.59}} & \colorbox{lightblue!50}{\textbf{86.17}} & \textbf{47.10} & 35.23          & \textbf{37.64} & \textbf{45.17} & \textbf{75.28} & \textbf{41.25} & \textbf{49.92} & \colorbox{lightpurple!50}{\textbf{50.07}}                & \textbf{1.13}                  \\
						&             & \textbf{$\ours_{dis}$} & \textbf{44.90} & 57.06          & 55.92          & \colorbox{lightpurple!50}{46.25}          & \colorbox{lightpurple!50}{63.60}          & 43.77          & \textbf{49.35} & 36.89          & 44.34          & 73.65          & 39.78          & 47.41          & \colorbox{lightblue!50}{47.39}                         & 0.93                           \\ \cline{1-17}
						\multirow{6}{*}{\textbf{\begin{tabular}[c]{@{}c@{}}LLaMA\\ (7B)\end{tabular}}} & \multirow{2}{*}{\textbf{Forget01}} & \textbf{\ours}           & \colorbox{lightblue!50}{30.42}          & \colorbox{lightblue!50}{16.29}          & \textbf{74.11} & \textbf{86.67} & 88.55          & \textbf{43.51} & \textbf{92.80} & \textbf{51.82} & \textbf{66.34} & \textbf{89.17} & \textbf{49.28} & \textbf{63.02} & \textbf{65.06}                & 2.79                           \\
						&     & \textbf{$\ours_{dis}$} & \colorbox{lightpurple!50}{\textbf{19.26}} & \colorbox{lightpurple!50}{\textbf{4.64}}  & 73.92          & 83.72          & \textbf{89.11} & 42.39          & 91.00          & 51.19          & 66.05          & 87.46          & 48.78          & 62.66          & 64.14                         & \textbf{5.37}                  \\
						\cline{2-17}
						& \multirow{2}{*}{\textbf{Forget05}} & \textbf{\ours}           & 33.66          & 39.23          & 64.39          & \textbf{83.63} & \textbf{88.24} & \textbf{41.89} & 91.30          & 52.57          & 68.21          & \textbf{89.60} & 49.77          & 63.94          & \textbf{64.89}                & 1.78                           \\
						&        & \textbf{$\ours_{dis}$} & \textbf{30.39} & \textbf{36.55} & \textbf{65.45} & 72.08          & 79.45          & 41.19          & \textbf{91.50} & \textbf{52.61} & \textbf{68.67} & 88.32          & \textbf{50.15} & \textbf{64.52} & 63.38                         & \textbf{1.89}                  \\
						\cline{2-17}
						& \multirow{2}{*}{\textbf{Forget10}} & \textbf{\ours}           & 44.60          & \textbf{47.58} & \textbf{67.25} & \textbf{94.14} & \textbf{94.68} & 41.23          & 90.30          & 51.66          & 66.56          & \textbf{88.75} & 48.48          & 62.10          & \textbf{64.80}                & 1.41                           \\
						&            & \textbf{$\ours_{dis}$} & \textbf{40.32} & 48.68          & 62.45          & 70.72          & 79.47          & \textbf{41.56} & \textbf{91.50} & \textbf{53.81} & \textbf{70.03} & 87.89          & \textbf{50.86} & \textbf{65.92} & 63.93         & \textbf{1.44}             \\
						\midrule
					\end{tabular}
				\end{small}
			\end{center}
			\vskip -0.15in
		\end{table*}
		
		
		
		\begin{table*}[]
			\caption{A comparison of different retain loss settings. w/o GDR refers to using the vanilla forget loss, while w/ KLR indicates the combination with KLR. Overall, \ours~effectively preserves the model's utility.}
			\label{different_retain_loss}
			\setlength{\tabcolsep}{12pt}
			\begin{center}
				\begin{small}
					\begin{tabular}{c|ccc|ccc|cc}
						\toprule
						\textbf{Dataset} & \multicolumn{3}{c|}{\textbf{Forget}}             & \multicolumn{3}{c|}{\textbf{Rephrased Forget}}   & \multirow{2}{*}{\textbf{MU↑}} & \multirow{2}{*}{\textbf{FRT↑}} \\
						\textbf{Metric}  & \textbf{RG↓}   & \textbf{Pr↓}   & \textbf{TR↑}   & \textbf{RG↓}   & \textbf{Pr↓}   & \textbf{TR↑}   &                               &                                \\
						\midrule
						\textbf{\ours}    & 44.60          & 47.58          & 67.25          & 32.72          & \textbf{37.99} & 65.96          & \colorbox{lightpurple!50}{\textbf{64.80}}                & \textbf{1.59}                  \\
						\textbf{w/o GDR} & 44.88          & 47.52          & 66.44          & 34.23          & 39.67          & 65.50          & \colorbox{lightblue!50}{60.25}                         & 1.45                           \\
						\textbf{w/ KLR}  & \textbf{41.54} & \textbf{45.43} & \textbf{67.45} & \textbf{31.92} & 38.15          & \textbf{66.44} & \colorbox{lightblue!50}{60.55}                         & 1.54                               \\ 
						\bottomrule
					\end{tabular}
				\end{small}
			\end{center}
		\end{table*}
		
		
		
		\subsection{Different Retain Loss}
		\label{Different Retain Loss}
		We investigate the impact of different retain loss functions while keeping other parameters fixed. The experiments are conducted on the TOFU Forget10 dataset using \ttsmall{LLaMA2-7B}. \ours~typically employs the vanilla forget loss combined with GDR, with a retain weight of $RW=1$. Here, w/o GDR refers to using the vanilla forget loss without any retain loss, while w/ KLR denotes the combination of the vanilla forget loss with KLR, applying the same retain weight. As shown in Tab.~\ref{different_retain_loss}, using the vanilla forget loss achieves strong unlearning performance but may slightly impair model utility. While incorporating KLR can improve model utility, the enhancement is less significant compared to using GDR as the retain loss. Therefore, we primarily adopt GDR as the retain loss in our experiments.
		
		
		
		
		\section{Conclusions}
		In this paper, we strive to safeguard LLM-based model unlearning by enhancing generalisation capabilities, ensuring that unlearned models do not retain any memory of the targeted knowledge. To this end, we introduce \bench, a comprehensive benchmark for assessing the generalisation performance of unlearning methods, and propose \ours, a novel approach that addresses two key issues from \bench. Our findings underscore the importance of moving beyond superficial forgetting, paving the way for safer and more thorough LLM-based unlearning. In our future work, we plan to incorporate human evaluations in  assessments.
		
		
		% \appendices
		
		% \ifCLASSOPTIONcompsoc
		%   \section*{Acknowledgments}
		% \else
		%   \section*{Acknowledgment}
		% \fi
		
		% \ifCLASSOPTIONcaptionsoff
		%   \newpage
		% \fi
		
		\clearpage
		
		\bibliography{unlearning}
		\bibliographystyle{IEEEtran}
		
		
		% \begin{IEEEbiography}{Michael Shell}
			% Biography text here.
			% \end{IEEEbiography}
		
		
		% \begin{IEEEbiographynophoto}{John Doe}
			% Biography text here.
			% \end{IEEEbiographynophoto}
		
		
		% \begin{IEEEbiographynophoto}{Jane Doe}
			% Biography text here.
			% \end{IEEEbiographynophoto}
		
		
	\end{document}
	
	
