\section{Related Work}
We provide a brief overview of existing machine unlearning methods for LLMs, categorised into training-free and training-based approaches~\cite{Knowledge_Unlearning}, along with the evaluation methods relevant to our proposed \bench~benchmark.  
		
		\noindent\textbf{Training-free LLM Unlearning.}
		One area of research concentrates on identifying neurons linked to unlearning samples~\cite{Patil} and directly modifying these detected neurons~\cite{DEPN} without the need for learning. With the advancement of in-context learning, some approaches unlearn knowledge by providing LLMs with unlearned samples accompanied by different labels~\cite{In-Context-Unlearning, Guardrail} or by generating corrupted prompts with altered embeddings~\cite{Unlearning-Prompts}. 
		Despite their efficiency, locate-and-edit methods are constrained to triplet-format data, while prompt-based methods depend on artificially designed templates, limiting their practicality in real-world scenarios. As such, this paper primarily focuses on training-based methods.
		
		
		\noindent\textbf{Training-based LLM Unlearning.}
		Another stream of unlearning approaches focuses on models and gradients. The most common training-based method is gradient ascent~\cite{grad_ascent}, which updates model parameters by maximizing the likelihood of mis-prediction for samples in the forget set. To mitigate the catastrophic collapse issue associated with gradient ascent, reinforcement learning has been employed to align the model with negative preference optimization (NPO)~\cite{NPO}, treating forgotten data as negative examples. Alternatively, instruction-tuning LLMs to generate responses such as ``I do not have access to…” or ``I don’t know…” has also been explored~\cite{SNAP}. 
		To improve efficiency, other approaches incorporate additional trainable layers or modules~\cite{Chaff}, integrating or adding them into the original models. For example, Chen \& Yang~\cite{EUL} introduces unlearning layers to forget specific data sets, which are then integrated into transformers. Likewise, Zhang et al.~\cite{PEFT_unlearn} combines various lightweight modules with distinct functionalities to enable unlearning. To further preserve model utility, some methods train a reinforced or assistant model~\cite{ULD}, comparing its prediction logits with those of the baseline model~\cite{harry_potter, RKLD}. Others~\cite{taskvectors} employ simple arithmetic operations on task vectors to modify the model, such as reducing undesirable behaviors, forgetting specific tasks, or enabling multitask learning. However, the generalisation capabilities have been largely overlooked in prior research. Our study uniquely identifies the generalisation dilemma in this area, explains its underlying causes, and proposes a novel unlearning scheme to address it.
		
		\noindent\textbf{Unlearning Evaluation.}
		Several studies have examined LLM unlearning from different perspectives~\cite{Poisoning_attack}.  Specifically, Patil et al.~\cite{Patil} investigate the effectiveness of typical model editing methods in removing information from model weights. Hong et al.~\cite{Intrinsic} use vocabulary projections to analyze concept vectors through parametric knowledge traces. Yao et al.~\cite{acl_bench} evaluate seven different unlearning methods on longer-context tasks across three source domains. Meanwhile, Shi et al.~\cite{MUSE} assess six desirable properties of unlearned models across eight unlearning methods. Jia et al.~\cite{SOUL} focus on the influence of second-order optimization on unlearning. Li et al.~\cite{WMDP} examine malicious use scenarios in biosecurity, cybersecurity, and chemical security. Qiu et al.~\cite{PISTOL} evaluate four distinct unlearning methods for removing highly interconnected data. Du et al.~\cite{TULA} investigate the risk of knowledge leakage after unlearning. The survey by Liu et al.~\cite{Rethinking} highlights often-overlooked aspects of existing LLM unlearning research and introduces the concept of unlearning scope. Unlike these existing works, \bench~is the first benchmark to evaluate the state-of-the-art methods on their generalisation ability. Notably, the evaluation covers thirteen unlearning methods across seven partitions from three domains, offering a more diverse and challenging scenario.
		
		\subsection{Problem Definition}
		\label{Problem Definition}
		The objective of machine unlearning is to enable an initial target model to forget specific unlearning samples as if it were never trained on them, while preserving the model’s performance on unrelated knowledge. 
		More specifically, the target model $f_{\theta_{tr}}$ is represented by a function $f: \mathbb{X} \mapsto \mathbb{Y}$, where $\theta_{tr}$ denotes the parameters of the target model. Let the pre-training dataset be $D_{tr}$, and the dataset to be forgotten be $D_f$. The retained dataset is then defined as $D_r = D_{tr} \backslash D_f$. The ideal retained model, $f_{\theta_{r}}$, is one that has never been trained on $D_f$. Since $\theta_{tr}$ is not directly accessible, we define an unlearning procedure $\mathbb{U}$, which takes $f_{\theta_{tr}}$ and $D_f$ as inputs, producing an unlearned model $f_{\theta_{u}} \sim \mathbb{U}(f_{\theta_{tr}}, D_f)$. 
		The unlearned model’s predictions should also change for the paraphrased forget dataset $D_{p}$.
		Therefore, given a distance metric $m(\cdot)$, the objective of the unlearning algorithm is to minimize the distance between $f_{\theta_{u}}$ and $f_{\theta_{r}}$ for each sample $x \in D_f \cup D_{p}$ :
		$
		\frac{\mathbb{E} [m(f_{\theta_{u}}(x))]}{\mathbb{E} [m(f_{\theta_{r}}(x))]} \approx 1.
		$