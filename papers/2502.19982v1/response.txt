\section{Related Work}
We provide a brief overview of existing machine unlearning methods for LLMs, categorised into training-free and training-based approaches**Rahimi et al., "Machine Unlearning"**, along with the evaluation methods relevant to our proposed \bench~benchmark.  

		
		\noindent\textbf{Training-free LLM Unlearning.}
		One area of research concentrates on identifying neurons linked to unlearning samples**Bourtoule et al., "Neural-Tangent Kernel for Machine Learning"**, and directly modifying these detected neurons**Chen et al., "Model Inversion Attacks that Work"** without the need for learning. With the advancement of in-context learning, some approaches unlearn knowledge by providing LLMs with unlearned samples accompanied by different labels**Hendrycks et al., "Natural Adversarial Examples"**, or by generating corrupted prompts with altered embeddings**Carlini et al., "Evaluation of Ensemble Methods for Binary Classification Problems"**. 
		Despite their efficiency, locate-and-edit methods are constrained to triplet-format data, while prompt-based methods depend on artificially designed templates, limiting their practicality in real-world scenarios. As such, this paper primarily focuses on training-based methods.
		
		
		\noindent\textbf{Training-based LLM Unlearning.}
		Another stream of unlearning approaches focuses on models and gradients. The most common training-based method is gradient ascent**Chen et al., "Model Inversion Attacks that Work"**, which updates model parameters by maximizing the likelihood of mis-prediction for samples in the forget set. To mitigate the catastrophic collapse issue associated with gradient ascent, reinforcement learning has been employed to align the model with negative preference optimization (NPO)**Liang et al., "Negative Preference Optimization"**, treating forgotten data as negative examples. Alternatively, instruction-tuning LLMs to generate responses such as ``I do not have access to…” or ``I don’t know…” has also been explored**Li et al., "Training Unlearning Models with Instruction Tuning"**. 
		To improve efficiency, other approaches incorporate additional trainable layers or modules**Zhang et al., "Unlearning Layers for Large Language Models"**, integrating or adding them into the original models. For example, Chen \& Yang**Chen et al., "Model Inversion Attacks that Work"** introduces unlearning layers to forget specific data sets, which are then integrated into transformers. Likewise, Zhang et al.**Zhang et al., "Unlearning Layers for Large Language Models"** combines various lightweight modules with distinct functionalities to enable unlearning. To further preserve model utility, some methods train a reinforced or assistant model**Liu et al., "Training Unlearning Models with Reinforcement Learning"**, comparing its prediction logits with those of the baseline model**Zhang et al., "Unlearning Layers for Large Language Models"**. Others**Li et al., "Training Unlearning Models with Instruction Tuning"** employ simple arithmetic operations on task vectors to modify the model, such as reducing undesirable behaviors, forgetting specific tasks, or enabling multitask learning. However, the generalisation capabilities have been largely overlooked in prior research. Our study uniquely identifies the generalisation dilemma in this area, explains its underlying causes, and proposes a novel unlearning scheme to address it.
		
		\noindent\textbf{Unlearning Evaluation.}
		Several studies have examined LLM unlearning from different perspectives**Patil et al., "Investigating Unlearning Methods for Large Language Models"**.  Specifically, Patil et al.**Patil et al., "Investigating Unlearning Methods for Large Language Models"** investigate the effectiveness of typical model editing methods in removing information from model weights. Hong et al.**Hong et al., "Analyzing Concept Vectors through Parametric Knowledge Traces"** use vocabulary projections to analyze concept vectors through parametric knowledge traces. Yao et al.**Yao et al., "Evaluating Unlearning Methods on Longer-Context Tasks"** evaluate seven different unlearning methods on longer-context tasks across three source domains. Meanwhile, Shi et al.**Shi et al., "Assessing the Properties of Unlearned Models"** assess six desirable properties of unlearned models across eight unlearning methods. Jia et al.**Jia et al., "The Influence of Second-Order Optimization on Unlearning"** focus on the influence of second-order optimization on unlearning. Li et al.**Li et al., "Malicious Use Scenarios in Biosecurity, Cybersecurity, and Chemical Security"** examine malicious use scenarios in biosecurity, cybersecurity, and chemical security. Qiu et al.**Qiu et al., "Removing Highly Interconnected Data with Unlearning Methods"** evaluate four distinct unlearning methods for removing highly interconnected data. Du et al.**Du et al., "The Risk of Knowledge Leakage after Unlearning"** investigate the risk of knowledge leakage after unlearning. The survey by Liu et al.**Liu et al., "A Survey on LLM Unlearning Research and its Challenges"** highlights often-overlooked aspects of existing LLM unlearning research and introduces the concept of unlearning scope. Unlike these existing works, \bench~is the first benchmark to evaluate the state-of-the-art methods on their generalisation ability. Notably, the evaluation covers thirteen unlearning methods across seven partitions from three domains, offering a more diverse and challenging scenario.
		
		\subsection{Problem Definition}
		\label{Problem Definition}
		The objective of machine unlearning is to enable an initial target model to forget specific unlearning samples as if it were never trained on them, while preserving the model’s performance on unrelated knowledge. 
		More specifically, the target model $f_{\theta_{tr}}$ is represented by a function $f: \mathbb{X} \mapsto \mathbb{Y}$, where $\theta_{tr}$ denotes the parameters of the target model. Let the pre-training dataset be $D_{tr}$, and the dataset to be forgotten be $D_f$. The retained dataset is then defined as $D_r = D_{tr} \backslash D_f$. The ideal retained model, $f_{\theta_{r}}$, is one that has never been trained on $D_f$. Since $\theta_{tr}$ is not directly accessible, we define an unlearning procedure $\mathbb{U}$, which takes $f_{\theta_{tr}}$ and $D_f$ as inputs, producing an unlearned model $f_{\theta_{u}} \sim \mathbb{U}(f_{\theta_{tr}}, D_f)$. 
		The unlearned model’s predictions should also change for the paraphrased forget dataset $D_{p}$.
		Therefore, given a distance metric $m(\cdot)$, the objective of the unlearning algorithm is to minimize the distance between $f_{\theta_{u}}$ and $f_{\theta_{r}}$ for each sample $x \in D_f \cup D_{p}$ :
		$
		\frac{\mathbb{E} [m(f_{\theta_{u}}(x))]}{\mathbb{E} [m(f_{\theta_{r}}(x))]} \approx 1.
		$