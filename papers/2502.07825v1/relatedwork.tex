\section{Related Work}
\textbf{Video World Models.} With the development of internet-scale datasets~\citep{Bain21,chen2024panda70m} and advanced model architecture~\citep{peebles2023dit,videoworldsimulators2024}, significant progress has been made in realistic video generation conditioned on text descriptions and
initial frames~\citep{blattmann2023SDVideo,lin2024opensoraplan, ma2024latte, yang2024cogvideox,opensora}. Building upon these foundations, current research has increasingly focused on action-controllable video generation, aiming to develop generalist world simulators~\citep{xiang2024pandora,yang2023learning,bruce2024genie,feng2024matrix,valevski2024diffusion,genie2,FangqiIRASim2024,2025gamegen} that can effectively model both physical dynamics and action consequences. However, these models typically require training from scratch on large-scale datasets and involve millions (or billions) of parameters, resulting in substantial computational overhead and slow inference speed. In contrast, we propose to adapt publicly available pre-trained video generative models~\citep{opensora,wu2024ivideogpt} into action-driven world simulators. Our proposed fine-tuning approach, DWS, achieves efficient model adaptation while requiring minimal computational overhead, significantly reducing both training costs and inference latency. Concurrent works~\citep{rigter2024avid,yu2025gamefactory} have also explored leveraging pre-trained models for action-conditioned video generation. However, their investigations are limited to diffusion-based models, and neither work validates the effectiveness of their approaches in facilitating downstream tasks such as model-based RL.

\noindent \textbf{Model-Based RL}
Model-based RL aims to build world models in which the trial-and-errors can take place without real cost. With a sufficiently accurate world model, agents can develop imagination abilities, allowing them to simulate interactions and generate synthetic experience data. This simulated data can then be leveraged to learn optimal policies for diverse decision-making tasks, effectively reducing the need for real-world interactions. \citet{sutton1991dyna} introduce the first general framework for model-based RL, highlighting the utility of an estimated dynamics model in facilitating the training of value functions and policies~\citep{Sutton1998ReinforcementL}. Recent years have witnessed remarkable progress in model-based methods for learning complex environmental dynamics, such as video games and visual control tasks, consistently outperforming their model-free counterparts. For example, built upon Recurrent State Space Models (RSSM)~\citep{hafner2019PlaNet}, which explicitly decouple the deterministic and stochastic components of environmental dynamics, the Dreamer series has demonstrated impressive performance across diverse domains, including Atari games~\citep{atari}, DeepMind Control Suite~\citep{tassa2018deepmind}, and Minecraft. 
To address the limitation of RNNs in expressing complex patterns, recent works have explored leveraging transformer models for enhanced sequence modeling and long-term dependency capture~\citep{micheli2023iris,robine2023twm,zhang2023storm,zhang2024marie}, and incorporating diffusion models to better represent multi-modal distributions in dynamic learning~\citep{ding2024dwm,alonso2024diamond}. However, although these works also employ transformer or diffusion models for world model learning, they predominantly rely on training from scratch and fail to leverage pre-trained knowledge for enhanced dynamics understanding, making them overly task-specific and limiting their ability to generalize across diverse tasks. Furthermore, while existing methods treat all imagined samples with uniform importance during training, our proposed DWS introduces a novel prioritization mechanism that selectively focuses on significant samples, thereby improving sample efficiency. 
\vspace{-0.5em}