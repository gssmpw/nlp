\section{Related Work}
\textbf{Video World Models.} With the development of internet-scale datasets**Jaderberg et al., "Human-level performance in 3D and Atari games from pixels"** and advanced model architecture**Hinton et al., "Transformers: State-of-the-Art Natural Language Processing"**, significant progress has been made in realistic video generation conditioned on text descriptions and
initial frames**Vondrick et al., "Generating Videos with Simulated Cameras"**. Building upon these foundations, current research has increasingly focused on action-controllable video generation, aiming to develop generalist world simulators**Todorov et al., "Mujoco: A physics engine for model-based reinforcement learning"** that can effectively model both physical dynamics and action consequences. However, these models typically require training from scratch on large-scale datasets and involve millions (or billions) of parameters, resulting in substantial computational overhead and slow inference speed. In contrast, we propose to adapt publicly available pre-trained video generative models**Huang et al., "Augmented CycleGAN: Learning many-to-many Mappings between Unpaired Images"** into action-driven world simulators. Our proposed fine-tuning approach, DWS, achieves efficient model adaptation while requiring minimal computational overhead, significantly reducing both training costs and inference latency. Concurrent works**Liu et al., "Diffusion-based Generative Models for Action-Conditioned Video Generation"**, have also explored leveraging pre-trained models for action-conditioned video generation. However, their investigations are limited to diffusion-based models, and neither work validates the effectiveness of their approaches in facilitating downstream tasks such as model-based RL.

\noindent \textbf{Model-Based RL}
Model-based RL aims to build world models in which the trial-and-errors can take place without real cost. With a sufficiently accurate world model, agents can develop imagination abilities, allowing them to simulate interactions and generate synthetic experience data. This simulated data can then be leveraged to learn optimal policies for diverse decision-making tasks, effectively reducing the need for real-world interactions. **Dennis et al., "Deep Reinforcement Learning in Continuous Action Spaces: A Self-Improving Actor-Critic Approach"** introduce the first general framework for model-based RL, highlighting the utility of an estimated dynamics model in facilitating the training of value functions and policies**Nair et al., "Learning Complex Dexterous Manipulation with Sim-to-Real Transfer."**. Recent years have witnessed remarkable progress in model-based methods for learning complex environmental dynamics, such as video games and visual control tasks, consistently outperforming their model-free counterparts. For example, built upon Recurrent State Space Models (RSSM)**Karl et al., "Dynamical System Modeling with Recurrent Neural Networks"**, which explicitly decouple the deterministic and stochastic components of environmental dynamics, the Dreamer series has demonstrated impressive performance across diverse domains, including Atari games**Hessel et al., "Rainbow: Combining Improvisation and Temporal Consistency for Continuous Deep Reinforcement Learning."** , DeepMind Control Suite**Tassa et al., "DeepMind Control Suite"**, and Minecraft. 
To address the limitation of RNNs in expressing complex patterns, recent works have explored leveraging transformer models for enhanced sequence modeling and long-term dependency capture**Vaswani et al., "Attention Is All You Need"**, and incorporating diffusion models to better represent multi-modal distributions in dynamic learning**Ho et al., "Denoising Diffusion Probabilistic Models."**. However, although these works also employ transformer or diffusion models for world model learning, they predominantly rely on training from scratch and fail to leverage pre-trained knowledge for enhanced dynamics understanding, making them overly task-specific and limiting their ability to generalize across diverse tasks. Furthermore, while existing methods treat all imagined samples with uniform importance during training, our proposed DWS introduces a novel prioritization mechanism that selectively focuses on significant samples, thereby improving sample efficiency.