@article{an2023fluctuation,
  title={Fluctuation-based Adaptive Structured Pruning for Large Language Models},
  author={An, Yongqi and Zhao, Xu and Yu, Tao and Tang, Ming and Wang, Jinqiao},
  journal={Proceedings of the AAAI Conference on Artificial Intelligence},
  year={2024}
}

@article{dettmers2022gpt3,
  title={Gpt3. int8 (): 8-bit matrix multiplication for transformers at scale},
  author={Dettmers, Tim and Lewis, Mike and Belkada, Younes and Zettlemoyer, Luke},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={30318--30332},
  year={2022}
}

@article{dettmers2023spqr,
  title={Spqr: A sparse-quantized representation for near-lossless llm weight compression},
  author={Dettmers, Tim and Svirschevski, Ruslan and Egiazarian, Vage and Kuznedelev, Denis and Frantar, Elias and Ashkboos, Saleh and Borzunov, Alexander and Hoefler, Torsten and Alistarh, Dan},
  journal={arXiv preprint arXiv:2306.03078},
  year={2023}
}

@inproceedings{diao2020drasic,
  title={Drasic: Distributed recurrent autoencoder for scalable image compression},
  author={Diao, Enmao and Ding, Jie and Tarokh, Vahid},
  booktitle={Data Compression Conference (DCC)},
  pages={3--12},
  year={2020},
  organization={IEEE}
}

@article{diao2023pruning,
  title={Pruning deep neural networks from a sparsity perspective},
  author={Diao, Enmao and Wang, Ganghua and Zhan, Jiawei and Yang, Yuhong and Ding, Jie and Tarokh, Vahid},
  journal={International Conference on Learning Representations (ICLR)},
  year={2023}
}

@inproceedings{dong2024pruner,
  title={Pruner-Zero: Evolving Symbolic Pruning Metric from Scratch for Large Language Models},
  author={Dong, Peijie and Li, Lujun and Tang, Zhenheng and Liu, Xiang and Pan, Xinglin and Wang, Qiang and Chu, Xiaowen},
  booktitle={Proceedings of the 41st International Conference on Machine Learning},
  year={2024},
  organization={PMLR},
  url={https://arxiv.org/abs/2406.02924},
  note={[arXiv: 2406.02924]}
}

@inproceedings{fang2023depgraph,
  title={Depgraph: Towards any structural pruning},
  author={Fang, Gongfan and Ma, Xinyin and Song, Mingli and Mi, Michael Bi and Wang, Xinchao},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={16091--16101},
  year={2023}
}

@inproceedings{frantar2023sparsegpt,
  title={Sparsegpt: Massive language models can be accurately pruned in one-shot},
  author={Frantar, Elias and Alistarh, Dan},
  booktitle={International Conference on Machine Learning},
  pages={10323--10337},
  year={2023},
  organization={PMLR}
}

@article{han2015learning,
  title={Learning both weights and connections for efficient neural network},
  author={Han, Song and Pool, Jeff and Tran, John and Dally, William},
  journal={Advances in neural information processing systems},
  volume={28},
  year={2015}
}

@inproceedings{hassibi1993optimal,
  title={Optimal brain surgeon and general network pruning},
  author={Hassibi, Babak and Stork, David G and Wolff, Gregory J},
  booktitle={IEEE international conference on neural networks},
  pages={293--299},
  year={1993},
  organization={IEEE}
}

@inproceedings{he2019filter,
  title={Filter pruning via geometric median for deep convolutional neural networks acceleration},
  author={He, Yang and Liu, Ping and Wang, Ziwei and Hu, Zhilan and Yang, Yi},
  booktitle={Proceedings of the IEEE/CVF conference on computer vision and pattern recognition},
  pages={4340--4349},
  year={2019}
}

@article{hoffmann2022training,
  title={Training compute-optimal large language models},
  author={Hoffmann, Jordan and Borgeaud, Sebastian and Mensch, Arthur and Buchatskaya, Elena and Cai, Trevor and Rutherford, Eliza and Casas, Diego de Las and Hendricks, Lisa Anne and Welbl, Johannes and Clark, Aidan and others},
  journal={arXiv preprint arXiv:2203.15556},
  year={2022}
}

@article{hu2021lora,
  title={Lora: Low-rank adaptation of large language models},
  author={Hu, Edward J and Shen, Yelong and Wallis, Phillip and Allen-Zhu, Zeyuan and Li, Yuanzhi and Wang, Shean and Wang, Lu and Chen, Weizhu},
  journal={arXiv preprint arXiv:2106.09685},
  year={2021}
}

@article{kovaleva2021bert,
  title={BERT busters: Outlier dimensions that disrupt transformers},
  author={Kovaleva, Olga and Kulshreshtha, Saurabh and Rogers, Anna and Rumshisky, Anna},
  journal={arXiv preprint arXiv:2105.06990},
  year={2021}
}

@article{lecun1989optimal,
  title={Optimal brain damage},
  author={LeCun, Yann and Denker, John and Solla, Sara},
  journal={Advances in neural information processing systems},
  volume={2},
  year={1989}
}

@article{li2016pruning,
  title={Pruning filters for efficient convnets},
  author={Li, Hao and Kadav, Asim and Durdanovic, Igor and Samet, Hanan and Graf, Hans Peter},
  journal={arXiv preprint arXiv:1608.08710},
  year={2016}
}

@article{li2021ell,
  title={L1 Regularization in Two-Layer Neural Networks},
  author={Li, Gen and Gu, Yuantao and Ding, Jie},
  journal={IEEE Signal Processing Letters},
  volume={29},
  pages={135--139},
  year={2021},
  publisher={IEEE}
}

@inproceedings{li2024adaptive,
  title={Als: Adaptive layer sparsity for large language models via activation correlation assessment},
  author={Li, Wei and Li, Lujun and Lee, Mark and Sun, Shengjie},
  booktitle={Proceedings of Thirty-Eighth Annual Conference on Neural Information Processing Systems},
  year={2024}
}

@inproceedings{li2024discovering,
  title={Discovering Sparsity Allocation for Layer-wise Pruning of Large Language Models},
  author={Li, Lujun and Dong, Peijie and Tang, Zhenheng and Liu, Xiang and Wang, Qiang and Luo, Wenhan and Xue, Wei and Liu, Qifeng and Chu, Xiaowen and Guo, Yike},
  booktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},
  year={2024}
}

@inproceedings{liu2017learning,
  title={Learning efficient convolutional networks through network slimming},
  author={Liu, Zhuang and Li, Jianguo and Shen, Zhiqiang and Huang, Gao and Yan, Shoumeng and Zhang, Changshui},
  booktitle={Proceedings of the IEEE international conference on computer vision},
  pages={2736--2744},
  year={2017}
}

@article{liu2023sparsity,
  title={Sparsity May Cry: Let Us Fail (Current) Sparse Neural Networks Together!},
  author={Liu, Shiwei and Chen, Tianlong and Zhang, Zhenyu and Chen, Xuxi and Huang, Tianjin and Jaiswal, Ajay and Wang, Zhangyang},
  journal={arXiv preprint arXiv:2303.02141},
  year={2023}
}

@article{liu2024intactkv,
  title={IntactKV: Improving Large Language Model Quantization by Keeping Pivot Tokens Intact},
  author={Liu, Ruikang and Bai, Haoli and Lin, Haokun and Li, Yuening and Gao, Han and Xu, Zhengzhuo and Hou, Lu and Yao, Jun and Yuan, Chun},
  journal={arXiv preprint arXiv:2403.01241},
  year={2024}
}

@article{ma2023llm,
  title={Llm-pruner: On the structural pruning of large language models},
  author={Ma, Xinyin and Fang, Gongfan and Wang, Xinchao},
  journal={Advances in neural information processing systems},
  volume={36},
  pages={21702--21720},
  year={2023}
}

@article{mushtaq2021spider,
  title={Spider: Searching personalized neural architecture for federated learning},
  author={Mushtaq, Erum and He, Chaoyang and Ding, Jie and Avestimehr, Salman},
  journal={arXiv preprint arXiv:2112.13939},
  year={2021}
}

@article{schaeffer2024emergent,
  title={Are emergent abilities of large language models a mirage?},
  author={Schaeffer, Rylan and Miranda, Brando and Koyejo, Sanmi},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2024}
}

@inproceedings{soltani2021information,
  title={On the information of feature maps and pruning of deep neural networks},
  author={Soltani, Mohammadreza and Wu, Suya and Ding, Jie and Ravier, Robert and Tarokh, Vahid},
  booktitle={International Conference on Pattern Recognition (ICPR)},
  pages={6988--6995},
  year={2021},
  organization={IEEE}
}

@article{sun2023simple,
  title={A Simple and Effective Pruning Approach for Large Language Models},
  author={Sun, Mingjie and Liu, Zhuang and Bair, Anna and Kolter, J Zico},
  journal={arXiv preprint arXiv:2306.11695},
  year={2023}
}

@article{sun2024massive,
  title={Massive Activations in Large Language Models},
  author={Sun, Mingjie and Chen, Xinlei and Kolter, J Zico and Liu, Zhuang},
  journal={arXiv preprint arXiv:2402.17762},
  year={2024}
}

@article{yang2022theoretical,
  title={A theoretical understanding of neural network compression from sparse linear approximation},
  author={Yang, Wenjing and Wang, Ganghua and Ding, Jie and Yang, Yuhong},
  journal={arXiv preprint arXiv:2206.05604},
  year={2022}
}

@article{yuan2021mest,
  title={Mest: Accurate and fast memory-economic sparse training framework on the edge},
  author={Yuan, Geng and Ma, Xiaolong and Niu, Wei and Li, Zhengang and Kong, Zhenglun and Liu, Ning and Gong, Yifan and Zhan, Zheng and He, Chaoyang and Jin, Qing and others},
  journal={Advances in Neural Information Processing Systems},
  volume={34},
  pages={20838--20850},
  year={2021}
}

@article{zhang2023pruning,
  title={Pruning meets low-rank parameter-efficient fine-tuning},
  author={Zhang, Mingyang and Shen, Chunhua and Yang, Zhen and Ou, Linlin and Yu, Xinyi and Zhuang, Bohan and others},
  journal={arXiv preprint arXiv:2305.18403},
  year={2023}
}

