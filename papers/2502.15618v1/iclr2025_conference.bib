@inproceedings{li2024adaptive,
  title={Als: Adaptive layer sparsity for large language models via activation correlation assessment},
  author={Li, Wei and Li, Lujun and Lee, Mark and Sun, Shengjie},
  booktitle={Proceedings of Thirty-Eighth Annual Conference on Neural Information Processing Systems},
  year={2024}
}


@inproceedings{diao2019restricted,
  title={Restricted recurrent neural networks},
  author={Diao, Enmao and Ding, Jie and Tarokh, Vahid},
  booktitle={IEEE International Conference on Big Data},
  pages={56--63},
  year={2019},
  organization={IEEE}
}

@article{mushtaq2021spider,
  title={Spider: Searching personalized neural architecture for federated learning},
  author={Mushtaq, Erum and He, Chaoyang and Ding, Jie and Avestimehr, Salman},
  journal={arXiv preprint arXiv:2112.13939},
  year={2021}
}

@article{diao2022gal,
  title={GAL: Gradient assisted learning for decentralized multi-organization collaborations},
  author={Diao, Enmao and Ding, Jie and Tarokh, Vahid},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={11854--11868},
  year={2022}
}


@article{li2021ell,
  title={L1 Regularization in Two-Layer Neural Networks},
  author={Li, Gen and Gu, Yuantao and Ding, Jie},
  journal={IEEE Signal Processing Letters},
  volume={29},
  pages={135--139},
  year={2021},
  publisher={IEEE}
}

@inproceedings{soltani2021information,
  title={On the information of feature maps and pruning of deep neural networks},
  author={Soltani, Mohammadreza and Wu, Suya and Ding, Jie and Ravier, Robert and Tarokh, Vahid},
  booktitle={International Conference on Pattern Recognition (ICPR)},
  pages={6988--6995},
  year={2021},
  organization={IEEE}
}

@inproceedings{diao2020drasic,
  title={Drasic: Distributed recurrent autoencoder for scalable image compression},
  author={Diao, Enmao and Ding, Jie and Tarokh, Vahid},
  booktitle={Data Compression Conference (DCC)},
  pages={3--12},
  year={2020},
  organization={IEEE}
}

@article{MAP,
  title={{MAP}: Multi-Human-Value Alignment Palette},
  author={Wang, Xinran and Le, Qi and Ahmed, Ammar and Diao, Enmao and Zhou, Yi and Baracaldo, Nathalie and Ding, Jie and Anwar, Ali},
  journal={International Conference on Learning Representations (ICLR)},
  year={2025}
}


@article{wang2023demystifying,
  title={Demystifying poisoning backdoor attacks from a statistical perspective},
  author={Wang, Ganghua and Xian, Xun and Srinivasa, Jayanth and Kundu, Ashish and Bi, Xuan and Hong, Mingyi and Ding, Jie},
  journal={International Conference on Learning Representations},
  year={2024}
}

@article{xian2023unified,
  title={A unified detection framework for inference-stage backdoor defenses},
  author={Xian, Xun and Wang, Ganghua and Srinivasa, Jayanth and Kundu, Ashish and Bi, Xuan and Hong, Mingyi and Ding, Jie},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  pages={7867--7894},
  year={2023}
}

@inproceedings{xian2023understanding,
  title={Understanding backdoor attacks through the adaptability hypothesis},
  author={Xian, Xun and Wang, Ganghua and Srinivasa, Jayanth and Kundu, Ashish and Bi, Xuan and Hong, Mingyi and Ding, Jie},
  booktitle={International Conference on Machine Learning},
  pages={37952--37976},
  year={2023},
  organization={PMLR}
}

@article{RAG,
  title={On the Vulnerability of Retrieval-Augmented Generation within Knowledge-Intensive Application Domains},
  author={Xian, X. and Wang, G. and Bi, X. and Zhang, R. and Srinivasa, J. and Kundu, A. and Fleming C. and and Hong, M. and Ding, J.},
  journal={arXiv preprint arXiv:2409.17275},
  year={2025},
}

@article{yang2022theoretical,
  title={A theoretical understanding of neural network compression from sparse linear approximation},
  author={Yang, Wenjing and Wang, Ganghua and Ding, Jie and Yang, Yuhong},
  journal={arXiv preprint arXiv:2206.05604},
  year={2022}
}


@inproceedings{li2024discovering,
  title={Discovering Sparsity Allocation for Layer-wise Pruning of Large Language Models},
  author={Li, Lujun and Dong, Peijie and Tang, Zhenheng and Liu, Xiang and Wang, Qiang and Luo, Wenhan and Xue, Wei and Liu, Qifeng and Chu, Xiaowen and Guo, Yike},
  booktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},
  year={2024}
}

@inproceedings{dong2024pruner,
  title={Pruner-Zero: Evolving Symbolic Pruning Metric from Scratch for Large Language Models},
  author={Dong, Peijie and Li, Lujun and Tang, Zhenheng and Liu, Xiang and Pan, Xinglin and Wang, Qiang and Chu, Xiaowen},
  booktitle={Proceedings of the 41st International Conference on Machine Learning},
  year={2024},
  organization={PMLR},
  url={https://arxiv.org/abs/2406.02924},
  note={[arXiv: 2406.02924]}
}

@article{jaccard1912distribution,
  title={The distribution of the flora in the alpine zone. 1},
  author={Jaccard, Paul},
  journal={New phytologist},
  volume={11},
  number={2},
  pages={37--50},
  year={1912},
  publisher={Wiley Online Library}
}

@article{wang2024model,
  title={Model compression and efficient inference for large language models: A survey},
  author={Wang, Wenxiao and Chen, Wei and Luo, Yicong and Long, Yongliu and Lin, Zhengkai and Zhang, Liye and Lin, Binbin and Cai, Deng and He, Xiaofei},
  journal={arXiv preprint arXiv:2402.09748},
  year={2024}
}

@inproceedings{rasley2020deepspeed,
  title={Deepspeed: System optimizations enable training deep learning models with over 100 billion parameters},
  author={Rasley, Jeff and Rajbhandari, Samyam and Ruwase, Olatunji and He, Yuxiong},
  booktitle={Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery \& Data Mining},
  pages={3505--3506},
  year={2020}
}

@article{xiao2023efficient,
  title={Efficient streaming language models with attention sinks},
  author={Xiao, Guangxuan and Tian, Yuandong and Chen, Beidi and Han, Song and Lewis, Mike},
  journal={arXiv preprint arXiv:2309.17453},
  year={2023}
}

@inproceedings{lee2024infinigen,
  title={$\{$InfiniGen$\}$: Efficient Generative Inference of Large Language Models with Dynamic $\{$KV$\}$ Cache Management},
  author={Lee, Wonbeom and Lee, Jungi and Seo, Junghwan and Sim, Jaewoong},
  booktitle={18th USENIX Symposium on Operating Systems Design and Implementation (OSDI 24)},
  pages={155--172},
  year={2024}
}

@article{ba2016layer,
  title={Layer normalization},
  author={Ba, Jimmy Lei},
  journal={arXiv preprint arXiv:1607.06450},
  year={2016}
}

@article{zhang2019root,
  title={Root mean square layer normalization},
  author={Zhang, Biao and Sennrich, Rico},
  journal={Advances in Neural Information Processing Systems},
  volume={32},
  year={2019}
}

@article{liu2024scissorhands,
  title={Scissorhands: Exploiting the persistence of importance hypothesis for llm kv cache compression at test time},
  author={Liu, Zichang and Desai, Aditya and Liao, Fangshuo and Wang, Weitao and Xie, Victor and Xu, Zhaozhuo and Kyrillidis, Anastasios and Shrivastava, Anshumali},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2024}
}

@article{liu2024intactkv,
  title={IntactKV: Improving Large Language Model Quantization by Keeping Pivot Tokens Intact},
  author={Liu, Ruikang and Bai, Haoli and Lin, Haokun and Li, Yuening and Gao, Han and Xu, Zhengzhuo and Hou, Lu and Yao, Jun and Yuan, Chun},
  journal={arXiv preprint arXiv:2403.01241},
  year={2024}
}

@inproceedings{liu2023deja,
  title={Deja vu: Contextual sparsity for efficient llms at inference time},
  author={Liu, Zichang and Wang, Jue and Dao, Tri and Zhou, Tianyi and Yuan, Binhang and Song, Zhao and Shrivastava, Anshumali and Zhang, Ce and Tian, Yuandong and Re, Christopher and others},
  booktitle={International Conference on Machine Learning},
  pages={22137--22176},
  year={2023},
  organization={PMLR}
}

@article{song2023powerinfer,
  title={Powerinfer: Fast large language model serving with a consumer-grade gpu},
  author={Song, Yixin and Mi, Zeyu and Xie, Haotong and Chen, Haibo},
  journal={arXiv preprint arXiv:2312.12456},
  year={2023}
}

@inproceedings{he2019filter,
  title={Filter pruning via geometric median for deep convolutional neural networks acceleration},
  author={He, Yang and Liu, Ping and Wang, Ziwei and Hu, Zhilan and Yang, Yi},
  booktitle={Proceedings of the IEEE/CVF conference on computer vision and pattern recognition},
  pages={4340--4349},
  year={2019}
}

@inproceedings{hassibi1993optimal,
  title={Optimal brain surgeon and general network pruning},
  author={Hassibi, Babak and Stork, David G and Wolff, Gregory J},
  booktitle={IEEE international conference on neural networks},
  pages={293--299},
  year={1993},
  organization={IEEE}
}

@article{lecun1989optimal,
  title={Optimal brain damage},
  author={LeCun, Yann and Denker, John and Solla, Sara},
  journal={Advances in neural information processing systems},
  volume={2},
  year={1989}
}

@inproceedings{fang2023depgraph,
  title={Depgraph: Towards any structural pruning},
  author={Fang, Gongfan and Ma, Xinyin and Song, Mingli and Mi, Michael Bi and Wang, Xinchao},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={16091--16101},
  year={2023}
}

@article{li2016pruning,
  title={Pruning filters for efficient convnets},
  author={Li, Hao and Kadav, Asim and Durdanovic, Igor and Samet, Hanan and Graf, Hans Peter},
  journal={arXiv preprint arXiv:1608.08710},
  year={2016}
}

@inproceedings{liu2017learning,
  title={Learning efficient convolutional networks through network slimming},
  author={Liu, Zhuang and Li, Jianguo and Shen, Zhiqiang and Huang, Gao and Yan, Shoumeng and Zhang, Changshui},
  booktitle={Proceedings of the IEEE international conference on computer vision},
  pages={2736--2744},
  year={2017}
}

@article{liu2023sparsity,
  title={Sparsity May Cry: Let Us Fail (Current) Sparse Neural Networks Together!},
  author={Liu, Shiwei and Chen, Tianlong and Zhang, Zhenyu and Chen, Xuxi and Huang, Tianjin and Jaiswal, Ajay and Wang, Zhangyang},
  journal={arXiv preprint arXiv:2303.02141},
  year={2023}
}

@article{diao2023pruning,
  title={Pruning deep neural networks from a sparsity perspective},
  author={Diao, Enmao and Wang, Ganghua and Zhan, Jiawei and Yang, Yuhong and Ding, Jie and Tarokh, Vahid},
  journal={International Conference on Learning Representations (ICLR)},
  year={2023}
}
@article{han2015learning,
  title={Learning both weights and connections for efficient neural network},
  author={Han, Song and Pool, Jeff and Tran, John and Dally, William},
  journal={Advances in neural information processing systems},
  volume={28},
  year={2015}
}

@article{frantar2022gptq,
  title={GPTQ: Accurate post-training compression for generative pretrained transformers},
  author={Frantar, Elias and Ashkboos, Saleh and Hoefler, Torsten and Alistarh, Dan},
  journal={arXiv preprint arXiv:2210.17323},
  volume={1},
  year={2022}
}

@article{ainslie2023gqa,
  title={Gqa: Training generalized multi-query transformer models from multi-head checkpoints},
  author={Ainslie, Joshua and Lee-Thorp, James and de Jong, Michiel and Zemlyanskiy, Yury and Lebr{\'o}n, Federico and Sanghai, Sumit},
  journal={arXiv preprint arXiv:2305.13245},
  year={2023}
}

@article{schaeffer2024emergent,
  title={Are emergent abilities of large language models a mirage?},
  author={Schaeffer, Rylan and Miranda, Brando and Koyejo, Sanmi},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2024}
}

@inproceedings{chien2023reducing,
  title={Reducing the Carbon Impact of Generative AI Inference (today and in 2035)},
  author={Chien, Andrew A and Lin, Liuzixuan and Nguyen, Hai and Rao, Varsha and Sharma, Tristan and Wijayawardana, Rajini},
  booktitle={Proceedings of the 2nd Workshop on Sustainable Computer Systems},
  pages={1--7},
  year={2023}
}

@inproceedings{he2016deep,
  title={Deep residual learning for image recognition},
  author={He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={770--778},
  year={2016}
}

@article{sun2024massive,
  title={Massive Activations in Large Language Models},
  author={Sun, Mingjie and Chen, Xinlei and Kolter, J Zico and Liu, Zhuang},
  journal={arXiv preprint arXiv:2402.17762},
  year={2024}
}

@article{dettmers2023spqr,
  title={Spqr: A sparse-quantized representation for near-lossless llm weight compression},
  author={Dettmers, Tim and Svirschevski, Ruslan and Egiazarian, Vage and Kuznedelev, Denis and Frantar, Elias and Ashkboos, Saleh and Borzunov, Alexander and Hoefler, Torsten and Alistarh, Dan},
  journal={arXiv preprint arXiv:2306.03078},
  year={2023}
}

@article{kovaleva2021bert,
  title={BERT busters: Outlier dimensions that disrupt transformers},
  author={Kovaleva, Olga and Kulshreshtha, Saurabh and Rogers, Anna and Rumshisky, Anna},
  journal={arXiv preprint arXiv:2105.06990},
  year={2021}
}

@article{hoffmann2022training,
  title={Training compute-optimal large language models},
  author={Hoffmann, Jordan and Borgeaud, Sebastian and Mensch, Arthur and Buchatskaya, Elena and Cai, Trevor and Rutherford, Eliza and Casas, Diego de Las and Hendricks, Lisa Anne and Welbl, Johannes and Clark, Aidan and others},
  journal={arXiv preprint arXiv:2203.15556},
  year={2022}
}

@article{hu2021lora,
  title={Lora: Low-rank adaptation of large language models},
  author={Hu, Edward J and Shen, Yelong and Wallis, Phillip and Allen-Zhu, Zeyuan and Li, Yuanzhi and Wang, Shean and Wang, Lu and Chen, Weizhu},
  journal={arXiv preprint arXiv:2106.09685},
  year={2021}
}

@misc{llama3,
  author = {{Meta AI}},
  title = {{LLaMA-3}},
  howpublished = {\url{https://llama.meta.com/llama3/}},
  note = {Accessed: 2024-05-15},
  year = {2024}
}
@article{raffel2020exploring,
  title={Exploring the limits of transfer learning with a unified text-to-text transformer},
  author={Raffel, Colin and Shazeer, Noam and Roberts, Adam and Lee, Katherine and Narang, Sharan and Matena, Michael and Zhou, Yanqi and Li, Wei and Liu, Peter J},
  journal={Journal of machine learning research},
  volume={21},
  number={140},
  pages={1--67},
  year={2020}
}
@article{gao2021framework,
  title={A framework for few-shot language model evaluation},
  author={Gao, Leo and Tow, Jonathan and Biderman, Stella and Black, Sid and DiPofi, Anthony and Foster, Charles and Golding, Laurence and Hsu, Jeffrey and McDonell, Kyle and Muennighoff, Niklas and others},
  journal={Version v0. 0.1. Sept},
  pages={8},
  year={2021}
}

@misc{NVIDIA2024,
  author = {NVIDIA},
  title = {Deep Learning Performance: Matrix Multiplication},
  year = {2024},
  url = {https://docs.nvidia.com/deeplearning/performance/dl-performance-matrix-multiplication/index.html},
  note = {Accessed: 2024-05-15}
}


@article{choquette2021nvidia,
  title={Nvidia a100 tensor core gpu: Performance and innovation},
  author={Choquette, Jack and Gandhi, Wishwesh and Giroux, Olivier and Stam, Nick and Krashinsky, Ronny},
  journal={IEEE Micro},
  volume={41},
  number={2},
  pages={29--35},
  year={2021},
  publisher={IEEE}
}

@inproceedings{deri2003nprobe,
  title={nProbe: an open source netflow probe for gigabit networks},
  author={Deri, Luca and SpA, NETikos},
  booktitle={TERENA Networking Conference},
  pages={1--4},
  year={2003}
}

@inproceedings{malan1998extensible,
  title={An extensible probe architecture for network protocol performance measurement},
  author={Malan, G Robert and Jahanian, Farnam},
  booktitle={Proceedings of the ACM SIGCOMM'98 conference on Applications, technologies, architectures, and protocols for computer communication},
  pages={215--227},
  year={1998}
}

@inproceedings{clark-etal-2019-boolq,
    title = "{B}ool{Q}: Exploring the Surprising Difficulty of Natural Yes/No Questions",
    author = "Clark, Christopher  and
      Lee, Kenton  and
      Chang, Ming-Wei  and
      Kwiatkowski, Tom  and
      Collins, Michael  and
      Toutanova, Kristina",
    booktitle = "Proceedings of the 2019 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)",
    month = jun,
    year = "2019",
    address = "Minneapolis, Minnesota",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/N19-1300",
    doi = "10.18653/v1/N19-1300",
    pages = "2924--2936",
}

@inproceedings{Bisk2020piqa,
  author = {Yonatan Bisk and Rowan Zellers and
            Ronan Le Bras and Jianfeng Gao
            and Yejin Choi},
  title = {PIQA: Reasoning about Physical Commonsense in
           Natural Language},
  booktitle = {Thirty-Fourth AAAI Conference on
               Artificial Intelligence},
  year = {2020},
}

@inproceedings{zellers2019hellaswag,
    title={HellaSwag: Can a Machine Really Finish Your Sentence?},
    author={Zellers, Rowan and Holtzman, Ari and Bisk, Yonatan and Farhadi, Ali and Choi, Yejin},
    booktitle ={Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics},
    year={2019}
}

@misc{ai2:winogrande,
      title={WinoGrande: An Adversarial Winograd Schema Challenge at Scale}, 
      author={Keisuke Sakaguchi and Ronan Le Bras and Chandra Bhagavatula and Yejin Choi},
      year={2019},
      eprint={1907.10641},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@article{allenai:arc,
      author    = {Peter Clark  and Isaac Cowhey and Oren Etzioni and Tushar Khot and
                    Ashish Sabharwal and Carissa Schoenick and Oyvind Tafjord},
      title     = {Think you have Solved Question Answering? Try ARC, the AI2 Reasoning Challenge},
      journal   = {arXiv:1803.05457v1},
      year      = {2018},
}

@inproceedings{OpenBookQA2018,
 title={Can a Suit of Armor Conduct Electricity? A New Dataset for Open Book Question Answering},
 author={Todor Mihaylov and Peter Clark and Tushar Khot and Ashish Sabharwal},
 booktitle={EMNLP},
 year={2018}
}

@article{wang2020minilm,
  title={Minilm: Deep self-attention distillation for task-agnostic compression of pre-trained transformers},
  author={Wang, Wenhui and Wei, Furu and Dong, Li and Bao, Hangbo and Yang, Nan and Zhou, Ming},
  journal={Advances in Neural Information Processing Systems},
  volume={33},
  pages={5776--5788},
  year={2020}
}

@article{sanh2019distilbert,
  title={DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter},
  author={Sanh, Victor and Debut, Lysandre and Chaumond, Julien and Wolf, Thomas},
  journal={arXiv preprint arXiv:1910.01108},
  year={2019}
}

@article{lin2023awq,
  title={Awq: Activation-aware weight quantization for llm compression and acceleration},
  author={Lin, Ji and Tang, Jiaming and Tang, Haotian and Yang, Shang and Dang, Xingyu and Han, Song},
  journal={arXiv preprint arXiv:2306.00978},
  year={2023}
}

@article{dettmers2022gpt3,
  title={Gpt3. int8 (): 8-bit matrix multiplication for transformers at scale},
  author={Dettmers, Tim and Lewis, Mike and Belkada, Younes and Zettlemoyer, Luke},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={30318--30332},
  year={2022}
}

@article{luccioni2023estimating,
  title={Estimating the carbon footprint of bloom, a 176b parameter language model},
  author={Luccioni, Alexandra Sasha and Viguier, Sylvain and Ligozat, Anne-Laure},
  journal={Journal of Machine Learning Research},
  volume={24},
  number={253},
  pages={1--15},
  year={2023}
}

@article{anand2023gpt4all,
  title={Gpt4all: Training an assistant-style chatbot with large scale data distillation from gpt-3.5-turbo},
  author={Anand, Yuvanesh and Nussbaum, Zach and Duderstadt, Brandon and Schmidt, Benjamin and Mulyar, Andriy},
  journal={GitHub},
  year={2023}
}

@article{openai2023gpt,
  title={Gpt-4 technical report. arxiv 2303.08774},
  author={OpenAI, R},
  journal={View in Article},
  volume={2},
  number={5},
  year={2023}
}

@article{zhang2022opt,
  title={Opt: Open pre-trained transformer language models},
  author={Zhang, Susan and Roller, Stephen and Goyal, Naman and Artetxe, Mikel and Chen, Moya and Chen, Shuohui and Dewan, Christopher and Diab, Mona and Li, Xian and Lin, Xi Victoria and others},
  journal={arXiv preprint arXiv:2205.01068},
  year={2022}
}

@article{touvron2023llama,
  title={Llama 2: Open foundation and fine-tuned chat models},
  author={Touvron, Hugo and Martin, Louis and Stone, Kevin and Albert, Peter and Almahairi, Amjad and Babaei, Yasmine and Bashlykov, Nikolay and Batra, Soumya and Bhargava, Prajjwal and Bhosale, Shruti and others},
  journal={arXiv preprint arXiv:2307.09288},
  year={2023}
}

@article{diao2024cola,
  title={{ColA}: Collaborative Adaptation with Gradient Learning},
  author={Diao, Enmao and Le, Qi and Wu, Suya and Wang, Xinran and Anwar, Ali and Ding, Jie and Tarokh, Vahid},
  journal={arXiv preprint arXiv:2404.13844},
  year={2024}
}

@article{zhang2023pruning,
  title={Pruning meets low-rank parameter-efficient fine-tuning},
  author={Zhang, Mingyang and Shen, Chunhua and Yang, Zhen and Ou, Linlin and Yu, Xinyi and Zhuang, Bohan and others},
  journal={arXiv preprint arXiv:2305.18403},
  year={2023}
}

@article{ma2023llm,
  title={Llm-pruner: On the structural pruning of large language models},
  author={Ma, Xinyin and Fang, Gongfan and Wang, Xinchao},
  journal={Advances in neural information processing systems},
  volume={36},
  pages={21702--21720},
  year={2023}
}

@article{mishra2021accelerating,
  title={Accelerating sparse deep neural networks},
  author={Mishra, Asit and Latorre, Jorge Albericio and Pool, Jeff and Stosic, Darko and Stosic, Dusan and Venkatesh, Ganesh and Yu, Chong and Micikevicius, Paulius},
  journal={arXiv preprint arXiv:2104.08378},
  year={2021}
}



@inproceedings{xie2017aggregated,
  title={Aggregated residual transformations for deep neural networks},
  author={Xie, Saining and Girshick, Ross and Doll{\'a}r, Piotr and Tu, Zhuowen and He, Kaiming},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={1492--1500},
  year={2017}
}

@article{vaswani2017attention,
  title={Attention is all you need},
  author={Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, {\L}ukasz and Polosukhin, Illia},
  journal={Advances in neural information processing systems},
  volume={30},
  year={2017}
}

@article{brown2020language,
  title={Language models are few-shot learners},
  author={Brown, Tom and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared D and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and others},
  journal={Advances in neural information processing systems},
  volume={33},
  pages={1877--1901},
  year={2020}
}


@article{yuan2021mest,
  title={Mest: Accurate and fast memory-economic sparse training framework on the edge},
  author={Yuan, Geng and Ma, Xiaolong and Niu, Wei and Li, Zhengang and Kong, Zhenglun and Liu, Ning and Gong, Yifan and Zhan, Zheng and He, Chaoyang and Jin, Qing and others},
  journal={Advances in Neural Information Processing Systems},
  volume={34},
  pages={20838--20850},
  year={2021}
}

@article{sun2023simple,
  title={A Simple and Effective Pruning Approach for Large Language Models},
  author={Sun, Mingjie and Liu, Zhuang and Bair, Anna and Kolter, J Zico},
  journal={arXiv preprint arXiv:2306.11695},
  year={2023}
}

@inproceedings{frantar2023sparsegpt,
  title={Sparsegpt: Massive language models can be accurately pruned in one-shot},
  author={Frantar, Elias and Alistarh, Dan},
  booktitle={International Conference on Machine Learning},
  pages={10323--10337},
  year={2023},
  organization={PMLR}
}

@article{merity2016pointer,
  title={Pointer sentinel mixture models},
  author={Merity, Stephen and Xiong, Caiming and Bradbury, James and Socher, Richard},
  journal={arXiv preprint arXiv:1609.07843},
  year={2016}
}

@article{dingFinetuningLargescale2023,
  title = {Parameter-Efficient Fine-Tuning of Large-Scale Pre-Trained Language Models},
  author = {Ding, Ning and Qin, Yujia and Yang, Guang and Wei, Fuchao and Yang, Zonghan and Su, Yusheng and Hu, Shengding and Chen, Yulin and Chan, Chi-Min and Chen, Weize and Yi, Jing and Zhao, Weilin and Wang, Xiaozhi and Liu, Zhiyuan and Zheng, Hai-Tao and Chen, Jianfei and Liu, Yang and Tang, Jie and Li, Juanzi and Sun, Maosong},
  year = {2023},
  month = mar,
  journal = {Nature Machine Intelligence},
  volume = {5},
  number = {3},
  pages = {220--235},
  issn = {2522-5839},
  doi = {10.1038/s42256-023-00626-4},
}

@article{an2023fluctuation,
  title={Fluctuation-based Adaptive Structured Pruning for Large Language Models},
  author={An, Yongqi and Zhao, Xu and Yu, Tao and Tang, Ming and Wang, Jinqiao},
  journal={Proceedings of the AAAI Conference on Artificial Intelligence},
  year={2024}
}



@InProceedings{pmlr-v202-frantar23a,
  title = 	 {{S}parse{GPT}: Massive Language Models Can be Accurately Pruned in One-Shot},
  author =       {Frantar, Elias and Alistarh, Dan},
  booktitle = 	 {Proceedings of the 40th International Conference on Machine Learning},
  pages = 	 {10323--10337},
  year = 	 {2023},
  editor = 	 {Krause, Andreas and Brunskill, Emma and Cho, Kyunghyun and Engelhardt, Barbara and Sabato, Sivan and Scarlett, Jonathan},
  volume = 	 {202},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {23--29 Jul},
  publisher =    {PMLR},
  pdf = 	 {https://proceedings.mlr.press/v202/frantar23a/frantar23a.pdf},
  url = 	 {https://proceedings.mlr.press/v202/frantar23a.html},
}



@InProceedings{pmlr-v139-hubara21a,
  title = 	 {Accurate Post Training Quantization With Small Calibration Sets},
  author =       {Hubara, Itay and Nahshan, Yury and Hanani, Yair and Banner, Ron and Soudry, Daniel},
  booktitle = 	 {Proceedings of the 38th International Conference on Machine Learning},
  pages = 	 {4466--4475},
  year = 	 {2021},
  editor = 	 {Meila, Marina and Zhang, Tong},
  volume = 	 {139},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {18--24 Jul},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v139/hubara21a/hubara21a.pdf},
  url = 	 {https://proceedings.mlr.press/v139/hubara21a.html},
}

@inproceedings{wang-etal-2020-structured,
    title = "Structured Pruning of Large Language Models",
    author = "Wang, Ziheng  and
      Wohlwend, Jeremy  and
      Lei, Tao",
    editor = "Webber, Bonnie  and
      Cohn, Trevor  and
      He, Yulan  and
      Liu, Yang",
    booktitle = "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)",
    month = nov,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.emnlp-main.496",
    doi = "10.18653/v1/2020.emnlp-main.496",
    pages = "6151--6162",
}

@article{sanh2020movement,
  title={Movement pruning: Adaptive sparsity by fine-tuning},
  author={Sanh, Victor and Wolf, Thomas and Rush, Alexander},
  journal={Advances in neural information processing systems},
  volume={33},
  pages={20378--20389},
  year={2020}
}

@inproceedings{he2020learning,
  title={Learning filter pruning criteria for deep convolutional neural networks acceleration},
  author={He, Yang and Ding, Yuhang and Liu, Ping and Zhu, Linchao and Zhang, Hanwang and Yang, Yi},
  booktitle={Proceedings of the IEEE/CVF conference on computer vision and pattern recognition},
  pages={2009--2018},
  year={2020}
}