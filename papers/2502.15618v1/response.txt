\section{Related Work}
\vspace{-0.3cm}
\paragraph{Pruning with Calibration Dataset.} Pruning methods can be broadly classified into two categories**Li, "Unsupervised Sparse Neural Networks"**__**Frankle et al., "The Lottery Ticket Hypothesis"**: \textit{unstructured pruning} and \textit{structured pruning}. Unstructured pruning**Molchanov et al., "Regularizing Neural Networks by Preventing Features from Dying Out"** removes individual weights, whereas structured pruning**Guo et al., "Resource Efficient Training Requirements for Deeper Residual Networks"** removes complete structures from the model, such as channels and attention heads. Pruning LLMs often involves calibration datasets due to the emergence of outliers in their internal representations. For unstructured pruning, SparseGPT**Kusupati et al., "Sparse GPT: Efficient Training of Large Language Models through Adaptive Sparse Sampling"** uses synchronized second-order Hessian updates to solve row-wise weight reconstruction problems and update weights. Wanda**Tang et al., "Wanda: A Pruning Metric for Neural Networks"** introduces a pruning metric that considers both the magnitude of weights and activation values to determine which weights to prune. For structured pruning, FLAP**Liu et al., "FLAP: Efficient Weight Channel Pruning via Fast Locality-Aware Fluctuation"** introduces a fluctuation metric to decide which weight channels to prune. LLM-Pruner**Ye et al., "LLM-Pruner: Effective Pruning for Large Language Models"** employs approximated second-order Taylor expansions of the error function to remove the least important coupled structures and then applies fine-tuning to recover model performance. LoRA-Prune**Wu et al., "LoRA-Prune: Efficient Weight Channel Pruning via Learning Orthogonal Regularization"** uses a LoRA**Kaplan et al., "Learning Orthogonal Representations for Neural Machine Translation"**-guided pruning metric that leverages the weights and gradients of LoRA to direct the iterative process of pruning and tuning. However, the fine-tuning process requires substantial computational resources**Guo et al., "Regularizing Neural Networks by Preventing Features from Dying Out"**, and we have found that fine-tuning might cause LLMs to lose their generalizability; for example, they may perform worse on certain downstream tasks, such as commonsense reasoning tasks.
\vspace{-0.3cm}
\paragraph{Large-Magnitude Outliers of LLMs.} Unlike small neural networks, LLMs exhibit large-magnitude outlier features**Li et al., "Understanding the Impact of Large Model Depth on Training and Inference"**. **Wang et al., "Understanding and Improving Weight Distribution in Deep Neural Networks"** shows that these large-magnitude features begin to emerge when the size of LLMs exceeds 6.7 billion parameters, and these outlier features are concentrated in certain channels. The phenomenon of massive activations**Chen et al., "On the Power of Very Deep Models for Small-Scale Problems"** has been observed, where a few activations exhibit significantly larger values than others, potentially leading to the concentration of attention probabilities on their corresponding tokens. These emergent properties suggest the need to customize the pruning of channels for different batches to maintain model performance. This observation motivates us to propose Probe Pruning.
\begin{figure*}
    \centering
    \includegraphics[width=1\linewidth]{Figures/main.pdf}
    \caption{Probe Pruning (PP) is executed in four stages: (1) PP selects key samples and tokens from the layer-normalized hidden states, based on residual importance, to create a \textit{small yet crucial} probe. (2) PP deploys this probe to run a few model layers ahead and obtains the probe's intermediate hidden states. (3) PP integrates the probing states with historical states and uses the integrated states to calculate the pruning metric and prune weight channels. (4) PP performs full inference on the remaining weights.}
    \vspace{-0.5cm}
    \label{fig:main_figure}
\end{figure*}
\vspace{-0.3cm}