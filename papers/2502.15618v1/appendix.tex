\centerline{\LARGE Appendix for ``Probe Pruning''}
\section{Implementation Details} \label{appendix:implementation_details}
\vspace{-0.3cm}
For all methods, we leave the first three layers unchanged, similar to~\cite{ma2023llm, zhang2023pruning}, because pruning parameters in these layers has a substantial impact on the model. The pruning ratio represents the average pruning ratio across all attention and MLP blocks in the model. For instance, when targeting pruning ratios of 20\% and 40\% for LLaMA-2-7B, we prune 22\% and 44\% from attention and MLP blocks 4 to 32, respectively.

For a fair comparison, we utilize the exact same subset of the C4~\cite{raffel2020exploring} dataset as the calibration dataset.

For PP, FLAP~\cite{an2023fluctuation}, and Wanda-sp~\cite{an2023fluctuation}, we use 2,000 samples with sequence lengths of 1,024 tokens as the calibration dataset for the text generation task, and 2,000 samples with sequence lengths of 512 tokens for the commonsense reasoning task.

For LLM-Pruner~\cite{ma2023llm}, we follow the original implementation details in~\cite{ma2023llm}. We use 10 randomly selected samples, each truncated to a length of 128 tokens, to build importance metrics, and 20,000 samples with sequence lengths of 256 tokens for recovery retraining. Specifically, in the recovery stage, we employ the AdamW~\cite{he2020learning} optimizer with 100 warmup steps, set the LoRA~\cite{hu2021lora} rank $r$ to 8, use a learning rate of $1\times10^{-4}$, a batch size of 64, and perform recovery retraining for 2 epochs.

For LoRAPrune~\cite{zhang2023pruning}, we follow the original implementation details in~\cite{zhang2023pruning}. We randomly sample 20,000 sentences from the C4 dataset, each having a length of 512 tokens, according to the original calibration dataset preparation process. The training hyperparameters include setting the LoRA rank to 8, a learning rate of $1\times10^{-4}$, a batch size of 128, and a total of 2 training epochs. When fusing pruning with fine-tuning, we employ a cubic sparsity scheduler~\cite{sanh2020movement} to iteratively prune the model until we reach the target sparsity. When only pruning is performed, with no tuning conducted to match other one-shot pruning methods, we use 10 selected samples with sequence lengths of 512 tokens to estimate importance and perform one-shot pruning with no weight updates. All training processes are optimized using the AdamW optimizer with a linear learning rate decay.
% \newpage
\section{Ablation Studies} \label{appendix:ablations}
In this section, we present various ablation studies. Section~\ref{appendix-flap:calibration_dataset} investigates how different calibration datasets influence the fix-pruned model, which relies exclusively on such calibration dataset. Section~\ref{appendix-flap:square_attention} evaluates the effect of manually squaring the attention metric in the FLAP model~\cite{an2023fluctuation} versus not squaring it. Section~\ref{appendix-pp: batch-dependent} illustrates the batch-dependent outliers at token positions. Section~\ref{appendix-pp:residual_importance} studies the effectiveness of residual importance. Section~\ref{appendix-pp:historical_data_integration} studies the integration of historical states and their influence on the performance of Probe Pruning (PP). Section~\ref{appendix-pp:parallel_probing} verifies the possibility of running the probe in parallel with the actual computation of earlier pruned blocks. Section~\ref{appendix-pp:discrepency_between_attention_mlp} analyzes the discrepancies between pruning the attention and MLP blocks at varying pruning ratios.
\subsection{Calibration Dataset} \label{appendix-flap:calibration_dataset}
We present the performance of FLAP~\cite{an2023fluctuation} using different calibration datasets to test WikiText2 Perplexity, as shown in Table~\ref{tab-appendix:ablation_flap_calibration_comparison}. The results indicate that structured pruning methods, which rely solely on calibration datasets, may introduce biases. For instance, when using the WikiText2 validation set as a calibration dataset, FLAP achieves a perplexity of 18.5 at a 40\% pruning ratio on WikiText2. However, with the C4 dataset as the calibration dataset, the perplexity deteriorates to 38.9.
\begin{table}[ht!]
\centering
\caption{Comparison of FLAP performance at different pruning ratios and calibration datasets on LLaMA-2-7B and LLaMA-2-13B models.}
\label{tab-appendix:ablation_flap_calibration_comparison}
\renewcommand{\arraystretch}{1.1}
\resizebox{0.8\columnwidth}{!}{%
\begin{tabular}{ccccc}
\toprule
Method                & Pruning Ratio            & Calibration Dataset   & LLaMA-2-7B         & LLaMA-2-13B         \\ \midrule
\multirow{4}{*}{FLAP} &  20\%                        & C4                    & 10.30(0.1)         & 7.5(0.1)   \\
                      &  20\% & WikiText2 - validation & \textbf{7.9(0.1)}  & \textbf{6.5(0.1)}\\ \cmidrule(l){2-5}  
                      
                      &  40\%                        & C4                    & 38.9(1.3)          & 15.5(0.0)                          \\
                      &  40\% & WikiText2 - validation & \textbf{18.5(0.2)} & \textbf{10.5(0.1)}      \\ \bottomrule
\end{tabular}%
}
\end{table}
\begin{table}[h!]
\centering
\caption{Comparasion of FLAP with and without squaring the attention metric, while keeping the MLP metric consistently unsquared, on LLaMA-2-7B and LLaMA-2-13B Models.}
\label{tab-appendix:ablation_flap_attention}
\renewcommand{\arraystretch}{1.2}
\resizebox{\columnwidth}{!}{%
\begin{tabular}{ccccc|ccc}
\toprule
                &                          & \multicolumn{3}{c|}{LLaMA-2-7B}                           & \multicolumn{3}{c}{LLaMA-2-13B}                         \\ \cmidrule(l){3-8} 
Method          & Pruning Ratio            & Attention Pruning Ratio & MLP Pruning Ratio & WikiText2   & Attention Pruning Ratio & MLP Pruning Ratio & WikiText2 \\ \midrule
FLAP w/o square & 20\% & 17.8\%(0.1)             & 21.3\%(0.1)      & 10.3(0.1)   & 24.7\%(0.1)             & 18.0\%(0.1)       & \textbf{7.5(0.1)}  \\
FLAP   &  20\%                        & 0.6\%(0.1)              & 30.8\%(0.1)       & \textbf{9.1(0.1) }   & 0.0\%(0.0)                & 31.5\%(0.1)       & 7.7(0.1)  \\ \midrule
FLAP w/o square & 40\% & 35.4\%(0.1)             & 42.6\%(0.1)       & 38.9(1.3)   & 37.5\%(0.1)             & 41,0\%(0.1)       & 15.5(0.0) \\
FLAP   &  40\%                        & 17.6\%(0.1)             & 52.6\%(0.1)       & \textbf{29.1(0.4})   & 11.4\%(0.1)             & 55.6\%(0.1)       & \textbf{13.6(0.1)} \\ \bottomrule
\end{tabular}%
}
\end{table}
\subsection{Manually Squaring the Attention Metric} \label{appendix-flap:square_attention}
In the FLAP implementation available at \url{https://github.com/CASIA-IVA-Lab/FLAP}, the attention metric is manually squared. Table~\ref{tab-appendix:ablation_flap_attention} demonstrates the impact of manually squaring the attention metric in FLAP versus not squaring it. The findings indicate that squaring the metric results in less aggressive pruning of attention blocks. For instance, with LLaMA-2-7B at a 20\% overall pruning ratio, the non-squared FLAP method prunes 17.8\% of attention weights, in contrast to only 0.6\% when squaring is implemented. This implies that squaring significantly mitigates attention pruning.

Additionally, less aggressive pruning of attention blocks correlates with better model performance. Specifically, on LLaMA-2-7B at a 40\% overall pruning ratio, non-squared FLAP prunes 35.4\% of attention weights, resulting in a WikiText2 perplexity of 38.9. Conversely, squared FLAP prunes at a reduced rate of 17.6\%, achieving a lower perplexity of 29.1. These outcomes suggest that more conservative pruning of attention blocks can enhance model performance.
% \begin{table}[h!]
% \centering
% \caption{Comparasion of FLAP with and without squaring the attention metric, while keeping the MLP metric consistently unsquared, on LLaMA-2-7B and LLaMA-2-13B Models.}
% \label{tab-appendix:ablation_flap_attention}
% \renewcommand{\arraystretch}{1.2}
% \resizebox{\columnwidth}{!}{%
% \begin{tabular}{ccccc|ccc}
% \toprule
%                 &                          & \multicolumn{3}{c|}{LLaMA-2-7B}                           & \multicolumn{3}{c}{LLaMA-2-13B}                         \\ \cmidrule(l){3-8} 
% Method          & Pruning Ratio            & Attention Pruning Ratio & MLP Pruning Ratio & WikiText2   & Attention Pruning Ratio & MLP Pruning Ratio & WikiText2 \\ \midrule
% FLAP w/o square & 20\% & 17.8\%(0.1)             & 21.3\%(0.1)      & 10.3(0.1)   & 24.7\%(0.1)             & 18.0\%(0.1)       & \textbf{7.5(0.1)}  \\
% FLAP   &  20\%                        & 0.6\%(0.1)              & 30.8\%(0.1)       & \textbf{9.1(0.1) }   & 0.0\%(0.0)                & 31.5\%(0.1)       & 7.7(0.1)  \\ \midrule
% FLAP w/o square & 40\% & 35.4\%(0.1)             & 42.6\%(0.1)       & 38.9(1.3)   & 37.5\%(0.1)             & 41,0\%(0.1)       & 15.5(0.0) \\
% FLAP   &  40\%                        & 17.6\%(0.1)             & 52.6\%(0.1)       & \textbf{29.1(0.4})   & 11.4\%(0.1)             & 55.6\%(0.1)       & \textbf{13.6(0.1)} \\ \bottomrule
% \end{tabular}%
% }
% \end{table}
\subsection{Batch-Dependent Outliers at Token Positions} \label{appendix-pp: batch-dependent}
\begin{figure}[h]
    \centering
    \begin{subfigure}[b]{0.48\textwidth}
        \centering
        \includegraphics[width=\textwidth]{Figures/layer10batchnature.png} % Example image
        \caption{Layer 10} % Subcaption for the left figure
        \label{fig:layer10}
    \end{subfigure}\hfill % Ensures that the space between the figures can adjust
    \begin{subfigure}[b]{0.48\textwidth}
        \centering
        \includegraphics[width=\textwidth]{Figures/layer10batchnature.png} % Example image
        \caption{Layer 20} % Subcaption for the right figure
        \label{fig:layer20}
    \end{subfigure}
    \caption{Visualization of the $\normltwo$ norm for each token position of the input hidden states at layers 10 and 20 across the batch and feature dimensions. Experiments are conducted on the LLaMA-2-7B model using the WikiText2 dataset.} % The main caption for both subfigures
    \label{fig:batch-dependent}
\end{figure}
In the main text Section~\ref{introduction}, we stated that large language models (LLMs) exhibit batch-dependent outliers, necessitating online dynamic pruning to address these dynamic outliers. Figure~\ref{fig:batch-dependent} presents the calculated $\normltwo$ norms for each token position of the input hidden states at layers 10 and 20 across the batch and feature dimensions. The results demonstrate the presence of batch-dependent outliers at each token position, aligning with the observations from existing works~\cite{liu2024intactkv, sun2024massive}.

\subsection{Residual Importance} \label{appendix-pp:residual_importance}
In the main text Section~\ref{section:probe_generation}, we noted that layer normalization significantly alters the input hidden states, thereby preventing their importance from accurately identifying key samples and tokens. To validate this observation, Table~\ref{tab:ablation_residual_importance} compares the effectiveness of identifying key samples and tokens based on residual importance with identification based on the importance of layer-normalized input hidden states (PP without residual importance). The experimental results demonstrate the effectiveness of residual importance. 
\begin{table}[ht!]
\centering
\caption{Impact of residual importance on probe generation for LLaMA-2-7B. Applying residual importance results in better probe performance.}
\vspace{-0.3cm}
\label{tab:ablation_residual_importance}
\renewcommand{\arraystretch}{1.2}
\resizebox{\columnwidth}{!}{%
\begin{tabular}{ccc|cccccccc}
\toprule
Method           & Pruning Ratio & WikiText2          & BoolQ              & PIQA               & HellaSwag          & WinoGrande         & ARC-c              & ARC-e              & OBQA               & Average       \\ \midrule
PP w/o residual importance & 20\%       & 10.3(0.0)          & 64.3(0.1)          & 74.2(0.2)          & 55.3(0.1)          & 53.4(0.5)          & 32.1(0.2)          & 55.6(0.1)          & 40.2(0.2)          & 53.6          \\
\gr PP               & 20\%       & \textbf{8.1(0.1)}  & \textbf{69.0(0.1)} & \textbf{78.1(0.0)} & \textbf{73.5(0.0)} & \textbf{66.7(0.3)} & \textbf{42.8(0.1)} & \textbf{68.5(0.0)} & \textbf{40.9(0.2)} & \textbf{62.8} \\ \midrule
PP w/o residual importance & 40\%       & 37.1(0.4)          & 62.1(0.0)          & 61.1(0.0)          & 31.0(0.0)          & 50.2(0.1)          & 20.4(0.2)          & 34.4(0.2)          & 36.7(0.3)          & 42.3          \\
\gr PP               & 40\%       & \textbf{16.8(0.1)} & \textbf{62.7(0.2)} & \textbf{74.9(0.1)} & \textbf{63.6(0.0)} & \textbf{57.5(0.2)} & \textbf{35.5(0.1)} & \textbf{61.7(0.2)} & \textbf{40.3(0.4)} & \textbf{56.6} \\ \bottomrule
\end{tabular}%
}
\end{table}

\subsection{Historical States Integration} \label{appendix-pp:historical_data_integration}
In Table~\ref{tab-appendix:ablation_historical_data}, the results illustrate how incorporating historical states into the pruning decision process enhances the effectiveness of PP. Specifically, when PP leverages historical states, there is a consistent improvement in performance metrics across all models and pruning ratios compared to scenarios where only probing states are utilized (PP w/o historical states). For instance, at a 40\% pruning ratio, using a probe generated from 5\% of the batch and 50\% of the sequence, PP with historical states reduces the perplexity on WikiText2 from 20.1 to 16.9 and improves the average accuracy from 51.2\% to 56.6\%, compared to using only the current probing states without historical data.
\begin{table}[h!]
\centering
\caption{Performance of integrating historical states under different probe combinations on LLaMA-2-7B. historical states can enhance PP performance.}
\label{tab-appendix:ablation_historical_data}
\renewcommand{\arraystretch}{1.2}
\resizebox{\columnwidth}{!}{%
\begin{tabular}{cccc|cccccccc}
\toprule
Method                 & Probe Generation    & Pruning Ratio & WikiText2 & BoolQ     & PIQA      & HellaSwag & WinoGrande & ARC-c     & ARC-e     & OBQA      & Average \\ \midrule
PP w/o historical states & 5\% batch, 50\% seq & 20\%       & 8.2(0.1)  & 68.4(0.0) & 75.8(0.0) & 70.4(0.0) & 63.2(0.0)  & 38.9(0.0) & 64.4(0.0) & 42.2(0.0) & 60.5    \\
PP w/o historical states & 10\% batch          & 20\%       & 7.9(0.1)  & 69.8(0.0) & 75.7(0.0) & 70.7(0.0) & 63.7(0.0)  & 39.2(0.0) & 64.9(0.0) & 41.4(0.0) & 60.8    \\
PP w/o historical states & 20\% batch          & 20\%       & 7.7(0.1)  & 69.3(0.0) & 76.7(0.0) & 70.9(0.0) & 63.8(0.0)  & 40.1(0.0) & 65.4(0.0) & 40.2(0.0) & 60.9    \\
PP                     & 5\% batch, 50\% seq & 20\%       & 8.2(0.1)  & 69.0(0.1) & 78.1(0.0) & 73.5(0.0) & 66.7(0.3)  & 42.8(0.1) & 68.5(0.0) & 40.9(0.2) & 62.8    \\
PP                     & 10\% batch          & 20\%       & 8.0(0.1)  & 67.3(0.1) & 77.8(0.1) & 73.7(0.0) & 64.8(0.1)  & 41.5(0.1) & 67.4(0.2) & 41.3(0.3) & 62      \\
PP                     & 20\% batch          & 20\%       & 7.8(0.1)  & 68.1(0.1) & 77.5(0.1) & 73.7(0.0) & 66.7(0.3)  & 42.2(0.1) & 68.2(0.1) & 42.7(0.4) & 62.7    \\ \midrule
PP w/o historical states & 5\% batch, 50\% seq & 40\%       & 20.1(0.3) & 57.4(0.0) & 71.3(0.0) & 55.7(0.0) & 54.6(0.0)  & 31.7(0.0) & 53.3(0.0) & 34.6(0.0) & 51.2    \\
PP w/o historical states & 10\% batch          & 40\%       & 17.2(0.4) & 62.1(0.0) & 72.1(0.0) & 56.9(0.0) & 58.3(0.0)  & 34.3(0.0) & 57.9(0.0) & 35.4(0.0) & 53.9    \\
PP w/o historical states & 20\% batch          & 40\%       & 15.6(0.2) & 63.8(0.0) & 72.3(0.0) & 57.6(0.0) & 56.5(0.0)  & 33.5(0.0) & 57.7(0.0) & 36.0(0.0) & 53.9    \\
PP                     & 5\% batch, 50\% seq & 40\%       & 16.9(0.1) & 62.7(0.2) & 74.9(0.1) & 63.6(0.0) & 57.5(0.2)  & 35.5(0.1) & 61.7(0.2) & 40.3(0.4) & 56.6    \\
PP                     & 10\% batch          & 40\%       & 15.8(0.3) & 64.3(0.1) & 74.5(0.1) & 64.2(0.1) & 57.9(0.4)  & 37.6(0.1) & 62.9(0.2) & 40.7(1.1) & 57.4    \\
PP                     & 20\% batch          & 40\%       & 15.1(0.2) & 64.7(0.1) & 74.3(0.1) & 64.4(0.1) & 58.1(0.3)  & 37.7(0.3) & 62.5(0.1) & 41.3(0.2) & 57.6    \\ \bottomrule
\end{tabular}%
}
\end{table}

\subsection{Parallel Probing}  \label{appendix-pp:parallel_probing}
We verify the possibility of running the probe in parallel with the actual computation of earlier pruned blocks. We present the results in Table~\ref{tab:PP-parallel} below. Here, \textbf{PP-Parallel} represents the approach where, when the actual computation happens on earlier pruned blocks, we generate the probe from the residuals of these earlier pruned blocks and perform the probing. \textbf{PP} represents the default setting used in the main text of the paper. The results show that we can still obtain performance gains and achieve comparable results to PP. For example, at a 40\% pruning ratio, PP-Parallel achieves a perplexity of 17.9 on WikiText2, which is close to that of PP and much lower than the 38.9 achieved by FLAP. Furthermore, PP-Parallel achieves 61.4\% accuracy on ARC-e, which is close to that of PP and much higher than the 52.5\% achieved by FLAP. However, we are just demonstrating the feasibility of further improving PP's inference speed here; the actual parallelism is hardware-dependent and implementation-dependent.
\begin{table}[!h]
\centering
\caption{Zero-shot performance of LLaMA-2-7B after pruning attention and MLP blocks without fine-tuning.}
\label{tab:PP-parallel}
\resizebox{0.5\textwidth}{!}{ % Scales the table to 60% of its original width
\renewcommand{\arraystretch}{1.05}
\begin{tabular}{ccc|c}
\toprule
Method             & Pruning Ratio & WikiText2 & ARC-e     \\ \midrule
Dense              & 0\%           & 6.0(0.1)  & 67.3(0.0) \\ \midrule
Full-Batch Probing & 20\%          & 7.3(0.1)  & 67.2(0.0) \\
Wanda-sp           & 20\%          & 10.6(0.1) & 63.9(0.3) \\
FLAP               & 20\%          & 10.3(0.1) & 63.1(0.1) \\
PP-Parallel        & 20\%          & \textbf{8.1(0.1)}  & 67.9(0.1) \\
\gr PP                 & 20\%          & \textbf{8.1(0.1)}  & \textbf{68.5(0.0)} \\ \midrule
Full-Batch Probing & 40\%          & 13.6(0.1) & 64.7(0.0) \\
Wanda-sp           & 40\%          & 43.8(1.5) & 54.4(0.1) \\
FLAP               & 40\%          & 38.9(1.3) & 52.5(0.2) \\
PP-Parallel        & 20\%          & 17.9(0.1) & 61.4(0.2) \\
\gr PP                 & 40\%          & \textbf{16.8(0.1)} & \textbf{61.7(0.2)} \\ \bottomrule
\end{tabular}
}
\end{table}
\subsection{Discrepency between Pruning Attention and MLP.} \label{appendix-pp:discrepency_between_attention_mlp}
\begin{table}[t]
% \vspace{-16cm}
\centering
\caption{Performance of pruning attention heads versus MLPs at different ratios on LLaMA-2-7B, comparing the effects of pruning only the attention heads or only the MLPs.}
\label{tab:differentattnmlpratio}
\renewcommand{\arraystretch}{1.05}
\resizebox{\columnwidth}{!}{%
\begin{tabular}{cccc|cccccccc}
\toprule
\multicolumn{3}{c}{Pruning Ratio}     & Text Generation $\downarrow$   & \multicolumn{8}{c}{Commonsense Reasoning $\uparrow$}                                                                                                                        \\ \midrule
Attention & MLP     & All          & WikiText2          & BoolQ              & PIQA               & HellaSwag          & WinoGrande         & ARC-c              & ARC-e              & OBQA               & Average       \\ \midrule
0\%    & 0\%  & 0\%           & 6.0(0.1)           & 74.6(0.0)          & 77.9(0.0)          & 75(0.0)            & 67.7(0.0)          & 42.7(0.0)          & 67.3(0.0)          & 42.6(0.0)          & 64.0            \\ \midrule
20\%   & 0\%  & 7\%           & \textbf{6.8(0.1)}  & \textbf{71.1(0.1)} & \textbf{78.6(0.1)} & \textbf{74.7(0.0)} & 66.3(0.1)          & \textbf{42.9(0.0)} & \textbf{69.0(0.1)} & \textbf{43.1(0.1)} & \textbf{63.7} \\ 
0\%    & 20\% & \textbf{13\%} & 7.2(0.1)           & 68.4(0.1)          & 77.7(0.0)          & 74.3(0.0)          & \textbf{67.8(0.1)} & 41.8(0.2)          & 66.9(0.1)          & 41.3(0.1)          & 62.6          \\ \midrule
40\%   & 0\%  & 14\%          & \textbf{10.0(0.0)} & 65.3(0.1)          & \textbf{77.2(0.1)} & 69.3(0.1)          & 58.4(0.2)          & \textbf{38.1(0.1)} & \textbf{64.9(0.0)} & \textbf{40.3(0.1)} & 59.1          \\ 
0\%    & 40\% & \textbf{25\%} & 10.1(0.1)          & \textbf{65.9(0.0)} & 76.0(0.2)          & \textbf{69.4(0.1)} & \textbf{63.8(0.0)} & 36.5(0.2)          & 62.4(0.0)          & \textbf{40.3(0.6)} & \textbf{59.2} \\ \midrule
60\%   & 0\%  & 21\%          & 33.5(0.4)          & 60.8(0.1)          & \textbf{71.4(0.1)} & 42.2(0.1)          & 51.8(0.3)          & 29.9(0.2)          & 49.8(0.1)          & \textbf{36.4(0.2)} & 49.0            \\ 
0\%    & 60\% & \textbf{39\%} & \textbf{21.1(0.2)} & \textbf{62.8(0.1)} & 71.1(0.2)          & \textbf{55.3(0.1)} & \textbf{58.6(0.3)} & \textbf{31.4(0.2)} & \textbf{53.4(0.0)} & 34.8(0.2)          & \textbf{52.5} \\ \midrule
40\%   & 20\% & 27\%          & 11.9(0.1)          & 65.0(0.1)          & \textbf{76.4(0.1)} & 68.4(0.1)          & 59.3(0.4)          & \textbf{39.0(0.1)} & \textbf{64.8(0.2)} & 40.6(0.3)          & 59.1          \\ 
20\%   & 40\% & \textbf{33\%} & \textbf{11.5(0.1)} & \textbf{67.7(0.1)} & 75.4(0.3)          & \textbf{69.1(0.0)} & \textbf{62.7(0.2)} & 38.3(0.1)          & 64.1(0.2)          & \textbf{41.0(0.4)} & \textbf{59.8} \\ \midrule
60\%   & 20\% & 34\%          & 38.4(0.3)          & 62.0(0.1)          & \textbf{72.6(0.2)} & 43.7(0.1)          & 51.0(0.2)          & 29.9(0.1)          & 52.3(0.2)          & \textbf{38.5(0.4)} & 50.0            \\ 
20\%   & 60\% & \textbf{46\%} & \textbf{23.8(0.4)} & \textbf{62.5(0.1)} & 70.8(0.2)          & \textbf{55.6(0.1)} & \textbf{58.5(0.2)} & \textbf{33.2(0.2)} & \textbf{54.6(0.1)} & 36.2(0.1)          & \textbf{53.1} \\ \midrule
 60\%   & 40\% & 47\%          & 44.3(0.5)          & \textbf{62.0(0.0)} & \textbf{70.8(0.2)} & 42.8(0.1)          & 51.0(0.4)          & 29.0(0.2)          & 51.2(0.2)          & \textbf{36.9(0.5)} & 49.1          \\ 
40\%   & 60\% & \textbf{53\%} & \textbf{33.5(1.2)} & 60.6(0.1)          & 70.3(0.1)          & \textbf{50.7(0.0)} & \textbf{53.8(0.3)} & \textbf{30.1(0.5)} & \textbf{52.8(0.2)} & 35.9(0.1)          & \textbf{50.6} \\ \bottomrule
\end{tabular}%
}
\end{table}
We find that the pruning ratios for attention and MLP layers should be considered independently, as they may reach saturation at different points. Table~\ref{tab:differentattnmlpratio} demonstrates a clear discrepancy in performance between pruning attention heads and MLPs, especially as the pruning ratios increase. While lower pruning ratios (20\%) result in similar performance impacts for both components, higher ratios (40\%, 60\%) suggest that attention heads reach saturation, particularly in demanding tasks such as WikiText2 and HellaSwag. For example, at a 60\% pruning ratio for attention, performance on WikiText2 drops dramatically to 33.5, compared to 21.1 when the MLP is pruned at the same level. Similarly, performance on HellaSwag decreases significantly to 42.2 when pruning attention, compared to 55.3 when pruning the MLP at the same level. Additionally, considering each module's actual FLOPs reveals a larger performance gap, emphasizing the need for a strategic approach to pruning neural network components.

% \clearpage

\section{Additional Experimental Results} \label{appendix:detailed_results}
In this section, we present the detailed experimental results for each task. The performance without fine-tuning is shown in Tables~\ref{tab-appendix:main_result_llama-2-7b}, \ref{tab-appendix:main_result_llama-2-13b}, \ref{tab-appendix:main_result_opt-13b}, and \ref{tab-appendix:main_result_llama-3-8b}. The comparison of PP with fine-tuned baselines is provided in Tables~\ref{tab-appendix:main_result_llama-2-7b_finetuning} and \ref{tab-appendix:main_result_llama-2-13b_finetuning}. PP consistently surpasses all baselines, including those with fine-tuning, in almost all experimental settings.

\begin{table}[ht!]
\centering
\caption{Zero-shot performance of LLaMA-2-7B after pruning attention and MLP blocks without fine-tuning: PP demonstrates superior performance in nearly all scenarios.}
\label{tab-appendix:main_result_llama-2-7b}
\renewcommand{\arraystretch}{1.2}
\resizebox{\columnwidth}{!}{%
\begin{tabular}{ccc|cccccccc}
\toprule
Method             & Pruning Ratio                     & WikiText2          & BoolQ              & PIQA               & HellaSwag          & WinoGrande         & ARC-c              & ARC-e              & OBQA               & Average       \\ \midrule
Dense              & 0\%                           & 6.0(0.1)           & 74.6(0.0)          & 77.9(0.0)          & 75(0.0)            & 67.7(0.0)          & 42.7(0.0)          & 67.3(0.0)          & 42.6(0.0)          & 64.0            \\ \midrule
Full-Batch         & 20\%                          & 7.3(0.1)           & 67.9(0.0)          & 77.0(0.0)          & 74.5(0.0)          & 65.9(0.0)          & 42.7(0.0)          & 67.2(0.0)          & 43.2(0.0)          & 62.6          \\
Wanda-sp           & 20\%                          & 10.6(0.1)          & 65.3(0.1)          & 77.2(0.1)          & \textbf{74.1(0.0)} & 67.1(0.2)          & 41.1(0.1)          & 63.9(0.3)          & 41.8(0.2)          & 61.5          \\
FLAP               & 20\%                          & 10.3(0.1)          & 67.3(0.5)          & 76.6(0.2)          & 73.0(0.1)          & \textbf{67.4(0.0)} & 40.6(0.3)          & 63.1(0.1)          & \textbf{42.0(0.1)} & 61.4          \\
LoRAPrune          &  20\%                         & 22.7(0.9)          & 64.2(0.6)          & 74.6(0.3)          & 66.5(0.5)          & 58.8(1.2)          & 37.7(0.7)          & 63.9(0.6)          & 39.4(1.1)          & 57.9          \\
LLM-Pruner         &  20\%                         & 17.5(1.6)          & 62.5(0.3)          & 75.3(0.8)          & 66.0(0.7)          & 57.2(1.7)          & 37.7(1.0)          & 62.4(0.7)          & 40.5(0.2)          & 57.4          \\
\gr PP                 & 20\%                      & \textbf{8.1(0.1)}  & \textbf{69.0(0.1) }         & \textbf{78.1(0.0)} & 73.5(0.0)          & 66.7(0.3)          & \textbf{42.8(0.1)} & \textbf{68.5(0.0)} & 40.9(0.2)          & \textbf{62.8} \\ \midrule
Full-Batch         & 40\%  & 13.6(0.1)             & 64.8(0.0)          & 74.9(0.0)          & 67.6(0.0)          & 59.0(0.0)          & 38.7(0.0)          & 64.7(0.0)          & 41.0(0.0)          & 58.7          \\
Wanda-sp           &  40\%                         & 43.8(1.5)          & 62.5(0.1)          & 72.5(0.1)          & 63.3(0.0)          & 56.9(0.1)          & 33.4(0.2)          & 54.4(0.1)          & \textbf{40.8(0.4)} & 54.8          \\
FLAP               &  40\%                         & 38.9(1.3)          & \textbf{63.5(0.1)} & 71.7(0.3)          & 63.3(0.1)          & \textbf{59.8(0.1)} & 33.8(0.6)          & 52.5(0.2)          & 40.0(0.6)          & 54.9          \\
LoRAPrune          &  40\%                         & 129.5(3.0)         & 54.0(4.2)          & 65.0(0.5)          & 45.1(1.3)          & 52.1(0.3)          & 25.8(0.2)          & 43.6(0.7)          & 32.1(0.6)          & 45.4          \\
LLM-Pruner         &  40\%                         & 51.1(4.3)          & 55.5(5.0)          & 69.8(1.1)          & 49.6(2.1)          & 51.2(0.3)          & 27.8(0.6)          & 46.0(2.0)          & 35.0(0.5)          & 47.8          \\
\gr PP                 & 40\%                      & \textbf{16.8(0.1) }         & 62.7(0.2)          & \textbf{74.9(0.1)} & \textbf{63.6(0.0)} & 57.5(0.2)          & \textbf{35.5(0.1)} & \textbf{61.7(0.2)} & 40.3(0.4)          & \textbf{56.6} \\ \bottomrule
\end{tabular}%
}
\end{table}

\begin{table}[ht!]
\centering
\caption{Zero-shot performance of LLaMA-2-13B after pruning attention and MLP blocks without fine-tuning: PP demonstrates superior performance in nearly all scenarios.}
\label{tab-appendix:main_result_llama-2-13b}
\renewcommand{\arraystretch}{1.2}
\resizebox{\columnwidth}{!}{%
\begin{tabular}{ccc|cccccccc}
\toprule
Method             & Pruning Ratio                     & WikiText2          & BoolQ              & PIQA               & HellaSwag          & WinoGrande         & ARC-c              & ARC-e              & OBQA               & Average       \\ \midrule
Dense              & 0\%                    & 5.1(0.1)           & 72.1(0.0)          & 79.6(0.0)          & 78.7(0.0)          & 70.7(0.0)          & 46.5(0.0)          & 71.3(0.0)          & 44.2(0.0)          & 66.2          \\ \midrule
Full-Batch         & 20\%  & 6.2(0.1)           & 69.0(0.0)          & 78.7(0.0)          & 77.9(0.0)          & 70.1(0.0)          & 47.7(0.0)          & 71.1(0.0)          & 42.8(0.0)          & 65.3          \\
Wanda-sp           & 20\%                          & 9.0(0.1)           & 70.4(1.0)          & 79.4(0.1)          & \textbf{78.4(0.0)} & 70.2(0.1)          & 44.3(0.6)          & 69.9(0.3)          & 42.5(0.2)          & 65.0            \\
FLAP               & 20\%                          & 7.5(0.1)           & 71.1(0.5)          & 78.7(0.1)          & 77.3(0.0)          & \textbf{71.2(0.2)} & 44.6(0.1)          & 66.7(0.1)          & 42.5(0.2)          & 64.6          \\
LoRAPrune          & 20\%                          & 16.1(0.7)          & 63.6(0.2)          & 75.4(0.1)          & 69.4(0.8)          & 63.6(0.3)          & 37.6(0.4)          & 62.6(0.7)          & 40.3(0.5)          & 58.9          \\
LLM-Pruner         &  20\%                         & 11.3(0.7)          & 63.4(1.8)          & 77.7(0.1)          & 72.3(0.5)          & 63.0(1.1)          & 42.3(0.6)          & 67.8(0.3)          & 42.9(0.7)          & 61.3          \\
\gr PP                 & 20\%                          & \textbf{6.7(0.1)}  & \textbf{72.0(0.2)} & \textbf{79.5(0.1)} & 77.6(0.0)          & 68.5(0.1)          & \textbf{44.7(0.2)} & \textbf{71.5(0.1)} & \textbf{43.0(0.2)} & \textbf{65.3} \\ \midrule
Full-Batch         & 40\%  & 8.9(0.1)           & 68.4(0.0)          & 77.7(0.0)          & 74.5(0.0)          & 65.4(0.0)          & 42.4(0.0)          & 69.3(0.0)          & 42.8(0.0)          & 62.9          \\
Wanda-sp           &  40\%                         & 21.6(0.4)          & 62.4(0.0)          & 74.5(0.3)          & 68.0(0.0)          & 63.0(0.4)          & 34.8(0.5)          & 54.9(0.3)          & 38.9(0.4)          & 56.6          \\
FLAP               &  40\%                         & 15.5(0.0)          & 62.9(0.1)          & 76.8(0.3)          & \textbf{72.4(0.1)} & \textbf{66.9(0.3)} & 40.4(0.4)          & 63.1(0.4)          & 41.8(0.1)          & 60.6          \\
LoRAPrune          &  40\%                         & 74.8(6.4)          & 57.9(3.5)          & 66.8(0.9)          & 51.5(0.6)          & 53.6(0.5)          & 28.5(0.3)          & 46.0(0.8)          & 32.4(1.2)          & 48.1          \\
LLM-Pruner         &   40\%                        & 34.5(2.4)          & 57.0(2.2)          & 72.5(1.1)          & 57.8(2.0)          & 54.2(0.8)          & 33.3(1.3)          & 51.5(1.9)          & 37.7(1.2)          & 52.0            \\
\gr PP                 &   40\%                        & \textbf{11.3(0.1)}          & \textbf{65.8(0.1)} & \textbf{77.1(0.2)} & 71.6(0.0)          & 61.3(0.4)          & \textbf{40.9(0.3)} & \textbf{67.9(0.1)} & \textbf{42.5(0.3)} & \textbf{61.0}   \\  \bottomrule
\end{tabular}%
}
\end{table}

\begin{table}[ht!]
\centering
\caption{Zero-shot performance of OPT-13B after pruning attention and MLP blocks without fine-tuning: PP demonstrates superior performance in nearly all scenarios.}
\label{tab-appendix:main_result_opt-13b}
\renewcommand{\arraystretch}{1.2}
\resizebox{\columnwidth}{!}{%
\begin{tabular}{ccc|cccccccc}
\toprule
Method     & Pruning Ratio & WikiText2           & BoolQ              & PIQA               & HellaSwag          & WinoGrande         & ARC-c              & ARC-e              & OBQA               & Average       \\ \midrule
Dense      & 0\%        & 11.6(0.1)           & 68.1(0.0)          & 75.3(0.0)          & 67.9(0.0)          & 66.8(0.0)          & 35(0.0)            & 51.1(0.0)          & 36.4(0.0)          & 57.2          \\ \midrule
Full-Batch & 20\%       & 12.6(0.1)           & 63.9(0.0)          & 75.7(0.0)          & 67.6(0.0)          & 67.0(0.0)          & 34.3(0.0)          & 50.7(0.0)          & 35.4(0.0)          & 56.4          \\
Wanda-sp   & 20\%       & 17.4(0.1)           & 66.0(0.2)          & 75.4(0.1)          & 63.0(0.1)          & 64.8(0.3)          & 33.7(0.0)          & 48.2(0.2)          & 35.0(0.1)          & 55.2          \\
FLAP       & 20\%       & 18.8(0.2)           & \textbf{68.1(0.4)} & 75.1(0.1)          & 62.5(0.2)          & 62.6(0.3)          & 31.8(0.3)          & 49.5(0.1)          & 34.5(0.1)          & 54.9          \\
\gr PP         & 20\%       & \textbf{14.7(0.1)}  & 67.4(0.1)          & \textbf{75.5(0.1)} & \textbf{65.7(0.0)} & \textbf{64.9(0.3)} & \textbf{33.8(0.1)} & \textbf{51.6(0.0)} & \textbf{36.5(0.2)} & \textbf{56.5} \\ \midrule
Full-Batch & 40\%       & 17.9(0.2)           & 52.1(0.0)          & 75.7(0.0)          & 64.8(0.0)          & \textbf{65.5(0.0)} & 32.8(0.0)          & 50.1(0.0)          & 36.8(0.0)          & 54            \\
Wanda-sp   & 40\%       & 42.7(0.7)           & \textbf{63.7(0.1)} & 71.8(0.3)          & 53.2(0.1)          & 57.6(0.2)          & 29.6(0.4)          & 43.3(0.1)          & 34.3(0.2)          & 50.5          \\
FLAP       & 40\%       & 51.0(0.7)           & 62.7(0.0)          & 72.4(0.0)          & 53.3(0.2)          & 58.3(0.5)          & 29.4(0.3)          & 45.2(0.4)          & 34.1(0.1)          & 50.8          \\
\gr PP         & 40\%       & \textbf{26.7(0.3)}  & 61.1(0.2)          & \textbf{74.3(0.1)} & \textbf{58.7(0.0)} & \textbf{59.3(0.1)} & \textbf{33.6(0.1)} & \textbf{49.7(0.1)} & \textbf{35.3(0.4)} & 53.1          \\ \bottomrule
\end{tabular}%
}
\end{table}
% \vspace{-5cm}
\begin{table}[ht!]

\centering
\caption{Zero-shot performance of pruning LLaMA-3-8B with MLP pruned. PP consistently demonstrates superior performance across nearly all tested scenarios.}
\label{tab-appendix:main_result_llama-3-8b}
\renewcommand{\arraystretch}{1.2}
\resizebox{\columnwidth}{!}{%
\begin{tabular}{ccc|cccccccc}
\toprule
Method               & Pruning Ratio & WikiText2 & BoolQ              & PIQA               & HellaSwag          & WinoGrande         & ARC-c              & ARC-e              & OBQA               & Average       \\ \midrule
Dense                & 0\%        & 6.8(0.0)  & 81.7(0.0)          & 79.5(0.0)          & 76.3(0.0)          & 72.5(0.0)          & 47.2(0.0)          & 61.7(0.0)          & 40.2(0.0)          & 65.6          \\ \midrule
Full-Batch & 20\%       & 8.5(0.0)  & 79.0(0.0)          & 80.1(0.0)          & 74.8(0.0)          & 73.9(0.0)          & 44.9(0.0)          & 60.7(0.0)          & 40.2(0.0)          & 64.8          \\
Wanda-sp             & 20\%       & 10.0(0.0) & 75.1(0.3)          & 78.5(0.0)          & 69.6(0.2)          & 71.4(0.4)          & 38.7(0.4)          & 56.9(0.4)          & 39.0(0.2)          & 61.3          \\
FLAP                 & 20\%       & 10.0(0.0) & \textbf{79.4(0.2)} & \textbf{78.7(0.1)} & 70.3(0.0)          & 71.4(0.5)          & 40.8(0.1)          & 57.8(0.0)          & 39.4(0.3)          & 62.5          \\
\gr PP                   & 20\%       & \textbf{9.3(0.0)}  & 77.4(0.0)          & 78.5(0.0)          & \textbf{73.1(0.0)} & \textbf{72.5(0.3)} & \textbf{43.2(0.3)} & \textbf{59.1(0.2)} & \textbf{40.2(0.5)} & \textbf{63.4} \\ \midrule
Full-Batch & 40\%       & 12.3(0.1) & 73.1(0.0)          & 77.8(0.0)          & 70.5(0.0)          & 70.3(0.0)          & 42.9(0.0)          & 58.9(0.0)          & 39.8(0.0)          & 61.9          \\
Wanda-sp             & 40\%       & 18.4(0.1) & 66.6(0.1)          & 73.4(0.2)          & 56.7(0.1)          & 63.2(0.2)          & 31.8(0.2)          & 47.0(0.5)          & 34.5(0.2)          & 53.3          \\
FLAP                 & 40\%       & 18.5(0.2) & 67.3(1.0)          & 73.5(0.0)          & 57.2(0.2)          & 66.7(0.5)          & 31.7(0.3)          & 44.6(0.3)          & 34.4(0.3)          & 53.6          \\
\gr PP                   & 40\%       & \textbf{14.9(0.1)} & \textbf{70.3(0.1)} & \textbf{76.3(0.2)} & \textbf{65.3(0.1)} & \textbf{67.2(0.2)} & \textbf{39.0(0.3)} & \textbf{57.4(0.1)} & \textbf{36.9(0.3)} & \textbf{58.9} \\ \bottomrule
\end{tabular}%
}
\end{table}
\clearpage
\begin{table}[ht!]
% \vspace{-5cm}
\centering
\caption{Comparison of PP with fine-tuned baselines on LLaMA-2-7B model, with attention and MLP layers pruned: PP consistently outperforms across scenarios without fine-tuning.}
\label{tab-appendix:main_result_llama-2-7b_finetuning}
\renewcommand{\arraystretch}{1.2}
\resizebox{\columnwidth}{!}{%
\begin{tabular}{cccc|cccccccc}
\toprule
Method             & Pruning Ratio            & Fine-tuning         & WikiText2          & BoolQ              & PIQA               & HellaSwag          & WinoGrande         & ARC-c              & ARC-e              & OBQA               & Average       \\ \midrule
Dense              & 0\%                   & \tXmark & 6.0(0.1)           & 74.6(0.0)          & 77.9(0.0)          & 75(0.0)            & 67.7(0.0)          & 42.7(0.0)          & 67.3(0.0)          & 42.6(0.0)          & 64.0            \\ \midrule
LoRAPrune w/ LoRA  &  20\%                        & \cmark & 8.7(0.2)           & \textbf{67.0(0.9)} & 76.5(0.2)          & 69.9(0.1)          & 63.2(0.3)          & 36.7(0.2)          & 58.9(0.9)          & \textbf{42.3(0.2)}          & 59.2          \\
LLM-Pruner w/ LoRA &  20\%                        & \cmark & 10.2(0.3)          & 66.6(1.3)          & 76.1(0.6)          & 68.4(0.5)          & 62.8(1.1)          & 36.3(0.4)          & 59.8(0.3)          & 40.7(0.7)          & 58.7          \\
\gr PP                 & 20\%                         & \tXmark & \textbf{8.1(0.1)}  & 69.0(0.1)          & \textbf{78.1(0.0)} & \textbf{73.5(0.0)}          & \textbf{66.7(0.3)}          & \textbf{42.8(0.1)} & \textbf{68.5(0.0)} & 40.9(0.2)          & \textbf{62.8} \\ \midrule
LoRAPrune w/ LoRA  &  40\%                        & \cmark & \textbf{13.6(0.4)} & \textbf{62.9(0.2) }         & 70.8(0.1)          & 58.6(0.1)          & 55.5(0.7)          & 30.9(0.4)          & 49.6(0.4)          & 36.7(0.4)          & 52.1          \\
LLM-Pruner w/ LoRA &  40\%                        & \cmark & 20.3(1.3)          & 57.5(4.0)          & 71.3(1.2)          & 55.7(1.3)          & 53.1(0.5)          & 28.9(0.7)          & 50.4(0.5)          & 37.3(0.6)          & 50.6          \\
\gr PP                 & 40\%                         & \tXmark & 16.8(0.1)          & 62.7(0.2)          & \textbf{74.9(0.1)} & \textbf{63.6(0.0)} & \textbf{57.5(0.2)}          & \textbf{35.5(0.1)} & \textbf{61.7(0.2)} & \textbf{40.3(0.4) }         & \textbf{56.6} \\ \bottomrule
\end{tabular}%
}
\end{table}

\begin{table}[ht!]
% \vspace{-16cm}
\centering
\caption{Comparison of PP with fine-tuned baselines on LLaMA-2-13B model, with attention and MLP layers pruned: PP consistently outperforms across scenarios without fine-tuning.}
\label{tab-appendix:main_result_llama-2-13b_finetuning}
\renewcommand{\arraystretch}{1.2}
\resizebox{\columnwidth}{!}{%
\begin{tabular}{cccc|cccccccc}
\toprule
Method             & Pruning Ratio            & Fine-tuning         & WikiText2          & BoolQ              & PIQA               & HellaSwag          & WinoGrande         & ARC-c              & ARC-e              & OBQA               & Average       \\ \midrule
Dense              & 0\%                   & \tXmark & 5.1(0.1)           & 72.1(0.0)          & 79.6(0.0)          & 78.7(0.0)          & 70.7(0.0)          & 46.5(0.0)          & 71.3(0.0)          & 44.2(0.0)          & 66.2          \\ \midrule
LoRAPrune w/ LoRA  & 20\%                         & \cmark & 7.4(0.0)           & 64.4(0.5)          & 78.1(0.1)          & 74.8(0.2)          & 66.0(0.3)          & 40.4(0.3)          & 61.7(0.9)          & 41.6(0.2)          & 61.0            \\
LLM-Pruner w/ LoRA & 20\%                         & \cmark & 8.4(0.5)           & 70.2(1.4)          & 78.3(0.3)          & 73.8(0.3)          & 65.8(1.3)          & 40.1(0.5)          & 64.2(0.4)          & 42.0(0.4)          & 62.1          \\
\gr PP                 & 20\%                         & \tXmark & \textbf{6.7(0.1)}  & \textbf{72.0(0.2)} & \textbf{79.5(0.1)} & \textbf{77.6(0.0)}          & \textbf{68.5(0.1) }         & \textbf{44.7(0.2)} & \textbf{71.5(0.1)} & \textbf{43.0(0.2)} & \textbf{65.3} \\ \midrule
LoRAPrune w/ LoRA  &  40\%                        & \cmark & \textbf{11.1(0.3)} & 62.5(0.1)          & 74.1(0.4)          & 65.5(0.1)          & 60.4(0.3)          & 33.0(0.4)          & 53.9(0.7)          & 39.3(0.6)          & 55.5          \\
LLM-Pruner w/ LoRA &   40\%                       & \cmark & 15.3(0.7)          & 63.9(0.4)          & 73.5(0.6)          & 62.4(1.4)          & 57.5(1.1)          & 33.2(1.2)          & 55.2(0.7)          & 37.5(0.8)          & 54.7          \\
\gr PP                 &   40\%                       & \tXmark & 11.3(0.1)          & \textbf{65.8(0.1)} & \textbf{77.1(0.2)} & \textbf{71.6(0.0)}          & \textbf{61.3(0.4)}          & \textbf{40.9(0.3)} & \textbf{67.9(0.1)} & \textbf{42.5(0.3)} & \textbf{61.0}   \\  \bottomrule
\end{tabular}%
}
\end{table}
