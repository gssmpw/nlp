\section{Related Work}
\vspace{-0.3cm}
\paragraph{Pruning with Calibration Dataset.} Pruning methods can be broadly classified into two categories____: \textit{unstructured pruning} and \textit{structured pruning}. Unstructured pruning____ removes individual weights, whereas structured pruning____ removes complete structures from the model, such as channels and attention heads. Pruning LLMs often involves calibration datasets due to the emergence of outliers in their internal representations. For unstructured pruning, SparseGPT____ uses synchronized second-order Hessian updates to solve row-wise weight reconstruction problems and update weights. Wanda____ introduces a pruning metric that considers both the magnitude of weights and activation values to determine which weights to prune. For structured pruning, FLAP____ introduces a fluctuation metric to decide which weight channels to prune. LLM-Pruner____ employs approximated second-order Taylor expansions of the error function to remove the least important coupled structures and then applies fine-tuning to recover model performance. LoRA-Prune____ uses a LoRA____-guided pruning metric that leverages the weights and gradients of LoRA to direct the iterative process of pruning and tuning. However, the fine-tuning process requires substantial computational resources____, and we have found that fine-tuning might cause LLMs to lose their generalizability; for example, they may perform worse on certain downstream tasks, such as commonsense reasoning tasks.
\vspace{-0.3cm}
\paragraph{Large-Magnitude Outliers of LLMs.} Unlike small neural networks, LLMs exhibit large-magnitude outlier features____. ____ shows that these large-magnitude features begin to emerge when the size of LLMs exceeds 6.7 billion parameters, and these outlier features are concentrated in certain channels. The phenomenon of massive activations____ has been observed, where a few activations exhibit significantly larger values than others, potentially leading to the concentration of attention probabilities on their corresponding tokens. These emergent properties suggest the need to customize the pruning of channels for different batches to maintain model performance. This observation motivates us to propose Probe Pruning.
\begin{figure*}
    \centering
    \includegraphics[width=1\linewidth]{Figures/main.pdf}
    \caption{Probe Pruning (PP) is executed in four stages: (1) PP selects key samples and tokens from the layer-normalized hidden states, based on residual importance, to create a \textit{small yet crucial} probe. (2) PP deploys this probe to run a few model layers ahead and obtains the probe's intermediate hidden states. (3) PP integrates the probing states with historical states and uses the integrated states to calculate the pruning metric and prune weight channels. (4) PP performs full inference on the remaining weights.}
    \vspace{-0.5cm}
    \label{fig:main_figure}
\end{figure*}
\vspace{-0.3cm}