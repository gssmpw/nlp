\PassOptionsToPackage{table}{xcolor}
\documentclass{article} % For LaTeX2e
\usepackage{iclr2025_conference,times}

% Optional math commands from https://github.com/goodfeli/dlbook_notation.
\input{math_commands.tex}

\usepackage{hyperref}
\usepackage{url}
\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsmath,amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{wrapfig}
\usepackage{caption}
\usepackage[ruled,vlined]{algorithm2e}
\usepackage{mathtools}
\usepackage{mathrsfs}
\usepackage[utf8]{inputenc}
\usepackage{hyperref}
\usepackage{xspace}
\usepackage{booktabs}
\usepackage{multirow}
\usepackage{soul}
\usepackage{enumitem}
\usepackage{tabu}
\usepackage{pifont}
\usepackage[table]{xcolor}
\usepackage[normalem]{ulem} 
\usepackage{subcaption}
\def\modelM{\mathcal{M}}
\def\tranF{\mathcal{F}}
\def\tranT{\mathcal{T}}
\newcommand{\Cout}{C_{\text{out}}}
\newcommand{\Cin}{C_{\text{in}}}
\newcommand{\Wfinal}{\mW^{\text{final}}}
\newcommand{\LN}{\mathrm{LN}}
\def\de{\overset{\Delta}{=}}

\def\equationautorefname~#1\null{Eq. (#1)\null}

\newcommand{\gr}{\rowcolor[gray]{.95}}
\newcommand{\tablestyle}[2]{\setlength{\tabcolsep}{#1}\renewcommand{\arraystretch}{#2}\centering\footnotesize}
\newcommand{\cmark}{\ding{51}}%
\newcommand{\tXmark}{\ding{55}}%
\newcommand{\anwar}[1]{\textcolor{red}{{[Anwar: #1]}}}
\newcommand{\qi}[1]{\textcolor{blue}{{[Qi: #1]}}}
\newcommand{\red}[1]{\textcolor{red}{{[#1]}}}
\newcommand{\blue}[1]{\textcolor{blue}{{[#1]}}}
\newcommand{\etc}{{etc.}\tXspace}
\newcommand{\ie}{{i.e.,}\tXspace}
\newcommand{\eg}{{e.g.,}\tXspace}
\newcommand{\etal}{{et al.}\tXspace}
\newcommand{\wrt}{{w.r.t.}\tXspace}
\newcommand{\aka}{{a.k.a.}\tXspace}
\newcommand{\NA}{---}
\newcommand{\jie}[1]{\textcolor{red}{(Jie: #1)}}
\newcommand{\authorskip}{\hspace{1.2mm}}

\SetKwInput{Input}{Input}
\SetKwProg{kwSystem}{System executes}{:}{}
% \SetKwInput{kwSystem}{System executes}
\SetKwProg{kwClient}{ClientUpdate}{:}{}
\SetKwProg{ServerUpdate}{ServerUpdate}{:}{}
\SetKwProg{ServerDistribute}{ServerDistribute}{:}{}
\SetKwProg{kwServerAggregation}{ServerAggregation}{:}{}
\SetKwProg{kwDynaComm}{DynaComm}{:}{}


% Camera Ready
\title{Probe Pruning: Accelerating LLMs through Dynamic Pruning via Model-Probing}

% Authors must not appear in the submitted version. They should be hidden
% as long as the \iclrfinalcopy macro remains commented out below.
% Non-anonymous submissions will be rejected without review.
% \thanks{Equal contribution. Correspondence to \url{mingjies@cs.cmu.edu} and \url{zhuangl@meta.com}.} 
\author{
Qi Le$^1$,
\authorskip Enmao Diao,
\authorskip Ziyan Wang$^2$, 
\authorskip Xinran Wang$^{1}$,
\authorskip Jie Ding$^{1}$,
\authorskip Li Yang$^{2}$,
\authorskip Ali Anwar$^{1}$\\
$^1$University of Minnesota ~~~~~ $^2$University of North Carolina at Charlotte\vspace{0.3ex}\\  
\{le000288, wang8740, dingj, aanwar\}@umn.edu, diao\_em@hotmail.com, \\
\{zwang53, lyang50\}@charlotte.edu
}


% The \author macro works with any number of authors. There are two commands
% used to separate the names and addresses of multiple authors: \And and \AND.
%
% Using \And between authors leaves it to \LaTeX{} to determine where to break
% the lines. Using \AND forces a linebreak at that point. So, if \LaTeX{}
% puts 3 of 4 authors names on the first line, and the last on the second
% line, try using \AND instead of \And before the third author name.

\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}

\iclrfinalcopy % Uncomment for camera-ready version, but NOT for submission.
\begin{document}

\maketitle
\vspace{-0.2cm}
\begin{abstract}
\vspace{-0.2cm}
We introduce Probe Pruning (PP), a novel framework for online, dynamic, structured pruning of Large Language Models (LLMs) applied in a batch-wise manner. PP leverages the insight that not all samples and tokens contribute equally to the model's output, and probing a small portion of each batch effectively identifies crucial weights, enabling tailored dynamic pruning for different batches. It comprises three main stages: probing, history-informed pruning, and full inference. In the probing stage, PP selects a \textit{small yet crucial} set of hidden states, based on residual importance, to run a few model layers ahead. During the history-informed pruning stage, PP strategically integrates the probing states with historical states. Subsequently, it structurally prunes weights based on the integrated states and the PP importance score, a metric developed specifically to assess the importance of each weight channel in maintaining performance. In the final stage, full inference is conducted on the remaining weights. A major advantage of PP is its compatibility with existing models, as it operates \textit{without requiring additional neural network modules or fine-tuning}. Comprehensive evaluations of PP on LLaMA-2/3 and OPT models reveal that even minimal probing—using just 1.5\% of FLOPs—can substantially enhance the efficiency of structured pruning of LLMs. For instance, when evaluated on LLaMA-2-7B with WikiText2, PP achieves a 2.56$\times$ lower ratio of performance degradation per unit of runtime reduction compared to the state-of-the-art method at a 40\% pruning ratio. Our code is available at \url{https://github.com/Qi-Le1/Probe_Pruning}.
\end{abstract}
\vspace{-0.3cm}
\section{Introduction}
\label{introduction}
\vspace{-0.3cm}
Large Language Models (LLMs)~\citep{vaswani2017attention, zhang2022opt, touvron2023llama, diao2024cola} have recently achieved significant success, leading to the development of numerous applications~\citep{openai2023gpt, anand2023gpt4all}. However, the inference for these models, often containing billions of parameters, presents challenges. These challenges primarily arise from the substantial computational demands and the risk of high latency~\citep{ma2023llm}.

Structured pruning is a promising hardware-friendly approach to reduce computational consumption and accelerate inference~\citep{yuan2021mest}. It removes complete structures from models, such as weight channels and attention heads. Compared with other methods like unstructured pruning~\citep{frantar2023sparsegpt, sun2023simple}, parameter sharing~\citep{diao2019restricted}, offloading~\citep{rasley2020deepspeed,diao2022gal,diao2024cola}, and quantization~\citep{dettmers2022gpt3, lin2023awq, frantar2022gptq}, structured pruning reduces computational resources and speeds up inference without requiring specific hardware. However, when applied to LLMs, structured pruning often results in a performance gap compared to dense models~\citep{wang2024model}. 

A major factor contributing to the performance gap in LLMs may be the emergence of significant outlier phenomena in internal representations~\citep{dettmers2022gpt3, liu2024intactkv, sun2024massive}. Current advanced structured pruning methods typically utilize calibration datasets to assess the importance of weights using pruning metrics. For example, the FLAP method~\citep{an2023fluctuation} uses a calibration dataset to compute fluctuation metrics for each input feature and its corresponding channel in attention or MLP block weight matrices, specifically in the output projection (O) or fully connected layer 2 (FC2). Similarly, LLM-Pruner~\citep{ma2023llm} employs approximated second-order Taylor expansions of the error function, calculated using a calibration dataset, to eliminate the least important coupled structures. Although the calibration dataset provides valuable insights for pruning by identifying non-critical weights, this approach overlooks the batch-dependent nature of outlier properties in LLMs~\citep{liu2023deja, song2023powerinfer, liu2024intactkv, sun2024massive}, which vary across different input batches and cannot be accurately predicted prior to inference. Experimental illustrations can be found in Appendix~\ref{appendix-pp: batch-dependent}. Consequently, pruning decisions based solely on calibration dataset may fail to address these dynamic outliers during real-time inference, resulting in suboptimal model performance. Fine-tuning can serve as a method to recover model performance~\citep{wang2024model}, but it is resource-intensive and may be impractical for certain real-world applications.

To effectively handle batch-dependent outliers and reduce the performance gap between pruned and dense models without extensive fine-tuning, we propose Probe Pruning (PP). PP is an online dynamic structured pruning framework that prunes the model during inference based on each batch's hidden states. Notably, PP relies solely on the original model structure and hidden states, \textit{without requiring additional neural network modules or fine-tuning}. We overcome two key challenges:
\begin{itemize}
    % Using hidden states obtained from calibration datasets can enhance pruning effectiveness. However, 
    \item \textbf{Leveraging Calibration Dataset may Introduce Biases}: Relying exclusively on the calibration dataset may introduce biases, as the pruned channels are entirely determined by the calibration dataset. For example, when FLAP used the WikiText2 validation set as a calibration dataset, it achieved a perplexity of 18.5 on the WikiText2 test set of LLaMA-2-7B with a 40\% pruning ratio. In contrast, using the C4 dataset as a calibration dataset, the perplexity increased to 38.9 on the WikiText2 test set. \textit{We propose history-informed pruning with importance-scaled fusion to leverage the benefits of the calibration dataset while minimizing associated biases.}

    \item \textbf{Dynamic Pruning Without Access to Intermediate Hidden States}: Deciding online which channels to prune during inference for each batch is challenging. Without gradients, calculating pruning metrics for attention and MLP blocks requires intermediate hidden states, which are the input tensors to the attention output projection and MLP's FC2 layer. These states are unavailable when the input hidden states initially enter these blocks. Moreover, not all samples and tokens contribute equally to the model's output, and large-magnitude outliers in LLMs often have a significant impact on the model’s behavior. \textit{Therefore, we propose a probing method that selects key samples and tokens from the input hidden states, runs a few model layers ahead, and obtains intermediate hidden state information.} Without such probing, accessing intermediate hidden states requires significant computational costs.
    % to balance computational cost with performance
\end{itemize}
Specifically, PP leverages a \textit{small yet crucial} segment of hidden states to run a few model layers ahead and capture the probe's intermediate hidden states, which contain essential information for guiding pruning decisions within the attention or MLP blocks of the current batch. By strategically integrating the probing states with historical states, we can dynamically determine which channels to prune. After pruning the weight channels, we run full inference on the remaining weights. Furthermore, our probing is minimal yet effective: for example, it operates with only 5\% of the samples and 50\% of the tokens, utilizing just 1.5\% of the floating point operations (FLOPs) of dense model inference, and yet it has proven effective. Experimental results confirm that this minimal probe effectively captures critical intermediate hidden state information.
\vspace{-0.3cm}
\section{Related Work}
\vspace{-0.3cm}
\paragraph{Pruning with Calibration Dataset.} Pruning methods can be broadly classified into two categories~\citep{yuan2021mest}: \textit{unstructured pruning} and \textit{structured pruning}. Unstructured pruning~\citep{lecun1989optimal, hassibi1993optimal, han2015learning, mushtaq2021spider, li2021ell, soltani2021information, yang2022theoretical, diao2023pruning, liu2023sparsity, li2024adaptive, li2024discovering, dong2024pruner} removes individual weights, whereas structured pruning~\citep{li2016pruning, liu2017learning, he2019filter, diao2020drasic, fang2023depgraph} removes complete structures from the model, such as channels and attention heads. Pruning LLMs often involves calibration datasets due to the emergence of outliers in their internal representations. For unstructured pruning, SparseGPT~\citep{frantar2023sparsegpt} uses synchronized second-order Hessian updates to solve row-wise weight reconstruction problems and update weights. Wanda~\citep{sun2023simple} introduces a pruning metric that considers both the magnitude of weights and activation values to determine which weights to prune. For structured pruning, FLAP~\citep{an2023fluctuation} introduces a fluctuation metric to decide which weight channels to prune. LLM-Pruner~\citep{ma2023llm} employs approximated second-order Taylor expansions of the error function to remove the least important coupled structures and then applies fine-tuning to recover model performance. LoRA-Prune~\citep{zhang2023pruning} uses a LoRA~\citep{hu2021lora}-guided pruning metric that leverages the weights and gradients of LoRA to direct the iterative process of pruning and tuning. However, the fine-tuning process requires substantial computational resources~\citep{hoffmann2022training}, and we have found that fine-tuning might cause LLMs to lose their generalizability; for example, they may perform worse on certain downstream tasks, such as commonsense reasoning tasks.
\vspace{-0.3cm}
\paragraph{Large-Magnitude Outliers of LLMs.} Unlike small neural networks, LLMs exhibit large-magnitude outlier features~\citep{kovaleva2021bert, dettmers2022gpt3, dettmers2023spqr, schaeffer2024emergent, sun2024massive}. \cite{dettmers2022gpt3} shows that these large-magnitude features begin to emerge when the size of LLMs exceeds 6.7 billion parameters, and these outlier features are concentrated in certain channels. The phenomenon of massive activations~\citep{sun2024massive, liu2024intactkv} has been observed, where a few activations exhibit significantly larger values than others, potentially leading to the concentration of attention probabilities on their corresponding tokens. These emergent properties suggest the need to customize the pruning of channels for different batches to maintain model performance. This observation motivates us to propose Probe Pruning.
\begin{figure*}
    \centering
    \includegraphics[width=1\linewidth]{Figures/main.pdf}
    \caption{Probe Pruning (PP) is executed in four stages: (1) PP selects key samples and tokens from the layer-normalized hidden states, based on residual importance, to create a \textit{small yet crucial} probe. (2) PP deploys this probe to run a few model layers ahead and obtains the probe's intermediate hidden states. (3) PP integrates the probing states with historical states and uses the integrated states to calculate the pruning metric and prune weight channels. (4) PP performs full inference on the remaining weights.}
    \vspace{-0.5cm}
    \label{fig:main_figure}
\end{figure*}
\vspace{-0.3cm}
\section{Notations and Preliminaries} \label{notations_preliminaries}
\vspace{-0.25cm}
An LLM $\modelM$ consists of $L$ blocks, each of which can be either an attention block or a Multi-Layer Perceptron (MLP) block. Each attention block is characterized by four linear projections: Query (Q), Key (K), Value (V), and Output (O). Similarly, each MLP block includes two linear layers: Fully Connected layer 1 (FC1) and Fully Connected layer 2 (FC2).

Each block $l$ transforms the input hidden state $\tX^{l} \in \mathbb{R}^{N \times S \times D}$ into the output hidden state $\tX^{l+1} \in \mathbb{R}^{N \times S \times D}$. Here, $N$, $S$, and $D$ denote the batch size, sequence length, and feature dimension, respectively. The transformations in each block $l$ can be expressed as:
\begin{equation}
\tX^{l+1} = \tX^{l} + \tranF^{l}(\tX^{l}),
\end{equation}
where $\tranF^{l}$ encompasses all transformations within block $l$. This function can be further decomposed as:
\begin{equation}
\tranF^{l}(\tX^{l}) = \tX^{l, \text{int}} (\mW^{l, \text{final}})^{T}, \quad \tX^{l, \text{int}} = \tranT^{l}(\LN^{l}(\tX^{l})),
\end{equation}
where $\tranT^{l}$ represents all intermediate transformations applied to the input hidden state $\tX^{l}$, excluding the layer normalization $\LN^{l}$ and final weight matrix $\mW^{l, \text{final}} \in \mathbb{R}^{\Cout \times \Cin}$. The final weight matrix is either the Output projection (O) in an attention block or FC2 in an MLP block. The intermediate hidden state $\tX^{l, \text{int}} \in \mathbb{R}^{N \times S \times \Cin}$ results from applying these intermediate transformations to $\tX^{l}$. Additionally, we define the \textit{residual importance} as the $\normltwo$ norm of the input hidden states $\tX^{l}$ across specific dimensions, a concept further detailed in Section~\ref{section:probe_generation}.

In structured pruning of LLMs, entire coupled structures are pruned~\citep{ma2023llm, an2023fluctuation}. Specifically, in block $l$, the preceding weight matrices are adjusted by pruning their output channels, which correspond one-to-one with the input channels pruned by the final weight matrix. For example, in an MLP block, the weight matrices are adjusted based on the set of unpruned channel indices $\sC^{l} \subseteq \{1, 2, \ldots, \Cin\}$ as follows: 
\begin{equation} 
\tilde{\mW}^{l, \text{FC1}} = \mW^{l, \text{FC1}}[\sC^{l}, :], \quad \tilde{\mW}^{l, \text{FC2}} = \mW^{l, \text{FC2}}[:, \sC^{l}], 
\end{equation} 
where $\tilde{\mW}^{l, \text{FC1}} \in \sR^{|\sC^{l}| \times \Cin}$ and $\tilde{\mW}^{l, \text{FC2}} \in \sR^{\Cout \times |\sC^{l}|}$. The notation $|\sC^{l}|$ represents the cardinality of $\sC^{l}$. Similarly, in the attention block, attention heads can be treated as coupled structures~\citep{ma2023llm, an2023fluctuation}, and entire attention heads are pruned.
\vspace{-0.3cm}
\section{Methodology} \label{section:methodology}
\vspace{-0.4cm}
The objective of Probe Pruning (PP) is to implement online dynamic structured pruning in a batch-wise manner. The main idea of our work is illustrated in Figure~\ref{fig:main_figure}. Our core strategy involves: (1) \textbf{Probing} (Sections~\ref{section:probing} and \ref{section:probe_generation}), which consists of two steps: first, generating a probe based on residual importance; second, using the probe to run the unpruned model to gather valuable intermediate hidden state information. (2) \textbf{History-informed pruning} (Section~\ref{section:historical_pruning}), which carefully merges the probing states with historical states using importance-scaled fusion to capture the essential characteristics of each batch. Afterward, we prune the model using a novel pruning metric (Section~\ref{section:prune_metric}) that more effectively selects channels for pruning than existing metrics.
\vspace{-0.3cm}
\subsection{Probing} \label{section:probing}
\vspace{-0.3cm}
We introduce a novel concept called probing, which leverages the existing model structure and hidden states to form a predictive mechanism. Specifically, when the input hidden states reach block $l$, probing first utilizes residual importance to select key samples and tokens, forming the probe $\tP^{l}$ from $\LN^{l}(\tX^{l})$. $\LN^{l}$ represents the layer normalization at block $l$. The process of probe generation is detailed in the next section. It then runs the intermediate transformation in block $l$, denoted by $\tranT^{l}(\tP^{l})$. Notably, effective probing consumes few computational resources and can obtain important intermediate-state information to guide pruning decisions. 
% as the probe greatly reduces the batch and sequence dimensions.
\vspace{-0.3cm}
\paragraph{Upper Bound of Probing.} As an alternative, we can generate the probe by using all the input hidden states in the current batch, $\tP^{l} = \LN^{l}(\tX^{l})$, a method we refer to as \textit{Full-Batch Probing}. By utilizing the entire batch without reducing the dimensions $N$ or $S$, Full-Batch Probing captures the complete intermediate hidden state information, which could potentially lead to optimal pruning performance. However, this approach significantly increases computational resource requirements and latency. Therefore, Full-Batch Probing serves as a theoretical upper bound for our method. Our aim for PP is to select pruning channels similar to those chosen by Full-Batch Probing. We believe that a higher proportion of common pruning channels between PP and Full-Batch Probing indicates better model performance and higher quality of the probe.
\vspace{-0.2cm}
\paragraph{Why Does Probing Work?} Probing is effective because not all samples and tokens contribute equally to the model's output, and large-magnitude outliers in LLMs significantly influence the model’s behavior. In natural language sequences, certain tokens carry more semantic or syntactic significance than others~\citep{xiao2023efficient,sun2024massive, liu2024intactkv}. By selecting key samples and tokens based on residual importance, the probe focuses on the most informative parts within the batch. This targeted approach allows the probe to capture essential intermediate hidden state information that is most influential in determining which channels can be pruned. Consequently, even though the probe processes a reduced subset of the batch, it provides sufficient insight to guide pruning decisions, potentially achieving results comparable to Full-Batch Probing with significantly lower computational costs.
\vspace{-0.2cm}
\paragraph{Computational Complexity.} Only minimal computational complexity is required for probing. Specifically, for an LLM characterized by six linear transformations per attention and MLP block (Q/K/V/O and FC1/FC2) that incorporate weight transformations and the attention mechanism, the dense matrix computational complexity for an LLM totals $O(6 N S \Cin \Cout + 2 N S^2 \Cin)$. For probing, by reducing the batch size to $x\%$ and the sequence length to $y\%$ of their original sizes, the complexity reduces to $O(4 x\% \cdot y\% \cdot N S \Cin \Cout + 2 x\% \cdot (y\%)^2 \cdot N S^2 \Cin)$.
\vspace{-0.2cm}
\subsection{Probe Generation} \label{section:probe_generation}
\begin{algorithm}[htp]
\begin{footnotesize}
\SetAlgoLined
\DontPrintSemicolon
\Input{
An LLM $\modelM$ with $L$ blocks, each containing the Transformation $\tranF^{l}$, the Intermediate transformation $\tranT^{l}$, and Layer Normalization $\LN^{l}$; calibration dataset $D$; Inference batches $B$.
}
\kwSystem{}{
    Run the calibration dataset $D$ using model $\modelM$ to obtain historical states $\tV$. \;
    % \For{\textup{each batch} $b \in B$}{
    \For{$t$\textup{-th batch} $B^{t}$}{
    % \ForEach{batch $b \in B$}{
        Initialize the hidden state $\tX^{0}$ for batch $B^{t}$. \;
        \For{\textup{each block} $l = 0, \dots, L-1$}{
            Generate a probe $\tP^{l}$ from $\LN^{l}(\tX^{l})$, utilizing the residual importance (Section~\ref{section:probe_generation}). \;
            Use $\tP^{l}$ to execute the intermediate transformation of block $l$ and gather the resulting intermediate hidden states, denoted as $\tX^{l, \text{int}, \text{probe}} = \tranT^{l}(\tP^{l})$. \;
            Use importance-scaled fusion to integrate the probing states $\tX'^{l, \text{int}, \text{probe}}$ with historical states (Section~\ref{section:historical_pruning}). \;
            Compute the PPsp pruning metric from the integrated states (Section~\ref{section:prune_metric}), and subsequently prune the weight channels accordingly. \;
            Execute full inference on $\tX^{l}$ using the pruned weights $\tilde{\mW}^{l}$, denoted by $\tilde{\tranF}^{l}(\tX^{l})$. \;
        }
    }
}
\caption{Probe Pruning}
\label{alg:pp}
\end{footnotesize}
\end{algorithm}
\vspace{-0.3cm}
PP measures the \textit{residual importance}, which is the $\normltwo$ norm of $\tX^{l}$ across specific dimensions to identify key samples and tokens. Once identified, these key samples and tokens are selected from $\LN^{l}(\tX^{l})$ to generate a probe for block $l$, where $\LN^{l}$ denotes layer normalization at block $l$. We do not utilize the importance derived from $\LN^{l}(\tX^{l})$ to identify key samples and tokens because layer normalization substantially alters the input hidden states.

To measure the residual importance along the target dimension, we compute the $\normltwo$ norm of $\tX^{l}$ across non-target dimensions. The target dimension may be either the batch or sequence dimension.
\begin{align}
\tU^{l, \text{batch}}_{i} &= \|\tX^{l}_{i, :, :}\|_{2}, \quad \text{for } i = 1, \dotsc, N, \label{eq:importance_score_for_probe_generation_batch}\\ 
\tU^{l, \text{seq}}_{j} &= \|\tX^{l}_{:, j, :}\|_{2}, \quad \text{for } j = 1, \dotsc, S.  \label{eq:importance_score_for_probe_generation_seq}
\end{align}
After computing the importance scores, we sort them in descending order and store the indices in \(\sI\):
\begin{align}
\sI^{l, \text{batch}} = \text{argsort}(-\tU^{l, \text{batch}}), \label{eq:sorted_indices_batch}\\ 
\sI^{l, \text{seq}} = \text{argsort}(-\tU^{l, \text{seq}}). \label{eq:sorted_indices_seq}
\end{align}
Using the sorted indices, we then generate the probe by selecting the top $x\%$ of samples or $y\%$ of tokens from the layer-normalized $\tX^{l}$:
\begin{equation}
\tP^{l} = \begin{cases}
\LN^{l}(\tX^{l})_{\sI^{l, \text{batch}}_{:x\%}, :, :} & \text{if selecting top } x\% \text{ of samples}, \\
\LN^{l}(\tX^{l})_{:, \sI^{l, \text{seq}}_{:y\%}, :} & \text{if selecting top } y\% \text{ of tokens}. 
\end{cases}
\label{eq:probe_generation_from_scores}
\end{equation}
This method ensures that the probe consists of the most significant samples and tokens, as ranked by their importance scores.  

PP implements a sequential approach to prune both sequence and batch dimensions effectively. Initially, the top $y\%$ of tokens are selected from the residual $\tX^{l}$, guided by Eqs.~(\ref{eq:importance_score_for_probe_generation_seq}) and~(\ref{eq:sorted_indices_seq}), leveraging the sequence distribution within the current batch: $\tX^{l}_{:, \sI^{l, \text{seq}}_{:y\%}, :}$. Subsequently, we apply this reduced sequence set to determine the top $x\%$ of samples using Eqs.~(\ref{eq:importance_score_for_probe_generation_batch}) and~(\ref{eq:sorted_indices_batch}), resulting in the indices $\sI^{l, \text{batch}|\text{seq}}$. Finally, we select the key samples and tokens for probe generation as $\LN^{l}(\tX^{l})_{\sI^{l, \text{batch}|\text{seq}}_{:x\%}, \sI^{l, \text{seq}}_{:y\%}, :}$.
\vspace{-0.4cm}
\subsection{History-Informed Pruning with Importance-Scaled Fusion}  \label{section:historical_pruning}
\vspace{-0.2cm}
The intermediate hidden states of the probe, given by
\begin{equation}
\tX^{l, \text{int}, \text{probe}} = \tranT^{l}(\tP^{l})
\end{equation}
contain crucial information that guides pruning decisions. However, when the probe is very small—for instance, when $N$ and $S$ are reduced to 5\%—they might lead to inappropriate pruning decisions due to limited context. To address this issue and enhance performance, we introduce history-informed pruning with importance-scaled fusion. 

To simplify notation, we omit the superscript $l$, which denotes the block number, in this section. For intermediate hidden states $\tX^{\text{int}}$ of shape $\left(N, S, \Cin\right)$, the following relationship holds:
\begin{equation}
\sum_{j=1}^{S}\sum_{i=1}^{N} ( \tX^{\text{int}}_{i, j, k})^2 = \sum_{j=1}^{S}|| \tX^{\text{int}}_{:, j, k} ||_2^2  = || \tX^{\text{int}}_{:, :, k} ||_2^2
\label{eq:split_metric_into_two_step}
\end{equation}
We compress the batch dimension in the first step of Eq.~\ref{eq:split_metric_into_two_step} to store historical states because memory limitations prevent storing the intermediate hidden states of all samples. We sum over the sequence dimension in the second step of Eq.~\ref{eq:split_metric_into_two_step} to obtain the tensor in shape $\sR^{\Cin}$, which is used to compute the pruning metric (see Section~\ref{section:prune_metric}).

As in previous studies~\citep{sun2023simple, an2023fluctuation}, we process the calibration dataset $D$ using the model $\modelM$ to obtain initial historical states. For each block, initial historical states are represented by $\tV|^{0} \in \sR^{S \times \Cin}$, computed as the first step of Eq.~\ref{eq:split_metric_into_two_step} to reduce the batch dimension: $\tV|^{0} = || \tX^{\text{int}}_{:, j, k} ||_2^2 = \sum_{i=1}^{N} ( \tX^{\text{int}}_{i, j, k})^2$. Similarly, to reduce the batch dimension of probe's intermediate hidden states $\tX^{\text{int}, \text{probe}} \in \sR^{x\% \cdot N \times y\% \cdot S \times \Cin}$, we calculate probing states as $|| \tX^{\text{int}, \text{probe}}_{:, j, k} ||_2^2 = \sum_{i=1}^{x\% \cdot N} ( \tX^{\text{int}, \text{probe}}_{i, j, k})^2$.
\vspace{-0.2cm}
\paragraph{Importance-Scaled Fusion.} Since probing can be performed with selected tokens, it is necessary to align the sequence dimension. We define $\tV^{\text{probe}} = \tV_{\sI^{\text{seq}}_{:y\%}, :}$, where $\tV^{\text{probe}} \in \sR^{y\% \cdot S \times \Cin}$ and $\sI^{\text{seq}}_{:y\%}$, obtained from Eq.~\ref{eq:sorted_indices_seq}, represents the indices of the top $y\%$ of tokens. We then apply importance-scaled fusion to obtain integrated states:
\begin{equation}
\label{importance-scaled fusion}
\hat{\tX}^{\text{int}, \text{probe}} = \frac{|| \tX^{\text{int}, \text{probe}}_{:, j, k} ||_2^2}{|| \tX^{\text{int}, \text{probe}}_{:, j, k} ||_2^2 + \tV^{\text{probe}}} \cdot || \tX^{\text{int}, \text{probe}}_{:, j, k} ||_2^2 + \frac{ \tV^{\text{probe}}}{|| \tX^{\text{int}, \text{probe}}_{:, j, k} ||_2^2 +  \tV^{\text{probe}}} \cdot  \tV^{\text{probe}},
\end{equation}
where $\hat{\tX}^{\text{int}, \text{probe}} \in \sR^{y\% \cdot S \times \Cin}$. Following the second step of Eq.~\ref{eq:split_metric_into_two_step}, we sum $\hat{\tX}^{\text{int}, \text{probe}}$ over the sequence dimension to obtain $\sum_{j=1}^{y\% \cdot S}\hat{\tX}^{\text{int}, \text{probe}}_{j, k}$. Note that without importance-scaled fusion, $\sum_{j=1}^{y\% \cdot S}\hat{\tX}^{\text{int}, \text{probe}}_{j, k}$ can reduce to $\|\tX^{\text{int}}_{:, :, k}\|_2^2$. Then, we use $\mW^{\text{final}}$ and $\sum_{j=1}^{y\% \cdot S}\hat{\tX}^{\text{int}, \text{probe}}_{j, k}$ as a surrogate of $\|\tX^{\text{int}}_{:, :, k}\|_2^2$ to calculate the pruning metric based on Eq.~(\ref{eq:ppwanda_score}), and prune the weight channels accordingly. Finally, we run full inference on the remaining weights.
\vspace{-0.2cm}
% obtain $\hat{\tX}^{'\text{int}, \text{probe}}$, which represents the square of the $\normltwo$ norm along the feature dimension. 
\paragraph{Update Historical States with Full Inference.} To enhance the tracking of intermediate hidden state attributes, we implement an exponential moving average during full inference on the selected weight channels $\sC$. The update formula is expressed as:
\begin{equation}
\label{moving_average}
     \tV_{:, \sC}|^{t} = \lambda \tV_{:, \sC}|^{t-1} + (1-\lambda) || \tilde{\tX}^{\text{int}}_{:, j, \sC} ||_2^{2}|^{t},
\end{equation}
The value of $\tV$ is updated for $t$-th inference batch, and $\tilde{\tX}^{\text{int}}$ represents the intermediate hidden states during full inference. We consistently set $\lambda = 0.99$ across all implementations.
\vspace{-0.3cm}
\subsection{Pruning Metric} \label{section:prune_metric}
\vspace{-0.2cm}
We propose a new structured pruning metric named PPsp, where "sp" stands for structured pruning. This metric more effectively selects channels for pruning compared to existing metrics. We adapt the unstructured pruning metric Wanda~\citep{sun2023simple} to a structured pruning scenario. PPsp introduces two enhancements: (1) we preserve the inherent importance of individual weights, as represented by the squared value of the Wanda metric; and (2) we calculate the $\normltwo$ norm of the importance scores for MLP input channels and attention heads to determine the pruning structures' importance, rather than summing these scores across pruning structures.

We introduce the pruning metric for a general scenario. To enhance clarity, we omit the superscript $l$, which denotes the block number, in this section.  At each block, given intermediate hidden states $\tX^{\text{int}}$ of shape $\left(N, S, \Cin\right)$, where $N$ and $S$ represent the batch and sequence dimensions respectively, and the weight matrix $\mW^{\text{final}}$ of shape $\left(\Cout, \Cin\right)$, Wanda~\citep{sun2023simple} defines the importance of the individual weight $\mW^{\text{final}}_{i, k}$ as: 
\begin{equation}\label{eq:wanda_score}
    \tI_{i, k} = | \mW^{\text{final}}_{i, k} |\cdot || \tX^{\text{int}}_{:, :, k} ||_{2},
\end{equation}
where $|\cdot|$ denotes the absolute value operation, and $|| \tX^{\text{int}}_{:, :, k} ||_{2}$ evaluates the $\normltwo$ norm of the $k$th feature across the $\left(N, S\right)$ dimensions. These two scalar values are then multiplied to produce the final importance. However, as derived in Wanda~\citep{sun2023simple}, the inherent importance of an individual weight is defined by: 
\begin{equation}\label{eq:wanda_score_square}
    \tI_{i, k} = (| \mW^{\text{final}}_{i, k} |\cdot || \tX^{\text{int}}_{:, :, k} ||_{2})^2 = | \mW^{\text{final}}_{i, k} |^{2} \cdot || \tX^{\text{int}}_{:, :, k} ||_{2}^{2}.
\end{equation}
Wanda discards the squaring in Eq.~(\ref{eq:wanda_score_square}) in local weight importance ordering, as the non-negative nature of $|\mW^{\text{final}}_{i, k}|$ and $|| \tX^{\text{int}}_{:, :, j} ||_{2}$ does not impact the relative ordering of importance. However, when it comes to structured pruning, maintaining the inherent importance of individual weights is essential. Thus, we square the Wanda metric and compute the Euclidean distance across the $\Cout$ dimension of the input channel. The formula is given by:
\begin{equation}\label{eq:ppwanda_score}
    \tI_{k}=\left\| \left\{ | \mW^{\text{final}}_{i, k} |^2 \cdot || \tX^{\text{int}}_{:, :, k} ||_2^2 \right\}_{i=0}^{\Cout} \right\|_2,
\end{equation}
where $\{\cdot\}$ signifies the set of elements, and $\tI \in \sR^{\Cin}$. 
\vspace{-0.4cm}
\section{Experimental Setup} \label{experimental_setup}
\begin{table}[h]
\centering
\vspace{-0.4cm}
\caption{Comparison of LLM structured pruning methods. Our implementation loads the full model for dynamic pruning, while other methods load only the pruned version.}
\vspace{-0.2cm}
\label{tab:pruning_methods}
\scalebox{0.75}{
\begin{tabular}{ccccc}
\toprule
Method    & No Fine-tuning & Time-Efficient & Easy Integration & Dynamic Pruning \\
\midrule
Wanda-sp  & \cmark    & \cmark         & \cmark           & \tXmark          \\
FLAP      & \cmark    & \cmark         & \cmark           & \tXmark          \\
LLM-Pruner & \tXmark   & \tXmark         & \tXmark           & \tXmark          \\
LoRAPrune & \tXmark    & \tXmark         & \tXmark           & \tXmark          \\
PP        & \cmark    & \cmark         & \cmark           & \cmark          \\
\bottomrule
\end{tabular}
}
\end{table}
\vspace{-0.2cm}
We conduct three experiments using different random seeds for all tests and show the standard error across these three seeds in brackets. We conduct all experiments on NVIDIA A100 GPUs.
\vspace{-0.25cm}
\paragraph{Models and Evaluation.} We evaluate PP on three popular model families: LLaMA-2 7B/13B~\citep{touvron2023llama}, LLaMA-3 8B~\citep{llama3}, and OPT-13B~\citep{zhang2022opt}. Following previous work~\citep{sun2023simple, an2023fluctuation}, we evaluate the models on two zero-shot task categories. We evaluate accuracy on commonsense reasoning tasks, including BoolQ~\citep{clark-etal-2019-boolq}, PIQA~\citep{Bisk2020piqa}, HellaSwag~\citep{zellers2019hellaswag}, WinoGrande~\citep{ai2:winogrande}, ARC-Easy~\citep{allenai:arc}, ARC-Challenge~\citep{allenai:arc}, and OpenbookQA~\citep{OpenBookQA2018}. For evaluating perplexity on the text generation task, we use WikiText2~\citep{merity2016pointer}. We set the batch size to 20 for all tasks. For the commonsense reasoning tasks, our implementation follows~\citep{gao2021framework}, setting the sequence length of each batch to match its longest sample. For the text generation task, we set the sequence length to 1024. For PP, we set the default probe size to 5\% of the batch size and 50\% of the sequence length, approximating 1.5\% of the FLOPs cost relative to dense model inference. Figure~\ref{fig:differentprobestudy} shows ablation study results for various probe combinations, indicating small probes enhance model performance. Ablation studies of the PP and FLAP are available in Appendix~\ref{appendix:ablations}, and additional experimental results are available in Appendix~\ref{appendix:detailed_results}. 
\vspace{-0.3cm}
\paragraph{Baselines.} We compare our method, PP, with four previous approaches: Wanda-sp~\citep{an2023fluctuation}, FLAP~\citep{an2023fluctuation}, LoRAPrune~\citep{zhang2023pruning}, and LLM-Pruner~\citep{ma2023llm}. We also compare PP with its upper bound, Full-Batch Probing, as introduced in Section~\ref{section:probing}. Following~\citep{sun2023simple, an2023fluctuation}, we use the C4~\citep{raffel2020exploring} dataset as the calibration dataset for all methods. We use 2,000 calibration samples for PP, Wanda-sp, and FLAP, and 20,000 calibration samples for tuning LoRAPrune and LLM-Pruner. We evaluate pruning ratios of 20\% and 40\%.
% same subset of the
\vspace{-0.4cm}
\section{Results} \label{results_section}
\vspace{-0.3cm}
\begin{table}[ht!]
\centering
\caption{Zero-shot performance of LLaMA-2-7B/13B and OPT-13B after pruning attention and MLP blocks without fine-tuning: PP demonstrates superior performance in nearly all scenarios. Arrows indicate metric direction ($\downarrow$: lower is better; $\uparrow$: higher is better).}
\label{tab:main_result_threemodels}
\renewcommand{\arraystretch}{1.2}
\resizebox{\columnwidth}{!}{%
\begin{tabular}{ccccc|ccc}
\toprule
                   &                                                 & \multicolumn{3}{c|}{Text Generation $\downarrow$}                          & \multicolumn{3}{c}{Commonsense Reasoning $\uparrow$}     \\ \cmidrule(l){3-8} 
Method             & Pruning Ratio                     & LLaMA-2-7B         & LLaMA-2-13B        & OPT-13B             & LLaMA-2-7B    & LLaMA-2-13B   & OPT-13B       \\ \midrule
Dense              & 0\%                   & 6.0(0.1)           & 5.1(0.1)           & 11.6(0.1)           & 64.0            & 66.2          & 57.2          \\ \midrule
Full-Batch Probing         & 20\%                  & 7.3(0.1)           & 6.2(0.1)           & 12.6(0.1)           & 62.6          & 65.3          & 56.4          \\
Wanda-sp           & 20\%                  & 10.6(0.1)          & 9.0(0.1)           & 17.4(0.1)           & 61.5          & 65.0            & 55.2          \\
FLAP               & 20\%                  & 10.3(0.1)          & 7.5(0.1)           & 18.8(0.2)           & 61.4          & 64.6          & 54.9          \\
LoRAPrune w/o LoRA         & 20\%                  & 22.7(0.9)          & 16.1(0.7)          & \NA             & 57.9          & 58.9          & \NA       \\
LLM-Pruner w/o LoRA         & 20\%                  & 17.5(1.6)          & 11.3(0.7)          & \NA             & 57.4          & 61.3          & \NA       \\
\gr PP                 & 20\%              & \textbf{8.1(0.1)}  & \textbf{6.7(0.1)}  & \textbf{14.7(0.1)}  & \textbf{62.8} & \textbf{65.3} & \textbf{56.5} \\ \midrule
Full-Batch Probing        & 40\%                  & 13.6(0.1)           & 8.9(0.1)           & 17.9(0.2)           & 58.7          & 62.9          & 54.0            \\
Wanda-sp           & 40\%                  & 43.8(1.5)          & 21.6(0.4)          & 42.7(0.7)           & 54.8          & 56.6          & 50.5          \\
FLAP               & 40\%                  & 38.9(1.3)          & 15.5(0.0)          & 51.0(0.7)           & 54.9          & 60.6          & 50.8          \\
LoRAPrune w/o LoRA         & 40\%                  & 129.5(3.0)         & 74.8(6.4)          & \NA             & 45.4          & 48.1          & \NA       \\
LLM-Pruner w/o LoRA        &  40\%                 & 51.1(4.3)          & 34.5(2.4)          & \NA             & 47.8          & 52.0            & \NA       \\
\gr PP             &  40\%                 & \textbf{16.8(0.1) }         & \textbf{11.3(0.1)}          & \textbf{26.7(0.3)}  & \textbf{56.6} & \textbf{61.0}   & \textbf{53.1} \\ \bottomrule
\end{tabular}%
}
\end{table}
\begin{table}[ht!]
\centering
\vspace{-0.2cm}
\caption{Zero-shot performance of LLaMA-3-8B after pruning MLP blocks without fine-tuning: PP demonstrates superior performance in nearly all scenarios.}
\label{tab:main_result_llama-3-8b}
\renewcommand{\arraystretch}{1.2}
\resizebox{\columnwidth}{!}{%
\begin{tabular}{ccc|cccccccc}
\toprule
Method               & Pruning Ratio & WikiText2 $\downarrow$ & BoolQ              & PIQA               & HellaSwag          & WinoGrande         & ARC-c              & ARC-e              & OBQA               & Average $\uparrow$      \\ \midrule
Dense                & 0\%        & 6.8(0.0)  & 81.7(0.0)          & 79.5(0.0)          & 76.3(0.0)          & 72.5(0.0)          & 47.2(0.0)          & 61.7(0.0)          & 40.2(0.0)          & 65.6          \\ \midrule
Full-Batch Probing & 20\%       & 8.5(0.0)  & 79.0(0.0)          & 80.1(0.0)          & 74.8(0.0)          & 73.9(0.0)          & 44.9(0.0)          & 60.7(0.0)          & 40.2(0.0)          & 64.8          \\
Wanda-sp             & 20\%       & 10.0(0.0) & 75.1(0.3)          & 78.5(0.0)          & 69.6(0.2)          & 71.4(0.4)          & 38.7(0.4)          & 56.9(0.4)          & 39.0(0.2)          & 61.3          \\
FLAP                 & 20\%       & 10.0(0.0) & \textbf{79.4(0.2)} & \textbf{78.7(0.1)} & 70.3(0.0)          & 71.4(0.5)          & 40.8(0.1)          & 57.8(0.0)          & 39.4(0.3)          & 62.5          \\
\gr PP                   & 20\%       & \textbf{9.3(0.0)}  & 77.4(0.0)          & 78.5(0.0)          & \textbf{73.1(0.0)} & \textbf{72.5(0.3)} & \textbf{43.2(0.3)} & \textbf{59.1(0.2)} & \textbf{40.2(0.5)} & \textbf{63.4} \\ \midrule
Full-Batch Probing & 40\%       & 12.3(0.1) & 73.1(0.0)          & 77.8(0.0)          & 70.5(0.0)          & 70.3(0.0)          & 42.9(0.0)          & 58.9(0.0)          & 39.8(0.0)          & 61.9          \\
Wanda-sp             & 40\%       & 18.4(0.1) & 66.6(0.1)          & 73.4(0.2)          & 56.7(0.1)          & 63.2(0.2)          & 31.8(0.2)          & 47.0(0.5)          & 34.5(0.2)          & 53.3          \\
FLAP                 & 40\%       & 18.5(0.2) & 67.3(1.0)          & 73.5(0.0)          & 57.2(0.2)          & 66.7(0.5)          & 31.7(0.3)          & 44.6(0.3)          & 34.4(0.3)          & 53.6          \\
\gr PP                   & 40\%       & \textbf{14.9(0.1)} & \textbf{70.3(0.1)} & \textbf{76.3(0.2)} & \textbf{65.3(0.1)} & \textbf{67.2(0.2)} & \textbf{39.0(0.3)} & \textbf{57.4(0.1)} & \textbf{36.9(0.3)} & \textbf{58.9} \\ \bottomrule
\vspace{-1.2cm}
\end{tabular}%
}
\end{table}
\paragraph{Main Results.} We present the zero-shot performance, without fine-tuning, of four models on text generation and commonsense reasoning tasks, as shown in Tables~\ref{tab:main_result_threemodels} and~\ref{tab:main_result_llama-3-8b}. Probe Pruning (PP) consistently outperforms all baselines across various models and pruning ratios. For instance, on WikiText2 at a 40\% pruning ratio, PP achieves lower perplexities than competing methods: 16.8 with LLaMA-2-7B, 11.3 with LLaMA-2-13B, and 26.7 with OPT-13B. Moreover, PP attains significantly lower perplexities and higher reasoning task accuracies than both LLM-Pruner and LoRAPrune. For example, on LLaMA-2-13B at a 40\% pruning ratio, PP achieves an average accuracy of 61.0\%, significantly higher than 52.0\% for LLM-Pruner and 48.1\% for LoRAPrune. On LLaMA-3-8B, PP surpasses Wanda-sp and FLAP in nearly all tasks, confirming its effectiveness and robustness. For instance, at a 40\% pruning ratio, PP achieves an average accuracy of 58.9\%, outperforming Wanda-sp (53.3\%) and FLAP (53.6\%). In Section~\ref{section:probing}, we stated that Full-Batch Probing represents the upper bound of PP. Experimental results confirm that Full-Batch Probing excels in all tested scenarios, supporting our hypothesis. Compared to Full-Batch Probing, which requires significant extra computational resources—more than dense model inference—PP achieves comparable results while utilizing minimal computational resources, only 1.5\% of the FLOPs compared to dense model inference. These results imply the effectiveness of PP and demonstrate that the probe's intermediate hidden states can help identify the important weights for processing different batches.
\begin{figure*}[ht]
    \centering
    \includegraphics[width=1\linewidth]{Figures/commonpruningchannels.pdf}
    \vspace{-0.7cm}
    \caption{Jaccard Index of common pruning channels: comparing PP and Full-Batch Probing, and comparing fix-pruned model (without PP) and Full-Batch Probing for each batch.}
    \vspace{-0.75cm}
    \label{fig:commonpruningchannels}
\end{figure*}
\paragraph{Jaccard Index of Common Pruning Channels.} To verify our assumption in Section~\ref{section:probing} that a greater overlap of pruning channels between PP and Full-Batch Probing correlates with enhanced model performance and probe quality, we measure the Jaccard Index~\citep{jaccard1912distribution} of common pruning channels in two comparisons: between PP and Full-Batch Probing, and between the fix-pruned model (without PP) and Full-Batch Probing. The Jaccard Index is a statistical measure of the similarity between two sets, defined as the size of their intersection divided by the size of their union. We consistently apply the PPsp metric in all comparisons. As shown in Figure~\ref{fig:commonpruningchannels}, PP consistently selects pruning channels more similar to those selected by Full-Batch Probing across almost all attention and MLP blocks, in contrast to the fix-pruned model (without PP). This increased alignment of channels contributes to improved overall performance and indicates that the probe's intermediate hidden states can help guide pruning decisions.
\begin{figure*}[ht]
    \centering
    \includegraphics[width=1\linewidth]{Figures/differentprobestudy.pdf}
    \vspace{-0.65cm}
    \caption{Performance of different probe combinations at a 40\% pruning ratio.}
    \vspace{-0.45cm}
    \label{fig:differentprobestudy}
\end{figure*}

\begin{wraptable}[10]{r}{0.3\textwidth} % 'r' aligns the table to the right, adjust width as needed
 \centering
 \small
 \vspace{-0.3cm}
 \caption{ \small{Comparison of FLOPs between dense model inference and probing.}}
 \vspace{-0.1cm}
 \label{tab:flops_comparison}
 \renewcommand{\arraystretch}{1.1}
 \resizebox{0.3\textwidth}{!}{ % adjust the width to fit within the wraptable
 \begin{tabular}{ccc}
 \toprule
 \multirow{2}{*}{Method} & \multicolumn{2}{c}{\begin{tabular}[c]{@{}c@{}}Computational Cost \\ (TFLOPs)\end{tabular}} \\ \cmidrule(l){2-3} 
                         & \multicolumn{1}{c|}{WikiText2}                           & ARC-c                           \\ \midrule
 Dense                   & \multicolumn{1}{c|}{4420}                                & 4377                            \\
 \gr Probing                   & \multicolumn{1}{c|}{66 (1.5\%)}                                 & 69 (1.6\%)                      \\ \bottomrule
 \end{tabular}
 }
\end{wraptable}
\vspace{-0.3cm}
\paragraph{Effect of Probe Combinations on Performance.} We find that even a small probe can improve model performance. The results are shown in Figure~\ref{fig:differentprobestudy}. We investigate how different probe sizes affect PP's performance by varying the probe batch size from 1 to 20 (specifically, 1, 5, and 20) and the probe sequence ratio from 0.05 to 1.0 (specifically, 0.05, 0.1, 0.3, 0.5, 0.8, and 1.0).  First, we observe that once we apply PP, even a small probe with a batch size of 1 and a probe sequence ratio of 0.05 can yield performance improvements. For example, for LLaMA-2-7B, the perplexity drops from 29.8 to 21.7; for OPT-13B, it drops from 35.4 to 27.7. Furthermore, we observe that increasing both the probe batch size and sequence ratio leads to improved performance. Interestingly, we find that the initial increase in sequence ratio from 0.05 to 0.3 brings the most rapid performance improvement. This indicates that sequence information becomes significantly effective for pruning once it exceeds a certain size threshold relative to the current batch's sequence length.

\begin{wraptable}[13]{r}{0.55\textwidth} % 'r' aligns the table to the right, adjust width as needed
 \centering
 \small
 \vspace{-0.35cm}
\captionof{table}{Breakdown of inference runtime across all batches of WikiText2 at a 40\% pruning ratio. The speedup is calculated by dividing the dense model's inference runtime by the methods' inference runtime.}
\vspace{-0.3cm}
\label{tab:inference_speed}
\renewcommand{\arraystretch}{1.1}
\resizebox{0.55\textwidth}{!}{
\begin{tabular}{c|c|cccc}
\toprule
\multirow{2}{*}{Method} & \multirow{2}{*}{PRR} & \multicolumn{4}{c}{Runtime (s)}                 \\ \cmidrule(l){3-6} 
                        &                      & Attention & Speedup      & MLP   & Speedup      \\ \midrule
Dense                   & -                    & 0.612     & -            & 0.416 & -            \\
FLAP                    & 95.64                & 0.419     & 1.46$\times$ & 0.265 & 1.57$\times$ \\
Wanda-sp                & 106.48               & 0.395     & 1.55$\times$ & 0.278 & 1.50$\times$ \\
\gr PP                      & 37.37                & 0.420      & 1.46$\times$ & 0.319 & 1.30$\times$ \\ \bottomrule
\end{tabular}
}
\end{wraptable}

\vspace{-0.3cm}
\paragraph{Computational Cost and Inference Speed.} We use the DeepSpeed package~\citep{rasley2020deepspeed} to measure the FLOPs. The results in Table~\ref{tab:flops_comparison} show that the computational overhead of probing is approximating 1.5\% of the FLOPs of the dense model inference. This finding aligns with our analyzed computational complexity in Section~\ref{section:probing}. Additionally, we evaluate each block's end-to-end runtime across all batches of WikiText2 and the inference speedup at a 40\% pruning ratio on NVIDIA A100 GPUs, similar to previous studies~\citep{sun2023simple, ma2023llm}. The results for LLaMA-2-7B on WikiText2 are presented in Table~\ref{tab:inference_speed}. We find that the inference speeds of PP are comparable to those of other structured pruning baselines, yet it delivers superior performance. Specifically, in the attention block, PP achieves a speedup of $1.46\times$, and in the MLP block, a speedup of $1.30\times$. The slight delay observed in the MLP block can be attributed to inherent system costs, such as weight extraction. This gap narrows under conditions with larger batch sizes or longer sequence lengths, leading to comparable speeds between PP and the baselines. 
\vspace{-0.3cm}
\paragraph{Performance Runtime Ratio.} To illustrate the trade-off between model performance and inference speed, we introduce Performance Runtime Ratio (PRR), which quantifies the ratio of performance degradation per unit of runtime reduction. Importantly, a \textit{smaller} PRR value is preferable as it indicates minimal performance degradation per unit of runtime reduction. The metric is defined as:
\begin{equation}
\text{PRR}= \frac{|\text{Perf}_{\text{dense}} - \text{Perf}_{\text{pruned}}|}{\text{Runtime}_{\text{dense}} - \text{Runtime}_{\text{pruned}}},
\label{eq:relation_performance_runtime}
\end{equation}
where $\text{Perf}_{\text{pruned}}$ and $\text{Runtime}_{\text{pruned}}$ denote the performance and runtime of the pruned model, respectively, and $\text{Perf}_{\text{dense}}$ and $\text{Runtime}_{\text{dense}}$ denote the performance and runtime of the dense model, respectively. As shown in Table~\ref{tab:inference_speed}, the PRR of PP is 37.37, indicating a increase of 37.37 in perplexity per second of runtime reduction on the attention and MLP block. In comparison, FLAP and Wanda-sp have PRR values of 95.65 and 106.48, respectively. PP's PRR values are 2.56$\times$ (95.65 compared to 37.37) and 2.85$\times$ (106.48 compared to 37.37) more efficient than those of FLAP and Wanda-sp, respectively, indicating a significantly lower rate of performance degradation.
\vspace{-0.35cm}
\paragraph{Compared with Fine-tuned Baselines.} Table~\ref{tab:main_result_threemodels_w_finetuning} compares the performance of PP with fine-tuned baselines LoRAPrune and LLM-Pruner across different pruning ratios for text generation and commonsense reasoning tasks. Without fine-tuning, PP consistently outperforms or closely matches the fine-tuned models. At a 20\% pruning ratio, PP excels in both tasks across LLaMA-2-7B and LLaMA-2-13B models. At a 40\% pruning ratio, PP achieves comparable perplexity and significantly higher reasoning task accuracies. For example, PP achieves 61 on LLaMA-2-13B, while LoRAPrune achieves 55.5 and LLM-Pruner achieves 54.7.
\begin{table}[ht!]
\centering
\caption{Comparison of PP with fine-tuned baselines on LLaMA-2-7B/13B models, with attention and MLP layers pruned: PP consistently outperforms across scenarios without fine-tuning.}
\vspace{-0.3cm}
\label{tab:main_result_threemodels_w_finetuning}
\renewcommand{\arraystretch}{1.1}
\resizebox{1\textwidth}{!}{%
\begin{tabular}{ccccc|cc}
\toprule
                   &                          &                       & \multicolumn{2}{c|}{Text Generation $\downarrow$}                          & \multicolumn{2}{c}{Commonsense Reasoning $\uparrow$}     \\ \cmidrule(l){4-7} 
Method             & Pruning Ratio            & Fine-tuning        & LLaMA-2-7B         & LLaMA-2-13B                     & LLaMA-2-7B    & LLaMA-2-13B          \\ \midrule
Dense              & 0\%                   & \tXmark & 6.0(0.1)           & 5.1(0.1)                           & 64.0            & 66.2                    \\ \midrule
LoRAPrune w/ LoRA  & 20\%                         & \cmark & 8.7(0.2)           & 7.4(0.0)                        & 59.2          & 61.0                   \\
LLM-Pruner w/ LoRA & 20\%                         & \cmark & 10.2(0.3)          & 8.4(0.5)                        & 58.7          & 62.1                 \\
\gr PP                 & 20\%                         & \tXmark & \textbf{8.1(0.1)}  & \textbf{6.7(0.1)}    & \textbf{62.8} & \textbf{65.3}  \\ \midrule
LoRAPrune w/ LoRA  & 40\%                         & \cmark & \textbf{13.6(0.4)} & \textbf{11.1(0.3)}             & 52.1          & 55.5                \\
LLM-Pruner w/ LoRA &  40\%                        & \cmark & 20.3(1.3)          & 15.3(0.7)                       & 50.6          & 54.7                 \\
\gr PP                 &  40\%                        & \tXmark & 16.8(0.1)          & 11.3(0.1)           & \textbf{56.6} & \textbf{61.0}    \\ \bottomrule
\end{tabular}%
}
\vspace{-0.4cm}
\end{table}
\begin{wrapfigure}[12]{r}{0.3\textwidth} % "r" for right; use "l" for left; 0.5\textwidth for the width of the figure.
    \centering
    \vspace{-0.1cm}
    \includegraphics[width=0.3\textwidth]{Figures/ablationforfusion.pdf} % slightly less than the wrapfigure width to ensure it fits within the padding.
    \vspace{-0.7cm}
    \caption{Importance-scaled fusion studies.}
    \vspace{-1.7cm}
    \label{fig:ablationforfusion}
\end{wrapfigure}
\vspace{-0.7cm}
\paragraph{Importance-Scaled Fusion.} We compare importance-scaled fusion to three fixed integration ratios—0.1, 0.5, and 0.9—which assign a fixed ratio to the probing states during integration with historical states. We conduct experiments on LLaMA-2-7B using the WikiText2 dataset at a 40\% pruning ratio, keeping the probe batch size fixed at 1. The results in Figure~\ref{fig:ablationforfusion} demonstrate that importance-scaled fusion can leverage the benefits of the calibration dataset while minimizing associated biases.
\vspace{-0.2cm}
\paragraph{Pruning Metric.} Our PPsp consistently outperforms both Wanda-sp and FLAP across various pruning scenarios. We conduct experiments on fix-pruned models, each uniquely generated by one of three evaluated metrics, using only the calibration dataset. we evaluated three metrics at a uniform 40\% pruning ratio across all blocks on the WikiText2 dataset. As shown in Table~\ref{tab:comparing_different_metrics}, PPsp significantly reduces perplexity, achieving the lowest scores of 29.7 and 35.5 on the LLaMA-2-7B and OPT-13B models, respectively, compared to FLAP's 38.2 and 41.1, and Wanda-sp's 43.8 and 42.7.
\begin{table*}[ht!]
\vspace{-0.2cm}
\centering
\caption{Perplexity of WikiText2 across different metrics on models pruned by the calibration dataset, showing that PPsp performs best among the three metrics.}
\vspace{-0.2cm}
\label{tab:comparing_different_metrics}
\renewcommand{\arraystretch}{1.6}
\resizebox{\columnwidth}{!}{%
\begin{tabular}{ccccc|ccc}
\toprule
           &                        & \multicolumn{3}{c|}{LLaMA-2-7B}                             & \multicolumn{3}{c}{OPT-13B}                                 \\ \cmidrule(l){3-8} 
Metric     & Formula  & Attention          & MLP                & All                & Attention          & MLP                & All               \\ \midrule
Wanda-sp    &   $\sum_{i=1}^{\Cout}  |\mW^{\text{final}}_{i, k}| \cdot ||\tX^{\text{int}}_{:, :, k}||_2$             & 21.1(0.2)          & \textbf{10.9(0.1)}          & 43.8(1.5)         & 13.2(1.3) & 27.5(0.4) & 42.7(0.7)\\
FLAP       &   $\frac{1}{N-1} \sum_{n=1}^N ||\mW^{\text{final}}_{:, k}||^2_2 \cdot (\tX^{\text{int}}_{n, :, k} - \overline{\tX^{\text{int}}_{:, :, k}})^2$            & 17.7(0.3) & 11.0(0.1)  & 38.2(0.3) & \textbf{11.6(0.1)} & 27.3(0.0) & 41.1(0.3) \\
\gr PPsp &  $\left\| \left\{ | \mW^{\text{final}}_{i, k} |^2 \cdot || \tX^{\text{int}}_{:, :, k} ||_2^2 \right\}_{i=0}^{\Cout} \right\|_2$              & \textbf{15.4(0.6)} & \textbf{10.9(0.1)} & \textbf{29.7(0.3)} & 12.9(1.0) & \textbf{25.1(0.3)} & \textbf{35.5(0.3)} \\ \bottomrule
\end{tabular}%
}
\vspace{-0.25cm}
\end{table*}
\vspace{-0.6cm}
\section{Conclusion}
\vspace{-0.4cm}
In this paper, we propose Probe Pruning (PP), a novel online dynamic pruning framework that uses a \textit{small yet crucial} portion of hidden states to run the model and gain crucial pruning information to guide full inference. Notably, PP only relies on the original model structure and hidden states, \textit{without requiring additional neural network modules, or fine-tuning.} Furthermore, PP consistently surpasses all baselines, including those with fine-tuning in almost all experimental settings. Future research directions include refining the probe generation and probing process, integrating PP with advanced decoding and alignment techniques~\citep{MAP}, and exploring its robustness against poisoned models~\citep{xian2023understanding,xian2023unified,wang2023demystifying} or adversarial prompts~\citep{RAG}.


\section*{Acknowledgment}
The authors acknowledge the Minnesota Supercomputing Institute (MSI) at the University of Minnesota for providing resources that contributed to the research results reported in this paper. URL: http://www.msi.umn.edu.

The work of Qi Le was supported by the Amazon Machine Learning System Fellowship. The work of Xinran Wang and Ali Anwar was supported by the 3M Science and Technology Graduate Fellowship and the Samsung Global Research Outreach Award. The work of Jie Ding was supported in part by the National Science Foundation under CAREER Grant No. 2338506. The work of Ziyan Wang and Li Yang was supported by National Science Foundation under Grant No. 2348376.
% \newpage

\bibliography{iclr2025_conference}
\bibliographystyle{iclr2025_conference}

\newpage 
\appendix
\input{appendix}

\end{document}
