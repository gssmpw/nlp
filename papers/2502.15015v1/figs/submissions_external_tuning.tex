{\renewcommand{\arraystretch}{1.3}
\setlength{\tabcolsep}{4pt}
\hyphenpenalty=10000\exhyphenpenalty=10000
\begin{tabularx}{0.98\textwidth}{@{}p{1.5cm}>{\raggedright}p{2.5cm}>{\raggedright}p{1.5cm}p{1.5cm}X@{}}
\toprule
\textbf{Submission} & \textbf{Authors} & \textbf{Institutions} & \textbf{Framework} & \textbf{Description} \\ \midrule
\makecell{\textsc{PyTorch}\\ \textsc{Distr.}\\ \textsc{Submission}} & Hao-Jun Shi, Tsung-Hsien Lee, Anna Cai, Shintaro Iwasaki, Wenyin Fu, Yuchen Hao, Mike Rabbat & Meta Platforms  & \pytorch & Based on the Distributed Shampoo algorithm of \citet{anil2020shampoo} with an implementation tailored to leverage PyTorch performance optimizations. See~\cite{shi2023distributeddataparallelpytorchimplementation} for details. The submission uses a list of five hyperparameter settings. \\
\makecell{\textsc{Schedule} \\ \textsc{Free}\\ \textsc{AdamW}} & Alice Yang, Aaron Defazio, Konstantin Mishchenko & Meta AI, Samsung AI  & \pytorch & A externally tuned version of \sfadam \citep{defazio2024road} with a list of five \hp configurations. \\
\makecell{\textsc{General-}\\ \textsc{ized}\\ \textsc{Adam}} & George Dahl, Sourabh Medapati, Zack Nado, Rohan Anil, Shankar Krishnan, Naman Agarwal, Priya Kasimbeg, Vlad Feinberg & Google  & \jax & Submission with an \adam-style update rule, tuning over the use of Nesterov acceleration and preconditioning. Essentially tuning over \adamw \citep{Kingma2015}, \nadamw, and \sgd \citep{Robbins1951} with or without momentum.\\
\cycliclr & Niccolò Ajroldi, Antonio Orvieto, Jonas Geiping & MPI-IS, ELLIS Institute Tübingen  & \pytorch & Revisits the work of \citet{Loshchilov2017} and \citet{smith2017cyclical}, coupling \nadamw \citep{Dozat2016,Loshchilov2019} with a cyclic learning rate scheduler. Each cycle involves a linear warmup phase for the LR, followed by cosine annealing. \\
\nadamp & George Dahl, Sourabh Medapati, Zack Nado, Rohan Anil, Shankar Krishnan, Naman Agarwal, Priya Kasimbeg, Vlad Feinberg & Google  & \jax & Uses \nadamw with an extra tunable hyperparameter $p$ enabling $p$th root of denominator inside \nadamw update rule instead of the default of $2$. \\
\rowcolor{TUgray_light}\baseline & &  & \jax &  Baseline using \nadamw \citep{Dozat2016,Loshchilov2019} and a linear learning rate warmup followed by a cosine decay \citep{Dahl2023AlgoPerf}. \\
\amos & Ran Tian & Google & \jax & Submission based on the \amos optimizer \citep{Tian2022} with a list of five \hp settings.\\
\makecell{\textsc{CASPR}\\ \textsc{Adaptive}} & Sai Surya Duvvuri, Inderjit S. Dhillon, Cho-Jui Hsieh & UT Austin, UCLA, Google  & \jax &  A submission based on \citep{Duvvuri2024} with a list of five \hp configurations.\\
\lawaq & Niccolò Ajroldi, Antonio Orvieto, Jonas Geiping & MPI-IS, ELLIS Institute Tübingen  & \pytorch & Employs Latest Weight Averaging \citep{izmailov2018swa,kaddour2022lawa} on top of \nadamw \citep{Dozat2016,Loshchilov2019}, maintaining a queue of previous model weights. The queue is periodically updated during training and passed to the competition API for evaluation. \\
\lawaema & Niccolò Ajroldi, Antonio Orvieto, Jonas Geiping & MPI-IS, ELLIS Institute Tübingen & \pytorch &  Similar to \lawaq but maintaining an exponential moving average of the model weights, which is updated periodically during training and returned to the competition API for evaluation. \\
\makecell{\textsc{Schedule}\\ \textsc{Free}\\ \textsc{Prodigy}} & Alice Yang, Aaron Defazio, Konstantin Mishchenko &  Meta AI, Samsung AI & \pytorch & Combining Schedule-free \citep{defazio2024road} with the \textsc{Prodigy} optimizer \citep{Mishchenko2024}. \\
\bottomrule
\end{tabularx}
}
