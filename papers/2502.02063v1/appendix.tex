\appendix
\onecolumn

\section{Additional Qualitative Results} \label{sup:add_qual_res}
% Refer to the Demo Video
We provide a supplementary video on our project page, demonstrating side-by-side comparisons between baseline models (MDM, T2MGPT) and their {\modulename}-enhanced versions ({\modulename}-MDM, {\modulename}-T2MGPT). 
The video showcases the superiority of our method as 
{\modulename} enables more precise control over generated motions through text prompts, accurately capturing nuanced word differences and maintaining proper chronological order.


\section{Ablation Study}

\subsection{Different Text Encoder for {\modulename}}
To understand how different text encoders affect {\modulename}'s performance, we compare CLIP~\cite{radford2021learning} with BERT~\cite{devlin2018bert} using MDM as the base model. 
As shown in Tab.~\ref{tab:quant_bert}, BERT-based {\modulename} achieves better performance in text-motion alignment metrics (R-Precision: 0.511 vs. 0.478, MM-Dist.: 2.938 vs. 3.272) and motion diversity (9.630 vs. 9.468). 
While CLIP shows slightly better FID (0.303 vs. 0.346), the overall results suggest that BERT's contextualized word embeddings might be more suitable for capturing motion-relevant semantics.

This performance difference could be attributed to BERT's bidirectional context modeling and its pretraining objectives that focus on understanding relationships between word tokens, which aligns well with our goal of preserving composite-aware semantics and understanding the nuanced differences between token embeddings.

\begin{table}[h!]
\fontsize{9pt}{9pt}\selectfont
  \aboverulesep=0ex
  \belowrulesep=0.5ex 
\setlength{\tabcolsep}{8pt}
\centering
\caption{Quantitative results with different text encoder for {\modulename} on HumanML3D dataset.}
\begin{tabular}{lcccccccc}
\toprule
\multirow{2}{*}{Method} & \multirow{2}{*}{\modulename}  & \multirow{2}{*}{Text Encoder} & \multicolumn{3}{c}{R-Precision$\uparrow$} & \multirow{2}{*}{FID$\downarrow$} & \multirow{2}{*}{MM-Dist.$\downarrow$} & \multirow{2}{*}{Div.$\uparrow$} \\
 \cmidrule{4-6}
 & & & Top1 & Top2 & Top3 &  &   \\
\midrule
MDM & $\checkmark$ & BERT & \textbf{0.511} & \textbf{0.703} & \textbf{0.799} & 0.346 & \textbf{2.938} & \textbf{9.630} \\
MDM & $\checkmark$ & CLIP & 0.478 & 0.666 & 0.765 & \textbf{0.303} & 3.272 & 9.468 \\ 
\bottomrule
\label{tab:quant_bert}
\end{tabular}
\end{table}


\subsection{CLIP Embeddings from Different Layers for {\modulename}}

While our main experiments use CLIP's latent embeddings for semantic injection, it is interesting to understand how CLIP token embeddings from different layers for {\modulename} could potentially influence the performance of text-to-motion generation.
We compare the token embeddings from the output of the final layer and the output of the last layer before final projection.
Tab.~\ref{tab:quant_clip_latent} presents this comparison using MDM as the base model.
Using final layer embeddings shows stronger text-motion alignment (R-Precision Top1: 0.517 vs. 0.478) and lower multimodal distance (MM-Dist.: 2.945 vs. 3.272).
However, this comes at the cost of motion quality, as indicated by the higher FID score (0.410 vs. 0.303). 
This trade-off suggests that while the final-layer embeddings might better capture text-motion correspondences, the latent embeddings may provide a better balance between motion quality and semantic alignment.

\begin{table}[h!]
\fontsize{9pt}{9pt}\selectfont
  \aboverulesep=0ex
  \belowrulesep=0.5ex 
\setlength{\tabcolsep}{8pt}
\centering
\caption{Quantitative results with CLIP embeddings from different layers for {\modulename} on HumanML3D dataset.}
\begin{tabular}{lcccccccc}
\toprule
\multirow{2}{*}{Method} & \multirow{2}{*}{\modulename}  & \multirow{2}{*}{CLIP Embedding} & \multicolumn{3}{c}{R-Precision$\uparrow$} & \multirow{2}{*}{FID$\downarrow$} & \multirow{2}{*}{MM-Dist.$\downarrow$} & \multirow{2}{*}{Div.$\uparrow$} \\
 \cmidrule{4-6}
 & & & Top1 & Top2 & Top3 &  &   \\
\midrule
MDM & $\checkmark$ & Final & \textbf{0.517} & \textbf{0.709} & \textbf{0.806} & 0.410 & \textbf{2.945} & \textbf{9.735} \\
MDM & $\checkmark$ & Latent & 0.478 & 0.666 & 0.765 & \textbf{0.303} & 3.272 & 9.468 \\ 
\bottomrule
\label{tab:quant_clip_latent}
\end{tabular}
\end{table}




\section{Limitation with MLD} \label{sup:limit_mld}

We evaluate {\modulename} with MLD \cite{chen2023executing}, a diffusion-based method that operates in a learned latent space by compressing motion sequences into fixed-length vectors. 
As shown in Tab. \ref{tab:quant_mld}, {\modulename} provides limited improvements: while FID slightly improves (0.532→0.502), text-motion alignment metrics show marginal decreases (Top1 R-Precision: 0.469→0.452, MM-Distance: 3.282→3.389).
This performance pattern differs from our results with other models and can be attributed to MLD's choice of using fixed-length latent vectors for motion representation. 
While this design benefits motion editing tasks, it inherently limits the model's ability to capture composite motion structures. 
The observation suggests that {\modulename}'s effectiveness depends on the underlying motion representation's ability to preserve temporal and structural information.


\begin{table}[h!]
\fontsize{9pt}{9pt}\selectfont
  \aboverulesep=0ex
  \belowrulesep=0.5ex 
\setlength{\tabcolsep}{8pt}
\centering
\caption{Quantitative results with MLD and \modulename-MLD on HumanML3D dataset.}
\begin{tabular}{lccccccc}
\toprule
\multirow{2}{*}{Method} & \multirow{2}{*}{\modulename} & \multicolumn{3}{c}{R-Precision$\uparrow$} & \multirow{2}{*}{FID$\downarrow$} & \multirow{2}{*}{MM-Dist.$\downarrow$} & \multirow{2}{*}{Div.$\uparrow$} \\
 \cmidrule{3-5}
 & & Top1 & Top2 & Top3 &  &   \\
\midrule
MLD & & \textbf{0.469} & \textbf{0.659} & \textbf{0.760} & 0.532 & \textbf{3.282} & \textbf{9.570} \\
MLD & $\checkmark$ & 0.452 & 0.637 & 0.740 & \textbf{0.502} & 3.389 & 9.132 \\ 
\bottomrule
\label{tab:quant_mld}
\end{tabular}
\end{table}


\section{Visualization of Text-Motion Attention} \label{sup:vis_attn_weights}

% To understand how {\modulename} supports text-motion alignment, we analyze cross-attention in the \modulename-MDM model. 
% Figure \ref{fig:attn} shows the attention heatmap between generated motion and text at the first diffusion step. Early keyframes focus attention on initial words matching the waving motion. As keyframes progress, attention shifts to later text keywords, like `\textit{sit}' and `\textit{down}', illustrating how {\modulename} helps the model generate precise temporal motions.

We visualize the text-motion attention in {\modulename}-MDM to understand how text tokens influence motion generation at different motion frames. 
Figure \ref{fig:attn} shows the attention patterns for the prompt "\textit{a person wave his arms and then sit down}". 
The visualization demonstrates {\modulename}'s ability to capture temporal dependencies: early frames (0-40) attend strongly to words related to the waving motion, while later frames (60-100) shift attention to sit-related tokens. 
This progressive attention transition validates our design of composite-aware semantic injection for handling sequential motion descriptions.

%%% Visualization of attention weights 
\begin{figure*}[!h]
    \centering
    \includegraphics[width=\textwidth]{figures/Figure_attn.pdf}
    \vspace{-30pt}
    \caption{Visualization of attention weights in {\modulename}-MDM. 
    Top: Generated motion sequence for the prompt "\textit{a person wave his arms and then sit down}". 
    Bottom: Attention heatmaps for four attention heads and their average from the last layer. 
        % It shows the dynamic text-motion alignment as the sequence progresses from waving to sitting motion.
    }

    \vspace{-20pt}
\label{fig:attn}
\end{figure*}



% \section{User Study} \label{sub:user_study}


% \section{Additional Quantitative Evaluation} \label{sup:motioncritic}

