\vspace{-10pt}
\section{Introduction} \label{sec:intro}
\vspace{-3pt}

\begin{figure}[!ht]
    \centering
    \includegraphics[width=\linewidth]{figures/Fig1.pdf}
    \vspace{-35pt}
    \caption{(\textit{\textbf{Top}}) Fixed-length semantic injection, which primarily relied on the [CLS] token embedding from CLIP~\cite{radford2021learning} to represent the entire text prompt, fails to capture the subtle differences in individual words. As a result, it generates highly similar motions from distinct text prompts. (\textit{\textbf{Bottom}}) Our Composite aware semantic injection method allows each motion frame to dynamically attend to every word token (e.g., ``left'' or ``right'' hand), enhancing the motion-text correspondence.}
    \vspace{-10pt}
\label{fig:motivation}
\end{figure}

Human motion generation from text descriptions has gained increasing attention from the community, due to its immense potential for animating and editing lifelike motions with free-form text prompts \cite{MotionFix, goel2024iterative}.
The advancements in generative modeling techniques and efficient motion tokenization methods have been driving the progress in text to motion generation, with particular focus on enhancing motion quality.
Several diffusion based \cite{tevet2023human, zhang2022motiondiffuse, guo2023momask, chen2023executing, CondMDICohan2024, chang2024learning} and autoregressive based \cite{zhang2023generating, huang2024como, jiang2024motiongpt, chen2024motionllm} methods have shown impressive capability in generating realistic and diverse motions from the input prompts. 
Typically, these methods leverage the pre-trained CLIP model \cite{radford2021learning} to summarize the whole motion description with various lengths and complexity into a fixed-length embedding, denoted as [CLS]. 
The compressed text embedding is then used as a conditional vector, injected into the motion generation model.
% The CLIP text encoder uses a fixed length high-dimensional token [CLS] to summarize motion descriptions with various lengths and complexity.
The assumption is that the [CLS] token contains rich and global semantics that are suitable for motion generation. 
While this approach may seem to work to a certain degree for most existing models, the composite nature of human motions, causal textual order, and the fine-grained alignment of the text and motion tokens are poorly preserved. 
As illustrated in Fig.~\ref{fig:motivation}, the fixed-length semantic injection struggles to produce distinguishable conditional vectors for longer descriptions, leading to poor generalization and limited control over generated motions.



To address these limitations, we introduce {\modulename}, Composite Aware Semantic Injection Mechanism (Sec.~\ref{sec:casim}). 
It comprises a composite aware text encoder and a text-motion aligner that learns dynamic alignment between each text tokens and motion frames.
Compared with fixed-length approaches, {\modulename} offers a more generalized and robust architecture, allowing each textual component to influence either specific composites or global attributes of the motion sequence.
Another key advantage of {\modulename} is its model-agnostic nature -- it can be easily integrated with both diffusion-based and autoregressive-based approaches while consistently improving text-motion matching and retrieval accuracy.
Furthermore, our method is compatible with various motion representations, from raw motion~\cite{tevet2023human} to different quantization approaches including VQVAE~\cite{zhang2023generating}, PoseCode~\cite{huang2024como}, and RVQ-VAE~\cite{guo2023momask}.


% What did we achieve?
Our experimental results (Sec.~\ref{sec:experiments}) demonstrate remarkable quantitative improvements in FID, R-precision, and Multimodality Distance, on HumanML3D and KIT benchmarks, as we apply our {\modulename} to 5 SOTA methods, including MDM~\cite{tevet2023human}, MotionDiffuse~\cite{zhang2022motiondiffuse}, T2MGPT~\cite{zhang2023generating}, COMO~\cite{huang2024como}, and MoMask~\cite{guo2023momask}.
{\modulename} can also improve motion quality and text-motion alignment for long-term human motion generation \cite{shafir2024human, lee2024t2lm}.
Analysis of attention weights inside {\modulename} verifies that our semantic injection mechanism effectively learns the expected dynamic matching between text tokens and motion frames, validating our design principles.
Qualitative results further demonstrate the superiority of {\modulename} over fixed-length semantic injection methods, showing precise motion control from text prompts and robust generalization to unseen text inputs.


