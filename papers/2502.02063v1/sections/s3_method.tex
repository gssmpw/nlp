\vspace{-5pt}
\section{Composite Aware Semantic Injection} \label{sec:casim}
\vspace{-1pt}

{\modulename}, Composite Aware Semantic Injection Mechanism, is designed to capture fine-grained semantic relationships between text descriptions and motion sequences. It preserves the composite nature of human motions and their causal textual ordering and allows each motion frame to dynamically align with relevant textual components at different granularities.
{\modulename} consists of two principal components: a composite aware text encoder and a text-motion aligner. 
{\modulename} exhibits model-agnostic properties, as it is applicable to both autoregressive and diffusion-based motion generators, which represent the two predominant genres for state-of-the-art models. 
In Section \ref{sub:formulation}, we introduce the formulation of {\modulename}, detailing its two major components. 
Section \ref{sub:autoregressive} describes for autoregressive motion generators and how {\modulename} is integrated. 
Section \ref{sub:duffsion} discusses diffusion-based motion generators and explains how to adopt {\modulename} 
% is adapted
in this framework.


% Key cross attention module + clip-based token embedding as a semantic injection module
\vspace{-3pt}
\subsection{{\modulename} Formulation} % module name
\label{sub:formulation}
\vspace{-3pt}

\textbf{Composite Aware Text Encoder.} Unlike traditional approaches that compress text descriptions into fixed-length [CLS] token, our text encoder preserves composite aware semantics through individual token-level embeddings. 
As shown in Fig. \ref{fig:CASIM} (left), the encoder comprises a pretrained text encoder inside which are blocks of multihead self-attention layers that learn the latent features for the input text. 
% , crucial for text-driven motion generation. 
We leverage the pre-trained text encoder from CLIP~\cite{radford2021learning}, project the latent encoder output to another embedding space, and inject the resulting token embeddings to the motion generator.
% The resulting text embeddings serve as conditions for the text-motion aligner. 
% The features capture both sentence-level context and word-level details.
Compared to fixed-length CLIP embeddings, our injection method preserve the semantics as granular as the token-level, which are essential for composite aware motion generation.

\textbf{Text-Motion Aligner.} The text-motion aligner is the core design of {\modulename} and can be integrated inside various motion generation models.
Specifically, it establishes dynamic correspondence between motion frames and text tokens using multi-head attention (MHA). 
As illustrated in Fig.~\ref{fig:CASIM} (middle), each motion token is used as query to attend with the keys and values obtained from all the text tokens. 
The subsequent motion embeddings then are updated through the attention-weighted aggregation from all relevant text tokens. 
Depending on the motion generation approach, the aligner employs multihead self-attention (MHSA) for autoregressive generation or multihead cross-attention (MHCA) for diffusion-based generation. 
% We detail these architectural variants in subsequent sections.


% \subsection{{\modulename} for Autoregressive Motion Generation}
% \label{sub:autoregressive}
% % Specific adaptation to autoregressive methods, T2MGPT, COMO
% Autoregressive motion generation is inspired by principles from language modeling \cite{jiang2024motiongpt, chen2024motionllm}. 
% These models first generate motion tokens, and subsequently, decode motion tokens into a complete motion sequence.

% These models initially employ an auto-encoder to transform a motion sequence $X=\{x_1,\dots,x_T\}$ into a sequence of motion tokens $M=\{m_1,
% \dots ,m_{T/l}\}$, wherein each subsequence of $l$ frames $X_l=\{x_i,...,x_{i+l}\}$ is encoded into a single motion token $z_l = \mathcal{E}(X_l)$. Concurrently, another decoder is trained to reconstruct the original motion $\hat{X}=\mathcal{D}(M)$. Subsequently, the models are trained to generate sequences of motion tokens autoregressively, conditioned upon a textual prompt $C$. \begin{equation}
%     P(M|T) = P(m_1|C)\prod_{i=1}^{T/l}P(m_{i+1}|m_1,\dots,m_i,C)
% \end{equation}

% In the context of autoregressive motion generation models, we select the MHSA block to serve as the aligner between text and motion sequences for each generative step $P(m_i|m_j^{<i},C)=MHSA(M^{<i}\oplus C)$. Initially, we concatenate the text token sequence $C=\{c_1,\dots c_L\}$ with the motion token sequence $M^{<i}$, and subsequently input this concatenated sequence into an autoregressive transformer configured in an encoder-only mode with stacks of MHSA blocks. The autoregressive transformer executes self-attention mechanisms between the motion and text token sequences to effectively align and update the motion tokens, facilitating the generation of the subsequent token, as demonstrated in Figure \ref{fig:CASIM} right.
\vspace{-3pt}
\subsection{{\modulename} for Autoregressive Motion Generation}
\label{sub:autoregressive}
\vspace{-1pt}

Autoregressive motion generation approaches, inspired by language modeling principles \cite{jiang2024motiongpt, chen2024motionllm}, typically follow a two-stage process. First, they learn a discrete representation of motions through tokenization. Given a motion sequence $X=\{x_1,\dots,x_T\}$, an encoder-decoder network is trained to transform it into a sequence of motion tokens $M=\{m_1,\dots,m_{T/l}\}$, where each subsequence of $l$ frames $X_l=\{x_i,\dots,x_{i+l}\}$ is mapped to a discrete token $m_i = \mathcal{E}(X_l)$. The decoder $\mathcal{D}$ learns to reconstruct the original motion: $\hat{X}=\mathcal{D}(M)$.
In the second stage, these methods train an autoregressive model to predict motion tokens sequentially conditioned on a text prompt $C$. The generation process is as follows:
\begin{equation}
P(M|C) = P(m_1|C)\prod_{i=1}^{T/l}P(m_{i+1}|m_1,\dots,m_i,C)
\end{equation}
where each new motion token is predicted based on both the text condition and previously generated tokens. The final motion is obtained by passing the generated tokens through the pretrained decoder $\mathcal{D}$.

% For autoregressive generation, {\modulename} utilizes MHSA as the text-motion aligner at each generation step $P(m_i|m_j^{<i},C)=MHSA(M^{<i}\oplus C)$. As shown in Figure \ref{fig:CASIM} (right), we concatenate the text token sequence $C={c_1,\dots,c_L}$ with the previously generated motion tokens $M^{<i}$. This concatenated sequence is processed by an encoder-only transformer with stacked MHSA blocks, which performs self-attention between motion and text tokens to guide the generation of the next motion token.


For autoregressive generation, {\modulename} leverages a GPT-style transformer \cite{radford2018improving, zhang2023generating} with MHSA blocks for the text-motion aligner to predict the motion tokens.
At generation step $i$, we concatenate the text token sequence $C=\{c_1,\dots,c_N\}$ with previously generated motion tokens $M^{<i}$, denoted as $C\oplus M^{<i}$. 
As shown in Figure \ref{fig:CASIM} (right), this concatenated sequence is processed by an GPT-style transformer with stacked MHSA blocks that enable dynamic interaction between motion and text tokens. 
The probability of generating the next token is computed as:
\begin{equation}
P(m_i|m_j^{<i},C)=\sigma(\mhsa(C \oplus M^{<i}))),
\end{equation} where $\sigma$ represents the softmax function.

This generation step is performed iteratively until the end-of-sequence token is predicted.
The integration of {\modulename} with autoregressive models enables each motion token to be generated with awareness of both previously generated motions and the full text description.


\begin{table}[t!]
% \small
% \footnotesize
% \scriptsize
\fontsize{7.5pt}{7.5pt}\selectfont
  \aboverulesep=0ex
  \belowrulesep=0.5ex 
\setlength{\tabcolsep}{4.2pt}
\centering
\caption{Results on the HumanML3D dataset. The original and {\modulename}-integrated models are shaded.}
\vspace{-5pt}
\begin{tabular}{lcccccc}
\toprule
\multirow{2}{*}{Method}  & \multicolumn{3}{c}{R-Precision$\uparrow$} & \multirow{2}{*}{FID$\downarrow$} & \multirow{2}{*}{MM-Dist.$\downarrow$} & \multirow{2}{*}{Div.$\uparrow$} \\
 \cmidrule{2-4}
 & Top1 & Top2 & Top3 &  &  &  \\
\midrule 
T2M & 0.457 & 0.639 & 0.740 & 1.067 & 3.340 & 9.188 \\
TM2T & 0.424 & 0.618 & 0.729 & 1.501 & 3.467 & 8.589 \\
MotionDiffuse & 0.491 & 0.681 & 0.782 & 0.630 & 3.113 & 9.410 \\
MLD & 0.469 & 0.659 & 0.760 & 0.532 & 3.282 & 9.570 \\
\rowcolor{Gray} MDM & 0.455 & 0.645 & 0.749 & 0.489 & 3.330 & 9.920 \\
\rowcolor{Gray} T2MGPT & 0.491 & 0.680 & 0.775 & 0.116 & 3.118 & 9.761 \\
FineMoGen & 0.504 & 0.690 & 0.784 & 0.151 & 2.998 & 9.263 \\
MotionGPT & 0.492 & 0.681 & 0.778 & 0.232 & 3.096 & 9.528 \\
CoMo & 0.502 & 0.692 & 0.790 & 0.262 & 3.032 & \textbf{9.936} \\
GraphMotion & 0.504 & 0.699 & 0.785 & 0.116 & 3.070 & 9.692 \\
\midrule
\rowcolor{Gray}
\modulename-MDM & 0.502 & 0.694 & 0.793 & 0.165 & 3.020 & 9.394 \\
\rowcolor{Gray}
\modulename-T2MGPT & \textbf{0.539} & \textbf{0.730} & \textbf{0.823} & \textbf{0.105} & \textbf{2.838} & 9.785 \\
%  & CoMo  & $\checkmark$ & 0.545 & 0.741 & 0.835 & 0.200 & 2.747 & 9.887 \\
\bottomrule
\label{tab:quant_res1}
\end{tabular}
\vspace{-5pt}
\end{table}


\begin{table}[t!]
% \small
% \footnotesize
% \scriptsize
\fontsize{7.5pt}{7.5pt}\selectfont
  \aboverulesep=0ex
  \belowrulesep=0.5ex 
\setlength{\tabcolsep}{4.2pt}
\centering
% \caption{Evaluation of generated motion on HumanML3D and KIT-ML dataset. {\modulename} integrated models is shaded}
\caption{Results on the KIT-ML dataset.
The original and {\modulename}-integrated models are shaded.}
\vspace{-5pt}
\begin{tabular}{lcccccc}
\toprule
\multirow{2}{*}{Method}  & \multicolumn{3}{c}{R-Precision$\uparrow$} & \multirow{2}{*}{FID$\downarrow$} & \multirow{2}{*}{MM-Dist.$\downarrow$} & \multirow{2}{*}{Div.$\uparrow$} \\
 \cmidrule{2-4}
 & Top1 & Top2 & Top3 &  &  &  \\
\midrule 
% Real & 0.424 & 0.649 & 0.779 & 0.031 & 2.788 & 11.080 \\
T2M & 0.370 & 0.569 & 0.693 & 2.770 & 3.401 & 10.910 \\
TM2T & 0.280 & 0.463 & 0.587 & 3.599 & 4.591 & 9.473 \\
MotionDiffuse & 0.417 & 0.621 & 0.739 & 1.954 & 2.958 & 11.100 \\
MLD & 0.390 & 0.609 & 0.734 & 0.404 & 3.204 & 10.800 \\
\rowcolor{Gray} MDM & 0.164 & 0.291 & 0.396 & 0.497 & 9.191 & 10.847 \\
\rowcolor{Gray} T2MGPT & 0.402 & 0.619 & 0.737 & 0.717 & 3.053 & 10.862 \\
FineMoGen & 0.432 & 0.649 & 0.772 & \textbf{0.178} & 2.869 & 10.850 \\
MotionGPT & 0.366 & 0.558 & 0.680 & 0.510 & 3.527 & 10.350 \\
CoMo & 0.422 & 0.638 & 0.765 & 0.332 & 2.873 & 10.950 \\
GraphMotion & 0.429 & 0.648 & 0.769 & 0.313 & 3.076 & 11.120 \\
\midrule
\rowcolor{Gray}
\modulename-MDM & \textbf{0.448} & \textbf{0.665} & \textbf{0.784} & 0.354 & \textbf{2.684} & \textbf{11.179} \\
\rowcolor{Gray}
\modulename-T2MGPT & 0.412 & 0.627 & 0.751 & 0.577 & 2.986 & 10.829 \\
\bottomrule
\label{tab:quant_res2}
\end{tabular}
\vspace{-5pt}
\end{table}




\vspace{-3pt}
\subsection{{\modulename} for Motion Diffusion Generation}
\label{sub:duffsion}
\vspace{-1pt}

Unlike autoregressive approaches, motion diffusion models generate motions through iterative denoising of a Gaussian noise sequence $X^\tau$. Let $T$ denote the sequence length and $D$ denote the motion dimension, then $X^\tau \in \mathbb{R}^{T \times D}$ represents the noised motion at diffusion step $\tau$. Given a noise-free motion sequence $X^0$, the diffusion process at each step $t$ samples the denoised motion according to:
\begin{equation}
X^{\tau-1} \sim \mathcal{N}(\mu_{\theta}(X^\tau, \tau, C), \Sigma(\tau)),
\end{equation}
where $\Sigma(\tau)$ is a fixed variance schedule, $C \in \mathbb{R}^{L \times d}$ denotes the text tokens with sequence length $L$ and embedding dimension $d$. The mean term $\mu_{\theta}(X^\tau, \tau, C)$ can be parameterized as $\mu_{\theta}(X^\tau, \hat{X^0})$, where $\hat{X^0}=G_\theta(X^\tau, \tau, C)$ is the predicted noise-free motion by the denoiser $G_\theta$.

While there are no constraints on how the denoiser should be designed as long as the input and output shape matches, the transformer encoder and decoder are the two widely adopted options in the literature.
To integrate {\modulename} with these denoiser variants in diffusion models, we adapt our text-motion aligner to employ MHSA for the transformer encoder and MHCA for the transformer decoder.


\textbf{Denoising with Transformer Encoder.} In the encoder-based approach, we first augment the text embeddings $C$ with diffusion step embeddings $TE(\tau)$, where $TE(\cdot)$ denotes the positional encoding function. The augmented text embeddings are then concatenated with the current motion embeddings $X^t$, and processed through MHSA blocks:
\begin{equation}
\hat{X^0} = \mhsa( (C+TE(\tau))\oplus X^\tau),
\end{equation}
where $\oplus$ denotes sequence concatenation.

\textbf{Denoising with Transformer Decoder.} The decoder-based variant first processes motion features through self-attention layers, enabling each motion frame to attend to other frames. It then applies cross-attention (MHCA) between the processed motion embeddings $X^\tau$ and the timestep-augmented text embeddings:
\begin{equation}
\hat{X^0}  = \mhca(X^\tau, C+TE(\tau)),
\end{equation}

Both formulations enable dynamic text-motion alignment during the denoising process. 
We analyze the performance of all model genres and variants in Section \ref{sub:quantitative}.


