\begin{figure*}[!t]
    \centering
    \includegraphics[width=\textwidth]{figures/Fig2.pdf}
    \vspace{-15pt}
    \caption{{\modulename} consists of two major components: Composite Aware Text Encoder (Left) for extracting granular word-level embeddings and Text-Motion Aligner (Middle) for aligning motion embeddings with relevant textual embeddings inside a motion generator. The attention score distribution between different motion tokens and the text tokens is visualized on the upper left. 
    % Red box show case a example of attention allocation between a subset of word and motion tokens.
    The Text-Motion Aligner can be integrated with three genres of motion generation models (Right).}
    \vspace{-5pt}
\label{fig:CASIM}
\end{figure*}

\vspace{-5pt}
\section{Related Works} \label{sec:related_works}
\vspace{-3pt}

\subsection{Human Motion Generation}
\vspace{-3pt}
% What are various tasks of X to motion generation
Generating realistic human motions has been a longstanding challenge in computer graphics and computer vision.
The field has evolved to embrace various input modalities and conditions for motion synthesis.
Image and video-based approaches have focused primarily on human pose and shape estimation~\cite{zhao2019semantic} and 3D body tracking~\cite{stathopoulos2024score}, enabling motion reconstruction and prediction from visual inputs.
Audio-driven motion generation is another important direction, with music-to-dance synthesis~\cite{alexanderson2023listen} and speech-to-gesture generation~\cite{chang2022ivi, chang2023importance} 
showing promising results in creating natural human movements that align with acoustic signals.
Text-to-motion generation has gained significant attention, as it offers intuitive control over motion synthesis through free-form text prompts~\cite{Guo_2022_CVPR}.
Scene-aware motion generation considers environmental constraints and spatial relationships, enabling the synthesis of contextually appropriate movements within 3D environments~\cite{cen2024text_scene_motion}.
Generating a coordinated group of human motions and interactions~\cite{chang2024learning, chang2024equivalency} 
has recently emerged as a novel research directionh adds another layer of difficulty to single-person motion generation due to the complex human interactions.
Lastly, several works \cite{li2024unimotion, zhou2024avatargpt} have attempted to unify motion generation, planning, and understanding in a single framework, extending the capabilities of large language models to human motion domains.


% \vspace{-5pt}
\subsection{Text-to-Motion Generation Models}
% \vspace{-3pt}

Text-to-motion generation models can be broadly categorized into two approaches: diffusion-based and autoregressive-based methods. 
Diffusion-based models leverage an iterative denoising scheme \cite{dhariwal2021diffusion} to generate motions from textual conditions. 
Notable works include MDM \cite{tevet2023human}, MotionDiffuse \cite{zhang2022motiondiffuse}, MLD \cite{chen2023executing}, GMD \cite{karunratanakul2023gmd}, FineMoGen \cite{zhang2023finemogen}, and GraphMotion \cite{jin2023act}. For example, MDM employs a transformer encoder within each diffusion step, processing concatenated motion frames with text and timestamp embeddings. While MLD adopts a similar architecture, it operates in a learned latent space by compressing motion sequences into fixed-length representations.
Autoregressive-based approaches, including T2M \cite{zhang2023generating}, TM2T \cite{guo2022tm2t}, MotionGPT \cite{jiang2024motiongpt}, MotionLLM \cite{chen2024motionllm}, T2MGPT \cite{zhang2023generating}, and CoMo \cite{huang2024como}, generate motions sequentially and typically require effective motion tokenization strategies. 
For instance, T2MGPT utilizes VQVAE \cite{van2017neural} for motion tokenization and implements a decoder-only architecture for motion token generation. CoMo follows a similar generator architecture but distinguishes itself by adopting heuristics-based posecodes \cite{delmas2022posescript} as its discrete motion representation.




\vspace{-2mm}
\subsection{Semantic Injection for Motion Generation}
\vspace{-1mm}

% More deeper dive into semantic injection and alignment for specific models, 
% Start with clip-based fixed length embedding
% such as finemogen, augmented prompts from chatgpt, CoMo keywords, heuristic-based hierarchical GraphMotion semantic injection 
% and how we differs from theirs.
For text-to-motion generation, the input prompts are typically encoded into a latent space with a well-trained text encoder before being passed to motion generation models. 
Previous works, such as MDM, MLD, T2MGPT and CoMo, leverage the pretrained CLIP text embedding to represent the full text prompt.
% as one of the inputs to their transformer-based encoder or decoder.
CoMo \cite{huang2024como} and FGMDM \cite{shi2023generating} includes several fine-grained keywords and descriptions from GPT4 \cite{openai2024gpt4technicalreport} as augmented prompts for motion generation.
Finemogen \cite{zhang2023finemogen} targets at fine-grained motion control and editing, by specifying the spatial and temporal motion descriptions.
GraphMotion \cite{jin2023act} parses the sentence structure into a hierarchical semantic graph for any given input texts. 
It utilizes a graph reasoning network and a coarse-to-fine diffusion model for motion generation.
Our {\modulename} is conceptually similar to GraphMotion as both are aimed at strengthening the text-motion correspondence by design. 
However, GraphMotion uses heuristic knowledge to create a static semantic graph and is only tied to its coarse-to-fine model.
{\modulename} learns the dynamic alignment and hierarchical structure in a soft manner. 
It can also be flexibly integrated with the most existing models.


