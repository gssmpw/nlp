
\begin{table}[t!]
\fontsize{7.5pt}{7.5pt}\selectfont
  \aboverulesep=0ex
  \belowrulesep=0.5ex 
\setlength{\tabcolsep}{4.2pt}
\centering
\caption{Quantitative results for various text-to-motion methods with {\modulename} on HumanML3D dataset.}
\vspace{-5pt}
\begin{tabular}{lcccccc}
\toprule
\multirow{2}{*}{Method} & \multirow{2}{*}{\modulename} & \multicolumn{3}{c}{R-Precision$\uparrow$} & \multirow{2}{*}{FID$\downarrow$} & \multirow{2}{*}{MM-Dist.$\downarrow$} \\
 \cmidrule{3-5}
 & & Top1 & Top2 & Top3 &  &   \\
\midrule 
\multirow{2}{*}{MotionDiffuse}  & & 0.491 & 0.681 & 0.782 & 0.630 & 3.113 \\
 & $\checkmark$  & \textbf{0.506} & \textbf{0.700} & \textbf{0.799} & \textbf{0.565} & \textbf{2.981} \\
\midrule
\multirow{2}{*}{MDM}  & & 0.471 & 0.661 & 0.760 & 0.325 & 3.249 \\
  & $\checkmark$ & \textbf{0.502} & \textbf{0.694} & \textbf{0.793} & \textbf{0.165} & \textbf{3.020} \\
\midrule
\multirow{2}{*}{T2MGPT}  & & 0.484 & 0.672 & 0.770 & 0.117 & 3.153 \\
  & $\checkmark$ & \textbf{0.539} & \textbf{0.730} & \textbf{0.823} & \textbf{0.105} & \textbf{2.838} \\
\midrule
\multirow{2}{*}{CoMo}  & & 0.502 & 0.692 & 0.790 & 0.262 & 3.032 \\
  & $\checkmark$ & \textbf{0.545} & \textbf{0.741} & \textbf{0.835} & \textbf{0.200} & \textbf{2.747} \\
\midrule
 % & MoMask  & & 0.521 & 0.713 & 0.807 & 0.045 & 2.958 & 9.645 \\
\multirow{2}{*}{MoMask}  & & 0.510 & 0.703 & 0.801 & 0.064 & 2.997 \\
  & $\checkmark$ & \textbf{0.532} & \textbf{0.719} & \textbf{0.813} & \textbf{0.057} & \textbf{2.911} \\
% \midrule
\bottomrule
\vspace{-5pt}

\label{tab:quant_res3}
\end{tabular}
\end{table}


\begin{table}[t!]

\fontsize{7.5pt}{7.5pt}\selectfont
  \aboverulesep=0ex
  \belowrulesep=0.5ex 
\setlength{\tabcolsep}{5.5pt}
\centering
\caption{Quantitative results for various text-to-motion methods with {\modulename} on KIT-ML dataset.}
\vspace{-7.5pt}
\begin{tabular}{lcccccc}
\toprule
\multirow{2}{*}{Method} & \multirow{2}{*}{\modulename} & \multicolumn{3}{c}{R-Precision$\uparrow$} & \multirow{2}{*}{FID$\downarrow$} & \multirow{2}{*}{MM-Dist.$\downarrow$} \\
 \cmidrule{3-5}
 & & Top1 & Top2 & Top3 &  &   \\
\midrule 

\multirow{2}{*}{MDM} &  & 0.164 & 0.291 & 0.396 & 0.497 & 9.191 \\
 & $\checkmark$ & \textbf{0.448} & \textbf{0.665} & \textbf{0.784} & \textbf{0.354} & \textbf{2.684} \\
\midrule 
\multirow{2}{*}{T2MGPT} & & 0.402 & 0.619 & 0.737 & 0.717 & 3.053 \\
 & $\checkmark$ & \textbf{0.412} & \textbf{0.627} & \textbf{0.751} & \textbf{0.577} & \textbf{2.986} \\
\midrule
\multirow{2}{*}{CoMo} &  & 0.399 & - & - & \textbf{0.399} & 2.898 \\
 & $\checkmark$ & \textbf{0.422} & \textbf{0.641} & \textbf{0.762} & 0.408 & \textbf{2.852} \\
\bottomrule
\label{tab:quant_res4}
\vspace{-5pt}
\end{tabular}
\end{table}



\vspace{-5pt}
\section{Experiments} \label{sec:experiments}
\vspace{-3pt}

We evaluate {\modulename} on two standard datasets through extensive quantitative and qualitative analyses. Section \ref{sub:eval_setup} describes our experimental setup, followed by quantitative results in Section \ref{sub:quantitative}, qualitative analysis in Section \ref{sub:qualitative}, and extension to long-form generation in Section \ref{sub:long-term}.

\vspace{-3pt}
\subsection{Evaluation Setup}
\label{sub:eval_setup}
\vspace{-3pt}

\textbf{Datasets and Metrics.} 
We evaluate on HumanML3D \cite{Guo_2022_CVPR} and KIT Motion Language (KIT-ML) \cite{Plappert2016}.
HumanML3D is a large-scale motion language dataset, containing a total of 14,616 motions from AMASS \cite{mahmood2019amass} and HumanAct12 \cite{guo2020action2motion} datasets. 
Each motion is paired with 3 textual annotations, totaling 44,970
descriptions.
KIT-ML dataset consists of a total of 3,911 motions and 6,278 text annotations, providing a small-scale evaluation benchmark. 
The datasets are split into train-valid-test sets with a ratio of 0.8:0.05:0.15. 
Performance is measured using the following metrics: 
(a) \textit{Frechet Inception Distance (FID)}, which evaluates the overall motion quality by computing the distributional difference between the latent features of the generated motions and those of real motions from test set; 
(b) \textit{R-Precision}, which reports the retrieval accuracy between input text and generated motions from their latent space; 
(c) \textit{MM-Distance}, which reports the distance between the input text and generated motions at the latent space; and
(d) \textit{Diversity}, which assesses the diversity of all generated motions.
All metrics are computed using a separate text-motion matching network from \cite{Guo_2022_CVPR}.

\textbf{Baseline Models.} 
We conduct experiments for {\modulename} with five state-of-the-art (SOTA) models: 
T2MGPT \cite{zhang2023generating}, CoMo \cite{huang2024como}, MoMask \cite{guo2023momask},
MotionDiffuse \cite{zhang2022motiondiffuse}, 
and MDM \cite{tevet2023human}, covering various genres of motion generation models as well as both continuous and discrete motion representations.

T2MGPT and CoMo are \textbf{autoregressive} motion generation models, which first tokenize the motion sequence using a quantization-based encoder-decoder structure. 
% After the motions are processed into a sequence of tokens, they employ an autoregressive generator that takes the injected text semantics as the initial token.
We use the autoregressive form of {\modulename}, as detailed in Section \ref{sub:autoregressive}, in place of the fixed-length text injection for their motion token generation.
% Among them, T2MGPT and CoMo are two \textbf{autoregressive} motion generation models. Here, we adopt the autoregressive form of {\modulename} detailed in section \ref{sub:autoregressive} to replace the original autoregressive generator that takes the fix-length [CLS] token as text injection.
% MotionDiffuse and MDM are \textbf{diffusion-based} motion generation models, which undertake conditional denoising at each step of the diffusion process on the entire motion sequence. 
% The core architecture under each diffusion scheme is either a transformer encoder or decoder, that maps the noised motion into the denoised one of the same shape. 
% We apply the composite aware text injection for both the encoder and decoder variants of MDM, while for MotionDiffuse, we use the encoder-based semantic injection. 
MotionDiffuse and MDM are \textbf{diffusion-based} motion generation models that conditionally denoise full motion sequences at each diffusion step with the fixed-length semantic embedding. 
We apply the composite aware text injection for both the encoder and decoder variants of MDM, while for MotionDiffuse, we use the encoder-based semantic injection. 
MoMask employs a hierarchical motion quantization scheme and a multi-layer masked motion generation framework.  
We apply the encoder-based semantic injection for their M- and R- Transformers in our study.

All the other settings follow the baseline methods and hyperparameters remain unchanged.




\begin{table}[t!]
\fontsize{7.25pt}{7.25pt}\selectfont
  \aboverulesep=0ex
  \belowrulesep=0.5ex 
\setlength{\tabcolsep}{3.0pt}
\centering
\caption{Additional quantiative results on HumanML3D with varying architectures and configuration settings.}
\vspace{-5pt}
\begin{tabular}{llcccccc}
\toprule
\multirow{2}{*}{Method} & \multirow{2}{*}{Arch} & \multirow{2}{*}{\modulename} & \multicolumn{3}{c}{R-Precision$\uparrow$} & \multirow{2}{*}{FID$\downarrow$} & \multirow{2}{*}{MM-Dist.$\downarrow$} \\
 \cmidrule{4-6}
 & & &  Top1 & Top2 & Top3 &  &   \\
\midrule 
% MDM (Paper) & Enc & & 0.455 & 0.645 & 0.749 & 0.489 & 3.330 \\
\textcolor{gray}{MDM (Paper)}  & \textcolor{gray}{Enc} & & \textcolor{gray}{0.418} & \textcolor{gray}{0.604} & \textcolor{gray}{0.707} & \textcolor{gray}{0.489} & \textcolor{gray}{3.630} \\
\textcolor{gray}{MDM (Paper)} & \textcolor{gray}{Dec} & & \textcolor{gray}{--} & \textcolor{gray}{--} & \textcolor{gray}{0.608} & \textcolor{gray}{0.767} & \textcolor{gray}{5.507} \\
MDM 50steps  & Enc & & 0.455 & 0.645 & 0.749 & 0.489 & 3.330 \\
MDM & Enc & & 0.471 & 0.661 & 0.760 & 0.325 & 3.249 \\
MDM 50steps & Enc & $\checkmark$ & 0.489 & 0.685 & 0.787 & 0.355 & 3.100 \\
MDM & Enc & $\checkmark$& 0.463 & 0.658 & 0.705 & 0.265 & 3.266 \\
MDM 50steps & Dec & $\checkmark$ & \textbf{0.509} & \textbf{0.698} & \textbf{0.793} & 0.230 & 3.035 \\
MDM & Dec & $\checkmark$ & 0.502 & 0.694 & \textbf{0.793} & \textbf{0.165} & \textbf{3.020} \\
\midrule
% T2MGPT (Paper) & AR & & 0.491 & 0.680 & 0.775 & 0.116 & 3.118 \\
T2MGPT $\tau=0$  & AR & & 0.417 & 0.589 & 0.685 & 0.140 & 3.730 \\
T2MGPT $\tau=0.5$  & AR & & 0.491 & 0.680 & 0.775 & 0.116 & 3.118 \\
T2MGPT $\tau=[0, 1]$  & AR & & 0.492 & 0.679 & 0.775 & 0.141 & 3.121 \\
% T2MGPT & AR & & 0.484 & 0.672 & 0.770 & 0.117 & 3.153  \\
T2MGPT $\tau=0$ & AR & $\checkmark$ & 0.465 & 0.651 & 0.746 & 0.117 & 3.308  \\
T2MGPT $\tau=0.5$ & AR & $\checkmark$ & \textbf{0.539} & \textbf{0.730} & \textbf{0.823} & \textbf{0.105} & \textbf{2.838}  \\
T2MGPT $\tau=[0,1]$ & AR & $\checkmark$ & 0.538 & 0.730 & 0.821 & \textbf{0.105} & 2.841  \\
\midrule
CoMo  & AR & & 0.502 & 0.692 & 0.790 & 0.262 & 3.032  \\
CoMo no keywords  & AR & & 0.487 & - & - & 0.263 & 3.044  \\
CoMo  & AR & $\checkmark$ & \textbf{0.545} & \textbf{0.741} & \textbf{0.835} & \textbf{0.200} & \textbf{2.747}  \\
CoMo no keywords  & AR & $\checkmark$ & 0.539 & 0.738 & 0.831 & 0.226 & 2.777  \\
\bottomrule
\label{tab:quant_res5}
\vspace{-10pt}
\end{tabular}
\end{table}



\begin{figure*}[!t]
    \centering
    \includegraphics[width=\textwidth]{figures/Fig3.pdf}
    \vspace{-15pt}
    \captionof{figure}{Qualitative comparison between two baselines, their {\modulename}-enhanced models, and ground truth (GT) on HumanML3D test prompts. 
    Action verbs and their modifiers are highlighted in red, with motion sequences shown in color gradients (light to dark) and root trajectories in black.
    {\modulename}-MDM and {\modulename}-T2MGPT generate the motions that better match the descriptions, showing stronger text-motion correspondence and better controllability.}
    \vspace{-5pt}
\label{fig:qual_res}
\end{figure*}

\vspace{-3pt}
\subsection{Quantitative Analysis} 
\label{sub:quantitative}
\vspace{-3pt}

\textbf{Comparison with SOTA Methods.}
For quantitative evaluation, we report the results for each metric averaged over 20 repeated iterations on both datasets. 
Tab.~\ref{tab:quant_res1} and \ref{tab:quant_res2} present quantitative comparison of {\modulename}-MDM and {\modulename}-T2MGPT with the SOTA text-to-motion generation methods on both datasets.
Our method outperforms most SOTA methods in terms of R-Precision and MM-Dist, showing the effectiveness and robustness of {\modulename} in learning the text-motion correspondence.
In terms of motion quality, both {\modulename}-MDM and {\modulename}-T2MGPT achieve comparable FID score with some leading methods, such as GraphMotion, FineMoGen, and T2MGPT, on both benchmarks. 
Notably, the positive results are achieved through our semantic injection mechanism alone, without additional heuristic knowledge or textual semantics from external source like GraphMotion, CoMo, and FineMoGen.


\textbf{Leveling up SOTA model performances.}
To demonstrate {\modulename}'s effectiveness across different architectures, we integrate it with five representative models and evaluate their performance improvements. 
As shown in Tab.~\ref{tab:quant_res3}, {\modulename} consistently enhances all baseline models on HumanML3D dataset, with particularly notable gains in text-motion alignment metrics (R-Precision and MM-Dist).

For diffusion-based models, {\modulename} brings substantial improvements. MDM sees a significant boost in both motion quality (FID: 0.325→0.165) and multimodality alignment (Top1 R-Precision: 0.471→0.502, MM-Dist: 3.249→3.020). 
Similar improvements are observed for MotionDiffuse, suggesting that {\modulename}'s semantic injection mechanism effectively addresses the limitations of fixed-length text injection in diffusion models.
Autoregressive models, despite their stronger baseline performance, also benefit from {\modulename}. 
T2MGPT and CoMo show remarkable improvements in text-motion matching, and seems to benefit more with the composite aware semantics. 
The performance in Top1 R-Precision increases by 5.5\% and 4.3\% respectively and their MM-Distance drops to the lowest among all methods, with 2.838 and 2.747 respectively.
Even MoMask, which already achieves strong FID scores, sees consistent improvements across all metrics.
The benefits of {\modulename} remain evident on the more challenging KIT-ML dataset (Tab.~\ref{tab:quant_res4}). 
{\modulename}-MDM achieves significant improvements in text-motion alignment (R-Precision Top1: 0.164→0.448 and Top3: 0.396→0.784) and motion quality (FID: 0.497→0.354). 
{\modulename}-T2MGPT also sees consistent performance gain across the metrics.
For {\modulename}-CoMo,
% \footnote{CoMo's extra fine-grained keyword not open-sourced for KIT-ML dataset}
the motion quality remains on par with the baseline while R-Precision and MM-Distance improves.

The quantitative results suggest that {\modulename} is effective across different genres of motion generation models and across different datasets.
The consistent improvement in text-motion matching demonstrates its capability in learning dynamic semantic relationships without compromising motion quality.


\textbf{Architecture and configuration analysis.}
We conduct extensive experiments to validate {\modulename}'s effectiveness across different architectural choices and configuration settings, as shown in Tab.~\ref{tab:quant_res5}.
For diffusion-based MDM, we explore both encoder and decoder-based implementations of {\modulename}. Both variants demonstrate substantial improvements over their baseline (Encoder: Top1 R-Precision 0.471→0.489, FID 0.325→0.265; Decoder: Top3 R-Precision 0.608→0.793, FID 0.767→0.165), showcasing {\modulename}'s adaptability to different architectural choices. Particularly noteworthy is that these improvements hold even with significantly reduced computation--using only 50 diffusion steps instead of 1000, both variants maintain strong performance gains while achieving 20× faster inference.

For autoregressive models like T2MGPT, we examine {\modulename}'s behavior under different teacher forcing settings. During training, random masking ($\tau=[0,1]$ or $\tau=0.5$) \footnote{$\tau$ variable here represent coefficient for teacher forcing, which follows original notation. This is different from diffusion step index used earlier.} helps bridge the gap between training and inference, where the predicted tokens may differ from the ground truth. 
While the best performance occurs when $\tau=0.5$, {\modulename} can further improve its text-motion alignment (R-Precision: 0.491→0.539, MM-Dist: 3.118→2.838).
{\modulename} demonstrates robust performance across these hyperparameter configurations,
with all $\tau=0$, $\tau=0.5$ and $\tau = [0,1]$ 
achieving significant improvements, showing its resilience to different autoregressive model configurations.

The CoMo experiments reveal another interesting aspect of {\modulename}'s capabilities. 
The original CoMo relies on 11 additional keywords per description, augmented through GPT-4 to provide detailed motion characteristics across body parts and styles. 
Remarkably, {\modulename}-CoMo without any keywords outperforms the keyword-augmented baseline (R-Precision: 0.539 vs 0.487, FID: 0.226 vs 0.263), demonstrating {\modulename}'s ability to extract rich motion semantics directly from the input text without requiring external semantic augmentation.

Across all experiments, {\modulename} shows consistent performance improvements regardless of model architecture (diffusion or autoregressive), configuration choices (encoder/decoder, teacher forcing rates), or additional inputs (with/without keywords). This robust adaptability suggests that {\modulename}'s semantic injection mechanism provides fundamental improvements in learning text-motion correspondence that generalize across different modeling approaches.


\begin{figure}[!t]
    \centering
    \includegraphics[width=\linewidth]{figures/Figure_wordcloud.pdf}
    \vspace{-20pt}
    \captionof{figure}{Analysis of attention patterns in {\modulename}. 
    Left: Word cloud showing top-5 attended words across all test prompts, highlighting focus on action verbs, motion modifiers, and spatial references. 
    Right: Word cloud for prompts containing `\textit{walk}', revealing attention to motion-specific contextual attributes.}
    \vspace{-10pt}
\label{fig:wordcloud}
\end{figure}

\vspace{-3pt}
\subsection{Qualitative Analysis}
\label{sub:qualitative}
\vspace{-1pt}




\begin{table*}[t!]
\fontsize{8.25pt}{8.25pt}\selectfont
  \aboverulesep=0ex
  \belowrulesep=0.5ex 
\setlength{\tabcolsep}{6.0pt}
\centering
\caption{Results on the HumanML3D dataset for long-term motion generation.}
 \vspace{-5pt}
\begin{tabular}{lcccccccc|cc}
\toprule
\multirow{2}{*}{Method} & \multirow{2}{*}{\makecell{Handshake\\(\#frames)}} & \multirow{2}{*}{\modulename} & \multicolumn{6}{c}{\textbf{Motion}} & \multicolumn{2}{c}{\textbf{Transition}} \\
\cmidrule{4-11}
 & & & Top1$\uparrow$ & Top2$\uparrow$ & Top3$\uparrow$ & FID$\downarrow$ & MM-Dist$\downarrow$ & Div.$\rightarrow$& FID$\downarrow$ & Div.$\rightarrow$ \\
\midrule
GT & & & 0.511 & 0.703 & 0.797 & 0.002 & 2.974 & 9.503 & 0.050 & 9.570 \\
 \cmidrule{1-11}
\multirow{6}{*}{DoubleTake} & \multirow{2}{*}{20} &  & 0.309 & 0.477 & 0.589 & 0.953 & 5.713 & 9.624 & \textbf{1.540} & 8.750 \\
 &  & $\checkmark$ & \textbf{0.358} & \textbf{0.526} & \textbf{0.627} & \textbf{0.463} & \textbf{5.508} & \textbf{9.668} & 2.052 & \textbf{8.775} \\
 \cmidrule{2-11}
 & \multirow{2}{*}{30} & & -- & -- & 0.600 & 1.030 & 5.600 & 9.530 & 2.220 & 8.640 \\
 &  & $\checkmark$  & \textbf{0.345} & \textbf{0.516} & \textbf{0.612} & \textbf{0.707} & \textbf{5.532} & \textbf{9.864} & \textbf{1.896} & \textbf{8.805} \\
 \cmidrule{2-11}
 & \multirow{2}{*}{40} & & -- & -- & 0.580 & 1.160 & 5.670 & 9.610 & 2.410 & 8.610 \\
 &  & $\checkmark$ & \textbf{0.330} & \textbf{0.497} & \textbf{0.594} & \textbf{0.959} & \textbf{5.584} & \textbf{9.897} & \textbf{1.905} & \textbf{8.723} \\
\bottomrule
\end{tabular}%
\label{tab:dt_res}
 \vspace{-5pt}
\end{table*}




\textbf{Qualitative Comparisons.}
Fig.~\ref{fig:qual_res} demonstrates {\modulename}'s ability to improve motion generation quality through representative examples from the HumanML3D test set. Compared to baseline models, both {\modulename}-MDM and {\modulename}-T2MGPT show superior ability in following complex action sequences and maintaining temporal order. 
Their generated motions closely align with ground truth (GT), particularly in capturing the nuaunced semantics for spatial relationships and action transitions.

Specifically, for the input text ``\textit{a man runs to the right then runs to the left then back to the middle}", both {\modulename}-MDM and {\modulename}-T2MGPT accurately capture directional changes and chronological order, while baseline models struggle with spatial positioning and temporal progression. 
For another text, ``\textit{a person is holding his arms straight out to the sides then lowers them, claps, and steps forward to sit in a chair}", our models precisely follow each action component in sequence, whereas baseline models either miss critical components or fail to maintain the correct order. 
Given the third prompt, ``\textit{a person walks forward then turns completely around and does a cartwheel}", {\modulename}-MDM and {\modulename}-T2MGPT successfully reproduce the complete action sequence including the cartwheel motion, despite its infrequent appearance in the dataset.

The qualitative results demonstrate {\modulename}'s effectiveness in enabling precise text-based motion control while maintaining generalization to less common actions. 
The improved text-motion alignment across various examples suggests that our dynamic semantic injection successfully addresses the limitations of fixed-length text representations in existing methods.




\textbf{Interpretability Analysis.}
With the impressive quantitative and qualitative performance, it is interesting to understand how {\modulename} processes textual information. 
We first analyze its attention patterns through word clouds. 
Fig.~\ref{fig:wordcloud} (left) visualizes the most attended words in HumanML3D test set descriptions. The left word cloud shows the top-five attended words across all text prompts, revealing {\modulename}'s focus on motion-critical elements: action verbs (e.g., ``hand", ``step"), motion modifiers (e.g., ``slowly", ``quickly"), and spatial references (e.g., ``forward", ``circle"). For text containing the word `\textit{walk}' (Fig.~\ref{fig:wordcloud} right), the attention focuses on contextual motion attributes like direction (``forward", ``backward") and style (``slowly"), demonstrating {\modulename}'s capability in capturing motion-specific semantic relationships.

We also visualize the attention weights to analyze how {\modulename} dynamically aligns text with motion frames (Appendix \ref{sup:vis_attn_weights}). 
The visualization reveals clear temporal correspondence between text tokens and motion progression, validating our design of composite aware semantic injection in learning complex and dynamic text-motion relationships.


\vspace{-3pt}
% \subsection{Long-term Motion Generation}
\subsection{Long-term Motion Generation}
\label{sub:long-term}
\vspace{-1pt}

% While {\modulename} primarily targets single motion generation, we explore its effectiveness in long-term motion generation using DoubleTake \cite{shafir2024human}, a framework that employs a two-stage diffusion process to generate smooth transitions between MDM-generated motions.
% As shown in Tab. \ref{tab:dt_res}, {\modulename} enhances motion quality, text-motion matching, and diversity across different handshake sizes in the generated motion clips, demonstrating its benefits beyond single motion generation.
% However, when evaluated on transition periods between motion clips, the improvements are less consistent and vary with handshake size.
% This mixed performance on transitions, while not directly addressed by {\modulename}'s design, opens interesting questions about how semantic injection methods influence diffusion-based transition generation in long-term motion synthesis.

While {\modulename} primarily targets single motion generation, we explore its effectiveness in long-term motion generation using DoubleTake \cite{shafir2024human}, a framework that employs an additional two-stage diffusion process to generate smooth transitions between the motion clips generated by diffusion models like MDM.

As shown in Tab.~\ref{tab:dt_res}, {\modulename} consistently improves the motion clips across metrics and handshake sizes overall. 
For handshake size of 20 frames, {\modulename} significantly reduces the motion FID from 0.953 to 0.463 while improving text alignment (Top1: 0.309→0.358) and maintaining motion diversity (9.624→9.668). 
Similar improvements are observed with longer handshake periods of 30 and 40 frames, though the gains in FID gradually decrease as the transition period extends.
Interestingly, the transition quality shows mixed results. With 20-frame handshake, the transition FID slightly increases (1.540→2.052), while longer handshakes see improvements (30 frames: 2.220→1.896; 40 frames: 2.410→1.905). 
This suggests that {\modulename}'s semantic injection, while effective for individual motion generation, interacts differently for compositing several motion clips during the motion blending and transition generation process.
The observations raise interesting questions about how semantic injection methods influence diffusion-based transition generation in long-term motion synthesis, particularly regarding the balance between local motion quality and smooth transitions. 


%%% 1 or 2 figures with qualitative samples and comparison for different methods, in-domain, spatial control, long text.



