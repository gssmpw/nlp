\section{Related Work}
\label{sec:Related Work}

\paragraph{Class Incremental Learning (CIL)}
Class-incremental learning is a method that continuously acquires knowledge in the order of classes, aiming to address catastrophic forgetting~\citep{CF_mccloskey1989catastrophic}. Existing work~\citep{CIL_survey_wang2024comprehensive} broadly categorizes these approaches into three main types.
Replay-based methods involve storing data or features of old classes or generating data that includes old classes. They can be further divided into Experience Replay~\citep{iCaRL_rebuffi2017icarl, RM_bang2021rainbow}, Generative Replay~\citep{CIL_G_replay_liu2020generative, CIL_G_replay_shin2017continual}, and Feature Replay~\citep{CIL_F_replay_belouadah2019il2m}.
Regularization-based methods focus on designing loss functions that incorporate second-order penalties based on the contribution of parameters to different tasks~\citep{ewc_kirkpatrick2017overcoming, CIL_loss_jung2020continual}. They also rely on knowledge distillation, using the model from the previous phase as a teacher to constrain current model~\citep{lwf_li2017learning, iCaRL_rebuffi2017icarl, PODnet_douillard2020podnet, DER_buzzega2020dark}.
Architecture-based methods dynamically adjust model parameters based on new data, including assigning specific parameters for different data~\citep{CIL_archi_gurbuz2022nispa, CIL_archi_serra2018overcoming} and breaking down model parameters into task-specific or shared parts~\citep{DyTox_douillard2022dytox}.

\paragraph{Class Incremental Semantic Segmentation (CISS)}
 CISS is similar to class incremental learning (CIL) but extends the task to pixel-level predictions~\citep{reminder_phan2022class,pcss_camuffo2023continual,rcil_zhang2022representation,ewf_xiao2023endpoints}. MiB~\citep{MiB_cermelli2020modeling} first introduces the concept of semantic shift unique to CISS, employing distillation strategies to mitigate this issue. PLOP~\citep{PLOP_douillard2021plop} utilizes pseudo-labeling techniques for incremental segmentation to address background shift, while SSUL~\citep{SSUL_cha2021ssul} further incorporates salient information, introducing the concept of ``unknown classes'' into each learning phase and using a memory pool to store old data to prevent catastrophic forgetting. RECALL~\citep{recall_maracani2021recall} and DiffusePast~\citep{diffusepast_chen2023diffusepast} extend traditional replay methods by incorporating synthetic samples of previous classes generated using Diffusion~\citep{diffusion_ho2020denoising} or GAN~\citep{GAN_goodfellow2020generative} models. MicroSeg~\citep{microseg_zhang2022mining} employs a proposal generator to simulate unseen classes. CoinSeg~\citep{coinseg_zhang2023coinseg} highlights differences within and between classes, designing a contrastive loss to adjust the feature distribution of classes. PFCSS~\citep{PFCSS_lin2023preparing} emphasizes the preemptive learning of future knowledge to enhance the model's discrimination ability between new and old classes.

\iffalse
\fy{This subject is not highly related to this paper, do not directly take in it here.}
\paragraph{Salient Object Detection}
Salient object detection is a classic computer vision task. In its early phases, mathematical methods or manually designed algorithms were used, leveraging easily perceivable features such as brightness and color~\citep{SOD_color_zhang2017salient} in images to perform saliency detection~\citep{SOD_1_perazzi2012saliency, SOD_2_scharfenberger2013statistical, SOD_3_jiang2013salient}. For example,~\citep{SOD_FT_achanta2009frequency} detects salient regions in images using frequency-tuned approaches and then combines them with spatial domain features for prediction. More recently, neural network-based methods~\citep{SOD_deep_1_wu2019cascaded, SOD_deep_2_liu2019simple, SOD_deep_3_liu2019simple, SOD_deep_4_wang2022curiosity} have come to the forefront, making full use of features extracted by deep networks for detection. For instance, ~\citep{SOD_deep_used_hou2017deeply} utilizes short connections to transfer contextual information across the network and employs a multi-layer supervision strategy to effectively guide the network in learning salient features. ~\citep{SOD_deep_5_hao2024simple} employs a network design based on Vision Transformers, incorporating a local information capture module to detect targets at various scales.
\fi