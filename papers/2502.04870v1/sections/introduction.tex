\section{Introduction}
\label{sec:Introduction}

\begin{figure*}[ht]
    \centering
    \includegraphics[width=0.9\linewidth]{figs/iclr_rebuttal_pdf_yuxiao/intro_icml.pdf}
    \vspace{-10pt}
    \caption{(a) Due to the existence of \textit{separate optimization}, the previous method SSUL-M misclassifies a ``horse'' as a ``cow'' with higher logit scores when learning ``horse'' following ``cow''. 
    While our IPSeg leverages image posterior (IP) guidance to produce accurate predictions on these two similar-look classes.
    The ``logit scores'' refer to pixel-wise prediction, and the image posterior refers to our introduced image-wise prediction.
    % In class-wise prediction visualizations, we use deeper colors to indicate higher prediction scores.
    The logit numbers are used for better illustration.
    (b) The quantitative performance comparison with state-of-the-art methods under the long-term incremental challenge (VOC 2-2).}
    \label{fig:vis_intro}
\end{figure*}

Deep learning methods have achieved significant success in vision~\citep{Cl_in_cv_qu2021recent} and language~\citep{CL_in_NLP_ke2022continual} tasks with fixed or stationary data distributions. However, real-world scenarios are characterized by dynamic and non-stationary data distributions, posing the challenge of \textit{catastrophic forgetting}~\citep{CF_mccloskey1989catastrophic, mcclelland1995there}. Incremental learning, a.k.a. continual learning or lifelong learning~\citep{lifelong_silver2013lifelong}, has been proposed to enable models to adapt to new data distributions without forgetting previous knowledge~\citep{lifelong2_kudithipudi2022biological}. Within this domain, Class Incremental Learning (CIL) methods~\citep{CIL_archi_serra2018overcoming, lwf_li2017learning, iCaRL_rebuffi2017icarl, OCIL_survey_mai2022online, wang2024hierarchical} have shown great potential in learning new classes from incoming data, particularly for classification tasks~\citep{CL_in_classification_de2021continual}.

% Inspired by current CIL solutions, prior research in CISS has focused on replay-based~\citep{iCaRL_rebuffi2017icarl,alife_oh2022alife} and architecture-based~\citep{rcil_zhang2022representation,idec_zhao2023inherit} methods. 
Class Incremental Semantic Segmentation (CISS) extends the principles of CIL to pixel-wise tasks.
In addition to catastrophic forgetting, CISS encounters an even more critical challenge: \textit{semantic drift}~\citep{CISS_survey_yuan2023survey} or \textit{background shift}~\citep{MiB_cermelli2020modeling}, which describes the incremental change in the semantic meaning of pixel labels. 
% to the gradual change in the semantics of pixel labels over time.
Several studies~\citep{PLOP_douillard2021plop, SSUL_cha2021ssul, microseg_zhang2022mining, coinseg_zhang2023coinseg} attribute \textit{semantic drift}
to the evolving semantic content of the background across incremental stages.
% to the evolving semantic content of the background across incremental phases.
Subsequent works~\citep{MiB_cermelli2020modeling, PLOP_douillard2021plop} early pioneer this investigation using knowledge distillation and pseudo-labeling. 
More recent works~\citep{SSUL_cha2021ssul, microseg_zhang2022mining, coinseg_zhang2023coinseg} further use saliency maps and segment proposals to differentiate between the foreground and background pixels. 
% However, these works predominantly target the separation of \textit{noisy semantics}, 
These works introduce naive and inappropriate pseudo-labeling, ignoring decoupling the learning of these semantics, leaving \textit{noisy semantics} remains a critical challenge to be solved.
% still leaving room for further optimization.
% still leaving room for further optimization.
% 接下来承接上一段具体描述两个问题 noisy semantics, separate optimization.

In this paper, we delve into \textit{semantic drift} challenge and identify an additional but more essential issue, \textit{separate optimization}.
% , as being of significant importance.
\textit{Separate optimization} refers to the learning manner within CISS methods that independently and sequentially update the task heads for each target class set and freeze previous task heads to prevent catastrophic forgetting.
This leads to a scenario in which different task heads trained in different stages always have misaligned probability scales, especially on similar-looking classes. It finally results in error classification and magnifying semantic drift.
% This leads to a scenario where task heads trained earlier may produce higher scores than those trained later for similar-looking categories.
~\cref{fig:vis_intro}(a) directly presents the impact of \textit{separate optimization}, where the SSUL-M model mistakenly classifies a horse as a ``cow'' class with a higher logit score when separately training the corresponding task heads.
% after learning ``horse'' following ``cow''.
Under the combined impacts of \textit{separate optimization} and \textit{noisy semantics}, the previous efforts are still short of effectively addressing the \textit{semantic drift} challenge.
% still far from overcoming \textit{semantic drift} challenge.




Motivated by our observations and analyses, we introduce \textbf{I}mage \textbf{P}osterior and Semantics Decoupling for Class-Incremental Semantic \textbf{Seg}mentation (IPSeg) to address the aforementioned challenges.
We propose \textbf{image posterior guidance} to mitigate \textit{separate optimization} by rectifying the misaligned pixel-wise predictions using image-wise predictions.
As illustrated in ~\cref{fig:vis_intro}(a), IPSeg correctly predicts ``horse'' with the assistance of image posterior guidance.
% This simple improvement significantly improves our baseline.
Furthermore, we propose \textbf{permanent-temporary semantics decoupling} to decouple \textit{noisy semantics} into two groups, one characterized by simple and stable semantics, and the other by complex and dynamic semantics. We also introduce separate learning strategies for better decoupling.
% To accommodate these distinct semantic groups, we design separate permanent and temporary branches with varied life cycles to learn the associated concepts.
% And we design permanent and temporary branches of different life cycles to separately learn the corresponding concepts. 


% In this way, IPSeg effectively mitigates the challenge of \textit{semantic drift}.

Extensive experimental results on two popular benchmarks, Pascal VOC 2012 and ADE20K, demonstrate the effectiveness and robustness of IPSeg. Our method consistently outperforms the state-of-the-art methods across various incremental scenarios, particularly in long-term challenges as gaining \(\textbf{24.8}\)\% improvement in VOC 2-2 task. 
% as in ~\cref{fig:vis_intro}(b). 
% Experiment results further reveal that IPSeg has good properties of both learning plasticity and memory stability.

\iffalse
% 这里要修改成直述当前方法的问题
MiB~\citep{MiB_cermelli2020modeling} and PLOP~\citep{PLOP_douillard2021plop} pioneer this investigation using knowledge distillation and pseudo-labeling techniques. % 这一句不好
SSUL~\citep{SSUL_cha2021ssul} further filters the unknown class from the background region with salient maps.
MicroSeg~\citep{microseg_zhang2022mining} and CoinSeg~\citep{coinseg_zhang2023coinseg} propose to use segment proposals to distinguish the foreground region from the background. 
\fi
% 有点太啰嗦了，而且没有突出现有研究在semantic drift的不足，看不出semantic drift问题的重要性

% 我们在abstract里claim的两个问题没有得到很好的体现与说明
% 两段并一段，直接陈述两个问题
\iffalse
These methods partly alleviate \textit{semantic drift} by merely finding new semantics from the background. However, the \textit{semantic drift} is left behind as there are still many \textit{noisy semantics} that need to be learned in the background, and these methods lack a unified optimization. For example, SSUL~\citep{SSUL_cha2021ssul} highlights the unknown class and allocates a parallel head to learn it, but ignores the impact of the unknown class on the training of the current task.
% 我们在abstract里claim的两个问题没有得到很好的体现与说明
% 两段并一段，直接陈述两个问题

% As demonstrated in Figure~\ref{fig:vis_intro}(a), in the incremental paradigm, when the model learns the concept of ``horse'' after already learning ``cow'',  it treats ``cow" as a previous class and can segment ``cow" effectively. Therefore, the model tends to misclassify ``horse'' as the familiar class ``cow'' with high scores. This \textbf{separate optimization} caused by \textit{semantic drift} makes the model predict higher scores on old classes and leads to subsequent classification errors or even failure. , the SSUL-M~\citep{SSUL_cha2021ssul} misclassifies ``horse'' as ``cow'' due to higher scores, while IPSeg achieves accurate results with image posterior guidance. 

Additionally, in the incremental paradigm, when the model learns the concept of ``cow'' before learning ``horse'',  it treats ``horse" as an unknown but similar class. Therefore, the current task head tends to learn ``cow'' with ground truth but misclassifies ``horse'' as the familiar class ``cow'' without appropriate optimization. This \textit{separate optimization} in \textit{semantic drift} causes the model to predict higher scores on already learned classes and leads to subsequent classification errors or even failure. As demonstrated in Figure~\ref{fig:vis_intro}(a), SSUL-M~\citep{SSUL_cha2021ssul} misclassifies ``horse'' as ``cow'' due to higher scores, while IPSeg achieves accurate results with image posterior guidance. 
\fi

% Together with \textbf{separate optimization}, CISS is more challenging. 
% When learning about the concept of ``cow'' after ``horse'',  the ``horse'' is treated as the unknown. In this case, the model tends to predict ``horse'' into ``cow'' with high scores due to no penalties. This \textbf{separate optimization} under \textit{semantic drift} makes the model predict the higher scores on current classes and leads to subsequent classification chaos or even failure. As shown in~\cref{fig:vis_intro}, the SSUL-M finally predicts ``horse'' into ``cow'' due to higher high scores.

% Optimization is also tricky when \textit{semantic drift} exists.

% However, these methods fail to effectively mitigate \textit{semantic drift} due to noisy concepts in unknown classes and their lack of effective modeling. 
% , inadvertently transferring the issue from the background to the unknown class due to insufficient decoupling and \textbf{noisy concepts}.

% As shown in~\cref{fig:vis_intro}, though SSUL-M can output high scores for the ground truth class ``horse'', the ``horse'' is incorrectly predicted as ``cow'' due to significantly higher scores of ``cow''. Analyzing dozens of similar cases, our research identifies that \textit{semantic drift} also arises from \textbf{separate optimization} in addition to \textbf{noisy concepts}. 
% CISS can not jointly optimize models of different tasks to achieve consistent output scales leading to the case in ~\cref{fig:vis_intro}.
% CISS can not jointly optimize models of different tasks to achieve consistent output intensity leading to the case in~\cref{fig:vis_intro}.
% Noisy concepts and separate optimization intertwine in incremental learning processes, making \textit{semantic drift} a tricky challenge. 



% branches with different learning life cycles, permanent and temporary learning branches, specifically learning the corresponding knowledge. 


% problem 
% Many previous works~\citep{PLOP_douillard2021plop,SSUL_cha2021ssul,microseg_zhang2022mining,coinseg_zhang2023coinseg} explore \textit{semantic drift} and mainly attribute it to the gradual change of the semantic content of the background across incremental phases. 
% % MiB~\citep{MiB_cermelli2020modeling} and PLOP~\citep{PLOP_douillard2021plop} are pioneers in this field and explore this problem with knowledge distillation and pseudo labels. SSUL~\citep{SSUL_cha2021ssul} further decouples the relative background class concept into the absolute background, past classes, and unknown classes.
% However, these prior methods did not really address \textit{semantic drift} but transferred it from the background to the unknown class concept by incompletely decoupling the noisy background into the pure background and the noisy unknown.
% The noisy unknown class in this incomplete concept decoupling makes the CISS difficult to optimize and still far from \textit{semantic drift}. 
% In this paper, we find that \textit{semantic drift} is not only related to the \textbf{noisy class concepts decoupling} but also caused by \textbf{separate optimization} in CISS. The models of different tasks can not be jointly optimized to have consistent output scales as fully supervised models and result in severe misclassification. These two aspects interweave in the incremental procedures and make \textit{semantic drift} hard to overcome.


% Extensive experimental analysis on two popular benchmarks Pascal VOC 2012 and ADE20K demonstrates the effectiveness and competitive performance of our IPSeg. IPSeg achieves consistent performance leadership under different incremental scenarios, especially in long-term challenges. Ablation and qualitative results further show that IPSeg achieves good learning plasticity and memory stability.


 % Further, we propose to mitigate \textit{semantic drift} via decoupling multiple class objects from a single background concept with our proposed unknown concept decoupling. It mines object knowledge from ``unknown'' objects of the current training stage and helps models produce appropriate output scores for them.


% % Based on our empirical analysis, we find \textbf{inconsistent output scale} of different task heads seriously deteriorates the problem \textit{semantic drift} due to separate training. \fy{As shown in fig., the average output logit of A is higher than B though the average score of B is considerably high to be viewed as a correct prediction. This phenomenon happens with a higher probability between class objects of similar appearance due to close visual patterns.}
% \fy{We need to present a visualization fig of our mentioned problem.} 

% our method 
% To tackle the above-mentioned problem, we start with theoretical and empirical analysis, and answer the cause of the above-mentioned challenge. Importantly, we find that image-level posterior is a good compensation factor to inconsistent scales from Bayesian theory. 

\iffalse
\begin{figure}[t]
\centering
% \begin{warpfigure}{r}{4.5pt}
% \includegraphics[width=1.0\linewidth]{figs/fig0_s.jpg}  
\includegraphics[width=1.0\linewidth]{figs/fig0.jpg}
\caption{(a) The visualization of \textit{semantic drift} problem, the green color represents ``cow'' and red for the ground-truth class ``horse''. SSUL outputs high scores for both ``cow'' and ``horse'', but fails to predict it as ``horse''. While IPSeg correctly recognizes it with reasonable output scores for two classes. (b) Performance comparison under long-term (VOC 2-2) incremental challenge.}
\label{fig:vis_intro}
% \end{warpfigure}
\end{figure}
\fi





% Motivated by our analysis, we propose our method \textbf{I}mage \textbf{P}osterior for class-incremental semantic \textbf{Seg}mentation to address the above-mentioned challenge. Specifically, we propose to establish image-level classification modeling as a posterior across tasks in the class incremental manner. In the inference, the image-level class-wise prediction logit plays a role in adjusting the pixel-level class-wise prediction logit to suitable ranges. Further, we propose to mitigate \textit{semantic drift} via decoupling multiple class objects from a single background concept with our proposed unknown concept decoupling. It mines object knowledge from ``unknown'' objects of the current training stage and helps models produce appropriate output scores for them.

% Extensive experiment analysis on two popular benchmarks Pascal VOC 2012 and ADE20K demonstrates the effectiveness and competitive performance of our IPSeg. \fy{further describe xxx}


% 结构：
% 1. Background - class incremental learning/class incremental semantic segmentation
% 2. problem - semantic drift
% 3. our paper/method -
% 4. details of our method -
% 5. summary - 
% 要画一个图（展示语义漂移问题，构思：以概率柱状图形式展现）

    
