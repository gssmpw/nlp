\section{Method}
\label{sec:Method}

% 现在的逻辑太差了，太散了，读下来割裂感很强。
% 1. notations and problem definition -- 给出问题定义，引出semantic overlap （考虑是否应当从method section中拆分）
% 2.method overview -- 承上起下，先直接给出我们对1中问题的解决方法，然后简单描述方法pipeline，统领后续两段的具体方法表述 (通过紧凑的语言给出的我们的solution)
% 3.Method Part 1 -- 
% 4.Method Part 2 -- 

% Methods 5.21
% 1.Preliminary 
% 2.Methods 
% 

% 首先是问题的形式化定义
% In this section, we first present the necessary notation and question definition and our analysis of the problem \textit{semantic drift} in ~\cref{sec3-1:preliminary}. Then, we introduce our proposed IPSeg with details in ~\cref{sec3-3} and ~\cref{sec3-4}. 
In this section, we begin by presenting the necessary notation and definition of the problem, followed by our analysis of \textit{semantic drift} in Section~\ref{sec3-1:preliminary}. Next, we introduce our proposed method, IPSeg, with detailed designs including image posterior and semantics decoupling in Section~\ref{sec3-3} and Section~\ref{sec3-4}.


% In this section, we first present an intuitive analysis of class incremental learning and give the definition of the semantic overlap phenomenon. Then, we present our solution in detail and claim its effectiveness in this challenge.

\subsection{Preliminary}
\label{sec3-1:preliminary}

% 1. 给出统一的符号/概念定义 
% 2. 给出具体的问题描述 
% 重点 -- 为什么现有范式有问题？为什么Image Posterior对这个问题有效？

\paragraph{Notation and problem formulation} Following previous works~\citep{SSUL_cha2021ssul,microseg_zhang2022mining,coinseg_zhang2023coinseg}, 
in CISS, a model needs to learn the target classes $\mathcal{C}_{1:T}$ from a series of incremental tasks as $t=1,2,3,...,T$. For task $t$, the model learns from a unique training dataset $\mathcal{D}_t$ which consists of training data and ground truth pairs $\mathcal{D}_t = \{(x_{i}^t, y_{i}^t)\}_{i=1}^{\left|\mathcal{D}_t\right|}$. Here $i$ denotes the sample index, $t$ for the task index, and $\left|\mathcal{D}_t\right|$ for the training dataset scale. 
$x_{i,j}^t$ and $y_{i, j}^t$ denote the $j$-th pixels and the annotation in the image $x_i^t$.
In each incremental phase $t$, the model can only access the class set $\mathcal{C}_t \cup c_b$ where $\mathcal{C}_t$ denotes the class set of current task $t$ and $c_b$ for background class. 
% After completing the $t$-th $(t>1)$ tasks, the model $f_t$ is expected to be able to predict pixels with ever-seen label set, $\mathcal{C}_{1:t} = c_b \cup \mathcal{C}_{1:t-1} \cup \mathcal{C}_{t}$.
% Specifically, we denote $x_{i}^t$ as an image belonging to task $t$ while $x$ represents an image without any task assignments. 

To prevent catastrophic forgetting, architecture-based methods allocate and optimize distinct sets of parameters for each class, instead of directly updating the whole model $f_t$. Typically, $f_t$ is composed of a frozen backbone $h_\theta$ and a series of learnable task heads $\phi_{1:t}$, with one task head corresponding to a specific task. 
% The model only needs to optimize the parameters of the newly added task head $\phi_t$ in task \(t\). 
In task $t$, only the new task head $\phi_t$ is set to be optimized.
In inference, the prediction for the $j$-th pixel in image $x_i$ can be obtained by:
\begin{equation}
    \vspace{-10pt}
    \hat{y}_{i,j} = f_{t}(x_{i,j}) = \mathop{\arg\max}\limits_{c\in\mathcal{C}_{1:T}} \phi_{1:T}^c(h_\theta(x_{i,j})).
    % \vspace{-10pt}
\end{equation}

Where \(\phi_{1:T}^c(\cdot)\) denotes the $C$-dimension outputs. 
% In addition to the notations mentioned above, 
Additionally, we introduce the image-level labels $\mathcal{Y}_i$ of the image $x_i$, a memory buffer $\mathcal{M}$, and an extra image classification head $\psi$ in our implementation. A comprehensive list and explanation of symbols can be found in the appendix.
% image-level labels, 
% image classification heads \psi


% \textbf{Semantic Drift} 
% is a problem that refers to the gradual change of the semantic content of the background class as the learning foreground classes change, according to the definition in previous works~\citep{MiB_cermelli2020modeling, PLOP_douillard2021plop}. So it is also treated as the same concept as \textit{background drift}.
% Previous studies ~\citep{SSUL_cha2021ssul, coinseg_zhang2023coinseg} have attempted to mitigate \textit{semantic drift} by decoupling the background class \( c_b \) into several subclasses. However, it works with the limited effect but introduces more tricky noisy dummy labels $c_u$. It still leaves the problem of noisy knowledge coupling. Moreover, the inconsistent output scale caused by unaligned training and testing targets exacerbates \textit{semantic drift}. During training, the task heads \( \cup_{c \in \mathcal{C}_t}{\phi_t^c} \) can only access supervision from the current classes \( c_b \cup \mathcal{C}_t \), whereas the testing target requires the integrated incremental model \( \cup_{c \in \mathcal{C}_{1:T}}{\phi_t^c} \) to output predictions for the full class domain \( \mathcal{C}_{1:T} \). In incremental learning scenarios, it is impractical to jointly optimize models of different tasks \( \phi_{t_1} \) and \( \phi_{t_2} \) (\forall \( t_1 \neq t_2 \)) concurrently under unified supervision without compromising memory stability. Therefore, ensuring that  models from different phases \( \phi_{t_1} \) and \( \phi_{t_2} \) (\forall \( t_1 \neq t_2 \)) produce consistent output scales is challenging.

% Upon rethinking the problem of \textit{semantic drift}, we attribute it to two aspects \textbf{separate optimization} and \textbf{noisy concept}. \textbf{Separate optimization} refers to the absence of unified supervision to update all task heads and results in inconsistent output scales across different task heads. \textbf{Noisy concept} refers to the noisy and complex semantic contents of classes \( c_b \) and \( c_u \), which is also related to the lack of effective modeling to fulfill their potential.

% \iffalse

% Separate optimization refers to the absence of unified supervision for updating all task heads, leading to inconsistent output scales across different tasks. And noisy concepts refer to the noisy and complex semantic contents within classes \( c_b \). We propose to address these two problems in IPSeg.

% Different from commonly seen jointly training all classes, target classes are separately optimized and learned in different phases in class incremental learning. Without unified learning, the incremental learning models always 

% Moreover, the inconsistency in output scales across different task heads further exacerbates \textit{semantic drift}. During training, the task head \( \phi_t \) receives supervision exclusively from the current classes and is frozen to solve catastrophic forgetting as the incremental process continues. Consequently, \( \phi_t \) will classify unknown objectives as familiar classes simply because they are similar.
% 这里的表意也有问题，需要重写



% , making the model learning and optimization hard.
% Despite these efforts, \textit{semantic drift} remains unresolved as the semantic content of \( c_b \) is overly complex and noisy. Extracting partial concepts from the background region and designing only a single architecture for learning are far from sufficient. 
% lacking the corresponding learning and optimization, making the models failed to effectively learn from chaos and inaccurate labels.
% 这里也应该更具体的阐述这一方式的不足，比如同时学习模糊的类别导致的优化困难or...


\iffalse
Previous works~\citep{ewc_kirkpatrick2017overcoming} attempt to mitigate \textit{semantic drift} by decoupling the background class \( c_b \) into several subclasses. 
Specifically, the background class \( c_b \) is splited into \( c'_b \) and \( c_u \), representing the ``pure'' background and unknown classes respectively.
% Specifically, the background class \( c_b \) is splited into \( c'_b \), representing the ``pure'' background, and \( c_u \), representing unknown classes that complement to the current visible classes \( c_b' \cup \mathcal{C}_t \). 
The most advanced methods~\citep{microseg_zhang2022mining,SSUL_cha2021ssul} further subdivide the unknown classes \( c_u \) into past classes \( \mathcal{C}_{1:t-1} \) and dummy unknown classes \( c'_u \) using pseudo-label techniques.

Despite these efforts, \textit{semantic drift} remains unresolved as the semantic content of \( c_b \) is overly complex and noisy. Extracting partial concepts from the background region and designing only a single architecture for learning are far from sufficient. 
\fi

% Additionally, previous solutions~\citep{PLOP_douillard2021plop} that assign pseudo-label to unknown classes \( c_u \) show limited effectiveness in enhancing learning plasticity and memory stability. Other approaches based on unknown classes \( c_u \), such as further decoupling and weight transfer~\citep{SSUL_cha2021ssul}, also have limited improvement in preventing \textit{semantic drift}.


% Moreover, the inconsistent output scale among different task heads exacerbates \textit{semantic drift}. During training, the task head \( \phi_t \) only receives supervision from the current classes, whereas the testing target requires the incremental model \( f_t=\{h_\theta, \phi_{1:t}\} \) to output predictions for the full class domain \( \mathcal{C}_{1:T} \). In incremental learning scenarios, it is impractical to jointly optimize task heads \( \phi_{t_1} \) and \( \phi_{t_2} \) $(\forall t_1 \neq t_2 )$ for different tasks under unified supervision. Consequently, producing consistent output scales for different models \( f_{t_1} \) and \( f_{t_2} \) proves to be highly challenging.


% \fi
% A significant reason for \textit{semantic drift} is the absence of unified supervision to update all task heads, resulting in inconsistent output scales across different task heads. Additionally, the noisy and complex semantic contents of \( c_b \) and \( c_u \) lack effective modeling to fulfill their roles.

% It is common practice to train a task head \( \phi_t^{c_u} \) separately with weight transfer to improve the training stability of CISS, but this does not effectively prevent \textit{semantic drift}.

% Previous works~\citep{SSUL_cha2021ssul,coinseg_zhang2023coinseg} try to alleviate \textit{semantic drift} by decoupling the single background class $c_b$ into several classes. In detail, the background class $c_b$ is decoupled into multiple classes $c'_b$ and $c_u$ where $c'_b$ stands for the ``pure'' background and $c_u$ for unknown classes complementary to the currently visible classes $c_b \cup \mathcal{C}_t$. Meanwhile, the current best method further divides unknown classes $c_u$ into the past classes $\mathcal{C}_{1:t-1}$ and dummy unknown classes $c'_u$. 

% Though many attempts at this orientation, we find \textit{semantic drift} have not been solved well as shown in ~\cref{fig:vis_intro}. Based on our empirical conclusion, there remain several challenges. The most notable one is that the semantic content of $c_u$ is too heavy and noisy to learn. In other words, \textit{semantic drift} is transferred from $c_b$ to $c_u$. 
% Besides this, previous solutions assign pixels with unknown classes $c_u$, but with limited effect on learning plasticity and memory stability. It is typical to separately train a task head $\phi_t^{c_u}$ in each with weight transfer to improve the training stability of CISS but can not help prevent \textit{semantic drift}. 

% Besides, inconsistent output scale caused by unaligned training and testing targets also deteriorates the \textit{semantic drift}. The training target constrains the current task heads $\cup_{c\in\mathcal{C}_t}{\phi_t^c}$ output correct predictions on the current classes $c_b \cup \mathcal{C}_t$. While the testing target constrains the whole model $\cup_{c\in\mathcal{C}_{1:T}}{\phi_t^c}$ output correct predictions on the full class domain $\mathcal{C}_{1:T}$. In incremental learning scenarios, it is not feasible to jointly optimize different $\phi_{t_1}$ and $\phi_{t_2}, t_1 \neq t_2$ at the same current by the unified supervision without changing their memory stability. Thus, it is hard to ensure that different models $\phi_{t_1}$ and $\phi_{t_2}, t_1 \neq t_2$ have consistent output scale.

% Rethinking the problem of \textit{semantic drift}, we summarize it as a form of \textit{catastrophic forgetting}. An important reason for \textit{semantic drift} is the lack of unified supervision to update all task heads which leads to inconsistent output scales of different task heads. On the other hand, the noisy and heavy semantic contents of $c_b$ and $c_u$ lack effective modeling to play their roles.

% \footnote{The side effect of it is also discussed in our Appendix.}


% To further mitigate semantic drift, the training supervision setting in CISS needs to be re-considered. In the incremental task $t$, the current training supervision classes are $\mathcal{C}_{t} \cup c_b$, where $c_b$ only denotes the pure background class. Previous works~\citep{} decouples the single background class $c_b$ into multiple classes $c_b, c_u$ where $c_u$ denotes the unseen classes compared to the $\mathcal{C}_{t} \cup c_b$. And ~\citep{} further adds the previous classes $\mathcal{C}_{1:t-1}$ into the current training supervision by pseudo labels from $\cup_{c\in\mathcal{C}_{1:t-1}}{\psi_c}$. Thus, the heavy semantic class $c_b$ is decomposed into multiple semantic expressions, the classes from the past and the classes not ever seen. In this pixel label assignment mechanism, the training supervision of task $t$ is $\{c_b \cup \mathcal{C}_{1:t}\cup \mathcal{C}_{1:t}\}$. 

\paragraph{Semantic Drift}  
Previous work~\citep{ewc_kirkpatrick2017overcoming} mainly attributes the \textit{semantic drift} to \textit{noisy semantics} within the background class $c_b$. They attempt to mitigate this challenge by decoupling the class \( c_b \) into subclasses $c'_b$ and $c_u$, where $c'_b$ denotes the pure background and $c_u$ denotes the unknown class. The most advanced methods~\citep{microseg_zhang2022mining,SSUL_cha2021ssul} further decouple the unknown classes \( c_u \) into past seen classes \( \mathcal{C}_{1:t-1} \) and dummy unknown class \( c'_u \) using pseudo labeling. 
However, \textit{semantic drift} remains unresolved as the decoupled classes are still evolving across incremental phases while the coupled training strategy is not able to cope with noisy pseudo labels.
% the decoupled classes are still changing across incremental phases and models are always hard to learn these chaotic classes.

Additionally, another essential challenge, \textit{separate optimization} inherent within incremental learning also contributes to \textit{semantic drift} but attracts little attention. Recent work~\citep{eclipse_kim2024eclipse} finds a similar phenomenon that freezing parameters from the old stage can preserve the model's prior knowledge but introduces error propagation and confusion between similar classes. 
In architecture-based methods, the task head \( \phi_t \) is exclusively trained by supervision from the current classes and will be frozen to resist catastrophic forgetting in the following incremental phases. In the following task $t_1, t_1 > t$, \( \phi_t \) may predict high scores on objects from other appearance-similar classes, without any penalty and optimization. 
In this incremental learning manner, task heads trained in different stages always have misaligned probability scales, and generate error predictions, especially on similar classes.
% Meanwhile, the new task head $\phi_{t_1}$ just predicts moderate scores which might be slightly lower than error predictions from \( \phi_t \).
% In this way, it is common that earlier incremental task heads may have larger output scales than the later heads, especially in similar classes.
This \textit{separate optimization} manner ultimately causes the incremental models to misclassify some categories and makes \textit{semantic drift} more difficult to thoroughly address. In the appendix, some cases can be found to help understand this challenge.
% And newer task heads have to predict higher scores. The output scales are inconsistent across different task heads and predictions are always wrong in similar classes, making \textit{semantic drift} harder to tackle.

% Upon rethinking the two factors of \textit{semantic drift} problem, we propose our method, IPSeg.

\begin{figure*}[t]
    \centering
    \includegraphics[width=0.9\textwidth]{figs/iclr_rebuttal_pdf_yuxiao/pipeline.pdf}
    \caption{Overall architecture of our proposed IPSeg, mainly composed of image posterior and permanent-temporary semantics decoupling two parts. In the latter part, $\phi_p$ denotes the permanent learning branch and $\phi_1, \phi_2, ..., \phi_t$ for temporary ones. The black solid lines are used to indicate the data flow in training and the green ones are for inference.}
    \label{fig:overview}
\end{figure*}


\subsection{Overview}
\label{sec3-2:overview}

As illustrated in Figure~\ref{fig:overview}, we propose \textbf{I}mage \textbf{P}osterior and Semantics Decoupling for Class-incremental Semantic \textbf{Seg}mentation (IPSeg) to mitigate \textit{semantic drift} through two main strategies: image posterior guidance and permanent-temporary semantics decoupling. In Section~\ref{sec3-3}, we describe how the IPSeg model uses image posterior guidance to mitigate \textit{separate optimization}. 
To address \textit{noisy semantics}, IPSeg employs branches with different learning cycles to decouple the learning of noisy semantics. Detailed explanations of this approach are provided in Section~\ref{sec3-4}.
% 表达啰嗦不干练

% Based on our analysis in ~\cref{sec3-1:preliminary}, we propose our method, \textbf{I}mage \textbf{P}osterior for Class-incremental Semantic \textbf{Seg}mentation (IPSeg) to solve \textit{semantic drift} from two aspects, image posterior guidance and permanent and temporary knowledge decoupling. 
% In ~\cref{sec3-3}, IPSeg models image posterior to compensate for the insufficient \textbf{separate training} by the design of building an extra class incremental classifier. To mitigate the side effects of heavy and noisy concepts $c_b$ and $c_u$,  IPSeg adopts two branches to decouple permanent and temporary knowledge and mitigate the noisy effect with details in ~\cref{sec3-4}.
% IPSeg solves pixel semantic drift from two unique designs, image posterior and unknown object mining. The image posterior method directly adjusts pixel-level class-wise predictions from different heads.
% The unknown object mining method is designed to dig knowledge from the current unknown class objects by concept decoupling. We will introduce them in the following subsections in detail.
% \fy{Or we write this overview by overall introducing our method.}



\subsection{Image Posterior Guidance}
\label{sec3-3}

% As previously discussed, the most straightforward solution to solve separate optimization is to rescale the inconsistent outputs using global statistical values. 
% 这里的表达不太好，前文没有做铺垫
As previously discussed, the \textit{separate optimization} leads to misaligned probability scales across different incremental task heads and error predictions. 
We propose leveraging the image-level posterior as the global guidance to correct the probability distributions of different task heads. The rationale for using the image posterior probabilities is based on the following fact:

\textbf{Fact}: \textit{For any image, if its image-level class domain is \(\mathcal{C}_I\) and its pixel-level class domain is \(\mathcal{C}_P\), the class domains \(\mathcal{C}_I\) and \(\mathcal{C}_P\) are the same, i.e., \(\mathcal{C}_I = \mathcal{C}_P\).}

Inspired by this fact, we propose to use an extra image posterior branch $\psi$ to predict image classification labels and train it in an incremental learning manner. As illustrated in Figure~\ref{fig:overview}, $\psi$ is composed of Pooling, Fully connected (FC) layers, and Multi-Layer Perceptrons (with one MLP per step) with the input dimension of 4096 and the output dimension of \(\left|\mathcal{C}_{1:T}\right|\), where the FC layers serve as shared intermediate feature processors, and the MLPs serve as incremental classification heads for incremental classes. 
% During inference, this branch predicts the image-level posterior probabilities on the target class set \(\mathcal{C}_{1:T}\). 

% This image posterior prediction branch is implemented using simple MLPs with the input dimension of 4096 and the output dimension of \(\left|\mathcal{C}_{1:t}\right|\).


% Data sample \( x_i^m \) from \(\mathcal{M}\) is associated with its corresponding image-level ground truth \(\mathcal{Y}_i^m\), where the classes in \(\mathcal{Y}_i^m\) depend on specific tasks. For example, \(\mathcal{Y}_i^m \subset \mathcal{C}_{t_1}\) if \((x_i^m, y_i^m) \in \mathcal{D}_{t_1}\). Similarly, data sample \( x_i^t \) from \(\mathcal{D}_t\) has image-level labels \(\mathcal{Y}_i^t\) from the current task class domain \(\mathcal{C}_{t}\).
% During incremental training for task \( t \),

In task \( t \) (\( t > 1 \)), the model can only access data \( x^{m}_i \) from the memory buffer \(\mathcal{M}\) and \( x^{t}_i \) from the current training dataset \(\mathcal{D}_t\). Previous works~\cite{SSUL_cha2021ssul, coinseg_zhang2023coinseg} put \( x^{m}_i \) into the training phase to revisit and reinforce prior knowledge of segmentation by simply rehearsal. IPSeg further takes advantage of the rich class distribution knowledge in \( x^{m}_i \) to train and enhance the image posterior branch.
% However, we observe that \( x^{m}_i \) contains rich category information, which can be leveraged not only to guide pixel-wise predictions but also to effectively train image-level tasks.
% 这个符号可能会引起歧义
% 这个表达也很差
% are associated with their corresponding image-level ground truth \(\tilde{\mathcal{Y}}^{t}_i\) and

In IPSeg, the mixed data samples \( x^{m,t}_i \) from  \(\mathcal{M}\) and \(\mathcal{D}_t\) are processed by the network backbone \( h_{\theta} \) into the image feature \( h_{\theta}(x^{m,t}_i) \), and further processed by image posterior branch \(\psi\) into the image classification prediction \(\hat{\mathcal{Y}}_i^{m,t}\). The objective function for training $\psi$ is: 
\vspace{-5pt}
\begin{equation}
    % \vspace{-10pt}
    \begin{aligned}
        \mathcal{L}_{\text{\tiny IP}} &= \mathcal{L}_{\text{\tiny BCE}}(\hat{\mathcal{Y}}^{m,t}_i, \tilde{\mathcal{Y}}^{m,t}_i) = \mathcal{L}_{\text{\tiny BCE}}(\psi(h_\theta(x^{m,t}_i)), \tilde{\mathcal{Y}}^{m,t}_i), \\
        \tilde{\mathcal{Y}}^{m,t}_i &= \mathcal{Y}^{m,t}_i \cup \tilde{\mathcal{Y}}_{\phi_{1:t-1}(h_\theta(x^{m,t}_i))}.
    \end{aligned}
    % \vspace{-10pt}
\end{equation}
Where image classification label \(\tilde{\mathcal{Y}}^{m,t}_i\) consists of two parts, the ground truth label \(\mathcal{Y}^{m,t}_i\) of the data \( x^{m,t}_i \) and pseudo label \(\tilde{\mathcal{Y}}_{\phi_{1:t-1}(h_\theta(x^{m,t}_i))}\) on past seen classes $\mathcal{C}_{1:t-1}$. 
Instead of relying solely on the label \(\mathcal{Y}^{m,t}_i\), we use the image-level pseudo labels from previous task heads prediction to enhance the model's discriminative ability on prior classes.

% 后面这一项太复杂了，增加了阅读难度
% The entire \(\tilde{\mathcal{Y}}^{t}_i\) is processed into pseudo-label to train image posterior branch. 
% Instead of relying solely on the label \(\mathcal{Y}^{t}_i\), we use the pseudo-label \(\tilde{\mathcal{Y}}^{t}_i\) to provide more comprehensive and diverse supervision. 
% ..., we also use the image-level pseudo labels from the previous task heads prediction, to provide informative signals of previous incremental tasks.

During inference, the image posterior branch predicts posterior probabilities on all classes \(\mathcal{C}_{1:T}\). For a testing image \( x_i \), the final pixel-wise scores are computed by element-wise multiplication between the image posterior probabilities from \(\psi\) and the pixel-wise probabilities from \(\phi_{0:T}\): 
% \begin{equation}
%     p_{i} = 
%     \underbrace{\texttt{Concat}(~~\alpha_{\text{\tiny BC}},~\sigma(~\psi(h_\theta(x_i))~)~~) }_{\text{Image Posterior Probability}}
%     ~~\cdot  ~~
%     \sigma (~\underbrace{\phi_{\yx{0}:T}(h_{\theta}(x_i))}_{\text{Pixel-wise Probability}}~).
%     \label{equ_3}
% \end{equation}
\begin{equation}
    p_{i} = 
    \underbrace{\texttt{Concat}(\alpha_{\text{\tiny BC}},\sigma(~\psi(h_\theta(x_i)))) }_{\text{Image Posterior Probability}}
    \cdot  
    \sigma (\underbrace{\phi_{0:T}(h_{\theta}(x_i))}_{\text{Pixel-wise Probability}}).
    \label{equ_3}
\end{equation}

Where \(\sigma(\cdot)\) denotes the Sigmoid function.
The hyperparameter \(\alpha_{\text{\tiny BC}}\) is used to compensate for the lack of background posterior probability, with the default value \(\alpha_{\text{\tiny BC}}=0.9\). 
The result \( p_i \) is the rectified pixel-wise prediction with a shape of \([C, HW]\), and \( p^c_{i,j} \) is prediction of the \( j \)-th pixel on class $c$. The prediction of the $j$-th pixel can be written as:
\begin{equation}
    \hat{y}_{i,j} = \mathop{\arg\max}\limits_{c\in\mathcal{C}_{1:t}} p_{i,j}^c.
    \label{equ_4}
\end{equation}

% The prediction \(\hat{y}_{i,j}\) can be written as:
% \begin{equation}
%     \hat{y}_{i,j} = \mathop{\arg\max}\limits_{c\in\mathcal{C}_{1:t}} p_{i,j}^c.
% \end{equation}
\subsection{Permanent-Temporary Semantics Decoupling}
\label{sec3-4}

To further address \textit{semantic drift} caused by the coupled learning of complex and noisy pseudo labels \( c_b \) and \( c_u \) along with incomplete yet accurate label \(\mathcal{C}_t\), we propose a decoupling strategy that segregates the learning process for different semantics. Here is our empirical observation:

\textbf{Observation}: \textit{Given an image in incremental task t, the semantic contents of it can be divided into four parts: past classes \(\mathcal{C}_{1:t-1}\), target classes \(\mathcal{C}_{t}\), unknown foreground \(c'_u\) and pure background \(c'_b\).}
% ..., its content can be divided into four semantics: ...

Based on this observation, we first introduce dummy label \(c_f=\mathcal{C}_{1:t-1} \cup c'_u\) to represent the foreground regions that encompass both past seen classes and unknown classes, which are not the primary targets in the current task. Subsequently, we decouple the regions of a training image into two sets: \( \mathcal{C}_t  \cup  c_f\) and \( c'_b \cup c'_u \). The former set  \( \mathcal{C}_t  \cup  c_f\) are current target classes and other foreground objects, which are temporary concepts belonging to specific incremental steps, and change drastically as the incremental steps progress. In contrast, \( c'_b \cup c'_u \) are pseudo labels representing pure background and unknown objects, which are permanent concepts, exist across the whole incremental steps and maintain stable (\( c'_b \) remains fixed, \( c'_u \) shrinks but does not disappear). 
% For instance, ``cat'' and ``horse'' are target classes in the current task but change into the region of past seen foreground classes in subsequent tasks, while the concepts of background and unknown objects remain consistent. For instance, given all target classes \{\(c'_b, \mathcal{C}_1, \mathcal{C}_2, \mathcal{C}_3, ..., \mathcal{C}_T\)\}, 

The learning of these two sets is also decoupled. The current task head \(\phi_t\) serves as the temporary branch to learn the semantics \( \mathcal{C}_t \) $\cup$ \( c_f \) existing in the current incremental phase. Besides, we introduce a permanent branch \(\phi_p\) to learn the permanent dummy semantics \( c'_b \) and \( c'_u \). \(\phi_p\) has the same network architecture as \(\phi_{t}\). They are composed of three 3x3 convolution layers and several upsampling layers. It's worth noting that $\phi_p$ and $\phi_t$  have different learning cycles as illustrated in Figure~\ref{fig:overview}. The permanent branch $\phi_p$ is trained and optimized across all incremental phases to distinguish unknown objects and the background. While temporary branch $\phi_t$ ($t=1,2,...,T$) is temporarily trained in the corresponding task phase $t$ to recognize target classes $\mathcal{C}_t$.
Following our decoupling strategy, we can reassign the labels of image \( x_i \) as:
% .... 
% Compared to the popular taxonomy in previous works, which is based on foreground classes, we group all classes into two groups: the static group and the dynamic group. In different incremental phases, the fine-grained class concepts of known foreground change—for example, the target class set may be ``cat, horse'' in the current task and changes into ``cow, sofa'' in the next. However, the concepts of background and unknown foreground remain constant.
% % However, the distinction between background and foreground remains constant. 
% % This static group includes the permanent concepts across different incremental phases that is the background and foreground.

% Based on this taxonomy, we decouple the regions of a given image into two groups: \( c'_b \cup c_f \) and \( \mathcal{C}_t  \cup  c'_{f}\). Here, \( c'_b \) and \( c_f \) are dummy labels representing ``pure'' background and foreground objects, respectively. While \(\mathcal{C}_t \) and \(c'_{f}\) are target foreground classes and other foreground objects in current task $t$. The group \( c'_b \cup c_f \) stands for permanent concepts across all incremental phases, while the group \( \mathcal{C}_t  \cup  c'_{f}\) includes detailed but temporary concepts that change with different phases.

% \begin{equation}
% \scriptsize
% \begin{aligned}
% \tilde{y}^p_{i} = \begin{cases}
% c_i, & \text{if} ~y^t_{i}\in\mathcal{C}_t \vee ((y^t_{i}=c_b)\wedge(f_{t-1}(x_i)\in\mathcal{C}_{1:t-1})) \\
% % c_i, & \text{if} ~(y^t_{i}=c_b)\wedge(f_{t-1}(x_i)\in\mathcal{C}_{1:t-1}) \\
% c'_u, & \text{if} ~(y^t_{i}=c_b)\wedge (f_{t-1}(x_i)\notin\mathcal{C}_{1:t-1})\wedge(S(x_i)=1) \\
% c^{'}_b, & \text{else,}
% \end{cases},
% \qquad
% \tilde{y}^t_{i} = \begin{cases}
% y^t_{i}, & \text{if} ~y^t_{i}\in\mathcal{C}_t \\
% c_f, & \text{if} ~(y^t_{i}=c_b)\wedge(S(x_i)=1) \\
% c^{'}_b, & \text{else,}
% \end{cases}
% \end{aligned}
% \normalsize
% \end{equation}
% \begin{equation}
% % \scriptsize
% \tilde{y}^p_{i} = \begin{cases}
% c_i, & \text{if} ~y^t_{i}\in\mathcal{C}_t \vee ((y^t_{i}=c_b)\wedge(f_{t-1}(x_i)\in\mathcal{C}_{1:t-1})) \\
% % c_i, & \text{if} ~(y^t_{i}=c_b)\wedge(f_{t-1}(x_i)\in\mathcal{C}_{1:t-1}) \\
% c'_u, & \text{if} ~(y^t_{i}=c_b)\wedge (f_{t-1}(x_i)\notin\mathcal{C}_{1:t-1})\wedge(S(x_i)=1) \\
% c^{'}_b, & \text{else,}
% \end{cases},
% \end{equation}

% \begin{equation}
% \tilde{y}^t_{i} = \begin{cases}
% y^t_{i}, & \text{if} ~y^t_{i}\in\mathcal{C}_t \\
% c_f, & \text{if} ~(y^t_{i}=c_b)\wedge(S(x_i)=1) \\
% c^{'}_b, & \text{else,}
% \end{cases}
% % \normalsize
% \end{equation}
\vspace{-10pt}
\begin{equation}
    \scriptsize
    \begin{aligned}
        \tilde{y}^p_{i} &= \begin{cases}
            c_i, & \text{if} ~y^t_{i}\in\mathcal{C}_t \vee \left( (y^t_{i}=c_b) \wedge \left( f_{t-1}(x_i)\in\mathcal{C}_{1:t-1} \right) \right) \\
            c'_u, & \text{if} ~\left( y^t_{i}=c_b \right) \wedge \left( f_{t-1}(x_i) \notin \mathcal{C}_{1:t-1} \right) \wedge \left( S(x_i)=1 \right) \\
            c'_b, & \text{else,}
        \end{cases}, \\
        \tilde{y}^t_{i} &= \begin{cases}
            y^t_{i}, & \text{if} ~y^t_{i}\in\mathcal{C}_t \\
            c_f, & \text{if} ~\left( y^t_{i}=c_b \right) \wedge \left( S(x_i)=1 \right) \\
            c'_b, & \text{else,}
        \end{cases}.
    \end{aligned}
    \normalsize
    \label{equ_label}
\end{equation}
\vspace{-10pt}

% \begin{equation}
% \scriptsize
% \begin{aligned}
% \tilde{y}^p_{i} = \begin{cases}
% c_{i}, & \text{if}  ~(y^t_{i}=c_b)\wedge(f_{t-1}(x_i)\in\mathcal{C}_{1:t-1} ~or~ ~y^t_{i}\in\mathcal{C}_t\\
% c'_u, & \text{if} ~(y^t_{i}=c_b)\wedge (f_{t-1}(x_i)\notin\mathcal{C}_{1:t-1})\wedge(S(x_i)=1) \\
% c^{'}_b, & \text{else,}
% \end{cases},
% \qquad
% \tilde{y}^t_{i} = \begin{cases}
% y^t_{i}, & \text{if} ~y^t_{i}\in\mathcal{C}_t \\
% c_f, & \text{if} ~(y^t_{i}=c_b)\wedge(S(x_i)=1) \\
% c^{'}_b, & \text{else,}
% \end{cases}
% \end{aligned}
% \normalsize
% \end{equation}
Where \(f_{t-1}(\cdot)\) is the model of task $t-1$ and \( S(\cdot) \) is the salient object detector as used in SSUL~\citep{SSUL_cha2021ssul}. \(\tilde{y}_{i}^{p}\) is the label used to train \(\phi_p\), and \(\tilde{y}_{i}^{t}\) is the label used to train \(\phi_t\) for the current task \( t \). \(c_i\) is the ignored region not included in the loss calculation. The visualization of semantics decoupling is provided in the appendix. 


The objective functions for these two branches is defined as:
\begin{equation}
    \begin{aligned}
    \mathcal{L}_{p} = \mathcal{L}_{\text{\tiny BCE}}(~\phi_p(h_\theta(x_i^t)), \tilde{y}_i^p ~), \\
    \mathcal{L}_{\text{\tiny current}} = \mathcal{L}_{\text{\tiny BCE}}(~\phi_t(h_\theta(x_i^t)), \tilde{y}_i^t~).
    \end{aligned}
\end{equation}

% It's worth noting that the learning lifecycle of permanent branch \(\phi_p\) spans all incremental tasks from $1$ to $T$, and \(\phi_p\) generates the logit of \( c'_b \) and \( c'_u \). While the temporary branch \(\phi_t\) updates merely during task $t$, which produces the logit of \( c'_b \), \( \mathcal{C}_t \) and \(c_f\), helping the model distinguish target classes from other foreground. 

% To better learn these two groups of concepts, we propose to train them with separate branches. Specifically, we introduce a permanent branch \(\phi_p\) to learn the permanent dummy classes \( c'_b \) and \( c'_u \). \(\phi_p\) has a simple network structure the same as the other parallel task heads. It consists of three 3x3 convolution layers and several upsampling layers. Furthermore, the existing incremental heads \(\phi_t\) are served as the temporary branch designed to learn the concepts \( \mathcal{C}_t \) and \( c_f \) existing in the current incremental phase. It's worth noting that the learning lifecycle of permanent branch \(\phi_p\) spans all incremental tasks from $1$ to $T$, and \(\phi_p\) generates the logit of \( c'_b \) and \( c'_u \). While the temporary branch \(\phi_t\) updates merely during task $t$, which produces the logit of \( c'_b \), \( \mathcal{C}_t \) and \(c_f\), helping the model distinguish target classes from other foreground. 
% 最后关于permanent branch 和 temporary branch 的叙述不好。


% Where \(\{\tilde{y}_i^p\}_{\mathcal{C} \in \{ c'_u, c'_b \}}\) indicates the annotations of \(c'_u\) and \(c'_b\) within \(\tilde{y}_i^p\). Since \(y^t_i\) and \(f_{t-1}(x_i)\) are merely designed to obtain the regions corresponding to \(c'_u\) and \(c'_b\), they do not participate in the loss calculation. 
\vspace{-10pt}
Finally, the total optimization objective function is:
\begin{equation}
    \mathcal{L}_{total}=\mathcal{L}_{\text{\tiny IP}}+\lambda_{1}\mathcal{L}_{\text{\tiny current}}+\lambda_{2}\mathcal{L}_{p},
\end{equation}
where \(\lambda_1\) and \(\lambda_2\) are trade-off hyperparameters to balance different training objective functions.

During inference, as illustrated by the green lines in Figure 2, the permanent branch $\phi_p$ predicts on the background $c_b'$ and unknown objects $c_u'$, with only $c_b'$ used for inference. Meanwhile, the temporary branch $\phi_t$ ($t=1,2,...,T$) predicts for the target classes $\mathcal{C}_t$, the foreground region \( c_f \) and the background $c_b'$, where $\mathcal{C}_t$ and \( c_f \) are used for inference. The pixel-level prediction $\phi_{0:T}(h_\theta(x_i))$ is formulated as:
\begin{equation}
\phi_{0:T}(h_{\theta}(x_i)) = \texttt{Concat}( ~\phi_{p}(h_\theta(x_i))~,~\phi_{1:T}(h_\theta(x_i))).
\end{equation} 
Where $\phi_{p}(h_\theta(x_i))$ and $\phi_{1:T}(h_\theta(x_i))$ represent background prediction from permanent branch and the aggregated foreground predictions from all temporary branches. The pixel-level prediction is then producted by image posterior probability to form the final prediction maps as Eq~\ref{equ_3} and Eq~\ref{equ_4}.


Furthermore, to mitigate the issue of inaccurate predictions on other foreground classes \(c_f\) within each task head $\phi_t$ during inference, we introduce a Noise Filtering trick, filtering out prediction errors associated with \(c_f\). The prediction for the \( j \)-th pixel \(\hat{y}_{i,j}\) is processed as:
\begin{equation}
\hat{y}_{i,j}=\begin{cases}\alpha_{\text{\tiny NF}} \cdot \hat{y}_{i,j}&\text{if} ~max(~p^f_{i,j},~p^c_{i,j}~)=p^f_{i,j}\\\hat{y}_{i,j}&\text{if}~max(~p^f_{i,j},~p^c_{i,j}~)=p^c_{i,j}\end{cases}
\end{equation}
Where \(\alpha_{\text{\tiny NF}}\) is noise filtering term with the default value \(\alpha_{\text{\tiny NF}}=0.4\). And \(p^f_{i,j}\) and \(p^c_{i,j}\) are the \( j \)-th pixel logit outputs on the foreground \(c_f\) and target class $\mathcal{C}_t$ respectively.
% As we analyze the class incremental learning challenge in ~\cref{sec3-1:preliminary}, the image-level posterior probability $P(x \in \mathcal{X}_c \mid \mathcal{D}, \theta)$ plays a strong role to discriminate the pixel-level class prediction of class $c$, $P(x_i \in \mathcal{X}_c \mid \mathcal{D}, \theta)$ for any pixel $x_i$ in this image. This can also be explained by the following physical fact.

% As we claim in above, we faced the challenge of inconsistent output scales due to separate optimization, the best way to solve this problem is to re-scale them with global constraint factors.
\iffalse
As we claim above, we face the challenge of inconsistent output scales due to individual optimizations. The best and most straightforward solution to this problem is to rescale them with global statistical values. In this way, we propose to use the image-level posterior as this global guidance. The reason for choosing the image posterior is based on the following fact.

\textbf{Fact 1}: \textit{For any image, if its image-level class domain is $\mathcal{C}_I$ and its pixel-level class domain is $\mathcal{C}_P$, then the class domains $\mathcal{C}_I$ and $\mathcal{C}_P$ is the same, that is, $\mathcal{C}_I = \mathcal{C}_P$.}

% This fact tells us that we can constrain the model only to predict the score of classes in $\mathcal{C}_x$ through post-filtering. However, in practical CISS scenarios, image-level labels can not be directly obtained off-the-shell. 

Inspired by the above fact, we propose to build an extra prediction branch for predicting image classification labels and train it in an incremental learning manner. In testing, it predicts image-level posterior from the whole class set $\mathcal{C}_{1:T}$. In Detail, This image posterior prediction branch is implemented by a simple MLP with input dim of $4096$ and output dim of $\left|\mathcal{C}_{1:T}\right|$. 

In task $t, t > 1$, the model can get access to data from both the memory buffer $\mathcal{M}$ and training data $\mathcal{D}_t$ from the current task $t$. 
Data of the memory buffer $x_j^m$ is with the corresponding ground truth $\mathcal{Y}_j^{m}$. The class domain of $\mathcal{Y}_j^{m}$ depends on the specific task it belongs to, like $\mathcal{Y}_j^{m} \subset \mathcal{C}_{t_1}$, if $(x_j^m, y_j^m) \in \mathcal{D}_{t_1}$. 
While the image $x_i^t$ of the current task is with image-level labels $\mathcal{Y}_i^t$ from the current task class domain $\mathcal{C}_{t}$. 
During the incremental training task $t$, a data sample $x_k$ from these two sources is processed by network backbone $h_{\theta}$ into image feature $h_{\theta}(x_k)$. Then feature $h_{\theta}(x_k)$ is processed by \texttt{Adaptive Average Pooling} and \texttt{MLP} to get the final prediction $\hat{\mathcal{Y}}_k^t$, which is supervised by
% Feature $\psi(x_k)$ is processed by \texttt{Adaptive Average Pooling} and \texttt{Flatten} into a 4096-dim tensor and then mapped into a $\left|\mathcal{C}_{1:t}\right|$-dim prediction $\hat{y}_k$ by image posterior branch. Finally, the training objective function of the image posterior is 
\begin{equation}
    \mathcal{L}_{\text{\tiny IP}} = \mathcal{L}_{\text{\tiny BCE}}(\hat{\mathcal{Y}}_k, \tilde{\mathcal{Y}}_k) = \mathcal{L}_{\text{\tiny BCE}}(\psi(h_\theta(x_k)), \tilde{\mathcal{Y}}_k), ~~\tilde{\mathcal{Y}}_k = (\cup_{c\in\mathcal{C}_{1:t-1}}\phi^c(h_\theta(x_k))))\cup \mathcal{Y}_k
\end{equation}
% where $\hat{y}_k$ is the prediction of the image posterior branch on $\left|\mathcal{C}_{1:t}\right|$ classes and $y_k$ is the corresponding image-level ground-truth in one-hot form, originated from the pixel-wise annotation of task $t$.

Instead of directly using partial label $\mathcal{Y}_k$, pseudo label $\tilde{\mathcal{Y}}_k$ is preferred for rich knowledge behind it. 

In the testing, the image posterior branch directly predicts posterior probabilities on all classes $\mathcal{C}_{1:T}$. Given a test image $x_i$, its final prediction scores can be obtained by element-wise multiplying between image posterior probabilities from $\psi$ and pixel-wise predictions from $\cup_{c\in\mathcal{C}_{1:T}}\phi^c$, 
\begin{equation}
    p_{i} = \texttt{Concat}(\alpha_{\text{\tiny BR}} \cdot \phi^{c_b}(h_\theta(x_{i})), \sigma(\psi(h_\theta(x_i))) \cdot \sigma(\texttt{Concat}(\cup_{c\in\mathcal{C}_{1:T}}\phi^{c}(h_\theta(x_{i}))))).
\end{equation}
$\alpha_{\text{\tiny BR}}$ is a hyper-parameter used to compensate for the background class due to its absence in posterior probabilities. The result $p_i$ is a tensor of the shape $[C, HW]$, where $p_{i,j}$ denotes the $C$-dim prediction results on the $j$-th pixel. And $p_{i,j}^c$ is the score of the $c$-th class. The prediction of the $j$-th pixel is
\begin{equation}
    \hat{y}_{i,j} = \mathop{\arg\max}\limits_{c\in\mathcal{C}_{1:t}} p_{i,j}^c.
\end{equation}
\fi
% In the practical implementation, we adopt an improved implementation by utilizing already existing class-wise knowledge from the trained $\psi_{c_i}, c_i \in \mathcal{C}_{1:t-1}$. Instead of directly using image level classes from $\mathcal{C}_{t_1}$ for a memory sample of task $t-1$, IPSeg extends its class scope to  $\mathcal{C}_{1:t}$ by merging prediction from all segmentation heads.
% \fy{using formulation to better express.}

\iffalse
To further mitigate \textit{semantic drift} due to the mixture of noisy dummy labels $c_b, c_u$ and correct but partial labels $\mathcal{C}_t$, we propose to further decouple their learning and optimization from the semantic concepts. 

Compared to this popular taxonomy in previous works which is based on foreground classes, we group all classes into two groups: the static group and the dynamic group. In different incremental phases, the fine-grained class concepts of foreground objects change from phase to phase, for example, the foreground class set is ``cat, horse'' in the current task, and it could rapidly change into ``cow, sofa'' in the next. However, compared to these fine-grained class concepts, the meaning of background and foreground never changes. This is the static concepts group across different incremental phases. 

Based on this taxonomy, we decouple all classes into two groups: $c'_b \cup c_f$ and $c_b \cup \mathcal{C}_t$. $c'_b$ and $c_f$ are dummy labels represent for ``pure'' background and foreground objects respectively. The group $c'_b \cup c_f$ stands for the permanent concepts across all incremental phases. The group $c_b \cup \mathcal{C}_t$ are detailed but temporary concepts, changing with different phases. 

To better learn the two different groups of concepts, we also propose to learn them with different branches. Specifically, we propose to build a permanent learning head $\phi_p$ to learn the permanent classes dummy classes $c_b$ and $c_u$. In addition, the existing incremental heads $\cup_{c\in\mathcal{C}_t}\{\phi_t^c\}$ are designed to learn from the temporary concepts $c_b \cup \mathcal{C}_t$.

Based on our decoupling rules, we also re-assign the labels of image $x_i$ as 
\begin{equation}
    \left\{
    \begin{array}{l}
    \tilde{y}_{i}^{p} = y_i^t \vee (\neg y_i^t \wedge S(x_i))     &  \\
    \tilde{y}_{i}^{t} = y_i^t \vee (\neg y_i^t \wedge \phi^{t-1}(x_i)) \vee (\neg y_i^t \wedge \neg \phi^{t-1}(x_i) \wedge S(x_i))     & 
    \end{array}
    \right.
\end{equation}
where $S(\cdot)$ is the salient object detector as the same as in~\citep{}. $\tilde{y}_{i}^{p}$ is the labels assigned to the learning of permanent branch $\phi_p$. And $\tilde{y}_{i}^{t}$ is the labels assigned to the incremental learning heads $\cup_{c\in\mathcal{C}_t}\{\phi_t^c\}$ of current task $t$. Note that the proposed label re-assignment mechanism only works for this decoupling.

Based on the above pixel-level label re-assignment, the optimization for these two branches are 
\begin{equation}
    \mathcal{L}_{\text{\tiny current}} = \mathcal{L}_{\text{\tiny BCE}}(\texttt{Concat}(\cup_{c \in \mathcal{C}_{inner}} \phi_c^t(x_i^t)), \tilde{y}_i^t).
\end{equation}
\begin{equation}
    \mathcal{L}_{p} = \mathcal{L}_{\text{\tiny BCE}}(\phi_p(x_i^t), \check{y}_i^p).
\end{equation}

Finally, the total optimization objective function of IPSeg is 
\begin{equation}
    \mathcal{L}_{total}=\mathcal{L}_{\text{\tiny IP}}+\lambda_{1}\mathcal{L}_{\text{\tiny current}}+\lambda_{2}\mathcal{L}_{p},
\end{equation}
where $\lambda_1$ and $\lambda_2$ are super-parameters for trade-off between different supervision.
\fi


% $\cup_{c\in\mathcal{C}_{1:t}}\phi_t^c$. 

% Different from previous works training mixed and noisy classes $\{c_b \cup c_u \cup \mathcal{C}_t\}$ all together, IPSeg decouples these classes into two clusters, noisy and changing classes $c'_b \cup c_u$ and relatively clean classes $c_b \cup \mathcal{C}_t$.

% The reason for this taxonomy is that the concepts of background and foreground are relatively cleaner than fine-grained class concepts, e.g. cow, horse, bus, and et.al.

% IPSeg considers maintaining a long-term task branch to learn the unchanging class concepts of background and foreground which is not changed across tasks and task class scopes. $\phi_{o}$.


% in addition to adding new task heads to gradually learn different inter-task classes. 
% In this way, the learning labels for the inter-task branch are $\tilde{y}_{i}^{t} = y_i^t \vee (\neg y_i^t \wedge S(x_i))$ and labels for the outer-task branch $\psi_u$ are $\check{y}_{i}^t = y_i^t \vee (\neg y_i^t \wedge \psi^{t-1}(x_i)) \vee (\neg y_i^t \wedge \neg \psi^{t-1}(x_i) \wedge S(x_i))$.

% via an individual branch instead of mixed training together with foreground classes.

% the training supervision setting in CISS needs to be re-considered. In the incremental task $t$, the current training supervision classes are $\mathcal{C}_{t} \cup c_b$, where $c_b$ only denotes the pure background class. Previous works~\citep{} decouples the single background class $c_b$ into multiple classes $c_b, c_u$ where $c_u$ denotes the unseen classes compared to the $\mathcal{C}_{t} \cup c_b$. And ~\citep{} further adds the previous classes $\mathcal{C}_{1:t-1}$ into the current training supervision by pseudo labels from $\cup_{c\in\mathcal{C}_{1:t-1}}{\psi_c}$. Thus, the heavy semantic class $c_b$ is decomposed into multiple semantic expressions, the classes from the past and the classes not ever seen. In this pixel label assignment mechanism, the training supervision of task $t$ is $\{c_b \cup \mathcal{C}_{1:t}\cup \mathcal{C}_{1:t}\}$. 

\iffalse
However, the task heads $\psi^t = \{\cup_{c\in\mathcal{C}_t}\psi_c^t, \psi_{c_b}^t, \psi_{c_u}^t\}$ are not well organized to align with these classes. The classes $c_u$ and $\mathcal{C}_{1:t-1}$ are mixed and learned by a single task head $\psi_{c_u}$, leading to heavy semantic expression. On the other hand, noisy labels always lead to unstable optimization, and make the inconsistent output problems more severe due to coupled classes candidates in $c_u$.

Based on this discovery, we propose a decoupled class assignment mechanism that decouples the noisy and heavy label assignment from both global (inter-task) and current (inner-task) perspectives. 

In contrast to assigning a single image with messy class labels and proposing together, IPSeg proposes to separate them into the current object classes $\mathcal{C}_t \cup c_b$ and classes not belonging to current classes $\mathcal{C}_{1:t} \cup c_u$. For brevity, we name them inner-task classes $\mathcal{C}_{inner}$ and outer-task classes $\mathcal{C}_{outer}$. 

\fy{There is a problem that task prediction heads might be ambiguous referring to previous works or IPSeg.}

The learning of these two class scopes is also decoupled. Different from training a set of prediction head $\{\cup_{c\in\mathcal{C}_t}\psi_c^t, \psi_{c_b}^t, \psi_{c_u}^t\}$ using all noisy labels in a phase $k$, IPSeg considers maintain a long-term task branch to learn the changing outer-task classes across tasks in addition to adding new task heads to gradually learn different inter-task classes. In this way, the learning labels for the inter-task branch are $\tilde{y}_{i}^{t} = y_i^t \vee (\neg y_i^t \wedge S(x_i))$ and labels for the outer-task branch $\psi_u$ are $\check{y}_{i}^t = y_i^t \vee (\neg y_i^t \wedge \psi^{t-1}(x_i)) \vee (\neg y_i^t \wedge \neg \psi^{t-1}(x_i) \wedge S(x_i))$. \fy{The reasoning for this design is that.}

Based on the above pixel-level class label assignment, the optimization for these two branches are 
\begin{equation}
    \mathcal{L}_{\text{\tiny inner}} = \mathcal{L}_{\text{\tiny BCE}}(\texttt{Concat}(\cup_{c \in \mathcal{C}_{inner}} \psi_c^t(x_i^t)), \tilde{y}_i^t).
\end{equation}
\begin{equation}
    \mathcal{L}_{\text{\tiny outer}} = \mathcal{L}_{\text{\tiny BCE}}(\psi_u(x_i^t), \check{y}_i^t).
\end{equation}

Finally, the total optimization objective function is 
\begin{equation}
    L_{total}=L_{IP}+\lambda_{1}\mathcal{L}_{\text{\tiny inner}}+\lambda_{2}\mathcal{L}_{\text{\tiny outer}},
\end{equation}
where $\lambda_1$ and $\lambda_2$ are super-parameters for trade-off between different supervision.
\fi

\subsection{Improving Memory Buffer}

% Besides the distinctive design outlined above, the memory buffer \(\mathcal{M}\) plays a crucial role in supporting implementation. Following prior works, the memory buffer \(\mathcal{M}\) utilizes minimal capacity to store past samples and classes while adhering to privacy policies. We enhance the efficiency of the memory buffer with a class-balanced sampling strategy and storage cost reduction.

% 这部分并不重要，不需要单独拆两段论述了，可以整合一下
% Besides the distinctive design outlined above, 
The memory buffer \(\mathcal{M}\) plays a crucial role in our implementation and we implement the memory buffer based on unbiased learning and storage efficiency. 
% \paragraph{Class rebalance design}
IPSeg employs a class-balanced sampling strategy, ensuring the image posterior branch can adequately access samples from all classes. Specifically, given the memory size \(\left|\mathcal{M}\right|\) and the number of already seen classes \(\left|\mathcal{C}_{1:t}\right|\), the sampling strategy ensures there are at least \(\left|\mathcal{M}\right|//\left|\mathcal{C}_{1:t}\right|\) samples for each class.
% To ensure that the image posterior branch can adequately access samples from all classes and is trained with the least bias towards any specific class, IPSeg employs a class-balanced sampling strategy. Specifically, for a given memory size \(\left|\mathcal{M}\right|\) and the number of already seen classes \(\left|\mathcal{C}_{1:t}\right|\), we ensure that samples from each class appear at least \(\left|\mathcal{M}\right|//\left|\mathcal{C}_{1:t}\right|\) times.
IPSeg also optimizes the storage cost of $\mathcal{M}$ by only storing image-level labels and object salient masks for samples. Image-level labels are required for the image posterior branch for unbiased classification. While the salient masks split images into background and foreground objects, labeled with 0 and 1 respectively. This simplification mechanism requires less storage cost compared to previous methods that store the whole pixel-wise annotations on all classes. More details can be found in the appendix.
% \paragraph{Storage cost reduction} In IPSeg, only image-level labels and object salient masks of samples from \(\mathcal{M}\) are needed. Image-level labels are required for the image posterior (IP) branch to obtain IP guidance. While the salient masks simply divide the image into background and foreground objects, labeled with 0 and 1 respectively. This simplification mechanism requires less storage cost compared to previous methods that store the whole pixel-wise annotations of all classes, allowing IPSeg to store the same number of samples more efficiently.


\iffalse
Besides the above distinctive design, the memory buffer $\mathcal{M}$ is also important to support our method implementation. Following previous works, the memory buffer $\mathcal{M}$ utilizes minimal capacity to access past samples and classes without violating privacy policies. We improve the implementation of the memory buffer with a class-balanced sampling strategy.

\paragraph{Class rebalance design for memory buffer} To ensure that the image posterior branch can adequately access samples from all classes and 
is trained without bias to a specific class, IPSeg selects representative samples from both old and new classes and employs a class-balanced strategy for sample selection. Specifically, for a given memory size $\left|\mathcal{M}\right|$ and the number of already seen classes $\left|\mathcal{C}_{1:t}\right|$, we strictly ensure that each class samples appear at least $\left|M\right|/\left|\mathcal{C}_{1:t}\right|$ times. 
Further, the number of foreground object classes and pixels are counted as candidate selection criteria.

\paragraph{Simplified annotation for memory} In IPSeg, only image-level labels and object salient masks are needed. Image-level labels are needed for the image posterior branch. The salient mask simply divides the image into the background and foreground objects, labeled with $0$ and $1$ respectively. This simplification method needs just $1/\lceil\log_2C\rceil$ of storage cost to store the same number of samples as previous works do. 
\fi

\iffalse
\subsection{Image Posterior Guidance}
\label{sec3-3}

\textbf{Fact 1} \textit{For one image, if its image-level class scope is $\mathcal{C}_I$ and its pixels class scope is $\mathcal{C}_P$, class scope $\mathcal{C}_I$ and $\mathcal{C}_P$ are the same, that is $\mathcal{C}_I = \mathcal{C}_P$.}

This fact tells us that we can constrain the model only to predict the score of classes in $\mathcal{C}_x$ through post-filtering. However, in practical scenarios, image-level labels can not be directly obtained off-the-shell. 

To well utilize the above cues from the image level, we propose to build an image posterior prediction branch which trained in an incremental manner. In Detail, This image posterior prediction branch is implemented by a simple MLP with input dim of $4096$ and output dim of $\left|C\right|$. 

\fy{Image level gt/prediction is not differentiated from pixel-level...}
In task $t, t > 0$, the model can get access to two kinds of data, data in memory pool $x^M$ and data of the current task $x^t$. Data of the memory pool is with the corresponding class scope $\mathcal{C}_{1:t-1}$, while data of the current task is with the current task class scope $\mathcal{C}_{t}$. It ensures the image posterior branch can only get limited access to the previous class scope. In image posterior construction, a data sample $x_k$ from these two sources is processed by network backbone $h_{\phi}$ into image feature $h_{\phi}(x_k)$. Feature $h_{\phi}(x_k)$ is processed by \texttt{Adaptive Average Pooling} and \texttt{Flatten} into a 4096-dim tensor and then mapped into a $\left|\mathcal{C}_{1:t}\right|$-dim prediction $\hat{y}_k$ by image posterior branch. Finally, the training objective function of the image posterior is 
\begin{equation}
    \mathcal{L}_{\text{\tiny IP}} = \mathcal{L}_{\text{\tiny BCE}}(\hat{y}_k, y_k) 
\end{equation}
where $\hat{y}_k$ is the prediction of the image posterior branch on $\left|\mathcal{C}_{1:t}\right|$ classes and $y_k$ is the corresponding image-level ground-truth in one-hot form, originated from the pixel-wise annotation of task $t$.

In the inference, the trained image posterior branch directly predicts posterior probabilities on all classes. Given an image sample $x$ to be tested, the final prediction score on its pixel $x_{j}$ of a certain class $c$ can be obtained by element-wise multiplying between image posterior probability and pixel-wise prediction, 
\begin{equation}
    p_{j}^{c} = \sigma(\hat{y}^c) \times \sigma(\psi_{c}(h_\phi(x_{i,j}))).
\end{equation}
The final prediction on this pixel is,
\begin{equation}
    \hat{y}_{j} = \mathop{\arg\max}\limits_{c\in\mathcal{C}_{1:t}} p_{j}^c
\end{equation}

In the practical implementation, we adopt an improved implementation by utilizing already existing class-wise knowledge from the trained $\psi_{c_i}, c_i \in \mathcal{C}_{1:t-1}$. Instead of directly using image level classes from $\mathcal{C}_{t_1}$ for a memory sample of task $t-1$, IPSeg extends its class scope to  $\mathcal{C}_{1:t}$ by merging prediction from all segmentation heads.
\fy{using formulation to better express.}

\textbf{Class rebalance design for memory pool.} To ensure that the image posterior branch can adequately receive samples from all classes and maintain class scaling capability, for a given memory constraint, IPSeg selects representative samples from both old and new classes and employs a class-balanced strategy for collection to support effective training of image posterior branch. Specifically, for a given memory size $\left|M\right|$ and the number of already seen classes $\left|\mathcal{C}_{1:t}\right|$, we strictly ensure that each class's data appears at least $\left|M\right|/\left|\mathcal{C}_{1:t}\right|$ times. 
Further, for the memory pool candidates selection, the number of foreground pixels and classes are tallied as selection criteria. 
\fi

\iffalse
\fy{ During the incremental process, each phase only accesses annotated data for specific classes, leading to a complete loss of information for unknown foregrounds and foregrounds that should belong to old classes. Our method integrates saliency information to enhance the ability of each newly added classification head to recognize background and non-current classes. We add and maintain a continuously trained \(head[0]\), which is specifically responsible for identifying background and unknown classes. Pseudo-labeling is utilized to help classification heads maintain responsiveness to knowledge of old classes. Each classification head can not only recognize the classes of the current phase but also effectively distinguish between the background and non-current phase classes, thereby addressing the issue of complete loss of information for non-current classes during the incremental process, as discussed in section~\cref{sec3-1:preliminary}.

\paragraph{Step-wise knowledge mining for non-current}
We added phase-specific background and non-current recognition capabilities to the classification heads at different phases of the main network branch. By using saliency information to highlight potential foregrounds and combining class labels specific to this phase, we integrate the recognition of backgrounds and non-current classes into the training. This ensures that each classification head "sees" non-phase classes during training, achieving feature scale consistency within the heads at different phases of the model. During the training phase, auxiliary information such as a saliency mask and pseudo-labels are added to the ground truth, resulting in two new ground truths: ground truth for global and ground truth for current, corresponding to \(global\_gt\) and \(curr\_gt\) in Figure~\cref{fig:overview}, respectively. This process can be described as follows:
\[gt_{current}=gt_t\cup(\sim gt_t\cap Sal(x_m))\]
\[gt_{global}=gt_{t}\cup(\sim gt_{t}\cap\Theta_{t-1}(x_{m}))\cup(\sim gt_{t}\cap\sim\Theta_{t-1}(x_{m})\cap Sal(x_{m}))\]

Where, \(gt\) denotes the true class labels for this phase, \(Sal\) represents the saliency detection, and \( \Theta_{t-1}(x_{m})\) denotes the output from the model of the previous phase.  \( \Theta_{t-1}(x_{m})\)' is obtained according to the following rules:

\[\theta_{t-1}(x_m)_{(i,j)}=\begin{cases}\theta_{t-1}(x_m)_{(i,j)}~~~where~\theta_{t-1}(x_m)_{(i,j)}>\tau\\0~~~~~~~~~~~~~~~~~~~~~~~others\end{cases}\]

Additionally, to ensure better recognition of both current and other classes, this method fully mines foreground knowledge hidden at different depths within the model. It incorporates a feature integration mechanism that operates layer by layer to enhance its generalization capability and accuracy:

\begin{equation}
    {fm}_{{new}}={M}^{'}({G}_{1}(\Psi_{1}),G_{2}(\Psi_{2}),G_{3}(\Psi_{3}), ASPP(\Psi_{4}))
\end{equation}

% \[{fm}_{{new}}={M}^{'}({G}_{1}(\Psi_{1}),G_{2}(\Psi_{2}),G_{3}(\Psi_{3}), ASPP(\Psi_{4}))\]

Where $Psi_{i}$ represents the semantic information extracted from different layers of the backbone. $ASPP$ is the original method used in DeepLab V3. $G_{i}$ stands for the information mining function, and $M^{'}$ is the feature integration function, which aligns the processed semantic information from different layers into a new feature map.

\paragraph{Dual-loss for global and current}
After obtaining the two types of ground truth, we designed a composite loss strategy that combines global output and current output. In this approach, \(global\_gt\) focuses on constraining \(head[0]\) to recognize unknown classes, while \(curr\_gt\) focuses on constraining the new phase classification head to distinguish between its current and non-current items. This design makes the model more efficient when dealing with images with complex backgrounds and multiple classes, enhancing the new classification head's ability to recognize non-phase classes while also improving the identification of backgrounds and unknown classes. It provides a structured solution to balance the recognition of various classes. This can be described as follows:
\[output_{global}=H_0^c(fm_{new})\oplus Concat_{i=1}^tS(H_i^c(fm_{new}))\]
\[output_{current}=S^{-1}(H_t^c(fm_{new}))\oplus Concat_{i=1}^tS(H_i^c(fm_{new}))\]
\[L_{global}=BCE(output_{global},gt\_global)\]
\[L_{current}=BCE( output_{current} , gt\_curr)\]

Where, \(S\) denotes the removal of results from background and non-current in the classifier, \(S^{-1}\) denotes the removal of results from various classes, and \(\oplus\) represents the concatenation operation. Finally, the design of the loss during the training phase can be described as follows:
\[L_{total}=L_{scale}+\lambda_{1}L_{global}+\lambda_{2}L_{current}\]
Where \(\lambda_{i}\) is a hyper-parameter controlling the loss.}
\fi

\iffalse
\paragraph{Intra-head soft mask for output}
During the inference phase, a \(softmax\) operation is used within each classifier, in conjunction with a soft mask to process the semantic segmentation results, as illustrated in the bottom of Figure~\cref{fig:overview}. This approach further leverages non-current information within the classifier, and the soft mask enables decision-making without compromising the integrity of the original semantic outputs, effectively preventing classification errors. This can be described as follows:
\[M_t^c=Softmax(h_t^c)\]
\[h_{t_{(i,j)}}^{c}=\begin{cases}\lambda_{mask}\cdot class~t~~~~where~ M_{t_{(i,j)}}^{c}=non\_current\\class~t~~~~~~~~~~~~~~~~~where~M_{t_{(i,j)}}^{c}=class~t\\bg~~~~~~~~~~~~~~~~~~~~~~~~~where~ M_{t_{(i,j)}}^{c}=bg\end{cases}\]

Where,  \(h_t^c\) represents the output results from the classification head, \(M_t^c\) denotes the integrated information after applying a \(softmax\) operation to  \(h_t^c\), and \(\lambda_{mask}\) represents the soft mask coefficient.
\fi
