% \vspace{-5pt}

\begin{table*}[t]
    \centering
    \caption{Comparison with state-of-the-art methods on Pascal VOC 2012 dataset across 4 typical incremental scenarios. ``-'' denotes the results are not provided in the original paper. \textsuperscript{†} denotes the result is reproduced using the official code with Swin-B backbone. ``IPSeg w/o M'' denotes the data-free version of IPSeg, which is trained without memory buffer.}
    \resizebox{0.97\linewidth}{!}{
    \begin{tabular}{c|l|c||ccc|ccc|ccc|ccc}
    \toprule
    \multicolumn{2}{c |}{\multirow{2}{*}{Method}}  & \multirow{2}{*}{Backbone} & \multicolumn{3}{c|}{\textbf{VOC 15-5 (2 steps)}} &\multicolumn{3}{c|}{\textbf{VOC 15-1 (6 steps)}}  &\multicolumn{3}{c|}{\textbf{VOC 10-1 (11 steps)}} &\multicolumn{3}{c}{\textbf{VOC 2-2 (10 steps)}} \\
     \multicolumn{2}{c |}{} & & 0-15 & 16-20 & all & 0-15 & 16-20 & all & 0-10 & 11-20 & all & 0-2 & 3-20 & all \\
    \midrule
    \multirow{9}{*}{\rotatebox{90}{Data-free}}
    % & Joint & Resnet-101 & 80.5 & 73.0 & 78.2 & 80.5 & 73.0 & 78.2 & 79.1 & 77.1 & 78.2 & 73.9 & 78.9 & 78.2\\
    
    % & LwF-MC~\citep{iCaRL_rebuffi2017icarl} & Resnet-101 & 58.1 & 35.0 & 52.3 & 6.4 & 8.4 & 6.9 & 4.7 & 5.9 & 5.0 & 3.5 & 4.7 & 4.5\\
    % & ILT~\citep{ilt_michieli2019incremental} & Resnet-101 & 67.1 & 39.2 & 60.5 & 8.8 & 8.0 & 8.6 & 7.2 & 3.7 & 5.5 & 5.8 & 5.0 & 5.1\\
    & MiB~\citep{MiB_cermelli2020modeling} & Resnet-101 & 71.8 & 43.3 & 64.7 & 46.2 & 12.9 & 37.9 & 12.3 & 13.1 & 12.7 & 41.1 & 23.4 & 25.9\\
    & SSUL~\citep{SSUL_cha2021ssul} & Resnet-101 & 77.8 & 50.1 & 71.2 & 77.3 & 36.6 & 67.6 & 71.3 & 46.0 & 59.3 & 62.4 & 42.5 & 45.3 \\
    % & RCIL~\citep{rcil_zhang2022representation} & Resnet-101 & \textbf{78.8} & 52.0 & 72.4 & 70.6 & 23.7 & 59.4 & 55.4 & 15.1 & 34.3 & 28.3 & 19.0 & 19.4\\
    & IDEC~\citep{idec_zhao2023inherit} & Resnet-101 & 78.0 & 51.8 & 71.8 & 77.0 & 36.5 & 67.3 & 70.7 & 46.3 & 59.1 & - & - & -\\
    % & PLOP+LGKD~\citep{lgkd_yang2023label} & Resnet-101 & 75.2 & 54.8 & 71.1 & 69.3 & 30.9 & 61.1  & - & - & - & - & - & - \\
    % & \textcolor{blue}{PLOP+LGKD~\citep{lgkd_yang2023label}} & \textcolor{blue}{Resnet-101} & \textcolor{blue}{75.2} & \textcolor{blue}{54.8} & \textcolor{blue}{71.1} & \textcolor{blue}{69.3} & \textcolor{blue}{30.9} & \textcolor{blue}{61.1} & \textcolor{blue}{-} & \textcolor{blue}{-} & \textcolor{blue}{-} & \textcolor{blue}{-} & \textcolor{blue}{-} & \textcolor{blue}{-} \\

    & PLOP+NeST~\citep{nest_xie2024early}& Resnet-101 & 77.6 & \textbf{55.8} & 72.4 & 72.2 & 33.7 & 63.1 & 54.2 & 17.8 & 36.9 & - & - & - \\
    & LAG~\citep{lag_yuan2024learning} & Resnet-101 & 77.3 & 51.8 & 71.2 & 75.0 & 37.5 & 66.1 & 69.6 & 42.6 & 56.7 & - & - & -\\
    & IPSeg w/o M (ours) & Resnet-101 & \textbf{78.5} & 55.2 & \textbf{72.9} & \textbf{77.4} & \textbf{41.9} & \textbf{68.9} & \textbf{74.9} & \textbf{52.9} & \textbf{64.4} & \textbf{64.7} & \textbf{51.5} & \textbf{53.4} \\
    \cmidrule{2-15}
    & SSUL~\citep{SSUL_cha2021ssul} & Swin-B & 79.7 & 55.3 & 73.9 & 78.1 & 33.4 & 67.5 & 74.3 & 51.0 & 63.2 & 60.3 & 40.6 & 44.0 \\
    & MicroSeg~\citep{microseg_zhang2022mining} & Swin-B & \textbf{81.9} & 54.0 & 75.2 & 80.5 & 40.8 & 71.0 & 73.5 & 53.0 & 63.8 & 64.8 & 43.4 & 46.5 \\
    & PLOP+NeST~\citep{nest_xie2024early} & Swin-B & 80.5 & \textbf{70.8} & \textbf{78.2} & 76.8 & \textbf{57.2} & 72.2 & 64.3 & 28.3 & 47.3 & - & - & -\\
    & IPSeg w/o M (ours) & Swin-B & 81.4 & 62.4 & 76.9 & \textbf{82.4} & 52.9 & \textbf{75.4} & \textbf{80.0} & \textbf{61.2} & \textbf{71.0} & \textbf{72.1} & \textbf{64.5} & \textbf{65.5} \\
    \midrule
    \multirow{12}{*}{\rotatebox{90}{Replay}} 
    & \textcolor{gray}{Joint} & \textcolor{gray}{Resnet-101} & \textcolor{gray}{80.5} & \textcolor{gray}{73.0} & \textcolor{gray}{78.2} & \textcolor{gray}{80.5} & \textcolor{gray}{73.0} & \textcolor{gray}{78.2} & \textcolor{gray}{79.1} & \textcolor{gray}{77.1} & \textcolor{gray}{78.2} & \textcolor{gray}{73.9} & \textcolor{gray}{78.9} & \textcolor{gray}{78.2}\\
    & SDR~\citep{sdr_michieli2021continual} & Resnet-101 & 75.4 & 52.6 & 69.9 & 44.7 & 21.8 & 39.2 & 32.4 & 17.1 & 25.1 & 13.0 & 5.1 & 6.2\\
    & PLOP-M~\citep{PLOP_douillard2021plop} & Resnet-101 & 78.5 & 65.6 & 75.4 & 71.1 & 52.6 & 66.7 & 57.9 & 51.6 & 54.9 & - & - & -\\
    & SSUL-M~\citep{SSUL_cha2021ssul} & Resnet-101 & 79.5 & 52.9 & 73.2 & 78.9 & 43.9 & 70.6 & 74.8 & 48.9 & 65.5 & 58.8 & 45.8 & 47.6\\
    & MicroSeg-M~\citep{microseg_zhang2022mining} & Resnet-101 & \textbf{82.0} & 59.2 & 76.6 & \textbf{81.3} & 52.5 & 74.4 & \textbf{77.2} & 57.2 & 67.7 & 60.0 & 50.9 & 52.2\\
    & PFCSS-M~\citep{PFCSS_lin2023preparing} & Resnet-101 & 79.9 & 70.2 & 77.1 & 77.1 & \textbf{60.4} & 73.1 & 69.5 & 63.2 & 66.5 & - & - & -\\
    & Adapter~\citep{adapter_zhu2024adaptive} & Resnet-101 & - & - & - & 79.9 & 51.9 & 73.2 & 74.9 & 54.3 & 65.1 & 62.8 & 57.9 & 58.6 \\
    & IPSeg (ours) & Resnet-101 & 79.5 & \textbf{71.0} & \textbf{77.5} & 79.6 & 58.9 & \textbf{74.7} & 75.9 & \textbf{66.4} & \textbf{71.4} & \textbf{62.4} & \textbf{61.0} & \textbf{61.2}\\
    \cmidrule{2-15}
    % & Joint & Swin-B & 83.8 & 79.3 & 82.7 & 83.8 & 79.3 & 82.7 & 82.4 & 83.0 & 82.7 & 75.8 & 83.9 & 82.7\\
    & \textcolor{gray}{Joint} & \textcolor{gray}{Swin-B} & \textcolor{gray}{83.8} & \textcolor{gray}{79.3} & \textcolor{gray}{82.7} & \textcolor{gray}{83.8} & \textcolor{gray}{79.3} & \textcolor{gray}{82.7} & \textcolor{gray}{82.4} & \textcolor{gray}{83.0} & \textcolor{gray}{82.7} & \textcolor{gray}{75.8} & \textcolor{gray}{83.9} & \textcolor{gray}{82.7}\\
    & SSUL-M\textsuperscript{†}~\citep{SSUL_cha2021ssul} & Swin-B & 79.3 & 55.1 & 73.5 & 78.8 & 49.7 & 71.9 & 75.3 & 54.1 & 65.2 & 61.1 & 47.5 & 49.4\\
    & MicroSeg-M\textsuperscript{†}~\citep{microseg_zhang2022mining} & Swin-B & 82.9 & 60.1 & 77.5 & 82.0 & 47.3 & 73.3 & 78.9 & 59.2 & 70.1 & 62.7 & 51.4 & 53.0\\
    & CoinSeg-M~\citep{coinseg_zhang2023coinseg} & Swin-B & \textbf{84.1} & 69.9 & 80.8 & \textbf{84.1} & 65.5 & 79.6 & \textbf{81.3} & 64.4 & 73.7 & 68.4 & 65.6 & 66.0\\
    & IPSeg (ours) & Swin-B & 83.3 & \textbf{73.3} & \textbf{80.9} & 83.5 & \textbf{75.1} & \textbf{81.5} & 80.3 & \textbf{76.7} & \textbf{78.6} & \textbf{73.1} & \textbf{72.3} & \textbf{72.4}\\
    \bottomrule
    \end{tabular}
    % \vspace{-10pt}
    }
    \label{tab:voc_res}
\end{table*}


\section{Experiments}
\label{sec:Experiments}

% \vspace{-5pt}
\subsection{Experimental Setups}
\label{sec4-1:exp_setup}

\paragraph{Dataset} 
Following previous works~\citep{coinseg_zhang2023coinseg,SSUL_cha2021ssul}, we evaluate our method using the Pascal VOC 2012~\citep{voc} and ADE20K~\citep{ade20k} datasets. Pascal VOC 2012 includes 20 foreground classes and one background class, with 10,582 training images and 1,449 validation images. ADE20K, a larger-scale dataset, comprises 150 classes of stuff and objects, with 20,210 training images and 2,000 validation images.











\paragraph{Protocols} 
We primarily use the \textit{overlap} setting to evaluate our method. This setting is more challenging and realistic than the \textit{disjoint} setting~\citep{sats_qiu2023sats}, as the images may contain both seen and unseen classes across different incremental steps. We evaluate IPSeg under several incremental scenarios, denoted as $M$-$N$, where $M$ is the number of classes learned initially, and $N$ is the number of classes learned in each incremental step. For example, VOC 15-1 (6 steps) means learning 15 classes initially and one new class in each subsequent step until all 20 classes are learned. We use the mean Intersection over Union (mIoU) as the evaluation metric.

% \vspace{-5pt}
\paragraph{Implementation details}
Following previous works~\citep{sppa_lin2022continual,DKD_baek2022decomposed,ewf_xiao2023endpoints}, IPSeg utilizes DeepLab V3~\citep{deeplab_v3_chen2017rethinking} as the segmentation model with ResNet-101~\citep{resnet_he2016deep} and Swin Transformer-base (Swin-B)~\citep{swin_liu2021swin} pre-trained on ImageNet-1K~\citep{imagenet_1k_deng2009imagenet} as the backbones. The training batch size is 16 for Pascal VOC 2012 and 8 for ADE20K. IPSeg uses the SGD optimizer with a momentum of 0.9 and a weight decay of $1e$-$4$. The learning rates for both datasets are set to 0.01, with learning rate policies of poly for Pascal VOC 2012 and warm poly for ADE20K. All experiments are conducted with 2 NVIDIA GeForce RTX 3090 GPUs. For a fair comparison, the memory size is set as the same as SSUL~\citep{SSUL_cha2021ssul} that \(|\mathcal{M}|=100\) for Pascal VOC 2012 and \(|\mathcal{M}|=300\) for ADE20K.
Following SSUL~\citep{SSUL_cha2021ssul}, IPSeg is trained for 50 epochs on Pascal VOC 2012 and 60 epochs on ADE20K.
Pseudo-label~\citep{coinseg_zhang2023coinseg} and saliency information~\citep{SOD_deep_used_hou2017deeply} are adopted as previous methods~\citep{SSUL_cha2021ssul, microseg_zhang2022mining}. To avoid information leaking, the ground truth of the training data in the image posterior branch only consists of annotations and pseudo-labels from the corresponding steps.

% \vspace{-5pt}
\paragraph{Baselines} We compare IPSeg with various CISS methods, including MiB~\citep{MiB_cermelli2020modeling}, SDR~\citep{sdr_michieli2021continual}, and PLOP~\citep{PLOP_douillard2021plop}, as well as state-of-the-art methods such as SSUL~\citep{SSUL_cha2021ssul}, MicroSeg~\citep{microseg_zhang2022mining}, PFCSS~\citep{PFCSS_lin2023preparing}, CoinSeg~\citep{coinseg_zhang2023coinseg}, NeST~\citep{nest_xie2024early}, LAG~\citep{lag_yuan2024learning}, and Adapter~\citep{adapter_zhu2024adaptive}. Among these, PFCSS, CoinSeg, and Adapter are the current state-of-the-art replay methods. For a fair comparison, we reproduce some works using their official code with the Swin-B backbone. Additionally, we provide the results of \textbf{Joint} as a theoretical upper bound for incremental tasks. We report incremental results in three parts: initial classes, new classes, and overall classes.

\subsection{Main Results}

\input{sections/figure}

IPSeg is initially designed with a memory buffer \(\mathcal{M}\), enabling it to fully leverage category distribution knowledge from previous samples to mitigate the separate optimization. Consequently, our primary objective is to demonstrate the superiority of IPSeg by comparing it with other replay-based methods, such as ``CoinSeg-M''. Additionally, we also present the results of IPSeg without \(\mathcal{M}\) (denotes as ``IPSeg w/o M'') to show the potential and robustness of IPSeg.

\begin{table*}[t]
    \centering
        \caption{Comparison with state-of-the-art methods on ADE20K dataset. \textsuperscript{†} denotes the result is reproduced using the official code with Swin-B backbone. \textsuperscript{*} denotes the results from a longer training schedule of 100 epochs, while 60 epochs in ours.}

    \resizebox{0.97\linewidth}{!}{
    \begin{tabular}{c|l|c||ccc|ccc|ccc|ccc}
    \toprule
    \multicolumn{2}{c |}{\multirow{2}{*}{Method}}  & \multirow{2}{*}{Backbone} & \multicolumn{3}{c|}{\textbf{ADE 100-5 (11 steps)}} &\multicolumn{3}{c|}{\textbf{ADE 100-10 (6 steps)}}  &\multicolumn{3}{c|}{\textbf{ADE 100-50 (2 steps)}} &\multicolumn{3}{c}{\textbf{ADE 50-50 (3 steps)}} \\
     \multicolumn{2}{c |}{} & & 0-100 & 101-150 & all & 0-100 & 101-150 & all & 0-100 & 101-150 & all & 0-50 & 51-150 & all \\
    \midrule
    \multirow{12}{*}{\rotatebox{90}{Data-free}} 
    % & LwF-MC & Resnet-101 & \\
    % & Joint & Resnet-101 & 43.5 & 29.4 & 38.3 & 43.5 & 29.4 & 38.8 & 43.5 & 29.4 & 38.8 & 50.3 & 32.7 & 38.8 \\
   
    % & ILT & Resnet-101 & 0.1 & 1.3 & 0.5 & 0.1 & 3.1 & 1.1 & 18.3 & 14.4 & 17.0 & 3.5 & 12.9 & 9.7\\
    & MiB~\citep{MiB_cermelli2020modeling} & Resnet-101 & 36.0 & 5.7 & 26.0 & 38.2 & 11.1 & 29.2 & 40.5 & 17.2 & 32.8 & 45.6 & 21.0 & 29.3\\
    & SSUL~\citep{SSUL_cha2021ssul} & Resnet-101&39.9&17.4&32.5&40.2&18.8&33.1&41.3&18.0&33.6&48.4&20.2&29.6 \\
    % & RCIL~\citep{rcil_zhang2022representation} & Resnet-101 & 38.5 & 11.5 & 29.6 & 39.3 & 17.6 & 32.1 & 42.3 & 18.8 & 34.5 & 48.3 & 25.0 & 32.5\\
    & MicroSeg~\citep{microseg_zhang2022mining} & Resnet-101 & 40.4&20.5&33.8&\textbf{41.5}&21.6&34.9&40.2&18.8&33.1&48.6&24.8&32.9\\
    & IDEC~\citep{idec_zhao2023inherit} & Resnet-101 & 39.2 & 14.6 & 31.0 & 40.3 & 17.6 & 32.7 & 42.0 & 18.2 & 34.1 & 47.4 & 26.0 & 33.1\\
    & AWT+MiB~\citep{goswami2023attribution} & Resnet-101 & 38.6 & 16.0 & 31.1 & 39.1 & 21.4 & 33.2 & 40.9 & \textbf{24.7} & 35.6 & 46.6 & 27.0 & 33.5 \\
    
    % & PLOP+LGKD~\citep{lgkd_yang2023label} & Resnet-101 & - & - & - & 42.1 & 22.0 & 35.4 & 43.6 & 25.7 & 37.5 & \textbf{49.4} & \textbf{29.4} & \textbf{36.0} \\
    % & ECLIPSE*~\citep{eclipse_kim2024eclipse} & Resnet-101  & \textbf{43.3} & 16.3 & 34.2 & \textbf{43.4} & 17.4 & 34.6 & \textbf{45.0} & 21.7 & 37.1 & - & - & - \\
    
    
    
    % & \textcolor{blue}{PLOP+LGKD~\citep{lgkd_yang2023label}} & \textcolor{blue}{Resnet-101} & \textcolor{blue}{-} & \textcolor{blue}{-} & \textcolor{blue}{-} & \textcolor{blue}{\textbf{42.1}} & \textcolor{blue}{22.0} & \textcolor{blue}{\textbf{35.4}} & \textcolor{blue}{\textbf{43.6}} & \textcolor{blue}{\textbf{25.7}} & \textcolor{blue}{\textbf{37.5}} & \textcolor{blue}{\textbf{49.4}} & \textcolor{blue}{\textbf{29.4}} & \textcolor{blue}{\textbf{36.0}} \\
    & PLOP+NeST~\citep{nest_xie2024early} & Resnet-101 & 39.3 & 17.4 & 32.0 & 40.9 & 22.0 & 34.7 & \textbf{42.2} & 24.3 & \textbf{36.3} & \textbf{48.7} & \textbf{27.7} & \textbf{34.8} \\
    & BAM~\citep{bam_zhang2025background} & Resnet-101 & 40.5 & 21.1 & 34.1 & 41.1 & 23.1 & 35.2 & 42.0 & 23.0 & 35.7 & 47.9 & 26.5 & 33.7 \\
    % & IPSeg w/o M (ours) & Resnet-101 & \textbf{41.0}&\textbf{22.4}&\textbf{34.8}&41.0&\textbf{23.6}&35.3&41.3&24.0&35.5&46.7&26.2&33.1\\
    & IPSeg w/o M (ours) & Resnet-101 & \textbf{41.0} & \textbf{22.4} & \textbf{34.8} & 41.0 & \textbf{23.6} & \textbf{35.3} & 41.3 & 24.0 & 35.5 & 46.7 & 26.2 & 33.1 \\

    \cmidrule{2-15}
    & SSUL\textsuperscript{†}~\citep{SSUL_cha2021ssul} & Swin-B & 41.3&16.0&32.9&40.7&19.0&33.5&41.9&20.1&34.6&49.5&21.3&30.7\\
    & CoinSeg~\citep{coinseg_zhang2023coinseg} & Swin-B & \textbf{43.1}&24.1&36.8&42.1&24.5&36.2&41.6&26.7&36.6&49.0&28.9&35.6\\
    & PLOP+NeST~\citep{nest_xie2024early} & Swin-B & 39.7 & 18.3 & 32.6 & 41.7 & 24.2 & 35.9 & \textbf{43.5} & 26.5 & 37.9 & \textbf{50.6} & 28.9 & 36.2 \\
    % & IPSeg w/o M (ours) & Swin-B & \textbf{43.1}&\textbf{26.2}&\textbf{37.6}&\textbf{42.5}&\textbf{27.8}&\textbf{37.6}&43.2&\textbf{29.0}&\textbf{38.4}&49.3&\textbf{33.0}&\textbf{38.5}\\
    & IPSeg w/o M (ours) & Swin-B & \textbf{43.1} & \textbf{26.2} & \textbf{37.6} & \textbf{42.5} & \textbf{27.8} & \textbf{37.6} & 43.2 & \textbf{29.0} & \textbf{38.4} & 49.3 & \textbf{33.0} & \textbf{38.5} \\

    
    

    \midrule
    \multirow{9}{*}{\rotatebox{90}{Replay}} 
    % & SDR & Resnet-101 & \\
    % & PLOP-M & Resnet-101 & \\
    % & SSUL-M & Resnet-101 & 42.5 & 15.9 & 34.0 & 42.2 & 16.0 & 33.9 & 42.2 & 14.0 & 32.8 & 49.6 & 25.9 & 33.8\\
    % & MicroSeg-M & Resnet-101 & \textbf{43.6} & 22.4 & 36.6 & \textbf{43.7} & 22.2 & 36.6 & 43.4 & 20.9 & 35.9 & 49.8 & 22.0 & 31.4\\
    % & PFCSS-M & Resnet-101 & - & - & - & 41.6 & 25.8 & 36.3 & 43.1 & 26.0 & 37.5 & 49.2 & 28.5 & 35.4\\
    % & IPSeg(ours) & Resnet-101 & \\
    % \cmidrule{2-15}
    % & Joint & Swin-B & 47.2 & 31.9 & 42.1 & 47.2 & 31.9 & 42.1 & 47.2 & 31.9 & 42.1 & 54.6 & 35.5 & 42.1\\
     & \textcolor{gray}{Joint} & \textcolor{gray}{Resnet-101} & \textcolor{gray}{43.5} & \textcolor{gray}{29.4} & \textcolor{gray}{38.3} & \textcolor{gray}{43.5} & \textcolor{gray}{29.4} & \textcolor{gray}{38.8} & \textcolor{gray}{43.5} & \textcolor{gray}{29.4} & \textcolor{gray}{38.8} & \textcolor{gray}{50.3} & \textcolor{gray}{32.7} & \textcolor{gray}{38.8}\\
    & SSUL-M~\citep{SSUL_cha2021ssul}& Resnet-101 & \textbf{42.9} & 17.8 & 34.6 & \textbf{42.9} & 17.7 & 34.5 & 42.8 & 17.5 & 34.4 & 49.1 & 20.1 & 29.8 \\
    & TIKP~\citep{TIKP_yu2024tikp} & Resnet-101 & 37.5 & 17.6 & 30.9 & 41.0 & 19.6 & 33.8 & 42.2 & 20.2 & 34.9 & 48.8 & 25.9 & 33.6 \\
    & Adapter\textsuperscript{*}~\citep{adapter_zhu2024adaptive} & Resnet-101 & 42.6 & 18.0 & 34.5 & \textbf{42.9} & 19.9 & 35.3 & \textbf{43.1} & 23.6 & \textbf{36.7} & \textbf{49.3} & \textbf{27.3} & \textbf{34.7}\\
    & IPSeg (ours) & Resnet-101 & 42.4 & \textbf{22.7} & \textbf{35.9} & 42.1 & \textbf{22.3} & \textbf{35.6} & 41.7 & \textbf{25.2} & 36.3 & 47.3 & 26.7 & 33.6 \\
    \cmidrule{2-15}
    & \textcolor{gray}{Joint} & \textcolor{gray}{Swin-B} & \textcolor{gray}{47.2} & \textcolor{gray}{31.9} & \textcolor{gray}{42.1} & \textcolor{gray}{47.2} & \textcolor{gray}{31.9} & \textcolor{gray}{42.1} & \textcolor{gray}{47.2} & \textcolor{gray}{31.9} & \textcolor{gray}{42.1} & \textcolor{gray}{54.6} & \textcolor{gray}{35.5} & \textcolor{gray}{42.1}\\
    & SSUL-M\textsuperscript{†}~\citep{SSUL_cha2021ssul} & Swin-B & 41.6 & 20.1 & 34.5 & 41.6 & 19.9 & 34.4 & 41.5 & 48.0 & 33.7 & 47.6 & 18.8 & 28.5\\
    % & MicroSeg-M & Swin-B & \\
    & CoinSeg-M\textsuperscript{†}~\citep{coinseg_zhang2023coinseg} & Swin-B & 42.8 & 24.8 & 36.8 & 39.6 & 24.8 & 34.7 & 38.7 & 23.7 & 33.7 & 48.8 & 28.9 & 35.4\\
    & IPSeg (ours) & Swin-B & \textbf{43.2} & \textbf{30.4} & \textbf{38.9} & \textbf{43.0} & \textbf{30.9} & \textbf{39.0} & \textbf{43.8} & \textbf{31.5} & \textbf{39.7} & \textbf{51.1} & \textbf{34.8} & \textbf{40.3}\\
    \bottomrule
    \end{tabular}
    }
    \label{tab:ade_res}
    % \vspace{-5pt}
\end{table*}

% \vspace{-8pt}
\paragraph{Results on Pascal VOC 2012} 
We evaluate IPSeg in various incremental scenarios on Pascal VOC 2012, including standard incremental scenarios (15-5 and 15-1) and long-term incremental scenarios (10-1 and 2-2). As shown in Table~\ref{tab:voc_res}, among the replay-based methods, IPSeg achieves the best results across all incremental scenarios on Pascal VOC 2012 with both ResNet-101 and Swin-B backbones. Notably, in the long-term incremental scenarios 10-1 and 2-2, IPSeg achieves performance gains of $\textbf{4.9}$\% and $\textbf{6.4}$\% over the second-best method, CoinSeg-M, with the same Swin-B backbone.
Meanwhile, the data-free version of IPSeg (denotes as ``IPSeg w/o M'' ) also demonstrates competitive performance, though without specialized designs.

The superior and robust performance of IPSeg is mainly attributed to the reliable role of guidance provided by the image posterior branch.
The image posterior design effectively helps IPSeg avoid catastrophic forgetting and achieve excellent performance on new classes, which often suffer from semantic drift due to separate optimization in new steps.
Additionally, the semantics decoupling design enables IPSeg to better learn foreground classes within each incremental step. These designs bring the improvement of $\textbf{12.3}$\% and $\textbf{6.7}$\% over CoinSeg-M on new classes (11-20) in the 10-1 and 2-2 scenarios, respectively.

% Additionally, we compare the standard version of IPSeg with a data-free variant (denoted as ``IPSeg w/o M'' in Table~\ref{tab:voc_res}) and IPSeg shows competitive performance even without the memory buffer. However, the performance gap between the data-free and data-replay settings highlights the essential role of the memory buffer in enhancing IPSeg's effectiveness.


Furthermore, as illustrated in Figure~\ref{fig:performance_voc}(a), IPSeg experiences less performance degradation compared to previous state-of-the-art methods as the number of incremental steps increases, which indicates that IPSeg has stronger resistance to catastrophic forgetting. This conclusion is further supported by the data in Figure~\ref{fig:performance_voc}(b) and Figure~\ref{fig:performance_voc}(c), which show that IPSeg exhibits minimal performance declines as the incremental process continues. In contrast, other methods only maintain comparable performance during the initial incremental learning step but quickly degrade in subsequent steps due to catastrophic forgetting. This detailed trend of performance decline across steps validates the effectiveness and robustness of IPSeg in resisting forgetting.

% but IPSeg's performance decline is minimal as the incremental process continues. 
% VOC 2-2 includes only 2 initial classes and 18 new classes, and IPSeg starts to outperform other methods from step 3, demonstrating its superior capability in learning new classes.
% ~\cref{fig:performance_voc}(b) and ~\cref{fig:performance_voc}(c) show the results of each learning step in standard incremental scenario 15-1 and long-term incremental (2-2) scenarios. 

% while learning steps increases
% as illustrated in ~\cref{fig:performance_voc}(a), IPSeg exhibits a progressively stronger resistance to catastrophic forgetting as new classes are incrementally learned. General CISS methods, however, just maintain an advantage in basic semantic segmentation performance and exhibit similar degrees of forgetting when entering new steps (shown as similar slopes in ~\cref{fig:performance_voc}(a)). 



% \newpage
% \begin{table}[h]
%     \centering
%         \caption{Comparison with state-of-the-art methods on ADE20K dataset. \textsuperscript{†} denotes the result is reproduced using the official code with Swin-B backbone. \yx{* denotes that this method uses Mask2Former~\citep{mask2former_cheng2022masked} as the segmentation model.}}

%     \resizebox{1.0\linewidth}{!}{
%     \begin{tabular}{c|l|c||ccc|ccc|ccc|ccc}
%     \toprule
%     \multicolumn{2}{c |}{\multirow{2}{*}{Method}}  & \multirow{2}{*}{Backbone} & \multicolumn{3}{c|}{\textbf{ADE 100-5 (11 steps)}} &\multicolumn{3}{c|}{\textbf{ADE 100-10 (6 steps)}}  &\multicolumn{3}{c|}{\textbf{ADE 100-50 (2 steps)}} &\multicolumn{3}{c}{\textbf{ADE 50-50 (3 steps)}} \\
%      \multicolumn{2}{c |}{} & & 0-100 & 101-150 & all & 0-100 & 101-150 & all & 0-100 & 101-150 & all & 0-100 & 101-150 & all \\
%     \midrule
%     \multirow{6}{*}{\rotatebox{90}{Data-free}} 
%     % & LwF-MC & Resnet-101 & \\
%     % & Joint & Resnet-101 & 43.5 & 29.4 & 38.3 & 43.5 & 29.4 & 38.8 & 43.5 & 29.4 & 38.8 & 50.3 & 32.7 & 38.8 \\
%     & \textcolor{gray}{Joint} & \textcolor{gray}{Resnet-101} & \textcolor{gray}{43.5} & \textcolor{gray}{29.4} & \textcolor{gray}{38.3} & \textcolor{gray}{43.5} & \textcolor{gray}{29.4} & \textcolor{gray}{38.8} & \textcolor{gray}{43.5} & \textcolor{gray}{29.4} & \textcolor{gray}{38.8} & \textcolor{gray}{50.3} & \textcolor{gray}{32.7} & \textcolor{gray}{38.8}\\
%     % & ILT & Resnet-101 & 0.1 & 1.3 & 0.5 & 0.1 & 3.1 & 1.1 & 18.3 & 14.4 & 17.0 & 3.5 & 12.9 & 9.7\\
%     & MiB~\citep{MiB_cermelli2020modeling} & Resnet-101 & 36.0 & 5.7 & 26.0 & 38.2 & 11.1 & 29.2 & 40.5 & 17.2 & 32.8 & 45.6 & 21.0 & 29.3\\
    
%     & RCIL~\citep{rcil_zhang2022representation} & Resnet-101 & 38.5 & 11.5 & 29.6 & 39.3 & 17.6 & 32.1 & 42.3 & 18.8 & 34.5 & 48.3 & 25.0 & 32.5\\
%     & IDEC~\citep{idec_zhao2023inherit} & Resnet-101 & 39.2 & 14.6 & 31.0 & 40.3 & 17.6 & 32.7 & 42.0 & 18.2 & 34.1 & 47.4 & 26.0 & 33.1\\
%     & AWT+MiB~\citep{goswami2023attribution} & Resnet-101 & 38.6 & 16.0 & 31.1 & 39.1 & 21.4 & 33.2 & 40.9 & 24.7 & 35.6 & 46.6 & 27.0 & 33.5 \\
    
%     % & PLOP+LGKD~\citep{lgkd_yang2023label} & Resnet-101 & - & - & - & 42.1 & 22.0 & 35.4 & 43.6 & 25.7 & 37.5 & \textbf{49.4} & \textbf{29.4} & \textbf{36.0} \\
%     % & ECLIPSE*~\citep{eclipse_kim2024eclipse} & Resnet-101  & \textbf{43.3} & 16.3 & 34.2 & \textbf{43.4} & 17.4 & 34.6 & \textbf{45.0} & 21.7 & 37.1 & - & - & - \\
%     & \textcolor{blue}{PLOP+LGKD~\citep{lgkd_yang2023label}} & \textcolor{blue}{Resnet-101} & \textcolor{blue}{-} & \textcolor{blue}{-} & \textcolor{blue}{-} & \textcolor{blue}{42.1} & \textcolor{blue}{22.0} & \textcolor{blue}{35.4} & \textcolor{blue}{43.6} & \textcolor{blue}{\textbf{25.7}} & \textcolor{blue}{\textbf{37.5}} & \textcolor{blue}{\textbf{49.4}} & \textcolor{blue}{\textbf{29.4}} & \textcolor{blue}{\textbf{36.0}} \\
% & \textcolor{blue}{ECLIPSE*~\citep{eclipse_kim2024eclipse}} & \textcolor{blue}{Resnet-101} & \textcolor{blue}{\textbf{43.3}} & \textcolor{blue}{16.3} & \textcolor{blue}{34.2} & \textcolor{blue}{\textbf{43.4}} & \textcolor{blue}{17.4} & \textcolor{blue}{34.6} & \textcolor{blue}{\textbf{45.0}} & \textcolor{blue}{21.7} & \textcolor{blue}{37.1} & \textcolor{blue}{-} & \textcolor{blue}{-} & \textcolor{blue}{-} \\
%     & PLOP+NeST~\citep{nest_xie2024early} & Resnet-101 & 39.3 & 17.4 & 32.0 & 40.9 & 22.0 & 34.7 & 42.2 & 24.3 & 36.3 & 48.7 & 27.7 & 34.8 \\
    

%     \midrule
%     \multirow{7}{*}{\rotatebox{90}{Replay}} 
%     % & SDR & Resnet-101 & \\
%     % & PLOP-M & Resnet-101 & \\
%     % & SSUL-M & Resnet-101 & 42.5 & 15.9 & 34.0 & 42.2 & 16.0 & 33.9 & 42.2 & 14.0 & 32.8 & 49.6 & 25.9 & 33.8\\
%     % & MicroSeg-M & Resnet-101 & \textbf{43.6} & 22.4 & 36.6 & \textbf{43.7} & 22.2 & 36.6 & 43.4 & 20.9 & 35.9 & 49.8 & 22.0 & 31.4\\
%     % & PFCSS-M & Resnet-101 & - & - & - & 41.6 & 25.8 & 36.3 & 43.1 & 26.0 & 37.5 & 49.2 & 28.5 & 35.4\\
%     % & IPSeg(ours) & Resnet-101 & \\
%     % \cmidrule{2-15}
%     % & Joint & Swin-B & 47.2 & 31.9 & 42.1 & 47.2 & 31.9 & 42.1 & 47.2 & 31.9 & 42.1 & 54.6 & 35.5 & 42.1\\
%     & SSUL-M~\citep{SSUL_cha2021ssul}& Resnet-101 & 42.9 & 17.8 & 34.6 & 42.9 & 17.7 & 34.5 & 42.8 & 17.5 & 34.4 & 49.1 & 20.1 & 29.8 \\
%     & TIKP~\citep{TIKP_yu2024tikp} & Resnet-101 & 37.5 & 17.6 & 30.9 & 41.0 & 19.6 & 33.8 & 42.2 & 20.2 & 34.9 & 48.8 & 25.9 & 33.6 \\
%     & IPSeg (ours) & Resnet-101 & 42.4 & \textbf{22.7} & \textbf{35.9} & 42.1 & \textbf{22.3} & \textbf{35.6} & 41.7 & 25.2 & 36.3 & 47.3 & 26.7 & 33.6 \\
%     \cmidrule{2-15}
%     & \textcolor{gray}{Joint} & \textcolor{gray}{Swin-B} & \textcolor{gray}{47.2} & \textcolor{gray}{31.9} & \textcolor{gray}{42.1} & \textcolor{gray}{47.2} & \textcolor{gray}{31.9} & \textcolor{gray}{42.1} & \textcolor{gray}{47.2} & \textcolor{gray}{31.9} & \textcolor{gray}{42.1} & \textcolor{gray}{54.6} & \textcolor{gray}{35.5} & \textcolor{gray}{42.1}\\
%     & SSUL-M\textsuperscript{†}~\citep{SSUL_cha2021ssul} & Swin-B & 41.6 & 20.1 & 34.5 & 41.6 & 19.9 & 34.4 & 41.5 & 48.0 & 33.7 & 47.6 & 18.8 & 28.5\\
%     % & MicroSeg-M & Swin-B & \\
%     & CoinSeg-M\textsuperscript{†}~\citep{coinseg_zhang2023coinseg} & Swin-B & 42.8 & 24.8 & 36.8 & 39.6 & 24.8 & 34.7 & 38.7 & 23.7 & 33.7 & 48.8 & 28.9 & 35.4\\
%     & IPSeg (ours) & Swin-B & \textbf{43.2} & \textbf{30.4} & \textbf{38.9} & \textbf{43.0} & \textbf{30.9} & \textbf{39.0} & \textbf{43.8} & \textbf{31.5} & \textbf{39.7} & \textbf{51.1} & \textbf{34.8} & \textbf{40.3}\\
%     \bottomrule
%     \end{tabular}
%     }
%     \label{tab:ade_res}
%     \vspace{-5pt}
% \end{table}









\begin{table*}[t]
    \caption{Ablation on different label choices to incrementally train the image posterior branch.}
    \centering
    \resizebox{0.97\linewidth}{!}{
    \begin{tabular}{l||c|c||ccc|ccc|ccc}
    \toprule
    \multirow{2}{*}{Methods} & \multicolumn{2}{c||}{Labels} & \multicolumn{3}{c|}{\textbf{VOC 15-5 (2 steps)}} &\multicolumn{3}{c|}{\textbf{VOC 10-1 (11 steps)}} &\multicolumn{3}{c}{\textbf{VOC 2-2 (10 steps)}} \\
    & \(\mathcal{C}_{1:t-1}\) & \(\mathcal{C}_{t}\) & 0-15 & 16-20 & all & 0-10 & 11-20 & all & 0-2 & 3-20 & all \\
    \midrule
    Part-GT & \XSolidBrush & GT & 83.3 & 72.8 & 80.8 & 79.3 & 74.5 & 77.0 & 72.6 & 69.4 & 69.8\\
    Pseudo (ours) & PL & GT & 83.3 & 73.3 & 80.9 & 80.3 & 76.7 & 78.6 & 73.1 & 72.3 & 72.4 \\
    Full-GT & GT & GT & 83.2 & 73.8 & 81.0 & 80.1 & 78.0 & 79.2 & 75.2 & 74.6 & 74.8 \\
    \bottomrule
    \end{tabular}
    }
    \label{tab:impact_image-level_pseudo_label}
    % \vspace{-10pt}
\end{table*}

% \vspace{-3pt}
\paragraph{Results on ADE20K}
We also conduct a comparison between IPSeg and its competitors under different incremental scenarios on the more challenging ADE20K dataset with two backbones. As shown in Table~\ref{tab:ade_res}, IPSeg consistently achieves performance advantages compared with other replay-based methods, which is similar to those observed on Pascal VOC 2012. Notably, IPSeg with Swin-B backbone demonstrates more significant improvements over its competitors across all incremental scenarios on the ADE20K dataset, with the smallest improvement of $\textbf{2.1}\%$ in the 100-5 scenario and the largest improvement of $\textbf{6.0}\%$ in the 100-50 scenario. The superior performance on the more realistic and complex ADE20K dataset further demonstrates the effectiveness and robustness of IPSeg. 
% Consistently, IPSeg w/o M shows its competitive performance in data-free cases.

% \begin{table}[h]
%     \centering
%     \caption{Overall ablation Studies for IPSeg on VOC 15-1.}
%     \begin{tabular}{c c c | ccc}
%     \toprule
%     \multirow{2}{*}{IP} & \multirow{2}{*}{KD} & \multirow{2}{*}{NS} & \multicolumn{3}{c}{\textbf{VOC 15-1 (6 steps)}} \\
%     & & & 0-15 & 16-20 & all \\
    
%     \midrule
%     \XSolidBrush & \XSolidBrush & \XSolidBrush  & 78.8 & 49.7 & 71.9 \\
%     \Checkmark & \XSolidBrush & \XSolidBrush & 79.4 & 69.6 & 77.0 \\
%     \XSolidBrush & \Checkmark & \XSolidBrush& 83.1 & 65.1 & 78.8 \\
%     \XSolidBrush & \Checkmark & \Checkmark  & 83.4 & 69.6 & 80.1  \\
%     \Checkmark & \Checkmark & \XSolidBrush  & 83.4 & 74.7 & 81.3  \\
    
%     \Checkmark & \Checkmark & \Checkmark  & \textbf{83.6} & \textbf{75.1} & \textbf{81.6}  \\
%     \bottomrule
%     \end{tabular}   
%     \label{tab:total_ablation}
% \end{table}

\subsection{Ablation Study}
\label{sec4-3:ablations}

% \input{sections/ablation_exps}

\begin{table}[t]
    \caption{Overall ablation study for IPSeg on VOC 15-1.}
    \centering
    \resizebox{0.93\linewidth}{!}{
    \begin{tabular}{c c c | ccc}
        \toprule
        \multirow{2}{*}{IP} & \multirow{2}{*}{SD} & \multirow{2}{*}{NF} & \multicolumn{3}{c}{\textbf{VOC 15-1 (6 steps)}} \\
        & & & 0-15 & 16-20 & all \\
        
        \midrule
        \XSolidBrush & \XSolidBrush & \XSolidBrush  & 78.8 & 49.7 & 71.9 \\
        \Checkmark & \XSolidBrush & \XSolidBrush & 79.4 & 69.6 &\reshll{77.0}{5.1} \\
        \XSolidBrush & \Checkmark & \XSolidBrush& 83.1 & 65.1 &\reshll{78.8}{6.9} \\
        % \XSolidBrush & \Checkmark & \Checkmark  & 83.4 & 69.6 &\reshll{80.1}{8.2}  \\
        \Checkmark & \Checkmark & \XSolidBrush  & 83.4 & 74.7 &\reshll{81.3}{9.4}  \\
        
        \Checkmark & \Checkmark & \Checkmark  & \textbf{83.6} & \textbf{75.1} & \reshl{81.6}{9.7}  \\
        \bottomrule
    \end{tabular}
    }
    \label{tab:abs_overall}
    \vspace{-10pt}
\end{table}





% \paragraph{Ablation study on IPSeg} We analyze the effect of proposed components in IPSeg under the 15-1 setting of Pascal VOC 2012 dataset with Swin-B backbone. We ablate the main parts including Image Posterior (\textbf{IP}) and Knowledge Decoupling (\textbf{KD}), together with the design of label Re-Assignment (\textbf{RA}) in KD. As shown in ~\cref{tab:hyper_para}(a), the second row shows \textbf{IP} brings significant improvement on new classes which demonstrates its effectiveness against performance degradation caused by semantic drift and separate optimization. The third row demonstrates excellent ability of learning foregrounds within steps from \textbf{KD}. While the fourth row demonstrates IPSeg's outstanding performance on both old and new classes when combining \textbf{IP} and \textbf{KD}. And the last row shows the improvement design  \textbf{RA} further brings performance gains based on \textbf{KD}, making IPSeg achieves better results.

\paragraph{Ablation on IP branch} The image posterior (IP) branch is trained incrementally but faces challenges due to the lack of labels for old classes. To address this issue, we employ image-level pseudo-label \(\tilde{\mathcal{Y}}^{m,t}_i\) (PL) instead of directly using the partial ground truth label \(\mathcal{Y}^{m,t}_i\), providing comprehensive supervision at the risk of introducing noise due to the inconsistencies between previous heads predictions and current training labels. As shown in Table~\ref{tab:impact_image-level_pseudo_label}, our method achieves significant improvement compared to using only partial ground truth (Part-GT),
% There are still potential to gain further improvements from using full ground truth (Full-GT).
and narrows the gap with the upper bound (Full-GT).
This indicates that using \(\tilde{\mathcal{Y}}^{m,t}_i\) is an efficient trade-off, where the benefits of additional supervision from pseudo labels outweigh the potential noise. With this training design, the image posterior branch helps IPSeg effectively mitigate separate optimization and shows superior performance.

\vspace{-8pt}
\paragraph{Ablation on proposed components} 
% We analyze the effect of the proposed components in IPSeg under the 15-1 setting of the Pascal VOC 2012 dataset using the Swin-B backbone. We ablate the main parts, including Image Posterior (\textbf{IP}), Knowledge Decoupling (\textbf{KD}), and the design of label Re-Assignment (\textbf{RA}) in KD. As shown in ~\cref{tab:hyper_para}(a), the second row indicates that \textbf{IP} brings significant improvement to new classes. Benefiting from \textbf{IP}'s ability to maintain consistency between tasks, the reliable Image Posterior Guidance it provides effectively prevents model performance degradation caused by separate optimization and semantic drift, as shown in ~\cref{fig:vis_intro}(a). The third row illustrates the excellent ability of \textbf{KD} in learning foregrounds at each step. \textbf{KD} consists of two components: a permanent learning branch and a temporary learning branch, which decouple noisy information into static and dynamic groups, allowing the model to learn the corresponding knowledge within each step. The fourth row showcases IPSeg's outstanding performance on both old and new classes when combining \textbf{IP} and \textbf{KD}. Finally, the last row shows that the \textbf{RA} design further enhances performance based on \textbf{KD}, leading to even better results for IPSeg.
We analyze the effect of the components in IPSeg, including Image Posterior (\textbf{IP}), Semantics Decoupling (\textbf{SD}), and the Noise Filtering (\textbf{NF}) in SD. All ablations are implemented in Pascal VOC 2012 using the Swin-B backbone. As shown in Table~\ref{tab:abs_overall}, the second row indicates that \textbf{IP} brings significant improvement to new classes. Benefiting from \textbf{IP}'s ability to align probability scales between different task heads, the reliable guidance prevents model performance from degradation caused by \textit{separate optimization}. The third row shows the excellent ability of \textbf{SD} in learning foreground targets at each step by effective decoupling. 
% Using the key components IP and SD together further improves performance to \(81.3\%\) as the fourth row shows.
% \textbf{SD} consists of a permanent branch and different temporary branches, and decouples noisy information into stable and dynamic groups, allowing the model to individually learn the decoupled semantics. 
The fourth and fifth rows demonstrate IPSeg's outstanding performance on both old and new classes using \textbf{IP} and \textbf{SD} together. More ablation studies and visualization results for IPSeg are provided in the appendix. 
% These results validate the effectiveness of every part in IPSeg.
% And \textbf{NF} serves as a part of \textbf{SD} further improves IPSeg.
% The last row shows that \textbf{NF}additional trick, further enhances performance. 



% \yx{\paragraph{Impact of image-level pseudo label \(\tilde{\mathcal{Y}}_k\)} As mentioned in ~\cref{sec3-3}, IP branch is trained in an incremental manner but faces challenges due to the absence of labels for old classes. Therefore, instead of directly using the label \(\mathcal{Y}_k\), we choose the pseudo label \(\tilde{\mathcal{Y}}_k\) for richer supervision though it will inevitably introduce noises when the inconsistency appears among the prediction of previous heads and the label in current datasets. To assess the effectiveness of using image-level pseudo labels (PL) as global guidance, we conduct comparative experiments with and without them. As shown in ~\cref{tab:impact_image-level_pseudo_label}, our method effectively boosts model performance, achieving up to a $\textbf{2.6}$\% performance gain, compared to only using partial ground truth (Part-GT). Besides, it is worth noting that using full ground truth (Full-GT) can achieve even further performance gains. According to these observations, using pseudo label \(\tilde{\mathcal{Y}}_k\) is an efficient trade-off where the benefits of additional supervision outweigh the potential drawbacks of label inaccuracies. 
% }


% 
% \yx{
% \paragraph{Impact of Memory Buffer} IPSeg is a method specifically designed for data-replay scenarios, with the Image Posterior branch being highly dependent on a memory buffer. To assess the impact of the memory buffer on model performance, we conduct experiments using a data-free version of IPSeg (denoted as IPSeg w/o M), and the results are presented in \cref{tab:impact_mem}. It is noteworthy that IPSeg still exhibits competitive performance even in the absence of the memory buffer. However, the performance gap observed between the data-free and data-replay settings underscores the crucial role that the memory buffer plays in the effectiveness of IPSeg.
% }

