\newpage
\section{Appendix}
\subsection{Symbols and Explanations}
Table~\ref{tab:symbol} provides key symbols used in our paper along with their explanations to facilitate a better understanding.
\begin{table}[h]
    \caption{Symbols and explanations}
    \centering
    \renewcommand{\arraystretch}{1.3}
    \begin{tabular}{c c | l}
        \toprule
        \multicolumn{2}{c|}{Symbol} & Explanations\\
        \midrule
        \multirow{6}{*}{\rotatebox{90}{Model architecture}} 
        &\(f_t\) & Model of task $t$. \\
        &\(h_{\theta}\) & The backbone extracting features. \\
        &\(\phi_{1:t}\) & All segmentation heads, outputting the final results. \\
        &\(\phi_t\) & The head of task $t$, representing the temporary branch. \\
        &\(\phi_p\) & The permanent branch. \\
        &\(\psi\) & The image posterior branch. \\
        \midrule
        \multirow{8}{*}{\rotatebox{90}{Semantic concepts}} 
        &\(\mathcal{C}_t\) & The target classes set of task $t$.\\
        &\(\mathcal{C}_{1:t-1}\) & The old classes set. \\
        &\(c'_b\) & The pure background. \\
        &\(c'_u\) & Unknown foreground. \\
        & \(c_i\) & The ignored region that does not participate in loss calculation.\\
        &\(c_f\) & The foreground regions that do not belong to target classes \(\mathcal{C}_t\). \\
        &\(c_u\) & Unknown classes defined in previous methods, consisting of \(\mathcal{C}_{1:t-1}\) and \(c'_u\).  \\
        &\(c_b\) & The background defined in previous methods, consisting of \(c'_b\) and \(c_u\).\\
        \midrule
        \multirow{8}{*}{\rotatebox{90}{Data and label}} 
        &\(\mathcal{D}_t\) & Training dataset of current incremental task $t$. \\
        &\(\mathcal{M}\) & Memory buffer, with fixed size of 100 for Pascal VOC and 300 for ADE20K. \\
        &\(x^t_i\) & The $i$-th image in \(\mathcal{D}_t\). \\
        &\(y^t_i\) & The pixel annotations of \(x^t_i\). \\
        &\(x^{m,t}_i\) & The mixed data selected from  \(\mathcal{M}\) and \(\mathcal{D}_t\). \\
        &\(\tilde{\mathcal{Y}}_{i}^{m,t}\) & The image-level pseudo-label of \(x^{m,t}_i\). \\
        &\(\tilde{y}_{i}^{p}\) & Label assigned to the permanent branch \(\phi_p\). \\
        &\(\tilde{y}_{i}^{t}\) & Label assigned to the temporary branch \(\phi_t\). \\
        \bottomrule
    \end{tabular}
    \label{tab:symbol}
\end{table}

% \newpage/

\subsection{Observation}

\paragraph{Visualization of separate optimization}As shown in Figure~\ref{fig:vis_so}, to illustrate the inconsistent outputs caused by \textit{separate optimization}, we select three pairs of similar classes from Pascal VOC 2012: ``cow'' and ``horse'', ``bus'' and ``car'', ``sofa'' and ``chair'', and split them into two separate groups for learning. Sequence B follows a reverse learning order compared to Sequence A, and the goal is to examine the model's final predictions with different learning sequence. Columns A and B are the models' final predictions of sequence A and sequence B respectively. These visualizations indicate that the earlier head of the model tends to produce high scores for certain classes, regardless of the learning order, suggesting that \textit{separate optimization} causes a persistent bias towards the classes learned first.

\begin{figure}[h]
    \centering
    \includegraphics[width=0.75\linewidth]{figs/iclr_rebuttal_pdf_yuxiao/separate.pdf}
    \caption{The visualization of separate optimization. \textit{Sequence A}: first learn ``cow'', ``bus'', ``sofa'' in step 1, then ``horse'', ``car'', ``chair'' in step 2. \textit{Sequence B}: first learn ``horse'', ``car'', ``chair'' in step 1, then ``cow'', ``bus'', ``sofa'' in step 2.
    % Columns A and B are the models' predictions of reverse incremental tasks \textit{sequence A} and \textit{sequence B}, respectively.
    }
    \label{fig:vis_so}
    \vspace{-5pt}
\end{figure}

\paragraph{Impact of IPSeg on separate optimization}To validate the impact of IPSeg on \textit{separate optimization}, we calculate the average probability distribution of the incorrect prediction area (red box in the image) as depicted in Figure~\ref{fig:distribution}. SSUL-M misclassifies the little sheep as cow with abnormal probability distribution. In contrast,  IPSeg utilizes image posterior guidance to produce more accurate and harmonious prediction. Compared to previous works, IPseg maintains a harmonious and realistic probability distribution more similar to that of the theoretical upper bound, Joint-Training (Joint), demonstrating its superior capability in dealing with \textit{separate optimization}.

\begin{figure}[t]
    \centering
    \includegraphics[width=1\linewidth]{figs/iclr_rebuttal_pdf_yuxiao/separate2.pdf}
    \caption{The probability distributions for SSUL-M, IPSeg, and Joint-Training (Joint) in the regions of incorrect predictions. Class indexes ``10'' and ``17'' represent ``cow'' and ``sheep'' respectively.}
    \label{fig:distribution}
\end{figure}






\paragraph{Visualization of semantics decoupling} Figure~\ref{fig:vis_KD} is the semantics decoupling illustration for image \(x^t_i\). In this case, the current classes \(\mathcal{C}_t\) ``person'' is provided with ground truth as \(y^t_i\). The foreground classes ``sofa'' and ``cat'', however, are unknown without ground truth. IPSeg uses a saliency map to locate the current unknown object ``sofa'' and ``cat'' as other foregrounds \(c_f\) and further utilizes pseudo label to distinguish ``sofa'' as unknown foreground \(c'_u\). It is worth noting that the regions of ``person'' and ``cat'' belong to the ignored regions \(c_i\) that do not participate in loss calculation. In this way, the remaining region is labeled as ``pure" background \(c'_b\). The ``pure" background \(c'_b\) and unknown foreground \(c'_u\) are considered as static and permanent concepts. The target classes \(\mathcal{C}_t\) with ground truth \(y^t_i\) and other foreground \(c_f\) are considered as dynamic and temporary concepts. 

\begin{figure}[t]
    \centering
    \includegraphics[width=0.6\linewidth]{figs/iclr_rebuttal_pdf_yuxiao/decoupling.pdf}
    \caption{Semantics decoupling strategy}
    \label{fig:vis_KD}
    \vspace{-20pt}
\end{figure}



% \newpage

\paragraph{Qualitative analysis of IPSeg} Figure~\ref{fig:vis_main} presents a qualitative analysis of IPSeg compared with SSUL-M and CoinSeg-M. Visualization results are from each incremental step in the VOC 2-2 scenario. The results in rows 1, 3, and 5 demonstrate that both SSUL-M and CoinSeg-M mistakenly predict ``horse'' as ``cow'' at step 6, while IPSeg correctly identifies ``horse''. 


\begin{figure}[H]
    \centering
    \includegraphics[width=0.9\linewidth]{figs/iclr_rebuttal_pdf_yuxiao/vis.pdf}
    \caption{Qualitative analysis of IPSeg on Pascal VOC 2012. Texts and bounding boxes in white indicate incorrect class predictions starting from the corresponding incremental step. Color-shaded boxes with classes and indexes represent the learned classes in the corresponding learning steps.}
    \label{fig:vis_main}
    \vspace{-5pt}
\end{figure}

\vspace{-10pt}
In rows 2, 4, and 6, IPSeg consistently predicts the old class ``chair'', whereas SSUL-M predicts ``sofa'' as ``table'' at step 6, and both SSUL-M and CoinSeg-M mistake ``chair'' for ``sofa'' at step 8. These visualization results reveal that IPSeg not only achieves excellent learning plasticity but also maintains strong memory stability.

















% \newpage

\subsection{Additional Results and Analysis}


\paragraph{Evaluation on image-level predictions} To investigate the ability to resist catastrophic forgetting of the image posterior (IP) branch and the segmentation branch, we evaluate the image level accuracy performance of the base 15 classes using the IP branch and the segmentation branch at each step on Pascal VOC 15-1 as shown in Table~\ref{tab:IP_base_acc}. ``IP" refers to the image-level accuracy of the IP branch, ``Pixel" refers to the image-level accuracy of the segmentation branch, where class $\mathcal{C}$ exists if a pixel is predicted as $\mathcal{C}$. ``Pixel+IP" denotes the final result of IPSeg. The ablation shows that: the image-level performance suffers less forgetting than the segmentation, and our method shows similar property against forgetting with the help of IP.

\begin{table}[h]
    \centering
    \caption{The image-level accuracy of IP branch and the segmentation branch on 15 base classes.}
    \begin{tabular}{l|c c c c c c}
         \toprule
         ACC (\%) & step 0 & step 1  & step 2 & step 3 & step 4 & step 5 \\
         \midrule
         IP & 87.44 & 86.41 & 86.99 & 86.82 & 86.86 & 86.29 \\
         Pixel & 88.17 & 86.42 & 86.30 & 85.43 & 84.84 & 84.70 \\
         Pixel+IP & 93.07 & 92.24 & 92.41 & 91.93 & 91.95 & 91.02 \\
         \bottomrule
    \end{tabular}
    \label{tab:IP_base_acc}
\end{table}

We also present the image-level accuracy on all seen classes at each step to analyze their performance on both old classes and new classes in Table~\ref{tab:IP_all_acc}. For the segmentation branch, the image-level accuracy of it on all seen classes gradually degrades after learning new classes, performing worse than its accuracy on base classes. This indicates the segmentation branch performs poorly on new classes, which is consistent with our description about separate optimization. In contrast, the IP branch experiences less deterioration from separate optimization and help our method maintain a good balance between retaining old knowledge and learning new knowledge. 

\begin{table}[h]
    \centering
    \caption{The image-level accuracy of IP branch and the segmentation branch on all seen classes.}
    \begin{tabular}{l|c c c c c c}
         \toprule
         ACC (\%) & step 0 & step 1  & step 2 & step 3 & step 4 & \textbf{step 5 (Final)} \\
         \midrule
         IP & 87.44 & 82.54 & 81.14 & 81.32 & 82.09 & \textbf{82.34} \\
         Pixel & 88.17 & 83.56 & 82.29 & 78.23 & 77.60 & \textbf{76.57} \\
         Pixel+IP & 93.07 & 90.05 & 90.13 & 87.30 & 87.68 & \textbf{88.03} \\
         \bottomrule
    \end{tabular}
    \label{tab:IP_all_acc}
\end{table}



\iffalse
\yx{
\paragraph{Evaluation on image posterior branch} Image posterior (IP) branch is trainable during different incremental steps, and it will undoubtedly suffer from catastrophic forgetting. To evaluate the effectiveness of IP branch, we conduct a quantitative evaluation of it as shown in Table~\ref{tab:IP_branch}. We train IP branch with a multi-label classification task in three scenarios and report the precision, recall, and F1  results: ``Seq" refers to training the IP branch sequentially without any additional tricks, indicating the worst case suffering from catastrophic forgetting, ``Ours" refers to the training setting used in our paper, and ``Joint" refers to training with all task data jointly serving as the upper bound. It is obvious that IP branch suffers little forgetting. We achieve this performance mainly owing to two specific settings: 
\begin{itemize}
    \item Mixed training data: IP branch uses data from both the memory buffer and the current task dataset for training. Compared to fine-grained task heads, it is easier for IP branch to learn image-level knowledge from memory buffer.
    \item Image-level pseudo-label for supervision: IPSeg introduces image-level pseudo-label on past classes as supervision to mitigate catastrophic forgetting challenge. The ablation result in Table~\ref{tab:impact_image-level_pseudo_label} partially reflects the effectiveness of this design.
\end{itemize}

\begin{table}[h]
    \centering
    \color{blue}
    \caption{The performance of image posterior branch on different settings.}
    \begin{tabular}{c|c c c}
         \toprule
          Method & Precision (\%) & Recall (\%)  & F1 (\%) \\
         \midrule
         Seq & 55.62 & 24.67 & 23.78\\
         Ours & 78.28 & 87.03 & 80.68 \\
         Joint & 89.96 & 90.00 & 89.89 \\
         \bottomrule
    \end{tabular}
    \label{tab:IP_branch}
\end{table}
}
\fi


\paragraph{Evaluation on model parameters, training and inference costs} We provide a comprehensive analysis of the model parameters, training, and inference costs as shown in Table~\ref{tab:para_costs}. We test and report the results of IPSeg, SSUL-M and CoinSeg-M with Swin-B on the VOC 15-1 setting. We set \textit{image\_size=512x512, epochs=50, and batch\_size=16} in training and \textit{image\_size=512x512} for inference test. All results are run on RTX 3090 GPU.
\begin{itemize}
    \item \textbf{Model Parameters:} Using the thop tool, we analyze and compare the trainable parameters for these methods. The sizes of increased parameters in them are close, with average $3.84M$ per step. Additionaly, IPSeg has $29.72M$ parameters more than SSUL due to the additional image posterior branch.
    \item \textbf{Training:} Due to the introduced image posterior branch, IPSeg needs more training cost compared with SSUL-M but  less than CoinSeg-M. 
    \item \textbf{Inference:} The inference speed of IPSeg ($27.3$ FPS) is slightly lower than SSUL-M ($33.7$ FPS) and similar to CoinSeg-M ($28.2$ FPS). Due to the proposed image posterior branch, the model's floating-point operations ($137.1$ GFLOPs) are higher than the baseline ($94.9$ GFLOPs), and with an approximately $1$ GB increase in GPU usage. Note that the increase in FLOPs mainly stems from IPSeg’s use of image-level predictions to guide final outputs. Specifically, IPSeg broadcasts image-level predictions to match the shape of pixel-level logits and combines them through element-wise multiplication, which are inherently parallelizable and can be optimized and accelerated by GPUs, ensuring that the inference speed remains largely unaffected.
\end{itemize}

Overall, IPSeg introduces an additional image posterior branch with slight increases in model parameters, training cost, and inference but brings great performance improvement. It is a worthwhile trade-off between performance and cost.


\begin{table}[h]
    \centering
    \setlength{\tabcolsep}{3pt}  % 列间距3pt
    \caption{Comparison of IPSeg with baseline on model parameters, training and inference costs. }
    \resizebox{1.0\linewidth}{!}{
    \begin{tabular}{l||cccccc|cc|rcc}
        \toprule
         \multirow{2}{*}{Method} & \multicolumn{6}{c|}{Incremental Steps} & \multicolumn{2}{c|}{Training}& \multicolumn{3}{c}{Inference}\\
         & 0 & 1 & 2 & 3 & 4 & 5 & Time & GPU usage & FPS & FLOPs & GPU usage \\
         \midrule
         IPseg & 135.92 M & 139.76 M & 143.60 M & 147.66 M & 151.28 M & 155.12 M & 9h 14min & 21.1G & 27.3 & 137.1G & 6.2G \\
         SSUL-M & 106.20 M & 110.03 M & 113.89 M & 117.95 M & 121.56 M & 125.40 M & 7h 13min & 19.4G & 33.7 & 94.9G & 5.3G \\
         CoinSeg-M & 107.02 M & 111.15 M & 115.29 M & 119.42 M & 123.55 M & 127.68 M & $>$ 15h & 21.3G & 28.2 & 96.3G & 5.6G \\
         \bottomrule
    \end{tabular}
    }
    \label{tab:para_costs}
\end{table}


\iffalse
\yx{
\paragraph{Ablation study on salient map} The salient map is effective for Pascal VOC 2012, but not as useful for ADE20K as we expected. Specifically, the use of saliency map on ADE20K presents challenges in correctly identifying target regions, especially for objects that are not at the center of the image and cover large areas. To evaluate the quality of salient map, we conduct ablation on the ADE20K 100-10 task with the following settings as shown in Table~\ref{tab:SAM}: ``w/o Sal" uses no saliency map supervision, ``w/ Sal" uses saliency map supervision as we do in paper, and ``w/ SAM" uses saliency map extracted by SAM model. We use the official Grounded SAM ~\citep{g_sam_ren2024grounded, sam_kirillov2023segment} code with all classes in ADE20K used as text prompts to extract corresponding masks. Additionally, we also report performance differences on ``Thing" and ``Stuff" classes defined in ADE20K panoptic segmentation~\citep{ade20k_thing_and_stuff_zhou2019semantic} to investigate the bias of saliency map on different semantic regions. Basically, we find the following conclusion:


\begin{itemize}
    \item Using the default saliency map supervision to implement knowledge decoupling strategy, IPSeg gets a performance improvement by $+1.6$\% mIoU. And using saliency extracted by SAM further improves IPSeg performance by $0.7$\% mIoU.
    \item The default saliency maps perform well in identifying ``Things" classes but struggle with ``Stuff" classes, resulting in a performance gain of $+1.9$\% on ``Things" classes but merely $+0.1$\% on ``Stuff" classes. Furthermore, the SAM-based saliency maps provide better supervision for both ``Things" and ``Stuff" classes, with improvements of $+0.6$\% on ``Things" and $+1.1$\% on ``Stuff" compared to ``w/ Sal".
\end{itemize}

\begin{table}[h]
    \centering
    \color{blue}
    \caption{Ablation study on salient map of IPSeg on ADE20K 100-10 tasks.}
    \begin{tabular}{l|c c c| c c}
         \toprule
          Method & 0-100 & 101-150 & all & Things & Stuff \\
         \midrule
         w/o Sal&42.1&28.0&37.4&37.2&37.5 \\
         w/ Sal&43.0&30.9&39.0&39.1 &37.6\\
         w/ SAM&43.6&31.8&39.7 &39.7& 38.7\\
         \bottomrule
    \end{tabular}
    \label{tab:SAM}
\end{table}
}
\fi


\paragraph{The details of efficient data storage in memory buffer}  
% For raw data, IPSeg directly stores the image paths in a JSON file, as done in previous works~\citep{SSUL_cha2021ssul,microseg_zhang2022mining,coinseg_zhang2023coinseg}. For pixel-level labels, instead of storing full-class annotations (with a data type of \textit{uint8} ) as prior approaches, IPSeg only stores the salient mask, where the background and foreground are labeled as $0$ and $1$, respectively (with a data type of \textit{bool} ). Theoretically, the storage space could be reduced to $1/8$. For image-level labels, IPSeg stores the class labels of the images as arrays in the JSON file, where $1$ indicates the presence of a class and $0$ indicates its absence, and the memory cost for this is negligible.

For raw data, IPSeg directly stores the image paths in a JSON file, as done in previous works~\citep{SSUL_cha2021ssul,microseg_zhang2022mining,coinseg_zhang2023coinseg}. For image-level labels, IPSeg stores the class labels of the images as arrays in the same JSON file with multi-hot encoding, where $1$ indicates the presence of a class and $0$ indicates absence. The memory cost for this is negligible. For pixel-level labels, instead of storing full-class annotations (with a data type of \textit{uint8} ) as prior approaches, IPSeg only stores the salient mask, where the background and foreground are labeled as $0$ and $1$, respectively (with a data type of \textit{bool} ). Theoretically, the storage space could be reduced to $1/8$. 
   


\paragraph{Ablation study on memory buffer} Since IPSeg is designed for data-replay scenarios, the IP branch heavily relies on a memory buffer. To evaluate the impact of the memory buffer on performance, we compare the standard version of IPSeg with a data-free variant (denoted as IPSeg w/o M). As shown in Table~\ref{tab:impact_mem}, IPSeg demonstrates competitive performance even without the memory buffer. However, the performance gap between the data-free and data-replay settings highlights the essential role of the memory buffer in enhancing IPSeg's effectiveness.

\begin{table}[h]
    \centering
    \caption{Comparison with other methods in data-free version using Swin-B backbone.}
    \resizebox{0.75\linewidth}{!}{
    \begin{tabular}{l|ccc|ccc|ccc|ccc}
    \toprule
    \multirow{2}{*}{Method} & \multicolumn{3}{c|}{\textbf{VOC 15-5 (2 steps)}} &\multicolumn{3}{c|}{\textbf{VOC 15-1 (6 steps)}}  &\multicolumn{3}{c|}{\textbf{VOC 10-1 (11 steps)}} &\multicolumn{3}{c}{\textbf{VOC 2-2 (10 steps)}} \\
    & 0-15 & 16-20 & all & 0-15 & 16-20 & all & 0-10 & 11-20 & all & 0-2 & 3-20 & all \\
    \midrule
    SSUL & 79.7 & 55.3 & 73.9 & 78.1 & 33.4 & 67.5 & 74.3 & 51.0 & 63.2 & 60.3 & 40.6 & 44.0 \\
    MicroSeg & \textbf{81.9} & 54.0 & 75.2 & 80.5 & 40.8 & 71.0 & 73.5 & 53.0 & 63.8 & 64.8 & 43.4 & 46.5 \\
    IPSeg w/o M & 81.4 & \textbf{62.4} & \textbf{76.9} & \textbf{82.4} & \textbf{52.9} & \textbf{75.4} & \textbf{80.0} & \textbf{61.2} & \textbf{71.0} & \textbf{72.1} & \textbf{64.5} & \textbf{65.5} \\
    \midrule
    IPSeg w/ M & 83.3 & 73.3 & 80.9 & 83.5 & 75.1 & 81.5 & 80.3 & 76.7 & 78.6 & 73.1 & 72.3 & 72.4\\
    \bottomrule
    \end{tabular}
    }
    \label{tab:impact_mem}
    \vspace{-10pt}
\end{table}



\paragraph{Ablation study for hyper-parameters: weight terms of loss.} We conduct an ablation study on the two weight terms \(\lambda_1\) and \(\lambda_2\), testing values of 0.1, 0.25, 0.5, 0.75, and 1.0. The results are shown in Table~\ref{tab:loss_weight}. It is obvious that the setting of \(\lambda_1=0.5\) and \(\lambda_2=0.5\) achieves the best performance, which is the default value of IPSeg.

\paragraph{Ablation study of hyper-parameters} Table~\ref{tab:ablation_hyperpara}(a) and (b) illustrate the effects of hyper-parameters: memory size \(\left|\mathcal{M}\right|\), the strength of noise filtering \(\alpha_{\text{\tiny NF}}\), and background compensation \(\alpha_{\text{\tiny BC}}\), which shows that IPseg is not sensitive to the value of \(\alpha_{\text{\tiny NF}}\) and \(\alpha_{\text{\tiny BC}}\)
and we set the default values for these parameters to  \(\left|\mathcal{M}\right| = 100\), \(\alpha_{\text{\tiny NF}}=0.4\) and \(\alpha_{\text{\tiny BR}}=0.9\).

\begin{table}[h]
    \centering
    \caption{Ablation Studies on Pascal VOC 15-1 task for hyper-parameters: weight terms of loss,  \(\lambda_1\) and \(\lambda_2\).}
    \begin{tabular}{c|c c c c c}
         \toprule\(\)
         \(\lambda_1\) \textbackslash ~\(\lambda_2\) & 0.1 & 0.25 & 0.5 & 0.75 & 1.0 \\
         \midrule
         0.1 & 79.2 & 80.3 & 80.4 & 80.8 & 78.0 \\
         0.25 & 79.8 & 80.3 & 81.1 & 81.0 & 80.9 \\
         0.5 & 80.3 & 80.9 & \textbf{81.5} & 81.2 &  80.9 \\
         0.75 & 81.1 & 81.3 & 81.1 & 81.0 & 81.0 \\
         1.0 & 80.8 & 81.3 & 81.1 & 81.2 & 80.1 \\
         \bottomrule
    \end{tabular}
    \label{tab:loss_weight}
\end{table}

\begin{table}[t]
    \centering
    \caption{(a): Ablation studies for hyper-parameters: memory size \(|M|\), the ratio of noise filtering \(\alpha_{\text{\tiny NF}}\). (b): Ablation studies for hyper-parameters: the ratio of background compensation \(\alpha_{\text{\tiny BC}}\).}
    \begin{minipage}{0.5\textwidth}
        \centering
        \resizebox{0.8\textwidth}{!}{
            \begin{tabular}{c||c|ccc}
            \toprule
             & value  &  0-15 & 16-20 & all \\
            \midrule
            {\multirow{3}{*}{\(|M|\)}} & 50 & 83.4 & 72.3 & 80.8 \\
            & 100 & 83.5 & 75.1 & 81.5 \\
            & 200 & 83.5 & 75.5 & 81.7 \\
            \midrule
            {\multirow{5}{*}{\(\alpha_{\text{\tiny NF}}\)}}
            % & 0 & 82.8 & 74.2 & 80.7 \\
            & 0.2 & 83.5 & 75.0 & 81.4 \\
            & 0.4 & \textbf{83.6} & \textbf{75.1} & \textbf{81.6} \\
            & 0.6 & 83.4 & 74.6 & 81.3 \\
            & 0.8 & 83.3 & 74.2 & 81.2 \\
            & 1.0 & 83.4 & 74.7 & 81.3 \\
            \bottomrule
            \end{tabular}
        }
        \caption*{(a)}
    \end{minipage}\hfill
    \begin{minipage}{0.5\textwidth}
        \centering
        \resizebox{0.66\textwidth}{!}{
            \begin{tabular}{c | ccc}
            \toprule
            {\multirow{2}{*}{ \(\alpha_{\text{\tiny BC}}\)}} & \multicolumn{3}{c}{\textbf{VOC 15-1 (6 steps)}} \\ & 0-15 & 16-20 & all \\
            \midrule
            1 & 82.9 & 74.9 & 81.0 \\
            0.9 & \textbf{83.5} & \textbf{75.1} & \textbf{81.6} \\
            0.8 & 83.5 & 75.0 & 81.5\\
            0.7 & 83.4 & 74.4 & 81.3\\
            0.6 & 83.3 & 74.3 & 81.2\\
            0.5 & 83.2 & 73.8 & 81.0\\
            0 & 80.6 & 66.6 & 77.3\\
            \bottomrule
            \end{tabular}
        }
        \caption*{(b)}
    \end{minipage}
    % \vspace{-20pt}
    \label{tab:ablation_hyperpara}
\end{table}

% \begin{table}[h]
%     \caption{Ablation studies for hyper-parameters: memory size \(|M|\), the ratio of noise filtering \(\alpha_{\text{\tiny NF}}\).}
%     \centering
%     \resizebox{0.35\linewidth}{!}{
%     \begin{tabular}{c||c|ccc}
%         \toprule
%          & value  &  0-15 & 16-20 & all \\
%         \midrule
%         {\multirow{3}{*}{\(|M|\)}} & 50 & 83.4 & 72.3 & 80.8 \\
%         & 100 & 83.5 & 75.1 & 81.5 \\
%         & 200 & 83.5 & 75.5 & 81.7 \\
%         \midrule
%         {\multirow{5}{*}{\(\alpha_{\text{\tiny NF}}\)}}
%         % & 0 & 82.8 & 74.2 & 80.7 \\
%         & 0.2 & 83.5 & 75.0 & 81.4 \\
%         & 0.4 & \textbf{83.6} & \textbf{75.1} & \textbf{81.6} \\
%         & 0.6 & 83.4 & 74.6 & 81.3 \\
%         & 0.8 & 83.3 & 74.2 & 81.2 \\
%         & 1.0 & 83.4 & 74.7 & 81.3 \\
%         \bottomrule
%     \end{tabular}
%     }
%     \label{tab:abs_hyperpara1}
%     % \vspace{-10pt}
% \end{table}

% \begin{table}[h]
%     \caption{Ablation studies for hyper-parameters: the ratio of background compensation \(\alpha_{\text{\tiny BC}}\).}
%     \centering
%     \resizebox{0.3\linewidth}{!}{
%     \begin{tabular}{c | ccc}
%         \toprule
%         {\multirow{2}{*}{ \(\alpha_{\text{\tiny BC}}\)}} & \multicolumn{3}{c}{\textbf{VOC 15-1 (6 steps)}} \\ & 0-15 & 16-20 & all \\
%         \midrule
%         1 & 82.9 & 74.9 & 81.0 \\
%         0.9 & \textbf{83.5} & \textbf{75.1} & \textbf{81.6} \\
%         0.8 & 83.5 & 75.0 & 81.5\\
%         0.7 & 83.4 & 74.4 & 81.3\\
%         0.6 & 83.3 & 74.3 & 81.2\\
%         0.5 & 83.2 & 73.8 & 81.0\\
%         0 & 80.6 & 66.6 & 77.3\\
%         \bottomrule
%     \end{tabular}
%     }
%     \label{tab:abs_hyperpara2}
%     % \vspace{-10pt}
% \end{table}

\paragraph{Impact of semantics decoupling on temporary concepts} To understand which types of concepts most benefit from IPSeg, we categorize the 20 classes of Pascal VOC into 15 base classes and 5 new classes based on the incremental process. According to the learning objectives, all foreground classes are treated as temporary concepts in the corresponding step and the background is constantly considered as permanent ones.
The comparison shown in Table~\ref{tab:overall_results_of_KD} indicates that the new classes gain more significant performance improvement than the base classes. Furthermore, the permanent concepts (i.e., the background) achieve less improvement compared to the temporary concepts. This observation suggests that IPSeg is more effective in enhancing the learning of new foreground classes.

\begin{table}[h]
    \caption{The ablation study of \textbf{SD} over background (BG), base foreground classes (Base) and new foreground classes (New) on Pascal VOC 15-1 with Swin-B backbone.}
    \centering
    \resizebox{0.55\linewidth}{!}{
    \begin{tabular}{c || ccc|c}
    \toprule
    \textbf{SD} & BG & Base(1-15) & New(16-20) & All \\
    \midrule
    \XSolidBrush & 92.4 & 78.5 & 69.6 & 77.0 \\
    \Checkmark & 94.3(+1.9) & 82.3(+3.7) & 75.1(\textbf{+5.5}) & 81.5 \\
    \bottomrule
    \end{tabular}
    }
    \label{tab:overall_results_of_KD}
\end{table}


\paragraph{Detailed results of semantics decoupling on temporary concepts} To understand which types of classes or concepts benefit from our method, we compare IPseg against the baseline on 20 classes of the VOC 15-1 setting. The detailed results are presented in Table~\ref{tab:detailed_results_of_KD}, which demonstrates that IPSeg is more effective in enhancing the learning of new foreground classes.

\begin{table}[h]
    % \captionsetup{skip=0pt}
    \centering
    \caption{Detailed results of the ablation study for semantics decoupling (\textbf{SD}) over each class on Pascal VOC 15-1 with Swin-B backbone. Texts in red indicate 5 new classes.}
    \begin{tabular}{c||c|c|c|c|c|c|c|c|c|c|c}
    \toprule
    \textbf{SD} & \multicolumn{11}{c}{Detailed results} \\
    \midrule
    \multirow{4}{*}{\XSolidBrush}
    & BG & plane & bike & bird & boat & bottle & bus & car & cat & chair & cow \\ 
    & 92.4 & 87.4 & 37.7 & 89.1 & 67.8 & 80.4 & 93.8 & 86.9 & 93.6 & 43.9 & 85.7 \\
    & tabel & dog & horse & motor & person & \textcolor{red}{plant} & \textcolor{red}{sheep} & \textcolor{red}{sofa} & \textcolor{red}{train} & \textcolor{red}{TV} & \textbf{mIoU}  \\
    & 63.8 & 90.0 & 87.2 & 85.9 & 84.1 & \textcolor{red}{57.5} & \textcolor{red}{81.6} & \textcolor{red}{53.3} & \textcolor{red}{87.1} & \textcolor{red}{68.4} & \textbf{77.0}\\
    
    \midrule
    \multirow{4}{*}{\Checkmark}
    & BG & plane & bike & bird & boat & bottle & bus & car & cat & chair & cow \\ 
    & 94.3 & 91.8 & 43.8 & 93.8 & 75.0 & 86.0 & 94.2 & 91.2 & 96.1 & 44.3 & 94.6\\
    % \cmidrule{2-12}
    & tabel & dog & horse & motor & person & \textcolor{red}{plant} & \textcolor{red}{sheep} & \textcolor{red}{sofa} & \textcolor{red}{train} & \textcolor{red}{TV} & \textbf{mIoU}  \\
    & 67.3 & 94.5 & 93.0 & 88.8 & 88.2 & \textcolor{red}{65.6} & \textcolor{red}{90.1} & \textcolor{red}{57.9} & \textcolor{red}{89.3} & \textcolor{red}{72.7}  & \textbf{81.5}\\
    \bottomrule
    \end{tabular}
    \label{tab:detailed_results_of_KD}
\end{table} 

\paragraph{Class-wise results of IPSeg}Table~\ref{tab:detailed_results} shows the detailed experimental results of IPSeg for each class across four incremental scenarios of Pascal VOC 2012. IPSeg demonstrates superior performance in various incremental learning tasks, including standard tasks with a large number of initial classes (e.g., 15-5 and 15-1) and long-range tasks with fewer initial classes (e.g., 10-1 and 2-2). Notably, in the 2-2 task, the mIoU for ``cow" and ``horse" reaches $70.7$\% and $81.4$\%, respectively. This indicates that IPSeg maintains excellent prediction performance even when classes with similar semantic information are trained in different stages.

\begin{table}[h]
    \caption{Class-wise results of IPSeg over each class.}
    \centering
    \resizebox{0.8\linewidth}{!}{
    \begin{tabular}{l||c|c|c|c|c|c|c|c|c|c|c}
    \toprule
    \multirow{4}{*}{\textbf{VOC 15-5}}
    & BG & plane & bike & bird & boat & bottle & bus & car & cat & chair & cow \\ 
    & 93.6 & 92.2 & 44.9 & 93.8 & 74.4 & 85.2 & 93.9 & 90.8 & 96.2 & 43.0 & 94.5 \\
    % \cmidrule{2-12}
    & tabel & dog & horse & motor & person & plant & sheep & sofa & train & TV & \textbf{mIoU}  \\
    & 68.0 & 94.2 & 92.9 & 88.0 & 88.0 & 66.3 & 91.5 & 46.4 & 87.8 & 74.4  & \textbf{80.9}\\
    \midrule
    \multirow{4}{*}{\textbf{VOC 15-1}}
    & BG & plane & bike & bird & boat & bottle & bus & car & cat & chair & cow \\ 
    & 94.3 & 91.8 & 43.8 & 93.8 & 75.0 & 86.0 & 94.2 & 91.2 & 96.1 & 44.4 & 93.5 \\
    & tabel & dog & horse & motor & person & plant & sheep & sofa & train & TV & \textbf{mIoU}  \\
    & 67.4 & 94.5 & 92.9 & 88.8 & 88.2 & 66.4 & 88.6 & 58.1 & 89.5 & 72.7 & \textbf{81.5}\\
    \midrule
    \multirow{4}{*}{\textbf{VOC 10-1}}
    & BG & plane & bike & bird & boat & bottle & bus & car & cat & chair & cow \\ 
    & 93.1 & 93.1 & 42.2 & 93.2 & 72.1 & 83.5 & 93.9 & 91.9 & 95.8 & 38.0 & 86.9 \\
    & tabel & dog & horse & motor & person & plant & sheep & sofa & train & TV & \textbf{mIoU}  \\
    & 54.1 & 91.8 & 86.1 & 87.5 & 87.5 & 64.2 & 85.6 & 49.9 & 88.4 & 72.1 & \textbf{78.6}\\
    \midrule
    \multirow{4}{*}{\textbf{VOC 2-2}}
    & BG & plane & bike & bird & boat & bottle & bus & car & cat & chair & cow \\ 
    & 91.4 & 88.6 & 39.3 & 87.9 & 71.5 & 71.9 & 89.1 & 78.2 & 89.3 & 28.8 & 70.7\\
    & tabel & dog & horse & motor & person & plant & sheep & sofa & train & TV & \textbf{mIoU}  \\
    & 55.2 & 83.5 & 84.1 & 77.6 & 82.6 & 63.6 & 72.1 & 44.4 & 82.1 & 69.1 & \textbf{72.4}\\
    \bottomrule
    \end{tabular}
    }
    \label{tab:detailed_results}
\end{table}


\paragraph{Experimental results of disjoint setting}
To demonstrate IPSeg's robust learning capability under different incremental learning settings and to further prove its superiority over general methods, we evaluate IPSeg using the disjoint setting on the Pascal VOC 2012 dataset for the 15-1 and 15-5 tasks, as shown in Table~\ref{tab:voc_res_disjoint}. The results indicate that IPSeg consistently achieves the best performance compared to state-of-the-art methods. Additionally, similar to the results in the overlap setting, IPSeg exhibits a strong ability to learn new classes while retaining knowledge of the old classes. Specifically, IPSeg outperformed the second-best method by $\textbf{10.1}$\% in the 15-5 task and by $\textbf{22.8}$\% in the 15-1 task in terms of new class performance.

\begin{table}[h]
    \caption{Comparison with state-of-the-art methods on Pascal VOC 2012 dataset for disjoint setup.}
    \centering
    \resizebox{0.53\linewidth}{!}{
    \begin{tabular}{c|c||ccc|ccc}
    \toprule
    \multicolumn{2}{c |}{\multirow{2}{*}{Method}}  & \multicolumn{3}{c|}{\textbf{VOC 15-5 (2 steps)}} &\multicolumn{3}{c}{\textbf{VOC 15-1 (6 steps)}}  \\
     \multicolumn{2}{c |}{} & 0-15 & 16-20 & all & 0-15 & 16-20 & all \\
    \midrule
    \multirow{4}{*}{\rotatebox{90}{Data-free}}
    & LwF-MC & 67.2 & 41.2 & 60.7 & 4.5 & 7.0 & 5.2\\
    & ILT & 63.2 & 39.5 & 57.3 & 3.7 & 5.7 & 4.2\\
    & MiB & 71.8 & 43.3 & 64.7 & 46.2 & 12.9 & 37.9\\
    & RCIL & 75.0 & 42.8 & 67.3 & 66.1 & 18.2 & 54.7\\
    \midrule
    \multirow{5}{*}{\rotatebox{90}{Replay-based}} 
    & SDR & 74.6 & 44.1 & 67.3 & 59.4 & 14.3 & 48.7\\
    & SSUL-M & 76.5 & 48.6 & 69.8 & 76.5 & 43.4 & 68.6\\
    & MicroSeg-M & 80.7 & 55.2 & 74.7 & 80.0 & 47.6 & 72.3\\
    & CoinSeg-M & 82.9 & 61.7 & 77.9 & 82.0 & 49.6 & 74.3\\
    & IPSeg(ours) & \textbf{82.7} & \textbf{71.8} & \textbf{80.1} & \textbf{82.6} & \textbf{72.4} & \textbf{80.2}\\
    \bottomrule
    \end{tabular}
    }
    \label{tab:voc_res_disjoint}
\end{table}



% \newpage
\paragraph{More qualitative results on Pascal VOC 2012}
In addition to the qualitative results of the VOC 2-2 task shown in the main paper, we present additional qualitative analysis in Figure~\ref{fig:vis_voc}. We select the 15-1 task and perform a visual analysis for each newly added class. Each image includes both old and new classes, covering indoor and outdoor scenes as well as various objects and environments. For example, the first row shows the learning ability for "plant". After step 1, the model consistently predicts ``plant" correctly while retaining the ability to recognize "dog". Similarly, rows 2-5 show consistent performance. For each new class (i.e., sheep, sofa, train, and TV in the figure), IPSeg quickly adapts to them while retaining the ability to recognize old classes (i.e., bird, person, cat in the figure). These results clearly and intuitively demonstrate IPSeg's strong capability in addressing incremental tasks.



% \newpage


\paragraph{More qualitative results on ADE20K}
The qualitative results of the 100-10 task on the ADE20K dataset are shown in Figure~\ref{fig:vis_ade}. We select five examples to illustrate the model's ability to predict various classes as the learning step increases. Row 1 shows the performance of predicting the new class "ship" in step 1, where the model effectively recognizes both the old class "sky" and the new class "ship." Similarly, in rows 2-5, for the newly introduced classes (tent, oven, screen, flag), IPSeg demonstrates excellent performance in learning the new classes without forgetting the old ones. This indicates that IPSeg achieves a balance between stability and plasticity even on more challenging 
 and realistic datasets.

\begin{figure}[H]
    \centering
    \includegraphics[width=1\linewidth]{figs/iclr_rebuttal_pdf_yuxiao/voc.pdf}
    \caption{Qualitative results on Pascal VOC 2012 dataset with the 15-1 scenario.}
    \label{fig:vis_voc}
\end{figure}

\newpage

\begin{figure}[H]
    \centering
    \includegraphics[width=1\linewidth]{figs/iclr_rebuttal_pdf_yuxiao/ade.pdf}
    \caption{Qualitative results on ADE20K dataset with the 100-10 scenario.}
    \label{fig:vis_ade}
\end{figure}

% \paragraph{Comparison of different image labels}~\cref{tab:different_image_label} shows the results from different label choices used to train the Image Posterior branch. ``Joint'' represents the upper bound result. "Full-label" indicates that the training set annotations contain information from all past classes. "Partial-label" means that the training set annotations include only the information from the current step. "Pseudo-label" implies that the training set annotations are composed of information from the current step and pseudo labels.

% Although the full-label approach achieves the best performance, it involves some degree of knowledge leakage from past classes, which we aim to avoid. The partial-label approach contains only the information about the classes from the current step. In contrast, the pseudo-label approach leverages the model's knowledge of old classes based on the partial labels, thereby obtaining more information and achieving the best performance. This is the method we adopt.


% \begin{table}[ht]
%     \caption{Comparison of different label choices to train the Image Posterior branch. The choice of Pseudo-label is adopted in IPSeg.}
%     \centering
%     \resizebox{1.0\linewidth}{!}{
%     \begin{tabular}{l||ccc|ccc|ccc}
%     \toprule
%     \multirow{2}{*}{Image labels} & \multicolumn{3}{c|}{\textbf{VOC 15-5 (2 steps)}} &\multicolumn{3}{c|}{\textbf{VOC 10-1 (11 steps)}} &\multicolumn{3}{c}{\textbf{VOC 2-2 (10 steps)}} \\
%     & 0-15 & 16-20 & all & 0-10 & 11-20 & all & 0-2 & 3-20 & all \\
%     \midrule
%     Full-label & 83.2 & 73.8 & 81.0 & 80.1 & 78.0 & 79.2 & 75.2 & 74.6 & 74.8 \\
%     Pseudo-label & 83.3 & 73.3 & 80.9 & 80.3 & 76.7 & 78.6 & 73.1 & 72.3 & 72.4 \\
%     Partial-label & 83.3 & 72.8 & 80.8 & 79.3 & 74.5 & 77.0 & 72.6 & 69.4 & 69.8\\
%     \midrule
%     Joint  & 83.8 & 79.3 & 82.7 & 82.4 & 83.0 & 82.7 & 75.8 & 83.9 & 82.7\\
%     \bottomrule
%     \end{tabular}
%     }
%     % \vspace{-20pt}
%     \label{tab:different_image_label}
% \end{table}

% \begin{table}[h]
%     \caption{Overall comparison of the ablation study for \textbf{SD} over background (BG), base foreground classes  and new foreground classes (New) on Pascal VOC 15-1.}
%     \centering
%     \begin{tabular}{c || ccc|c}
%     \toprule
%     \textbf{SD} & BG & Base(1-15) & New(16-20) & All \\
%     \midrule
%     \XSolidBrush & 92.4 & 78.5 & 69.6 & 77.0 \\
%     \Checkmark & 94.3(+1.9) & 82.3(+3.7) & 75.1(\textbf{+5.5}) & 81.5 \\
%     \bottomrule
%     \end{tabular}
%     \label{tab:overall_results_of_KD}
% \end{table}



% \paragraph{More ablation of hyper-parameters}


\iffalse
\subsection{Disscusion on Semantic Drift: from Optimization}
Semantic drift is not a newly raised problem, here we hold a brief discussion from the supervision and optimization aspect.
Previous works mainly discuss the background caused by the heavy representation of background class. In training task $t$, all pixels not belonging to classes of $\mathcal{D}_t$ are assigned with background class $c_b$. This misleading design truly leads to a certain degree of semantic drift. But we will reveal another cause of this problem here.

\vspace{20pt}
Let us assume two segmentation models $f_{\theta_1}, f_{\theta_2}$, which are trained from class incremental setting and fully supervised setting separately. Following classic implementation in CISS~\citep{SSUL_cha2021ssul,coinseg_zhang2023coinseg,microseg_zhang2022mining}, $f_{\theta_1}$ adopts the sigmoid with binary cross-entropy loss (BCE) for optimization and strictly follows the CISS definition. $f_{\theta_2}$ is assumed to simply adopt the softmax with cross-entropy loss (CE) for optimization.

For the training and optimization implementation of $f_{\theta_1}$, its training objective function is
\begin{equation}
        \mathcal{L}_{\text{\tiny BCE}}(\theta_1) = - \frac{1}{\left|\mathcal{D}_t\right|} \sum_{c\in\mathcal{D}_t} (y_{i,j}^{t,c} \log \sigma(s_{i,j}^{t,c}) + (1-y_{i,j}^{t,c}) \log (1-\sigma(s_{i,j}^{t,c}))).
\end{equation}
where $\sigma(\cdot)$ denotes the sigmoid function. The model $f_{\theta_1}$ can get a gradient at score $s_{i,j}^{t,c}$ as
\begin{equation}
\label{equ-gbce}
    g_{\theta_1} = \frac{\partial\mathcal{L}_{\text{\tiny BCE}}}{\partial s_{i,j}^{t, c}} = \sigma(s_{i,j}^{t,c}) - y_{i,j}^{t,c}
\end{equation}
For fully supervised implementation $f_{\theta_2}$, its training objective is
\begin{equation}
    \mathcal{L}_{\text{\tiny CE}}(\theta_2) = - \frac{1}{\left|D\right|} \sum_{c\in\mathcal{D}} (y_{i,j}^{t,c} \log p_{i,j}^{t,c}), \texttt{~~where~} p_{i,j}^{t,c} = \frac{\sigma(s_{i,j}^{t,c})}{\sum_{k\neq c} \sigma(s_{i,j}^{t,k}) + \sigma(s_{i,j}^{t,c})},
\end{equation}
where ~$p_{i,j}^{t,c}$ is obtained from \texttt{softmax} function as the equation shows. And we can get the gradient for the score $s_{i,j}^{t,c}$ as
\begin{equation}
\label{equ-gce}
    g_{\theta_2} = \frac{\partial\mathcal{L}_\text{\tiny CE}}{\partial s_{i,j}^{t, c}} = p_{i,j}^{t,c} - y_{i,j}^{t,c} = \frac{\sigma(s_{i,j}^{t,c})}{\sum_{k\neq c} \sigma(s_{i,j}^{t,k}) + \sigma(s_{i,j}^{t,c})} - y_{i,j}^{t,c}.
\end{equation}

Based on the~\cref{equ-gbce} and~\cref{equ-gce}, we find for the case $y_{i,j}^{t,c} = 1$, both two objective functions encourage the model, exactly the task head $\phi_c$ to output a high enough score. But the difference exists that using the CE function can make the other heads output small scores where the training achieves convergence, hence the gradient is small enough. 

Further, let us discuss what happens if a pixel $x_{i,j}$ has ground truth class $c_1, c_1 \neq c \land c_1 \notin \mathcal{C}_t$. In the training of $f_{\theta_1}$, it is viewed as $c_b$ based on the CISS setting. If this pixel if been wrong predicted as $c$ by $f_{\theta_1}$, only the task heads $\phi_{c_b}$ and $\phi_{c}$ will get updates. The task head $\phi_{c_1}$ can not be updated due to invisibility at task $t$ phases. While $f_{\theta_2}$ works all well. This introduces a problem that some task heads tend to always output larger scores after the corresponding training stage. And this plays a bad role in inference. This phenomenon is another form of semantic drift except for background shift. It mainly occurs as some task heads output a relatively higher score compared to the head which should be the correct one because of the ill-posed optimization in CISS.
\fi
% With the advance of numerical normalization across scores, output scores tend to keep in a reasonable range.

% If $y_{i,j}^{t,c} = 0$, it means an another class $c_1, c_1 \neq c$ is the correct ground-truth, $y_{i,j}^{c_1}=1$. There are two possible situations: 1) it is a class inner the task $t$; 2) it is a class outer the task $t$. In the former situation, BCE and CE functions can achieve similar effects on optimizing the corresponding $\psi_{c_1}$ while the BCE function has no chance to guarantee the output scale consistency. 

% This phenomenon is another form of semantic drift except for background shift. It mainly occurs as some task heads output a relatively higher score compared to the head which should be the correct one. According to our analysis, we mainly attribute this problem to two reasons, disjoint training stage setting and the character of BCE loss.