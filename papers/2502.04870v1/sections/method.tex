\section{Method}
\label{sec:Method}

% 现在的逻辑太差了，太散了，读下来割裂感很强。
% 
% 1.problem definition -- 给出问题定义，引出semantic overlap （考虑是否应当从method section中拆分）
% 2.method overview -- 承上起下，先直接给出我们对1中问题的解决方法，然后简单描述方法pipeline，统领后续两段的具体方法表述 (通过紧凑的语言给出的我们的solution)
% 3.Method Part 1 -- 
% 4.Method Part 2 -- 
% 考虑是否保持现有的从global到local角度的表达

% 首先是问题的形式化定义
In this section, we first present a theoretic analysis of class incremental learning and give the definition of the semantic overlap phenomenon. Then, we present our solution in detail and claim its effectiveness in this challenge.

\subsection{Preliminary}
\label{sec3-1:PSO_in_CL}
% \subsection{Probability Scale Inconsistency in Continual Learning}

% two theoretical analysis systems - 1. bayesian into tii and wtp 2. differences between bce and ce.

Continual learning methods take sequential arriving data $\mathcal{D}_t$, with their data distribution domain $\mathcal{X}_t$ and labels $\mathcal{Y}_t$. Typically, class incremental learning regards different tasks should hold disjoint label spaces where $\mathcal{Y}_t \cap \mathcal{Y}_{t_i} = \varnothing, \forall t \neq t_i$. In practice, there might be more than one target label in one task and results in $\mathcal{Y}_t = \{ \mathcal{Y}_{t, k} \}$, where $k \in \{1,2,3,..., \mid \mathcal{Y}_t \mid\}$ denotes label domain of task $t$.

Intuitively, the learning target of class incremental learning can be written as the following probability, $P(x \in \mathcal{X}_{t,c} \mid \mathcal{D}, \mathcal{\theta})$, Based on the Bayes' theorem, the above probability can be decomposed into, 
\begin{equation}
     P(x \in \mathcal{X}_{t,c} \mid \mathcal{D}, \theta) =  P(x \in \mathcal{X}_{t,c} \mid x \in \mathcal{X}_t, \mathcal{D}, \theta) P(x \in \mathcal{X}_{t} \mid \mathcal{D}, \theta) 
\end{equation}
where the first term on the right side denotes the inner-task probabilities and the second term denotes the inter-task probabilities. This equation directly tells us how to improve the performance of class incremental learning methods, directly improving inner-task classification accuracy $P(x \in \mathcal{X}_{t,c} \mid x \in \mathcal{X}_t, \mathcal{D}, \theta)$ or inter-task classification accuracy $P(x \in \mathcal{X}_{t} \mid \mathcal{D}, \theta)$. 

From this decomposition, some works propose to improve inter-task ability by constructing task-specific parameters for every task. Instead of optimizing a whole model $\theta_t$ for a task, some works improve memory utilization by learning a new task head $\psi_t$ with a shared backbone. Thus, the above learning paradigm is transformed from learning xxx.

\fy{until this part, our motivation is still not clear from the mathematical analysis. This part needs further checking.}
% 怎样通过数学分析直接给出我们的motivation，也就是为什么 $P(x \in X_c \mid \theta, D)这个因子很重要。$

\iffalse
\fy{This subsection needs to realize: reasonable challenge/problem definition with formulation analysis, strongly claim our insights and findings.}

\iffalse
To overcome catastrophic forgetting, some works solve this problem from a scheme named, ``new task new model'', in class incremental learning. In detail, this kind of method trains a new model for every new incoming task. Some works improve memory utilization by learning a new task head instead of a whole model. In this solution scheme for the class incremental learning, for a given input \(x\) and task \(T\), result \(\hat{y}_i\) can be defined as:
\fi

\fy{here, we give a simple math formulation to this kind of method.}

\[\hat{y}_i=h_i(M(x))\]

Where \(M\) is the shared model, \(h_i\) is the independent task head used to handle different tasks \(T\).

\fy{Then describe the probability overlap challenge from the probability view.}

One of the issues with this approach is the problem of interference among task heads. Specifically, the outputs of different task heads may overlap in the probability space, making it difficult to accurately distinguish between task classes. Suppose we have different tasks \(T\), each with its own independent task head \(h_i\), sharing a model \(M\). For any input sample \(x\), the output probability space of the task heads is:

\[P_{T_i}(\hat{y}_i|x)=h_i(M(x))\]

Where \(P_{T_i}\) represents the probability distribution of the model predicting \(x\) belonging to task \(T\). For the final output \(\hat{y}\), the final output is determined by selecting the class corresponding to the maximum probability among the output probability distributions of each task head:

\[\hat{y}=argmax_k\sum_{i=1}^nP_{T_i}(\hat{y}_i=k|x)\]

For the same input \(x\), if the shared model \(M\) projects similar features for tasks \({T_1}\) and \({T_2}\), it will cause overlapping probability distributions between the output results of task heads \({h_1}\) and \({h_2}\). Consequently, the model will be unable to accurately determine which task the input belongs to when integrating these distributions for the final output, leading to erroneous classification.

The reasons for this issue can be summarized as follows:\textbf{Inconsistent Probability Space Distribution}: In class incremental learning, optimization of each task head for different stages of data results in inconsistent feature distributions in the probability space, impacting the accuracy of the final result.\textbf{Inconsistent Result Aggregation Scale}: Inconsistent scales of output from different stage task heads may lead to certain stage results being excessively amplified or overshadowing other reasonable outputs during final integration.\textbf{Information Loss}: New stages focus only on new data, disregarding latent information in historical and existing data, gradually causing the model to lose its ability to recognize previously learned features during the incremental learning process.


\fy{Dive into class incremental semantic segmentation, the probability overlap phenomenon deteriorates and evolves into pixel-level semantic overlap due to a fine-grained classification demand.}


\paragraph{SO in CISS}
Class Incremental Semantic Segmentation is an extension of class incremental learning, focusing on pixel-level classification. This fine-grained classification demand brings additional complexity. The task \({T_i}\) and result \(\hat{y}\) are respectively mapped to stages  \({S_i}\) and pixel results \(\hat{y}_i^p\). The final prediction for each pixel \(p\) in the input image \(I\) is:

\[\hat{y}^p=argmax_c\sum_{i=1}^nP_{S_i}(\hat{y}_i^p=c\mid I)\]

Here, \(\hat{y}^p\) is the prediction result of pixel \(p\), and \(c\) denotes the class.

At this time, the Probability Space Overlap evolves into a specific semantic overlap. This is manifested as the model incrementally learns the semantic information of newly added classes. The new classification head is only trained on the classes that are introduced in the current phase and has not seen the classes from other phases. This results in the classification heads from various phases producing inappropriately high response values for the classes they have not been trained on during model inference. When integrating the outputs of all classification heads for final classification decisions, errors in classification signals occur due to the inconsistency in output scales and distributions, overlap the correct results in the softmax layer. This is reflected as a significant decrease in the mean Intersection over Union (mIoU). As shown in the~\cref{fig:overlap}, both classification heads may respond with high semantic features to the same area, but due to the inconsistency in output scales and distributions, the model produces incorrect classification decisions.

\begin{figure}[ht]
    \centering
    \includegraphics[width=0.7\linewidth]{figs/overlap.png}
    \caption{a: Output results for the individual class "cow"; b: Output results for the individual class "horse"; c: Output results after classification decision}
    \label{fig:overlap}
\end{figure}

\fi
\subsection{Overview }
\label{sec3-2:Overview}
% Hypothesis: Pixel Semantic Drift
% Overview 


To address the unique challenges of Probability Space Overlap in CISS, we propose a new class-incremental semantic segmentation framework named ``\textbf{class-wise global insights} + \textbf{ step-wise local details}," as illustrated in~\cref{fig:overview}. The upper branch, scale branch, leverages a memory mechanism with a limited but diverse set of samples to mine the potential guiding ability of the global class information inherent in these samples. The lower branch, the current knowledge mining branch, integrates the model’s predictions for the background and non-current classes into the training of each phase, unleashing the model's feature integration capabilities for the current phase data and exposing the model to classes it does not own. By combining this global guidance with the reinforcement of local details for each classification head, we not only optimize the distribution and scale consistency of output features among different classification heads but also enhance the model’s predictive capabilities for the background and non-current classes. 

% \begin{figure}[ht]
%     \centering
%     \includegraphics[width=\textwidth]{figs/overview.png}
%     \caption{Overview}
%     \label{fig:overview}
% \end{figure}


\textbf{Memory pool.}
Following previous works, the memory pool utilizes minimal capacity to access past phase classes without violating privacy policies. A class-balanced sampling strategy is employed and data will flow into the scale branch.

\textbf{Pseudo label and saliency map.}
When the incremental process enters a new phase, data is fed into the model from the previous phase to obtain segmentation results for the old classes, known as pseudo-labels. While saliency map is used to mark areas in the image where there is a likelihood of foreground presence. This method can further annotate non-current information in images to guide the model.

\textbf{Scale branch.}
This branch focuses on integrating sample class different from memory data, generating image-level class information, and globally guiding the model's segmentation results. We provided a detailed explanation in~\cref{sec3-3:Global_Insights}.


\textbf{Current knowledge mining branch}
This branch integrates the model's predictions of background and non-current elements into each phase's training, leveraging foreground information hidden at various depths within the model. It locally constrains the output of each classification head. More details will be presented in~\cref{sec3-4:Local_Details}

\subsection{Scale consistence via global guidance}
% \subsection{Global Insights: Global Scale Guidance via Memory}
\label{sec3-3:Global_Insights}

\fy{This section is not compact in expression, and our contribution is not highlighted. To overcome and improve our writing quality, the first is to make our logic clear and easy to understand.}
% 现在的内容一方面没逻辑、太散、太乱，另一方面方法陈述部分不直接太琐碎，细节淹没了主体
% 每一部分的内容都应当突出， 当前部分是如何为整体服务的， 而不是简单的陈列、堆砌


General replay methods use a memory pool to store old classes and mix them into the incremental learning process of new phases, which to some extent reduces the incorrect predictions of the new phase classification head for old classes, thus mitigating catastrophic forgetting. In contrast, our method introduces a global scale guidance branch, rethinking the implied information about old classes in the memory mechanism and exploring its potential to guide model outcomes. This approach extracts global image-level class information from both old and new data. It outputs high-accuracy image-level class information, thereby addressing the issues of feature distribution and output scale inconsistency discussed in~\cref{sec3-1:PSO_in_CL}.

\paragraph{Global guidance branch}
We design an additional parallel branch to perform image-level few-shot multi-label classification tasks while maintaining the main network backbone. This branch focuses on extracting distinct class features different from a small number of samples, rethinking the image-level guidance effects of the memory mechanism in existing incremental semantic segmentation tasks. It fully utilizes the characteristics of the memory mechanism, ``few samples but large class feature differences," to explore its potential in guiding model outcomes.


The branch processes samples mixed from old class data collected from the memory pool and new class data of the current phase, ensuring that the scale head sees and captures information from all classes in every incremental phase and aligns them to the same scale.
After receiving the feature map extracted by the backbone, it is processed through \textit{AdaptiveAvgPool} to reduce redundancy, then the features are flattened, and processed by the scale head to obtain intermediate results \(scale\_output_{t}\) for different incremental phases. Finally, a layer of \textit{MLP} is used to output image-level class information. Formally, the \(Scale\_output\) can be expressed as:

\[scale\_output_t=H_t^s(flatten(AAP(F(x_m))))\]
\[Scale\_output=MLP~(Concat_{t=0}^N~scale\_output_t)\]

Where,\(x_m\) is the mixed input image, \(F\) denotes the feature extractor, \(AAP\) is AdaptiveAvgPool, and \(H_t^s\) is the scale head for the incremental phase \(t\). We also employ the binary cross-entropy loss for supervision:
\[L_{scale}=BCE(Scale\_output,y_{scale})\]

Where \(y_{scale}\) is obtained from the true class labels of the images, originating from the original semantic segmentation ground truth.

\paragraph{Class-wise Scale for output}
After obtaining global guidance, this information will be utilized to make class-wise scaling decisions for the final semantic segmentation results, expressed as:

\[Res_{Final}=Sig(H_{0-t}^c(F(x_m)))\otimes Sig(Scale\_output)\]

Where \(Sig\) is the sigmoid function, and \(H_{0-t}^c\) represents the classifier of semantic segmentation.

\paragraph{Class balanced strategy for memory pool}
To ensure that the scale branch can adequately receive samples from all classes and maintain class scaling capability, for a given memory constraint, we selected representative samples from both old and new classes and employed a class-balanced strategy for collection to support effective few-shot learning in the scale branch. Specifically, for a given memory size \(M\) and the current total number of classes \(C\), we strictly ensure that each class's data appears at least \(M//C\) times. Then, for each image, the foreground pixel count is calculated and class counts are tallied as selection criteria. The class-balanced strategy particularly focuses on balancing sample quantities in the context of few-shot scenarios, which is crucial for improving the model's accuracy in multi-label classification tasks.


\paragraph{Background inhibition}
In this section, segmentation information for each class undergoes varying degrees of scaling through the scale head, while the background, as a special class, will have differing impacts on segmentation results compared to other classes. Therefore, to ensure overall segmentation effectiveness, we consider the background class separately and set appropriate parameter \( \alpha\) to suppress the output of the background, as illustrated in the top right corner of~\cref{fig:overview}.


\subsection{Mining from unknown knowledge}
\label{sec3-4:Local_Details}
% 梳理 -- 这一部分在解决什么问题？直接回答是通过什么方法解决的，然后详细的给出方法描述。和Global部分一样的问题, 需要推倒，梳理逻辑之后重写。

During the incremental process, each phase only accesses annotated data for specific classes, leading to a complete loss of information for unknown foregrounds and foregrounds that should belong to old classes. Our method integrates saliency information to enhance the ability of each newly added classification head to recognize background and non-current classes. We add and maintain a continuously trained \(head[0]\), which is specifically responsible for identifying background and unknown classes. Pseudo-labeling is utilized to help classification heads maintain responsiveness to knowledge of old classes. Each classification head can not only recognize the classes of the current phase but also effectively distinguish between the background and non-current phase classes, thereby addressing the issue of complete loss of information for non-current classes during the incremental process, as discussed in section~\cref{sec3-1:PSO_in_CL}.

\paragraph{Step-wise knowledge mining for non-current}
We added stage-specific background and non-current recognition capabilities to the classification heads at different stages of the main network branch. By using saliency information to highlight potential foregrounds and combining class labels specific to this stage, we integrate the recognition of backgrounds and non-current classes into the training. This ensures that each classification head "sees" non-stage classes during training, achieving feature scale consistency within the heads at different stages of the model. During the training phase, auxiliary information such as a saliency mask and pseudo-labels are added to the ground truth, resulting in two new ground truths: ground truth for global and ground truth for current, corresponding to \(global\_gt\) and \(curr\_gt\) in Figure~\cref{fig:overview}, respectively. This process can be described as follows:
\[gt_{current}=gt_t\cup(\sim gt_t\cap Sal(x_m))\]
\[gt_{global}=gt_{t}\cup(\sim gt_{t}\cap\Theta_{t-1}(x_{m}))\cup(\sim gt_{t}\cap\sim\Theta_{t-1}(x_{m})\cap Sal(x_{m}))\]

Where, \(gt\) denotes the true class labels for this stage, \(Sal\) represents the saliency detection, and \( \Theta_{t-1}(x_{m})\) denotes the output from the model of the previous stage.  \( \Theta_{t-1}(x_{m})\)' is obtained according to the following rules:

\[\theta_{t-1}(x_m)_{(i,j)}=\begin{cases}\theta_{t-1}(x_m)_{(i,j)}~~~where~\theta_{t-1}(x_m)_{(i,j)}>\tau\\0~~~~~~~~~~~~~~~~~~~~~~~others\end{cases}\]

Additionally, to ensure better recognition of both current and other classes, this method fully mines foreground knowledge hidden at different depths within the model. It incorporates a feature integration mechanism that operates layer by layer to enhance its generalization capability and accuracy:

\begin{equation}
    {fm}_{{new}}={M}^{'}({G}_{1}(\Psi_{1}),G_{2}(\Psi_{2}),G_{3}(\Psi_{3}), ASPP(\Psi_{4}))
\end{equation}

% \[{fm}_{{new}}={M}^{'}({G}_{1}(\Psi_{1}),G_{2}(\Psi_{2}),G_{3}(\Psi_{3}), ASPP(\Psi_{4}))\]

Where $Psi_{i}$ represents the semantic information extracted from different layers of the backbone. $ASPP$ is the original method used in DeepLab V3. $G_{i}$ stands for the information mining function, and $M^{'}$ is the feature integration function, which aligns the processed semantic information from different layers into a new feature map.

\paragraph{Dual-loss for global and current}
After obtaining the two types of ground truth, we designed a composite loss strategy that combines global output and current output. In this approach, \(global\_gt\) focuses on constraining \(head[0]\) to recognize unknown classes, while \(curr\_gt\) focuses on constraining the new stage classification head to distinguish between its current and non-current items. This design makes the model more efficient when dealing with images with complex backgrounds and multiple classes, enhancing the new classification head's ability to recognize non-stage classes while also improving the identification of backgrounds and unknown classes. It provides a structured solution to balance the recognition of various classes. This can be described as follows:
\[output_{global}=H_0^c(fm_{new})\oplus Concat_{i=1}^tS(H_i^c(fm_{new}))\]
\[output_{current}=S^{-1}(H_t^c(fm_{new}))\oplus Concat_{i=1}^tS(H_i^c(fm_{new}))\]
\[L_{global}=BCE(output_{global},gt\_global)\]
\[L_{current}=BCE( output_{current} , gt\_curr)\]

Where, \(S\) denotes the removal of results from background and non-current in the classifier, \(S^{-1}\) denotes the removal of results from various classes, and \(\oplus\) represents the concatenation operation. Finally, the design of the loss during the training phase can be described as follows:
\[L_{total}=L_{scale}+\lambda_{1}L_{global}+\lambda_{2}L_{current}\]
Where, \(\lambda_{i}\) is a hyper-parameter controlling the loss.

\iffalse
\paragraph{Intra-head soft mask for output}
During the inference stage, a \(softmax\) operation is used within each classifier, in conjunction with a soft mask to process the semantic segmentation results, as illustrated in the bottom of Figure~\cref{fig:overview}. This approach further leverages non-current information within the classifier, and the soft mask enables decision-making without compromising the integrity of the original semantic outputs, effectively preventing classification errors. This can be described as follows:
\[M_t^c=Softmax(h_t^c)\]
\[h_{t_{(i,j)}}^{c}=\begin{cases}\lambda_{mask}\cdot class~t~~~~where~ M_{t_{(i,j)}}^{c}=non\_current\\class~t~~~~~~~~~~~~~~~~~where~M_{t_{(i,j)}}^{c}=class~t\\bg~~~~~~~~~~~~~~~~~~~~~~~~~where~ M_{t_{(i,j)}}^{c}=bg\end{cases}\]

Where,  \(h_t^c\) represents the output results from the classification head, \(M_t^c\) denotes the integrated information after applying a \(softmax\) operation to  \(h_t^c\), and \(\lambda_{mask}\) represents the soft mask coefficient.
\fi

\paragraph{Simplified annotation for memory}
In traditional incremental semantic segmentation, the system remembers and labels pixel-level information for all classes it has encountered. However, the data processing in this method is different. Here, only the class information for each image and pixel annotations for classes not in the current phase are needed. This means that for images in the memory pool, we do not need detailed pixel-level semantic segmentation annotations, but only image-level class information, as shown in Figure~\cref{fig:simple_annotation}.
Additionally, when images in the memory do not contain the classes required in the current training phase, we use saliency detection techniques to generate a binary mask. This mask simply divides the image into two parts: the background (marked as 0) and parts not belonging to the current class (marked as 1). This approach can significantly reduce the cost and difficulty of annotation.

\begin{figure}[ht]
    \centering
    \includegraphics[width=0.5\linewidth]{figs/simple_annotation.png}
    \caption{General memory labels require pixel-level semantic segmentation annotations, while our memory labels only need image-level class annotations (people, boats, trains) and saliency detection information (non-current mask)}
    \label{fig:simple_annotation}
\end{figure}

\iffalse
\paragraph{Dynamic weight transfer strategy}
This method utilizes a continuously trained \(head[0]\) to provide weight initialization for new classification heads. As shown on the right side of Figure~\cref{fig:overview}, \(head[0]\) is always involved in training without being frozen, and its output, combined with the outputs of other phase classification heads, forms the global output, which is supervised by ground truth augmented with pseudo-labels. Thus, \(head[0]\) focuses on the background and unknown classes (note that the unknown class \textbf{excludes old and current classes}, encompassing the remaining unidentified classes). The well-trained weights of head[0] can serve as a strong starting point for initializing new heads. The entire process is illustrated at the bottom of Figure~\cref{fig:overview}.
\fi

\iffalse
\subsection{Summary}
\label{sec3-5:Summary}
The Global Insights method adjusts semantic segmentation results based on global image-level class information. This method is suitable for cases where there are few classes in the image and objects are concentrated, as it can be simplified to a classification problem almost. The Local Details method avoids confusion between classes by adding predictions of non-current classes in each classifier, especially in complex scenes where multiple classes coexist. In general, the newly proposed CISS framework in this paper can \textbf{quickly and accurately adjust class information in simple scenes through Global Insights}, while \textbf{maintaining high accuracy in complex and variable environments through Local Details}. This combined usage provides a more comprehensive and flexible incremental learning strategy.
\fi