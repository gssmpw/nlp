\section{Conclusions and Limitations}
\label{sec:Conclusions}
In this paper, We propose IPSeg, a simple yet effective method designed to address the issue of semantic drift in class incremental semantic segmentation. We begin by analyzing the details of semantic drift, identifying two key issues: separate optimization and noisy semantics. To mitigate these issues, IPSeg introduces two specific designs: image posterior guidance and semantics decoupling. Experimental results on the Pascal VOC 2012 and ADE20K datasets demonstrate the superior performance of our method, particularly in long-term incremental scenarios.

\textit{Limitations} While IPSeg introduces a novel and promising approach for the class incremental semantic segmentation challenge, We have to claim that IPSeg is based upon the memory buffer for improving classification ability. The basis limits its potential in privacy-sensitive scenarios. Eliminating the need for a memory buffer and extending IPSeg to a wider range of applications are our future targets.
% we notice that its performance declines when the memory buffer is unavailable or limited. 
% This is because IPSeg heavily relies on mining category-level knowledge from previous data to guide predictions. Addressing this performance loss in memory-constrained scenarios is a key area for future improvement.
% In this paper, we propose a simple but effective method, IPSeg, with specific designs for the semantic drift issue in class incremental semantic segmentation.
% We first analyze the causes of semantic drift and find two factors of separate optimization and noisy knowledge.
% Based on this finding, we propose to mitigate this issue with two designs, image posterior guidance and noisy knowledge decoupling. 
% The superior performance of experiments demonstrates the effectiveness of our method, especially on long-term incremental challenges.
% Furthermore, IPSeg exhibits excellent properties of both learning plasticity and memory stability

\section*{Impact Statement} Though IPSeg exhibits superior performance and properties of learning plasticity and memory stability, we realize that the usage of memory buffer leaves a lot of room for discussion. On the one hand, the use of memory buffers brings additional storage costs and the risk of information leakage. On the other hand, the use of memory buffers is related to privacy issues in some cases, such as storing private information without approval. These issues need to be treated with caution in artificial intelligence applications.
% we also should rethink the balance of model performance and privacy protection for safety AI.
