\subsection{Evolution of Convergence Across the Network Hierarchy}
\label{sec: convergence-evolution}



\paragraph{How convergence varies with network depth.}
When comparing representational convergence across the network hierarchy for different seeds of the same architecture, we observe that convergence is strongest in the earliest layers and gradually diminishes in deeper layers (Fig.~\ref{fig:network-hierarchy}). This pattern is consistent across all three metrics and across networks trained on different datasets (CIFAR100 and ImageNet). The high alignment in early layers likely arises because they capture fundamental, low-frequency features (\emph{e.g.,} edges, corners, contrast) that are universal across representations~\citep{rahaman2019spectral, bau2017network, zeiler2014visualizing}. In contrast, deeper layers, while still showing significant alignment ($>$ 0.5), exhibit greater variability due to their sensitivity to specific training conditions and noise. This trend also holds when comparing networks with different architectures, underscoring the robustness of hierarchical convergence across diverse models.

\input{./figures-code/training-evo}

\paragraph{Minimal transformations needed to align representations.}
Across all layers, we find that alignment scores increase as the mapping functions become more permissive (Permutation $\rightarrow$ Procrustes $\rightarrow$ Linear), as expected. However, linear mappings provide only modest improvements over Procrustes correlations, indicating that rotational transformations are sufficient to capture the majority of alignment information. This suggests that the added flexibility of linear mappings---such as scaling and shearing---does not substantially enhance alignment beyond what is achieved with Procrustes transformations. Importantly, because Procrustes is symmetric, this result highlights that alignment reflects a deeper similarity in the geometric structure of representations, rather than merely the ability of one representation to predict another.



\paragraph{Simple permutations achieve significant alignment.}  
\label{par: privileged-axes}
Despite the strict constraints imposed by permutation-based alignment, Permutation scores achieve surprisingly high alignment levels, indicating a strong one-to-one correspondence between individual neurons across network instances. This suggests that convergent learning extends down to the level of single neurons, even without allowing for more flexible transformations.

\input{./figures-code/privileged-axes-table}
To further probe this result and assess the depth of convergent learning, we tested the sensitivity of permutation alignment to changes in the representational basis. Specifically, we applied a random rotation matrix $\bm{Q} \in \mathbb{R}^{n \times n}$ to the converged basis of a neural representation, where $n$ is the number of neurons in a given layer. The rotation matrix was sampled from a Haar distribution via a $QR$ decomposition, ensuring that all orthogonal matrices were equally likely. We then recomputed the Permutation score after applying this rotation.


We conducted this analysis by taking response matrices from two identical DCNNs (initialized with different random seeds) at a given convolutional layer, $\{\bm{X}_1, \bm{X}_2\} \in \mathbb{R}^{m \times n}$, where $m$ represents the number of stimuli. We applied the random rotation $\bm{Q}$ to one networkâ€™s responses and computed the resulting permutation-based correlation score, $s_{\text{perm}}(\bm{X}_1\bm{Q}, \bm{X}_2)$. This process was repeated across all convolutional layers, with the alignment differences summarized in Table~\ref{tab:perm-rotated}.

These rotations consistently reduced alignment, with a drop between $\sim6-51\%$ on CIFAR-100 and $\sim 10-202\%$ on ImageNet for ResNet18 across all layers. This significant decrease highlights that the learned representations are not rotationally invariant and that the specific basis in which features are encoded is meaningfully preserved across networks. In other words, convergent learning aligns not just the overall representational structure but also the specific axes along which features are encoded. This observation echoes recent findings by~\citep{khosla2024privileged}, who report the existence of privileged axes in biological systems as well as the penultimate layer representations of trained artificial networks.  



\paragraph{Hierarchical correspondence holds across metrics.}
\label{par: inter-network}
\input{./figures-code/inter-model-proc}
\input{./figures-code/inter-model-sm}
Previous studies have shown that, for architecturally identical networks trained from different initializations, the most similar layer in one network to a given layer in another is the corresponding architectural layer~\citep{kornblith2019similarity}. However, this finding has primarily been supported using metrics invariant to affine transformations (\emph{e.g.,} CCA, SVCCA). Here, we extend this result by showing that stricter metrics---such as Procrustes and soft-matching scores---also reveal the same hierarchical correspondence (Figs.~\ref{tab:procrustes_inter_model}, ~\ref{tab:sm_inter_model}), even when comparing networks with different architectures. This suggests that the hierarchical alignment of representations is a fundamental property of neural networks, robust to both architectural differences and the choice of alignment metric. Moreover, our results show that both representational shape (captured by Procrustes) and neuron-level tuning (captured by soft-matching) follow similar alignment patterns, reinforcing the consistency of this hierarchical organization across different levels of representational analysis.