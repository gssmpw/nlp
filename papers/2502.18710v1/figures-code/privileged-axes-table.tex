%\begin{table}[h!]
%    \centering
%    \resizebox{0.5\textwidth}{!}{
%    \begin{tabular}{cccc}
%    \toprule
%    Model (Dataset) & Native Basis & Rotated Basis & Mean Difference $(\%)$\\
%    \midrule
%        ResNet18 (CIFAR100) & $0.43$ & $0.37$ & $13.23\%$ \\
%        ResNet50 (CIFAR100) & $0.41$ & $0.39$ & $5.48\%$ \\
%        VGG16 (CIFAR100) & $0.44$ & $0.36$ & $16.36\%$ \\
%        VGG19 (CIFAR100) & $0.44$ & $0.37$ & $14.59\%$ \\
%        ResNet18 (ImageNet) & $0.44$ & $0.30$ & $29.72\%$ \\
%        ResNet50 (ImageNet) & $0.44$ & $0.34$ & $24.17\%$ \\
%        VGG16 (ImageNet) & $0.49$ & $0.32$ & $36.22\%$ \\
%        VGG19 (ImageNet) & $0.47$ & $0.31$ & $34.17\%$ \\
%    \bottomrule
%    \end{tabular}
%    }
%    \caption{\textbf{Sensitivity to Individual Neuron Tuning.} We rotate the native axes for each of the networks by a random rotation matrix, as described in Section~\nameref{sec:privileged-axes} and recompute alignment scores. We average the alignment scores across all layers, and observe a drop in these scores, pointing towards the fact that single neuron tuning affects how networks represent information.}
%    \label{tab: perm-rotated}
%\end{table}

\begin{table*}[!b]
    \centering
    %\resizebox{0.5\textwidth}{!}{
    \begin{tabular}{cccc}
    \toprule
    Model (Dataset) & Native (Min / Max) & Rotated (Min / Max) & Difference ($\%$) (Min / Max)\\
    \midrule
        ResNet18 (CIFAR100) & $0.247 \ / \ 0.752$ & $0.215 \ / \ 0.689$ & $6.40\% \ / \ 51.26\%$ \\
        ResNet50 (CIFAR100) & $0.254 \ / \ 0.828$ & $0.242 \ / \ 0.828$ & $-3.38\% \ / \ 35.29\%$ \\
        VGG16 (CIFAR100) & $0.277 \ / \ 0.769$ & $0.239 \ / \ 0.661$ & $5.66\% \ / \ 63.97\%$ \\
        VGG19 (CIFAR100) & $0.273 \ / \ 0.758$ & $0.231 \ / \ 0.684$ & $2.15\% \ / \ 35.36\%$ \\
        ResNet18 (ImageNet) & $0.202 \ / \ 0.721$ & $0.154 \ / \ 0.581$ & $10.47\% \ / \ 201.84\%$ \\
        ResNet50 (ImageNet) & $0.225 \ / \ 0.791$ & $0.143 \ / \ 0.739$ & $2.61\% \ / \ 180.75\%$ \\
        VGG16 (ImageNet) & $0.288 \ / \ 0.746$ & $0.181 \ / \ 0.599$ & $21.72\% \ / \ 81.12\%$ \\
        VGG19 (ImageNet) & $0.292 \ / \ 0.799$ & $0.171 \ / \ 0.562$ & $41.95\% \ / \ 89.68\%$ \\
    \bottomrule
    \end{tabular}
    %}
    \caption{\textbf{Sensitivity of permutation scores to representational axes.} We rotate the native axes for each of the networks by a random rotation matrix and recompute alignment scores. We report the min / max alignment scores across all layers for the alignment observed in the native and rotated bases and observe that rotations almost always reduce the alignment score, highlighting the distinguished nature of the representational axes for all layers in networks.}
    \label{tab:perm-rotated}
\end{table*}
