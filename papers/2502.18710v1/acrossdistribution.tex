\subsection{Convergence across distribution shifts}

\label{sec:ood-results}
In the previous sections, we examined representational alignment under the same input distributions used for training. However, a critical question remains: does representational convergence persist under distribution shifts? To explore this, we analyzed the internal representations of ImageNet-trained DCNNs when exposed to out-of-distribution (OOD) stimuli. We used $17$ OOD datasets from~\citep{geirhos2018imagenet}, all sharing the same $16$ coarse labels as ImageNet~\citep{deng2009imagenet}, allowing for a controlled comparison of representational alignment under varying distributional shifts. The specifics of each OOD dataset has been described in Sec.~\nameref{sec: ood-dataset-details}.


We computed representational alignment across these datasets using the Procrustes metric and observed a consistent pattern: OOD inputs amplify differences in the later layers of the networks, while early layers maintain comparable alignment levels between in-distribution and OOD stimuli (Fig.~\ref{fig:ood-amplification}). We hypothesize that this pattern arises as a result of early layers capturing basic, universal features (\emph{e.g.,} edges, corners, textures) that remain nearly identical across distributions, whereas later layers encode more task-specific features that are more sensitive to distributional shifts, thus amplifying the divergence between models.

Moreover, we found a strong correlation between representational alignment in later layers and the networks' classification accuracy on the OOD datasets. Datasets where models maintained higher accuracy showed stronger alignment, whereas datasets with lower accuracy exhibited weaker alignment. This correlation was notably weaker in early layers but increased progressively with network depth across all architectures (Fig.~\ref{fig: ood-convergence}). We extend these analyses to other vision networks in Fig.~\ref{fig:ood-convergence-appendix}.

These results have several important implications. First, the stability of early-layer alignment across distributions suggests that these layers encode generalizable features that are consistent across both network initializations and input distributions. This highlights their role as a shared foundation for higher-level processing, which becomes more specialized and sensitive to distribution shifts in later layers. Second, these findings inform model-brain comparisons. Prior studies have shown that diverse architectures and learning objectives can yield similar brain predictivity~\citep{conwell2024large}. However, the observed amplification of representational divergence in later layers under OOD conditions suggests that using OOD stimuli could be an effective strategy for distinguishing between models and identifying more brain-like models. 

An important potential consequence of this result is that if the alignment between models in early layers is largely constant across data distributions, it might be possible to fine-tune models only in later layers to improve their OOD generalization capabilities.