Deep Neural Networks (DNNs) are becoming increasingly popular in neuroscience for predicting neural responses (\citep{yamins2014performance}, \citep{yamins2016using}, \citep{khaligh2014deep}) or as models for reverse-engineering algorithms of neural computation (\citep{schrimpf2018brain}, \citep{schrimpf2020integrative}, \citep{cichy2016comparison}. This congruence invokes the necessity to gain a deep understanding of how DNNs learn to represent information. A core question in this domain is whether independently trained networks converge on similar internal representations---and if so, under what conditions and along which dimensions this convergence unfolds. Comparative analysis of model representations helps reverse-engineer neural networks by linking architectural components, training objectives, and data inputs to emergent behavior. For neuroscientific modeling using deep neural networks, a foundational question lies in ascertaining which aspects of their representations vary across different architectural choices, and which aspects---if any---are universal across such choices. Thus, studying this representational convergence has far-reaching implications. 

Over the past decade, there has been growing recognition that similar representations can emerge across diverse models, even when these models differ in architecture, training procedures, or data modalities. Early work demonstrated that independent training runs of the same architecture often develop a core set of features that align well across networks. For example, early layers in convolutional networks consistently learn Gabor-like filters, a phenomenon observed across a range of architectures and tasks~\citep{yosinski2014transferable}. Efforts to quantify these similarities have employed techniques such as singular vector canonical correlation analysis (SVCCA)~\citep{raghu2017svcca} and centered kernel alignment (CKA)~\citep{kornblith2019similarity}, which have underscored the high degree of alignment in early layers and noted convergence even in later stages of the network. These findings provide empirical grounding for theories of representational convergence, hinting at the existence of universal principles governing learning---principles that may also shed light on how biological neural circuits process information.

More recent studies have extended this line of inquiry, demonstrating that as models become more capable---whether through increased scale, multitask training, or cross-modal learning---their representations converge not only within a single modality but also across modalities. For instance, the Platonic Representation Hypothesis argues that as models solve a larger number of tasks and scale up in capacity, they are pressured into discovering a universal, modality-agnostic representation of the underlying reality~\citep{huh2024platonic}. Yet, many open questions remain about the conditions under which different networks converge on similar representations and the implications of such convergence.

To address these gaps, in our work, we examine representational alignment along three key axes: 
\paragraph{Across layers:} One important axis of inquiry is representational alignment across layers. Prior work has shown that early layers tend to extract general, low-level features---such as edge detectors in vision networks---while deeper layers develop more task-specific representations. Studies by~\citep{kornblith2019similarity},~\citep{mehrer2020individual}, and~\citep{li2015convergent} have quantified these changes using methods like canonical correlation analysis (CCA) and CKA. However, these approaches often rely on single metrics that obscure the minimal transformations needed to align representations. Understanding the precise nature of these transformations is critical for dissecting how representations in different networks relate to each other (\emph{e.g.}, are they similar in information content, representational geometry, or even at the level of single-neuron tuning?). It is also unknown whether the hierarchical (layer-wise correspondence) results hold true for other metrics with more restricted invariances than affine transformations.

\paragraph{Across training:} A second key question concerns the evolution of representational convergence over the course of training. Most studies compare networks at their converged state. Yet, a deeper understanding requires examining not only the final representations, but also the learning trajectories that lead there. Conventionally, it is assumed that as different networks optimize on a task, their internal representations become more similar, driven by the final task solution. This assumption is the basis for the \emph{contravariance principle}~\citep{cao2021explanatory}, which posits that when a network is pushed to achieve a challenging task, there is less room for variation in the final solution, forcing representations to converge. However, the question of when representational convergence occurs \emph{during} training remains underexplored. Understanding this dynamic can illuminate the roles of initialization, early data statistics, architectural biases, learning dynamics, and the final task solution in shaping alignment. Previous studies have shown a \emph{``lower layers learn first''}~\citep{raghu2017svcca} behavior by comparing layers across time on CIFAR-10 using SVCCA, but little else is known about the dynamics of convergence, especially for more complex vision networks and using other metrics. 

\paragraph{Across distribution shifts:} Although many deep neural networks exhibit highly human-like responses to in-distribution stimuli, there is mounting evidence that their responses can diverge dramatically under out-of-distribution (OOD) conditions~\citep{prasad2022exploring, geirhos2021partial, geirhos2018generalisation}. Despite this, the effect of OOD inputs on model-to-model representational convergence remains poorly understood. Exploring this axis is crucial for rigorously testing the universality of learned representations.

\subsection*{Key contributions}  In this work, we address these gaps by systematically investigating representational convergence along these three axes. First, we employ three alignment metrics---linear regression, Procrustes analysis, and permutation-based methods---each with different restrictions on the freedom of the mapping function, to identify the minimal set of transformations needed to align representations across networks reasonably well for each layer. This approach allows us to dissect how representations relate to each other, whether in terms of information content, representational geometry, or single-neuron tuning. Second, we examine the temporal dynamics of convergence during training, revealing that nearly all alignment occurs within the first epoch, challenging the assumption that convergence is tied to task-specific learning. Finally, we explore how changes in input statistics affect representational alignment across layers, demonstrating how OOD inputs differentially affect later versus early layers.  






