%%%%%%%% ICML 2025 EXAMPLE LATEX SUBMISSION FILE %%%%%%%%%%%%%%%%%

\documentclass{article}

\usepackage{amsmath}
\usepackage{enumitem}
% Recommended, but optional, packages for figures and better typesetting:
\usepackage{microtype}
\usepackage{graphicx}
\usepackage{subfigure}
\usepackage{booktabs} % for professional tables
\usepackage{amsmath}% hyperref makes hyperlinks in the resulting PDF.
% If your build breaks (sometimes temporarily if a hyperlink spans a page)
% please comment out the following usepackage line and replace
% \usepackage{icml2025} with \usepackage[nohyperref]{icml2025} above.
\usepackage{hyperref}


% Attempt to make hyperref and algorithmic work together better:
\newcommand{\theHalgorithm}{\arabic{algorithm}}

% Use the following line for the initial blind version submitted for review:
% \usepackage{icml2025}

% If accepted, instead use the following line for the camera-ready submission:
\usepackage[accepted]{icml2025}
% \usepackage{icml2025}

% For theorems and such
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{amsthm}
\usepackage{multirow}
% if you use cleveref..
\usepackage[capitalize,noabbrev]{cleveref}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% THEOREMS
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\theoremstyle{plain}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{assumption}[theorem]{Assumption}
\theoremstyle{remark}
\newtheorem{remark}[theorem]{Remark}

% Todonotes is useful during development; simply uncomment the next line
%    and comment out the line below the next line to turn off comments
%\usepackage[disable,textsize=tiny]{todonotes}
\usepackage[textsize=tiny]{todonotes}
\DeclareMathOperator*{\argmin}{argmin}

% The \icmltitle you define below is probably too long as a header.
% Therefore, a short form for the running title is supplied here:
\icmltitlerunning{Robust Federated Finetuning of LLMs via Alternating Optimization of LoRA}

\begin{document}

\twocolumn[
\icmltitle{Robust Federated Finetuning of LLMs via Alternating Optimization of LoRA}

% It is OKAY to include author information, even for blind
% submissions: the style file will automatically remove it for you
% unless you've provided the [accepted] option to the icml2025
% package.

% List of affiliations: The first argument should be a (short)
% identifier you will use later to specify author affiliations
% Academic affiliations should list Department, University, City, Region, Country
% Industry affiliations should list Company, City, Region, Country

% You can specify symbols, otherwise they are numbered in order.
% Ideally, you should not use this facility. Affiliations will be numbered
% in order of appearance and this is the preferred way.
\icmlsetsymbol{equal}{*}

\begin{icmlauthorlist}
\icmlauthor{Shuangyi Chen}{yyy,equal}
\icmlauthor{Yuanxin Guo}{yyy,xxx,equal}
\icmlauthor{Yue Ju}{xxx}
\icmlauthor{Hardik Dalal}{xxx}
\icmlauthor{Ashish Khisti}{yyy}
\end{icmlauthorlist}

\icmlaffiliation{yyy}{ECE Department, University of Toronto, Toronto, Canada}
\icmlaffiliation{xxx}{Ericsson-GAIA Montréal, Canada}

\icmlcorrespondingauthor{Shuangyi Chen}{shuangyi.chen@mail.utoronto.ca}
\icmlcorrespondingauthor{Ashish Khisti}{akhisti@ece.utoronto.ca}
% You may provide any keywords that you
% find helpful for describing your paper; these are used to populate
% the "keywords" metadata in the PDF but will not be shown in the document
\icmlkeywords{Machine Learning, ICML}

\vskip 0.3in
]

% this must go after the closing bracket ] following \twocolumn[ ...

% This command actually creates the footnote in the first column
% listing the affiliations and the copyright notice.
% The command takes one argument, which is text to display at the start of the footnote.
% The \icmlEqualContribution command is standard text for equal contribution.
% Remove it (just {}) if you do not need this facility.

%\printAffiliationsAndNotice{}  % leave blank if no need to mention equal contribution
\printAffiliationsAndNotice{\icmlEqualContribution} % otherwise use the standard text.

\begin{abstract}
Parameter-Efficient Fine-Tuning (PEFT) methods like Low-Rank Adaptation (LoRA) optimize federated training by reducing computational and communication costs.  We propose RoLoRA, a federated framework using alternating optimization to fine-tune LoRA adapters. Our approach emphasizes the importance of learning up and down projection matrices to enhance expressiveness and robustness. We use both theoretical analysis and extensive experiments to demonstrate the advantages of RoLoRA over prior approaches that either generate imperfect model updates or limit expressiveness of the model. We present theoretical analysis on a simplified linear model to demonstrate the importance of learning both down-projection and up-projection matrices in LoRA. 
We provide extensive experimental evaluations on a toy neural network on MNIST as well as large language models including RoBERTa-Large, Llama-2-7B on diverse tasks to demonstrate the advantages of RoLoRA over other methods.
\end{abstract}


\section{Introduction}
\label{intro}
The remarkable performance of large language models (LLMs) stems from their ability to learn at scale. With their broad adaptability and extensive scope, LLMs depend on vast and diverse datasets to effectively generalize across a wide range of tasks and domains. Federated learning \cite{mcmahan2017communication} offers a promising solution for leveraging data from multiple sources, which could be  particularly advantageous for LLMs.

Recently, Parameter-Efficient Fine-Tuning (PEFT) has emerged as an innovative training strategy that updates only a small subset of model parameters, substantially reducing computational and memory demands. A notable method in this category is LoRA \cite{hu2021lora}, which utilizes low-rank matrices to approximate weight changes during fine-tuning. These matrices are integrated with pre-trained weights for inference, facilitating reduced data transfer in scenarios such as federated learning, where update size directly impacts communication efficiency. Many works integrate LoRA into federated setting \cite{zhang-etal-2023-fedpetuning,babakniya2023slora, kuang2023federatedscopellm, chen2024robustfederatedfinetuningfoundation, sun2024improving}.  FedPETuning \cite{zhang-etal-2023-fedpetuning} compares various PEFT methods in a federated setting. SLoRA   \cite{babakniya2023slora} presents a hybrid approach that combines sparse fine-tuning with LoRA to address data heterogeneity in federated settings. Furthermore, FS-LLM \cite{kuang2023federatedscopellm} presents  a framework for fine-tuning LLMs in federated environments. However, these studies typically apply the FedAVG algorithm directly to LoRA modules, resulting in in-exact model updates, as we will discuss later in the paper. 
% \vspace{-0.1cm}
\begin{figure*}[tp] 
    \centering
    \includegraphics[width=0.76\textwidth]{rolora-overview.png} % Adjust width as needed
    \caption{RoLoRA framework overview.}
    \label{fig:overview} % Allows referencing the figure with \ref{fig:example}
\end{figure*}



To address the issue of in-exact model updates, a few recent works have proposed modifications to the down-projection and up-projection components in LoRA. In FlexLoRA \cite{bai2024federatedfinetuninglargelanguage}, the authors propose updating these projections with matrix multiplication followed by truncated SVD. A related method is also considered in \cite{wang2024florafederatedfinetuninglarge}. Another approach, by Sun et al., introduces a federated finetuning framework named FFA-LoRA \cite{sun2024improving}, which builds on LoRA by freezing the down-projection matrices across all clients and updating only the up-projection matrices. They apply differential privacy \cite{dwork2006calibrating} to provide privacy guarantees for clients' data. With a sufficient number of finetuning parameters, FFA-LoRA, using a larger learning rate, can achieve performance comparable to FedAVG of LoRA while reducing communication costs by half. However, we observe that with fewer finetuning parameters, FFA-LoRA is less robust than FedAVG of LoRA, primarily due to its reduced expressiveness from freezing down-projections. In this work, we explore the necessity of learning down-projection matrices and propose a federated fine-tuning framework with computational and communication advantages.
% While previous works \cite{hao2024floralowrankadapterssecretly, zhu2024asymmetrylowrankadaptersfoundation, zhang2023lorafa} consider similar methods of freezing down-projection matrices as in FFA-LoRA within a centralized setting, we aim to investigate the necessity of learning down-projection matrices from multiple data sources.

We connect the objective of learning down-projection matrices in a federated setting to multitask linear representation learning (MLRL), an approach in which a shared low-rank representation is jointly learned across multiple tasks. While, to the best of our knowledge, the alternating optimization of down- and up-projection matrices has not been explored within the context of LoRA, prior works on MLRL \cite{pmlr-v139-collins21a, thekumparampil2021sampleefficientlinearmetalearning} have demonstrated the importance of alternately updating low-rank representations and task-specific heads, demonstrating the necessity of learning a shared representation. Inspired by MLRL, we tackle this challenge by employing alternating optimization for LoRA adapters. We theoretically establish that alternating updates to the two components of LoRA, while maintaining a common global model, enable effective optimization of down-projections and ensure convergence to the global minimizer in a tractable setting.

% \begin{figure*}[tp] 
%     \centering
%     \includegraphics[width=0.8\textwidth]{rolora-overview.png} % Adjust width as needed
%     \caption{RoLoRA framework overview.}
%     \label{fig:overview} % Allows referencing the figure with \ref{fig:example}
% \end{figure*}

\subsection{Main Contributions}
\begin{itemize}[noitemsep]
    \item \textbf{RoLoRA framework.} We propose RoLoRA, a robust federated fine-tuning framework based on the alternating optimization of LoRA as shown in Figure~\ref{fig:overview}. RoLoRA fully leverages the expressiveness of LoRA adapters while keeping the computational and communication advantages. 
    \item \textbf{Theoretical Insights.} We show that in a tractable setting involving a local linear model, RoLoRA converges exponentially to the global minimizer when clients solve linear regression problems, using rank-1 LoRA adapters. In this case, RoLoRA is reduced to an alternating minimization-descent approach, outperforming FFA-LoRA, whose fixed down-projection limits performance. This highlights the importance of training the down-projection in LoRA for improved federated learning performance.
    \item \textbf{Empirical results.} Through evaluations on a two-layer neural network with MNIST and on large language models (RoBERTa-Large, Llama-2-7B) across various tasks (GLUE, HumanEval, MMLU, Commonsense reasoning tasks), we demonstrate that RoLoRA maintains robustness against reductions in fine-tuning parameters and increases in client numbers compared to prior approaches.
\end{itemize}

\subsection{Notations}
\label{subsec:notation}
We adopts the notation that lower-case letters represent scalar variables, lower-case bold-face letters denote column vectors, and upper-case bold-face letters denote matrices. The $d \times d$ identity matrix is represented by $\mathbf{I}_d$. Depending on the context, $\lVert .\rVert$ denotes the $l_2$ norm of a vector or the Frobenius norm of a matrix, $\lVert .\rVert_{op}$ denotes the operator norm of a matrix, $|. |$ denotes the absolute value of a scalar, ${}^\top$ denotes matrix or vector transpose. For a number $N$, $[N] = \{1,\dots, N\}$.

\section{Related Works} \label{related-works}
\paragraph{Parameter Efficient Fine Tuning (PEFT): LoRA and Its Enhancements}
 Parameter efficient finetuning (PEFT) allows for updates to a smaller subset of parameters, significantly reducing the computational and memory requirements. One of the most well-known methods is LoRA\cite{hu2021lora}.
LoRA uses low-rank matrices to approximate changes in weights during fine-tuning, allowing them to be integrated with pre-trained weights before inference. In \cite{zhang2023lorafa}, the authors propose a memory-efficient fine-tuning method, LoRA-FA, which keeps the projection-down weight fixed and updates the projection-up weight during fine-tuning. In \cite{zhu2024asymmetrylowrankadaptersfoundation}, the authors highlight the asymmetry between the projection-up and projection-down matrices and focus solely on comparing the effects of freezing either the projection-up or projection-down matrices. \cite{hao2024floralowrankadapterssecretly} introduces the idea of resampling the projection-down matrices, aligning with our observation that freezing projection-down matrices negatively impacts a model's expressiveness. Furthermore, \cite{hayou2024lora} explore the distinct roles of projection-up and projection-down matrices, enhancing performance by assigning different learning rates to each. 
%Their theoretical analysis confirms that the optimal approach involves using a lower learning rate for projection-down matrices, rather than freezing the projection-down matrices, further supporting our findings. 
% \cite{zhang2023adalora} designs AdaLoRA by using SVD decomposition and pruning less significant singular values for more efficient updates. VeRA \cite{kopiczko2024vera} is proposed to further reduce the number of trainable parameters during finetuning by using a single pair of low-rank matrices shared across all layers and learning small scaling vectors. Liu et al. analyze magnitude and directional updates in LoRA versus full parameter fine-tuning and introduce DoRA\cite{liu2024dora}, which decomposes pre-trained weights for fine-tuning and applies LoRA for directional updates.
\paragraph{PEFT in Federated Setting}

PEFT adjusts only a few lightweight or a small portion of the total parameters for specific tasks, keeping most foundational model parameters unchanged. This feature can help reduce data transfer in federated learning, where communication depends on the size of updates. Zhang et al. \cite{zhang-etal-2023-fedpetuning} compares multiple PEFT methods in federated setting, including Adapter\cite{houlsby2019parameterefficient}, LoRA\cite{hu2021lora}, Prompt tuning\cite{liu2022ptuning} and Bit-Fit\cite{zaken2022bitfit}. SLoRA\cite{babakniya2023slora}, which combines sparse finetuning and LoRA, is proposed to address the data heterogeneity in federated setting. 
% These two works commonly apply FedAVG directly to LoRA modules, without considering that the aggregation approach introduces interference. With the above consideration, 
As discussed before, \cite{sun2024improving} design a federated finetuning framework FFA-LoRA by freezing projection-down matrices for all the clients and only updating projection-up matrices. %in FS-LLM \cite{kuang2023federatedscopellm}, a framework for finetuning LLM, is introduced. 
FLoRA \cite{wang2024florafederatedfinetuninglarge} considers clients with heterogeneous-rank LoRA adapters and proposes a federated fine-tuning approach. 

\section{Preliminaries}
\subsection{Low-Rank Adaptation: LoRA} \label{lora-federated}
Low-Rank Adaptation (LoRA) \cite{hu2021lora} fine-tunes large language models efficiently by maintaining the original model weights fixed and adding small, trainable matrices in each layer. These matrices perform low-rank decompositions of updates, reducing the number of trainable parameters. This approach is based on the finding that updates to model weights during task-specific tuning are usually of low rank, which allows for fewer parameters to be adjusted. For example, for a pre-trained weight matrix $\mathbf{W}_0 \in \mathbb{R}^{d\times d}$, the update is a low-rank product $\mathbf{A}\mathbf{B}$, where the down-projection $\mathbf{A}\in \mathbb{R}^{d\times r}$ and the up-projection $\mathbf{B} \in \mathbb{R}^{r\times d}$, with $r \ll d$. Only $\mathbf{A}$ and $\mathbf{B}$ are trainable, allowing $\mathbf{W} = \mathbf{W}_0 + \alpha \mathbf{A}\mathbf{B}$, with $\alpha$ adjusting the update's impact.
Applying LoRA in a federated setting is a practical choice. By using LoRA adapters, clients can fine-tune foundation models efficiently with limited resources. Since only these specific matrices need to be transmitted to a central server, this approach significantly reduces communication costs. This makes LoRA an advantageous solution for enhancing model performance in collaborative scenario comparing to full parameter finetuning in the federated setting. 
\subsection{FedAVG of LoRA Introduces Interference}\label{interference}
Integrating LoRA within a federated setting presents challenges. In such a setup, each of the $N$ clients is provided with the pretrained model weights $\mathbf{W}_0$, which remain fixed during finetuning. Clients are required only to send the updated matrices $\mathbf{B}_i$ and $\mathbf{A}_i$ to a central server for aggregation. While most current studies, such as SLoRA \cite{babakniya2023slora} and FedPETuning \cite{zhang-etal-2023-fedpetuning}, commonly apply FedAVG directly to these matrices as shown in \eqref{fedavg-lora}, this approach might not be optimal. The precise update for each client’s model, $\Delta \mathbf{W}_i$, should be calculated as the product of the low-rank matrices $\mathbf{A}_i$ and $\mathbf{B}_i$. Consequently, aggregation on the individual matrices leads to inaccurate model aggregation. 
{\footnotesize
\begin{align}
    \frac{1}{N} \sum_{i=1}^N  \Delta \mathbf{W}_i = \frac{1}{N} (\mathbf{A}_1\mathbf{B}_1+\mathbf{A}_2\mathbf{B}_2+...+ \mathbf{A}_N\mathbf{B}_N) \label{fedavg-mul}\\
    \neq  \frac{1}{N} (\mathbf{A_1}+\mathbf{A_2}+...+\mathbf{A_N}) \frac{1}{N} (\mathbf{B_1}+\mathbf{B_2}+...+\mathbf{B_N}) \label{fedavg-lora}
\end{align}}

There are a few options to avoid it.
\vspace{-0.2cm}
    \paragraph{Updating B and A by matrix multiplication and truncated-SVD.} One approach \cite{wang2024florafederatedfinetuninglarge, bai2024federatedfinetuninglargelanguage} involves first computing the product of local matrices $\mathbf{B}_i$ and $\mathbf{A}_i$ to accurately recover $\Delta \mathbf{W}_i$. Then, the global $\mathbf{B}$ and $\mathbf{A}$ of next iteration are obtained by performing truncated SVD on the averaged set of ${\Delta \mathbf{W}_i}$. However, this method introduces computational overhead due to the matrix multiplication and SVD operations.
    \vspace{-0.2cm}
    \paragraph{Freezing A (B) during finetuning.} Another method is to make clients freeze $\mathbf{B}$ or $\mathbf{A}$ as in Sun et al. \cite{sun2024improving}, leading to precise computation of ${\Delta \mathbf{W}}$. However, this method limits the expressiveness of the adapter.




With these considerations, we propose a federated finetuning framework, named RoLoRA, based on alternating optimization of LoRA.
\section{RoLoRA Framework} \label{rolora-framework}
In this section, we describe the framework design of RoLoRA and discuss its practical advantages. 
\vspace{-0.1cm}
\paragraph{Alternating Optimization and Corresponding Aggregation}
Motivated by the observations discussed in Section~\ref{interference}, we propose applying alternating optimization to the local LoRA adapters of each client in a setting with $N$ clients. Unlike the approach in FFA-LoRA, where $\mathbf{A}$ is consistently frozen, we suggest an alternating update strategy. There are alternating odd and even communication rounds designated for updating, aggregating $\mathbf{A}$ and $\mathbf{B}$, respectively. {\footnotesize
\begin{align}
 &\text{In the odd comm. round:} \qquad  \frac{1}{N} \sum_{i=1}^N  \Delta \mathbf{W}_{i}^{2t+1} \nonumber\\&= \frac{1}{N} (\mathbf{A}_1^{t}\mathbf{B}_1^{t+1}+\mathbf{A}_2^{t}\mathbf{B}_2^{t+1}+...+ \mathbf{A}_N^{t}\mathbf{B}_N^{t+1}) \label{round-t}\\
  &= \frac{1}{N} \mathbf{A}^t(\mathbf{B}_1^{t+1}+\mathbf{B}_2^{t+1}+...+ \mathbf{B}_N^{t+1})  \nonumber\\
  & \text{In the even comm. round:} \quad  \frac{1}{N} \sum_{i=1}^N  \Delta \mathbf{W}_{i}^{2t+2} \nonumber \\&= \frac{1}{N} (\mathbf{A}_1^{t+1}\mathbf{B}_1^{t+1}+\mathbf{A}_2^{t+1}\mathbf{B}_2^{t+1}+...+ \mathbf{A}_N^{t+1}\mathbf{B}_N^{t+1}) \label{roundt+1}\\
&= \frac{1}{N} (\mathbf{A}_1^{t+1}+\mathbf{A}_2^{t+1}+...+ \mathbf{A}_N^{t+1})\mathbf{B}^{t+1} \nonumber
\end{align}
} 

As in Algorithm~\ref{alg:rolora-llm}, all clients freeze $\mathbf{A}^t$ and update $\mathbf{B}^{t}$ in the odd communication round. The central server then aggregates these updates to compute $\mathbf{B}^{t+1} = \frac{1}{N}\sum_{i=1}^{N}\mathbf{B}^{t+1}_i$ and distributes $\mathbf{B}^{t+1}$ back to the clients. In the subsequent communication round, clients freeze $\mathbf{B}^{t+1}$ and update $\mathbf{A}^{t}$. The server aggregates these to obtain $\mathbf{A}^{t+1} = \frac{1}{N}\sum_{i=1}^{N}\mathbf{A}^{t+1}_i$ and returns $\mathbf{A}^{t+1}$ to the clients. 
It is important to note that in round $2t+1$, the frozen $\mathbf{A}_i^t$ are identical across all clients, as they are synchronized with $\mathbf{A}^t$ from the central server at the beginning of the round. This strategy ensures that the update and aggregation method introduces no interference, as demonstrated in \eqref{round-t} and \eqref{roundt+1}.
\begin{algorithm}[]
   \caption{RoLoRA iterations}
   \label{alg:rolora-llm}
\begin{algorithmic}[1]
   \STATE {\bfseries Input:} number of iterations $T$, number of clients $N$
   % \STATE Initialize /$\mathbf{a}^0 \sim \mathcal{N}(0,\sigma^2), b^0=0$
   \FOR{$t=1$ {\bfseries to} $T$}
   \FOR{$i=1$ {\bfseries to} $N$}
   \STATE Fix $\mathbf{A}^{t}$, $\mathbf{B}_i^{t+1} = \text{GD-update}(\mathbf{A}^{t},\mathbf{B}^{t})$
   \STATE Transmits $\mathbf{B}_i^{t+1}$ to server
   \ENDFOR
    \STATE Server aggregates $\mathbf{B}^{t+1}=\frac{1}{N}\sum_{i=1}^{N}\mathbf{B}_i^{t+1}$, broadcasts $\mathbf{B}^{t+1}$
   \FOR{$i=1$ {\bfseries to} $N$}
   \STATE Fix $\mathbf{B}^{t+1}$, $\mathbf{A}_i^{t+1} = \text{GD-update}(\mathbf{A}^{t},\mathbf{B}^{t+1})$
   \STATE Transmits $\mathbf{A}_i^{t+1}$ to server
   \ENDFOR
    \STATE Server aggregates $\mathbf{A}^{t+1}=\frac{1}{N}\sum_{i=1}^{N}\mathbf{A}_i^{t+1}$, broadcasts $\mathbf{A}^{t+1}$
   \ENDFOR
\end{algorithmic}
\end{algorithm}

\vspace{-0.2cm}
\paragraph{Computation and Communication Cost}
The parameter-freezing nature of RoLoRA enhances computational and communication efficiency. In each communication round, the number of trainable parameters in the model is effectively halved compared to FedAVG of LoRA. The only additional cost for RoLoRA compared to FFA-LoRA is the alternating freezing of the corresponding parameters. We remark this additional cost is negligible because it is applied to the clients' models and can be executed concurrently during the server's aggregation.

\section{Analysis} \label{sec:analysis}
In this section, we provide an intuitive analysis of the necessity of training down-projection of LoRA module in a federated setting.  We first theoretically compare RoLoRA and FFA-LoRA on a linear model. Then we empirically verify the effectiveness of the theoretical analysis on a two-layer neural network.
\subsection{Federated LoRA on a Toy Model} 
\label{setup-analysis-homo}
Consider a federated setting with $N$ clients, each with the following local linear model
\begin{equation}
    f_i(\mathbf{X}_i)=\mathbf{X}_i\mathbf{a}\mathbf{b}^\top \label{vec-vec-rank1}
\end{equation}
where $\mathbf{Y}_i \in \mathbb{R}^{m \times d}$, $ \mathbf{X}_i \in \mathbb{R}^{m \times d}$ with the sample size $m$, $\mathbf{a} \in \mathbb{R}^{d}$ (a unit vector) and $\mathbf{b} \in \mathbb{R}^{d}$ are the LoRA weights corresponding to rank $r=1$. In this setting, we model the local data of $i$-th client such that 
\begin{align}
\mathbf{Y}_i=\mathbf{X}_i\mathbf{a}^*\mathbf{b}^{*^\top} \label{data-gen}
\end{align}
for some ground truth LoRA weights $\mathbf{a}^* \in \mathbb{R}^{d}$ (a unit vector) and $\mathbf{b}^* \in \mathbb{R}^d$. We consider the following objective
\begin{equation}
    \min_{\mathbf{a}\in \mathbb{R}^{d},\mathbf{b}\in \mathbb{R}^d} \frac{1}{N}\sum_{i=1}^N l_i(\mathbf{a},\mathbf{b}) \label{obj-xi}
\end{equation}
where the local loss is $l_i(\mathbf{a},\mathbf{b}) = \frac{1}{m}\lVert  \mathbf{X}_i\mathbf{a}^*\mathbf{b}^{*^\top}- \mathbf{X}_i\mathbf{a}\mathbf{b}^\top\rVert^2$. Each $\mathbf{X}_i$ is assumed to be a Gaussian random matrix, where each entry is independently and identically distributed according to a standard Gaussian distribution.

 We remind the reader that Section~\ref{subsec:notation} provides a summary of mathematical notations and also point to
 Table~\ref{tab:notation} in Appendix~\ref{sec:notation} for a summary of the symbols used throughout the theoretical analysis.
 
\paragraph{Results.}
In this section, we assume homogeneous clients where there is a single target model as in \eqref{data-gen}. In the special case with the model as in \eqref{vec-vec-rank1} and the objective in \eqref{obj-xi}, we modify RoLoRA from Algorithm~\ref{alg:rolora-llm} to Algorithm~\ref{alg:rolora-linear}, employing alternating minimization for $\mathbf{b}$ (line 5) and gradient descent for $\mathbf{a}$ (line 9). Details are described in Algorithm~\ref{alg:rolora-linear}.  We note that the analysis of the alternating minimization-gradient descent algorithm is inspired by  \cite{pmlr-v139-collins21a,seyedehsara2022fastsampleefficientfederatedlow,Vaswani_2024} for a different setting of MLRL.
\begin{algorithm}[]
   \caption{RoLoRA for linear regressor, Alt-min-GD iterations}
   \label{alg:rolora-linear}
\begin{algorithmic}[1]
   \STATE {\bfseries Input:} GD Step size $\eta$, number of iterations $T$, number of clients $N$
   % \STATE Initialize /$\mathbf{a}^0 \sim \mathcal{N}(0,\sigma^2), b^0=0$
   \FOR{$t=1$ {\bfseries to} $T$}
   \STATE Let $\mathbf{a} \leftarrow$ $\mathbf{a}^{t-1}, \mathbf{b} \leftarrow$ $\mathbf{b}^{t-1}$.
   \FOR{$i=1$ {\bfseries to} $N$}
   \STATE  set $\tilde{\mathbf{b}}_i \leftarrow \argmin_{\mathbf{b}} l_i(\mathbf{a},\mathbf{b})$
   \ENDFOR
    \STATE $\Bar{\mathbf{b}}=\frac{1}{N}\sum_{i=1}^{N}\tilde{\mathbf{b}}_i$
   \FOR{$i=1$ {\bfseries to} $N$}
   \STATE Compute $\nabla_{\mathbf{a}} l_i(\mathbf{a},\Bar{\mathbf{b}})$
   \ENDFOR
    \STATE $\hat{\mathbf{a}}^{+} \leftarrow \mathbf{a}-\frac{\eta}{N}\sum_{i=1}^{N}\nabla_{\mathbf{a}} l_i(\mathbf{a},\Bar{\mathbf{b}}),~ \hat{\mathbf{a}} \leftarrow \frac{\hat{\mathbf{a}}^{+}}{\lVert \hat{\mathbf{a}}^{+}\rVert}$
    \STATE $\mathbf{a}^t \leftarrow \hat{\mathbf{a}},~ \mathbf{b}^t\leftarrow  \Bar{\mathbf{b}}$
   \ENDFOR
\end{algorithmic}
\end{algorithm}
%The key differences between our algorithm and theirs lies in the handling of $\mathbf{b}_i$. While they maintain diversity in $\mathbf{b}_i$, our algorithm allows the server to aggregate $\mathbf{b}_i$, ensuring that a common global model is shared with clients during training.  
We aim to show that the training procedure described in Algorithm~\ref{alg:rolora-linear} learns the target model $(\mathbf{a}^*,\mathbf{b}^*)$. First, we make typical assumptions on the ground truth $\mathbf{b}^*$.

\begin{assumption}  \label{client-norm-1}
     There exists $L_{max} < \infty$ (known a priori),  s.t. $\lVert \mathbf{b}^*\rVert \leq  L_{max}$.
\end{assumption} 
Next, to obtain the convergence results, we define the angle distance between two unit vectors.

\begin{definition} (Angle Distance)
    For two unit vectors $\mathbf{a}, \mathbf{a}^* \in \mathbb{R}^{d}$, the angle distance between $\mathbf{a}$ and $\mathbf{a}^*$ is defined as 
    \begin{equation}
        |\sin \theta ({\mathbf{a}, \mathbf{a}^*})| = \lVert (\mathbf{I}_d- \mathbf{a} \mathbf{a}^\top)\mathbf{a}^*\rVert
    \end{equation}
    where $\mathbf{I}_d- \mathbf{a} \mathbf{a}^\top$ is the projection operator to the direction orthogonal to $\mathbf{a}$.
\end{definition}
Let $\delta^t = \lVert (\mathbf{I}_d-\mathbf{a}^*\mathbf{a}^{*^\top})\mathbf{a}^t\rVert = \lVert (\mathbf{I}_d-\mathbf{a}^t\mathbf{a}^{t^\top})\mathbf{a}^*\rVert$ denote the angle distance between $\mathbf{a}^{*}$ and $\mathbf{a}^t$ of $t$-th iteration. We initialize $\mathbf{a}^0$ such that $|\sin{\theta}(\mathbf{a}^*,\mathbf{a}^0)|= \delta_0$, where $0<\delta_0<  1$, and $b^0$ is zero. All clients obtain the same initialization for parameters. We show that the algorithm learns the target model by showing the angle distance between $\mathbf{a}$ and $\mathbf{a}^*$ is decreasing in each iteration. Now we are ready to state our main results. 

% \begin{lemma} \label{lemma:delta-t}
%     Let $\delta^t = |\sin{\theta}(\mathbf{a}^*,\mathbf{a}^t)|$. Assume that Assumption~\ref{client-norm-1} holds and $\delta^t \leq \delta^{t-1} \leq \dots \leq \delta^0$. For any $t$ and $\eta \leq \frac{1}{L_{max}^2}$, if $ m= \Omega(q)$ for $q=\max\left(\frac{\log(N)}{[\min(\epsilon_1,\epsilon_2)]^2},\frac{d\log(\frac{2}{\epsilon_0})}{\epsilon_2^2}\right)$, then with probability at least $1-2q^{-10}$ for , we have
%     \begin{align}
%           \delta^{t+1} \leq \delta^{t} \sqrt{1-\eta (1-(\delta^{0})^2)L_{max}^2}    
%     \end{align}
% \end{lemma}

\begin{lemma}\label{lemma:delta-t}
    Let $\delta^t = \lVert (\mathbf{I}_d-\mathbf{a}^*\mathbf{a}^{*^\top})\mathbf{a}^t\rVert $ be the angle distance between $\mathbf{a}^{*}$ and $\mathbf{a}^{t}$ of $t$-th iteration. Assume that Assumption~\ref{client-norm-1} holds and $\delta^t \leq \delta^{t-1} \leq \dots \leq \delta^0$. Let $m$ be the number of samples for each updating step, let auxiliary error thresholds $\epsilon'=\frac{\epsilon_2}{(1-\epsilon_0)(1-\epsilon_1)},\tilde{\epsilon}=\frac{\epsilon_3}{1-\epsilon_0}$ for $\epsilon_0, \epsilon_1, \epsilon_2, \epsilon_3 \in (0,1)$, if $
    m= \Omega(q)$ for $q=\max\left(\frac{\log(N)}{[\min(\epsilon_1,\epsilon_2)]^2},\frac{d\log(\frac{2}{\epsilon_0})}{\epsilon_2^2}\right)$,
    and auxiliary error thresholds are small such that $\epsilon',\tilde{\epsilon}<\frac{1-(\delta^0)^2}{16}$, for any $t$ and $\eta \leq \frac{1}{L_{max}^2}$, then we have,
    \begin{align}
        \delta^{t+1} \leq \delta^{t} \sqrt{1-\eta (1-\delta^{0^2})\|\mathbf b^*\|^2} 
    \end{align}
    with probability at least $1-2q^{-10}$.
    % with probability at least $1-2 \exp{(d\log(1+\frac{2}{\epsilon_0})-c_5\epsilon_3^2Nm)}-Np_0$.
\end{lemma}


Theorem~\ref{convergence-1} follows by recursively applying Lemma~\ref{lemma:delta-t} and taking a union bound over all $t\in[T]$.

\begin{theorem}(Convergence of RoLoRA for linear regressor in homogeneous setting) \label{convergence-1}
    Suppose we are in the setting described in Section~\ref{setup-analysis-homo} and apply Algorithm~\ref{alg:rolora-linear} for optimization. Given a random initial $\mathbf{a}^0$, an initial angle distance $\delta_0 \in (0,1)$, we set step size $\eta  \leq \frac{1}{L_{max}^2}$ and the number of iterations $T \geq \frac{2}{c(1-(\delta^{0})^2)}\log (\frac{\delta^0}{\epsilon}$), for $c \in (0,1)$. Under these conditions, if with sufficient number of samples $m = \Omega(q)$ and small auxiliary error thresholds $\epsilon'=\frac{\epsilon_2}{(1-\epsilon_0)(1-\epsilon_1)},\tilde{\epsilon}=\frac{\epsilon_3}{1-\epsilon_0},$ such that $ \epsilon', \tilde{\epsilon} < \frac{1-(\delta^0)^2}{16}$ , we achieve that with probability at least $1-2Tq^{-10}$ for $q=\max\left(\frac{\log(N)}{[\min(\epsilon_1,\epsilon_2)]^2},\frac{d\log(\frac{2}{\epsilon_0})}{\epsilon_2^2}\right)$,
\begin{align}
\sin \theta (\mathbf{a}^T, \mathbf{a}^*)\leq \epsilon \nonumber
\end{align}
which we refer to as $\epsilon$-accurate recovery. In addition, 
\begin{align}
    \lVert \mathbf{a}^T(\mathbf{b}^{T+1})^\top- \mathbf{a}^* ({\mathbf{b}}^*)^\top\rVert \leq (1+\epsilon')\epsilon \lVert  \mathbf a^* {\mathbf{b}}^{*^\top}\rVert. \nonumber 
\end{align}
\end{theorem}
% Theorem~\ref{convergence-1} follows by recursively applying Lemma~\ref{lemma:delta-t} and taking a union bound over all $t\in[T]$.
% \begin{lemma} \label{lemma:delta-t}
%     Let $\delta^t = |\sin{\theta}(\mathbf{a}^*,\mathbf{a}^t)|$. Assume that Assumption~\ref{client-norm-1} holds and $\delta^t \leq \delta^{t-1} \leq \dots \leq \delta^0$. if $\eta =\frac{1}{2L_{max}^2}$, then with high probability,
%     \begin{align}
%         |\sin \theta (\mathbf{a}^{t+1},\mathbf{a}^*)| = \delta^{t+1} \leq \delta^{t} \sqrt{\frac{1+(\delta^{0})^2}{2}}
%     \end{align}
% \end{lemma}
Theorem~\ref{convergence-1} and Lemma~\ref{lemma:delta-t} show that with a random initialization for the unit vector $\mathbf{a}$ $(\delta^0\in (0,1))$, RoLoRA makes the global model converge to the target model exponentially fast with large $q$. The requirement for sample complexity is well-supported, as demonstrated in \cite{collins2022fedavgfinetuninglocal, du2021fewshot}. 

While the proof of the above results are relegated to the Appendix, we provide a brief outline of the proof. In Appendices~\ref{sec:homo-analysis}, we first analyze the minimization step for updating ${\mathbf{b}_i^t}$ (Lemma~\ref{lemma:b-bar-g-bound}), then establish a bound on the deviation of the gradient from its expectation with respect to $\mathbf{a}$ (Lemma~\ref{lemma:grad-egrad-bound}), and finally derive a bound for $|\sin \theta (\mathbf{a}^{t+1},\mathbf{a}^*)|$ based on the gradient descent update rule for $\mathbf{a}$ (Lemma~\ref{lemma:delta-t}). The proof of Theorem~\ref{convergence-1} is in Section~\ref{subsec:mainProof}.

\paragraph{Intuition on Freezing-A Scheme (FFA-LoRA) can Saturate.}
We begin by applying the FFA-LoRA scheme to a centralized setting, aiming to solve the following optimization problem:
\begin{equation} 
    \min_{\mathbf{b} \in \mathbb{R}^d} \lVert \mathbf{X}\mathbf{a}^*\mathbf{b}^{*^\top}-\mathbf{X}\mathbf{a}^{0} \mathbf{b}^\top \rVert^2 
\end{equation}
where $\mathbf{a}^* \in \mathbb{R}^d$ and $\mathbf{b}^* \in \mathbb{R}^d$ represent the ground truth parameters, and $\mathbf{a}^0 \in \mathbb{R}^d$ is the random initialization. The objective can be transformed to $\sum_{p=i}^{d}(\mathbf{a}^*{b}^{*}_p-\mathbf{a}^{0} {b}_p)^\top \mathbf{X}^\top \mathbf{X}(\mathbf{a}^*{b}^{*}_p-\mathbf{a}^{0} {b}_p)$, with ${b}_p$ as the $p$-th entry of $\mathbf{b}$, ${b}_p^*$ as the $p$-th entry of $\mathbf{b}^*$. In FFA-LoRA scheme, $\mathbf{a}^0$ remains fixed during training. If $\mathbf{a}^0$ is not initialized to be parallel to $\mathbf{a}^*$, the objective can never be reduced to zero. This is because optimizing $\mathbf{b}$ only scales the vector $\mathbf{a}^{0} b_p$ along the direction of $\mathbf{a}^{0}$, without altering the angular distance between $\mathbf{a}^{0}$ and $\mathbf{a}^*$.

Suppose we are in the federated setting described in Section~\ref{setup-analysis-homo}, we apply FFA-LoRA, to optimize the objective in \eqref{obj-xi}. In FFA-LoRA scheme, we fix $\mathbf{a}$ of all clients to a random unit vector $\mathbf{a}^0$, where the initial angle distance $\delta^0 = |\sin \theta ( \mathbf{a}^*, \mathbf{a}^0)|, \delta^0\in (0,1)$. And we only update $\mathbf{b}_i$ by minimizing $l_i$ and aggregate them.  
\begin{proposition} \label{fa-saturate-homo}(FFA-LoRA lower bound)
Suppose we are in the setting described in Section~\ref{setup-analysis-homo}. For any set of ground truth parameters ($\mathbf{a}^*,\mathbf{b}^*$), the initialization $\mathbf{a}^0$, initial angle distance $\delta^0\in (0,1)$, we apply FFA-LoRA scheme to obtain a shared global model ($\mathbf{a}^0,{\mathbf{b}}^{FFA}$), yielding an expected global loss of 
\begin{align}
    &\mathbb{E}[\frac{1}{Nm}\sum_{i=1}^N\lVert  \mathbf{X}_i\mathbf{a}^*\mathbf{b}^{*^\top}- \mathbf{X}_i\mathbf{a}^0({{\mathbf{b}}^{FFA})^\top}\rVert^2] \nonumber \\
    &= (1+\tilde{c})\| \mathbf{b}^*\|^2(\delta^0)^2 \label{loss-ffa}
\end{align}
where the expectation is over all the randomness in the $\mathbf{X}_i$, and $\tilde{c}=O(\frac{1}{Nm})$.
\end{proposition}
% \begin{proof}
    
% \end{proof}
See Appendix~\ref{fa-heter-saturate} for the proof. Proposition~\ref{fa-saturate-homo} shows that for any choice of $\delta^0 \in (0,1)$, the global objective reached by FFA-LoRA is shown as in \eqref{loss-ffa}. The global objective of FFA-LoRA is dominated by $\|\mathbf b^*\|^2 (\delta^0)^2$  which is due to the angular distance between $\mathbf{a}^0$ and $\mathbf{a}^*$. 

By Theorem~\ref{convergence-1}, we demonstrate that RoLoRA achieves $\epsilon$-accurate recovery of the global minimizer. Specifically, the expected global loss of RoLoRA can be upper bounded by $(1+\tilde{c})\| \mathbf{b}^*\|^2\epsilon^2$. Under the same initialization and ground truth parameters for both FFA-LoRA and RoLoRA, RoLoRA's ability to update $\mathbf{a}$ reduces the global loss caused by the angle distance between $\mathbf{a}$ and $\mathbf{a}^*$ from $\| \mathbf{b}^*\|^2(\delta^0)^2$ to $\| \mathbf{b}^*\|^2\epsilon^2$. By increasing the number of iterations, $\epsilon$ can be made arbitrarily small. 

In Appendix~\ref{rank-1-heter-clients}, we analyze the convergence of RoLoRA with single rank-1 LoRA structure in a federated setting with \textit{heterogeneous} clients. By showing the decreasing of the angle distance between the ground truth $\mathbf{a}^*$ and the shared down-projection $\mathbf{a}$, we demonstrate that RoLoRA allows the global model to converge to global minimum while the global loss of FFA-LoRA can be dominated by the term caused by the angle distance between the random initialization $\mathbf{a}^0$ and $\mathbf{a}^*$.

% \paragraph{Discussion.} 

Through this analysis of the LoRA structure with rank-1, we highlight the necessity of updating the down-projections.


\subsection{Verifying Results On a Two-Layer NN}
 \vspace{-0.2cm}
\begin{figure}[h]
  \centering
  \includegraphics[width=0.84\linewidth]{mnist_noniid_iid-1.png}
  \label{fig:mnist_}
 \caption{(Left) Comparison of three methods on a toy model with 5 clients. (Right) Comparison of three methods on a toy model with 10 clients.}
    \label{fig:mnist-results}
\end{figure}
 \vspace{-0.2cm}
The previous analysis considers a simple linear model for each client. To assess the validity in a non-linear model, we consider a two-layer neural network model on each client given by
\begin{equation}
    f_i(x_i) = \textsf{ReLU}( x_i\mathbf{A}\mathbf{B})\mathbf{W}_{out}
\end{equation}
where $ \mathbf{W}_{out} \in \mathbb{R}^{d\times c}$,  $\mathbf{A}\in \mathbb{R}^{d\times r}$ and $\mathbf{B}\in\mathbb{R}^{r\times d}$ are weights. We train the model on MNIST \cite{deng2012mnist} with 60,000 training images. We consider two different ways to distribute training images to clients. The first is to distribute the images to 5 clients and each client gets access to training images of two specific labels, while the second is to distribute the images to 10 clients and each client only has training images of one specific label. There is no overlap in the training samples each client can access. Only weights matrices $\mathbf{B}$ and $\mathbf{A}$ are tunable, while $\mathbf{W}_{out}$ are fixed. We apply the typical initialization, where $\mathbf{A}$ is initialized to a random Gaussian matrix, and $\mathbf{B}$ is initialized to zero. We use $c=10, d=784, r=16$ and make each client train 5 epochs locally with batch-size 64 and aggregate clients' update following three methods: FedAVG of LoRA, referred as LoRA; FFA-LoRA \cite{sun2024improving}, which freezes $\mathbf{A}$ during training, and RoLoRA, which alternately update $\mathbf{B}$ and $\mathbf{A}$. We experiment with multiple learning rates, display the best performance of each method in Figure~\ref{fig:mnist-results}.

 As shown in Figure~\ref{fig:mnist-results}, we evaluate the performance of the model in each iteration on the test set with 10,000 images. 
We observe that the accuracy of FFA-LoRA plateaus around 55\% in both settings, which aligns with our theoretical analysis. The decline in LoRA’s performance with an increasing number of clients is most likely due to less accurate model aggregation, as demonstrated in \eqref{fedavg-mul} and \eqref{fedavg-lora}. Notably, RoLoRA demonstrates greater robustness in these settings.
\section{Experiments on LLMs}\label{exp}
In this section, we evaluate the performance of RoLoRA in various federated settings. 
% \begin{figure*}[ht]
% \begin{center}
% \begin{subfigure}
%   \centering
%   \includegraphics[width=0.185\linewidth]{icml2024/sst2.png}
%   % \caption{SST-2}
%   \label{fig:sub1}
% \end{subfigure}
% \hfill
% \begin{subfigure}
%   \centering
%   \includegraphics[width=0.185\linewidth]{icml2024/qnli.png}
%   % \caption{QNLI}
%   \label{fig:sub2}
% \end{subfigure}
% \hfill
% \begin{subfigure}
%   \centering
%   \includegraphics[width=0.185\linewidth]{icml2024/mnli.png}
%   % \caption{MNLI}
%   \label{fig:sub3}
% \end{subfigure}
% \hfill
% \begin{subfigure}
%   \centering
%   \includegraphics[width=0.185\linewidth]{icml2024/qqp.png}
%   % \caption{QQP}
%   \label{fig:sub4}
% \end{subfigure}
% \hfill
% \begin{subfigure}
%   \centering
%   \includegraphics[width=0.185\linewidth]{icml2024/rte.png}
%   % \caption{RTE}
%   \label{fig:sub5}
% \end{subfigure}
%  \caption{Results with RoBERTa-Large models on GLUE of different budget of finetuning parameters. The accuracy is computed by averaging over different ranks $\{1,2,4,8\}$. The number of clients is 3.}
%     \label{fig:five_subfigures}
% \hfill
% \end{center}
% \end{figure*}
% \begin{table*}
% {\scriptsize
%     \centering
%     \begin{tabular}{ccccccccc}
%     \toprule
%  Rank  &Methods &Comm. cost    & SST-2 & QNLI & MNLI & QQP  & RTE & Avg.\\
%       \midrule 
%   &LoRA   & $\times 16$ & $\text{95.68}_{\pm \text{0.14}}$ & $\text{91.46}_{\pm \text{0.30}}$& $\text{85.93}_{\pm \text{0.01}}$  & $\text{85.95}_{\pm \text{0.18}}$ & $\text{81.35}_{\pm \text{0.74}}$  &88.07\\
% r=8  &FFA-LoRA & $\times 8$  & $\text{94.99}_{\pm \text{0.10}}$ & $\text{91.09}_{\pm \text{0.36}}$ &$\text{85.21}_{\pm \text{0.03}}$  &$\text{85.76}_{\pm \text{0.08}}$  & $\text{80.14}_{\pm \text{1.02}}$ & 87.44\\
%   &RoLoRA & $\times 8$  & $\text{95.45}_{\pm \text{0.14}}$ &$\text{91.84}_{\pm \text{0.09}}$  & $\text{85.76}_{\pm \text{0.01}}$ &$\text{85.91}_{\pm \text{0.22}}$  & $\text{81.32}_{\pm \text{0.78}}$ &88.06\\
%   \midrule 
%   &LoRA  & $\times 8$  & $\text{95.62}_{\pm \text{0.17}}$ & $\text{91.59}_{\pm\text{0.21}}$ & $\text{86.20}_{\pm \text{0.05}}$ & $\text{86.13}_{\pm \text{0.10}}$ & $\text{81.46}_{\pm \text{1.22}}$ & 88.20\\
% r=4  &FFA-LoRA  & $\times 4$ & $\text{95.18}_{\pm \text{0.09}}$ & $\text{91.35}_{\pm \text{0.32}}$ & $\text{84.58}_{\pm \text{0.21}}$ & $\text{85.50}_{\pm \text{0.25}}$ & $\text{81.10}_{\pm \text{0.33}}$ & 87.48\\
%   &RoLoRA & $\times 4$ & $\text{95.49}_{\pm \text{0.16}}$ & $\text{91.64}_{\pm \text{0.30}}$ & $\text{85.70}_{\pm \text{0.04}}$ & $\text{86.14}_{\pm \text{0.06}}$ & $\text{82.43}_{\pm \text{0.84}}$ & 88.28\\
%   \midrule 
%   &LoRA &$\times 4$  & $\text{95.64}_{\pm \text{0.11}}$ & $\text{92.04}_{\pm \text{0.11}}$ & $\text{85.85}_{\pm \text{0.19}}$ & $\text{86.16}_{\pm \text{0.08}}$ & $\text{82.19}_{\pm \text{1.03}}$ & 88.38\\
% r=2   &FFA-LoRA&$\times 2$   & $\text{94.91}_{\pm \text{0.16}}$ & $\text{90.11}_{\pm \text{0.17}}$ & $\text{84.06}_{\pm \text{0.19}}$ & $\text{85.48}_{\pm \text{0.01}}$ & $\text{80.86}_{\pm \text{0.51}}$ & 87.08\\
%   &RoLoRA  & $\times 2$  & $\text{95.60}_{\pm \text{0.10}}$ & $\text{91.62}_{\pm \text{0.32}}$ & $\text{85.55}_{\pm \text{0.05}}$ & $\text{86.16}_{\pm \text{0.18}}$ & $\text{82.19}_{\pm \text{1.03}}$ & 88.22\\
%   \midrule 
%   &LoRA &$\times 2$ & $\text{95.32}_{\pm \text{0.18}}$ & $\text{90.48}_{\pm \text{0.56}}$ & $\text{85.08}_{\pm \text{0.04}}$ & $\text{85.01}_{\pm \text{0.05}}$ & $\text{81.10}_{\pm \text{0.95}}$&87.40\\
% r=1   &FFA-LoRA&$\times 1$   & $\text{94.49}_{\pm \text{0.22}}$ & $\text{89.87}_{\pm \text{0.37}}$ & $\text{82.60}_{\pm \text{0.03}}$ & $\text{84.42}_{\pm \text{0.50}}$ & $\text{79.66}_{\pm \text{1.08}}$ &86.21\\
%   &RoLoRA & $\times 1$ & $\text{95.22}_{\pm \text{0.14}}$ & $\text{91.01}_{\pm \text{0.23}}$ & $\text{84.97}_{\pm \text{0.05}}$ & $\text{85.24}_{\pm \text{0.18}}$ & $\text{80.23}_{\pm \text{1.02}}$ &87.33\\
%   \bottomrule
%     \end{tabular}
%     \caption{Results with RoBERTa-Large models on GLUE. We report the average and std. over five seeds. The number of clients is 3. Please refer to Table~\ref{tab:comm_size} in Appendix~\ref{ft-dynamics} for the actual communication cost.}
%     \label{tab:glue1}}
% \end{table*}
Considering all clients will participate in each round, we will explore the following methods based on FedAVG.
\begin{itemize}[noitemsep]
    \item \textbf{LoRA} means LoRA adapter and its finetuning algorithm are directly applied to local finetuning of clients in the federated system. Specifically, in iteration $t$, the server receives $\mathbf{A}_i^{t}$ and $\mathbf{B}_i^{t}$ from client $i$ and aggregates by  $\mathbf{A}^{t} = \mathsf{Avg}(\mathbf{A}_i^{t})$ and $\mathbf{B}^{t} = \mathsf{Avg}(\mathbf{B}_i^{t})$. 
    \item \textbf{LoRA-FFA} \cite{sun2024improving} is a baseline that enable the clients to finetune $\mathbf{B}$ and keep $\mathbf{A}$ frozen locally. Thus, in iteration $t$, the server aggregates by $\mathbf{B}^{t} = \mathsf{Avg}(\mathbf{B}_i^{t})$. 
    % \item \textbf{FlexLoRA} \cite{bai2024federatedfinetuninglargelanguage} allows the server to aggregate the products of $\mathbf{A}_i^{t}$ and $\mathbf{B}_i^{t}$ received from clients, compute the average, perform truncated-SVD on the average, and subsequently send the resulting low rank matrices back to the clients.
    \item \textbf{RoLoRA} enables clients to alternate updating $\mathbf{A}$ and $\mathbf{B}$ as described in Section~\ref{rolora-framework}.  
\end{itemize}

\vspace{-0.2cm}
FlexLoRA \cite{bai2024federatedfinetuninglargelanguage} fine-tunes large models in a federated setting by aggregating the matrix products of LoRA components and compressing them using truncated-SVD. However, due to its significant memory and computation overheads it is not directly comparable with other schemes. Nonetheless, we include its performance in Table~\ref{tab:Clients-num-flex-appendix} in Appendix.

\vspace{-0.2cm}
\paragraph{Implementation \& Configurations.}We implement all the methods based on FederatedScope-LLM \cite{kuang2023federatedscopellm}. We use NVIDIA GeForce RTX 4090 or NVIDIA A40 for all the experiments. To make a fair comparison, for each dataset, we obtain the best performance on test set and report the average over multiple seeds. Specifically, the learning rate is chosen from the set $\{5e-4, 1e-3, 2e-3, 5e-3, 1e-2, 2e-2, 5e-2, 1e-1\}$. Other hyper-parameters for experiments are specified in Table~\ref{tab:exp-set} in Appendix~\ref{exp-setup}. Please note that in all tasks, we compare the performance of the three methods \textit{under the same number of communication rounds}.
\subsection{Language Understanding Tasks}

\paragraph{Model and Datasets.} We take the pre-trained RoBERTa-Large (355M) \cite{liu2019roberta} models from the HuggingFace Transformers library, and evaluate the performance of three federated finetuning methods on 5 datasets (SST-2, QNLI, MNLI, QQP, RTE) from the GLUE \cite{wang2019glue}. GLUE benchmark is a comprehensive set of tasks for evaluating the performance of language models on a variety of natural language understanding tasks. Due to the limitation of the unpublished test set in GLUE, we follow the previous studies \cite{zhang-etal-2023-fedpetuning} and use the original validation set as the new test set and split a part of the training set as the validation set.

\begin{table*}[h!]
{\scriptsize
\centering
    \begin{tabular}{ccccccccc}
    \toprule
        rank & Clients num & Method & SST-2 & QNLI & MNLI & QQP & RTE & Avg. \\
        \midrule 
 & & LoRA  & $ \text{\textbf{95.62}}_{\pm \text{0.17}}$ & $\text{ 91.59}_{\pm\text{0.21}}$ & $\text{\textbf{86.20}}_{\pm \text{0.05}}$ & $\text{86.13}_{\pm \text{0.10}}$ & $\text{81.46}_{\pm \text{1.22}}$ & 88.20\\
     4   & 3 & FFA-LoRA   & $\text{95.18}_{\pm \text{0.09}}$ & $\text{91.35}_{\pm \text{0.32}}$ & $\text{84.58}_{\pm \text{0.21}}$ & $\text{85.50}_{\pm \text{0.25}}$ & $\text{81.10}_{\pm \text{0.33}}$ & 87.48\\
        &  & RoLoRA &  $\text{95.49}_{\pm \text{0.16}}$ & $\text{\textbf{91.64}}_{\pm \text{0.30}}$ & $\text{85.70}_{\pm \text{0.04}}$ & $\text{\textbf{86.14}}_{\pm \text{0.06}}$ & $\text{\textbf{82.43}}_{\pm \text{0.84}}$ & \textbf{88.28}\\
         \midrule 
           &       & LoRA & $\text{94.3}_{\pm \text{0.27}}$ & $\text{86.67}_{\pm \text{2.02}}$ &$\text{78.55}_{\pm \text{7.31}}$ &$\text{83.1}_{\pm \text{0.04}}$ & $\text{51.87}_{\pm \text{3.24}}$ & 78.90 \\
4   &      20 & FFA-LoRA & $\text{93.88}_{\pm \text{0.06}}$ & $\text{89.11}_{\pm \text{0.19}}$ & $\text{80.99}_{\pm \text{1.74}}$&$\text{83.92}_{\pm \text{0.2}}$ & $\text{57.16}_{\pm \text{1.46}}$& 80.01\\
    &     & RoLoRA & $\text{\textbf{94.88}}_{\pm \text{0.18}}$ & $\text{\textbf{90.35}}_{\pm \text{0.37}}$ & $\text{\textbf{85.28}}_{\pm \text{1.04}}$& $\text{\textbf{85.83}}_{\pm \text{0.1}}$ &$\text{\textbf{78.82}}_{\pm \text{1.7}}$ & \textbf{87.03}\\
         \midrule
    &             &LoRA&$\text{93.00}_{\pm \text{0.35}}$&$\text{78.13}_{\pm \text{5.13}}$&$\text{52.64}_{\pm \text{15.07}}$&$\text{77.60}_{\pm \text{1.47}}$&$\text{52.23}_{\pm \text{1.1}}$& 70.72\\
 4    &   50 &FFA-LoRA&$\text{93.23}_{\pm \text{0.12}}$&$\text{85.05}_{\pm \text{0.34}}$&$\text{69.97}_{\pm \text{5.57}}$&$\text{78.44}_{\pm \text{0.41}}$&$\text{55.72}_{\pm \text{1.99}}$& 76.48\\
    &     &RoLoRA&$\text{\textbf{94.80}}_{\pm \text{0.17}}$&$\text{\textbf{90.00}}_{\pm \text{0.63}}$&$\text{\textbf{82.98}}_{\pm \text{3.36}}$&$\text{\textbf{85.71}}_{\pm \text{0.18}}$&$\text{\textbf{75.57}}_{\pm \text{2.88}}$ & \textbf{85.81}\\
\midrule
        & & LoRA & $\text{93.00}_{\pm \text{0.23}}$ & $\text{79.87}_{\pm \text{1.52}}$ & $\text{56.96}_{\pm \text{2.02}}$ & $\text{77.45}_{\pm \text{1.97}}$ & $\text{53.79}_{\pm \text{6.57}}$ & 64.03\\
    8    &  50 & FFA-LoRA & $\text{92.74}_{\pm \text{0.13}}$ & $\text{83.69}_{\pm \text{0.75}}$ & $\text{64.51}_{\pm \text{1.92}}$ & $\text{79.71}_{\pm \text{2.04}}$ & $\text{53.07}_{\pm \text{1.3}}$ & 72.46\\
        &  & RoLoRA & $\text{\textbf{94.53}}_{\pm \text{0.17}}$ & $\text{\textbf{90.1}}_{\pm \text{0.45}}$ & $\text{\textbf{85.17}}_{\pm \text{0.41}}$ & $\text{\textbf{85.25}}_{\pm \text{0.13}}$ & $\text{\textbf{76.3}}_{\pm \text{4.9}}$ & \textbf{86.27}\\
 
         \bottomrule
    \end{tabular}
        \caption{Results with RoBERTa-Large models with varying client numbers (3, 20, 50), maintaining a constant sample count during fine-tuning.
        \label{tab:Clients-num}}
}
\end{table*}







 \vspace{-0.2cm}
\begin{figure}[h]
\begin{center}
\begin{subfigure}
  \centering
  \includegraphics[width=0.425\linewidth]{sst2-4-50.png}
  % \caption{SST-2}
  \label{fig:sub1-}
\end{subfigure}
\begin{subfigure}
  \centering
  \includegraphics[width=0.4\linewidth]{qnli-4-50.png}
  % \caption{QNLI}
  \label{fig:sub2-}
\end{subfigure}
\begin{subfigure}
  \centering
  \includegraphics[width=0.425\linewidth]{Mnli-4-50.png}
  % \caption{MNLI}
  \label{fig:sub3-}
\end{subfigure}
\begin{subfigure}
  \centering
  \includegraphics[width=0.4\linewidth]{QQP-4-50.png}
  % \caption{MNLI}
  \label{fig:sub4-}
\end{subfigure}
 \caption{Accuracies over rounds with RoBERTa-Large models on SST-2, QNLI, MNLI, and QQP. The total number of clients is 50. We use rank 4.}
 \label{fig:convergence-speed}
 \end{center}
 \end{figure}
  \vspace{-0.5cm}
\begin{figure*}[ht]
\begin{center}
\begin{subfigure}
  \centering
  \includegraphics[width=0.161\linewidth]{sst2.png}
  % \caption{SST-2}
  \label{fig:sub1}
\end{subfigure}
\hfill
\begin{subfigure}
  \centering
  \includegraphics[width=0.154\linewidth]{qnli.png}
  % \caption{QNLI}
  \label{fig:sub2}
\end{subfigure}
\hfill
\begin{subfigure}
  \centering
  \includegraphics[width=0.154\linewidth]{mnli.png}
  % \caption{MNLI}
  \label{fig:sub3}
\end{subfigure}
\hfill
\begin{subfigure}
  \centering
  \includegraphics[width=0.154\linewidth]{qqp.png}
  % \caption{QQP}
  \label{fig:sub4}
\end{subfigure}
\hfill
\begin{subfigure}
  \centering
  \includegraphics[width=0.154\linewidth]{rte.png}
  % \caption{RTE}
  \label{fig:sub5}
\end{subfigure}
 \caption{Results with RoBERTa-Large models on GLUE under different fine-tuning parameter budgets, involving three clients with rank 4.}
    \label{fig:five_subfigures}
\hfill
\end{center}
\end{figure*}




%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%





\paragraph{Effect of Number of Clients}
In this section, we study the effect of the number of clients. The configurations are presented in Table~\ref{tab:layer_type_index-client-num} in Appendix. In Table~\ref{tab:Clients-num}, we increased the number of clients from 3 to 20, and then to 50, ensuring that there is no overlap in the training samples each client can access. Consequently, each client receives a smaller fraction of the total dataset. We observe that as the number of clients increases, while maintaining the same number of fine-tuning samples, the performance of the LoRA method significantly deteriorates for most datasets. In contrast, RoLoRA maintains its accuracy levels. The performance of FFA-LoRA also declines, attributed to the limited expressiveness of the random initialization of $\mathbf{A}$ for clients' heterogeneous data. Notably, RoLoRA achieves this accuracy while incurring only half the communication costs associated with LoRA. Figure~\ref{fig:convergence-speed} illustrates the dynamics during finetuning for three methods, highlighting that the convergence speed of RoLoRA is substantially better than that of the other two methods.

\paragraph{Effect of Number of Finetuning Parameters}
In Figure~\ref{fig:five_subfigures}, we compare three methods across five GLUE datasets. We apply LoRA module to every weight matrix of the selected layers, given different budgets of LoRA parameters. For each dataset, we experiment with three budgets (${\mathcal{P}_1, \mathcal{P}_2, \mathcal{P}_3}$) ranging from low to high. The corresponding layer sets that are attached with LoRA adapters, ${\mathcal{P}_1, \mathcal{P}_2, \mathcal{P}_3}$, are detailed in Table~\ref{tab:layer_type_index} in Appendix~\ref{exp-setup}. The figures indicates that with sufficient number of finetuning parameters, FFA-LoRA can achieve comparable best accuracy as LoRA and RoLoRA, aligning with the results in \cite{sun2024improving}; as the number of LoRA parameters is reduced, the performance of the three methods deteriorates to varying degrees. However, RoLoRA, which achieves performance comparable to LoRA, demonstrates greater robustness compared to FFA-LoRA, especially under conditions of limited fine-tuning parameters. It is important to note that with the same finetuning parameters, the communication cost of RoLoRA and FFA-LoRA is always half of that of LoRA due to their parameter freezing nature. This implies that RoLoRA not only sustains its performance but also enhances communication efficiency. Additional results of varifying ranks are provided in Figure~\ref{fig:five_subfigures-rank-8}, \ref{fig:five_subfigures-rank-2}, and \ref{fig:five_subfigures-rank-1} in Appendix~\ref{app:num-lora-appendix}.
% We expand the middle set of data of each of Figure~\ref{fig:five_subfigures}, corresponding to $\mathcal{P}_2$, and show the details of the performance of three methods in Table~\ref{tab:glue1}. RoLoRA appears to outperform FFA-LoRA particularly at lower ranks.

% \begin{figure*}[ht]
% \begin{center}
% \begin{subfigure}
%   \centering
%   \includegraphics[width=0.185\linewidth]{icml2024/sst2.png}
%   % \caption{SST-2}
%   \label{fig:sub1}
% \end{subfigure}
% \hfill
% \begin{subfigure}
%   \centering
%   \includegraphics[width=0.185\linewidth]{icml2024/qnli.png}
%   % \caption{QNLI}
%   \label{fig:sub2}
% \end{subfigure}
% \hfill
% \begin{subfigure}
%   \centering
%   \includegraphics[width=0.185\linewidth]{icml2024/mnli.png}
%   % \caption{MNLI}
%   \label{fig:sub3}
% \end{subfigure}
% \hfill
% \begin{subfigure}
%   \centering
%   \includegraphics[width=0.185\linewidth]{icml2024/qqp.png}
%   % \caption{QQP}
%   \label{fig:sub4}
% \end{subfigure}
% \hfill
% \begin{subfigure}
%   \centering
%   \includegraphics[width=0.185\linewidth]{icml2024/rte.png}
%   % \caption{RTE}
%   \label{fig:sub5}
% \end{subfigure}
%  \caption{Results with RoBERTa-Large models on GLUE of different budget of finetuning parameters. The accuracy is computed by averaging over different ranks $\{1,2,4,8\}$. The number of clients is 3.}
%     \label{fig:five_subfigures}
% \hfill
% \end{center}
% \end{figure*}
% \begin{table*}
% {\scriptsize
%     \centering
%     \begin{tabular}{ccccccccc}
%     \toprule
%  Rank  &Methods &Comm. cost    & SST-2 & QNLI & MNLI & QQP  & RTE & Avg.\\
%       \midrule 
%   &LoRA   & $\times 16$ & $\text{95.68}_{\pm \text{0.14}}$ & $\text{91.46}_{\pm \text{0.30}}$& $\text{85.93}_{\pm \text{0.01}}$  & $\text{85.95}_{\pm \text{0.18}}$ & $\text{81.35}_{\pm \text{0.74}}$  &88.07\\
% r=8  &FFA-LoRA & $\times 8$  & $\text{94.99}_{\pm \text{0.10}}$ & $\text{91.09}_{\pm \text{0.36}}$ &$\text{85.21}_{\pm \text{0.03}}$  &$\text{85.76}_{\pm \text{0.08}}$  & $\text{80.14}_{\pm \text{1.02}}$ & 87.44\\
%   &RoLoRA & $\times 8$  & $\text{95.45}_{\pm \text{0.14}}$ &$\text{91.84}_{\pm \text{0.09}}$  & $\text{85.76}_{\pm \text{0.01}}$ &$\text{85.91}_{\pm \text{0.22}}$  & $\text{81.32}_{\pm \text{0.78}}$ &88.06\\
%   \midrule 
%   &LoRA  & $\times 8$  & $\text{95.62}_{\pm \text{0.17}}$ & $\text{91.59}_{\pm\text{0.21}}$ & $\text{86.20}_{\pm \text{0.05}}$ & $\text{86.13}_{\pm \text{0.10}}$ & $\text{81.46}_{\pm \text{1.22}}$ & 88.20\\
% r=4  &FFA-LoRA  & $\times 4$ & $\text{95.18}_{\pm \text{0.09}}$ & $\text{91.35}_{\pm \text{0.32}}$ & $\text{84.58}_{\pm \text{0.21}}$ & $\text{85.50}_{\pm \text{0.25}}$ & $\text{81.10}_{\pm \text{0.33}}$ & 87.48\\
%   &RoLoRA & $\times 4$ & $\text{95.49}_{\pm \text{0.16}}$ & $\text{91.64}_{\pm \text{0.30}}$ & $\text{85.70}_{\pm \text{0.04}}$ & $\text{86.14}_{\pm \text{0.06}}$ & $\text{82.43}_{\pm \text{0.84}}$ & 88.28\\
%   \midrule 
%   &LoRA &$\times 4$  & $\text{95.64}_{\pm \text{0.11}}$ & $\text{92.04}_{\pm \text{0.11}}$ & $\text{85.85}_{\pm \text{0.19}}$ & $\text{86.16}_{\pm \text{0.08}}$ & $\text{82.19}_{\pm \text{1.03}}$ & 88.38\\
% r=2   &FFA-LoRA&$\times 2$   & $\text{94.91}_{\pm \text{0.16}}$ & $\text{90.11}_{\pm \text{0.17}}$ & $\text{84.06}_{\pm \text{0.19}}$ & $\text{85.48}_{\pm \text{0.01}}$ & $\text{80.86}_{\pm \text{0.51}}$ & 87.08\\
%   &RoLoRA  & $\times 2$  & $\text{95.60}_{\pm \text{0.10}}$ & $\text{91.62}_{\pm \text{0.32}}$ & $\text{85.55}_{\pm \text{0.05}}$ & $\text{86.16}_{\pm \text{0.18}}$ & $\text{82.19}_{\pm \text{1.03}}$ & 88.22\\
%   \midrule 
%   &LoRA &$\times 2$ & $\text{95.32}_{\pm \text{0.18}}$ & $\text{90.48}_{\pm \text{0.56}}$ & $\text{85.08}_{\pm \text{0.04}}$ & $\text{85.01}_{\pm \text{0.05}}$ & $\text{81.10}_{\pm \text{0.95}}$&87.40\\
% r=1   &FFA-LoRA&$\times 1$   & $\text{94.49}_{\pm \text{0.22}}$ & $\text{89.87}_{\pm \text{0.37}}$ & $\text{82.60}_{\pm \text{0.03}}$ & $\text{84.42}_{\pm \text{0.50}}$ & $\text{79.66}_{\pm \text{1.08}}$ &86.21\\
%   &RoLoRA & $\times 1$ & $\text{95.22}_{\pm \text{0.14}}$ & $\text{91.01}_{\pm \text{0.23}}$ & $\text{84.97}_{\pm \text{0.05}}$ & $\text{85.24}_{\pm \text{0.18}}$ & $\text{80.23}_{\pm \text{1.02}}$ &87.33\\
%   \bottomrule
%     \end{tabular}
%     \caption{Results with RoBERTa-Large models on GLUE. We report the average and std. over five seeds. The number of clients is 3. Please refer to Table~\ref{tab:comm_size} in Appendix~\ref{ft-dynamics} for the actual communication cost.}
%     \label{tab:glue1}}
% \end{table*}
% \begin{figure*}[ht]
% \begin{center}
% \begin{subfigure}
%   \centering
%   \includegraphics[width=0.185\linewidth]{icml2024/sst-2-50.png}
%   % \caption{SST-2}
%   \label{fig:sub1-}
% \end{subfigure}
% \hfill
% \begin{subfigure}
%   \centering
%   \includegraphics[width=0.185\linewidth]{icml2024/qnli-2-50.png}
%   % \caption{QNLI}
%   \label{fig:sub2-}
% \end{subfigure}
% \hfill
% \begin{subfigure}
%   \centering
%   \includegraphics[width=0.185\linewidth]{icml2024/mnli-2-50.png}
%   % \caption{MNLI}
%   \label{fig:sub3-}
% \end{subfigure}
%  \caption{Accuracies over rounds with RoBERTa-Large models on SST-2, QNLI and MNLI development set. The total number of clients is 50.}
%     \label{fig:convergence-speed}
% \hfill
% \end{center}
% \end{figure*}

% \begin{table*}[t]
% {\scriptsize
%     \centering
%     \begin{tabular}{cccccccc}
%     \toprule
%  Rank  &Methods &Comm.    & SST-2 & QNLI & MNLI & QQP  & RTE \\
%       \midrule 
%   &LoRA   & $\times 16$ & $95.68_{\pm 0.14}$ & $91.46_{\pm 0.30}$& $85.93_{\pm 0.01}$  & $85.95_{\pm 0.18}$ & $81.35_{\pm 0.74}$  \\
% r=8  &FFA-LoRA & $\times 8$  & $94.99_{\pm 0.10}$ & $91.09_{\pm 0.36}$ &$85.21_{\pm 0.03}$  &$85.76_{\pm 0.08}$  & $80.14_{\pm 1.02}$  \\
%   &RoLoRA & $\times 8$  & $95.45_{\pm 0.14}$ &$91.84_{\pm 0.09}$  & $85.76_{\pm 0.01}$ &$85.91_{\pm 0.22}$  & $81.32_{\pm 0.78}$ \\
%   \midrule 
%   &LoRA  & $\times 8$  & $95.62_{\pm 0.17}$ & $91.59_{\pm0.21}$ & $86.20_{\pm 0.05}$ & $86.13_{\pm 0.10}$ & $81.46_{\pm 1.22}$ \\
% r=4  &FFA-LoRA  & $\times 4$ & $95.18_{\pm 0.09}$ & $91.35_{\pm 0.32}$ & $84.58_{\pm 0.21}$ & $85.50_{\pm 0.25}$ & $81.10_{\pm 0.33}$  \\
%   &RoLoRA & $\times 4$ & $95.49_{\pm 0.16}$ & $91.64_{\pm 0.30}$ & $85.70_{\pm 0.04}$ & $86.14_{\pm 0.06}$ & $82.43_{\pm 0.84}$ \\
%   \midrule 
%   &LoRA &$\times 4$  & $95.64_{\pm 0.11}$ & $92.04_{\pm 0.11}$ & $85.85_{\pm 0.19}$ & $86.16_{\pm 0.08}$ & $82.19_{\pm 1.03}$  \\
% r=2   &FFA-LoRA&$\times 2$   & $94.91_{\pm 0.16}$ & $90.11_{\pm 0.17}$ & $84.06_{\pm 0.19}$ & $85.48_{\pm 0.01}$ & $80.86_{\pm 0.51}$ \\
%   &RoLoRA  & $\times 2$  & $95.60_{\pm 0.10}$ & $91.62_{\pm 0.32}$ & $85.55_{\pm 0.05}$ & $86.16_{\pm 0.18}$ & $82.19_{\pm 1.03}$ \\
%   \midrule 
%   &LoRA &$\times 2$ & $95.32_{\pm 0.18}$ & $90.48_{\pm 0.56}$ & $85.08_{\pm 0.04}$ & $85.01_{\pm 0.05}$ & $81.10_{\pm 0.95}$\\
% r=1   &FFA-LoRA&$\times 1$   & $94.49_{\pm 0.22}$ & $89.87_{\pm 0.37}$ & $82.60_{\pm 0.03}$ & $84.42_{\pm 0.50}$ & $79.66_{\pm 1.08}$ \\
%   &RoLoRA & $\times 1$ & $95.22_{\pm 0.14}$ & $91.01_{\pm 0.23}$ & $84.97_{\pm 0.05}$ & $85.24_{\pm 0.18}$ & $80.23_{\pm 1.02}$ \\
%   \bottomrule
%     \end{tabular}
%     \caption{Results with RoBERTa-Large models on GLUE development set. We report the average and std. over five seeds. The layers attached with LoRA adapters for each dataset are shown in Appendix and the associated number of LoRA parameters used is denoted by the middle set of data on the x-axis in Figure~\ref{fig:sub1} to \ref{fig:sub5}. Higher is better for all metrics.}
%     \label{tab:glue1}}
% \end{table*}
% Acknowledgements should only appear in the accepted version.


\vspace{-0.2cm}
\paragraph{Align Communication Cost for Three Methods}

In Figure~\ref{fig:align-comm-cost}, we conduct a comparison of three methods under the constraint of identical communication costs under the assumption that the number of clients is small. To align the communication costs across these methods, two approaches are considered. The first approach involves doubling the rank of FFA-LoRA and RoLoRA, with results presented in Appendix~\ref{app:num-lora-appendix}. The second approach requires doubling the number of layers equipped with LoRA modules. In Figure~\ref{fig:align-comm-cost}, the latter strategy is employed. Specifically, for both FFA-LoRA and RoLoRA, we adjust the communication costs by doubling the number of layers equipped with LoRA modules, thereby standardizing the size of the transmitted messages. The configurations are presented in Table~\ref{tab:layer_type_index-align-comm-cost} in Appendix. Figure~\ref{fig:align-comm-cost} demonstrates that when operating within a constrained communication cost budget, the performance of RoLoRA surpasses that of the other two methods for most of the tasks. 

\begin{figure}[h]
\begin{center}
\begin{subfigure}
  \centering
  \includegraphics[width=0.357\linewidth]{qqp-exp3-4-3.png}
  % \caption{SST-2}
  \label{fig:sub1-1}
\end{subfigure}
\begin{subfigure}
  \centering
  \includegraphics[width=0.35\linewidth]{mnli-exp3-4-3.png}
  % \caption{QNLI}
  \label{fig:sub2-2}
\end{subfigure} \\
\begin{subfigure}
  \centering
  \includegraphics[width=0.35\linewidth]{qnli-exp3-4-3.png}
  % \caption{QNLI}
  \label{fig:sub2-3}
\end{subfigure}
\begin{subfigure}
  \centering
  \includegraphics[width=0.35\linewidth]{rte-exp3-4-3.png}
  % \caption{QNLI}
  \label{fig:sub2-4}
\end{subfigure}
 \caption{RoBERTa-Large accuracies on QQP, MNLI, QNLI, and RTE with specific uplink communication budget. It involves 3 clients using rank 4. Error bars reflect standard deviations.}
 \label{fig:align-comm-cost}
 \end{center}
 \end{figure}

\subsection{Commonsense Reasoning Tasks}
\paragraph{Model, Datasets.}
We evaluate RoLoRA against FFA-LoRA and LoRA on Llama-2-7B\cite{touvron2023llama2openfoundation} for commonsense reasoning tasks. The commonsense reasoning tasks include 8 sub-tasks, each provided with predefined training and testing datasets. Following the setting in \cite{hu2021lora}, we merge the training datasets from all 8 sub-tasks to create a unified training dataset, which is then evenly distributed among the clients. Evaluations are conducted individually on the testing dataset for each sub-task.

\paragraph{Results}
In Table~\ref{tab:reasoning}, we compare the results of three methods with Llama-2-7B models on 8 commonsense reasoning tasks. The configurations are presented in Appendix~\ref{sec:setup-CRT}. The performance is reported as the mean accuracy with standard deviations across 3 trials. RoLoRA consistently achieves the highest accuracy across all tasks, demonstrating significant improvements over both LoRA and FFA-LoRA. We also highlights that FFA-LoRA exhibits large performance variances across trials, such as a standard deviation of 9.55 for PIQA and 8.44 for SIQA, respectively. This significant variability is likely due to the initialization quality of parameter $\mathbf{A}$, as different initializations could lead to varying optimization trajectories and final performance outcomes as discussed in Section~\ref{sec:analysis}. Additional results on this task are presented in Table~\ref{tab:reasoning-rank2-appendix} in Appendix~\ref{sec:setup-CRT}.
% \begin{table*}
% {\scriptsize
%     \centering
%     \begin{tabular}{ccccccccc}
%     \toprule
%          & BoolQ & PIQA & SIQA & HellaSwag & WinoGrande & ARC-e & ARC-c & OBQA\\
%          \midrule
%         LoRA & $\text{34.36}_{\pm \text{16.90}}$ & $\text{42.87}_{\pm \text{14.05}}$ & $\text{19.12}_{\pm \text{4.22}}$ & $\text{26.21}_{\pm \text{1.91}}$ & $\text{47.2}_{\pm \text{0.64}}$ & $\text{10.31}_{\pm \text{5.96}}$ & $\text{9.84}_{\pm \text{6.13}}$  &$\text{12.33}_{\pm \text{7.46}}$ \\
%         FFA-LoRA & $\text{44.04}_{\pm \text{11.48}}$ & $\text{51.46}_{\pm \text{9.81}}$ & $\text{25.38}_{\pm \text{11.27}}$ & $\text{23.86}_{\pm \text{2.67}}$ & $\text{46.93}_{\pm \text{1.54}}$ & $\text{22.25}_{\pm \text{7.92}}$ & $\text{20.65}_{\pm \text{6.33}}$ & $\text{20.67}_{\pm \text{5.33}}$\\
%         RoLoRA  &  $\text{61.3}_{\pm \text{0.99}}$ & $\text{60.81}_{\pm \text{6.35}}$ & $\text{37.97}_{\pm \text{5.39}}$ & $\text{29.62}_{\pm \text{2.62}}$ & $\text{49.59}_{\pm \text{1.2}}$ & $\text{37.05}_{\pm \text{2.92}}$ & $\text{29.09}_{\pm \text{3.33}}$ & $\text{28.93}_{\pm \text{4.64}}$\\
%         \bottomrule
%     \end{tabular}
%     \caption{Results with Llama-2-7B models on commonsense reasoning tasks. The number of clients is 50.}
%     \label{tab:reasoning}}
% \end{table*}

% \begin{table*}[h]
% {\scriptsize
%     \centering
%     \begin{tabular}{ccccccccc}
%     \toprule
%          & BoolQ & PIQA & SIQA & HellaSwag & WinoGrande & ARC-e & ARC-c & OBQA\\
%          \midrule
%         LoRA & $\text{61.42}_{\pm \text{0.29}}$ & $\text{33.19}_{\pm \text{9.8}}$ & $\text{31.88}_{\pm \text{3.95}}$ & $\text{21.23}_{\pm \text{2.82}}$ & $\text{31.36}_{\pm \text{5.02}}$ & $\text{27.36}_{\pm \text{0.89}}$ & $\text{32.03}_{\pm \text{1.14}}$  &$\text{26.07}_{\pm \text{2.32}}$ \\
%         FFA-LoRA & $\text{53.43}_{\pm \text{4.3}}$ & $\text{35.49}_{\pm \text{9.55}}$ & $\text{10.63}_{\pm \text{8.44}}$ & $\text{11.81}_{\pm \text{4.53}}$ & $\text{1.61}_{\pm \text{2.14}}$ & $\text{6.88}_{\pm \text{0.42}}$ & $\text{7.93}_{\pm \text{0.89}}$ & $\text{15.0}_{\pm \text{5.41}}$\\
%         RoLoRA  &  $\text{\textbf{61.83}}_{\pm \text{0.22}}$ & $\text{\textbf{61.26}}_{\pm \text{3.3}}$ & $\text{\textbf{39.76}}_{\pm \text{0.41}}$ & $\text{\textbf{27.49}}_{\pm \text{2.34}}$ & $\text{\textbf{47.67}}_{\pm \text{0.75}}$ & $\text{\textbf{33.19}}_{\pm \text{1.29}}$ & $\text{\textbf{40.13}}_{\pm \text{1.73}}$ & $\text{\textbf{31.67}}_{\pm \text{1.4}}$\\
%         \bottomrule
%     \end{tabular}
%     \caption{Results with Llama-2-7B models on commonsense reasoning tasks. This involves 50 clients using rank 8.}
%     \label{tab:reasoning}}
% \end{table*}
 \vspace{-0.2cm}
\begin{table}[h]
{\scriptsize
    \centering
    \begin{tabular}{ccccc}
    \toprule
         & BoolQ & PIQA & SIQA & HellaSwag \\
         \midrule
        LoRA & $\text{61.42}_{\pm \text{0.29}}$ & $\text{33.19}_{\pm \text{9.8}}$ & $\text{31.88}_{\pm \text{3.95}}$ & $\text{21.23}_{\pm \text{2.82}}$  \\
        FFA-LoRA & $\text{53.43}_{\pm \text{4.3}}$ & $\text{35.49}_{\pm \text{9.55}}$ & $\text{10.63}_{\pm \text{8.44}}$ & $\text{11.81}_{\pm \text{4.53}}$ \\
        RoLoRA  &  $\text{\textbf{61.83}}_{\pm \text{0.22}}$ & $\text{\textbf{61.26}}_{\pm \text{3.3}}$ & $\text{\textbf{39.76}}_{\pm \text{0.41}}$ & $\text{\textbf{27.49}}_{\pm \text{2.34}}$ \\
        \midrule
        & WinoGrande & ARC-e & ARC-c & OBQA \\
        \midrule
         LoRA &  $\text{31.36}_{\pm \text{5.02}}$ & $\text{27.36}_{\pm \text{0.89}}$ & $\text{32.03}_{\pm \text{1.14}}$  &$\text{26.07}_{\pm \text{2.32}}$ \\
        FFA-LoRA &  $\text{1.61}_{\pm \text{2.14}}$ & $\text{6.88}_{\pm \text{0.42}}$ & $\text{7.93}_{\pm \text{0.89}}$ & $\text{15.0}_{\pm \text{5.41}}$\\
        RoLoRA  &  $\text{\textbf{47.67}}_{\pm \text{0.75}}$ & $\text{\textbf{33.19}}_{\pm \text{1.29}}$ & $\text{\textbf{40.13}}_{\pm \text{1.73}}$ & $\text{\textbf{31.67}}_{\pm \text{1.4}}$\\
        \bottomrule
    \end{tabular}
    \caption{Results with Llama-2-7B models on commonsense reasoning tasks. This involves 50 clients using rank 8.}
    \label{tab:reasoning}}
\end{table}
 \vspace{-0.5cm}



% \subsection{Language Generation Tasks}
% % \paragraph{Model, Datasets and Metrics.}
% % We evaluate the performance of three federated finetuning methods with the model Llama-2-7B \cite{touvron2023llama2openfoundation}, on two datasets: CodeAlpaca \cite{codealpaca} for coding tasks, and Alpaca \cite{alpaca} for general instruction-following tasks. Using HumanEval \cite{chen2021codex} as the metric for CodeAlpaca, we assess the model’s ability to generate accurate code solutions. For Alpaca, we employ MMLU (Massive Multitask Language Understanding) \cite{hendrycks2021measuringmassivemultitasklanguage} to evaluate the model's performance across diverse domains. This provides an assessment of Llama-2-7B's coding proficiency, general language capabilities and the reasoning when finetuning in the federated setting.

% \paragraph{Results}
% Table ~\ref{llama-results} presents the evaluation results of the Llama-2-7B model using three methods, across two tasks: HumanEval, and MMLU. The metrics reported include the average and standard deviation of performance over five seeds, with 50 clients involved. The configurations are presented in Section~\ref{sec:setup-LGT} in Appendices. The results show that RoLoRA achieves the highest scores across most metrics, demonstrating slightly improved performance compared to LoRA and FFA-LoRA. The improvements are more evident in certain subcategories of the MMLU dataset. 

% \begin{table}[]
% {\scriptsize
% \centering
% \begin{tabular}{lccc}
% \toprule
%                            & LoRA & FFA-LoRA & RoLoRA \\ \midrule
% HumanEval                  &   $\text{12.96}_{\pm \text{0.37}}$    &     $\text{13.29}_{\pm \text{0.21}}$     &   
% $\text{13.45}_{\pm \text{0.28}}$ \\ \midrule
% MMLU                       &   $\text{45.81}_{\pm \text{0.03}}$   &   $\text{45.80}_{\pm \text{0.02}}$        &     $\text{45.93}_{\pm \text{0.01}}$     \\
% \multicolumn{1}{r}{human}  &  $\text{43.26}_{\pm \text{0.04}}$    &         $\text{43.24}_{\pm \text{0.01}}$   &   $\text{43.46}_{\pm \text{0.02}}$     \\
% \multicolumn{1}{r}{other}  &  $\text{52.67}_{\pm \text{0.06}}$    &   $\text{52.72}_{\pm \text{0.05}}$       &       $\text{52.83}_{\pm \text{0.04}}$   \\
% \multicolumn{1}{r}{social} &    $\text{51.73}_{\pm \text{0.04}}$    &           $\text{51.64}_{\pm \text{0.05}}$ &        $\text{51.81}_{\pm \text{0.04}}$ \\
% \multicolumn{1}{r}{stem}   &   $\text{37.10}_{\pm \text{0.03}}$   &  $\text{37.12}_{\pm \text{0.01}}$        &      $\text{37.05}_{\pm \text{0.02}}$   \\  
% % \midrule
% % GSM8K                      &   $\text{13.82}_{\pm \text{0.03}}$   &     $\text{14.00}_{\pm \text{0.01}}$     &  $\text{14.07}_{\pm \text{0.04}}$     \\
% \bottomrule
% \end{tabular}
% \caption{Results with Llama-2-7B model on HumanEval, and MMLU. We report the average and std. over five seeds. The number of clients is 50. The metric used across all tasks is accuracy, where higher values indicate better performance.}
% \label{llama-results}}
% \end{table}

\paragraph{More results.} We include more experimental results on Llama-2-7B on HumanEval and MMLU tasks in Appendix~\ref{sec:setup-LGT}. 


\section{Conclusion}
In this work, we introduce RoLoRA, a federated framework that leverages alternating optimization to finetune LoRA adapters. Our approach highlights the role of learning down-projection matrices to enhance both expressiveness and robustness. Through theoretical analysis on a simplified linear model, and comprehensive experiments on a toy neural network and large language models like RoBERTa-Large and Llama-2-7B, we show that RoLoRA outperforms existing methods that limit model updates or expressiveness.
 % In this work, we introduce RoLoRA, a federated framework that leverages alternating optimization to fine-tune LoRA adapters. Our approach highlights the critical role of learning down-projection matrices to enhance both expressiveness and robustness. Through theoretical analysis and extensive experiments, we demonstrate that RoLoRA outperforms existing methods, which often produce suboptimal model updates or constrain model expressiveness. Using a simplified linear model, we theoretically validate the significance of jointly learning both down-projection matrices in LoRA. Additionally, we conduct comprehensive experimental evaluations on a toy neural network with MNIST and on large language models, including RoBERTa-Large and Llama-2-7B, across diverse tasks, showcasing the superiority of RoLoRA over other approaches.
% \section*{Impact Statement}
% This paper presents work whose goal is to advance the field of Machine Learning. There are many potential societal consequences of our work, none of which we feel must be specifically highlighted here.

\bibliography{example_paper}
\bibliographystyle{icml2025}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% APPENDIX
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage
\appendix
\onecolumn
\section{Theoretical Analysis}
\subsection{Notation}\label{sec:notation}
Table~\ref{tab:notation} provides a summary of the symbols used throughout this theoretical analysis.
\begin{table}[h]
    \centering
    \begin{tabular}{c|c}
    \toprule
        Notation & Description \\
        \midrule
      $\mathbf{a}^*, \mathbf{b}_i^*$   & Ground truth parameters of client $i$\\
       $ \Bar{\mathbf{b}}^*$  & Average of $\mathbf{b}_i^*$\\
       $\mathbf{a}^t, \mathbf{b}^t$  & Global model parameters of $t$-th iteration \\
        $\delta_t$ & The angle distance between $\mathbf{a}^*$ and $\mathbf{a}^t$, $|\sin \theta (\mathbf{a}^*,\mathbf{a}^t)|$\\
        $\eta$  & Step size\\
        $\mathbf{I_d}$ & $d \times d$ identity matrix \\
        $\lVert \cdot\rVert$ & $l_2$ norm of a vector\\
        $\|\cdot\|_{op}$ & Operator norm ($l_2$ norm) of a matrix\\
        $|\cdot|$ & Absolute value of a scalar\\
        $\|\cdot\|_{\psi_2}$ & Sub-Gaussian norm of a sub-Gaussian random variable\\   
        $\|\cdot\|_{\psi_1}$ & Sub-exponential norm of a sub-exponential random variable\\   
        $N $ & Total number of clients \\
        $\hat{\mathbf{a}}^{+}$ & Updated $\mathbf{a}$ by gradient descent\\
        $\hat{\mathbf{a}}$ & Normalized  $\hat{\mathbf{a}}^{+}$\\
        $\Bar{\mathbf{b}} $  & Average of $\mathbf{b}_i$\\
        \bottomrule
    \end{tabular}
    \caption{Notations}
    \label{tab:notation}
\end{table}



% \subsection{Vector-scalar case with IID clients}
% Consider a federated setting with $N$ clients,  each with the following local linear model
% \begin{equation}
%     f_i(\mathbf{X}_i)=\mathbf{X}_i\mathbf{a}b 
% \end{equation}
% where $\mathbf{y}_i \in \mathbb{R}^{m}$, $ \mathbf{X}_i \in \mathbb{R}^{m \times d}$ with sample size $m$, $\mathbf{a} \in \mathbb{R}^{d}$ (a unit vector) and ${b} \in \mathbb{R}$ are the LoRA weights corresponding to rank $r=1$. In this setting, we model the local data of $i$-th client such that $\mathbf{y}_i=\mathbf{X}_i\mathbf{a}^*b^*$, for some ground truth LoRA weights $\mathbf{a}^* \in \mathbb{R}^{d}$ (a unit vector) and ${b}^* \in \mathbb{R}$. We consider the following objective
% \begin{equation}
%     \min_{\mathbf{a}\in \mathbb{R}^{d},b\in \mathbb{R}} \frac{1}{N}\sum_{i=1}^N l_i(\mathbf{a},b) 
% \end{equation}
% where the local population loss $l_i(\mathbf{a},b) = \lVert  \mathbf{X}_i\mathbf{a}^*b^*- \mathbf{X}_i\mathbf{a}b\rVert^2$. Each $\mathbf{X}_i$ is assumed to be a Gaussian random matrix, where each entry is independently and identically distributed according to a standard Gaussian distribution. Following Algorithm~\ref{alg:rolora-linear}, we start by computing the update for $b_i$. We drop superscript $t$ for simplicity. First, let $g = \mathbf{a}^\top \mathbf{a}^* b^*$, we get:
% \begin{align}
%     b_i &= \frac{\mathbf{a}^\top \mathbf{X}_i^\top \mathbf{X}_i \mathbf{a}^* b^* }{\mathbf{a}^\top \mathbf{X}_i^\top\mathbf{X}_i \mathbf{a}} \\
%     & = \frac{\mathbf{a}^\top \mathbf{X}_i^\top \mathbf{X}_i \mathbf{a}\mathbf{a}^\top \mathbf{a}^* b^* + \mathbf{a}^\top \mathbf{X}_i^\top \mathbf{X}_i (\mathbf{I}_d - \mathbf{a}\mathbf{a}^\top) \mathbf{a}^* b^*}{\mathbf{a}^\top \mathbf{X}_i^\top\mathbf{X}_i \mathbf{a}} \\
%     & = g + \frac{ \mathbf{a}^\top \mathbf{X}_i^\top \mathbf{X}_i (\mathbf{I}_d - \mathbf{a}\mathbf{a}^\top) \mathbf{a}^* b^*}{\mathbf{a}^\top \mathbf{X}_i^\top\mathbf{X}_i \mathbf{a}}.
% \end{align}
% Therefore, 
% \begin{equation}\label{difference_bi_g}
%     |b_i - g|  \leq |\mathbf{a}^\top \mathbf{X}_i^\top\mathbf{X}_i \mathbf{a}|^{-1}\cdot |\mathbf{a}^\top \mathbf{X}_i^\top \mathbf{X}_i (\mathbf{I}_d - \mathbf{a}\mathbf{a}^\top)\mathbf{a}^*b^*| = \|\mathbf{X}_i \mathbf{a}\|^{-2}\cdot |\mathbf{a}^\top \mathbf{X}_i^\top \mathbf{X}_i (\mathbf{I}_d - \mathbf{a}\mathbf{a}^\top)\mathbf{a}^*b^*|.
% \end{equation}
% Note that since each entry of $\mathbf{X}_i$ is independent and identically distributed according to a standard Gaussian, and $\lVert \mathbf{a}\rVert = 1$, $\mathbf{X}_i \mathbf{a} $ is a random standard Gaussian vector. By Theorem 3.1.1 of \cite{vershynin2018high}, the following is true for any $\epsilon_1 \in (0,1)$
% \begin{align}
%     \mathbb{P} \left\{ \lVert \mathbf{X}_i \mathbf{a}\rVert^2 \leq (1-\epsilon_1)m\right\} \leq \exp{(- \frac{c_1  \epsilon_1^2 m}{K^4})} \label{denominator-bound}
% \end{align}
% where $K = \|\xi\|_{\psi_2} \ge 1$ for $\xi \sim \mathcal N(0,1)$ and $c_1$ is some absolute constant. Next we upper bound $|\mathbf{a}^\top \mathbf{X}_i^\top \mathbf{X}_i (\mathbf{I}_d - \mathbf{a}\mathbf{a}^\top)\mathbf{a}^*b^*|$. We use $\mathbf{x}_{i,j}$ to denote the $j$-th column of $\mathbf{X}_{i}$.
% \begin{align}
%     |\mathbf{a}^\top \mathbf{X}_i^\top \mathbf{X}_i (\mathbf{I}_d - \mathbf{a}\mathbf{a}^\top)\mathbf{a}^*b^*| = \left|\sum_{j=1}^{d} (\mathbf{a}^\top \mathbf{x}_{i,j})(\mathbf{x}_{i,j}^\top(\mathbf{I}_d - \mathbf{a}\mathbf{a}^\top)\mathbf{a}^*b^*)\right| \label{numerator}
% \end{align}
% Consider that $\mathbb{E}[\mathbf{a}^\top \mathbf{X}_i^\top \mathbf{X}_i (\mathbf{I}_d - \mathbf{a}\mathbf{a}^\top)\mathbf{a}^*b^*] = \mathbf{a}^\top \mathbb{E}[\mathbf{X}_i^\top \mathbf{X}_i] (\mathbf{I}_d - \mathbf{a}\mathbf{a}^\top)\mathbf{a}^*b^*=0$.The summands on the right-hand side of \eqref{numerator} are products of sub-Gaussian random variables, making them sub-exponential. Using the inequality where $\|\xi \upsilon\|_{\psi_1} \le \|\xi\|_{\psi_2} \|\upsilon\|_{\psi_2}$ for sub-Gaussian random variables $\xi,\upsilon$, we have that 
% \[
% \|\mathbf{a}^\top \mathbf{x}_{i,j}^\top \mathbf{x}_{i,j} (\mathbf{I}_d - \mathbf{a}\mathbf{a}^\top)\mathbf{a}^*b^*\|_{\psi_1} \le c_2 \cdot \| (\mathbf I_d - \mathbf{aa}^\top) \mathbf a^*  b^*\|
% \]
% for all $i, j$, where $c_2$ is some absolute constant. Furthermore, these summands are mutually independent and have zero mean. By applying sub-exponential Bernstein's inequality (Theorem 2.8.1 of \cite{vershynin2018high}) with $t=\epsilon_2 m \lVert (\mathbf{I}_d - \mathbf{a}\mathbf{a}^\top) \mathbf{a}^*b^*\rVert$, where $\epsilon_2 \in (0,1)$, we get
% \begin{align}
%     \mathbb{P} \left\{ \left|\sum_{j=1}^{d} (\mathbf{a}^\top \mathbf{x}_{i,j})(\mathbf{x}_{i,j}^\top(\mathbf{I}_d - \mathbf{a}\mathbf{a}^\top)\mathbf{a}^*b^*)\right|  \geq \epsilon_2 m \lVert (\mathbf{I}_d - \mathbf{a}\mathbf{a}^\top ) \mathbf{a}^*b^*\rVert\right\}  \leq 2\exp{(-c_3 \epsilon_2^2 m )}   \label{numerator-bound}
% \end{align}
% for some absolute constant $c_3$. Combining \eqref{difference_bi_g},\eqref{denominator-bound} and \eqref{numerator-bound}, we get
% \begin{align}
%      \mathbb{P} \left\{|b_i-g| \le \frac{\epsilon_2}{1-\epsilon_1} \|(\mathbf{I}_d - \mathbf{a}\mathbf{a}^\top ) \mathbf{a}^*b^*\| \right\} \geq 1- 2\exp{(-c_3 \epsilon_2^2 m )} - \exp{(- \frac{c_1 \epsilon_1^2 m}{K^4})} 
% \end{align}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Auxiliary}

\begin{definition}[Sub-Gaussian Norm]
    The sub-Gaussian norm of a random variable $\xi $, denoted as $ \|\xi\|_{\psi_2} $, is defined as:
\[
\|\xi\|_{\psi_2} = \inf \{ t > 0 : \mathbb{E}[\exp(\xi^2 / t^2)] \leq 2 \}.
\]
A random variable is said to be \textit{sub-Gaussian} if its sub-Gaussian norm is finite. Gaussian random variables are sub-Gaussian. The sub-Gaussian norm of a standard Gaussian random variable $\xi \sim \mathcal N(0,1)$ is $\| \xi \|_{\psi_2} = \sqrt{8/3}$.
\end{definition}

\begin{definition}[Sub-Exponential Norm]
    The sub-exponential norm of a random variable \( \xi \), denoted as \( \|\xi\|_{\psi_1} \), is defined as:
\[
\|\xi\|_{\psi_1} = \inf \{ t > 0 : \mathbb{E}[\exp(|\xi| / t)] \leq 2 \}.
\]
\end{definition}

A random variable is said to be \textit{sub-exponential} if its sub-exponential norm is finite.

\begin{lemma}[The product of sub-Gaussians is sub-exponential] \label{lemma:sub-exp-sub-gau}
    Let $\xi$ and $\upsilon$ be sub-Gaussian random variables. Then $\xi \upsilon$ is sub-exponential. Moreover, 
    \[
    \| \xi \upsilon \|_{\psi_1} \leq \| \xi \|_{\psi_2} \cdot \| \upsilon \|_{\psi_2}
    \]
\end{lemma}

\begin{lemma}[Sum of independent sub-Gaussians] \label{lemma:sum-indep-sub-gau}
    Let $X_1,\cdots, X_N$ be independent mean-zero sub-Gaussian random variables. Then $\sum_{i=1}^N X_i$ is also sub-Gaussian with 
    \[
    \left\| \sum_{i=1}^N X_i \right\|^2_{\psi_2} \le C \sum_{i=1}^N \|X_i\|^2_{\psi_2} ,
    %\le  C \left(\sum_{i=1}^N \|X_i\|_{\psi_2}\right)^2
    \]
    where $C$ is some absolute constant. 
\end{lemma}
\begin{proof}
    See proof of Lemma 2.6.1 of \cite{vershynin2018high}.
\end{proof}

\begin{corollary}\label{cor:inn-prod-sub-gau}
    For random vector $\mathbf x\in \mathbb R^d$ with entries being independent standard Gaussian random variables, the inner product $\mathbf a^\top \mathbf x$ is sub-Gaussian for any fixed $\mathbf a \in \mathbb R^d$, and 
    \[
    \left\| \mathbf a^\top \mathbf x \right\|_{\psi_2} \le C'\|\mathbf a\|
    \]
    where $C'$ is some absolute constant.
\end{corollary}

\begin{proof}
    Note that $\mathbf a^\top \mathbf x = \sum_{i = 1}^d a_i \xi_i$, where $\xi_i \sim \mathcal N(0,1)$ is the $i$-th entry of the random vector $\mathbf x$. Choose $C$ to be the absolute constant specified in Lemma \ref{lemma:sum-indep-sub-gau} for standard Gaussian random variables, and set $C' = \sqrt{8C/3}$. We have
    \[
    \left\| \mathbf a^\top \mathbf x \right\|^2_{\psi_2} \le C \sum_{i=1}^N \|a_i\xi_i\|^2_{\psi_2} \overset{(a)}{=} C \sum_{i=1}^N a_i^2 \|\xi_i\|^2_{\psi_2} \overset{(b)}{=} \frac{8}{3}\cdot C \|\mathbf a\|^2~~\Rightarrow~~ \left\| \mathbf a^\top \mathbf x \right\|_{\psi_2} \le \sqrt{\frac{8C}{3}}\|\mathbf a\| = C'\|\mathbf a\|.
    \]    
    Step (a) makes use of the homogeneity of the sub-Gaussian norm, and step (b) uses the fact that $\|\xi\|_{\psi_2} = \sqrt{8/3}$ for $\xi \sim \mathcal N(0,1)$.
\end{proof}

\begin{definition}[$\epsilon$-net]
Consider a subset $\mathcal A \subseteq \mathbb R^d$ in the $d$-dimensional Euclidean space. Let $\epsilon > 0$. A subset $\mathcal N \subseteq \mathcal A$ is called an $\epsilon$-net of $\mathcal A$ if every point of $\mathcal A$ is within a distance $\epsilon$ of some point in $\mathcal N$, i.e., 
\[
\forall\,\mathbf x \in \mathcal A, ~\exists\, \mathbf x' \in \mathcal N, ~\| \mathbf x-\mathbf x'\| \le \epsilon.
\]
    
\end{definition}

\begin{lemma}[Computing the operator norm on a net] \label{eps-net}
Let $\mathbf{a} \in \mathbb{R}^d$ and $\epsilon \in [0,1)$. Then, for any $\epsilon$-net $\mathcal{N}$ of the sphere  $\mathcal{S}^{d-1}$, we have
\[
    \lVert \mathbf{a} \rVert \leq \frac{1}{1-\epsilon} \sup_{\mathbf{x} \in \mathcal{N}} \langle \mathbf{a}, \mathbf{x}\rangle 
\]
\end{lemma}
\begin{proof}
    See proof of Lemma 4.4.1 of \cite{vershynin2018high}.
\end{proof}

\begin{theorem}[Bernstein's inequality] \label{berstein}
Let $X_1, \dots, X_N$ be independent mean-zero sub-exponential random variables. Then, for every $t \geq 0$, we have
\[
\mathbb{P} \left( \left| \sum_{i=1}^N X_i \right| \geq t \right) \leq 2 \exp \left( -c \min \left( \frac{t^2}{\sum_{i=1}^N \| X_i \|_{\psi_1}^2}, \frac{t}{\max_i \| X_i \|_{\psi_1}} \right) \right),
\]
where $c > 0$ is an absolute constant.
\end{theorem}
\begin{proof}
    See proof of Theorem 2.8.1 of \cite{vershynin2018high}.
\end{proof}




\subsection{Vector-vector case with homogeneous clients} \label{sec:homo-analysis}
Theorem~\ref{convergence-1} follows by recursively applying Lemma~\ref{lemma:delta-t} for $T$ iterations. In Lemma~\ref{lemma:b-bar-g-bound} and Lemma~\ref{lemma:grad-egrad-bound}, we start by obtaining the important bounds that will be reused. Using Lemma~\ref{lemma:b-bar-g-bound} and Lemma~\ref{lemma:grad-egrad-bound}, based on the update rule of $\mathbf{a}$, we analyze the convergence behavior of ${\mathbf{a}}$ in Lemma~\ref{lemma:delta-t}.

\begin{lemma} \label{lemma:b-bar-g-bound}
    Let $\mathbf{a} = \mathbf{a}^t$. Let $\delta^t = \lVert (\mathbf{I}_d-\mathbf{a}^*\mathbf{a}^{*^\top})\mathbf{a}\rVert = \lVert (\mathbf{I}_d-\mathbf{a}\mathbf{a}^\top)\mathbf{a}^*\rVert$ denote the angle distance between $\mathbf{a}^{*}$ and $\mathbf{a}$. Let $\mathbf{g}^\top = \mathbf{a}^\top\mathbf{a}^* \mathbf{b}^{*^\top}, \Bar{\mathbf{b}} = \frac{1}{N}\sum_{i=1}^N{\mathbf{b}}_i$, If $m=\Omega(q)$, and $q=\max(\frac{\log(N)}{[\min(\epsilon_1,\epsilon_2)]^2},\frac{d\log(\frac{2}{\epsilon_0})}{\epsilon_2^2})$, then with probability $1-q^{-10}$,
    \begin{align}\label{eq:b_bar_g_diff}
        \lVert \Bar{\mathbf{b}}-\mathbf{g}\rVert \le \epsilon' \delta^t \lVert \mathbf{b}^* \rVert
    \end{align}
    where $\epsilon' = \frac{\epsilon_2}{(1-\epsilon_0)(1-\epsilon_{1})}$, for $\epsilon_0,\epsilon_1,\epsilon_2 \in (0,1)$.
    % and $p_0=2\exp{(d\log(1+{\textstyle\frac{2}{\epsilon_0}})-c_3 \epsilon_2^2 m )} + \exp{(- \frac{c_1 \epsilon_1^2 m}{K^4})} $.
\end{lemma}
\begin{proof}
    
 We drop superscript $t$ for simplicity. Following Algorithm~\ref{alg:rolora-linear}, we start by computing the update for $\Tilde{\mathbf{b}}_i$.  With $\mathbf{g}^\top = \mathbf{a}^\top \mathbf{a}^* \mathbf{b}^{*^\top}$, we get:
\begin{align}
    \Tilde{\mathbf{b}}_i^\top &= \frac{\mathbf{a}^\top \mathbf{X}_i^\top \mathbf{X}_i \mathbf{a}^* \mathbf{b}^{*^\top} }{\mathbf{a}^\top \mathbf{X}_i^\top\mathbf{X}_i \mathbf{a}} \\
    & = \frac{\mathbf{a}^\top \mathbf{X}_i^\top \mathbf{X}_i \mathbf{a}\mathbf{a}^\top \mathbf{a}^* \mathbf{b}^{*^\top} + \mathbf{a}^\top \mathbf{X}_i^\top \mathbf{X}_i (\mathbf{I}_d - \mathbf{a}\mathbf{a}^\top) \mathbf{a}^* \mathbf{b}^{*^\top}}{\mathbf{a}^\top \mathbf{X}_i^\top\mathbf{X}_i \mathbf{a}} \\
    & = \mathbf{g}^\top + \frac{ \mathbf{a}^\top \mathbf{X}_i^\top \mathbf{X}_i (\mathbf{I}_d - \mathbf{a}\mathbf{a}^\top) \mathbf{a}^* \mathbf{b}^{*^\top}}{\mathbf{a}^\top \mathbf{X}_i^\top\mathbf{X}_i \mathbf{a}}.
\end{align}
Therefore, 
\begin{equation}\label{difference_bi_g}
    \|\Tilde{\mathbf{b}}_i - \mathbf{g}\|  \leq |\mathbf{a}^\top \mathbf{X}_i^\top\mathbf{X}_i \mathbf{a}|^{-1}\cdot \lVert \mathbf{a}^\top \mathbf{X}_i^\top \mathbf{X}_i (\mathbf{I}_d - \mathbf{a}\mathbf{a}^\top)\mathbf{a}^*\mathbf{b}^{*^\top}\rVert = \|\mathbf{X}_i \mathbf{a}\|^{-2}\cdot \lVert \mathbf{a}^\top \mathbf{X}_i^\top \mathbf{X}_i (\mathbf{I}_d - \mathbf{a}\mathbf{a}^\top)\mathbf{a}^*\mathbf{b}^{*^\top}\rVert.
\end{equation}
Note that since each entry of $\mathbf{X}_i$ is independent and identically distributed according to a standard Gaussian, and $\lVert \mathbf{a}\rVert = 1$, $\mathbf{X}_i \mathbf{a} $ is a random standard Gaussian vector. By Theorem 3.1.1 of \cite{vershynin2018high}, the following is true for any $\epsilon_1 \in (0,1)$
\begin{align}
    \mathbb{P} \left\{ \lVert \mathbf{X}_i \mathbf{a}\rVert^2 \leq (1-\epsilon_1)m\right\} \leq \exp{(- \frac{c_1  \epsilon_1^2 m}{K^4})} \label{denominator-bound}
\end{align}
where $K = \|\xi\|_{\psi_2} \ge 1$ for $\xi \sim \mathcal N(0,1)$ and $c_1$ is some large absolute constant that makes \eqref{denominator-bound} holds. Next we upper bound $\rVert \mathbf{a}^\top \mathbf{X}_i^\top \mathbf{X}_i (\mathbf{I}_d - \mathbf{a}\mathbf{a}^\top)\mathbf{a}^*\mathbf{b}^{*^\top}\lVert$. Note that $\mathbb{E}[\mathbf{a}^\top \mathbf{X}_i^\top \mathbf{X}_i (\mathbf{I}_d - \mathbf{a}\mathbf{a}^\top)\mathbf{a}^*\mathbf{b}^{*^\top}] = \mathbf{a}^\top \mathbb{E}[\mathbf{X}_i^\top \mathbf{X}_i] (\mathbf{I}_d - \mathbf{a}\mathbf{a}^\top)\mathbf{a}^*\mathbf{b}^{*^\top}=m\mathbf{a}^\top(\mathbf{I}_d - \mathbf{a}\mathbf{a}^\top)\mathbf{a}^*\mathbf{b}^{*^\top}=0$. First we need to apply sub-exponential Berstein inequality to bound the deviation from this mean, and then apply epsilon net argument. Let $\mathcal N$ be any $\epsilon_0$-net of the unit sphere $\mathcal S^{d-1}$ in the $d$-dimensional real Euclidean space, then by Lemma~\ref{eps-net}, we have
\begin{align}
    \lVert \mathbf{a}^\top \mathbf{X}_i^\top \mathbf{X}_i (\mathbf{I}_d - \mathbf{a}\mathbf{a}^\top)\mathbf{a}^*\mathbf{b}^{*^\top}\rVert &\le \frac{1}{1-\epsilon_0} \max_{\mathbf{w}\in \mathcal N}  \mathbf{a}^\top \mathbf{X}_i^\top \mathbf{X}_i (\mathbf{I}_d - \mathbf{a}\mathbf{a}^\top)\mathbf{a}^*\mathbf{b}^{*^\top}\mathbf{w}\\
    &\le \frac{1}{1-\epsilon_0} \max_{\mathbf{w}\in \mathcal N}  |\mathbf{a}^\top \mathbf{X}_i^\top \mathbf{X}_i (\mathbf{I}_d - \mathbf{a}\mathbf{a}^\top)\mathbf{a}^*\mathbf{b}^{*^\top}\mathbf{w}| \label{eps-numerator}
\end{align}


Meanwhile, denote the $j$-th row of $\mathbf{X}_{i}$ by $\mathbf{x}_{i,j}^\top$, for every $\mathbf w \in \mathcal N$, we have
\begin{align}
    \mathbf{a}^\top \mathbf{X}_i^\top \mathbf{X}_i (\mathbf{I}_d - \mathbf{a}\mathbf{a}^\top)\mathbf{a}^*\mathbf{b}^{*^\top}\mathbf{w} = \sum_{j=1}^{m} (\mathbf{a}^\top \mathbf{x}_{i,j})(\mathbf{x}_{i,j}^\top(\mathbf{I}_d - \mathbf{a}\mathbf{a}^\top)\mathbf{a}^*\mathbf{b}^{*^\top}\mathbf{w}) \label{numerator}
\end{align}
 On the right hand side of \eqref{numerator}, $\mathbf{a}^\top \mathbf{x}_{i,j}$ and $\mathbf{x}_{i,j}^\top(\mathbf{I}_d - \mathbf{a}\mathbf{a}^\top)\mathbf{a}^*\mathbf{b}^{*^\top}\mathbf{w}$ are sub-Gaussian random variables. Thus, the summands on the right-hand side of \eqref{numerator} are products of sub-Gaussian random variables, making them sub-exponential. Now by choosing $c_2 = (C')^2$ for the $C'$ in Corollary \ref{cor:inn-prod-sub-gau}, we have the following chain of inequalities for all $i, j$:
\begin{align}
\|(\mathbf{a}^\top \mathbf{x}_{i,j})(\mathbf{x}_{i,j}^\top(\mathbf{I}_d - \mathbf{a}\mathbf{a}^\top)\mathbf{a}^*\mathbf{b}^{*^\top}\mathbf{w})\|_{\psi_1} &\le \|\mathbf{a}^\top \mathbf{x}_{i,j} \|_{\psi_2} \cdot \| \mathbf{x}_{i,j}^\top (\mathbf{I}_d - \mathbf{a}\mathbf{a}^\top)\mathbf{a}^*\mathbf b^{*^\top} \mathbf w \|_{\psi_2}\label{eq:bd_psi1_1}\\
&\le c_2\cdot \| \mathbf{a} \|  \cdot \| (\mathbf I_d - \mathbf{aa}^\top) \mathbf a^*  {\mathbf{b}^{*}}^\top \mathbf w \| \label{eq:bd_psi1_2}\\
& \le  c_2\cdot \| \mathbf{a} \|  \cdot \| (\mathbf I_d - \mathbf{aa}^\top) \mathbf a^*  {\mathbf{b}^{*}}^\top \|_{op} \|\mathbf w \|\label{eq:bd_psi1_3}\\
& \le  c_2  \cdot \| (\mathbf I_d - \mathbf{aa}^\top) \mathbf a^*  {\mathbf{b}^{*}}^\top \|_{op} \label{eq:bd_psi1_4}
%& =  c_2  \cdot \| (\mathbf I_d - \mathbf{aa}^\top) \mathbf a^* \|\cdot\| {\mathbf{b}^{*}}\|\label{eq:bd_psi1_5}
\end{align}
Equation \eqref{eq:bd_psi1_1} is due to Lemma \ref{lemma:sub-exp-sub-gau}, \eqref{eq:bd_psi1_2} is due to Corollary \ref{cor:inn-prod-sub-gau}, \eqref{eq:bd_psi1_4} is by the fact that $\|\mathbf a\| = \|\mathbf w\| = 1$.

Furthermore, these summands are mutually independent and have zero mean. By applying sub-exponential Bernstein's inequality (Theorem~\ref{berstein}) with $t=\epsilon_2 m \lVert (\mathbf{I}_d - \mathbf{a}\mathbf{a}^\top) \mathbf{a}^*\mathbf{b}^{*^\top}\rVert_{op}$, we get
\begin{align}
    &\mathbb{P} \left\{ \left| \mathbf{a}^\top \mathbf{X}_i^\top \mathbf{X}_i (\mathbf{I}_d - \mathbf{a}\mathbf{a}^\top)\mathbf{a}^*\mathbf{b}^{*^\top}\mathbf{w} \right|  \geq \epsilon_2 m \lVert (\mathbf{I}_d - \mathbf{a}\mathbf{a}^\top ) \mathbf{a}^*\mathbf{b}^{*^\top}\rVert_{op}\right\} \\&= \mathbb{P} \left\{ \left|\sum_{j=1}^{m} (\mathbf{a}^\top \mathbf{x}_{i,j})(\mathbf{x}_{i,j}^\top(\mathbf{I}_d - \mathbf{a}\mathbf{a}^\top)\mathbf{a}^*\mathbf{b}^{*^\top}\mathbf{w})\right|  \geq \epsilon_2 m \lVert (\mathbf{I}_d - \mathbf{a}\mathbf{a}^\top ) \mathbf{a}^*\mathbf{b}^{*^\top}\rVert_{op}\right\}  \\ 
    & \leq 2\exp\left(-c \min \left(\frac{\epsilon_2^2 m^2 \lVert (\mathbf{I}_d - \mathbf{a}\mathbf{a}^\top ) \mathbf{a}^*\mathbf{b}^{*^\top}\rVert_{op}^2}{\sum_{j=1}^{m} \| (\mathbf{a}^\top \mathbf{x}_{i,j}) (\mathbf{x}_{i,j}^\top(\mathbf{I}_d - \mathbf{a}\mathbf{a}^\top)\mathbf{a}^*\mathbf{b}^{*^\top}\mathbf{w})\|^2_{\psi_1}}  , \frac{\epsilon_2 m \lVert (\mathbf{I}_d - \mathbf{a}\mathbf{a}^\top ) \mathbf{a}^*\mathbf{b}^{*^\top}\rVert_{op}}{\max_j \| (\mathbf{a}^\top \mathbf{x}_{i,j}) (\mathbf{x}_{i,j}^\top(\mathbf{I}_d - \mathbf{a}\mathbf{a}^\top)\mathbf{a}^*\mathbf{b}^{*^\top}\mathbf{w})\|_{\psi_1}} \right) \right) \\
    &= 2\exp{(-c_3 \epsilon_2^2 m )}   \label{numerator-bound}
\end{align}
for any fixed $\mathbf w \in \mathcal N$, $\epsilon_2 \in (0,1)$, and some absolute constant $c_3$. \eqref{numerator-bound} follows because 
\begin{align}
    \frac{\epsilon_2^2 m^2 \lVert (\mathbf{I}_d - \mathbf{a}\mathbf{a}^\top ) \mathbf{a}^*\mathbf{b}^{*^\top}\rVert_{op}^2}{\sum_{j=1}^{m} \| (\mathbf{a}^\top \mathbf{x}_{i,j}) (\mathbf{x}_{i,j}^\top(\mathbf{I}_d - \mathbf{a}\mathbf{a}^\top)\mathbf{a}^*\mathbf{b}^{*^\top}\mathbf{w})\|^2_{\psi_1}} \geq \frac{\epsilon_2^2 m}{c_2^2} \\
    \frac{\epsilon_2 m \lVert (\mathbf{I}_d - \mathbf{a}\mathbf{a}^\top ) \mathbf{a}^*\mathbf{b}^{*^\top}\rVert_{op}}{\max_j \| (\mathbf{a}^\top \mathbf{x}_{i,j}) (\mathbf{x}_{i,j}^\top(\mathbf{I}_d - \mathbf{a}\mathbf{a}^\top)\mathbf{a}^*\mathbf{b}^{*^\top}\mathbf{w})\|_{\psi_1}} \geq \frac{\epsilon_2 m }{c_2}
\end{align}
And $\frac{\epsilon_2^2 m}{c_2^2} \leq \frac{\epsilon_2 m }{c_2}$. Now we apply union bound over all elements in $\mathcal N$. By Corollary 4.2.13 in \cite{vershynin2018high}, there exists an $\epsilon_0$-net $\mathcal N$ with $|\mathcal N| \le (\frac{2}{\epsilon_0} +1)^d$, therefore for this choice of $\mathcal N$,                                               
\begin{align}
    &\mathbb{P} \left\{ \max_{\mathbf w \in \mathcal N}\left| \mathbf{a}^\top \mathbf{X}_i^\top \mathbf{X}_i (\mathbf{I}_d - \mathbf{a}\mathbf{a}^\top)\mathbf{a}^*\mathbf{b}^{*^\top}\mathbf{w} \right|  \geq \epsilon_2 m \lVert (\mathbf{I}_d - \mathbf{a}\mathbf{a}^\top ) \mathbf{a}^*\mathbf{b}^{*^\top}\rVert_{op}\right\}\\
    &\leq \sum_{\mathbf w \in \mathcal N}\mathbb{P} \left\{ \left| \mathbf{a}^\top \mathbf{X}_i^\top \mathbf{X}_i (\mathbf{I}_d - \mathbf{a}\mathbf{a}^\top)\mathbf{a}^*\mathbf{b}^{*^\top}\mathbf{w} \right|  \geq \epsilon_2 m \lVert (\mathbf{I}_d - \mathbf{a}\mathbf{a}^\top ) \mathbf{a}^*\mathbf{b}^{*^\top}\rVert_{op}\right\}\\
    &\leq \left(\frac{2}{\epsilon_0} +1\right)^d\cdot 2\exp{(-c_3 \epsilon_2^2 m )}\\
    &= 2\exp{(d\log(1+{\textstyle\frac{2}{\epsilon_0}})-c_3 \epsilon_2^2 m )}\label{numerator-bound-2}
\end{align}                                                                        


Combining \eqref{difference_bi_g},\eqref{denominator-bound}, \eqref{eps-numerator}, and \eqref{numerator-bound-2}, we get
\begin{align}
     \mathbb{P} \left\{\lVert \Tilde{\mathbf{b}}_i-\mathbf{g}\rVert \le \epsilon' \|(\mathbf{I}_d - \mathbf{a}\mathbf{a}^\top ) \mathbf{a}^*\mathbf{b}^{*^\top}\|_{op} \right\} \geq 1-  p_0\label{difference_bi_g_bound}
\end{align}

where $\epsilon' = \frac{\epsilon_2}{(1-\epsilon_0)(1-\epsilon_{1})}$ and $p_0 = 2\exp{(d\log(1+{\textstyle\frac{2}{\epsilon_0}})-c_3 \epsilon_2^2 m )} + \exp{(- \frac{c_1 \epsilon_1^2 m}{K^4})} $. Using a union bound over $i \in [N]$, we have
\begin{align}
  \mathbb{P} \left\{\bigcap_{i=1}^N \lVert \Tilde{\mathbf{b}}_i- \mathbf{g} \rVert \leq \epsilon' \|(\mathbf{I}_d - \mathbf{a}\mathbf{a}^\top) \mathbf{a}^* \mathbf{b}^{*^\top}\|_{op} \right\} \geq 1 - Np_0.  
\end{align}
Next we bound $\lVert \mathbf{\Bar{b}}-\mathbf{g}\rVert$ where $ \Bar{\mathbf{b}}$ is the average of $\{\mathbf{b}_i\}_{i=1}^N$.
\begin{align}
\lVert \mathbf{\Bar{b}}-\mathbf{g}\rVert & = \lVert \frac{1}{N}\sum_{i=1}^{N}(\Tilde{\mathbf{b}}_i-\mathbf{g})\rVert    \\
&\leq \frac{1}{N}\sum_{i=1}^{N}\lVert\Tilde{\mathbf{b}}_i-\mathbf{g}\rVert \label{b-bar-g-difference-bound-2}\\
&\leq \epsilon' \|(\mathbf{I}_d - \mathbf{a}\mathbf{a}^\top ) \mathbf{a}^*\mathbf{b}^{*^\top}\|_{op} \\
&= \epsilon' \|(\mathbf{I}_d - \mathbf{a}\mathbf{a}^\top ) \mathbf{a}^*\| \cdot \|\mathbf{b}^{*^\top}\|\label{b-bar-g-difference-bound-4} \\
& = \epsilon' \delta^t \lVert \mathbf{b}^*\rVert \label{b-bar-g-difference-bound}
\end{align}
with probability $1-Np_0$. \eqref{b-bar-g-difference-bound-2} follows by Jensen's inequality. \eqref{b-bar-g-difference-bound-4} follows since $\|\mathbf{u}\mathbf{v}^\top\|_{op} = \| \mathbf{u}\| \cdot  \|\mathbf{v} \|$. \eqref{b-bar-g-difference-bound} follows since $\delta^t = \lVert (\mathbf{I}_d-\mathbf{a}\mathbf{a}^{\top})\mathbf{a}^*\rVert$. 

If $m=\Omega(q)$, where $q=\max(\frac{\log(N)}{[\min(\epsilon_1,\epsilon_2)]^2},\frac{d\log(\frac{2}{\epsilon_0})}{\epsilon_2^2})$, then $1-Np_0 >1-\exp(-Cq)> 1-q^{-10}$ for large absolute constant $C$. Then with probability at least $1-q^{-10}$, 
\begin{align}
    \lVert \mathbf{\Bar{b}}-\mathbf{g}\rVert \leq \epsilon' \delta^t \| \mathbf{b}^* \|
\end{align}
\end{proof}
% Based on \eqref{b-bar-g-difference-bound}, we bound $ \lVert \mathbf{a}^* \mathbf{b}^{*^\top} - \mathbf{a}\Bar{\mathbf{b}}^\top\rVert_{op}$ as follows:
% \begin{align}
%    \lVert \mathbf{a}^* \mathbf{b}^{*^\top} - \mathbf{a}\Bar{\mathbf{b}}^\top\rVert_{op} &= \lVert (\mathbf{I}_d-\mathbf{a}\mathbf{a}^\top)\mathbf{a}^*\mathbf{b}^{*^\top}+\mathbf{a}\mathbf{a}^\top\mathbf{a}^*\mathbf{b}^{*^\top}-\mathbf{a}\mathbf{\Bar{b}}^\top\rVert_{op} \\
%    & = \lVert (\mathbf{I}_d-\mathbf{a}\mathbf{a}^\top)\mathbf{a}^*\mathbf{b}^{*^\top}+\mathbf{a}(\mathbf{g}-\mathbf{\Bar{b}})^\top\rVert_{op} \\
%    & \leq \lVert (\mathbf{I}_d-\mathbf{a}\mathbf{a}^\top)\mathbf{a}^*\mathbf{b}^{*^\top}\rVert_{op} + \lVert \mathbf{a}(\mathbf{g}-\mathbf{\Bar{b}})^\top\rVert_{op} \\
%    & \leq \lVert (\mathbf{I}_d-\mathbf{a}\mathbf{a}^\top)\mathbf{a}^*\rVert\cdot\lVert \mathbf{b}^* \rVert + \lVert \mathbf{a} \rVert \cdot \lVert \mathbf{g}-\mathbf{\Bar{b}} \rVert \\
%    & \leq \delta^t \lVert \mathbf{b}^* \rVert+ \frac{(1-\epsilon_{0})\epsilon_1}{1-\epsilon_1}\delta^t \lVert \mathbf{b}^*\rVert \\
%    & = (1+\frac{(1-\epsilon_{0})\epsilon_1}{1-\epsilon_1})\delta^t \lVert \mathbf{b}^*\rVert \label{a-b-bound}
% \end{align}
% Since $\mathbf{g}^\top = \mathbf{a}^\top \mathbf{a}^*\mathbf{b}^{*^\top}$, $\lVert \mathbf{g}\rVert$ is bounded such that $\lVert \mathbf{g}\rVert = |\langle \mathbf{a},\mathbf{a}^*\rangle|\cdot \lVert \mathbf{b}^*\rVert = |\cos \theta (\mathbf{a},\mathbf{a}^*)|\cdot \lVert \mathbf{b}^*\rVert \leq \lVert \mathbf{b}^*\rVert$. Based on it, we upper bound $\lVert \Bar{\mathbf{b}}\rVert$:
% \begin{align}
%     \lVert \Bar{\mathbf{b}} \rVert & =  \lVert \Bar{\mathbf{b}} -\mathbf{g} + \mathbf{g}\rVert \\
%     & \leq \lVert\Bar{\mathbf{b}} -\mathbf{g} \rVert + \lVert\mathbf{g} \rVert \\
%     & \leq \lVert\Bar{\mathbf{b}} -\mathbf{g} \rVert + \lVert \mathbf{b}^*\rVert \\
%     &\leq \frac{(1-\epsilon_{0})\epsilon_1}{1-\epsilon_1}\delta^t \lVert \mathbf{b}^*\rVert + \lVert \mathbf{b}^*\rVert \\
%     & = (1+\frac{(1-\epsilon_{0})\epsilon_1}{1-\epsilon_1}\delta^t)\lVert \mathbf{b}^*\rVert \label{b-bar-bound}
% \end{align}



\begin{lemma} \label{lemma:grad-egrad-bound}
     Let $\mathbf{a} = \mathbf{a}^t$. Let $\delta^t = \lVert (\mathbf{I}_d-\mathbf{a}^*\mathbf{a}^{*^\top})\mathbf{a}\rVert = \lVert (\mathbf{I}_d-\mathbf{a}\mathbf{a}^\top)\mathbf{a}^*\rVert$ denote the angle distance between $\mathbf{a}^{*}$ and $\mathbf{a}$. Then for $Nm=\Omega(\frac{d\log(\frac{2}{\epsilon_0})}{\epsilon_3^2})$ and $q=\max(\frac{\log(N)}{[\min(\epsilon_1,\epsilon_2)]^2},\frac{d\log(\frac{2}{\epsilon_0})}{\epsilon_2^2})$, with probability at least $1-2q^{-10}$,
    \begin{align}
        \lVert  \nabla_{\mathbf{a}}l(\mathbf{a},\Bar{\mathbf{b}})-\mathbb{E}[\nabla_{\mathbf{a}}l(\mathbf{a},\Bar{\mathbf{b}})]\rVert \leq 2\Tilde{\epsilon} ((\epsilon')^2+\epsilon')\delta^{t}\lVert \mathbf{b}^*\rVert^2 
    \end{align}
    where $\Tilde{\epsilon} = \frac{\epsilon_3}{1-\epsilon_0}$, and $\epsilon' = \frac{\epsilon_2}{(1-\epsilon_0)(1-\epsilon_{1})}$, for $\epsilon_0,\epsilon_1,\epsilon_2,\epsilon_3 \in (0,1)$.
\end{lemma}
\begin{proof}
% Recall that $\hat{\mathbf{a}}^{+} = \mathbf{a}-\frac{\eta}{Nm} \sum_{i=1}^{N}\nabla_{\mathbf{a}}l_i(\mathbf{a},\Bar{\mathbf{b}})$.
Based on the loss function $l(\mathbf{a},\mathbf{b}) = \frac{1}{N} \sum_{i=1}^{N} l_i(\mathbf{a},\mathbf{b}) = \frac{1}{Nm} \sum_{i=1}^{N} \lVert  \mathbf{X}_i\mathbf{a}^*\mathbf{b}^{*^\top}- \mathbf{X}_i\mathbf{a}\mathbf{b}^\top\rVert^2$, we bound the expected gradient with respect to $\mathbf{a}$ and the deviation from it. The gradient with respect to $\mathbf{a}$ and its expectation are computed as:
\begin{align}
\nabla_{\mathbf{a}}l(\mathbf{a},\Bar{\mathbf{b}}) &= \frac{2}{Nm}\sum_{i=1}^{N}( \mathbf{X}_i^\top\mathbf{X}_i\mathbf{a}\Bar{\mathbf{b}}^\top \Bar{\mathbf{b}}- \mathbf{X}_i^\top\mathbf{Y}_i\Bar{\mathbf{b}} )\\
& =  \frac{2}{Nm}\sum_{i=1}^{N}(\mathbf{X}_i^\top\mathbf{X}_i\mathbf{a}\Bar{\mathbf{b}}^\top \Bar{\mathbf{b}}-\mathbf{X}_i^\top\mathbf{X}_i\mathbf{a}^* \mathbf{b}^{*^\top}\Bar{\mathbf{b}})\\
& = \frac{2}{Nm}\sum_{i=1}^{N} \mathbf{X}_i^\top\mathbf{X}_i (\mathbf{a}\Bar{\mathbf{b}}^\top-\mathbf{a}^* \mathbf{b}^{*^\top}) \Bar{\mathbf{b}} \\
\mathbb{E}[\nabla_{\mathbf{a}}l(\mathbf{a},\Bar{\mathbf{b}})]  &=  \frac{2}{Nm}\sum_{i=1}^{N} m(\mathbf{a}\Bar{\mathbf{b}}^\top-\mathbf{a}^* \mathbf{b}^{*^\top}) \Bar{\mathbf{b}} \\
& = 2(\mathbf{a}\Bar{\mathbf{b}}^\top-\mathbf{a}^* \mathbf{b}^{*^\top}) \Bar{\mathbf{b}} \label{exp-grad}
\end{align}
% Thus $\lVert\mathbb{E}[\nabla_{\mathbf{a}}l(\mathbf{a},\Bar{\mathbf{b}})] \rVert$ is upper-bounded as follows:
% \begin{align}
%     \lVert\mathbb{E}[\nabla_{\mathbf{a}}l(\mathbf{a},\Bar{\mathbf{b}})] \rVert &\leq 2 \lVert \mathbf{a}^* \mathbf{b}^{*^\top} - \mathbf{a}\Bar{\mathbf{b}}^\top \rVert_{op} \cdot \lVert \Bar{\mathbf{b}}\rVert \nonumber\\
%     &\leq 2 (1+\frac{(1-\epsilon_{0})\epsilon_1}{1-\epsilon_1})\delta^t \lVert \mathbf{b}^*\rVert \cdot (1+\frac{(1-\epsilon_{0})\epsilon_1}{1-\epsilon_1}\delta^t)\lVert \mathbf{b}^*\rVert \label{expectation-grad-1}\\
%     & \leq 2 (1+\frac{(1-\epsilon_{0})\epsilon_1}{1-\epsilon_1})^2\delta^t \lVert \mathbf{b}^*\rVert^2 \label{expectation-grad-2}
% \end{align}
% where \eqref{expectation-grad-1} follows by \eqref{a-b-bound} and \eqref{b-bar-bound}, and \eqref{expectation-grad-2} follows by $\delta^t<1$. 

Next, we bound $\lVert  \nabla_{\mathbf{a}}l(\mathbf{a},\Bar{\mathbf{b}})-\mathbb{E}[\nabla_{\mathbf{a}}l(\mathbf{a},\Bar{\mathbf{b}})]\rVert$. Construct $\epsilon_0$-net $\mathcal{N}$ over $d$ dimensional unit spheres $\mathcal{S}^{d-1}$, by Lemma~\ref{eps-net}, we have
\begin{align}
    \lVert  \nabla_{\mathbf{a}}l(\mathbf{a},\Bar{\mathbf{b}})-\mathbb{E}[\nabla_{\mathbf{a}}l(\mathbf{a},\Bar{\mathbf{b}})]\rVert &\leq \frac{1}{1-\epsilon_0} \max_{\mathbf{w}\in \mathcal{N}} \left|\mathbf{w}^\top\nabla_{\mathbf{a}}l(\mathbf{a},\Bar{\mathbf{b}})-\mathbf{w}^\top\mathbb{E}[\nabla_{\mathbf{a}}l(\mathbf{a},\Bar{\mathbf{b}})]\right| \label{grad-egrad-bound-with-w}\\
    & \le \frac{1}{1-\epsilon_0} \frac{2}{Nm}\max_{\mathbf{w}\in \mathcal{N}} \left| \sum_{i=1}^{N}\sum_{j=1}^{m}(\mathbf{x}_{i,j}^\top\mathbf{w})(\mathbf{x}_{i,j}(\mathbf{a}\Bar{\mathbf{b}}^\top-\mathbf{a}^* \mathbf{b}^{*^\top})) \Bar{\mathbf{b}} - \mathbf{w}^\top(\mathbf{a}\Bar{\mathbf{b}}^\top-\mathbf{a}^* \mathbf{b}^{*^\top}) \Bar{\mathbf{b}} \right| \label{eps-grad-egrad}
\end{align}
where $\mathbf{x}_{i,j}^\top$ is the $j$-th row of $\mathbf{X}_{i}$. Observe that $\mathbf{x}_{i,j}^\top\mathbf{w}$ and $\mathbf{x}_{i,j}(\mathbf{a}\Bar{\mathbf{b}}^\top-\mathbf{a}^* \mathbf{b}^{*^\top})\Bar{\mathbf{b}}$ are sub-Gaussian variables. Thus the product of them are sub-exponentials. For the right hand side of \eqref{eps-grad-egrad}, the summands are sub-exponential random variables with sub-exponential norm
\begin{align}
    &\| (\mathbf{x}_{i,j}^\top\mathbf{w})(\mathbf{x}_{i,j}(\mathbf{a}\Bar{\mathbf{b}}^\top-\mathbf{a}^* \mathbf{b}^{*^\top})) \Bar{\mathbf{b}} - \mathbf{w}^\top(\mathbf{a}\Bar{\mathbf{b}}^\top-\mathbf{a}^* \mathbf{b}^{*^\top}) \Bar{\mathbf{b}} \|_{\psi_1}\label{eq:bd2_psi1_1}\\
    &\leq \| (\mathbf{x}_{i,j}^\top\mathbf{w})(\mathbf{x}_{i,j}(\mathbf{a}\Bar{\mathbf{b}}^\top-\mathbf{a}^* \mathbf{b}^{*^\top})) \Bar{\mathbf{b}} \|_{\psi_1} + \| \mathbf{w}^\top(\mathbf{a}\Bar{\mathbf{b}}^\top-\mathbf{a}^* \mathbf{b}^{*^\top}) \Bar{\mathbf{b}}\|_{\psi_1}\label{eq:bd2_psi1_2}\\
    &\leq \| (\mathbf{x}_{i,j}^\top\mathbf{w})(\mathbf{x}_{i,j}(\mathbf{a}\Bar{\mathbf{b}}^\top-\mathbf{a}^* \mathbf{b}^{*^\top})) \Bar{\mathbf{b}} \|_{\psi_1} + \frac{| \mathbf{w}^\top(\mathbf{a}\Bar{\mathbf{b}}^\top-\mathbf{a}^* \mathbf{b}^{*^\top}) \Bar{\mathbf{b}}|}{\log 2}\label{eq:bd2_psi1_3}\\
    &\leq c_2\cdot \| 
    \mathbf w \|\cdot \lVert \mathbf{a}\Bar{\mathbf{b}}^\top-\mathbf{a}^* \mathbf{b}^{*^\top} \rVert_{op} \cdot \lVert \Bar{\mathbf{b}} \rVert + \frac{| \mathbf{w}^\top(\mathbf{a}\Bar{\mathbf{b}}^\top-\mathbf{a}^* \mathbf{b}^{*^\top}) \Bar{\mathbf{b}}|}{\log 2} \label{eq:bd2_psi1_4}\\
    &\leq c_2\cdot \| 
    \mathbf w \|\cdot \lVert \mathbf{a}\Bar{\mathbf{b}}^\top-\mathbf{a}^* \mathbf{b}^{*^\top} \rVert_{op} \cdot \lVert \Bar{\mathbf{b}} \rVert + \frac{\| \mathbf{w}\|\cdot\|(\mathbf{a}\Bar{\mathbf{b}}^\top-\mathbf{a}^* \mathbf{b}^{*^\top}) \Bar{\mathbf{b}}\|}{\log 2}\label{eq:bd2_psi1_5}\\
    &\leq c_2\cdot \| 
    \mathbf w \|\cdot \lVert \mathbf{a}\Bar{\mathbf{b}}^\top-\mathbf{a}^* \mathbf{b}^{*^\top} \rVert_{op} \cdot \lVert \Bar{\mathbf{b}} \rVert + \frac{\| \mathbf{w}\|\cdot\|\mathbf{a}\Bar{\mathbf{b}}^\top-\mathbf{a}^* \mathbf{b}^{*^\top}\|_{op}\cdot \| \Bar{\mathbf{b}}\|}{\log 2}\label{eq:bd2_psi1_6}\\
    &= c_4 \cdot\lVert \mathbf{a}\Bar{\mathbf{b}}^\top-\mathbf{a}^* \mathbf{b}^{*^\top} \rVert_{op} \cdot \lVert \Bar{\mathbf{b}} \rVert \label{eq:bd2_psi1_7}
\end{align}
where $c_4 = c_2 + \frac{1}{\log 2}$ is some absolute constant greater than 1. Equation \eqref{eq:bd2_psi1_3} is due to the fact that for a constant $c \in\mathbb R$, 
\[
\|c\|_{\psi_1} = \inf_t \exp\left\{\frac{|c|}{t} \le 2\right\} = \frac{|c|}{\log 2}.
\]
Equation \eqref{eq:bd2_psi1_4} is derived similarly as \eqref{eq:bd_psi1_1}-\eqref{eq:bd_psi1_3}.

The summands in \eqref{eps-grad-egrad} are mutually independent and have zero mean. Applying sub-exponential Bernstein inequality (Theorem~\ref{berstein}) with $t=\epsilon_3 Nm\lVert \mathbf{a}\Bar{\mathbf{b}}^\top-\mathbf{a}^* \mathbf{b}^{*^\top} \rVert_{op} \cdot \lVert \Bar{\mathbf{b}} \rVert $,
\begin{align}
    % &\mathbb{P} \left\{ \left|\mathbf{w}^\top\nabla_{\mathbf{a}}l(\mathbf{a},\Bar{\mathbf{b}})-\mathbf{w}^\top\mathbb{E}[\nabla_{\mathbf{a}}l(\mathbf{a},\Bar{\mathbf{b}})]\right|  \geq \epsilon_3 Nm\lVert\mathbf{a}\Bar{\mathbf{b}}^\top-\mathbf{a}^* \mathbf{b}^{*^\top} \rVert_{op}  \lVert \Bar{\mathbf{b}} \rVert \right\} = \\& 
    & \mathbb{P} \left\{  \left| \sum_{i=1}^{N}\sum_{j=1}^{m}[((\mathbf{x}_{i,j}^\top\mathbf{w})(\mathbf{x}_{i,j}(\mathbf{a}\Bar{\mathbf{b}}^\top-\mathbf{a}^* \mathbf{b}^{*^\top})) - \mathbf{w}^\top(\mathbf{a}\Bar{\mathbf{b}}^\top-\mathbf{a}^* \mathbf{b}^{*^\top}) )\Bar{\mathbf{b}}] \right| \geq \epsilon_3 Nm \lVert \mathbf{a}\Bar{\mathbf{b}}^\top-\mathbf{a}^* \mathbf{b}^{*^\top} \rVert_{op} \cdot \lVert \Bar{\mathbf{b}} \rVert  \right\} \label{berstein-2}\\
    & \leq 2\exp \left( -c \min (\frac{\epsilon_3^2Nm}{c_4^2}, \frac{\epsilon_3 N m}{c_4})\right) \\
    & = 2\exp{(-c_5\epsilon_3^2 Nm)}
\end{align}
for any fixed $\mathbf w \in \mathcal N$, $\epsilon_3 \in (0,1)$ and some absolute constant $c_5$. 

Now we apply union bound over all $\mathbf{w} \in \mathcal{N}$ using Corollary 4.2.13 of \cite{vershynin2018high}. We can conclude that 
\begin{align}
    &\mathbb{P}  \left\{ \max_{\mathbf w \in \mathcal N}   \left|\mathbf{w}^\top\nabla_{\mathbf{a}}l(\mathbf{a},\Bar{\mathbf{b}})-\mathbf{w}^\top\mathbb{E}[\nabla_{\mathbf{a}}l(\mathbf{a},\Bar{\mathbf{b}})]\right|\geq 2\epsilon_3 \lVert \mathbf{a}\Bar{\mathbf{b}}^\top-\mathbf{a}^* \mathbf{b}^{*^\top} \rVert_{op} \cdot \lVert \Bar{\mathbf{b}} \rVert  \right\} \label{eps-net-boung-grad-egrad-w}\\
     &\leq \sum_{\mathbf w \in \mathcal N} \mathbb{P}  \left\{   \left|\mathbf{w}^\top\nabla_{\mathbf{a}}l(\mathbf{a},\Bar{\mathbf{b}})-\mathbf{w}^\top\mathbb{E}[\nabla_{\mathbf{a}}l(\mathbf{a},\Bar{\mathbf{b}})]\right|\geq 2\epsilon_3 \lVert \mathbf{a}\Bar{\mathbf{b}}^\top-\mathbf{a}^* \mathbf{b}^{*^\top} \rVert_{op} \cdot \lVert \Bar{\mathbf{b}} \rVert   \right\}\\
    & \leq 2 \exp{(d\log(1+\frac{2}{\epsilon_0})-c_5\epsilon_3^2Nm)}
\end{align}
Combining \eqref{b-bar-g-difference-bound}, \eqref{grad-egrad-bound-with-w} , and \eqref{eps-net-boung-grad-egrad-w}, with probability at least $1-2 \exp{(d\log(1+\frac{2}{\epsilon_0})-c_5\epsilon_3^2Nm)}-q^{-10}$,
\begin{align}
\lVert  \nabla_{\mathbf{a}}l(\mathbf{a},\Bar{\mathbf{b}})-\mathbb{E}[\nabla_{\mathbf{a}}l(\mathbf{a},\Bar{\mathbf{b}})]\rVert &\leq \frac{1}{1-\epsilon_0} \max_{\mathbf{w}\in \mathcal{N}} \left|\mathbf{w}^\top\nabla_{\mathbf{a}}l(\mathbf{a},\Bar{\mathbf{b}})-\mathbf{w}^\top\mathbb{E}[\nabla_{\mathbf{a}}l(\mathbf{a},\Bar{\mathbf{b}})]\right| \\
% &\leq \frac{2\epsilon_3}{1-\epsilon_0} |\mathbf{w}^\top(\mathbf{a}\Bar{\mathbf{b}}^\top-\mathbf{a}^* \mathbf{b}^{*^\top} ) \Bar{\mathbf{b}} |  \label{grad-egrad-6} \\
%     &\leq \frac{2\epsilon_3}{1-\epsilon_0} \|\mathbf{w}\|\cdot \lVert\mathbf{a}\Bar{\mathbf{b}}^\top-\mathbf{a}^* \mathbf{b}^{*^\top} \rVert_{op} \cdot \lVert \Bar{\mathbf{b}} \rVert \\
   &\leq \frac{2\epsilon_3}{1-\epsilon_0} \lVert\mathbf{a}\Bar{\mathbf{b}}^\top-\mathbf{a}^* \mathbf{b}^{*^\top} \rVert_{op} \cdot \lVert \Bar{\mathbf{b}} \rVert \label{grad-egrad-4} \\
    & = \frac{2\epsilon_3}{1-\epsilon_0}  \| \mathbf{a}(\Bar{\mathbf{b}}-\mathbf{g})^\top - (\mathbf{I}_d-\mathbf{a}\mathbf{a}^\top)\mathbf{a}^*\mathbf{b}^{*^\top} \|_{op} \cdot \lVert \Bar{\mathbf{b}} \rVert
 \\ &\leq \frac{2\epsilon_3}{1-\epsilon_0} (\lVert \mathbf{a}^\top (\Bar{\mathbf{b}}-\mathbf{g} )\rVert + \| (\mathbf{I}_d-\mathbf{a}\mathbf{a}^\top)\mathbf{a}^*  \mathbf{b}^{*^\top}\|_{op}) \lVert \Bar{\mathbf{b}} \rVert  \label{grad-egrad-5}\\
 &\leq \frac{2\epsilon_3}{1-\epsilon_0} (\lVert \Bar{\mathbf{b}}-\mathbf{g} \rVert + \| (\mathbf{I}_d-\mathbf{a}\mathbf{a}^\top)\mathbf{a}^*  \| \cdot \|\mathbf{b}^{*}\|) \lVert \Bar{\mathbf{b}} \rVert\\
    & =  \frac{2\epsilon_3}{1-\epsilon_0} (\lVert \Bar{\mathbf{b}}-\mathbf{g} \rVert + \delta^t \lVert \mathbf{b}^*\rVert) \lVert \Bar{\mathbf{b}}-\mathbf{g}+\mathbf{g} \rVert \label{grad-egrad-1} \\
    & \leq \frac{2\epsilon_3}{1-\epsilon_0} (\lVert \Bar{\mathbf{b}}-\mathbf{g} \rVert + \delta^t \lVert \mathbf{b}^*\rVert)(\lVert \Bar{\mathbf{b}}-\mathbf{g} \rVert + \lVert \mathbf{g} \rVert) \\
    & \leq \frac{2\epsilon_3}{1-\epsilon_0} (\lVert \Bar{\mathbf{b}}-\mathbf{g} \rVert + \delta^t \lVert \mathbf{b}^*\rVert)(\lVert \Bar{\mathbf{b}}-\mathbf{g} \rVert + \lVert \mathbf{b}^* \rVert) \label{grad-egrad-2} \\
    &  = \frac{2\epsilon_3}{1-\epsilon_0} ( \lVert \Bar{\mathbf{b}}-\mathbf{g} \rVert^2 + \delta^t \lVert \Bar{\mathbf{b}}-\mathbf{g} \rVert  \lVert \mathbf{b}^* \rVert + \lVert \Bar{\mathbf{b}}-\mathbf{g} \rVert  \lVert \mathbf{b}^* \rVert + \delta^t  \lVert \mathbf{b}^* \rVert^2) \\
    &  \leq \frac{2\epsilon_3}{1-\epsilon_0} ((\epsilon')^2 (\delta^{t})^2 + \epsilon' (\delta^{t})^2 + \epsilon' \delta^t + \delta^t )\lVert \mathbf{b}^* \rVert^2 \\
    & \leq \frac{2\epsilon_3}{1-\epsilon_0} (\epsilon'+1)^2 \delta^t \lVert \mathbf{b}^* \rVert^2 \label{grad-egrad-3}\\
    &  = 2\Tilde{\epsilon} (\epsilon'+1)^2 \delta^t \lVert \mathbf{b}^* \rVert^2 \label{grad-egrad-final}
\end{align}


% \begin{align}
%     \lVert  \nabla_{\mathbf{a}}l(\mathbf{a},\Bar{\mathbf{b}})-\mathbb{E}[\nabla_{\mathbf{a}}l(\mathbf{a},\Bar{\mathbf{b}})]\rVert &\leq \frac{\epsilon_3}{1-\epsilon_0} \max_{\mathbf w \in \mathcal N}\lVert\mathbf{w}^\top(\mathbf{a}\Bar{\mathbf{b}}^\top-\mathbf{a}^* \mathbf{b}^{*^\top}) \rVert \cdot \lVert \Bar{\mathbf{b}} \rVert \\
%     & = \frac{\epsilon_3}{1-\epsilon_0} \max_{\mathbf w \in \mathcal N} \lVert \mathbf{w}^\top\mathbf{a}(\Bar{\mathbf{b}}-\mathbf{g})^\top-\mathbf{w}^\top(\mathbf{I}_d-\mathbf{a}\mathbf{a}^\top)\mathbf{a}^*\mathbf{b}^{*^\top} \rVert \cdot \lVert \Bar{\mathbf{b}} \rVert \label{grad-egrad-bound-2}\\
%     &\leq  \frac{\epsilon_3}{1-\epsilon_0}\lVert \mathbf{w}\rVert  \cdot \lVert \mathbf{a}\rVert \cdot \lVert \Bar{\mathbf{b}}-\mathbf{g} \rVert\cdot \lVert \Bar{\mathbf{b}} \rVert  \\
%     & = \frac{\epsilon_3}{1-\epsilon_0}\lVert \Bar{\mathbf{b}}-\mathbf{g} \rVert\cdot \lVert \Bar{\mathbf{b}}\rVert  \label{grad-egrad-final-bound} \\
%     &  = \frac{\epsilon_3}{1-\epsilon_0}\lVert \Bar{\mathbf{b}}-\mathbf{g} \rVert \cdot \lVert \Bar{\mathbf{b}} -\mathbf{g}+\mathbf{g}\rVert\\
%     & \leq \frac{\epsilon_3}{1-\epsilon_0}\lVert \Bar{\mathbf{b}}-\mathbf{g} \rVert \cdot (\lVert \Bar{\mathbf{b}}-\mathbf{g} \rVert + \lVert \mathbf{g} \rVert)\\
%     & \leq \frac{\epsilon_3}{1-\epsilon_0}\lVert \Bar{\mathbf{b}}-\mathbf{g} \rVert \cdot (\lVert \Bar{\mathbf{b}}-\mathbf{g} \rVert + \lVert \mathbf{b}^* \rVert) \\
%     & \leq \frac{\epsilon_3}{1-\epsilon_0}(\epsilon^2 \delta^{t^2}+\epsilon\delta^{t})\lVert \mathbf{b}^*\rVert^2 \\
%     &\leq \frac{\epsilon_3}{1-\epsilon_0}(\epsilon^2+\epsilon)\delta^{t}\lVert \mathbf{b}^*\rVert^2 \\
%     & = \epsilon'(\epsilon^2+\epsilon)\delta^{t}\lVert \mathbf{b}^*\rVert^2
%     % \\
%  %    &\leq \frac{\epsilon_3}{1-\epsilon_0} 
%  % (1+\frac{(1-\epsilon_{0})\epsilon_1}{1-\epsilon_1})\delta^t \lVert \mathbf{b}^*\rVert \cdot (1+\frac{(1-\epsilon_{0})\epsilon_1}{1-\epsilon_1}\delta^t)\lVert \mathbf{b}^*\rVert \\
%  % &\leq \frac{\epsilon_3}{1-\epsilon_0} (1+\frac{(1-\epsilon_{0})\epsilon_1}{1-\epsilon_1})^2\delta^t \lVert \mathbf{b}^*\rVert^2 \label{grad-egrad-final-bound}
% \end{align}
with $\Tilde{\epsilon} = \frac{\epsilon_3}{1-\epsilon_0}$.  \eqref{grad-egrad-4} uses \eqref{eps-net-boung-grad-egrad-w}. \eqref{grad-egrad-5} follows by triangle inequality. \eqref{grad-egrad-1} follows by $\delta^t = \|(\mathbf{I}_d - \mathbf{a}\mathbf{a}^\top)\mathbf{a}^* \|$. \eqref{grad-egrad-2} uses $\lVert \mathbf{g} \rVert = \lVert  \mathbf{b}^* \mathbf{a}^{*^\top} \mathbf{a} \rVert \leq \lVert  \mathbf{b}^* \rVert $. \eqref{grad-egrad-3} follows by $(\delta^{t})^{2} < \delta^t$ since $\delta^t \in (0,1)$. 

If $Nm=\Omega(\frac{d\log(\frac{2}{\epsilon_0})}{\epsilon_3^2})$, then existing large constant $C$, 
\begin{align}
1-2 \exp{(d\log(1+\frac{2}{\epsilon_0})-c_5\epsilon_3^2Nm)}-q^{-10}&>1-\exp(-Cd)-q^{-10} \\ 
&> 1-d^{-10}-q^{-10}\\
&>1-2q^{-10}
\end{align}Thus with probability at least $1-2q^{-10}$, \eqref{grad-egrad-final} holds.
\end{proof}

\begin{lemma}[Lemma 5.3]
    Let $\mathbf{a} = \mathbf{a}^t$. Let $\delta^t = \lVert (\mathbf{I}_d-\mathbf{a}^*\mathbf{a}^{*^\top})\mathbf{a}\rVert = \lVert (\mathbf{I}_d-\mathbf{a}\mathbf{a}^\top)\mathbf{a}^*\rVert$ denote the angle distance between $\mathbf{a}^{*}$ and $\mathbf{a}$. Assume that Assumption~\ref{client-norm-1} holds and $\delta^t \leq \delta^{t-1} \leq \dots \leq \delta^0$. Let $m$ be the number of samples for each updating step, let $\epsilon'=\frac{\epsilon_2}{(1-\epsilon_0)(1-\epsilon_1)},\Tilde{\epsilon}=\frac{\epsilon_3}{1-\epsilon_0}$ for $\epsilon_0, \epsilon_1, \epsilon_2, \epsilon_3 \in (0,1)$, if
    \[
    m= \Omega\left(\max\left\{\frac{\log(N)}{[\min(\epsilon_1,\epsilon_2)]^2},\frac{d\log(\frac{2}{\epsilon_0})}{\epsilon_2^2}\right\}\right)
    \]
    
    
    and $\epsilon',\Tilde{\epsilon}<\frac{1-(\delta^0)^2}{16}$, for any $t$ and $\eta \leq \frac{1}{L_{max}^2}$, then we have,
    \begin{align}
        \delta^{t+1} \leq \delta^{t} \sqrt{1-\eta (1-\delta^{0^2})\|\mathbf b^*\|^2} 
    \end{align}
    with probability at least $1-2q^{-10}$ for $q = \max\left\{\frac{\log(N)}{[\min(\epsilon_1,\epsilon_2)]^2},\frac{d\log(\frac{2}{\epsilon_0})}{\epsilon_2^2}\right\}$.
    % with probability at least $1-2 \exp{(d\log(1+\frac{2}{\epsilon_0})-c_5\epsilon_3^2Nm)}-Np_0$.
\end{lemma}
\begin{proof}

Recall that $\hat{\mathbf{a}}^{+} = \mathbf{a}-\eta   \nabla_{\mathbf{a}}l(\mathbf{a},\Bar{\mathbf{b}})$. We substract and add $\mathbb{E}[\nabla_{\mathbf{a}}l(\mathbf{a},\Bar{\mathbf{b}})]$, obtain
\begin{align}
    \hat{\mathbf{a}}^{+} = \mathbf{a}-\eta\mathbb{E}[\nabla_{\mathbf{a}}l(\mathbf{a},\Bar{\mathbf{b}})] + \eta (\mathbb{E}[\nabla_{\mathbf{a}}l(\mathbf{a},\Bar{\mathbf{b}})]-\nabla_{\mathbf{a}}l(\mathbf{a},\Bar{\mathbf{b}}))
\end{align}
Multiply both sides by the projection operator $\mathbf{P} = \mathbf{I}_d-\mathbf{a}^*(\mathbf{a}^*)^\top$,
\begin{align}
   \mathbf{P} \hat{\mathbf{a}}^{+} &= \mathbf{P}\mathbf{a}-\eta\mathbf{P}\mathbb{E}[\nabla_{\mathbf{a}}l(\mathbf{a},\Bar{\mathbf{b}})] + \eta \mathbf{P}(\mathbb{E}[\nabla_{\mathbf{a}}l(\mathbf{a},\Bar{\mathbf{b}})]-\nabla_{\mathbf{a}}l(\mathbf{a},\Bar{\mathbf{b}})) \\
   & = \mathbf{P}\mathbf{a}- 2\eta\mathbf{P}(\mathbf{a}\Bar{\mathbf{b}}^\top-\mathbf{a}^* \mathbf{b}^{*^\top}) \Bar{\mathbf{b}} + \eta \mathbf{P}(\mathbb{E}[\nabla_{\mathbf{a}}l(\mathbf{a},\Bar{\mathbf{b}})]-\nabla_{\mathbf{a}}l(\mathbf{a},\Bar{\mathbf{b}})) \label{gd-step-2}\\
   & =  \mathbf{P}\mathbf{a}-2\eta\mathbf{P}\mathbf{a}\Bar{\mathbf{b}}^\top \Bar{\mathbf{b}} + \eta \mathbf{P}(\mathbb{E}[\nabla_{\mathbf{a}}l(\mathbf{a},\Bar{\mathbf{b}})]-\nabla_{\mathbf{a}}l(\mathbf{a},\Bar{\mathbf{b}})) \label{gd-step-3} \\
   & = \mathbf{P}\mathbf{a}(1-2\eta \Bar{\mathbf{b}}^\top \Bar{\mathbf{b}})+\eta \mathbf{P}(\mathbb{E}[\nabla_{\mathbf{a}}l(\mathbf{a},\Bar{\mathbf{b}})]-\nabla_{\mathbf{a}}l(\mathbf{a},\Bar{\mathbf{b}})) 
\end{align}
where \eqref{gd-step-2} uses $\mathbb{E}[\nabla_{\mathbf{a}}l(\mathbf{a},\Bar{\mathbf{b}})] = 2(\mathbf{a}\Bar{\mathbf{b}}^\top-\mathbf{a}^* \mathbf{b}^{*^\top}) \Bar{\mathbf{b}} $, \eqref{gd-step-3} follows by $\mathbf{P}\mathbf{a}^*=0$. Thus, we get 
\begin{align}
   \lVert \mathbf{P} \hat{\mathbf{a}}^{+} \rVert \leq  \lVert \mathbf{P}\mathbf{a}\rVert|1-2\eta \Bar{\mathbf{b}}^\top \Bar{\mathbf{b}}|+\eta \lVert (\mathbb{E}[\nabla_{\mathbf{a}}l(\mathbf{a},\Bar{\mathbf{b}})]-\nabla_{\mathbf{a}}l(\mathbf{a},\Bar{\mathbf{b}})) \rVert
\end{align}
Normalizing the left hand side, we obtain
\begin{align}
    \frac{\lVert\mathbf{P} \hat{\mathbf{a}}^{+}\rVert}{\lVert \hat{\mathbf{a}}^{+} \rVert} &\leq \frac{\lVert \mathbf{P}\mathbf{a}\rVert|1-2\eta \Bar{\mathbf{b}}^\top \Bar{\mathbf{b}}|+\eta \lVert (\mathbb{E}[\nabla_{\mathbf{a}}l(\mathbf{a},\Bar{\mathbf{b}})]-\nabla_{\mathbf{a}}l(\mathbf{a},\Bar{\mathbf{b}})) \rVert}{\lVert \hat{\mathbf{a}}^{+} \rVert} \label{normalize-2} \\
    \Rightarrow  \delta^{t+1}& \leq\frac{\delta^t|1-2\eta \Bar{\mathbf{b}}^\top \Bar{\mathbf{b}}|+ \eta  \lVert \mathbb{E}[\nabla_{\mathbf{a}}l(\mathbf{a},\Bar{\mathbf{b}})]-\nabla_{\mathbf{a}}l(\mathbf{a},\Bar{\mathbf{b}}) \rVert}{\lVert \hat{\mathbf{a}}^{+} \rVert} \\
    & = \frac{E_1+E_2}{\lVert \hat{\mathbf{a}}^{+} \rVert}
\end{align}
where \eqref{normalize-2} follows by $\delta^{t+1} =   \frac{\lVert\mathbf{P} \hat{\mathbf{a}}^{+}\rVert}{\lVert \hat{\mathbf{a}}^{+} \rVert} $ and $\delta^t = \lVert \mathbf{P}\mathbf{a}\rVert$. We need to upper bound $E_1$ and $E_2$ accordingly. $E_2$ is upper bounded based on Lemma~\ref{lemma:grad-egrad-bound}. With probability at least $1-2q^{-10}$,
\begin{align}
    E_2 &= \eta  \lVert \mathbb{E}[\nabla_{\mathbf{a}}l(\mathbf{a},\Bar{\mathbf{b}})]-\nabla_{\mathbf{a}}l(\mathbf{a},\Bar{\mathbf{b}}) \rVert \\
    &\leq 2\eta \Tilde{\epsilon} (\epsilon'+1)^2\delta^t\lVert \mathbf{b}^*\rVert^2 \label{E2}
\end{align}

To upper bound $E_1$, we need to lower bound $\lVert \Bar{\mathbf{b}}\rVert^2$. We can first lower bound $\lVert \Bar{\mathbf{b}}\rVert$ by:
\begin{align}
    \lVert \Bar{\mathbf{b}}\rVert & = \lVert \mathbf{g} - (\mathbf{g}-\Bar{\mathbf{b}})\rVert \\
    &\geq  \lVert \mathbf{g} \rVert - \lVert \mathbf{g}-\Bar{\mathbf{b}} \rVert \\
    &=\sqrt{1-(\delta^{t})^2} \lVert {\mathbf{b}^*}\rVert- \lVert \mathbf{g}-\Bar{\mathbf{b}} \rVert \label{b-bar-3}\\
    &\geq \sqrt{1-(\delta^{t})^2} \lVert {\mathbf{b}^*}\rVert - \epsilon'\delta^t \lVert \mathbf{b}^*\rVert  \label{b-bar}
\end{align}
with probability at least $1-q^{-10}$. \eqref{b-bar-3} follows by $\mathbf{g}^\top = \mathbf{a}^\top \mathbf{a}^* \mathbf{b}^{*^\top}$ and $\mathbf{a}^\top \mathbf{a}^* = \cos{\theta}(\mathbf{a}, \mathbf{a}^*)$, \eqref{b-bar} follows by Lemma~\ref{lemma:b-bar-g-bound}.
Assuming $\delta^t\leq \dots \leq \delta^0$, we choose $\epsilon'<\frac{{1-(\delta^0))^2}}{16}$ to make $\sqrt{1-(\delta^{t})^2} \lVert {\mathbf{b}^*}\rVert - \epsilon'\delta^t \lVert \mathbf{b}^*\rVert\geq0$.
Hence $\lVert \Bar{\mathbf{b}}\rVert^2$ is lower bounded by:
\begin{align}
    \lVert \Bar{\mathbf{b}}\rVert^2 & \geq (\sqrt{1-(\delta^{t})^2} \lVert {\mathbf{b}^*}\rVert - \epsilon'\delta^t \lVert \mathbf{b}^*\rVert )^2 \\
    & = (1-(\delta^{t})^2)\lVert {\mathbf{b}^*}\rVert^2 + (\epsilon')^2(\delta^{t})^2\lVert{\mathbf{b}^*}\rVert^2 -2\epsilon' \delta^{t}\sqrt{1-(\delta^{t})^2}\lVert {\mathbf{b}^*}\rVert^2 \\
    &\geq (1-(\delta^{t})^2)\lVert {\mathbf{b}^*}\rVert^2 + (\epsilon')^2(\delta^{t})^2\lVert{\mathbf{b}^*}\rVert^2 - \epsilon' \lVert{\mathbf{b}^*}\rVert^2 \label{b-bar-square-lower-bound-step-1}\\
    &\geq (1-(\delta^{0})^2)\lVert {\mathbf{b}^*}\rVert^2 - \epsilon' \lVert{\mathbf{b}^*}\rVert^2 \label{b-bar-square-lower-bound-step-2}
\end{align}
with probability at least $1-q^{-10}$. \eqref{b-bar-square-lower-bound-step-1} follows by $xy\leq \frac{1}{2}$ for $x^2+y^2=1$, \eqref{b-bar-square-lower-bound-step-2} follows by assuming $\delta^t \leq \delta^{t-1}\leq \dots \leq \delta^{0}$. $E_1$ is upper bounded below.
\begin{align}
    E_1 &= \delta^t|1-2\eta \Bar{\mathbf{b}}^\top \Bar{\mathbf{b}}| \\
    & \leq \delta^t |1-2\eta ((1-(\delta^{0})^2) - \epsilon' )\lVert {\mathbf{b}^*}\rVert^2 | \label{E1}
\end{align}
with probability at least $1-q^{-10}$. Next we lower bound $\lVert \hat{\mathbf{a}}^{+} \rVert$.
\begin{align}
    \lVert \hat{\mathbf{a}}^{+} \rVert^2 & = \lVert \mathbf{a}-\eta \nabla_{\mathbf{a}}l(\mathbf{a},\Bar{\mathbf{b}})\rVert^2 \\
    & = \mathbf{a}^\top\mathbf{a}+\eta^2\lVert\nabla_{\mathbf{a}}l(\mathbf{a},\Bar{\mathbf{b}}) \rVert^2 - 2\eta\mathbf{a}^\top \nabla_{\mathbf{a}}l(\mathbf{a},\Bar{\mathbf{b}}) \label{a-hat-norm-2} \\
    & \geq \mathbf{a}^\top\mathbf{a} - 2\eta\mathbf{a}^\top \nabla_{\mathbf{a}}l(\mathbf{a},\Bar{\mathbf{b}}) \label{a-hat-norm-3} \\
    & = 1- 2\eta\mathbf{a}^\top \nabla_{\mathbf{a}}l(\mathbf{a},\Bar{\mathbf{b}})  \label{a-hat-norm-4}\\
    & = 1- 2\eta  \mathbf{a}^\top (\nabla_{\mathbf{a}}l(\mathbf{a},\Bar{\mathbf{b}}) -  \mathbb{E}[\nabla_{\mathbf{a}}l(\mathbf{a},\Bar{\mathbf{b}})])-2\eta  \mathbf{a}^\top  \mathbb{E}[\nabla_{\mathbf{a}}l(\mathbf{a},\Bar{\mathbf{b}})] 
    % & \geq 1-2\eta \lVert \nabla_{\mathbf{a}}l(\mathbf{a},\Bar{\mathbf{b}}) - \mathbb{E}[\nabla_{\mathbf{a}}l(\mathbf{a},\Bar{\mathbf{b}})]\rVert -2\eta \lVert\mathbb{E}[\nabla_{\mathbf{a}}l(\mathbf{a},\Bar{\mathbf{b}})] \rVert 
\end{align}
where \eqref{a-hat-norm-3} follows by $\eta^2\lVert\nabla_{\mathbf{a}}l(\mathbf{a},\Bar{\mathbf{b}}) \rVert^2 \geq 0$, and \eqref{a-hat-norm-4} follows by $\mathbf{a}^\top\mathbf{a}=1$. The first subtrahend $2\eta  \mathbf{a}^\top (\nabla_{\mathbf{a}}l(\mathbf{a},\Bar{\mathbf{b}}) -  \mathbb{E}[\nabla_{\mathbf{a}}l(\mathbf{a},\Bar{\mathbf{b}})])$ is upper bounded such that
\begin{align}
    2\eta  \mathbf{a}^\top (\nabla_{\mathbf{a}}l(\mathbf{a},\Bar{\mathbf{b}}) -  \mathbb{E}[\nabla_{\mathbf{a}}l(\mathbf{a},\Bar{\mathbf{b}})]) & \leq  2\eta \| \mathbf{a} \|\cdot \| (\nabla_{\mathbf{a}}l(\mathbf{a},\Bar{\mathbf{b}}) -  \mathbb{E}[\nabla_{\mathbf{a}}l(\mathbf{a},\Bar{\mathbf{b}})])\|   \\
    & = 2\eta \| (\nabla_{\mathbf{a}}l(\mathbf{a},\Bar{\mathbf{b}}) -  \mathbb{E}[\nabla_{\mathbf{a}}l(\mathbf{a},\Bar{\mathbf{b}})])\| \\
    &\leq 4\eta \Tilde{\epsilon} (\epsilon'+1)^2\lVert \mathbf{b}^* \rVert^2 \label{denominator-a-grad-egrad}
\end{align}
% \begin{align}
%     2\eta  \mathbf{a}^\top (\nabla_{\mathbf{a}}l(\mathbf{a},\Bar{\mathbf{b}}) -  \mathbb{E}[\nabla_{\mathbf{a}}l(\mathbf{a},\Bar{\mathbf{b}})]) & \leq  2\eta | \mathbf{a}^\top (\nabla_{\mathbf{a}}l(\mathbf{a},\Bar{\mathbf{b}}) -  \mathbb{E}[\nabla_{\mathbf{a}}l(\mathbf{a},\Bar{\mathbf{b}})])|   \\
%     & = 4\eta {\epsilon_3} |\mathbf{a}^\top (\mathbf{a}\Bar{\mathbf{b}}^\top-\mathbf{a}^* \mathbf{b}^{*^\top} ) \Bar{\mathbf{b}} | \label{denominator-a-grad-egrad-2} \\
%     &<  4\eta \Tilde{\epsilon} |\mathbf{a}^\top (\mathbf{a}\Bar{\mathbf{b}}^\top-\mathbf{a}^* \mathbf{b}^{*^\top} ) \Bar{\mathbf{b}} | \label{denominator-a-grad-egrad-2-1} \\
%     & \leq 4\eta \Tilde{\epsilon} \| \mathbf{a}^\top (\mathbf{a}\Bar{\mathbf{b}}^\top-\mathbf{a}^* \mathbf{b}^{*^\top} ) \| \cdot \|\Bar{\mathbf{b}} \| \label{denominator-a-grad-egrad-3} \\
%     & =4\eta \Tilde{\epsilon}\lVert \mathbf{a}^\top (\mathbf{a}(\mathbf{\Bar{b}}-\mathbf{g})^\top -(\mathbf{I}_d-\mathbf{a}\mathbf{a}^\top)\mathbf{a}^*\mathbf{b}^{*^\top})\rVert \cdot \lVert \Bar{\mathbf{b}} \rVert  \\
%     & =4\eta \Tilde{\epsilon}\lVert \mathbf{a}^\top \mathbf{a}(\mathbf{\Bar{b}}-\mathbf{g})^\top -\mathbf{a}^\top(\mathbf{I}_d-\mathbf{a}\mathbf{a}^\top)\mathbf{a}^*\mathbf{b}^{*^\top}\rVert \cdot \lVert \Bar{\mathbf{b}} \rVert  \\
%     & = 4\eta \Tilde{\epsilon}\lVert \mathbf{\Bar{b}}-\mathbf{g} \rVert\cdot\lVert \Bar{\mathbf{b}} \rVert \label{denominator-a-grad-egrad-5}\\
%     & = 4\eta \Tilde{\epsilon}\lVert \mathbf{\Bar{b}}-\mathbf{g} \rVert\cdot\lVert \Bar{\mathbf{b}} -\mathbf{g} + \mathbf{g} \rVert \\
%     &\leq 4\eta \Tilde{\epsilon}\lVert \mathbf{\Bar{b}}-\mathbf{g} \rVert\cdot (\|\mathbf{\Bar{b}}-\mathbf{g} \|+ \| \mathbf{b}^*\|)  \label{denominator-a-grad-egrad-7}\\
%     &\leq 4\eta \Tilde{\epsilon} ((\epsilon')^2+\epsilon')\lVert \mathbf{b}^* \rVert^2 \label{denominator-a-grad-egrad}
% \end{align}
with probability at least $1-2q^{-10}$.  
% \eqref{denominator-a-grad-egrad-2-1} follows by $\epsilon_3<\frac{\epsilon_3}{1-\epsilon_0}=\Tilde{\epsilon}$. \eqref{denominator-a-grad-egrad-3} follows by Cauchy-Schwarz inequality. \eqref{denominator-a-grad-egrad-5} uses $\mathbf{a}^\top(\mathbf{I}_d-\mathbf{a}\mathbf{a}^\top)=0$, and $\mathbf{a}^\top \mathbf{a}=1$ for unit vector $\mathbf{a}$. \eqref{denominator-a-grad-egrad-7} uses $\|\mathbf{g} \| = \| \cos \theta (\mathbf{a},\mathbf{a}^*) \mathbf{b}^*\|\leq \|\mathbf{b}^*\|$. 
\eqref{denominator-a-grad-egrad} uses Lemma~\ref{lemma:b-bar-g-bound}. The second subtrahend is upper bounded such that
\begin{align}
    2\eta  \mathbf{a}^\top  \mathbb{E}[\nabla_{\mathbf{a}}l(\mathbf{a},\Bar{\mathbf{b}})] & =  4\eta  \mathbf{a}^\top (\mathbf{a}\Bar{\mathbf{b}}^\top-\mathbf{a}^* \mathbf{b}^{*^\top}) \Bar{\mathbf{b}} \\
    & = 4\eta   \mathbf{a}^\top (\mathbf{a}\Bar{\mathbf{b}}^\top-\mathbf{a}^* \mathbf{b}^{*^\top}) \mathbf{g} - 4\eta  \mathbf{a}^\top (\mathbf{a}\Bar{\mathbf{b}}^\top-\mathbf{a}^* \mathbf{b}^{*^\top}) (\mathbf{g}-\Bar{\mathbf{b}} ) \label{a-expectation-inner-product}
\end{align}
where $\mathbf{a}^\top (\mathbf{a}\Bar{\mathbf{b}}^\top-\mathbf{a}^* \mathbf{b}^{*^\top}) \mathbf{g}=-\mathbf{a}^\top ((\mathbf{I}_d-\mathbf{a}\mathbf{a}^\top)\mathbf{a}^*\mathbf{b}^{*^\top}+\mathbf{a}(\mathbf{g}-\mathbf{\Bar{b}})^\top) \mathbf{g}=(\mathbf{\Bar{b}}-\mathbf{g})^\top \mathbf{g}$. The second term is simplified via $\mathbf{a}^\top (\mathbf{a}\Bar{\mathbf{b}}^\top-\mathbf{a}^* \mathbf{b}^{*^\top}) (\mathbf{g}-\Bar{\mathbf{b}} ) =\mathbf{a}^\top((\mathbf{I}_d-\mathbf{a}\mathbf{a}^\top)\mathbf{a}^*\mathbf{b}^{*^\top}+\mathbf{a}(\mathbf{g}-\mathbf{\Bar{b}})^\top) (\mathbf{\Bar{b}}-\mathbf{g}) = -(\mathbf{g}-\Bar{\mathbf{b}} )^2$. Both simplifications use $\mathbf{a}^\top(\mathbf{I}_d-\mathbf{a}\mathbf{a}^\top)=0$ and $\mathbf{a}^\top\mathbf{a}=1$. \eqref{a-expectation-inner-product} becomes 
\begin{align}
    2\eta  \mathbf{a}^\top  \mathbb{E}[\nabla_{\mathbf{a}}l(\mathbf{a},\Bar{\mathbf{b}})] &= 4\eta (\mathbf{\Bar{b}}-\mathbf{g})^\top \mathbf{g} + 4\eta (\mathbf{g}-\Bar{\mathbf{b}} )^2 \\
    &\leq 4\eta \lVert \mathbf{g}-\mathbf{\Bar{b}} \rVert \lVert \mathbf{b}^* \rVert +4 \eta \lVert \mathbf{g}-\mathbf{\Bar{b}} \rVert^2\\
    & \leq 4\eta \epsilon' \delta^t \lVert \mathbf{b}^*\rVert^2 + 4\eta (\epsilon')^2 (\delta^{t})^2 \lVert \mathbf{b}^*\rVert^2  \label{denominator-a-expectation-grad-1}\\
    &\leq 4\eta ((\epsilon')^2+\epsilon')\lVert \mathbf{b}^*\rVert^2 \label{denominator-a-expectation-grad}
\end{align}
with probability at least $1-q^{-10}$. \eqref{denominator-a-expectation-grad-1} uses Lemma~\ref{lemma:b-bar-g-bound}. Combining \eqref{denominator-a-grad-egrad} and \eqref{denominator-a-expectation-grad}, we obtain
\begin{align}
    \lVert \hat{\mathbf{a}}^+\rVert^2 &\geq 1-4\eta \Tilde{\epsilon} (\epsilon'+1)^2\lVert \mathbf{b}^* \rVert^2 - 4\eta ((\epsilon')^2+\epsilon')\lVert \mathbf{b}^*\rVert^2 \label{sqrt-a-hat-plus}
\end{align}
with probability at least $1-2q^{-10}$. Combining \eqref{E1}, \eqref{E2} and \eqref{sqrt-a-hat-plus}, we obtain
\begin{align}
   \delta^{t+1} \leq \frac{E_1+E_2}{\lVert \hat{\mathbf{a}}^+\rVert} \leq \frac{\delta^t( 1-2\eta ((1-(\delta^{0})^2) - \epsilon' )\lVert {\mathbf{b}^*}\rVert^2 +2\eta \Tilde{\epsilon} (\epsilon' +1)^2\lVert \mathbf{b}^*\rVert^2)}{\sqrt{1-4\eta \Tilde{\epsilon} (\epsilon'+1)^2\lVert \mathbf{b}^* \rVert^2 - 4\eta ((\epsilon')^2+\epsilon')\lVert \mathbf{b}^*\rVert^2}} = \delta^t C
\end{align}
We can choose $\epsilon',\Tilde{\epsilon}<\frac{1-(\delta^0)^2}{16}$ such that $(1-(\delta^{0})^2)> \max (4(\Tilde{\epsilon}(\epsilon'+1)^2+(\epsilon')^2+\epsilon'), 2\epsilon' +2\Tilde{\epsilon}(\epsilon'+1)^2)$ holds. Then we obtain
\begin{align}
    C &= \frac{ 1-2\eta ((1-(\delta^{0})^2) - \epsilon' )\lVert {\mathbf{b}^*}\rVert^2 +2\eta \Tilde{\epsilon} (\epsilon' +1)^2\lVert \mathbf{b}^*\rVert^2}{\sqrt{1-4\eta \Tilde{\epsilon} (\epsilon'+1)^2\lVert \mathbf{b}^* \rVert^2 - 4\eta ((\epsilon')^2+\epsilon')\lVert \mathbf{b}^*\rVert^2}} \\
    &=  \frac{ 1-2\eta (1-(\delta^{0})^2)\lVert {\mathbf{b}^*}\rVert^2 + 2\eta \epsilon' \lVert {\mathbf{b}^*}\rVert^2 +2\eta \Tilde{\epsilon} (\epsilon' +1)^2\lVert \mathbf{b}^*\rVert^2}{\sqrt{1-4\eta (\Tilde{\epsilon} (\epsilon'+1)^2 +  (\epsilon')^2+\epsilon')\lVert \mathbf{b}^*\rVert^2}} \\
    &\leq \frac{ 1-2\eta (1-(\delta^{0})^2)\lVert {\mathbf{b}^*}\rVert^2 + \eta (2\epsilon' + 2\Tilde{\epsilon} (\epsilon'+1)^2)\lVert \mathbf{b}^*\rVert^2}{\sqrt{1-4\eta (\Tilde{\epsilon} (\epsilon'+1)^2 +  (\epsilon')^2+\epsilon')\lVert \mathbf{b}^*\rVert^2}} \\
    &\leq \frac{1-\eta (1-(\delta^{0})^2)\lVert \mathbf{b}^*\rVert^2}{\sqrt{1-\eta (1-(\delta^{0})^2)\lVert \mathbf{b}^*\rVert^2}} \\
    & = \sqrt{1-\eta (1-(\delta^{0})^2)\lVert \mathbf{b}^*\rVert^2}
\end{align}
Assuming $\eta \leq \frac{1}{L_{max}^2} \le \frac{1}{\lVert \mathbf{b}^*\rVert^2}$, $1-\eta (1-(\delta^{0})^2)\lVert \mathbf{b}^*\rVert^2$ is strictly positive. Therefore we obtain, with probability at least $1-2q^{-10}$, 
\begin{align}
    \delta^{t+1} &\leq \delta^{t} \sqrt{1-\eta (1-(\delta^{0})^2)\lVert \mathbf{b}^*\rVert^2}.
\end{align}

    
\end{proof}
\subsection{Proof of Theorem 5.4} 
\label{subsec:mainProof}
\begin{proof}
    In Lemma~\ref{lemma:delta-t}, we have shown the angle distance between $\mathbf{a}$ and $\mathbf{a}^*$ decreasing in $t$-th iteration such that with probability at least $1-2q^{-10}$ for $q=\max\{\log(N),d\}$, $\delta^{t+1} \leq \delta^{t} C $ for $c\in(0,1), C = \sqrt{1-c(1-(\delta^{0})^2)}$ with proper choice of step size $\eta$.
    \paragraph{Proving $\delta^{1}\leq \delta^{0} C $}.
    Now we are to prove that for the first iteration, $\delta^{1}\leq \delta^{0} C $ with certain probability. 

    By Lemma~\ref{lemma:b-bar-g-bound}, we get $\|\Bar{\mathbf{b}}-\mathbf{g}\| \leq \epsilon' \delta^0 \|\mathbf{b}^* \|$ with probability at least $1-q^{-10}$. 

    By Lemma~\ref{lemma:grad-egrad-bound}, we get $\lVert  \nabla_{\mathbf{a}}l(\mathbf{a},\Bar{\mathbf{b}})-\mathbb{E}[\nabla_{\mathbf{a}}l(\mathbf{a},\Bar{\mathbf{b}})]\rVert \leq 2\Tilde{\epsilon} ((\epsilon')^2+\epsilon')\delta^{0}\lVert \mathbf{b}^*\rVert^2$ with probability at least $1-2q^{-10}$.

    We drop superscript of the iteration index for simplicity. Recall that $\hat{\mathbf{a}}^{+} = \mathbf{a}-\eta   \nabla_{\mathbf{a}}l(\mathbf{a},\Bar{\mathbf{b}})$. We substract and add $\mathbb{E}[\nabla_{\mathbf{a}}l(\mathbf{a},\Bar{\mathbf{b}})]$, obtain
\begin{align}
    \hat{\mathbf{a}}^{+} = \mathbf{a}-\eta\mathbb{E}[\nabla_{\mathbf{a}}l(\mathbf{a},\Bar{\mathbf{b}})] + \eta (\mathbb{E}[\nabla_{\mathbf{a}}l(\mathbf{a},\Bar{\mathbf{b}})]-\nabla_{\mathbf{a}}l(\mathbf{a},\Bar{\mathbf{b}}))
\end{align}
Multiply both sides by the projection operator $\mathbf{P} = \mathbf{I}_d-\mathbf{a}^*(\mathbf{a}^*)^\top$,
\begin{align}
   \mathbf{P} \hat{\mathbf{a}}^{+} &= \mathbf{P}\mathbf{a}-\eta\mathbf{P}\mathbb{E}[\nabla_{\mathbf{a}}l(\mathbf{a},\Bar{\mathbf{b}})] + \eta \mathbf{P}(\mathbb{E}[\nabla_{\mathbf{a}}l(\mathbf{a},\Bar{\mathbf{b}})]-\nabla_{\mathbf{a}}l(\mathbf{a},\Bar{\mathbf{b}})) \\
   & = \mathbf{P}\mathbf{a}- 2\eta\mathbf{P}(\mathbf{a}\Bar{\mathbf{b}}^\top-\mathbf{a}^* \mathbf{b}^{*^\top}) \Bar{\mathbf{b}} + \eta \mathbf{P}(\mathbb{E}[\nabla_{\mathbf{a}}l(\mathbf{a},\Bar{\mathbf{b}})]-\nabla_{\mathbf{a}}l(\mathbf{a},\Bar{\mathbf{b}})) \label{gd-step-2-1}\\
   & =  \mathbf{P}\mathbf{a}-2\eta\mathbf{P}\mathbf{a}\Bar{\mathbf{b}}^\top \Bar{\mathbf{b}} + \eta \mathbf{P}(\mathbb{E}[\nabla_{\mathbf{a}}l(\mathbf{a},\Bar{\mathbf{b}})]-\nabla_{\mathbf{a}}l(\mathbf{a},\Bar{\mathbf{b}})) \label{gd-step-3-1} \\
   & = \mathbf{P}\mathbf{a}(1-2\eta \Bar{\mathbf{b}}^\top \Bar{\mathbf{b}})+\eta \mathbf{P}(\mathbb{E}[\nabla_{\mathbf{a}}l(\mathbf{a},\Bar{\mathbf{b}})]-\nabla_{\mathbf{a}}l(\mathbf{a},\Bar{\mathbf{b}})) 
\end{align}

where \eqref{gd-step-2-1} uses $\mathbb{E}[\nabla_{\mathbf{a}}l(\mathbf{a},\Bar{\mathbf{b}})] = 2(\mathbf{a}\Bar{\mathbf{b}}^\top-\mathbf{a}^* \mathbf{b}^{*^\top}) \Bar{\mathbf{b}} $, \eqref{gd-step-3-1} follows by $\mathbf{P}\mathbf{a}^*=0$. Thus, we get 
\begin{align}
   \lVert \mathbf{P} \hat{\mathbf{a}}^{+} \rVert \leq  \lVert \mathbf{P}\mathbf{a}\rVert|1-2\eta \Bar{\mathbf{b}}^\top \Bar{\mathbf{b}}|+\eta \lVert (\mathbb{E}[\nabla_{\mathbf{a}}l(\mathbf{a},\Bar{\mathbf{b}})]-\nabla_{\mathbf{a}}l(\mathbf{a},\Bar{\mathbf{b}})) \rVert
\end{align}
Normalizing the left hand side, we obtain
\begin{align}
    \frac{\lVert\mathbf{P} \hat{\mathbf{a}}^{+}\rVert}{\lVert \hat{\mathbf{a}}^{+} \rVert} &\leq \frac{\lVert \mathbf{P}\mathbf{a}\rVert|1-2\eta \Bar{\mathbf{b}}^\top \Bar{\mathbf{b}}|+\eta \lVert (\mathbb{E}[\nabla_{\mathbf{a}}l(\mathbf{a},\Bar{\mathbf{b}})]-\nabla_{\mathbf{a}}l(\mathbf{a},\Bar{\mathbf{b}})) \rVert}{\lVert \hat{\mathbf{a}}^{+} \rVert} \label{normalize-2-1} \\
    \Rightarrow  \delta^{1}& \leq\frac{\delta^0|1-2\eta \Bar{\mathbf{b}}^\top \Bar{\mathbf{b}}|+ \eta  \lVert \mathbb{E}[\nabla_{\mathbf{a}}l(\mathbf{a},\Bar{\mathbf{b}})]-\nabla_{\mathbf{a}}l(\mathbf{a},\Bar{\mathbf{b}}) \rVert}{\lVert \hat{\mathbf{a}}^{+} \rVert} \\
    & = \frac{E_1+E_2}{\lVert \hat{\mathbf{a}}^{+} \rVert}
\end{align}
where \eqref{normalize-2-1} follows by $\delta^{1} =   \frac{\lVert\mathbf{P} \hat{\mathbf{a}}^{+}\rVert}{\lVert \hat{\mathbf{a}}^{+} \rVert} $ and $\delta^0 = \lVert \mathbf{P}\mathbf{a}\rVert$. We need to upper bound $E_1$ and $E_2$ accordingly. $E_2$ is upper bounded based on Lemma~\ref{lemma:grad-egrad-bound}. With probability at least $1-2q^{-10}$,
\begin{align}
    E_2 &= \eta  \lVert \mathbb{E}[\nabla_{\mathbf{a}}l(\mathbf{a},\Bar{\mathbf{b}})]-\nabla_{\mathbf{a}}l(\mathbf{a},\Bar{\mathbf{b}}) \rVert \\
    &\leq 2\eta \Tilde{\epsilon} (\epsilon'+1)^2\delta^0\lVert \mathbf{b}^*\rVert^2 \label{E2-1}
\end{align}
To upper bound $E_1$, we need to lower bound $\lVert \Bar{\mathbf{b}}\rVert^2$. We can first lower bound $\lVert \Bar{\mathbf{b}}\rVert$ by:
    \begin{align}
    \lVert \Bar{\mathbf{b}}\rVert & = \lVert \mathbf{g} - (\mathbf{g}-\Bar{\mathbf{b}})\rVert \\
    &\geq  \lVert \mathbf{g} \rVert - \lVert \mathbf{g}-\Bar{\mathbf{b}} \rVert \\
    &=\sqrt{1-(\delta^{0})^2} \lVert {\mathbf{b}^*}\rVert- \lVert \mathbf{g}-\Bar{\mathbf{b}} \rVert \label{b-bar-3-1}\\
    &\geq \sqrt{1-(\delta^{0})^2} \lVert {\mathbf{b}^*}\rVert - \epsilon'\delta^0 \lVert \mathbf{b}^*\rVert  \label{b-bar-1}
\end{align}
with probability at least $1-q^{-10}$. \eqref{b-bar-3-1} follows by $\mathbf{g}^\top = \mathbf{a}^\top \mathbf{a}^* \mathbf{b}^{*^\top}$ and $\mathbf{a}^\top \mathbf{a}^* = \cos{\theta}(\mathbf{a}, \mathbf{a}^*)$, \eqref{b-bar-1} follows by Lemma~\ref{lemma:b-bar-g-bound}. Still we choose $\epsilon'<\frac{{1-(\delta^0))^2}}{16}$ to make $\sqrt{1-(\delta^{0})^2} \lVert {\mathbf{b}^*}\rVert - \epsilon'\delta^0 \lVert \mathbf{b}^*\rVert\geq0$.
Hence $\lVert \Bar{\mathbf{b}}\rVert^2$ is lower bounded by:
\begin{align}
    \lVert \Bar{\mathbf{b}}\rVert^2 & \geq (\sqrt{1-(\delta^{0})^2} \lVert {\mathbf{b}^*}\rVert - \epsilon'\delta^0 \lVert \mathbf{b}^*\rVert )^2 \\
    & = (1-(\delta^{0})^2)\lVert {\mathbf{b}^*}\rVert^2 + (\epsilon')^2(\delta^{0})^2\lVert{\mathbf{b}^*}\rVert^2 -2\epsilon' \delta^{0}\sqrt{1-(\delta^{0})^2}\lVert {\mathbf{b}^*}\rVert^2 \\
    &\geq (1-(\delta^{0})^2)\lVert {\mathbf{b}^*}\rVert^2 + (\epsilon')^2(\delta^{0})^2\lVert{\mathbf{b}^*}\rVert^2 - \epsilon' \lVert{\mathbf{b}^*}\rVert^2 \label{b-bar-square-lower-bound-step-1-1}\\
    &\geq (1-(\delta^{0})^2)\lVert {\mathbf{b}^*}\rVert^2 - \epsilon' \lVert{\mathbf{b}^*}\rVert^2 \label{b-bar-square-lower-bound-step-2-1}
\end{align}
with probability at least $1-q^{-10}$. \eqref{b-bar-square-lower-bound-step-1-1} follows by $xy\leq \frac{1}{2}$ for $x^2+y^2=1$. 
$E_1$ is upper bounded below.
\begin{align}
    E_1 &= \delta^0|1-2\eta \Bar{\mathbf{b}}^\top \Bar{\mathbf{b}}| \\
    & \leq \delta^0 |1-2\eta ((1-(\delta^{0})^2) - \epsilon' )\lVert {\mathbf{b}^*}\rVert^2 | \label{E1-1}
\end{align}
with probability at least $1-q^{-10}$. Next we lower bound $\lVert \hat{\mathbf{a}}^{+} \rVert$.
\begin{align}
    \lVert \hat{\mathbf{a}}^{+} \rVert^2 & = \lVert \mathbf{a}-\eta \nabla_{\mathbf{a}}l(\mathbf{a},\Bar{\mathbf{b}})\rVert^2 \\
    & = \mathbf{a}^\top\mathbf{a}+\eta^2\lVert\nabla_{\mathbf{a}}l(\mathbf{a},\Bar{\mathbf{b}}) \rVert^2 - 2\eta\mathbf{a}^\top \nabla_{\mathbf{a}}l(\mathbf{a},\Bar{\mathbf{b}}) \label{a-hat-norm-2-1} \\
    & \geq \mathbf{a}^\top\mathbf{a} - 2\eta\mathbf{a}^\top \nabla_{\mathbf{a}}l(\mathbf{a},\Bar{\mathbf{b}}) \label{a-hat-norm-3-1} \\
    & = 1- 2\eta\mathbf{a}^\top \nabla_{\mathbf{a}}l(\mathbf{a},\Bar{\mathbf{b}})  \label{a-hat-norm-4-1}\\
    & = 1- 2\eta  \mathbf{a}^\top (\nabla_{\mathbf{a}}l(\mathbf{a},\Bar{\mathbf{b}}) -  \mathbb{E}[\nabla_{\mathbf{a}}l(\mathbf{a},\Bar{\mathbf{b}})])-2\eta  \mathbf{a}^\top  \mathbb{E}[\nabla_{\mathbf{a}}l(\mathbf{a},\Bar{\mathbf{b}})] 
    % & \geq 1-2\eta \lVert \nabla_{\mathbf{a}}l(\mathbf{a},\Bar{\mathbf{b}}) - \mathbb{E}[\nabla_{\mathbf{a}}l(\mathbf{a},\Bar{\mathbf{b}})]\rVert -2\eta \lVert\mathbb{E}[\nabla_{\mathbf{a}}l(\mathbf{a},\Bar{\mathbf{b}})] \rVert 
\end{align}
where \eqref{a-hat-norm-3-1} follows by $\eta^2\lVert\nabla_{\mathbf{a}}l(\mathbf{a},\Bar{\mathbf{b}}) \rVert^2 \geq 0$, and \eqref{a-hat-norm-4-1} follows by $\mathbf{a}^\top\mathbf{a}=1$. The first subtrahend $2\eta  \mathbf{a}^\top (\nabla_{\mathbf{a}}l(\mathbf{a},\Bar{\mathbf{b}}) -  \mathbb{E}[\nabla_{\mathbf{a}}l(\mathbf{a},\Bar{\mathbf{b}})])$ is upper bounded such that
    \begin{align}
    2\eta  \mathbf{a}^\top (\nabla_{\mathbf{a}}l(\mathbf{a},\Bar{\mathbf{b}}) -  \mathbb{E}[\nabla_{\mathbf{a}}l(\mathbf{a},\Bar{\mathbf{b}})]) & \leq  2\eta \| \mathbf{a} \|\cdot \| (\nabla_{\mathbf{a}}l(\mathbf{a},\Bar{\mathbf{b}}) -  \mathbb{E}[\nabla_{\mathbf{a}}l(\mathbf{a},\Bar{\mathbf{b}})])\|   \\
    & = 2\eta \| (\nabla_{\mathbf{a}}l(\mathbf{a},\Bar{\mathbf{b}}) -  \mathbb{E}[\nabla_{\mathbf{a}}l(\mathbf{a},\Bar{\mathbf{b}})])\| \\
    &\leq 4\eta \Tilde{\epsilon} (\epsilon'+1)^2\lVert \mathbf{b}^* \rVert^2 \label{denominator-a-grad-egrad-1}
\end{align}
\eqref{denominator-a-grad-egrad-1} uses Lemma~\ref{lemma:b-bar-g-bound}. The second subtrahend is upper bounded such that
\begin{align}
    2\eta  \mathbf{a}^\top  \mathbb{E}[\nabla_{\mathbf{a}}l(\mathbf{a},\Bar{\mathbf{b}})] & =  4\eta  \mathbf{a}^\top (\mathbf{a}\Bar{\mathbf{b}}^\top-\mathbf{a}^* \mathbf{b}^{*^\top}) \Bar{\mathbf{b}} \\
    & = 4\eta   \mathbf{a}^\top (\mathbf{a}\Bar{\mathbf{b}}^\top-\mathbf{a}^* \mathbf{b}^{*^\top}) \mathbf{g} - 4\eta  \mathbf{a}^\top (\mathbf{a}\Bar{\mathbf{b}}^\top-\mathbf{a}^* \mathbf{b}^{*^\top}) (\mathbf{g}-\Bar{\mathbf{b}} ) \label{a-expectation-inner-product-1}
\end{align}
where $\mathbf{a}^\top (\mathbf{a}\Bar{\mathbf{b}}^\top-\mathbf{a}^* \mathbf{b}^{*^\top}) \mathbf{g}=-\mathbf{a}^\top ((\mathbf{I}_d-\mathbf{a}\mathbf{a}^\top)\mathbf{a}^*\mathbf{b}^{*^\top}+\mathbf{a}(\mathbf{g}-\mathbf{\Bar{b}})^\top) \mathbf{g}=(\mathbf{\Bar{b}}-\mathbf{g})^\top \mathbf{g}$. The second term is simplified via $\mathbf{a}^\top (\mathbf{a}\Bar{\mathbf{b}}^\top-\mathbf{a}^* \mathbf{b}^{*^\top}) (\mathbf{g}-\Bar{\mathbf{b}} ) =\mathbf{a}^\top((\mathbf{I}_d-\mathbf{a}\mathbf{a}^\top)\mathbf{a}^*\mathbf{b}^{*^\top}+\mathbf{a}(\mathbf{g}-\mathbf{\Bar{b}})^\top) (\mathbf{\Bar{b}}-\mathbf{g}) = -(\mathbf{g}-\Bar{\mathbf{b}} )^2$. Both simplifications use $\mathbf{a}^\top(\mathbf{I}_d-\mathbf{a}\mathbf{a}^\top)=0$ and $\mathbf{a}^\top\mathbf{a}=1$. \eqref{a-expectation-inner-product-1} becomes 
\begin{align}
    2\eta  \mathbf{a}^\top  \mathbb{E}[\nabla_{\mathbf{a}}l(\mathbf{a},\Bar{\mathbf{b}})] &= 4\eta (\mathbf{\Bar{b}}-\mathbf{g})^\top \mathbf{g} + 4\eta (\mathbf{g}-\Bar{\mathbf{b}} )^2 \\
    &\leq 4\eta \lVert \mathbf{g}-\mathbf{\Bar{b}} \rVert \lVert \mathbf{b}^* \rVert +4 \eta \lVert \mathbf{g}-\mathbf{\Bar{b}} \rVert^2\\
    & \leq 4\eta \epsilon' \delta^0 \lVert \mathbf{b}^*\rVert^2 + 4\eta (\epsilon')^2 (\delta^{t})^2 \lVert \mathbf{b}^*\rVert^2  \label{denominator-a-expectation-grad-1-1}\\
    &\leq 4\eta ((\epsilon')^2+\epsilon')\lVert \mathbf{b}^*\rVert^2 \label{denominator-a-expectation-grad-1}
\end{align}
with probability at least $1-q^{-10}$. \eqref{denominator-a-expectation-grad-1-1} uses Lemma~\ref{lemma:b-bar-g-bound}. Combining \eqref{denominator-a-grad-egrad-1} and \eqref{denominator-a-expectation-grad-1}, we obtain
\begin{align}
    \lVert \hat{\mathbf{a}}^+\rVert^2 &\geq 1-4\eta \Tilde{\epsilon} (\epsilon'+1)^2\lVert \mathbf{b}^* \rVert^2 - 4\eta ((\epsilon')^2+\epsilon')\lVert \mathbf{b}^*\rVert^2 \label{sqrt-a-hat-plus-1}
\end{align}
with probability at least $1-2q^{-10}$. Combining \eqref{E1-1}, \eqref{E2-1} and \eqref{sqrt-a-hat-plus-1}, we obtain
\begin{align}
   \delta^{1} \leq \frac{E_1+E_2}{\lVert \hat{\mathbf{a}}^+\rVert} \leq \frac{\delta^0( 1-2\eta ((1-(\delta^{0})^2) - \epsilon' )\lVert {\mathbf{b}^*}\rVert^2 +2\eta \Tilde{\epsilon} (\epsilon' +1)^2\lVert \mathbf{b}^*\rVert^2)}{\sqrt{1-4\eta \Tilde{\epsilon} (\epsilon'+1)^2\lVert \mathbf{b}^* \rVert^2 - 4\eta ((\epsilon')^2+\epsilon')\lVert \mathbf{b}^*\rVert^2}} = \delta^0 C
\end{align}
Still we can choose $\epsilon',\Tilde{\epsilon}<\frac{1-(\delta^0)^2}{16}$ such that $(1-(\delta^{0})^2)> \max (4(\Tilde{\epsilon}(\epsilon'+1)^2+(\epsilon')^2+\epsilon'), 2\epsilon' +2\Tilde{\epsilon}(\epsilon'+1)^2)$ holds. Then we obtain
\begin{align}
    C &= \frac{ 1-2\eta ((1-(\delta^{0})^2) - \epsilon' )\lVert {\mathbf{b}^*}\rVert^2 +2\eta \Tilde{\epsilon} (\epsilon' +1)^2\lVert \mathbf{b}^*\rVert^2}{\sqrt{1-4\eta \Tilde{\epsilon} (\epsilon'+1)^2\lVert \mathbf{b}^* \rVert^2 - 4\eta ((\epsilon')^2+\epsilon')\lVert \mathbf{b}^*\rVert^2}} \\
    &=  \frac{ 1-2\eta (1-(\delta^{0})^2)\lVert {\mathbf{b}^*}\rVert^2 + 2\eta \epsilon' \lVert {\mathbf{b}^*}\rVert^2 +2\eta \Tilde{\epsilon} (\epsilon' +1)^2\lVert \mathbf{b}^*\rVert^2}{\sqrt{1-4\eta (\Tilde{\epsilon} (\epsilon'+1)^2 +  (\epsilon')^2+\epsilon')\lVert \mathbf{b}^*\rVert^2}} \\
    &\leq \frac{ 1-2\eta (1-(\delta^{0})^2)\lVert {\mathbf{b}^*}\rVert^2 + \eta (2\epsilon' + 2\Tilde{\epsilon} (\epsilon'+1)^2)\lVert \mathbf{b}^*\rVert^2}{\sqrt{1-4\eta (\Tilde{\epsilon} (\epsilon'+1)^2 +  (\epsilon')^2+\epsilon')\lVert \mathbf{b}^*\rVert^2}} \\
    &\leq \frac{1-\eta (1-(\delta^{0})^2)\lVert \mathbf{b}^*\rVert^2}{\sqrt{1-\eta (1-(\delta^{0})^2)\lVert \mathbf{b}^*\rVert^2}} \\
    & = \sqrt{1-\eta (1-(\delta^{0})^2)\lVert \mathbf{b}^*\rVert^2}
\end{align}
Assuming $\eta \leq \frac{1}{L_{max}^2}\leq \frac{1}{\lVert \mathbf{b}^*\rVert^2}$, $1-\eta (1-(\delta^{0})^2)\lVert \mathbf{b}^*\rVert^2$ is strictly positive. Therefore we obtain, with probability at least $1-2q^{-10}$, 
\begin{align}
    \delta^{1} &\leq \delta^{0} \sqrt{1-\eta (1-(\delta^{0})^2)\lVert \mathbf{b}^*\rVert^2} .
\end{align}

    
    
    %%%%%%%%%%%%%%%%%%%%%%%
    \paragraph{Inductive Hypothesis.}
    Based on inductive hypothesis, by proving $\delta^{1}\leq \delta^{0} C$, the assumption that $\delta^{t}\leq \delta^{t-1} C\leq \dots \leq \delta^{1} C^{t-1}$, and proving $\delta^{t+1}\leq \delta^{t} C $, we conclude that $\delta^{t}\leq \delta^{t-1} C$ holds for all $t \in [T]$ iterations.
    We take a union bound over all $t \in [T]$ such that,
    \begin{align}
        \mathbb{P} \left\{\bigcap_{t=0}^{T-1} \delta^{t+1} \leq \delta^{t} \sqrt{1-c(1-(\delta^{0})^2)} \right\} \geq 1 - 2Tq^{-10}.  
    \end{align}
    \paragraph{Solve for $T$.}
    In order to achieve $\epsilon$-recovery of $\mathbf{a}^*$, we need
    \begin{align}
        \delta^{0} ({1-c(1-(\delta^{0})^2)})^{\frac{T}{2}} &\leq \epsilon \\
        ({1-c (1-(\delta^{0})^2)})^{\frac{T}{2}} &\leq \frac{\epsilon}{\delta^0} \\
        \frac{T}{2} \log{({1-c (1-(\delta^{0})^2)})} &\leq \log (\frac{\epsilon}{\delta^0})  \\
        \end{align} 
        We proceed such that
        \begin{align}
        T &\geq \frac{2\log (\frac{\epsilon}{\delta^0})}{\log{({1-c (1-(\delta^{0})^2)})}} \\
         &> \frac{2\log (\frac{\epsilon}{\delta^0})}{-c (1-(\delta^{0})^2)} \label{T-2}\\
         &= \frac{2}{c (1-(\delta^{0})^2)} \log(\frac{\delta^0}{\epsilon}) 
    \end{align} 
    where \eqref{T-2} follows by using $\log(1-x)<-x$ for $|x|<1$. Thus, with probability at least $1-2Tq^{-10}$, $\delta^T = \sin \theta ({\mathbf{a}^T},\mathbf{a}^*) \leq \epsilon$. 

    \paragraph{Convergence to the target model.} We now aim to upper bound $\| \mathbf{a}^T(\mathbf{b}^{T+1})^\top- \mathbf{a}^* ({\mathbf{b}}^*)^\top\|$. Recall that $(\mathbf g^T)^\top = (\mathbf a^T)^\top \mathbf a^* \mathbf b^{*^\top} $ and $\delta^T = \|  (\mathbf I_d - \mathbf{a}^T(\mathbf a^T)^\top )  \mathbf{a}^* \|$, we have
    \begin{align}
        \lVert \mathbf{a}^T(\mathbf{b}^{T+1})^\top- \mathbf{a}^* \mathbf b^{*^\top}\rVert &= \lVert \mathbf{a}^T(\mathbf{b}^{T+1})^\top - \mathbf{a}^T(\mathbf{g}^{T})^\top + \mathbf{a}^T(\mathbf{g}^{T})^\top- \mathbf{a}^* \mathbf b^{*^\top}\rVert \\
        &\le  \lVert \mathbf{a}^T(\mathbf{b}^{T+1})^\top - \mathbf{a}^T(\mathbf{g}^{T})^\top\| + \| \mathbf{a}^T(\mathbf{g}^{T})^\top- \mathbf{a}^* \mathbf b^{*^\top}\rVert \\
        &= \lVert \mathbf{a}^T(\mathbf{b}^{T+1}-\mathbf{g}^{T})^\top \| + \| (\mathbf{a}^T(\mathbf a^T)^\top  - \mathbf I_d) \mathbf{a}^* \mathbf b^{*^\top}\rVert \\
        &= \|\mathbf{a}^T\| \|\mathbf{b}^{T+1}-\mathbf{g}^{T} \| + \|  (\mathbf I_d - \mathbf{a}^T(\mathbf a^T)^\top )  \mathbf{a}^* \| \|{\mathbf{b}}^*\rVert \\
        &\le \epsilon' \delta^T \|\mathbf b^*\| + \delta^T\|\mathbf b^*\| \label{eq:conv_target_1}\\
        &= (1 + \epsilon') \epsilon \|\mathbf b^*\| \\
        &= (1 + \epsilon') \epsilon \|\mathbf a^* \mathbf b^{*^\top}\|\label{eq:conv_target_2}
    \end{align}
    where \eqref{eq:conv_target_1}  is by Lemma \ref{lemma:b-bar-g-bound} and the fact that $\|\mathbf a^T\| = 1$, and \eqref{eq:conv_target_2} is due to the fact that $\| \mathbf x \mathbf y^\top\| =\| \mathbf x \|\| \mathbf y\| $ and $\|\mathbf a^*\| = 1$.
\end{proof}


\subsubsection{Proof of Proposition 5.5} \label{fa-heter-saturate}
\begin{proof}
    We start by fixing $\mathbf{a}^0$ and updating $\mathbf{b}_i$ to minimize the objective. Let $\mathbf{a}=\mathbf{a}^0$. We obtain
    \begin{align}
        \mathbf{b}_i^\top & = \frac{\mathbf{a}^\top \mathbf{X}_i^\top \mathbf{X}_i \mathbf{a}^* \mathbf{b}^{*^\top} }{\mathbf{a}^\top \mathbf{X}_i^\top\mathbf{X}_i \mathbf{a}} \\
        ({\mathbf{b}}^{FFA})^\top  & = \frac{1}{N} \sum_{i=1}^{N} \frac{\mathbf{a}^\top \mathbf{X}_i^\top \mathbf{X}_i \mathbf{a}^* \mathbf{b}^{*^\top} }{\mathbf{a}^\top \mathbf{X}_i^\top\mathbf{X}_i \mathbf{a}}
    \end{align}
   let $\Bar{\mathbf{b}}={\mathbf{b}}^{FFA}$. We aim to compute the expected value of $\frac{1}{N}\sum_{i=1}^{N}\frac{1}{m}\lVert \mathbf{X}_i \mathbf{a}^*\mathbf{b}^{*^{\top}}- \mathbf{X}_i \mathbf{a}\mathbf{\Bar{b}}^{\top}\rVert^2 $ where the expectation is over all the randomness in the $\mathbf{X}_i$. We define 
    \begin{align}
        s_i = \frac{\mathbf{a}^\top \mathbf{X}_i^\top \mathbf{X}_i \mathbf{a}^* }{\mathbf{a}^\top \mathbf{X}_i^\top\mathbf{X}_i \mathbf{a}}  = \frac{(\mathbf{X_i}\mathbf{a})^\top(\mathbf{X_i}\mathbf{a}^*)}{\lVert \mathbf{X}_i \mathbf{a} \rVert^2}
    \end{align}
    so that $\Bar{\mathbf{b}} = \frac{1}{N}\sum_{i=1}^{N}s_i \mathbf{b}^*=\Bar{s}\mathbf{b}^*$. For each $i$, the norm becomes
    \begin{align}
        \lVert \mathbf{X}_i \mathbf{a}^*\mathbf{b}^{*^{\top}}- \mathbf{X}_i \mathbf{a}\mathbf{\Bar{b}}^{\top}\rVert^2 & = \lVert(\mathbf{X}_i \mathbf{a}^* - \Bar{s}\mathbf{X}_i \mathbf{a}){\mathbf{b}}^{*^\top}\rVert^2 \\
        & = \lVert \mathbf{X}_i \mathbf{a}^* - \Bar{s}\mathbf{X}_i \mathbf{a} \rVert^2 \lVert \mathbf{b}^* \rVert^2
    \end{align}
    using the fact that $\lVert \mathbf{u}\mathbf{v}^\top\rVert^2 = \lVert \mathbf{u}\rVert^2 \lVert \mathbf{v}\rVert^2$ for vectors $\mathbf{u}$ and $\mathbf{v}$. Therefore, $\mathbb{E}[\frac{1}{N}\sum_{i=1}^{N}\frac{1}{m}\lVert \mathbf{X}_i \mathbf{a}^*\mathbf{b}^{*^{\top}}- \mathbf{X}_i \mathbf{a}\mathbf{\Bar{b}}^{\top}\rVert^2] $ is reduced to $\mathbb{E}[\frac{1}{N}\sum_{i=1}^{N}\frac{1}{m}\lVert \mathbf{X}_i \mathbf{a}^*- \mathbf{X}_i \mathbf{a}\rVert^2] \cdot \lVert\mathbf{b}^*\rVert^2$.

    Since each entry of $\mathbf{X}_i$ is independently and identically distributed according to a standard Gaussian distribution, both $\mathbf{a}^*$ and $\mathbf{a}$ are unit vectors, the vectors $ \mathbf{X}_i \mathbf{a}^*$ and $ \mathbf{X}_i \mathbf{a}$ are $\mathcal{N}(0,\mathbf{I}_m)$. The cross-covariance is $\alpha \mathbf{I}_m$ where $\alpha = \mathbf{a}^\top \mathbf{a}^*$. 
    
    By linearity, we can show that $\frac{1}{N}\sum_{i=1}^{N}\frac{1}{m}\lVert \mathbf{X}_i \mathbf{a}^*- \Bar{s} \mathbf{X}_i \mathbf{a}\rVert^2$ has the same expectation as $\frac{1}{m}\lVert \mathbf{X}_1 \mathbf{a}^* - \Bar{s}\mathbf{X}_1 \mathbf{a} \rVert^2$ because all $(\mathbf{X}_i \mathbf{a}^*, \mathbf{X}_i \mathbf{a})$ are i.i.d. pairs. Let $z_1 = \frac{s_1}{N}$ and $z_2 = \frac{s_2+\dots + s_N}{N}$, we have $\lVert \mathbf{X}_1 \mathbf{a}^* - z_1\mathbf{X}_1 \mathbf{a} - z_2\mathbf{X}_1 \mathbf{a} \rVert^2$. Let $\mathbf v = \mathbf{X}_1 \mathbf{a}^*, \mathbf{u}_1 = z_1 \mathbf{X}_1 \mathbf{a}$ and $\mathbf{u}_2 = z_2 \mathbf{X}_1 \mathbf{a}$. Thus,
    \begin{align}
        \lVert \mathbf{X}_1 \mathbf{a}^* - z_1\mathbf{X}_1 \mathbf{a} - z_2\mathbf{X}_1 \mathbf{a} \rVert^2 &= \|\mathbf{v}-\mathbf{u}_1-\mathbf{u}_2 \|^2 \\
        & = \mathbf{v}^\top \mathbf{v} + \mathbf{u}_1^\top \mathbf{u}_1 + \mathbf{u}_2^\top \mathbf{u}_2 -2 \mathbf{v}^\top \mathbf{u}_1 - 2\mathbf{v}^\top \mathbf{u}_2 + 2 \mathbf{u}_1^\top \mathbf{u}_2 \label{expand}
    \end{align}
    Now we compute the expectation term by term.
    \paragraph{Expected value of $\mathbf{v}^\top \mathbf{v}$} We have $\mathbb{E}[\mathbf{v}^\top \mathbf{v}]= \mathbb{E}[\| \mathbf{X}_1 \mathbf{a}^*\|^2]=m$.
    \paragraph{Expected value of $\mathbf{u}_1^\top \mathbf{u}_1$} We have
    \begin{align}
        \mathbf{u}_1^\top \mathbf{u}_1 & = z_1^2 \| \mathbf{X}_1 \mathbf{a} \|^2 \\
        & = \frac{s_1^2}{N^2} \| \mathbf{X}_1 \mathbf{a} \|^2 \\
        & = \frac{1}{N^2} \frac{((\mathbf{X}_1 \mathbf{a})^\top(\mathbf{X}_1 \mathbf{a}^*))^2}{\|  \mathbf{X}_1 \mathbf{a} \|^4} \| \mathbf{X}_1 \mathbf{a} \|^2 \\
         & = \frac{1}{N^2} \frac{((\mathbf{X}_1 \mathbf{a})^\top(\mathbf{X}_1 \mathbf{a}^*))^2}{\|  \mathbf{X}_1 \mathbf{a} \|^2} \label{u1-square-4}
    \end{align}
    Note that $(\mathbf{X}_1 \mathbf{a}^*, \mathbf{X}_1 \mathbf{a})$ is a correlated Gaussian pair with correlation $\alpha = \mathbf{a}^\top\mathbf{a}^*$. Without loss of generality, we assume $\mathbf{a}=\mathbf{e}_1$ thus $\mathbf{a}^*=\alpha \mathbf{e}_1+\sqrt{1-\alpha^2}\mathbf{e}_2$, where $\mathbf{e}_1$ and $\mathbf{e}_2$ are standard basis vectors in $\mathbb{R}^d$. So we can get $\mathbf{X}_1\mathbf{a}=\mathbf{X}_1 \mathbf{e}_1=\mathbf{x}_{1,1}$, where $\mathbf{x}_{1,1}$ denotes the first column of $\mathbf{X}_1$. Accordingly $\mathbf{X}_1\mathbf{a}^* = \alpha \mathbf{X}_1 \mathbf{e}_1+\sqrt{1-\alpha^2}\mathbf{X}_1 \mathbf{e}_2= \alpha \mathbf{x}_{1,1} + \sqrt{1-\alpha^2} \mathbf{x}_{1,2}$ where $\mathbf{x}_{1,2}$ denotes the second column of $\mathbf{X}_{1}$. Therefore \eqref{u1-square-4} can be written as $\frac{1}{N^2}\frac{(\mathbf{x}_{1,1}^\top(\alpha \mathbf{x}_{1,1} + \beta \mathbf{x}_{1,2}))^2}{\| \mathbf{x}_{1,1} \|^2}$. Now we take expectation of it.
    \begin{align}
        \mathbb{E}\left[\frac{1}{N^2} \frac{((\mathbf{X}_1 \mathbf{a})^\top(\mathbf{X}_1 \mathbf{a}^*))^2}{\|  \mathbf{X}_1 \mathbf{a} \|^2} \right] = \mathbb{E}\left[\frac{1}{N^2}\frac{(\mathbf{x}_{1,1}^\top(\alpha \mathbf{x}_{1,1} + \beta \mathbf{x}_{1,2}))^2}{\| \mathbf{x}_{1,1} \|^2}\right] & = \frac{1}{N^2} \mathbb{E}\left[ \frac{(\mathbf{x}_{1,1}^\top(\alpha \mathbf{x}_{1,1} + \beta \mathbf{x}_{1,2}))^2}{\| \mathbf{x}_{1,1} \|^2} \right] 
    \end{align}
    Let $r_1=\| \mathbf{x}_{1,1}\|^2$ and $r_2 = \mathbf{x}_{1,1}^\top \mathbf{x}_{1,2}$. We have
    \begin{align}
        \mathbb{E}\left[ \frac{(\mathbf{x}_{1,1}^\top(\alpha \mathbf{x}_{1,1} + \beta \mathbf{x}_{1,2}))^2}{\| \mathbf{x}_{1,1} \|^2} \right]  & = \mathbb{E}\left[ \frac{(\alpha r_1+\beta r_2)^2}{r_1} \right] \\
        & = \mathbb{E}\left[ \frac{\alpha^2 r_1^2 +\beta^2 r_2^2 + 2\alpha \beta r_1 r_2}{r_1} \right]\\
        & = \mathbb{E}\left[ \alpha^2 r_1\right] + \mathbb{E}\left[ \frac{\beta^2 r_2^2}{r_1}\right] + \mathbb{E}\left[ 2\alpha \beta r_2\right]
    \end{align}
    where $\mathbb{E}\left[ \alpha^2 r_1\right] = \alpha^2 \mathbb{E}\left[ \| \mathbf{x}_{1,1}\|^2 \right]=\alpha^2 m$, and $\mathbb{E}\left[ 2\alpha \beta r_2\right] = 2\alpha \beta \mathbb{E}\left[ r_2\right]=2\alpha \beta \mathbb{E}\left[ \mathbf{x}_{1,1}^\top \mathbf{x}_{1,2} \right]=0$ because $\mathbf{x}_{1,1}$ and $\mathbf{x}_{1,2}$ are independent standard Gaussian vectors. Then we analyze $\mathbb{E}\left[ \frac{\beta^2 r_2^2}{r_1}\right] = \beta^2 \mathbb{E}\left[ \frac{r_2^2}{r_1}\right]$. Condition on $\mathbf{x}_{1,1}$, 
    \begin{align}
        \mathbb{E}\left[ r_2 | \mathbf{x}_{1,1}\right]=\mathbb{E}\left[ \mathbf{x}_{1,1}^\top \mathbf{x}_{1,2} | \mathbf{x}_{1,1}\right]= \mathbf{x}_{1,1}^\top \mathbb{E}\left[ \mathbf{x}_{1,2} \right] = 0
    \end{align}
    and Var$(r_2|\mathbf{x}_{1,1}) = \| \mathbf{x}_{1,1} \|^2=r_1$, thus
    \begin{align}
        r_2|\mathbf{x}_{1,1} = \mathbf{x}_{1,1}^\top \mathbf{x}_{1,2} | \mathbf{x}_{1,1}  \sim \mathcal{N}(0,r_1) \label{x-1-x-2-con-x-1}
    \end{align} Then we obtain  
    \begin{align}
        \mathbb{E}\left[ r_2^2 | \mathbf{x}_{1,1}\right]=r_1 \label{x-1-x-2-square-given-x1}
    \end{align}
    Therefore $\mathbb{E}\left[ \frac{r_2^2}{r_1} | \mathbf{x}_{1,1}\right]=\frac{\mathbb{E}\left[ r_2^2 | \mathbf{x}_{1,1} \right]}{r_1}= 1$. We take total expectation $\mathbb{E}\left[ \frac{r_2^2}{r_1}\right] = \mathbb{E}\left[ \mathbb{E}\left[ \frac{r_2^2}{r_1} | \mathbf{x}_{1,1}\right] \right]=1$. Summarizing,
    \begin{align}
         \mathbb{E}\left[\frac{((\mathbf{X}_1 \mathbf{a})^\top(\mathbf{X}_1 \mathbf{a}^*))^2}{\|  \mathbf{X}_1 \mathbf{a} \|^2} \right] & =  \mathbb{E}\left[ \frac{(\alpha r_1+\beta r_2)^2}{r_1} \right]  \\
        & =  \mathbb{E}\left[ \alpha^2 r_1\right] + \mathbb{E}\left[ \frac{\beta^2 r_2^2}{r_1}\right] + \mathbb{E}\left[ 2\alpha \beta r_2\right]  \\
        & = 
   \alpha^2 m + \beta^2  \label{E-u1-u1-1} \\
   \mathbb{E}\left[ \mathbf{u}_1^\top \mathbf{u}_1 \right] & = \frac{1}{N^2}\mathbb{E}\left[\frac{((\mathbf{X}_1 \mathbf{a})^\top(\mathbf{X}_1 \mathbf{a}^*))^2}{\|  \mathbf{X}_1 \mathbf{a} \|^2} \right]\\&= \frac{\alpha^2 m + (1-\alpha^2)}{N^2} \label{u1-u1}
    \end{align}
    \paragraph{Expected value of $\mathbf{u}_2^\top \mathbf{u}_2$} We have $\mathbf{u}_2^\top \mathbf{u}_2=z_2^2 \| \mathbf{X}_1 \mathbf{a}\|^2$ where $z_2 = \frac{s_2 + \dots +s_N}{N}$ is independent of pair $(\mathbf{X}_1 \mathbf{a}^*, \mathbf{X}_1 \mathbf{a})$. To compute $\mathbb{E} \left[ z_2^2 \| \mathbf{X}_1 \mathbf{a}\|^2 \right]$, first we condition on $z_2$ to obtain,  
    \begin{align}
        \mathbb{E} \left[ z_2^2 \| \mathbf{X}_1 \mathbf{a}\|^2 | z_2 \right] =  z_2^2  \mathbb{E} \left[  \| \mathbf{X}_1 \mathbf{a}\|^2\right] =z^2_2 m
    \end{align}
    Then we take total expectation $ \mathbb{E} \left[ z_2^2 \| \mathbf{X}_1 \mathbf{a}\|^2 \right] = \mathbb{E} \left[ \mathbb{E} \left[ z_2^2 \| \mathbf{X}_1 \mathbf{a}\|^2 | z_2 \right] \right] = \mathbb{E} \left[z_2^2 m \right] = m \mathbb{E} \left[z_2^2 \right] $. 
    \begin{align}
        \mathbb{E} \left[z_2^2 \right] & = \mathbb{E} \left[ \frac{(s_2+\dots +s_N)^2}{N^2} \right] \\
        & = \frac{1}{N^2} \mathbb{E} \left[ \sum_{i=2}^{N} s_i^2 + \sum_{\substack{i=1, j=1\\ i\neq j}}^{N} s_i s_j\right] \\
        & = \frac{1}{N^2}  \left(\sum_{i=2}^{N} \mathbb{E} \left[s_i^2 \right]+ \sum_{\substack{i=1, j=1\\ i\neq j}}^{N}  \mathbb{E} \left[ s_i s_j \right]\right)
    \end{align}
    Write $s_i = \frac{(\mathbf{X_i}\mathbf{a})^\top(\mathbf{X_i}\mathbf{a}^*)}{\lVert \mathbf{X}_i \mathbf{a} \rVert^2}$. Without loss of generality, we assume $\mathbf{a}=\mathbf{e}_1$ thus $\mathbf{a}^*=\alpha \mathbf{e}_1+\sqrt{1-\alpha^2}\mathbf{e}_2$, where $\mathbf{e}_1$ and $\mathbf{e}_2$ are standard basis vectors in $\mathbb{R}^d$. 
    Thus, we have 
$
\mathbf{X}_i\mathbf{a} = \mathbf{X}_i\mathbf{e}_1 = \mathbf{x}_{i,1},
$
where \(\mathbf{x}_{i,1}\) represents the first column of \(\mathbf{X}_i\). Similarly, 
$
\mathbf{X}_i\mathbf{a}^* = \alpha \mathbf{X}_i\mathbf{e}_1 + \sqrt{1-\alpha^2}\mathbf{X}_i\mathbf{e}_2 = \alpha \mathbf{x}_{i,1} + \sqrt{1-\alpha^2} \mathbf{x}_{i,2},
$
where \(\mathbf{x}_{i,2}\) denotes the second column of \(\mathbf{X}_i\).

Hence,
\begin{align}
(\mathbf{X_i}\mathbf{a})^\top(\mathbf{X_i}\mathbf{a}^*)  = \mathbf{x}_{i,1}^\top (\alpha \mathbf{x}_{i,1} + \sqrt{1-\alpha^2} \mathbf{x}_{i,2}) = \alpha \| \mathbf{x}_{i,1} \|^2 + \sqrt{1-\alpha^2} (\mathbf{x}_{i,1}^\top \mathbf{x}_{i,2}) \label{x-i-1-top-x-i-a-star}
\end{align}
With $\| \mathbf{X_i}\mathbf{a} \|^2 = \| \mathbf{x}_{i,1}\|^2$, we have
\begin{align}
    s_i = \frac{\alpha \| \mathbf{x}_{i,1} \|^2 + \sqrt{1-\alpha^2} (\mathbf{x}_{i,1}^\top \mathbf{x}_{i,2})}{\| \mathbf{x}_{i,1}\|^2}=\alpha + \sqrt{1-\alpha^2} \frac{\mathbf{x}_{i,1}^\top \mathbf{x}_{i,2}}{\| \mathbf{x}_{i,1}\|^2} \label{s-i}
\end{align}
Let $R = \frac{\mathbf{x}_{i,1}^\top \mathbf{x}_{i,2}}{\| \mathbf{x}_{i,1}\|^2}$. Then 
\begin{align}
    \mathbb{E}\left[ s_i^2 \right] & = \mathbb{E}\left[ \left( \alpha + \sqrt{1-\alpha^2} R\right)^2 \right]\\
    & =  \mathbb{E}\left[ \alpha^2 + (1-\alpha^2)R^2+ 2\alpha\sqrt{1-\alpha^2} R\right] \\
    & = \alpha^2+ (1-\alpha^2) \mathbb{E}\left[ R^2 \right] +2\alpha\sqrt{1-\alpha^2} \mathbb{E}\left[ R \right] \label{E-s-i-square}
\end{align}
For $\mathbb{E}\left[ R \right]= \mathbb{E}\left[ \frac{\mathbf{x}_{i,1}^\top \mathbf{x}_{i,2}}{\| \mathbf{x}_{i,1}\|^2} \right]$, similarly as in \eqref{x-1-x-2-con-x-1}, $\mathbf{x}_{i,1}^\top \mathbf{x}_{i,2}| \mathbf{x}_{i,1} \sim \mathcal{N}(0, \| \mathbf{x}_{i,1}\|^2)$, thus $\frac{\mathbf{x}_{i,1}^\top \mathbf{x}_{i,2}}{\| \mathbf{x}_{i,1}\|^2} |\mathbf{x}_{i,1} \sim  \mathcal{N}(0, \frac{1}{\| \mathbf{x}_{i,1}\|^2})$, then 
\begin{align}
    \mathbb{E}\left[ R \right]=\mathbb{E}\left[ \mathbb{E}\left[ R | \mathbf{x}_{i,1} \right] \right] =0
\end{align}
For $\mathbb{E}\left[ R^2 \right]$, since $\mathbf{x}_{i,1}^\top \mathbf{x}_{i,2}| \mathbf{x}_{i,1} \sim \mathcal{N}(0, \| \mathbf{x}_{i,1}\|^2)$, so $\mathbb{E}\left[  (\mathbf{x}_{i,1}^\top \mathbf{x}_{i,2})^2 |\mathbf{x}_{i,1} \right]=\| \mathbf{x}_{i,1}\|^2$. Thus, with $R^2 = \frac{(\mathbf{x}_{i,1}^\top \mathbf{x}_{i,2})^2}{\| \mathbf{x}_{i,1}\|^4}$, 
\begin{align}
   &\mathbb{E}\left[ R^2 | \mathbf{x}_{i,1} \right]= \frac{\mathbb{E}\left[  (\mathbf{x}_{i,1}^\top \mathbf{x}_{i,2})^2 |\mathbf{x}_{i,1} \right]}{\| \mathbf{x}_{i,1}\|^4} = \frac{1}{\| \mathbf{x}_{i,1}\|^2} \\
   &\mathbb{E}\left[ R^2 \right] = \mathbb{E}\left[ \frac{1}{\| \mathbf{x}_{i,1}\|^2} \right]
\end{align}
For a $m$-dimensional standard Gaussian vector, $\| \mathbf{x}_{i,1}\|^2$ follows a chi-squared distribution with $m$ degrees of freedom. Therefore, $\mathbb{E}\left[ R^2 \right] = \frac{1}{m-2}$. \eqref{E-s-i-square} becomes
\begin{align}
     \mathbb{E}\left[ s_i^2 \right] &= \alpha^2+ (1-\alpha^2) \mathbb{E}\left[ R^2 \right] +2\alpha\sqrt{1-\alpha^2} \mathbb{E}\left[ R \right]\\
     &= \alpha^2 +(1-\alpha^2)\frac{1}{m-2}
\end{align}
Now we compute $\mathbb{E}\left[ s_is_j \right]$ for $i\neq j$. By independence of $s_i$ and $s_j$, $\mathbb{E}\left[ s_is_j \right]= \mathbb{E}\left[ s_i \right] \cdot \mathbb{E}\left[ s_j \right]=\mathbb{E}\left[ s_i \right]^2$. Take expectation of \eqref{s-i}, \begin{align}
    \mathbb{E}\left[ s_i \right] &= \mathbb{E}\left[ \alpha + \sqrt{1-\alpha^2} \frac{\mathbf{x}_{i,1}^\top \mathbf{x}_{i,2}}{\| \mathbf{x}_{i,1}\|^2} \right] \\
    & = \alpha + \sqrt{1-\alpha^2}   \mathbb{E}\left[ \frac{\mathbf{x}_{i,1}^\top \mathbf{x}_{i,2}}{\| \mathbf{x}_{i,1}\|^2} \right] \\
    &= \alpha + \sqrt{1-\alpha^2}   \mathbb{E}\left[ R \right] \\
    & = \alpha \label{expectation-s-i}
\end{align}
Hence, $\mathbb{E}\left[ s_is_j \right]=\alpha^2$. Summarizing,
\begin{align}
\mathbb{E}\left[\mathbf{u}_2^\top \mathbf{u}_2\right] &=m  \mathbb{E}\left[ z_2^2\right]\\
& = \frac{m}{N^2} \left((N-1)\mathbb{E}\left[ s_i^2\right]  + (N-1)(N-2)\mathbb{E}\left[ s_i s_j\right]\right) \\
& = \frac{m}{N^2}\left( (N-1) 
 (\alpha^2 +(1-\alpha^2)\frac{1}{m-2})+(N-1)(N-2)\alpha^2\right) \\
 & = \frac{m}{N^2}\left[ (N-1)^2 \alpha^2 + (N-1)\frac{1-\alpha^2}{m-2}\right] \label{u2-u2}
\end{align}




    
    \paragraph{Expected value of $\mathbf{v}^\top \mathbf{u}_1$} We have $\mathbf{v}^\top \mathbf{u}_1 = z_1 (\mathbf{X}_1 \mathbf{a}^*)^\top (\mathbf{X}_1 \mathbf{a})= \frac{s_1}{N}(\mathbf{X}_1 \mathbf{a}^*)^\top (\mathbf{X}_1 \mathbf{a})$. We factor out $\frac{1}{N}$, $\mathbb{E}\left[ \mathbf{v}^\top \mathbf{u}_1\right]=\frac{1}{N}\mathbb{E}\left[ s_1 (\mathbf{X}_1 \mathbf{a}^*)^\top (\mathbf{X}_1 \mathbf{a})\right]$. By \eqref{E-u1-u1-1},
    \begin{align}
        \mathbb{E}\left[ s_1 (\mathbf{X}_1 \mathbf{a}^*)^\top (\mathbf{X}_1 \mathbf{a})\right] & = \mathbb{E}\left[\frac{((\mathbf{X}_1 \mathbf{a})^\top(\mathbf{X}_1 \mathbf{a}^*))^2}{\|  \mathbf{X}_1 \mathbf{a} \|^2} \right] \\
        & = \alpha^2m + (1-\alpha^2)
    \end{align}
Then 
\begin{align}
    \mathbb{E}\left[ \mathbf{v}^\top \mathbf{u}_1\right]= \frac{\alpha^2m + (1-\alpha^2)}{N} \label{v-u1}
\end{align}

    
    \paragraph{Expected value of $\mathbf{v}^\top \mathbf{u}_2$} We have $\mathbf{v}^\top \mathbf{u}_2 = z_2 (\mathbf{X}_1 \mathbf{a}^*)^\top (\mathbf{X}_1 \mathbf{a})$. Condition on $z_2$ which is independent of $(\mathbf{X}_1 \mathbf{a}^*)^\top (\mathbf{X}_1 \mathbf{a})$, we obtain
    \begin{align}
        \mathbb{E}\left[  z_2 (\mathbf{X}_1 \mathbf{a}^*)^\top (\mathbf{X}_1 \mathbf{a}) |z_2\right] & = z_2 \mathbb{E}\left[  (\mathbf{X}_1 \mathbf{a}^*)^\top (\mathbf{X}_1 \mathbf{a}) \right] 
    \end{align}
Still we assume $\mathbf{a}=\mathbf{e}_1$ thus $\mathbf{a}^*=\alpha \mathbf{e}_1+\sqrt{1-\alpha^2}\mathbf{e}_2$, where $\mathbf{e}_1$ and $\mathbf{e}_2$ are standard basis vectors in $\mathbb{R}^d$. 
With $\mathbf{X}_1\mathbf{a}=\mathbf{X}_1 \mathbf{e}_1=\mathbf{x}_{1,1}$, where $\mathbf{x}_{1,1}$ denotes the first column of $\mathbf{X}_1$, and $\mathbf{X}_1\mathbf{a}^* = \alpha \mathbf{X}_1 \mathbf{e}_1+\sqrt{1-\alpha^2}\mathbf{X}_1 \mathbf{e}_2= \alpha \mathbf{x}_{1,1} + \sqrt{1-\alpha^2} \mathbf{x}_{1,2}$ where $\mathbf{x}_{1,2}$ denotes the second column of $\mathbf{X}_{1}$, using \eqref{x-i-1-top-x-i-a-star}, 
\begin{align}
    \mathbb{E}\left[  (\mathbf{X}_1 \mathbf{a}^*)^\top (\mathbf{X}_1 \mathbf{a}) \right] &= \mathbb{E}\left[ \alpha \| \mathbf{x}_{1,1}\|^2 + \sqrt{1-\alpha^2} (\mathbf{x}_{1,1}^\top \mathbf{x}_{1,2})\right] \\
    & =  \alpha \mathbb{E}\left[ \| \mathbf{x}_{1,1}\|^2 \right] + z_2 \sqrt{1-\alpha^2} \mathbb{E}\left[ \mathbf{x}_{1,1}^\top \mathbf{x}_{1,2}\right] \\
    & = \alpha m \label{exp-x1-a-x-1-a-star}
\end{align}
Thus $z_2 \mathbb{E}\left[  (\mathbf{X}_1 \mathbf{a}^*)^\top (\mathbf{X}_1 \mathbf{a}) \right]= z_2 \alpha m$, Then we take total expectation
\begin{align}
\mathbb{E}\left[\mathbb{E}\left[  z_2 (\mathbf{X}_1 \mathbf{a}^*)^\top (\mathbf{X}_1 \mathbf{a}) |z_2\right]\right] &= \mathbb{E}\left[ z_2 \alpha m \right] \\
& = \alpha m \mathbb{E}\left[ z_2 \right]
\end{align}
where $z_2=\frac{s_2+\dots +s_N}{N}$. Therefore,
\begin{align}
    \alpha m \mathbb{E}\left[ z_2 \right] = \frac{\alpha m}{N} \sum_{i=2}^N \mathbb{E}\left[s_i\right]
    =\frac{ m}{N} (N-1) \alpha^2 \label{expectation-z-2}
\end{align}
where \eqref{expectation-z-2} follows by $\mathbb{E}\left[ s_i \right]=\alpha$. Summarizing, we obtain $\mathbb{E}\left[ \mathbf{v}^\top \mathbf{u}_2\right]= \frac{ m}{N} (N-1) \alpha^2$.



    
    \paragraph{Expected value of $\mathbf{u}_1^\top \mathbf{u}_2$} We have $\mathbf{u}_1^\top \mathbf{u}_2= z_1 z_2 \| \mathbf{X}_1 \mathbf{a} \|^2$. By definition of $z_1$ and $z_2$, we obtain
    \begin{align}
         z_1 z_2 \| \mathbf{X}_1 \mathbf{a} \|^2 = \frac{1}{N^2}((\mathbf{X}_1 \mathbf{a}^*)^\top (\mathbf{X}_1 \mathbf{a}) ) \sum_{i=2}^N s_i
    \end{align}
    Since $(\mathbf{X}_1 \mathbf{a}^*)^\top (\mathbf{X}_1 \mathbf{a})$ depends only on $\mathbf{X}_1$ , $\sum_{i=2}^N s_i$ is independent of $\mathbf{X}_1$, we obtain
    \begin{align}
        \mathbb{E}\left[ \frac{1}{N^2}((\mathbf{X}_1 \mathbf{a}^*)^\top (\mathbf{X}_1 \mathbf{a}) ) \sum_{i=2}^N s_i \right]
       &  = \frac{1}{N^2} \mathbb{E}\left[ (\mathbf{X}_1 \mathbf{a}^*)^\top (\mathbf{X}_1 \mathbf{a})\right] \cdot \mathbb{E}\left[\sum_{i=2}^N s_i \right] \\
       & = \frac{1}{N^2} \mathbb{E}\left[ (\mathbf{X}_1 \mathbf{a}^*)^\top (\mathbf{X}_1 \mathbf{a})\right] \cdot (N-1) \mathbb{E}\left[ s_i \right] \\
       & = \frac{(N-1)m\alpha^2 }{N^2} \label{u1-u2}
    \end{align}
    where \eqref{u1-u2} follows by $\mathbb{E}\left[ (\mathbf{X}_1 \mathbf{a}^*)^\top (\mathbf{X}_1 \mathbf{a})\right]=\alpha m$ and $\mathbb{E}\left[ s_i \right]=\alpha$.
    
    Combining \eqref{u1-u1}, \eqref{u2-u2},\eqref{v-u1},\eqref{expectation-z-2},\eqref{u1-u2} and \eqref{expand}, 
    \begin{align}
        \frac{1}{m}\lVert \mathbf{X}_1 \mathbf{a}^*- \Bar{s} \mathbf{X}_1 \mathbf{a}\rVert^2 & = \frac{1}{m}(\mathbf{v}^\top \mathbf{v} + \mathbf{u}_1^\top \mathbf{u}_1 + \mathbf{u}_2^\top \mathbf{u}_2 -2 \mathbf{v}^\top \mathbf{u}_1 - 2\mathbf{v}^\top \mathbf{u}_2 + 2 \mathbf{u}_1^\top \mathbf{u}_2) \\
        & = (1-\alpha^2)\left[ 1+ \frac{N(4-m)-2}{N^2m(m-2)}\right] \\
        & = (\delta^0)^2(1+\Tilde{c})
    \end{align}
    where $\delta^0$ is the angle distance between $\mathbf{a}$ and $\mathbf{a}^*$. The quantity $ \Tilde{c}= \frac{N(4-m)-2}{N^2m(m-2)}=O(\frac{1}{Nm})$ as $N$ and $m$ approach infinity. Therefore,

\begin{align}
        \mathbb{E}\left[\frac{1}{N}\sum_{i=1}^{N}\frac{1}{m}\lVert \mathbf{X}_i \mathbf{a}^*\mathbf{b}^{*^{\top}}- \mathbf{X}_i \mathbf{a}\mathbf{\Bar{b}}^{\top}\rVert^2\right] = (1+\Tilde{c})  (\delta^0)^2 \lVert \mathbf{b}^*\rVert^2
    \end{align}

\end{proof}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Vector-vector case with heterogeneous clients} \label{rank-1-heter-clients}
Consider a federated setting with $N$ clients,  each with the following local linear model
\begin{equation}
    f_i(\mathbf{X}_i)=\mathbf{X}_i\mathbf{a}\mathbf{b}^\top \label{linear-model}
\end{equation}
where $\mathbf{a} \in \mathbb{R}^{d}$ is a unit vector and $\mathbf{b} \in \mathbb{R}^d$ are the LoRA weights corresponding to rank $r=1$. In this setting, we model the local data of $i$-th client such that $\mathbf{Y}_i=\mathbf{X}_i\mathbf{a}^*\mathbf{b}_i^{*^\top}$ for some ground truth LoRA weights $\mathbf{a}^* \in \mathbb{R}^{d}$, which is a unit vector, and local $\mathbf{b}_i^* \in \mathbb{R}^d$. We consider the following objective
\begin{equation}
    \min_{\mathbf{a}\in \mathbb{R}^{d},\mathbf{b}\in \mathbb{R}^d} \frac{1}{N}\sum_{i=1}^N l_i(\mathbf{a},\mathbf{b}) 
\end{equation}
We consider the local population loss $l_i(\mathbf{a},\mathbf{b}) = \lVert \mathbf{a}^*\mathbf{b}_i^{*^\top}- \mathbf{a}\mathbf{b}^\top\rVert^2$.

We aim to learn a shared model ($\mathbf{a},\mathbf{b}$) for all the clients. It is straightforward to observe that ($\mathbf{{a}}',\mathbf{b}'$) is a global minimizer of if and only if $\mathbf{{a}}'\mathbf{b'}^{^\top}=\mathbf{a}^*\Bar{\mathbf{b}}^*$, where $\Bar{\mathbf{b}}^*=\frac{1}{N}\sum_{i=1}^{N} \mathbf{b}_i^*$. The solution is unique and satisfies $\mathbf{a}'=\mathbf{a}^*$ and $\mathbf{b}'=\Bar{\mathbf{b}}^*$. With this global minimizer, we obtain the corresponding minimum global error of $\frac{1}{N} \sum_{i=1}^N \lVert \mathbf{a}^*(\mathbf{b}_i^*-\Bar{\mathbf{b}}^*)^\top \rVert^2$.


We aim to show that the training procedure described in Algorithm~\ref{alg:rolora-linear} learns the global minimizer $(\mathbf{a}^*,\Bar{\mathbf{b}}^*)$. First, we make typical assumption and definition.
\begin{assumption}  \label{client-norm}
     There exists $L_{max} < \infty$ (known a priori),  s.t. $\lVert \Bar{\mathbf{b}}^*\rVert \leq  L_{max}$.
\end{assumption} 


\begin{definition} (Client variance) \label{client-var}
    For $\gamma>0$, we define $\gamma^2 := \frac{1}{N} \sum_{i=1}^{N}\lVert \mathbf{b}_i^*-\Bar{\mathbf{b}}^*\rVert^2$, where $\Bar{\mathbf{b}}^* = \frac{1}{N} \sum_{i=1}^{N} \mathbf{b}_i^*$. 
\end{definition}

\begin{theorem} (Convergence of RoLoRA for linear regressor in heterogeneous setting) \label{convergence}
Let $\delta^t = \lVert (\mathbf{I}_d-\mathbf{a}^*\mathbf{a}^{*^\top})\mathbf{a}^t\rVert $ be the angle distance between $\mathbf{a}^{*}$ and $\mathbf{a}^{t}$ of $t$-th iteration. Suppose we are in the setting described in Section~\ref{rank-1-heter-clients} and apply Algorithm~\ref{alg:rolora-linear} for optimization. Given a random initial $\mathbf{a}^0$, an initial angle distance $\delta_0 \in (0,1)$, we set the step size $\eta \leq \frac{1}{2L^2_{\max}}$ and the number of iterations $T \geq \frac{1}{c (1-(\delta^{0})^2)} \log(\frac{\delta^0}{\epsilon}) $ for $c \in (0,1)$. Under these conditions, we achieve the following
\begin{align}
    \sin \theta (\mathbf{a}^T, \mathbf{a}^*)\leq \epsilon \nonumber , \text{ and } \lVert \mathbf{a}^T(\mathbf{b}^{T+1})^\top- \mathbf{a}^* (\Bar{\mathbf{b}}^*)^\top\rVert \leq \epsilon \lVert \mathbf{a}^* (\Bar{\mathbf{b}}^*)^\top \rVert \nonumber
\end{align}
which we refer to as $\epsilon$-accurate recovery of the global minimizer.
\end{theorem}
Theorem~\ref{convergence} follows by recursively applying Lemma~\ref{converge-iter} for $T$ iterations. We start by computing the update rule for ${\mathbf{a}}$ as in Lemma~\ref{a-update-homo}. Using Lemma~\ref{a-update-homo}, we analyze the convergence of ${\mathbf{a}}$ in Lemma~\ref{converge-iter}. We also show the global error that can be achieved by FFA-LoRA within this setting in Proposition~\ref{fa-saturate}.



\begin{lemma} (Update for ${\mathbf{a}}$) \label{a-update-homo}
    In RoLoRA for linear regressor, the update for ${\mathbf{a}}$ and $b$ in each iteration is:
    \begin{align}
    \mathbf{b}^{t+1} &=\Bar{\mathbf{b}} = \Bar{\mathbf{b}}^*\mathbf{a^*}^\top \mathbf{a}^t \\
       \mathbf{a}^{t+1} &= \hat{\mathbf{a}} = \frac{\mathbf{a}^t- 2\eta (\mathbf{a}^t\Bar{\mathbf{b}}^\top\Bar{\mathbf{b}}-\mathbf{a}^*\mathbf{\Bar{b}}^{*^\top}\Bar{\mathbf{b}})}{\lVert \hat{\mathbf{a}}^{+} \rVert}
    \end{align}
where $\Bar{\mathbf{b}}^* = \sum_{i=1}^{N} \mathbf{b}_i^*, \lVert \hat{\mathbf{a}}^{+}\rVert =  \lVert \mathbf{a}^t- 2\eta (\mathbf{a}^t\Bar{\mathbf{b}}^\top\Bar{\mathbf{b}}-\mathbf{a}^*\mathbf{\Bar{b}}^{*^\top}\Bar{\mathbf{b}})\rVert$.
\end{lemma}
\begin{proof}
\textbf{Minimization for $b_i$. } At the start of each iteration, each client computes the analytic solution for $\mathbf{b}_i$ by fixing $\mathbf{a}$ and solving their local objective $\argmin_{\mathbf{b}_i} \lVert \mathbf{a}^*\mathbf{b}_i^{*^\top}- \mathbf{a}\mathbf{b}_i^\top \rVert^2$, where $\mathbf{a}^*$ and $\mathbf{a}$ are both unit vectors. Setting $\mathbf{a} = \mathbf{a}^t$, we obtain $\mathbf{b}_i$ such that
\begin{equation}
    \mathbf{b}_i = \frac{\mathbf{b}_i^{*}\mathbf{a}^{*^\top} \mathbf{a} }{\mathbf{a}^\top \mathbf{a}} =\mathbf{b}_i^{*}  \mathbf{a}^{*^\top} \mathbf{a} \label{b-update}
\end{equation}
\eqref{b-update} follows since $\mathbf{a}^\top \mathbf{a}=1$.

\textbf{Aggregation for $\mathbf{b}_i$. } The server simply computes the average of $\{\mathbf{b}_i\}_{i=1}^N$ and gets 
\begin{equation}
\Bar{\mathbf{b}} = \sum_{i=1}^N \mathbf{b}_i = \sum_{i=1}^N \mathbf{b}_i^* \mathbf{a^*}^\top \mathbf{a} = \Bar{\mathbf{b}}^*\mathbf{a^*}^\top \mathbf{a}
\end{equation}
The server then sends $\Bar{\mathbf{b}}$ to clients for synchronization.

\textbf{Gradient Descent for $\hat{\mathbf{a}}$.}
In this step, each client fixes $\mathbf{b}_i$ to $\Bar{\mathbf{b}}$ received from the server and update $\mathbf{a}$ using gradient descent. With the following gradient
\begin{equation}
    \nabla_{\mathbf{a}}l_i(\mathbf{a}, \Bar{\mathbf{b}}) = 2(\mathbf{a}\Bar{\mathbf{b}}^\top\Bar{\mathbf{b}}-\mathbf{a}^*\mathbf{b}_i^{*^\top}\Bar{\mathbf{b}})
\end{equation}
Thus, with step size $\eta$, $\mathbf{a}$ is updated such as
\begin{align}
\hat{\mathbf{a}}^{+} &= \mathbf{a}-\frac{\eta}{N}\sum_{i=1}^{N} \nabla_{\mathbf{a}}l_i(\mathbf{a},\Bar{\mathbf{b}}) \nonumber\\
&= \mathbf{a}-2\frac{\eta}{N} \sum_{i=1}^{N} (\mathbf{a}\Bar{\mathbf{b}}^\top\Bar{\mathbf{b}}-\mathbf{a}^*\mathbf{b}_i^{*^\top}\Bar{\mathbf{b}}) \nonumber\\
& = \mathbf{a}- 2\eta (\mathbf{a}\Bar{\mathbf{b}}^\top\Bar{\mathbf{b}}-\mathbf{a}^*\mathbf{\Bar{b}}^{*^\top}\Bar{\mathbf{b}}) \\
\hat{\mathbf{a}} &= \frac{\mathbf{a}- 2\eta (\mathbf{a}\Bar{\mathbf{b}}^\top\Bar{\mathbf{b}}-\mathbf{a}^*\mathbf{\Bar{b}}^{*^\top}\Bar{\mathbf{b}})}{\lVert \hat{\mathbf{a}}^{+} \rVert}
\end{align}
\end{proof}





\begin{lemma} \label{converge-iter}
    Let $\delta_t = |\sin \theta (\mathbf{a}^*, \mathbf{a}^t)|$ be the angle distance between $\mathbf{a}^*$ and $ \mathbf{a}^t$. Assume that Assumption~\ref{client-norm} holds and $\delta_t \leq \delta_{t-1} \leq \dots \leq \delta_0$, if $\eta \leq \frac{1}{2L^2_{\max}}$, then 
    \begin{equation}
        |\sin \theta (\mathbf{a}^{t+1}, \mathbf{a}^*)| = \delta_{t+1} \leq \delta_{t} \cdot (1-2\eta (1-(\delta^0)^2) \|\Bar{\mathbf b}^*\|^2)
    \end{equation}
\end{lemma}
\begin{proof}
    From Lemma~\ref{a-update-homo}, $\mathbf{a}^{t+1}$ and $\mathbf{b}^{t+1}$ are computed as follows:
    \begin{align}
    \mathbf{b}^{t+1} &=\Bar{\mathbf{b}} = \Bar{\mathbf{b}}^*\mathbf{a^*}^\top \mathbf{a}^t \\
       \mathbf{a}^{t+1} &=  \frac{\mathbf{a}^t- 2\eta (\mathbf{a}^t\Bar{\mathbf{b}}^\top\Bar{\mathbf{b}}-\mathbf{a}^*\mathbf{\Bar{b}}^{*^\top}\Bar{\mathbf{b}})}{\lVert \mathbf{a}^t- 2\eta (\mathbf{a}^t\Bar{\mathbf{b}}^\top\Bar{\mathbf{b}}-\mathbf{a}^*\mathbf{\Bar{b}}^{*^\top}\Bar{\mathbf{b}})\rVert}\label{as-update}
    \end{align}
    Note that $\mathbf{a}^t$ and $\mathbf{a}^{t+1}$ are both unit vectors. Now, we multiply both sides of Equation~\eqref{as-update} by the projection operator $\mathbf{P} = \mathbf{I}_d-\mathbf{a}^*(\mathbf{a}^*)^\top$, which is the projection to the direction orthogonal to $\mathbf{a}^*$. We obtain:
    \begin{align}
        \mathbf{P}\mathbf{a}^{t+1} &= \frac{\mathbf{P}\mathbf{a}^t- 2\eta \mathbf{P}\mathbf{a}^t \Bar{\mathbf{b}}^\top\Bar{\mathbf{b}}+\mathbf{P}\mathbf{a}^*\mathbf{\Bar{b}}^{*^\top}\Bar{\mathbf{b}}}{\lVert \mathbf{a}^t- 2\eta (\mathbf{a}^t\Bar{\mathbf{b}}^\top\Bar{\mathbf{b}}-\mathbf{a}^*\mathbf{\Bar{b}}^{*^\top}\Bar{\mathbf{b}})\rVert} \label{projection} \\
        & = \frac{\mathbf{P}\mathbf{a}^t- 2\eta \mathbf{P}\mathbf{a}^t\Bar{\mathbf{b}}^\top\Bar{\mathbf{b}}}{\lVert \mathbf{a}^t- 2\eta (\mathbf{a}^t\Bar{\mathbf{b}}^\top\Bar{\mathbf{b}}-\mathbf{a}^*\mathbf{\Bar{b}}^{*^\top}\Bar{\mathbf{b}})\rVert} \label{proj-simp}
    \end{align}

    The third term of the numerator is canceled since $\mathbf{P}\mathbf{a}^* = (\mathbf{I}_d-\mathbf{a}^*(\mathbf{a}^*)^\top)\mathbf{a}^*=0$. Thus,
    \begin{equation}
        \|\mathbf{P}\mathbf{a}^{t+1}\| \leq \frac{\|\mathbf{P}\mathbf{a}^t\| |1- 2\eta \mathbf{\Bar{b}}^{\top}\Bar{\mathbf{b}}|}{\lVert \mathbf{a}^t- 2\eta (\mathbf{a}^t\Bar{\mathbf{b}}^\top\Bar{\mathbf{b}}-\mathbf{a}^*\mathbf{\Bar{b}}^{*^\top}\Bar{\mathbf{b}})\rVert}
    \end{equation}
    
    
    Let $\delta_t = |\sin \theta (\mathbf{a}^*, \mathbf{a}^t)|$. Equation~\eqref{projection} becomes:
    \begin{align}
        \delta_{t+1} &\leq  \delta^{t} \frac{ |1- 2\eta \mathbf{\Bar{b}}^{\top}\Bar{\mathbf{b}}|}{\lVert \mathbf{a}^t- 2\eta (\mathbf{a}^t\Bar{\mathbf{b}}^\top\Bar{\mathbf{b}}-\mathbf{a}^*\mathbf{\Bar{b}}^{*^\top}\Bar{\mathbf{b}})\rVert} \\
        & = \delta_{t} \frac{|1- 2\eta \mathbf{\Bar{b}}^{\top}\Bar{\mathbf{b}}|}{\lVert \mathbf{a}^t(1- 2\eta \mathbf{\Bar{b}}^{\top}\Bar{\mathbf{b}}) +2\eta\mathbf{a}^*\mathbf{\Bar{b}}^{*^\top}\Bar{\mathbf{b}}\rVert} \\
        & = \delta_{t} C
    \end{align}

  Obviously $C \ge 0$. We drop the superscript $t$ when it is clear from context. Note that we have
  \begin{align}
      C^2 &= \frac{|1- 2\eta \mathbf{\Bar{b}}^{\top}\Bar{\mathbf{b}}|^2}{\lVert \mathbf{a}(1- 2\eta \mathbf{\Bar{b}}^{\top}\Bar{\mathbf{b}}) +2\eta\mathbf{a}^*\mathbf{\Bar{b}}^{*^\top}\Bar{\mathbf{b}}\rVert^2} \\
      & = \frac{|1- 2\eta \mathbf{\Bar{b}}^{\top}\Bar{\mathbf{b}} |^2}{(1- 2\eta \mathbf{\Bar{b}}^{\top}\Bar{\mathbf{b}})^2 \mathbf{a}^\top\mathbf{a} + 4\eta^2 (\Bar{\mathbf{b}}^{*^\top}\Bar{\mathbf{b}})^2 + 4\eta (1- 2\eta \mathbf{\Bar{b}}^{\top}\Bar{\mathbf{b}}) \mathbf{a}^\top \mathbf{a}^* \Bar{\mathbf{b}}^{*^\top}\Bar{\mathbf{b}} } \\
      & = \frac{|1- 2\eta \mathbf{\Bar{b}}^{\top}\Bar{\mathbf{b}}|^2}{(1- 2\eta \mathbf{\Bar{b}}^{\top}\Bar{\mathbf{b}})^2  + 4\eta^2 (\Bar{\mathbf{b}}^{*^\top}\Bar{\mathbf{b}})^2  + 4\eta (1- 2\eta \mathbf{\Bar{b}}^{\top}\Bar{\mathbf{b}})\mathbf{a}^\top \mathbf{a}^* \Bar{\mathbf{b}}^{*^\top}\Bar{\mathbf{b}} }  \label{ineq-C}
  \end{align}
  Recall that $\Bar{\mathbf{b}} = \Bar{\mathbf{b}}^* \mathbf{a}^{*^\top}\mathbf{a} = \Bar{\mathbf{b}}^* \cos \theta (\mathbf{a}^*, \mathbf{a})$, \eqref{ineq-C} becomes:
  %\begin{align}
   %   C^2 &= \frac{| 1-2\eta \Bar{b}^{*^2}\cos^2\theta (\mathbf{a}^*, \mathbf{a}) |^2}{(1-2\eta \Bar{b}^2)^2  + 4\eta^2 \Bar{b}^2 \Bar{b}^{*^2}  + (1-2\eta \Bar{b}^2)2\eta\Bar{b}^{*^2} {\cos^2 \theta (\mathbf{a}^*, \mathbf{a}) }} \\
   %   & = \frac{| 1-2\eta \Bar{b}^{*^2}\cos^2\theta (\mathbf{a}^*, \mathbf{a}) |^2}{1+4\eta^2\Bar{b}^{*^4}{\cos^2 \theta (\mathbf{a}^*, \mathbf{a}) }} \\
   %   & = \frac{| 1-2\eta L_{\max}^2\cos^2\theta (\mathbf{a}^*, \mathbf{a}) |^2}{1+4\eta^2L_{\max}^4{\cos^2 \theta (\mathbf{a}^*, \mathbf{a}) }} \\
    %  & \leq 1 \label{ineq-C-2} 
  %\end{align}
  \begin{align}
      C^2 &= \frac{|1- 2\eta \mathbf{\Bar{b}}^{\top}\Bar{\mathbf{b}}|^2}{(1- 2\eta \mathbf{\Bar{b}}^{\top}\Bar{\mathbf{b}})^2  + 4\eta^2 (\Bar{\mathbf{b}}^{*^\top}\Bar{\mathbf{b}})^2  + 4\eta (1- 2\eta \mathbf{\Bar{b}}^{\top}\Bar{\mathbf{b}})\mathbf{a}^\top \mathbf{a}^* \Bar{\mathbf{b}}^{*^\top}\Bar{\mathbf{b}} }  \\
      & = \frac{|1- 2\eta \mathbf{\Bar{b}}^{\top}\Bar{\mathbf{b}}|^2}{1  +  4 \eta^2 \mathbf{\Bar{b}}^{\top}\Bar{\mathbf{b}} (\mathbf{\Bar{b}}^{*^\top}\Bar{\mathbf{b}}^*  - \mathbf{\Bar{b}}^{\top}\Bar{\mathbf{b}})} \\
      & \leq (1-2\eta \mathbf{\Bar{b}}^{\top}\Bar{\mathbf{b}})^2 \label{ineq-C-2}\\
      &= (1-2\eta \|\Bar{\mathbf{b}}\|^2)^2 \label{ineq-C-3}
  \end{align}
  where \eqref{ineq-C-2} holds because $
\mathbf{\Bar{b}}^{*^\top}\Bar{\mathbf{b}}^*  - \mathbf{\Bar{b}}^{\top}\Bar{\mathbf{b}} =(1-\cos^2 \theta (\mathbf{a}^*, \mathbf{a}))\mathbf{\Bar{b}}^{*^\top}\Bar{\mathbf{b}}^*  \geq 0$. Equation \eqref{ineq-C-3} implies $C\le 1-2\eta \|\Bar{\mathbf{b}}\|^2$  if $2\eta \|\Bar{\mathbf{b}}\|^2\le 1$, which can be ensured by choosing a proper step size $\eta \leq \frac{1}{2L_{max}^2} \leq \frac{1}{2\|\Bar{\mathbf{b}}\|^2}$. Now by the assumption that $\delta_t \leq \delta_{t-1} \leq \dots \leq \delta_0$,
\begin{align}
    C&\le 1-2\eta \|\Bar{\mathbf{b}}\|^2 \\
    &= 1-2\eta \cos^2 \theta (\mathbf{a}^*, \mathbf{a}) \|\Bar{\mathbf{b}}^*\|^2 \\
    &= 1-2\eta (1-(\delta^t)^2) \|\Bar{\mathbf{b}}^*\|^2 \\
    &\le 1-2\eta (1-(\delta^0)^2) \|\Bar{\mathbf{b}}^*\|^2 
\end{align}

Summarizing, we obtain $\delta^{t+1}\leq \delta^{t} C \le\delta^{t}  (1 - 2 \eta (1-(\delta^0)^2)\|\Bar{\mathbf{b}}^*\|^2)$.
\end{proof}
\begin{proposition} \label{fa-saturate}(FFA-LoRA lower bound)
Suppose we are in the setting described in Section~\ref{rank-1-heter-clients}. For any set of ground truth parameters ($\mathbf{a}^*,\{\mathbf{b}_i^*\}_{i=1}^N$), initialization $\mathbf{a}^0$, initial angle distance $\delta_0\in (0,1)$, we apply Freezing-A scheme to obtain a shared global model ($\mathbf{a}^0,\mathbf{b}^{FFA}$), where ${\mathbf{b}}^{FFA} = \Bar{\mathbf{b}}^*{\mathbf{a}^*}^\top \mathbf{a}^0$. The global loss is 
\begin{equation}
    \frac{1}{N}\sum_{i=1}^N l_i(\mathbf{a}^0,{\mathbf{b}}^{FFA}) = \gamma^2 + \| \mathbf{\Bar{b}}^*\|^2\delta_0^2 \label{fa-lower-bound}
\end{equation}
\end{proposition}
\begin{proof}
    Through single step of minimization on $\mathbf{b}_i$ and corresponding aggregation, the minimum of the global objective is reached by FFA-LoRA. $\Bar{\mathbf{b}}^{FFA}$ is obtained through:
    \begin{align}
        \mathbf{b}_i &= \frac{\mathbf{b}_i^* \mathbf{a^*}^\top \mathbf{a}^0}{\mathbf{a}^{0^\top}\mathbf{a}^0} = \mathbf{b}_i^* \mathbf{a^*}^\top \mathbf{a}^0 \\
        {\mathbf{b}}^{FFA} &= \frac{1}{N} \sum_{i=1}^{N} \mathbf{b}_i = \Bar{\mathbf{b}}^*\mathbf{a^*}^\top \mathbf{a}^0
    \end{align}
    Next we compute the global loss with a shared global model $(\mathbf{a}^0, \Bar{\mathbf{b}}^{FFA})$. Note that we use Tr$(.)$ to denote the trace of a matrix.
    \begin{align}
        &\frac{1}{N} \sum_{i=1}^{N} l_i(\mathbf{a}^0, {\mathbf{b}}^{FFA}) \\&= \frac{1}{N} \sum_{i=1}^{N} \lVert \mathbf{a}^* (\mathbf{b}_i^*)^\top - \mathbf{a}^0 ({\mathbf{b}}^{FFA})^\top  \rVert^2 \\
        & = \frac{1}{N} \sum_{i=1}^{N} \lVert \mathbf{a}^* (\mathbf{b}_i^*)^\top -\mathbf{a}^*(\Bar{\mathbf{b}}^*)^\top+ \mathbf{a}^*(\Bar{\mathbf{b}}^* )^\top- \mathbf{a}^0 ({\mathbf{b}}^{FFA})^\top  \rVert^2  \\
        & = \frac{1}{N} \sum_{i=1}^{N} (\lVert \mathbf{a}^* (\mathbf{b}_i^* )^\top-\mathbf{a}^*(\Bar{\mathbf{b}}^*)^\top \rVert^2 + \lVert \mathbf{a}^*(\Bar{\mathbf{b}}^*)^\top - \mathbf{a}^0 ({\mathbf{b}}^{FFA} )^\top \rVert^2 \nonumber \\
        &+ 2\text{Tr}((\mathbf{a}^* (\mathbf{b}_i^*)^\top -\mathbf{a}^*(\Bar{\mathbf{b}}^*)^\top)^\top(\mathbf{a}^*(\Bar{\mathbf{b}}^*)^\top - \mathbf{a}^0( {\mathbf{b}}^{FFA})^\top )) \\
        & = \frac{1}{N} \sum_{i=1}^{N} (\lVert \mathbf{a}^* (\mathbf{b}_i^*)^\top -\mathbf{a}^*(\Bar{\mathbf{b}}^*)^\top \rVert^2 + \lVert \mathbf{a}^*(\Bar{\mathbf{b}}^*)^\top - \mathbf{a}^0 ( {\mathbf{b}}^{FFA})^\top \rVert^2) \nonumber\\
       & + 2\text{Tr}((\mathbf{a}^* \frac{1}{N} \sum_{i=1}^{N} (\mathbf{b}_i^*)^\top -\mathbf{a}^*(\Bar{\mathbf{b}}^*)^\top)^\top(\mathbf{a}^*(\Bar{\mathbf{b}}^*)^\top - \mathbf{a}^0( {\mathbf{b}}^{FFA})^\top ))  \\
        & = \frac{1}{N} \sum_{i=1}^{N} (\lVert  \mathbf{a}(\mathbf{b}_i^*-\Bar{\mathbf{b}}^*)^\top \rVert^2 + \lVert \mathbf{a}^*(\Bar{\mathbf{b}}^*)^\top -  \mathbf{a}^0 {\mathbf{a}^0}^\top \mathbf{a}^* (\Bar{\mathbf{b}}^*)^\top \rVert^2) \label{heter-loss-6}  \\
        & = \frac{1}{N} \sum_{i=1}^{N} \lVert \mathbf{b}_i^*-\Bar{
        \mathbf{b}}^*\rVert^2 + \frac{1}{N} \sum_{i=1}^{N} \lVert(\mathbf{I}_d-\mathbf{a}^0\mathbf{a}^{0^\top})\mathbf{a}^*(\Bar{\mathbf{b}}^*)^\top \rVert^2  \label{heter-loss-7}\\
        & = \frac{1}{N} \sum_{i=1}^{N} \lVert \mathbf{b}_i^*-\Bar{
        \mathbf{b}}^*\rVert^2 + \frac{1}{N} \sum_{i=1}^{N} \| (\mathbf{I}_d-\mathbf{a}^0\mathbf{a}^{0^\top})\mathbf{a}^*\|^2 \|\Bar{\mathbf{b}}^* \|^2  \label{heter-loss-8}\\
        &=\gamma^2 +\|\Bar{\mathbf{b}}^* \|^2 \delta_0^2 \label{geq-1}
    \end{align} 
    where \eqref{heter-loss-6} holds since the last term is 0, \eqref{heter-loss-7} and \eqref{heter-loss-8} hold since $\|\mathbf{u}\mathbf{v}^\top\|=\|\mathbf{u}\|\cdot \|\mathbf{v}^\top \|$ for vector $\mathbf{u}$ and $\mathbf{v}$, \eqref{geq-1} holds because of Definition~\ref{client-var}.
\end{proof}

\paragraph{Proof of Theorem A.14}
\begin{proof}
    In order to achieve $\epsilon$-recovery of $\mathbf{a}^*$, we need
    \begin{align}
        \delta^{0} ({1-c(1-(\delta^{0})^2)})^{T} &\leq \epsilon \\
        ({1-c (1-(\delta^{0})^2)})^{T} &\leq \frac{\epsilon}{\delta^0} \\
        T \log{({1-c (1-(\delta^{0})^2)})} &\leq \log (\frac{\epsilon}{\delta^0})  \\
        \end{align} 
        We proceed such that
        \begin{align}
        T &\geq \frac{\log (\frac{\epsilon}{\delta^0})}{\log{({1-c (1-(\delta^{0})^2)})}} \\
         &> \frac{\log (\frac{\epsilon}{\delta^0})}{-c (1-(\delta^{0})^2)} \label{T-3} \\
         &= \frac{1}{c (1-(\delta^{0})^2)} \log(\frac{\delta^0}{\epsilon}) 
    \end{align} 
    where \eqref{T-3} follows by using $\log(1-x)<-x$ for $|x|<1$.

Now we show the convergence to the global minimizer.
Recall that $\mathbf{b}^{T+1} = \Bar{\mathbf b}^{*}\mathbf{a}^{*^\top}\mathbf{a}^T$ and $\delta^T = \|  (\mathbf I_d - \mathbf{a}^T(\mathbf a^T)^\top )  \mathbf{a}^* \|$, we have
    \begin{align}
        \lVert \mathbf{a}^T({\mathbf{b}}^{T+1})^\top- \mathbf{a}^* \Bar{\mathbf b}^{*^\top}\rVert &= \lVert \mathbf{a}^T (\mathbf{a}^T)^\top \mathbf{a}^* \Bar{\mathbf b}^{*^\top}  - \mathbf{a}^* \Bar{\mathbf b}^{*^\top}\rVert \\
        & = \| (\mathbf{a}^T (\mathbf{a}^T)^\top-\mathbf{I}_d) \mathbf{a}^* \Bar{ \mathbf b}^{*^\top}\|\\
        & = \| (\mathbf{I}_d-\mathbf{a}^T (\mathbf{a}^T)^\top) \mathbf{a}^*\| \cdot \|\Bar{\mathbf b}^{*} \|\\
        &\leq \epsilon \|\Bar{\mathbf b}^{*} \| \\
        & =  \epsilon \| \mathbf{a}^* \Bar{\mathbf b}^{*^\top} \label{eq:conv_target_2_1}\|
    \end{align}
    where \eqref{eq:conv_target_2_1} is due to the fact that $\| \mathbf x \mathbf y^\top\| =\| \mathbf x \|\| \mathbf y\| $ and $\|\mathbf a^*\| = 1$.
    
\end{proof}
Proposition~\ref{fa-saturate} shows that for any $\delta_0 \in (0,1)$, the global objective of FFA-LoRA is given by \eqref{geq-1}, comprising two terms: $\gamma^2$, reflecting the heterogeneity of $\{\mathbf{b}_i^*\}_{i=1}^N$, and $\|\Bar{\mathbf{b}}^* \|^2 \delta_0^2$, due to the angular distance between $\mathbf{a}^0$ and $\mathbf{a}^*$. By Theorem~\ref{convergence}, RoLoRA achieves $\epsilon$-accurate recovery of the global minimizer, with global loss upper bounded by $\gamma^2 + \|\Bar{\mathbf{b}}^* \|^2\epsilon^2$, since RoLoRA reduces the angular distance loss from $\|\Bar{\mathbf{b}}^* \|^2\delta_0^2$ to $\|\Bar{\mathbf{b}}^* \|^2\epsilon^2$. We can make $\epsilon$ arbitrarily small by increasing the iterations.

\section{Experiments} 
\subsection{Hyper-parameters for GLUE task} \label{exp-setup}
\begin{table}[h]
    \centering
{\footnotesize
    \begin{tabular}{cccccc}
    \toprule
         & SST-2 & QNLI & MNLI & QQP & RTE \\
         \midrule
    Total comm. rounds     & 500&500&500&500&200 \\
    Batch Size &64&32&32&32&32\\
    Local Epochs & 20&20&20&20&20 \\
    \bottomrule
    \end{tabular}
    \caption{Hyper-parameters configurations. Note the total communication rounds are for the setting with 3 clients. When increasing the number of clients, we decrease the total communication rounds accordingly to maintain a constant sample count used during fine-tuning }
    \label{tab:exp-set}}
\end{table}


% \begin{table}[h]
% \centering
% {\footnotesize
%     \begin{tabular}{ccccccc}
%     \toprule
%                             & Layer Attributes & SST-2 & QNLI & MNLI & QQP & RTE \\
%                             \midrule
% \multirow{2}{*}{$\mathcal{P}_2$} & Type             &     $W_v,W_q$   &  $W_v,W_q$     &   $W_v,W_q$    &   $W_v,W_q$   &   $W_v,W_q$   \\
%                     & Index            &   $\{18,\ldots,23\}$    &   $\{15,\ldots,23\}$   &  $\{15,\ldots,23\}$    &   $\{15,\ldots,23\}$  &  $\{16,\ldots,23\}$    \\ \bottomrule
%     \end{tabular}}
%     \caption{The selected layer set attached with LoRA}
%     \label{tab:layer_type_index-client-num}
% \end{table}




We show the hyper-parameter configurations for each dataset in Table~\ref{tab:exp-set}. 

% \subsection{Setup for Language Generation Task}\label{sec:setup-LGT}
% \paragraph{Model, Datasets and Metrics.}
% We evaluate the performance of three federated finetuning methods with the model Llama-2-7B \cite{touvron2023llama2openfoundation}, on two datasets: CodeAlpaca \cite{codealpaca} for coding tasks, and Alpaca \cite{alpaca} for general instruction-following tasks. Using HumanEval \cite{chen2021codex} as the metric for CodeAlpaca, we assess the model’s ability to generate accurate code solutions. For Alpaca, we employ MMLU (Massive Multitask Language Understanding) \cite{hendrycks2021measuringmassivemultitasklanguage} to evaluate the model's performance across diverse domains. This provides an assessment of Llama-2-7B's coding proficiency, and general language capabilities when finetuning in the federated setting. We show the setup in Table~\ref{tab:my_label}.
% \begin{table}[h]
%     \centering
%     \begin{tabular}{ccccc}
%     \toprule
%         Total comm. rounds & Batch size &  Local Epochs & Layer type attached with LoRA &Layer index attached with LoRA \\
%         \midrule
%         100 & 1 & 30 & $W_k,W_v, W_q, W_o$ & $\{24,\ldots,31\}$\\
%         \bottomrule
%     \end{tabular}
%     \caption{Configurations for language generation tasks.}
%     \label{tab:my_label}
% \end{table}

\subsection{More Experimental Results} \label{more-results}
% \subsubsection{DeBERTa-XLarge Results} 
% In Table~\ref{tab:same_comm_deb}, we show the results with DeBERTa-XLarge (900M). For both FFA-LoRA and RoLoRA, we modify the communication costs by equipping twice as many layers with LoRA adapters compared to the standard LoRA method. So the communication costs are aligned for three methods.
% \begin{table}[h]
% {\footnotesize
%     \centering
%     \begin{tabular}{ccccccc}
%     \toprule
%         Model/Rank & Methods  & QNLI & MNLI & QQP & RTE \\
%         \midrule
%         &LoRA   & 90.16 & 83.58 & 85.67 & 74.97 \\
%        $\textsf{Deb-XL}_{r=4}$  &FFA-LoRA    & 90.08 & 83.31 & 85.79 &  78.73 \\
%          &RoLoRA   & 91.36 & 84.63 & 86.54 & 81.48  \\
%          \midrule
%          & LoRA  &90.12  & 83.25 & 84.56 & 72.55\\
%        $\textsf{Deb-XL}_{r=2}$  & FFA-LoRA  & 90.28& 83.47 & 85.59 & 79.06  \\
%          & RoLoRA   & 91.32 & 84.84& 86.50 & 81.34  \\
%            \bottomrule
%     \end{tabular}
%     \caption{Results with DeBERTa-XLarge models on GLUE. Same message size in each round for three methods for each dataset. The number of clients is 3.}
%     \label{tab:same_comm_deb}}
% \end{table}
\subsubsection{Effect of Number of Clients}
Table~\ref{tab:layer_type_index-client-num} shows the selected layer set attached with LoRA modules for Table~\ref{tab:Clients-num}. We present Table~\ref{tab:Clients-num} with the results of FlexLoRA~\cite{bai2024federatedfinetuninglargelanguage} added in Table~\ref{tab:Clients-num-flex-appendix}.
\begin{table}[h!]
\centering
{\footnotesize
    \begin{tabular}{ccccccc}
    \toprule
                            & Layer Attributes & SST-2 & QNLI & MNLI & QQP & RTE \\
                            \midrule
\multirow{2}{*}{$\mathcal{P}_2$} & Type             &     $W_v,W_q$   &  $W_v,W_q$     &   $W_v,W_q$    &   $W_v,W_q$   &   $W_v,W_q$   \\
                    & Index            &   $\{18,\ldots,23\}$    &   $\{15,\ldots,23\}$   &  $\{15,\ldots,23\}$    &   $\{15,\ldots,23\}$  &  $\{16,\ldots,23\}$    \\ \bottomrule
    \end{tabular}}
    \caption{The selected layer set attached with LoRA modules for Table~\ref{tab:Clients-num}}
    \label{tab:layer_type_index-client-num}
\end{table}



\begin{table*}[h!]
{\small
\centering
    \begin{tabular}{ccccccccc}
    \toprule
        rank & Clients num & Method & SST-2 & QNLI & MNLI & QQP & RTE & Avg. \\
        \midrule 
        \multirow{4}{*}{4} & \multirow{4}{*}{3} & LoRA  & $ \text{\textbf{95.62}}_{\pm \text{0.17}}$ & $\text{ 91.59}_{\pm\text{0.21}}$ & $\text{\textbf{86.20}}_{\pm \text{0.05}}$ & $\text{86.13}_{\pm \text{0.10}}$ & $\text{81.46}_{\pm \text{1.22}}$ & 88.20\\
        &  & FFA-LoRA   & $\text{95.18}_{\pm \text{0.09}}$ & $\text{91.35}_{\pm \text{0.32}}$ & $\text{84.58}_{\pm \text{0.21}}$ & $\text{85.50}_{\pm \text{0.25}}$ & $\text{81.10}_{\pm \text{0.33}}$ & 87.48\\
        &  & FlexLoRA   & $\text{94.91}_{\pm \text{0.18}}$ & $\text{90.16}_{\pm \text{0.49}}$ & $\text{85.16}_{\pm \text{0.69}}$ & $\text{85.69}_{\pm \text{0.17}}$ & $\text{79.3}_{\pm \text{1.05}}$ & 87.04\\
        &  & RoLoRA &  $\text{95.49}_{\pm \text{0.16}}$ & $\text{\textbf{91.64}}_{\pm \text{0.30}}$ & $\text{85.70}_{\pm \text{0.04}}$ & $\text{\textbf{86.14}}_{\pm \text{0.06}}$ & $\text{\textbf{82.43}}_{\pm \text{0.84}}$ & \textbf{88.28}\\
         \midrule 
    \multirow{4}{*}{4} & \multirow{4}{*}{20}        & LoRA & $\text{94.3}_{\pm \text{0.27}}$ & $\text{86.67}_{\pm \text{2.02}}$ &$\text{78.55}_{\pm \text{7.31}}$ &$\text{83.1}_{\pm \text{0.04}}$ & $\text{51.87}_{\pm \text{3.24}}$ & 78.90 \\
   &       & FFA-LoRA & $\text{93.88}_{\pm \text{0.06}}$ & $\text{89.11}_{\pm \text{0.19}}$ & $\text{80.99}_{\pm \text{1.74}}$&$\text{83.92}_{\pm \text{0.2}}$ & $\text{57.16}_{\pm \text{1.46}}$& 80.01\\
   &       & FlexLoRA & $\text{90.97}_{\pm \text{1.78}}$  & $\text{54.36}_{\pm \text{0.36}}$ & $\text{53.30}_{\pm \text{14.59}}$&$\text{69.18}_{\pm \text{10.39}}$ &$\text{53.19}_{\pm \text{1.45}}$ & 64.20 \\
    &     & RoLoRA & $\text{\textbf{94.88}}_{\pm \text{0.18}}$ & $\text{\textbf{90.35}}_{\pm \text{0.37}}$ & $\text{\textbf{85.28}}_{\pm \text{1.04}}$& $\text{\textbf{85.83}}_{\pm \text{0.1}}$ &$\text{\textbf{78.82}}_{\pm \text{1.7}}$ & \textbf{87.03}\\
         \midrule
   \multirow{4}{*}{4} & \multirow{4}{*}{50}            &LoRA&$\text{93.00}_{\pm \text{0.35}}$&$\text{78.13}_{\pm \text{5.13}}$&$\text{52.64}_{\pm \text{15.07}}$&$\text{77.60}_{\pm \text{1.47}}$&$\text{52.23}_{\pm \text{1.1}}$& 70.72\\
     &    &FFA-LoRA&$\text{93.23}_{\pm \text{0.12}}$&$\text{85.05}_{\pm \text{0.34}}$&$\text{69.97}_{\pm \text{5.57}}$&$\text{78.44}_{\pm \text{0.41}}$&$\text{55.72}_{\pm \text{1.99}}$& 76.48\\
     &    &FlexLoRA&$\text{54.08}_{\pm \text{5.5}}$&$\text{55.4}_{\pm \text{2.03}}$&$\text{39.14}_{\pm \text{2.35}}$&$\text{72.00}_{\pm \text{7.64}}$&$\text{52.71}_{\pm \text{0.00}}$& 54.67 \\
    &     &RoLoRA&$\text{\textbf{94.80}}_{\pm \text{0.17}}$&$\text{\textbf{90.00}}_{\pm \text{0.63}}$&$\text{\textbf{82.98}}_{\pm \text{3.36}}$&$\text{\textbf{85.71}}_{\pm \text{0.18}}$&$\text{\textbf{75.57}}_{\pm \text{2.88}}$ & \textbf{85.81}\\
\midrule
        \multirow{4}{*}{8} & \multirow{4}{*}{50} & LoRA & $\text{93.00}_{\pm \text{0.23}}$ & $\text{79.87}_{\pm \text{1.52}}$ & $\text{56.96}_{\pm \text{2.02}}$ & $\text{77.45}_{\pm \text{1.97}}$ & $\text{53.79}_{\pm \text{6.57}}$ & 64.03\\
        &  & FFA-LoRA & $\text{92.74}_{\pm \text{0.13}}$ & $\text{83.69}_{\pm \text{0.75}}$ & $\text{64.51}_{\pm \text{1.92}}$ & $\text{79.71}_{\pm \text{2.04}}$ & $\text{53.07}_{\pm \text{1.3}}$ & 72.46\\
        &  & FlexLoRA & $\text{50.92}_{\pm \text{0.00}}$ & $\text{56.92}_{\pm \text{1.04}}$ & $\text{37.43}_{\pm \text{2.80}}$ & $\text{66.40}_{\pm \text{4.74}}$ & $\text{52.59}_{\pm \text{0.21}}$ & 52.85\\
        &  & RoLoRA & $\text{\textbf{94.53}}_{\pm \text{0.17}}$ & $\text{\textbf{90.1}}_{\pm \text{0.45}}$ & $\text{\textbf{85.17}}_{\pm \text{0.41}}$ & $\text{\textbf{85.25}}_{\pm \text{0.13}}$ & $\text{\textbf{76.3}}_{\pm \text{4.9}}$ & \textbf{86.27}\\
 
         \bottomrule
    \end{tabular}
        \caption{Results for four methods with RoBERTa-Large models with varying client numbers (3, 20, 50), maintaining a constant sample count during fine-tuning.
        \label{tab:Clients-num-flex-appendix}}
}
\end{table*}


 \subsubsection{Effect of Number of LoRA Parameters} \label{app:num-lora-appendix}
 In Table~\ref{tab:layer_type_index}, we include the details about layers attached with LoRA adapters for different budget of finetuning parameters, for each dataset.

 \begin{table}[h!]
\centering
{\footnotesize
    \begin{tabular}{ccccccc}
    \toprule
                            & Layer Attributes & SST-2 & QNLI & MNLI & QQP & RTE \\
                            \midrule
\multirow{2}{*}{$\mathcal{P}_1$} & Type             &   $W_v$    &    $W_v,W_q$    &   $W_v,W_q$     &    $W_v,W_q$   &    $W_v,W_q$   \\
                    & Index            &  $\{21,\ldots,23\}$  &   $\{21,\ldots,23\}$    &   $\{21,\ldots,23\}$    &   $\{21,\ldots,23\}$   &  $\{21,\ldots,23\}$   \\
\multirow{2}{*}{$\mathcal{P}_2$} & Type             &     $W_v,W_q$   &  $W_v,W_q$     &   $W_v,W_q$    &   $W_v,W_q$   &   $W_v,W_q$   \\
                    & Index            &   $\{18,\ldots,23\}$    &   $\{15,\ldots,23\}$   &  $\{15,\ldots,23\}$    &   $\{15,\ldots,23\}$  &  $\{16,\ldots,23\}$   \\
\multirow{2}{*}{$\mathcal{P}_3$} & Type             &  $W_v,W_q$     &   $W_v,W_q$    &   $W_v,W_q$    &  $W_v,W_q$    &  $W_v,W_q$    \\
                    & Index            &    $\{0,\ldots,23\}$  &  $\{12,\ldots,23\}$    &   $\{12,\ldots,23\}$   &   $\{12,\ldots,23\}$  &   $\{12,\ldots,23\}$  \\ \bottomrule
    \end{tabular}}
    \caption{The selected layer set attached with LoRA for the setup of Figure~\ref{fig:five_subfigures}}
    \label{tab:layer_type_index}
\end{table}

\begin{figure}[h!]
\begin{center}
\begin{subfigure}
  \centering
  \includegraphics[width=0.161\linewidth]{sst2-exp-2-r8.png}
  % \caption{SST-2}
  \label{fig:sub1}
\end{subfigure}
\hfill
\begin{subfigure}
  \centering
  \includegraphics[width=0.154\linewidth]{qnli-exp-2-r8.png}
  % \caption{QNLI}
  \label{fig:sub2}
\end{subfigure}
\hfill
\begin{subfigure}
  \centering
  \includegraphics[width=0.154\linewidth]{mnli-exp-2-r8.png}
  % \caption{MNLI}
  \label{fig:sub3}
\end{subfigure}
\hfill
\begin{subfigure}
  \centering
  \includegraphics[width=0.154\linewidth]{qqp-exp-2-r8.png}
  % \caption{QQP}
  \label{fig:sub4}
\end{subfigure}
\hfill
\begin{subfigure}
  \centering
  \includegraphics[width=0.154\linewidth]{rte-exp-2-r8.png}
  % \caption{RTE}
  \label{fig:sub5}
\end{subfigure}
 \caption{Results with RoBERTa-Large models on GLUE of different budget of finetuning parameters. It involves 3 clients using rank 8.}
    \label{fig:five_subfigures-rank-8}
\hfill
\end{center}
\end{figure}
\begin{figure}[h!]
\begin{center}
\begin{subfigure}
  \centering
  \includegraphics[width=0.164\linewidth]{sst2-exp-2-r2.png}
  % \caption{SST-2}
  \label{fig:sub1}
\end{subfigure}
\hfill
\begin{subfigure}
  \centering
  \includegraphics[width=0.154\linewidth]{qnli-exp-2-r2.png}
  % \caption{QNLI}
  \label{fig:sub2}
\end{subfigure}
\hfill
\begin{subfigure}
  \centering
  \includegraphics[width=0.154\linewidth]{mnli-exp-2-r2.png}
  % \caption{MNLI}
  \label{fig:sub3}
\end{subfigure}
\hfill
\begin{subfigure}
  \centering
  \includegraphics[width=0.154\linewidth]{qqp-exp-2-r2.png}
  % \caption{QQP}
  \label{fig:sub4}
\end{subfigure}
\hfill
\begin{subfigure}
  \centering
  \includegraphics[width=0.154\linewidth]{rte-exp-2-r2.png}
  % \caption{RTE}
  \label{fig:sub5}
\end{subfigure}
 \caption{Results with RoBERTa-Large models on GLUE of different budget of finetuning parameters. It involves 3 clients using rank 2.}
    \label{fig:five_subfigures-rank-2}
\hfill
\end{center}
\end{figure}
\begin{figure}[h!]
\begin{center}
\begin{subfigure}
  \centering
  \includegraphics[width=0.164\linewidth]{sst2-exp-2-r1.png}
  % \caption{SST-2}
  \label{fig:sub1}
\end{subfigure}
\hfill
\begin{subfigure}
  \centering
  \includegraphics[width=0.154\linewidth]{qnli-exp-2-r1.png}
  % \caption{QNLI}
  \label{fig:sub2}
\end{subfigure}
\hfill
\begin{subfigure}
  \centering
  \includegraphics[width=0.154\linewidth]{mnli-exp-2-r1.png}
  % \caption{MNLI}
  \label{fig:sub3}
\end{subfigure}
\hfill
\begin{subfigure}
  \centering
  \includegraphics[width=0.154\linewidth]{qqp-exp-2-r1.png}
  % \caption{QQP}
  \label{fig:sub4}
\end{subfigure}
\hfill
\begin{subfigure}
  \centering
  \includegraphics[width=0.154\linewidth]{rte-exp-2-r1.png}
  % \caption{RTE}
  \label{fig:sub5}
\end{subfigure}
 \caption{Results with RoBERTa-Large models on GLUE of different budget of finetuning parameters. It involves 3 clients using rank 1.}
    \label{fig:five_subfigures-rank-1}
\hfill
\end{center}
\end{figure}



 \subsubsection{Align the Communication Cost}\label{sec:align-appendices}
 In Table~\ref{tab:layer_type_index-align-comm-cost}, we include the details about layers attached with LoRA adapters.

\begin{table}[h!]
\centering
{\footnotesize
    \begin{tabular}{ccccc}
    \toprule
                            & Communication Cost & LoRA & FFA-LoRA & RoLoRA  \\
                            \midrule
\multirow{2}{*}{187.5 KB} & Type             &     $W_v,W_q$   &  $W_v,W_q$     &   $W_v,W_q$     \\
                    & Index            &   $\{21,\ldots,23\}$    &   $\{18,\ldots,23\}$   &  $\{18,\ldots,23\}$     \\
                    \midrule
                    \multirow{2}{*}{250 KB} & Type             &     $W_v,W_q$   &  $W_v,W_q$     &   $W_v,W_q$     \\
                    & Index            &   $\{20,\ldots,23\}$    &   $\{16,\ldots,23\}$   &  $\{16,\ldots,23\}$       \\ \bottomrule
    \end{tabular}}
    \caption{The selected layer set attached with LoRA modules for the setup of Figure~\ref{fig:align-comm-cost}}
    \label{tab:layer_type_index-align-comm-cost}
\end{table}
 
%  \begin{figure}[h!]
% \begin{center}
% \begin{subfigure}
%   \centering
%   \includegraphics[width=0.3\linewidth]{icml2024/rte-exp3-4-3.png}
%   % \caption{SST-2}
%   \label{fig:sub1-3}
% \end{subfigure}
% \begin{subfigure}
%   \centering
%   \includegraphics[width=0.3\linewidth]{icml2024/qnli-exp3-4-3.png}
%   % \caption{QNLI}
%   \label{fig:sub2-4}
% \end{subfigure}
%  \caption{RoBERTa-Large accuracies on QNLI and RTE with specific uplink communication budget. It involves 3 clients using rank 4. Error bars reflect standard deviations.}
%  \label{fig:align-comm-cost-appendix}
%  \end{center}
%  \end{figure}
 \subsubsection{Commonsense Reasoning Tasks} \label{sec:setup-CRT}
We present the configurations for Table~\ref{tab:reasoning} in Table~\ref{tab:commonsense-task}. We show the results under the same setup but using rank-2 LoRA modules in Table~\ref{tab:reasoning-rank2-appendix}.  

\begin{table}[h]
    \centering
    \begin{tabular}{ccccc}
    \toprule
        Total comm. rounds & Batch size &  Local Epochs & Layer type attached with LoRA &Layer index attached with LoRA \\
        \midrule
        10 & 1 & 30 & $W_k,W_v, W_q, W_o$ & $\{26,\ldots,31\}$\\
        \bottomrule
    \end{tabular}
    \caption{Configurations for Commonsense Reasoning tasks.}
    \label{tab:commonsense-task}
\end{table}
\begin{table}
{\small
    \centering
    \begin{tabular}{ccccccccc}
    \toprule
         & BoolQ & PIQA & SIQA & HellaSwag & WinoGrande & ARC-e & ARC-c & OBQA\\
         \midrule
        LoRA & $\text{34.36}_{\pm \text{16.90}}$ & $\text{42.87}_{\pm \text{14.05}}$ & $\text{19.12}_{\pm \text{4.22}}$ & $\text{26.21}_{\pm \text{1.91}}$ & $\text{47.2}_{\pm \text{0.64}}$ & $\text{10.31}_{\pm \text{5.96}}$ & $\text{9.84}_{\pm \text{6.13}}$  &$\text{12.33}_{\pm \text{7.46}}$ \\
        FFA-LoRA & $\text{44.04}_{\pm \text{11.48}}$ & $\text{51.46}_{\pm \text{9.81}}$ & $\text{25.38}_{\pm \text{11.27}}$ & $\text{23.86}_{\pm \text{2.67}}$ & $\text{46.93}_{\pm \text{1.54}}$ & $\text{22.25}_{\pm \text{7.92}}$ & $\text{20.65}_{\pm \text{6.33}}$ & $\text{20.67}_{\pm \text{5.33}}$\\
        RoLoRA  &  $\text{61.3}_{\pm \text{0.99}}$ & $\text{60.81}_{\pm \text{6.35}}$ & $\text{37.97}_{\pm \text{5.39}}$ & $\text{29.62}_{\pm \text{2.62}}$ & $\text{49.59}_{\pm \text{1.2}}$ & $\text{37.05}_{\pm \text{2.92}}$ & $\text{29.09}_{\pm \text{3.33}}$ & $\text{28.93}_{\pm \text{4.64}}$\\
        \bottomrule
    \end{tabular}
    \caption{Results with Llama-2-7B models on commonsense reasoning tasks. It involves 50 clients using rank 2.}
    \label{tab:reasoning-rank2-appendix}}
\end{table}
 \subsubsection{Language Generation Tasks} \label{sec:setup-LGT}
\paragraph{Model, Datasets and Metrics.}
We evaluate the performance of three federated finetuning methods with the model Llama-2-7B \cite{touvron2023llama2openfoundation}, on two datasets: CodeAlpaca \cite{codealpaca} for coding tasks, and Alpaca \cite{alpaca} for general instruction-following tasks. Using HumanEval \cite{chen2021codex} as the metric for CodeAlpaca, we assess the model’s ability to generate accurate code solutions. For Alpaca, we employ MMLU (Massive Multitask Language Understanding) \cite{hendrycks2021measuringmassivemultitasklanguage} to evaluate the model's performance across diverse domains. This provides an assessment of Llama-2-7B's coding proficiency, and general language capabilities when finetuning in the federated setting. We show the setup in Table~\ref{tab:my_label}.
\paragraph{Results}
Table ~\ref{llama-results} presents the evaluation results of the Llama-2-7B model using three methods, across two tasks: HumanEval, and MMLU. The metrics reported include the average and standard deviation of performance over five seeds, with 50 clients involved. The results show that RoLoRA achieves the highest scores across most metrics, demonstrating slightly improved performance compared to LoRA and FFA-LoRA. The improvements are more evident in certain subcategories of the MMLU dataset. 



\begin{table}[h!]
    \centering
    \begin{tabular}{ccccc}
    \toprule
        Total comm. rounds & Batch size &  Local Epochs & Layer type attached with LoRA &Layer index attached with LoRA \\
        \midrule
        100 & 1 & 30 & $W_k,W_v, W_q, W_o$ & $\{24,\ldots,31\}$\\
        \bottomrule
    \end{tabular}
    \caption{Configurations for language generation tasks.}
    \label{tab:my_label}
\end{table}
\begin{table}[]

\centering
\begin{tabular}{lccc}
\toprule
                           & LoRA & FFA-LoRA & RoLoRA \\ \midrule
HumanEval                  &   $\text{12.96}_{\pm \text{0.37}}$    &     $\text{13.29}_{\pm \text{0.21}}$     &   
$\text{13.45}_{\pm \text{0.28}}$ \\ \midrule
MMLU                       &   $\text{45.81}_{\pm \text{0.03}}$   &   $\text{45.80}_{\pm \text{0.02}}$        &     $\text{45.93}_{\pm \text{0.01}}$     \\
\multicolumn{1}{r}{human}  &  $\text{43.26}_{\pm \text{0.04}}$    &         $\text{43.24}_{\pm \text{0.01}}$   &   $\text{43.46}_{\pm \text{0.02}}$     \\
\multicolumn{1}{r}{other}  &  $\text{52.67}_{\pm \text{0.06}}$    &   $\text{52.72}_{\pm \text{0.05}}$       &       $\text{52.83}_{\pm \text{0.04}}$   \\
\multicolumn{1}{r}{social} &    $\text{51.73}_{\pm \text{0.04}}$    &           $\text{51.64}_{\pm \text{0.05}}$ &        $\text{51.81}_{\pm \text{0.04}}$ \\
\multicolumn{1}{r}{stem}   &   $\text{37.10}_{\pm \text{0.03}}$   &  $\text{37.12}_{\pm \text{0.01}}$        &      $\text{37.05}_{\pm \text{0.02}}$   \\  
% \midrule
% GSM8K                      &   $\text{13.82}_{\pm \text{0.03}}$   &     $\text{14.00}_{\pm \text{0.01}}$     &  $\text{14.07}_{\pm \text{0.04}}$     \\
\bottomrule
\end{tabular}
\caption{Results with Llama-2-7B model on HumanEval, and MMLU. We report the average and std. over five seeds. The number of clients is 50. The metric used across all tasks is accuracy, where higher values indicate better performance.}
\label{llama-results}
\end{table}
% \subsubsection{Communication Cost} \label{actual-comm-cost}
% In Table~\ref{tab:comm_size}, we show the uplink communication cost for three methods for the layer set $\mathcal{P}_2$ using rank=1.
% \begin{table}[h]
% {\footnotesize
%     \centering
%     \begin{tabular}{ccccccc}
%     \toprule
%  Rank  &Methods     & SST-2 & QNLI & MNLI & QQP  & RTE \\
%       \midrule 
%   &LoRA     &46.9 & 93.8 & 93.8& 140.6 & 125  \\
% r=1  &FFA-LoRA    & 23.5 & 46.9 & 46.9& 70.3 & 62.5\\
%   &RoLoRA    & 23.5 & 46.9 & 46.9 & 70.3  & 62.5 \\

%   \bottomrule
%     \end{tabular}
%     \caption{Uplink message size (KB) in each communication round for experiments in Table~\ref{tab:glue1} when rank=1. The message size will proportionally increase if utilizing different ranks.}
%     \label{tab:comm_size}}
% \end{table}




% \subsubsection{Finetuning Dynamics of the Setup with Severe Data Heterogeneity} \label{ft-dynamics}
% In Figure~\ref{fig:convergence-speed}, we show the convergence of three methods with 50 clients. RoLoRA demonstrates superior convergence speed compared to the other two methods.

% \begin{figure*}[h]
% \begin{center}
% \begin{subfigure}
%   \centering
%   \includegraphics[width=0.185\linewidth]{icml2024/sst-2-50.png}
%   % \caption{SST-2}
%   \label{fig:sub1-}
% \end{subfigure}
% \hfill
% \begin{subfigure}
%   \centering
%   \includegraphics[width=0.18\linewidth]{icml2024/qnli-2-50.png}
%   % \caption{QNLI}
%   \label{fig:sub2-}
% \end{subfigure}
% \hfill
% \begin{subfigure}
%   \centering
%   \includegraphics[width=0.174\linewidth]{icml2024/mnli-2-50.png}
%   % \caption{MNLI}
%   \label{fig:sub3-}
% \end{subfigure}
% \hfill
% \begin{subfigure}
%   \centering
%   \includegraphics[width=0.181\linewidth]{icml2024/qqp-2-50.png}
%   % \caption{MNLI}
%   \label{fig:sub4-}
% \end{subfigure}
%  \caption{Accuracies over rounds with RoBERTa-Large models on SST-2, QNLI, MNLI, and QQP. The total number of clients is 50.}
%     \label{fig:convergence-speed}
% \hfill
% \end{center}
% \end{figure*}
% \newpage
% \subsubsection{DeBERTa-XLarge Results} 
% In Table~\ref{tab:same_comm_deb}, we show the results with DeBERTa-XLarge (900M). For both FFA-LoRA and RoLoRA, we modify the communication costs by equipping twice as many layers with LoRA adapters compared to the standard LoRA method. So the communication costs are aligned for three methods.
% \begin{table}[h]
% {\footnotesize
%     \centering
%     \begin{tabular}{ccccccc}
%     \toprule
%         Model/Rank & Methods  & QNLI & MNLI & QQP & RTE \\
%         \midrule
%         &LoRA   & 90.16 & 83.58 & 85.67 & 74.97 \\
%        $\textsf{Deb-XL}_{r=4}$  &FFA-LoRA    & 90.08 & 83.31 & 85.79 &  78.73 \\
%          &RoLoRA   & 91.36 & 84.63 & 86.54 & 81.48  \\
%          \midrule
%          & LoRA  &90.12  & 83.25 & 84.56 & 72.55\\
%        $\textsf{Deb-XL}_{r=2}$  & FFA-LoRA  & 90.28& 83.47 & 85.59 & 79.06  \\
%          & RoLoRA   & 91.32 & 84.84& 86.50 & 81.34  \\
%            \bottomrule
%     \end{tabular}
%     \caption{Results with DeBERTa-XLarge models on GLUE. Same message size in each round for three methods for each dataset. The number of clients is 3.}
%     \label{tab:same_comm_deb}}
% \end{table}

% \subsubsection{Performance when QLoRA is Applied} In Table~\ref{tab:quantization-base-model}, we quantize the frozen pre-trained weights to 8 bit and 4 bit for each client, and apply QLoRA\cite{dettmers2023qlora}. The relative accuracy is computed as $\frac{\mathsf{Acc}_{\mathsf{FP}}-\mathsf{Avg}(\mathsf{Acc}_{8b}+\mathsf{Acc}_{4b})}{\mathsf{Acc}_{\mathsf{FP}}}$. We use sufficient finetuning parameters and the selected layer set is $\mathcal{P}_1$ to study the effect of the quantized foundation model in the federated setting.
% \begin{table}[h]
% {\footnotesize
%     \centering
%     \begin{tabular}{ccccccccccc}
%     \toprule
%       Methods   &  & QQP &  &  &QNLI &  &  & MNLI & & \\
%          \midrule
%          & FP & 8-bit & 4-bit & FP &8-bit  &4-bit  & FP &8-bit  &4-bit & Relative Acc.\\
%          \midrule
%      LoRA   &85.86 & 85.78 & 85.47 & 92.87 & 92.71 &92.00  &87.69  & 86.57 &  86.49 &$\downarrow 0.7\%$\\
%      FFA-LoRA  & 85.74 & 85.59 & 85.35 & 92.51 & 91.20 & 91.12 &85.75 & 84.86 & 83.19 &$\downarrow 1.3\%$\\
%      RoLoRA  & 85.64 & 85.51 & 85.47 & 92.48 & 92.51 & 91.63 & 87.95 &87.27 &  86.12& $\downarrow 0.7\%$\\
%      \bottomrule
%     \end{tabular}
%     \caption{Results of RoBERTa-Large models on QQP, QNLI, MNLI with the quantized base model. We use rank 2.}
%     \label{tab:quantization-base-model}}
% \end{table}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\end{document}


% This document was modified from the file originally made available by
% Pat Langley and Andrea Danyluk for ICML-2K. This version was created
% by Iain Murray in 2018, and modified by Alexandre Bouchard in
% 2019 and 2021 and by Csaba Szepesvari, Gang Niu and Sivan Sabato in 2022.
% Modified again in 2023 and 2024 by Sivan Sabato and Jonathan Scarlett.
% Previous contributors include Dan Roy, Lise Getoor and Tobias
% Scheffer, which was slightly modified from the 2010 version by
% Thorsten Joachims & Johannes Fuernkranz, slightly modified from the
% 2009 version by Kiri Wagstaff and Sam Roweis's 2008 version, which is
% slightly modified from Prasad Tadepalli's 2007 version which is a
% lightly changed version of the previous year's version by Andrew
% Moore, which was in turn edited from those of Kristian Kersting and
% Codrina Lauth. Alex Smola contributed to the algorithmic style files.
