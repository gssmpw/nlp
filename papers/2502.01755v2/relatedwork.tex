\section{Related Works}
\label{related-works}
\paragraph{Parameter Efficient Fine Tuning (PEFT): LoRA and Its Enhancements}
 Parameter efficient finetuning (PEFT) allows for updates to a smaller subset of parameters, significantly reducing the computational and memory requirements. One of the most well-known methods is LoRA\cite{hu2021lora}.
LoRA uses low-rank matrices to approximate changes in weights during fine-tuning, allowing them to be integrated with pre-trained weights before inference. In \cite{zhang2023lorafa}, the authors propose a memory-efficient fine-tuning method, LoRA-FA, which keeps the projection-down weight fixed and updates the projection-up weight during fine-tuning. In \cite{zhu2024asymmetrylowrankadaptersfoundation}, the authors highlight the asymmetry between the projection-up and projection-down matrices and focus solely on comparing the effects of freezing either the projection-up or projection-down matrices. \cite{hao2024floralowrankadapterssecretly} introduces the idea of resampling the projection-down matrices, aligning with our observation that freezing projection-down matrices negatively impacts a model's expressiveness. Furthermore, \cite{hayou2024lora} explore the distinct roles of projection-up and projection-down matrices, enhancing performance by assigning different learning rates to each. 
%Their theoretical analysis confirms that the optimal approach involves using a lower learning rate for projection-down matrices, rather than freezing the projection-down matrices, further supporting our findings. 
% \cite{zhang2023adalora} designs AdaLoRA by using SVD decomposition and pruning less significant singular values for more efficient updates. VeRA \cite{kopiczko2024vera} is proposed to further reduce the number of trainable parameters during finetuning by using a single pair of low-rank matrices shared across all layers and learning small scaling vectors. Liu et al. analyze magnitude and directional updates in LoRA versus full parameter fine-tuning and introduce DoRA\cite{liu2024dora}, which decomposes pre-trained weights for fine-tuning and applies LoRA for directional updates.
\paragraph{PEFT in Federated Setting}

PEFT adjusts only a few lightweight or a small portion of the total parameters for specific tasks, keeping most foundational model parameters unchanged. This feature can help reduce data transfer in federated learning, where communication depends on the size of updates. Zhang et al. \cite{zhang-etal-2023-fedpetuning} compares multiple PEFT methods in federated setting, including Adapter\cite{houlsby2019parameterefficient}, LoRA\cite{hu2021lora}, Prompt tuning\cite{liu2022ptuning} and Bit-Fit\cite{zaken2022bitfit}. SLoRA\cite{babakniya2023slora}, which combines sparse finetuning and LoRA, is proposed to address the data heterogeneity in federated setting. 
% These two works commonly apply FedAVG directly to LoRA modules, without considering that the aggregation approach introduces interference. With the above consideration, 
As discussed before, \cite{sun2024improving} design a federated finetuning framework FFA-LoRA by freezing projection-down matrices for all the clients and only updating projection-up matrices. %in FS-LLM \cite{kuang2023federatedscopellm}, a framework for finetuning LLM, is introduced. 
FLoRA \cite{wang2024florafederatedfinetuninglarge} considers clients with heterogeneous-rank LoRA adapters and proposes a federated fine-tuning approach.