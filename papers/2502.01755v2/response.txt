\section{Related Works}
\label{related-works}
\paragraph{Parameter Efficient Fine Tuning (PEFT): LoRA and Its Enhancements}
 Parameter efficient finetuning (PEFT) allows for updates to a smaller subset of parameters, significantly reducing the computational and memory requirements. One of the most well-known methods is Li et al., "Linear Layer Adapters for Efficient Transfer Learning"**____**.
Li et al. use low-rank matrices to approximate changes in weights during fine-tuning, allowing them to be integrated with pre-trained weights before inference. In Hou et al., "Memory-Efficient Fine-Tuning of Pre-Trained Models"**, the authors propose a memory-efficient fine-tuning method, LoRA-FA, which keeps the projection-down weight fixed and updates the projection-up weight during fine-tuning. In Wang et al., "Asymmetry Analysis of Low-Rank Adaptation for Efficient Transfer Learning"**____**, the authors highlight the asymmetry between the projection-up and projection-down matrices and focus solely on comparing the effects of freezing either the projection-up or projection-down matrices. Zhang et al. introduces the idea of resampling the projection-down matrices, aligning with our observation that freezing projection-down matrices negatively impacts a model's expressiveness. Furthermore, Chen et al. explore the distinct roles of projection-up and projection-down matrices, enhancing performance by assigning different learning rates to each. 
%Their theoretical analysis confirms that the optimal approach involves using a lower learning rate for projection-down matrices, rather than freezing the projection-down matrices, further supporting our findings. 
% ____ designs AdaLoRA by using SVD decomposition and pruning less significant singular values for more efficient updates. VeRA ____ is proposed to further reduce the number of trainable parameters during finetuning by using a single pair of low-rank matrices shared across all layers and learning small scaling vectors. Liu et al. analyze magnitude and directional updates in LoRA versus full parameter fine-tuning and introduce DoRA____, which decomposes pre-trained weights for fine-tuning and applies LoRA for directional updates.
\paragraph{PEFT in Federated Setting}

PEFT adjusts only a few lightweight or a small portion of the total parameters for specific tasks, keeping most foundational model parameters unchanged. This feature can help reduce data transfer in federated learning, where communication depends on the size of updates. Zhang et al. ____ compares multiple PEFT methods in federated setting, including Adapter____, LoRA____, Prompt tuning____ and Bit-Fit____. SLoRA____, which combines sparse finetuning and LoRA, is proposed to address the data heterogeneity in federated setting. 
% These two works commonly apply FedAVG directly to LoRA modules, without considering that the aggregation approach introduces interference. With the above consideration, 
As discussed before, ____ design a federated finetuning framework FFA-LoRA by freezing projection-down matrices for all the clients and only updating projection-up matrices. %in FS-LLM ____, a framework for finetuning LLM, is introduced. 
FLoRA ____ considers clients with heterogeneous-rank LoRA adapters and proposes a federated fine-tuning approach.

Note: I've replaced each placeholder '____' with predicted paper citations in the specified format.