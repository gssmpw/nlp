[
  {
    "index": 0,
    "papers": [
      {
        "key": "hu2021lora",
        "author": "Edward J. Hu and Yelong Shen and Phillip Wallis and Zeyuan Allen-Zhu and Yuanzhi Li and Shean Wang and Lu Wang and Weizhu Chen",
        "title": "LoRA: Low-Rank Adaptation of Large Language Models"
      }
    ]
  },
  {
    "index": 1,
    "papers": [
      {
        "key": "zhang2023lorafa",
        "author": "Longteng Zhang and Lin Zhang and Shaohuai Shi and Xiaowen Chu and Bo Li",
        "title": "LoRA-FA: Memory-efficient Low-rank Adaptation for Large Language Models Fine-tuning"
      }
    ]
  },
  {
    "index": 2,
    "papers": [
      {
        "key": "zhu2024asymmetrylowrankadaptersfoundation",
        "author": "Jiacheng Zhu and Kristjan Greenewald and Kimia Nadjahi and Haitz S\u00e1ez de Oc\u00e1riz Borde and Rickard Br\u00fcel Gabrielsson and Leshem Choshen and Marzyeh Ghassemi and Mikhail Yurochkin and Justin Solomon",
        "title": "Asymmetry in Low-Rank Adapters of Foundation Models"
      }
    ]
  },
  {
    "index": 3,
    "papers": [
      {
        "key": "hao2024floralowrankadapterssecretly",
        "author": "Yongchang Hao and Yanshuai Cao and Lili Mou",
        "title": "Flora: Low-Rank Adapters Are Secretly Gradient Compressors"
      }
    ]
  },
  {
    "index": 4,
    "papers": [
      {
        "key": "hayou2024lora",
        "author": "Soufiane Hayou and Nikhil Ghosh and Bin Yu",
        "title": "LoRA+: Efficient Low Rank Adaptation of Large Models"
      }
    ]
  },
  {
    "index": 5,
    "papers": [
      {
        "key": "zhang2023adalora",
        "author": "Qingru Zhang and Minshuo Chen and Alexander Bukharin and Nikos Karampatziakis and Pengcheng He and Yu Cheng and Weizhu Chen and Tuo Zhao",
        "title": "AdaLoRA: Adaptive Budget Allocation for Parameter-Efficient Fine-Tuning"
      }
    ]
  },
  {
    "index": 6,
    "papers": [
      {
        "key": "kopiczko2024vera",
        "author": "Dawid Jan Kopiczko and Tijmen Blankevoort and Yuki M Asano",
        "title": "Ve{RA}: Vector-based Random Matrix Adaptation"
      }
    ]
  },
  {
    "index": 7,
    "papers": [
      {
        "key": "liu2024dora",
        "author": "Shih-Yang Liu and Chien-Yi Wang and Hongxu Yin and Pavlo Molchanov and Yu-Chiang Frank Wang and Kwang-Ting Cheng and Min-Hung Chen",
        "title": "DoRA: Weight-Decomposed Low-Rank Adaptation"
      }
    ]
  },
  {
    "index": 8,
    "papers": [
      {
        "key": "zhang-etal-2023-fedpetuning",
        "author": "Zhang, Zhuo  and\nYang, Yuanhang  and\nDai, Yong  and\nWang, Qifan  and\nYu, Yue  and\nQu, Lizhen  and\nXu, Zenglin",
        "title": "{F}ed{PET}uning: When Federated Learning Meets the Parameter-Efficient Tuning Methods of Pre-trained Language Models"
      }
    ]
  },
  {
    "index": 9,
    "papers": [
      {
        "key": "houlsby2019parameterefficient",
        "author": "Neil Houlsby and Andrei Giurgiu and Stanislaw Jastrzebski and Bruna Morrone and Quentin de Laroussilhe and Andrea Gesmundo and Mona Attariyan and Sylvain Gelly",
        "title": "Parameter-Efficient Transfer Learning for NLP"
      }
    ]
  },
  {
    "index": 10,
    "papers": [
      {
        "key": "hu2021lora",
        "author": "Edward J. Hu and Yelong Shen and Phillip Wallis and Zeyuan Allen-Zhu and Yuanzhi Li and Shean Wang and Lu Wang and Weizhu Chen",
        "title": "LoRA: Low-Rank Adaptation of Large Language Models"
      }
    ]
  },
  {
    "index": 11,
    "papers": [
      {
        "key": "liu2022ptuning",
        "author": "Xiao Liu and Kaixuan Ji and Yicheng Fu and Weng Lam Tam and Zhengxiao Du and Zhilin Yang and Jie Tang",
        "title": "P-Tuning v2: Prompt Tuning Can Be Comparable to Fine-tuning Universally Across Scales and Tasks"
      }
    ]
  },
  {
    "index": 12,
    "papers": [
      {
        "key": "zaken2022bitfit",
        "author": "Elad Ben Zaken and Shauli Ravfogel and Yoav Goldberg",
        "title": "BitFit: Simple Parameter-efficient Fine-tuning for Transformer-based Masked Language-models"
      }
    ]
  },
  {
    "index": 13,
    "papers": [
      {
        "key": "babakniya2023slora",
        "author": "Sara Babakniya and Ahmed Elkordy and Yahya Ezzeldin and Qingfeng Liu and Kee-Bong Song and MOSTAFA EL-Khamy and Salman Avestimehr",
        "title": "{SL}o{RA}: Federated Parameter Efficient Fine-Tuning of Language Models"
      }
    ]
  },
  {
    "index": 14,
    "papers": [
      {
        "key": "sun2024improving",
        "author": "Youbang Sun and Zitao Li and Yaliang Li and Bolin Ding",
        "title": "Improving Lo{RA} in Privacy-preserving Federated Learning"
      }
    ]
  },
  {
    "index": 15,
    "papers": [
      {
        "key": "kuang2023federatedscopellm",
        "author": "Weirui Kuang and Bingchen Qian and Zitao Li and Daoyuan Chen and Dawei Gao and Xuchen Pan and Yuexiang Xie and Yaliang Li and Bolin Ding and Jingren Zhou",
        "title": "FederatedScope-LLM: A Comprehensive Package for Fine-tuning Large Language Models in Federated Learning"
      }
    ]
  },
  {
    "index": 16,
    "papers": [
      {
        "key": "wang2024florafederatedfinetuninglarge",
        "author": "Ziyao Wang and Zheyu Shen and Yexiao He and Guoheng Sun and Hongyi Wang and Lingjuan Lyu and Ang Li",
        "title": "FLoRA: Federated Fine-Tuning Large Language Models with Heterogeneous Low-Rank Adaptations"
      }
    ]
  }
]