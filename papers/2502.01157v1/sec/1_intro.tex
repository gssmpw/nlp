% \vspace{-5em}
\input{fig/subteaser/item}

\input{fig/ray_based_effects/item}

\section{Introduction}
\label{sec:intro}
% 
Neural radiance fields~(NeRF) have revolutionized 3D computer vision by allowing the extraction of dense 3D representations from collections of 2D images~\cite{nerf}.
In their early days, radiance fields suffered from low training and testing/rendering speed.
Since then, researchers have developed more efficient models~\cite{ingp}, as well as techniques for distilling models which enable highly efficient rendering/testing~\cite{mobilenerf}.
In particular, the real-time rendering performance was made possible by leveraging the rasterization pipeline readily available on GPU hardware.
Soon thereafter, researchers proposed to also incorporate rasterization into the training process, leading to the development of the now wildly popular 3D Gaussian Splatting~(3DGS)~\cite{gsplat}.


% --- 
By comparing the development of radiance fields to that of classical graphics, one may argue that history is repeating itself: rasterization was introduced in computer graphics to \textit{approximate} the rendering equation~\cite{kajiya}, so to make it amenable to real-time rendering workloads.
However, light effects that are trivial to implement in ray tracing~(e.g. reflections,  refractions, and transparency; see \cref{fig:ray-based-effects}), are rather difficult to implement with rasterization.\footnote{After all, is this really that surprising, given that the physics of light is taught as the propagation of light rays through an environment?}
Consider the ``Graphics Gems'' series of books\footnote{\url{http://www.graphicsgems.org}} as a practical example, given how much of their content describes clever implementation tricks to express advanced light effects with rasterization engines.
Nonetheless, since the introduction of dedicated hardware support (NVIDIA RTX) in 2018, real-time rendering engines have, at least partially, returned to simulate light transport via ray tracing.

% --- 
Recently, research on 3DGS representations has flourished, and much of this research is seemingly trying to fix issues \textit{caused} by rasterization, such as the removal of 3DGS popping artifacts~\cite{stopthepop}, or the introduction of more complex camera projection models~\cite{spherical3dgs}.
Others integrated ray tracing with 3DGS, so to accelerate rendering and enable more complex light behavior~\cite{3dgrt}.
But these developments have not discouraged researchers from investigating new, better, 3D representations.
The community craves a return to polygonal meshes, as meshes are the unquestionable workhorse of modern computer graphics.
And while we can find very interesting attempts at employing meshes for the modeling of radiance fields~\cite{tetrasplat, dmtet, tetranerf}, as we will later discuss, none of these has realistically been able to oust 3DGS as~\textit{the} representation for learning radiance fields.

% --- What we do
Rather than revisiting history, and proposing clever engineering tricks that enable rasterization to work slightly better, we take a drastically different approach.
In particular, we highlight how, over two decades ago,~\citet{ray_plane} showed that fields represented by volumetric meshes admit a very efficient ray-tracing algorithm which requires no special hardware.
This approach has been subsequently overlooked in the resurgence of volume rendering methods, and we hope to re-introduce volumetric mesh models which can benefit from it to the differentiable rendering community.\footnote{It is also interesting to note that the volume rendering tutorial by~\citet{max05} that is cited in conjunction with~\citet{nerf} describes volumetric rendering within the context of volumetric meshes that \textit{partition} space, which is more similar to~\cite{fastray} than to~\cite{nerf}.}
In this paper, we make this representation differentiable, and carefully design refinement techniques for the underlying mesh.
These refinements allow us to accurately represent the surface of objects, and yet render efficiently by skipping empty portions of volume.

% --- contributions
In a nutshell, our solution, which we name \texttt{Radiant Foam}~(\cref{fig:subteaser}) provides 3DGS-like rendering speed and quality, but has a training modality based on rays that resembles the one from NeRF~\cite{nerf}.
This implies that many NeRF techniques can now be seamlessly adapted to our method, with the significant advantage that the underlying geometry is \textit{explicitly} represented by a volumetric mesh.
We parameterize this volumetric mesh as a 3D Voronoi diagram~(\cref{fig:teaser}), which enables training of a mesh structure with dynamic connectivity through gradient descent by avoiding the discontinuities associated with discrete changes in other representations.
We also propose a coarse-to-fine training approach, which enables rapid construction of mesh models with adaptive resolution.

\input{fig/ray_tracing/item}