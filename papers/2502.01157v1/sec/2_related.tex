\section{Related Work}
\label{sec:related}
% 
Neural Radiance Fields (NeRFs)~\cite{nerf} represent 3D scenes as volumetric radiance fields encoded within coordinate-based neural networks. 
This representation allows querying the network at any spatial location to retrieve volumetric density and view-dependent color, facilitating the generation of photo-realistic novel views.
The success of NeRFs has led to numerous follow-up works. 
For instance, significant effort has been devoted to enhancing training speed~\cite{ingp, kilonerf}, quality~\cite{mipnerf, mipnerf360}, and to extract surfaces from the representation~\cite{neus, volsdf}.
Finally, several works investigated ways to speed up the inference by baking the neural fields to more performant representations~\cite{mobilenerf, quadfields, smerf, adaptive-shells, binary-opacity-grids}.
While achieving high quality and fast rendering speeds, these methods often employ multi-stage training procedures.

\paragraph{Point-based rasterization}
Point-based rendering with primitives such as circles, spheres, or ellipsoids~\cite{surface-splatting, surfels, object-ewa} laid the foundations for point-based rasterization.
Differentiable rendering using depth-based blending has also been extended to volumetric particles in Pulsar~\cite{pulsar}, which employs sphere-based differentiable rasterization to render scenes with millions of spheres in real time.
While Pulsar provides a differentiable representation, \citet{gsplat} provides more expressive primitives in the form of soft, anistropic Gaussians, which can be differentiably optimized to fit the scene content.
This approach has inspired several follow-up works aimed at reducing its reliance on strong initialization~\cite{3dgs-mcmc}, reducing rendering time and memory footprints~\cite{light-3dgs, compressed-3dgs, reducing-mem-3dgs}, enabling the reconstruction of surfaces~\cite{sugar, 2dsplat}, and improving their ability to be trained for large spatial extents~\cite{octree-gs, hierarchicalgaussians}.

\paragraph{Extensions of 3DGS}
While significant progress has been made, these approaches still inherit the inherent limitations of rasterization. 
They struggle to handle camera distortions, model secondary lighting effects, or simulate sensor-specific properties like rolling shutter or motion blur. 
Recent efforts have aimed to address these challenges.
For example~\citet{radsplat} distilled a Zip-NeRF~\cite{zipnerf} into a 3DGS so to model distorted cameras and rolling shutter effects.
% 
To capture secondary lighting effects, recent works have explored incorporating occlusion information into spherical harmonics for each Gaussian~\cite{relightable-3dgs, gs-ir}, or leveraging shading models and environment maps~\cite{gaussianshader}. 
These methods either rely on rasterization during inference~\cite{relightable-3dgs}, or during training~\cite{gs-ir, gaussianshader} hence inheriting the limitations of rasterization.
For complex lens effects, \citet{3dgs-on-the-move} modeled motion blur and rolling shutter by \textit{approximating} them in screen space through rasterization and pixel velocities.
\quad
Our approach introduces a principled framework for efficient ray tracing of volumetric primitives, overcoming the aforementioned limitations and enabling the simulation of effects like reflection, refraction, and camera distortion.

\input{fig/delaunay_voronoi/item}

\subsection{Ray tracing of volumetric primitives}
Ray tracing has been a cornerstone of photo-realistic rendering since its introduction~\cite{illumination-model}.
It enables the accurate simulation of light interactions with scene geometry, making it indispensable for applications requiring effects such as shadows, reflections, and refractions. 
Modern advancements in hardware have further accelerated ray tracing, allowing for real-time applications~\cite{harmonic-coordinates}.
Recently, \citet{3dgrt} proposed to use the NVIDIA OptiX ray tracer with 3DGS~\cite{gsplat} for fast ray-tracing.
While the method performs real-time ray tracing, it suffers from the tendency of 3DGS~\cite{gsplat} to produce overlapping primitives which degrade the quality of the hierarchical acceleration structure and increase the number of intersections per ray.
\quad
Our method represents the scene using a non-overlapping polyhedral mesh without the need for a secondary acceleration structure, thereby efficiently avoiding these limitations.

\subsection{Delaunay triangulation and Voronoi diagrams}
% 
In computer graphics, Delaunay triangulations~\cite{delaunay} and Voronoi diagrams~\cite{voronoi} have been extensively studied for meshing (surfaces and volumes), and spatial partitioning~\cite{delaunay_voronoi_watson, voronoi_guibas, delaunay_shewchuk}.
Recently, they have also found novel applications in the realm of differentiable rendering.
DMTet~\cite{dmtet} introduced a deformable tetrahedral grid, generated using Delaunay triangulation, as an underlying 3D representation, while Tet-Splatting~\cite{tetrasplat} implements a differentiable rasterizer for this representation.
DeRF~\cite{derf} leverages Voronoi diagrams to spatially decompose a scene, resulting in faster training and inference, while Tetra-NeRF~\cite{tetranerf} employs a tetrahedral Delaunay mesh to model scenes more effectively.
While these methods inherit the ray-marching approach of~\cite{nerf} for volume rendering, they share a common limitation: slow rendering speeds caused by many MLP evaluations required per ray.
% 
\quad
% 
In contrast, with our method we propose to \textit{optimize} the mesh topology generated by the Voronoi diagram as a \textit{differentiable} polygonal mesh.
Our efficient ray tracing of volumetric particles also significantly accelerates the rendering speed.
