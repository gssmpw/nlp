\section{Background and Related Work}
\label{sec: Prelims}

We start by providing the required background on graph and sequence models. We further discuss the existing literature on over-smoothing and over-squashing in GNNs and vanishing gradients in recurrent sequence models. 
\vspace{-0.2cm}
\subsection{Message Passing Neural Networks}

Let a graph $G$ be a tuple $(V, E)$ where $V$ is the set of nodes and $ E$ is the set of edges. We denote edge from node $u\in V$ to node $v\in V$ with $(u,v)\in E$. The connectivity structure of the graph is encoded through an \textit{adjacency matrix} defined as $\mathbf{A} \in \mathbb{R}^{n\times n}$ where $n$ is the number of nodes in the graph. We assume that $G$ is an undirected graph and that there is a set of feature vectors $\{\mathbf{h}_{v}\}_{v\in V} \in \mathbb{R}^d$, with each feature vector being associated with a node in the graph. Graph Neural Networks (GNNs) are functions $f_{\boldsymbol{\theta}}: (G, \{\mathbf{h}_{v}\}) \mapsto \mathbf{y}$, with parameters $\boldsymbol{\theta}$ trained via gradient descent and $\mathbf{y}$ being a node-level or graph level prediction label. These models typically take the form of Message Passing Neural Networks (MPNNs), which compute latent representation by composing $K$ layers of the following node-wise operation:
\begin{equation}
    \mathbf{h}_{u}^{(k)} = \phi^{(k)} ( \mathbf{h}_{u}^{(k-1)}, \psi^{(k)} ( \{ \mathbf{h}_{v}^{(k-1)} : (u,v)\in E \} ) ),
\end{equation}
for $k=\{1,\hdots, K\}$, where $\psi^{(k)}$ is a \textit{permutation invariant aggregation function} and $\phi^{(k)}$ \textit{combines} the incoming messages from one's neighbors with the previous embedding of oneself to produce an updated representation. The most commonly used aggregation function takes the form 
\begin{equation}
    \psi^{(k)} ( \{ \mathbf{h}_{v}^{(k-1)} : (u,v)\in E \} )
    = \sum_{u}\Tilde{\mathbf{A}}_{uv}\mathbf{h}_{v}^{(k-1)}
\end{equation}
where $\Tilde{\mathbf{A}} = \mathbf{D}^{-\frac{1}{2}}\mathbf{A}\mathbf{D}^{-\frac{1}{2}}$, and $\mathbf{D}\in\mathbb{R}^{n\times n}$ is a diagonal matrix where $\mathbf{D}_{ii}=\sum_j\mathbf{A}_{ij}$. One can also consider a matrix representation of the features $\mathbf{H}^{(k)}\in\mathbb{R}^{n\times d_k}$. Throughout the paper, we will use the terms GNN and MPNN interchangeably, and will generally consider the most widely used instance of GNNs, which are Graph Convolutional Networks (GCNs)____ whose matrix update equation is given by:
\begin{equation}
\mathbf{H}^{(k)}=\sigma\big(\Hat{\mathbf{A}}\mathbf{H}^{(k-1)}\mathbf{W}^{(k-1)}\big),
    \label{eq:gcn}
\end{equation}
where $\Hat{\mathbf{A}}=\left(\mathbf{D}+\mathbf{I}\right)^{-1/2} \left(\mathbf{A} + \mathbf{I}\right)\left(\mathbf{D}+\mathbf{I}\right)^{-1/2}$ is the adjacency matrix with added self connections through the identity matrix $\mathbf{I}$, and $\sigma(\cdot)$ is a nonlinearity. Our analysis also applies to Graph Attention Networks (GATs) ____, where the fixed normalized adjacency is replaced by a learned adjacency matrix which dynamically modulates connectivity while preserving the key spectral properties used in our analysis.


\begin{figure}
	\centering
       \includegraphics[ width=0.7\linewidth]{Figures/edge_of_chaos_evolution.pdf}
       \caption{Latent evolution of 2-dimensional node features when passing through layers of a GNN-SSM with $\text{eig}(\Lambda)\approx 1$. The colors indicate the norm of the feature at each node, and the vector field indicates direction. }
       \label{fig:illustration}
       \vspace{-0.4cm}
\end{figure}
\vspace{-0.2cm}

\subsection{Recurrent Neural Networks}


%Let $S_d^\ell$ denote the space of length-$\ell$ vector-valued sequences, defined as:
%\[
%S_d^\ell := \left\{ (\mathbf{u}_t)_{t \in [\ell]} : \mathbf{u}_t \in \mathbb{R}^d \right\} \equiv \mathbb{R}^{\ell \times d}
%\]
%We denote the time index with a subscript roman letter and additional dimensions with greek superscripts, e.g., $x_\alpha^{(t)}$ for $t \in [\ell]$ and $\alpha \in [d]$.

A Recurrent Neural Network (RNN) is a function $g_{\boldsymbol{\theta}}: \mathbf{x} \mapsto \mathbf{y}$, where  $\mathbf{x} = (\mathbf{x}^{(1)}, \mathbf{x}^{(2)}, \ldots, \mathbf{x}^{(K)})$ and $\mathbf{y} = (\mathbf{y}^{(1)}, \mathbf{y}^{(2)}, \ldots, \mathbf{y}^{(K)})$, where $\mathbf{x}^{(k)} \in \mathbb{R}^d$ is the input vector at time step $k$ and $\mathbf{y}^{(k)} \in \mathbb{R}^m$ is the output vector at time step $k$, and $\boldsymbol{\theta}$ are learnable parameters. RNNs are designed to handle sequential data by maintaining a hidden state $\mathbf{h}^{(k)}\in\mathbb{R}^{d_h}$ that captures information from previous time steps. This hidden state\footnote{Note that we purposefully maintain the same notation for the hidden state as the one in the previous subsection for node features.} allows the network to model sequential dependencies in the data. The update equations for the hidden state of the RNN are as follows:
\begin{equation}
    \mathbf{h}^{(k)} = \sigma(\mathbf{W}_h \mathbf{h}^{(k-1)} + \mathbf{W}_x \mathbf{x}^{(k)}).
    \label{eq:rnn}
\end{equation}

This type of approach has deep connections with ideas from dynamical systems ____ and chaotic systems ____. These ideas have become more relevant in recent work ____, where the nonlinearity in \eqref{eq:rnn} is removed in the interest of parallelization and the ability to directly control the dynamics of the system through the eigenvalues of state transition matrix $\mathbf{W}_h$. We note that these types of approaches are also popular in the \textit{reservoir computing} literature ____, where the state transition matrix is left untrained and more emphasis is placed on the dynamics of the model.
\vspace{-0.2cm}

\subsection{The Vanishing and Exploding Gradient Problem}

Both RNNs and GNNs are trained using the chain rule. One can backpropagate gradients w.r.t. the weights at $i^{\text{th}}$ layer of a $K$-layer GNN or RNN as 
\begin{equation}
	\frac{\partial\mathcal{L}}{\partial\mathbf{\boldsymbol{\theta}}^{(i)}}
	=
	\frac{\partial\mathcal{L}}{\partial\mathbf{H}^{(K)}}
	\prod_{k=i+1}^{K}
	\frac{\partial\mathbf{H}^{(k)}}{\partial\mathbf{H}^{(k-1)}}
	\frac{\partial\mathbf{H}^{(i)}}{\partial\mathbf{\boldsymbol{\theta}}^{(i)}},
\end{equation}
where matrix $\mathbf{H}^{(k)}$ in an RNN will contain a single state vector. As identified by ____, a major issue in training this type of models arises from the \textit{product Jacobian}, given by:  
\begin{align}
	\mathbf{J}
	&=
	\prod_{k=i+1}^{K}
	\frac{\partial\mathbf{H}^{(k)}}{\partial\mathbf{H}^{(k-1)}} = \prod_{k=i+1}^{K}\mathbf{J}_k.
\end{align}
In general, we have that if $||\mathbf{J}_k||_2 \approx \lambda$ for all layers then $||\mathbf{J}||_2 \le \lambda^{K-i}$. This means that we require $\lambda \approx 1$ for gradients to neither explode nor vanish, a condition also known as \textit{edge of chaos}. %A way of determining the stability of the product Jacobian is through its \textit{Lyapunov exponents} $\xi_1\ge \xi_2\ge \hdots \ge \xi_{d_h}$ which are defined as the asymptotic
%time-averaged logarithms of the singular values of $\mathbf{J}$ ____:
%\begin{equation}
%    \xi_i = \lim_{n\rightarrow\infty}\frac{1}{n}\sum_{k=0}^{n-1}\log(\sigma_i(\prod_{k=0}^{n}\mathbf{J}_k))
%\end{equation}

%where $\sigma_i(\prod_{k=0}^{n}\mathbf{J}_k)$ is the $i^{\text{th}}$ singular value of the product Jacobian. Thus, positive Lyapunov exponents indicate exponential growth while negative ones indicate exponential contraction. In turn, $\xi_i = 0$ indicates edge of chaos behavior. 
\vspace{-0.2cm}



\subsection{Over-smoothing, Over-squashing, and Vanishing Gradients in GNNs}
\vspace{-0.1cm}
\paragraph{Over-smoothing.} Over-smoothing ____ describes the tendency of GNNs to produce \emph{smoother} representations as more and more layers are added. In particular, this has been related to the convergence to the $1$-dimensional kernel of the graph Laplacian and equivalently as a minimization process of the Dirichlet energy ____. In Section \ref{sec:over-smoothing}, we study this issue from the lens of vanishing gradients and show that \textbf{over-smoothing has a much more simple explanation}: it occurs due to the norm-contracting nature of GNN updates.
\vspace{-0.15cm}

\paragraph{Over-squashing.} Over-squashing ____ was originally introduced as a \textit{bottleneck} resulting from `squashing' into node representations amounts of information that are growing potentially exponentially quickly due to the topology of the graph. It is often characterized by the quantity $\left \lVert \partial \mathbf{h}_u^{(K)} / \partial \mathbf{h}_v^{(0)} \right \rVert$ being low, implying that the final representation of node $u$ is not very sensitive to the initial representation at some other node $v$. While the relationship between over-squashing and vanishing gradients was hinted at by ____, in Section \ref{sec:over-squashing} we explore this relationship in detail by showing that \textbf{techniques aimed to mitigate vanishing gradients in sequence models help to mitigate over-squashing in GNNs}.

\vspace{-0.3cm}
\paragraph{Vanishing gradients.}
Vanishing gradients have been extensively studied in RNNs ____, while this problem has been surprisingly mostly overlooked in the GNN community. For a detailed discussion on the relevant literature, we point the reader to the Appendix~\ref{app:supplementary_related_work}. We simply highlight that there are works that have seen success in taking ideas from sequence modelling ____ or signal propagation ____ and bridging them to GNNs, but they rarely have a detailed discussion on vanishing gradients. In Section \ref{sec:over-squashing}, we show that \textbf{vanishing gradient mitigation techniques from RNNs seem to be very effective towards the mitigation of over-smoothing and over-squashing in GNNs} and argue that the two communities have at a fundamental level very aligned problems and goals.

\vspace{-0.2cm}