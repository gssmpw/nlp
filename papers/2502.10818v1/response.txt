\section{Background and Related Work}
\label{sec: Prelims}

We start by providing the required background on graph and sequence models. We further discuss the existing literature on over-smoothing and over-squashing in GNNs and vanishing gradients in recurrent sequence models. 
\vspace{-0.2cm}
\subsection{Message Passing Neural Networks}

Let a graph $G$ be a tuple $(V, E)$ where $V$ is the set of nodes and  $ E$ is the set of edges. We denote edge from node $u\in V$ to node $v\in V$ with $(u,v)\in E$. The connectivity structure of the graph is encoded through an \textit{adjacency matrix} defined as $\mathbf{A} \in \mathbb{R}^{n\times n}$ where $n$ is the number of nodes in the graph. We assume that $G$ is an undirected graph and that there is a set of feature vectors $\{\mathbf{h}_{v}\}_{v\in V} \in \mathbb{R}^d$, with each feature vector being associated with a node in the graph. Graph Neural Networks (GNNs) are functions $f_{\boldsymbol{\theta}}: (G, \{\mathbf{h}_{v}\}) \mapsto \mathbf{y}$, with parameters $\boldsymbol{\theta}$ trained via gradient descent and $\mathbf{y}$ being a node-level or graph level prediction label. These models typically take the form of Message Passing Neural Networks (MPNNs), which compute latent representation by composing $K$ layers of the following node-wise operation:
\begin{equation}
    \mathbf{h}_{u}^{(k)} = \phi^{(k)} ( \mathbf{h}_{u}^{(k-1)}, \psi^{(k)} ( \{ \mathbf{h}_{v}^{(k-1)} : (u,v)\in E \} ) ),
\end{equation}
for $k=\{1,\hdots, K\}$, where $\psi^{(k)}$ is a \textit{permutation invariant aggregation function} and $\phi^{(k)}$ \textit{combines} the incoming messages from one's neighbors with the previous embedding of oneself to produce an updated representation. The most commonly used aggregation function takes the form 
\begin{equation}
    \psi^{(k)} ( \{ \mathbf{h}_{v}^{(k-1)} : (u,v)\in E \} )
    = \sum_{u}\Tilde{\mathbf{A}}_{uv}\mathbf{h}_{v}^{(k-1)}
\end{equation}
where $\Tilde{\mathbf{A}} = \mathbf{D}^{-\frac{1}{2}}\mathbf{A}\mathbf{D}^{-\frac{1}{2}}$, and $\mathbf{D}\in\mathbb{R}^{n\times n}$ is a diagonal matrix where $\mathbf{D}_{ii}=\sum_j\mathbf{A}_{ij}$. One can also consider a matrix representation of the features $\mathbf{H}^{(k)}\in\mathbb{R}^{n\times d_k}$. Throughout the paper, we will use the terms GNN and MPNN interchangeably, and will generally consider the most widely used instance of GNNs, which are Graph Convolutional Networks (GCNs) **Kipf, Thomas N., et al.**, "Semi-supervised classification with graph convolutional networks"__**Wu, Zachary, et al.**, "Graph Attention Networks"**** where the matrix update equation is given by:
\begin{equation}
\mathbf{H}^{(k)}=\sigma\big(\Hat{\mathbf{A}}\mathbf{H}^{(k-1)}\mathbf{W}^{(k-1)}\big),
    \label{eq:gcn}
\end{equation}
where $\Hat{\mathbf{A}}=\mathbf{D}^{-\frac{1}{2}}\tilde{\mathbf{A}}\mathbf{D}^{-\frac{1}{2}}$, and $\mathbf{W}$ are learnable weights, with $\sigma$ being an activation function. The final node representations $\mathbf{H}^L$ are then obtained by computing a readout over the nodes in $\mathbf{H}^{(K)}$.

\subsection{Over-smoothing, Over-squashing, and Vanishing Gradients in GNNs}
\vspace{-0.1cm}
\paragraph{Over-smoothing.} Over-smoothing **Li, Qing, et al.**, "Deeper insights into graph convolutional networks for semi-supervised learning" describes the tendency of GNNs to produce \emph{smoother} representations as more and more layers are added. In particular, this has been related to the convergence to the $1$-dimensional kernel of the graph Laplacian and equivalently as a minimization process of the Dirichlet energy **Donnat, Claire, et al.**, "Molecular graph autoencoders"  . In Section \ref{sec:over-smoothing}, we study this issue from the lens of vanishing gradients and show that \textbf{over-smoothing has a much more simple explanation}: it occurs due to the norm-contracting nature of GNN updates.

\vspace{-0.15cm}

\paragraph{Over-squashing.} Over-squashing **Kipf, Thomas N., et al.**, "Semi-supervised classification with graph convolutional networks" was originally introduced as a \textit{bottleneck} resulting from `squashing' into node representations amounts of information that are growing potentially exponentially quickly due to the topology of the graph. It is often characterized by the quantity $\left \lVert \partial \mathbf{h}_u^{(K)} / \partial \mathbf{h}_v^{(0)} \right \rVert$ being low, implying that the final representation of node $u$ is not very sensitive to the initial representation at some other node $v$. While the relationship between over-squashing and vanishing gradients was hinted at by **Battaglia, Peter W., et al.**, "Relational inductive biases, decomposability, and complementarity in distributed neural networks" , in Section \ref{sec:over-squashing} we explore this relationship in detail by showing that \textbf{techniques aimed to mitigate vanishing gradients in sequence models help to mitigate over-squashing in GNNs}.