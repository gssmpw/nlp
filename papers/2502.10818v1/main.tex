%%%%%%%% ICML 2025 EXAMPLE LATEX SUBMISSION FILE %%%%%%%%%%%%%%%%%

\documentclass{article}



% Recommended, but optional, packages for figures and better typesetting:
\usepackage{microtype}
\usepackage{graphicx}
\usepackage{booktabs} % for professional tables
\usepackage{subfig}
\usepackage{enumitem}



% hyperref makes hyperlinks in the resulting PDF.
% If your build breaks (sometimes temporarily if a hyperlink spans a page)
% please comment out the following usepackage line and replace
% \usepackage{icml2025} with \usepackage[nohyperref]{icml2025} above.
\usepackage{hyperref}


% Attempt to make hyperref and algorithmic work together better:
\newcommand{\theHalgorithm}{\arabic{algorithm}}

% Use the following line for the initial blind version submitted for review:
%\usepackage{icml2025}

% If accepted, instead use the following line for the camera-ready submission:
\usepackage[accepted]{icml2025}

% For theorems and such
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{amsthm}

\usepackage{mathabx}
\usepackage{booktabs}
\usepackage{arydshln}
\usepackage{dsfont}
%\usepackage{subcaption}
\usepackage{hyperref}
\usepackage{url}
\usepackage{wrapfig}
\usepackage{tcolorbox}

\usepackage{multirow}
\usepackage{multicol}

\newtheorem{prop}{Proposition}



%\linebreak[10000]


% if you use cleveref..
\usepackage[capitalize,noabbrev]{cleveref}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% THEOREMS
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\theoremstyle{plain}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{assumption}[theorem]{Assumption}
\theoremstyle{remark}
\newtheorem{remark}[theorem]{Remark}

% Todonotes is useful during development; simply uncomment the next line
%    and comment out the line below the next line to turn off comments
%\usepackage[disable,textsize=tiny]{todonotes}
\usepackage[textsize=tiny]{todonotes}

\usepackage{amsmath}


\usepackage{enumitem} 

\newcommand{\revise}[1]{\textcolor{red}{#1}}
\newcommand{\ie}{i.e., }
\newcommand{\eg}{e.g., }
\newcommand{\lip}[1]{\left\lVert {#1} \right\rVert_{\text{Lip}}}
\newcommand{\norm}[1]{\left\lVert {#1} \right\rVert}
\newcommand{\vect}[1]{\text{vec}\left( {#1}\right)}

% The \icmltitle you define below is probably too long as a header.
% Therefore, a short form for the running title is supplied here:
\icmltitlerunning{On Vanishing Gradients, Over-Smoothing, and Over-Squashing in GNNs}

\begin{document}

\twocolumn[
\icmltitle{On Vanishing Gradients, Over-Smoothing, and Over-Squashing in GNNs:\\ Bridging Recurrent and Graph Learning}

% It is OKAY to include author information, even for blind
% submissions: the style file will automatically remove it for you
% unless you've provided the [accepted] option to the icml2025
% package.

% List of affiliations: The first argument should be a (short)
% identifier you will use later to specify author affiliations
% Academic affiliations should list Department, University, City, Region, Country
% Industry affiliations should list Company, City, Region, Country

% You can specify symbols, otherwise they are numbered in order.
% Ideally, you should not use this facility. Affiliations will be numbered
% in order of appearance and this is the preferred way.
\icmlsetsymbol{equal}{*}


\begin{icmlauthorlist}
\icmlauthor{√Ålvaro Arroyo}{equal,mlrg,omi}
\icmlauthor{Alessio Gravina}{equal,pisa}
\icmlauthor{Benjamin Gutteridge}{mlrg}
\icmlauthor{Federico Barbero}{compsci}
\icmlauthor{Claudio Gallicchio}{pisa}\\
\icmlauthor{Xiaowen Dong}{mlrg,omi}
\icmlauthor{Michael Bronstein}{compsci,aithyra}
\icmlauthor{Pierre Vandergheynst}{lts2}
\end{icmlauthorlist}


\icmlaffiliation{mlrg}{Machine Learning Research Group, University of Oxford}
\icmlaffiliation{omi}{Oxford-Man Insitute, University of Oxford}
\icmlaffiliation{compsci}{Department of Computer Science, University of Oxford}
\icmlaffiliation{pisa}{Deptartment of Computer Science, Univesity of Pisa}
\icmlaffiliation{aithyra}{AITHYRA}
\icmlaffiliation{lts2}{LTS2 Lab, EPFL}


\icmlcorrespondingauthor{Alvaro Arroyo}{alvaro.arroyo@eng.ox.ac.uk}


% You may provide any keywords that you
% find helpful for describing your paper; these are used to populate
% the "keywords" metadata in the PDF but will not be shown in the document
\icmlkeywords{Machine Learning, ICML}

\vskip 0.3in
]

% this must go after the closing bracket ] following \twocolumn[ ...

% This command actually creates the footnote in the first column
% listing the affiliations and the copyright notice.
% The command takes one argument, which is text to display at the start of the footnote.
% The \icmlEqualContribution command is standard text for equal contribution.
% Remove it (just {}) if you do not need this facility.

%\printAffiliationsAndNotice{}  % leave blank if no need to mention equal contribution
\printAffiliationsAndNotice{\icmlEqualContribution} % otherwise use the standard text.

\begin{abstract}    
    Graph Neural Networks (GNNs) are models that leverage the graph structure to transmit information between nodes, typically through the message-passing operation. While widely successful, this approach is well-known to suffer from the over-smoothing and over-squashing phenomena, which result in representational collapse as the number of layers increases and insensitivity to the information contained at distant and poorly connected nodes, respectively. In this paper, we present a unified view of these problems through the lens of \textit{vanishing gradients}, using ideas from linear control theory for our analysis. We propose an interpretation of GNNs as recurrent models and empirically demonstrate that a simple state-space formulation of a GNN effectively alleviates over-smoothing and over-squashing \emph{at no extra trainable parameter cost}. Further, we show theoretically and empirically that (i) GNNs are by design prone to extreme gradient vanishing even after a few layers; (ii) Over-smoothing is directly related to the mechanism causing vanishing gradients; (iii) Over-squashing is most easily alleviated by a combination of graph rewiring and vanishing gradient mitigation. We believe our work will help bridge the gap between the recurrent and graph neural network literature and will unlock the design of new deep and performant GNNs. 
\end{abstract}

\section{Introduction}

Graph Neural Networks (GNNs) \cite{sperduti1993encoding, gori2005new, scarselli2008graph, bruna2014spectral, defferrard2017convolutional} have become a widely used architecture for processing information on graph domains. Most GNNs operate via \textit{message passing}, where information is exchanged between neighboring nodes, giving rise to Message-Passing Neural Networks (MPNNs). Some of the most popular instances of this type of architecture include GCN \cite{kipf2017semisupervised}, GAT \cite{Velickovic2018GraphAN}, GIN \cite{xu2018powerful}, and GraphSAGE \cite{hamilton2017inductive}. 

Despite its widespread use, this paradigm also suffers from some fundamental limitations. Most importantly, we highlight the issue of \textit{over-smoothing} \cite{nt2019revisiting,cai2020note, rusch2023survey}, where feature representations become exponentially similar as the number of layers increases,  and \textit{over-squashing} \cite{Alon2021OnTB,topping2021understanding,di2023over}, which describes the difficulty of propagating information across faraway nodes, as the exponential growth in a node's receptive field results in many messages being compressed into fixed-size vectors. Although these two issues have been studied extensively, and there exists evidence that they are trade-offs of each other \cite{giraldo2023trade}, 
there is no unified theoretical framework that explains \textit{why architectures which solve these problems work} and whether there exists a common \textit{underlying cause} that governs these problems.

In this work, we analyze over-smoothing and over-squashing from the lens of \textbf{vanishing gradients}. In particular, we ask several questions about the appearance and consequences of this phenomenon in GNNs: (i) How prone are GNNs to gradient vanishing? (ii) What is the effect of gradient vanishing on over-smoothing? (iii) Can preventing vanishing gradients effectively mitigate over-squashing? (iv) Can methods used in the non-linear \cite{hochreiter1997long, pascanu2013difficulty,beck2024xlstm} 
 and more recently linear \cite{gu2023mamba,orvieto2023resurrecting} recurrent neural network (RNN) literature be effective at dealing with over-smoothing and over-squashing? With the latter questions, we aim to fill the gaps and open questions left in the over-squashing analysis of \citet{di2023over}. Further, we hope the point on over-smoothing will help provide a theoretical explanation for why certain architectural choices have led to the design of deep and performant GNNs.
 
\paragraph{Contributions and outline.} In summary, the contributions of this work are the following:
\begin{itemize}[itemsep=0.5pt,topsep=0.5pt]
    \item In Section \ref{sec: method}, we explore the connection between GNNs and sequence models and demonstrate how graph convolutional and attentional models are susceptible to a phenomenon we term \textit{extreme gradient vanishing}. We propose GNN-SSM, a GNN model that is written as a state-space model, allowing for a better control of the spectrum of the Jacobian. \vspace{-0.15cm}
    \item In Section \ref{sec:over-smoothing}, we show how vanishing gradients contribute to over-smoothing, providing a much more precise explanation for why over-smoothing occurs in practice in GNNs by studying the spectrum of the layer-wise Jacobians. We show that GNN-SSMs are able to \textit{exactly} control the rate of smoothing.\vspace{-0.15cm}
    \item In Section \ref{sec:over-squashing}, we show how vanishing gradients are related to over-squashing. We argue that over-squashing should therefore be tackled by approaches that both perform graph rewiring \emph{and} mitigate vanishing gradients. \vspace{-0.35cm}
\end{itemize}
Overall, we believe that our work provides a new and interesting perspective on well-known problems that occur in GNNs, from the point of view of sequence models. We believe this to be an important observation connecting two very wide -- yet surprisingly disjoint -- bodies of literature. 
\vspace{-0.3cm}

\section{Background and Related Work}
\label{sec: Prelims}

We start by providing the required background on graph and sequence models. We further discuss the existing literature on over-smoothing and over-squashing in GNNs and vanishing gradients in recurrent sequence models. 
\vspace{-0.2cm}
\subsection{Message Passing Neural Networks}

Let a graph $G$ be a tuple $(V, E)$ where $V$ is the set of nodes and $ E$ is the set of edges. We denote edge from node $u\in V$ to node $v\in V$ with $(u,v)\in E$. The connectivity structure of the graph is encoded through an \textit{adjacency matrix} defined as $\mathbf{A} \in \mathbb{R}^{n\times n}$ where $n$ is the number of nodes in the graph. We assume that $G$ is an undirected graph and that there is a set of feature vectors $\{\mathbf{h}_{v}\}_{v\in V} \in \mathbb{R}^d$, with each feature vector being associated with a node in the graph. Graph Neural Networks (GNNs) are functions $f_{\boldsymbol{\theta}}: (G, \{\mathbf{h}_{v}\}) \mapsto \mathbf{y}$, with parameters $\boldsymbol{\theta}$ trained via gradient descent and $\mathbf{y}$ being a node-level or graph level prediction label. These models typically take the form of Message Passing Neural Networks (MPNNs), which compute latent representation by composing $K$ layers of the following node-wise operation:
\begin{equation}
    \mathbf{h}_{u}^{(k)} = \phi^{(k)} ( \mathbf{h}_{u}^{(k-1)}, \psi^{(k)} ( \{ \mathbf{h}_{v}^{(k-1)} : (u,v)\in E \} ) ),
\end{equation}
for $k=\{1,\hdots, K\}$, where $\psi^{(k)}$ is a \textit{permutation invariant aggregation function} and $\phi^{(k)}$ \textit{combines} the incoming messages from one's neighbors with the previous embedding of oneself to produce an updated representation. The most commonly used aggregation function takes the form 
\begin{equation}
    \psi^{(k)} ( \{ \mathbf{h}_{v}^{(k-1)} : (u,v)\in E \} )
    = \sum_{u}\Tilde{\mathbf{A}}_{uv}\mathbf{h}_{v}^{(k-1)}
\end{equation}
where $\Tilde{\mathbf{A}} = \mathbf{D}^{-\frac{1}{2}}\mathbf{A}\mathbf{D}^{-\frac{1}{2}}$, and $\mathbf{D}\in\mathbb{R}^{n\times n}$ is a diagonal matrix where $\mathbf{D}_{ii}=\sum_j\mathbf{A}_{ij}$. One can also consider a matrix representation of the features $\mathbf{H}^{(k)}\in\mathbb{R}^{n\times d_k}$. Throughout the paper, we will use the terms GNN and MPNN interchangeably, and will generally consider the most widely used instance of GNNs, which are Graph Convolutional Networks (GCNs)~\citep{kipf2017semisupervised} whose matrix update equation is given by:
\begin{equation}
\mathbf{H}^{(k)}=\sigma\big(\Hat{\mathbf{A}}\mathbf{H}^{(k-1)}\mathbf{W}^{(k-1)}\big),
    \label{eq:gcn}
\end{equation}
where $\Hat{\mathbf{A}}=\left(\mathbf{D}+\mathbf{I}\right)^{-1/2} \left(\mathbf{A} + \mathbf{I}\right)\left(\mathbf{D}+\mathbf{I}\right)^{-1/2}$ is the adjacency matrix with added self connections through the identity matrix $\mathbf{I}$, and $\sigma(\cdot)$ is a nonlinearity. Our analysis also applies to Graph Attention Networks (GATs) \cite{Velickovic2018GraphAN}, where the fixed normalized adjacency is replaced by a learned adjacency matrix which dynamically modulates connectivity while preserving the key spectral properties used in our analysis.


\begin{figure}
	\centering
       \includegraphics[ width=0.7\linewidth]{Figures/edge_of_chaos_evolution.pdf}
       \caption{Latent evolution of 2-dimensional node features when passing through layers of a GNN-SSM with $\text{eig}(\Lambda)\approx 1$. The colors indicate the norm of the feature at each node, and the vector field indicates direction. }
       \label{fig:illustration}
       \vspace{-0.4cm}
\end{figure}
\vspace{-0.2cm}

\subsection{Recurrent Neural Networks}


%Let $S_d^\ell$ denote the space of length-$\ell$ vector-valued sequences, defined as:
%\[
%S_d^\ell := \left\{ (\mathbf{u}_t)_{t \in [\ell]} : \mathbf{u}_t \in \mathbb{R}^d \right\} \equiv \mathbb{R}^{\ell \times d}
%\]
%We denote the time index with a subscript roman letter and additional dimensions with greek superscripts, e.g., $x_\alpha^{(t)}$ for $t \in [\ell]$ and $\alpha \in [d]$.

A Recurrent Neural Network (RNN) is a function $g_{\boldsymbol{\theta}}: \mathbf{x} \mapsto \mathbf{y}$, where  $\mathbf{x} = (\mathbf{x}^{(1)}, \mathbf{x}^{(2)}, \ldots, \mathbf{x}^{(K)})$ and $\mathbf{y} = (\mathbf{y}^{(1)}, \mathbf{y}^{(2)}, \ldots, \mathbf{y}^{(K)})$, where $\mathbf{x}^{(k)} \in \mathbb{R}^d$ is the input vector at time step $k$ and $\mathbf{y}^{(k)} \in \mathbb{R}^m$ is the output vector at time step $k$, and $\boldsymbol{\theta}$ are learnable parameters. RNNs are designed to handle sequential data by maintaining a hidden state $\mathbf{h}^{(k)}\in\mathbb{R}^{d_h}$ that captures information from previous time steps. This hidden state\footnote{Note that we purposefully maintain the same notation for the hidden state as the one in the previous subsection for node features.} allows the network to model sequential dependencies in the data. The update equations for the hidden state of the RNN are as follows:
\begin{equation}
    \mathbf{h}^{(k)} = \sigma(\mathbf{W}_h \mathbf{h}^{(k-1)} + \mathbf{W}_x \mathbf{x}^{(k)}).
    \label{eq:rnn}
\end{equation}

This type of approach has deep connections with ideas from dynamical systems \cite{strogatz2018nonlinear} and chaotic systems \cite{engelken2023lyapunov}. These ideas have become more relevant in recent work \cite{Gu2019LearningMR, orvieto2023resurrecting}, where the nonlinearity in \eqref{eq:rnn} is removed in the interest of parallelization and the ability to directly control the dynamics of the system through the eigenvalues of state transition matrix $\mathbf{W}_h$. We note that these types of approaches are also popular in the \textit{reservoir computing} literature \cite{jaeger2001echo}, where the state transition matrix is left untrained and more emphasis is placed on the dynamics of the model.
\vspace{-0.2cm}

\subsection{The Vanishing and Exploding Gradient Problem}

Both RNNs and GNNs are trained using the chain rule. One can backpropagate gradients w.r.t. the weights at $i^{\text{th}}$ layer of a $K$-layer GNN or RNN as 
\begin{equation}
	\frac{\partial\mathcal{L}}{\partial\mathbf{\boldsymbol{\theta}}^{(i)}}
	=
	\frac{\partial\mathcal{L}}{\partial\mathbf{H}^{(K)}}
	\prod_{k=i+1}^{K}
	\frac{\partial\mathbf{H}^{(k)}}{\partial\mathbf{H}^{(k-1)}}
	\frac{\partial\mathbf{H}^{(i)}}{\partial\mathbf{\boldsymbol{\theta}}^{(i)}},
\end{equation}
where matrix $\mathbf{H}^{(k)}$ in an RNN will contain a single state vector. As identified by \citet{pascanu2013difficulty}, a major issue in training this type of models arises from the \textit{product Jacobian}, given by:  
\begin{align}
	\mathbf{J}
	&=
	\prod_{k=i+1}^{K}
	\frac{\partial\mathbf{H}^{(k)}}{\partial\mathbf{H}^{(k-1)}} = \prod_{k=i+1}^{K}\mathbf{J}_k.
\end{align}
In general, we have that if $||\mathbf{J}_k||_2 \approx \lambda$ for all layers then $||\mathbf{J}||_2 \le \lambda^{K-i}$. This means that we require $\lambda \approx 1$ for gradients to neither explode nor vanish, a condition also known as \textit{edge of chaos}. %A way of determining the stability of the product Jacobian is through its \textit{Lyapunov exponents} $\xi_1\ge \xi_2\ge \hdots \ge \xi_{d_h}$ which are defined as the asymptotic
%time-averaged logarithms of the singular values of $\mathbf{J}$ \cite{engelken2023lyapunov}:
%\begin{equation}
%    \xi_i = \lim_{n\rightarrow\infty}\frac{1}{n}\sum_{k=0}^{n-1}\log(\sigma_i(\prod_{k=0}^{n}\mathbf{J}_k))
%\end{equation}

%where $\sigma_i(\prod_{k=0}^{n}\mathbf{J}_k)$ is the $i^{\text{th}}$ singular value of the product Jacobian. Thus, positive Lyapunov exponents indicate exponential growth while negative ones indicate exponential contraction. In turn, $\xi_i = 0$ indicates edge of chaos behavior. 
\vspace{-0.2cm}



\subsection{Over-smoothing, Over-squashing, and Vanishing Gradients in GNNs}
\vspace{-0.1cm}
\paragraph{Over-smoothing.} Over-smoothing \cite{cai2020note, oono2020graph, rusch2023survey} describes the tendency of GNNs to produce \emph{smoother} representations as more and more layers are added. In particular, this has been related to the convergence to the $1$-dimensional kernel of the graph Laplacian and equivalently as a minimization process of the Dirichlet energy \cite{di2022graph}. In Section \ref{sec:over-smoothing}, we study this issue from the lens of vanishing gradients and show that \textbf{over-smoothing has a much more simple explanation}: it occurs due to the norm-contracting nature of GNN updates.
\vspace{-0.15cm}

\paragraph{Over-squashing.} Over-squashing \cite{Alon2021OnTB, topping2021understanding, di2023over, barbero2024transformers} was originally introduced as a \textit{bottleneck} resulting from `squashing' into node representations amounts of information that are growing potentially exponentially quickly due to the topology of the graph. It is often characterized by the quantity $\left \lVert \partial \mathbf{h}_u^{(K)} / \partial \mathbf{h}_v^{(0)} \right \rVert$ being low, implying that the final representation of node $u$ is not very sensitive to the initial representation at some other node $v$. While the relationship between over-squashing and vanishing gradients was hinted at by \citet{di2023over}, in Section \ref{sec:over-squashing} we explore this relationship in detail by showing that \textbf{techniques aimed to mitigate vanishing gradients in sequence models help to mitigate over-squashing in GNNs}.

\vspace{-0.3cm}
\paragraph{Vanishing gradients.}
Vanishing gradients have been extensively studied in RNNs \cite{bengio1994learning, hochreiter1997long, pascanu2013difficulty}, while this problem has been surprisingly mostly overlooked in the GNN community. For a detailed discussion on the relevant literature, we point the reader to the Appendix~\ref{app:supplementary_related_work}. We simply highlight that there are works that have seen success in taking ideas from sequence modelling \cite{rusch2022graph, gravina2022anti,rusch2023gradient, wang2024mamba,behrouz2024graphmamba, kiani2024unitary} or signal propagation \cite{epping2024graph, scholkemper2024residual} and bridging them to GNNs, but they rarely have a detailed discussion on vanishing gradients. In Section \ref{sec:over-squashing}, we show that \textbf{vanishing gradient mitigation techniques from RNNs seem to be very effective towards the mitigation of over-smoothing and over-squashing in GNNs} and argue that the two communities have at a fundamental level very aligned problems and goals.

\vspace{-0.2cm}



\section{Connecting Sequence and Graph Learning through State-Space Models}
\label{sec: method}


In this section, we study GNNs from a sequence model perspective. We show that the most common classes of GNNs are more prone to vanishing gradients than feedforward or recurrent networks due to the spectral contractive nature of the normalized adjacency matrix. We then propose GNN-SSMs, a state-space-model-inspired construction of a GNN that allows more direct control of the spectrum.
\vspace{-0.3cm}
\subsection{Similarities and differences between learning on sequences and graphs}
\vspace{-0.1cm}
The GNN architectures that first popularized deep learning on graphs \citep{bruna2014spectral, defferrard2017convolutional} were initially presented as a generalization of Convolutional Neural Networks (CNNs) to irregular domains. GCNs \citep{kipf2017semisupervised} subsequently restricted the architecture in \citet{defferrard2017convolutional} to a one-hop neighborhood. While this is still termed ‚Äúconvolutional‚Äù (due to weight sharing across nodes), the iterative process of aggregating information from each node‚Äôs neighborhood can also be viewed as \textit{recurrent-like} state updates.

If we consider an RNN unrolled over time, it forms a directed path graph feeding into a state node with a self-connection‚Äî making it a special case of a GNN. Conversely, node representations in GNNs can be stacked using matrix vectorization, allowing us to interpret GNN layer operations as iterative state updates. This connection suggests that the main difficulty faced by RNNs ‚Äî namely the vanishing and exploding gradients problem \citep{pascanu2013difficulty} ‚Äî may likewise hinder the learning ability of GNNs. We note, however, that one key difference between RNNs and GNNs is that RNN memory \textit{only} depends on how much information is dissipated by the model during the hidden state update, whereas GNNs normalize messages by the inverse node degree, which introduces an additional information dissipation step that we will explore in more detail in Section \ref{sec:over-squashing}.
\vspace{-0.25cm}
\subsection{Graph convolutional and attentional models are prone to extreme gradient vanishing} 
\vspace{-0.1cm}
Based on the previously introduced notion of stacking node representations using the matrix vectorization operation, we now analyze the gradient dynamics of GNN. In particular, we focus on the gradient propagation capabilities of graph convolutional and attentional models at initialization, given their widespread use in the literature. Specifically, we demonstrate that the singular values of the layer-wise Jacobian in these models form a highly contractive mapping, which prevents effective information propagation beyond a few layers. We formalize this claim in Lemma~\ref{lem:JacobianSpectrum} and Theorem~\ref{thm:JacobianDistribution}, and we refer the reader to Appendix~\ref{app:proofs_jac} for the corresponding proofs.

\begin{lemma}[Spectrum of the Jacobian's singular values]
\label{lem:JacobianSpectrum}
Let $\mathbf{H}^{(k)} \;=\; \tilde{\mathbf{A}}\;\mathbf{H}^{(k-1)}\;\mathbf{W}$  be a linear GCN layer, where $\tilde{\mathbf{A}}$ has eigenvalues $\{\lambda_1,\ldots,\lambda_n\}$ and $\mathbf{W}\,\mathbf{W}^T$ has eigenvalues $\{\mu_1,\ldots,\mu_{d_k}\}$. Consider the layer-wise Jacobian $\mathbf{J} = \partial\,\mathrm{vec}\bigl(\mathbf{H}^{(k)}\bigr) / \partial\,\mathrm{vec}\bigl(\mathbf{H}^{(k-1)}\bigr)$, Then the squared singular values of $\mathbf{J}$ are given by the set 
\[
  \bigl\{\,\lambda_i^2 \,\mu_j \;\bigm|\;
    i=1,\ldots,n,\;\; j=1,\ldots,d_k\bigr\}.\\
\]
\end{lemma}


\begin{theorem}[Jacobian singular-value distribution]
\label{thm:JacobianDistribution}
Assume the setting of Lemma~\ref{lem:JacobianSpectrum}, and let 
$\mathbf{W}\in\mathbb{R}^{d_{k-1}\times d_k}$ be initialized with i.i.d.\ 
$\mathcal{N}(0,\sigma^2)$ entries. Denote the squared singular values of the 
Jacobian by $\gamma_{i,j}$. Then, for sufficiently large $d_k$ the empirical eigenvalue distribution of $\mathbf{W}\mathbf{W}^T$ converges to the 
Marchenko-Pastur distribution. Then, 
the mean and variance of each $\gamma_{i,j}$ are
 \vspace{-0.05cm}
\begin{align}
  \mathbb{E}\bigl[\gamma_{i,j}\bigr]
  &= 
  \lambda_i^2 \,\sigma^2,
  \label{eq:exp} \\[2pt]
  \mathrm{Var}\bigl[\gamma_{i,j}\bigr]
  &=
  \lambda_i^4 \,\sigma^4\,\frac{d_k}{d_{k-1}}.
  \label{eq:var}
\end{align}
 \vspace{-0.6cm}
\end{theorem} 

Theorem~\ref{thm:JacobianDistribution} shows that the singular-value spectrum of the Jacobian is modulated by the squared spectrum of the normalized adjacency. Since $|\lambda_i|\le 1$ for all eigenvalues of the normalized adjacency, the ability of GCNs to propagate gradients is strictly worse than that of RNNs or MLPs. In particular, iterating these operations causes the majority of the spectrum to shrink to zero more quickly than in classical deep linear~\cite{saxe2013exact} or nonlinear~\cite{pennington2017resurrecting} networks. Moreover, using sigmoidal activations and orthogonal weights will not push the singular-value spectrum to the edge of chaos as in \citet{pennington2017resurrecting}, due to the additional contraction from the adjacency. The effect of each operation on the layer-wise Jacobian is empirically demonstrated in Figure \ref{fig:jac}, which also showcases the contraction effect of the normalized adjacency. The figure reveals that even a single layer‚Äôs Jacobian exhibits a long tail of squared singular values near zero. This spectral structure leads to ill-conditioned gradient propagation and non-isometric (not norm-preserving) signal dynamics. The same results hold for GATs, as the adjacency still exhibits a contractive spectral structure despite being learned during training Finally, we also plot the Jacobian's singular value spectrum for several GNN architectures in Appendix \ref{app:additional_mpnn_jacobian}, which shows how this phenomenon is also present in other GNNs, even though it is less pronounced.% than in the GCN and GAT cases. 


Note that to overcome this contraction without altering the architecture, one would have to both set $\sigma^2$ in a way that precisely compensates for the normalized adjacency (which can be computationally expensive to estimate)and choose the nonlinearity carefully. In the next subsection, we present a general, simple, and computationally efficient method to place the Jacobian at the edge of chaos at initialization by writing feature updates in a state-space representation.



\begin{figure}
	\centering
       \includegraphics[ width=0.7\linewidth]{Figures/eig_change_Grid_random.pdf}
       \vspace{-0.35cm}
       \caption{Histogram of eigenvalue modulus of the Jacobian for linear, linear convolutional, and nonlinear convolutional layers.}
       \label{fig:jac}
       \vspace{-0.4cm}
\end{figure}

\vspace{-0.3cm}
\subsection{GNN-SSM: Improving the training dynamics of GNNs through state-space models}

To allow direct control of the signal propagation dynamics of any GNN, we can rewrite its layer-to-layer update as a state-space model. Concretely, we express the update as
\begin{align}
	\mathbf{H}^{(k+1)} &= \mathbf{\Lambda}\mathbf{H}^{(k)} +  \mathbf{B}\mathbf{X}^{(k)} \notag \\
    &=\mathbf{\Lambda}\mathbf{H}^{(k)} +  \mathbf{B}\mathbf{F}_{\boldsymbol{\theta}}(\mathbf{H}^{(k)}, k),
    \label{eq:ssm}
\end{align}
where we refer to $\mathbf{\Lambda}$ as the \textit{state transition matrix} and $\mathbf{B}$ as the \textit{input matrix},\footnote{Here, we deviate from the traditional state-space formalism, which uses $\mathbf{A}$ as the state transition matrix, since we use this notation for the adjacency.} and $\mathbf{F}_{\boldsymbol{\theta}}(\mathbf{H}^{(k)}, k)$ as a time-varying \textit{coupling function} which connects each node to some neighborhood. We refer to the model defined in Equation~\eqref{eq:ssm} as \texttt{GNN-SSM}. From an RNN perspective, \(\mathbf{\Lambda}\) plays the role of the ``memory", in charge of recalling all the representations at each layer at the readout layer, while the neighborhood aggregation  plays the role of an
input injected into the state via~\(\mathbf{B}\). In traditional GNNs, this
recurrent memory mechanism is absent, so these models act in a \emph{memoryless} way:
features at one layer do not explicitly store or retrieve past information in the way a stateful model would.


In the state-space view, the eigenvalues of \(\mathbf{\Lambda}\) determine the \emph{memory
dynamics}: large eigenvalues can preserve signals (or, if above unity, cause exploding modes),
whereas small eigenvalues quickly attenuate them. Meanwhile, \(\mathbf{B}\) controls which aspects
of the node features get injected into the hidden state at each step. Because this
framework is agnostic to the exact coupling function, any MPNN layer can serve as \(\mathbf{F}_{\boldsymbol{\theta}}\). We showcase the effect of these matrices on the layer-wise Jacobian in Proposition \ref{prop:ssm-jac}. 


\begin{proposition}[Effect of state-space matrices]
Consider the setting in \eqref{eq:ssm} and $\Gamma = \partial\ \mathrm{vec}(\mathbf{F}_{\boldsymbol{\theta}}(\mathbf{H}^{(k)}))/\partial\ \mathrm{vec}(\mathbf{H}^{(k)})$. Let $\otimes$ denote the Kronecker product.  Then, the norm of the vectorized Jacobian $\mathbf{J}$ is bounded as:
\begin{align}
\|\mathbf{J}\|_2 &\leq \|I_{d_k} \otimes \mathbf{\Lambda}\|_2 + \|I_{d_k} \otimes \mathbf{B}\|_2 \|\Gamma\|_2 \nonumber \\
&= \|\mathbf{\Lambda}\|_2 + \|\mathbf{B}\|_2 \|\Gamma\|_2,
\end{align}
\vspace{-0.8cm}
\label{prop:ssm-jac}
\end{proposition}
The result above shows that the spectrum of the Jacobian is controlled through the eigenvalues of the memory matrix $\mathbf{\Lambda}$. Since $\text{eig}(\Gamma)\approx 0$ (see previous subsection), it suffices to have $\text{eig}(\Lambda)\approx 1$ to bring the vectorized Jacobian to the edge of chaos. We empirically validate this in Figure \ref{fig:jacobian-ssm-fig}% below
.
\begin{figure}[H]
	\centering
        \includegraphics[width=0.48\linewidth]{Figures/eigs_GCN.pdf} \includegraphics[ width=0.48\linewidth]{Figures/eigs_GCN_struct.pdf}
        \vspace{-0.3cm}
        \caption{Vectorized Jacobian for different models. \textbf{Left:} GCN.  \textbf{Right:} GCN-SMM with $\text{eig}(\mathbf{\Lambda})\approx 1$ and $\text{eig}(\mathbf{B})\approx 0.1$.}
        \label{fig:jacobian-ssm-fig}
\end{figure}
\vspace{-0.4cm}

For simplicity and clarity of conclusions, we consider \(\mathbf{\Lambda}\) and \(\mathbf{B}\) to be \emph{shared across
layers} and \emph{fixed} (i.e., not trained by gradient descent). Only the coupling function
\(\mathbf{F}_{\boldsymbol{\theta}}\) is optimized. Empirically, we observe that this simpler scheme actually improves downstream performance in some settings. We highlight, however, that this is the most simple instance of a more general framework that aims to incorporate ideas from recurrent processing into GNNs without losing permutation-equivariance. One could easily extend this state-space idea to include more complex gating
\cite{hochreiter1997long, rusch2023gradient} or other constraints on the state transition matrix.  

\vspace{-0.1cm}

\begin{tcolorbox}[boxsep=0mm,left=2.5mm,right=2.5mm]
\textbf{Message of the Section:} {\em } \textit{GNNs, especially GCNs and GATs, experience a phenomenon we term ``extreme gradient vanishing" due to the use of a normalized adjacency for the message passing operation. Reinterpreting any GNN as a recurrent model enables direct control of the Jacobian spectrum, which mitigates this issue while preserving architectural generality.}  
\end{tcolorbox}

\vspace{-0.2cm}


\section{How is Over-smoothing linked to Vanishing Gradients?}\label{sec:over-smoothing}
In this section, we study the practical implications of the mechanism causing vanishing gradients in GNNs in relation to the over-smoothing phenomenon. We show theoretically how over-smoothing is a consequence of GNN layers acting as \textit{contractions}, which make node features collapse to a zero fixed point under certain conditions. We experimentally validate our points by analyzing Dirichlet energy, node feature norms, and node classification performance for increasing numbers of layers. Overall, we believe this section provides a more \textit{practical and general} understanding of over-smoothing, by analyzing any GNN from the point of view of its layer-wise Jacobians.
\subsection{Over-smoothing secretly occurs due to contractions in the Jacobian} 

Over-smoothing describes the tendency of node features to become too similar to each other as more layers are added in GNNs \cite{cai2020note,rusch2023survey}. This phenomenon is largely due to the nature of the operations that are performed by GNN layers and their relationship with \emph{heat equations over graphs}. Consequently, a common way of measuring over-smoothing in GNNs is via the \emph{graph Dirichlet energy} $ \mathcal{E}(\mathbf{H})$. Given a graph node signal $\mathbf{H} \in \mathbb{R}^{n\times d}$ on a graph $G$, $\mathcal{E}(\mathbf{H})$ takes the form:
\begin{equation}
 \mathcal{E}(\mathbf{H}) = \text{tr} \left(\mathbf{H}^\top \boldsymbol{\Delta}\mathbf{H}\right) = \sum_{(u,v) \in E} \left \lVert \mathbf{h}_u - \mathbf{h}_v \right \rVert^2,
\end{equation}
where $\boldsymbol{\Delta}$ is the (unweighted) graph Laplacian \cite{chung1997spectral}. The graph Dirichlet energy measures the \textit{smoothness} of a signal over a graph and will be minimized when the signal is constant over each node -- at least when using the non-normalized Laplacian. Notably, models like GCNs can be seen as minimizers of the Dirichlet energy\footnote{In general, this depends on the spectral properties of the learned weight matrices and on the details of the non-linearities used \cite{di2022graph}.} \cite{bodnar2022neural, di2022graph}. 
\begin{figure*}
    \centering
    % Left figure
    \includegraphics[width=0.3\textwidth]{Figures/oversmoothing_nonlin_Cora_notitle.pdf}%
    \quad
    % Middle figure (pushed upward)
    \raisebox{0.35cm}{ % Adjust the value (e.g., -1cm, -2cm) to move the image up
        \includegraphics[width=0.28\textwidth]{Figures/contractive_Cora_evolution.pdf}%
    }%
    \quad
    % Right figure
    \includegraphics[width=0.291\textwidth]{Figures/Cora_results.pdf}%
    \vspace{-0.2cm}
    \caption{Experimental evaluation on Cora for an increasing number of layers. \textbf{Left:} Dirichlet Energy evolution for different $||\Lambda||_2$. \textbf{Middle:} 2-Dimensional feature projection evolution with a fixed point at zero. \textbf{Right:} Node classification performance.}
    \label{fig:over-smoothing-results}
    \vspace{-0.4cm}
\end{figure*}


We consider in our analysis GNN layers as in Equation \ref{eq:gcn}. We view a GNN layer as a map $f_{k}: \mathbb{R}^{nd} \to \mathbb{R}^{nd}$ and construct a deep GNN $f$ via composition of $K$ layers, i.e. $f = f_K \circ \dots \circ f_1$. We also require the condition of our non-linearity $\sigma$ that $\sigma(0) = 0$, noting that this is the case for ReLU and $\tanh$, for example. We start by showing that in such a model $\mathbf{0}$ is a \emph{fixed point}.\footnote{In our analysis, it is important that the input to the GNN is a vector in $\mathbb{R}^{nd}$ rather than a matrix in $\mathbb{R}^{n\times d}$, as the Jacobians and norms are different for the two cases. For this reason, it is important to take care in the definitions of these objects.} 


\begin{lemma}
\label{lemma:fixed-point-gcn}
    Consider a GNN layer $f_K$ as in Equation \ref{eq:gcn}, with non-linearity $\sigma$ such that $\sigma(0) = 0$ (e.g. $\text{ReLU}$ or $\tanh$). Then, $f(\mathbf{0}) = \mathbf{0}$, i.e. $\mathbf{0}$ is a fixed point of $f$.
\end{lemma}

% \begin{proof}
%     $f_\ell(\mathbf{0}) = \sigma\left(\hat{\mathbf{A}}\mathbf{0}\mathbf{W}\right) = \sigma\left(\mathbf{0}\right) = \mathbf{0}$.
% \end{proof}

Let $\mathbf{J}_{f} \in \mathbb{R}^{nd \times nd}$ denote the layer-wise Jacobian of a GNN $f$. The supremum of the Jacobian (if well-defined) of $f$ over a convex set $U$ corresponds to the Lipschitz constant $\lip{f}$ \cite{hassan2002nonlinear}, i.e.
\begin{equation}
    \lip{f} = \sup_{\mathbf{H} \in U} \left \lVert \mathbf{J}_{f}(\mathbf{H}) \right \rVert,
\end{equation}
where by submultiplicativity of Lipschitz constants we have that $\lip{f} \leq \prod_{k=1}^K \lip{f_k}$. In the case of ReLU, as the function is not differentiable at $0$, one has to consider the (Clarke) generalized Jacobian \cite{jordan2020exactly}, a detail that we ignore to ease notation as it is not important for the scope of our work. We point to the Appendix~\ref{app:smoothing-results} for a more detailed explanation of the objects in question. A Lipschitz function $f$ is \emph{contractive} if $\lip{f} < 1$. We now assume that $\lip{f_k} < 1$ for all $k$, meaning that each layer is a \emph{contraction mapping}.\footnote{Note that the analysis holds for any submultiplicative matrix norm.} We derive the following result:



\begin{proposition}[Convergence to a unique fixed point.]
\label{prop:convergence-to-fixed-point}
    Let $\lip{f_k} < 1 - \epsilon$ for some $\epsilon > 0$ for all $k=1\dots L$. Then, for $\mathbf{H} \in U \subseteq \mathbb{R}^{nd}$, we have that:

    \begin{equation}
        \left \lVert f(\mathbf{H}) \right \rVert < (1 - \epsilon)^K \norm{\mathbf{H}} < \norm{\mathbf{H}}.
    \end{equation}

    In particular, as $K \to \infty$, $f(\mathbf{H}) \to \mathbf{0}$.
\end{proposition}

In other words, if layers $f_k$ are contractive, their repeated application will monotonically converge to the \emph{unique} fixed point $\mathbf{0}$, by Lemma \ref{lemma:fixed-point-gcn} and the Banach fixed point theorem. Each GNN layer has therefore the effect of \emph{contracting} the input vectors, reducing their norms. The rate of reduction is dictated by the spectral norm of the Jacobian of that layer and therefore by the spectral norm of $\mathbf{W}_k$, as shown in Section \ref{sec: method}. In Proposition \ref{prop:dir-energy-to-0}, we show how the layer-wise Jacobians relate to the Dirichlet energy.

\begin{proposition}[Contractions decrease Dirichlet energy.]
\label{prop:dir-energy-to-0}
    Let $f$ be a GNN, $|E|$ be the number of edges in $G$, and $\mathbf{H} \in \mathbb{R}^{nd}$. We have the following bound:
    \begin{equation}
        \mathcal{E}(f(\mathbf{H})) \leq  2 \lvert E \rvert \prod_{k=1}^K \lip{f_k}^2 \norm{\mathbf{H}}^2.
    \end{equation}
    In particular, if $\lip{f_k} < 1 - \epsilon$ for some $\epsilon > 0$ for all $k=1\dots K$, then as $K \to \infty$ we have that $\mathcal{E}(f(\mathbf{H})) \to 0$.
\end{proposition}

This result shows that the energy is directly controlled by the norm of the input signal $\mathbf{H}$ and by the contracting effect of the layers $f_k$. The repeated application of contractive layers results in the Dirichlet energy being artificially lowered as the signals are gradually reducing in norm. 


This provides a much more practical explanation for over-smoothing than the one usually encountered in the literature. Contrary to common consensus, which explains over-smoothing by showing that the signal is projected into the $1$-dimensional kernel of the graph Laplacian.  We instead describe over-smoothing as occurring due to the contractive nature of GNNs and their inputs converging in norm to exactly $\mathbf{0}$.
\vspace{-0.25cm}
\paragraph{Important consequences of our theoretical results.} The most important takeaway of the analysis above is that \textit{vanishing gradients are directly connected to over-smoothing}. In particular, the same mechanism that causes gradient vanishing issues - the non-isometric propagation of signals due to contractions of the Jacobian - is responsible for the collapse of all features to a unique zero fixed point where the Dirichlet energy is minimized. The quick collapse of traditional graph convolutional and attentional models can therefore be understood from the extreme gradient vanishing result introduced in Section \ref{sec: method}. 

This result also provides a connection between the study of GNNs and the study of signal propagation (or dynamical isometry) in feedforward networks \cite{saxe2013exact,poole2016exponential,pennington2017resurrecting} and recurrent neural networks \cite{hochreiter1997long, arjovsky2016unitary, orvieto2023resurrecting}. In the dynamical isometry literature, the primary interest is to improve the learning times of deep feedforward networks, whereas the recurrent neural network literature is interested in memory and long-range information retrieval. We highlight that \textit{translating ideas from these fields of study will enable the construction of deep and performant GNNs}, even though these techniques were originally developed with other objectives in mind. This also serves as an explanation of why simple modifications such as residual connections or normalization worked in practice to mitigate over-smoothing, given their links to dynamical isometry \cite{yang2019mean,metereztowards}. 

Finally, we highlight that this result provides an objective \textit{evaluation metric} to gauge whether a GNN will over-smooth or not. We hope that the eigenalysis of the Jacobian will become a widespread empirical test used for this purpose. In Appendix \ref{app:additional_over-smoothing}, we showcase how several models with edge-of-chaos Jacobians result in no over-smoothing.%, reinforcing the generality of this result beyond classical MPNNs.

\vspace{-0.35cm}

\subsection{Experimental validation of theoretical results}
\vspace{-0.1cm}

To validate the theory above, we perform a series of empirical tests. In particular, we check the evolution of the Dirichlet energy, latent vector norms, and node classification accuracy on the Cora dataset as the number of layers of different models is increased. The results are presented in Figure \ref{fig:over-smoothing-results}. Further, we present additional experiments for different graph structures and models in Appendix \ref{app:additional_over-smoothing}.


From Figure \ref{fig:over-smoothing-results}, we see that one can exactly control the evolution of the Dirichlet energy of the system through the spectrum of the Jacobian, which can, in turn, be modified through the spectrum of $\mathbf{\Lambda}$. Furthermore, this shrinks faster the lower the norm of the Jacobian is, which validates Proposition \ref{prop:dir-energy-to-0}. From the phase plot describing the two-dimensional evolution of the features, it is also clear that these converge to a unique fixed point at zero, in line with  Proposition \ref{prop:convergence-to-fixed-point}.  Beyond a Dirichlet energy analysis of the system, notice that node classification performance does not deteriorate when $\text{eig}(\mathbf{\Lambda})\approx 1$, and improves over simply applying an SSM layer with no modulation of the hyperparameters or a residual connection. The dynamics of the GNN in this setting are shown in Figure \ref{fig:illustration}.




\begin{tcolorbox}[boxsep=0mm,left=2.5mm,right=2.5mm]
\textbf{Message of the Section:} {\em \textit{There exists a direct link between the over-smoothing phenomenon in GNNs and the appearance of vanishing gradients. In particular, for contractive layerwise Jacobians and certain nonlinearities, node features collapse to a zero fixed point and thus minimize the Dirichlet energy.  Hence, analyzing the spectrum of the layer-wise Jacobians will reveal if a GNN will over-smooth or not. Furthermore, borrowing techniques from RNNs to design new GNNs is expected to be an effective strategy to prevent over-smoothing.}}   
\end{tcolorbox}


\vspace{-0.3cm}

\section{The Impact of Vanishing Gradients on Over-squashing}
\label{sec:over-squashing}

In this section, we study the connection between vanishing gradients and over-squashing in GNNs.

\vspace{-0.2cm}

\subsection{Mitigating over-squashing by combining increased  connectivity and non-dissipativity}
Over-squashing is typically measured via the sensitivity of a node embedding after $k$ layers with respect to the input of another node using the node-wise Jacobian.
\begin{theorem}[Sensitivity bounds, \citet{di2023over}]\label{theo:sensitivity_digiovanni}
    Consider a standard MPNN with $k$ layers, where $c_\sigma$ is the Lipschitz constant of the activation $\sigma$, $w$ is the maximal entry-value over all weight matrices, and $d$ is the embedding dimension. For $u,v\in V$ we have
    \begin{equation}\label{eq:mpnn_over-squashing}
    \left\|\frac{\partial\mathbf{h}_v^{(k)}}{\partial\mathbf{h}_u^{(0)}}\right\| \leq \underbrace{(c_\sigma w d)^k}_{model}\underbrace{(\mathbf{O}^k)_{vu}}_{topology},
\end{equation}    
with $\mathbf{O}=c_r\mathbf{I}+c_a\mathbf{A}\in\mathbb{R}^{n\times n}$ is the message passing matrix adopted by the MPNN, with $c_r$ and $c_a$ are the contributions of the self-connection and aggregation term.
\end{theorem}

 Theorem~\ref{theo:sensitivity_digiovanni} shows that the sensitivity of the node embedding arises from a combination of (i) a term based on the graph topology and (ii) a term dependent on the model dynamics, with over-squashing occurring when the right-hand side of Equation \eqref{eq:mpnn_over-squashing} becomes too small. We highlight that this differs from the standard product Jacobian which arises in RNNs. This is because in MPNNs, messages are scaled by the inverse node degree, incurring an extra information dissipation step. Consequently, while recurrent architectures only need to adjust their dynamics to ensure long memory, MPNNs must \textit{simultaneously} enhance graph connectivity and modify their dynamics to mitigate vanishing gradients.

Even though the sensitivity bound in Theorem~\ref{theo:sensitivity_digiovanni} is controlled by two components, the majority of the literature has typically focused on addressing only the topological term via \textit{graph rewiring} \cite{diffusion_improves, topping2021understanding, karhadkar2022fosr, barbero2023locality, finkelshteincooperative2024}, with some methods also targeting the model dynamics \cite{gravina2022anti, gravina_swan, heilig2024injecting}. In fact, \citet{di2023over} explicitly discourages increasing the model term in Theorem~\ref{theo:sensitivity_digiovanni} and claims that doing so could lead to over-fitting and poorer generalization. However, we argue that increasing the model term ‚Äî directly linked to vanishing gradients as discussed in Section~\ref{sec:over-smoothing} ‚Äî is essential to mitigate over-squashing. Rather than harming performance, boosting this term helps prevent over-smoothing, since even in a well-connected graph where information can be reached in fewer hops, unaddressed vanishing gradients due to the model term will cause the target node's features to decay to zero during message passing.



Frameworks combining these strategies include \citet{gutteridge2023drew}, which integrates graph rewiring with a delay term, and \citet{ding2024recurrent}, which merges multi-hop aggregation with ideas from SSMs.\footnote{Further links between the delay term and vanishing gradients are discussed in Appendix \ref{app:drew}. Further, we show that models tend to converge to the edge of chaos during training in Appendix \ref{app:additional_gpp}.} These approaches have generally led to state-of-the-art results, significantly improving performance over standalone rewiring techniques. 


\vspace{-0.3cm}

\subsection{Empirical validation of claims}\label{sec:over-squashing_results}

We focus our empirical validation on answering the following questions: (i) What is the result of combining an effective rewiring scheme with vanishing gradient mitigation? (ii) Will this result in similar state-of-the-art results? 
To investigate this, we construct a minimal model that combines high connectivity with non-dissipativity. In particular, we make of the GNN-SSM model and employ a k-hop aggregation scheme for the coupling function $\mathbf{F}_{\boldsymbol{\theta}}$, which we term \texttt{kGNN-SSM} (more details are provided in Appendix~\ref{app:kgnn_ssm}). 

\begin{figure}
	\centering
        \includegraphics[ width=0.525\linewidth]{Figures/ringtransfer_nonlinear_khop.pdf}\includegraphics[width=0.465\linewidth]{Figures/controlled_ring_khop.pdf}
        \vspace{-0.3cm}
        \caption{\textbf{Left:} Performance on the RingTransfer task for different models. \textbf{Right: }Effect of dissipativity on performance.}
        \label{fig:ringtransfer}
        \vspace{-0.5cm}
\end{figure}


\vspace{-0.4cm}
\begin{table}[H]
    \footnotesize

    \centering
    \caption{Ablation on LRGB datasets. Here, $d \uparrow $ means and increase in the latent dimention, while $-$ indicates the removal of a component and $+$ indicates an addition of a component. }
    \label{tab:accuracy}
    \vspace{1mm}
    \begin{tabular}{lcc}
    \toprule
    \multirow{2}{*}{Model} & \texttt{Pept-func} & \texttt{Pept-struct}       \\
                           & {\small AP $\uparrow$} & {\small MAE $\downarrow$}\\\midrule
    GCN          &    $60.93_{\pm0.138}$                                &   $33.41_{\pm0.041}$      \\
    \midrule
    kGCN-SSM                     &              $\underline{69.02}_{\pm0.218}$                      &     $28.98_{\pm0.324}$      \\
    $\,$ + $d \uparrow $            &         $\mathbf{72.12}_{\pm0.268}$                          &  $27.01_{\pm0.071}$         \\
    $\,$ $-$ $\text{eig}(\Lambda)\approx1$              & $61.41_{\pm0.724}$                              &  $\mathbf{25.81}_{\pm0.032}$  \\
    $\,$ $-$ SSM          &    $57.76_{\pm1.971}$                                &   $\underline{26.02}_{\pm0.213}$       \\
    $\,$ $-$ khop          &    $60.93_{\pm0.138}$                                &   $33.41_{\pm0.041}$       \\
    \midrule
    DRew-GCN       &     $68.04_{\pm1.442}$                         &  $27.66_{\pm0.187}$   \\
    $\,$ + $d \uparrow $       &               $68.05_{\pm0.626}$              &    $27.64_{\pm0.067}$ \\
    $\,$ $-$ Delay       &                 $49.02_{\pm2.512}$             &    $27.08_{\pm0.041}$ \\
     \bottomrule
    \end{tabular}
    \label{tab:LRGB}
    \vspace{-0.2cm}
\end{table}
\vspace{-0.2cm}

We start by testing the performance on the RingTransfer task introduced in \citet{di2023over}, as it is a task where we certifiably know that long-range dependencies exist. We modify the $\text{eig}(\Lambda)$ in the \texttt{kGNN-SSM} to move the Jacobian from the edge of stability to a progressively more dissipative state. The results are shown in Figure \ref{fig:ringtransfer}. From the figure, we see that (i) \texttt{kGNN-SSM} achieves state-of-the art performance only when coupling strong connectivity and an edge of chaos Jacobian (ii) making the model more dissipative directly results in worse long-range modeling capabilities. We believe the latter point demonstrates the importance of the model term in Theorem \ref{theo:sensitivity_digiovanni}. 



Next, we ablate each component of the model on three graph property prediction tasks introduced in \citet{gravina2022anti} alongside the real-world long-range graph benchmark (LRGB) from \citet{dwivedi2022LRGB}, focusing on the \texttt{peptides-func} and \texttt{peptides-struct} tasks. Additional details regarding the datasets and the experimental setting are reported in Appendix~\ref{app:experimental_details}. Here, we focus on ablating the effect of rewiring, adding an SSM layer, and placing the model at the edge of chaos through $\mathbf{\Lambda}$. In the LRBG tasks, we additionally ablate the effect of increasing the hidden memory size, as we consider forty layers in the \texttt{peptides-func} dataset, which requires more long-range capabilities. Here, we also ablate \texttt{DRew} \cite{gutteridge2023drew} under the same settings. We also provide a more detailed comparison with other models in Appendix \ref{app:additional_gpp}, and provide additional comments around the LRGB tasks in Appendix \ref{app:additional_LRGB}.

\vspace{-0.5cm}



\begin{table}[H]
%\begin{wraptable}{r}{9cm}%\begin{table}[h]
%\setlength{\tabcolsep}{1pt}
\centering
\caption{Mean and std. for test {\small$log_{10}(\mathrm{MSE})$} averaged over 4 random weight initializations on the GPP tasks. Lower is better. 
%\one{First}, \two{second}, and \three{third} best results for each task are color-coded.  
}
\label{tab:results_GraphProp}
\footnotesize
\setlength{\tabcolsep}{4pt}
%\vspace{1mm}
\begin{tabular}{lccc}
\toprule
Model &\texttt{Diam.} & \texttt{SSSP} & \texttt{Ecc.} \\\midrule
GCN                 & 0.742$_{\pm0.047}$ & 0.950$_{\pm9.18\cdot10^{-5}}$ & 0.847$_{\pm0.003}$  \\ 
$\,$ + SSM           & -2.431$_{\pm0.033}$ &    -2.821$_{\pm0.565}$  & -2.245$_{\pm0.003}$\\
$\,$ + $\text{eig}(\Lambda)\approx 1$ & \underline{-2.444}$_{\pm0.098}$ & \underline{-3.593}$_{\pm0.103}$ & \underline{-2.258}$_{\pm0.009}$ \\
$\,$ + k-hop         & \textbf{-3.075$_{\pm0.055}$} & \textbf{-3.604$_{\pm0.029}$} & \textbf{-4.265$_{\pm0.178}$} \\\midrule

% These results are DRew-GCN (khop=10)
DRew-GCN  & -2.369$_{\pm0.105}$ & -1.591$_{\pm0.003}$ & -2.100$_{\pm0.026}$\\
$\,$ + delay &  -2.402$_{\pm0.110}$ & -1.602$_{\pm0.008}$ & -2.029$_{\pm0.024}$\\
\bottomrule      
\end{tabular}
\vspace{-0.4cm}
\end{table}


The results are shown in Tables \ref{tab:LRGB} and \ref{tab:results_GraphProp}. Across the board, we observe that \texttt{kGNN-SSM} not only matches \texttt{DRew-Delay}, but also outperforms it by a large amount in all cases, showcasing the strength of our state-space approach. In particular, we generally observe significant decreases in performance when removing both the high connectivity and non-dissipativity components of the model, highlighting their individual importance. Finally, we see that increasing memory size plays a big role in the \texttt{peptides-struct} task, which is in line with what has been observed in sequence modeling \cite{gu2021}.



\begin{tcolorbox}[boxsep=0mm,left=2.5mm,right=2.5mm]
\textbf{Message of the Section:} {\em } Oversquashing in GNNs arises from both graph connectivity and the model‚Äôs capacity to avoid vanishing gradients. While most studies focus on connectivity, we argue that preserving signal strength through non-dissipative model dynamics is equally important. High connectivity allows nodes of interest to be reached in fewer message-passing steps while model dynamics ensure information is preserved. 
\end{tcolorbox}

\section{Conclusion}
\vspace{-0.15cm}
\label{sec: Conclusion}
In this work, we revisit the well-known problems of over-smoothing and over-squashing in GNNs from the lens of \textit{vanishing gradients}, by studying GNNs from the perspective of recurrent and state-space models. In particular, we show that GNNs are prone to a phenomenon we term \textit{extreme gradient vanishing}, which results in ill-conditioned signal propagation with few layers. As such, we argue that it is important to control the layerwise Jacobian and propose a state-space-inspired GNN model, termed \texttt{GNN-SSM}, to do so. We then uncover that vanishing gradients result in a \textit{very specific} form of over-smoothing in which all signals converge exactly to the $\mathbf{0}$ vector, and support this claim empirically.  Finally, we theoretically argue and empirically show that mitigation of over-squashing is best achieved through a combination of strong graph connectivity and non-dissipative model dynamics. 
\vspace{-0.45cm}
\paragraph{Limitations and Future Work.} We believe our work opens up a number of interesting directions that aim to bridge the gap between graph and sequence modeling. In particular, we hope that this work will encourage researchers to adapt vanishing gradient mitigation methods from the sequence modeling community to GNNs, and conversely explore how graph learning ideas can be brought to recurrent models. In our work, we mostly focused on GCN and GAT type updates, but we believe that our analysis can be extended to understand how different choices of updates and non-linearities affect training dynamics, which we leave for a future work.
\vspace{-0.3cm}

\section*{Acknowledgements}
\vspace{-0.1cm}

AA and XD thank the Oxford-Man Institute for financial support. AA thanks T. Anderson Keller for engaging comments on early versions of the manuscript, Max Welling for valuable pointers to the dynamical isometry literature, and T. Konstantin Rusch for sharing the code used for the Dirichlet energy experiments. 

\vspace{-0.35cm}

\section*{Impact Statement}
\vspace{-0.1cm}

This work advances the study of Graph Neural Networks (GNNs) by establishing a theoretical link between vanishing gradients, over-smoothing, and over-squashing in Message Passing Neural Networks (MPNNs). We propose a principled framework that investigates whether a common underlying cause governs these phenomena, alongside a state-space-inspired GNN model that effectively mitigates them. The research presented herein contributes to the broader understanding and development of GNNs. In this work, we do not release any datasets or models that could pose a significant risk of misuse. We believe our research does not have any direct or indirect negative societal
implications or harmful consequences. As far as we are aware, this study does not raise any ethical concerns or potential negative impacts. 


\bibliography{references}
\bibliographystyle{icml2025}

\newpage

\onecolumn

\appendix



\section{Theoretical Results}

\subsection{Proofs of Jacobian Theorems}
\label{app:proofs_jac}

\begin{definition}[Vectorization and Kronecker product]
\label{def:vectorization}

Let $\mathbf{X} \in \mathbb{R}^{m \times n}$ be a real matrix. The \emph{vectorization} of $\mathbf{X}$, denoted $\mathrm{vec}(\mathbf{X})$, is the $(mn)$-dimensional column vector obtained by stacking the columns of $\mathbf{X}$:
\[
  \mathrm{vec}(\mathbf{X}) 
  \;=\; 
  \begin{bmatrix}
    \mathbf{X}_{:,1} \\[6pt]
    \mathbf{X}_{:,2} \\
    \vdots \\
    \mathbf{X}_{:,n}
  \end{bmatrix}
  \;\in\;\mathbb{R}^{mn}.
\]
One key property of the vectorization operator is its relationship to the Kronecker product. In particular, for compatible matrices $\mathbf{A}, \mathbf{B}, \mathbf{C}$, we have
\[
  \mathrm{vec}\bigl(\mathbf{A}\,\mathbf{B}\,\mathbf{C}\bigr)
  \;=\;
  (\mathbf{C}^T \otimes \mathbf{A}) \,\mathrm{vec}\bigl(\mathbf{B}\bigr).
\]
Here, $\otimes$ denotes the Kronecker product.
\end{definition}

\begin{definition}[Wishart matrix]
\label{def:wishart}
    Let $\mathbf{X}\in \mathbb{R}^{n\times p}$ be a matrix with i.i.d.\ entries 
      $X_{ij}\sim \mathcal{N}(0,\sigma^2)$. The random matrix 
      $\mathbf{X}^T \mathbf{X}\in\mathbb{R}^{p\times p}$ is called a \emph{Wishart matrix} 
      (up to a scaling factor). In particular, such a matrix follows the Wishart distribution 
      $\mathcal{W}_p(n,\sigma^2)$ in certain parametrizations.
\end{definition}

\begin{definition}[Marchenko--Pastur distribution. \cite{marchenko1967distribution}]
\label{def:mp}

      In the high-dimensional limit ($n,p \to \infty$ at a fixed ratio $p/n \to c$), 
      the empirical eigenvalue distribution of the (properly normalized) Wishart matrix
      $\mathbf{X}^T \mathbf{X}$ converges to the \emph{Marchenko--Pastur distribution}. 
      Concretely, if $\mathbf{X}\in\mathbb{R}^{n\times p}$ has entries 
      $\mathcal{N}(0,1)$, then the eigenvalues of $\mathbf{X}^T \mathbf{X}$ lie within 
      $[(1-\sqrt{c})^2,\,(1+\sqrt{c})^2]$ for large $n,p$, and their density 
      converges to
      \[
        f_{\mathrm{MP}}(x) \;=\; \frac{1}{2\pi c\,x}\,\sqrt{(x - a_{\min})(a_{\max} - x)},
        \quad
        x \in [a_{\min}, a_{\max}],
      \]
      with $a_{\min} = (1-\sqrt{c})^2$ and $a_{\max} = (1+\sqrt{c})^2$. 
      If the entries of $\mathbf{X}$ have variance $\sigma^2 \neq 1$, then 
      the support is rescaled by $\sigma^2$.
\end{definition}

\begin{lemma}[Spectrum of the Jacobian's singular values]
Let $\mathbf{H}^{(k)} \;=\; \tilde{\mathbf{A}}\;\mathbf{H}^{(k-1)}\;\mathbf{W}$  be a linear GCN layer, where $\tilde{\mathbf{A}}$ has eigenvalues $\{\lambda_1,\ldots,\lambda_n\}$ and $\mathbf{W}\,\mathbf{W}^T$ has eigenvalues $\{\mu_1,\ldots,\mu_{d_k}\}$. Consider the layer-wise Jacobian $\mathbf{J} = \partial\,\mathrm{vec}\bigl(\mathbf{H}^{(k)}\bigr) / \partial\,\mathrm{vec}\bigl(\mathbf{H}^{(k-1)}\bigr)$, Then the squared singular values of $\mathbf{J}$ are given by the set 
\[
  \bigl\{\,\lambda_i^2 \,\mu_j \;\bigm|\;
    i=1,\ldots,n,\;\; j=1,\ldots,d_k\bigr\}.
\]

\begin{proof}
By the property of vectorization (Definition~\ref{def:vectorization}), we have
\[
  \mathrm{vec}\bigl(\tilde{\mathbf{A}}\,\mathbf{H}^{(k-1)}\,\mathbf{W}\bigr)
  \;=\;
  (\mathbf{W}^T \otimes \tilde{\mathbf{A}})
  \,\mathrm{vec}\bigl(\mathbf{H}^{(k-1)}\bigr).
\]
Hence 
\[
  \mathbf{J}
  \;=\;
  \mathbf{W}^T\otimes\tilde{\mathbf{A}}.
\]
By properties of the Kronecker product, the eigenvalues of $\mathbf{J}\,\mathbf{J}^T$ are the products of the eigenvalues of 
$\mathbf{W}^T \mathbf{W}$ and $\tilde{\mathbf{A}}^2$. Equivalently,
\[
  \mathrm{spec}\bigl(\mathbf{J}\,\mathbf{J}^T\bigr)
  \;=\;
  \mathrm{spec}\bigl(\mathbf{W}^T\mathbf{W}\bigr)
    \;\;\otimes\;\;
  \mathrm{spec}\bigl(\tilde{\mathbf{A}}^2\bigr),
\]
where $\mathrm{spec}$ is the vectorized version of the set of eigenvalues of a matrix. If $\mathbf{W}^T \mathbf{W}$ has eigenvalues $\{\mu_j\}_{j=1}^{d_k}$ and
$\tilde{\mathbf{A}}^2$ has eigenvalues $\{\lambda_i^2\}_{i=1}^n$, 
then the squared singular values of $\mathbf{J}$ are precisely 
$\lambda_i^2 \,\mu_j$ for $i\in\{1,\ldots,n\}$, $j\in\{1,\ldots,d_k\}$. 
\end{proof}
\end{lemma}


\begin{theorem}[Jacobian singular-value distribution]
Assume the setting of Lemma~\ref{lem:JacobianSpectrum}, and let 
$\mathbf{W}\in\mathbb{R}^{d_{k-1}\times d_k}$ be initialized with i.i.d.\ 
$\mathcal{N}(0,\sigma^2)$ entries. Denote the squared singular values of the 
Jacobian by $\gamma_{i,j}$. Then, for sufficiently large $d_k$ the empirical eigenvalue distribution $\mathbf{W}\mathbf{W}^T$ converges to the 
Marchenko-Pastur distribution. Then, 
the mean and variance of each $\gamma_{i,j}$ are
\begin{align}
  \mathbb{E}\bigl[\gamma_{i,j}\bigr]
  &= 
  \lambda_i^2 \,\sigma^2,
   \\[6pt]
  \mathrm{Var}\bigl[\gamma_{i,j}\bigr]
  &=
  \lambda_i^4 \,\sigma^4\,\frac{d_k}{d_{k-1}}.
  \label{eq:var_app}
\end{align}

\begin{proof}
In this setting, $\mathbf{W}\mathbf{W}^T$ is  Wishart if $\mathbf{W}$ has i.i.d.\ Gaussian entries. Its eigenvalues 
$\mu_j$ thus converge to the Marchenko--Pastur distribution for large $d_k$. From standard results on the moments of Wishart 
eigenvalues, 
\[
  \mathbb{E}(\mu_j)
  \;=\;
  \sigma^2, 
  \quad
  \mathrm{Var}(\mu_j)
  \;=\;
  \sigma^4 \,\frac{d_k}{d_{k-1}}.
\]
Since $\gamma_{i,j} = \lambda_i^2 \,\mu_j$, we obtain 
\[
  \mathbb{E}[\gamma_{i,j}]
  \;=\; 
  \lambda_i^2\,\mathbb{E}[\mu_j]
  \;=\;
  \lambda_i^2\,\sigma^2,
\]
\[
  \mathrm{Var}[\gamma_{i,j}]
  \;=\;
  \lambda_i^4\,\mathrm{Var}(\mu_j)
  \;=\;
  \lambda_i^4 \,\sigma^4 \,\frac{d_k}{d_{k-1}}.
\]
This completes the proof.
\end{proof}
\end{theorem}


\begin{proposition}[Effect of state-space matrices]
Consider the setting in \eqref{eq:ssm} and $\Gamma = \partial\ \mathrm{vec}(\mathbf{F}_{\boldsymbol{\theta}}(\mathbf{H}^{(k)}))/\partial\ \mathrm{vec}(\mathbf{H}^{(k)})$. Let $\otimes$ denote the Kronecker product.  Then, the norm of the vectorized Jacobian $\mathbf{J}$ is bounded as:
\begin{align}
\|\mathbf{J}\|_2 &\leq \|I_{d_k} \otimes \mathbf{\Lambda}\|_2 + \|I_{d_k} \otimes \mathbf{B}\|_2 \|\Gamma\|_2 \nonumber \\
&= \|\mathbf{\Lambda}\|_2 + \|\mathbf{B}\|_2 \|\Gamma\|_2,
\end{align}
\end{proposition}
\begin{proof}
We start by writing
\[
  \mathbf{J} 
  \;=\; (I_{d_k} \otimes \mathbf{\Lambda}) \;+\; (I_{d_k} \otimes \mathbf{B})\,\Gamma.
\]
Using the triangle inequality for the spectral norm,
\[
  \|\mathbf{J}\|_2
  \;=\;
  \bigl\|
      (I_{d_k} \otimes \mathbf{\Lambda})
      \;+\;
      (I_{d_k} \otimes \mathbf{B})\,\Gamma
  \bigr\|_2
  \;\le\;
  \|I_{d_k} \otimes \mathbf{\Lambda}\|_2
  \;+\;
  \|(I_{d_k} \otimes \mathbf{B})\,\Gamma\|_2.
\]
By the submultiplicative property of the spectral norm,
\[
  \|(I_{d_k} \otimes \mathbf{B})\,\Gamma\|_2
  \;\le\;
  \|I_{d_k} \otimes \mathbf{B}\|_2 \,\|\Gamma\|_2.
\]
Since \(\|I_{d_k} \otimes \mathbf{M}\|_2 = \|\mathbf{M}\|_2\) for any matrix \(\mathbf{M}\), we obtain
\[
  \|I_{d_k} \otimes \mathbf{\Lambda}\|_2
  \;=\;
  \|\mathbf{\Lambda}\|_2
  \quad \text{and} \quad
  \|I_{d_k} \otimes \mathbf{B}\|_2
  \;=\;
  \|\mathbf{B}\|_2.
\]
Hence,
\[
  \|\mathbf{J}\|_2
  \;\le\;
  \|\mathbf{\Lambda}\|_2
  \;+\;
  \|\mathbf{B}\|_2\,\|\Gamma\|_2.
\]
\end{proof}


\subsection{Proofs to Smoothing Theorems}
\label{app:smoothing-results}
\begin{definition}[Lipschitz continuity]
    A function $f: \mathbb{R}^n \to \mathbb{R}^m$ is Lipschitz continuous if there exists an $L \geq 0$ such that for all $\mathbf{x}, \mathbf{y} \in \mathbb{R}^n$, we have that: 

    \begin{equation*}
        \left \lVert f(\mathbf{x}) - f(\mathbf{y}) \right \rVert \leq L \left \lVert \mathbf{x} - \mathbf{y} \right \rVert, 
    \end{equation*}

where we equip $\mathbb{R}^n$ and $\mathbb{R}^m$ with their respective norms. The minimal such $L$ is called the Lipschitz constant of $f$.
\end{definition}

The notion of Lipschitz continuity is effectively a bound on the rate of change of a function. It is therefore not surprising that one can relate the Lipschitz constant to the Jacobian of $f$. In particular, we state a useful and well-known result \cite{hassan2002nonlinear} that relates the (continuous) Jacobian map $\mathbf{J}_f$ of a continuous function $f: \mathbb{R}^n \to \mathbb{R}^m$ to its Lipschitz constant $L \geq 0$. In particular, the Lipschitz constant is is the supremum of the (induced) norm of the Jacobian taken over its domain.
\begin{lemma}[\citet{hassan2002nonlinear}]
    Let $f: \mathbb{R}^n \to \mathbb{R}^m$ be continuous, with continuous Jacobian $\mathbf{J}_f$. Consider a convex set $U \subseteq \mathcal{R}^n$ If there exists $L \geq 0$ such that $\left \lVert \mathbf{J}_f(\mathbf{x}) \right \rVert \leq L$ for all $\mathbf{x} \in U$, then $\left\lVert f(\mathbf{x}) - f(\mathbf{y}) \right \rVert \leq L \left\lVert \mathbf{x} - \mathbf{y} \right \rVert$. In particular, we have that the Lipschitz constant of $f$ $L$ is: \begin{equation*}
        L = \sup_{\mathbf{x} \in U} \left \lVert \mathbf{J}_f(\mathbf{x}) \right \rVert.
    \end{equation*}
\end{lemma}

The condition of $U$ being convex is a technicality that is easily achieved in practice with the assumption that input features are bounded and that therefore they live in a convex hull $U$. In particular, at each layer $k$ one can also find a convex hull $U_k$ such that the image of the layer $k-1$ is contained within $U_k$. We highlight that for non-linearities such as ReLU, there are technical difficulties when taking this supremum as there is a non-differentiable point at $0$. This can be circumvented by considering instead a supremum of the (Clarke) generalized Jacobian \citep{jordan2020exactly}. We ignore this small detail in this work for simplicity as for ReLU this is equivalent to considering the supremum over $U/\mathbf{0}$, i.e. simply ignoring the problematic point $\mathbf{0}$. 


\begin{lemma}
    Consider a GNN layer $f_\ell$ as in Equation \ref{eq:gcn}, with non-linearity $\sigma$ such that $\sigma(0) = 0$ (e.g. ReLU or $\tanh$). Then, $f(\mathbf{0}) = \mathbf{0}$, i.e. $\mathbf{0}$ is a fixed point of $f$.
\end{lemma}
\begin{proof}
     $f_\ell(\mathbf{0}) = \sigma\left(\hat{\mathbf{A}}\mathbf{0}\mathbf{W}\right) = \sigma\left(\mathbf{0}\right) = \mathbf{0}$.
\end{proof}


\begin{proposition}[Convergence to unique fixed point.]
    Let $\lip{f_\ell} \leq 1 - \epsilon$ for some $\epsilon > 0$ for all $\ell=1\dots L$. Then, for $\mathbf{H} \in U \subseteq \mathbb{R}^{nd}$, we have that:

    \begin{equation}
        \left \lVert f(\mathbf{H}) \right \rVert \leq (1 - \epsilon)^L \norm{\mathbf{H}} < \norm{\mathbf{H}}.
    \end{equation}

    In particular, as $L \to \infty$, $f(\mathbf{H}) \to \mathbf{0}$.
\end{proposition}
 \begin{proof}
     By Lipschitz regularity of $f$ over $U$, we have that $\norm{f(\mathbf{x}) - f(\mathbf{y})} \leq \lip{f} \norm{\mathbf{x} - \mathbf{y}}$. Recall that by Lemma \ref{lemma:fixed-point-gcn}, we have that $f(\mathbf{0}) = \mathbf{0}$. This implies:

     \begin{align*}
         \norm{f(\mathbf{H}) - f(\mathbf{0})} &= \norm{f(\mathbf{H})} \\ 
         &\leq \lip{f} \norm{\mathbf{H}} \\
         &\leq \prod_{\ell=1}^L \lip{f_\ell} \norm{\mathbf{H}} \\
         &< \norm{\mathbf{H}},
     \end{align*}

     where in the last step we use the fact that Lipschitz constants are submultiplicative and that for all $\ell$ we have that $\lip{f_\ell} < 1$ by assumption. The final statement is immediate by the Banach fixed point theorem and by noting that $f_\ell$ all share the same fixed point $\mathbf{0}$ by Lemma \ref{lemma:fixed-point-gcn}. \end{proof}


\begin{proposition}[Contractions decrease Dirichlet energy.]
    Let $f$ be a GNN, $|E|$ be the number of edges in $G$, and $\mathbf{H} \in \mathbb{R}^{nd}$. We have the following bound:
    \begin{equation}
        \mathcal{E}(f(\mathbf{H})) \leq 2 \lvert E \rvert \prod_{\ell=1}^L \lip{f_\ell}^2 \norm{\mathbf{H}}^2.
    \end{equation}

    In particular, if $\lip{f_\ell} \leq 1 - \epsilon$ for some $\epsilon > 0$ for all $\ell=1\dots L$, then as $L \to \infty$,  $\mathcal{E}(f(\mathbf{H})) \to 0$.
\end{proposition}
\begin{proof}
We denote by $f(\mathbf{H})|_i \in \mathbb{R}^d$, the $d$-dimensional evaluation of f(H) at node $i$. We make use of the inequality $\norm{f(\mathbf{H})|_i} \leq \norm{\mathbf{H}}$.

     \begin{align*}
         \mathcal{E}(f(\mathbf{H})) &= \sum_{i \sim j}\norm{f(\mathbf{H})|_i - f(\mathbf{H})|_j}^2 \\
         &\leq \sum_{i \sim j}\norm{f(\mathbf{H})|_i}^2+ \norm{f(\mathbf{H})|_j}^2 \\
         &\leq 2\sum_{i \sim j} \norm{f(\mathbf{H})}^2 \\
         &\leq 2\lip{f}^2\sum_{i \sim j} \norm{\mathbf{H}}^2 \\
         &= 2\lip{f}^2\lvert E \rvert \norm{\mathbf{H}}^2 \\
         &\leq 2\prod_{\ell=1}^L \lip{f_\ell}^2 \lvert E \rvert \norm{\mathbf{H}}^2. 
     \end{align*}

     It is then clear that, if $\lip{f_\ell} \leq 1 - \epsilon$ for some $\epsilon > 0$ for all $\ell=1\dots L$, $\prod_{\ell=1}^L \lip{f_\ell}^2 \leq (1-\epsilon)^{2L} \to 0$ as $L \to \infty$.
 \end{proof}

\section{kGNN-SSM: A simple method to combine high connectivity and non-dissipativity.}\label{app:kgnn_ssm}

To test our assumption on more complex downstream tasks, we construct a minimal model that combines high connectivity with non-dissipativity. To guarantee high connectivity, we employ a k-hop aggregation scheme. In particular, each node $i$ at layer $k$ will aggregate information as
\begin{align}
\label{Eq:dynamic_mpnn}
a_{i,k}^{(k)} &= \psi^{k}\Big(\{h_j^{(k)} : j \in \mathcal{N}_{k}(i)\}\Big),
\end{align}
where
\begin{equation}\label{eq:k-hop}
    \mathcal{N}_{k}(i) := \{ j \in V : d_{G}(i,j) = k\}\notag
\end{equation}
and $d_G : V\times V \rightarrow \mathbb{R}_{\geq 0}$ is the length of the minimal walk connecting nodes $i$ and $j$. This approach avoids a large amount of information being squashed into a single vector, and is more in line with the recurrent paradigm. We note that this scheme is similar to \cite{ding2024recurrent}, but in this case we do not consider different block or parameter sharing, and our recurrent mechanism is based on an untrained SSM layer.

We denote a GNN endowed with this rewiring scheme and wrapped with our SSM layer as \texttt{kGNN-SSM}.


\section{Experimental Details}\label{app:experimental_details}
In this section, we provide additional experimental details, including dataset and experimental setting description and employed hyperparameters. 

\paragraph{Over-smoothing task.} In this task, we aim to analyze the dynamics of the Dirichlet energy across three different graph topologies: Cora \cite{cora}, Texas \cite{Pei2020GeomGCNGG}, and a grid graph. The Cora dataset is a citation network consisting of 2,708 nodes (papers) and 10,556 edges (citations). The Texas dataset represents a webpage graph with 183 nodes (web pages) and 499 edges (hyperlinks). Lastly, the grid graph is a two-dimensional $10\times10$ regular grid with 4-neighbor connectivity. For all three graphs, node features are randomly initialized from a normal distribution with a mean of 0 and variance of 1. These node features are then propagated over 80 layers (or iterations%, in the case of ADGN, SWAN, adnd PHDGN
) using untrained GNNs to observe the energy dynamics.

\paragraph{Graph Property Prediction.} This experiment consists of predicting two node-level (\ie eccentricity and single source shortest path) and one graph-level (\ie graph diameter) properties on synthetic graphs sampled from different distribution, \ie Erd\H{o}s‚ÄìR\'{e}nyi, Barabasi-Albert, grid, caveman, tree, ladder, line, star, caterpillar, and lobster. Each graph contains between 25 and 35 nodes, with nodes assigned with input features sampled from a uniform distribution in the interval $[0,1)$. The target values correspond to the predicted graph property. The dataset consists of 5,120 graphs for training, 640 for validation and 1,280 for testing. 

We employ the same experimental setting and data outlined in \cite{gravina2022anti}. Each model is designed as three components: the encoder, the graph convolution, and the readout. We perform hyperparameter tuning via grid search, optimizing the Mean Square Error (MSE). The models are trained using the Adam optimizer for a maximum of 1500 epochs, with early stopping based on the validation error, applying a 100 epochs patience. For each model configuration, we perform 4 training runs with different weight initializations and report the average results. We report in Table~\ref{tab:hyperparams} the employed grid of hyperparameters. 

\paragraph{Long-Range Graph Benchmark.} We consider the \texttt{peptides-func} and \texttt{peptides-struct} datasets from \cite{dwivedi2022LRGB}. Both datasets consist of 15,535 graphs, where each graph corresponds to 1D amino acid chain (\ie peptide), where nodes are the heavy atoms of the peptide and edges are the bonds between them. \texttt{peptides-func} is a multi-label graph classification dataset whose objective is to predict the peptide function, such as antibacterial and antiviral function. \texttt{peptides-struct} is a multi-label graph regression dataset focused on predicting the 3D structural properties of peptides, such as the inertia of the molecule and maximum atom-pair distance. 

We use the same experimental setting and splits from \cite{dwivedi2022LRGB}. We perform hyperparameter tuning via grid search, optimizing the Average Precision (AP) in the Peptide-func and Mean Absolute Error (MAE) in the Peptide-struct. The models are trained using the AdamW optimizer for a maximum of 300 epochs. For each model configuration, we perform four training runs with different weight initializations and report the average results. We report in Table~\ref{tab:hyperparams} the employed grid of hyperparameters. 


\paragraph{Tested Hyperparameters.} In Table~\ref{tab:hyperparams} we report the grid of hyperparameters employed in our experiments by our method. 

\begin{table}
\centering
\caption{The grid of hyperparameters employed during model selection for the graph property prediction tasks (\emph{GraphProp}), and \texttt{peptides-func} and \texttt{peptides-struct}.}
\vspace{1mm}
\label{tab:hyperparams}
\begin{tabular}{lll}
\toprule
\multirow{2}{*}{\textbf{Hyperparameters}}  & \multicolumn{2}{c}{\textbf{Values}}\\\cmidrule{2-3}
                & {\bf \emph{GraphProp}}   & \texttt{peptides-} (\texttt{func}, \texttt{struct})\\\midrule
Optimizer       & $\;$ Adam                     & \hspace{1cm}AdamW\\
Learning rate   & $\;$ 0.003                    & \hspace{1cm}0.001\\
Weight decay    & $\;$ $10^{-6}$                & \hspace{1cm}- \\
N. Layers   & $\;$ 10                & \hspace{1cm}17, 40\\
embedding dim   & $\;$ 20, 30               & \hspace{1cm}105 \\
$\sigma$        & $\;$ tanh                     & \hspace{1cm}ReLU\\
$\text{eig}(\Lambda)$ & $\;$  0.5, 0.75, 1.0     & \hspace{1cm}
1.0 \\
%\revise{$\gamma$ (conv)} & 0.5, 0.75, 1.0, 1.25     & \revise{ALVARO}\\
%\revise{$\gamma$  (inp)} & 0.5, 0.75, 1.0, 1.25     & \revise{ALVARO}\\
%weight sharing  & True              & \revise{ALVARO}\\
\bottomrule
\end{tabular}
\end{table}



\section{Additional empirical results}\label{app:additional}
In this section, we propose additional empirical results on over-smoothing and over-squashing, as well as the eigendistribution of the layerwise Jacobians of various standard GNNs.

\subsection{Additional MPNN Jacobians}\label{app:additional_mpnn_jacobian}

Here, we present in Figure~\ref{fig:test_full_synthetic} the eigendistribution of the layerwise Jacobians of GCN, GIN \cite{xu2018powerful} and Gated-GCN \cite{bresson2017residual}. Across the board, we observe similar contraction effects in the Jacobian as those presented in the main paper, with a long number of eigenvalues accumulating at zero, with no significant changes in the distribution during training. However, the maximum eigenvalues for both GIN and Gated-GCN are much larger than those of GCN. We also compare a nonlinear feedforward network and a nonlinear GCN in Figure \ref{fig:mlp-v-gcn}.

\begin{figure}[H]
	\centering
	\includegraphics[width=1\linewidth]{Figures/hist_eig.pdf}
	\caption{Eigenvalues of layer-to-layer Jacobian of different GNN models.}
\label{fig:test_full_synthetic}
\end{figure}

\begin{figure}[H]
	\centering
    
    \includegraphics[ width=0.35\linewidth]{Figures/eig_change_Grid_random_MLP.pdf}
    \caption{Histogram of eigenvalue modulus of the layerwise Jacobian for a nonlinear convolutional and a nonlinear feedforward layer.}  
    \label{fig:mlp-v-gcn}
\end{figure}



\subsection{Over-smoothing}\label{app:additional_over-smoothing}
Here, we include additional results related to over-smoothing experiments. Figure \ref{fig:oversmoothing_lin} shows the effect of $||\Lambda||_2$ in GCN-SSM on different graph structures, showing that lower Jacobian norms leads to a rapid decay of the Dirichlet energy, whereas values closer to one result in a more stable energy evolution. This result is also confirmed by Figure~\ref{fig:oversmoot_cora_adgn_swan_phdgn} and Figure~\ref{fig:oversmoot_multimodel}. The former presents the vectorized Jacobian for ADGN \cite{gravina2022anti}, SWAN \cite{gravina_swan}, and PHDGN \cite{heilig2024injecting} on Cora, while the latter the Dirichlet energy evolution of different models on different topologies. Notably, in Figure~\ref{fig:oversmoot_multimodel}, ADGN, SWAN, and PHDGN exhibit stable Dirichlet energy across layers, and Figure~\ref{fig:oversmoot_cora_adgn_swan_phdgn} reveals that these Jacobian norms are close to one.
These results confirm that stable dynamics also ensure a non-decaying Dirichlet energy, effectively preventing over-smoothing.


\begin{figure}[H]
	\centering
		\includegraphics[height=4cm,width=0.32\linewidth]{Figures/oversmoothing_nonlin_Cora.pdf}
	   \includegraphics[height=4cm,width=0.32\linewidth]{Figures/oversmoothing_nonlin_Grid.pdf}
		\includegraphics[height=4cm,width=0.32\linewidth]{Figures/oversmoothing_nonlin_Texas.pdf}
	\caption{Dirichlet Energy evolution of GCN-SSM for different $||\Lambda||_2$ on different graph topologies. \textbf{Left:} Cora. \textbf{Middle:} Grid graph. \textbf{Right:} Texas. }
    \label{fig:oversmoothing_lin}
\end{figure}


\begin{figure}[H]
	\centering
		\includegraphics[height=4cm,width=0.32\linewidth]{Figures/oversmoothing_nonlin_Cora_GAT.pdf}
	   \includegraphics[height=4cm,width=0.32\linewidth]{Figures/oversmoothing_nonlin_Grid_GAT.pdf}
		\includegraphics[height=4cm,width=0.32\linewidth]{Figures/oversmoothing_nonlin_Texas_GAT.pdf}
	\caption{Dirichlet Energy evolution of GAT-SSM for different $||\Lambda||_2$ on different graph topologies. \textbf{Left:} Cora. \textbf{Middle:} Grid graph. \textbf{Right:} Texas. }
    \label{fig:oversmoothing_gat}
\end{figure}


\begin{figure}[H]
    \centering
    \includegraphics[width=0.33\linewidth]{Figures/eigs_ADGNCora.pdf}
    \includegraphics[width=0.33\linewidth]{Figures/eigs_SWAN_attentionTrueCora.pdf}
    \includegraphics[width=0.33\linewidth]{Figures/eigs_PHDGNCora.pdf}
    \caption{Vectorized Jacobian for ADGN \cite{gravina2022anti}, SWAN \cite{gravina_swan}, and PHDGN \cite{heilig2024injecting} on Cora. \textbf{Left:} ADGN. \textbf{Middle:} SWAN. \textbf{Right:} PHDGN.}
    \label{fig:oversmoot_cora_adgn_swan_phdgn}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.33\linewidth]{Figures/oversmoothing_vanishing_Cora_new.pdf}
    \includegraphics[width=0.33\linewidth]{Figures/oversmoothing_vanishing_Grid_new.pdf}
    \includegraphics[width=0.33\linewidth]{Figures/oversmoothing_vanishing_Texas_new.pdf}
    \caption{Dirichlet Energy evolution of different models on different topologies. \textbf{Left:} Cora. \textbf{Middle:} Grid graph. \textbf{Right:} Texas.}
    \label{fig:oversmoot_multimodel}
\end{figure}

\subsection{Link between delay and vanishing gradients}
\label{app:drew}

Here, we show how the delay term in \cite{gutteridge2023drew} is directly related to preventing vanishing gradients. We do so by showing that adding the delay term to a GCN is effective at preventing over-smoothing, see Figure \ref{fig:oversmoot_gcn}, as well as by checking the histogram of eigenvalues of the Jacobian, see Figure  \ref{fig:drew-hist}.

\begin{figure}[H]
	\centering
		\includegraphics[height=3.5cm,width=0.32\linewidth]{Figures/oversmoothing_drew_Cora.pdf}
	   \includegraphics[height=3.5cm,width=0.32\linewidth]{Figures/oversmoothing_drew_Grid.pdf}
		\includegraphics[height=3.5cm,width=0.32\linewidth]{Figures/oversmoothing_drew_Texas.pdf}
	\caption{Dirichlet Energy evolution of GCN (+delay mechanism) on different topologies. \textbf{Left:} Cora. \textbf{Middle:} Grid graph. \textbf{Right:} Texas.}
    \label{fig:oversmoot_gcn}
\end{figure}


\begin{figure}[H]
	\centering
        \includegraphics[ width=0.225\linewidth]{Figures/ringtransfer_nonlinear.pdf}\includegraphics[width=0.2\linewidth]{Figures/controlled_ring.pdf}        \caption{\textbf{Left:} Performance on the RingTransfer task for DRew \cite{gutteridge2023drew}. \textbf{Right: }Effect of dissipativity on performance.}
        \label{fig:ringtransfer_drew}
        \vspace{-0.5cm}
\end{figure}



\begin{figure}[H]
	\centering
    \includegraphics[width=0.4\linewidth]{Figures/eigs_DRew_GCN.pdf}
	\caption{Eigenvalue distribution of DRew-GCN+delay on the ring transfer task.}
    \label{fig:drew-hist}
\end{figure}



\subsection{Graph Property Prediction}\label{app:additional_gpp}


\textbf{Edge-of-chaos behavior and long-range propagation.}  
To further support our claim that mitigating gradient vanishing is key to strong long-range performance, Figure~\ref{fig:gpp} shows each method‚Äôs average Jacobian eigenvalue distance to the edge-of-chaos (EoC) region. The figure demonstrates that methods such as ADGN \cite{gravina2022anti} and SWAN \cite{gravina_swan}, which remain closer to EoC, effectively propagate information over large graph radii, resulting in superior performance across all three tasks. Figure~\ref{fig:gpp_ablation} presents an ablation study on multiple ADGN variants, controlled by the hyperparameter \(\gamma\), which governs the positioning of the Jacobian eigenvalues (\(\gamma < 0\) places them outside the stability region, \(\gamma > 0\) inside, and \(\gamma = 0\) on the unit circle). Notably, regardless of the initial value of \(\gamma\), ADGN consistently converges towards the EoC region as performance improves.

\begin{figure}[H]
	\centering
	\includegraphics[width
    =0.84\linewidth]{Figures/graph_prop_pred_swan.pdf}
	\caption{Performance on Graph Property Prediction tasks and average Jacobian eigenvalue distance to the edge of chaos (EoC) region for different GNN models.}
	\label{fig:gpp}
\end{figure}

\begin{figure}[H]
	\centering
	\includegraphics[width=0.92\linewidth]{Figures/graph_prop_pred_ablation_ADGN.pdf}
	\caption{Performance on Graph Property Prediction tasks and average Jacobian eigenvalue distance to the edge of chaos (EoC) region for different ADGN dynamics, \ie $\gamma\in[-0.1, 1]$. Negative values of $\gamma$ places the eigenvalues of the ADGN Jacobian outside the stability region, otherwise for positive values.}
	\label{fig:gpp_ablation}
\end{figure}

\textbf{Complete results.} Table~\ref{tab:complete_results_gpp} compares our method on graph property prediction tasks against a range of state-of-the-art approaches, including GCN~\cite{kipf2017semisupervised}, GAT~\cite{Velickovic2018GraphAN}, GraphSAGE~\cite{hamilton2017inductive}, GIN~\cite{xu2018powerful}, GCNII~\cite{gcnii}, DGC~\cite{DGC}, GRAND~\cite{chamberlain2021grand}, GraphCON~\cite{rusch2022graph}, ADGN~\cite{gravina2022anti}, SWAN~\cite{gravina_swan}, PH-DGN~\cite{heilig2024injecting}, and DRew~\cite{gutteridge2023drew}. Our method achieves exceptional results across all three tasks, consistently surpassing MPNN baselines, differential equation-inspired GNNs, and multi-hop GNNs. These findings underscore how combining powerful model dynamics with improved connectivity provides substantial benefits in tasks that require long-range information propagation.


\begin{table}[H]
%\setlength{\tabcolsep}{1pt}
\centering
\caption{Mean test set {\small$log_{10}(\mathrm{MSE})$}($\downarrow$) and std averaged on 4 random weight initializations on Graph Property Prediction tasks. The lower, the better. Baseline results are reported from \cite{gravina2022anti, gravina_swan, heilig2024injecting}.
}
\label{tab:complete_results_gpp}
\vspace{1mm}
\footnotesize
\begin{tabular}{lccc}
\toprule
Model &\texttt{Diameter} & \texttt{SSSP} & \texttt{Eccentricity} \\\midrule
\textbf{MPNNs} \\
$\,$ GCN            & 0.7424$_{\pm0.0466}$ & 0.9499$_{\pm0.0001}$ & 0.8468$_{\pm0.0028}$ \\
$\,$ GAT            & 0.8221$_{\pm0.0752}$ & 0.6951$_{\pm0.1499}$           & 0.7909$_{\pm0.0222}$  \\
$\,$ GraphSAGE      & 0.8645$_{\pm0.0401}$ & 0.2863$_{\pm0.1843}$           &  0.7863$_{\pm0.0207}$\\
$\,$ GIN            & 0.6131$_{\pm0.0990}$ & -0.5408$_{\pm0.4193}$          & 0.9504$_{\pm0.0007}$\\
$\,$  GCNII          & 0.5287$_{\pm0.0570}$ & -1.1329$_{\pm0.0135}$          & 0.7640$_{\pm0.0355}$\\
\midrule
\multicolumn{4}{l}{\textbf{Differential Equation inspired GNNs}} \\
$\,$ DGC            & 0.6028$_{\pm0.0050}$ & -0.1483$_{\pm0.0231}$          & 0.8261$_{\pm0.0032}$\\
$\,$ GRAND          & 0.6715$_{\pm0.0490}$ & -0.0942$_{\pm0.3897}$          & 0.6602$_{\pm0.1393}$ \\
$\,$ GraphCON       & 0.0964$_{\pm0.0620}$ & -1.3836$_{\pm0.0092}$ & 0.6833$_{\pm0.0074}$\\
$\,$ ADGN & -0.5188$_{\pm0.1812}$ & -3.2417$_{\pm0.0751}$ & 0.4296$_{\pm0.1003}$  \\
$\,$ SWAN & -0.5981$_{\pm0.1145}$  & -3.5425$_{\pm0.0830}$  & -0.0739$_{\pm0.2190}$ \\
$\,$ PH-DGN   & -0.5473$_{\pm0.1074}$ & \textbf{-4.2993$_{\pm0.0721 }$} & -0.9348$_{\pm0.2097}$\\
\midrule
 \multicolumn{4}{l}{\textbf{Graph Transformers}} \\
 $\,$ GPS & -0.5121$_{\pm0.0426}$ &  -3.5990$_{\pm0.1949}$  & 0.6077$_{\pm0.0282}$\\
\midrule
\textbf{Multi-hop GNNs} \\
$\,$ DRew-GCN %(khop=10)  
& -2.3692$_{\pm0.1054}$ & -1.5905$_{\pm0.0034}$ & -2.1004$_{\pm0.0256}$\\
$\quad$ %DRew-GCN (khop=10)  
+ delay &  -2.4018$_{\pm0.1097}$ & -1.6023$_{\pm0.0078}$ & -2.0291$_{\pm0.0240}$\\
\midrule
\textbf{Our} \\
$\,$ GCN-SSM   & -2.4312$_{\pm0.0329}$ &    -2.8206$_{\pm0.5654}$  & -2.2446$_{\pm0.0027}$\\
$\quad$ + $\text{eig}(\Lambda)\approx 1$ & \underline{-2.4442}$_{\pm0.0984}$ & -3.5928$_{\pm0.1026}$ & \underline{-2.2583}$_{\pm0.0085}$ \\
$\quad$ + k-hop         & \textbf{-3.0748}$_{\pm0.0545}$ & \underline{-3.6044}$_{\pm0.0291}$ & \textbf{-4.2652}$_{\pm0.1776}$ \\
\bottomrule      
\end{tabular}
\end{table}


\subsection{Additional comments on LRGB tasks}\label{app:additional_LRGB}

In our experiments with the LRGB tasks, we observe that the \texttt{peptides-func} task exhibits significantly longer-range dependencies than the \texttt{peptides-struct} task. Notably, the \texttt{peptides-struct} task performs best when the model is not initialized at the edge of chaos and requires fewer layers. Conversely, on \texttt{peptides-struct} the model performs best when it is set to be at the edge of chaos, and shows a monotonic performance increase with additional layers, with optimal results achieved when using forty layers.

Furthermore, we highlight that while our experiments with a small hidden dimension adhere to the parameter budget established in \cite{dwivedi2022LRGB}, increasing the hidden dimension ($d \uparrow$) to 256 causes us to exceed the 500k parameter budget limit, even though our model maintains the same number of parameters as a regular GCN. While this budget is a useful tool to benchmark different models, we highlight that this restriction results in models running with fewer layers and small hidden dimensions. However, a large number of layers is crucial for effective long-range learning in graphs that are not highly connected, while increasing the hidden dimension also directly affects the bound in Theorem \ref{theo:sensitivity_digiovanni}. As such, we believe that this parameter budget indirectly benefits models with higher connectivity graphs, inadvertently hindering models that do not perform edge addition.

\section{Supplementary Related Work}\label{app:supplementary_related_work}
\paragraph{Long-range propagation on GNNs.} Learning long-range dependencies on graphs involves effectively propagating and preserving information across distant nodes% while mitigating issues such as over-smoothing and over-squashing
. Despite recent advancements, ensuring effective long-range communication between nodes remains an open problem \cite{shi2023expositionoversquashingproblemgnns}. Several techniques have been proposed to address this issue, including graph rewiring methods, such as \cite{diffusion_improves, topping2021understanding, karhadkar2022fosr, barbero2023locality, gutteridge2023drew, 10.5555/3618408.3618515}, which modify the graph topology to enhance connectivity and facilitate information flow. Similarly, Graph Transformers enhance the connectivity to capture both local and global interactions, as demonstrated by \cite{ying2021transformers, dwivedi2021generalization, graphtransformer, san, graphgps, wu2023difformer}. Other approaches incorporate non-local dynamics by using a fractional power of the graph shift operator \cite{maskey2024fractional}, leverage quantum diffusion kernels \cite{markovich2023qdc}, regularize the model's weight space \cite{gravina2022anti, gravina_swan}, or exploit port-hamiltonian dynamics \cite{heilig2024injecting}.

Despite the effectiveness of these methods in learning long-range dependencies on graphs, they primarily introduce solutions to mitigate the problem rather than establishing a unified theoretical framework that defines its underlying cause.

\paragraph{Vanishing gradients in sequence modelling and deep learning.} One of the primary challenges in training recurrent neural networks lies in the vanishing (and sometimes exploding) gradient problem, which can hinder the model‚Äôs ability to learn and retain information over long sequences. In response, researchers have proposed numerous architectures aimed at preserving or enhancing gradients through time. Examples include Unitary RNNs \cite{arjovsky2016unitary}, Orthogonal RNNs \cite{Henaff2016}, coRNNs \cite{rusch2021a},  Linear Recurrent Units \cite{orvieto2023resurrecting}, and Structured State Space Models \cite{gu2021, gu2023mamba}. By leveraging properties such as orthogonality, carefully designed parameterizations, or alternative update mechanisms, these models seek to alleviate gradient decay and capture longer-range temporal relationships more effectively.

\paragraph{Dynamical-systems-inspired neural networks.} Since the introduction of Neural ODEs in \citet{chen2018neural}, there have been various methods that employ ideas of dynamical systems within neural networks, including continuous-time methods \cite{rubanova2019latent, norcliffe2020second, calvo2023beyond, calvo2024partially, bergna2024uncertainty, moreno2024rough, calvo2025observation} or state-space approaches \cite{chang2023low,duran2024outlier,duran2024bone}. Within graph neural networks, we highlight PDE-GCN \cite{eliasof2021pde}, GRAND \cite{chamberlain2021grand}, BLEND \cite{chamberlain2021beltrami} and Neural Sheaf Diffusion \cite{bodnar2022neural}.

\end{document}


% This document was modified from the file originally made available by
% Pat Langley and Andrea Danyluk for ICML-2K. This version was created
% by Iain Murray in 2018, and modified by Alexandre Bouchard in
% 2019 and 2021 and by Csaba Szepesvari, Gang Niu and Sivan Sabato in 2022.
% Modified again in 2023 and 2024 by Sivan Sabato and Jonathan Scarlett.
% Previous contributors include Dan Roy, Lise Getoor and Tobias
% Scheffer, which was slightly modified from the 2010 version by
% Thorsten Joachims & Johannes Fuernkranz, slightly modified from the
% 2009 version by Kiri Wagstaff and Sam Roweis's 2008 version, which is
% slightly modified from Prasad Tadepalli's 2007 version which is a
% lightly changed version of the previous year's version by Andrew
% Moore, which was in turn edited from those of Kristian Kersting and
% Codrina Lauth. Alex Smola contributed to the algorithmic style files.
