
\documentclass{article} % For LaTeX2e
\usepackage{iclr2025_conference,times}

% Optional math commands from https://github.com/goodfeli/dlbook_notation.
\input{math_commands.tex}

\usepackage{hyperref}
\hypersetup{colorlinks,linkcolor={blue},citecolor={purple},urlcolor={red}} 
\input{def}
\newcommand{\SH}[1]{{\color{blue} {\bf (SH: #1)}}}
\newcommand{\AY}[1]{{\color{purple} {\bf (AY: #1)}}}
\def\shihao{\textcolor{black}}

\def\rebuttal{\textcolor{black}}

\usepackage{hyperref}
\usepackage{url}
\usepackage{wrapfig}


\title{Improving deep regression with tightness}

\author{Shihao Zhang$^1$, Yuguang Yan$^2$, Angela Yao$^1$ \\
$^1$National University of Singapore ~~
$^2$Guangdong University of Technology\\
\texttt{zhang.shihao@u.nus.edu ~ 
ygyan@gdut.edu.cn~ 
ayao@comp.nus.edu.sg} \\
}

\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}

\usepackage{comment}
\usepackage{color}
\def\red{\textcolor{red}}

\usepackage{soul}

\iclrfinalcopy % Uncomment for camera-ready version, but NOT for submission.
\begin{document}


\maketitle

\begin{abstract}

For deep regression, preserving the ordinality of the targets with respect to the feature representation improves performance across various tasks. However, a theoretical explanation for the benefits of ordinality is still lacking. This work reveals that preserving ordinality reduces the conditional entropy $\mH(\bZ|\bY)$ of representation $\bZ$ conditional on the target $\bY$. However, our findings reveal that typical regression losses do little to reduce $\mH(\bZ|\bY)$, even though it is vital for generalization performance.  
With this motivation, we introduce an optimal transport-based regularizer to preserve the similarity relationships of targets in the feature space to reduce $\mH(\bZ|\bY)$.
Additionally, we introduce a simple yet efficient strategy of duplicating the regressor targets, also with the aim of reducing $\mH(\bZ|\bY)$. 
Experiments on three real-world regression tasks verify the effectiveness of our strategies to improve deep regression. Code: \url{https://github.com/needylove/Regression_tightness}.

\end{abstract}

\section{Introduction}
Classification and regression are two fundamental tasks in machine learning.  Classification maps input data to categorical targets, while regression maps the data to continuous target space. 
Representation learning for classification in deep neural networks is well-studied~\citep{boudiaf2020unifying, achille2018information}, but is less explored for regression. One emerging observation in deep regression is the importance of feature ordinality~\citep{zhang2023improving}. 
Preserving the ordinality of targets within the feature space leads to better performance, and various regularizers have been proposed
\citep{gong2022ranksim, keramati2023conr} to enhance ordinality.
But what is the link between ordinality and regression performance? 

The information bottleneck principle~\citep{shwartz2017opening} suggests that a neural network learns representations $\bZ$ that retain sufficient information about the target $\bY$ while compressing irrelevant information. The two aims can be regarded as minimizing the conditional entropies $\mH(\bY|\bZ)$ and $\mH(\bZ|\bY)$, respectively~\citep{zhang2024deep}.  Compression reduces representation complexity, prevents overfitting, and bounds the generalization error~\citep{tishby2015deep, kawaguchi2023does, zhang2024deep}. We find that preserving ordinality enhances compression by minimizing $\mH(\bZ|\bY)$, \ie the conditional entropy of the learned representation $\bZ$ with respect to the target $\bY$. 
Following~\citep{boudiaf2020unifying,zhang2024deep}, we refer to this conditional entropy as \emph{tightness}, and its compression as as \emph{tightening the representation}.


But are ordinal feature spaces not learned naturally by the regressor?  
We explore this question through gradient analysis and comparing the differences between regression and classification. 
We find that typical regressors 
are weak in tightening the learned representations.
Specifically, given a fixed linear regressor with weight vector $\bm{\theta}$, the update direction of $\bz_i$ for a given sample $i$ tends to follow the direction of $\bm{\theta}$. The movement of $\bz_i$ can be regarded as a probability density shift \citep{sonoda2019transport}. Deep regressors 
update and tighten the representation in limited directions perpendicular to $\bm{\theta}$. 
In contrast, we find that deep classifiers update $\bz_i$ more flexibly and in diverse directions, 
leading to better-tightened representations. Such a finding sheds insight into why reformulating regression as a classification task may be more effective
\citep{farebrother2024stop, liu2019counting}
and why classification losses benefit 
representation learning for regression \citep{zhang2023improving}. 

So how can regression representations be further tightened? 
We take inspiration from 
classification, where one-hot encodings allow a separate set of classification weights $\bm{\theta}_k$ for each class $k$.  Similarly, we augment the target space of the regressor into multiple targets.  The multiple-target strategy adds extra dimensions to the regression output and incorporates additional regressors, making it more flexible to tighten the feature representations.  
Additionally, we introduce a Regression Optimal Transport (ROT) Regularizer, or ROT-Reg. ROT-Reg captures local similarity relationships through optimal transport plans.  By encouraging similar plans between the target and feature space, the regularizer can tighten representations locally.  It also helps to preserve the target space topology, 
which is also desirable for regression representations \citep{zhang2024deep}.

Our main contributions are three-fold:
\begin{itemize}
    \item  We are the first to analyze the need for preserving target ordinality with respect to the representation space for deep regression and link it to feature tightness.
    \item We reveal the weakness of standard regression in tightening learned feature representations, as the representation updating direction is constrained to follow a single line.
    \item We introduce a multi-target learning strategy and an optimal transport-based regularize, which tighten regression representations globally and locally, respectively.
\end{itemize}

\section{Related work}

\textbf{Regression representation learning}. 
Existing works mainly focus on the properties of continuity and ordinality. For continuity, DIR \citep{yang2021delving} tackles missing data by smoothing based on the continuity of both targets and representations.  VIR \citep{wang2024variational} computes the representations with additional information from data with similar targets. 
Preserving the representation's continuity  can also encourage the feature manifold to be homeomorphic with respect to the target space and is highly desirable~\citep{zhang2024deep}. 

For ordinality, RankSim \citep{gong2022ranksim} explicitly preserves the ordinality for better performance. Conr \citep{keramati2023conr} further introduces a contrastive regularizer to preserve the ordinality. It is worth mentioning that the continuity sometimes overlaps with the ordinality, and obtaining neighbor samples in continuity also requires ordinality. Although ordinality plays a key role in regression representation learning, it importance and 
characteristics are underexplored.  
This work tackles these questions by establishing connections between target ordinality and representation tightness.

\textbf{Recasting regression as a classification}. For a diverse set of regression tasks, formulating them into a classification problem yields better performance \citep{li2022simcc, bhat2021adabins, farebrother2024stop}. Previous works have hinted at task-specific reasons. For pose estimation, classification provides denser and more effective supervision \citep{gu2022dive}. For crowd counting, classification is more robust to noise \citep{xiong2022discrete}. Later, \citet{pintea2023step} empirically found that classification helps when the data is imbalanced, and \citet{zhang2023improving} suggests regression lags in its ability to learn a high entropy feature space. A high entropy feature space implies the representations preserve necessary information about the target. In this work, we provide a derivation and further suggest regression has insufficient ability to compress the representations. 

\section{On the tightness of regression representations}
\label{section:analysis}
\subsection{Notations \& Definitions}

Consider a dataset $\{\bx_i, \bz_i, y_i\}_{i=1}^N$ sampled from a distribution $\mP$, where $\bx_i$ is the input, 
$y \in \mmR$ is the corresponding label, and $\bz_i \in \mZ \subset \mmR^{d}$ is the feature corresponding to the input $\bx_i$ extracted by a neural network. 
A regressor $f_{\bm{\theta}}$ parameterized by $\bm{\theta}$ maps $\bz_i$ to a predicted target $\hat{y}_i = f_{\bm{\theta}}(\bz_i)$. Specifically, when $f_{\bm{\theta}}$ is a linear regressor, which is typically the case in deep neural networks, we have $\hat{y}_i = \bm{\theta}^{\trsp}\bz_i$. The encoder and $f_{\bm{\theta}}$ are trained by minimizing a task-specific regression loss $\mL_{reg}$. Typically, the mean-squared error is used, i.e. $\mL_{reg} = \frac{1}{N}\sum_{i=1}^N(y_i - \hat{y}_i)^2$. 

To formulate regression as a classification problem, the continuous target $y$ is quantized to $K$ classes $y_i^c \in \{1, \cdots, K\}$,
and the cross-entropy loss is used to train the encoder and classifiers $\mL_{CE} = -\frac{1}{N} \sum_{i=1}^{N}\log \frac{\exp \bm{\theta}_{y^c_i}^{\trsp}\bz_i}{\sum_{j=1}^{K} \exp \bm{\theta}_j^{\trsp}\bz_i}$, where $\bm{\theta}_k$ is the classifier \footnote{In this work, a classifier represents a single $\bm{\theta}_j$ rather than the whole set $\{\bm{\theta}_j| j\in K\}$.} corresponding to the class $k$. The function $d(*,*)$ measures some distance between two points, \eg Euclidean distance. 
 
\subsection{Ordinality and Tightness}
This section shows that preserving ordinality tightens the learned representation, and conversely, tightening the representation will help preserve ordinality.  
A lower $\mH(\bZ|\bY)$ represents a higher compression
\citep{zhang2024deep}
. The compression is maximized when $\mH(\bZ|\bY)$ is in its minimal ($\mH(\bZ|\bY)=-\infty$ for \rebuttal{differential} entropy and $\mH(\bZ|\bY)=0$ for the discrete entropy).  

First, we define ordinality following~\citep{gong2022ranksim}:
\begin{deftn}
(Ordinality). The ordinality is perfectly preserved if 
$\forall i,j,k$, the following holds:  
$d(y_i, y_j) \leq d(y_i, y_k) \Rightarrow  d(\bz_i, \bz_j) \leq d(\bz_i, \bz_k)$. 
\end{deftn}

\begin{thm}
\label{thm:ranktoTightness}
    Let $\mB(\bz,\epsilon) = \{\bz' \in \mZ| d(\bz, \bz') \leq \epsilon \}$ be the closed ball center at $\bz$ with radius $\epsilon$. 
    Assume that $\forall (\bx, \bz, y) \in \mP$ and $ \forall \epsilon >0, \exists (\bx', \bz', y') \in \mP$ such that $\bz'\in \mB(\bz,\epsilon)$ and $y' \neq y$. 
    Then if the ordinality is perfectly preserved, $\forall (\bx_i, \bz_i, y_i), (\bx_j, \bz_j, y_j) \in \mP$, the following hold: $y_i = y_j \Rightarrow d(\bz_i,\bz_j) = 0$.
\end{thm}

The detailed proof of Theorem \ref{thm:ranktoTightness} is given in Appendix \ref{appendix:proof_1}. Theorem \ref{thm:ranktoTightness} states that if the ordinality is perfectly preserved, then the tightness (i.e. $\mH(\bZ|\bY)$) is minimized.  This suggests that preserving ordinality will tighten the representations. The assumption in Theorem \ref{thm:ranktoTightness} aligns with the learning target that learning continuously changes representations from continuous targets.

Conversely, if the representations can be correctly mapped to the target and are perfectly tightened, then the representations collapse into a manifold homeomorphic to the target space (e.g., collapse into a single line when the target space is a line) [\citep{zhang2024deep}, Proposition 2]. Thus, ordinality will be perfectly preserved locally. Note that reserving ordinality globally constrains the line to be straight, which is not necessary. 

\subsection{Regression tightness}
Why are additional efforts to emphasize ordinality necessary?
In this work, we find that standard deep regressors are weak in their ability to tighten the representations due to the gradient update direction with respect to the representations. Consider a fixed linear regression with a typical regression loss (e.g., MSE, L1),
which has the following gradient with respect to $\bz_i$:
\begin{align} 
    \frac{\partial \mL_{\text{reg}}}{\partial \bz_i} &= \mL'_{\text{reg}}(\bm{\theta}^{\trsp} \bz_i-y_i)  \bm{\theta}^{\trsp}.
\end{align}
Here, the direction of $\frac{\partial \mL_{\text{reg}}}{\partial \bz_i}$ is determined solely by the direction of $\bm{\theta}$.  As such, during learning, all the 
$\bz_i$ are moved either towards the direction of $\bm{\theta}$ (or away). This movement can be regarded as a probability density shift~\citep{sonoda2019transport}
, so regression suffers from a weak ability to change the probability density in directions perpendicular to $\bm{\theta}$, which indicates a limited ability to tighten the representations in those directions.
In other words, regressors can only move $\bz_i$ to $S_{y_i}$, but cannot tighten $S_{y_i}$, where $S_{y_i} = \{\bz|f_{\bm{\theta}}(\bz) = y_i\}$ is the solution space of $f_{\bm{\theta}}(\bz) = y_i$.
More generally, for a differentiable regressor, we have the following:

\begin{thm}
\label{thm：gradient}
     Assume $f_{\bm{\theta}}$ is differentiable and $S_{y'_i}$ is a convex set, then $\forall \bz'_i, \bz'_j \in S_{y'_i}$:
    \begin{align}
        \frac{\partial \mL_{\text{reg}}}{\partial \bz_i} (\bz'_i - \bz'_j) =0,
    \end{align}
    where $y'_i$ is the predicted target of $\bz_i$.
\end{thm}

The detailed proof of Theorem \ref{thm：gradient} is given in Appendix \ref{appendix:theorem2}. The regressor $f_{\bm{\theta}}$ is generally differentiable for gradient backpropagation, and $S_{y_i}$ is commonly a convex set with widely used regressors, such as the linear regressor. Theorem \ref{thm：gradient} shows that the gradient with respect to the representation will be perpendicular to its solution space and has no effect within the solution space. In other words, with a fixed regressor, the gradient only moves the representations to the corresponding solution space and lags in its ability to tighten the feature space. 

In reality, the regressor is not fixed (i.e., updating with training), and the solution space is also changing during training. 
In the case of a linear regressor, the gradient with respect to $\bm{\theta}$ over a batch of $b$ samples can be given as:
\begin{align}
\label{equation:theta}
    \frac{\partial\mL_{\text{reg}}}{\partial\bm{\theta}} &= \frac{1}{b}\sum_{i=1}^b \mL'_{\text{reg}}(\bm{\theta}^{\trsp} \bz_i-y_i) \bz_i^{\trsp} = \frac{1}{b}\sum_{i=1}^b \bw_i\bz_i^{\trsp}.
\end{align}
Here, the direction of $\frac{\partial\mL_{\text{reg}}}{\partial\bm{\theta}}$ will tend to be the weighted mean of the direction of $\bz_i$. As discussed, the direction of $\bz_i$ approaches the direction of $\bm{\theta}$. Thus, $\bz_i$ will distribute around $\bm{\theta}$ and offset each other, resulting in a limited impact on the direction of $\bm{\theta}$. 

It is worth mentioning that the tightness here is specific to $\mH(\bZ|\bY=y_i)$ within $S_{y_i}$, which is indirectly related to the predicted results and performance. By contrast, the tightness outside $S_{y_i}$ directly affects the predicted results and potentially plays a more important role.

\subsection{Comparison in Tightness for Classification}
\label{subsection:classification}

Comparing classification with regression, we find classification has a higher flexibility to tighten representations in diverse directions $\bm{\theta}_k$, which suggests an ability to better tighten the representation. For the gradient with respect to $\bz_i$ \rebuttal{over a batch of b samples}:
\begin{align} 
    \frac{\partial \mL_{CE}}{\partial \bz_i} = \frac{\partial(-\frac{1}{b} \sum_{i=1}^{b} \bm{\theta}_{y_i}^{\trsp}\bz_i)}{\partial \bz_i} + \frac{\partial(\frac{1}{b} \sum_{i=1}^{b} \log \sum_{j=1}^{K} e^{ \bm{\theta}_j^{\trsp}\bz_i})}{\partial \bz_i} = \frac{1}{b} \sum_{i=1}^{b} \Bigl(\sum_{j=1}^{K} p_{ij}\bm{\theta}_{j}^{\trsp} -\bm{\theta}_{y_i}^{\trsp}\Bigr)
\end{align}
where $p_{ij}=\frac{\exp \bm{\theta}_j^{\trsp}\bz_i}{\sum_{k=1}^{K} \exp \bm{\theta}_k^{\trsp}\bz_i}$ is the probability of sample $i$ belonging to class $j$.
Here, the direction of $\frac{\partial \mL_{CE}}{\partial \bz_i}$ is affected by all $\bm{\theta}_k$, and $\bz_i$ will approach $\bm{\theta}_{y_i}$ with training. In contrast, the direction of $\frac{\partial \mL_{Reg}}{\partial \bz_i}$ is purely determined by $\bm{\theta}$.
Classification moves $\bz_i$ to its corresponding classifier $\bm{\theta}_{y_i}$ even if the sample is correctly classified. At the same time, regression does not have any effect on $\bz_i$ if it is correctly predicted (i.e., $\frac{\partial \mL_{\text{reg}}}{\partial \bz_i} = 0$). {This suggests classification has a higher ability to tighten the representations in the solution space $S_{y_i}$. Here $S_{y_i}$ for classification is defined as the set of $\bz_i$ that are classified as class $y_i$.}

In reality, the classifiers $\bm{\theta}_k$ are not fixed and are updated with training. The gradient with respect to $k^{\text{th}}$ classifier $\bm{\theta_k}$ \rebuttal{over a batch of b samples} is given as:
\begin{align}
    \frac{\partial \mL_{CE}}{\partial \bm{\theta}_k} = -\frac{1}{b} \sum_{i:y_i=k}\bz_i^{\trsp} + \frac{1}{b}\sum_{i=1}^{b}\frac{e^{ \bm{\theta}_k^{\trsp}\bz_i^{\trsp}}}{\sum_{j=1}^{K} e^{ \bm{\theta}_j^{\trsp}\bz_i}}\bz_i^{\trsp} 
    = \frac{1}{b}\sum_{i=1}^{b}(p_{ik} -\delta_{y_i,k})\bz_i^{\trsp} 
\end{align}
where $p_{ik}=\frac{e^{ \bm{\theta}_k^{\trsp}\bz_i}}{\sum_{j=1}^{K} e^{ \bm{\theta}_j^{\trsp}\bz_i}}$ is the probability of sample $i$ belongs to the class $k$, and $\delta_{y_i,k}$ is the Kronecker delta function. For classification, the direction of $\bm{\theta}_k$ 
will biased to the $\bz_i$ with respect to the class $k$
, while $\bz_i$ will also bias to its corresponding classifier. In contrast, for regression, the direction of $\bm{\theta}$ will tend to be the weighted mean of the direction of $\bz_i$.  Thus, the effect of the many $\bz_i$ on the direction of $\bm{\theta}$ will offset each other and have a limited impact. As a result, changes in the directions of $\bm{\theta}_k$ are generally greater than the change of the $\bm{\theta}$ direction in regression, and therefore classification can move $\bz$ more flexible and thus can potentially better tighten the representation.


\section{Method}
Our analysis in Sec. \ref{section:analysis} inspires us to tighten the regression representations. To this aim, we introduce the Multiple Target (\textbf{MT}) strategy and the Regression Optimal Transport Regularizer (\textbf{ROT-Reg}) to tighten the representations globally \rebuttal{(i.e., $\min_{\bZ} \mH(\bZ|\bY)$)} and locally \rebuttal{(i.e., $\min_{\bZ} \mH(\bZ|\bY=y_i), \bZ \in \mB(\bz_i, \epsilon)$, $\epsilon$ control the degree of locality).}
Inspired by the effect of multiple classifiers in classification, the MT strategy introduces additional targets as constraints to compress the representations. 
For ROT-Reg, we exploit it to encourage representations to have local structures similar to the targets, which implicitly tightens the representations.

\subsection{Target space with extra dimensions better tighten the feature space}
Our analysis in Sec. \ref{subsection:classification} suggests that classification outperforms regression in its ability to compress the representations in multiple directions, which come from multiple classifiers. 
Inspired by this, we introduce a simple yet efficient strategy, which adds extra dimensions for the target space to bring in extra regressors as constraints. Here, the additional regressors have a similar effect as individual classifiers. As shown in Figure \ref{figure_extraDimensions}, the additional constraints will result in a lower-dimensional $S_{y_i}$, which indicates higher compression. The number of additional targets depends on the intrinsic dimension of the feature manifold. 
In our Multiple Targets strategy, the final predicted target is the average over the multiple predicted targets:
\begin{align}
\label{equation:MT}
    \hat{y_i} = \frac{1}{M}\sum_{t=1}^M \hat{y}_i^t,
\end{align}
where $M$ is the number of the total target dimension and $\hat{y}_i^t$ is the $t^{\textit{th}}$ predicted target. 


\begin{figure}[!t]
\centering
\includegraphics[width=0.8\linewidth]{fig/1.pdf}
\caption{Illustration of the MT strategy. Changing the target from $y$ to $[y, y]$ will introduce an additional regressor to predict the additional $y$. The original solution space $S_{y_0}$ is a line in the feature manifold. The additional $y$ introduces a new constraint, tightening $S_{y_0}$ from a line to a point.}
\label{figure_extraDimensions}
\end{figure}

\subsection{Regression Optimal Transport Regularizer (ROT-Reg)}
The MT strategy tightens the representations globally through additional regressors.
We propose to further tighten the representations locally.  Specifically, we preserve the local similarity relations between the target and representation space. 
The local similarities are characterized by a self entropic optimal transport model \citep{yan2024optimal,landa2021doubly}.  The model determines the optimal plan is to move a set of samples to the set itself with minimal transport costs,
while each sample cannot be moved to itself.

Formally,
Given a set $S = \{ \bs_1, \ldots, \bs_n \}$,
the corresponding weight vector $\mathbf{p} = \mathbb{R}^{n}$ reflects how many masses the samples have,
where the weights simplify the simplex constraint $\sum_{i=1}^{n} p_i = 1$.
Usually, one can easily implement $\mathbf{p}$ as a uniform distribution, i.e., $p_i = \frac{1}{n}, ~\forall~i \in [n]$.
$C_{ij}^{\bS}$ is the transport cost between $\bs_i$ and $\bs_j$,
which is usually adopted as the Euclidean distance between the samples,
and $T_{ij}^{\bS}$ indicates how many masses are transported from the locations of $\bs_i$ to $\bs_j$.
The self entropic optimal transport is defined as follows:
\begin{align}
    \mathcal{T}(\bS) =\arg\min_{\bT} &~ \langle \bC^{\bS}, \bT \rangle + \gamma \Omega(\bT) \\
    \mbox{s.t.} &~ \bT \1_{n} = \mathbf{p}, \bT^{\top} \1_{n} = \mathbf{p}, T_{ii}=0 ~~ \forall i \in [n],
\end{align}
where $\gamma$ is a trade-off parameter,
and $\Omega(\bT) = \sum_{i=1}^{n} \sum_{j=1}^{n} T_{ij}^{\bS} \log T_{ij}^{\bS}$ is the negative entropic regularization,
which is used to smoothen the solution and speed up the optimization \citep{NIPS2013_af21d0c9}.

Given the solution $\widetilde{\bT}^{\bS}$ minimizing the above objective,
the element $T_{ij}^{\bS}$ measures the similarity relation between samples $\bs_i$ and $\bs_j$,
since two samples with a large distance $C^{\bS}_{ij}$ will induce a small transport mass $T^{\bS}_{ij}$ between them.
As a result,
the optimal total transport cost $\langle \bC^s, \widetilde{\bT}^{\bS} \rangle$ reflects the tightness of the samples.

Motivated by this,
we employ the self optimal transport model to capture local similarity relations of target and representation spaces, respectively,
and encourage a relation consistency between two spaces.
In specific,
we first construct a self optimal transport model on the target space to obtain $\widetilde{\bT}^{\bY} = \mathcal{T}(\bY)$,
which describes the local similarity relations between the regression targets.
After that,
we learn regression representations $\bZ$ such that the corresponding optimal transport matrix 
$\widetilde{\bT}^{\bZ} = \mathcal{T}(\bZ)$ is consistent with $\widetilde{\bT}^{\bY}$,
which is achieved by the following loss function
\begin{align}
    \mL_{ot} = \big| \langle \bC^{\bZ}, \widetilde{\bT}^{\bY} \rangle - \langle \bC^{\bZ}, \widetilde{\bT}^{\bZ} \rangle \big|.
\end{align}
ROT-Reg is updating $\bC^{\bZ}$ through gradient backpropagation to minimize $\mL_{ot}$. In contrast, simply minimizing the gap of $\widetilde{\bT}^{\bY}$ and $\widetilde{\bT}^{\bZ}$ can introduce optimization challenges, as the two matrices are obtained iteratively through the Sinkhorn algorithm rather than simply through gradient backpropagation. In addition, directly minimizing $||\bC^\bZ - \bC^\bY ||_F$ imposes an overly strict constraint on the feature manifold, forcing it to become identical to the target space, which is unnecessary.

It is worth mentioning that $\gamma$ controls the `smooth' of the transport plan $\bT$ \rebuttal{, and determines the degree of locality}. When $\gamma=0$, $\bT$ will approach the minimal spanning tree (i.e., only transports mass to its nearest neighbor), and $\mL_{ot}$ will encourage the representations to have the same minimal spanning tree to the targets, which is shown to be a strategy to preserve the topology of the target space \citep{moor2020topological}. In fact, the topological auto-encoder \citep{moor2020topological} preserves the topological information in this way. Compared to topology autoencoder, $\mL_{ot}$ captures more local structures of targets when $\gamma > 0$. The final loss $\mL_f$ sums the task-specific loss $\mL_t$ and the regularizer with a trade-off hyper-parameter $\lambda$ :
\begin{align}
    \mL_f = \mL_t + \lambda \mL_{ot},
\end{align}


\section{Experiments}
We experiment on three deep regression tasks: age estimation, depth estimation, and coordinate prediction and compare with
RankSim \citep{gong2022ranksim},
Ordinal Entropy (OE) \citep{zhang2023improving},
and PH-Reg \citep{zhang2024deep}.
RankSim preserves ordinality explicitly to serve as an ordinality baseline. OE leverages classification for better regression representations and serves as a regression baseline. PH-Reg preserves the topological structure of the target space by the Topological autoencoder \citep{moor2020topological} and tightens the representation by Birdal's regularizer \citep{birdal2021intrinsic}, serving as a topology baseline. More details are given in Appendix \ref{appendix:details_real_world_tasks}.


\subsection{Real-world Datasets: Age Estimation and Depth Estimation}
For age estimation, we use AgeDB-DIR \citep{yang2021delving} and evaluate using Mean Absolute Error (MAE) as the evaluation metric. $\gamma$ and $\lambda$ are set to $0.1$ and $100$, respectively.
For depth estimation, we use NYUD2-DIR \citep{yang2021delving} and evaluate using the root mean squared error (RMSE) and the threshold accuracy $\delta_1$ as the evaluation metrics. $\gamma$ and $\lambda$ are set to $0.05$ and $10$, respectively. We set the total target dimension $M$ to be $8$ for both tasks.
Both AgeDB-DIR and NYUD2-DIR contain three disjoint subsets (i.e., Many, Med, and Few) divided from the whole set. We exploit the regression baseline models of \citep{yang2021delving}, which use ResNet-50 \citep{he2016deep} as the backbone, and follow their setting for both tasks.

Tables \ref{tab:age} and \ref{Table:depth} show results on age estimation and depth estimation respectively.  
Both the Multiple Targets strategy (\textbf{MT}) and $\mL_{ot}$ improve regression performance, and combining both further boosts the performance. Specifically, combining both achieves $0.52$ overall improvements (i.e. ALL) on age estimation, and a $0.156$ reduction of RMSE on depth estimation.


\begin{table}[t!]
	\caption{Quantitative comparison (MAE) on AgeDB-DIR. We report results as mean $\pm$ standard deviation over $10$ runs.
	\textbf{Bold} numbers indicate the best performance.
	}
 	\label{tab:age}
	\centering
		\scalebox{1}{\begin{tabular}{c|ccccc}
			\hline
			\multirow{1}[0]{*}{Method}
			& ALL & Many & Med. & Few  \\
			\hline
			Baseline & 7.80 $\pm$ 0.12 & 6.80 $\pm$ 0.06 & 9.11	$\pm$ 0.31 & 13.63	$\pm$ 0.43  \\
        $+$ RankSim  & 7.62	$\pm$ 0.13 &6.70	$\pm$ 0.10 & 8.90	$\pm$ 0.33 & 12.74	$\pm$ 0.48  \\
			 $+$ OE  & 7.65	$\pm$ 0.13 & 6.72	$\pm$ 0.09 & 8.77	$\pm$ 0.49 & 13.28	$\pm$ 0.73  \\
		+PH-Reg &  7.32 $\pm$ 0.09 & 6.50	$\pm$ 0.15 & 8.38	$\pm$ 0.11 & 12.18	$\pm$ 0.38  \\
  \hline
		+ MT &    7.67	$\pm$ 0.06 & 6.72	$\pm$ 0.08 & 8.87	$\pm$ 0.13 &  13.36	$\pm$ 0.16\\
  		+$\mL_{oe}$ & 7.36	$\pm$ 0.08 & 6.55	$\pm$ 0.07 &  8.40	$\pm$ 0.14 & 12.14	$\pm$ 0.33  \\
  	+ MT + $\mL_{oe}$ & \textbf{7.28}	$\pm$ \textbf{0.05} & \textbf{6.52}	$\pm$ \textbf{0.10} &  \textbf{8.26}	$\pm$ \textbf{0.19} & \textbf{11.86}	$\pm$ \textbf{0.24}  \\
   \hline
		\end{tabular}}
\end{table}


\begin{table}[t!]
	\caption{Quantitative comparison  on NYUD2-DIR. 
 }
 \label{Table:depth}
	\centering
		\scalebox{1}{\begin{tabular}{c|cccc|cccc}
			\hline
			\multirow{2}[0]{*}{Method} & \multicolumn{4}{c|}{RMSE~$\downarrow$} & \multicolumn{4}{c}{$\delta_1$~$\uparrow$}  \\
 			\cline{2-9}
			& ALL & Many & Med. & Few & ALL & Many & Med. & Few\\
			\hline
			Baseline  & 1.477 & 0.591 & 0.952 & 2.123 & 0.677 & 0.777 & 0.693 & 0.570 \\
   			+RankSim & 1.522 & \textbf{0.565}  & 0.889 & 2.213  & 0.666  & 0.791  & 0.735  & 0.513  \\
   			+OE & 1.419 & 0.671  &0.925 & 2.005  & 0.668  & 0.727  & 0.702  & 0.596  \\
   			+PH-Reg & 1.450 & 0.789  &0.911 & 2.002  & 0.620  & 0.621  & 0.680  & 0.596  \\
			\hline
   			+ MT & 1.367 & 0.605  &\textbf{0.854} & 1.952  & \textbf{0.715}  & \textbf{0.776}  & \textbf{0.759}  & 0.636  \\
   			+$\mL_{ot}$ & 1.353 & 0.654  &0.934 & 1.899  & 0.689  & 0.736  & 0.697  & 0.638  \\
   			+ MT + $\mL_{ot}$ & \textbf{1.321} & 0.685  & 0.951 & \textbf{1.829}  & 0.701  & 0.731  & 0.689  & \textbf{0.675}  \\
      \hline
		\end{tabular}}
\end{table}

\subsection{$\mL_{ot}$ preserves the local similarity relationships}
The effectiveness of $\mL_{ot}$ is verified  
with the coordinate prediction task from \cite{zhang2024deep}. 
This task predicts data coordinates sampled 
from manifolds such as Mammoth, Torus, and Circle, which have different topologies. 
The inputs are noisy data samples and the goal is to recover the true data coordinates. 
Figure \ref{Figure:coordinate_prediction} shows that $\mL_{ot}$ successfully preserves the similarity relationships of the targets, resulting in a feature manifold similar to the targets. 
Quantitative comparisons in Table \ref{tab:synthetic} indicate that $\mL_{oe}$ performs similarly to PH-Reg, specifically designed to preserve similarity relationships. However, the Multiple Targets (MT) strategy has a limited impact in this context, likely because the target space is three-dimensional, providing sufficient constraints for the feature manifold. 
 

\begin{table}[t!]
	\caption{Results ($\mL_{\text{mse}}$) on the coordinate prediction dataset. We report results as mean $\pm$ standard deviation over $10$ runs. \textbf{Bold} numbers indicate the best performance.
 }
	\label{tab:synthetic}
	\centering
		\scalebox{1}{\begin{tabular}{c|ccc}
			\hline
			\multirow{1}[0]{*}{Method}
			& Mammoth & Torus & Circle  \\
			\hline
			Baseline  & 211 $\pm$ 55 & 3.01 $\pm$ 0.11 & 0.154 $\pm$ 0.006  \\
			$+$ InfDrop  & 367 $\pm$ 50 & 2.05 $\pm$ 0.04& 0.093 $\pm$ 0.003  \\
			$+$ OE  & 187 $\pm$ 88 & 2.83 $\pm$ 0.07 & 0.114 $\pm$ 0.007  \\
			 +Topological Autoencoder & 80 $\pm$ 61& 0.95 $\pm$ 0.05& 0.036 $\pm$ 0.004  \\
			 + PH-Reg & \textbf{49} $\pm$ \textbf{27} & \textbf{0.61} $\pm$ \textbf{0.05}& 0.013 $\pm$0.008  \\
			\hline
			 + MT & 174 $\pm$ 76& 2.99 $\pm$ 0.11&  0.152 $\pm$ 0.005    \\
			 + $\mL_{ot}$ & 87 $\pm$ 26& 0.77 $\pm$ 0.02&  0.010 $\pm$ 0.001    \\    
			 + MT+ $\mL_{ot}$& 76 $\pm$ 53 & 0.75 $\pm$ 0.03&  \textbf{0.010} $\pm$ \textbf{0.001}    \\    
    			\hline
		\end{tabular}}
\end{table}

\begin{figure}[!t]

	\centering
	\subfigure[Target Space]{	\includegraphics[width=0.235\linewidth,height=0.25\linewidth]{fig/Circle_target.png}}
	\subfigure[Regression]{		\includegraphics[width=0.235\linewidth,height=0.25\linewidth]{fig/Circle_reg.png}}
	\subfigure[+ PH-Reg]{	\includegraphics[width=0.235\linewidth,height=0.25\linewidth]{fig/Circle_Ph-reg.png}}
         \subfigure[+ $\mL_{ot}$]{\includegraphics[width=0.23\linewidth,height=0.25\linewidth]{fig/Circle_ours.png}}
	\caption{Visualization of the feature manifolds, which shows that $\mL_{ot}$ preserves the local similarity relationships of the target space.
 \label{Figure:coordinate_prediction}
 }
\end{figure}



\subsection{Tightness and Ordinality affect each other}

\textbf{Compression for a better ordinality}.
We examine the impact of tightness on the ordinality. Table \ref{Table:TightnessToOrdinality} presents the Spearman's rank correlation coefficient \citep{spearman1961proof} and Kendall rank correlation coefficient \citep{kendall1938new} between the feature similarities (based on Cosine distance and Euclidean distance) and the label similarities. The two correlation coefficients measure how well ordinality is preserved.
Since tightness compresses the feature manifold, reducing its volume, we use volume as a proxy for tightness, and the volume is approximated by the mean of the similarities between samples.
The experiments are conducted on NYUD2-DIR, we randomly sample $1000$ pixels from a batch of $8$ test images. The label similarities are calculated as the Euclidean distances between the $1000$ pixels, while the corresponding feature similarities are the distances between their corresponding representations.
The results in Table \ref{Table:TightnessToOrdinality} show \rebuttal{standard regression fails to preserve the ordinality, while} MT and $\mL_{ot}$ both improve the ordinality, although they are designed to tighten the representations. Combining both has a similar effect on preserving ordinality as RankSim, which is specific designed for this purpose. The lower volumes of our method compared to the baseline indicate that the feature manifold is more compressed. We provide the visualization of the feature similarities in Appendix \ref{appendix:featureSimilarity}.



\begin{table}[t!]
	\caption{Correlation between feature and label similarities. Results are mean $\pm$ std dev over $10$ runs.}
	\centering
		\scalebox{0.8}{\begin{tabular}{c|c|ccc|ccc}
			\hline
			\multirow{2}[0]{*}{Method} &\multicolumn{1}{c|}{RMSE}& \multicolumn{3}{c|}{Cosine Distance} & \multicolumn{3}{c}{Euclidean Distance}  \\
 			\cline{3-8}
			&(ALL)& Spearman’s~$\uparrow$ & Kendall’s~$\uparrow$ & volume & Spearman’s~$\uparrow$ & Kendall’s~$\uparrow$ & volume\\
			\hline
			Baseline & 1.477 & 0.39 $\pm$ 0.15 & 0.27 $\pm$ 0.11 & 0.573 $\pm$ 0.071 & 0.35 $\pm$ 0.14 &  0.24 $\pm$ 0.10 & 7.72 $\pm$ 0.92\\
			+ RankSim & 1.522  & 0.09 $\pm$ 0.04 & 0.06 $\pm$ 0.03 & 0.000 $\pm$ 0.000 & 0.60 $\pm$ 0.16 &  0.44 $\pm$ 0.13 & 4.26 $\pm$ 1.29\\
			+ MT & 1.367  & 0.49 $\pm$ 0.14 & 0.34 $\pm$ 0.10 & 0.492 $\pm$ 0.047 & 0.47 $\pm$ 0.11 &  0.33 $\pm$ 0.08 & 6.56 $\pm$ 1.18\\
			+ $\mL_{ot}$ & 1.353 & 0.48 $\pm$ 0.16 & 0.34 $\pm$ 0.12 & 0.010 $\pm$ 0.003 & 0.42 $\pm$ 0.15 &  0.29 $\pm$ 0.11 & 5.57 $\pm$ 0.77\\
		+ MT + $\mL_{ot}$ & 1.321 & 0.64 $\pm$ 0.09 & 0.46 $\pm$ 0.08 & 0.006 $\pm$ 0.002 & 0.61 $\pm$ 0.11 &  0.44 $\pm$ 0.09 & 4.16 $\pm$ 0.51\\
            \hline
		\end{tabular}}
     \label{Table:TightnessToOrdinality}
\end{table}


\textbf{Ordinality for a better compression}. 
To further verify that preserving ordinality leads to better compression, we visualize the feature manifold of the depth estimation task in 3D space. This is done by changing the last hidden layer's feature space to three dimensions. As shown in Figure \ref{Figure:featurespace}, explicitly preserving the ordinality (i.e., +RankSim) compresses the feature manifold into a thin line, which shows a similar effect to explicitly tightening the representations (i.e., +MT).


\begin{figure}[!t]
	\centering
	\subfigure[Baseline]{	\includegraphics[width=0.3\linewidth,height=0.25\linewidth]{fig/featurespace_baseline.png}}
    \subfigure[+ RankSim]{\includegraphics[width=0.3\linewidth,height=0.25\linewidth]{fig/featureSpace_ranksim.png}}
    \subfigure[+ MT]{\includegraphics[width=0.3\linewidth,height=0.25\linewidth]{fig/featurespace_mt.png}}
	\caption{Visualizations of the feature manifold \rebuttal{on NYUD2-DIR for depth estimation}. 
 Preserving the ordinality (+ RankSim) has an effect similar to MT, which explicitly tightens the representations.
 }
 \label{Figure:featurespace}
 \vspace{-0.2cm}
\end{figure}

\subsection{Tightness of regression}

Our theoretical analysis in Sec. \ref{section:analysis} focuses on the gradient direction of representations. However, in reality, the neural network updates its parameters to update the representations indirectly. 
Here, we verify our analysis by visualizing the updating of $\bz$ and $\bm{\theta}$ in the depth estimation task.

\textbf{The update of $\bz$}. We change the last hidden layer's feature space to $2$ dimensional for visualization. We randomly sample $1000$ pixels from a batch of $8$ images in the test set of NYUD2-DIR to visualize the feature manifold.
Figure \ref{Figure:ChangeofZ} displays the feature manifolds at epoch 1 (blue dots) and epoch 10 (the final epoch, red dots), the corresponding pixel representations are connected by black arrows. Aligned with our theoretical analysis, the directions of the representation updates follow the direction of $\bm{\theta}$. To verify this quantitatively, we calculate the principal component of the updating directions using PCA. We find that the cosine distance between this principal component and $\bm{\theta}$ is very small (0.03), indicating that the updating directions of representations from the beginning to the end of training follow the direction of $\bm{\theta}$. 
The visualization also shows that the feature manifold tightened limited in the direction perpendicular to $\bm{\theta}$ throughout the training. 
The visualizations of feature manifolds at each epoch are provided in Appendix \ref{appendix:updatingofZ}, which reveals the tightening effect in the direction perpendicular to $\bm{\theta}$ between adjacent epochs is even smaller.

\textbf{The update of $\bm{\theta}$}. 
As discussed in Sec. \ref{section:analysis}, the effect of $\bz_i$ on the direction of $\bm{\theta}$ tend to offset each other and result in a limited impact, while changes in the directions of $\bm{\theta}_k$ in classification are generally greater.
Here we quantitatively verify this by calculating the cosine distances between $\Tilde{\theta}$ and $\bm{\theta}$ at each epoch from $1$ to $10$ (final epoch), where $\Tilde{\theta}$ represents $\bm{\theta}$ at epoch $10$. We also convert this regression task into a classification task by uniformly discretizing the target range into $10$ classes, and monitoring the change of $\bm{\theta}_{k}$ in the same way. As shown in Figure \ref{Figure:ChangeofTheta}, the changes is $\bm{\theta}_{k}$ are all larger than the changes in $\bm{\theta}$. The maximum cosine distance between $\bm{\theta}$ at different epochs is very small (i.e., $0.0004$), which also verified the limited change of $\bm{\theta}$.


\textbf{Multiple $\bm{\theta}$s}. 
Adding additional $\bm{\theta}$s (our MT strategy) \shihao{, with random initialization,} does not change the updating speed of $\bm{\theta}$ (see Figure \ref{figure:appendix:multipleTheta} in the Appendix \ref{appendix:multipleTheta}). The updating directions of all the $\bm{\theta}$s are even aligned. Let $v_{\bm{\theta}}^{i} = \bm{\theta}^{i+1} - \bm{\theta}^{i}$ be the updating vector of $\bm{\theta}$ at iteration $i$. Figure \ref{Figure:ChangeofMultipleTheta} plots the set of points $\{v_{\bm{\theta}}^{i}| i=500k, k \in \mZ, 0 \leq k \leq 100 \}$ for three $\bm{\theta}$s. This visualizes the change of $\bm{\theta}$s throughout the training process. The three $\bm{\theta}$s are distributed along a line, with the original as the center. When we calculate the principle components of $\{v_{\bm{\theta}}\}$ for the three $\bm{\theta}$s using PCA, the maximum cosine distances between the three principle components are less than $1e-4$, which quantitatively shows the updating directions of all $\bm{\theta}$s are in the same direction. As shown in Eq. \ref{equation:theta}, $v_{\bm{\theta}}$ is the weighted mean of a batch of $\bz$. Different $\bm{\theta}$ leads to different magnitudes of the weight means, while the directions remain steady.
It is worth mentioning that the multiple $\bm{\theta}$s do not collapse to a single $\bm{\theta}$ 
\rebuttal{in reality, although their updating directions are the same. This is because $\bm{\theta}$s are randomly initialized, and their directions remain nearly identical during training (See Figure \ref{Figure:ChangeofTheta}), due to the three reasons: 
1) The magnitude of $\frac{\partial \mL_{\text{reg}}}{\partial \bm{\theta}}$ is `scaled' by $\bw_i$, since $\bw_i$ often follows a Gaussian distribution centered at the origin, as assumed in models like Bayesian linear regression. When $\bw$ and $\bz$ are independent or weak dependent, $\mathbb{E}[\bw_i\bz_i]$ will approach 0 and causing $\frac{\partial \mL_{\text{reg}}}{\partial \bm{\theta}}$ be `scaled' to 0.
2) According to the central limit theorem, the updates of $\bm{\theta}$ follow a Gaussian distribution. This causes partial offsets between updates and results in a reduced accumulated effect. In addition, we empirically observe the mean of this Gaussian distribution approaches 0 (see Figure \ref{Figure:ChangeofMultipleTheta}), indicating that $\bw$ and $\bz$ are independent or weakly dependent.
3) The effect of $\bz_i$ on the direction of $\frac{\partial \mL_{\text{reg}}}{\partial \bm{\theta}}$ over a batch of samples offsets each other, resulting in the stability of the direction of $\bm{\theta}$ throughout training. This occurs because $\bz_i$ tends to be distributed around $\bm{\theta}$.} 
More details can be found in Appendix \ref{appendix:multipleTheta}.




\begin{figure}[!t]
	\centering
     \subfigure[Update of $\bz$]
    {\label{Figure:ChangeofZ}
\includegraphics[width=0.25\linewidth,height=0.24\linewidth]{fig/Change_Z.png}}
	\subfigure[Update of $\bm{\theta}$]{	\label{Figure:ChangeofTheta}\includegraphics[width=0.3\linewidth,height=0.24\linewidth]{fig/Change_theta.png}}
    \subfigure[Update directions of $\bm{\theta}$]{\label{Figure:ChangeofMultipleTheta}\includegraphics[width=0.30\linewidth,height=0.24\linewidth]{fig/theta_whole.png}}
 	\caption{(a) Visualization of the $\bz$ update, which aligns with $\theta$, (b) $\bm{\theta}$ update, which is steady through the training process, (c) the updating directions of $\bm{\theta}$s, which distributed along a line, with the original as the center.}
\end{figure}




\subsection{ablation study}
We conduct the ablation study on AgeDB-DIR for age estimations. The results are given in Table \ref{table:ablation}.

\begin{table}[t!]
	\caption{Ablation study on AgeDB-DIR for age estimation. We report MAE mean $\pm$ standard deviation \textbf{over 10 runs}, and the default $\lambda$, $\gamma$ and $M$ are set to $100$, $0.1$ and $8$, respectively.}
	\centering

		\scalebox{0.9}{\begin{tabular}{l|ccccc}
			\hline
			\multirow{1}[0]{*}{Method}
			& ALL & Many & Med. & Few  \\
			\hline
			Baseline & 7.80 $\pm$ 0.12 & 6.80 $\pm$ 0.06 & 9.11	$\pm$ 0.31 & 13.63	$\pm$ 0.43  \\
      \hline
			+ $\mL_{oe}$ &  &    &   &    \\
       $\lambda=1$   & 7.68	$\pm$ 0.08 & 6.75	$\pm$ 0.11 & 8.81	$\pm$ 0.19 & 13.38	$\pm$ 0.37  \\
         $\lambda=10$ & 7.55	$\pm$ 0.05 & 6.64	$\pm$ 0.07 &  8.71	$\pm$ 0.12 & 12.88	$\pm$ 0.35  \\
   $\lambda=100$ & 7.36	$\pm$ 0.08 & 6.55	$\pm$ 0.07 &  8.40	$\pm$ 0.14 & 12.14	$\pm$ 0.33  \\
   $\lambda=1000$  & 8.80	$\pm$ 0.19 & 7.17	$\pm$ 0.10 &  11.32	$\pm$ 0.48 & 17.26	$\pm$ 0.53  \\
    $\gamma=0.1$   & 7.36	$\pm$ 0.08 & 6.55	$\pm$ 0.07 &  8.40	$\pm$ 0.14 & 12.14	$\pm$ 0.33  \\
    $\gamma=1$   & 7.47	$\pm$ 0.12 &  6.61	$\pm$ 0.08  & 8.57	$\pm$ 0.31  & 12.55	$\pm$ 0.49  \\
    $\gamma=10$   & 7.51	$\pm$ 0.07 &  6.63	$\pm$ 0.08  &  8.60	$\pm$ 0.25 & 12.75	$\pm$ 0.31  \\
\hline
+ MT &  &    &   &    \\
M=2 & 7.72	$\pm$ 0.11 &  6.77	$\pm$ 0.07  &  8.92	$\pm$ 0.20 &  13.37	$\pm$ 0.50   \\
M=4  & 7.74	$\pm$ 0.06 &  6.77	$\pm$ 0.10  & 8.96	$\pm$ 0.13  &   13.52	$\pm$ 0.41  \\
M=8  &    7.67	$\pm$ 0.06 & 6.72	$\pm$ 0.08 & 8.87	$\pm$ 0.13 &  13.36	$\pm$ 0.16     \\
M=16  & 7.70	$\pm$ 0.11 &  6.73	$\pm$ 0.13  &8.93	$\pm$ 0.25   &   13.44	$\pm$ 0.36  \\
M=32   & 7.71	$\pm$ 0.08 & 6.74	$\pm$ 0.10   & 8.96	$\pm$ 0.32  &   13.40	$\pm$ 0.32  \\
noise & 8.00	$\pm$ 0.23 &  6.91	$\pm$ 0.20  & 9.43	$\pm$ 0.33  &  14.36	$\pm$ 0.67   \\
\hline
		\end{tabular}}
   \label{table:ablation}
\end{table}

\textbf{Hyperparameter $\lambda$,  $\gamma$}.  
We maintain the $\lambda$,  $\gamma$ at their default value $100$, $0.1$, and very one of them to examine their impact. 
As shown in Table \ref{table:ablation}, The MAE (ALL) decreases consistently as $\lambda$ increases. However, it overtakes the task-specific learning target when set too high (e.g., 1000) and decreases the performance. For the $\gamma$, the MAE (ALL) decreases consistently as $\gamma$ decreases. However, we empirically find a too low $\gamma$ (e.g., 0.01) will easily result in NaN values when calculating the transport matrixes using the Sinkhorn algorithm \citep{NIPS2013_af21d0c9}. We thus set $\gamma$ to be $0.1$.

\textbf{Number of the total targets $M$}. As shown in Table \ref{table:ablation}, the performance generally improves with the increase of $M$, when $M \leq 8$, and it keeps steady when $M$ increases further. 
\rebuttal{The primary factor affect the selection of $M$ is the intrinsic dimension of the feature manifold, which determines how many additional constraints (i.e., $M-1$) are required to compress the manifold. The range of $\bY$ have a limited impact on the selection of $M$, as $M$ equals $8$ works well on NYUD2-DIR ($y \in [0.7, 10]$) and AgeDB-DIR ( $y \in [0, 101]$)}.

\textbf{Mean $\hat{y}$ vs. a single $\hat{y}$}. We verify the strategy of the mean operation in MT (see Eq. \ref{equation:MT}), \rebuttal{which potentially bring in an ensemble effect}. We find $\hat{y}_i^t$ are very similar for all $t$. 
For a model with $M$,
the MAE(ALL) results calculated by $\hat{y}_i^t$ for $t \in T$ is with mean equals $7.579$ and standard deviation $0.0003$.
Thus \rebuttal{the improvement of MT is not due to the ensemble effect, and} the mean operation is optional.

\textbf{Additional $y$ vs. noise}. Add additional targets as noise, as shown in Table \ref{table:ablation}, does not work. 

\textbf{Time and memory consumption}.
We monitor the time and memory consumption for training a model from the beginning to the end with a batch size equal to 128. Table \ref{Table:timeandMemory} shows the added memory is negligible($1.7\%$), and the added time is limited ($17$ min).

\begin{table}[t!]
	\caption{Time and memory consumption.
	}
	\centering
		\scalebox{1}{\begin{tabular}{c|ccc}
			\hline
			\multirow{1}[0]{*}{Method}
			& Time (mins) & Memory(MB)   \\
			\hline
			Baseline & 65  & 14433   \\
			+ MT & 70 & 14457   \\
			+ $\mL_{ot}$ & 74  & 14587   \\
			+ MT + $\mL_{ot}$ & 82  & 14689   \\
   \hline
		\end{tabular}}
 \label{Table:timeandMemory}
\end{table}


\section{Conclusion}
In this paper, for the regression task, we provide a theoretical analysis that suggests preserving ordinality enhances the representation tightness, and regression suffers from a weak ability to tighten the representations. Motivated by classification and the self entropic optimal transport, we introduce a simple yet effective method to tighten regression representations. %Experiments on three regression tasks demonstrate the necessity to tighten the representations and the effect of our method in tightening the representations and preserving the local structure of the targets. 

\textbf{Acknowledgement}. This research / project is supported by the Ministry of Education, Singapore, under the Academic Research Fund Tier 1 (FY2022), National Natural Science Foundation of China (62206061, U24A20233),
Guangdong Basic and Applied Basic Research Foundation (2024A1515011901),
Guangzhou Basic and Applied Basic Research Foundation (2023A04J1700).


\bibliography{iclr2025_conference}
\bibliographystyle{iclr2025_conference}



\newpage
\appendix
\section{Appendix}
%You may include other additional sections here.

\subsection{Proof of Theorem \ref{thm:ranktoTightness}}
\label{appendix:proof_1}
\textbf{Theorem \ref{thm:ranktoTightness}}
\emph{
    Let $\mB(\bz,\epsilon) = \{\bz' \in \mZ| d(\bz, \bz') \leq \epsilon \}$ be the closed ball center at $\bz$ with radius $\epsilon$. 
    Assume that $\forall (\bx, \bz, y) \in \mP$ and $ \forall \epsilon >0, \exists (\bx', \bz', y') \in \mP$ such that $\bz'\in \mB(\bz,\epsilon)$ and $y' \neq y$. 
    Then if the ordinality is perfectly preserved, $\forall (\bx_i, \bz_i, y_i), (\bx_j, \bz_j, y_j) \in \mP$, the following hold: $y_i = y_j \Rightarrow d(\bz_i,\bz_j) = 0$.}
    
\begin{proof}
    \begin{align}
        d(\bz_i, \bz_j) &= d(\bz_i - \bz_k + \bz_k - \bz_j)\\
        & \leq d(\bz_i - \bz_k) + d(\bz_k - \bz_j),
    \end{align}
    where $\bz_k \in \mB(\bz,\epsilon)$. Since $d(y_k, y_j) \leq d(y_k, y_i)$, and the ordinality is perfectly preserved, we have:
    \begin{align}
        d(\bz_k - \bz_j) \leq d(\bz_i - \bz_k).
    \end{align}
    Thus:
    \begin{align}
        0 \leq d(\bz_i, \bz_j)\leq 2d(\bz_i - \bz_k) \leq 2\epsilon.
    \end{align}
    Let $\epsilon \rightarrow 0$, the result follows.
\end{proof}

\subsection{Proof of Theorem \ref{thm：gradient}}
\label{appendix:theorem2}
We first give a lemma:
\begin{lemma}
\label{lemma_gradient}
    Let $S_y = \{\bz| g(\bz) = y\}$ be a convex set, where $\bz \in \mmR^n$ is the representation, $y$ is the target and $g$ is the regressor. Assume $g$ is differentiable, then $\forall \bz_k, \bz_i, \bz_j \in S_y$, we have:
    \begin{align}
        \nabla g(\bz_k)(\bz_i-\bz_j)= 0.
    \end{align}
\end{lemma}
\begin{proof}
    Let $\bz_k^\epsilon = (1- \epsilon)\bz_k + \epsilon \bz_i$, where $\epsilon \in [0, 1]$. Since $g$ is differentiable, using Taylor expansion, we have: 
    \begin{align}
        g(\bz_k^\epsilon) &=g((1- \epsilon)\bz_k + \epsilon \bz_i)
        \\&= g(\bz_k + \epsilon(\bz_i - \bz_k))\\
        &= g(\bz_k) + \epsilon\nabla g(\bz_k)(\bz_i - \bz_k) + o(\epsilon).
    \end{align}
    Since $S_y$ is a convex set, we have $\bz_k^\epsilon \in S_y$. Thus:
    \begin{align}
        g(\bz_k^\epsilon) &= g(\bz_k) + \epsilon\nabla g(\bz_k)(\bz_i - \bz_k) + o(\epsilon)\\
        y &=y + \epsilon\nabla g(\bz_k)(\bz_i - \bz_k) + o(\epsilon) \\
        \frac{o(\epsilon)}{\epsilon}&= \nabla g(\bz_k)(\bz_k - \bz_i).
    \end{align}    
    Let $\epsilon \rightarrow 0$:
    \begin{align}
        \nabla g(\bz_k)(\bz_k - \bz_i) = \lim_{\epsilon \rightarrow 0}  \frac{o(\epsilon)}{\epsilon} =0.
    \end{align}
Similarly, we have:
\begin{align}
    \nabla g(\bz_k)(\bz_k - \bz_j) = 0.
\end{align}
Combining the two equations above, we have:
\begin{align}
    \nabla g(\bz_k)(\bz_i - \bz_j) &= \nabla g(\bz_k)(\bz_i - \bz_k + \bz_k - \bz_j)\\
    &= \nabla g(\bz_k)(\bz_i - \bz_k) + \nabla g(\bz_k)(\bz_k - \bz_j)\\
    &=0.
\end{align}
\end{proof}

% For higher dimensional $\by$, we can apply Lemma \ref{lemma_gradient} separately for each dimension.

\textbf{Theorem \ref{thm：gradient}}
\emph{
     Assume $f_{\bm{\theta}}$ is differentiable and $S_{y'_i}$ is a convex set, then $\forall \bz'_i, \bz'_j \in S_{y'_i}$:
    \begin{align}
        \frac{\partial \mL_{\text{reg}}}{\partial \bz_i} (\bz'_i - \bz'_j) =0,
    \end{align}
    where $y'_i$ is the predicted target of $\bz_i$.}
    
\begin{proof}
\begin{align}
    \frac{\partial \mL_{\text{reg}}}{\partial \bz_i} &= \frac{\partial \mL_{\text{reg}}(g(\bz_i)-y_i)}{\partial \bz_i}\\
    &=\frac{\partial \mL_{\text{reg}}(g(\bz_i)-y_i)}{\partial (g(\bz_i)-y_i)}\frac{\partial (g(\bz_i)-y_i)}{\partial \bz_i}\\
    &= \mL'_{\text{reg}}(g(\bz_i)-y_i) \nabla g(\bz_i).
\end{align}

Based on Lemma \ref{lemma_gradient}, we have:
    \begin{align}
        \nabla g(\bz_i)(\bz'_i-\bz'_j)= 0.
    \end{align}

Thus,
    \begin{align}
        \frac{\partial \mL_{\text{reg}}}{\partial \bz_i} (\bz'_i - \bz'_j) &= \mL'_{\text{reg}}(g(\bz_i)-y_i) \nabla g(\bz_i)(\bz'_i - \bz'_j)\\
        &= \mL'_{\text{reg}}(g(\bz_i)-y_i) \times 0 \\
        &=0.
    \end{align}
\end{proof}

\section{Details}

\subsection{Details about the real-world tasks}
\label{appendix:details_real_world_tasks}
For the age estimation on AgeDB-DIR, we adopt the suggested hyper-parameters to train the RankSim, where $\lambda, \gamma$ are set to $2, 1000$, and the results of OE and PH-Reg are adopted from their published papers. the evaluation metric MAE: $\frac{1}{N}\sum_{i=1}^{N} |y_i - y'_i|$, where N is the total number of samples, $y_i, y'_i$ are the label and predicted result.

For depth estimation on NYUD2-DIR, we adopt the suggested hyper-parameters of OE and PH-Reg to train the models. For RankSim, we train the model with $\gamma$ range from $1$ to $1000$. We report the best results for all the three baselines. The evaluation metric: threshold accuracy $\delta_1 \triangleq$ \% of $y_p, ~\mathrm{s.t.}~ \max(\frac{y_p}{y'_p}, \frac{y'_p}{y_p}) < 1.25$, and the root mean squared error (RMS):  $\sqrt{\frac{1}{n}\sum_p(y_p - y'_p)^2}$.


\section{visualizations}
\label{appendix:visualizations}
\subsection{Visualization of the updating of $\bz$}
\label{appendix:featureSimilarity}
We provide the visualization of the feature similarities in Figure. \ref{Figure:similarity_matrix}. 

\begin{figure}[!t]
	\centering
	\subfigure[Baseline]{	\includegraphics[width=0.19\linewidth,height=0.23\linewidth]{fig/Similarity_baseline.png}}
	\subfigure[+ RankSim]{		\includegraphics[width=0.19\linewidth,height=0.23\linewidth]{fig/Similarity_ranksim.png}}
	\subfigure[+ MT]{	\includegraphics[width=0.19\linewidth,height=0.23\linewidth]{fig/Similarity_MT3.png}}
         \subfigure[+ $\mL_{ot}$]{\includegraphics[width=0.19\linewidth,height=0.23\linewidth]{fig/Similarity_ot.png}}
        \subfigure[+ MT + $\mL_{ot}$]{\includegraphics[width=0.19\linewidth,height=0.23\linewidth]{fig/Similarity_MT_OT.png}}
	\caption{Feature similarity matrices (Eucildean Distance). Tightening the representations results in a better ordinality.
 }
 \label{Figure:similarity_matrix}
\end{figure}

\subsection{Visualization of the updating of $\bz$}
\label{appendix:updatingofZ}
The visualizations of feature manifolds at each epoch are provided in Figure \ref{Figure:zAtEachEpoch}. 
\rebuttal{
For the neural collapse of regression, the feature manifold will collapse into a single line when the target space is a line and the compression is maximized \citep{zhang2024deep}. This trend can be observed in Figure \ref{Figure:zAtEachEpoch}, where the feature manifold looks like a thick line and evolves toward a thinner line over training. However, standard regression’s limited ability to tighten representations results in a slower collapse. In contrast, our proposed method and RankSim both accelerate this collapse, as shown in Figure \ref{Figure:featurespace}.}
\begin{figure}[!t]
	\centering
	\subfigure[epoch 1 to 2]{	\includegraphics[width=0.32\linewidth,height=0.33\linewidth]{fig/Change_Z_1_to_2.png}}
 	\subfigure[epoch 2 to 3]{	\includegraphics[width=0.32\linewidth,height=0.33\linewidth]{fig/Change_Z_2_to_3.png}}
  	\subfigure[epoch 3 to 4]{	\includegraphics[width=0.32\linewidth,height=0.33\linewidth]{fig/Change_Z_3_to_4.png}}
   	\subfigure[epoch 4 to 5]{	\includegraphics[width=0.32\linewidth,height=0.33\linewidth]{fig/Change_Z_4_to_5.png}}
    	\subfigure[epoch 5 to 6]{	\includegraphics[width=0.32\linewidth,height=0.33\linewidth]{fig/Change_Z_5_to_6.png}}
     	\subfigure[epoch 6 to 7]{	\includegraphics[width=0.32\linewidth,height=0.33\linewidth]{fig/Change_Z_6_to_7.png}}
      	\subfigure[epoch 7 to 8]{	\includegraphics[width=0.32\linewidth,height=0.33\linewidth]{fig/Change_Z_7_to_8.png}}
       	\subfigure[epoch 8 to 9]{	\includegraphics[width=0.32\linewidth,height=0.33\linewidth]{fig/Change_Z_8_to_9.png}}
        \subfigure[epoch 9 to 10]{	\includegraphics[width=0.32\linewidth,height=0.33\linewidth]{fig/Change_Z_9_to_10.png}}
	\caption{Change of $\bz$ between adjoin epochs. }
 \label{Figure:zAtEachEpoch}
\end{figure}

\subsection{Updating of multiple $\theta$}
\label{appendix:multipleTheta}
The experiments are conducted on NYUD2-DIR, we change the last hidden layer's feature space to three dimensions for visualization, and the M in our MT strategy is set to 3.
The change of multiple $\bm{\theta}$s throughout the training is given in Figure \ref{figure:appendix:multipleTheta}. We further plot the change of $\{v_{\bm{\theta}}^{i}| i=500, k \in \mZ, 0 \leq k \leq 500 \}$ for three $\bm{\theta}$s. The visualizations are given in Figure \ref{figure:z}. The visualization shows the updating directions of $\bm{\theta}$s align each others, even for a neural network without training.


\begin{figure}[!t]
\centering
\includegraphics[width=0.9\linewidth]{fig/theta_3d.png}
\caption{Change of the multiple $\bm{\theta}$s.}
\label{figure:appendix:multipleTheta}
\end{figure}

\begin{figure}[!t]
\centering
\includegraphics[width=0.9\linewidth]{fig/theta_begin.png}
\caption{Change of $\bm{\theta}$s within the iteration $[0, 500]$.}
\label{figure:z}
\end{figure}

\end{document}
