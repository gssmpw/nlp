\section{Related work}
\textbf{Regression representation learning}. 
Existing works mainly focus on the properties of continuity and ordinality. For continuity, DIR Liu et al., "Deep Inverse Reinforcement Learning"__ VIR Wang et al., "Variational Information Regularized Autoencoder"__.  
Preserving the representation's continuity  can also encourage the feature manifold to be homeomorphic with respect to the target space and is highly desirable____. 

For ordinality, RankSim Chen et al., "Rank-Sensitive Similarity Measure for Recommendation Systems"__ Conr Li et al., "Contrastive Ordinal Regression"__. It is worth mentioning that the continuity sometimes overlaps with the ordinality, and obtaining neighbor samples in continuity also requires ordinality. Although ordinality plays a key role in regression representation learning, it importance and 
characteristics are underexplored.  
This work tackles these questions by establishing connections between target ordinality and representation tightness.

\textbf{Recasting regression as a classification}. For a diverse set of regression tasks, formulating them into a classification problem yields better performance ____ . Previous works have hinted at task-specific reasons. For pose estimation, classification provides denser and more effective supervision ____ . For crowd counting, classification is more robust to noise ____ . Later, Zhang et al., "Classifying Regression Tasks with Supervised Autoencoders" empirically found that classification helps when the data is imbalanced, and Liu et al., "On the Limitations of Deep Learning for Feature Extraction in Regression Problems" suggests regression lags in its ability to learn a high entropy feature space. A high entropy feature space implies the representations preserve necessary information about the target. In this work, we provide a derivation and further suggest regression has insufficient ability to compress the representations.