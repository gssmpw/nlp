\section{Conclusion}

We conduct a systematic evaluation of the robustness of LLMs in commonsense reasoning in both Chinese and English. 
% , marking the first comprehensive effort in this problem. 
To facilitate this evaluation process, we introduce two large-scale, finely-annotated datasets: HellaSwag-Pro and Chinese HellaSwag. 
In addition, we design various prompts to evaluate 41 LLMs, offering several key findings that may advance the field of commonsense reasoning. 
We believe this work will serve as a valuable resource to support further research into the commonsense reasoning of LLMs.