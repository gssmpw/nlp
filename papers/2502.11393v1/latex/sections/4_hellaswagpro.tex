\section{HellaSwag-Pro}

To comprehensively assess the robustness of LLMs in commonsense reasoning, we construct the HellaSwag-Pro adversarial dataset based on the bilingual English-Chinese HellaSwag. Inspired by the Bloom Cognitive Model (\citealp{krathwohl1973taxonomy}), we design seven variants for each question to examine the robustness of LLMs from multiple perspectives, as illustrated in Table \ref{variant type}.

\subsection{Variant Types}
\begin{itemize}[leftmargin=*]
    % \setlength{}
    \item \textbf{Problem restatement} aims to test the impact of textual description variations on model understanding. Specifically, we rephrase the context and correct choice while preserving the incorrect choices.

    \item \textbf{Reverse conversion} evaluates the model's capacity for reverse reasoning. \citep{guo2024exploring} indicates that in the mathematical domain, LLMs excel in forward reasoning compared to reverse reasoning. We seek to explore whether this phenomenon persists in commonsense reasoning. We utilize the original correct choice as the context, the original context as the correct choice, and generate three additional incorrect choices for this variant .

    \item \textbf{Causal inference} explores whether the model's ability to choose correctly stems from superficial pattern memorization or genuine understanding of causality. We merge the context and correct choice, generate one correct reason and produce three additional incorrect choices.

    \item \textbf{Sentence ordering} focuses on the model's comprehension of inter-sentence relationships. Correct ordering requires a genuine understanding of sentence relationships, such as progression or contrast. We concatenate the context and correct choice into a complete paragraph, then scramble the sentences' order and number them. The correct choice represents the sentences' order in the original paragraph with three incorrect choices featuring erroneous sequences.

    \item \textbf{Scenario refinement} investigates the model's ability to make context-appropriate choices, even in counterfactual situations. \citep{ma2024kor} reveals that models perform poorly in counterfactual reasoning. We select a relatively plausible choice from the original incorrect choices, then minimally modify the context to render this choice correct.

    \item \textbf{Negation transformation} probes the model's robustness to negative words. Research suggests that LLMs struggle with processing negations. We append negative descriptions to the context, transforming phrases like ``the man will'' to ``the man will not''. The originally least plausible choice becomes the correct choice for the variant, while retaining the original correct choice and generating two other plausible choices as negative choices.

    \item \textbf{Critical testing} examines the model's ability to question the premise when essential descriptors are absent. It involves modifying the context to lack necessary statements, rendering all choices unsuitable. We minimally alter the context to preclude the selection of any choice and include a new choice which is ``None of the above four choices are suitable''.
\end{itemize}

\subsection{Data Generation}

In this section, we also employ Qwen-Max for question reformulation based on comparative analysis. We utilize in-context learning to guide Qwen-Max in question reformulation, adhering to defined transformation rules and examples. However, we observe that Qwen-Max is not consistently reliable, exhibiting issues such as: (1) generating variants inconsistent with definitions, such as restating the wrong choices in problem restatement, (2) producing multiple correct choices or overly simplistic incorrect choices in reverse conversion and causal inference, and (3) generating invalid contexts, particularly in scenario refinement.

On one hand, we implement programmatic solutions, such as overwriting incorrect choices in question restatement with original incorrect choices. For reverse conversion and causal inference, we adopt an over-generate-and-filter approach like \textit{Chinese HellaSwag}. On the other hand, we conduct comprehensive manual verification of all questions generated to ensure data quality. We initially generated 24,260 pieces of data, and finally manually selected 1,600 pieces of initial data in Chinese and English, with each piece of initial data seven variants, totaling in 11,200 pieces of variant data. Through this rigorous process, we ultimately produce a thoroughly validated, high-quality bilingual robust commonsense reasoning dataset, HellaSwag-Pro.


