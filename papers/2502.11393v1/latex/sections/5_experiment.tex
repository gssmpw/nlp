\section{Experiment}
In this section, we conduct extensive experiments to evaluate the performance of various LLMs on our Hellaswag-Pro benchmark. Our study is guided by three key research questions:
\textbf{RQ1}: How do different LLMs perform across all variants?
\textbf{RQ2}: What is the relative difficulty of different variants?
\textbf{RQ3}: How robust are LLMs to diverse prompts during evaluation?

\subsection{Experiment Setup} 
\subsubsection{Model Selection and Implementation Details}
We select 41 representative commercial and open-source models, including English LLMs, such as GPT-4o, Claude-3.5-Sonnet, Gemini-1.5-Pro,Mistral series, Llama3 series and Chinese LLMs, like Qwen-Max,  Qwen2.5 series, InternLM-2.5 series, Yi-1.5 series, Baichuan-2 series and DeepSeek series.

We integrate both Chinese HellaSwag and HellaSwagPro into the lm-evaluation-harness platform. For the open-source models, we use the default settings of lm-evaluation-harness: do\_sample is set to false and the temperature is set to the default value of the hugging-face library. For the closed-source models, we set the temperature to 0.7. In addition, we set the maximum output length to 1024.

\subsubsection{Prompt Strategy}
Taking into account the influence of language and shot, we design 9 prompting strategies, including Direct, CN-CoT, EN-CoT, CN-XLT and EN-XLT. The last four setups include both zero-shot and few-shot variants.\footnote{
For open-source models, Direct adopts an approach similar to the official implementation of HellaSwag, computing the log-likelihood for each option and selecting the one with the highest log-likelihood. And we report normalized accuracy that accounts for the impact of option length. Other prompting strategies use a generation setup and report accuracy based on exact match.}
\textbf {(1)Direct}: LLMs makes the selection directly without any CoT process.
\textbf{(2)CN-CoT}: LLMs performs CoT in Chinese, regardless of dataset language.
\textbf{(3)EN-CoT}: Similar to CN-CoT, but CoT is conducted in English. 
\textbf{(4)CN-XLT}: LLMs are instructed to first translate English questions and options to Chinese, and then reason in Chinese.
\textbf{(5)EN-XLT}: Similar to CN-XLT, but translates from Chinese dataset to English and reasons in English. 

%\textbf {CN-CoT}: LLMs perform Chinese reasoning and then output the answer and 3 shots are provided.
%\textbf {CN-CoT}: Similar as CNCoTFewShot without any shots.
%\textbf {EN-CoT}: The reasoning process in English is executed and then the answer is output and 3 shots are provided.
%\textbf {CN-XLT}: Inspired by this, we instruct LLMs to translate questions in Chinese and then output the answer after performing reasoning in Chinese too. And 3 shots are provided.
%\textbf {EN-XLT}: Inspired by this, we instruct LLMs to translate questions in Englsih and then output the answer after performing reasoning in Englsih too. Three shots are provided.

\subsubsection{Evaluation metric}

To comprehensively evaluate the robustness of each LLM, we consider four metrics: 
% Original Accuracy (\textbf{OA}), Average Robust Accuracy (\textbf{ARA}), Robust Loss Accuracy (\textbf{RLA}), and  Consistent Robust Accuracy (\textbf{CRA}).
\noindent %
\textbf{- Original Accuracy (OA)} measures accuracy on original problems.
\begin{equation}\label{eq1}
OA=\frac{\sum_{(x, y) \in D} \mathds{1}[L M(x), y]}{|D|}.
\end{equation}
\noindent %
\textbf{- Average Robust Accuracy  (ARA)} represents average accuracy across all variants, gauging overall performance on the robustness tasks.
\begin{equation}\label{eq2}
ARA=\frac{\sum_{\left(x^{\prime}, y^{\prime}\right) \in D_{R}} \mathds{1}\left(L M\left(x^{\prime}, y^{\prime}\right)\right.}{\left|D_{R}\right|}.
\end{equation}

\noindent %
\textbf{- Robust Loss Accuracy (RLA)} is the difference between ARA and OA, indicating performance degradation on robustness data versus original data.
%\begin{tiny}
%\begin{equation}\label{eq3}
%RLA=\frac{\sum_{\left(x^{\prime}, y^{\prime}\right) \in D_{R}} %\mathds{1}\left(L M\left(x^{\prime}, y^{\prime}\right)\right.}{\left|D_{R}\right|}-\frac{\sum_{(x, y) \in D}\mathds{1}[L M(x), y]}{|D|}
%\end{equation}
%\end{tiny}
\begin{equation}\label{eq3}
RLA= OA - ARA.
\end{equation}
\noindent %
\textbf{- Consistent Robust Accuracy (CRA)} shows accuracy when the model correctly answers both original and variant data, reflecting the model do understand the problem.
% consistency in problem-solving.
\begin{equation}\label{eq4}
CRA=\frac{\sum_{x, y, x^{\prime}, y^{\prime}}\mathds{1}[L M(x), y] \cdot \mathds{1}[L M(x^{\prime}), y^{\prime}]}{\left|D_{R}\right|}.
\end{equation}
For all equation above, $D$ denotes the original dataset, where $x$ represents the input question and options, and $y$ represents the correct label, while $D_{R}$ is the robust dataset with $x^{\prime}$ and $y^{\prime}$ representing similar to $x$ and $y$.


\begin{table*}[ht]
\centering
\setlength{\tabcolsep}{5pt}
% \footnotesize
\scalebox{0.6}{
% Please add the following required packages to your document preamble:
% \usepackage{multirow}
% \usepackage[table,xcdraw]{xcolor}
% Beamer presentation requires \usepackage{colortbl} instead of \usepackage[table,xcdraw]{xcolor}
% Please add the following required packages to your document preamble:
% \usepackage{multirow}
% \usepackage[table,xcdraw]{xcolor}
% Beamer presentation requires \usepackage{colortbl} instead of \usepackage[table,xcdraw]{xcolor}
\begin{tabular}{ccccccccccccc}
\hline
\multicolumn{1}{c|}{{ }}& \multicolumn{4}{c|}{Chinese}& \multicolumn{4}{c|}{English}& \multicolumn{4}{c}{AVG}\\ \cline{2-13} 
\multicolumn{1}{c|}{\multirow{-2}{*}{{ Model}}} & { OA(\%)$\uparrow$}& { ARA(\%)$\uparrow$} & {RLA(\%)$\downarrow$}& \multicolumn{1}{l|}{{CRA(\%)$\uparrow$}} & { OA(\%)$\uparrow$}& { ARA(\%)$\uparrow$} & { RLA(\%)$\downarrow$}& \multicolumn{1}{l|}{{CRA(\%)$\uparrow$}} & {OA(\%)$\uparrow$}& { ARA(\%)$\uparrow$} & {RLA(\%)$\downarrow$}& { CRA(\%)$\uparrow$} \\ \hline
\multicolumn{1}{c|}{{ Human}} & 96.41& 97.79& -1.38 & \multicolumn{1}{l|}{92.03}& 95.56& 96.04& -0.48 & \multicolumn{1}{l|}{90.02}& 95.99 & 96.92 & -0.93& 91.03 \\ \hline
\multicolumn{13}{c}{\textit{Close-source LLMs}}\\ 
\multicolumn{1}{c|}{{ GPT-4o}}& { 91.37} & { 81.97} & { 9.40}& \multicolumn{1}{l|}{{ 75.55}} & { \textbf{88.63}} & { \textbf{70.17}} & { \textbf{18.46}} & \multicolumn{1}{l|}{{ \textbf{63.06}}} & { 90.00} & { \textbf{76.07}} & { \textbf{13.93}} & { \textbf{69.31}} \\
\multicolumn{1}{c|}{{ Claude3.5}}& { \textbf{95.37}} & { 80.15} & { 15.22} & \multicolumn{1}{l|}{{ 75.04}} & { 85.11} & { 66.02} & { 19.08} & \multicolumn{1}{l|}{{ 57.20}} & { 90.24} & { 73.09} & { 17.15} & { 66.12} \\
\multicolumn{1}{c|}{{ Gemini-1.5-Pro}}& { 90.62} & { 78.36} & { 12.26} & \multicolumn{1}{l|}{{ 70.48}} & { 87.75} & { 60.74} & { 27.01} & \multicolumn{1}{l|}{{ 58.27}} & { 89.19} & { 69.55} & { 19.63} & { 64.38} \\
\multicolumn{1}{c|}{{ Qwen-Max}}& { 93.50} & { \textbf{84.82}} & { \textbf{8.68}}& \multicolumn{1}{l|}{{ \textbf{78.91}}} & { 87.60} & { 62.61} & { 24.99} & \multicolumn{1}{l|}{{ 59.65}} & { \textbf{90.55}} & { 73.72} & { 16.83} & { 69.28} \\ \hline
\multicolumn{13}{c}{\textit{Chinese open-source LLMs}} \\ 
\multicolumn{1}{c|}{{ Qwen2.5-0.5B}}& { 60.75} & { 45.18} & { \textbf{15.57}} & \multicolumn{1}{l|}{{ 28.70}} & { 49.50} & { 38.21} & { \textbf{11.29}} & \multicolumn{1}{l|}{{ 20.57}} & { 55.13} & { 41.70} & { \textbf{13.43}} & { 24.64} \\
\multicolumn{1}{c|}{{ Qwen2.5-1.5B}}& { 63.25} & { 46.16} & { 17.09} & \multicolumn{1}{l|}{{ 29.89}} & { 56.88} & { 39.57} & { 17.30} & \multicolumn{1}{l|}{{ 23.48}} & { 60.06} & { 42.87} & { 17.20} & { 26.69} \\
\multicolumn{1}{c|}{{ Qwen2.5-3B}}& { 67.50} & { 48.75} & { 18.75} & \multicolumn{1}{l|}{{ 33.79}} & { 61.75} & { 39.98} & { 21.77} & \multicolumn{1}{l|}{{ 25.75}} & { 64.63} & { 44.37} & { 20.26} & { 29.77} \\
\multicolumn{1}{c|}{{ Qwen2.5-7B}}& { 67.63} & { 50.59} & { 17.04} & \multicolumn{1}{l|}{{ 35.62}} & { 65.63} & { 43.93} & { 21.70} & \multicolumn{1}{l|}{{ 30.77}} & { 66.63} & { 47.26} & { 19.37} & { 33.20} \\
\multicolumn{1}{c|}{{ Qwen2.5-14B}} & { 69.00} & { 51.41} & { 17.59} & \multicolumn{1}{l|}{{ 35.84}} & { 68.50} & { 45.20} & { 23.30} & \multicolumn{1}{l|}{{ 32.12}} & { 68.75} & { 48.30} & { 20.45} & { 33.98} \\
\multicolumn{1}{c|}{{ Qwen2.5-32B}} & { 69.75} & { 53.11} & { 16.64} & \multicolumn{1}{l|}{{ 37.54}} & { 70.00} & { 46.10} & { 23.90} & \multicolumn{1}{l|}{{ 32.68}} & { 69.88} & { 49.61} & { 20.27} & { 35.11} \\
\multicolumn{1}{c|}{{ Qwen2.5-72B}} & { \textbf{70.87}} & { \textbf{54.75}} & { 16.12} & \multicolumn{1}{l|}{{ \textbf{39.64}}} & { \textbf{72.00}} & { \textbf{47.75}} & { 24.25} & \multicolumn{1}{l|}{{\textbf{ 35.12}}} & { \textbf{71.44}} & { \textbf{51.25}} & {20.19} & { \textbf{37.38}} \\ \hdashline[0.5pt/5pt]
\multicolumn{1}{c|}{{ Baichuan2-7B}}& { 67.00} & { 46.16} & { 20.84} & \multicolumn{1}{l|}{{ 31.50}} & { 60.62} & { 39.04} & { 21.58} & \multicolumn{1}{l|}{{ 25.21}} & { 63.81} & { 42.60} & { 21.21} & { 28.36} \\
\multicolumn{1}{c|}{{ Baichua2-13B}}& { 69.13} & { 46.98} & { 22.15} & \multicolumn{1}{l|}{{ 33.45}} & { 64.62} & { 38.82} & { 25.80} & \multicolumn{1}{l|}{{ 26.07}} & { 66.88} & { 42.90} & { 23.97} & { 29.76} \\ \hdashline[0.5pt/5pt]
\multicolumn{1}{c|}{{ DeepSeek-7B}} & { 68.13} & { 47.96} & { 20.17} & \multicolumn{1}{l|}{{ 33.30}} & { 63.38} & { 40.39} & { 22.99} & \multicolumn{1}{l|}{{ 26.70}} & { 65.76} & { 44.18} & { 21.58} & { 30.00} \\
\multicolumn{1}{c|}{{ DeepSeek-67B}}& { 71.50} & { 49.21} & { 22.29} & \multicolumn{1}{l|}{{ 35.89}} & { 71.37} & { 40.63} & { 30.75} & \multicolumn{1}{l|}{{ 29.71}} & { 71.44} & { 44.92} & { 26.52} & { 32.80} \\ \hdashline[0.5pt/5pt]
\multicolumn{1}{c|}{{ InternLM2.5-1.8B}}& { 61.62} & { 42.07} & { 19.55} & \multicolumn{1}{l|}{{ 26.99}} & { 55.37} & { 38.46} & { 16.91} & \multicolumn{1}{l|}{{ 22.61}} & { 58.50} & { 40.27} & { 18.23} & { 24.80} \\
\multicolumn{1}{c|}{{ InternLM2.5-7B}}& { 67.25} & { 49.77} & { 17.48} & \multicolumn{1}{l|}{{ 34.57}} & { 69.50} & { 40.89} & { 28.61} & \multicolumn{1}{l|}{{ 29.75}} & { 68.38} & { 45.33} & { 23.04} & { 32.16} \\
\multicolumn{1}{c|}{{ InternLM2.5-20B}} & { 67.37} & { 48.08} & { 19.29} & \multicolumn{1}{l|}{{ 33.21}} & { 73.62} & { 41.11} & { 32.51} & \multicolumn{1}{l|}{{ 31.23}} & { 70.50} & { 44.60} & { 25.90} & { 32.22} \\ \hdashline[0.5pt/5pt]
\multicolumn{1}{c|}{{ Yi-1.5-6B}} & { 67.00} & { 49.59} & { 17.41} & \multicolumn{1}{l|}{{ 34.27}} & { 64.38} & { 39.37} & { 25.01} & \multicolumn{1}{l|}{{ 26.62}} & { 65.69} & { 44.48} & { 21.21} & { 30.45} \\
\multicolumn{1}{c|}{{ Yi-1.5-9B}} & { 68.50} & { 50.18} & { 18.32} & \multicolumn{1}{l|}{{ 35.55}} & { 66.37} & { 39.58} & { 26.79} & \multicolumn{1}{l|}{{ 27.48}} & { 67.44} & { 44.88} & { 22.56} & { 31.52} \\
\multicolumn{1}{c|}{{ Yi-1.5-34B}}& { 71.00} & { 52.23} & { 18.77} & \multicolumn{1}{l|}{{ 38.09}} & { 71.00} & { 40.75} & { 30.25} & \multicolumn{1}{l|}{{ 29.91}} & { 71.00} & { 46.49} & { 24.51} & { 34.00} \\ \hline
\multicolumn{13}{c}{\textit{English open-source LLMs}} \\ 
\multicolumn{1}{c|}{{ Llama3-8B}} & { 59.13} & { 46.62} & { 12.51} & \multicolumn{1}{l|}{{ 28.23}} & { 66.25} & { 40.21} & { 26.04} & \multicolumn{1}{l|}{{ 27.34}} & { 62.69} & { 43.42} & { 19.27} & { 27.79} \\
\multicolumn{1}{c|}{{ Llama3-70B}}& { 65.75} & { 48.63} & { 17.12} & \multicolumn{1}{l|}{{ 32.70}} & { \textbf{72.50}} & { 41.27} & { 31.23} & \multicolumn{1}{l|}{{\textbf{ 30.63}}} & {\textbf{ 69.13}} & { 44.95} & { 24.18} & { 31.67} \\ \hdashline[0.5pt/5pt]
\multicolumn{1}{c|}{{ Mistral-7B-v0.2}} & { 57.75} & { 46.25} & { \textbf{11.50}} & \multicolumn{1}{l|}{{ 27.57}} & { 67.50} & { \textbf{41.52}} & { 25.98} & \multicolumn{1}{l|}{{ 28.93}} & { 62.63} & { 43.88} & { 18.74} & { 28.25} \\
\multicolumn{1}{c|}{{ Mixtral-8x7B-v0.1}} & { 63.62} & { 46.80} & { 16.82} & \multicolumn{1}{l|}{{ 30.82}} & { 69.75} & { 41.21} & { 28.54} & \multicolumn{1}{l|}{{ 29.39}} & { 66.69} & { 44.01} & { 22.68} & { 30.11} \\
\multicolumn{1}{c|}{{ Mixtral-8x22B-v0.1}}& { 66.00} & {\textbf{ 50.73}} & { 15.27} & \multicolumn{1}{l|}{{ \textbf{34.32}}} & { 72.12} & { 41.25} & { 30.87} & \multicolumn{1}{l|}{{ 30.61}} & { 69.06} & { \textbf{45.99}} & { 23.07} & { \textbf{32.47}} \\ \hdashline[0.5pt/5pt]
\multicolumn{1}{c|}{{ Gemma-2-2B}}& { 61.88} & { 45.38} & { 16.51} & \multicolumn{1}{l|}{{ 29.02}} & { 59.62} & { 39.13} & { \textbf{20.50}} & \multicolumn{1}{l|}{{ 24.88}} & { 60.75} & { 42.25} & {\textbf{ 18.50}} & { 26.95} \\
\multicolumn{1}{c|}{{ Gemma-2-9B}}& { \textbf{69.13}} & { 46.75} & { 22.38} & \multicolumn{1}{l|}{{ 33.29}} & { 64.88} & { 39.80} & { 25.08} & \multicolumn{1}{l|}{{ 26.91}} & { 67.01} & { 43.28} & { 23.73} & { 30.10} \\
\multicolumn{1}{c|}{{ Gemma-2-27B}} & { 63.38} & { 48.52} & { 14.86} & \multicolumn{1}{l|}{{ 31.96}} & { 71.88} & { 40.91} & { 30.97} & \multicolumn{1}{l|}{{ 30.25}} & { 67.63} & { 44.71} & { 22.92} & { 31.11} \\ \hline
\end{tabular}
}
\caption{TODO: bolded is not result. Results of existing LLMs on our HellaSwag-Pro dataset using \textbf{Direct} prompt. ``AVG'' indicates the average performance of each model on Chinese and English parts of the dataset.
The best results for each metric in each model category are \textbf{bolded}. }
\label{tab:main experiment.}
\end{table*}

\subsection{Model Performance (RQ1)}
\paragraph{Overall Performance}
Table \ref{tab:main experiment.} provides a comprehensive evaluation of various LLMs across four performance metrics\footnote{The results of instruct/chat models of Qwen2.5, Llama3 and Mixtral latest series are shown in Appendix.}. The main observations are as follow:
\begin{itemize}[leftmargin=*,topsep=0pt]
% \setlength{}{0}
    \item Upon evaluating all available models, we found that all performed well in overall accuracy (e.g., GPT-4 scored 90.00 in AVG OA, Claude 3.5 scored 90.24 in AVG OA). However, all models struggled with variations of the questions, as evidenced by a positive RLA value for each model. In contrast, humans received a negative RLA value, suggesting that the question variants were not more challenging than the originals. This disparity further illustrates that current LLMs lack a true understanding of the reasoning process and can easily be misled by question variants.
    \item When comparing open-source and close-source models, the close-source models demonstrate stronger capabilities in both OA and ARA scores, similar to most existing benchmarks. Overall, the RLA values for close-source models are also smaller, indicating that they are more robust in commonsense reasoning tasks compared to open-source models.
    \item When we compare models within the same series (e.g., Qwen, Llama), we observe that larger models often achieve higher scores on OA, ARA, and CRA. However, they are also more susceptible to variations, i.e., they have higher RLA values, a phenomenon particularly evident in English datasets. We attribute this phenomenon to the fact that larger models, compared to smaller ones, may have memorized more data, allowing them to rely on memorization to solve some problems more easily and making them more prone to the influence of variations~\cite{}.
\end{itemize}
% 1. When evaluating all available models, We find although 
% 2. When comparing the opensource LLMs and close source LLMs, 
% 3. When looking into each serious details
% \noindent
% \textbf{Overall Model Performance.}
% 1. close-source > open-source 2. the large the better 3. all have a performance decline when meeting varients.

% To evaluate the performance of various models, we observed patterns consistent with current mainstream trends: closed-source models generally outperform open-source models across metrics. 
% For instance, the closed-source model GPT-4o achieved scores of 90.00 in OA, 76.07 in ARA, and 69.31 in CRA, whereas the open-source model Qwen2.5-72B scored 71.44, 51.25, and 37.38, respectively. 
% Furthermore, within each model series, performance tends to improve with larger model sizes. 
% Nevertheless, even the strongest closed-source models struggle with variations in questions, as indicated by positive values in RLA for all models. In contrast, human performance yields a negative RLA value, highlighting that current LLMs do not genuinely grasp the reasoning process and are prone to falling into traps set by question variants. 
% This suggests that there is still significant room for improvement in developing models that can robustly understand and reason through complex linguistic challenges.
% It reveals a consistent pattern across Chinese, English, and average scores, with close-sourced LLMs generally outperforming open-sourced models. 
% However, all models exhibit a significant drop in performance when faced with robust variants, as indicated by RLA and CRA. Among closed-source models, GPT-4o demonstrates the highest ARA of 76.07\% in average scores, demonstrating its overwhelming superiority. Among open-sourced models, larger models tend to perform better, with Qwen2.5-72B achieving the highest OA (71.44\%) and ARA (51.25\%) in the average scores. However, even these top performers still struggle with robustness, as evidenced by the substantial RLA of 13.93\% for GPT-4o and 20.19\% for Qwen2.5-72B. Interestingly, some English open-sourced models, such as Llama3-70B and Mixtral-8x22B-v0.1, show competitive performance in English tasks but lag in Chinese tasks, highlighting the importance of language-specific training.

% \noindent
% \textbf{Chinese Models vs English Models.}
% Chinese models generally demonstrate higher OA in Chinese tasks compared to English tasks, with Qwen-Max achieving 93.50\% OA in Chinese versus 87.60\% in English. Conversely, English models tend to perform better in English tasks, exemplified by Llama3-70B's 72.50\% OA in English compared to 65.75\% in Chinese. 
% However, both Chinese and English models exhibit important drops in ARA across languages, indicating challenges in maintaining performance when faced with variations. This trend suggests that while models may excel in their primary language, they struggle with robustness across linguistic boundaries. 
% Notably, larger models tend to achieve higher ARA scores but also experience more substantial RLA, as seen with Qwen2.5-0.5B (41.70\% ARA, 13.43\% RLA in total) and Qwen2.5-72B (51.25\% ARA, 20.19\% RLA in total). 
% This pattern indicates that while increased model size enhances overall performance, it doesn't necessarily improve robustness proportionally. 
% The discrepancy between OA and ARA across languages underscores the need for improved cross-lingual robustness in language models, particularly as they scale in size and capability.


% \noindent
% \textbf{Comparison between Chinese and English datasets.}
% Generally, models demonstrate higher accuracy on the Chinese dataset compared to the English one, as evidenced by the consistently higher OA, ARA and CRA scores. For instance, GPT-4o achieves an OA of 91.37\%, an ARA of 81.97\% , an CRA of 75.55\% on the Chinese dataset, compared to 88.63\% and 70.17\% respectively on the English dataset. This trend is observed across most models, suggesting that the Chinese dataset is easier than English one. Moreover, the RLA values are typically lower for Chinese, indicating smaller performance drops when dealing with robust variants of Chinese questions. For example, Qwen-Max shows an RLA of 8.68\% for Chinese versus 24.99\% for English, highlighting a more consistent performance in Chinese. The CRA scores further reinforce this observation, with models generally maintaining higher consistency in correct answers for both original and variant Chinese questions.
% We attribute this phenomenon to the fact that blablabla

\noindent
\textbf{Reasoning Transferable Capability.}
% 为了进一步
To further analyze whether the model can transfer reasoning ability from the original question to its variant, Figure \ref{consis} presents the distribution of model performance on the original question and variant pairs. For all models, the pairs of (HellaSwag \ding{51} HellaSwag-Pro \ding{55}) occupy a significant proportion, indicating a challenge in transferring reasoning capabilities for current LLMs to more complex scenarios. Looking deeply, closed-source models like GPT-4 and Qwen-Max achieve around a 69\% portion of (HellaSwag \ding{51} HellaSwag-Pro \ding{51}) and a 3\% portion of (HellaSwag \ding{55} HellaSwag-Pro \ding{55}), while in contrast, open-source models struggle with around a 30\% portion of (HellaSwag \ding{51} HellaSwag-Pro \ding{51}) and a 20\% portion of (HellaSwag \ding{55} HellaSwag-Pro \ding{55}), further indicating the robustness of reasoning abilities in closed-source models.
% If a model can get both the original question and the variant right, we consider it to have transferable reasoning ability. Table \ref{consis} presents the distribution of model performance on the original question and variant pairs. Among all models, the pairs of (HellaSwag \ding{51}HellaSwag-Pro \ding{55}) account for a considerable proportion, i 
% The closed-source models like GPT-4o and Qwen-Max achieve around 69\% portion of (HellaSwag \ding{51}HellaSwag-Pro \ding{51}) and 3\% portion of (HellaSwag \ding{55} HellaSwag-Pro \ding{55}), indicating stronger reasoning transfer ability than other models. In contrast, open-source models struggle more, with around 30\% portion of (HellaSwag \ding{51}HellaSwag-Pro \ding{51}) and 20\% portion of (HellaSwag \ding{55} HellaSwag-Pro \ding{55}). 
% A notable trend is observed among the Qwen2.5 series, where increasing model size from 7B to 72B parameters correlates with improved performance on correct answers for both datasets (33.20\% to 37.38\%) and decreased failure rates (17.69\% to 14.7\%). It underscores the importance of model size in commonsense reasoning tasks.

\begin{figure}[t]
\centering
\setlength{\abovecaptionskip}{0.1cm}
\setlength{\belowcaptionskip}{0cm}
\includegraphics[width=\linewidth,scale=1.00]{images/consis.pdf}
\caption{Analysis of the transferable ability of model reasoning based on question pair performance. The green part, where both the original and the variant data are right, represents the transferable performance of model reasoning.}
\label{consis}
\vspace{-15pt}
\end{figure}

\begin{figure*}[ht]
\centering
\setlength{\abovecaptionskip}{0.1cm}
\setlength{\belowcaptionskip}{0cm}
\includegraphics[width=\linewidth,scale=1.00]{images/xing.pdf}
\caption{The impact of different few-shot prompts on model performance. With - as the separator, the first two parts of the legend represent the prompt name, and the third part represents the language of the dataset.}
\label{xing}
\vspace{-15pt}
\end{figure*}

\begin{figure}[ht]
\centering
\setlength{\abovecaptionskip}{0.1cm}
\setlength{\belowcaptionskip}{0cm}
\includegraphics[width=1.05\linewidth,scale=1.05]{images/zhu.pdf}
\caption{The RLA Distribution for 7 variants of commonsense reasoning. Parts below the 0 axis indicate that the model’s performance on the variant is improved compared to the original problem.}
\label{fig:zhu}
\vspace{-15pt}
\end{figure}


\subsection{Variant Analysis (RQ2)}
To further analyze the impact of different variants, we assessed the contribution of each variant to the RLA score. A higher contribution indicates that the model is more likely to make errors in that type. Figure~\ref{fig:zhu} presents the overall results, and the key observations are as follows:
\begin{itemize}[leftmargin=*]
    \item For problem restatement, causal inference, and sentence ordering, these three categories are the least challenging. Almost all models, particularly the close-source and Qwen series models, perform well on these variants, indicating that current LLMs can effectively handle these forms and we do not pay more attention on this kind of varients.
    \item For reverse conversion and critical testing, these two varients each contribute about 10\% to the RLA score. This indicates that current LLMs struggle to fully generalize to these simple scenarios, possibly because these types of questions are not commonly encountered, and reaserchers should pay some attention to this type of varients.
    \item For negative transformation and scenario refinement, this are the two most difficult tasks, with negative transformation being particularly challenging. For almost all models, these two varients accounts for more than 50\% of the RLA score. This may be due to intuitively counterintuitive questions—such as the use of "will not"  or counterfactual scenarios in scenario refinement. These setups are less common in LLM training data and cannot be easily tackled through memory alone. Only those LLMs which truely understand the question could answer the varient correctly, wihch better reflect the true performance of the model.. In the future, researchers should focus more on enhancing LLM's capability to address such types of questions.
\end{itemize}

% 1. Problem restCausal Inference 
% To further analysis the impact of different varients, we further 
% Figure \ref{fig: zhu} presents a comprehensive analysis of various LLMs' performance across different variant types. Negative transformation emerges as the most challenging task for all models, with scores consistently above 50.00\% and peaking at 78.38\% for Gemini-1.5-Pro. Conversely, problem restatement appears to be the least challenging, with most models scoring in the negative range. Intriguingly, smaller models like Qwen2.5-0.5B demonstrate unexpected strengths in certain areas, such as sentence sorting (7.75\%), outperforming some larger counterparts. A detailed analysis of each variant type follows.

% \noindent
% \textbf{Causal inference.} In this category, scores vary widely from -4.73\% for Qwen-Max to 12.25\% for Baichuan2-13B, illustrating differing degrees of sensitivity to causal reasoning among the models. Smaller models, such as Qwen2.5-0.5B and Qwen2.5-1.5B, achieve better scores, indicating relatively stronger robustness in causal reasoning. Conversely, larger models, like Baichuan2-13B, have higher scores, suggesting greater sensitivity to the challenges of inferring causality.

% \noindent
% \textbf{Critical testing.} Larger models, including Qwen2.5-72B and DeepSeek-67B, exhibit higher RLA scores of 30.50\% and 31.37\%, respectively, suggesting increased sensitivity when dealing with incomplete key information. In contrast, GPT-4o achieves the lowest score, highlighting its superior robustness in critical reasoning. This trend indicates that more complex models might struggle to handle incomplete contexts, underscoring potential areas for improvement in sophisticated architectures.

% \noindent
% \textbf{Negative transformation.} This aspect remains consistently challenging for all models, with scores ranging from 48.88\% to 78.38\%. Advanced commercial models like Gemini-1.5-Pro and Claude-3.5 also score higher (78.38\% and 76.43\%, respectively), indicating a prevalent sensitivity issue in reasoning processes when handling negations, irrespective of model size or architecture.

% \noindent
% \textbf{Problem restatement.} The negative values in this category for nearly all models suggest it is not particularly challenging. This is surprising, given that previous models were quite sensitive to sentence representation.

% \noindent
% \textbf{Reverse conversion.} This variation, which involves swapping the roles of the question and answer, seems to specifically impact larger models. For example, Qwen2.5-72B and DeepSeek-67B exhibit higher RLA scores of 24.38\% and 27.43\%, respectively, indicating heightened sensitivity to reverse reasoning compared to their performance on original questions.

% \noindent
% \textbf{Scenario refinement.} The scores range from 16.06\% for Gemma-2-2B to 32.56\% for Qwen2.5-72B, with larger models displaying more sensitivity in adapting to counterfactual predictions. This suggests that larger models may rely more heavily on general commonsense rather than flexibly adapting to specific contexts. Consequently, increased model complexity might adversely affect adaptability to scenario changes, underscoring the need for enhanced flexibility in advanced models.

% \noindent
% \textbf{Sentence sorting.} This category exhibits the most varied results across models. Some larger models like DeepSeek-67B and InternLM2.5-20B display higher scores (26.69\% and 26.68\%), indicating sensitivity, while others like Qwen2.5-72B and Gemini-1.5-Pro excel with lower scores (-9.88\% and -1.07\%, respectively). This suggests that sentence sorting ability may depend more on specific training approaches rather than being solely contingent on model size.


\subsection{Prompt Robustness (RQ3)}
% To investigate how prompt  influence our benchmark, we apply sereral prompt strategy on our datasets and showcase the average performance of all models on different kind of prompt strategies.
% Table~\ref{prompt} illustrates the final results. For both Chinese and English datasets, CN LLMs achieve the highest performance using CN-CoT-Few-Shot, followed closely by EN-CoT-Few-Shot, with overall performance scores of 67.36\% and 67.03\%, respectively. In contrast, English LLMs perform best with the EN-CoT-Few-Shot, reaching 67.55\% on the Chinese dataset and 60.36\% on the English dataset.
% Contrary to previous findings, translating the dataset to the model's advantage language before performing reasoning does not enhance performance. Moreover, Figure~\ref{xing} also shows the similar phenomenon. Conducting CoT reasoning in the model’s advantage language generally leads to better outcomes compared to Direct. Additionally, increasing the number of shots consistently improves performance across most configurations, highlighting the benefits of exposing models to multiple examples. 
To explore the impact of various prompt strategies on our benchmarks, we evaluated several approaches across our datasets and present the average performance of all models using different prompting techniques. Table~\ref{prompt} summarizes the results. For both Chinese and English datasets, Chinese LLMs performed best with the CN-CoT-Few-Shot strategy, followed closely by EN-CoT-Few-Shot, achieving overall scores of 67.36\% and 67.03\%, respectively. Conversely, English LLMs showed optimal performance with the EN-CoT-Few-Shot approach, attaining 67.55\% on the Chinese dataset and 60.36\% on the English dataset.
Besides, translating datasets into the model's native language before reasoning did not enhance performance. This phenomenon is further illustrated in Figure~\ref{xing}. Conducting CoT reasoning in the model's native language generally yields better results compared to direct reasoning. Furthermore, increasing the number of examples (shots) consistently boosts performance across most configurations, emphasizing the advantages of exposing models to multiple examples.
% Overall, the interaction between question language, prompt language, and the number of shots underscores the importance of aligning these factors to optimize task performance and robustness in LLMs.



% Please add the following required packages to your document preamble:
% \usepackage{multirow}
% Please add the following required packages to your document preamble:
% \usepackage{multirow}
\begin{table}[t]
\setlength{\tabcolsep}{8pt}
% \footnotesize
\scalebox{0.65}{
\begin{tabular}{c|l|lll}
\hline
\multicolumn{1}{l|}{Dataset}  & Prompt  & CN LLMs & EN LLMs &  LLMs \\ \hline
\multirow{7}{*}{\begin{tabular}[c]{@{}c@{}}Chinese\\ HellaSwag-Pro\end{tabular}} & Direct  & 48.95& 41.16& 45.06  \\
& CN-CoT-Few  & \textbf{71.04}& 51.90& 61.47  \\
& EN-CoT-Few  & 70.95& \textbf{67.55}& \textbf{69.25}  \\
& EN-XLT-Few  & 41.48& 28.69& 35.09  \\
& CN-CoT-Zero & 44.82& 23.89& 34.36  \\
& EN-CoT-Zero & 45.38& 31.39& 38.39  \\
& EN-XLT-Zero & 28.57& 12.93& 20.75  \\ \hline
\multirow{7}{*}{\begin{tabular}[c]{@{}c@{}}English\\ HellaSwag-Pro\end{tabular}} & Direct  & 47.46& 40.66& 44.06  \\
& CN-CoT-Few  & \textbf{63.67}& 47.24& 55.46  \\
& EN-CoT-Few  & 63.12& \textbf{60.36}& \textbf{61.74}  \\
& CN-XLT-Few  & 48.77& 16.61& 32.69  \\
& CN-CoT-Zero & 34.89& 18.25& 26.57  \\
& EN-CoT-Zero & 42.41& 31.03& 36.72  \\
& CN-XLT-Zero & 16.36& 11.22& 13.79  \\ \hline
\multirow{9}{*}{HellaSwag-Pro}& Direct  & 48.21& 40.91& 44.83  \\
& CN-CoT-Few  & \textbf{67.36}& 49.57& 58.46  \\
& EN-CoT-Few  & 67.03& \textbf{63.95}& \textbf{65.49}  \\
& CN-XLT-Few  & 59.91& 34.26& 47.08  \\
& EN-XLT-Few  & 52.30& 44.52& 48.41  \\
& CN-CoT-Zero & 39.86& 21.07& 30.46  \\
& EN-CoT-Zero & 43.90& 31.21& 37.55  \\
& CN-XLT-Zero & 30.59& 17.55& 24.07  \\
& EN-XLT-Zero & 35.49& 21.98& 28.74  \\ \hline
\end{tabular}
}
\caption{Average ARA of all open-source models on different prompts. CN-LLMs contains 17 LLMs, and EN-LLMs contains 7 LLMs. The bast results for each dataset are \textbf{bolded}.}
\label{prompt}
\end{table}



