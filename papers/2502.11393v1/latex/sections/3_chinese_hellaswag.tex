\section{Chinese HellaSwag} \label{sec:chinese_hellaswag}
To establish a bilingual benchmark for the robustness of commonsense reasoning, one of the primary challenges is the limited availability of Chinese datasets. To address this, we propose a two-stage approach to develop Chinese HellaSwag for Chinese commonsense reasoning.For each question, a model is provided with a context and need to select the most suitable one from four choices. Additionally, each question is annotated at a fine-grained level with its broad category, detailed subcategory and length, serving as supplementary structures for generalizability.


\subsection{Dataset Construction}
The construction of our dataset comprises two stages, as presented in Figure \ref{overview}: 
(1) \textit{Initial dataset generation}: An over-generate-then-filter (\citealp{yuan2023distilling}) approach is employed to automatically generate question-answer pairs, followed by manual quality control; 
(2) \textit{Difficult samples replacement}: We propose human-in-the-loop alternating adversarial filtering to add difficult samples. Specifically, four strong LLMs continuously alternate their roles as generator and discriminator to iteratively optimize the initial dataset, combined with human verification to progressively increase difficulty.


\begin{figure}[t]   
\centering
\setlength{\abovecaptionskip}{0.1cm}
\setlength{\belowcaptionskip}{0cm}
\includegraphics[width=\linewidth,scale=1.00]{images/overview.pdf}

\caption{The workflow of two stage to generate Chinese HellaSwag. Stage 1 includes steps 1-3 to generate initial samples, while Stage 2 includes step 4 to increase the number of difficult samples.}
\label{overview}
% \vspace{-15pt}
\end{figure}

\textbf{Initial dataset generation.} First, we identify seven broad categories of everyday commonsense based on existing literature (\citealp{zellers2019hellaswag};\citealp{koupaee2018wikihow};\citealp{caba2015activitynet}) and practical experience, each comprising eight detailed subcategories. As presented in Figure \ref{cn_hellaswag}, these categories encompass family, health, shopping, sociality, education, leisure, traffic, and work. To enhance the dataset's diversity, we further established three length tiers: short (under 20 characters), medium (20-40 characters), and long (over 40 characters).
The generation of the initial dataset consists of three steps:

\noindent %
\textbf{Step 1}: We employ the state-of-the-art Chinese LLM Qwen-Max (\citealp{yang2024qwen2}) to over-generate context by in-context learning (\citealp{brown2020language}), incorporating detailed category definition, length requirement, and carefully crafted 5-shot examples. We then filter this generated context based on character count and Jaccard similarity, eliminating samples that don't meet length criteria or exceeded the similarity threshold.

\noindent %
\textbf{Step 2}: We also utilize Qwen-Max to over-generate 10 potential choices for each context. 

\noindent %
\textbf{Step 3}: The model evaluates each question on a 10-point scale and selects six choices: one correct answer (10 points) and five high-scoring incorrect choices (e.g., 8, 7, 7, 6, 5 points). Human annotators then review and curate four choices for each context, ensuring a single correct answer while retaining plausible distractors. These annotators also assign category labels to each question.

Through model filtering, we initially generated 12,960 samples, which human annotators further refined to 12,287. To maintain category balance, we ultimately selected 12,000 samples, allocating 1,500 to each broad category.

%Firstly，we distill seven broad categories, each comprising eight detailed subcategories of everyday commonsense, based on existing literature and practical experience. As illustrated in Figure \ref{cn_hellaswag}, the seven broad categories include family, health, shopping, sociality, education, leisure, traffic and work. To enhance the dataset's diversity, we further design three length types: fewer than 20 characters, 20–40 characters, and more than 40 characters. 

%To ensure the quality of the dataset, we employ the powerful Chinese LLM Qwen-Max to generate the dataset. First, we assess the context length and calculated semantic similarity, removing samples that did not meet the length requirements or exceeded the similarity threshold.Next, we generated ten choices per question, with the model scoring and selecting six options, including one correct answer and five high-scoring incorrect alternatives. Further details are provided in the appendix. Finally, human annotators reviewed and selected four options per question, ensuring one unique correct answer while maintaining highly plausible incorrect choices. Annotators were also responsible for labeling the category tags. This process ultimately produced 12,000 initial samples.

\begin{figure}[t]   
\centering
\setlength{\abovecaptionskip}{0.1cm}
\setlength{\belowcaptionskip}{0cm}
\includegraphics[width=\linewidth,scale=0.95]{images/cn_hellaswag_v1.pdf}
\caption{Overview of Chinese HellaSwag categories. There are seven broad categories in total, each with eight detailed subcategories.}
\label{cn_hellaswag}
% \vspace{-15pt}
\end{figure}

\textbf{Difficult samples replacement.}
Inspired by the Adversarial Filtering (AF) (\citealp{zellers2018swag}), we introduce a human-in-the-loop alternating adversarial filtering method (\textbf{Step 4}) to further increase the dataset's difficulty. 
This approach involves a generator to generate more challenging wrong choices to replace easily identified wrong choices and multiple discriminators to identify wrong choices. 
Potential incorrect options are over-generated by the generator and then evaluated by a series of discriminators. We utilize four strong LLMs and alternate their roles as generator and discriminator. If a generated wrong choice successfully confuses the discriminators, it will replace the original corresponding choice. And human annotators then filter out wrong choices that are too difficult for humans to identify. This iterative process continues until Chinese Hellaswag achieves accuracy comparable to the English Hellaswag. At this stage, we replace the original samples with 2451 hard samples. Additional details on human annotation can be found in the Appendix.

\subsection{Dataset Statistics}

Table \ref{tab:statistics} presents the statistics of Chinese Hellaswag, with eight broad categories and 1,500 in each category. This dataset possesses the following characteristics:

\noindent %
\textbf{- Chinese focus:} Chinese Hellaswag concentrates on the Chinese language, representing the first Chinese daily commonsense reasoning dataset. It can provide a comprehensive evaluation of existing language models' Chinese commonsense reasoning capabilities.

\noindent %
\textbf{- Diversity:} The dataset covers eight main categories, each with seven detailed subcategories, totaling 56 categories. It also incorporates three length categories, demonstrating its diversity.

\noindent %
\textbf{- High quality:} Through the two-stage approach, we implement comprehensive supervision in dataset construction to ensure fine-grained annotation and high discrimination of the dataset.


% \begin{table}[hb]
% \begin{tabular}{l|llll}
% \hline
% Length Type & Long & Medium & Short & Total \\ \hline
% \#Question  & 4,179 & 4,033   & 3,788  & 12,000 \\ \hline
% \end{tabular}
% \caption{Statistics for Chinese HellaSwag.}
% \label{statistics}
% \end{table}

\begin{table}[hb]
    \centering
\setlength{\abovecaptionskip}{0.1cm}
\setlength{\belowcaptionskip}{0cm}
\resizebox{0.9\linewidth}{!}{
\begin{tabular}{l|llll}
\toprule
Length Type  & Long  & Medium & Short & Total  \\
\midrule
\# Questions & 4,179 & 4,033  & 3,788 & 12,000 \\
\bottomrule
\end{tabular} }
    \caption{Statistics for Chinese HellaSwag.}
    \label{tab:statistics}
\end{table}


