\section{Introduction}

\textit{``The measure of intelligence is the ability to change.''}

\textit{\hfill—— Albert Einstein}

Commonsense reasoning is a crucial part of intelligence, involving contextual understanding, implicit knowledge, and logical deduction (\citealp{liu2004conceptnet, cambria2011isanette, davis2015commonsense}). 
Recent studies have focused on enhancing these capabilities in LLMs, achieving impressive performance (\citealp{yang2024qwen2, openai2024gpt, team2024gemini}). However, even slight changes to questions can lead to incorrect responses from the same models.
%However, these models remain sensitive to changes in question expression. 
%if human can identify the correct option, they can recognize the other as incorrect naturally. In contrast, while LLMs can identify the correct answer, they struggle to directly reason about why the alternative is wrong ~\cite{balepur2024s}. 
For instance, in binary commonsense questions, human naturally recognizes both correct and incorrect options through a single inference process, while LLMs, though able to identify the correct answer, struggle to reason about why the alternative is wrong ~\cite{balepur2024s}. 
Therefore, we ask the question:
%Recent research has focused on enhancing these capabilities in large language models (LLMs) (\citealp{yang2024qwen2, openai2024gpt, team2024gemini}).
%In particular, both closed-source LLMs like GPT-4o \cite{DBLP:journals/corr/abs-2303-08774} and open-source LLMs such as Qwen2.5-72B \cite{yang2024qwen2} have reported near 90\% precision in HellaSwag~\cite{zellers2019hellaswag}. 
%However, this raises an important question: 
\textit{Does this high-level performance stem from a genuine understanding of commonsense knowledge, or is it simply a result of memorizing specific expression patterns in pre-training data?}


\begin{figure}[t]   
\centering
\setlength{\abovecaptionskip}{-0.10cm}
\setlength{\belowcaptionskip}{0cm}
\includegraphics[width=\linewidth,scale=1.00]{images/introduction.pdf}
% \caption{The difference between our work and existing works. Traditional evaluation has reached almost saturation performance, but our evaluation launches new challenges for the mathematical reasoning ability.}
    %\vspace{-10pt}
\caption{Comparison of GPT-4o's responses to an original question and its several meaning-preserving variants. GPT-4o successfully handles the original question but struggles with its variants on the same knowledge.}
\label{example}
\vspace{-10pt}
\end{figure}

% Despite the outstanding performance, current LLMs still heavily rely on memorizing superficial patterns, which has not been fully investigated by existing studies. 
To answer this question, an effective approach is to systematically evaluate the robustness of LLMs in answering commonsense reasoning questions. 
As illustrated in Figure~\ref{example}, we find that GPT-4o correctly answers an original question but fails on its variants, \ie questions about the same commonsense knowledge but in different reasoning forms, such as reverse conversion. 
This indicates that GPT-4o has not fully grasped the commonsense knowledge behind the question; a genuine understanding of commonsense knowledge should be able to generalize to these question variants. 
% As shown in Figure~\ref{example}, GPT-4o can correctly answer the original question but fails on its variants, 
% \ie questions about the same commonsense knowledge but different forms of reasoning. If an LLM can correctly respond to various variants (\ie is robust to different forms of reasoning), it demonstrates a true understanding of the knowledge rather than merely memorizing superficial patterns. 


However, existing benchmarks do not yet support a thorough evaluation of LLM robustness in commonsense reasoning.
Most work evaluates LLMs on general benchmarks~\cite{zellers2019hellaswag, talmor2019commonsenseqa, mihaylov2018can}, or in specific domains of commonsense knowledge~\cite{zhou2019going, qin2021timedial, bisk2020piqa}. Although some efforts have considered the robustness of commonsense reasoning, they either focus on whether models can learn genuine question-answer correlations under initial questions~\cite{jia2017adversarial,branco2021shortcutted}, or examine only one type of simplistic question variant such as question paraphrasing~\cite{zhou2021rica,ismayilzada-etal-2023-crow,balepur2024s}, lacking investigation into robustness across diverse and complex variants. 
%they focus on only one type of simplistic question variant such as question paraphrasing, lacking investigation into robustness across diverse and complex variants. 
% , or address overly simplistic scenarios.

% Existing research mainly evaluates LLMs on general benchmarks~\cite{zellers2019hellaswag, talmor2019commonsenseqa, mihaylov2018can}, 
% or specific domains of commonsense knowledge \cite{zhou2019going, qin2021timedial, bisk2020piqa}. 
% Although some efforts have been made in examining the robustness of commonsense reasoning (\citealp{zhou2021rica},\citealp{balepur2024s}), they focus on a single method without comparison across variants, or deal with overly simplistic scenarios. 

% To address this gap, we present the first comprehensive investigation on the robustness of commonsense reasoning, aiming to systematically assess the genuine capabilities of LLMs in this area. 

To address this gap, we present the first extensive evaluation on the robustness of commonsense reasoning for LLMs, starting with dataset construction. 
% First, inspired by Bloom Cognitive Model~\cite{krathwohl1973taxonomy}, we design and collect seven variants: problem restatement, reverse conversion, causal inference, sentence ordering, scenario refinement, negative transformation, and critical testing. Details are provided in Table \ref{overview}. 
Firstly, recognizing that existing benchmarks are predominantly in English, which limits the assessment of non-English LLMs (\citealp{davis2023benchmarks}), we develop a Chinese commonsense reasoning dataset based on the widely-used HellaSwag benchmark~\cite{zellers2019hellaswag}, containing 12,000 questions. Specifically, we design 56 fine-grained categories, and propose a two-stage data annotation method including initial dataset generation and difficult sample replacement. 
Secondly, we design and compile seven variants from existing studies (\cf Table~\ref{variant type}), which can be characterized under Bloom Cognitive Model (\cf Appendix ~\ref{bloom_model}).
% Specifically, we meticulously designed 56 fine-grained categories and 3 length types for Chinese commonsense reasoning. Then, we propose a two-stage data annotation method, involving initial dataset generation followed by difficult sample replacement. This approach generated 12,000 high-quality, finely-annotated multiple-choice problems.
We then create the variants for the Chinese and English versions of HellaSwag, obtaining HellaSwag-Pro, a high-quality human-verified dataset with 11,200 variants from 1,600 original questions. 
% Finally, building on both Chinese and English versions of HellaSwag, we employed an over-generate-then-filter method to create HellaSwag-Pro. It contains 11,200 high-quality human-verified data entries, starting with 1,600 initial bilingual entries.

Using HellaSwag-Pro, we conduct a comprehensive evaluation on the robustness of 41 closed-source and open-source LLMs with nine different prompt strategies. 
We derive several key findings:
\textbf{(1)} 
% All LLMs are far from robust in commonsense reasoning tasks. 
All LLMs are far from robust in commonsense reasoning tasks, as evidenced by their poor performance on question variants and the significant gap compared to human performance. 
Nevertheless, GPT-4o achieves the best robustness among all the evaluated LLMs. 
\textbf{(2)} Among all types of variants, negative transformation is the most challenging, with an average accuracy of only 9.01\%, while problem restatement poses minimal difficulty. 
% little challenge. 
\textbf{(3)} LLMs achieve the best robustness in the language on which they were adequately trained. 
\textbf{(4)} Incorporating chain-of-thought (CoT) reasoning and using few-shot demonstrations can strengthen their robustness. 

% 1) All models are far from robust in commonsense reasoning tasks and fail to reach human-level performance. Even when they can solve original problems correctly, variations still pose significant challenges. Overall, among all models evaluated, GPT-4o performs best on our bilingual robustness benchmark, while the robustness of open-source models aligns with the scaling law.
% 2) Among all variations, negative transformation proves to be the most challenging category, with an average accuracy of only 9.01\% across all models. However, problem restatement no longer poses a challenge to the models.
% 3) Our evaluation of bilingual prompting strategies demonstrates that models perform better on evaluation datasets and prompts in the language they were trained on. Using a non-training language for reasoning can even impair model performance. Incorporating CoT and increasing the number of shots can enhance the models' robustness performance.

Our contributions are three-fold. 
\textbf{(1)} We present the first extensive evaluation on the robustness of commonsense reasoning for LLMs by designing and compiling seven types of variants. 
\textbf{(2)} We have developed a bilingual, large-scale, human-annotated benchmark for evaluating LLM robustness in commonsense reasoning, which will be publicly released upon acceptance. 
% , and a finely-annotated Chinese commonsense reasoning dataset. 
\textbf{(3)} We conduct in-depth experiments on 41 representative LLMs with diverse prompts, yielding critical insights. 

% \textbf{(1)} We present the first comprehensive investigation for evaluating the robustness of commonsense reasoning in both Chinese and English, advancing the assessment of LLMs' capabilities towards more granular examination.
% \textbf{(1)} We develop a bilingual large-scale benchmark for robustness evaluation of commonsense reasoning. To build this, we also propose a two-stage method to create a finely-annotated Chinese commonsense reasoning dataset. 
% \textbf{(1)} We conduct extensive experiments on 41 LLMs, yielding critical insights. 
% We aim to catalyze further advancements in LLMs' commonsense reasoning capabilities through this work.



