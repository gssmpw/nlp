\section{Experiment}
In this section, we conduct extensive experiments to evaluate the performance of various LLMs on our HellaSwag-Pro benchmark. Our study is guided by three key research questions:
\textbf{RQ1}: How do different LLMs perform across all variants?
\textbf{RQ2}: What is the relative difficulty of different variants?
\textbf{RQ3}: Which prompting strategies yield the best robustness in LLMs?

\subsection{Experimental Setup} 
\paragraph{Model Selection and Implementation Details}
We select 41 representative closed-source and open-source LLMs. 
For English LLMs, we use GPT-4o \cite{DBLP:journals/corr/abs-2303-08774}, Claude-3.5-Sonnet \cite{anthropic2024claude}, Gemini-1.5-Pro \cite{DBLP:journals/corr/abs-2312-11805}, Mistral series \cite{DBLP:journals/corr/abs-2310-06825}, Llama3 series \cite{DBLP:journals/corr/abs-2407-21783} and Gemma2 series \cite{riviere2024gemma}. 
For Chinese LLMs, we use Qwen-Max \cite{qwen}, Qwen2.5 series \cite{yang2024qwen2}, InternLM2.5 series \cite{2023internlm}, Yi1.5 series \cite{DBLP:journals/corr/abs-2403-04652}, Baichuan2 series \cite{DBLP:journals/corr/abs-2309-10305} and DeepSeek series \cite{DBLP:journals/corr/abs-2401-02954}. 

We integrate both the Chinese HellaSwag and HellaSwag-Pro into the \texttt{lm-evaluation-harness} platform \cite{eval-harness}. For the open-source models, we use the default settings of the platform: do\_sample is set to false and the temperature is set to the default value of the \texttt{hugging-face} library as 1.0. For the closed-source models, we set the temperature to 0.7. In addition, we set the maximum output length to 1024.

\paragraph{Prompting Strategy} \label{sec:prompting_strategy}
We design nine prompting strategies to evaluate the LLMs across different languages and number of demonstrations. 
\textbf {(1) Direct}: LLM takes the original dataset question directly as input\footnote{
For open-source models, the \textbf{Direct} approach follows the official HellaSwag implementation, computing the log-likelihood for each option and selecting the one with the highest value. We report the normalized accuracy to account for the option length. Other prompting strategies use a generation setup and report accuracy based on exact match.}.
\textbf{(2) CN-CoT}: LLM is instructed to perform CoT in Chinese, regardless of the language of the dataset.
\textbf{(3) EN-CoT}: LLM is instructed to perform CoT in English. 
\textbf{(4) CN-XLT}: LLM is instructed to first translate the English question into Chinese, then reason in Chinese.
\textbf{(5) EN-XLT}: LLM is instructed to first translate the Chinese question into English, then reason in English.
The last four strategies include both zero-shot and three-shot variants.

%\textbf {CN-CoT}: LLMs perform Chinese reasoning and then output the answer and 3 shots are provided.
%\textbf {CN-CoT}: Similar as CNCoTFewShot without any shots.
%\textbf {EN-CoT}: The reasoning process in English is executed and then the answer is output and 3 shots are provided.
%\textbf {CN-XLT}: Inspired by this, we instruct LLMs to translate questions in Chinese and then output the answer after performing reasoning in Chinese too. And 3 shots are provided.
%\textbf {EN-XLT}: Inspired by this, we instruct LLMs to translate questions in Englsih and then output the answer after performing reasoning in Englsih too. Three shots are provided.
\begin{table*}[ht]
\centering
\setlength{\abovecaptionskip}{0.05cm}
\setlength{\belowcaptionskip}{0cm}
\setlength{\tabcolsep}{2pt}
% \footnotesize
\scalebox{0.67}{
% Please add the following required packages to your document preamble:
% \usepackage{multirow}
% \usepackage[table,xcdraw]{xcolor}
% Beamer presentation requires \usepackage{colortbl} instead of \usepackage[table,xcdraw]{xcolor}
% Please add the following required packages to your document preamble:
% \usepackage{multirow}
% \usepackage[table,xcdraw]{xcolor}
% Beamer presentation requires \usepackage{colortbl} instead of \usepackage[table,xcdraw]{xcolor}
\begin{tabular}{ccccccccccccc}
\hline
\multicolumn{1}{c|}{{ }}& \multicolumn{4}{c|}{Chinese}& \multicolumn{4}{c|}{English}& \multicolumn{4}{c}{AVG}\\ \cline{2-13} 
\multicolumn{1}{c|}{\multirow{-2}{*}{{ Model}}} & { OA(\%)$\uparrow$}& { ARA(\%)$\uparrow$} & {RLA(\%)$\downarrow$}& \multicolumn{1}{l|}{{CRA(\%)$\uparrow$}} & { OA(\%)$\uparrow$}& { ARA(\%)$\uparrow$} & { RLA(\%)$\downarrow$}& \multicolumn{1}{l|}{{CRA(\%)$\uparrow$}} & {OA(\%)$\uparrow$}& { ARA(\%)$\uparrow$} & {RLA(\%)$\downarrow$}& { CRA(\%)$\uparrow$} \\ \hline
\multicolumn{1}{c|}{{ Human}} & 96.41& 97.79& -1.38 & \multicolumn{1}{l|}{92.03}& 95.56& 96.04& -0.48 & \multicolumn{1}{l|}{90.02}& 95.99 & 96.92 & -0.93& 91.03 \\ 
\multicolumn{1}{c|}{{ Random}} & 25.00& 25.00& 0.00 & \multicolumn{1}{l|}{0.0015}& 25.00& 25.00& 0.00 & \multicolumn{1}{l|}{0.0015}& 25.00& 25.00& 0.00 & 0.0015 \\ \hline
\multicolumn{13}{c}{\textbf{\textit{Closed-source LLMs}}}\\ 
\multicolumn{1}{c|}{{ Qwen-Max}}& { 93.50} & { 84.82} & { 8.68}& \multicolumn{1}{l|}{{ 78.91}} & { 87.60} & { 62.61} & { 24.99} & \multicolumn{1}{l|}{{ 59.65}} & { 90.55} & { 73.72} & { 16.83} & { 69.28} \\ \hline
\multicolumn{13}{c}{\textbf{\textit{Open-source LLMs}}} \\ 
\multicolumn{1}{c|}{{ Qwen2.5-0.5B}}& { 60.75} & { 45.18} & { \textbf{15.57}} & \multicolumn{1}{l|}{{ 28.70}} & { 49.50} & { 38.21} & { \textbf{11.29}} & \multicolumn{1}{l|}{{ 20.57}} & { 55.13} & { 41.70} & { \textbf{13.43}} & { 24.64} \\
\multicolumn{1}{c|}{{ Qwen2.5-1.5B}}& { 63.25} & { 46.16} & { 17.09} & \multicolumn{1}{l|}{{ 29.89}} & { 56.88} & { 39.57} & { 17.30} & \multicolumn{1}{l|}{{ 23.48}} & { 60.06} & { 42.87} & { 17.20} & { 26.69} \\
\multicolumn{1}{c|}{{ Qwen2.5-3B}}& { 67.50} & { 48.75} & { 18.75} & \multicolumn{1}{l|}{{ 33.79}} & { 61.75} & { 39.98} & { 21.77} & \multicolumn{1}{l|}{{ 25.75}} & { 64.63} & { 44.37} & { 20.26} & { 29.77} \\
\multicolumn{1}{c|}{{ Qwen2.5-7B}}& { 67.63} & { 50.59} & { 17.04} & \multicolumn{1}{l|}{{ 35.62}} & { 65.63} & { 43.93} & { 21.70} & \multicolumn{1}{l|}{{ 30.77}} & { 66.63} & { 47.26} & { 19.37} & { 33.20} \\
\multicolumn{1}{c|}{{ Qwen2.5-14B}} & { 69.00} & { 51.41} & { 17.59} & \multicolumn{1}{l|}{{ 35.84}} & { 68.50} & { 45.20} & { 23.30} & \multicolumn{1}{l|}{{ 32.12}} & { 68.75} & { 48.30} & { 20.45} & { 33.98} \\
\multicolumn{1}{c|}{{ Qwen2.5-32B}} & { 69.75} & { 53.11} & { 16.64} & \multicolumn{1}{l|}{{ 37.54}} & { 70.00} & { 46.10} & { 23.90} & \multicolumn{1}{l|}{{ 32.68}} & { 69.88} & { 49.61} & { 20.27} & { 35.11} \\
\multicolumn{1}{c|}{{ Qwen2.5-72B}} & { 70.87} & { \textbf{54.75}} & { 16.12} & \multicolumn{1}{l|}{{ \textbf{39.64}}} & {{ 72.00}} & { \textbf{47.75}} & { 24.25} & \multicolumn{1}{l|}{{\textbf{ 35.12}}} & { \textbf{71.44}} & { \textbf{51.25}} & { 20.19} & { \textbf{37.38}} \\ \hdashline[0.5pt/5pt]
\multicolumn{1}{c|}{{ Baichuan2-7B}}& { 67.00} & { 46.16} & { 20.84} & \multicolumn{1}{l|}{{ 31.50}} & { 60.62} & { 39.04} & { 21.58} & \multicolumn{1}{l|}{{ 25.21}} & { 63.81} & { 42.60} & { 21.21} & { 28.36} \\
\multicolumn{1}{c|}{{ Baichua2-13B}}& { 69.13} & { 46.98} & { 22.15} & \multicolumn{1}{l|}{{ 33.45}} & { 64.62} & { 38.82} & { 25.80} & \multicolumn{1}{l|}{{ 26.07}} & { 66.88} & { 42.90} & { 23.97} & { 29.76} \\ \hdashline[0.5pt/5pt]
\multicolumn{1}{c|}{{ DeepSeek-7B}} & { 68.13} & { 47.96} & { 20.17} & \multicolumn{1}{l|}{{ 33.30}} & { 63.38} & { 40.39} & { 22.99} & \multicolumn{1}{l|}{{ 26.70}} & { 65.76} & { 44.18} & { 21.58} & { 30.00} \\
\multicolumn{1}{c|}{{ DeepSeek-67B}}& { \textbf{71.50}} & { 49.21} & { 22.29} & \multicolumn{1}{l|}{{ 35.89}} & { 71.37} & { 40.63} & { 30.75} & \multicolumn{1}{l|}{{ 29.71}} & \textbf{ 71.44} & { 44.92} & { 26.52} & { 32.80} \\ \hdashline[0.5pt/5pt]
\multicolumn{1}{c|}{{ InternLM2.5-1.8B}}& { 61.62} & { 42.07} & { 19.55} & \multicolumn{1}{l|}{{ 26.99}} & { 55.37} & { 38.46} & { 16.91} & \multicolumn{1}{l|}{{ 22.61}} & { 58.50} & { 40.27} & { 18.23} & { 24.80} \\
\multicolumn{1}{c|}{{ InternLM2.5-7B}}& { 67.25} & { 49.77} & { 17.48} & \multicolumn{1}{l|}{{ 34.57}} & { 69.50} & { 40.89} & { 28.61} & \multicolumn{1}{l|}{{ 29.75}} & { 68.38} & { 45.33} & { 23.04} & { 32.16} \\
\multicolumn{1}{c|}{{ InternLM2.5-20B}} & { 67.37} & { 48.08} & { 19.29} & \multicolumn{1}{l|}{{ 33.21}} & \textbf{ 73.62} & { 41.11} & { 32.51} & \multicolumn{1}{l|}{{ 31.23}} & { 70.50} & { 44.60} & { 25.90} & { 32.22} \\ \hdashline[0.5pt/5pt]
\multicolumn{1}{c|}{{ Yi1.5-6B}} & { 67.00} & { 49.59} & { 17.41} & \multicolumn{1}{l|}{{ 34.27}} & { 64.38} & { 39.37} & { 25.01} & \multicolumn{1}{l|}{{ 26.62}} & { 65.69} & { 44.48} & { 21.21} & { 30.45} \\
\multicolumn{1}{c|}{{ Yi1.5-9B}} & { 68.50} & { 50.18} & { 18.32} & \multicolumn{1}{l|}{{ 35.55}} & { 66.37} & { 39.58} & { 26.79} & \multicolumn{1}{l|}{{ 27.48}} & { 67.44} & { 44.88} & { 22.56} & { 31.52} \\
\multicolumn{1}{c|}{{ Yi1.5-34B}}& { 71.00} & { 52.23} & { 18.77} & \multicolumn{1}{l|}{{ 38.09}} & { 71.00} & { 40.75} & { 30.25} & \multicolumn{1}{l|}{{ 29.91}} & { 71.00} & { 46.49} & { 24.51} & { 34.00} \\ \hline
\end{tabular}
}
\caption{Results of existing \textbf{Chinese} LLMs on HellaSwag-Pro using \textbf{Direct} prompt. ``AVG'' indicates the average performance on Chinese and English parts of the dataset. 
The best results in each model category are \textbf{bolded}. }
\label{tab:chinese main experiment.}
\vspace{-10pt}
\end{table*}



\begin{table*}[ht]
\centering
\setlength{\abovecaptionskip}{0.05cm}
\setlength{\belowcaptionskip}{0cm}
\setlength{\tabcolsep}{2pt}
% \footnotesize
\scalebox{0.67}{
% Please add the following required packages to your document preamble:
% \usepackage{multirow}
% \usepackage[table,xcdraw]{xcolor}
% Beamer presentation requires \usepackage{colortbl} instead of \usepackage[table,xcdraw]{xcolor}
% Please add the following required packages to your document preamble:
% \usepackage{multirow}
% \usepackage[table,xcdraw]{xcolor}
% Beamer presentation requires \usepackage{colortbl} instead of \usepackage[table,xcdraw]{xcolor}
\begin{tabular}{ccccccccccccc}
\hline
\multicolumn{1}{c|}{{ }}& \multicolumn{4}{c|}{Chinese}& \multicolumn{4}{c|}{English}& \multicolumn{4}{c}{AVG}\\ \cline{2-13} 
\multicolumn{1}{c|}{\multirow{-2}{*}{{ Model}}} & { OA(\%)$\uparrow$}& { ARA(\%)$\uparrow$} & {RLA(\%)$\downarrow$}& \multicolumn{1}{l|}{{CRA(\%)$\uparrow$}} & { OA(\%)$\uparrow$}& { ARA(\%)$\uparrow$} & { RLA(\%)$\downarrow$}& \multicolumn{1}{l|}{{CRA(\%)$\uparrow$}} & {OA(\%)$\uparrow$}& { ARA(\%)$\uparrow$} & {RLA(\%)$\downarrow$}& { CRA(\%)$\uparrow$} \\ \hline
\multicolumn{13}{c}{\textbf{\textit{Closed-source LLMs}}}\\ 
\multicolumn{1}{c|}{{ GPT-4o}}& { 91.37} & { \textbf{81.97}} & { \textbf{9.40}}& \multicolumn{1}{l|}{{ \textbf{75.55}}} & { \textbf{88.63}} & { \textbf{70.17}} & { \textbf{18.46}} & \multicolumn{1}{l|}{{ \textbf{63.06}}} & { 90.00} & { \textbf{76.07}} & { \textbf{13.93}} & { \textbf{69.31}} \\
\multicolumn{1}{c|}{{ Claude-3.5}}& { \textbf{95.37}} & { 80.15} & { 15.22} & \multicolumn{1}{l|}{{ 75.04}} & { 85.11} & { 66.02} & { 19.08} & \multicolumn{1}{l|}{{ 57.20}} & {\textbf{ 90.24}} & { 73.09} & { 17.15} & { 66.12} \\
\multicolumn{1}{c|}{{ Gemini-1.5-Pro}}& { 90.62} & { 78.36} & { 12.26} & \multicolumn{1}{l|}{{ 70.48}} & { 87.75} & { 60.74} & { 27.01} & \multicolumn{1}{l|}{{ 58.27}} & { 89.19} & { 69.55} & { 19.63} & { 64.38} \\
\hline
\multicolumn{13}{c}{\textbf{\textit{Open-source LLMs}}} \\ 
\multicolumn{1}{c|}{{ Llama3-8B}} & { 59.13} & { 46.62} & { 12.51} & \multicolumn{1}{l|}{{ 28.23}} & { 66.25} & { 40.21} & { 26.04} & \multicolumn{1}{l|}{{ 27.34}} & { 62.69} & { 43.42} & { 19.27} & { 27.79} \\
\multicolumn{1}{c|}{{ Llama3-70B}}& { 65.75} & { 48.63} & { 17.12} & \multicolumn{1}{l|}{{ 32.70}} & { \textbf{72.50}} & { 41.27} & { 31.23} & \multicolumn{1}{l|}{{\textbf{ 30.63}}} & {\textbf{ 69.13}} & { 44.95} & { 24.18} & { 31.67} \\ \hdashline[0.5pt/5pt]
\multicolumn{1}{c|}{{ Mistral-7B-v0.1}} & { 57.75} & { 46.25} & { \textbf{11.50}} & \multicolumn{1}{l|}{{ 27.57}} & { 67.50} & { \textbf{41.52}} & { 25.98} & \multicolumn{1}{l|}{{ 28.93}} & { 62.63} & { 43.88} & { 18.74} & { 28.25} \\
\multicolumn{1}{c|}{{ Mixtral-8x7B-v0.1}} & { 63.62} & { 46.80} & { 16.82} & \multicolumn{1}{l|}{{ 30.82}} & { 69.75} & { 41.21} & { 28.54} & \multicolumn{1}{l|}{{ 29.39}} & { 66.69} & { 44.01} & { 22.68} & { 30.11} \\
\multicolumn{1}{c|}{{ Mixtral-8x22B-v0.1}}& { 66.00} & {\textbf{ 50.73}} & { 15.27} & \multicolumn{1}{l|}{{ \textbf{34.32}}} & { 72.12} & { 41.25} & { 30.87} & \multicolumn{1}{l|}{{ 30.61}} & { 69.06} & { \textbf{45.99}} & { 23.07} & { \textbf{32.47}} \\ \hdashline[0.5pt/5pt]
\multicolumn{1}{c|}{{ Gemma2-2B}}& { 61.88} & { 45.38} & { 16.51} & \multicolumn{1}{l|}{{ 29.02}} & { 59.62} & { 39.13} & { \textbf{20.50}} & \multicolumn{1}{l|}{{ 24.88}} & { 60.75} & { 42.25} & {\textbf{ 18.50}} & { 26.95} \\
\multicolumn{1}{c|}{{ Gemma2-9B}}& { \textbf{69.13}} & { 46.75} & { 22.38} & \multicolumn{1}{l|}{{ 33.29}} & { 64.88} & { 39.80} & { 25.08} & \multicolumn{1}{l|}{{ 26.91}} & { 67.01} & { 43.28} & { 23.73} & { 30.10} \\
\multicolumn{1}{c|}{{ Gemma2-27B}} & { 63.38} & { 48.52} & { 14.86} & \multicolumn{1}{l|}{{ 31.96}} & { 71.88} & { 40.91} & { 30.97} & \multicolumn{1}{l|}{{ 30.25}} & { 67.63} & { 44.71} & { 22.92} & { 31.11} \\ \hline
\end{tabular}
}
\caption{Results of existing \textbf{English} LLMs on HellaSwag-Pro using \textbf{Direct} prompt (Same settings as Table \ref{tab:chinese main experiment.}).  }
\label{tab:english main experiment.}
\vspace{-10pt}
\end{table*}
\paragraph{Evaluation Metric}
We consider four evaluation metrics to measure the performance and robustness of LLMs. 
Denote the original dataset $\mathcal{D} = \{(x, y)\}$, where $x$ and $y$ represent the question and the correct label, respectively. 
Denote the dataset of all seven-type variants $\mathcal{D}_r = \{(x', y')\}$, where each $(x', y')$ corresponds to an original $(x, y)$ in $\mathcal{D}$.  

\noindent %
\textbf{Original Accuracy (OA)} measures the accuracy on original questions.
\begin{equation}
\small
\label{eq1}
OA=\frac{\sum_{(x, y) \in \mathcal{D}} \mathds{1}[\text{LM}(x), y]}{|\mathcal{D}|}.
\end{equation}
\noindent %
\textbf{Average Robust Accuracy (ARA)} measures the average accuracy across all variants.
\begin{equation}
\small
\label{eq2}
ARA=\frac{\sum_{\left(x^{\prime}, y^{\prime}\right) \in \mathcal{D}_{r}} \mathds{1}\left[\text{LM}(x'), y'\right].}{\left|\mathcal{D}_{r}\right|}.
\end{equation}

\noindent %
\textbf{Robust Loss Accuracy (RLA)} refers to the performance gap between all variants and original questions, \ie the difference between OA and ARA. 
%\begin{tiny}
%\begin{equation}\label{eq3}
%RLA=\frac{\sum_{\left(x^{\prime}, y^{\prime}\right) \in D_{R}} %\mathds{1}\left(L M\left(x^{\prime}, y^{\prime}\right)\right.}{\left|D_{R}\right|}-\frac{\sum_{(x, y) \in D}\mathds{1}[L M(x), y]}{|D|}
%\end{equation}
%\end{tiny}
\begin{equation}
\small
\label{eq3}
RLA= OA - ARA.
\end{equation}
\noindent %
\textbf{Consistent Robust Accuracy (CRA)} refers to the joint accuracy of LLM correctly answering the variant and its original question, reflecting the LLM's genuine understanding of the knowledge. 
% consistency in problem-solving.
\begin{equation}
\small
\label{eq4}
CRA=\frac{\sum_{(x^{\prime}, y^{\prime}) \in \mathcal{D}_r}\mathds{1}[\text{LM}(x), y] \cdot \mathds{1}[\text{LM}(x^{\prime}), y^{\prime}]}{\left|\mathcal{D}_{r}\right|}.
\end{equation}




\subsection{LLM Performance (RQ1)}
\paragraph{Overall Performance}
The results for \textbf{Direct} prompting on all LLMs are listed in Table~\ref{tab:chinese main experiment.} and Table~\ref{tab:english main experiment.}\footnote{The results of instruct and chat models of Qwen2.5, LLaMA3 and Mixtral\_v0.1 series are shown in Appendix~\ref{detailed}.}. The main observations are as follows.

Firstly, all evaluated LLMs perform well in OA (e.g., in AVG OA, GPT-4o scores 90.00, and Claude-3.5 scores 90.24). 
However, all LLMs show a performance drop on variants, as evidenced by a positive AVG RLA value for all LLMs. 
In contrast, human receive a near-zero RLA value, suggesting that the question variants are not more challenging than the originals for human. 
This disparity further illustrates that current LLMs lack a true understanding of the commonsense knowledge and can easily be affected by the reasoning form. 

Secondly, comparing open-source and closed-source LLMs, closed-source models achieve larger OA, ARA and CRA scores and smaller average RLA scores than open-source LLMs, indicating better robustness in commonsense reasoning. 

Finally, when we compare models within the same series (e.g., Qwen2.5, Llama3), we observe that larger models often achieve higher scores on OA, ARA, and CRA. 
However, their RLA shows no consistent relationship with model size. Across different families, AVG RLA patterns vary - fluctuating with size in Qwen2.5 and Gemma3, while increasing with size in Yi1.5 and Llama3.
This indicates that larger model size does not guarantee better robustness.
%, possibly related to inverse scaling effects~\cite{mckenzie2023inverse}.

%However, they are also more susceptible to variations, \ie they have higher RLA values, a phenomenon particularly evident in English datasets. We attribute this phenomenon to the fact that larger LLMs may have memorized more data than smaller LLMs~\cite{DBLP:conf/iclr/CarliniIJLTZ23}, allowing them to rely on memorization to solve some problems more easily and making them more prone to the influence of variations. 
% \begin{itemize}[leftmargin=*,topsep=0pt]
% % \setlength{}{0}
%  \item All evaluated LLMs performed well in OA (e.g., in AVG OA, GPT-4o scored 90.00, and Claude3.5 scored 90.24). 
%  However, all LLMs show a performance drop on variants, as evidenced by a positive AVG RLA value for all LLMs. 
%  In contrast, human receive a negative RLA value, suggesting that the question variants are not more challenging than the originals for human. 
%  This disparity further illustrates that current LLMs lack a true understanding of the commonsense knowledge and can easily be affected by the reasoning form. 
%  \item Comparing open-source and closed-source LLMs, the closed-source models demonstrate larger OA and ARA scores than open-source LLMs. 
%  The RLA values for closed-source LLMs are also generally smaller, indicating that they are more robust in commonsense reasoning than open-source LLMs. 
%  \item When we compare models within the same series (e.g., Qwen2.5, Llama3), we observe that larger models often achieve higher scores on OA, ARA, and CRA. However, they are also more susceptible to variations, \ie they have higher RLA values, a phenomenon particularly evident in English datasets. We attribute this phenomenon to the fact that larger LLMs may have memorized more data than smaller LLMs~\cite{DBLP:conf/iclr/CarliniIJLTZ23}, allowing them to rely on memorization to solve some problems more easily and making them more prone to the influence of variations. 
% \end{itemize}

% 1. When evaluating all available models, We find although 
% 2. When comparing the opensource LLMs and close source LLMs, 
% 3. When looking into each serious details
% \noindent
% \textbf{Overall Model Performance.}
% 1. close-source > open-source 2. the large the better 3. all have a performance decline when meeting varients.

% To evaluate the performance of various models, we observed patterns consistent with current mainstream trends: closed-source models generally outperform open-source models across metrics. 
% For instance, the closed-source model GPT-4o achieved scores of 90.00 in OA, 76.07 in ARA, and 69.31 in CRA, whereas the open-source model Qwen2.5-72B scored 71.44, 51.25, and 37.38, respectively. 
% Furthermore, within each model series, performance tends to improve with larger model sizes. 
% Nevertheless, even the strongest closed-source models struggle with variations in questions, as indicated by positive values in RLA for all models. In contrast, human performance yields a negative RLA value, highlighting that current LLMs do not genuinely grasp the reasoning process and are prone to falling into traps set by question variants. 
% This suggests that there is still significant room for improvement in developing models that can robustly understand and reason through complex linguistic challenges.
% It reveals a consistent pattern across Chinese, English, and average scores, with close-sourced LLMs generally outperforming open-sourced models. 
% However, all models exhibit a significant drop in performance when faced with robust variants, as indicated by RLA and CRA. Among closed-source models, GPT-4o demonstrates the highest ARA of 76.07\% in average scores, demonstrating its overwhelming superiority. Among open-sourced models, larger models tend to perform better, with Qwen2.5-72B achieving the highest OA (71.44\%) and ARA (51.25\%) in the average scores. However, even these top performers still struggle with robustness, as evidenced by the substantial RLA of 13.93\% for GPT-4o and 20.19\% for Qwen2.5-72B. Interestingly, some English open-sourced models, such as Llama3-70B and Mixtral-8x22B-v0.1, show competitive performance in English tasks but lag in Chinese tasks, highlighting the importance of language-specific training.

% \noindent
% \textbf{Chinese Models vs English Models.}
% Chinese models generally demonstrate higher OA in Chinese tasks compared to English tasks, with Qwen-Max achieving 93.50\% OA in Chinese versus 87.60\% in English. Conversely, English models tend to perform better in English tasks, exemplified by Llama3-70B's 72.50\% OA in English compared to 65.75\% in Chinese. 
% However, both Chinese and English models exhibit important drops in ARA across languages, indicating challenges in maintaining performance when faced with variations. This trend suggests that while models may excel in their primary language, they struggle with robustness across linguistic boundaries. 
% Notably, larger models tend to achieve higher ARA scores but also experience more substantial RLA, as seen with Qwen2.5-0.5B (41.70\% ARA, 13.43\% RLA in total) and Qwen2.5-72B (51.25\% ARA, 20.19\% RLA in total). 
% This pattern indicates that while increased model size enhances overall performance, it doesn't necessarily improve robustness proportionally. 
% The discrepancy between OA and ARA across languages underscores the need for improved cross-lingual robustness in language models, particularly as they scale in size and capability.


% \noindent
% \textbf{Comparison between Chinese and English datasets.}
% Generally, models demonstrate higher accuracy on the Chinese dataset compared to the English one, as evidenced by the consistently higher OA, ARA and CRA scores. For instance, GPT-4o achieves an OA of 91.37\%, an ARA of 81.97\% , an CRA of 75.55\% on the Chinese dataset, compared to 88.63\% and 70.17\% respectively on the English dataset. This trend is observed across most models, suggesting that the Chinese dataset is easier than English one. Moreover, the RLA values are typically lower for Chinese, indicating smaller performance drops when dealing with robust variants of Chinese questions. For example, Qwen-Max shows an RLA of 8.68\% for Chinese versus 24.99\% for English, highlighting a more consistent performance in Chinese. The CRA scores further reinforce this observation, with models generally maintaining higher consistency in correct answers for both original and variant Chinese questions.
% We attribute this phenomenon to the fact that blablabla


\paragraph{Analysis on Reasoning Robustness}
To further analyze whether LLMs can maintain reasoning ability from the original question to its variant, Figure~\ref{consis} presents the pairwise performance statistic of the original question and its variant. 
For all LLMs, a significant proportion of variants are answered incorrectly despite LLMs being able to solve the source example. More specifically, closed-source LLMs like GPT-4o and Qwen-Max achieve a 69\% success rate on both HellaSwag and HellaSwag-Pro, with only 3\% failing both. In contrast, open-source LLMs struggle with around 30\% and 20\%, respectively. This shows that closed-source LLMs achieve better alignment between the performance of the original question and its variant, thus better robustness in reasoning ability.

%We use ``HellaSwag \ding{51} HellaSwag-Pro \ding{55}'' to denote that the LLM correctly answers the original question but fails on its variant. 
%For all LLMs, ``HellaSwag \ding{51} HellaSwag-Pro \ding{55}'' occupy a significant proportion, indicating a lack of robustness for current LLMs. 
%Detailed speaking, closed-source LLMs like GPT-4o and Qwen-Max achieve around 69\% of ``HellaSwag \ding{51} HellaSwag-Pro \ding{51}'' and 3\% of ``HellaSwag \ding{55} HellaSwag-Pro \ding{55}''. In contrast, open-source LLMs struggle with around 30\% and 20\%, respectively. This shows that closed-source LLMs achieve better alignment between the performance of the original question and its variant, thus better robustness in reasoning ability. 

\begin{figure}[t]
\centering
\setlength{\abovecaptionskip}{-0.10cm}
\setlength{\belowcaptionskip}{0cm}
\includegraphics[width=1.0\linewidth,scale=1.00]{images/consis.pdf}
\caption{Pairwise performance statistics of the original question and its variant. We use ``HellaSwag \ding{51} HellaSwag-Pro \ding{55}'' to denote that the LLM correctly answers the original question but fails on its variant. }
\label{consis}
\vspace{-10pt}
\end{figure}


\begin{figure}[t]
\centering
\setlength{\abovecaptionskip}{-0.1cm}
\setlength{\belowcaptionskip}{0cm}
\includegraphics[width=1.0\linewidth,scale=1.0]{images/zhu.pdf}
\caption{Each variant's contribution to the RLA score.}
% Parts below the 0 axis indicate that the model’s performance on the variant is improved compared to the original problem.}
\label{fig:zhu}
\vspace{-10pt}
\end{figure}


% If a model can get both the original question and the variant right, we consider it to have transferable reasoning ability. Table \ref{consis} presents the distribution of model performance on the original question and variant pairs. Among all models, the pairs of (HellaSwag \ding{51}HellaSwag-Pro \ding{55}) account for a considerable proportion, i 
% The closed-source models like GPT-4o and Qwen-Max achieve around 69\% portion of (HellaSwag \ding{51}HellaSwag-Pro \ding{51}) and 3\% portion of (HellaSwag \ding{55} HellaSwag-Pro \ding{55}), indicating stronger reasoning transfer ability than other models. In contrast, open-source models struggle more, with around 30\% portion of (HellaSwag \ding{51}HellaSwag-Pro \ding{51}) and 20\% portion of (HellaSwag \ding{55} HellaSwag-Pro \ding{55}). 
% A notable trend is observed among the Qwen2.5 series, where increasing model size from 7B to 72B parameters correlates with improved performance on correct answers for both datasets (33.20\% to 37.38\%) and decreased failure rates (17.69\% to 14.7\%). It underscores the importance of model size in commonsense reasoning tasks.


\subsection{Variant Analysis (RQ2)}
To further analyze the robustness on different variants, we assess the contribution of each variant to the RLA score, as shown in Figure~\ref{fig:zhu}. A higher contribution indicates more non-robust in that type. The key observations are as follows:

\emph{Problem restatement}, \emph{causal inference}, and \emph{sentence ordering} are the least challenging. Almost all LLMs perform well on these variants particularly closed-source LLMs and Qwen2.5 series, indicating that LLMs can effectively handle these forms.

%, thus we pay less attention on these forms.
\emph{Reverse conversion} and \emph{critical testing} each contribute about 10\% to the RLA score. This indicates that current LLMs struggle to fully generalize to these variants, possibly because these variants do not largely exist in the training data. 
% , and reaserchers should pay some attention to this type of variants.

\emph{Negative transformation} and \emph{scenario refinement} are the two most difficult variants, with \emph{negative transformation} being particularly challenging. For almost all LLMs, these two variants account for more than 50\% of the RLA score. 
%This may be related to the tiny data proportion in the training data of these two types, and the intrinsic challenge of counterfactual reasoning. 
This might be due to statistical bias in these two types of data during pre-training and the exploitation of shortcuts in the corpus~\cite{chen-etal-2023-say, wu-etal-2024-reasoning}. 

% 1. Problem restCausal Inference 
% To further analysis the impact of different varients, we further 
% Figure \ref{fig: zhu} presents a comprehensive analysis of various LLMs' performance across different variant types. Negative transformation emerges as the most challenging task for all models, with scores consistently above 50.00\% and peaking at 78.38\% for Gemini-1.5-Pro. Conversely, problem restatement appears to be the least challenging, with most models scoring in the negative range. Intriguingly, smaller models like Qwen2.5-0.5B demonstrate unexpected strengths in certain areas, such as sentence sorting (7.75\%), outperforming some larger counterparts. A detailed analysis of each variant type follows.

% \noindent
% \textbf{Causal inference.} In this category, scores vary widely from -4.73\% for Qwen-Max to 12.25\% for Baichuan2-13B, illustrating differing degrees of sensitivity to causal reasoning among the models. Smaller models, such as Qwen2.5-0.5B and Qwen2.5-1.5B, achieve better scores, indicating relatively stronger robustness in causal reasoning. Conversely, larger models, like Baichuan2-13B, have higher scores, suggesting greater sensitivity to the challenges of inferring causality.

% \noindent
% \textbf{Critical testing.} Larger models, including Qwen2.5-72B and DeepSeek-67B, exhibit higher RLA scores of 30.50\% and 31.37\%, respectively, suggesting increased sensitivity when dealing with incomplete key information. In contrast, GPT-4o achieves the lowest score, highlighting its superior robustness in critical reasoning. This trend indicates that more complex models might struggle to handle incomplete contexts, underscoring potential areas for improvement in sophisticated architectures.

% \noindent
% \textbf{Negative transformation.} This aspect remains consistently challenging for all models, with scores ranging from 48.88\% to 78.38\%. Advanced commercial models like Gemini-1.5-Pro and Claude-3.5 also score higher (78.38\% and 76.43\%, respectively), indicating a prevalent sensitivity issue in reasoning processes when handling negations, irrespective of model size or architecture.

% \noindent
% \textbf{Problem restatement.} The negative values in this category for nearly all models suggest it is not particularly challenging. This is surprising, given that previous models were quite sensitive to sentence representation.

% \noindent
% \textbf{Reverse conversion.} This variation, which involves swapping the roles of the question and answer, seems to specifically impact larger models. For example, Qwen2.5-72B and DeepSeek-67B exhibit higher RLA scores of 24.38\% and 27.43\%, respectively, indicating heightened sensitivity to reverse reasoning compared to their performance on original questions.

% \noindent
% \textbf{Scenario refinement.} The scores range from 16.06\% for Gemma-2-2B to 32.56\% for Qwen2.5-72B, with larger models displaying more sensitivity in adapting to counterfactual predictions. This suggests that larger models may rely more heavily on general commonsense rather than flexibly adapting to specific contexts. Consequently, increased model complexity might adversely affect adaptability to scenario changes, underscoring the need for enhanced flexibility in advanced models.

% \noindent
% \textbf{Sentence sorting.} This category exhibits the most varied results across models. Some larger models like DeepSeek-67B and InternLM2.5-20B display higher scores (26.69\% and 26.68\%), indicating sensitivity, while others like Qwen2.5-72B and Gemini-1.5-Pro excel with lower scores (-9.88\% and -1.07\%, respectively). This suggests that sentence sorting ability may depend more on specific training approaches rather than being solely contingent on model size.



\begin{figure*}[t]
\centering
\setlength{\abovecaptionskip}{0.05cm}
\setlength{\belowcaptionskip}{0cm}
\includegraphics[width=1.0\linewidth,scale=1.0]{images/xing_v2.pdf}
\caption{Performance on different 3-shot prompts. For the legend, the first two parts are the prompt name, and the third part is the dataset language. NT, CT, CI, SO, SR, RC, PR are the abbreviations for the variant names of Negation Transformation, Critical Testing, Causal Inference, Sentence Ordering, Scenario Refinement, Reverse Conversion and Problem Restatement. }
\label{xing}
\vspace{-10pt}
\end{figure*}







\subsection{Different Prompting Strategies (RQ3)}
To explore the impact of various prompting strategies on our benchmark, we test the performance of all LLMs under different prompting strategies (\cf Section~\ref{sec:prompting_strategy}). 
The results are summarized in Table~\ref{prompt}. 


For both Chinese and English datasets, Chinese LLMs perform best under CN-CoT strategy with shots, followed closely by EN-CoT with shots, achieving overall scores of 67.36\% and 67.03\%, respectively. 
Conversely, English LLMs show optimal performance using EN-CoT approach with shots, attaining 67.55\% on the Chinese dataset and 60.36\% on the English one.
This shows that different LLMs favor the prompts in their native language. 
Besides, translating datasets into LLMs' native languages before reasoning does not enhance performance (e.g., 28.69\% for EN LLMs using EN-XLT with shots vs 41.69\% for EN LLMs using Direct).
This phenomenon is further illustrated in Figure~\ref{xing}.
\begin{table}[H]
\centering
\setlength{\abovecaptionskip}{0.05cm}
\setlength{\belowcaptionskip}{0cm}
\setlength{\tabcolsep}{8pt}
% \footnotesize
\scalebox{0.8}{
\begin{tabular}{cccccc}
\hline
\multicolumn{3}{c|}{Prompt}& \multicolumn{3}{c}{LLM} \\
Strategy & Language & \multicolumn{1}{l|}{\#shot} & CN  & EN  & AVG \\ \hline
\multicolumn{6}{c}{\textbf{\textit{Chinese HellaSwag-Pro}}} \\
Direct& -& \multicolumn{1}{l|}{0}  & 48.95& 41.16& 45.06\\
\hdashline[0.5pt/5pt]
CoT  & CN& \multicolumn{1}{l|}{3}  & \textbf{71.04} & 51.90& 61.47\\ 
CoT  & EN& \multicolumn{1}{l|}{3}  & 70.95& \textbf{67.55} & \textbf{69.25} \\
XLT  & EN& \multicolumn{1}{l|}{3}  & 41.48& 28.69& 35.09\\
\hdashline[0.5pt/5pt]
CoT  & CN& \multicolumn{1}{l|}{0}  & 44.82& 23.89& 34.36\\
CoT  & EN& \multicolumn{1}{l|}{0}  & 45.38& 31.39& 38.39\\
XLT  & EN& \multicolumn{1}{l|}{0}  & 28.57& 12.93& 20.75\\ \hline
\multicolumn{6}{c}{\textbf{\textit{English HellaSwag-Pro}}} \\
Direct& -& \multicolumn{1}{l|}{0}  & 47.46& 40.66& 44.06\\
\hdashline[0.5pt/5pt]
CoT  & CN& \multicolumn{1}{l|}{3}  & \textbf{63.67} & 47.24& 55.46\\
CoT  & EN& \multicolumn{1}{l|}{3}  & 63.12& \textbf{60.36} & \textbf{61.74} \\
XLT  & CN& \multicolumn{1}{l|}{3}  & 48.77& 16.61& 32.69\\
\hdashline[0.5pt/5pt]
CoT  & CN& \multicolumn{1}{l|}{0}  & 34.89& 18.25& 26.57\\
CoT  & EN& \multicolumn{1}{l|}{0}  & 42.41& 31.03& 36.72\\
XLT  & CN& \multicolumn{1}{l|}{0}  & 16.36& 11.22& 13.79\\ \hline
\multicolumn{6}{c}{\textbf{\textit{HellaSwag-Pro}}}\\
Direct& -& \multicolumn{1}{l|}{0}  & 48.21& 40.91& 44.83\\
\hdashline[0.5pt/5pt]
CoT  & CN& \multicolumn{1}{l|}{3}  & \textbf{67.36} & 49.57& 58.46\\
CoT  & EN& \multicolumn{1}{l|}{3}  & 67.03& \textbf{63.95} & \textbf{65.49} \\
XLT  & CN& \multicolumn{1}{l|}{3}  & 59.91& 34.26& 47.08\\
XLT  & EN& \multicolumn{1}{l|}{3}  & 52.30& 44.52& 48.41\\
\hdashline[0.5pt/5pt]
CoT  & CN& \multicolumn{1}{l|}{0}  & 39.86& 21.07& 30.46\\
CoT  & EN& \multicolumn{1}{l|}{0}  & 43.90& 31.21& 37.55\\
XLT  & CN& \multicolumn{1}{l|}{0}  & 30.59& 17.55& 24.07\\
XLT  & EN& \multicolumn{1}{l|}{0}  & 35.49& 21.98& 28.74  \\ \hline
\end{tabular}
}
\caption{Average ARA of all open-source LLMs on different prompting strategies. CN-LLMs contains 17 LLMs, and EN-LLMs contains 7 LLMs. The best results for each dataset are \textbf{bolded}. Detailed results for all evaluated models are provided in the Appendix \ref{detailed}.}
\label{prompt}
\vspace{-10pt}
\end{table}


Our findings differ from previous research \cite{huang-etal-2023-languages,shi2022language}, which suggested that translating non-English tasks into English (XLT) would perform better than using native languages. And these research only focused on English LLMs while overlooking Chinese LLMs. We find that LLMs perform better when reasoning directly in their native language compared to XLT, addressing this gap in previous research.
%Conducting CoT reasoning in the LLM's native language generally yields better results compared to direct reasoning. 
%Furthermore, using few-shot demonstrations consistently boosts performance across most prompting strategies and LLMs, emphasizing the advantages of in-context examples. 

% Furthermore, increasing the number of demonstrations consistently boosts performance across most prompting strategies and LLMs, emphasizing the advantages of in-context examples. 





