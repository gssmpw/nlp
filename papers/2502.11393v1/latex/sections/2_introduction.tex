\section{Introduction}


Commonsense reasoning is a crucial part of intelligence, involving contextual understanding, implicit knowledge, and logical deduction （\citealp{liu2004conceptnet, cambria2011isanette, davis2015commonsense}). Recent research has focused on enhancing these capabilities in large language models (LLMs) （\citealp{yang2024qwen2, openai2024gpt, team2024gemini}), leading to impressive results on benchmarks like HellaSwag (\citealp{zellers2019hellaswag}), MCTACO (\citealp{zhou2019going}), and PIQA (\citealp{bisk2020piqa}). 
%(daily commonsense)
%(temporal commonsense)
%(physics commonsense)
In particular, both proprietary models like GPT-4o and open-source alternatives such as Qwen2.5-72B have reported near 90\% precision in HellaSwag. However, this raises an important question: Does this high-level performance stem from a genuine understanding of commonsense, or is it simply a result of memorizing superficial patterns?


\begin{figure}[t]   
\centering
\setlength{\abovecaptionskip}{0.1cm}
\setlength{\belowcaptionskip}{0cm}
\includegraphics[width=\linewidth,scale=1.00]{images/example.pdf}
% \caption{The difference between our work and existing works. Traditional evaluation has reached almost saturation performance, but our evaluation launches new challenges for the mathematical reasoning ability.}
    %\vspace{-10pt}
\caption{Comparison of GPT-4o's responses to various questions about the same sentence reveals that it successfully handles positive reasoning but struggles with negative reasoning.}
\label{example}
\vspace{-15pt}
\end{figure}
%As shown in Figure \ref{example}, while GPT-4o can successfully perform positive reasoning, choosing the correct subsequent event based on the context, it has difficulty performing reverse reasoning, inferring the context from the subsequent event. A model that truly masters commonsense reasoning should perform well on various variants of the same task.
%As shown in Figure \ref{example}, while GPT-4o can successfully perform positive reasoning, i.e. reasoning about potential subsequent event from a positive perspective, it struggles with negative reasoning, which involves deducing what won't happen from a negative standpoint. In fact, our experiments have showed that the accuracy for positive reasoning is around 90\%, while negative reasoning barely reaches 5\%. This leads us to the question whether commonsense reasoning is non-robust only in this particular variant, or if it exhibits similar behavior in other variants as well.

%Existing work on commonsense reasoning primarily focuses on general benchmarks (\citealp{zellers2019hellaswag}; \citealp{talmor2019commonsenseqa}; \citealp{mihaylov2018can}) or explores specific aspects such as temporal (\citealp{zhou2019going}; \citealp{qin2021timedial}) and physical (\citealp{bisk2020piqa}) commonsense knowledge to assess model performance in this area. Only a small portion of research has examined robustness in simplified forms. For instance, RICA\citep{zhou2021rica} investigates robustness in commonsense through easy text perturbations. This is clearly far from sufficient.

%Previous work has either established general benchmarks (\citealp{zellers2019hellaswag}; \citealp{talmor2019commonsenseqa}; \citealp{mihaylov2018can}) or explored specific aspects such as temporal (\citealp{zhou2019going}; \citealp{qin2021timedial}) and physical (\citealp{bisk2020piqa}) commonsense knowledge to assess model performance in this area. However, they have overlooked the robustness of commonsense reasoning.
% current models may largely remain memorizing superficial patterns, which has not been fullly investigated by existing work. 

Despite the outstanding performance, current LLMs still largely rely on memorizing superficial patterns, which has not been fully investigated by existing work. As shown in Figure \ref{example}, GPT-4o can correctly answer the original question but fails on its variant, \textit{i.e.}, transformation that changes the reasoning approach without altering the question's essence. If a model truly grasps the commonsense knowledge inherent in a question, it should be able to answer all its variants correctly. This robustness is crucial for testing a model's genuine commonsense reasoning ability. 
Existing research mainly evaluates LLMs on general benchmarks \cite{zellers2019hellaswag, talmor2019commonsenseqa, mihaylov2018can}, 
or specific domains of commonsense knowledge \cite{zhou2019going, qin2021timedial, bisk2020piqa}. 
% such as temporal \cite{zhou2019going, qin2021timedial} and physical \cite{bisk2020piqa} knowledge. 
%to assess model performance in this area and overlooks the robustness of commonsense reasoning. 
Although some effort has been made in examining the robustness of commonsense reasoning \cite{}, they either focus on a single method without comparative analysis, or deal with overly simplistic scenarios \citep{zhou2021rica}. 

To address this gap, we present the first comprehensive investigation on the robustness of commonsense reasoning, aiming to systematically assess the genuine capabilities of LLMs in this area. 
Moreover, recognizing that existing benchmarks are predominantly in English, limiting non-English assessment (\citealp{davis2023benchmarks}), we develop a Chinese commonsense reasoning dataset for robustness evaluation, following the widely-used HellaSwag. 
Specifically, we meticulously design 56 fine-grained categories and 3 length types for Chinese commonsense reasoning and propose a two-stage method of initial dataset generation and difficult samples replacement. This approach generates 12,000 high-quality, finely-annotated, multiple-choice problems. Inspired by the Bloom Cognitive
Model \cite{krathwohl1973taxonomy}, we collect and devise seven methods to expand the initial data in both Chinese and English HellaSwag to construct HellaSwag-Pro:
1) \textbf{Problem restatement} reformulates the context and correct answer to assess the impact of different text presentations.
2) \textbf{Reverse conversion} uses the correct answer as the question and the context as the answer, testing reverse reasoning ability.
3) \textbf{Causal inference} combines the context and correct answer to probe causality, comparing models' analytical skills.
4) \textbf{Sentence ordering} merges and scrambles the context and correct answer, then asks for reordering to test event sequencing comprehension.
5) \textbf{Scenario refinement} alters context details to enable counterfactual choices, examining contextual decision-making skills.
6) \textbf{Negation Transformation} converts positive statements to negative ones, testing reasoning from a negative perspective.
7) \textbf{Critical testing} omits crucial information from the context, preventing answer selection to assess critical thinking skills. Finally, HellaSwag-Pro contains 11,200 high-quality, human-verified data entries with 1,600 initial bilingual entries.

Using this benchmark, we design four representative prompt strategies to test 41 commercial and open-source models. Through comprehensive evaluation, we derive several key findings:
1) All models are far from robust in commonsense reasoning tasks and fail to reach human-level performance. Even when they can solve original problems correctly, variations still pose significant challenges. Overall, among all models evaluated, GPT-4o performs best on our bilingual robustness benchmark, while the robustness of open-source models aligns with the scaling law.
2) Among all variations, negative transformation proves to be the most challenging category, with an average accuracy of only 9.01\% across all models. However, problem restatement no longer poses a challenge to the models.
3) Our evaluation of bilingual prompting strategies demonstrates that models perform better on evaluation datasets and prompts in the language they were trained on. Using a non-training language for reasoning can even impair model performance. Incorporating CoT and increasing the number of shots can enhance the models' robustness performance.

Our contributions can be summarized as follows:
1) We present the first comprehensive investigation for evaluating the robustness of commonsense reasoning in both Chinese and English, advancing the assessment of LLMs' capabilities towards more granular examination.
2) We propose a two-stage method to create a finely-annotated Chinese commonsense reasoning dataset. Building on this, we develop a bilingual, large-scale benchmark for robustness evaluation of commonsense reasoning, facilitating comprehensive assessment of LLMs' capabilities in this domain.
3) We design diverse prompts and conduct extensive experiments, yielding critical insights. 
We aim to catalyze further advancements in LLMs' commonsense reasoning capabilities through this work.



